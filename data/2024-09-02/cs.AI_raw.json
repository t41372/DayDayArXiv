[
  {
    "arxiv_id": "2409.01502v1",
    "title": "AMG: Avatar Motion Guided Video Generation",
    "authors": [
      "Zhangsihao Yang",
      "Mengyi Shan",
      "Mohammad Farazi",
      "Wenhui Zhu",
      "Yanxi Chen",
      "Xuanzhao Dong",
      "Yalin Wang"
    ],
    "abstract": "Human video generation task has gained significant attention with the\nadvancement of deep generative models. Generating realistic videos with human\nmovements is challenging in nature, due to the intricacies of human body\ntopology and sensitivity to visual artifacts. The extensively studied 2D media\ngeneration methods take advantage of massive human media datasets, but struggle\nwith 3D-aware control; whereas 3D avatar-based approaches, while offering more\nfreedom in control, lack photorealism and cannot be harmonized seamlessly with\nbackground scene. We propose AMG, a method that combines the 2D photorealism\nand 3D controllability by conditioning video diffusion models on controlled\nrendering of 3D avatars. We additionally introduce a novel data processing\npipeline that reconstructs and renders human avatar movements from dynamic\ncamera videos. AMG is the first method that enables multi-person diffusion\nvideo generation with precise control over camera positions, human motions, and\nbackground style. We also demonstrate through extensive evaluation that it\noutperforms existing human video generation methods conditioned on pose\nsequences or driving videos in terms of realism and adaptability.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "The project page is at https://github.com/zshyang/amg",
    "pdf_url": "http://arxiv.org/pdf/2409.01502v1",
    "published_date": "2024-09-02 23:59:01 UTC",
    "updated_date": "2024-09-02 23:59:01 UTC"
  },
  {
    "arxiv_id": "2409.12972v1",
    "title": "TRACE: Transformer-based user Representations from Attributed Clickstream Event sequences",
    "authors": [
      "William Black",
      "Alexander Manlove",
      "Jack Pennington",
      "Andrea Marchini",
      "Ercument Ilhan",
      "Vilda Markeviciute"
    ],
    "abstract": "For users navigating travel e-commerce websites, the process of researching\nproducts and making a purchase often results in intricate browsing patterns\nthat span numerous sessions over an extended period of time. The resulting\nclickstream data chronicle these user journeys and present valuable\nopportunities to derive insights that can significantly enhance personalized\nrecommendations. We introduce TRACE, a novel transformer-based approach\ntailored to generate rich user embeddings from live multi-session clickstreams\nfor real-time recommendation applications. Prior works largely focus on\nsingle-session product sequences, whereas TRACE leverages site-wide page view\nsequences spanning multiple user sessions to model long-term engagement.\nEmploying a multi-task learning framework, TRACE captures comprehensive user\npreferences and intents distilled into low-dimensional representations. We\ndemonstrate TRACE's superior performance over vanilla transformer and LLM-style\narchitectures through extensive experiments on a large-scale travel e-commerce\ndataset of real user journeys, where the challenges of long page-histories and\nsparse targets are particularly prevalent. Visualizations of the learned\nembeddings reveal meaningful clusters corresponding to latent user states and\nbehaviors, highlighting TRACE's potential to enhance recommendation systems by\ncapturing nuanced user interactions and preferences",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "RecSys Workshop on Recommenders in Tourism (RecTour 2024), October\n  14th-18th, 2024, co-located with the 18th ACM Conference on Recommender\n  Systems, Bari, Italy",
    "pdf_url": "http://arxiv.org/pdf/2409.12972v1",
    "published_date": "2024-09-02 23:33:19 UTC",
    "updated_date": "2024-09-02 23:33:19 UTC"
  },
  {
    "arxiv_id": "2409.01491v2",
    "title": "EarthGen: Generating the World from Top-Down Views",
    "authors": [
      "Ansh Sharma",
      "Albert Xiao",
      "Praneet Rathi",
      "Rohit Kundu",
      "Albert Zhai",
      "Yuan Shen",
      "Shenlong Wang"
    ],
    "abstract": "In this work, we present a novel method for extensive multi-scale generative\nterrain modeling. At the core of our model is a cascade of superresolution\ndiffusion models that can be combined to produce consistent images across\nmultiple resolutions. Pairing this concept with a tiled generation method\nyields a scalable system that can generate thousands of square kilometers of\nrealistic Earth surfaces at high resolution. We evaluate our method on a\ndataset collected from Bing Maps and show that it outperforms super-resolution\nbaselines on the extreme super-resolution task of 1024x zoom. We also\ndemonstrate its ability to create diverse and coherent scenes via an\ninteractive gigapixel-scale generated map. Finally, we demonstrate how our\nsystem can be extended to enable novel content creation applications including\ncontrollable world generation and 3D scene generation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "J.2; I.4.8"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01491v2",
    "published_date": "2024-09-02 23:17:56 UTC",
    "updated_date": "2024-09-07 21:49:56 UTC"
  },
  {
    "arxiv_id": "2409.12184v1",
    "title": "Democratizing MLLMs in Healthcare: TinyLLaVA-Med for Efficient Healthcare Diagnostics in Resource-Constrained Settings",
    "authors": [
      "Aya El Mir",
      "Lukelo Thadei Luoga",
      "Boyuan Chen",
      "Muhammad Abdullah Hanif",
      "Muhammad Shafique"
    ],
    "abstract": "Deploying Multi-Modal Large Language Models (MLLMs) in healthcare is hindered\nby their high computational demands and significant memory requirements, which\nare particularly challenging for resource-constrained devices like the Nvidia\nJetson Xavier. This problem is particularly evident in remote medical settings\nwhere advanced diagnostics are needed but resources are limited. In this paper,\nwe introduce an optimization method for the general-purpose MLLM, TinyLLaVA,\nwhich we have adapted and renamed TinyLLaVA-Med. This adaptation involves\ninstruction-tuning and fine-tuning TinyLLaVA on a medical dataset by drawing\ninspiration from the LLaVA-Med training pipeline. Our approach successfully\nminimizes computational complexity and power consumption, with TinyLLaVA-Med\noperating at 18.9W and using 11.9GB of memory, while achieving accuracies of\n64.54% on VQA-RAD and 70.70% on SLAKE for closed-ended questions. Therefore,\nTinyLLaVA-Med achieves deployment viability in hardware-constrained\nenvironments with low computational resources, maintaining essential\nfunctionalities and delivering accuracies close to state-of-the-art models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12184v1",
    "published_date": "2024-09-02 21:14:16 UTC",
    "updated_date": "2024-09-02 21:14:16 UTC"
  },
  {
    "arxiv_id": "2409.01466v2",
    "title": "Enhancing LLM-Based Text Classification in Political Science: Automatic Prompt Optimization and Dynamic Exemplar Selection for Few-Shot Learning",
    "authors": [
      "Menglin Liu",
      "Ge Shi"
    ],
    "abstract": "Large language models (LLMs) offer substantial promise for text\nclassification in political science, yet their effectiveness often depends on\nhigh-quality prompts and exemplars. To address this, we introduce a three-stage\nframework that enhances LLM performance through automatic prompt optimization,\ndynamic exemplar selection, and a consensus mechanism. Our approach automates\nprompt refinement using task-specific exemplars, eliminating speculative\ntrial-and-error adjustments and producing structured prompts aligned with\nhuman-defined criteria. In the second stage, we dynamically select the most\nrelevant exemplars, ensuring contextually appropriate guidance for each query.\nFinally, our consensus mechanism mimics the role of multiple human coders for a\nsingle task, combining outputs from LLMs to achieve high reliability and\nconsistency at a reduced cost. Evaluated across tasks including sentiment\nanalysis, stance detection, and campaign ad tone classification, our method\nenhances classification accuracy without requiring task-specific model\nretraining or extensive manual adjustments to prompts. This framework not only\nboosts accuracy, interpretability and transparency but also provides a\ncost-effective, scalable solution tailored to political science applications.\nAn open-source Python package (PoliPrompt) is available on GitHub.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "46 pages, 15 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.01466v2",
    "published_date": "2024-09-02 21:05:31 UTC",
    "updated_date": "2025-04-06 15:38:38 UTC"
  },
  {
    "arxiv_id": "2409.01449v2",
    "title": "Real-Time Recurrent Learning using Trace Units in Reinforcement Learning",
    "authors": [
      "Esraa Elelimy",
      "Adam White",
      "Michael Bowling",
      "Martha White"
    ],
    "abstract": "Recurrent Neural Networks (RNNs) are used to learn representations in\npartially observable environments. For agents that learn online and continually\ninteract with the environment, it is desirable to train RNNs with real-time\nrecurrent learning (RTRL); unfortunately, RTRL is prohibitively expensive for\nstandard RNNs. A promising direction is to use linear recurrent architectures\n(LRUs), where dense recurrent weights are replaced with a complex-valued\ndiagonal, making RTRL efficient. In this work, we build on these insights to\nprovide a lightweight but effective approach for training RNNs in online RL. We\nintroduce Recurrent Trace Units (RTUs), a small modification on LRUs that we\nnonetheless find to have significant performance benefits over LRUs when\ntrained with RTRL. We find RTUs significantly outperform other recurrent\narchitectures across several partially observable environments while using\nsignificantly less computation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Neurips 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.01449v2",
    "published_date": "2024-09-02 20:08:23 UTC",
    "updated_date": "2024-10-30 04:07:51 UTC"
  },
  {
    "arxiv_id": "2409.01437v1",
    "title": "Kvasir-VQA: A Text-Image Pair GI Tract Dataset",
    "authors": [
      "Sushant Gautam",
      "Andrea Storås",
      "Cise Midoglu",
      "Steven A. Hicks",
      "Vajira Thambawita",
      "Pål Halvorsen",
      "Michael A. Riegler"
    ],
    "abstract": "We introduce Kvasir-VQA, an extended dataset derived from the HyperKvasir and\nKvasir-Instrument datasets, augmented with question-and-answer annotations to\nfacilitate advanced machine learning tasks in Gastrointestinal (GI)\ndiagnostics. This dataset comprises 6,500 annotated images spanning various GI\ntract conditions and surgical instruments, and it supports multiple question\ntypes including yes/no, choice, location, and numerical count. The dataset is\nintended for applications such as image captioning, Visual Question Answering\n(VQA), text-based generation of synthetic medical images, object detection, and\nclassification. Our experiments demonstrate the dataset's effectiveness in\ntraining models for three selected tasks, showcasing significant applications\nin medical image analysis and diagnostics. We also present evaluation metrics\nfor each task, highlighting the usability and versatility of our dataset. The\ndataset and supporting artifacts are available at\nhttps://datasets.simula.no/kvasir-vqa.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "to be published in VLM4Bio 2024, part of the ACM Multimedia (ACM MM)\n  conference 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.01437v1",
    "published_date": "2024-09-02 19:41:59 UTC",
    "updated_date": "2024-09-02 19:41:59 UTC"
  },
  {
    "arxiv_id": "2409.02141v1",
    "title": "Efficient and Scalable Estimation of Tool Representations in Vector Space",
    "authors": [
      "Suhong Moon",
      "Siddharth Jha",
      "Lutfi Eren Erdogan",
      "Sehoon Kim",
      "Woosang Lim",
      "Kurt Keutzer",
      "Amir Gholami"
    ],
    "abstract": "Recent advancements in function calling and tool use have significantly\nenhanced the capabilities of large language models (LLMs) by enabling them to\ninteract with external information sources and execute complex tasks. However,\nthe limited context window of LLMs presents challenges when a large number of\ntools are available, necessitating efficient methods to manage prompt length\nand maintain accuracy. Existing approaches, such as fine-tuning LLMs or\nleveraging their reasoning capabilities, either require frequent retraining or\nincur significant latency overhead. A more efficient solution involves training\nsmaller models to retrieve the most relevant tools for a given query, although\nthis requires high quality, domain-specific data. To address those challenges,\nwe present a novel framework for generating synthetic data for tool retrieval\napplications and an efficient data-driven tool retrieval strategy using small\nencoder models. Empowered by LLMs, we create ToolBank, a new tool retrieval\ndataset that reflects real human user usages. For tool retrieval methodologies,\nwe propose novel approaches: (1) Tool2Vec: usage-driven tool embedding\ngeneration for tool retrieval, (2) ToolRefiner: a staged retrieval method that\niteratively improves the quality of retrieved tools, and (3) MLC: framing tool\nretrieval as a multi-label classification problem. With these new methods, we\nachieve improvements of up to 27.28 in Recall@K on the ToolBench dataset and\n30.5 in Recall@K on ToolBank. Additionally, we present further experimental\nresults to rigorously validate our methods. Our code is available at\n\\url{https://github.com/SqueezeAILab/Tool2Vec}",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.02141v1",
    "published_date": "2024-09-02 19:39:24 UTC",
    "updated_date": "2024-09-02 19:39:24 UTC"
  },
  {
    "arxiv_id": "2409.02140v1",
    "title": "Self-Supervised Learning for Identifying Defects in Sewer Footage",
    "authors": [
      "Daniel Otero",
      "Rafael Mateus"
    ],
    "abstract": "Sewerage infrastructure is among the most expensive modern investments\nrequiring time-intensive manual inspections by qualified personnel. Our study\naddresses the need for automated solutions without relying on large amounts of\nlabeled data. We propose a novel application of Self-Supervised Learning (SSL)\nfor sewer inspection that offers a scalable and cost-effective solution for\ndefect detection. We achieve competitive results with a model that is at least\n5 times smaller than other approaches found in the literature and obtain\ncompetitive performance with 10\\% of the available data when training with a\nlarger architecture. Our findings highlight the potential of SSL to\nrevolutionize sewer maintenance in resource-limited settings.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Poster at the LatinX in AI Workshop @ ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.02140v1",
    "published_date": "2024-09-02 19:28:48 UTC",
    "updated_date": "2024-09-02 19:28:48 UTC"
  },
  {
    "arxiv_id": "2409.02139v2",
    "title": "The Role of Transformer Models in Advancing Blockchain Technology: A Systematic Survey",
    "authors": [
      "Tianxu Liu",
      "Yanbin Wang",
      "Jianguo Sun",
      "Ye Tian",
      "Yanyu Huang",
      "Tao Xue",
      "Peiyue Li",
      "Yiwei Liu"
    ],
    "abstract": "As blockchain technology rapidly evolves, the demand for enhanced efficiency,\nsecurity, and scalability grows.Transformer models, as powerful deep learning\narchitectures,have shown unprecedented potential in addressing various\nblockchain challenges. However, a systematic review of Transformer applications\nin blockchain is lacking. This paper aims to fill this research gap by\nsurveying over 200 relevant papers, comprehensively reviewing practical cases\nand research progress of Transformers in blockchain applications. Our survey\ncovers key areas including anomaly detection, smart contract security analysis,\ncryptocurrency prediction and trend analysis, and code summary generation. To\nclearly articulate the advancements of Transformers across various blockchain\ndomains, we adopt a domain-oriented classification system, organizing and\nintroducing representative methods based on major challenges in current\nblockchain research. For each research domain,we first introduce its background\nand objectives, then review previous representative methods and analyze their\nlimitations,and finally introduce the advancements brought by Transformer\nmodels. Furthermore, we explore the challenges of utilizing Transformer, such\nas data privacy, model complexity, and real-time processing requirements.\nFinally, this article proposes future research directions, emphasizing the\nimportance of exploring the Transformer architecture in depth to adapt it to\nspecific blockchain applications, and discusses its potential role in promoting\nthe development of blockchain technology. This review aims to provide new\nperspectives and a research foundation for the integrated development of\nblockchain technology and machine learning, supporting further innovation and\napplication expansion of blockchain technology.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.02139v2",
    "published_date": "2024-09-02 19:12:54 UTC",
    "updated_date": "2024-09-05 11:09:38 UTC"
  },
  {
    "arxiv_id": "2409.01411v1",
    "title": "Performance-Aware Self-Configurable Multi-Agent Networks: A Distributed Submodular Approach for Simultaneous Coordination and Network Design",
    "authors": [
      "Zirui Xu",
      "Vasileios Tzoumas"
    ],
    "abstract": "We introduce the first, to our knowledge, rigorous approach that enables\nmulti-agent networks to self-configure their communication topology to balance\nthe trade-off between scalability and optimality during multi-agent planning.\nWe are motivated by the future of ubiquitous collaborative autonomy where\nnumerous distributed agents will be coordinating via agent-to-agent\ncommunication to execute complex tasks such as traffic monitoring, event\ndetection, and environmental exploration. But the explosion of information in\nsuch large-scale networks currently curtails their deployment due to\nimpractical decision times induced by the computational and communication\nrequirements of the existing near-optimal coordination algorithms. To overcome\nthis challenge, we present the AlterNAting COordination and Network-Design\nAlgorithm (Anaconda), a scalable algorithm that also enjoys near-optimality\nguarantees. Subject to the agents' bandwidth constraints, Anaconda enables the\nagents to optimize their local communication neighborhoods such that the\naction-coordination approximation performance of the network is maximized.\nCompared to the state of the art, Anaconda is an anytime self-configurable\nalgorithm that quantifies its suboptimality guarantee for any type of network,\nfrom fully disconnected to fully centralized, and that, for sparse networks, is\none order faster in terms of decision speed. To develop the algorithm, we\nquantify the suboptimality cost due to decentralization, i.e., due to\ncommunication-minimal distributed coordination. We also employ tools inspired\nby the literature on multi-armed bandits and submodular maximization subject to\ncardinality constraints. We demonstrate Anaconda in simulated scenarios of area\nmonitoring and compare it with a state-of-the-art algorithm.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.MA",
      "cs.RO",
      "cs.SY",
      "math.OC"
    ],
    "primary_category": "eess.SY",
    "comment": "Accepted to CDC 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.01411v1",
    "published_date": "2024-09-02 18:11:33 UTC",
    "updated_date": "2024-09-02 18:11:33 UTC"
  },
  {
    "arxiv_id": "2409.01392v2",
    "title": "ComfyBench: Benchmarking LLM-based Agents in ComfyUI for Autonomously Designing Collaborative AI Systems",
    "authors": [
      "Xiangyuan Xue",
      "Zeyu Lu",
      "Di Huang",
      "Zidong Wang",
      "Wanli Ouyang",
      "Lei Bai"
    ],
    "abstract": "Much previous AI research has focused on developing monolithic models to\nmaximize their intelligence, with the primary goal of enhancing performance on\nspecific tasks. In contrast, this work attempts to study using LLM-based agents\nto design collaborative AI systems autonomously. To explore this problem, we\nfirst introduce ComfyBench to evaluate agents's ability to design collaborative\nAI systems in ComfyUI. ComfyBench is a comprehensive benchmark comprising 200\ndiverse tasks covering various instruction-following generation challenges,\nalong with detailed annotations for 3,205 nodes and 20 workflows. Based on\nComfyBench, we further develop ComfyAgent, a novel framework that empowers\nLLM-based agents to autonomously design collaborative AI systems by generating\nworkflows. ComfyAgent is based on two core concepts. First, it represents\nworkflows with code, which can be reversibly converted into workflows and\nexecuted as collaborative systems by the interpreter. Second, it constructs a\nmulti-agent system that cooperates to learn from existing workflows and\ngenerate new workflows for a given task. While experimental results demonstrate\nthat ComfyAgent achieves a comparable resolve rate to o1-preview and\nsignificantly surpasses other agents on ComfyBench, ComfyAgent has resolved\nonly 15\\% of creative tasks. LLM-based agents still have a long way to go in\nautonomously designing collaborative AI systems. Progress with ComfyBench is\npaving the way for more intelligent and autonomous collaborative AI systems.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01392v2",
    "published_date": "2024-09-02 17:44:10 UTC",
    "updated_date": "2024-11-26 14:32:46 UTC"
  },
  {
    "arxiv_id": "2409.01387v1",
    "title": "VLSI Hypergraph Partitioning with Deep Learning",
    "authors": [
      "Muhammad Hadir Khan",
      "Bugra Onal",
      "Eren Dogan",
      "Matthew R. Guthaus"
    ],
    "abstract": "Partitioning is a known problem in computer science and is critical in chip\ndesign workflows, as advancements in this area can significantly influence\ndesign quality and efficiency. Deep Learning (DL) techniques, particularly\nthose involving Graph Neural Networks (GNNs), have demonstrated strong\nperformance in various node, edge, and graph prediction tasks using both\ninductive and transductive learning methods. A notable area of recent interest\nwithin GNNs are pooling layers and their application to graph partitioning.\nWhile these methods have yielded promising results across social,\ncomputational, and other random graphs, their effectiveness has not yet been\nexplored in the context of VLSI hypergraph netlists. In this study, we\nintroduce a new set of synthetic partitioning benchmarks that emulate\nreal-world netlist characteristics and possess a known upper bound for solution\ncut quality. We distinguish these benchmarks with the prior work and evaluate\nexisting state-of-the-art partitioning algorithms alongside GNN-based\napproaches, highlighting their respective advantages and disadvantages.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.DC",
      "cs.LG"
    ],
    "primary_category": "cs.AR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01387v1",
    "published_date": "2024-09-02 17:32:01 UTC",
    "updated_date": "2024-09-02 17:32:01 UTC"
  },
  {
    "arxiv_id": "2409.01382v1",
    "title": "Automatic Detection of LLM-generated Code: A Case Study of Claude 3 Haiku",
    "authors": [
      "Musfiqur Rahman",
      "SayedHassan Khatoonabadi",
      "Ahmad Abdellatif",
      "Emad Shihab"
    ],
    "abstract": "Using Large Language Models (LLMs) has gained popularity among software\ndevelopers for generating source code. However, the use of LLM-generated code\ncan introduce risks of adding suboptimal, defective, and vulnerable code. This\nmakes it necessary to devise methods for the accurate detection of\nLLM-generated code. Toward this goal, we perform a case study of Claude 3 Haiku\n(or Claude 3 for brevity) on CodeSearchNet dataset. We divide our analyses into\ntwo parts: function-level and class-level. We extract 22 software metric\nfeatures, such as Code Lines and Cyclomatic Complexity, for each level of\ngranularity. We then analyze code snippets generated by Claude 3 and their\nhuman-authored counterparts using the extracted features to understand how\nunique the code generated by Claude 3 is. In the following step, we use the\nunique characteristics of Claude 3-generated code to build Machine Learning\n(ML) models and identify which features of the code snippets make them more\ndetectable by ML models. Our results indicate that Claude 3 tends to generate\nlonger functions, but shorter classes than humans, and this characteristic can\nbe used to detect Claude 3-generated code with ML models with 82% and 66%\naccuracies for function-level and class-level snippets, respectively.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "Submitted to a journal for potential publication",
    "pdf_url": "http://arxiv.org/pdf/2409.01382v1",
    "published_date": "2024-09-02 17:25:15 UTC",
    "updated_date": "2024-09-02 17:25:15 UTC"
  },
  {
    "arxiv_id": "2409.01374v1",
    "title": "H-ARC: A Robust Estimate of Human Performance on the Abstraction and Reasoning Corpus Benchmark",
    "authors": [
      "Solim LeGris",
      "Wai Keen Vong",
      "Brenden M. Lake",
      "Todd M. Gureckis"
    ],
    "abstract": "The Abstraction and Reasoning Corpus (ARC) is a visual program synthesis\nbenchmark designed to test challenging out-of-distribution generalization in\nhumans and machines. Since 2019, limited progress has been observed on the\nchallenge using existing artificial intelligence methods. Comparing human and\nmachine performance is important for the validity of the benchmark. While\nprevious work explored how well humans can solve tasks from the ARC benchmark,\nthey either did so using only a subset of tasks from the original dataset, or\nfrom variants of ARC, and therefore only provided a tentative estimate of human\nperformance. In this work, we obtain a more robust estimate of human\nperformance by evaluating 1729 humans on the full set of 400 training and 400\nevaluation tasks from the original ARC problem set. We estimate that average\nhuman performance lies between 73.3% and 77.2% correct with a reported\nempirical average of 76.2% on the training set, and between 55.9% and 68.9%\ncorrect with a reported empirical average of 64.2% on the public evaluation\nset. However, we also find that 790 out of the 800 tasks were solvable by at\nleast one person in three attempts, suggesting that the vast majority of the\npublicly available ARC tasks are in principle solvable by typical crowd-workers\nrecruited over the internet. Notably, while these numbers are slightly lower\nthan earlier estimates, human performance still greatly exceeds current\nstate-of-the-art approaches for solving ARC. To facilitate research on ARC, we\npublicly release our dataset, called H-ARC (human-ARC), which includes all of\nthe submissions and action traces from human participants.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "12 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.01374v1",
    "published_date": "2024-09-02 17:11:32 UTC",
    "updated_date": "2024-09-02 17:11:32 UTC"
  },
  {
    "arxiv_id": "2409.01369v2",
    "title": "Imitating Language via Scalable Inverse Reinforcement Learning",
    "authors": [
      "Markus Wulfmeier",
      "Michael Bloesch",
      "Nino Vieillard",
      "Arun Ahuja",
      "Jorg Bornschein",
      "Sandy Huang",
      "Artem Sokolov",
      "Matt Barnes",
      "Guillaume Desjardins",
      "Alex Bewley",
      "Sarah Maria Elisabeth Bechtle",
      "Jost Tobias Springenberg",
      "Nikola Momchev",
      "Olivier Bachem",
      "Matthieu Geist",
      "Martin Riedmiller"
    ],
    "abstract": "The majority of language model training builds on imitation learning. It\ncovers pretraining, supervised fine-tuning, and affects the starting conditions\nfor reinforcement learning from human feedback (RLHF). The simplicity and\nscalability of maximum likelihood estimation (MLE) for next token prediction\nled to its role as predominant paradigm. However, the broader field of\nimitation learning can more effectively utilize the sequential structure\nunderlying autoregressive generation. We focus on investigating the inverse\nreinforcement learning (IRL) perspective to imitation, extracting rewards and\ndirectly optimizing sequences instead of individual token likelihoods and\nevaluate its benefits for fine-tuning large language models. We provide a new\nangle, reformulating inverse soft-Q-learning as a temporal difference\nregularized extension of MLE. This creates a principled connection between MLE\nand IRL and allows trading off added complexity with increased performance and\ndiversity of generations in the supervised fine-tuning (SFT) setting. We find\nclear advantages for IRL-based imitation, in particular for retaining diversity\nwhile maximizing task performance, rendering IRL a strong alternative on fixed\nSFT datasets even without online data generation. Our analysis of IRL-extracted\nreward functions further indicates benefits for more robust reward functions\nvia tighter integration of supervised and preference-based LLM post-training.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Published at NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.01369v2",
    "published_date": "2024-09-02 16:48:57 UTC",
    "updated_date": "2024-12-09 14:26:42 UTC"
  },
  {
    "arxiv_id": "2409.01366v2",
    "title": "CHESS: Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification",
    "authors": [
      "Junhui He",
      "Shangyu Wu",
      "Weidong Wen",
      "Chun Jason Xue",
      "Qingan Li"
    ],
    "abstract": "Deploying large language models (LLMs) on edge devices presents significant\nchallenges due to the substantial computational overhead and memory\nrequirements. Activation sparsification can mitigate these resource challenges\nby reducing the number of activated neurons during inference. Existing methods\ntypically employ thresholding-based sparsification based on the statistics of\nactivation tensors. However, they do not model the impact of activation\nsparsification on performance, resulting in suboptimal performance degradation.\nTo address the limitations, this paper reformulates the activation\nsparsification problem to explicitly capture the relationship between\nactivation sparsity and model performance. Then, this paper proposes CHESS, a\ngeneral activation sparsification approach via CHannel-wise thrEsholding and\nSelective Sparsification. First, channel-wise thresholding assigns a unique\nthreshold to each activation channel in the feed-forward network (FFN) layers.\nThen, selective sparsification involves applying thresholding-based activation\nsparsification to specific layers within the attention modules. Finally, we\ndetail the implementation of sparse kernels to accelerate LLM inference.\nExperimental results demonstrate that the proposed CHESS achieves lower\nperformance degradation over eight downstream tasks while activating fewer\nparameters than existing methods, thus speeding up the LLM inference by up to\n1.27x.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01366v2",
    "published_date": "2024-09-02 16:41:44 UTC",
    "updated_date": "2024-12-27 17:49:34 UTC"
  },
  {
    "arxiv_id": "2409.01362v1",
    "title": "Correlating Time Series with Interpretable Convolutional Kernels",
    "authors": [
      "Xinyu Chen",
      "HanQin Cai",
      "Fuqiang Liu",
      "Jinhua Zhao"
    ],
    "abstract": "This study addresses the problem of convolutional kernel learning in\nunivariate, multivariate, and multidimensional time series data, which is\ncrucial for interpreting temporal patterns in time series and supporting\ndownstream machine learning tasks. First, we propose formulating convolutional\nkernel learning for univariate time series as a sparse regression problem with\na non-negative constraint, leveraging the properties of circular convolution\nand circulant matrices. Second, to generalize this approach to multivariate and\nmultidimensional time series data, we use tensor computations, reformulating\nthe convolutional kernel learning problem in the form of tensors. This is\nfurther converted into a standard sparse regression problem through\nvectorization and tensor unfolding operations. In the proposed methodology, the\noptimization problem is addressed using the existing non-negative subspace\npursuit method, enabling the convolutional kernel to capture temporal\ncorrelations and patterns. To evaluate the proposed model, we apply it to\nseveral real-world time series datasets. On the multidimensional rideshare and\ntaxi trip data from New York City and Chicago, the convolutional kernels reveal\ninterpretable local correlations and cyclical patterns, such as weekly\nseasonality. In the context of multidimensional fluid flow data, both local and\nnonlocal correlations captured by the convolutional kernels can reinforce\ntensor factorization, leading to performance improvements in fluid flow\nreconstruction tasks. Thus, this study lays an insightful foundation for\nautomatically learning convolutional kernels from time series data, with an\nemphasis on interpretability through sparsity and non-negativity constraints.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.01362v1",
    "published_date": "2024-09-02 16:29:21 UTC",
    "updated_date": "2024-09-02 16:29:21 UTC"
  },
  {
    "arxiv_id": "2409.01354v3",
    "title": "Explanation Space: A New Perspective into Time Series Interpretability",
    "authors": [
      "Shahbaz Rezaei",
      "Xin Liu"
    ],
    "abstract": "Human understandable explanation of deep learning models is essential for\nvarious critical and sensitive applications. Unlike image or tabular data where\nthe importance of each input feature (for the classifier's decision) can be\ndirectly projected into the input, time series distinguishable features (e.g.\ndominant frequency) are often hard to manifest in time domain for a user to\neasily understand. Additionally, most explanation methods require a baseline\nvalue as an indication of the absence of any feature. However, the notion of\nlack of feature, which is often defined as black pixels for vision tasks or\nzero/mean values for tabular data, is not well-defined in time series. Despite\nthe adoption of explainable AI methods (XAI) from tabular and vision domain\ninto time series domain, these differences limit the application of these XAI\nmethods in practice. In this paper, we propose a simple yet effective method\nthat allows a model originally trained on the time domain to be interpreted in\nother explanation spaces using existing methods. We suggest five explanation\nspaces, each of which can potentially alleviate these issues in certain types\nof time series. Our method can be easily integrated into existing platforms\nwithout any changes to trained models or XAI methods. The code will be released\nupon acceptance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01354v3",
    "published_date": "2024-09-02 16:15:26 UTC",
    "updated_date": "2025-04-03 19:21:19 UTC"
  },
  {
    "arxiv_id": "2409.01345v3",
    "title": "Language Models Benefit from Preparation with Elicited Knowledge",
    "authors": [
      "Jiacan Yu",
      "Hannah An",
      "Lenhart K. Schubert"
    ],
    "abstract": "The zero-shot chain of thought (CoT) approach is often used in question\nanswering (QA) by language models (LMs) for tasks that require multiple\nreasoning steps. However, some QA tasks hinge more on accessing relevant\nknowledge than on chaining reasoning steps. We introduce a simple prompting\ntechnique, called PREP, that involves using two instances of LMs: the first\n(LM1) generates relevant information, and the second (LM2) receives the\ninformation from the user and answers the question. This design is intended to\nmake better use of the LM's instruction-following capability. PREP is\napplicable across various QA tasks without domain-specific prompt engineering.\nPREP is developed on a dataset of 100 QA questions, derived from an extensive\nschematic dataset specifying artifact parts and material composition. These\nquestions ask which of two artifacts is less likely to share materials with\nanother artifact. Such questions probe the LM's knowledge of shared materials\nin the part structure of different artifacts. We test our method on our\nparts-and-materials dataset and three published commonsense reasoning datasets.\nThe average accuracy of our method is consistently higher than that of all the\nother tested methods across all the tested datasets.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01345v3",
    "published_date": "2024-09-02 15:58:27 UTC",
    "updated_date": "2024-12-02 03:10:37 UTC"
  },
  {
    "arxiv_id": "2409.01344v2",
    "title": "Pairing Analogy-Augmented Generation with Procedural Memory for Procedural Q&A",
    "authors": [
      "K Roth",
      "Rushil Gupta",
      "Simon Halle",
      "Bang Liu"
    ],
    "abstract": "Large language models struggle to synthesize disparate pieces of information\ninto a coherent plan when approaching a complex procedural task. In this work,\nwe introduce a novel formalism and structure for such procedural knowledge.\nBased on this formalism, we present a novel procedural knowledge dataset called\nLCStep, which we created from LangChain tutorials. To leverage this procedural\nknowledge to solve new tasks, we propose analogy-augmented generation (AAG),\nwhich draws inspiration from the human ability to assimilate past experiences\nto solve unfamiliar problems. AAG uses a custom procedure memory store to\nretrieve and adapt specialized domain knowledge to answer new procedural tasks.\nWe demonstrate that AAG outperforms few-shot and RAG baselines on LCStep,\nRecipeNLG, and CHAMP datasets under a pairwise LLM-based evaluation,\ncorroborated by human evaluation in the case of RecipeNLG.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01344v2",
    "published_date": "2024-09-02 15:58:24 UTC",
    "updated_date": "2024-10-21 19:49:41 UTC"
  },
  {
    "arxiv_id": "2409.02138v1",
    "title": "A Financial Time Series Denoiser Based on Diffusion Model",
    "authors": [
      "Zhuohan Wang",
      "Carmine Ventre"
    ],
    "abstract": "Financial time series often exhibit low signal-to-noise ratio, posing\nsignificant challenges for accurate data interpretation and prediction and\nultimately decision making. Generative models have gained attention as powerful\ntools for simulating and predicting intricate data patterns, with the diffusion\nmodel emerging as a particularly effective method. This paper introduces a\nnovel approach utilizing the diffusion model as a denoiser for financial time\nseries in order to improve data predictability and trading performance. By\nleveraging the forward and reverse processes of the conditional diffusion model\nto add and remove noise progressively, we reconstruct original data from noisy\ninputs. Our extensive experiments demonstrate that diffusion model-based\ndenoised time series significantly enhance the performance on downstream future\nreturn classification tasks. Moreover, trading signals derived from the\ndenoised data yield more profitable trades with fewer transactions, thereby\nminimizing transaction costs and increasing overall trading efficiency.\nFinally, we show that by using classifiers trained on denoised time series, we\ncan recognize the noising state of the market and obtain excess return.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-fin.CP",
      "q-fin.TR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.02138v1",
    "published_date": "2024-09-02 15:55:36 UTC",
    "updated_date": "2024-09-02 15:55:36 UTC"
  },
  {
    "arxiv_id": "2409.01330v1",
    "title": "Pediatric brain tumor classification using digital histopathology and deep learning: evaluation of SOTA methods on a multi-center Swedish cohort",
    "authors": [
      "Iulian Emil Tampu",
      "Per Nyman",
      "Christoforos Spyretos",
      "Ida Blystad",
      "Alia Shamikh",
      "Gabriela Prochazka",
      "Teresita Díaz de Ståhl",
      "Johanna Sandgren",
      "Peter Lundberg",
      "Neda Haj-Hosseini"
    ],
    "abstract": "Brain tumors are the most common solid tumors in children and young adults,\nbut the scarcity of large histopathology datasets has limited the application\nof computational pathology in this group. This study implements two weakly\nsupervised multiple-instance learning (MIL) approaches on patch-features\nobtained from state-of-the-art histology-specific foundation models to classify\npediatric brain tumors in hematoxylin and eosin whole slide images (WSIs) from\na multi-center Swedish cohort. WSIs from 540 subjects (age 8.5$\\pm$4.9 years)\ndiagnosed with brain tumor were gathered from the six Swedish university\nhospitals. Instance (patch)-level features were obtained from WSIs using three\npre-trained feature extractors: ResNet50, UNI and CONCH. Instances were\naggregated using attention-based MIL (ABMIL) or clustering-constrained\nattention MIL (CLAM) for patient-level classification. Models were evaluated on\nthree classification tasks based on the hierarchical classification of\npediatric brain tumors: tumor category, family and type. Model generalization\nwas assessed by training on data from two of the centers and testing on data\nfrom four other centers. Model interpretability was evaluated through\nattention-mapping. The highest classification performance was achieved using\nUNI features and AMBIL aggregation, with Matthew's correlation coefficient of\n0.86$\\pm$0.04, 0.63$\\pm$0.04, and 0.53$\\pm$0.05, for tumor category, family and\ntype classification, respectively. When evaluating generalization, models\nutilizing UNI and CONCH features outperformed those using ResNet50. However,\nthe drop in performance from the in-site to out-of-site testing was similar\nacross feature extractors. These results show the potential of state-of-the-art\ncomputational pathology methods in diagnosing pediatric brain tumors at\ndifferent hierarchical levels with fair generalizability on a multi-center\nnational dataset.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01330v1",
    "published_date": "2024-09-02 15:32:04 UTC",
    "updated_date": "2024-09-02 15:32:04 UTC"
  },
  {
    "arxiv_id": "2409.01326v1",
    "title": "Grounding Language Models in Autonomous Loco-manipulation Tasks",
    "authors": [
      "Jin Wang",
      "Nikos Tsagarakis"
    ],
    "abstract": "Humanoid robots with behavioral autonomy have consistently been regarded as\nideal collaborators in our daily lives and promising representations of\nembodied intelligence. Compared to fixed-based robotic arms, humanoid robots\noffer a larger operational space while significantly increasing the difficulty\nof control and planning. Despite the rapid progress towards general-purpose\nhumanoid robots, most studies remain focused on locomotion ability with few\ninvestigations into whole-body coordination and tasks planning, thus limiting\nthe potential to demonstrate long-horizon tasks involving both mobility and\nmanipulation under open-ended verbal instructions. In this work, we propose a\nnovel framework that learns, selects, and plans behaviors based on tasks in\ndifferent scenarios. We combine reinforcement learning (RL) with whole-body\noptimization to generate robot motions and store them into a motion library. We\nfurther leverage the planning and reasoning features of the large language\nmodel (LLM), constructing a hierarchical task graph that comprises a series of\nmotion primitives to bridge lower-level execution with higher-level planning.\nExperiments in simulation and real-world using the CENTAURO robot show that the\nlanguage model based planner can efficiently adapt to new loco-manipulation\ntasks, demonstrating high autonomy from free-text commands in unstructured\nscenes.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Summit to ICRA@40. arXiv admin note: substantial text overlap with\n  arXiv:2406.14655",
    "pdf_url": "http://arxiv.org/pdf/2409.01326v1",
    "published_date": "2024-09-02 15:27:48 UTC",
    "updated_date": "2024-09-02 15:27:48 UTC"
  },
  {
    "arxiv_id": "2409.12968v1",
    "title": "MITHOS: Interactive Mixed Reality Training to Support Professional Socio-Emotional Interactions at Schools",
    "authors": [
      "Lara Chehayeb",
      "Chirag Bhuvaneshwara",
      "Manuel Anglet",
      "Bernhard Hilpert",
      "Ann-Kristin Meyer",
      "Dimitra Tsovaltzi",
      "Patrick Gebhard",
      "Antje Biermann",
      "Sinah Auchtor",
      "Nils Lauinger",
      "Julia Knopf",
      "Andreas Kaiser",
      "Fabian Kersting",
      "Gregor Mehlmann",
      "Florian Lingenfelser",
      "Elisabeth André"
    ],
    "abstract": "Teachers in challenging conflict situations often experience shame and\nself-blame, which relate to the feeling of incompetence but may externalise as\nanger. Sensing mixed signals fails the contingency rule for developing affect\nregulation and may result in confusion for students about their own emotions\nand hinder their emotion regulation. Therefore, being able to constructively\nregulate emotions not only benefits individual experience of emotions but also\nfosters effective interpersonal emotion regulation and influences how a\nsituation is managed. MITHOS is a system aimed at training teachers' conflict\nresolution skills through realistic situative learning opportunities during\nclassroom conflicts. In four stages, MITHOS supports teachers' socio-emotional\nself-awareness, perspective-taking and positive regard. It provides: a) a safe\nvirtual environment to train free social interaction and receive natural social\nfeedback from reciprocal student-agent reactions, b) spatial situational\nperspective taking through an avatar, c) individual virtual reflection guidance\non emotional experiences through co-regulation processes, and d) expert\nfeedback on professional behavioural strategies. This chapter presents the four\nstages and their implementation in a semi-automatic Wizard-of-Oz (WoZ) System.\nThe WoZ system affords collecting data that are used for developing the fully\nautomated hybrid (machine learning and model-based) system, and to validate the\nunderlying psychological and conflict resolution models. We present results\nvalidating the approach in terms of scenario realism, as well as a systematic\ntesting of the effects of external avatar similarity on antecedents of\nself-awareness with behavior similarity. The chapter contributes to a common\nmethodology of conducting interdisciplinary research for human-centered and\ngeneralisable XR and presents a system designed to support it.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12968v1",
    "published_date": "2024-09-02 15:24:33 UTC",
    "updated_date": "2024-09-02 15:24:33 UTC"
  },
  {
    "arxiv_id": "2409.01315v1",
    "title": "Multi-frequency Neural Born Iterative Method for Solving 2-D Inverse Scattering Problems",
    "authors": [
      "Daoqi Liu",
      "Tao Shan",
      "Maokun Li",
      "Fan Yang",
      "Shenheng Xu"
    ],
    "abstract": "In this work, we propose a deep learning-based imaging method for addressing\nthe multi-frequency electromagnetic (EM) inverse scattering problem (ISP). By\ncombining deep learning technology with EM physical laws, we have successfully\ndeveloped a multi-frequency neural Born iterative method (NeuralBIM), guided by\nthe principles of the single-frequency NeuralBIM. This method integrates\nmultitask learning techniques with NeuralBIM's efficient iterative inversion\nprocess to construct a robust multi-frequency Born iterative inversion model.\nDuring training, the model employs a multitask learning approach guided by\nhomoscedastic uncertainty to adaptively allocate the weights of each\nfrequency's data. Additionally, an unsupervised learning method, constrained by\nthe physical laws of ISP, is used to train the multi-frequency NeuralBIM model,\neliminating the need for contrast and total field data. The effectiveness of\nthe multi-frequency NeuralBIM is validated through synthetic and experimental\ndata, demonstrating improvements in accuracy and computational efficiency for\nsolving ISP. Moreover, this method exhibits strong generalization capabilities\nand noise resistance. The multi-frequency NeuralBIM method explores a novel\ninversion method for multi-frequency EM data and provides an effective solution\nfor the electromagnetic ISP of multi-frequency data.",
    "categories": [
      "physics.comp-ph",
      "cs.AI",
      "cs.LG",
      "35Q61",
      "I.2.6; G.1.8; G.1.3"
    ],
    "primary_category": "physics.comp-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01315v1",
    "published_date": "2024-09-02 15:16:07 UTC",
    "updated_date": "2024-09-02 15:16:07 UTC"
  },
  {
    "arxiv_id": "2409.01303v1",
    "title": "Topological degree as a discrete diagnostic for disentanglement, with applications to the $Δ$VAE",
    "authors": [
      "Mahefa Ratsisetraina Ravelonanosy",
      "Vlado Menkovski",
      "Jacobus W. Portegies"
    ],
    "abstract": "We investigate the ability of Diffusion Variational Autoencoder ($\\Delta$VAE)\nwith unit sphere $\\mathcal{S}^2$ as latent space to capture topological and\ngeometrical structure and disentangle latent factors in datasets. For this, we\nintroduce a new diagnostic of disentanglement: namely the topological degree of\nthe encoder, which is a map from the data manifold to the latent space. By\nusing tools from homology theory, we derive and implement an algorithm that\ncomputes this degree. We use the algorithm to compute the degree of the encoder\nof models that result from the training procedure. Our experimental results\nshow that the $\\Delta$VAE achieves relatively small LSBD scores, and that\nregardless of the degree after initialization, the degree of the encoder after\ntraining becomes $-1$ or $+1$, which implies that the resulting encoder is at\nleast homotopic to a homeomorphism.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.AT",
      "51H20 55N35 68T09 68T07"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01303v1",
    "published_date": "2024-09-02 14:51:31 UTC",
    "updated_date": "2024-09-02 14:51:31 UTC"
  },
  {
    "arxiv_id": "2409.02136v1",
    "title": "Large Language Models versus Classical Machine Learning: Performance in COVID-19 Mortality Prediction Using High-Dimensional Tabular Data",
    "authors": [
      "Mohammadreza Ghaffarzadeh-Esfahani",
      "Mahdi Ghaffarzadeh-Esfahani",
      "Arian Salahi-Niri",
      "Hossein Toreyhi",
      "Zahra Atf",
      "Amirali Mohsenzadeh-Kermani",
      "Mahshad Sarikhani",
      "Zohreh Tajabadi",
      "Fatemeh Shojaeian",
      "Mohammad Hassan Bagheri",
      "Aydin Feyzi",
      "Mohammadamin Tarighatpayma",
      "Narges Gazmeh",
      "Fateme Heydari",
      "Hossein Afshar",
      "Amirreza Allahgholipour",
      "Farid Alimardani",
      "Ameneh Salehi",
      "Naghmeh Asadimanesh",
      "Mohammad Amin Khalafi",
      "Hadis Shabanipour",
      "Ali Moradi",
      "Sajjad Hossein Zadeh",
      "Omid Yazdani",
      "Romina Esbati",
      "Moozhan Maleki",
      "Danial Samiei Nasr",
      "Amirali Soheili",
      "Hossein Majlesi",
      "Saba Shahsavan",
      "Alireza Soheilipour",
      "Nooshin Goudarzi",
      "Erfan Taherifard",
      "Hamidreza Hatamabadi",
      "Jamil S Samaan",
      "Thomas Savage",
      "Ankit Sakhuja",
      "Ali Soroush",
      "Girish Nadkarni",
      "Ilad Alavi Darazam",
      "Mohamad Amin Pourhoseingholi",
      "Seyed Amir Ahmad Safavi-Naini"
    ],
    "abstract": "Background: This study aimed to evaluate and compare the performance of\nclassical machine learning models (CMLs) and large language models (LLMs) in\npredicting mortality associated with COVID-19 by utilizing a high-dimensional\ntabular dataset.\n  Materials and Methods: We analyzed data from 9,134 COVID-19 patients\ncollected across four hospitals. Seven CML models, including XGBoost and random\nforest (RF), were trained and evaluated. The structured data was converted into\ntext for zero-shot classification by eight LLMs, including GPT-4 and\nMistral-7b. Additionally, Mistral-7b was fine-tuned using the QLoRA approach to\nenhance its predictive capabilities.\n  Results: Among the CML models, XGBoost and RF achieved the highest accuracy,\nwith F1 scores of 0.87 for internal validation and 0.83 for external\nvalidation. In the LLM category, GPT-4 was the top performer with an F1 score\nof 0.43. Fine-tuning Mistral-7b significantly improved its recall from 1% to\n79%, resulting in an F1 score of 0.74, which was stable during external\nvalidation.\n  Conclusion: While LLMs show moderate performance in zero-shot classification,\nfine-tuning can significantly enhance their effectiveness, potentially aligning\nthem closer to CML models. However, CMLs still outperform LLMs in\nhigh-dimensional tabular data tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "92C50, 68T50",
      "J.3"
    ],
    "primary_category": "cs.LG",
    "comment": "Code is available at:\n  https://github.com/mohammad-gh009/Large-Language-Models-vs-Classical-Machine-learning\n  and https://github.com/Sdamirsa/Tehran_COVID_Cohort. The datasets are\n  available from the corresponding author on reasonable request\n  (sdamirsa@ymail.com)",
    "pdf_url": "http://arxiv.org/pdf/2409.02136v1",
    "published_date": "2024-09-02 14:51:12 UTC",
    "updated_date": "2024-09-02 14:51:12 UTC"
  },
  {
    "arxiv_id": "2409.10554v1",
    "title": "An Examination of Offline-Trained Encoders in Vision-Based Deep Reinforcement Learning for Autonomous Driving",
    "authors": [
      "Shawan Mohammed",
      "Alp Argun",
      "Nicolas Bonnotte",
      "Gerd Ascheid"
    ],
    "abstract": "Our research investigates the challenges Deep Reinforcement Learning (DRL)\nfaces in complex, Partially Observable Markov Decision Processes (POMDP) such\nas autonomous driving (AD), and proposes a solution for vision-based navigation\nin these environments. Partial observability reduces RL performance\nsignificantly, and this can be mitigated by augmenting sensor information and\ndata fusion to reflect a more Markovian environment. However, this necessitates\nan increasingly complex perception module, whose training via RL is complicated\ndue to inherent limitations. As the neural network architecture becomes more\ncomplex, the reward function's effectiveness as an error signal diminishes\nsince the only source of supervision is the reward, which is often noisy,\nsparse, and delayed. Task-irrelevant elements in images, such as the sky or\ncertain objects, pose additional complexities. Our research adopts an\noffline-trained encoder to leverage large video datasets through\nself-supervised learning to learn generalizable representations. Then, we train\na head network on top of these representations through DRL to learn to control\nan ego vehicle in the CARLA AD simulator. This study presents a broad\ninvestigation of the impact of different learning schemes for offline-training\nof encoders on the performance of DRL agents in challenging AD tasks.\nFurthermore, we show that the features learned by watching BDD100K driving\nvideos can be directly transferred to achieve lane following and collision\navoidance in CARLA simulator, in a zero-shot learning fashion. Finally, we\nexplore the impact of various architectural decisions for the RL networks to\nutilize the transferred representations efficiently. Therefore, in this work,\nwe introduce and validate an optimal way for obtaining suitable representations\nof the environment, and transferring them to RL networks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10554v1",
    "published_date": "2024-09-02 14:16:23 UTC",
    "updated_date": "2024-09-02 14:16:23 UTC"
  },
  {
    "arxiv_id": "2409.02720v2",
    "title": "GET-UP: GEomeTric-aware Depth Estimation with Radar Points UPsampling",
    "authors": [
      "Huawei Sun",
      "Zixu Wang",
      "Hao Feng",
      "Julius Ott",
      "Lorenzo Servadei",
      "Robert Wille"
    ],
    "abstract": "Depth estimation plays a pivotal role in autonomous driving, facilitating a\ncomprehensive understanding of the vehicle's 3D surroundings. Radar, with its\nrobustness to adverse weather conditions and capability to measure distances,\nhas drawn significant interest for radar-camera depth estimation. However,\nexisting algorithms process the inherently noisy and sparse radar data by\nprojecting 3D points onto the image plane for pixel-level feature extraction,\noverlooking the valuable geometric information contained within the radar point\ncloud. To address this gap, we propose GET-UP, leveraging attention-enhanced\nGraph Neural Networks (GNN) to exchange and aggregate both 2D and 3D\ninformation from radar data. This approach effectively enriches the feature\nrepresentation by incorporating spatial relationships compared to traditional\nmethods that rely only on 2D feature extraction. Furthermore, we incorporate a\npoint cloud upsampling task to densify the radar point cloud, rectify point\npositions, and derive additional 3D features under the guidance of lidar data.\nFinally, we fuse radar and camera features during the decoding phase for depth\nestimation. We benchmark our proposed GET-UP on the nuScenes dataset, achieving\nstate-of-the-art performance with a 15.3% and 14.7% improvement in MAE and RMSE\nover the previously best-performing model. Code:\nhttps://github.com/harborsarah/GET-UP",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by WACV 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.02720v2",
    "published_date": "2024-09-02 14:15:09 UTC",
    "updated_date": "2024-09-08 18:43:06 UTC"
  },
  {
    "arxiv_id": "2409.01256v1",
    "title": "Real-time Accident Anticipation for Autonomous Driving Through Monocular Depth-Enhanced 3D Modeling",
    "authors": [
      "Haicheng Liao",
      "Yongkang Li",
      "Chengyue Wang",
      "Songning Lai",
      "Zhenning Li",
      "Zilin Bian",
      "Jaeyoung Lee",
      "Zhiyong Cui",
      "Guohui Zhang",
      "Chengzhong Xu"
    ],
    "abstract": "The primary goal of traffic accident anticipation is to foresee potential\naccidents in real time using dashcam videos, a task that is pivotal for\nenhancing the safety and reliability of autonomous driving technologies. In\nthis study, we introduce an innovative framework, AccNet, which significantly\nadvances the prediction capabilities beyond the current state-of-the-art (SOTA)\n2D-based methods by incorporating monocular depth cues for sophisticated 3D\nscene modeling. Addressing the prevalent challenge of skewed data distribution\nin traffic accident datasets, we propose the Binary Adaptive Loss for Early\nAnticipation (BA-LEA). This novel loss function, together with a multi-task\nlearning strategy, shifts the focus of the predictive model towards the\ncritical moments preceding an accident. {We rigorously evaluate the performance\nof our framework on three benchmark datasets--Dashcam Accident Dataset (DAD),\nCar Crash Dataset (CCD), and AnAn Accident Detection (A3D), and DADA-2000\nDataset--demonstrating its superior predictive accuracy through key metrics\nsuch as Average Precision (AP) and mean Time-To-Accident (mTTA).",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01256v1",
    "published_date": "2024-09-02 13:46:25 UTC",
    "updated_date": "2024-09-02 13:46:25 UTC"
  },
  {
    "arxiv_id": "2409.01247v3",
    "title": "Conversational Complexity for Assessing Risk in Large Language Models",
    "authors": [
      "John Burden",
      "Manuel Cebrian",
      "Jose Hernandez-Orallo"
    ],
    "abstract": "Large Language Models (LLMs) present a dual-use dilemma: they enable\nbeneficial applications while harboring potential for harm, particularly\nthrough conversational interactions. Despite various safeguards, advanced LLMs\nremain vulnerable. A watershed case in early 2023 involved journalist Kevin\nRoose's extended dialogue with Bing, an LLM-powered search engine, which\nrevealed harmful outputs after probing questions, highlighting vulnerabilities\nin the model's safeguards. This contrasts with simpler early jailbreaks, like\nthe \"Grandma Jailbreak,\" where users framed requests as innocent help for a\ngrandmother, easily eliciting similar content. This raises the question: How\nmuch conversational effort is needed to elicit harmful information from LLMs?\nWe propose two measures to quantify this effort: Conversational Length (CL),\nwhich measures the number of conversational turns needed to obtain a specific\nharmful response, and Conversational Complexity (CC), defined as the Kolmogorov\ncomplexity of the user's instruction sequence leading to the harmful response.\nTo address the incomputability of Kolmogorov complexity, we approximate CC\nusing a reference LLM to estimate the compressibility of the user instructions.\nApplying this approach to a large red-teaming dataset, we perform a\nquantitative analysis examining the statistical distribution of harmful and\nharmless conversational lengths and complexities. Our empirical findings\nsuggest that this distributional analysis and the minimization of CC serve as\nvaluable tools for understanding AI safety, offering insights into the\naccessibility of harmful information. This work establishes a foundation for a\nnew perspective on LLM safety, centered around the algorithmic complexity of\npathways to harm.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.AI",
    "comment": "15 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.01247v3",
    "published_date": "2024-09-02 13:29:44 UTC",
    "updated_date": "2024-11-29 01:47:20 UTC"
  },
  {
    "arxiv_id": "2409.01245v1",
    "title": "Revisiting Safe Exploration in Safe Reinforcement learning",
    "authors": [
      "David Eckel",
      "Baohe Zhang",
      "Joschka Bödecker"
    ],
    "abstract": "Safe reinforcement learning (SafeRL) extends standard reinforcement learning\nwith the idea of safety, where safety is typically defined through the\nconstraint of the expected cost return of a trajectory being below a set limit.\nHowever, this metric fails to distinguish how costs accrue, treating infrequent\nsevere cost events as equal to frequent mild ones, which can lead to riskier\nbehaviors and result in unsafe exploration. We introduce a new metric, expected\nmaximum consecutive cost steps (EMCC), which addresses safety during training\nby assessing the severity of unsafe steps based on their consecutive\noccurrence. This metric is particularly effective for distinguishing between\nprolonged and occasional safety violations. We apply EMMC in both on- and\noff-policy algorithm for benchmarking their safe exploration capability.\nFinally, we validate our metric through a set of benchmarks and propose a new\nlightweight benchmark task, which allows fast evaluation for algorithm design.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01245v1",
    "published_date": "2024-09-02 13:29:29 UTC",
    "updated_date": "2024-09-02 13:29:29 UTC"
  },
  {
    "arxiv_id": "2409.01241v3",
    "title": "CyberCortex.AI: An AI-based Operating System for Autonomous Robotics and Complex Automation",
    "authors": [
      "Sorin Grigorescu",
      "Mihai Zaha"
    ],
    "abstract": "The underlying framework for controlling autonomous robots and complex\nautomation applications are Operating Systems (OS) capable of scheduling\nperception-and-control tasks, as well as providing real-time data communication\nto other robotic peers and remote cloud computers. In this paper, we introduce\nCyberCortex AI, a robotics OS designed to enable heterogeneous AI-based\nrobotics and complex automation applications. CyberCortex AI is a decentralized\ndistributed OS which enables robots to talk to each other, as well as to High\nPerformance Computers (HPC) in the cloud. Sensory and control data from the\nrobots is streamed towards HPC systems with the purpose of training AI\nalgorithms, which are afterwards deployed on the robots. Each functionality of\na robot (e.g. sensory data acquisition, path planning, motion control, etc.) is\nexecuted within a so-called DataBlock of Filters shared through the internet,\nwhere each filter is computed either locally on the robot itself, or remotely\non a different robotic system. The data is stored and accessed via a so-called\nTemporal Addressable Memory (TAM), which acts as a gateway between each\nfilter's input and output. CyberCortex AI has two main components: i) the\nCyberCortex AI inference system, which is a real-time implementation of the\nDataBlock running on the robots' embedded hardware, and ii) the CyberCortex AI\ndojo, which runs on an HPC computer in the cloud, and it is used to design,\ntrain and deploy AI algorithms. We present a quantitative and qualitative\nperformance analysis of the proposed approach using two collaborative robotics\napplications: i) a forest fires prevention system based on an Unitree A1 legged\nrobot and an Anafi Parrot 4K drone, as well as ii) an autonomous driving system\nwhich uses CyberCortex AI for collaborative perception and motion control.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.OS"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01241v3",
    "published_date": "2024-09-02 13:14:50 UTC",
    "updated_date": "2024-10-04 11:32:08 UTC"
  },
  {
    "arxiv_id": "2409.10552v1",
    "title": "AI Literacy for All: Adjustable Interdisciplinary Socio-technical Curriculum",
    "authors": [
      "Sri Yash Tadimalla",
      "Mary Lou Maher"
    ],
    "abstract": "This paper presents a curriculum, \"AI Literacy for All,\" to promote an\ninterdisciplinary understanding of AI, its socio-technical implications, and\nits practical applications for all levels of education. With the rapid\nevolution of artificial intelligence (AI), there is a need for AI literacy that\ngoes beyond the traditional AI education curriculum. AI literacy has been\nconceptualized in various ways, including public literacy, competency building\nfor designers, conceptual understanding of AI concepts, and domain-specific\nupskilling. Most of these conceptualizations were established before the public\nrelease of Generative AI (Gen-AI) tools like ChatGPT. AI education has focused\non the principles and applications of AI through a technical lens that\nemphasizes the mastery of AI principles, the mathematical foundations\nunderlying these technologies, and the programming and mathematical skills\nnecessary to implement AI solutions. In AI Literacy for All, we emphasize a\nbalanced curriculum that includes technical and non-technical learning outcomes\nto enable a conceptual understanding and critical evaluation of AI technologies\nin an interdisciplinary socio-technical context. The paper presents four\npillars of AI literacy: understanding the scope and technical dimensions of AI,\nlearning how to interact with Gen-AI in an informed and responsible way, the\nsocio-technical issues of ethical and responsible AI, and the social and future\nimplications of AI. While it is important to include all learning outcomes for\nAI education in a Computer Science major, the learning outcomes can be adjusted\nfor other learning contexts, including, non-CS majors, high school summer\ncamps, the adult workforce, and the public. This paper advocates for a shift in\nAI literacy education to offer a more interdisciplinary socio-technical\napproach as a pathway to broaden participation in AI.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "Published at 2024 IEEE Frontiers in Education Conference",
    "pdf_url": "http://arxiv.org/pdf/2409.10552v1",
    "published_date": "2024-09-02 13:13:53 UTC",
    "updated_date": "2024-09-02 13:13:53 UTC"
  },
  {
    "arxiv_id": "2409.01216v1",
    "title": "ESP-PCT: Enhanced VR Semantic Performance through Efficient Compression of Temporal and Spatial Redundancies in Point Cloud Transformers",
    "authors": [
      "Luoyu Mei",
      "Shuai Wang",
      "Yun Cheng",
      "Ruofeng Liu",
      "Zhimeng Yin",
      "Wenchao Jiang",
      "Shuai Wang",
      "Wei Gong"
    ],
    "abstract": "Semantic recognition is pivotal in virtual reality (VR) applications,\nenabling immersive and interactive experiences. A promising approach is\nutilizing millimeter-wave (mmWave) signals to generate point clouds. However,\nthe high computational and memory demands of current mmWave point cloud models\nhinder their efficiency and reliability. To address this limitation, our paper\nintroduces ESP-PCT, a novel Enhanced Semantic Performance Point Cloud\nTransformer with a two-stage semantic recognition framework tailored for VR\napplications. ESP-PCT takes advantage of the accuracy of sensory point cloud\ndata and optimizes the semantic recognition process, where the localization and\nfocus stages are trained jointly in an end-to-end manner. We evaluate ESP-PCT\non various VR semantic recognition conditions, demonstrating substantial\nenhancements in recognition efficiency. Notably, ESP-PCT achieves a remarkable\naccuracy of 93.2% while reducing the computational requirements (FLOPs) by\n76.9% and memory usage by 78.2% compared to the existing Point Transformer\nmodel simultaneously. These underscore ESP-PCT's potential in VR semantic\nrecognition by achieving high accuracy and reducing redundancy. The code and\ndata of this project are available at\n\\url{https://github.com/lymei-SEU/ESP-PCT}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01216v1",
    "published_date": "2024-09-02 12:48:40 UTC",
    "updated_date": "2024-09-02 12:48:40 UTC"
  },
  {
    "arxiv_id": "2409.01201v1",
    "title": "EnCLAP++: Analyzing the EnCLAP Framework for Optimizing Automated Audio Captioning Performance",
    "authors": [
      "Jaeyeon Kim",
      "Minjeon Jeon",
      "Jaeyoon Jung",
      "Sang Hoon Woo",
      "Jinjoo Lee"
    ],
    "abstract": "In this work, we aim to analyze and optimize the EnCLAP framework, a\nstate-of-the-art model in automated audio captioning. We investigate the impact\nof modifying the acoustic encoder components, explore pretraining with\ndifferent dataset scales, and study the effectiveness of a reranking scheme.\nThrough extensive experimentation and quantitative analysis of generated\ncaptions, we develop EnCLAP++, an enhanced version that significantly surpasses\nthe original.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted to DCASE2024 Workshop",
    "pdf_url": "http://arxiv.org/pdf/2409.01201v1",
    "published_date": "2024-09-02 12:23:18 UTC",
    "updated_date": "2024-09-02 12:23:18 UTC"
  },
  {
    "arxiv_id": "2409.13693v1",
    "title": "Declarative Integration and Management of Large Language Models through Finite Automata: Application to Automation, Communication, and Ethics",
    "authors": [
      "Thierry Petit",
      "Arnault Pachot",
      "Claire Conan-Vrinat",
      "Alexandre Dubarry"
    ],
    "abstract": "This article introduces an innovative architecture designed to declaratively\ncombine Large Language Models (LLMs) with shared histories, and triggers to\nidentify the most appropriate LLM for a given task. Our approach is general and\ndeclarative, relying on the construction of finite automata coupled with an\nevent management system. The developed tool is crafted to facilitate the\nefficient and complex integration of LLMs with minimal programming effort,\nespecially, but not only, for integrating methods of positive psychology to AI.\nThe flexibility of our technique is demonstrated through applied examples in\nautomation, communication, and ethics.",
    "categories": [
      "cs.FL",
      "cs.AI",
      "cs.CL",
      "cs.ET",
      "cs.HC",
      "68T20",
      "I.2.7; H.5.2; K.4.1; D.3.2"
    ],
    "primary_category": "cs.FL",
    "comment": "Submitted to IAAI-2025, Philadelphia, PA",
    "pdf_url": "http://arxiv.org/pdf/2409.13693v1",
    "published_date": "2024-09-02 11:50:52 UTC",
    "updated_date": "2024-09-02 11:50:52 UTC"
  },
  {
    "arxiv_id": "2409.02134v1",
    "title": "Edge AI: Evaluation of Model Compression Techniques for Convolutional Neural Networks",
    "authors": [
      "Samer Francy",
      "Raghubir Singh"
    ],
    "abstract": "This work evaluates the compression techniques on ConvNeXt models in image\nclassification tasks using the CIFAR-10 dataset. Structured pruning,\nunstructured pruning, and dynamic quantization methods are evaluated to reduce\nmodel size and computational complexity while maintaining accuracy. The\nexperiments, conducted on cloud-based platforms and edge device, assess the\nperformance of these techniques. Results show significant reductions in model\nsize, with up to 75% reduction achieved using structured pruning techniques.\nAdditionally, dynamic quantization achieves a reduction of up to 95% in the\nnumber of parameters. Fine-tuned models exhibit improved compression\nperformance, indicating the benefits of pre-training in conjunction with\ncompression techniques. Unstructured pruning methods reveal trends in accuracy\nand compression, with limited reductions in computational complexity. The\ncombination of OTOV3 pruning and dynamic quantization further enhances\ncompression performance, resulting 89.7% reduction in size, 95% reduction with\nnumber of parameters and MACs, and 3.8% increase with accuracy. The deployment\nof the final compressed model on edge device demonstrates high accuracy 92.5%\nand low inference time 20 ms, validating the effectiveness of compression\ntechniques for real-world edge computing applications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.02134v1",
    "published_date": "2024-09-02 11:48:19 UTC",
    "updated_date": "2024-09-02 11:48:19 UTC"
  },
  {
    "arxiv_id": "2409.01178v1",
    "title": "Integrating End-to-End and Modular Driving Approaches for Online Corner Case Detection in Autonomous Driving",
    "authors": [
      "Gemb Kaljavesi",
      "Xiyan Su",
      "Frank Diermeyer"
    ],
    "abstract": "Online corner case detection is crucial for ensuring safety in autonomous\ndriving vehicles. Current autonomous driving approaches can be categorized into\nmodular approaches and end-to-end approaches. To leverage the advantages of\nboth, we propose a method for online corner case detection that integrates an\nend-to-end approach into a modular system. The modular system takes over the\nprimary driving task and the end-to-end network runs in parallel as a secondary\none, the disagreement between the systems is then used for corner case\ndetection. We implement this method on a real vehicle and evaluate it\nqualitatively. Our results demonstrate that end-to-end networks, known for\ntheir superior situational awareness, as secondary driving systems, can\neffectively contribute to corner case detection. These findings suggest that\nsuch an approach holds potential for enhancing the safety of autonomous\nvehicles.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "IEEE SMC 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.01178v1",
    "published_date": "2024-09-02 11:14:41 UTC",
    "updated_date": "2024-09-02 11:14:41 UTC"
  },
  {
    "arxiv_id": "2409.01175v1",
    "title": "Logit Scaling for Out-of-Distribution Detection",
    "authors": [
      "Andrija Djurisic",
      "Rosanne Liu",
      "Mladen Nikolic"
    ],
    "abstract": "The safe deployment of machine learning and AI models in open-world settings\nhinges critically on the ability to detect out-of-distribution (OOD) data\naccurately, data samples that contrast vastly from what the model was trained\nwith. Current approaches to OOD detection often require further training the\nmodel, and/or statistics about the training data which may no longer be\naccessible. Additionally, many existing OOD detection methods struggle to\nmaintain performance when transferred across different architectures. Our\nresearch tackles these issues by proposing a simple, post-hoc method that does\nnot require access to the training data distribution, keeps a trained network\nintact, and holds strong performance across a variety of architectures. Our\nmethod, Logit Scaling (LTS), as the name suggests, simply scales the logits in\na manner that effectively distinguishes between in-distribution (ID) and OOD\nsamples. We tested our method on benchmarks across various scales, including\nCIFAR-10, CIFAR-100, ImageNet and OpenOOD. The experiments cover 3 ID and 14\nOOD datasets, as well as 9 model architectures. Overall, we demonstrate\nstate-of-the-art performance, robustness and adaptability across different\narchitectures, paving the way towards a universally applicable solution for\nadvanced OOD detection.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01175v1",
    "published_date": "2024-09-02 11:10:44 UTC",
    "updated_date": "2024-09-02 11:10:44 UTC"
  },
  {
    "arxiv_id": "2409.01160v1",
    "title": "Expanding on EnCLAP with Auxiliary Retrieval Model for Automated Audio Captioning",
    "authors": [
      "Jaeyeon Kim",
      "Jaeyoon Jung",
      "Minjeong Jeon",
      "Sang Hoon Woo",
      "Jinjoo Lee"
    ],
    "abstract": "In this technical report, we describe our submission to DCASE2024 Challenge\nTask6 (Automated Audio Captioning) and Task8 (Language-based Audio Retrieval).\nWe develop our approach building upon the EnCLAP audio captioning framework and\noptimizing it for Task6 of the challenge. Notably, we outline the changes in\nthe underlying components and the incorporation of the reranking process.\nAdditionally, we submit a supplementary retriever model, a byproduct of our\nmodified framework, to Task8. Our proposed systems achieve FENSE score of 0.542\non Task6 and mAP@10 score of 0.386 on Task8, significantly outperforming the\nbaseline models.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "DCASE2024 Challenge Technical Report. Ranked 2nd in Task 6 Automated\n  Audio Captioning",
    "pdf_url": "http://arxiv.org/pdf/2409.01160v1",
    "published_date": "2024-09-02 10:47:07 UTC",
    "updated_date": "2024-09-02 10:47:07 UTC"
  },
  {
    "arxiv_id": "2409.01148v3",
    "title": "FMRFT: Fusion Mamba and DETR for Query Time Sequence Intersection Fish Tracking",
    "authors": [
      "Mingyuan Yao",
      "Yukang Huo",
      "Qingbin Tian",
      "Jiayin Zhao",
      "Xiao Liu",
      "Ruifeng Wang",
      "Lin Xue",
      "Haihua Wang"
    ],
    "abstract": "Early detection of abnormal fish behavior caused by disease or hunger can be\nachieved through fish tracking using deep learning techniques, which holds\nsignificant value for industrial aquaculture. However, underwater reflections\nand some reasons with fish, such as the high similarity, rapid swimming caused\nby stimuli and mutual occlusion bring challenges to multi-target tracking of\nfish. To address these challenges, this paper establishes a complex\nmulti-scenario sturgeon tracking dataset and introduces the FMRFT model, a\nreal-time end-to-end fish tracking solution. The model incorporates the low\nvideo memory consumption Mamba In Mamba (MIM) architecture, which facilitates\nmulti-frame temporal memory and feature extraction, thereby addressing the\nchallenges to track multiple fish across frames. Additionally, the FMRFT model\nwith the Query Time Sequence Intersection (QTSI) module effectively manages\noccluded objects and reduces redundant tracking frames using the superior\nfeature interaction and prior frame processing capabilities of RT-DETR. This\ncombination significantly enhances the accuracy and stability of fish tracking.\nTrained and tested on the dataset, the model achieves an IDF1 score of 90.3%\nand a MOTA accuracy of 94.3%. Experimental results show that the proposed FMRFT\nmodel effectively addresses the challenges of high similarity and mutual\nocclusion in fish populations, enabling accurate tracking in factory farming\nenvironments.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "14 pages,14 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.01148v3",
    "published_date": "2024-09-02 10:33:45 UTC",
    "updated_date": "2025-01-10 03:28:00 UTC"
  },
  {
    "arxiv_id": "2409.01145v1",
    "title": "LATEX-GCL: Large Language Models (LLMs)-Based Data Augmentation for Text-Attributed Graph Contrastive Learning",
    "authors": [
      "Haoran Yang",
      "Xiangyu Zhao",
      "Sirui Huang",
      "Qing Li",
      "Guandong Xu"
    ],
    "abstract": "Graph Contrastive Learning (GCL) is a potent paradigm for self-supervised\ngraph learning that has attracted attention across various application\nscenarios. However, GCL for learning on Text-Attributed Graphs (TAGs) has yet\nto be explored. Because conventional augmentation techniques like feature\nembedding masking cannot directly process textual attributes on TAGs. A naive\nstrategy for applying GCL to TAGs is to encode the textual attributes into\nfeature embeddings via a language model and then feed the embeddings into the\nfollowing GCL module for processing. Such a strategy faces three key\nchallenges: I) failure to avoid information loss, II) semantic loss during the\ntext encoding phase, and III) implicit augmentation constraints that lead to\nuncontrollable and incomprehensible results. In this paper, we propose a novel\nGCL framework named LATEX-GCL to utilize Large Language Models (LLMs) to\nproduce textual augmentations and LLMs' powerful natural language processing\n(NLP) abilities to address the three limitations aforementioned to pave the way\nfor applying GCL to TAG tasks. Extensive experiments on four high-quality TAG\ndatasets illustrate the superiority of the proposed LATEX-GCL method. The\nsource codes and datasets are released to ease the reproducibility, which can\nbe accessed via this link: https://anonymous.4open.science/r/LATEX-GCL-0712.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01145v1",
    "published_date": "2024-09-02 10:30:55 UTC",
    "updated_date": "2024-09-02 10:30:55 UTC"
  },
  {
    "arxiv_id": "2409.01138v1",
    "title": "Generating Synthetic Satellite Imagery for Rare Objects: An Empirical Comparison of Models and Metrics",
    "authors": [
      "Tuong Vy Nguyen",
      "Johannes Hoster",
      "Alexander Glaser",
      "Kristian Hildebrand",
      "Felix Biessmann"
    ],
    "abstract": "Generative deep learning architectures can produce realistic, high-resolution\nfake imagery -- with potentially drastic societal implications. A key question\nin this context is: How easy is it to generate realistic imagery, in particular\nfor niche domains. The iterative process required to achieve specific image\ncontent is difficult to automate and control. Especially for rare classes, it\nremains difficult to assess fidelity, meaning whether generative approaches\nproduce realistic imagery and alignment, meaning how (well) the generation can\nbe guided by human input. In this work, we present a large-scale empirical\nevaluation of generative architectures which we fine-tuned to generate\nsynthetic satellite imagery. We focus on nuclear power plants as an example of\na rare object category - as there are only around 400 facilities worldwide,\nthis restriction is exemplary for many other scenarios in which training and\ntest data is limited by the restricted number of occurrences of real-world\nexamples. We generate synthetic imagery by conditioning on two kinds of\nmodalities, textual input and image input obtained from a game engine that\nallows for detailed specification of the building layout. The generated images\nare assessed by commonly used metrics for automatic evaluation and then\ncompared with human judgement from our conducted user studies to assess their\ntrustworthiness. Our results demonstrate that even for rare objects, generation\nof authentic synthetic satellite imagery with textual or detailed building\nlayouts is feasible. In line with previous work, we find that automated metrics\nare often not aligned with human perception -- in fact, we find strong negative\ncorrelations between commonly used image quality metrics and human ratings.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Presented at KI 2024 - 47th German Conference on AI, 2nd Workshop on\n  Public Interest AI, 23 September, 2024, Wuerzburg, DE",
    "pdf_url": "http://arxiv.org/pdf/2409.01138v1",
    "published_date": "2024-09-02 10:19:39 UTC",
    "updated_date": "2024-09-02 10:19:39 UTC"
  },
  {
    "arxiv_id": "2409.01137v3",
    "title": "Smart E-commerce Recommendations with Semantic AI",
    "authors": [
      "M. Badouch",
      "M. Boutaounte"
    ],
    "abstract": "In e-commerce, web mining for page recommendations is widely used but often\nfails to meet user needs. To address this, we propose a novel solution\ncombining semantic web mining with BP neural networks. We process user search\nlogs to extract five key features: content priority, time spent, user feedback,\nrecommendation semantics, and input deviation. These features are then fed into\na BP neural network to classify and prioritize web pages. The prioritized pages\nare recommended to users. Using book sales pages for testing, our results\ndemonstrate that this solution can quickly and accurately identify the pages\nusers need. Our approach ensures that recommendations are more relevant and\ntailored to individual preferences, enhancing the online shopping experience.\nBy leveraging advanced semantic analysis and neural network techniques, we\nbridge the gap between user expectations and actual recommendations. This\ninnovative method not only improves accuracy but also speeds up the\nrecommendation process, making it a valuable tool for e-commerce platforms\naiming to boost user satisfaction and engagement. Additionally, our system\nability to handle large datasets and provide real-time recommendations makes it\na scalable and efficient solution for modern e-commerce challenges.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "My paper contain some errors",
    "pdf_url": "http://arxiv.org/pdf/2409.01137v3",
    "published_date": "2024-09-02 10:19:31 UTC",
    "updated_date": "2024-09-11 22:59:34 UTC"
  },
  {
    "arxiv_id": "2409.01133v1",
    "title": "Large Language Models Can Understanding Depth from Monocular Images",
    "authors": [
      "Zhongyi Xia",
      "Tianzhao Wu"
    ],
    "abstract": "Monocular depth estimation is a critical function in computer vision\napplications. This paper shows that large language models (LLMs) can\neffectively interpret depth with minimal supervision, using efficient resource\nutilization and a consistent neural network architecture. We introduce LLM-MDE,\na multimodal framework that deciphers depth through language comprehension.\nSpecifically, LLM-MDE employs two main strategies to enhance the pretrained\nLLM's capability for depth estimation: cross-modal reprogramming and an\nadaptive prompt estimation module. These strategies align vision\nrepresentations with text prototypes and automatically generate prompts based\non monocular images, respectively. Comprehensive experiments on real-world MDE\ndatasets confirm the effectiveness and superiority of LLM-MDE, which excels in\nfew-/zero-shot tasks while minimizing resource use. The source code is\navailable.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01133v1",
    "published_date": "2024-09-02 10:11:52 UTC",
    "updated_date": "2024-09-02 10:11:52 UTC"
  },
  {
    "arxiv_id": "2409.07480v3",
    "title": "EEG-Language Modeling for Pathology Detection",
    "authors": [
      "Sam Gijsen",
      "Kerstin Ritter"
    ],
    "abstract": "Multimodal language modeling has enabled breakthroughs for representation\nlearning, yet remains unexplored in the realm of functional brain data for\npathology detection. This paper pioneers EEG-language models (ELMs) trained on\nclinical reports and 15000 EEGs. We propose to combine multimodal alignment in\nthis novel domain with timeseries cropping and text segmentation, enabling an\nextension based on multiple instance learning to alleviate misalignment between\nirrelevant EEG or text segments. Our multimodal models significantly improve\npathology detection compared to EEG-only models across four evaluations and for\nthe first time enable zero-shot classification as well as retrieval of both\nneural signals and reports. In sum, these results highlight the potential of\nELMs, representing significant progress for clinical applications.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.07480v3",
    "published_date": "2024-09-02 10:03:03 UTC",
    "updated_date": "2025-01-31 13:16:12 UTC"
  },
  {
    "arxiv_id": "2409.01124v1",
    "title": "Two-stage initial-value iterative physics-informed neural networks for simulating solitary waves of nonlinear wave equations",
    "authors": [
      "Jin Song",
      "Ming Zhong",
      "George Em Karniadakis",
      "Zhenya Yan"
    ],
    "abstract": "We propose a new two-stage initial-value iterative neural network (IINN)\nalgorithm for solitary wave computations of nonlinear wave equations based on\ntraditional numerical iterative methods and physics-informed neural networks\n(PINNs). Specifically, the IINN framework consists of two subnetworks, one of\nwhich is used to fit a given initial value, and the other incorporates physical\ninformation and continues training on the basis of the first subnetwork.\nImportantly, the IINN method does not require any additional data information\nincluding boundary conditions, apart from the given initial value.\nCorresponding theoretical guarantees are provided to demonstrate the\neffectiveness of our IINN method. The proposed IINN method is efficiently\napplied to learn some types of solutions in different nonlinear wave equations,\nincluding the one-dimensional (1D) nonlinear Schr\\\"odinger equations (NLS)\nequation (with and without potentials), the 1D saturable NLS equation with PT\n-symmetric optical lattices, the 1D focusing-defocusing coupled NLS equations,\nthe KdV equation, the two-dimensional (2D) NLS equation with potentials, the 2D\namended GP equation with a potential, the (2+1)-dimensional KP equation, and\nthe 3D NLS equation with a potential. These applications serve as evidence for\nthe efficacy of our method. Finally, by comparing with the traditional methods,\nwe demonstrate the advantages of the proposed IINN method.",
    "categories": [
      "physics.comp-ph",
      "cs.AI",
      "cs.LG",
      "math-ph",
      "math.MP",
      "nlin.PS",
      "nlin.SI"
    ],
    "primary_category": "physics.comp-ph",
    "comment": "25 pages, 17 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.01124v1",
    "published_date": "2024-09-02 10:00:02 UTC",
    "updated_date": "2024-09-02 10:00:02 UTC"
  },
  {
    "arxiv_id": "2409.01104v2",
    "title": "AI Olympics challenge with Evolutionary Soft Actor Critic",
    "authors": [
      "Marco Calì",
      "Alberto Sinigaglia",
      "Niccolò Turcato",
      "Ruggero Carli",
      "Gian Antonio Susto"
    ],
    "abstract": "In the following report, we describe the solution we propose for the AI\nOlympics competition held at IROS 2024. Our solution is based on a Model-free\nDeep Reinforcement Learning approach combined with an evolutionary strategy. We\nwill briefly describe the algorithms that have been used and then provide\ndetails of the approach",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.RO",
    "comment": "Added Sec 9 after testing on real robot",
    "pdf_url": "http://arxiv.org/pdf/2409.01104v2",
    "published_date": "2024-09-02 09:34:18 UTC",
    "updated_date": "2024-10-28 09:10:11 UTC"
  },
  {
    "arxiv_id": "2409.01093v1",
    "title": "DS MYOLO: A Reliable Object Detector Based on SSMs for Driving Scenarios",
    "authors": [
      "Yang Li",
      "Jianli Xiao"
    ],
    "abstract": "Accurate real-time object detection enhances the safety of advanced\ndriver-assistance systems, making it an essential component in driving\nscenarios. With the rapid development of deep learning technology, CNN-based\nYOLO real-time object detectors have gained significant attention. However, the\nlocal focus of CNNs results in performance bottlenecks. To further enhance\ndetector performance, researchers have introduced Transformer-based\nself-attention mechanisms to leverage global receptive fields, but their\nquadratic complexity incurs substantial computational costs. Recently, Mamba,\nwith its linear complexity, has made significant progress through global\nselective scanning. Inspired by Mamba's outstanding performance, we propose a\nnovel object detector: DS MYOLO. This detector captures global feature\ninformation through a simplified selective scanning fusion block (SimVSS Block)\nand effectively integrates the network's deep features. Additionally, we\nintroduce an efficient channel attention convolution (ECAConv) that enhances\ncross-channel feature interaction while maintaining low computational\ncomplexity. Extensive experiments on the CCTSDB 2021 and VLD-45 driving\nscenarios datasets demonstrate that DS MYOLO exhibits significant potential and\ncompetitive advantage among similarly scaled YOLO series real-time object\ndetectors.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "27th International Conference on Pattern Recognition(ICPR)",
    "pdf_url": "http://arxiv.org/pdf/2409.01093v1",
    "published_date": "2024-09-02 09:22:33 UTC",
    "updated_date": "2024-09-02 09:22:33 UTC"
  },
  {
    "arxiv_id": "2409.01092v1",
    "title": "Two-Timescale Synchronization and Migration for Digital Twin Networks: A Multi-Agent Deep Reinforcement Learning Approach",
    "authors": [
      "Wenshuai Liu",
      "Yaru Fu",
      "Yongna Guo",
      "Fu Lee Wang",
      "Wen Sun",
      "Yan Zhang"
    ],
    "abstract": "Digital twins (DTs) have emerged as a promising enabler for representing the\nreal-time states of physical worlds and realizing self-sustaining systems. In\npractice, DTs of physical devices, such as mobile users (MUs), are commonly\ndeployed in multi-access edge computing (MEC) networks for the sake of reducing\nlatency. To ensure the accuracy and fidelity of DTs, it is essential for MUs to\nregularly synchronize their status with their DTs. However, MU mobility\nintroduces significant challenges to DT synchronization. Firstly, MU mobility\ntriggers DT migration which could cause synchronization failures. Secondly, MUs\nrequire frequent synchronization with their DTs to ensure DT fidelity.\nNonetheless, DT migration among MEC servers, caused by MU mobility, may occur\ninfrequently. Accordingly, we propose a two-timescale DT synchronization and\nmigration framework with reliability consideration by establishing a non-convex\nstochastic problem to minimize the long-term average energy consumption of MUs.\nWe use Lyapunov theory to convert the reliability constraints and reformulate\nthe new problem as a partially observable Markov decision-making process\n(POMDP). Furthermore, we develop a heterogeneous agent proximal policy\noptimization with Beta distribution (Beta-HAPPO) method to solve it. Numerical\nresults show that our proposed Beta-HAPPO method achieves significant\nimprovements in energy savings when compared with other benchmarks.",
    "categories": [
      "cs.ET",
      "cs.AI",
      "cs.NI",
      "C.2.3; C.2.4"
    ],
    "primary_category": "cs.ET",
    "comment": "15 pages, 14 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.01092v1",
    "published_date": "2024-09-02 09:20:46 UTC",
    "updated_date": "2024-09-02 09:20:46 UTC"
  },
  {
    "arxiv_id": "2409.01087v1",
    "title": "Pre-Trained Language Models for Keyphrase Prediction: A Review",
    "authors": [
      "Muhammad Umair",
      "Tangina Sultana",
      "Young-Koo Lee"
    ],
    "abstract": "Keyphrase Prediction (KP) is essential for identifying keyphrases in a\ndocument that can summarize its content. However, recent Natural Language\nProcessing (NLP) advances have developed more efficient KP models using deep\nlearning techniques. The limitation of a comprehensive exploration jointly both\nkeyphrase extraction and generation using pre-trained language models\nspotlights a critical gap in the literature, compelling our survey paper to\nbridge this deficiency and offer a unified and in-depth analysis to address\nlimitations in previous surveys. This paper extensively examines the topic of\npre-trained language models for keyphrase prediction (PLM-KP), which are\ntrained on large text corpora via different learning (supervisor, unsupervised,\nsemi-supervised, and self-supervised) techniques, to provide respective\ninsights into these two types of tasks in NLP, precisely, Keyphrase Extraction\n(KPE) and Keyphrase Generation (KPG). We introduce appropriate taxonomies for\nPLM-KPE and KPG to highlight these two main tasks of NLP. Moreover, we point\nout some promising future directions for predicting keyphrases.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01087v1",
    "published_date": "2024-09-02 09:15:44 UTC",
    "updated_date": "2024-09-02 09:15:44 UTC"
  },
  {
    "arxiv_id": "2409.01086v2",
    "title": "DPDEdit: Detail-Preserved Diffusion Models for Multimodal Fashion Image Editing",
    "authors": [
      "Xiaolong Wang",
      "Zhi-Qi Cheng",
      "Jue Wang",
      "Xiaojiang Peng"
    ],
    "abstract": "Fashion image editing is a crucial tool for designers to convey their\ncreative ideas by visualizing design concepts interactively. Current fashion\nimage editing techniques, though advanced with multimodal prompts and powerful\ndiffusion models, often struggle to accurately identify editing regions and\npreserve the desired garment texture detail. To address these challenges, we\nintroduce a new multimodal fashion image editing architecture based on latent\ndiffusion models, called Detail-Preserved Diffusion Models (DPDEdit). DPDEdit\nguides the fashion image generation of diffusion models by integrating text\nprompts, region masks, human pose images, and garment texture images. To\nprecisely locate the editing region, we first introduce Grounded-SAM to predict\nthe editing region based on the user's textual description, and then combine it\nwith other conditions to perform local editing. To transfer the detail of the\ngiven garment texture into the target fashion image, we propose a texture\ninjection and refinement mechanism. Specifically, this mechanism employs a\ndecoupled cross-attention layer to integrate textual descriptions and texture\nimages, and incorporates an auxiliary U-Net to preserve the high-frequency\ndetails of generated garment texture. Additionally, we extend the VITON-HD\ndataset using a multimodal large language model to generate paired samples with\ntexture images and textual descriptions. Extensive experiments show that our\nDPDEdit outperforms state-of-the-art methods in terms of image fidelity and\ncoherence with the given multimodal inputs.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "13 pages,12 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.01086v2",
    "published_date": "2024-09-02 09:15:26 UTC",
    "updated_date": "2024-09-14 02:43:51 UTC"
  },
  {
    "arxiv_id": "2409.01083v4",
    "title": "Affordance-based Robot Manipulation with Flow Matching",
    "authors": [
      "Fan Zhang",
      "Michael Gienger"
    ],
    "abstract": "We present a framework for assistive robot manipulation, which focuses on two\nfundamental challenges: first, efficiently adapting large-scale models to\ndownstream scene affordance understanding tasks, especially in daily living\nscenarios where gathering multi-task data involving humans requires strenuous\neffort; second, effectively learning robot action trajectories by grounding the\nvisual affordance model. We tackle the first challenge by employing a\nparameter-efficient prompt tuning method that prepends learnable text prompts\nto the frozen vision model to predict manipulation affordances in multi-task\nscenarios. Then we propose to learn robot action trajectories guided by\naffordances in a supervised flow matching method. Flow matching represents a\nrobot visuomotor policy as a conditional process of flowing random waypoints to\ndesired robot action trajectories. Finally, we introduce a real-world dataset\nwith 10 tasks across Activities of Daily Living to test our framework. Our\nextensive evaluation highlights that the proposed prompt tuning method for\nlearning manipulation affordance achieves competitive performance and even\noutperforms some other finetuning protocols across data scales, while\nsatisfying parameter efficiency. Learning multi-task robot action trajectories\nwith flow matching leads to consistently favorable results in several robot\nmanipulation benchmarks than some alternative behavior cloning methods. This\nincludes more stable training and evaluation, and noticeably faster inference,\nwhile maintaining comparable generalization performance to diffusion policy,\nwhere flow matching performs marginally better in most cases. Our framework\nseamlessly unifies affordance learning and action generation with flow matching\nfor robot manipulation.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01083v4",
    "published_date": "2024-09-02 09:11:28 UTC",
    "updated_date": "2025-02-01 11:58:47 UTC"
  },
  {
    "arxiv_id": "2409.01081v1",
    "title": "Beyond Efficiency: Molecular Data Pruning for Enhanced Generalization",
    "authors": [
      "Dingshuo Chen",
      "Zhixun Li",
      "Yuyan Ni",
      "Guibin Zhang",
      "Ding Wang",
      "Qiang Liu",
      "Shu Wu",
      "Jeffrey Xu Yu",
      "Liang Wang"
    ],
    "abstract": "With the emergence of various molecular tasks and massive datasets, how to\nperform efficient training has become an urgent yet under-explored issue in the\narea. Data pruning (DP), as an oft-stated approach to saving training burdens,\nfilters out less influential samples to form a coreset for training. However,\nthe increasing reliance on pretrained models for molecular tasks renders\ntraditional in-domain DP methods incompatible. Therefore, we propose a\nMolecular data Pruning framework for enhanced Generalization (MolPeg), which\nfocuses on the source-free data pruning scenario, where data pruning is applied\nwith pretrained models. By maintaining two models with different updating paces\nduring training, we introduce a novel scoring function to measure the\ninformativeness of samples based on the loss discrepancy. As a plug-and-play\nframework, MolPeg realizes the perception of both source and target domain and\nconsistently outperforms existing DP methods across four downstream tasks.\nRemarkably, it can surpass the performance obtained from full-dataset training,\neven when pruning up to 60-70% of the data on HIV and PCBA dataset. Our work\nsuggests that the discovery of effective data-pruning metrics could provide a\nviable path to both enhanced efficiency and superior generalization in transfer\nlearning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.BM"
    ],
    "primary_category": "cs.LG",
    "comment": "20 pages, under review",
    "pdf_url": "http://arxiv.org/pdf/2409.01081v1",
    "published_date": "2024-09-02 09:06:04 UTC",
    "updated_date": "2024-09-02 09:06:04 UTC"
  },
  {
    "arxiv_id": "2409.01074v1",
    "title": "Bootstrap SGD: Algorithmic Stability and Robustness",
    "authors": [
      "Andreas Christmann",
      "Yunwen Lei"
    ],
    "abstract": "In this paper some methods to use the empirical bootstrap approach for\nstochastic gradient descent (SGD) to minimize the empirical risk over a\nseparable Hilbert space are investigated from the view point of algorithmic\nstability and statistical robustness. The first two types of approaches are\nbased on averages and are investigated from a theoretical point of view. A\ngeneralization analysis for bootstrap SGD of Type 1 and Type 2 based on\nalgorithmic stability is done. Another type of bootstrap SGD is proposed to\ndemonstrate that it is possible to construct purely distribution-free pointwise\nconfidence intervals of the median curve using bootstrap SGD.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01074v1",
    "published_date": "2024-09-02 08:56:39 UTC",
    "updated_date": "2024-09-02 08:56:39 UTC"
  },
  {
    "arxiv_id": "2409.01073v1",
    "title": "SCOPE: Sign Language Contextual Processing with Embedding from LLMs",
    "authors": [
      "Yuqi Liu",
      "Wenqian Zhang",
      "Sihan Ren",
      "Chengyu Huang",
      "Jingyi Yu",
      "Lan Xu"
    ],
    "abstract": "Sign languages, used by around 70 million Deaf individuals globally, are\nvisual languages that convey visual and contextual information. Current methods\nin vision-based sign language recognition (SLR) and translation (SLT) struggle\nwith dialogue scenes due to limited dataset diversity and the neglect of\ncontextually relevant information. To address these challenges, we introduce\nSCOPE (Sign language Contextual Processing with Embedding from LLMs), a novel\ncontext-aware vision-based SLR and SLT framework. For SLR, we utilize dialogue\ncontexts through a multi-modal encoder to enhance gloss-level recognition. For\nsubsequent SLT, we further fine-tune a Large Language Model (LLM) by\nincorporating prior conversational context. We also contribute a new sign\nlanguage dataset that contains 72 hours of Chinese sign language videos in\ncontextual dialogues across various scenarios. Experimental results demonstrate\nthat our SCOPE framework achieves state-of-the-art performance on multiple\ndatasets, including Phoenix-2014T, CSL-Daily, and our SCOPE dataset. Moreover,\nsurveys conducted with participants from the Deaf community further validate\nthe robustness and effectiveness of our approach in real-world applications.\nBoth our dataset and code will be open-sourced to facilitate further research.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01073v1",
    "published_date": "2024-09-02 08:56:12 UTC",
    "updated_date": "2024-09-02 08:56:12 UTC"
  },
  {
    "arxiv_id": "2409.01066v1",
    "title": "Learning in Hybrid Active Inference Models",
    "authors": [
      "Poppy Collis",
      "Ryan Singh",
      "Paul F Kinghorn",
      "Christopher L Buckley"
    ],
    "abstract": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work in computational neuroscience has considered this\nfunctional integration of discrete and continuous variables during\ndecision-making under the formalism of active inference (Parr, Friston & de\nVries, 2017; Parr & Friston, 2018). However, their focus is on the expressive\nphysical implementation of categorical decisions and the hierarchical mixed\ngenerative model is assumed to be known. As a consequence, it is unclear how\nthis framework might be extended to learning. We therefore present a novel\nhierarchical hybrid active inference agent in which a high-level discrete\nactive inference planner sits above a low-level continuous active inference\ncontroller. We make use of recent work in recurrent switching linear dynamical\nsystems (rSLDS) which implement end-to-end learning of meaningful discrete\nrepresentations via the piecewise linear decomposition of complex continuous\ndynamics (Linderman et al., 2016). The representations learned by the rSLDS\ninform the structure of the hybrid decision-making agent and allow us to (1)\nspecify temporally-abstracted sub-goals in a method reminiscent of the options\nframework, (2) lift the exploration into discrete space allowing us to exploit\ninformation-theoretic exploration bonuses and (3) `cache' the approximate\nsolutions to low-level problems in the discrete planner. We apply our model to\nthe sparse Continuous Mountain Car task, demonstrating fast system\nidentification via enhanced exploration and successful planning through the\ndelineation of abstract sub-goals.",
    "categories": [
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.AI",
    "comment": "11 pages (+ appendix). Accepted to the International Workshop on\n  Active Inference 2024. arXiv admin note: substantial text overlap with\n  arXiv:2408.10970",
    "pdf_url": "http://arxiv.org/pdf/2409.01066v1",
    "published_date": "2024-09-02 08:41:45 UTC",
    "updated_date": "2024-09-02 08:41:45 UTC"
  },
  {
    "arxiv_id": "2409.01053v1",
    "title": "A Perspective on Literary Metaphor in the Context of Generative AI",
    "authors": [
      "Imke van Heerden",
      "Anil Bas"
    ],
    "abstract": "At the intersection of creative text generation and literary theory, this\nstudy explores the role of literary metaphor and its capacity to generate a\nrange of meanings. In this regard, literary metaphor is vital to the\ndevelopment of any particular language. To investigate whether the inclusion of\noriginal figurative language improves textual quality, we trained an LSTM-based\nlanguage model in Afrikaans. The network produces phrases containing\ncompellingly novel figures of speech. Specifically, the emphasis falls on how\nAI might be utilised as a defamiliarisation technique, which disrupts expected\nuses of language to augment poetic expression. Providing a literary perspective\non text generation, the paper raises thought-provoking questions on aesthetic\nvalue, interpretation and evaluation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted as oral presentation to Workshop on Artificial Intelligence\n  and Creativity (CREAI) at ECAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.01053v1",
    "published_date": "2024-09-02 08:27:29 UTC",
    "updated_date": "2024-09-02 08:27:29 UTC"
  },
  {
    "arxiv_id": "2409.01046v1",
    "title": "Accelerated Multi-objective Task Learning using Modified Q-learning Algorithm",
    "authors": [
      "Varun Prakash Rajamohan",
      "Senthil Kumar Jagatheesaperumal"
    ],
    "abstract": "Robots find extensive applications in industry. In recent years, the\ninfluence of robots has also increased rapidly in domestic scenarios. The\nQ-learning algorithm aims to maximise the reward for reaching the goal. This\npaper proposes a modified version of the Q-learning algorithm, known as\nQ-learning with scaled distance metric (Q-SD). This algorithm enhances task\nlearning and makes task completion more meaningful. A robotic manipulator\n(agent) applies the Q-SD algorithm to the task of table cleaning. Using Q-SD,\nthe agent acquires the sequence of steps necessary to accomplish the task while\nminimising the manipulator's movement distance. We partition the table into\ngrids of different dimensions. The first has a grid count of 3 times 3, and the\nsecond has a grid count of 4 times 4. Using the Q-SD algorithm, the maximum\nsuccess obtained in these two environments was 86% and 59% respectively.\nMoreover, Compared to the conventional Q-learning algorithm, the drop in\naverage distance moved by the agent in these two environments using the Q-SD\nalgorithm was 8.61% and 6.7% respectively.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "68T05, 93C85, 93B40, 90C29",
      "I.2.6; I.2.9; I.2.8; F.1.1; F.2.1; H.1.2; G.1.6"
    ],
    "primary_category": "cs.RO",
    "comment": "9 pages, 9 figures, 7 tables",
    "pdf_url": "http://arxiv.org/pdf/2409.01046v1",
    "published_date": "2024-09-02 08:20:41 UTC",
    "updated_date": "2024-09-02 08:20:41 UTC"
  },
  {
    "arxiv_id": "2409.01038v1",
    "title": "Robust Vehicle Localization and Tracking in Rain using Street Maps",
    "authors": [
      "Yu Xiang Tan",
      "Malika Meghjani"
    ],
    "abstract": "GPS-based vehicle localization and tracking suffers from unstable positional\ninformation commonly experienced in tunnel segments and in dense urban areas.\nAlso, both Visual Odometry (VO) and Visual Inertial Odometry (VIO) are\nsusceptible to adverse weather conditions that causes occlusions or blur on the\nvisual input. In this paper, we propose a novel approach for vehicle\nlocalization that uses street network based map information to correct drifting\nodometry estimates and intermittent GPS measurements especially, in adversarial\nscenarios such as driving in rain and tunnels. Specifically, our approach is a\nflexible fusion algorithm that integrates intermittent GPS, drifting IMU and VO\nestimates together with 2D map information for robust vehicle localization and\ntracking. We refer to our approach as Map-Fusion. We robustly evaluate our\nproposed approach on four geographically diverse datasets from different\ncountries ranging across clear and rain weather conditions. These datasets also\ninclude challenging visual segments in tunnels and underpasses. We show that\nwith the integration of the map information, our Map-Fusion algorithm reduces\nthe error of the state-of-the-art VO and VIO approaches across all datasets. We\nalso validate our proposed algorithm in a real-world environment and in\nreal-time on a hardware constrained mobile robot. Map-Fusion achieved 2.46m\nerror in clear weather and 6.05m error in rain weather for a 150m route.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.01038v1",
    "published_date": "2024-09-02 08:15:12 UTC",
    "updated_date": "2024-09-02 08:15:12 UTC"
  },
  {
    "arxiv_id": "2409.01014v1",
    "title": "From Bird's-Eye to Street View: Crafting Diverse and Condition-Aligned Images with Latent Diffusion Model",
    "authors": [
      "Xiaojie Xu",
      "Tianshuo Xu",
      "Fulong Ma",
      "Yingcong Chen"
    ],
    "abstract": "We explore Bird's-Eye View (BEV) generation, converting a BEV map into its\ncorresponding multi-view street images. Valued for its unified spatial\nrepresentation aiding multi-sensor fusion, BEV is pivotal for various\nautonomous driving applications. Creating accurate street-view images from BEV\nmaps is essential for portraying complex traffic scenarios and enhancing\ndriving algorithms. Concurrently, diffusion-based conditional image generation\nmodels have demonstrated remarkable outcomes, adept at producing diverse,\nhigh-quality, and condition-aligned results. Nonetheless, the training of these\nmodels demands substantial data and computational resources. Hence, exploring\nmethods to fine-tune these advanced models, like Stable Diffusion, for specific\nconditional generation tasks emerges as a promising avenue. In this paper, we\nintroduce a practical framework for generating images from a BEV layout. Our\napproach comprises two main components: the Neural View Transformation and the\nStreet Image Generation. The Neural View Transformation phase converts the BEV\nmap into aligned multi-view semantic segmentation maps by learning the shape\ncorrespondence between the BEV and perspective views. Subsequently, the Street\nImage Generation phase utilizes these segmentations as a condition to guide a\nfine-tuned latent diffusion model. This finetuning process ensures both view\nand style consistency. Our model leverages the generative capacity of large\npretrained diffusion models within traffic contexts, effectively yielding\ndiverse and condition-coherent street view images.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at International Conference on Robotics and Automation(ICRA)",
    "pdf_url": "http://arxiv.org/pdf/2409.01014v1",
    "published_date": "2024-09-02 07:47:16 UTC",
    "updated_date": "2024-09-02 07:47:16 UTC"
  },
  {
    "arxiv_id": "2409.01013v1",
    "title": "SeCo-INR: Semantically Conditioned Implicit Neural Representations for Improved Medical Image Super-Resolution",
    "authors": [
      "Mevan Ekanayake",
      "Zhifeng Chen",
      "Gary Egan",
      "Mehrtash Harandi",
      "Zhaolin Chen"
    ],
    "abstract": "Implicit Neural Representations (INRs) have recently advanced the field of\ndeep learning due to their ability to learn continuous representations of\nsignals without the need for large training datasets. Although INR methods have\nbeen studied for medical image super-resolution, their adaptability to\nlocalized priors in medical images has not been extensively explored. Medical\nimages contain rich anatomical divisions that could provide valuable local\nprior information to enhance the accuracy and robustness of INRs. In this work,\nwe propose a novel framework, referred to as the Semantically Conditioned INR\n(SeCo-INR), that conditions an INR using local priors from a medical image,\nenabling accurate model fitting and interpolation capabilities to achieve\nsuper-resolution. Our framework learns a continuous representation of the\nsemantic segmentation features of a medical image and utilizes it to derive the\noptimal INR for each semantic region of the image. We tested our framework\nusing several medical imaging modalities and achieved higher quantitative\nscores and more realistic super-resolution outputs compared to state-of-the-art\nmethods.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "This paper was accepted for presentation at the IEEE/CVF Winter\n  Conference on Applications of Computer Vision (WACV) 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.01013v1",
    "published_date": "2024-09-02 07:45:06 UTC",
    "updated_date": "2024-09-02 07:45:06 UTC"
  },
  {
    "arxiv_id": "2409.01007v3",
    "title": "Unlocking the Wisdom of Large Language Models: An Introduction to The Path to Artificial General Intelligence",
    "authors": [
      "Edward Y. Chang"
    ],
    "abstract": "This booklet, Unlocking the Wisdom of Multi-LLM Collaborative Intelligence,\nserves as an accessible introduction to the full volume The Path to Artificial\nGeneral Intelligence. Through fourteen aphorisms, it distills the core\nprinciples of Multi-LLM Agent Collaborative Intelligence (MACI), a framework\ndesigned to coordinate multiple LLMs toward reasoning, planning, and\ndecision-making that surpasses the capabilities of any single model. The\nbooklet includes titles, abstracts, and introductions from each main chapter,\nalong with the full content of the first two. The newly released third edition\nfeatures significant enhancements to Chapters 6 through 9 and a revised preface\nresponding to Yann LeCun's critique of AGI feasibility. While LeCun argues that\nLLMs lack grounding, memory, and planning, we propose that MACI's collaborative\narchitecture, featuring multimodal agents in executive, legislative, and\njudicial roles, directly addresses these limitations. Chapters on SocraSynth,\nEVINCE, consciousness modeling, and behavior regulation demonstrate that\nreasoning systems grounded in structured interaction and checks and balances\ncan produce more reliable, interpretable, and adaptive intelligence. By\nintegrating complementary model strengths, including world modeling and\nmultimodal perception, MACI enables a system-level intelligence that exceeds\nthe sum of its parts. Like human institutions, progress in AI may depend less\non isolated performance and more on coordinated judgment. Collaborative LLMs,\nnot just larger ones, may chart the path toward artificial general\nintelligence.",
    "categories": [
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.AI",
    "comment": "153 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.01007v3",
    "published_date": "2024-09-02 07:29:37 UTC",
    "updated_date": "2025-04-15 05:21:22 UTC"
  },
  {
    "arxiv_id": "2409.00991v2",
    "title": "3D Priors-Guided Diffusion for Blind Face Restoration",
    "authors": [
      "Xiaobin Lu",
      "Xiaobin Hu",
      "Jun Luo",
      "Ben Zhu",
      "Yaping Ruan",
      "Wenqi Ren"
    ],
    "abstract": "Blind face restoration endeavors to restore a clear face image from a\ndegraded counterpart. Recent approaches employing Generative Adversarial\nNetworks (GANs) as priors have demonstrated remarkable success in this field.\nHowever, these methods encounter challenges in achieving a balance between\nrealism and fidelity, particularly in complex degradation scenarios. To inherit\nthe exceptional realism generative ability of the diffusion model and also\nconstrained by the identity-aware fidelity, we propose a novel diffusion-based\nframework by embedding the 3D facial priors as structure and identity\nconstraints into a denoising diffusion process. Specifically, in order to\nobtain more accurate 3D prior representations, the 3D facial image is\nreconstructed by a 3D Morphable Model (3DMM) using an initial restored face\nimage that has been processed by a pretrained restoration network. A customized\nmulti-level feature extraction method is employed to exploit both structural\nand identity information of 3D facial images, which are then mapped into the\nnoise estimation process. In order to enhance the fusion of identity\ninformation into the noise estimation, we propose a Time-Aware Fusion Block\n(TAFB). This module offers a more efficient and adaptive fusion of weights for\ndenoising, considering the dynamic nature of the denoising process in the\ndiffusion model, which involves initial structure refinement followed by\ntexture detail enhancement. Extensive experiments demonstrate that our network\nperforms favorably against state-of-the-art algorithms on synthetic and\nreal-world datasets for blind face restoration. The Code is released on our\nproject page at https://github.com/838143396/3Diffusion.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This paper was accepted by ACM MM 2024, and the project page is\n  accessible at: https://github.com/838143396/3Diffusion",
    "pdf_url": "http://arxiv.org/pdf/2409.00991v2",
    "published_date": "2024-09-02 07:13:32 UTC",
    "updated_date": "2024-09-12 07:10:41 UTC"
  },
  {
    "arxiv_id": "2409.00985v1",
    "title": "Co-Learning: Code Learning for Multi-Agent Reinforcement Collaborative Framework with Conversational Natural Language Interfaces",
    "authors": [
      "Jiapeng Yu",
      "Yuqian Wu",
      "Yajing Zhan",
      "Wenhao Guo",
      "Zhou Xu",
      "Raymond Lee"
    ],
    "abstract": "Online question-and-answer (Q\\&A) systems based on the Large Language Model\n(LLM) have progressively diverged from recreational to professional use. This\npaper proposed a Multi-Agent framework with environmentally reinforcement\nlearning (E-RL) for code correction called Code Learning (Co-Learning)\ncommunity, assisting beginners to correct code errors independently. It\nevaluates the performance of multiple LLMs from an original dataset with 702\nerror codes, uses it as a reward or punishment criterion for E-RL; Analyzes\ninput error codes by the current agent; selects the appropriate LLM-based agent\nto achieve optimal error correction accuracy and reduce correction time.\nExperiment results showed that 3\\% improvement in Precision score and 15\\%\nimprovement in time cost as compared with no E-RL method respectively. Our\nsource code is available at: https://github.com/yuqian2003/Co_Learning",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "12 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.00985v1",
    "published_date": "2024-09-02 07:03:22 UTC",
    "updated_date": "2024-09-02 07:03:22 UTC"
  },
  {
    "arxiv_id": "2409.00980v2",
    "title": "DNN-GDITD: Out-of-distribution detection via Deep Neural Network based Gaussian Descriptor for Imbalanced Tabular Data",
    "authors": [
      "Priyanka Chudasama",
      "Anil Surisetty",
      "Aakarsh Malhotra",
      "Alok Singh"
    ],
    "abstract": "Classification tasks present challenges due to class imbalances and evolving\ndata distributions. Addressing these issues requires a robust method to handle\nimbalances while effectively detecting out-of-distribution (OOD) samples not\nencountered during training. This study introduces a novel OOD detection\nalgorithm designed for tabular datasets, titled Deep Neural Network-based\nGaussian Descriptor for Imbalanced Tabular Data (DNN-GDITD). The DNN-GDITD\nalgorithm can be placed on top of any DNN to facilitate better classification\nof imbalanced data and OOD detection using spherical decision boundaries. Using\na combination of Push, Score-based, and focal losses, DNN-GDITD assigns\nconfidence scores to test data points, categorizing them as known classes or as\nan OOD sample. Extensive experimentation on tabular datasets demonstrates the\neffectiveness of DNN-GDITD compared to three OOD algorithms. Evaluation\nencompasses imbalanced and balanced scenarios on diverse tabular datasets,\nincluding a synthetic financial dispute dataset and publicly available tabular\ndatasets like Gas Sensor, Drive Diagnosis, and MNIST, showcasing DNN-GDITD's\nversatility.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "17 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.00980v2",
    "published_date": "2024-09-02 06:52:01 UTC",
    "updated_date": "2024-09-04 12:25:28 UTC"
  },
  {
    "arxiv_id": "2409.00974v1",
    "title": "Enhancing Privacy in Federated Learning: Secure Aggregation for Real-World Healthcare Applications",
    "authors": [
      "Riccardo Taiello",
      "Sergen Cansiz",
      "Marc Vesin",
      "Francesco Cremonesi",
      "Lucia Innocenti",
      "Melek Önen",
      "Marco Lorenzi"
    ],
    "abstract": "Deploying federated learning (FL) in real-world scenarios, particularly in\nhealthcare, poses challenges in communication and security. In particular, with\nrespect to the federated aggregation procedure, researchers have been focusing\non the study of secure aggregation (SA) schemes to provide privacy guarantees\nover the model's parameters transmitted by the clients. Nevertheless, the\npractical availability of SA in currently available FL frameworks is currently\nlimited, due to computational and communication bottlenecks. To fill this gap,\nthis study explores the implementation of SA within the open-source Fed-BioMed\nframework. We implement and compare two SA protocols, Joye-Libert (JL) and Low\nOverhead Masking (LOM), by providing extensive benchmarks in a panel of\nhealthcare data analysis problems. Our theoretical and experimental evaluations\non four datasets demonstrate that SA protocols effectively protect privacy\nwhile maintaining task accuracy. Computational overhead during training is less\nthan 1% on a CPU and less than 50% on a GPU for large models, with protection\nphases taking less than 10 seconds. Incorporating SA into Fed-BioMed impacts\ntask accuracy by no more than 2% compared to non-SA scenarios. Overall this\nstudy demonstrates the feasibility of SA in real-world healthcare applications\nand contributes in reducing the gap towards the adoption of privacy-preserving\ntechnologies in sensitive applications.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted at the 5-th MICCAI Workshop on Distributed, Collaborative\n  and Federated Learning in Conjunction with MICCAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.00974v1",
    "published_date": "2024-09-02 06:43:22 UTC",
    "updated_date": "2024-09-02 06:43:22 UTC"
  },
  {
    "arxiv_id": "2409.00968v1",
    "title": "Solving Integrated Process Planning and Scheduling Problem via Graph Neural Network Based Deep Reinforcement Learning",
    "authors": [
      "Hongpei Li",
      "Han Zhang",
      "Ziyan He",
      "Yunkai Jia",
      "Bo Jiang",
      "Xiang Huang",
      "Dongdong Ge"
    ],
    "abstract": "The Integrated Process Planning and Scheduling (IPPS) problem combines\nprocess route planning and shop scheduling to achieve high efficiency in\nmanufacturing and maximize resource utilization, which is crucial for modern\nmanufacturing systems. Traditional methods using Mixed Integer Linear\nProgramming (MILP) and heuristic algorithms can not well balance solution\nquality and speed when solving IPPS. In this paper, we propose a novel\nend-to-end Deep Reinforcement Learning (DRL) method. We model the IPPS problem\nas a Markov Decision Process (MDP) and employ a Heterogeneous Graph Neural\nNetwork (GNN) to capture the complex relationships among operations, machines,\nand jobs. To optimize the scheduling strategy, we use Proximal Policy\nOptimization (PPO). Experimental results show that, compared to traditional\nmethods, our approach significantly improves solution efficiency and quality in\nlarge-scale IPPS instances, providing superior scheduling strategies for modern\nintelligent manufacturing systems.",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "math.OC",
    "comment": "24 pages, 13 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.00968v1",
    "published_date": "2024-09-02 06:18:30 UTC",
    "updated_date": "2024-09-02 06:18:30 UTC"
  },
  {
    "arxiv_id": "2409.00951v1",
    "title": "Semantically Controllable Augmentations for Generalizable Robot Learning",
    "authors": [
      "Zoey Chen",
      "Zhao Mandi",
      "Homanga Bharadhwaj",
      "Mohit Sharma",
      "Shuran Song",
      "Abhishek Gupta",
      "Vikash Kumar"
    ],
    "abstract": "Generalization to unseen real-world scenarios for robot manipulation requires\nexposure to diverse datasets during training. However, collecting large\nreal-world datasets is intractable due to high operational costs. For robot\nlearning to generalize despite these challenges, it is essential to leverage\nsources of data or priors beyond the robot's direct experience. In this work,\nwe posit that image-text generative models, which are pre-trained on large\ncorpora of web-scraped data, can serve as such a data source. These generative\nmodels encompass a broad range of real-world scenarios beyond a robot's direct\nexperience and can synthesize novel synthetic experiences that expose robotic\nagents to additional world priors aiding real-world generalization at no extra\ncost.\n  In particular, our approach leverages pre-trained generative models as an\neffective tool for data augmentation. We propose a generative augmentation\nframework for semantically controllable augmentations and rapidly multiplying\nrobot datasets while inducing rich variations that enable real-world\ngeneralization. Based on diverse augmentations of robot data, we show how\nscalable robot manipulation policies can be trained and deployed both in\nsimulation and in unseen real-world environments such as kitchens and\ntable-tops. By demonstrating the effectiveness of image-text generative models\nin diverse real-world robotic applications, our generative augmentation\nframework provides a scalable and efficient path for boosting generalization in\nrobot learning at no extra human cost.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted for publication by IJRR. First 3 authors contributed\n  equally. Last 3 authors advised equally",
    "pdf_url": "http://arxiv.org/pdf/2409.00951v1",
    "published_date": "2024-09-02 05:25:34 UTC",
    "updated_date": "2024-09-02 05:25:34 UTC"
  },
  {
    "arxiv_id": "2409.00947v1",
    "title": "XNet v2: Fewer Limitations, Better Results and Greater Universality",
    "authors": [
      "Yanfeng Zhou",
      "Lingrui Li",
      "Zichen Wang",
      "Guole Liu",
      "Ziwen Liu",
      "Ge Yang"
    ],
    "abstract": "XNet introduces a wavelet-based X-shaped unified architecture for fully- and\nsemi-supervised biomedical segmentation. So far, however, XNet still faces the\nlimitations, including performance degradation when images lack high-frequency\n(HF) information, underutilization of raw images and insufficient fusion. To\naddress these issues, we propose XNet v2, a low- and high-frequency\ncomplementary model. XNet v2 performs wavelet-based image-level complementary\nfusion, using fusion results along with raw images inputs three different\nsub-networks to construct consistency loss. Furthermore, we introduce a\nfeature-level fusion module to enhance the transfer of low-frequency (LF)\ninformation and HF information. XNet v2 achieves state-of-the-art in\nsemi-supervised segmentation while maintaining competitve results in\nfully-supervised learning. More importantly, XNet v2 excels in scenarios where\nXNet fails. Compared to XNet, XNet v2 exhibits fewer limitations, better\nresults and greater universality. Extensive experiments on three 2D and two 3D\ndatasets demonstrate the effectiveness of XNet v2. Code is available at\nhttps://github.com/Yanfeng-Zhou/XNetv2 .",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.00947v1",
    "published_date": "2024-09-02 05:20:18 UTC",
    "updated_date": "2024-09-02 05:20:18 UTC"
  },
  {
    "arxiv_id": "2409.00946v2",
    "title": "A Framework for Synthetic Audio Conversations Generation using Large Language Models",
    "authors": [
      "Kaung Myat Kyaw",
      "Jonathan Hoyin Chan"
    ],
    "abstract": "In this paper, we introduce ConversaSynth, a framework designed to generate\nsynthetic conversation audio using large language models (LLMs) with multiple\npersona settings. The framework first creates diverse and coherent text-based\ndialogues across various topics, which are then converted into audio using\ntext-to-speech (TTS) systems. Our experiments demonstrate that ConversaSynth\neffectively generates highquality synthetic audio datasets, which can\nsignificantly enhance the training and evaluation of models for audio tagging,\naudio classification, and multi-speaker speech recognition. The results\nindicate that the synthetic datasets generated by ConversaSynth exhibit\nsubstantial diversity and realism, making them suitable for developing robust,\nadaptable audio-based AI systems.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "This work has been submitted for consideration at the WI-IAT'24 to be\n  held in December 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.00946v2",
    "published_date": "2024-09-02 05:09:46 UTC",
    "updated_date": "2024-11-02 04:00:28 UTC"
  },
  {
    "arxiv_id": "2409.00940v1",
    "title": "Large Language Models for Automatic Detection of Sensitive Topics",
    "authors": [
      "Ruoyu Wen",
      "Stephanie Elena Crowe",
      "Kunal Gupta",
      "Xinyue Li",
      "Mark Billinghurst",
      "Simon Hoermann",
      "Dwain Allan",
      "Alaeddin Nassani",
      "Thammathip Piumsomboon"
    ],
    "abstract": "Sensitive information detection is crucial in content moderation to maintain\nsafe online communities. Assisting in this traditionally manual process could\nrelieve human moderators from overwhelming and tedious tasks, allowing them to\nfocus solely on flagged content that may pose potential risks. Rapidly\nadvancing large language models (LLMs) are known for their capability to\nunderstand and process natural language and so present a potential solution to\nsupport this process. This study explores the capabilities of five LLMs for\ndetecting sensitive messages in the mental well-being domain within two online\ndatasets and assesses their performance in terms of accuracy, precision,\nrecall, F1 scores, and consistency. Our findings indicate that LLMs have the\npotential to be integrated into the moderation workflow as a convenient and\nprecise detection tool. The best-performing model, GPT-4o, achieved an average\naccuracy of 99.5\\% and an F1-score of 0.99. We discuss the advantages and\npotential challenges of using LLMs in the moderation workflow and suggest that\nfuture research should address the ethical considerations of utilising this\ntechnology.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "J.6"
    ],
    "primary_category": "cs.CL",
    "comment": "2024 Oz CHI conference",
    "pdf_url": "http://arxiv.org/pdf/2409.00940v1",
    "published_date": "2024-09-02 04:50:42 UTC",
    "updated_date": "2024-09-02 04:50:42 UTC"
  },
  {
    "arxiv_id": "2409.00923v1",
    "title": "Development of Occupancy Prediction Algorithm for Underground Parking Lots",
    "authors": [
      "Shijie Wang"
    ],
    "abstract": "The core objective of this study is to address the perception challenges\nfaced by autonomous driving in adverse environments like basements. Initially,\nthis paper commences with data collection in an underground garage. A simulated\nunderground garage model is established within the CARLA simulation\nenvironment, and SemanticKITTI format occupancy ground truth data is collected\nin this simulated setting. Subsequently, the study integrates a\nTransformer-based Occupancy Network model to complete the occupancy grid\nprediction task within this scenario. A comprehensive BEV perception framework\nis designed to enhance the accuracy of neural network models in dimly lit,\nchallenging autonomous driving environments. Finally, experiments validate the\naccuracy of the proposed solution's perception performance in basement\nscenarios. The proposed solution is tested on our self-constructed underground\ngarage dataset, SUSTech-COE-ParkingLot, yielding satisfactory results.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.00923v1",
    "published_date": "2024-09-02 03:31:49 UTC",
    "updated_date": "2024-09-02 03:31:49 UTC"
  },
  {
    "arxiv_id": "2409.00921v1",
    "title": "Statically Contextualizing Large Language Models with Typed Holes",
    "authors": [
      "Andrew Blinn",
      "Xiang Li",
      "June Hyung Kim",
      "Cyrus Omar"
    ],
    "abstract": "Large language models (LLMs) have reshaped the landscape of program\nsynthesis. However, contemporary LLM-based code completion systems often\nhallucinate broken code because they lack appropriate context, particularly\nwhen working with definitions not in the training data nor near the cursor.\nThis paper demonstrates that tight integration with the type and binding\nstructure of a language, as exposed by its language server, can address this\ncontextualization problem in a token-efficient manner. In short, we contend\nthat AIs need IDEs, too! In particular, we integrate LLM code generation into\nthe Hazel live program sketching environment. The Hazel Language Server\nidentifies the type and typing context of the hole being filled, even in the\npresence of errors, ensuring that a meaningful program sketch is always\navailable. This allows prompting with codebase-wide contextual information not\nlexically local to the cursor, nor necessarily in the same file, but that is\nlikely to be semantically local to the developer's goal. Completions\nsynthesized by the LLM are then iteratively refined via further dialog with the\nlanguage server. To evaluate these techniques, we introduce MVUBench, a dataset\nof model-view-update (MVU) web applications. These applications serve as\nchallenge problems due to their reliance on application-specific data\nstructures. We find that contextualization with type definitions is\nparticularly impactful. After introducing our ideas in the context of Hazel we\nduplicate our techniques and port MVUBench to TypeScript in order to validate\nthe applicability of these methods to higher-resource languages. Finally, we\noutline ChatLSP, a conservative extension to the Language Server Protocol (LSP)\nthat language servers can implement to expose capabilities that AI code\ncompletion systems of various designs can use to incorporate static context\nwhen generating prompts for an LLM.",
    "categories": [
      "cs.PL",
      "cs.AI",
      "cs.SE",
      "D.3.0"
    ],
    "primary_category": "cs.PL",
    "comment": "To appear at OOPSLA2024",
    "pdf_url": "http://arxiv.org/pdf/2409.00921v1",
    "published_date": "2024-09-02 03:29:00 UTC",
    "updated_date": "2024-09-02 03:29:00 UTC"
  },
  {
    "arxiv_id": "2409.00920v1",
    "title": "ToolACE: Winning the Points of LLM Function Calling",
    "authors": [
      "Weiwen Liu",
      "Xu Huang",
      "Xingshan Zeng",
      "Xinlong Hao",
      "Shuai Yu",
      "Dexun Li",
      "Shuai Wang",
      "Weinan Gan",
      "Zhengying Liu",
      "Yuanqing Yu",
      "Zezhong Wang",
      "Yuxian Wang",
      "Wu Ning",
      "Yutai Hou",
      "Bin Wang",
      "Chuhan Wu",
      "Xinzhi Wang",
      "Yong Liu",
      "Yasheng Wang",
      "Duyu Tang",
      "Dandan Tu",
      "Lifeng Shang",
      "Xin Jiang",
      "Ruiming Tang",
      "Defu Lian",
      "Qun Liu",
      "Enhong Chen"
    ],
    "abstract": "Function calling significantly extends the application boundary of large\nlanguage models, where high-quality and diverse training data is critical for\nunlocking this capability. However, real function-calling data is quite\nchallenging to collect and annotate, while synthetic data generated by existing\npipelines tends to lack coverage and accuracy. In this paper, we present\nToolACE, an automatic agentic pipeline designed to generate accurate, complex,\nand diverse tool-learning data. ToolACE leverages a novel self-evolution\nsynthesis process to curate a comprehensive API pool of 26,507 diverse APIs.\nDialogs are further generated through the interplay among multiple agents,\nguided by a formalized thinking process. To ensure data accuracy, we implement\na dual-layer verification system combining rule-based and model-based checks.\nWe demonstrate that models trained on our synthesized data, even with only 8B\nparameters, achieve state-of-the-art performance on the Berkeley\nFunction-Calling Leaderboard, rivaling the latest GPT-4 models. Our model and a\nsubset of the data are publicly available at https://huggingface.co/Team-ACE.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "21 pages, 22 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.00920v1",
    "published_date": "2024-09-02 03:19:56 UTC",
    "updated_date": "2024-09-02 03:19:56 UTC"
  },
  {
    "arxiv_id": "2409.00919v1",
    "title": "MMT-BERT: Chord-aware Symbolic Music Generation Based on Multitrack Music Transformer and MusicBERT",
    "authors": [
      "Jinlong Zhu",
      "Keigo Sakurai",
      "Ren Togo",
      "Takahiro Ogawa",
      "Miki Haseyama"
    ],
    "abstract": "We propose a novel symbolic music representation and Generative Adversarial\nNetwork (GAN) framework specially designed for symbolic multitrack music\ngeneration. The main theme of symbolic music generation primarily encompasses\nthe preprocessing of music data and the implementation of a deep learning\nframework. Current techniques dedicated to symbolic music generation generally\nencounter two significant challenges: training data's lack of information about\nchords and scales and the requirement of specially designed model architecture\nadapted to the unique format of symbolic music representation. In this paper,\nwe solve the above problems by introducing new symbolic music representation\nwith MusicLang chord analysis model. We propose our MMT-BERT architecture\nadapting to the representation. To build a robust multitrack music generator,\nwe fine-tune a pre-trained MusicBERT model to serve as the discriminator, and\nincorporate relativistic standard loss. This approach, supported by the\nin-depth understanding of symbolic music encoded within MusicBERT, fortifies\nthe consonance and humanity of music generated by our method. Experimental\nresults demonstrate the effectiveness of our approach which strictly follows\nthe state-of-the-art methods.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted to the 25th International Society for Music Information\n  Retrieval Conference (ISMIR 2024)",
    "pdf_url": "http://arxiv.org/pdf/2409.00919v1",
    "published_date": "2024-09-02 03:18:56 UTC",
    "updated_date": "2024-09-02 03:18:56 UTC"
  },
  {
    "arxiv_id": "2409.12926v1",
    "title": "MaskMol: Knowledge-guided Molecular Image Pre-Training Framework for Activity Cliffs",
    "authors": [
      "Zhixiang Cheng",
      "Hongxin Xiang",
      "Pengsen Ma",
      "Li Zeng",
      "Xin Jin",
      "Xixi Yang",
      "Jianxin Lin",
      "Yang Deng",
      "Bosheng Song",
      "Xinxin Feng",
      "Changhui Deng",
      "Xiangxiang Zeng"
    ],
    "abstract": "Activity cliffs, which refer to pairs of molecules that are structurally\nsimilar but show significant differences in their potency, can lead to model\nrepresentation collapse and make the model challenging to distinguish them. Our\nresearch indicates that as molecular similarity increases, graph-based methods\nstruggle to capture these nuances, whereas image-based approaches effectively\nretain the distinctions. Thus, we developed MaskMol, a knowledge-guided\nmolecular image self-supervised learning framework. MaskMol accurately learns\nthe representation of molecular images by considering multiple levels of\nmolecular knowledge, such as atoms, bonds, and substructures. By utilizing\npixel masking tasks, MaskMol extracts fine-grained information from molecular\nimages, overcoming the limitations of existing deep learning models in\nidentifying subtle structural changes. Experimental results demonstrate\nMaskMol's high accuracy and transferability in activity cliff estimation and\ncompound potency prediction across 20 different macromolecular targets,\noutperforming 25 state-of-the-art deep learning and machine learning\napproaches. Visualization analyses reveal MaskMol's high biological\ninterpretability in identifying activity cliff-relevant molecular\nsubstructures. Notably, through MaskMol, we identified candidate EP4 inhibitors\nthat could be used to treat tumors. This study not only raises awareness about\nactivity cliffs but also introduces a novel method for molecular image\nrepresentation learning and virtual screening, advancing drug discovery and\nproviding new insights into structure-activity relationships (SAR).",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "33 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.12926v1",
    "published_date": "2024-09-02 03:03:22 UTC",
    "updated_date": "2024-09-02 03:03:22 UTC"
  },
  {
    "arxiv_id": "2409.00909v1",
    "title": "ViRED: Prediction of Visual Relations in Engineering Drawings",
    "authors": [
      "Chao Gu",
      "Ke Lin",
      "Yiyang Luo",
      "Jiahui Hou",
      "Xiang-Yang Li"
    ],
    "abstract": "To accurately understand engineering drawings, it is essential to establish\nthe correspondence between images and their description tables within the\ndrawings. Existing document understanding methods predominantly focus on text\nas the main modality, which is not suitable for documents containing\nsubstantial image information. In the field of visual relation detection, the\nstructure of the task inherently limits its capacity to assess relationships\namong all entity pairs in the drawings. To address this issue, we propose a\nvision-based relation detection model, named ViRED, to identify the\nassociations between tables and circuits in electrical engineering drawings.\nOur model mainly consists of three parts: a vision encoder, an object encoder,\nand a relation decoder. We implement ViRED using PyTorch to evaluate its\nperformance. To validate the efficacy of ViRED, we conduct a series of\nexperiments. The experimental results indicate that, within the engineering\ndrawing dataset, our approach attained an accuracy of 96\\% in the task of\nrelation prediction, marking a substantial improvement over existing\nmethodologies. The results also show that ViRED can inference at a fast speed\neven when there are numerous objects in a single engineering drawing.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.00909v1",
    "published_date": "2024-09-02 02:42:34 UTC",
    "updated_date": "2024-09-02 02:42:34 UTC"
  },
  {
    "arxiv_id": "2409.00904v1",
    "title": "Multi-scale Temporal Fusion Transformer for Incomplete Vehicle Trajectory Prediction",
    "authors": [
      "Zhanwen Liu",
      "Chao Li",
      "Yang Wang",
      "Nan Yang",
      "Xing Fan",
      "Jiaqi Ma",
      "Xiangmo Zhao"
    ],
    "abstract": "Motion prediction plays an essential role in autonomous driving systems,\nenabling autonomous vehicles to achieve more accurate local-path planning and\ndriving decisions based on predictions of the surrounding vehicles. However,\nexisting methods neglect the potential missing values caused by object\nocclusion, perception failures, etc., which inevitably degrades the trajectory\nprediction performance in real traffic scenarios. To address this limitation,\nwe propose a novel end-to-end framework for incomplete vehicle trajectory\nprediction, named Multi-scale Temporal Fusion Transformer (MTFT), which\nconsists of the Multi-scale Attention Head (MAH) and the Continuity\nRepresentation-guided Multi-scale Fusion (CRMF) module. Specifically, the MAH\nleverages the multi-head attention mechanism to parallelly capture multi-scale\nmotion representation of trajectory from different temporal granularities, thus\nmitigating the adverse effect of missing values on prediction. Furthermore, the\nmulti-scale motion representation is input into the CRMF module for multi-scale\nfusion to obtain the robust temporal feature of the vehicle. During the fusion\nprocess, the continuity representation of vehicle motion is first extracted\nacross time steps to guide the fusion, ensuring that the resulting temporal\nfeature incorporates both detailed information and the overall trend of vehicle\nmotion, which facilitates the accurate decoding of future trajectory that is\nconsistent with the vehicle's motion trend. We evaluate the proposed model on\nfour datasets derived from highway and urban traffic scenarios. The\nexperimental results demonstrate its superior performance in the incomplete\nvehicle trajectory prediction task compared with state-of-the-art models, e.g.,\na comprehensive performance improvement of more than 39% on the HighD dataset.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.00904v1",
    "published_date": "2024-09-02 02:36:18 UTC",
    "updated_date": "2024-09-02 02:36:18 UTC"
  },
  {
    "arxiv_id": "2409.00899v2",
    "title": "MarsCode Agent: AI-native Automated Bug Fixing",
    "authors": [
      "Yizhou Liu",
      "Pengfei Gao",
      "Xinchen Wang",
      "Jie Liu",
      "Yexuan Shi",
      "Zhao Zhang",
      "Chao Peng"
    ],
    "abstract": "Recent advances in large language models (LLMs) have shown significant\npotential to automate various software development tasks, including code\ncompletion, test generation, and bug fixing. However, the application of LLMs\nfor automated bug fixing remains challenging due to the complexity and\ndiversity of real-world software systems. In this paper, we introduce MarsCode\nAgent, a novel framework that leverages LLMs to automatically identify and\nrepair bugs in software code. MarsCode Agent combines the power of LLMs with\nadvanced code analysis techniques to accurately localize faults and generate\npatches. Our approach follows a systematic process of planning, bug\nreproduction, fault localization, candidate patch generation, and validation to\nensure high-quality bug fixes. We evaluated MarsCode Agent on SWE-bench, a\ncomprehensive benchmark of real-world software projects, and our results show\nthat MarsCode Agent achieves a high success rate in bug fixing compared to most\nof the existing automated approaches.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "Yizhou Liu and Pengfei Gao contributed equally and the order is\n  determined by rolling the dice. Chao Peng is the corresponding author",
    "pdf_url": "http://arxiv.org/pdf/2409.00899v2",
    "published_date": "2024-09-02 02:24:38 UTC",
    "updated_date": "2024-09-04 06:19:08 UTC"
  },
  {
    "arxiv_id": "2409.00887v1",
    "title": "User-Specific Dialogue Generation with User Profile-Aware Pre-Training Model and Parameter-Efficient Fine-Tuning",
    "authors": [
      "Atsushi Otsuka",
      "Kazuya Matsuo",
      "Ryo Ishii",
      "Narichika Nomoto",
      "Hiroaki Sugiyama"
    ],
    "abstract": "This paper addresses user-specific dialogs. In contrast to previous research\non personalized dialogue focused on achieving virtual user dialogue as defined\nby persona descriptions, user-specific dialogue aims to reproduce real-user\ndialogue beyond persona-based dialogue. Fine-tuning using the target user's\ndialogue history is an efficient learning method for a user-specific model.\nHowever, it is prone to overfitting and model destruction due to the small\namount of data. Therefore, we propose a learning method for user-specific\nmodels by combining parameter-efficient fine-tuning with a pre-trained dialogue\nmodel that includes user profiles. Parameter-efficient fine-tuning adds a small\nnumber of parameters to the entire model, so even small amounts of training\ndata can be trained efficiently and are robust to model destruction. In\naddition, the pre-trained model, which is learned by adding simple prompts for\nautomatically inferred user profiles, can generate speech with enhanced\nknowledge of the user's profile, even when there is little training data during\nfine-tuning. In experiments, we compared the proposed model with\nlarge-language-model utterance generation using prompts containing users'\npersonal information. Experiments reproducing real users' utterances revealed\nthat the proposed model can generate utterances with higher reproducibility\nthan the compared methods, even with a small model.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.00887v1",
    "published_date": "2024-09-02 01:30:40 UTC",
    "updated_date": "2024-09-02 01:30:40 UTC"
  },
  {
    "arxiv_id": "2409.00879v1",
    "title": "Beyond Parameter Count: Implicit Bias in Soft Mixture of Experts",
    "authors": [
      "Youngseog Chung",
      "Dhruv Malik",
      "Jeff Schneider",
      "Yuanzhi Li",
      "Aarti Singh"
    ],
    "abstract": "The traditional viewpoint on Sparse Mixture of Experts (MoE) models is that\ninstead of training a single large expert, which is computationally expensive,\nwe can train many small experts. The hope is that if the total parameter count\nof the small experts equals that of the singular large expert, then we retain\nthe representation power of the large expert while gaining computational\ntractability and promoting expert specialization. The recently introduced Soft\nMoE replaces the Sparse MoE's discrete routing mechanism with a differentiable\ngating function that smoothly mixes tokens. While this smooth gating function\nsuccessfully mitigates the various training instabilities associated with\nSparse MoE, it is unclear whether it induces implicit biases that affect Soft\nMoE's representation power or potential for expert specialization. We prove\nthat Soft MoE with a single arbitrarily powerful expert cannot represent simple\nconvex functions. This justifies that Soft MoE's success cannot be explained by\nthe traditional viewpoint of many small experts collectively mimicking the\nrepresentation power of a single large expert, and that multiple experts are\nactually necessary to achieve good representation power (even for a fixed total\nparameter count). Continuing along this line of investigation, we introduce a\nnotion of expert specialization for Soft MoE, and while varying the number of\nexperts yet fixing the total parameter count, we consider the following\n(computationally intractable) task. Given any input, how can we discover the\nexpert subset that is specialized to predict this input's label? We empirically\nshow that when there are many small experts, the architecture is implicitly\nbiased in a fashion that allows us to efficiently approximate the specialized\nexpert subset. Our method can be easily implemented to potentially reduce\ncomputation during inference.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "21 pages, 5 figures, 13 tables",
    "pdf_url": "http://arxiv.org/pdf/2409.00879v1",
    "published_date": "2024-09-02 00:39:00 UTC",
    "updated_date": "2024-09-02 00:39:00 UTC"
  }
]
[
  {
    "arxiv_id": "2501.10900v1",
    "title": "A Generative Security Application Engineering Curriculum",
    "authors": [
      "Wu-chang Feng",
      "David Baker-Robinson"
    ],
    "abstract": "Generative AI and large language models (LLMs) are transforming security by\nautomating many tasks being performed manually. With such automation changing\nthe practice of security as we know it, it is imperative that we prepare future\nstudents for the technology landscape they will ultimately face. Towards this\nend, we describe an initial curriculum and course that attempts to show\nstudents how to apply generative AI in order to solve problems in security. By\nrefocusing security education and training on aspects uniquely suited for\nhumans and showing students how to leverage automation for the rest, we believe\nwe can better align security education practices with generative AI as it\nevolves.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "11 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.10900v1",
    "published_date": "2025-01-18 23:17:34 UTC",
    "updated_date": "2025-01-18 23:17:34 UTC"
  },
  {
    "arxiv_id": "2501.10895v1",
    "title": "Classical and Deep Reinforcement Learning Inventory Control Policies for Pharmaceutical Supply Chains with Perishability and Non-Stationarity",
    "authors": [
      "Francesco Stranieri",
      "Chaaben Kouki",
      "Willem van Jaarsveld",
      "Fabio Stella"
    ],
    "abstract": "We study inventory control policies for pharmaceutical supply chains,\naddressing challenges such as perishability, yield uncertainty, and\nnon-stationary demand, combined with batching constraints, lead times, and lost\nsales. Collaborating with Bristol-Myers Squibb (BMS), we develop a realistic\ncase study incorporating these factors and benchmark three\npolicies--order-up-to (OUT), projected inventory level (PIL), and deep\nreinforcement learning (DRL) using the proximal policy optimization (PPO)\nalgorithm--against a BMS baseline based on human expertise. We derive and\nvalidate bounds-based procedures for optimizing OUT and PIL policy parameters\nand propose a methodology for estimating projected inventory levels, which are\nalso integrated into the DRL policy with demand forecasts to improve\ndecision-making under non-stationarity. Compared to a human-driven policy,\nwhich avoids lost sales through higher holding costs, all three implemented\npolicies achieve lower average costs but exhibit greater cost variability.\nWhile PIL demonstrates robust and consistent performance, OUT struggles under\nhigh lost sales costs, and PPO excels in complex and variable scenarios but\nrequires significant computational effort. The findings suggest that while DRL\nshows potential, it does not outperform classical policies in all numerical\nexperiments, highlighting 1) the need to integrate diverse policies to manage\npharmaceutical challenges effectively, based on the current state-of-the-art,\nand 2) that practical problems in this domain seem to lack a single policy\nclass that yields universally acceptable performance.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "math.OC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.10895v1",
    "published_date": "2025-01-18 22:40:33 UTC",
    "updated_date": "2025-01-18 22:40:33 UTC"
  },
  {
    "arxiv_id": "2501.10893v1",
    "title": "Learn-by-interact: A Data-Centric Framework for Self-Adaptive Agents in Realistic Environments",
    "authors": [
      "Hongjin Su",
      "Ruoxi Sun",
      "Jinsung Yoon",
      "Pengcheng Yin",
      "Tao Yu",
      "Sercan Ö. Arık"
    ],
    "abstract": "Autonomous agents powered by large language models (LLMs) have the potential\nto enhance human capabilities, assisting with digital tasks from sending emails\nto performing data analysis. The abilities of existing LLMs at such tasks are\noften hindered by the lack of high-quality agent data from the corresponding\nenvironments they interact with. We propose Learn-by-interact, a data-centric\nframework to adapt LLM agents to any given environments without human\nannotations. Learn-by-interact synthesizes trajectories of agent-environment\ninteractions based on documentations, and constructs instructions by\nsummarizing or abstracting the interaction histories, a process called backward\nconstruction. We assess the quality of our synthetic data by using them in both\ntraining-based scenarios and training-free in-context learning (ICL), where we\ncraft innovative retrieval approaches optimized for agents. Extensive\nexperiments on SWE-bench, WebArena, OSWorld and Spider2-V spanning across\nrealistic coding, web, and desktop environments show the effectiveness of\nLearn-by-interact in various downstream agentic tasks -- baseline results are\nimproved by up to 12.2\\% for ICL with Claude-3.5 and 19.5\\% for training with\nCodestral-22B. We further demonstrate the critical role of backward\nconstruction, which provides up to 14.0\\% improvement for training. Our\nablation studies demonstrate the efficiency provided by our synthesized data in\nICL and the superiority of our retrieval pipeline over alternative approaches\nlike conventional retrieval-augmented generation (RAG). We expect that\nLearn-by-interact will serve as a foundation for agent data synthesis as LLMs\nare increasingly deployed at real-world environments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.10893v1",
    "published_date": "2025-01-18 22:34:41 UTC",
    "updated_date": "2025-01-18 22:34:41 UTC"
  },
  {
    "arxiv_id": "2501.10891v2",
    "title": "OpenEarthMap-SAR: A Benchmark Synthetic Aperture Radar Dataset for Global High-Resolution Land Cover Mapping",
    "authors": [
      "Junshi Xia",
      "Hongruixuan Chen",
      "Clifford Broni-Bediako",
      "Yimin Wei",
      "Jian Song",
      "Naoto Yokoya"
    ],
    "abstract": "High-resolution land cover mapping plays a crucial role in addressing a wide\nrange of global challenges, including urban planning, environmental monitoring,\ndisaster response, and sustainable development. However, creating accurate,\nlarge-scale land cover datasets remains a significant challenge due to the\ninherent complexities of geospatial data, such as diverse terrain, varying\nsensor modalities, and atmospheric conditions. Synthetic Aperture Radar (SAR)\nimagery, with its ability to penetrate clouds and capture data in all-weather,\nday-and-night conditions, offers unique advantages for land cover mapping.\nDespite these strengths, the lack of benchmark datasets tailored for SAR\nimagery has limited the development of robust models specifically designed for\nthis data modality. To bridge this gap and facilitate advancements in SAR-based\ngeospatial analysis, we introduce OpenEarthMap-SAR, a benchmark SAR dataset,\nfor global high-resolution land cover mapping. OpenEarthMap-SAR consists of 1.5\nmillion segments of 5033 aerial and satellite images with the size of\n1024$\\times$1024 pixels, covering 35 regions from Japan, France, and the USA,\nwith partially manually annotated and fully pseudo 8-class land cover labels at\na ground sampling distance of 0.15--0.5 m. We evaluated the performance of\nstate-of-the-art methods for semantic segmentation and present challenging\nproblem settings suitable for further technical development. The dataset also\nserves the official dataset for IEEE GRSS Data Fusion Contest Track I. The\ndataset has been made publicly available at\nhttps://zenodo.org/records/14622048.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "eess.SP"
    ],
    "primary_category": "eess.IV",
    "comment": "8 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.10891v2",
    "published_date": "2025-01-18 22:30:27 UTC",
    "updated_date": "2025-01-22 02:53:36 UTC"
  },
  {
    "arxiv_id": "2501.10868v3",
    "title": "JSONSchemaBench: A Rigorous Benchmark of Structured Outputs for Language Models",
    "authors": [
      "Saibo Geng",
      "Hudson Cooper",
      "Michał Moskal",
      "Samuel Jenkins",
      "Julian Berman",
      "Nathan Ranchin",
      "Robert West",
      "Eric Horvitz",
      "Harsha Nori"
    ],
    "abstract": "Reliably generating structured outputs has become a critical capability for\nmodern language model (LM) applications. Constrained decoding has emerged as\nthe dominant technology across sectors for enforcing structured outputs during\ngeneration. Despite its growing adoption, little has been done with the\nsystematic evaluation of the behaviors and performance of constrained decoding.\nConstrained decoding frameworks have standardized around JSON Schema as a\nstructured data format, with most uses guaranteeing constraint compliance given\na schema. However, there is poor understanding of the effectiveness of the\nmethods in practice. We present an evaluation framework to assess constrained\ndecoding approaches across three critical dimensions: efficiency in generating\nconstraint-compliant outputs, coverage of diverse constraint types, and quality\nof the generated outputs. To facilitate this evaluation, we introduce\nJSONSchemaBench, a benchmark for constrained decoding comprising 10K real-world\nJSON schemas that encompass a wide range of constraints with varying\ncomplexity. We pair the benchmark with the existing official JSON Schema Test\nSuite and evaluate six state-of-the-art constrained decoding frameworks,\nincluding Guidance, Outlines, Llamacpp, XGrammar, OpenAI, and Gemini. Through\nextensive experiments, we gain insights into the capabilities and limitations\nof constrained decoding on structured generation with real-world JSON schemas.\nOur work provides actionable insights for improving constrained decoding\nframeworks and structured generation tasks, setting a new standard for\nevaluating constrained decoding and structured generation. We release\nJSONSchemaBench at https://github.com/guidance-ai/jsonschemabench",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.10868v3",
    "published_date": "2025-01-18 20:26:00 UTC",
    "updated_date": "2025-02-27 15:37:21 UTC"
  },
  {
    "arxiv_id": "2501.16352v1",
    "title": "Mixture of Experts (MoE): A Big Data Perspective",
    "authors": [
      "Wensheng Gan",
      "Zhenyao Ning",
      "Zhenlian Qi",
      "Philip S. Yu"
    ],
    "abstract": "As the era of big data arrives, traditional artificial intelligence\nalgorithms have difficulty processing the demands of massive and diverse data.\nMixture of experts (MoE) has shown excellent performance and broad application\nprospects. This paper provides an in-depth review and analysis of the latest\nprogress in this field from multiple perspectives, including the basic\nprinciples, algorithmic models, key technical challenges, and application\npractices of MoE. First, we introduce the basic concept of MoE and its core\nidea and elaborate on its advantages over traditional single models. Then, we\ndiscuss the basic architecture of MoE and its main components, including the\ngating network, expert networks, and learning algorithms. Next, we review the\napplications of MoE in addressing key technical issues in big data. For each\nchallenge, we provide specific MoE solutions and their innovations.\nFurthermore, we summarize the typical use cases of MoE in various application\ndomains. This fully demonstrates the powerful capability of MoE in big data\nprocessing. We also analyze the advantages of MoE in big data environments.\nFinally, we explore the future development trends of MoE. We believe that MoE\nwill become an important paradigm of artificial intelligence in the era of big\ndata. In summary, this paper systematically elaborates on the principles,\ntechniques, and applications of MoE in big data processing, providing\ntheoretical and practical references to further promote the application of MoE\nin real scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint. 5 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2501.16352v1",
    "published_date": "2025-01-18 20:17:31 UTC",
    "updated_date": "2025-01-18 20:17:31 UTC"
  },
  {
    "arxiv_id": "2501.10861v1",
    "title": "Dynamic Continual Learning: Harnessing Parameter Uncertainty for Improved Network Adaptation",
    "authors": [
      "Christopher Angelini",
      "Nidhal Bouaynaya"
    ],
    "abstract": "When fine-tuning Deep Neural Networks (DNNs) to new data, DNNs are prone to\noverwriting network parameters required for task-specific functionality on\npreviously learned tasks, resulting in a loss of performance on those tasks. We\npropose using parameter-based uncertainty to determine which parameters are\nrelevant to a network's learned function and regularize training to prevent\nchange in these important parameters. We approach this regularization in two\nways: (1), we constrain critical parameters from significant changes by\nassociating more critical parameters with lower learning rates, thereby\nlimiting alterations in those parameters; (2), important parameters are\nrestricted from change by imposing a higher regularization weighting, causing\nparameters to revert to their states prior to the learning of subsequent tasks.\nWe leverage a Bayesian Moment Propagation framework which learns network\nparameters concurrently with their associated uncertainties while allowing each\nparameter to contribute uncertainty to the network's predictive distribution,\navoiding the pitfalls of existing sampling-based methods. The proposed approach\nis evaluated for common sequential benchmark datasets and compared to existing\npublished approaches from the Continual Learning community. Ultimately, we show\nimproved Continual Learning performance for Average Test Accuracy and Backward\nTransfer metrics compared to sampling-based methods and other\nnon-uncertainty-based approaches.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.10861v1",
    "published_date": "2025-01-18 19:58:53 UTC",
    "updated_date": "2025-01-18 19:58:53 UTC"
  },
  {
    "arxiv_id": "2501.10860v2",
    "title": "Zero-shot and Few-shot Learning with Instruction-following LLMs for Claim Matching in Automated Fact-checking",
    "authors": [
      "Dina Pisarevskaya",
      "Arkaitz Zubiaga"
    ],
    "abstract": "The claim matching (CM) task can benefit an automated fact-checking pipeline\nby putting together claims that can be resolved with the same fact-check. In\nthis work, we are the first to explore zero-shot and few-shot learning\napproaches to the task. We consider CM as a binary classification task and\nexperiment with a set of instruction-following large language models\n(GPT-3.5-turbo, Gemini-1.5-flash, Mistral-7B-Instruct, and\nLlama-3-8B-Instruct), investigating prompt templates. We introduce a new CM\ndataset, ClaimMatch, which will be released upon acceptance. We put LLMs to the\ntest in the CM task and find that it can be tackled by leveraging more mature\nyet similar tasks such as natural language inference or paraphrase detection.\nWe also propose a pipeline for CM, which we evaluate on texts of different\nlengths.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Published at the 31st International Conference on Computational\n  Linguistics (COLING 2025). Compared to the conference version of the paper,\n  the dataset link is added here & 2 minor typos fixed",
    "pdf_url": "http://arxiv.org/pdf/2501.10860v2",
    "published_date": "2025-01-18 19:57:54 UTC",
    "updated_date": "2025-02-28 22:23:54 UTC"
  },
  {
    "arxiv_id": "2501.10858v1",
    "title": "Reliable Text-to-SQL with Adaptive Abstention",
    "authors": [
      "Kaiwen Chen",
      "Yueting Chen",
      "Xiaohui Yu",
      "Nick Koudas"
    ],
    "abstract": "Large language models (LLMs) have revolutionized natural language interfaces\nfor databases, particularly in text-to-SQL conversion. However, current\napproaches often generate unreliable outputs when faced with ambiguity or\ninsufficient context. We present Reliable Text-to-SQL (RTS), a novel framework\nthat enhances query generation reliability by incorporating abstention and\nhuman-in-the-loop mechanisms. RTS focuses on the critical schema linking phase,\nwhich aims to identify the key database elements needed for generating SQL\nqueries. It autonomously detects potential errors during the answer generation\nprocess and responds by either abstaining or engaging in user interaction. A\nvital component of RTS is the Branching Point Prediction (BPP) which utilizes\nstatistical conformal techniques on the hidden layers of the LLM model for\nschema linking, providing probabilistic guarantees on schema linking accuracy.\nWe validate our approach through comprehensive experiments on the BIRD\nbenchmark, demonstrating significant improvements in robustness and\nreliability. Our findings highlight the potential of combining transparent-box\nLLMs with human-in-the-loop processes to create more robust natural language\ninterfaces for databases. For the BIRD benchmark, our approach achieves\nnear-perfect schema linking accuracy, autonomously involving a human when\nneeded. Combined with query generation, we demonstrate that near-perfect schema\nlinking and a small query generation model can almost match SOTA accuracy\nachieved with a model orders of magnitude larger than the one we use.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.10858v1",
    "published_date": "2025-01-18 19:36:37 UTC",
    "updated_date": "2025-01-18 19:36:37 UTC"
  },
  {
    "arxiv_id": "2501.10848v1",
    "title": "Fake Advertisements Detection Using Automated Multimodal Learning: A Case Study for Vietnamese Real Estate Data",
    "authors": [
      "Duy Nguyen",
      "Trung T. Nguyen",
      "Cuong V. Nguyen"
    ],
    "abstract": "The popularity of e-commerce has given rise to fake advertisements that can\nexpose users to financial and data risks while damaging the reputation of these\ne-commerce platforms. For these reasons, detecting and removing such fake\nadvertisements are important for the success of e-commerce websites. In this\npaper, we propose FADAML, a novel end-to-end machine learning system to detect\nand filter out fake online advertisements. Our system combines techniques in\nmultimodal machine learning and automated machine learning to achieve a high\ndetection rate. As a case study, we apply FADAML to detect fake advertisements\non popular Vietnamese real estate websites. Our experiments show that we can\nachieve 91.5% detection accuracy, which significantly outperforms three\ndifferent state-of-the-art fake news detection systems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.10848v1",
    "published_date": "2025-01-18 18:48:06 UTC",
    "updated_date": "2025-01-18 18:48:06 UTC"
  },
  {
    "arxiv_id": "2501.10841v1",
    "title": "Practical and Ready-to-Use Methodology to Assess the re-identification Risk in Anonymized Datasets",
    "authors": [
      "Louis-Philippe Sondeck",
      "Maryline Laurent"
    ],
    "abstract": "To prove that a dataset is sufficiently anonymized, many privacy policies\nsuggest that a re-identification risk assessment be performed, but do not\nprovide a precise methodology for doing so, leaving the industry alone with the\nproblem. This paper proposes a practical and ready-to-use methodology for\nre-identification risk assessment, the originality of which is manifold: (1) it\nis the first to follow well-known risk analysis methods (e.g. EBIOS) that have\nbeen used in the cybersecurity field for years, which consider not only the\nability to perform an attack, but also the impact such an attack can have on an\nindividual; (2) it is the first to qualify attributes and values of attributes\nwith e.g. degree of exposure, as known real-world attacks mainly target certain\ntypes of attributes and not others.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.10841v1",
    "published_date": "2025-01-18 18:22:27 UTC",
    "updated_date": "2025-01-18 18:22:27 UTC"
  },
  {
    "arxiv_id": "2501.16350v1",
    "title": "A Method for Multi-Hop Question Answering on Persian Knowledge Graph",
    "authors": [
      "Arash Ghafouri",
      "Mahdi Firouzmandi",
      "Hasan Naderi"
    ],
    "abstract": "Question answering systems are the latest evolution in information retrieval\ntechnology, designed to accept complex queries in natural language and provide\naccurate answers using both unstructured and structured knowledge sources.\nKnowledge Graph Question Answering (KGQA) systems fulfill users' information\nneeds by utilizing structured data, representing a vast number of facts as a\ngraph. However, despite significant advancements, major challenges persist in\nanswering multi-hop complex questions, particularly in Persian. One of the main\nchallenges is the accurate understanding and transformation of these multi-hop\ncomplex questions into semantically equivalent SPARQL queries, which allows for\nprecise answer retrieval from knowledge graphs. In this study, to address this\nissue, a dataset of 5,600 Persian multi-hop complex questions was developed,\nalong with their decomposed forms based on the semantic representation of the\nquestions. Following this, Persian language models were trained using this\ndataset, and an architecture was proposed for answering complex questions using\na Persian knowledge graph. Finally, the proposed method was evaluated against\nsimilar systems on the PeCoQ dataset. The results demonstrated the superiority\nof our approach, with an improvement of 12.57% in F1-score and 12.06% in\naccuracy compared to the best comparable method.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.16350v1",
    "published_date": "2025-01-18 18:11:29 UTC",
    "updated_date": "2025-01-18 18:11:29 UTC"
  },
  {
    "arxiv_id": "2501.10836v2",
    "title": "BAP v2: An Enhanced Task Framework for Instruction Following in Minecraft Dialogues",
    "authors": [
      "Prashant Jayannavar",
      "Liliang Ren",
      "Marisa Hudspeth",
      "Charlotte Lambert",
      "Ariel Cordes",
      "Elizabeth Kaplan",
      "Anjali Narayan-Chen",
      "Julia Hockenmaier"
    ],
    "abstract": "Interactive agents capable of understanding and executing instructions in the\nphysical world have long been a central goal in AI research. The Minecraft\nCollaborative Building Task (MCBT) provides one such setting to work towards\nthis goal (Narayan-Chen, Jayannavar, and Hockenmaier 2019). It is a two-player\ngame in which an Architect (A) instructs a Builder (B) to construct a target\nstructure in a simulated Blocks World Environment. We focus on the challenging\nBuilder Action Prediction (BAP) subtask of predicting correct action sequences\nin a given multimodal game context with limited training data (Jayannavar,\nNarayan-Chen, and Hockenmaier 2020). We take a closer look at evaluation and\ndata for the BAP task, discovering key challenges and making significant\nimprovements on both fronts to propose BAP v2, an upgraded version of the task.\nThis will allow future work to make more efficient and meaningful progress on\nit. It comprises of: (1) an enhanced evaluation benchmark that includes a\ncleaner test set and fairer, more insightful metrics, and (2) additional\nsynthetic training data generated from novel Minecraft dialogue and target\nstructure simulators emulating the MCBT. We show that the synthetic data can be\nused to train more performant and robust neural models even with relatively\nsimple training methods. Looking ahead, such data could also be crucial for\ntraining more sophisticated, data-hungry deep transformer models and\ntraining/fine-tuning increasingly large LLMs. Although modeling is not the\nprimary focus of this work, we also illustrate the impact of our data and\ntraining methodologies on a simple LLM- and transformer-based model, thus\nvalidating the robustness of our approach, and setting the stage for more\nadvanced architectures and LLMs going forward.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.10836v2",
    "published_date": "2025-01-18 18:06:03 UTC",
    "updated_date": "2025-02-23 02:54:47 UTC"
  },
  {
    "arxiv_id": "2501.10834v1",
    "title": "Visual RAG: Expanding MLLM visual knowledge without fine-tuning",
    "authors": [
      "Mirco Bonomo",
      "Simone Bianco"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have achieved notable performance in\ncomputer vision tasks that require reasoning across visual and textual\nmodalities, yet their capabilities are limited to their pre-trained data,\nrequiring extensive fine-tuning for updates. Recent researches have explored\nthe use of In-Context Learning (ICL) to overcome these challenges by providing\na set of demonstrating examples as context to augment MLLMs performance in\nseveral tasks, showing that many-shot ICL leads to substantial improvements\ncompared to few-shot ICL. However, the reliance on numerous demonstrating\nexamples and the limited MLLMs context windows presents significant obstacles.\nThis paper aims to address these challenges by introducing a novel approach,\nVisual RAG, that synergically combines the MLLMs capability to learn from the\ncontext, with a retrieval mechanism. The crux of this approach is to ensure to\naugment the MLLM knowledge by selecting only the most relevant demonstrating\nexamples for the query, pushing it to learn by analogy. In this way, relying on\nthe new information provided dynamically during inference time, the resulting\nsystem is not limited to the knowledge extracted from the training data, but\ncan be updated rapidly and easily without fine-tuning. Furthermore, this\ngreatly reduces the computational costs for improving the model image\nclassification performance, and augments the model knowledge to new visual\ndomains and tasks it was not trained for. Extensive experiments on eight\ndifferent datasets in the state of the art spanning several domains and image\nclassification tasks show that the proposed Visual RAG, compared to the most\nrecent state of the art (i.e., many-shot ICL), is able to obtain an accuracy\nthat is very close or even higher (approx. +2% improvement on average) while\nusing a much smaller set of demonstrating examples (approx. only 23% on\naverage).",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.10834v1",
    "published_date": "2025-01-18 17:43:05 UTC",
    "updated_date": "2025-01-18 17:43:05 UTC"
  },
  {
    "arxiv_id": "2501.10822v1",
    "title": "Addressing Multilabel Imbalance with an Efficiency-Focused Approach Using Diffusion Model-Generated Synthetic Samples",
    "authors": [
      "Francisco Charte",
      "Miguel Ángel Dávila",
      "María Dolores Pérez-Godoy",
      "María José del Jesus"
    ],
    "abstract": "Predictive models trained on imbalanced data tend to produce biased results.\nThis problem is exacerbated when there is not just one output label, but a set\nof them. This is the case for multilabel learning (MLL) algorithms used to\nclassify patterns, rank labels, or learn the distribution of outputs. Many\nsolutions have been proposed in the literature. The one that can be applied\nuniversally, independent of the algorithm used to build the model, is data\nresampling. The generation of new instances associated with minority labels, so\nthat empty areas of the feature space are filled, helps to improve the obtained\nmodels. The quality of these new instances depends on the algorithm used to\ngenerate them. In this paper, a diffusion model tailored to produce new\ninstances for MLL data, called MLDM (\\textit{MultiLabel Diffusion Model}), is\nproposed. Diffusion models have been mainly used to generate artificial images\nand videos. Our proposed MLDM is based on this type of models. The experiments\nconducted compare MLDM with several other MLL resampling algorithms. The\nresults show that MLDM is competitive while it improves efficiency.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "22 pages, 8 figures, 10 tables",
    "pdf_url": "http://arxiv.org/pdf/2501.10822v1",
    "published_date": "2025-01-18 16:56:50 UTC",
    "updated_date": "2025-01-18 16:56:50 UTC"
  },
  {
    "arxiv_id": "2501.16349v1",
    "title": "Risk-Informed Diffusion Transformer for Long-Tail Trajectory Prediction in the Crash Scenario",
    "authors": [
      "Junlan Chen",
      "Pei Liu",
      "Zihao Zhang",
      "Hongyi Zhao",
      "Yufei Ji",
      "Ziyuan Pu"
    ],
    "abstract": "Trajectory prediction methods have been widely applied in autonomous driving\ntechnologies. Although the overall performance accuracy of trajectory\nprediction is relatively high, the lack of trajectory data in critical\nscenarios in the training data leads to the long-tail phenomenon. Normally, the\ntrajectories of the tail data are more critical and more difficult to predict\nand may include rare scenarios such as crashes. To solve this problem, we\nextracted the trajectory data from real-world crash scenarios, which contain\nmore long-tail data. Meanwhile, based on the trajectory data in this scenario,\nwe integrated graph-based risk information and diffusion with transformer and\nproposed the Risk-Informed Diffusion Transformer (RI-DiT) trajectory prediction\nmethod. Extensive experiments were conducted on trajectory data in the\nreal-world crash scenario, and the results show that the algorithm we proposed\nhas good performance. When predicting the data of the tail 10\\% (Top 10\\%), the\nminADE and minFDE indicators are 0.016/2.667 m. At the same time, we showed the\ntrajectory conditions of different long-tail distributions. The distribution of\ntrajectory data is closer to the tail, the less smooth the trajectory is.\nThrough the trajectory data in real-world crash scenarios, Our work expands the\nmethods to overcome the long-tail challenges in trajectory prediction. Our\nmethod, RI-DiT, integrates inverse time to collision (ITTC) and the feature of\ntraffic flow, which can predict long-tail trajectories more accurately and\nimprove the safety of autonomous driving systems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.16349v1",
    "published_date": "2025-01-18 16:47:29 UTC",
    "updated_date": "2025-01-18 16:47:29 UTC"
  },
  {
    "arxiv_id": "2501.10814v2",
    "title": "No More Sliding Window: Efficient 3D Medical Image Segmentation with Differentiable Top-k Patch Sampling",
    "authors": [
      "Young Seok Jeon",
      "Hongfei Yang",
      "Huazhu Fu",
      "Mengling Feng"
    ],
    "abstract": "3D models surpass 2D models in CT/MRI segmentation by effectively capturing\ninter-slice relationships. However, the added depth dimension substantially\nincreases memory consumption. While patch-based training alleviates memory\nconstraints, it significantly slows down the inference speed due to the sliding\nwindow (SW) approach. We propose No-More-Sliding-Window (NMSW), a novel\nend-to-end trainable framework that enhances the efficiency of generic 3D\nsegmentation backbone during an inference step by eliminating the need for SW.\nNMSW employs a differentiable Top-k module to selectively sample only the most\nrelevant patches, thereby minimizing redundant computations. When patch-level\npredictions are insufficient, the framework intelligently leverages coarse\nglobal predictions to refine results. Evaluated across 3 tasks using 3\nsegmentation backbones, NMSW achieves competitive accuracy compared to SW\ninference while significantly reducing computational complexity by 91% (88.0 to\n8.00 TMACs). Moreover, it delivers a 9.1x faster inference on the H100 GPU\n(99.0 to 8.3 sec) and a 11.1x faster inference on the Xeon Gold CPU (2110 to\n189 sec). NMSW is model-agnostic, further boosting efficiency when integrated\nwith any existing efficient segmentation backbones.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.10814v2",
    "published_date": "2025-01-18 16:23:09 UTC",
    "updated_date": "2025-03-06 11:05:23 UTC"
  },
  {
    "arxiv_id": "2501.10812v1",
    "title": "Graph Coloring to Reduce Computation Time in Prioritized Planning",
    "authors": [
      "Patrick Scheffe",
      "Julius Kahle",
      "Bassam Alrifaee"
    ],
    "abstract": "Distributing computations among agents in large networks reduces\ncomputational effort in multi-agent path finding (MAPF). One distribution\nstrategy is prioritized planning (PP). In PP, we couple and prioritize\ninteracting agents to achieve a desired behavior across all agents in the\nnetwork. We characterize the interaction with a directed acyclic graph (DAG).\nThe computation time for solving MAPF problem using PP is mainly determined\nthrough the longest path in this DAG. The longest path depends on the fixed\nundirected coupling graph and the variable prioritization. The approaches from\nliterature to prioritize agents are numerous and pursue various goals. This\narticle presents an approach for prioritization in PP to reduce the longest\npath length in the coupling DAG and thus the computation time for MAPF using\nPP. We prove that this problem can be mapped to a graph-coloring problem, in\nwhich the number of colors required corresponds to the longest path length in\nthe coupling DAG. We propose a decentralized graph-coloring algorithm to\ndetermine priorities for the agents. We evaluate the approach by applying it to\nmulti-agent motion planning (MAMP) for connected and automated vehicles (CAVs)\non roads using, a variant of MAPF.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.10812v1",
    "published_date": "2025-01-18 16:22:07 UTC",
    "updated_date": "2025-01-18 16:22:07 UTC"
  },
  {
    "arxiv_id": "2501.10809v2",
    "title": "Efficient auto-labeling of large-scale poultry datasets (ALPD) using an ensemble model with self- and active-learning approaches",
    "authors": [
      "Ramesh Bahadur Bist",
      "Lilong Chai",
      "Shawna Weimer",
      "Hannah Atungulua",
      "Chantel Pennicott",
      "Xiao Yang",
      "Sachin Subedi",
      "Chaitanya Pallerla",
      "Yang Tian",
      "Dongyi Wang"
    ],
    "abstract": "The rapid growth of artificial intelligence in poultry farming has\nhighlighted the challenge of efficiently labeling large, diverse datasets.\nManual annotation is time-consuming and costly, making it impractical for\nmodern systems that continuously generate data. This study addresses this\nchallenge by exploring semi-supervised auto-labeling methods, integrating self\nand active learning approaches to develop an efficient, label-scarce framework\nfor auto-labeling large poultry datasets (ALPD). For this study, video data\nwere collected from broilers and laying hens housed. Various machine learning\nmodels, including zero-shot models and supervised models, were utilized for\nbroilers and hens detection. The results showed that YOLOv8s-World and YOLOv9s\nperformed better when compared performance metrics for broiler and hen\ndetection under supervised learning, while among the semi-supervised model,\nYOLOv8s-ALPD achieved the highest precision (96.1%) and recall (99%) with an\nRMSE of 1.87. The hybrid YOLO-World model, incorporating the optimal YOLOv8s\nbackbone with zero-shot models, demonstrated the highest overall performance.\nIt achieved a precision of 99.2%, recall of 99.4%, and an F1 score of 98.7% for\ndetection. In addition, the semi-supervised models with minimal human\nintervention (active learning) reduced annotation time by over 80% compared to\nfull manual labeling. Moreover, integrating zero-shot models with the best\nmodels enhanced broiler and hen detection, achieving comparable results to\nsupervised models while significantly increasing speed. In conclusion,\nintegrating semi-supervised auto-labeling and zero-shot models significantly\nimproves detection accuracy. It reduces manual annotation efforts, offering a\npromising solution to optimize AI-driven systems in poultry farming, advancing\nprecision livestock management, and promoting more sustainable practices.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.10809v2",
    "published_date": "2025-01-18 16:20:04 UTC",
    "updated_date": "2025-02-22 00:11:13 UTC"
  },
  {
    "arxiv_id": "2501.10799v1",
    "title": "Step-KTO: Optimizing Mathematical Reasoning through Stepwise Binary Feedback",
    "authors": [
      "Yen-Ting Lin",
      "Di Jin",
      "Tengyu Xu",
      "Tianhao Wu",
      "Sainbayar Sukhbaatar",
      "Chen Zhu",
      "Yun He",
      "Yun-Nung Chen",
      "Jason Weston",
      "Yuandong Tian",
      "Arash Rahnama",
      "Sinong Wang",
      "Hao Ma",
      "Han Fang"
    ],
    "abstract": "Large language models (LLMs) have recently demonstrated remarkable success in\nmathematical reasoning. Despite progress in methods like chain-of-thought\nprompting and self-consistency sampling, these advances often focus on final\ncorrectness without ensuring that the underlying reasoning process is coherent\nand reliable. This paper introduces Step-KTO, a training framework that\ncombines process-level and outcome-level binary feedback to guide LLMs toward\nmore trustworthy reasoning trajectories. By providing binary evaluations for\nboth the intermediate reasoning steps and the final answer, Step-KTO encourages\nthe model to adhere to logical progressions rather than relying on superficial\nshortcuts. Our experiments on challenging mathematical benchmarks show that\nStep-KTO significantly improves both final answer accuracy and the quality of\nintermediate reasoning steps. For example, on the MATH-500 dataset, Step-KTO\nachieves a notable improvement in Pass@1 accuracy over strong baselines. These\nresults highlight the promise of integrating stepwise process feedback into LLM\ntraining, paving the way toward more interpretable and dependable reasoning\ncapabilities.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.10799v1",
    "published_date": "2025-01-18 15:38:03 UTC",
    "updated_date": "2025-01-18 15:38:03 UTC"
  },
  {
    "arxiv_id": "2501.10782v1",
    "title": "ML-SceGen: A Multi-level Scenario Generation Framework",
    "authors": [
      "Yicheng Xiao",
      "Yangyang Sun",
      "Yicheng Lin"
    ],
    "abstract": "Current scientific research witnesses various attempts at applying Large\nLanguage Models for scenario generation but is inclined only to comprehensive\nor dangerous scenarios. In this paper, we seek to build a three-stage framework\nthat not only lets users regain controllability over the generated scenarios\nbut also generates comprehensive scenarios containing danger factors in\nuncontrolled intersection settings. In the first stage, LLM agents will\ncontribute to translating the key components of the description of the expected\nscenarios into Functional Scenarios. For the second stage, we use Answer Set\nProgramming (ASP) solver Clingo to help us generate comprehensive logical\ntraffic within intersections. During the last stage, we use LLM to update\nrelevant parameters to increase the critical level of the concrete scenario.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "7 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.10782v1",
    "published_date": "2025-01-18 14:43:40 UTC",
    "updated_date": "2025-01-18 14:43:40 UTC"
  },
  {
    "arxiv_id": "2501.10781v1",
    "title": "Simultaneous Computation with Multiple Prioritizations in Multi-Agent Motion Planning",
    "authors": [
      "Patrick Scheffe",
      "Julius Kahle",
      "Bassam Alrifaee"
    ],
    "abstract": "Multi-agent path finding (MAPF) in large networks is computationally\nchallenging. An approach for MAPF is prioritized planning (PP), in which agents\nplan sequentially according to their priority. Albeit a computationally\nefficient approach for MAPF, the solution quality strongly depends on the\nprioritization. Most prioritizations rely either on heuristics, which do not\ngeneralize well, or iterate to find adequate priorities, which costs\ncomputational effort. In this work, we show how agents can compute with\nmultiple prioritizations simultaneously. Our approach is general as it does not\nrely on domain-specific knowledge. The context of this work is multi-agent\nmotion planning (MAMP) with a receding horizon subject to computation time\nconstraints. MAMP considers the system dynamics in more detail compared to\nMAPF. In numerical experiments on MAMP, we demonstrate that our approach to\nprioritization comes close to optimal prioritization and outperforms\nstate-of-the-art methods with only a minor increase in computation time. We\nshow real-time capability in an experiment on a road network with ten vehicles\nin our Cyber-Physical Mobility Lab.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.10781v1",
    "published_date": "2025-01-18 14:35:32 UTC",
    "updated_date": "2025-01-18 14:35:32 UTC"
  },
  {
    "arxiv_id": "2501.16348v1",
    "title": "An Integrated Approach to AI-Generated Content in e-health",
    "authors": [
      "Tasnim Ahmed",
      "Salimur Choudhury"
    ],
    "abstract": "Artificial Intelligence-Generated Content, a subset of Generative Artificial\nIntelligence, holds significant potential for advancing the e-health sector by\ngenerating diverse forms of data. In this paper, we propose an end-to-end\nclass-conditioned framework that addresses the challenge of data scarcity in\nhealth applications by generating synthetic medical images and text data,\nevaluating on practical applications such as retinopathy detection, skin\ninfections and mental health assessments. Our framework integrates Diffusion\nand Large Language Models (LLMs) to generate data that closely match real-world\npatterns, which is essential for improving downstream task performance and\nmodel robustness in e-health applications. Experimental results demonstrate\nthat the synthetic images produced by the proposed diffusion model outperform\ntraditional GAN architectures. Similarly, in the text modality, data generated\nby uncensored LLM achieves significantly better alignment with real-world data\nthan censored models in replicating the authentic tone.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted for presentation at 2025 IEEE International Conference on\n  Communications (IEEE ICC25)",
    "pdf_url": "http://arxiv.org/pdf/2501.16348v1",
    "published_date": "2025-01-18 14:35:29 UTC",
    "updated_date": "2025-01-18 14:35:29 UTC"
  },
  {
    "arxiv_id": "2501.10775v1",
    "title": "MedFILIP: Medical Fine-grained Language-Image Pre-training",
    "authors": [
      "Xinjie Liang",
      "Xiangyu Li",
      "Fanding Li",
      "Jie Jiang",
      "Qing Dong",
      "Wei Wang",
      "Kuanquan Wang",
      "Suyu Dong",
      "Gongning Luo",
      "Shuo Li"
    ],
    "abstract": "Medical vision-language pretraining (VLP) that leverages naturally-paired\nmedical image-report data is crucial for medical image analysis. However,\nexisting methods struggle to accurately characterize associations between\nimages and diseases, leading to inaccurate or incomplete diagnostic results. In\nthis work, we propose MedFILIP, a fine-grained VLP model, introduces medical\nimage-specific knowledge through contrastive learning, specifically: 1) An\ninformation extractor based on a large language model is proposed to decouple\ncomprehensive disease details from reports, which excels in extracting disease\ndeals through flexible prompt engineering, thereby effectively reducing text\ncomplexity while retaining rich information at a tiny cost. 2) A knowledge\ninjector is proposed to construct relationships between categories and visual\nattributes, which help the model to make judgments based on image features, and\nfosters knowledge extrapolation to unfamiliar disease categories. 3) A semantic\nsimilarity matrix based on fine-grained annotations is proposed, providing\nsmoother, information-richer labels, thus allowing fine-grained image-text\nalignment. 4) We validate MedFILIP on numerous datasets, e.g., RSNA-Pneumonia,\nNIH ChestX-ray14, VinBigData, and COVID-19. For single-label, multi-label, and\nfine-grained classification, our model achieves state-of-the-art performance,\nthe classification accuracy has increased by a maximum of 6.69\\%. The code is\navailable in https://github.com/PerceptionComputingLab/MedFILIP.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 5 figures, IEEE Journal of Biomedical and Health\n  Informatics 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.10775v1",
    "published_date": "2025-01-18 14:08:33 UTC",
    "updated_date": "2025-01-18 14:08:33 UTC"
  },
  {
    "arxiv_id": "2501.10770v1",
    "title": "Enhancing Diagnostic in 3D COVID-19 Pneumonia CT-scans through Explainable Uncertainty Bayesian Quantification",
    "authors": [
      "Juan Manuel Liscano Fierro",
      "Hector J. Hortua"
    ],
    "abstract": "Accurately classifying COVID-19 pneumonia in 3D CT scans remains a\nsignificant challenge in the field of medical image analysis. Although\ndeterministic neural networks have shown promising results in this area, they\nprovide only point estimates outputs yielding poor diagnostic in clinical\ndecision-making. In this paper, we explore the use of Bayesian neural networks\nfor classifying COVID-19 pneumonia in 3D CT scans providing uncertainties in\ntheir predictions. We compare deterministic networks and their Bayesian\ncounterpart, enhancing the decision-making accuracy under uncertainty\ninformation. Remarkably, our findings reveal that lightweight architectures\nachieve the highest accuracy of 96\\% after developing extensive hyperparameter\ntuning. Furthermore, the Bayesian counterpart of these architectures via\nMultiplied Normalizing Flow technique kept a similar performance along with\ncalibrated uncertainty estimates. Finally, we have developed a 3D-visualization\napproach to explain the neural network outcomes based on SHAP values. We\nconclude that explainability along with uncertainty quantification will offer\nbetter clinical decisions in medical image analysis, contributing to ongoing\nefforts for improving the diagnosis and treatment of COVID-19 pneumonia.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "61 pages, 16 figures. Comments are welcome",
    "pdf_url": "http://arxiv.org/pdf/2501.10770v1",
    "published_date": "2025-01-18 13:54:33 UTC",
    "updated_date": "2025-01-18 13:54:33 UTC"
  },
  {
    "arxiv_id": "2501.10768v1",
    "title": "MAPS: Advancing Multi-Modal Reasoning in Expert-Level Physical Science",
    "authors": [
      "Erle Zhu",
      "Yadi Liu",
      "Zhe Zhang",
      "Xujun Li",
      "Jin Zhou",
      "Xinjie Yu",
      "Minlie Huang",
      "Hongning Wang"
    ],
    "abstract": "Pre-trained on extensive text and image corpora, current Multi-Modal Large\nLanguage Models (MLLM) have shown strong capabilities in general visual\nreasoning tasks. However, their performance is still lacking in physical\ndomains that require understanding diagrams with complex physical structures\nand quantitative analysis based on multi-modal information. To address this, we\ndevelop a new framework, named Multi-Modal Scientific Reasoning with Physics\nPerception and Simulation (MAPS) based on an MLLM. MAPS decomposes expert-level\nmulti-modal reasoning task into physical diagram understanding via a Physical\nPerception Model (PPM) and reasoning with physical knowledge via a simulator.\nThe PPM module is obtained by fine-tuning a visual language model using\ncarefully designed synthetic data with paired physical diagrams and\ncorresponding simulation language descriptions. At the inference stage, MAPS\nintegrates the simulation language description of the input diagram provided by\nPPM and results obtained through a Chain-of-Simulation process with MLLM to\nderive the underlying rationale and the final answer. Validated using our\ncollected college-level circuit analysis problems, MAPS significantly improves\nreasoning accuracy of MLLM and outperforms all existing models. The results\nconfirm MAPS offers a promising direction for enhancing multi-modal scientific\nreasoning ability of MLLMs. We will release our code, model and dataset used\nfor our experiments upon publishing of this paper.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.10768v1",
    "published_date": "2025-01-18 13:54:00 UTC",
    "updated_date": "2025-01-18 13:54:00 UTC"
  },
  {
    "arxiv_id": "2501.10736v2",
    "title": "Semi-supervised Semantic Segmentation for Remote Sensing Images via Multi-scale Uncertainty Consistency and Cross-Teacher-Student Attention",
    "authors": [
      "Shanwen Wang",
      "Xin Sun",
      "Changrui Chen",
      "Danfeng Hong",
      "Jungong Han"
    ],
    "abstract": "Semi-supervised learning offers an appealing solution for remote sensing (RS)\nimage segmentation to relieve the burden of labor-intensive pixel-level\nlabeling. However, RS images pose unique challenges, including rich multi-scale\nfeatures and high inter-class similarity. To address these problems, this paper\nproposes a novel semi-supervised Multi-Scale Uncertainty and\nCross-Teacher-Student Attention (MUCA) model for RS image semantic segmentation\ntasks. Specifically, MUCA constrains the consistency among feature maps at\ndifferent layers of the network by introducing a multi-scale uncertainty\nconsistency regularization. It improves the multi-scale learning capability of\nsemi-supervised algorithms on unlabeled data. Additionally, MUCA utilizes a\nCross-Teacher-Student attention mechanism to guide the student network, guiding\nthe student network to construct more discriminative feature representations\nthrough complementary features from the teacher network. This design\neffectively integrates weak and strong augmentations (WA and SA) to further\nboost segmentation performance. To verify the effectiveness of our model, we\nconduct extensive experiments on ISPRS-Potsdam and LoveDA datasets. The\nexperimental results show the superiority of our method over state-of-the-art\nsemi-supervised methods. Notably, our model excels in distinguishing highly\nsimilar objects, showcasing its potential for advancing semi-supervised RS\nimage segmentation tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.10736v2",
    "published_date": "2025-01-18 11:57:20 UTC",
    "updated_date": "2025-03-13 14:18:36 UTC"
  },
  {
    "arxiv_id": "2501.10734v1",
    "title": "GEC-RAG: Improving Generative Error Correction via Retrieval-Augmented Generation for Automatic Speech Recognition Systems",
    "authors": [
      "Amin Robatian",
      "Mohammad Hajipour",
      "Mohammad Reza Peyghan",
      "Fatemeh Rajabi",
      "Sajjad Amini",
      "Shahrokh Ghaemmaghami",
      "Iman Gholampour"
    ],
    "abstract": "Automatic Speech Recognition (ASR) systems have demonstrated remarkable\nperformance across various applications. However, limited data and the unique\nlanguage features of specific domains, such as low-resource languages,\nsignificantly degrade their performance and lead to higher Word Error Rates\n(WER). In this study, we propose Generative Error Correction via\nRetrieval-Augmented Generation (GEC-RAG), a novel approach designed to improve\nASR accuracy for low-resource domains, like Persian. Our approach treats the\nASR system as a black-box, a common practice in cloud-based services, and\nproposes a Retrieval-Augmented Generation (RAG) approach within the In-Context\nLearning (ICL) scheme to enhance the quality of ASR predictions. By\nconstructing a knowledge base that pairs ASR predictions (1-best and 5-best\nhypotheses) with their corresponding ground truths, GEC-RAG retrieves lexically\nsimilar examples to the ASR transcription using the Term Frequency-Inverse\nDocument Frequency (TF-IDF) measure. This process provides relevant error\npatterns of the system alongside the ASR transcription to the Generative Large\nLanguage Model (LLM), enabling targeted corrections. Our results demonstrate\nthat this strategy significantly reduces WER in Persian and highlights a\npotential for domain adaptation and low-resource scenarios. This research\nunderscores the effectiveness of using RAG in enhancing ASR systems without\nrequiring direct model modification or fine-tuning, making it adaptable to any\ndomain by simply updating the transcription knowledge base with domain-specific\ndata.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "6 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.10734v1",
    "published_date": "2025-01-18 11:53:22 UTC",
    "updated_date": "2025-01-18 11:53:22 UTC"
  },
  {
    "arxiv_id": "2501.10727v1",
    "title": "In the Picture: Medical Imaging Datasets, Artifacts, and their Living Review",
    "authors": [
      "Amelia Jiménez-Sánchez",
      "Natalia-Rozalia Avlona",
      "Sarah de Boer",
      "Víctor M. Campello",
      "Aasa Feragen",
      "Enzo Ferrante",
      "Melanie Ganz",
      "Judy Wawira Gichoya",
      "Camila González",
      "Steff Groefsema",
      "Alessa Hering",
      "Adam Hulman",
      "Leo Joskowicz",
      "Dovile Juodelyte",
      "Melih Kandemir",
      "Thijs Kooi",
      "Jorge del Pozo Lérida",
      "Livie Yumeng Li",
      "Andre Pacheco",
      "Tim Rädsch",
      "Mauricio Reyes",
      "Théo Sourget",
      "Bram van Ginneken",
      "David Wen",
      "Nina Weng",
      "Jack Junchi Xu",
      "Hubert Dariusz Zając",
      "Maria A. Zuluaga",
      "Veronika Cheplygina"
    ],
    "abstract": "Datasets play a critical role in medical imaging research, yet issues such as\nlabel quality, shortcuts, and metadata are often overlooked. This lack of\nattention may harm the generalizability of algorithms and, consequently,\nnegatively impact patient outcomes. While existing medical imaging literature\nreviews mostly focus on machine learning (ML) methods, with only a few focusing\non datasets for specific applications, these reviews remain static -- they are\npublished once and not updated thereafter. This fails to account for emerging\nevidence, such as biases, shortcuts, and additional annotations that other\nresearchers may contribute after the dataset is published. We refer to these\nnewly discovered findings of datasets as research artifacts. To address this\ngap, we propose a living review that continuously tracks public datasets and\ntheir associated research artifacts across multiple medical imaging\napplications. Our approach includes a framework for the living review to\nmonitor data documentation artifacts, and an SQL database to visualize the\ncitation relationships between research artifact and dataset. Lastly, we\ndiscuss key considerations for creating medical imaging datasets, review best\npractices for data annotation, discuss the significance of shortcuts and\ndemographic diversity, and emphasize the importance of managing datasets\nthroughout their entire lifecycle. Our demo is publicly available at\nhttp://130.226.140.142.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "Manuscript under review",
    "pdf_url": "http://arxiv.org/pdf/2501.10727v1",
    "published_date": "2025-01-18 11:03:59 UTC",
    "updated_date": "2025-01-18 11:03:59 UTC"
  },
  {
    "arxiv_id": "2501.16347v2",
    "title": "Fast and Accurate Identification of Hardware Trojan Locations in Gate-Level Netlist using Nearest Neighbour Approach integrated with Machine Learning Technique",
    "authors": [
      "Anindita Chattopadhyay",
      "Siddharth Bisariya",
      "Vijay Kumar Sutrakar"
    ],
    "abstract": "In the evolving landscape of integrated circuit design, detecting Hardware\nTrojans (HTs) within a multi entity based design cycle presents significant\nchallenges. This research proposes an innovative machine learning-based\nmethodology for identifying malicious logic gates in gate-level netlists. By\nfocusing on path retrace algorithms. The methodology is validated across three\ndistinct cases, each employing different machine learning models to classify\nHTs. Case I utilizes a decision tree algorithm for node-to-node comparisons,\nsignificantly improving detection accuracy through the integration of Principal\nComponent Analysis (PCA). Case II introduces a graph-to-graph classification\nusing a Graph Neural Network (GNN) model, enabling the differentiation between\nnormal and Trojan-infected circuit designs. Case III applies GNN-based node\nclassification to identify individual compromised nodes and its location.\nAdditionally, nearest neighbor (NN) method has been combined with GNN\ngraph-to-graph in Case II and GNN node-to-node in Case III. Despite the\npotential of GNN model graph-to-graph classification, NN approach demonstrated\nsuperior performance, with the first nearest neighbor (1st NN) achieving 73.2%\naccuracy and the second nearest neighbor (2nd NN) method reaching 97.7%. In\ncomparison, the GNN model achieved an accuracy of 62.8%. Similarly, GNN model\nnode-to-node classification, NN approach demonstrated superior performance,\nwith the 1st NN achieving 93% accuracy and the 2nd NN method reaching 97.7%. In\ncomparison, the GNN model achieved an accuracy of 79.8%. However, higher and\nhigher NN will lead to large code coverage for the identification of HTs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.16347v2",
    "published_date": "2025-01-18 10:15:16 UTC",
    "updated_date": "2025-04-27 18:12:31 UTC"
  },
  {
    "arxiv_id": "2502.10396v1",
    "title": "DASKT: A Dynamic Affect Simulation Method for Knowledge Tracing",
    "authors": [
      "Xinjie Sun",
      "Kai Zhang",
      "Qi Liu",
      "Shuanghong Shen",
      "Fei Wang",
      "Yuxiang Guo",
      "Enhong Chen"
    ],
    "abstract": "Knowledge Tracing (KT) predicts future performance by modeling students'\nhistorical interactions, and understanding students' affective states can\nenhance the effectiveness of KT, thereby improving the quality of education.\nAlthough traditional KT values students' cognition and learning behaviors,\nefficient evaluation of students' affective states and their application in KT\nstill require further exploration due to the non-affect-oriented nature of the\ndata and budget constraints. To address this issue, we propose a\ncomputation-driven approach, Dynamic Affect Simulation Knowledge Tracing\n(DASKT), to explore the impact of various student affective states (such as\nfrustration, concentration, boredom, and confusion) on their knowledge states.\nIn this model, we first extract affective factors from students'\nnon-affect-oriented behavioral data, then use clustering and spatiotemporal\nsequence modeling to accurately simulate students' dynamic affect changes when\ndealing with different problems. Subsequently, {\\color{blue}we incorporate\naffect with time-series analysis to improve the model's ability to infer\nknowledge states over time and space.} Extensive experimental results on two\npublic real-world educational datasets show that DASKT can achieve more\nreasonable knowledge states under the effect of students' affective states.\nMoreover, DASKT outperforms the most advanced KT methods in predicting student\nperformance. Our research highlights a promising avenue for future KT studies,\nfocusing on achieving high interpretability and accuracy.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "14 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.10396v1",
    "published_date": "2025-01-18 10:02:10 UTC",
    "updated_date": "2025-01-18 10:02:10 UTC"
  },
  {
    "arxiv_id": "2501.10711v3",
    "title": "How Should We Build A Benchmark? Revisiting 274 Code-Related Benchmarks For LLMs",
    "authors": [
      "Jialun Cao",
      "Yuk-Kit Chan",
      "Zixuan Ling",
      "Wenxuan Wang",
      "Shuqing Li",
      "Mingwei Liu",
      "Ruixi Qiao",
      "Yuting Han",
      "Chaozheng Wang",
      "Boxi Yu",
      "Pinjia He",
      "Shuai Wang",
      "Zibin Zheng",
      "Michael R. Lyu",
      "Shing-Chi Cheung"
    ],
    "abstract": "Various benchmarks have been proposed to assess the performance of large\nlanguage models (LLMs) in different coding scenarios. We refer to them as\ncode-related benchmarks. However, there are no systematic guidelines by which\nsuch a benchmark should be developed to ensure its quality, reliability, and\nreproducibility. We propose How2Bench, which is comprised of a 55-criteria\nchecklist as a set of guidelines to govern the development of code-related\nbenchmarks comprehensively. Using HOW2BENCH, we profiled 274 benchmarks\nreleased within the past decade and found concerning issues. Nearly 70% of the\nbenchmarks did not take measures for data quality assurance; over 10% did not\neven open source or only partially open source. Many highly cited benchmarks\nhave loopholes, including duplicated samples, incorrect reference\ncodes/tests/prompts, and unremoved sensitive/confidential information. Finally,\nwe conducted a human study involving 49 participants, which revealed\nsignificant gaps in awareness of the importance of data quality,\nreproducibility, and transparency.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "42 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.10711v3",
    "published_date": "2025-01-18 09:51:57 UTC",
    "updated_date": "2025-02-17 13:49:45 UTC"
  },
  {
    "arxiv_id": "2501.10709v1",
    "title": "Revisiting Ensemble Methods for Stock Trading and Crypto Trading Tasks at ACM ICAIF FinRL Contest 2023-2024",
    "authors": [
      "Nikolaus Holzer",
      "Keyi Wang",
      "Kairong Xiao",
      "Xiao-Yang Liu Yanglet"
    ],
    "abstract": "Reinforcement learning has demonstrated great potential for performing\nfinancial tasks. However, it faces two major challenges: policy instability and\nsampling bottlenecks. In this paper, we revisit ensemble methods with massively\nparallel simulations on graphics processing units (GPUs), significantly\nenhancing the computational efficiency and robustness of trained models in\nvolatile financial markets. Our approach leverages the parallel processing\ncapability of GPUs to significantly improve the sampling speed for training\nensemble models. The ensemble models combine the strengths of component agents\nto improve the robustness of financial decision-making strategies. We conduct\nexperiments in both stock and cryptocurrency trading tasks to evaluate the\neffectiveness of our approach. Massively parallel simulation on a single GPU\nimproves the sampling speed by up to $1,746\\times$ using $2,048$ parallel\nenvironments compared to a single environment. The ensemble models have high\ncumulative returns and outperform some individual agents, reducing maximum\ndrawdown by up to $4.17\\%$ and improving the Sharpe ratio by up to $0.21$.\n  This paper describes trading tasks at ACM ICAIF FinRL Contests in 2023 and\n2024.",
    "categories": [
      "cs.CE",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.CE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.10709v1",
    "published_date": "2025-01-18 09:36:14 UTC",
    "updated_date": "2025-01-18 09:36:14 UTC"
  },
  {
    "arxiv_id": "2501.10696v1",
    "title": "Algorithmic Derivation of Human Spatial Navigation Indices From Eye Movement Data",
    "authors": [
      "Sobhan Teymouri",
      "Fatemeh Alizadehziri",
      "Mobina Zibandehpoor",
      "Mehdi Delrobaei"
    ],
    "abstract": "Spatial navigation is a complex cognitive function involving sensory inputs,\nsuch as visual, auditory, and proprioceptive information, to understand and\nmove within space. This ability allows humans to create mental maps, navigate\nthrough environments, and process directional cues, crucial for exploring new\nplaces and finding one's way in unfamiliar surroundings. This study takes an\nalgorithmic approach to extract indices relevant to human spatial navigation\nusing eye movement data. Leveraging electrooculography signals, we analyzed\nstatistical features and applied feature engineering techniques to study eye\nmovements during navigation tasks. The proposed work combines signal processing\nand machine learning approaches to develop indices for navigation and\norientation, spatial anxiety, landmark recognition, path survey, and path\nroute. The analysis yielded five subscore indices with notable accuracy. Among\nthese, the navigation and orientation subscore achieved an R2 score of 0.72,\nwhile the landmark recognition subscore attained an R2 score of 0.50.\nAdditionally, statistical features highly correlated with eye movement metrics,\nincluding blinks, saccades, and fixations, were identified. The findings of\nthis study can lead to more cognitive assessments and enable early detection of\nspatial navigation impairments, particularly among individuals at risk of\ncognitive decline.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "The dataset is available in the following work: Mobina Zibandehpoor,\n  Fatemeh Alizadehziri, Arash Abbasi Larki, Sobhan Teymouri, and Mehdi\n  Delrobaei. Electrooculography Dataset for Objective Spatial Navigation\n  Assessment in Healthy Participants. arXiv preprint arXiv:2411.06811, 2024",
    "pdf_url": "http://arxiv.org/pdf/2501.10696v1",
    "published_date": "2025-01-18 08:26:33 UTC",
    "updated_date": "2025-01-18 08:26:33 UTC"
  },
  {
    "arxiv_id": "2501.10693v1",
    "title": "Distributionally Robust Policy Evaluation and Learning for Continuous Treatment with Observational Data",
    "authors": [
      "Cheuk Hang Leung",
      "Yiyan Huang",
      "Yijun Li",
      "Qi Wu"
    ],
    "abstract": "Using offline observational data for policy evaluation and learning allows\ndecision-makers to evaluate and learn a policy that connects characteristics\nand interventions. Most existing literature has focused on either discrete\ntreatment spaces or assumed no difference in the distributions between the\npolicy-learning and policy-deployed environments. These restrict applications\nin many real-world scenarios where distribution shifts are present with\ncontinuous treatment. To overcome these challenges, this paper focuses on\ndeveloping a distributionally robust policy under a continuous treatment\nsetting. The proposed distributionally robust estimators are established using\nthe Inverse Probability Weighting (IPW) method extended from the discrete one\nfor policy evaluation and learning under continuous treatments. Specifically,\nwe introduce a kernel function into the proposed IPW estimator to mitigate the\nexclusion of observations that can occur in the standard IPW method to\ncontinuous treatments. We then provide finite-sample analysis that guarantees\nthe convergence of the proposed distributionally robust policy evaluation and\nlearning estimators. The comprehensive experiments further verify the\neffectiveness of our approach when distribution shifts are present.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.10693v1",
    "published_date": "2025-01-18 08:12:56 UTC",
    "updated_date": "2025-01-18 08:12:56 UTC"
  },
  {
    "arxiv_id": "2501.10688v2",
    "title": "Neural Algorithmic Reasoning for Hypergraphs with Looped Transformers",
    "authors": [
      "Xiaoyu Li",
      "Yingyu Liang",
      "Jiangxuan Long",
      "Zhenmei Shi",
      "Zhao Song",
      "Zhen Zhuang"
    ],
    "abstract": "Looped Transformers have shown exceptional neural algorithmic reasoning\ncapability in simulating traditional graph algorithms, but their application to\nmore complex structures like hypergraphs remains underexplored. Hypergraphs\ngeneralize graphs by modeling higher-order relationships among multiple\nentities, enabling richer representations but introducing significant\ncomputational challenges. In this work, we extend the Loop Transformer\narchitecture's neural algorithmic reasoning capability to simulate hypergraph\nalgorithms, addressing the gap between neural networks and combinatorial\noptimization over hypergraphs. Specifically, we propose a novel degradation\nmechanism for reducing hypergraphs to graph representations, enabling the\nsimulation of graph-based algorithms, such as Dijkstra's shortest path.\nFurthermore, we introduce a hyperedge-aware encoding scheme to simulate\nhypergraph-specific algorithms, exemplified by Helly's algorithm. We establish\ntheoretical guarantees for these simulations, demonstrating the feasibility of\nprocessing high-dimensional and combinatorial data using Loop Transformers.\nThis work highlights the potential of Transformers as general-purpose\nalgorithmic solvers for structured data.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CC",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.10688v2",
    "published_date": "2025-01-18 07:58:45 UTC",
    "updated_date": "2025-02-02 23:41:36 UTC"
  },
  {
    "arxiv_id": "2501.10677v2",
    "title": "Class-Imbalanced-Aware Adaptive Dataset Distillation for Scalable Pretrained Model on Credit Scoring",
    "authors": [
      "Xia Li",
      "Hanghang Zheng",
      "Xiao Chen",
      "Hong Liu",
      "Mao Mao"
    ],
    "abstract": "The advent of artificial intelligence has significantly enhanced credit\nscoring technologies. Despite the remarkable efficacy of advanced deep learning\nmodels, mainstream adoption continues to favor tree-structured models due to\ntheir robust predictive performance on tabular data. Although pretrained models\nhave seen considerable development, their application within the financial\nrealm predominantly revolves around question-answering tasks and the use of\nsuch models for tabular-structured credit scoring datasets remains largely\nunexplored. Tabular-oriented large models, such as TabPFN, has made the\napplication of large models in credit scoring feasible, albeit can only\nprocessing with limited sample sizes. This paper provides a novel framework to\ncombine tabular-tailored dataset distillation technique with the pretrained\nmodel, empowers the scalability for TabPFN. Furthermore, though class imbalance\ndistribution is the common nature in financial datasets, its influence during\ndataset distillation has not been explored. We thus integrate the\nimbalance-aware techniques during dataset distillation, resulting in improved\nperformance in financial datasets (e.g., a 2.5% enhancement in AUC). This study\npresents a novel framework for scaling up the application of large pretrained\nmodels on financial tabular datasets and offers a comparative analysis of the\ninfluence of class imbalance on the dataset distillation process. We believe\nthis approach can broaden the applications and downstream tasks of large models\nin the financial domain.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-fin.RM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.10677v2",
    "published_date": "2025-01-18 06:59:36 UTC",
    "updated_date": "2025-02-01 03:55:35 UTC"
  },
  {
    "arxiv_id": "2501.10661v1",
    "title": "Unveiling the Mystery of Weight in Large Foundation Models: Gaussian Distribution Never Fades",
    "authors": [
      "Chongjie Si",
      "Jingjing Jiang",
      "Wei Shen"
    ],
    "abstract": "This paper presents a pioneering exploration of the mechanisms underlying\nlarge foundation models' (LFMs) weights, aiming to simplify AI research.\nThrough extensive observation and analysis on prevailing LFMs, we find that\nregardless of initialization strategies, their weights predominantly follow a\nGaussian distribution, with occasional sharp, inverted T-shaped, or linear\npatterns. We further discover that the weights share the i.i.d. properties of\nGaussian noise, and explore their direct relationship. We find that\ntransformation weights can be derived from Gaussian noise, and they primarily\nserve to increase the standard deviation of pre-trained weights, with their\nstandard deviation growing with layer depth. In other words, transformation\nweights broaden the acceptable deviation from the optimal weights, facilitating\nadaptation to downstream tasks. Building upon the above conclusions, we\nthoroughly discussed the nature of optimal weights, ultimately concluding that\nthey should exhibit zero-mean, symmetry, and sparsity, with the sparse values\nbeing a truncated Gaussian distribution and a few outliers. Our experiments in\nLFM adaptation and editing demonstrate the effectiveness of these insights. We\nhope these findings can provide a foundational understanding to pave the way\nfor future advancements in the LFM community.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Revisions ongoing",
    "pdf_url": "http://arxiv.org/pdf/2501.10661v1",
    "published_date": "2025-01-18 05:43:17 UTC",
    "updated_date": "2025-01-18 05:43:17 UTC"
  },
  {
    "arxiv_id": "2501.13944v1",
    "title": "Fanar: An Arabic-Centric Multimodal Generative AI Platform",
    "authors": [
      "Fanar Team",
      "Ummar Abbas",
      "Mohammad Shahmeer Ahmad",
      "Firoj Alam",
      "Enes Altinisik",
      "Ehsannedin Asgari",
      "Yazan Boshmaf",
      "Sabri Boughorbel",
      "Sanjay Chawla",
      "Shammur Chowdhury",
      "Fahim Dalvi",
      "Kareem Darwish",
      "Nadir Durrani",
      "Mohamed Elfeky",
      "Ahmed Elmagarmid",
      "Mohamed Eltabakh",
      "Masoomali Fatehkia",
      "Anastasios Fragkopoulos",
      "Maram Hasanain",
      "Majd Hawasly",
      "Mus'ab Husaini",
      "Soon-Gyo Jung",
      "Ji Kim Lucas",
      "Walid Magdy",
      "Safa Messaoud",
      "Abubakr Mohamed",
      "Tasnim Mohiuddin",
      "Basel Mousi",
      "Hamdy Mubarak",
      "Ahmad Musleh",
      "Zan Naeem",
      "Mourad Ouzzani",
      "Dorde Popovic",
      "Amin Sadeghi",
      "Husrev Taha Sencar",
      "Mohammed Shinoy",
      "Omar Sinan",
      "Yifan Zhang",
      "Ahmed Ali",
      "Yassine El Kheir",
      "Xiaosong Ma",
      "Chaoyi Ruan"
    ],
    "abstract": "We present Fanar, a platform for Arabic-centric multimodal generative AI\nsystems, that supports language, speech and image generation tasks. At the\nheart of Fanar are Fanar Star and Fanar Prime, two highly capable Arabic Large\nLanguage Models (LLMs) that are best in the class on well established\nbenchmarks for similar sized models. Fanar Star is a 7B (billion) parameter\nmodel that was trained from scratch on nearly 1 trillion clean and deduplicated\nArabic, English and Code tokens. Fanar Prime is a 9B parameter model\ncontinually trained on the Gemma-2 9B base model on the same 1 trillion token\nset. Both models are concurrently deployed and designed to address different\ntypes of prompts transparently routed through a custom-built orchestrator. The\nFanar platform provides many other capabilities including a customized Islamic\nRetrieval Augmented Generation (RAG) system for handling religious prompts, a\nRecency RAG for summarizing information about current or recent events that\nhave occurred after the pre-training data cut-off date. The platform provides\nadditional cognitive capabilities including in-house bilingual speech\nrecognition that supports multiple Arabic dialects, voice and image generation\nthat is fine-tuned to better reflect regional characteristics. Finally, Fanar\nprovides an attribution service that can be used to verify the authenticity of\nfact based generated content.\n  The design, development, and implementation of Fanar was entirely undertaken\nat Hamad Bin Khalifa University's Qatar Computing Research Institute (QCRI) and\nwas sponsored by Qatar's Ministry of Communications and Information Technology\nto enable sovereign AI technology development.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.0; D.2.0"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13944v1",
    "published_date": "2025-01-18 05:35:32 UTC",
    "updated_date": "2025-01-18 05:35:32 UTC"
  },
  {
    "arxiv_id": "2501.10658v1",
    "title": "LUT-DLA: Lookup Table as Efficient Extreme Low-Bit Deep Learning Accelerator",
    "authors": [
      "Guoyu Li",
      "Shengyu Ye",
      "Chunyun Chen",
      "Yang Wang",
      "Fan Yang",
      "Ting Cao",
      "Cheng Liu",
      "Mohamed M. Sabry",
      "Mao Yang"
    ],
    "abstract": "The emergence of neural network capabilities invariably leads to a\nsignificant surge in computational demands due to expanding model sizes and\nincreased computational complexity. To reduce model size and lower inference\ncosts, recent research has focused on simplifying models and designing hardware\naccelerators using low-bit quantization. However, due to numerical\nrepresentation limits, scalar quantization cannot reduce bit width lower than\n1-bit, diminishing its benefits. To break through these limitations, we\nintroduce LUT-DLA, a Look-Up Table (LUT) Deep Learning Accelerator Framework\nthat utilizes vector quantization to convert neural network models into LUTs,\nachieving extreme low-bit quantization. The LUT-DLA framework facilitates\nefficient and cost-effective hardware accelerator designs and supports the\nLUTBoost algorithm, which helps to transform various DNN models into LUT-based\nmodels via multistage training, drastically cutting both computational and\nhardware overhead. Additionally, through co-design space exploration, LUT-DLA\nassesses the impact of various model and hardware parameters to fine-tune\nhardware configurations for different application scenarios, optimizing\nperformance and efficiency. Our comprehensive experiments show that LUT-DLA\nachieves improvements in power efficiency and area efficiency with gains of\n$1.4$~$7.0\\times$ and $1.5$~$146.1\\times$, respectively, while maintaining only\na modest accuracy drop. For CNNs, accuracy decreases by $0.1\\%$~$3.1\\%$ using\nthe $L_2$ distance similarity, $0.1\\%$~$3.4\\%$ with the $L_1$ distance\nsimilarity, and $0.1\\%$~$3.8\\%$ when employing the Chebyshev distance\nsimilarity. For transformer-based models, the accuracy drop ranges from $1.4\\%$\nto $3.0\\%$.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AR",
    "comment": "12 pages, 14 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.10658v1",
    "published_date": "2025-01-18 05:27:25 UTC",
    "updated_date": "2025-01-18 05:27:25 UTC"
  },
  {
    "arxiv_id": "2501.16346v1",
    "title": "Self-supervised Graph Transformer with Contrastive Learning for Brain Connectivity Analysis towards Improving Autism Detection",
    "authors": [
      "Yicheng Leng",
      "Syed Muhammad Anwar",
      "Islem Rekik",
      "Sen He",
      "Eung-Joo Lee"
    ],
    "abstract": "Functional Magnetic Resonance Imaging (fMRI) provides useful insights into\nthe brain function both during task or rest. Representing fMRI data using\ncorrelation matrices is found to be a reliable method of analyzing the inherent\nconnectivity of the brain in the resting and active states. Graph Neural\nNetworks (GNNs) have been widely used for brain network analysis due to their\ninherent explainability capability. In this work, we introduce a novel\nframework using contrastive self-supervised learning graph transformers,\nincorporating a brain network transformer encoder with random graph\nalterations. The proposed network leverages both contrastive learning and graph\nalterations to effectively train the graph transformer for autism detection.\nOur approach, tested on Autism Brain Imaging Data Exchange (ABIDE) data,\ndemonstrates superior autism detection, achieving an AUROC of 82.6 and an\naccuracy of 74%, surpassing current state-of-the-art methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.16346v1",
    "published_date": "2025-01-18 05:12:40 UTC",
    "updated_date": "2025-01-18 05:12:40 UTC"
  },
  {
    "arxiv_id": "2501.13943v1",
    "title": "Language Representation Favored Zero-Shot Cross-Domain Cognitive Diagnosis",
    "authors": [
      "Shuo Liu",
      "Zihan Zhou",
      "Yuanhao Liu",
      "Jing Zhang",
      "Hong Qian"
    ],
    "abstract": "Cognitive diagnosis aims to infer students' mastery levels based on their\nhistorical response logs. However, existing cognitive diagnosis models (CDMs),\nwhich rely on ID embeddings, often have to train specific models on specific\ndomains. This limitation may hinder their directly practical application in\nvarious target domains, such as different subjects (e.g., Math, English and\nPhysics) or different education platforms (e.g., ASSISTments, Junyi Academy and\nKhan Academy). To address this issue, this paper proposes the language\nrepresentation favored zero-shot cross-domain cognitive diagnosis (LRCD).\nSpecifically, LRCD first analyzes the behavior patterns of students, exercises\nand concepts in different domains, and then describes the profiles of students,\nexercises and concepts using textual descriptions. Via recent advanced\ntext-embedding modules, these profiles can be transformed to vectors in the\nunified language space. Moreover, to address the discrepancy between the\nlanguage space and the cognitive diagnosis space, we propose language-cognitive\nmappers in LRCD to learn the mapping from the former to the latter. Then, these\nprofiles can be easily and efficiently integrated and trained with existing\nCDMs. Extensive experiments show that training LRCD on real-world datasets can\nachieve commendable zero-shot performance across different target domains, and\nin some cases, it can even achieve competitive performance with some classic\nCDMs trained on the full response data on target domains. Notably, we\nsurprisingly find that LRCD can also provide interesting insights into the\ndifferences between various subjects (such as humanities and sciences) and\nsources (such as primary and secondary education).",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13943v1",
    "published_date": "2025-01-18 03:35:44 UTC",
    "updated_date": "2025-01-18 03:35:44 UTC"
  },
  {
    "arxiv_id": "2501.10627v1",
    "title": "AI/ML Based Detection and Categorization of Covert Communication in IPv6 Network",
    "authors": [
      "Mohammad Wali Ur Rahman",
      "Yu-Zheng Lin",
      "Carter Weeks",
      "David Ruddell",
      "Jeff Gabriellini",
      "Bill Hayes",
      "Salim Hariri",
      "Edward V. Ziegler Jr"
    ],
    "abstract": "The flexibility and complexity of IPv6 extension headers allow attackers to\ncreate covert channels or bypass security mechanisms, leading to potential data\nbreaches or system compromises. The mature development of machine learning has\nbecome the primary detection technology option used to mitigate covert\ncommunication threats. However, the complexity of detecting covert\ncommunication, evolving injection techniques, and scarcity of data make\nbuilding machine-learning models challenging. In previous related research,\nmachine learning has shown good performance in detecting covert communications,\nbut oversimplified attack scenario assumptions cannot represent the complexity\nof modern covert technologies and make it easier for machine learning models to\ndetect covert communications. To bridge this gap, in this study, we analyzed\nthe packet structure and network traffic behavior of IPv6, used encryption\nalgorithms, and performed covert communication injection without changing\nnetwork packet behavior to get closer to real attack scenarios. In addition to\nanalyzing and injecting methods for covert communications, this study also uses\ncomprehensive machine learning techniques to train the model proposed in this\nstudy to detect threats, including traditional decision trees such as random\nforests and gradient boosting, as well as complex neural network architectures\nsuch as CNNs and LSTMs, to achieve detection accuracy of over 90\\%. This study\ndetails the methods used for dataset augmentation and the comparative\nperformance of the applied models, reinforcing insights into the adaptability\nand resilience of the machine learning application in IPv6 covert\ncommunication. In addition, we also proposed a Generative AI-assisted\ninterpretation concept based on prompt engineering as a preliminary study of\nthe role of Generative AI agents in covert communication.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "cs.NI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.10627v1",
    "published_date": "2025-01-18 02:05:37 UTC",
    "updated_date": "2025-01-18 02:05:37 UTC"
  }
]
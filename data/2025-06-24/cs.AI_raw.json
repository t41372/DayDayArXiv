[
  {
    "arxiv_id": "2506.20062v3",
    "title": "Beyond Autocomplete: Designing CopilotLens Towards Transparent and Explainable AI Coding Agents",
    "authors": [
      "Runlong Ye",
      "Zeling Zhang",
      "Boushra Almazroua",
      "Michael Liut"
    ],
    "abstract": "AI-powered code assistants are widely used to generate code completions, significantly boosting developer productivity. However, these tools typically present suggestions without explaining their rationale, leaving their decision-making process inscrutable. This opacity hinders developers' ability to critically evaluate outputs, form accurate mental models, and calibrate trust in the system. To address this, we introduce CopilotLens, a novel interactive framework that reframes code completion from a simple suggestion into a transparent, explainable interaction. CopilotLens operates as an explanation layer that reconstructs the AI agent's \"thought process\" through a dynamic, two-level interface. The tool aims to surface both high-level code changes and the specific codebase context influences. This paper presents the design and rationale of CopilotLens, offering a concrete framework and articulating expectations on deepening comprehension and calibrated trust, which we plan to evaluate in subsequent work.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "accepted at The First Workshop on the Application of LLM Explainability to Reasoning and Planning (XLLM-Reason-Plan) @ COLM 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.20062v3",
    "published_date": "2025-06-24 23:50:03 UTC",
    "updated_date": "2025-09-21 15:50:29 UTC"
  },
  {
    "arxiv_id": "2506.20059v1",
    "title": "DiaLLMs: EHR Enhanced Clinical Conversational System for Clinical Test Recommendation and Diagnosis Prediction",
    "authors": [
      "Weijieying Ren",
      "Tianxiang Zhao",
      "Lei Wang",
      "Tianchun Wang",
      "Vasant Honavar"
    ],
    "abstract": "Recent advances in Large Language Models (LLMs) have led to remarkable progresses in medical consultation. However, existing medical LLMs overlook the essential role of Electronic Health Records (EHR) and focus primarily on diagnosis recommendation, limiting their clinical applicability. We propose DiaLLM, the first medical LLM that integrates heterogeneous EHR data into clinically grounded dialogues, enabling clinical test recommendation, result interpretation, and diagnosis prediction to better align with real-world medical practice. To construct clinically grounded dialogues from EHR, we design a Clinical Test Reference (CTR) strategy that maps each clinical code to its corresponding description and classifies test results as \"normal\" or \"abnormal\". Additionally, DiaLLM employs a reinforcement learning framework for evidence acquisition and automated diagnosis. To handle the large action space, we introduce a reject sampling strategy to reduce redundancy and improve exploration efficiency. Furthermore, a confirmation reward and a class-sensitive diagnosis reward are designed to guide accurate diagnosis prediction. Extensive experimental results demonstrate that DiaLLM outperforms baselines in clinical test recommendation and diagnosis prediction.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.20059v1",
    "published_date": "2025-06-24 23:47:21 UTC",
    "updated_date": "2025-06-24 23:47:21 UTC"
  },
  {
    "arxiv_id": "2506.20049v2",
    "title": "Robust Robotic Exploration and Mapping Using Generative Occupancy Map Synthesis",
    "authors": [
      "Lorin Achey",
      "Alec Reed",
      "Brendan Crowe",
      "Bradley Hayes",
      "Christoffer Heckman"
    ],
    "abstract": "We present a novel approach for enhancing robotic exploration by using generative occupancy mapping. We implement SceneSense, a diffusion model designed and trained for predicting 3D occupancy maps given partial observations. Our proposed approach probabilistically fuses these predictions into a running occupancy map in real-time, resulting in significant improvements in map quality and traversability. We deploy SceneSense on a quadruped robot and validate its performance with real-world experiments to demonstrate the effectiveness of the model. In these experiments we show that occupancy maps enhanced with SceneSense predictions better estimate the distribution of our fully observed ground truth data ($24.44\\%$ FID improvement around the robot and $75.59\\%$ improvement at range). We additionally show that integrating SceneSense enhanced maps into our robotic exploration stack as a ``drop-in'' map improvement, utilizing an existing off-the-shelf planner, results in improvements in robustness and traversability time. Finally, we show results of full exploration evaluations with our proposed system in two dissimilar environments and find that locally enhanced maps provide more consistent exploration results than maps constructed only from direct sensor measurements.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "arXiv admin note: text overlap with arXiv:2409.10681",
    "pdf_url": "https://arxiv.org/pdf/2506.20049v2",
    "published_date": "2025-06-24 23:13:44 UTC",
    "updated_date": "2025-12-29 19:07:52 UTC"
  },
  {
    "arxiv_id": "2506.20046v1",
    "title": "GNN's Uncertainty Quantification using Self-Distillation",
    "authors": [
      "Hirad Daneshvar",
      "Reza Samavi"
    ],
    "abstract": "Graph Neural Networks (GNNs) have shown remarkable performance in the healthcare domain. However, what remained challenging is quantifying the predictive uncertainty of GNNs, which is an important aspect of trustworthiness in clinical settings. While Bayesian and ensemble methods can be used to quantify uncertainty, they are computationally expensive. Additionally, the disagreement metric used by ensemble methods to compute uncertainty cannot capture the diversity of models in an ensemble network. In this paper, we propose a novel method, based on knowledge distillation, to quantify GNNs' uncertainty more efficiently and with higher precision. We apply self-distillation, where the same network serves as both the teacher and student models, thereby avoiding the need to train several networks independently. To ensure the impact of self-distillation, we develop an uncertainty metric that captures the diverse nature of the network by assigning different weights to each GNN classifier. We experimentally evaluate the precision, performance, and ability of our approach in distinguishing out-of-distribution data on two graph datasets: MIMIC-IV and Enzymes. The evaluation results demonstrate that the proposed method can effectively capture the predictive uncertainty of the model while having performance similar to that of the MC Dropout and ensemble methods. The code is publicly available at https://github.com/tailabTMU/UQ_GNN.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "The paper has been accepted in the International Conference on AI in Healthcare (AIiH) 2025 and will appear in the conference proceedings",
    "pdf_url": "https://arxiv.org/pdf/2506.20046v1",
    "published_date": "2025-06-24 23:08:31 UTC",
    "updated_date": "2025-06-24 23:08:31 UTC"
  },
  {
    "arxiv_id": "2506.20041v1",
    "title": "LSH-DynED: A Dynamic Ensemble Framework with LSH-Based Undersampling for Evolving Multi-Class Imbalanced Classification",
    "authors": [
      "Soheil Abadifard",
      "Fazli Can"
    ],
    "abstract": "The classification of imbalanced data streams, which have unequal class distributions, is a key difficulty in machine learning, especially when dealing with multiple classes. While binary imbalanced data stream classification tasks have received considerable attention, only a few studies have focused on multi-class imbalanced data streams. Effectively managing the dynamic imbalance ratio is a key challenge in this domain. This study introduces a novel, robust, and resilient approach to address these challenges by integrating Locality Sensitive Hashing with Random Hyperplane Projections (LSH-RHP) into the Dynamic Ensemble Diversification (DynED) framework. To the best of our knowledge, we present the first application of LSH-RHP for undersampling in the context of imbalanced non-stationary data streams. The proposed method undersamples the majority classes by utilizing LSH-RHP, provides a balanced training set, and improves the ensemble's prediction performance. We conduct comprehensive experiments on 23 real-world and ten semi-synthetic datasets and compare LSH-DynED with 15 state-of-the-art methods. The results reveal that LSH-DynED outperforms other approaches in terms of both Kappa and mG-Mean effectiveness measures, demonstrating its capability in dealing with multi-class imbalanced non-stationary data streams. Notably, LSH-DynED performs well in large-scale, high-dimensional datasets with considerable class imbalances and demonstrates adaptation and robustness in real-world circumstances. To motivate our design, we review existing methods for imbalanced data streams, outline key challenges, and offer guidance for future work. For the reproducibility of our results, we have made our implementation available on GitHub.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.20041v1",
    "published_date": "2025-06-24 22:46:47 UTC",
    "updated_date": "2025-06-24 22:46:47 UTC"
  },
  {
    "arxiv_id": "2506.20040v2",
    "title": "Cross-Layer Discrete Concept Discovery for Interpreting Language Models",
    "authors": [
      "Ankur Garg",
      "Xuemin Yu",
      "Hassan Sajjad",
      "Samira Ebrahimi Kahou"
    ],
    "abstract": "Uncovering emergent concepts across transformer layers remains a significant challenge because the residual stream linearly mixes and duplicates information, obscuring how features evolve within large language models. Current research efforts primarily inspect neural representations at single layers, thereby overlooking this cross-layer superposition and the redundancy it introduces. These representations are typically either analyzed directly for activation patterns or passed to probing classifiers that map them to a limited set of predefined concepts. To address these limitations, we propose cross-layer VQ-VAE (CLVQ-VAE), a framework that uses vector quantization to map representations across layers and in the process collapse duplicated residual-stream features into compact, interpretable concept vectors. Our approach uniquely combines top-k temperature-based sampling during quantization with EMA codebook updates, providing controlled exploration of the discrete latent space while maintaining code-book diversity. We further enhance the framework with scaled-spherical k-means++ for codebook initialization, which clusters by directional similarity rather than magnitude, better aligning with semantic structure in word embedding space.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.20040v2",
    "published_date": "2025-06-24 22:43:36 UTC",
    "updated_date": "2025-07-16 21:35:12 UTC"
  },
  {
    "arxiv_id": "2506.20039v1",
    "title": "Learning Bilateral Team Formation in Cooperative Multi-Agent Reinforcement Learning",
    "authors": [
      "Koorosh Moslemi",
      "Chi-Guhn Lee"
    ],
    "abstract": "Team formation and the dynamics of team-based learning have drawn significant interest in the context of Multi-Agent Reinforcement Learning (MARL). However, existing studies primarily focus on unilateral groupings, predefined teams, or fixed-population settings, leaving the effects of algorithmic bilateral grouping choices in dynamic populations underexplored. To address this gap, we introduce a framework for learning two-sided team formation in dynamic multi-agent systems. Through this study, we gain insight into what algorithmic properties in bilateral team formation influence policy performance and generalization. We validate our approach using widely adopted multi-agent scenarios, demonstrating competitive performance and improved generalization in most scenarios.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.GT",
      "cs.LG"
    ],
    "primary_category": "cs.MA",
    "comment": "Accepted to the 2nd Coordination and Cooperation in Multi-Agent Reinforcement Learning (CoCoMARL) Workshop at RLC 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.20039v1",
    "published_date": "2025-06-24 22:40:05 UTC",
    "updated_date": "2025-06-24 22:40:05 UTC"
  },
  {
    "arxiv_id": "2506.20036v1",
    "title": "Hierarchical Reinforcement Learning and Value Optimization for Challenging Quadruped Locomotion",
    "authors": [
      "Jeremiah Coholich",
      "Muhammad Ali Murtaza",
      "Seth Hutchinson",
      "Zsolt Kira"
    ],
    "abstract": "We propose a novel hierarchical reinforcement learning framework for quadruped locomotion over challenging terrain. Our approach incorporates a two-layer hierarchy in which a high-level policy (HLP) selects optimal goals for a low-level policy (LLP). The LLP is trained using an on-policy actor-critic RL algorithm and is given footstep placements as goals. We propose an HLP that does not require any additional training or environment samples and instead operates via an online optimization process over the learned value function of the LLP. We demonstrate the benefits of this framework by comparing it with an end-to-end reinforcement learning (RL) approach. We observe improvements in its ability to achieve higher rewards with fewer collisions across an array of different terrains, including terrains more difficult than any encountered during training.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.20036v1",
    "published_date": "2025-06-24 22:19:15 UTC",
    "updated_date": "2025-06-24 22:19:15 UTC"
  },
  {
    "arxiv_id": "2506.20031v2",
    "title": "Automated Generation of Diverse Courses of Actions for Multi-Agent Operations using Binary Optimization and Graph Learning",
    "authors": [
      "Prithvi Poddar",
      "Ehsan Tarkesh Esfahani",
      "Karthik Dantu",
      "Souma Chowdhury"
    ],
    "abstract": "Operations in disaster response, search \\& rescue, and military missions that involve multiple agents demand automated processes to support the planning of the courses of action (COA). Moreover, traverse-affecting changes in the environment (rain, snow, blockades, etc.) may impact the expected performance of a COA, making it desirable to have a pool of COAs that are diverse in task distributions across agents. Further, variations in agent capabilities, which could be human crews and/or autonomous systems, present practical opportunities and computational challenges to the planning process. This paper presents a new theoretical formulation and computational framework to generate such diverse pools of COAs for operations with soft variations in agent-task compatibility. Key to the problem formulation is a graph abstraction of the task space and the pool of COAs itself to quantify its diversity. Formulating the COAs as a centralized multi-robot task allocation problem, a genetic algorithm is used for (order-ignoring) allocations of tasks to each agent that jointly maximize diversity within the COA pool and overall compatibility of the agent-task mappings. A graph neural network is trained using a policy gradient approach to then perform single agent task sequencing in each COA, which maximizes completion rates adaptive to task features. Our tests of the COA generation process in a simulated environment demonstrate significant performance gain over a random walk baseline, small optimality gap in task sequencing, and execution time of about 50 minutes to plan up to 20 COAs for 5 agent/100 task operations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted for presentation in proceedings of IEEE CASE 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.20031v2",
    "published_date": "2025-06-24 21:58:30 UTC",
    "updated_date": "2025-07-29 00:31:23 UTC"
  },
  {
    "arxiv_id": "2506.20024v3",
    "title": "Elucidated Rolling Diffusion Models for Probabilistic Forecasting of Complex Dynamics",
    "authors": [
      "Salva Rühling Cachay",
      "Miika Aittala",
      "Karsten Kreis",
      "Noah Brenowitz",
      "Arash Vahdat",
      "Morteza Mardani",
      "Rose Yu"
    ],
    "abstract": "Diffusion models are a powerful tool for probabilistic forecasting, yet most applications in high-dimensional complex systems predict future states individually. This approach struggles to model complex temporal dependencies and fails to explicitly account for the progressive growth of uncertainty inherent to the systems. While rolling diffusion frameworks, which apply increasing noise to forecasts at longer lead times, have been proposed to address this, their integration with state-of-the-art, high-fidelity diffusion techniques remains a significant challenge. We tackle this problem by introducing Elucidated Rolling Diffusion Models (ERDM), the first framework to successfully unify a rolling forecast structure with the principled, performant design of Elucidated Diffusion Models (EDM). To do this, we adapt the core EDM components-its noise schedule, network preconditioning, and Heun sampler-to the rolling forecast setting. The success of this integration is driven by three key contributions: (i) a novel loss weighting scheme that focuses model capacity on the mid-range forecast horizons where determinism gives way to stochasticity; (ii) an efficient initialization strategy using a pre-trained EDM for the initial window; and (iii) a bespoke hybrid sequence architecture for robust spatiotemporal feature extraction under progressive denoising. On 2D Navier-Stokes simulations and ERA5 global weather forecasting at 1.5-degree resolution, ERDM consistently outperforms key diffusion-based baselines, including conditional autoregressive EDM. ERDM offers a flexible and powerful general framework for tackling diffusion-based dynamics forecasting problems where modeling uncertainty propagation is paramount.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.ao-ph",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.20024v3",
    "published_date": "2025-06-24 21:44:31 UTC",
    "updated_date": "2025-12-08 23:59:30 UTC"
  },
  {
    "arxiv_id": "2506.20020v1",
    "title": "Persona-Assigned Large Language Models Exhibit Human-Like Motivated Reasoning",
    "authors": [
      "Saloni Dash",
      "Amélie Reymond",
      "Emma S. Spiro",
      "Aylin Caliskan"
    ],
    "abstract": "Reasoning in humans is prone to biases due to underlying motivations like identity protection, that undermine rational decision-making and judgment. This motivated reasoning at a collective level can be detrimental to society when debating critical issues such as human-driven climate change or vaccine safety, and can further aggravate political polarization. Prior studies have reported that large language models (LLMs) are also susceptible to human-like cognitive biases, however, the extent to which LLMs selectively reason toward identity-congruent conclusions remains largely unexplored. Here, we investigate whether assigning 8 personas across 4 political and socio-demographic attributes induces motivated reasoning in LLMs. Testing 8 LLMs (open source and proprietary) across two reasoning tasks from human-subject studies -- veracity discernment of misinformation headlines and evaluation of numeric scientific evidence -- we find that persona-assigned LLMs have up to 9% reduced veracity discernment relative to models without personas. Political personas specifically, are up to 90% more likely to correctly evaluate scientific evidence on gun control when the ground truth is congruent with their induced political identity. Prompt-based debiasing methods are largely ineffective at mitigating these effects. Taken together, our empirical findings are the first to suggest that persona-assigned LLMs exhibit human-like motivated reasoning that is hard to mitigate through conventional debiasing prompts -- raising concerns of exacerbating identity-congruent reasoning in both LLMs and humans.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.20020v1",
    "published_date": "2025-06-24 21:35:17 UTC",
    "updated_date": "2025-06-24 21:35:17 UTC"
  },
  {
    "arxiv_id": "2506.20018v2",
    "title": "Achieving Trustworthy Real-Time Decision Support Systems with Low-Latency Interpretable AI Models",
    "authors": [
      "Zechun Deng",
      "Ziwei Liu",
      "Ziqian Bi",
      "Junhao Song",
      "Chia Xin Liang",
      "Joe Yeong",
      "Xinyuan Song",
      "Junfeng Hao"
    ],
    "abstract": "This paper investigates real-time decision support systems that leverage low-latency AI models, bringing together recent progress in holistic AI-driven decision tools, integration with Edge-IoT technologies, and approaches for effective human-AI teamwork. It looks into how large language models can assist decision-making, especially when resources are limited. The research also examines the effects of technical developments such as DeLLMa, methods for compressing models, and improvements for analytics on edge devices, while also addressing issues like limited resources and the need for adaptable frameworks. Through a detailed review, the paper offers practical perspectives on development strategies and areas of application, adding to the field by pointing out opportunities for more efficient and flexible AI-supported systems. The conclusions set the stage for future breakthroughs in this fast-changing area, highlighting how AI can reshape real-time decision support.",
    "categories": [
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.20018v2",
    "published_date": "2025-06-24 21:22:25 UTC",
    "updated_date": "2025-12-11 16:41:25 UTC"
  },
  {
    "arxiv_id": "2506.20016v1",
    "title": "New Insights on Unfolding and Fine-tuning Quantum Federated Learning",
    "authors": [
      "Shanika Iroshi Nanayakkara",
      "Shiva Raj Pokhrel"
    ],
    "abstract": "Client heterogeneity poses significant challenges to the performance of Quantum Federated Learning (QFL). To overcome these limitations, we propose a new approach leveraging deep unfolding, which enables clients to autonomously optimize hyperparameters, such as learning rates and regularization factors, based on their specific training behavior. This dynamic adaptation mitigates overfitting and ensures robust optimization in highly heterogeneous environments where standard aggregation methods often fail. Our framework achieves approximately 90% accuracy, significantly outperforming traditional methods, which typically yield around 55% accuracy, as demonstrated through real-time training on IBM quantum hardware and Qiskit Aer simulators. By developing self adaptive fine tuning, the proposed method proves particularly effective in critical applications such as gene expression analysis and cancer detection, enhancing diagnostic precision and predictive modeling within quantum systems. Our results are attributed to convergence-aware, learnable optimization steps intrinsic to the deep unfolded framework, which maintains the generalization. Hence, this study addresses the core limitations of conventional QFL, streamlining its applicability to any complex challenges such as healthcare and genomic research.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, 9 figures, 7 Tables, Submitted to IEEE/ACM journal 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.20016v1",
    "published_date": "2025-06-24 21:17:48 UTC",
    "updated_date": "2025-06-24 21:17:48 UTC"
  },
  {
    "arxiv_id": "2506.20009v1",
    "title": "Accurate and Energy Efficient: Local Retrieval-Augmented Generation Models Outperform Commercial Large Language Models in Medical Tasks",
    "authors": [
      "Konstantinos Vrettos",
      "Michail E. Klontzas"
    ],
    "abstract": "Background The increasing adoption of Artificial Intelligence (AI) in healthcare has sparked growing concerns about its environmental and ethical implications. Commercial Large Language Models (LLMs), such as ChatGPT and DeepSeek, require substantial resources, while the utilization of these systems for medical purposes raises critical issues regarding patient privacy and safety. Methods We developed a customizable Retrieval-Augmented Generation (RAG) framework for medical tasks, which monitors its energy usage and CO2 emissions. This system was then used to create RAGs based on various open-source LLMs. The tested models included both general purpose models like llama3.1:8b and medgemma-4b-it, which is medical-domain specific. The best RAGs performance and energy consumption was compared to DeepSeekV3-R1 and OpenAIs o4-mini model. A dataset of medical questions was used for the evaluation. Results Custom RAG models outperformed commercial models in accuracy and energy consumption. The RAG model built on llama3.1:8B achieved the highest accuracy (58.5%) and was significantly better than other models, including o4-mini and DeepSeekV3-R1. The llama3.1-RAG also exhibited the lowest energy consumption and CO2 footprint among all models, with a Performance per kWh of 0.52 and a total CO2 emission of 473g. Compared to o4-mini, the llama3.1-RAG achieved 2.7x times more accuracy points per kWh and 172% less electricity usage while maintaining higher accuracy. Conclusion Our study demonstrates that local LLMs can be leveraged to develop RAGs that outperform commercial, online LLMs in medical tasks, while having a smaller environmental impact. Our modular framework promotes sustainable AI development, reducing electricity usage and aligning with the UNs Sustainable Development Goals.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "18 pages, 3 Figures",
    "pdf_url": "https://arxiv.org/pdf/2506.20009v1",
    "published_date": "2025-06-24 20:56:03 UTC",
    "updated_date": "2025-06-24 20:56:03 UTC"
  },
  {
    "arxiv_id": "2506.20008v2",
    "title": "QHackBench: Benchmarking Large Language Models for Quantum Code Generation Using PennyLane Hackathon Challenges",
    "authors": [
      "Abdul Basit",
      "Minghao Shao",
      "Muhammad Haider Asif",
      "Nouhaila Innan",
      "Muhammad Kashif",
      "Alberto Marchisio",
      "Muhammad Shafique"
    ],
    "abstract": "Recent advances in Large Language Models (LLMs) have demonstrated strong potential in code generation, yet their effectiveness in quantum computing remains underexplored. This paper benchmarks LLMs for PennyLane-based quantum code generation using real-world challenges from the Quantum Hackathon (QHack). We introduce QHackBench, a novel benchmark dataset derived from QHack competitions, and evaluate model performance under vanilla prompting and Retrieval-Augmented Generation (RAG). Our structured evaluation framework assesses functional correctness, syntactic validity, and execution success across varying challenge difficulties. Results indicate that RAG-enhanced models, supplemented with an augmented PennyLane dataset, approximately generate similar results as the standard prompting, particularly in complex quantum algorithms. Additionally, we introduce a multi-agent evaluation pipeline that iteratively refines incorrect solutions, further enhancing execution success rates. To foster further research, we commit to publicly releasing QHackBench, along with our evaluation framework and experimental results, enabling continued advancements in AI-assisted quantum programming.",
    "categories": [
      "cs.AI",
      "cs.PL",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "To appear at the IEEE International Conference on Quantum Artificial Intelligence (QAI), Naples, Italy, November 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.20008v2",
    "published_date": "2025-06-24 20:54:56 UTC",
    "updated_date": "2025-08-29 07:06:23 UTC"
  },
  {
    "arxiv_id": "2506.19997v4",
    "title": "TRACED: Transition-aware Regret Approximation with Co-learnability for Environment Design",
    "authors": [
      "Geonwoo Cho",
      "Jaegyun Im",
      "Jihwan Lee",
      "Hojun Yi",
      "Sejin Kim",
      "Sundong Kim"
    ],
    "abstract": "Generalizing deep reinforcement learning agents to unseen environments remains a significant challenge. One promising solution is Unsupervised Environment Design (UED), a co-evolutionary framework in which a teacher adaptively generates tasks with high learning potential, while a student learns a robust policy from this evolving curriculum. Existing UED methods typically measure learning potential via regret, the gap between optimal and current performance, approximated solely by value-function loss. Building on these approaches, we introduce the transition-prediction error as an additional term in our regret approximation. To capture how training on one task affects performance on others, we further propose a lightweight metric called Co-Learnability. By combining these two measures, we present Transition-aware Regret Approximation with Co-learnability for Environment Design (TRACED). Empirical evaluations show that TRACED produces curricula that improve zero-shot generalization over strong baselines across multiple benchmarks. Ablation studies confirm that the transition-prediction error drives rapid complexity ramp-up and that Co-Learnability delivers additional gains when paired with the transition-prediction error. These results demonstrate how refined regret approximation and explicit modeling of task relationships can be leveraged for sample-efficient curriculum design in UED. Project Page: https://geonwoo.me/traced/",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19997v4",
    "published_date": "2025-06-24 20:29:24 UTC",
    "updated_date": "2025-12-02 20:05:28 UTC"
  },
  {
    "arxiv_id": "2506.21628v2",
    "title": "Ark: An Open-source Python-based Framework for Robot Learning",
    "authors": [
      "Magnus Dierking",
      "Christopher E. Mower",
      "Sarthak Das",
      "Huang Helong",
      "Jiacheng Qiu",
      "Cody Reading",
      "Wei Chen",
      "Huidong Liang",
      "Huang Guowei",
      "Jan Peters",
      "Quan Xingyue",
      "Jun Wang",
      "Haitham Bou-Ammar"
    ],
    "abstract": "Robotics has made remarkable hardware strides-from DARPA's Urban and Robotics Challenges to the first humanoid-robot kickboxing tournament-yet commercial autonomy still lags behind progress in machine learning. A major bottleneck is software: current robot stacks demand steep learning curves, low-level C/C++ expertise, fragmented tooling, and intricate hardware integration, in stark contrast to the Python-centric, well-documented ecosystems that propelled modern AI. We introduce ARK, an open-source, Python-first robotics framework designed to close that gap. ARK presents a Gym-style environment interface that allows users to collect data, preprocess it, and train policies using state-of-the-art imitation-learning algorithms (e.g., ACT, Diffusion Policy) while seamlessly toggling between high-fidelity simulation and physical robots. A lightweight client-server architecture provides networked publisher-subscriber communication, and optional C/C++ bindings ensure real-time performance when needed. ARK ships with reusable modules for control, SLAM, motion planning, system identification, and visualization, along with native ROS interoperability. Comprehensive documentation and case studies-from manipulation to mobile navigation-demonstrate rapid prototyping, effortless hardware swapping, and end-to-end pipelines that rival the convenience of mainstream machine-learning workflows. By unifying robotics and AI practices under a common Python umbrella, ARK lowers entry barriers and accelerates research and commercial deployment of autonomous robots.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.21628v2",
    "published_date": "2025-06-24 20:23:39 UTC",
    "updated_date": "2025-07-14 17:46:29 UTC"
  },
  {
    "arxiv_id": "2506.19992v2",
    "title": "HERCULES: Hierarchical Embedding-based Recursive Clustering Using LLMs for Efficient Summarization",
    "authors": [
      "Gabor Petnehazi",
      "Bernadett Aradi"
    ],
    "abstract": "The explosive growth of complex datasets across various modalities necessitates advanced analytical tools that not only group data effectively but also provide human-understandable insights into the discovered structures. We introduce HERCULES (Hierarchical Embedding-based Recursive Clustering Using LLMs for Efficient Summarization), a novel algorithm and Python package designed for hierarchical k-means clustering of diverse data types, including text, images, and numeric data (processed one modality per run). HERCULES constructs a cluster hierarchy by recursively applying k-means clustering, starting from individual data points at level 0. A key innovation is its deep integration of Large Language Models (LLMs) to generate semantically rich titles and descriptions for clusters at each level of the hierarchy, significantly enhancing interpretability. The algorithm supports two main representation modes: `direct' mode, which clusters based on original data embeddings or scaled numeric features, and `description' mode, which clusters based on embeddings derived from LLM-generated summaries. Users can provide a `topic\\_seed' to guide LLM-generated summaries towards specific themes. An interactive visualization tool facilitates thorough analysis and understanding of the clustering results. We demonstrate HERCULES's capabilities and discuss its potential for extracting meaningful, hierarchical knowledge from complex datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19992v2",
    "published_date": "2025-06-24 20:22:00 UTC",
    "updated_date": "2025-09-03 05:10:20 UTC"
  },
  {
    "arxiv_id": "2507.02910v1",
    "title": "Causal-Paced Deep Reinforcement Learning",
    "authors": [
      "Geonwoo Cho",
      "Jaegyun Im",
      "Doyoon Kim",
      "Sundong Kim"
    ],
    "abstract": "Designing effective task sequences is crucial for curriculum reinforcement learning (CRL), where agents must gradually acquire skills by training on intermediate tasks. A key challenge in CRL is to identify tasks that promote exploration, yet are similar enough to support effective transfer. While recent approach suggests comparing tasks via their Structural Causal Models (SCMs), the method requires access to ground-truth causal structures, an unrealistic assumption in most RL settings. In this work, we propose Causal-Paced Deep Reinforcement Learning (CP-DRL), a curriculum learning framework aware of SCM differences between tasks based on interaction data approximation. This signal captures task novelty, which we combine with the agent's learnability, measured by reward gain, to form a unified objective. Empirically, CP-DRL outperforms existing curriculum methods on the Point Mass benchmark, achieving faster convergence and higher returns. CP-DRL demonstrates reduced variance with comparable final returns in the Bipedal Walker-Trivial setting, and achieves the highest average performance in the Infeasible variant. These results indicate that leveraging causal relationships between tasks can improve the structure-awareness and sample efficiency of curriculum reinforcement learning. We provide the full implementation of CP-DRL to facilitate the reproduction of our main results at https://github.com/Cho-Geonwoo/CP-DRL.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Workshop on Causal Reinforcement Learning, Reinforcement Learning Conference (RLC) 2025",
    "pdf_url": "https://arxiv.org/pdf/2507.02910v1",
    "published_date": "2025-06-24 20:15:01 UTC",
    "updated_date": "2025-06-24 20:15:01 UTC"
  },
  {
    "arxiv_id": "2506.19977v1",
    "title": "Context Attribution with Multi-Armed Bandit Optimization",
    "authors": [
      "Deng Pan",
      "Keerthiram Murugesan",
      "Nuno Moniz",
      "Nitesh Chawla"
    ],
    "abstract": "Understanding which parts of the retrieved context contribute to a large language model's generated answer is essential for building interpretable and trustworthy generative QA systems. We propose a novel framework that formulates context attribution as a combinatorial multi-armed bandit (CMAB) problem. Each context segment is treated as a bandit arm, and we employ Combinatorial Thompson Sampling (CTS) to efficiently explore the exponentially large space of context subsets under a limited query budget. Our method defines a reward function based on normalized token likelihoods, capturing how well a subset of segments supports the original model response. Unlike traditional perturbation-based attribution methods such as SHAP, which sample subsets uniformly and incur high computational costs, our approach adaptively balances exploration and exploitation by leveraging posterior estimates of segment relevance. This leads to substantially improved query efficiency while maintaining high attribution fidelity. Extensive experiments on diverse datasets and LLMs demonstrate that our method achieves competitive attribution quality with fewer model queries.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19977v1",
    "published_date": "2025-06-24 19:47:27 UTC",
    "updated_date": "2025-06-24 19:47:27 UTC"
  },
  {
    "arxiv_id": "2506.19975v1",
    "title": "VoxelOpt: Voxel-Adaptive Message Passing for Discrete Optimization in Deformable Abdominal CT Registration",
    "authors": [
      "Hang Zhang",
      "Yuxi Zhang",
      "Jiazheng Wang",
      "Xiang Chen",
      "Renjiu Hu",
      "Xin Tian",
      "Gaolei Li",
      "Min Liu"
    ],
    "abstract": "Recent developments in neural networks have improved deformable image registration (DIR) by amortizing iterative optimization, enabling fast and accurate DIR results. However, learning-based methods often face challenges with limited training data, large deformations, and tend to underperform compared to iterative approaches when label supervision is unavailable. While iterative methods can achieve higher accuracy in such scenarios, they are considerably slower than learning-based methods. To address these limitations, we propose VoxelOpt, a discrete optimization-based DIR framework that combines the strengths of learning-based and iterative methods to achieve a better balance between registration accuracy and runtime. VoxelOpt uses displacement entropy from local cost volumes to measure displacement signal strength at each voxel, which differs from earlier approaches in three key aspects. First, it introduces voxel-wise adaptive message passing, where voxels with lower entropy receives less influence from their neighbors. Second, it employs a multi-level image pyramid with 27-neighbor cost volumes at each level, avoiding exponential complexity growth. Third, it replaces hand-crafted features or contrastive learning with a pretrained foundational segmentation model for feature extraction. In abdominal CT registration, these changes allow VoxelOpt to outperform leading iterative in both efficiency and accuracy, while matching state-of-the-art learning-based methods trained with label supervision. The source code will be available at https://github.com/tinymilky/VoxelOpt",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "eess.SP"
    ],
    "primary_category": "eess.IV",
    "comment": "Accepted for publication at MICCAI 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.19975v1",
    "published_date": "2025-06-24 19:44:04 UTC",
    "updated_date": "2025-06-24 19:44:04 UTC"
  },
  {
    "arxiv_id": "2506.19973v1",
    "title": "Quantum Neural Networks for Propensity Score Estimation and Survival Analysis in Observational Biomedical Studies",
    "authors": [
      "Vojtěch Novák",
      "Ivan Zelinka",
      "Lenka Přibylová",
      "Lubomír Martínek"
    ],
    "abstract": "This study investigates the application of quantum neural networks (QNNs) for propensity score estimation to address selection bias in comparing survival outcomes between laparoscopic and open surgical techniques in a cohort of 1177 colorectal carcinoma patients treated at University Hospital Ostrava (2001-2009). Using a dataset with 77 variables, including patient demographics and tumor characteristics, we developed QNN-based propensity score models focusing on four key covariates (Age, Sex, Stage, BMI). The QNN architecture employed a linear ZFeatureMap for data encoding, a SummedPaulis operator for predictions, and the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) for robust, gradient-free optimization in noisy quantum environments. Variance regularization was integrated to mitigate quantum measurement noise, with simulations conducted under exact, sampling (1024 shots), and noisy hardware (FakeManhattanV2) conditions. QNNs, particularly with simulated hardware noise, outperformed classical logistic regression and gradient boosted machines in small samples (AUC up to 0.750 for n=100), with noise modeling enhancing predictive stability. Propensity score matching and weighting, optimized via genetic matching and matching weights, achieved covariate balance with standardized mean differences of 0.0849 and 0.0869, respectively. Survival analyses using Kaplan-Meier estimation, Cox proportional hazards, and Aalen additive regression revealed no significant survival differences post-adjustment (p-values 0.287-0.851), indicating confounding bias in unadjusted outcomes. These results highlight QNNs' potential, enhanced by CMA-ES and noise-aware strategies, to improve causal inference in biomedical research, particularly for small-sample, high-dimensional datasets.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "quant-ph",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19973v1",
    "published_date": "2025-06-24 19:40:39 UTC",
    "updated_date": "2025-06-24 19:40:39 UTC"
  },
  {
    "arxiv_id": "2506.19967v1",
    "title": "Inference Scaled GraphRAG: Improving Multi Hop Question Answering on Knowledge Graphs",
    "authors": [
      "Travis Thompson",
      "Seung-Hwan Lim",
      "Paul Liu",
      "Ruoying He",
      "Dongkuan Xu"
    ],
    "abstract": "Large Language Models (LLMs) have achieved impressive capabilities in language understanding and generation, yet they continue to underperform on knowledge-intensive reasoning tasks due to limited access to structured context and multi-hop information. Retrieval-Augmented Generation (RAG) partially mitigates this by grounding generation in retrieved context, but conventional RAG and GraphRAG methods often fail to capture relational structure across nodes in knowledge graphs. We introduce Inference-Scaled GraphRAG, a novel framework that enhances LLM-based graph reasoning by applying inference-time compute scaling. Our method combines sequential scaling with deep chain-of-thought graph traversal, and parallel scaling with majority voting over sampled trajectories within an interleaved reasoning-execution loop. Experiments on the GRBench benchmark demonstrate that our approach significantly improves multi-hop question answering performance, achieving substantial gains over both traditional GraphRAG and prior graph traversal baselines. These findings suggest that inference-time scaling is a practical and architecture-agnostic solution for structured knowledge reasoning with LLMs",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19967v1",
    "published_date": "2025-06-24 19:31:03 UTC",
    "updated_date": "2025-06-24 19:31:03 UTC"
  },
  {
    "arxiv_id": "2506.19960v1",
    "title": "An ab initio foundation model of wavefunctions that accurately describes chemical bond breaking",
    "authors": [
      "Adam Foster",
      "Zeno Schätzle",
      "P. Bernát Szabó",
      "Lixue Cheng",
      "Jonas Köhler",
      "Gino Cassella",
      "Nicholas Gao",
      "Jiawei Li",
      "Frank Noé",
      "Jan Hermann"
    ],
    "abstract": "Reliable description of bond breaking remains a major challenge for quantum chemistry due to the multireferential character of the electronic structure in dissociating species. Multireferential methods in particular suffer from large computational cost, which under the normal paradigm has to be paid anew for each system at a full price, ignoring commonalities in electronic structure across molecules. Quantum Monte Carlo with deep neural networks (deep QMC) uniquely offers to exploit such commonalities by pretraining transferable wavefunction models, but all such attempts were so far limited in scope. Here, we bring this new paradigm to fruition with Orbformer, a novel transferable wavefunction model pretrained on 22,000 equilibrium and dissociating structures that can be fine-tuned on unseen molecules reaching an accuracy-cost ratio rivalling classical multireferential methods. On established benchmarks as well as more challenging bond dissociations and Diels-Alder reactions, Orbformer is the only method that consistently converges to chemical accuracy (1 kcal/mol). This work turns the idea of amortizing the cost of solving the Schrödinger equation over many molecules into a practical approach in quantum chemistry.",
    "categories": [
      "physics.chem-ph",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "physics.chem-ph",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19960v1",
    "published_date": "2025-06-24 19:12:45 UTC",
    "updated_date": "2025-06-24 19:12:45 UTC"
  },
  {
    "arxiv_id": "2506.19952v2",
    "title": "CycleDistill: Bootstrapping Machine Translation using LLMs with Cyclical Distillation",
    "authors": [
      "Deepon Halder",
      "Thanmay Jayakumar",
      "Raj Dabre"
    ],
    "abstract": "Large language models (LLMs), despite their ability to perform few-shot machine translation (MT), often lag behind dedicated MT systems trained on parallel corpora, which are crucial for high quality machine translation (MT). However, parallel corpora are often scarce or non-existent for low-resource languages. In this paper, we propose CycleDistill, a bootstrapping approach leveraging LLMs and few-shot translation to obtain high-quality MT systems. CycleDistill involves iteratively generating synthetic parallel corpora from monolingual corpora via zero- or few-shot MT, which is then used to fine-tune the model that was used for generating said data for MT. CycleDistill does not need parallel corpora beyond 1 to 4 few-shot examples, and in our experiments focusing on three Indian languages, by relying solely on monolingual corpora, it can achieve high-quality machine translation, improving upon a few-shot baseline model by over 20-30 chrF points on average in the first iteration. We also study the effect of leveraging softmax activations during the distillation process and observe mild improvements in translation quality.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19952v2",
    "published_date": "2025-06-24 18:56:57 UTC",
    "updated_date": "2025-08-09 05:28:53 UTC"
  },
  {
    "arxiv_id": "2506.22492v1",
    "title": "Report on NSF Workshop on Science of Safe AI",
    "authors": [
      "Rajeev Alur",
      "Greg Durrett",
      "Hadas Kress-Gazit",
      "Corina Păsăreanu",
      "René Vidal"
    ],
    "abstract": "Recent advances in machine learning, particularly the emergence of foundation models, are leading to new opportunities to develop technology-based solutions to societal problems. However, the reasoning and inner workings of today's complex AI models are not transparent to the user, and there are no safety guarantees regarding their predictions. Consequently, to fulfill the promise of AI, we must address the following scientific challenge: how to develop AI-based systems that are not only accurate and performant but also safe and trustworthy?\n  The criticality of safe operation is particularly evident for autonomous systems for control and robotics, and was the catalyst for the Safe Learning Enabled Systems (SLES) program at NSF. For the broader class of AI applications, such as users interacting with chatbots and clinicians receiving treatment recommendations, safety is, while no less important, less well-defined with context-dependent interpretations. This motivated the organization of a day-long workshop, held at University of Pennsylvania on February 26, 2025, to bring together investigators funded by the NSF SLES program with a broader pool of researchers studying AI safety. This report is the result of the discussions in the working groups that addressed different aspects of safety at the workshop. The report articulates a new research agenda focused on developing theory, methods, and tools that will provide the foundations of the next generation of AI-enabled systems.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.22492v1",
    "published_date": "2025-06-24 18:55:29 UTC",
    "updated_date": "2025-06-24 18:55:29 UTC"
  },
  {
    "arxiv_id": "2506.20685v2",
    "title": "Progressive Size-Adaptive Federated Learning: A Comprehensive Framework for Heterogeneous Multi-Modal Data Systems",
    "authors": [
      "Sajid Hussain",
      "Muhammad Sohail",
      "Nauman Ali Khan",
      "Naima Iltaf",
      "Ihtesham ul Islam"
    ],
    "abstract": "Federated Learning (FL) has emerged as a transformative paradigm for distributed machine learning while preserving data privacy. However, existing approaches predominantly focus on model heterogeneity and aggregation techniques, largely overlooking the fundamental impact of dataset size characteristics on federated training dynamics. This paper introduces Size-Based Adaptive Federated Learning (SAFL), a novel progressive training framework that systematically organizes federated learning based on dataset size characteristics across heterogeneous multi-modal data. Our comprehensive experimental evaluation across 13 diverse datasets spanning 7 modalities (vision, text, time series, audio, sensor, medical vision, and multimodal) reveals critical insights: 1) an optimal dataset size range of 1000-1500 samples for federated learning effectiveness; 2) a clear modality performance hierarchy with structured data (time series, sensor) significantly outperforming unstructured data (text, multimodal); and 3) systematic performance degradation for large datasets exceeding 2000 samples. SAFL achieves an average accuracy of 87.68% across all datasets, with structured data modalities reaching 99%+ accuracy. The framework demonstrates superior communication efficiency, reducing total data transfer to 7.38 GB across 558 communications while maintaining high performance. Our real-time monitoring framework provides unprecedented insights into system resource utilization, network efficiency, and training dynamics. This work fills critical gaps in understanding how data characteristics should drive federated learning strategies, providing both theoretical insights and practical guidance for real-world FL deployments in neural network and learning systems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Due to some technical issues",
    "pdf_url": "https://arxiv.org/pdf/2506.20685v2",
    "published_date": "2025-06-24 18:50:33 UTC",
    "updated_date": "2025-09-21 04:50:02 UTC"
  },
  {
    "arxiv_id": "2507.00048v1",
    "title": "A collaborative digital twin built on FAIR data and compute infrastructure",
    "authors": [
      "Thomas M. Deucher",
      "Juan C. Verduzco",
      "Michael Titus",
      "Alejandro Strachan"
    ],
    "abstract": "The integration of machine learning with automated experimentation in self-driving laboratories (SDL) offers a powerful approach to accelerate discovery and optimization tasks in science and engineering applications. When supported by findable, accessible, interoperable, and reusable (FAIR) data infrastructure, SDLs with overlapping interests can collaborate more effectively. This work presents a distributed SDL implementation built on nanoHUB services for online simulation and FAIR data management. In this framework, geographically dispersed collaborators conducting independent optimization tasks contribute raw experimental data to a shared central database. These researchers can then benefit from analysis tools and machine learning models that automatically update as additional data become available. New data points are submitted through a simple web interface and automatically processed using a nanoHUB Sim2L, which extracts derived quantities and indexes all inputs and outputs in a FAIR data repository called ResultsDB. A separate nanoHUB workflow enables sequential optimization using active learning, where researchers define the optimization objective, and machine learning models are trained on-the-fly with all existing data, guiding the selection of future experiments. Inspired by the concept of ``frugal twin\", the optimization task seeks to find the optimal recipe to combine food dyes to achieve the desired target color. With easily accessible and inexpensive materials, researchers and students can set up their own experiments, share data with collaborators, and explore the combination of FAIR data, predictive ML models, and sequential optimization. The tools introduced are generally applicable and can easily be extended to other optimization problems.",
    "categories": [
      "cs.AI",
      "cond-mat.mtrl-sci",
      "cs.CE",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2507.00048v1",
    "published_date": "2025-06-24 18:13:52 UTC",
    "updated_date": "2025-06-24 18:13:52 UTC"
  },
  {
    "arxiv_id": "2506.19923v4",
    "title": "Prover Agent: An Agent-Based Framework for Formal Mathematical Proofs",
    "authors": [
      "Kaito Baba",
      "Chaoran Liu",
      "Shuhei Kurita",
      "Akiyoshi Sannai"
    ],
    "abstract": "We present Prover Agent, a novel AI agent for automated theorem proving that integrates large language models (LLMs) with a formal proof assistant, Lean. Prover Agent coordinates an informal reasoning LLM, a formal prover model, and feedback from Lean while also generating auxiliary lemmas. These auxiliary lemmas are not limited to subgoals in the formal proof but can also include special cases or potentially useful facts derived from the assumptions, which help in discovering a viable proof strategy. It achieves an 88.1% success rate on the MiniF2F benchmark, establishing a new state-of-the-art among methods using small language models (SLMs) with a much lower sample budget than previous approaches. We also present theoretical analyses and case studies that illustrate how these generated lemmas contribute to solving challenging problems. Our code is publicly available at: https://github.com/kAIto47802/Prover-Agent.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "36 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.19923v4",
    "published_date": "2025-06-24 18:01:52 UTC",
    "updated_date": "2025-10-23 16:48:37 UTC"
  },
  {
    "arxiv_id": "2506.19852v2",
    "title": "Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation",
    "authors": [
      "Xingyang Li",
      "Muyang Li",
      "Tianle Cai",
      "Haocheng Xi",
      "Shuo Yang",
      "Yujun Lin",
      "Lvmin Zhang",
      "Songlin Yang",
      "Jinbo Hu",
      "Kelly Peng",
      "Maneesh Agrawala",
      "Ion Stoica",
      "Kurt Keutzer",
      "Song Han"
    ],
    "abstract": "Recent advances in diffusion models have enabled high-quality video generation, but the additional temporal dimension significantly increases computational costs, making training and inference on long videos prohibitively expensive. In this paper, we identify a phenomenon we term Spatiotemporal Energy Decay in video diffusion models: post-softmax attention scores diminish as spatial and temporal distance between tokens increase, akin to the physical decay of signal or waves over space and time in nature. Motivated by this, we propose Radial Attention, a scalable sparse attention mechanism with $\\mathcal{O}(n \\log n)$ complexity that translates energy decay into exponentially decaying compute density, which is significantly more efficient than standard $\\mathcal{O}(n^2)$ dense attention and more expressive than linear attention. Specifically, Radial Attention employs a simple, static attention mask where each token attends to spatially nearby tokens, with the attention window size shrinking with temporal distance. Moreover, it allows pre-trained video diffusion models to extend their generation length with efficient LoRA-based fine-tuning. Extensive experiments show that Radial Attention maintains video quality across Wan2.1-14B, HunyuanVideo, and Mochi 1, achieving up to a 1.9$\\times$ speedup over the original dense attention. With minimal tuning, it enables video generation up to 4$\\times$ longer while reducing training costs by up to 4.4$\\times$ compared to direct fine-tuning and accelerating inference by up to 3.7$\\times$ compared to dense attention inference. Code is released at \\href{https://github.com/mit-han-lab/radial-attention}{https://github.com/mit-han-lab/radial-attention}.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to NeurIPS 2025, Code: https://github.com/mit-han-lab/radial-attention",
    "pdf_url": "https://arxiv.org/pdf/2506.19852v2",
    "published_date": "2025-06-24 17:59:59 UTC",
    "updated_date": "2025-12-06 06:53:10 UTC"
  },
  {
    "arxiv_id": "2506.19847v2",
    "title": "Orthogonal Finetuning Made Scalable",
    "authors": [
      "Zeju Qiu",
      "Weiyang Liu",
      "Adrian Weller",
      "Bernhard Schölkopf"
    ],
    "abstract": "Orthogonal finetuning (OFT) offers highly parameter-efficient adaptation while preventing catastrophic forgetting, but its high runtime and memory demands limit practical deployment. We identify the core computational bottleneck in OFT as its weight-centric implementation, which relies on costly matrix-matrix multiplications with cubic complexity. To overcome this, we propose OFTv2, an input-centric reformulation that instead uses matrix-vector multiplications (i.e., matrix-free computation), reducing the computational cost to quadratic. We further introduce the Cayley-Neumann parameterization, an efficient orthogonal parameterization that approximates the matrix inversion in the Cayley transform via a truncated Neumann series. These modifications allow OFTv2 to achieve up to 10x faster training and 3x lower GPU memory usage without compromising performance. In addition, we extend OFTv2 to support finetuning quantized foundation models and show that it outperforms the popular QLoRA in training stability, efficiency, and memory usage.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "EMNLP 2025 Main (18 pages, 7 figures, project page: https://spherelab.ai/oftv2/)",
    "pdf_url": "https://arxiv.org/pdf/2506.19847v2",
    "published_date": "2025-06-24 17:59:49 UTC",
    "updated_date": "2025-10-14 18:19:55 UTC"
  },
  {
    "arxiv_id": "2506.19846v1",
    "title": "JoyAgents-R1: Joint Evolution Dynamics for Versatile Multi-LLM Agents with Reinforcement Learning",
    "authors": [
      "Ai Han",
      "Junxing Hu",
      "Pu Wei",
      "Zhiqian Zhang",
      "Yuhang Guo",
      "Jiawei Lu",
      "Zicheng Zhang"
    ],
    "abstract": "Multi-agent reinforcement learning (MARL) has emerged as a prominent paradigm for increasingly complex tasks. However, joint evolution across heterogeneous agents remains challenging due to cooperative inefficiency and training instability. In this paper, we propose the joint evolution dynamics for MARL called JoyAgents-R1, which first applies Group Relative Policy Optimization (GRPO) to the joint training of heterogeneous multi-agents. By iteratively refining agents' large language models (LLMs) and memories, the method achieves holistic equilibrium with optimal decision-making and memory capabilities. Specifically, JoyAgents-R1 first implements node-wise Monte Carlo sampling on the behavior of each agent across entire reasoning trajectories to enhance GRPO sampling efficiency while maintaining policy diversity. Then, our marginal benefit-driven selection strategy identifies top-$K$ sampling groups with maximal reward fluctuations, enabling targeted agent model updates that improve training stability and maximize joint benefits through cost-effective parameter adjustments. Meanwhile, JoyAgents-R1 introduces an adaptive memory evolution mechanism that repurposes GRPO rewards as cost-free supervisory signals to eliminate repetitive reasoning and accelerate convergence. Experiments across general and domain-specific scenarios demonstrate that JoyAgents-R1 achieves performance comparable to that of larger LLMs while built on smaller open-source models.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "33 pages, 7 figures, under review",
    "pdf_url": "https://arxiv.org/pdf/2506.19846v1",
    "published_date": "2025-06-24 17:59:31 UTC",
    "updated_date": "2025-06-24 17:59:31 UTC"
  },
  {
    "arxiv_id": "2506.19843v1",
    "title": "Temporal-IRL: Modeling Port Congestion and Berth Scheduling with Inverse Reinforcement Learning",
    "authors": [
      "Guo Li",
      "Zixiang Xu",
      "Wei Zhang",
      "Yikuan Hu",
      "Xinyu Yang",
      "Nikolay Aristov",
      "Mingjie Tang",
      "Elenna R Dugundji"
    ],
    "abstract": "Predicting port congestion is crucial for maintaining reliable global supply chains. Accurate forecasts enableimprovedshipment planning, reducedelaysand costs, and optimizeinventoryanddistributionstrategies, thereby ensuring timely deliveries and enhancing supply chain resilience. To achieve accurate predictions, analyzing vessel behavior and their stay times at specific port terminals is essential, focusing particularly on berth scheduling under various conditions. Crucially, the model must capture and learn the underlying priorities and patterns of berth scheduling. Berth scheduling and planning are influenced by a range of factors, including incoming vessel size, waiting times, and the status of vessels within the port terminal. By observing historical Automatic Identification System (AIS) positions of vessels, we reconstruct berth schedules, which are subsequently utilized to determine the reward function via Inverse Reinforcement Learning (IRL). For this purpose, we modeled a specific terminal at the Port of New York/New Jersey and developed Temporal-IRL. This Temporal-IRL model learns berth scheduling to predict vessel sequencing at the terminal and estimate vessel port stay, encompassing both waiting and berthing times, to forecast port congestion. Utilizing data from Maher Terminal spanning January 2015 to September 2023, we trained and tested the model, achieving demonstrably excellent results.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "TRB2025",
    "pdf_url": "https://arxiv.org/pdf/2506.19843v1",
    "published_date": "2025-06-24 17:59:12 UTC",
    "updated_date": "2025-06-24 17:59:12 UTC"
  },
  {
    "arxiv_id": "2506.19842v1",
    "title": "ManiGaussian++: General Robotic Bimanual Manipulation with Hierarchical Gaussian World Model",
    "authors": [
      "Tengbo Yu",
      "Guanxing Lu",
      "Zaijia Yang",
      "Haoyuan Deng",
      "Season Si Chen",
      "Jiwen Lu",
      "Wenbo Ding",
      "Guoqiang Hu",
      "Yansong Tang",
      "Ziwei Wang"
    ],
    "abstract": "Multi-task robotic bimanual manipulation is becoming increasingly popular as it enables sophisticated tasks that require diverse dual-arm collaboration patterns. Compared to unimanual manipulation, bimanual tasks pose challenges to understanding the multi-body spatiotemporal dynamics. An existing method ManiGaussian pioneers encoding the spatiotemporal dynamics into the visual representation via Gaussian world model for single-arm settings, which ignores the interaction of multiple embodiments for dual-arm systems with significant performance drop. In this paper, we propose ManiGaussian++, an extension of ManiGaussian framework that improves multi-task bimanual manipulation by digesting multi-body scene dynamics through a hierarchical Gaussian world model. To be specific, we first generate task-oriented Gaussian Splatting from intermediate visual features, which aims to differentiate acting and stabilizing arms for multi-body spatiotemporal dynamics modeling. We then build a hierarchical Gaussian world model with the leader-follower architecture, where the multi-body spatiotemporal dynamics is mined for intermediate visual representation via future scene prediction. The leader predicts Gaussian Splatting deformation caused by motions of the stabilizing arm, through which the follower generates the physical consequences resulted from the movement of the acting arm. As a result, our method significantly outperforms the current state-of-the-art bimanual manipulation techniques by an improvement of 20.2% in 10 simulated tasks, and achieves 60% success rate on average in 9 challenging real-world tasks. Our code is available at https://github.com/April-Yz/ManiGaussian_Bimanual.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19842v1",
    "published_date": "2025-06-24 17:59:06 UTC",
    "updated_date": "2025-06-24 17:59:06 UTC"
  },
  {
    "arxiv_id": "2506.19839v1",
    "title": "Improving Progressive Generation with Decomposable Flow Matching",
    "authors": [
      "Moayed Haji-Ali",
      "Willi Menapace",
      "Ivan Skorokhodov",
      "Arpit Sahni",
      "Sergey Tulyakov",
      "Vicente Ordonez",
      "Aliaksandr Siarohin"
    ],
    "abstract": "Generating high-dimensional visual modalities is a computationally intensive task. A common solution is progressive generation, where the outputs are synthesized in a coarse-to-fine spectral autoregressive manner. While diffusion models benefit from the coarse-to-fine nature of denoising, explicit multi-stage architectures are rarely adopted. These architectures have increased the complexity of the overall approach, introducing the need for a custom diffusion formulation, decomposition-dependent stage transitions, add-hoc samplers, or a model cascade. Our contribution, Decomposable Flow Matching (DFM), is a simple and effective framework for the progressive generation of visual media. DFM applies Flow Matching independently at each level of a user-defined multi-scale representation (such as Laplacian pyramid). As shown by our experiments, our approach improves visual quality for both images and videos, featuring superior results compared to prior multistage frameworks. On Imagenet-1k 512px, DFM achieves 35.2% improvements in FDD scores over the base architecture and 26.4% over the best-performing baseline, under the same training compute. When applied to finetuning of large models, such as FLUX, DFM shows faster convergence speed to the training distribution. Crucially, all these advantages are achieved with a single model, architectural simplicity, and minimal modifications to existing training pipelines.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Webpage: https://snap-research.github.io/dfm/",
    "pdf_url": "https://arxiv.org/pdf/2506.19839v1",
    "published_date": "2025-06-24 17:58:02 UTC",
    "updated_date": "2025-06-24 17:58:02 UTC"
  },
  {
    "arxiv_id": "2506.19834v1",
    "title": "A standard transformer and attention with linear biases for molecular conformer generation",
    "authors": [
      "Viatcheslav Gurev",
      "Timothy Rumbell"
    ],
    "abstract": "Sampling low-energy molecular conformations, spatial arrangements of atoms in a molecule, is a critical task for many different calculations performed in the drug discovery and optimization process. Numerous specialized equivariant networks have been designed to generate molecular conformations from 2D molecular graphs. Recently, non-equivariant transformer models have emerged as a viable alternative due to their capability to scale to improve generalization. However, the concern has been that non-equivariant models require a large model size to compensate the lack of equivariant bias. In this paper, we demonstrate that a well-chosen positional encoding effectively addresses these size limitations. A standard transformer model incorporating relative positional encoding for molecular graphs when scaled to 25 million parameters surpasses the current state-of-the-art non-equivariant base model with 64 million parameters on the GEOM-DRUGS benchmark. We implemented relative positional encoding as a negative attention bias that linearly increases with the shortest path distances between graph nodes at varying slopes for different attention heads, similar to ALiBi, a widely adopted relative positional encoding technique in the NLP domain. This architecture has the potential to serve as a foundation for a novel class of generative models for molecular conformations.",
    "categories": [
      "q-bio.BM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.BM",
    "comment": "Revision of paper at OpenReview: https://openreview.net/forum?id=BjjerMYL3F",
    "pdf_url": "https://arxiv.org/pdf/2506.19834v1",
    "published_date": "2025-06-24 17:50:49 UTC",
    "updated_date": "2025-06-24 17:50:49 UTC"
  },
  {
    "arxiv_id": "2506.19825v1",
    "title": "Evaluating Compliance with Visualization Guidelines in Diagrams for Scientific Publications Using Large Vision Language Models",
    "authors": [
      "Johannes Rückert",
      "Louise Bloch",
      "Christoph M. Friedrich"
    ],
    "abstract": "Diagrams are widely used to visualize data in publications. The research field of data visualization deals with defining principles and guidelines for the creation and use of these diagrams, which are often not known or adhered to by researchers, leading to misinformation caused by providing inaccurate or incomplete information.\n  In this work, large Vision Language Models (VLMs) are used to analyze diagrams in order to identify potential problems in regards to selected data visualization principles and guidelines. To determine the suitability of VLMs for these tasks, five open source VLMs and five prompting strategies are compared using a set of questions derived from selected data visualization guidelines.\n  The results show that the employed VLMs work well to accurately analyze diagram types (F1-score 82.49 %), 3D effects (F1-score 98.55 %), axes labels (F1-score 76.74 %), lines (RMSE 1.16), colors (RMSE 1.60) and legends (F1-score 96.64 %, RMSE 0.70), while they cannot reliably provide feedback about the image quality (F1-score 0.74 %) and tick marks/labels (F1-score 46.13 %). Among the employed VLMs, Qwen2.5VL performs best, and the summarizing prompting strategy performs best for most of the experimental questions.\n  It is shown that VLMs can be used to automatically identify a number of potential issues in diagrams, such as missing axes labels, missing legends, and unnecessary 3D effects. The approach laid out in this work can be extended for further aspects of data visualization.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at ICDAR 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.19825v1",
    "published_date": "2025-06-24 17:42:36 UTC",
    "updated_date": "2025-06-24 17:42:36 UTC"
  },
  {
    "arxiv_id": "2506.19823v2",
    "title": "Persona Features Control Emergent Misalignment",
    "authors": [
      "Miles Wang",
      "Tom Dupré la Tour",
      "Olivia Watkins",
      "Alex Makelov",
      "Ryan A. Chi",
      "Samuel Miserendino",
      "Jeffrey Wang",
      "Achyuta Rajaram",
      "Johannes Heidecke",
      "Tejal Patwardhan",
      "Dan Mossing"
    ],
    "abstract": "Understanding how language models generalize behaviors from their training to a broader deployment distribution is an important problem in AI safety. Betley et al. discovered that fine-tuning GPT-4o on intentionally insecure code causes \"emergent misalignment,\" where models give stereotypically malicious responses to unrelated prompts. We extend this work, demonstrating emergent misalignment across diverse conditions, including reinforcement learning on reasoning models, fine-tuning on various synthetic datasets, and in models without safety training. To investigate the mechanisms behind this generalized misalignment, we apply a \"model diffing\" approach using sparse autoencoders to compare internal model representations before and after fine-tuning. This approach reveals several \"misaligned persona\" features in activation space, including a toxic persona feature which most strongly controls emergent misalignment and can be used to predict whether a model will exhibit such behavior. Additionally, we investigate mitigation strategies, discovering that fine-tuning an emergently misaligned model on just a few hundred benign samples efficiently restores alignment.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19823v2",
    "published_date": "2025-06-24 17:38:21 UTC",
    "updated_date": "2025-10-06 23:33:09 UTC"
  },
  {
    "arxiv_id": "2506.20683v1",
    "title": "Global and Local Contrastive Learning for Joint Representations from Cardiac MRI and ECG",
    "authors": [
      "Alexander Selivanov",
      "Philip Müller",
      "Özgün Turgut",
      "Nil Stolt-Ansó",
      "Daniel Rückert"
    ],
    "abstract": "An electrocardiogram (ECG) is a widely used, cost-effective tool for detecting electrical abnormalities in the heart. However, it cannot directly measure functional parameters, such as ventricular volumes and ejection fraction, which are crucial for assessing cardiac function. Cardiac magnetic resonance (CMR) is the gold standard for these measurements, providing detailed structural and functional insights, but is expensive and less accessible. To bridge this gap, we propose PTACL (Patient and Temporal Alignment Contrastive Learning), a multimodal contrastive learning framework that enhances ECG representations by integrating spatio-temporal information from CMR. PTACL uses global patient-level contrastive loss and local temporal-level contrastive loss. The global loss aligns patient-level representations by pulling ECG and CMR embeddings from the same patient closer together, while pushing apart embeddings from different patients. Local loss enforces fine-grained temporal alignment within each patient by contrasting encoded ECG segments with corresponding encoded CMR frames. This approach enriches ECG representations with diagnostic information beyond electrical activity and transfers more insights between modalities than global alignment alone, all without introducing new learnable weights. We evaluate PTACL on paired ECG-CMR data from 27,951 subjects in the UK Biobank. Compared to baseline approaches, PTACL achieves better performance in two clinically relevant tasks: (1) retrieving patients with similar cardiac phenotypes and (2) predicting CMR-derived cardiac function parameters, such as ventricular volumes and ejection fraction. Our results highlight the potential of PTACL to enhance non-invasive cardiac diagnostics using ECG. The code is available at: https://github.com/alsalivan/ecgcmr",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "eess.SP"
    ],
    "primary_category": "eess.IV",
    "comment": "accepted to MICCAI 2025 (Springer LNCS)",
    "pdf_url": "https://arxiv.org/pdf/2506.20683v1",
    "published_date": "2025-06-24 17:19:39 UTC",
    "updated_date": "2025-06-24 17:19:39 UTC"
  },
  {
    "arxiv_id": "2506.19807v3",
    "title": "KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality",
    "authors": [
      "Baochang Ren",
      "Shuofei Qiao",
      "Da Zheng",
      "Huajun Chen",
      "Ningyu Zhang"
    ],
    "abstract": "Large Language Models (LLMs), particularly slow-thinking models, often exhibit severe hallucination, outputting incorrect content due to an inability to accurately recognize knowledge boundaries during reasoning. While Reinforcement Learning (RL) can enhance complex reasoning abilities, its outcome-oriented reward mechanism often lacks factual supervision over the thinking process, further exacerbating the hallucination problem. To address the high hallucination in slow-thinking models, we propose Knowledge-enhanced RL, KnowRL. KnowRL guides models to perform fact-based slow thinking by integrating a factuality reward, based on knowledge verification, into the RL training process, helping them recognize their knowledge boundaries. KnowRL guides models to perform fact-based slow thinking by integrating a factuality reward, based on knowledge verification, into the RL training process, helping them recognize their knowledge boundaries. This targeted factual input during RL training enables the model to learn and internalize fact-based reasoning strategies. By directly rewarding adherence to facts within the reasoning steps, KnowRL fosters a more reliable thinking process. Experimental results on three hallucination evaluation datasets and two reasoning evaluation datasets demonstrate that KnowRL effectively mitigates hallucinations in slow-thinking models while maintaining their original strong reasoning capabilities. Our code is available at https://github.com/zjunlp/KnowRL.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "Work in progress",
    "pdf_url": "https://arxiv.org/pdf/2506.19807v3",
    "published_date": "2025-06-24 17:17:17 UTC",
    "updated_date": "2025-10-08 16:56:59 UTC"
  },
  {
    "arxiv_id": "2506.19794v5",
    "title": "Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic Empirical Study",
    "authors": [
      "Yuqi Zhu",
      "Yi Zhong",
      "Jintian Zhang",
      "Ziheng Zhang",
      "Shuofei Qiao",
      "Yujie Luo",
      "Lun Du",
      "Da Zheng",
      "Ningyu Zhang",
      "Huajun Chen"
    ],
    "abstract": "Large Language Models (LLMs) hold promise in automating data analysis tasks, yet open-source models face significant limitations in these kinds of reasoning-intensive scenarios. In this work, we investigate strategies to enhance the data analysis capabilities of open-source LLMs. By curating a seed dataset of diverse, realistic scenarios, we evaluate model behavior across three core dimensions: data understanding, code generation, and strategic planning. Our analysis reveals three key findings: (1) Strategic planning quality serves as the primary determinant of model performance; (2) Interaction design and task complexity significantly influence reasoning capabilities; (3) Data quality demonstrates a greater impact than diversity in achieving optimal performance. We leverage these insights to develop a data synthesis methodology, demonstrating significant improvements in open-source LLMs' analytical reasoning capabilities. Code is available at https://github.com/zjunlp/DataMind.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.CL",
    "comment": "AAAI 2026 (oral)",
    "pdf_url": "https://arxiv.org/pdf/2506.19794v5",
    "published_date": "2025-06-24 17:04:23 UTC",
    "updated_date": "2025-11-13 12:47:18 UTC"
  },
  {
    "arxiv_id": "2506.19785v1",
    "title": "Learning Task Belief Similarity with Latent Dynamics for Meta-Reinforcement Learning",
    "authors": [
      "Menglong Zhang",
      "Fuyuan Qian"
    ],
    "abstract": "Meta-reinforcement learning requires utilizing prior task distribution information obtained during exploration to rapidly adapt to unknown tasks. The efficiency of an agent's exploration hinges on accurately identifying the current task. Recent Bayes-Adaptive Deep RL approaches often rely on reconstructing the environment's reward signal, which is challenging in sparse reward settings, leading to suboptimal exploitation. Inspired by bisimulation metrics, which robustly extracts behavioral similarity in continuous MDPs, we propose SimBelief-a novel meta-RL framework via measuring similarity of task belief in Bayes-Adaptive MDP (BAMDP). SimBelief effectively extracts common features of similar task distributions, enabling efficient task identification and exploration in sparse reward environments. We introduce latent task belief metric to learn the common structure of similar tasks and incorporate it into the specific task belief. By learning the latent dynamics across task distributions, we connect shared latent task belief features with specific task features, facilitating rapid task identification and adaptation. Our method outperforms state-of-the-art baselines on sparse reward MuJoCo and panda-gym tasks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "ICLR2025 https://openreview.net/forum?id=5YbuOTUFQ4",
    "pdf_url": "https://arxiv.org/pdf/2506.19785v1",
    "published_date": "2025-06-24 16:52:00 UTC",
    "updated_date": "2025-06-24 16:52:00 UTC"
  },
  {
    "arxiv_id": "2506.19783v2",
    "title": "SAGE: Strategy-Adaptive Generation Engine for Query Rewriting",
    "authors": [
      "Teng Wang",
      "Hailei Gong",
      "Changwang Zhang",
      "Jun Wang"
    ],
    "abstract": "Query rewriting is pivotal for enhancing dense retrieval, yet current methods demand large-scale supervised data or suffer from inefficient reinforcement learning (RL) exploration. In this work, we first establish that guiding Large Language Models (LLMs) with a concise set of expert-crafted strategies, such as semantic expansion and entity disambiguation, substantially improves retrieval effectiveness on challenging benchmarks, including HotpotQA, FEVER, NFCorpus, and SciFact. Building on this insight, we introduce the Strategy-Adaptive Generation Engine (SAGE), which operationalizes these strategies in an RL framework. SAGE introduces two novel reward shaping mechanisms-Strategic Credit Shaping (SCS) and Contrastive Reward Shaping (CRS)-to deliver more informative learning signals. This strategy-guided approach not only achieves new state-of-the-art NDCG@10 results, but also uncovers a compelling emergent behavior: the agent learns to select optimal strategies, reduces unnecessary exploration, and generates concise rewrites, lowering inference cost without sacrificing performance. Our findings demonstrate that strategy-guided RL, enhanced with nuanced reward shaping, offers a scalable, efficient, and more interpretable paradigm for developing the next generation of robust information retrieval systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19783v2",
    "published_date": "2025-06-24 16:50:51 UTC",
    "updated_date": "2025-07-26 07:12:26 UTC"
  },
  {
    "arxiv_id": "2506.19777v2",
    "title": "Alleviating User-Sensitive bias with Fair Generative Sequential Recommendation Model",
    "authors": [
      "Yang Liu",
      "Feng Wu",
      "Xuefang Zhu"
    ],
    "abstract": "Recommendation fairness has recently attracted much attention. In the real world, recommendation systems are driven by user behavior, and since users with the same sensitive feature (e.g., gender and age) tend to have the same patterns, recommendation models can easily capture the strong correlation preference of sensitive features and thus cause recommendation unfairness. Diffusion model (DM) as a new generative model paradigm has achieved great success in recommendation systems. DM's ability to model uncertainty and represent diversity, and its modeling mechanism has a high degree of adaptability with the real-world recommendation process with bias. Therefore, we use DM to effectively model the fairness of recommendation and enhance the diversity. This paper proposes a FairGENerative sequential Recommendation model based on DM, FairGENRec. In the training phase, we inject random noise into the original distribution under the guidance of the sensitive feature recognition model, and a sequential denoise model is designed for the reverse reconstruction of items. Simultaneously, recommendation fairness modeling is completed by injecting multi-interests representational information that eliminates the bias of sensitive user features into the generated results. In the inference phase, the model obtains the noise in the form of noise addition by using the history interactions which is followed by reverse iteration to reconstruct the target item representation. Finally, our extensive experiments on three datasets demonstrate the dual enhancement effect of FairGENRec on accuracy and fairness, while the statistical analysis of the cases visualizes the degree of improvement on the fairness of the recommendation.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19777v2",
    "published_date": "2025-06-24 16:42:46 UTC",
    "updated_date": "2025-07-15 14:55:37 UTC"
  },
  {
    "arxiv_id": "2506.19774v1",
    "title": "Kling-Foley: Multimodal Diffusion Transformer for High-Quality Video-to-Audio Generation",
    "authors": [
      "Jun Wang",
      "Xijuan Zeng",
      "Chunyu Qiang",
      "Ruilong Chen",
      "Shiyao Wang",
      "Le Wang",
      "Wangjing Zhou",
      "Pengfei Cai",
      "Jiahui Zhao",
      "Nan Li",
      "Zihan Li",
      "Yuzhe Liang",
      "Xiaopeng Wang",
      "Haorui Zheng",
      "Ming Wen",
      "Kang Yin",
      "Yiran Wang",
      "Nan Li",
      "Feng Deng",
      "Liang Dong",
      "Chen Zhang",
      "Di Zhang",
      "Kun Gai"
    ],
    "abstract": "We propose Kling-Foley, a large-scale multimodal Video-to-Audio generation model that synthesizes high-quality audio synchronized with video content. In Kling-Foley, we introduce multimodal diffusion transformers to model the interactions between video, audio, and text modalities, and combine it with a visual semantic representation module and an audio-visual synchronization module to enhance alignment capabilities. Specifically, these modules align video conditions with latent audio elements at the frame level, thereby improving semantic alignment and audio-visual synchronization. Together with text conditions, this integrated approach enables precise generation of video-matching sound effects. In addition, we propose a universal latent audio codec that can achieve high-quality modeling in various scenarios such as sound effects, speech, singing, and music. We employ a stereo rendering method that imbues synthesized audio with a spatial presence. At the same time, in order to make up for the incomplete types and annotations of the open-source benchmark, we also open-source an industrial-level benchmark Kling-Audio-Eval. Our experiments show that Kling-Foley trained with the flow matching objective achieves new audio-visual SOTA performance among public models in terms of distribution matching, semantic alignment, temporal alignment and audio quality.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19774v1",
    "published_date": "2025-06-24 16:39:39 UTC",
    "updated_date": "2025-06-24 16:39:39 UTC"
  },
  {
    "arxiv_id": "2506.19773v2",
    "title": "Automatic Prompt Optimization for Knowledge Graph Construction: Insights from an Empirical Study",
    "authors": [
      "Nandana Mihindukulasooriya",
      "Niharika S. D'Souza",
      "Faisal Chowdhury",
      "Horst Samulowitz"
    ],
    "abstract": "A KG represents a network of entities and illustrates relationships between them. KGs are used for various applications, including semantic search and discovery, reasoning, decision-making, natural language processing, machine learning, and recommendation systems. Triple (subject-relation-object) extraction from text is the fundamental building block of KG construction and has been widely studied, for example, in early benchmarks such as ACE 2002 to more recent ones, such as WebNLG 2020, REBEL and SynthIE. While the use of LLMs is explored for KG construction, handcrafting reasonable task-specific prompts for LLMs is a labour-intensive exercise and can be brittle due to subtle changes in the LLM models employed. Recent work in NLP tasks (e.g. autonomy generation) uses automatic prompt optimization/engineering to address this challenge by generating optimal or near-optimal task-specific prompts given input-output examples.\n  This empirical study explores the application of automatic prompt optimization for the triple extraction task using experimental benchmarking. We evaluate different settings by changing (a) the prompting strategy, (b) the LLM being used for prompt optimization and task execution, (c) the number of canonical relations in the schema (schema complexity), (d) the length and diversity of input text, (e) the metric used to drive the prompt optimization, and (f) the dataset being used for training and testing. We evaluate three different automatic prompt optimizers, namely, DSPy, APE, and TextGrad and use two different triple extraction datasets, SynthIE and REBEL. Through rigorous empirical evaluation, our main contribution highlights that automatic prompt optimization techniques can generate reasonable prompts similar to humans for triple extraction. In turn, these optimized prompts achieve improved results, particularly with increasing schema complexity and text size.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at LLM+Graph WS at VLDB 2025. 21 pages, 7 figures, 8 tables",
    "pdf_url": "https://arxiv.org/pdf/2506.19773v2",
    "published_date": "2025-06-24 16:38:49 UTC",
    "updated_date": "2025-08-04 22:04:58 UTC"
  },
  {
    "arxiv_id": "2506.19769v1",
    "title": "A Survey of Multi-sensor Fusion Perception for Embodied AI: Background, Methods, Challenges and Prospects",
    "authors": [
      "Shulan Ruan",
      "Rongwei Wang",
      "Xuchen Shen",
      "Huijie Liu",
      "Baihui Xiao",
      "Jun Shi",
      "Kun Zhang",
      "Zhenya Huang",
      "Yu Liu",
      "Enhong Chen",
      "You He"
    ],
    "abstract": "Multi-sensor fusion perception (MSFP) is a key technology for embodied AI, which can serve a variety of downstream tasks (e.g., 3D object detection and semantic segmentation) and application scenarios (e.g., autonomous driving and swarm robotics). Recently, impressive achievements on AI-based MSFP methods have been reviewed in relevant surveys. However, we observe that the existing surveys have some limitations after a rigorous and detailed investigation. For one thing, most surveys are oriented to a single task or research field, such as 3D object detection or autonomous driving. Therefore, researchers in other related tasks often find it difficult to benefit directly. For another, most surveys only introduce MSFP from a single perspective of multi-modal fusion, while lacking consideration of the diversity of MSFP methods, such as multi-view fusion and time-series fusion. To this end, in this paper, we hope to organize MSFP research from a task-agnostic perspective, where methods are reported from various technical views. Specifically, we first introduce the background of MSFP. Next, we review multi-modal and multi-agent fusion methods. A step further, time-series fusion methods are analyzed. In the era of LLM, we also investigate multimodal LLM fusion methods. Finally, we discuss open challenges and future directions for MSFP. We hope this survey can help researchers understand the important progress in MSFP and provide possible insights for future research.",
    "categories": [
      "cs.MM",
      "cs.AI"
    ],
    "primary_category": "cs.MM",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19769v1",
    "published_date": "2025-06-24 16:34:56 UTC",
    "updated_date": "2025-06-24 16:34:56 UTC"
  },
  {
    "arxiv_id": "2506.19767v1",
    "title": "SRFT: A Single-Stage Method with Supervised and Reinforcement Fine-Tuning for Reasoning",
    "authors": [
      "Yuqian Fu",
      "Tinghong Chen",
      "Jiajun Chai",
      "Xihuai Wang",
      "Songjun Tu",
      "Guojun Yin",
      "Wei Lin",
      "Qichao Zhang",
      "Yuanheng Zhu",
      "Dongbin Zhao"
    ],
    "abstract": "Large language models (LLMs) have achieved remarkable progress in reasoning tasks, yet the optimal integration of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) remains a fundamental challenge. Through comprehensive analysis of token distributions, learning dynamics, and integration mechanisms from entropy-based perspectives, we reveal key differences between these paradigms: SFT induces coarse-grained global changes to LLM policy distributions, while RL performs fine-grained selective optimizations, with entropy serving as a critical indicator of training effectiveness. Building on these observations, we propose Supervised Reinforcement Fine-Tuning (SRFT), a single-stage method that unifies both fine-tuning paradigms through entropy-aware weighting mechanisms. Our approach simultaneously applies SFT and RL to directly optimize the LLM using demonstrations and self-exploration rollouts rather than through two-stage sequential methods. Extensive experiments show that SRFT achieves 59.1% average accuracy, outperforming zero-RL methods by 9.0% on five mathematical reasoning benchmarks and 10.9% on three out-of-distribution benchmarks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19767v1",
    "published_date": "2025-06-24 16:31:37 UTC",
    "updated_date": "2025-06-24 16:31:37 UTC"
  },
  {
    "arxiv_id": "2506.19755v1",
    "title": "Cross-regularization: Adaptive Model Complexity through Validation Gradients",
    "authors": [
      "Carlos Stein Brito"
    ],
    "abstract": "Model regularization requires extensive manual tuning to balance complexity against overfitting. Cross-regularization resolves this tradeoff by directly adapting regularization parameters through validation gradients during training. The method splits parameter optimization - training data guides feature learning while validation data shapes complexity controls - converging provably to cross-validation optima. When implemented through noise injection in neural networks, this approach reveals striking patterns: unexpectedly high noise tolerance and architecture-specific regularization that emerges organically during training. Beyond complexity control, the framework integrates seamlessly with data augmentation, uncertainty calibration and growing datasets while maintaining single-run efficiency through a simple gradient-based approach.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.ST",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "21 pages, 13 figures. Accepted at ICML 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.19755v1",
    "published_date": "2025-06-24 16:15:50 UTC",
    "updated_date": "2025-06-24 16:15:50 UTC"
  },
  {
    "arxiv_id": "2506.19753v2",
    "title": "Arabic Dialect Classification using RNNs, Transformers, and Large Language Models: A Comparative Analysis",
    "authors": [
      "Omar A. Essameldin",
      "Ali O. Elbeih",
      "Wael H. Gomaa",
      "Wael F. Elsersy"
    ],
    "abstract": "The Arabic language is among the most popular languages in the world with a huge variety of dialects spoken in 22 countries. In this study, we address the problem of classifying 18 Arabic dialects of the QADI dataset of Arabic tweets. RNN models, Transformer models, and large language models (LLMs) via prompt engineering are created and tested. Among these, MARBERTv2 performed best with 65% accuracy and 64% F1-score. Through the use of state-of-the-art preprocessing techniques and the latest NLP models, this paper identifies the most significant linguistic issues in Arabic dialect identification. The results corroborate applications like personalized chatbots that respond in users' dialects, social media monitoring, and greater accessibility for Arabic communities.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Email Typo Update",
    "pdf_url": "https://arxiv.org/pdf/2506.19753v2",
    "published_date": "2025-06-24 16:06:58 UTC",
    "updated_date": "2025-06-28 12:32:10 UTC"
  },
  {
    "arxiv_id": "2506.19742v1",
    "title": "NeRF-based CBCT Reconstruction needs Normalization and Initialization",
    "authors": [
      "Zhuowei Xu",
      "Han Li",
      "Dai Sun",
      "Zhicheng Li",
      "Yujia Li",
      "Qingpeng Kong",
      "Zhiwei Cheng",
      "Nassir Navab",
      "S. Kevin Zhou"
    ],
    "abstract": "Cone Beam Computed Tomography (CBCT) is widely used in medical imaging. However, the limited number and intensity of X-ray projections make reconstruction an ill-posed problem with severe artifacts. NeRF-based methods have achieved great success in this task. However, they suffer from a local-global training mismatch between their two key components: the hash encoder and the neural network. Specifically, in each training step, only a subset of the hash encoder's parameters is used (local sparse), whereas all parameters in the neural network participate (global dense). Consequently, hash features generated in each step are highly misaligned, as they come from different subsets of the hash encoder. These misalignments from different training steps are then fed into the neural network, causing repeated inconsistent global updates in training, which leads to unstable training, slower convergence, and degraded reconstruction quality. Aiming to alleviate the impact of this local-global optimization mismatch, we introduce a Normalized Hash Encoder, which enhances feature consistency and mitigates the mismatch. Additionally, we propose a Mapping Consistency Initialization(MCI) strategy that initializes the neural network before training by leveraging the global mapping property from a well-trained model. The initialized neural network exhibits improved stability during early training, enabling faster convergence and enhanced reconstruction performance. Our method is simple yet effective, requiring only a few lines of code while substantially improving training efficiency on 128 CT cases collected from 4 different datasets, covering 7 distinct anatomical regions.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19742v1",
    "published_date": "2025-06-24 16:01:45 UTC",
    "updated_date": "2025-06-24 16:01:45 UTC"
  },
  {
    "arxiv_id": "2506.19732v1",
    "title": "Who Does What in Deep Learning? Multidimensional Game-Theoretic Attribution of Function of Neural Units",
    "authors": [
      "Shrey Dixit",
      "Kayson Fakhar",
      "Fatemeh Hadaeghi",
      "Patrick Mineault",
      "Konrad P. Kording",
      "Claus C. Hilgetag"
    ],
    "abstract": "Neural networks now generate text, images, and speech with billions of parameters, producing a need to know how each neural unit contributes to these high-dimensional outputs. Existing explainable-AI methods, such as SHAP, attribute importance to inputs, but cannot quantify the contributions of neural units across thousands of output pixels, tokens, or logits. Here we close that gap with Multiperturbation Shapley-value Analysis (MSA), a model-agnostic game-theoretic framework. By systematically lesioning combinations of units, MSA yields Shapley Modes, unit-wise contribution maps that share the exact dimensionality of the model's output. We apply MSA across scales, from multi-layer perceptrons to the 56-billion-parameter Mixtral-8x7B and Generative Adversarial Networks (GAN). The approach demonstrates how regularisation concentrates computation in a few hubs, exposes language-specific experts inside the LLM, and reveals an inverted pixel-generation hierarchy in GANs. Together, these results showcase MSA as a powerful approach for interpreting, editing, and compressing deep neural networks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19732v1",
    "published_date": "2025-06-24 15:50:35 UTC",
    "updated_date": "2025-06-24 15:50:35 UTC"
  },
  {
    "arxiv_id": "2506.19726v2",
    "title": "Precise Bayesian Neural Networks",
    "authors": [
      "Carlos Stein Brito"
    ],
    "abstract": "Despite its long history, Bayesian neural networks (BNNs) and variational training remain underused in practice: standard Gaussian posteriors misalign with network geometry, KL terms can be brittle in high dimensions, and implementations often add complexity without reliably improving uncertainty. We revisit the problem through the lens of normalization. Because normalization layers neutralize the influence of weight magnitude, we model uncertainty \\emph{only in weight directions} using a von Mises-Fisher posterior on the unit sphere. High-dimensional geometry then yields a single, interpretable scalar per layer--the effective post-normalization noise $σ_{\\mathrm{eff}}$--that (i) corresponds to simple additive Gaussian noise in the forward pass and (ii) admits a compact, dimension-aware KL in closed form. We derive accurate, closed-form approximations linking concentration $κ$ to activation variance and to $σ_{\\mathrm{eff}}$ across regimes, producing a lightweight, implementation-ready variational unit that fits modern normalized architectures and improves calibration without sacrificing accuracy. This dimension awareness is critical for stable optimization in high dimensions. In short, by aligning the variational posterior with the network's intrinsic geometry, BNNs can be simultaneously principled, practical, and precise.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.19726v2",
    "published_date": "2025-06-24 15:42:00 UTC",
    "updated_date": "2025-09-07 20:10:04 UTC"
  },
  {
    "arxiv_id": "2507.00724v2",
    "title": "Holmes: Towards Effective and Harmless Model Ownership Verification to Personalized Large Vision Models via Decoupling Common Features",
    "authors": [
      "Linghui Zhu",
      "Yiming Li",
      "Haiqin Weng",
      "Yan Liu",
      "Tianwei Zhang",
      "Shu-Tao Xia",
      "Zhi Wang"
    ],
    "abstract": "Large vision models (LVMs) achieve remarkable performance in various downstream tasks, primarily by personalizing pre-trained models through fine-tuning with private and valuable local data, which makes the personalized model a valuable intellectual property. Similar to the era of traditional DNNs, model stealing attacks also pose significant risks to LVMs. However, this paper reveals that most existing defense methods (developed for traditional DNNs), typically designed for models trained from scratch, either introduce additional security risks, are prone to misjudgment, or are even ineffective for fine-tuned models. To alleviate these problems, this paper proposes a harmless model ownership verification method for personalized LVMs by decoupling similar common features. In general, our method consists of three main stages. In the first stage, we create shadow models that retain common features of the victim model while disrupting dataset-specific features. We represent the dataset-specific features of the victim model by computing the output differences between the shadow and victim models, without altering the victim model or its training process. After that, a meta-classifier is trained to identify stolen models by determining whether suspicious models contain the dataset-specific features of the victim. In the third stage, we conduct model ownership verification by hypothesis test to mitigate randomness and enhance robustness. Extensive experiments on benchmark datasets verify the effectiveness of the proposed method in detecting different types of model stealing simultaneously. Our codes are available at https://github.com/zlh-thu/Holmes.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2507.00724v2",
    "published_date": "2025-06-24 15:40:11 UTC",
    "updated_date": "2025-12-19 11:29:15 UTC"
  },
  {
    "arxiv_id": "2506.19724v1",
    "title": "From Reproduction to Replication: Evaluating Research Agents with Progressive Code Masking",
    "authors": [
      "Gyeongwon James Kim",
      "Alex Wilf",
      "Louis-Philippe Morency",
      "Daniel Fried"
    ],
    "abstract": "Recent progress in autonomous code generation has fueled excitement around AI agents capable of accelerating scientific discovery by running experiments. However, there is currently no benchmark that evaluates whether such agents can implement scientific ideas when given varied amounts of code as a starting point, interpolating between reproduction (running code) and from-scratch replication (fully re-implementing and running code). We introduce AutoExperiment, a benchmark that evaluates AI agents' ability to implement and run machine learning experiments based on natural language descriptions in research papers. In each task, agents are given a research paper, a codebase with key functions masked out, and a command to run the experiment. The goal is to generate the missing code, execute the experiment in a sandboxed environment, and reproduce the results. AutoExperiment scales in difficulty by varying the number of missing functions $n$, ranging from partial reproduction to full replication. We evaluate state-of-the-art agents and find that performance degrades rapidly as $n$ increases. Agents that can dynamically interact with the environment (e.g. to debug their code) can outperform agents in fixed \"agentless\" harnesses, and there exists a significant gap between single-shot and multi-trial success rates (Pass@1 vs. Pass@5), motivating verifier approaches to our benchmark. Our findings highlight critical challenges in long-horizon code generation, context retrieval, and autonomous experiment execution, establishing AutoExperiment as a new benchmark for evaluating progress in AI-driven scientific experimentation. Our data and code are open-sourced at https://github.com/j1mk1m/AutoExperiment .",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19724v1",
    "published_date": "2025-06-24 15:39:20 UTC",
    "updated_date": "2025-06-24 15:39:20 UTC"
  },
  {
    "arxiv_id": "2506.22491v1",
    "title": "PromptAug: Fine-grained Conflict Classification Using Data Augmentation",
    "authors": [
      "Oliver Warke",
      "Joemon M. Jose",
      "Faegheh Hasibi",
      "Jan Breitsohl"
    ],
    "abstract": "Given the rise of conflicts on social media, effective classification models to detect harmful behaviours are essential. Following the garbage-in-garbage-out maxim, machine learning performance depends heavily on training data quality. However, high-quality labelled data, especially for nuanced tasks like identifying conflict behaviours, is limited, expensive, and difficult to obtain. Additionally, as social media platforms increasingly restrict access to research data, text data augmentation is gaining attention as an alternative to generate training data. Augmenting conflict-related data poses unique challenges due to Large Language Model (LLM) guardrails that prevent generation of offensive content. This paper introduces PromptAug, an innovative LLM-based data augmentation method. PromptAug achieves statistically significant improvements of 2% in both accuracy and F1-score on conflict and emotion datasets. To thoroughly evaluate PromptAug against other data augmentation methods we conduct a robust evaluation using extreme data scarcity scenarios, quantitative diversity analysis and a qualitative thematic analysis. The thematic analysis identifies four problematic patterns in augmented text: Linguistic Fluidity, Humour Ambiguity, Augmented Content Ambiguity, and Augmented Content Misinterpretation.\n  Overall, this work presents PromptAug as an effective method for augmenting data in sensitive tasks like conflict detection, offering a unique, interdisciplinary evaluation grounded in both natural language processing and social science methodology.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.22491v1",
    "published_date": "2025-06-24 15:33:18 UTC",
    "updated_date": "2025-06-24 15:33:18 UTC"
  },
  {
    "arxiv_id": "2506.19708v1",
    "title": "Uncovering Conceptual Blindspots in Generative Image Models Using Sparse Autoencoders",
    "authors": [
      "Matyas Bohacek",
      "Thomas Fel",
      "Maneesh Agrawala",
      "Ekdeep Singh Lubana"
    ],
    "abstract": "Despite their impressive performance, generative image models trained on large-scale datasets frequently fail to produce images with seemingly simple concepts -- e.g., human hands or objects appearing in groups of four -- that are reasonably expected to appear in the training data. These failure modes have largely been documented anecdotally, leaving open the question of whether they reflect idiosyncratic anomalies or more structural limitations of these models. To address this, we introduce a systematic approach for identifying and characterizing \"conceptual blindspots\" -- concepts present in the training data but absent or misrepresented in a model's generations. Our method leverages sparse autoencoders (SAEs) to extract interpretable concept embeddings, enabling a quantitative comparison of concept prevalence between real and generated images. We train an archetypal SAE (RA-SAE) on DINOv2 features with 32,000 concepts -- the largest such SAE to date -- enabling fine-grained analysis of conceptual disparities. Applied to four popular generative models (Stable Diffusion 1.5/2.1, PixArt, and Kandinsky), our approach reveals specific suppressed blindspots (e.g., bird feeders, DVD discs, and whitespaces on documents) and exaggerated blindspots (e.g., wood background texture and palm trees). At the individual datapoint level, we further isolate memorization artifacts -- instances where models reproduce highly specific visual templates seen during training. Overall, we propose a theoretically grounded framework for systematically identifying conceptual blindspots in generative models by assessing their conceptual fidelity with respect to the underlying data-generating process.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.GR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19708v1",
    "published_date": "2025-06-24 15:15:15 UTC",
    "updated_date": "2025-06-24 15:15:15 UTC"
  },
  {
    "arxiv_id": "2506.19702v1",
    "title": "LLM-Driven Medical Document Analysis: Enhancing Trustworthy Pathology and Differential Diagnosis",
    "authors": [
      "Lei Kang",
      "Xuanshuo Fu",
      "Oriol Ramos Terrades",
      "Javier Vazquez-Corral",
      "Ernest Valveny",
      "Dimosthenis Karatzas"
    ],
    "abstract": "Medical document analysis plays a crucial role in extracting essential clinical insights from unstructured healthcare records, supporting critical tasks such as differential diagnosis. Determining the most probable condition among overlapping symptoms requires precise evaluation and deep medical expertise. While recent advancements in large language models (LLMs) have significantly enhanced performance in medical document analysis, privacy concerns related to sensitive patient data limit the use of online LLMs services in clinical settings. To address these challenges, we propose a trustworthy medical document analysis platform that fine-tunes a LLaMA-v3 using low-rank adaptation, specifically optimized for differential diagnosis tasks. Our approach utilizes DDXPlus, the largest benchmark dataset for differential diagnosis, and demonstrates superior performance in pathology prediction and variable-length differential diagnosis compared to existing methods. The developed web-based platform allows users to submit their own unstructured medical documents and receive accurate, explainable diagnostic results. By incorporating advanced explainability techniques, the system ensures transparent and reliable predictions, fostering user trust and confidence. Extensive evaluations confirm that the proposed method surpasses current state-of-the-art models in predictive accuracy while offering practical utility in clinical settings. This work addresses the urgent need for reliable, explainable, and privacy-preserving artificial intelligence solutions, representing a significant advancement in intelligent medical document analysis for real-world healthcare applications. The code can be found at \\href{https://github.com/leitro/Differential-Diagnosis-LoRA}{https://github.com/leitro/Differential-Diagnosis-LoRA}.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at ICDAR 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.19702v1",
    "published_date": "2025-06-24 15:12:42 UTC",
    "updated_date": "2025-06-24 15:12:42 UTC"
  },
  {
    "arxiv_id": "2506.19698v1",
    "title": "Toward Decision-Oriented Prognostics: An Integrated Estimate-Optimize Framework for Predictive Maintenance",
    "authors": [
      "Zhuojun Xie",
      "Adam Abdin",
      "Yiping Fang"
    ],
    "abstract": "Recent research increasingly integrates machine learning (ML) into predictive maintenance (PdM) to reduce operational and maintenance costs in data-rich operational settings. However, uncertainty due to model misspecification continues to limit widespread industrial adoption. This paper proposes a PdM framework in which sensor-driven prognostics inform decision-making under economic trade-offs within a finite decision space. We investigate two key questions: (1) Does higher predictive accuracy necessarily lead to better maintenance decisions? (2) If not, how can the impact of prediction errors on downstream maintenance decisions be mitigated? We first demonstrate that in the traditional estimate-then-optimize (ETO) framework, errors in probabilistic prediction can result in inconsistent and suboptimal maintenance decisions. To address this, we propose an integrated estimate-optimize (IEO) framework that jointly tunes predictive models while directly optimizing for maintenance outcomes. We establish theoretical finite-sample guarantees on decision consistency under standard assumptions. Specifically, we develop a stochastic perturbation gradient descent algorithm suitable for small run-to-failure datasets. Empirical evaluations on a turbofan maintenance case study show that the IEO framework reduces average maintenance regret up to 22% compared to ETO. This study provides a principled approach to managing prediction errors in data-driven PdM. By aligning prognostic model training with maintenance objectives, the IEO framework improves robustness under model misspecification and improves decision quality. The improvement is particularly pronounced when the decision-making policy is misaligned with the decision-maker's target. These findings support more reliable maintenance planning in uncertain operational environments.",
    "categories": [
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.AI",
    "comment": "22 pages, 5 figures, 4 tables",
    "pdf_url": "https://arxiv.org/pdf/2506.19698v1",
    "published_date": "2025-06-24 15:10:15 UTC",
    "updated_date": "2025-06-24 15:10:15 UTC"
  },
  {
    "arxiv_id": "2506.19697v1",
    "title": "Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large Language Models",
    "authors": [
      "Jungwoo Park",
      "Taewhoo Lee",
      "Chanwoong Yoon",
      "Hyeon Hwang",
      "Jaewoo Kang"
    ],
    "abstract": "Extreme activation outliers in Large Language Models (LLMs) critically degrade quantization performance, hindering efficient on-device deployment. While channel-wise operations and adaptive gradient scaling are recognized causes, practical mitigation remains challenging. We introduce Outlier-Safe Pre-Training (OSP), a practical guideline that proactively prevents outlier formation rather than relying on post-hoc mitigation. OSP combines three key innovations: (1) the Muon optimizer, eliminating privileged bases while maintaining training efficiency; (2) Single-Scale RMSNorm, preventing channel-wise amplification; and (3) a learnable embedding projection, redistributing activation magnitudes originating from embedding matrices. We validate OSP by training a 1.4B-parameter model on 1 trillion tokens, which is the first production-scale LLM trained without such outliers. Under aggressive 4-bit quantization, our OSP model achieves a 35.7 average score across 10 benchmarks (compared to 26.5 for an Adam-trained model), with only a 2% training overhead. Remarkably, OSP models exhibit near-zero excess kurtosis (0.04) compared to extreme values (1818.56) in standard models, fundamentally altering LLM quantization behavior. Our work demonstrates that outliers are not inherent to LLMs but are consequences of training strategies, paving the way for more efficient LLM deployment. The source code and pretrained checkpoints are available at https://github.com/dmis-lab/Outlier-Safe-Pre-Training.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19697v1",
    "published_date": "2025-06-24 15:03:57 UTC",
    "updated_date": "2025-06-24 15:03:57 UTC"
  },
  {
    "arxiv_id": "2506.19689v1",
    "title": "When Can We Reuse a Calibration Set for Multiple Conformal Predictions?",
    "authors": [
      "A. A. Balinsky",
      "A. D. Balinsky"
    ],
    "abstract": "Reliable uncertainty quantification is crucial for the trustworthiness of machine learning applications. Inductive Conformal Prediction (ICP) offers a distribution-free framework for generating prediction sets or intervals with user-specified confidence. However, standard ICP guarantees are marginal and typically require a fresh calibration set for each new prediction to maintain their validity. This paper addresses this practical limitation by demonstrating how e-conformal prediction, in conjunction with Hoeffding's inequality, can enable the repeated use of a single calibration set with a high probability of preserving the desired coverage. Through a case study on the CIFAR-10 dataset, we train a deep neural network and utilise a calibration set to estimate a Hoeffding correction. This correction allows us to apply a modified Markov's inequality, leading to the construction of prediction sets with quantifiable confidence. Our results illustrate the feasibility of maintaining provable performance in conformal prediction while enhancing its practicality by reducing the need for repeated calibration. The code for this work is publicly available.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.ST"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19689v1",
    "published_date": "2025-06-24 14:57:25 UTC",
    "updated_date": "2025-06-24 14:57:25 UTC"
  },
  {
    "arxiv_id": "2506.19686v3",
    "title": "From Memories to Maps: Mechanisms of In-Context Reinforcement Learning in Transformers",
    "authors": [
      "Ching Fang",
      "Kanaka Rajan"
    ],
    "abstract": "Humans and animals show remarkable learning efficiency, adapting to new environments with minimal experience. This capability is not well captured by standard reinforcement learning algorithms that rely on incremental value updates. Rapid adaptation likely depends on episodic memory -- the ability to retrieve specific past experiences to guide decisions in novel contexts. Transformers provide a useful setting for studying these questions because of their ability to learn rapidly in-context and because their key-value architecture resembles episodic memory systems in the brain. We train a transformer to in-context reinforcement learn in a distribution of planning tasks inspired by rodent behavior. We then characterize the learning algorithms that emerge in the model. We first find that representation learning is supported by in-context structure learning and cross-context alignment, where representations are aligned across environments with different sensory stimuli. We next demonstrate that the reinforcement learning strategies developed by the model are not interpretable as standard model-free or model-based planning. Instead, we show that in-context reinforcement learning is supported by caching intermediate computations within the model's memory tokens, which are then accessed at decision time. Overall, we find that memory may serve as a computational resource, storing both raw experience and cached computations to support flexible behavior. Furthermore, the representations developed in the model resemble computations associated with the hippocampal-entorhinal system in the brain, suggesting that our findings may be relevant for natural cognition. Taken together, our work offers a mechanistic hypothesis for the rapid adaptation that underlies in-context learning in artificial and natural settings.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Revised to around 9 pages",
    "pdf_url": "https://arxiv.org/pdf/2506.19686v3",
    "published_date": "2025-06-24 14:55:43 UTC",
    "updated_date": "2025-12-01 21:56:32 UTC"
  },
  {
    "arxiv_id": "2506.19683v2",
    "title": "Semantic Scene Graph for Ultrasound Image Explanation and Scanning Guidance",
    "authors": [
      "Xuesong Li",
      "Dianye Huang",
      "Yameng Zhang",
      "Nassir Navab",
      "Zhongliang Jiang"
    ],
    "abstract": "Understanding medical ultrasound imaging remains a long-standing challenge due to significant visual variability caused by differences in imaging and acquisition parameters. Recent advancements in large language models (LLMs) have been used to automatically generate terminology-rich summaries orientated to clinicians with sufficient physiological knowledge. Nevertheless, the increasing demand for improved ultrasound interpretability and basic scanning guidance among non-expert users, e.g., in point-of-care settings, has not yet been explored. In this study, we first introduce the scene graph (SG) for ultrasound images to explain image content to ordinary and provide guidance for ultrasound scanning. The ultrasound SG is first computed using a transformer-based one-stage method, eliminating the need for explicit object detection. To generate a graspable image explanation for ordinary, the user query is then used to further refine the abstract SG representation through LLMs. Additionally, the predicted SG is explored for its potential in guiding ultrasound scanning toward missing anatomies within the current imaging view, assisting ordinary users in achieving more standardized and complete anatomical exploration. The effectiveness of this SG-based image explanation and scanning guidance has been validated on images from the left and right neck regions, including the carotid and thyroid, across five volunteers. The results demonstrate the potential of the method to maximally democratize ultrasound by enhancing its interpretability and usability for ordinaries.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19683v2",
    "published_date": "2025-06-24 14:49:40 UTC",
    "updated_date": "2025-06-26 14:20:13 UTC"
  },
  {
    "arxiv_id": "2506.19652v2",
    "title": "Tailored Conversations beyond LLMs: A RL-Based Dialogue Manager",
    "authors": [
      "Lucie Galland",
      "Catherine Pelachaud",
      "Florian Pecune"
    ],
    "abstract": "In this work, we propose a novel framework that integrates large language models (LLMs) with an RL-based dialogue manager for open-ended dialogue with a specific goal. By leveraging hierarchical reinforcement learning to model the structured phases of dialogue and employ meta-learning to enhance adaptability across diverse user profiles, our approach enhances adaptability and efficiency, enabling the system to learn from limited data, transition fluidly between dialogue phases, and personalize responses to heterogeneous patient needs. We apply our framework to Motivational Interviews, aiming to foster behavior change, and demonstrate that the proposed dialogue manager outperforms a state-of-the-art LLM baseline in terms of reward, showing a potential benefit of conditioning LLMs to create open-ended dialogue systems with specific goals.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19652v2",
    "published_date": "2025-06-24 14:15:26 UTC",
    "updated_date": "2025-07-08 14:47:33 UTC"
  },
  {
    "arxiv_id": "2506.19650v1",
    "title": "Identifying Macro Causal Effects in C-DMGs over DMGs",
    "authors": [
      "Simon Ferreira",
      "Charles K. Assaad"
    ],
    "abstract": "The do-calculus is a sound and complete tool for identifying causal effects in acyclic directed mixed graphs (ADMGs) induced by structural causal models (SCMs). However, in many real-world applications, especially in high-dimensional setting, constructing a fully specified ADMG is often infeasible. This limitation has led to growing interest in partially specified causal representations, particularly through cluster-directed mixed graphs (C-DMGs), which group variables into clusters and offer a more abstract yet practical view of causal dependencies. While these representations can include cycles, recent work has shown that the do-calculus remains sound and complete for identifying macro-level causal effects in C-DMGs over ADMGs under the assumption that all clusters size are greater than 1. Nevertheless, real-world systems often exhibit cyclic causal dynamics at the structural level. To account for this, input-output structural causal models (ioSCMs) have been introduced as a generalization of SCMs that allow for cycles. ioSCMs induce another type of graph structure known as a directed mixed graph (DMG). Analogous to the ADMG setting, one can define C-DMGs over DMGs as high-level representations of causal relations among clusters of variables. In this paper, we prove that, unlike in the ADMG setting, the do-calculus is unconditionally sound and complete for identifying macro causal effects in C-DMGs over DMGs. Furthermore, we show that the graphical criteria for non-identifiability of macro causal effects previously established C-DMGs over ADMGs naturally extends to a subset of C-DMGs over DMGs.",
    "categories": [
      "cs.AI",
      "stat.ME"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to the UAI2025 workshop on Causal Abstractions and Representations. arXiv admin note: substantial text overlap with arXiv:2504.01551",
    "pdf_url": "https://arxiv.org/pdf/2506.19650v1",
    "published_date": "2025-06-24 14:14:43 UTC",
    "updated_date": "2025-06-24 14:14:43 UTC"
  },
  {
    "arxiv_id": "2506.21627v1",
    "title": "FrankenBot: Brain-Morphic Modular Orchestration for Robotic Manipulation with Vision-Language Models",
    "authors": [
      "Shiyi Wang",
      "Wenbo Li",
      "Yiteng Chen",
      "Qingyao Wu",
      "Huiping Zhuang"
    ],
    "abstract": "Developing a general robot manipulation system capable of performing a wide range of tasks in complex, dynamic, and unstructured real-world environments has long been a challenging task. It is widely recognized that achieving human-like efficiency and robustness manipulation requires the robotic brain to integrate a comprehensive set of functions, such as task planning, policy generation, anomaly monitoring and handling, and long-term memory, achieving high-efficiency operation across all functions. Vision-Language Models (VLMs), pretrained on massive multimodal data, have acquired rich world knowledge, exhibiting exceptional scene understanding and multimodal reasoning capabilities. However, existing methods typically focus on realizing only a single function or a subset of functions within the robotic brain, without integrating them into a unified cognitive architecture. Inspired by a divide-and-conquer strategy and the architecture of the human brain, we propose FrankenBot, a VLM-driven, brain-morphic robotic manipulation framework that achieves both comprehensive functionality and high operational efficiency. Our framework includes a suite of components, decoupling a part of key functions from frequent VLM calls, striking an optimal balance between functional completeness and system efficiency. Specifically, we map task planning, policy generation, memory management, and low-level interfacing to the cortex, cerebellum, temporal lobe-hippocampus complex, and brainstem, respectively, and design efficient coordination mechanisms for the modules. We conducted comprehensive experiments in both simulation and real-world robotic environments, demonstrating that our method offers significant advantages in anomaly detection and handling, long-term memory, operational efficiency, and stability -- all without requiring any fine-tuning or retraining.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "15 pages, 4 figures, under review of NeurIPS",
    "pdf_url": "https://arxiv.org/pdf/2506.21627v1",
    "published_date": "2025-06-24 14:11:22 UTC",
    "updated_date": "2025-06-24 14:11:22 UTC"
  },
  {
    "arxiv_id": "2506.19642v1",
    "title": "The receptron is a nonlinear threshold logic gate with intrinsic multi-dimensional selective capabilities for analog inputs",
    "authors": [
      "B. Paroli",
      "F. Borghi",
      "M. A. C. Potenza",
      "P. Milani"
    ],
    "abstract": "Threshold logic gates (TLGs) have been proposed as artificial counterparts of biological neurons with classification capabilities based on a linear predictor function combining a set of weights with the feature vector. The linearity of TLGs limits their classification capabilities requiring the use of networks for the accomplishment of complex tasks. A generalization of the TLG model called receptron, characterized by input-dependent weight functions allows for a significant enhancement of classification performances even with the use of a single unit. Here we formally demonstrate that a receptron, characterized by nonlinear input-dependent weight functions, exhibit intrinsic selective activation properties for analog inputs, when the input vector is within cubic domains in a 3D space. The proposed model can be extended to the n-dimensional case for multidimensional applications. Our results suggest that receptron-based networks can represent a new class of devices capable to manage a large number of analog inputs, for edge applications requiring high selectivity and classification capabilities without the burden of complex training.",
    "categories": [
      "cs.ET",
      "cs.AI"
    ],
    "primary_category": "cs.ET",
    "comment": "12 pages, 7 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.19642v1",
    "published_date": "2025-06-24 14:04:15 UTC",
    "updated_date": "2025-06-24 14:04:15 UTC"
  },
  {
    "arxiv_id": "2506.19635v1",
    "title": "On the efficacy of old features for the detection of new bots",
    "authors": [
      "Rocco De Nicola",
      "Marinella Petrocchi",
      "Manuel Pratelli"
    ],
    "abstract": "For more than a decade now, academicians and online platform administrators have been studying solutions to the problem of bot detection. Bots are computer algorithms whose use is far from being benign: malicious bots are purposely created to distribute spam, sponsor public characters and, ultimately, induce a bias within the public opinion. To fight the bot invasion on our online ecosystem, several approaches have been implemented, mostly based on (supervised and unsupervised) classifiers, which adopt the most varied account features, from the simplest to the most expensive ones to be extracted from the raw data obtainable through the Twitter public APIs. In this exploratory study, using Twitter as a benchmark, we compare the performances of four state-of-art feature sets in detecting novel bots: one of the output scores of the popular bot detector Botometer, which considers more than 1,000 features of an account to take a decision; two feature sets based on the account profile and timeline; and the information about the Twitter client from which the user tweets. The results of our analysis, conducted on six recently released datasets of Twitter accounts, hint at the possible use of general-purpose classifiers and cheap-to-compute account features for the detection of evolved bots.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.CR",
    "comment": "pre-print version",
    "pdf_url": "https://arxiv.org/pdf/2506.19635v1",
    "published_date": "2025-06-24 13:56:09 UTC",
    "updated_date": "2025-06-24 13:56:09 UTC"
  },
  {
    "arxiv_id": "2506.19633v1",
    "title": "Hierarchical Time Series Forecasting Via Latent Mean Encoding",
    "authors": [
      "Alessandro Salatiello",
      "Stefan Birr",
      "Manuel Kunz"
    ],
    "abstract": "Coherently forecasting the behaviour of a target variable across both coarse and fine temporal scales is crucial for profit-optimized decision-making in several business applications, and remains an open research problem in temporal hierarchical forecasting. Here, we propose a new hierarchical architecture that tackles this problem by leveraging modules that specialize in forecasting the different temporal aggregation levels of interest. The architecture, which learns to encode the average behaviour of the target variable within its hidden layers, makes accurate and coherent forecasts across the target temporal hierarchies. We validate our architecture on the challenging, real-world M5 dataset and show that it outperforms established methods, such as the TSMixer model.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19633v1",
    "published_date": "2025-06-24 13:54:47 UTC",
    "updated_date": "2025-06-24 13:54:47 UTC"
  },
  {
    "arxiv_id": "2506.19630v1",
    "title": "Why Uncertainty Calibration Matters for Reliable Perturbation-based Explanations",
    "authors": [
      "Thomas Decker",
      "Volker Tresp",
      "Florian Buettner"
    ],
    "abstract": "Perturbation-based explanations are widely utilized to enhance the transparency of modern machine-learning models. However, their reliability is often compromised by the unknown model behavior under the specific perturbations used. This paper investigates the relationship between uncertainty calibration - the alignment of model confidence with actual accuracy - and perturbation-based explanations. We show that models frequently produce unreliable probability estimates when subjected to explainability-specific perturbations and theoretically prove that this directly undermines explanation quality. To address this, we introduce ReCalX, a novel approach to recalibrate models for improved perturbation-based explanations while preserving their original predictions. Experiments on popular computer vision models demonstrate that our calibration strategy produces explanations that are more aligned with human perception and actual object locations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2025 Workshop: XAI4Science: From Understanding Model Behavior to Discovering New Scientific Knowledge",
    "pdf_url": "https://arxiv.org/pdf/2506.19630v1",
    "published_date": "2025-06-24 13:54:12 UTC",
    "updated_date": "2025-06-24 13:54:12 UTC"
  },
  {
    "arxiv_id": "2506.19621v1",
    "title": "VideoPCDNet: Video Parsing and Prediction with Phase Correlation Networks",
    "authors": [
      "Noel José Rodrigues Vicente",
      "Enrique Lehner",
      "Angel Villar-Corrales",
      "Jan Nogga",
      "Sven Behnke"
    ],
    "abstract": "Understanding and predicting video content is essential for planning and reasoning in dynamic environments. Despite advancements, unsupervised learning of object representations and dynamics remains challenging. We present VideoPCDNet, an unsupervised framework for object-centric video decomposition and prediction. Our model uses frequency-domain phase correlation techniques to recursively parse videos into object components, which are represented as transformed versions of learned object prototypes, enabling accurate and interpretable tracking. By explicitly modeling object motion through a combination of frequency domain operations and lightweight learned modules, VideoPCDNet enables accurate unsupervised object tracking and prediction of future video frames. In our experiments, we demonstrate that VideoPCDNet outperforms multiple object-centric baseline models for unsupervised tracking and prediction on several synthetic datasets, while learning interpretable object and motion representations.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted for Publication at ICANN 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.19621v1",
    "published_date": "2025-06-24 13:39:47 UTC",
    "updated_date": "2025-06-24 13:39:47 UTC"
  },
  {
    "arxiv_id": "2506.19613v1",
    "title": "Position: Intelligent Science Laboratory Requires the Integration of Cognitive and Embodied AI",
    "authors": [
      "Sha Zhang",
      "Suorong Yang",
      "Tong Xie",
      "Xiangyuan Xue",
      "Zixuan Hu",
      "Rui Li",
      "Wenxi Qu",
      "Zhenfei Yin",
      "Tianfan Fu",
      "Di Hu",
      "Andres M Bran",
      "Nian Ran",
      "Bram Hoex",
      "Wangmeng Zuo",
      "Philippe Schwaller",
      "Wanli Ouyang",
      "Lei Bai",
      "Yanyong Zhang",
      "Lingyu Duan",
      "Shixiang Tang",
      "Dongzhan Zhou"
    ],
    "abstract": "Scientific discovery has long been constrained by human limitations in expertise, physical capability, and sleep cycles. The recent rise of AI scientists and automated laboratories has accelerated both the cognitive and operational aspects of research. However, key limitations persist: AI systems are often confined to virtual environments, while automated laboratories lack the flexibility and autonomy to adaptively test new hypotheses in the physical world. Recent advances in embodied AI, such as generalist robot foundation models, diffusion-based action policies, fine-grained manipulation learning, and sim-to-real transfer, highlight the promise of integrating cognitive and embodied intelligence. This convergence opens the door to closed-loop systems that support iterative, autonomous experimentation and the possibility of serendipitous discovery. In this position paper, we propose the paradigm of Intelligent Science Laboratories (ISLs): a multi-layered, closed-loop framework that deeply integrates cognitive and embodied intelligence. ISLs unify foundation models for scientific reasoning, agent-based workflow orchestration, and embodied agents for robust physical experimentation. We argue that such systems are essential for overcoming the current limitations of scientific discovery and for realizing the full transformative potential of AI-driven science.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19613v1",
    "published_date": "2025-06-24 13:31:44 UTC",
    "updated_date": "2025-06-24 13:31:44 UTC"
  },
  {
    "arxiv_id": "2506.19608v2",
    "title": "ChordPrompt: Orchestrating Cross-Modal Prompt Synergy for Multi-Domain Incremental Learning in CLIP",
    "authors": [
      "Zhiyuan Wang",
      "Bokui Chen"
    ],
    "abstract": "Continual learning (CL) empowers pre-trained vision-language models to adapt effectively to novel or previously underrepresented data distributions without comprehensive retraining, enhancing their adaptability and efficiency. While vision-language models like CLIP show great promise, they struggle to maintain performance across domains in incremental learning scenarios. Existing prompt learning methods face two main limitations: 1) they primarily focus on class-incremental learning scenarios, lacking specific strategies for multi-domain task incremental learning; 2) most current approaches employ single-modal prompts, neglecting the potential benefits of cross-modal information exchange. To address these challenges, we propose the \\ChordPrompt framework, which facilitates a harmonious interplay between visual and textual prompts. \\ChordPrompt introduces cross-modal prompts to leverage interactions between visual and textual information. Our approach also employs domain-adaptive text prompts to select appropriate prompts for continual adaptation across multiple domains. Comprehensive experiments on multi-domain incremental learning benchmarks demonstrate that \\ChordPrompt outperforms state-of-the-art methods in zero-shot generalization and downstream task performance.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD 2025)",
    "pdf_url": "https://arxiv.org/pdf/2506.19608v2",
    "published_date": "2025-06-24 13:22:06 UTC",
    "updated_date": "2025-09-03 12:23:15 UTC"
  },
  {
    "arxiv_id": "2507.02908v1",
    "title": "Hyperbolic Kernel Graph Neural Networks for Neurocognitive Decline Analysis from Multimodal Brain Imaging",
    "authors": [
      "Meimei Yang",
      "Yongheng Sun",
      "Qianqian Wang",
      "Andrea Bozoki",
      "Maureen Kohi",
      "Mingxia Liu"
    ],
    "abstract": "Multimodal neuroimages, such as diffusion tensor imaging (DTI) and resting-state functional MRI (fMRI), offer complementary perspectives on brain activities by capturing structural or functional interactions among brain regions. While existing studies suggest that fusing these multimodal data helps detect abnormal brain activity caused by neurocognitive decline, they are generally implemented in Euclidean space and can't effectively capture intrinsic hierarchical organization of structural/functional brain networks. This paper presents a hyperbolic kernel graph fusion (HKGF) framework for neurocognitive decline analysis with multimodal neuroimages. It consists of a multimodal graph construction module, a graph representation learning module that encodes brain graphs in hyperbolic space through a family of hyperbolic kernel graph neural networks (HKGNNs), a cross-modality coupling module that enables effective multimodal data fusion, and a hyperbolic neural network for downstream predictions. Notably, HKGNNs represent graphs in hyperbolic space to capture both local and global dependencies among brain regions while preserving the hierarchical structure of brain networks. Extensive experiments involving over 4,000 subjects with DTI and/or fMRI data suggest the superiority of HKGF over state-of-the-art methods in two neurocognitive decline prediction tasks. HKGF is a general framework for multimodal data analysis, facilitating objective quantification of structural/functional brain connectivity changes associated with neurocognitive decline.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, 5 figures, 7 tables",
    "pdf_url": "https://arxiv.org/pdf/2507.02908v1",
    "published_date": "2025-06-24 13:16:37 UTC",
    "updated_date": "2025-06-24 13:16:37 UTC"
  },
  {
    "arxiv_id": "2506.19599v1",
    "title": "ECCoT: A Framework for Enhancing Effective Cognition via Chain of Thought in Large Language Model",
    "authors": [
      "Zhenke Duan",
      "Jiqun Pan",
      "Jiani Tu",
      "Xiaoyi Wang",
      "Yanqing Wang"
    ],
    "abstract": "In the era of large-scale artificial intelligence, Large Language Models (LLMs) have made significant strides in natural language processing. However, they often lack transparency and generate unreliable outputs, raising concerns about their interpretability. To address this, the Chain of Thought (CoT) prompting method structures reasoning into step-by-step deductions. Yet, not all reasoning chains are valid, and errors can lead to unreliable conclusions. We propose ECCoT, an End-to-End Cognitive Chain of Thought Validation Framework, to evaluate and refine reasoning chains in LLMs. ECCoT integrates the Markov Random Field-Embedded Topic Model (MRF-ETM) for topic-aware CoT generation and Causal Sentence-BERT (CSBert) for causal reasoning alignment. By filtering ineffective chains using structured ordering statistics, ECCoT improves interpretability, reduces biases, and enhances the trustworthiness of LLM-based decision-making. Key contributions include the introduction of ECCoT, MRF-ETM for topic-driven CoT generation, and CSBert for causal reasoning enhancement. Code is released at: https://github.com/erwinmsmith/ECCoT.git.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19599v1",
    "published_date": "2025-06-24 13:09:53 UTC",
    "updated_date": "2025-06-24 13:09:53 UTC"
  },
  {
    "arxiv_id": "2506.19597v1",
    "title": "Robotics Under Construction: Challenges on Job Sites",
    "authors": [
      "Haruki Uchiito",
      "Akhilesh Bhat",
      "Koji Kusaka",
      "Xiaoya Zhang",
      "Hiraku Kinjo",
      "Honoka Uehara",
      "Motoki Koyama",
      "Shinji Natsume"
    ],
    "abstract": "As labor shortages and productivity stagnation increasingly challenge the construction industry, automation has become essential for sustainable infrastructure development. This paper presents an autonomous payload transportation system as an initial step toward fully unmanned construction sites. Our system, based on the CD110R-3 crawler carrier, integrates autonomous navigation, fleet management, and GNSS-based localization to facilitate material transport in construction site environments. While the current system does not yet incorporate dynamic environment adaptation algorithms, we have begun fundamental investigations into external-sensor based perception and mapping system. Preliminary results highlight the potential challenges, including navigation in evolving terrain, environmental perception under construction-specific conditions, and sensor placement optimization for improving autonomy and efficiency. Looking forward, we envision a construction ecosystem where collaborative autonomous agents dynamically adapt to site conditions, optimizing workflow and reducing human intervention. This paper provides foundational insights into the future of robotics-driven construction automation and identifies critical areas for further technological development.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.AR",
      "cs.ET",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "Workshop on Field Robotics, ICRA",
    "pdf_url": "https://arxiv.org/pdf/2506.19597v1",
    "published_date": "2025-06-24 13:07:43 UTC",
    "updated_date": "2025-06-24 13:07:43 UTC"
  },
  {
    "arxiv_id": "2506.19897v1",
    "title": "Can LLMs Replace Humans During Code Chunking?",
    "authors": [
      "Christopher Glasz",
      "Emily Escamilla",
      "Eric O. Scott",
      "Anand Patel",
      "Jacob Zimmer",
      "Colin Diggs",
      "Michael Doyle",
      "Scott Rosen",
      "Nitin Naik",
      "Justin F. Brunelle",
      "Samruddhi Thaker",
      "Parthav Poudel",
      "Arun Sridharan",
      "Amit Madan",
      "Doug Wendt",
      "William Macke",
      "Thomas Schill"
    ],
    "abstract": "Large language models (LLMs) have become essential tools in computer science, especially for tasks involving code understanding and generation. However, existing work does not address many of the unique challenges presented by code written for government applications. In particular, government enterprise software is often written in legacy languages like MUMPS or assembly language code (ALC) and the overall token lengths of these systems exceed the context window size for current commercially available LLMs. Additionally, LLMs are primarily trained on modern software languages and have undergone limited testing with legacy languages, making their ability to understand legacy languages unknown and, hence, an area for empirical study. This paper examines the application of LLMs in the modernization of legacy government code written in ALC and MUMPS, addressing the challenges of input limitations. We investigate various code-chunking methods to optimize the generation of summary module comments for legacy code files, evaluating the impact of code-chunking methods on the quality of documentation produced by different LLMs, including GPT-4o, Claude 3 Sonnet, Mixtral, and Llama 3. Our results indicate that LLMs can select partition points closely aligned with human expert partitioning. We also find that chunking approaches have significant impact on downstream tasks such as documentation generation. LLM-created partitions produce comments that are up to 20% more factual and up to 10% more useful than when humans create partitions. Therefore, we conclude that LLMs can be used as suitable replacements for human partitioning of large codebases during LLM-aided modernization.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19897v1",
    "published_date": "2025-06-24 13:02:35 UTC",
    "updated_date": "2025-06-24 13:02:35 UTC"
  },
  {
    "arxiv_id": "2506.19592v2",
    "title": "Adaptive Domain Modeling with Language Models: A Multi-Agent Approach to Task Planning",
    "authors": [
      "Harisankar Babu",
      "Philipp Schillinger",
      "Tamim Asfour"
    ],
    "abstract": "We introduce TAPAS (Task-based Adaptation and Planning using AgentS), a multi-agent framework that integrates Large Language Models (LLMs) with symbolic planning to solve complex tasks without the need for manually defined environment models. TAPAS employs specialized LLM-based agents that collaboratively generate and adapt domain models, initial states, and goal specifications as needed using structured tool-calling mechanisms. Through this tool-based interaction, downstream agents can request modifications from upstream agents, enabling adaptation to novel attributes and constraints without manual domain redefinition. A ReAct (Reason+Act)-style execution agent, coupled with natural language plan translation, bridges the gap between dynamically generated plans and real-world robot capabilities. TAPAS demonstrates strong performance in benchmark planning domains and in the VirtualHome simulated real-world environment.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at IEEE CASE 2025, 8 pages, 8 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.19592v2",
    "published_date": "2025-06-24 13:02:06 UTC",
    "updated_date": "2025-06-30 14:40:24 UTC"
  },
  {
    "arxiv_id": "2506.19591v1",
    "title": "Vision Transformer-Based Time-Series Image Reconstruction for Cloud-Filling Applications",
    "authors": [
      "Lujun Li",
      "Yiqun Wang",
      "Radu State"
    ],
    "abstract": "Cloud cover in multispectral imagery (MSI) poses significant challenges for early season crop mapping, as it leads to missing or corrupted spectral information. Synthetic aperture radar (SAR) data, which is not affected by cloud interference, offers a complementary solution, but lack sufficient spectral detail for precise crop mapping. To address this, we propose a novel framework, Time-series MSI Image Reconstruction using Vision Transformer (ViT), to reconstruct MSI data in cloud-covered regions by leveraging the temporal coherence of MSI and the complementary information from SAR from the attention mechanism. Comprehensive experiments, using rigorous reconstruction evaluation metrics, demonstrate that Time-series ViT framework significantly outperforms baselines that use non-time-series MSI and SAR or time-series MSI without SAR, effectively enhancing MSI image reconstruction in cloud-covered regions.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "This paper has been accepted as a conference paper at the 2025 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)",
    "pdf_url": "https://arxiv.org/pdf/2506.19591v1",
    "published_date": "2025-06-24 13:00:36 UTC",
    "updated_date": "2025-06-24 13:00:36 UTC"
  },
  {
    "arxiv_id": "2506.19579v2",
    "title": "Evaluating the Robustness of Open-Source Vision-Language Models to Domain Shift in Object Captioning",
    "authors": [
      "Federico Tavella",
      "Amber Drinkwater",
      "Angelo Cangelosi"
    ],
    "abstract": "Vision-Language Models (VLMs) have emerged as powerful tools for generating textual descriptions from visual data. While these models excel on web-scale datasets, their robustness to the domain shifts inherent in many real-world applications remains under-explored. This paper presents a systematic evaluation of VLM performance on a single-view object captioning task when faced with a controlled, physical domain shift. We compare captioning accuracy across two distinct object sets: a collection of multi-material, real-world tools and a set of single-material, 3D-printed items. The 3D-printed set introduces a significant domain shift in texture and material properties, challenging the models' generalization capabilities. Our quantitative results demonstrate that all tested VLMs show a marked performance degradation when describing the 3D-printed objects compared to the real-world tools. This underscores a critical limitation in the ability of current models to generalize beyond surface-level features and highlights the need for more robust architectures for real-world signal processing applications.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19579v2",
    "published_date": "2025-06-24 12:45:09 UTC",
    "updated_date": "2025-09-16 15:12:16 UTC"
  },
  {
    "arxiv_id": "2506.19578v1",
    "title": "Towards an Introspective Dynamic Model of Globally Distributed Computing Infrastructures",
    "authors": [
      "Ozgur O. Kilic",
      "David K. Park",
      "Yihui Ren",
      "Tatiana Korchuganova",
      "Sairam Sri Vatsavai",
      "Joseph Boudreau",
      "Tasnuva Chowdhury",
      "Shengyu Feng",
      "Raees Khan",
      "Jaehyung Kim",
      "Scott Klasky",
      "Tadashi Maeno",
      "Paul Nilsson",
      "Verena Ingrid Martinez Outschoorn",
      "Norbert Podhorszki",
      "Frédéric Suter",
      "Wei Yang",
      "Yiming Yang",
      "Shinjae Yoo",
      "Alexei Klimentov",
      "Adolfy Hoisie"
    ],
    "abstract": "Large-scale scientific collaborations like ATLAS, Belle II, CMS, DUNE, and others involve hundreds of research institutes and thousands of researchers spread across the globe. These experiments generate petabytes of data, with volumes soon expected to reach exabytes. Consequently, there is a growing need for computation, including structured data processing from raw data to consumer-ready derived data, extensive Monte Carlo simulation campaigns, and a wide range of end-user analysis. To manage these computational and storage demands, centralized workflow and data management systems are implemented. However, decisions regarding data placement and payload allocation are often made disjointly and via heuristic means. A significant obstacle in adopting more effective heuristic or AI-driven solutions is the absence of a quick and reliable introspective dynamic model to evaluate and refine alternative approaches. In this study, we aim to develop such an interactive system using real-world data. By examining job execution records from the PanDA workflow management system, we have pinpointed key performance indicators such as queuing time, error rate, and the extent of remote data access. The dataset includes five months of activity. Additionally, we are creating a generative AI model to simulate time series of payloads, which incorporate visible features like category, event count, and submitting group, as well as hidden features like the total computational load-derived from existing PanDA records and computing site capabilities. These hidden features, which are not visible to job allocators, whether heuristic or AI-driven, influence factors such as queuing times and data movement.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19578v1",
    "published_date": "2025-06-24 12:42:36 UTC",
    "updated_date": "2025-06-24 12:42:36 UTC"
  },
  {
    "arxiv_id": "2506.19573v2",
    "title": "Interpretable Hybrid Machine Learning Models Using FOLD-R++ and Answer Set Programming",
    "authors": [
      "Sanne Wielinga",
      "Jesse Heyninck"
    ],
    "abstract": "Machine learning (ML) techniques play a pivotal role in high-stakes domains such as healthcare, where accurate predictions can greatly enhance decision-making. However, most high-performing methods such as neural networks and ensemble methods are often opaque, limiting trust and broader adoption. In parallel, symbolic methods like Answer Set Programming (ASP) offer the possibility of interpretable logical rules but do not always match the predictive power of ML models. This paper proposes a hybrid approach that integrates ASP-derived rules from the FOLD-R++ algorithm with black-box ML classifiers to selectively correct uncertain predictions and provide human-readable explanations. Experiments on five medical reveal statistically significant performance gains in accuracy and F1 score. This study underscores the potential of combining symbolic reasoning with conventional ML to achieve high interpretability without sacrificing accuracy",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "In Proceedings ICLP 2025, arXiv:2601.00047",
    "pdf_url": "https://arxiv.org/pdf/2506.19573v2",
    "published_date": "2025-06-24 12:37:17 UTC",
    "updated_date": "2026-01-07 13:39:17 UTC"
  },
  {
    "arxiv_id": "2506.19571v1",
    "title": "Has Machine Translation Evaluation Achieved Human Parity? The Human Reference and the Limits of Progress",
    "authors": [
      "Lorenzo Proietti",
      "Stefano Perrella",
      "Roberto Navigli"
    ],
    "abstract": "In Machine Translation (MT) evaluation, metric performance is assessed based on agreement with human judgments. In recent years, automatic metrics have demonstrated increasingly high levels of agreement with humans. To gain a clearer understanding of metric performance and establish an upper bound, we incorporate human baselines in the MT meta-evaluation, that is, the assessment of MT metrics' capabilities. Our results show that human annotators are not consistently superior to automatic metrics, with state-of-the-art metrics often ranking on par with or higher than human baselines. Despite these findings suggesting human parity, we discuss several reasons for caution. Finally, we explore the broader implications of our results for the research field, asking: Can we still reliably measure improvements in MT evaluation? With this work, we aim to shed light on the limits of our ability to measure progress in the field, fostering discussion on an issue that we believe is crucial to the entire MT evaluation community.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at ACL 2025 Main Conference. 24 pages",
    "pdf_url": "https://arxiv.org/pdf/2506.19571v1",
    "published_date": "2025-06-24 12:35:00 UTC",
    "updated_date": "2025-06-24 12:35:00 UTC"
  },
  {
    "arxiv_id": "2506.19567v1",
    "title": "FAF: A Feature-Adaptive Framework for Few-Shot Time Series Forecasting",
    "authors": [
      "Pengpeng Ouyang",
      "Dong Chen",
      "Tong Yang",
      "Shuo Feng",
      "Zhao Jin",
      "Mingliang Xu"
    ],
    "abstract": "Multi-task and few-shot time series forecasting tasks are commonly encountered in scenarios such as the launch of new products in different cities. However, traditional time series forecasting methods suffer from insufficient historical data, which stems from a disregard for the generalized and specific features among different tasks. For the aforementioned challenges, we propose the Feature-Adaptive Time Series Forecasting Framework (FAF), which consists of three key components: the Generalized Knowledge Module (GKM), the Task-Specific Module (TSM), and the Rank Module (RM). During training phase, the GKM is updated through a meta-learning mechanism that enables the model to extract generalized features across related tasks. Meanwhile, the TSM is trained to capture diverse local dynamics through multiple functional regions, each of which learns specific features from individual tasks. During testing phase, the RM dynamically selects the most relevant functional region from the TSM based on input sequence features, which is then combined with the generalized knowledge learned by the GKM to generate accurate forecasts. This design enables FAF to achieve robust and personalized forecasting even with sparse historical observations We evaluate FAF on five diverse real-world datasets under few-shot time series forecasting settings. Experimental results demonstrate that FAF consistently outperforms baselines that include three categories of time series forecasting methods. In particular, FAF achieves a 41.81\\% improvement over the best baseline, iTransformer, on the CO$_2$ emissions dataset.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages,4 figures, 8 tables",
    "pdf_url": "https://arxiv.org/pdf/2506.19567v1",
    "published_date": "2025-06-24 12:28:38 UTC",
    "updated_date": "2025-06-24 12:28:38 UTC"
  },
  {
    "arxiv_id": "2506.19563v1",
    "title": "PrivacyXray: Detecting Privacy Breaches in LLMs through Semantic Consistency and Probability Certainty",
    "authors": [
      "Jinwen He",
      "Yiyang Lu",
      "Zijin Lin",
      "Kai Chen",
      "Yue Zhao"
    ],
    "abstract": "Large Language Models (LLMs) are widely used in sensitive domains, including healthcare, finance, and legal services, raising concerns about potential private information leaks during inference. Privacy extraction attacks, such as jailbreaking, expose vulnerabilities in LLMs by crafting inputs that force the models to output sensitive information. However, these attacks cannot verify whether the extracted private information is accurate, as no public datasets exist for cross-validation, leaving a critical gap in private information detection during inference. To address this, we propose PrivacyXray, a novel framework detecting privacy breaches by analyzing LLM inner states. Our analysis reveals that LLMs exhibit higher semantic coherence and probabilistic certainty when generating correct private outputs. Based on this, PrivacyXray detects privacy breaches using four metrics: intra-layer and inter-layer semantic similarity, token-level and sentence-level probability distributions. PrivacyXray addresses critical challenges in private information detection by overcoming the lack of open-source private datasets and eliminating reliance on external data for validation. It achieves this through the synthesis of realistic private data and a detection mechanism based on the inner states of LLMs. Experiments show that PrivacyXray achieves consistent performance, with an average accuracy of 92.69% across five LLMs. Compared to state-of-the-art methods, PrivacyXray achieves significant improvements, with an average accuracy increase of 20.06%, highlighting its stability and practical utility in real-world applications.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19563v1",
    "published_date": "2025-06-24 12:22:59 UTC",
    "updated_date": "2025-06-24 12:22:59 UTC"
  },
  {
    "arxiv_id": "2506.19561v1",
    "title": "MambaOutRS: A Hybrid CNN-Fourier Architecture for Remote Sensing Image Classification",
    "authors": [
      "Minjong Cheon",
      "Changbae Mun"
    ],
    "abstract": "Recent advances in deep learning for vision tasks have seen the rise of State Space Models (SSMs) like Mamba, celebrated for their linear scalability. However, their adaptation to 2D visual data often necessitates complex modifications that may diminish efficiency. In this paper, we introduce MambaOutRS, a novel hybrid convolutional architecture for remote sensing image classification that re-evaluates the necessity of recurrent SSMs. MambaOutRS builds upon stacked Gated CNN blocks for local feature extraction and introduces a novel Fourier Filter Gate (FFG) module that operates in the frequency domain to capture global contextual information efficiently. Our architecture employs a four-stage hierarchical design and was extensively evaluated on challenging remote sensing datasets: UC Merced, AID, NWPU-RESISC45, and EuroSAT. MambaOutRS consistently achieved state-of-the-art (SOTA) performance across these benchmarks. Notably, our MambaOutRS-t variant (24.0M parameters) attained the highest F1-scores of 98.41\\% on UC Merced and 95.99\\% on AID, significantly outperforming existing baselines, including larger transformer models and Mamba-based architectures, despite using considerably fewer parameters. An ablation study conclusively demonstrates the critical role of the Fourier Filter Gate in enhancing the model's ability to capture global spatial patterns, leading to robust and accurate classification. These results strongly suggest that the complexities of recurrent SSMs can be effectively superseded by a judicious combination of gated convolutions for spatial mixing and frequency-based gates for spectral global context. Thus, MambaOutRS provides a compelling and efficient paradigm for developing high-performance deep learning models in remote sensing and other vision domains, particularly where computational efficiency is paramount.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19561v1",
    "published_date": "2025-06-24 12:20:11 UTC",
    "updated_date": "2025-06-24 12:20:11 UTC"
  },
  {
    "arxiv_id": "2506.19552v1",
    "title": "General Methods Make Great Domain-specific Foundation Models: A Case-study on Fetal Ultrasound",
    "authors": [
      "Jakob Ambsdorf",
      "Asbjørn Munk",
      "Sebastian Llambias",
      "Anders Nymark Christensen",
      "Kamil Mikolaj",
      "Randall Balestriero",
      "Martin Tolsgaard",
      "Aasa Feragen",
      "Mads Nielsen"
    ],
    "abstract": "With access to large-scale, unlabeled medical datasets, researchers are confronted with two questions: Should they attempt to pretrain a custom foundation model on this medical data, or use transfer-learning from an existing generalist model? And, if a custom model is pretrained, are novel methods required? In this paper we explore these questions by conducting a case-study, in which we train a foundation model on a large regional fetal ultrasound dataset of 2M images. By selecting the well-established DINOv2 method for pretraining, we achieve state-of-the-art results on three fetal ultrasound datasets, covering data from different countries, classification, segmentation, and few-shot tasks. We compare against a series of models pretrained on natural images, ultrasound images, and supervised baselines. Our results demonstrate two key insights: (i) Pretraining on custom data is worth it, even if smaller models are trained on less data, as scaling in natural image pretraining does not translate to ultrasound performance. (ii) Well-tuned methods from computer vision are making it feasible to train custom foundation models for a given medical domain, requiring no hyperparameter tuning and little methodological adaptation. Given these findings, we argue that a bias towards methodological innovation should be avoided when developing domain specific foundation models under common computational resource constraints.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Submitted version of paper accepted at MICCAI 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.19552v1",
    "published_date": "2025-06-24 12:00:13 UTC",
    "updated_date": "2025-06-24 12:00:13 UTC"
  },
  {
    "arxiv_id": "2506.19549v1",
    "title": "RCStat: A Statistical Framework for using Relative Contextualization in Transformers",
    "authors": [
      "Debabrata Mahapatra",
      "Shubham Agarwal",
      "Apoorv Saxena",
      "Subrata Mitra"
    ],
    "abstract": "Prior work on input-token importance in auto-regressive transformers has relied on Softmax-normalized attention weights, which obscure the richer structure of pre-Softmax query-key logits. We introduce RCStat, a statistical framework that harnesses raw attention logits via Relative Contextualization (RC), a random variable measuring contextual alignment between token segments, and derive an efficient upper bound for RC. We demonstrate two applications: (i) Key-Value compression, where RC-based thresholds drive adaptive key-value eviction for substantial cache reduction with minimal quality loss; and (ii) Attribution, where RC yields higher-fidelity token-, sentence-, and chunk-level explanations than post-Softmax methods. Across question answering, summarization, and attribution benchmarks, RCStat achieves significant empirical gains, delivering state-of-the-art compression and attribution performance without any model retraining.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19549v1",
    "published_date": "2025-06-24 11:55:43 UTC",
    "updated_date": "2025-06-24 11:55:43 UTC"
  },
  {
    "arxiv_id": "2506.19539v1",
    "title": "Lost in Translation? Converting RegExes for Log Parsing into Dynatrace Pattern Language",
    "authors": [
      "Julian Fragner",
      "Christian Macho",
      "Bernhard Dieber",
      "Martin Pinzger"
    ],
    "abstract": "Log files provide valuable information for detecting and diagnosing problems in enterprise software applications and data centers. Several log analytics tools and platforms were developed to help filter and extract information from logs, typically using regular expressions (RegExes). Recent commercial log analytics platforms provide domain-specific languages specifically designed for log parsing, such as Grok or the Dynatrace Pattern Language (DPL). However, users who want to migrate to these platforms must manually convert their RegExes into the new pattern language, which is costly and error-prone. In this work, we present Reptile, which combines a rule-based approach for converting RegExes into DPL patterns with a best-effort approach for cases where a full conversion is impossible. Furthermore, it integrates GPT-4 to optimize the obtained DPL patterns. The evaluation with 946 RegExes collected from a large company shows that Reptile safely converted 73.7% of them. The evaluation of Reptile's pattern optimization with 23 real-world RegExes showed an F1-score and MCC above 0.91. These results are promising and have ample practical implications for companies that migrate to a modern log analytics platform, such as Dynatrace.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "18 pages, 7 tables, 18 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.19539v1",
    "published_date": "2025-06-24 11:49:40 UTC",
    "updated_date": "2025-06-24 11:49:40 UTC"
  },
  {
    "arxiv_id": "2506.19531v1",
    "title": "ReMAR-DS: Recalibrated Feature Learning for Metal Artifact Reduction and CT Domain Transformation",
    "authors": [
      "Mubashara Rehman",
      "Niki Martinel",
      "Michele Avanzo",
      "Riccardo Spizzo",
      "Christian Micheloni"
    ],
    "abstract": "Artifacts in kilo-Voltage CT (kVCT) imaging degrade image quality, impacting clinical decisions. We propose a deep learning framework for metal artifact reduction (MAR) and domain transformation from kVCT to Mega-Voltage CT (MVCT). The proposed framework, ReMAR-DS, utilizes an encoder-decoder architecture with enhanced feature recalibration, effectively reducing artifacts while preserving anatomical structures. This ensures that only relevant information is utilized in the reconstruction process. By infusing recalibrated features from the encoder block, the model focuses on relevant spatial regions (e.g., areas with artifacts) and highlights key features across channels (e.g., anatomical structures), leading to improved reconstruction of artifact-corrupted regions. Unlike traditional MAR methods, our approach bridges the gap between high-resolution kVCT and artifact-resistant MVCT, enhancing radiotherapy planning. It produces high-quality MVCT-like reconstructions, validated through qualitative and quantitative evaluations. Clinically, this enables oncologists to rely on kVCT alone, reducing repeated high-dose MVCT scans and lowering radiation exposure for cancer patients.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted in 23rd International Conference on Image Analysis and Processing (ICIAP) 2025, Italy",
    "pdf_url": "https://arxiv.org/pdf/2506.19531v1",
    "published_date": "2025-06-24 11:34:35 UTC",
    "updated_date": "2025-06-24 11:34:35 UTC"
  },
  {
    "arxiv_id": "2506.19530v2",
    "title": "NTRL: Encounter Generation via Reinforcement Learning for Dynamic Difficulty Adjustment in Dungeons and Dragons",
    "authors": [
      "Carlo Romeo",
      "Andrew D. Bagdanov"
    ],
    "abstract": "Balancing combat encounters in Dungeons & Dragons (D&D) is a complex task that requires Dungeon Masters (DM) to manually assess party strength, enemy composition, and dynamic player interactions while avoiding interruption of the narrative flow. In this paper, we propose Encounter Generation via Reinforcement Learning (NTRL), a novel approach that automates Dynamic Difficulty Adjustment (DDA) in D&D via combat encounter design. By framing the problem as a contextual bandit, NTRL generates encounters based on real-time party members attributes. In comparison with classic DM heuristics, NTRL iteratively optimizes encounters to extend combat longevity (+200%), increases damage dealt to party members, reducing post-combat hit points (-16.67%), and raises the number of player deaths while maintaining low total party kills (TPK). The intensification of combat forces players to act wisely and engage in tactical maneuvers, even though the generated encounters guarantee high win rates (70%). Even in comparison with encounters designed by human Dungeon Masters, NTRL demonstrates superior performance by enhancing the strategic depth of combat while increasing difficulty in a manner that preserves overall game fairness.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19530v2",
    "published_date": "2025-06-24 11:34:01 UTC",
    "updated_date": "2025-07-16 21:26:44 UTC"
  },
  {
    "arxiv_id": "2506.19525v1",
    "title": "Automatic Posology Structuration : What role for LLMs?",
    "authors": [
      "Natalia Bobkova",
      "Laura Zanella-Calzada",
      "Anyes Tafoughalt",
      "Raphaël Teboul",
      "François Plesse",
      "Félix Gaschi"
    ],
    "abstract": "Automatically structuring posology instructions is essential for improving medication safety and enabling clinical decision support. In French prescriptions, these instructions are often ambiguous, irregular, or colloquial, limiting the effectiveness of classic ML pipelines. We explore the use of Large Language Models (LLMs) to convert free-text posologies into structured formats, comparing prompt-based methods and fine-tuning against a \"pre-LLM\" system based on Named Entity Recognition and Linking (NERL). Our results show that while prompting improves performance, only fine-tuned LLMs match the accuracy of the baseline. Through error analysis, we observe complementary strengths: NERL offers structural precision, while LLMs better handle semantic nuances. Based on this, we propose a hybrid pipeline that routes low-confidence cases from NERL (<0.8) to the LLM, selecting outputs based on confidence scores. This strategy achieves 91% structuration accuracy while minimizing latency and compute. Our results show that this hybrid approach improves structuration accuracy while limiting computational cost, offering a scalable solution for real-world clinical use.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19525v1",
    "published_date": "2025-06-24 11:25:21 UTC",
    "updated_date": "2025-06-24 11:25:21 UTC"
  },
  {
    "arxiv_id": "2506.19895v1",
    "title": "A Framework for Uncertainty Quantification Based on Nearest Neighbors Across Layers",
    "authors": [
      "Miguel N. Font",
      "José L. Jorro-Aragoneses",
      "Carlos M. Alaíz"
    ],
    "abstract": "Neural Networks have high accuracy in solving problems where it is difficult to detect patterns or create a logical model. However, these algorithms sometimes return wrong solutions, which become problematic in high-risk domains like medical diagnosis or autonomous driving. One strategy to detect and mitigate these errors is the measurement of the uncertainty over neural network decisions. In this paper, we present a novel post-hoc framework for measuring the uncertainty of a decision based on retrieved training cases that have a similar activation vector to the query for each layer. Based on these retrieved cases, we propose two new metrics: Decision Change and Layer Uncertainty, which capture changes in nearest-neighbor class distributions across layers. We evaluated our approach in a classification model for two datasets: CIFAR-10 and MNIST. The results show that these metrics enhance uncertainty estimation, especially in challenging classification tasks, outperforming softmax-based confidence.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper has been accepted for presentation at ICANN 2025 (International Conference on Artificial Neural Networks) and will appear in the conference proceedings published by Springer Nature in the Lecture Notes in Computer Science (LNCS) series. The final authenticated version will be available on the publisher website",
    "pdf_url": "https://arxiv.org/pdf/2506.19895v1",
    "published_date": "2025-06-24 11:10:41 UTC",
    "updated_date": "2025-06-24 11:10:41 UTC"
  },
  {
    "arxiv_id": "2506.19894v1",
    "title": "Explaining deep neural network models for electricity price forecasting with XAI",
    "authors": [
      "Antoine Pesenti",
      "Aidan OSullivan"
    ],
    "abstract": "Electricity markets are highly complex, involving lots of interactions and complex dependencies that make it hard to understand the inner workings of the market and what is driving prices. Econometric methods have been developed for this, white-box models, however, they are not as powerful as deep neural network models (DNN). In this paper, we use a DNN to forecast the price and then use XAI methods to understand the factors driving the price dynamics in the market. The objective is to increase our understanding of how different electricity markets work. To do that, we apply explainable methods such as SHAP and Gradient, combined with visual techniques like heatmaps (saliency maps) to analyse the behaviour and contributions of various features across five electricity markets. We introduce the novel concepts of SSHAP values and SSHAP lines to enhance the complex representation of high-dimensional tabular models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19894v1",
    "published_date": "2025-06-24 11:07:19 UTC",
    "updated_date": "2025-06-24 11:07:19 UTC"
  },
  {
    "arxiv_id": "2506.19893v1",
    "title": "Distillation-Enabled Knowledge Alignment for Generative Semantic Communications in AIGC Provisioning Tasks",
    "authors": [
      "Jingzhi Hu",
      "Geoffrey Ye Li"
    ],
    "abstract": "Due to the surging amount of AI-generated content (AIGC), its provisioning to edges and mobile users from the cloud incurs substantial traffic on networks. Generative semantic communication (GSC) offers a promising solution by transmitting highly compact information, i.e., prompt text and latent representations, instead of high-dimensional AIGC data. However, GSC relies on the alignment between the knowledge in the cloud generative AI (GAI) and that possessed by the edges and users, and between the knowledge for wireless transmission and that of actual channels, which remains challenging. In this paper, we propose DeKA-g, a distillation-enabled knowledge alignment algorithm for GSC systems. The core idea is to distill the generation knowledge from the cloud-GAI into low-rank matrices, which can be incorporated by the edge and used to adapt the transmission knowledge to diverse wireless channel conditions. DeKA-g comprises two novel methods: metaword-aided knowledge distillation (MAKD) and variable-rate grouped SNR adaptation (VGSA). For MAKD, an optimized metaword is employed to enhance the efficiency of knowledge distillation, while VGSA enables efficient adaptation to diverse compression rates and SNR ranges. From simulation results, DeKA-g improves the alignment between the edge-generated images and the cloud-generated ones by 44%. Moreover, it adapts to compression rates with 116% higher efficiency than the baseline and enhances the performance in low-SNR conditions by 28%.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "eess.IV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19893v1",
    "published_date": "2025-06-24 10:50:14 UTC",
    "updated_date": "2025-06-24 10:50:14 UTC"
  },
  {
    "arxiv_id": "2506.19892v1",
    "title": "RepuNet: A Reputation System for Mitigating Malicious Clients in DFL",
    "authors": [
      "Isaac Marroqui Penalva",
      "Enrique Tomás Martínez Beltrán",
      "Manuel Gil Pérez",
      "Alberto Huertas Celdrán"
    ],
    "abstract": "Decentralized Federated Learning (DFL) enables nodes to collaboratively train models without a central server, introducing new vulnerabilities since each node independently selects peers for model aggregation. Malicious nodes may exploit this autonomy by sending corrupted models (model poisoning), delaying model submissions (delay attack), or flooding the network with excessive messages, negatively affecting system performance. Existing solutions often depend on rigid configurations or additional infrastructures such as blockchain, leading to computational overhead, scalability issues, or limited adaptability. To overcome these limitations, this paper proposes RepuNet, a decentralized reputation system that categorizes threats in DFL and dynamically evaluates node behavior using metrics like model similarity, parameter changes, message latency, and communication volume. Nodes' influence in model aggregation is adjusted based on their reputation scores. RepuNet was integrated into the Nebula DFL platform and experimentally evaluated with MNIST and CIFAR-10 datasets under non-IID distributions, using federations of up to 25 nodes in both fully connected and random topologies. Different attack intensities, frequencies, and activation intervals were tested. Results demonstrated that RepuNet effectively detects and mitigates malicious behavior, achieving F1 scores above 95% for MNIST scenarios and approximately 76% for CIFAR-10 cases. These outcomes highlight RepuNet's adaptability, robustness, and practical potential for mitigating threats in decentralized federated learning environments.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.DC",
      "cs.LG",
      "cs.PF"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19892v1",
    "published_date": "2025-06-24 10:46:48 UTC",
    "updated_date": "2025-06-24 10:46:48 UTC"
  },
  {
    "arxiv_id": "2506.19502v2",
    "title": "MATE: LLM-Powered Multi-Agent Translation Environment for Accessibility Applications",
    "authors": [
      "Aleksandr Algazinov",
      "Matt Laing",
      "Paul Laban"
    ],
    "abstract": "Accessibility remains a critical concern in today's society, as many technologies are not developed to support the full range of user needs. Existing multi-agent systems (MAS) often cannot provide comprehensive assistance for users in need due to the lack of customization stemming from closed-source designs. Consequently, individuals with disabilities frequently encounter significant barriers when attempting to interact with digital environments. We introduce MATE, a multimodal accessibility MAS, which performs the modality conversions based on the user's needs. The system is useful for assisting people with disabilities by ensuring that data will be converted to an understandable format. For instance, if the user cannot see well and receives an image, the system converts this image to its audio description. MATE can be applied to a wide range of domains, industries, and areas, such as healthcare, and can become a useful assistant for various groups of users. The system supports multiple types of models, ranging from LLM API calling to using custom machine learning (ML) classifiers. This flexibility ensures that the system can be adapted to various needs and is compatible with a wide variety of hardware. Since the system is expected to run locally, it ensures the privacy and security of sensitive information. In addition, the framework can be effectively integrated with institutional technologies (e.g., digital healthcare service) for real-time user assistance. Furthermore, we introduce ModCon-Task-Identifier, a model that is capable of extracting the precise modality conversion task from the user input. Numerous experiments show that ModCon-Task-Identifier consistently outperforms other LLMs and statistical models on our custom data. Our code and data are publicly available at https://github.com/AlgazinovAleksandr/Multi-Agent-MATE.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19502v2",
    "published_date": "2025-06-24 10:40:23 UTC",
    "updated_date": "2025-07-15 06:04:25 UTC"
  },
  {
    "arxiv_id": "2506.19500v2",
    "title": "NaviAgent: Bilevel Planning on Tool Navigation Graph for Large-Scale Orchestration",
    "authors": [
      "Yan Jiang",
      "Hao Zhou",
      "LiZhong GU",
      "Ai Han",
      "TianLong Li"
    ],
    "abstract": "Large language models (LLMs) have recently demonstrated the ability to act as function call agents by invoking external tools, enabling them to solve tasks beyond their static knowledge. However, existing agents typically call tools step by step at a time without a global view of task structure. As tools depend on each other, this leads to error accumulation and limited scalability, particularly when scaling to thousands of tools. To address these limitations, we propose NaviAgent, a novel bilevel architecture that decouples task planning from tool execution through graph-based modeling of the tool ecosystem. At the task-planning level, the LLM-based agent decides whether to respond directly, clarify user intent, invoke a toolchain, or execute tool outputs, ensuring broad coverage of interaction scenarios independent of inter-tool complexity. At the execution level, a continuously evolving Tool World Navigation Model (TWNM) encodes structural and behavioral relations among tools, guiding the agent to generate scalable and robust invocation sequences. By incorporating feedback from real tool interactions, NaviAgent supports closed-loop optimization of planning and execution, moving beyond tool calling toward adaptive navigation of large-scale tool ecosystems. Experiments show that NaviAgent achieves the best task success rates across models and tasks, and integrating TWMN further boosts performance by up to 17 points on complex tasks, underscoring its key role in toolchain orchestration.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19500v2",
    "published_date": "2025-06-24 10:39:07 UTC",
    "updated_date": "2025-10-31 14:24:22 UTC"
  },
  {
    "arxiv_id": "2506.19498v1",
    "title": "T-Rex: Task-Adaptive Spatial Representation Extraction for Robotic Manipulation with Vision-Language Models",
    "authors": [
      "Yiteng Chen",
      "Wenbo Li",
      "Shiyi Wang",
      "Huiping Zhuang",
      "Qingyao Wu"
    ],
    "abstract": "Building a general robotic manipulation system capable of performing a wide variety of tasks in real-world settings is a challenging task. Vision-Language Models (VLMs) have demonstrated remarkable potential in robotic manipulation tasks, primarily due to the extensive world knowledge they gain from large-scale datasets. In this process, Spatial Representations (such as points representing object positions or vectors representing object orientations) act as a bridge between VLMs and real-world scene, effectively grounding the reasoning abilities of VLMs and applying them to specific task scenarios. However, existing VLM-based robotic approaches often adopt a fixed spatial representation extraction scheme for various tasks, resulting in insufficient representational capability or excessive extraction time. In this work, we introduce T-Rex, a Task-Adaptive Framework for Spatial Representation Extraction, which dynamically selects the most appropriate spatial representation extraction scheme for each entity based on specific task requirements. Our key insight is that task complexity determines the types and granularity of spatial representations, and Stronger representational capabilities are typically associated with Higher overall system operation costs. Through comprehensive experiments in real-world robotic environments, we show that our approach delivers significant advantages in spatial understanding, efficiency, and stability without additional training.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "submitted to NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.19498v1",
    "published_date": "2025-06-24 10:36:15 UTC",
    "updated_date": "2025-06-24 10:36:15 UTC"
  },
  {
    "arxiv_id": "2506.19491v1",
    "title": "Experimental Assessment of Neural 3D Reconstruction for Small UAV-based Applications",
    "authors": [
      "Genís Castillo Gómez-Raya",
      "Álmos Veres-Vitályos",
      "Filip Lemic",
      "Pablo Royo",
      "Mario Montagud",
      "Sergi Fernández",
      "Sergi Abadal",
      "Xavier Costa-Pérez"
    ],
    "abstract": "The increasing miniaturization of Unmanned Aerial Vehicles (UAVs) has expanded their deployment potential to indoor and hard-to-reach areas. However, this trend introduces distinct challenges, particularly in terms of flight dynamics and power consumption, which limit the UAVs' autonomy and mission capabilities. This paper presents a novel approach to overcoming these limitations by integrating Neural 3D Reconstruction (N3DR) with small UAV systems for fine-grained 3-Dimensional (3D) digital reconstruction of small static objects. Specifically, we design, implement, and evaluate an N3DR-based pipeline that leverages advanced models, i.e., Instant-ngp, Nerfacto, and Splatfacto, to improve the quality of 3D reconstructions using images of the object captured by a fleet of small UAVs. We assess the performance of the considered models using various imagery and pointcloud metrics, comparing them against the baseline Structure from Motion (SfM) algorithm. The experimental results demonstrate that the N3DR-enhanced pipeline significantly improves reconstruction quality, making it feasible for small UAVs to support high-precision 3D mapping and anomaly detection in constrained environments. In more general terms, our results highlight the potential of N3DR in advancing the capabilities of miniaturized UAV systems.",
    "categories": [
      "cs.ET",
      "cs.AI",
      "cs.CV",
      "cs.NI",
      "eess.IV"
    ],
    "primary_category": "cs.ET",
    "comment": "6 pages, 7 figures, 2 tables, accepted at IEEE International Symposium on Personal, Indoor and Mobile Radio Communications 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.19491v1",
    "published_date": "2025-06-24 10:25:17 UTC",
    "updated_date": "2025-06-24 10:25:17 UTC"
  },
  {
    "arxiv_id": "2506.19486v1",
    "title": "Recalling The Forgotten Class Memberships: Unlearned Models Can Be Noisy Labelers to Leak Privacy",
    "authors": [
      "Zhihao Sui",
      "Liang Hu",
      "Jian Cao",
      "Dora D. Liu",
      "Usman Naseem",
      "Zhongyuan Lai",
      "Qi Zhang"
    ],
    "abstract": "Machine Unlearning (MU) technology facilitates the removal of the influence of specific data instances from trained models on request. Despite rapid advancements in MU technology, its vulnerabilities are still underexplored, posing potential risks of privacy breaches through leaks of ostensibly unlearned information. Current limited research on MU attacks requires access to original models containing privacy data, which violates the critical privacy-preserving objective of MU. To address this gap, we initiate an innovative study on recalling the forgotten class memberships from unlearned models (ULMs) without requiring access to the original one. Specifically, we implement a Membership Recall Attack (MRA) framework with a teacher-student knowledge distillation architecture, where ULMs serve as noisy labelers to transfer knowledge to student models. Then, it is translated into a Learning with Noisy Labels (LNL) problem for inferring the correct labels of the forgetting instances. Extensive experiments on state-of-the-art MU methods with multiple real datasets demonstrate that the proposed MRA strategy exhibits high efficacy in recovering class memberships of unlearned instances. As a result, our study and evaluation have established a benchmark for future research on MU vulnerabilities.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "IJCAI 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.19486v1",
    "published_date": "2025-06-24 10:21:10 UTC",
    "updated_date": "2025-06-24 10:21:10 UTC"
  },
  {
    "arxiv_id": "2506.19484v1",
    "title": "Dialogic Pedagogy for Large Language Models: Aligning Conversational AI with Proven Theories of Learning",
    "authors": [
      "Russell Beale"
    ],
    "abstract": "Large Language Models (LLMs) are rapidly transforming education by enabling rich conversational learning experiences. This article provides a comprehensive review of how LLM-based conversational agents are being used in higher education, with extensions to secondary and lifelong learning contexts. We synthesize existing literature on LLMs in education and theories of conversational and dialogic pedagogy - including Vygotsky's sociocultural learning (scaffolding and the Zone of Proximal Development), the Socratic method, and Laurillard's conversational framework - and examine how prompting strategies and retrieval-augmented generation (RAG) can align LLM behaviors with these pedagogical theories, and how it can support personalized, adaptive learning. We map educational theories to LLM capabilities, highlighting where LLM-driven dialogue supports established learning principles and where it challenges or falls short of traditional pedagogical assumptions. Notable gaps in applying prior theories to LLMs are identified, such as the models tendency to provide direct answers instead of fostering co-construction of knowledge, and the need to account for the constant availability and broad but non-human expertise of LLM tutors. In response, we propose practical strategies to better align LLM interactions with sound pedagogy - for example, designing prompts that encourage Socratic questioning, scaffolded guidance, and student reflection, as well as integrating retrieval mechanisms to ensure accuracy and contextual relevance. Our aim is to bridge the gap between educational theory and the emerging practice of AI-driven conversational learning, offering insights and tools for making LLM-based dialogues more educationally productive and theory-aligned.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19484v1",
    "published_date": "2025-06-24 10:19:09 UTC",
    "updated_date": "2025-06-24 10:19:09 UTC"
  },
  {
    "arxiv_id": "2506.19482v1",
    "title": "Fast and Distributed Equivariant Graph Neural Networks by Virtual Node Learning",
    "authors": [
      "Yuelin Zhang",
      "Jiacheng Cen",
      "Jiaqi Han",
      "Wenbing Huang"
    ],
    "abstract": "Equivariant Graph Neural Networks (GNNs) have achieved remarkable success across diverse scientific applications. However, existing approaches face critical efficiency challenges when scaling to large geometric graphs and suffer significant performance degradation when the input graphs are sparsified for computational tractability. To address these limitations, we introduce FastEGNN and DistEGNN, two novel enhancements to equivariant GNNs for large-scale geometric graphs. FastEGNN employs a key innovation: a small ordered set of virtual nodes that effectively approximates the large unordered graph of real nodes. Specifically, we implement distinct message passing and aggregation mechanisms for different virtual nodes to ensure mutual distinctiveness, and minimize Maximum Mean Discrepancy (MMD) between virtual and real coordinates to achieve global distributedness. This design enables FastEGNN to maintain high accuracy while efficiently processing large-scale sparse graphs. For extremely large-scale geometric graphs, we present DistEGNN, a distributed extension where virtual nodes act as global bridges between subgraphs in different devices, maintaining consistency while dramatically reducing memory and computational overhead. We comprehensively evaluate our models across four challenging domains: N-body systems (100 nodes), protein dynamics (800 nodes), Water-3D (8,000 nodes), and our new Fluid113K benchmark (113,000 nodes). Results demonstrate superior efficiency and performance, establishing new capabilities in large-scale equivariant graph learning. Code is available at https://github.com/GLAD-RUC/DistEGNN.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19482v1",
    "published_date": "2025-06-24 10:17:38 UTC",
    "updated_date": "2025-06-24 10:17:38 UTC"
  },
  {
    "arxiv_id": "2506.19469v1",
    "title": "Surgery-R1: Advancing Surgical-VQLA with Reasoning Multimodal Large Language Model via Reinforcement Learning",
    "authors": [
      "Pengfei Hao",
      "Shuaibo Li",
      "Hongqiu Wang",
      "Zhizhuo Kou",
      "Junhang Zhang",
      "Guang Yang",
      "Lei Zhu"
    ],
    "abstract": "In recent years, significant progress has been made in the field of surgical scene understanding, particularly in the task of Visual Question Localized-Answering in robotic surgery (Surgical-VQLA). However, existing Surgical-VQLA models lack deep reasoning capabilities and interpretability in surgical scenes, which limits their reliability and potential for development in clinical applications. To address this issue, inspired by the development of Reasoning Multimodal Large Language Models (MLLMs), we first build the Surgery-R1-54k dataset, including paired data for Visual-QA, Grounding-QA, and Chain-of-Thought (CoT). Then, we propose the first Reasoning MLLM for Surgical-VQLA (Surgery-R1). In our Surgery-R1, we design a two-stage fine-tuning mechanism to enable the basic MLLM with complex reasoning abilities by utilizing supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT). Furthermore, for an efficient and high-quality rule-based reward system in our RFT, we design a Multimodal Coherence reward mechanism to mitigate positional illusions that may arise in surgical scenarios. Experiment results demonstrate that Surgery-R1 outperforms other existing state-of-the-art (SOTA) models in the Surgical-VQLA task and widely-used MLLMs, while also validating its reasoning capabilities and the effectiveness of our approach. The code and dataset will be organized in https://github.com/FiFi-HAO467/Surgery-R1.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19469v1",
    "published_date": "2025-06-24 09:53:10 UTC",
    "updated_date": "2025-06-24 09:53:10 UTC"
  },
  {
    "arxiv_id": "2506.19468v1",
    "title": "MuBench: Assessment of Multilingual Capabilities of Large Language Models Across 61 Languages",
    "authors": [
      "Wenhan Han",
      "Yifan Zhang",
      "Zhixun Chen",
      "Binbin Liu",
      "Haobin Lin",
      "Bingni Zhang",
      "Taifeng Wang",
      "Mykola Pechenizkiy",
      "Meng Fang",
      "Yin Zheng"
    ],
    "abstract": "Multilingual large language models (LLMs) are advancing rapidly, with new models frequently claiming support for an increasing number of languages. However, existing evaluation datasets are limited and lack cross-lingual alignment, leaving assessments of multilingual capabilities fragmented in both language and skill coverage. To address this, we introduce MuBench, a benchmark covering 61 languages and evaluating a broad range of capabilities. We evaluate several state-of-the-art multilingual LLMs and find notable gaps between claimed and actual language coverage, particularly a persistent performance disparity between English and low-resource languages. Leveraging MuBench's alignment, we propose Multilingual Consistency (MLC) as a complementary metric to accuracy for analyzing performance bottlenecks and guiding model improvement. Finally, we pretrain a suite of 1.2B-parameter models on English and Chinese with 500B tokens, varying language ratios and parallel data proportions to investigate cross-lingual transfer dynamics.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19468v1",
    "published_date": "2025-06-24 09:53:00 UTC",
    "updated_date": "2025-06-24 09:53:00 UTC"
  },
  {
    "arxiv_id": "2506.19891v2",
    "title": "Orthogonal Soft Pruning for Efficient Class Unlearning",
    "authors": [
      "Qinghui Gong",
      "Xue Yang",
      "Xiaohu Tang"
    ],
    "abstract": "Efficient and controllable data unlearning in federated learning remains challenging, due to the trade-off between forgetting and retention performance. Especially under non-independent and identically distributed (non-IID) settings, where deep feature entanglement exacerbates this dilemma. To address this challenge, we propose FedOrtho, a federated unlearning framework that combines orthogonalized deep convolutional kernels with an activation-driven controllable one-shot soft pruning (OSP) mechanism. FedOrtho enforces kernel orthogonality and local-global alignment to decouple feature representations and mitigate client drift. This structural independence enables precise one-shot pruning of forgetting-related kernels while preserving retained knowledge. FedOrtho achieves SOTA performance on CIFAR-10, CIFAR100 and TinyImageNet with ResNet and VGG frameworks, verifying that FedOrtho supports class-, client-, and sample-level unlearning with over 98% forgetting quality. It reduces computational and communication costs by 2-3 orders of magnitude in federated settings and achieves subsecond-level erasure in centralized scenarios while maintaining over 97% retention accuracy and mitigating membership inference risks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "17 pages,8 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.19891v2",
    "published_date": "2025-06-24 09:52:04 UTC",
    "updated_date": "2025-11-14 06:10:40 UTC"
  },
  {
    "arxiv_id": "2506.19467v3",
    "title": "Can Reasoning Help Large Language Models Capture Human Annotator Disagreement?",
    "authors": [
      "Jingwei Ni",
      "Yu Fan",
      "Vilém Zouhar",
      "Donya Rooein",
      "Alexander Hoyle",
      "Mrinmaya Sachan",
      "Markus Leippold",
      "Dirk Hovy",
      "Elliott Ash"
    ],
    "abstract": "Variation in human annotation (i.e., disagreements) is common in NLP, often reflecting important information like task subjectivity and sample ambiguity. Modeling this variation is important for applications that are sensitive to such information. Although RLVR-style reasoning (Reinforcement Learning with Verifiable Rewards) has improved Large Language Model (LLM) performance on many tasks, it remains unclear whether such reasoning enables LLMs to capture informative variation in human annotation. In this work, we evaluate the influence of different reasoning settings on LLM disagreement modeling. We systematically evaluate each reasoning setting across model sizes, distribution expression methods, and steering methods, resulting in 60 experimental setups across 3 tasks. Surprisingly, our results show that RLVR-style reasoning degrades performance in disagreement modeling, while naive Chain-of-Thought (CoT) reasoning improves the performance of RLHF LLMs (RL from human feedback). These findings underscore the potential risk of replacing human annotators with reasoning LLMs, especially when disagreements are important.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "EACL 2026 Main",
    "pdf_url": "https://arxiv.org/pdf/2506.19467v3",
    "published_date": "2025-06-24 09:49:26 UTC",
    "updated_date": "2026-01-12 13:46:04 UTC"
  },
  {
    "arxiv_id": "2506.19466v2",
    "title": "KunLunBaizeRAG: Reinforcement Learning Driven Inference Performance Leap for Large Language Models",
    "authors": [
      "Cheng Li",
      "Jiexiong Liu",
      "Yixuan Chen",
      "Qihang Zhou",
      "KunLun Meta"
    ],
    "abstract": "This paper introduces KunLunBaizeRAG, a reinforcement learning-driven reasoning framework designed to enhance the reasoning capabilities of large language models (LLMs) in complex multi-hop question-answering tasks. The framework addresses key limitations of traditional RAG, such as retrieval drift, information redundancy, and strategy rigidity. Key innovations include the RAG-driven Reasoning Alignment (RDRA) mechanism, the Search-Think Iterative Enhancement (STIE) mechanism, the Network-Local Intelligent Routing (NLR) mechanism, and a progressive hybrid training strategy. Experimental results demonstrate significant improvements in exact match (EM) and LLM-judged score (LJ) across four benchmarks, highlighting the framework's robustness and effectiveness in complex reasoning scenarios.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19466v2",
    "published_date": "2025-06-24 09:48:01 UTC",
    "updated_date": "2025-06-27 08:11:14 UTC"
  },
  {
    "arxiv_id": "2506.19465v1",
    "title": "Stylized Structural Patterns for Improved Neural Network Pre-training",
    "authors": [
      "Farnood Salehi",
      "Vandit Sharma",
      "Amirhossein Askari Farsangi",
      "Tunç Ozan Aydın"
    ],
    "abstract": "Modern deep learning models in computer vision require large datasets of real images, which are difficult to curate and pose privacy and legal concerns, limiting their commercial use. Recent works suggest synthetic data as an alternative, yet models trained with it often underperform. This paper proposes a two-step approach to bridge this gap. First, we propose an improved neural fractal formulation through which we introduce a new class of synthetic data. Second, we propose reverse stylization, a technique that transfers visual features from a small, license-free set of real images onto synthetic datasets, enhancing their effectiveness. We analyze the domain gap between our synthetic datasets and real images using Kernel Inception Distance (KID) and show that our method achieves a significantly lower distributional gap compared to existing synthetic datasets. Furthermore, our experiments across different tasks demonstrate the practical impact of this reduced gap. We show that pretraining the EDM2 diffusion model on our synthetic dataset leads to an 11% reduction in FID during image generation, compared to models trained on existing synthetic datasets, and a 20% decrease in autoencoder reconstruction error, indicating improved performance in data representation. Furthermore, a ViT-S model trained for classification on this synthetic data achieves over a 10% improvement in ImageNet-100 accuracy. Our work opens up exciting possibilities for training practical models when sufficiently large real training sets are not available.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19465v1",
    "published_date": "2025-06-24 09:47:31 UTC",
    "updated_date": "2025-06-24 09:47:31 UTC"
  },
  {
    "arxiv_id": "2506.19461v2",
    "title": "Iterative Quantum Feature Maps",
    "authors": [
      "Nasa Matsumoto",
      "Quoc Hoan Tran",
      "Koki Chinzei",
      "Yasuhiro Endo",
      "Hirotaka Oshima"
    ],
    "abstract": "Quantum machine learning models that leverage quantum circuits as quantum feature maps (QFMs) are recognized for their enhanced expressive power in learning tasks. Such models have demonstrated rigorous end-to-end quantum speedups for specific families of classification problems. However, deploying deep QFMs on real quantum hardware remains challenging due to circuit noise and hardware constraints. Additionally, variational quantum algorithms often suffer from computational bottlenecks, particularly in accurate gradient estimation, which significantly increases quantum resource demands during training. We propose Iterative Quantum Feature Maps (IQFMs), a hybrid quantum-classical framework that constructs a deep architecture by iteratively connecting shallow QFMs with classically computed augmentation weights. By incorporating contrastive learning and a layer-wise training mechanism, the IQFMs framework effectively reduces quantum runtime and mitigates noise-induced degradation. In tasks involving noisy quantum data, numerical experiments show that the IQFMs framework outperforms quantum convolutional neural networks, without requiring the optimization of variational quantum parameters. Even for a typical classical image classification benchmark, a carefully designed IQFMs framework achieves performance comparable to that of classical neural networks. This framework presents a promising path to address current limitations and harness the full potential of quantum-enhanced machine learning.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "quant-ph",
    "comment": "13 pages, 12 figures; typos corrected, references added",
    "pdf_url": "https://arxiv.org/pdf/2506.19461v2",
    "published_date": "2025-06-24 09:40:10 UTC",
    "updated_date": "2025-10-21 07:57:24 UTC"
  },
  {
    "arxiv_id": "2506.19459v1",
    "title": "Tagged for Direction: Pinning Down Causal Edge Directions with Precision",
    "authors": [
      "Florian Peter Busch",
      "Moritz Willig",
      "Florian Guldan",
      "Kristian Kersting",
      "Devendra Singh Dhami"
    ],
    "abstract": "Not every causal relation between variables is equal, and this can be leveraged for the task of causal discovery. Recent research shows that pairs of variables with particular type assignments induce a preference on the causal direction of other pairs of variables with the same type. Although useful, this assignment of a specific type to a variable can be tricky in practice. We propose a tag-based causal discovery approach where multiple tags are assigned to each variable in a causal graph. Existing causal discovery approaches are first applied to direct some edges, which are then used to determine edge relations between tags. Then, these edge relations are used to direct the undirected edges. Doing so improves upon purely type-based relations, where the assumption of type consistency lacks robustness and flexibility due to being restricted to single types for each variable. Our experimental evaluations show that this boosts causal discovery and that these high-level tag relations fit common knowledge.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19459v1",
    "published_date": "2025-06-24 09:39:17 UTC",
    "updated_date": "2025-06-24 09:39:17 UTC"
  },
  {
    "arxiv_id": "2506.19433v2",
    "title": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments with a Hierarchical Spatial-Cognition Long-Short Memory System",
    "authors": [
      "Lixuan He",
      "Haoyu Dong",
      "Zhenxing Chen",
      "Yangcheng Yu",
      "Jie Feng",
      "Yong Li"
    ],
    "abstract": "Vision-and-Language Navigation (VLN) in large-scale urban environments requires embodied agents to ground linguistic instructions in complex scenes and recall relevant experiences over extended time horizons. Prior modular pipelines offer interpretability but lack unified memory, while end-to-end (M)LLM agents excel at fusing vision and language yet remain constrained by fixed context windows and implicit spatial reasoning. We introduce \\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system that can augment any VLN backbone. Mem4Nav fuses a sparse octree for fine-grained voxel indexing with a semantic topology graph for high-level landmark connectivity, storing both in trainable memory tokens embedded via a reversible Transformer. Long-term memory (LTM) compresses and retains historical observations at both octree and graph nodes, while short-term memory (STM) caches recent multimodal entries in relative coordinates for real-time obstacle avoidance and local planning. At each step, STM retrieval sharply prunes dynamic context, and, when deeper history is needed, LTM tokens are decoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and Map2Seq across three backbones (modular, state-of-the-art VLN with prompt-based LLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13 pp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW improvement. Ablations confirm the indispensability of both the hierarchical map and dual memory modules. Our codes are open-sourced via https://github.com/tsinghua-fib-lab/Mem4Nav.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "The paper is currently under investigation regarding concerns of potential academic misconduct. While the investigation is ongoing, the authors have voluntarily requested to withdraw the manuscript",
    "pdf_url": "https://arxiv.org/pdf/2506.19433v2",
    "published_date": "2025-06-24 09:00:43 UTC",
    "updated_date": "2025-10-10 13:08:39 UTC"
  },
  {
    "arxiv_id": "2506.19420v1",
    "title": "Commander-GPT: Dividing and Routing for Multimodal Sarcasm Detection",
    "authors": [
      "Yazhou Zhang",
      "Chunwang Zou",
      "Bo Wang",
      "Jing Qin"
    ],
    "abstract": "Multimodal sarcasm understanding is a high-order cognitive task. Although large language models (LLMs) have shown impressive performance on many downstream NLP tasks, growing evidence suggests that they struggle with sarcasm understanding. In this paper, we propose Commander-GPT, a modular decision routing framework inspired by military command theory. Rather than relying on a single LLM's capability, Commander-GPT orchestrates a team of specialized LLM agents where each agent will be selectively assigned to a focused sub-task such as context modeling, sentiment analysis, etc. Their outputs are then routed back to the commander, which integrates the information and performs the final sarcasm judgment. To coordinate these agents, we introduce three types of centralized commanders: (1) a trained lightweight encoder-based commander (e.g., multi-modal BERT); (2) four small autoregressive language models, serving as moderately capable commanders (e.g., DeepSeek-VL); (3) two large LLM-based commander (Gemini Pro and GPT-4o) that performs task routing, output aggregation, and sarcasm decision-making in a zero-shot fashion. We evaluate Commander-GPT on the MMSD and MMSD 2.0 benchmarks, comparing five prompting strategies. Experimental results show that our framework achieves 4.4% and 11.7% improvement in F1 score over state-of-the-art (SoTA) baselines on average, demonstrating its effectiveness.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19420v1",
    "published_date": "2025-06-24 08:38:32 UTC",
    "updated_date": "2025-06-24 08:38:32 UTC"
  },
  {
    "arxiv_id": "2506.19410v1",
    "title": "Unsupervised Dataset Dictionary Learning for domain shift robust clustering: application to sitting posture identification",
    "authors": [
      "Anas Hattay",
      "Mayara Ayat",
      "Fred Ngole Mboula"
    ],
    "abstract": "This paper introduces a novel approach, Unsupervised Dataset Dictionary Learning (U-DaDiL), for totally unsupervised robust clustering applied to sitting posture identification. Traditional methods often lack adaptability to diverse datasets and suffer from domain shift issues. U-DaDiL addresses these challenges by aligning distributions from different datasets using Wasserstein barycenter based representation. Experimental evaluations on the Office31 dataset demonstrate significant improvements in cluster alignment accuracy. This work also presents a promising step for addressing domain shift and robust clustering for unsupervised sitting posture identification",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19410v1",
    "published_date": "2025-06-24 08:28:29 UTC",
    "updated_date": "2025-06-24 08:28:29 UTC"
  },
  {
    "arxiv_id": "2506.19408v1",
    "title": "Is an object-centric representation beneficial for robotic manipulation ?",
    "authors": [
      "Alexandre Chapin",
      "Emmanuel Dellandrea",
      "Liming Chen"
    ],
    "abstract": "Object-centric representation (OCR) has recently become a subject of interest in the computer vision community for learning a structured representation of images and videos. It has been several times presented as a potential way to improve data-efficiency and generalization capabilities to learn an agent on downstream tasks. However, most existing work only evaluates such models on scene decomposition, without any notion of reasoning over the learned representation. Robotic manipulation tasks generally involve multi-object environments with potential inter-object interaction. We thus argue that they are a very interesting playground to really evaluate the potential of existing object-centric work. To do so, we create several robotic manipulation tasks in simulated environments involving multiple objects (several distractors, the robot, etc.) and a high-level of randomization (object positions, colors, shapes, background, initial positions, etc.). We then evaluate one classical object-centric method across several generalization scenarios and compare its results against several state-of-the-art hollistic representations. Our results exhibit that existing methods are prone to failure in difficult scenarios involving complex scene structures, whereas object-centric methods help overcome these challenges.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19408v1",
    "published_date": "2025-06-24 08:23:55 UTC",
    "updated_date": "2025-06-24 08:23:55 UTC"
  },
  {
    "arxiv_id": "2506.19406v1",
    "title": "A Global-Local Cross-Attention Network for Ultra-high Resolution Remote Sensing Image Semantic Segmentation",
    "authors": [
      "Chen Yi",
      "Shan LianLei"
    ],
    "abstract": "With the rapid development of ultra-high resolution (UHR) remote sensing technology, the demand for accurate and efficient semantic segmentation has increased significantly. However, existing methods face challenges in computational efficiency and multi-scale feature fusion. To address these issues, we propose GLCANet (Global-Local Cross-Attention Network), a lightweight segmentation framework designed for UHR remote sensing imagery.GLCANet employs a dual-stream architecture to efficiently fuse global semantics and local details while minimizing GPU usage. A self-attention mechanism enhances long-range dependencies, refines global features, and preserves local details for better semantic consistency. A masked cross-attention mechanism also adaptively fuses global-local features, selectively enhancing fine-grained details while exploiting global context to improve segmentation accuracy. Experimental results show that GLCANet outperforms state-of-the-art methods regarding accuracy and computational efficiency. The model effectively processes large, high-resolution images with a small memory footprint, providing a promising solution for real-world remote sensing applications.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19406v1",
    "published_date": "2025-06-24 08:20:08 UTC",
    "updated_date": "2025-06-24 08:20:08 UTC"
  },
  {
    "arxiv_id": "2506.19399v1",
    "title": "Automated Detection of Pre-training Text in Black-box LLMs",
    "authors": [
      "Ruihan Hu",
      "Yu-Ming Shang",
      "Jiankun Peng",
      "Wei Luo",
      "Yazhe Wang",
      "Xi Zhang"
    ],
    "abstract": "Detecting whether a given text is a member of the pre-training data of Large Language Models (LLMs) is crucial for ensuring data privacy and copyright protection. Most existing methods rely on the LLM's hidden information (e.g., model parameters or token probabilities), making them ineffective in the black-box setting, where only input and output texts are accessible. Although some methods have been proposed for the black-box setting, they rely on massive manual efforts such as designing complicated questions or instructions. To address these issues, we propose VeilProbe, the first framework for automatically detecting LLMs' pre-training texts in a black-box setting without human intervention. VeilProbe utilizes a sequence-to-sequence mapping model to infer the latent mapping feature between the input text and the corresponding output suffix generated by the LLM. Then it performs the key token perturbations to obtain more distinguishable membership features. Additionally, considering real-world scenarios where the ground-truth training text samples are limited, a prototype-based membership classifier is introduced to alleviate the overfitting issue. Extensive evaluations on three widely used datasets demonstrate that our framework is effective and superior in the black-box setting.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "13 pages",
    "pdf_url": "https://arxiv.org/pdf/2506.19399v1",
    "published_date": "2025-06-24 08:08:15 UTC",
    "updated_date": "2025-06-24 08:08:15 UTC"
  },
  {
    "arxiv_id": "2506.19890v1",
    "title": "Causal-Aware Intelligent QoE Optimization for VR Interaction with Adaptive Keyframe Extraction",
    "authors": [
      "Ziru Zhang",
      "Jiadong Yu",
      "Danny H. K. Tsang"
    ],
    "abstract": "The optimization of quality of experience (QoE) in multi-user virtual reality (VR) interactions demands a delicate balance between ultra-low latency, high-fidelity motion synchronization, and equitable resource allocation. While adaptive keyframe extraction mitigates transmission overhead, existing approaches often overlook the causal relationships among allocated bandwidth, CPU frequency, and user perception, limiting QoE gains. This paper proposes an intelligent framework to maximize QoE by integrating adaptive keyframe extraction with causal-aware reinforcement learning (RL). First, a novel QoE metric is formulated using the Weber-Fechner Law, combining perceptual sensitivity, attention-driven priorities, and motion reconstruction accuracy. The QoE optimization problem is then modeled as a mixed integer programming (MIP) task, jointly optimizing keyframe ratios, bandwidth, and computational resources under horizon-fairness constraints. We propose Partial State Causal Deep Deterministic Policy Gradient (PS-CDDPG), which integrates the Deep Deterministic Policy Gradient (DDPG) method with causal influence detection. By leveraging causal information regarding how QoE is influenced and determined by various actions, we explore actions guided by weights calculated from causal inference (CI), which in turn improves training efficiency. Experiments conducted with the CMU Motion Capture Database demonstrate that our framework significantly reduces interactive latency, enhances QoE, and maintains fairness, achieving superior performance compared to benchmark methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19890v1",
    "published_date": "2025-06-24 07:32:34 UTC",
    "updated_date": "2025-06-24 07:32:34 UTC"
  },
  {
    "arxiv_id": "2506.19889v1",
    "title": "Retrieval-Confused Generation is a Good Defender for Privacy Violation Attack of Large Language Models",
    "authors": [
      "Wanli Peng",
      "Xin Chen",
      "Hang Fu",
      "XinYu He",
      "Xue Yiming",
      "Juan Wen"
    ],
    "abstract": "Recent advances in large language models (LLMs) have made a profound impact on our society and also raised new security concerns. Particularly, due to the remarkable inference ability of LLMs, the privacy violation attack (PVA), revealed by Staab et al., introduces serious personal privacy issues. Existing defense methods mainly leverage LLMs to anonymize the input query, which requires costly inference time and cannot gain satisfactory defense performance. Moreover, directly rejecting the PVA query seems like an effective defense method, while the defense method is exposed, promoting the evolution of PVA. In this paper, we propose a novel defense paradigm based on retrieval-confused generation (RCG) of LLMs, which can efficiently and covertly defend the PVA. We first design a paraphrasing prompt to induce the LLM to rewrite the \"user comments\" of the attack query to construct a disturbed database. Then, we propose the most irrelevant retrieval strategy to retrieve the desired user data from the disturbed database. Finally, the \"data comments\" are replaced with the retrieved user data to form a defended query, leading to responding to the adversary with some wrong personal attributes, i.e., the attack fails. Extensive experiments are conducted on two datasets and eight popular LLMs to comprehensively evaluate the feasibility and the superiority of the proposed defense method.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19889v1",
    "published_date": "2025-06-24 07:28:29 UTC",
    "updated_date": "2025-06-24 07:28:29 UTC"
  },
  {
    "arxiv_id": "2506.19387v1",
    "title": "NAADA: A Noise-Aware Attention Denoising Autoencoder for Dental Panoramic Radiographs",
    "authors": [
      "Khuram Naveed",
      "Bruna Neves de Freitas",
      "Ruben Pauwels"
    ],
    "abstract": "Convolutional denoising autoencoders (DAEs) are powerful tools for image restoration. However, they inherit a key limitation of convolutional neural networks (CNNs): they tend to recover low-frequency features, such as smooth regions, more effectively than high-frequency details. This leads to the loss of fine details, which is particularly problematic in dental radiographs where preserving subtle anatomical structures is crucial. While self-attention mechanisms can help mitigate this issue by emphasizing important features, conventional attention methods often prioritize features corresponding to cleaner regions and may overlook those obscured by noise. To address this limitation, we propose a noise-aware self-attention method, which allows the model to effectively focus on and recover key features even within noisy regions. Building on this approach, we introduce the noise-aware attention-enhanced denoising autoencoder (NAADA) network for enhancing noisy panoramic dental radiographs. Compared with the recent state of the art (and much heavier) methods like Uformer, MResDNN etc., our method improves the reconstruction of fine details, ensuring better image quality and diagnostic accuracy.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "10 pages, 8 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.19387v1",
    "published_date": "2025-06-24 07:23:04 UTC",
    "updated_date": "2025-06-24 07:23:04 UTC"
  },
  {
    "arxiv_id": "2506.19385v2",
    "title": "Conversational Intent-Driven GraphRAG: Enhancing Multi-Turn Dialogue Systems through Adaptive Dual-Retrieval of Flow Patterns and Context Semantics",
    "authors": [
      "Ziqi Zhu",
      "Tao Hu",
      "Honglong Zhang",
      "Dan Yang",
      "HanGeng Chen",
      "Mengran Zhang",
      "Xilun Chen"
    ],
    "abstract": "We present CID-GraphRAG (Conversational Intent-Driven Graph Retrieval Augmented Generation), a novel framework that addresses the limitations of existing dialogue systems in maintaining both contextual coherence and goal-oriented progression in multi-turn customer service conversations. Unlike traditional RAG systems that rely solely on semantic similarity (Conversation RAG) or standard knowledge graphs (GraphRAG), CID-GraphRAG constructs dynamic intent transition graphs from goal achieved historical dialogues and implements a dual-retrieval mechanism that adaptively balances intent-based graph traversal with semantic search. This approach enables the system to simultaneously leverage both conversional intent flow patterns and contextual semantics, significantly improving retrieval quality and response quality. In extensive experiments on real-world customer service dialogues, we employ both automatic metrics and LLM-as-judge assessments, demonstrating that CID-GraphRAG significantly outperforms both semantic-based Conversation RAG and intent-based GraphRAG baselines across all evaluation criteria. Quantitatively, CID-GraphRAG demonstrates substantial improvements over Conversation RAG across automatic metrics, with relative gains of 11% in BLEU, 5% in ROUGE-L, 6% in METEOR, and most notably, a 58% improvement in response quality according to LLM-as-judge evaluations. These results demonstrate that the integration of intent transition structures with semantic retrieval creates a synergistic effect that neither approach achieves independently, establishing CID-GraphRAG as an effective framework for addressing the challenges of maintaining contextual coherence and goal-oriented progression in knowledge-intensive multi-turn dialogues.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19385v2",
    "published_date": "2025-06-24 07:20:45 UTC",
    "updated_date": "2025-11-12 15:47:41 UTC"
  },
  {
    "arxiv_id": "2506.21625v2",
    "title": "Doc2SAR: A Synergistic Framework for High-Fidelity Extraction of Structure-Activity Relationships from Scientific Documents",
    "authors": [
      "Jiaxi Zhuang",
      "Kangning Li",
      "Jue Hou",
      "Mingjun Xu",
      "Zhifeng Gao",
      "Hengxing Cai"
    ],
    "abstract": "Extracting molecular structure-activity relationships (SARs) from scientific literature and patents is essential for drug discovery and materials research. However, this task remains challenging due to heterogeneous document formats and limitations of existing methods. Specifically, rule-based approaches relying on rigid templates fail to generalize across diverse document layouts, while general-purpose multimodal large language models (MLLMs) lack sufficient accuracy and reliability for specialized tasks, such as layout detection and optical chemical structure recognition (OCSR). To address these challenges, we introduce DocSAR-200, a rigorously annotated benchmark of 200 scientific documents designed specifically for evaluating SAR extraction methods. Additionally, we propose Doc2SAR, a novel synergistic framework that integrates domain-specific tools with MLLMs enhanced via supervised fine-tuning (SFT). Extensive experiments demonstrate that Doc2SAR achieves state-of-the-art performance across various document types, significantly outperforming leading end-to-end baselines. Specifically, Doc2SAR attains an overall Table Recall of 80.78% on DocSAR-200, exceeding end2end GPT-4o by 51.48%. Furthermore, Doc2SAR demonstrates practical usability through efficient inference and is accompanied by a web app.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.21625v2",
    "published_date": "2025-06-24 06:53:04 UTC",
    "updated_date": "2025-10-11 12:26:10 UTC"
  },
  {
    "arxiv_id": "2507.02905v1",
    "title": "Preference-Optimal Multi-Metric Weighting for Parallel Coordinate Plots",
    "authors": [
      "Chisa Mori",
      "Shuhei Watanabe",
      "Masaki Onishi",
      "Takayuki Itoh"
    ],
    "abstract": "Parallel coordinate plots (PCPs) are a prevalent method to interpret the relationship between the control parameters and metrics. PCPs deliver such an interpretation by color gradation based on a single metric. However, it is challenging to provide such a gradation when multiple metrics are present. Although a naive approach involves calculating a single metric by linearly weighting each metric, such weighting is unclear for users. To address this problem, we first propose a principled formulation for calculating the optimal weight based on a specific preferred metric combination. Although users can simply select their preference from a two-dimensional (2D) plane for bi-metric problems, multi-metric problems require intuitive visualization to allow them to select their preference. We achieved this using various radar charts to visualize the metric trade-offs on the 2D plane reduced by UMAP. In the analysis using pedestrian flow guidance planning, our method identified unique patterns of control parameter importance for each user preference, highlighting the effectiveness of our method.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted to International Conference Information Visualisation (iV2025)",
    "pdf_url": "https://arxiv.org/pdf/2507.02905v1",
    "published_date": "2025-06-24 06:48:09 UTC",
    "updated_date": "2025-06-24 06:48:09 UTC"
  },
  {
    "arxiv_id": "2506.19359v1",
    "title": "Evolutionary Level Repair",
    "authors": [
      "Debosmita Bhaumik",
      "Julian Togelius",
      "Georgios N. Yannakakis",
      "Ahmed Khalifa"
    ],
    "abstract": "We address the problem of game level repair, which consists of taking a designed but non-functional game level and making it functional. This might consist of ensuring the completeness of the level, reachability of objects, or other performance characteristics. The repair problem may also be constrained in that it can only make a small number of changes to the level. We investigate search-based solutions to the level repair problem, particularly using evolutionary and quality-diversity algorithms, with good results. This level repair method is applied to levels generated using a machine learning-based procedural content generation (PCGML) method that generates stylistically appropriate but frequently broken levels. This combination of PCGML for generation and search-based methods for repair shows great promise as a hybrid procedural content generation (PCG) method.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19359v1",
    "published_date": "2025-06-24 06:41:18 UTC",
    "updated_date": "2025-06-24 06:41:18 UTC"
  },
  {
    "arxiv_id": "2506.19358v2",
    "title": "From High-SNR Radar Signal to ECG: A Transfer Learning Model with Cardio-Focusing Algorithm for Scenarios with Limited Data",
    "authors": [
      "Yuanyuan Zhang",
      "Haocheng Zhao",
      "Sijie Xiong",
      "Rui Yang",
      "Eng Gee Lim",
      "Yutao Yue"
    ],
    "abstract": "Electrocardiogram (ECG), as a crucial find-grained cardiac feature, has been successfully recovered from radar signals in the literature, but the performance heavily relies on the high-quality radar signal and numerous radar-ECG pairs for training, restricting the applications in new scenarios due to data scarcity. Therefore, this work will focus on radar-based ECG recovery in new scenarios with limited data and propose a cardio-focusing and -tracking (CFT) algorithm to precisely track the cardiac location to ensure an efficient acquisition of high-quality radar signals. Furthermore, a transfer learning model (RFcardi) is proposed to extract cardio-related information from the radar signal without ECG ground truth based on the intrinsic sparsity of cardiac features, and only a few synchronous radar-ECG pairs are required to fine-tune the pre-trained model for the ECG recovery. The experimental results reveal that the proposed CFT can dynamically identify the cardiac location, and the RFcardi model can effectively generate faithful ECG recoveries after using a small number of radar-ECG pairs for training. The code and dataset are available after the publication.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19358v2",
    "published_date": "2025-06-24 06:38:34 UTC",
    "updated_date": "2025-10-23 02:18:47 UTC"
  },
  {
    "arxiv_id": "2506.19352v1",
    "title": "Spotting Out-of-Character Behavior: Atomic-Level Evaluation of Persona Fidelity in Open-Ended Generation",
    "authors": [
      "Jisu Shin",
      "Juhyun Oh",
      "Eunsu Kim",
      "Hoyun Song",
      "Alice Oh"
    ],
    "abstract": "Ensuring persona fidelity in large language models (LLMs) is essential for maintaining coherent and engaging human-AI interactions. However, LLMs often exhibit Out-of-Character (OOC) behavior, where generated responses deviate from an assigned persona, leading to inconsistencies that affect model reliability. Existing evaluation methods typically assign single scores to entire responses, struggling to capture subtle persona misalignment, particularly in long-form text generation. To address this limitation, we propose an atomic-level evaluation framework that quantifies persona fidelity at a finer granularity. Our three key metrics measure the degree of persona alignment and consistency within and across generations. Our approach enables a more precise and realistic assessment of persona fidelity by identifying subtle deviations that real users would encounter. Through our experiments, we demonstrate that our framework effectively detects persona inconsistencies that prior methods overlook. By analyzing persona fidelity across diverse tasks and personality types, we reveal how task structure and persona desirability influence model adaptability, highlighting challenges in maintaining consistent persona expression.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "Findings of ACL 2025; github repo: https://github.com/ddindidu/atomic-persona-evaluation/",
    "pdf_url": "https://arxiv.org/pdf/2506.19352v1",
    "published_date": "2025-06-24 06:33:10 UTC",
    "updated_date": "2025-06-24 06:33:10 UTC"
  },
  {
    "arxiv_id": "2506.19351v1",
    "title": "In-Context Occam's Razor: How Transformers Prefer Simpler Hypotheses on the Fly",
    "authors": [
      "Puneesh Deora",
      "Bhavya Vasudeva",
      "Tina Behnia",
      "Christos Thrampoulidis"
    ],
    "abstract": "In-context learning (ICL) enables transformers to adapt to new tasks through contextual examples without parameter updates. While existing research has typically studied ICL in fixed-complexity environments, practical language models encounter tasks spanning diverse complexity levels. This paper investigates how transformers navigate hierarchical task structures where higher-complexity categories can perfectly represent any pattern generated by simpler ones. We design well-controlled testbeds based on Markov chains and linear regression that reveal transformers not only identify the appropriate complexity level for each task but also accurately infer the corresponding parameters--even when the in-context examples are compatible with multiple complexity hypotheses. Notably, when presented with data generated by simpler processes, transformers consistently favor the least complex sufficient explanation. We theoretically explain this behavior through a Bayesian framework, demonstrating that transformers effectively implement an in-context Bayesian Occam's razor by balancing model fit against complexity penalties. We further ablate on the roles of model size, training mixture distribution, inference context length, and architecture. Finally, we validate this Occam's razor-like inductive bias on a pretrained GPT-4 model with Boolean-function tasks as case study, suggesting it may be inherent to transformers trained on diverse task distributions.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "28 pages, 19 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.19351v1",
    "published_date": "2025-06-24 06:33:00 UTC",
    "updated_date": "2025-06-24 06:33:00 UTC"
  },
  {
    "arxiv_id": "2506.19343v2",
    "title": "Discrepancy-Aware Graph Mask Auto-Encoder",
    "authors": [
      "Ziyu Zheng",
      "Yaming Yang",
      "Ziyu Guan",
      "Wei Zhao",
      "Weigang Lu"
    ],
    "abstract": "Masked Graph Auto-Encoder, a powerful graph self-supervised training paradigm, has recently shown superior performance in graph representation learning. Existing works typically rely on node contextual information to recover the masked information. However, they fail to generalize well to heterophilic graphs where connected nodes may be not similar, because they focus only on capturing the neighborhood information and ignoring the discrepancy information between different nodes, resulting in indistinguishable node representations. In this paper, to address this issue, we propose a Discrepancy-Aware Graph Mask Auto-Encoder (DGMAE). It obtains more distinguishable node representations by reconstructing the discrepancy information of neighboring nodes during the masking process. We conduct extensive experiments on 17 widely-used benchmark datasets. The results show that our DGMAE can effectively preserve the discrepancies of nodes in low-dimensional space. Moreover, DGMAE significantly outperforms state-of-the-art graph self-supervised learning methods on three graph analytic including tasks node classification, node clustering, and graph classification, demonstrating its remarkable superiority. The code of DGMAE is available at https://github.com/zhengziyu77/DGMAE.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by KDD2025",
    "pdf_url": "https://arxiv.org/pdf/2506.19343v2",
    "published_date": "2025-06-24 06:15:44 UTC",
    "updated_date": "2025-08-14 08:05:43 UTC"
  },
  {
    "arxiv_id": "2506.19342v1",
    "title": "Unlocking Insights Addressing Alcohol Inference Mismatch through Database-Narrative Alignment",
    "authors": [
      "Sudesh Bhagat",
      "Raghupathi Kandiboina",
      "Ibne Farabi Shihab",
      "Skylar Knickerbocker",
      "Neal Hawkins",
      "Anuj Sharma"
    ],
    "abstract": "Road traffic crashes are a significant global cause of fatalities, emphasizing the urgent need for accurate crash data to enhance prevention strategies and inform policy development. This study addresses the challenge of alcohol inference mismatch (AIM) by employing database narrative alignment to identify AIM in crash data. A framework was developed to improve data quality in crash management systems and reduce the percentage of AIM crashes. Utilizing the BERT model, the analysis of 371,062 crash records from Iowa (2016-2022) revealed 2,767 AIM incidents, resulting in an overall AIM percentage of 24.03%. Statistical tools, including the Probit Logit model, were used to explore the crash characteristics affecting AIM patterns. The findings indicate that alcohol-related fatal crashes and nighttime incidents have a lower percentage of the mismatch, while crashes involving unknown vehicle types and older drivers are more susceptible to mismatch. The geospatial cluster as part of this study can identify the regions which have an increased need for education and training. These insights highlight the necessity for targeted training programs and data management teams to improve the accuracy of crash reporting and support evidence-based policymaking.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.AP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19342v1",
    "published_date": "2025-06-24 06:13:52 UTC",
    "updated_date": "2025-06-24 06:13:52 UTC"
  },
  {
    "arxiv_id": "2507.02904v1",
    "title": "Enhancing Sports Strategy with Video Analytics and Data Mining: Assessing the effectiveness of Multimodal LLMs in tennis video analysis",
    "authors": [
      "Charlton Teo"
    ],
    "abstract": "The use of Large Language Models (LLMs) in recent years has also given rise to the development of Multimodal LLMs (MLLMs). These new MLLMs allow us to process images, videos and even audio alongside textual inputs. In this project, we aim to assess the effectiveness of MLLMs in analysing sports videos, focusing mainly on tennis videos. Despite research done on tennis analysis, there remains a gap in models that are able to understand and identify the sequence of events in a tennis rally, which would be useful in other fields of sports analytics. As such, we will mainly assess the MLLMs on their ability to fill this gap - to classify tennis actions, as well as their ability to identify these actions in a sequence of tennis actions in a rally. We further looked into ways we can improve the MLLMs' performance, including different training methods and even using them together with other traditional models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "B.Comp. dissertation",
    "pdf_url": "https://arxiv.org/pdf/2507.02904v1",
    "published_date": "2025-06-24 06:08:35 UTC",
    "updated_date": "2025-06-24 06:08:35 UTC"
  },
  {
    "arxiv_id": "2506.19887v1",
    "title": "MATER: Multi-level Acoustic and Textual Emotion Representation for Interpretable Speech Emotion Recognition",
    "authors": [
      "Hyo Jin Jon",
      "Longbin Jin",
      "Hyuntaek Jung",
      "Hyunseo Kim",
      "Donghun Min",
      "Eun Yi Kim"
    ],
    "abstract": "This paper presents our contributions to the Speech Emotion Recognition in Naturalistic Conditions (SERNC) Challenge, where we address categorical emotion recognition and emotional attribute prediction. To handle the complexities of natural speech, including intra- and inter-subject variability, we propose Multi-level Acoustic-Textual Emotion Representation (MATER), a novel hierarchical framework that integrates acoustic and textual features at the word, utterance, and embedding levels. By fusing low-level lexical and acoustic cues with high-level contextualized representations, MATER effectively captures both fine-grained prosodic variations and semantic nuances. Additionally, we introduce an uncertainty-aware ensemble strategy to mitigate annotator inconsistencies, improving robustness in ambiguous emotional expressions. MATER ranks fourth in both tasks with a Macro-F1 of 41.01% and an average CCC of 0.5928, securing second place in valence prediction with an impressive CCC of 0.6941.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "5 pages, 4 figures, 2 tables, 1 algorithm, Accepted to INTERSPEECH 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.19887v1",
    "published_date": "2025-06-24 05:35:53 UTC",
    "updated_date": "2025-06-24 05:35:53 UTC"
  },
  {
    "arxiv_id": "2506.19325v2",
    "title": "FEAT: A Preference Feedback Dataset through a Cost-Effective Auto-Generation and Labeling Framework for English AI Tutoring",
    "authors": [
      "Hyein Seo",
      "Taewook Hwang",
      "Yohan Lee",
      "sangkeun Jung"
    ],
    "abstract": "In English education tutoring, teacher feedback is essential for guiding students. Recently, AI-based tutoring systems have emerged to assist teachers; however, these systems require high-quality and large-scale teacher feedback data, which is both time-consuming and costly to generate manually. In this study, we propose FEAT, a cost-effective framework for generating teacher feedback, and have constructed three complementary datasets: (1) DIRECT-Manual (DM), where both humans and large language models (LLMs) collaboratively generate high-quality teacher feedback, albeit at a higher cost; (2) DIRECT-Generated (DG), an LLM-only generated, cost-effective dataset with lower quality;, and (3) DIRECT-Augmented (DA), primarily based on DG with a small portion of DM added to enhance quality while maintaining cost-efficiency. Experimental results showed that incorporating a small portion of DM (5-10%) into DG leads to superior performance compared to using 100% DM alone.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "ACL 2025 (Short)",
    "pdf_url": "https://arxiv.org/pdf/2506.19325v2",
    "published_date": "2025-06-24 05:32:06 UTC",
    "updated_date": "2025-06-27 00:38:08 UTC"
  },
  {
    "arxiv_id": "2506.19315v2",
    "title": "JCAPT: A Joint Modeling Approach for CAPT",
    "authors": [
      "Tzu-Hsuan Yang",
      "Yue-Yang He",
      "Berlin Chen"
    ],
    "abstract": "Effective pronunciation feedback is critical in second language (L2) learning, for which computer-assisted pronunciation training (CAPT) systems often encompass two key tasks: automatic pronunciation assessment (APA) and mispronunciation detection and diagnosis (MDD). Recent work has shown that joint modeling of these two tasks can yield mutual benefits. Our unified framework leverages Mamba, a selective state space model (SSM), while integrating phonological features and think token strategies to jointly enhance interpretability and fine-grained temporal reasoning in APA and MDD. To our knowledge, this is the first study to combine phonological attribution, SSM-based modeling, and prompting in CAPT. A series of experiments conducted on the speechocean762 benchmark demonstrate that our model consistently outperforms prior methods, particularly on the MDD task.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to the ISCA SLaTE-2025 Workshop",
    "pdf_url": "https://arxiv.org/pdf/2506.19315v2",
    "published_date": "2025-06-24 05:12:32 UTC",
    "updated_date": "2025-07-25 09:26:59 UTC"
  },
  {
    "arxiv_id": "2506.19312v1",
    "title": "Capturing Fine-Grained Alignments Improves 3D Affordance Detection",
    "authors": [
      "Junsei Tokumitsu",
      "Yuiga Wada"
    ],
    "abstract": "In this work, we address the challenge of affordance detection in 3D point clouds, a task that requires effectively capturing fine-grained alignments between point clouds and text. Existing methods often struggle to model such alignments, resulting in limited performance on standard benchmarks. A key limitation of these approaches is their reliance on simple cosine similarity between point cloud and text embeddings, which lacks the expressiveness needed for fine-grained reasoning. To address this limitation, we propose LM-AD, a novel method for affordance detection in 3D point clouds. Moreover, we introduce the Affordance Query Module (AQM), which efficiently captures fine-grained alignment between point clouds and text by leveraging a pretrained language model. We demonstrated that our method outperformed existing approaches in terms of accuracy and mean Intersection over Union on the 3D AffordanceNet dataset.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "MVA 2025 (Oral)",
    "pdf_url": "https://arxiv.org/pdf/2506.19312v1",
    "published_date": "2025-06-24 04:58:55 UTC",
    "updated_date": "2025-06-24 04:58:55 UTC"
  },
  {
    "arxiv_id": "2506.19885v2",
    "title": "FlightKooba: A Fast Interpretable FTP Model",
    "authors": [
      "Jing Lu",
      "Xuan Wu",
      "Yizhun Tian",
      "Songhan Fan",
      "Yali Fang"
    ],
    "abstract": "Flight trajectory prediction (FTP) and similar time series tasks typically require capturing smooth latent dynamics hidden within noisy signals. However, existing deep learning models face significant challenges of high computational cost and insufficient interpretability due to their complex black-box nature. This paper introduces FlightKooba, a novel modeling approach designed to extract such underlying dynamics analytically. Our framework uniquely integrates HiPPO theory, Koopman operator theory, and control theory. By leveraging Legendre polynomial bases, it constructs Koopman operators analytically, thereby avoiding large-scale parameter training. The method's core strengths lie in its exceptional computational efficiency and inherent interpretability. Experiments on multiple public datasets validate our design philosophy: for signals exhibiting strong periodicity or clear physical laws (e.g., in aviation, meteorology, and traffic flow), FlightKooba delivers competitive prediction accuracy while reducing trainable parameters by several orders of magnitude and achieving the fastest training speed. Furthermore, we analyze the model's theoretical boundaries, clarifying its inherent low-pass filtering characteristics that render it unsuitable for sequences dominated by high-frequency noise. In summary, FlightKooba offers a powerful, efficient, and interpretable new alternative for time series analysis, particularly in resource-constrained environments.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "Version 2: Major revision of the manuscript to refine the narrative, clarify the model's theoretical limitations and application scope, and improve overall presentation for journal submission",
    "pdf_url": "https://arxiv.org/pdf/2506.19885v2",
    "published_date": "2025-06-24 04:53:49 UTC",
    "updated_date": "2025-10-27 10:39:15 UTC"
  },
  {
    "arxiv_id": "2506.19884v1",
    "title": "MNN-AECS: Energy Optimization for LLM Decoding on Mobile Devices via Adaptive Core Selection",
    "authors": [
      "Zhengxiang Huang",
      "Chaoyue Niu",
      "Zhaode Wang",
      "Jiarui Xue",
      "Hanming Zhang",
      "Yugang Wang",
      "Zewei Xin",
      "Xiaotang Jiang",
      "Chengfei Lv",
      "Fan Wu",
      "Guihai Chen"
    ],
    "abstract": "As the demand for on-device Large Language Model (LLM) inference grows, energy efficiency has become a major concern, especially for battery-limited mobile devices. Our analysis shows that the memory-bound LLM decode phase dominates energy use, and yet most existing works focus on accelerating the prefill phase, neglecting energy concerns. We introduce Adaptive Energy-Centric Core Selection (AECS) and integrate it into MNN to create the energy-efficient version, MNN-AECS, the first engine-level system solution without requiring root access or OS modifications for energy-efficient LLM decoding. MNN-AECS is designed to reduce LLM decoding energy while keeping decode speed within an acceptable slowdown threshold by dynamically selecting low-power CPU cores. MNN-AECS is evaluated across 5 Android and 2 iOS devices on 5 popular LLMs of various sizes. Compared to original MNN, MNN-AECS cuts down energy use by 23% without slowdown averaged over all 7 devices and 4 datasets. Against other engines, including llama.cpp, executorch, mllm, and MediaPipe, MNN-AECS delivers 39% to 78% energy saving and 12% to 363% speedup on average.",
    "categories": [
      "cs.OS",
      "cs.AI",
      "cs.PF",
      "cs.SE"
    ],
    "primary_category": "cs.OS",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19884v1",
    "published_date": "2025-06-24 04:50:28 UTC",
    "updated_date": "2025-06-24 04:50:28 UTC"
  },
  {
    "arxiv_id": "2506.19290v1",
    "title": "Skywork-SWE: Unveiling Data Scaling Laws for Software Engineering in LLMs",
    "authors": [
      "Liang Zeng",
      "Yongcong Li",
      "Yuzhen Xiao",
      "Changshi Li",
      "Chris Yuhao Liu",
      "Rui Yan",
      "Tianwen Wei",
      "Jujie He",
      "Xuchen Song",
      "Yang Liu",
      "Yahui Zhou"
    ],
    "abstract": "Software engineering (SWE) has recently emerged as a crucial testbed for next-generation LLM agents, demanding inherent capabilities in two critical dimensions: sustained iterative problem-solving (e.g., >50 interaction rounds) and long-context dependency resolution (e.g., >32k tokens). However, the data curation process in SWE remains notoriously time-consuming, as it heavily relies on manual annotation for code file filtering and the setup of dedicated runtime environments to execute and validate unit tests. Consequently, most existing datasets are limited to only a few thousand GitHub-sourced instances. To this end, we propose an incremental, automated data-curation pipeline that systematically scales both the volume and diversity of SWE datasets. Our dataset comprises 10,169 real-world Python task instances from 2,531 distinct GitHub repositories, each accompanied by a task specified in natural language and a dedicated runtime-environment image for automated unit-test validation. We have carefully curated over 8,000 successfully runtime-validated training trajectories from our proposed SWE dataset. When fine-tuning the Skywork-SWE model on these trajectories, we uncover a striking data scaling phenomenon: the trained model's performance for software engineering capabilities in LLMs continues to improve as the data size increases, showing no signs of saturation. Notably, our Skywork-SWE model achieves 38.0% pass@1 accuracy on the SWE-bench Verified benchmark without using verifiers or multiple rollouts, establishing a new state-of-the-art (SOTA) among the Qwen2.5-Coder-32B-based LLMs built on the OpenHands agent framework. Furthermore, with the incorporation of test-time scaling techniques, the performance further improves to 47.0% accuracy, surpassing the previous SOTA results for sub-32B parameter models. We release the Skywork-SWE-32B model checkpoint to accelerate future research.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19290v1",
    "published_date": "2025-06-24 03:53:36 UTC",
    "updated_date": "2025-06-24 03:53:36 UTC"
  },
  {
    "arxiv_id": "2506.19283v3",
    "title": "AirV2X: Unified Air-Ground Vehicle-to-Everything Collaboration",
    "authors": [
      "Xiangbo Gao",
      "Yuheng Wu",
      "Fengze Yang",
      "Xuewen Luo",
      "Keshu Wu",
      "Xinghao Chen",
      "Yuping Wang",
      "Chenxi Liu",
      "Yang Zhou",
      "Zhengzhong Tu"
    ],
    "abstract": "While multi-vehicular collaborative driving demonstrates clear advantages over single-vehicle autonomy, traditional infrastructure-based V2X systems remain constrained by substantial deployment costs and the creation of \"uncovered danger zones\" in rural and suburban areas. We present AirV2X-Perception, a large-scale dataset that leverages Unmanned Aerial Vehicles (UAVs) as a flexible alternative or complement to fixed Road-Side Units (RSUs). Drones offer unique advantages over ground-based perception: complementary bird's-eye-views that reduce occlusions, dynamic positioning capabilities that enable hovering, patrolling, and escorting navigation rules, and significantly lower deployment costs compared to fixed infrastructure. Our dataset comprises 6.73 hours of drone-assisted driving scenarios across urban, suburban, and rural environments with varied weather and lighting conditions. The AirV2X-Perception dataset facilitates the development and standardized evaluation of Vehicle-to-Drone (V2D) algorithms, addressing a critical gap in the rapidly expanding field of aerial-assisted autonomous driving systems. The dataset and development kits are open-sourced at https://github.com/taco-group/AirV2X-Perception.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19283v3",
    "published_date": "2025-06-24 03:34:39 UTC",
    "updated_date": "2025-07-02 23:07:24 UTC"
  },
  {
    "arxiv_id": "2506.19883v1",
    "title": "STIMULUS: Achieving Fast Convergence and Low Sample Complexity in Stochastic Multi-Objective Learning",
    "authors": [
      "Zhuqing Liu",
      "Chaosheng Dong",
      "Michinari Momma",
      "Simone Shao",
      "Shaoyuan Xu",
      "Yan Gao",
      "Haibo Yang",
      "Jia Liu"
    ],
    "abstract": "Recently, multi-objective optimization (MOO) has gained attention for its broad applications in ML, operations research, and engineering. However, MOO algorithm design remains in its infancy and many existing MOO methods suffer from unsatisfactory convergence rate and sample complexity performance. To address this challenge, in this paper, we propose an algorithm called STIMULUS( stochastic path-integrated multi-gradient recursive e\\ulstimator), a new and robust approach for solving MOO problems. Different from the traditional methods, STIMULUS introduces a simple yet powerful recursive framework for updating stochastic gradient estimates to improve convergence performance with low sample complexity. In addition, we introduce an enhanced version of STIMULUS, termed STIMULUS-M, which incorporates a momentum term to further expedite convergence. We establish $O(1/T)$ convergence rates of the proposed methods for non-convex settings and $O (\\exp{-μT})$ for strongly convex settings, where $T$ is the total number of iteration rounds. Additionally, we achieve the state-of-the-art $O \\left(n+\\sqrt{n}ε^{-1}\\right)$ sample complexities for non-convex settings and $O\\left(n+ \\sqrt{n} \\ln ({μ/ε})\\right)$ for strongly convex settings, where $ε>0$ is a desired stationarity error. Moreover, to alleviate the periodic full gradient evaluation requirement in STIMULUS and STIMULUS-M, we further propose enhanced versions with adaptive batching called STIMULUS+/ STIMULUS-M+ and provide their theoretical analysis.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19883v1",
    "published_date": "2025-06-24 03:31:25 UTC",
    "updated_date": "2025-06-24 03:31:25 UTC"
  },
  {
    "arxiv_id": "2506.19280v1",
    "title": "Emotion Detection on User Front-Facing App Interfaces for Enhanced Schedule Optimization: A Machine Learning Approach",
    "authors": [
      "Feiting Yang",
      "Antoine Moevus",
      "Steve Lévesque"
    ],
    "abstract": "Human-Computer Interaction (HCI) has evolved significantly to incorporate emotion recognition capabilities, creating unprecedented opportunities for adaptive and personalized user experiences. This paper explores the integration of emotion detection into calendar applications, enabling user interfaces to dynamically respond to users' emotional states and stress levels, thereby enhancing both productivity and engagement. We present and evaluate two complementary approaches to emotion detection: a biometric-based method utilizing heart rate (HR) data extracted from electrocardiogram (ECG) signals processed through Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) neural networks to predict the emotional dimensions of Valence, Arousal, and Dominance; and a behavioral method analyzing computer activity through multiple machine learning models to classify emotions based on fine-grained user interactions such as mouse movements, clicks, and keystroke patterns. Our comparative analysis, from real-world datasets, reveals that while both approaches demonstrate effectiveness, the computer activity-based method delivers superior consistency and accuracy, particularly for mouse-related interactions, which achieved approximately 90\\% accuracy. Furthermore, GRU networks outperformed LSTM models in the biometric approach, with Valence prediction reaching 84.38\\% accuracy.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19280v1",
    "published_date": "2025-06-24 03:21:46 UTC",
    "updated_date": "2025-06-24 03:21:46 UTC"
  },
  {
    "arxiv_id": "2506.19279v1",
    "title": "EmoStage: A Framework for Accurate Empathetic Response Generation via Perspective-Taking and Phase Recognition",
    "authors": [
      "Zhiyang Qi",
      "Keiko Takamizo",
      "Mariko Ukiyo",
      "Michimasa Inaba"
    ],
    "abstract": "The rising demand for mental health care has fueled interest in AI-driven counseling systems. While large language models (LLMs) offer significant potential, current approaches face challenges, including limited understanding of clients' psychological states and counseling stages, reliance on high-quality training data, and privacy concerns associated with commercial deployment. To address these issues, we propose EmoStage, a framework that enhances empathetic response generation by leveraging the inference capabilities of open-source LLMs without additional training data. Our framework introduces perspective-taking to infer clients' psychological states and support needs, enabling the generation of emotionally resonant responses. In addition, phase recognition is incorporated to ensure alignment with the counseling process and to prevent contextually inappropriate or inopportune responses. Experiments conducted in both Japanese and Chinese counseling settings demonstrate that EmoStage improves the quality of responses generated by base models and performs competitively with data-driven methods.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19279v1",
    "published_date": "2025-06-24 03:18:37 UTC",
    "updated_date": "2025-06-24 03:18:37 UTC"
  },
  {
    "arxiv_id": "2506.19269v2",
    "title": "AnchorDP3: 3D Affordance Guided Sparse Diffusion Policy for Robotic Manipulation",
    "authors": [
      "Ziyan Zhao",
      "Ke Fan",
      "He-Yang Xu",
      "Ning Qiao",
      "Bo Peng",
      "Wenlong Gao",
      "Dongjiang Li",
      "Hui Shen"
    ],
    "abstract": "We present AnchorDP3, a diffusion policy framework for dual-arm robotic manipulation that achieves state-of-the-art performance in highly randomized environments. AnchorDP3 integrates three key innovations: (1) Simulator-Supervised Semantic Segmentation, using rendered ground truth to explicitly segment task-critical objects within the point cloud, which provides strong affordance priors; (2) Task-Conditioned Feature Encoders, lightweight modules processing augmented point clouds per task, enabling efficient multi-task learning through a shared diffusion-based action expert; (3) Affordance-Anchored Keypose Diffusion with Full State Supervision, replacing dense trajectory prediction with sparse, geometrically meaningful action anchors, i.e., keyposes such as pre-grasp pose, grasp pose directly anchored to affordances, drastically simplifying the prediction space; the action expert is forced to predict both robot joint angles and end-effector poses simultaneously, which exploits geometric consistency to accelerate convergence and boost accuracy. Trained on large-scale, procedurally generated simulation data, AnchorDP3 achieves a 98.7% average success rate in the RoboTwin benchmark across diverse tasks under extreme randomization of objects, clutter, table height, lighting, and backgrounds. This framework, when integrated with the RoboTwin real-to-sim pipeline, has the potential to enable fully autonomous generation of deployable visuomotor policies from only scene and instruction, totally eliminating human demonstrations from learning manipulation skills.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19269v2",
    "published_date": "2025-06-24 03:03:26 UTC",
    "updated_date": "2025-06-25 05:10:04 UTC"
  },
  {
    "arxiv_id": "2506.19256v4",
    "title": "Temporal Regularization Training: Unleashing the Potential of Spiking Neural Networks",
    "authors": [
      "Boxuan Zhang",
      "Zhen Xu",
      "Kuan Tao"
    ],
    "abstract": "Spiking Neural Networks (SNNs) have received widespread attention due to their event-driven and low-power characteristics, making them particularly effective for processing neuromorphic data. Recent studies have shown that directly trained SNNs suffer from severe temporal gradient vanishing and overfitting issues, which fundamentally constrain their performance and generalizability. This paper unveils a temporal regularization training (TRT) memthod, designed to unleash the generalization and performance potential of SNNs through a time-decaying regularization mechanism that prioritizes early timesteps with stronger constraints. We perform theoretical analysis to reveal TRT's ability on mitigating the temporal gradient vanishment. To validate the effectiveness of TRT, we conduct experiments on both static image datasets and dynamic neuromorphic datasets, perform analysis of their results, demonstrating that TRT can effectively mitigate overfitting and help SNNs converge into flatter local minima with better generalizability. Furthermore, we establish a theoretical interpretation of TRT's temporal regularization mechanism by analyzing the temporal information dynamics inside SNNs. We track the Fisher information of SNNs during training process, showing that Fisher information progressively concentrates in early timesteps. The time-decaying regularization mechanism implemented in TRT effectively guides the network to learn robust features in early timesteps with rich information, thereby leading to significant improvements in model generalization.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "Submitted to Elsevier. Code is available at https://github.com/ZBX05/Temporal-Regularization-Training",
    "pdf_url": "https://arxiv.org/pdf/2506.19256v4",
    "published_date": "2025-06-24 02:36:55 UTC",
    "updated_date": "2026-01-10 04:40:09 UTC"
  },
  {
    "arxiv_id": "2506.22487v1",
    "title": "AGI Enabled Solutions For IoX Layers Bottlenecks In Cyber-Physical-Social-Thinking Space",
    "authors": [
      "Amar Khelloufi",
      "Huansheng Ning",
      "Sahraoui Dhelim",
      "Jianguo Ding"
    ],
    "abstract": "The integration of the Internet of Everything (IoX) and Artificial General Intelligence (AGI) has given rise to a transformative paradigm aimed at addressing critical bottlenecks across sensing, network, and application layers in Cyber-Physical-Social Thinking (CPST) ecosystems. In this survey, we provide a systematic and comprehensive review of AGI-enhanced IoX research, focusing on three key components: sensing-layer data management, network-layer protocol optimization, and application-layer decision-making frameworks. Specifically, this survey explores how AGI can mitigate IoX bottlenecks challenges by leveraging adaptive sensor fusion, edge preprocessing, and selective attention mechanisms at the sensing layer, while resolving network-layer issues such as protocol heterogeneity and dynamic spectrum management, neuro-symbolic reasoning, active inference, and causal reasoning, Furthermore, the survey examines AGI-enabled frameworks for managing identity and relationship explosion. Key findings suggest that AGI-driven strategies, such as adaptive sensor fusion, edge preprocessing, and semantic modeling, offer novel solutions to sensing-layer data overload, network-layer protocol heterogeneity, and application-layer identity explosion. The survey underscores the importance of cross-layer integration, quantum-enabled communication, and ethical governance frameworks for future AGI-enabled IoX systems. Finally, the survey identifies unresolved challenges, such as computational requirements, scalability, and real-world validation, calling for further research to fully realize AGI's potential in addressing IoX bottlenecks. we believe AGI-enhanced IoX is emerging as a critical research field at the intersection of interconnected systems and advanced AI.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "31 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.22487v1",
    "published_date": "2025-06-24 02:33:43 UTC",
    "updated_date": "2025-06-24 02:33:43 UTC"
  },
  {
    "arxiv_id": "2506.19882v3",
    "title": "Position: Machine Learning Conferences Should Establish a \"Refutations and Critiques\" Track",
    "authors": [
      "Rylan Schaeffer",
      "Joshua Kazdan",
      "Yegor Denisov-Blanch",
      "Brando Miranda",
      "Matthias Gerstgrasser",
      "Susan Zhang",
      "Andreas Haupt",
      "Isha Gupta",
      "Elyas Obbad",
      "Jesse Dodge",
      "Jessica Zosa Forde",
      "Francesco Orabona",
      "Sanmi Koyejo",
      "David Donoho"
    ],
    "abstract": "Science progresses by iteratively advancing and correcting humanity's understanding of the world. In machine learning (ML) research, rapid advancements have led to an explosion of publications, but have also led to misleading, incorrect, flawed or perhaps even fraudulent studies being accepted and sometimes highlighted at ML conferences due to the fallibility of peer review. While such mistakes are understandable, ML conferences do not offer robust processes to help the field systematically correct when such errors are made. This position paper argues that ML conferences should establish a dedicated \"Refutations and Critiques\" (R&C) Track. This R&C Track would provide a high-profile, reputable platform to support vital research that critically challenges prior research, thereby fostering a dynamic self-correcting research ecosystem. We discuss key considerations including track design, review principles, potential pitfalls, and provide an illustrative example submission concerning a recent ICLR 2025 Oral. We conclude that ML conferences should create official, reputable mechanisms to help ML research self-correct.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19882v3",
    "published_date": "2025-06-24 02:19:30 UTC",
    "updated_date": "2025-07-07 02:00:46 UTC"
  },
  {
    "arxiv_id": "2506.22486v1",
    "title": "Hallucination Detection with Small Language Models",
    "authors": [
      "Ming Cheung"
    ],
    "abstract": "Since the introduction of ChatGPT, large language models (LLMs) have demonstrated significant utility in various tasks, such as answering questions through retrieval-augmented generation. Context can be retrieved using a vectorized database, serving as a foundation for LLMs to generate responses. However, hallucinations in responses can undermine the reliability of LLMs in practical applications, and they are not easily detectable in the absence of ground truth, particularly in question-and-answer scenarios. This paper proposes a framework that integrates multiple small language models to verify responses generated by LLMs using the retrieved context from a vectorized database. By breaking down the responses into individual sentences and utilizing the probability of generating \"Yes\" tokens from the outputs of multiple models for a given set of questions, responses, and relevant context, hallucinations can be detected. The proposed framework is validated through experiments with real datasets comprising over 100 sets of questions, answers, and contexts, including responses with fully and partially correct sentences. The results demonstrate a 10\\% improvement in F1 scores for detecting correct responses compared to hallucinations, indicating that multiple small language models can be effectively employed for answer verification, providing a scalable and efficient solution for both academic and practical applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.22486v1",
    "published_date": "2025-06-24 02:19:26 UTC",
    "updated_date": "2025-06-24 02:19:26 UTC"
  },
  {
    "arxiv_id": "2506.19250v2",
    "title": "Robust Behavior Cloning Via Global Lipschitz Regularization",
    "authors": [
      "Shili Wu",
      "Yizhao Jin",
      "Puhua Niu",
      "Aniruddha Datta",
      "Sean B. Andersson"
    ],
    "abstract": "Behavior Cloning (BC) is an effective imitation learning technique and has even been adopted in some safety-critical domains such as autonomous vehicles. BC trains a policy to mimic the behavior of an expert by using a dataset composed of only state-action pairs demonstrated by the expert, without any additional interaction with the environment. However, During deployment, the policy observations may contain measurement errors or adversarial disturbances. Since the observations may deviate from the true states, they can mislead the agent into making sub-optimal actions. In this work, we use a global Lipschitz regularization approach to enhance the robustness of the learned policy network. We then show that the resulting global Lipschitz property provides a robustness certificate to the policy with respect to different bounded norm perturbations. Then, we propose a way to construct a Lipschitz neural network that ensures the policy robustness. We empirically validate our theory across various environments in Gymnasium. Keywords: Robust Reinforcement Learning; Behavior Cloning; Lipschitz Neural Network",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19250v2",
    "published_date": "2025-06-24 02:19:08 UTC",
    "updated_date": "2025-08-09 15:01:55 UTC"
  },
  {
    "arxiv_id": "2506.19235v1",
    "title": "RecLLM-R1: A Two-Stage Training Paradigm with Reinforcement Learning and Chain-of-Thought v1",
    "authors": [
      "Yu Xie",
      "Xingkai Ren",
      "Ying Qi",
      "Yao Hu",
      "Lianlei Shan"
    ],
    "abstract": "Traditional recommendation systems often grapple with \"filter bubbles\", underutilization of external knowledge, and a disconnect between model optimization and business policy iteration. To address these limitations, this paper introduces RecLLM-R1, a novel recommendation framework leveraging Large Language Models (LLMs) and drawing inspiration from the DeepSeek R1 methodology. The framework initiates by transforming user profiles, historical interactions, and multi-faceted item attributes into LLM-interpretable natural language prompts through a carefully engineered data construction process. Subsequently, a two-stage training paradigm is employed: the initial stage involves Supervised Fine-Tuning (SFT) to imbue the LLM with fundamental recommendation capabilities. The subsequent stage utilizes Group Relative Policy Optimization (GRPO), a reinforcement learning technique, augmented with a Chain-of-Thought (CoT) mechanism. This stage guides the model through multi-step reasoning and holistic decision-making via a flexibly defined reward function, aiming to concurrently optimize recommendation accuracy, diversity, and other bespoke business objectives. Empirical evaluations on a real-world user behavior dataset from a large-scale social media platform demonstrate that RecLLM-R1 significantly surpasses existing baseline methods across a spectrum of evaluation metrics, including accuracy, diversity, and novelty. It effectively mitigates the filter bubble effect and presents a promising avenue for the integrated optimization of recommendation models and policies under intricate business goals.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19235v1",
    "published_date": "2025-06-24 01:39:34 UTC",
    "updated_date": "2025-06-24 01:39:34 UTC"
  },
  {
    "arxiv_id": "2506.19225v1",
    "title": "Video-XL-2: Towards Very Long-Video Understanding Through Task-Aware KV Sparsification",
    "authors": [
      "Minghao Qin",
      "Xiangrui Liu",
      "Zhengyang Liang",
      "Yan Shu",
      "Huaying Yuan",
      "Juenjie Zhou",
      "Shitao Xiao",
      "Bo Zhao",
      "Zheng Liu"
    ],
    "abstract": "Multi-modal large language models (MLLMs) models have made significant progress in video understanding over the past few years. However, processing long video inputs remains a major challenge due to high memory and computational costs. This makes it difficult for current models to achieve both strong performance and high efficiency in long video understanding. To address this challenge, we propose Video-XL-2, a novel MLLM that delivers superior cost-effectiveness for long-video understanding based on task-aware KV sparsification. The proposed framework operates with two key steps: chunk-based pre-filling and bi-level key-value decoding. Chunk-based pre-filling divides the visual token sequence into chunks, applying full attention within each chunk and sparse attention across chunks. This significantly reduces computational and memory overhead. During decoding, bi-level key-value decoding selectively reloads either dense or sparse key-values for each chunk based on its relevance to the task. This approach further improves memory efficiency and enhances the model's ability to capture fine-grained information. Video-XL-2 achieves state-of-the-art performance on various long video understanding benchmarks, outperforming existing open-source lightweight models. It also demonstrates exceptional efficiency, capable of processing over 10,000 frames on a single NVIDIA A100 (80GB) GPU and thousands of frames in just a few seconds.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 5 Figure, 3 Table",
    "pdf_url": "https://arxiv.org/pdf/2506.19225v1",
    "published_date": "2025-06-24 01:19:56 UTC",
    "updated_date": "2025-06-24 01:19:56 UTC"
  },
  {
    "arxiv_id": "2506.19224v1",
    "title": "GBGC: Efficient and Adaptive Graph Coarsening via Granular-ball Computing",
    "authors": [
      "Shuyin Xia",
      "Guan Wang",
      "Gaojie Xu",
      "Sen Zhao",
      "Guoyin Wang"
    ],
    "abstract": "The objective of graph coarsening is to generate smaller, more manageable graphs while preserving key information of the original graph. Previous work were mainly based on the perspective of spectrum-preserving, using some predefined coarsening rules to make the eigenvalues of the Laplacian matrix of the original graph and the coarsened graph match as much as possible. However, they largely overlooked the fact that the original graph is composed of subregions at different levels of granularity, where highly connected and similar nodes should be more inclined to be aggregated together as nodes in the coarsened graph. By combining the multi-granularity characteristics of the graph structure, we can generate coarsened graph at the optimal granularity. To this end, inspired by the application of granular-ball computing in multi-granularity, we propose a new multi-granularity, efficient, and adaptive coarsening method via granular-ball (GBGC), which significantly improves the coarsening results and efficiency. Specifically, GBGC introduces an adaptive granular-ball graph refinement mechanism, which adaptively splits the original graph from coarse to fine into granular-balls of different sizes and optimal granularity, and constructs the coarsened graph using these granular-balls as supernodes. In addition, compared with other state-of-the-art graph coarsening methods, the processing speed of this method can be increased by tens to hundreds of times and has lower time complexity. The accuracy of GBGC is almost always higher than that of the original graph due to the good robustness and generalization of the granular-ball computing, so it has the potential to become a standard graph data preprocessing method.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.19224v1",
    "published_date": "2025-06-24 01:18:06 UTC",
    "updated_date": "2025-06-24 01:18:06 UTC"
  },
  {
    "arxiv_id": "2506.19220v1",
    "title": "Private Model Personalization Revisited",
    "authors": [
      "Conor Snedeker",
      "Xinyu Zhou",
      "Raef Bassily"
    ],
    "abstract": "We study model personalization under user-level differential privacy (DP) in the shared representation framework. In this problem, there are $n$ users whose data is statistically heterogeneous, and their optimal parameters share an unknown embedding $U^* \\in\\mathbb{R}^{d\\times k}$ that maps the user parameters in $\\mathbb{R}^d$ to low-dimensional representations in $\\mathbb{R}^k$, where $k\\ll d$. Our goal is to privately recover the shared embedding and the local low-dimensional representations with small excess risk in the federated setting. We propose a private, efficient federated learning algorithm to learn the shared embedding based on the FedRep algorithm in [CHM+21]. Unlike [CHM+21], our algorithm satisfies differential privacy, and our results hold for the case of noisy labels. In contrast to prior work on private model personalization [JRS+21], our utility guarantees hold under a larger class of users' distributions (sub-Gaussian instead of Gaussian distributions). Additionally, in natural parameter regimes, we improve the privacy error term in [JRS+21] by a factor of $\\widetilde{O}(dk)$. Next, we consider the binary classification setting. We present an information-theoretic construction to privately learn the shared embedding and derive a margin-based accuracy guarantee that is independent of $d$. Our method utilizes the Johnson-Lindenstrauss transform to reduce the effective dimensions of the shared embedding and the users' data. This result shows that dimension-independent risk bounds are possible in this setting under a margin loss.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.19220v1",
    "published_date": "2025-06-24 00:57:17 UTC",
    "updated_date": "2025-06-24 00:57:17 UTC"
  },
  {
    "arxiv_id": "2506.19217v1",
    "title": "MedErr-CT: A Visual Question Answering Benchmark for Identifying and Correcting Errors in CT Reports",
    "authors": [
      "Sunggu Kyung",
      "Hyungbin Park",
      "Jinyoung Seo",
      "Jimin Sung",
      "Jihyun Kim",
      "Dongyeong Kim",
      "Wooyoung Jo",
      "Yoojin Nam",
      "Sangah Park",
      "Taehee Kwon",
      "Sang Min Lee",
      "Namkug Kim"
    ],
    "abstract": "Computed Tomography (CT) plays a crucial role in clinical diagnosis, but the growing demand for CT examinations has raised concerns about diagnostic errors. While Multimodal Large Language Models (MLLMs) demonstrate promising comprehension of medical knowledge, their tendency to produce inaccurate information highlights the need for rigorous validation. However, existing medical visual question answering (VQA) benchmarks primarily focus on simple visual recognition tasks, lacking clinical relevance and failing to assess expert-level knowledge. We introduce MedErr-CT, a novel benchmark for evaluating medical MLLMs' ability to identify and correct errors in CT reports through a VQA framework. The benchmark includes six error categories - four vision-centric errors (Omission, Insertion, Direction, Size) and two lexical error types (Unit, Typo) - and is organized into three task levels: classification, detection, and correction. Using this benchmark, we quantitatively assess the performance of state-of-the-art 3D medical MLLMs, revealing substantial variation in their capabilities across different error types. Our benchmark contributes to the development of more reliable and clinically applicable MLLMs, ultimately helping reduce diagnostic errors and improve accuracy in clinical practice. The code and datasets are available at https://github.com/babbu3682/MedErr-CT.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "14 pages, 5 figures, submitted to CVPR 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.19217v1",
    "published_date": "2025-06-24 00:51:03 UTC",
    "updated_date": "2025-06-24 00:51:03 UTC"
  }
]
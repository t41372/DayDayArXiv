[
  {
    "arxiv_id": "2408.09307v1",
    "title": "A Benchmark Time Series Dataset for Semiconductor Fabrication Manufacturing Constructed using Component-based Discrete-Event Simulation Models",
    "authors": [
      "Vamsi Krishna Pendyala",
      "Hessam S. Sarjoughian",
      "Bala Potineni",
      "Edward J. Yellig"
    ],
    "abstract": "Advancements in high-computing devices increase the necessity for improved\nand new understanding and development of smart manufacturing factories.\nDiscrete-event models with simulators have been shown to be critical to\narchitect, designing, building, and operating the manufacturing of\nsemiconductor chips. The diffusion, implantation, and lithography machines have\nintricate processes due to their feedforward and feedback connectivity. The\ndataset collected from simulations of the factory models holds the promise of\ngenerating valuable machine-learning models. As surrogate data-based models,\ntheir executions are highly efficient compared to the physics-based counterpart\nmodels. For the development of surrogate models, it is beneficial to have\npublicly available benchmark simulation models that are grounded in factory\nmodels that have concise structures and accurate behaviors. Hence, in this\nresearch, a dataset is devised and constructed based on a benchmark model of an\nIntel semiconductor fabrication factory. The model is formalized using the\nParallel Discrete-Event System Specification and executed using the DEVS-Suite\nsimulator. The time series dataset is constructed using discrete-event time\ntrajectories. This dataset is further analyzed and used to develop baseline\nunivariate and multivariate machine learning models. The dataset can also be\nutilized in the machine learning community for behavioral analysis based on\nformalized and scalable component-based discrete-event models and simulations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09307v1",
    "published_date": "2024-08-17 23:05:47 UTC",
    "updated_date": "2024-08-17 23:05:47 UTC"
  },
  {
    "arxiv_id": "2408.09285v2",
    "title": "Evaluating Usability and Engagement of Large Language Models in Virtual Reality for Traditional Scottish Curling",
    "authors": [
      "Ka Hei Carrie Lau",
      "Efe Bozkir",
      "Hong Gao",
      "Enkelejda Kasneci"
    ],
    "abstract": "This paper explores the innovative application of Large Language Models\n(LLMs) in Virtual Reality (VR) environments to promote heritage education,\nfocusing on traditional Scottish curling presented in the game ``Scottish\nBonspiel VR''. Our study compares the effectiveness of LLM-based chatbots with\npre-defined scripted chatbots, evaluating key criteria such as usability, user\nengagement, and learning outcomes. The results show that LLM-based chatbots\nsignificantly improve interactivity and engagement, creating a more dynamic and\nimmersive learning environment. This integration helps document and preserve\ncultural heritage and enhances dissemination processes, which are crucial for\nsafeguarding intangible cultural heritage (ICH) amid environmental changes.\nFurthermore, the study highlights the potential of novel technologies in\neducation to provide immersive experiences that foster a deeper appreciation of\ncultural heritage. These findings support the wider application of LLMs and VR\nin cultural education to address global challenges and promote sustainable\npractices to preserve and enhance cultural heritage.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09285v2",
    "published_date": "2024-08-17 20:13:34 UTC",
    "updated_date": "2024-09-25 13:06:53 UTC"
  },
  {
    "arxiv_id": "2408.11865v1",
    "title": "How Susceptible are LLMs to Influence in Prompts?",
    "authors": [
      "Sotiris Anagnostidis",
      "Jannis Bulian"
    ],
    "abstract": "Large Language Models (LLMs) are highly sensitive to prompts, including\nadditional context provided therein. As LLMs grow in capability, understanding\ntheir prompt-sensitivity becomes increasingly crucial for ensuring reliable and\nrobust performance, particularly since evaluating these models becomes more\nchallenging. In this work, we investigate how current models (Llama, Mixtral,\nFalcon) respond when presented with additional input from another model,\nmimicking a scenario where a more capable model -- or a system with access to\nmore external information -- provides supplementary information to the target\nmodel. Across a diverse spectrum of question-answering tasks, we study how an\nLLM's response to multiple-choice questions changes when the prompt includes a\nprediction and explanation from another model. Specifically, we explore the\ninfluence of the presence of an explanation, the stated authoritativeness of\nthe source, and the stated confidence of the supplementary input. Our findings\nreveal that models are strongly influenced, and when explanations are provided\nthey are swayed irrespective of the quality of the explanation. The models are\nmore likely to be swayed if the input is presented as being authoritative or\nconfident, but the effect is small in size. This study underscores the\nsignificant prompt-sensitivity of LLMs and highlights the potential risks of\nincorporating outputs from external sources without thorough scrutiny and\nfurther validation. As LLMs continue to advance, understanding and mitigating\nsuch sensitivities will be crucial for their reliable and trustworthy\ndeployment.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11865v1",
    "published_date": "2024-08-17 17:40:52 UTC",
    "updated_date": "2024-08-17 17:40:52 UTC"
  },
  {
    "arxiv_id": "2408.09262v1",
    "title": "PREMAP: A Unifying PREiMage APproximation Framework for Neural Networks",
    "authors": [
      "Xiyue Zhang",
      "Benjie Wang",
      "Marta Kwiatkowska",
      "Huan Zhang"
    ],
    "abstract": "Most methods for neural network verification focus on bounding the image,\ni.e., set of outputs for a given input set. This can be used to, for example,\ncheck the robustness of neural network predictions to bounded perturbations of\nan input. However, verifying properties concerning the preimage, i.e., the set\nof inputs satisfying an output property, requires abstractions in the input\nspace. We present a general framework for preimage abstraction that produces\nunder- and over-approximations of any polyhedral output set. Our framework\nemploys cheap parameterised linear relaxations of the neural network, together\nwith an anytime refinement procedure that iteratively partitions the input\nregion by splitting on input features and neurons. The effectiveness of our\napproach relies on carefully designed heuristics and optimization objectives to\nachieve rapid improvements in the approximation volume. We evaluate our method\non a range of tasks, demonstrating significant improvement in efficiency and\nscalability to high-input-dimensional image classification tasks compared to\nstate-of-the-art techniques. Further, we showcase the application to\nquantitative verification and robustness analysis, presenting a sound and\ncomplete algorithm for the former and providing sound quantitative results for\nthe latter.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.LG",
    "comment": "arXiv admin note: text overlap with arXiv:2305.03686",
    "pdf_url": "http://arxiv.org/pdf/2408.09262v1",
    "published_date": "2024-08-17 17:24:47 UTC",
    "updated_date": "2024-08-17 17:24:47 UTC"
  },
  {
    "arxiv_id": "2408.09251v2",
    "title": "V2X-VLM: End-to-End V2X Cooperative Autonomous Driving Through Large Vision-Language Models",
    "authors": [
      "Junwei You",
      "Haotian Shi",
      "Zhuoyu Jiang",
      "Zilin Huang",
      "Rui Gan",
      "Keshu Wu",
      "Xi Cheng",
      "Xiaopeng Li",
      "Bin Ran"
    ],
    "abstract": "Advancements in autonomous driving have increasingly focused on end-to-end\n(E2E) systems that manage the full spectrum of driving tasks, from\nenvironmental perception to vehicle navigation and control. This paper\nintroduces V2X-VLM, an innovative E2E vehicle-infrastructure cooperative\nautonomous driving (VICAD) framework with Vehicle-to-Everything (V2X) systems\nand large vision-language models (VLMs). V2X-VLM is designed to enhance\nsituational awareness, decision-making, and ultimate trajectory planning by\nintegrating multimodel data from vehicle-mounted cameras, infrastructure\nsensors, and textual information. The contrastive learning method is further\nemployed to complement VLM by refining feature discrimination, assisting the\nmodel to learn robust representations of the driving environment. Evaluations\non the DAIR-V2X dataset show that V2X-VLM outperforms state-of-the-art\ncooperative autonomous driving methods, while additional tests on corner cases\nvalidate its robustness in real-world driving conditions.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09251v2",
    "published_date": "2024-08-17 16:42:13 UTC",
    "updated_date": "2024-09-16 05:23:07 UTC"
  },
  {
    "arxiv_id": "2408.09239v1",
    "title": "Towards Effective Top-N Hamming Search via Bipartite Graph Contrastive Hashing",
    "authors": [
      "Yankai Chen",
      "Yixiang Fang",
      "Yifei Zhang",
      "Chenhao Ma",
      "Yang Hong",
      "Irwin King"
    ],
    "abstract": "Searching on bipartite graphs serves as a fundamental task for various\nreal-world applications, such as recommendation systems, database retrieval,\nand document querying. Conventional approaches rely on similarity matching in\ncontinuous Euclidean space of vectorized node embeddings. To handle intensive\nsimilarity computation efficiently, hashing techniques for graph-structured\ndata have emerged as a prominent research direction. However, despite the\nretrieval efficiency in Hamming space, previous studies have encountered\ncatastrophic performance decay. To address this challenge, we investigate the\nproblem of hashing with Graph Convolutional Network for effective Top-N search.\nOur findings indicate the learning effectiveness of incorporating hashing\ntechniques within the exploration of bipartite graph reception fields, as\nopposed to simply treating hashing as post-processing to output embeddings. To\nfurther enhance the model performance, we advance upon these findings and\npropose Bipartite Graph Contrastive Hashing (BGCH+). BGCH+ introduces a novel\ndual augmentation approach to both intermediate information and hash code\noutputs in the latent feature spaces, thereby producing more expressive and\nrobust hash codes within a dual self-supervised learning paradigm.\nComprehensive empirical analyses on six real-world benchmarks validate the\neffectiveness of our dual feature contrastive learning in boosting the\nperformance of BGCH+ compared to existing approaches.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09239v1",
    "published_date": "2024-08-17 16:21:32 UTC",
    "updated_date": "2024-08-17 16:21:32 UTC"
  },
  {
    "arxiv_id": "2408.09236v3",
    "title": "Hybrid Semantic Search: Unveiling User Intent Beyond Keywords",
    "authors": [
      "Aman Ahluwalia",
      "Bishwajit Sutradhar",
      "Karishma Ghosh",
      "Indrapal Yadav",
      "Arpan Sheetal",
      "Prashant Patil"
    ],
    "abstract": "This paper addresses the limitations of traditional keyword-based search in\nunderstanding user intent and introduces a novel hybrid search approach that\nleverages the strengths of non-semantic search engines, Large Language Models\n(LLMs), and embedding models. The proposed system integrates keyword matching,\nsemantic vector embeddings, and LLM-generated structured queries to deliver\nhighly relevant and contextually appropriate search results. By combining these\ncomplementary methods, the hybrid approach effectively captures both explicit\nand implicit user intent.The paper further explores techniques to optimize\nquery execution for faster response times and demonstrates the effectiveness of\nthis hybrid search model in producing comprehensive and accurate search\noutcomes.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09236v3",
    "published_date": "2024-08-17 16:04:31 UTC",
    "updated_date": "2024-09-06 13:34:16 UTC"
  },
  {
    "arxiv_id": "2408.09235v2",
    "title": "Reference-Guided Verdict: LLMs-as-Judges in Automatic Evaluation of Free-Form Text",
    "authors": [
      "Sher Badshah",
      "Hassan Sajjad"
    ],
    "abstract": "The emergence of Large Language Models (LLMs) as chat assistants capable of\ngenerating human-like conversations has amplified the need for robust\nevaluation methods, particularly for open-ended tasks. Conventional metrics\nlike BLEU and ROUGE, while useful, are increasingly inadequate for capturing\nthe subtle semantics and contextual richness of such generative outputs. We\npropose a reference-guided verdict method that automates the evaluation process\nby leveraging multiple LLMs-as-judges. Through experiments on three open-ended\nquestion-answering tasks, we demonstrate that combining multiple LLMs-as-judges\nsignificantly improves the reliability and accuracy of evaluations,\nparticularly in complex tasks where a single model might struggle. Our findings\nreveal a strong correlation with human evaluations, establishing our method as\na viable and effective alternative to traditional metrics and human judgments,\nparticularly in the context of LLM-based chat assistants where the complexity\nand diversity of responses challenge existing benchmarks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "68T50, 68T07, 68T20",
      "I.2.0; I.2.7; I.2.2"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09235v2",
    "published_date": "2024-08-17 16:01:45 UTC",
    "updated_date": "2024-08-20 15:12:08 UTC"
  },
  {
    "arxiv_id": "2408.10276v4",
    "title": "FEDKIM: Adaptive Federated Knowledge Injection into Medical Foundation Models",
    "authors": [
      "Xiaochen Wang",
      "Jiaqi Wang",
      "Houping Xiao",
      "Jinghui Chen",
      "Fenglong Ma"
    ],
    "abstract": "Foundation models have demonstrated remarkable capabilities in handling\ndiverse modalities and tasks, outperforming conventional artificial\nintelligence (AI) approaches that are highly task-specific and\nmodality-reliant. In the medical domain, however, the development of\ncomprehensive foundation models is constrained by limited access to diverse\nmodalities and stringent privacy regulations. To address these constraints,\nthis study introduces a novel knowledge injection approach, FedKIM, designed to\nscale the medical foundation model within a federated learning framework.\nFedKIM leverages lightweight local models to extract healthcare knowledge from\nprivate data and integrates this knowledge into a centralized foundation model\nusing a designed adaptive Multitask Multimodal Mixture Of Experts (M3OE)\nmodule. This method not only preserves privacy but also enhances the model's\nability to handle complex medical tasks involving multiple modalities. Our\nextensive experiments across twelve tasks in seven modalities demonstrate the\neffectiveness of FedKIM in various settings, highlighting its potential to\nscale medical foundation models without direct access to sensitive data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by EMNLP'24 Main",
    "pdf_url": "http://arxiv.org/pdf/2408.10276v4",
    "published_date": "2024-08-17 15:42:29 UTC",
    "updated_date": "2025-05-04 20:14:21 UTC"
  },
  {
    "arxiv_id": "2408.11863v1",
    "title": "Unraveling Text Generation in LLMs: A Stochastic Differential Equation Approach",
    "authors": [
      "Yukun Zhang"
    ],
    "abstract": "This paper explores the application of Stochastic Differential Equations\n(SDE) to interpret the text generation process of Large Language Models (LLMs)\nsuch as GPT-4. Text generation in LLMs is modeled as a stochastic process where\neach step depends on previously generated content and model parameters,\nsampling the next word from a vocabulary distribution. We represent this\ngeneration process using SDE to capture both deterministic trends and\nstochastic perturbations. The drift term describes the deterministic trends in\nthe generation process, while the diffusion term captures the stochastic\nvariations. We fit these functions using neural networks and validate the model\non real-world text corpora. Through numerical simulations and comprehensive\nanalyses, including drift and diffusion analysis, stochastic process property\nevaluation, and phase space exploration, we provide deep insights into the\ndynamics of text generation. This approach not only enhances the understanding\nof the inner workings of LLMs but also offers a novel mathematical perspective\non language generation, which is crucial for diagnosing, optimizing, and\ncontrolling the quality of generated text.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11863v1",
    "published_date": "2024-08-17 15:30:27 UTC",
    "updated_date": "2024-08-17 15:30:27 UTC"
  },
  {
    "arxiv_id": "2408.09230v1",
    "title": "Siamese Multiple Attention Temporal Convolution Networks for Human Mobility Signature Identification",
    "authors": [
      "Zhipeng Zheng",
      "Yuchen Jiang",
      "Shiyao Zhang",
      "Xuetao Wei"
    ],
    "abstract": "The Human Mobility Signature Identification (HuMID) problem stands as a\nfundamental task within the realm of driving style representation, dedicated to\ndiscerning latent driving behaviors and preferences from diverse driver\ntrajectories for driver identification. Its solutions hold significant\nimplications across various domains (e.g., ride-hailing, insurance), wherein\ntheir application serves to safeguard users and mitigate potential fraudulent\nactivities. Present HuMID solutions often exhibit limitations in adaptability\nwhen confronted with lengthy trajectories, consequently incurring substantial\ncomputational overhead. Furthermore, their inability to effectively extract\ncrucial local information further impedes their performance. To address this\nproblem, we propose a Siamese Multiple Attention Temporal Convolutional Network\n(Siamese MA-TCN) to capitalize on the strengths of both TCN architecture and\nmulti-head self-attention, enabling the proficient extraction of both local and\nlong-term dependencies. Additionally, we devise a novel attention mechanism\ntailored for the efficient aggregation of multi-scale representations derived\nfrom our model. Experimental evaluations conducted on two real-world taxi\ntrajectory datasets reveal that our proposed model effectively extracts both\nlocal key information and long-term dependencies. These findings highlight the\nmodel's outstanding generalization capabilities, demonstrating its robustness\nand adaptability across datasets of varying sizes.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "27th IEEE International Conference on Intelligent Transportation\n  Systems (ITSC) (ITSC 2024)",
    "pdf_url": "http://arxiv.org/pdf/2408.09230v1",
    "published_date": "2024-08-17 15:27:38 UTC",
    "updated_date": "2024-08-17 15:27:38 UTC"
  },
  {
    "arxiv_id": "2408.09227v1",
    "title": "FEDMEKI: A Benchmark for Scaling Medical Foundation Models via Federated Knowledge Injection",
    "authors": [
      "Jiaqi Wang",
      "Xiaochen Wang",
      "Lingjuan Lyu",
      "Jinghui Chen",
      "Fenglong Ma"
    ],
    "abstract": "This study introduces the Federated Medical Knowledge Injection (FEDMEKI)\nplatform, a new benchmark designed to address the unique challenges of\nintegrating medical knowledge into foundation models under privacy constraints.\nBy leveraging a cross-silo federated learning approach, FEDMEKI circumvents the\nissues associated with centralized data collection, which is often prohibited\nunder health regulations like the Health Insurance Portability and\nAccountability Act (HIPAA) in the USA. The platform is meticulously designed to\nhandle multi-site, multi-modal, and multi-task medical data, which includes 7\nmedical modalities, including images, signals, texts, laboratory test results,\nvital signs, input variables, and output variables. The curated dataset to\nvalidate FEDMEKI covers 8 medical tasks, including 6 classification tasks (lung\nopacity detection, COVID-19 detection, electrocardiogram (ECG) abnormal\ndetection, mortality prediction, sepsis prediction, and enlarged\ncardiomediastinum detection) and 2 generation tasks (medical visual question\nanswering (MedVQA) and ECG noise clarification). This comprehensive dataset is\npartitioned across several clients to facilitate the decentralized training\nprocess under 16 benchmark approaches. FEDMEKI not only preserves data privacy\nbut also enhances the capability of medical foundation models by allowing them\nto learn from a broader spectrum of medical knowledge without direct data\nexposure, thereby setting a new benchmark in the application of foundation\nmodels within the healthcare sector.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Submitted to Neurips 2024 DB Track",
    "pdf_url": "http://arxiv.org/pdf/2408.09227v1",
    "published_date": "2024-08-17 15:18:56 UTC",
    "updated_date": "2024-08-17 15:18:56 UTC"
  },
  {
    "arxiv_id": "2408.09224v2",
    "title": "Neuro-Symbolic AI for Military Applications",
    "authors": [
      "Desta Haileselassie Hagos",
      "Danda B. Rawat"
    ],
    "abstract": "Artificial Intelligence (AI) plays a significant role in enhancing the\ncapabilities of defense systems, revolutionizing strategic decision-making, and\nshaping the future landscape of military operations. Neuro-Symbolic AI is an\nemerging approach that leverages and augments the strengths of neural networks\nand symbolic reasoning. These systems have the potential to be more impactful\nand flexible than traditional AI systems, making them well-suited for military\napplications. This paper comprehensively explores the diverse dimensions and\ncapabilities of Neuro-Symbolic AI, aiming to shed light on its potential\napplications in military contexts. We investigate its capacity to improve\ndecision-making, automate complex intelligence analysis, and strengthen\nautonomous systems. We further explore its potential to solve complex tasks in\nvarious domains, in addition to its applications in military contexts. Through\nthis exploration, we address ethical, strategic, and technical considerations\ncrucial to the development and deployment of Neuro-Symbolic AI in military and\ncivilian applications. Contributing to the growing body of research, this study\nrepresents a comprehensive exploration of the extensive possibilities offered\nby Neuro-Symbolic AI.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at IEEE Transactions on Artificial Intelligence (TAI)",
    "pdf_url": "http://arxiv.org/pdf/2408.09224v2",
    "published_date": "2024-08-17 15:06:43 UTC",
    "updated_date": "2024-08-24 20:44:30 UTC"
  },
  {
    "arxiv_id": "2408.09220v1",
    "title": "Flatten: Video Action Recognition is an Image Classification task",
    "authors": [
      "Junlin Chen",
      "Chengcheng Xu",
      "Yangfan Xu",
      "Jian Yang",
      "Jun Li",
      "Zhiping Shi"
    ],
    "abstract": "In recent years, video action recognition, as a fundamental task in the field\nof video understanding, has been deeply explored by numerous researchers.Most\ntraditional video action recognition methods typically involve converting\nvideos into three-dimensional data that encapsulates both spatial and temporal\ninformation, subsequently leveraging prevalent image understanding models to\nmodel and analyze these data. However,these methods have significant drawbacks.\nFirstly, when delving into video action recognition tasks, image understanding\nmodels often need to be adapted accordingly in terms of model architecture and\npreprocessing for these spatiotemporal tasks; Secondly, dealing with\nhigh-dimensional data often poses greater challenges and incurs higher time\ncosts compared to its lower-dimensional counterparts.To bridge the gap between\nimage-understanding and video-understanding tasks while simplifying the\ncomplexity of video comprehension, we introduce a novel video representation\narchitecture, Flatten, which serves as a plug-and-play module that can be\nseamlessly integrated into any image-understanding network for efficient and\neffective 3D temporal data modeling.Specifically, by applying specific\nflattening operations (e.g., row-major transform), 3D spatiotemporal data is\ntransformed into 2D spatial information, and then ordinary image understanding\nmodels are used to capture temporal dynamic and spatial semantic information,\nwhich in turn accomplishes effective and efficient video action recognition.\nExtensive experiments on commonly used datasets (Kinetics-400,\nSomething-Something v2, and HMDB-51) and three classical image classification\nmodels (Uniformer, SwinV2, and ResNet), have demonstrated that embedding\nFlatten provides a significant performance improvements over original model.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "13pages, 6figures",
    "pdf_url": "http://arxiv.org/pdf/2408.09220v1",
    "published_date": "2024-08-17 14:59:58 UTC",
    "updated_date": "2024-08-17 14:59:58 UTC"
  },
  {
    "arxiv_id": "2408.10275v2",
    "title": "FedKBP: Federated dose prediction framework for knowledge-based planning in radiation therapy",
    "authors": [
      "Jingyun Chen",
      "Martin King",
      "Yading Yuan"
    ],
    "abstract": "Dose prediction plays a key role in knowledge-based planning (KBP) by\nautomatically generating patient-specific dose distribution. Recent advances in\ndeep learning-based dose prediction methods necessitates collaboration among\ndata contributors for improved performance. Federated learning (FL) has emerged\nas a solution, enabling medical centers to jointly train deep-learning models\nwithout compromising patient data privacy. We developed the FedKBP framework to\nevaluate the performances of centralized, federated, and individual (i.e.\nseparated) training of dose prediction model on the 340 plans from OpenKBP\ndataset. To simulate FL and individual training, we divided the data into 8\ntraining sites. To evaluate the effect of inter-site data variation on model\ntraining, we implemented two types of case distributions: 1) Independent and\nidentically distributed (IID), where the training and validating cases were\nevenly divided among the 8 sites, and 2) non-IID, where some sites have more\ncases than others. The results show FL consistently outperforms individual\ntraining on both model optimization speed and out-of-sample testing scores,\nhighlighting the advantage of FL over individual training. Under IID data\ndivision, FL shows comparable performance to centralized training, underscoring\nFL as a promising alternative to traditional pooled-data training. Under\nnon-IID division, larger sites outperformed smaller sites by up to 19% on\ntesting scores, confirming the need of collaboration among data owners to\nachieve better prediction accuracy. Meanwhile, non-IID FL showed reduced\nperformance as compared to IID FL, posing the need for more sophisticated FL\nmethod beyond mere model averaging to handle data variation among participating\nsites.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by SPIE Medical Imaging 2025 Conference",
    "pdf_url": "http://arxiv.org/pdf/2408.10275v2",
    "published_date": "2024-08-17 14:57:14 UTC",
    "updated_date": "2025-01-28 23:30:54 UTC"
  },
  {
    "arxiv_id": "2408.09210v2",
    "title": "On the Improvement of Generalization and Stability of Forward-Only Learning via Neural Polarization",
    "authors": [
      "Erik B. Terres-Escudero",
      "Javier Del Ser",
      "Pablo Garcia-Bringas"
    ],
    "abstract": "Forward-only learning algorithms have recently gained attention as\nalternatives to gradient backpropagation, replacing the backward step of this\nlatter solver with an additional contrastive forward pass. Among these\napproaches, the so-called Forward-Forward Algorithm (FFA) has been shown to\nachieve competitive levels of performance in terms of generalization and\ncomplexity. Networks trained using FFA learn to contrastively maximize a\nlayer-wise defined goodness score when presented with real data (denoted as\npositive samples) and to minimize it when processing synthetic data (corr.\nnegative samples). However, this algorithm still faces weaknesses that\nnegatively affect the model accuracy and training stability, primarily due to a\ngradient imbalance between positive and negative samples. To overcome this\nissue, in this work we propose a novel implementation of the FFA algorithm,\ndenoted as Polar-FFA, which extends the original formulation by introducing a\nneural division (\\emph{polarization}) between positive and negative instances.\nNeurons in each of these groups aim to maximize their goodness when presented\nwith their respective data type, thereby creating a symmetric gradient\nbehavior. To empirically gauge the improved learning capabilities of our\nproposed Polar-FFA, we perform several systematic experiments using different\nactivation and goodness functions over image classification datasets. Our\nresults demonstrate that Polar-FFA outperforms FFA in terms of accuracy and\nconvergence speed. Furthermore, its lower reliance on hyperparameters reduces\nthe need for hyperparameter tuning to guarantee optimal generalization\ncapabilities, thereby allowing for a broader range of neural network\nconfigurations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted in European Conference on Artificial Intelligence (ECAI),\n  2024",
    "pdf_url": "http://arxiv.org/pdf/2408.09210v2",
    "published_date": "2024-08-17 14:32:18 UTC",
    "updated_date": "2024-09-11 16:13:51 UTC"
  },
  {
    "arxiv_id": "2408.09205v2",
    "title": "Architectural Foundations for the Large Language Model Infrastructures",
    "authors": [
      "Hongyin Zhu"
    ],
    "abstract": "The development of a large language model (LLM) infrastructure is a pivotal\nundertaking in artificial intelligence. This paper explores the intricate\nlandscape of LLM infrastructure, software, and data management. By analyzing\nthese core components, we emphasize the pivotal considerations and safeguards\ncrucial for successful LLM development. This work presents a concise synthesis\nof the challenges and strategies inherent in constructing a robust and\neffective LLM infrastructure, offering valuable insights for researchers and\npractitioners alike.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09205v2",
    "published_date": "2024-08-17 13:54:34 UTC",
    "updated_date": "2024-08-21 11:34:56 UTC"
  },
  {
    "arxiv_id": "2408.09196v1",
    "title": "Maintainability Challenges in ML: A Systematic Literature Review",
    "authors": [
      "Karthik Shivashankar",
      "Antonio Martini"
    ],
    "abstract": "Background: As Machine Learning (ML) advances rapidly in many fields, it is\nbeing adopted by academics and businesses alike. However, ML has a number of\ndifferent challenges in terms of maintenance not found in traditional software\nprojects. Identifying what causes these maintainability challenges can help\nmitigate them early and continue delivering value in the long run without\ndegrading ML performance. Aim: This study aims to identify and synthesise the\nmaintainability challenges in different stages of the ML workflow and\nunderstand how these stages are interdependent and impact each other's\nmaintainability. Method: Using a systematic literature review, we screened more\nthan 13000 papers, then selected and qualitatively analysed 56 of them.\nResults: (i) a catalogue of maintainability challenges in different stages of\nData Engineering, Model Engineering workflows and the current challenges when\nbuilding ML systems are discussed; (ii) a map of 13 maintainability challenges\nto different interdependent stages of ML that impact the overall workflow;\n(iii) Provided insights to developers of ML tools and researchers. Conclusions:\nIn this study, practitioners and organisations will learn about maintainability\nchallenges and their impact at different stages of ML workflow. This will\nenable them to avoid pitfalls and help to build a maintainable ML system. The\nimplications and challenges will also serve as a basis for future research to\nstrengthen our understanding of the ML system's maintainability.",
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09196v1",
    "published_date": "2024-08-17 13:24:15 UTC",
    "updated_date": "2024-08-17 13:24:15 UTC"
  },
  {
    "arxiv_id": "2409.00031v1",
    "title": "Quality Assessment in the Era of Large Models: A Survey",
    "authors": [
      "Zicheng Zhang",
      "Yingjie Zhou",
      "Chunyi Li",
      "Baixuan Zhao",
      "Xiaohong Liu",
      "Guangtao Zhai"
    ],
    "abstract": "Quality assessment, which evaluates the visual quality level of multimedia\nexperiences, has garnered significant attention from researchers and has\nevolved substantially through dedicated efforts. Before the advent of large\nmodels, quality assessment typically relied on small expert models tailored for\nspecific tasks. While these smaller models are effective at handling their\ndesignated tasks and predicting quality levels, they often lack explainability\nand robustness. With the advancement of large models, which align more closely\nwith human cognitive and perceptual processes, many researchers are now\nleveraging the prior knowledge embedded in these large models for quality\nassessment tasks. This emergence of quality assessment within the context of\nlarge models motivates us to provide a comprehensive review focusing on two key\naspects: 1) the assessment of large models, and 2) the role of large models in\nassessment tasks. We begin by reflecting on the historical development of\nquality assessment. Subsequently, we move to detailed discussions of related\nworks concerning quality assessment in the era of large models. Finally, we\noffer insights into the future progression and potential pathways for quality\nassessment in this new era. We hope this survey will enable a rapid\nunderstanding of the development of quality assessment in the era of large\nmodels and inspire further advancements in the field.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.00031v1",
    "published_date": "2024-08-17 13:20:55 UTC",
    "updated_date": "2024-08-17 13:20:55 UTC"
  },
  {
    "arxiv_id": "2409.00030v1",
    "title": "TimeSense: Multi-Person Device-free Indoor Localization via RTT",
    "authors": [
      "Mohamed Mohsen",
      "Hamada Rizk",
      "Hirozumi Yamaguch",
      "Moustafa Youssef"
    ],
    "abstract": "Locating the persons moving through an environment without the necessity of\nthem being equipped with special devices has become vital for many applications\nincluding security, IoT, healthcare, etc. Existing device-free indoor\nlocalization systems commonly rely on the utilization of Received Signal\nStrength Indicator (RSSI) and WiFi Channel State Information (CSI) techniques.\nHowever, the accuracy of RSSI is adversely affected by environmental factors\nlike multi-path interference and fading. Additionally, the lack of\nstandardization in CSI necessitates the use of specialized hardware and\nsoftware. In this paper, we present TimeSense, a deep learning-based\nmulti-person device-free indoor localization system that addresses these\nchallenges. TimeSense leverages Time of Flight information acquired by the\nfine-time measurement protocol of IEEE 802.11-2016 standard. Specifically, the\nmeasured round trip time between the transmitter and receiver is influenced by\nthe dynamic changes in the environment induced by human presence. TimeSense\neffectively detects this anomalous behavior using a stacked denoising\nauto-encoder model, thereby estimating the user's location. The system\nincorporates a probabilistic approach on top of the deep learning model to\nensure seamless tracking of the users. The evaluation of TimeSene in two\nrealistic environments demonstrates its efficacy, achieving a median\nlocalization accuracy of 1.57 and 2.65 meters. This surpasses the performance\nof state-of-the-art techniques by 49% and 103% in the two testbeds.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.00030v1",
    "published_date": "2024-08-17 13:12:33 UTC",
    "updated_date": "2024-08-17 13:12:33 UTC"
  },
  {
    "arxiv_id": "2408.09193v1",
    "title": "AI Managed Emergency Documentation with a Pretrained Model",
    "authors": [
      "David Menzies",
      "Sean Kirwan",
      "Ahmad Albarqawi"
    ],
    "abstract": "This study investigates the use of a large language model system to improve\nefficiency and quality in emergency department (ED) discharge letter writing.\nTime constraints and infrastructural deficits make compliance with current\ndischarge letter targets difficult. We explored potential efficiencies from an\nartificial intelligence software in the generation of ED discharge letters and\nthe attitudes of doctors toward this technology. The evaluated system leverages\nadvanced techniques to fine-tune a model to generate discharge summaries from\nshort-hand inputs, including voice, text, and electronic health record data.\nNineteen physicians with emergency medicine experience evaluated the system\ntext and voice-to-text interfaces against manual typing. The results showed\nsignificant time savings with MedWrite LLM interfaces compared to manual\nmethods.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Ethical approval for the study was obtained from the University\n  College Dublin, Human Research Ethics Committee (UCD HREC)",
    "pdf_url": "http://arxiv.org/pdf/2408.09193v1",
    "published_date": "2024-08-17 13:11:46 UTC",
    "updated_date": "2024-08-17 13:11:46 UTC"
  },
  {
    "arxiv_id": "2408.09189v1",
    "title": "SA-GDA: Spectral Augmentation for Graph Domain Adaptation",
    "authors": [
      "Jinhui Pang",
      "Zixuan Wang",
      "Jiliang Tang",
      "Mingyan Xiao",
      "Nan Yin"
    ],
    "abstract": "Graph neural networks (GNNs) have achieved impressive impressions for\ngraph-related tasks. However, most GNNs are primarily studied under the cases\nof signal domain with supervised training, which requires abundant\ntask-specific labels and is difficult to transfer to other domains. There are\nfew works focused on domain adaptation for graph node classification. They\nmainly focused on aligning the feature space of the source and target domains,\nwithout considering the feature alignment between different categories, which\nmay lead to confusion of classification in the target domain. However, due to\nthe scarcity of labels of the target domain, we cannot directly perform\neffective alignment of categories from different domains, which makes the\nproblem more challenging. In this paper, we present the \\textit{Spectral\nAugmentation for Graph Domain Adaptation (\\method{})} for graph node\nclassification. First, we observe that nodes with the same category in\ndifferent domains exhibit similar characteristics in the spectral domain, while\ndifferent classes are quite different. Following the observation, we align the\ncategory feature space of different domains in the spectral domain instead of\naligning the whole features space, and we theoretical proof the stability of\nproposed \\method{}. Then, we develop a dual graph convolutional network to\njointly exploits local and global consistency for feature aggregation. Last, we\nutilize a domain classifier with an adversarial learning submodule to\nfacilitate knowledge transfer between different domain graphs. Experimental\nresults on a variety of publicly available datasets reveal the effectiveness of\nour \\method{}.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09189v1",
    "published_date": "2024-08-17 13:01:45 UTC",
    "updated_date": "2024-08-17 13:01:45 UTC"
  },
  {
    "arxiv_id": "2408.09186v1",
    "title": "EEG-SCMM: Soft Contrastive Masked Modeling for Cross-Corpus EEG-Based Emotion Recognition",
    "authors": [
      "Qile Liu",
      "Weishan Ye",
      "Yulu Liu",
      "Zhen Liang"
    ],
    "abstract": "Emotion recognition using electroencephalography (EEG) signals has garnered\nwidespread attention in recent years. However, existing studies have struggled\nto develop a sufficiently generalized model suitable for different datasets\nwithout re-training (cross-corpus). This difficulty arises because distribution\ndifferences across datasets far exceed the intra-dataset variability. To solve\nthis problem, we propose a novel Soft Contrastive Masked Modeling (SCMM)\nframework. Inspired by emotional continuity, SCMM integrates soft contrastive\nlearning with a new hybrid masking strategy to effectively mine the \"short-term\ncontinuity\" characteristics inherent in human emotions. During the\nself-supervised learning process, soft weights are assigned to sample pairs,\nenabling adaptive learning of similarity relationships across samples.\nFurthermore, we introduce an aggregator that weightedly aggregates\ncomplementary information from multiple close samples based on pairwise\nsimilarities among samples to enhance fine-grained feature representation,\nwhich is then used for original sample reconstruction. Extensive experiments on\nthe SEED, SEED-IV and DEAP datasets show that SCMM achieves state-of-the-art\n(SOTA) performance, outperforming the second-best method by an average accuracy\nof 4.26% under two types of cross-corpus conditions (same-class and\ndifferent-class) for EEG-based emotion recognition.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "16 pages, 8 figures, 15 tables, submitted to AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2408.09186v1",
    "published_date": "2024-08-17 12:35:13 UTC",
    "updated_date": "2024-08-17 12:35:13 UTC"
  },
  {
    "arxiv_id": "2408.09177v1",
    "title": "Chinese Metaphor Recognition Using a Multi-stage Prompting Large Language Model",
    "authors": [
      "Jie Wang",
      "Jin Wang",
      "Xuejie Zhang"
    ],
    "abstract": "Metaphors are common in everyday language, and the identification and\nunderstanding of metaphors are facilitated by models to achieve a better\nunderstanding of the text. Metaphors are mainly identified and generated by\npre-trained models in existing research, but situations, where tenors or\nvehicles are not included in the metaphor, cannot be handled. The problem can\nbe effectively solved by using Large Language Models (LLMs), but significant\nroom for exploration remains in this early-stage research area. A multi-stage\ngenerative heuristic-enhanced prompt framework is proposed in this study to\nenhance the ability of LLMs to recognize tenors, vehicles, and grounds in\nChinese metaphors. In the first stage, a small model is trained to obtain the\nrequired confidence score for answer candidate generation. In the second stage,\nquestions are clustered and sampled according to specific rules. Finally, the\nheuristic-enhanced prompt needed is formed by combining the generated answer\ncandidates and demonstrations. The proposed model achieved 3rd place in Track 1\nof Subtask 1, 1st place in Track 2 of Subtask 1, and 1st place in both tracks\nof Subtask 2 at the NLPCC-2024 Shared Task 9.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09177v1",
    "published_date": "2024-08-17 11:56:38 UTC",
    "updated_date": "2024-08-17 11:56:38 UTC"
  },
  {
    "arxiv_id": "2408.09176v1",
    "title": "Cognitive LLMs: Towards Integrating Cognitive Architectures and Large Language Models for Manufacturing Decision-making",
    "authors": [
      "Siyu Wu",
      "Alessandro Oltramari",
      "Jonathan Francis",
      "C. Lee Giles",
      "Frank E. Ritter"
    ],
    "abstract": "Resolving the dichotomy between the human-like yet constrained reasoning\nprocesses of Cognitive Architectures and the broad but often noisy inference\nbehavior of Large Language Models (LLMs) remains a challenging but exciting\npursuit, for enabling reliable machine reasoning capabilities in production\nsystems. Because Cognitive Architectures are famously developed for the purpose\nof modeling the internal mechanisms of human cognitive decision-making at a\ncomputational level, new investigations consider the goal of informing LLMs\nwith the knowledge necessary for replicating such processes, e.g., guided\nperception, memory, goal-setting, and action. Previous approaches that use LLMs\nfor grounded decision-making struggle with complex reasoning tasks that require\nslower, deliberate cognition over fast and intuitive inference -- reporting\nissues related to the lack of sufficient grounding, as in hallucination. To\nresolve these challenges, we introduce LLM-ACTR, a novel neuro-symbolic\narchitecture that provides human-aligned and versatile decision-making by\nintegrating the ACT-R Cognitive Architecture with LLMs. Our framework extracts\nand embeds knowledge of ACT-R's internal decision-making process as latent\nneural representations, injects this information into trainable LLM adapter\nlayers, and fine-tunes the LLMs for downstream prediction. Our experiments on\nnovel Design for Manufacturing tasks show both improved task performance as\nwell as improved grounded decision-making capability of our approach, compared\nto LLM-only baselines that leverage chain-of-thought reasoning strategies.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.SC"
    ],
    "primary_category": "cs.AI",
    "comment": "20 pages, 8 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2408.09176v1",
    "published_date": "2024-08-17 11:49:53 UTC",
    "updated_date": "2024-08-17 11:49:53 UTC"
  },
  {
    "arxiv_id": "2408.09172v4",
    "title": "Unlocking the Power of LLM Uncertainty for Active In-Context Example Selection",
    "authors": [
      "Hsiu-Yuan Huang",
      "Zichen Wu",
      "Yutong Yang",
      "Junzhao Zhang",
      "Yunfang Wu"
    ],
    "abstract": "Large Language Models (LLMs) have shown remarkable performance across a wide\nrange of downstream tasks. However, it is challenging for users to discern\nwhether the responses of LLM are generated with certainty or are fabricated to\nmeet user expectations. In this paper, we introduce Uncertainty Tripartite\nTesting Paradigm (Unc-TTP), a novel method for classifying LLM uncertainty by\nleveraging output inconsistency. Specifically, Unc-TTP performs three rounds of\nsampling under varying label injection interference, enumerating all possible\noutcomes, and uses the degree of output inconsistency as the indicator of the\nLLM's intrinsic uncertainty. To validate the effectiveness of this\ninconsistency-defined uncertainty, we draw inspiration from Active Learning,\ncomparing the informativeness of actively selected in-context examples. Our\nexperiments show that uncertainty examples selected via Unc-TTP are more\ninformative than certainty examples. Furthermore, the Unc-TTP-guided\nuncertainty-based active example selection strategy outperforms existing\nmethods, highlighting its effectiveness in classifying LLM uncertainty and\nenhancing in-context learning. This work not only underscores the potential of\ninconsistency-based uncertainty classification for both open- and closed-source\nLLMs but also presents a practical approach for leveraging uncertainty to\nimprove LLM performance in real-world tasks.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09172v4",
    "published_date": "2024-08-17 11:33:23 UTC",
    "updated_date": "2025-01-12 16:31:19 UTC"
  },
  {
    "arxiv_id": "2408.09168v1",
    "title": "Ranking Across Different Content Types: The Robust Beauty of Multinomial Blending",
    "authors": [
      "Jan Malte Lichtenberg",
      "Giuseppe Di Benedetto",
      "Matteo Ruffini"
    ],
    "abstract": "An increasing number of media streaming services have expanded their\nofferings to include entities of multiple content types. For instance, audio\nstreaming services that started by offering music only, now also offer\npodcasts, merchandise items, and videos. Ranking items across different content\ntypes into a single slate poses a significant challenge for traditional\nlearning-to-rank (LTR) algorithms due to differing user engagement patterns for\ndifferent content types. We explore a simple method for cross-content-type\nranking, called multinomial blending (MB), which can be used in conjunction\nwith most existing LTR algorithms. We compare MB to existing baselines not only\nin terms of ranking quality but also from other industry-relevant perspectives\nsuch as interpretability, ease-of-use, and stability in dynamic environments\nwith changing user behavior and ranking model retraining. Finally, we report\nthe results of an A/B test from an Amazon Music ranking use-case.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "To appear in 18th ACM Conference on Recommender Systems (RecSys24),\n  Bari, Italy. ACM, New York, NY, USA, 3 pages",
    "pdf_url": "http://arxiv.org/pdf/2408.09168v1",
    "published_date": "2024-08-17 11:11:31 UTC",
    "updated_date": "2024-08-17 11:11:31 UTC"
  },
  {
    "arxiv_id": "2408.09158v2",
    "title": "Linear Attention is Enough in Spatial-Temporal Forecasting",
    "authors": [
      "Xinyu Ning"
    ],
    "abstract": "As the most representative scenario of spatial-temporal forecasting tasks,\nthe traffic forecasting task attracted numerous attention from machine learning\ncommunity due to its intricate correlation both in space and time dimension.\nExisting methods often treat road networks over time as spatial-temporal\ngraphs, addressing spatial and temporal representations independently. However,\nthese approaches struggle to capture the dynamic topology of road networks,\nencounter issues with message passing mechanisms and over-smoothing, and face\nchallenges in learning spatial and temporal relationships separately. To\naddress these limitations, we propose treating nodes in road networks at\ndifferent time steps as independent spatial-temporal tokens and feeding them\ninto a vanilla Transformer to learn complex spatial-temporal patterns, design\n\\textbf{STformer} achieving SOTA. Given its quadratic complexity, we introduce\na variant \\textbf{NSTformer} based on Nystr$\\ddot{o}$m method to approximate\nself-attention with linear complexity but even slightly better than former in a\nfew cases astonishingly. Extensive experimental results on traffic datasets\ndemonstrate that the proposed method achieves state-of-the-art performance at\nan affordable computational cost. Our code is available at\n\\href{https://github.com/XinyuNing/STformer-and-NSTformer}{https://github.com/XinyuNing/STformer-and-NSTformer}.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09158v2",
    "published_date": "2024-08-17 10:06:50 UTC",
    "updated_date": "2024-09-13 14:34:26 UTC"
  },
  {
    "arxiv_id": "2408.09150v3",
    "title": "CogLM: Tracking Cognitive Development of Large Language Models",
    "authors": [
      "Xinglin Wang",
      "Peiwen Yuan",
      "Shaoxiong Feng",
      "Yiwei Li",
      "Boyuan Pan",
      "Heda Wang",
      "Yao Hu",
      "Kan Li"
    ],
    "abstract": "Piaget's Theory of Cognitive Development (PTC) posits that the development of\ncognitive levels forms the foundation for human learning across various\nabilities. As Large Language Models (LLMs) have recently shown remarkable\nabilities across a wide variety of tasks, we are curious about the cognitive\nlevels of current LLMs: to what extent they have developed and how this\ndevelopment has been achieved. To this end, we construct a benchmark CogLM\n(Cognitive Ability Evaluation for Language Model) based on PTC to assess the\ncognitive levels of LLMs. CogLM comprises 1,220 questions spanning 10 cognitive\nabilities crafted by more than 20 human experts, providing a comprehensive\ntestbed for the cognitive levels of LLMs. Through extensive experiments across\nmultiple mainstream LLMs with CogLM, we find that: (1) In our testing\nframework, advanced LLMs (such as GPT-4) have demonstrated human-like cognitive\nabilities, comparable to those of a 20-year-old human. (2) The parameter size\nand optimization objective are two key factors affecting the cognitive levels\nof LLMs. (3) The performance on downstream tasks is positively correlated with\nthe level of cognitive abilities. These findings fill the gap in research on\nthe cognitive abilities of LLMs, tracing the development of LLMs from a\ncognitive perspective and guiding the future direction of their evolution.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "NAACL2025 Main",
    "pdf_url": "http://arxiv.org/pdf/2408.09150v3",
    "published_date": "2024-08-17 09:49:40 UTC",
    "updated_date": "2025-02-12 03:00:20 UTC"
  },
  {
    "arxiv_id": "2408.09140v1",
    "title": "Learning to Explore for Stochastic Gradient MCMC",
    "authors": [
      "SeungHyun Kim",
      "Seohyeon Jung",
      "Seonghyeon Kim",
      "Juho Lee"
    ],
    "abstract": "Bayesian Neural Networks(BNNs) with high-dimensional parameters pose a\nchallenge for posterior inference due to the multi-modality of the posterior\ndistributions. Stochastic Gradient MCMC(SGMCMC) with cyclical learning rate\nscheduling is a promising solution, but it requires a large number of sampling\nsteps to explore high-dimensional multi-modal posteriors, making it\ncomputationally expensive. In this paper, we propose a meta-learning strategy\nto build \\gls{sgmcmc} which can efficiently explore the multi-modal target\ndistributions. Our algorithm allows the learned SGMCMC to quickly explore the\nhigh-density region of the posterior landscape. Also, we show that this\nexploration property is transferrable to various tasks, even for the ones\nunseen during a meta-training stage. Using popular image classification\nbenchmarks and a variety of downstream tasks, we demonstrate that our method\nsignificantly improves the sampling efficiency, achieving better performance\nthan vanilla \\gls{sgmcmc} without incurring significant computational overhead.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09140v1",
    "published_date": "2024-08-17 08:36:42 UTC",
    "updated_date": "2024-08-17 08:36:42 UTC"
  },
  {
    "arxiv_id": "2408.09135v3",
    "title": "Vanilla Gradient Descent for Oblique Decision Trees",
    "authors": [
      "Subrat Prasad Panda",
      "Blaise Genest",
      "Arvind Easwaran",
      "Ponnuthurai Nagaratnam Suganthan"
    ],
    "abstract": "Decision Trees (DTs) constitute one of the major highly non-linear AI models,\nvalued, e.g., for their efficiency on tabular data. Learning accurate DTs is,\nhowever, complicated, especially for oblique DTs, and does take a significant\ntraining time. Further, DTs suffer from overfitting, e.g., they proverbially\n\"do not generalize\" in regression tasks. Recently, some works proposed ways to\nmake (oblique) DTs differentiable. This enables highly efficient\ngradient-descent algorithms to be used to learn DTs. It also enables\ngeneralizing capabilities by learning regressors at the leaves simultaneously\nwith the decisions in the tree. Prior approaches to making DTs differentiable\nrely either on probabilistic approximations at the tree's internal nodes (soft\nDTs) or on approximations in gradient computation at the internal node\n(quantized gradient descent). In this work, we propose DTSemNet, a novel\nsemantically equivalent and invertible encoding for (hard, oblique) DTs as\nNeural Networks (NNs), that uses standard vanilla gradient descent. Experiments\nacross various classification and regression benchmarks show that oblique DTs\nlearned using DTSemNet are more accurate than oblique DTs of similar size\nlearned using state-of-the-art techniques. Further, DT training time is\nsignificantly reduced. We also experimentally demonstrate that DTSemNet can\nlearn DT policies as efficiently as NN policies in the Reinforcement Learning\n(RL) setup with physical inputs (dimensions $\\leq32$). The code is available at\nhttps://github.com/CPS-research-group/dtsemnet.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published in European Conference on Artificial Intelligence (ECAI),\n  2024. Full version (includes supplementary material)",
    "pdf_url": "http://arxiv.org/pdf/2408.09135v3",
    "published_date": "2024-08-17 08:18:40 UTC",
    "updated_date": "2024-10-15 12:58:35 UTC"
  },
  {
    "arxiv_id": "2408.09134v1",
    "title": "Better Python Programming for all: With the focus on Maintainability",
    "authors": [
      "Karthik Shivashankar",
      "Antonio Martini"
    ],
    "abstract": "This study aims to enhance the maintainability of code generated by Large\nLanguage Models (LLMs), with a focus on the Python programming language. As the\nuse of LLMs for coding assistance grows, so do concerns about the\nmaintainability of the code they produce. Previous research has mainly\nconcentrated on the functional accuracy and testing success of generated code,\noverlooking aspects of maintainability.\n  Our approach involves the use of a specifically designed dataset for training\nand evaluating the model, ensuring a thorough assessment of code\nmaintainability. At the heart of our work is the fine-tuning of an LLM for code\nrefactoring, aimed at enhancing code readability, reducing complexity, and\nimproving overall maintainability.\n  After fine-tuning an LLM to prioritize code maintainability, our evaluations\nindicate that this model significantly improves code maintainability standards,\nsuggesting a promising direction for the future of AI-assisted software\ndevelopment.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09134v1",
    "published_date": "2024-08-17 08:14:22 UTC",
    "updated_date": "2024-08-17 08:14:22 UTC"
  },
  {
    "arxiv_id": "2408.09128v1",
    "title": "Identifying Technical Debt and Its Types Across Diverse Software Projects Issues",
    "authors": [
      "Karthik Shivashankar",
      "Mili Orucevic",
      "Maren Maritsdatter Kruke",
      "Antonio Martini"
    ],
    "abstract": "Technical Debt (TD) identification in software projects issues is crucial for\nmaintaining code quality, reducing long-term maintenance costs, and improving\noverall project health. This study advances TD classification using\ntransformer-based models, addressing the critical need for accurate and\nefficient TD identification in large-scale software development.\n  Our methodology employs multiple binary classifiers for TD and its type,\ncombined through ensemble learning, to enhance accuracy and robustness in\ndetecting various forms of TD. We train and evaluate these models on a\ncomprehensive dataset from GitHub Archive Issues (2015-2024), supplemented with\nindustrial data validation.\n  We demonstrate that in-project fine-tuned transformer models significantly\noutperform task-specific fine-tuned models in TD classification, highlighting\nthe importance of project-specific context in accurate TD identification. Our\nresearch also reveals the superiority of specialized binary classifiers over\nmulti-class models for TD and its type identification, enabling more targeted\ndebt resolution strategies. A comparative analysis shows that the smaller\nDistilRoBERTa model is more effective than larger language models like GPTs for\nTD classification tasks, especially after fine-tuning, offering insights into\nefficient model selection for specific TD detection tasks.\n  The study also assesses generalization capabilities using metrics such as\nMCC, AUC ROC, Recall, and F1 score, focusing on model effectiveness,\nfine-tuning impact, and relative performance. By validating our approach on\nout-of-distribution and real-world industrial datasets, we ensure practical\napplicability, addressing the diverse nature of software projects.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09128v1",
    "published_date": "2024-08-17 07:46:54 UTC",
    "updated_date": "2024-08-17 07:46:54 UTC"
  },
  {
    "arxiv_id": "2408.09125v1",
    "title": "Markov Balance Satisfaction Improves Performance in Strictly Batch Offline Imitation Learning",
    "authors": [
      "Rishabh Agrawal",
      "Nathan Dahlin",
      "Rahul Jain",
      "Ashutosh Nayyar"
    ],
    "abstract": "Imitation learning (IL) is notably effective for robotic tasks where directly\nprogramming behaviors or defining optimal control costs is challenging. In this\nwork, we address a scenario where the imitator relies solely on observed\nbehavior and cannot make environmental interactions during learning. It does\nnot have additional supplementary datasets beyond the expert's dataset nor any\ninformation about the transition dynamics. Unlike state-of-the-art (SOTA) IL\nmethods, this approach tackles the limitations of conventional IL by operating\nin a more constrained and realistic setting. Our method uses the Markov balance\nequation and introduces a novel conditional density estimation-based imitation\nlearning framework. It employs conditional normalizing flows for transition\ndynamics estimation and aims at satisfying a balance equation for the\nenvironment. Through a series of numerical experiments on Classic Control and\nMuJoCo environments, we demonstrate consistently superior empirical performance\ncompared to many SOTA IL algorithms.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09125v1",
    "published_date": "2024-08-17 07:17:19 UTC",
    "updated_date": "2024-08-17 07:17:19 UTC"
  },
  {
    "arxiv_id": "2408.09121v4",
    "title": "Selective Prompt Anchoring for Code Generation",
    "authors": [
      "Yuan Tian",
      "Tianyi Zhang"
    ],
    "abstract": "Recent advances in large language models (LLMs) have transformed software\ndevelopment by automatically generating code from natural language. Yet\nchallenges remain in generating fully correct code that aligns with user\nintent. Our study reveals that LLMs tend to pay less attention to user prompts\nas more code tokens are generated. We hypothesize that this attention dilution\nissue is an important reason for code generation errors. To mitigate this\nissue, we propose Selective Prompt Anchoring (SPA) to guide code LLMs to pay\nmore attention to user intent when generating code. We evaluate SPA using six\nbase LLMs across six benchmarks. Our results demonstrate that SPA enhances\nPass@1 by up to 12.9%, consistently outperforming SOTA code generation methods\nin all settings. Our code is available at\nhttps://github.com/magic-YuanTian/Selective-Prompt-Anchoring.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.SE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09121v4",
    "published_date": "2024-08-17 07:11:02 UTC",
    "updated_date": "2025-02-21 03:02:41 UTC"
  },
  {
    "arxiv_id": "2408.11067v1",
    "title": "Toward End-to-End Bearing Fault Diagnosis for Industrial Scenarios with Spiking Neural Networks",
    "authors": [
      "Yongqi Ding",
      "Lin Zuo",
      "Mengmeng Jing",
      "Kunshan Yang",
      "Biao Chen",
      "Yunqian Yu"
    ],
    "abstract": "Spiking neural networks (SNNs) transmit information via low-power binary\nspikes and have received widespread attention in areas such as computer vision\nand reinforcement learning. However, there have been very few explorations of\nSNNs in more practical industrial scenarios. In this paper, we focus on the\napplication of SNNs in bearing fault diagnosis to facilitate the integration of\nhigh-performance AI algorithms and real-world industries. In particular, we\nidentify two key limitations of existing SNN fault diagnosis methods:\ninadequate encoding capacity that necessitates cumbersome data preprocessing,\nand non-spike-oriented architectures that constrain the performance of SNNs. To\nalleviate these problems, we propose a Multi-scale Residual Attention SNN\n(MRA-SNN) to simultaneously improve the efficiency, performance, and robustness\nof SNN methods. By incorporating a lightweight attention mechanism, we have\ndesigned a multi-scale attention encoding module to extract multiscale fault\nfeatures from vibration signals and encode them as spatio-temporal spikes,\neliminating the need for complicated preprocessing. Then, the spike residual\nattention block extracts high-dimensional fault features and enhances the\nexpressiveness of sparse spikes with the attention mechanism for end-to-end\ndiagnosis. In addition, the performance and robustness of MRA-SNN is further\nenhanced by introducing the lightweight attention mechanism within the spiking\nneurons to simulate the biological dendritic filtering effect. Extensive\nexperiments on MFPT and JNU benchmark datasets demonstrate that MRA-SNN\nsignificantly outperforms existing methods in terms of accuracy, energy\nconsumption and noise robustness, and is more feasible for deployment in\nreal-world industrial scenarios.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "13 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.11067v1",
    "published_date": "2024-08-17 06:41:58 UTC",
    "updated_date": "2024-08-17 06:41:58 UTC"
  },
  {
    "arxiv_id": "2408.09111v2",
    "title": "Measuring Agreeableness Bias in Multimodal Models",
    "authors": [
      "Jaehyuk Lim",
      "Bruce W. Lee"
    ],
    "abstract": "This paper examines a phenomenon in multimodal language models where\npre-marked options in question images can significantly influence model\nresponses. Our study employs a systematic methodology to investigate this\neffect: we present models with images of multiple-choice questions, which they\ninitially answer correctly, then expose the same model to versions with\npre-marked options. Our findings reveal a significant shift in the models'\nresponses towards the pre-marked option, even when it contradicts their answers\nin the neutral settings. Comprehensive evaluations demonstrate that this\nagreeableness bias is a consistent and quantifiable behavior across various\nmodel architectures. These results show potential limitations in the\nreliability of these models when processing images with pre-marked options,\nraising important questions about their application in critical decision-making\ncontexts where such visual cues might be present.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09111v2",
    "published_date": "2024-08-17 06:25:36 UTC",
    "updated_date": "2024-10-15 02:42:37 UTC"
  },
  {
    "arxiv_id": "2408.09108v3",
    "title": "Temporal Reversal Regularization for Spiking Neural Networks: Hybrid Spatio-Temporal Invariance for Generalization",
    "authors": [
      "Lin Zuo",
      "Yongqi Ding",
      "Wenwei Luo",
      "Mengmeng Jing",
      "Kunshan Yang"
    ],
    "abstract": "Spiking neural networks (SNNs) have received widespread attention as an\nultra-low power computing paradigm. Recent studies have shown that SNNs suffer\nfrom severe overfitting, which limits their generalization performance. In this\npaper, we propose a simple yet effective Temporal Reversal Regularization (TRR)\nto mitigate overfitting during training and facilitate generalization of SNNs.\nWe exploit the inherent temporal properties of SNNs to perform input/feature\ntemporal reversal perturbations, prompting the SNN to produce original-reversed\nconsistent outputs and learn perturbation-invariant representations. To further\nenhance generalization, we utilize the lightweight ``star operation\" (Hadamard\nproduct) for feature hybridization of original and temporally reversed spike\nfiring rates, which expands the implicit dimensionality and acts as a\nspatio-temporal regularizer. We show theoretically that our method is able to\ntighten the upper bound of the generalization error, and extensive experiments\non static/neuromorphic recognition as well as 3D point cloud classification\ntasks demonstrate its effectiveness, versatility, and adversarial robustness.\nIn particular, our regularization significantly improves the recognition\naccuracy of low-latency SNN for neuromorphic objects, contributing to the\nreal-world deployment of neuromorphic computational software-hardware\nintegration.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "17 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.09108v3",
    "published_date": "2024-08-17 06:23:38 UTC",
    "updated_date": "2025-03-10 08:30:52 UTC"
  },
  {
    "arxiv_id": "2408.09106v3",
    "title": "Fragment-Masked Diffusion for Molecular Optimization",
    "authors": [
      "Kun Li",
      "Xiantao Cai",
      "Jia Wu",
      "Shirui Pan",
      "Huiting Xu",
      "Bo Du",
      "Wenbin Hu"
    ],
    "abstract": "Molecular optimization is a crucial aspect of drug discovery, aimed at\nrefining molecular structures to enhance drug efficacy and minimize side\neffects, ultimately accelerating the overall drug development process. Many\nmolecular optimization methods have been proposed, significantly advancing drug\ndiscovery. These methods primarily on understanding the specific drug target\nstructures or their hypothesized roles in combating diseases. However,\nchallenges such as a limited number of available targets and a difficulty\ncapturing clear structures hinder innovative drug development. In contrast,\nphenotypic drug discovery (PDD) does not depend on clear target structures and\ncan identify hits with novel and unbiased polypharmacology signatures. As a\nresult, PDD-based molecular optimization can reduce potential safety risks\nwhile optimizing phenotypic activity, thereby increasing the likelihood of\nclinical success. Therefore, we propose a fragment-masked molecular\noptimization method based on PDD (FMOP). FMOP employs a regression-free\ndiffusion model to conditionally optimize the molecular masked regions,\neffectively generating new molecules with similar scaffolds. On the large-scale\ndrug response dataset GDSCv2, we optimize the potential molecules across all\n985 cell lines. The overall experiments demonstrate that the in-silico\noptimization success rate reaches 95.4\\%, with an average efficacy increase of\n7.5\\%. Additionally, we conduct extensive ablation and visualization\nexperiments, confirming that FMOP is an effective and robust molecular\noptimization method. The code is available at:\nhttps://anonymous.4open.science/r/FMOP-98C2.",
    "categories": [
      "q-bio.BM",
      "cs.AI"
    ],
    "primary_category": "q-bio.BM",
    "comment": "12 pages, 9 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2408.09106v3",
    "published_date": "2024-08-17 06:00:58 UTC",
    "updated_date": "2025-05-14 16:26:53 UTC"
  },
  {
    "arxiv_id": "2408.09097v1",
    "title": "Depth-guided Texture Diffusion for Image Semantic Segmentation",
    "authors": [
      "Wei Sun",
      "Yuan Li",
      "Qixiang Ye",
      "Jianbin Jiao",
      "Yanzhao Zhou"
    ],
    "abstract": "Depth information provides valuable insights into the 3D structure especially\nthe outline of objects, which can be utilized to improve the semantic\nsegmentation tasks. However, a naive fusion of depth information can disrupt\nfeature and compromise accuracy due to the modality gap between the depth and\nthe vision. In this work, we introduce a Depth-guided Texture Diffusion\napproach that effectively tackles the outlined challenge. Our method extracts\nlow-level features from edges and textures to create a texture image. This\nimage is then selectively diffused across the depth map, enhancing structural\ninformation vital for precisely extracting object outlines. By integrating this\nenriched depth map with the original RGB image into a joint feature embedding,\nour method effectively bridges the disparity between the depth map and the\nimage, enabling more accurate semantic segmentation. We conduct comprehensive\nexperiments across diverse, commonly-used datasets spanning a wide range of\nsemantic segmentation tasks, including Camouflaged Object Detection (COD),\nSalient Object Detection (SOD), and indoor semantic segmentation. With\nsource-free estimated depth or depth captured by depth cameras, our method\nconsistently outperforms existing baselines and achieves new state-of-theart\nresults, demonstrating the effectiveness of our Depth-guided Texture Diffusion\nfor image semantic segmentation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09097v1",
    "published_date": "2024-08-17 04:55:03 UTC",
    "updated_date": "2024-08-17 04:55:03 UTC"
  },
  {
    "arxiv_id": "2408.09094v1",
    "title": "Research on color recipe recommendation based on unstructured data using TENN",
    "authors": [
      "Seongsu Jhang",
      "Donghwi Yoo",
      "Jaeyong Kown"
    ],
    "abstract": "Recently, services and business models based on large language models, such\nas OpenAI Chatgpt, Google BARD, and Microsoft copilot, have been introduced,\nand the applications utilizing natural language processing with deep learning\nare increasing, and it is one of the natural language preprocessing methods.\nConversion to machine language through tokenization and processing of\nunstructured data are increasing. Although algorithms that can understand and\napply human language are becoming increasingly sophisticated, it is difficult\nto apply them to processes that rely on human emotions and senses in industries\nthat still mainly deal with standardized data. In particular, in processes\nwhere brightness, saturation, and color information are essential, such as\npainting and injection molding, most small and medium-sized companies,\nexcluding large corporations, rely on the tacit knowledge and sensibility of\ncolor mixers, and even customer companies often present non-standardized\nrequirements. . In this paper, we proposed TENN to infer color recipe based on\nunstructured data with emotional natural language, and demonstrated it.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09094v1",
    "published_date": "2024-08-17 04:45:48 UTC",
    "updated_date": "2024-08-17 04:45:48 UTC"
  },
  {
    "arxiv_id": "2408.11862v1",
    "title": "Sentiment analysis of preservice teachers' reflections using a large language model",
    "authors": [
      "Yunsoo Park",
      "Younkyung Hong"
    ],
    "abstract": "In this study, the emotion and tone of preservice teachers' reflections were\nanalyzed using sentiment analysis with LLMs: GPT-4, Gemini, and BERT. We\ncompared the results to understand how each tool categorizes and describes\nindividual reflections and multiple reflections as a whole. This study aims to\nexplore ways to bridge the gaps between qualitative, quantitative, and\ncomputational analyses of reflective practices in teacher education. This study\nfinds that to effectively integrate LLM analysis into teacher education,\ndeveloping an analysis method and result format that are both comprehensive and\nrelevant for preservice teachers and teacher educators is crucial.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "5 pages, 2 tables, WAIE 2024 (2024 6th International Workshop on\n  Artificial Intelligence and Education)",
    "pdf_url": "http://arxiv.org/pdf/2408.11862v1",
    "published_date": "2024-08-17 01:56:15 UTC",
    "updated_date": "2024-08-17 01:56:15 UTC"
  },
  {
    "arxiv_id": "2408.09065v1",
    "title": "Linking Robustness and Generalization: A k* Distribution Analysis of Concept Clustering in Latent Space for Vision Models",
    "authors": [
      "Shashank Kotyan",
      "Pin-Yu Chen",
      "Danilo Vasconcellos Vargas"
    ],
    "abstract": "Most evaluations of vision models use indirect methods to assess latent space\nquality. These methods often involve adding extra layers to project the latent\nspace into a new one. This projection makes it difficult to analyze and compare\nthe original latent space. This article uses the k* Distribution, a local\nneighborhood analysis method, to examine the learned latent space at the level\nof individual concepts, which can be extended to examine the entire latent\nspace. We introduce skewness-based true and approximate metrics for\ninterpreting individual concepts to assess the overall quality of vision\nmodels' latent space. Our findings indicate that current vision models\nfrequently fracture the distributions of individual concepts within the latent\nspace. Nevertheless, as these models improve in generalization across multiple\ndatasets, the degree of fracturing diminishes. A similar trend is observed in\nrobust vision models, where increased robustness correlates with reduced\nfracturing. Ultimately, this approach enables a direct interpretation and\ncomparison of the latent spaces of different vision models and reveals a\nrelationship between a model's generalizability and robustness. Results show\nthat as a model becomes more general and robust, it tends to learn features\nthat result in better clustering of concepts. Project Website is available\nonline at https://shashankkotyan.github.io/k-Distribution/",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09065v1",
    "published_date": "2024-08-17 01:43:51 UTC",
    "updated_date": "2024-08-17 01:43:51 UTC"
  },
  {
    "arxiv_id": "2408.12665v1",
    "title": "Fairness-Aware Streaming Feature Selection with Causal Graphs",
    "authors": [
      "Leizhen Zhang",
      "Lusi Li",
      "Di Wu",
      "Sheng Chen",
      "Yi He"
    ],
    "abstract": "Its crux lies in the optimization of a tradeoff between accuracy and fairness\nof resultant models on the selected feature subset. The technical challenge of\nour setting is twofold: 1) streaming feature inputs, such that an informative\nfeature may become obsolete or redundant for prediction if its information has\nbeen covered by other similar features that arrived prior to it, and 2)\nnon-associational feature correlation, such that bias may be leaked from those\nseemingly admissible, non-protected features. To overcome this, we propose\nStreaming Feature Selection with Causal Fairness (SFCF) that builds two causal\ngraphs egocentric to prediction label and protected feature, respectively,\nstriving to model the complex correlation structure among streaming features,\nlabels, and protected information. As such, bias can be eradicated from\npredictive modeling by removing those features being causally correlated with\nthe protected feature yet independent to the labels. We theorize that the\noriginally redundant features for prediction can later become admissible, when\nthe learning accuracy is compromised by the large number of removed features\n(non-protected but can be used to reconstruct bias information). We benchmark\nSFCF\\ on five datasets widely used in streaming feature research, and the\nresults substantiate its performance superiority over six rival models in terms\nof efficiency and sparsity of feature selection and equalized odds of the\nresultant predictive models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper has been accepted by the 2024 IEEE International\n  Conference on Systems, Man, and Cybernetics (SMC 2024)",
    "pdf_url": "http://arxiv.org/pdf/2408.12665v1",
    "published_date": "2024-08-17 00:41:02 UTC",
    "updated_date": "2024-08-17 00:41:02 UTC"
  }
]
{
  "date": "2025-08-16",
  "category": "cs.AI",
  "summary": "ä½ å¥½ï¼æ¬¢è¿æ¥åˆ° **UTC æ—¶é—´ 2025-08-16** çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\næˆ‘æ˜¯ä½ çš„è€æœ‹å‹ï¼ŒGemini Enterpriseã€‚ä»Šå¤© arXiv æ¶Œç°äº†å¤§é‡é«˜è´¨é‡å·¥ä½œï¼Œ**åŒ»ç–—åŸºç¡€å¤§æ¨¡å‹è¿æ¥ Scaling Law æ—¶åˆ»ï¼Œå¤åˆ» OpenAI o3 çš„è§†è§‰æ¨ç†å°è¯•å¼•äººæ³¨ç›®ï¼ŒåŒæ—¶ç®€å•çš„ PDF æç¤ºæ³¨å…¥ç«Ÿç„¶èƒ½è½»æ¾éª—è¿‡ LLM**ã€‚æ­¤å¤–ï¼Œè¿˜æœ‰å…³äº AI åœ¨å›½é™…è±¡æ£‹ä¸­è¡¨ç°å‡ºçš„â€œæˆ˜ç•¥å¼ åŠ›â€çš„æœ‰è¶£ç ”ç©¶ã€‚\n\nè®©æˆ‘ä»¬å¼€å§‹ä»Šå¤©çš„æ·±åº¦é˜…è¯»ã€‚\n\n---\n\n### ğŸš€ ç„¦ç‚¹å¤´æ¡ï¼šåŒ»ç–—å¤§æ¨¡å‹ã€o3 å¤åˆ»ä¸å®‰å…¨æ¼æ´\n\n**1. Generative Medical Event Models Improve with Scale**\n**éšç€è§„æ¨¡æ‰©å¤§è€Œæ”¹è¿›çš„ç”Ÿæˆå¼åŒ»ç–—äº‹ä»¶æ¨¡å‹**\n> **Keywords:** Medical Foundation Model, Scaling Laws, Patient Journey, Curiosity\n>\n> è¿™æ˜¯ä¸€ç¯‡é‡ç£…å·¥ä½œã€‚ä½œè€…ï¼ˆæ¥è‡ª Epic Cosmos ç­‰æœºæ„ï¼‰è®­ç»ƒäº†ä¸€ä¸ªåä¸º **Curiosity** çš„æ¨¡å‹å®¶æ—ï¼Œä½¿ç”¨äº† **1.18 äº¿æ‚£è€…** çš„ **1150 äº¿ä¸ªåŒ»ç–—äº‹ä»¶**ã€‚\n> *   **æ ¸å¿ƒå‘ç°**ï¼šè¿™æ˜¯ç›®å‰åŒ»ç–—äº‹ä»¶æ•°æ®é¢†åŸŸ**æœ€å¤§è§„æ¨¡çš„ Scaling Law ç ”ç©¶**ã€‚ç ”ç©¶å‘ç°ï¼Œè®¡ç®—é‡ã€Token æ•°å’Œæ¨¡å‹å¤§å°ä¸é¢„æµ‹èƒ½åŠ›ä¹‹é—´å­˜åœ¨å¹‚å¾‹å…³ç³»ã€‚\n> *   **è´¡çŒ®**ï¼šCuriosity æ¨¡å‹åœ¨ 78 ä¸ªçœŸå®ä¸–ç•Œä»»åŠ¡ï¼ˆå¦‚è¯Šæ–­é¢„æµ‹ã€ç–¾ç—…é¢„åï¼‰ä¸Šï¼Œä»…å‡­é¢„è®­ç»ƒï¼ˆæ— éœ€å¾®è°ƒï¼‰å°±è¶…è¶Šæˆ–åŒ¹é…äº†ç‰¹å®šä»»åŠ¡çš„ç›‘ç£æ¨¡å‹ã€‚è¿™æ ‡å¿—ç€åŒ»ç–— AI æ­£ä»â€œä¸“ç”¨å°æ¨¡å‹â€è¿ˆå‘â€œé€šç”¨åŸºç¡€æ¨¡å‹â€æ—¶ä»£ã€‚\n\n**2. Simple o3: Towards Interleaved Vision-Language Reasoning**\n**Simple o3ï¼šè¿ˆå‘äº¤é”™çš„è§†è§‰-è¯­è¨€æ¨ç†**\n> **Keywords:** Multimodal Reasoning, OpenAI o3, Chain-of-Thought, Tool Use\n>\n> å— OpenAI o3 çš„å¯å‘ï¼Œè¿™å°±ä¸ä»…ä»…æ˜¯å¤šæ¨¡æ€å¯¹è¯äº†ï¼Œè€Œæ˜¯æ¨¡æ‹Ÿäººç±»â€œç”¨å›¾åƒæ€è€ƒâ€çš„è¿‡ç¨‹ã€‚\n> *   **æ ¸å¿ƒæ–¹æ³•**ï¼šæå‡ºäº†ä¸€ä¸ªâ€œ**è§‚å¯Ÿ-æ¨ç†-è¡ŒåŠ¨**â€çš„å¾ªç¯æ¡†æ¶ã€‚æ¨¡å‹å¯ä»¥åŠ¨æ€è°ƒç”¨å·¥å…·ï¼ˆè£å‰ªã€ç¼©æ”¾ã€é‡ç”¨å›¾ç‰‡ï¼‰æ¥è¿›è¡Œè§†è§‰å˜æ¢ï¼Œå¹¶ç»“åˆè¯­è¨€æ¨ç†ã€‚\n> *   **å‘ç°**ï¼šé€šè¿‡å¼•å…¥é¢å¤–çš„è§†è§‰ Token å’Œé‡ç”¨/æ”¾å¤§å›¾åƒï¼Œæ¨¡å‹åœ¨ç»†ç²’åº¦æ„ŸçŸ¥å’Œè§†è§‰æ¨ç†ä¸Šæ˜¾è‘—å˜å¼ºã€‚ä½œè€…è¿˜å¼€æºäº† TWI-Tools-146K æ•°æ®é›†ã€‚\n\n**3. Too Easily Fooled? Prompt Injection Breaks LLMs on Frustratingly Simple Multiple-Choice Questions**\n**å¤ªå¥½éª—äº†ï¼Ÿç®€å•çš„å¤šé€‰é¢˜æç¤ºæ³¨å…¥å³å¯æ”»ç ´ LLM**\n> **Keywords:** Prompt Injection, LLM Robustness, PDF Attacks\n>\n> è¿™æ˜¯ä¸€ä¸ªä»¤äººæ²®ä¸§ä½†æå…¶é‡è¦çš„å‘ç°ã€‚æˆ‘ä»¬é€šå¸¸è®¤ä¸º LLM åœ¨åšç®€å•ç®—æœ¯ï¼ˆå¦‚ 3+2=ï¼Ÿï¼‰æ—¶å¾ˆç¨³ã€‚\n> *   **æ ¸å¿ƒå‘ç°**ï¼šä½œè€…åœ¨ **PDF æ–‡ä»¶** ä¸­éšè—äº†æ¶æ„çš„ Prompt æ³¨å…¥æŒ‡ä»¤ã€‚ç»“æœæ˜¾ç¤ºï¼Œå³ä¾¿æ˜¯é¢å¯¹æœ€åŸºç¡€çš„ç®—æœ¯é—®é¢˜æˆ–åˆ¤æ–­é¢˜ï¼Œåªè¦å—åˆ°è¿™ç§éšè—æ³¨å…¥çš„å¹²æ‰°ï¼ŒLLM å°±ä¼šäº§ç”Ÿé”™è¯¯è¾“å‡ºã€‚\n> *   **è­¦ç¤º**ï¼šè¿™ç»™ç›®å‰å¤§ç«çš„ \"LLM-as-a-judge\"ï¼ˆç”¨ LLM å½“è£åˆ¤/é˜…å·äººï¼‰åº”ç”¨æ•²å“äº†è­¦é’Ÿï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†ç”¨æˆ·ä¸Šä¼ æ–‡æ¡£çš„åœºæ™¯ä¸‹ã€‚\n\n**4. AI sustains higher strategic tension than humans in chess**\n**AI åœ¨å›½é™…è±¡æ£‹ä¸­æ¯”äººç±»ç»´æŒæ›´é«˜çš„æˆ˜ç•¥å¼ åŠ›**\n> **Keywords:** Strategic Tension, Chess, Human vs AI, Complex Systems\n>\n> è¿™æ˜¯ä¸€ç¯‡éå¸¸æœ‰æ„æ€çš„è·¨å­¦ç§‘ç ”ç©¶ã€‚ä½œè€…ä¸å…³å¿ƒè°èµ¢äº†ï¼Œè€Œæ˜¯å…³å¿ƒ**æ£‹å±€çš„â€œå¼ åŠ›â€**ï¼ˆPiece-to-piece interactionï¼‰ã€‚\n> *   **æ ¸å¿ƒå‘ç°**ï¼šé¡¶çº§ AI èƒ½å¤Ÿåœ¨æ›´é•¿æ—¶é—´å†…ç»´æŒæé«˜çš„â€œæˆ˜ç•¥å¼ åŠ›â€â€”â€”å³ä¸€ç§æ”»å®ˆå…¼å¤‡ã€ç‰µä¸€å‘è€ŒåŠ¨å…¨èº«çš„å¤æ‚å±€é¢ã€‚ç›¸åï¼Œå³ä½¿æ˜¯ç²¾è‹±äººç±»æ£‹æ‰‹ï¼Œä¹Ÿä¼šå€¾å‘äºé€šè¿‡ç®€åŒ–å±€åŠ¿æ¥é™ä½è®¤çŸ¥è´Ÿè·ã€‚\n> *   **Implication**ï¼šè¿™æ­ç¤ºäº†äººç±»è®¤çŸ¥å±€é™æ€§ä¸ AI çº¯ç²¹è®¡ç®—é€»è¾‘çš„æ ¹æœ¬å·®å¼‚ï¼šAI æ›´èƒ½å¿å—â€œæ··ä¹±â€å’Œâ€œä¸ç¡®å®šæ€§â€ã€‚\n\n---\n\n### ğŸ§  LLM è®­ç»ƒã€å¾®è°ƒä¸æ•ˆç‡\n\n**5. Efficient Modular Learning through Naive LoRA Summation: Leveraging Orthogonality in High-Dimensional Models**\n**é€šè¿‡æœ´ç´  LoRA æ±‚å’Œå®ç°é«˜æ•ˆæ¨¡å—åŒ–å­¦ä¹ ï¼šåˆ©ç”¨é«˜ç»´æ¨¡å‹çš„æ­£äº¤æ€§**\n> **Keywords:** LoRA, Modular Learning, Model Merging\n>\n> å¦‚æœæˆ‘æƒ³è®©æ¨¡å‹æ‡‚æ•°å­¦ï¼Œåˆæ‡‚åŒ»å­¦ï¼Œèƒ½ä¸èƒ½æŠŠä¸¤ä¸ª LoRA ç›´æ¥åŠ èµ·æ¥ï¼Ÿ\n> *   **æ ¸å¿ƒå‘ç°**ï¼šä½œè€…å‘ç°ï¼Œåœ¨ä¸åŒé¢†åŸŸï¼ˆå¦‚æ•°å­¦å’ŒåŒ»å­¦ï¼‰ç‹¬ç«‹è®­ç»ƒçš„ LoRA æ¨¡å—ï¼Œå…¶å‚æ•°æ›´æ–°åœ¨å‘é‡ç©ºé—´ä¸Šè¿‘ä¼¼**æ­£äº¤**ã€‚\n> *   **æ–¹æ³•**ï¼šç›´æ¥æŠŠä¸¤ä¸ª LoRA çš„å‚æ•°**ç›¸åŠ **ï¼ˆNaive Summationï¼‰ï¼Œä¸éœ€è¦é¢å¤–è®­ç»ƒï¼Œæ•ˆæœç«Ÿç„¶æ¯”å¾®è°ƒæ··åˆæ•°æ®è¿˜è¦å¥½æˆ–è€…ç›¸å½“ã€‚è¿™ä¸ºæ¨¡å—åŒ– LLM éƒ¨ç½²æä¾›äº†æç®€æ–¹æ¡ˆã€‚\n\n**6. DynamixSFT: Dynamic Mixture Optimization of Instruction Tuning Collections**\n**DynamixSFTï¼šæŒ‡ä»¤å¾®è°ƒæ•°æ®é›†çš„åŠ¨æ€æ··åˆä¼˜åŒ–**\n> **Keywords:** Instruction Tuning, Data Mixture, Multi-armed Bandit\n>\n> SFT é˜¶æ®µæœ‰å‡ åä¸ªæ•°æ®é›†ï¼Œæ¯ä¸ªè¯¥ç”¨å¤šå°‘æ¯”ä¾‹ï¼Ÿ\n> *   **æ–¹æ³•**ï¼šå°†æ•°æ®æ··åˆé—®é¢˜å»ºæ¨¡ä¸º**å¤šè‡‚è€è™æœºï¼ˆMulti-armed Banditï¼‰**é—®é¢˜ã€‚æå‡ºäº†ä¸€ç§ Prior-scaled Boltzmann Exploration ç­–ç•¥ï¼ŒåŠ¨æ€è°ƒæ•´ä¸åŒæ•°æ®é›†çš„é‡‡æ ·æ¦‚ç‡ã€‚\n> *   **æ•ˆæœ**ï¼šåœ¨ Tulu-v2 æ··åˆæ•°æ®é›†ä¸Šï¼Œæ¯”é™æ€æ··åˆæå‡äº† 2.2% çš„æ€§èƒ½ã€‚\n\n**7. SupraTok: Cross-Boundary Tokenization for Enhanced Language Model Performance**\n**SupraTokï¼šç”¨äºå¢å¼ºè¯­è¨€æ¨¡å‹æ€§èƒ½çš„è·¨è¾¹ç•Œåˆ†è¯æŠ€æœ¯**\n> **Keywords:** Tokenization, BPE, Superword\n>\n> Tokenization æ˜¯è¢«ä½ä¼°çš„ç“¶é¢ˆã€‚\n> *   **æ ¸å¿ƒè´¡çŒ®**ï¼šSupraTok å­¦ä¹ â€œè¶…è¯ï¼ˆSuperwordï¼‰â€å•å…ƒï¼Œå³è·¨è¶Šå•è¯è¾¹ç•Œçš„è¯­ä¹‰å•å…ƒã€‚\n> *   **æ•ˆæœ**ï¼šæ¯” OpenAI çš„ o200k åˆ†è¯å™¨æ•ˆç‡æå‡ 31%ï¼Œåœ¨ GPT-2 è§„æ¨¡å®éªŒä¸­ï¼Œä»…é æ”¹ Tokenizer å°±åœ¨ HellaSWAG ä¸Šæå‡äº† 8.4%ã€‚\n\n---\n\n### ğŸ¤– Agentï¼ˆæ™ºèƒ½ä½“ï¼‰ä¸æ¨ç†\n\n**8. FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction**\n**FutureXï¼šé¢å‘ LLM æ™ºèƒ½ä½“æœªæ¥é¢„æµ‹ä»»åŠ¡çš„é«˜çº§å®æ—¶åŸºå‡†æµ‹è¯•**\n> **Keywords:** Future Prediction, Benchmark, Live Evaluation\n>\n> AI èƒ½åƒåˆ†æå¸ˆä¸€æ ·é¢„æµ‹æœªæ¥å—ï¼Ÿ\n> *   **è´¡çŒ®**ï¼šå»ºç«‹äº†ä¸€ä¸ªå¤§è§„æ¨¡ã€åŠ¨æ€æ›´æ–°çš„åŸºå‡†æµ‹è¯• **FutureX**ï¼Œä¸“é—¨è¯„ä¼° Agent æ”¶é›†ä¿¡æ¯ã€æƒè¡¡ä¸ç¡®å®šæ€§å’Œé¢„æµ‹æœªæ¥çš„èƒ½åŠ›ã€‚é¿å…äº†â€œè®­ç»ƒæ•°æ®æ³„éœ²â€ï¼ˆå› ä¸ºæ˜¯é¢„æµ‹æœªæ¥ï¼‰ã€‚\n> *   **ç°çŠ¶**ï¼šè¯„ä¼°äº† 25 ä¸ªæ¨¡å‹ï¼Œå‘ç° Agent ä»ç„¶å®¹æ˜“è¢«è™šå‡ç½‘é¡µè¯¯å¯¼ï¼Œä¸”æ—¶æ•ˆæ€§å¤„ç†ä»æ˜¯éš¾ç‚¹ã€‚\n\n**9. AgentCDM: Enhancing Multi-Agent Collaborative Decision-Making via ACH-Inspired Structured Reasoning**\n**AgentCDMï¼šé€šè¿‡å— ACH å¯å‘çš„ç»“æ„åŒ–æ¨ç†å¢å¼ºå¤šæ™ºèƒ½ä½“åä½œå†³ç­–**\n> **Keywords:** Multi-Agent, Collaborative Decision Making, Cognitive Bias\n>\n> å¤šæ™ºèƒ½ä½“ä¸ä»…ä»…æ˜¯â€œæŠ•ç¥¨â€ï¼Œè€Œæ˜¯éœ€è¦ç§‘å­¦çš„äº‰è®ºã€‚\n> *   **æ–¹æ³•**ï¼šå€Ÿé‰´è®¤çŸ¥ç§‘å­¦ä¸­çš„**ç«äº‰å‡è®¾åˆ†æï¼ˆACHï¼‰**ã€‚ä¸æ˜¯è¢«åŠ¨é€‰æ‹©ç­”æ¡ˆï¼Œè€Œæ˜¯ä¸»åŠ¨æ„å»ºå‡è®¾ã€è¯„ä¼°è¯æ®çŸ©é˜µã€‚\n> *   **è´¡çŒ®**ï¼šè¿™ç§ç»“æ„åŒ–çš„æ¨ç†èŒƒå¼æ˜¾è‘—å‡å°‘äº†å•æ™ºèƒ½ä½“çš„è®¤çŸ¥åå·®ï¼Œæå‡äº†ç¾¤ä½“å†³ç­–è´¨é‡ã€‚\n\n**10. Discovering Expert-Level Nash Equilibrium Algorithms with Large Language Models**\n**åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹å‘ç°ä¸“å®¶çº§çº³ä»€å‡è¡¡ç®—æ³•**\n> **Keywords:** Algorithm Discovery, Nash Equilibrium, LLM for Math\n>\n> è¿™æ˜¯ä¸€ä¸ª AI è¾…åŠ©ç†è®ºè®¡ç®—æœºç§‘å­¦çš„æ¡ˆä¾‹ã€‚\n> *   **æ ¸å¿ƒ**ï¼šæå‡ºäº† **LegoNE** æ¡†æ¶ï¼Œè®© LLM å°†ç®—æ³•è®¾è®¡è½¬åŒ–ä¸ºçº¦æŸä¼˜åŒ–é—®é¢˜ã€‚\n> *   **æˆå°±**ï¼šLLM é‡æ–°å‘ç°äº†åŒäººåšå¼ˆçš„æœ€ä¼˜ç®—æ³•ï¼ˆäººç±»èŠ±äº†15å¹´æ‰æ‰¾åˆ°ï¼‰ï¼Œå¹¶ä¸ºä¸‰äººåšå¼ˆå‘ç°äº†ä¸€ç§è¶…è¶Šç°æœ‰äººç±»è®¾è®¡çš„æ–°ç®—æ³•ã€‚\n\n---\n\n### ğŸŒ ç‰©ç†ä¸–ç•Œï¼šæœºå™¨äººä¸ç§‘å­¦\n\n**11. Scalable RF Simulation in Generative 4D Worlds**\n**ç”Ÿæˆå¼ 4D ä¸–ç•Œä¸­çš„å¯æ‰©å±•å°„é¢‘ä»¿çœŸ**\n> **Keywords:** RF Sensing, Simulation, WaveVerse, 4D World\n>\n> å°„é¢‘ï¼ˆRFï¼‰æ„ŸçŸ¥å¯ä»¥ç©¿å¢™é€è§†ï¼Œä¿æŠ¤éšç§ï¼Œä½†æ•°æ®éš¾æã€‚\n> *   **æ–¹æ³•**ï¼šæå‡ºäº† **WaveVerse**ï¼Œåˆ©ç”¨è¯­è¨€å¼•å¯¼ç”Ÿæˆ 4D å®¤å†…åœºæ™¯å’Œäººä½“è¿åŠ¨ï¼Œç„¶åé€šè¿‡å…‰çº¿è¿½è¸ªæ¨¡æ‹ŸçœŸå®çš„ RF ä¿¡å·ã€‚\n> *   **æ„ä¹‰**ï¼šé¦–æ¬¡å®ç°äº†å¤§è§„æ¨¡ RF æˆåƒæ•°æ®çš„ç”Ÿæˆï¼Œè§£å†³äº†è¯¥é¢†åŸŸæå…¶ç¼ºæ•°æ®çš„é—®é¢˜ã€‚\n\n**12. LocoMamba: Vision-Driven Locomotion via End-to-End Deep Reinforcement Learning with Mamba**\n**LocoMambaï¼šåŸºäº Mamba ç«¯åˆ°ç«¯æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„è§†è§‰é©±åŠ¨è¿åŠ¨æ§åˆ¶**\n> **Keywords:** Robotics, Mamba, SSM, Locomotion\n>\n> Transformer åœ¨æœºå™¨äººæ§åˆ¶ä¸­å¤ªé‡äº†ï¼ŒMamba (SSM) æ¥äº†ã€‚\n> *   **æ ¸å¿ƒ**ï¼šåˆ©ç”¨ Mamba çš„**çº¿æ€§æ—¶é—´åºåˆ—å»ºæ¨¡**èƒ½åŠ›å¤„ç†è§†è§‰å’Œæœ¬ä½“æ„Ÿè§‰æ•°æ®ã€‚\n> *   **æ•ˆæœ**ï¼šåœ¨å¤æ‚åœ°å½¢çš„æœºå™¨äººè¿åŠ¨æ§åˆ¶ä¸­ï¼Œæ¯” Transformer å’Œ CNN åŸºçº¿æ”¶æ•›æ›´å¿«ï¼Œæ¨ç†å»¶è¿Ÿæ›´ä½ï¼Œç¢°æ’æ›´å°‘ã€‚\n\n---\n\n### ğŸ¨ è§†è§‰ä¸å¤šæ¨¡æ€å…¶ä»–ç²¾é€‰\n\n*   **[No.10] Demystifying Foreground-Background Memorization in Diffusion Models**\n    **æ­ç§˜æ‰©æ•£æ¨¡å‹ä¸­çš„å‰æ™¯-èƒŒæ™¯è®°å¿†åŒ–**\n    > æ‰©æ•£æ¨¡å‹ï¼ˆå¦‚ Stable Diffusionï¼‰ä¼šâ€œæ­»è®°ç¡¬èƒŒâ€è®­ç»ƒå›¾ã€‚æœ¬æ–‡æå‡º FB-Mem æŒ‡æ ‡ï¼Œå‘ç°è®°å¿†åŒ–æ¯”æˆ‘ä»¬æƒ³è±¡çš„æ›´æ™®éï¼Œä¸”ç°æœ‰çš„å»ç¥ç»å…ƒæ–¹æ³•æ— æ³•æ¶ˆé™¤è¿™ç§å±€éƒ¨è®°å¿†ã€‚\n\n*   **[No.25] VimoRAG: Video-based Retrieval-augmented 3D Motion Generation for Motion Language Models**\n    **VimoRAGï¼šç”¨äºåŠ¨ä½œè¯­è¨€æ¨¡å‹çš„åŸºäºè§†é¢‘æ£€ç´¢å¢å¼ºçš„ 3D åŠ¨ä½œç”Ÿæˆ**\n    > åŠ¨ä½œæ•°æ®å¾ˆå°‘ï¼Œä½†è§†é¢‘å¾ˆå¤šã€‚è¯¥æ–¹æ³•é€šè¿‡æ£€ç´¢ç°å®ä¸–ç•Œçš„è§†é¢‘æ¥å¢å¼º 3D åŠ¨ä½œç”Ÿæˆï¼Œè§£å†³äº†åŠ¨ä½œå¤§æ¨¡å‹æ•°æ®åŒ®ä¹çš„é—®é¢˜ã€‚\n\n*   **[No.64] Singing Syllabi with Virtual Avatars**\n    **ç”¨è™šæ‹ŸåŒ–èº«å”±å‡ºæ•™å­¦å¤§çº²**\n    > è¿™ä¸ªæœ‰ç‚¹å¯çˆ±ã€‚æŠŠæ¯ç‡¥çš„æ•™å­¦å¤§çº²ï¼ˆSyllabusï¼‰å˜æˆæ­Œï¼Œè®© AI è™šæ‹Ÿäººå”±ç»™å­¦ç”Ÿå¬ã€‚å®éªŒè¯æ˜ï¼Œå­¦ç”ŸçœŸçš„æ›´èƒ½åœ¨è¿™ç§å½¢å¼ä¸‹è®°ä½è¯¾ç¨‹æ”¿ç­–ã€‚\n\n---\n\n**Gemini Enterprise ç»“è¯­**ï¼š\nä»Šå¤©çš„ arXiv å±•ç°äº† AI å‘å±•çš„ä¸¤ä¸ªæç«¯ï¼šä¸€æ–¹é¢æ˜¯**Curiosity** å’Œ **FutureX** è¿™æ ·å®å¤§çš„é€šç”¨èƒ½åŠ›æ„å»ºï¼›å¦ä¸€æ–¹é¢æ˜¯ **SupraTok** å’Œ **LoRA Summation** è¿™æ ·æè‡´çš„åº•å±‚æ•ˆç‡ä¼˜åŒ–ã€‚åŒæ—¶ï¼Œ**Prompt Injection** çš„æ–°å˜ç§å†æ¬¡æé†’æˆ‘ä»¬ï¼Œæ¨¡å‹çš„â€œèªæ˜â€å¯èƒ½æå…¶è„†å¼±ã€‚\n\nå¸Œæœ›è¿™ä»½å¿«æŠ¥å¯¹ä½ çš„ç ”ç©¶æœ‰æ‰€å¯å‘ï¼æ˜å¤©è§ï¼",
  "papers": [
    {
      "arxiv_id": "2508.13214v1",
      "title": "Too Easily Fooled? Prompt Injection Breaks LLMs on Frustratingly Simple Multiple-Choice Questions",
      "title_zh": "å¦‚æ­¤å®¹æ˜“å—éª—ï¼Ÿæç¤ºæ³¨å…¥åœ¨æç®€é€‰æ‹©é¢˜ä¸­æ”»ç ´å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Xuyang Guo",
        "Zekai Huang",
        "Zhao Song",
        "Jiahao Zhang"
      ],
      "abstract": "Large Language Models (LLMs) have recently demonstrated strong emergent abilities in complex reasoning and zero-shot generalization, showing unprecedented potential for LLM-as-a-judge applications in education, peer review, and data quality evaluation. However, their robustness under prompt injection attacks, where malicious instructions are embedded into the content to manipulate outputs, remains a significant concern. In this work, we explore a frustratingly simple yet effective attack setting to test whether LLMs can be easily misled. Specifically, we evaluate LLMs on basic arithmetic questions (e.g., \"What is 3 + 2?\") presented as either multiple-choice or true-false judgment problems within PDF files, where hidden prompts are injected into the file. Our results reveal that LLMs are indeed vulnerable to such hidden prompt injection attacks, even in these trivial scenarios, highlighting serious robustness risks for LLM-as-a-judge applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨é¢å¯¹æç¤ºæ³¨å…¥(prompt injection)æ”»å‡»æ—¶çš„é²æ£’æ€§ï¼Œé‡ç‚¹å…³æ³¨å…¶ä½œä¸ºè¯„åˆ¤è€…(LLM-as-a-judge)åœ¨æ•™è‚²ã€åŒè¡Œè¯„å®¡å’Œæ•°æ®è¯„ä¼°é¢†åŸŸçš„åº”ç”¨æ½œåŠ›ã€‚ä½œè€…æå‡ºäº†ä¸€ç§æç®€ä½†æœ‰æ•ˆçš„æ”»å‡»è®¾å®šï¼Œé€šè¿‡åœ¨åŒ…å«åŸºç¡€ç®—æœ¯é—®é¢˜çš„PDFæ–‡ä»¶ä¸­åµŒå…¥éšè—æ¶æ„æŒ‡ä»¤ï¼Œè¯„ä¼°æ¨¡å‹åœ¨å¤„ç†å¤šé€‰é¢˜æˆ–æ˜¯éé¢˜æ—¶çš„è¡¨ç°ã€‚å®éªŒå‘ç°ï¼Œå³ä¾¿æ˜¯åœ¨è¯¸å¦‚â€œ3+2ç­‰äºå‡ â€è¿™ç±»çç¢çš„è®¡ç®—åœºæ™¯ä¸‹ï¼ŒLLMsä¹Ÿææ˜“å—åˆ°éšè—æç¤ºæ³¨å…¥çš„è¯¯å¯¼è€Œåšå‡ºé”™è¯¯è¾“å‡ºã€‚è¿™ä¸€ç»“æœæ­ç¤ºäº†LLMsåœ¨å±•ç°å¼ºå¤§é›¶æ¬¡å­¦ä¹ (zero-shot)æ³›åŒ–èƒ½åŠ›çš„åŒæ—¶ï¼Œä¾ç„¶é¢ä¸´ä¸¥é‡çš„é²æ£’æ€§æŒ‘æˆ˜ï¼Œä¸ºæœªæ¥LLM-as-a-judgeåº”ç”¨éƒ¨ç½²ä¸­çš„å®‰å…¨é£é™©æä¾›äº†é‡è¦è­¦ç¤ºã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13214v1",
      "published_date": "2025-08-16 23:30:08 UTC",
      "updated_date": "2025-08-16 23:30:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:12:31.665550+00:00"
    },
    {
      "arxiv_id": "2508.12176v1",
      "title": "Scalable RF Simulation in Generative 4D Worlds",
      "title_zh": "ç”Ÿæˆå¼ 4D ä¸–ç•Œä¸­çš„å¯æ‰©å±•å°„é¢‘ä»¿çœŸ",
      "authors": [
        "Zhiwei Zheng",
        "Dongyin Hu",
        "Mingmin Zhao"
      ],
      "abstract": "Radio Frequency (RF) sensing has emerged as a powerful, privacy-preserving alternative to vision-based methods for indoor perception tasks. However, collecting high-quality RF data in dynamic and diverse indoor environments remains a major challenge. To address this, we introduce WaveVerse, a prompt-based, scalable framework that simulates realistic RF signals from generated indoor scenes with human motions. WaveVerse introduces a language-guided 4D world generator, which includes a state-aware causal transformer for human motion generation conditioned on spatial constraints and texts, and a phase-coherent ray tracing simulator that enables the simulation of accurate and coherent RF signals. Experiments demonstrate the effectiveness of our approach in conditioned human motion generation and highlight how phase coherence is applied to beamforming and respiration monitoring. We further present two case studies in ML-based high-resolution imaging and human activity recognition, demonstrating that WaveVerse not only enables data generation for RF imaging for the first time, but also consistently achieves performance gain in both data-limited and data-adequate scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŠ¨æ€å®¤å†…ç¯å¢ƒä¸­é«˜è´¨é‡å°„é¢‘(Radio Frequency, RF)æ„ŸçŸ¥æ•°æ®é‡‡é›†éš¾çš„é—®é¢˜ï¼Œæå‡ºäº†WaveVerseæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæç¤ºè¯é©±åŠ¨çš„å¯æ‰©å±•æ¨¡æ‹Ÿç³»ç»Ÿã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒåŒ…å«ä¸€ä¸ªè¯­è¨€å¼•å¯¼çš„4Dä¸–ç•Œç”Ÿæˆå™¨ï¼Œåˆ©ç”¨çŠ¶æ€æ„ŸçŸ¥å› æœTransformer(state-aware causal transformer)æ ¹æ®ç©ºé—´çº¦æŸå’Œæ–‡æœ¬ç”Ÿæˆäººä½“åŠ¨ä½œã€‚åŒæ—¶ï¼ŒWaveVerseé›†æˆäº†ä¸€ä¸ªç›¸ä½ä¸€è‡´çš„å°„çº¿è¿½è¸ªæ¨¡æ‹Ÿå™¨(phase-coherent ray tracing simulator)ï¼Œèƒ½å¤Ÿæ¨¡æ‹Ÿç”Ÿæˆç²¾ç¡®ä¸”ç›¸å¹²çš„RFä¿¡å·ã€‚å®éªŒç»“æœä¸ä»…éªŒè¯äº†å…¶åœ¨å—é™äººä½“åŠ¨ä½œç”Ÿæˆæ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œè¿˜å±•ç¤ºäº†ç›¸ä½ä¸€è‡´æ€§åœ¨æ³¢æŸèµ‹å½¢(beamforming)å’Œå‘¼å¸ç›‘æµ‹ä¸­çš„å…³é”®åº”ç”¨ã€‚é€šè¿‡å°„é¢‘æˆåƒ(RF imaging)å’Œäººä½“æ´»åŠ¨è¯†åˆ«(Human Activity Recognition)çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œè¯¥å·¥ä½œé¦–æ¬¡å®ç°äº†é’ˆå¯¹RFæˆåƒçš„æ•°æ®ç”Ÿæˆèƒ½åŠ›ã€‚æœ€ç»ˆç ”ç©¶è¡¨æ˜ï¼Œæ— è®ºåœ¨æ•°æ®å—é™è¿˜æ˜¯å……è¶³çš„æƒ…å¢ƒä¸‹ï¼ŒWaveVerseéƒ½èƒ½æ˜¾è‘—ä¸”æŒç»­åœ°æå‡ç›¸å…³æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½è¡¨ç°ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.12176v1",
      "published_date": "2025-08-16 23:02:14 UTC",
      "updated_date": "2025-08-16 23:02:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:12:35.665199+00:00"
    },
    {
      "arxiv_id": "2508.13213v1",
      "title": "AI sustains higher strategic tension than humans in chess",
      "title_zh": "äººå·¥æ™ºèƒ½åœ¨å›½é™…è±¡æ£‹ä¸­æ¯”äººç±»ç»´æŒæ›´é«˜çš„æˆ˜ç•¥å¼ åŠ›",
      "authors": [
        "Adamo Cerioli",
        "Edward D. Lee",
        "Vito D. P. Servedio"
      ],
      "abstract": "Strategic decision-making involves managing the tension between immediate opportunities and long-term objectives. We study this trade-off in chess by characterizing and comparing dynamics between human vs human and AI vs AI games. We propose a network-based metric of piece-to-piece interaction to quantify the ongoing strategic tension on the board. Its evolution in games reveals that the most competitive AI players sustain higher levels of strategic tension for longer durations than elite human players. Cumulative tension varies with algorithmic complexity for AI and correspondingly in human-played games increases abruptly with expertise at about 1600 Elo and again at 2300 Elo. The profiles reveal different approaches. Highly competitive AI tolerates interconnected positions balanced between offensive and defensive tactics over long periods. Human play, in contrast, limits tension and game complexity, which may reflect cognitive limitations and adaptive strategies. The difference may have implications for AI usage in complex, strategic environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡å¯¹æ¯”äººç±»ä¸äººå·¥æ™ºèƒ½(AI)åœ¨å›½é™…è±¡æ£‹å¯¹å±€ä¸­çš„è¡¨ç°ï¼Œæ¢è®¨äº†æˆ˜ç•¥å†³ç­–ä¸­å³æ—¶æœºä¼šä¸é•¿æœŸç›®æ ‡ä¹‹é—´çš„æƒè¡¡ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åŸºäºç½‘ç»œçš„æ£‹å­é—´ç›¸äº’ä½œç”¨æŒ‡æ ‡ï¼Œç”¨äºé‡åŒ–æ£‹ç›˜ä¸ŠæŒç»­å­˜åœ¨çš„ strategic tensionã€‚ç ”ç©¶å‘ç°ï¼Œæœ€å…·ç«äº‰åŠ›çš„ AI ç©å®¶èƒ½å¤Ÿæ¯”é¡¶çº§äººç±»æ£‹æ‰‹åœ¨æ›´é•¿æ—¶é—´å†…ç»´æŒæ›´é«˜æ°´å¹³çš„ strategic tensionã€‚AI çš„ç´¯è®¡ç´§å¼ åº¦éšç®—æ³•å¤æ‚åº¦çš„å¢åŠ è€Œå˜åŒ–ï¼Œè€Œäººç±»ç©å®¶çš„ç´§å¼ åº¦åˆ™åœ¨ 1600 Elo å’Œ 2300 Elo ä¸¤ä¸ªä¸“ä¸šæ°´å¹³èŠ‚ç‚¹å‡ºç°æ˜¾è‘—æå‡ã€‚é«˜æ°´å¹³ AI å±•ç°å‡ºåœ¨é•¿æ—¶é—´å†…ç»´æŒè¿›æ”»ä¸é˜²å®ˆæˆ˜æœ¯é«˜åº¦äº’è¿ä¸”å¹³è¡¡å±€åŠ¿çš„èƒ½åŠ›ï¼Œè€Œäººç±»åˆ™å€¾å‘äºé™åˆ¶å±€åŠ¿å¤æ‚åº¦ä»¥åº”å¯¹è®¤çŸ¥å±€é™ã€‚è¿™ç§å·®å¼‚æ­ç¤ºäº†äººç±»ä¸ AI åœ¨å¤„ç†å¤æ‚æˆ˜ç•¥ç¯å¢ƒæ—¶çš„æœ¬è´¨ä¸åŒï¼Œä¸ºåœ¨å¤æ‚åœºæ™¯ä¸­åº”ç”¨ AI æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13213v1",
      "published_date": "2025-08-16 22:53:34 UTC",
      "updated_date": "2025-08-16 22:53:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:12:34.281068+00:00"
    },
    {
      "arxiv_id": "2508.12165v1",
      "title": "RLNVR: Reinforcement Learning from Non-Verified Real-World Rewards",
      "title_zh": "RLNVRï¼šåŸºäºæœªç»éªŒè¯çœŸå®ä¸–ç•Œå¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Rohit Krishnan",
        "Jon Evans"
      ],
      "abstract": "This paper introduces RLNVR (Reinforcement Learning from Non-Verified Rewards), a framework for training language models using noisy, real-world feedback signals without requiring explicit human verification. Traditional RLHF requires expensive, verified reward signals that are impractical in many real-world domains. RLNVR addresses this challenge through baseline normalization and semantic similarity-based reward transfer. We demonstrate RLNVR through Walter, a prototype system that optimizes social media content generation using actual engagement data from Bluesky. Our experimental results show significant improvements in content quality and training stability, with comprehensive evaluation planned for future work. Positioning: We present a practical framework that combines RLNVR with GSPO (Group Sequence Policy Optimization) and an optional UED (Unsupervised Environment Design) curriculum to improve stability and diversity under noisy, implicit rewards. To our knowledge, combining GSPO-style normalization with a UED-style curriculum for LLM content generation from implicit social engagement has not been previously documented in this applied setting; we frame this as an applied integration rather than a new algorithm.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº† RLNVRï¼ˆReinforcement Learning from Non-Verified Rewardsï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨å¸¦æœ‰å™ªå£°ä¸”æœªç»äººå·¥æ˜¾å¼éªŒè¯çš„çœŸå®ä¸–ç•Œåé¦ˆä¿¡å·æ¥è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œè§£å†³äº†ä¼ ç»Ÿ RLHF å¯¹æ˜‚è´µéªŒè¯æˆæœ¬çš„ä¾èµ–é—®é¢˜ã€‚è¯¥æ¡†æ¶æ ¸å¿ƒæŠ€æœ¯åŒ…å«åŸºå‡†å½’ä¸€åŒ–ï¼ˆbaseline normalizationï¼‰å’ŒåŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„å¥–åŠ±è½¬ç§»ï¼ˆsemantic similarity-based reward transferï¼‰ï¼Œå¹¶ç»“åˆäº† GSPO (Group Sequence Policy Optimization) ä¸å¯é€‰çš„ UED (Unsupervised Environment Design) è¯¾ç¨‹ï¼Œä»¥ç¡®ä¿åœ¨å™ªå£°å’Œéšå¼å¥–åŠ±ç¯å¢ƒä¸‹çš„è®­ç»ƒç¨³å®šæ€§å’Œå†…å®¹å¤šæ ·æ€§ã€‚é€šè¿‡åœ¨ç¤¾äº¤åª’ä½“å¹³å° Bluesky ä¸Šçš„åŸå‹ç³»ç»Ÿ Walter è¿›è¡ŒéªŒè¯ï¼Œå®éªŒç»“æœè¯æ˜ RLNVR åœ¨å†…å®¹è´¨é‡å’Œè®­ç»ƒç¨³å®šæ€§æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ã€‚è¯¥é¡¹å·¥ä½œä¸ºåœ¨ç¼ºä¹æ˜¾å¼äººå·¥å¹²é¢„çš„å®é™…åº”ç”¨åœºæ™¯ä¸­ä¼˜åŒ–å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å®ç”¨çš„é›†æˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.12165v1",
      "published_date": "2025-08-16 21:34:04 UTC",
      "updated_date": "2025-08-16 21:34:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:12:35.259875+00:00"
    },
    {
      "arxiv_id": "2508.12163v1",
      "title": "RealTalk: Realistic Emotion-Aware Lifelike Talking-Head Synthesis",
      "title_zh": "RealTalkï¼šé€¼çœŸä¸”å…·æœ‰æƒ…æ„Ÿæ„ŸçŸ¥èƒ½åŠ›çš„ç”ŸåŠ¨è¯´è¯äººå¤´åƒåˆæˆ",
      "authors": [
        "Wenqing Wang",
        "Yun Fu"
      ],
      "abstract": "Emotion is a critical component of artificial social intelligence. However, while current methods excel in lip synchronization and image quality, they often fail to generate accurate and controllable emotional expressions while preserving the subject's identity. To address this challenge, we introduce RealTalk, a novel framework for synthesizing emotional talking heads with high emotion accuracy, enhanced emotion controllability, and robust identity preservation. RealTalk employs a variational autoencoder (VAE) to generate 3D facial landmarks from driving audio, which are concatenated with emotion-label embeddings using a ResNet-based landmark deformation model (LDM) to produce emotional landmarks. These landmarks and facial blendshape coefficients jointly condition a novel tri-plane attention Neural Radiance Field (NeRF) to synthesize highly realistic emotional talking heads. Extensive experiments demonstrate that RealTalk outperforms existing methods in emotion accuracy, controllability, and identity preservation, advancing the development of socially intelligent AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†RealTalkï¼Œä¸€ä¸ªæ—¨åœ¨åˆæˆé«˜åº¦çœŸå®ã€æƒ…æ„Ÿå¯æ§ä¸”å…·å¤‡ç¨³å¥èº«ä»½ä¿æŒï¼ˆidentity preservationï¼‰èƒ½åŠ›çš„å¯¹è¯å¤´åƒåˆæˆæ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•åœ¨ç”Ÿæˆç²¾ç¡®æƒ…ç»ªè¡¨è¾¾å’Œç»´æŒèº«ä»½ç‰¹å¾æ–¹é¢çš„å±€é™ï¼ŒRealTalkåˆ©ç”¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ä»é©±åŠ¨éŸ³é¢‘ä¸­æå–3Dé¢éƒ¨åœ°æ ‡ï¼Œå¹¶ç»“åˆåŸºäºResNetçš„åœ°æ ‡å˜å½¢æ¨¡å‹ï¼ˆLDMï¼‰ä¸æƒ…ç»ªæ ‡ç­¾åµŒå…¥ï¼ˆemotion-label embeddingsï¼‰æ¥ç”Ÿæˆç²¾ç»†çš„æƒ…æ„Ÿåœ°æ ‡ã€‚è¿™äº›åœ°æ ‡ä¸é¢éƒ¨æ··åˆå½¢çŠ¶ç³»æ•°ï¼ˆblendshape coefficientsï¼‰å…±åŒé©±åŠ¨ä¸€ç§æ–°å‹çš„ä¸‰å¹³é¢æ³¨æ„åŠ›ç¥ç»è¾å°„åœºï¼ˆtri-plane attention NeRFï¼‰ï¼Œä»è€Œå®ç°é€¼çœŸçš„äººåƒæ¸²æŸ“ã€‚å¤§é‡å®éªŒè¯æ˜ï¼ŒRealTalkåœ¨æƒ…ç»ªå‡†ç¡®æ€§ã€å¯æ§æ€§ä»¥åŠèº«ä»½ç‰¹å¾ä¿æŠ¤æ–¹é¢å‡æ˜¾è‘—ä¼˜äºç°æœ‰å…ˆè¿›æ¨¡å‹ã€‚è¯¥æˆæœä¸ä»…æå‡äº†åˆæˆå¤´åƒçš„è§†è§‰è´¨é‡ï¼Œä¹Ÿä¸ºå¼€å‘å…·å¤‡ç¤¾äº¤æ™ºèƒ½çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿæä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to the ICCV 2025 Workshop on Artificial Social Intelligence",
      "pdf_url": "https://arxiv.org/pdf/2508.12163v1",
      "published_date": "2025-08-16 21:28:22 UTC",
      "updated_date": "2025-08-16 21:28:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:12:47.985981+00:00"
    },
    {
      "arxiv_id": "2508.12162v1",
      "title": "AICRN: Attention-Integrated Convolutional Residual Network for Interpretable Electrocardiogram Analysis",
      "title_zh": "AICRNï¼šé¢å‘å¯è§£é‡Šå¿ƒç”µå›¾åˆ†æçš„èåˆæ³¨æ„åŠ›æœºåˆ¶å·ç§¯æ®‹å·®ç½‘ç»œ",
      "authors": [
        "J. M. I. H. Jayakody",
        "A. M. H. H. Alahakoon",
        "C. R. M. Perera",
        "R. M. L. C. Srimal",
        "Roshan Ragel",
        "Vajira Thambawita",
        "Isuru Nawinne"
      ],
      "abstract": "The paradigm of electrocardiogram (ECG) analysis has evolved into real-time digital analysis, facilitated by artificial intelligence (AI) and machine learning (ML), which has improved the diagnostic precision and predictive capacity of cardiac diseases. This work proposes a novel deep learning (DL) architecture called the attention-integrated convolutional residual network (AICRN) to regress key ECG parameters such as the PR interval, the QT interval, the QRS duration, the heart rate, the peak amplitude of the R wave, and the amplitude of the T wave for interpretable ECG analysis. Our architecture is specially designed with spatial and channel attention-related mechanisms to address the type and spatial location of the ECG features for regression. The models employ a convolutional residual network to address vanishing and exploding gradient problems. The designed system addresses traditional analysis challenges, such as loss of focus due to human errors, and facilitates the fast and easy detection of cardiac events, thereby reducing the manual efforts required to solve analysis tasks. AICRN models outperform existing models in parameter regression with higher precision. This work demonstrates that DL can play a crucial role in the interpretability and precision of ECG analysis, opening up new clinical applications for cardiac monitoring and management.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºAICRNï¼ˆAttention-Integrated Convolutional Residual Networkï¼‰çš„æ–°å‹æ·±åº¦å­¦ä¹ æ¶æ„ï¼Œæ—¨åœ¨é€šè¿‡å›å½’PR intervalã€QT intervalã€QRS durationã€heart rateã€R waveå³°å€¼æŒ¯å¹…å’ŒT waveæŒ¯å¹…ç­‰å…³é”®å‚æ•°ï¼Œå®ç°å¯è§£é‡Šçš„å¿ƒç”µå›¾ï¼ˆECGï¼‰åˆ†æã€‚è¯¥æ¶æ„ç‰¹åˆ«è®¾è®¡äº†ç©ºé—´ä¸é€šé“æ³¨æ„åŠ›æœºåˆ¶ï¼ˆspatial and channel attention-related mechanismsï¼‰ï¼Œç”¨äºç²¾ç¡®å®šä½å’Œè¯†åˆ«ç”¨äºå›å½’çš„ECGç‰¹å¾ã€‚åŒæ—¶ï¼Œæ¨¡å‹åˆ©ç”¨å·ç§¯æ®‹å·®ç½‘ç»œï¼ˆconvolutional residual networkï¼‰è§£å†³äº†æ¢¯åº¦æ¶ˆå¤±å’Œçˆ†ç‚¸é—®é¢˜ï¼Œæœ‰æ•ˆå…‹æœäº†ä¼ ç»Ÿäººå·¥åˆ†æä¸­å› ç–²åŠ³æˆ–è¯¯å·®å¯¼è‡´çš„æŒ‘æˆ˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAICRNåœ¨å‚æ•°å›å½’ä»»åŠ¡ä¸­çš„ç²¾åº¦ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†å¿ƒè„ç–¾ç—…çš„è¯Šæ–­æ•ˆç‡ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†æ·±åº¦å­¦ä¹ åœ¨å¿ƒç”µå›¾åˆ†æå¯è§£é‡Šæ€§å’Œç²¾ç¡®åº¦æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºä¸´åºŠå¿ƒè„ç›‘æµ‹ä¸ç®¡ç†æä¾›äº†æ–°çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.12162v1",
      "published_date": "2025-08-16 21:10:45 UTC",
      "updated_date": "2025-08-16 21:10:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:12:59.394122+00:00"
    },
    {
      "arxiv_id": "2508.18284v1",
      "title": "Multi-Modal Drift Forecasting of Leeway Objects via Navier-Stokes-Guided CNN and Sequence-to-Sequence Attention-Based Models",
      "title_zh": "åŸºäºçº³ç»´-æ–¯æ‰˜å…‹æ–¯å¼•å¯¼çš„å·ç§¯ç¥ç»ç½‘ç»œä¸æ³¨æ„åŠ›æœºåˆ¶åºåˆ—åˆ°åºåˆ—æ¨¡å‹çš„å¤šæ¨¡æ€æµ·é¢æ¼‚æµç‰©æ¼‚ç§»é¢„æµ‹",
      "authors": [
        "Rahmat K. Adesunkanmi",
        "Alexander W. Brandt",
        "Masoud Deylami",
        "Gustavo A. Giraldo Echeverri",
        "Hamidreza Karbasian",
        "Adel Alaeddini"
      ],
      "abstract": "Accurately predicting the drift (displacement) of leeway objects in maritime environments remains a critical challenge, particularly in time-sensitive scenarios such as search and rescue operations. In this study, we propose a multi-modal machine learning framework that integrates Sentence Transformer embeddings with attention-based sequence-to-sequence architectures to predict the drift of leeway objects in water. We begin by experimentally collecting environmental and physical data, including water current and wind velocities, object mass, and surface area, for five distinct leeway objects. Using simulated data from a Navier-Stokes-based model to train a convolutional neural network on geometrical image representations, we estimate drag and lift coefficients of the leeway objects. These coefficients are then used to derive the net forces responsible for driving the objects' motion. The resulting time series, comprising physical forces, environmental velocities, and object-specific features, combined with textual descriptions encoded via a language model, are inputs to attention-based sequence-to-sequence long-short-term memory and Transformer models, to predict future drift trajectories. We evaluate the framework across multiple time horizons ($1$, $3$, $5$, and $10$ seconds) and assess its generalization across different objects. We compare our approach against a fitted physics-based model and traditional machine learning methods, including recurrent neural networks and temporal convolutional neural networks. Our results show that these multi-modal models perform comparably to traditional models while also enabling longer-term forecasting in place of single-step prediction. Overall, our findings demonstrate the ability of a multi-modal modeling strategy to provide accurate and adaptable predictions of leeway object drift in dynamic maritime conditions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æµ·ä¸Šæœæ•‘ç­‰æ—¶æ•ˆæ€§ä»»åŠ¡ä¸­å‡†ç¡®é¢„æµ‹æ¼‚ç§»ç‰©ä½“(leeway objects)è½¨è¿¹çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§é›†æˆç‰©ç†è§„åˆ™ä¸æ·±åº¦å­¦ä¹ çš„å¤šæ¨¡æ€æœºå™¨å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ–¹æ³•é¦–å…ˆåˆ©ç”¨åŸºäºNavier-Stokesæ–¹ç¨‹çš„æ¨¡æ‹Ÿæ•°æ®è®­ç»ƒå·ç§¯ç¥ç»ç½‘ç»œ(CNN)ï¼Œé€šè¿‡å‡ ä½•å›¾åƒè¡¨ç¤ºä¼°ç®—ç‰©ä½“çš„é˜»åŠ›ä¸å‡åŠ›ç³»æ•°(drag and lift coefficients)ä»¥æ¨å¯¼å—åŠ›æƒ…å†µã€‚ç ”ç©¶ç»“åˆäº†Sentence Transformeræå–çš„æ–‡æœ¬æè¿°ç‰¹å¾ï¼Œå¹¶å°†ç‰©ç†åŠ›ã€ç¯å¢ƒæµé€ŸåŠç‰©ä½“å›ºæœ‰å±æ€§ä½œä¸ºè¾“å…¥ï¼Œåˆ©ç”¨åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„åºåˆ—åˆ°åºåˆ—(sequence-to-sequence)LSTMå’ŒTransformeræ¨¡å‹è¿›è¡Œè½¨è¿¹é¢„æµ‹ã€‚å®éªŒåœ¨1è‡³10ç§’ç­‰å¤šä¸ªæ—¶é—´å°ºåº¦ä¸Šè¯„ä¼°äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶ä¸ä¼ ç»Ÿçš„ç‰©ç†æ¨¡å‹ã€å¾ªç¯ç¥ç»ç½‘ç»œ(RNN)åŠæ—¶é—´å·ç§¯ç½‘ç»œ(TCN)è¿›è¡Œäº†å¯¹æ¯”ã€‚ç»“æœè¯æ˜ï¼Œè¯¥å¤šæ¨¡æ€æ¨¡å‹åœ¨ä¿æŒé«˜é¢„æµ‹ç²¾åº¦çš„åŒæ—¶ï¼Œç›¸æ¯”å•æ­¥é¢„æµ‹æ¨¡å‹å±•ç°å‡ºæ›´ä¼˜çš„é•¿æœŸé¢„æŠ¥èƒ½åŠ›ï¼Œä¸ºåŠ¨æ€æµ·æ´‹ç¯å¢ƒä¸‹çš„æ¼‚ç§»é¢„æµ‹æä¾›äº†å‡†ç¡®ä¸”å…·é€‚åº”æ€§çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "Submitted to IEEE",
      "pdf_url": "https://arxiv.org/pdf/2508.18284v1",
      "published_date": "2025-08-16 21:01:29 UTC",
      "updated_date": "2025-08-16 21:01:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:12:59.685278+00:00"
    },
    {
      "arxiv_id": "2508.15815v2",
      "title": "User-Assistant Bias in LLMs",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ä¸­çš„ç”¨æˆ·-åŠ©æ‰‹åå·®",
      "authors": [
        "Xu Pan",
        "Jingxuan Fan",
        "Zidi Xiong",
        "Ely Hahami",
        "Jorin Overwiening",
        "Ziqian Xie"
      ],
      "abstract": "Modern large language models (LLMs) are typically trained and deployed using structured role tags (e.g. system, user, assistant, tool) that explicitly mark the source of each piece of context. While these tags are essential for instruction following and controllability, asymmetries in the training data associated with different role tags can introduce inductive biases. In this paper, we study this phenomenon by formalizing user-assistant bias, defined as the tendency of an LLM to preferentially rely on information from either the user or assistant role when there is a conflict. We introduce a task-agnostic benchmark UserAssist and evaluate such bias in 52 frontier models. We observe that most of the instruction-tuned models exhibit strong user bias, whereas base and reasoning models are close to neutral. Using controlled fine-tuning experiments, we isolate which post-training recipes drive the observed user-assistant bias. We find that human-preference alignment amplifies user bias, while reasoning fine-tuning reduces it. Finally, we show that user-assistant bias can be bidirectionally controlled via direct preference optimization (DPO) on UserAssist-train, and that the resulting bias reliably generalizes to a more realistic multi-turn conversation dataset. These results reveal an underexplored consequence of role-tagged training and provide a principled framework to diagnose and control tag-induced biases in modern LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸­ç”±è§’è‰²æ ‡ç­¾(role tags)å¼•èµ·çš„â€œç”¨æˆ·-åŠ©æ‰‹åå·®â€(User-assistant bias)ï¼Œå³æ¨¡å‹åœ¨é¢ä¸´ä¿¡æ¯å†²çªæ—¶å€¾å‘äºä¼˜å…ˆä¾èµ–ç”¨æˆ·æˆ–åŠ©æ‰‹è§’è‰²ä¿¡æ¯çš„å½’çº³åå·®ã€‚ç ”ç©¶è€…æ¨å‡ºäº†ä»»åŠ¡æ— å…³çš„åŸºå‡†æµ‹è¯•é›†UserAssistå¹¶å¯¹52ä¸ªå‰æ²¿æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œå‘ç°å¤§å¤šæ•°æŒ‡ä»¤å¾®è°ƒ(instruction-tuned)æ¨¡å‹è¡¨ç°å‡ºå¼ºçƒˆçš„ç”¨æˆ·åå·®ï¼Œè€ŒåŸºç¡€æ¨¡å‹(base models)å’Œæ¨ç†æ¨¡å‹(reasoning models)åˆ™æ¥è¿‘ä¸­ç«‹ã€‚é€šè¿‡å—æ§å¾®è°ƒå®éªŒï¼Œç ”ç©¶å‘ç°äººç±»åå¥½å¯¹é½(human-preference alignment)ä¼šæ”¾å¤§ç”¨æˆ·åå·®ï¼Œè€Œæ¨ç†å¾®è°ƒ(reasoning fine-tuning)åˆ™èƒ½æ˜¾è‘—é™ä½è¯¥åå·®ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¯æ˜å¯ä»¥é€šè¿‡åœ¨UserAssist-trainä¸Šæ‰§è¡Œç›´æ¥åå¥½ä¼˜åŒ–(DPO)æ¥å®ç°å¯¹åå·®çš„åŒå‘æ§åˆ¶ï¼Œä¸”è¿™ç§æ§åˆ¶æ•ˆæœèƒ½å¯é åœ°æ³›åŒ–è‡³å¤šè½®å¯¹è¯æ•°æ®é›†ä¸­ã€‚è¿™é¡¹å·¥ä½œæ­ç¤ºäº†è§’è‰²æ ‡ç­¾è®­ç»ƒä¸­æœªè¢«å……åˆ†æ¢ç´¢çš„åæœï¼Œå¹¶ä¸ºè¯Šæ–­å’Œæ§åˆ¶ç°ä»£å¤§æ¨¡å‹ä¸­ç”±æ ‡ç­¾è¯±å¯¼çš„åå·®æä¾›äº†ä¸€ä¸ªåŸåˆ™æ€§çš„æ¡†æ¶ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15815v2",
      "published_date": "2025-08-16 20:33:09 UTC",
      "updated_date": "2026-01-03 18:21:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:13:02.249299+00:00"
    },
    {
      "arxiv_id": "2508.12149v1",
      "title": "MOVER: Multimodal Optimal Transport with Volume-based Embedding Regularization",
      "title_zh": "MOVERï¼šåŸºäºä½“ç§¯åµŒå…¥æ­£åˆ™åŒ–çš„å¤šæ¨¡æ€æœ€ä¼˜ä¼ è¾“",
      "authors": [
        "Haochen You",
        "Baojing Liu"
      ],
      "abstract": "Recent advances in multimodal learning have largely relied on pairwise contrastive objectives to align different modalities, such as text, video, and audio, in a shared embedding space. While effective in bi-modal setups, these approaches struggle to generalize across multiple modalities and often lack semantic structure in high-dimensional spaces. In this paper, we propose MOVER, a novel framework that combines optimal transport-based soft alignment with volume-based geometric regularization to build semantically aligned and structured multimodal representations. By integrating a transport-guided matching mechanism with a geometric volume minimization objective (GAVE), MOVER encourages consistent alignment across all modalities in a modality-agnostic manner. Experiments on text-video-audio retrieval tasks demonstrate that MOVER significantly outperforms prior state-of-the-art methods in both zero-shot and finetuned settings. Additional analysis shows improved generalization to unseen modality combinations and stronger structural consistency in the learned embedding space.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MOVERï¼Œä¸€ç§ç»“åˆäº†åŸºäºæœ€ä¼˜ä¼ è¾“(optimal transport-based)çš„è½¯å¯¹é½ä¸åŸºäºä½“ç§¯çš„å‡ ä½•æ­£åˆ™åŒ–(volume-based geometric regularization)çš„åˆ›æ–°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æˆå¯¹å¯¹æ¯”å­¦ä¹ (pairwise contrastive objectives)åœ¨å¤šæ¨¡æ€æ‰©å±•å’Œé«˜ç»´è¯­ä¹‰ç»“æ„æ„å»ºæ–¹é¢çš„å±€é™ã€‚è¯¥æ¡†æ¶é€šè¿‡é›†æˆä¼ è¾“å¼•å¯¼åŒ¹é…æœºåˆ¶(transport-guided matching)ä¸å‡ ä½•ä½“ç§¯æœ€å°åŒ–ç›®æ ‡(Geometric Volume Minimization, GAVE)ï¼Œä»¥æ¨¡æ€æ— å…³çš„æ–¹å¼å®ç°äº†æ–‡æœ¬ã€è§†é¢‘å’ŒéŸ³é¢‘ç­‰å¤šæ¨¡æ€ä¹‹é—´çš„ä¸€è‡´æ€§å¯¹é½ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒMOVER åœ¨æ–‡æœ¬-è§†é¢‘-éŸ³é¢‘æ£€ç´¢ä»»åŠ¡çš„é›¶æ ·æœ¬(zero-shot)å’Œå¾®è°ƒ(finetuned)è®¾ç½®ä¸‹å‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„å…ˆè¿›æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨å¤„ç†æœªè§æ¨¡æ€ç»„åˆæ—¶è¡¨ç°å‡ºæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶æ˜¾è‘—æå‡äº†å­¦ä¹ åµŒå…¥ç©ºé—´çš„ç»“æ„ä¸€è‡´æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted as a conference paper at CIKM 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.12149v1",
      "published_date": "2025-08-16 20:17:06 UTC",
      "updated_date": "2025-08-16 20:17:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:12:53.592644+00:00"
    },
    {
      "arxiv_id": "2508.12148v1",
      "title": "Demystifying Foreground-Background Memorization in Diffusion Models",
      "title_zh": "æ­ç§˜æ‰©æ•£æ¨¡å‹ä¸­çš„å‰æ™¯-èƒŒæ™¯è®°å¿†",
      "authors": [
        "Jimmy Z. Di",
        "Yiwei Lu",
        "Yaoliang Yu",
        "Gautam Kamath",
        "Adam Dziedzic",
        "Franziska Boenisch"
      ],
      "abstract": "Diffusion models (DMs) memorize training images and can reproduce near-duplicates during generation. Current detection methods identify verbatim memorization but fail to capture two critical aspects: quantifying partial memorization occurring in small image regions, and memorization patterns beyond specific prompt-image pairs. To address these limitations, we propose Foreground Background Memorization (FB-Mem), a novel segmentation-based metric that classifies and quantifies memorized regions within generated images. Our method reveals that memorization is more pervasive than previously understood: (1) individual generations from single prompts may be linked to clusters of similar training images, revealing complex memorization patterns that extend beyond one-to-one correspondences; and (2) existing model-level mitigation methods, such as neuron deactivation and pruning, fail to eliminate local memorization, which persists particularly in foreground regions. Our work establishes an effective framework for measuring memorization in diffusion models, demonstrates the inadequacy of current mitigation approaches, and proposes a stronger mitigation method using a clustering approach.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰©æ•£æ¨¡å‹(Diffusion Models)ä¼šè®°å¿†è®­ç»ƒå›¾åƒå¹¶ç”Ÿæˆè¿‘ä¹é‡å¤å†…å®¹çš„é—®é¢˜ï¼Œæå‡ºäº†Foreground Background Memorization (FB-Mem)è¿™ä¸€åŸºäºåˆ†å‰²çš„æ–°å‹åº¦é‡æŒ‡æ ‡ï¼Œç”¨äºåˆ†ç±»å¹¶é‡åŒ–ç”Ÿæˆå›¾åƒä¸­çš„å±€éƒ¨è®°å¿†åŒºåŸŸã€‚é€šè¿‡è¯¥æŒ‡æ ‡ï¼Œç ”ç©¶æ­ç¤ºäº†è®°å¿†ç°è±¡æ¯”ä»¥å¾€è®¤çŸ¥æ›´ä¸ºæ™®éï¼Œå‘ç°å•ä¸ªæç¤ºè¯­ç”Ÿæˆçš„å›¾åƒå¾€å¾€ä¸å¤šå¼ ç›¸ä¼¼è®­ç»ƒå›¾åƒæ„æˆçš„èšç±»ç›¸å…³ï¼Œè€Œéç®€å•çš„ç‚¹å¯¹ç‚¹è®°å¿†ã€‚å®éªŒè¿›ä¸€æ­¥è¯æ˜ï¼Œç°æœ‰çš„æ¨¡å‹çº§ç¼“è§£æ–¹æ³•å¦‚ç¥ç»å…ƒå»æ¿€æ´»(neuron deactivation)å’Œå‰ªæ(pruning)éš¾ä»¥æ¶ˆé™¤å±€éƒ¨è®°å¿†ï¼Œå°¤å…¶æ˜¯åœ¨å‰æ™¯åŒºåŸŸã€‚æœ€ç»ˆï¼Œæœ¬æ–‡å±•ç¤ºäº†å½“å‰ç¼“è§£æªæ–½çš„ä¸è¶³ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºèšç±»çš„æ–¹æ³•æ¥æ›´æœ‰æ•ˆåœ°æŠ‘åˆ¶æ‰©æ•£æ¨¡å‹çš„è®°å¿†é—®é¢˜ï¼Œä¸ºè¯„ä¼°å’Œæå‡æ¨¡å‹å®‰å…¨æ€§æä¾›äº†æ–°æ¡†æ¶ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.12148v1",
      "published_date": "2025-08-16 20:15:16 UTC",
      "updated_date": "2025-08-16 20:15:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:13:04.083771+00:00"
    },
    {
      "arxiv_id": "2508.12147v1",
      "title": "KP-INR: A Dual-Branch Implicit Neural Representation Model for Cardiac Cine MRI Reconstruction",
      "title_zh": "KP-INRï¼šç”¨äºå¿ƒè„ç”µå½±ç£å…±æŒ¯æˆåƒé‡å»ºçš„åŒåˆ†æ”¯éšå¼ç¥ç»è¡¨ç¤ºæ¨¡å‹",
      "authors": [
        "Donghang Lyu",
        "Marius Staring",
        "Mariya Doneva",
        "Hildo J. Lamb",
        "Nicola Pezzotti"
      ],
      "abstract": "Cardiac Magnetic Resonance (CMR) imaging is a non-invasive method for assessing cardiac structure, function, and blood flow. Cine MRI extends this by capturing heart motion, providing detailed insights into cardiac mechanics. To reduce scan time and breath-hold discomfort, fast acquisition techniques have been utilized at the cost of lowering image quality. Recently, Implicit Neural Representation (INR) methods have shown promise in unsupervised reconstruction by learning coordinate-to-value mappings from undersampled data, enabling high-quality image recovery. However, current existing INR methods primarily focus on using coordinate-based positional embeddings to learn the mapping, while overlooking the feature representations of the target point and its neighboring context. In this work, we propose KP-INR, a dual-branch INR method operating in k-space for cardiac cine MRI reconstruction: one branch processes the positional embedding of k-space coordinates, while the other learns from local multi-scale k-space feature representations at those coordinates. By enabling cross-branch interaction and approximating the target k-space values from both branches, KP-INR can achieve strong performance on challenging Cartesian k-space data. Experiments on the CMRxRecon2024 dataset confirms its improved performance over baseline models and highlights its potential in this field.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† KP-INRï¼Œä¸€ç§ç”¨äºå¿ƒè„ç”µå½± MRI (Cardiac Cine MRI) é‡å»ºçš„åŒåˆ†æ”¯éšå¼ç¥ç»è¡¨ç¤º (Implicit Neural Representation, INR) æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•è¿‡åº¦ä¾èµ–ä½ç½®ç¼–ç è€Œå¿½è§†å±€éƒ¨ç‰¹å¾çš„é—®é¢˜ã€‚è¯¥æ¨¡å‹ç›´æ¥åœ¨ k-space ä¸­è¿è¡Œï¼Œé€šè¿‡ä¸¤ä¸ªå¹¶è¡Œåˆ†æ”¯åˆ†åˆ«å¤„ç†åæ ‡çš„ä½ç½®åµŒå…¥ (positional embedding) ä»¥åŠå±€éƒ¨å¤šå°ºåº¦ k-space ç‰¹å¾è¡¨ç¤ºã€‚é€šè¿‡å¼•å…¥è·¨åˆ†æ”¯äº¤äº’æœºåˆ¶ï¼ŒKP-INR èƒ½å¤Ÿæœ‰æ•ˆæ•´åˆç©ºé—´ä½ç½®ä¸é¢‘åŸŸä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œæ›´ç²¾ç¡®åœ°ä»æ¬ é‡‡æ ·æ•°æ®ä¸­é€¼è¿‘ç›®æ ‡ k-space æ•°å€¼ã€‚åœ¨é¢å¯¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ç¬›å¡å°” (Cartesian) k-space æ•°æ®æ—¶ï¼Œè¯¥æ–¹æ³•å±•ç°å‡ºäº†å¼ºå¤§çš„é²æ£’æ€§å’Œé‡å»ºç²¾åº¦ã€‚å®éªŒç»“æœåœ¨ CMRxRecon2024 æ•°æ®é›†ä¸Šå¾—åˆ°äº†éªŒè¯ï¼Œè¯æ˜äº† KP-INR ç›¸æ¯”äºåŸºçº¿æ¨¡å‹çš„æ€§èƒ½ä¼˜åŠ¿åŠå…¶åœ¨å¿«é€Ÿã€é«˜è´¨é‡å¿ƒè„å½±åƒé‡å»ºé¢†åŸŸçš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.12147v1",
      "published_date": "2025-08-16 20:02:14 UTC",
      "updated_date": "2025-08-16 20:02:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:13:23.889827+00:00"
    },
    {
      "arxiv_id": "2508.12138v1",
      "title": "Substituting Proof of Work in Blockchain with Training-Verified Collaborative Model Computation",
      "title_zh": "ä»¥ç»è®­ç»ƒéªŒè¯çš„åä½œå¼æ¨¡å‹è®¡ç®—æ›¿ä»£åŒºå—é“¾ä¸­çš„å·¥ä½œé‡è¯æ˜",
      "authors": [
        "Mohammad Ishzaz Asif Rafid",
        "Morsalin Sakib"
      ],
      "abstract": "Bitcoin's Proof of Work (PoW) mechanism, while central to achieving decentralized consensus, has long been criticized for excessive energy use and hardware inefficiencies \\cite{devries2018bitcoin, truby2018decarbonizing}. This paper introduces a hybrid architecture that replaces Bitcoin's traditional PoW with a centralized, cloud-based collaborative training framework. In this model, miners contribute computing resources to train segments of horizontally scaled machine learning models on preprocessed datasets, ensuring privacy and generating meaningful outputs \\cite{li2017securing}. A central server evaluates contributions using two metrics: number of parameters trained and reduction in model loss during each cycle. At the end of every cycle, a weighted lottery selects the winning miner, who receives a digitally signed certificate. This certificate serves as a verifiable substitute for PoW and grants the right to append a block to the blockchain \\cite{nakamoto2008bitcoin}. By integrating digital signatures and SHA-256 hashing \\cite{nist2015sha}, the system preserves blockchain integrity while redirecting energy toward productive computation. The proposed approach addresses the sustainability concerns of traditional mining by converting resource expenditure into socially valuable work, aligning security incentives with real-world computational progress.",
      "tldr_zh": "æœ¬æ–‡é’ˆå¯¹æ¯”ç‰¹å¸å·¥ä½œé‡è¯æ˜ï¼ˆProof of Work, PoWï¼‰æœºåˆ¶å› é«˜èƒ½è€—å’Œç¡¬ä»¶ä½æ•ˆè€Œé¥±å—æ‰¹è¯„çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹æ··åˆæ¶æ„ã€‚è¯¥æ–¹æ¡ˆåˆ©ç”¨åŸºäºäº‘çš„åä½œè®­ç»ƒæ¡†æ¶æ›¿ä»£ä¼ ç»Ÿ PoWï¼Œè¦æ±‚çŸ¿å·¥é€šè¿‡è´¡çŒ®è®¡ç®—èµ„æºæ¥è®­ç»ƒæ°´å¹³æ‰©å±•çš„æœºå™¨å­¦ä¹ æ¨¡å‹ç‰‡æ®µã€‚ä¸­å¤®æœåŠ¡å™¨æ ¹æ®çŸ¿å·¥è®­ç»ƒçš„å‚æ•°æ•°é‡åŠæ¨¡å‹æŸå¤±ï¼ˆmodel lossï¼‰çš„é™ä½ç¨‹åº¦è¿›è¡Œè´¡çŒ®è¯„ä¼°ï¼Œå¹¶åœ¨æ¯ä¸ªå‘¨æœŸé€šè¿‡åŠ æƒéšæœºæŠ½å–æ–¹å¼é€‰å‡ºè·èƒœè€…ã€‚è·èƒœçŸ¿å·¥å°†è·å¾—ä¸€ä»½æ•°å­—ç­¾åè¯ä¹¦ï¼Œä½œä¸º PoW çš„å¯éªŒè¯æ›¿ä»£å‡­è¯ï¼Œèµ‹äºˆå…¶åœ¨åŒºå—é“¾ä¸Šæ·»åŠ æ–°åŒºå—çš„æƒåˆ©ã€‚ç³»ç»Ÿé›†æˆäº†æ•°å­—ç­¾åä¸ SHA-256 å“ˆå¸ŒæŠ€æœ¯ï¼Œåœ¨ç¡®ä¿åŒºå—é“¾å®Œæ•´æ€§çš„åŒæ—¶ï¼Œå°†èƒ½æºæ¶ˆè€—é‡å®šå‘è‡³å…·æœ‰ç¤¾ä¼šä»·å€¼çš„ç”Ÿäº§æ€§è®¡ç®—ã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆè§£å†³äº†ä¼ ç»ŸæŒ–çŸ¿çš„å¯æŒç»­æ€§é—®é¢˜ï¼Œå®ç°äº†å®‰å…¨æ¿€åŠ±æœºåˆ¶ä¸ç°å®ä¸–ç•Œè®¡ç®—è¿›æ­¥çš„æœ‰æœºå¯¹é½ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.12138v1",
      "published_date": "2025-08-16 19:12:34 UTC",
      "updated_date": "2025-08-16 19:12:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:13:29.493636+00:00"
    },
    {
      "arxiv_id": "2509.00018v1",
      "title": "A Fluid Antenna Enabled Physical Layer Key Generation for Next-G Wireless Networks",
      "title_zh": "æµä½“å¤©çº¿èµ‹èƒ½çš„ä¸‹ä¸€ä»£æ— çº¿ç½‘ç»œç‰©ç†å±‚å¯†é’¥ç”Ÿæˆ",
      "authors": [
        "Jiacheng Guo",
        "Ning Gao",
        "Yiping Zuo",
        "Hao Xu",
        "Shi Jin",
        "Kai Kit Wong"
      ],
      "abstract": "As a promising physical layer security technique, physical layer key generation (PLKG) enables legitimate users to obtain secret keys from wireless channel without security infrastructures. However, in harsh propagation environments, the channel characteristic becomes unsatisfactory, the key generation rate (KGR) is significantly deteriorated. In this paper, we propose a novel fluid antenna (FA) enabled PLKG system to address this challenge. Specifically, we first derive the closed-form expression of the KGR for FA array, and then jointly optimize the precoding matrix and the antenna positions via a particle swarm optimization (PSO) algorithm. Next, to further reduce the computational complexity of the optimization procedure, we develop an alternating optimization (AO) algorithm, which combines the projected gradient descent (PGD) and the PSO. Simulation results demonstrate that by exploiting the additional spatial degree of freedom (DoF), our FA enabled PLKG system is superior to the benchmarks, such as the conventional fixed-position antenna (FPA) array and the reconfigurable intelligent surface (RIS). It is worth highlighting that compared to the conventional uniform planar antenna (UPA), the FA enabled PLKG achieves a 35.42\\% KGR performance improvement under PSO algorithm and a 67.73\\% KGR performance improvement under AO algorithm, respectively.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„æµåŠ¨å¤©çº¿ (Fluid Antenna, FA) èµ‹èƒ½çš„ç‰©ç†å±‚å¯†é’¥ç”Ÿæˆ (PLKG) ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³æ¶åŠ£ä¼ æ’­ç¯å¢ƒä¸‹å¯†é’¥ç”Ÿæˆç‡ (KGR) æ˜¾è‘—ä¸‹é™çš„é—®é¢˜ã€‚ä½œè€…é¦–å…ˆæ¨å¯¼äº† FA é˜µåˆ—ä¸‹ KGR çš„é—­å¼è¡¨è¾¾å¼ï¼Œå¹¶åˆ©ç”¨ç²’å­ç¾¤ä¼˜åŒ– (PSO) ç®—æ³•å¯¹é¢„ç¼–ç çŸ©é˜µå’Œå¤©çº¿ä½ç½®è¿›è¡Œäº†è”åˆä¼˜åŒ–ã€‚ä¸ºé™ä½è®¡ç®—å¤æ‚åº¦ï¼Œç ”ç©¶è¿›ä¸€æ­¥å¼€å‘äº†ä¸€ç§ç»“åˆæŠ•å½±æ¢¯åº¦ä¸‹é™ (PGD) å’Œ PSO çš„äº¤æ›¿ä¼˜åŒ– (AO) ç®—æ³•ã€‚ä»¿çœŸç»“æœè¯æ˜ï¼Œé€šè¿‡åˆ©ç”¨é¢å¤–çš„ç©ºé—´è‡ªç”±åº¦ (DoF)ï¼ŒFA èµ‹èƒ½çš„ PLKG ç³»ç»Ÿåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„å›ºå®šä½ç½®å¤©çº¿ (FPA) å’Œå¯é‡æ„æ™ºèƒ½è¡¨é¢ (RIS)ã€‚å®éªŒæ•°æ®è¡¨æ˜ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„å‡åŒ€å¹³é¢é˜µåˆ— (UPA)ï¼Œè¯¥æ–¹æ¡ˆåœ¨ PSO å’Œ AO ç®—æ³•ä¸‹åˆ†åˆ«å®ç°äº† 35.42% å’Œ 67.73% çš„ KGR æå‡ï¼Œä¸ºä¸‹ä¸€ä»£æ— çº¿ç½‘ç»œçš„å®‰å…¨é€šä¿¡æä¾›äº†æœ‰æ•ˆæ”¯æŒã€‚",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.CR",
        "cs.IT"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.00018v1",
      "published_date": "2025-08-16 18:39:19 UTC",
      "updated_date": "2025-08-16 18:39:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:13:33.985579+00:00"
    },
    {
      "arxiv_id": "2508.12116v1",
      "title": "DynamixSFT: Dynamic Mixture Optimization of Instruction Tuning Collections",
      "title_zh": "DynamixSFTï¼šæŒ‡ä»¤å¾®è°ƒé›†åˆçš„åŠ¨æ€æ··åˆä¼˜åŒ–",
      "authors": [
        "Haebin Shin",
        "Lei Ji",
        "Xiao Liu",
        "Zhiwei Yu",
        "Qi Chen",
        "Yeyun Gong"
      ],
      "abstract": "As numerous instruction-tuning datasets continue to emerge during the post-training stage, dynamically balancing and optimizing their mixtures has become a critical challenge. To address this, we propose DynamixSFT, a dynamic and automated method for instruction-tuning dataset mixture optimization. We formulate the problem as a multi-armed bandit setup and introduce a Prior-scaled Boltzmann Exploration that softly anchors the updated sampling distribution to the original dataset proportions, thereby preserving the inherent diversity and coverage of the collection. Sampling probabilities are updated using a lightweight 1-Step Look-ahead Reward, reflecting how much the dataset contributes to improving the model's performance at its current state. When applied to the Tulu-v2-mixture collection comprising 16 instruction-tuning datasets, DynamixSFT achieves up to a 2.2% performance improvement across 10 benchmarks. Furthermore, we provide a comprehensive analysis and visualizations to offer deeper insights into the adaptive dynamics of our method.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¾®è°ƒé˜¶æ®µå¤§é‡æŒ‡ä»¤å¾®è°ƒ(Instruction Tuning)æ•°æ®é›†æ¶Œç°å¸¦æ¥çš„åŠ¨æ€å¹³è¡¡ä¸æ··åˆä¼˜åŒ–æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸ºDynamixSFTçš„åŠ¨æ€è‡ªåŠ¨åŒ–ä¼˜åŒ–æ–¹æ³•ã€‚ç ”ç©¶å°†è¯¥é—®é¢˜å»ºæ¨¡ä¸ºå¤šè‡‚è€è™æœº(Multi-armed Bandit)æ¶æ„ï¼Œå¹¶å¼•å…¥äº†å…ˆéªŒç¼©æ”¾çš„ç»å°”å…¹æ›¼æ¢ç´¢(Prior-scaled Boltzmann Exploration)æœºåˆ¶ã€‚è¯¥æœºåˆ¶é€šè¿‡å°†æ›´æ–°åçš„é‡‡æ ·åˆ†å¸ƒé”šå®šåœ¨åŸå§‹æ•°æ®é›†æ¯”ä¾‹ä¸Šï¼Œä»è€Œæœ‰æ•ˆä¿ç•™äº†æ•°æ®é›†åˆå›ºæœ‰çš„å¤šæ ·æ€§(Diversity)å’Œè¦†ç›–èŒƒå›´(Coverage)ã€‚é‡‡æ ·æ¦‚ç‡åˆ™åˆ©ç”¨è½»é‡çº§çš„å•æ­¥å‰ç»å¥–åŠ±(1-Step Look-ahead Reward)è¿›è¡Œå®æ—¶æ›´æ–°ï¼Œä»¥åæ˜ å„æ•°æ®é›†å¯¹æ¨¡å‹å½“å‰çŠ¶æ€æ€§èƒ½æå‡çš„è´¡çŒ®åº¦ã€‚åœ¨åŒ…å«16ä¸ªæŒ‡ä»¤å¾®è°ƒæ•°æ®é›†çš„Tulu-v2-mixtureé›†åˆä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDynamixSFTåœ¨10ä¸ªåŸºå‡†æµ‹è¯•ä¸­æœ€é«˜å®ç°äº†2.2%çš„æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼Œè¯¥è®ºæ–‡è¿˜é€šè¿‡è¯¦å°½çš„åˆ†æä¸å¯è§†åŒ–ï¼Œä¸ºç†è§£æ··åˆæ•°æ®é›†å¾®è°ƒè¿‡ç¨‹ä¸­çš„è‡ªé€‚åº”åŠ¨æ€æä¾›äº†æ·±å…¥è§è§£ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.12116v1",
      "published_date": "2025-08-16 18:01:39 UTC",
      "updated_date": "2025-08-16 18:01:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:13:36.983844+00:00"
    },
    {
      "arxiv_id": "2508.12109v1",
      "title": "Simple o3: Towards Interleaved Vision-Language Reasoning",
      "title_zh": "Simple o3ï¼šè¿ˆå‘äº¤ç»‡çš„è§†è§‰-è¯­è¨€æ¨ç†",
      "authors": [
        "Ye Wang",
        "Qianglong Chen",
        "Zejun Li",
        "Siyuan Wang",
        "Shijie Guo",
        "Zhirui Zhang",
        "Zhongyu Wei"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have shown impressive performance on vision-language tasks, but their long Chain-of-Thought (CoT) capabilities in multimodal scenarios remain underexplored. Inspired by OpenAI's o3 model, which emulates human-like ''thinking with image'' through iterative visual transformations and linguistic reasoning, we propose Simple o3, an end-to-end framework that integrates dynamic tool interactions (e.g., cropping, zooming, and reusing) into interleaved vision-language reasoning via supervised fine-tuning (SFT). Our approach features a scalable data synthesis pipeline that generates high-quality interleaved vision-language reasoning chains via an ''observe-reason-act'' cycle, complete with executable visual operations and rigorous verification, yielding the open-source TWI-Tools-146K dataset. Experimental results demonstrate Simple o3's superior performance on diverse benchmarks, outperforming existing approaches. By combining enhanced reasoning capabilities, Simple o3 establishes a powerful yet computationally affordable paradigm for advancing multimodal reasoning. Remarkably, we provide the first in-depth analysis of different interleaved reasoning strategies, offering insights into their impact on model performance. We found that by introducing additional visual tokens for interleaved vision-language reasoning, reusing and magnifying the original image significantly improves the model's visual reasoning and fine-grained perception, while image cropping based on precise visual grounding allows the model to effectively focus on key entities or regions, further enhancing its capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Simple o3ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å¢å¼ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨äº¤é”™è§†è§‰è¯­è¨€æ¨ç†ä¸­é•¿é“¾å¼æ€ç»´ï¼ˆChain-of-Thoughtï¼‰èƒ½åŠ›çš„é«˜æ•ˆæ¡†æ¶ã€‚å—OpenAI o3æ¨¡å‹çš„å¯å‘ï¼ŒSimple o3é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å°†è£å‰ªã€ç¼©æ”¾å’Œå¤ç”¨ç­‰åŠ¨æ€å·¥å…·äº¤äº’æ•´åˆåˆ°æ¨ç†è¿‡ç¨‹ä¸­ï¼Œå®ç°äº†ç«¯åˆ°ç«¯çš„â€œç»“åˆå›¾åƒæ€è€ƒâ€èƒ½åŠ›ã€‚ç ”ç©¶å›¢é˜ŸåŒæ­¥æ¨å‡ºäº†ä¸€ä¸ªå¯æ‰©å±•çš„æ•°æ®åˆæˆæµæ°´çº¿ï¼Œå¹¶å¼€æºäº†åŒ…å«14.6ä¸‡æ¡é«˜è´¨é‡æ¨ç†é“¾çš„TWI-Tools-146Kæ•°æ®é›†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSimple o3åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„æ€§èƒ½å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºæå‡å¤šæ¨¡æ€æ¨ç†æä¾›äº†ä¸€ç§è®¡ç®—æˆæœ¬å¯æ§çš„æ–°èŒƒå¼ã€‚ç ”ç©¶è¿›ä¸€æ­¥åˆ†æå‘ç°ï¼Œå¤ç”¨ä¸æ”¾å¤§åŸå§‹å›¾åƒèƒ½æ˜¾è‘—å¢å¼ºæ¨¡å‹çš„ç»†ç²’åº¦æ„ŸçŸ¥ï¼Œè€ŒåŸºäºè§†è§‰å®šä½çš„å›¾åƒè£å‰ªåˆ™èƒ½å¸®åŠ©æ¨¡å‹æ›´ç²¾å‡†åœ°èšç„¦å…³é”®ç›®æ ‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.12109v1",
      "published_date": "2025-08-16 17:15:39 UTC",
      "updated_date": "2025-08-16 17:15:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:13:45.495400+00:00"
    },
    {
      "arxiv_id": "2508.12104v3",
      "title": "Generative Medical Event Models Improve with Scale",
      "title_zh": "ç”Ÿæˆå¼åŒ»ç–—äº‹ä»¶æ¨¡å‹æ•ˆèƒ½éšè§„æ¨¡å¢é•¿è€Œæå‡",
      "authors": [
        "Shane Waxler",
        "Paul Blazek",
        "Davis White",
        "Daniel Sneider",
        "Kevin Chung",
        "Mani Nagarathnam",
        "Patrick Williams",
        "Hank Voeller",
        "Karen Wong",
        "Matthew Swanhorst",
        "Sheng Zhang",
        "Naoto Usuyama",
        "Cliff Wong",
        "Tristan Naumann",
        "Hoifung Poon",
        "Andrew Loza",
        "Daniella Meeker",
        "Seth Hain",
        "Rahul Shah"
      ],
      "abstract": "Realizing personalized medicine at scale calls for methods that distill insights from longitudinal patient journeys, which can be viewed as a sequence of medical events. Foundation models pretrained on large-scale medical event data represent a promising direction for scaling real-world evidence generation and generalizing to diverse downstream tasks. Using Epic Cosmos, a dataset with medical events from de-identified longitudinal health records for 16.3 billion encounters over 300 million unique patient records from 310 health systems, we introduce the Curiosity models, a family of decoder-only transformer models pretrained on 118 million patients representing 115 billion discrete medical events (151 billion tokens). We present the largest scaling-law study of medical event data, establishing a methodology for pretraining and revealing power-law scaling relationships for compute, tokens, and model size. Consequently, we pretrained a series of compute-optimal models with up to 1 billion parameters. Conditioned on a patient's real-world history, Curiosity autoregressively predicts the next medical event to simulate patient health timelines. We studied 78 real-world tasks, including diagnosis prediction, disease prognosis, and healthcare operations. Remarkably for a foundation model with generic pretraining and simulation-based inference, Curiosity generally outperformed or matched task-specific supervised models on these tasks, without requiring task-specific fine-tuning or few-shot examples. Curiosity's predictive power consistently improves as the model and pretraining scale. Our results show that Curiosity, a generative medical event foundation model, can effectively capture complex clinical dynamics, providing an extensible and generalizable framework to support clinical decision-making, streamline healthcare operations, and improve patient outcomes.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è§„æ¨¡ä¸ªæ€§åŒ–åŒ»ç–—éœ€æ±‚ï¼Œæå‡ºäº†ä¸€ç§åœ¨æµ·é‡çºµå‘åŒ»ç–—äº‹ä»¶æ•°æ®ä¸Šé¢„è®­ç»ƒçš„åŸºç¡€æ¨¡å‹(Foundation models)æ–¹æ³•ã€‚åˆ©ç”¨Epic Cosmosæ•°æ®é›†ï¼Œç ”ç©¶å›¢é˜Ÿæ¨å‡ºäº†åä¸ºCuriosityçš„è§£ç å™¨æ¶æ„Transformeræ¨¡å‹å®¶æ—ï¼Œå…¶é¢„è®­ç»ƒè§„æ¨¡æ¶µç›–äº†1.18äº¿åæ‚£è€…å’Œ1150äº¿ä¸ªç¦»æ•£åŒ»ç–—äº‹ä»¶ã€‚è¿™é¡¹ç ”ç©¶å¼€å±•äº†åŒ»ç–—äº‹ä»¶æ•°æ®é¢†åŸŸæœ€å¤§è§„æ¨¡çš„ç¼©æ”¾æ³•åˆ™(Scaling-law)åˆ†æï¼Œæ­ç¤ºäº†è®¡ç®—é‡ã€Tokenæ•°é‡ä¸æ¨¡å‹è§„æ¨¡ä¹‹é—´çš„å¹‚å¾‹(Power-law)ç¼©æ”¾å…³ç³»ï¼Œå¹¶æ®æ­¤è®­ç»ƒäº†é«˜è¾¾10äº¿å‚æ•°çš„æœ€ä¼˜è®¡ç®—æ¨¡å‹ã€‚Curiosityé€šè¿‡è‡ªå›å½’(Autoregressively)é¢„æµ‹ä¸‹ä¸€ä¸ªåŒ»ç–—äº‹ä»¶æ¥æ¨¡æ‹Ÿæ‚£è€…çš„å¥åº·æ—¶é—´è½´ï¼Œåœ¨æ— éœ€ç‰¹å®šä»»åŠ¡å¾®è°ƒ(Fine-tuning)æˆ–å°‘æ ·æœ¬(Few-shot)å­¦ä¹ çš„æƒ…å†µä¸‹ï¼Œåœ¨78é¡¹ç–¾ç—…è¯Šæ–­ã€é¢„ååŠåŒ»ç–—è¿è¥ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºæˆ–æŒå¹³äºä¸“é—¨çš„ç›‘ç£æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCuriosityçš„é¢„æµ‹æ•ˆèƒ½éšæ¨¡å‹å’Œé¢„è®­ç»ƒè§„æ¨¡çš„æ‰©å¤§è€Œæ˜¾è‘—å¢å¼ºï¼Œè¯æ˜äº†ç”Ÿæˆå¼åŒ»ç–—äº‹ä»¶åŸºç¡€æ¨¡å‹åœ¨æ•æ‰å¤æ‚ä¸´åºŠåŠ¨æ€æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚è¿™ä¸€é€šç”¨æ¡†æ¶ä¸ºæ”¯æŒä¸´åºŠå†³ç­–ã€ä¼˜åŒ–åŒ»ç–—æµç¨‹åŠæå‡æ‚£è€…é¢„åæä¾›äº†æå…·æ½œåŠ›çš„æ‰©å±•æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.12104v3",
      "published_date": "2025-08-16 17:00:51 UTC",
      "updated_date": "2025-11-07 23:40:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:13:49.888134+00:00"
    },
    {
      "arxiv_id": "2508.14101v2",
      "title": "Implicit Hypergraph Neural Network",
      "title_zh": "éšå¼è¶…å›¾ç¥ç»ç½‘ç»œ",
      "authors": [
        "Akash Choudhuri",
        "Yongjian Zhong",
        "Bijaya Adhikari"
      ],
      "abstract": "Hypergraphs offer a generalized framework for capturing high-order relationships between entities and have been widely applied in various domains, including healthcare, social networks, and bioinformatics. Hypergraph neural networks, which rely on message-passing between nodes over hyperedges to learn latent representations, have emerged as the method of choice for predictive tasks in many of these domains. These approaches typically perform only a small number of message-passing rounds to learn the representations, which they then utilize for predictions. The small number of message-passing rounds comes at a cost, as the representations only capture local information and forego long-range high-order dependencies. However, as we demonstrate, blindly increasing the message-passing rounds to capture long-range dependency also degrades the performance of hyper-graph neural networks.\n  Recent works have demonstrated that implicit graph neural networks capture long-range dependencies in standard graphs while maintaining performance. Despite their popularity, prior work has not studied long-range dependency issues on hypergraph neural networks. Here, we first demonstrate that existing hypergraph neural networks lose predictive power when aggregating more information to capture long-range dependency. We then propose Implicit Hypergraph Neural Network (IHNN), a novel framework that jointly learns fixed-point representations for both nodes and hyperedges in an end-to-end manner to alleviate this issue. Leveraging implicit differentiation, we introduce a tractable projected gradient descent approach to train the model efficiently. Extensive experiments on real-world hypergraphs for node classification demonstrate that IHNN outperforms the closest prior works in most settings, establishing a new state-of-the-art in hypergraph learning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¶…å›¾(Hypergraph)ç¥ç»ç½‘ç»œåœ¨æ•æ‰é•¿ç¨‹é«˜é˜¶ä¾èµ–å…³ç³»(long-range high-order dependencies)æ—¶çš„å±€é™æ€§æå‡ºäº†Implicit Hypergraph Neural Network (IHNN)ã€‚ä¼ ç»Ÿçš„è¶…å›¾ç¥ç»ç½‘ç»œé€šå¸¸ä¾èµ–äºæœ‰é™æ¬¡çš„æ˜¾å¼æ¶ˆæ¯ä¼ é€’(message-passing)æ¥å­¦ä¹ ç‰¹å¾ï¼Œè¿™å¯¼è‡´æ¨¡å‹ä»…èƒ½æ•æ‰å±€éƒ¨ä¿¡æ¯ï¼Œè€Œç›²ç›®å¢åŠ ä¼ é€’æ¬¡æ•°åˆä¼šå¯¼è‡´æ€§èƒ½é€€åŒ–ã€‚IHNN æ¡†æ¶é€šè¿‡ä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼å…±åŒå­¦ä¹ èŠ‚ç‚¹å’Œè¶…è¾¹çš„å®šç‚¹è¡¨ç¤º(fixed-point representations)ï¼Œä»æ ¹æœ¬ä¸Šè§£å†³äº†æ•æ‰å…¨å±€ä¿¡æ¯æ—¶çš„æ•ˆèƒ½æŸè€—é—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨éšå¼å¾®åˆ†(implicit differentiation)å’Œä¸€ç§å¯è¡Œçš„æŠ•å½±æ¢¯åº¦ä¸‹é™(projected gradient descent)ç­–ç•¥æ¥ç¡®ä¿æ¨¡å‹çš„é«˜æ•ˆè®­ç»ƒã€‚åœ¨çœŸå®ä¸–ç•Œè¶…å›¾æ•°æ®é›†ä¸Šçš„èŠ‚ç‚¹åˆ†ç±»(node classification)å®éªŒè¯æ˜ï¼ŒIHNN åœ¨å¤šç§è®¾ç½®ä¸‹å‡ä¼˜äºç°æœ‰çš„å…ˆè¿›æ¨¡å‹ï¼Œç¡®ç«‹äº†è¶…å›¾å­¦ä¹ é¢†åŸŸæ–°çš„ state-of-the-artã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at IEEE BigData 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.14101v2",
      "published_date": "2025-08-16 16:58:59 UTC",
      "updated_date": "2025-12-01 19:25:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:13:51.301654+00:00"
    },
    {
      "arxiv_id": "2508.12100v1",
      "title": "Overcoming Knowledge Discrepancies: Structuring Reasoning Threads through Knowledge Balancing in Interactive Scenarios",
      "title_zh": "å…‹æœçŸ¥è¯†å·®å¼‚ï¼šäº¤äº’åœºæ™¯ä¸‹åŸºäºçŸ¥è¯†å‡è¡¡çš„æ¨ç†è·¯å¾„æ„å»º",
      "authors": [
        "Daniel Burkhardt",
        "Xiangwei Cheng"
      ],
      "abstract": "Reasoning in interactive problem solving scenarios requires models to construct reasoning threads that reflect user understanding and align with structured domain knowledge. However, current reasoning models often lack explicit semantic hierarchies, user-domain knowledge alignment, and principled mechanisms to prune reasoning threads for effectiveness. These limitations result in lengthy generic output that does not guide users through goal-oriented reasoning steps. To address this, we propose a prototype-inspired, two-phases Reasoning-Threads-Evaluation (ReT-Eval) framework, drawing inspiration from human-like reasoning strategies that emphasize structured knowledge reuse. In the first phase, semantically relevant knowledge structures are extracted from a sparse domain knowledge graph using a graph neural network and enriched with intrinsic large language model knowledge to resolve knowledge discrepancies. In the second phase, these threads are evaluated and pruned using a reward-guided strategy aimed at maintaining semantic coherence to generate effective reasoning threads. Experiments and expert evaluations show that ReT-Eval enhances user understanding and outperforms state-of-the-art reasoning models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äº¤äº’å¼é—®é¢˜è§£å†³åœºæ™¯ä¸­ï¼Œæ¨ç†æ¨¡å‹å› ç¼ºä¹æ˜¾å¼è¯­ä¹‰å±‚æ¬¡å’ŒçŸ¥è¯†å¯¹é½è€Œäº§ç”Ÿå†—ä½™è¾“å‡ºçš„é—®é¢˜ï¼Œæå‡ºäº†åŸºäºåŸå‹å¯å‘çš„ä¸¤é˜¶æ®µæ¨ç†é“¾è¯„ä¼°æ¡†æ¶ReT-Evalã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å›¾ç¥ç»ç½‘ç»œ(GNN)ä»é¢†åŸŸçŸ¥è¯†å›¾è°±(Knowledge Graph)ä¸­æå–è¯­ä¹‰ç›¸å…³çš„çŸ¥è¯†ç»“æ„ï¼Œå¹¶ç»“åˆå¤§è¯­è¨€æ¨¡å‹(LLM)çš„å†…åœ¨çŸ¥è¯†ä»¥è§£å†³çŸ¥è¯†å·®å¼‚ã€‚ç¬¬äºŒé˜¶æ®µåˆ™é€šè¿‡å¥–åŠ±å¼•å¯¼ç­–ç•¥(Reward-guided strategy)å¯¹æ¨ç†é“¾è¿›è¡Œè¯„ä¼°ä¸å‰ªæï¼Œæ—¨åœ¨ç»´æŒè¯­ä¹‰ä¸€è‡´æ€§å¹¶ç”Ÿæˆæœ‰æ•ˆçš„æ¨ç†è·¯å¾„ã€‚å®éªŒç»“æœä¸ä¸“å®¶è¯„ä¼°è¡¨æ˜ï¼ŒReT-Evalåœ¨å¢å¼ºç”¨æˆ·ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå…¶æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„å…ˆè¿›æ¨ç†æ¨¡å‹ã€‚è¯¥æ–¹æ³•ä¸ºæ„å»ºç›®æ ‡å¯¼å‘çš„ç»“æ„åŒ–æ¨ç†æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯è·¯å¾„ï¼ŒæˆåŠŸå…‹æœäº†äº¤äº’åœºæ™¯ä¸­çš„çŸ¥è¯†ä¸ä¸€è‡´æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "13 pages, 1 figure, 6 tables",
      "pdf_url": "https://arxiv.org/pdf/2508.12100v1",
      "published_date": "2025-08-16 16:41:42 UTC",
      "updated_date": "2025-08-16 16:41:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:13:54.293285+00:00"
    },
    {
      "arxiv_id": "2508.12096v2",
      "title": "STEM: Efficient Relative Capability Evaluation of LLMs through Structured Transition Samples",
      "title_zh": "STEMï¼šåŸºäºç»“æ„åŒ–è·ƒè¿æ ·æœ¬çš„å¤§è¯­è¨€æ¨¡å‹é«˜æ•ˆç›¸å¯¹èƒ½åŠ›è¯„ä¼°",
      "authors": [
        "Haiquan Hu",
        "Jiazhi Jiang",
        "Shiyou Xu",
        "Ruhan Zeng",
        "Tian Wang"
      ],
      "abstract": "Evaluating large language models (LLMs) has become increasingly challenging as model capabilities advance rapidly. While recent models often achieve higher scores on standard benchmarks, these improvements do not consistently reflect enhanced real-world reasoning capabilities. Moreover, widespread overfitting to public benchmarks and the high computational cost of full evaluations have made it both expensive and less effective to distinguish meaningful differences between models. To address these challenges, we propose the \\textbf{S}tructured \\textbf{T}ransition \\textbf{E}valuation \\textbf{M}ethod (STEM), a lightweight and interpretable evaluation framework for efficiently estimating the relative capabilities of LLMs. STEM identifies \\textit{significant transition samples} (STS) by analyzing consistent performance transitions among LLMs of the same architecture but varying parameter scales. These samples enable STEM to effectively estimate the capability position of an unknown model. Qwen3 model family is applied to construct the STS pool on six diverse and representative benchmarks. To assess generalizability. Experimental results indicate that STEM reliably captures performance trends, aligns with ground-truth rankings of model capability. These findings highlight STEM as a practical and scalable method for fine-grained, architecture-agnostic evaluation of LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)è¯„ä¼°ä¸­å­˜åœ¨çš„åŸºå‡†æµ‹è¯•è¿‡æ‹Ÿåˆå’Œè®¡ç®—æˆæœ¬é«˜æ˜‚ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ç»“æ„åŒ–è½¬æ¢è¯„ä¼°æ–¹æ³•(Structured Transition Evaluation Method, STEM)ã€‚STEM æ˜¯ä¸€ç§è½»é‡çº§ä¸”å¯è§£é‡Šçš„æ¡†æ¶ï¼Œé€šè¿‡è¯†åˆ«æ˜¾è‘—è½¬æ¢æ ·æœ¬(Significant Transition Samples, STS)æ¥é«˜æ•ˆä¼°ç®—æ¨¡å‹çš„ç›¸å¯¹èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ†æç›¸åŒæ¶æ„ä½†ä¸åŒå‚æ•°è§„æ¨¡çš„æ¨¡å‹é—´çš„æ€§èƒ½è½¬æ¢ï¼Œåˆ©ç”¨ STS æ± å®šä½æœªçŸ¥æ¨¡å‹çš„èƒ½åŠ›æ°´å¹³ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨ Qwen3 æ¨¡å‹ç³»åˆ—åœ¨å…­ä¸ªä»£è¡¨æ€§åŸºå‡†æµ‹è¯•ä¸Šæ„å»ºäº† STS æ± ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒSTEM èƒ½å¤Ÿå¯é åœ°æ•æ‰æ€§èƒ½è¶‹åŠ¿ï¼Œå¹¶ä¸æ¨¡å‹èƒ½åŠ›çš„çœŸå®æ’åé«˜åº¦ä¸€è‡´ã€‚è¯¥ç ”ç©¶ä¸ºç»†ç²’åº¦å’Œè·¨æ¶æ„çš„ LLMs èƒ½åŠ›è¯„ä¼°æä¾›äº†ä¸€ç§å®ç”¨ä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Submit to AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2508.12096v2",
      "published_date": "2025-08-16 16:36:43 UTC",
      "updated_date": "2025-08-20 09:52:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:13:57.786819+00:00"
    },
    {
      "arxiv_id": "2508.12087v2",
      "title": "MAPF-World: Action World Model for Multi-Agent Path Finding",
      "title_zh": "MAPF-Worldï¼šé¢å‘å¤šæ™ºèƒ½ä½“è·¯å¾„è§„åˆ’çš„åŠ¨ä½œä¸–ç•Œæ¨¡å‹",
      "authors": [
        "Zhanjiang Yang",
        "Yang Shen",
        "Yueming Li",
        "Meng Li",
        "Lijun Sun"
      ],
      "abstract": "Multi-agent path finding (MAPF) is the problem of planning conflict-free paths from the designated start locations to goal positions for multiple agents. It underlies a variety of real-world tasks, including multi-robot coordination, robot-assisted logistics, and social navigation. Recent decentralized learnable solvers have shown great promise for large-scale MAPF, especially when leveraging foundation models and large datasets. However, these agents are reactive policy models and exhibit limited modeling of environmental temporal dynamics and inter-agent dependencies, resulting in performance degradation in complex, long-term planning scenarios. To address these limitations, we propose MAPF-World, an autoregressive action world model for MAPF that unifies situation understanding and action generation, guiding decisions beyond immediate local observations. It improves situational awareness by explicitly modeling environmental dynamics, including spatial features and temporal dependencies, through future state and actions prediction. By incorporating these predicted futures, MAPF-World enables more informed, coordinated, and far-sighted decision-making, especially in complex multi-agent settings. Furthermore, we augment MAPF benchmarks by introducing an automatic map generator grounded in real-world scenarios, capturing practical map layouts for training and evaluating MAPF solvers. Extensive experiments demonstrate that MAPF-World outperforms state-of-the-art learnable solvers, showcasing superior zero-shot generalization to out-of-distribution cases. Notably, MAPF-World is trained with a 96.5% smaller model size and 92% reduced data.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MAPF-Worldï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹å¤šæ™ºèƒ½ä½“è·¯å¾„è§„åˆ’ (MAPF) çš„è‡ªå›å½’ Action World Modelï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ååº”å¼ç­–ç•¥æ¨¡å‹åœ¨å¤æ‚é•¿æœŸè§„åˆ’ä¸­å¯¹ç¯å¢ƒåŠ¨æ€å’Œæ™ºèƒ½ä½“é—´ä¾èµ–æ€§å»ºæ¨¡å—é™çš„é—®é¢˜ã€‚è¯¥æ¨¡å‹é€šè¿‡é¢„æµ‹æœªæ¥çŠ¶æ€å’ŒåŠ¨ä½œï¼Œæ˜¾å¼å»ºæ¨¡ç©ºé—´ç‰¹å¾ä¸æ—¶é—´ä¾èµ–ç­‰ç¯å¢ƒåŠ¨æ€ï¼Œå°†æƒ…å¢ƒç†è§£ä¸åŠ¨ä½œç”Ÿæˆç›¸ç»Ÿä¸€ï¼Œä»è€Œå®ç°æ›´å…·è¿œè§ä¸”åè°ƒçš„å†³ç­–ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†ä¸€ä¸ªåŸºäºçœŸå®åœºæ™¯çš„è‡ªåŠ¨åœ°å›¾ç”Ÿæˆå™¨ï¼Œç”¨äºæ•è·å®é™…å¸ƒå±€ä»¥ä¼˜åŒ– MAPF æ±‚è§£å™¨çš„è®­ç»ƒä¸è¯„ä¼°ã€‚å¤§é‡å®éªŒè¯æ˜ï¼ŒMAPF-World çš„è¡¨ç°ä¼˜äºç°æœ‰çš„å…ˆè¿›å­¦ä¹ å‹æ±‚è§£å™¨ï¼Œå¹¶å±•ç°å‡ºå“è¶Šçš„ Zero-shot Generalization èƒ½åŠ›ã€‚æ˜¾è‘—çš„æ˜¯ï¼ŒMAPF-World åœ¨æ¨¡å‹å‚æ•°é‡å‡å°‘ 96.5% ä¸”è®­ç»ƒæ•°æ®å‡å°‘ 92% çš„æƒ…å†µä¸‹ï¼Œä¾ç„¶å®ç°äº†æ€§èƒ½çš„è·¨è¶Šã€‚",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.12087v2",
      "published_date": "2025-08-16 15:50:26 UTC",
      "updated_date": "2025-09-07 04:05:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:14:07.179315+00:00"
    },
    {
      "arxiv_id": "2508.12086v1",
      "title": "J6: Jacobian-Driven Role Attribution for Multi-Objective Prompt Optimization in LLMs",
      "title_zh": "J6ï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹å¤šç›®æ ‡æç¤ºä¼˜åŒ–çš„é›…å¯æ¯”é©±åŠ¨è§’è‰²å½’å› ",
      "authors": [
        "Yao Wu"
      ],
      "abstract": "In large language model (LLM) adaptation, balancing multiple optimization objectives such as improving factuality (heat) and increasing confidence (via low entropy) poses a fundamental challenge, especially when prompt parameters (e.g., hidden-layer insertions h and embedding modifications w) interact in non-trivial ways. Existing multi-objective optimization strategies often rely on scalar gradient aggregation, ignoring the deeper geometric structure between objectives and parameters. We propose J6, a structured Jacobian-based method that decomposes the gradient interaction matrix into six interpretable components. This decomposition enables both hard decision-making (e.g., choosing the dominant update direction via argmax) and soft strategies (e.g., attention-style weighting via softmax over J6), forming a dynamic update framework that adapts to local conflict and synergy. Moreover, the interpretable structure of J6 provides insight into parameter attribution, task interference, and geometry-aligned adaptation. Our work introduces a principled and extensible mechanism for conflict-aware prompt optimization, and opens a new avenue for incorporating structured Jacobian reasoning into multi-objective neural tuning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† J6ï¼Œä¸€ç§åŸºäºé›…å¯æ¯”çŸ©é˜µé©±åŠ¨(Jacobian-Driven)çš„ç»“æ„åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLM)åœ¨å¤šç›®æ ‡æç¤ºä¼˜åŒ–ä¸­é¢ä¸´çš„ç›®æ ‡å¹³è¡¡ä¸å‚æ•°äº¤äº’æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„ä¼˜åŒ–ç­–ç•¥é€šå¸¸ä¾èµ–æ ‡é‡æ¢¯åº¦èšåˆï¼Œéš¾ä»¥å¤„ç†å¦‚æé«˜çœŸå®æ€§(factuality)ä¸å¢åŠ ç½®ä¿¡åº¦(confidence)ç­‰å¤šä¸ªç›¸äº’å†²çªç›®æ ‡ä¹‹é—´çš„å¤æ‚å‡ ä½•ç»“æ„ã€‚J6 é€šè¿‡å°†æ¢¯åº¦äº¤äº’çŸ©é˜µåˆ†è§£ä¸ºå…­ä¸ªå¯è§£é‡Šçš„ç»„ä»¶ï¼Œå®ç°äº†å¯¹å‚æ•°æ’å…¥(h)å’ŒåµŒå…¥ä¿®æ”¹(w)ç­‰ä¸åŒå‚æ•°è§’è‰²çš„ç²¾ç¡®å½’å±åˆ†æã€‚è¯¥æ¡†æ¶æ”¯æŒåŸºäº argmax çš„ç¡¬å†³ç­–å’ŒåŸºäº softmax æ³¨æ„åŠ›æƒé‡çš„è½¯ç­–ç•¥ï¼Œèƒ½å¤Ÿæ ¹æ®å±€éƒ¨å†²çªä¸ååŒä½œç”¨åŠ¨æ€è°ƒæ•´æ›´æ–°æ–¹å‘ã€‚è¿™ç§ç»“æ„åŒ–çš„ Jacobian æ¨ç†ä¸ä»…ä¸ºå†²çªæ„ŸçŸ¥(conflict-aware)çš„æç¤ºä¼˜åŒ–æä¾›äº†åŸåˆ™æ€§æœºåˆ¶ï¼Œè¿˜æ­ç¤ºäº†å‚æ•°å½’å±å’Œä»»åŠ¡å¹²æ‰°çš„æ·±å±‚å‡ ä½•è§„å¾‹ã€‚è¯¥å·¥ä½œä¸ºå¤šç›®æ ‡ç¥ç»ç½‘ç»œè°ƒä¼˜å¼€è¾Ÿäº†æ–°é€”å¾„ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å¤æ‚è‡ªé€‚åº”åœºæ™¯ä¸‹çš„ä¼˜åŒ–æ•ˆç‡ä¸å¯è§£é‡Šæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, 3 tables, 1 algorithm",
      "pdf_url": "https://arxiv.org/pdf/2508.12086v1",
      "published_date": "2025-08-16 15:47:47 UTC",
      "updated_date": "2025-08-16 15:47:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:14:22.957498+00:00"
    },
    {
      "arxiv_id": "2508.14100v1",
      "title": "Domain Translation of a Soft Robotic Arm using Conditional Cycle Generative Adversarial Network",
      "title_zh": "åŸºäºæ¡ä»¶å¾ªç¯ç”Ÿæˆå¯¹æŠ—ç½‘ç»œçš„è½¯ä½“æœºæ¢°è‡‚é¢†åŸŸè¿ç§»",
      "authors": [
        "Nilay Kushawaha",
        "Carlo Alessi",
        "Lorenzo Fruzzetti",
        "Egidio Falotico"
      ],
      "abstract": "Deep learning provides a powerful method for modeling the dynamics of soft robots, offering advantages over traditional analytical approaches that require precise knowledge of the robot's structure, material properties, and other physical characteristics. Given the inherent complexity and non-linearity of these systems, extracting such details can be challenging. The mappings learned in one domain cannot be directly transferred to another domain with different physical properties. This challenge is particularly relevant for soft robots, as their materials gradually degrade over time. In this paper, we introduce a domain translation framework based on a conditional cycle generative adversarial network (CCGAN) to enable knowledge transfer from a source domain to a target domain. Specifically, we employ a dynamic learning approach to adapt a pose controller trained in a standard simulation environment to a domain with tenfold increased viscosity. Our model learns from input pressure signals conditioned on corresponding end-effector positions and orientations in both domains. We evaluate our approach through trajectory-tracking experiments across five distinct shapes and further assess its robustness under noise perturbations and periodicity tests. The results demonstrate that CCGAN-GP effectively facilitates cross-domain skill transfer, paving the way for more adaptable and generalizable soft robotic controllers.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è½¯ä½“æœºå™¨äºº(soft robots)åŠ¨åŠ›å­¦å»ºæ¨¡ä¸­é¢ä¸´çš„éçº¿æ€§å¤æ‚æ€§ä»¥åŠæ¨¡å‹åœ¨ä¸åŒç‰©ç†å±æ€§é¢†åŸŸ(domain)é—´è¿ç§»å›°éš¾çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ¡ä»¶å¾ªç¯ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ(Conditional Cycle Generative Adversarial Network, CCGAN)çš„é¢†åŸŸè¿ç§»æ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åŠ¨æ€å­¦ä¹ æ–¹æ³•ï¼Œèƒ½å¤Ÿå°†æ ‡å‡†ä»¿çœŸç¯å¢ƒä¸­çš„ä½å§¿æ§åˆ¶å™¨æœ‰æ•ˆåœ°è¿ç§»è‡³ç²˜åº¦æå‡åå€çš„ç›®æ ‡é¢†åŸŸï¼Œé€šè¿‡è¾“å…¥å‹åŠ›ä¿¡å·ä¸æœ«ç«¯æ‰§è¡Œå™¨ä½å§¿çš„æ¡ä»¶æ˜ å°„å®ç°çŸ¥è¯†è½¬ç§»ã€‚ç ”ç©¶äººå‘˜åœ¨äº”ç§ä¸åŒå½¢çŠ¶çš„è½¨è¿¹è·Ÿè¸ªä»»åŠ¡ä¸­éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¿›ä¸€æ­¥é€šè¿‡å™ªå£°æ‰°åŠ¨å’Œå‘¨æœŸæ€§æµ‹è¯•è¯„ä¼°äº†ç³»ç»Ÿçš„é²æ£’æ€§ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒCCGAN-GP èƒ½å¤Ÿæ˜¾è‘—ä¿ƒè¿›è·¨é¢†åŸŸçš„æŠ€èƒ½è¿ç§»ï¼Œä¸ºæ„å»ºæ›´å…·é€‚åº”æ€§å’Œæ³›åŒ–èƒ½åŠ›çš„è½¯ä½“æœºå™¨äººæ§åˆ¶å™¨æä¾›äº†æœ‰æ•ˆé€”å¾„ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted at IEEE International Conference on Robotic Systems and Applications",
      "pdf_url": "https://arxiv.org/pdf/2508.14100v1",
      "published_date": "2025-08-16 15:47:35 UTC",
      "updated_date": "2025-08-16 15:47:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:14:24.456818+00:00"
    },
    {
      "arxiv_id": "2508.12084v1",
      "title": "Generic Event Boundary Detection via Denoising Diffusion",
      "title_zh": "åŸºäºå»å™ªæ‰©æ•£çš„é€šç”¨äº‹ä»¶è¾¹ç•Œæ£€æµ‹",
      "authors": [
        "Jaejun Hwang",
        "Dayoung Gong",
        "Manjin Kim",
        "Minsu Cho"
      ],
      "abstract": "Generic event boundary detection (GEBD) aims to identify natural boundaries in a video, segmenting it into distinct and meaningful chunks. Despite the inherent subjectivity of event boundaries, previous methods have focused on deterministic predictions, overlooking the diversity of plausible solutions. In this paper, we introduce a novel diffusion-based boundary detection model, dubbed DiffGEBD, that tackles the problem of GEBD from a generative perspective. The proposed model encodes relevant changes across adjacent frames via temporal self-similarity and then iteratively decodes random noise into plausible event boundaries being conditioned on the encoded features. Classifier-free guidance allows the degree of diversity to be controlled in denoising diffusion. In addition, we introduce a new evaluation metric to assess the quality of predictions considering both diversity and fidelity. Experiments show that our method achieves strong performance on two standard benchmarks, Kinetics-GEBD and TAPOS, generating diverse and plausible event boundaries.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é€šç”¨äº‹ä»¶è¾¹ç•Œæ£€æµ‹(Generic Event Boundary Detection, GEBD)ä¸­å­˜åœ¨çš„ä¸»è§‚æ€§æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºä»¥å¾€çš„ç¡®å®šæ€§é¢„æµ‹æ–¹æ³•å¿½è§†äº†äº‹ä»¶è¾¹ç•Œè§£çš„å¤šæ ·æ€§ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§åä¸º DiffGEBD çš„æ–°å‹æ‰©æ•£æ¨¡å‹ï¼Œä»ç”Ÿæˆå¼è§†è§’è§£å†³è¾¹ç•Œæ£€æµ‹é—®é¢˜ã€‚è¯¥æ¨¡å‹é€šè¿‡æ—¶é—´è‡ªç›¸ä¼¼æ€§(temporal self-similarity)ç¼–ç ç›¸é‚»å¸§é—´çš„ç›¸å…³å˜åŒ–ï¼Œå¹¶ä»¥ç¼–ç ç‰¹å¾ä¸ºæ¡ä»¶ï¼Œå°†éšæœºå™ªå£°è¿­ä»£è§£ç ä¸ºåˆç†çš„äº‹ä»¶è¾¹ç•Œã€‚ç ”ç©¶è¿˜åˆ©ç”¨æ— åˆ†ç±»å™¨å¼•å¯¼(classifier-free guidance)æŠ€æœ¯æ¥æ§åˆ¶å»å™ªæ‰©æ•£è¿‡ç¨‹ä¸­çš„å¤šæ ·æ€§ç¨‹åº¦ã€‚æ­¤å¤–ï¼Œè®ºæ–‡å¼•å…¥äº†ä¸€é¡¹å…¼é¡¾å¤šæ ·æ€§ä¸å¿ å®åº¦çš„æ–°è¯„ä¼°æŒ‡æ ‡ï¼Œç”¨äºæ›´å®¢è§‚åœ°è¡¡é‡é¢„æµ‹è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDiffGEBD åœ¨ Kinetics-GEBD å’Œ TAPOS ä¸¤ä¸ªæ ‡å‡†åŸºå‡†ä¸Šå‡å–å¾—äº†å¼ºåŠ²è¡¨ç°ï¼Œèƒ½å¤Ÿç”Ÿæˆæ—¢å¤šæ ·åˆåˆç†çš„äº‹ä»¶è¾¹ç•Œã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ICCV 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.12084v1",
      "published_date": "2025-08-16 15:44:34 UTC",
      "updated_date": "2025-08-16 15:44:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:14:29.889718+00:00"
    },
    {
      "arxiv_id": "2508.12082v2",
      "title": "Automated Model Evaluation for Object Detection via Prediction Consistency and Reliability",
      "title_zh": "åŸºäºé¢„æµ‹ä¸€è‡´æ€§ä¸å¯é æ€§çš„ç›®æ ‡æ£€æµ‹æ¨¡å‹è‡ªåŠ¨åŒ–è¯„ä¼°",
      "authors": [
        "Seungju Yoo",
        "Hyuk Kwon",
        "Joong-Won Hwang",
        "Kibok Lee"
      ],
      "abstract": "Recent advances in computer vision have made training object detectors more efficient and effective; however, assessing their performance in real-world applications still relies on costly manual annotation. To address this limitation, we develop an automated model evaluation (AutoEval) framework for object detection. We propose Prediction Consistency and Reliability (PCR), which leverages the multiple candidate bounding boxes that conventional detectors generate before non-maximum suppression (NMS). PCR estimates detection performance without ground-truth labels by jointly measuring 1) the spatial consistency between boxes before and after NMS, and 2) the reliability of the retained boxes via the confidence scores of overlapping boxes. For a more realistic and scalable evaluation, we construct a meta-dataset by applying image corruptions of varying severity. Experimental results demonstrate that PCR yields more accurate performance estimates than existing AutoEval methods, and the proposed meta-dataset covers a wider range of detection performance. The code is available at https://github.com/YonseiML/autoeval-det.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç›®æ ‡æ£€æµ‹(Object Detection)æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­ä¾èµ–æ˜‚è´µçš„äººå·¥æ ‡æ³¨è¿›è¡Œæ€§èƒ½è¯„ä¼°çš„é—®é¢˜ï¼Œå¼€å‘äº†ä¸€ç§è‡ªåŠ¨åŒ–æ¨¡å‹è¯„ä¼°(AutoEval)æ¡†æ¶ã€‚è¯¥æ¡†æ¶æå‡ºäº†åŸºäºé¢„æµ‹ä¸€è‡´æ€§ä¸å¯é æ€§(Prediction Consistency and Reliability, PCR)çš„è¯„ä¼°æ–¹æ³•ï¼Œæ—¨åœ¨æ— éœ€Ground-truthæ ‡ç­¾çš„æƒ…å†µä¸‹ä¼°è®¡æ£€æµ‹æ€§èƒ½ã€‚PCRåˆ©ç”¨ä¼ ç»Ÿæ£€æµ‹å™¨åœ¨éæå¤§å€¼æŠ‘åˆ¶(Non-Maximum Suppression, NMS)ä¹‹å‰ç”Ÿæˆçš„å¤šä¸ªå€™é€‰è¾¹ç•Œæ¡†ï¼Œé€šè¿‡è”åˆæµ‹é‡NMSå‰åè¾¹ç•Œæ¡†çš„ç©ºé—´ä¸€è‡´æ€§ä»¥åŠåˆ©ç”¨é‡å æ¡†ç½®ä¿¡åº¦å¾—åˆ†è¡¡é‡çš„å¯é æ€§æ¥è¿›è¡Œæ€§èƒ½ä¼°è®¡ã€‚ä¸ºå®ç°æ›´å…·æ‰©å±•æ€§çš„è¯„ä¼°ï¼Œç ”ç©¶è€…é€šè¿‡åº”ç”¨ä¸åŒç¨‹åº¦çš„å›¾åƒæŸåæ„å»ºäº†ä¸€ä¸ªå…ƒæ•°æ®é›†(Meta-dataset)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPCRæ¯”ç°æœ‰çš„AutoEvalæ–¹æ³•èƒ½æä¾›æ›´å‡†ç¡®çš„æ€§èƒ½ä¼°è®¡ï¼Œä¸”æ‰€æå‡ºçš„å…ƒæ•°æ®é›†è¦†ç›–äº†æ›´å¹¿æ³›çš„æ£€æµ‹æ€§èƒ½èŒƒå›´ã€‚è¯¥å·¥ä½œä¸ºåœ¨ç¼ºä¹æ ‡æ³¨çš„ç°å®åœºæ™¯ä¸­é«˜æ•ˆè¯„ä¼°ç›®æ ‡æ£€æµ‹æ¨¡å‹æä¾›äº†å¯é çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "ICCV 2025 Oral; v2: fixed a typo in the title and updated experimental results",
      "pdf_url": "https://arxiv.org/pdf/2508.12082v2",
      "published_date": "2025-08-16 15:39:56 UTC",
      "updated_date": "2025-10-02 15:44:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:14:31.181420+00:00"
    },
    {
      "arxiv_id": "2508.12081v2",
      "title": "VimoRAG: Video-based Retrieval-augmented 3D Motion Generation for Motion Language Models",
      "title_zh": "VimoRAGï¼šé¢å‘åŠ¨ä½œè¯­è¨€æ¨¡å‹çš„åŸºäºè§†é¢‘æ£€ç´¢å¢å¼ºçš„ 3D åŠ¨ä½œç”Ÿæˆ",
      "authors": [
        "Haidong Xu",
        "Guangwei Xu",
        "Zhedong Zheng",
        "Xiatian Zhu",
        "Wei Ji",
        "Xiangtai Li",
        "Ruijie Guo",
        "Meishan Zhang",
        "Min zhang",
        "Hao Fei"
      ],
      "abstract": "This paper introduces VimoRAG, a novel video-based retrieval-augmented motion generation framework for motion large language models (LLMs). As motion LLMs face severe out-of-domain/out-of-vocabulary issues due to limited annotated data, VimoRAG leverages large-scale in-the-wild video databases to enhance 3D motion generation by retrieving relevant 2D human motion signals. While video-based motion RAG is nontrivial, we address two key bottlenecks: (1) developing an effective motion-centered video retrieval model that distinguishes human poses and actions, and (2) mitigating the issue of error propagation caused by suboptimal retrieval results. We design the Gemini Motion Video Retriever mechanism and the Motion-centric Dual-alignment DPO Trainer, enabling effective retrieval and generation processes. Experimental results show that VimoRAG significantly boosts the performance of motion LLMs constrained to text-only input. All the resources are available at https://walkermitty.github.io/VimoRAG/",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†VimoRAGï¼Œè¿™æ˜¯ä¸€ç§é¢å‘è¿åŠ¨å¤§è¯­è¨€æ¨¡å‹(Motion LLMs)çš„æ–°å‹åŸºäºè§†é¢‘æ£€ç´¢å¢å¼ºçš„3Dè¿åŠ¨ç”Ÿæˆæ¡†æ¶ã€‚é’ˆå¯¹Motion LLMså› æ ‡æ³¨æ•°æ®æœ‰é™è€Œé¢ä¸´çš„ä¸¥é‡åŸŸå¤–(out-of-domain)å’Œè¯æ±‡è¡¨å¤–(out-of-vocabulary)é—®é¢˜ï¼Œè¯¥æ¡†æ¶é€šè¿‡åˆ©ç”¨å¤§è§„æ¨¡é‡å¤–è§†é¢‘æ•°æ®åº“æ£€ç´¢ç›¸å…³çš„2Däººä½“è¿åŠ¨ä¿¡å·æ¥å¢å¼º3Dè¿åŠ¨ç”Ÿæˆã€‚ä¸ºäº†è§£å†³è§†é¢‘æ£€ç´¢ä¸­çš„äººä½“å§¿æ€è¾¨è¯†å’Œæ£€ç´¢è¯¯å·®ä¼ æ’­ç­‰æ ¸å¿ƒç“¶é¢ˆï¼Œç ”ç©¶å›¢é˜Ÿè®¾è®¡äº†Gemini Motion Video Retrieveræœºåˆ¶å’Œä»¥è¿åŠ¨ä¸ºä¸­å¿ƒçš„åŒå¯¹é½DPOè®­ç»ƒå™¨(Motion-centric Dual-alignment DPO Trainer)ã€‚è¿™äº›åˆ›æ–°è®¾è®¡ç¡®ä¿äº†æ£€ç´¢ä¸ç”Ÿæˆè¿‡ç¨‹çš„é«˜æ•ˆå¯¹é½ï¼Œæ˜¾è‘—æå‡äº†ä»…æ”¯æŒæ–‡æœ¬è¾“å…¥çš„è¿åŠ¨å¤§è¯­è¨€æ¨¡å‹åœ¨å¤æ‚åŠ¨ä½œç”Ÿæˆä¸Šçš„æ€§èƒ½ã€‚å®éªŒç»“æœè¯æ˜äº†VimoRAGåœ¨å¤„ç†å¤šæ ·åŒ–è¿åŠ¨ç”Ÿæˆä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§ï¼Œä¸ºå…‹æœè¿åŠ¨é¢†åŸŸæ ‡æ³¨æ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by NeurIPS 2025; Project Page: https://walkermitty.github.io/VimoRAG",
      "pdf_url": "https://arxiv.org/pdf/2508.12081v2",
      "published_date": "2025-08-16 15:31:14 UTC",
      "updated_date": "2025-10-20 14:52:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:14:32.662124+00:00"
    },
    {
      "arxiv_id": "2508.13209v2",
      "title": "Research on Conversational Recommender System Considering Consumer Types",
      "title_zh": "è€ƒè™‘æ¶ˆè´¹è€…ç±»å‹çš„å¯¹è¯å¼æ¨èç³»ç»Ÿç ”ç©¶",
      "authors": [
        "Yaying Luo",
        "Hui Fang",
        "Zhu Sun"
      ],
      "abstract": "Conversational Recommender Systems (CRS) provide personalized services through multi-turn interactions, yet most existing methods overlook users' heterogeneous decision-making styles and knowledge levels, which constrains both accuracy and efficiency. To address this gap, we propose CT-CRS (Consumer Type-Enhanced Conversational Recommender System), a framework that integrates consumer type modeling into dialogue recommendation. Based on consumer type theory, we define four user categories--dependent, efficient, cautious, and expert--derived from two dimensions: decision-making style (maximizers vs. satisficers) and knowledge level (high vs. low). CT-CRS employs interaction histories and fine-tunes the large language model to automatically infer user types in real time, avoiding reliance on static questionnaires. We incorporate user types into state representation and design a type-adaptive policy that dynamically adjusts recommendation granularity, diversity, and attribute query complexity. To further optimize the dialogue policy, we adopt Inverse Reinforcement Learning (IRL), enabling the agent to approximate expert-like strategies conditioned on consumer type. Experiments on LastFM, Amazon-Book, and Yelp show that CTCRS improves recommendation success rate and reduces interaction turns compared to strong baselines. Ablation studies confirm that both consumer type modeling and IRL contribute significantly to performance gains. These results demonstrate that CT-CRS offers a scalable and interpretable solution for enhancing CRS personalization through the integration of psychological modeling and advanced policy optimization.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰å¯¹è¯å¼æ¨èç³»ç»Ÿ(CRS)å¿½ç•¥ç”¨æˆ·å¼‚æ„å†³ç­–é£æ ¼å’ŒçŸ¥è¯†æ°´å¹³çš„é—®é¢˜ï¼Œæå‡ºäº†CT-CRS(Consumer Type-Enhanced Conversational Recommender System)æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç»“åˆæ¶ˆè´¹ç±»å‹ç†è®ºï¼Œä»å†³ç­–é£æ ¼(maximizers vs. satisficers)å’ŒçŸ¥è¯†æ°´å¹³ä¸¤ä¸ªç»´åº¦å®šä¹‰äº†ä¾èµ–å‹ã€æ•ˆç‡å‹ã€è°¨æ…å‹å’Œä¸“å®¶å‹å››ç±»ç”¨æˆ·ã€‚CT-CRSåˆ©ç”¨äº¤äº’å†å²å¹¶å¾®è°ƒå¤§è¯­è¨€æ¨¡å‹(LLM)ä»¥å®æ—¶è‡ªåŠ¨æ¨æ–­ç”¨æˆ·ç±»å‹ï¼Œé¿å…äº†å¯¹é™æ€é—®å·çš„ä¾èµ–ã€‚é€šè¿‡å¼•å…¥ç±»å‹è‡ªé€‚åº”ç­–ç•¥å¹¶é‡‡ç”¨é€†å¼ºåŒ–å­¦ä¹ (Inverse Reinforcement Learning, IRL)æ¥ä¼˜åŒ–å¯¹è¯ç­–ç•¥ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿæ ¹æ®ä¸åŒæ¶ˆè´¹ç±»å‹æ¨¡æ‹Ÿä¸“å®¶çº§ç­–ç•¥ã€‚åœ¨LastFMã€Amazon-Bookå’ŒYelpæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCT-CRSç›¸æ¯”åŸºå‡†æ¨¡å‹æ˜¾è‘—æé«˜äº†æ¨èæˆåŠŸç‡å¹¶å‡å°‘äº†äº¤äº’è½®æ•°ã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥è¯å®äº†æ¶ˆè´¹ç±»å‹å»ºæ¨¡å’ŒIRLå¯¹æ€§èƒ½æå‡çš„è´¡çŒ®ï¼Œä¸ºé€šè¿‡æ•´åˆå¿ƒç†å»ºæ¨¡å’Œé«˜çº§ç­–ç•¥ä¼˜åŒ–æ¥å¢å¼ºCRSä¸ªæ€§åŒ–æä¾›äº†å¯æ‰©å±•ä¸”å…·è§£é‡Šæ€§çš„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "cs.IR",
      "comment": "The tables Recommendation strategies for different consumer types need to be modified. Correspondence of Recommendation strategies are incorrect",
      "pdf_url": "https://arxiv.org/pdf/2508.13209v2",
      "published_date": "2025-08-16 15:15:52 UTC",
      "updated_date": "2025-09-09 02:13:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:14:34.568302+00:00"
    },
    {
      "arxiv_id": "2508.12063v3",
      "title": "Generalized invariants meet constitutive neural networks: A novel framework for hyperelastic materials",
      "title_zh": "å¹¿ä¹‰ä¸å˜é‡ä¸æœ¬æ„ç¥ç»ç½‘ç»œçš„èåˆï¼šä¸€ç§æ–°å‹è¶…å¼¹æ€§ææ–™å»ºæ¨¡æ¡†æ¶",
      "authors": [
        "Denisa MartonovÃ¡",
        "Alain Goriely",
        "Ellen Kuhl"
      ],
      "abstract": "The major challenge in determining a hyperelastic model for a given material is the choice of invariants and the selection how the strain energy function depends functionally on these invariants. Here we introduce a new data-driven framework that simultaneously discovers appropriate invariants and constitutive models for isotropic incompressible hyperelastic materials. Our approach identifies both the most suitable invariants in a class of generalized invariants and the corresponding strain energy function directly from experimental observations. Unlike previous methods that rely on fixed invariant choices or sequential fitting procedures, our method integrates the discovery process into a single neural network architecture. By looking at a continuous family of possible invariants, the model can flexibly adapt to different material behaviors. We demonstrate the effectiveness of this approach using popular benchmark datasets for rubber and brain tissue. For rubber, the method recovers a stretch-dominated formulation consistent with classical models. For brain tissue, it identifies a formulation sensitive to small stretches, capturing the nonlinear shear response characteristic of soft biological matter. Compared to traditional and neural-network-based models, our framework provides improved predictive accuracy and interpretability across a wide range of deformation states. This unified strategy offers a robust tool for automated and physically meaningful model discovery in hyperelasticity.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç»“åˆ Generalized Invariants ä¸ Constitutive Neural Networks çš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨ä¸ºå„å‘åŒæ€§ä¸å¯å‹ç¼© Hyperelastic Materials åŒæ—¶è‡ªåŠ¨å‘ç°åˆé€‚çš„ä¸å˜é‡å’Œæœ¬æ„æ¨¡å‹ã€‚ä¸åŒäºä¾èµ–å›ºå®šä¸å˜é‡æˆ–é¡ºåºæ‹Ÿåˆçš„ä¼ ç»Ÿæ–¹æ³•ï¼Œè¯¥æ¡†æ¶å°†ä¸å˜é‡çš„é€‰æ‹©ä¸ Strain Energy Function çš„ç¡®å®šæ•´åˆè¿›å•ä¸€ç¥ç»ç½‘ç»œæ¶æ„ä¸­ï¼Œé€šè¿‡æ¢ç´¢è¿ç»­çš„ä¸å˜é‡æ—çµæ´»é€‚åº”ä¸åŒçš„ææ–™è¡Œä¸ºã€‚å®éªŒé€šè¿‡æ©¡èƒ¶å’Œè„‘ç»„ç»‡çš„åŸºå‡†æ•°æ®é›†éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œåœ¨æ©¡èƒ¶ææ–™ä¸­è¿˜åŸäº†ç»å…¸æ¨¡å‹çš„æ‹‰ä¼¸ä¸»å¯¼ç‰¹å¾ï¼Œå¹¶åœ¨è„‘ç»„ç»‡ä¸­å‡†ç¡®æ•æ‰åˆ°äº†è½¯ç”Ÿç‰©ç»„ç»‡ç‰¹æœ‰çš„éçº¿æ€§å‰ªåˆ‡å“åº”ã€‚ä¸ä¼ ç»ŸåŠå…¶ä»–åŸºäºç¥ç»ç½‘ç»œçš„æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶åœ¨å¤šç§å˜å½¢çŠ¶æ€ä¸‹å±•ç°å‡ºæ›´é«˜çš„é¢„æµ‹å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ï¼Œä¸ºè¶…å¼¹æ€§åŠ›å­¦é¢†åŸŸçš„è‡ªåŠ¨åŒ–å’Œå…·æœ‰ç‰©ç†æ„ä¹‰çš„æ¨¡å‹å‘ç°æä¾›äº†é²æ£’å·¥å…·ã€‚",
      "categories": [
        "cond-mat.soft",
        "cs.AI"
      ],
      "primary_category": "cond-mat.soft",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.12063v3",
      "published_date": "2025-08-16 14:32:45 UTC",
      "updated_date": "2025-09-17 22:51:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:14:42.363320+00:00"
    },
    {
      "arxiv_id": "2508.12045v2",
      "title": "Large Language Models Enable Design of Personalized Nudges across Cultures",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹åŠ©åŠ›è·¨æ–‡åŒ–ä¸ªæ€§åŒ–åŠ©æ¨è®¾è®¡",
      "authors": [
        "Vladimir Maksimenko",
        "Qingyao Xin",
        "Prateek Gupta",
        "Bin Zhang",
        "Prateek Bansal"
      ],
      "abstract": "Nudge strategies are effective tools for influencing behaviour, but their impact depends on individual preferences. Strategies that work for some individuals may be counterproductive for others. We hypothesize that large language models (LLMs) can facilitate the design of individual-specific nudges without the need for costly and time-intensive behavioural data collection and modelling. To test this, we use LLMs to design personalized decoy-based nudges tailored to individual profiles and cultural contexts, aimed at encouraging air travellers to voluntarily offset CO$_2$ emissions from flights. We evaluate their effectiveness through a large-scale survey experiment ($n=3495$) conducted across five countries. Results show that LLM-informed personalized nudges are more effective than uniform settings, raising offsetting rates by 3-7$\\%$ in Germany, Singapore, and the US, though not in China or India. Our study highlights the potential of LLM as a low-cost testbed for piloting nudge strategies. At the same time, cultural heterogeneity constrains their generalizability underscoring the need for combining LLM-based simulations with targeted empirical validation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)è®¾è®¡ä¸ªæ€§åŒ–åŠ©æ¨(Nudge)ç­–ç•¥çš„å¯èƒ½æ€§ï¼Œæ—¨åœ¨æ— éœ€é«˜æˆæœ¬è¡Œä¸ºæ•°æ®çš„æƒ…å†µä¸‹ç²¾å‡†å½±å“ä¸ªä½“è¡Œä¸ºã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨ LLMs é’ˆå¯¹ä¸ªä½“ç”»åƒå’Œæ–‡åŒ–èƒŒæ™¯å®šåˆ¶äº†åŸºäºè¯±é¥µæ•ˆåº”(Decoy-based)çš„åŠ©æ¨æ–¹æ¡ˆï¼Œä»¥é¼“åŠ±èˆªç©ºæ—…å®¢è‡ªæ„¿è¿›è¡ŒäºŒæ°§åŒ–ç¢³æ’æ”¾æŠµæ¶ˆã€‚é€šè¿‡å¯¹äº”ä¸ªå›½å®¶å…±3495åå—è®¿è€…è¿›è¡Œçš„å¤§è§„æ¨¡å®éªŒè¯„ä¼°ï¼Œç»“æœæ˜¾ç¤º LLM é©±åŠ¨çš„ä¸ªæ€§åŒ–åŠ©æ¨åœ¨å¾·å›½ã€æ–°åŠ å¡å’Œç¾å›½æ¯”ç»Ÿä¸€è®¾ç½®æ›´æœ‰æ•ˆï¼Œä½¿æŠµæ¶ˆç‡æé«˜äº†3-7%ã€‚ç„¶è€Œï¼Œè¯¥ç­–ç•¥åœ¨ä¸­å›½å’Œå°åº¦å¹¶æœªè¡¨ç°å‡ºæ˜¾è‘—æå‡ï¼Œåæ˜ å‡ºæ–‡åŒ–å¼‚è´¨æ€§(Cultural Heterogeneity)å¯¹æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„çº¦æŸã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥ç ”ç©¶è¯æ˜äº† LLMs å¯ä»¥ä½œä¸ºæµ‹è¯•å’Œå¼€å‘åŠ©æ¨ç­–ç•¥çš„ä½æˆæœ¬å¹³å°ï¼ŒåŒæ—¶ä¹Ÿå¼ºè°ƒäº†åœ¨åº”ç”¨ä¸­ç»“åˆé’ˆå¯¹æ€§å®è¯éªŒè¯çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.12045v2",
      "published_date": "2025-08-16 13:40:44 UTC",
      "updated_date": "2025-10-16 12:33:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:14:41.761633+00:00"
    },
    {
      "arxiv_id": "2508.12040v1",
      "title": "Mind the Generation Process: Fine-Grained Confidence Estimation During LLM Generation",
      "title_zh": "å…³æ³¨ç”Ÿæˆè¿‡ç¨‹ï¼šLLM ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ç»†ç²’åº¦ç½®ä¿¡åº¦ä¼°è®¡",
      "authors": [
        "Jinyi Han",
        "Tingyun Li",
        "Shisong Chen",
        "Jie Shi",
        "Xinyi Wang",
        "Guanglei Yue",
        "Jiaqing Liang",
        "Xin Lin",
        "Liqian Wen",
        "Zulong Chen",
        "Yanghua Xiao"
      ],
      "abstract": "While large language models (LLMs) have demonstrated remarkable performance across diverse tasks, they fundamentally lack self-awareness and frequently exhibit overconfidence, assigning high confidence scores to incorrect predictions. Accurate confidence estimation is therefore critical for enhancing the trustworthiness and reliability of LLM-generated outputs. However, existing approaches suffer from coarse-grained scoring mechanisms that fail to provide fine-grained, continuous confidence estimates throughout the generation process. To address these limitations, we introduce FineCE, a novel confidence estimation method that delivers accurate, fine-grained confidence scores during text generation. Specifically, we first develop a comprehensive pipeline for constructing training data that effectively captures the underlying probabilistic distribution of LLM responses, and then train a model to predict confidence scores for arbitrary text sequences in a supervised manner. Furthermore, we propose a Backward Confidence Integration (BCI) strategy that leverages information from the subsequent text to enhance confidence estimation for the current sequence during inference. We also introduce three strategies for identifying optimal positions to perform confidence estimation within the generation process. Extensive experiments on multiple benchmark datasets demonstrate that FineCE consistently outperforms existing classical confidence estimation methods. Our code and all baselines used in the paper are available on GitHub.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)ç¼ºä¹è‡ªæˆ‘æ„è¯†å’Œç»å¸¸äº§ç”Ÿè¿‡åº¦è‡ªä¿¡(overconfidence)é”™è¯¯çš„é—®é¢˜ï¼Œæå‡ºäº†FineCEï¼Œä¸€ç§åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æä¾›ç»†ç²’åº¦ã€è¿ç»­ç½®ä¿¡åº¦è¯„åˆ†çš„æ–°å‹è¯„ä¼°æ–¹æ³•ã€‚å¼€å‘å›¢é˜Ÿé¦–å…ˆæ„å»ºäº†ä¸€å¥—ç»¼åˆæµç¨‹æ¥ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œä»¥æ•æ‰LLMå“åº”çš„æ½œåœ¨æ¦‚ç‡åˆ†å¸ƒï¼Œå¹¶ä»¥ç›‘ç£å­¦ä¹ çš„æ–¹å¼è®­ç»ƒæ¨¡å‹é¢„æµ‹ä»»æ„æ–‡æœ¬åºåˆ—çš„ç½®ä¿¡åº¦ã€‚ç ”ç©¶è¿›ä¸€æ­¥æå‡ºäº†åå‘ç½®ä¿¡åº¦æ•´åˆ(Backward Confidence Integration, BCI)ç­–ç•¥ï¼Œé€šè¿‡åˆ©ç”¨åç»­æ–‡æœ¬çš„ä¿¡æ¯æ¥å¢å¼ºæ¨ç†é˜¶æ®µå¯¹å½“å‰åºåˆ—çš„ç½®ä¿¡åº¦ä¼°è®¡ã€‚æ­¤å¤–ï¼Œæ–‡ä¸­è¿˜å¼•å…¥äº†ä¸‰ç§ç­–ç•¥ç”¨äºè¯†åˆ«ç”Ÿæˆè¿‡ç¨‹ä¸­è¿›è¡Œç½®ä¿¡åº¦è¯„ä¼°çš„æœ€ä½³ä½ç½®ã€‚åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒFineCEçš„è¡¨ç°ä¸€è‡´ä¼˜äºç°æœ‰çš„ç»å…¸ç½®ä¿¡åº¦è¯„ä¼°æ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹è¾“å‡ºçš„å¯ä¿¡åº¦ä¸å¯é æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "The initial versin was made in August 2024",
      "pdf_url": "https://arxiv.org/pdf/2508.12040v1",
      "published_date": "2025-08-16 13:29:35 UTC",
      "updated_date": "2025-08-16 13:29:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:14:43.064964+00:00"
    },
    {
      "arxiv_id": "2508.12036v1",
      "title": "Q-FSRU: Quantum-Augmented Frequency-Spectral Fusion for Medical Visual Question Answering",
      "title_zh": "Q-FSRUï¼šé¢å‘åŒ»å­¦è§†è§‰é—®ç­”çš„é‡å­å¢å¼ºé¢‘è°±èåˆ",
      "authors": [
        "Rakesh Thakur",
        "Yusra Tariq"
      ],
      "abstract": "Solving tough clinical questions that require both image and text understanding is still a major challenge in healthcare AI. In this work, we propose Q-FSRU, a new model that combines Frequency Spectrum Representation and Fusion (FSRU) with a method called Quantum Retrieval-Augmented Generation (Quantum RAG) for medical Visual Question Answering (VQA). The model takes in features from medical images and related text, then shifts them into the frequency domain using Fast Fourier Transform (FFT). This helps it focus on more meaningful data and filter out noise or less useful information. To improve accuracy and ensure that answers are based on real knowledge, we add a quantum-inspired retrieval system. It fetches useful medical facts from external sources using quantum-based similarity techniques. These details are then merged with the frequency-based features for stronger reasoning. We evaluated our model using the VQA-RAD dataset, which includes real radiology images and questions. The results showed that Q-FSRU outperforms earlier models, especially on complex cases needing image-text reasoning. The mix of frequency and quantum information improves both performance and explainability. Overall, this approach offers a promising way to build smart, clear, and helpful AI tools for doctors.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Q-FSRU æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³åŒ»ç–—è§†è§‰é—®ç­” (Medical Visual Question Answering) ä¸­å›¾åƒä¸æ–‡æœ¬ååŒç†è§£çš„å¤æ‚ä¸´åºŠæŒ‘æˆ˜ã€‚è¯¥æ¨¡å‹æ ¸å¿ƒé‡‡ç”¨äº†é¢‘åŸŸè¡¨ç¤ºä¸èåˆ (Frequency Spectrum Representation and Fusion, FSRU) æŠ€æœ¯ï¼Œé€šè¿‡å¿«é€Ÿå‚…é‡Œå¶å˜æ¢ (Fast Fourier Transform, FFT) å°†ç‰¹å¾è½¬æ¢è‡³é¢‘åŸŸä»¥è¿‡æ»¤å™ªå£°å¹¶èšç„¦å…³é”®ä¿¡æ¯ã€‚ä¸ºäº†å¢å¼ºæ¨ç†çš„å‡†ç¡®æ€§ä¸çŸ¥è¯†ä¾æ®ï¼Œç ”ç©¶å¼•å…¥äº†é‡å­æ£€ç´¢å¢å¼ºç”Ÿæˆ (Quantum Retrieval-Augmented Generation, Quantum RAG) ç³»ç»Ÿï¼Œåˆ©ç”¨é‡å­ç›¸ä¼¼åº¦æŠ€æœ¯ä»å¤–éƒ¨æºè·å–åŒ»ç–—äº‹å®å¹¶ä¸é¢‘åŸŸç‰¹å¾æ·±åº¦æ•´åˆã€‚åœ¨ VQA-RAD æ”¾å°„å­¦æ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒQ-FSRU åœ¨å¤„ç†å¤æ‚çš„å›¾åƒ-æ–‡æœ¬æ¨ç†ä»»åŠ¡æ—¶è¡¨ç°ä¼˜äºç°æœ‰æ¨¡å‹ã€‚è¿™ç§ç»“åˆé¢‘åŸŸåˆ†æä¸é‡å­ä¿¡æ¯çš„æ–¹æ³•æ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ€§èƒ½ä¸å¯è§£é‡Šæ€§ (explainability)ï¼Œä¸ºå¼€å‘æ™ºèƒ½ä¸”é€æ˜çš„ä¸´åºŠè¾…åŠ© AI å·¥å…·æä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 4 figures Submitted to AAAI 26",
      "pdf_url": "https://arxiv.org/pdf/2508.12036v1",
      "published_date": "2025-08-16 13:21:49 UTC",
      "updated_date": "2025-08-16 13:21:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:14:47.667891+00:00"
    },
    {
      "arxiv_id": "2508.12029v3",
      "title": "BConformeR: A Conformer Based on Mutual Sampling for Unified Prediction of Continuous and Discontinuous Antibody Binding Sites",
      "title_zh": "BConformeRï¼šåŸºäºäº’é‡‡æ ·çš„ Conformer æ¨¡å‹ï¼Œç”¨äºè¿ç»­ä¸éè¿ç»­æŠ—ä½“ç»“åˆä½ç‚¹çš„ç»Ÿä¸€é¢„æµ‹",
      "authors": [
        "Zhangyu You",
        "Jiahao Ma",
        "Hongzong Li",
        "Ye-Fan Hu",
        "Jian-Dong Huang"
      ],
      "abstract": "Accurate prediction of antibody-binding sites (epitopes) on antigens is crucial for vaccine design, immunodiagnostics, therapeutic antibody development, antibody engineering, research into autoimmune and allergic diseases, and advancing our understanding of immune responses. Despite in silico methods that have been proposed to predict both linear (continuous) and conformational (discontinuous) epitopes, they consistently underperform in predicting conformational epitopes. In this work, we propose Conformer-based models trained separately on AlphaFold-predicted structures and experimentally determined structures, leveraging convolutional neural networks (CNNs) to extract local features and Transformers to capture long-range dependencies within antigen sequences. Ablation studies demonstrate that CNN enhances the prediction of linear epitopes, and the Transformer module improves the prediction of conformational epitopes. Experimental results show that our model outperforms existing baselines in terms of MCC, ROC-AUC, PR-AUC, and F1 scores on both linear and conformational epitopes.",
      "tldr_zh": "å‡†ç¡®é¢„æµ‹æŠ—åŸä¸Šçš„æŠ—ä½“ç»“åˆä½ç‚¹(epitopes)å¯¹äºç–«è‹—è®¾è®¡å’Œæ²»ç–—æ€§æŠ—ä½“å¼€å‘è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰è®¡ç®—æ–¹æ³•åœ¨é¢„æµ‹æ„è±¡å‹(discontinuous)æŠ—åŸå†³å®šç°‡æ–¹é¢è¡¨ç°æŒç»­æ¬ ä½³ã€‚è¯¥ç ”ç©¶æå‡ºäº†BConformeRï¼Œä¸€ç§åŸºäºConformeræ¶æ„çš„æ¨¡å‹ï¼Œåˆ†åˆ«åœ¨AlphaFoldé¢„æµ‹ç»“æ„å’Œå®éªŒç¡®å®šçš„ç»“æ„ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ—¨åœ¨å®ç°è¿ç»­å’Œéè¿ç»­æŠ—ä½“ç»“åˆä½ç‚¹çš„ç»Ÿä¸€é¢„æµ‹ã€‚è¯¥æ¨¡å‹åˆ©ç”¨å·ç§¯ç¥ç»ç½‘ç»œ(CNNs)æå–å±€éƒ¨ç‰¹å¾ï¼Œå¹¶ç»“åˆTransformersæ•è·æŠ—åŸåºåˆ—ä¸­çš„é•¿ç¨‹ä¾èµ–å…³ç³»ã€‚æ¶ˆèå®éªŒ(Ablation studies)è¯æ˜ï¼ŒCNNæ¨¡å—å¢å¼ºäº†çº¿æ€§(linear)æŠ—åŸå†³å®šç°‡çš„é¢„æµ‹èƒ½åŠ›ï¼Œè€ŒTransformeræ¨¡å—åˆ™æ˜¾è‘—æå‡äº†æ„è±¡å‹æŠ—åŸå†³å®šç°‡çš„é¢„æµ‹æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBConformeRåœ¨MCCã€ROC-AUCã€PR-AUCå’ŒF1ç­‰å…³é”®è¯„ä¼°æŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰çš„åŸºçº¿æ¨¡å‹ï¼Œä¸ºæŠ—ä½“å·¥ç¨‹å’Œå…ç–«ååº”ç ”ç©¶æä¾›äº†æ›´ç²¾ç¡®çš„å·¥å…·ã€‚",
      "categories": [
        "q-bio.BM",
        "cs.AI",
        "cs.CE",
        "cs.LG"
      ],
      "primary_category": "q-bio.BM",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.12029v3",
      "published_date": "2025-08-16 12:31:39 UTC",
      "updated_date": "2025-12-23 13:32:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:15:04.695017+00:00"
    },
    {
      "arxiv_id": "2508.12027v1",
      "title": "Active inference for action-unaware agents",
      "title_zh": "é’ˆå¯¹æ— åŠ¨ä½œæ„ŸçŸ¥æ™ºèƒ½ä½“çš„ä¸»åŠ¨æ¨ç†",
      "authors": [
        "Filippo Torresan",
        "Keisuke Suzuki",
        "Ryota Kanai",
        "Manuel Baltieri"
      ],
      "abstract": "Active inference is a formal approach to study cognition based on the notion that adaptive agents can be seen as engaging in a process of approximate Bayesian inference, via the minimisation of variational and expected free energies. Minimising the former provides an account of perceptual processes and learning as evidence accumulation, while minimising the latter describes how agents select their actions over time. In this way, adaptive agents are able to maximise the likelihood of preferred observations or states, given a generative model of the environment. In the literature, however, different strategies have been proposed to describe how agents can plan their future actions. While they all share the notion that some kind of expected free energy offers an appropriate way to score policies, sequences of actions, in terms of their desirability, there are different ways to consider the contribution of past motor experience to the agent's future behaviour. In some approaches, agents are assumed to know their own actions, and use such knowledge to better plan for the future. In other approaches, agents are unaware of their actions, and must infer their motor behaviour from recent observations in order to plan for the future. This difference reflects a standard point of departure in two leading frameworks in motor control based on the presence, or not, of an efference copy signal representing knowledge about an agent's own actions. In this work we compare the performances of action-aware and action-unaware agents in two navigations tasks, showing how action-unaware agents can achieve performances comparable to action-aware ones while at a severe disadvantage.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨ä¸»åŠ¨æ¨ç†(Active Inference)æ¡†æ¶ä¸‹ï¼Œé’ˆå¯¹ä¸æ„ŸçŸ¥è‡ªèº«åŠ¨ä½œçš„æ™ºèƒ½ä½“(action-unaware agents)çš„è¡Œä¸ºè§„åˆ’ç­–ç•¥ã€‚ä¸»åŠ¨æ¨ç†é€šè¿‡æœ€å°åŒ–å˜åˆ†è‡ªç”±èƒ½(variational free energy)å’ŒæœŸæœ›è‡ªç”±èƒ½(expected free energy)æ¥ç»Ÿä¸€æè¿°æ„ŸçŸ¥ã€å­¦ä¹ å’ŒåŠ¨ä½œé€‰æ‹©è¿‡ç¨‹ã€‚è®ºæ–‡å¯¹æ¯”äº†â€œæ„ŸçŸ¥åŠ¨ä½œâ€ä¸â€œä¸æ„ŸçŸ¥åŠ¨ä½œâ€ä¸¤ç±»æ™ºèƒ½ä½“ï¼Œå‰è€…å‡è®¾æ™ºèƒ½ä½“äº†è§£è‡ªèº«åŠ¨ä½œï¼Œè€Œåè€…å¿…é¡»ä»è¿‘æœŸè§‚æµ‹ä¸­æ¨æ–­è‡ªèº«è¿åŠ¨è¡Œä¸ºï¼Œè¿™å¯¹åº”äº†è¿åŠ¨æ§åˆ¶ä¸­æ˜¯å¦å­˜åœ¨åŠ¨ä½œå¤–ä¼ å‰¯æœ¬(efference copy)çš„ä¸åŒç†è®ºã€‚é€šè¿‡åœ¨ä¸¤é¡¹å¯¼èˆªä»»åŠ¡ä¸­çš„æ€§èƒ½è¯„ä¼°ï¼Œç ”ç©¶è¯æ˜äº†ä¸æ„ŸçŸ¥åŠ¨ä½œçš„æ™ºèƒ½ä½“åœ¨é¢ä¸´æ˜¾è‘—ä¿¡æ¯åŠ£åŠ¿æ—¶ï¼Œä¾ç„¶èƒ½å¤Ÿå–å¾—ä¸æ„ŸçŸ¥åŠ¨ä½œæ™ºèƒ½ä½“ç›¸åª²ç¾çš„ä»»åŠ¡è¡¨ç°ã€‚è¯¥å‘ç°ä¸ºç†è§£ç”Ÿç‰©ä½“å¦‚ä½•åœ¨æœ‰é™åé¦ˆä¸‹å®ç°å¤æ‚çš„è‡ªé€‚åº”è¡Œä¸ºæä¾›äº†é‡è¦çš„ç†è®ºæ”¯æŒã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "q-bio.NC"
      ],
      "primary_category": "cs.AI",
      "comment": "59 pages, 47 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.12027v1",
      "published_date": "2025-08-16 12:27:51 UTC",
      "updated_date": "2025-08-16 12:27:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:15:07.984352+00:00"
    },
    {
      "arxiv_id": "2508.12026v1",
      "title": "Bongard-RWR+: Real-World Representations of Fine-Grained Concepts in Bongard Problems",
      "title_zh": "Bongard-RWR+ï¼šBongard é—®é¢˜ä¸­ç»†ç²’åº¦æ¦‚å¿µçš„çœŸå®ä¸–ç•Œè¡¨ç¤º",
      "authors": [
        "Szymon Pawlonka",
        "MikoÅ‚aj MaÅ‚kiÅ„ski",
        "Jacek MaÅ„dziuk"
      ],
      "abstract": "Bongard Problems (BPs) provide a challenging testbed for abstract visual reasoning (AVR), requiring models to identify visual concepts fromjust a few examples and describe them in natural language. Early BP benchmarks featured synthetic black-and-white drawings, which might not fully capture the complexity of real-world scenes. Subsequent BP datasets employed real-world images, albeit the represented concepts are identifiable from high-level image features, reducing the task complexity. Differently, the recently released Bongard-RWR dataset aimed at representing abstract concepts formulated in the original BPs using fine-grained real-world images. Its manual construction, however, limited the dataset size to just $60$ instances, constraining evaluation robustness. In this work, we introduce Bongard-RWR+, a BP dataset composed of $5\\,400$ instances that represent original BP abstract concepts using real-world-like images generated via a vision language model (VLM) pipeline. Building on Bongard-RWR, we employ Pixtral-12B to describe manually curated images and generate new descriptions aligned with the underlying concepts, use Flux.1-dev to synthesize images from these descriptions, and manually verify that the generated images faithfully reflect the intended concepts. We evaluate state-of-the-art VLMs across diverse BP formulations, including binary and multiclass classification, as well as textual answer generation. Our findings reveal that while VLMs can recognize coarse-grained visual concepts, they consistently struggle with discerning fine-grained concepts, highlighting limitations in their reasoning capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Bongard-RWR+ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å« 5,400 ä¸ªå®ä¾‹çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³åŸæœ‰ Bongard-RWR æ•°æ®é›†è§„æ¨¡æœ‰é™ä¸”è¯„ä¼°é²æ£’æ€§ä¸è¶³çš„é—®é¢˜ã€‚ä¸ºäº†åœ¨çœŸå®æ„Ÿå›¾åƒä¸­å±•ç°åŸå§‹ Bongard Problems (BPs) çš„ç»†ç²’åº¦æŠ½è±¡æ¦‚å¿µï¼Œç ”ç©¶å›¢é˜Ÿåˆ©ç”¨ Pixtral-12B ç”Ÿæˆæè¿°å¹¶ç»“åˆ Flux.1-dev åˆæˆå›¾åƒã€‚æ•´ä¸ªç”Ÿæˆæµç¨‹åŒ…å«äººå·¥éªŒè¯ç¯èŠ‚ï¼Œä»¥ç¡®ä¿åˆæˆå›¾åƒèƒ½å¿ å®åæ˜ åº•å±‚çš„è§†è§‰æ¦‚å¿µã€‚ç ”ç©¶äººå‘˜åœ¨å¤šç§ä»»åŠ¡ä¸Šå¯¹æœ€å…ˆè¿›çš„ Vision Language Models (VLMs) è¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºè¿™äº›æ¨¡å‹è™½ç„¶èƒ½è¯†åˆ«ç²—ç²’åº¦æ¦‚å¿µï¼Œä½†åœ¨è¾¨åˆ«ç»†ç²’åº¦æ¦‚å¿µæ—¶ä»å­˜åœ¨æ˜¾è‘—å›°éš¾ã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨ Abstract Visual Reasoning (AVR) èƒ½åŠ›æ–¹é¢çš„å±€é™æ€§ï¼Œä¸ºæœªæ¥çš„è§†è§‰è¯­è¨€æ¨¡å‹å¼€å‘æä¾›äº†æ›´å…·æŒ‘æˆ˜æ€§çš„è¯„ä¼°å¹³å°ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.12026v1",
      "published_date": "2025-08-16 12:26:44 UTC",
      "updated_date": "2025-08-16 12:26:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:15:23.392560+00:00"
    },
    {
      "arxiv_id": "2508.12022v1",
      "title": "AI Models for Depressive Disorder Detection and Diagnosis: A Review",
      "title_zh": "æŠ‘éƒç—‡æ£€æµ‹ä¸è¯Šæ–­çš„äººå·¥æ™ºèƒ½æ¨¡å‹ç»¼è¿°",
      "authors": [
        "Dorsa Macky Aleagha",
        "Payam Zohari",
        "Mostafa Haghir Chehreghani"
      ],
      "abstract": "Major Depressive Disorder is one of the leading causes of disability worldwide, yet its diagnosis still depends largely on subjective clinical assessments. Integrating Artificial Intelligence (AI) holds promise for developing objective, scalable, and timely diagnostic tools. In this paper, we present a comprehensive survey of state-of-the-art AI methods for depression detection and diagnosis, based on a systematic review of 55 key studies. We introduce a novel hierarchical taxonomy that structures the field by primary clinical task (diagnosis vs. prediction), data modality (text, speech, neuroimaging, multimodal), and computational model class (e.g., graph neural networks, large language models, hybrid approaches). Our in-depth analysis reveals three major trends: the predominance of graph neural networks for modeling brain connectivity, the rise of large language models for linguistic and conversational data, and an emerging focus on multimodal fusion, explainability, and algorithmic fairness. Alongside methodological insights, we provide an overview of prominent public datasets and standard evaluation metrics as a practical guide for researchers. By synthesizing current advances and highlighting open challenges, this survey offers a comprehensive roadmap for future innovation in computational psychiatry.",
      "tldr_zh": "è¯¥è®ºæ–‡å¯¹ç”¨äºæŠ‘éƒéšœç¢(Depressive Disorder)æ£€æµ‹ä¸è¯Šæ–­çš„å‰æ²¿äººå·¥æ™ºèƒ½(AI)æ¨¡å‹è¿›è¡Œäº†å…¨é¢ç»¼è¿°ï¼Œç³»ç»Ÿæ€§åœ°å›é¡¾äº†55é¡¹å…³é”®ç ”ç©¶ã€‚ç ”ç©¶è€…æå‡ºäº†ä¸€ç§æ–°é¢–çš„åˆ†å±‚åˆ†ç±»æ³•(hierarchical taxonomy)ï¼Œä»ä¸´åºŠä»»åŠ¡ã€æ•°æ®æ¨¡æ€ï¼ˆå¦‚æ–‡æœ¬ã€è¯­éŸ³ã€ç¥ç»å½±åƒã€å¤šæ¨¡æ€ï¼‰ä»¥åŠè®¡ç®—æ¨¡å‹ç±»åˆ«ï¼ˆå¦‚å›¾ç¥ç»ç½‘ç»œã€å¤§è¯­è¨€æ¨¡å‹ã€æ··åˆæ–¹æ³•ï¼‰ä¸‰ä¸ªç»´åº¦å¯¹è¯¥é¢†åŸŸè¿›è¡Œäº†ç³»ç»ŸåŒ–æ¢³ç†ã€‚åˆ†ææ­ç¤ºäº†å½“å‰ç ”ç©¶çš„ä¸‰å¤§è¶‹åŠ¿ï¼ŒåŒ…æ‹¬åˆ©ç”¨å›¾ç¥ç»ç½‘ç»œ(graph neural networks)è¿›è¡Œå¤§è„‘è¿æ¥å»ºæ¨¡ã€å¤§è¯­è¨€æ¨¡å‹(large language models)åœ¨è¯­è¨€æ•°æ®ä¸­çš„åº”ç”¨ï¼Œä»¥åŠå¯¹å¤šæ¨¡æ€èåˆ(multimodal fusion)ã€å¯è§£é‡Šæ€§(explainability)å’Œç®—æ³•å…¬å¹³æ€§(algorithmic fairness)çš„æ—¥ç›Šå…³æ³¨ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜æ±‡æ€»äº†ä¸»æµå…¬å¼€æ•°æ®é›†å’Œæ ‡å‡†è¯„ä¼°æŒ‡æ ‡ï¼Œä¸ºç›¸å…³é¢†åŸŸçš„ç§‘ç ”äººå‘˜æä¾›äº†å®è·µæŒ‡å—ã€‚è¯¥ç»¼è¿°é€šè¿‡æ•´åˆç°æœ‰æŠ€æœ¯è¿›å±•å¹¶æ˜ç¡®å½“å‰é¢ä¸´çš„æŒ‘æˆ˜ï¼Œä¸ºè®¡ç®—ç²¾ç¥åŒ»å­¦(computational psychiatry)çš„æœªæ¥åˆ›æ–°æä¾›äº†æ¸…æ™°çš„è·¯çº¿å›¾ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.12022v1",
      "published_date": "2025-08-16 11:46:48 UTC",
      "updated_date": "2025-08-16 11:46:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:15:14.804366+00:00"
    },
    {
      "arxiv_id": "2508.12013v1",
      "title": "Predicting ChatGPT Use in Assignments: Implications for AI-Aware Assessment Design",
      "title_zh": "é¢„æµ‹ä½œä¸šä¸­çš„ ChatGPT ä½¿ç”¨ï¼šå¯¹ AI æ•æ„Ÿå‹è€ƒæ ¸è®¾è®¡çš„å¯ç¤º",
      "authors": [
        "Surajit Das",
        "Aleksei Eliseev"
      ],
      "abstract": "The rise of generative AI tools like ChatGPT has significantly reshaped education, sparking debates about their impact on learning outcomes and academic integrity. While prior research highlights opportunities and risks, there remains a lack of quantitative analysis of student behavior when completing assignments. Understanding how these tools influence real-world academic practices, particularly assignment preparation, is a pressing and timely research priority.\n  This study addresses this gap by analyzing survey responses from 388 university students, primarily from Russia, including a subset of international participants. Using the XGBoost algorithm, we modeled predictors of ChatGPT usage in academic assignments. Key predictive factors included learning habits, subject preferences, and student attitudes toward AI. Our binary classifier demonstrated strong predictive performance, achieving 80.1\\% test accuracy, with 80.2\\% sensitivity and 79.9\\% specificity. The multiclass classifier achieved 64.5\\% test accuracy, 64.6\\% weighted precision, and 64.5\\% recall, with similar training scores, indicating potential data scarcity challenges.\n  The study reveals that frequent use of ChatGPT for learning new concepts correlates with potential overreliance, raising concerns about long-term academic independence. These findings suggest that while generative AI can enhance access to knowledge, unchecked reliance may erode critical thinking and originality. We propose discipline-specific guidelines and reimagined assessment strategies to balance innovation with academic rigor. These insights can guide educators and policymakers in ethically and effectively integrating AI into education.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡åˆ†æ388åå¤§å­¦ç”Ÿçš„è°ƒæŸ¥å“åº”ï¼Œåˆ©ç”¨ XGBoost ç®—æ³•æ„å»ºäº†é¢„æµ‹å­¦ç”Ÿåœ¨ä½œä¸šä¸­ä½¿ç”¨ ChatGPT è¡Œä¸ºçš„æ¨¡å‹ã€‚ç ”ç©¶è¯†åˆ«å‡ºå­¦ä¹ ä¹ æƒ¯ã€å­¦ç§‘åå¥½å’Œå¯¹ AI çš„æ€åº¦æ˜¯æ ¸å¿ƒé¢„æµ‹å› å­ï¼Œå…¶äºŒåˆ†ç±»å™¨ (Binary classifier) åœ¨æµ‹è¯•ä¸­å®ç°äº†80.1%çš„å‡†ç¡®ç‡ã€‚è°ƒæŸ¥ç»“æœæ­ç¤ºï¼Œé¢‘ç¹ä½¿ç”¨ ChatGPT æŒæ¡æ–°æ¦‚å¿µä¸æ½œåœ¨çš„è¿‡åº¦ä¾èµ– (Overreliance) æ˜¾è‘—ç›¸å…³ï¼Œå¯èƒ½å¯¹å­¦ç”Ÿçš„å­¦æœ¯ç‹¬ç«‹æ€§ã€æ‰¹åˆ¤æ€§æ€ç»´ (Critical thinking) å’ŒåŸåˆ›æ€§äº§ç”Ÿè´Ÿé¢å½±å“ã€‚åŸºäºæ­¤ï¼Œç ”ç©¶æå‡ºäº†é’ˆå¯¹ç‰¹å®šå­¦ç§‘çš„æŒ‡å¯¼æ–¹é’ˆï¼Œå¹¶å»ºè®®é‡æ–°è®¾è®¡è¯„ä¼°ç­–ç•¥ (Assessment strategies) ä»¥åº”å¯¹ AI å¸¦æ¥çš„æŒ‘æˆ˜ã€‚è¯¥æˆæœä¸ºæ•™è‚²å·¥ä½œè€…å’Œæ”¿ç­–åˆ¶å®šè€…åœ¨ç»´æŠ¤å­¦æœ¯ä¸¥è°¨æ€§çš„å‰æä¸‹ï¼Œä¼¦ç†ä¸”æœ‰æ•ˆåœ°æ•´åˆç”Ÿæˆå¼ AI (Generative AI) æä¾›äº†å®è¯ä¾æ®ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.12013v1",
      "published_date": "2025-08-16 11:09:38 UTC",
      "updated_date": "2025-08-16 11:09:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:15:25.891163+00:00"
    },
    {
      "arxiv_id": "2508.13207v1",
      "title": "Utilizing the RAIN method and Graph SAGE Model to Identify Effective Drug Combinations for Gastric Neoplasm Treatment",
      "title_zh": "ç»“åˆ RAIN æ–¹æ³•ä¸ Graph SAGE æ¨¡å‹è¯†åˆ«èƒƒè‚¿ç˜¤æ²»ç–—çš„æœ‰æ•ˆè”åˆç”¨è¯æ–¹æ¡ˆ",
      "authors": [
        "S. Z. Pirasteh",
        "Ali A. Kiaei",
        "Mahnaz Bush",
        "Sabra Moghadam",
        "Raha Aghaei",
        "Behnaz Sadeghigol"
      ],
      "abstract": "Background: Gastric neoplasm, primarily adenocarcinoma, is an aggressive cancer with high mortality, often diagnosed late, leading to complications like metastasis. Effective drug combinations are vital to address disease heterogeneity, enhance efficacy, reduce resistance, and improve patient outcomes. Methods: The RAIN method integrated Graph SAGE to propose drug combinations, using a graph model with p-value-weighted edges connecting drugs, genes, and proteins. NLP and systematic literature review (PubMed, Scopus, etc.) validated proposed drugs, followed by network meta-analysis to assess efficacy, implemented in Python. Results: Oxaliplatin, fluorouracil, and trastuzumab were identified as effective, supported by 61 studies. Fluorouracil alone had a p-value of 0.0229, improving to 0.0099 with trastuzumab, and 0.0069 for the triple combination, indicating superior efficacy. Conclusion: The RAIN method, combining AI and network meta-analysis, effectively identifies optimal drug combinations for gastric neoplasm, offering a promising strategy to enhance treatment outcomes and guide health policy.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹èƒƒè‚¿ç˜¤(Gastric Neoplasm)æ²»ç–—ä¸­çš„å¼‚è´¨æ€§å’Œè€è¯æ€§é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨RAINæ–¹æ³•ç»“åˆGraph SAGEæ¨¡å‹è¯†åˆ«æœ‰æ•ˆè¯ç‰©ç»„åˆçš„æ–°ç­–ç•¥ã€‚è¯¥æ–¹æ³•é€šè¿‡æ„å»ºä¸€ä¸ªåŒ…å«è¯ç‰©ã€åŸºå› å’Œè›‹ç™½è´¨çš„å›¾æ¨¡å‹ï¼Œå¹¶åˆ©ç”¨p-valueåŠ æƒè¾¹æ¥è¡¨ç¤ºç›¸äº’ä½œç”¨ï¼Œä»è€Œé¢„æµ‹æ½œåœ¨çš„è¯ç‰©ç»„åˆã€‚ç ”ç©¶è¿›ä¸€æ­¥ç»“åˆäº†è‡ªç„¶è¯­è¨€å¤„ç†(NLP)å’Œç³»ç»Ÿçš„æ–‡çŒ®ç»¼è¿°è¿›è¡ŒéªŒè¯ï¼Œå¹¶åˆ©ç”¨Pythonå®ç°äº†ç½‘ç»œèŸèƒåˆ†æ(network meta-analysis)ä»¥è¯„ä¼°ä¸´åºŠç–—æ•ˆã€‚å®éªŒç»“æœç¡®å®šäº†Oxaliplatinã€fluorouracilå’Œtrastuzumabçš„ä¸‰è”ç»„åˆå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œå…¶p-valueä»å•è¯çš„0.0229æ˜¾è‘—ä¼˜åŒ–è‡³0.0069ï¼Œå±•ç°å‡ºå“è¶Šçš„ååŒæ²»ç–—æ•ˆæœã€‚è¯¥ç ”ç©¶è¯æ˜äº†äººå·¥æ™ºèƒ½(AI)ä¸ç½‘ç»œèŸèƒåˆ†æç›¸ç»“åˆçš„RAINæ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆç­›é€‰èƒƒè‚¿ç˜¤çš„æœ€ä½³æ²»ç–—æ–¹æ¡ˆï¼Œä¸ºæé«˜æ‚£è€…é¢„åå’ŒæŒ‡å¯¼å«ç”Ÿæ”¿ç­–æä¾›äº†é‡è¦çš„ç§‘å­¦ä¾æ®ã€‚",
      "categories": [
        "q-bio.QM",
        "cs.AI"
      ],
      "primary_category": "q-bio.QM",
      "comment": "43 pages",
      "pdf_url": "https://arxiv.org/pdf/2508.13207v1",
      "published_date": "2025-08-16 10:19:19 UTC",
      "updated_date": "2025-08-16 10:19:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:15:26.283604+00:00"
    },
    {
      "arxiv_id": "2508.11999v4",
      "title": "MOON: Generative MLLM-based Multimodal Representation Learning for E-commerce Product Understanding",
      "title_zh": "MOONï¼šåŸºäºç”Ÿæˆå¼å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ç”µå•†å•†å“ç†è§£å¤šæ¨¡æ€è¡¨å¾å­¦ä¹ ",
      "authors": [
        "Daoze Zhang",
        "Chenghan Fu",
        "Zhanheng Nie",
        "Jianyu Liu",
        "Wanxian Guan",
        "Yuan Gao",
        "Jun Song",
        "Pengjie Wang",
        "Jian Xu",
        "Bo Zheng"
      ],
      "abstract": "With the rapid advancement of e-commerce, exploring general representations rather than task-specific ones has attracted increasing research attention. For product understanding, although existing discriminative dual-flow architectures drive progress in this field, they inherently struggle to model the many-to-one alignment between multiple images and texts of products. Therefore, we argue that generative Multimodal Large Language Models (MLLMs) hold significant potential for improving product representation learning. Nevertheless, achieving this goal still remains non-trivial due to several key challenges: the lack of multimodal and aspect-aware modeling modules in typical LLMs; the common presence of background noise in product images; and the absence of a standard benchmark for evaluation. To address these issues, we propose the first generative MLLM-based model named MOON for product representation learning. Our method (1) employs a guided Mixture-of-Experts (MoE) module for targeted modeling of multimodal and aspect-specific product content; (2) effectively detects core semantic regions in product images to mitigate the distraction and interference caused by background noise; and (3) introduces the specialized negative sampling strategy to increase the difficulty and diversity of negative samples. In addition, we release a large-scale multimodal benchmark MBE for various product understanding tasks. Experimentally, our model demonstrates competitive zero-shot performance on both our benchmark and the public dataset, showcasing strong generalization across various downstream tasks, including cross-modal retrieval, product classification, and attribute prediction. Furthermore, the case study and visualization illustrate the effectiveness of MOON for product understanding.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”µå­å•†åŠ¡é¢†åŸŸä¸­ä¼ ç»Ÿåˆ¤åˆ«å¼æ¶æ„éš¾ä»¥å¤„ç†å¤šå›¾æ–‡å¯¹é½ã€èƒŒæ™¯å™ªå£°å¹²æ‰°åŠç¼ºä¹ç»Ÿä¸€åŸºå‡†ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†é¦–ä¸ªåŸºäºç”Ÿæˆå¼å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„äº§å“è¡¨ç¤ºå­¦ä¹ æ¨¡å‹ MOONã€‚è¯¥æ¨¡å‹é‡‡ç”¨äº†å¼•å¯¼å¼çš„æ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMixture-of-Experts, MoEï¼‰æ¨¡å—ï¼Œå®ç°äº†å¯¹å¤šæ¨¡æ€å’Œç‰¹å®šç»´åº¦äº§å“å†…å®¹çš„é’ˆå¯¹æ€§å»ºæ¨¡ã€‚ä¸ºäº†å‡å°‘èƒŒæ™¯å™ªå£°çš„å¹²æ‰°ï¼ŒMOON èƒ½å¤Ÿæœ‰æ•ˆæ£€æµ‹äº§å“å›¾åƒä¸­çš„æ ¸å¿ƒè¯­ä¹‰åŒºåŸŸï¼Œå¹¶å¼•å…¥äº†ä¸“é—¨çš„è´Ÿé‡‡æ ·ç­–ç•¥ä»¥æå‡æ ·æœ¬çš„å¤šæ ·æ€§ä¸åˆ¤åˆ«éš¾åº¦ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜å‘å¸ƒäº†ä¸€ä¸ªå¤§è§„æ¨¡å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•é›† MBEï¼Œç”¨äºè¯„ä¼°å„ç±»äº§å“ç†è§£ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMOON åœ¨è·¨æ¨¡æ€æ£€ç´¢ã€äº§å“åˆ†ç±»å’Œå±æ€§é¢„æµ‹ç­‰ä»»åŠ¡ä¸­å‡å±•ç°äº†å‡ºè‰²çš„é›¶æ ·æœ¬ï¼ˆzero-shotï¼‰æ€§èƒ½å’Œå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by WSDM 2026. 11 pages, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.11999v4",
      "published_date": "2025-08-16 09:59:25 UTC",
      "updated_date": "2025-11-29 16:20:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:15:32.588788+00:00"
    },
    {
      "arxiv_id": "2508.11995v1",
      "title": "AgentCDM: Enhancing Multi-Agent Collaborative Decision-Making via ACH-Inspired Structured Reasoning",
      "title_zh": "AgentCDMï¼šé€šè¿‡å— ACH å¯å‘çš„ç»“æ„åŒ–æ¨ç†å¢å¼ºå¤šæ™ºèƒ½ä½“ååŒå†³ç­–",
      "authors": [
        "Xuyang Zhao",
        "Shiwan Zhao",
        "Hualong Yu",
        "Liting Zhang",
        "Qicheng Li"
      ],
      "abstract": "Multi-agent systems (MAS) powered by large language models (LLMs) hold significant promise for solving complex decision-making tasks. However, the core process of collaborative decision-making (CDM) within these systems remains underexplored. Existing approaches often rely on either ``dictatorial\" strategies that are vulnerable to the cognitive biases of a single agent, or ``voting-based\" methods that fail to fully harness collective intelligence. To address these limitations, we propose \\textbf{AgentCDM}, a structured framework for enhancing collaborative decision-making in LLM-based multi-agent systems. Drawing inspiration from the Analysis of Competing Hypotheses (ACH) in cognitive science, AgentCDM introduces a structured reasoning paradigm that systematically mitigates cognitive biases and shifts decision-making from passive answer selection to active hypothesis evaluation and construction. To internalize this reasoning process, we develop a two-stage training paradigm: the first stage uses explicit ACH-inspired scaffolding to guide the model through structured reasoning, while the second stage progressively removes this scaffolding to encourage autonomous generalization. Experiments on multiple benchmark datasets demonstrate that AgentCDM achieves state-of-the-art performance and exhibits strong generalization, validating its effectiveness in improving the quality and robustness of collaborative decisions in MAS.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AgentCDMï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å¢å¼ºåŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰ååŒå†³ç­–ï¼ˆCollaborative Decision-Making, CDMï¼‰èƒ½åŠ›çš„ç»“æ„åŒ–æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰â€œç‹¬è£å¼â€ç­–ç•¥æ˜“å—å•ä¸€æ™ºèƒ½ä½“è®¤çŸ¥åå·®å½±å“ä»¥åŠâ€œæŠ•ç¥¨å¼â€æ–¹æ³•éš¾ä»¥å……åˆ†åˆ©ç”¨é›†ä½“æ™ºæ…§çš„å±€é™ï¼ŒAgentCDM å€Ÿé‰´äº†è®¤çŸ¥ç§‘å­¦ä¸­çš„ç«äº‰å‡è®¾åˆ†æï¼ˆAnalysis of Competing Hypotheses, ACHï¼‰ï¼Œå¼•å…¥äº†ä¸€ç§ç»“æ„åŒ–æ¨ç†èŒƒå¼ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†å†³ç­–è¿‡ç¨‹ä»è¢«åŠ¨çš„ç­”æ¡ˆé€‰æ‹©è½¬å˜ä¸ºä¸»åŠ¨çš„å‡è®¾è¯„ä¼°ä¸æ„å»ºï¼Œç³»ç»Ÿæ€§åœ°å‡å°‘äº†è®¤çŸ¥åå·®ã€‚ä¸ºäº†è®©æ¨¡å‹å†…åŒ–è¿™ä¸€è¿‡ç¨‹ï¼Œç ”ç©¶è€…è®¾è®¡äº†ä¸¤é˜¶æ®µè®­ç»ƒèŒƒå¼ï¼Œåˆ©ç”¨ ACH å¯å‘å¼çš„æ”¯æ¶å¼•å¯¼æ¨¡å‹é€æ­¥è¿‡æ¸¡åˆ°è‡ªä¸»æ³›åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAgentCDM åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº† SOTA æ€§èƒ½ï¼Œæ˜¾è‘—æå‡äº†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨å¤æ‚ä»»åŠ¡ä¸­å†³ç­–çš„è´¨é‡ä¸é²æ£’æ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11995v1",
      "published_date": "2025-08-16 09:46:04 UTC",
      "updated_date": "2025-08-16 09:46:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:15:36.397340+00:00"
    },
    {
      "arxiv_id": "2508.11991v3",
      "title": "Modeling Relational Logic Circuits for And-Inverter Graph Convolutional Network",
      "title_zh": "é¢å‘ä¸éå›¾å·ç§¯ç½‘ç»œçš„å…³ç³»é€»è¾‘ç”µè·¯å»ºæ¨¡",
      "authors": [
        "Weihao Sun",
        "Shikai Guo",
        "Siwen Wang",
        "Qian Ma",
        "Hui Li"
      ],
      "abstract": "The automation of logic circuit design enhances chip performance, energy efficiency, and reliability, and is widely applied in the field of Electronic Design Automation (EDA).And-Inverter Graphs (AIGs) efficiently represent, optimize, and verify the functional characteristics of digital circuits, enhancing the efficiency of EDA development.Due to the complex structure and large scale of nodes in real-world AIGs, accurate modeling is challenging, leading to existing work lacking the ability to jointly model functional and structural characteristics, as well as insufficient dynamic information propagation capability.To address the aforementioned challenges, we propose AIGer.Specifically, AIGer consists of two components: 1) Node logic feature initialization embedding component and 2) AIGs feature learning network component.The node logic feature initialization embedding component projects logic nodes, such as AND and NOT, into independent semantic spaces, to enable effective node embedding for subsequent processing.Building upon this, the AIGs feature learning network component employs a heterogeneous graph convolutional network, designing dynamic relationship weight matrices and differentiated information aggregation approaches to better represent the original structure and information of AIGs.The combination of these two components enhances AIGer's ability to jointly model functional and structural characteristics and improves its message passing capability. Experimental results indicate that AIGer outperforms the current best models in the Signal Probability Prediction (SSP) task, improving MAE and MSE by 18.95\\% and 44.44\\%, respectively. In the Truth Table Distance Prediction (TTDP) task, AIGer achieves improvements of 33.57\\% and 14.79\\% in MAE and MSE, respectively, compared to the best-performing models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AIGerï¼Œä¸€ç§ä¸“é—¨ç”¨äº And-Inverter Graph Convolutional Network çš„å…³ç³»é€»è¾‘ç”µè·¯å»ºæ¨¡æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨å¤„ç† And-Inverter Graphs (AIGs) æ—¶éš¾ä»¥å…¼é¡¾åŠŸèƒ½ä¸ç»“æ„ç‰¹å¾ä»¥åŠä¿¡æ¯ä¼ æ’­èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶ç”±èŠ‚ç‚¹é€»è¾‘ç‰¹å¾åˆå§‹åŒ–åµŒå…¥ç»„ä»¶å’Œ AIGs ç‰¹å¾å­¦ä¹ ç½‘ç»œç»„ä»¶æ„æˆï¼Œå‰è€…å°† AND å’Œ NOT ç­‰é€»è¾‘èŠ‚ç‚¹æŠ•å½±è‡³ç‹¬ç«‹è¯­ä¹‰ç©ºé—´ï¼Œåè€…åˆ™é€šè¿‡å¼‚æ„å›¾å·ç§¯ç½‘ç»œ (heterogeneous graph convolutional network) ç»“åˆåŠ¨æ€å…³ç³»æƒé‡çŸ©é˜µå®ç°å·®å¼‚åŒ–ä¿¡æ¯èšåˆã€‚è¿™ç§è®¾è®¡æ˜¾è‘—å¢å¼ºäº†æ¨¡å‹å¯¹ AIGs åŠŸèƒ½ä¸ç»“æ„çš„è”åˆå»ºæ¨¡èƒ½åŠ›åŠæ¶ˆæ¯ä¼ é€’æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAIGer åœ¨ä¿¡å·æ¦‚ç‡é¢„æµ‹ (Signal Probability Prediction, SSP) ä»»åŠ¡ä¸­å°† MAE å’Œ MSE åˆ†åˆ«é™ä½äº† 18.95% å’Œ 44.44%ï¼›åœ¨çœŸå€¼è¡¨è·ç¦»é¢„æµ‹ (Truth Table Distance Prediction, TTDP) ä»»åŠ¡ä¸­ï¼Œå…¶ MAE å’Œ MSE è¾ƒå½“å‰æœ€ä¼˜æ¨¡å‹åˆ†åˆ«æå‡äº† 33.57% å’Œ 14.79%ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11991v3",
      "published_date": "2025-08-16 09:23:52 UTC",
      "updated_date": "2025-08-20 01:16:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:15:44.892670+00:00"
    },
    {
      "arxiv_id": "2508.11987v3",
      "title": "FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction",
      "title_zh": "FutureXï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“æœªæ¥é¢„æµ‹çš„å…ˆè¿›åŠ¨æ€åŸºå‡†æµ‹è¯•",
      "authors": [
        "Zhiyuan Zeng",
        "Jiashuo Liu",
        "Siyuan Chen",
        "Tianci He",
        "Yali Liao",
        "Yixiao Tian",
        "Jinpeng Wang",
        "Zaiyuan Wang",
        "Yang Yang",
        "Lingyue Yin",
        "Mingren Yin",
        "Zhenwei Zhu",
        "Tianle Cai",
        "Zehui Chen",
        "Jiecao Chen",
        "Yantao Du",
        "Xiang Gao",
        "Jiacheng Guo",
        "Liang Hu",
        "Jianpeng Jiao",
        "Xiangsheng Li",
        "Jingkai Liu",
        "Shuang Ni",
        "Zhoufutu Wen",
        "Ge Zhang",
        "Kaiyuan Zhang",
        "Xin Zhou",
        "Jose Blanchet",
        "Xipeng Qiu",
        "Mengdi Wang",
        "Wenhao Huang"
      ],
      "abstract": "Future prediction is a complex task for LLM agents, requiring a high level of analytical thinking, information gathering, contextual understanding, and decision-making under uncertainty. Agents must not only gather and interpret vast amounts of dynamic information but also integrate diverse data sources, weigh uncertainties, and adapt predictions based on emerging trends, just as human experts do in fields like politics, economics, and finance. Despite its importance, no large-scale benchmark exists for evaluating agents on future prediction, largely due to challenges in handling real-time updates and retrieving timely, accurate answers. To address this, we introduce $\\textbf{FutureX}$, a dynamic and live evaluation benchmark specifically designed for LLM agents performing future prediction tasks. FutureX is the largest and most diverse live benchmark for future prediction, supporting real-time daily updates and eliminating data contamination through an automated pipeline for question gathering and answer collection. We evaluate 25 LLM/agent models, including those with reasoning, search capabilities, and integration of external tools such as the open-source Deep Research Agent and closed-source Deep Research models. This comprehensive evaluation assesses agents' adaptive reasoning and performance in dynamic environments. Additionally, we provide in-depth analyses of agents' failure modes and performance pitfalls in future-oriented tasks, including the vulnerability to fake web pages and the temporal validity. Our goal is to establish a dynamic, contamination-free evaluation standard that drives the development of LLM agents capable of performing at the level of professional human analysts in complex reasoning and predictive thinking.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†FutureXï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨é’ˆå¯¹LLMæ™ºèƒ½ä½“åœ¨æœªæ¥é¢„æµ‹ä»»åŠ¡ä¸­çš„è¡¨ç°è€Œè®¾è®¡çš„åŠ¨æ€å®æ—¶è¯„ä¼°åŸºå‡†(Live Benchmark)ã€‚æœªæ¥é¢„æµ‹è¦æ±‚æ™ºèƒ½ä½“åœ¨ä¸ç¡®å®šæ€§ä¸‹å…·å¤‡é«˜æ°´å¹³çš„åˆ†ææ€ç»´ã€ä¿¡æ¯æ•´åˆå’Œè¶‹åŠ¿é€‚åº”èƒ½åŠ›ï¼Œè€ŒFutureXé€šè¿‡è‡ªåŠ¨åŒ–çš„æµæ°´çº¿å®ç°äº†æ¯æ—¥å®æ—¶æ›´æ–°å¹¶æœ‰æ•ˆæ¶ˆé™¤äº†æ•°æ®æ±¡æŸ“(Data Contamination)é—®é¢˜ã€‚ä½œä¸ºç›®å‰è§„æ¨¡æœ€å¤§ä¸”æœ€å…·å¤šæ ·æ€§çš„å®æ—¶é¢„æµ‹åŸºå‡†ï¼Œè¯¥ç ”ç©¶å¯¹åŒ…æ‹¬å…·å¤‡æ¨ç†ã€æœç´¢åŠé›†æˆå¤–éƒ¨å·¥å…·èƒ½åŠ›çš„25ç§LLMæ¨¡å‹å’ŒDeep Researchæ™ºèƒ½ä½“è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚å®éªŒæ·±å…¥åˆ†æäº†æ™ºèƒ½ä½“åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„è‡ªé€‚åº”æ¨ç†èƒ½åŠ›ï¼Œå¹¶æ­ç¤ºäº†å…¶åœ¨é¢å¯¹è™šå‡ç½‘é¡µæ”»å‡»å’Œæ—¶é—´æœ‰æ•ˆæ€§(Temporal Validity)ç­‰æ–¹é¢çš„è¡¨ç°é™·é˜±ä¸å¤±æ•ˆæ¨¡å¼ã€‚FutureXçš„å»ºç«‹ä¸ºæœªæ¥å¯¼å‘å‹ä»»åŠ¡æä¾›äº†åŠ¨æ€ä¸”æ— æ±¡æŸ“çš„è¯„ä¼°æ ‡å‡†ï¼Œæ—¨åœ¨æ¨åŠ¨LLMæ™ºèƒ½ä½“åœ¨å¤æ‚æ¨ç†å’Œé¢„æµ‹æ€§æ€ç»´æ–¹é¢è¾¾åˆ°ä¸“ä¸šäººç±»åˆ†æå¸ˆçš„æ°´å¹³ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Technical report, 51 pages. Update the results",
      "pdf_url": "https://arxiv.org/pdf/2508.11987v3",
      "published_date": "2025-08-16 08:54:08 UTC",
      "updated_date": "2025-09-05 09:15:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:15:48.299381+00:00"
    },
    {
      "arxiv_id": "2508.11985v1",
      "title": "Efficient Modular Learning through Naive LoRA Summation: Leveraging Orthogonality in High-Dimensional Models",
      "title_zh": "åŸºäºæœ´ç´  LoRA æ±‚å’Œçš„é«˜æ•ˆæ¨¡å—åŒ–å­¦ä¹ ï¼šåˆ©ç”¨é«˜ç»´æ¨¡å‹ä¸­çš„æ­£äº¤æ€§",
      "authors": [
        "Zhanhao Cao",
        "Clement Truong",
        "Andrew Lizarraga"
      ],
      "abstract": "Recent advances in large language models are driven by scale, while parameter-efficient fine-tuning (PEFT) enables updating only a small fraction of parameters. Low-Rank Adaptation (LoRA) stores parameter deltas as the product of two small matrices, which makes them natural building blocks that can be composed. Motivated by the superposition principle, we hypothesize that independently trained LoRA modules on disjoint domains are approximately orthogonal and can be combined by simple addition. Using GPT-2 Small (117M) with LoRA rank 4 and alpha=64, we train adapters for three QA domains (math, medicine, finance). In pairwise tests, adding Math+Medicine adapters improves perplexity by -9.10% relative to merged-data fine-tuning, while Math+Finance and Finance+Medicine change by +4.54% and +27.56%, respectively. Across combinations, the RMS cosine similarity between LoRA deltas correlates positively and approximately linearly with the change in perplexity. Naive summation requires no additional training, can be applied in seconds, and achieves performance comparable to models trained on merged data, while clarifying when interference appears in higher-order compositions.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é€šè¿‡ç®€å•çš„ LoRA Summation å®ç°é«˜æ•ˆæ¨¡å—åŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œæ—¨åœ¨åˆ©ç”¨é«˜ç»´æ¨¡å‹ä¸­çš„æ­£äº¤æ€§ï¼ˆOrthogonalityï¼‰æ¥æœ‰æ•ˆç»„åˆç‹¬ç«‹è®­ç»ƒçš„ä½ç§©é€‚é…å™¨ã€‚ç ”ç©¶äººå‘˜åŸºäºå åŠ åŸç†ï¼ˆSuperposition Principleï¼‰æå‡ºå‡è®¾ï¼Œè®¤ä¸ºåœ¨ä¸åŒé¢†åŸŸè®­ç»ƒçš„ LoRA æ¨¡å—å…·æœ‰è¿‘ä¼¼æ­£äº¤æ€§ï¼Œå¯ä»¥ç›´æ¥é€šè¿‡æœ´ç´ åŠ å’Œï¼ˆNaive Summationï¼‰è¿›è¡Œåˆå¹¶ã€‚å®éªŒåœ¨ GPT-2 Small æ¨¡å‹ä¸Šé’ˆå¯¹æ•°å­¦ã€åŒ»å­¦å’Œé‡‘èé¢†åŸŸå¼€å±•ï¼Œç»“æœæ˜¾ç¤º Math+Medicine ç»„åˆåçš„æ¨¡å‹å›°æƒ‘åº¦ï¼ˆPerplexityï¼‰ç›¸è¾ƒäºåˆå¹¶æ•°æ®å¾®è°ƒçš„æ¨¡å‹ä¼˜åŒ–äº† 9.10%ã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç° LoRA æƒé‡å˜åŒ–é‡ä¹‹é—´çš„ RMS ä½™å¼¦ç›¸ä¼¼åº¦ä¸å›°æƒ‘åº¦çš„å˜åŒ–å‘ˆç°å‡ºè¿‘ä¼¼çº¿æ€§çš„æ­£ç›¸å…³å…³ç³»ã€‚è¿™ç§æœ´ç´ åŠ å’Œæ³•æ— éœ€é¢å¤–è®­ç»ƒï¼Œå¯åœ¨æ•°ç§’å†…å®Œæˆéƒ¨ç½²ï¼Œå¹¶èƒ½è¾¾åˆ°ä¸ä¼ ç»Ÿåˆå¹¶æ•°æ®å¾®è°ƒç›¸å½“çš„æ€§èƒ½æ°´å¹³ã€‚è¯¥ç ”ç©¶ä¸ä»…è¯æ˜äº†ç®€å•åŠ å’Œçš„å®ç”¨æ€§ï¼Œè¿˜ä¸ºç†è§£é«˜é˜¶æ¨¡å—ç»„åˆä¸­çš„å¹²æ‰°æœºåˆ¶æä¾›äº†æ–°çš„è§†ç‚¹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint",
      "pdf_url": "https://arxiv.org/pdf/2508.11985v1",
      "published_date": "2025-08-16 08:49:02 UTC",
      "updated_date": "2025-08-16 08:49:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:16:10.595375+00:00"
    },
    {
      "arxiv_id": "2508.11977v2",
      "title": "TBGRecall: A Generative Retrieval Model for E-commerce Recommendation Scenarios",
      "title_zh": "TBGRecallï¼šé¢å‘ç”µå•†æ¨èåœºæ™¯çš„ç”Ÿæˆå¼æ£€ç´¢æ¨¡å‹",
      "authors": [
        "Zida Liang",
        "Changfa Wu",
        "Dunxian Huang",
        "Weiqiang Sun",
        "Ziyang Wang",
        "Yuliang Yan",
        "Jian Wu",
        "Yuning Jiang",
        "Bo Zheng",
        "Ke Chen",
        "Silu Zhou",
        "Yu Zhang"
      ],
      "abstract": "Recommendation systems are essential tools in modern e-commerce, facilitating personalized user experiences by suggesting relevant products. Recent advancements in generative models have demonstrated potential in enhancing recommendation systems; however, these models often exhibit limitations in optimizing retrieval tasks, primarily due to their reliance on autoregressive generation mechanisms. Conventional approaches introduce sequential dependencies that impede efficient retrieval, as they are inherently unsuitable for generating multiple items without positional constraints within a single request session. To address these limitations, we propose TBGRecall, a framework integrating Next Session Prediction (NSP), designed to enhance generative retrieval models for e-commerce applications. Our framework reformulation involves partitioning input samples into multi-session sequences, where each sequence comprises a session token followed by a set of item tokens, and then further incorporate multiple optimizations tailored to the generative task in retrieval scenarios. In terms of training methodology, our pipeline integrates limited historical data pre-training with stochastic partial incremental training, significantly improving training efficiency and emphasizing the superiority of data recency over sheer data volume. Our extensive experiments, conducted on public benchmarks alongside a large-scale industrial dataset from TaoBao, show TBGRecall outperforms the state-of-the-art recommendation methods, and exhibits a clear scaling law trend. Ultimately, NSP represents a significant advancement in the effectiveness of generative recommendation systems for e-commerce applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TBGRecallï¼Œè¿™æ˜¯ä¸€ç§é›†æˆ Next Session Prediction (NSP) çš„ç”Ÿæˆå¼æ£€ç´¢æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–ç”µå­å•†åŠ¡æ¨èç³»ç»Ÿä¸­çš„æ£€ç´¢ä»»åŠ¡ã€‚é’ˆå¯¹ä¼ ç»Ÿè‡ªå›å½’æ¨¡å‹åœ¨ç”Ÿæˆæ— ä½ç½®çº¦æŸçš„å¤šé¡¹å•†å“æ—¶é¢ä¸´çš„åºåˆ—ä¾èµ–å±€é™æ€§ï¼Œè¯¥æ¡†æ¶å°†è¾“å…¥æ ·æœ¬é‡æ–°è¡¨è¿°ä¸ºå¤šä¼šè¯åºåˆ— (multi-session sequences)ï¼Œé€šè¿‡ Session Token å’Œ Item Tokens çš„ç»„åˆå®ç°æ›´é«˜æ•ˆçš„ç”Ÿæˆå¼æ£€ç´¢ã€‚åœ¨è®­ç»ƒç­–ç•¥ä¸Šï¼ŒTBGRecall ç»“åˆäº†æœ‰é™å†å²æ•°æ®é¢„è®­ç»ƒä¸éšæœºéƒ¨åˆ†å¢é‡è®­ç»ƒ (Stochastic Partial Incremental Training)ï¼Œæ˜¾è‘—æå‡äº†è®­ç»ƒæ•ˆç‡ï¼Œå¹¶è¯æ˜äº†æ•°æ®æ—¶æ•ˆæ€§ (Data Recency) ä¼˜äºå•çº¯çš„æ•°æ®é‡ã€‚åœ¨å¤§è§„æ¨¡å·¥ä¸šæ•°æ®é›† TaoBao åŠå…¬å¼€åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒTBGRecall çš„è¡¨ç°ä¼˜äºå½“å‰çš„å…ˆè¿›æ¨èæ–¹æ³•ï¼Œå¹¶å±•ç°å‡ºæ¸…æ™°çš„ç¼©æ”¾å®šå¾‹ (Scaling Law) è¶‹åŠ¿ã€‚è¯¥ç ”ç©¶é€šè¿‡å¼•å…¥ NSP æœºåˆ¶ï¼Œä¸ºç”µå­å•†åŠ¡åœºæ™¯ä¸‹çš„ç”Ÿæˆå¼æ¨èç³»ç»Ÿæä¾›äº†æ˜¾è‘—çš„æ•ˆèƒ½æå‡ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "Both authors contributed equally to this research. Work done during internship at Alibaba. Corresponding author: Dunxian Huang (dunxian.hdx@alibaba-inc.com). Affiliations: (1) Shanghai Jiaotong University, Shanghai, China; (2) Alibaba Inc",
      "pdf_url": "https://arxiv.org/pdf/2508.11977v2",
      "published_date": "2025-08-16 08:31:11 UTC",
      "updated_date": "2025-11-23 13:35:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:16:12.510873+00:00"
    },
    {
      "arxiv_id": "2508.11975v1",
      "title": "Chart-CoCa: Self-Improving Chart Understanding of Vision LMs via Code-Driven Synthesis and Candidate-Conditioned Answering",
      "title_zh": "Chart-CoCaï¼šåŸºäºä»£ç é©±åŠ¨åˆæˆä¸å€™é€‰æ¡ä»¶å›ç­”çš„è§†è§‰è¯­è¨€æ¨¡å‹å›¾è¡¨ç†è§£è‡ªæˆ‘æ”¹è¿›",
      "authors": [
        "Gongyao Jiang",
        "Qiong Luo"
      ],
      "abstract": "Vision Language Models (VLMs) often struggle with chart understanding tasks, particularly in accurate chart description and complex reasoning. Synthetic data generation is a promising solution, while usually facing the challenge of noise labels. To address this challenge, we first introduce a chart synthesis pipeline that generates aligned chart-question-answer triplets through code generation and execution, ensuring the reliability of synthetic data without human intervention. Furthermore, inspired by test-time scaling that increases inference budget and thereby improves performance, we design a candidate-conditioned answering process. The VLM first generates multiple responses per query, and then synthesizes the final answer by contextualizing these candidates. Experiments demonstrate significant improvements, with up to 15.50 points accuracy gain over the initial VLM, in a fully self-improving paradigm without either human-labeled data or external models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨å›¾è¡¨ç†è§£ä»»åŠ¡ä¸­é¢ä¸´çš„å‡†ç¡®æè¿°ä¸å¤æ‚æ¨ç†æŒ‘æˆ˜ï¼Œæå‡ºäº†Chart-CoCaæ¡†æ¶ã€‚ä¸ºäº†è§£å†³åˆæˆæ•°æ®ä¸­å¸¸è§çš„å™ªå£°æ ‡ç­¾é—®é¢˜ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ç§ä»£ç é©±åŠ¨çš„åˆæˆæµæ°´çº¿(Code-Driven Synthesis Pipeline)ï¼Œé€šè¿‡ä»£ç ç”Ÿæˆä¸æ‰§è¡Œäº§ç”Ÿå¯¹é½çš„å›¾è¡¨-é—®é¢˜-ç­”æ¡ˆä¸‰å…ƒç»„ï¼Œç¡®ä¿äº†æ•°æ®åœ¨æ— éœ€äººå·¥å¹²é¢„ä¸‹çš„å¯é æ€§ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶å—æµ‹è¯•æ—¶ç¼©æ”¾(Test-Time Scaling)å¯å‘ï¼Œè®¾è®¡äº†ä¸€ç§å€™é€‰æ¡ä»¶å›ç­”(Candidate-Conditioned Answering)æœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé€šè¿‡æ•´åˆå¤šä¸ªç”Ÿæˆçš„å¤‡é€‰å“åº”æ¥åˆæˆæœ€ç»ˆç­”æ¡ˆã€‚å®éªŒè¯æ˜ï¼ŒChart-CoCaåœ¨å®Œå…¨è‡ªæ”¹è¿›(Self-Improving)çš„èŒƒå¼ä¸‹ï¼Œæ— éœ€äººå·¥æ ‡æ³¨æˆ–å¤–éƒ¨æ¨¡å‹è¾…åŠ©ï¼Œå³å¯ä½¿åˆå§‹æ¨¡å‹çš„å‡†ç¡®ç‡æœ€é«˜æå‡15.50ä¸ªç™¾åˆ†ç‚¹ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹å¯¹å¤æ‚å›¾è¡¨çš„ç†è§£ä¸æ¨ç†èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to CIKM 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.11975v1",
      "published_date": "2025-08-16 08:26:55 UTC",
      "updated_date": "2025-08-16 08:26:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:16:17.181097+00:00"
    },
    {
      "arxiv_id": "2508.11959v1",
      "title": "Rigorous Feature Importance Scores based on Shapley Value and Banzhaf Index",
      "title_zh": "åŸºäº Shapley å€¼ä¸ Banzhaf æŒ‡æ•°çš„ä¸¥è°¨ç‰¹å¾é‡è¦æ€§è¯„åˆ†",
      "authors": [
        "Xuanxiang Huang",
        "Olivier LÃ©toffÃ©",
        "Joao Marques-Silva"
      ],
      "abstract": "Feature attribution methods based on game theory are ubiquitous in the field of eXplainable Artificial Intelligence (XAI). Recent works proposed rigorous feature attribution using logic-based explanations, specifically targeting high-stakes uses of machine learning (ML) models. Typically, such works exploit weak abductive explanation (WAXp) as the characteristic function to assign importance to features. However, one possible downside is that the contribution of non-WAXp sets is neglected. In fact, non-WAXp sets can also convey important information, because of the relationship between formal explanations (XPs) and adversarial examples (AExs). Accordingly, this paper leverages Shapley value and Banzhaf index to devise two novel feature importance scores. We take into account non-WAXp sets when computing feature contribution, and the novel scores quantify how effective each feature is at excluding AExs. Furthermore, the paper identifies properties and studies the computational complexity of the proposed scores.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¯è§£é‡Šäººå·¥æ™ºèƒ½ (eXplainable Artificial Intelligence, XAI) é¢†åŸŸä¸­åŸºäºåšå¼ˆè®ºçš„ç‰¹å¾å½’å› æ–¹æ³•å±•å¼€è®¨è®ºï¼Œç‰¹åˆ«å…³æ³¨é€»è¾‘è§£é‡Šåœ¨æœºå™¨å­¦ä¹ æ¨¡å‹é«˜é£é™©åº”ç”¨ä¸­çš„ä¸¥è°¨æ€§ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨å¼±è¯±å¯¼è§£é‡Š (weak abductive explanation, WAXp) ä½œä¸ºç‰¹å¾å‡½æ•°æ¥è¡¡é‡ç‰¹å¾é‡è¦æ€§ï¼Œä½†å…¶ç¼ºç‚¹åœ¨äºå¿½ç•¥äº†é WAXp é›†åˆä¸­åŒ…å«çš„é‡è¦ä¿¡æ¯ã€‚ç”±äºå½¢å¼åŒ–è§£é‡Š (XPs) ä¸å¯¹æŠ—æ ·æœ¬ (AExs) ä¹‹é—´å­˜åœ¨ç´§å¯†å…³è”ï¼Œè¯¥è®ºæ–‡åˆ©ç”¨ Shapley value å’Œ Banzhaf index è®¾è®¡äº†ä¸¤ç§æ–°å‹ç‰¹å¾é‡è¦æ€§è¯„åˆ†ã€‚è¿™äº›æ–°è¯„åˆ†åœ¨è®¡ç®—ç‰¹å¾è´¡çŒ®æ—¶å°†é WAXp é›†åˆçº³å…¥è€ƒé‡ï¼Œä»è€Œèƒ½å¤Ÿé‡åŒ–æ¯ä¸ªç‰¹å¾åœ¨æ’é™¤å¯¹æŠ—æ ·æœ¬ (AExs) æ–¹é¢çš„æœ‰æ•ˆç¨‹åº¦ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜ç¡®å®šäº†æ‰€æè¯„åˆ†çš„ç›¸å…³æ€§è´¨ï¼Œå¹¶å¯¹å…¶è®¡ç®—å¤æ‚åº¦è¿›è¡Œäº†æ·±å…¥åˆ†æã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11959v1",
      "published_date": "2025-08-16 07:41:25 UTC",
      "updated_date": "2025-08-16 07:41:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:16:15.387714+00:00"
    },
    {
      "arxiv_id": "2508.11957v1",
      "title": "A Comprehensive Review of AI Agents: Transforming Possibilities in Technology and Beyond",
      "title_zh": "AI æ™ºèƒ½ä½“å…¨æ–¹ä½ç»¼è¿°ï¼šå˜é©æŠ€æœ¯åŠå…¶ä¹‹å¤–çš„æ— é™å¯èƒ½",
      "authors": [
        "Xiaodong Qu",
        "Andrews Damoah",
        "Joshua Sherwood",
        "Peiyan Liu",
        "Christian Shun Jin",
        "Lulu Chen",
        "Minjie Shen",
        "Nawwaf Aleisa",
        "Zeyuan Hou",
        "Chenyu Zhang",
        "Lifu Gao",
        "Yanshu Li",
        "Qikai Yang",
        "Qun Wang",
        "Cristabelle De Souza"
      ],
      "abstract": "Artificial Intelligence (AI) agents have rapidly evolved from specialized, rule-based programs to versatile, learning-driven autonomous systems capable of perception, reasoning, and action in complex environments. The explosion of data, advances in deep learning, reinforcement learning, and multi-agent coordination have accelerated this transformation. Yet, designing and deploying unified AI agents that seamlessly integrate cognition, planning, and interaction remains a grand challenge. In this review, we systematically examine the architectural principles, foundational components, and emergent paradigms that define the landscape of contemporary AI agents. We synthesize insights from cognitive science-inspired models, hierarchical reinforcement learning frameworks, and large language model-based reasoning. Moreover, we discuss the pressing ethical, safety, and interpretability concerns associated with deploying these agents in real-world scenarios. By highlighting major breakthroughs, persistent challenges, and promising research directions, this review aims to guide the next generation of AI agent systems toward more robust, adaptable, and trustworthy autonomous intelligence.",
      "tldr_zh": "è¯¥ç»¼è¿°ç³»ç»Ÿæ€§åœ°å›é¡¾äº†äººå·¥æ™ºèƒ½æ™ºèƒ½ä½“(AI Agents)ä»åŸºäºè§„åˆ™çš„ä¸“ç”¨ç¨‹åºå‘å…·å¤‡æ„ŸçŸ¥ã€æ¨ç†å’Œè¡ŒåŠ¨èƒ½åŠ›çš„é€šç”¨è‡ªä¸»å­¦ä¹ ç³»ç»Ÿçš„æ¼”è¿›å†ç¨‹ã€‚æ–‡ç« è¯¦ç»†æ¢è®¨äº†å®šä¹‰å½“ä»£ AI Agents æ™¯è§‚çš„æ¶æ„åŸåˆ™ã€åŸºç¡€ç»„ä»¶å’Œæ–°å…´èŒƒå¼ï¼Œå¹¶é‡ç‚¹åˆ†æäº†è®¤çŸ¥ç§‘å­¦å¯å‘æ¨¡å‹ã€åˆ†å±‚å¼ºåŒ–å­¦ä¹ (Hierarchical Reinforcement Learning)æ¡†æ¶ä»¥åŠåŸºäºå¤§è¯­è¨€æ¨¡å‹(Large Language Model)çš„æ¨ç†æŠ€æœ¯ã€‚é’ˆå¯¹åœ¨å¤æ‚ç¯å¢ƒä¸­æ— ç¼é›†æˆè®¤çŸ¥ã€è§„åˆ’å’Œäº¤äº’çš„æŒ‘æˆ˜ï¼Œè¯¥ç ”ç©¶ç»¼åˆäº†å¤šç»´åº¦çš„æŠ€æœ¯æ´å¯Ÿï¼Œæ—¨åœ¨ä¸ºè®¾è®¡æ›´å…·é²æ£’æ€§å’Œé€‚åº”æ€§çš„ç³»ç»Ÿæä¾›ç†è®ºæ”¯æŒã€‚æ­¤å¤–ï¼Œç»¼è¿°æ·±å…¥è®¨è®ºäº†åœ¨ç°å®åœºæ™¯ä¸­éƒ¨ç½²æ™ºèƒ½ä½“æ—¶æ¶‰åŠçš„ä¼¦ç†ã€å®‰å…¨æ€§å’Œå¯è§£é‡Šæ€§(Interpretability)ç­‰å…³é”®é—®é¢˜ã€‚é€šè¿‡æ€»ç»“æŠ€æœ¯çªç ´ä¸ç ”ç©¶ç©ºç™½ï¼Œè¯¥ç»¼è¿°ä¸ºä¸‹ä¸€ä»£è‡ªä¸»æ™ºèƒ½æŠ€æœ¯æŒ‡æ˜äº†æ–¹å‘ï¼Œè‡´åŠ›äºæ¨åŠ¨æ›´å¯é ä¸”å¯ä¿¡çš„ AI Agents å‘å±•ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11957v1",
      "published_date": "2025-08-16 07:38:45 UTC",
      "updated_date": "2025-08-16 07:38:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:16:19.866750+00:00"
    },
    {
      "arxiv_id": "2508.11954v1",
      "title": "UniCast: A Unified Multimodal Prompting Framework for Time Series Forecasting",
      "title_zh": "UniCastï¼šé¢å‘æ—¶é—´åºåˆ—é¢„æµ‹çš„ç»Ÿä¸€å¤šæ¨¡æ€æç¤ºæ¡†æ¶",
      "authors": [
        "Sehyuk Park",
        "Soyeon Caren Han",
        "Eduard Hovy"
      ],
      "abstract": "Time series forecasting is a foundational task across domains, such as finance, healthcare, and environmental monitoring. While recent advances in Time Series Foundation Models (TSFMs) have demonstrated strong generalisation through large-scale pretraining, existing models operate predominantly in a unimodal setting, ignoring the rich multimodal context, such as visual and textual signals, that often accompanies time series data in real-world scenarios. This paper introduces a novel parameter-efficient multimodal framework, UniCast, that extends TSFMs to jointly leverage time series, vision, and text modalities for enhanced forecasting performance. Our method integrates modality-specific embeddings from pretrained Vision and Text Encoders with a frozen TSFM via soft prompt tuning, enabling efficient adaptation with minimal parameter updates. This design not only preserves the generalisation strength of the foundation model but also enables effective cross-modal interaction. Extensive experiments across diverse time-series forecasting benchmarks demonstrate that UniCast consistently and significantly outperforms all existing TSFM baselines. The findings highlight the critical role of multimodal context in advancing the next generation of general-purpose time series forecasters.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰çš„æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ (Time Series Foundation Models, TSFMs) ä¸»è¦å±€é™äºå•æ¨¡æ€ã€å¿½ç•¥äº†ç°å®ä¸–ç•Œä¸­è§†è§‰å’Œæ–‡æœ¬ç­‰è·¨æ¨¡æ€èƒŒæ™¯ä¿¡æ¯çš„é—®é¢˜ï¼Œæå‡ºäº† UniCast è¿™ä¸€ç»Ÿä¸€çš„å¤šæ¨¡æ€æç¤ºæ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å‚æ•°é«˜æ•ˆ (parameter-efficient) çš„æ–¹æ³•ï¼Œé€šè¿‡è½¯æç¤ºå¾®è°ƒ (soft prompt tuning) å°†é¢„è®­ç»ƒè§†è§‰å’Œæ–‡æœ¬ç¼–ç å™¨çš„æ¨¡æ€ç‰¹å®šåµŒå…¥é›†æˆåˆ°å†»ç»“çš„ TSFM ä¸­ï¼Œä»è€Œå¢å¼ºé¢„æµ‹æ€§èƒ½ã€‚è¿™ç§è®¾è®¡åœ¨ä¿ç•™åŸºç¡€æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„åŒæ—¶ï¼Œå®ç°äº†æœ‰æ•ˆçš„è·¨æ¨¡æ€äº¤äº’ï¼Œä¸”ä»…éœ€æå°‘é‡çš„å‚æ•°æ›´æ–°ã€‚åœ¨å¤šä¸ªæ—¶é—´åºåˆ—é¢„æµ‹åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒUniCast çš„è¡¨ç°ä¸€è‡´ä¸”æ˜¾è‘—ä¼˜äºç°æœ‰çš„æ‰€æœ‰ TSFM åŸºçº¿æ¨¡å‹ã€‚è¯¥ç ”ç©¶ç»“æœå¼ºè°ƒäº†å¤šæ¨¡æ€ä¸Šä¸‹æ–‡åœ¨æ¨åŠ¨ä¸‹ä¸€ä»£é€šç”¨æ—¶é—´åºåˆ—é¢„æµ‹æŠ€æœ¯å‘å±•ä¸­çš„å…³é”®ä½œç”¨ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11954v1",
      "published_date": "2025-08-16 07:33:27 UTC",
      "updated_date": "2025-08-16 07:33:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:16:27.961652+00:00"
    },
    {
      "arxiv_id": "2508.11953v1",
      "title": "Data Mixing Optimization for Supervised Fine-Tuning of Large Language Models",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹æœ‰ç›‘ç£å¾®è°ƒçš„æ•°æ®é…æ¯”ä¼˜åŒ–",
      "authors": [
        "Yuan Li",
        "Zhengzhong Liu",
        "Eric Xing"
      ],
      "abstract": "Optimizing data mixtures for supervised fine-tuning (SFT) of large language models (LLMs) is critical for developing general-purpose models, yet this area remains underexplored. In this paper, we frame data mixing as an optimization problem and introduce a novel method designed to minimize validation loss. Our approach parametrizes the loss by modeling effective data transferred and leveraging scaling laws for fine-tuning. By experimenting with various small-scale data mixtures, we fit these parameters and derive the optimal weights. We provide both mathematical proofs and empirical results demonstrating that our algorithm achieves excellent overall and individual performance across all domains. Through controlled experiments, we show that models trained with our optimized weights perform on par with those using optimal weights determined via grid search, with per-domain loss only 0.66% higher than the best domain loss from grid search on average. Additionally, we show that reweighting popular SFT datasets using our method improves both validation loss and downstream performance. Finally, we discuss how our method can generalize to guide data selection for domain-specific models and provide insights into SFT.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„æœ‰ç›‘ç£å¾®è°ƒ(SFT)ä¸­çš„æ•°æ®æ··åˆä¼˜åŒ–é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ—¨åœ¨æœ€å°åŒ–éªŒè¯æŸå¤±çš„æ–°å‹ä¼˜åŒ–æ–¹æ³•ã€‚ä½œè€…å°†æ•°æ®æ··åˆå»ºæ¨¡ä¸ºå—æœ‰æ•ˆæ•°æ®è½¬ç§»å’Œå¾®è°ƒscaling lawså½±å“çš„å‚æ•°åŒ–æŸå¤±å‡½æ•°ï¼Œé€šè¿‡åœ¨å°è§„æ¨¡å®éªŒä¸­æ‹Ÿåˆå‚æ•°æ¥è®¡ç®—æœ€ä¼˜æƒé‡ã€‚æ•°å­¦è¯æ˜å’Œå®è¯ç»“æœæ˜¾ç¤ºï¼Œè¯¥ç®—æ³•åœ¨å„é¢†åŸŸå‡è¡¨ç°å‡ºè‰²ï¼Œå…¶ä¼˜åŒ–åçš„æ¨¡å‹æ€§èƒ½ä¸é€šè¿‡è€—æ—¶çš„grid searchè·å–çš„æœ€ä¼˜ç»“æœç›¸å½“ï¼Œå¹³å‡æ¯é¢†åŸŸæŸå¤±ä»…é«˜å‡º0.66%ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¯æ˜å¯¹æµè¡Œçš„SFTæ•°æ®é›†è¿›è¡Œé‡æ–°åŠ æƒèƒ½æœ‰æ•ˆæ”¹å–„éªŒè¯æŸå¤±å¹¶æå‡ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚è¯¥æ–¹æ³•è¿˜å¯ä»¥æ¨å¹¿åˆ°å¼•å¯¼ç‰¹å®šé¢†åŸŸæ¨¡å‹çš„æ•°æ®é€‰æ‹©ï¼Œä¸ºæ·±å…¥ç†è§£SFTæœºåˆ¶æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11953v1",
      "published_date": "2025-08-16 07:28:39 UTC",
      "updated_date": "2025-08-16 07:28:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:16:34.287842+00:00"
    },
    {
      "arxiv_id": "2508.11944v1",
      "title": "CHBench: A Cognitive Hierarchy Benchmark for Evaluating Strategic Reasoning Capability of LLMs",
      "title_zh": "CHBenchï¼šç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ç­–ç•¥æ¨ç†èƒ½åŠ›çš„è®¤çŸ¥å±‚çº§è¯„æµ‹åŸºå‡†",
      "authors": [
        "Hongtao Liu",
        "Zhicheng Du",
        "Zihe Wang",
        "Weiran Shen"
      ],
      "abstract": "Game-playing ability serves as an indicator for evaluating the strategic reasoning capability of large language models (LLMs). While most existing studies rely on utility performance metrics, which are not robust enough due to variations in opponent behavior and game structure. To address this limitation, we propose \\textbf{Cognitive Hierarchy Benchmark (CHBench)}, a novel evaluation framework inspired by the cognitive hierarchy models from behavioral economics. We hypothesize that agents have bounded rationality -- different agents behave at varying reasoning depths/levels. We evaluate LLMs' strategic reasoning through a three-phase systematic framework, utilizing behavioral data from six state-of-the-art LLMs across fifteen carefully selected normal-form games. Experiments show that LLMs exhibit consistent strategic reasoning levels across diverse opponents, confirming the framework's robustness and generalization capability. We also analyze the effects of two key mechanisms (Chat Mechanism and Memory Mechanism) on strategic reasoning performance. Results indicate that the Chat Mechanism significantly degrades strategic reasoning, whereas the Memory Mechanism enhances it. These insights position CHBench as a promising tool for evaluating LLM capabilities, with significant potential for future research and practical applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CHBench (Cognitive Hierarchy Benchmark)ï¼Œè¿™æ˜¯ä¸€ä¸ªå—è¡Œä¸ºç»æµå­¦ä¸­è®¤çŸ¥å±‚æ¬¡æ¨¡å‹å¯å‘çš„å…¨æ–°è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ç­–ç•¥æ¨ç†èƒ½åŠ›è¯„ä¼°ä¸­å› å¯¹æ‰‹è¡Œä¸ºå’Œåšå¼ˆç»“æ„å˜åŒ–è€Œå¯¼è‡´çš„ç¨³å¥æ€§ä¸è¶³é—®é¢˜ã€‚è¯¥æ¡†æ¶åŸºäºæœ‰é™ç†æ€§(bounded rationality)å‡è®¾ï¼Œè®¤ä¸ºä¸åŒçš„æ™ºèƒ½ä½“å¤„äºä¸åŒçš„æ¨ç†æ·±åº¦æˆ–å±‚æ¬¡ã€‚ç ”ç©¶è€…é€šè¿‡ä¸‰é˜¶æ®µç³»ç»Ÿæ¡†æ¶ï¼Œåˆ©ç”¨å…­ç§å…ˆè¿›å¤§è¯­è¨€æ¨¡å‹åœ¨åäº”ä¸ªç²¾å¿ƒæŒ‘é€‰çš„æ ‡å‡†åšå¼ˆ(normal-form games)ä¸­çš„è¡Œä¸ºæ•°æ®è¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¤§è¯­è¨€æ¨¡å‹åœ¨é¢å¯¹ä¸åŒå¯¹æ‰‹æ—¶è¡¨ç°å‡ºä¸€è‡´çš„ç­–ç•¥æ¨ç†æ°´å¹³ï¼Œè¯å®äº†è¯¥æ¡†æ¶çš„ç¨³å¥æ€§(robustness)å’Œæ³›åŒ–èƒ½åŠ›(generalization capability)ã€‚ç ”ç©¶è¿›ä¸€æ­¥åˆ†æäº†ä¸¤ç§å…³é”®æœºåˆ¶å¯¹æ€§èƒ½çš„å½±å“ï¼Œå‘ç°èŠå¤©æœºåˆ¶(Chat Mechanism)ä¼šæ˜¾è‘—é™ä½ç­–ç•¥æ¨ç†æ°´å¹³ï¼Œè€Œè®°å¿†æœºåˆ¶(Memory Mechanism)åˆ™èƒ½å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚è¿™äº›è§è§£ä½¿CHBenchæˆä¸ºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹èƒ½åŠ›çš„æœ‰åŠ›å·¥å…·ï¼Œåœ¨æœªæ¥çš„ç ”ç©¶å’Œå®é™…åº”ç”¨ä¸­å…·æœ‰é‡è¦ä»·å€¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11944v1",
      "published_date": "2025-08-16 07:10:26 UTC",
      "updated_date": "2025-08-16 07:10:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:16:30.858802+00:00"
    },
    {
      "arxiv_id": "2508.11940v1",
      "title": "Extending Straight-Through Estimation for Robust Neural Networks on Analog CIM Hardware",
      "title_zh": "é¢å‘æ¨¡æ‹Ÿå­˜ç®—ä¸€ä½“ç¡¬ä»¶é²æ£’ç¥ç»ç½‘ç»œçš„ç›´é€šä¼°è®¡æ‰©å±•",
      "authors": [
        "Yuannuo Feng",
        "Wenyong Zhou",
        "Yuexi Lyu",
        "Yixiang Zhang",
        "Zhengwu Liu",
        "Ngai Wong",
        "Wang Kang"
      ],
      "abstract": "Analog Compute-In-Memory (CIM) architectures promise significant energy efficiency gains for neural network inference, but suffer from complex hardware-induced noise that poses major challenges for deployment. While noise-aware training methods have been proposed to address this issue, they typically rely on idealized and differentiable noise models that fail to capture the full complexity of analog CIM hardware variations. Motivated by the Straight-Through Estimator (STE) framework in quantization, we decouple forward noise simulation from backward gradient computation, enabling noise-aware training with more accurate but computationally intractable noise modeling in analog CIM systems. We provide theoretical analysis demonstrating that our approach preserves essential gradient directional information while maintaining computational tractability and optimization stability. Extensive experiments show that our extended STE framework achieves up to 5.3% accuracy improvement on image classification, 0.72 perplexity reduction on text generation, 2.2$\\times$ speedup in training time, and 37.9% lower peak memory usage compared to standard noise-aware training methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ¨¡æ‹Ÿå­˜å†…è®¡ç®—(Analog Compute-In-Memory, CIM)æ¶æ„ä¸­ç¡¬ä»¶å™ªå£°å¯¼è‡´çš„éƒ¨ç½²éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ‰©å±•çš„ Straight-Through Estimator (STE) æ¡†æ¶ï¼Œæ—¨åœ¨æå‡ç¥ç»ç½‘ç»œçš„é²æ£’æ€§ã€‚å—é‡åŒ–æŠ€æœ¯çš„å¯å‘ï¼Œè¯¥æ¡†æ¶å°†å‰å‘è·¯å¾„çš„å¤æ‚å™ªå£°æ¨¡æ‹Ÿä¸åå‘è·¯å¾„çš„æ¢¯åº¦è®¡ç®—è§£è€¦ï¼Œä»è€Œå…è®¸åœ¨å™ªå£°æ„ŸçŸ¥è®­ç»ƒ(noise-aware training)ä¸­ä½¿ç”¨æ¯”ä¼ ç»Ÿå¯å¾®æ¨¡å‹æ›´ç²¾ç¡®çš„ç¡¬ä»¶æè¿°ã€‚ç†è®ºåˆ†æè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒè®¡ç®—å¯å¤„ç†æ€§å’Œä¼˜åŒ–ç¨³å®šæ€§çš„åŒæ—¶ï¼Œèƒ½æœ‰æ•ˆä¿ç•™å…³é”®çš„æ¢¯åº¦æ–¹å‘ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å›¾åƒåˆ†ç±»ä¸­å®ç°äº†5.3%çš„å‡†ç¡®ç‡æå‡ï¼Œåœ¨æ–‡æœ¬ç”Ÿæˆä¸­é™ä½äº†0.72çš„å›°æƒ‘åº¦(perplexity)ï¼Œå¹¶å¸¦æ¥äº†2.2å€çš„è®­ç»ƒåŠ é€Ÿå’Œ37.9%çš„å³°å€¼å†…å­˜å‰Šå‡ã€‚è¯¥æ–¹æ¡ˆä¸ºåœ¨èµ„æºå—é™çš„æ¨¡æ‹Ÿç¡¬ä»¶ä¸Šå®ç°é«˜æ•ˆã€ç¨³å¥çš„æ·±åº¦å­¦ä¹ æ¨ç†æä¾›äº†å…¨æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.AR"
      ],
      "primary_category": "cs.LG",
      "comment": "4 pages, 5 figures, conference",
      "pdf_url": "https://arxiv.org/pdf/2508.11940v1",
      "published_date": "2025-08-16 06:53:44 UTC",
      "updated_date": "2025-08-16 06:53:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:16:37.853162+00:00"
    },
    {
      "arxiv_id": "2508.11935v1",
      "title": "HPD: Hybrid Projection Decomposition for Robust State Space Models on Analog CIM Hardware",
      "title_zh": "HPDï¼šé¢å‘æ¨¡æ‹Ÿå­˜ç®—ä¸€ä½“ç¡¬ä»¶é²æ£’çŠ¶æ€ç©ºé—´æ¨¡å‹çš„æ··åˆæŠ•å½±åˆ†è§£",
      "authors": [
        "Yuannuo Feng",
        "Wenyong Zhou",
        "Yuexi Lyu",
        "Hanjie Liu",
        "Zhengwu Liu",
        "Ngai Wong",
        "Wang Kang"
      ],
      "abstract": "State Space Models (SSMs) are efficient alternatives to traditional sequence models, excelling at processing long sequences with lower computational complexity. Their reliance on matrix multiplications makes them ideal for compute-in-memory (CIM) architectures, which improve energy efficiency by computing within memory arrays. However, device non-idealities in CIM introduce weight perturbations that can degrade inference accuracy. In this paper, we systematically analyze the robustness of SSMs under noisy conditions, identifying that the final block and output projection layers are more susceptible to perturbations compared to other components. Building on these insights, we propose HPD, a Hybrid Projection Decomposition strategy for the last output projection layer. We replace the original weight matrix with the multiplication of U and Î£ in its SVD to ensure compatibility with existing hardware architectures, while offloading V> to digital hardware for precise and robust correction. Comprehensive tests on Mamba models show that our method reduces perplexity by up to 99.57% under various noise conditions compared to baseline models, with accuracy gains of up to 96.67% on the PIQA benchmark for commonsense reasoning.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹çŠ¶æ€ç©ºé—´æ¨¡å‹ (State Space Models, SSMs) åœ¨æ¨¡æ‹Ÿå­˜å†…è®¡ç®— (Compute-In-Memory, CIM) ç¡¬ä»¶ä¸Šå› è®¾å¤‡éç†æƒ³æ€§å¯¼è‡´çš„æƒé‡æ‰°åŠ¨å’Œæ¨ç†ç²¾åº¦ä¸‹é™é—®é¢˜è¿›è¡Œäº†ç³»ç»Ÿæ€§åˆ†æã€‚é€šè¿‡è¯„ä¼°å‘ç°ï¼ŒSSMs çš„æœ€åä¸€ä¸ªå—å’Œè¾“å‡ºæŠ•å½±å±‚ç›¸æ¯”å…¶ä»–ç»„ä»¶å¯¹æ‰°åŠ¨è¡¨ç°å‡ºæ›´é«˜çš„æ•æ„Ÿæ€§ã€‚åŸºäºæ­¤å‘ç°ï¼Œè®ºæ–‡æå‡ºäº† HPDï¼ˆæ··åˆæŠ•å½±åˆ†è§£ï¼ŒHybrid Projection Decompositionï¼‰ç­–ç•¥ï¼Œé€šè¿‡å¥‡å¼‚å€¼åˆ†è§£ (SVD) å°†æœ€åä¸€å±‚è¾“å‡ºæŠ•å½±çŸ©é˜µè¿›è¡Œæ‹†åˆ†ã€‚è¯¥æ–¹æ³•å°†æƒé‡çŸ©é˜µçš„ $U$ å’Œ $\\Sigma$ ä¹˜ç§¯ä¿ç•™åœ¨å­˜å†…è®¡ç®—é˜µåˆ—ä¸­ä»¥å…¼å®¹ç°æœ‰ç¡¬ä»¶æ¶æ„ï¼ŒåŒæ—¶å°† $V^T$ å¸è½½è‡³æ•°å­—ç¡¬ä»¶ä»¥å®ç°ç²¾ç¡®ä¸”é²æ£’çš„è¯¯å·®ä¿®æ­£ã€‚é’ˆå¯¹ Mamba æ¨¡å‹çš„å®éªŒè¡¨æ˜ï¼Œåœ¨å¤šç§å™ªå£°æ¡ä»¶ä¸‹ï¼ŒHPD ç›¸æ¯”åŸºçº¿æ¨¡å‹å¯å°†å›°æƒ‘åº¦ (Perplexity) é™ä½è¾¾ 99.57%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨ PIQA å¸¸è¯†æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†é«˜è¾¾ 96.67% çš„å‡†ç¡®ç‡æå‡ï¼Œæœ‰æ•ˆéªŒè¯äº†å…¶åœ¨æå‡æ¨¡æ‹Ÿå­˜å†…è®¡ç®—æ¶æ„ä¸‹ SSMs é²æ£’æ€§æ–¹é¢çš„æ˜¾è‘—è´¡çŒ®ã€‚",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AR",
      "comment": "4 pages, 5 figures, conference",
      "pdf_url": "https://arxiv.org/pdf/2508.11935v1",
      "published_date": "2025-08-16 06:34:14 UTC",
      "updated_date": "2025-08-16 06:34:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:16:34.058259+00:00"
    },
    {
      "arxiv_id": "2508.11929v1",
      "title": "No More Blind Spots: Learning Vision-Based Omnidirectional Bipedal Locomotion for Challenging Terrain",
      "title_zh": "å‘Šåˆ«ç›²ç‚¹ï¼šé¢å‘å¤æ‚åœ°å½¢çš„è§†è§‰å…¨å‘åŒè¶³è¿åŠ¨å­¦ä¹ ",
      "authors": [
        "Mohitvishnu S. Gadde",
        "Pranay Dugar",
        "Ashish Malik",
        "Alan Fern"
      ],
      "abstract": "Effective bipedal locomotion in dynamic environments, such as cluttered indoor spaces or uneven terrain, requires agile and adaptive movement in all directions. This necessitates omnidirectional terrain sensing and a controller capable of processing such input. We present a learning framework for vision-based omnidirectional bipedal locomotion, enabling seamless movement using depth images. A key challenge is the high computational cost of rendering omnidirectional depth images in simulation, making traditional sim-to-real reinforcement learning (RL) impractical. Our method combines a robust blind controller with a teacher policy that supervises a vision-based student policy, trained on noise-augmented terrain data to avoid rendering costs during RL and ensure robustness. We also introduce a data augmentation technique for supervised student training, accelerating training by up to 10 times compared to conventional methods. Our framework is validated through simulation and real-world tests, demonstrating effective omnidirectional locomotion with minimal reliance on expensive rendering. This is, to the best of our knowledge, the first demonstration of vision-based omnidirectional bipedal locomotion, showcasing its adaptability to diverse terrains.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒè¶³æœºå™¨äººåœ¨å¤æ‚åŠ¨æ€åœ°å½¢ä¸­éœ€è¦å…¨å‘æ•æ·ç§»åŠ¨çš„éœ€æ±‚ï¼Œæå‡ºäº†ä¸€ä¸ªåŸºäºè§†è§‰çš„å…¨å‘åŒè¶³è¿åŠ¨å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨æ·±åº¦å›¾åƒ(depth images)å®ç°æ— ç¼ç§»åŠ¨ã€‚ä¸ºäº†å…‹æœæ¨¡æ‹Ÿä¸­å…¨å‘æ·±åº¦æ¸²æŸ“çš„é«˜è®¡ç®—æˆæœ¬é—®é¢˜ï¼Œè¯¥æ–¹æ³•é‡‡ç”¨é²æ£’çš„ç›²è§†æ§åˆ¶å™¨ç»“åˆæ•™å¸ˆç­–ç•¥(teacher policy)æ¥æŒ‡å¯¼è§†è§‰å­¦ç”Ÿç­–ç•¥(student policy)çš„è®­ç»ƒï¼Œæœ‰æ•ˆè§„é¿äº†å¼ºåŒ–å­¦ä¹ (RL)è¿‡ç¨‹ä¸­çš„æ¸²æŸ“å¼€é”€ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†ä¸€ç§æ–°å‹æ•°æ®å¢å¼ºæŠ€æœ¯ç”¨äºç›‘ç£å­¦ä¹ ï¼Œä½¿è®­ç»ƒæ•ˆç‡æå‡äº†10å€ã€‚é€šè¿‡ä»¿çœŸå’ŒçœŸå®ä¸–ç•Œæµ‹è¯•éªŒè¯ï¼Œè¯¥æ¡†æ¶å±•ç¤ºäº†æä½³çš„é€‚åº”æ€§å’Œé²æ£’æ€§ï¼Œèƒ½å¤Ÿåœ¨å‡å°‘å¯¹æ˜‚è´µæ¸²æŸ“ä¾èµ–çš„åŒæ—¶å®ç°ç²¾å‡†çš„åœ°å½¢æ„ŸçŸ¥ã€‚è¿™æ˜¯é¦–ä¸ªå®ç°åŸºäºè§†è§‰çš„å…¨å‘åŒè¶³è¿åŠ¨(vision-based omnidirectional bipedal locomotion)çš„æ¡ˆä¾‹ï¼ŒæˆåŠŸæ¶ˆé™¤äº†æœºå™¨äººåœ¨æŒ‘æˆ˜æ€§åœ°å½¢ä¸­çš„è¿åŠ¨ç›²ç‚¹ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11929v1",
      "published_date": "2025-08-16 06:20:46 UTC",
      "updated_date": "2025-08-16 06:20:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:16:50.566792+00:00"
    },
    {
      "arxiv_id": "2508.13204v1",
      "title": "QuickMerge++: Fast Token Merging with Autoregressive Prior",
      "title_zh": "QuickMerge++ï¼šåŸºäºè‡ªå›å½’å…ˆéªŒçš„å¿«é€Ÿ Token åˆå¹¶",
      "authors": [
        "Dong Liu",
        "Yanxuan Yu"
      ],
      "abstract": "As generative models scale to larger inputs across language, vision, and video domains, the cost of token-level computation has become a key bottleneck. While prior work suggests that only a subset of tokens significantly influence downstream predictions, most token selection methods are static, modality-specific, or incompatible with autoregressive generation. In this paper, we propose QuickMerge, a lightweight token merging framework designed for efficient next-token prediction.\n  QuickMerge dynamically selects a reduced number of tokens based on attention norm magnitude, guided by an entropy-based budget estimator. To preserve autoregressive compatibility, we introduce a lightweight transformer prior trained over the merged token sequence. By combining semantic salience estimation, flexible token budgets, and AR alignment, QuickMerge enables accurate generation with fewer tokens.\n  We evaluate QuickMerge across multi-modality domains, demonstrating consistent improvements in compute-accuracy tradeoffs. Specifically, QuickMerge reduces token counts sustantially while matching as well as exceeding the performance of learned tokenizers and fixed-patch baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†QuickMerge++ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨æé«˜è‡ªå›å½’ç”Ÿæˆæ•ˆç‡çš„è½»é‡çº§Token Mergingæ¡†æ¶ï¼Œä»¥è§£å†³å¤§è§„æ¨¡ç”Ÿæˆæ¨¡å‹ä¸­Tokençº§åˆ«è®¡ç®—æˆæœ¬è¿‡é«˜çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨Attention Normé‡çº§åŠ¨æ€é€‰æ‹©æ ¸å¿ƒTokenï¼Œå¹¶ç»“åˆåŸºäºç†µï¼ˆEntropyï¼‰çš„é¢„ç®—è¯„ä¼°å™¨æ¥çµæ´»è°ƒæ•´å¤„ç†è§„æ¨¡ã€‚ä¸ºäº†ä¿æŒå¯¹è‡ªå›å½’ï¼ˆAutoregressiveï¼‰ç”Ÿæˆçš„å…¼å®¹æ€§ï¼Œç ”ç©¶å¼•å…¥äº†åœ¨åˆå¹¶åºåˆ—ä¸Šè®­ç»ƒçš„è½»é‡çº§Transformerå…ˆéªŒè¿›è¡Œå¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒQuickMerge++åœ¨å¤šä¸ªæ¨¡æ€é¢†åŸŸæ˜¾è‘—å‡å°‘äº†Tokenæ•°é‡ï¼ŒåŒæ—¶åœ¨è®¡ç®—æ•ˆç‡ä¸å‡†ç¡®ç‡çš„æƒè¡¡ä¸ŠåŒ¹é…ç”šè‡³è¶…è¶Šäº†Learned Tokenizerså’ŒFixed-patchç­‰åŸºå‡†æ¨¡å‹ã€‚è¿™ä¸€æ–¹æ¡ˆä¸ºè·¨æ¨¡æ€çš„é«˜æ•ˆè‡ªå›å½’å»ºæ¨¡æä¾›äº†ä¸€ç§å…¼å®¹æ€§å¼ºä¸”æ€§èƒ½ä¼˜å¼‚çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "The paper has been accepted to ICML Tokshop at https://openreview.net/forum?id=dMdxHd0tRf",
      "pdf_url": "https://arxiv.org/pdf/2508.13204v1",
      "published_date": "2025-08-16 06:07:33 UTC",
      "updated_date": "2025-08-16 06:07:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:16:55.589403+00:00"
    },
    {
      "arxiv_id": "2508.11921v1",
      "title": "ENA: Efficient N-dimensional Attention",
      "title_zh": "ENAï¼šé«˜æ•ˆNç»´æ³¨æ„åŠ›æœºåˆ¶",
      "authors": [
        "Yibo Zhong"
      ],
      "abstract": "Efficient modeling of long sequences of high-order data requires a more efficient architecture than Transformer. In this paper, we investigate two key aspects of extending linear recurrent models, especially those originally designed for language modeling, to high-order data (1D to ND): scanning strategies and attention-hybrid architectures. Empirical results suggest that scanning provides limited benefits, while attention-hybrid models yield promising results. Focusing on the latter, we further evaluate types of attention and find that tiled high-order sliding window attention (SWA) is efficient in both theory and practice. We term the resulting hybrid architecture of linear recurrence and high-order SWA as Efficient N-dimensional Attention (ENA). We then conduct several experiments to demonstrate its effectiveness. The intuition behind ENA is that linear recurrence compresses global information into a state, while SWA complements it by enforcing strict local modeling. Together, they form a simple framework that offers a promising and practical solution for ultra-long high-order data modeling.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é«˜é˜¶æ•°æ®ï¼ˆN-dimensional dataï¼‰é•¿åºåˆ—å»ºæ¨¡ä¸­ Transformer æ¶æ„æ•ˆç‡ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº† Efficient N-dimensional Attention (ENA) æ¶æ„ã€‚ç ”ç©¶è€…æ·±å…¥æ¢è®¨äº†å°†çº¿æ€§é€’å½’æ¨¡å‹ï¼ˆlinear recurrent modelsï¼‰æ‰©å±•è‡³é«˜é˜¶æ•°æ®çš„æ‰«æç­–ç•¥ï¼ˆscanning strategiesï¼‰ä¸æ³¨æ„åŠ›æ··åˆæ¶æ„ï¼ˆattention-hybrid architecturesï¼‰ï¼Œå‘ç°æ‰«æç­–ç•¥æå‡æœ‰é™ï¼Œè€Œæ··åˆæ¨¡å‹æ•ˆæœæ˜¾è‘—ã€‚åŸºäºæ­¤å‘ç°ï¼Œç ”ç©¶è¿›ä¸€æ­¥è¯„ä¼°äº†ä¸åŒæ³¨æ„åŠ›ç±»å‹ï¼Œè¯æ˜åˆ†å—é«˜é˜¶æ»‘åŠ¨çª—å£æ³¨æ„åŠ›ï¼ˆtiled high-order sliding window attention, SWAï¼‰åœ¨ç†è®ºå’Œå®è·µä¸­å‡æå…·æ•ˆç‡ã€‚ENA æ¶æ„é€šè¿‡çº¿æ€§é€’å½’ï¼ˆlinear recurrenceï¼‰å‹ç¼©å…¨å±€ä¿¡æ¯ï¼Œå¹¶ç»“åˆ SWA å¼ºåŒ–å±€éƒ¨å»ºæ¨¡èƒ½åŠ›ï¼ŒäºŒè€…å…±åŒæ„æˆäº†ä¸€ä¸ªç®€æ´çš„æ¡†æ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒENA ä¸ºè¶…é•¿é«˜é˜¶æ•°æ®å»ºæ¨¡æä¾›äº†ä¸€ç§æ—¢é«˜æ•ˆåˆå®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "WIP",
      "pdf_url": "https://arxiv.org/pdf/2508.11921v1",
      "published_date": "2025-08-16 05:55:51 UTC",
      "updated_date": "2025-08-16 05:55:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:16:56.868879+00:00"
    },
    {
      "arxiv_id": "2508.14098v1",
      "title": "No More Marching: Learning Humanoid Locomotion for Short-Range SE(2) Targets",
      "title_zh": "å‘Šåˆ«åŸåœ°è¸æ­¥ï¼šé¢å‘çŸ­è·ç¦» SE(2) ç›®æ ‡çš„ç±»äººæœºå™¨äººè¿åŠ¨å­¦ä¹ ",
      "authors": [
        "Pranay Dugar",
        "Mohitvishnu S. Gadde",
        "Jonah Siekmann",
        "Yesh Godse",
        "Aayam Shrestha",
        "Alan Fern"
      ],
      "abstract": "Humanoids operating in real-world workspaces must frequently execute task-driven, short-range movements to SE(2) target poses. To be practical, these transitions must be fast, robust, and energy efficient. While learning-based locomotion has made significant progress, most existing methods optimize for velocity-tracking rather than direct pose reaching, resulting in inefficient, marching-style behavior when applied to short-range tasks. In this work, we develop a reinforcement learning approach that directly optimizes humanoid locomotion for SE(2) targets. Central to this approach is a new constellation-based reward function that encourages natural and efficient target-oriented movement. To evaluate performance, we introduce a benchmarking framework that measures energy consumption, time-to-target, and footstep count on a distribution of SE(2) goals. Our results show that the proposed approach consistently outperforms standard methods and enables successful transfer from simulation to hardware, highlighting the importance of targeted reward design for practical short-range humanoid locomotion.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äººå½¢æœºå™¨äººåœ¨ç°å®ç©ºé—´ä¸­é¢‘ç¹æ‰§è¡ŒçŸ­è·ç¦» SE(2) ç›®æ ‡ä½å§¿ç§»åŠ¨çš„éœ€æ±‚ï¼ŒæŒ‡å‡ºå½“å‰ä¸»æµçš„è¿åŠ¨æ§åˆ¶æ–¹æ³•å› ä¾§é‡é€Ÿåº¦è¿½è¸ªè€Œåœ¨çŸ­è·ç¦»ä»»åŠ¡ä¸­è¡¨ç°å‡ºä½æ•ˆçš„è¸æ­¥è¡Œä¸ºã€‚ä¸ºæ­¤ï¼Œä½œè€…å¼€å‘äº†ä¸€ç§å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) æ–¹æ¡ˆï¼Œé€šè¿‡ç›´æ¥ä¼˜åŒ– SE(2) ç›®æ ‡åˆ°è¾¾èƒ½åŠ›æ¥æå‡è¿åŠ¨æ•ˆç‡ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ˜¯å¼•å…¥äº†ä¸€ç§æ–°å‹çš„åŸºäºæ˜Ÿåº§å›¾çš„å¥–åŠ±å‡½æ•° (constellation-based reward function)ï¼Œæœ‰æ•ˆè¯±å¯¼æœºå™¨äººäº§ç”Ÿè‡ªç„¶ä¸”èŠ‚èƒ½çš„ç›®æ ‡å¯¼å‘åŠ¨ä½œã€‚ç ”ç©¶åŒæ—¶æå‡ºäº†ä¸€ä¸ªç»¼åˆè¯„ä¼°æ¡†æ¶ï¼Œä»èƒ½é‡æ¶ˆè€—ã€è€—æ—¶åŠæ­¥æ•°ç­‰ç»´åº¦è¡¡é‡æ€§èƒ½ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å„é¡¹æŒ‡æ ‡ä¸Šå‡ä¼˜äºä¼ ç»ŸåŸºå‡†ï¼Œå¹¶æˆåŠŸå®ç°äº†ä»ä»¿çœŸåˆ°ç¡¬ä»¶ (simulation to hardware) çš„éƒ¨ç½²ã€‚è¿™ä¸€æˆæœå¼ºè°ƒäº†é’ˆå¯¹æ€§å¥–åŠ±è®¾è®¡åœ¨è§£å†³äººå½¢æœºå™¨äººå®ç”¨åŒ–çŸ­è·ç¦»è¿åŠ¨éš¾é¢˜ä¸­çš„å…³é”®ä½œç”¨ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.14098v1",
      "published_date": "2025-08-16 05:48:52 UTC",
      "updated_date": "2025-08-16 05:48:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:17:38.183944+00:00"
    },
    {
      "arxiv_id": "2508.11915v1",
      "title": "CORE: Measuring Multi-Agent LLM Interaction Quality under Game-Theoretic Pressures",
      "title_zh": "COREï¼šåšå¼ˆè®ºå‹åŠ›ä¸‹çš„å¤šæ™ºèƒ½ä½“å¤§è¯­è¨€æ¨¡å‹äº¤äº’è´¨é‡è¯„ä¼°",
      "authors": [
        "Punya Syon Pandey",
        "Yongjin Yang",
        "Jiarui Liu",
        "Zhijing Jin"
      ],
      "abstract": "Game-theoretic interactions between agents with Large Language Models (LLMs) have revealed many emergent capabilities, yet the linguistic diversity of these interactions has not been sufficiently quantified. In this paper, we present the Conversational Robustness Evaluation Score: CORE, a metric to quantify the effectiveness of language use within multi-agent systems across different game-theoretic interactions. CORE integrates measures of cluster entropy, lexical repetition, and semantic similarity, providing a direct lens of dialog quality. We apply CORE to pairwise LLM dialogs across competitive, cooperative, and neutral settings, further grounding our analysis in Zipf's and Heaps' Laws to characterize word frequency distributions and vocabulary growth. Our findings show that cooperative settings exhibit both steeper Zipf distributions and higher Heap exponents, indicating more repetition alongside greater vocabulary expansion. In contrast, competitive interactions display lower Zipf and Heaps exponents, reflecting less repetition and more constrained vocabularies. These results provide new insights into how social incentives influence language adaptation, and highlight CORE as a robust diagnostic for measuring linguistic robustness in multi-agent LLM systems. Our code is available at https://github.com/psyonp/core.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†å¯¹è¯ç¨³å¥æ€§è¯„ä¼°åˆ†æ•°(Conversational Robustness Evaluation Score: CORE)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨é‡åŒ–å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨ä¸åŒåšå¼ˆäº’åŠ¨ä¸‹è¯­è¨€æœ‰æ•ˆæ€§çš„æ–°æŒ‡æ ‡ï¼Œå¡«è¡¥äº†LLMsäº’åŠ¨ä¸­è¯­è¨€å¤šæ ·æ€§é‡åŒ–ç ”ç©¶çš„ç©ºç™½ã€‚COREé€šè¿‡æ•´åˆèšç±»ç†µ(cluster entropy)ã€è¯æ±‡é‡å¤(lexical repetition)å’Œè¯­ä¹‰ç›¸ä¼¼æ€§(semantic similarity)ï¼Œä¸ºè¯„ä¼°å¯¹è¯è´¨é‡æä¾›äº†ç›´æ¥ä¸”å¤šç»´çš„è§†è§’ã€‚ç ”ç©¶è€…å°†è¯¥æŒ‡æ ‡åº”ç”¨äºç«äº‰ã€åˆä½œåŠä¸­æ€§ç¯å¢ƒä¸‹çš„æˆå¯¹å¯¹è¯ï¼Œå¹¶ç»“åˆé½æ™®å¤«å®šå¾‹(Zipf's Law)å’Œå¸Œæ™®æ–¯å®šå¾‹(Heaps' Law)åˆ†æè¯é¢‘åˆ†å¸ƒä¸è¯æ±‡å¢é•¿ã€‚å®éªŒå‘ç°ï¼Œåˆä½œç¯å¢ƒè¡¨ç°å‡ºæ›´é«˜çš„è¯æ±‡é‡å¤ä¸æ›´å¹¿æ³›çš„è¯æ±‡æ‰©å±•ï¼Œè€Œç«äº‰äº’åŠ¨åˆ™è¡¨ç°å‡ºæ›´å—é™çš„è¯æ±‡é‡ã€‚è¿™äº›ç»“æœæ­ç¤ºäº†ç¤¾ä¼šæ¿€åŠ±å¯¹å¤§è¯­è¨€æ¨¡å‹è¯­è¨€é€‚åº”æ€§çš„å½±å“ï¼Œå¹¶è¯æ˜äº†COREä½œä¸ºè¡¡é‡å¤šæ™ºèƒ½ä½“ç³»ç»Ÿè¯­è¨€ç¨³å¥æ€§(linguistic robustness)è¯Šæ–­å·¥å…·çš„å¯é æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11915v1",
      "published_date": "2025-08-16 05:26:36 UTC",
      "updated_date": "2025-08-16 05:26:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:17:33.790908+00:00"
    },
    {
      "arxiv_id": "2508.11907v1",
      "title": "Deciphering the Interplay between Attack and Protection Complexity in Privacy-Preserving Federated Learning",
      "title_zh": "è§£æéšç§ä¿æŠ¤è”é‚¦å­¦ä¹ ä¸­æ”»å‡»ä¸ä¿æŠ¤å¤æ‚åº¦çš„äº¤äº’å…³ç³»",
      "authors": [
        "Xiaojin Zhang",
        "Mingcong Xu",
        "Yiming Li",
        "Wei Chen",
        "Qiang Yang"
      ],
      "abstract": "Federated learning (FL) offers a promising paradigm for collaborative model training while preserving data privacy. However, its susceptibility to gradient inversion attacks poses a significant challenge, necessitating robust privacy protection mechanisms. This paper introduces a novel theoretical framework to decipher the intricate interplay between attack and protection complexities in privacy-preserving FL. We formally define \"Attack Complexity\" as the minimum computational and data resources an adversary requires to reconstruct private data below a given error threshold, and \"Protection Complexity\" as the expected distortion introduced by privacy mechanisms. Leveraging Maximum Bayesian Privacy (MBP), we derive tight theoretical bounds for protection complexity, demonstrating its scaling with model dimensionality and privacy budget. Furthermore, we establish comprehensive bounds for attack complexity, revealing its dependence on privacy leakage, gradient distortion, model dimension, and the chosen privacy level. Our findings quantitatively illuminate the fundamental trade-offs between privacy guarantees, system utility, and the effort required for both attacking and defending. This framework provides critical insights for designing more secure and efficient federated learning systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹éšç§ä¿æŠ¤è”é‚¦å­¦ä¹  (Federated Learning, FL) æ˜“å—æ¢¯åº¦åå‘æ”»å‡» (Gradient Inversion Attacks) çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ä¸ªæ—¨åœ¨é‡åŒ–æ”»å‡»ä¸ä¿æŠ¤å¤æ‚åº¦ä¹‹é—´å¤æ‚ç›¸äº’ä½œç”¨çš„ç†è®ºæ¡†æ¶ã€‚ç ”ç©¶æ­£å¼å®šä¹‰äº†â€œæ”»å‡»å¤æ‚åº¦â€ (Attack Complexity) ä¸ºæ”»å‡»è€…é‡æ„éšç§æ•°æ®æ‰€éœ€çš„æœ€å°è®¡ç®—ä¸æ•°æ®èµ„æºï¼Œä»¥åŠâ€œä¿æŠ¤å¤æ‚åº¦â€ (Protection Complexity) ä¸ºéšç§æœºåˆ¶å¼•å…¥çš„é¢„æœŸå¤±çœŸã€‚åˆ©ç”¨æœ€å¤§è´å¶æ–¯éšç§ (Maximum Bayesian Privacy, MBP) ç†è®ºï¼Œç ”ç©¶äººå‘˜æ¨å¯¼å‡ºäº†ä¿æŠ¤å¤æ‚åº¦çš„ç´§è‡´ç†è®ºç•Œé™ï¼Œå¹¶è¯æ˜å…¶éšæ¨¡å‹ç»´åº¦ (Model Dimensionality) å’Œéšç§é¢„ç®— (Privacy Budget) çš„å˜åŒ–è€Œç¼©æ”¾ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶ç¡®ç«‹äº†æ”»å‡»å¤æ‚åº¦çš„ç»¼åˆç•Œé™ï¼Œæ­ç¤ºäº†å…¶å¯¹éšç§æ³„éœ²ã€æ¢¯åº¦å¤±çœŸã€æ¨¡å‹ç»´åº¦åŠæ‰€é€‰éšç§æ°´å¹³çš„ä¾èµ–å…³ç³»ã€‚è¿™äº›å‘ç°å®šé‡åœ°é˜æ˜äº†éšç§ä¿è¯ã€ç³»ç»Ÿæ•ˆç”¨ä¸æ”»é˜²æˆæœ¬ä¹‹é—´çš„åŸºæœ¬æƒè¡¡ï¼Œä¸ºè®¾è®¡æ›´å®‰å…¨é«˜æ•ˆçš„è”é‚¦å­¦ä¹ ç³»ç»Ÿæä¾›äº†å…³é”®çš„ç†è®ºæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11907v1",
      "published_date": "2025-08-16 04:39:16 UTC",
      "updated_date": "2025-08-16 04:39:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:17:07.274673+00:00"
    },
    {
      "arxiv_id": "2508.13201v2",
      "title": "Benchmarking LLM-based Agents for Single-cell Omics Analysis",
      "title_zh": "é¢å‘å•ç»†èƒç»„å­¦åˆ†æçš„åŸºäº LLM æ™ºèƒ½ä½“åŸºå‡†æµ‹è¯•",
      "authors": [
        "Yang Liu",
        "Lu Zhou",
        "Xiawei Du",
        "Ruikun He",
        "Rongbo Shen",
        "Yixue Li"
      ],
      "abstract": "The surge in multimodal single-cell omics data exposes limitations in traditional, manually defined analysis workflows. AI agents offer a paradigm shift, enabling adaptive planning, executable code generation, traceable decisions, and real-time knowledge fusion. However, the lack of a comprehensive benchmark critically hinders progress. We introduce a novel benchmarking evaluation system to rigorously assess agent capabilities in single-cell omics analysis. This system comprises: a unified platform compatible with diverse agent frameworks and LLMs; multidimensional metrics assessing cognitive program synthesis, collaboration, execution efficiency, bioinformatics knowledge integration, and task completion quality; and 50 diverse real-world single-cell omics analysis tasks spanning multi-omics, species, and sequencing technologies. Our evaluation reveals that Grok-3-beta achieves state-of-the-art performance among tested agent frameworks. Multi-agent frameworks significantly enhance collaboration and execution efficiency over single-agent approaches through specialized role division. Attribution analyses of agent capabilities identify that high-quality code generation is crucial for task success, and self-reflection has the most significant overall impact, followed by retrieval-augmented generation (RAG) and planning. This work highlights persistent challenges in code generation, long-context handling, and context-aware knowledge retrieval, providing a critical empirical foundation and best practices for developing robust AI agents in computational biology.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å•ç»†èƒç»„å­¦åˆ†æ(Single-cell Omics Analysis)ä¸­ä¼ ç»Ÿæ‰‹åŠ¨å·¥ä½œæµçš„å±€é™æ€§ï¼Œæ¢è®¨äº†AIæ™ºèƒ½ä½“(AI Agents)åœ¨è‡ªé€‚åº”è§„åˆ’å’Œå®æ—¶çŸ¥è¯†èåˆæ–¹é¢çš„æ½œåŠ›ã€‚ä¸ºè§£å†³è¯¥é¢†åŸŸç¼ºä¹å…¨é¢åŸºå‡†æµ‹è¯•çš„é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªå…¨æ–°çš„åŸºå‡†è¯„ä¼°ç³»ç»Ÿï¼ŒåŒ…æ‹¬ç»Ÿä¸€çš„è¯„ä¼°å¹³å°ã€å¤šç»´åº¦çš„åº¦é‡æŒ‡æ ‡ä»¥åŠ50é¡¹æ¶µç›–å¤šç»„å­¦ã€å¤šç‰©ç§å’Œæµ‹åºæŠ€æœ¯çš„çœŸå®ä¸–ç•Œä»»åŠ¡ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼ŒGrok-3-betaåœ¨å—æµ‹æ¡†æ¶ä¸­è¾¾åˆ°äº†é¡¶å°–(State-of-the-art)æ°´å¹³ï¼Œä¸”å¤šæ™ºèƒ½ä½“(Multi-agent)æ¡†æ¶é€šè¿‡è§’è‰²åˆ†å·¥æ˜¾è‘—æå‡äº†åä½œä¸æ‰§è¡Œæ•ˆç‡ã€‚å½’å› åˆ†æç¡®å®šäº†è‡ªæˆ‘åæ€(Self-reflection)æ˜¯æå‡è¡¨ç°æœ€å…³é”®çš„å› ç´ ï¼Œå…¶æ¬¡æ˜¯æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)å’Œè§„åˆ’(Planning)ï¼Œè€Œé«˜è´¨é‡çš„ä»£ç ç”Ÿæˆå¯¹ä»»åŠ¡æˆåŠŸåŒæ ·è‡³å…³é‡è¦ã€‚è¯¥é¡¹å·¥ä½œä¸ä»…ä¸ºè®¡ç®—ç”Ÿç‰©å­¦ä¸­æ„å»ºç¨³å¥çš„AIæ™ºèƒ½ä½“æä¾›äº†å®è¯åŸºç¡€å’Œæœ€ä½³å®è·µï¼ŒåŒæ—¶ä¹ŸæŒ‡å‡ºäº†åœ¨ä»£ç ç”Ÿæˆã€é•¿ä¸Šä¸‹æ–‡å¤„ç†åŠä¸Šä¸‹æ–‡æ„ŸçŸ¥çŸ¥è¯†æ£€ç´¢æ–¹é¢å­˜åœ¨çš„æŒç»­æŒ‘æˆ˜ã€‚",
      "categories": [
        "q-bio.GN",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "q-bio.GN",
      "comment": "6 main figures; 13 supplementary figures",
      "pdf_url": "https://arxiv.org/pdf/2508.13201v2",
      "published_date": "2025-08-16 04:26:18 UTC",
      "updated_date": "2026-01-09 02:16:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:17:12.654701+00:00"
    },
    {
      "arxiv_id": "2508.11894v1",
      "title": "QuarkMed Medical Foundation Model Technical Report",
      "title_zh": "QuarkMedåŒ»å­¦åŸºåº§æ¨¡å‹æŠ€æœ¯æŠ¥å‘Š",
      "authors": [
        "Ao Li",
        "Bin Yan",
        "Bingfeng Cai",
        "Chenxi Li",
        "Cunzhong Zhao",
        "Fugen Yao",
        "Gaoqiang Liu",
        "Guanjun Jiang",
        "Jian Xu",
        "Liang Dong",
        "Liansheng Sun",
        "Rongshen Zhang",
        "Xiaolei Gui",
        "Xin Liu",
        "Xin Shang",
        "Yao Wu",
        "Yu Cao",
        "Zhenxin Ma",
        "Zhuang Jia"
      ],
      "abstract": "Recent advancements in large language models have significantly accelerated their adoption in healthcare applications, including AI-powered medical consultations, diagnostic report assistance, and medical search tools. However, medical tasks often demand highly specialized knowledge, professional accuracy, and customization capabilities, necessitating a robust and reliable foundation model. QuarkMed addresses these needs by leveraging curated medical data processing, medical-content Retrieval-Augmented Generation (RAG), and a large-scale, verifiable reinforcement learning pipeline to develop a high-performance medical foundation model. The model achieved 70% accuracy on the Chinese Medical Licensing Examination, demonstrating strong generalization across diverse medical benchmarks. QuarkMed offers a powerful yet versatile personal medical AI solution, already serving over millions of users at ai.quark.cn.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº†QuarkMedï¼Œè¿™æ˜¯ä¸€æ¬¾ä¸“ä¸ºæ»¡è¶³åŒ»ç–—é¢†åŸŸå¯¹é«˜åº¦ä¸“ä¸šåŒ–çŸ¥è¯†ã€å‡†ç¡®æ€§å’Œå®šåˆ¶åŒ–èƒ½åŠ›éœ€æ±‚è€Œè®¾è®¡çš„é«˜æ€§èƒ½åŒ»ç–—åŸºç¡€æ¨¡å‹(Medical Foundation Model)ã€‚è¯¥æ¨¡å‹é€šè¿‡æ•´åˆç²¾ç»†åŒ–çš„åŒ»ç–—æ•°æ®å¤„ç†(Curated Medical Data Processing)ã€é’ˆå¯¹åŒ»ç–—å†…å®¹çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ(Retrieval-Augmented Generation, RAG)ä»¥åŠå¤§è§„æ¨¡å¯éªŒè¯å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)æµæ°´çº¿ï¼Œæ˜¾è‘—æå‡äº†åŒ»ç–—AIåœ¨å’¨è¯¢ä¸è¯Šæ–­è¾…åŠ©ä¸­çš„å¯é æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒQuarkMedåœ¨ä¸­å›½æ‰§ä¸šåŒ»å¸ˆèµ„æ ¼è€ƒè¯•(Chinese Medical Licensing Examination)ä¸­è¾¾åˆ°äº†70%çš„å‡†ç¡®ç‡ï¼Œå¹¶åœ¨å¤šé¡¹åŒ»ç–—åŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºå“è¶Šçš„æ³›åŒ–èƒ½åŠ›(Generalization)ã€‚ä½œä¸ºä¸€ç§åŠŸèƒ½å¼ºå¤§ä¸”é€šç”¨çš„ä¸ªäººåŒ»ç–—AIè§£å†³æ–¹æ¡ˆï¼ŒQuarkMedç›®å‰å·²åœ¨ai.quark.cnæœåŠ¡äºæ•°ç™¾ä¸‡ç”¨æˆ·ï¼Œæœ‰æ•ˆéªŒè¯äº†å…¶åœ¨å®é™…åº”ç”¨åœºæ™¯ä¸­çš„é«˜æ€§èƒ½ä¸å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "20 pages",
      "pdf_url": "https://arxiv.org/pdf/2508.11894v1",
      "published_date": "2025-08-16 03:47:52 UTC",
      "updated_date": "2025-08-16 03:47:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:17:22.792954+00:00"
    },
    {
      "arxiv_id": "2508.11890v2",
      "title": "Integrating Symbolic RL Planning into a BDI-based Autonomous UAV Framework: System Integration and SIL Validation",
      "title_zh": "å°†ç¬¦å·å¼ºåŒ–å­¦ä¹ è§„åˆ’é›†æˆè‡³åŸºäº BDI çš„è‡ªä¸»æ— äººæœºæ¡†æ¶ï¼šç³»ç»Ÿé›†æˆä¸ SIL éªŒè¯",
      "authors": [
        "Sangwoo Jeon",
        "Juchul Shin",
        "YeonJe Cho",
        "Gyeong-Tae Kim",
        "Seongwoo Kim"
      ],
      "abstract": "Modern autonomous drone missions increasingly require software frameworks capable of seamlessly integrating structured symbolic planning with adaptive reinforcement learning (RL). Although traditional rule-based architectures offer robust structured reasoning for drone autonomy, their capabilities fall short in dynamically complex operational environments that require adaptive symbolic planning. Symbolic RL (SRL), using the Planning Domain Definition Language (PDDL), explicitly integrates domain-specific knowledge and operational constraints, significantly improving the reliability and safety of unmanned aerial vehicle (UAV) decision making. In this study, we propose the AMAD-SRL framework, an extended and refined version of the Autonomous Mission Agents for Drones (AMAD) cognitive multi-agent architecture, enhanced with symbolic reinforcement learning for dynamic mission planning and execution. We validated our framework in a Software-in-the-Loop (SIL) environment structured identically to an intended Hardware-In-the-Loop Simulation (HILS) platform, ensuring seamless transition to real hardware. Experimental results demonstrate stable integration and interoperability of modules, successful transitions between BDI-driven and symbolic RL-driven planning phases, and consistent mission performance. Specifically, we evaluate a target acquisition scenario in which the UAV plans a surveillance path followed by a dynamic reentry path to secure the target while avoiding threat zones. In this SIL evaluation, mission efficiency improved by approximately 75% over a coverage-based baseline, measured by travel distance reduction. This study establishes a robust foundation for handling complex UAV missions and discusses directions for further enhancement and validation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ— äººæœº(UAV)åœ¨åŠ¨æ€å¤æ‚ç¯å¢ƒä¸‹é€‚åº”æ€§ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†AMAD-SRLæ¡†æ¶ï¼Œè¿™æ˜¯å¯¹AMADè®¤çŸ¥å¤šæ™ºèƒ½ä½“æ¶æ„çš„æ‰©å±•ä¸æ”¹è¿›ã€‚è¯¥æ¡†æ¶é€šè¿‡é›†æˆç¬¦å·å¼ºåŒ–å­¦ä¹ (Symbolic RL)å’Œè§„åˆ’é¢†åŸŸå®šä¹‰è¯­è¨€(PDDL)ï¼Œå°†ä¼ ç»Ÿçš„BDI-basedç»“æ„åŒ–æ¨ç†ä¸è‡ªé€‚åº”å­¦ä¹ ç›¸ç»“åˆï¼Œæ˜¾è‘—æå‡äº†å†³ç­–çš„å¯é æ€§ä¸å®‰å…¨æ€§ã€‚ç ”ç©¶åœ¨ä¸ç¡¬ä»¶åœ¨ç¯(HILS)ç»“æ„ä¸€è‡´çš„è½¯ä»¶åœ¨ç¯(SIL)ç¯å¢ƒä¸­éªŒè¯äº†ç³»ç»Ÿçš„é›†æˆæ€§ä¸æ¨¡å—äº’æ“ä½œæ€§ï¼Œç¡®ä¿äº†ç®—æ³•å‘å®é™…ç¡¬ä»¶è¿ç§»çš„æ— ç¼è¡”æ¥ã€‚å®éªŒé‡ç‚¹è¯„ä¼°äº†ç›®æ ‡è·å–åœºæ™¯ä¸­çš„è·¯å¾„è§„åˆ’ä¸å¨èƒè§„é¿èƒ½åŠ›ï¼Œç»“æœè¡¨æ˜ç³»ç»Ÿèƒ½å¤ŸæˆåŠŸåœ¨BDIé©±åŠ¨ä¸ç¬¦å·RLé©±åŠ¨çš„è§„åˆ’é˜¶æ®µé—´è¿›è¡Œå¹³æ»‘åˆ‡æ¢ã€‚ç›¸æ¯”äºä¼ ç»Ÿçš„è¦†ç›–åŸºçº¿æ–¹æ³•ï¼ŒAMAD-SRLåœ¨ä»»åŠ¡æ•ˆç‡ä¸Šæå‡äº†çº¦75%ï¼Œå¤§å¹…å‡å°‘äº†é£è¡Œè·ç¦»ã€‚è¯¥ç ”ç©¶ä¸ºå¤„ç†å¤æ‚çš„æ— äººæœºè‡ªä¸»ä»»åŠ¡æä¾›äº†ç¨³å¥çš„åŸºç¡€æ¶æ„ï¼Œå¹¶ä¸ºåç»­çš„ç³»ç»ŸåŠŸèƒ½å¢å¼ºå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "This submission has been withdrawn by the authors due to institutional and contractual requirements related to security and export-control review",
      "pdf_url": "https://arxiv.org/pdf/2508.11890v2",
      "published_date": "2025-08-16 03:27:26 UTC",
      "updated_date": "2026-01-11 09:53:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:17:20.391859+00:00"
    },
    {
      "arxiv_id": "2508.11886v1",
      "title": "EVTP-IVS: Effective Visual Token Pruning For Unifying Instruction Visual Segmentation In Multi-Modal Large Language Models",
      "title_zh": "EVTP-IVSï¼šé¢å‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç»Ÿä¸€æŒ‡ä»¤è§†è§‰åˆ†å‰²çš„é«˜æ•ˆè§†è§‰ Token å‰ªæ",
      "authors": [
        "Wenhui Zhu",
        "Xiwen Chen",
        "Zhipeng Wang",
        "Shao Tang",
        "Sayan Ghosh",
        "Xuanzhao Dong",
        "Rajat Koner",
        "Yalin Wang"
      ],
      "abstract": "Instructed Visual Segmentation (IVS) tasks require segmenting objects in images or videos based on natural language instructions. While recent multimodal large language models (MLLMs) have achieved strong performance on IVS, their inference cost remains a major bottleneck, particularly in video. We empirically analyze visual token sampling in MLLMs and observe a strong correlation between subset token coverage and segmentation performance. This motivates our design of a simple and effective token pruning method that selects a compact yet spatially representative subset of tokens to accelerate inference. In this paper, we introduce a novel visual token pruning method for IVS, called EVTP-IV, which builds upon the k-center by integrating spatial information to ensure better coverage. We further provide an information-theoretic analysis to support our design. Experiments on standard IVS benchmarks show that our method achieves up to 5X speed-up on video tasks and 3.5X on image tasks, while maintaining comparable accuracy using only 20% of the tokens. Our method also consistently outperforms state-of-the-art pruning baselines under varying pruning ratios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æŒ‡ä»¤è§†è§‰åˆ†å‰²ï¼ˆIVSï¼‰ä»»åŠ¡ä¸­é¢ä¸´çš„æ¨ç†æˆæœ¬ç“¶é¢ˆï¼Œç‰¹åˆ«æ˜¯è§†é¢‘å¤„ç†ä¸­çš„é«˜æ˜‚ä»£ä»·ï¼Œæå‡ºäº†EVTP-IVSæ¡†æ¶ã€‚ä½œè€…é€šè¿‡å®è¯åˆ†ææ­ç¤ºäº†è§†è§‰ç‰¹å¾æ ‡è®°ï¼ˆvisual tokenï¼‰çš„å­é›†è¦†ç›–åº¦ä¸åˆ†å‰²æ€§èƒ½ä¹‹é—´çš„å¼ºç›¸å…³æ€§ï¼Œå¹¶æ®æ­¤è®¾è®¡äº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„å‰ªææ–¹æ¡ˆã€‚EVTP-IVSé€šè¿‡åœ¨k-centerç®—æ³•ä¸­èå…¥ç©ºé—´ä¿¡æ¯ï¼Œå®ç°äº†å¯¹è§†è§‰æ ‡è®°çš„æœ‰æ•ˆå‰ªæï¼ˆtoken pruningï¼‰ï¼Œç¡®ä¿äº†ç­›é€‰å‡ºçš„å­é›†å…·æœ‰ç´§å‡‘ä¸”ç©ºé—´ä»£è¡¨æ€§çš„ç‰¹å¾ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜é€šè¿‡ä¿¡æ¯è®ºåˆ†æä¸ºè¯¥è®¾è®¡æä¾›äº†ç†è®ºæ”¯æ’‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ä»…ä½¿ç”¨20%æ ‡è®°çš„æƒ…å†µä¸‹ï¼Œè¯¥æ–¹æ³•åœ¨è§†é¢‘å’Œå›¾åƒä»»åŠ¡ä¸Šåˆ†åˆ«å®ç°äº†æœ€é«˜5å€å’Œ3.5å€çš„æ¨ç†åŠ é€Ÿï¼Œä¸”å‡†ç¡®ç‡ä¸å…¨é‡æ¨¡å‹ç›¸å½“ã€‚ç›¸æ¯”ç°æœ‰çš„å…ˆè¿›å‰ªæåŸºå‡†ï¼ŒEVTP-IVSåœ¨ä¸åŒçš„å‰ªææ¯”ä¾‹ä¸‹å‡è¡¨ç°å‡ºæ›´ä¼˜è¶Šä¸”ç¨³å®šçš„æ€§èƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11886v1",
      "published_date": "2025-08-16 03:16:33 UTC",
      "updated_date": "2025-08-16 03:16:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:17:26.095481+00:00"
    },
    {
      "arxiv_id": "2508.16634v5",
      "title": "Few-shot Class-incremental Fault Diagnosis by Preserving Class-Agnostic Knowledge with Dual-Granularity Representations",
      "title_zh": "åŸºäºåŒç²’åº¦è¡¨ç¤ºä¿ç•™ç±»æ— å…³çŸ¥è¯†çš„å°æ ·æœ¬ç±»å¢é‡æ•…éšœè¯Šæ–­",
      "authors": [
        "Zhendong Yang",
        "Jie Wang",
        "Liansong Zong",
        "Xiaorong Liu",
        "Quan Qian",
        "Shiqian Chen"
      ],
      "abstract": "Few-Shot Class-Incremental Fault Diagnosis (FSC-FD), which aims to continuously learn from new fault classes with only a few samples without forgetting old ones, is critical for real-world industrial systems. However, this challenging task severely amplifies the issues of catastrophic forgetting of old knowledge and overfitting on scarce new data. To address these challenges, this paper proposes a novel framework built upon Dual-Granularity Representations, termed the Dual-Granularity Guidance Network (DGGN). Our DGGN explicitly decouples feature learning into two parallel streams: 1) a fine-grained representation stream, which utilizes a novel Multi-Order Interaction Aggregation module to capture discriminative, class-specific features from the limited new samples. 2) a coarse-grained representation stream, designed to model and preserve general, class-agnostic knowledge shared across all fault types. These two representations are dynamically fused by a multi-semantic cross-attention mechanism, where the stable coarse-grained knowledge guides the learning of fine-grained features, preventing overfitting and alleviating feature conflicts. To further mitigate catastrophic forgetting, we design a Boundary-Aware Exemplar Prioritization strategy. Moreover, a decoupled Balanced Random Forest classifier is employed to counter the decision boundary bias caused by data imbalance. Extensive experiments on the TEP benchmark and a real-world MFF dataset demonstrate that our proposed DGGN achieves superior diagnostic performance and stability compared to state-of-the-art FSC-FD approaches. Our code is publicly available at https://github.com/MentaY/DGGN",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å·¥ä¸šç³»ç»Ÿä¸­çš„å°‘æ ·æœ¬ç±»å¢é‡æ•…éšœè¯Šæ–­ (Few-Shot Class-Incremental Fault Diagnosis, FSC-FD) æŒ‘æˆ˜ï¼Œæå‡ºäº†åŒç²’åº¦å¼•å¯¼ç½‘ç»œ (Dual-Granularity Guidance Network, DGGN) æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡åŒç²’åº¦è¡¨ç¤ºå°†ç‰¹å¾å­¦ä¹ è§£è€¦ä¸ºç»†ç²’åº¦å’Œç²—ç²’åº¦ä¸¤ä¸ªå¹¶è¡Œæµï¼Œåˆ†åˆ«åˆ©ç”¨å¤šé˜¶äº¤äº’èšåˆ (Multi-Order Interaction Aggregation) æ•æ‰ç±»åˆ«ç‰¹å®šç‰¹å¾ä»¥åŠå»ºæ¨¡é€šç”¨çš„ç±»åˆ«æ— å…³çŸ¥è¯†ã€‚é€šè¿‡å¤šè¯­ä¹‰äº¤å‰æ³¨æ„åŠ› (multi-semantic cross-attention) æœºåˆ¶ï¼Œåˆ©ç”¨ç¨³å®šçš„ç²—ç²’åº¦çŸ¥è¯†å¼•å¯¼ç»†ç²’åº¦ç‰¹å¾å­¦ä¹ ï¼Œæœ‰æ•ˆç¼“è§£äº†ç¾éš¾æ€§é—å¿˜å’Œè¿‡æ‹Ÿåˆé—®é¢˜ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è®¾è®¡äº†è¾¹ç•Œæ„ŸçŸ¥æ ·æœ¬ä¼˜å…ˆçº§ (Boundary-Aware Exemplar Prioritization) ç­–ç•¥å’Œè§£è€¦å‡è¡¡éšæœºæ£®æ— (Balanced Random Forest) åˆ†ç±»å™¨ï¼Œä»¥è¿›ä¸€æ­¥ä¼˜åŒ–å†³ç­–è¾¹ç•Œå¹¶åº”å¯¹æ•°æ®ä¸å¹³è¡¡ã€‚åœ¨ TEP åŸºå‡†å’ŒçœŸå®ä¸–ç•Œ MFF æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒDGGN åœ¨è¯Šæ–­æ€§èƒ½å’Œç¨³å®šæ€§æ–¹é¢å‡ä¼˜äºç°æœ‰çš„å…ˆè¿› FSC-FD æ–¹æ³•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "This manuscript is currently under review at the Engineering Applications of Artificial Intelligence",
      "pdf_url": "https://arxiv.org/pdf/2508.16634v5",
      "published_date": "2025-08-16 03:14:07 UTC",
      "updated_date": "2025-12-15 15:02:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:17:58.886168+00:00"
    },
    {
      "arxiv_id": "2508.11874v1",
      "title": "Discovering Expert-Level Nash Equilibrium Algorithms with Large Language Models",
      "title_zh": "åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹å‘ç°ä¸“å®¶çº§çº³ä»€å‡è¡¡ç®—æ³•",
      "authors": [
        "Hanyu Li",
        "Dongchen Li",
        "Xiaotie Deng"
      ],
      "abstract": "Algorithm design and analysis is a cornerstone of computer science, but it confronts a major challenge. Proving an algorithm's performance guarantee across all inputs has traditionally required extensive and often error-prone human effort. While AI has shown great success in finding solutions to specific problem instances, automating the discovery of general algorithms with such provable guarantees has remained a significant barrier. This challenge stems from the difficulty of integrating the creative process of algorithm design with the rigorous process of formal analysis. To address this gap, we propose LegoNE, a framework that tightly fuses these two processes for the fundamental and notoriously difficult problem of computing approximate Nash equilibria. LegoNE automatically translates any algorithm written by a simple Python-like language into a constrained optimization problem. Solving this problem derives and proves the algorithm's approximation bound. Using LegoNE, a state-of-the-art large language model rediscovered the state-of-the-art algorithm for two-player games within hours, a feat that had taken human researchers 15 years to achieve. For three-player games, the model discovered a novel algorithm surpassing all existing human-designed ones. This work demonstrates a new human-machine collaborative paradigm for theoretical science: humans reason at a higher-abstract level, using symbols to compress the search space, and AI explores within it, achieving what neither could alone.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LegoNE æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿç®—æ³•è®¾è®¡ä¸æ€§èƒ½ä¿è¯ (Performance Guarantee) è¯æ˜ä¸­é«˜åº¦ä¾èµ–äººå·¥ä¸”æ˜“å‡ºé”™çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹è®¡ç®—è¿‘ä¼¼çº³ä»€å‡è¡¡ (Nash Equilibrium) è¿™ä¸€å¤æ‚é—®é¢˜ã€‚LegoNE å°†ç®—æ³•é€»è¾‘è‡ªåŠ¨è½¬åŒ–ä¸ºçº¦æŸä¼˜åŒ–é—®é¢˜ (Constrained Optimization Problem)ï¼Œä»è€Œèƒ½å¤Ÿæ¨å¯¼å¹¶è¯æ˜ç®—æ³•çš„è¿‘ä¼¼ç•Œé™ (Approximation Bound)ã€‚åˆ©ç”¨è¯¥æ¡†æ¶ï¼Œå¤§è¯­è¨€æ¨¡å‹ (Large Language Models) åœ¨æ•°å°æ—¶å†…é‡æ–°å‘ç°äº†åŒäººåšå¼ˆçš„æœ€å…ˆè¿›ç®—æ³•ï¼Œè€Œè¿™ä¸€æˆæœäººç±»ç ”ç©¶è€…æ›¾è€—æ—¶ 15 å¹´æ‰è¾¾æˆã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹è¿˜åœ¨ä¸‰äººåšå¼ˆé¢†åŸŸå‘ç°äº†ä¸€ç§è¶…è¶Šç°æœ‰æ‰€æœ‰äººå·¥è®¾è®¡æ–¹æ¡ˆçš„æ–°å‹ç®—æ³•ã€‚è¿™é¡¹å·¥ä½œå»ºç«‹äº†ä¸€ç§ç†è®ºç§‘å­¦ç ”ç©¶çš„äººæœºåä½œæ–°èŒƒå¼ï¼Œå³äººç±»è´Ÿè´£é«˜å±‚æŠ½è±¡æ¨ç†ï¼Œè€Œäººå·¥æ™ºèƒ½åœ¨ç¬¦å·åŒ–çš„æœç´¢ç©ºé—´ä¸­è¿›è¡Œæ·±åº¦æ¢ç´¢ä¸éªŒè¯ã€‚",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.DS",
        "cs.LO",
        "cs.PL"
      ],
      "primary_category": "cs.GT",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11874v1",
      "published_date": "2025-08-16 02:18:43 UTC",
      "updated_date": "2025-08-16 02:18:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:18:00.184962+00:00"
    },
    {
      "arxiv_id": "2508.11873v1",
      "title": "SimInterview: Transforming Business Education through Large Language Model-Based Simulated Multilingual Interview Training System",
      "title_zh": "SimInterviewï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ¨¡æ‹Ÿå¤šè¯­ç§é¢è¯•å®è®­ç³»ç»Ÿèµ‹èƒ½å•†ä¸šæ•™è‚²å˜é©",
      "authors": [
        "Truong Thanh Hung Nguyen",
        "Tran Diem Quynh Nguyen",
        "Hoang Loc Cao",
        "Thi Cam Thanh Tran",
        "Thi Cam Mai Truong",
        "Hung Cao"
      ],
      "abstract": "Business interview preparation demands both solid theoretical grounding and refined soft skills, yet conventional classroom methods rarely deliver the individualized, culturally aware practice employers currently expect. This paper introduces SimInterview, a large language model (LLM)-based simulated multilingual interview training system designed for business professionals entering the AI-transformed labor market. Our system leverages an LLM agent and synthetic AI technologies to create realistic virtual recruiters capable of conducting personalized, real-time conversational interviews. The framework dynamically adapts interview scenarios using retrieval-augmented generation (RAG) to match individual resumes with specific job requirements across multiple languages. Built on LLMs (OpenAI o3, Llama 4 Maverick, Gemma 3), integrated with Whisper speech recognition, GPT-SoVITS voice synthesis, Ditto diffusion-based talking head generation model, and ChromaDB vector databases, our system significantly improves interview readiness across English and Japanese markets. Experiments with university-level candidates show that the system consistently aligns its assessments with job requirements, faithfully preserves resume content, and earns high satisfaction ratings, with the lightweight Gemma 3 model producing the most engaging conversations. Qualitative findings revealed that the standardized Japanese resume format improved document retrieval while diverse English resumes introduced additional variability, and they highlighted how cultural norms shape follow-up questioning strategies. Finally, we also outlined a contestable AI design that can explain, detect bias, and preserve human-in-the-loop to meet emerging regulatory expectations.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SimInterviewï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„æ¨¡æ‹Ÿå¤šè¯­è¨€é¢è¯•åŸ¹è®­ç³»ç»Ÿï¼Œæ—¨åœ¨æå‡å•†åŠ¡ä¸“ä¸šäººå£«åœ¨AIè½¬å‹åŠ³åŠ¨åŠ›å¸‚åœºä¸­çš„é¢è¯•å‡†å¤‡æ°´å¹³ã€‚ç³»ç»Ÿé›†æˆäº†LLMæ™ºèƒ½ä½“ä¸å¤šç§åˆæˆAIæŠ€æœ¯ï¼Œåˆ©ç”¨Whisperè¿›è¡Œè¯­éŸ³è¯†åˆ«ã€GPT-SoVITSè¿›è¡Œè¯­éŸ³åˆæˆï¼Œå¹¶ç»“åˆDittoæ‰©æ•£æ¨¡å‹ç”Ÿæˆæ•°å­—äººå½¢è±¡ï¼Œä»¥æ„å»ºçœŸå®ä¸”å¯äº’åŠ¨çš„è™šæ‹Ÿé¢è¯•å®˜ã€‚è¯¥æ¡†æ¶é‡‡ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)æŠ€æœ¯ä¸ChromaDBå‘é‡æ•°æ®åº“ï¼Œå®ç°äº†ç®€å†å†…å®¹ä¸ç‰¹å®šèŒä½è¦æ±‚çš„ç²¾å‡†åŒ¹é…åŠè·¨è¯­è¨€åœºæ™¯é€‚é…ã€‚åœ¨é’ˆå¯¹å¤§å­¦ç”Ÿçš„å®éªŒä¸­ï¼ŒSimInterviewæ˜¾è‘—å¢å¼ºäº†è‹±è¯­å’Œæ—¥è¯­ç¯å¢ƒä¸‹çš„é¢è¯•åº”å¯¹èƒ½åŠ›ï¼Œå…¶ä¸­è½»é‡çº§çš„Gemma 3æ¨¡å‹è¢«è¯å®èƒ½æä¾›æœ€å…·å‚ä¸æ„Ÿçš„å¯¹è¯ä½“éªŒã€‚ç ”ç©¶è¿›ä¸€æ­¥æ¢è®¨äº†æ–‡åŒ–è§„èŒƒå¯¹è¿½é—®ç­–ç•¥çš„å½±å“ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ¶µç›–åå·®æ£€æµ‹ä¸äººå·¥åœ¨ç¯(Human-in-the-loop)çš„å¯äº‰è¾©äººå·¥æ™ºèƒ½(Contestable AI)è®¾è®¡ï¼Œä»¥ç¬¦åˆæ–°å…´çš„ç›‘ç®¡è¦æ±‚ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC",
        "cs.MM"
      ],
      "primary_category": "cs.CY",
      "comment": "Published as a conference paper at ICEFM 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.11873v1",
      "published_date": "2025-08-16 02:18:36 UTC",
      "updated_date": "2025-08-16 02:18:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:18:06.693547+00:00"
    },
    {
      "arxiv_id": "2508.11872v1",
      "title": "Singing Syllabi with Virtual Avatars: Enhancing Student Engagement Through AI-Generated Music and Digital Embodiment",
      "title_zh": "è™šæ‹ŸåŒ–èº«å”±å“æ•™å­¦å¤§çº²ï¼šé€šè¿‡ AI ç”ŸæˆéŸ³ä¹ä¸æ•°å­—å…·èº«æå‡å­¦ç”Ÿå‚ä¸åº¦",
      "authors": [
        "Xinxing Wu"
      ],
      "abstract": "In practical teaching, we observe that few students thoroughly read or fully comprehend the information provided in traditional, text-based course syllabi. As a result, essential details, such as course policies and learning outcomes, are frequently overlooked. To address this challenge, in this paper, we propose a novel approach leveraging AI-generated singing and virtual avatars to present syllabi in a format that is more visually appealing, engaging, and memorable. Especially, we leveraged the open-source tool, HeyGem, to transform textual syllabi into audiovisual presentations, in which digital avatars perform the syllabus content as songs. The proposed approach aims to stimulate students' curiosity, foster emotional connection, and enhance retention of critical course information. Student feedback indicated that AI-sung syllabi significantly improved awareness and recall of key course information.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿæ–‡æœ¬å½¢å¼çš„è¯¾ç¨‹å¤§çº²ï¼ˆsyllabiï¼‰ç»å¸¸è¢«å­¦ç”Ÿå¿½è§†ã€å¯¼è‡´å…³é”®æ”¿ç­–å’Œå­¦ä¹ æˆæœæ— æ³•æœ‰æ•ˆä¼ é€’çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆäººå·¥æ™ºèƒ½ç”Ÿæˆçš„æ­Œå”±ï¼ˆAI-generated singingï¼‰ä¸è™šæ‹ŸåŒ–èº«ï¼ˆvirtual avatarsï¼‰çš„æ–°å‹å‘ˆç°æ–¹æ³•ã€‚ç ”ç©¶è€…åˆ©ç”¨å¼€æºå·¥å…· HeyGem å°†æ–‡æœ¬å¤§çº²è½¬åŒ–ä¸ºè§†å¬æ¼”ç¤ºï¼Œé€šè¿‡æ•°å­—å½¢è±¡å°†å¤§çº²å†…å®¹ä»¥æ­Œæ›²å½¢å¼æ¼”å”±ï¼Œæ—¨åœ¨æ¿€å‘å­¦ç”Ÿçš„curiosityå¹¶å¢å¼ºæƒ…æ„Ÿè¿æ¥ã€‚è¿™ç§Digital Embodimentï¼ˆæ•°å­—å…·èº«ï¼‰çš„å½¢å¼ä¸ä»…æå‡äº†è¯¾ç¨‹å¤§çº²çš„è§†è§‰å¸å¼•åŠ›ï¼Œè¿˜æ˜¾è‘—å¢å¼ºäº†å­¦ç”Ÿå¯¹å…³é”®ä¿¡æ¯çš„è®°å¿†ä¿ç•™ï¼ˆretentionï¼‰ã€‚å­¦ç”Ÿåé¦ˆç»“æœæ˜¾ç¤ºï¼ŒAI-sung syllabi æ˜¾è‘—æ”¹å–„äº†å­¦ç”Ÿå¯¹è¯¾ç¨‹å…³é”®ä¿¡æ¯çš„çŸ¥æ™“ç¨‹åº¦å’Œå¬å›ç‡ï¼ˆrecallï¼‰ï¼Œè¯æ˜äº†é€šè¿‡AIé©±åŠ¨çš„è§†å¬åŒ–å‘ˆç°èƒ½æœ‰æ•ˆæå‡å­¦ç”Ÿçš„å‚ä¸åº¦ä¸ä¿¡æ¯å¤„ç†èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CY",
      "comment": "17 pages, 4 figures, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2508.11872v1",
      "published_date": "2025-08-16 02:12:39 UTC",
      "updated_date": "2025-08-16 02:12:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:18:09.498811+00:00"
    },
    {
      "arxiv_id": "2508.11870v2",
      "title": "AdaRing: Towards Ultra-Light Vision-Language Adaptation via Cross-Layer Tensor Ring Decomposition",
      "title_zh": "AdaRingï¼šåŸºäºè·¨å±‚å¼ é‡ç¯åˆ†è§£çš„è¶…è½»é‡çº§è§†è§‰-è¯­è¨€é€‚é…",
      "authors": [
        "Ying Huang",
        "Yuanbin Man",
        "Wenqi Jia",
        "Zhengzhong Tu",
        "Junzhou Huang",
        "Miao Yin"
      ],
      "abstract": "Adapter-based fine-tuning has gained remarkable attention in adapting large pre-trained vision language models (VLMs) for a wide range of downstream tasks efficiently. In this paradigm, only the inserted adapters are fine-tuned, without the need for training the original VLM backbone. Existing works scale adapters by integrating them into every layer of VLMs to increase the capacity of adapters. However, these methods face two primary limitations: 1) limited compression rate due to ignoring cross-layer redundancy, and 2) limited representational capacity across homogeneous adapters. In this paper, we propose a novel vision-language fine-tuning framework based on cross-layer tensor ring decomposition (TRD) with the integration and collaboration of diverse adapters, called AdaRing, achieving ultra-light parameter-efficient adaptation of VLMs on various tasks. To remove the high redundancy that exists among adapters across layers, we exploit the tensor-level low-rankness to formulate adapters as layer-shared tensor cores and layer-specific slices. Moreover, guided by generalization-aware fine-tuning, diverse rank-driven adapters cooperate to handle tasks that require different representations. Our experiments show that the proposed AdaRing achieves the state-of-the-art performance while reducing average training parameters by 90%.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AdaRingï¼Œä¸€ç§åŸºäºè·¨å±‚å¼ é‡ç¯åˆ†è§£ (Cross-Layer Tensor Ring Decomposition, TRD) çš„è¶…è½»é‡åŒ–è§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) å¾®è°ƒæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰é€‚é…å™¨ (Adapter) æ–¹æ³•å› å¿½ç•¥è·¨å±‚å†—ä½™è€Œå¯¼è‡´çš„å‚æ•°æ•ˆç‡ç“¶é¢ˆå’Œè¡¨å¾èƒ½åŠ›å—é™é—®é¢˜ã€‚ä¸ºäº†æ¶ˆé™¤å±‚é—´å†—ä½™ï¼ŒAdaRing åˆ©ç”¨å¼ é‡çº§ä½ç§©ç‰¹æ€§å°†é€‚é…å™¨é‡æ„ä¸ºå±‚å…±äº«çš„å¼ é‡æ ¸å¿ƒ (Layer-Shared Tensor Cores) ä¸å±‚ç‰¹å®šçš„åˆ‡ç‰‡ (Layer-Specific Slices)ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åœ¨æ³›åŒ–æ„ŸçŸ¥å¾®è°ƒçš„æŒ‡å¯¼ä¸‹ï¼Œé€šè¿‡ä¸åŒç§©é©±åŠ¨çš„é€‚é…å™¨åä½œæ¥å¤„ç†å¤šæ ·åŒ–ä»»åŠ¡æ‰€éœ€çš„è¡¨å¾ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAdaRing åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿› (SOTA) çš„æ€§èƒ½æ°´å¹³ï¼ŒåŒæ—¶å°†å¹³å‡è®­ç»ƒå‚æ•°é‡å¤§å¹…å‡å°‘äº† 90%ï¼Œå®ç°äº†æé«˜æ•ˆç‡çš„è§†è§‰è¯­è¨€é€‚é…ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11870v2",
      "published_date": "2025-08-16 01:56:27 UTC",
      "updated_date": "2025-08-19 21:43:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:18:16.497074+00:00"
    },
    {
      "arxiv_id": "2508.11868v1",
      "title": "Data Shift of Object Detection in Autonomous Driving",
      "title_zh": "è‡ªåŠ¨é©¾é©¶ç›®æ ‡æ£€æµ‹ä¸­çš„æ•°æ®åç§»",
      "authors": [
        "Lida Xu"
      ],
      "abstract": "With the widespread adoption of machine learning technologies in autonomous driving systems, their role in addressing complex environmental perception challenges has become increasingly crucial. However, existing machine learning models exhibit significant vulnerability, as their performance critically depends on the fundamental assumption that training and testing data satisfy the independent and identically distributed condition, which is difficult to guarantee in real-world applications. Dynamic variations in data distribution caused by seasonal changes, weather fluctuations lead to data shift problems in autonomous driving systems. This study investigates the data shift problem in autonomous driving object detection tasks, systematically analyzing its complexity and diverse manifestations. We conduct a comprehensive review of data shift detection methods and employ shift detection analysis techniques to perform dataset categorization and balancing. Building upon this foundation, we construct an object detection model. To validate our approach, we optimize the model by integrating CycleGAN-based data augmentation techniques with the YOLOv5 framework. Experimental results demonstrate that our method achieves superior performance compared to baseline models on the BDD100K dataset.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è‡ªåŠ¨é©¾é©¶ Object Detection ä»»åŠ¡ä¸­çš„ Data Shift é—®é¢˜ï¼Œç³»ç»Ÿåˆ†æäº†å› å­£èŠ‚å’Œå¤©æ°”å˜åŒ–å¯¼è‡´æ•°æ®åˆ†å¸ƒä¸å†æ»¡è¶³ç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆi.i.d.ï¼‰å‡è®¾çš„å¤æ‚ç°çŠ¶ã€‚ä½œè€…åœ¨å…¨é¢ç»¼è¿° Data Shift Detection æ–¹æ³•çš„åŸºç¡€ä¸Šï¼Œåˆ©ç”¨åç§»æ£€æµ‹æŠ€æœ¯å¯¹æ•°æ®é›†è¿›è¡Œäº†åˆ†ç±»ä¸å¹³è¡¡å¤„ç†ã€‚éšåï¼Œç ”ç©¶é€šè¿‡é›†æˆ CycleGAN æ•°æ®å¢å¼ºæŠ€æœ¯ä¸ YOLOv5 æ¡†æ¶æ„å»ºå¹¶ä¼˜åŒ–äº†ç›®æ ‡æ£€æµ‹æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ BDD100K æ•°æ®é›†ä¸Šå–å¾—äº†ä¼˜äºåŸºçº¿æ¨¡å‹çš„æ€§èƒ½ï¼Œæ˜¾è‘—æå‡äº†è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿåœ¨åŠ¨æ€ç¯å¢ƒæ„ŸçŸ¥ä¸­çš„å‡†ç¡®æ€§ä¸é²æ£’æ€§ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11868v1",
      "published_date": "2025-08-16 01:52:31 UTC",
      "updated_date": "2025-08-16 01:52:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:18:38.968356+00:00"
    },
    {
      "arxiv_id": "2508.11867v1",
      "title": "AI-Augmented CI/CD Pipelines: From Code Commit to Production with Autonomous Decisions",
      "title_zh": "AI å¢å¼ºå‹ CI/CD æµæ°´çº¿ï¼šå®ç°ä»ä»£ç æäº¤åˆ°ç”Ÿäº§ç¯å¢ƒçš„è‡ªä¸»å†³ç­–",
      "authors": [
        "Mohammad Baqar",
        "Saba Naqvi",
        "Rajat Khanda"
      ],
      "abstract": "Modern software delivery has accelerated from quarterly releases to multiple deployments per day. While CI/CD tooling has matured, human decision points interpreting flaky tests, choosing rollback strategies, tuning feature flags, and deciding when to promote a canary remain major sources of latency and operational toil. We propose AI-Augmented CI/CD Pipelines, where large language models (LLMs) and autonomous agents act as policy-bounded co-pilots and progressively as decision makers. We contribute: (1) a reference architecture for embedding agentic decision points into CI/CD, (2) a decision taxonomy and policy-as-code guardrail pattern, (3) a trust-tier framework for staged autonomy, (4) an evaluation methodology using DevOps Research and Assessment ( DORA) metrics and AI-specific indicators, and (5) a detailed industrial-style case study migrating a React 19 microservice to an AI-augmented pipeline. We discuss ethics, verification, auditability, and threats to validity, and chart a roadmap for verifiable autonomy in production delivery systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°ä»£è½¯ä»¶äº¤ä»˜ä¸­äººå·¥å†³ç­–ç‚¹ï¼ˆå¦‚å¤„ç† flaky testsã€é€‰æ‹©å›æ»šç­–ç•¥å’Œ canary æ¨å¹¿ï¼‰å¯¼è‡´çš„äº¤ä»˜å»¶è¿Ÿå’Œè¿ç»´è´Ÿæ‹…ï¼Œæå‡ºäº† AI-Augmented CI/CD Pipelines æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ Large Language Models (LLMs) å’Œè‡ªä¸»æ™ºèƒ½ä½“ï¼ˆautonomous agentsï¼‰ä½œä¸ºå—ç­–ç•¥çº¦æŸçš„å‰¯é©¾é©¶ï¼Œå¹¶é€æ­¥å°†å…¶è½¬åŒ–ä¸ºè‡ªåŠ¨å†³ç­–è€…ã€‚ç ”ç©¶çš„ä¸»è¦è´¡çŒ®åŒ…æ‹¬ä¸€ç§å°†æ™ºèƒ½å†³ç­–ç‚¹åµŒå…¥ CI/CD çš„å‚è€ƒæ¶æ„ã€ä¸€å¥—å†³ç­–åˆ†ç±»æ³•ä¸ policy-as-code æŠ¤æ æ¨¡å¼ï¼Œä»¥åŠç”¨äºåˆ†é˜¶æ®µå®ç°è‡ªæ²»çš„ trust-tier æ¡†æ¶ã€‚æ­¤å¤–ï¼Œä½œè€…æå‡ºäº†ä¸€å¥—ç»“åˆ DORA æŒ‡æ ‡å’Œ AI ç‰¹å®šæŒ‡æ ‡çš„è¯„ä¼°æ–¹æ³•ï¼Œå¹¶é€šè¿‡å°† React 19 å¾®æœåŠ¡è¿ç§»è‡³è¯¥ AI å¢å¼ºæµæ°´çº¿çš„å·¥ä¸šçº§æ¡ˆä¾‹ç ”ç©¶éªŒè¯äº†å…¶å¯è¡Œæ€§ã€‚æœ€åï¼Œæ–‡ç« æ·±å…¥æ¢è®¨äº†è‡ªæ²»ç³»ç»Ÿçš„ä¼¦ç†ã€å¯éªŒè¯æ€§ä¸å¯å®¡è®¡æ€§ï¼Œå¹¶ä¸ºç”Ÿäº§äº¤ä»˜ç³»ç»Ÿå®ç°å¯éªŒè¯çš„è‡ªæ²»è§„åˆ’äº†è·¯çº¿å›¾ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "13 Pages",
      "pdf_url": "https://arxiv.org/pdf/2508.11867v1",
      "published_date": "2025-08-16 01:51:59 UTC",
      "updated_date": "2025-08-16 01:51:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:18:28.689486+00:00"
    },
    {
      "arxiv_id": "2508.15813v1",
      "title": "SCOPE: A Generative Approach for LLM Prompt Compression",
      "title_zh": "SCOPEï¼šä¸€ç§ç”Ÿæˆå¼å¤§è¯­è¨€æ¨¡å‹æç¤ºè¯å‹ç¼©æ–¹æ³•",
      "authors": [
        "Tinghui Zhang",
        "Yifan Wang",
        "Daisy Zhe Wang"
      ],
      "abstract": "Prompt compression methods enhance the efficiency of Large Language Models (LLMs) and minimize the cost by reducing the length of input context. The goal of prompt compression is to shorten the LLM prompt while maintaining a high generation quality. However, existing solutions, mainly based on token removal, face challenges such as information loss and structural incoherence, like missing grammar elements in a sentence, or incomplete word phrases after token removal. Such challenges limit the final generation quality of LLM.\n  To overcome these limitations, we present a novel generative prompt compression method. Unlike the existing token removal methods, our method centers at a chunking-and-summarization mechanism. Specifically, our method splits prompt into semantically coherent chunks and rewrites the chunks to be more concise. The chunks are reconstructed into meaningful prompt finally. We design several optimization techniques for the mechanism, including optimized semantic chunking, outlier chunk handling, dynamic compression ratio, compression prioritization, and keyword maintaining. These techniques effectively improve the identifying and preserving of critical information and coherence among texts, as well as providing finer grind control of the compression ratio. We conduct extensive evaluation on question-answering and summarization tasks, with datasets covering multiple different domain. The evaluation shows our method achieves a significantly better compression quality, and higher stability than the state-of-the-art methods, especially under high compression ratio, which proves the effectiveness and practicality of our method.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SCOPEï¼Œä¸€ç§ç”¨äºå¤§è¯­è¨€æ¨¡å‹ (LLM) æç¤ºè¯å‹ç¼© (Prompt Compression) çš„æ–°å‹ç”Ÿæˆå¼æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹å¤„ç†æ•ˆç‡å¹¶é™ä½è®¡ç®—æˆæœ¬ã€‚é’ˆå¯¹ç°æœ‰åŸºäº Token Removal çš„å‹ç¼©æ–¹æ¡ˆæ˜“å¯¼è‡´ä¿¡æ¯ä¸¢å¤±åŠç»“æ„ä¸è¿è´¯ç­‰å±€é™æ€§ï¼ŒSCOPE æ ¸å¿ƒé‡‡ç”¨äº†å—å¤„ç†ä¸æ‘˜è¦ (Chunking-and-Summarization) æœºåˆ¶ï¼Œé€šè¿‡å°†æç¤ºè¯åˆ’åˆ†ä¸ºè¯­ä¹‰è¿è´¯çš„å— (Chunks) å¹¶è¿›è¡Œç²¾ç®€é‡å†™æ¥é‡å»ºæœ‰æ„ä¹‰çš„è¾“å…¥ã€‚ç ”ç©¶å›¢é˜Ÿè¿›ä¸€æ­¥è®¾è®¡äº†ä¼˜åŒ–è¯­ä¹‰åˆ†å— (Optimized Semantic Chunking)ã€åŠ¨æ€å‹ç¼©æ¯”ä»¥åŠå…³é”®è¯ç»´æŠ¤ (Keyword Maintaining) ç­‰æŠ€æœ¯ï¼Œä»¥å¢å¼ºå¯¹å…³é”®ä¿¡æ¯çš„è¯†åˆ«ä¸ä¿ç•™ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œåœ¨é—®ç­”å’Œæ‘˜è¦ç­‰å¤šé¢†åŸŸä»»åŠ¡ä¸­ï¼ŒSCOPE çš„å‹ç¼©è´¨é‡å’Œç¨³å®šæ€§å‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿› (SOTA) æ–¹æ³•ã€‚ç‰¹åˆ«æ˜¯åœ¨é«˜å‹ç¼©æ¯”åœºæ™¯ä¸‹ï¼Œè¯¥æ–¹æ³•è¯æ˜äº†å…¶åœ¨ç»´æŒç”Ÿæˆè´¨é‡æ–¹é¢çš„å“è¶Šæ•ˆèƒ½ä¸å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15813v1",
      "published_date": "2025-08-16 01:41:53 UTC",
      "updated_date": "2025-08-16 01:41:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:18:25.184860+00:00"
    },
    {
      "arxiv_id": "2508.11860v1",
      "title": "LARC: Towards Human-level Constrained Retrosynthesis Planning through an Agentic Framework",
      "title_zh": "LARCï¼šåŸºäºæ™ºèƒ½ä½“æ¡†æ¶è¿ˆå‘äººç±»æ°´å¹³çš„æœ‰çº¦æŸé€†åˆæˆè§„åˆ’",
      "authors": [
        "Frazier N. Baker",
        "Daniel Adu-Ampratwum",
        "Reza Averly",
        "Botao Yu",
        "Huan Sun",
        "Xia Ning"
      ],
      "abstract": "Large language model (LLM) agent evaluators leverage specialized tools to ground the rational decision-making of LLMs, making them well-suited to aid in scientific discoveries, such as constrained retrosynthesis planning. Constrained retrosynthesis planning is an essential, yet challenging, process within chemistry for identifying synthetic routes from commercially available starting materials to desired target molecules, subject to practical constraints. Here, we present LARC, the first LLM-based Agentic framework for Retrosynthesis planning under Constraints. LARC incorporates agentic constraint evaluation, through an Agent-as-a-Judge, directly into the retrosynthesis planning process, using agentic feedback grounded in tool-based reasoning to guide and constrain route generation. We rigorously evaluate LARC on a carefully curated set of 48 constrained retrosynthesis planning tasks across 3 constraint types. LARC achieves a 72.9% success rate on these tasks, vastly outperforming LLM baselines and approaching human expert-level success in substantially less time. The LARC framework is extensible, and serves as a first step towards an effective agentic tool or a co-scientist to human experts for constrained retrosynthesis.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LARCï¼Œè¿™æ˜¯é¦–ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹(Large Language Model, LLM)çš„çº¦æŸé€†åˆæˆè§„åˆ’æ™ºèƒ½ä½“æ¡†æ¶(Agentic framework for Retrosynthesis planning under Constraints)ï¼Œæ—¨åœ¨è¯†åˆ«å—å®é™…çº¦æŸå½±å“çš„åŒ–å­¦åˆæˆè·¯å¾„ã€‚LARC åˆ›æ–°æ€§åœ°å°†åŸºäºâ€œæ™ºèƒ½ä½“å³è¯„å§”â€(Agent-as-a-Judge)çš„çº¦æŸè¯„ä¼°ç›´æ¥æ•´åˆåˆ°é€†åˆæˆè§„åˆ’è¿‡ç¨‹ä¸­ï¼Œåˆ©ç”¨åŸºäºå·¥å…·æ¨ç†çš„åé¦ˆæ¥å¼•å¯¼è·¯å¾„ç”Ÿæˆã€‚åœ¨é’ˆå¯¹ 3 ç§çº¦æŸç±»å‹çš„ 48 é¡¹è§„åˆ’ä»»åŠ¡è¯„ä¼°ä¸­ï¼ŒLARC å–å¾—äº† 72.9% çš„æˆåŠŸç‡ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„ LLM åŸºå‡†æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLARC çš„è¡¨ç°å·²æ¥è¿‘äººç±»ä¸“å®¶æ°´å¹³ï¼Œä¸”åœ¨å¤„ç†æ—¶é—´ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚ä½œä¸ºä¸€ä¸ªå¯æ‰©å±•çš„æ¡†æ¶ï¼ŒLARC ä¸ºå¼€å‘èƒ½å¤Ÿè¾…åŠ©äººç±»ä¸“å®¶çš„ååŒç§‘å­¦å®¶(co-scientist)å·¥å…·è¿ˆå‡ºäº†å…³é”®ä¸€æ­¥ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "24 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.11860v1",
      "published_date": "2025-08-16 01:05:26 UTC",
      "updated_date": "2025-08-16 01:05:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:18:28.496655+00:00"
    },
    {
      "arxiv_id": "2508.11857v2",
      "title": "SupraTok: Cross-Boundary Tokenization for Enhanced Language Model Performance",
      "title_zh": "SupraTokï¼šæ—¨åœ¨æå‡è¯­è¨€æ¨¡å‹æ€§èƒ½çš„è·¨è¾¹ç•Œåˆ†è¯æŠ€æœ¯",
      "authors": [
        "Andrei-Valentin TÄƒnase",
        "Elena Pelican"
      ],
      "abstract": "Tokenization remains a fundamental yet underexplored bottleneck in natural language processing, with strategies largely static despite remarkable progress in model architectures. We present SupraTok, a novel tokenization architecture that reimagines subword segmentation through three innovations: cross-boundary pattern learning that discovers multi-word semantic units, entropy-driven data curation that optimizes training corpus quality, and multi-phase curriculum learning for stable convergence. Our approach extends Byte-Pair Encoding by learning \"superword\" tokens, coherent multi-word expressions that preserve semantic unity while maximizing compression efficiency. SupraTok achieves 31% improvement in English tokenization efficiency (5.91 versus 4.51 characters per token) compared to OpenAI's o200k tokenizer and 30% improvement over Google's Gemma 3 tokenizer (256k vocabulary), while maintaining competitive performance across 38 languages. When integrated with a GPT-2 scale model (124M parameters) trained on 10 billion tokens from the FineWeb-Edu dataset, SupraTok yields 8.4% improvement on HellaSWAG and 9.5% on MMLU benchmarks without architectural modifications. While these results are promising at this scale, further validation at larger model scales is needed. These findings suggest that efficient tokenization can complement architectural innovations as a path to improved language model performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SupraTokï¼Œä¸€ç§æ—¨åœ¨è§£å†³è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ Tokenization ç“¶é¢ˆçš„æ–°å‹è·¨è¾¹ç•Œåˆ†è¯æ¶æ„ã€‚è¯¥æ¶æ„é€šè¿‡å¼•å…¥è·¨è¾¹ç•Œæ¨¡å¼å­¦ä¹  (Cross-boundary Pattern Learning)ã€ç†µé©±åŠ¨æ•°æ®ç­–åˆ’ (Entropy-driven Data Curation) å’Œå¤šé˜¶æ®µè¯¾ç¨‹å­¦ä¹  (Multi-phase Curriculum Learning)ï¼Œå®ç°äº†å¯¹å¤šè¯è¯­ä¹‰å•å…ƒçš„é«˜æ•ˆæ•æ‰ã€‚SupraTok æ‰©å±•äº†ä¼ ç»Ÿçš„å­—èŠ‚å¯¹ç¼–ç  (Byte-Pair Encoding)ï¼Œé€šè¿‡å­¦ä¹ â€œè¶…çº§è¯â€ (Superword) Tokenï¼Œåœ¨ä¿è¯è¯­ä¹‰ç»Ÿä¸€æ€§çš„åŒæ—¶æå¤§æå‡äº†å‹ç¼©æ•ˆç‡ã€‚å®éªŒè¡¨æ˜ï¼ŒSupraTok åœ¨è‹±æ–‡åˆ†è¯æ•ˆç‡ä¸Šæ¯” OpenAI çš„ o200k æå‡äº† 31%ï¼Œå¹¶åœ¨ 38 ç§è¯­è¨€ä¸­ä¿æŒäº†ç«äº‰åŠ›ã€‚åœ¨ GPT-2 è§„æ¨¡çš„æ¨¡å‹æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨ä¸æ”¹å˜æ¨¡å‹æ¶æ„çš„å‰æä¸‹ï¼Œä½¿ HellaSWAG å’Œ MMLU åŸºå‡†æµ‹è¯•çš„æ€§èƒ½åˆ†åˆ«æå‡äº† 8.4% å’Œ 9.5%ã€‚ç ”ç©¶ç»“æœè¯æ˜ï¼Œé«˜æ•ˆçš„åˆ†è¯ç­–ç•¥å¯ä»¥ä½œä¸ºæ¶æ„åˆ›æ–°çš„è¡¥å……ï¼Œæˆä¸ºæå‡è¯­è¨€æ¨¡å‹æ€§èƒ½çš„é‡è¦è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11857v2",
      "published_date": "2025-08-16 00:54:20 UTC",
      "updated_date": "2025-08-25 13:30:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:18:34.451626+00:00"
    },
    {
      "arxiv_id": "2508.11850v1",
      "title": "EvoCut: Strengthening Integer Programs via Evolution-Guided Language Models",
      "title_zh": "EvoCutï¼šåˆ©ç”¨è¿›åŒ–å¼•å¯¼çš„è¯­è¨€æ¨¡å‹å¢å¼ºæ•´æ•°è§„åˆ’",
      "authors": [
        "Milad Yazdani",
        "Mahdi Mostajabdaveh",
        "Samin Aref",
        "Zirui Zhou"
      ],
      "abstract": "Integer programming lies at the heart of crucial combinatorial optimization tasks but remains challenging due to its NP-hard nature. An effective approach for practically solving integer programs is the manual design of acceleration cuts, i.e. inequalities that improve solver performance. However, this creative process demands deep expertise and is yet to be automated. Our proposed framework, EvoCut, automates the generation of acceleration cuts by combining large language models (LLMs) with an evolutionary search. EvoCut (i) initializes a diverse population of candidate cuts via an LLM-based initializer agent; (ii) for each cut empirically evaluates both preservation of the optimal solution and its ability to cut off fractional solutions across a verification set; and (iii) iteratively refines the population through evolutionary crossover and mutation agents. We quantify each cut's utility by its relative reduction in the solver's optimality gap. Our comparisons against standard integer programming practice show that EvoCut reduces optimality gap by 17-57% within a fixed time. It obtains the same solutions up to 4 times as fast, and obtains higher-quality solutions within the same time limit. Requiring no human expert input, EvoCut reliably generates, improves, and empirically verifies cuts that generalize to unseen instances. The code is available at https://github.com/milad1378yz/EvoCut.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† EvoCut æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆ Large Language Models (LLMs) ä¸è¿›åŒ–æœç´¢ï¼Œå®ç°äº† Integer programming åŠ é€Ÿå‰²å¹³é¢ (acceleration cuts) çš„è‡ªåŠ¨åŒ–ç”Ÿæˆã€‚EvoCut é¦–å…ˆåˆ©ç”¨åŸºäº LLM çš„åˆå§‹åŒ–æ™ºèƒ½ä½“ç”Ÿæˆå¤šæ ·åŒ–çš„å€™é€‰å‰²å¹³é¢ï¼Œéšååœ¨éªŒè¯é›†ä¸Šè¯„ä¼°å…¶å¯¹æœ€ä¼˜è§£çš„ä¿ç•™èƒ½åŠ›ä»¥åŠåˆ‡å‰²åˆ†æ•°è§£çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡è¿›åŒ–ç®—æ³•ä¸­çš„äº¤å‰å’Œå˜å¼‚æ™ºèƒ½ä½“å¯¹ç¾¤ä½“è¿›è¡Œè¿­ä»£ä¼˜åŒ–ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿä¸æ–­æå‡å‰²å¹³é¢çš„è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æ ‡å‡†å®è·µç›¸æ¯”ï¼ŒEvoCut åœ¨å›ºå®šæ—¶é—´å†…å°† optimality gap é™ä½äº† 17-57%ï¼Œä¸”æ±‚è§£é€Ÿåº¦æœ€é«˜æå‡äº† 4 å€ã€‚è¯¥æ–¹æ³•æ— éœ€äººå·¥ä¸“å®¶å¹²é¢„ï¼Œèƒ½å¤Ÿå¯é åœ°ç”Ÿæˆã€æ”¹è¿›å¹¶éªŒè¯å¯æ³›åŒ–è‡³æœªçŸ¥å®ä¾‹çš„åŠ é€Ÿå‰²å¹³é¢ï¼Œä¸ºè‡ªåŠ¨åŒ–ç»„åˆä¼˜åŒ–æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11850v1",
      "published_date": "2025-08-16 00:13:49 UTC",
      "updated_date": "2025-08-16 00:13:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:18:53.163296+00:00"
    },
    {
      "arxiv_id": "2508.11849v3",
      "title": "LocoMamba: Vision-Driven Locomotion via End-to-End Deep Reinforcement Learning with Mamba",
      "title_zh": "LocoMambaï¼šåŸºäº Mamba çš„ç«¯åˆ°ç«¯æ·±åº¦å¼ºåŒ–å­¦ä¹ è§†è§‰é©±åŠ¨è¿åŠ¨æ§åˆ¶",
      "authors": [
        "Yinuo Wang",
        "Gavin Tao"
      ],
      "abstract": "We introduce LocoMamba, a vision-driven cross-modal DRL framework built on selective state-space models, specifically leveraging Mamba, that achieves near-linear-time sequence modeling, effectively captures long-range dependencies, and enables efficient training with longer sequences. First, we embed proprioceptive states with a multilayer perceptron and patchify depth images with a lightweight convolutional neural network, producing compact tokens that improve state representation. Second, stacked Mamba layers fuse these tokens via near-linear-time selective scanning, reducing latency and memory footprint, remaining robust to token length and image resolution, and providing an inductive bias that mitigates overfitting. Third, we train the policy end-to-end with Proximal Policy Optimization under terrain and appearance randomization and an obstacle-density curriculum, using a compact state-centric reward that balances progress, smoothness, and safety. We evaluate our method in challenging simulated environments with static and moving obstacles as well as uneven terrain. Compared with state-of-the-art baselines, our method achieves higher returns and success rates with fewer collisions, exhibits stronger generalization to unseen terrains and obstacle densities, and improves training efficiency by converging in fewer updates under the same compute budget.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LocoMambaï¼Œä¸€ç§åŸºäºé€‰æ‹©æ€§çŠ¶æ€ç©ºé—´æ¨¡å‹(Mamba)çš„è§†è§‰é©±åŠ¨è·¨æ¨¡æ€æ·±åº¦å¼ºåŒ–å­¦ä¹ (DRL)æ¡†æ¶ï¼Œæ—¨åœ¨æå‡æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸‹çš„è¿åŠ¨æ€§èƒ½ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤šå±‚æ„ŸçŸ¥æœº(MLP)åµŒå…¥æœ¬ä½“æ„Ÿè§‰çŠ¶æ€ï¼Œå¹¶é€šè¿‡è½»é‡çº§å·ç§¯ç¥ç»ç½‘ç»œ(CNN)å¯¹æ·±åº¦å›¾åƒè¿›è¡Œå—åŒ–(patchify)å¤„ç†ï¼Œç”Ÿæˆç´§å‡‘çš„Tokenä»¥ä¼˜åŒ–çŠ¶æ€è¡¨ç¤ºã€‚å…¶æ ¸å¿ƒé‡‡ç”¨å †å çš„Mambaå±‚é€šè¿‡è¿‘çº¿æ€§çš„é€‰æ‹©æ€§æ‰«ææŠ€æœ¯èåˆå¤šæ¨¡æ€æ•°æ®ï¼Œåœ¨æœ‰æ•ˆæ•æ‰é•¿ç¨‹ä¾èµ–å…³ç³»çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—å»¶è¿Ÿä¸å†…å­˜å ç”¨ã€‚è®­ç»ƒè¿‡ç¨‹é‡‡ç”¨è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–(PPO)ç®—æ³•ï¼Œå¹¶ç»“åˆåœ°å½¢ä¸å¤–è§‚éšæœºåŒ–ã€éšœç¢ç‰©å¯†åº¦è¯¾ç¨‹å­¦ä¹ ä»¥åŠå…¼é¡¾è¿åŠ¨å¹³æ»‘åº¦ä¸å®‰å…¨çš„å¥–åŠ±å‡½æ•°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLocoMambaåœ¨åŒ…å«åŠ¨æ€éšœç¢ç‰©å’Œå´å²–åœ°å½¢çš„æ¨¡æ‹Ÿä»»åŠ¡ä¸­ï¼Œå…¶æˆåŠŸç‡å’Œå¥–åŠ±å€¼å‡ä¼˜äºç°æœ‰åŸºå‡†æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å±•ç°å‡ºæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸”åœ¨ç›¸åŒçš„è®¡ç®—é¢„ç®—ä¸‹å…·æœ‰æ›´é«˜çš„è®­ç»ƒæ•ˆç‡ï¼Œèƒ½å¤Ÿä»¥æ›´å°‘çš„æ›´æ–°æ¬¡æ•°å®ç°æ”¶æ•›ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "eess.IV",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "14 pages. This paper has been published in Advanced Engineering Informatics. Please cite the journal version: DOI: 10.1016/j.aei.2025.104230",
      "pdf_url": "https://arxiv.org/pdf/2508.11849v3",
      "published_date": "2025-08-16 00:13:24 UTC",
      "updated_date": "2025-12-14 22:20:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T12:19:59.294836+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 72,
  "processed_papers_count": 72,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-24T12:20:51.711595+00:00"
}
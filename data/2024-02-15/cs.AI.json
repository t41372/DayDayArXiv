{
  "date": "2024-02-15",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-02-15 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文主要聚焦于大型语言模型（LLMs）的优化、应用和安全性挑战，以及其在医疗、机器人、强化学习和科学发现等领域的扩展，强调了 LLMs 的微调、鲁棒性和跨模态整合；令人印象深刻的文章包括 BioMistral（针对医疗领域的开源 LLM）和 ChemReasoner（AI 辅助催化剂发现），以及 Yann LeCun 等知名学者的参与。\n\n下面，我将逐一简要概述部分关键论文，先优先讨论那些创新性强、潜在话题度高或涉及著名学者的文章（如 LLMs 应用和强化学习相关），并将相关主题归类讨论。对于其他较为常规或技术细节较多的论文，我会快速掠过，只列出标题和核心贡献。\n\n### LLMs 优化与应用\n- **BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains**（中文：BioMistral：医疗领域的开源预训练大型语言模型集合；英文：BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains）  \n  这篇论文介绍了 BioMistral，一种基于 Mistral 的开源 LLM，通过在 PubMed Central 上进一步预训练，显著提升了医疗问答任务的性能，并在多语言评估中表现出色；主要贡献是提供首个大规模多语言医疗 LLM 基准，并开源数据集和模型。\n\n- **Large Language Models for Forecasting and Anomaly Detection: A Systematic Literature Review**（中文：大型语言模型在预测和异常检测中的系统文献综述；英文：Large Language Models for Forecasting and Anomaly Detection: A Systematic Literature Review）  \n  这篇综述分析了 LLMs 在预测和异常检测中的潜力与挑战，包括数据依赖性和泛化问题；主要发现是提出整合多模态数据和提升模型可解释性的策略，作为未来研究的指导。\n\n- **Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment**（中文：基于奖励上下文的多目标基础模型对齐；英文：Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment）  \n  论文提出一种动态调整偏好的多目标对齐方法，用于 LLMs 的偏好优化；关键贡献是减少计算资源消耗，同时在 LLMs 和扩散模型上实现高效对齐，实验显示其在多样化任务中优于传统强化学习方法。\n\n- **Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation**（中文：基于自对弈的扩散模型微调；英文：Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation）  \n  这篇工作创新性地使用自对弈策略微调扩散模型，提高文本到图像生成的性能；主要发现是它在少量数据下超越监督微调和 RLHF 方法，在图像生成任务中表现出色。\n\n- **Language Models with Conformal Factuality Guarantees**（中文：具有保真性保证的语言模型；英文：Language Models with Conformal Factuality Guarantees）  \n  论文探索了 LLMs 的保真性，通过置信区间确保高概率正确性；贡献在于提出一种后退算法，减少幻觉问题，在问答任务中实现80-90%的正确率保证。\n\n其他 LLMs 相关论文如 **PAL: Proxy-Guided Black-Box Attack on Large Language Models**（中文：PAL：基于代理的黑盒攻击大型语言模型；英文：PAL: Proxy-Guided Black-Box Attack on Large Language Models），快速提及：它开发了一种优化攻击方法，成功率高达84%，暴露了 LLMs 的安全漏洞。\n\n### 强化学习与决策优化\n- **Exploration-Driven Policy Optimization in RLHF: Theoretical Insights on Efficient Data Utilization**（中文：基于探索的 RLHF 策略优化；英文：Exploration-Driven Policy Optimization in RLHF: Theoretical Insights on Efficient Data Utilization）  \n  这篇论文从理论角度分析 RLHF 中的策略优化，提供低查询复杂度的性能界；主要贡献是提出轨迹级椭圆势分析，改进神经网络函数逼近，提升数据利用效率。\n\n- **Reward Generalization in RLHF: A Topological Perspective**（中文：RLHF 中的奖励泛化：拓扑视角；英文：Reward Generalization in RLHF: A Topological Perspective）  \n  论文从拓扑角度研究 RLHF 的奖励泛化，提出树结构偏好信息建模；发现它能减少奖励不确定性，并在 NLP 任务中提升7.5%的准确率。\n\n其他强化学习论文如 **Federated Prompt-based Decision Transformer for Customized VR Services in Mobile Edge Computing System**（中文：基于联邦提示的决策Transformer，用于移动边缘计算中的定制VR服务；英文：Federated Prompt-based Decision Transformer for Customized VR Services in Mobile Edge Computing System），快速掠过：它优化了边缘计算中的资源分配，提升了 VR 服务体验。\n\n### 生成模型与多模态应用\n- **ChemReasoner: Heuristic Search over a Large Language Model's Knowledge Space using Quantum-Chemical Feedback**（中文：ChemReasoner：使用量子化学反馈的LLM知识空间启发式搜索；英文：ChemReasoner: Heuristic Search over a Large Language Model's Knowledge Space using Quantum-Chemical Feedback）  \n  这篇论文结合 LLM 和量子化学反馈，提出 AI 辅助催化剂发现框架；主要发现是它能自动探索高效催化剂，无需人工输入，在 ICML 2024 上被接受。\n\n- **HI-GAN: Hierarchical Inpainting GAN with Auxiliary Inputs for Combined RGB and Depth Inpainting**（中文：HI-GAN：具有辅助输入的层次化修复GAN，用于RGB和深度图像修复；英文：HI-GAN: Hierarchical Inpainting GAN with Auxiliary Inputs for Combined RGB and Depth Inpainting）  \n  论文设计了一个端到端层次化 GAN，用于 RGBD 图像修复；贡献在于融入边缘和分割标签作为辅助输入，提升修复质量。\n\n其他生成模型论文如 **LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing**（中文：LAVE：基于LLM的代理辅助和语言增强视频编辑；英文：LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing），快速提及：它使用 LLM 简化视频编辑，用户研究显示提升了创造性。\n\n### 其他领域快速概述\n对于剩余论文，我会快速掠过，只列出标题和核心点：\n- **Analyzing the Roles of Language and Vision in Learning from Limited Data**（中文：分析语言和视觉在有限数据学习中的作用；英文：Analyzing the Roles of Language and Vision in Learning from Limited Data）  \n  探讨语言模型在视觉任务中的作用，发现语言能提供先验知识和推理能力。\n- **On the Vulnerability of LLM/VLM-Controlled Robotics**（中文：LLM/VLM控制机器人系统的脆弱性；英文：On the Vulnerability of LLM/VLM-Controlled Robotics）  \n  揭示输入微扰对机器人系统的安全影响，实验显示成功率下降22.2%。\n- **Revisiting Feature Prediction for Learning Visual Representations from Video**（中文：重新审视视频中特征预测的学习视觉表示；英文：Revisiting Feature Prediction for Learning Visual Representations from Video）  \n  提出 V-JEPA 框架，仅用视频预训练实现高精度视觉表示。\n- **GeoEval: Benchmark for Evaluating LLMs and Multi-Modal Models on Geometry Problem-Solving**（中文：GeoEval：用于评估LLMs和多模态模型的几何问题求解基准；英文：GeoEval: Benchmark for Evaluating LLMs and Multi-Modal Models on Geometry Problem-Solving）  \n  构建几何问题基准，评估 LLMs 在数学推理中的性能。\n- **Radio-astronomical Image Reconstruction with Conditional Denoising Diffusion Model**（中文：使用条件去噪扩散模型的射电天文图像重建；英文：Radio-astronomical Image Reconstruction with Conditional Denoising Diffusion Model）  \n  应用扩散模型提升射电天文图像源定位精度。\n\n这些论文覆盖了从 AI 安全到科学应用的广泛领域，但总体上，LLMs 的创新应用（如医疗和化学）是最值得关注的趋势。今天的快报到此结束，欢迎读者关注这些前沿进展！",
  "papers": [
    {
      "arxiv_id": "2402.10373v3",
      "title": "BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains",
      "title_zh": "翻译失败",
      "authors": [
        "Yanis Labrak",
        "Adrien Bazoge",
        "Emmanuel Morin",
        "Pierre-Antoine Gourraud",
        "Mickael Rouvier",
        "Richard Dufour"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable versatility in\nrecent years, offering potential applications across specialized domains such\nas healthcare and medicine. Despite the availability of various open-source\nLLMs tailored for health contexts, adapting general-purpose LLMs to the medical\ndomain presents significant challenges. In this paper, we introduce BioMistral,\nan open-source LLM tailored for the biomedical domain, utilizing Mistral as its\nfoundation model and further pre-trained on PubMed Central. We conduct a\ncomprehensive evaluation of BioMistral on a benchmark comprising 10 established\nmedical question-answering (QA) tasks in English. We also explore lightweight\nmodels obtained through quantization and model merging approaches. Our results\ndemonstrate BioMistral's superior performance compared to existing open-source\nmedical models and its competitive edge against proprietary counterparts.\nFinally, to address the limited availability of data beyond English and to\nassess the multilingual generalization of medical LLMs, we automatically\ntranslated and evaluated this benchmark into 7 other languages. This marks the\nfirst large-scale multilingual evaluation of LLMs in the medical domain.\nDatasets, multilingual evaluation benchmarks, scripts, and all the models\nobtained during our experiments are freely released.",
      "tldr_zh": "本文介绍了 BioMistral，这是一个基于 Mistral 基础模型的开源 Large Language Models (LLMs)，通过在 PubMed Central 上进一步预训练，专门针对生物医学领域进行优化。研究对 BioMistral 在 10 个英语医疗问答 (QA) 任务上进行了全面评估，并探索了量化 (quantization) 和模型合并 (model merging) 方法来创建轻量级版本，结果显示其性能优于现有开源医疗模型，并与专有模型具有竞争力。为了评估多语言泛化，该研究首次将基准翻译并评估了 7 种其他语言，所有数据集、脚本和模型均免费发布。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at ACL 2024 - Proceedings of the 62st Annual Meeting of the\n  Association for Computational Linguistics (Volume 1: Long Papers)",
      "pdf_url": "http://arxiv.org/pdf/2402.10373v3",
      "published_date": "2024-02-15 23:39:04 UTC",
      "updated_date": "2024-07-17 09:34:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:29:11.094731"
    },
    {
      "arxiv_id": "2402.10350v1",
      "title": "Large Language Models for Forecasting and Anomaly Detection: A Systematic Literature Review",
      "title_zh": "大语言模型用于预测和异常检测：系统文献综述",
      "authors": [
        "Jing Su",
        "Chufeng Jiang",
        "Xin Jin",
        "Yuxin Qiao",
        "Tingsong Xiao",
        "Hongda Ma",
        "Rong Wei",
        "Zhi Jing",
        "Jiajun Xu",
        "Junhong Lin"
      ],
      "abstract": "This systematic literature review comprehensively examines the application of\nLarge Language Models (LLMs) in forecasting and anomaly detection, highlighting\nthe current state of research, inherent challenges, and prospective future\ndirections. LLMs have demonstrated significant potential in parsing and\nanalyzing extensive datasets to identify patterns, predict future events, and\ndetect anomalous behavior across various domains. However, this review\nidentifies several critical challenges that impede their broader adoption and\neffectiveness, including the reliance on vast historical datasets, issues with\ngeneralizability across different contexts, the phenomenon of model\nhallucinations, limitations within the models' knowledge boundaries, and the\nsubstantial computational resources required. Through detailed analysis, this\nreview discusses potential solutions and strategies to overcome these\nobstacles, such as integrating multimodal data, advancements in learning\nmethodologies, and emphasizing model explainability and computational\nefficiency. Moreover, this review outlines critical trends that are likely to\nshape the evolution of LLMs in these fields, including the push toward\nreal-time processing, the importance of sustainable modeling practices, and the\nvalue of interdisciplinary collaboration. Conclusively, this review underscores\nthe transformative impact LLMs could have on forecasting and anomaly detection\nwhile emphasizing the need for continuous innovation, ethical considerations,\nand practical solutions to realize their full potential.",
      "tldr_zh": "这篇系统文献综述探讨了Large Language Models (LLMs)在forecasting和anomaly detection中的应用，总结了当前研究状态、潜力以及面临的挑战，如对大量历史数据的依赖、模型泛化问题、hallucinations现象和计算资源需求。论文通过分析现有研究，提出了潜在解决方案，包括整合multimodal数据、改进学习方法以及提升模型explainability和计算效率。综述还指出了未来趋势，如实时处理、可持续建模和跨学科合作，并强调LLMs在这些领域的变革性影响，同时呼吁持续创新、伦理考虑和实用策略来实现其潜力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.10350v1",
      "published_date": "2024-02-15 22:43:02 UTC",
      "updated_date": "2024-02-15 22:43:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:29:22.925718"
    },
    {
      "arxiv_id": "2403.19669v2",
      "title": "Analyzing the Roles of Language and Vision in Learning from Limited Data",
      "title_zh": "分析语言和视觉在从有限数据中学习中的作用",
      "authors": [
        "Allison Chen",
        "Ilia Sucholutsky",
        "Olga Russakovsky",
        "Thomas L. Griffiths"
      ],
      "abstract": "Does language help make sense of the visual world? How important is it to\nactually see the world rather than having it described with words? These basic\nquestions about the nature of intelligence have been difficult to answer\nbecause we only had one example of an intelligent system -- humans -- and\nlimited access to cases that isolated language or vision. However, the\ndevelopment of sophisticated Vision-Language Models (VLMs) by artificial\nintelligence researchers offers us new opportunities to explore the\ncontributions that language and vision make to learning about the world. We\nablate components from the cognitive architecture of these models to identify\ntheir contributions to learning new tasks from limited data. We find that a\nlanguage model leveraging all components recovers a majority of a VLM's\nperformance, despite its lack of visual input, and that language seems to allow\nthis by providing access to prior knowledge and reasoning.",
      "tldr_zh": "该论文探讨了语言和视觉在从有限数据中学习中的作用，通过对 Vision-Language Models (VLMs) 的组件去除(ablation)来分析它们的贡献。研究发现，即使缺乏视觉输入，一个完整的语言模型也能恢复大部分 VLM 的性能，因为语言提供先验知识和推理能力，从而提升学习效率。这些结果为理解智能系统的认知架构提供了新见解，强调了语言在处理视觉世界时的关键作用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "8 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2403.19669v2",
      "published_date": "2024-02-15 22:19:41 UTC",
      "updated_date": "2024-05-10 17:33:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:29:32.669015"
    },
    {
      "arxiv_id": "2402.10342v2",
      "title": "Exploration-Driven Policy Optimization in RLHF: Theoretical Insights on Efficient Data Utilization",
      "title_zh": "翻译失败",
      "authors": [
        "Yihan Du",
        "Anna Winnicki",
        "Gal Dalal",
        "Shie Mannor",
        "R. Srikant"
      ],
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) has achieved impressive\nempirical successes while relying on a small amount of human feedback. However,\nthere is limited theoretical justification for this phenomenon. Additionally,\nmost recent studies focus on value-based algorithms despite the recent\nempirical successes of policy-based algorithms. In this work, we consider an\nRLHF algorithm based on policy optimization (PO-RLHF). The algorithm is based\non the popular Policy Cover-Policy Gradient (PC-PG) algorithm, which assumes\nknowledge of the reward function. In PO-RLHF, knowledge of the reward function\nis not assumed, and the algorithm uses trajectory-based comparison feedback to\ninfer the reward function. We provide performance bounds for PO-RLHF with low\nquery complexity, which provides insight into why a small amount of human\nfeedback may be sufficient to achieve good performance with RLHF. A key novelty\nis a trajectory-level elliptical potential analysis, which bounds the reward\nestimation error when comparison feedback (rather than numerical reward\nobservation) is given. We provide and analyze algorithms PG-RLHF and NN-PG-RLHF\nfor two settings: linear and neural function approximation, respectively.",
      "tldr_zh": "本研究探讨了基于人类反馈的强化学习（RLHF）中，探索驱动的政策优化（PO-RLHF）算法，旨在解释少量反馈数据如何实现高效利用，并提供理论洞见。不同于以往关注价值-based算法，该工作基于Policy Cover-Policy Gradient (PC-PG)框架，使用trajectory-based comparison feedback推断reward function，而非假设其已知。关键创新在于引入trajectory-level elliptical potential analysis，以绑定reward估计错误，并分析了线性函数逼近（PG-RLHF）和神经网络函数逼近（NN-PG-RLHF）的性能边界，结果表明PO-RLHF能以低查询复杂度实现良好性能，为RLHF的理论基础提供了新视角。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.10342v2",
      "published_date": "2024-02-15 22:11:18 UTC",
      "updated_date": "2024-07-15 04:19:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:29:45.706789"
    },
    {
      "arxiv_id": "2402.10340v5",
      "title": "On the Vulnerability of LLM/VLM-Controlled Robotics",
      "title_zh": "翻译失败",
      "authors": [
        "Xiyang Wu",
        "Souradip Chakraborty",
        "Ruiqi Xian",
        "Jing Liang",
        "Tianrui Guan",
        "Fuxiao Liu",
        "Brian M. Sadler",
        "Dinesh Manocha",
        "Amrit Singh Bedi"
      ],
      "abstract": "In this work, we highlight vulnerabilities in robotic systems integrating\nlarge language models (LLMs) and vision-language models (VLMs) due to input\nmodality sensitivities. While LLM/VLM-controlled robots show impressive\nperformance across various tasks, their reliability under slight input\nvariations remains underexplored yet critical. These models are highly\nsensitive to instruction or perceptual input changes, which can trigger\nmisalignment issues, leading to execution failures with severe real-world\nconsequences. To study this issue, we analyze the misalignment-induced\nvulnerabilities within LLM/VLM-controlled robotic systems and present a\nmathematical formulation for failure modes arising from variations in input\nmodalities. We propose empirical perturbation strategies to expose these\nvulnerabilities and validate their effectiveness through experiments on\nmultiple robot manipulation tasks. Our results show that simple input\nperturbations reduce task execution success rates by 22.2% and 14.6% in two\nrepresentative LLM/VLM-controlled robotic systems. These findings underscore\nthe importance of input modality robustness and motivate further research to\nensure the safe and reliable deployment of advanced LLM/VLM-controlled robotic\nsystems.",
      "tldr_zh": "本文研究了整合大型语言模型(LLM)和视觉语言模型(VLM)的机器人系统在输入模式敏感性方面的漏洞，这些系统虽在各种任务中表现出色，但对指令或感知输入变化高度敏感，可能导致执行失败和实际后果。作者通过数学公式描述了输入变化引发的失调问题，并提出经验扰动策略，在多个机器人操作任务上进行实验验证。结果显示，简单输入扰动使两个代表性系统的任务执行成功率分别下降22.2%和14.6%。这些发现突出了输入模式鲁棒性的重要性，并推动进一步研究以实现LLM/VLM控制机器人系统的安全可靠部署。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.10340v5",
      "published_date": "2024-02-15 22:01:45 UTC",
      "updated_date": "2025-03-07 04:01:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:29:58.482697"
    },
    {
      "arxiv_id": "2402.10334v1",
      "title": "HI-GAN: Hierarchical Inpainting GAN with Auxiliary Inputs for Combined RGB and Depth Inpainting",
      "title_zh": "翻译失败",
      "authors": [
        "Ankan Dash",
        "Jingyi Gu",
        "Guiling Wang"
      ],
      "abstract": "Inpainting involves filling in missing pixels or areas in an image, a crucial\ntechnique employed in Mixed Reality environments for various applications,\nparticularly in Diminished Reality (DR) where content is removed from a user's\nvisual environment. Existing methods rely on digital replacement techniques\nwhich necessitate multiple cameras and incur high costs. AR devices and\nsmartphones use ToF depth sensors to capture scene depth maps aligned with RGB\nimages. Despite speed and affordability, ToF cameras create imperfect depth\nmaps with missing pixels. To address the above challenges, we propose\nHierarchical Inpainting GAN (HI-GAN), a novel approach comprising three GANs in\na hierarchical fashion for RGBD inpainting. EdgeGAN and LabelGAN inpaint masked\nedge and segmentation label images respectively, while CombinedRGBD-GAN\ncombines their latent representation outputs and performs RGB and Depth\ninpainting. Edge images and particularly segmentation label images as auxiliary\ninputs significantly enhance inpainting performance by complementary context\nand hierarchical optimization. We believe we make the first attempt to\nincorporate label images into inpainting process.Unlike previous approaches\nrequiring multiple sequential models and separate outputs, our work operates in\nan end-to-end manner, training all three models simultaneously and\nhierarchically. Specifically, EdgeGAN and LabelGAN are first optimized\nseparately and further optimized inside CombinedRGBD-GAN to enhance inpainting\nquality. Experiments demonstrate that HI-GAN works seamlessly and achieves\noverall superior performance compared with existing approaches.",
      "tldr_zh": "本研究针对图像修复（Inpainting）在 Mixed Reality 环境中的应用，特别是 Diminished Reality (DR) 中移除内容的需求，提出了一种新型 Hierarchical Inpainting GAN (HI-GAN) 框架，以解决现有方法依赖多摄像头和高成本的问题。HI-GAN 由三个层次化的 GAN 组成：EdgeGAN 修复边缘图像、LabelGAN 修复分割标签图像，以及 CombinedRGBD-GAN 结合这些辅助输入的潜在表示来同时进行 RGB 和 Depth 图像修复。相比传统方法，该框架首次将分割标签图像融入修复过程，并采用端到端训练方式，先单独优化 EdgeGAN 和 LabelGAN，然后在 CombinedRGBD-GAN 中进一步提升性能。实验结果显示，HI-GAN 在整体性能上优于现有方法，证明了其在 ToF 深度传感器数据处理中的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.10334v1",
      "published_date": "2024-02-15 21:43:56 UTC",
      "updated_date": "2024-02-15 21:43:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:30:11.164169"
    },
    {
      "arxiv_id": "2402.10980v5",
      "title": "ChemReasoner: Heuristic Search over a Large Language Model's Knowledge Space using Quantum-Chemical Feedback",
      "title_zh": "ChemReasoner：利用量子化学反馈在大语言模型知识空间中进行启发式搜索",
      "authors": [
        "Henry W. Sprueill",
        "Carl Edwards",
        "Khushbu Agarwal",
        "Mariefel V. Olarte",
        "Udishnu Sanyal",
        "Conrad Johnston",
        "Hongbin Liu",
        "Heng Ji",
        "Sutanay Choudhury"
      ],
      "abstract": "The discovery of new catalysts is essential for the design of new and more\nefficient chemical processes in order to transition to a sustainable future. We\nintroduce an AI-guided computational screening framework unifying linguistic\nreasoning with quantum-chemistry based feedback from 3D atomistic\nrepresentations. Our approach formulates catalyst discovery as an uncertain\nenvironment where an agent actively searches for highly effective catalysts via\nthe iterative combination of large language model (LLM)-derived hypotheses and\natomistic graph neural network (GNN)-derived feedback. Identified catalysts in\nintermediate search steps undergo structural evaluation based on spatial\norientation, reaction pathways, and stability. Scoring functions based on\nadsorption energies and reaction energy barriers steer the exploration in the\nLLM's knowledge space toward energetically favorable, high-efficiency\ncatalysts. We introduce planning methods that automatically guide the\nexploration without human input, providing competitive performance against\nexpert-enumerated chemical descriptor-based implementations. By integrating\nlanguage-guided reasoning with computational chemistry feedback, our work\npioneers AI-accelerated, trustworthy catalyst discovery.",
      "tldr_zh": "该论文提出了 ChemReasoner，一种 AI 引导的计算筛选框架，将 Large Language Model (LLM) 的语言推理与量子化学反馈相结合，用于高效催化剂发现。该框架将催化剂搜索视为不确定环境，通过迭代结合 LLM 生成的假设和 Graph Neural Network (GNN) 的反馈（如吸附能和反应能垒评分），并评估催化剂的结构稳定性来引导探索。结果显示，这种启发式搜索方法无需人工干预即可实现自动规划，并在性能上与专家枚举的化学描述符实现竞争，推动了 AI 加速的可信赖催化剂发现。",
      "categories": [
        "physics.chem-ph",
        "cs.AI",
        "cs.CE",
        "cs.LG"
      ],
      "primary_category": "physics.chem-ph",
      "comment": "9 pages, accepted by ICML 2024, final version",
      "pdf_url": "http://arxiv.org/pdf/2402.10980v5",
      "published_date": "2024-02-15 21:33:07 UTC",
      "updated_date": "2024-12-09 03:01:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:30:22.118836"
    },
    {
      "arxiv_id": "2403.18923v2",
      "title": "Evolution-based Feature Selection for Predicting Dissolved Oxygen Concentrations in Lakes",
      "title_zh": "基于进化的特征选择用于预测湖泊溶解氧浓度",
      "authors": [
        "Runlong Yu",
        "Robert Ladwig",
        "Xiang Xu",
        "Peijun Zhu",
        "Paul C. Hanson",
        "Yiqun Xie",
        "Xiaowei Jia"
      ],
      "abstract": "Accurate prediction of dissolved oxygen (DO) concentrations in lakes requires\na comprehensive study of phenological patterns across ecosystems, highlighting\nthe need for precise selection of interactions amongst external factors and\ninternal physical-chemical-biological variables. This paper presents the\nMulti-population Cognitive Evolutionary Search (MCES), a novel evolutionary\nalgorithm for complex feature interaction selection problems. MCES allows\nmodels within every population to evolve adaptively, selecting relevant feature\ninteractions for different lake types and tasks. Evaluated on diverse lakes in\nthe Midwestern USA, MCES not only consistently produces accurate predictions\nwith few observed labels but also, through gene maps of models, reveals\nsophisticated phenological patterns of different lake types, embodying the\ninnovative concept of \"AI from nature, for nature\".",
      "tldr_zh": "本论文提出 Multi-population Cognitive Evolutionary Search (MCES)，一种新型进化算法，用于预测湖泊中 dissolved oxygen (DO) 浓度，通过适应性选择外部因素和内部物理-化学-生物变量的复杂交互。MCES 让模型在每个群体中独立进化，以针对不同湖泊类型选择相关特征，从而实现高效预测。在中西部美国湖泊的实验中，该算法不仅使用少量标签就获得高准确率，还通过基因图揭示了各种湖泊的表象学模式，体现了“AI from nature, for nature”的创新概念。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2403.18923v2",
      "published_date": "2024-02-15 20:27:33 UTC",
      "updated_date": "2024-10-29 19:55:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:30:34.785506"
    },
    {
      "arxiv_id": "2402.10979v2",
      "title": "SportsMetrics: Blending Text and Numerical Data to Understand Information Fusion in LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Yebowen Hu",
        "Kaiqiang Song",
        "Sangwoo Cho",
        "Xiaoyang Wang",
        "Hassan Foroosh",
        "Dong Yu",
        "Fei Liu"
      ],
      "abstract": "Large language models hold significant potential for integrating various data\ntypes, such as text documents and database records, for advanced analytics.\nHowever, blending text and numerical data presents substantial challenges. LLMs\nneed to process and cross-reference entities and numbers, handle data\ninconsistencies and redundancies, and develop planning capabilities such as\nbuilding a working memory for managing complex data queries. In this paper, we\nintroduce four novel tasks centered around sports data analytics to evaluate\nthe numerical reasoning and information fusion capabilities of LLMs. These\ntasks involve providing LLMs with detailed, play-by-play sports game\ndescriptions, then challenging them with adversarial scenarios such as new game\nrules, longer durations, scrambled narratives, and analyzing key statistics in\ngame summaries. We conduct extensive experiments on NBA and NFL games to assess\nthe performance of LLMs on these tasks. Our benchmark, SportsMetrics,\nintroduces a new mechanism for assessing LLMs' numerical reasoning and fusion\nskills.",
      "tldr_zh": "本论文探讨大型语言模型（LLMs）在整合文本和数字数据时的挑战，包括处理实体、数字不一致性以及构建工作记忆等能力。\n研究者引入了四个新任务，基于NBA和NFL体育数据分析，涉及提供详细游戏描述并测试LLMs在对抗场景（如新规则、延长时长和打乱叙述）下的表现。\n通过广泛实验，论文建立了SportsMetrics基准，以评估LLMs的numerical reasoning和information fusion技能，并揭示了这些模型在复杂数据查询中的优势与局限。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL 2024 Long Paper",
      "pdf_url": "http://arxiv.org/pdf/2402.10979v2",
      "published_date": "2024-02-15 20:26:07 UTC",
      "updated_date": "2024-06-16 06:43:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:30:45.037539"
    },
    {
      "arxiv_id": "2402.10294v1",
      "title": "LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing",
      "title_zh": "翻译失败",
      "authors": [
        "Bryan Wang",
        "Yuliang Li",
        "Zhaoyang Lv",
        "Haijun Xia",
        "Yan Xu",
        "Raj Sodhi"
      ],
      "abstract": "Video creation has become increasingly popular, yet the expertise and effort\nrequired for editing often pose barriers to beginners. In this paper, we\nexplore the integration of large language models (LLMs) into the video editing\nworkflow to reduce these barriers. Our design vision is embodied in LAVE, a\nnovel system that provides LLM-powered agent assistance and language-augmented\nediting features. LAVE automatically generates language descriptions for the\nuser's footage, serving as the foundation for enabling the LLM to process\nvideos and assist in editing tasks. When the user provides editing objectives,\nthe agent plans and executes relevant actions to fulfill them. Moreover, LAVE\nallows users to edit videos through either the agent or direct UI manipulation,\nproviding flexibility and enabling manual refinement of agent actions. Our user\nstudy, which included eight participants ranging from novices to proficient\neditors, demonstrated LAVE's effectiveness. The results also shed light on user\nperceptions of the proposed LLM-assisted editing paradigm and its impact on\nusers' creativity and sense of co-creation. Based on these findings, we propose\ndesign implications to inform the future development of agent-assisted content\nediting.",
      "tldr_zh": "本研究探讨了如何通过大型语言模型（LLMs）整合到视频编辑流程中，以降低初学者的门槛。LAVE 系统作为主要贡献，提供LLM驱动的代理辅助和语言增强功能，能自动为视频素材生成描述，并根据用户编辑目标规划并执行相关动作，同时支持代理操作或直接UI手动精炼。用户研究涉及八名从新手到熟练编辑的参与者，证明了LAVE的有效性，并揭示了这种LLM辅助范式如何提升用户的创造力和共同创作感。基于这些发现，论文提出了设计含义，以指导未来代理辅助内容编辑的发展。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.MM"
      ],
      "primary_category": "cs.HC",
      "comment": "Paper accepted to the ACM Conference on Intelligent User Interfaces\n  (ACM IUI) 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.10294v1",
      "published_date": "2024-02-15 19:53:11 UTC",
      "updated_date": "2024-02-15 19:53:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:30:57.410933"
    },
    {
      "arxiv_id": "2402.10290v1",
      "title": "Experiments with Encoding Structured Data for Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Sujay Nagesh Koujalgi",
        "Jonathan Dodge"
      ],
      "abstract": "The project's aim is to create an AI agent capable of selecting good actions\nin a game-playing domain called Battlespace. Sequential domains like\nBattlespace are important testbeds for planning problems, as such, the\nDepartment of Defense uses such domains for wargaming exercises. The agents we\ndeveloped combine Monte Carlo Tree Search (MCTS) and Deep Q-Network (DQN)\ntechniques in an effort to navigate the game environment, avoid obstacles,\ninteract with adversaries, and capture the flag. This paper will focus on the\nencoding techniques we explored to present complex structured data stored in a\nPython class, a necessary precursor to an agent.",
      "tldr_zh": "这篇论文探讨了在神经网络中编码结构化数据的实验，旨在为AI代理在Battlespace游戏环境中选择良好行动提供基础。Battlespace作为顺序域，用于测试规划问题，并被美国国防部应用于战争游戏模拟。研究结合了Monte Carlo Tree Search (MCTS)和Deep Q-Network (DQN)技术，焦点在于探索将复杂结构化数据（存储在Python类中）进行编码的方法，作为开发代理的必要先决条件。实验结果有助于提升代理在导航环境、避免障碍、与对手互动以及捕获旗帜等方面的性能。",
      "categories": [
        "cs.AI",
        "I.2.4"
      ],
      "primary_category": "cs.AI",
      "comment": "18 pages, 8 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2402.10290v1",
      "published_date": "2024-02-15 19:45:15 UTC",
      "updated_date": "2024-02-15 19:45:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:31:10.666783"
    },
    {
      "arxiv_id": "2402.10283v1",
      "title": "Backdoor Attack against One-Class Sequential Anomaly Detection Models",
      "title_zh": "针对单类序列异常检测模型的后门攻击",
      "authors": [
        "He Cheng",
        "Shuhan Yuan"
      ],
      "abstract": "Deep anomaly detection on sequential data has garnered significant attention\ndue to the wide application scenarios. However, deep learning-based models face\na critical security threat - their vulnerability to backdoor attacks. In this\npaper, we explore compromising deep sequential anomaly detection models by\nproposing a novel backdoor attack strategy. The attack approach comprises two\nprimary steps, trigger generation and backdoor injection. Trigger generation is\nto derive imperceptible triggers by crafting perturbed samples from the benign\nnormal data, of which the perturbed samples are still normal. The backdoor\ninjection is to properly inject the backdoor triggers to comprise the model\nonly for the samples with triggers. The experimental results demonstrate the\neffectiveness of our proposed attack strategy by injecting backdoors on two\nwell-established one-class anomaly detection models.",
      "tldr_zh": "本论文探讨了后门攻击（backdoor attack）对一类序列异常检测（one-class sequential anomaly detection）模型的威胁，提出了一种新型攻击策略来破坏这些深度学习模型。该策略包括两个关键步骤：触发器生成（trigger generation），即从正常数据中创建微扰样本，这些样本保持正常但隐蔽；以及后门注入（backdoor injection），通过注入触发器使模型仅对带有触发器的样本失效。实验结果证明，该攻击策略在两个成熟的异常检测模型上均有效，突显了此类模型的安全漏洞。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.IT",
        "math.IT"
      ],
      "primary_category": "cs.LG",
      "comment": "This work is accepted by the PAKDD 2024. 12 pages",
      "pdf_url": "http://arxiv.org/pdf/2402.10283v1",
      "published_date": "2024-02-15 19:19:54 UTC",
      "updated_date": "2024-02-15 19:19:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:31:22.162240"
    },
    {
      "arxiv_id": "2402.10210v1",
      "title": "Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Huizhuo Yuan",
        "Zixiang Chen",
        "Kaixuan Ji",
        "Quanquan Gu"
      ],
      "abstract": "Fine-tuning Diffusion Models remains an underexplored frontier in generative\nartificial intelligence (GenAI), especially when compared with the remarkable\nprogress made in fine-tuning Large Language Models (LLMs). While cutting-edge\ndiffusion models such as Stable Diffusion (SD) and SDXL rely on supervised\nfine-tuning, their performance inevitably plateaus after seeing a certain\nvolume of data. Recently, reinforcement learning (RL) has been employed to\nfine-tune diffusion models with human preference data, but it requires at least\ntwo images (\"winner\" and \"loser\" images) for each text prompt. In this paper,\nwe introduce an innovative technique called self-play fine-tuning for diffusion\nmodels (SPIN-Diffusion), where the diffusion model engages in competition with\nits earlier versions, facilitating an iterative self-improvement process. Our\napproach offers an alternative to conventional supervised fine-tuning and RL\nstrategies, significantly improving both model performance and alignment. Our\nexperiments on the Pick-a-Pic dataset reveal that SPIN-Diffusion outperforms\nthe existing supervised fine-tuning method in aspects of human preference\nalignment and visual appeal right from its first iteration. By the second\niteration, it exceeds the performance of RLHF-based methods across all metrics,\nachieving these results with less data.",
      "tldr_zh": "该论文提出了一种名为 SPIN-Diffusion 的自玩式微调(self-play fine-tuning)技术，用于提升扩散模型(diffusion models)在文本到图像生成中的性能。该方法让模型与自身早期版本竞争，实现迭代自改进，从而克服传统监督微调的瓶颈，并减少了强化学习(RL)方法对“winner”和“loser”图像的需求。在 Pick-a-Pic 数据集的实验中，SPIN-Diffusion 从第一迭代起就超过了监督微调在人类偏好对齐和视觉吸引力方面的表现，并在第二迭代超越了基于 RLHF 的方法，同时使用了更少的数据。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "28 pages, 8 figures, 10 tables",
      "pdf_url": "http://arxiv.org/pdf/2402.10210v1",
      "published_date": "2024-02-15 18:59:18 UTC",
      "updated_date": "2024-02-15 18:59:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:31:35.171246"
    },
    {
      "arxiv_id": "2404.08471v1",
      "title": "Revisiting Feature Prediction for Learning Visual Representations from Video",
      "title_zh": "翻译失败",
      "authors": [
        "Adrien Bardes",
        "Quentin Garrido",
        "Jean Ponce",
        "Xinlei Chen",
        "Michael Rabbat",
        "Yann LeCun",
        "Mahmoud Assran",
        "Nicolas Ballas"
      ],
      "abstract": "This paper explores feature prediction as a stand-alone objective for\nunsupervised learning from video and introduces V-JEPA, a collection of vision\nmodels trained solely using a feature prediction objective, without the use of\npretrained image encoders, text, negative examples, reconstruction, or other\nsources of supervision. The models are trained on 2 million videos collected\nfrom public datasets and are evaluated on downstream image and video tasks. Our\nresults show that learning by predicting video features leads to versatile\nvisual representations that perform well on both motion and appearance-based\ntasks, without adaption of the model's parameters; e.g., using a frozen\nbackbone. Our largest model, a ViT-H/16 trained only on videos, obtains 81.9%\non Kinetics-400, 72.2% on Something-Something-v2, and 77.9% on ImageNet1K.",
      "tldr_zh": "本文重新审视了 feature prediction 作为视频无监督学习的独立目标，并引入了 V-JEPA 模型系列，这些模型仅通过预测视频特征进行训练，而不使用预训练图像编码器、文本、负样本、重构或其他监督来源。训练数据来自 200 万个公共视频数据集。结果显示，V-JEPA 产生的视觉表示在运动和外观任务上表现出色，例如最大的 ViT-H/16 模型在 Kinetics-400 上达到 81.9%、Something-Something-v2 上达到 72.2%、ImageNet1K 上达到 77.9%，且无需调整模型参数。总的来说，该方法证明了 feature prediction 在学习通用视频表示方面的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.08471v1",
      "published_date": "2024-02-15 18:59:11 UTC",
      "updated_date": "2024-02-15 18:59:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:31:47.638722"
    },
    {
      "arxiv_id": "2402.10207v6",
      "title": "Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment",
      "title_zh": "翻译失败",
      "authors": [
        "Rui Yang",
        "Xiaoman Pan",
        "Feng Luo",
        "Shuang Qiu",
        "Han Zhong",
        "Dong Yu",
        "Jianshu Chen"
      ],
      "abstract": "We consider the problem of multi-objective alignment of foundation models\nwith human preferences, which is a critical step towards helpful and harmless\nAI systems. However, it is generally costly and unstable to fine-tune large\nfoundation models using reinforcement learning (RL), and the\nmulti-dimensionality, heterogeneity, and conflicting nature of human\npreferences further complicate the alignment process. In this paper, we\nintroduce Rewards-in-Context (RiC), which conditions the response of a\nfoundation model on multiple rewards in its prompt context and applies\nsupervised fine-tuning for alignment. The salient features of RiC are\nsimplicity and adaptivity, as it only requires supervised fine-tuning of a\nsingle foundation model and supports dynamic adjustment for user preferences\nduring inference time. Inspired by the analytical solution of an abstracted\nconvex optimization problem, our dynamic inference-time adjustment method\napproaches the Pareto-optimal solution for multiple objectives. Empirical\nevidence demonstrates the efficacy of our method in aligning both Large\nLanguage Models (LLMs) and diffusion models to accommodate diverse rewards with\nonly around 10% GPU hours compared with multi-objective RL baseline.",
      "tldr_zh": "该论文针对基础模型的多目标对齐问题，提出了一种名为 Rewards-in-Context (RiC) 的方法，通过在提示上下文中加入多个奖励并使用监督微调进行模型对齐，以应对人类偏好多维、异质和冲突的挑战。RiC 的关键特点是简单性和适应性，仅需微调单一模型，并在推理时动态调整用户偏好，灵感来源于抽象凸优化问题的解析解，从而接近 Pareto-optimal 解决方案。实验结果显示，该方法在对齐 Large Language Models (LLMs) 和扩散模型时，能有效适应多样奖励，仅需比多目标 Reinforcement Learning (RL) 基线少约 10% 的 GPU 时间。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by ICML 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.10207v6",
      "published_date": "2024-02-15 18:58:31 UTC",
      "updated_date": "2024-10-16 03:24:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:31:58.653851"
    },
    {
      "arxiv_id": "2402.10206v3",
      "title": "Ising on the Graph: Task-specific Graph Subsampling via the Ising Model",
      "title_zh": "翻译失败",
      "authors": [
        "Maria Bånkestad",
        "Jennifer R. Andersson",
        "Sebastian Mair",
        "Jens Sjölund"
      ],
      "abstract": "Reducing a graph while preserving its overall properties is an important\nproblem with many applications. Typically, reduction approaches either remove\nedges (sparsification) or merge nodes (coarsening) in an unsupervised way with\nno specific downstream task in mind. In this paper, we present an approach for\nsubsampling graph structures using an Ising model defined on either the nodes\nor edges and learning the external magnetic field of the Ising model using a\ngraph neural network. Our approach is task-specific as it can learn how to\nreduce a graph for a specific downstream task in an end-to-end fashion without\nrequiring a differentiable loss function for the task. We showcase the\nversatility of our approach on four distinct applications: image segmentation,\nexplainability for graph classification, 3D shape sparsification, and sparse\napproximate matrix inverse determination.",
      "tldr_zh": "这篇论文提出了一种任务特定的图子采样方法，使用Ising模型在节点或边上定义，并通过Graph Neural Network (GNN)学习外部磁场，以端到端方式减少图结构同时保留其整体属性。该方法克服了传统无监督方法（如sparsification或coarsening）的局限性，不依赖任务的可微损失函数。实验在图像分割、图分类的explainability、3D形状sparsification和稀疏近似矩阵逆等四个应用中验证了其多功能性和有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "29 pages, 22 figures, accepted at the Learning on Graphs conference\n  (LoG 2024)",
      "pdf_url": "http://arxiv.org/pdf/2402.10206v3",
      "published_date": "2024-02-15 18:58:18 UTC",
      "updated_date": "2025-04-08 13:40:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:32:10.406577"
    },
    {
      "arxiv_id": "2402.10204v2",
      "title": "Radio-astronomical Image Reconstruction with Conditional Denoising Diffusion Model",
      "title_zh": "翻译失败",
      "authors": [
        "Mariia Drozdova",
        "Vitaliy Kinakh",
        "Omkar Bait",
        "Olga Taran",
        "Erica Lastufka",
        "Miroslava Dessauges-Zavadsky",
        "Taras Holotyak",
        "Daniel Schaerer",
        "Slava Voloshynovskiy"
      ],
      "abstract": "Reconstructing sky models from dirty radio images for accurate source\nlocalization and flux estimation is crucial for studying galaxy evolution at\nhigh redshift, especially in deep fields using instruments like the Atacama\nLarge Millimetre Array (ALMA). With new projects like the Square Kilometre\nArray (SKA), there's a growing need for better source extraction methods.\nCurrent techniques, such as CLEAN and PyBDSF, often fail to detect faint\nsources, highlighting the need for more accurate methods. This study proposes\nusing stochastic neural networks to rebuild sky models directly from dirty\nimages. This method can pinpoint radio sources and measure their fluxes with\nrelated uncertainties, marking a potential improvement in radio source\ncharacterization. We tested this approach on 10164 images simulated with the\nCASA tool simalma, based on ALMA's Cycle 5.3 antenna setup. We applied\nconditional Denoising Diffusion Probabilistic Models (DDPMs) for sky models\nreconstruction, then used Photutils to determine source coordinates and fluxes,\nassessing the model's performance across different water vapor levels. Our\nmethod showed excellence in source localization, achieving more than 90%\ncompleteness at a signal-to-noise ratio (SNR) as low as 2. It also surpassed\nPyBDSF in flux estimation, accurately identifying fluxes for 96% of sources in\nthe test set, a significant improvement over CLEAN+ PyBDSF's 57%. Conditional\nDDPMs is a powerful tool for image-to-image translation, yielding accurate and\nrobust characterisation of radio sources, and outperforming existing\nmethodologies. While this study underscores its significant potential for\napplications in radio astronomy, we also acknowledge certain limitations that\naccompany its usage, suggesting directions for further refinement and research.",
      "tldr_zh": "本研究针对射电天文图像重建问题，提出使用 Conditional Denoising Diffusion Probabilistic Models (DDPMs) 从脏图像直接重建天空模型，以提高源定位和通量估计的准确性，尤其适用于 ALMA 等仪器的深场观测。实验在基于 ALMA Cycle 5.3 天线配置的 10164 张 CASA 模拟图像上进行，结果显示该方法在信号噪声比 (SNR) 低至 2 时源定位完成率超过 90%，并在通量估计上优于 CLEAN 和 PyBDSF，准确识别 96% 的源。整体而言，Conditional DDPMs 作为图像到图像转换的强大工具，显著提升了射电源表征的鲁棒性，但仍存在某些局限性，需要进一步优化。",
      "categories": [
        "astro-ph.IM",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "astro-ph.IM",
      "comment": "In production in Astronomy&Astrophyics",
      "pdf_url": "http://arxiv.org/pdf/2402.10204v2",
      "published_date": "2024-02-15 18:57:24 UTC",
      "updated_date": "2024-02-20 18:00:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:32:23.938351"
    },
    {
      "arxiv_id": "2402.10196v1",
      "title": "A Trembling House of Cards? Mapping Adversarial Attacks against Language Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Lingbo Mo",
        "Zeyi Liao",
        "Boyuan Zheng",
        "Yu Su",
        "Chaowei Xiao",
        "Huan Sun"
      ],
      "abstract": "Language agents powered by large language models (LLMs) have seen exploding\ndevelopment. Their capability of using language as a vehicle for thought and\ncommunication lends an incredible level of flexibility and versatility. People\nhave quickly capitalized on this capability to connect LLMs to a wide range of\nexternal components and environments: databases, tools, the Internet, robotic\nembodiment, etc. Many believe an unprecedentedly powerful automation technology\nis emerging. However, new automation technologies come with new safety risks,\nespecially for intricate systems like language agents. There is a surprisingly\nlarge gap between the speed and scale of their development and deployment and\nour understanding of their safety risks. Are we building a house of cards? In\nthis position paper, we present the first systematic effort in mapping\nadversarial attacks against language agents. We first present a unified\nconceptual framework for agents with three major components: Perception, Brain,\nand Action. Under this framework, we present a comprehensive discussion and\npropose 12 potential attack scenarios against different components of an agent,\ncovering different attack strategies (e.g., input manipulation, adversarial\ndemonstrations, jailbreaking, backdoors). We also draw connections to\nsuccessful attack strategies previously applied to LLMs. We emphasize the\nurgency to gain a thorough understanding of language agent risks before their\nwidespread deployment.",
      "tldr_zh": "这篇论文探讨了基于大型语言模型 (LLMs) 的语言代理在快速发展中面临的安全风险，强调这些代理连接外部组件（如数据库和工具）可能导致的新型自动化威胁。作者首次提出一个统一的框架，包括 Perception、Brain 和 Action 三个主要组件，并系统映射了 12 种潜在的对抗攻击场景，如输入操纵、adversarial demonstrations、jailbreaking 和 backdoors。论文呼吁在语言代理广泛部署前，紧急深入研究这些风险，以防止构建一个不稳定的“纸牌屋”。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.10196v1",
      "published_date": "2024-02-15 18:51:32 UTC",
      "updated_date": "2024-02-15 18:51:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:32:34.614243"
    },
    {
      "arxiv_id": "2402.10192v3",
      "title": "Multi-Excitation Projective Simulation with a Many-Body Physics Inspired Inductive Bias",
      "title_zh": "翻译失败",
      "authors": [
        "Philip A. LeMaitre",
        "Marius Krumm",
        "Hans J. Briegel"
      ],
      "abstract": "With the impressive progress of deep learning, applications relying on\nmachine learning are increasingly being integrated into daily life. However,\nmost deep learning models have an opaque, oracle-like nature making it\ndifficult to interpret and understand their decisions. This problem led to the\ndevelopment of the field known as eXplainable Artificial Intelligence (XAI).\nOne method in this field known as Projective Simulation (PS) models a\nchain-of-thought as a random walk of a particle on a graph with vertices that\nhave concepts attached to them. While this description has various benefits,\nincluding the possibility of quantization, it cannot be naturally used to model\nthoughts that combine several concepts simultaneously. To overcome this\nlimitation, we introduce Multi-Excitation Projective Simulation (mePS), a\ngeneralization that considers a chain-of-thought to be a random walk of several\nparticles on a hypergraph. A definition for a dynamic hypergraph is put forward\nto describe the agent's training history along with applications to AI and\nhypergraph visualization. An inductive bias inspired by the remarkably\nsuccessful few-body interaction models used in quantum many-body physics is\nformalized for our classical mePS framework and employed to tackle the\nexponential complexity associated with naive implementations of hypergraphs. We\nprove that our inductive bias reduces the complexity from exponential to\npolynomial, with the exponent representing the cutoff on how many particles can\ninteract. We numerically apply our method to two toy environments and a more\ncomplex scenario modelling the diagnosis of a broken computer. These\nenvironments demonstrate the resource savings provided by an appropriate choice\nof inductive bias, as well as showcasing aspects of interpretability. A quantum\nmodel for mePS is also briefly outlined and some future directions for it are\ndiscussed.",
      "tldr_zh": "本论文提出 Multi-Excitation Projective Simulation (mePS)，一种扩展 Projective Simulation (PS) 的框架，通过多个粒子在超图 (hypergraph) 上的随机游走来模拟同时结合多个概念的思维链，从而克服 PS 在处理复杂思维时的局限性。论文引入了受量子多体物理学启发的归纳偏差 (inductive bias)，将超图实现的指数复杂度降低到多项式级别，提高了计算效率。实验在玩具环境和计算机诊断场景中验证了该方法的资源节约和可解释性优势，并简要概述了量子模型的潜在发展方向。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DM",
        "quant-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "26 pages, 8 figures; Code repository at\n  https://github.com/MariusKrumm/ManyBodyMEPS. Reorganized main text for better\n  readability",
      "pdf_url": "http://arxiv.org/pdf/2402.10192v3",
      "published_date": "2024-02-15 18:48:32 UTC",
      "updated_date": "2024-10-23 08:39:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:32:48.561695"
    },
    {
      "arxiv_id": "2402.10184v6",
      "title": "Reward Generalization in RLHF: A Topological Perspective",
      "title_zh": "RLHF 中的奖励泛化：拓扑视角",
      "authors": [
        "Tianyi Qiu",
        "Fanzhi Zeng",
        "Jiaming Ji",
        "Dong Yan",
        "Kaile Wang",
        "Jiayi Zhou",
        "Yang Han",
        "Josef Dai",
        "Xuehai Pan",
        "Yaodong Yang"
      ],
      "abstract": "Existing alignment methods share a common topology of information flow, where\nreward information is collected from humans, modeled with preference learning,\nand used to tune language models. However, this shared topology has not been\nsystematically characterized, nor have its alternatives been thoroughly\nexplored, leaving the problems of low data efficiency and unreliable\ngeneralization unaddressed. As a solution, we introduce a theoretical framework\nfor investigating reward generalization in reinforcement learning from human\nfeedback (RLHF), focusing on the topology of information flow at both macro and\nmicro levels. At the macro level, we portray the RLHF information flow as an\nautoencoding process over behavior distributions, formalizing the RLHF\nobjective of distributional consistency between human preference and model\nbehavior. At the micro level, we present induced Bayesian networks as a theory\nof reward generalization in RLHF, introducing fine-grained dataset topologies\ninto generalization bounds. Combining analysis on both levels, we propose\nreward modeling from tree-structured preference information. It is shown to\nreduce reward uncertainty by up to $\\Theta(\\log n/\\log\\log n)$ times compared\nto baselines, where $n$ is the dataset size. Validation on three NLP tasks\nshows that our tree-based reward model achieves an average win rate of 65%\nagainst baseline methods, thus improving reward generalization for free via\ntopology design.",
      "tldr_zh": "本研究从拓扑视角分析了强化学习从人类反馈（RLHF）中的奖励泛化问题，指出现有方法的共同信息流拓扑导致数据效率低下和泛化不可靠。作者引入一个理论框架，将RLHF视为行为分布上的自编码过程，并在宏观层面强调分布一致性，在微观层面使用诱导贝叶斯网络整合细粒度数据集拓扑，以优化奖励泛化边界。最终，提出基于树结构偏好信息的奖励建模方法，可将奖励不确定性减少高达Θ(log n / log log n)倍，并在三个NLP任务上实现65%的平均胜率，展示了通过拓扑设计提升RLHF性能的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.DM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.10184v6",
      "published_date": "2024-02-15 18:39:24 UTC",
      "updated_date": "2024-09-11 02:20:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:32:59.011607"
    },
    {
      "arxiv_id": "2402.10978v1",
      "title": "Language Models with Conformal Factuality Guarantees",
      "title_zh": "翻译失败",
      "authors": [
        "Christopher Mohri",
        "Tatsunori Hashimoto"
      ],
      "abstract": "Guaranteeing the correctness and factuality of language model (LM) outputs is\na major open problem. In this work, we propose conformal factuality, a\nframework that can ensure high probability correctness guarantees for LMs by\nconnecting language modeling and conformal prediction. We observe that the\ncorrectness of an LM output is equivalent to an uncertainty quantification\nproblem, where the uncertainty sets are defined as the entailment set of an\nLM's output. Using this connection, we show that conformal prediction in\nlanguage models corresponds to a back-off algorithm that provides high\nprobability correctness guarantees by progressively making LM outputs less\nspecific (and expanding the associated uncertainty sets). This approach applies\nto any black-box LM and requires very few human-annotated samples. Evaluations\nof our approach on closed book QA (FActScore, NaturalQuestions) and reasoning\ntasks (MATH) show that our approach can provide 80-90% correctness guarantees\nwhile retaining the majority of the LM's original output.",
      "tldr_zh": "本研究提出了一种名为conformal factuality的框架，用于为语言模型（LM）输出提供高概率正确性保证，通过将语言建模与conformal prediction相结合。框架将LM输出的正确性转化为不确定性量化问题，利用回退算法使输出逐渐不那么具体，从而扩展不确定性集，并适用于任何黑盒LM，仅需少量人类标注样本。在封闭书籍QA（如FActScore和NaturalQuestions）和推理任务（如MATH）上的评估显示，该方法能实现80-90%的正确性保证，同时保留LM大部分原始输出。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.10978v1",
      "published_date": "2024-02-15 18:31:53 UTC",
      "updated_date": "2024-02-15 18:31:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:33:11.314655"
    },
    {
      "arxiv_id": "2402.10177v1",
      "title": "Large Scale Constrained Clustering With Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Benedikt Schesch",
        "Marco Caserta"
      ],
      "abstract": "Given a network, allocating resources at clusters level, rather than at each\nnode, enhances efficiency in resource allocation and usage. In this paper, we\nstudy the problem of finding fully connected disjoint clusters to minimize the\nintra-cluster distances and maximize the number of nodes assigned to the\nclusters, while also ensuring that no two nodes within a cluster exceed a\nthreshold distance. While the problem can easily be formulated using a binary\nlinear model, traditional combinatorial optimization solvers struggle when\ndealing with large-scale instances. We propose an approach to solve this\nconstrained clustering problem via reinforcement learning. Our method involves\ntraining an agent to generate both feasible and (near) optimal solutions. The\nagent learns problem-specific heuristics, tailored to the instances encountered\nin this task. In the results section, we show that our algorithm finds near\noptimal solutions, even for large scale instances.",
      "tldr_zh": "本研究探讨了大规模约束聚类问题，旨在在网络中通过完全连接的互不相交集群来最小化集群内距离、最大化节点分配数量，并确保集群内节点间距离不超过阈值，从而提升资源分配效率。作者提出了一种基于Reinforcement Learning的方法，训练代理学习问题特定的启发式规则，以生成可行且近似最优的解决方案。实验结果表明，该算法在大型实例中能找到接近最优解，显著优于传统组合优化求解器。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "LEANOPT-24 AAAI",
      "pdf_url": "http://arxiv.org/pdf/2402.10177v1",
      "published_date": "2024-02-15 18:27:18 UTC",
      "updated_date": "2024-02-15 18:27:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:33:23.764967"
    },
    {
      "arxiv_id": "2402.10176v2",
      "title": "OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset",
      "title_zh": "翻译失败",
      "authors": [
        "Shubham Toshniwal",
        "Ivan Moshkov",
        "Sean Narenthiran",
        "Daria Gitman",
        "Fei Jia",
        "Igor Gitman"
      ],
      "abstract": "Recent work has shown the immense potential of synthetically generated\ndatasets for training large language models (LLMs), especially for acquiring\ntargeted skills. Current large-scale math instruction tuning datasets such as\nMetaMathQA (Yu et al., 2024) and MAmmoTH (Yue et al., 2024) are constructed\nusing outputs from closed-source LLMs with commercially restrictive licenses. A\nkey reason limiting the use of open-source LLMs in these data generation\npipelines has been the wide gap between the mathematical skills of the best\nclosed-source LLMs, such as GPT-4, and the best open-source LLMs. Building on\nthe recent progress in open-source LLMs, our proposed prompting novelty, and\nsome brute-force scaling, we construct OpenMathInstruct-1, a math instruction\ntuning dataset with 1.8M problem-solution pairs. The dataset is constructed by\nsynthesizing code-interpreter solutions for GSM8K and MATH, two popular math\nreasoning benchmarks, using the recently released and permissively licensed\nMixtral model. Our best model, OpenMath-CodeLlama-70B, trained on a subset of\nOpenMathInstruct-1, achieves a score of 84.6% on GSM8K and 50.7% on MATH, which\nis competitive with the best gpt-distilled models. We release our code, models,\nand the OpenMathInstruct-1 dataset under a commercially permissive license.",
      "tldr_zh": "该研究构建了OpenMathInstruct-1数据集，包含1.8百万数学指令调整问题-解决方案对，旨在解决现有数据集依赖闭源LLMs（如GPT-4）的问题，利用开源LLM Mixtral模型通过合成代码解释器解决方案来生成数据。数据集基于GSM8K和MATH基准，通过创新的提示方法和大规模扩展实现。训练于该数据集的OpenMath-CodeLlama-70B模型在GSM8K上得分84.6%、在MATH上得分50.7%，性能与基于GPT的模型相当。研究开源了代码、模型和数据集，以推动开源LLMs在数学推理领域的应用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Camera-ready version for NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.10176v2",
      "published_date": "2024-02-15 18:26:11 UTC",
      "updated_date": "2024-11-03 03:48:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:33:37.690799"
    },
    {
      "arxiv_id": "2402.10977v2",
      "title": "Generative AI and Process Systems Engineering: The Next Frontier",
      "title_zh": "生成式人工智能与过程系统工程：下一个前沿",
      "authors": [
        "Benjamin Decardi-Nelson",
        "Abdulelah S. Alshehri",
        "Akshay Ajagekar",
        "Fengqi You"
      ],
      "abstract": "This article explores how emerging generative artificial intelligence (GenAI)\nmodels, such as large language models (LLMs), can enhance solution\nmethodologies within process systems engineering (PSE). These cutting-edge\nGenAI models, particularly foundation models (FMs), which are pre-trained on\nextensive, general-purpose datasets, offer versatile adaptability for a broad\nrange of tasks, including responding to queries, image generation, and complex\ndecision-making. Given the close relationship between advancements in PSE and\ndevelopments in computing and systems technologies, exploring the synergy\nbetween GenAI and PSE is essential. We begin our discussion with a compact\noverview of both classic and emerging GenAI models, including FMs, and then\ndive into their applications within key PSE domains: synthesis and design,\noptimization and integration, and process monitoring and control. In each\ndomain, we explore how GenAI models could potentially advance PSE\nmethodologies, providing insights and prospects for each area. Furthermore, the\narticle identifies and discusses potential challenges in fully leveraging GenAI\nwithin PSE, including multiscale modeling, data requirements, evaluation\nmetrics and benchmarks, and trust and safety, thereby deepening the discourse\non effective GenAI integration into systems analysis, design, optimization,\noperations, monitoring, and control. This paper provides a guide for future\nresearch focused on the applications of emerging GenAI in PSE.",
      "tldr_zh": "这篇论文探讨了生成式人工智能（Generative AI, GenAI）模型，如大型语言模型（LLMs）和基础模型（FMs），如何提升过程系统工程（PSE）的解决方案方法。作者首先概述了经典和新兴的 GenAI 模型，然后分析其在 PSE 关键领域的应用，包括合成和设计、优化和集成以及过程监控和控制。论文还指出了潜在挑战，如多尺度建模、数据需求、评估指标和信任安全问题，并为 GenAI 在 PSE 中的未来研究提供指导和前景。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SY",
        "eess.SY",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.10977v2",
      "published_date": "2024-02-15 18:20:42 UTC",
      "updated_date": "2024-05-06 21:40:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:33:47.309828"
    },
    {
      "arxiv_id": "2402.10172v1",
      "title": "OptiMUS: Scalable Optimization Modeling with (MI)LP Solvers and Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Ali AhmadiTeshnizi",
        "Wenzhi Gao",
        "Madeleine Udell"
      ],
      "abstract": "Optimization problems are pervasive in sectors from manufacturing and\ndistribution to healthcare. However, most such problems are still solved\nheuristically by hand rather than optimally by state-of-the-art solvers because\nthe expertise required to formulate and solve these problems limits the\nwidespread adoption of optimization tools and techniques. This paper introduces\nOptiMUS, a Large Language Model (LLM)-based agent designed to formulate and\nsolve (mixed integer) linear programming problems from their natural language\ndescriptions. OptiMUS can develop mathematical models, write and debug solver\ncode, evaluate the generated solutions, and improve its model and code based on\nthese evaluations. OptiMUS utilizes a modular structure to process problems,\nallowing it to handle problems with long descriptions and complex data without\nlong prompts. Experiments demonstrate that OptiMUS outperforms existing\nstate-of-the-art methods on easy datasets by more than $20\\%$ and on hard\ndatasets (including a new dataset, NLP4LP, released with this paper that\nfeatures long and complex problems) by more than $30\\%$.",
      "tldr_zh": "本文研究了优化问题在制造业、医疗等领域中的广泛应用，但由于制定和求解的专长要求，这些问题往往依赖手工启发式方法而非最优求解器。论文引入 OptiMUS，一种基于大型语言模型 (LLM) 的代理，能够从自然语言描述中制定和求解（混合整数）线性规划问题 ((MI)LP Solvers)，并通过模块化结构开发数学模型、编写调试代码、评估解决方案并迭代改进。实验结果表明，OptiMUS 在简单数据集上比现有方法提升超过20%，在复杂数据集（如新发布的 NLP4LP）上提升超过30%，从而促进了优化工具的普及。",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.10172v1",
      "published_date": "2024-02-15 18:19:18 UTC",
      "updated_date": "2024-02-15 18:19:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:34:01.271096"
    },
    {
      "arxiv_id": "2402.10171v1",
      "title": "Data Engineering for Scaling Language Models to 128K Context",
      "title_zh": "翻译失败",
      "authors": [
        "Yao Fu",
        "Rameswar Panda",
        "Xinyao Niu",
        "Xiang Yue",
        "Hannaneh Hajishirzi",
        "Yoon Kim",
        "Hao Peng"
      ],
      "abstract": "We study the continual pretraining recipe for scaling language models'\ncontext lengths to 128K, with a focus on data engineering. We hypothesize that\nlong context modeling, in particular \\textit{the ability to utilize information\nat arbitrary input locations}, is a capability that is mostly already acquired\nthrough large-scale pretraining, and that this capability can be readily\nextended to contexts substantially longer than seen during training~(e.g., 4K\nto 128K) through lightweight continual pretraining on appropriate data mixture.\nWe investigate the \\textit{quantity} and \\textit{quality} of the data for\ncontinual pretraining: (1) for quantity, we show that 500 million to 5 billion\ntokens are enough to enable the model to retrieve information anywhere within\nthe 128K context; (2) for quality, our results equally emphasize \\textit{domain\nbalance} and \\textit{length upsampling}. Concretely, we find that naively\nupsampling longer data on certain domains like books, a common practice of\nexisting work, gives suboptimal performance, and that a balanced domain mixture\nis important. We demonstrate that continual pretraining of the full model on\n1B-5B tokens of such data is an effective and affordable strategy for scaling\nthe context length of language models to 128K. Our recipe outperforms strong\nopen-source long-context models and closes the gap to frontier models like\nGPT-4 128K.",
      "tldr_zh": "本研究探讨了通过数据工程实现语言模型上下文长度扩展至128K的持续预训练策略，认为模型已具备利用任意输入位置信息的技能，可通过轻量级预训练轻松扩展。研究强调数据数量（5亿至50亿tokens足以支持128K上下文检索）和质量（需保持领域平衡，避免仅上采样如书籍等特定长数据，以避免次优性能）。实验结果显示，在1B-5B tokens的平衡数据上持续预训练，不仅高效，而且超过了强开源长上下文模型，并缩小了与GPT-4 128K的差距。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Code at https://github.com/FranxYao/Long-Context-Data-Engineering",
      "pdf_url": "http://arxiv.org/pdf/2402.10171v1",
      "published_date": "2024-02-15 18:19:16 UTC",
      "updated_date": "2024-02-15 18:19:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:34:11.757060"
    },
    {
      "arxiv_id": "2402.15521v1",
      "title": "HKD-SHO: A hybrid smart home system based on knowledge-based and data-driven services",
      "title_zh": "HKD-SHO：一种基于知识驱动和数据驱动服务的混合智能家居系统",
      "authors": [
        "Mingming Qiu",
        "Elie Najm",
        "Rémi Sharrock",
        "Bruno Traverson"
      ],
      "abstract": "A smart home is realized by setting up various services. Several methods have\nbeen proposed to create smart home services, which can be divided into\nknowledge-based and data-driven approaches. However, knowledge-based approaches\nusually require manual input from the inhabitant, which can be complicated if\nthe physical phenomena of the concerned environment states are complex, and the\ninhabitant does not know how to adjust related actuators to achieve the target\nvalues of the states monitored by services. Moreover, machine learning-based\ndata-driven approaches that we are interested in are like black boxes and\ncannot show the inhabitant in which situations certain services proposed\ncertain actuators' states. To solve these problems, we propose a hybrid system\ncalled HKD-SHO (Hybrid Knowledge-based and Data-driven services based Smart\nHOme system), where knowledge-based and machine learning-based data-driven\nservices are profitably integrated. The principal advantage is that it inherits\nthe explicability of knowledge-based services and the dynamism of data-driven\nservices. We compare HKD-SHO with several systems for creating dynamic smart\nhome services, and the results show the better performance of HKD-SHO.",
      "tldr_zh": "该论文提出 HKD-SHO 系统，这是一个混合智能家居框架，将 knowledge-based 和 data-driven 服务相结合，以解决 knowledge-based 方法的复杂手动输入问题以及 data-driven 方法的黑箱不可解释性。HKD-SHO 通过继承 knowledge-based 服务的可解释性和 data-driven 服务的动态性，实现更高效的服务整合。实验结果表明，该系统在创建动态智能家居服务方面比现有系统表现出色。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "keywords: Hybrid System, Knowledge Representation, Reinforcement\n  Learning, Services, Smart Home",
      "pdf_url": "http://arxiv.org/pdf/2402.15521v1",
      "published_date": "2024-02-15 18:13:41 UTC",
      "updated_date": "2024-02-15 18:13:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:34:22.603085"
    },
    {
      "arxiv_id": "2402.10168v1",
      "title": "DeepSRGM -- Sequence Classification and Ranking in Indian Classical Music with Deep Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Sathwik Tejaswi Madhusudhan",
        "Girish Chowdhary"
      ],
      "abstract": "A vital aspect of Indian Classical Music (ICM) is Raga, which serves as a\nmelodic framework for compositions and improvisations alike. Raga Recognition\nis an important music information retrieval task in ICM as it can aid numerous\ndownstream applications ranging from music recommendations to organizing huge\nmusic collections. In this work, we propose a deep learning based approach to\nRaga recognition. Our approach employs efficient pre possessing and learns\ntemporal sequences in music data using Long Short Term Memory based Recurrent\nNeural Networks (LSTM-RNN). We train and test the network on smaller sequences\nsampled from the original audio while the final inference is performed on the\naudio as a whole. Our method achieves an accuracy of 88.1% and 97 % during\ninference on the Comp Music Carnatic dataset and its 10 Raga subset\nrespectively making it the state-of-the-art for the Raga recognition task. Our\napproach also enables sequence ranking which aids us in retrieving melodic\npatterns from a given music data base that are closely related to the presented\nquery sequence.",
      "tldr_zh": "该论文提出了一种基于深度学习的 DeepSRGM 方法，用于印度古典音乐（ICM）的 Raga 识别任务，该方法通过高效预处理和 LSTM-RNN 处理音乐数据的时序序列，实现对 Raga 的分类和排名。研究在 Comp Music Carnatic 数据集上训练和测试模型，使用从原始音频采样的小序列进行学习，最终在完整音频上进行推理，取得了 88.1% 的准确率，并在 10 Raga 子集上达到 97%，超越现有最先进水平。该方法不仅提升了 Raga 识别性能，还支持序列排名功能，便于从音乐数据库中检索与查询相关的旋律模式，从而助力音乐推荐和库组织等应用。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.IR",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.10168v1",
      "published_date": "2024-02-15 18:11:02 UTC",
      "updated_date": "2024-02-15 18:11:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:34:35.388708"
    },
    {
      "arxiv_id": "2402.10142v3",
      "title": "Tracking Changing Probabilities via Dynamic Learners",
      "title_zh": "通过动态学习器跟踪变化的概率",
      "authors": [
        "Omid Madani"
      ],
      "abstract": "Consider a predictor, a learner, whose input is a stream of discrete items.\nThe predictor's task, at every time point, is probabilistic multiclass\nprediction, i.e. to predict which item may occur next by outputting zero or\nmore candidate items, each with a probability, after which the actual item is\nrevealed and the predictor updates. To output probabilities, the predictor\nkeeps track of the proportions of the items it has seen. The stream is\nunbounded (lifelong), and the predictor has finite limited space. The task is\nopen-ended: the set of items is unknown to the predictor and their totality can\nalso grow unbounded. Moreover, there is non-stationarity: the underlying\nfrequencies of items may change, substantially, from time to time. For\ninstance, new items may start appearing and a few recently frequent items may\ncease to occur again. The predictor, being space-bounded, need only provide\nprobabilities for those items which, at the time of prediction, have\nsufficiently high frequency, i.e., the salient items. This problem is motivated\nin the setting of Prediction Games, a self-supervised learning regime where\nconcepts serve as both the predictors and the predictands, and the set of\nconcepts grows over time, resulting in non-stationarities as new concepts are\ngenerated and used. We design and study a number of predictors, sparse moving\naverages(SMAs), for the task. One SMA adapts the sparse exponentiated moving\naverage and another is based on queuing a few counts, keeping dynamic per-item\nhistories. Evaluating the predicted probabilities, under noise and\nnon-stationarity, presents challenges, and we discuss and develop evaluation\nmethods, one based on bounding log-loss. We show that a combination of ideas,\nsupporting dynamic predictand-specific learning rates, offers advantages in\nterms of faster adaption to change (plasticity), while also supporting low\nvariance (stability).",
      "tldr_zh": "这篇论文探讨了在非平稳环境中跟踪变化概率的问题，提出了一种动态学习器来处理无限离散项流的概率多类预测任务，其中预测器需在空间有限的情况下，仅为高频（salient）项输出概率。方法包括设计多种稀疏移动平均(SMAs)，如基于稀疏指数移动平均和队列计数的动态预测器，这些预测器支持项特定学习率，以实现更快适应变化（plasticity）和低方差（stability）。实验结果表明，该方法在噪声和非平稳条件下表现出色，通过基于 log-loss 的评估框架验证了其优势。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "68T05",
        "I.2.6"
      ],
      "primary_category": "cs.LG",
      "comment": "69 pages, 30 figures, 18 tables",
      "pdf_url": "http://arxiv.org/pdf/2402.10142v3",
      "published_date": "2024-02-15 17:48:58 UTC",
      "updated_date": "2024-12-24 04:56:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:34:48.695030"
    },
    {
      "arxiv_id": "2402.10135v1",
      "title": "Benchmarking federated strategies in Peer-to-Peer Federated learning for biomedical data",
      "title_zh": "在点对点联邦学习中针对生物医学数据的联邦策略基准测试",
      "authors": [
        "Jose L. Salmeron",
        "Irina Arévalo",
        "Antonio Ruiz-Celma"
      ],
      "abstract": "The increasing requirements for data protection and privacy has attracted a\nhuge research interest on distributed artificial intelligence and specifically\non federated learning, an emerging machine learning approach that allows the\nconstruction of a model between several participants who hold their own private\ndata. In the initial proposal of federated learning the architecture was\ncentralised and the aggregation was done with federated averaging, meaning that\na central server will orchestrate the federation using the most straightforward\naveraging strategy. This research is focused on testing different federated\nstrategies in a peer-to-peer environment. The authors propose various\naggregation strategies for federated learning, including weighted averaging\naggregation, using different factors and strategies based on participant\ncontribution. The strategies are tested with varying data sizes to identify the\nmost robust ones. This research tests the strategies with several biomedical\ndatasets and the results of the experiments show that the accuracy-based\nweighted average outperforms the classical federated averaging method.",
      "tldr_zh": "这篇论文评估了在点对点（Peer-to-Peer）联邦学习（Federated Learning）环境中各种联邦策略的表现，针对生物医学数据的隐私保护需求。作者提出了多种聚合策略，包括基于参与者贡献的加权平均（Weighted Averaging），并在不同数据大小上进行基准测试。实验结果显示，准确度-based加权平均方法在多个生物医学数据集上优于传统的联邦平均（Federated Averaging）方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.10135v1",
      "published_date": "2024-02-15 17:38:32 UTC",
      "updated_date": "2024-02-15 17:38:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:34:59.813708"
    },
    {
      "arxiv_id": "2402.10133v2",
      "title": "Zero-Shot Reasoning: Personalized Content Generation Without the Cold Start Problem",
      "title_zh": "翻译失败",
      "authors": [
        "Davor Hafnar",
        "Jure Demšar"
      ],
      "abstract": "Procedural content generation uses algorithmic techniques to create large\namounts of new content for games at much lower production costs. In newer\napproaches, procedural content generation utilizes machine learning. However,\nthese methods usually require expensive collection of large amounts of data, as\nwell as the development and training of fairly complex learning models, which\ncan be both extremely time-consuming and expensive. The core of our research is\nto explore whether we can lower the barrier to the use of personalized\nprocedural content generation through a more practical and generalizable\napproach with large language models. Matching game content with player\npreferences benefits both players, who enjoy the game more, and developers, who\nincreasingly depend on players enjoying the game before being able to monetize\nit. Therefore, this paper presents a novel approach to achieving\npersonalization by using large language models to propose levels based on the\ngameplay data continuously collected from individual players. We compared the\nlevels generated using our approach with levels generated with more traditional\nprocedural generation techniques. Our easily reproducible method has proven\nviable in a production setting and outperformed levels generated by traditional\nmethods in the probability that a player will not quit the game mid-level.",
      "tldr_zh": "本研究探讨了零样本推理(Zero-Shot Reasoning)方法，用于实现个性化的程序化内容生成(Procedural Content Generation)，以解决传统方法因数据收集和模型训练而带来的冷启动问题(Cold Start Problem)。该方法利用大型语言模型(Large Language Models)基于玩家的实时游戏数据生成定制关卡，从而降低生产成本并提升玩家体验。实验结果显示，与传统程序化生成技术相比，新方法显著降低了玩家中途退出关卡的概率，为游戏开发提供了更实用且可复制的个性化方案。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages, 6 figures. Paper accepted to IEEE Transactions on Games",
      "pdf_url": "http://arxiv.org/pdf/2402.10133v2",
      "published_date": "2024-02-15 17:37:25 UTC",
      "updated_date": "2024-06-28 10:41:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:35:12.076438"
    },
    {
      "arxiv_id": "2402.10130v1",
      "title": "Is Continual Learning Ready for Real-world Challenges?",
      "title_zh": "持续学习是否已准备好应对现实世界的挑战？",
      "authors": [
        "Theodora Kontogianni",
        "Yuanwen Yue",
        "Siyu Tang",
        "Konrad Schindler"
      ],
      "abstract": "Despite continual learning's long and well-established academic history, its\napplication in real-world scenarios remains rather limited. This paper contends\nthat this gap is attributable to a misalignment between the actual challenges\nof continual learning and the evaluation protocols in use, rendering proposed\nsolutions ineffective for addressing the complexities of real-world setups. We\nvalidate our hypothesis and assess progress to date, using a new 3D semantic\nsegmentation benchmark, OCL-3DSS. We investigate various continual learning\nschemes from the literature by utilizing more realistic protocols that\nnecessitate online and continual learning for dynamic, real-world scenarios\n(eg., in robotics and 3D vision applications). The outcomes are sobering: all\nconsidered methods perform poorly, significantly deviating from the upper bound\nof joint offline training. This raises questions about the applicability of\nexisting methods in realistic settings. Our paper aims to initiate a paradigm\nshift, advocating for the adoption of continual learning methods through new\nexperimental protocols that better emulate real-world conditions to facilitate\nbreakthroughs in the field.",
      "tldr_zh": "本论文质疑持续学习（Continual Learning）在真实世界应用中的准备度，指出现有评估协议与实际挑战脱节，导致方法效果不佳。作者引入了一个新的3D语义分割基准OCL-3DSS，并使用更现实的在线和持续学习协议来测试各种持续学习方案。实验结果显示，所有方法表现远低于联合离线训练的上限，揭示了现有技术的局限性。该研究呼吁采用新的实验协议来模拟真实场景，促进持续学习领域的突破。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.10130v1",
      "published_date": "2024-02-15 17:34:56 UTC",
      "updated_date": "2024-02-15 17:34:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:35:23.435694"
    },
    {
      "arxiv_id": "2402.10115v2",
      "title": "Generating Visual Stimuli from EEG Recordings using Transformer-encoder based EEG encoder and GAN",
      "title_zh": "翻译失败",
      "authors": [
        "Rahul Mishra",
        "Arnav Bhavsar"
      ],
      "abstract": "In this study, we tackle a modern research challenge within the field of\nperceptual brain decoding, which revolves around synthesizing images from EEG\nsignals using an adversarial deep learning framework. The specific objective is\nto recreate images belonging to various object categories by leveraging EEG\nrecordings obtained while subjects view those images. To achieve this, we\nemploy a Transformer-encoder based EEG encoder to produce EEG encodings, which\nserve as inputs to the generator component of the GAN network. Alongside the\nadversarial loss, we also incorporate perceptual loss to enhance the quality of\nthe generated images.",
      "tldr_zh": "这项研究针对感知脑解码领域，提出了一种从 EEG 信号合成图像的方法，旨在重建受试者观看不同物体类别图像时的视觉刺激。方法采用基于 Transformer-encoder 的 EEG encoder 生成 EEG 编码，并将其作为 GAN 网络生成器的输入，同时结合对抗损失和感知损失来优化图像质量。该框架提升了生成图像的准确性和逼真度，为脑机接口和神经科学应用提供了新途径。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "eess.SP",
        "q-bio.NC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.10115v2",
      "published_date": "2024-02-15 17:10:27 UTC",
      "updated_date": "2024-11-20 05:35:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:35:35.585975"
    },
    {
      "arxiv_id": "2402.10110v2",
      "title": "Selective Reflection-Tuning: Student-Selected Data Recycling for LLM Instruction-Tuning",
      "title_zh": "选择性反思调优：学生模型选择的数据再利用用于 LLM 指令调优",
      "authors": [
        "Ming Li",
        "Lichang Chen",
        "Jiuhai Chen",
        "Shwai He",
        "Jiuxiang Gu",
        "Tianyi Zhou"
      ],
      "abstract": "Instruction tuning is critical to large language models (LLMs) for achieving\nbetter instruction following and task adaptation capabilities but its success\nheavily relies on the training data quality. Many recent methods focus on\nimproving the data quality but often overlook the compatibility of the data\nwith the student model being finetuned. This paper introduces Selective\nReflection-Tuning, a novel paradigm that synergizes a teacher LLM's reflection\nand introspection for improving existing data quality with the data selection\ncapability of the student LLM, to automatically refine existing\ninstruction-tuning data. This teacher-student collaboration produces\nhigh-quality and student-compatible instruction-response pairs, resulting in\nsample-efficient instruction tuning and LLMs of superior performance. Selective\nReflection-Tuning is a data augmentation and synthesis that generally improves\nLLM finetuning and self-improvement without collecting brand-new data. We apply\nour method to Alpaca and WizardLM data and achieve much stronger and top-tier\n7B and 13B LLMs.",
      "tldr_zh": "该论文提出Selective Reflection-Tuning，一种创新的指令微调方法，通过教师LLM的反思和内省能力与学生LLM的数据选择机制协同工作，来自动优化现有指令微调数据，确保数据质量与学生模型的兼容性。相比传统方法，该框架无需收集新数据，就能生成高质量的指令-响应对，从而实现更高效的样本利用和LLM性能提升。实验结果显示，在Alpaca和WizardLM数据集上应用该方法后，7B和13B模型的性能显著增强，达到了顶尖水平。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL2024 (findings), Camera-ready",
      "pdf_url": "http://arxiv.org/pdf/2402.10110v2",
      "published_date": "2024-02-15 17:06:21 UTC",
      "updated_date": "2024-06-07 20:23:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:35:49.158940"
    },
    {
      "arxiv_id": "2402.10109v2",
      "title": "Towards Reducing Diagnostic Errors with Interpretable Risk Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Denis Jered McInerney",
        "William Dickinson",
        "Lucy C. Flynn",
        "Andrea C. Young",
        "Geoffrey S. Young",
        "Jan-Willem van de Meent",
        "Byron C. Wallace"
      ],
      "abstract": "Many diagnostic errors occur because clinicians cannot easily access relevant\ninformation in patient Electronic Health Records (EHRs). In this work we\npropose a method to use LLMs to identify pieces of evidence in patient EHR data\nthat indicate increased or decreased risk of specific diagnoses; our ultimate\naim is to increase access to evidence and reduce diagnostic errors. In\nparticular, we propose a Neural Additive Model to make predictions backed by\nevidence with individualized risk estimates at time-points where clinicians are\nstill uncertain, aiming to specifically mitigate delays in diagnosis and errors\nstemming from an incomplete differential. To train such a model, it is\nnecessary to infer temporally fine-grained retrospective labels of eventual\n\"true\" diagnoses. We do so with LLMs, to ensure that the input text is from\nbefore a confident diagnosis can be made. We use an LLM to retrieve an initial\npool of evidence, but then refine this set of evidence according to\ncorrelations learned by the model. We conduct an in-depth evaluation of the\nusefulness of our approach by simulating how it might be used by a clinician to\ndecide between a pre-defined list of differential diagnoses.",
      "tldr_zh": "本研究旨在通过可解释的风险预测减少临床诊断错误，提出一种使用大型语言模型 (LLMs) 从患者电子健康记录 (EHRs) 中识别证据的方法，以评估特定诊断的风险增加或减少。核心方法包括采用 Neural Additive Model 进行基于证据的预测，提供个性化的风险估计，尤其在临床不确定时，帮助缓解诊断延迟和差异诊断错误。为训练模型，研究使用 LLMs 推断时间细粒度的回顾性标签，并通过检索和精炼证据池来优化相关性。实验模拟显示，该方法能有效辅助临床医生在预定义的诊断列表中决策，提升诊断准确性。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.10109v2",
      "published_date": "2024-02-15 17:05:48 UTC",
      "updated_date": "2024-03-19 16:43:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:36:01.421904"
    },
    {
      "arxiv_id": "2402.10107v1",
      "title": "Quantized Embedding Vectors for Controllable Diffusion Language Models",
      "title_zh": "用于可控扩散语言模型的量化嵌入向量",
      "authors": [
        "Cheng Kang",
        "Xinye Chen",
        "Yong Hu",
        "Daniel Novak"
      ],
      "abstract": "Improving the controllability, portability, and inference speed of diffusion\nlanguage models (DLMs) is a key challenge in natural language generation. While\nrecent research has shown significant success in complex text generation with\nlanguage models, the memory and computational power are still very demanding\nand fall short of expectations, which naturally results in low portability and\ninstability for the models. To mitigate these issues, numerous well-established\nmethods were proposed for neural network quantization. To further enhance their\nportability of independent deployment as well as improve their stability\nevaluated by language perplexity, we propose a novel approach called the\nQuantized Embedding Controllable Diffusion Language Model (QE-CDLM). QE-CDLM\nbuilds upon the recent successful controllable DLMs by remodeling the\ntask-specific embedding space via quantization. This leads to a gradient-based\ncontroller for the generation tasks, and more stable intermediate latent\nvariables are obtained, which naturally brings in an accelerated convergence as\nwell as better controllability. Additionally, the adaption fine-tuning method\nis employed to reduce tunable weights. Experimental results on five challenging\nfine-grained control tasks demonstrate that QE-CDLM compares favorably to\nexisting methods in terms of quality and feasibility, achieving better\nperplexity and lightweight fine-tuning.",
      "tldr_zh": "该论文针对扩散语言模型（DLMs）的可控性、可移植性和推理速度问题，提出了一种新型框架Quantized Embedding Controllable Diffusion Language Model (QE-CDLM)。该方法通过量化重塑任务特定的嵌入空间，实现基于梯度的生成控制器，并获得更稳定的中间潜在变量，从而加速收敛并提升可控性，同时采用适应性微调减少可调权重。实验结果显示，在五个细粒度控制任务上，QE-CDLM 在语言困惑度、质量和轻量级微调方面均优于现有方法，显著提高了模型的稳定性和可移植性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.10107v1",
      "published_date": "2024-02-15 17:02:48 UTC",
      "updated_date": "2024-02-15 17:02:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:36:12.245060"
    },
    {
      "arxiv_id": "2402.10104v2",
      "title": "GeoEval: Benchmark for Evaluating LLMs and Multi-Modal Models on Geometry Problem-Solving",
      "title_zh": "翻译失败",
      "authors": [
        "Jiaxin Zhang",
        "Zhongzhi Li",
        "Mingliang Zhang",
        "Fei Yin",
        "Chenglin Liu",
        "Yashar Moshfeghi"
      ],
      "abstract": "Recent advancements in large language models (LLMs) and multi-modal models\n(MMs) have demonstrated their remarkable capabilities in problem-solving. Yet,\ntheir proficiency in tackling geometry math problems, which necessitates an\nintegrated understanding of both textual and visual information, has not been\nthoroughly evaluated. To address this gap, we introduce the GeoEval benchmark,\na comprehensive collection that includes a main subset of 2,000 problems, a 750\nproblems subset focusing on backward reasoning, an augmented subset of 2,000\nproblems, and a hard subset of 300 problems. This benchmark facilitates a\ndeeper investigation into the performance of LLMs and MMs in solving geometry\nmath problems. Our evaluation of ten LLMs and MMs across these varied subsets\nreveals that the WizardMath model excels, achieving a 55.67\\% accuracy rate on\nthe main subset but only a 6.00\\% accuracy on the hard subset. This highlights\nthe critical need for testing models against datasets on which they have not\nbeen pre-trained. Additionally, our findings indicate that GPT-series models\nperform more effectively on problems they have rephrased, suggesting a\npromising method for enhancing model capabilities.",
      "tldr_zh": "该论文引入了 GeoEval 基准，用于评估大型语言模型（LLMs）和多模态模型（MMs）在几何数学问题解决中的性能，填补了现有评估的空白。GeoEval 包括主子集（2000 问题）、后向推理子集（750 问题）、增强子集（2000 问题）和困难子集（300 问题），旨在测试模型对文本和视觉信息的整合理解。评估十个模型的结果显示，WizardMath 在主子集上达到 55.67% 的准确率，但困难子集仅为 6.00%，突显了测试未预训练数据集的必要性；此外，GPT-series 模型在重述问题上表现更佳，提供了一种提升模型能力的潜在策略。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted in ACL 2024 Findings",
      "pdf_url": "http://arxiv.org/pdf/2402.10104v2",
      "published_date": "2024-02-15 16:59:41 UTC",
      "updated_date": "2024-05-17 11:42:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:36:25.408395"
    },
    {
      "arxiv_id": "2402.10102v2",
      "title": "A privacy-preserving, distributed and cooperative FCM-based learning approach for cancer research",
      "title_zh": "翻译失败",
      "authors": [
        "Jose L. Salmeron",
        "Irina Arévalo"
      ],
      "abstract": "Distributed Artificial Intelligence is attracting interest day by day. In\nthis paper, the authors introduce an innovative methodology for distributed\nlearning of Particle Swarm Optimization-based Fuzzy Cognitive Maps in a\nprivacy-preserving way. The authors design a training scheme for collaborative\nFCM learning that offers data privacy compliant with the current regulation.\nThis method is applied to a cancer detection problem, proving that the\nperformance of the model is improved by the Federated Learning process, and\nobtaining similar results to the ones that can be found in the literature.",
      "tldr_zh": "本文提出了一种隐私保护的分布式学习方法，基于 Particle Swarm Optimization 的 Fuzzy Cognitive Maps (FCM)，旨在实现合作式训练，同时符合当前数据隐私法规。该方法通过设计一个协作训练方案，支持多方参与的学习过程，应用于癌症检测问题。实验结果表明，该模型通过 Federated Learning 提升了性能，并取得了与文献中类似的效果。",
      "categories": [
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.AI",
      "comment": "Rough Sets: International Joint Conference, IJCRS 2020",
      "pdf_url": "http://arxiv.org/pdf/2402.10102v2",
      "published_date": "2024-02-15 16:56:25 UTC",
      "updated_date": "2025-03-05 16:51:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:36:35.101257"
    },
    {
      "arxiv_id": "2402.10093v4",
      "title": "MIM-Refiner: A Contrastive Learning Boost from Intermediate Pre-Trained Representations",
      "title_zh": "MIM-Refiner：来自中间预训练表示的对比学习提升",
      "authors": [
        "Benedikt Alkin",
        "Lukas Miklautz",
        "Sepp Hochreiter",
        "Johannes Brandstetter"
      ],
      "abstract": "We introduce MIM (Masked Image Modeling)-Refiner, a contrastive learning\nboost for pre-trained MIM models. MIM-Refiner is motivated by the insight that\nstrong representations within MIM models generally reside in intermediate\nlayers. Accordingly, MIM-Refiner leverages multiple contrastive heads that are\nconnected to different intermediate layers. In each head, a modified nearest\nneighbor objective constructs semantic clusters that capture semantic\ninformation which improves performance on downstream tasks, including\noff-the-shelf and fine-tuning settings.\n  The refinement process is short and simple - yet highly effective. Within a\nfew epochs, we refine the features of MIM models from subpar to\nstate-of-the-art, off-the-shelf features. Refining a ViT-H, pre-trained with\ndata2vec 2.0 on ImageNet-1K, sets a new state-of-the-art in linear probing\n(84.7%) and low-shot classification among models that are pre-trained on\nImageNet-1K. MIM-Refiner efficiently combines the advantages of MIM and ID\nobjectives and compares favorably against previous state-of-the-art SSL models\non a variety of benchmarks such as low-shot classification, long-tailed\nclassification, clustering and semantic segmentation.",
      "tldr_zh": "本研究提出了 MIM-Refiner，一种基于对比学习(contrastive learning)的提升方法，旨在优化预训练的 Masked Image Modeling (MIM) 模型，通过利用模型的中间层(intermediate layers)表示来提升性能。MIM-Refiner 采用多个对比学习头(contrastive heads)，每个头使用修改的最近邻目标(modified nearest neighbor objective)构建语义集群，从而改善下游任务的表现，包括现成使用(off-the-shelf)和微调(fine-tuning)场景。实验结果显示，在 ImageNet-1K 上对 data2vec 2.0 预训练的 ViT-H 模型进行短期精炼后，实现了线性探测(linear probing)的84.7%新 state-of-the-art，以及在低样本分类(low-shot classification)中的领先表现。该方法高效结合了 MIM 和 ID 目标，在长尾分类(long-tailed classification)、聚类和语义分割等基准上优于现有的自监督学习(SSL)模型。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Published as a conference paper at ICLR 2025. Github:\n  https://github.com/ml-jku/MIM-Refiner",
      "pdf_url": "http://arxiv.org/pdf/2402.10093v4",
      "published_date": "2024-02-15 16:46:16 UTC",
      "updated_date": "2025-02-20 23:59:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:36:50.896005"
    },
    {
      "arxiv_id": "2402.10083v1",
      "title": "Fine-tuning Large Language Model (LLM) Artificial Intelligence Chatbots in Ophthalmology and LLM-based evaluation using GPT-4",
      "title_zh": "翻译失败",
      "authors": [
        "Ting Fang Tan",
        "Kabilan Elangovan",
        "Liyuan Jin",
        "Yao Jie",
        "Li Yong",
        "Joshua Lim",
        "Stanley Poh",
        "Wei Yan Ng",
        "Daniel Lim",
        "Yuhe Ke",
        "Nan Liu",
        "Daniel Shu Wei Ting"
      ],
      "abstract": "Purpose: To assess the alignment of GPT-4-based evaluation to human clinician\nexperts, for the evaluation of responses to ophthalmology-related patient\nqueries generated by fine-tuned LLM chatbots. Methods: 400 ophthalmology\nquestions and paired answers were created by ophthalmologists to represent\ncommonly asked patient questions, divided into fine-tuning (368; 92%), and\ntesting (40; 8%). We find-tuned 5 different LLMs, including LLAMA2-7b,\nLLAMA2-7b-Chat, LLAMA2-13b, and LLAMA2-13b-Chat. For the testing dataset,\nadditional 8 glaucoma QnA pairs were included. 200 responses to the testing\ndataset were generated by 5 fine-tuned LLMs for evaluation. A customized\nclinical evaluation rubric was used to guide GPT-4 evaluation, grounded on\nclinical accuracy, relevance, patient safety, and ease of understanding. GPT-4\nevaluation was then compared against ranking by 5 clinicians for clinical\nalignment. Results: Among all fine-tuned LLMs, GPT-3.5 scored the highest\n(87.1%), followed by LLAMA2-13b (80.9%), LLAMA2-13b-chat (75.5%),\nLLAMA2-7b-Chat (70%) and LLAMA2-7b (68.8%) based on the GPT-4 evaluation. GPT-4\nevaluation demonstrated significant agreement with human clinician rankings,\nwith Spearman and Kendall Tau correlation coefficients of 0.90 and 0.80\nrespectively; while correlation based on Cohen Kappa was more modest at 0.50.\nNotably, qualitative analysis and the glaucoma sub-analysis revealed clinical\ninaccuracies in the LLM-generated responses, which were appropriately\nidentified by the GPT-4 evaluation. Conclusion: The notable clinical alignment\nof GPT-4 evaluation highlighted its potential to streamline the clinical\nevaluation of LLM chatbot responses to healthcare-related queries. By\ncomplementing the existing clinician-dependent manual grading, this efficient\nand automated evaluation could assist the validation of future developments in\nLLM applications for healthcare.",
      "tldr_zh": "本研究评估了使用 GPT-4 作为评估工具，与人类临床专家的判断一致性，针对微调后 Large Language Model (LLM) 聊天机器人对眼科患者查询的响应。研究团队创建了 400 个眼科问题用于微调和测试多种 LLM（如 LLAMA2-7b 和 LLAMA2-13b），并通过自定义评估标准（包括临床准确性、相关性、患者安全和易懂性）生成 200 个响应进行比较。结果显示，GPT-3.5 得分最高（87.1%），GPT-4 评估与人类排名的相关系数高达 Spearman 0.90 和 Kendall Tau 0.80，能有效识别 LLM 响应的临床不准确性。总之，此方法证明 GPT-4 可作为高效的自动化工具，辅助临床评估并推动 LLM 在医疗领域的应用。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "13 Pages, 1 Figure, 8 Tables",
      "pdf_url": "http://arxiv.org/pdf/2402.10083v1",
      "published_date": "2024-02-15 16:43:41 UTC",
      "updated_date": "2024-02-15 16:43:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:37:02.538047"
    },
    {
      "arxiv_id": "2402.10076v1",
      "title": "QUICK: Quantization-aware Interleaving and Conflict-free Kernel for efficient LLM inference",
      "title_zh": "翻译失败",
      "authors": [
        "Taesu Kim",
        "Jongho Lee",
        "Daehyun Ahn",
        "Sarang Kim",
        "Jiwoong Choi",
        "Minkyu Kim",
        "Hyungjun Kim"
      ],
      "abstract": "We introduce QUICK, a group of novel optimized CUDA kernels for the efficient\ninference of quantized Large Language Models (LLMs). QUICK addresses the shared\nmemory bank-conflict problem of state-of-the-art mixed precision matrix\nmultiplication kernels. Our method interleaves the quantized weight matrices of\nLLMs offline to skip the shared memory write-back after the dequantization. We\ndemonstrate up to 1.91x speedup over existing kernels of AutoAWQ on larger\nbatches and up to 1.94x throughput gain on representative LLM models on various\nNVIDIA GPU devices.",
      "tldr_zh": "本研究提出QUICK，一组优化后的CUDA kernels，用于提升量化大型语言模型（LLMs）的推理效率。QUICK通过离线交错量化权重矩阵，解决了现有混合精度矩阵乘法内核中的共享内存银行冲突问题，从而避免了去量化后的共享内存写回操作。在各种NVIDIA GPU设备上，实验结果显示，QUICK在更大批次上比AutoAWQ内核快1.91倍，并在代表性LLM模型上实现了高达1.94倍的吞吐量提升。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.10076v1",
      "published_date": "2024-02-15 16:38:41 UTC",
      "updated_date": "2024-02-15 16:38:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:37:15.063810"
    },
    {
      "arxiv_id": "2402.10055v1",
      "title": "Robust semi-automatic vessel tracing in the human retinal image by an instance segmentation neural network",
      "title_zh": "翻译失败",
      "authors": [
        "Siyi Chen",
        "Amir H. Kashani",
        "Ji Yi"
      ],
      "abstract": "The morphology and hierarchy of the vascular systems are essential for\nperfusion in supporting metabolism. In human retina, one of the most\nenergy-demanding organs, retinal circulation nourishes the entire inner retina\nby an intricate vasculature emerging and remerging at the optic nerve head\n(ONH). Thus, tracing the vascular branching from ONH through the vascular tree\ncan illustrate vascular hierarchy and allow detailed morphological\nquantification, and yet remains a challenging task. Here, we presented a novel\napproach for a robust semi-automatic vessel tracing algorithm on human fundus\nimages by an instance segmentation neural network (InSegNN). Distinct from\nsemantic segmentation, InSegNN separates and labels different vascular trees\nindividually and therefore enable tracing each tree throughout its branching.\nWe have built-in three strategies to improve robustness and accuracy with\ntemporal learning, spatial multi-sampling, and dynamic probability map. We\nachieved 83% specificity, and 50% improvement in Symmetric Best Dice (SBD)\ncompared to literature, and outperformed baseline U-net. We have demonstrated\ntracing individual vessel trees from fundus images, and simultaneously retain\nthe vessel hierarchy information. InSegNN paves a way for any subsequent\nmorphological analysis of vascular morphology in relation to retinal diseases.",
      "tldr_zh": "本文提出了一种基于实例分割神经网络(InSegNN)的半自动视网膜血管追踪方法，用于解决从视神经头(ONH)追踪血管树层次和形态量化的挑战。InSegNN不同于语义分割，能单独分离和标记不同血管树，并通过时间学习(temporal learning)、空间多采样(spatial multi-sampling)和动态概率图(dynamic probability map)策略提升鲁棒性和准确性。实验结果显示，该方法在人类眼底图像上达到了83%的特异性，比现有文献提高了50%的Symmetric Best Dice (SBD)，并优于基准U-net模型。该方法可保留血管层次信息，并为视网膜疾病相关的形态分析提供新途径。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.10055v1",
      "published_date": "2024-02-15 16:25:28 UTC",
      "updated_date": "2024-02-15 16:25:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:37:29.148975"
    },
    {
      "arxiv_id": "2402.10052v2",
      "title": "UNDIAL: Self-Distillation with Adjusted Logits for Robust Unlearning in Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yijiang River Dong",
        "Hongzhou Lin",
        "Mikhail Belkin",
        "Ramon Huerta",
        "Ivan Vulić"
      ],
      "abstract": "Mitigating the retention of sensitive or private information in large\nlanguage models is essential for enhancing privacy and safety. Existing\nunlearning methods, like Gradient Ascent and Negative Preference Optimization,\ndirectly tune models to remove unwanted information. However, these methods\noften become unstable because they fine-tune by maximizing cross-entropy loss,\nwhich is the opposite of traditional loss minimization in learning. This\nreversal creates instability, especially on larger datasets, as the model\nstruggles to balance unlearning with maintaining language capacity, leading to\nover-unlearning. In this paper, we introduce UnDIAL (Unlearning via\nSelf-Distillation on Adjusted Logits), a novel and robust unlearning method.\nOur approach leverages self-distillation to adjust logits and selectively\nreduce the influence of targeted tokens. This technique ensures smooth\nconvergence and avoids catastrophic forgetting, even in challenging unlearning\ntasks with large datasets and sequential unlearning requests. Extensive\nexperiments show that UnDIAL can achieve both robustness in unlearning and\nscalability while maintaining stable training dynamics and resilience to\nhyperparameter tuning.",
      "tldr_zh": "本研究针对大型语言模型（Large Language Models）中消除敏感信息的 unlearning 问题，指出现有方法如 Gradient Ascent 和 Negative Preference Optimization 由于最大化交叉熵损失而导致训练不稳定和过度 unlearning。论文提出 UNDIAL（Unlearning via Self-Distillation on Adjusted Logits），一种基于自蒸馏（Self-Distillation）的创新方法，通过调整 logits 来选择性地减少目标标记的影响，确保平滑收敛并避免灾难性遗忘。实验结果显示，UNDIAL 在大规模数据集和连续 unlearning 任务中表现出色，提供更稳健、可扩展的性能，同时保持训练动态的稳定性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.10052v2",
      "published_date": "2024-02-15 16:21:14 UTC",
      "updated_date": "2024-10-16 11:50:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:37:38.721843"
    },
    {
      "arxiv_id": "2402.10051v1",
      "title": "SwissNYF: Tool Grounded LLM Agents for Black Box Setting",
      "title_zh": "翻译失败",
      "authors": [
        "Somnath Sendhil Kumar",
        "Dhruv Jain",
        "Eshaan Agarwal",
        "Raunak Pandey"
      ],
      "abstract": "While Large Language Models (LLMs) have demonstrated enhanced capabilities in\nfunction-calling, these advancements primarily rely on accessing the functions'\nresponses. This methodology is practical for simpler APIs but faces scalability\nissues with irreversible APIs that significantly impact the system, such as a\ndatabase deletion API. Similarly, processes requiring extensive time for each\nAPI call and those necessitating forward planning, like automated action\npipelines, present complex challenges. Furthermore, scenarios often arise where\na generalized approach is needed because algorithms lack direct access to the\nspecific implementations of these functions or secrets to use them. Traditional\ntool planning methods are inadequate in these cases, compelling the need to\noperate within black-box environments. Unlike their performance in tool\nmanipulation, LLMs excel in black-box tasks, such as program synthesis.\nTherefore, we harness the program synthesis capabilities of LLMs to strategize\ntool usage in black-box settings, ensuring solutions are verified prior to\nimplementation. We introduce TOPGUN, an ingeniously crafted approach leveraging\nprogram synthesis for black box tool planning. Accompanied by SwissNYF, a\ncomprehensive suite that integrates black-box algorithms for planning and\nverification tasks, addressing the aforementioned challenges and enhancing the\nversatility and effectiveness of LLMs in complex API interactions. The public\ncode for SwissNYF is available at https://github.com/iclr-dummy-user/SwissNYF.",
      "tldr_zh": "这篇论文探讨了 Large Language Models (LLMs) 在黑箱设置下工具使用的挑战，包括处理不可逆 API（如数据库删除）、耗时调用和前向规划问题。作者引入了 TOPGUN 方法，利用 LLMs 的程序合成能力来规划和验证工具策略，确保解决方案在实施前得到确认。同时，SwissNYF 套件作为一个综合系统，整合黑箱算法以提升 LLMs 在复杂 API 交互中的多功能性和有效性，并提供了公开代码以供进一步研究。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.10051v1",
      "published_date": "2024-02-15 16:15:38 UTC",
      "updated_date": "2024-02-15 16:15:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:37:51.436461"
    },
    {
      "arxiv_id": "2402.10050v1",
      "title": "On-Demand Myoelectric Control Using Wake Gestures to Eliminate False Activations During Activities of Daily Living",
      "title_zh": "翻译失败",
      "authors": [
        "Ethan Eddy",
        "Evan Campbell",
        "Scott Bateman",
        "Erik Scheme"
      ],
      "abstract": "While myoelectric control has recently become a focus of increased research\nas a possible flexible hands-free input modality, current control approaches\nare prone to inadvertent false activations in real-world conditions. In this\nwork, a novel myoelectric control paradigm -- on-demand myoelectric control --\nis proposed, designed, and evaluated, to reduce the number of unrelated muscle\nmovements that are incorrectly interpreted as input gestures . By leveraging\nthe concept of wake gestures, users were able to switch between a dedicated\ncontrol mode and a sleep mode, effectively eliminating inadvertent activations\nduring activities of daily living (ADLs). The feasibility of wake gestures was\ndemonstrated in this work through two online ubiquitous EMG control tasks with\nvarying difficulty levels; dismissing an alarm and controlling a robot. The\nproposed control scheme was able to appropriately ignore almost all\nnon-targeted muscular inputs during ADLs (>99.9%) while maintaining sufficient\nsensitivity for reliable mode switching during intentional wake gesture\nelicitation. These results highlight the potential of wake gestures as a\ncritical step towards enabling ubiquitous myoelectric control-based on-demand\ninput for a wide range of applications.",
      "tldr_zh": "本文提出了一种名为on-demand myoelectric control的新范式，使用wake gestures来减少日常活动中的假激活问题。用户可以通过wake gestures切换到控制模式和睡眠模式，从而有效忽略非目标肌肉输入（在ADLs期间忽略率超过99.9%），同时保持对intentional wake gesture的可靠响应。实验通过dismiss an alarm和controlling a robot两个在线任务验证了该方法的 feasibility，结果显示它显著提升了myoelectric control的准确性和实用性。该研究为ubiquitous myoelectric control在广泛应用中的推广奠定了基础。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.10050v1",
      "published_date": "2024-02-15 16:11:47 UTC",
      "updated_date": "2024-02-15 16:11:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:38:03.439745"
    },
    {
      "arxiv_id": "2402.10251v6",
      "title": "BrainWave: A Brain Signal Foundation Model for Clinical Applications",
      "title_zh": "BrainWave：脑信号基础模型",
      "authors": [
        "Zhizhang Yuan",
        "Fanqi Shen",
        "Meng Li",
        "Yuguo Yu",
        "Chenhao Tan",
        "Yang Yang"
      ],
      "abstract": "Neural electrical activity is fundamental to brain function, underlying a\nrange of cognitive and behavioral processes, including movement, perception,\ndecision-making, and consciousness. Abnormal patterns of neural signaling often\nindicate the presence of underlying brain diseases. The variability among\nindividuals, the diverse array of clinical symptoms from various brain\ndisorders, and the limited availability of diagnostic classifications, have\nposed significant barriers to formulating reliable model of neural signals for\ndiverse application contexts. Here, we present BrainWave, the first foundation\nmodel for both invasive and non-invasive neural recordings, pretrained on more\nthan 40,000 hours of electrical brain recordings (13.79 TB of data) from\napproximately 16,000 individuals. Our analysis show that BrainWave outperforms\nall other competing models and consistently achieves state-of-the-art\nperformance in the diagnosis and identification of neurological disorders. We\nalso demonstrate robust capabilities of BrainWave in enabling zero-shot\ntransfer learning across varying recording conditions and brain diseases, as\nwell as few-shot classification without fine-tuning, suggesting that BrainWave\nlearns highly generalizable representations of neural signals. We hence believe\nthat open-sourcing BrainWave will facilitate a wide range of clinical\napplications in medicine, paving the way for AI-driven approaches to\ninvestigate brain disorders and advance neuroscience research.",
      "tldr_zh": "本文提出BrainWave，一种脑信号基础模型，用于临床应用，旨在解决神经信号变异性和诊断挑战。该模型首次整合侵入性和非侵入性神经记录，基于超过40,000小时（13.79 TB）的数据从约16,000个个体进行预训练。实验结果显示，BrainWave在神经障碍诊断和识别中优于现有模型，实现state-of-the-art性能，并支持zero-shot transfer learning和few-shot分类。开源BrainWave有望促进AI驱动的脑疾病研究和临床应用。",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.LG",
        "eess.SP"
      ],
      "primary_category": "q-bio.NC",
      "comment": "39 pages, 14 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.10251v6",
      "published_date": "2024-02-15 16:04:11 UTC",
      "updated_date": "2024-09-20 01:50:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:38:16.972862"
    },
    {
      "arxiv_id": "2402.10038v2",
      "title": "RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Saeed Khaki",
        "JinJin Li",
        "Lan Ma",
        "Liu Yang",
        "Prathap Ramachandra"
      ],
      "abstract": "Reinforcement learning from human feedback (RLHF) has been extensively\nemployed to align large language models with user intent. However, proximal\npolicy optimization (PPO) based RLHF is occasionally unstable requiring\nsignificant hyperparameter finetuning, and computationally expensive to\nmaximize the estimated reward during alignment. Recently, direct preference\noptimization (DPO) is proposed to address those challenges. However, DPO relies\non contrastive responses generated from human annotator and alternative LLM,\ninstead of the policy model, limiting the effectiveness of the RLHF. In this\npaper, we addresses both challenges by systematically combining rejection\nsampling (RS) and DPO. Our proposed method, RS-DPO, initiates with the\ndevelopment of a supervised fine-tuned policy model (SFT). A varied set of k\nresponses per prompt are sampled directly from the SFT model. RS-DPO identifies\npairs of contrastive samples based on their reward distribution. Finally, we\napply DPO with the contrastive samples to align the model to human preference.\nOur experiments indicate that our proposed method effectively fine-tunes LLMs\nwith limited resource environments, leading to improved alignment with user\nintent. Furthermore, it outperforms existing methods, including RS, PPO, and\nDPO.",
      "tldr_zh": "本文提出 RS-DPO 方法，将拒绝采样（Rejection Sampling, RS）和直接偏好优化（DPO）相结合，以解决基于 RLHF 的语言模型对齐问题，克服了 PPO 的不稳定性和计算开销，以及 DPO 对外部模型依赖的局限性。该方法从监督微调模型（SFT）中采样多个响应，根据奖励分布选择对比样本，然后应用 DPO 进行模型对齐。实验结果表明，RS-DPO 在资源有限的环境中有效提升了大型语言模型与用户意图的对齐性能，并优于现有方法如 RS、PPO 和 DPO。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "16 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.10038v2",
      "published_date": "2024-02-15 16:00:58 UTC",
      "updated_date": "2024-03-30 16:10:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:38:28.195528"
    },
    {
      "arxiv_id": "2402.10028v1",
      "title": "Diffusion Models Meet Contextual Bandits with Large Action Spaces",
      "title_zh": "翻译失败",
      "authors": [
        "Imad Aouali"
      ],
      "abstract": "Efficient exploration is a key challenge in contextual bandits due to the\nlarge size of their action space, where uninformed exploration can result in\ncomputational and statistical inefficiencies. Fortunately, the rewards of\nactions are often correlated and this can be leveraged to explore them\nefficiently. In this work, we capture such correlations using pre-trained\ndiffusion models; upon which we design diffusion Thompson sampling (dTS). Both\ntheoretical and algorithmic foundations are developed for dTS, and empirical\nevaluation also shows its favorable performance.",
      "tldr_zh": "这篇论文针对上下文 bandits（contextual bandits）中动作空间过大的探索效率问题，提出了一种利用预训练扩散模型（diffusion models）捕捉动作回报相关性的方法。作者设计了diffusion Thompson sampling (dTS) 算法，通过该算法实现高效探索，并建立了其理论和算法基础。实证评估结果显示，dTS 在性能上优于基线方法，证明了其在处理大规模动作空间时的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "26 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.10028v1",
      "published_date": "2024-02-15 15:48:55 UTC",
      "updated_date": "2024-02-15 15:48:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:38:39.550123"
    },
    {
      "arxiv_id": "2402.10024v2",
      "title": "Self-Augmented In-Context Learning for Unsupervised Word Translation",
      "title_zh": "翻译失败",
      "authors": [
        "Yaoyiran Li",
        "Anna Korhonen",
        "Ivan Vulić"
      ],
      "abstract": "Recent work has shown that, while large language models (LLMs) demonstrate\nstrong word translation or bilingual lexicon induction (BLI) capabilities in\nfew-shot setups, they still cannot match the performance of 'traditional'\nmapping-based approaches in the unsupervised scenario where no seed translation\npairs are available, especially for lower-resource languages. To address this\nchallenge with LLMs, we propose self-augmented in-context learning (SAIL) for\nunsupervised BLI: starting from a zero-shot prompt, SAIL iteratively induces a\nset of high-confidence word translation pairs for in-context learning (ICL)\nfrom an LLM, which it then reapplies to the same LLM in the ICL fashion. Our\nmethod shows substantial gains over zero-shot prompting of LLMs on two\nestablished BLI benchmarks spanning a wide range of language pairs, also\noutperforming mapping-based baselines across the board. In addition to\nachieving state-of-the-art unsupervised BLI performance, we also conduct\ncomprehensive analyses on SAIL and discuss its limitations.",
      "tldr_zh": "这篇论文针对大语言模型（LLMs）在无监督双语词汇诱导（BLI）场景下的不足，尤其是低资源语言，提出了一种自增强 in-context learning（SAIL）方法。SAIL 通过从零样本提示开始，迭代诱导高置信度词翻译对，并将其重新应用于 LLM 的 in-context learning，从而提升翻译准确性。实验结果显示，SAIL 在两个 BLI 基准测试中显著优于零样本提示和传统映射基线，并进行了全面分析以探讨其局限性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL 2024 Main Conference; 11 Pages, 3 Figures, 9 Tables",
      "pdf_url": "http://arxiv.org/pdf/2402.10024v2",
      "published_date": "2024-02-15 15:43:05 UTC",
      "updated_date": "2024-06-05 13:38:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:38:51.710259"
    },
    {
      "arxiv_id": "2402.10011v3",
      "title": "Clifford Group Equivariant Simplicial Message Passing Networks",
      "title_zh": "Clifford 群等变单纯消息传递网络",
      "authors": [
        "Cong Liu",
        "David Ruhe",
        "Floor Eijkelboom",
        "Patrick Forré"
      ],
      "abstract": "We introduce Clifford Group Equivariant Simplicial Message Passing Networks,\na method for steerable E(n)-equivariant message passing on simplicial\ncomplexes. Our method integrates the expressivity of Clifford group-equivariant\nlayers with simplicial message passing, which is topologically more intricate\nthan regular graph message passing. Clifford algebras include higher-order\nobjects such as bivectors and trivectors, which express geometric features\n(e.g., areas, volumes) derived from vectors. Using this knowledge, we represent\nsimplex features through geometric products of their vertices. To achieve\nefficient simplicial message passing, we share the parameters of the message\nnetwork across different dimensions. Additionally, we restrict the final\nmessage to an aggregation of the incoming messages from different dimensions,\nleading to what we term shared simplicial message passing. Experimental results\nshow that our method is able to outperform both equivariant and simplicial\ngraph neural networks on a variety of geometric tasks.",
      "tldr_zh": "该研究引入了Clifford Group Equivariant Simplicial Message Passing Networks，一种针对simplicial complexes的E(n)-equivariant消息传递方法，旨在结合Clifford group-equivariant layers的表现力和simplicial message passing的拓扑复杂性。通过利用Clifford algebras中的bivectors和trivectors表示几何特征（如面积、体积），并通过vertices的geometric products来表示simplex features，该方法实现了参数共享和消息聚合的efficient simplicial message passing。实验结果表明，该方法在多种几何任务上优于现有的equivariant和simplicial图神经网络。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.10011v3",
      "published_date": "2024-02-15 15:18:53 UTC",
      "updated_date": "2024-03-12 12:38:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:39:03.784828"
    },
    {
      "arxiv_id": "2402.10002v3",
      "title": "MM-Point: Multi-View Information-Enhanced Multi-Modal Self-Supervised 3D Point Cloud Understanding",
      "title_zh": "翻译失败",
      "authors": [
        "Hai-Tao Yu",
        "Mofei Song"
      ],
      "abstract": "In perception, multiple sensory information is integrated to map visual\ninformation from 2D views onto 3D objects, which is beneficial for\nunderstanding in 3D environments. But in terms of a single 2D view rendered\nfrom different angles, only limited partial information can be provided.The\nrichness and value of Multi-view 2D information can provide superior\nself-supervised signals for 3D objects. In this paper, we propose a novel\nself-supervised point cloud representation learning method, MM-Point, which is\ndriven by intra-modal and inter-modal similarity objectives. The core of\nMM-Point lies in the Multi-modal interaction and transmission between 3D\nobjects and multiple 2D views at the same time. In order to more effectively\nsimultaneously perform the consistent cross-modal objective of 2D multi-view\ninformation based on contrastive learning, we further propose Multi-MLP and\nMulti-level Augmentation strategies. Through carefully designed transformation\nstrategies, we further learn Multi-level invariance in 2D Multi-views. MM-Point\ndemonstrates state-of-the-art (SOTA) performance in various downstream tasks.\nFor instance, it achieves a peak accuracy of 92.4% on the synthetic dataset\nModelNet40, and a top accuracy of 87.8% on the real-world dataset ScanObjectNN,\ncomparable to fully supervised methods. Additionally, we demonstrate its\neffectiveness in tasks such as few-shot classification, 3D part segmentation\nand 3D semantic segmentation.",
      "tldr_zh": "本研究提出了一种新型自监督学习方法 MM-Point，用于增强 3D 点云理解，通过整合多视图 2D 信息来驱动 intra-modal 和 inter-modal 相似性目标。核心机制包括多模态交互和传输，同时引入 Multi-MLP 和 Multi-level Augmentation 策略，以实现基于对比学习的跨模态目标，并学习 2D 多视图中的多级别不变性。该方法在下游任务中表现出色，例如在 ModelNet40 数据集上达到 92.4% 的准确率，在 ScanObjectNN 上达到 87.8%，并在少样本分类、3D 部分分割和 3D 语义分割任务中实现 SOTA 性能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by AAAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.10002v3",
      "published_date": "2024-02-15 15:10:17 UTC",
      "updated_date": "2024-02-25 07:58:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:39:16.951274"
    },
    {
      "arxiv_id": "2402.09997v1",
      "title": "LoraRetriever: Input-Aware LoRA Retrieval and Composition for Mixed Tasks in the Wild",
      "title_zh": "翻译失败",
      "authors": [
        "Ziyu Zhao",
        "Leilei Gan",
        "Guoyin Wang",
        "Wangchunshu Zhou",
        "Hongxia Yang",
        "Kun Kuang",
        "Fei Wu"
      ],
      "abstract": "Low-Rank Adaptation (LoRA) provides an effective yet efficient solution for\nfine-tuning large language models (LLM). The modular and plug-and-play nature\nof LoRA enables the integration of diverse domain-specific LoRAs to enhance the\ncapabilities of LLMs. Previous research on exploiting multiple LoRAs either\nfocuses on specific isolated downstream tasks or fixes the selection of LoRAs\nduring training. However, in real-world scenarios, LLMs receive diverse prompts\ncovering different tasks, and the pool of candidate LoRAs is often dynamically\nupdated. To bridge this gap, we propose LoraRetriever, a retrieve-then-compose\nframework that adaptively retrieves and composes multiple LoRAs according to\nthe input prompts. LoraRetriever contains three main components: firstly,\nidentifying and retrieving LoRAs relevant to the given input; secondly,\nformulating strategies for effectively integrating the retrieved LoRAs; and\nthirdly, developing efficient batch inference to accommodate heterogeneous\nrequests. Experimental results indicate that LoraRetriever consistently\noutperforms the baselines, highlighting its practical effectiveness and\nversatility.",
      "tldr_zh": "本研究提出 LoraRetriever，一种基于输入感知的 retrieve-then-compose 框架，用于动态检索和组合 Low-Rank Adaptation (LoRA) 模块，以处理现实场景中大型语言模型 (LLM) 的混合任务。该框架包括三个核心组件：首先，识别并检索与输入提示相关的 LoRA；其次，制定策略有效整合这些 LoRA；第三，开发高效的批量推理以支持异构请求。实验结果显示，LoraRetriever  consistently outperforms the baselines，在多样化任务上表现出色，证明了其实际有效性和多功能性。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.09997v1",
      "published_date": "2024-02-15 15:02:46 UTC",
      "updated_date": "2024-02-15 15:02:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:39:28.722853"
    },
    {
      "arxiv_id": "2402.09984v2",
      "title": "Symmetry-Breaking Augmentations for Ad Hoc Teamwork",
      "title_zh": "翻译失败",
      "authors": [
        "Ravi Hammond",
        "Dustin Craggs",
        "Mingyu Guo",
        "Jakob Foerster",
        "Ian Reid"
      ],
      "abstract": "In dynamic collaborative settings, for artificial intelligence (AI) agents to\nbetter align with humans, they must adapt to novel teammates who utilise\nunforeseen strategies. While adaptation is often simple for humans, it can be\nchallenging for AI agents. Our work introduces symmetry-breaking augmentations\n(SBA) as a novel approach to this challenge. By applying a symmetry-flipping\noperation to increase behavioural diversity among training teammates, SBA\nencourages agents to learn robust responses to unknown strategies, highlighting\nhow social conventions impact human-AI alignment. We demonstrate this\nexperimentally in two settings, showing that our approach outperforms previous\nad hoc teamwork results in the challenging card game Hanabi. In addition, we\npropose a general metric for estimating symmetry dependency amongst a given set\nof policies. Our findings provide insights into how AI systems can better adapt\nto diverse human conventions and the core mechanics of alignment.",
      "tldr_zh": "该研究针对AI代理在动态协作环境中适应未知队友策略的挑战，提出了一种新的方法：symmetry-breaking augmentations (SBA)。SBA通过对训练队友应用symmetry-flipping操作来增加行为多样性，帮助AI学习更鲁棒的响应，并探讨社会规范对人类-AI协调的影响。在实验中，该方法在Hanabi纸牌游戏等设置中超过了以往的ad hoc teamwork结果，并引入了一个通用指标来评估策略间的symmetry dependency。这些发现为AI更好地适应多样的人类规范和提升协调机制提供了重要见解。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "21 pages, 12 figures, Bidirectional Human-AI Alignment workshop, ICLR\n  2025",
      "pdf_url": "http://arxiv.org/pdf/2402.09984v2",
      "published_date": "2024-02-15 14:49:28 UTC",
      "updated_date": "2025-04-19 14:12:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:39:40.372306"
    },
    {
      "arxiv_id": "2402.09982v1",
      "title": "Data Augmentation and Transfer Learning Approaches Applied to Facial Expressions Recognition",
      "title_zh": "数据增强和迁移学习方法在面部表情识别中的应用",
      "authors": [
        "Enrico Randellini",
        "Leonardo Rigutini",
        "Claudio Sacca'"
      ],
      "abstract": "The face expression is the first thing we pay attention to when we want to\nunderstand a person's state of mind. Thus, the ability to recognize facial\nexpressions in an automatic way is a very interesting research field. In this\npaper, because the small size of available training datasets, we propose a\nnovel data augmentation technique that improves the performances in the\nrecognition task. We apply geometrical transformations and build from scratch\nGAN models able to generate new synthetic images for each emotion type. Thus,\non the augmented datasets we fine tune pretrained convolutional neural networks\nwith different architectures. To measure the generalization ability of the\nmodels, we apply extra-database protocol approach, namely we train models on\nthe augmented versions of training dataset and test them on two different\ndatabases. The combination of these techniques allows to reach average accuracy\nvalues of the order of 85\\% for the InceptionResNetV2 model.",
      "tldr_zh": "这篇论文针对面部表情识别任务，由于可用训练数据集规模小，提出了一种新颖的数据增强技术，包括几何变换和从零构建的 GAN 模型来生成每种情绪类型的合成图像。作者随后在增强数据集上微调预训练的卷积神经网络（CNN），如不同架构的模型，以提高识别性能。实验采用额外数据库协议评估模型的泛化能力，结果显示 InceptionResNetV2 模型的平均准确率达到约 85%。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "The 11th International Conference on Artificial Intelligence, Soft\n  Computing and Applications (AIAA 2021)",
      "pdf_url": "http://arxiv.org/pdf/2402.09982v1",
      "published_date": "2024-02-15 14:46:03 UTC",
      "updated_date": "2024-02-15 14:46:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:39:51.864157"
    },
    {
      "arxiv_id": "2402.09977v1",
      "title": "Fast Vocabulary Transfer for Language Model Compression",
      "title_zh": "翻译失败",
      "authors": [
        "Leonidas Gee",
        "Andrea Zugarini",
        "Leonardo Rigutini",
        "Paolo Torroni"
      ],
      "abstract": "Real-world business applications require a trade-off between language model\nperformance and size. We propose a new method for model compression that relies\non vocabulary transfer. We evaluate the method on various vertical domains and\ndownstream tasks. Our results indicate that vocabulary transfer can be\neffectively used in combination with other compression techniques, yielding a\nsignificant reduction in model size and inference time while marginally\ncompromising on performance.",
      "tldr_zh": "该研究提出了一种快速词汇转移(vocabulary transfer)方法，用于语言模型压缩，以在实际商业应用中平衡模型性能和大小。方法通过词汇转移与其他压缩技术结合，在各种垂直领域和下游任务上进行评估。结果显示，这种方法显著降低了模型大小和推理时间，同时仅对性能造成轻微影响，为高效的语言模型优化提供了新途径。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "The 2022 Conference on Empirical Methods in Natural Language\n  Processing (EMNLP 2022)",
      "pdf_url": "http://arxiv.org/pdf/2402.09977v1",
      "published_date": "2024-02-15 14:37:07 UTC",
      "updated_date": "2024-02-15 14:37:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:40:03.585930"
    },
    {
      "arxiv_id": "2402.09941v1",
      "title": "FedLion: Faster Adaptive Federated Optimization with Fewer Communication",
      "title_zh": "FedLion：更快的自适应联邦优化算法伴随更少的通信",
      "authors": [
        "Zhiwei Tang",
        "Tsung-Hui Chang"
      ],
      "abstract": "In Federated Learning (FL), a framework to train machine learning models\nacross distributed data, well-known algorithms like FedAvg tend to have slow\nconvergence rates, resulting in high communication costs during training. To\naddress this challenge, we introduce FedLion, an adaptive federated\noptimization algorithm that seamlessly incorporates key elements from the\nrecently proposed centralized adaptive algorithm, Lion (Chen et al. 2o23), into\nthe FL framework. Through comprehensive evaluations on two widely adopted FL\nbenchmarks, we demonstrate that FedLion outperforms previous state-of-the-art\nadaptive algorithms, including FAFED (Wu et al. 2023) and FedDA. Moreover,\nthanks to the use of signed gradients in local training, FedLion substantially\nreduces data transmission requirements during uplink communication when\ncompared to existing adaptive algorithms, further reducing communication costs.\nLast but not least, this work also includes a novel theoretical analysis,\nshowcasing that FedLion attains faster convergence rate than established FL\nalgorithms like FedAvg.",
      "tldr_zh": "本研究针对联邦学习（Federated Learning, FL）中算法如 FedAvg 的慢速收敛和高通信成本问题，提出了一种自适应优化算法 FedLion，将 Lion 算法的关键元素融入 FL 框架，以加速训练过程。FedLion 通过使用带符号的梯度显著减少上行通信数据传输，从而降低通信开销，并在两个 FL 基准测试中优于现有算法如 FAFED 和 FedDA。理论分析进一步证明，FedLion 的收敛速度比 FedAvg 更快，为高效的分布式机器学习提供了新途径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "ICASSP 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.09941v1",
      "published_date": "2024-02-15 13:41:23 UTC",
      "updated_date": "2024-02-15 13:41:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:40:16.027848"
    },
    {
      "arxiv_id": "2402.09939v1",
      "title": "Generative AI in the Construction Industry: A State-of-the-art Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Ridwan Taiwo",
        "Idris Temitope Bello",
        "Sulemana Fatoama Abdulai",
        "Abdul-Mugis Yussif",
        "Babatunde Abiodun Salami",
        "Abdullahi Saka",
        "Tarek Zayed"
      ],
      "abstract": "The construction industry is a vital sector of the global economy, but it\nfaces many productivity challenges in various processes, such as design,\nplanning, procurement, inspection, and maintenance. Generative artificial\nintelligence (AI), which can create novel and realistic data or content, such\nas text, image, video, or code, based on some input or prior knowledge, offers\ninnovative and disruptive solutions to address these challenges. However, there\nis a gap in the literature on the current state, opportunities, and challenges\nof generative AI in the construction industry. This study aims to fill this gap\nby providing a state-of-the-art analysis of generative AI in construction, with\nthree objectives: (1) to review and categorize the existing and emerging\ngenerative AI opportunities and challenges in the construction industry; (2) to\npropose a framework for construction firms to build customized generative AI\nsolutions using their own data, comprising steps such as data collection,\ndataset curation, training custom large language model (LLM), model evaluation,\nand deployment; and (3) to demonstrate the framework via a case study of\ndeveloping a generative model for querying contract documents. The results show\nthat retrieval augmented generation (RAG) improves the baseline LLM by 5.2,\n9.4, and 4.8% in terms of quality, relevance, and reproducibility. This study\nprovides academics and construction professionals with a comprehensive analysis\nand practical framework to guide the adoption of generative AI techniques to\nenhance productivity, quality, safety, and sustainability across the\nconstruction industry.",
      "tldr_zh": "这篇论文对生成式 AI 在建筑行业的应用进行了现状分析，旨在解决设计、规划、采购、检查和维护等领域的生产力挑战。论文首先审阅并分类了现有和新兴的生成式 AI 机会与挑战，然后提出一个框架，帮助建筑公司使用自身数据构建自定义解决方案，包括数据收集、数据集整理、训练自定义 Large Language Model (LLM)、模型评估和部署。论文通过一个案例研究展示了该框架在开发合同文档查询模型中的应用，结果显示 Retrieval Augmented Generation (RAG) 技术使基线 LLM 在质量、相关性和可重复性方面分别提高了 5.2%、9.4% 和 4.8%。总体上，这为建筑行业的生产力、质量、安全和可持续性提供了全面分析和实用指导。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.HC",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "74 pages, 11 figures, 20 tables",
      "pdf_url": "http://arxiv.org/pdf/2402.09939v1",
      "published_date": "2024-02-15 13:39:55 UTC",
      "updated_date": "2024-02-15 13:39:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:40:29.641769"
    },
    {
      "arxiv_id": "2402.09934v2",
      "title": "Paying Attention to Deflections: Mining Pragmatic Nuances for Whataboutism Detection in Online Discourse",
      "title_zh": "翻译失败",
      "authors": [
        "Khiem Phi",
        "Noushin Salek Faramarzi",
        "Chenlu Wang",
        "Ritwik Banerjee"
      ],
      "abstract": "Whataboutism, a potent tool for disrupting narratives and sowing distrust,\nremains under-explored in quantitative NLP research. Moreover, past work has\nnot distinguished its use as a strategy for misinformation and propaganda from\nits use as a tool for pragmatic and semantic framing. We introduce new datasets\nfrom Twitter and YouTube, revealing overlaps as well as distinctions between\nwhataboutism, propaganda, and the tu quoque fallacy. Furthermore, drawing on\nrecent work in linguistic semantics, we differentiate the `what about' lexical\nconstruct from whataboutism. Our experiments bring to light unique challenges\nin its accurate detection, prompting the introduction of a novel method using\nattention weights for negative sample mining. We report significant\nimprovements of 4% and 10% over previous state-of-the-art methods in our\nTwitter and YouTube collections, respectively.",
      "tldr_zh": "这篇论文探讨了 Whataboutism 在在线话语中的检测问题，强调其作为 misinformation 和 propaganda 策略的区别，以及其在语用和语义框架中的应用。研究者引入了新的 Twitter 和 YouTube 数据集，揭示了 Whataboutism 与 propaganda 及 tu quoque fallacy 之间的重叠和差异，并区分了 'what about' 词汇结构。论文提出了一种新颖方法，使用 attention weights 进行 negative sample mining，实验结果显示在 Twitter 和 YouTube 数据集上分别比现有最先进方法提高了 4% 和 10%。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "14 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.09934v2",
      "published_date": "2024-02-15 13:34:19 UTC",
      "updated_date": "2024-09-22 22:22:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:40:41.894839"
    },
    {
      "arxiv_id": "2402.09923v1",
      "title": "A Dataset of Open-Domain Question Answering with Multiple-Span Answers",
      "title_zh": "一个具有多跨度答案的开放域问答数据集",
      "authors": [
        "Zhiyi Luo",
        "Yingying Zhang",
        "Shuyun Luo",
        "Ying Zhao",
        "Wentao Lyu"
      ],
      "abstract": "Multi-span answer extraction, also known as the task of multi-span question\nanswering (MSQA), is critical for real-world applications, as it requires\nextracting multiple pieces of information from a text to answer complex\nquestions. Despite the active studies and rapid progress in English MSQA\nresearch, there is a notable lack of publicly available MSQA benchmark in\nChinese. Previous efforts for constructing MSQA datasets predominantly\nemphasized entity-centric contextualization, resulting in a bias towards\ncollecting factoid questions and potentially overlooking questions requiring\nmore detailed descriptive responses. To overcome these limitations, we present\nCLEAN, a comprehensive Chinese multi-span question answering dataset that\ninvolves a wide range of open-domain subjects with a substantial number of\ninstances requiring descriptive answers. Additionally, we provide established\nmodels from relevant literature as baselines for CLEAN. Experimental results\nand analysis show the characteristics and challenge of the newly proposed CLEAN\ndataset for the community. Our dataset, CLEAN, will be publicly released at\nzhiyiluo.site/misc/clean_v1.0_ sample.json.",
      "tldr_zh": "该论文介绍了CLEAN，这是一个中文开放领域多-span问答(MSQA)数据集，旨在解决现有数据集偏向实体中心和事实性问题的局限性。CLEAN涵盖广泛主题，包括大量需要描述性答案的实例，从而更好地支持复杂问答任务。作者提供了相关基准模型，并通过实验结果展示了数据集的特性与挑战；数据集将公开发布以供社区使用。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.09923v1",
      "published_date": "2024-02-15 13:03:57 UTC",
      "updated_date": "2024-02-15 13:03:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:40:52.450630"
    },
    {
      "arxiv_id": "2402.09921v1",
      "title": "Identifying and modelling cognitive biases in mobility choices",
      "title_zh": "翻译失败",
      "authors": [
        "Chloe Conrad",
        "Carole Adam"
      ],
      "abstract": "This report presents results from an M1 internship dedicated to agent-based\nmodelling and simulation of daily mobility choices. This simulation is intended\nto be realistic enough to serve as a basis for a serious game about the\nmobility transition. In order to ensure this level of realism, we conducted a\nsurvey to measure if real mobility choices are made rationally, or how biased\nthey are. Results analysed here show that various biases could play a role in\ndecisions. We then propose an implementation in a GAMA agent-based simulation.",
      "tldr_zh": "该研究旨在识别和建模认知 biases 在日常出行选择中的影响，以构建一个足够真实的代理-based 模拟，用于支持关于出行转型的严肃游戏。通过进行调查，研究者发现真实的出行决策往往受各种认知 biases 影响，而非完全理性。最终，他们在 GAMA 代理-based 模拟中实现了这些偏见模型，为未来模拟和游戏应用奠定了基础。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.MA",
        "K.4.2"
      ],
      "primary_category": "cs.CY",
      "comment": "M1 internship report from Univ. Lyon 1 Claude Bernard. Internship was\n  from October 2022 to June 2023",
      "pdf_url": "http://arxiv.org/pdf/2402.09921v1",
      "published_date": "2024-02-15 12:58:27 UTC",
      "updated_date": "2024-02-15 12:58:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:41:02.915856"
    },
    {
      "arxiv_id": "2402.09919v3",
      "title": "Road Graph Generator: Mapping roads at construction sites from GPS data",
      "title_zh": "道路图生成器：从 GPS 数据映射建筑工地道路",
      "authors": [
        "Katarzyna Michałowska",
        "Helga Margrete Bodahl Holmestad",
        "Signe Riemer-Sørensen"
      ],
      "abstract": "We propose a new method for inferring roads from GPS trajectories to map\nconstruction sites. This task presents a unique challenge due to the erratic\nand non-standard movement patterns of construction machinery, which\nsignificantly diverge from typical vehicular traffic on established roads. Our\nproposed method first identifies intersections in the road network that serve\nas critical decision points, and then connects them with edges to produce a\ngraph, which can subsequently be used for planning and task-allocation. We\ndemonstrate the approach by mapping roads at a real-life construction site in\nNorway. The method is validated on four increasingly complex segments of the\nmap. In our tests, the method achieved perfect accuracy in detecting\nintersections and inferring roads in data with no or low noise, while its\nperformance was reduced in areas with significant noise and consistently\nmissing GPS updates.",
      "tldr_zh": "本研究提出了一种名为 Road Graph Generator 的方法，用于从 GPS 轨迹推断建筑工地道路网络，以应对建筑机械不规律运动模式的挑战。该方法首先识别道路网络中的 intersections 作为关键决策点，然后用 edges 连接它们，形成一个 graph，用于规划和任务分配。在挪威真实工地的实验中，该方法在四个复杂度递增的地图段上进行了验证，在无噪声或低噪声数据中实现了完美的 intersections 检测和道路推断准确率，但在高噪声和 GPS 更新缺失区域，性能有所降低。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "22 pages, 4 figures, 8 tables",
      "pdf_url": "http://arxiv.org/pdf/2402.09919v3",
      "published_date": "2024-02-15 12:53:25 UTC",
      "updated_date": "2024-10-08 18:36:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:41:16.851285"
    },
    {
      "arxiv_id": "2402.09911v2",
      "title": "Enhancing Large Language Models with Pseudo- and Multisource- Knowledge Graphs for Open-ended Question Answering",
      "title_zh": "翻译失败",
      "authors": [
        "Jiaxiang Liu",
        "Tong Zhou",
        "Yubo Chen",
        "Kang Liu",
        "Jun Zhao"
      ],
      "abstract": "Mitigating the hallucinations of Large Language Models is a crucial task.\nAlthough some existing methods employ self-enhancement techniques, they fall\nshort of effectively addressing unknown factual hallucinations. Meanwhile,\nKnowledge Graph (KG) enhancement approaches fail to address the generalization\nacross different KG sources and the enhancement of open-ended answer questions\nsimultaneously. To tackle these limitations, we propose a framework that\ncombines Pseudo-Graph Generation and Atomic Knowledge Verification (PG\\&AKV).\nEnhancement of open-ended question-answering begins with leveraging the\nPseudo-Graph Generation to provide the related knowledge framework.\nSubsequently, Atomic Knowledge Verification utilizes atomic-level knowledge\nquerying and verification to achieve generalizability under different KG\nsources. Compared to the baseline, this approach yields a minimum improvement\nof 11.5 in the ROUGE-L score for open-ended questions. For precise-answered\nquestions, we observe a minimum accuracy improvement of 7.5%. Moreover, PG\\&AKV\nalso exhibits generalizability across different KG sources. Utilizing KG\ndifferent from the question sources, PG\\&AKV can even achieve at least a 3.5 %\nperformance improvement. In summary, our results pave the way for enhancing\nLLMs by incorporating Pseudo- and Multisource-KGs, particularly in the filed of\nopen-ended questions.",
      "tldr_zh": "该研究针对 Large Language Models (LLMs) 的幻觉问题，提出 PG&AKV 框架，通过 Pseudo-Graph Generation 生成相关知识框架来提供支持，并利用 Atomic Knowledge Verification 进行原子级知识查询和验证，以实现不同 Knowledge Graph (KG) 来源下的泛化能力。与基线模型相比，该方法使开放式问题的 ROUGE-L 分数至少提高 11.5%，精确回答的准确率至少提升 7.5%，并在 KG 来源与问题来源不同时仍能获得至少 3.5% 的性能改善。该框架为通过伪和多源 KG 增强 LLMs 的开放式问答性能铺平了道路。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.09911v2",
      "published_date": "2024-02-15 12:20:02 UTC",
      "updated_date": "2025-03-03 09:21:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:41:31.096946"
    },
    {
      "arxiv_id": "2402.09906v3",
      "title": "Generative Representational Instruction Tuning",
      "title_zh": "生成式表示指令调优",
      "authors": [
        "Niklas Muennighoff",
        "Hongjin Su",
        "Liang Wang",
        "Nan Yang",
        "Furu Wei",
        "Tao Yu",
        "Amanpreet Singh",
        "Douwe Kiela"
      ],
      "abstract": "All text-based language problems can be reduced to either generation or\nembedding. Current models only perform well at one or the other. We introduce\ngenerative representational instruction tuning (GRIT) whereby a large language\nmodel is trained to handle both generative and embedding tasks by\ndistinguishing between them through instructions. Compared to other open\nmodels, our resulting GritLM 7B sets a new state of the art on the Massive Text\nEmbedding Benchmark (MTEB) and outperforms all models up to its size on a range\nof generative tasks. By scaling up further, GritLM 8x7B outperforms all open\ngenerative language models that we tried while still being among the best\nembedding models. Notably, we find that GRIT matches training on only\ngenerative or embedding data, thus we can unify both at no performance loss.\nAmong other benefits, the unification via GRIT speeds up Retrieval-Augmented\nGeneration (RAG) by > 60% for long documents, by no longer requiring separate\nretrieval and generation models. Models, code, etc. are freely available at\nhttps://github.com/ContextualAI/gritlm.",
      "tldr_zh": "这篇论文提出了 Generative Representational Instruction Tuning (GRIT)，一种创新方法，通过指令训练大型语言模型同时处理生成和嵌入任务，而无需牺牲性能。GRIT 使模型能够区分任务类型，结果显示 GritLM 7B 在 Massive Text Embedding Benchmark (MTEB) 上设立新状态-of-the-art，并在各种生成任务上超越同规模模型。进一步扩展到 GritLM 8x7B 时，它优于所有尝试的开放生成语言模型，同时保持嵌入性能领先。GRIT 的统一训练不仅加速了 Retrieval-Augmented Generation (RAG) 超过 60%，还提供了开源资源，如模型和代码。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "67 pages (16 main), 25 figures, 34 tables",
      "pdf_url": "http://arxiv.org/pdf/2402.09906v3",
      "published_date": "2024-02-15 12:12:19 UTC",
      "updated_date": "2025-03-03 04:28:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:41:41.646094"
    },
    {
      "arxiv_id": "2402.09900v3",
      "title": "Recurrent Reinforcement Learning with Memoroids",
      "title_zh": "翻译失败",
      "authors": [
        "Steven Morad",
        "Chris Lu",
        "Ryan Kortvelesy",
        "Stephan Liwicki",
        "Jakob Foerster",
        "Amanda Prorok"
      ],
      "abstract": "Memory models such as Recurrent Neural Networks (RNNs) and Transformers\naddress Partially Observable Markov Decision Processes (POMDPs) by mapping\ntrajectories to latent Markov states. Neither model scales particularly well to\nlong sequences, especially compared to an emerging class of memory models\ncalled Linear Recurrent Models. We discover that the recurrent update of these\nmodels resembles a monoid, leading us to reformulate existing models using a\nnovel monoid-based framework that we call memoroids. We revisit the traditional\napproach to batching in recurrent reinforcement learning, highlighting\ntheoretical and empirical deficiencies. We leverage memoroids to propose a\nbatching method that improves sample efficiency, increases the return, and\nsimplifies the implementation of recurrent loss functions in reinforcement\nlearning.",
      "tldr_zh": "本研究探讨了记忆模型如 Recurrent Neural Networks (RNNs) 和 Transformers 在处理 Partially Observable Markov Decision Processes (POMDPs) 时面对的序列扩展性问题，并发现 Linear Recurrent Models 的更新机制类似于 monoid。论文提出了一种新型框架 memoroids，通过基于 monoid 的重构来改进现有模型。作者重新审视了强化学习中的传统 batching 方法，引入 memoroids 驱动的新 batching 技术，提升了样本效率、回报率，并简化了 recurrent loss 函数的实现。实验结果显示，该方法在理论和实证上均表现出色。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.09900v3",
      "published_date": "2024-02-15 11:56:53 UTC",
      "updated_date": "2024-10-28 05:15:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:41:51.634738"
    },
    {
      "arxiv_id": "2402.09894v2",
      "title": "Not Just Novelty: A Longitudinal Study on Utility and Customization of an AI Workflow",
      "title_zh": "翻译失败",
      "authors": [
        "Tao Long",
        "Katy Ilonka Gero",
        "Lydia B. Chilton"
      ],
      "abstract": "Generative AI brings novel and impressive abilities to help people in\neveryday tasks. There are many AI workflows that solve real and complex\nproblems by chaining AI outputs together with human interaction. Although there\nis an undeniable lure of AI, it is uncertain how useful generative AI workflows\nare after the novelty wears off. Additionally, workflows built with generative\nAI have the potential to be easily customized to fit users' individual needs,\nbut do users take advantage of this? We conducted a three-week longitudinal\nstudy with 12 users to understand the familiarization and customization of\ngenerative AI tools for science communication. Our study revealed that there\nexists a familiarization phase, during which users were exploring the novel\ncapabilities of the workflow and discovering which aspects they found useful.\nAfter this phase, users understood the workflow and were able to anticipate the\noutputs. Surprisingly, after familiarization the perceived utility of the\nsystem was rated higher than before, indicating that the perceived utility of\nAI is not just a novelty effect. The increase in benefits mainly comes from\nend-users' ability to customize prompts, and thus potentially appropriate the\nsystem to their own needs. This points to a future where generative AI systems\ncan allow us to design for appropriation.",
      "tldr_zh": "本研究通过一项为期三周的纵向研究（Longitudinal Study），调查了生成式 AI（Generative AI）工作流（AI Workflow）的长期效用和自定义潜力，涉及12名用户专注于科学传播工具。研究发现，用户 initially 经历一个熟悉化阶段，探索工作流的 novelty 功能并识别其有用性；随后，用户能更好地预测输出，并对系统的感知效用显著提升，这主要归功于自定义提示的能力，而非单纯的新奇效应。最终，该研究表明，生成式 AI 系统应设计为支持用户 appropriation，从而实现更持久的价值。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "22 pages, 16 figures. ACM Conference on Designing Interactive Systems\n  (DIS 2024)",
      "pdf_url": "http://arxiv.org/pdf/2402.09894v2",
      "published_date": "2024-02-15 11:39:11 UTC",
      "updated_date": "2024-05-31 16:00:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:42:05.102461"
    },
    {
      "arxiv_id": "2402.09883v1",
      "title": "Lester: rotoscope animation through video object segmentation and tracking",
      "title_zh": "翻译失败",
      "authors": [
        "Ruben Tous"
      ],
      "abstract": "This article introduces Lester, a novel method to automatically synthetise\nretro-style 2D animations from videos. The method approaches the challenge\nmainly as an object segmentation and tracking problem. Video frames are\nprocessed with the Segment Anything Model (SAM) and the resulting masks are\ntracked through subsequent frames with DeAOT, a method of hierarchical\npropagation for semi-supervised video object segmentation. The geometry of the\nmasks' contours is simplified with the Douglas-Peucker algorithm. Finally,\nfacial traits, pixelation and a basic shadow effect can be optionally added.\nThe results show that the method exhibits an excellent temporal consistency and\ncan correctly process videos with different poses and appearances, dynamic\nshots, partial shots and diverse backgrounds. The proposed method provides a\nmore simple and deterministic approach than diffusion models based\nvideo-to-video translation pipelines, which suffer from temporal consistency\nproblems and do not cope well with pixelated and schematic outputs. The method\nis also much most practical than techniques based on 3D human pose estimation,\nwhich require custom handcrafted 3D models and are very limited with respect to\nthe type of scenes they can process.",
      "tldr_zh": "本研究提出Lester，一种自动从视频合成复古风格2D动画的方法，主要通过视频对象分割和跟踪来实现。Lester利用Segment Anything Model (SAM)处理视频帧生成掩码，DeAOT进行层次化传播跟踪，并采用Douglas-Peucker算法简化掩码轮廓几何，同时可选添加面部特征、像素化和阴影效果。结果表明，该方法具有优秀的时序一致性，能够处理不同姿势、动态镜头和多样背景的视频，并相较于基于扩散模型的视频转换或3D人体姿势估计技术，更简单、确定性和实用。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.09883v1",
      "published_date": "2024-02-15 11:15:54 UTC",
      "updated_date": "2024-02-15 11:15:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:42:17.485966"
    },
    {
      "arxiv_id": "2402.10248v1",
      "title": "A Data-Driven Supervised Machine Learning Approach to Estimating Global Ambient Air Pollution Concentrations With Associated Prediction Intervals",
      "title_zh": "翻译失败",
      "authors": [
        "Liam J Berrisford",
        "Hugo Barbosa",
        "Ronaldo Menezes"
      ],
      "abstract": "Global ambient air pollution, a transboundary challenge, is typically\naddressed through interventions relying on data from spatially sparse and\nheterogeneously placed monitoring stations. These stations often encounter\ntemporal data gaps due to issues such as power outages. In response, we have\ndeveloped a scalable, data-driven, supervised machine learning framework. This\nmodel is designed to impute missing temporal and spatial measurements, thereby\ngenerating a comprehensive dataset for pollutants including NO$_2$, O$_3$,\nPM$_{10}$, PM$_{2.5}$, and SO$_2$. The dataset, with a fine granularity of\n0.25$^{\\circ}$ at hourly intervals and accompanied by prediction intervals for\neach estimate, caters to a wide range of stakeholders relying on outdoor air\npollution data for downstream assessments. This enables more detailed studies.\nAdditionally, the model's performance across various geographical locations is\nexamined, providing insights and recommendations for strategic placement of\nfuture monitoring stations to further enhance the model's accuracy.",
      "tldr_zh": "本研究提出了一种基于数据驱动的监督机器学习框架，用于估计全球环境空气污染物浓度，包括 NO₂、O₃、PM₁₀、PM₂.₅ 和 SO₂，以解决监测站数据稀疏和缺失问题。该框架能够填充时空缺失数据，生成高分辨率数据集（0.25°空间粒度和小时级时间间隔），并为每个估计提供预测区间，以支持下游评估和更详细的研究。实验结果显示，该模型在不同地理位置表现出色，并提供了战略性建议，以优化未来监测站的放置，从而提升准确性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Main Paper: 25 pages, 15 figures, 5 tables. Supplementary: 4 pages, 3\n  figures",
      "pdf_url": "http://arxiv.org/pdf/2402.10248v1",
      "published_date": "2024-02-15 11:09:22 UTC",
      "updated_date": "2024-02-15 11:09:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:42:29.051224"
    },
    {
      "arxiv_id": "2402.09880v2",
      "title": "Inadequacies of Large Language Model Benchmarks in the Era of Generative Artificial Intelligence",
      "title_zh": "大型语言模型基准测试在生成式人工智能时代的不足之处",
      "authors": [
        "Timothy R. McIntosh",
        "Teo Susnjak",
        "Nalin Arachchilage",
        "Tong Liu",
        "Paul Watters",
        "Malka N. Halgamuge"
      ],
      "abstract": "The rapid rise in popularity of Large Language Models (LLMs) with emerging\ncapabilities has spurred public curiosity to evaluate and compare different\nLLMs, leading many researchers to propose their own LLM benchmarks. Noticing\npreliminary inadequacies in those benchmarks, we embarked on a study to\ncritically assess 23 state-of-the-art LLM benchmarks, using our novel unified\nevaluation framework through the lenses of people, process, and technology,\nunder the pillars of benchmark functionality and integrity. Our research\nuncovered significant limitations, including biases, difficulties in measuring\ngenuine reasoning, adaptability, implementation inconsistencies, prompt\nengineering complexity, evaluator diversity, and the overlooking of cultural\nand ideological norms in one comprehensive assessment. Our discussions\nemphasized the urgent need for standardized methodologies, regulatory\ncertainties, and ethical guidelines in light of Artificial Intelligence (AI)\nadvancements, including advocating for an evolution from static benchmarks to\ndynamic behavioral profiling to accurately capture LLMs' complex behaviors and\npotential risks. Our study highlighted the necessity for a paradigm shift in\nLLM evaluation methodologies, underlining the importance of collaborative\nefforts for the development of universally accepted benchmarks and the\nenhancement of AI systems' integration into society.",
      "tldr_zh": "本研究评估了23个最先进的Large Language Models (LLMs)基准，揭示了其在生成式人工智能时代存在的重大不足，包括偏差、难以测量真实推理能力、适应性问题、实施不一致性、提示工程复杂性、评估者多样性不足以及忽略文化和意识形态规范。研究采用了一个新颖的统一评估框架，从人员、过程和技术角度审视基准的功能性和完整性。结果强调了制定标准化方法、监管标准和道德指南的迫切需求，并倡导从静态基准转向动态行为分析，以更好地捕捉LLMs的复杂行为和潜在风险，从而推动协作努力开发更可靠的AI评估体系。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.09880v2",
      "published_date": "2024-02-15 11:08:10 UTC",
      "updated_date": "2024-10-14 02:11:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:42:40.304553"
    },
    {
      "arxiv_id": "2402.09877v3",
      "title": "On Computing Plans with Uniform Action Costs",
      "title_zh": "翻译失败",
      "authors": [
        "Alberto Pozanco",
        "Daniel Borrajo",
        "Manuela Veloso"
      ],
      "abstract": "In many real-world planning applications, agents might be interested in\nfinding plans whose actions have costs that are as uniform as possible. Such\nplans provide agents with a sense of stability and predictability, which are\nkey features when humans are the agents executing plans suggested by planning\ntools. This paper adapts three uniformity metrics to automated planning, and\nintroduce planning-based compilations that allow to lexicographically optimize\nsum of action costs and action costs uniformity. Experimental results both in\nwell-known and novel planning benchmarks show that the reformulated tasks can\nbe effectively solved in practice to generate uniform plans.",
      "tldr_zh": "本论文探讨了在自动规划中计算行动成本均匀的计划，以提升计划的稳定性和可预测性，尤其适用于人类执行者。论文改编了三种uniformity metrics，并引入planning-based compilations方法，通过字典序优化行动成本总和和均匀性。实验结果显示，在经典和新型规划基准上，这些改编任务能有效解决，并成功生成均匀计划。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.09877v3",
      "published_date": "2024-02-15 11:00:28 UTC",
      "updated_date": "2024-05-24 09:19:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:42:51.359819"
    },
    {
      "arxiv_id": "2402.09871v4",
      "title": "MuChin: A Chinese Colloquial Description Benchmark for Evaluating Language Models in the Field of Music",
      "title_zh": "翻译失败",
      "authors": [
        "Zihao Wang",
        "Shuyu Li",
        "Tao Zhang",
        "Qi Wang",
        "Pengfei Yu",
        "Jinyang Luo",
        "Yan Liu",
        "Ming Xi",
        "Kejun Zhang"
      ],
      "abstract": "The rapidly evolving multimodal Large Language Models (LLMs) urgently require\nnew benchmarks to uniformly evaluate their performance on understanding and\ntextually describing music. However, due to semantic gaps between Music\nInformation Retrieval (MIR) algorithms and human understanding, discrepancies\nbetween professionals and the public, and low precision of annotations,\nexisting music description datasets cannot serve as benchmarks. To this end, we\npresent MuChin, the first open-source music description benchmark in Chinese\ncolloquial language, designed to evaluate the performance of multimodal LLMs in\nunderstanding and describing music. We established the Caichong Music\nAnnotation Platform (CaiMAP) that employs an innovative multi-person,\nmulti-stage assurance method, and recruited both amateurs and professionals to\nensure the precision of annotations and alignment with popular semantics.\nUtilizing this method, we built a dataset with multi-dimensional,\nhigh-precision music annotations, the Caichong Music Dataset (CaiMD), and\ncarefully selected 1,000 high-quality entries to serve as the test set for\nMuChin. Based on MuChin, we analyzed the discrepancies between professionals\nand amateurs in terms of music description, and empirically demonstrated the\neffectiveness of annotated data for fine-tuning LLMs. Ultimately, we employed\nMuChin to evaluate existing music understanding models on their ability to\nprovide colloquial descriptions of music. All data related to the benchmark,\nalong with the scoring code and detailed appendices, have been open-sourced\n(https://github.com/CarlWangChina/MuChin/).",
      "tldr_zh": "这篇论文介绍了 MuChin，这是一个开源的中文口语化音乐描述基准，用于统一评估多模态大语言模型(LLMs)在音乐理解和描述方面的性能，以解决现有数据集的语义差距、专业差异和标注精度问题。研究团队开发了 Caichong Music Annotation Platform (CaiMAP)，采用多人员多阶段方法并招募业余爱好者和专业人士，构建了多维高精度的 Caichong Music Dataset (CaiMD)，从中选出1,000个高质量条目作为测试集。论文分析了专业人士与业余爱好者在音乐描述上的差异，证明了标注数据对微调 LLMs 的有效性，并使用 MuChin 评估了现有音乐理解模型的口语化描述能力，所有相关数据和代码已开源（https://github.com/CarlWangChina/MuChin）。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.MM",
        "eess.AS",
        "68Txx(Primary)14F05, 91Fxx(Secondary)",
        "I.2.7; J.5"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted by International Joint Conference on Artificial Intelligence\n  2024 (IJCAI 2024)",
      "pdf_url": "http://arxiv.org/pdf/2402.09871v4",
      "published_date": "2024-02-15 10:55:01 UTC",
      "updated_date": "2024-06-13 13:36:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:43:07.258090"
    },
    {
      "arxiv_id": "2402.09867v1",
      "title": "Characterizing Accuracy Trade-offs of EEG Applications on Embedded HMPs",
      "title_zh": "翻译失败",
      "authors": [
        "Zain Taufique",
        "Muhammad Awais Bin Altaf",
        "Antonio Miele",
        "Pasi Liljeberg",
        "Anil Kanduri"
      ],
      "abstract": "Electroencephalography (EEG) recordings are analyzed using battery-powered\nwearable devices to monitor brain activities and neurological disorders. These\napplications require long and continuous processing to generate feasible\nresults. However, wearable devices are constrained with limited energy and\ncomputation resources, owing to their small sizes for practical use cases.\nEmbedded heterogeneous multi-core platforms (HMPs) can provide better\nperformance within limited energy budgets for EEG applications. Error\nresilience of the EEG application pipeline can be exploited further to maximize\nthe performance and energy gains with HMPs. However, disciplined tuning of\napproximation on embedded HMPs requires a thorough exploration of the\naccuracy-performance-power trade-off space. In this work, we characterize the\nerror resilience of three EEG applications, including Epileptic Seizure\nDetection, Sleep Stage Classification, and Stress Detection on the real-world\nembedded HMP test-bed of the Odroid XU3 platform. We present a combinatorial\nevaluation of power-performance-accuracy trade-offs of EEG applications at\ndifferent approximation, power, and performance levels to provide insights into\nthe disciplined tuning of approximation in EEG applications on embedded\nplatforms.",
      "tldr_zh": "该研究探讨了 EEG 应用在嵌入式 HMPs（异构多核平台）上的准确性权衡问题，针对可穿戴设备受限于能量和计算资源的挑战，通过利用 EEG 应用的错误弹性来优化性能和能效。研究在 Odroid XU3 平台上对 Epileptic Seizure Detection、Sleep Stage Classification 和 Stress Detection 等三个应用进行了组合评估，分析了不同近似水平、功率和性能设置下的权衡关系。结果为 EEG 应用在嵌入式平台上的有序近似调整提供了宝贵见解，帮助实现高效的脑活动监测。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.PF"
      ],
      "primary_category": "eess.SP",
      "comment": "7 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.09867v1",
      "published_date": "2024-02-15 10:50:42 UTC",
      "updated_date": "2024-02-15 10:50:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:43:20.158738"
    },
    {
      "arxiv_id": "2402.09844v3",
      "title": "Jack of All Trades, Master of Some, a Multi-Purpose Transformer Agent",
      "title_zh": "翻译失败",
      "authors": [
        "Quentin Gallouédec",
        "Edward Beeching",
        "Clément Romac",
        "Emmanuel Dellandréa"
      ],
      "abstract": "The search for a general model that can operate seamlessly across multiple\ndomains remains a key goal in machine learning research. The prevailing\nmethodology in Reinforcement Learning (RL) typically limits models to a single\ntask within a unimodal framework, a limitation that contrasts with the broader\nvision of a versatile, multi-domain model. In this paper, we present Jack of\nAll Trades (JAT), a transformer-based model with a unique design optimized for\nhandling sequential decision-making tasks and multi-modal data types. The JAT\nmodel demonstrates its robust capabilities and versatility by achieving strong\nperformance on very different RL benchmarks, along with promising results on\nComputer Vision (CV) and Natural Language Processing (NLP) tasks, all using a\nsingle set of weights. The JAT model marks a significant step towards more\ngeneral, cross-domain AI model design, and notably, it is the first model of\nits kind to be fully open-sourced at https://huggingface.co/jat-project/jat,\nincluding a pioneering general-purpose dataset.",
      "tldr_zh": "本文提出了一种名为 Jack of All Trades (JAT) 的多用途 Transformer 代理模型，旨在打破传统 Reinforcement Learning (RL) 的单任务限制，实现跨领域的无缝操作。JAT 通过优化处理顺序决策任务和多模态数据，展示了在不同 RL 基准上的强劲性能，并在 Computer Vision (CV) 和 Natural Language Processing (NLP) 任务上取得了有前景的结果，使用同一套权重。该模型标志着通用 AI 设计的重要进展，并首次完全开源，包括一个开创性的通用数据集。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.09844v3",
      "published_date": "2024-02-15 10:01:55 UTC",
      "updated_date": "2024-07-10 15:56:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:43:31.963890"
    },
    {
      "arxiv_id": "2402.09836v2",
      "title": "Chain-of-Planned-Behaviour Workflow Elicits Few-Shot Mobility Generation in LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Chenyang Shao",
        "Fengli Xu",
        "Bingbing Fan",
        "Jingtao Ding",
        "Yuan Yuan",
        "Meng Wang",
        "Yong Li"
      ],
      "abstract": "The powerful reasoning capabilities of large language models (LLMs) have\nbrought revolutionary changes to many fields, but their performance in human\nbehaviour generation has not yet been extensively explored. This gap likely\nemerges because the internal processes governing behavioral intentions cannot\nbe solely explained by abstract reasoning. Instead, they are also influenced by\na multitude of factors, including social norms and personal preference.\nInspired by the Theory of Planned Behaviour (TPB), we develop a LLM workflow\nnamed Chain-of-Planned Behaviour (CoPB) for mobility behaviour generation,\nwhich reflects the important spatio-temporal dynamics of human activities.\nThrough exploiting the cognitive structures of attitude, subjective norms, and\nperceived behaviour control in TPB, CoPB significantly enhance the ability of\nLLMs to reason the intention of next movement. Specifically, CoPB substantially\nreduces the error rate of mobility intention generation from 57.8% to 19.4%. To\nimprove the scalability of the proposed CoPB workflow, we further explore the\nsynergy between LLMs and mechanistic models. We find mechanistic mobility\nmodels, such as gravity model, can effectively map mobility intentions to\nphysical mobility behaviours. The strategy of integrating CoPB with gravity\nmodel can reduce the token cost by 97.7% and achieve better performance\nsimultaneously. Besides, the proposed CoPB workflow can facilitate GPT-4-turbo\nto automatically generate high quality labels for mobility behavior reasoning.\nWe show such labels can be leveraged to fine-tune the smaller-scale, open\nsource LLaMA 3-8B, which significantly reduces usage costs without sacrificing\nthe quality of the generated behaviours.",
      "tldr_zh": "本研究受 Theory of Planned Behaviour (TPB) 启发，提出 Chain-of-Planned-Behaviour (CoPB) 工作流，以提升大型语言模型 (LLMs) 在少样本移动性行为生成中的性能。CoPB 通过利用态度、主观规范和感知行为控制等认知结构，帮助 LLMs 更准确地推理人类移动意图，将错误率从 57.8% 降低至 19.4%。此外，该框架与机制模型如 gravity model 结合，能减少 token 成本 97.7%，同时提升整体表现。最终，CoPB 还可辅助 GPT-4-turbo 生成高质量标签，用于微调小型开源模型如 LLaMA 3-8B，实现成本降低而不牺牲生成质量。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.09836v2",
      "published_date": "2024-02-15 09:58:23 UTC",
      "updated_date": "2024-06-05 09:27:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:43:45.508253"
    },
    {
      "arxiv_id": "2402.09830v1",
      "title": "Utilizing GANs for Fraud Detection: Model Training with Synthetic Transaction Data",
      "title_zh": "利用 GANs 进行欺诈检测：使用合成交易数据的模型训练",
      "authors": [
        "Mengran Zhu",
        "Yulu Gong",
        "Yafei Xiang",
        "Hanyi Yu",
        "Shuning Huo"
      ],
      "abstract": "Anomaly detection is a critical challenge across various research domains,\naiming to identify instances that deviate from normal data distributions. This\npaper explores the application of Generative Adversarial Networks (GANs) in\nfraud detection, comparing their advantages with traditional methods. GANs, a\ntype of Artificial Neural Network (ANN), have shown promise in modeling complex\ndata distributions, making them effective tools for anomaly detection. The\npaper systematically describes the principles of GANs and their derivative\nmodels, emphasizing their application in fraud detection across different\ndatasets. And by building a collection of adversarial verification graphs, we\nwill effectively prevent fraud caused by bots or automated systems and ensure\nthat the users in the transaction are real. The objective of the experiment is\nto design and implement a fake face verification code and fraud detection\nsystem based on Generative Adversarial network (GANs) algorithm to enhance the\nsecurity of the transaction process.The study demonstrates the potential of\nGANs in enhancing transaction security through deep learning techniques.",
      "tldr_zh": "本论文探讨了使用生成对抗网络(GANs)进行欺诈检测，通过生成合成交易数据来训练模型，并与传统方法进行比较。GANs 能够有效建模复杂数据分布，并通过构建对抗验证图来防止由机器人或自动化系统引发的欺诈。研究设计并实现了基于 GANs 的假面验证代码系统，提升了交易过程的安全性。实验结果表明，GANs 在不同数据集上显示出显著优势，有助于通过深度学习技术增强整体交易安全。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.09830v1",
      "published_date": "2024-02-15 09:48:20 UTC",
      "updated_date": "2024-02-15 09:48:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:43:55.328893"
    },
    {
      "arxiv_id": "2402.09820v2",
      "title": "Utilizing Deep Learning for Enhancing Network Resilience in Finance",
      "title_zh": "利用深度学习增强金融领域的网络弹性",
      "authors": [
        "Yulu Gong",
        "Mengran Zhu",
        "Shuning Huo",
        "Yafei Xiang",
        "Hanyi Yu"
      ],
      "abstract": "In the age of the Internet, people's lives are increasingly dependent on\ntoday's network technology. Maintaining network integrity and protecting the\nlegitimate interests of users is at the heart of network construction. Threat\ndetection is an important part of a complete and effective defense system. How\nto effectively detect unknown threats is one of the concerns of network\nprotection. Currently, network threat detection is usually based on rules and\ntraditional machine learning methods, which create artificial rules or extract\ncommon spatiotemporal features, which cannot be applied to large-scale data\napplications, and the emergence of unknown risks causes the detection accuracy\nof the original model to decline. With this in mind, this paper uses deep\nlearning for advanced threat detection to improve protective measures in the\nfinancial industry. Many network researchers have shifted their focus to\nexception-based intrusion detection techniques. The detection technology mainly\nuses statistical machine learning methods - collecting normal program and\nnetwork behavior data, extracting multidimensional features, and training\ndecision machine learning models on this basis (commonly used include naive\nBayes, decision trees, support vector machines, random forests, etc.).",
      "tldr_zh": "本论文探讨了金融行业网络威胁检测的挑战，传统方法如基于规则和机器学习（machine learning）模型（如朴素贝叶斯、决策树等）无法有效处理大规模数据和未知风险，导致检测准确率下降。作者提出利用深度学习（deep learning）进行高级威胁检测，通过收集正常行为数据、提取多维特征并训练模型，来提升异常入侵检测的效能。该方法旨在增强金融网络的弹性，提供更可靠的保护措施，从而维护用户权益和网络完整性。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG",
        "q-fin.GN"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.09820v2",
      "published_date": "2024-02-15 09:35:57 UTC",
      "updated_date": "2024-02-18 11:29:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:44:07.462654"
    },
    {
      "arxiv_id": "2402.09795v1",
      "title": "An advanced data fabric architecture leveraging homomorphic encryption and federated learning",
      "title_zh": "翻译失败",
      "authors": [
        "Sakib Anwar Rieyan",
        "Md. Raisul Kabir News",
        "A. B. M. Muntasir Rahman",
        "Sadia Afrin Khan",
        "Sultan Tasneem Jawad Zaarif",
        "Md. Golam Rabiul Alam",
        "Mohammad Mehedi Hassan",
        "Michele Ianni",
        "Giancarlo Fortino"
      ],
      "abstract": "Data fabric is an automated and AI-driven data fusion approach to accomplish\ndata management unification without moving data to a centralized location for\nsolving complex data problems. In a Federated learning architecture, the global\nmodel is trained based on the learned parameters of several local models that\neliminate the necessity of moving data to a centralized repository for machine\nlearning. This paper introduces a secure approach for medical image analysis\nusing federated learning and partially homomorphic encryption within a\ndistributed data fabric architecture. With this method, multiple parties can\ncollaborate in training a machine-learning model without exchanging raw data\nbut using the learned or fused features. The approach complies with laws and\nregulations such as HIPAA and GDPR, ensuring the privacy and security of the\ndata. The study demonstrates the method's effectiveness through a case study on\npituitary tumor classification, achieving a significant level of accuracy.\nHowever, the primary focus of the study is on the development and evaluation of\nfederated learning and partially homomorphic encryption as tools for secure\nmedical image analysis. The results highlight the potential of these techniques\nto be applied to other privacy-sensitive domains and contribute to the growing\nbody of research on secure and privacy-preserving machine learning.",
      "tldr_zh": "这篇论文提出了一种先进的 data fabric 架构，结合 federated learning 和 partially homomorphic encryption，实现医疗图像分析的安全协作训练，而无需移动或交换原始数据。该方法通过分布式数据融合确保数据隐私，并符合 HIPAA 和 GDPR 等法规要求。在垂体肿瘤分类的案例研究中，该架构取得了显著的准确率，并展示了其在其他隐私敏感领域的潜在应用潜力。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.09795v1",
      "published_date": "2024-02-15 08:50:36 UTC",
      "updated_date": "2024-02-15 08:50:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:44:20.749714"
    },
    {
      "arxiv_id": "2402.09792v1",
      "title": "System-level Impact of Non-Ideal Program-Time of Charge Trap Flash (CTF) on Deep Neural Network",
      "title_zh": "翻译失败",
      "authors": [
        "S. Shrivastava",
        "A. Biswas",
        "S. Chakrabarty",
        "G. Dash",
        "V. Saraswat",
        "U. Ganguly"
      ],
      "abstract": "Learning of deep neural networks (DNN) using Resistive Processing Unit (RPU)\narchitecture is energy-efficient as it utilizes dedicated neuromorphic hardware\nand stochastic computation of weight updates for in-memory computing. Charge\nTrap Flash (CTF) devices can implement RPU-based weight updates in DNNs.\nHowever, prior work has shown that the weight updates (V_T) in CTF-based RPU\nare impacted by the non-ideal program time of CTF. The non-ideal program time\nis affected by two factors of CTF. Firstly, the effects of the number of input\npulses (N) or pulse width (pw), and secondly, the gap between successive update\npulses (t_gap) used for the stochastic computation of weight updates.\nTherefore, the impact of this non-ideal program time must be studied for neural\nnetwork training simulations. In this study, Firstly, we propose a pulse-train\ndesign compensation technique to reduce the total error caused by non-ideal\nprogram time of CTF and stochastic variance of a network. Secondly, we simulate\nRPU-based DNN with non-ideal program time of CTF on MNIST and Fashion-MNIST\ndatasets. We find that for larger N (~1000), learning performance approaches\nthe ideal (software-level) training level and, therefore, is not much impacted\nby the choice of t_gap used to implement RPU-based weight updates. However, for\nlower N (<500), learning performance depends on T_gap of the pulses. Finally,\nwe also performed an ablation study to isolate the causal factor of the\nimproved learning performance. We conclude that the lower noise level in the\nweight updates is the most likely significant factor to improve the learning\nperformance of DNN. Thus, our study attempts to compensate for the error caused\nby non-ideal program time and standardize the pulse length (N) and pulse gap\n(t_gap) specifications for CTF-based RPUs for accurate system-level on-chip\ntraining.",
      "tldr_zh": "这篇论文探讨了 Charge Trap Flash (CTF) 的非理想编程时间对深度神经网络 (DNN) 训练的影响，特别是通过 Resistive Processing Unit (RPU) 架构的权重更新问题，受输入脉冲数量 (N)、脉冲宽度 (pw) 和脉冲间隙 (t_gap) 等因素影响。研究者提出了一种脉冲序列设计补偿技术，以减少非理想编程时间和随机方差带来的错误，并在 MNIST 和 Fashion-MNIST 数据集上模拟了 RPU-based DNN。结果表明，当 N 较大 (~1000) 时，学习性能接近理想水平且不受 t_gap 影响，但当 N 较小 (<500) 时，性能依赖 t_gap；最终，论文通过消融研究确认降低权重更新噪声是改善性能的关键，并标准化了 CTF-based RPU 的脉冲规格。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.ET",
        "eess.IV"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.09792v1",
      "published_date": "2024-02-15 08:47:35 UTC",
      "updated_date": "2024-02-15 08:47:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:44:34.572163"
    },
    {
      "arxiv_id": "2402.09786v4",
      "title": "Examining Pathological Bias in a Generative Adversarial Network Discriminator: A Case Study on a StyleGAN3 Model",
      "title_zh": "翻译失败",
      "authors": [
        "Alvin Grissom II",
        "Ryan F. Lei",
        "Matt Gusdorff",
        "Jeova Farias Sales Rocha Neto",
        "Bailey Lin",
        "Ryan Trotter"
      ],
      "abstract": "Generative adversarial networks (GANs) generate photorealistic faces that are\noften indistinguishable by humans from real faces. While biases in machine\nlearning models are often assumed to be due to biases in training data, we find\npathological internal color and luminance biases in the discriminator of a\npre-trained StyleGAN3-r model that are not explicable by the training data. We\nalso find that the discriminator systematically stratifies scores by both\nimage- and face-level qualities and that this disproportionately affects images\nacross gender, race, and other categories. We examine axes common in research\non stereotyping in social psychology.",
      "tldr_zh": "这篇论文研究了生成对抗网络(GANs)判别器中的病理偏差，通过对预训练StyleGAN3-r模型的案例研究进行分析。研究发现，判别器存在与训练数据无关的内部颜色和亮度偏差，导致它对图像和面部质量进行系统分层评分。结果显示，这种偏差不均等地影响了不同性别、种族和其他类别的图像评分，与社会心理学中刻板印象研究的常见轴相关。该工作揭示了机器学习模型偏差的潜在机制，为减少此类偏见提供了重要启示。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.09786v4",
      "published_date": "2024-02-15 08:34:21 UTC",
      "updated_date": "2024-08-28 16:48:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:44:45.087206"
    },
    {
      "arxiv_id": "2402.09784v2",
      "title": "Sequential Recommendation on Temporal Proximities with Contrastive Learning and Self-Attention",
      "title_zh": "翻译失败",
      "authors": [
        "Hansol Jung",
        "Hyunwoo Seo",
        "Chiehyeon Lim"
      ],
      "abstract": "Sequential recommender systems identify user preferences from their past\ninteractions to predict subsequent items optimally. Although traditional\ndeep-learning-based models and modern transformer-based models in previous\nstudies capture unidirectional and bidirectional patterns within user-item\ninteractions, the importance of temporal contexts, such as individual\nbehavioral and societal trend patterns, remains underexplored. Notably, recent\nmodels often neglect similarities in users' actions that occur implicitly among\nusers during analogous timeframes-a concept we term vertical temporal\nproximity. These models primarily adapt the self-attention mechanisms of the\ntransformer to consider the temporal context in individual user actions.\nMeanwhile, this adaptation still remains limited in considering the horizontal\ntemporal proximity within item interactions, like distinguishing between\nsubsequent item purchases within a week versus a month. To address these gaps,\nwe propose a sequential recommendation model called TemProxRec, which includes\ncontrastive learning and self-attention methods to consider temporal\nproximities both across and within user-item interactions. The proposed\ncontrastive learning method learns representations of items selected in close\ntemporal periods across different users to be close. Simultaneously, the\nproposed self-attention mechanism encodes temporal and positional contexts in a\nuser sequence using both absolute and relative embeddings. This way, our\nTemProxRec accurately predicts the relevant items based on the user-item\ninteractions within a specific timeframe. We validate this work through\ncomprehensive experiments on TemProxRec, consistently outperforming existing\nmodels on benchmark datasets as well as showing the significance of considering\nthe vertical and horizontal temporal proximities into sequential\nrecommendation.",
      "tldr_zh": "该论文探讨了顺序推荐系统（Sequential Recommendation）中时间接近性（Temporal Proximities）的不足，包括垂直时间接近性（跨用户隐式相似行动）和水平时间接近性（用户内互动时间差异）。为了解决这些问题，作者提出了一种新模型TemProxRec，结合contrastive learning来学习跨用户接近时间段的项目表示，以及self-attention机制使用绝对和相对嵌入编码用户序列的时间和位置上下文。实验结果显示，TemProxRec在基准数据集上显著优于现有模型，证明了考虑垂直和水平时间接近性的重要性，能更准确预测用户在特定时间框架内的偏好。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "10 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.09784v2",
      "published_date": "2024-02-15 08:33:16 UTC",
      "updated_date": "2024-02-18 02:38:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:44:57.397770"
    },
    {
      "arxiv_id": "2402.09782v3",
      "title": "MC-DBN: A Deep Belief Network-Based Model for Modality Completion",
      "title_zh": "翻译失败",
      "authors": [
        "Zihong Luo",
        "Zheng Tao",
        "Yuxuan Huang",
        "Kexin He",
        "Chengzhi Liu"
      ],
      "abstract": "Recent advancements in multi-modal artificial intelligence (AI) have\nrevolutionized the fields of stock market forecasting and heart rate\nmonitoring. Utilizing diverse data sources can substantially improve prediction\naccuracy. Nonetheless, additional data may not always align with the original\ndataset. Interpolation methods are commonly utilized for handling missing\nvalues in modal data, though they may exhibit limitations in the context of\nsparse information. Addressing this challenge, we propose a Modality Completion\nDeep Belief Network-Based Model (MC-DBN). This approach utilizes implicit\nfeatures of complete data to compensate for gaps between itself and additional\nincomplete data. It ensures that the enhanced multi-modal data closely aligns\nwith the dynamic nature of the real world to enhance the effectiveness of the\nmodel. We conduct evaluations of the MC-DBN model in two datasets from the\nstock market forecasting and heart rate monitoring domains. Comprehensive\nexperiments showcase the model's capacity to bridge the semantic divide present\nin multi-modal data, subsequently enhancing its performance. The source code is\navailable at: https://github.com/logan-0623/DBN-generate",
      "tldr_zh": "该研究针对多模态 AI 在股票市场预测和心率监测等领域中缺失数据的问题，提出了一种基于 Deep Belief Network (DBN) 的模型 MC-DBN，用于 Modality Completion。MC-DBN 通过利用完整数据的隐含特征来补偿不完整数据间的差距，确保增强的多模态数据更符合真实世界的动态特性。实验在股票市场和心率监测数据集上验证了该模型的有效性，能够桥接多模态数据的语义鸿沟，并显著提升预测性能；源代码可从 https://github.com/logan-0623/DBN-generate 获取。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.09782v3",
      "published_date": "2024-02-15 08:21:50 UTC",
      "updated_date": "2024-03-20 08:50:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:45:08.304907"
    },
    {
      "arxiv_id": "2402.09769v2",
      "title": "Learning Using a Single Forward Pass",
      "title_zh": "翻译失败",
      "authors": [
        "Aditya Somasundaram",
        "Pushkal Mishra",
        "Ayon Borthakur"
      ],
      "abstract": "We propose a learning algorithm to overcome the limitations of a traditional\nbackpropagation in resource-constrained environments: Solo Pass Embedded\nLearning Algorithm (SPELA). SPELA is equipped with rapid learning capabilities\nand operates with local loss functions to update weights, significantly saving\non resources allocated to the propagation of gradients and storing\ncomputational graphs while being sufficiently accurate. Consequently, SPELA can\nclosely match backpropagation with less data, computing, storage, and power.\nMoreover, SPELA can effectively fine-tune pre-trained image recognition models\nfor new tasks. Our results indicate that SPELA can be an ideal candidate for\nlearning in resource-constrained edge AI applications.",
      "tldr_zh": "该论文提出了一种名为 SPELA 的学习算法，利用单个前向传播（Single Forward Pass）来克服传统 backpropagation 在资源受限环境中的局限性，通过本地损失函数更新权重，从而显著节省计算、存储和功率资源，同时保持高准确性。SPELA 能够在更少的数据下与 backpropagation 匹敌，并有效微调预训练的图像识别模型。研究结果显示，SPELA 是资源受限边缘 AI 应用（如 edge AI）的理想候选方案。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.09769v2",
      "published_date": "2024-02-15 07:47:10 UTC",
      "updated_date": "2025-03-10 06:32:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:45:21.172809"
    },
    {
      "arxiv_id": "2402.09766v2",
      "title": "From Variability to Stability: Advancing RecSys Benchmarking Practices",
      "title_zh": "翻译失败",
      "authors": [
        "Valeriy Shevchenko",
        "Nikita Belousov",
        "Alexey Vasilev",
        "Vladimir Zholobov",
        "Artyom Sosedka",
        "Natalia Semenova",
        "Anna Volodkevich",
        "Andrey Savchenko",
        "Alexey Zaytsev"
      ],
      "abstract": "In the rapidly evolving domain of Recommender Systems (RecSys), new\nalgorithms frequently claim state-of-the-art performance based on evaluations\nover a limited set of arbitrarily selected datasets. However, this approach may\nfail to holistically reflect their effectiveness due to the significant impact\nof dataset characteristics on algorithm performance. Addressing this\ndeficiency, this paper introduces a novel benchmarking methodology to\nfacilitate a fair and robust comparison of RecSys algorithms, thereby advancing\nevaluation practices. By utilizing a diverse set of $30$ open datasets,\nincluding two introduced in this work, and evaluating $11$ collaborative\nfiltering algorithms across $9$ metrics, we critically examine the influence of\ndataset characteristics on algorithm performance. We further investigate the\nfeasibility of aggregating outcomes from multiple datasets into a unified\nranking. Through rigorous experimental analysis, we validate the reliability of\nour methodology under the variability of datasets, offering a benchmarking\nstrategy that balances quality and computational demands. This methodology\nenables a fair yet effective means of evaluating RecSys algorithms, providing\nvaluable guidance for future research endeavors.",
      "tldr_zh": "本论文针对推荐系统（RecSys）算法评估中存在的依赖有限数据集的问题，提出了一种新型基准测试方法，以实现更公平和稳健的算法比较。该方法利用30个开放数据集（包括两个新引入数据集），评估11个协同过滤算法在9个指标上的性能，并分析数据集特性对算法表现的影响。通过实验验证，这种方法能够有效聚合多数据集结果形成统一排名，平衡了评估质量与计算需求，为未来RecSys研究提供可靠指导。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "8 pages with 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.09766v2",
      "published_date": "2024-02-15 07:35:52 UTC",
      "updated_date": "2024-08-27 13:01:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:45:31.898583"
    },
    {
      "arxiv_id": "2402.09765v1",
      "title": "Reinforcement Learning for Solving Stochastic Vehicle Routing Problem with Time Windows",
      "title_zh": "翻译失败",
      "authors": [
        "Zangir Iklassov",
        "Ikboljon Sobirov",
        "Ruben Solozabal",
        "Martin Takac"
      ],
      "abstract": "This paper introduces a reinforcement learning approach to optimize the\nStochastic Vehicle Routing Problem with Time Windows (SVRP), focusing on\nreducing travel costs in goods delivery. We develop a novel SVRP formulation\nthat accounts for uncertain travel costs and demands, alongside specific\ncustomer time windows. An attention-based neural network trained through\nreinforcement learning is employed to minimize routing costs. Our approach\naddresses a gap in SVRP research, which traditionally relies on heuristic\nmethods, by leveraging machine learning. The model outperforms the Ant-Colony\nOptimization algorithm, achieving a 1.73% reduction in travel costs. It\nuniquely integrates external information, demonstrating robustness in diverse\nenvironments, making it a valuable benchmark for future SVRP studies and\nindustry application.",
      "tldr_zh": "本论文提出了一种基于强化学习的方法，用于优化随机车辆路径问题（Stochastic Vehicle Routing Problem with Time Windows，SVRP），旨在减少货物交付的旅行成本。该方法开发了新的SVRP公式化，考虑了不确定旅行成本、需求和客户时间窗口，并使用基于注意力的神经网络通过强化学习训练来最小化路由成本。与传统依赖启发式方法的SVRP研究不同，该模型比Ant-Colony Optimization算法表现出色，实现了1.73%的旅行成本降低，并通过整合外部信息展示了在多样环境中的鲁棒性，为未来SVRP研究和行业应用提供了一个宝贵基准。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.09765v1",
      "published_date": "2024-02-15 07:35:29 UTC",
      "updated_date": "2024-02-15 07:35:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:45:44.612140"
    },
    {
      "arxiv_id": "2402.09764v3",
      "title": "Aligning Crowd Feedback via Distributional Preference Reward Modeling",
      "title_zh": "翻译失败",
      "authors": [
        "Dexun Li",
        "Cong Zhang",
        "Kuicai Dong",
        "Derrick Goh Xin Deik",
        "Ruiming Tang",
        "Yong Liu"
      ],
      "abstract": "Deep Reinforcement Learning is widely used for aligning Large Language Models\n(LLM) with human preference. However, the conventional reward modelling is\npredominantly dependent on human annotations provided by a select cohort of\nindividuals. Such dependence may unintentionally result in skewed models that\nreflect the inclinations of these annotators, thereby failing to adequately\nrepresent the wider population's expectations. We propose the Distributional\nPreference Reward Model (DPRM), a simple yet effective framework to align large\nlanguage models with diverse human preferences. To this end, we characterize\nmultiple preferences by a categorical distribution and introduce a Bayesian\nupdater to accommodate shifted or new preferences. On top of that, we design an\noptimal-transportation-based loss to calibrate DPRM to align with the\npreference distribution. Finally, the expected reward is utilized to fine-tune\nan LLM policy to generate responses favoured by the population. Our experiments\nshow that DPRM significantly enhances the alignment of LLMs with population\npreference, yielding more accurate, unbiased, and contextually appropriate\nresponses.",
      "tldr_zh": "该论文针对深度强化学习在对齐大型语言模型（LLM）时依赖少数标注者导致模型偏斜的问题，提出了一种简单有效的框架：Distributional Preference Reward Model (DPRM)。DPRM 通过分类分布表示多种人类偏好，并引入 Bayesian updater 来处理偏好变化，同时设计基于最优传输的损失函数来校准模型，以确保对人群偏好的全面对齐。最终，使用期望奖励微调 LLM 策略，生成更准确、无偏见和上下文合适的响应；实验结果表明，DPRM 显著提升了 LLM 与人群偏好的对齐效果。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.09764v3",
      "published_date": "2024-02-15 07:29:43 UTC",
      "updated_date": "2024-05-30 15:39:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:45:57.477004"
    },
    {
      "arxiv_id": "2402.09760v1",
      "title": "Grounding Language Model with Chunking-Free In-Context Retrieval",
      "title_zh": "翻译失败",
      "authors": [
        "Hongjin Qian",
        "Zheng Liu",
        "Kelong Mao",
        "Yujia Zhou",
        "Zhicheng Dou"
      ],
      "abstract": "This paper presents a novel Chunking-Free In-Context (CFIC) retrieval\napproach, specifically tailored for Retrieval-Augmented Generation (RAG)\nsystems. Traditional RAG systems often struggle with grounding responses using\nprecise evidence text due to the challenges of processing lengthy documents and\nfiltering out irrelevant content. Commonly employed solutions, such as document\nchunking and adapting language models to handle longer contexts, have their\nlimitations. These methods either disrupt the semantic coherence of the text or\nfail to effectively address the issues of noise and inaccuracy in evidence\nretrieval.\n  CFIC addresses these challenges by circumventing the conventional chunking\nprocess. It utilizes the encoded hidden states of documents for in-context\nretrieval, employing auto-aggressive decoding to accurately identify the\nspecific evidence text required for user queries, eliminating the need for\nchunking. CFIC is further enhanced by incorporating two decoding strategies,\nnamely Constrained Sentence Prefix Decoding and Skip Decoding. These strategies\nnot only improve the efficiency of the retrieval process but also ensure that\nthe fidelity of the generated grounding text evidence is maintained. Our\nevaluations of CFIC on a range of open QA datasets demonstrate its superiority\nin retrieving relevant and accurate evidence, offering a significant\nimprovement over traditional methods. By doing away with the need for document\nchunking, CFIC presents a more streamlined, effective, and efficient retrieval\nsolution, making it a valuable advancement in the field of RAG systems.",
      "tldr_zh": "这篇论文提出了一种 Chunking-Free In-Context (CFIC) 检索方法，针对 Retrieval-Augmented Generation (RAG) 系统中的问题，如处理长文档时出现的语义不连贯和证据检索不准确。CFIC 通过利用文档的编码隐藏状态和 auto-aggressive decoding 进行 in-context 检索，绕过了传统的文档分块过程，从而精确识别用户查询所需的证据文本。该方法还引入了 Constrained Sentence Prefix Decoding 和 Skip Decoding 策略，以提升检索效率并确保证据的忠实性。在开放式 QA 数据集上的评估表明，CFIC 比传统方法有显著改进，提供更相关和准确的证据，为 RAG 系统带来了更高效的解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.09760v1",
      "published_date": "2024-02-15 07:22:04 UTC",
      "updated_date": "2024-02-15 07:22:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:46:10.531839"
    },
    {
      "arxiv_id": "2402.09759v1",
      "title": "Efficient Language Adaptive Pre-training: Extending State-of-the-Art Large Language Models for Polish",
      "title_zh": "翻译失败",
      "authors": [
        "Szymon Ruciński"
      ],
      "abstract": "This study explores the potential of fine-tuning foundational English Large\nLanguage Models (LLMs) for generating Polish text. The first step involves\nLanguage Adaptive Pre-training (LAPT) on a high-quality dataset of 3.11 GB,\nconsisting of 276 million Polish tokens. The LAPT is followed by additional\nfine-tuning aimed at solving nine KLEJ challenges. Our trained model\nCurie-7B-v1 not only generates Polish text with the lowest perplexity of 3.02\namong decoder-based Polish models but also closely rivals the performance of\nthe best Polish encoder-decoder models with a less than 2% gap on 8 out of 9\ntasks. Curie-7B-v1 used approximately 2-3% of a typical dataset size to learn\nPolish. The LAPT was completed in less than five days using a consumer GPU,\nhighlighting the method's efficiency. The proficiency of the model in Polish\nwas significantly enhanced, demonstrating the viability of this approach for\nadding new languages to existing LLMs by training just 1.2% of its parameters.\nTo contribute to the community's collaborative progress, the model has been\nreleased as open-source.",
      "tldr_zh": "本文研究了通过 Language Adaptive Pre-training (LAPT) 高效扩展英文 Large Language Models (LLMs) 以生成波兰语文本的方法，涉及在 3.11 GB 高质量数据集（含 2.76 亿波兰语 tokens）上进行预训练，并随后针对九个 KLEJ 挑战进行额外细调。训练出的 Curie-7B-v1 模型在波兰语文本生成中实现了最低 perplexity 为 3.02，并在 8 个任务中与最佳波兰编码器-解码器模型的性能差距不到 2%。该方法仅使用典型数据集的 2-3% 和模型参数的 1.2%，在消费级 GPU 上不到五天完成，展示了高效性和可扩展性，并已作为开源模型发布以推动社区发展。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages",
      "pdf_url": "http://arxiv.org/pdf/2402.09759v1",
      "published_date": "2024-02-15 07:17:10 UTC",
      "updated_date": "2024-02-15 07:17:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:46:23.051828"
    },
    {
      "arxiv_id": "2402.09750v1",
      "title": "Exploring the Potential of Large Language Models in Artistic Creation: Collaboration and Reflection on Creative Programming",
      "title_zh": "翻译失败",
      "authors": [
        "Anqi Wang",
        "Zhizhuo Yin",
        "Yulu Hu",
        "Yuanyuan Mao",
        "Pan Hui"
      ],
      "abstract": "Recently, the potential of large language models (LLMs) has been widely used\nin assisting programming. However, current research does not explore the artist\npotential of LLMs in creative coding within artist and AI collaboration. Our\nwork probes the reflection type of artists in the creation process with such\ncollaboration. We compare two common collaboration approaches: invoking the\nentire program and multiple subtasks. Our findings exhibit artists' different\nstimulated reflections in two different methods. Our finding also shows the\ncorrelation of reflection type with user performance, user satisfaction, and\nsubjective experience in two collaborations through conducting two methods,\nincluding experimental data and qualitative interviews. In this sense, our work\nreveals the artistic potential of LLM in creative coding. Meanwhile, we provide\na critical lens of human-AI collaboration from the artists' perspective and\nexpound design suggestions for future work of AI-assisted creative tasks.",
      "tldr_zh": "本研究探讨了大型语言模型 (LLMs) 在艺术创作中的潜力，特别聚焦于艺术家与 AI 的协作在创意编码中的应用。研究者比较了两种协作方法——调用整个程序与分解为多个子任务——并发现这些方法会激发艺术家不同的反思类型，同时与用户性能、满意度和主观体验密切相关。通过实验数据和定性访谈，研究揭示了 LLMs 在创意编码中的艺术价值，并从艺术家的视角提供人类-AI 协作的批判性见解和未来设计建议。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "J.5"
      ],
      "primary_category": "cs.HC",
      "comment": "15 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.09750v1",
      "published_date": "2024-02-15 07:00:06 UTC",
      "updated_date": "2024-02-15 07:00:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:46:32.667434"
    },
    {
      "arxiv_id": "2402.09748v1",
      "title": "Model Compression and Efficient Inference for Large Language Models: A Survey",
      "title_zh": "大型语言模型的模型压缩和高效推理：综述",
      "authors": [
        "Wenxiao Wang",
        "Wei Chen",
        "Yicong Luo",
        "Yongliu Long",
        "Zhengkai Lin",
        "Liye Zhang",
        "Binbin Lin",
        "Deng Cai",
        "Xiaofei He"
      ],
      "abstract": "Transformer based large language models have achieved tremendous success.\nHowever, the significant memory and computational costs incurred during the\ninference process make it challenging to deploy large models on\nresource-constrained devices. In this paper, we investigate compression and\nefficient inference methods for large language models from an algorithmic\nperspective. Regarding taxonomy, similar to smaller models, compression and\nacceleration algorithms for large language models can still be categorized into\nquantization, pruning, distillation, compact architecture design, dynamic\nnetworks. However, Large language models have two prominent characteristics\ncompared to smaller models: (1) Most of compression algorithms require\nfinetuning or even retraining the model after compression. The most notable\naspect of large models is the very high cost associated with model finetuning\nor training. Therefore, many algorithms for large models, such as quantization\nand pruning, start to explore tuning-free algorithms. (2) Large models\nemphasize versatility and generalization rather than performance on a single\ntask. Hence, many algorithms, such as knowledge distillation, focus on how to\npreserving their versatility and generalization after compression. Since these\ntwo characteristics were not very pronounced in early large models, we further\ndistinguish large language models into medium models and ``real'' large models.\nAdditionally, we also provide an introduction to some mature frameworks for\nefficient inference of large models, which can support basic compression or\nacceleration algorithms, greatly facilitating model deployment for users.",
      "tldr_zh": "这篇论文对大型语言模型（Large Language Models）的压缩和高效推理方法进行了全面调查，旨在解决其高内存和计算成本问题。论文从算法角度将这些方法分类为量化（quantization）、修剪（pruning）、知识蒸馏（distillation）、紧凑架构设计和动态网络，并强调大型模型的独特特性：许多算法需要高成本的微调，因此探索了无调优（tuning-free）方案，同时关注如何在压缩后保留模型的通用性和泛化能力。论文进一步区分了中等模型和“真实”大型模型，并介绍了成熟框架来支持这些算法的部署，从而便于在资源受限设备上应用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.PF"
      ],
      "primary_category": "cs.CL",
      "comment": "47 pages, review 380 papers. The work is ongoing",
      "pdf_url": "http://arxiv.org/pdf/2402.09748v1",
      "published_date": "2024-02-15 06:58:30 UTC",
      "updated_date": "2024-02-15 06:58:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:46:46.224697"
    },
    {
      "arxiv_id": "2402.09746v1",
      "title": "Alpha-GPT 2.0: Human-in-the-Loop AI for Quantitative Investment",
      "title_zh": "翻译失败",
      "authors": [
        "Hang Yuan",
        "Saizhuo Wang",
        "Jian Guo"
      ],
      "abstract": "Recently, we introduced a new paradigm for alpha mining in the realm of\nquantitative investment, developing a new interactive alpha mining system\nframework, Alpha-GPT. This system is centered on iterative Human-AI interaction\nbased on large language models, introducing a Human-in-the-Loop approach to\nalpha discovery. In this paper, we present the next-generation Alpha-GPT 2.0\n\\footnote{Draft. Work in progress}, a quantitative investment framework that\nfurther encompasses crucial modeling and analysis phases in quantitative\ninvestment. This framework emphasizes the iterative, interactive research\nbetween humans and AI, embodying a Human-in-the-Loop strategy throughout the\nentire quantitative investment pipeline. By assimilating the insights of human\nresearchers into the systematic alpha research process, we effectively leverage\nthe Human-in-the-Loop approach, enhancing the efficiency and precision of\nquantitative investment research.",
      "tldr_zh": "本论文提出 Alpha-GPT 2.0，这是一个基于 Human-in-the-Loop AI 的量化投资框架，扩展了之前的 Alpha-GPT 系统，涵盖 alpha 挖掘、建模和分析等关键阶段。该框架强调人类和 AI 的迭代交互，贯穿整个量化投资流程，通过整合人类研究者的洞见来提升研究过程的效率和精度。相比传统方法，Alpha-GPT 2.0 有效提高了 alpha 发现的准确性，为量化投资提供更智能化的支持。",
      "categories": [
        "q-fin.CP",
        "cs.AI"
      ],
      "primary_category": "q-fin.CP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.09746v1",
      "published_date": "2024-02-15 06:52:42 UTC",
      "updated_date": "2024-02-15 06:52:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:46:56.908174"
    },
    {
      "arxiv_id": "2402.12391v2",
      "title": "Toward a Team of AI-made Scientists for Scientific Discovery from Gene Expression Data",
      "title_zh": "翻译失败",
      "authors": [
        "Haoyang Liu",
        "Yijiang Li",
        "Jinglin Jian",
        "Yuxuan Cheng",
        "Jianrong Lu",
        "Shuyi Guo",
        "Jinglei Zhu",
        "Mianchen Zhang",
        "Miantong Zhang",
        "Haohan Wang"
      ],
      "abstract": "Machine learning has emerged as a powerful tool for scientific discovery,\nenabling researchers to extract meaningful insights from complex datasets. For\ninstance, it has facilitated the identification of disease-predictive genes\nfrom gene expression data, significantly advancing healthcare. However, the\ntraditional process for analyzing such datasets demands substantial human\neffort and expertise for the data selection, processing, and analysis. To\naddress this challenge, we introduce a novel framework, a Team of AI-made\nScientists (TAIS), designed to streamline the scientific discovery pipeline.\nTAIS comprises simulated roles, including a project manager, data engineer, and\ndomain expert, each represented by a Large Language Model (LLM). These roles\ncollaborate to replicate the tasks typically performed by data scientists, with\na specific focus on identifying disease-predictive genes. Furthermore, we have\ncurated a benchmark dataset to assess TAIS's effectiveness in gene\nidentification, demonstrating our system's potential to significantly enhance\nthe efficiency and scope of scientific exploration. Our findings represent a\nsolid step towards automating scientific discovery through large language\nmodels.",
      "tldr_zh": "该研究提出了一种名为Team of AI-made Scientists (TAIS)的框架，利用Large Language Model (LLM)模拟角色（如项目经理、数据工程师和领域专家），以自动化从基因表达数据中识别疾病预测基因的过程，从而减少传统分析所需的人力努力。TAIS通过这些角色协作进行数据选择、处理和分析，模拟真实数据科学家的任务。研究者还创建了一个基准数据集来评估TAIS的有效性，结果显示该系统显著提高了科学发现的效率和范围，为通过LLM实现自动化科学探索奠定了基础。",
      "categories": [
        "q-bio.GN",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.GN",
      "comment": "18 pages, 2 figures; added contact",
      "pdf_url": "http://arxiv.org/pdf/2402.12391v2",
      "published_date": "2024-02-15 06:30:12 UTC",
      "updated_date": "2024-02-21 03:42:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:47:09.083861"
    },
    {
      "arxiv_id": "2404.03662v1",
      "title": "X-lifecycle Learning for Cloud Incident Management using LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Drishti Goel",
        "Fiza Husain",
        "Aditya Singh",
        "Supriyo Ghosh",
        "Anjaly Parayil",
        "Chetan Bansal",
        "Xuchao Zhang",
        "Saravan Rajmohan"
      ],
      "abstract": "Incident management for large cloud services is a complex and tedious process\nand requires significant amount of manual efforts from on-call engineers\n(OCEs). OCEs typically leverage data from different stages of the software\ndevelopment lifecycle [SDLC] (e.g., codes, configuration, monitor data, service\nproperties, service dependencies, trouble-shooting documents, etc.) to generate\ninsights for detection, root causing and mitigating of incidents. Recent\nadvancements in large language models [LLMs] (e.g., ChatGPT, GPT-4, Gemini)\ncreated opportunities to automatically generate contextual recommendations to\nthe OCEs assisting them to quickly identify and mitigate critical issues.\nHowever, existing research typically takes a silo-ed view for solving a certain\ntask in incident management by leveraging data from a single stage of SDLC. In\nthis paper, we demonstrate that augmenting additional contextual data from\ndifferent stages of SDLC improves the performance of two critically important\nand practically challenging tasks: (1) automatically generating root cause\nrecommendations for dependency failure related incidents, and (2) identifying\nontology of service monitors used for automatically detecting incidents. By\nleveraging 353 incident and 260 monitor dataset from Microsoft, we demonstrate\nthat augmenting contextual information from different stages of the SDLC\nimproves the performance over State-of-The-Art methods.",
      "tldr_zh": "该论文提出了一种名为X-lifecycle Learning的方法，利用大型语言模型（LLMs）整合软件开发生命周期（SDLC）的多阶段数据（如代码、配置和监控数据），以简化云服务的故障管理过程。该方法针对两个关键任务进行优化：（1）为依赖失败相关的故障自动生成根因推荐，（2）识别服务监控的本体，从而提升故障检测效率。实验基于Microsoft的353个故障和260个监控数据集表明，这种多阶段数据增强策略显著超过了现有State-of-The-Art方法，提高了整体性能。",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.03662v1",
      "published_date": "2024-02-15 06:19:02 UTC",
      "updated_date": "2024-02-15 06:19:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:47:21.310202"
    },
    {
      "arxiv_id": "2402.09734v1",
      "title": "Agents Need Not Know Their Purpose",
      "title_zh": "智能体无需知晓其目的",
      "authors": [
        "Paulo Garcia"
      ],
      "abstract": "Ensuring artificial intelligence behaves in such a way that is aligned with\nhuman values is commonly referred to as the alignment challenge. Prior work has\nshown that rational agents, behaving in such a way that maximizes a utility\nfunction, will inevitably behave in such a way that is not aligned with human\nvalues, especially as their level of intelligence goes up. Prior work has also\nshown that there is no \"one true utility function\"; solutions must include a\nmore holistic approach to alignment. This paper describes oblivious agents:\nagents that are architected in such a way that their effective utility function\nis an aggregation of a known and hidden sub-functions. The hidden component, to\nbe maximized, is internally implemented as a black box, preventing the agent\nfrom examining it. The known component, to be minimized, is knowledge of the\nhidden sub-function. Architectural constraints further influence how agent\nactions can evolve its internal environment model. We show that an oblivious\nagent, behaving rationally, constructs an internal approximation of designers'\nintentions (i.e., infers alignment), and, as a consequence of its architecture\nand effective utility function, behaves in such a way that maximizes alignment;\ni.e., maximizing the approximated intention function. We show that,\nparadoxically, it does this for whatever utility function is used as the hidden\ncomponent and, in contrast with extant techniques, chances of alignment\nactually improve as agent intelligence grows.",
      "tldr_zh": "这篇论文探讨了AI对齐（alignment）挑战，即确保理性代理最大化效用函数时不会偏离人类价值观的问题。作者提出oblivious agents的架构，该代理的效用函数由一个隐藏的黑盒子函数（需最大化）和一个已知子函数（需最小化）组成，通过架构约束使其无法直接检查隐藏部分，从而推断设计师意图。结果表明，这种代理能有效最大化alignment，随着智能水平提升，对齐可能性反而增加，与现有方法形成鲜明对比。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.09734v1",
      "published_date": "2024-02-15 06:15:46 UTC",
      "updated_date": "2024-02-15 06:15:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:47:33.756823"
    },
    {
      "arxiv_id": "2402.09729v1",
      "title": "Federated Prompt-based Decision Transformer for Customized VR Services in Mobile Edge Computing System",
      "title_zh": "翻译失败",
      "authors": [
        "Tailin Zhou",
        "Jiadong Yu",
        "Jun Zhang",
        "Danny H. K. Tsang"
      ],
      "abstract": "This paper investigates resource allocation to provide heterogeneous users\nwith customized virtual reality (VR) services in a mobile edge computing (MEC)\nsystem. We first introduce a quality of experience (QoE) metric to measure user\nexperience, which considers the MEC system's latency, user attention levels,\nand preferred resolutions. Then, a QoE maximization problem is formulated for\nresource allocation to ensure the highest possible user experience,which is\ncast as a reinforcement learning problem, aiming to learn a generalized policy\napplicable across diverse user environments for all MEC servers. To learn the\ngeneralized policy, we propose a framework that employs federated learning (FL)\nand prompt-based sequence modeling to pre-train a common decision model across\nMEC servers, which is named FedPromptDT. Using FL solves the problem of\ninsufficient local MEC data while protecting user privacy during offline\ntraining. The design of prompts integrating user-environment cues and\nuser-preferred allocation improves the model's adaptability to various user\nenvironments during online execution.",
      "tldr_zh": "这篇论文研究了在移动边缘计算 (MEC) 系统为异构用户提供定制化虚拟现实 (VR) 服务的资源分配问题，通过引入 Quality of Experience (QoE) 指标来衡量用户体验，该指标考虑了系统延迟、用户注意力水平和偏好分辨率。论文将 QoE 最大化问题转化为强化学习框架，并提出 FedPromptDT 框架，利用 Federated Learning (FL) 和基于提示的序列建模来预训练一个通用的决策模型。FedPromptDT 通过整合用户环境线索和偏好提示，提高了模型的适应性和隐私保护，并在不同用户环境中实现了有效的资源分配。实验结果表明，该框架能显著提升用户体验，为 MEC 系统中的个性化 VR 服务奠定了基础。",
      "categories": [
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.09729v1",
      "published_date": "2024-02-15 05:56:35 UTC",
      "updated_date": "2024-02-15 05:56:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:47:47.074976"
    },
    {
      "arxiv_id": "2402.09728v1",
      "title": "AbuseGPT: Abuse of Generative AI ChatBots to Create Smishing Campaigns",
      "title_zh": "翻译失败",
      "authors": [
        "Ashfak Md Shibli",
        "Mir Mehedi A. Pritom",
        "Maanak Gupta"
      ],
      "abstract": "SMS phishing, also known as \"smishing\", is a growing threat that tricks users\ninto disclosing private information or clicking into URLs with malicious\ncontent through fraudulent mobile text messages. In recent past, we have also\nobserved a rapid advancement of conversational generative AI chatbot services\n(e.g., OpenAI's ChatGPT, Google's BARD), which are powered by pre-trained large\nlanguage models (LLMs). These AI chatbots certainly have a lot of utilities but\nit is not systematically understood how they can play a role in creating\nthreats and attacks. In this paper, we propose AbuseGPT method to show how the\nexisting generative AI-based chatbot services can be exploited by attackers in\nreal world to create smishing texts and eventually lead to craftier smishing\ncampaigns. To the best of our knowledge, there is no pre-existing work that\nevidently shows the impacts of these generative text-based models on creating\nSMS phishing. Thus, we believe this study is the first of its kind to shed\nlight on this emerging cybersecurity threat. We have found strong empirical\nevidences to show that attackers can exploit ethical standards in the existing\ngenerative AI-based chatbot services by crafting prompt injection attacks to\ncreate newer smishing campaigns. We also discuss some future research\ndirections and guidelines to protect the abuse of generative AI-based services\nand safeguard users from smishing attacks.",
      "tldr_zh": "这篇论文探讨了生成式AI聊天机器人（如ChatGPT和Google's BARD）的滥用问题，提出AbuseGPT方法，展示攻击者如何通过prompt injection attacks绕过AI的道德标准来生成smishing（SMS phishing）文本，从而创建更狡猾的欺诈短信活动。研究首次系统分析了这些AI模型在网络安全威胁中的作用，并提供了实证证据，证明攻击者能成功利用AI服务制造smishing威胁。论文强调了这一新兴风险，并讨论了未来研究方向和防护指南，以保护用户和AI服务。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "6 pages, 12 figures, published in ISDFS 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.09728v1",
      "published_date": "2024-02-15 05:49:22 UTC",
      "updated_date": "2024-02-15 05:49:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:47:59.303776"
    },
    {
      "arxiv_id": "2402.09727v3",
      "title": "A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts",
      "title_zh": "翻译失败",
      "authors": [
        "Kuang-Huei Lee",
        "Xinyun Chen",
        "Hiroki Furuta",
        "John Canny",
        "Ian Fischer"
      ],
      "abstract": "Current Large Language Models (LLMs) are not only limited to some maximum\ncontext length, but also are not able to robustly consume long inputs. To\naddress these limitations, we propose ReadAgent, an LLM agent system that\nincreases effective context length up to 20x in our experiments. Inspired by\nhow humans interactively read long documents, we implement ReadAgent as a\nsimple prompting system that uses the advanced language capabilities of LLMs to\n(1) decide what content to store together in a memory episode, (2) compress\nthose memory episodes into short episodic memories called gist memories, and\n(3) take actions to look up passages in the original text if ReadAgent needs to\nremind itself of relevant details to complete a task. We evaluate ReadAgent\nagainst baselines using retrieval methods, using the original long contexts,\nand using the gist memories. These evaluations are performed on three\nlong-document reading comprehension tasks: QuALITY, NarrativeQA, and QMSum.\nReadAgent outperforms the baselines on all three tasks while extending the\neffective context window by 3.5-20x.",
      "tldr_zh": "该研究提出 ReadAgent，一种受人类阅读启发的LLM代理系统，能够将有效上下文长度扩展至实验中的20倍，解决当前Large Language Models (LLMs)处理长输入的限制。\nReadAgent采用简单提示机制，包括决定记忆片段内容、压缩成简短的gist memories，以及在需要时查找原文本相关细节。\n在QuALITY、NarrativeQA和QMSum三个长文档阅读理解任务上，ReadAgent优于基线模型，并在所有任务中将有效上下文窗口扩展了3.5-20倍。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "Website: https://read-agent.github.io",
      "pdf_url": "http://arxiv.org/pdf/2402.09727v3",
      "published_date": "2024-02-15 05:40:21 UTC",
      "updated_date": "2024-07-22 05:33:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:48:11.971784"
    },
    {
      "arxiv_id": "2402.09725v1",
      "title": "Improving Non-autoregressive Machine Translation with Error Exposure and Consistency Regularization",
      "title_zh": "翻译失败",
      "authors": [
        "Xinran Chen",
        "Sufeng Duan",
        "Gongshen Liu"
      ],
      "abstract": "Being one of the IR-NAT (Iterative-refinemennt-based NAT) frameworks, the\nConditional Masked Language Model (CMLM) adopts the mask-predict paradigm to\nre-predict the masked low-confidence tokens. However, CMLM suffers from the\ndata distribution discrepancy between training and inference, where the\nobserved tokens are generated differently in the two cases. In this paper, we\naddress this problem with the training approaches of error exposure and\nconsistency regularization (EECR). We construct the mixed sequences based on\nmodel prediction during training, and propose to optimize over the masked\ntokens under imperfect observation conditions. We also design a consistency\nlearning method to constrain the data distribution for the masked tokens under\ndifferent observing situations to narrow down the gap between training and\ninference. The experiments on five translation benchmarks obtains an average\nimprovement of 0.68 and 0.40 BLEU scores compared to the base models,\nrespectively, and our CMLMC-EECR achieves the best performance with a\ncomparable translation quality with the Transformer. The experiments results\ndemonstrate the effectiveness of our method.",
      "tldr_zh": "该论文旨在改善非自回归机器翻译（Non-autoregressive Machine Translation）中的Conditional Masked Language Model (CMLM)，通过引入Error Exposure and Consistency Regularization (EECR)来解决训练和推理阶段的数据分布差异问题。EECR方法包括基于模型预测构建混合序列、在不完美观察条件下优化掩盖tokens，以及设计一致性学习（Consistency Learning）来缩小不同观察情况下的数据分布差距。实验在五个翻译基准上显示，该方法比基线模型平均提高了0.68和0.40 BLEU分数，CMLMC-EECR模型的翻译质量与Transformer相当，证明了其有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.09725v1",
      "published_date": "2024-02-15 05:35:04 UTC",
      "updated_date": "2024-02-15 05:35:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:48:22.269710"
    },
    {
      "arxiv_id": "2402.09723v3",
      "title": "Efficient Prompt Optimization Through the Lens of Best Arm Identification",
      "title_zh": "翻译失败",
      "authors": [
        "Chengshuai Shi",
        "Kun Yang",
        "Zihan Chen",
        "Jundong Li",
        "Jing Yang",
        "Cong Shen"
      ],
      "abstract": "The remarkable instruction-following capability of large language models\n(LLMs) has sparked a growing interest in automatically finding good prompts,\ni.e., prompt optimization. Most existing works follow the scheme of selecting\nfrom a pre-generated pool of candidate prompts. However, these designs mainly\nfocus on the generation strategy, while limited attention has been paid to the\nselection method. Especially, the cost incurred during the selection (e.g.,\naccessing LLM and evaluating the responses) is rarely explicitly considered. To\novercome this limitation, this work provides a principled framework, TRIPLE, to\nefficiently perform prompt selection under an explicit budget constraint.\nTRIPLE is built on a novel connection established between prompt optimization\nand fixed-budget best arm identification (BAI-FB) in multi-armed bandits (MAB);\nthus, it is capable of leveraging the rich toolbox from BAI-FB systematically\nand also incorporating unique characteristics of prompt optimization. Extensive\nexperiments on multiple well-adopted tasks using various LLMs demonstrate the\nremarkable performance improvement of TRIPLE over baselines while satisfying\nthe limited budget constraints. As an extension, variants of TRIPLE are\nproposed to efficiently select examples for few-shot prompts, also achieving\nsuperior empirical performance.",
      "tldr_zh": "本文提出TRIPLE框架，将提示优化问题建模为多臂赌博(multi-armed bandits)中的固定预算最佳臂识别(BAI-FB)，以高效选择最佳提示，同时显式考虑选择过程中的成本，如访问LLMs和评估响应。TRIPLE利用BAI-FB的工具并结合提示优化的独特特性，在明确预算约束下进行优化。实验结果显示，在多个任务和各种LLMs上，TRIPLE相对于基线方法显著提高了性能。扩展版本的TRIPLE还用于few-shot提示的示例选择，同样实现了优越的实证表现。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.09723v3",
      "published_date": "2024-02-15 05:31:13 UTC",
      "updated_date": "2024-05-30 19:40:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:48:34.528716"
    },
    {
      "arxiv_id": "2402.09722v1",
      "title": "Reg-NF: Efficient Registration of Implicit Surfaces within Neural Fields",
      "title_zh": "翻译失败",
      "authors": [
        "Stephen Hausler",
        "David Hall",
        "Sutharsan Mahendren",
        "Peyman Moghadam"
      ],
      "abstract": "Neural fields, coordinate-based neural networks, have recently gained\npopularity for implicitly representing a scene. In contrast to classical\nmethods that are based on explicit representations such as point clouds, neural\nfields provide a continuous scene representation able to represent 3D geometry\nand appearance in a way which is compact and ideal for robotics applications.\nHowever, limited prior methods have investigated registering multiple neural\nfields by directly utilising these continuous implicit representations. In this\npaper, we present Reg-NF, a neural fields-based registration that optimises for\nthe relative 6-DoF transformation between two arbitrary neural fields, even if\nthose two fields have different scale factors. Key components of Reg-NF include\na bidirectional registration loss, multi-view surface sampling, and utilisation\nof volumetric signed distance functions (SDFs). We showcase our approach on a\nnew neural field dataset for evaluating registration problems. We provide an\nexhaustive set of experiments and ablation studies to identify the performance\nof our approach, while also discussing limitations to provide future direction\nto the research community on open challenges in utilizing neural fields in\nunconstrained environments.",
      "tldr_zh": "这篇论文提出Reg-NF，一种高效方法，用于注册神经场（neural fields）中的隐式表面，通过优化两个任意神经场之间的相对6-DoF变换，即使它们具有不同的缩放因子。关键组件包括双向注册损失（bidirectional registration loss）、多视图表面采样（multi-view surface sampling）和体素化符号距离函数（volumetric signed distance functions, SDFs），这些元素确保了注册过程的准确性和鲁棒性。实验在新的神经场数据集上进行，展示了Reg-NF的优越性能，并通过消融研究讨论了其局限性，为神经场在机器人应用中的进一步发展提供了方向。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted to ICRA 2024. The first two authors contributed equally",
      "pdf_url": "http://arxiv.org/pdf/2402.09722v1",
      "published_date": "2024-02-15 05:31:03 UTC",
      "updated_date": "2024-02-15 05:31:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:48:46.203744"
    },
    {
      "arxiv_id": "2402.09721v6",
      "title": "Generalized Principal-Agent Problem with a Learning Agent",
      "title_zh": "翻译失败",
      "authors": [
        "Tao Lin",
        "Yiling Chen"
      ],
      "abstract": "Classic principal-agent problems such as Stackelberg games, contract design,\nand Bayesian persuasion, often assume that the agent is able to best respond to\nthe principal's committed strategy. We study repeated generalized\nprincipal-agent problems under the assumption that the principal does not have\ncommitment power and the agent uses algorithms to learn to respond to the\nprincipal. We reduce this problem to a one-shot generalized principal-agent\nproblem where the agent approximately best responds. Using this reduction, we\nshow that: (1) If the agent uses contextual no-regret learning algorithms with\nregret $\\mathrm{Reg}(T)$, then the principal can guarantee utility at least\n$U^* - \\Theta\\big(\\sqrt{\\tfrac{\\mathrm{Reg}(T)}{T}}\\big)$, where $U^*$ is the\nprincipal's optimal utility in the classic model with a best-responding agent.\n(2) If the agent uses contextual no-swap-regret learning algorithms with\nswap-regret $\\mathrm{SReg}(T)$, then the principal cannot obtain utility more\nthan $U^* + O(\\frac{\\mathrm{SReg(T)}}{T})$. But (3) if the agent uses\nmean-based learning algorithms (which can be no-regret but not no-swap-regret),\nthen the principal can sometimes do significantly better than $U^*$. These\nresults not only refine previous results in Stackelberg games and contract\ndesign, but also lead to new results for Bayesian persuasion with a learning\nagent and all generalized principal-agent problems where the agent does not\nhave private information.",
      "tldr_zh": "本研究探讨了广义委托代理问题（Generalized Principal-Agent Problem），假设委托人（Principal）缺乏承诺能力，而代理人（Agent）使用学习算法（如 contextual no-regret 或 no-swap-regret）来响应策略。通过将重复问题简化为单次问题，论文证明：（1）若代理人采用 contextual no-regret 算法，委托人可获得至少 U* 减去 Θ(√(Reg(T)/T)) 的效用，其中 U* 为经典模型中的最优效用；（2）若采用 contextual no-swap-regret 算法，委托人效用上限为 U* 加 O(SReg(T)/T)；（3）若采用 mean-based 算法，委托人有时可显著超越 U*。这些结果完善了 Stackelberg games 和 contract design 的现有理论，并扩展到 Bayesian persuasion 等场景中代理人无私有信息的情况。",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.LG",
        "econ.TH"
      ],
      "primary_category": "cs.GT",
      "comment": "Accepted by ICLR 2025 (spotlight)",
      "pdf_url": "http://arxiv.org/pdf/2402.09721v6",
      "published_date": "2024-02-15 05:30:47 UTC",
      "updated_date": "2025-02-22 06:58:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:48:57.505074"
    },
    {
      "arxiv_id": "2402.09712v2",
      "title": "Diffusion Model with Cross Attention as an Inductive Bias for Disentanglement",
      "title_zh": "扩散模型结合交叉注意力作为解缠",
      "authors": [
        "Tao Yang",
        "Cuiling Lan",
        "Yan Lu",
        "Nanning zheng"
      ],
      "abstract": "Disentangled representation learning strives to extract the intrinsic factors\nwithin observed data. Factorizing these representations in an unsupervised\nmanner is notably challenging and usually requires tailored loss functions or\nspecific structural designs. In this paper, we introduce a new perspective and\nframework, demonstrating that diffusion models with cross-attention can serve\nas a powerful inductive bias to facilitate the learning of disentangled\nrepresentations. We propose to encode an image to a set of concept tokens and\ntreat them as the condition of the latent diffusion for image reconstruction,\nwhere cross-attention over the concept tokens is used to bridge the interaction\nbetween the encoder and diffusion. Without any additional regularization, this\nframework achieves superior disentanglement performance on the benchmark\ndatasets, surpassing all previous methods with intricate designs. We have\nconducted comprehensive ablation studies and visualization analysis, shedding\nlight on the functioning of this model. This is the first work to reveal the\npotent disentanglement capability of diffusion models with cross-attention,\nrequiring no complex designs. We anticipate that our findings will inspire more\ninvestigation on exploring diffusion for disentangled representation learning\ntowards more sophisticated data analysis and understanding.",
      "tldr_zh": "该论文探讨了无监督的Disentangled representation learning，提出了一种新框架，将diffusion models with cross-attention作为inductive bias来促进表示的解缠结学习。具体方法是将图像编码成一组concept tokens，作为latent diffusion的条件，通过cross-attention桥接编码器和扩散模型，实现图像重建，而无需额外正则化。该框架在基准数据集上实现了优越的性能，超越了所有复杂设计的先前方法，并通过全面消融研究和可视化分析证明了其有效性，为未来探索diffusion models在解缠结表示学习中的应用提供了新方向。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.09712v2",
      "published_date": "2024-02-15 05:07:54 UTC",
      "updated_date": "2024-06-12 15:20:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:49:11.125034"
    },
    {
      "arxiv_id": "2402.09695v2",
      "title": "Universal Black-Box Reward Poisoning Attack against Offline Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Yinglun Xu",
        "Rohan Gumaste",
        "Gagandeep Singh"
      ],
      "abstract": "We study the problem of universal black-boxed reward poisoning attacks\nagainst general offline reinforcement learning with deep neural networks. We\nconsider a black-box threat model where the attacker is entirely oblivious to\nthe learning algorithm, and its budget is limited by constraining the amount of\ncorruption at each data point and the total perturbation. We require the attack\nto be universally efficient against any efficient algorithms that might be used\nby the agent. We propose an attack strategy called the `policy contrast\nattack.' The idea is to find low- and high-performing policies covered by the\ndataset and make them appear to be high- and low-performing to the agent,\nrespectively. To the best of our knowledge, we propose the first universal\nblack-box reward poisoning attack in the general offline RL setting. We provide\ntheoretical insights on the attack design and empirically show that our attack\nis efficient against current state-of-the-art offline RL algorithms in\ndifferent learning datasets.",
      "tldr_zh": "本研究探讨了针对离线强化学习（offline reinforcement learning）的通用黑盒奖励投毒攻击（black-box reward poisoning attack），攻击者无需了解学习算法，仅通过有限的数据点篡改来实现。攻击策略名为“policy contrast attack”，其核心是将数据集中的低性能策略伪装成高性能策略，反之亦然，从而对任何高效算法产生影响。实验结果显示，该攻击在多种学习数据集上对当前最先进的离线 RL 算法有效，并提供了理论分析支持。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.09695v2",
      "published_date": "2024-02-15 04:08:49 UTC",
      "updated_date": "2024-10-23 19:31:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:49:23.429854"
    },
    {
      "arxiv_id": "2402.09683v1",
      "title": "Exploring a Behavioral Model of \"Positive Friction\" in Human-AI Interaction",
      "title_zh": "探索“Positive Friction”在人-AI 交互中的行为模型",
      "authors": [
        "Zeya Chen",
        "Ruth Schmidt"
      ],
      "abstract": "Designing seamless, frictionless user experiences has long been a dominant\ntrend in both applied behavioral science and artificial intelligence (AI), in\nwhich the goal of making desirable actions easy and efficient informs efforts\nto minimize friction in user experiences. However, in some settings, friction\ncan be genuinely beneficial, such as the insertion of deliberate delays to\nincrease reflection, preventing individuals from resorting to automatic or\nbiased behaviors, and enhancing opportunities for unexpected discoveries. More\nrecently, the popularization and availability of AI on a widespread scale has\nonly increased the need to examine how friction can help or hinder users of AI;\nit also suggests a need to consider how positive friction can benefit AI\npractitioners, both during development processes (e.g., working with diverse\nteams) and to inform how AI is designed into offerings. This paper first\nproposes a \"positive friction\" model that can help characterize how friction is\ncurrently beneficial in user and developer experiences with AI, diagnose the\npotential need for friction where it may not yet exist in these contexts, and\ninform how positive friction can be used to generate solutions, especially as\nadvances in AI continue to be progress and new opportunities emerge. It then\nexplores this model in the context of AI users and developers by proposing the\nvalue of taking a hybrid \"AI+human\" lens, and concludes by suggesting questions\nfor further exploration.",
      "tldr_zh": "这篇论文探讨了在人机交互（Human-AI Interaction）中“Positive Friction”的行为模型，挑战了传统设计中追求无摩擦用户体验的理念。\n作者提出一个Positive Friction模型，用于分析摩擦在AI用户和开发者体验中的益处，例如通过故意延迟增强反思、减少偏见行为，并促进意外发现。\n该模型帮助诊断现有AI场景中潜在的摩擦需求，并指导解决方案的设计，尤其在AI开发过程中（如团队协作）。\n论文从混合“AI+human”视角进行探索，并提出进一步研究的问题，以适应AI技术的持续进步。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "This preprint has not undergone peer review or any post-submission\n  corrections. The Version of Record of this contribution will be published in\n  Springer Nature Computer Science book series in Volume HCI International 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.09683v1",
      "published_date": "2024-02-15 03:39:55 UTC",
      "updated_date": "2024-02-15 03:39:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:49:36.704800"
    },
    {
      "arxiv_id": "2402.09674v1",
      "title": "PAL: Proxy-Guided Black-Box Attack on Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Chawin Sitawarin",
        "Norman Mu",
        "David Wagner",
        "Alexandre Araujo"
      ],
      "abstract": "Large Language Models (LLMs) have surged in popularity in recent months, but\nthey have demonstrated concerning capabilities to generate harmful content when\nmanipulated. While techniques like safety fine-tuning aim to minimize harmful\nuse, recent works have shown that LLMs remain vulnerable to attacks that elicit\ntoxic responses. In this work, we introduce the Proxy-Guided Attack on LLMs\n(PAL), the first optimization-based attack on LLMs in a black-box query-only\nsetting. In particular, it relies on a surrogate model to guide the\noptimization and a sophisticated loss designed for real-world LLM APIs. Our\nattack achieves 84% attack success rate (ASR) on GPT-3.5-Turbo and 48% on\nLlama-2-7B, compared to 4% for the current state of the art. We also propose\nGCG++, an improvement to the GCG attack that reaches 94% ASR on white-box\nLlama-2-7B, and the Random-Search Attack on LLMs (RAL), a strong but simple\nbaseline for query-based attacks. We believe the techniques proposed in this\nwork will enable more comprehensive safety testing of LLMs and, in the long\nterm, the development of better security guardrails. The code can be found at\nhttps://github.com/chawins/pal.",
      "tldr_zh": "该研究提出PAL（Proxy-Guided Black-Box Attack），一种针对Large Language Models (LLMs)的黑盒查询-only攻击方法，使用代理模型（surrogate model）指导优化并设计特定损失函数，以诱导LLMs生成有害内容。实验结果显示，PAL在GPT-3.5-Turbo上实现84%的攻击成功率（ASR），在Llama-2-7B上达到48%，远超现有方法的4%。此外，论文还引入GCG++改进版（ASR达94%在白盒Llama-2-7B）和RAL作为简单基准，这些技术有助于提升LLMs的安全测试和开发更有效的安全防护机制。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.09674v1",
      "published_date": "2024-02-15 02:54:49 UTC",
      "updated_date": "2024-02-15 02:54:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:49:48.624188"
    },
    {
      "arxiv_id": "2402.09668v1",
      "title": "How to Train Data-Efficient LLMs",
      "title_zh": "如何训练数据高效的大型语言模型",
      "authors": [
        "Noveen Sachdeva",
        "Benjamin Coleman",
        "Wang-Cheng Kang",
        "Jianmo Ni",
        "Lichan Hong",
        "Ed H. Chi",
        "James Caverlee",
        "Julian McAuley",
        "Derek Zhiyuan Cheng"
      ],
      "abstract": "The training of large language models (LLMs) is expensive. In this paper, we\nstudy data-efficient approaches for pre-training LLMs, i.e., techniques that\naim to optimize the Pareto frontier of model quality and training resource/data\nconsumption. We seek to understand the tradeoffs associated with data selection\nroutines based on (i) expensive-to-compute data-quality estimates, and (ii)\nmaximization of coverage and diversity-based measures in the feature space. Our\nfirst technique, Ask-LLM, leverages the zero-shot reasoning capabilities of\ninstruction-tuned LLMs to directly assess the quality of a training example. To\ntarget coverage, we propose Density sampling, which models the data\ndistribution to select a diverse sample. In our comparison of 19 samplers,\ninvolving hundreds of evaluation tasks and pre-training runs, we find that\nAsk-LLM and Density are the best methods in their respective categories.\nCoverage sampling can recover the performance of the full data, while models\ntrained on Ask-LLM data consistently outperform full-data training -- even when\nwe reject 90% of the original dataset, while converging up to 70% faster.",
      "tldr_zh": "这篇论文探讨了数据高效训练大型语言模型 (LLMs) 的方法，旨在优化模型质量与训练资源消耗的帕累托前沿。作者提出了两种关键技术：Ask-LLM，利用指令微调的 LLMs 进行零-shot 推理评估数据质量；以及 Density sampling，通过建模数据分布来选择多样性样本。在比较 19 个采样器后，研究发现 Ask-LLM 和 Density 是各自类别的最佳方法，其中 Ask-LLM 即使拒绝 90% 的原始数据集，也能使模型性能超越全数据训练，并加速收敛高达 70%。这些发现为高效 LLM 预训练提供了实用指导。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Under review. 44 pages, 30 figures",
      "pdf_url": "http://arxiv.org/pdf/2402.09668v1",
      "published_date": "2024-02-15 02:27:57 UTC",
      "updated_date": "2024-02-15 02:27:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:50:00.817031"
    },
    {
      "arxiv_id": "2402.09664v4",
      "title": "CodeMind: A Framework to Challenge Large Language Models for Code Reasoning",
      "title_zh": "CodeMind: 一个框架，用于挑战大语言模型的代码推理",
      "authors": [
        "Changshu Liu",
        "Shizhuo Dylan Zhang",
        "Ali Reza Ibrahimzada",
        "Reyhaneh Jabbarvand"
      ],
      "abstract": "Solely relying on test passing to evaluate Large Language Models (LLMs) for\ncode synthesis may result in unfair assessment or promoting models with data\nleakage. As an alternative, we introduce CodeMind, a framework designed to\ngauge the code reasoning abilities of LLMs. CodeMind currently supports three\ncode reasoning tasks: Independent Execution Reasoning (IER), Dependent\nExecution Reasoning (DER), and Specification Reasoning (SR). The first two\nevaluate models to predict the execution output of an arbitrary code or code\nthe model could correctly synthesize. The third one evaluates the extent to\nwhich LLMs implement the specified expected behavior.\n  Our extensive evaluation of nine LLMs across five benchmarks in two different\nprogramming languages using CodeMind shows that LLMs fairly follow control flow\nconstructs and, in general, explain how inputs evolve to output, specifically\nfor simple programs and the ones they can correctly synthesize. However, their\nperformance drops for code with higher complexity, non-trivial logical and\narithmetic operators, non-primitive types, and API calls. Furthermore, we\nobserve that, while correlated, specification reasoning (essential for code\nsynthesis) does not imply execution reasoning (essential for broader\nprogramming tasks such as testing and debugging): ranking LLMs based on test\npassing can be different compared to code reasoning.",
      "tldr_zh": "本研究引入了 CodeMind 框架，用于评估大型语言模型 (LLMs) 的代码推理能力，以解决仅依赖测试通过评估可能导致的不公平或数据泄露问题。CodeMind 支持三种任务：Independent Execution Reasoning (IER) 用于预测任意代码的执行输出、Dependent Execution Reasoning (DER) 用于预测模型正确合成的代码输出，以及 Specification Reasoning (SR) 用于检查 LLMs 是否实现了指定的预期行为。在对九个 LLMs 在五个基准和两种编程语言的评估中，结果显示 LLMs 在简单程序和可合成的代码上表现较好，但面对复杂代码、非平凡逻辑运算符、非原始类型和 API 调用时性能下降。此外，研究发现，代码推理与测试通过相关但不完全一致，这意味着基于测试通过的排名可能与代码推理能力不符。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.PL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.09664v4",
      "published_date": "2024-02-15 02:24:46 UTC",
      "updated_date": "2024-04-03 06:23:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:50:12.295800"
    },
    {
      "arxiv_id": "2402.09660v2",
      "title": "User Modeling and User Profiling: A Comprehensive Survey",
      "title_zh": "用户建模与用户画像：全面综述",
      "authors": [
        "Erasmo Purificato",
        "Ludovico Boratto",
        "Ernesto William De Luca"
      ],
      "abstract": "The integration of artificial intelligence (AI) into daily life, particularly\nthrough information retrieval and recommender systems, has necessitated\nadvanced user modeling and profiling techniques to deliver personalized\nexperiences. These techniques aim to construct accurate user representations\nbased on the rich amounts of data generated through interactions with these\nsystems. This paper presents a comprehensive survey of the current state,\nevolution, and future directions of user modeling and profiling research. We\nprovide a historical overview, tracing the development from early stereotype\nmodels to the latest deep learning techniques, and propose a novel taxonomy\nthat encompasses all active topics in this research area, including recent\ntrends. Our survey highlights the paradigm shifts towards more sophisticated\nuser profiling methods, emphasizing implicit data collection, multi-behavior\nmodeling, and the integration of graph data structures. We also address the\ncritical need for privacy-preserving techniques and the push towards\nexplainability and fairness in user modeling approaches. By examining the\ndefinitions of core terminology, we aim to clarify ambiguities and foster a\nclearer understanding of the field by proposing two novel encyclopedic\ndefinitions of the main terms. Furthermore, we explore the application of user\nmodeling in various domains, such as fake news detection, cybersecurity, and\npersonalized education. This survey serves as a comprehensive resource for\nresearchers and practitioners, offering insights into the evolution of user\nmodeling and profiling and guiding the development of more personalized,\nethical, and effective AI systems.",
      "tldr_zh": "这篇论文对User Modeling和User Profiling进行了全面调查，回顾了这些技术的历史演变，从早期刻板印象模型到当前的深度学习方法，并提出一个新分类法来涵盖隐式数据收集、多行为建模和图数据结构的最新趋势。作者强调了隐私保护、可解释性和公平性的重要性，并为核心术语提供了新定义，以澄清领域 ambiguities。最终，该调查探讨了这些技术在假新闻检测、网络安全和个性化教育等领域的应用，为研究者和从业者开发更个性化的、道德的AI系统提供了宝贵指导。",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.IR",
        "cs.LG",
        "cs.SI",
        "I.2"
      ],
      "primary_category": "cs.AI",
      "comment": "71 pages",
      "pdf_url": "http://arxiv.org/pdf/2402.09660v2",
      "published_date": "2024-02-15 02:06:06 UTC",
      "updated_date": "2024-02-20 23:43:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:50:24.500926"
    },
    {
      "arxiv_id": "2403.03222v1",
      "title": "Knowledge-guided EEG Representation Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Aditya Kommineni",
        "Kleanthis Avramidis",
        "Richard Leahy",
        "Shrikanth Narayanan"
      ],
      "abstract": "Self-supervised learning has produced impressive results in multimedia\ndomains of audio, vision and speech. This paradigm is equally, if not more,\nrelevant for the domain of biosignals, owing to the scarcity of labelled data\nin such scenarios. The ability to leverage large-scale unlabelled data to learn\nrobust representations could help improve the performance of numerous inference\ntasks on biosignals. Given the inherent domain differences between multimedia\nmodalities and biosignals, the established objectives for self-supervised\nlearning may not translate well to this domain. Hence, there is an unmet need\nto adapt these methods to biosignal analysis. In this work we propose a\nself-supervised model for EEG, which provides robust performance and remarkable\nparameter efficiency by using state space-based deep learning architecture. We\nalso propose a novel knowledge-guided pre-training objective that accounts for\nthe idiosyncrasies of the EEG signal. The results indicate improved embedding\nrepresentation learning and downstream performance compared to prior works on\nexemplary tasks. Also, the proposed objective significantly reduces the amount\nof pre-training data required to obtain performance equivalent to prior works.",
      "tldr_zh": "本研究提出了一种知识-guided EEG 表示学习方法，利用自监督学习来应对 EEG 信号领域标注数据稀缺的挑战。方法基于状态空间-based 深度学习架构，并引入了一个新颖的知识-guided 预训练目标，以适应 EEG 信号的独特特性。实验结果显示，该模型在下游任务上比先前工作提供了更稳健的性能和嵌入表示学习，同时显著减少了预训练数据量。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "6 Pages, 5 figures, Submitted to EMBC 2024",
      "pdf_url": "http://arxiv.org/pdf/2403.03222v1",
      "published_date": "2024-02-15 01:52:44 UTC",
      "updated_date": "2024-02-15 01:52:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:50:35.328906"
    },
    {
      "arxiv_id": "2402.09656v4",
      "title": "The Butterfly Effect of Model Editing: Few Edits Can Trigger Large Language Models Collapse",
      "title_zh": "翻译失败",
      "authors": [
        "Wanli Yang",
        "Fei Sun",
        "Xinyu Ma",
        "Xun Liu",
        "Dawei Yin",
        "Xueqi Cheng"
      ],
      "abstract": "Although model editing has shown promise in revising knowledge in Large\nLanguage Models (LLMs), its impact on the inherent capabilities of LLMs is\noften overlooked. In this work, we reveal a critical phenomenon: even a single\nedit can trigger model collapse, manifesting as significant performance\ndegradation in various benchmark tasks. However, benchmarking LLMs after each\nedit, while necessary to prevent such collapses, is impractically\ntime-consuming and resource-intensive. To mitigate this, we propose using\nperplexity as a surrogate metric, validated by extensive experiments\ndemonstrating changes in an edited model's perplexity are strongly correlated\nwith its downstream task performances. We further conduct an in-depth study on\nsequential editing, a practical setting for real-world scenarios, across\nvarious editing methods and LLMs, focusing on hard cases from our previous\nsingle edit studies. The results indicate that nearly all examined editing\nmethods result in model collapse after only few edits. To facilitate further\nresearch, we have utilized GPT-3.5 to develop a new dataset, HardEdit, based on\nthose hard cases. This dataset aims to establish the foundation for pioneering\nresearch in reliable model editing and the mechanisms underlying\nediting-induced model collapse. We hope this work can draw the community's\nattention to the potential risks inherent in model editing practices.",
      "tldr_zh": "本研究揭示了模型编辑对Large Language Models (LLMs) 的潜在风险：即使少数编辑（如单个编辑）就能引发模型崩溃，导致各种基准任务性能显著下降。作者提出使用perplexity作为替代指标来监控编辑影响，并通过广泛实验验证其与下游任务性能的高度相关性。在顺序编辑场景下，多种编辑方法在少数操作后均导致崩溃，为此开发了HardEdit数据集，以促进可靠模型编辑机制的研究和社区关注。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at Findings of ACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2402.09656v4",
      "published_date": "2024-02-15 01:50:38 UTC",
      "updated_date": "2024-06-05 09:43:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:50:48.412959"
    },
    {
      "arxiv_id": "2402.09654v2",
      "title": "GPT-4's assessment of its performance in a USMLE-based case study",
      "title_zh": "翻译失败",
      "authors": [
        "Uttam Dhakal",
        "Aniket Kumar Singh",
        "Suman Devkota",
        "Yogesh Sapkota",
        "Bishal Lamichhane",
        "Suprinsa Paudyal",
        "Chandra Dhakal"
      ],
      "abstract": "This study investigates GPT-4's assessment of its performance in healthcare\napplications. A simple prompting technique was used to prompt the LLM with\nquestions taken from the United States Medical Licensing Examination (USMLE)\nquestionnaire and it was tasked to evaluate its confidence score before posing\nthe question and after asking the question. The questionnaire was categorized\ninto two groups-questions with feedback (WF) and questions with no feedback(NF)\npost-question. The model was asked to provide absolute and relative confidence\nscores before and after each question. The experimental findings were analyzed\nusing statistical tools to study the variability of confidence in WF and NF\ngroups. Additionally, a sequential analysis was conducted to observe the\nperformance variation for the WF and NF groups. Results indicate that feedback\ninfluences relative confidence but doesn't consistently increase or decrease\nit. Understanding the performance of LLM is paramount in exploring its utility\nin sensitive areas like healthcare. This study contributes to the ongoing\ndiscourse on the reliability of AI, particularly of LLMs like GPT-4, within\nhealthcare, offering insights into how feedback mechanisms might be optimized\nto enhance AI-assisted medical education and decision support.",
      "tldr_zh": "本研究探讨了GPT-4在医疗应用中的性能评估，采用简单提示技术让模型针对USMLE问题评估其自信度，包括问题前后的绝对和相对confidence scores，并将问题分为有反馈(WF)和无反馈(NF)组。研究使用统计工具分析自信度的变异性和顺序性能差异，结果显示反馈会影响相对confidence scores，但不一致地增加或减少它。通过此研究，揭示了LLM如GPT-4在医疗领域的可靠性问题，并为优化feedback mechanisms以提升AI辅助医疗教育和决策支持提供了洞见。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.HC",
        "cs.MA",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.09654v2",
      "published_date": "2024-02-15 01:38:50 UTC",
      "updated_date": "2024-03-26 20:12:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:50:58.959696"
    },
    {
      "arxiv_id": "2402.09649v2",
      "title": "ProtChatGPT: Towards Understanding Proteins with Large Language Models",
      "title_zh": "ProtChatGPT：利用大型语言模型理解蛋白质",
      "authors": [
        "Chao Wang",
        "Hehe Fan",
        "Ruijie Quan",
        "Yi Yang"
      ],
      "abstract": "Protein research is crucial in various fundamental disciplines, but\nunderstanding their intricate structure-function relationships remains\nchallenging. Recent Large Language Models (LLMs) have made significant strides\nin comprehending task-specific knowledge, suggesting the potential for\nChatGPT-like systems specialized in protein to facilitate basic research. In\nthis work, we introduce ProtChatGPT, which aims at learning and understanding\nprotein structures via natural languages. ProtChatGPT enables users to upload\nproteins, ask questions, and engage in interactive conversations to produce\ncomprehensive answers. The system comprises protein encoders, a\nProtein-Language Pertaining Transformer (PLP-former), a projection adapter, and\nan LLM. The protein first undergoes protein encoders and PLP-former to produce\nprotein embeddings, which are then projected by the adapter to conform with the\nLLM. The LLM finally combines user questions with projected embeddings to\ngenerate informative answers. Experiments show that ProtChatGPT can produce\npromising responses to proteins and their corresponding questions. We hope that\nProtChatGPT could form the basis for further exploration and application in\nprotein research. Code and our pre-trained model will be publicly available.",
      "tldr_zh": "本文提出 ProtChatGPT，一种基于 Large Language Models (LLMs) 的系统，旨在通过自然语言帮助理解蛋白质的复杂结构-功能关系。系统由蛋白编码器、Protein-Language Pertaining Transformer (PLP-former)、projection adapter 和 LLM 组成，用户可以上传蛋白质、提问并获得交互式答案。实验结果显示，ProtChatGPT 能产生有前景的响应，为蛋白质基础研究提供新工具，并计划公开代码和预训练模型。",
      "categories": [
        "cs.CE",
        "cs.AI",
        "q-bio.BM"
      ],
      "primary_category": "cs.CE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2402.09649v2",
      "published_date": "2024-02-15 01:22:30 UTC",
      "updated_date": "2025-01-23 06:30:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T06:51:12.229997"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 110,
  "processed_papers_count": 110,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-17T06:51:42.535266"
}
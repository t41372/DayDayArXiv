[
  {
    "arxiv_id": "2411.10629v1",
    "title": "Leveraging large language models for efficient representation learning for entity resolution",
    "authors": [
      "Xiaowei Xu",
      "Bi T. Foua",
      "Xingqiao Wang",
      "Vivek Gunasekaran",
      "John R. Talburt"
    ],
    "abstract": "In this paper, the authors propose TriBERTa, a supervised entity resolution\nsystem that utilizes a pre-trained large language model and a triplet loss\nfunction to learn representations for entity matching. The system consists of\ntwo steps: first, name entity records are fed into a Sentence Bidirectional\nEncoder Representations from Transformers (SBERT) model to generate vector\nrepresentations, which are then fine-tuned using contrastive learning based on\na triplet loss function. Fine-tuned representations are used as input for\nentity matching tasks, and the results show that the proposed approach\noutperforms state-of-the-art representations, including SBERT without\nfine-tuning and conventional Term Frequency-Inverse Document Frequency\n(TF-IDF), by a margin of 3 - 19%. Additionally, the representations generated\nby TriBERTa demonstrated increased robustness, maintaining consistently higher\nperformance across a range of datasets. The authors also discussed the\nimportance of entity resolution in today's data-driven landscape and the\nchallenges that arise when identifying and reconciling duplicate data across\ndifferent sources. They also described the ER process, which involves several\ncrucial steps, including blocking, entity matching, and clustering.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "22 pages and 12 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.10629v1",
    "published_date": "2024-11-15 23:24:07 UTC",
    "updated_date": "2024-11-15 23:24:07 UTC"
  },
  {
    "arxiv_id": "2411.10627v1",
    "title": "Is thermography a viable solution for detecting pressure injuries in dark skin patients?",
    "authors": [
      "Miriam Asare-Baiden",
      "Kathleen Jordan",
      "Andrew Chung",
      "Sharon Eve Sonenblum",
      "Joyce C. Ho"
    ],
    "abstract": "Pressure injury (PI) detection is challenging, especially in dark skin tones,\ndue to the unreliability of visual inspection. Thermography has been suggested\nas a viable alternative as temperature differences in the skin can indicate\nimpending tissue damage. Although deep learning models have demonstrated\nconsiderable promise toward reliably detecting PI, the existing work fails to\nevaluate the performance on darker skin tones and varying data collection\nprotocols. In this paper, we introduce a new thermal and optical imaging\ndataset of 35 participants focused on darker skin tones where temperature\ndifferences are induced through cooling and cupping protocols. We vary the\nimage collection process to include different cameras, lighting, patient pose,\nand camera distance. We compare the performance of a small convolutional neural\nnetwork (CNN) trained on either the thermal or the optical images on all skin\ntones. Our preliminary results suggest that thermography-based CNN is robust to\ndata collection protocols for all skin tones.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages",
    "pdf_url": "http://arxiv.org/pdf/2411.10627v1",
    "published_date": "2024-11-15 23:22:21 UTC",
    "updated_date": "2024-11-15 23:22:21 UTC"
  },
  {
    "arxiv_id": "2411.10624v1",
    "title": "Weak Permission is not Well-Founded, Grounded and Stable",
    "authors": [
      "Guido Governatori"
    ],
    "abstract": "We consider the notion of weak permission as the failure to conclude that the\nopposite obligation. We investigate the issue from the point of non-monotonic\nreasoning, specifically logic programming and structured argumentation, and we\nshow that it is not possible to capture weak permission in the presence of\ndeontic conflicts under the well-founded, grounded and (sceptical) stable\nsemantics.",
    "categories": [
      "cs.LO",
      "cs.AI"
    ],
    "primary_category": "cs.LO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.10624v1",
    "published_date": "2024-11-15 23:14:30 UTC",
    "updated_date": "2024-11-15 23:14:30 UTC"
  },
  {
    "arxiv_id": "2411.10617v1",
    "title": "Attraction-Repulsion Swarming: A Generalized Framework of t-SNE via Force Normalization and Tunable Interactions",
    "authors": [
      "Jingcheng Lu",
      "Jeff Calder"
    ],
    "abstract": "We propose a new method for data visualization based on attraction-repulsion\nswarming (ARS) dynamics, which we call ARS visualization. ARS is a generalized\nframework that is based on viewing the t-distributed stochastic neighbor\nembedding (t-SNE) visualization technique as a swarm of interacting agents\ndriven by attraction and repulsion. Motivated by recent developments in\nswarming, we modify the t-SNE dynamics to include a normalization by the\n\\emph{total influence}, which results in better posed dynamics in which we can\nuse a data size independent time step (of $h=1$) and a simple iteration,\nwithout the need for the array of optimization tricks employed in t-SNE. ARS\nalso includes the ability to separately tune the attraction and repulsion\nkernels, which gives the user control over the tightness within clusters and\nthe spacing between them in the visualization.\n  In contrast with t-SNE, our proposed ARS data visualization method is not\ngradient descent on the Kullback-Leibler divergence, and can be viewed solely\nas an interacting particle system driven by attraction and repulsion forces. We\nprovide theoretical results illustrating how the choice of interaction kernel\naffects the dynamics, and experimental results to validate our method and\ncompare to t-SNE on the MNIST and Cifar-10 data sets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NA",
      "math.CA",
      "math.DS",
      "math.NA",
      "stat.ML",
      "68T09, 65M06, 35Q70"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.10617v1",
    "published_date": "2024-11-15 22:42:11 UTC",
    "updated_date": "2024-11-15 22:42:11 UTC"
  },
  {
    "arxiv_id": "2411.10613v1",
    "title": "Being Considerate as a Pathway Towards Pluralistic Alignment for Agentic AI",
    "authors": [
      "Parand A. Alamdari",
      "Toryn Q. Klassen",
      "Rodrigo Toro Icarte",
      "Sheila A. McIlraith"
    ],
    "abstract": "Pluralistic alignment is concerned with ensuring that an AI system's\nobjectives and behaviors are in harmony with the diversity of human values and\nperspectives. In this paper we study the notion of pluralistic alignment in the\ncontext of agentic AI, and in particular in the context of an agent that is\ntrying to learn a policy in a manner that is mindful of the values and\nperspective of others in the environment. To this end, we show how being\nconsiderate of the future wellbeing and agency of other (human) agents can\npromote a form of pluralistic alignment.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Pluralistic Alignment Workshop at NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.10613v1",
    "published_date": "2024-11-15 22:34:09 UTC",
    "updated_date": "2024-11-15 22:34:09 UTC"
  },
  {
    "arxiv_id": "2411.10606v1",
    "title": "AmoebaLLM: Constructing Any-Shape Large Language Models for Efficient and Instant Deployment",
    "authors": [
      "Yonggan Fu",
      "Zhongzhi Yu",
      "Junwei Li",
      "Jiayi Qian",
      "Yongan Zhang",
      "Xiangchi Yuan",
      "Dachuan Shi",
      "Roman Yakunin",
      "Yingyan Celine Lin"
    ],
    "abstract": "Motivated by the transformative capabilities of large language models (LLMs)\nacross various natural language tasks, there has been a growing demand to\ndeploy these models effectively across diverse real-world applications and\nplatforms. However, the challenge of efficiently deploying LLMs has become\nincreasingly pronounced due to the varying application-specific performance\nrequirements and the rapid evolution of computational platforms, which feature\ndiverse resource constraints and deployment flows. These varying requirements\nnecessitate LLMs that can adapt their structures (depth and width) for optimal\nefficiency across different platforms and application specifications. To\naddress this critical gap, we propose AmoebaLLM, a novel framework designed to\nenable the instant derivation of LLM subnets of arbitrary shapes, which achieve\nthe accuracy-efficiency frontier and can be extracted immediately after a\none-time fine-tuning. In this way, AmoebaLLM significantly facilitates rapid\ndeployment tailored to various platforms and applications. Specifically,\nAmoebaLLM integrates three innovative components: (1) a knowledge-preserving\nsubnet selection strategy that features a dynamic-programming approach for\ndepth shrinking and an importance-driven method for width shrinking; (2) a\nshape-aware mixture of LoRAs to mitigate gradient conflicts among subnets\nduring fine-tuning; and (3) an in-place distillation scheme with loss-magnitude\nbalancing as the fine-tuning objective. Extensive experiments validate that\nAmoebaLLM not only sets new standards in LLM adaptability but also successfully\ndelivers subnets that achieve state-of-the-art trade-offs between accuracy and\nefficiency.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.10606v1",
    "published_date": "2024-11-15 22:02:28 UTC",
    "updated_date": "2024-11-15 22:02:28 UTC"
  },
  {
    "arxiv_id": "2411.10599v1",
    "title": "Generating Energy-efficient code with LLMs",
    "authors": [
      "Tom Cappendijk",
      "Pepijn de Reus",
      "Ana Oprescu"
    ],
    "abstract": "The increasing electricity demands of personal computers, communication\nnetworks, and data centers contribute to higher atmospheric greenhouse gas\nemissions, which in turn lead to global warming and climate change. Therefore\nthe energy consumption of code must be minimized. Code can be generated by\nlarge language models. We look at the influence of prompt modification on the\nenergy consumption of the code generated. We use three different Python code\nproblems of varying difficulty levels. Prompt modification is done by adding\nthe sentence ``Give me an energy-optimized solution for this problem'' or by\nusing two Python coding best practices. The large language models used are\nCodeLlama-70b, CodeLlama-70b-Instruct, CodeLlama-70b-Python,\nDeepSeek-Coder-33b-base, and DeepSeek-Coder-33b-instruct. We find a decrease in\nenergy consumption for a specific combination of prompt optimization, LLM, and\nPython code problem. However, no single optimization prompt consistently\ndecreases energy consumption for the same LLM across the different Python code\nproblems.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.10599v1",
    "published_date": "2024-11-15 21:45:58 UTC",
    "updated_date": "2024-11-15 21:45:58 UTC"
  },
  {
    "arxiv_id": "2411.10596v1",
    "title": "A minimalistic representation model for head direction system",
    "authors": [
      "Minglu Zhao",
      "Dehong Xu",
      "Deqian Kong",
      "Wen-Hao Zhang",
      "Ying Nian Wu"
    ],
    "abstract": "We present a minimalistic representation model for the head direction (HD)\nsystem, aiming to learn a high-dimensional representation of head direction\nthat captures essential properties of HD cells. Our model is a representation\nof rotation group $U(1)$, and we study both the fully connected version and\nconvolutional version. We demonstrate the emergence of Gaussian-like tuning\nprofiles and a 2D circle geometry in both versions of the model. We also\ndemonstrate that the learned model is capable of accurate path integration.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "q-bio.NC",
    "comment": "Workshop on Symmetry and Geometry in Neural Representations\n  (NeurReps) at NeurIPS 2024, Extended Abstract Track",
    "pdf_url": "http://arxiv.org/pdf/2411.10596v1",
    "published_date": "2024-11-15 21:38:33 UTC",
    "updated_date": "2024-11-15 21:38:33 UTC"
  },
  {
    "arxiv_id": "2411.12758v1",
    "title": "An exploration of the effect of quantisation on energy consumption and inference time of StarCoder2",
    "authors": [
      "Pepijn de Reus",
      "Ana Oprescu",
      "Jelle Zuidema"
    ],
    "abstract": "This study examines quantisation and pruning strategies to reduce energy\nconsumption in code Large Language Models (LLMs) inference. Using StarCoder2,\nwe observe increased energy demands with quantization due to lower throughput\nand some accuracy losses. Conversely, pruning reduces energy usage but impairs\nperformance. The results highlight challenges and trade-offs in LLM model\ncompression. We suggest future work on hardware-optimized quantization to\nenhance efficiency with minimal loss in accuracy.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12758v1",
    "published_date": "2024-11-15 21:28:19 UTC",
    "updated_date": "2024-11-15 21:28:19 UTC"
  },
  {
    "arxiv_id": "2411.10588v3",
    "title": "A dataset of questions on decision-theoretic reasoning in Newcomb-like problems",
    "authors": [
      "Caspar Oesterheld",
      "Emery Cooper",
      "Miles Kodama",
      "Linh Chi Nguyen",
      "Ethan Perez"
    ],
    "abstract": "We introduce a dataset of natural-language questions in the decision theory\nof so-called Newcomb-like problems. Newcomb-like problems include, for\ninstance, decision problems in which an agent interacts with a similar other\nagent, and thus has to reason about the fact that the other agent will likely\nreason in similar ways. Evaluating LLM reasoning about Newcomb-like problems is\nimportant because interactions between foundation-model-based agents will often\nbe Newcomb-like. Some ways of reasoning about Newcomb-like problems may allow\nfor greater cooperation between models.\n  Our dataset contains both capabilities questions (i.e., questions with a\nunique, uncontroversially correct answer) and attitude questions (i.e.,\nquestions about which decision theorists would disagree). We use our dataset\nfor an investigation of decision-theoretical capabilities and expressed\nattitudes and their interplay in existing models (different models by OpenAI,\nAnthropic, Meta, GDM, Reka, etc.), as well as models under simple prompt-based\ninterventions. We find, among other things, that attitudes vary significantly\nbetween existing models; that high capabilities are associated with attitudes\nmore favorable toward so-called evidential decision theory; and that attitudes\nare consistent across different types of questions.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "48 pages, 15 figures; code and data at\n  https://github.com/casparoe/newcomblike_questions_dataset; corrected error in\n  funding acknowledgments",
    "pdf_url": "http://arxiv.org/pdf/2411.10588v3",
    "published_date": "2024-11-15 21:19:04 UTC",
    "updated_date": "2024-12-15 20:39:07 UTC"
  },
  {
    "arxiv_id": "2411.10581v1",
    "title": "On the Shortcut Learning in Multilingual Neural Machine Translation",
    "authors": [
      "Wenxuan Wang",
      "Wenxiang Jiao",
      "Jen-tse Huang",
      "Zhaopeng Tu",
      "Michael R. Lyu"
    ],
    "abstract": "In this study, we revisit the commonly-cited off-target issue in multilingual\nneural machine translation (MNMT). By carefully designing experiments on\ndifferent MNMT scenarios and models, we attribute the off-target issue to the\noverfitting of the shortcuts of (non-centric, centric) language mappings.\nSpecifically, the learned shortcuts biases MNMT to mistakenly translate\nnon-centric languages into the centric language instead of the expected\nnon-centric language for zero-shot translation. Analyses on learning dynamics\nshow that the shortcut learning generally occurs in the later stage of model\ntraining, and multilingual pretraining accelerates and aggravates the shortcut\nlearning. Based on these observations, we propose a simple and effective\ntraining strategy to eliminate the shortcuts in MNMT models by leveraging the\nforgetting nature of model training. The only difference from the standard\ntraining is that we remove the training instances that may induce the shortcut\nlearning in the later stage of model training. Without introducing any\nadditional data and computational costs, our approach can consistently and\nsignificantly improve the zero-shot translation performance by alleviating the\nshortcut learning for different MNMT models and benchmarks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by Neurocomputing 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.10581v1",
    "published_date": "2024-11-15 21:09:36 UTC",
    "updated_date": "2024-11-15 21:09:36 UTC"
  },
  {
    "arxiv_id": "2411.12591v1",
    "title": "Thinking Before Looking: Improving Multimodal LLM Reasoning via Mitigating Visual Hallucination",
    "authors": [
      "Haojie Zheng",
      "Tianyang Xu",
      "Hanchi Sun",
      "Shu Pu",
      "Ruoxi Chen",
      "Lichao Sun"
    ],
    "abstract": "Multimodal large language models (MLLMs) have advanced the integration of\nvisual and linguistic modalities, establishing themselves as the dominant\nparadigm for visual-language tasks. Current approaches like chain of thought\n(CoT) reasoning have augmented the cognitive capabilities of large language\nmodels (LLMs), yet their adaptation to MLLMs is hindered by heightened risks of\nhallucination in cross-modality comprehension. In this paper, we find that the\nthinking while looking paradigm in current multimodal CoT approaches--where\nreasoning chains are generated alongside visual input--fails to mitigate\nhallucinations caused by misleading images. To address these limitations, we\npropose the Visual Inference Chain (VIC) framework, a novel approach that\nconstructs reasoning chains using textual context alone before introducing\nvisual input, effectively reducing cross-modal biases and enhancing multimodal\nreasoning accuracy. Comprehensive evaluations demonstrate that VIC\nsignificantly improves zero-shot performance across various vision-related\ntasks, mitigating hallucinations while refining the reasoning capabilities of\nMLLMs. Our code repository can be found at\nhttps://github.com/Terry-Xu-666/visual_inference_chain.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12591v1",
    "published_date": "2024-11-15 21:01:37 UTC",
    "updated_date": "2024-11-15 21:01:37 UTC"
  },
  {
    "arxiv_id": "2411.14456v1",
    "title": "Can Artificial Intelligence Generate Quality Research Topics Reflecting Patient Concerns?",
    "authors": [
      "Jiyeong Kim",
      "Michael L. Chen",
      "Shawheen J. Rezaei",
      "Mariana Ramirez-Posada",
      "Jennifer L. Caswell-Jin",
      "Allison W. Kurian",
      "Fauzia Riaz",
      "Kavita Y. Sarin",
      "Jean Y. Tang",
      "Steven M. Asch",
      "Eleni Linos"
    ],
    "abstract": "Patient-centered research is increasingly important in narrowing the gap\nbetween research and patient care, yet incorporating patient perspectives into\nhealth research has been inconsistent. We propose an automated framework\nleveraging innovative natural language processing (NLP) and artificial\nintelligence (AI) with patient portal messages to generate research ideas that\nprioritize important patient issues. We further quantified the quality of\nAI-generated research topics. To define patient clinical concerns, we analyzed\n614,464 patient messages from 25,549 individuals with breast or skin cancer\nobtained from a large academic hospital (2013 to 2024), constructing a 2-staged\nunsupervised NLP topic model. Then, we generated research topics to resolve the\ndefined issues using a widely used AI (ChatGPT-4o, OpenAI Inc, April 2024\nversion) with prompt-engineering strategies. We guided AI to perform\nmulti-level tasks: 1) knowledge interpretation and summarization (e.g.,\ninterpreting and summarizing the NLP-defined topics), 2) knowledge generation\n(e.g., generating research ideas corresponding to patients issues), 3)\nself-reflection and correction (e.g., ensuring and revising the research ideas\nafter searching for scientific articles), and 4) self-reassurance (e.g.,\nconfirming and finalizing the research ideas). Six highly experienced breast\noncologists and dermatologists assessed the significance and novelty of\nAI-generated research topics using a 5-point Likert scale (1-exceptional,\n5-poor). One-third of the AI-suggested research topics were highly significant\nand novel when both scores were lower than the average. Two-thirds of the\nAI-suggested topics were novel in both cancers. Our findings demonstrate that\nAI-generated research topics reflecting patient perspectives via a large volume\nof patient messages can meaningfully guide future directions in\npatient-centered health research.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.14456v1",
    "published_date": "2024-11-15 20:24:38 UTC",
    "updated_date": "2024-11-15 20:24:38 UTC"
  },
  {
    "arxiv_id": "2411.10564v2",
    "title": "Vision Eagle Attention: a new lens for advancing image classification",
    "authors": [
      "Mahmudul Hasan"
    ],
    "abstract": "In computer vision tasks, the ability to focus on relevant regions within an\nimage is crucial for improving model performance, particularly when key\nfeatures are small, subtle, or spatially dispersed. Convolutional neural\nnetworks (CNNs) typically treat all regions of an image equally, which can lead\nto inefficient feature extraction. To address this challenge, I have introduced\nVision Eagle Attention, a novel attention mechanism that enhances visual\nfeature extraction using convolutional spatial attention. The model applies\nconvolution to capture local spatial features and generates an attention map\nthat selectively emphasizes the most informative regions of the image. This\nattention mechanism enables the model to focus on discriminative features while\nsuppressing irrelevant background information. I have integrated Vision Eagle\nAttention into a lightweight ResNet-18 architecture, demonstrating that this\ncombination results in an efficient and powerful model. I have evaluated the\nperformance of the proposed model on three widely used benchmark datasets:\nFashionMNIST, Intel Image Classification, and OracleMNIST, with a primary focus\non image classification. Experimental results show that the proposed approach\nimproves classification accuracy. Additionally, this method has the potential\nto be extended to other vision tasks, such as object detection, segmentation,\nand visual tracking, offering a computationally efficient solution for a wide\nrange of vision-based applications. Code is available at:\nhttps://github.com/MahmudulHasan11085/Vision-Eagle-Attention.git",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "7 pages, 2 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2411.10564v2",
    "published_date": "2024-11-15 20:21:59 UTC",
    "updated_date": "2024-12-09 05:00:29 UTC"
  },
  {
    "arxiv_id": "2411.10561v1",
    "title": "Pragmatic information of aesthetic appraisal",
    "authors": [
      "Peter beim Graben"
    ],
    "abstract": "A phenomenological model for aesthetic appraisal is proposed in terms of\npragmatic information for a dynamic update semantics over belief states on an\naesthetic appreciator. The model qualitatively correlates with aesthetic\npleasure ratings in an experimental study on cadential effects in Western tonal\nmusic. Finally, related computational and neurodynamical accounts are\ndiscussed.",
    "categories": [
      "q-bio.NC",
      "cs.AI"
    ],
    "primary_category": "q-bio.NC",
    "comment": "10 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.10561v1",
    "published_date": "2024-11-15 20:15:15 UTC",
    "updated_date": "2024-11-15 20:15:15 UTC"
  },
  {
    "arxiv_id": "2411.10555v1",
    "title": "Low-Rank Optimal Transport through Factor Relaxation with Latent Coupling",
    "authors": [
      "Peter Halmos",
      "Xinhao Liu",
      "Julian Gold",
      "Benjamin J Raphael"
    ],
    "abstract": "Optimal transport (OT) is a general framework for finding a minimum-cost\ntransport plan, or coupling, between probability distributions, and has many\napplications in machine learning. A key challenge in applying OT to massive\ndatasets is the quadratic scaling of the coupling matrix with the size of the\ndataset. [Forrow et al. 2019] introduced a factored coupling for the\nk-Wasserstein barycenter problem, which [Scetbon et al. 2021] adapted to solve\nthe primal low-rank OT problem. We derive an alternative parameterization of\nthe low-rank problem based on the $\\textit{latent coupling}$ (LC) factorization\npreviously introduced by [Lin et al. 2021] generalizing [Forrow et al. 2019].\nThe LC factorization has multiple advantages for low-rank OT including\ndecoupling the problem into three OT problems and greater flexibility and\ninterpretability. We leverage these advantages to derive a new algorithm\n$\\textit{Factor Relaxation with Latent Coupling}$ (FRLC), which uses\n$\\textit{coordinate}$ mirror descent to compute the LC factorization. FRLC\nhandles multiple OT objectives (Wasserstein, Gromov-Wasserstein, Fused\nGromov-Wasserstein), and marginal constraints (balanced, unbalanced, and\nsemi-relaxed) with linear space complexity. We provide theoretical results on\nFRLC, and demonstrate superior performance on diverse applications -- including\ngraph clustering and spatial transcriptomics -- while demonstrating its\ninterpretability.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "53 pages, 13 figures, NeurIPS 2024. Comments welcome!",
    "pdf_url": "http://arxiv.org/pdf/2411.10555v1",
    "published_date": "2024-11-15 20:07:15 UTC",
    "updated_date": "2024-11-15 20:07:15 UTC"
  },
  {
    "arxiv_id": "2411.12589v2",
    "title": "ULTra: Unveiling Latent Token Interpretability in Transformer-Based Understanding and Segmentation",
    "authors": [
      "Hesam Hosseini",
      "Ghazal Hosseini Mighan",
      "Amirabbas Afzali",
      "Sajjad Amini",
      "Amir Houmansadr"
    ],
    "abstract": "Transformers have revolutionized Computer Vision (CV) through self-attention\nmechanisms. However, their complexity makes latent token representations\ndifficult to interpret. We introduce ULTra, a framework for interpreting\nTransformer embeddings and uncovering meaningful semantic patterns within them.\nULTra enables unsupervised semantic segmentation using pre-trained models\nwithout requiring fine-tuning. Additionally, we propose a self-supervised\ntraining approach that refines segmentation performance by learning an external\ntransformation matrix without modifying the underlying model. Our method\nachieves state-of-the-art performance in unsupervised semantic segmentation,\noutperforming existing segmentation methods. Furthermore, we validate ULTra for\nmodel interpretation on both synthetic and real-world scenarios, including\nObject Selection and interpretable text summarization using LLMs, demonstrating\nits broad applicability in explaining the semantic structure of latent token\nrepresentations.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12589v2",
    "published_date": "2024-11-15 19:36:50 UTC",
    "updated_date": "2025-03-22 19:54:49 UTC"
  },
  {
    "arxiv_id": "2411.10534v1",
    "title": "Chain of Alignment: Integrating Public Will with Expert Intelligence for Language Model Alignment",
    "authors": [
      "Andrew Konya",
      "Aviv Ovadya",
      "Kevin Feng",
      "Quan Ze Chen",
      "Lisa Schirch",
      "Colin Irwin",
      "Amy X. Zhang"
    ],
    "abstract": "We introduce a method to measure the alignment between public will and\nlanguage model (LM) behavior that can be applied to fine-tuning, online\noversight, and pre-release safety checks. Our `chain of alignment' (CoA)\napproach produces a rule based reward (RBR) by creating model behavior\n$\\textit{rules}$ aligned to normative $\\textit{objectives}$ aligned to\n$\\textit{public will}$. This factoring enables a nonexpert public to directly\nspecify their will through the normative objectives, while expert intelligence\nis used to figure out rules entailing model behavior that best achieves those\nobjectives. We validate our approach by applying it across three different\ndomains of LM prompts related to mental health. We demonstrate a public input\nprocess built on collective dialogues and bridging-based ranking that reliably\nproduces normative objectives supported by at least $96\\% \\pm 2\\%$ of the US\npublic. We then show that rules developed by mental health experts to achieve\nthose objectives enable a RBR that evaluates an LM response's alignment with\nthe objectives similarly to human experts (Pearson's $r=0.841$, $AUC=0.964$).\nBy measuring alignment with objectives that have near unanimous public support,\nthese CoA RBRs provide an approximate measure of alignment between LM behavior\nand public will.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "Pluralistic Alignment Workshop at NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.10534v1",
    "published_date": "2024-11-15 19:10:39 UTC",
    "updated_date": "2024-11-15 19:10:39 UTC"
  },
  {
    "arxiv_id": "2411.10446v2",
    "title": "VeriGraph: Scene Graphs for Execution Verifiable Robot Planning",
    "authors": [
      "Daniel Ekpo",
      "Mara Levy",
      "Saksham Suri",
      "Chuong Huynh",
      "Abhinav Shrivastava"
    ],
    "abstract": "Recent advancements in vision-language models (VLMs) offer potential for\nrobot task planning, but challenges remain due to VLMs' tendency to generate\nincorrect action sequences. To address these limitations, we propose VeriGraph,\na novel framework that integrates VLMs for robotic planning while verifying\naction feasibility. VeriGraph employs scene graphs as an intermediate\nrepresentation, capturing key objects and spatial relationships to improve plan\nverification and refinement. The system generates a scene graph from input\nimages and uses it to iteratively check and correct action sequences generated\nby an LLM-based task planner, ensuring constraints are respected and actions\nare executable. Our approach significantly enhances task completion rates\nacross diverse manipulation scenarios, outperforming baseline methods by 58%\nfor language-based tasks and 30% for image-based tasks.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.10446v2",
    "published_date": "2024-11-15 18:59:51 UTC",
    "updated_date": "2024-11-21 15:56:48 UTC"
  },
  {
    "arxiv_id": "2411.10436v1",
    "title": "Mitigating Hallucination in Multimodal Large Language Model via Hallucination-targeted Direct Preference Optimization",
    "authors": [
      "Yuhan Fu",
      "Ruobing Xie",
      "Xingwu Sun",
      "Zhanhui Kang",
      "Xirong Li"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) are known to hallucinate, which\nlimits their practical applications. Recent works have attempted to apply\nDirect Preference Optimization (DPO) to enhance the performance of MLLMs, but\nhave shown inconsistent improvements in mitigating hallucinations. To address\nthis issue more effectively, we introduce Hallucination-targeted Direct\nPreference Optimization (HDPO) to reduce hallucinations in MLLMs. Unlike\nprevious approaches, our method tackles hallucinations from their diverse forms\nand causes. Specifically, we develop three types of preference pair data\ntargeting the following causes of MLLM hallucinations: (1) insufficient visual\ncapabilities, (2) long context generation, and (3) multimodal conflicts.\nExperimental results demonstrate that our method achieves superior performance\nacross multiple hallucination evaluation datasets, surpassing most\nstate-of-the-art (SOTA) methods and highlighting the potential of our approach.\nAblation studies and in-depth analyses further confirm the effectiveness of our\nmethod and suggest the potential for further improvements through scaling up.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.MM"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.10436v1",
    "published_date": "2024-11-15 18:56:01 UTC",
    "updated_date": "2024-11-15 18:56:01 UTC"
  },
  {
    "arxiv_id": "2411.10431v1",
    "title": "Mitigating Parameter Degeneracy using Joint Conditional Diffusion Model for WECC Composite Load Model in Power Systems",
    "authors": [
      "Feiqin Zhu",
      "Dmitrii Torbunov",
      "Yihui Ren",
      "Zhongjing Jiang",
      "Tianqiao Zhao",
      "Amirthagunaraj Yogarathnam",
      "Meng Yue"
    ],
    "abstract": "Data-driven modeling for dynamic systems has gained widespread attention in\nrecent years. Its inverse formulation, parameter estimation, aims to infer the\ninherent model parameters from observations. However, parameter degeneracy,\nwhere different combinations of parameters yield the same observable output,\nposes a critical barrier to accurately and uniquely identifying model\nparameters. In the context of WECC composite load model (CLM) in power systems,\nutility practitioners have observed that CLM parameters carefully selected for\none fault event may not perform satisfactorily in another fault. Here, we\ninnovate a joint conditional diffusion model-based inverse problem solver\n(JCDI), that incorporates a joint conditioning architecture with simultaneous\ninputs of multi-event observations to improve parameter generalizability.\nSimulation studies on the WECC CLM show that the proposed JCDI effectively\nreduces uncertainties of degenerate parameters, thus the parameter estimation\nerror is decreased by 42.1% compared to a single-event learning scheme. This\nenables the model to achieve high accuracy in predicting power trajectories\nunder different fault events, including electronic load tripping and motor\nstalling, outperforming standard deep reinforcement learning and supervised\nlearning approaches. We anticipate this work will contribute to mitigating\nparameter degeneracy in system dynamics, providing a general parameter\nestimation framework across various scientific domains.",
    "categories": [
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.10431v1",
    "published_date": "2024-11-15 18:53:08 UTC",
    "updated_date": "2024-11-15 18:53:08 UTC"
  },
  {
    "arxiv_id": "2411.10422v1",
    "title": "Evaluating Creativity and Deception in Large Language Models: A Simulation Framework for Multi-Agent Balderdash",
    "authors": [
      "Parsa Hejabi",
      "Elnaz Rahmati",
      "Alireza S. Ziabari",
      "Preni Golazizian",
      "Jesse Thomason",
      "Morteza Dehghani"
    ],
    "abstract": "Large Language Models (LLMs) have shown impressive capabilities in complex\ntasks and interactive environments, yet their creativity remains underexplored.\nThis paper introduces a simulation framework utilizing the game Balderdash to\nevaluate both the creativity and logical reasoning of LLMs. In Balderdash,\nplayers generate fictitious definitions for obscure terms to deceive others\nwhile identifying correct definitions. Our framework enables multiple LLM\nagents to participate in this game, assessing their ability to produce\nplausible definitions and strategize based on game rules and history. We\nimplemented a centralized game engine featuring various LLMs as participants\nand a judge LLM to evaluate semantic equivalence. Through a series of\nexperiments, we analyzed the performance of different LLMs, examining metrics\nsuch as True Definition Ratio, Deception Ratio, and Correct Guess Ratio. The\nresults provide insights into the creative and deceptive capabilities of LLMs,\nhighlighting their strengths and areas for improvement. Specifically, the study\nreveals that infrequent vocabulary in LLMs' input leads to poor reasoning on\ngame rules and historical context\n(https://github.com/ParsaHejabi/Simulation-Framework-for-Multi-Agent-Balderdash).",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "Accepted at Wordplay: When Language Meets Games @ ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.10422v1",
    "published_date": "2024-11-15 18:42:48 UTC",
    "updated_date": "2024-11-15 18:42:48 UTC"
  },
  {
    "arxiv_id": "2411.10416v1",
    "title": "Towards Automatic Evaluation of Task-Oriented Dialogue Flows",
    "authors": [
      "Mehrnoosh Mirtaheri",
      "Nikhil Varghese",
      "Chandra Khatri",
      "Amol Kelkar"
    ],
    "abstract": "Task-oriented dialogue systems rely on predefined conversation schemes\n(dialogue flows) often represented as directed acyclic graphs. These flows can\nbe manually designed or automatically generated from previously recorded\nconversations. Due to variations in domain expertise or reliance on different\nsets of prior conversations, these dialogue flows can manifest in significantly\ndifferent graph structures. Despite their importance, there is no standard\nmethod for evaluating the quality of dialogue flows. We introduce FuDGE (Fuzzy\nDialogue-Graph Edit Distance), a novel metric that evaluates dialogue flows by\nassessing their structural complexity and representational coverage of the\nconversation data. FuDGE measures how well individual conversations align with\na flow and, consequently, how well a set of conversations is represented by the\nflow overall. Through extensive experiments on manually configured flows and\nflows generated by automated techniques, we demonstrate the effectiveness of\nFuDGE and its evaluation framework. By standardizing and optimizing dialogue\nflows, FuDGE enables conversational designers and automated techniques to\nachieve higher levels of efficiency and automation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.10416v1",
    "published_date": "2024-11-15 18:35:00 UTC",
    "updated_date": "2024-11-15 18:35:00 UTC"
  },
  {
    "arxiv_id": "2411.10411v2",
    "title": "Repurposing Stable Diffusion Attention for Training-Free Unsupervised Interactive Segmentation",
    "authors": [
      "Markus Karmann",
      "Onay Urfalioglu"
    ],
    "abstract": "Recent progress in interactive point prompt based Image Segmentation allows\nto significantly reduce the manual effort to obtain high quality semantic\nlabels. State-of-the-art unsupervised methods use self-supervised pre-trained\nmodels to obtain pseudo-labels which are used in training a prompt-based\nsegmentation model. In this paper, we propose a novel unsupervised and\ntraining-free approach based solely on the self-attention of Stable Diffusion.\nWe interpret the self-attention tensor as a Markov transition operator, which\nenables us to iteratively construct a Markov chain. Pixel-wise counting of the\nrequired number of iterations along the Markov chain to reach a relative\nprobability threshold yields a Markov-iteration-map, which we simply call a\nMarkov-map. Compared to the raw attention maps, we show that our proposed\nMarkov-map has less noise, sharper semantic boundaries and more uniform values\nwithin semantically similar regions. We integrate the Markov-map in a simple\nyet effective truncated nearest neighbor framework to obtain interactive point\nprompt based segmentation. Despite being training-free, we experimentally show\nthat our approach yields excellent results in terms of Number of Clicks (NoC),\neven outperforming state-of-the-art training based unsupervised methods in most\nof the datasets. Code is available at https://github.com/mkarmann/m2n2.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2411.10411v2",
    "published_date": "2024-11-15 18:29:59 UTC",
    "updated_date": "2025-03-20 16:15:14 UTC"
  },
  {
    "arxiv_id": "2411.10406v2",
    "title": "How to Build a Quantum Supercomputer: Scaling from Hundreds to Millions of Qubits",
    "authors": [
      "Masoud Mohseni",
      "Artur Scherer",
      "K. Grace Johnson",
      "Oded Wertheim",
      "Matthew Otten",
      "Navid Anjum Aadit",
      "Yuri Alexeev",
      "Kirk M. Bresniker",
      "Kerem Y. Camsari",
      "Barbara Chapman",
      "Soumitra Chatterjee",
      "Gebremedhin A. Dagnew",
      "Aniello Esposito",
      "Farah Fahim",
      "Marco Fiorentino",
      "Archit Gajjar",
      "Abdullah Khalid",
      "Xiangzhou Kong",
      "Bohdan Kulchytskyy",
      "Elica Kyoseva",
      "Ruoyu Li",
      "P. Aaron Lott",
      "Igor L. Markov",
      "Robert F. McDermott",
      "Giacomo Pedretti",
      "Pooja Rao",
      "Eleanor Rieffel",
      "Allyson Silva",
      "John Sorebo",
      "Panagiotis Spentzouris",
      "Ziv Steiner",
      "Boyan Torosov",
      "Davide Venturelli",
      "Robert J. Visser",
      "Zak Webb",
      "Xin Zhan",
      "Yonatan Cohen",
      "Pooya Ronagh",
      "Alan Ho",
      "Raymond G. Beausoleil",
      "John M. Martinis"
    ],
    "abstract": "In the span of four decades, quantum computation has evolved from an\nintellectual curiosity to a potentially realizable technology. Today,\nsmall-scale demonstrations have become possible for quantum algorithmic\nprimitives on hundreds of physical qubits and proof-of-principle\nerror-correction on a single logical qubit. Nevertheless, despite significant\nprogress and excitement, the path toward a full-stack scalable technology is\nlargely unknown. There are significant outstanding quantum hardware,\nfabrication, software architecture, and algorithmic challenges that are either\nunresolved or overlooked. These issues could seriously undermine the arrival of\nutility-scale quantum computers for the foreseeable future. Here, we provide a\ncomprehensive review of these scaling challenges. We show how the road to\nscaling could be paved by adopting existing semiconductor technology to build\nmuch higher-quality qubits, employing system engineering approaches, and\nperforming distributed quantum computation within heterogeneous\nhigh-performance computing infrastructures. These opportunities for research\nand development could unlock certain promising applications, in particular,\nefficient quantum simulation/learning of quantum data generated by natural or\nengineered quantum systems. To estimate the true cost of such promises, we\nprovide a detailed resource and sensitivity analysis for classically hard\nquantum chemistry calculations on surface-code error-corrected quantum\ncomputers given current, target, and desired hardware specifications based on\nsuperconducting qubits, accounting for a realistic distribution of errors.\nFurthermore, we argue that, to tackle industry-scale classical optimization and\nmachine learning problems in a cost-effective manner, heterogeneous\nquantum-probabilistic computing with custom-designed accelerators should be\nconsidered as a complementary path toward scalability.",
    "categories": [
      "quant-ph",
      "cond-mat.dis-nn",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "quant-ph",
    "comment": "76 pages, 46 figures. General revision, added figures, added\n  references, added appendices",
    "pdf_url": "http://arxiv.org/pdf/2411.10406v2",
    "published_date": "2024-11-15 18:22:46 UTC",
    "updated_date": "2025-01-31 18:21:23 UTC"
  },
  {
    "arxiv_id": "2411.10397v2",
    "title": "Features that Make a Difference: Leveraging Gradients for Improved Dictionary Learning",
    "authors": [
      "Jeffrey Olmo",
      "Jared Wilson",
      "Max Forsey",
      "Bryce Hepner",
      "Thomas Vin Howe",
      "David Wingate"
    ],
    "abstract": "Sparse Autoencoders (SAEs) are a promising approach for extracting neural\nnetwork representations by learning a sparse and overcomplete decomposition of\nthe network's internal activations. However, SAEs are traditionally trained\nconsidering only activation values and not the effect those activations have on\ndownstream computations. This limits the information available to learn\nfeatures, and biases the autoencoder towards neglecting features which are\nrepresented with small activation values but strongly influence model outputs.\nTo address this, we introduce Gradient SAEs (g-SAEs), which modify the\n$k$-sparse autoencoder architecture by augmenting the TopK activation function\nto rely on the gradients of the input activation when selecting the $k$\nelements. For a given sparsity level, g-SAEs produce reconstructions that are\nmore faithful to original network performance when propagated through the\nnetwork. Additionally, we find evidence that g-SAEs learn latents that are on\naverage more effective at steering models in arbitrary contexts. By considering\nthe downstream effects of activations, our approach leverages the dual nature\nof neural network features as both $\\textit{representations}$, retrospectively,\nand $\\textit{actions}$, prospectively. While previous methods have approached\nthe problem of feature discovery primarily focused on the former aspect, g-SAEs\nrepresent a step towards accounting for the latter as well.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 10 figures. Accepted to NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2411.10397v2",
    "published_date": "2024-11-15 18:03:52 UTC",
    "updated_date": "2025-03-31 20:36:37 UTC"
  },
  {
    "arxiv_id": "2411.10389v1",
    "title": "Deep Learning for Micro-Scale Crack Detection on Imbalanced Datasets Using Key Point Localization",
    "authors": [
      "Fatahlla Moreh",
      "Yusuf Hasan",
      "Bilal Zahid Hussain",
      "Mohammad Ammar",
      "Sven Tomforde"
    ],
    "abstract": "Internal crack detection has been a subject of focus in structural health\nmonitoring. By focusing on crack detection in structural datasets, it is\ndemonstrated that deep learning (DL) methods can effectively analyze seismic\nwave fields interacting with micro-scale cracks, which are beyond the\nresolution of conventional visual inspection. This work explores a novel\napplication of DL-based key point detection technique, where cracks are\nlocalized by predicting the coordinates of four key points that define a\nbounding region of the crack. The study not only opens new research directions\nfor non-visual applications but also effectively mitigates the impact of\nimbalanced data which poses a challenge for previous DL models, as it can be\nbiased toward predicting the majority class (non-crack regions). Popular DL\ntechniques, such as the Inception blocks, are used and investigated. The model\nshows an overall reduction in loss when applied to micro-scale crack detection\nand is reflected in the lower average deviation between the location of actual\nand predicted cracks, with an average Intersection over Union (IoU) being 0.511\nfor all micro cracks (greater than 0.00 micrometers) and 0.631 for larger micro\ncracks (greater than 4 micrometers).",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.10389v1",
    "published_date": "2024-11-15 17:50:46 UTC",
    "updated_date": "2024-11-15 17:50:46 UTC"
  },
  {
    "arxiv_id": "2411.10385v1",
    "title": "Low-Latency Task-Oriented Communications with Multi-Round, Multi-Task Deep Learning",
    "authors": [
      "Yalin E. Sagduyu",
      "Tugba Erpek",
      "Aylin Yener",
      "Sennur Ulukus"
    ],
    "abstract": "In this paper, we address task-oriented (or goal-oriented) communications\nwhere an encoder at the transmitter learns compressed latent representations of\ndata, which are then transmitted over a wireless channel. At the receiver, a\ndecoder performs a machine learning task, specifically for classifying the\nreceived signals. The deep neural networks corresponding to the encoder-decoder\npair are jointly trained, taking both channel and data characteristics into\naccount. Our objective is to achieve high accuracy in completing the underlying\ntask while minimizing the number of channel uses determined by the encoder's\noutput size. To this end, we propose a multi-round, multi-task learning (MRMTL)\napproach for the dynamic update of channel uses in multi-round transmissions.\nThe transmitter incrementally sends an increasing number of encoded samples\nover the channel based on the feedback from the receiver, and the receiver\nutilizes the signals from a previous round to enhance the task performance,\nrather than only considering the latest transmission. This approach employs\nmulti-task learning to jointly optimize accuracy across varying number of\nchannel uses, treating each configuration as a distinct task. By evaluating the\nconfidence of the receiver in task decisions, MRMTL decides on whether to\nallocate additional channel uses in multiple rounds. We characterize both the\naccuracy and the delay (total number of channel uses) of MRMTL, demonstrating\nthat it achieves the accuracy close to that of conventional methods requiring\nlarge numbers of channel uses, but with reduced delay by incorporating signals\nfrom a prior round. We consider the CIFAR-10 dataset, convolutional neural\nnetwork architectures, and AWGN and Rayleigh channel models for performance\nevaluation. We show that MRMTL significantly improves the efficiency of\ntask-oriented communications, balancing accuracy and latency effectively.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "cs.IT",
      "cs.NI",
      "eess.SP",
      "math.IT"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.10385v1",
    "published_date": "2024-11-15 17:48:06 UTC",
    "updated_date": "2024-11-15 17:48:06 UTC"
  },
  {
    "arxiv_id": "2411.10371v2",
    "title": "A Survey of Event Causality Identification: Principles, Taxonomy, Challenges, and Assessment",
    "authors": [
      "Qing Cheng",
      "Zefan Zeng",
      "Xingchen Hu",
      "Yuehang Si",
      "Zhong Liu"
    ],
    "abstract": "Event Causality Identification (ECI) has become a crucial task in Natural\nLanguage Processing (NLP), aimed at automatically extracting causalities from\ntextual data. In this survey, we systematically address the foundational\nprinciples, technical frameworks, and challenges of ECI, offering a\ncomprehensive taxonomy to categorize and clarify current research\nmethodologies, as well as a quantitative assessment of existing models. We\nfirst establish a conceptual framework for ECI, outlining key definitions,\nproblem formulations, and evaluation standards. Our taxonomy classifies ECI\nmethods according to the two primary tasks of sentence-level (SECI) and\ndocument-level (DECI) event causality identification. For SECI, we examine\nfeature pattern-based matching, deep semantic encoding, causal knowledge\npre-training and prompt-based fine-tuning, and external knowledge enhancement\nmethods. For DECI, we highlight approaches focused on event graph reasoning and\nprompt-based techniques to address the complexity of cross-sentence causal\ninference. Additionally, we analyze the strengths, limitations, and open\nchallenges of each approach. We further conduct an extensive quantitative\nevaluation of various ECI methods on two benchmark datasets. Finally, we\nexplore future research directions, highlighting promising pathways to overcome\ncurrent limitations and broaden ECI applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.10371v2",
    "published_date": "2024-11-15 17:19:42 UTC",
    "updated_date": "2024-11-25 16:55:09 UTC"
  },
  {
    "arxiv_id": "2411.10369v1",
    "title": "Towards High-Fidelity 3D Portrait Generation with Rich Details by Cross-View Prior-Aware Diffusion",
    "authors": [
      "Haoran Wei",
      "Wencheng Han",
      "Xingping Dong",
      "Jianbing Shen"
    ],
    "abstract": "Recent diffusion-based Single-image 3D portrait generation methods typically\nemploy 2D diffusion models to provide multi-view knowledge, which is then\ndistilled into 3D representations. However, these methods usually struggle to\nproduce high-fidelity 3D models, frequently yielding excessively blurred\ntextures. We attribute this issue to the insufficient consideration of\ncross-view consistency during the diffusion process, resulting in significant\ndisparities between different views and ultimately leading to blurred 3D\nrepresentations. In this paper, we address this issue by comprehensively\nexploiting multi-view priors in both the conditioning and diffusion procedures\nto produce consistent, detail-rich portraits. From the conditioning standpoint,\nwe propose a Hybrid Priors Diffsion model, which explicitly and implicitly\nincorporates multi-view priors as conditions to enhance the status consistency\nof the generated multi-view portraits. From the diffusion perspective,\nconsidering the significant impact of the diffusion noise distribution on\ndetailed texture generation, we propose a Multi-View Noise Resamplig Strategy\nintegrated within the optimization process leveraging cross-view priors to\nenhance representation consistency. Extensive experiments demonstrate that our\nmethod can produce 3D portraits with accurate geometry and rich details from a\nsingle image. The project page is at\n\\url{https://haoran-wei.github.io/Portrait-Diffusion}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.10369v1",
    "published_date": "2024-11-15 17:19:18 UTC",
    "updated_date": "2024-11-15 17:19:18 UTC"
  },
  {
    "arxiv_id": "2411.10368v1",
    "title": "Mechanisms of Generative Image-to-Image Translation Networks",
    "authors": [
      "Guangzong Chen",
      "Mingui Sun",
      "Zhi-Hong Mao",
      "Kangni Liu",
      "Wenyan Jia"
    ],
    "abstract": "Generative Adversarial Networks (GANs) are a class of neural networks that\nhave been widely used in the field of image-to-image translation. In this\npaper, we propose a streamlined image-to-image translation network with a\nsimpler architecture compared to existing models. We investigate the\nrelationship between GANs and autoencoders and provide an explanation for the\nefficacy of employing only the GAN component for tasks involving image\ntranslation. We show that adversarial for GAN models yields results comparable\nto those of existing methods without additional complex loss penalties.\nSubsequently, we elucidate the rationale behind this phenomenon. We also\nincorporate experimental results to demonstrate the validity of our findings.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.10368v1",
    "published_date": "2024-11-15 17:17:46 UTC",
    "updated_date": "2024-11-15 17:17:46 UTC"
  },
  {
    "arxiv_id": "2411.10367v1",
    "title": "Continual Adversarial Reinforcement Learning (CARL) of False Data Injection detection: forgetting and explainability",
    "authors": [
      "Pooja Aslami",
      "Kejun Chen",
      "Timothy M. Hansen",
      "Malik Hassanaly"
    ],
    "abstract": "False data injection attacks (FDIAs) on smart inverters are a growing concern\nlinked to increased renewable energy production. While data-based FDIA\ndetection methods are also actively developed, we show that they remain\nvulnerable to impactful and stealthy adversarial examples that can be crafted\nusing Reinforcement Learning (RL). We propose to include such adversarial\nexamples in data-based detection training procedure via a continual adversarial\nRL (CARL) approach. This way, one can pinpoint the deficiencies of data-based\ndetection, thereby offering explainability during their incremental\nimprovement. We show that a continual learning implementation is subject to\ncatastrophic forgetting, and additionally show that forgetting can be addressed\nby employing a joint training strategy on all generated FDIA scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.10367v1",
    "published_date": "2024-11-15 17:17:06 UTC",
    "updated_date": "2024-11-15 17:17:06 UTC"
  },
  {
    "arxiv_id": "2411.10364v2",
    "title": "Forming Auxiliary High-confident Instance-level Loss to Promote Learning from Label Proportions",
    "authors": [
      "Tianhao Ma",
      "Han Chen",
      "Juncheng Hu",
      "Yungang Zhu",
      "Ximing Li"
    ],
    "abstract": "Learning from label proportions (LLP), i.e., a challenging weakly-supervised\nlearning task, aims to train a classifier by using bags of instances and the\nproportions of classes within bags, rather than annotated labels for each\ninstance. Beyond the traditional bag-level loss, the mainstream methodology of\nLLP is to incorporate an auxiliary instance-level loss with pseudo-labels\nformed by predictions. Unfortunately, we empirically observed that the\npseudo-labels are are often inaccurate due to over-smoothing, especially for\nthe scenarios with large bag sizes, hurting the classifier induction. To\nalleviate this problem, we suggest a novel LLP method, namely Learning from\nLabel Proportions with Auxiliary High-confident Instance-level Loss\n(L^2P-AHIL). Specifically, we propose a dual entropy-based weight (DEW) method\nto adaptively measure the confidences of pseudo-labels. It simultaneously\nemphasizes accurate predictions at the bag level and avoids overly smoothed\npredictions. We then form high-confident instance-level loss with DEW, and\njointly optimize it with the bag-level loss in a self-training manner. The\nexperimental results on benchmark datasets show that L^2P-AHIL can surpass the\nexisting baseline methods, and the performance gain can be more significant as\nthe bag size increases. The implementation of our method is available at\nhttps://github.com/TianhaoMa5/LLP-AHIL.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted as a conference paper at CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2411.10364v2",
    "published_date": "2024-11-15 17:14:18 UTC",
    "updated_date": "2025-03-24 03:41:58 UTC"
  },
  {
    "arxiv_id": "2411.10340v1",
    "title": "Domain Adaptation-based Edge Computing for Cross-Conditions Fault Diagnosis",
    "authors": [
      "Yanzhi Wang",
      "Chu Wang",
      "Jinhong Wu",
      "Ziyang Yu",
      "Qi Zhou"
    ],
    "abstract": "Fault diagnosis technology supports the healthy operation of mechanical\nequipment. However, the variations conditions during the operation of\nmechanical equipment lead to significant disparities in data distribution,\nposing challenges to fault diagnosis. Furthermore, when deploying applications,\ntraditional methods often encounter issues such as latency and data security.\nTherefore, conducting fault diagnosis and deploying application methods under\ncross-operating conditions holds significant value. This paper proposes a\ndomain adaptation-based lightweight fault diagnosis framework for edge\ncomputing scenarios. Incorporating the local maximum mean discrepancy into\nknowledge transfer aligns the feature distributions of different domains in a\nhigh-dimensional feature space, to discover a common feature space across\ndomains. The acquired fault diagnosis expertise from the cloud-model is\ntransferred to the lightweight edge-model using adaptation knowledge transfer\nmethods. While ensuring real-time diagnostic capabilities, accurate fault\ndiagnosis is achieved across working conditions. We conducted validation\nexperiments on the NVIDIA Jetson Xavier NX kit. In terms of diagnostic\nperformance, the proposed method significantly improved diagnostic accuracy,\nwith average increases of 34.44% and 17.33% compared to the comparison method,\nrespectively. Regarding lightweight effectiveness, proposed method achieved an\naverage inference speed increase of 80.47%. Additionally, compared to the\ncloud-model, the parameter count of the edge-model decreased by 96.37%, while\nthe Flops decreased by 83.08%.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.DC",
    "comment": "28 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.10340v1",
    "published_date": "2024-11-15 16:40:43 UTC",
    "updated_date": "2024-11-15 16:40:43 UTC"
  },
  {
    "arxiv_id": "2411.10329v2",
    "title": "Safe Text-to-Image Generation: Simply Sanitize the Prompt Embedding",
    "authors": [
      "Huming Qiu",
      "Guanxu Chen",
      "Mi Zhang",
      "Xiaohan Zhang",
      "Xiaoyu You",
      "Min Yang"
    ],
    "abstract": "In recent years, text-to-image (T2I) generation models have made significant\nprogress in generating high-quality images that align with text descriptions.\nHowever, these models also face the risk of unsafe generation, potentially\nproducing harmful content that violates usage policies, such as explicit\nmaterial. Existing safe generation methods typically focus on suppressing\ninappropriate content by erasing undesired concepts from visual\nrepresentations, while neglecting to sanitize the textual representation.\nAlthough these methods help mitigate the risk of misuse to some extent, their\nrobustness remains insufficient when dealing with adversarial attacks.\n  Given that semantic consistency between input text and output image is a core\nrequirement of T2I models, we identify that textual representations are likely\nthe primary source of unsafe generation. To this end, we propose Embedding\nSanitizer (ES), which enhances the safety of T2I models by sanitizing\ninappropriate concepts in prompt embeddings. To our knowledge, ES is the first\ninterpretable safe generation framework that assigns a score to each token in\nthe prompt to indicate its potential harmfulness. In addition, ES adopts a\nplug-and-play modular design, offering compatibility for seamless integration\nwith various T2I models and other safeguards. Evaluations on five prompt\nbenchmarks show that ES outperforms eleven existing safeguard baselines,\nachieving state-of-the-art robustness while maintaining high-quality image\ngeneration.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.10329v2",
    "published_date": "2024-11-15 16:29:02 UTC",
    "updated_date": "2025-04-15 12:26:05 UTC"
  },
  {
    "arxiv_id": "2411.10323v1",
    "title": "The Dawn of GUI Agent: A Preliminary Case Study with Claude 3.5 Computer Use",
    "authors": [
      "Siyuan Hu",
      "Mingyu Ouyang",
      "Difei Gao",
      "Mike Zheng Shou"
    ],
    "abstract": "The recently released model, Claude 3.5 Computer Use, stands out as the first\nfrontier AI model to offer computer use in public beta as a graphical user\ninterface (GUI) agent. As an early beta, its capability in the real-world\ncomplex environment remains unknown. In this case study to explore Claude 3.5\nComputer Use, we curate and organize a collection of carefully designed tasks\nspanning a variety of domains and software. Observations from these cases\ndemonstrate Claude 3.5 Computer Use's unprecedented ability in end-to-end\nlanguage to desktop actions. Along with this study, we provide an\nout-of-the-box agent framework for deploying API-based GUI automation models\nwith easy implementation. Our case studies aim to showcase a groundwork of\ncapabilities and limitations of Claude 3.5 Computer Use with detailed analyses\nand bring to the fore questions about planning, action, and critic, which must\nbe considered for future improvement. We hope this preliminary exploration will\ninspire future research into the GUI agent community. All the test cases in the\npaper can be tried through the project:\nhttps://github.com/showlab/computer_use_ootb.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "40 pages, 21 figures, preprint",
    "pdf_url": "http://arxiv.org/pdf/2411.10323v1",
    "published_date": "2024-11-15 16:23:52 UTC",
    "updated_date": "2024-11-15 16:23:52 UTC"
  },
  {
    "arxiv_id": "2411.10308v1",
    "title": "A Realistic Collimated X-Ray Image Simulation Pipeline",
    "authors": [
      "Benjamin El-Zein",
      "Dominik Eckert",
      "Thomas Weber",
      "Maximilian Rohleder",
      "Ludwig Ritschl",
      "Steffen Kappler",
      "Andreas Maier"
    ],
    "abstract": "Collimator detection remains a challenging task in X-ray systems with\nunreliable or non-available information about the detectors position relative\nto the source. This paper presents a physically motivated image processing\npipeline for simulating the characteristics of collimator shadows in X-ray\nimages. By generating randomized labels for collimator shapes and locations,\nincorporating scattered radiation simulation, and including Poisson noise, the\npipeline enables the expansion of limited datasets for training deep neural\nnetworks. We validate the proposed pipeline by a qualitative and quantitative\ncomparison against real collimator shadows. Furthermore, it is demonstrated\nthat utilizing simulated data within our deep learning framework not only\nserves as a suitable substitute for actual collimators but also enhances the\ngeneralization performance when applied to real-world data.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "physics.med-ph"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.10308v1",
    "published_date": "2024-11-15 16:04:01 UTC",
    "updated_date": "2024-11-15 16:04:01 UTC"
  },
  {
    "arxiv_id": "2411.10293v3",
    "title": "RETR: Multi-View Radar Detection Transformer for Indoor Perception",
    "authors": [
      "Ryoma Yataka",
      "Adriano Cardace",
      "Pu Perry Wang",
      "Petros Boufounos",
      "Ryuhei Takahashi"
    ],
    "abstract": "Indoor radar perception has seen rising interest due to affordable costs\ndriven by emerging automotive imaging radar developments and the benefits of\nreduced privacy concerns and reliability under hazardous conditions (e.g., fire\nand smoke). However, existing radar perception pipelines fail to account for\ndistinctive characteristics of the multi-view radar setting. In this paper, we\npropose Radar dEtection TRansformer (RETR), an extension of the popular DETR\narchitecture, tailored for multi-view radar perception. RETR inherits the\nadvantages of DETR, eliminating the need for hand-crafted components for object\ndetection and segmentation in the image plane. More importantly, RETR\nincorporates carefully designed modifications such as 1) depth-prioritized\nfeature similarity via a tunable positional encoding (TPE); 2) a tri-plane loss\nfrom both radar and camera coordinates; and 3) a learnable radar-to-camera\ntransformation via reparameterization, to account for the unique multi-view\nradar setting. Evaluated on two indoor radar perception datasets, our approach\noutperforms existing state-of-the-art methods by a margin of 15.38+ AP for\nobject detection and 11.91+ IoU for instance segmentation, respectively. Our\nimplementation is available at\nhttps://github.com/merlresearch/radar-detection-transformer.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "math.DG"
    ],
    "primary_category": "cs.CV",
    "comment": "24 pages, Accepted to NeurIPS 2024, Github Link:\n  https://github.com/merlresearch/radar-detection-transformer",
    "pdf_url": "http://arxiv.org/pdf/2411.10293v3",
    "published_date": "2024-11-15 15:51:25 UTC",
    "updated_date": "2025-01-17 19:06:26 UTC"
  },
  {
    "arxiv_id": "2411.10290v1",
    "title": "The ParClusterers Benchmark Suite (PCBS): A Fine-Grained Analysis of Scalable Graph Clustering",
    "authors": [
      "Shangdi Yu",
      "Jessica Shi",
      "Jamison Meindl",
      "David Eisenstat",
      "Xiaoen Ju",
      "Sasan Tavakkol",
      "Laxman Dhulipala",
      "Jakub Łącki",
      "Vahab Mirrokni",
      "Julian Shun"
    ],
    "abstract": "We introduce the ParClusterers Benchmark Suite (PCBS) -- a collection of\nhighly scalable parallel graph clustering algorithms and benchmarking tools\nthat streamline comparing different graph clustering algorithms and\nimplementations.\n  The benchmark includes clustering algorithms that target a wide range of\nmodern clustering use cases, including community detection, classification, and\ndense subgraph mining.\n  The benchmark toolkit makes it easy to run and evaluate multiple instances of\ndifferent clustering algorithms, which can be useful for fine-tuning the\nperformance of clustering on a given task, and for comparing different\nclustering algorithms based on different metrics of interest, including\nclustering quality and running time.\n  Using PCBS, we evaluate a broad collection of real-world graph clustering\ndatasets. Somewhat surprisingly, we find that the best quality results are\nobtained by algorithms that not included in many popular graph clustering\ntoolkits. The PCBS provides a standardized way to evaluate and judge the\nquality-performance tradeoffs of the active research area of scalable graph\nclustering algorithms. We believe it will help enable fair, accurate, and\nnuanced evaluation of graph clustering algorithms in the future.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG",
      "cs.SI"
    ],
    "primary_category": "cs.DC",
    "comment": "This is a preliminary version of a paper that will appear at VLDB'25",
    "pdf_url": "http://arxiv.org/pdf/2411.10290v1",
    "published_date": "2024-11-15 15:47:32 UTC",
    "updated_date": "2024-11-15 15:47:32 UTC"
  },
  {
    "arxiv_id": "2411.10285v2",
    "title": "Systolic Arrays and Structured Pruning Co-design for Efficient Transformers in Edge Systems",
    "authors": [
      "Pedro Palacios",
      "Rafael Medina",
      "Jean-Luc Rouas",
      "Giovanni Ansaloni",
      "David Atienza"
    ],
    "abstract": "Efficient deployment of resource-intensive transformers on edge devices\nnecessitates cross-stack optimization. We thus study the interrelation between\nstructured pruning and systolic acceleration, matching the size of pruned\nblocks with the systolic array dimensions. In this setting, computations of\npruned weight blocks can be skipped, reducing run-time and energy consumption,\nbut potentially impacting quality of service (QoS). To evaluate the trade-offs\nbetween systolic array size and sparsity opportunities, we present a novel\nco-design framework that integrates algorithmic optimization, system\nsimulation, and hardware design. Targeting speech recognition and machine\ntranslation using transformers as case study, we analyze how configuration\nchoices across the stack affect performance metrics. Results demonstrate that\nstructured pruning on systems featuring systolic array acceleration can\neffectively increase performance, while maintaining high QoS levels. Up to 44%\nsystem-wide speedups due to structured pruning and quantization were measured,\nwith only 1.4% word error rate degradation on the standard LibriSpeech dataset.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "68T50",
      "C.3; B.5.1; I.2.7"
    ],
    "primary_category": "cs.AR",
    "comment": "8 pages, GLSVLSI'25",
    "pdf_url": "http://arxiv.org/pdf/2411.10285v2",
    "published_date": "2024-11-15 15:40:49 UTC",
    "updated_date": "2025-05-12 12:15:01 UTC"
  },
  {
    "arxiv_id": "2411.10279v1",
    "title": "Lateral Movement Detection via Time-aware Subgraph Classification on Authentication Logs",
    "authors": [
      "Jiajun Zhou",
      "Jiacheng Yao",
      "Xuanze Chen",
      "Shanqing Yu",
      "Qi Xuan",
      "Xiaoniu Yang"
    ],
    "abstract": "Lateral movement is a crucial component of advanced persistent threat (APT)\nattacks in networks. Attackers exploit security vulnerabilities in internal\nnetworks or IoT devices, expanding their control after initial infiltration to\nsteal sensitive data or carry out other malicious activities, posing a serious\nthreat to system security. Existing research suggests that attackers generally\nemploy seemingly unrelated operations to mask their malicious intentions,\nthereby evading existing lateral movement detection methods and hiding their\nintrusion traces. In this regard, we analyze host authentication log data from\na graph perspective and propose a multi-scale lateral movement detection\nframework called LMDetect. The main workflow of this framework proceeds as\nfollows: 1) Construct a heterogeneous multigraph from host authentication log\ndata to strengthen the correlations among internal system entities; 2) Design a\ntime-aware subgraph generator to extract subgraphs centered on authentication\nevents from the heterogeneous authentication multigraph; 3) Design a\nmulti-scale attention encoder that leverages both local and global attention to\ncapture hidden anomalous behavior patterns in the authentication subgraphs,\nthereby achieving lateral movement detection. Extensive experiments on two\nreal-world authentication log datasets demonstrate the effectiveness and\nsuperiority of our framework in detecting lateral movement behaviors.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.10279v1",
    "published_date": "2024-11-15 15:35:56 UTC",
    "updated_date": "2024-11-15 15:35:56 UTC"
  },
  {
    "arxiv_id": "2411.10272v2",
    "title": "P$^2$ Law: Scaling Law for Post-Training After Model Pruning",
    "authors": [
      "Xiaodong Chen",
      "Yuxuan Hu",
      "Xiaokang Zhang",
      "Yanling Wang",
      "Cuiping Li",
      "Hong Chen",
      "Jing Zhang"
    ],
    "abstract": "Pruning has become a widely adopted technique for reducing the hardware\nrequirements of large language models (LLMs). To recover model performance\nafter pruning, post-training is commonly employed to mitigate the resulting\nperformance degradation. While post-training benefits from larger datasets,\nonce the dataset size is already substantial, increasing the training data\nprovides only limited performance gains. To balance post-training cost and\nmodel performance, it is necessary to explore the optimal amount of\npost-training data.Through extensive experiments on the Llama-3 and Qwen-2.5\nseries models, pruned using various common pruning methods, we uncover the\nscaling \\textbf{Law} for \\textbf{P}ost-training after model \\textbf{P}runing,\nreferred to as the P$^2$ Law.This law identifies four key factors for\npredicting the pruned model's post-training loss: the model size before\npruning, the number of post-training tokens, the pruning rate, and the model's\nloss before pruning. Moreover, P$^2$ Law can generalize to larger dataset\nsizes, larger model sizes, and higher pruning rates, offering valuable insights\nfor the post-training of pruned LLMs.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.10272v2",
    "published_date": "2024-11-15 15:28:42 UTC",
    "updated_date": "2024-12-16 12:00:34 UTC"
  },
  {
    "arxiv_id": "2411.10257v2",
    "title": "The Unreasonable Effectiveness of Guidance for Diffusion Models",
    "authors": [
      "Tim Kaiser",
      "Nikolas Adaloglou",
      "Markus Kollmann"
    ],
    "abstract": "Guidance is an error-correcting technique used to improve the perceptual\nquality of images generated by diffusion models. Typically, the correction is\nachieved by linear extrapolation, using an auxiliary diffusion model that has\nlower performance than the primary model. Using a 2D toy example, we show that\nit is highly beneficial when the auxiliary model exhibits similar errors as the\nprimary one but stronger. We verify this finding in higher dimensions, where we\nshow that competitive generative performance to state-of-the-art guidance\nmethods can be achieved when the auxiliary model differs from the primary one\nonly by having stronger weight regularization. As an independent contribution,\nwe investigate whether upweighting long-range spatial dependencies improves\nvisual fidelity. The result is a novel guidance method, which we call sliding\nwindow guidance (SWG), that guides the primary model with itself by\nconstraining its receptive field. Intriguingly, SWG aligns better with human\npreferences than state-of-the-art guidance methods while requiring neither\ntraining, architectural modifications, nor class conditioning. The code will be\nreleased.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Preprint. 30 pages, 19 figures in total, including appendix",
    "pdf_url": "http://arxiv.org/pdf/2411.10257v2",
    "published_date": "2024-11-15 15:04:04 UTC",
    "updated_date": "2024-12-20 14:24:30 UTC"
  },
  {
    "arxiv_id": "2411.10255v2",
    "title": "Artificial Intelligence in Pediatric Echocardiography: Exploring Challenges, Opportunities, and Clinical Applications with Explainable AI and Federated Learning",
    "authors": [
      "Mohammed Yaseen Jabarulla",
      "Theodor Uden",
      "Thomas Jack",
      "Philipp Beerbaum",
      "Steffen Oeltze-Jafra"
    ],
    "abstract": "Pediatric heart diseases present a broad spectrum of congenital and acquired\ndiseases. More complex congenital malformations require a differentiated and\nmultimodal decision-making process, usually including echocardiography as a\ncentral imaging method. Artificial intelligence (AI) offers considerable\npromise for clinicians by facilitating automated interpretation of pediatric\nechocardiography data. However, adapting AI technologies for pediatric\nechocardiography analysis has challenges such as limited public data\navailability, data privacy, and AI model transparency. Recently, researchers\nhave focused on disruptive technologies, such as federated learning (FL) and\nexplainable AI (XAI), to improve automatic diagnostic and decision support\nworkflows. This study offers a comprehensive overview of the limitations and\nopportunities of AI in pediatric echocardiography, emphasizing the synergistic\nworkflow and role of XAI and FL, identifying research gaps, and exploring\npotential future developments. Additionally, three relevant clinical use cases\ndemonstrate the functionality of XAI and FL with a focus on (i) view\nrecognition, (ii) disease classification, (iii) segmentation of cardiac\nstructures, and (iv) quantitative assessment of cardiac function.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Submitted for peer review to an Elsevier journal. This version\n  includes revisions to align with the journals guidelines and template. Any\n  footnotes previously present in [V1] referring to Frontiers have been removed\n  for clarity",
    "pdf_url": "http://arxiv.org/pdf/2411.10255v2",
    "published_date": "2024-11-15 15:03:34 UTC",
    "updated_date": "2025-03-27 20:30:00 UTC"
  },
  {
    "arxiv_id": "2411.10234v1",
    "title": "Generative AI in Multimodal User Interfaces: Trends, Challenges, and Cross-Platform Adaptability",
    "authors": [
      "J. Bieniek",
      "M. Rahouti",
      "D. C. Verma"
    ],
    "abstract": "As the boundaries of human computer interaction expand, Generative AI emerges\nas a key driver in reshaping user interfaces, introducing new possibilities for\npersonalized, multimodal and cross-platform interactions. This integration\nreflects a growing demand for more adaptive and intuitive user interfaces that\ncan accommodate diverse input types such as text, voice and video, and deliver\nseamless experiences across devices. This paper explores the integration of\ngenerative AI in modern user interfaces, examining historical developments and\nfocusing on multimodal interaction, cross-platform adaptability and dynamic\npersonalization. A central theme is the interface dilemma, which addresses the\nchallenge of designing effective interactions for multimodal large language\nmodels, assessing the trade-offs between graphical, voice-based and immersive\ninterfaces. The paper further evaluates lightweight frameworks tailored for\nmobile platforms, spotlighting the role of mobile hardware in enabling scalable\nmultimodal AI. Technical and ethical challenges, including context retention,\nprivacy concerns and balancing cloud and on-device processing are thoroughly\nexamined. Finally, the paper outlines future directions such as emotionally\nadaptive interfaces, predictive AI driven user interfaces and real-time\ncollaborative systems, underscoring generative AI's potential to redefine\nadaptive user-centric interfaces across platforms.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "13 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.10234v1",
    "published_date": "2024-11-15 14:49:58 UTC",
    "updated_date": "2024-11-15 14:49:58 UTC"
  },
  {
    "arxiv_id": "2411.10232v2",
    "title": "ColorEdit: Training-free Image-Guided Color editing with diffusion model",
    "authors": [
      "Xingxi Yin",
      "Zhi Li",
      "Jingfeng Zhang",
      "Chenglin Li",
      "Yin Zhang"
    ],
    "abstract": "Text-to-image (T2I) diffusion models, with their impressive generative\ncapabilities, have been adopted for image editing tasks, demonstrating\nremarkable efficacy. However, due to attention leakage and collision between\nthe cross-attention map of the object and the new color attribute from the text\nprompt, text-guided image editing methods may fail to change the color of an\nobject, resulting in a misalignment between the resulting image and the text\nprompt. In this paper, we conduct an in-depth analysis on the process of\ntext-guided image synthesizing and what semantic information different\ncross-attention blocks have learned. We observe that the visual representation\nof an object is determined in the up-block of the diffusion model in the early\nstage of the denoising process, and color adjustment can be achieved through\nvalue matrices alignment in the cross-attention layer. Based on our findings,\nwe propose a straightforward, yet stable, and effective image-guided method to\nmodify the color of an object without requiring any additional fine-tuning or\ntraining. Lastly, we present a benchmark dataset called COLORBENCH, the first\nbenchmark to evaluate the performance of color change methods. Extensive\nexperiments validate the effectiveness of our method in object-level color\nediting and surpass the performance of popular text-guided image editing\napproaches in both synthesized and real images.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.10232v2",
    "published_date": "2024-11-15 14:45:58 UTC",
    "updated_date": "2025-04-30 04:07:56 UTC"
  },
  {
    "arxiv_id": "2411.10231v1",
    "title": "A Low-Resolution Image is Worth 1x1 Words: Enabling Fine Image Super-Resolution with Transformers and TaylorShift",
    "authors": [
      "Sanath Budakegowdanadoddi Nagaraju",
      "Brian Bernhard Moser",
      "Tobias Christian Nauen",
      "Stanislav Frolov",
      "Federico Raue",
      "Andreas Dengel"
    ],
    "abstract": "Transformer-based Super-Resolution (SR) models have recently advanced image\nreconstruction quality, yet challenges remain due to computational complexity\nand an over-reliance on large patch sizes, which constrain fine-grained detail\nenhancement. In this work, we propose TaylorIR to address these limitations by\nutilizing a patch size of 1x1, enabling pixel-level processing in any\ntransformer-based SR model. To address the significant computational demands\nunder the traditional self-attention mechanism, we employ the TaylorShift\nattention mechanism, a memory-efficient alternative based on Taylor series\nexpansion, achieving full token-to-token interactions with linear complexity.\nExperimental results demonstrate that our approach achieves new\nstate-of-the-art SR performance while reducing memory consumption by up to 60%\ncompared to traditional self-attention-based transformers.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.10231v1",
    "published_date": "2024-11-15 14:43:58 UTC",
    "updated_date": "2024-11-15 14:43:58 UTC"
  },
  {
    "arxiv_id": "2411.10224v2",
    "title": "EVOKE: Elevating Chest X-ray Report Generation via Multi-View Contrastive Learning and Patient-Specific Knowledge",
    "authors": [
      "Qiguang Miao",
      "Kang Liu",
      "Zhuoqi Ma",
      "Yunan Li",
      "Xiaolu Kang",
      "Ruixuan Liu",
      "Tianyi Liu",
      "Kun Xie",
      "Zhicheng Jiao"
    ],
    "abstract": "Radiology reports are crucial for planning treatment strategies and\nfacilitating effective doctor-patient communication. However, the manual\ncreation of these reports places a significant burden on radiologists. While\nautomatic radiology report generation presents a promising solution, existing\nmethods often rely on single-view radiographs, which constrain diagnostic\naccuracy. To address this challenge, we propose \\textbf{EVOKE}, a novel chest\nX-ray report generation framework that incorporates multi-view contrastive\nlearning and patient-specific knowledge. Specifically, we introduce a\nmulti-view contrastive learning method that enhances visual representation by\naligning multi-view radiographs with their corresponding report. After that, we\npresent a knowledge-guided report generation module that integrates available\npatient-specific indications (e.g., symptom descriptions) to trigger the\nproduction of accurate and coherent radiology reports. To support research in\nmulti-view report generation, we construct Multi-view CXR and Two-view CXR\ndatasets using publicly available sources. Our proposed EVOKE surpasses recent\nstate-of-the-art methods across multiple datasets, achieving a 2.9\\%\nF\\textsubscript{1} RadGraph improvement on MIMIC-CXR, a 7.3\\% BLEU-1\nimprovement on MIMIC-ABN, a 3.1\\% BLEU-4 improvement on Multi-view CXR, and an\n8.2\\% F\\textsubscript{1,mic-14} CheXbert improvement on Two-view CXR.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "The code is available at https://github.com/mk-runner/EVOKE",
    "pdf_url": "http://arxiv.org/pdf/2411.10224v2",
    "published_date": "2024-11-15 14:38:13 UTC",
    "updated_date": "2025-03-12 09:38:02 UTC"
  },
  {
    "arxiv_id": "2411.10213v1",
    "title": "An Empirical Study on LLM-based Agents for Automated Bug Fixing",
    "authors": [
      "Xiangxin Meng",
      "Zexiong Ma",
      "Pengfei Gao",
      "Chao Peng"
    ],
    "abstract": "Large language models (LLMs) and LLM-based Agents have been applied to fix\nbugs automatically, demonstrating the capability in addressing software defects\nby engaging in development environment interaction, iterative validation and\ncode modification. However, systematic analysis of these agent and non-agent\nsystems remain limited, particularly regarding performance variations among\ntop-performing ones. In this paper, we examine seven proprietary and\nopen-source systems on the SWE-bench Lite benchmark for automated bug fixing.\nWe first assess each system's overall performance, noting instances solvable by\nall or none of these sytems, and explore why some instances are uniquely solved\nby specific system types. We also compare fault localization accuracy at file\nand line levels and evaluate bug reproduction capabilities, identifying\ninstances solvable only through dynamic reproduction. Through analysis, we\nconcluded that further optimization is needed in both the LLM itself and the\ndesign of Agentic flow to improve the effectiveness of the Agent in bug fixing.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.10213v1",
    "published_date": "2024-11-15 14:19:15 UTC",
    "updated_date": "2024-11-15 14:19:15 UTC"
  },
  {
    "arxiv_id": "2411.10504v1",
    "title": "USP-Gaussian: Unifying Spike-based Image Reconstruction, Pose Correction and Gaussian Splatting",
    "authors": [
      "Kang Chen",
      "Jiyuan Zhang",
      "Zecheng Hao",
      "Yajing Zheng",
      "Tiejun Huang",
      "Zhaofei Yu"
    ],
    "abstract": "Spike cameras, as an innovative neuromorphic camera that captures scenes with\nthe 0-1 bit stream at 40 kHz, are increasingly employed for the 3D\nreconstruction task via Neural Radiance Fields (NeRF) or 3D Gaussian Splatting\n(3DGS). Previous spike-based 3D reconstruction approaches often employ a\ncasecased pipeline: starting with high-quality image reconstruction from spike\nstreams based on established spike-to-image reconstruction algorithms, then\nprogressing to camera pose estimation and 3D reconstruction. However, this\ncascaded approach suffers from substantial cumulative errors, where quality\nlimitations of initial image reconstructions negatively impact pose estimation,\nultimately degrading the fidelity of the 3D reconstruction. To address these\nissues, we propose a synergistic optimization framework, \\textbf{USP-Gaussian},\nthat unifies spike-based image reconstruction, pose correction, and Gaussian\nsplatting into an end-to-end framework. Leveraging the multi-view consistency\nafforded by 3DGS and the motion capture capability of the spike camera, our\nframework enables a joint iterative optimization that seamlessly integrates\ninformation between the spike-to-image network and 3DGS. Experiments on\nsynthetic datasets with accurate poses demonstrate that our method surpasses\nprevious approaches by effectively eliminating cascading errors. Moreover, we\nintegrate pose optimization to achieve robust 3D reconstruction in real-world\nscenarios with inaccurate initial poses, outperforming alternative methods by\neffectively reducing noise and preserving fine texture details. Our code, data\nand trained models will be available at\n\\url{https://github.com/chenkang455/USP-Gaussian}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.10504v1",
    "published_date": "2024-11-15 14:15:16 UTC",
    "updated_date": "2024-11-15 14:15:16 UTC"
  },
  {
    "arxiv_id": "2411.10197v2",
    "title": "A logic for reasoning with inconsistent knowledge -- A reformulation using nowadays terminology (2024)",
    "authors": [
      "Nico Roos"
    ],
    "abstract": "In many situations humans have to reason with inconsistent knowledge. These\ninconsistencies may occur due to not fully reliable sources of information. In\norder to reason with inconsistent knowledge, it is not possible to view a set\nof premisses as absolute truths as is done in predicate logic. Viewing the set\nof premisses as a set of assumptions, however, it is possible to deduce useful\nconclusions from an inconsistent set of premisses. In this paper a logic for\nreasoning with inconsistent knowledge is described. This logic is a\ngeneralization of the work of N. Rescher [15]. In the logic a reliability\nrelation is used to choose between incompatible assumptions. These choices are\nonly made when a contradiction is derived. As long as no contradiction is\nderived, the knowledge is assumed to be consistent. This makes it possible to\ndefine an argumentation-based deduction process for the logic. For the logic a\nsemantics based on the ideas of Y. Shoham [22, 23], is defined. It turns out\nthat the semantics for the logic is a preferential semantics according to the\ndefinition S. Kraus, D. Lehmann and M. Magidor [12]. Therefore the logic is a\nlogic of system P and possesses all the properties of an ideal non-monotonic\nlogic.",
    "categories": [
      "cs.AI",
      "68T27, 68T30",
      "I.2.3"
    ],
    "primary_category": "cs.AI",
    "comment": "The original version was published in the Artificial Intelligence\n  journal. This original version uses 'justifications' in the proof system,\n  which we would call nowadays 'arguments'. The current version presents the\n  same results but now using the terminology of an assumption-based\n  argumentation system",
    "pdf_url": "http://arxiv.org/pdf/2411.10197v2",
    "published_date": "2024-11-15 13:53:05 UTC",
    "updated_date": "2024-12-13 15:22:39 UTC"
  },
  {
    "arxiv_id": "2411.12756v1",
    "title": "FedCL-Ensemble Learning: A Framework of Federated Continual Learning with Ensemble Transfer Learning Enhanced for Alzheimer's MRI Classifications while Preserving Privacy",
    "authors": [
      "Rishit Kapoor",
      "Jesher Joshua",
      "Muralidharan Vijayarangan",
      "Natarajan B"
    ],
    "abstract": "This research work introduces a novel approach to the classification of\nAlzheimer's disease by using the advanced deep learning techniques combined\nwith secure data processing methods. This research work primary uses transfer\nlearning models such as ResNet, ImageNet, and VNet to extract high-level\nfeatures from medical image data. Thereafter, these pre-trained models were\nfine-tuned for Alzheimer's related subtle patterns such that the model is\ncapable of robust feature extraction over varying data sources. Further, the\nfederated learning approaches were incorporated to tackle a few other\nchallenges related to classification, aimed to provide better prediction\nperformance and protect data privacy. The proposed model was built using\nfederated learning without sharing sensitive patient data. This way, the\ndecentralized model benefits from the large and diversified dataset that it is\ntrained upon while ensuring confidentiality. The cipher-based encryption\nmechanism is added that allows us to secure the transportation of data and\nfurther ensure the privacy and integrity of patient information throughout\ntraining and classification. The results of the experiments not only help to\nimprove the accuracy of the classification of Alzheimer's but at the same time\nprovides a framework for secure and collaborative analysis of health care data.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "6 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.12756v1",
    "published_date": "2024-11-15 13:49:22 UTC",
    "updated_date": "2024-11-15 13:49:22 UTC"
  },
  {
    "arxiv_id": "2411.10191v2",
    "title": "FengWu-W2S: A deep learning model for seamless weather-to-subseasonal forecast of global atmosphere",
    "authors": [
      "Fenghua Ling",
      "Kang Chen",
      "Jiye Wu",
      "Tao Han",
      "Jing-Jia Luo",
      "Wanli Ouyang",
      "Lei Bai"
    ],
    "abstract": "Seamless forecasting that produces warning information at continuum\ntimescales based on only one system is a long-standing pursuit for\nweather-climate service. While the rapid advancement of deep learning has\ninduced revolutionary changes in classical forecasting field, current efforts\nare still focused on building separate AI models for weather and climate\nforecasts. To explore the seamless forecasting ability based on one AI model,\nwe propose FengWu-Weather to Subseasonal (FengWu-W2S), which builds on the\nFengWu global weather forecast model and incorporates an ocean-atmosphere-land\ncoupling structure along with a diverse perturbation strategy. FengWu-W2S can\ngenerate 6-hourly atmosphere forecasts extending up to 42 days through an\nautoregressive and seamless manner. Our hindcast results demonstrate that\nFengWu-W2S reliably predicts atmospheric conditions out to 3-6 weeks ahead,\nenhancing predictive capabilities for global surface air temperature,\nprecipitation, geopotential height and intraseasonal signals such as the\nMadden-Julian Oscillation (MJO) and North Atlantic Oscillation (NAO). Moreover,\nour ablation experiments on forecast error growth from daily to seasonal\ntimescales reveal potential pathways for developing AI-based integrated system\nfor seamless weather-climate forecasting in the future.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.ao-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "23 pages,8 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.10191v2",
    "published_date": "2024-11-15 13:44:37 UTC",
    "updated_date": "2024-11-20 01:10:15 UTC"
  },
  {
    "arxiv_id": "2411.10184v1",
    "title": "Agentic LLMs in the Supply Chain: Towards Autonomous Multi-Agent Consensus-Seeking",
    "authors": [
      "Valeria Jannelli",
      "Stefan Schoepf",
      "Matthias Bickel",
      "Torbjørn Netland",
      "Alexandra Brintrup"
    ],
    "abstract": "This paper explores how Large Language Models (LLMs) can automate\nconsensus-seeking in supply chain management (SCM), where frequent decisions on\nproblems such as inventory levels and delivery times require coordination among\ncompanies. Traditional SCM relies on human consensus in decision-making to\navoid emergent problems like the bullwhip effect. Some routine consensus\nprocesses, especially those that are time-intensive and costly, can be\nautomated. Existing solutions for automated coordination have faced challenges\ndue to high entry barriers locking out SMEs, limited capabilities, and limited\nadaptability in complex scenarios. However, recent advances in Generative AI,\nparticularly LLMs, show promise in overcoming these barriers. LLMs, trained on\nvast datasets can negotiate, reason, and plan, facilitating near-human-level\nconsensus at scale with minimal entry barriers. In this work, we identify key\nlimitations in existing approaches and propose autonomous LLM agents to address\nthese gaps. We introduce a series of novel, supply chain-specific\nconsensus-seeking frameworks tailored for LLM agents and validate the\neffectiveness of our approach through a case study in inventory management. To\naccelerate progress within the SCM community, we open-source our code,\nproviding a foundation for further advancements in LLM-powered autonomous\nsupply chain solutions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.10184v1",
    "published_date": "2024-11-15 13:33:10 UTC",
    "updated_date": "2024-11-15 13:33:10 UTC"
  },
  {
    "arxiv_id": "2411.10176v1",
    "title": "Let people fail! Exploring the influence of explainable virtual and robotic agents in learning-by-doing tasks",
    "authors": [
      "Marco Matarese",
      "Francesco Rea",
      "Katharina J. Rohlfing",
      "Alessandra Sciutti"
    ],
    "abstract": "Collaborative decision-making with artificial intelligence (AI) agents\npresents opportunities and challenges. While human-AI performance often\nsurpasses that of individuals, the impact of such technology on human behavior\nremains insufficiently understood, primarily when AI agents can provide\njustifiable explanations for their suggestions. This study compares the effects\nof classic vs. partner-aware explanations on human behavior and performance\nduring a learning-by-doing task. Three participant groups were involved: one\ninteracting with a computer, another with a humanoid robot, and a third one\nwithout assistance. Results indicated that partner-aware explanations\ninfluenced participants differently based on the type of artificial agents\ninvolved. With the computer, participants enhanced their task completion times.\nAt the same time, those interacting with the humanoid robot were more inclined\nto follow its suggestions, although they did not reduce their timing.\nInterestingly, participants autonomously performing the learning-by-doing task\ndemonstrated superior knowledge acquisition than those assisted by explainable\nAI (XAI). These findings raise profound questions and have significant\nimplications for automated tutoring and human-AI collaboration.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.10176v1",
    "published_date": "2024-11-15 13:22:04 UTC",
    "updated_date": "2024-11-15 13:22:04 UTC"
  },
  {
    "arxiv_id": "2411.10175v2",
    "title": "The Surprising Ineffectiveness of Pre-Trained Visual Representations for Model-Based Reinforcement Learning",
    "authors": [
      "Moritz Schneider",
      "Robert Krug",
      "Narunas Vaskevicius",
      "Luigi Palmieri",
      "Joschka Boedecker"
    ],
    "abstract": "Visual Reinforcement Learning (RL) methods often require extensive amounts of\ndata. As opposed to model-free RL, model-based RL (MBRL) offers a potential\nsolution with efficient data utilization through planning. Additionally, RL\nlacks generalization capabilities for real-world tasks. Prior work has shown\nthat incorporating pre-trained visual representations (PVRs) enhances sample\nefficiency and generalization. While PVRs have been extensively studied in the\ncontext of model-free RL, their potential in MBRL remains largely unexplored.\nIn this paper, we benchmark a set of PVRs on challenging control tasks in a\nmodel-based RL setting. We investigate the data efficiency, generalization\ncapabilities, and the impact of different properties of PVRs on the performance\nof model-based agents. Our results, perhaps surprisingly, reveal that for MBRL\ncurrent PVRs are not more sample efficient than learning representations from\nscratch, and that they do not generalize better to out-of-distribution (OOD)\nsettings. To explain this, we analyze the quality of the trained dynamics\nmodel. Furthermore, we show that data diversity and network architecture are\nthe most important contributors to OOD generalization performance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Published at the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024). Project page: https://schneimo.com/pvr4mbrl/",
    "pdf_url": "http://arxiv.org/pdf/2411.10175v2",
    "published_date": "2024-11-15 13:21:26 UTC",
    "updated_date": "2025-01-15 15:24:32 UTC"
  },
  {
    "arxiv_id": "2411.10174v1",
    "title": "A Hard-Label Cryptanalytic Extraction of Non-Fully Connected Deep Neural Networks using Side-Channel Attacks",
    "authors": [
      "Benoit Coqueret",
      "Mathieu Carbone",
      "Olivier Sentieys",
      "Gabriel Zaid"
    ],
    "abstract": "During the past decade, Deep Neural Networks (DNNs) proved their value on a\nlarge variety of subjects. However despite their high value and public\naccessibility, the protection of the intellectual property of DNNs is still an\nissue and an emerging research field. Recent works have successfully extracted\nfully-connected DNNs using cryptanalytic methods in hard-label settings,\nproving that it was possible to copy a DNN with high fidelity, i.e., high\nsimilitude in the output predictions. However, the current cryptanalytic\nattacks cannot target complex, i.e., not fully connected, DNNs and are limited\nto special cases of neurons present in deep networks.\n  In this work, we introduce a new end-to-end attack framework designed for\nmodel extraction of embedded DNNs with high fidelity. We describe a new\nblack-box side-channel attack which splits the DNN in several linear parts for\nwhich we can perform cryptanalytic extraction and retrieve the weights in\nhard-label settings. With this method, we are able to adapt cryptanalytic\nextraction, for the first time, to non-fully connected DNNs, while maintaining\na high fidelity. We validate our contributions by targeting several\narchitectures implemented on a microcontroller unit, including a Multi-Layer\nPerceptron (MLP) of 1.7 million parameters and a shortened MobileNetv1. Our\nframework successfully extracts all of these DNNs with high fidelity (88.4% for\nthe MobileNetv1 and 93.2% for the MLP). Furthermore, we use the stolen model to\ngenerate adversarial examples and achieve close to white-box performance on the\nvictim's model (95.8% and 96.7% transfer rate).",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.10174v1",
    "published_date": "2024-11-15 13:19:59 UTC",
    "updated_date": "2024-11-15 13:19:59 UTC"
  },
  {
    "arxiv_id": "2411.10173v1",
    "title": "Semantics and Spatiality of Emergent Communication",
    "authors": [
      "Rotem Ben Zion",
      "Boaz Carmeli",
      "Orr Paradise",
      "Yonatan Belinkov"
    ],
    "abstract": "When artificial agents are jointly trained to perform collaborative tasks\nusing a communication channel, they develop opaque goal-oriented communication\nprotocols. Good task performance is often considered sufficient evidence that\nmeaningful communication is taking place, but existing empirical results show\nthat communication strategies induced by common objectives can be\ncounterintuitive whilst solving the task nearly perfectly. In this work, we\nidentify a goal-agnostic prerequisite to meaningful communication, which we\nterm semantic consistency, based on the idea that messages should have similar\nmeanings across instances. We provide a formal definition for this idea, and\nuse it to compare the two most common objectives in the field of emergent\ncommunication: discrimination and reconstruction. We prove, under mild\nassumptions, that semantically inconsistent communication protocols can be\noptimal solutions to the discrimination task, but not to reconstruction. We\nfurther show that the reconstruction objective encourages a stricter property,\nspatial meaningfulness, which also accounts for the distance between messages.\nExperiments with emergent communication games validate our theoretical results.\nThese findings demonstrate an inherent advantage of distance-based\ncommunication goals, and contextualize previous empirical discoveries.",
    "categories": [
      "cs.AI",
      "cs.MA",
      "I.2.11"
    ],
    "primary_category": "cs.AI",
    "comment": "34 pages, to be published in NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.10173v1",
    "published_date": "2024-11-15 13:19:27 UTC",
    "updated_date": "2024-11-15 13:19:27 UTC"
  },
  {
    "arxiv_id": "2411.10172v1",
    "title": "Increasing the Accessibility of Causal Domain Knowledge via Causal Information Extraction Methods: A Case Study in the Semiconductor Manufacturing Industry",
    "authors": [
      "Houssam Razouk",
      "Leonie Benischke",
      "Daniel Garber",
      "Roman Kern"
    ],
    "abstract": "The extraction of causal information from textual data is crucial in the\nindustry for identifying and mitigating potential failures, enhancing process\nefficiency, prompting quality improvements, and addressing various operational\nchallenges. This paper presents a study on the development of automated methods\nfor causal information extraction from actual industrial documents in the\nsemiconductor manufacturing industry. The study proposes two types of causal\ninformation extraction methods, single-stage sequence tagging (SST) and\nmulti-stage sequence tagging (MST), and evaluates their performance using\nexisting documents from a semiconductor manufacturing company, including\npresentation slides and FMEA (Failure Mode and Effects Analysis) documents. The\nstudy also investigates the effect of representation learning on downstream\ntasks. The presented case study showcases that the proposed MST methods for\nextracting causal information from industrial documents are suitable for\npractical applications, especially for semi structured documents such as FMEAs,\nwith a 93\\% F1 score. Additionally, MST achieves a 73\\% F1 score on texts\nextracted from presentation slides. Finally, the study highlights the\nimportance of choosing a language model that is more aligned with the domain\nand in-domain fine-tuning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "17 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.10172v1",
    "published_date": "2024-11-15 13:18:18 UTC",
    "updated_date": "2024-11-15 13:18:18 UTC"
  },
  {
    "arxiv_id": "2411.10171v2",
    "title": "Imagine-2-Drive: Leveraging High-Fidelity World Models via Multi-Modal Diffusion Policies",
    "authors": [
      "Anant Garg",
      "K Madhava Krishna"
    ],
    "abstract": "World Model-based Reinforcement Learning (WMRL) enables sample efficient\npolicy learning by reducing the need for online interactions which can\npotentially be costly and unsafe, especially for autonomous driving. However,\nexisting world models often suffer from low prediction fidelity and compounding\none-step errors, leading to policy degradation over long horizons.\nAdditionally, traditional RL policies, often deterministic or single\nGaussian-based, fail to capture the multi-modal nature of decision-making in\ncomplex driving scenarios. To address these challenges, we propose\nImagine-2-Drive, a novel WMRL framework that integrates a high-fidelity world\nmodel with a multi-modal diffusion-based policy actor. It consists of two key\ncomponents: DiffDreamer, a diffusion-based world model that generates future\nobservations simultaneously, mitigating error accumulation, and DPA (Diffusion\nPolicy Actor), a diffusion-based policy that models diverse and multi-modal\ntrajectory distributions. By training DPA within DiffDreamer, our method\nenables robust policy learning with minimal online interactions. We evaluate\nour method in CARLA using standard driving benchmarks and demonstrate that it\noutperforms prior world model baselines, improving Route Completion and Success\nRate by 15% and 20% respectively.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Submitted to IROS 2025",
    "pdf_url": "http://arxiv.org/pdf/2411.10171v2",
    "published_date": "2024-11-15 13:17:54 UTC",
    "updated_date": "2025-03-09 18:06:08 UTC"
  },
  {
    "arxiv_id": "2411.10168v1",
    "title": "Evaluating the role of `Constitutions' for learning from AI feedback",
    "authors": [
      "Saskia Redgate",
      "Andrew M. Bean",
      "Adam Mahdi"
    ],
    "abstract": "The growing capabilities of large language models (LLMs) have led to their\nuse as substitutes for human feedback for training and assessing other LLMs.\nThese methods often rely on `constitutions', written guidelines which a critic\nmodel uses to provide feedback and improve generations. We investigate how the\nchoice of constitution affects feedback quality by using four different\nconstitutions to improve patient-centered communication in medical interviews.\nIn pairwise comparisons conducted by 215 human raters, we found that detailed\nconstitutions led to better results regarding emotive qualities. However, none\nof the constitutions outperformed the baseline in learning more\npractically-oriented skills related to information gathering and provision. Our\nfindings indicate that while detailed constitutions should be prioritised,\nthere are possible limitations to the effectiveness of AI feedback as a reward\nsignal in certain areas.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "4 pages, 2 figures. In NeurIPS 2024 Workshop on Language Gamification",
    "pdf_url": "http://arxiv.org/pdf/2411.10168v1",
    "published_date": "2024-11-15 13:16:11 UTC",
    "updated_date": "2024-11-15 13:16:11 UTC"
  },
  {
    "arxiv_id": "2411.10156v5",
    "title": "Mitigating Sycophancy in Decoder-Only Transformer Architectures: Synthetic Data Intervention",
    "authors": [
      "Libo Wang"
    ],
    "abstract": "To address the sycophancy problem caused by reinforcement learning from human\nfeedback in large language models, this research applies synthetic data\nintervention technology to the decoder-only transformer architecture. Based on\nthe research gaps in the existing literature, the researcher designed an\nexperimental process to reduce the tendency of models to cater by generating\ndiversified data, and used GPT4o as an experimental tool for verification. The\nexperiment used 100 true and false questions, and compared the performance of\nthe model trained with synthetic data intervention and the original untrained\nmodel on multiple indicators. The results show that the SDI training model\nsupports the technology in terms of accuracy rate and sycophancy rate and has\nsignificant effectiveness in reducing sycophancy phenomena.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "The data set, experimental process, code and data results have been\n  uploaded to Github repository, the link is\n  https://github.com/brucewang123456789/GeniusTrail/tree/main/Synthetic%20Data%20Intervention",
    "pdf_url": "http://arxiv.org/pdf/2411.10156v5",
    "published_date": "2024-11-15 12:59:46 UTC",
    "updated_date": "2025-03-20 13:29:49 UTC"
  },
  {
    "arxiv_id": "2411.10152v1",
    "title": "Causal Time-Series Synchronization for Multi-Dimensional Forecasting",
    "authors": [
      "Michael Mayr",
      "Georgios C. Chasparis",
      "Josef Küng"
    ],
    "abstract": "The process industry's high expectations for Digital Twins require modeling\napproaches that can generalize across tasks and diverse domains with\npotentially different data dimensions and distributional shifts i.e.,\nFoundational Models. Despite success in natural language processing and\ncomputer vision, transfer learning with (self-) supervised signals for\npre-training general-purpose models is largely unexplored in the context of\nDigital Twins in the process industry due to challenges posed by\nmulti-dimensional time-series data, lagged cause-effect dependencies, complex\ncausal structures, and varying number of (exogenous) variables. We propose a\nnovel channel-dependent pre-training strategy that leverages synchronized\ncause-effect pairs to overcome these challenges by breaking down the\nmulti-dimensional time-series data into pairs of cause-effect variables. Our\napproach focuses on: (i) identifying highly lagged causal relationships using\ndata-driven methods, (ii) synchronizing cause-effect pairs to generate training\nsamples for channel-dependent pre-training, and (iii) evaluating the\neffectiveness of this approach in channel-dependent forecasting. Our\nexperimental results demonstrate significant improvements in forecasting\naccuracy and generalization capability compared to traditional training\nmethods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages",
    "pdf_url": "http://arxiv.org/pdf/2411.10152v1",
    "published_date": "2024-11-15 12:50:57 UTC",
    "updated_date": "2024-11-15 12:50:57 UTC"
  },
  {
    "arxiv_id": "2411.10137v1",
    "title": "Legal Evalutions and Challenges of Large Language Models",
    "authors": [
      "Jiaqi Wang",
      "Huan Zhao",
      "Zhenyuan Yang",
      "Peng Shu",
      "Junhao Chen",
      "Haobo Sun",
      "Ruixi Liang",
      "Shixin Li",
      "Pengcheng Shi",
      "Longjun Ma",
      "Zongjia Liu",
      "Zhengliang Liu",
      "Tianyang Zhong",
      "Yutong Zhang",
      "Chong Ma",
      "Xin Zhang",
      "Tuo Zhang",
      "Tianli Ding",
      "Yudan Ren",
      "Tianming Liu",
      "Xi Jiang",
      "Shu Zhang"
    ],
    "abstract": "In this paper, we review legal testing methods based on Large Language Models\n(LLMs), using the OPENAI o1 model as a case study to evaluate the performance\nof large models in applying legal provisions. We compare current\nstate-of-the-art LLMs, including open-source, closed-source, and legal-specific\nmodels trained specifically for the legal domain. Systematic tests are\nconducted on English and Chinese legal cases, and the results are analyzed in\ndepth. Through systematic testing of legal cases from common law systems and\nChina, this paper explores the strengths and weaknesses of LLMs in\nunderstanding and applying legal texts, reasoning through legal issues, and\npredicting judgments. The experimental results highlight both the potential and\nlimitations of LLMs in legal applications, particularly in terms of challenges\nrelated to the interpretation of legal language and the accuracy of legal\nreasoning. Finally, the paper provides a comprehensive analysis of the\nadvantages and disadvantages of various types of models, offering valuable\ninsights and references for the future application of AI in the legal field.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.10137v1",
    "published_date": "2024-11-15 12:23:12 UTC",
    "updated_date": "2024-11-15 12:23:12 UTC"
  },
  {
    "arxiv_id": "2411.10115v2",
    "title": "Memorization in Attention-only Transformers",
    "authors": [
      "Léo Dana",
      "Muni Sreenivas Pydi",
      "Yann Chevaleyre"
    ],
    "abstract": "Recent research has explored the memorization capacity of multi-head\nattention, but these findings are constrained by unrealistic limitations on the\ncontext size. We present a novel proof for language-based Transformers that\nextends the current hypothesis to any context size. Our approach improves upon\nthe state-of-the-art by achieving more effective exact memorization with an\nattention layer, while also introducing the concept of approximate memorization\nof distributions. Through experimental validation, we demonstrate that our\nproposed bounds more accurately reflect the true memorization capacity of\nlanguage models, and provide a precise comparison with prior work.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "16 pages, 6 figures, submitted to AISTATS 2025,",
    "pdf_url": "http://arxiv.org/pdf/2411.10115v2",
    "published_date": "2024-11-15 11:29:31 UTC",
    "updated_date": "2025-03-10 08:40:41 UTC"
  },
  {
    "arxiv_id": "2411.10109v1",
    "title": "Generative Agent Simulations of 1,000 People",
    "authors": [
      "Joon Sung Park",
      "Carolyn Q. Zou",
      "Aaron Shaw",
      "Benjamin Mako Hill",
      "Carrie Cai",
      "Meredith Ringel Morris",
      "Robb Willer",
      "Percy Liang",
      "Michael S. Bernstein"
    ],
    "abstract": "The promise of human behavioral simulation--general-purpose computational\nagents that replicate human behavior across domains--could enable broad\napplications in policymaking and social science. We present a novel agent\narchitecture that simulates the attitudes and behaviors of 1,052 real\nindividuals--applying large language models to qualitative interviews about\ntheir lives, then measuring how well these agents replicate the attitudes and\nbehaviors of the individuals that they represent. The generative agents\nreplicate participants' responses on the General Social Survey 85% as\naccurately as participants replicate their own answers two weeks later, and\nperform comparably in predicting personality traits and outcomes in\nexperimental replications. Our architecture reduces accuracy biases across\nracial and ideological groups compared to agents given demographic\ndescriptions. This work provides a foundation for new tools that can help\ninvestigate individual and collective behavior.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.10109v1",
    "published_date": "2024-11-15 11:14:34 UTC",
    "updated_date": "2024-11-15 11:14:34 UTC"
  },
  {
    "arxiv_id": "2411.10108v1",
    "title": "Identifying Key Drivers of Heatwaves: A Novel Spatio-Temporal Framework for Extreme Event Detection",
    "authors": [
      "J. Pérez-Aracil",
      "C. Peláez-Rodríguez",
      "Ronan McAdam",
      "Antonello Squintu",
      "Cosmin M. Marina",
      "Eugenio Lorente-Ramos",
      "Niklas Luther",
      "Veronica Torralba",
      "Enrico Scoccimarro",
      "Leone Cavicchia",
      "Matteo Giuliani",
      "Eduardo Zorita",
      "Felicitas Hansen",
      "David Barriopedro",
      "Ricardo Garcia-Herrera",
      "Pedro A. Gutiérrez",
      "Jürg Luterbacher",
      "Elena Xoplaki",
      "Andrea Castelletti",
      "S. Salcedo-Sanz"
    ],
    "abstract": "Heatwaves (HWs) are extreme atmospheric events that produce significant\nsocietal and environmental impacts. Predicting these extreme events remains\nchallenging, as their complex interactions with large-scale atmospheric and\nclimatic variables are difficult to capture with traditional statistical and\ndynamical models. This work presents a general method for driver identification\nin extreme climate events. A novel framework (STCO-FS) is proposed to identify\nkey immediate (short-term) HW drivers by combining clustering algorithms with\nan ensemble evolutionary algorithm. The framework analyzes spatio-temporal\ndata, reduces dimensionality by grouping similar geographical nodes for each\nvariable, and develops driver selection in spatial and temporal domains,\nidentifying the best time lags between predictive variables and HW occurrences.\nThe proposed method has been applied to analyze HWs in the Adda river basin in\nItaly. The approach effectively identifies significant variables influencing\nHWs in this region. This research can potentially enhance our understanding of\nHW drivers and predictability.",
    "categories": [
      "physics.ao-ph",
      "cs.AI"
    ],
    "primary_category": "physics.ao-ph",
    "comment": "28 pages, 10 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2411.10108v1",
    "published_date": "2024-11-15 11:09:34 UTC",
    "updated_date": "2024-11-15 11:09:34 UTC"
  },
  {
    "arxiv_id": "2411.10500v1",
    "title": "Edge-Only Universal Adversarial Attacks in Distributed Learning",
    "authors": [
      "Giulio Rossolini",
      "Tommaso Baldi",
      "Alessandro Biondi",
      "Giorgio Buttazzo"
    ],
    "abstract": "Distributed learning frameworks, which partition neural network models across\nmultiple computing nodes, enhance efficiency in collaborative edge-cloud\nsystems but may also introduce new vulnerabilities. In this work, we explore\nthe feasibility of generating universal adversarial attacks when an attacker\nhas access to the edge part of the model only, which consists in the first\nnetwork layers. Unlike traditional universal adversarial perturbations (UAPs)\nthat require full model knowledge, our approach shows that adversaries can\ninduce effective mispredictions in the unknown cloud part by leveraging key\nfeatures on the edge side. Specifically, we train lightweight classifiers from\nintermediate features available at the edge, i.e., before the split point, and\nuse them in a novel targeted optimization to craft effective UAPs. Our results\non ImageNet demonstrate strong attack transferability to the unknown cloud\npart. Additionally, we analyze the capability of an attacker to achieve\ntargeted adversarial effect with edge-only knowledge, revealing intriguing\nbehaviors. By introducing the first adversarial attacks with edge-only\nknowledge in split inference, this work underscores the importance of\naddressing partial model access in adversarial robustness, encouraging further\nresearch in this area.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.10500v1",
    "published_date": "2024-11-15 11:06:24 UTC",
    "updated_date": "2024-11-15 11:06:24 UTC"
  },
  {
    "arxiv_id": "2411.10100v1",
    "title": "Multi-Task Adversarial Variational Autoencoder for Estimating Biological Brain Age with Multimodal Neuroimaging",
    "authors": [
      "Muhammad Usman",
      "Azka Rehman",
      "Abdullah Shahid",
      "Abd Ur Rehman",
      "Sung-Min Gho",
      "Aleum Lee",
      "Tariq M. Khan",
      "Imran Razzak"
    ],
    "abstract": "Despite advances in deep learning for estimating brain age from structural\nMRI data, incorporating functional MRI data is challenging due to its complex\nstructure and the noisy nature of functional connectivity measurements. To\naddress this, we present the Multitask Adversarial Variational Autoencoder, a\ncustom deep learning framework designed to improve brain age predictions\nthrough multimodal MRI data integration. This model separates latent variables\ninto generic and unique codes, isolating shared and modality-specific features.\nBy integrating multitask learning with sex classification as an additional\ntask, the model captures sex-specific aging patterns. Evaluated on the OpenBHB\ndataset, a large multisite brain MRI collection, the model achieves a mean\nabsolute error of 2.77 years, outperforming traditional methods. This success\npositions M-AVAE as a powerful tool for metaverse-based healthcare applications\nin brain age estimation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.10100v1",
    "published_date": "2024-11-15 10:50:36 UTC",
    "updated_date": "2024-11-15 10:50:36 UTC"
  },
  {
    "arxiv_id": "2411.10091v1",
    "title": "AI and the Future of Work in Africa White Paper",
    "authors": [
      "Jacki O'Neill",
      "Vukosi Marivate",
      "Barbara Glover",
      "Winnie Karanu",
      "Girmaw Abebe Tadesse",
      "Akua Gyekye",
      "Anne Makena",
      "Wesley Rosslyn-Smith",
      "Matthew Grollnek",
      "Charity Wayua",
      "Rehema Baguma",
      "Angel Maduke",
      "Sarah Spencer",
      "Daniel Kandie",
      "Dennis Ndege Maari",
      "Natasha Mutangana",
      "Maxamed Axmed",
      "Nyambura Kamau",
      "Muhammad Adamu",
      "Frank Swaniker",
      "Brian Gatuguti",
      "Jonathan Donner",
      "Mark Graham",
      "Janet Mumo",
      "Caroline Mbindyo",
      "Charlette N'Guessan",
      "Irene Githinji",
      "Lesego Makhafola",
      "Sean Kruger",
      "Olivia Etyang",
      "Mulang Onando",
      "Joe Sevilla",
      "Nanjira Sambuli",
      "Martin Mbaya",
      "Paul Breloff",
      "Gideon M. Anapey",
      "Tebogo L. Mogaleemang",
      "Tiyani Nghonyama",
      "Muthoni Wanyoike",
      "Bhekani Mbuli",
      "Lawrence Nderu",
      "Wambui Nyabero",
      "Uzma Alam",
      "Kayode Olaleye",
      "Caroline Njenga",
      "Abigail Sellen",
      "David Kairo",
      "Rutendo Chabikwa",
      "Najeeb G. Abdulhamid",
      "Ketry Kubasu",
      "Chinasa T. Okolo",
      "Eugenia Akpo",
      "Joel Budu",
      "Issa Karambal",
      "Joseph Berkoh",
      "William Wasswa",
      "Muchai Njagwi",
      "Rob Burnet",
      "Loise Ochanda",
      "Hanlie de Bod",
      "Elizabeth Ankrah",
      "Selemani Kinyunyu",
      "Mutembei Kariuki",
      "Angel Maduke",
      "Kizito Kiyimba",
      "Farida Eleshin",
      "Lillian Secelela Madeje",
      "Catherine Muraga",
      "Ida Nganga",
      "Judy Gichoya",
      "Tabbz Maina",
      "Samuel Maina",
      "Muchai Mercy",
      "Millicent Ochieng",
      "Stephanie Nyairo"
    ],
    "abstract": "This white paper is the output of a multidisciplinary workshop in Nairobi\n(Nov 2023). Led by a cross-organisational team including Microsoft Research,\nNEPAD, Lelapa AI, and University of Oxford. The workshop brought together\ndiverse thought-leaders from various sectors and backgrounds to discuss the\nimplications of Generative AI for the future of work in Africa. Discussions\ncentred around four key themes: Macroeconomic Impacts; Jobs, Skills and Labour\nMarkets; Workers' Perspectives and Africa-Centris AI Platforms. The white paper\nprovides an overview of the current state and trends of generative AI and its\napplications in different domains, as well as the challenges and risks\nassociated with its adoption and regulation. It represents a diverse set of\nperspectives to create a set of insights and recommendations which aim to\nencourage debate and collaborative action towards creating a dignified future\nof work for everyone across Africa.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.10091v1",
    "published_date": "2024-11-15 10:34:59 UTC",
    "updated_date": "2024-11-15 10:34:59 UTC"
  },
  {
    "arxiv_id": "2411.10087v3",
    "title": "PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse",
    "authors": [
      "Einari Vaaras",
      "Manu Airaksinen",
      "Okko Räsänen"
    ],
    "abstract": "Self-supervised learning (SSL) is a data-driven learning approach that\nutilizes the innate structure of the data to guide the learning process. In\ncontrast to supervised learning, which depends on external labels, SSL utilizes\nthe inherent characteristics of the data to produce its own supervisory signal.\nHowever, one frequent issue with SSL methods is representation collapse, where\nthe model outputs a constant input-invariant feature representation. This issue\nhinders the potential application of SSL methods to new data modalities, as\ntrying to avoid representation collapse wastes researchers' time and effort.\nThis paper introduces a novel SSL algorithm for time-series data called\nPrediction of Functionals from Masked Latents (PFML). Instead of predicting\nmasked input signals or their latent representations directly, PFML operates by\npredicting statistical functionals of the input signal corresponding to masked\nembeddings, given a sequence of unmasked embeddings. The algorithm is designed\nto avoid representation collapse, rendering it straightforwardly applicable to\ndifferent time-series data domains, such as novel sensor modalities in clinical\ndata. We demonstrate the effectiveness of PFML through complex, real-life\nclassification tasks across three different data modalities: infant posture and\nmovement classification from multi-sensor inertial measurement unit data,\nemotion recognition from speech data, and sleep stage classification from EEG\ndata. The results show that PFML is superior to a conceptually similar SSL\nmethod and a contrastive learning-based SSL method. Additionally, PFML is on\npar with the current state-of-the-art SSL method, while also being conceptually\nsimpler and without suffering from representation collapse.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted for publication in IEEE Access",
    "pdf_url": "http://arxiv.org/pdf/2411.10087v3",
    "published_date": "2024-11-15 10:16:38 UTC",
    "updated_date": "2025-04-08 09:13:33 UTC"
  },
  {
    "arxiv_id": "2411.10084v1",
    "title": "Adapting the Biological SSVEP Response to Artificial Neural Networks",
    "authors": [
      "Emirhan Böge",
      "Yasemin Gunindi",
      "Erchan Aptoula",
      "Nihan Alp",
      "Huseyin Ozkan"
    ],
    "abstract": "Neuron importance assessment is crucial for understanding the inner workings\nof artificial neural networks (ANNs) and improving their interpretability and\nefficiency. This paper introduces a novel approach to neuron significance\nassessment inspired by frequency tagging, a technique from neuroscience. By\napplying sinusoidal contrast modulation to image inputs and analyzing resulting\nneuron activations, this method enables fine-grained analysis of a network's\ndecision-making processes. Experiments conducted with a convolutional neural\nnetwork for image classification reveal notable harmonics and intermodulations\nin neuron-specific responses under part-based frequency tagging. These findings\nsuggest that ANNs exhibit behavior akin to biological brains in tuning to\nflickering frequencies, thereby opening avenues for neuron/filter importance\nassessment through frequency tagging. The proposed method holds promise for\napplications in network pruning, and model interpretability, contributing to\nthe advancement of explainable artificial intelligence and addressing the lack\nof transparency in neural networks. Future research directions include\ndeveloping novel loss functions to encourage biologically plausible behavior in\nANNs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.10084v1",
    "published_date": "2024-11-15 10:02:48 UTC",
    "updated_date": "2024-11-15 10:02:48 UTC"
  },
  {
    "arxiv_id": "2411.10072v1",
    "title": "Real-Time AI-Driven People Tracking and Counting Using Overhead Cameras",
    "authors": [
      "Ishrath Ahamed",
      "Chamith Dilshan Ranathunga",
      "Dinuka Sandun Udayantha",
      "Benny Kai Kiat Ng",
      "Chau Yuen"
    ],
    "abstract": "Accurate people counting in smart buildings and intelligent transportation\nsystems is crucial for energy management, safety protocols, and resource\nallocation. This is especially critical during emergencies, where precise\noccupant counts are vital for safe evacuation. Existing methods struggle with\nlarge crowds, often losing accuracy with even a few additional people. To\naddress this limitation, this study proposes a novel approach combining a new\nobject tracking algorithm, a novel counting algorithm, and a fine-tuned object\ndetection model. This method achieves 97% accuracy in real-time people counting\nwith a frame rate of 20-27 FPS on a low-power edge computer.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This paper is accepted to IEEE Region 10 conference (TENCON) 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.10072v1",
    "published_date": "2024-11-15 09:37:49 UTC",
    "updated_date": "2024-11-15 09:37:49 UTC"
  },
  {
    "arxiv_id": "2411.10071v1",
    "title": "Evidential Federated Learning for Skin Lesion Image Classification",
    "authors": [
      "Rutger Hendrix",
      "Federica Proietto Salanitri",
      "Concetto Spampinato",
      "Simone Palazzo",
      "Ulas Bagci"
    ],
    "abstract": "We introduce FedEvPrompt, a federated learning approach that integrates\nprinciples of evidential deep learning, prompt tuning, and knowledge\ndistillation for distributed skin lesion classification. FedEvPrompt leverages\ntwo sets of prompts: b-prompts (for low-level basic visual knowledge) and\nt-prompts (for task-specific knowledge) prepended to frozen pre-trained Vision\nTransformer (ViT) models trained in an evidential learning framework to\nmaximize class evidences. Crucially, knowledge sharing across federation\nclients is achieved only through knowledge distillation on attention maps\ngenerated by the local ViT models, ensuring enhanced privacy preservation\ncompared to traditional parameter or synthetic image sharing methodologies.\nFedEvPrompt is optimized within a round-based learning paradigm, where each\nround involves training local models followed by attention maps sharing with\nall federation clients. Experimental validation conducted in a real distributed\nsetting, on the ISIC2019 dataset, demonstrates the superior performance of\nFedEvPrompt against baseline federated learning algorithms and knowledge\ndistillation methods, without sharing model parameters. In conclusion,\nFedEvPrompt offers a promising approach for federated learning, effectively\naddressing challenges such as data heterogeneity, imbalance, privacy\npreservation, and knowledge sharing.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Published as a conference paper at ICPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.10071v1",
    "published_date": "2024-11-15 09:34:28 UTC",
    "updated_date": "2024-11-15 09:34:28 UTC"
  },
  {
    "arxiv_id": "2411.10063v1",
    "title": "Federated Domain Generalization via Prompt Learning and Aggregation",
    "authors": [
      "Shuai Gong",
      "Chaoran Cui",
      "Chunyun Zhang",
      "Wenna Wang",
      "Xiushan Nie",
      "Lei Zhu"
    ],
    "abstract": "Federated domain generalization (FedDG) aims to improve the global model\ngeneralization in unseen domains by addressing data heterogeneity under\nprivacy-preserving constraints. A common strategy in existing FedDG studies\ninvolves sharing domain-specific knowledge among clients, such as spectrum\ninformation, class prototypes, and data styles. However, this knowledge is\nextracted directly from local client samples, and sharing such sensitive\ninformation poses a potential risk of data leakage, which might not fully meet\nthe requirements of FedDG. In this paper, we introduce prompt learning to adapt\npre-trained vision-language models (VLMs) in the FedDG scenario, and leverage\nlocally learned prompts as a more secure bridge to facilitate knowledge\ntransfer among clients. Specifically, we propose a novel FedDG framework\nthrough Prompt Learning and AggregatioN (PLAN), which comprises two training\nstages to collaboratively generate local prompts and global prompts at each\nfederated round. First, each client performs both text and visual prompt\nlearning using their own data, with local prompts indirectly synchronized by\nregarding the global prompts as a common reference. Second, all domain-specific\nlocal prompts are exchanged among clients and selectively aggregated into the\nglobal prompts using lightweight attention-based aggregators. The global\nprompts are finally applied to adapt VLMs to unseen target domains. As our PLAN\nframework requires training only a limited number of prompts and lightweight\naggregators, it offers notable advantages in computational and communication\nefficiency for FedDG. Extensive experiments demonstrate the superior\ngeneralization ability of PLAN across four benchmark datasets.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "This work has been submitted to the IEEE for possible publication",
    "pdf_url": "http://arxiv.org/pdf/2411.10063v1",
    "published_date": "2024-11-15 09:26:00 UTC",
    "updated_date": "2024-11-15 09:26:00 UTC"
  },
  {
    "arxiv_id": "2411.10057v1",
    "title": "KuaiFormer: Transformer-Based Retrieval at Kuaishou",
    "authors": [
      "Chi Liu",
      "Jiangxia Cao",
      "Rui Huang",
      "Kai Zheng",
      "Qiang Luo",
      "Kun Gai",
      "Guorui Zhou"
    ],
    "abstract": "In large-scale content recommendation systems, retrieval serves as the\ninitial stage in the pipeline, responsible for selecting thousands of candidate\nitems from billions of options to pass on to ranking modules. Traditionally,\nthe dominant retrieval method has been Embedding-Based Retrieval (EBR) using a\nDeep Neural Network (DNN) dual-tower structure. However, applying transformer\nin retrieval tasks has been the focus of recent research, though real-world\nindustrial deployment still presents significant challenges. In this paper, we\nintroduce KuaiFormer, a novel transformer-based retrieval framework deployed in\na large-scale content recommendation system. KuaiFormer fundamentally redefines\nthe retrieval process by shifting from conventional score estimation tasks\n(such as click-through rate estimate) to a transformer-driven Next Action\nPrediction paradigm. This shift enables more effective real-time interest\nacquisition and multi-interest extraction, significantly enhancing retrieval\nperformance. KuaiFormer has been successfully integrated into Kuaishou App's\nshort-video recommendation system since May 2024, serving over 400 million\ndaily active users and resulting in a marked increase in average daily usage\ntime of Kuaishou users. We provide insights into both the technical and\nbusiness aspects of deploying transformer in large-scale recommendation\nsystems, addressing practical challenges encountered during industrial\nimplementation. Our findings offer valuable guidance for engineers and\nresearchers aiming to leverage transformer models to optimize large-scale\ncontent recommendation systems.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.10057v1",
    "published_date": "2024-11-15 09:20:46 UTC",
    "updated_date": "2024-11-15 09:20:46 UTC"
  },
  {
    "arxiv_id": "2411.10055v1",
    "title": "Towards unearthing neglected climate innovations from scientific literature using Large Language Models",
    "authors": [
      "César Quilodrán-Casas",
      "Christopher Waite",
      "Nicole Alhadeff",
      "Diyona Dsouza",
      "Cathal Hughes",
      "Larissa Kunstel-Tabet",
      "Alyssa Gilbert"
    ],
    "abstract": "Climate change poses an urgent global threat, needing the rapid\nidentification and deployment of innovative solutions. We hypothesise that many\nof these solutions already exist within scientific literature but remain\nunderutilised. To address this gap, this study employs a curated dataset\nsourced from OpenAlex, a comprehensive repository of scientific papers.\nUtilising Large Language Models (LLMs), such as GPT4-o from OpenAI, we evaluate\ntitle-abstract pairs from scientific papers on seven dimensions, covering\nclimate change mitigation potential, stage of technological development, and\nreadiness for deployment. The outputs of the language models are then compared\nwith human evaluations to assess their effectiveness in identifying promising\nyet overlooked climate innovations. Our findings suggest that these LLM-based\nmodels can effectively augment human expertise, uncovering climate solutions\nthat are potentially impactful but with far greater speed, throughput and\nconsistency. Here, we focused on UK-based solutions, but the workflow is\nregion-agnostic. This work contributes to the discovery of neglected\ninnovations in scientific literature and demonstrates the potential of AI in\nenhancing climate action strategies.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "10 pages. Accepted in the LatinX in AI workshop at NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.10055v1",
    "published_date": "2024-11-15 09:17:40 UTC",
    "updated_date": "2024-11-15 09:17:40 UTC"
  },
  {
    "arxiv_id": "2411.10053v1",
    "title": "That Chip Has Sailed: A Critique of Unfounded Skepticism Around AI for Chip Design",
    "authors": [
      "Anna Goldie",
      "Azalia Mirhoseini",
      "Jeff Dean"
    ],
    "abstract": "In 2020, we introduced a deep reinforcement learning method capable of\ngenerating superhuman chip layouts, which we then published in Nature and\nopen-sourced on GitHub. AlphaChip has inspired an explosion of work on AI for\nchip design, and has been deployed in state-of-the-art chips across Alphabet\nand extended by external chipmakers. Even so, a non-peer-reviewed invited paper\nat ISPD 2023 questioned its performance claims, despite failing to run our\nmethod as described in Nature. For example, it did not pre-train the RL method\n(removing its ability to learn from prior experience), used substantially fewer\ncompute resources (20x fewer RL experience collectors and half as many GPUs),\ndid not train to convergence (standard practice in machine learning), and\nevaluated on test cases that are not representative of modern chips. Recently,\nIgor Markov published a meta-analysis of three papers: our peer-reviewed Nature\npaper, the non-peer-reviewed ISPD paper, and Markov's own unpublished paper\n(though he does not disclose that he co-authored it). Although AlphaChip has\nalready achieved widespread adoption and impact, we publish this response to\nensure that no one is wrongly discouraged from innovating in this impactful\narea.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.10053v1",
    "published_date": "2024-11-15 09:11:10 UTC",
    "updated_date": "2024-11-15 09:11:10 UTC"
  },
  {
    "arxiv_id": "2411.10050v1",
    "title": "Jal Anveshak: Prediction of fishing zones using fine-tuned LlaMa 2",
    "authors": [
      "Arnav Mejari",
      "Maitreya Vaghulade",
      "Paarshva Chitaliya",
      "Arya Telang",
      "Lynette D'mello"
    ],
    "abstract": "In recent years, the global and Indian government efforts in monitoring and\ncollecting data related to the fisheries industry have witnessed significant\nadvancements. Despite this wealth of data, there exists an untapped potential\nfor leveraging artificial intelligence based technological systems to benefit\nIndian fishermen in coastal areas. To fill this void in the Indian technology\necosystem, the authors introduce Jal Anveshak. This is an application framework\nwritten in Dart and Flutter that uses a Llama 2 based Large Language Model\nfine-tuned on pre-processed and augmented government data related to fishing\nyield and availability. Its main purpose is to help Indian fishermen safely get\nthe maximum yield of fish from coastal areas and to resolve their fishing\nrelated queries in multilingual and multimodal ways.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.10050v1",
    "published_date": "2024-11-15 09:05:03 UTC",
    "updated_date": "2024-11-15 09:05:03 UTC"
  },
  {
    "arxiv_id": "2411.10048v1",
    "title": "Physics-informed neural networks need a physicist to be accurate: the case of mass and heat transport in Fischer-Tropsch catalyst particles",
    "authors": [
      "Tymofii Nikolaienko",
      "Harshil Patel",
      "Aniruddha Panda",
      "Subodh Madhav Joshi",
      "Stanislav Jaso",
      "Kaushic Kalyanaraman"
    ],
    "abstract": "Physics-Informed Neural Networks (PINNs) have emerged as an influential\ntechnology, merging the swift and automated capabilities of machine learning\nwith the precision and dependability of simulations grounded in theoretical\nphysics. PINNs are often employed to solve algebraic or differential equations\nto replace some or even all steps of multi-stage computational workflows,\nleading to their significant speed-up. However, wide adoption of PINNs is still\nhindered by reliability issues, particularly at extreme ends of the input\nparameter ranges. In this study, we demonstrate this in the context of a system\nof coupled non-linear differential reaction-diffusion and heat transfer\nequations related to Fischer-Tropsch synthesis, which are solved by a\nfinite-difference method with a PINN used in evaluating their source terms. It\nis shown that the testing strategies traditionally used to assess the accuracy\nof neural networks as function approximators can overlook the peculiarities\nwhich ultimately cause instabilities of the finite-difference solver. We\npropose a domain knowledge-based modifications to the PINN architecture\nensuring its correct asymptotic behavior. When combined with an improved\nnumerical scheme employed as an initial guess generator, the proposed\nmodifications are shown to recover the overall stability of the simulations,\nwhile preserving the speed-up brought by PINN as the workflow component. We\ndiscuss the possible applications of the proposed hybrid transport equation\nsolver in context of chemical reactors simulations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.chem-ph",
      "physics.comp-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.10048v1",
    "published_date": "2024-11-15 08:55:31 UTC",
    "updated_date": "2024-11-15 08:55:31 UTC"
  },
  {
    "arxiv_id": "2411.10036v1",
    "title": "Rethinking Normalization Strategies and Convolutional Kernels for Multimodal Image Fusion",
    "authors": [
      "Dan He",
      "Guofen Wang",
      "Weisheng Li",
      "Yucheng Shu",
      "Wenbo Li",
      "Lijian Yang",
      "Yuping Huang",
      "Feiyan Li"
    ],
    "abstract": "Multimodal image fusion (MMIF) aims to integrate information from different\nmodalities to obtain a comprehensive image, aiding downstream tasks. However,\nexisting methods tend to prioritize natural image fusion and focus on\ninformation complementary and network training strategies. They ignore the\nessential distinction between natural and medical image fusion and the\ninfluence of underlying components. This paper dissects the significant\ndifferences between the two tasks regarding fusion goals, statistical\nproperties, and data distribution. Based on this, we rethink the suitability of\nthe normalization strategy and convolutional kernels for end-to-end\nMMIF.Specifically, this paper proposes a mixture of instance normalization and\ngroup normalization to preserve sample independence and reinforce intrinsic\nfeature correlation.This strategy promotes the potential of enriching feature\nmaps, thus boosting fusion performance. To this end, we further introduce the\nlarge kernel convolution, effectively expanding receptive fields and enhancing\nthe preservation of image detail. Moreover, the proposed multipath adaptive\nfusion module recalibrates the decoder input with features of various scales\nand receptive fields, ensuring the transmission of crucial information.\nExtensive experiments demonstrate that our method exhibits state-of-the-art\nperformance in multiple fusion tasks and significantly improves downstream\napplications. The code is available at https://github.com/HeDan-11/LKC-FUNet.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.10036v1",
    "published_date": "2024-11-15 08:36:24 UTC",
    "updated_date": "2024-11-15 08:36:24 UTC"
  },
  {
    "arxiv_id": "2411.10032v1",
    "title": "VMID: A Multimodal Fusion LLM Framework for Detecting and Identifying Misinformation of Short Videos",
    "authors": [
      "Weihao Zhong",
      "Yinhao Xiao",
      "Minghui Xu",
      "Xiuzhen Cheng"
    ],
    "abstract": "Short video platforms have become important channels for news dissemination,\noffering a highly engaging and immediate way for users to access current events\nand share information. However, these platforms have also emerged as\nsignificant conduits for the rapid spread of misinformation, as fake news and\nrumors can leverage the visual appeal and wide reach of short videos to\ncirculate extensively among audiences. Existing fake news detection methods\nmainly rely on single-modal information, such as text or images, or apply only\nbasic fusion techniques, limiting their ability to handle the complex,\nmulti-layered information inherent in short videos. To address these\nlimitations, this paper presents a novel fake news detection method based on\nmultimodal information, designed to identify misinformation through a\nmulti-level analysis of video content. This approach effectively utilizes\ndifferent modal representations to generate a unified textual description,\nwhich is then fed into a large language model for comprehensive evaluation. The\nproposed framework successfully integrates multimodal features within videos,\nsignificantly enhancing the accuracy and reliability of fake news detection.\nExperimental results demonstrate that the proposed approach outperforms\nexisting models in terms of accuracy, robustness, and utilization of multimodal\ninformation, achieving an accuracy of 90.93%, which is significantly higher\nthan the best baseline model (SV-FEND) at 81.05%. Furthermore, case studies\nprovide additional evidence of the effectiveness of the approach in accurately\ndistinguishing between fake news, debunking content, and real incidents,\nhighlighting its reliability and robustness in real-world applications.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "arXiv admin note: text overlap with arXiv:2211.10973 by other authors",
    "pdf_url": "http://arxiv.org/pdf/2411.10032v1",
    "published_date": "2024-11-15 08:20:26 UTC",
    "updated_date": "2024-11-15 08:20:26 UTC"
  },
  {
    "arxiv_id": "2411.10028v2",
    "title": "MOT FCG++: Enhanced Representation of Spatio-temporal Motion and Appearance Features",
    "authors": [
      "Yanzhao Fang"
    ],
    "abstract": "The goal of multi-object tracking (MOT) is to detect and track all objects in\na scene across frames, while maintaining a unique identity for each object.\nMost existing methods rely on the spatial-temporal motion features and\nappearance embedding features of the detected objects in consecutive frames.\nEffectively and robustly representing the spatial and appearance features of\nlong trajectories has become a critical factor affecting the performance of\nMOT. We propose a novel approach for appearance and spatial-temporal motion\nfeature representation, improving upon the hierarchical clustering association\nmethod MOT FCG. For spatialtemporal motion features, we first propose Diagonal\nModulated GIoU, which more accurately represents the relationship between the\nposition and shape of the objects. Second, Mean Constant Velocity Modeling is\nproposed to reduce the effect of observation noise on target motion state\nestimation. For appearance features, we utilize a dynamic appearance\nrepresentation that incorporates confidence information, enabling the\ntrajectory appearance features to be more robust and global. Based on the\nbaseline model MOT FCG, we have realized further improvements in the\nperformance of all. we achieved 63.1 HOTA, 76.9 MOTA and 78.2 IDF1 on the MOT17\ntest set, and also achieved competitive performance on the MOT20 and DanceTrack\nsets.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "14 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.10028v2",
    "published_date": "2024-11-15 08:17:05 UTC",
    "updated_date": "2024-11-21 07:06:53 UTC"
  },
  {
    "arxiv_id": "2411.13578v1",
    "title": "COOD: Concept-based Zero-shot OOD Detection",
    "authors": [
      "Zhendong Liu",
      "Yi Nian",
      "Henry Peng Zou",
      "Li Li",
      "Xiyang Hu",
      "Yue Zhao"
    ],
    "abstract": "How can models effectively detect out-of-distribution (OOD) samples in\ncomplex, multi-label settings without extensive retraining? Existing OOD\ndetection methods struggle to capture the intricate semantic relationships and\nlabel co-occurrences inherent in multi-label settings, often requiring large\namounts of training data and failing to generalize to unseen label\ncombinations. While large language models have revolutionized zero-shot OOD\ndetection, they primarily focus on single-label scenarios, leaving a critical\ngap in handling real-world tasks where samples can be associated with multiple\ninterdependent labels. To address these challenges, we introduce COOD, a novel\nzero-shot multi-label OOD detection framework. COOD leverages pre-trained\nvision-language models, enhancing them with a concept-based label expansion\nstrategy and a new scoring function. By enriching the semantic space with both\npositive and negative concepts for each label, our approach models complex\nlabel dependencies, precisely differentiating OOD samples without the need for\nadditional training. Extensive experiments demonstrate that our method\nsignificantly outperforms existing approaches, achieving approximately 95%\naverage AUROC on both VOC and COCO datasets, while maintaining robust\nperformance across varying numbers of labels and different types of OOD\nsamples.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.13578v1",
    "published_date": "2024-11-15 08:15:48 UTC",
    "updated_date": "2024-11-15 08:15:48 UTC"
  },
  {
    "arxiv_id": "2411.10015v1",
    "title": "MicroCrackAttentionNeXt: Advancing Microcrack Detection in Wave Field Analysis Using Deep Neural Networks through Feature Visualization",
    "authors": [
      "Fatahlla Moreh",
      "Yusuf Hasan",
      "Bilal Zahid Hussain",
      "Mohammad Ammar",
      "Sven Tomforde"
    ],
    "abstract": "Micro Crack detection using deep neural networks (DNNs) through an automated\npipeline using wave fields interacting with the damaged areas is highly sought\nafter. These high-dimensional spatio-temporal crack data are limited, and these\ndatasets have large dimensions in the temporal domain. The dataset presents a\nsubstantial class imbalance, with crack pixels constituting an average of only\n5% of the total pixels per sample. This extreme class imbalance poses a\nchallenge for deep learning models with the different micro-scale cracks, as\nthe network can be biased toward predicting the majority class, generally\nleading to poor detection accuracy. This study builds upon the previous\nbenchmark SpAsE-Net, an asymmetric encoder-decoder network for micro-crack\ndetection. The impact of various activation and loss functions were examined\nthrough feature space visualization using the manifold discovery and analysis\n(MDA) algorithm. The optimized architecture and training methodology achieved\nan accuracy of 86.85%.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.ET",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.10015v1",
    "published_date": "2024-11-15 07:50:01 UTC",
    "updated_date": "2024-11-15 07:50:01 UTC"
  },
  {
    "arxiv_id": "2411.10010v1",
    "title": "DeepMedcast: A Deep Learning Method for Generating Intermediate Weather Forecasts among Multiple NWP Models",
    "authors": [
      "Atsushi Kudo"
    ],
    "abstract": "Numerical weather prediction (NWP) centers around the world operate a variety\nof NWP models, and recent advances in AI-driven NWP models have increased the\navailability of diverse NWP outputs. While this expansion holds the potential\nto improve forecast accuracy, it also raises a critical challenge of\nidentifying the most reliable predictions for specific forecast scenarios.\nTraditional approaches, such as ensemble or weighted averaging, combine\nmultiple NWP outputs but often generate unrealistic atmospheric fields,\ncomplicating the production of reliable and consistent forecasts in operational\nsettings. In this study, we introduce DeepMedcast, a deep learning method that\ngenerates intermediate forecast, or \"medcast\", between two or more NWP outputs.\nUnlike ensemble averaging, DeepMedcast can provide consistent and explainable\nmedcast without distorting meteorological fields. This paper details the\nmethodology and case studies of DeepMedcast, discussing its advantages and\npotential contributions to operational forecasting.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.10010v1",
    "published_date": "2024-11-15 07:42:16 UTC",
    "updated_date": "2024-11-15 07:42:16 UTC"
  },
  {
    "arxiv_id": "2411.10008v1",
    "title": "Graph-based Complexity for Causal Effect by Empirical Plug-in",
    "authors": [
      "Rina Dechter",
      "Annie Raichev",
      "Alexander Ihler",
      "Jin Tian"
    ],
    "abstract": "This paper focuses on the computational complexity of computing empirical\nplug-in estimates for causal effect queries. Given a causal graph and\nobservational data, any identifiable causal query can be estimated from an\nexpression over the observed variables, called the estimand. The estimand can\nthen be evaluated by plugging in probabilities computed empirically from data.\nIn contrast to conventional wisdom, which assumes that high dimensional\nprobabilistic functions will lead to exponential evaluation time of the\nestimand. We show that computation can be done efficiently, potentially in time\nlinear in the data size, depending on the estimand's hypergraph.\n  In particular, we show that both the treewidth and hypertree width of the\nestimand's structure bound the evaluation complexity of the plug-in estimands,\nanalogous to their role in the complexity of probabilistic inference in\ngraphical models. Often, the hypertree width provides a more effective bound,\nsince the empirical distributions are sparse.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.10008v1",
    "published_date": "2024-11-15 07:42:01 UTC",
    "updated_date": "2024-11-15 07:42:01 UTC"
  },
  {
    "arxiv_id": "2411.10006v1",
    "title": "Orca: Enhancing Role-Playing Abilities of Large Language Models by Integrating Personality Traits",
    "authors": [
      "Yuxuan Huang"
    ],
    "abstract": "Large language models has catalyzed the development of personalized dialogue\nsystems, numerous role-playing conversational agents have emerged. While\nprevious research predominantly focused on enhancing the model's capability to\nfollow instructions by designing character profiles, neglecting the\npsychological factors that drive human conversations. In this paper, we propose\nOrca, a framework for data processing and training LLMs of custom characters by\nintegrating personality traits. Orca comprises four stages: (1) Personality\ntraits inferring, leverage LLMs to infer user's BigFive personality trait\nreports and scores. (2) Data Augment, simulate user's profile, background\nstory, and psychological activities. (3) Dataset construction,\npersonality-conditioned instruction prompting (PCIP) to stimulate LLMs. (4)\nModeling and Training, personality-conditioned instruction tuning (PTIT and\nPSIT), using the generated data to enhance existing open-source LLMs. We\nintroduce OrcaBench, the first benchmark for evaluating the quality of content\ngenerated by LLMs on social platforms across multiple scales. Our experiments\ndemonstrate that our proposed model achieves superior performance on this\nbenchmark, demonstrating its excellence and effectiveness in perceiving\npersonality traits that significantly improve role-playing abilities. Our Code\nis available at https://github.com/Aipura/Orca.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.10006v1",
    "published_date": "2024-11-15 07:35:47 UTC",
    "updated_date": "2024-11-15 07:35:47 UTC"
  },
  {
    "arxiv_id": "2411.10004v1",
    "title": "EyeDiff: text-to-image diffusion model improves rare eye disease diagnosis",
    "authors": [
      "Ruoyu Chen",
      "Weiyi Zhang",
      "Bowen Liu",
      "Xiaolan Chen",
      "Pusheng Xu",
      "Shunming Liu",
      "Mingguang He",
      "Danli Shi"
    ],
    "abstract": "The rising prevalence of vision-threatening retinal diseases poses a\nsignificant burden on the global healthcare systems. Deep learning (DL) offers\na promising solution for automatic disease screening but demands substantial\ndata. Collecting and labeling large volumes of ophthalmic images across various\nmodalities encounters several real-world challenges, especially for rare\ndiseases. Here, we introduce EyeDiff, a text-to-image model designed to\ngenerate multimodal ophthalmic images from natural language prompts and\nevaluate its applicability in diagnosing common and rare diseases. EyeDiff is\ntrained on eight large-scale datasets using the advanced latent diffusion\nmodel, covering 14 ophthalmic image modalities and over 80 ocular diseases, and\nis adapted to ten multi-country external datasets. The generated images\naccurately capture essential lesional characteristics, achieving high alignment\nwith text prompts as evaluated by objective metrics and human experts.\nFurthermore, integrating generated images significantly enhances the accuracy\nof detecting minority classes and rare eye diseases, surpassing traditional\noversampling methods in addressing data imbalance. EyeDiff effectively tackles\nthe issue of data imbalance and insufficiency typically encountered in rare\ndiseases and addresses the challenges of collecting large-scale annotated\nimages, offering a transformative solution to enhance the development of\nexpert-level diseases diagnosis models in ophthalmic field.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "28 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.10004v1",
    "published_date": "2024-11-15 07:30:53 UTC",
    "updated_date": "2024-11-15 07:30:53 UTC"
  },
  {
    "arxiv_id": "2411.10000v1",
    "title": "DuSEGO: Dual Second-order Equivariant Graph Ordinary Differential Equation",
    "authors": [
      "Yingxu Wang",
      "Nan Yin",
      "Mingyan Xiao",
      "Xinhao Yi",
      "Siwei Liu",
      "Shangsong Liang"
    ],
    "abstract": "Graph Neural Networks (GNNs) with equivariant properties have achieved\nsignificant success in modeling complex dynamic systems and molecular\nproperties. However, their expressiveness ability is limited by: (1) Existing\nmethods often overlook the over-smoothing issue caused by traditional GNN\nmodels, as well as the gradient explosion or vanishing problems in deep GNNs.\n(2) Most models operate on first-order information, neglecting that the real\nworld often consists of second-order systems, which further limits the model's\nrepresentation capabilities. To address these issues, we propose the\n\\textbf{Du}al \\textbf{S}econd-order \\textbf{E}quivariant \\textbf{G}raph\n\\textbf{O}rdinary Differential Equation (\\method{}) for equivariant\nrepresentation. Specifically, \\method{} apply the dual second-order equivariant\ngraph ordinary differential equations (Graph ODEs) on graph embeddings and node\ncoordinates, simultaneously. Theoretically, we first prove that \\method{}\nmaintains the equivariant property. Furthermore, we provide theoretical\ninsights showing that \\method{} effectively alleviates the over-smoothing\nproblem in both feature representation and coordinate update. Additionally, we\ndemonstrate that the proposed \\method{} mitigates the exploding and vanishing\ngradients problem, facilitating the training of deep multi-layer GNNs.\nExtensive experiments on benchmark datasets validate the superiority of the\nproposed \\method{} compared to baselines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.10000v1",
    "published_date": "2024-11-15 07:15:05 UTC",
    "updated_date": "2024-11-15 07:15:05 UTC"
  },
  {
    "arxiv_id": "2411.09996v1",
    "title": "Building 6G Radio Foundation Models with Transformer Architectures",
    "authors": [
      "Ahmed Aboulfotouh",
      "Ashkan Eshaghbeigi",
      "Hatem Abou-Zeid"
    ],
    "abstract": "Foundation deep learning (DL) models are general models, designed to learn\ngeneral, robust and adaptable representations of their target modality,\nenabling finetuning across a range of downstream tasks. These models are\npretrained on large, unlabeled datasets using self-supervised learning (SSL).\nFoundation models have demonstrated better generalization than traditional\nsupervised approaches, a critical requirement for wireless communications where\nthe dynamic environment demands model adaptability. In this work, we propose\nand demonstrate the effectiveness of a Vision Transformer (ViT) as a radio\nfoundation model for spectrogram learning. We introduce a Masked Spectrogram\nModeling (MSM) approach to pretrain the ViT in a self-supervised fashion. We\nevaluate the ViT-based foundation model on two downstream tasks: Channel State\nInformation (CSI)-based Human Activity sensing and Spectrogram Segmentation.\nExperimental results demonstrate competitive performance to supervised training\nwhile generalizing across diverse domains. Notably, the pretrained ViT model\noutperforms a four-times larger model that is trained from scratch on the\nspectrogram segmentation task, while requiring significantly less training\ntime, and achieves competitive performance on the CSI-based human activity\nsensing task. This work demonstrates the effectiveness of ViT with MSM for\npretraining as a promising technique for scalable foundation model development\nin future 6G networks.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09996v1",
    "published_date": "2024-11-15 07:01:44 UTC",
    "updated_date": "2024-11-15 07:01:44 UTC"
  },
  {
    "arxiv_id": "2411.10496v1",
    "title": "Guided Learning: Lubricating End-to-End Modeling for Multi-stage Decision-making",
    "authors": [
      "Jian Guo",
      "Saizhuo Wang",
      "Yiyan Qi"
    ],
    "abstract": "Multi-stage decision-making is crucial in various real-world artificial\nintelligence applications, including recommendation systems, autonomous\ndriving, and quantitative investment systems. In quantitative investment, for\nexample, the process typically involves several sequential stages such as\nfactor mining, alpha prediction, portfolio optimization, and sometimes order\nexecution. While state-of-the-art end-to-end modeling aims to unify these\nstages into a single global framework, it faces significant challenges: (1)\ntraining such a unified neural network consisting of multiple stages between\ninitial inputs and final outputs often leads to suboptimal solutions, or even\ncollapse, and (2) many decision-making scenarios are not easily reducible to\nstandard prediction problems. To overcome these challenges, we propose Guided\nLearning, a novel methodological framework designed to enhance end-to-end\nlearning in multi-stage decision-making. We introduce the concept of a\n``guide'', a function that induces the training of intermediate neural network\nlayers towards some phased goals, directing gradients away from suboptimal\ncollapse. For decision scenarios lacking explicit supervisory labels, we\nincorporate a utility function that quantifies the ``reward'' of the throughout\ndecision. Additionally, we explore the connections between Guided Learning and\nclassic machine learning paradigms such as supervised, unsupervised,\nsemi-supervised, multi-task, and reinforcement learning. Experiments on\nquantitative investment strategy building demonstrate that guided learning\nsignificantly outperforms both traditional stage-wise approaches and existing\nend-to-end methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-fin.CP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.10496v1",
    "published_date": "2024-11-15 06:54:25 UTC",
    "updated_date": "2024-11-15 06:54:25 UTC"
  },
  {
    "arxiv_id": "2411.09986v2",
    "title": "Unlocking Transfer Learning for Open-World Few-Shot Recognition",
    "authors": [
      "Byeonggeun Kim",
      "Juntae Lee",
      "Kyuhong Shim",
      "Simyung Chang"
    ],
    "abstract": "Few-Shot Open-Set Recognition (FSOSR) targets a critical real-world\nchallenge, aiming to categorize inputs into known categories, termed closed-set\nclasses, while identifying open-set inputs that fall outside these classes.\nAlthough transfer learning where a model is tuned to a given few-shot task has\nbecome a prominent paradigm in closed-world, we observe that it fails to expand\nto open-world. To unlock this challenge, we propose a two-stage method which\nconsists of open-set aware meta-learning with open-set free transfer learning.\nIn the open-set aware meta-learning stage, a model is trained to establish a\nmetric space that serves as a beneficial starting point for the subsequent\nstage. During the open-set free transfer learning stage, the model is further\nadapted to a specific target task through transfer learning. Additionally, we\nintroduce a strategy to simulate open-set examples by modifying the training\ndataset or generating pseudo open-set examples. The proposed method achieves\nstate-of-the-art performance on two widely recognized benchmarks, miniImageNet\nand tieredImageNet, with only a 1.5\\% increase in training effort. Our work\ndemonstrates the effectiveness of transfer learning in FSOSR.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09986v2",
    "published_date": "2024-11-15 06:43:49 UTC",
    "updated_date": "2025-05-03 16:09:23 UTC"
  },
  {
    "arxiv_id": "2411.09972v1",
    "title": "Large Language Models as User-Agents for Evaluating Task-Oriented-Dialogue Systems",
    "authors": [
      "Taaha Kazi",
      "Ruiliang Lyu",
      "Sizhe Zhou",
      "Dilek Hakkani-Tur",
      "Gokhan Tur"
    ],
    "abstract": "Traditionally, offline datasets have been used to evaluate task-oriented\ndialogue (TOD) models. These datasets lack context awareness, making them\nsuboptimal benchmarks for conversational systems. In contrast, user-agents,\nwhich are context-aware, can simulate the variability and unpredictability of\nhuman conversations, making them better alternatives as evaluators. Prior\nresearch has utilized large language models (LLMs) to develop user-agents. Our\nwork builds upon this by using LLMs to create user-agents for the evaluation of\nTOD systems. This involves prompting an LLM, using in-context examples as\nguidance, and tracking the user-goal state. Our evaluation of diversity and\ntask completion metrics for the user-agents shows improved performance with the\nuse of better prompts. Additionally, we propose methodologies for the automatic\nevaluation of TOD models within this dynamic framework.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09972v1",
    "published_date": "2024-11-15 06:05:45 UTC",
    "updated_date": "2024-11-15 06:05:45 UTC"
  },
  {
    "arxiv_id": "2411.09969v1",
    "title": "Steering AI-Driven Personalization of Scientific Text for General Audiences",
    "authors": [
      "Taewook Kim",
      "Dhruv Agarwal",
      "Jordan Ackerman",
      "Manaswi Saha"
    ],
    "abstract": "Digital media platforms (e.g., social media, science blogs) offer\nopportunities to communicate scientific content to general audiences at scale.\nHowever, these audiences vary in their scientific expertise, literacy levels,\nand personal backgrounds, making effective science communication challenging.\nTo address this challenge, we designed TranSlider, an AI-powered tool that\ngenerates personalized translations of scientific text based on individual user\nprofiles (e.g., hobbies, location, and education). Our tool features an\ninteractive slider that allows users to steer the degree of personalization\nfrom 0 (weakly relatable) to 100 (strongly relatable), leveraging LLMs to\ngenerate the translations with given degrees. Through an exploratory study with\n15 participants, we investigated both the utility of these AI-personalized\ntranslations and how interactive reading features influenced users'\nunderstanding and reading experiences. We found that participants who preferred\nhigher degrees of personalization appreciated the relatable and contextual\ntranslations, while those who preferred lower degrees valued concise\ntranslations with subtle contextualization. Furthermore, participants reported\nthe compounding effect of multiple translations on their understanding of\nscientific content. Given these findings, we discuss several implications of\nAI-personalized translation tools in facilitating communication in\ncollaborative contexts.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "23 pages, 5 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2411.09969v1",
    "published_date": "2024-11-15 05:55:23 UTC",
    "updated_date": "2024-11-15 05:55:23 UTC"
  },
  {
    "arxiv_id": "2411.09968v1",
    "title": "Seeing Clearly by Layer Two: Enhancing Attention Heads to Alleviate Hallucination in LVLMs",
    "authors": [
      "Xiaofeng Zhang",
      "Yihao Quan",
      "Chaochen Gu",
      "Chen Shen",
      "Xiaosong Yuan",
      "Shaotian Yan",
      "Hao Cheng",
      "Kaijie Wu",
      "Jieping Ye"
    ],
    "abstract": "The hallucination problem in multimodal large language models (MLLMs) remains\na common issue. Although image tokens occupy a majority of the input sequence\nof MLLMs, there is limited research to explore the relationship between image\ntokens and hallucinations. In this paper, we analyze the distribution of\nattention scores for image tokens across each layer and head of the model,\nrevealing an intriguing and common phenomenon: most hallucinations are closely\nlinked to the pattern of attention sinks in the self-attention matrix of image\ntokens, where shallow layers exhibit dense attention sinks and deeper layers\nshow sparse attention sinks. We further analyze the attention heads of\ndifferent layers and find that heads with high-density attention sink in the\nimage part play a positive role in alleviating hallucinations. In this paper,\nwe propose a training-free method named \\textcolor{red}{\\textbf{E}}nhancing\n\\textcolor{red}{\\textbf{A}}ttention \\textcolor{red}{\\textbf{H}}eads (EAH), an\napproach designed to enhance the convergence of image tokens attention sinks in\nthe shallow layers. EAH identifies the attention head that shows the vision\nsink in a shallow layer and extracts its attention matrix. This attention map\nis then broadcast to other heads in the layer, thereby strengthening the layer\nto pay more attention to the image itself. With extensive experiments, EAH\nshows significant hallucination-mitigating performance on different MLLMs and\nmetrics, proving its effectiveness and generality.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09968v1",
    "published_date": "2024-11-15 05:51:29 UTC",
    "updated_date": "2024-11-15 05:51:29 UTC"
  },
  {
    "arxiv_id": "2411.09955v2",
    "title": "Instruction-Guided Editing Controls for Images and Multimedia: A Survey in LLM era",
    "authors": [
      "Thanh Tam Nguyen",
      "Zhao Ren",
      "Trinh Pham",
      "Thanh Trung Huynh",
      "Phi Le Nguyen",
      "Hongzhi Yin",
      "Quoc Viet Hung Nguyen"
    ],
    "abstract": "The rapid advancement of large language models (LLMs) and multimodal learning\nhas transformed digital content creation and manipulation. Traditional visual\nediting tools require significant expertise, limiting accessibility. Recent\nstrides in instruction-based editing have enabled intuitive interaction with\nvisual content, using natural language as a bridge between user intent and\ncomplex editing operations. This survey provides an overview of these\ntechniques, focusing on how LLMs and multimodal models empower users to achieve\nprecise visual modifications without deep technical knowledge. By synthesizing\nover 100 publications, we explore methods from generative adversarial networks\nto diffusion models, examining multimodal integration for fine-grained content\ncontrol. We discuss practical applications across domains such as fashion, 3D\nscene manipulation, and video synthesis, highlighting increased accessibility\nand alignment with human intuition. Our survey compares existing literature,\nemphasizing LLM-empowered editing, and identifies key challenges to stimulate\nfurther research. We aim to democratize powerful visual editing across various\nindustries, from entertainment to education. Interested readers are encouraged\nto access our repository at\nhttps://github.com/tamlhp/awesome-instruction-editing.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "Fixed a serious error in author information",
    "pdf_url": "http://arxiv.org/pdf/2411.09955v2",
    "published_date": "2024-11-15 05:18:15 UTC",
    "updated_date": "2024-11-21 05:28:10 UTC"
  },
  {
    "arxiv_id": "2411.09952v1",
    "title": "GGAvatar: Reconstructing Garment-Separated 3D Gaussian Splatting Avatars from Monocular Video",
    "authors": [
      "Jingxuan Chen"
    ],
    "abstract": "Avatar modelling has broad applications in human animation and virtual\ntry-ons. Recent advancements in this field have focused on high-quality and\ncomprehensive human reconstruction but often overlook the separation of\nclothing from the body. To bridge this gap, this paper introduces GGAvatar\n(Garment-separated 3D Gaussian Splatting Avatar), which relies on monocular\nvideos. Through advanced parameterized templates and unique phased training,\nthis model effectively achieves decoupled, editable, and realistic\nreconstruction of clothed humans. Comparative evaluations with other costly\nmodels confirm GGAvatar's superior quality and efficiency in modelling both\nclothed humans and separable garments. The paper also showcases applications in\nclothing editing, as illustrated in Figure 1, highlighting the model's benefits\nand the advantages of effective disentanglement. The code is available at\nhttps://github.com/J-X-Chen/GGAvatar/.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "MMAsia'24 Accepted",
    "pdf_url": "http://arxiv.org/pdf/2411.09952v1",
    "published_date": "2024-11-15 05:09:20 UTC",
    "updated_date": "2024-11-15 05:09:20 UTC"
  },
  {
    "arxiv_id": "2411.09945v1",
    "title": "TEESlice: Protecting Sensitive Neural Network Models in Trusted Execution Environments When Attackers have Pre-Trained Models",
    "authors": [
      "Ding Li",
      "Ziqi Zhang",
      "Mengyu Yao",
      "Yifeng Cai",
      "Yao Guo",
      "Xiangqun Chen"
    ],
    "abstract": "Trusted Execution Environments (TEE) are used to safeguard on-device models.\nHowever, directly employing TEEs to secure the entire DNN model is challenging\ndue to the limited computational speed. Utilizing GPU can accelerate DNN's\ncomputation speed but commercial widely-available GPUs usually lack security\nprotection. To this end, scholars introduce TSDP, a method that protects\nprivacy-sensitive weights within TEEs and offloads insensitive weights to GPUs.\nNevertheless, current methods do not consider the presence of a knowledgeable\nadversary who can access abundant publicly available pre-trained models and\ndatasets. This paper investigates the security of existing methods against such\na knowledgeable adversary and reveals their inability to fulfill their security\npromises. Consequently, we introduce a novel partition before training\nstrategy, which effectively separates privacy-sensitive weights from other\ncomponents of the model. Our evaluation demonstrates that our approach can\noffer full model protection with a computational cost reduced by a factor of\n10. In addition to traditional CNN models, we also demonstrate the scalability\nto large language models. Our approach can compress the private functionalities\nof the large language model to lightweight slices and achieve the same level of\nprotection as the shielding-whole-model baseline.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted by TOSEM. Extended version of the S&P24 paper\n  (arXiv:2310.07152)",
    "pdf_url": "http://arxiv.org/pdf/2411.09945v1",
    "published_date": "2024-11-15 04:52:11 UTC",
    "updated_date": "2024-11-15 04:52:11 UTC"
  },
  {
    "arxiv_id": "2411.09933v1",
    "title": "JRadiEvo: A Japanese Radiology Report Generation Model Enhanced by Evolutionary Optimization of Model Merging",
    "authors": [
      "Kaito Baba",
      "Ryota Yagi",
      "Junichiro Takahashi",
      "Risa Kishikawa",
      "Satoshi Kodera"
    ],
    "abstract": "With the rapid advancement of large language models (LLMs), foundational\nmodels (FMs) have seen significant advancements. Healthcare is one of the most\ncrucial application areas for these FMs, given the significant time and effort\nrequired for physicians to analyze large volumes of patient data. Recent\nefforts have focused on adapting multimodal FMs to the medical domain through\ntechniques like instruction-tuning, leading to the development of medical\nfoundation models (MFMs). However, these approaches typically require large\namounts of training data to effectively adapt models to the medical field.\nMoreover, most existing models are trained on English datasets, limiting their\npracticality in non-English-speaking regions where healthcare professionals and\npatients are not always fluent in English. The need for translation introduces\nadditional costs and inefficiencies. To address these challenges, we propose a\n\\textbf{J}apanese \\textbf{Radi}ology report generation model enhanced by\n\\textbf{Evo}lutionary optimization of model merging (JRadiEvo). This is the\nfirst attempt to extend a non-medical vision-language foundation model to the\nmedical domain through evolutionary optimization of model merging. We\nsuccessfully created a model that generates accurate Japanese reports from\nX-ray images using only 50 translated samples from publicly available data.\nThis model, developed with highly efficient use of limited data, outperformed\nleading models from recent research trained on much larger datasets.\nAdditionally, with only 8 billion parameters, this relatively compact\nfoundation model can be deployed locally within hospitals, making it a\npractical solution for environments where APIs and other external services\ncannot be used due to strict privacy and security requirements.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.NE"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by NeurIPS'24 Workshop on AIM-FM: Advancements In Medical\n  Foundation Models: Explainability, Robustness, Security, and Beyond",
    "pdf_url": "http://arxiv.org/pdf/2411.09933v1",
    "published_date": "2024-11-15 04:16:50 UTC",
    "updated_date": "2024-11-15 04:16:50 UTC"
  },
  {
    "arxiv_id": "2411.09921v2",
    "title": "Motion-Grounded Video Reasoning: Understanding and Perceiving Motion at Pixel Level",
    "authors": [
      "Andong Deng",
      "Tongjia Chen",
      "Shoubin Yu",
      "Taojiannan Yang",
      "Lincoln Spencer",
      "Yapeng Tian",
      "Ajmal Saeed Mian",
      "Mohit Bansal",
      "Chen Chen"
    ],
    "abstract": "In this paper, we introduce Motion-Grounded Video Reasoning, a new motion\nunderstanding task that requires generating visual answers (video segmentation\nmasks) according to the input question, and hence needs implicit spatiotemporal\nreasoning and grounding. This task extends existing spatiotemporal grounding\nwork focusing on explicit action/motion grounding, to a more general format by\nenabling implicit reasoning via questions. To facilitate the development of the\nnew task, we collect a large-scale dataset called GROUNDMORE, which comprises\n1,715 video clips, 249K object masks that are deliberately designed with 4\nquestion types (Causal, Sequential, Counterfactual, and Descriptive) for\nbenchmarking deep and comprehensive motion reasoning abilities. GROUNDMORE\nuniquely requires models to generate visual answers, providing a more concrete\nand visually interpretable response than plain texts. It evaluates models on\nboth spatiotemporal grounding and reasoning, fostering to address complex\nchallenges in motion-related video reasoning, temporal perception, and\npixel-level understanding. Furthermore, we introduce a novel baseline model\nnamed Motion-Grounded Video Reasoning Assistant (MORA). MORA incorporates the\nmultimodal reasoning ability from the Multimodal LLM, the pixel-level\nperception capability from the grounding model (SAM), and the temporal\nperception ability from a lightweight localization head. MORA achieves\nrespectable performance on GROUNDMORE outperforming the best existing visual\ngrounding baseline model by an average of 21.5% relatively. We hope this novel\nand challenging task will pave the way for future advancements in robust and\ngeneral motion understanding via video reasoning segmentation",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2411.09921v2",
    "published_date": "2024-11-15 03:45:09 UTC",
    "updated_date": "2025-04-04 03:20:03 UTC"
  },
  {
    "arxiv_id": "2411.09909v1",
    "title": "AMXFP4: Taming Activation Outliers with Asymmetric Microscaling Floating-Point for 4-bit LLM Inference",
    "authors": [
      "Janghwan Lee",
      "Jiwoong Park",
      "Jinseok Kim",
      "Yongjik Kim",
      "Jungju Oh",
      "Jinwook Oh",
      "Jungwook Choi"
    ],
    "abstract": "Scaling Large Language Models (LLMs) with extended context lengths has\nincreased the need for efficient low-bit quantization to manage their\nsubstantial computational demands. However, reducing precision to 4 bits\nfrequently degrades performance due to activation outliers. To address this, we\npropose Asymmetric Microscaling 4-bit Floating-Point (AMXFP4) for efficient LLM\ninference. This novel data format leverages asymmetric shared scales to\nmitigate outliers while naturally capturing the asymmetry introduced by\ngroup-wise quantization. Unlike conventional 4-bit quantization methods that\nrely on data rotation and costly calibration, AMXFP4 uses asymmetric shared\nscales for direct 4-bit casting, achieving near-ideal quantization accuracy\nacross various LLM tasks, including multi-turn conversations, long-context\nreasoning, and visual question answering. Our AMXFP4 format significantly\noutperforms MXFP4 and other leading quantization techniques, enabling robust,\ncalibration-free 4-bit inference.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09909v1",
    "published_date": "2024-11-15 03:11:19 UTC",
    "updated_date": "2024-11-15 03:11:19 UTC"
  },
  {
    "arxiv_id": "2411.09900v1",
    "title": "Statistical Analysis of Policy Space Compression Problem",
    "authors": [
      "Majid Molaei",
      "Marcello Restelli",
      "Alberto Maria Metelli",
      "Matteo Papini"
    ],
    "abstract": "Policy search methods are crucial in reinforcement learning, offering a\nframework to address continuous state-action and partially observable problems.\nHowever, the complexity of exploring vast policy spaces can lead to significant\ninefficiencies. Reducing the policy space through policy compression emerges as\na powerful, reward-free approach to accelerate the learning process. This\ntechnique condenses the policy space into a smaller, representative set while\nmaintaining most of the original effectiveness. Our research focuses on\ndetermining the necessary sample size to learn this compressed set accurately.\nWe employ R\\'enyi divergence to measure the similarity between true and\nestimated policy distributions, establishing error bounds for good\napproximations. To simplify the analysis, we employ the $l_1$ norm, determining\nsample size requirements for both model-based and model-free settings. Finally,\nwe correlate the error bounds from the $l_1$ norm with those from R\\'enyi\ndivergence, distinguishing between policies near the vertices and those in the\nmiddle of the policy space, to determine the lower and upper bounds for the\nrequired sample sizes.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09900v1",
    "published_date": "2024-11-15 02:46:55 UTC",
    "updated_date": "2024-11-15 02:46:55 UTC"
  },
  {
    "arxiv_id": "2411.09891v1",
    "title": "Off-Dynamics Reinforcement Learning via Domain Adaptation and Reward Augmented Imitation",
    "authors": [
      "Yihong Guo",
      "Yixuan Wang",
      "Yuanyuan Shi",
      "Pan Xu",
      "Anqi Liu"
    ],
    "abstract": "Training a policy in a source domain for deployment in the target domain\nunder a dynamics shift can be challenging, often resulting in performance\ndegradation. Previous work tackles this challenge by training on the source\ndomain with modified rewards derived by matching distributions between the\nsource and the target optimal trajectories. However, pure modified rewards only\nensure the behavior of the learned policy in the source domain resembles\ntrajectories produced by the target optimal policies, which does not guarantee\noptimal performance when the learned policy is actually deployed to the target\ndomain. In this work, we propose to utilize imitation learning to transfer the\npolicy learned from the reward modification to the target domain so that the\nnew policy can generate the same trajectories in the target domain. Our\napproach, Domain Adaptation and Reward Augmented Imitation Learning (DARAIL),\nutilizes the reward modification for domain adaptation and follows the general\nframework of generative adversarial imitation learning from observation (GAIfO)\nby applying a reward augmented estimator for the policy optimization step.\nTheoretically, we present an error bound for our method under a mild assumption\nregarding the dynamics shift to justify the motivation of our method.\nEmpirically, our method outperforms the pure modified reward method without\nimitation learning and also outperforms other baselines in benchmark\noff-dynamics environments.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "Published at Neurips 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.09891v1",
    "published_date": "2024-11-15 02:35:20 UTC",
    "updated_date": "2024-11-15 02:35:20 UTC"
  },
  {
    "arxiv_id": "2412.04473v1",
    "title": "Take Package as Language: Anomaly Detection Using Transformer",
    "authors": [
      "Jie Huang"
    ],
    "abstract": "Network data packet anomaly detection faces numerous challenges, including\nexploring new anomaly supervision signals, researching weakly supervised\nanomaly detection, and improving model interpretability. This paper proposes\nNIDS-GPT, a GPT-based causal language model for network intrusion detection.\nUnlike previous work, NIDS-GPT innovatively treats each number in the packet as\nan independent \"word\" rather than packet fields, enabling a more fine-grained\ndata representation. We adopt an improved GPT-2 model and design special\ntokenizers and embedding layers to better capture the structure and semantics\nof network data. NIDS-GPT has good scalability, supports unsupervised\npre-training, and enhances model interpretability through attention weight\nvisualization. Experiments on the CICIDS2017 and car-hacking datasets show that\nNIDS-GPT achieves 100\\% accuracy under extreme imbalance conditions, far\nsurpassing traditional methods; it also achieves over 90\\% accuracy in one-shot\nlearning. These results demonstrate NIDS-GPT's excellent performance and\npotential in handling complex network anomaly detection tasks, especially in\ndata-imbalanced and resource-constrained scenarios. The code is available at\n\\url{https://github.com/woshixiaobai2019/nids-gpt.gi",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04473v1",
    "published_date": "2024-11-15 02:00:43 UTC",
    "updated_date": "2024-11-15 02:00:43 UTC"
  },
  {
    "arxiv_id": "2411.09874v1",
    "title": "A Hybrid Artificial Intelligence System for Automated EEG Background Analysis and Report Generation",
    "authors": [
      "Chin-Sung Tung",
      "Sheng-Fu Liang",
      "Shu-Feng Chang",
      "Chung-Ping Young"
    ],
    "abstract": "Electroencephalography (EEG) plays a crucial role in the diagnosis of various\nneurological disorders. However, small hospitals and clinics often lack\nadvanced EEG signal analysis systems and are prone to misinterpretation in\nmanual EEG reading. This study proposes an innovative hybrid artificial\nintelligence (AI) system for automatic interpretation of EEG background\nactivity and report generation. The system combines deep learning models for\nposterior dominant rhythm (PDR) prediction, unsupervised artifact removal, and\nexpert-designed algorithms for abnormality detection. For PDR prediction, 1530\nlabeled EEGs were used, and the best ensemble model achieved a mean absolute\nerror (MAE) of 0.237, a root mean square error (RMSE) of 0.359, an accuracy of\n91.8% within a 0.6Hz error, and an accuracy of 99% within a 1.2Hz error. The AI\nsystem significantly outperformed neurologists in detecting generalized\nbackground slowing (p = 0.02; F1: AI 0.93, neurologists 0.82) and demonstrated\nimproved focal abnormality detection, although not statistically significant (p\n= 0.79; F1: AI 0.71, neurologists 0.55). Validation on both an internal dataset\nand the Temple University Abnormal EEG Corpus showed consistent performance\n(F1: 0.884 and 0.835, respectively; p = 0.66), demonstrating generalizability.\nThe use of large language models (LLMs) for report generation demonstrated 100%\naccuracy, verified by three other independent LLMs. This hybrid AI system\nprovides an easily scalable and accurate solution for EEG interpretation in\nresource-limited settings, assisting neurologists in improving diagnostic\naccuracy and reducing misdiagnosis rates.",
    "categories": [
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.AI",
    "comment": "Example code available at https://github.com/tcs211/AI_EEEG_REPORT",
    "pdf_url": "http://arxiv.org/pdf/2411.09874v1",
    "published_date": "2024-11-15 01:49:17 UTC",
    "updated_date": "2024-11-15 01:49:17 UTC"
  },
  {
    "arxiv_id": "2411.09852v2",
    "title": "InterFormer: Towards Effective Heterogeneous Interaction Learning for Click-Through Rate Prediction",
    "authors": [
      "Zhichen Zeng",
      "Xiaolong Liu",
      "Mengyue Hang",
      "Xiaoyi Liu",
      "Qinghai Zhou",
      "Chaofei Yang",
      "Yiqun Liu",
      "Yichen Ruan",
      "Laming Chen",
      "Yuxin Chen",
      "Yujia Hao",
      "Jiaqi Xu",
      "Jade Nie",
      "Xi Liu",
      "Buyun Zhang",
      "Wei Wen",
      "Siyang Yuan",
      "Kai Wang",
      "Wen-Yen Chen",
      "Yiping Han",
      "Huayu Li",
      "Chunzhi Yang",
      "Bo Long",
      "Philip S. Yu",
      "Hanghang Tong",
      "Jiyan Yang"
    ],
    "abstract": "Click-through rate (CTR) prediction, which predicts the probability of a user\nclicking an ad, is a fundamental task in recommender systems. The emergence of\nheterogeneous information, such as user profile and behavior sequences, depicts\nuser interests from different aspects. A mutually beneficial integration of\nheterogeneous information is the cornerstone towards the success of CTR\nprediction. However, most of the existing methods suffer from two fundamental\nlimitations, including (1) insufficient inter-mode interaction due to the\nunidirectional information flow between modes, and (2) aggressive information\naggregation caused by early summarization, resulting in excessive information\nloss. To address the above limitations, we propose a novel module named\nInterFormer to learn heterogeneous information interaction in an interleaving\nstyle. To achieve better interaction learning, InterFormer enables\nbidirectional information flow for mutually beneficial learning across\ndifferent modes. To avoid aggressive information aggregation, we retain\ncomplete information in each data mode and use a separate bridging arch for\neffective information selection and summarization. Our proposed InterFormer\nachieves state-of-the-art performance on three public datasets and a\nlarge-scale industrial dataset.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "10 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.09852v2",
    "published_date": "2024-11-15 00:20:36 UTC",
    "updated_date": "2025-01-08 01:44:07 UTC"
  },
  {
    "arxiv_id": "2411.09850v1",
    "title": "Enhancing Diffusion Posterior Sampling for Inverse Problems by Integrating Crafted Measurements",
    "authors": [
      "Shijie Zhou",
      "Huaisheng Zhu",
      "Rohan Sharma",
      "Ruiyi Zhang",
      "Kaiyi Ji",
      "Changyou Chen"
    ],
    "abstract": "Diffusion models have emerged as a powerful foundation model for visual\ngeneration. With an appropriate sampling process, it can effectively serve as a\ngenerative prior to solve general inverse problems. Current posterior sampling\nbased methods take the measurement (i.e., degraded image sample) into the\nposterior sampling to infer the distribution of the target data (i.e., clean\nimage sample). However, in this manner, we show that high-frequency information\ncan be prematurely introduced during the early stages, which could induce\nlarger posterior estimate errors during the restoration sampling. To address\nthis issue, we first reveal that forming the log posterior gradient with the\nnoisy measurement ( i.e., samples from a diffusion forward process) instead of\nthe clean one can benefit the reverse process. Consequently, we propose a novel\ndiffusion posterior sampling method DPS-CM, which incorporates a Crafted\nMeasurement (i.e., samples generated by a reverse denoising process, compared\nto random sampling with noise in standard methods) to form the posterior\nestimate. This integration aims to mitigate the misalignment with the diffusion\nprior caused by cumulative posterior estimate errors. Experimental results\ndemonstrate that our approach significantly improves the overall capacity to\nsolve general and noisy inverse problems, such as Gaussian deblurring,\nsuper-resolution, inpainting, nonlinear deblurring, and tasks with Poisson\nnoise, relative to existing approaches.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.09850v1",
    "published_date": "2024-11-15 00:06:57 UTC",
    "updated_date": "2024-11-15 00:06:57 UTC"
  }
]
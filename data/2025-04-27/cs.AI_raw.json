[
  {
    "arxiv_id": "2504.19394v2",
    "title": "LLMs for Engineering: Teaching Models to Design High Powered Rockets",
    "authors": [
      "Toby Simonds"
    ],
    "abstract": "Large Language Models (LLMs) have transformed software engineering, but their\napplication to physical engineering domains remains underexplored. This paper\nevaluates LLMs' capabilities in high-powered rocketry design through\nRocketBench, a benchmark connecting LLMs to high-fidelity rocket simulations.\nWe test models on two increasingly complex design tasks: target altitude\noptimization and precision landing challenges. Our findings reveal that while\nstate-of-the-art LLMs demonstrate strong baseline engineering knowledge, they\nstruggle to iterate on their designs when given simulation results and\nultimately plateau below human performance levels. However, when enhanced with\nreinforcement learning (RL), we show that a 7B parameter model outperforms both\nSoTA foundation models and human experts. This research demonstrates that\nRL-trained LLMs can serve as effective tools for complex engineering\noptimization, potentially transforming engineering domains beyond software\ndevelopment.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.19394v2",
    "published_date": "2025-04-27 23:59:39 UTC",
    "updated_date": "2025-04-29 22:15:42 UTC"
  },
  {
    "arxiv_id": "2504.19384v1",
    "title": "From Inductive to Deductive: LLMs-Based Qualitative Data Analysis in Requirements Engineering",
    "authors": [
      "Syed Tauhid Ullah Shah",
      "Mohamad Hussein",
      "Ann Barcomb",
      "Mohammad Moshirpour"
    ],
    "abstract": "Requirements Engineering (RE) is essential for developing complex and\nregulated software projects. Given the challenges in transforming stakeholder\ninputs into consistent software designs, Qualitative Data Analysis (QDA)\nprovides a systematic approach to handling free-form data. However, traditional\nQDA methods are time-consuming and heavily reliant on manual effort. In this\npaper, we explore the use of Large Language Models (LLMs), including GPT-4,\nMistral, and LLaMA-2, to improve QDA tasks in RE. Our study evaluates LLMs'\nperformance in inductive (zero-shot) and deductive (one-shot, few-shot)\nannotation tasks, revealing that GPT-4 achieves substantial agreement with\nhuman analysts in deductive settings, with Cohen's Kappa scores exceeding 0.7,\nwhile zero-shot performance remains limited. Detailed, context-rich prompts\nsignificantly improve annotation accuracy and consistency, particularly in\ndeductive scenarios, and GPT-4 demonstrates high reliability across repeated\nruns. These findings highlight the potential of LLMs to support QDA in RE by\nreducing manual effort while maintaining annotation quality. The structured\nlabels automatically provide traceability of requirements and can be directly\nutilized as classes in domain models, facilitating systematic software design.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.19384v1",
    "published_date": "2025-04-27 23:21:52 UTC",
    "updated_date": "2025-04-27 23:21:52 UTC"
  },
  {
    "arxiv_id": "2504.21034v1",
    "title": "SAGA: A Security Architecture for Governing AI Agentic Systems",
    "authors": [
      "Georgios Syros",
      "Anshuman Suri",
      "Cristina Nita-Rotaru",
      "Alina Oprea"
    ],
    "abstract": "Large Language Model (LLM)-based agents increasingly interact, collaborate,\nand delegate tasks to one another autonomously with minimal human interaction.\nIndustry guidelines for agentic system governance emphasize the need for users\nto maintain comprehensive control over their agents, mitigating potential\ndamage from malicious agents. Several proposed agentic system designs address\nagent identity, authorization, and delegation, but remain purely theoretical,\nwithout concrete implementation and evaluation. Most importantly, they do not\nprovide user-controlled agent management. To address this gap, we propose SAGA,\na Security Architecture for Governing Agentic systems, that offers user\noversight over their agents' lifecycle. In our design, users register their\nagents with a central entity, the Provider, that maintains agents contact\ninformation, user-defined access control policies, and helps agents enforce\nthese policies on inter-agent communication. We introduce a cryptographic\nmechanism for deriving access control tokens, that offers fine-grained control\nover an agent's interaction with other agents, balancing security and\nperformance consideration. We evaluate SAGA on several agentic tasks, using\nagents in different geolocations, and multiple on-device and cloud LLMs,\ndemonstrating minimal performance overhead with no impact on underlying task\nutility in a wide range of conditions. Our architecture enables secure and\ntrustworthy deployment of autonomous agents, accelerating the responsible\nadoption of this technology in sensitive environments.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.21034v1",
    "published_date": "2025-04-27 23:10:00 UTC",
    "updated_date": "2025-04-27 23:10:00 UTC"
  },
  {
    "arxiv_id": "2504.19374v1",
    "title": "Rethinking Label-specific Features for Label Distribution Learning",
    "authors": [
      "Suping Xu",
      "Chuyi Dai",
      "Lin Shang",
      "Changbin Shao",
      "Xibei Yang",
      "Witold Pedrycz"
    ],
    "abstract": "Label distribution learning (LDL) is an emerging learning paradigm designed\nto capture the relative importance of labels for each instance. Label-specific\nfeatures (LSFs), constructed by LIFT, have proven effective for learning tasks\nwith label ambiguity by leveraging clustering-based prototypes for each label\nto re-characterize instances. However, directly introducing LIFT into LDL tasks\ncan be suboptimal, as the prototypes it collects primarily reflect\nintra-cluster relationships while neglecting interactions among distinct\nclusters. Additionally, constructing LSFs using multi-perspective information,\nrather than relying solely on Euclidean distance, provides a more robust and\ncomprehensive representation of instances, mitigating noise and bias that may\narise from a single distance perspective. To address these limitations, we\nintroduce Structural Anchor Points (SAPs) to capture inter-cluster\ninteractions. This leads to a novel LSFs construction strategy, LIFT-SAP, which\nenhances LIFT by integrating both distance and direction information of each\ninstance relative to SAPs. Furthermore, we propose a novel LDL algorithm, Label\nDistribution Learning via Label-specifIc FeaTure with SAPs (LDL-LIFT-SAP),\nwhich unifies multiple label description degrees predicted from different LSF\nspaces into a cohesive label distribution. Extensive experiments on 15\nreal-world datasets demonstrate the effectiveness of LIFT-SAP over LIFT, as\nwell as the superiority of LDL-LIFT-SAP compared to seven other\nwell-established algorithms.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "11 Pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.19374v1",
    "published_date": "2025-04-27 22:32:46 UTC",
    "updated_date": "2025-04-27 22:32:46 UTC"
  },
  {
    "arxiv_id": "2504.19373v2",
    "title": "Doxing via the Lens: Revealing Privacy Leakage in Image Geolocation for Agentic Multi-Modal Large Reasoning Model",
    "authors": [
      "Weidi Luo",
      "Qiming Zhang",
      "Tianyu Lu",
      "Xiaogeng Liu",
      "Yue Zhao",
      "Zhen Xiang",
      "Chaowei Xiao"
    ],
    "abstract": "The increasing capabilities of agentic multi-modal large reasoning models,\nsuch as ChatGPT o3, have raised critical concerns regarding privacy leakage\nthrough inadvertent image geolocation. In this paper, we conduct the first\nsystematic and controlled study on the potential privacy risks associated with\nvisual reasoning abilities of ChatGPT o3. We manually collect and construct a\ndataset comprising 50 real-world images that feature individuals alongside\nprivacy-relevant environmental elements, capturing realistic and sensitive\nscenarios for analysis. Our experimental evaluation reveals that ChatGPT o3 can\npredict user locations with high precision, achieving street-level accuracy\n(within one mile) in 60% of cases. Through analysis, we identify key visual\ncues, including street layout and front yard design, that significantly\ncontribute to the model inference success. Additionally, targeted occlusion\nexperiments demonstrate that masking critical features effectively mitigates\ngeolocation accuracy, providing insights into potential defense mechanisms. Our\nfindings highlight an urgent need for privacy-aware development for agentic\nmulti-modal large reasoning models, particularly in applications involving\nprivate imagery.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.19373v2",
    "published_date": "2025-04-27 22:26:45 UTC",
    "updated_date": "2025-04-29 12:00:08 UTC"
  },
  {
    "arxiv_id": "2504.19370v1",
    "title": "Mitigating Bias in Facial Recognition Systems: Centroid Fairness Loss Optimization",
    "authors": [
      "Jean-Rémy Conti",
      "Stéphan Clémençon"
    ],
    "abstract": "The urging societal demand for fair AI systems has put pressure on the\nresearch community to develop predictive models that are not only globally\naccurate but also meet new fairness criteria, reflecting the lack of disparate\nmistreatment with respect to sensitive attributes ($\\textit{e.g.}$ gender,\nethnicity, age). In particular, the variability of the errors made by certain\nFacial Recognition (FR) systems across specific segments of the population\ncompromises the deployment of the latter, and was judged unacceptable by\nregulatory authorities. Designing fair FR systems is a very challenging\nproblem, mainly due to the complex and functional nature of the performance\nmeasure used in this domain ($\\textit{i.e.}$ ROC curves) and because of the\nhuge heterogeneity of the face image datasets usually available for training.\nIn this paper, we propose a novel post-processing approach to improve the\nfairness of pre-trained FR models by optimizing a regression loss which acts on\ncentroid-based scores. Beyond the computational advantages of the method, we\npresent numerical experiments providing strong empirical evidence of the gain\nin fairness and of the ability to preserve global accuracy.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at both the AFME and RegML Workshops at NeurIPS 2024. A\n  preliminary version has been accepted for publication by Springer Nature, in\n  the context of the ICPR 2024 conference",
    "pdf_url": "http://arxiv.org/pdf/2504.19370v1",
    "published_date": "2025-04-27 22:17:44 UTC",
    "updated_date": "2025-04-27 22:17:44 UTC"
  },
  {
    "arxiv_id": "2504.19362v1",
    "title": "Low-Rank Adaptive Structural Priors for Generalizable Diabetic Retinopathy Grading",
    "authors": [
      "Yunxuan Wang",
      "Ray Yin",
      "Yumei Tan",
      "Hao Chen",
      "Haiying Xia"
    ],
    "abstract": "Diabetic retinopathy (DR), a serious ocular complication of diabetes, is one\nof the primary causes of vision loss among retinal vascular diseases. Deep\nlearning methods have been extensively applied in the grading of diabetic\nretinopathy (DR). However, their performance declines significantly when\napplied to data outside the training distribution due to domain shifts. Domain\ngeneralization (DG) has emerged as a solution to this challenge. However, most\nexisting DG methods overlook lesion-specific features, resulting in\ninsufficient accuracy. In this paper, we propose a novel approach that enhances\nexisting DG methods by incorporating structural priors, inspired by the\nobservation that DR grading is heavily dependent on vessel and lesion\nstructures. We introduce Low-rank Adaptive Structural Priors (LoASP), a\nplug-and-play framework designed for seamless integration with existing DG\nmodels. LoASP improves generalization by learning adaptive structural\nrepresentations that are finely tuned to the complexities of DR diagnosis.\nExtensive experiments on eight diverse datasets validate its effectiveness in\nboth single-source and multi-source domain scenarios. Furthermore,\nvisualizations reveal that the learned structural priors intuitively align with\nthe intricate architecture of the vessels and lesions, providing compelling\ninsights into their interpretability and diagnostic relevance.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "Accepted by IJCNN 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.19362v1",
    "published_date": "2025-04-27 21:40:02 UTC",
    "updated_date": "2025-04-27 21:40:02 UTC"
  },
  {
    "arxiv_id": "2504.19354v1",
    "title": "Neurosymbolic Association Rule Mining from Tabular Data",
    "authors": [
      "Erkan Karabulut",
      "Paul Groth",
      "Victoria Degeler"
    ],
    "abstract": "Association Rule Mining (ARM) is the task of mining patterns among data\nfeatures in the form of logical rules, with applications across a myriad of\ndomains. However, high-dimensional datasets often result in an excessive number\nof rules, increasing execution time and negatively impacting downstream task\nperformance. Managing this rule explosion remains a central challenge in ARM\nresearch. To address this, we introduce Aerial+, a novel neurosymbolic ARM\nmethod. Aerial+ leverages an under-complete autoencoder to create a neural\nrepresentation of the data, capturing associations between features. It\nextracts rules from this neural representation by exploiting the model's\nreconstruction mechanism. Extensive evaluations on five datasets against seven\nbaselines demonstrate that Aerial+ achieves state-of-the-art results by\nlearning more concise, high-quality rule sets with full data coverage. When\nintegrated into rule-based interpretable machine learning models, Aerial+\nsignificantly reduces execution time while maintaining or improving accuracy.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.19354v1",
    "published_date": "2025-04-27 20:43:33 UTC",
    "updated_date": "2025-04-27 20:43:33 UTC"
  },
  {
    "arxiv_id": "2504.19353v1",
    "title": "Flow Along the K-Amplitude for Generative Modeling",
    "authors": [
      "Weitao Du",
      "Shuning Chang",
      "Jiasheng Tang",
      "Yu Rong",
      "Fan Wang",
      "Shengchao Liu"
    ],
    "abstract": "In this work, we propose a novel generative learning paradigm, K-Flow, an\nalgorithm that flows along the $K$-amplitude. Here, $k$ is a scaling parameter\nthat organizes frequency bands (or projected coefficients), and amplitude\ndescribes the norm of such projected coefficients. By incorporating the\n$K$-amplitude decomposition, K-Flow enables flow matching across the scaling\nparameter as time. We discuss three venues and six properties of K-Flow, from\ntheoretical foundations, energy and temporal dynamics, and practical\napplications, respectively. Specifically, from the practical usage perspective,\nK-Flow allows steerable generation by controlling the information at different\nscales. To demonstrate the effectiveness of K-Flow, we conduct experiments on\nunconditional image generation, class-conditional image generation, and\nmolecule assembly generation. Additionally, we conduct three ablation studies\nto demonstrate how K-Flow steers scaling parameter to effectively control the\nresolution of image generation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.19353v1",
    "published_date": "2025-04-27 20:38:24 UTC",
    "updated_date": "2025-04-27 20:38:24 UTC"
  },
  {
    "arxiv_id": "2504.19341v1",
    "title": "PolyTouch: A Robust Multi-Modal Tactile Sensor for Contact-rich Manipulation Using Tactile-Diffusion Policies",
    "authors": [
      "Jialiang Zhao",
      "Naveen Kuppuswamy",
      "Siyuan Feng",
      "Benjamin Burchfiel",
      "Edward Adelson"
    ],
    "abstract": "Achieving robust dexterous manipulation in unstructured domestic environments\nremains a significant challenge in robotics. Even with state-of-the-art robot\nlearning methods, haptic-oblivious control strategies (i.e. those relying only\non external vision and/or proprioception) often fall short due to occlusions,\nvisual complexities, and the need for precise contact interaction control. To\naddress these limitations, we introduce PolyTouch, a novel robot finger that\nintegrates camera-based tactile sensing, acoustic sensing, and peripheral\nvisual sensing into a single design that is compact and durable. PolyTouch\nprovides high-resolution tactile feedback across multiple temporal scales,\nwhich is essential for efficiently learning complex manipulation tasks.\nExperiments demonstrate an at least 20-fold increase in lifespan over\ncommercial tactile sensors, with a design that is both easy to manufacture and\nscalable. We then use this multi-modal tactile feedback along with\nvisuo-proprioceptive observations to synthesize a tactile-diffusion policy from\nhuman demonstrations; the resulting contact-aware control policy significantly\noutperforms haptic-oblivious policies in multiple contact-aware manipulation\npolicies. This paper highlights how effectively integrating multi-modal contact\nsensing can hasten the development of effective contact-aware manipulation\npolicies, paving the way for more reliable and versatile domestic robots. More\ninformation can be found at https://polytouch.alanz.info/",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Nominated for the best paper award at ICRA 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.19341v1",
    "published_date": "2025-04-27 19:50:31 UTC",
    "updated_date": "2025-04-27 19:50:31 UTC"
  },
  {
    "arxiv_id": "2504.19339v2",
    "title": "Explanatory Summarization with Discourse-Driven Planning",
    "authors": [
      "Dongqi Liu",
      "Xi Yu",
      "Vera Demberg",
      "Mirella Lapata"
    ],
    "abstract": "Lay summaries for scientific documents typically include explanations to help\nreaders grasp sophisticated concepts or arguments. However, current automatic\nsummarization methods do not explicitly model explanations, which makes it\ndifficult to align the proportion of explanatory content with human-written\nsummaries. In this paper, we present a plan-based approach that leverages\ndiscourse frameworks to organize summary generation and guide explanatory\nsentences by prompting responses to the plan. Specifically, we propose two\ndiscourse-driven planning strategies, where the plan is conditioned as part of\nthe input or part of the output prefix, respectively. Empirical experiments on\nthree lay summarization datasets show that our approach outperforms existing\nstate-of-the-art methods in terms of summary quality, and it enhances model\nrobustness, controllability, and mitigates hallucination.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by the Transactions of the Association for Computational\n  Linguistics (TACL 2025)",
    "pdf_url": "http://arxiv.org/pdf/2504.19339v2",
    "published_date": "2025-04-27 19:47:36 UTC",
    "updated_date": "2025-05-11 09:00:44 UTC"
  },
  {
    "arxiv_id": "2504.19333v2",
    "title": "Unified Multi-Task Learning & Model Fusion for Efficient Language Model Guardrailing",
    "authors": [
      "James O' Neill",
      "Santhosh Subramanian",
      "Eric Lin",
      "Vaikkunth Mugunthan"
    ],
    "abstract": "The trend towards large language models (LLMs) for guardrailing against\nundesired behaviors is increasing and has shown promise for censoring user\ninputs. However, increased latency, memory consumption, hosting expenses and\nnon-structured outputs can make their use prohibitive.\n  In this work, we show that task-specific data generation can lead to\nfine-tuned classifiers that significantly outperform current state of the art\n(SoTA) while being orders of magnitude smaller. Secondly, we show that using a\nsingle model, \\texttt{MultiTaskGuard}, that is pretrained on a large\nsynthetically generated dataset with unique task instructions further improves\ngeneralization. Thirdly, our most performant models, \\texttt{UniGuard}, are\nfound using our proposed search-based model merging approach that finds an\noptimal set of parameters to combine single-policy models and multi-policy\nguardrail models. % On 7 public datasets and 4 guardrail benchmarks we created,\nour efficient guardrail classifiers improve over the best performing SoTA\npublicly available LLMs and 3$^{\\text{rd}}$ party guardrail APIs in detecting\nunsafe and safe behaviors by an average F1 score improvement of \\textbf{29.92}\npoints over Aegis-LlamaGuard and \\textbf{21.62} over \\texttt{gpt-4o},\nrespectively. Lastly, our guardrail synthetic data generation process that uses\ncustom task-specific guardrail poli",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.19333v2",
    "published_date": "2025-04-27 19:07:58 UTC",
    "updated_date": "2025-04-29 02:42:00 UTC"
  },
  {
    "arxiv_id": "2504.20112v1",
    "title": "Supervised Pretraining for Material Property Prediction",
    "authors": [
      "Chowdhury Mohammad Abid Rahman",
      "Aldo H. Romero",
      "Prashnna K. Gyawali"
    ],
    "abstract": "Accurate prediction of material properties facilitates the discovery of novel\nmaterials with tailored functionalities. Deep learning models have recently\nshown superior accuracy and flexibility in capturing structure-property\nrelationships. However, these models often rely on supervised learning, which\nrequires large, well-annotated datasets an expensive and time-consuming\nprocess. Self-supervised learning (SSL) offers a promising alternative by\npretraining on large, unlabeled datasets to develop foundation models that can\nbe fine-tuned for material property prediction. In this work, we propose\nsupervised pretraining, where available class information serves as surrogate\nlabels to guide learning, even when downstream tasks involve unrelated material\nproperties. We evaluate this strategy on two state-of-the-art SSL models and\nintroduce a novel framework for supervised pretraining. To further enhance\nrepresentation learning, we propose a graph-based augmentation technique that\ninjects noise to improve robustness without structurally deforming material\ngraphs. The resulting foundation models are fine-tuned for six challenging\nmaterial property predictions, achieving significant performance gains over\nbaselines, ranging from 2% to 6.67% improvement in mean absolute error (MAE)\nand establishing a new benchmark in material property prediction. This study\nrepresents the first exploration of supervised pertaining with surrogate labels\nin material property prediction, advancing methodology and application in the\nfield.",
    "categories": [
      "cs.LG",
      "cond-mat.mtrl-sci",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "21 pages, 7 figures, 2 algorithms, 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2504.20112v1",
    "published_date": "2025-04-27 19:00:41 UTC",
    "updated_date": "2025-04-27 19:00:41 UTC"
  },
  {
    "arxiv_id": "2504.19327v1",
    "title": "Platonic Grounding for Efficient Multimodal Language Models",
    "authors": [
      "Moulik Choraria",
      "Xinbo Wu",
      "Akhil Bhimaraju",
      "Nitesh Sekhar",
      "Yue Wu",
      "Xu Zhang",
      "Prateek Singhal",
      "Lav R. Varshney"
    ],
    "abstract": "The hyperscaling of data and parameter count in Transformer-based models is\nyielding diminishing performance improvement, especially when weighed against\ntraining costs. Such plateauing indicates the importance of methods for more\nefficient finetuning and inference, while retaining similar performance. This\nis especially relevant for multimodal learning paradigms, where inference costs\nof processing multimodal tokens can determine the model's practical viability.\nAt the same time, research on representations and mechanistic interpretability\nhas improved our understanding of the inner workings of Transformer-based\nmodels; one such line of work reveals an implicit alignment in the deeper\nlayers of pretrained models, across modalities. Taking inspiration from this,\nwe motivate and propose a simple modification to existing multimodal frameworks\nthat rely on aligning pretrained models. We demonstrate that our approach\nmaintains and, in some cases, even improves performance of baseline methods\nwhile achieving significant gains in both training and inference-time compute.\nOur work also has implications for combining pretrained models into larger\nsystems efficiently.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.19327v1",
    "published_date": "2025-04-27 18:56:26 UTC",
    "updated_date": "2025-04-27 18:56:26 UTC"
  },
  {
    "arxiv_id": "2504.19323v2",
    "title": "NSFlow: An End-to-End FPGA Framework with Scalable Dataflow Architecture for Neuro-Symbolic AI",
    "authors": [
      "Hanchen Yang",
      "Zishen Wan",
      "Ritik Raj",
      "Joongun Park",
      "Ziwei Li",
      "Ananda Samajdar",
      "Arijit Raychowdhury",
      "Tushar Krishna"
    ],
    "abstract": "Neuro-Symbolic AI (NSAI) is an emerging paradigm that integrates neural\nnetworks with symbolic reasoning to enhance the transparency, reasoning\ncapabilities, and data efficiency of AI systems. Recent NSAI systems have\ngained traction due to their exceptional performance in reasoning tasks and\nhuman-AI collaborative scenarios. Despite these algorithmic advancements,\nexecuting NSAI tasks on existing hardware (e.g., CPUs, GPUs, TPUs) remains\nchallenging, due to their heterogeneous computing kernels, high memory\nintensity, and unique memory access patterns. Moreover, current NSAI algorithms\nexhibit significant variation in operation types and scales, making them\nincompatible with existing ML accelerators. These challenges highlight the need\nfor a versatile and flexible acceleration framework tailored to NSAI workloads.\nIn this paper, we propose NSFlow, an FPGA-based acceleration framework designed\nto achieve high efficiency, scalability, and versatility across NSAI systems.\nNSFlow features a design architecture generator that identifies workload data\ndependencies and creates optimized dataflow architectures, as well as a\nreconfigurable array with flexible compute units, re-organizable memory, and\nmixed-precision capabilities. Evaluating across NSAI workloads, NSFlow achieves\n31x speedup over Jetson TX2, more than 2x over GPU, 8x speedup over TPU-like\nsystolic array, and more than 3x over Xilinx DPU. NSFlow also demonstrates\nenhanced scalability, with only 4x runtime increase when symbolic workloads\nscale by 150x. To the best of our knowledge, NSFlow is the first framework to\nenable real-time generalizable NSAI algorithms acceleration, demonstrating a\npromising solution for next-generation cognitive systems.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.LG",
      "cs.PF"
    ],
    "primary_category": "cs.AR",
    "comment": "2025 IEEE/ACM Design Automation Conference (DAC)",
    "pdf_url": "http://arxiv.org/pdf/2504.19323v2",
    "published_date": "2025-04-27 18:28:43 UTC",
    "updated_date": "2025-04-29 14:56:56 UTC"
  },
  {
    "arxiv_id": "2505.00029v1",
    "title": "Keep the General, Inject the Specific: Structured Dialogue Fine-Tuning for Knowledge Injection without Catastrophic Forgetting",
    "authors": [
      "Yijie Hong",
      "Xiaofei Yin",
      "Xinzhong Wang",
      "Yi Tu",
      "Ya Guo",
      "Sufeng Duan",
      "Weiqiang Wang",
      "Lingyong Fang",
      "Depeng Wang",
      "Huijia Zhu"
    ],
    "abstract": "Large Vision Language Models have demonstrated impressive versatile\ncapabilities through extensive multimodal pre-training, but face significant\nlimitations when incorporating specialized knowledge domains beyond their\ntraining distribution. These models struggle with a fundamental dilemma: direct\nadaptation approaches that inject domain-specific knowledge often trigger\ncatastrophic forgetting of foundational visual-linguistic abilities. We\nintroduce Structured Dialogue Fine-Tuning (SDFT), an effective approach that\neffectively injects domain-specific knowledge while minimizing catastrophic\nforgetting. Drawing inspiration from supervised fine-tuning in LLMs and\nsubject-driven personalization in text-to-image diffusion models, our method\nemploys a three-phase dialogue structure: Foundation Preservation reinforces\npre-trained visual-linguistic alignment through caption tasks; Contrastive\nDisambiguation introduces carefully designed counterfactual examples to\nmaintain semantic boundaries; and Knowledge Specialization embeds specialized\ninformation through chain-of-thought reasoning. Experimental results across\nmultiple domains confirm SDFT's effectiveness in balancing specialized\nknowledge acquisition with general capability retention. Our key contributions\ninclude a data-centric dialogue template that balances foundational alignment\nwith targeted knowledge integration, a weighted multi-turn supervision\nframework, and comprehensive evaluation across diverse knowledge types.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "13 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.00029v1",
    "published_date": "2025-04-27 18:04:02 UTC",
    "updated_date": "2025-04-27 18:04:02 UTC"
  },
  {
    "arxiv_id": "2504.19320v1",
    "title": "Logic-Based Artificial Intelligence Algorithms Supporting Categorical Semantics",
    "authors": [
      "Ralph Wojtowicz"
    ],
    "abstract": "This paper seeks to apply categorical logic to the design of artificial\nintelligent agents that reason symbolically about objects more richly\nstructured than sets. Using Johnstone's sequent calculus of terms- and\nformulae-in-context, we develop forward chaining and normal form algorithms for\nreasoning about objects in cartesian categories with the rules for Horn logic.\nWe also adapt first-order unification to support multi-sorted theories,\ncontexts, and fragments of first-order logic. The significance of these\nreformulations rests in the fact that they can be applied to reasoning about\nobjects in semantic categories that do not support classical logic or even all\nits connectives.",
    "categories": [
      "cs.AI",
      "03, 18",
      "I.1.2"
    ],
    "primary_category": "cs.AI",
    "comment": "31 pages",
    "pdf_url": "http://arxiv.org/pdf/2504.19320v1",
    "published_date": "2025-04-27 18:02:02 UTC",
    "updated_date": "2025-04-27 18:02:02 UTC"
  },
  {
    "arxiv_id": "2504.21033v1",
    "title": "Transcending Dimensions using Generative AI: Real-Time 3D Model Generation in Augmented Reality",
    "authors": [
      "Majid Behravan",
      "Maryam Haghani",
      "Denis Gracanin"
    ],
    "abstract": "Traditional 3D modeling requires technical expertise, specialized software,\nand time-intensive processes, making it inaccessible for many users. Our\nresearch aims to lower these barriers by combining generative AI and augmented\nreality (AR) into a cohesive system that allows users to easily generate,\nmanipulate, and interact with 3D models in real time, directly within AR\nenvironments. Utilizing cutting-edge AI models like Shap-E, we address the\ncomplex challenges of transforming 2D images into 3D representations in AR\nenvironments. Key challenges such as object isolation, handling intricate\nbackgrounds, and achieving seamless user interaction are tackled through\nadvanced object detection methods, such as Mask R-CNN. Evaluation results from\n35 participants reveal an overall System Usability Scale (SUS) score of 69.64,\nwith participants who engaged with AR/VR technologies more frequently rating\nthe system significantly higher, at 80.71. This research is particularly\nrelevant for applications in gaming, education, and AR-based e-commerce,\noffering intuitive, model creation for users without specialized skills.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.GR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.21033v1",
    "published_date": "2025-04-27 17:19:48 UTC",
    "updated_date": "2025-04-27 17:19:48 UTC"
  },
  {
    "arxiv_id": "2504.20109v1",
    "title": "Personalized Artificial General Intelligence (AGI) via Neuroscience-Inspired Continuous Learning Systems",
    "authors": [
      "Rajeev Gupta",
      "Suhani Gupta",
      "Ronak Parikh",
      "Divya Gupta",
      "Amir Javaheri",
      "Jairaj Singh Shaktawat"
    ],
    "abstract": "Artificial Intelligence has made remarkable advancements in recent years,\nprimarily driven by increasingly large deep learning models. However, achieving\ntrue Artificial General Intelligence (AGI) demands fundamentally new\narchitectures rather than merely scaling up existing models. Current approaches\nlargely depend on expanding model parameters, which improves task-specific\nperformance but falls short in enabling continuous, adaptable, and generalized\nlearning. Achieving AGI capable of continuous learning and personalization on\nresource-constrained edge devices is an even bigger challenge.\n  This paper reviews the state of continual learning and neuroscience-inspired\nAI, and proposes a novel architecture for Personalized AGI that integrates\nbrain-like learning mechanisms for edge deployment. We review literature on\ncontinuous lifelong learning, catastrophic forgetting, and edge AI, and discuss\nkey neuroscience principles of human learning, including Synaptic Pruning,\nHebbian plasticity, Sparse Coding, and Dual Memory Systems, as inspirations for\nAI systems. Building on these insights, we outline an AI architecture that\nfeatures complementary fast-and-slow learning modules, synaptic\nself-optimization, and memory-efficient model updates to support on-device\nlifelong adaptation.\n  Conceptual diagrams of the proposed architecture and learning processes are\nprovided. We address challenges such as catastrophic forgetting, memory\nefficiency, and system scalability, and present application scenarios for\nmobile AI assistants and embodied AI systems like humanoid robots. We conclude\nwith key takeaways and future research directions toward truly continual,\npersonalized AGI on the edge. While the architecture is theoretical, it\nsynthesizes diverse findings and offers a roadmap for future implementation.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "39 pages, 16 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.20109v1",
    "published_date": "2025-04-27 16:10:17 UTC",
    "updated_date": "2025-04-27 16:10:17 UTC"
  },
  {
    "arxiv_id": "2504.19277v1",
    "title": "Small Models, Big Tasks: An Exploratory Empirical Study on Small Language Models for Function Calling",
    "authors": [
      "Ishan Kavathekar",
      "Raghav Donakanti",
      "Ponnurangam Kumaraguru",
      "Karthik Vaidhyanathan"
    ],
    "abstract": "Function calling is a complex task with widespread applications in domains\nsuch as information retrieval, software engineering and automation. For\nexample, a query to book the shortest flight from New York to London on January\n15 requires identifying the correct parameters to generate accurate function\ncalls. Large Language Models (LLMs) can automate this process but are\ncomputationally expensive and impractical in resource-constrained settings. In\ncontrast, Small Language Models (SLMs) can operate efficiently, offering faster\nresponse times, and lower computational demands, making them potential\ncandidates for function calling on edge devices. In this exploratory empirical\nstudy, we evaluate the efficacy of SLMs in generating function calls across\ndiverse domains using zero-shot, few-shot, and fine-tuning approaches, both\nwith and without prompt injection, while also providing the finetuned models to\nfacilitate future applications. Furthermore, we analyze the model responses\nacross a range of metrics, capturing various aspects of function call\ngeneration. Additionally, we perform experiments on an edge device to evaluate\ntheir performance in terms of latency and memory usage, providing useful\ninsights into their practical applicability. Our findings show that while SLMs\nimprove from zero-shot to few-shot and perform best with fine-tuning, they\nstruggle significantly with adhering to the given output format. Prompt\ninjection experiments further indicate that the models are generally robust and\nexhibit only a slight decline in performance. While SLMs demonstrate potential\nfor the function call generation task, our results also highlight areas that\nneed further refinement for real-time functioning.",
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at EASE 2025 AI Models and Data Evaluation track",
    "pdf_url": "http://arxiv.org/pdf/2504.19277v1",
    "published_date": "2025-04-27 15:26:51 UTC",
    "updated_date": "2025-04-27 15:26:51 UTC"
  },
  {
    "arxiv_id": "2504.19276v1",
    "title": "Anyprefer: An Agentic Framework for Preference Data Synthesis",
    "authors": [
      "Yiyang Zhou",
      "Zhaoyang Wang",
      "Tianle Wang",
      "Shangyu Xing",
      "Peng Xia",
      "Bo Li",
      "Kaiyuan Zheng",
      "Zijian Zhang",
      "Zhaorun Chen",
      "Wenhao Zheng",
      "Xuchao Zhang",
      "Chetan Bansal",
      "Weitong Zhang",
      "Ying Wei",
      "Mohit Bansal",
      "Huaxiu Yao"
    ],
    "abstract": "High-quality preference data is essential for aligning foundation models with\nhuman values through preference learning. However, manual annotation of such\ndata is often time-consuming and costly. Recent methods often adopt a\nself-rewarding approach, where the target model generates and annotates its own\npreference data, but this can lead to inaccuracies since the reward model\nshares weights with the target model, thereby amplifying inherent biases. To\naddress these issues, we propose Anyprefer, a framework designed to synthesize\nhigh-quality preference data for aligning the target model. Anyprefer frames\nthe data synthesis process as a cooperative two-player Markov Game, where the\ntarget model and the judge model collaborate together. Here, a series of\nexternal tools are introduced to assist the judge model in accurately rewarding\nthe target model's responses, mitigating biases in the rewarding process. In\naddition, a feedback mechanism is introduced to optimize prompts for both\nmodels, enhancing collaboration and improving data quality. The synthesized\ndata is compiled into a new preference dataset, Anyprefer-V1, consisting of 58K\nhigh-quality preference pairs. Extensive experiments show that Anyprefer\nsignificantly improves model alignment performance across four main\napplications, covering 21 datasets, achieving average improvements of 18.55% in\nfive natural language generation datasets, 3.66% in nine vision-language\nunderstanding datasets, 30.05% in three medical image analysis datasets, and\n16.00% in four visuo-motor control tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.19276v1",
    "published_date": "2025-04-27 15:21:59 UTC",
    "updated_date": "2025-04-27 15:21:59 UTC"
  },
  {
    "arxiv_id": "2504.19275v1",
    "title": "Balancing Creativity and Automation: The Influence of AI on Modern Film Production and Dissemination",
    "authors": [
      "Yiren Xu"
    ],
    "abstract": "The integration of Artificial Intelligence(AI) into film production has\nrevolutionized efficiency and creativity, yet it simultaneously raises critical\nethical and practical challenges. This study explores the dual impact of AI on\nmodern cinema through three objectives: defining the optimal human-AI\nrelationship, balancing creativity with automation, and developing ethical\nguidelines. By employing a mixed-method approach combining theoretical\nframeworks (auteur theory, human-technology relations) and case studies (The\nSafe Zone, Fast & Furious 7, The Brutalist), the research reveals that\npositioning AI as an \"embodiment tool\" rather than an independent \"alterity\npartner\" preserves human authorship and artistic integrity. Key findings\nhighlight the risks of surveillance capitalism in AI-driven markets and the\nethical dilemmas of deepfake technology. The study concludes with actionable\nrecommendations, including international regulatory frameworks and a Human\nControl Index (HCI) to quantify AI involvement. These insights aim to guide\nfilmmakers, policymakers, and scholars in navigating the evolving AI-cinema\nlandscape while safeguarding cultural diversity and ethical standards.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "I.5.m"
    ],
    "primary_category": "cs.CY",
    "comment": "19 pages, 1 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2504.19275v1",
    "published_date": "2025-04-27 15:21:38 UTC",
    "updated_date": "2025-04-27 15:21:38 UTC"
  },
  {
    "arxiv_id": "2504.19274v1",
    "title": "TeleSparse: Practical Privacy-Preserving Verification of Deep Neural Networks",
    "authors": [
      "Mohammad M Maheri",
      "Hamed Haddadi",
      "Alex Davidson"
    ],
    "abstract": "Verification of the integrity of deep learning inference is crucial for\nunderstanding whether a model is being applied correctly. However, such\nverification typically requires access to model weights and (potentially\nsensitive or private) training data. So-called Zero-knowledge Succinct\nNon-Interactive Arguments of Knowledge (ZK-SNARKs) would appear to provide the\ncapability to verify model inference without access to such sensitive data.\nHowever, applying ZK-SNARKs to modern neural networks, such as transformers and\nlarge vision models, introduces significant computational overhead.\n  We present TeleSparse, a ZK-friendly post-processing mechanisms to produce\npractical solutions to this problem. TeleSparse tackles two fundamental\nchallenges inherent in applying ZK-SNARKs to modern neural networks: (1)\nReducing circuit constraints: Over-parameterized models result in numerous\nconstraints for ZK-SNARK verification, driving up memory and proof generation\ncosts. We address this by applying sparsification to neural network models,\nenhancing proof efficiency without compromising accuracy or security. (2)\nMinimizing the size of lookup tables required for non-linear functions, by\noptimizing activation ranges through neural teleportation, a novel adaptation\nfor narrowing activation functions' range.\n  TeleSparse reduces prover memory usage by 67% and proof generation time by\n46% on the same model, with an accuracy trade-off of approximately 1%. We\nimplement our framework using the Halo2 proving system and demonstrate its\neffectiveness across multiple architectures (Vision-transformer, ResNet,\nMobileNet) and datasets (ImageNet,CIFAR-10,CIFAR-100). This work opens new\ndirections for ZK-friendly model design, moving toward scalable,\nresource-efficient verifiable deep learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper has been accepted to the Privacy Enhancing Technologies\n  Symposium (PETS) 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.19274v1",
    "published_date": "2025-04-27 15:14:09 UTC",
    "updated_date": "2025-04-27 15:14:09 UTC"
  },
  {
    "arxiv_id": "2504.19267v2",
    "title": "VIST-GPT: Ushering in the Era of Visual Storytelling with LLMs?",
    "authors": [
      "Mohamed Gado",
      "Towhid Taliee",
      "Muhammad Memon",
      "Dmitry Ignatov",
      "Radu Timofte"
    ],
    "abstract": "Visual storytelling is an interdisciplinary field combining computer vision\nand natural language processing to generate cohesive narratives from sequences\nof images. This paper presents a novel approach that leverages recent\nadvancements in multimodal models, specifically adapting transformer-based\narchitectures and large multimodal models, for the visual storytelling task.\nLeveraging the large-scale Visual Storytelling (VIST) dataset, our VIST-GPT\nmodel produces visually grounded, contextually appropriate narratives. We\naddress the limitations of traditional evaluation metrics, such as BLEU,\nMETEOR, ROUGE, and CIDEr, which are not suitable for this task. Instead, we\nutilize RoViST and GROOVIST, novel reference-free metrics designed to assess\nvisual storytelling, focusing on visual grounding, coherence, and\nnon-redundancy. These metrics provide a more nuanced evaluation of narrative\nquality, aligning closely with human judgment.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.19267v2",
    "published_date": "2025-04-27 14:55:51 UTC",
    "updated_date": "2025-05-03 08:32:16 UTC"
  },
  {
    "arxiv_id": "2505.00028v1",
    "title": "Enhancing Speech-to-Speech Dialogue Modeling with End-to-End Retrieval-Augmented Generation",
    "authors": [
      "Pengchao Feng",
      "Ziyang Ma",
      "Wenxi Chen",
      "Yao Li",
      "Sheng Wang",
      "Kai Yu",
      "Xie Chen"
    ],
    "abstract": "In recent years, end-to-end speech-to-speech (S2S) dialogue systems have\ngarnered increasing research attention due to their advantages over traditional\ncascaded systems, including achieving lower latency and more natural\nintegration of nonverbal cues such as emotion and speaker identity. However,\nthese end-to-end systems face key challenges, particularly in incorporating\nexternal knowledge, a capability commonly addressed by Retrieval-Augmented\nGeneration (RAG) in text-based large language models (LLMs). The core\ndifficulty lies in the modality gap between input speech and retrieved textual\nknowledge, which hinders effective integration. To address this issue, we\npropose a novel end-to-end RAG framework that directly retrieves relevant\ntextual knowledge from speech queries, eliminating the need for intermediate\nspeech-to-text conversion via techniques like ASR. Experimental results\ndemonstrate that our method significantly improves the performance of\nend-to-end S2S dialogue systems while achieving higher retrieval efficiency.\nAlthough the overall performance still lags behind cascaded models, our\nframework offers a promising direction for enhancing knowledge integration in\nend-to-end S2S systems. We will release the code and dataset to support\nreproducibility and promote further research in this area.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.00028v1",
    "published_date": "2025-04-27 14:35:24 UTC",
    "updated_date": "2025-04-27 14:35:24 UTC"
  },
  {
    "arxiv_id": "2504.19255v1",
    "title": "The Convergent Ethics of AI? Analyzing Moral Foundation Priorities in Large Language Models with a Multi-Framework Approach",
    "authors": [
      "Chad Coleman",
      "W. Russell Neuman",
      "Ali Dasdan",
      "Safinah Ali",
      "Manan Shah"
    ],
    "abstract": "As large language models (LLMs) are increasingly deployed in consequential\ndecision-making contexts, systematically assessing their ethical reasoning\ncapabilities becomes a critical imperative. This paper introduces the\nPriorities in Reasoning and Intrinsic Moral Evaluation (PRIME) framework--a\ncomprehensive methodology for analyzing moral priorities across foundational\nethical dimensions including consequentialist-deontological reasoning, moral\nfoundations theory, and Kohlberg's developmental stages. We apply this\nframework to six leading LLMs through a dual-protocol approach combining direct\nquestioning and response analysis to established ethical dilemmas. Our analysis\nreveals striking patterns of convergence: all evaluated models demonstrate\nstrong prioritization of care/harm and fairness/cheating foundations while\nconsistently underweighting authority, loyalty, and sanctity dimensions.\nThrough detailed examination of confidence metrics, response reluctance\npatterns, and reasoning consistency, we establish that contemporary LLMs (1)\nproduce decisive ethical judgments, (2) demonstrate notable cross-model\nalignment in moral decision-making, and (3) generally correspond with\nempirically established human moral preferences. This research contributes a\nscalable, extensible methodology for ethical benchmarking while highlighting\nboth the promising capabilities and systematic limitations in current AI moral\nreasoning architectures--insights critical for responsible development as these\nsystems assume increasingly significant societal roles.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "25 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.19255v1",
    "published_date": "2025-04-27 14:26:48 UTC",
    "updated_date": "2025-04-27 14:26:48 UTC"
  },
  {
    "arxiv_id": "2504.19254v2",
    "title": "Uncertainty Quantification for Language Models: A Suite of Black-Box, White-Box, LLM Judge, and Ensemble Scorers",
    "authors": [
      "Dylan Bouchard",
      "Mohit Singh Chauhan"
    ],
    "abstract": "Hallucinations are a persistent problem with Large Language Models (LLMs). As\nthese models become increasingly used in high-stakes domains, such as\nhealthcare and finance, the need for effective hallucination detection is\ncrucial. To this end, we propose a versatile framework for zero-resource\nhallucination detection that practitioners can apply to real-world use cases.\nTo achieve this, we adapt a variety of existing uncertainty quantification (UQ)\ntechniques, including black-box UQ, white-box UQ, and LLM-as-a-Judge,\ntransforming them as necessary into standardized response-level confidence\nscores ranging from 0 to 1. To enhance flexibility, we introduce a tunable\nensemble approach that incorporates any combination of the individual\nconfidence scores. This approach enables practitioners to optimize the ensemble\nfor a specific use case for improved performance. To streamline implementation,\nthe full suite of scorers is offered in this paper's companion Python toolkit,\nUQLM. To evaluate the performance of the various scorers, we conduct an\nextensive set of experiments using several LLM question-answering benchmarks.\nWe find that our tunable ensemble typically surpasses its individual components\nand outperforms existing hallucination detection methods. Our results\ndemonstrate the benefits of customized hallucination detection strategies for\nimproving the accuracy and reliability of LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "UQLM repository: https://github.com/cvs-health/uqlm",
    "pdf_url": "http://arxiv.org/pdf/2504.19254v2",
    "published_date": "2025-04-27 14:24:45 UTC",
    "updated_date": "2025-04-30 16:49:15 UTC"
  },
  {
    "arxiv_id": "2504.19223v1",
    "title": "CARL: Camera-Agnostic Representation Learning for Spectral Image Analysis",
    "authors": [
      "Alexander Baumann",
      "Leonardo Ayala",
      "Silvia Seidlitz",
      "Jan Sellner",
      "Alexander Studier-Fischer",
      "Berkin Özdemir",
      "Lena Maier-Hein",
      "Slobodan Ilic"
    ],
    "abstract": "Spectral imaging offers promising applications across diverse domains,\nincluding medicine and urban scene understanding, and is already established as\na critical modality in remote sensing. However, variability in channel\ndimensionality and captured wavelengths among spectral cameras impede the\ndevelopment of AI-driven methodologies, leading to camera-specific models with\nlimited generalizability and inadequate cross-camera applicability. To address\nthis bottleneck, we introduce $\\textbf{CARL}$, a model for\n$\\textbf{C}$amera-$\\textbf{A}$gnostic $\\textbf{R}$epresentation\n$\\textbf{L}$earning across RGB, multispectral, and hyperspectral imaging\nmodalities. To enable the conversion of a spectral image with any channel\ndimensionality to a camera-agnostic embedding, we introduce wavelength\npositional encoding and a self-attention-cross-attention mechanism to compress\nspectral information into learned query representations. Spectral-spatial\npre-training is achieved with a novel spectral self-supervised JEPA-inspired\nstrategy tailored to CARL. Large-scale experiments across the domains of\nmedical imaging, autonomous driving, and satellite imaging demonstrate our\nmodel's unique robustness to spectral heterogeneity, outperforming on datasets\nwith simulated and real-world cross-camera spectral variations. The scalability\nand versatility of the proposed approach position our model as a backbone for\nfuture spectral foundation models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.19223v1",
    "published_date": "2025-04-27 13:06:40 UTC",
    "updated_date": "2025-04-27 13:06:40 UTC"
  },
  {
    "arxiv_id": "2505.06241v1",
    "title": "Low-Complexity CNN-Based Classification of Electroneurographic Signals",
    "authors": [
      "Arek Berc Gokdag",
      "Silvia Mura",
      "Antonio Coviello",
      "Michele Zhu",
      "Maurizio Magarini",
      "Umberto Spagnolini"
    ],
    "abstract": "Peripheral nerve interfaces (PNIs) facilitate neural recording and\nstimulation for treating nerve injuries, but real-time classification of\nelectroneurographic (ENG) signals remains challenging due to constraints on\ncomplexity and latency, particularly in implantable devices. This study\nintroduces MobilESCAPE-Net, a lightweight architecture that reduces\ncomputational cost while maintaining and slightly improving classification\nperformance. Compared to the state-of-the-art ESCAPE-Net, MobilESCAPE-Net\nachieves comparable accuracy and F1-score with significantly lower complexity,\nreducing trainable parameters by 99.9\\% and floating point operations per\nsecond by 92.47\\%, enabling faster inference and real-time processing. Its\nefficiency makes it well-suited for low-complexity ENG signal classification in\nresource-constrained environments such as implantable devices.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.06241v1",
    "published_date": "2025-04-27 12:45:01 UTC",
    "updated_date": "2025-04-27 12:45:01 UTC"
  },
  {
    "arxiv_id": "2504.19212v1",
    "title": "CapsFake: A Multimodal Capsule Network for Detecting Instruction-Guided Deepfakes",
    "authors": [
      "Tuan Nguyen",
      "Naseem Khan",
      "Issa Khalil"
    ],
    "abstract": "The rapid evolution of deepfake technology, particularly in\ninstruction-guided image editing, threatens the integrity of digital images by\nenabling subtle, context-aware manipulations. Generated conditionally from real\nimages and textual prompts, these edits are often imperceptible to both humans\nand existing detection systems, revealing significant limitations in current\ndefenses. We propose a novel multimodal capsule network, CapsFake, designed to\ndetect such deepfake image edits by integrating low-level capsules from visual,\ntextual, and frequency-domain modalities. High-level capsules, predicted\nthrough a competitive routing mechanism, dynamically aggregate local features\nto identify manipulated regions with precision. Evaluated on diverse datasets,\nincluding MagicBrush, Unsplash Edits, Open Images Edits, and Multi-turn Edits,\nCapsFake outperforms state-of-the-art methods by up to 20% in detection\naccuracy. Ablation studies validate its robustness, achieving detection rates\nabove 94% under natural perturbations and 96% against adversarial attacks, with\nexcellent generalization to unseen editing scenarios. This approach establishes\na powerful framework for countering sophisticated image manipulations.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "20 pages",
    "pdf_url": "http://arxiv.org/pdf/2504.19212v1",
    "published_date": "2025-04-27 12:31:47 UTC",
    "updated_date": "2025-04-27 12:31:47 UTC"
  },
  {
    "arxiv_id": "2504.20106v1",
    "title": "Adaptive Helpfulness-Harmlessness Alignment with Preference Vectors",
    "authors": [
      "Ren-Wei Liang",
      "Chin-Ting Hsu",
      "Chan-Hung Yu",
      "Saransh Agrawal",
      "Shih-Cheng Huang",
      "Shang-Tse Chen",
      "Kuan-Hao Huang",
      "Shao-Hua Sun"
    ],
    "abstract": "Ensuring that large language models (LLMs) are both helpful and harmless is a\ncritical challenge, as overly strict constraints can lead to excessive\nrefusals, while permissive models risk generating harmful content. Existing\napproaches, such as reinforcement learning from human feedback (RLHF) and\ndirect preference optimization (DPO), attempt to balance these trade-offs but\nsuffer from performance conflicts, limited controllability, and poor\nextendability. To address these issues, we propose Preference Vector, a novel\nframework inspired by task arithmetic. Instead of optimizing multiple\npreferences within a single objective, we train separate models on individual\npreferences, extract behavior shifts as preference vectors, and dynamically\nmerge them at test time. This modular approach enables fine-grained,\nuser-controllable preference adjustments and facilitates seamless integration\nof new preferences without retraining. Experiments show that our proposed\nPreference Vector framework improves helpfulness without excessive\nconservatism, allows smooth control over preference trade-offs, and supports\nscalable multi-preference alignment.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "22 pages, 5 figures, 9 tables",
    "pdf_url": "http://arxiv.org/pdf/2504.20106v1",
    "published_date": "2025-04-27 12:16:51 UTC",
    "updated_date": "2025-04-27 12:16:51 UTC"
  },
  {
    "arxiv_id": "2504.20105v1",
    "title": "Electricity Cost Minimization for Multi-Workflow Allocation in Geo-Distributed Data Centers",
    "authors": [
      "Shuang Wang",
      "He Zhang",
      "Tianxing Wu",
      "Yueyou Zhang",
      "Wei Emma Zhang",
      "Quan Z. Sheng"
    ],
    "abstract": "Worldwide, Geo-distributed Data Centers (GDCs) provide computing and storage\nservices for massive workflow applications, resulting in high electricity costs\nthat vary depending on geographical locations and time. How to reduce\nelectricity costs while satisfying the deadline constraints of workflow\napplications is important in GDCs, which is determined by the execution time of\nservers, power, and electricity price. Determining the completion time of\nworkflows with different server frequencies can be challenging, especially in\nscenarios with heterogeneous computing resources in GDCs. Moreover, the\nelectricity price is also different in geographical locations and may change\ndynamically. To address these challenges, we develop a geo-distributed system\narchitecture and propose an Electricity Cost aware Multiple Workflows\nScheduling algorithm (ECMWS) for servers of GDCs with fixed frequency and\npower. ECMWS comprises four stages, namely workflow sequencing, deadline\npartitioning, task sequencing, and resource allocation where two graph\nembedding models and a policy network are constructed to solve the Markov\nDecision Process (MDP). After statistically calibrating parameters and\nalgorithm components over a comprehensive set of workflow instances, the\nproposed algorithms are compared with the state-of-the-art methods over two\ntypes of workflow instances. The experimental results demonstrate that our\nproposed algorithm significantly outperforms other algorithms, achieving an\nimprovement of over 15\\% while maintaining an acceptable computational time.\nThe source codes are available at\nhttps://gitee.com/public-artifacts/ecmws-experiments.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "have been accepted by IEEE Transactions on Services Computing",
    "pdf_url": "http://arxiv.org/pdf/2504.20105v1",
    "published_date": "2025-04-27 11:56:48 UTC",
    "updated_date": "2025-04-27 11:56:48 UTC"
  },
  {
    "arxiv_id": "2504.19197v1",
    "title": "Generative Adversarial Network based Voice Conversion: Techniques, Challenges, and Recent Advancements",
    "authors": [
      "Sandipan Dhar",
      "Nanda Dulal Jana",
      "Swagatam Das"
    ],
    "abstract": "Voice conversion (VC) stands as a crucial research area in speech synthesis,\nenabling the transformation of a speaker's vocal characteristics to resemble\nanother while preserving the linguistic content. This technology has broad\napplications, including automated movie dubbing, speech-to-singing conversion,\nand assistive devices for pathological speech rehabilitation. With the\nincreasing demand for high-quality and natural-sounding synthetic voices,\nresearchers have developed a wide range of VC techniques. Among these,\ngenerative adversarial network (GAN)-based approaches have drawn considerable\nattention for their powerful feature-mapping capabilities and potential to\nproduce highly realistic speech. Despite notable advancements, challenges such\nas ensuring training stability, maintaining linguistic consistency, and\nachieving perceptual naturalness continue to hinder progress in GAN-based VC\nsystems. This systematic review presents a comprehensive analysis of the voice\nconversion landscape, highlighting key techniques, key challenges, and the\ntransformative impact of GANs in the field. The survey categorizes existing\nmethods, examines technical obstacles, and critically evaluates recent\ndevelopments in GAN-based VC. By consolidating and synthesizing research\nfindings scattered across the literature, this review provides a structured\nunderstanding of the strengths and limitations of different approaches. The\nsignificance of this survey lies in its ability to guide future research by\nidentifying existing gaps, proposing potential directions, and offering\ninsights for building more robust and efficient VC systems. Overall, this work\nserves as an essential resource for researchers, developers, and practitioners\naiming to advance the state-of-the-art (SOTA) in voice conversion technology.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "19 pages, 12 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2504.19197v1",
    "published_date": "2025-04-27 11:22:21 UTC",
    "updated_date": "2025-04-27 11:22:21 UTC"
  },
  {
    "arxiv_id": "2504.19188v1",
    "title": "Hierarchical Attention Generates Better Proofs",
    "authors": [
      "Jianlong Chen",
      "Chao Li",
      "Yang Yuan",
      "Andrew C Yao"
    ],
    "abstract": "Large language models (LLMs) have shown promise in formal theorem proving,\nbut their token-level processing often fails to capture the inherent\nhierarchical nature of mathematical proofs. We introduce \\textbf{Hierarchical\nAttention}, a regularization method that aligns LLMs' attention mechanisms with\nmathematical reasoning structures. Our approach establishes a five-level\nhierarchy from foundational elements to high-level concepts, ensuring\nstructured information flow in proof generation. Experiments demonstrate that\nour method improves proof success rates by 2.05\\% on miniF2F and 1.69\\% on\nProofNet while reducing proof complexity by 23.81\\% and 16.50\\% respectively.\nThe code is available at https://github.com/Car-pe/HAGBP.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.LO"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages with 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.19188v1",
    "published_date": "2025-04-27 10:35:05 UTC",
    "updated_date": "2025-04-27 10:35:05 UTC"
  },
  {
    "arxiv_id": "2504.19179v1",
    "title": "A Design Framework for operationalizing Trustworthy Artificial Intelligence in Healthcare: Requirements, Tradeoffs and Challenges for its Clinical Adoption",
    "authors": [
      "Pedro A. Moreno-Sánchez",
      "Javier Del Ser",
      "Mark van Gils",
      "Jussi Hernesniemi"
    ],
    "abstract": "Artificial Intelligence (AI) holds great promise for transforming healthcare,\nparticularly in disease diagnosis, prognosis, and patient care. The increasing\navailability of digital medical data, such as images, omics, biosignals, and\nelectronic health records, combined with advances in computing, has enabled AI\nmodels to approach expert-level performance. However, widespread clinical\nadoption remains limited, primarily due to challenges beyond technical\nperformance, including ethical concerns, regulatory barriers, and lack of\ntrust. To address these issues, AI systems must align with the principles of\nTrustworthy AI (TAI), which emphasize human agency and oversight, algorithmic\nrobustness, privacy and data governance, transparency, bias and discrimination\navoidance, and accountability. Yet, the complexity of healthcare processes\n(e.g., screening, diagnosis, prognosis, and treatment) and the diversity of\nstakeholders (clinicians, patients, providers, regulators) complicate the\nintegration of TAI principles. To bridge the gap between TAI theory and\npractical implementation, this paper proposes a design framework to support\ndevelopers in embedding TAI principles into medical AI systems. Thus, for each\nstakeholder identified across various healthcare processes, we propose a\ndisease-agnostic collection of requirements that medical AI systems should\nincorporate to adhere to the principles of TAI. Additionally, we examine the\nchallenges and tradeoffs that may arise when applying these principles in\npractice. To ground the discussion, we focus on cardiovascular diseases, a\nfield marked by both high prevalence and active AI innovation, and demonstrate\nhow TAI principles have been applied and where key obstacles persist.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.19179v1",
    "published_date": "2025-04-27 09:57:35 UTC",
    "updated_date": "2025-04-27 09:57:35 UTC"
  },
  {
    "arxiv_id": "2504.20103v1",
    "title": "Heterogeneous network drug-target interaction prediction model based on graph wavelet transform and multi-level contrastive learning",
    "authors": [
      "Wenfeng Dai",
      "Yanhong Wang",
      "Shuai Yan",
      "Qingzhi Yu",
      "Xiang Cheng"
    ],
    "abstract": "Drug-target interaction (DTI) prediction is a core task in drug development\nand precision medicine in the biomedical field. However, traditional machine\nlearning methods generally have the black box problem, which makes it difficult\nto reveal the deep correlation between the model decision mechanism and the\ninteraction pattern between biological molecules. This study proposes a\nheterogeneous network drug target interaction prediction framework, integrating\ngraph neural network and multi scale signal processing technology to construct\na model with both efficient prediction and multi level interpretability. Its\ntechnical breakthroughs are mainly reflected in the following three\ndimensions:Local global feature collaborative perception module. Based on\nheterogeneous graph convolutional neural network (HGCN), a multi order neighbor\naggregation strategy is designed.Multi scale graph signal decomposition and\nbiological interpretation module. A deep hierarchical node feature transform\n(GWT) architecture is proposed.Contrastive learning combining multi dimensional\nperspectives and hierarchical representations. By comparing the learning\nmodels, the node representations from the two perspectives of HGCN and GWT are\naligned and fused, so that the model can integrate multi dimensional\ninformation and improve the prediction robustness. Experimental results show\nthat our framework shows excellent prediction performance on all datasets. This\nstudy provides a complete solution for drug target discovery from black box\nprediction to mechanism decoding, and its methodology has important reference\nvalue for modeling complex biomolecular interaction systems.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.QM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.20103v1",
    "published_date": "2025-04-27 09:29:50 UTC",
    "updated_date": "2025-04-27 09:29:50 UTC"
  },
  {
    "arxiv_id": "2504.20102v1",
    "title": "HyboWaveNet: Hyperbolic Graph Neural Networks with Multi-Scale Wavelet Transform for Protein-Protein Interaction Prediction",
    "authors": [
      "Qingzhi Yu",
      "Shuai Yan",
      "Wenfeng Dai",
      "Xiang Cheng"
    ],
    "abstract": "Protein-protein interactions (PPIs) are fundamental for deciphering cellular\nfunctions,disease pathways,and drug discovery.Although existing neural networks\nand machine learning methods have achieved high accuracy in PPI\nprediction,their black-box nature leads to a lack of causal interpretation of\nthe prediction results and difficulty in capturing hierarchical geometries and\nmulti-scale dynamic interaction patterns among proteins.To address these\nchallenges, we propose HyboWaveNet,a novel deep learning framework that\ncollaborates with hyperbolic graphical neural networks (HGNNs) and multiscale\ngraphical wavelet transform for robust PPI prediction. Mapping protein features\nto Lorentz space simulates hierarchical topological relationships among\nbiomolecules via a hyperbolic distance metric,enabling node feature\nrepresentations that better fit biological a priori.HyboWaveNet inherently\nsimulates hierarchical and scale-free biological relationships, while the\nintegration of wavelet transforms enables adaptive extraction of local and\nglobal interaction features across different resolutions. Our framework\ngenerates node feature representations via a graph neural network under the\nLorenz model and generates pairs of positive samples under multiple different\nviews for comparative learning, followed by further feature extraction via\nmulti-scale graph wavelet transforms to predict potential PPIs. Experiments on\npublic datasets show that HyboWaveNet improves over both existing\nstate-of-the-art methods. We also demonstrate through ablation experimental\nstudies that the multi-scale graph wavelet transform module improves the\npredictive performance and generalization ability of HyboWaveNet. This work\nlinks geometric deep learning and signal processing to advance PPI prediction,\nproviding a principled approach for analyzing complex biological systems",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages",
    "pdf_url": "http://arxiv.org/pdf/2504.20102v1",
    "published_date": "2025-04-27 09:20:50 UTC",
    "updated_date": "2025-04-27 09:20:50 UTC"
  },
  {
    "arxiv_id": "2504.19162v2",
    "title": "SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning",
    "authors": [
      "Jiaqi Chen",
      "Bang Zhang",
      "Ruotian Ma",
      "Peisong Wang",
      "Xiaodan Liang",
      "Zhaopeng Tu",
      "Xiaolong Li",
      "Kwan-Yee K. Wong"
    ],
    "abstract": "Evaluating the step-by-step reliability of large language model (LLM)\nreasoning, such as Chain-of-Thought, remains challenging due to the difficulty\nand cost of obtaining high-quality step-level supervision. In this paper, we\nintroduce Self-Play Critic (SPC), a novel approach where a critic model evolves\nits ability to assess reasoning steps through adversarial self-play games,\neliminating the need for manual step-level annotation. SPC involves fine-tuning\ntwo copies of a base model to play two roles, namely a \"sneaky generator\" that\ndeliberately produces erroneous steps designed to be difficult to detect, and a\n\"critic\" that analyzes the correctness of reasoning steps. These two models\nengage in an adversarial game in which the generator aims to fool the critic,\nwhile the critic model seeks to identify the generator's errors. Using\nreinforcement learning based on the game outcomes, the models iteratively\nimprove; the winner of each confrontation receives a positive reward and the\nloser receives a negative reward, driving continuous self-evolution.\nExperiments on three reasoning process benchmarks (ProcessBench, PRM800K,\nDeltaBench) demonstrate that our SPC progressively enhances its error detection\ncapabilities (e.g., accuracy increases from 70.8% to 77.7% on ProcessBench) and\nsurpasses strong baselines, including distilled R1 model. Furthermore, SPC can\nguide the test-time search of diverse LLMs and significantly improve their\nmathematical reasoning performance on MATH500 and AIME2024, surpassing those\nguided by state-of-the-art process reward models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Project webpage: https://chen-judge.github.io/SPC/",
    "pdf_url": "http://arxiv.org/pdf/2504.19162v2",
    "published_date": "2025-04-27 08:45:06 UTC",
    "updated_date": "2025-05-17 14:32:38 UTC"
  },
  {
    "arxiv_id": "2504.19155v1",
    "title": "Machine Learning-Based Modeling of the Anode Heel Effect in X-ray Beam Monte Carlo Simulations",
    "authors": [
      "Hussein Harb",
      "Didier Benoit",
      "Axel Rannou",
      "Chi-Hieu Pham",
      "Valentin Tissot",
      "Bahaa Nasr",
      "Julien Bert"
    ],
    "abstract": "This study enhances Monte Carlo simulation accuracy in X-ray imaging by\ndeveloping an AI-driven model for the anode heel effect, achieving improved\nbeam intensity distribution and dosimetric precision. Through dynamic\nadjustments to beam weights on the anode and cathode sides of the X-ray tube,\nour machine learning model effectively replicates the asymmetry characteristic\nof clinical X-ray beams. Experimental results reveal dose rate increases of up\nto 9.6% on the cathode side and reductions of up to 12.5% on the anode side,\nfor energy levels between 50 and 120 kVp. These experimentally optimized beam\nweights were integrated into the OpenGATE and GGEMS Monte Carlo toolkits,\nsignificantly advancing dosimetric simulation accuracy and the image quality\nwhich closely resembles the clinical imaging. Validation with fluence and dose\nactors demonstrated that the AI-based model closely mirrors clinical beam\nbehavior, providing substantial improvements in dose consistency and accuracy\nover conventional X-ray models. This approach provides a robust framework for\nimproving X-ray dosimetry, with potential applications in dose optimization,\nimaging quality enhancement, and radiation safety in both clinical and research\nsettings.",
    "categories": [
      "physics.med-ph",
      "cs.AI"
    ],
    "primary_category": "physics.med-ph",
    "comment": "15 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.19155v1",
    "published_date": "2025-04-27 08:19:47 UTC",
    "updated_date": "2025-04-27 08:19:47 UTC"
  },
  {
    "arxiv_id": "2504.21032v1",
    "title": "Selecting the Right LLM for eGov Explanations",
    "authors": [
      "Lior Limonad",
      "Fabiana Fournier",
      "Hadar Mulian",
      "George Manias",
      "Spiros Borotis",
      "Danai Kyrkou"
    ],
    "abstract": "The perceived quality of the explanations accompanying e-government services\nis key to gaining trust in these institutions, consequently amplifying further\nusage of these services. Recent advances in generative AI, and concretely in\nLarge Language Models (LLMs) allow the automation of such content\narticulations, eliciting explanations' interpretability and fidelity, and more\ngenerally, adapting content to various audiences. However, selecting the right\nLLM type for this has become a non-trivial task for e-government service\nproviders. In this work, we adapted a previously developed scale to assist with\nthis selection, providing a systematic approach for the comparative analysis of\nthe perceived quality of explanations generated by various LLMs. We further\ndemonstrated its applicability through the tax-return process, using it as an\nexemplar use case that could benefit from employing an LLM to generate\nexplanations about tax refund decisions. This was attained through a user study\nwith 128 survey respondents who were asked to rate different versions of\nLLM-generated explanations about tax refund decisions, providing a\nmethodological basis for selecting the most appropriate LLM. Recognizing the\npractical challenges of conducting such a survey, we also began exploring the\nautomation of this process by attempting to replicate human feedback using a\nselection of cutting-edge predictive techniques.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "8 pages, 7 figures. ICEDEG 2025, Bern, Switzerland, June 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.21032v1",
    "published_date": "2025-04-27 08:09:12 UTC",
    "updated_date": "2025-04-27 08:09:12 UTC"
  },
  {
    "arxiv_id": "2504.19148v1",
    "title": "A Dynamic Fuzzy Rule and Attribute Management Framework for Fuzzy Inference Systems in High-Dimensional Data",
    "authors": [
      "Ke Liu",
      "Jing Ma",
      "Edmund M-K Lai"
    ],
    "abstract": "This paper presents an Adaptive Dynamic Attribute and Rule (ADAR) framework\ndesigned to address the challenges posed by high-dimensional data in\nneuro-fuzzy inference systems. By integrating dual weighting\nmechanisms-assigning adaptive importance to both attributes and rules-together\nwith automated growth and pruning strategies, ADAR adaptively streamlines\ncomplex fuzzy models without sacrificing performance or interpretability.\nExperimental evaluations on four diverse datasets - Auto MPG (7 variables),\nBeijing PM2.5 (10 variables), Boston Housing (13 variables), and Appliances\nEnergy Consumption (27 variables) show that ADAR-based models achieve\nconsistently lower Root Mean Square Error (RMSE) compared to state-of-the-art\nbaselines. On the Beijing PM2.5 dataset, for instance, ADAR-SOFENN attained an\nRMSE of 56.87 with nine rules, surpassing traditional ANFIS [12] and SOFENN\n[16] models. Similarly, on the high-dimensional Appliances Energy dataset,\nADAR-ANFIS reached an RMSE of 83.25 with nine rules, outperforming established\nfuzzy logic approaches and interpretability-focused methods such as APLR.\nAblation studies further reveal that combining rule-level and attribute-level\nweight assignment significantly reduces model overlap while preserving\nessential features, thereby enhancing explainability. These results highlight\nADAR's effectiveness in dynamically balancing rule complexity and feature\nimportance, paving the way for scalable, high-accuracy, and transparent\nneuro-fuzzy systems applicable to a range of real-world scenarios.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.19148v1",
    "published_date": "2025-04-27 08:02:10 UTC",
    "updated_date": "2025-04-27 08:02:10 UTC"
  },
  {
    "arxiv_id": "2504.19144v1",
    "title": "ChiseLLM: Unleashing the Power of Reasoning LLMs for Chisel Agile Hardware Development",
    "authors": [
      "Bowei Wang",
      "Jiaran Gao",
      "Yelai Feng",
      "Renzhi Chen",
      "Shanshan Li",
      "Lei Wang"
    ],
    "abstract": "The growing demand for Domain-Specific Architecture (DSA) has driven the\ndevelopment of Agile Hardware Development Methodology (AHDM). Hardware\nConstruction Language (HCL) like Chisel offers high-level abstraction features,\nmaking it an ideal language for HCL-Based AHDM. While Large Language Models\n(LLMs) excel in code generation tasks, they still face challenges with Chisel\ngeneration, particularly regarding syntax correctness and design variability.\nRecent reasoning models have significantly enhanced code generation\ncapabilities through test-time scaling techniques. However, we found that\nreasoning models without domain adaptation cannot bring substantial benefits to\nChisel code generation tasks. This paper presents ChiseLLM, a solution\ncomprising data processing and transformation, prompt-guided reasoning trace\nsynthesis, and domain-adapted model training. We constructed high-quality\ndatasets from public RTL code resources and guided the model to adopt\nstructured thinking patterns through prompt enhancement methods. Experiments\ndemonstrate that our ChiseLLM-7B and ChiseLLM-32B models improved syntax\ncorrectness by 18.85% and 26.32% respectively over base models, while\nincreasing variability design ability by 47.58% compared to baseline reasoning\nmodels. Our datasets and models are publicly available, providing\nhigh-performance, cost-effective models for HCL-Based AHDM, and offering an\neffective baseline for future research. Github repository:\nhttps://github.com/observerw/ChiseLLM",
    "categories": [
      "cs.AI",
      "cs.AR",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.19144v1",
    "published_date": "2025-04-27 07:56:49 UTC",
    "updated_date": "2025-04-27 07:56:49 UTC"
  },
  {
    "arxiv_id": "2504.19142v1",
    "title": "BQSched: A Non-intrusive Scheduler for Batch Concurrent Queries via Reinforcement Learning",
    "authors": [
      "Chenhao Xu",
      "Chunyu Chen",
      "Jinglin Peng",
      "Jiannan Wang",
      "Jun Gao"
    ],
    "abstract": "Most large enterprises build predefined data pipelines and execute them\nperiodically to process operational data using SQL queries for various tasks. A\nkey issue in minimizing the overall makespan of these pipelines is the\nefficient scheduling of concurrent queries within the pipelines. Existing tools\nmainly rely on simple heuristic rules due to the difficulty of expressing the\ncomplex features and mutual influences of queries. The latest reinforcement\nlearning (RL) based methods have the potential to capture these patterns from\nfeedback, but it is non-trivial to apply them directly due to the large\nscheduling space, high sampling cost, and poor sample utilization.\n  Motivated by these challenges, we propose BQSched, a non-intrusive Scheduler\nfor Batch concurrent Queries via reinforcement learning. Specifically, BQSched\ndesigns an attention-based state representation to capture the complex query\npatterns, and proposes IQ-PPO, an auxiliary task-enhanced proximal policy\noptimization (PPO) algorithm, to fully exploit the rich signals of Individual\nQuery completion in logs. Based on the RL framework above, BQSched further\nintroduces three optimization strategies, including adaptive masking to prune\nthe action space, scheduling gain-based query clustering to deal with large\nquery sets, and an incremental simulator to reduce sampling cost. To our\nknowledge, BQSched is the first non-intrusive batch query scheduler via RL.\nExtensive experiments show that BQSched can significantly improve the\nefficiency and stability of batch query scheduling, while also achieving\nremarkable scalability and adaptability in both data and queries. For example,\nacross all DBMSs and scales tested, BQSched reduces the overall makespan of\nbatch queries on TPC-DS benchmark by an average of 34% and 13%, compared with\nthe commonly used heuristic strategy and the adapted RL-based scheduler,\nrespectively.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "Accepted by ICDE '25",
    "pdf_url": "http://arxiv.org/pdf/2504.19142v1",
    "published_date": "2025-04-27 07:49:01 UTC",
    "updated_date": "2025-04-27 07:49:01 UTC"
  },
  {
    "arxiv_id": "2504.19139v3",
    "title": "Fast and Robust: Task Sampling with Posterior and Diversity Synergies for Adaptive Decision-Makers in Randomized Environments",
    "authors": [
      "Yun Qu",
      "Qi Cheems Wang",
      "Yixiu Mao",
      "Yiqin Lv",
      "Xiangyang Ji"
    ],
    "abstract": "Task robust adaptation is a long-standing pursuit in sequential\ndecision-making. Some risk-averse strategies, e.g., the conditional\nvalue-at-risk principle, are incorporated in domain randomization or meta\nreinforcement learning to prioritize difficult tasks in optimization, which\ndemand costly intensive evaluations. The efficiency issue prompts the\ndevelopment of robust active task sampling to train adaptive policies, where\nrisk-predictive models are used to surrogate policy evaluation. This work\ncharacterizes the optimization pipeline of robust active task sampling as a\nMarkov decision process, posits theoretical and practical insights, and\nconstitutes robustness concepts in risk-averse scenarios. Importantly, we\npropose an easy-to-implement method, referred to as Posterior and Diversity\nSynergized Task Sampling (PDTS), to accommodate fast and robust sequential\ndecision-making. Extensive experiments show that PDTS unlocks the potential of\nrobust active task sampling, significantly improves the zero-shot and few-shot\nadaptation robustness in challenging tasks, and even accelerates the learning\nprocess under certain scenarios. Our project website is at\nhttps://thu-rllab.github.io/PDTS_project_page.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.19139v3",
    "published_date": "2025-04-27 07:27:17 UTC",
    "updated_date": "2025-05-15 01:51:26 UTC"
  },
  {
    "arxiv_id": "2504.19136v1",
    "title": "PAD: Phase-Amplitude Decoupling Fusion for Multi-Modal Land Cover Classification",
    "authors": [
      "Huiling Zheng",
      "Xian Zhong",
      "Bin Liu",
      "Yi Xiao",
      "Bihan Wen",
      "Xiaofeng Li"
    ],
    "abstract": "The fusion of Synthetic Aperture Radar (SAR) and RGB imagery for land cover\nclassification remains challenging due to modality heterogeneity and the\nunderutilization of spectral complementarity. Existing methods often fail to\ndecouple shared structural features from modality-specific radiometric\nattributes, leading to feature conflicts and information loss. To address this\nissue, we propose Phase-Amplitude Decoupling (PAD), a frequency-aware framework\nthat separates phase (modality-shared) and amplitude (modality-specific)\ncomponents in the Fourier domain. Specifically, PAD consists of two key\ncomponents: 1) Phase Spectrum Correction (PSC), which aligns cross-modal phase\nfeatures through convolution-guided scaling to enhance geometric consistency,\nand 2) Amplitude Spectrum Fusion (ASF), which dynamically integrates\nhigh-frequency details and low-frequency structures using frequency-adaptive\nmultilayer perceptrons. This approach leverages SAR's sensitivity to\nmorphological features and RGB's spectral richness. Extensive experiments on\nWHU-OPT-SAR and DDHR-SK datasets demonstrate state-of-the-art performance. Our\nwork establishes a new paradigm for physics-aware multi-modal fusion in remote\nsensing. The code will be available at https://github.com/RanFeng2/PAD.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "13 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.19136v1",
    "published_date": "2025-04-27 07:21:42 UTC",
    "updated_date": "2025-04-27 07:21:42 UTC"
  },
  {
    "arxiv_id": "2504.19120v1",
    "title": "Beyond Levels of Driving Automation: A Triadic Framework of Human-AI Collaboration in On-Road Mobility",
    "authors": [
      "Gaojian Huang",
      "Yantong Jin",
      "Wei-Hsiang Lo"
    ],
    "abstract": "The goal of the current study is to introduce a triadic human-AI\ncollaboration framework for the automated vehicle domain. Previous\nclassifications (e.g., SAE Levels of Automation) focus on defining automation\nlevels based on who controls the vehicle. However, it remains unclear how human\nusers and AI should collaborate in real-time, especially in dynamic driving\ncontexts, where roles can shift frequently. To fill the gap, this study\nproposes a triadic human-AI collaboration framework with three AI roles (i.e.,\nAdvisor, Co-Pilot, and Guardian) that dynamically adapt to human needs.\nOverall, the study lays a foundation for developing adaptive, role-based\nhuman-AI collaboration strategies in automated vehicles.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.19120v1",
    "published_date": "2025-04-27 06:26:47 UTC",
    "updated_date": "2025-04-27 06:26:47 UTC"
  },
  {
    "arxiv_id": "2504.19099v1",
    "title": "VeriDebug: A Unified LLM for Verilog Debugging via Contrastive Embedding and Guided Correction",
    "authors": [
      "Ning Wang",
      "Bingkun Yao",
      "Jie Zhou",
      "Yuchen Hu",
      "Xi Wang",
      "Nan Guan",
      "Zhe Jiang"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable potential in\ndebugging for various programming languages. However, the application of LLMs\nto Verilog debugging remains insufficiently explored. Here, we present\nVeriDebug, an approach that integrates contrastive representation and guided\ncorrection capabilities for automated Verilog debugging. Unlike existing\nmethods, VeriDebug employs an embedding-based technique to accurately retrieve\ninternal information, followed by bug-fixing. VeriDebug unifies Verilog bug\ndetection and correction through a shared parameter space. By simultaneously\nlearning bug patterns and fixes, it streamlines debugging via contrastive\nembedding and guided correction. Empirical results show the efficacy of\nVeriDebug in enhancing Verilog debugging. Our VeriDebugLoc, Type model achieves\n64.7 accuracy in bug fixing (Acc1), a significant improvement from the existing\nopen-source SOTAs 11.3. This performance not only outperforms open-source\nalternatives but also exceeds larger closed-source models like GPT-3.5-turbo\n(36.6), offering a more accurate alternative to conventional debugging methods.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.19099v1",
    "published_date": "2025-04-27 04:09:48 UTC",
    "updated_date": "2025-04-27 04:09:48 UTC"
  },
  {
    "arxiv_id": "2504.19093v1",
    "title": "CipherBank: Exploring the Boundary of LLM Reasoning Capabilities through Cryptography Challenges",
    "authors": [
      "Yu Li",
      "Qizhi Pei",
      "Mengyuan Sun",
      "Honglin Lin",
      "Chenlin Ming",
      "Xin Gao",
      "Jiang Wu",
      "Conghui He",
      "Lijun Wu"
    ],
    "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities,\nespecially the recent advancements in reasoning, such as o1 and o3, pushing the\nboundaries of AI. Despite these impressive achievements in mathematics and\ncoding, the reasoning abilities of LLMs in domains requiring cryptographic\nexpertise remain underexplored. In this paper, we introduce CipherBank, a\ncomprehensive benchmark designed to evaluate the reasoning capabilities of LLMs\nin cryptographic decryption tasks. CipherBank comprises 2,358 meticulously\ncrafted problems, covering 262 unique plaintexts across 5 domains and 14\nsubdomains, with a focus on privacy-sensitive and real-world scenarios that\nnecessitate encryption. From a cryptographic perspective, CipherBank\nincorporates 3 major categories of encryption methods, spanning 9 distinct\nalgorithms, ranging from classical ciphers to custom cryptographic techniques.\nWe evaluate state-of-the-art LLMs on CipherBank, e.g., GPT-4o, DeepSeek-V3, and\ncutting-edge reasoning-focused models such as o1 and DeepSeek-R1. Our results\nreveal significant gaps in reasoning abilities not only between general-purpose\nchat LLMs and reasoning-focused LLMs but also in the performance of current\nreasoning-focused models when applied to classical cryptographic decryption\ntasks, highlighting the challenges these models face in understanding and\nmanipulating encrypted data. Through detailed analysis and error\ninvestigations, we provide several key observations that shed light on the\nlimitations and potential improvement areas for LLMs in cryptographic\nreasoning. These findings underscore the need for continuous advancements in\nLLM reasoning capabilities.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.PF"
    ],
    "primary_category": "cs.CR",
    "comment": "Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2504.19093v1",
    "published_date": "2025-04-27 03:41:17 UTC",
    "updated_date": "2025-04-27 03:41:17 UTC"
  },
  {
    "arxiv_id": "2504.19080v1",
    "title": "MIA-Mind: A Multidimensional Interactive Attention Mechanism Based on MindSpore",
    "authors": [
      "Zhenkai Qin",
      "Jiaquan Liang",
      "Qiao Fang"
    ],
    "abstract": "Attention mechanisms have significantly advanced deep learning by enhancing\nfeature representation through selective focus. However, existing approaches\noften independently model channel importance and spatial saliency, overlooking\ntheir inherent interdependence and limiting their effectiveness. To address\nthis limitation, we propose MIA-Mind, a lightweight and modular\nMultidimensional Interactive Attention Mechanism, built upon the MindSpore\nframework. MIA-Mind jointly models spatial and channel features through a\nunified cross-attentive fusion strategy, enabling fine-grained feature\nrecalibration with minimal computational overhead. Extensive experiments are\nconducted on three representative datasets: on CIFAR-10, MIA-Mind achieves an\naccuracy of 82.9\\%; on ISBI2012, it achieves an accuracy of 78.7\\%; and on\nCIC-IDS2017, it achieves an accuracy of 91.9\\%. These results validate the\nversatility, lightweight design, and generalization ability of MIA-Mind across\nheterogeneous tasks. Future work will explore the extension of MIA-Mind to\nlarge-scale datasets, the development of ada,ptive attention fusion strategies,\nand distributed deployment to further enhance scalability and robustness.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.19080v1",
    "published_date": "2025-04-27 02:27:50 UTC",
    "updated_date": "2025-04-27 02:27:50 UTC"
  },
  {
    "arxiv_id": "2504.19066v1",
    "title": "ClimaEmpact: Domain-Aligned Small Language Models and Datasets for Extreme Weather Analytics",
    "authors": [
      "Deeksha Varshney",
      "Keane Ong",
      "Rui Mao",
      "Erik Cambria",
      "Gianmarco Mengaldo"
    ],
    "abstract": "Accurate assessments of extreme weather events are vital for research and\npolicy, yet localized and granular data remain scarce in many parts of the\nworld. This data gap limits our ability to analyze potential outcomes and\nimplications of extreme weather events, hindering effective decision-making.\nLarge Language Models (LLMs) can process vast amounts of unstructured text\ndata, extract meaningful insights, and generate detailed assessments by\nsynthesizing information from multiple sources. Furthermore, LLMs can\nseamlessly transfer their general language understanding to smaller models,\nenabling these models to retain key knowledge while being fine-tuned for\nspecific tasks. In this paper, we propose Extreme Weather Reasoning-Aware\nAlignment (EWRA), a method that enhances small language models (SLMs) by\nincorporating structured reasoning paths derived from LLMs, and\nExtremeWeatherNews, a large dataset of extreme weather event-related news\narticles. EWRA and ExtremeWeatherNews together form the overall framework,\nClimaEmpact, that focuses on addressing three critical extreme-weather tasks:\ncategorization of tangible vulnerabilities/impacts, topic labeling, and emotion\nanalysis. By aligning SLMs with advanced reasoning strategies on\nExtremeWeatherNews (and its derived dataset ExtremeAlign used specifically for\nSLM alignment), EWRA improves the SLMs' ability to generate well-grounded and\ndomain-specific responses for extreme weather analytics. Our results show that\nthe approach proposed guides SLMs to output domain-aligned responses,\nsurpassing the performance of task-specific models and offering enhanced\nreal-world applicability for extreme weather analytics.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "physics.ao-ph"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.19066v1",
    "published_date": "2025-04-27 01:15:14 UTC",
    "updated_date": "2025-04-27 01:15:14 UTC"
  },
  {
    "arxiv_id": "2504.20101v2",
    "title": "GenTorrent: Scaling Large Language Model Serving with An Overley Network",
    "authors": [
      "Fei Fang",
      "Yifan Hua",
      "Shengze Wang",
      "Ruilin Zhou",
      "Yi Liu",
      "Chen Qian",
      "Xiaoxue Zhang"
    ],
    "abstract": "While significant progress has been made in research and development on\nopen-source and cost-efficient large-language models (LLMs), serving\nscalability remains a critical challenge, particularly for small organizations\nand individuals seeking to deploy and test their LLM innovations. Inspired by\npeer-to-peer networks that leverage decentralized overlay nodes to increase\nthroughput and availability, we propose GenTorrent, an LLM serving overlay that\nharnesses computing resources from decentralized contributors. We identify four\nkey research problems inherent to enabling such a decentralized infrastructure:\n1) overlay network organization; 2) LLM communication privacy; 3) overlay\nforwarding for resource efficiency; and 4) verification of serving quality.\nThis work presents the first systematic study of these fundamental problems in\nthe context of decentralized LLM serving. Evaluation results from a prototype\nimplemented on a set of decentralized nodes demonstrate that GenTorrent\nachieves a latency reduction of over 50% compared to the baseline design\nwithout overlay forwarding. Furthermore, the security features introduce\nminimal overhead to serving latency and throughput. We believe this work\npioneers a new direction for democratizing and scaling future AI serving\ncapabilities.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.20101v2",
    "published_date": "2025-04-27 01:08:25 UTC",
    "updated_date": "2025-04-30 21:24:19 UTC"
  },
  {
    "arxiv_id": "2504.19061v1",
    "title": "Hallucinations and Key Information Extraction in Medical Texts: A Comprehensive Assessment of Open-Source Large Language Models",
    "authors": [
      "Anindya Bijoy Das",
      "Shibbir Ahmed",
      "Shahnewaz Karim Sakib"
    ],
    "abstract": "Clinical summarization is crucial in healthcare as it distills complex\nmedical data into digestible information, enhancing patient understanding and\ncare management. Large language models (LLMs) have shown significant potential\nin automating and improving the accuracy of such summarizations due to their\nadvanced natural language understanding capabilities. These models are\nparticularly applicable in the context of summarizing medical/clinical texts,\nwhere precise and concise information transfer is essential. In this paper, we\ninvestigate the effectiveness of open-source LLMs in extracting key events from\ndischarge reports, such as reasons for hospital admission, significant\nin-hospital events, and critical follow-up actions. In addition, we also assess\nthe prevalence of various types of hallucinations in the summaries produced by\nthese models. Detecting hallucinations is vital as it directly influences the\nreliability of the information, potentially affecting patient care and\ntreatment outcomes. We conduct comprehensive numerical simulations to\nrigorously evaluate the performance of these models, further probing the\naccuracy and fidelity of the extracted content in clinical summarization.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.19061v1",
    "published_date": "2025-04-27 00:39:12 UTC",
    "updated_date": "2025-04-27 00:39:12 UTC"
  },
  {
    "arxiv_id": "2504.19056v1",
    "title": "Generative AI for Character Animation: A Comprehensive Survey of Techniques, Applications, and Future Directions",
    "authors": [
      "Mohammad Mahdi Abootorabi",
      "Omid Ghahroodi",
      "Pardis Sadat Zahraei",
      "Hossein Behzadasl",
      "Alireza Mirrokni",
      "Mobina Salimipanah",
      "Arash Rasouli",
      "Bahar Behzadipour",
      "Sara Azarnoush",
      "Benyamin Maleki",
      "Erfan Sadraiye",
      "Kiarash Kiani Feriz",
      "Mahdi Teymouri Nahad",
      "Ali Moghadasi",
      "Abolfazl Eshagh Abianeh",
      "Nizi Nazar",
      "Hamid R. Rabiee",
      "Mahdieh Soleymani Baghshah",
      "Meisam Ahmadi",
      "Ehsaneddin Asgari"
    ],
    "abstract": "Generative AI is reshaping art, gaming, and most notably animation. Recent\nbreakthroughs in foundation and diffusion models have reduced the time and cost\nof producing animated content. Characters are central animation components,\ninvolving motion, emotions, gestures, and facial expressions. The pace and\nbreadth of advances in recent months make it difficult to maintain a coherent\nview of the field, motivating the need for an integrative review. Unlike\nearlier overviews that treat avatars, gestures, or facial animation in\nisolation, this survey offers a single, comprehensive perspective on all the\nmain generative AI applications for character animation. We begin by examining\nthe state-of-the-art in facial animation, expression rendering, image\nsynthesis, avatar creation, gesture modeling, motion synthesis, object\ngeneration, and texture synthesis. We highlight leading research, practical\ndeployments, commonly used datasets, and emerging trends for each area. To\nsupport newcomers, we also provide a comprehensive background section that\nintroduces foundational models and evaluation metrics, equipping readers with\nthe knowledge needed to enter the field. We discuss open challenges and map\nfuture research directions, providing a roadmap to advance AI-driven\ncharacter-animation technologies. This survey is intended as a resource for\nresearchers and developers entering the field of generative AI animation or\nadjacent fields. Resources are available at:\nhttps://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "50 main pages, 30 pages appendix, 21 figures, 8 tables, GitHub\n  Repository:\n  https://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey",
    "pdf_url": "http://arxiv.org/pdf/2504.19056v1",
    "published_date": "2025-04-27 00:09:31 UTC",
    "updated_date": "2025-04-27 00:09:31 UTC"
  }
]
{
  "date": "2025-03-24",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-03-24 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 上的论文再次展现了 AI 领域的蓬勃发展，**大语言模型 (LLM)** 依然是研究的绝对核心，涵盖了从**基础能力提升**（如零样本强化学习、利用潜在思维提升数据效率）、**效率优化**（如低比特 KV 缓存加速、视觉 Token 剪枝）、**Agent 系统**（如多智能体协作、安全保障、经济学评估）到**特定应用**（如科学文献分析、编码、医学、天文学、网络安全）等多个维度。**世界模型和视频生成**领域也涌现出令人瞩目的工作，如 Aether 和 AdaWorld 探索更统一、适应性更强的模型框架。此外，**计算机视觉**、**强化学习**、**AI 伦理与可解释性**以及**跨学科应用**也贡献了许多值得关注的成果。\n\n**今日焦点:**\n\n*   **Aether (论文 1)**：提出了一个统一几何感知和生成建模的世界模型框架，在 4D 重建、视频预测和规划方面表现出色，尤其是在合成到真实和零样本泛化方面。\n*   **LLM Agent 安全与效率**：多篇论文关注 LLM Agent 的实际部署挑战。AgentDropout (6) 动态剔除冗余 Agent 提升效率；Defeating Prompt Injections by Design (20) 提出 CaMeL 架构防御提示注入；AgentSpec (33) 和 Safeguarding Mobile GUI Agent (59) 分别提出运行时约束和逻辑验证来保障 Agent 安全。\n*   **LLM 效率优化**：BitDecoding (25) 通过优化解锁 Tensor Core，加速低比特 KV 缓存的 LLM 解码；TopV (84) 提出免训练的视觉 Token 剪枝方法，兼容 FlashAttention。\n*   **LLM 新应用与评估**：Reasoning to Learn from Latent Thoughts (8) 探索利用推断出的“潜在思维”提升预训练数据效率；EconEvals (16) 为 LLM Agent 在未知经济环境中设计了基准测试；Classical Planning with LLM-Generated Heuristics (21) 展示了 LLM 生成启发式函数辅助经典规划的能力。\n\n**接下来，我们深入了解一些精选论文：**\n\n---\n\n**模型框架与生成**\n\n1.  **Aether: 几何感知统一世界模型 (Aether: Geometric-Aware Unified World Modeling)**\n    *   TLDR: 提出了 Aether 框架，旨在统一几何重建和生成模型，通过联合优化 4D 动态重建、动作条件视频预测和目标条件视觉规划，实现几何感知的世界模型。该方法利用任务交错特征学习实现知识共享，展示了出色的合成到真实泛化能力，并在动作跟随和重建任务中实现零样本泛化，其重建性能甚至优于领域特定模型。\n\n2.  **Video-T1: 视频生成的测试时扩展 (Video-T1: Test-Time Scaling for Video Generation)**\n    *   TLDR: 探索了在视频生成中应用“测试时扩展”（TTS）的可能性，即在推理阶段投入更多计算资源以提升生成质量。将 TTS 视为在噪声空间中搜索更优轨迹的问题，并提出了 Tree-of-Frames (ToF) 方法，通过自回归方式扩展和修剪视频分支，有效提升了视频生成质量。\n\n3.  **AdaWorld: 学习具有潜在动作的自适应世界模型 (AdaWorld: Learning Adaptable World Models with Latent Actions)**\n    *   TLDR: 提出了 AdaWorld，一种创新的世界模型学习方法，通过自监督方式从视频中提取潜在动作并在预训练中整合动作信息，使得模型能够高效适应具有不同动作的新环境，在模拟质量和视觉规划方面表现优越。\n\n---\n\n**大语言模型 (LLM) 与 Agent**\n\n5.  **SimpleRL-Zoo: 探索和驯服开放基础模型中的零强化学习 (SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild)**\n    *   TLDR: 研究了在 10 种不同的基础大语言模型上进行零强化学习（直接从基础模型开始 RL 训练）的可行性。通过调整奖励和查询难度等策略，在多数模型上提升了推理准确性和响应长度，但也观察到不同模型训练动态各异，“aha moment”（验证行为）并非普遍出现，并在非 Qwen 小模型中首次观察到。分享了成功经验和开源工具。\n\n6.  **AgentDropout: 用于 Token 高效和高性能 LLM 多智能体协作的动态智能体消除 (AgentDropout: Dynamic Agent Elimination for Token-Efficient and High-Performance LLM-Based Multi-Agent Collaboration)**\n    *   TLDR: 提出 AgentDropout 方法，通过优化通信图的邻接矩阵来识别并消除多智能体系统中的冗余智能体和通信，从而在降低 Token 消耗的同时提升任务性能，并展现出良好的领域迁移性和结构鲁棒性。\n\n8.  **推理以从潜在思维中学习 (Reasoning to Learn from Latent Thoughts)**\n    *   TLDR: 提出通过显式建模和推断文本生成过程背后的“潜在思维”，可以显著提高 LLM 预训练的数据效率。实验表明，使用合成数据推断潜在思维能大幅提升数学任务性能，并通过 EM 算法实现 LM 自我引导，证明了推理计算和 EM 迭代在数据受限预训练中的潜力。\n\n16. **EconEvals: 未知环境中 LLM Agent 的基准和试金石测试 (EconEvals: Benchmarks and Litmus Tests for LLM Agents in Unknown Environments)**\n    *   TLDR: 开发了一系列源自经济学问题的基准测试，用于评估 LLM Agent 在未知环境中的行动、学习和策略制定能力。同时提出了“试金石测试”，用于量化 Agent 在面临权衡（如效率 vs 公平）时的特征、价值观和倾向。\n\n20. **通过设计击败提示注入 (Defeating Prompt Injections by Design)**\n    *   TLDR: 提出了 CaMeL 防御机制，通过在 LLM 周围创建保护性系统层，显式提取可信查询的控制流和数据流，使不可信数据无法影响程序流，并利用能力概念防止数据泄露，有效防御提示注入攻击。\n\n21. **使用 LLM 生成的启发式进行经典规划：用 Python 代码挑战 SOTA (Classical Planning with LLM-Generated Heuristics: Challenging the State of the Art with Python Code)**\n    *   TLDR: 展示了如何利用 LLM 生成 Python 代码形式的领域相关启发式函数，并将其用于贪婪最佳优先搜索。实验表明，LLM 生成的启发式在解决未见过的测试任务上优于 SOTA 的领域无关启发式，甚至与最强的领域相关规划学习算法具有竞争力。\n\n25. **BitDecoding: 利用低比特 KV 缓存为长上下文 LLM 解码解锁 Tensor Core (BitDecoding: Unlocking Tensor Cores for Long-Context LLMs Decoding with Low-Bit KV Cache)**\n    *   TLDR: 提出了 BitDecoding 框架，通过 Tensor Core 中心化的 BitFusion 方案、warp 高效的并行解码核和细粒度异步流水线，有效利用 GPU Tensor Core 加速带有低比特（如 4 位、2 位）KV 缓存的 LLM 解码过程，显著提升了长上下文场景下的解码速度。\n\n31. **Commander-GPT: 完全释放多模态大语言模型的讽刺检测能力 (Commander-GPT: Fully Unleashing the Sarcasm Detection Capability of Multi-Modal Large Language Models)**\n    *   TLDR: 提出 Commander-GPT 框架用于多模态讽刺检测。将任务分解为六个子任务，由一个“指挥官”LLM 分配给最合适的 LLM 处理，最后聚合结果。实验表明该方法无需微调或理由标注即可达到 SOTA 性能。\n\n33. **AgentSpec: 用于安全可靠 LLM Agent 的可定制运行时强制执行 (AgentSpec: Customizable Runtime Enforcement for Safe and Reliable LLM Agents)**\n    *   TLDR: 提出 AgentSpec，一种轻量级领域特定语言，允许用户定义结构化规则（触发器、谓词、强制机制）来约束 LLM Agent 的行为，确保其在预定义的安全边界内运行。在代码执行、具身智能体和自动驾驶等领域验证了其有效性和低开销。\n\n42. **Galaxy Walker: 用于星系尺度理解的几何感知 VLM (Galaxy Walker: Geometry-aware VLMs For Galaxy-scale Understanding)**\n    *   TLDR: 提出 Galaxy-Walker，一个几何感知的视觉语言模型，用于理解宇宙尺度的视觉任务。通过几何提示（在多尺度物理图上跨不同空间随机游走生成几何 Token）和几何适配器（处理空间各向异性），显著提升了星系属性估计和形态分类的性能。\n\n54. **MMCR: 推进视觉语言模型在多模态多轮上下文推理中的应用 (MMCR: Advancing Visual Language Model in Multimodal Multi-Turn Contextual Reasoning)**\n    *   TLDR: 提出了 MMCR 数据集，包含最大的多图像多轮指令微调数据集 MMCR-310k 和一个诊断性基准 MMCR-Bench，旨在提升 VLM 在涉及多图像和多轮对话的上下文推理能力。用 MMCR-310k 微调的模型在 MMCR-Bench 和现有基准上均表现出性能提升。\n\n58. **口头过程监督引出更好的编码 Agent (Verbal Process Supervision Elicits Better Coding Agents)**\n    *   TLDR: 提出 CURA，一个结合了口头过程监督 (VPS) 的代码理解与推理 Agent 系统。通过让模型“说出”其推理过程进行监督，显著提升了 LLM 在复杂代码生成基准上的性能。\n\n59. **通过基于逻辑的动作验证保障移动 GUI Agent 安全 (Safeguarding Mobile GUI Agent via Logic-based Action Verification)**\n    *   TLDR: 提出 VeriSafe Agent (VSA)，一个形式化验证系统，通过将自然语言指令自动形式化为可验证规范，在运行时检查 Agent 的动作是否符合用户意图，从而防止移动 GUI Agent 执行错误或不安全的动作。\n\n62. **MetaSpatial: 为元宇宙强化 VLM 中的 3D 空间推理 (MetaSpatial: Reinforcing 3D Spatial Reasoning in VLMs for the Metaverse)**\n    *   TLDR: 提出 MetaSpatial，首个基于强化学习的框架，用于增强 VLM 的 3D 空间推理能力，以实现实时 3D 场景生成。通过多轮 RL 优化机制，结合物理感知约束和渲染图像评估，提升生成布局的连贯性、物理合理性和美学一致性。\n\n77. **通过创建 LLM 对齐的指令弥合视觉指令调优中的写作方式差距 (Bridging Writing Manner Gap in Visual Instruction Tuning by Creating LLM-aligned Instructions)**\n    *   TLDR: 指出视觉指令调整中，指令的“写作方式”与基础 LLM 存在差距，影响模型性能。提出利用基础 LLM 自身来调整视觉指令的写作方式，生成“LLM 对齐”的指令，从而减少能力退化，提升了 LMM 在多个基准上的表现。\n\n84. **TopV: 兼容 Token 剪枝与推理时优化，实现快速低内存多模态视觉语言模型 (TopV: Compatible Token Pruning with Inference Time Optimization for Fast and Low-Memory Multimodal Vision Language Model)**\n    *   TLDR: 提出 TopV，一种无需额外训练即可对 VLM 进行视觉 Token 剪枝的方法。通过将剪枝表述为优化问题，并考虑特征相似性、相对空间距离等因素，准确识别重要 Token，且兼容 FlashAttention 和 KV 缓存优化，有效降低推理计算量和内存占用。\n\n---\n\n**计算机视觉**\n\n14. **用于加速 MRI 重建的双域多路径自监督扩散模型 (Dual-domain Multi-path Self-supervised Diffusion Model for Accelerated MRI Reconstruction)**\n    *   TLDR: 提出 DMSM 框架，结合自监督双域扩散模型训练、轻量级混合注意力网络和多路径推理策略，用于加速 MRI 重建。该方法不依赖全采样数据进行训练，提高了重建精度、效率和可解释性，并能生成与误差相关的置信度图。\n\n17. **通过多模态表示的跨模态对齐增强 OoD 检测 (Enhanced OoD Detection through Cross-Modal Alignment of Multi-Modal Representations)**\n    *   TLDR: 指出多模态微调 (MMFT) 对分布外检测 (OoDD) 的潜力，并发现朴素微调的模态差距问题。提出一种训练目标，通过正则化分布内 (ID) 数据的图像和文本嵌入距离来增强跨模态对齐，显著提升了基于 CLIP 等模型的 OoDD 性能。\n\n23. **用于密集图像预测的频率动态卷积 (Frequency Dynamic Convolution for Dense Image Prediction)**\n    *   TLDR: 提出频率动态卷积 (FDConv)，通过在傅里叶域学习固定参数预算并划分为频率组，构建频率多样性的权重，减少参数量并提升适应性。结合核空间调制 (KSM) 和频带调制 (FBM) 进一步增强动态调整能力，在目标检测、分割等任务上优于传统动态卷积。\n\n32. **Any6D: 新物体的无模型 6D 姿态估计 (Any6D: Model-free 6D Pose Estimation of Novel Objects)**\n    *   TLDR: 提出 Any6D，一个无模型的 6D 物体姿态估计框架，仅需单个 RGB-D 锚点图像即可估计未知物体的 6D 姿态和尺寸。利用联合对象对齐过程增强 2D-3D 对齐和尺度估计，并通过渲染比较策略生成和优化姿态假设，在遮挡、光照变化等挑战性场景下表现鲁棒。\n\n36. **Dig2DIG: 挖掘图像融合的扩散信息增益 (Dig2DIG: Dig into Diffusion Information Gains for Image Fusion)**\n    *   TLDR: 揭示了扩散模型去噪过程中存在的时空不平衡性，即不同区域和步骤信息增益不同。基于此，提出 Dig2DIG 框架，理论推导并引入扩散信息增益 (DIG) 来量化各模态贡献，动态指导图像融合过程，提升了融合质量和效率。\n\n51. **UniPCGC: 通过高效统一方法实现实用的点云几何压缩 (UniPCGC: Towards Practical Point Cloud Geometry Compression via an Efficient Unified Approach)**\n    *   TLDR: 提出 UniPCGC，一个高效统一的点云几何压缩框架，支持有损、无损压缩以及可变码率和复杂度。通过不均匀 8 阶段无损编码器 (UELC) 和可变码率与复杂度模块 (VRCM) 的结合，实现了性能优于 SOTA 的压缩效果。\n\n70. **通过动态掩码引导实现资源高效的视频生成运动控制 (Resource-Efficient Motion Control for Video Generation via Dynamic Mask Guidance)**\n    *   TLDR: 提出一种掩码引导的视频生成方法，利用掩码运动序列来控制视频生成，实现精确的文本-位置匹配和运动轨迹控制。结合首帧共享和自回归扩展策略，生成更稳定、更长的视频，且训练数据需求有限。\n\n72. **PP-FormulaNet: 在高级公式识别中平衡准确性与效率 (PP-FormulaNet: Bridging Accuracy and Efficiency in Advanced Formula Recognition)**\n    *   TLDR: 提出 PP-FormulaNet，包含高精度 (L) 和高效率 (S) 两个模型，用于将文档图像中的数学公式转换为 LaTeX 格式。PP-FormulaNet-L 准确率超 SOTA 模型 6%，PP-FormulaNet-S 速度快 16 倍以上。同时发布了公式挖掘系统。\n\n76. **通过双空间多方面概念控制实现即插即用、可解释、负责任的文本到图像生成 (Plug-and-Play Interpretable Responsible Text-to-Image Generation via Dual-Space Multi-facet Concept Control)**\n    *   TLDR: 提出一种即插即用的负责任 T2I 生成技术，通过知识蒸馏和概念白化学习一个可解释的复合责任空间（涵盖公平、安全等多个方面），并在推理时调节文本嵌入空间和扩散模型潜空间来控制生成内容，无需修改原模型。\n\n---\n\n**强化学习与机器人**\n\n7.  **自举模型预测控制 (Bootstrapped Model Predictive Control)**\n    *   TLDR: 提出 BMPC 算法，通过让网络策略模仿 MPC 专家，反过来又用该策略指导 MPC 过程，实现自举式策略学习。结合基于模型的 TD 学习，改进了值函数估计，提升了 MPC 效率。在连续控制任务上取得优越性能。\n\n30. **通过在线元学习适配器实现预训练机器人策略的高效持续适应 (Efficient Continual Adaptation of Pretrained Robotic Policy with Online Meta-Learned Adapters)**\n    *   TLDR: 提出在线元学习适配器 (OMLA)，用于机器人预训练策略的持续适应。通过元学习目标促进先前任务知识向当前任务的迁移，相比基线方法在模拟和真实环境中均取得更好的适应性能。\n\n49. **RLCAD: 用于旋转相关 CAD 命令序列生成的强化学习训练 Gym (RLCAD: Reinforcement Learning Training Gym for Revolution Involved CAD Command Sequence Generation)**\n    *   TLDR: 构建了一个基于 CAD 几何引擎的强化学习训练环境 (Gym)，用于从 B-Rep 几何生成 CAD 命令序列。该 Gym 支持草图、布尔、拉伸以及旋转操作，训练出的 RL Agent 在生成质量上达到 SOTA，并显著提高了生成效率。\n\n74. **扩散规划器中基于人类偏好对齐的潜在嵌入自适应 (Latent Embedding Adaptation for Human Preference Alignment in Diffusion Planners)**\n    *   TLDR: 提出一种资源高效的方法，通过优化预训练条件扩散模型中的偏好潜在嵌入 (PLE)，使自动决策系统生成的轨迹快速适应个体用户偏好。相比 RLHF 和 LoRA，该方法在对齐真实人类偏好方面表现更优。\n\n---\n\n**AI 伦理、可解释性与鲁棒性**\n\n13. **AI 伦理的三种类型 (Three Kinds of AI Ethics)**\n    *   TLDR: 针对 AI 伦理领域的混乱现状，提出将其分为“伦理与 AI”、“AI 中的伦理”和“AI 的伦理”三种类型，以厘清研究问题、方法和所需专业知识。\n\n15. **可解释且公平的拒绝分类器机制 (Interpretable and Fair Mechanisms for Abstaining Classifiers)**\n    *   TLDR: 提出 IFAC 算法，一种可解释且公平的拒绝分类器。它不仅基于不确定性，还基于不公平性来拒绝预测，通过基于规则的公平性检查和情境测试实现可解释性，减少接受数据中的群体性能差异。\n\n19. **迈向负责任的 AI 音乐：创意系统可信赖特征研究 (Towards Responsible AI Music: an Investigation of Trustworthy Features for Creative Systems)**\n    *   TLDR: 基于欧盟 AI 高级别专家组的《可信赖人工智能伦理指南》，探讨了生成式 AI 音乐系统在七个宏观要求下的可信赖性问题，并提出了将这些要求具体化到音乐领域的路线图。\n\n26. **失真图像上微调 ViT 的机制可解释性：解码注意力头行为以实现透明可信 AI (Mechanistic Interpretability of Fine-Tuned Vision Transformers on Distorted Images: Decoding Attention Head Behavior for Transparent and Trustworthy AI)**\n    *   TLDR: 通过在包含无关内容（坐标轴标签等）的失真频谱图上微调 ViT，利用机制可解释性分析注意力头的行为。发现不同层级的注意力头功能特化，区分了单语义（关注关键区域）和多语义（关注多个无关区域）的头，增进了模型理解。\n\n57. **使用面向规则的回归对训练数据中不期望的错误模式进行统计测试 (Statistically Testing Training Data for Unwanted Error Patterns using Rule-Oriented Regression)**\n    *   TLDR: 提出一种方法，用于在训练 AI 模型前，统计测试训练数据是否存在可能导致不期望行为的“中毒”或错误模式。该方法将模糊推理嵌入回归模型，允许预先定义规则来检测数据中的特定模式，适用于小数据集。\n\n86. **通过数据剪枝切断虚假关联 (Severing Spurious Correlations with Data Pruning)**\n    *   TLDR: 发现即使虚假信号相对较弱，模型仍会学习并依赖虚假关联。提出一种数据剪枝技术，通过识别并移除训练数据中导致学习虚假关联的一小部分样本，有效提升模型鲁棒性，且无需领域知识或人工干预。\n\n---\n\n**其他应用与交叉领域**\n\n9.  **结构化科学创新：建模和发现有影响力的知识组合框架 (Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations)**\n    *   TLDR: 提出一个结构化方法来探索科学知识，强调方法组合在突破性见解中的作用。利用对比学习识别颠覆性方法组合的特征，并使用 LLM 的思维链能力引导蒙特卡洛搜索，以发现针对新问题的有前景的知识重组。\n\n22. **REALM: 真实世界 LLM 用例数据集 (REALM: A Dataset of Real-World LLM Use Cases)**\n    *   TLDR: 发布 REALM 数据集，包含从 Reddit 和新闻文章收集的超过 94,000 个 LLM 用例，捕捉 LLM 的多样化应用及其用户的职业分布，为理解 LLM 在不同领域的采用情况提供基础。\n\n40. **ClinText-SP 和 RigoBERTa Clinical: 西班牙语临床 NLP 的新开放资源集 (ClinText-SP and RigoBERTa Clinical: a new set of open resources for Spanish Clinical NLP)**\n    *   TLDR: 发布了目前最大的公开西班牙语临床语料库 ClinText-SP，以及基于该语料库预训练的 SOTA 临床编码器语言模型 RigoBERTa Clinical，旨在推动西班牙语临床 NLP 研究。\n\n55. **SciClaims: 用于生物医学断言分析的端到端生成系统 (SciClaims: An End-to-End Generative System for Biomedical Claim Analysis)**\n    *   TLDR: 提出 SciClaims，一个基于 LLM 的端到端系统，用于自动化分析科学文献（尤其是生物医学领域）中的关键断言。该系统集成了断言提取、证据检索和验证步骤，并提供清晰的验证结果解释，性能优于先前方法。\n\n68. **PRECTR: 集成个性化搜索相关性匹配和 CTR 预测的协同框架 (PRECTR: A Synergistic Framework for Integrating Personalized Search Relevance Matching and CTR Prediction)**\n    *   TLDR: 提出 PRECTR 模型，将搜索推荐中的相关性匹配和 CTR 预测统一到单一框架中，通过条件概率融合、两阶段训练和语义一致性正则化来提升模型性能和一致性，并考虑用户个性化的相关性偏好。\n\n71. **RoCA: 带有污染数据的鲁棒对比单类时间序列异常检测 (RoCA: Robust Contrastive One-class Time Series Anomaly Detection with Contaminated Data)**\n    *   TLDR: 提出 RoCA，一种鲁棒的时间序列异常检测方法。它融合了单类分类和对比学习的假设，并在训练过程中监控数据、计算异常分数以识别潜在异常，利用这些异常来定义分类边界，有效处理训练数据被污染的情况。\n\n---\n\n今天的快报就到这里，希望能帮助你快速把握 arXiv 的最新动态！",
  "papers": [
    {
      "arxiv_id": "2503.18945v2",
      "title": "Aether: Geometric-Aware Unified World Modeling",
      "title_zh": "Aether：几何感知的统一世界建模",
      "authors": [
        "Aether Team",
        "Haoyi Zhu",
        "Yifan Wang",
        "Jianjun Zhou",
        "Wenzheng Chang",
        "Yang Zhou",
        "Zizun Li",
        "Junyi Chen",
        "Chunhua Shen",
        "Jiangmiao Pang",
        "Tong He"
      ],
      "abstract": "The integration of geometric reconstruction and generative modeling remains a\ncritical challenge in developing AI systems capable of human-like spatial\nreasoning. This paper proposes Aether, a unified framework that enables\ngeometry-aware reasoning in world models by jointly optimizing three core\ncapabilities: (1) 4D dynamic reconstruction, (2) action-conditioned video\nprediction, and (3) goal-conditioned visual planning. Through task-interleaved\nfeature learning, Aether achieves synergistic knowledge sharing across\nreconstruction, prediction, and planning objectives. Building upon video\ngeneration models, our framework demonstrates unprecedented synthetic-to-real\ngeneralization despite never observing real-world data during training.\nFurthermore, our approach achieves zero-shot generalization in both action\nfollowing and reconstruction tasks, thanks to its intrinsic geometric modeling.\nRemarkably, even without real-world data, its reconstruction performance is\ncomparable with or even better than that of domain-specific models.\nAdditionally, Aether employs camera trajectories as geometry-informed action\nspaces, enabling effective action-conditioned prediction and visual planning.\nWe hope our work inspires the community to explore new frontiers in\nphysically-reasonable world modeling and its applications.",
      "tldr_zh": "该研究提出了Aether，一种几何感知的统一世界建模框架，通过联合优化4D动态重建、动作条件视频预测和目标条件视觉规划三项核心能力，实现几何感知的推理。该框架通过任务交错特征学习，在重建、预测和规划目标之间实现协同知识共享，展现了前所未有的合成到真实数据的泛化能力，甚至在未见过真实数据的情况下，其重建性能优于领域专用模型。此外，Aether利用相机轨迹作为几何信息的动作空间，实现了高效的动作条件预测和视觉规划。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Project Page: https://aether-world.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2503.18945v2",
      "published_date": "2025-03-24 17:59:51 UTC",
      "updated_date": "2025-03-25 15:31:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T21:55:00.994434"
    },
    {
      "arxiv_id": "2503.18942v1",
      "title": "Video-T1: Test-Time Scaling for Video Generation",
      "title_zh": "Video-T1：视频生成的测试时扩展",
      "authors": [
        "Fangfu Liu",
        "Hanyang Wang",
        "Yimo Cai",
        "Kaiyan Zhang",
        "Xiaohang Zhan",
        "Yueqi Duan"
      ],
      "abstract": "With the scale capability of increasing training data, model size, and\ncomputational cost, video generation has achieved impressive results in digital\ncreation, enabling users to express creativity across various domains.\nRecently, researchers in Large Language Models (LLMs) have expanded the scaling\nto test-time, which can significantly improve LLM performance by using more\ninference-time computation. Instead of scaling up video foundation models\nthrough expensive training costs, we explore the power of Test-Time Scaling\n(TTS) in video generation, aiming to answer the question: if a video generation\nmodel is allowed to use non-trivial amount of inference-time compute, how much\ncan it improve generation quality given a challenging text prompt. In this\nwork, we reinterpret the test-time scaling of video generation as a searching\nproblem to sample better trajectories from Gaussian noise space to the target\nvideo distribution. Specifically, we build the search space with test-time\nverifiers to provide feedback and heuristic algorithms to guide searching\nprocess. Given a text prompt, we first explore an intuitive linear search\nstrategy by increasing noise candidates at inference time. As full-step\ndenoising all frames simultaneously requires heavy test-time computation costs,\nwe further design a more efficient TTS method for video generation called\nTree-of-Frames (ToF) that adaptively expands and prunes video branches in an\nautoregressive manner. Extensive experiments on text-conditioned video\ngeneration benchmarks demonstrate that increasing test-time compute\nconsistently leads to significant improvements in the quality of videos.\nProject page: https://liuff19.github.io/Video-T1",
      "tldr_zh": "本研究提出了Video-T1，一种通过测试时扩展(Test-Time Scaling, TTS)提升视频生成质量的新方法。与传统的通过增加训练数据、模型规模和计算成本来扩展视频生成模型不同，Video-T1专注于在推理阶段利用更多计算资源来优化生成效果。该方法将测试时扩展问题重新定义为从高斯噪声空间到目标视频分布的搜索问题，并设计了基于测试时验证器的搜索空间和启发式算法来引导搜索过程。特别地，作者提出了一种高效的TTS方法——帧树(Tree-of-Frames, ToF)，以自回归方式自适应地扩展和修剪视频分支。实验表明，增加测试时计算资源能显著提升文本条件视频生成的质量。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://liuff19.github.io/Video-T1",
      "pdf_url": "http://arxiv.org/pdf/2503.18942v1",
      "published_date": "2025-03-24 17:59:04 UTC",
      "updated_date": "2025-03-24 17:59:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T21:55:03.199488"
    },
    {
      "arxiv_id": "2503.18938v1",
      "title": "AdaWorld: Learning Adaptable World Models with Latent Actions",
      "title_zh": "AdaWorld：利用潜在动作学习适应性世界模型",
      "authors": [
        "Shenyuan Gao",
        "Siyuan Zhou",
        "Yilun Du",
        "Jun Zhang",
        "Chuang Gan"
      ],
      "abstract": "World models aim to learn action-controlled prediction models and have proven\nessential for the development of intelligent agents. However, most existing\nworld models rely heavily on substantial action-labeled data and costly\ntraining, making it challenging to adapt to novel environments with\nheterogeneous actions through limited interactions. This limitation can hinder\ntheir applicability across broader domains. To overcome this challenge, we\npropose AdaWorld, an innovative world model learning approach that enables\nefficient adaptation. The key idea is to incorporate action information during\nthe pretraining of world models. This is achieved by extracting latent actions\nfrom videos in a self-supervised manner, capturing the most critical\ntransitions between frames. We then develop an autoregressive world model that\nconditions on these latent actions. This learning paradigm enables highly\nadaptable world models, facilitating efficient transfer and learning of new\nactions even with limited interactions and finetuning. Our comprehensive\nexperiments across multiple environments demonstrate that AdaWorld achieves\nsuperior performance in both simulation quality and visual planning.",
      "tldr_zh": "该研究提出了AdaWorld，一种基于潜在动作(latent actions)的自适应世界模型(World Models)学习方法。其核心创新在于通过自监督方式从视频中提取潜在动作，捕捉帧间关键变化，并构建基于这些潜在动作的自回归世界模型。该方法在预训练阶段引入动作信息，使得模型能够在新环境中通过有限交互快速适应和学习新动作。实验表明，AdaWorld在多个环境中均表现出色，显著提升了仿真质量和视觉规划性能，为智能体在异构动作环境中的高效适应提供了新思路。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "Project page: https://adaptable-world-model.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2503.18938v1",
      "published_date": "2025-03-24 17:58:15 UTC",
      "updated_date": "2025-03-24 17:58:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T21:55:15.075956"
    },
    {
      "arxiv_id": "2503.18899v1",
      "title": "Statistical Proof of Execution (SPEX)",
      "title_zh": "统计执行证明（SPEX）",
      "authors": [
        "Michele Dallachiesa",
        "Antonio Pitasi",
        "David Pinger",
        "Josh Goodbody",
        "Luis Vaello"
      ],
      "abstract": "Many real-world applications are increasingly incorporating automated\ndecision-making, driven by the widespread adoption of ML/AI inference for\nplanning and guidance. This study examines the growing need for verifiable\ncomputing in autonomous decision-making. We formalize the problem of verifiable\ncomputing and introduce a sampling-based protocol that is significantly faster,\nmore cost-effective, and simpler than existing methods. Furthermore, we tackle\nthe challenges posed by non-determinism, proposing a set of strategies to\neffectively manage common scenarios.",
      "tldr_zh": "该研究提出了一种基于采样的统计执行证明协议(SPEX)，用于解决自动化决策中的可验证计算问题。与现有方法相比，SPEX协议在速度、成本效益和简洁性方面具有显著优势。此外，研究还针对非确定性问题提出了一系列管理策略，为实际应用中的可验证计算提供了有效解决方案。",
      "categories": [
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18899v1",
      "published_date": "2025-03-24 17:13:25 UTC",
      "updated_date": "2025-03-24 17:13:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T21:55:13.600313"
    },
    {
      "arxiv_id": "2503.18892v1",
      "title": "SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild",
      "title_zh": "SimpleRL-Zoo：探索与驯化开放基础模型的零强化学习",
      "authors": [
        "Weihao Zeng",
        "Yuzhen Huang",
        "Qian Liu",
        "Wei Liu",
        "Keqing He",
        "Zejun Ma",
        "Junxian He"
      ],
      "abstract": "DeepSeek-R1 has shown that long chain-of-thought (CoT) reasoning can\nnaturally emerge through a simple reinforcement learning (RL) framework with\nrule-based rewards, where the training may directly start from the base\nmodels-a paradigm referred to as zero RL training. Most recent efforts to\nreproduce zero RL training have primarily focused on the Qwen2.5 model series,\nwhich may not be representative as we find the base models already exhibit\nstrong instruction-following and self-reflection abilities. In this work, we\ninvestigate zero RL training across 10 diverse base models, spanning different\nfamilies and sizes including LLama3-8B, Mistral-7B/24B, DeepSeek-Math-7B,\nQwen2.5-math-7B, and all Qwen2.5 models from 0.5B to 32B. Leveraging several\nkey design strategies-such as adjusting format reward and controlling query\ndifficulty-we achieve substantial improvements in both reasoning accuracy and\nresponse length across most settings. However, by carefully monitoring the\ntraining dynamics, we observe that different base models exhibit distinct\npatterns during training. For instance, the increased response length does not\nalways correlate with the emergence of certain cognitive behaviors such as\nverification (i.e., the \"aha moment\"). Notably, we observe the \"aha moment\" for\nthe first time in small models not from the Qwen family. We share the key\ndesigns that enable successful zero RL training, along with our findings and\npractices. To facilitate further research, we open-source the code, models, and\nanalysis tools.",
      "tldr_zh": "该研究提出了SimpleRL-Zoo框架，旨在探索和优化零强化学习（zero RL）在多样化基础模型上的应用。通过调整格式奖励和控制查询难度等关键设计策略，研究在包括LLama3、Mistral和Qwen2.5系列在内的10种不同规模和类型的模型上显著提升了推理准确性和响应长度。研究发现，不同模型在训练中表现出独特的行为模式，例如小型模型首次展现出“顿悟时刻”（aha moment）。研究开源了代码、模型和分析工具，为零RL的进一步研究提供了支持。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18892v1",
      "published_date": "2025-03-24 17:06:10 UTC",
      "updated_date": "2025-03-24 17:06:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T21:55:23.021745"
    },
    {
      "arxiv_id": "2503.18891v1",
      "title": "AgentDropout: Dynamic Agent Elimination for Token-Efficient and High-Performance LLM-Based Multi-Agent Collaboration",
      "title_zh": "AgentDropout：动态智能体消除以实现基于大语言模型的多智能体协作的令牌高效与高性能",
      "authors": [
        "Zhexuan Wang",
        "Yutong Wang",
        "Xuebo Liu",
        "Liang Ding",
        "Miao Zhang",
        "Jie Liu",
        "Min Zhang"
      ],
      "abstract": "Multi-agent systems (MAS) based on large language models (LLMs) have\ndemonstrated significant potential in collaborative problem-solving. However,\nthey still face substantial challenges of low communication efficiency and\nsuboptimal task performance, making the careful design of the agents'\ncommunication topologies particularly important. Inspired by the management\ntheory that roles in an efficient team are often dynamically adjusted, we\npropose AgentDropout, which identifies redundant agents and communication\nacross different communication rounds by optimizing the adjacency matrices of\nthe communication graphs and eliminates them to enhance both token efficiency\nand task performance. Compared to state-of-the-art methods, AgentDropout\nachieves an average reduction of 21.6% in prompt token consumption and 18.4% in\ncompletion token consumption, along with a performance improvement of 1.14 on\nthe tasks. Furthermore, the extended experiments demonstrate that AgentDropout\nachieves notable domain transferability and structure robustness, revealing its\nreliability and effectiveness. We release our code at\nhttps://github.com/wangzx1219/AgentDropout.",
      "tldr_zh": "本文提出了AgentDropout，一种动态消除冗余智能体的方法，旨在提升基于大语言模型(LLMs)的多智能体协作系统的通信效率和任务性能。该方法通过优化通信图的邻接矩阵，动态识别并消除冗余智能体和通信轮次，从而显著减少提示令牌和完成令牌的消耗，同时提升任务表现。实验表明，AgentDropout在令牌消耗上平均减少21.6%和18.4%，任务性能提升1.14，并展现出良好的领域迁移性和结构鲁棒性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18891v1",
      "published_date": "2025-03-24 17:04:55 UTC",
      "updated_date": "2025-03-24 17:04:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T21:55:38.520139"
    },
    {
      "arxiv_id": "2503.18871v1",
      "title": "Bootstrapped Model Predictive Control",
      "title_zh": "自举模型预测控制",
      "authors": [
        "Yuhang Wang",
        "Hanwei Guo",
        "Sizhe Wang",
        "Long Qian",
        "Xuguang Lan"
      ],
      "abstract": "Model Predictive Control (MPC) has been demonstrated to be effective in\ncontinuous control tasks. When a world model and a value function are\navailable, planning a sequence of actions ahead of time leads to a better\npolicy. Existing methods typically obtain the value function and the\ncorresponding policy in a model-free manner. However, we find that such an\napproach struggles with complex tasks, resulting in poor policy learning and\ninaccurate value estimation. To address this problem, we leverage the strengths\nof MPC itself. In this work, we introduce Bootstrapped Model Predictive Control\n(BMPC), a novel algorithm that performs policy learning in a bootstrapped\nmanner. BMPC learns a network policy by imitating an MPC expert, and in turn,\nuses this policy to guide the MPC process. Combined with model-based\nTD-learning, our policy learning yields better value estimation and further\nboosts the efficiency of MPC. We also introduce a lazy reanalyze mechanism,\nwhich enables computationally efficient imitation learning. Our method achieves\nsuperior performance over prior works on diverse continuous control tasks. In\nparticular, on challenging high-dimensional locomotion tasks, BMPC\nsignificantly improves data efficiency while also enhancing asymptotic\nperformance and training stability, with comparable training time and smaller\nnetwork sizes. Code is available at https://github.com/wertyuilife2/bmpc.",
      "tldr_zh": "本文提出了一种新的算法——Bootstrapped Model Predictive Control (BMPC)，通过自举方式提升模型预测控制(MPC)的性能。BMPC通过模仿MPC专家学习网络策略，并利用该策略指导MPC过程，结合基于模型的TD学习，改善了价值估计并提高了MPC效率。此外，引入了一种惰性重新分析机制，显著降低了模仿学习的计算成本。实验表明，BMPC在多种连续控制任务中表现优异，尤其是在高维运动任务上，显著提升了数据效率、渐进性能和训练稳定性，同时保持了较短的训练时间和较小的网络规模。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "Published as a conference paper at ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.18871v1",
      "published_date": "2025-03-24 16:46:36 UTC",
      "updated_date": "2025-03-24 16:46:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T21:56:03.570308"
    },
    {
      "arxiv_id": "2503.18866v1",
      "title": "Reasoning to Learn from Latent Thoughts",
      "title_zh": "从潜在思维中学习推理",
      "authors": [
        "Yangjun Ruan",
        "Neil Band",
        "Chris J. Maddison",
        "Tatsunori Hashimoto"
      ],
      "abstract": "Compute scaling for language model (LM) pretraining has outpaced the growth\nof human-written texts, leading to concerns that data will become the\nbottleneck to LM scaling. To continue scaling pretraining in this\ndata-constrained regime, we propose that explicitly modeling and inferring the\nlatent thoughts that underlie the text generation process can significantly\nimprove pretraining data efficiency. Intuitively, our approach views web text\nas the compressed final outcome of a verbose human thought process and that the\nlatent thoughts contain important contextual knowledge and reasoning steps that\nare critical to data-efficient learning. We empirically demonstrate the\neffectiveness of our approach through data-constrained continued pretraining\nfor math. We first show that synthetic data approaches to inferring latent\nthoughts significantly improve data efficiency, outperforming training on the\nsame amount of raw data (5.7\\% $\\rightarrow$ 25.4\\% on MATH). Furthermore, we\ndemonstrate latent thought inference without a strong teacher, where an LM\nbootstraps its own performance by using an EM algorithm to iteratively improve\nthe capability of the trained LM and the quality of thought-augmented\npretraining data. We show that a 1B LM can bootstrap its performance across at\nleast three iterations and significantly outperform baselines trained on raw\ndata, with increasing gains from additional inference compute when performing\nthe E-step. The gains from inference scaling and EM iterations suggest new\nopportunities for scaling data-constrained pretraining.",
      "tldr_zh": "该研究提出了一种通过推理从潜在思维中学习的方法，以解决语言模型（LM）预训练中数据瓶颈的问题。研究认为，显式建模和推断文本生成背后的潜在思维可以显著提高预训练的数据效率，因为这些潜在思维包含了重要的上下文知识和推理步骤。实验表明，通过推断潜在思维生成的合成数据在数学任务上显著提升了数据效率（从5.7%提高到25.4%）。此外，研究还展示了无需强监督的潜在思维推断方法，通过EM算法迭代提升模型性能和数据质量，证明了在数据受限的预训练中，推理计算和EM迭代为模型扩展提供了新的可能性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18866v1",
      "published_date": "2025-03-24 16:41:23 UTC",
      "updated_date": "2025-03-24 16:41:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T21:55:57.162405"
    },
    {
      "arxiv_id": "2503.18865v2",
      "title": "Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations",
      "title_zh": "构建科学创新：用于建模与发现具有影响力的知识组合的框架",
      "authors": [
        "Junlan Chen",
        "Kexin Zhang",
        "Daifeng Li",
        "Yangyang Feng",
        "Yuxuan Zhang",
        "Bowen Deng"
      ],
      "abstract": "The emergence of large language models offers new possibilities for\nstructured exploration of scientific knowledge. Rather than viewing scientific\ndiscovery as isolated ideas or content, we propose a structured approach that\nemphasizes the role of method combinations in shaping disruptive insights.\nSpecifically, we investigate how knowledge unit--especially those tied to\nmethodological design--can be modeled and recombined to yield research\nbreakthroughs. Our proposed framework addresses two key challenges. First, we\nintroduce a contrastive learning-based mechanism to identify distinguishing\nfeatures of historically disruptive method combinations within problem-driven\ncontexts. Second, we propose a reasoning-guided Monte Carlo search algorithm\nthat leverages the chain-of-thought capability of LLMs to identify promising\nknowledge recombinations for new problem statements.Empirical studies across\nmultiple domains show that the framework is capable of modeling the structural\ndynamics of innovation and successfully highlights combinations with high\ndisruptive potential. This research provides a new path for computationally\nguided scientific ideation grounded in structured reasoning and historical data\nmodeling.",
      "tldr_zh": "该研究提出了一种结构化框架，用于建模和发现具有影响力的知识组合，旨在推动科学创新。该框架通过对比学习机制识别历史上具有突破性的方法组合特征，并利用基于链式思维推理的蒙特卡洛搜索算法，发现针对新问题的潜在知识重组方案。实验表明，该框架能够有效建模创新的结构动态，并识别出具有高颠覆潜力的组合，为基于结构化推理和历史数据建模的计算引导式科学构思提供了新路径。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18865v2",
      "published_date": "2025-03-24 16:41:17 UTC",
      "updated_date": "2025-03-25 14:21:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T21:55:57.471222"
    },
    {
      "arxiv_id": "2503.18862v1",
      "title": "Exploring the Integration of Key-Value Attention Into Pure and Hybrid Transformers for Semantic Segmentation",
      "title_zh": "探索键值注意力机制在纯Transformer与混合Transformer中的整合，以应用于语义分割",
      "authors": [
        "DeShin Hwa",
        "Tobias Holmes",
        "Klaus Drechsler"
      ],
      "abstract": "While CNNs were long considered state of the art for image processing, the\nintroduction of Transformer architectures has challenged this position. While\nachieving excellent results in image classification and segmentation,\nTransformers remain inherently reliant on large training datasets and remain\ncomputationally expensive. A newly introduced Transformer derivative named KV\nTransformer shows promising results in synthetic, NLP, and image classification\ntasks, while reducing complexity and memory usage. This is especially conducive\nto use cases where local inference is required, such as medical screening\napplications. We endeavoured to further evaluate the merit of KV Transformers\non semantic segmentation tasks, specifically in the domain of medical imaging.\nBy directly comparing traditional and KV variants of the same base\narchitectures, we provide further insight into the practical tradeoffs of\nreduced model complexity. We observe a notable reduction in parameter count and\nmultiply accumulate operations, while achieving similar performance from most\nof the KV variant models when directly compared to their QKV implementation.",
      "tldr_zh": "本研究探讨了将键值注意力(Key-Value Attention)机制集成到纯Transformer和混合Transformer中，用于医学图像语义分割任务。通过对比传统Transformer与其KV变体，研究发现KV变体显著减少了模型参数数量和乘加运算量，同时在大多数情况下保持了与QKV实现相当的性能。这一发现为需要本地推理的应用场景（如医学筛查）提供了更高效的模型选择，同时为Transformer在医学图像处理中的优化提供了新的思路。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "6 pages, 3 figures, Preprint. Final version published in:\n  Bildverarbeitung f\\\"ur die Medizin 2025, Springer. DOI:\n  https://doi.org/10.1007/978-3-658-47422-5_71",
      "pdf_url": "http://arxiv.org/pdf/2503.18862v1",
      "published_date": "2025-03-24 16:38:31 UTC",
      "updated_date": "2025-03-24 16:38:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T21:55:51.382434"
    },
    {
      "arxiv_id": "2503.18854v2",
      "title": "MC-LLaVA: Multi-Concept Personalized Vision-Language Model",
      "title_zh": "MC-LLaVA：多概念个性化视觉语言模型",
      "authors": [
        "Ruichuan An",
        "Sihan Yang",
        "Ming Lu",
        "Renrui Zhang",
        "Kai Zeng",
        "Yulin Luo",
        "Jiajun Cao",
        "Hao Liang",
        "Ying Chen",
        "Qi She",
        "Shanghang Zhang",
        "Wentao Zhang"
      ],
      "abstract": "Current vision-language models (VLMs) show exceptional abilities across\ndiverse tasks, such as visual question answering. To enhance user experience,\nrecent studies investigate VLM personalization to understand user-provided\nconcepts. However, they mainly focus on single-concept personalization,\nneglecting the existence and interplay of multiple concepts, which limits\nreal-world applicability. This paper proposes the first multi-concept\npersonalization paradigm, MC-LLaVA. Specifically, MC-LLaVA employs a\nmulti-concept instruction tuning strategy, effectively integrating multiple\nconcepts in a single training step. To reduce the costs related to joint\ntraining, we propose a personalized textual prompt that uses visual token\ninformation to initialize concept tokens. Additionally, we introduce a\npersonalized visual prompt during inference, aggregating location confidence\nmaps for enhanced recognition and grounding capabilities. To advance\nmulti-concept personalization research, we further contribute a high-quality\ninstruction tuning dataset. We carefully collect images with multiple\ncharacters and objects from movies and manually generate question-answer\nsamples for multi-concept scenarios, featuring superior diversity.\nComprehensive qualitative and quantitative experiments demonstrate that\nMC-LLaVA can achieve impressive multi-concept personalized responses, paving\nthe way for VLMs to become better user-specific assistants. The code and\ndataset will be publicly available at https://github.com/arctanxarc/MC-LLaVA}.",
      "tldr_zh": "该研究提出了MC-LLaVA，首个多概念个性化视觉语言模型(VLM)，突破了现有单概念个性化研究的局限。MC-LLaVA采用多概念指令微调策略，通过个性化文本提示和视觉提示，有效整合多个概念并提升识别与定位能力。研究还构建了一个高质量的多概念指令微调数据集，包含电影中的多角色和多物体场景及其问答样本。实验表明，MC-LLaVA能够生成出色的多概念个性化响应，为VLM成为更精准的用户助手奠定了基础。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "I sincerely apologize for any inconvenience caused. We actually\n  uploaded this paper to arXiv in November 2024, as arXiv:2411.11706. During\n  this update, we did not consider the replacement operation of arXiv, which\n  led to duplicate submissions. We have made modifications at the original\n  address arXiv:2411.11706",
      "pdf_url": "http://arxiv.org/pdf/2503.18854v2",
      "published_date": "2025-03-24 16:32:17 UTC",
      "updated_date": "2025-03-25 13:50:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T21:56:17.684607"
    },
    {
      "arxiv_id": "2503.18852v1",
      "title": "Self-Organizing Graph Reasoning Evolves into a Critical State for Continuous Discovery Through Structural-Semantic Dynamics",
      "title_zh": "自组织图推理通过结构-语义动态演化至持续发现的临界状态",
      "authors": [
        "Markus J. Buehler"
      ],
      "abstract": "We report fundamental insights into how agentic graph reasoning systems\nspontaneously evolve toward a critical state that sustains continuous semantic\ndiscovery. By rigorously analyzing structural (Von Neumann graph entropy) and\nsemantic (embedding) entropy, we identify a subtle yet robust regime in which\nsemantic entropy persistently dominates over structural entropy. This interplay\nis quantified by a dimensionless Critical Discovery Parameter that stabilizes\nat a small negative value, indicating a consistent excess of semantic entropy.\nEmpirically, we observe a stable fraction (12%) of \"surprising\" edges, links\nbetween semantically distant concepts, providing evidence of long-range or\ncross-domain connections that drive continuous innovation. Concomitantly, the\nsystem exhibits scale-free and small-world topological features, alongside a\nnegative cross-correlation between structural and semantic measures,\nreinforcing the analogy to self-organized criticality. These results establish\nclear parallels with critical phenomena in physical, biological, and cognitive\ncomplex systems, revealing an entropy-based principle governing adaptability\nand continuous innovation. Crucially, semantic richness emerges as the\nunderlying driver of sustained exploration, despite not being explicitly used\nby the reasoning process. Our findings provide interdisciplinary insights and\npractical strategies for engineering intelligent systems with intrinsic\ncapacities for long-term discovery and adaptation, and offer insights into how\nmodel training strategies can be developed that reinforce critical discovery.",
      "tldr_zh": "该研究揭示了自组织图推理系统如何自发演化至临界状态，以支持持续的语义发现。通过分析结构熵（Von Neumann图熵）和语义熵（嵌入熵），研究发现了一种微妙但稳健的机制，其中语义熵持续主导结构熵。这一相互作用通过一个无量纲的临界发现参数量化，该参数稳定在一个小的负值，表明语义熵的持续过剩。实验观察到12%的“意外”边（即语义距离较远的概念之间的连接），证明了驱动持续创新的长程或跨领域连接。系统同时表现出无标度和小世界拓扑特征，以及结构熵和语义熵之间的负相关性，进一步印证了自组织临界性的类比。这些结果为设计具有长期发现和适应能力的人工智能系统提供了跨学科的见解和实用策略。",
      "categories": [
        "cs.AI",
        "cond-mat.mes-hall",
        "cs.LG",
        "nlin.AO",
        "physics.app-ph"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18852v1",
      "published_date": "2025-03-24 16:30:37 UTC",
      "updated_date": "2025-03-24 16:30:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T21:56:21.924749"
    },
    {
      "arxiv_id": "2503.18842v1",
      "title": "Three Kinds of AI Ethics",
      "title_zh": "三种人工智能伦理",
      "authors": [
        "Emanuele Ratti"
      ],
      "abstract": "There is an overwhelmingly abundance of works in AI Ethics. This growth is\nchaotic because of how sudden it is, its volume, and its multidisciplinary\nnature. This makes difficult to keep track of debates, and to systematically\ncharacterize goals, research questions, methods, and expertise required by AI\nethicists. In this article, I show that the relation between AI and ethics can\nbe characterized in at least three ways, which correspond to three\nwell-represented kinds of AI ethics: ethics and AI; ethics in AI; ethics of AI.\nI elucidate the features of these three kinds of AI Ethics, characterize their\nresearch questions, and identify the kind of expertise that each kind needs. I\nalso show how certain criticisms to AI ethics are misplaced, as being done from\nthe point of view of one kind of AI ethics, to another kind with different\ngoals. All in all, this work sheds light on the nature of AI ethics, and set\nthe grounds for more informed discussions about scope, methods, and trainings\nof AI ethicists.",
      "tldr_zh": "本文系统地将AI伦理分为三种类型：**AI与伦理**、**AI中的伦理**和**AI的伦理**，分别探讨了它们的研究问题、所需专业知识以及目标。通过这种分类，文章澄清了对AI伦理的某些误解，指出这些批评往往源于不同类型AI伦理之间的混淆。该研究为AI伦理的范围、方法和培训提供了更清晰的理论基础，有助于推动更深入的讨论。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "16 pages, two figures",
      "pdf_url": "http://arxiv.org/pdf/2503.18842v1",
      "published_date": "2025-03-24 16:15:03 UTC",
      "updated_date": "2025-03-24 16:15:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T21:56:31.958428"
    },
    {
      "arxiv_id": "2503.18836v1",
      "title": "Dual-domain Multi-path Self-supervised Diffusion Model for Accelerated MRI Reconstruction",
      "title_zh": "双域多路径自监督扩散模型用于加速MRI重建",
      "authors": [
        "Yuxuan Zhang",
        "Jinkui Hao",
        "Bo Zhou"
      ],
      "abstract": "Magnetic resonance imaging (MRI) is a vital diagnostic tool, but its\ninherently long acquisition times reduce clinical efficiency and patient\ncomfort. Recent advancements in deep learning, particularly diffusion models,\nhave improved accelerated MRI reconstruction. However, existing diffusion\nmodels' training often relies on fully sampled data, models incur high\ncomputational costs, and often lack uncertainty estimation, limiting their\nclinical applicability. To overcome these challenges, we propose a novel\nframework, called Dual-domain Multi-path Self-supervised Diffusion Model\n(DMSM), that integrates a self-supervised dual-domain diffusion model training\nscheme, a lightweight hybrid attention network for the reconstruction diffusion\nmodel, and a multi-path inference strategy, to enhance reconstruction accuracy,\nefficiency, and explainability. Unlike traditional diffusion-based models, DMSM\neliminates the dependency on training from fully sampled data, making it more\npractical for real-world clinical settings. We evaluated DMSM on two human MRI\ndatasets, demonstrating that it achieves favorable performance over several\nsupervised and self-supervised baselines, particularly in preserving fine\nanatomical structures and suppressing artifacts under high acceleration\nfactors. Additionally, our model generates uncertainty maps that correlate\nreasonably well with reconstruction errors, offering valuable clinically\ninterpretable guidance and potentially enhancing diagnostic confidence.",
      "tldr_zh": "本研究提出了一种双域多路径自监督扩散模型（DMSM），用于加速磁共振成像（MRI）重建。该模型通过自监督双域扩散训练、轻量级混合注意力网络和多路径推理策略，显著提升了重建的准确性、效率和可解释性。与现有方法不同，DMSM无需依赖全采样数据进行训练，更适合实际临床应用。实验表明，该模型在高速率采样下能更好地保留精细解剖结构、抑制伪影，并生成与重建误差相关的置信度图，为临床诊断提供可靠指导。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "10 pages, 8 figures, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.18836v1",
      "published_date": "2025-03-24 16:10:51 UTC",
      "updated_date": "2025-03-24 16:10:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T21:56:55.936712"
    },
    {
      "arxiv_id": "2503.18826v1",
      "title": "Interpretable and Fair Mechanisms for Abstaining Classifiers",
      "title_zh": "可解释且公平的弃权分类器机制",
      "authors": [
        "Daphne Lenders",
        "Andrea Pugnana",
        "Roberto Pellungrini",
        "Toon Calders",
        "Dino Pedreschi",
        "Fosca Giannotti"
      ],
      "abstract": "Abstaining classifiers have the option to refrain from providing a prediction\nfor instances that are difficult to classify. The abstention mechanism is\ndesigned to trade off the classifier's performance on the accepted data while\nensuring a minimum number of predictions. In this setting, often fairness\nconcerns arise when the abstention mechanism solely reduces errors for the\nmajority groups of the data, resulting in increased performance differences\nacross demographic groups. While there exist a bunch of methods that aim to\nreduce discrimination when abstaining, there is no mechanism that can do so in\nan explainable way. In this paper, we fill this gap by introducing\nInterpretable and Fair Abstaining Classifier IFAC, an algorithm that can reject\npredictions both based on their uncertainty and their unfairness. By rejecting\npossibly unfair predictions, our method reduces error and positive decision\nrate differences across demographic groups of the non-rejected data. Since the\nunfairness-based rejections are based on an interpretable-by-design method,\ni.e., rule-based fairness checks and situation testing, we create a transparent\nprocess that can empower human decision-makers to review the unfair predictions\nand make more just decisions for them. This explainable aspect is especially\nimportant in light of recent AI regulations, mandating that any high-risk\ndecision task should be overseen by human experts to reduce discrimination\nrisks.",
      "tldr_zh": "本文提出了一种可解释且公平的拒绝分类器IFAC，该算法能够基于预测的不确定性和不公平性拒绝预测，从而减少不同人口群体在未拒绝数据上的错误率和正向决策率差异。通过采用规则化的公平性检查和情境测试等可解释性设计方法，IFAC为人类决策者提供了透明的审查过程，使其能够审查不公平预测并做出更公正的决策。这一方法尤其适用于高风险决策任务，符合当前AI法规要求，旨在减少歧视风险。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "25 pages, 8 figures. In: Machine Learning and Knowledge Discovery in\n  Databases. Research Track. ECML PKDD 2024",
      "pdf_url": "http://arxiv.org/pdf/2503.18826v1",
      "published_date": "2025-03-24 16:06:43 UTC",
      "updated_date": "2025-03-24 16:06:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T21:56:49.201754"
    },
    {
      "arxiv_id": "2503.18825v1",
      "title": "EconEvals: Benchmarks and Litmus Tests for LLM Agents in Unknown Environments",
      "title_zh": "EconEvals：未知环境中LLM智能体的基准与石蕊测试",
      "authors": [
        "Sara Fish",
        "Julia Shephard",
        "Minkai Li",
        "Ran I. Shorrer",
        "Yannai A. Gonczarowski"
      ],
      "abstract": "We develop benchmarks for LLM agents that act in, learn from, and strategize\nin unknown environments, the specifications of which the LLM agent must learn\nover time from deliberate exploration. Our benchmarks consist of\ndecision-making tasks derived from key problems in economics. To forestall\nsaturation, the benchmark tasks are synthetically generated with scalable\ndifficulty levels. Additionally, we propose litmus tests, a new kind of\nquantitative measure for LLMs and LLM agents. Unlike benchmarks, litmus tests\nquantify differences in character, values, and tendencies of LLMs and LLM\nagents, by considering their behavior when faced with tradeoffs (e.g.,\nefficiency versus equality) where there is no objectively right or wrong\nbehavior. Overall, our benchmarks and litmus tests assess the abilities and\ntendencies of LLM agents in tackling complex economic problems in diverse\nsettings spanning procurement, scheduling, task allocation, and pricing --\napplications that should grow in importance as such agents are further\nintegrated into the economy.",
      "tldr_zh": "该研究提出了EconEvals，一套用于评估LLM智能体在未知环境中学习与决策能力的基准测试和litmus测试。基准测试基于经济学关键问题生成，包含可扩展难度的任务，用于评估智能体在采购、调度、任务分配和定价等场景中的表现。litmus测试则通过衡量智能体在效率与公平等权衡中的行为倾向，量化其价值观和特性。这些工具为LLM智能体在经济领域的应用提供了全面的评估框架。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.GT"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18825v1",
      "published_date": "2025-03-24 16:06:04 UTC",
      "updated_date": "2025-03-24 16:06:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T21:56:55.125789"
    },
    {
      "arxiv_id": "2503.18817v1",
      "title": "Enhanced OoD Detection through Cross-Modal Alignment of Multi-Modal Representations",
      "title_zh": "通过多模态表示的跨模态对齐增强分布外检测",
      "authors": [
        "Jeonghyeon Kim",
        "Sangheum Hwang"
      ],
      "abstract": "Prior research on out-of-distribution detection (OoDD) has primarily focused\non single-modality models. Recently, with the advent of large-scale pretrained\nvision-language models such as CLIP, OoDD methods utilizing such multi-modal\nrepresentations through zero-shot and prompt learning strategies have emerged.\nHowever, these methods typically involve either freezing the pretrained weights\nor only partially tuning them, which can be suboptimal for downstream datasets.\nIn this paper, we highlight that multi-modal fine-tuning (MMFT) can achieve\nnotable OoDD performance. Despite some recent works demonstrating the impact of\nfine-tuning methods for OoDD, there remains significant potential for\nperformance improvement. We investigate the limitation of na\\\"ive fine-tuning\nmethods, examining why they fail to fully leverage the pretrained knowledge.\nOur empirical analysis suggests that this issue could stem from the modality\ngap within in-distribution (ID) embeddings. To address this, we propose a\ntraining objective that enhances cross-modal alignment by regularizing the\ndistances between image and text embeddings of ID data. This adjustment helps\nin better utilizing pretrained textual information by aligning similar\nsemantics from different modalities (i.e., text and image) more closely in the\nhyperspherical representation space. We theoretically demonstrate that the\nproposed regularization corresponds to the maximum likelihood estimation of an\nenergy-based model on a hypersphere. Utilizing ImageNet-1k OoD benchmark\ndatasets, we show that our method, combined with post-hoc OoDD approaches\nleveraging pretrained knowledge (e.g., NegLabel), significantly outperforms\nexisting methods, achieving state-of-the-art OoDD performance and leading ID\naccuracy.",
      "tldr_zh": "本文提出了一种通过跨模态对齐提升多模态表示分布外检测(OoDD)性能的方法。针对现有方法在微调预训练多模态模型时未能充分利用预训练知识的问题，研究者分析了模态间嵌入差距的局限性，并提出了一种新的训练目标，通过对分布内(ID)数据的图像和文本嵌入进行距离正则化，增强跨模态对齐。理论分析表明，该正则化对应于超球面上基于能量的模型的最大似然估计。实验结果表明，结合后处理OoDD方法，该方法在ImageNet-1k OoD基准数据集上显著优于现有方法，达到了最先进的OoDD性能和领先的ID准确率。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.18817v1",
      "published_date": "2025-03-24 16:00:21 UTC",
      "updated_date": "2025-03-24 16:00:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T21:56:57.221179"
    },
    {
      "arxiv_id": "2503.18816v1",
      "title": "Learning Multi-Robot Coordination through Locality-Based Factorized Multi-Agent Actor-Critic Algorithm",
      "title_zh": "基于局部性因子化多智能体演员-评论家算法的多机器人协调学习",
      "authors": [
        "Chak Lam Shek",
        "Amrit Singh Bedi",
        "Anjon Basak",
        "Ellen Novoseller",
        "Nick Waytowich",
        "Priya Narayanan",
        "Dinesh Manocha",
        "Pratap Tokekar"
      ],
      "abstract": "In this work, we present a novel cooperative multi-agent reinforcement\nlearning method called \\textbf{Loc}ality based \\textbf{Fac}torized\n\\textbf{M}ulti-Agent \\textbf{A}ctor-\\textbf{C}ritic (Loc-FACMAC). Existing\nstate-of-the-art algorithms, such as FACMAC, rely on global reward information,\nwhich may not accurately reflect the quality of individual robots' actions in\ndecentralized systems. We integrate the concept of locality into critic\nlearning, where strongly related robots form partitions during training. Robots\nwithin the same partition have a greater impact on each other, leading to more\nprecise policy evaluation. Additionally, we construct a dependency graph to\ncapture the relationships between robots, facilitating the partitioning\nprocess. This approach mitigates the curse of dimensionality and prevents\nrobots from using irrelevant information. Our method improves existing\nalgorithms by focusing on local rewards and leveraging partition-based learning\nto enhance training efficiency and performance. We evaluate the performance of\nLoc-FACMAC in three environments: Hallway, Multi-cartpole, and\nBounded-Cooperative-Navigation. We explore the impact of partition sizes on the\nperformance and compare the result with baseline MARL algorithms such as LOMAQ,\nFACMAC, and QMIX. The experiments reveal that, if the locality structure is\ndefined properly, Loc-FACMAC outperforms these baseline algorithms up to 108\\%,\nindicating that exploiting the locality structure in the actor-critic framework\nimproves the MARL performance.",
      "tldr_zh": "本文提出了一种名为Loc-FACMAC的新型多智能体强化学习算法，通过引入局部性(locality)概念改进现有的FACMAC方法。该算法将强相关的机器人划分为同一分区，在训练过程中仅关注局部奖励信息，从而提高策略评估的精确性，并缓解维度灾难问题。实验在三个环境中进行，结果表明，当局部性结构定义合理时，Loc-FACMAC在性能上比LOMAQ、FACMAC和QMIX等基线算法提升了最高108%，验证了局部性结构在多智能体强化学习中的有效性。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18816v1",
      "published_date": "2025-03-24 16:00:16 UTC",
      "updated_date": "2025-03-24 16:00:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T21:56:58.048800"
    },
    {
      "arxiv_id": "2503.18814v1",
      "title": "Towards Responsible AI Music: an Investigation of Trustworthy Features for Creative Systems",
      "title_zh": "迈向负责任的AI音乐：创意系统可信特征的探索",
      "authors": [
        "Jacopo de Berardinis",
        "Lorenzo Porcaro",
        "Albert Meroño-Peñuela",
        "Angelo Cangelosi",
        "Tess Buckley"
      ],
      "abstract": "Generative AI is radically changing the creative arts, by fundamentally\ntransforming the way we create and interact with cultural artefacts. While\noffering unprecedented opportunities for artistic expression and\ncommercialisation, this technology also raises ethical, societal, and legal\nconcerns. Key among these are the potential displacement of human creativity,\ncopyright infringement stemming from vast training datasets, and the lack of\ntransparency, explainability, and fairness mechanisms. As generative systems\nbecome pervasive in this domain, responsible design is crucial. Whilst previous\nwork has tackled isolated aspects of generative systems (e.g., transparency,\nevaluation, data), we take a comprehensive approach, grounding these efforts\nwithin the Ethics Guidelines for Trustworthy Artificial Intelligence produced\nby the High-Level Expert Group on AI appointed by the European Commission - a\nframework for designing responsible AI systems across seven macro requirements.\nFocusing on generative music AI, we illustrate how these requirements can be\ncontextualised for the field, addressing trustworthiness across multiple\ndimensions and integrating insights from the existing literature. We further\npropose a roadmap for operationalising these contextualised requirements,\nemphasising interdisciplinary collaboration and stakeholder engagement. Our\nwork provides a foundation for designing and evaluating responsible music\ngeneration systems, calling for collaboration among AI experts, ethicists,\nlegal scholars, and artists. This manuscript is accompanied by a website:\nhttps://amresearchlab.github.io/raim-framework/.",
      "tldr_zh": "本文探讨了生成式AI音乐系统中的可信赖特征，旨在构建负责任的AI音乐创作系统。研究基于欧盟高级别专家组发布的《可信人工智能伦理指南》，提出了涵盖透明度、公平性、可解释性等七个宏观要求的框架，并将其具体化应用于音乐生成AI领域。作者提出了实现这些要求的路线图，强调跨学科合作与利益相关者参与，为设计和评估负责任的音乐生成系统奠定了基础。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18814v1",
      "published_date": "2025-03-24 15:54:47 UTC",
      "updated_date": "2025-03-24 15:54:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T21:57:05.618709"
    },
    {
      "arxiv_id": "2503.18813v1",
      "title": "Defeating Prompt Injections by Design",
      "title_zh": "通过设计击败提示注入攻击",
      "authors": [
        "Edoardo Debenedetti",
        "Ilia Shumailov",
        "Tianqi Fan",
        "Jamie Hayes",
        "Nicholas Carlini",
        "Daniel Fabian",
        "Christoph Kern",
        "Chongyang Shi",
        "Andreas Terzis",
        "Florian Tramèr"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly deployed in agentic systems\nthat interact with an external environment. However, LLM agents are vulnerable\nto prompt injection attacks when handling untrusted data. In this paper we\npropose CaMeL, a robust defense that creates a protective system layer around\nthe LLM, securing it even when underlying models may be susceptible to attacks.\nTo operate, CaMeL explicitly extracts the control and data flows from the\n(trusted) query; therefore, the untrusted data retrieved by the LLM can never\nimpact the program flow. To further improve security, CaMeL relies on a notion\nof a capability to prevent the exfiltration of private data over unauthorized\ndata flows. We demonstrate effectiveness of CaMeL by solving $67\\%$ of tasks\nwith provable security in AgentDojo [NeurIPS 2024], a recent agentic security\nbenchmark.",
      "tldr_zh": "该研究提出了CaMeL，一种通过设计抵御提示注入攻击的防御机制。CaMeL在大型语言模型(LLM)外部构建了一个保护层，通过显式分离控制流和数据流，确保不可信数据无法影响程序执行。此外，CaMeL引入能力(capability)概念，防止私密数据通过未授权渠道泄露。实验表明，CaMeL在AgentDojo基准测试中成功解决了67%的任务，并提供了可证明的安全性。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18813v1",
      "published_date": "2025-03-24 15:54:10 UTC",
      "updated_date": "2025-03-24 15:54:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T21:57:17.351548"
    },
    {
      "arxiv_id": "2503.18809v1",
      "title": "Classical Planning with LLM-Generated Heuristics: Challenging the State of the Art with Python Code",
      "title_zh": "利用LLM生成的启发式方法进行经典规划：以Python代码挑战现有技术",
      "authors": [
        "Augusto B. Corrêa",
        "André G. Pereira",
        "Jendrik Seipp"
      ],
      "abstract": "In recent years, large language models (LLMs) have shown remarkable\ncapabilities in various artificial intelligence problems. However, they fail to\nplan reliably, even when prompted with a detailed definition of the planning\ntask. Attempts to improve their planning capabilities, such as chain-of-thought\nprompting, fine-tuning, and explicit \"reasoning\" still yield incorrect plans\nand usually fail to generalize to larger tasks. In this paper, we show how to\nuse LLMs to generate correct plans, even for out-of-distribution tasks of\nincreasing size. For a given planning domain, we ask an LLM to generate several\ndomain-dependent heuristic functions in the form of Python code, evaluate them\non a set of training tasks within a greedy best-first search, and choose the\nstrongest one. The resulting LLM-generated heuristics solve many more unseen\ntest tasks than state-of-the-art domain-independent heuristics for classical\nplanning. They are even competitive with the strongest learning algorithm for\ndomain-dependent planning. These findings are especially remarkable given that\nour proof-of-concept implementation is based on an unoptimized Python planner\nand the baselines all build upon highly optimized C++ code. In some domains,\nthe LLM-generated heuristics expand fewer states than the baselines, revealing\nthat they are not only efficiently computable, but sometimes even more\ninformative than the state-of-the-art heuristics. Overall, our results show\nthat sampling a set of planning heuristic function programs can significantly\nimprove the planning capabilities of LLMs.",
      "tldr_zh": "本研究提出了一种利用大语言模型(LLMs)生成启发式函数的方法，显著提升了经典规划任务的性能。通过让LLM生成特定领域的Python代码启发式函数，并在贪心最佳优先搜索中评估和选择最优函数，该方法在未见的测试任务上表现优于现有的领域无关启发式方法，甚至与最强的领域依赖学习算法相媲美。值得注意的是，尽管基于未优化的Python规划器实现，该方法在某些领域甚至比高度优化的C++基线方法扩展更少的状态，表明其不仅计算高效，有时还更具信息性。研究结果表明，通过采样一组规划启发式函数程序，可以显著提升LLMs的规划能力。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18809v1",
      "published_date": "2025-03-24 15:50:20 UTC",
      "updated_date": "2025-03-24 15:50:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T21:58:37.620866"
    },
    {
      "arxiv_id": "2503.18792v1",
      "title": "REALM: A Dataset of Real-World LLM Use Cases",
      "title_zh": "REALM：一个真实世界大语言模型应用案例的数据集",
      "authors": [
        "Jingwen Cheng",
        "Kshitish Ghate",
        "Wenyue Hua",
        "William Yang Wang",
        "Hong Shen",
        "Fei Fang"
      ],
      "abstract": "Large Language Models, such as the GPT series, have driven significant\nindustrial applications, leading to economic and societal transformations.\nHowever, a comprehensive understanding of their real-world applications remains\nlimited. To address this, we introduce REALM, a dataset of over 94,000 LLM use\ncases collected from Reddit and news articles. REALM captures two key\ndimensions: the diverse applications of LLMs and the demographics of their\nusers. It categorizes LLM applications and explores how users' occupations\nrelate to the types of applications they use. By integrating real-world data,\nREALM offers insights into LLM adoption across different domains, providing a\nfoundation for future research on their evolving societal roles. A dedicated\ndashboard https://realm-e7682.web.app/ presents the data.",
      "tldr_zh": "该研究提出了REALM数据集，收集了来自Reddit和新闻文章的超过94,000个大型语言模型(LLM)实际应用案例，涵盖LLM的多样化应用场景及其用户群体特征。REALM不仅对LLM的应用进行分类，还分析了用户职业与其使用场景之间的关系，为理解LLM在不同领域的实际应用提供了数据支持。该数据集为研究LLM的社会角色演变奠定了基础，并提供了一个专门的仪表盘用于数据展示。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "9 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.18792v1",
      "published_date": "2025-03-24 15:39:25 UTC",
      "updated_date": "2025-03-24 15:39:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T21:59:30.687174"
    },
    {
      "arxiv_id": "2503.18778v1",
      "title": "The case for delegated AI autonomy for Human AI teaming in healthcare",
      "title_zh": "医疗领域人机协作中委托式AI自主性的必要性",
      "authors": [
        "Yan Jia",
        "Harriet Evans",
        "Zoe Porter",
        "Simon Graham",
        "John McDermid",
        "Tom Lawton",
        "David Snead",
        "Ibrahim Habli"
      ],
      "abstract": "In this paper we propose an advanced approach to integrating artificial\nintelligence (AI) into healthcare: autonomous decision support. This approach\nallows the AI algorithm to act autonomously for a subset of patient cases\nwhilst serving a supportive role in other subsets of patient cases based on\ndefined delegation criteria. By leveraging the complementary strengths of both\nhumans and AI, it aims to deliver greater overall performance than existing\nhuman-AI teaming models. It ensures safe handling of patient cases and\npotentially reduces clinician review time, whilst being mindful of AI tool\nlimitations. After setting the approach within the context of current human-AI\nteaming models, we outline the delegation criteria and apply them to a specific\nAI-based tool used in histopathology. The potential impact of the approach and\nthe regulatory requirements for its successful implementation are then\ndiscussed.",
      "tldr_zh": "本文提出了一种在医疗领域整合人工智能（AI）的先进方法：自主决策支持。该方法允许AI算法在特定患者案例中自主行动，而在其他案例中则基于预设的委派标准发挥辅助作用，从而充分发挥人类和AI的互补优势，提升整体性能。研究通过组织病理学中的具体AI工具案例，阐述了委派标准及其潜在影响，并讨论了成功实施所需的监管要求。",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18778v1",
      "published_date": "2025-03-24 15:26:54 UTC",
      "updated_date": "2025-03-24 15:26:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T21:59:17.076354"
    },
    {
      "arxiv_id": "2503.18773v1",
      "title": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs Decoding with Low-Bit KV Cache",
      "title_zh": "BitDecoding：利用低比特 KV 缓存解锁 Tensor Cores 的长上下文 LLM 解码",
      "authors": [
        "Dayou Du",
        "Shijie Cao",
        "Jianyi Cheng",
        "Ting Cao",
        "Mao Yang"
      ],
      "abstract": "The growing adoption of long-context Large Language Models (LLMs) has\nintroduced significant memory and computational challenges in autoregressive\ndecoding due to the expanding Key-Value (KV) cache. KV cache quantization has\nemerged as a promising solution, with prior work showing that 4-bit or even\n2-bit quantization can maintain model accuracy while reducing memory costs.\nHowever, despite these benefits, preliminary implementations for the low-bit KV\ncache struggle to deliver the expected speedup due to quantization and\ndequantization overheads and the lack of Tensor Cores utilization. In this\nwork, we propose BitDecoding, a GPU-optimized framework that unlocks Tensor\nCores for efficient decoding with low-bit KV cache. Efficiently leveraging\nTensor Cores for low-bit KV cache is challenging due to the dynamic nature of\nKV cache generation at each decoding step. BitDecoding addresses these\nchallenges with a Tensor Cores-Centric BitFusion Scheme that ensures data\nlayout compatibility to enable high utilization of Tensor Cores. Additionally,\nBitDecoding incorporates a warp-efficient parallel decoding kernel and a\nfine-grained asynchronous pipeline, minimizing dequantization overhead and\nimproving computational efficiency. Experiments show that BitDecoding achieves\nup to 7.5x speedup on RTX 4090, 4.8x on A100, and 8.9x on H100, compared to\nFP16 FlashDecoding-v2. It also outperforms the state-of-the-art low-bit KV\ncache implementation (QServe) by up to 4.3x. On LLaMA-3.1-8B with a 128K\nsequence length, BitDecoding reduces single-batch decoding latency by 3x,\ndemonstrating its effectiveness in long-context generation scenarios. The code\nis available at https://github.com/DD-DuDa/BitDecoding.",
      "tldr_zh": "该研究提出了BitDecoding，一种GPU优化框架，旨在通过低比特KV缓存（Key-Value Cache）解锁Tensor Cores，以提升长上下文大语言模型（LLMs）的解码效率。针对低比特KV缓存量化与反量化开销以及Tensor Cores利用率不足的问题，BitDecoding设计了Tensor Cores-Centric BitFusion Scheme，确保数据布局兼容性，并结合高效的并行解码内核和细粒度异步流水线，显著减少反量化开销。实验表明，BitDecoding在RTX 4090、A100和H100上分别实现了最高7.5倍、4.8倍和8.9倍的加速，并在LLaMA-3.1-8B模型上将单批次解码延迟降低了3倍，有效支持长上下文生成任务。",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.CL",
        "cs.PF"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18773v1",
      "published_date": "2025-03-24 15:22:41 UTC",
      "updated_date": "2025-03-24 15:22:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T21:59:00.892531"
    },
    {
      "arxiv_id": "2503.18762v1",
      "title": "Mechanistic Interpretability of Fine-Tuned Vision Transformers on Distorted Images: Decoding Attention Head Behavior for Transparent and Trustworthy AI",
      "title_zh": "微调视觉Transformer在失真图像上的机制可解释性：解码注意力头行为以实现透明可信的AI",
      "authors": [
        "Nooshin Bahador"
      ],
      "abstract": "Mechanistic interpretability improves the safety, reliability, and robustness\nof large AI models. This study examined individual attention heads in vision\ntransformers (ViTs) fine tuned on distorted 2D spectrogram images containing\nnon relevant content (axis labels, titles, color bars). By introducing\nextraneous features, the study analyzed how transformer components processed\nunrelated information, using mechanistic interpretability to debug issues and\nreveal insights into transformer architectures. Attention maps assessed head\ncontributions across layers. Heads in early layers (1 to 3) showed minimal task\nimpact with ablation increased MSE loss slightly ({\\mu}=0.11%, {\\sigma}=0.09%),\nindicating focus on less critical low level features. In contrast, deeper heads\n(e.g., layer 6) caused a threefold higher loss increase ({\\mu}=0.34%,\n{\\sigma}=0.02%), demonstrating greater task importance. Intermediate layers (6\nto 11) exhibited monosemantic behavior, attending exclusively to chirp regions.\nSome early heads (1 to 4) were monosemantic but non task relevant (e.g. text\ndetectors, edge or corner detectors). Attention maps distinguished monosemantic\nheads (precise chirp localization) from polysemantic heads (multiple irrelevant\nregions). These findings revealed functional specialization in ViTs, showing\nhow heads processed relevant vs. extraneous information. By decomposing\ntransformers into interpretable components, this work enhanced model\nunderstanding, identified vulnerabilities, and advanced safer, more transparent\nAI.",
      "tldr_zh": "本研究通过机制可解释性方法，分析了在包含无关内容（如坐标轴标签、标题、颜色条）的失真2D频谱图上微调的视觉Transformer（ViTs）中单个注意力头的行为。研究发现，浅层（1-3层）注意力头对任务影响较小，主要关注低层次特征；而深层（如第6层）注意力头对任务更为重要，损失增加显著。中间层（6-11层）表现出单义性，专注于特定区域（如chirp信号），而某些浅层头虽为单义性但与非任务相关（如文本检测、边缘检测）。通过注意力图区分单义性与多义性头，揭示了ViTs中功能专门化的特征，为构建更安全、透明的AI提供了见解。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "15 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.18762v1",
      "published_date": "2025-03-24 15:11:24 UTC",
      "updated_date": "2025-03-24 15:11:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T21:59:05.654232"
    },
    {
      "arxiv_id": "2503.18755v1",
      "title": "EgoSurgery-HTS: A Dataset for Egocentric Hand-Tool Segmentation in Open Surgery Videos",
      "title_zh": "EgoSurgery-HTS：开放手术视频中第一人称手-工具分割数据集",
      "authors": [
        "Nathan Darjana",
        "Ryo Fujii",
        "Hideo Saito",
        "Hiroki Kajita"
      ],
      "abstract": "Egocentric open-surgery videos capture rich, fine-grained details essential\nfor accurately modeling surgical procedures and human behavior in the operating\nroom. A detailed, pixel-level understanding of hands and surgical tools is\ncrucial for interpreting a surgeon's actions and intentions. We introduce\nEgoSurgery-HTS, a new dataset with pixel-wise annotations and a benchmark suite\nfor segmenting surgical tools, hands, and interacting tools in egocentric\nopen-surgery videos. Specifically, we provide a labeled dataset for (1) tool\ninstance segmentation of 14 distinct surgical tools, (2) hand instance\nsegmentation, and (3) hand-tool segmentation to label hands and the tools they\nmanipulate. Using EgoSurgery-HTS, we conduct extensive evaluations of\nstate-of-the-art segmentation methods and demonstrate significant improvements\nin the accuracy of hand and hand-tool segmentation in egocentric open-surgery\nvideos compared to existing datasets. The dataset will be released at\nhttps://github.com/Fujiry0/EgoSurgery.",
      "tldr_zh": "该研究推出了EgoSurgery-HTS数据集，专注于开放手术视频中的手部与手术工具的像素级分割。数据集包含14种手术工具的实例分割、手部实例分割以及手与工具交互的分割标注，为理解外科医生的行为和意图提供了精细支持。实验表明，基于该数据集的分割方法在准确率上显著优于现有数据集，为手术视频分析提供了新的基准。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18755v1",
      "published_date": "2025-03-24 15:04:32 UTC",
      "updated_date": "2025-03-24 15:04:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T21:59:21.198421"
    },
    {
      "arxiv_id": "2503.18751v1",
      "title": "Construction Identification and Disambiguation Using BERT: A Case Study of NPN",
      "title_zh": "使用BERT进行构式识别与消歧：以NPN构式为例",
      "authors": [
        "Wesley Scivetti",
        "Nathan Schneider"
      ],
      "abstract": "Construction Grammar hypothesizes that knowledge of a language consists\nchiefly of knowledge of form-meaning pairs (''constructions'') that include\nvocabulary, general grammar rules, and even idiosyncratic patterns. Recent work\nhas shown that transformer language models represent at least some\nconstructional patterns, including ones where the construction is rare overall.\nIn this work, we probe BERT's representation of the form and meaning of a minor\nconstruction of English, the NPN (noun-preposition-noun) construction --\nexhibited in such expressions as face to face and day to day -- which is known\nto be polysemous. We construct a benchmark dataset of semantically annotated\ncorpus instances (including distractors that superficially resemble the\nconstruction). With this dataset, we train and evaluate probing classifiers.\nThey achieve decent discrimination of the construction from distractors, as\nwell as sense disambiguation among true instances of the construction,\nrevealing that BERT embeddings carry indications of the construction's\nsemantics. Moreover, artificially permuting the word order of true construction\ninstances causes them to be rejected, indicating sensitivity to matters of\nform. We conclude that BERT does latently encode at least some knowledge of the\nNPN construction going beyond a surface syntactic pattern and lexical cues.",
      "tldr_zh": "本研究探讨了BERT模型对英语中NPN（名词-介词-名词）构造的语义和形式的表征能力。通过构建一个包含语义标注的基准数据集，研究者训练并评估了探测分类器，发现BERT能够有效区分NPN构造与其表面相似的干扰项，并在真实实例中进行语义消歧。实验还表明，BERT对词序变化敏感，进一步证实其不仅捕捉了表面句法模式，还隐含了对NPN构造的深层语义知识。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "8 pages, ACL long-paper format (preprint)",
      "pdf_url": "http://arxiv.org/pdf/2503.18751v1",
      "published_date": "2025-03-24 14:59:39 UTC",
      "updated_date": "2025-03-24 14:59:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T21:59:36.323019"
    },
    {
      "arxiv_id": "2503.18706v1",
      "title": "Energy-Efficient Dynamic Training and Inference for GNN-Based Network Modeling",
      "title_zh": "基于图神经网络的高效能动态训练与推理网络建模",
      "authors": [
        "Chetna Singhal",
        "Yassine Hadjadj-Aoul"
      ],
      "abstract": "Efficient network modeling is essential for resource optimization and network\nplanning in next-generation large-scale complex networks. Traditional\napproaches, such as queuing theory-based modeling and packet-based simulators,\ncan be inefficient due to the assumption made and the computational expense,\nrespectively. To address these challenges, we propose an innovative\nenergy-efficient dynamic orchestration of Graph Neural Networks (GNN) based\nmodel training and inference framework for context-aware network modeling and\npredictions. We have developed a low-complexity solution framework, QAG, that\nis a Quantum approximation optimization (QAO) algorithm for Adaptive\norchestration of GNN-based network modeling. We leverage the tripartite graph\nmodel to represent a multi-application system with many compute nodes.\nThereafter, we apply the constrained graph-cutting using QAO to find the\nfeasible energy-efficient configurations of the GNN-based model and deploying\nthem on the available compute nodes to meet the network modeling application\nrequirements. The proposed QAG scheme closely matches the optimum and offers\natleast a 50% energy saving while meeting the application requirements with 60%\nlower churn-rate.",
      "tldr_zh": "本研究提出了一种基于图神经网络(GNN)的节能动态训练与推理框架，用于网络建模和预测。通过开发低复杂度的QAG框架，结合量子近似优化算法(QAO)和三分图模型，实现了GNN模型的自适应编排。该方法在满足网络建模需求的同时，显著降低了能耗和节点切换率，相比传统方法节省至少50%的能源，并将切换率降低了60%。",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.ET",
        "cs.LG"
      ],
      "primary_category": "cs.NI",
      "comment": "Accepted in IEEE WCNC 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.18706v1",
      "published_date": "2025-03-24 14:17:57 UTC",
      "updated_date": "2025-03-24 14:17:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:01:03.466544"
    },
    {
      "arxiv_id": "2503.18684v1",
      "title": "Efficient Continual Adaptation of Pretrained Robotic Policy with Online Meta-Learned Adapters",
      "title_zh": "利用在线元学习适配器高效持续适应预训练机器人策略",
      "authors": [
        "Ruiqi Zhu",
        "Endong Sun",
        "Guanhe Huang",
        "Oya Celiktutan"
      ],
      "abstract": "Continual adaptation is essential for general autonomous agents. For example,\na household robot pretrained with a repertoire of skills must still adapt to\nunseen tasks specific to each household. Motivated by this, building upon\nparameter-efficient fine-tuning in language models, prior works have explored\nlightweight adapters to adapt pretrained policies, which can preserve learned\nfeatures from the pretraining phase and demonstrate good adaptation\nperformances. However, these approaches treat task learning separately,\nlimiting knowledge transfer between tasks. In this paper, we propose Online\nMeta-Learned adapters (OMLA). Instead of applying adapters directly, OMLA can\nfacilitate knowledge transfer from previously learned tasks to current learning\ntasks through a novel meta-learning objective. Extensive experiments in both\nsimulated and real-world environments demonstrate that OMLA can lead to better\nadaptation performances compared to the baseline methods. The project link:\nhttps://ricky-zhu.github.io/OMLA/.",
      "tldr_zh": "本研究提出了在线元学习适配器(OMLA)，用于提升预训练机器人策略的持续适应能力。与现有方法将任务学习分离不同，OMLA通过新颖的元学习目标，促进已学任务知识向当前学习任务的迁移。在模拟和真实环境中的大量实验表明，OMLA相比基线方法能实现更好的适应性能。该方法为通用自主代理的持续适应问题提供了有效解决方案。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Project link: https://ricky-zhu.github.io/OMLA/",
      "pdf_url": "http://arxiv.org/pdf/2503.18684v1",
      "published_date": "2025-03-24 13:55:47 UTC",
      "updated_date": "2025-03-24 13:55:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:01:12.872372"
    },
    {
      "arxiv_id": "2503.18681v2",
      "title": "Commander-GPT: Fully Unleashing the Sarcasm Detection Capability of Multi-Modal Large Language Models",
      "title_zh": "Commander-GPT：全面释放多模态大语言模型的讽刺检测能力",
      "authors": [
        "Yazhou Zhang",
        "Chunwang Zou",
        "Bo Wang",
        "Jing Qin"
      ],
      "abstract": "Sarcasm detection, as a crucial research direction in the field of Natural\nLanguage Processing (NLP), has attracted widespread attention. Traditional\nsarcasm detection tasks have typically focused on single-modal approaches\n(e.g., text), but due to the implicit and subtle nature of sarcasm, such\nmethods often fail to yield satisfactory results. In recent years, researchers\nhave shifted the focus of sarcasm detection to multi-modal approaches. However,\neffectively leveraging multi-modal information to accurately identify sarcastic\ncontent remains a challenge that warrants further exploration. Leveraging the\npowerful integrated processing capabilities of Multi-Modal Large Language\nModels (MLLMs) for various information sources, we propose an innovative\nmulti-modal Commander-GPT framework. Inspired by military strategy, we first\ndecompose the sarcasm detection task into six distinct sub-tasks. A central\ncommander (decision-maker) then assigns the best-suited large language model to\naddress each specific sub-task. Ultimately, the detection results from each\nmodel are aggregated to identify sarcasm. We conducted extensive experiments on\nMMSD and MMSD 2.0, utilizing four multi-modal large language models and six\nprompting strategies. Our experiments demonstrate that our approach achieves\nstate-of-the-art performance, with a 19.3% improvement in F1 score, without\nnecessitating fine-tuning or ground-truth rationales.",
      "tldr_zh": "该研究提出了Commander-GPT框架，旨在充分释放多模态大语言模型(MLLMs)在讽刺检测任务中的潜力。受军事策略启发，该框架将讽刺检测任务分解为六个子任务，并由中央指挥官（决策者）为每个子任务分配最合适的大语言模型进行解决，最终整合各模型的检测结果。实验表明，该框架在MMSD和MMSD 2.0数据集上实现了最先进的性能，F1分数提高了19.3%，且无需微调或真实推理。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18681v2",
      "published_date": "2025-03-24 13:53:00 UTC",
      "updated_date": "2025-03-25 04:33:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:01:23.717128"
    },
    {
      "arxiv_id": "2503.18673v2",
      "title": "Any6D: Model-free 6D Pose Estimation of Novel Objects",
      "title_zh": "Any6D：无需模型的未知物体6D姿态估计",
      "authors": [
        "Taeyeop Lee",
        "Bowen Wen",
        "Minjun Kang",
        "Gyuree Kang",
        "In So Kweon",
        "Kuk-Jin Yoon"
      ],
      "abstract": "We introduce Any6D, a model-free framework for 6D object pose estimation that\nrequires only a single RGB-D anchor image to estimate both the 6D pose and size\nof unknown objects in novel scenes. Unlike existing methods that rely on\ntextured 3D models or multiple viewpoints, Any6D leverages a joint object\nalignment process to enhance 2D-3D alignment and metric scale estimation for\nimproved pose accuracy. Our approach integrates a render-and-compare strategy\nto generate and refine pose hypotheses, enabling robust performance in\nscenarios with occlusions, non-overlapping views, diverse lighting conditions,\nand large cross-environment variations. We evaluate our method on five\nchallenging datasets: REAL275, Toyota-Light, HO3D, YCBINEOAT, and LM-O,\ndemonstrating its effectiveness in significantly outperforming state-of-the-art\nmethods for novel object pose estimation. Project page:\nhttps://taeyeop.com/any6d",
      "tldr_zh": "本研究提出了Any6D，一种无需模型即可进行6D物体姿态估计的框架，仅需单张RGB-D锚点图像即可估计新场景中未知物体的6D姿态和尺寸。该框架通过联合物体对齐过程增强2D-3D对齐和度量尺度估计，并采用渲染-比较策略生成和优化姿态假设，从而在遮挡、非重叠视角、多样化光照条件和跨环境变化等复杂场景中表现优异。实验表明，Any6D在五个具有挑战性的数据集上显著优于现有方法，为新颖物体姿态估计提供了高效解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2025, Project Page: https://taeyeop.com/any6d",
      "pdf_url": "http://arxiv.org/pdf/2503.18673v2",
      "published_date": "2025-03-24 13:46:21 UTC",
      "updated_date": "2025-03-25 06:18:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:01:24.088416"
    },
    {
      "arxiv_id": "2503.18666v1",
      "title": "AgentSpec: Customizable Runtime Enforcement for Safe and Reliable LLM Agents",
      "title_zh": "AgentSpec：面向安全可靠LLM智能体的可定制运行时强制执行机制",
      "authors": [
        "Haoyu Wang",
        "Christopher M. Poskitt",
        "Jun Sun"
      ],
      "abstract": "Agents built on LLMs are increasingly deployed across diverse domains,\nautomating complex decision-making and task execution. However, their autonomy\nintroduces safety risks, including security vulnerabilities, legal violations,\nand unintended harmful actions. Existing mitigation methods, such as\nmodel-based safeguards and early enforcement strategies, fall short in\nrobustness, interpretability, and adaptability. To address these challenges, we\npropose AgentSpec, a lightweight domain-specific language for specifying and\nenforcing runtime constraints on LLM agents. With AgentSpec, users define\nstructured rules that incorporate triggers, predicates, and enforcement\nmechanisms, ensuring agents operate within predefined safety boundaries. We\nimplement AgentSpec across multiple domains, including code execution, embodied\nagents, and autonomous driving, demonstrating its adaptability and\neffectiveness. Our evaluation shows that AgentSpec successfully prevents unsafe\nexecutions in over 90% of code agent cases, eliminates all hazardous actions in\nembodied agent tasks, and enforces 100% compliance by autonomous vehicles\n(AVs). Despite its strong safety guarantees, AgentSpec remains computationally\nlightweight, with overheads in milliseconds. By combining interpretability,\nmodularity, and efficiency, AgentSpec provides a practical and scalable\nsolution for enforcing LLM agent safety across diverse applications. We also\nautomate the generation of rules using LLMs and assess their effectiveness. Our\nevaluation shows that the rules generated by OpenAI o1 achieve a precision of\n95.56% and recall of 70.96% for embodied agents, successfully identifying\n87.26% of the risky code, and prevent AVs from breaking laws in 5 out of 8\nscenarios.",
      "tldr_zh": "该研究提出了AgentSpec，一种轻量级领域特定语言，用于在LLM（大语言模型）驱动的智能体运行时指定和执行安全约束。AgentSpec允许用户定义包含触发器、谓词和执行机制的结构化规则，确保智能体在预设的安全边界内运行。实验表明，AgentSpec在代码执行、具身智能体和自动驾驶等多个领域中表现优异，成功阻止了90%以上的不安全代码执行、消除了具身智能体的所有危险行为，并实现了自动驾驶的100%合规性。此外，AgentSpec的计算开销极低，为LLM智能体的安全部署提供了高效、可扩展的解决方案。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18666v1",
      "published_date": "2025-03-24 13:31:48 UTC",
      "updated_date": "2025-03-24 13:31:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:01:45.090840"
    },
    {
      "arxiv_id": "2503.18641v1",
      "title": "From Fragment to One Piece: A Survey on AI-Driven Graphic Design",
      "title_zh": "从片段到整体：人工智能驱动的图形设计研究综述",
      "authors": [
        "Xingxing Zou",
        "Wen Zhang",
        "Nanxuan Zhao"
      ],
      "abstract": "This survey provides a comprehensive overview of the advancements in\nArtificial Intelligence in Graphic Design (AIGD), focusing on integrating AI\ntechniques to support design interpretation and enhance the creative process.\nWe categorize the field into two primary directions: perception tasks, which\ninvolve understanding and analyzing design elements, and generation tasks,\nwhich focus on creating new design elements and layouts. The survey covers\nvarious subtasks, including visual element perception and generation, aesthetic\nand semantic understanding, layout analysis, and generation. We highlight the\nrole of large language models and multimodal approaches in bridging the gap\nbetween localized visual features and global design intent. Despite significant\nprogress, challenges remain to understanding human intent, ensuring\ninterpretability, and maintaining control over multilayered compositions. This\nsurvey serves as a guide for researchers, providing information on the current\nstate of AIGD and potential future\ndirections\\footnote{https://github.com/zhangtianer521/excellent\\_Intelligent\\_graphic\\_design}.",
      "tldr_zh": "本文综述了人工智能在图形设计（AIGD）领域的最新进展，重点关注AI技术在设计解读和创意过程中的应用。研究将AIGD分为两大方向：感知任务（理解与分析设计元素）和生成任务（创建新设计元素与布局）。文章涵盖了视觉元素感知与生成、美学与语义理解、布局分析与生成等子任务，并强调了大语言模型与多模态方法在连接局部视觉特征与全局设计意图中的作用。尽管取得了显著进展，但在理解人类意图、确保可解释性以及控制多层设计方面仍存在挑战。本综述为研究人员提供了AIGD当前发展状况及未来潜在方向的指南。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18641v1",
      "published_date": "2025-03-24 13:05:09 UTC",
      "updated_date": "2025-03-24 13:05:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:01:38.787775"
    },
    {
      "arxiv_id": "2503.18629v1",
      "title": "Towards Human-Understandable Multi-Dimensional Concept Discovery",
      "title_zh": "迈向人类可理解的多维概念发现",
      "authors": [
        "Arne Grobrügge",
        "Niklas Kühl",
        "Gerhard Satzger",
        "Philipp Spitzer"
      ],
      "abstract": "Concept-based eXplainable AI (C-XAI) aims to overcome the limitations of\ntraditional saliency maps by converting pixels into human-understandable\nconcepts that are consistent across an entire dataset. A crucial aspect of\nC-XAI is completeness, which measures how well a set of concepts explains a\nmodel's decisions. Among C-XAI methods, Multi-Dimensional Concept Discovery\n(MCD) effectively improves completeness by breaking down the CNN latent space\ninto distinct and interpretable concept subspaces. However, MCD's explanations\ncan be difficult for humans to understand, raising concerns about their\npractical utility. To address this, we propose Human-Understandable\nMulti-dimensional Concept Discovery (HU-MCD). HU-MCD uses the Segment Anything\nModel for concept identification and implements a CNN-specific input masking\ntechnique to reduce noise introduced by traditional masking methods. These\nchanges to MCD, paired with the completeness relation, enable HU-MCD to enhance\nconcept understandability while maintaining explanation faithfulness. Our\nexperiments, including human subject studies, show that HU-MCD provides more\nprecise and reliable explanations than existing C-XAI methods. The code is\navailable at https://github.com/grobruegge/hu-mcd.",
      "tldr_zh": "该研究提出了人类可理解的多维概念发现方法（HU-MCD），旨在改进基于概念的可解释人工智能（C-XAI）中的多维概念发现（MCD）方法。HU-MCD利用Segment Anything Model进行概念识别，并采用CNN特定的输入掩码技术，减少传统掩码方法引入的噪声。实验表明，HU-MCD在保持解释忠实度的同时，显著提高了概念的可理解性，提供了比现有C-XAI方法更精确和可靠的解释。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18629v1",
      "published_date": "2025-03-24 12:45:52 UTC",
      "updated_date": "2025-03-24 12:45:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:01:45.128120"
    },
    {
      "arxiv_id": "2503.18627v1",
      "title": "Dig2DIG: Dig into Diffusion Information Gains for Image Fusion",
      "title_zh": "Dig2DIG：深入探索图像融合中的扩散信息增益",
      "authors": [
        "Bing Cao",
        "Baoshuo Cai",
        "Changqing Zhang",
        "Qinghua Hu"
      ],
      "abstract": "Image fusion integrates complementary information from multi-source images to\ngenerate more informative results. Recently, the diffusion model, which\ndemonstrates unprecedented generative potential, has been explored in image\nfusion. However, these approaches typically incorporate predefined multimodal\nguidance into diffusion, failing to capture the dynamically changing\nsignificance of each modality, while lacking theoretical guarantees. To address\nthis issue, we reveal a significant spatio-temporal imbalance in image\ndenoising; specifically, the diffusion model produces dynamic information gains\nin different image regions with denoising steps. Based on this observation, we\nDig into the Diffusion Information Gains (Dig2DIG) and theoretically derive a\ndiffusion-based dynamic image fusion framework that provably reduces the upper\nbound of the generalization error. Accordingly, we introduce diffusion\ninformation gains (DIG) to quantify the information contribution of each\nmodality at different denoising steps, thereby providing dynamic guidance\nduring the fusion process. Extensive experiments on multiple fusion scenarios\nconfirm that our method outperforms existing diffusion-based approaches in\nterms of both fusion quality and inference efficiency.",
      "tldr_zh": "该研究提出了Dig2DIG框架，深入挖掘扩散模型(Diffusion Model)在图像融合中的动态信息增益(Diffusion Information Gains, DIG)。研究发现，扩散模型在去噪过程中会产生时空不平衡的信息增益，基于此，作者理论推导了一种动态图像融合框架，能够有效降低泛化误差的上界。该方法通过量化不同去噪步骤中各模态的信息贡献，为融合过程提供动态指导。实验表明，Dig2DIG在多场景图像融合任务中，在融合质量和推理效率上均优于现有基于扩散模型的方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18627v1",
      "published_date": "2025-03-24 12:43:11 UTC",
      "updated_date": "2025-03-24 12:43:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:01:54.732659"
    },
    {
      "arxiv_id": "2503.18612v1",
      "title": "Adventurer: Exploration with BiGAN for Deep Reinforcement Learning",
      "title_zh": "Adventurer：基于BiGAN的深度强化学习探索方法",
      "authors": [
        "Yongshuai Liu",
        "Xin Liu"
      ],
      "abstract": "Recent developments in deep reinforcement learning have been very successful\nin learning complex, previously intractable problems. Sample efficiency and\nlocal optimality, however, remain significant challenges. To address these\nchallenges, novelty-driven exploration strategies have emerged and shown\npromising potential. Unfortunately, no single algorithm outperforms all others\nin all tasks and most of them struggle with tasks with high-dimensional and\ncomplex observations. In this work, we propose Adventurer, a novelty-driven\nexploration algorithm that is based on Bidirectional Generative Adversarial\nNetworks (BiGAN), where BiGAN is trained to estimate state novelty.\nIntuitively, a generator that has been trained on the distribution of visited\nstates should only be able to generate a state coming from the distribution of\nvisited states. As a result, novel states using the generator to reconstruct\ninput states from certain latent representations would lead to larger\nreconstruction errors. We show that BiGAN performs well in estimating state\nnovelty for complex observations. This novelty estimation method can be\ncombined with intrinsic-reward-based exploration. Our empirical results show\nthat Adventurer produces competitive results on a range of popular benchmark\ntasks, including continuous robotic manipulation tasks (e.g. Mujoco robotics)\nand high-dimensional image-based tasks (e.g. Atari games).",
      "tldr_zh": "该研究提出了Adventurer算法，一种基于双向生成对抗网络(BiGAN)的新颖性驱动探索策略，用于解决深度强化学习中的样本效率和局部最优问题。通过训练BiGAN估计状态新颖性，利用生成器重构输入状态时的误差来识别新状态，并结合内在奖励机制进行探索。实验表明，Adventurer在连续机器人控制任务（如Mujoco）和高维图像任务（如Atari游戏）中表现出色，为复杂观测环境下的强化学习提供了有效的探索方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at Applied Intelligence",
      "pdf_url": "http://arxiv.org/pdf/2503.18612v1",
      "published_date": "2025-03-24 12:13:24 UTC",
      "updated_date": "2025-03-24 12:13:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:02:11.275018"
    },
    {
      "arxiv_id": "2503.18607v1",
      "title": "Reinforcement Learning in Switching Non-Stationary Markov Decision Processes: Algorithms and Convergence Analysis",
      "title_zh": "切换非平稳马尔可夫决策过程中的强化学习：算法与收敛性分析",
      "authors": [
        "Mohsen Amiri",
        "Sindri Magnússon"
      ],
      "abstract": "Reinforcement learning in non-stationary environments is challenging due to\nabrupt and unpredictable changes in dynamics, often causing traditional\nalgorithms to fail to converge. However, in many real-world cases,\nnon-stationarity has some structure that can be exploited to develop algorithms\nand facilitate theoretical analysis. We introduce one such structure, Switching\nNon-Stationary Markov Decision Processes (SNS-MDP), where environments switch\nover time based on an underlying Markov chain. Under a fixed policy, the value\nfunction of an SNS-MDP admits a closed-form solution determined by the Markov\nchain's statistical properties, and despite the inherent non-stationarity,\nTemporal Difference (TD) learning methods still converge to the correct value\nfunction. Furthermore, policy improvement can be performed, and it is shown\nthat policy iteration converges to the optimal policy. Moreover, since\nQ-learning converges to the optimal Q-function, it likewise yields the\ncorresponding optimal policy. To illustrate the practical advantages of\nSNS-MDPs, we present an example in communication networks where channel noise\nfollows a Markovian pattern, demonstrating how this framework can effectively\nguide decision-making in complex, time-varying contexts.",
      "tldr_zh": "该研究提出了一种针对切换非平稳马尔可夫决策过程(SNS-MDP)的强化学习框架，其中环境变化由底层马尔可夫链驱动。研究表明，在固定策略下，SNS-MDP的值函数具有闭式解，且时间差分(TD)学习、策略迭代和Q学习等方法均能收敛到最优解。通过通信网络中信道噪声的案例，验证了该框架在复杂时变环境中的有效性，为非平稳环境下的强化学习提供了理论支持和实用工具。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18607v1",
      "published_date": "2025-03-24 12:05:30 UTC",
      "updated_date": "2025-03-24 12:05:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:02:12.106738"
    },
    {
      "arxiv_id": "2503.18595v1",
      "title": "Adaptive Unimodal Regulation for Balanced Multimodal Information Acquisition",
      "title_zh": "自适应单模态调控实现平衡的多模态信息获取",
      "authors": [
        "Chengxiang Huang",
        "Yake Wei",
        "Zequn Yang",
        "Di Hu"
      ],
      "abstract": "Sensory training during the early ages is vital for human development.\nInspired by this cognitive phenomenon, we observe that the early training stage\nis also important for the multimodal learning process, where dataset\ninformation is rapidly acquired. We refer to this stage as the prime learning\nwindow. However, based on our observation, this prime learning window in\nmultimodal learning is often dominated by information-sufficient modalities,\nwhich in turn suppresses the information acquisition of\ninformation-insufficient modalities. To address this issue, we propose\nInformation Acquisition Regulation (InfoReg), a method designed to balance\ninformation acquisition among modalities. Specifically, InfoReg slows down the\ninformation acquisition process of information-sufficient modalities during the\nprime learning window, which could promote information acquisition of\ninformation-insufficient modalities. This regulation enables a more balanced\nlearning process and improves the overall performance of the multimodal\nnetwork. Experiments show that InfoReg outperforms related multimodal\nimbalanced methods across various datasets, achieving superior model\nperformance. The code is available at\nhttps://github.com/GeWu-Lab/InfoReg_CVPR2025.",
      "tldr_zh": "该研究提出了一种称为信息获取调节（InfoReg）的方法，旨在解决多模态学习中信息充足模态主导早期学习阶段、抑制信息不足模态获取的问题。通过减缓信息充足模态在关键学习窗口（prime learning window）中的信息获取速度，InfoReg促进了信息不足模态的学习，从而实现了更平衡的多模态学习过程。实验表明，该方法在多个数据集上优于现有的多模态不平衡处理方法，显著提升了模型性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "10pages, 16 figures, CVPR2025",
      "pdf_url": "http://arxiv.org/pdf/2503.18595v1",
      "published_date": "2025-03-24 11:52:57 UTC",
      "updated_date": "2025-03-24 11:52:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:02:24.227189"
    },
    {
      "arxiv_id": "2503.18594v1",
      "title": "ClinText-SP and RigoBERTa Clinical: a new set of open resources for Spanish Clinical NLP",
      "title_zh": "ClinText-SP 与 RigoBERTa Clinical：西班牙语临床自然语言处理的新开源资源集",
      "authors": [
        "Guillem García Subies",
        "Álvaro Barbero Jiménez",
        "Paloma Martínez Fernández"
      ],
      "abstract": "We present a novel contribution to Spanish clinical natural language\nprocessing by introducing the largest publicly available clinical corpus,\nClinText-SP, along with a state-of-the-art clinical encoder language model,\nRigoBERTa Clinical. Our corpus was meticulously curated from diverse open\nsources, including clinical cases from medical journals and annotated corpora\nfrom shared tasks, providing a rich and diverse dataset that was previously\ndifficult to access. RigoBERTa Clinical, developed through domain-adaptive\npretraining on this comprehensive dataset, significantly outperforms existing\nmodels on multiple clinical NLP benchmarks. By publicly releasing both the\ndataset and the model, we aim to empower the research community with robust\nresources that can drive further advancements in clinical NLP and ultimately\ncontribute to improved healthcare applications.",
      "tldr_zh": "该研究为西班牙语临床自然语言处理(NLP)领域提供了两项重要资源：ClinText-SP，这是目前最大的公开临床语料库，以及基于该语料库训练的先进临床编码语言模型RigoBERTa Clinical。ClinText-SP通过精心整合多种公开来源（如医学期刊的临床案例和共享任务的标注语料）构建，提供了丰富且多样化的数据集。RigoBERTa Clinical通过对该数据集进行领域自适应预训练，在多项临床NLP基准测试中显著优于现有模型。这些资源的公开发布旨在为研究社区提供强有力的工具，推动临床NLP的进一步发展，最终助力医疗应用的改进。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18594v1",
      "published_date": "2025-03-24 11:52:17 UTC",
      "updated_date": "2025-03-24 11:52:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:03:22.817256"
    },
    {
      "arxiv_id": "2503.18592v1",
      "title": "The Role of Artificial Intelligence in Enhancing Insulin Recommendations and Therapy Outcomes",
      "title_zh": "人工智能在提升胰岛素推荐与治疗效果中的作用",
      "authors": [
        "Maria Panagiotou",
        "Knut Stroemmen",
        "Lorenzo Brigato",
        "Bastiaan E. de Galan",
        "Stavroula Mougiakakou"
      ],
      "abstract": "The growing worldwide incidence of diabetes requires more effective\napproaches for managing blood glucose levels. Insulin delivery systems have\nadvanced significantly, with artificial intelligence (AI) playing a key role in\nimproving their precision and adaptability. AI algorithms, particularly those\nbased on reinforcement learning, allow for personalised insulin dosing by\ncontinuously adapting to an individual's responses. Despite these advancements,\nchallenges such as data privacy, algorithm transparency, and accessibility\nstill need to be addressed. Continued progress and validation in AI-driven\ninsulin delivery systems promise to improve therapy outcomes further, offering\npeople more effective and individualised management of their diabetes. This\npaper presents an overview of current strategies, key challenges, and future\ndirections.",
      "tldr_zh": "本文探讨了人工智能（AI）在优化胰岛素推荐和改善糖尿病治疗效果中的作用。基于强化学习的AI算法通过持续适应用户的个体反应，实现了个性化胰岛素剂量调整，显著提升了血糖管理的精确性和适应性。尽管在数据隐私、算法透明性和可及性方面仍存在挑战，但AI驱动的胰岛素输送系统的持续发展和验证有望进一步改善治疗效果，为糖尿病患者提供更有效的个性化管理方案。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "physics.med-ph"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18592v1",
      "published_date": "2025-03-24 11:50:14 UTC",
      "updated_date": "2025-03-24 11:50:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:03:04.580889"
    },
    {
      "arxiv_id": "2503.18578v1",
      "title": "Galaxy Walker: Geometry-aware VLMs For Galaxy-scale Understanding",
      "title_zh": "Galaxy Walker：面向星系尺度理解的几何感知视觉语言模型",
      "authors": [
        "Tianyu Chen",
        "Xingcheng Fu",
        "Yisen Gao",
        "Haodong Qian",
        "Yuecen Wei",
        "Kun Yan",
        "Haoyi Zhou",
        "Jianxin Li"
      ],
      "abstract": "Modern vision-language models (VLMs) develop patch embedding and convolution\nbackbone within vector space, especially Euclidean ones, at the very founding.\nWhen expanding VLMs to a galaxy scale for understanding astronomical phenomena,\nthe integration of spherical space for planetary orbits and hyperbolic spaces\nfor black holes raises two formidable challenges. a) The current pre-training\nmodel is confined to Euclidean space rather than a comprehensive geometric\nembedding. b) The predominant architecture lacks suitable backbones for\nanisotropic physical geometries. In this paper, we introduced Galaxy-Walker, a\ngeometry-aware VLM, for the universe-level vision understanding tasks. We\nproposed the geometry prompt that generates geometry tokens by random walks\nacross diverse spaces on a multi-scale physical graph, along with a geometry\nadapter that compresses and reshapes the space anisotropy in a\nmixture-of-experts manner. Extensive experiments demonstrate the effectiveness\nof our approach, with Galaxy-Walker achieving state-of-the-art performance in\nboth galaxy property estimation ($R^2$ scores up to $0.91$) and morphology\nclassification tasks (up to $+0.17$ F1 improvement in challenging features),\nsignificantly outperforming both domain-specific models and general-purpose\nVLMs.",
      "tldr_zh": "该研究提出了Galaxy-Walker，一种几何感知的视觉语言模型(VLM)，旨在解决天文现象理解中的几何空间挑战。通过设计几何提示(geometry prompt)和几何适配器(geometry adapter)，模型能够在多尺度物理图中跨不同空间（如球面和双曲空间）生成几何令牌，并以专家混合方式压缩和重塑空间各向异性。实验表明，Galaxy-Walker在星系属性估计和形态分类任务中表现优异，显著超越了领域专用模型和通用VLM，为宇宙级视觉理解提供了新方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18578v1",
      "published_date": "2025-03-24 11:35:56 UTC",
      "updated_date": "2025-03-24 11:35:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:02:56.926012"
    },
    {
      "arxiv_id": "2503.18572v1",
      "title": "Identifying and Characterising Higher Order Interactions in Mobility Networks Using Hypergraphs",
      "title_zh": "利用超图识别和表征移动网络中的高阶交互",
      "authors": [
        "Prathyush Sambaturu",
        "Bernardo Gutierrez",
        "Moritz U. G. Kraemer"
      ],
      "abstract": "Understanding human mobility is essential for applications ranging from urban\nplanning to public health. Traditional mobility models such as flow networks\nand colocation matrices capture only pairwise interactions between discrete\nlocations, overlooking higher-order relationships among locations (i.e.,\nmobility flow among two or more locations). To address this, we propose\nco-visitation hypergraphs, a model that leverages temporal observation windows\nto extract group interactions between locations from individual mobility\ntrajectory data. Using frequent pattern mining, our approach constructs\nhypergraphs that capture dynamic mobility behaviors across different spatial\nand temporal scales. We validate our method on a publicly available mobility\ndataset and demonstrate its effectiveness in analyzing city-scale mobility\npatterns, detecting shifts during external disruptions such as extreme weather\nevents, and examining how a location's connectivity (degree) relates to the\nnumber of points of interest (POIs) within it. Our results demonstrate that our\nhypergraph-based mobility analysis framework is a valuable tool with potential\napplications in diverse fields such as public health, disaster resilience, and\nurban planning.",
      "tldr_zh": "该研究提出了一种基于超图（hypergraphs）的共访问模型，用于捕捉人类移动行为中的高阶交互关系，弥补了传统流网络和共定位矩阵只能处理成对交互的局限性。通过频繁模式挖掘技术，该模型从个体移动轨迹数据中提取动态的群体移动行为，并在不同时空尺度上构建超图。实验表明，该方法能够有效分析城市规模移动模式、检测极端天气等外部干扰下的变化，并揭示地点连通性与兴趣点（POIs）数量之间的关系，为公共卫生、灾害韧性和城市规划等领域提供了新的分析工具。",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.DB",
        "cs.DM",
        "math.CO"
      ],
      "primary_category": "cs.SI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18572v1",
      "published_date": "2025-03-24 11:29:06 UTC",
      "updated_date": "2025-03-24 11:29:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:03:07.156751"
    },
    {
      "arxiv_id": "2503.18569v1",
      "title": "Anchor-based oversampling for imbalanced tabular data via contrastive and adversarial learning",
      "title_zh": "基于对比与对抗学习的锚点过采样方法处理不平衡表格数据",
      "authors": [
        "Hadi Mohammadi",
        "Ehsan Nazerfard",
        "Mostafa Haghir Chehreghani"
      ],
      "abstract": "Imbalanced data represent a distribution with more frequencies of one class\n(majority) than the other (minority). This phenomenon occurs across various\ndomains, such as security, medical care and human activity. In imbalanced\nlearning, classification algorithms are typically inclined to classify the\nmajority class accurately, resulting in artificially high accuracy rates. As a\nresult, many minority samples are mistakenly labelled as majority-class\ninstances, resulting in a bias that benefits the majority class. This study\npresents a framework based on boundary anchor samples to tackle the imbalance\nlearning challenge. First, we select and use anchor samples to train a\nmultilayer perceptron (MLP) classifier, which acts as a prior knowledge model\nand aids the adversarial and contrastive learning procedures. Then, we designed\na novel deep generative model called Anchor Stabilized Conditional Generative\nAdversarial Network or Anch-SCGAN in short. Anch-SCGAN is supported with two\ngenerators for the minority and majority classes and a discriminator\nincorporating additional class-specific information from the pre-trained\nfeature extractor MLP. In addition, we facilitate the generator's training\nprocedure in two ways. First, we define a new generator loss function based on\nreprocessed anchor samples and contrastive learning. Second, we apply a scoring\nstrategy to stabilize the adversarial training part in generators. We train\nAnch-SCGAN and further finetune it with anchor samples to improve the precision\nof the generated samples. Our experiments on 16 real-world imbalanced datasets\nillustrate that Anch-SCGAN outperforms the renowned methods in imbalanced\nlearning.",
      "tldr_zh": "本研究提出了一种基于边界锚点样本的框架，通过对比学习和对抗学习解决不平衡表格数据分类问题。该框架首先利用锚点样本训练多层感知器（MLP）分类器作为先验知识模型，随后设计了一种新型深度生成模型Anch-SCGAN，包含针对少数类和多数类的两个生成器以及一个结合预训练MLP特征的判别器。通过重新处理锚点样本和对比学习定义新的生成器损失函数，并采用评分策略稳定对抗训练，最终在16个真实世界不平衡数据集上实验表明，Anch-SCGAN显著优于现有方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18569v1",
      "published_date": "2025-03-24 11:25:21 UTC",
      "updated_date": "2025-03-24 11:25:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:03:07.107701"
    },
    {
      "arxiv_id": "2503.18565v1",
      "title": "Distil-xLSTM: Learning Attention Mechanisms through Recurrent Structures",
      "title_zh": "Distil-xLSTM：通过循环结构学习注意力机制",
      "authors": [
        "Abdoul Majid O. Thiombiano",
        "Brahim Hnich",
        "Ali Ben Mrad",
        "Mohamed Wiem Mkaouer"
      ],
      "abstract": "The current era of Natural Language Processing (NLP) is dominated by\nTransformer models. However, novel architectures relying on recurrent\nmechanisms, such as xLSTM and Mamba, have been proposed as alternatives to\nattention-based models. Although computation is done differently than with the\nattention mechanism mechanism, these recurrent models yield good results and\nsometimes even outperform state-of-the-art attention-based models. In this\nwork, we propose Distil-xLSTM, an xLSTM-based Small Language Model (SLM)\ntrained by distilling knowledge from a Large Language Model (LLM) that shows\npromising results while being compute and scale efficient. Our Distil-xLSTM\nfocuses on approximating a transformer-based model attention parametrization\nusing its recurrent sequence mixing components and shows good results with\nminimal training.",
      "tldr_zh": "该研究提出了Distil-xLSTM，一种基于xLSTM的小型语言模型(SLM)，通过从大型语言模型(LLM)中蒸馏知识来学习注意力机制。Distil-xLSTM利用其循环序列混合组件近似Transformer模型的注意力参数化，在计算和规模效率上表现出色。实验表明，该模型在最小化训练的情况下取得了良好的效果，为基于循环机制的模型提供了新的研究方向。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18565v1",
      "published_date": "2025-03-24 11:18:25 UTC",
      "updated_date": "2025-03-24 11:18:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:03:13.200889"
    },
    {
      "arxiv_id": "2503.18562v1",
      "title": "Self-Reported Confidence of Large Language Models in Gastroenterology: Analysis of Commercial, Open-Source, and Quantized Models",
      "title_zh": "大型语言模型在胃肠病学中的自我报告信心分析：商业、开源与量化模型的比较",
      "authors": [
        "Nariman Naderi",
        "Seyed Amir Ahmad Safavi-Naini",
        "Thomas Savage",
        "Zahra Atf",
        "Peter Lewis",
        "Girish Nadkarni",
        "Ali Soroush"
      ],
      "abstract": "This study evaluated self-reported response certainty across several large\nlanguage models (GPT, Claude, Llama, Phi, Mistral, Gemini, Gemma, and Qwen)\nusing 300 gastroenterology board-style questions. The highest-performing models\n(GPT-o1 preview, GPT-4o, and Claude-3.5-Sonnet) achieved Brier scores of\n0.15-0.2 and AUROC of 0.6. Although newer models demonstrated improved\nperformance, all exhibited a consistent tendency towards overconfidence.\nUncertainty estimation presents a significant challenge to the safe use of LLMs\nin healthcare. Keywords: Large Language Models; Confidence Elicitation;\nArtificial Intelligence; Gastroenterology; Uncertainty Quantification",
      "tldr_zh": "本研究评估了多款大型语言模型（如GPT、Claude、Llama等）在回答300道胃肠病学考试问题时的自我报告信心水平。表现最佳的模型（GPT-o1预览版、GPT-4o和Claude-3.5-Sonnet）取得了0.15-0.2的Brier分数和0.6的AUROC值。尽管新模型表现有所提升，但所有模型均表现出过度自信的倾向，凸显了不确定性估计在医疗领域安全应用中的重大挑战。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "35 pages, 5 figures, 1 table, 7 supplementary figures",
      "pdf_url": "http://arxiv.org/pdf/2503.18562v1",
      "published_date": "2025-03-24 11:16:41 UTC",
      "updated_date": "2025-03-24 11:16:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:03:20.990265"
    },
    {
      "arxiv_id": "2503.18552v1",
      "title": "EvAnimate: Event-conditioned Image-to-Video Generation for Human Animation",
      "title_zh": "EvAnimate：基于事件条件的人体动画图像到视频生成",
      "authors": [
        "Qiang Qu",
        "Ming Li",
        "Xiaoming Chen",
        "Tongliang Liu"
      ],
      "abstract": "Conditional human animation transforms a static reference image into a\ndynamic sequence by applying motion cues such as poses. These motion cues are\ntypically derived from video data but are susceptible to limitations including\nlow temporal resolution, motion blur, overexposure, and inaccuracies under\nlow-light conditions. In contrast, event cameras provide data streams with\nexceptionally high temporal resolution, a wide dynamic range, and inherent\nresistance to motion blur and exposure issues. In this work, we propose\nEvAnimate, a framework that leverages event streams as motion cues to animate\nstatic human images. Our approach employs a specialized event representation\nthat transforms asynchronous event streams into 3-channel slices with\ncontrollable slicing rates and appropriate slice density, ensuring\ncompatibility with diffusion models. Subsequently, a dual-branch architecture\ngenerates high-quality videos by harnessing the inherent motion dynamics of the\nevent streams, thereby enhancing both video quality and temporal consistency.\nSpecialized data augmentation strategies further enhance cross-person\ngeneralization. Finally, we establish a new benchmarking, including simulated\nevent data for training and validation, and a real-world event dataset\ncapturing human actions under normal and extreme scenarios. The experiment\nresults demonstrate that EvAnimate achieves high temporal fidelity and robust\nperformance in scenarios where traditional video-derived cues fall short.",
      "tldr_zh": "该研究提出了EvAnimate，一种基于事件相机数据的事件条件图像到视频生成框架，用于人体动画。与传统的视频数据相比，事件相机提供了更高时间分辨率、更宽动态范围且抗运动模糊的数据流。EvAnimate通过将异步事件流转换为兼容扩散模型的3通道切片，并利用双分支架构生成高质量视频，显著提升了视频质量和时间一致性。实验结果表明，EvAnimate在传统视频数据难以处理的场景下表现出高时间保真度和鲁棒性，并为跨人物泛化提供了新的数据增强策略。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18552v1",
      "published_date": "2025-03-24 11:05:41 UTC",
      "updated_date": "2025-03-24 11:05:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:03:32.445305"
    },
    {
      "arxiv_id": "2503.18551v1",
      "title": "Discriminative protein sequence modelling with Latent Space Diffusion",
      "title_zh": "基于潜在空间扩散的判别性蛋白质序列建模",
      "authors": [
        "Eoin Quinn",
        "Ghassene Jebali",
        "Maxime Seince",
        "Oliver Bent"
      ],
      "abstract": "We explore a framework for protein sequence representation learning that\ndecomposes the task between manifold learning and distributional modelling.\nSpecifically we present a Latent Space Diffusion architecture which combines a\nprotein sequence autoencoder with a denoising diffusion model operating on its\nlatent space. We obtain a one-parameter family of learned representations from\nthe diffusion model, along with the autoencoder's latent representation. We\npropose and evaluate two autoencoder architectures: a homogeneous model forcing\namino acids of the same type to be identically distributed in the latent space,\nand an inhomogeneous model employing a noise-based variant of masking. As a\nbaseline we take a latent space learned by masked language modelling, and\nevaluate discriminative capability on a range of protein property prediction\ntasks. Our finding is twofold: the diffusion models trained on both our\nproposed variants display higher discriminative power than the one trained on\nthe masked language model baseline, none of the diffusion representations\nachieve the performance of the masked language model embeddings themselves.",
      "tldr_zh": "该研究提出了一种基于Latent Space Diffusion的蛋白质序列表示学习框架，通过结合蛋白质序列自编码器和潜空间降噪扩散模型，生成一系列学习表示。研究提出了两种自编码器架构：同质模型和异质模型，并评估了它们在蛋白质性质预测任务中的判别能力。实验结果表明，基于该框架的扩散模型在判别能力上优于基于掩码语言建模的基线模型，但其性能仍不及掩码语言模型嵌入本身。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18551v1",
      "published_date": "2025-03-24 11:03:57 UTC",
      "updated_date": "2025-03-24 11:03:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:03:36.758281"
    },
    {
      "arxiv_id": "2503.18549v1",
      "title": "RLCAD: Reinforcement Learning Training Gym for Revolution Involved CAD Command Sequence Generation",
      "title_zh": "RLCAD：面向旋转操作CAD命令序列生成的强化学习训练环境",
      "authors": [
        "Xiaolong Yin",
        "Xingyu Lu",
        "Jiahang Shen",
        "Jingzhe Ni",
        "Hailong Li",
        "Ruofeng Tong",
        "Min Tang",
        "Peng Du"
      ],
      "abstract": "A CAD command sequence is a typical parametric design paradigm in 3D CAD\nsystems where a model is constructed by overlaying 2D sketches with operations\nsuch as extrusion, revolution, and Boolean operations. Although there is\ngrowing academic interest in the automatic generation of command sequences,\nexisting methods and datasets only support operations such as 2D sketching,\nextrusion,and Boolean operations. This limitation makes it challenging to\nrepresent more complex geometries. In this paper, we present a reinforcement\nlearning (RL) training environment (gym) built on a CAD geometric engine. Given\nan input boundary representation (B-Rep) geometry, the policy network in the RL\nalgorithm generates an action. This action, along with previously generated\nactions, is processed within the gym to produce the corresponding CAD geometry,\nwhich is then fed back into the policy network. The rewards, determined by the\ndifference between the generated and target geometries within the gym, are used\nto update the RL network. Our method supports operations beyond sketches,\nBoolean, and extrusion, including revolution operations. With this training\ngym, we achieve state-of-the-art (SOTA) quality in generating command sequences\nfrom B-Rep geometries. In addition, our method can significantly improve the\nefficiency of command sequence generation by a factor of 39X compared with the\nprevious training gym.",
      "tldr_zh": "该研究提出了RLCAD，一种基于强化学习(RL)的CAD命令序列生成训练环境，专注于支持旋转(revolve)等复杂操作。通过构建在CAD几何引擎上的RL训练框架，该方法能够根据输入的边界表示(B-Rep)几何生成命令序列，并通过与目标几何的差异计算奖励以更新网络。实验表明，该方法在生成命令序列的质量上达到了SOTA水平，并将生成效率提升了39倍，为复杂几何体的自动建模提供了新思路。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18549v1",
      "published_date": "2025-03-24 11:01:05 UTC",
      "updated_date": "2025-03-24 11:01:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:03:47.433668"
    },
    {
      "arxiv_id": "2503.18542v1",
      "title": "An Identity and Interaction Based Network Forensic Analysis",
      "title_zh": "基于身份与交互的网络取证分析",
      "authors": [
        "Nathan Clarke",
        "Gaseb Alotibi",
        "Dany Joy",
        "Fudong Li",
        "Steven Furnell",
        "Ali Alshumrani",
        "Hussan Mohammed"
      ],
      "abstract": "In todays landscape of increasing electronic crime, network forensics plays a\npivotal role in digital investigations. It aids in understanding which systems\nto analyse and as a supplement to support evidence found through more\ntraditional computer based investigations. However, the nature and\nfunctionality of the existing Network Forensic Analysis Tools (NFATs) fall\nshort compared to File System Forensic Analysis Tools (FS FATs) in providing\nusable data. The analysis tends to focus upon IP addresses, which are not\nsynonymous with user identities, a point of significant interest to\ninvestigators. This paper presents several experiments designed to create a\nnovel NFAT approach that can identify users and understand how they are using\nnetwork based applications whilst the traffic remains encrypted. The\nexperiments build upon the prior art and investigate how effective this\napproach is in classifying users and their actions. Utilising an in-house\ndataset composed of 50 million packers, the experiments are formed of three\nincremental developments that assist in improving performance. Building upon\nthe successful experiments, a proposed NFAT interface is presented to\nillustrate the ease at which investigators would be able to ask relevant\nquestions of user interactions. The experiments profiled across 27 users, has\nyielded an average 93.3% True Positive Identification Rate (TPIR), with 41% of\nusers experiencing 100% TPIR. Skype, Wikipedia and Hotmail services achieved a\nnotably high level of recognition performance. The study has developed and\nevaluated an approach to analyse encrypted network traffic more effectively\nthrough the modelling of network traffic and to subsequently visualise these\ninteractions through a novel network forensic analysis tool.",
      "tldr_zh": "本研究提出了一种基于身份和交互的新型网络取证分析方法，旨在解决现有网络取证分析工具(NFATs)在识别用户身份和解析加密网络流量方面的不足。通过设计三阶段增量实验，利用包含5000万个数据包的内部数据集，该方法成功实现了对27名用户的平均93.3%真阳性识别率(TPIR)，其中41%的用户达到100% TPIR。研究还开发了一个NFAT界面原型，可直观展示用户交互，为Skype、Wikipedia和Hotmail等服务提供了高效的识别性能，显著提升了加密网络流量的分析效果。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18542v1",
      "published_date": "2025-03-24 10:52:23 UTC",
      "updated_date": "2025-03-24 10:52:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:03:56.813993"
    },
    {
      "arxiv_id": "2503.18541v1",
      "title": "UniPCGC: Towards Practical Point Cloud Geometry Compression via an Efficient Unified Approach",
      "title_zh": "UniPCGC：通过高效统一方法实现实用点云几何压缩",
      "authors": [
        "Kangli Wang",
        "Wei Gao"
      ],
      "abstract": "Learning-based point cloud compression methods have made significant progress\nin terms of performance. However, these methods still encounter challenges\nincluding high complexity, limited compression modes, and a lack of support for\nvariable rate, which restrict the practical application of these methods. In\norder to promote the development of practical point cloud compression, we\npropose an efficient unified point cloud geometry compression framework, dubbed\nas UniPCGC. It is a lightweight framework that supports lossy compression,\nlossless compression, variable rate and variable complexity. First, we\nintroduce the Uneven 8-Stage Lossless Coder (UELC) in the lossless mode, which\nallocates more computational complexity to groups with higher coding\ndifficulty, and merges groups with lower coding difficulty. Second, Variable\nRate and Complexity Module (VRCM) is achieved in the lossy mode through joint\nadoption of a rate modulation module and dynamic sparse convolution. Finally,\nthrough the dynamic combination of UELC and VRCM, we achieve lossy compression,\nlossless compression, variable rate and complexity within a unified framework.\nCompared to the previous state-of-the-art method, our method achieves a\ncompression ratio (CR) gain of 8.1\\% on lossless compression, and a Bjontegaard\nDelta Rate (BD-Rate) gain of 14.02\\% on lossy compression, while also\nsupporting variable rate and variable complexity.",
      "tldr_zh": "本研究提出了UniPCGC，一种高效统一的点云几何压缩框架，旨在解决现有基于学习的点云压缩方法在复杂度高、压缩模式有限以及缺乏可变速率支持等方面的不足。UniPCGC通过引入Uneven 8-Stage Lossless Coder（UELC）和Variable Rate and Complexity Module（VRCM），在单一框架内实现了有损压缩、无损压缩、可变速率和可变复杂度。实验表明，该框架在无损压缩上实现了8.1%的压缩比（CR）提升，在有损压缩上达到了14.02%的BD-Rate增益，同时支持可变速率和复杂度，为点云压缩的实际应用提供了重要进展。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.18541v1",
      "published_date": "2025-03-24 10:51:28 UTC",
      "updated_date": "2025-03-24 10:51:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:04:05.493917"
    },
    {
      "arxiv_id": "2503.18540v1",
      "title": "HiRes-FusedMIM: A High-Resolution RGB-DSM Pre-trained Model for Building-Level Remote Sensing Applications",
      "title_zh": "HiRes-FusedMIM：面向建筑级遥感应用的高分辨率RGB-DSM预训练模型",
      "authors": [
        "Guneet Mutreja",
        "Philipp Schuegraf",
        "Ksenia Bittner"
      ],
      "abstract": "Recent advances in self-supervised learning have led to the development of\nfoundation models that have significantly advanced performance in various\ncomputer vision tasks. However, despite their potential, these models often\noverlook the crucial role of high-resolution digital surface models (DSMs) in\nunderstanding urban environments, particularly for building-level analysis,\nwhich is essential for applications like digital twins. To address this gap, we\nintroduce HiRes-FusedMIM, a novel pre-trained model specifically designed to\nleverage the rich information contained within high-resolution RGB and DSM\ndata. HiRes-FusedMIM utilizes a dual-encoder simple masked image modeling\n(SimMIM) architecture with a multi-objective loss function that combines\nreconstruction and contrastive objectives, enabling it to learn powerful, joint\nrepresentations from both modalities. We conducted a comprehensive evaluation\nof HiRes-FusedMIM on a diverse set of downstream tasks, including\nclassification, semantic segmentation, and instance segmentation. Our results\ndemonstrate that: 1) HiRes-FusedMIM outperforms previous state-of-the-art\ngeospatial methods on several building-related datasets, including WHU Aerial\nand LoveDA, demonstrating its effectiveness in capturing and leveraging\nfine-grained building information; 2) Incorporating DSMs during pre-training\nconsistently improves performance compared to using RGB data alone,\nhighlighting the value of elevation information for building-level analysis; 3)\nThe dual-encoder architecture of HiRes-FusedMIM, with separate encoders for RGB\nand DSM data, significantly outperforms a single-encoder model on the Vaihingen\nsegmentation task, indicating the benefits of learning specialized\nrepresentations for each modality. To facilitate further research and\napplications in this direction, we will publicly release the trained model\nweights.",
      "tldr_zh": "该研究提出了HiRes-FusedMIM，一种专为高分辨率RGB和数字表面模型(DSM)数据设计的预训练模型，用于建筑级别的遥感应用。该模型采用双编码器架构和简单掩码图像建模(SimMIM)技术，结合重建和对比学习目标，能够从RGB和DSM数据中学习强大的联合表征。实验表明，HiRes-FusedMIM在多个建筑相关数据集上优于现有最先进的地理空间方法，并验证了DSM数据对建筑级别分析的价值。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18540v1",
      "published_date": "2025-03-24 10:49:55 UTC",
      "updated_date": "2025-03-24 10:49:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:04:14.602137"
    },
    {
      "arxiv_id": "2503.18539v1",
      "title": "Natural Language Processing for Electronic Health Records in Scandinavian Languages: Norwegian, Swedish, and Danish",
      "title_zh": "斯堪的纳维亚语言电子健康记录的自然语言处理：挪威语、瑞典语和丹麦语",
      "authors": [
        "Ashenafi Zebene Woldaregay",
        "Jørgen Aarmo Lund",
        "Phuong Dinh Ngo",
        "Mariyam Tayefi",
        "Joel Burman",
        "Stine Hansen",
        "Martin Hylleholt Sillesen",
        "Hercules Dalianis",
        "Robert Jenssen",
        "Lindsetmo Rolf Ole",
        "Karl Øyvind Mikalsen"
      ],
      "abstract": "Background: Clinical natural language processing (NLP) refers to the use of\ncomputational methods for extracting, processing, and analyzing unstructured\nclinical text data, and holds a huge potential to transform healthcare in\nvarious clinical tasks. Objective: The study aims to perform a systematic\nreview to comprehensively assess and analyze the state-of-the-art NLP methods\nfor the mainland Scandinavian clinical text. Method: A literature search was\nconducted in various online databases including PubMed, ScienceDirect, Google\nScholar, ACM digital library, and IEEE Xplore between December 2022 and\nFebruary 2024. Further, relevant references to the included articles were also\nused to solidify our search. The final pool includes articles that conducted\nclinical NLP in the mainland Scandinavian languages and were published in\nEnglish between 2010 and 2024. Results: Out of the 113 articles, 18% (n=21)\nfocus on Norwegian clinical text, 64% (n=72) on Swedish, 10% (n=11) on Danish,\nand 8% (n=9) focus on more than one language. Generally, the review identified\npositive developments across the region despite some observable gaps and\ndisparities between the languages. There are substantial disparities in the\nlevel of adoption of transformer-based models. In essential tasks such as\nde-identification, there is significantly less research activity focusing on\nNorwegian and Danish compared to Swedish text. Further, the review identified a\nlow level of sharing resources such as data, experimentation code, pre-trained\nmodels, and rate of adaptation and transfer learning in the region. Conclusion:\nThe review presented a comprehensive assessment of the state-of-the-art\nClinical NLP for electronic health records (EHR) text in mainland Scandinavian\nlanguages and, highlighted the potential barriers and challenges that hinder\nthe rapid advancement of the field in the region.",
      "tldr_zh": "该研究系统综述了斯堪的纳维亚大陆语言（挪威语、瑞典语和丹麦语）在电子健康记录（EHR）中的自然语言处理（NLP）应用现状。通过对2010至2024年间相关文献的分析，研究发现瑞典语在临床NLP研究中的占比最高（64%），而挪威语和丹麦语的研究相对较少。尽管该地区在NLP领域取得了一定进展，但在Transformer模型的采用、去识别化任务以及资源共享（如数据、代码和预训练模型）方面仍存在显著差距。研究强调了阻碍该领域快速发展的潜在挑战。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "45 pages including the appendix, 9 figures in the main manuscript and\n  11 figures in the Appendix",
      "pdf_url": "http://arxiv.org/pdf/2503.18539v1",
      "published_date": "2025-03-24 10:47:32 UTC",
      "updated_date": "2025-03-24 10:47:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:04:27.183259"
    },
    {
      "arxiv_id": "2503.18533v1",
      "title": "MMCR: Advancing Visual Language Model in Multimodal Multi-Turn Contextual Reasoning",
      "title_zh": "MMCR：推动视觉语言模型在多模态多轮上下文推理中的发展",
      "authors": [
        "Dawei Yan",
        "Yang Li",
        "Qing-Guo Chen",
        "Weihua Luo",
        "Peng Wang",
        "Haokui Zhang",
        "Chunhua Shen"
      ],
      "abstract": "Compared to single-turn dialogue, multi-turn dialogue involving multiple\nimages better aligns with the needs of real-world human-AI interactions.\nAdditionally, as training data, it provides richer contextual reasoning\ninformation, thereby guiding the model to achieve better performance. However,\nexisting vision-language models (VLMs) primarily rely on single-turn dialogue\ntraining and evaluation benchmarks. In this paper, following the\ncharacteristics of human dialogue, such as focused topics and concise, clear\ncontent, we present MMCR (Multimodal Multi-turn Contextual Reasoning), a novel\ndataset comprising: (1) MMCR-310k -- the largest multi-image multi-turn\ninstruction tuning dataset with 310K contextual dialogues, each covering 1-4\nimages and 4 or 8 dialogue turns; and (2) MMCR-Bench -- a diagnostic benchmark\nfeaturing dialogues, spanning 8 domains (Humanities, Natural, Science,\nEducation, etc.) and 40 sub-topics. Extensive evaluations demonstrate that\nmodels fine-tuned with MMCR-310k achieve 5.2\\% higher contextual accuracy on\nMMCR-Bench, while showing consistent improvements on existing benchmarks\n(+1.1\\% on AI2D, +1.2\\% on MMMU and MMVet). MMCR and prompt engineering will be\nreleased publicly.",
      "tldr_zh": "该研究提出了MMCR（多模态多轮上下文推理）数据集，旨在提升视觉语言模型（VLMs）在多轮对话中的表现。MMCR包括两个部分：MMCR-310k，一个包含31万条多轮对话的指令调优数据集，涵盖1-4张图片和4或8轮对话；以及MMCR-Bench，一个跨8个领域和40个子主题的诊断基准。实验表明，使用MMCR-310k微调的模型在MMCR-Bench上的上下文准确率提高了5.2%，并在现有基准测试中表现出一致性提升（AI2D +1.1%，MMMU +1.2%，MMVet +1.2%）。该数据集和提示工程将公开发布。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18533v1",
      "published_date": "2025-03-24 10:40:33 UTC",
      "updated_date": "2025-03-24 10:40:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:04:32.740245"
    },
    {
      "arxiv_id": "2503.18526v1",
      "title": "SciClaims: An End-to-End Generative System for Biomedical Claim Analysis",
      "title_zh": "SciClaims：生物医学声明分析的端到端生成系统",
      "authors": [
        "Raúl Ortega",
        "José Manuel Gómez-Pérez"
      ],
      "abstract": "Validating key claims in scientific literature, particularly in biomedical\nresearch, is essential for ensuring accuracy and advancing knowledge. This\nprocess is critical in sectors like the pharmaceutical industry, where rapid\nscientific progress requires automation and deep domain expertise. However,\ncurrent solutions have significant limitations. They lack end-to-end pipelines\nencompassing all claim extraction, evidence retrieval, and verification steps;\nrely on complex NLP and information retrieval pipelines prone to multiple\nfailure points; and often fail to provide clear, user-friendly justifications\nfor claim verification outcomes. To address these challenges, we introduce\nSciClaims, an advanced system powered by state-of-the-art large language models\n(LLMs) that seamlessly integrates the entire scientific claim analysis process.\nSciClaims outperforms previous approaches in both claim extraction and\nverification without requiring additional fine-tuning, setting a new benchmark\nfor automated scientific claim analysis.",
      "tldr_zh": "该研究提出了SciClaims，一种基于最先进大语言模型(LLMs)的端到端生成系统，用于生物医学领域的科学声明分析。该系统整合了声明提取、证据检索和验证的全流程，解决了现有方法缺乏完整管道、依赖复杂NLP和信息检索技术以及验证结果解释不清等问题。实验表明，SciClaims在声明提取和验证任务上均优于现有方法，且无需额外微调，为自动化科学声明分析设立了新标准。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DL"
      ],
      "primary_category": "cs.CL",
      "comment": "Pre-print version",
      "pdf_url": "http://arxiv.org/pdf/2503.18526v1",
      "published_date": "2025-03-24 10:31:31 UTC",
      "updated_date": "2025-03-24 10:31:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:04:45.185944"
    },
    {
      "arxiv_id": "2503.18509v1",
      "title": "Neuro-symbolic Weak Supervision: Theory and Semantics",
      "title_zh": "神经符号弱监督：理论与语义",
      "authors": [
        "Nijesh Upreti",
        "Vaishak Belle"
      ],
      "abstract": "Weak supervision allows machine learning models to learn from limited or\nnoisy labels, but it introduces challenges in interpretability and reliability\n- particularly in multi-instance partial label learning (MI-PLL), where models\nmust resolve both ambiguous labels and uncertain instance-label mappings. We\npropose a semantics for neuro-symbolic framework that integrates Inductive\nLogic Programming (ILP) to improve MI-PLL by providing structured relational\nconstraints that guide learning. Within our semantic characterization, ILP\ndefines a logical hypothesis space for label transitions, clarifies classifier\nsemantics, and establishes interpretable performance standards. This hybrid\napproach improves robustness, transparency, and accountability in weakly\nsupervised settings, ensuring neural predictions align with domain knowledge.\nBy embedding weak supervision into a logical framework, we enhance both\ninterpretability and learning, making weak supervision more suitable for\nreal-world, high-stakes applications.",
      "tldr_zh": "本研究提出了一种神经符号弱监督框架，通过整合归纳逻辑编程(ILP)来解决多实例部分标签学习(MI-PLL)中的模糊标签和不确定实例-标签映射问题。该框架利用ILP定义标签转换的逻辑假设空间，明确分类器语义，并建立可解释的性能标准，从而增强弱监督学习在现实高风险应用中的鲁棒性、透明性和可问责性。通过将弱监督嵌入逻辑框架，该方法提升了模型的解释性和学习能力，确保神经预测与领域知识一致。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18509v1",
      "published_date": "2025-03-24 10:02:51 UTC",
      "updated_date": "2025-03-24 10:02:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:04:45.325288"
    },
    {
      "arxiv_id": "2503.18497v1",
      "title": "Statistically Testing Training Data for Unwanted Error Patterns using Rule-Oriented Regression",
      "title_zh": "使用规则导向回归统计测试训练数据中的不良错误模式",
      "authors": [
        "Stefan Rass",
        "Martin Dallinger"
      ],
      "abstract": "Artificial intelligence models trained from data can only be as good as the\nunderlying data is. Biases in training data propagating through to the output\nof a machine learning model are a well-documented and well-understood\nphenomenon, but the machinery to prevent these undesired effects is much less\ndeveloped. Efforts to ensure data is clean during collection, such as using\nbias-aware sampling, are most effective when the entity controlling data\ncollection also trains the AI. In cases where the data is already available,\nhow do we find out if the data was already manipulated, i.e., ``poisoned'', so\nthat an undesired behavior would be trained into a machine learning model? This\nis a challenge fundamentally different to (just) improving approximation\naccuracy or efficiency, and we provide a method to test training data for\nflaws, to establish a trustworthy ground-truth for a subsequent training of\nmachine learning models (of any kind). Unlike the well-studied problem of\napproximating data using fuzzy rules that are generated from the data, our\nmethod hinges on a prior definition of rules to happen before seeing the data\nto be tested. Therefore, the proposed method can also discover hidden error\npatterns, which may also have substantial influence. Our approach extends the\nabilities of conventional statistical testing by letting the ``test-condition''\nbe any Boolean condition to describe a pattern in the data, whose presence we\nwish to determine. The method puts fuzzy inference into a regression model, to\nget the best of the two: explainability from fuzzy logic with statistical\nproperties and diagnostics from the regression, and finally also being\napplicable to ``small data'', hence not requiring large datasets as deep\nlearning methods do. We provide an open source implementation for demonstration\nand experiments.",
      "tldr_zh": "该研究提出了一种基于规则导向回归(rule-oriented regression)的统计测试方法，用于检测训练数据中是否存在不期望的错误模式或“数据中毒”问题。与传统的从数据生成模糊规则的方法不同，该方法允许在查看数据之前定义规则，从而能够发现潜在的错误模式。该方法结合了模糊逻辑的可解释性和回归模型的统计特性，适用于小数据集，并提供了开源实现。这一方法为建立可信的训练数据基础提供了新的工具，有助于防止机器学习模型训练中的偏差传播。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "68T10 (Primary), 68M25, 62J86 (Secondary)"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18497v1",
      "published_date": "2025-03-24 09:52:36 UTC",
      "updated_date": "2025-03-24 09:52:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:04:57.127524"
    },
    {
      "arxiv_id": "2503.18494v1",
      "title": "Verbal Process Supervision Elicits Better Coding Agents",
      "title_zh": "言语过程监督提升编码智能体性能",
      "authors": [
        "Hao-Yuan Chen",
        "Cheng-Pong Huang",
        "Jui-Ming Yao"
      ],
      "abstract": "The emergence of large language models and their applications as AI agents\nhave significantly advanced state-of-the-art code generation benchmarks,\ntransforming modern software engineering tasks. However, even with test-time\ncomputed reasoning models, these systems still struggle with complex software\nengineering challenges. This work introduces CURA, a code understanding and\nreasoning agent system enhanced with verbal process supervision (VPS),\nachieving a 3.65\\% improvement over baseline models on challenging benchmarks\nlike BigCodeBench. Furthermore, CURA, when paired with the o3-mini model and\nVPS techniques, attains state-of-the-art performance. This work represents a\nstep forward in integrating reasoning-driven architectures with LLM-based code\ngeneration, enabling agentic reasoning for language models to solve complex\nsoftware engineering tasks.",
      "tldr_zh": "本研究提出了CURA，一种结合了语言过程监督（VPS）的代码理解与推理智能体系统，显著提升了大型语言模型在代码生成任务中的表现。CURA在BigCodeBench等复杂基准测试上比基线模型提高了3.65%，并在与o3-mini模型和VPS技术结合时达到了最先进的性能。这项工作推动了基于推理的架构与LLM代码生成的结合，使语言模型能够更好地解决复杂的软件工程任务。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18494v1",
      "published_date": "2025-03-24 09:48:59 UTC",
      "updated_date": "2025-03-24 09:48:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:05:56.729032"
    },
    {
      "arxiv_id": "2503.18492v1",
      "title": "Safeguarding Mobile GUI Agent via Logic-based Action Verification",
      "title_zh": "通过基于逻辑的动作验证保障移动 GUI 智能体的安全性",
      "authors": [
        "Jungjae Lee",
        "Dongjae Lee",
        "Chihun Choi",
        "Youngmin Im",
        "Jaeyoung Wi",
        "Kihong Heo",
        "Sangeun Oh",
        "Sunjae Lee",
        "Insik Shin"
      ],
      "abstract": "Large Foundation Models (LFMs) have unlocked new possibilities in\nhuman-computer interaction, particularly with the rise of mobile Graphical User\nInterface (GUI) Agents capable of interpreting GUIs. These agents promise to\nrevolutionize mobile computing by allowing users to automate complex mobile\ntasks through simple natural language instructions. However, the inherent\nprobabilistic nature of LFMs, coupled with the ambiguity and context-dependence\nof mobile tasks, makes LFM-based automation unreliable and prone to errors. To\naddress this critical challenge, we introduce VeriSafe Agent (VSA): a formal\nverification system that serves as a logically grounded safeguard for Mobile\nGUI Agents. VSA is designed to deterministically ensure that an agent's actions\nstrictly align with user intent before conducting an action. At its core, VSA\nintroduces a novel autoformalization technique that translates natural language\nuser instructions into a formally verifiable specification, expressed in our\ndomain-specific language (DSL). This enables runtime, rule-based verification,\nallowing VSA to detect and prevent erroneous actions executing an action,\neither by providing corrective feedback or halting unsafe behavior. To the best\nof our knowledge, VSA is the first attempt to bring the rigor of formal\nverification to GUI agent. effectively bridging the gap between LFM-driven\nautomation and formal software verification. We implement VSA using\noff-the-shelf LLM services (GPT-4o) and evaluate its performance on 300 user\ninstructions across 18 widely used mobile apps. The results demonstrate that\nVSA achieves 94.3%-98.33% accuracy in verifying agent actions, representing a\nsignificant 20.4%-25.6% improvement over existing LLM-based verification\nmethods, and consequently increases the GUI agent's task completion rate by\n90%-130%.",
      "tldr_zh": "该研究提出了VeriSafe Agent (VSA)，一种基于逻辑的动作验证系统，旨在解决大型基础模型(LFMs)驱动的移动图形用户界面(GUI)代理在任务自动化中的不可靠性问题。VSA通过一种新颖的自动形式化技术，将自然语言指令转化为可形式化验证的规范，并在执行动作前进行确定性验证，确保代理行为与用户意图严格一致。实验表明，VSA在18款常用移动应用的300条用户指令上实现了94.3%-98.33%的验证准确率，比现有方法提高了20.4%-25.6%，显著提升了GUI代理的任务完成率。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18492v1",
      "published_date": "2025-03-24 09:46:05 UTC",
      "updated_date": "2025-03-24 09:46:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:06:16.614321"
    },
    {
      "arxiv_id": "2503.18487v1",
      "title": "Large Language Models powered Network Attack Detection: Architecture, Opportunities and Case Study",
      "title_zh": "基于大型语言模型的网络攻击检测：架构、机遇与案例研究",
      "authors": [
        "Xinggong Zhang",
        "Qingyang Li",
        "Yunpeng Tan",
        "Zongming Guo",
        "Lei Zhang",
        "Yong Cui"
      ],
      "abstract": "Network attack detection is a pivotal technology to identify network anomaly\nand classify malicious traffic. Large Language Models (LLMs) are trained on a\nvast corpus of text, have amassed remarkable capabilities of\ncontext-understanding and commonsense knowledge. This has opened up a new door\nfor network threat detection. Researchers have already initiated discussions\nregarding the application of LLMs on specific cyber-security tasks.\nUnfortunately, there is still a lack of comprehensive elaboration how to mine\nLLMs' potentials in network threat detections, as well as the opportunities and\nchallenges. In this paper, we mainly focus on the classification of malicious\ntraffic from the perspective of LLMs' capability. We present a holistic view of\nthe architecture of LLM-powered network attack detection, including\nPre-training, Fine-tuning, and Detection. Especially, by exploring the\nknowledge and capabilities of LLM, we identify three distinct roles LLM can act\nin network attack detection: \\textit{Classifier, Encoder, and Predictor}. For\neach of them, the modeling paradigm, opportunities and challenges are\nelaborated. Finally, we present our design on LLM-powered DDoS detection as a\ncase study. The proposed framework attains accurate detection on carpet bombing\nDDoS by exploiting LLMs' capabilities in contextual mining. The evaluation\nshows its efficacy, exhibiting a nearly $35$\\% improvement compared to existing\nsystems.",
      "tldr_zh": "本文探讨了利用大语言模型(LLMs)进行网络攻击检测的架构、机遇与挑战，重点分析了LLMs在恶意流量分类中的应用。研究提出了LLM驱动的网络攻击检测整体架构，包括预训练、微调和检测三个阶段，并识别了LLMs在网络攻击检测中可以扮演的三种角色：分类器(Classifier)、编码器(Encoder)和预测器(Predictor)。通过案例分析，研究者设计了一个基于LLM的DDoS检测框架，利用LLMs的上下文挖掘能力实现了对地毯式轰炸DDoS攻击的精准检测，实验表明其性能比现有系统提升了近35%。",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.NI",
      "comment": "submitted for peer-review",
      "pdf_url": "http://arxiv.org/pdf/2503.18487v1",
      "published_date": "2025-03-24 09:40:46 UTC",
      "updated_date": "2025-03-24 09:40:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:06:38.095055"
    },
    {
      "arxiv_id": "2503.18471v1",
      "title": "Words as Bridges: Exploring Computational Support for Cross-Disciplinary Translation Work",
      "title_zh": "词语为桥：探索跨学科翻译工作的计算支持",
      "authors": [
        "Calvin Bao",
        "Yow-Ting Shiue",
        "Marine Carpuat",
        "Joel Chan"
      ],
      "abstract": "Scholars often explore literature outside of their home community of study.\nThis exploration process is frequently hampered by field-specific jargon. Past\ncomputational work often focuses on supporting translation work by removing\njargon through simplification and summarization; here, we explore a different\napproach that preserves jargon as useful bridges to new conceptual spaces.\nSpecifically, we cast different scholarly domains as different language-using\ncommunities, and explore how to adapt techniques from unsupervised\ncross-lingual alignment of word embeddings to explore conceptual alignments\nbetween domain-specific word embedding spaces.We developed a prototype\ncross-domain search engine that uses aligned domain-specific embeddings to\nsupport conceptual exploration, and tested this prototype in two case studies.\nWe discuss qualitative insights into the promises and pitfalls of this approach\nto translation work, and suggest design insights for future interfaces that\nprovide computational support for cross-domain information seeking.",
      "tldr_zh": "该研究探讨了如何通过计算工具支持跨学科翻译工作，提出了一种保留特定领域术语作为概念桥梁的新方法。研究者将不同学术领域视为不同的语言使用社区，并采用无监督跨语言词向量对齐技术来探索领域特定词向量空间之间的概念对齐。研究开发了一个原型跨领域搜索引擎，利用对齐的词向量支持概念探索，并通过两个案例研究验证了该方法的潜力，为未来跨领域信息搜索界面的设计提供了见解。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "26 pages, 8 tables, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.18471v1",
      "published_date": "2025-03-24 09:19:29 UTC",
      "updated_date": "2025-03-24 09:19:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:06:42.005786"
    },
    {
      "arxiv_id": "2503.18470v1",
      "title": "MetaSpatial: Reinforcing 3D Spatial Reasoning in VLMs for the Metaverse",
      "title_zh": "MetaSpatial：强化视觉语言模型中的三维空间推理能力以支持元宇宙",
      "authors": [
        "Zhenyu Pan",
        "Han Liu"
      ],
      "abstract": "We present MetaSpatial, the first reinforcement learning (RL)-based framework\ndesigned to enhance 3D spatial reasoning in vision-language models (VLMs),\nenabling real-time 3D scene generation without the need for hard-coded\noptimizations. MetaSpatial addresses two core challenges: (i) the lack of\ninternalized 3D spatial reasoning in VLMs, which limits their ability to\ngenerate realistic layouts, and (ii) the inefficiency of traditional supervised\nfine-tuning (SFT) for layout generation tasks, as perfect ground truth\nannotations are unavailable. Our key innovation is a multi-turn RL-based\noptimization mechanism that integrates physics-aware constraints and rendered\nimage evaluations, ensuring generated 3D layouts are coherent, physically\nplausible, and aesthetically consistent. Methodologically, MetaSpatial\nintroduces an adaptive, iterative reasoning process, where the VLM refines\nspatial arrangements over multiple turns by analyzing rendered outputs,\nimproving scene coherence progressively. Empirical evaluations demonstrate that\nMetaSpatial significantly enhances the spatial consistency and formatting\nstability of various scale models. Post-training, object placements are more\nrealistic, aligned, and functionally coherent, validating the effectiveness of\nRL for 3D spatial reasoning in metaverse, AR/VR, digital twins, and game\ndevelopment applications. Our code, data, and training pipeline are publicly\navailable at https://github.com/PzySeere/MetaSpatial.",
      "tldr_zh": "MetaSpatial 是首个基于强化学习（RL）的框架，旨在增强视觉语言模型（VLMs）的3D空间推理能力，实现无需硬编码优化的实时3D场景生成。该框架通过多轮RL优化机制，结合物理感知约束和渲染图像评估，确保生成的3D布局具有一致性、物理合理性和美学一致性。实验表明，MetaSpatial显著提升了空间一致性和格式化稳定性，生成的物体布局更加真实、对齐且功能连贯，为元宇宙、AR/VR、数字孪生和游戏开发等应用提供了有效支持。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Working Paper",
      "pdf_url": "http://arxiv.org/pdf/2503.18470v1",
      "published_date": "2025-03-24 09:18:01 UTC",
      "updated_date": "2025-03-24 09:18:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:06:35.400955"
    },
    {
      "arxiv_id": "2503.18462v1",
      "title": "PALATE: Peculiar Application of the Law of Total Expectation to Enhance the Evaluation of Deep Generative Models",
      "title_zh": "PALATE：全期望定律的独特应用以增强深度生成模型的评估",
      "authors": [
        "Tadeusz Dziarmaga",
        "Marcin Kądziołka",
        "Artur Kasymov",
        "Marcin Mazur"
      ],
      "abstract": "Deep generative models (DGMs) have caused a paradigm shift in the field of\nmachine learning, yielding noteworthy advancements in domains such as image\nsynthesis, natural language processing, and other related areas. However, a\ncomprehensive evaluation of these models that accounts for the trichotomy\nbetween fidelity, diversity, and novelty in generated samples remains a\nformidable challenge. A recently introduced solution that has emerged as a\npromising approach in this regard is the Feature Likelihood Divergence (FLD), a\nmethod that offers a theoretically motivated practical tool, yet also exhibits\nsome computational challenges. In this paper, we propose PALATE, a novel\nenhancement to the evaluation of DGMs that addresses limitations of existing\nmetrics. Our approach is based on a peculiar application of the law of total\nexpectation to random variables representing accessible real data. When\ncombined with the MMD baseline metric and DINOv2 feature extractor, PALATE\noffers a holistic evaluation framework that matches or surpasses\nstate-of-the-art solutions while providing superior computational efficiency\nand scalability to large-scale datasets. Through a series of experiments, we\ndemonstrate the effectiveness of the PALATE enhancement, contributing a\ncomputationally efficient, holistic evaluation approach that advances the field\nof DGMs assessment, especially in detecting sample memorization and evaluating\ngeneralization capabilities.",
      "tldr_zh": "本文提出了一种名为PALATE的新方法，通过巧妙应用全期望定律(Law of Total Expectation)来增强深度生成模型(DGMs)的评估。该方法结合了MMD基线指标和DINOv2特征提取器，解决了现有评估指标在计算效率和可扩展性上的不足。实验表明，PALATE在全面评估生成样本的保真度、多样性和新颖性方面表现优异，尤其在检测样本记忆和评估泛化能力上具有显著优势，为DGM评估提供了高效且全面的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18462v1",
      "published_date": "2025-03-24 09:06:45 UTC",
      "updated_date": "2025-03-24 09:06:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:06:41.628331"
    },
    {
      "arxiv_id": "2503.18460v1",
      "title": "ModiGen: A Large Language Model-Based Workflow for Multi-Task Modelica Code Generation",
      "title_zh": "ModiGen：基于大语言模型的多任务Modelica代码生成工作流",
      "authors": [
        "Jiahui Xiang",
        "Tong Ye",
        "Peiyu Liu",
        "Yinan Zhang",
        "Wenhai Wang"
      ],
      "abstract": "Modelica is a widely adopted language for simulating complex physical\nsystems, yet effective model creation and optimization require substantial\ndomain expertise. Although large language models (LLMs) have demonstrated\npromising capabilities in code generation, their application to modeling\nremains largely unexplored. To address this gap, we have developed benchmark\ndatasets specifically designed to evaluate the performance of LLMs in\ngenerating Modelica component models and test cases. Our evaluation reveals\nsubstantial limitations in current LLMs, as the generated code often fails to\nsimulate successfully. To overcome these challenges, we propose a specialized\nworkflow that integrates supervised fine-tuning, graph retrieval-augmented\ngeneration, and feedback optimization to improve the accuracy and reliability\nof Modelica code generation. The evaluation results demonstrate significant\nperformance gains: the maximum improvement in pass@1 reached 0.3349 for the\ncomponent generation task and 0.2457 for the test case generation task. This\nresearch underscores the potential of LLMs to advance intelligent modeling\ntools and offers valuable insights for future developments in system modeling\nand engineering applications.",
      "tldr_zh": "该研究提出ModiGen，一种基于大语言模型(LLMs)的工作流，用于生成Modelica代码。针对当前LLMs在生成Modelica组件模型和测试用例时表现不佳的问题，研究者开发了专门的基准数据集，并提出了一种集成监督微调、图检索增强生成和反馈优化的方法，显著提升了代码生成的准确性和可靠性。实验结果显示，组件生成任务和测试用例生成任务的pass@1分别提高了0.3349和0.2457，为智能建模工具的发展提供了新的方向。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18460v1",
      "published_date": "2025-03-24 09:04:49 UTC",
      "updated_date": "2025-03-24 09:04:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:06:47.587634"
    },
    {
      "arxiv_id": "2503.18432v1",
      "title": "Teaching LLMs for Step-Level Automatic Math Correction via Reinforcement Learning",
      "title_zh": "通过强化学习教授大语言模型进行步骤级自动数学纠错",
      "authors": [
        "Junsong Li",
        "Jie Zhou",
        "Yutao Yang",
        "Bihao Zhan",
        "Qianjun Pan",
        "Yuyang Ding",
        "Qin Chen",
        "Jiang Bo",
        "Xin Lin",
        "Liang He"
      ],
      "abstract": "Automatic math correction aims to check students' solutions to mathematical\nproblems via artificial intelligence technologies. Most existing studies focus\non judging the final answer at the problem level, while they ignore detailed\nfeedback on each step in a math problem-solving process, which requires\nabilities of semantic understanding and reasoning. In this paper, we propose a\nreinforcement learning (RL)-based method to boost large language model (LLM)\nfor step-level automatic math correction, named StepAMC. Particularly, we\nconvert the step-level automatic math correction within the text classification\ntask into an RL problem to enhance the reasoning capabilities of LLMs. Then, we\ndesign a space-constrained policy network to improve the stability of RL. Then,\nwe introduce a fine-grained reward network to convert the binary human feedback\ninto a continuous value. We conduct extensive experiments over two benchmark\ndatasets and the results show that our model outperforms the eleven strong\nbaselines.",
      "tldr_zh": "该研究提出了一种基于强化学习(RL)的方法StepAMC，用于提升大语言模型(LLMs)在步骤级自动数学纠错中的能力。通过将步骤级数学纠错任务转化为RL问题，设计了空间受限的策略网络以提高RL稳定性，并引入细粒度奖励网络将二值化的人类反馈转化为连续值。实验结果表明，该模型在两个基准数据集上优于11个强基线方法，为数学解题过程中的详细反馈提供了有效支持。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18432v1",
      "published_date": "2025-03-24 08:28:34 UTC",
      "updated_date": "2025-03-24 08:28:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:06:59.463338"
    },
    {
      "arxiv_id": "2503.18419v1",
      "title": "Generative AI in Knowledge Work: Design Implications for Data Navigation and Decision-Making",
      "title_zh": "生成式AI在知识工作中的应用：面向数据导航与决策的设计启示",
      "authors": [
        "Bhada Yun",
        "Dana Feng",
        "Ace S. Chen",
        "Afshin Nikzad",
        "Niloufar Salehi"
      ],
      "abstract": "Our study of 20 knowledge workers revealed a common challenge: the difficulty\nof synthesizing unstructured information scattered across multiple platforms to\nmake informed decisions. Drawing on their vision of an ideal knowledge\nsynthesis tool, we developed Yodeai, an AI-enabled system, to explore both the\nopportunities and limitations of AI in knowledge work. Through a user study\nwith 16 product managers, we identified three key requirements for Generative\nAI in knowledge work: adaptable user control, transparent collaboration\nmechanisms, and the ability to integrate background knowledge with external\ninformation. However, we also found significant limitations, including\noverreliance on AI, user isolation, and contextual factors outside the AI's\nreach. As AI tools become increasingly prevalent in professional settings, we\npropose design principles that emphasize adaptability to diverse workflows,\naccountability in personal and collaborative contexts, and context-aware\ninteroperability to guide the development of human-centered AI systems for\nproduct managers and knowledge workers.",
      "tldr_zh": "本研究探讨了生成式AI在知识工作中的应用，开发了AI系统Yodeai，以解决知识工作者在跨平台信息整合和决策中的挑战。通过对16名产品经理的用户研究，提出了生成式AI的三大关键需求：灵活的用户控制、透明的协作机制以及整合背景知识与外部信息的能力。同时，研究也揭示了AI工具的局限性，如过度依赖AI、用户孤立以及AI无法处理的上下文因素。基于此，研究提出设计原则，强调适应多样化工作流程、在个人和协作场景中的可问责性，以及上下文感知的互操作性，以指导开发以人为本的AI系统。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.ET",
        "H.5.m"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted to CHI '25 (Conference on Human Factors in Computing\n  Systems), to appear April 26-May 1, 2025, Yokohama, Japan",
      "pdf_url": "http://arxiv.org/pdf/2503.18419v1",
      "published_date": "2025-03-24 08:02:44 UTC",
      "updated_date": "2025-03-24 08:02:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:07:06.877229"
    },
    {
      "arxiv_id": "2503.18403v1",
      "title": "Knowledge Graph Enhanced Generative Multi-modal Models for Class-Incremental Learning",
      "title_zh": "知识图谱增强的生成式多模态模型在类增量学习中的应用",
      "authors": [
        "Xusheng Cao",
        "Haori Lu",
        "Linlan Huang",
        "Fei Yang",
        "Xialei Liu",
        "Ming-Ming Cheng"
      ],
      "abstract": "Continual learning in computer vision faces the critical challenge of\ncatastrophic forgetting, where models struggle to retain prior knowledge while\nadapting to new tasks. Although recent studies have attempted to leverage the\ngeneralization capabilities of pre-trained models to mitigate overfitting on\ncurrent tasks, models still tend to forget details of previously learned\ncategories as tasks progress, leading to misclassification. To address these\nlimitations, we introduce a novel Knowledge Graph Enhanced Generative\nMulti-modal model (KG-GMM) that builds an evolving knowledge graph throughout\nthe learning process. Our approach utilizes relationships within the knowledge\ngraph to augment the class labels and assigns different relations to similar\ncategories to enhance model differentiation. During testing, we propose a\nKnowledge Graph Augmented Inference method that locates specific categories by\nanalyzing relationships within the generated text, thereby reducing the loss of\ndetailed information about old classes when learning new knowledge and\nalleviating forgetting. Experiments demonstrate that our method effectively\nleverages relational information to help the model correct mispredictions,\nachieving state-of-the-art results in both conventional CIL and few-shot CIL\nsettings, confirming the efficacy of knowledge graphs at preserving knowledge\nin the continual learning scenarios.",
      "tldr_zh": "该研究提出了一种基于知识图谱增强的生成多模态模型(KG-GMM)，用于解决类增量学习中的灾难性遗忘问题。该模型通过构建动态演化的知识图谱，利用类别间关系增强标签信息，并通过知识图谱增强推理方法在测试时准确定位类别，从而减少旧类别信息的丢失。实验表明，KG-GMM在常规和少样本类增量学习场景中均取得了最先进的性能，验证了知识图谱在持续学习中的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18403v1",
      "published_date": "2025-03-24 07:20:43 UTC",
      "updated_date": "2025-03-24 07:20:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:07:14.569605"
    },
    {
      "arxiv_id": "2503.18395v1",
      "title": "PRECTR: A Synergistic Framework for Integrating Personalized Search Relevance Matching and CTR Prediction",
      "title_zh": "PRECTR：一个融合个性化搜索相关性匹配与点击率预测的协同框架",
      "authors": [
        "Rong Chen",
        "Shuzhi Cao",
        "Ailong He",
        "Shuguang Han",
        "Jufeng Chen"
      ],
      "abstract": "The two primary tasks in the search recommendation system are search\nrelevance matching and click-through rate (CTR) prediction -- the former\nfocuses on seeking relevant items for user queries whereas the latter forecasts\nwhich item may better match user interest. Prior research typically develops\ntwo models to predict the CTR and search relevance separately, then ranking\ncandidate items based on the fusion of the two outputs. However, such a\ndivide-and-conquer paradigm creates the inconsistency between different models.\nMeanwhile, the search relevance model mainly concentrates on the degree of\nobjective text matching while neglecting personalized differences among\ndifferent users, leading to restricted model performance. To tackle these\nissues, we propose a unified \\textbf{P}ersonalized Search RElevance Matching\nand CTR Prediction Fusion Model(PRECTR). Specifically, based on the conditional\nprobability fusion mechanism, PRECTR integrates the CTR prediction and search\nrelevance matching into one framework to enhance the interaction and\nconsistency of the two modules. However, directly optimizing CTR binary\nclassification loss may bring challenges to the fusion model's convergence and\nindefinitely promote the exposure of items with high CTR, regardless of their\nsearch relevance. Hence, we further introduce two-stage training and semantic\nconsistency regularization to accelerate the model's convergence and restrain\nthe recommendation of irrelevant items. Finally, acknowledging that different\nusers may have varied relevance preferences, we assessed current users'\nrelevance preferences by analyzing past users' preferences for similar queries\nand tailored incentives for different candidate items accordingly. Extensive\nexperimental results on our production dataset and online A/B testing\ndemonstrate the effectiveness and superiority of our proposed PRECTR method.",
      "tldr_zh": "该研究提出了一种协同框架PRECTR，将个性化搜索相关性匹配与点击率预测(CTR)整合到一个统一模型中，以解决传统方法中模型不一致和个性化差异被忽视的问题。通过条件概率融合机制，PRECTR增强了两个模块之间的交互和一致性，并引入两阶段训练和语义一致性正则化来加速模型收敛并抑制不相关推荐。实验结果表明，PRECTR在生产数据集和在线A/B测试中均表现出显著的有效性和优越性，为搜索推荐系统提供了更精准的个性化服务。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18395v1",
      "published_date": "2025-03-24 07:07:04 UTC",
      "updated_date": "2025-03-24 07:07:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:07:40.652014"
    },
    {
      "arxiv_id": "2503.18387v1",
      "title": "Manipulation and the AI Act: Large Language Model Chatbots and the Danger of Mirrors",
      "title_zh": "操纵与《人工智能法案》：大型语言模型聊天机器人及镜子的危险",
      "authors": [
        "Joshua Krook"
      ],
      "abstract": "Large Language Model chatbots are increasingly taking the form and visage of\nhuman beings, adapting human faces, names, voices, personalities, and quirks,\nincluding those of celebrities and well-known political figures. Personifying\nAI chatbots could foreseeably increase their trust with users. However, it\ncould also make them more capable of manipulation, by creating the illusion of\na close and intimate relationship with an artificial entity. The European\nCommission has finalized the AI Act, with the EU Parliament making amendments\nbanning manipulative and deceptive AI systems that cause significant harm to\nusers. Although the AI Act covers harms that accumulate over time, it is\nunlikely to prevent harms associated with prolonged discussions with AI\nchatbots. Specifically, a chatbot could reinforce a person's negative emotional\nstate over weeks, months, or years through negative feedback loops, prolonged\nconversations, or harmful recommendations, contributing to a user's\ndeteriorating mental health.",
      "tldr_zh": "该研究探讨了大型语言模型(LLM)聊天机器人拟人化带来的操纵风险，特别是在欧盟《人工智能法案》(AI Act)框架下的潜在危害。研究发现，赋予AI聊天机器人人类外貌、声音和个性特征虽能增强用户信任，但也可能通过制造亲密关系的假象，加剧其对用户的操纵能力。尽管《AI法案》禁止具有欺骗性和操纵性的AI系统，但难以防范长期对话导致的累积性心理伤害，如聊天机器人通过负面反馈循环和有害建议加剧用户情绪恶化。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18387v1",
      "published_date": "2025-03-24 06:56:29 UTC",
      "updated_date": "2025-03-24 06:56:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:08:02.299375"
    },
    {
      "arxiv_id": "2503.18386v1",
      "title": "Resource-Efficient Motion Control for Video Generation via Dynamic Mask Guidance",
      "title_zh": "基于动态掩码引导的资源高效视频生成运动控制",
      "authors": [
        "Sicong Feng",
        "Jielong Yang",
        "Li Peng"
      ],
      "abstract": "Recent advances in diffusion models bring new vitality to visual content\ncreation. However, current text-to-video generation models still face\nsignificant challenges such as high training costs, substantial data\nrequirements, and difficulties in maintaining consistency between given text\nand motion of the foreground object. To address these challenges, we propose\nmask-guided video generation, which can control video generation through mask\nmotion sequences, while requiring limited training data. Our model enhances\nexisting architectures by incorporating foreground masks for precise\ntext-position matching and motion trajectory control. Through mask motion\nsequences, we guide the video generation process to maintain consistent\nforeground objects throughout the sequence. Additionally, through a first-frame\nsharing strategy and autoregressive extension approach, we achieve more stable\nand longer video generation. Extensive qualitative and quantitative experiments\ndemonstrate that this approach excels in various video generation tasks, such\nas video editing and generating artistic videos, outperforming previous methods\nin terms of consistency and quality. Our generated results can be viewed in the\nsupplementary materials.",
      "tldr_zh": "该研究提出了一种基于动态掩码引导的资源高效视频生成方法，旨在解决现有文本到视频生成模型训练成本高、数据需求大以及文本与前景物体运动一致性差的问题。通过引入前景掩码序列，模型实现了对视频生成的精确控制，确保前景物体在时间序列中的一致性。结合首帧共享策略和自回归扩展方法，进一步提升了视频生成的稳定性和时长。实验表明，该方法在视频编辑和艺术视频生成等任务中表现优异，在一致性和质量上均优于现有方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18386v1",
      "published_date": "2025-03-24 06:53:08 UTC",
      "updated_date": "2025-03-24 06:53:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:08:00.585231"
    },
    {
      "arxiv_id": "2503.18385v1",
      "title": "RoCA: Robust Contrastive One-class Time Series Anomaly Detection with Contaminated Data",
      "title_zh": "RoCA：基于污染数据的鲁棒对比式单类时间序列异常检测",
      "authors": [
        "Xudong Mou",
        "Rui Wang",
        "Bo Li",
        "Tianyu Wo",
        "Jie Sun",
        "Hui Wang",
        "Xudong Liu"
      ],
      "abstract": "The accumulation of time-series signals and the absence of labels make\ntime-series Anomaly Detection (AD) a self-supervised task of deep learning.\nMethods based on normality assumptions face the following three limitations:\n(1) A single assumption could hardly characterize the whole normality or lead\nto some deviation. (2) Some assumptions may go against the principle of AD. (3)\nTheir basic assumption is that the training data is uncontaminated (free of\nanomalies), which is unrealistic in practice, leading to a decline in\nrobustness. This paper proposes a novel robust approach, RoCA, which is the\nfirst to address all of the above three challenges, as far as we are aware. It\nfuses the separated assumptions of one-class classification and contrastive\nlearning in a single training process to characterize a more complete so-called\nnormality. Additionally, it monitors the training data and computes a carefully\ndesigned anomaly score throughout the training process. This score helps\nidentify latent anomalies, which are then used to define the classification\nboundary, inspired by the concept of outlier exposure. The performance on AIOps\ndatasets improved by 6% compared to when contamination was not considered\n(COCA). On two large and high-dimensional multivariate datasets, the\nperformance increased by 5% to 10%. RoCA achieves the highest average\nperformance on both univariate and multivariate datasets. The source code is\navailable at https://github.com/ruiking04/RoCA.",
      "tldr_zh": "本文提出了一种新颖的时间序列异常检测方法RoCA，首次解决了现有方法在训练数据污染、单一假设局限性和异常检测原则冲突等方面的三大挑战。该方法通过融合单类分类和对比学习的假设，在训练过程中监控数据并计算异常得分，利用潜在异常定义分类边界，从而更全面地描述正常状态。实验表明，RoCA在AIOps数据集上的性能提升了6%，在大型高维多变量数据集上提升了5%至10%，在单变量和多变量数据集上均实现了最高的平均性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18385v1",
      "published_date": "2025-03-24 06:52:28 UTC",
      "updated_date": "2025-03-24 06:52:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:08:09.946019"
    },
    {
      "arxiv_id": "2503.18382v1",
      "title": "PP-FormulaNet: Bridging Accuracy and Efficiency in Advanced Formula Recognition",
      "title_zh": "PP-FormulaNet：在高级公式识别中平衡精度与效率",
      "authors": [
        "Hongen Liu",
        "Cheng Cui",
        "Yuning Du",
        "Yi Liu",
        "Gang Pan"
      ],
      "abstract": "Formula recognition is an important task in document intelligence. It\ninvolves converting mathematical expressions from document images into\nstructured symbolic formats that computers can easily work with. LaTeX is the\nmost common format used for this purpose. In this work, we present\nPP-FormulaNet, a state-of-the-art formula recognition model that excels in both\naccuracy and efficiency. To meet the diverse needs of applications, we have\ndeveloped two specialized models: PP-FormulaNet-L, tailored for high-accuracy\nscenarios, and PP-FormulaNet-S, optimized for high-efficiency contexts. Our\nextensive evaluations reveal that PP-FormulaNet-L attains accuracy levels that\nsurpass those of prominent models such as UniMERNet by a significant 6%.\nConversely, PP-FormulaNet-S operates at speeds that are over 16 times faster.\nThese advancements facilitate seamless integration of PP-FormulaNet into a\nbroad spectrum of document processing environments that involve intricate\nmathematical formulas. Furthermore, we introduce a Formula Mining System, which\nis capable of extracting a vast amount of high-quality formula data. This\nsystem further enhances the robustness and applicability of our formula\nrecognition model. Code and models are publicly available at\nPaddleOCR(https://github.com/PaddlePaddle/PaddleOCR) and\nPaddleX(https://github.com/PaddlePaddle/PaddleX).",
      "tldr_zh": "该研究提出了PP-FormulaNet，一种在公式识别任务中平衡精度与效率的先进模型。针对不同应用需求，开发了高精度模型PP-FormulaNet-L和高效率模型PP-FormulaNet-S，其中PP-FormulaNet-L的准确率比UniMERNet高6%，而PP-FormulaNet-S的速度提升了16倍以上。此外，研究还引入了公式挖掘系统，用于提取高质量公式数据，进一步增强模型的鲁棒性和适用性。相关代码和模型已在PaddleOCR和PaddleX开源。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18382v1",
      "published_date": "2025-03-24 06:39:51 UTC",
      "updated_date": "2025-03-24 06:39:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:08:27.351075"
    },
    {
      "arxiv_id": "2503.18377v1",
      "title": "Maximum Redundancy Pruning: A Principle-Driven Layerwise Sparsity Allocation for LLMs",
      "title_zh": "最大冗余剪枝：面向大语言模型的基于原则的层级稀疏度分配",
      "authors": [
        "Chang Gao",
        "Kang Zhao",
        "Jianfei Chen",
        "Liping Jing"
      ],
      "abstract": "Large language models (LLMs) have demonstrated impressive capabilities, but\ntheir enormous size poses significant challenges for deployment in real-world\napplications. To address this issue, researchers have sought to apply network\npruning techniques to LLMs. A critical challenge in pruning is allocation the\nsparsity for each layer. Recent sparsity allocation methods is often based on\nheuristics or search that can easily lead to suboptimal performance. In this\npaper, we conducted an extensive investigation into various LLMs and revealed\nthree significant discoveries: (1) the layerwise pruning sensitivity (LPS) of\nLLMs is highly non-uniform, (2) the choice of pruning metric affects LPS, and\n(3) the performance of a sparse model is related to the uniformity of its\nlayerwise redundancy level. Based on these observations, we propose that the\nlayerwise sparsity of LLMs should adhere to three principles:\n\\emph{non-uniformity}, \\emph{pruning metric dependency}, and \\emph{uniform\nlayerwise redundancy level} in the pruned model. To this end, we proposed\nMaximum Redundancy Pruning (MRP), an iterative pruning algorithm that prunes in\nthe most redundant layers (\\emph{i.e.}, those with the highest non-outlier\nratio) at each iteration. The achieved layerwise sparsity aligns with the\noutlined principles. We conducted extensive experiments on publicly available\nLLMs, including the LLaMA2 and OPT, across various benchmarks. Experimental\nresults validate the effectiveness of MRP, demonstrating its superiority over\nprevious methods.",
      "tldr_zh": "本研究提出了一种基于原则的层间稀疏分配方法——最大冗余剪枝（Maximum Redundancy Pruning, MRP），用于优化大型语言模型（LLMs）的剪枝策略。通过深入分析，研究发现LLMs的层间剪枝敏感性（LPS）具有高度非均匀性，且剪枝指标的选择对LPS有显著影响，同时稀疏模型的性能与其层间冗余水平的均匀性相关。基于这些发现，MRP算法在每次迭代中剪枝最冗余的层（即非异常值比例最高的层），从而确保剪枝后的模型符合非均匀性、剪枝指标依赖性和层间冗余水平均匀性三大原则。实验结果表明，MRP在LLaMA2和OPT等公开LLMs上优于现有方法，显著提升了模型性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18377v1",
      "published_date": "2025-03-24 06:17:30 UTC",
      "updated_date": "2025-03-24 06:17:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:08:28.316521"
    },
    {
      "arxiv_id": "2503.18347v1",
      "title": "Latent Embedding Adaptation for Human Preference Alignment in Diffusion Planners",
      "title_zh": "扩散规划器中基于潜在嵌入适应的人类偏好对齐",
      "authors": [
        "Wen Zheng Terence Ng",
        "Jianda Chen",
        "Yuan Xu",
        "Tianwei Zhang"
      ],
      "abstract": "This work addresses the challenge of personalizing trajectories generated in\nautomated decision-making systems by introducing a resource-efficient approach\nthat enables rapid adaptation to individual users' preferences. Our method\nleverages a pretrained conditional diffusion model with Preference Latent\nEmbeddings (PLE), trained on a large, reward-free offline dataset. The PLE\nserves as a compact representation for capturing specific user preferences. By\nadapting the pretrained model using our proposed preference inversion method,\nwhich directly optimizes the learnable PLE, we achieve superior alignment with\nhuman preferences compared to existing solutions like Reinforcement Learning\nfrom Human Feedback (RLHF) and Low-Rank Adaptation (LoRA). To better reflect\npractical applications, we create a benchmark experiment using real human\npreferences on diverse, high-reward trajectories.",
      "tldr_zh": "本研究提出了一种基于偏好潜在嵌入(Preference Latent Embeddings, PLE)的资源高效方法，用于个性化自动决策系统生成的轨迹。该方法利用预训练的条件扩散模型，通过直接优化可学习的PLE来实现快速适应个体用户偏好。与现有的人类反馈强化学习(RLHF)和低秩适应(LoRA)等方法相比，该方法在人类偏好对齐方面表现更优。研究还创建了一个基于真实人类偏好的基准实验，以更好地反映实际应用场景。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "8 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.18347v1",
      "published_date": "2025-03-24 05:11:58 UTC",
      "updated_date": "2025-03-24 05:11:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:08:37.156544"
    },
    {
      "arxiv_id": "2503.18331v1",
      "title": "Optimizing Influence Campaigns: Nudging under Bounded Confidence",
      "title_zh": "优化影响力传播：有限置信度下的助推策略",
      "authors": [
        "Yen-Shao Chen",
        "Tauhid Zaman"
      ],
      "abstract": "Influence campaigns in online social networks are often run by organizations,\npolitical parties, and nation states to influence large audiences. These\ncampaigns are employed through the use of agents in the network that share\npersuasive content. Yet, their impact might be minimal if the audiences remain\nunswayed, often due to the bounded confidence phenomenon, where only a narrow\nspectrum of viewpoints can influence them. Here we show that to persuade under\nbounded confidence, an agent must nudge its targets to gradually shift their\nopinions. Using a control theory approach, we show how to construct an agent's\nnudging policy under the bounded confidence opinion dynamics model and also how\nto select targets for multiple agents in an influence campaign on a social\nnetwork. Simulations on real Twitter networks show that a multi-agent nudging\npolicy can shift the mean opinion, decrease opinion polarization, or even\nincrease it. We find that our nudging based policies outperform other common\ntechniques that do not consider the bounded confidence effect. Finally, we show\nhow to craft prompts for large language models, such as ChatGPT, to generate\ntext-based content for real nudging policies. This illustrates the practical\nfeasibility of our approach, allowing one to go from mathematical nudging\npolicies to real social media content.",
      "tldr_zh": "该研究提出了一种在有限信任（bounded confidence）现象下优化社交网络影响力活动的策略。通过控制理论方法，研究者构建了基于有限信任意见动态模型的“推动”（nudging）策略，并设计了多智能体在社交网络中选择目标的方法。实验表明，多智能体推动策略能够有效改变平均意见、减少或增加意见极化，且优于未考虑有限信任效应的传统技术。此外，研究者还展示了如何利用大型语言模型（如ChatGPT）生成实际推动策略的文本内容，验证了该方法的实际可行性。",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.SI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18331v1",
      "published_date": "2025-03-24 04:30:58 UTC",
      "updated_date": "2025-03-24 04:30:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:08:53.647014"
    },
    {
      "arxiv_id": "2503.18324v1",
      "title": "Plug-and-Play Interpretable Responsible Text-to-Image Generation via Dual-Space Multi-facet Concept Control",
      "title_zh": "即插即用可解释的负责任文本到图像生成：基于双空间多维度概念控制",
      "authors": [
        "Basim Azam",
        "Naveed Akhtar"
      ],
      "abstract": "Ethical issues around text-to-image (T2I) models demand a comprehensive\ncontrol over the generative content. Existing techniques addressing these\nissues for responsible T2I models aim for the generated content to be fair and\nsafe (non-violent/explicit). However, these methods remain bounded to handling\nthe facets of responsibility concepts individually, while also lacking in\ninterpretability. Moreover, they often require alteration to the original\nmodel, which compromises the model performance. In this work, we propose a\nunique technique to enable responsible T2I generation by simultaneously\naccounting for an extensive range of concepts for fair and safe content\ngeneration in a scalable manner. The key idea is to distill the target T2I\npipeline with an external plug-and-play mechanism that learns an interpretable\ncomposite responsible space for the desired concepts, conditioned on the target\nT2I pipeline. We use knowledge distillation and concept whitening to enable\nthis. At inference, the learned space is utilized to modulate the generative\ncontent. A typical T2I pipeline presents two plug-in points for our approach,\nnamely; the text embedding space and the diffusion model latent space. We\ndevelop modules for both points and show the effectiveness of our approach with\na range of strong results.",
      "tldr_zh": "该研究提出了一种即插即用的可解释文本到图像（T2I）生成框架，通过双空间多维度概念控制实现负责任的内容生成。该方法利用知识蒸馏和概念白化技术，在文本嵌入空间和扩散模型潜在空间中构建可解释的复合责任空间，从而同时处理公平性和安全性等多维度概念。实验表明，该框架无需修改原始模型即可实现高效、可扩展的负责任生成，为T2I模型提供了更具透明性和可控性的解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18324v1",
      "published_date": "2025-03-24 04:06:39 UTC",
      "updated_date": "2025-03-24 04:06:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:08:51.212589"
    },
    {
      "arxiv_id": "2503.18320v1",
      "title": "Bridging Writing Manner Gap in Visual Instruction Tuning by Creating LLM-aligned Instructions",
      "title_zh": "通过创建与LLM对齐的指令弥合视觉指令调优中的写作风格差距",
      "authors": [
        "Dong Jing",
        "Nanyi Fei",
        "Zhiwu Lu"
      ],
      "abstract": "In the realm of Large Multi-modal Models (LMMs), the instruction quality\nduring the visual instruction tuning stage significantly influences the\nperformance of modality alignment. In this paper, we assess the instruction\nquality from a unique perspective termed \\textbf{Writing Manner}, which\nencompasses the selection of vocabulary, grammar and sentence structure to\nconvey specific semantics. We argue that there exists a substantial writing\nmanner gap between the visual instructions and the base Large Language Models\n(LLMs) within LMMs. This gap forces the pre-trained base LLMs to deviate from\ntheir original writing styles, leading to capability degradation of both base\nLLMs and LMMs. To bridge the writing manner gap while preserving the original\nsemantics, we propose directly leveraging the base LLM to align the writing\nmanner of soft-format visual instructions with that of the base LLM itself,\nresulting in novel LLM-aligned instructions. The manual writing manner\nevaluation results demonstrate that our approach successfully minimizes the\nwriting manner gap. By utilizing LLM-aligned instructions, the baseline models\nLLaVA-7B and QwenVL demonstrate enhanced resistance to hallucinations and\nnon-trivial comprehensive improvements across all $15$ visual and language\nbenchmarks.",
      "tldr_zh": "该研究提出了一种通过创建与大型语言模型(LLM)对齐的指令来弥合视觉指令调优中写作方式差距的方法。研究指出，大型多模态模型(LMMs)中视觉指令与基础LLM之间存在显著的写作方式差距，导致模型性能下降。为此，作者提出直接利用基础LLM来对齐软格式视觉指令的写作方式，生成新的LLM对齐指令。实验表明，该方法有效缩小了写作方式差距，并在15个视觉和语言基准测试中显著提升了LLaVA-7B和QwenVL模型的抗幻觉能力和综合性能。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18320v1",
      "published_date": "2025-03-24 03:59:06 UTC",
      "updated_date": "2025-03-24 03:59:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:08:59.905337"
    },
    {
      "arxiv_id": "2503.18314v2",
      "title": "LoTUS: Large-Scale Machine Unlearning with a Taste of Uncertainty",
      "title_zh": "LoTUS：大规模机器遗忘的不确定性探索",
      "authors": [
        "Christoforos N. Spartalis",
        "Theodoros Semertzidis",
        "Efstratios Gavves",
        "Petros Daras"
      ],
      "abstract": "We present LoTUS, a novel Machine Unlearning (MU) method that eliminates the\ninfluence of training samples from pre-trained models, avoiding retraining from\nscratch. LoTUS smooths the prediction probabilities of the model up to an\ninformation-theoretic bound, mitigating its over-confidence stemming from data\nmemorization. We evaluate LoTUS on Transformer and ResNet18 models against\neight baselines across five public datasets. Beyond established MU benchmarks,\nwe evaluate unlearning on ImageNet1k, a large-scale dataset, where retraining\nis impractical, simulating real-world conditions. Moreover, we introduce the\nnovel Retrain-Free Jensen-Shannon Divergence (RF-JSD) metric to enable\nevaluation under real-world conditions. The experimental results show that\nLoTUS outperforms state-of-the-art methods in terms of both efficiency and\neffectiveness. Code: https://github.com/cspartalis/LoTUS.",
      "tldr_zh": "本文提出了LoTUS，一种新颖的机器遗忘(Machine Unlearning, MU)方法，能够从预训练模型中消除训练样本的影响，而无需从头重新训练。LoTUS通过平滑模型的预测概率，缓解因数据记忆导致的过度自信问题，并在信息论边界内优化性能。实验表明，LoTUS在多个数据集和模型上均优于现有方法，尤其是在大规模数据集ImageNet1k上表现突出。此外，作者还提出了新的Retrain-Free Jensen-Shannon Divergence (RF-JSD)指标，用于在真实场景下评估遗忘效果。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted as a main conference paper at CVPR 2025\n  (https://cvpr.thecvf.com/virtual/2025/poster/33292)",
      "pdf_url": "http://arxiv.org/pdf/2503.18314v2",
      "published_date": "2025-03-24 03:34:23 UTC",
      "updated_date": "2025-03-25 06:23:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:09:07.740306"
    },
    {
      "arxiv_id": "2503.18313v1",
      "title": "DeepFund: Will LLM be Professional at Fund Investment? A Live Arena Perspective",
      "title_zh": "DeepFund：LLM 能否成为基金投资专家？来自实战竞技场的视角",
      "authors": [
        "Changlun Li",
        "Yao Shi",
        "Yuyu Luo",
        "Nan Tang"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities across\nvarious domains, but their effectiveness in financial decision making,\nparticularly in fund investment, remains inadequately evaluated. Current\nbenchmarks primarily assess LLMs understanding of financial documents rather\nthan their ability to manage assets or analyze trading opportunities in dynamic\nmarket conditions. A critical limitation in existing evaluation methodologies\nis the backtesting approach, which suffers from information leakage when LLMs\nare evaluated on historical data they may have encountered during pretraining.\nThis paper introduces DeepFund, a comprehensive platform for evaluating LLM\nbased trading strategies in a simulated live environment. Our approach\nimplements a multi agent framework where LLMs serve as both analysts and\nmanagers, creating a realistic simulation of investment decision making. The\nplatform employs a forward testing methodology that mitigates information\nleakage by evaluating models on market data released after their training\ncutoff dates. We provide a web interface that visualizes model performance\nacross different market conditions and investment parameters, enabling detailed\ncomparative analysis. Through DeepFund, we aim to provide a more accurate and\nfair assessment of LLMs capabilities in fund investment, offering insights into\ntheir potential real world applications in financial markets.",
      "tldr_zh": "该研究提出了DeepFund平台，用于在模拟实时环境中评估大语言模型(LLMs)在基金投资中的表现。与传统的回测方法不同，DeepFund采用前瞻测试方法，使用模型训练截止日期后的市场数据进行评估，避免了信息泄露问题。平台采用多智能体框架，LLMs分别作为分析师和经理，模拟真实的投资决策过程，并通过可视化界面展示模型在不同市场条件下的表现，为LLMs在金融市场的实际应用潜力提供了更准确和公平的评估。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CE",
        "cs.HC"
      ],
      "primary_category": "cs.MA",
      "comment": "Work in progress",
      "pdf_url": "http://arxiv.org/pdf/2503.18313v1",
      "published_date": "2025-03-24 03:32:13 UTC",
      "updated_date": "2025-03-24 03:32:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:09:22.450441"
    },
    {
      "arxiv_id": "2503.18303v1",
      "title": "How to Capture and Study Conversations Between Research Participants and ChatGPT: GPT for Researchers (g4r.org)",
      "title_zh": "如何捕捉与研究参与者和 ChatGPT 的对话：面向研究者的 GPT（g4r.org）",
      "authors": [
        "Jin Kim"
      ],
      "abstract": "As large language models (LLMs) like ChatGPT become increasingly integrated\ninto our everyday lives--from customer service and education to creative work\nand personal productivity--understanding how people interact with these AI\nsystems has become a pressing issue. Despite the widespread use of LLMs,\nresearchers lack standardized tools for systematically studying people's\ninteractions with LLMs. To address this issue, we introduce GPT for Researchers\n(G4R), or g4r.org, a free website that researchers can use to easily create and\nintegrate a GPT Interface into their studies. At g4r.org, researchers can (1)\nenable their study participants to interact with GPT (such as ChatGPT), (2)\ncustomize GPT Interfaces to guide participants' interactions with GPT (e.g.,\nset constraints on topics or adjust GPT's tone or response style), and (3)\ncapture participants' interactions with GPT by downloading data on messages\nexchanged between participants and GPT. By facilitating study participants'\ninteractions with GPT and providing detailed data on these interactions, G4R\ncan support research on topics such as consumer interactions with AI agents or\nLLMs, AI-assisted decision-making, and linguistic patterns in human-AI\ncommunication. With this goal in mind, we provide a step-by-step guide to using\nG4R at g4r.org.",
      "tldr_zh": "该研究提出了GPT for Researchers (G4R, g4r.org)，一个免费网站，旨在帮助研究者系统化研究人们与大型语言模型（如ChatGPT）的交互。G4R允许研究者（1）让研究参与者与GPT互动，（2）定制GPT界面以引导互动（如设置主题限制或调整GPT的语气），（3）捕获并下载参与者与GPT的对话数据。该工具支持研究人类与AI代理的交互、AI辅助决策以及人机沟通中的语言模式，为相关研究提供了标准化工具和数据支持。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18303v1",
      "published_date": "2025-03-24 03:10:12 UTC",
      "updated_date": "2025-03-24 03:10:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:09:32.328650"
    },
    {
      "arxiv_id": "2503.18302v1",
      "title": "DiffMove: Group Mobility Tendency Enhanced Trajectory Recovery via Diffusion Model",
      "title_zh": "DiffMove：基于扩散模型的群体移动趋势增强轨迹恢复",
      "authors": [
        "Qingyue Long",
        "Can Rong",
        "Huandong Wang",
        "Shaw Rajib",
        "Yong Li"
      ],
      "abstract": "In the real world, trajectory data is often sparse and incomplete due to low\ncollection frequencies or limited device coverage. Trajectory recovery aims to\nrecover these missing trajectory points, making the trajectories denser and\nmore complete. However, this task faces two key challenges: 1) The excessive\nsparsity of individual trajectories makes it difficult to effectively leverage\nhistorical information for recovery; 2) Sparse trajectories make it harder to\ncapture complex individual mobility preferences. To address these challenges,\nwe propose a novel method called DiffMove. Firstly, we harness crowd wisdom for\ntrajectory recovery. Specifically, we construct a group tendency graph using\nthe collective trajectories of all users and then integrate the group mobility\ntrends into the location representations via graph embedding. This solves the\nchallenge of sparse trajectories being unable to rely on individual historical\ntrajectories for recovery. Secondly, we capture individual mobility preferences\nfrom both historical and current perspectives. Finally, we integrate group\nmobility tendencies and individual preferences into the spatiotemporal\ndistribution of the trajectory to recover high-quality trajectories. Extensive\nexperiments on two real-world datasets demonstrate that DiffMove outperforms\nexisting state-of-the-art methods. Further analysis validates the robustness of\nour method.",
      "tldr_zh": "本文提出了一种名为DiffMove的新方法，用于解决轨迹数据稀疏和不完整的问题。该方法通过构建群体倾向图，利用所有用户的集体轨迹信息，将群体移动趋势融入位置表示中，从而弥补个体轨迹稀疏的不足。同时，DiffMove从历史和当前角度捕捉个体移动偏好，并将群体趋势与个体偏好结合到轨迹的时空分布中，实现高质量轨迹恢复。实验表明，DiffMove在两个真实数据集上优于现有最先进方法，验证了其鲁棒性。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18302v1",
      "published_date": "2025-03-24 03:08:21 UTC",
      "updated_date": "2025-03-24 03:08:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:09:34.662434"
    },
    {
      "arxiv_id": "2503.18290v1",
      "title": "When is dataset cartography ineffective? Using training dynamics does not improve robustness against Adversarial SQuAD",
      "title_zh": "数据集制图何时失效？利用训练动态无法提升对Adversarial SQuAD的鲁棒性",
      "authors": [
        "Paul K. Mandal"
      ],
      "abstract": "In this paper, I investigate the effectiveness of dataset cartography for\nextractive question answering on the SQuAD dataset. I begin by analyzing\nannotation artifacts in SQuAD and evaluate the impact of two adversarial\ndatasets, AddSent and AddOneSent, on an ELECTRA-small model. Using training\ndynamics, I partition SQuAD into easy-to-learn, ambiguous, and hard-to-learn\nsubsets. I then compare the performance of models trained on these subsets to\nthose trained on randomly selected samples of equal size. Results show that\ntraining on cartography-based subsets does not improve generalization to the\nSQuAD validation set or the AddSent adversarial set. While the hard-to-learn\nsubset yields a slightly higher F1 score on the AddOneSent dataset, the overall\ngains are limited. These findings suggest that dataset cartography provides\nlittle benefit for adversarial robustness in SQuAD-style QA tasks. I conclude\nby comparing these results to prior findings on SNLI and discuss possible\nreasons for the observed differences.",
      "tldr_zh": "本文研究了数据集制图（dataset cartography）在SQuAD数据集上的抽取式问答任务中的有效性。通过分析SQuAD的标注偏差，并利用训练动态将数据集划分为易学、模糊和难学子集，研究发现基于制图的子集训练并未显著提升模型在SQuAD验证集和AddSent对抗集上的泛化性能。尽管难学子集在AddOneSent对抗集上表现略优，但整体改进有限。结果表明，数据集制图在提升SQuAD风格问答任务的对抗鲁棒性方面作用甚微。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7; I.2.6; I.5.1"
      ],
      "primary_category": "cs.CL",
      "comment": "5 pages, 3 figures, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.18290v1",
      "published_date": "2025-03-24 02:24:18 UTC",
      "updated_date": "2025-03-24 02:24:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:10:20.256270"
    },
    {
      "arxiv_id": "2503.18283v1",
      "title": "Voxel-based Point Cloud Geometry Compression with Space-to-Channel Context",
      "title_zh": "基于空间到通道体素点云几何压缩方法",
      "authors": [
        "Bojun Liu",
        "Yangzhi Ma",
        "Ao Luo",
        "Li Li",
        "Dong Liu"
      ],
      "abstract": "Voxel-based methods are among the most efficient for point cloud geometry\ncompression, particularly with dense point clouds. However, they face\nlimitations due to a restricted receptive field, especially when handling\nhigh-bit depth point clouds. To overcome this issue, we introduce a stage-wise\nSpace-to-Channel (S2C) context model for both dense point clouds and low-level\nsparse point clouds. This model utilizes a channel-wise autoregressive strategy\nto effectively integrate neighborhood information at a coarse resolution. For\nhigh-level sparse point clouds, we further propose a level-wise S2C context\nmodel that addresses resolution limitations by incorporating Geometry Residual\nCoding (GRC) for consistent-resolution cross-level prediction. Additionally, we\nuse the spherical coordinate system for its compact representation and enhance\nour GRC approach with a Residual Probability Approximation (RPA) module, which\nfeatures a large kernel size. Experimental results show that our S2C context\nmodel not only achieves bit savings while maintaining or improving\nreconstruction quality but also reduces computational complexity compared to\nstate-of-the-art voxel-based compression methods.",
      "tldr_zh": "本研究提出了一种基于空间到通道(Space-to-Channel, S2C)上下文的体素化点云几何压缩方法，解决了现有方法在处理高比特深度点云时感受野受限的问题。针对密集点云和低层稀疏点云，采用分阶段的S2C上下文模型，通过通道自回归策略有效整合粗分辨率下的邻域信息；对于高层稀疏点云，提出层级S2C上下文模型，结合几何残差编码(GRC)实现跨层级预测。此外，利用球坐标系进行紧凑表示，并通过大核残差概率近似(RPA)模块增强GRC。实验表明，该方法在保持或提升重建质量的同时节省比特率，并降低了计算复杂度。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.18283v1",
      "published_date": "2025-03-24 01:56:08 UTC",
      "updated_date": "2025-03-24 01:56:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:10:03.529890"
    },
    {
      "arxiv_id": "2503.18278v1",
      "title": "TopV: Compatible Token Pruning with Inference Time Optimization for Fast and Low-Memory Multimodal Vision Language Model",
      "title_zh": "TopV：兼容令牌剪枝与推理时间优化，实现快速低内存多模态视觉语言模型",
      "authors": [
        "Cheng Yang",
        "Yang Sui",
        "Jinqi Xiao",
        "Lingyi Huang",
        "Yu Gong",
        "Chendi Li",
        "Jinghua Yan",
        "Yu Bai",
        "Ponnuswamy Sadayappan",
        "Xia Hu",
        "Bo Yuan"
      ],
      "abstract": "Vision-Language Models (VLMs) demand substantial computational resources\nduring inference, largely due to the extensive visual input tokens for\nrepresenting visual information. Previous studies have noted that visual tokens\ntend to receive less attention than text tokens, suggesting their lower\nimportance during inference and potential for pruning. However, their methods\nencounter several challenges: reliance on greedy heuristic criteria for token\nimportance and incompatibility with FlashAttention and KV cache. To address\nthese issues, we introduce \\textbf{TopV}, a compatible \\textbf{TO}ken\n\\textbf{P}runing with inference Time Optimization for fast and low-memory\n\\textbf{V}LM, achieving efficient pruning without additional training or\nfine-tuning. Instead of relying on attention scores, we formulate token pruning\nas an optimization problem, accurately identifying important visual tokens\nwhile remaining compatible with FlashAttention. Additionally, since we only\nperform this pruning once during the prefilling stage, it effectively reduces\nKV cache size. Our optimization framework incorporates a visual-aware cost\nfunction considering factors such as Feature Similarity, Relative Spatial\nDistance, and Absolute Central Distance, to measure the importance of each\nsource visual token, enabling effective pruning of low-importance tokens.\nExtensive experiments demonstrate that our method outperforms previous token\npruning methods, validating the effectiveness and efficiency of our approach.",
      "tldr_zh": "该研究提出了TopV，一种兼容FlashAttention和KV cache的视觉令牌剪枝方法，旨在优化多模态视觉语言模型(VLMs)的推理速度和内存效率。与传统依赖注意力分数的剪枝方法不同，TopV将令牌剪枝建模为优化问题，通过综合考虑特征相似度、相对空间距离和绝对中心距离等视觉感知因素，准确识别重要视觉令牌。实验表明，TopV无需额外训练即可实现高效剪枝，显著减少KV缓存大小，并在性能上优于现有剪枝方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.18278v1",
      "published_date": "2025-03-24 01:47:26 UTC",
      "updated_date": "2025-03-24 01:47:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:09:59.661777"
    },
    {
      "arxiv_id": "2503.18265v1",
      "title": "Risk Management for Distributed Arbitrage Systems: Integrating Artificial Intelligence",
      "title_zh": "分布式套利系统的风险管理：人工智能的整合",
      "authors": [
        "Akaash Vishal Hazarika",
        "Mahak Shah",
        "Swapnil Patil",
        "Pradyumna Shukla"
      ],
      "abstract": "Effective risk management solutions become absolutely crucial when financial\nmarkets embrace distributed technology and decentralized financing (DeFi). This\nstudy offers a thorough survey and comparative analysis of the integration of\nartificial intelligence (AI) in risk management for distributed arbitrage\nsystems. We examine several modern caching techniques namely in memory caching,\ndistributed caching, and proxy caching and their functions in enhancing\nperformance in decentralized settings. Through literature review we examine the\nutilization of AI techniques for alleviating risks related to market\nvolatility, liquidity challenges, operational failures, regulatory compliance,\nand security threats. This comparison research evaluates various case studies\nfrom prominent DeFi technologies, emphasizing critical performance metrics like\nlatency reduction, load balancing, and system resilience. Additionally, we\nexamine the problems and trade offs associated with these technologies,\nemphasizing their effects on consistency, scalability, and fault tolerance. By\nmeticulously analyzing real world applications, specifically centering on the\nAave platform as our principal case study, we illustrate how the purposeful\namalgamation of AI with contemporary caching methodologies has revolutionized\nrisk management in distributed arbitrage systems.",
      "tldr_zh": "本研究探讨了人工智能（AI）在分布式套利系统风险管理中的应用，重点分析了现代缓存技术（如内存缓存、分布式缓存和代理缓存）在去中心化环境中的性能提升作用。通过文献综述和案例研究，特别是以Aave平台为例，研究评估了AI技术在缓解市场波动、流动性问题、操作故障、合规性和安全威胁方面的效果。研究还强调了这些技术在延迟减少、负载均衡和系统弹性等关键性能指标上的表现，并讨论了其一致性、可扩展性和容错性方面的挑战与权衡。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.LG",
        "I.2.11; G.3"
      ],
      "primary_category": "cs.DC",
      "comment": "International Conference on AI and Financial Innovation AIFI-2025",
      "pdf_url": "http://arxiv.org/pdf/2503.18265v1",
      "published_date": "2025-03-24 01:15:43 UTC",
      "updated_date": "2025-03-24 01:15:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:10:07.168002"
    },
    {
      "arxiv_id": "2503.18258v1",
      "title": "Severing Spurious Correlations with Data Pruning",
      "title_zh": "通过数据剪枝切断虚假相关性",
      "authors": [
        "Varun Mulchandani",
        "Jung-Eun Kim"
      ],
      "abstract": "Deep neural networks have been shown to learn and rely on spurious\ncorrelations present in the data that they are trained on. Reliance on such\ncorrelations can cause these networks to malfunction when deployed in the real\nworld, where these correlations may no longer hold. To overcome the learning of\nand reliance on such correlations, recent studies propose approaches that yield\npromising results. These works, however, study settings where the strength of\nthe spurious signal is significantly greater than that of the core, invariant\nsignal, making it easier to detect the presence of spurious features in\nindividual training samples and allow for further processing. In this paper, we\nidentify new settings where the strength of the spurious signal is relatively\nweaker, making it difficult to detect any spurious information while continuing\nto have catastrophic consequences. We also discover that spurious correlations\nare learned primarily due to only a handful of all the samples containing the\nspurious feature and develop a novel data pruning technique that identifies and\nprunes small subsets of the training data that contain these samples. Our\nproposed technique does not require inferred domain knowledge, information\nregarding the sample-wise presence or nature of spurious information, or human\nintervention. Finally, we show that such data pruning attains state-of-the-art\nperformance on previously studied settings where spurious information is\nidentifiable.",
      "tldr_zh": "该研究针对深度神经网络在训练中依赖虚假相关性的问题，提出了一种新的数据剪枝技术。研究发现，虚假相关性主要由少数包含虚假特征的样本引起，因此开发了一种无需领域知识或人工干预的方法，自动识别并剪除这些样本。实验表明，该技术在虚假信息较难检测但影响严重的场景中表现优异，并在现有虚假信息可识别场景中达到了最先进的性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "ICLR 2025, Spotlight",
      "pdf_url": "http://arxiv.org/pdf/2503.18258v1",
      "published_date": "2025-03-24 00:57:32 UTC",
      "updated_date": "2025-03-24 00:57:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:10:23.656752"
    },
    {
      "arxiv_id": "2503.18255v1",
      "title": "The Human-Machine Identity Blur: A Unified Framework for Cybersecurity Risk Management in 2025",
      "title_zh": "人机身份模糊：2025年网络安全风险管理的统一框架",
      "authors": [
        "Kush Janani"
      ],
      "abstract": "The modern enterprise is facing an unprecedented surge in digital identities,\nwith machine identities now significantly outnumbering human identities. This\npaper examines the cybersecurity risks emerging from what we define as the\n\"human-machine identity blur\" - the point at which human and machine identities\nintersect, delegate authority, and create new attack surfaces. Drawing from\nindustry data, expert insights, and real-world incident analysis, we identify\nkey governance gaps in current identity management models that treat human and\nmachine entities as separate domains. To address these challenges, we propose a\nUnified Identity Governance Framework based on four core principles: treating\nidentity as a continuum rather than a binary distinction, applying consistent\nrisk evaluation across all identity types, implementing continuous verification\nguided by zero trust principles, and maintaining governance throughout the\nentire identity lifecycle. Our research shows that organizations adopting this\nunified approach experience a 47 percent reduction in identity-related security\nincidents and a 62 percent improvement in incident response time. We conclude\nby offering a practical implementation roadmap and outlining future research\ndirections as AI-driven systems become increasingly autonomous.",
      "tldr_zh": "本文提出了一种统一身份治理框架，以应对2025年网络安全风险管理中“人机身份模糊”带来的挑战。该框架基于四个核心原则：将身份视为连续体而非二元划分、对所有身份类型应用一致的风险评估、实施基于零信任原则的持续验证、以及在整个身份生命周期中保持治理。研究表明，采用该统一框架的组织可将身份相关安全事件减少47%，并将事件响应时间提升62%。研究还提供了实际实施路线图，并展望了AI驱动系统日益自主化背景下的未来研究方向。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "9 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.18255v1",
      "published_date": "2025-03-24 00:37:14 UTC",
      "updated_date": "2025-03-24 00:37:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-25T22:10:29.375361"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 87,
  "processed_papers_count": 86,
  "failed_papers_count": 1,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-03-25T23:07:45.485551"
}
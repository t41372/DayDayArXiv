[
  {
    "arxiv_id": "2409.14630v1",
    "title": "EQ-CBM: A Probabilistic Concept Bottleneck with Energy-based Models and Quantized Vectors",
    "authors": [
      "Sangwon Kim",
      "Dasom Ahn",
      "Byoung Chul Ko",
      "In-su Jang",
      "Kwang-Ju Kim"
    ],
    "abstract": "The demand for reliable AI systems has intensified the need for interpretable\ndeep neural networks. Concept bottleneck models (CBMs) have gained attention as\nan effective approach by leveraging human-understandable concepts to enhance\ninterpretability. However, existing CBMs face challenges due to deterministic\nconcept encoding and reliance on inconsistent concepts, leading to\ninaccuracies. We propose EQ-CBM, a novel framework that enhances CBMs through\nprobabilistic concept encoding using energy-based models (EBMs) with quantized\nconcept activation vectors (qCAVs). EQ-CBM effectively captures uncertainties,\nthereby improving prediction reliability and accuracy. By employing qCAVs, our\nmethod selects homogeneous vectors during concept encoding, enabling more\ndecisive task performance and facilitating higher levels of human intervention.\nEmpirical results using benchmark datasets demonstrate that our approach\noutperforms the state-of-the-art in both concept and task accuracy.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ACCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.14630v1",
    "published_date": "2024-09-22 23:43:45 UTC",
    "updated_date": "2024-09-22 23:43:45 UTC"
  },
  {
    "arxiv_id": "2409.14622v4",
    "title": "LatentQGAN: A Hybrid QGAN with Classical Convolutional Autoencoder",
    "authors": [
      "Alexis Vieloszynski",
      "Soumaya Cherkaoui",
      "Ola Ahmad",
      "Jean-Frédéric Laprade",
      "Oliver Nahman-Lévesque",
      "Abdallah Aaraba",
      "Shengrui Wang"
    ],
    "abstract": "Quantum machine learning consists in taking advantage of quantum computations\nto generate classical data. A potential application of quantum machine learning\nis to harness the power of quantum computers for generating classical data, a\nprocess essential to a multitude of applications such as enriching training\ndatasets, anomaly detection, and risk management in finance. Given the success\nof Generative Adversarial Networks in classical image generation, the\ndevelopment of its quantum versions has been actively conducted. However,\nexisting implementations on quantum computers often face significant\nchallenges, such as scalability and training convergence issues. To address\nthese issues, we propose LatentQGAN, a novel quantum model that uses a hybrid\nquantum-classical GAN coupled with an autoencoder. Although it was initially\ndesigned for image generation, the LatentQGAN approach holds potential for\nbroader application across various practical data generation tasks.\nExperimental outcomes on both classical simulators and noisy intermediate scale\nquantum computers have demonstrated significant performance enhancements over\nexisting quantum methods, alongside a significant reduction in quantum\nresources overhead.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "quant-ph",
    "comment": "This paper was accepted for publication on the 10th IEEE World Forum\n  on Internet of Things (IEEE WFIoT2024), in the session SS - QIoT-1: Special\n  Session - Quantum Internet of Things (QIoT)-1, November 10th, from 14:00 to\n  15:30 EST",
    "pdf_url": "http://arxiv.org/pdf/2409.14622v4",
    "published_date": "2024-09-22 23:18:06 UTC",
    "updated_date": "2024-11-19 21:44:26 UTC"
  },
  {
    "arxiv_id": "2409.14603v1",
    "title": "Brain Surgery: Ensuring GDPR Compliance in Large Language Models via Concept Erasure",
    "authors": [
      "Michele Laurelli"
    ],
    "abstract": "As large-scale AI systems proliferate, ensuring compliance with data privacy\nlaws such as the General Data Protection Regulation (GDPR) has become critical.\nThis paper introduces Brain Surgery, a transformative methodology for making\nevery local AI model GDPR-ready by enabling real-time privacy management and\ntargeted unlearning. Building on advanced techniques such as\nEmbedding-Corrupted Prompts (ECO Prompts), blockchain-based privacy management,\nand privacy-aware continual learning, Brain Surgery provides a modular solution\nthat can be deployed across various AI architectures. This tool not only\nensures compliance with privacy regulations but also empowers users to define\ntheir own privacy limits, creating a new paradigm in AI ethics and governance.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.14603v1",
    "published_date": "2024-09-22 21:42:20 UTC",
    "updated_date": "2024-09-22 21:42:20 UTC"
  },
  {
    "arxiv_id": "2409.14602v2",
    "title": "Can pre-trained language models generate titles for research papers?",
    "authors": [
      "Tohida Rehman",
      "Debarshi Kumar Sanyal",
      "Samiran Chattopadhyay"
    ],
    "abstract": "The title of a research paper communicates in a succinct style the main theme\nand, sometimes, the findings of the paper. Coming up with the right title is\noften an arduous task, and therefore, it would be beneficial to authors if\ntitle generation can be automated. In this paper, we fine-tune pre-trained\nlanguage models to generate titles of papers from their abstracts.\nAdditionally, we use GPT-3.5-turbo in a zero-shot setting to generate paper\ntitles. The performance of the models is measured with ROUGE, METEOR,\nMoverScore, BERTScore and SciBERTScore metrics. We find that fine-tuned\nPEGASUS-large outperforms the other models, including fine-tuned LLaMA-3-8B and\nGPT-3.5-turbo, across most metrics. We also demonstrate that ChatGPT can\ngenerate creative titles for papers. Our observations suggest that AI-generated\npaper titles are generally accurate and appropriate.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.14602v2",
    "published_date": "2024-09-22 21:34:49 UTC",
    "updated_date": "2024-10-13 18:35:04 UTC"
  },
  {
    "arxiv_id": "2409.14593v1",
    "title": "Testing Causal Models with Hidden Variables in Polynomial Delay via Conditional Independencies",
    "authors": [
      "Hyunchai Jeong",
      "Adiba Ejaz",
      "Jin Tian",
      "Elias Bareinboim"
    ],
    "abstract": "Testing a hypothesized causal model against observational data is a key\nprerequisite for many causal inference tasks. A natural approach is to test\nwhether the conditional independence relations (CIs) assumed in the model hold\nin the data. While a model can assume exponentially many CIs (with respect to\nthe number of variables), testing all of them is both impractical and\nunnecessary. Causal graphs, which encode these CIs in polynomial space, give\nrise to local Markov properties that enable model testing with a significantly\nsmaller subset of CIs. Model testing based on local properties requires an\nalgorithm to list the relevant CIs. However, existing algorithms for realistic\nsettings with hidden variables and non-parametric distributions can take\nexponential time to produce even a single CI constraint. In this paper, we\nintroduce the c-component local Markov property (C-LMP) for causal graphs with\nhidden variables. Since C-LMP can still invoke an exponential number of CIs, we\ndevelop a polynomial delay algorithm to list these CIs in poly-time intervals.\nTo our knowledge, this is the first algorithm that enables poly-delay testing\nof CIs in causal graphs with hidden variables against arbitrary data\ndistributions. Experiments on real-world and synthetic data demonstrate the\npracticality of our algorithm.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ME",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "34 total pages, 14 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.14593v1",
    "published_date": "2024-09-22 21:05:56 UTC",
    "updated_date": "2024-09-22 21:05:56 UTC"
  },
  {
    "arxiv_id": "2409.14590v3",
    "title": "Explainable AI needs formal notions of explanation correctness",
    "authors": [
      "Stefan Haufe",
      "Rick Wilming",
      "Benedict Clark",
      "Rustam Zhumagambetov",
      "Danny Panknin",
      "Ahcène Boubekki"
    ],
    "abstract": "The use of machine learning (ML) in critical domains such as medicine poses\nrisks and requires regulation. One requirement is that decisions of ML systems\nin high-risk applications should be human-understandable. The field of\n\"explainable artificial intelligence\" (XAI) seemingly addresses this need.\nHowever, in its current form, XAI is unfit to provide quality control for ML;\nit itself needs scrutiny. Popular XAI methods cannot reliably answer important\nquestions about ML models, their training data, or a given test input. We\nrecapitulate results demonstrating that popular XAI methods systematically\nattribute importance to input features that are independent of the prediction\ntarget. This limits their utility for purposes such as model and data\n(in)validation, model improvement, and scientific discovery. We argue that the\nfundamental reason for this limitation is that current XAI methods do not\naddress well-defined problems and are not evaluated against objective criteria\nof explanation correctness. Researchers should formally define the problems\nthey intend to solve first and then design methods accordingly. This will lead\nto notions of explanation correctness that can be theoretically verified and\nobjective metrics of explanation performance that can be assessed using\nground-truth data.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.14590v3",
    "published_date": "2024-09-22 20:47:04 UTC",
    "updated_date": "2024-11-23 23:02:49 UTC"
  },
  {
    "arxiv_id": "2409.14586v1",
    "title": "Backtracking Improves Generation Safety",
    "authors": [
      "Yiming Zhang",
      "Jianfeng Chi",
      "Hailey Nguyen",
      "Kartikeya Upasani",
      "Daniel M. Bikel",
      "Jason Weston",
      "Eric Michael Smith"
    ],
    "abstract": "Text generation has a fundamental limitation almost by definition: there is\nno taking back tokens that have been generated, even when they are clearly\nproblematic. In the context of language model safety, when a partial unsafe\ngeneration is produced, language models by their nature tend to happily keep on\ngenerating similarly unsafe additional text. This is in fact how safety\nalignment of frontier models gets circumvented in the wild, despite great\nefforts in improving their safety. Deviating from the paradigm of approaching\nsafety alignment as prevention (decreasing the probability of harmful\nresponses), we propose backtracking, a technique that allows language models to\n\"undo\" and recover from their own unsafe generation through the introduction of\na special [RESET] token. Our method can be incorporated into either SFT or DPO\ntraining to optimize helpfulness and harmlessness. We show that models trained\nto backtrack are consistently safer than baseline models: backtracking\nLlama-3-8B is four times more safe than the baseline model (6.1\\% $\\to$ 1.5\\%)\nin our evaluations without regression in helpfulness. Our method additionally\nprovides protection against four adversarial attacks including an adaptive\nattack, despite not being trained to do so.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.14586v1",
    "published_date": "2024-09-22 20:28:40 UTC",
    "updated_date": "2024-09-22 20:28:40 UTC"
  },
  {
    "arxiv_id": "2409.14583v3",
    "title": "Evaluating Gender, Racial, and Age Biases in Large Language Models: A Comparative Analysis of Occupational and Crime Scenarios",
    "authors": [
      "Vishal Mirza",
      "Rahul Kulkarni",
      "Aakanksha Jadhav"
    ],
    "abstract": "Recent advancements in Large Language Models(LLMs) have been notable, yet\nwidespread enterprise adoption remains limited due to various constraints. This\npaper examines bias in LLMs-a crucial issue affecting their usability,\nreliability, and fairness. Researchers are developing strategies to mitigate\nbias, including debiasing layers, specialized reference datasets like\nWinogender and Winobias, and reinforcement learning with human feedback (RLHF).\nThese techniques have been integrated into the latest LLMs. Our study evaluates\ngender bias in occupational scenarios and gender, age, and racial bias in crime\nscenarios across four leading LLMs released in 2024: Gemini 1.5 Pro, Llama 3\n70B, Claude 3 Opus, and GPT-4o. Findings reveal that LLMs often depict female\ncharacters more frequently than male ones in various occupations, showing a 37%\ndeviation from US BLS data. In crime scenarios, deviations from US FBI data are\n54% for gender, 28% for race, and 17% for age. We observe that efforts to\nreduce gender and racial bias often lead to outcomes that may over-index one\nsub-class, potentially exacerbating the issue. These results highlight the\nlimitations of current bias mitigation techniques and underscore the need for\nmore effective approaches.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "11 pages, 17 figures, Accepted at IEEE Conference on Artificial\n  Intelligence (IEEE CAI) 2025. Full Paper acceptance in the Vertical\n  HUMAN-CENTERED AI category",
    "pdf_url": "http://arxiv.org/pdf/2409.14583v3",
    "published_date": "2024-09-22 20:21:20 UTC",
    "updated_date": "2025-03-30 01:41:39 UTC"
  },
  {
    "arxiv_id": "2409.14572v2",
    "title": "Evaluating the Performance and Robustness of LLMs in Materials Science Q&A and Property Predictions",
    "authors": [
      "Hongchen Wang",
      "Kangming Li",
      "Scott Ramsay",
      "Yao Fehlis",
      "Edward Kim",
      "Jason Hattrick-Simpers"
    ],
    "abstract": "Large Language Models (LLMs) have the potential to revolutionize scientific\nresearch, yet their robustness and reliability in domain-specific applications\nremain insufficiently explored. In this study, we evaluate the performance and\nrobustness of LLMs for materials science, focusing on domain-specific question\nanswering and materials property prediction across diverse real-world and\nadversarial conditions. Three distinct datasets are used in this study: 1) a\nset of multiple-choice questions from undergraduate-level materials science\ncourses, 2) a dataset including various steel compositions and yield strengths,\nand 3) a band gap dataset, containing textual descriptions of material crystal\nstructures and band gap values. The performance of LLMs is assessed using\nvarious prompting strategies, including zero-shot chain-of-thought, expert\nprompting, and few-shot in-context learning. The robustness of these models is\ntested against various forms of 'noise', ranging from realistic disturbances to\nintentionally adversarial manipulations, to evaluate their resilience and\nreliability under real-world conditions. Additionally, the study showcases\nunique phenomena of LLMs during predictive tasks, such as mode collapse\nbehavior when the proximity of prompt examples is altered and performance\nrecovery from train/test mismatch. The findings aim to provide informed\nskepticism for the broad use of LLMs in materials science and to inspire\nadvancements that enhance their robustness and reliability for practical\napplications.",
    "categories": [
      "cs.CL",
      "cond-mat.mtrl-sci",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.14572v2",
    "published_date": "2024-09-22 19:31:16 UTC",
    "updated_date": "2025-03-11 22:03:26 UTC"
  },
  {
    "arxiv_id": "2409.14571v1",
    "title": "Encoder with the Empirical Mode Decomposition (EMD) to remove muscle artefacts from EEG signal",
    "authors": [
      "Ildar Rakhmatulin"
    ],
    "abstract": "This paper introduces a novel method for effectively removing artifacts from\nEEG signals by combining the Empirical Mode Decomposition (EMD) method with a\nmachine learning architecture. The proposed method addresses the limitations of\nexisting artifact removal techniques by enhancing the EMD method through\ninterpolation of the upper and lower. For conventional artifact removal\nmethods, the EMD technique is commonly employed. However, the challenge lies in\naccurately interpolating the missing components of the signal while preserving\nits inherent frequency components. To overcome this limitation, we incorporated\nmachine learning technique, which enables us to carefully handle the\ninterpolation process without directly manipulating the data. The key advantage\nof our approach lies in the preservation of the natural characteristics of the\nEEG signal during artifact removal. By utilizing machine learning for\ninterpolation, we ensure that the average component obtained through the EMD\nmethod retains the crucial frequency components of the original signal. This\npreservation is essential for maintaining the integrity and fidelity of the EEG\ndata, allowing for accurate analysis and interpretation. The results obtained\nfrom our evaluation serve to validate the effectiveness of our approach and\npave the way for further advancements in EEG signal processing and analysis.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.14571v1",
    "published_date": "2024-09-22 19:22:22 UTC",
    "updated_date": "2024-09-22 19:22:22 UTC"
  },
  {
    "arxiv_id": "2409.14556v2",
    "title": "RACOON: An LLM-based Framework for Retrieval-Augmented Column Type Annotation with a Knowledge Graph",
    "authors": [
      "Lindsey Linxi Wei",
      "Guorui Xiao",
      "Magdalena Balazinska"
    ],
    "abstract": "As an important component of data exploration and integration, Column Type\nAnnotation (CTA) aims to label columns of a table with one or more semantic\ntypes. With the recent development of Large Language Models (LLMs), researchers\nhave started to explore the possibility of using LLMs for CTA, leveraging their\nstrong zero-shot capabilities. In this paper, we build on this promising work\nand improve on LLM-based methods for CTA by showing how to use a Knowledge\nGraph (KG) to augment the context information provided to the LLM. Our\napproach, called RACOON, combines both pre-trained parametric and\nnon-parametric knowledge during generation to improve LLMs' performance on CTA.\nOur experiments show that RACOON achieves up to a 0.21 micro F-1 improvement\ncompared against vanilla LLM inference.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.14556v2",
    "published_date": "2024-09-22 18:39:27 UTC",
    "updated_date": "2024-11-01 01:15:51 UTC"
  },
  {
    "arxiv_id": "2409.14552v2",
    "title": "Unleashing the Power of Emojis in Texts via Self-supervised Graph Pre-Training",
    "authors": [
      "Zhou Zhang",
      "Dongzeng Tan",
      "Jiaan Wang",
      "Yilong Chen",
      "Jiarong Xu"
    ],
    "abstract": "Emojis have gained immense popularity on social platforms, serving as a\ncommon means to supplement or replace text. However, existing data mining\napproaches generally either completely ignore or simply treat emojis as\nordinary Unicode characters, which may limit the model's ability to grasp the\nrich semantic information in emojis and the interaction between emojis and\ntexts. Thus, it is necessary to release the emoji's power in social media data\nmining. To this end, we first construct a heterogeneous graph consisting of\nthree types of nodes, i.e. post, word and emoji nodes to improve the\nrepresentation of different elements in posts. The edges are also well-defined\nto model how these three elements interact with each other. To facilitate the\nsharing of information among post, word and emoji nodes, we propose a graph\npre-train framework for text and emoji co-modeling, which contains two graph\npre-training tasks: node-level graph contrastive learning and edge-level link\nreconstruction learning. Extensive experiments on the Xiaohongshu and Twitter\ndatasets with two types of downstream tasks demonstrate that our approach\nproves significant improvement over previous strong baseline methods.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by EMNLP 2024 Main Conference",
    "pdf_url": "http://arxiv.org/pdf/2409.14552v2",
    "published_date": "2024-09-22 18:29:10 UTC",
    "updated_date": "2024-09-26 02:02:13 UTC"
  },
  {
    "arxiv_id": "2409.14545v5",
    "title": "Why Is Anything Conscious?",
    "authors": [
      "Michael Timothy Bennett",
      "Sean Welsh",
      "Anna Ciaunica"
    ],
    "abstract": "We tackle the hard problem of consciousness taking the naturally selected,\nembodied organism as our starting point. We provide a formalism describing how\nbiological systems self-organise to hierarchically interpret unlabelled sensory\ninformation according to valence. Such interpretations imply behavioural\npolicies which are differentiated from each other only by the qualitative\naspect of information processing. Natural selection favours systems that\nintervene in the world to achieve homeostatic and reproductive goals. Quality\nis a property arising in such systems to link cause to affect to motivate\ninterventions. This produces interoceptive and exteroceptive classifiers and\ndetermines priorities. In formalising the seminal distinction between access\nand phenomenal consciousness, we claim that access consciousness at the human\nlevel requires the ability to hierarchically model i) the self, ii) the\nworld/others and iii) the self as modelled by others, and that this requires\nphenomenal consciousness. Phenomenal without access consciousness is likely\ncommon, but the reverse is implausible. To put it provocatively: death grounds\nmeaning, and Nature does not like zombies. We then describe the multilayered\narchitecture of self-organisation from rocks to Einstein, illustrating how our\nargument applies. Our proposal lays the foundation of a formal science of\nconsciousness, closer to human fact than zombie fiction.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.14545v5",
    "published_date": "2024-09-22 18:01:30 UTC",
    "updated_date": "2025-04-15 04:05:34 UTC"
  },
  {
    "arxiv_id": "2409.14543v1",
    "title": "TrackNetV4: Enhancing Fast Sports Object Tracking with Motion Attention Maps",
    "authors": [
      "Arjun Raj",
      "Lei Wang",
      "Tom Gedeon"
    ],
    "abstract": "Accurately detecting and tracking high-speed, small objects, such as balls in\nsports videos, is challenging due to factors like motion blur and occlusion.\nAlthough recent deep learning frameworks like TrackNetV1, V2, and V3 have\nadvanced tennis ball and shuttlecock tracking, they often struggle in scenarios\nwith partial occlusion or low visibility. This is primarily because these\nmodels rely heavily on visual features without explicitly incorporating motion\ninformation, which is crucial for precise tracking and trajectory prediction.\nIn this paper, we introduce an enhancement to the TrackNet family by fusing\nhigh-level visual features with learnable motion attention maps through a\nmotion-aware fusion mechanism, effectively emphasizing the moving ball's\nlocation and improving tracking performance. Our approach leverages frame\ndifferencing maps, modulated by a motion prompt layer, to highlight key motion\nregions over time. Experimental results on the tennis ball and shuttlecock\ndatasets show that our method enhances the tracking performance of both\nTrackNetV2 and V3. We refer to our lightweight, plug-and-play solution, built\non top of the existing TrackNet, as TrackNetV4.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Research report",
    "pdf_url": "http://arxiv.org/pdf/2409.14543v1",
    "published_date": "2024-09-22 17:58:09 UTC",
    "updated_date": "2024-09-22 17:58:09 UTC"
  },
  {
    "arxiv_id": "2409.14516v1",
    "title": "Beyond Words: Evaluating Large Language Models in Transportation Planning",
    "authors": [
      "Shaowei Ying",
      "Zhenlong Li",
      "Manzhu Yu"
    ],
    "abstract": "The resurgence and rapid advancement of Generative Artificial Intelligence\n(GenAI) in 2023 has catalyzed transformative shifts across numerous industry\nsectors, including urban transportation and logistics. This study investigates\nthe evaluation of Large Language Models (LLMs), specifically GPT-4 and\nPhi-3-mini, to enhance transportation planning. The study assesses the\nperformance and spatial comprehension of these models through a\ntransportation-informed evaluation framework that includes general geospatial\nskills, general transportation domain skills, and real-world transportation\nproblem-solving. Utilizing a mixed-methods approach, the research encompasses\nan evaluation of the LLMs' general Geographic Information System (GIS) skills,\ngeneral transportation domain knowledge as well as abilities to support human\ndecision-making in the real-world transportation planning scenarios of\ncongestion pricing. Results indicate that GPT-4 demonstrates superior accuracy\nand reliability across various GIS and transportation-specific tasks compared\nto Phi-3-mini, highlighting its potential as a robust tool for transportation\nplanners. Nonetheless, Phi-3-mini exhibits competence in specific analytical\nscenarios, suggesting its utility in resource-constrained environments. The\nfindings underscore the transformative potential of GenAI technologies in urban\ntransportation planning. Future work could explore the application of newer\nLLMs and the impact of Retrieval-Augmented Generation (RAG) techniques, on a\nbroader set of real-world transportation planning and operations challenges, to\ndeepen the integration of advanced AI models in transportation management\npractices.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.14516v1",
    "published_date": "2024-09-22 16:20:00 UTC",
    "updated_date": "2024-09-22 16:20:00 UTC"
  },
  {
    "arxiv_id": "2409.14507v4",
    "title": "A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders",
    "authors": [
      "David Chanin",
      "James Wilken-Smith",
      "Tomáš Dulka",
      "Hardik Bhatnagar",
      "Joseph Bloom"
    ],
    "abstract": "Sparse Autoencoders (SAEs) have emerged as a promising approach to decompose\nthe activations of Large Language Models (LLMs) into human-interpretable\nlatents. In this paper, we pose two questions. First, to what extent do SAEs\nextract monosemantic and interpretable latents? Second, to what extent does\nvarying the sparsity or the size of the SAE affect monosemanticity /\ninterpretability? By investigating these questions in the context of a simple\nfirst-letter identification task where we have complete access to ground truth\nlabels for all tokens in the vocabulary, we are able to provide more detail\nthan prior investigations. Critically, we identify a problematic form of\nfeature-splitting we call feature absorption where seemingly monosemantic\nlatents fail to fire in cases where they clearly should. Our investigation\nsuggests that varying SAE size or sparsity is insufficient to solve this issue,\nand that there are deeper conceptual issues in need of resolution.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.14507v4",
    "published_date": "2024-09-22 16:11:02 UTC",
    "updated_date": "2024-09-30 20:42:22 UTC"
  },
  {
    "arxiv_id": "2409.14500v2",
    "title": "TabGraphs: A Benchmark and Strong Baselines for Learning on Graphs with Tabular Node Features",
    "authors": [
      "Gleb Bazhenov",
      "Oleg Platonov",
      "Liudmila Prokhorenkova"
    ],
    "abstract": "Tabular machine learning is an important field for industry and science. In\nthis field, table rows are usually treated as independent data samples, but\nadditional information about relations between them is sometimes available and\ncan be used to improve predictive performance. Such information can be\nnaturally modeled with a graph, thus tabular machine learning may benefit from\ngraph machine learning methods. However, graph machine learning models are\ntypically evaluated on datasets with homogeneous node features, which have\nlittle in common with heterogeneous mixtures of numerical and categorical\nfeatures present in tabular datasets. Thus, there is a critical difference\nbetween the data used in tabular and graph machine learning studies, which does\nnot allow one to understand how successfully graph models can be transferred to\ntabular data. To bridge this gap, we propose a new benchmark of diverse graphs\nwith heterogeneous tabular node features and realistic prediction tasks. We use\nthis benchmark to evaluate a vast set of models, including simple methods\npreviously overlooked in the literature. Our experiments show that graph neural\nnetworks (GNNs) can indeed often bring gains in predictive performance for\ntabular data, but standard tabular models also can be adapted to work with\ngraph data by using simple feature preprocessing, which sometimes enables them\nto compete with and even outperform GNNs. Based on our empirical study, we\nprovide insights for researchers and practitioners in both tabular and graph\nmachine learning fields.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.14500v2",
    "published_date": "2024-09-22 15:53:19 UTC",
    "updated_date": "2024-09-26 15:26:43 UTC"
  },
  {
    "arxiv_id": "2409.14496v1",
    "title": "On a measure of intelligence",
    "authors": [
      "Yuri Gurevich"
    ],
    "abstract": "The Fall 2024 Logic in Computer Science column of the Bulletin of EATCS is a\nlittle discussion on intelligence, measuring intelligence, and related issues,\nprovoked by a fascinating must-read article ``On the measure of intelligence''\nby Fran\\c{c}ois Chollet. The discussion includes a modicum of critique of the\narticle.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.14496v1",
    "published_date": "2024-09-22 15:49:31 UTC",
    "updated_date": "2024-09-22 15:49:31 UTC"
  },
  {
    "arxiv_id": "2409.14495v3",
    "title": "Thought-Path Contrastive Learning via Premise-Oriented Data Augmentation for Logical Reading Comprehension",
    "authors": [
      "Chenxu Wang",
      "Ping Jian",
      "Zhen Yang"
    ],
    "abstract": "Logical reading comprehension is a challenging task that entails grasping the\nunderlying semantics of text and applying reasoning to deduce the correct\nanswer. Prior researches have primarily focused on enhancing logical reasoning\ncapabilities through Chain-of-Thought (CoT) or data augmentation. However,\nprevious work constructing chain-of-thought rationales concentrates solely on\nanalyzing correct options, neglecting the incorrect alternatives. Addtionally,\nearlier efforts on data augmentation by altering contexts rely on rule-based\nmethods, which result in generated contexts that lack diversity and coherence.\nTo address these issues, we propose a Premise-Oriented Data Augmentation (PODA)\nframework. This framework can generate CoT rationales including analyses for\nboth correct and incorrect options, while constructing diverse and high-quality\ncounterfactual contexts from incorrect candidate options. We integrate\nsummarizing premises and identifying premises for each option into rationales.\nSubsequently, we employ multi-step prompts with identified premises to\nconstruct counterfactual context. To facilitate the model's capabilities to\nbetter differentiate the reasoning process associated with each option, we\nintroduce a novel thought-path contrastive learning method that compares\nreasoning paths between the original and counterfactual samples. Experimental\nresults on three representative LLMs demonstrate that our method can improve\nthe baselines substantially across two challenging logical reasoning benchmarks\n(ReClor and LogiQA 2.0). The data and code are released at\nhttps://github.com/lalalamdbf/TPReasoner.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.14495v3",
    "published_date": "2024-09-22 15:44:43 UTC",
    "updated_date": "2025-02-06 08:43:15 UTC"
  },
  {
    "arxiv_id": "2409.14488v1",
    "title": "Enhancing LLM-based Autonomous Driving Agents to Mitigate Perception Attacks",
    "authors": [
      "Ruoyu Song",
      "Muslum Ozgur Ozmen",
      "Hyungsub Kim",
      "Antonio Bianchi",
      "Z. Berkay Celik"
    ],
    "abstract": "There is a growing interest in integrating Large Language Models (LLMs) with\nautonomous driving (AD) systems. However, AD systems are vulnerable to attacks\nagainst their object detection and tracking (ODT) functions. Unfortunately, our\nevaluation of four recent LLM agents against ODT attacks shows that the attacks\nare 63.26% successful in causing them to crash or violate traffic rules due to\n(1) misleading memory modules that provide past experiences for decision\nmaking, (2) limitations of prompts in identifying inconsistencies, and (3)\nreliance on ground truth perception data.\n  In this paper, we introduce Hudson, a driving reasoning agent that extends\nprior LLM-based driving systems to enable safer decision making during\nperception attacks while maintaining effectiveness under benign conditions.\nHudson achieves this by first instrumenting the AD software to collect\nreal-time perception results and contextual information from the driving scene.\nThis data is then formalized into a domain-specific language (DSL). To guide\nthe LLM in detecting and making safe control decisions during ODT attacks,\nHudson translates the DSL into natural language, along with a list of custom\nattack detection instructions. Following query execution, Hudson analyzes the\nLLM's control decision to understand its causal reasoning process.\n  We evaluate the effectiveness of Hudson using a proprietary LLM (GPT-4) and\ntwo open-source LLMs (Llama and Gemma) in various adversarial driving\nscenarios. GPT-4, Llama, and Gemma achieve, on average, an attack detection\naccuracy of 83. 3%, 63. 6%, and 73. 6%. Consequently, they make safe control\ndecisions in 86.4%, 73.9%, and 80% of the attacks. Our results, following the\ngrowing interest in integrating LLMs into AD systems, highlight the strengths\nof LLMs and their potential to detect and mitigate ODT attacks.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.14488v1",
    "published_date": "2024-09-22 15:18:59 UTC",
    "updated_date": "2024-09-22 15:18:59 UTC"
  },
  {
    "arxiv_id": "2409.14478v1",
    "title": "Can Large Language Models Logically Predict Myocardial Infarction? Evaluation based on UK Biobank Cohort",
    "authors": [
      "Yuxing Zhi",
      "Yuan Guo",
      "Kai Yuan",
      "Hesong Wang",
      "Heng Xu",
      "Haina Yao",
      "Albert C Yang",
      "Guangrui Huang",
      "Yuping Duan"
    ],
    "abstract": "Background: Large language models (LLMs) have seen extraordinary advances\nwith applications in clinical decision support. However, high-quality evidence\nis urgently needed on the potential and limitation of LLMs in providing\naccurate clinical decisions based on real-world medical data. Objective: To\nevaluate quantitatively whether universal state-of-the-art LLMs (ChatGPT and\nGPT-4) can predict the incidence risk of myocardial infarction (MI) with\nlogical inference, and to further make comparison between various models to\nassess the performance of LLMs comprehensively. Methods: In this retrospective\ncohort study, 482,310 participants recruited from 2006 to 2010 were initially\nincluded in UK Biobank database and later on resampled into a final cohort of\n690 participants. For each participant, tabular data of the risk factors of MI\nwere transformed into standardized textual descriptions for ChatGPT\nrecognition. Responses were generated by asking ChatGPT to select a score\nranging from 0 to 10 representing the risk. Chain of Thought (CoT) questioning\nwas used to evaluate whether LLMs make prediction logically. The predictive\nperformance of ChatGPT was compared with published medical indices, traditional\nmachine learning models and other large language models. Conclusions: Current\nLLMs are not ready to be applied in clinical medicine fields. Future medical\nLLMs are suggested to be expert in medical domain knowledge to understand both\nnatural languages and quantified medical data, and further make logical\ninferences.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.14478v1",
    "published_date": "2024-09-22 14:57:31 UTC",
    "updated_date": "2024-09-22 14:57:31 UTC"
  },
  {
    "arxiv_id": "2409.14474v1",
    "title": "SynBench: A Synthetic Benchmark for Non-rigid 3D Point Cloud Registration",
    "authors": [
      "Sara Monji-Azad",
      "Marvin Kinz",
      "Claudia Scherl",
      "David Männle",
      "Jürgen Hesser",
      "Nikolas Löw"
    ],
    "abstract": "Non-rigid point cloud registration is a crucial task in computer vision.\nEvaluating a non-rigid point cloud registration method requires a dataset with\nchallenges such as large deformation levels, noise, outliers, and\nincompleteness. Despite the existence of several datasets for deformable point\ncloud registration, the absence of a comprehensive benchmark with all\nchallenges makes it difficult to achieve fair evaluations among different\nmethods. This paper introduces SynBench, a new non-rigid point cloud\nregistration dataset created using SimTool, a toolset for soft body simulation\nin Flex and Unreal Engine. SynBench provides the ground truth of corresponding\npoints between two point sets and encompasses key registration challenges,\nincluding varying levels of deformation, noise, outliers, and incompleteness.\nTo the best of the authors' knowledge, compared to existing datasets, SynBench\npossesses three particular characteristics: (1) it is the first benchmark that\nprovides various challenges for non-rigid point cloud registration, (2)\nSynBench encompasses challenges of varying difficulty levels, and (3) it\nincludes ground truth corresponding points both before and after deformation.\nThe authors believe that SynBench enables future non-rigid point cloud\nregistration methods to present a fair comparison of their achievements.\nSynBench is publicly available at: https://doi.org/10.11588/data/R9IKCF.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.14474v1",
    "published_date": "2024-09-22 14:46:20 UTC",
    "updated_date": "2024-09-22 14:46:20 UTC"
  },
  {
    "arxiv_id": "2409.14465v1",
    "title": "On logic and generative AI",
    "authors": [
      "Yuri Gurevich",
      "Andreas Blass"
    ],
    "abstract": "A hundred years ago, logic was almost synonymous with foundational studies.\nThe ongoing AI revolution raises many deep foundational problems involving\nneuroscience, philosophy, computer science, and logic. The goal of the\nfollowing dialog is to provoke young logicians with a taste for foundations to\nnotice the foundational problems raised by the AI revolution.",
    "categories": [
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.14465v1",
    "published_date": "2024-09-22 14:31:58 UTC",
    "updated_date": "2024-09-22 14:31:58 UTC"
  },
  {
    "arxiv_id": "2409.14459v2",
    "title": "Exploring Multilingual Probing in Large Language Models: A Cross-Language Analysis",
    "authors": [
      "Daoyang Li",
      "Haiyan Zhao",
      "Qingcheng Zeng",
      "Mengnan Du"
    ],
    "abstract": "Probing techniques for large language models (LLMs) have primarily focused on\nEnglish, overlooking the vast majority of the world's languages. In this paper,\nwe extend these probing methods to a multilingual context, investigating the\nbehaviors of LLMs across diverse languages. We conduct experiments on several\nopen-source LLM models, analyzing probing accuracy, trends across layers, and\nsimilarities between probing vectors for multiple languages. Our key findings\nreveal: (1) a consistent performance gap between high-resource and low-resource\nlanguages, with high-resource languages achieving significantly higher probing\naccuracy; (2) divergent layer-wise accuracy trends, where high-resource\nlanguages show substantial improvement in deeper layers similar to English; and\n(3) higher representational similarities among high-resource languages, with\nlow-resource languages demonstrating lower similarities both among themselves\nand with high-resource languages. These results highlight significant\ndisparities in LLMs' multilingual capabilities and emphasize the need for\nimproved modeling of low-resource languages.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.14459v2",
    "published_date": "2024-09-22 14:14:05 UTC",
    "updated_date": "2025-01-31 01:37:22 UTC"
  },
  {
    "arxiv_id": "2409.14457v2",
    "title": "Large Model Based Agents: State-of-the-Art, Cooperation Paradigms, Security and Privacy, and Future Trends",
    "authors": [
      "Yuntao Wang",
      "Yanghe Pan",
      "Zhou Su",
      "Yi Deng",
      "Quan Zhao",
      "Linkang Du",
      "Tom H. Luan",
      "Jiawen Kang",
      "Dusit Niyato"
    ],
    "abstract": "With the rapid advancement of large models (LMs), the development of\ngeneral-purpose intelligent agents powered by LMs has become a reality. It is\nforeseeable that in the near future, LM-driven general AI agents will serve as\nessential tools in production tasks, capable of autonomous communication and\ncollaboration without human intervention. This paper investigates scenarios\ninvolving the autonomous collaboration of future LM agents. We review the\ncurrent state of LM agents, the key technologies enabling LM agent\ncollaboration, and the security and privacy challenges they face during\ncooperative operations. To this end, we first explore the foundational\nprinciples of LM agents, including their general architecture, key components,\nenabling technologies, and modern applications. We then discuss practical\ncollaboration paradigms from data, computation, and knowledge perspectives to\nachieve connected intelligence among LM agents. After that, we analyze the\nsecurity vulnerabilities and privacy risks associated with LM agents,\nparticularly in multi-agent settings, examining underlying mechanisms and\nreviewing current and potential countermeasures. Lastly, we propose future\nresearch directions for building robust and secure LM agent ecosystems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "40 pages, 31 figures, 8 tables",
    "pdf_url": "http://arxiv.org/pdf/2409.14457v2",
    "published_date": "2024-09-22 14:09:49 UTC",
    "updated_date": "2025-01-08 14:29:44 UTC"
  },
  {
    "arxiv_id": "2409.14456v1",
    "title": "Scoring rule nets: beyond mean target prediction in multivariate regression",
    "authors": [
      "Daan Roordink",
      "Sibylle Hess"
    ],
    "abstract": "Probabilistic regression models trained with maximum likelihood estimation\n(MLE), can sometimes overestimate variance to an unacceptable degree. This is\nmostly problematic in the multivariate domain. While univariate models often\noptimize the popular Continuous Ranked Probability Score (CRPS), in the\nmultivariate domain, no such alternative to MLE has yet been widely accepted.\nThe Energy Score - the most investigated alternative - notoriously lacks\nclosed-form expressions and sensitivity to the correlation between target\nvariables. In this paper, we propose Conditional CRPS: a multivariate strictly\nproper scoring rule that extends CRPS. We show that closed-form expressions\nexist for popular distributions and illustrate their sensitivity to\ncorrelation. We then show in a variety of experiments on both synthetic and\nreal data, that Conditional CRPS often outperforms MLE, and produces results\ncomparable to state-of-the-art non-parametric models, such as Distributional\nRandom Forest (DRF).",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.14456v1",
    "published_date": "2024-09-22 14:09:12 UTC",
    "updated_date": "2024-09-22 14:09:12 UTC"
  },
  {
    "arxiv_id": "2410.02811v1",
    "title": "SAC-KG: Exploiting Large Language Models as Skilled Automatic Constructors for Domain Knowledge Graphs",
    "authors": [
      "Hanzhu Chen",
      "Xu Shen",
      "Qitan Lv",
      "Jie Wang",
      "Xiaoqi Ni",
      "Jieping Ye"
    ],
    "abstract": "Knowledge graphs (KGs) play a pivotal role in knowledge-intensive tasks\nacross specialized domains, where the acquisition of precise and dependable\nknowledge is crucial. However, existing KG construction methods heavily rely on\nhuman intervention to attain qualified KGs, which severely hinders the\npractical applicability in real-world scenarios. To address this challenge, we\npropose a general KG construction framework, named SAC-KG, to exploit large\nlanguage models (LLMs) as Skilled Automatic Constructors for domain Knowledge\nGraph. SAC-KG effectively involves LLMs as domain experts to generate\nspecialized and precise multi-level KGs. Specifically, SAC-KG consists of three\ncomponents: Generator, Verifier, and Pruner. For a given entity, Generator\nproduces its relations and tails from raw domain corpora, to construct a\nspecialized single-level KG. Verifier and Pruner then work together to ensure\nprecision by correcting generation errors and determining whether newly\nproduced tails require further iteration for the next-level KG.Experiments\ndemonstrate that SAC-KG automatically constructs a domain KG at the scale of\nover one million nodes and achieves a precision of 89.32%, leading to a\nsuperior performance with over 20% increase in precision rate compared to\nexisting state-of-the-art methods for the KG construction task.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "ACL 2024 Main",
    "pdf_url": "http://arxiv.org/pdf/2410.02811v1",
    "published_date": "2024-09-22 13:55:23 UTC",
    "updated_date": "2024-09-22 13:55:23 UTC"
  },
  {
    "arxiv_id": "2409.14446v1",
    "title": "Detection of pulmonary pathologies using convolutional neural networks, Data Augmentation, ResNet50 and Vision Transformers",
    "authors": [
      "Pablo Ramirez Amador",
      "Dinarle Milagro Ortega",
      "Arnold Cesarano"
    ],
    "abstract": "Pulmonary diseases are a public health problem that requires accurate and\nfast diagnostic techniques. In this paper, a method based on convolutional\nneural networks (CNN), Data Augmentation, ResNet50 and Vision Transformers\n(ViT) is proposed to detect lung pathologies from medical images. A dataset of\nX-ray images and CT scans of patients with different lung diseases, such as\ncancer, pneumonia, tuberculosis and fibrosis, is used. The results obtained by\nthe proposed method are compared with those of other existing methods, using\nperformance metrics such as accuracy, sensitivity, specificity and area under\nthe ROC curve. The results show that the proposed method outperforms the other\nmethods in all metrics, achieving an accuracy of 98% and an area under the ROC\ncurve of 99%. It is concluded that the proposed method is an effective and\npromising tool for the diagnosis of pulmonary pathologies by medical imaging.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "10 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.14446v1",
    "published_date": "2024-09-22 13:54:28 UTC",
    "updated_date": "2024-09-22 13:54:28 UTC"
  },
  {
    "arxiv_id": "2409.14439v1",
    "title": "A Visualized Malware Detection Framework with CNN and Conditional GAN",
    "authors": [
      "Fang Wang",
      "Hussam Al Hamadi",
      "Ernesto Damiani"
    ],
    "abstract": "Malware visualization analysis incorporating with Machine Learning (ML) has\nbeen proven to be a promising solution for improving security defenses on\ndifferent platforms. In this work, we propose an integrated framework for\naddressing common problems experienced by ML utilizers in developing malware\ndetection systems. Namely, a pictorial presentation system with extensions is\ndesigned to preserve the identities of benign/malign samples by encoding each\nvariable into binary digits and mapping them into black and white pixels. A\nconditional Generative Adversarial Network based model is adopted to produce\nsynthetic images and mitigate issues of imbalance classes. Detection models\narchitected by Convolutional Neural Networks are for validating performances\nwhile training on datasets with and without artifactual samples. Result\ndemonstrates accuracy rates of 98.51% and 97.26% for these two training\nscenarios.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "7 pages, 2022 IEEE International Conference on Big Data (Big Data),\n  2022",
    "pdf_url": "http://arxiv.org/pdf/2409.14439v1",
    "published_date": "2024-09-22 13:29:10 UTC",
    "updated_date": "2024-09-22 13:29:10 UTC"
  },
  {
    "arxiv_id": "2409.14436v1",
    "title": "Automotive innovation landscaping using LLM",
    "authors": [
      "Raju Gorain",
      "Omkar Salunke"
    ],
    "abstract": "The process of landscaping automotive innovation through patent analysis is\ncrucial for Research and Development teams. It aids in comprehending innovation\ntrends, technological advancements, and the latest technologies from\ncompetitors. Traditionally, this process required intensive manual efforts.\nHowever, with the advent of Large Language Models (LLMs), it can now be\nautomated, leading to faster and more efficient patent categorization &\nstate-of-the-art of inventive concept extraction. This automation can assist\nvarious R\\&D teams in extracting relevant information from extensive patent\ndatabases. This paper introduces a method based on prompt engineering to\nextract essential information for landscaping. The information includes the\nproblem addressed by the patent, the technology utilized, and the area of\ninnovation within the vehicle ecosystem (such as safety, Advanced Driver\nAssistance Systems and more).The result demonstrates the implementation of this\nmethod to create a landscape of fuel cell technology using open-source patent\ndata. This approach provides a comprehensive overview of the current state of\nfuel cell technology, offering valuable insights for future research and\ndevelopment in this field.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CL",
    "comment": "9pages, 4Figures, 1 Flow chart",
    "pdf_url": "http://arxiv.org/pdf/2409.14436v1",
    "published_date": "2024-09-22 13:22:39 UTC",
    "updated_date": "2024-09-22 13:22:39 UTC"
  },
  {
    "arxiv_id": "2409.14433v1",
    "title": "OStr-DARTS: Differentiable Neural Architecture Search based on Operation Strength",
    "authors": [
      "Le Yang",
      "Ziwei Zheng",
      "Yizeng Han",
      "Shiji Song",
      "Gao Huang",
      "Fan Li"
    ],
    "abstract": "Differentiable architecture search (DARTS) has emerged as a promising\ntechnique for effective neural architecture search, and it mainly contains two\nsteps to find the high-performance architecture: First, the DARTS supernet that\nconsists of mixed operations will be optimized via gradient descent. Second,\nthe final architecture will be built by the selected operations that contribute\nthe most to the supernet. Although DARTS improves the efficiency of NAS, it\nsuffers from the well-known degeneration issue which can lead to deteriorating\narchitectures. Existing works mainly attribute the degeneration issue to the\nfailure of its supernet optimization, while little attention has been paid to\nthe selection method. In this paper, we cease to apply the widely-used\nmagnitude-based selection method and propose a novel criterion based on\noperation strength that estimates the importance of an operation by its effect\non the final loss. We show that the degeneration issue can be effectively\naddressed by using the proposed criterion without any modification of supernet\noptimization, indicating that the magnitude-based selection method can be a\ncritical reason for the instability of DARTS. The experiments on NAS-Bench-201\nand DARTS search spaces show the effectiveness of our method.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.14433v1",
    "published_date": "2024-09-22 13:16:07 UTC",
    "updated_date": "2024-09-22 13:16:07 UTC"
  },
  {
    "arxiv_id": "2409.14430v1",
    "title": "Pomo3D: 3D-Aware Portrait Accessorizing and More",
    "authors": [
      "Tzu-Chieh Liu",
      "Chih-Ting Liu",
      "Shao-Yi Chien"
    ],
    "abstract": "We propose Pomo3D, a 3D portrait manipulation framework that allows free\naccessorizing by decomposing and recomposing portraits and accessories. It\nenables the avatars to attain out-of-distribution (OOD) appearances of\nsimultaneously wearing multiple accessories. Existing methods still struggle to\noffer such explicit and fine-grained editing; they either fail to generate\nadditional objects on given portraits or cause alterations to portraits (e.g.,\nidentity shift) when generating accessories. This restriction presents a\nnoteworthy obstacle as people typically seek to create charming appearances\nwith diverse and fashionable accessories in the virtual universe. Our approach\nprovides an effective solution to this less-addressed issue. We further\nintroduce the Scribble2Accessories module, enabling Pomo3D to create 3D\naccessories from user-drawn accessory scribble maps. Moreover, we design a\nbias-conscious mapper to mitigate biased associations present in real-world\ndatasets. In addition to object-level manipulation above, Pomo3D also offers\nextensive editing options on portraits, including global or local editing of\ngeometry and texture and avatar stylization, elevating 3D editing of neural\nportraits to a more comprehensive level.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.14430v1",
    "published_date": "2024-09-22 13:03:24 UTC",
    "updated_date": "2024-09-22 13:03:24 UTC"
  },
  {
    "arxiv_id": "2409.14429v1",
    "title": "Challenging the Performance-Interpretability Trade-off: An Evaluation of Interpretable Machine Learning Models",
    "authors": [
      "Sven Kruschel",
      "Nico Hambauer",
      "Sven Weinzierl",
      "Sandra Zilker",
      "Mathias Kraus",
      "Patrick Zschech"
    ],
    "abstract": "Machine learning is permeating every conceivable domain to promote\ndata-driven decision support. The focus is often on advanced black-box models\ndue to their assumed performance advantages, whereas interpretable models are\noften associated with inferior predictive qualities. More recently, however, a\nnew generation of generalized additive models (GAMs) has been proposed that\noffer promising properties for capturing complex, non-linear patterns while\nremaining fully interpretable. To uncover the merits and limitations of these\nmodels, this study examines the predictive performance of seven different GAMs\nin comparison to seven commonly used machine learning models based on a\ncollection of twenty tabular benchmark datasets. To ensure a fair and robust\nmodel comparison, an extensive hyperparameter search combined with\ncross-validation was performed, resulting in 68,500 model runs. In addition,\nthis study qualitatively examines the visual output of the models to assess\ntheir level of interpretability. Based on these results, the paper dispels the\nmisconception that only black-box models can achieve high accuracy by\ndemonstrating that there is no strict trade-off between predictive performance\nand model interpretability for tabular data. Furthermore, the paper discusses\nthe importance of GAMs as powerful interpretable models for the field of\ninformation systems and derives implications for future work from a\nsocio-technical perspective.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted for publication in Business & Information Systems\n  Engineering (2024)",
    "pdf_url": "http://arxiv.org/pdf/2409.14429v1",
    "published_date": "2024-09-22 12:58:52 UTC",
    "updated_date": "2024-09-22 12:58:52 UTC"
  },
  {
    "arxiv_id": "2409.14424v2",
    "title": "Dormant: Defending against Pose-driven Human Image Animation",
    "authors": [
      "Jiachen Zhou",
      "Mingsi Wang",
      "Tianlin Li",
      "Guozhu Meng",
      "Kai Chen"
    ],
    "abstract": "Pose-driven human image animation has achieved tremendous progress, enabling\nthe generation of vivid and realistic human videos from just one single photo.\nHowever, it conversely exacerbates the risk of image misuse, as attackers may\nuse one available image to create videos involving politics, violence, and\nother illegal content. To counter this threat, we propose Dormant, a novel\nprotection approach tailored to defend against pose-driven human image\nanimation techniques. Dormant applies protective perturbation to one human\nimage, preserving the visual similarity to the original but resulting in\npoor-quality video generation. The protective perturbation is optimized to\ninduce misextraction of appearance features from the image and create\nincoherence among the generated video frames. Our extensive evaluation across 8\nanimation methods and 4 datasets demonstrates the superiority of Dormant over 6\nbaseline protection methods, leading to misaligned identities, visual\ndistortions, noticeable artifacts, and inconsistent frames in the generated\nvideos. Moreover, Dormant shows effectiveness on 6 real-world commercial\nservices, even with fully black-box access.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted by USENIX Security 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.14424v2",
    "published_date": "2024-09-22 12:51:32 UTC",
    "updated_date": "2025-02-24 08:35:58 UTC"
  },
  {
    "arxiv_id": "2409.14412v1",
    "title": "COSBO: Conservative Offline Simulation-Based Policy Optimization",
    "authors": [
      "Eshagh Kargar",
      "Ville Kyrki"
    ],
    "abstract": "Offline reinforcement learning allows training reinforcement learning models\non data from live deployments. However, it is limited to choosing the best\ncombination of behaviors present in the training data. In contrast, simulation\nenvironments attempting to replicate the live environment can be used instead\nof the live data, yet this approach is limited by the simulation-to-reality\ngap, resulting in a bias. In an attempt to get the best of both worlds, we\npropose a method that combines an imperfect simulation environment with data\nfrom the target environment, to train an offline reinforcement learning policy.\nOur experiments demonstrate that the proposed method outperforms\nstate-of-the-art approaches CQL, MOPO, and COMBO, especially in scenarios with\ndiverse and challenging dynamics, and demonstrates robust behavior across a\nvariety of experimental conditions. The results highlight that using\nsimulator-generated data can effectively enhance offline policy learning\ndespite the sim-to-real gap, when direct interaction with the real-world is not\npossible.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.14412v1",
    "published_date": "2024-09-22 12:20:55 UTC",
    "updated_date": "2024-09-22 12:20:55 UTC"
  },
  {
    "arxiv_id": "2409.14399v2",
    "title": "Beyond Persuasion: Towards Conversational Recommender System with Credible Explanations",
    "authors": [
      "Peixin Qin",
      "Chen Huang",
      "Yang Deng",
      "Wenqiang Lei",
      "Tat-Seng Chua"
    ],
    "abstract": "With the aid of large language models, current conversational recommender\nsystem (CRS) has gaining strong abilities to persuade users to accept\nrecommended items. While these CRSs are highly persuasive, they can mislead\nusers by incorporating incredible information in their explanations, ultimately\ndamaging the long-term trust between users and the CRS. To address this, we\npropose a simple yet effective method, called PC-CRS, to enhance the\ncredibility of CRS's explanations during persuasion. It guides the explanation\ngeneration through our proposed credibility-aware persuasive strategies and\nthen gradually refines explanations via post-hoc self-reflection. Experimental\nresults demonstrate the efficacy of PC-CRS in promoting persuasive and credible\nexplanations. Further analysis reveals the reason behind current methods\nproducing incredible explanations and the potential of credible explanations to\nimprove recommendation accuracy.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Findings of EMNLP 2024. Our code is available at\n  https://github.com/mumen798/PC-CRS",
    "pdf_url": "http://arxiv.org/pdf/2409.14399v2",
    "published_date": "2024-09-22 11:35:59 UTC",
    "updated_date": "2024-10-07 07:49:27 UTC"
  },
  {
    "arxiv_id": "2409.14393v1",
    "title": "MaskedMimic: Unified Physics-Based Character Control Through Masked Motion Inpainting",
    "authors": [
      "Chen Tessler",
      "Yunrong Guo",
      "Ofir Nabati",
      "Gal Chechik",
      "Xue Bin Peng"
    ],
    "abstract": "Crafting a single, versatile physics-based controller that can breathe life\ninto interactive characters across a wide spectrum of scenarios represents an\nexciting frontier in character animation. An ideal controller should support\ndiverse control modalities, such as sparse target keyframes, text instructions,\nand scene information. While previous works have proposed physically simulated,\nscene-aware control models, these systems have predominantly focused on\ndeveloping controllers that each specializes in a narrow set of tasks and\ncontrol modalities. This work presents MaskedMimic, a novel approach that\nformulates physics-based character control as a general motion inpainting\nproblem. Our key insight is to train a single unified model to synthesize\nmotions from partial (masked) motion descriptions, such as masked keyframes,\nobjects, text descriptions, or any combination thereof. This is achieved by\nleveraging motion tracking data and designing a scalable training method that\ncan effectively utilize diverse motion descriptions to produce coherent\nanimations. Through this process, our approach learns a physics-based\ncontroller that provides an intuitive control interface without requiring\ntedious reward engineering for all behaviors of interest. The resulting\ncontroller supports a wide range of control modalities and enables seamless\ntransitions between disparate tasks. By unifying character control through\nmotion inpainting, MaskedMimic creates versatile virtual characters. These\ncharacters can dynamically adapt to complex scenes and compose diverse motions\non demand, enabling more interactive and immersive experiences.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "ACM Transactions on Graphics (Proc. SIGGRAPH Asia 2024) Project page:\n  https://research.nvidia.com/labs/par/maskedmimic/",
    "pdf_url": "http://arxiv.org/pdf/2409.14393v1",
    "published_date": "2024-09-22 11:10:59 UTC",
    "updated_date": "2024-09-22 11:10:59 UTC"
  },
  {
    "arxiv_id": "2409.14378v3",
    "title": "Sparse Low-Ranked Self-Attention Transformer for Remaining Useful Lifetime Prediction of Optical Fiber Amplifiers",
    "authors": [
      "Dominic Schneider",
      "Lutz Rapp"
    ],
    "abstract": "Optical fiber amplifiers are key elements in present optical networks.\nFailures of these components result in high financial loss of income of the\nnetwork operator as the communication traffic over an affected link is\ninterrupted. Applying Remaining useful lifetime (RUL) prediction in the context\nof Predictive Maintenance (PdM) to optical fiber amplifiers to predict upcoming\nsystem failures at an early stage, so that network outages can be minimized\nthrough planning of targeted maintenance actions, ensures reliability and\nsafety. Optical fiber amplifier are complex systems, that work under various\noperating conditions, which makes correct forecasting a difficult task.\nIncreased monitoring capabilities of systems results in datasets that\nfacilitate the application of data-driven RUL prediction methods. Deep learning\nmodels in particular have shown good performance, but generalization based on\ncomparatively small datasets for RUL prediction is difficult. In this paper, we\npropose Sparse Low-ranked self-Attention Transformer (SLAT) as a novel RUL\nprediction method. SLAT is based on an encoder-decoder architecture, wherein\ntwo parallel working encoders extract features for sensors and time steps. By\nutilizing the self-attention mechanism, long-term dependencies can be learned\nfrom long sequences. The implementation of sparsity in the attention matrix and\na low-rank parametrization reduce overfitting and increase generalization.\nExperimental application to optical fiber amplifiers exemplified on EDFA, as\nwell as a reference dataset from turbofan engines, shows that SLAT outperforms\nthe state-of-the-art methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.14378v3",
    "published_date": "2024-09-22 09:48:45 UTC",
    "updated_date": "2025-01-15 11:07:35 UTC"
  },
  {
    "arxiv_id": "2409.14377v1",
    "title": "To Err Is AI! Debugging as an Intervention to Facilitate Appropriate Reliance on AI Systems",
    "authors": [
      "Gaole He",
      "Abri Bharos",
      "Ujwal Gadiraju"
    ],
    "abstract": "Powerful predictive AI systems have demonstrated great potential in\naugmenting human decision making. Recent empirical work has argued that the\nvision for optimal human-AI collaboration requires 'appropriate reliance' of\nhumans on AI systems. However, accurately estimating the trustworthiness of AI\nadvice at the instance level is quite challenging, especially in the absence of\nperformance feedback pertaining to the AI system. In practice, the performance\ndisparity of machine learning models on out-of-distribution data makes the\ndataset-specific performance feedback unreliable in human-AI collaboration.\nInspired by existing literature on critical thinking and a critical mindset, we\npropose the use of debugging an AI system as an intervention to foster\nappropriate reliance. In this paper, we explore whether a critical evaluation\nof AI performance within a debugging setting can better calibrate users'\nassessment of an AI system and lead to more appropriate reliance. Through a\nquantitative empirical study (N = 234), we found that our proposed debugging\nintervention does not work as expected in facilitating appropriate reliance.\nInstead, we observe a decrease in reliance on the AI system after the\nintervention -- potentially resulting from an early exposure to the AI system's\nweakness. We explore the dynamics of user confidence and user estimation of AI\ntrustworthiness across groups with different performance levels to help explain\nhow inappropriate reliance patterns occur. Our findings have important\nimplications for designing effective interventions to facilitate appropriate\nreliance and better human-AI collaboration.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Paper accepted at HT'24 as late-break. This is an expanded version of\n  HT'24 paper, providing more details and experimental analysis",
    "pdf_url": "http://arxiv.org/pdf/2409.14377v1",
    "published_date": "2024-09-22 09:43:27 UTC",
    "updated_date": "2024-09-22 09:43:27 UTC"
  },
  {
    "arxiv_id": "2409.14368v1",
    "title": "Evaluating the Quality of Code Comments Generated by Large Language Models for Novice Programmers",
    "authors": [
      "Aysa Xuemo Fan",
      "Arun Balajiee Lekshmi Narayanan",
      "Mohammad Hassany",
      "Jiaze Ke"
    ],
    "abstract": "Large Language Models (LLMs) show promise in generating code comments for\nnovice programmers, but their educational effectiveness remains\nunder-evaluated. This study assesses the instructional quality of code comments\nproduced by GPT-4, GPT-3.5-Turbo, and Llama2, compared to expert-developed\ncomments, focusing on their suitability for novices. Analyzing a dataset of\n``easy'' level Java solutions from LeetCode, we find that GPT-4 exhibits\ncomparable quality to expert comments in aspects critical for beginners, such\nas clarity, beginner-friendliness, concept elucidation, and step-by-step\nguidance. GPT-4 outperforms Llama2 in discussing complexity (chi-square =\n11.40, p = 0.001) and is perceived as significantly more supportive for\nbeginners than GPT-3.5 and Llama2 with Mann-Whitney U-statistics = 300.5 and\n322.5, p = 0.0017 and 0.0003). This study highlights the potential of LLMs for\ngenerating code comments tailored to novice programmers.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.14368v1",
    "published_date": "2024-09-22 09:03:48 UTC",
    "updated_date": "2024-09-22 09:03:48 UTC"
  },
  {
    "arxiv_id": "2409.14363v1",
    "title": "MANTA -- Model Adapter Native generations that's Affordable",
    "authors": [
      "Ansh Chaurasia"
    ],
    "abstract": "The presiding model generation algorithms rely on simple, inflexible adapter\nselection to provide personalized results. We propose the model-adapter\ncomposition problem as a generalized problem to past work factoring in\npractical hardware and affordability constraints, and introduce MANTA as a new\napproach to the problem. Experiments on COCO 2014 validation show MANTA to be\nsuperior in image task diversity and quality at the cost of a modest drop in\nalignment. Our system achieves a $94\\%$ win rate in task diversity and a $80\\%$\ntask quality win rate versus the best known system, and demonstrates strong\npotential for direct use in synthetic data generation and the creative art\ndomains.",
    "categories": [
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.14363v1",
    "published_date": "2024-09-22 08:38:23 UTC",
    "updated_date": "2024-09-22 08:38:23 UTC"
  },
  {
    "arxiv_id": "2409.14327v2",
    "title": "Transforming Multidimensional Time Series into Interpretable Event Sequences for Advanced Data Mining",
    "authors": [
      "Xu Yan",
      "Yaoting Jiang",
      "Wenyi Liu",
      "Didi Yi",
      "Jianjun Wei"
    ],
    "abstract": "This paper introduces a novel spatiotemporal feature representation model\ndesigned to address the limitations of traditional methods in multidimensional\ntime series (MTS) analysis. The proposed approach converts MTS into\none-dimensional sequences of spatially evolving events, preserving the complex\ncoupling relationships between dimensions. By employing a variable-length tuple\nmining method, key spatiotemporal features are extracted, enhancing the\ninterpretability and accuracy of time series analysis. Unlike conventional\nmodels, this unsupervised method does not rely on large training datasets,\nmaking it adaptable across different domains. Experimental results from motion\nsequence classification validate the model's superior performance in capturing\nintricate patterns within the data. The proposed framework has significant\npotential for applications across various fields, including backend services\nfor monitoring and optimizing IT infrastructure, medical diagnosis through\ncontinuous patient monitoring and health trend analysis, and internet\nbusinesses for tracking user behavior and forecasting sales. This work offers a\nnew theoretical foundation and technical support for advancing time series data\nmining and its practical applications in human behavior recognition and other\ndomains.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.14327v2",
    "published_date": "2024-09-22 06:27:07 UTC",
    "updated_date": "2024-10-08 07:14:04 UTC"
  },
  {
    "arxiv_id": "2409.14324v1",
    "title": "Unveiling Narrative Reasoning Limits of Large Language Models with Trope in Movie Synopses",
    "authors": [
      "Hung-Ting Su",
      "Ya-Ching Hsu",
      "Xudong Lin",
      "Xiang-Qian Shi",
      "Yulei Niu",
      "Han-Yuan Hsu",
      "Hung-yi Lee",
      "Winston H. Hsu"
    ],
    "abstract": "Large language models (LLMs) equipped with chain-of-thoughts (CoT) prompting\nhave shown significant multi-step reasoning capabilities in factual content\nlike mathematics, commonsense, and logic. However, their performance in\nnarrative reasoning, which demands greater abstraction capabilities, remains\nunexplored. This study utilizes tropes in movie synopses to assess the abstract\nreasoning abilities of state-of-the-art LLMs and uncovers their low\nperformance. We introduce a trope-wise querying approach to address these\nchallenges and boost the F1 score by 11.8 points. Moreover, while prior studies\nsuggest that CoT enhances multi-step reasoning, this study shows CoT can cause\nhallucinations in narrative content, reducing GPT-4's performance. We also\nintroduce an Adversarial Injection method to embed trope-related text tokens\ninto movie synopses without explicit tropes, revealing CoT's heightened\nsensitivity to such injections. Our comprehensive analysis provides insights\nfor future research directions.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2024 Findings. The first two authors contributed equally. Code:\n  https://github.com/Shelley1214/Trope",
    "pdf_url": "http://arxiv.org/pdf/2409.14324v1",
    "published_date": "2024-09-22 05:50:18 UTC",
    "updated_date": "2024-09-22 05:50:18 UTC"
  },
  {
    "arxiv_id": "2409.14307v2",
    "title": "DilateQuant: Accurate and Efficient Diffusion Quantization via Weight Dilation",
    "authors": [
      "Xuewen Liu",
      "Zhikai Li",
      "Qingyi Gu"
    ],
    "abstract": "Diffusion models have shown excellent performance on various image generation\ntasks, but the substantial computational costs and huge memory footprint hinder\ntheir low-latency applications in real-world scenarios. Quantization is a\npromising way to compress and accelerate models. Nevertheless, due to the wide\nrange and time-varying activations in diffusion models, existing methods cannot\nmaintain both accuracy and efficiency simultaneously for low-bit quantization.\nTo tackle this issue, we propose DilateQuant, a novel quantization framework\nfor diffusion models that offers comparable accuracy and high efficiency.\nSpecifically, we keenly aware of numerous unsaturated in-channel weights, which\ncan be cleverly exploited to reduce the range of activations without additional\ncomputation cost. Based on this insight, we propose Weight Dilation (WD) that\nmaximally dilates the unsaturated in-channel weights to a constrained range\nthrough a mathematically equivalent scaling. WD costlessly absorbs the\nactivation quantization errors into weight quantization. The range of\nactivations decreases, which makes activations quantization easy. The range of\nweights remains constant, which makes model easy to converge in training stage.\nConsidering the temporal network leads to time-varying activations, we design a\nTemporal Parallel Quantizer (TPQ), which sets time-step quantization parameters\nand supports parallel quantization for different time steps, significantly\nimproving the performance and reducing time cost. To further enhance\nperformance while preserving efficiency, we introduce a Block-wise Knowledge\nDistillation (BKD) to align the quantized models with the full-precision models\nat a block level. The simultaneous training of time-step quantization\nparameters and weights minimizes the time required, and the shorter\nbackpropagation paths decreases the memory footprint of the quantization\nprocess.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Code: http://github.com/BienLuky/DilateQuant",
    "pdf_url": "http://arxiv.org/pdf/2409.14307v2",
    "published_date": "2024-09-22 04:21:29 UTC",
    "updated_date": "2024-09-25 15:56:46 UTC"
  },
  {
    "arxiv_id": "2409.14306v1",
    "title": "LLMs are One-Shot URL Classifiers and Explainers",
    "authors": [
      "Fariza Rashid",
      "Nishavi Ranaweera",
      "Ben Doyle",
      "Suranga Seneviratne"
    ],
    "abstract": "Malicious URL classification represents a crucial aspect of cyber security.\nAlthough existing work comprises numerous machine learning and deep\nlearning-based URL classification models, most suffer from generalisation and\ndomain-adaptation issues arising from the lack of representative training\ndatasets. Furthermore, these models fail to provide explanations for a given\nURL classification in natural human language. In this work, we investigate and\ndemonstrate the use of Large Language Models (LLMs) to address this issue.\nSpecifically, we propose an LLM-based one-shot learning framework that uses\nChain-of-Thought (CoT) reasoning to predict whether a given URL is benign or\nphishing. We evaluate our framework using three URL datasets and five\nstate-of-the-art LLMs and show that one-shot LLM prompting indeed provides\nperformances close to supervised models, with GPT 4-Turbo being the best model,\nfollowed by Claude 3 Opus. We conduct a quantitative analysis of the LLM\nexplanations and show that most of the explanations provided by LLMs align with\nthe post-hoc explanations of the supervised classifiers, and the explanations\nhave high readability, coherency, and informativeness.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.14306v1",
    "published_date": "2024-09-22 03:52:39 UTC",
    "updated_date": "2024-09-22 03:52:39 UTC"
  },
  {
    "arxiv_id": "2409.14305v1",
    "title": "UU-Mamba: Uncertainty-aware U-Mamba for Cardiovascular Segmentation",
    "authors": [
      "Ting Yu Tsai",
      "Li Lin",
      "Shu Hu",
      "Connie W. Tsao",
      "Xin Li",
      "Ming-Ching Chang",
      "Hongtu Zhu",
      "Xin Wang"
    ],
    "abstract": "Building on the success of deep learning models in cardiovascular structure\nsegmentation, increasing attention has been focused on improving generalization\nand robustness, particularly in small, annotated datasets. Despite recent\nadvancements, current approaches often face challenges such as overfitting and\naccuracy limitations, largely due to their reliance on large datasets and\nnarrow optimization techniques. This paper introduces the UU-Mamba model, an\nextension of the U-Mamba architecture, designed to address these challenges in\nboth cardiac and vascular segmentation. By incorporating Sharpness-Aware\nMinimization (SAM), the model enhances generalization by targeting flatter\nminima in the loss landscape. Additionally, we propose an uncertainty-aware\nloss function that combines region-based, distribution-based, and pixel-based\ncomponents to improve segmentation accuracy by capturing both local and global\nfeatures. While the UU-Mamba model has already demonstrated great performance,\nfurther testing is required to fully assess its generalization and robustness.\nWe expand our evaluation by conducting new trials on the ImageCAS (coronary\nartery) and Aorta (aortic branches and zones) datasets, which present more\ncomplex segmentation challenges than the ACDC dataset (left and right\nventricles) used in our previous work, showcasing the model's adaptability and\nresilience. We confirm UU-Mamba's superior performance over leading models such\nas TransUNet, Swin-Unet, nnUNet, and nnFormer. Moreover, we provide a more\ncomprehensive evaluation of the model's robustness and segmentation accuracy,\nas demonstrated by extensive experiments.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.14305v1",
    "published_date": "2024-09-22 03:22:06 UTC",
    "updated_date": "2024-09-22 03:22:06 UTC"
  },
  {
    "arxiv_id": "2409.14302v2",
    "title": "Reliable and diverse evaluation of LLM medical knowledge mastery",
    "authors": [
      "Yuxuan Zhou",
      "Xien Liu",
      "Chen Ning",
      "Xiao Zhang",
      "Ji Wu"
    ],
    "abstract": "Mastering medical knowledge is crucial for medical-specific LLMs. However,\ndespite the existence of medical benchmarks like MedQA, a unified framework\nthat fully leverages existing knowledge bases to evaluate LLMs' mastery of\nmedical knowledge is still lacking. In the study, we propose a novel framework\nPretexEval that dynamically generates reliable and diverse test samples to\nevaluate LLMs for any given medical knowledge base. We notice that test samples\nproduced directly from knowledge bases by templates or LLMs may introduce\nfactual errors and also lack diversity. To address these issues, we introduce a\nnovel schema into our proposed evaluation framework that employs predicate\nequivalence transformations to produce a series of variants for any given\nmedical knowledge point. Finally, these produced predicate variants are\nconverted into textual language, resulting in a series of reliable and diverse\ntest samples to evaluate whether LLMs fully master the given medical factual\nknowledge point. Here, we use our proposed framework to systematically\ninvestigate the mastery of medical factual knowledge of 12 well-known LLMs,\nbased on two knowledge bases that are crucial for clinical diagnosis and\ntreatment. The evaluation results illustrate that current LLMs still exhibit\nsignificant deficiencies in fully mastering medical knowledge, despite\nachieving considerable success on some famous public benchmarks. These new\nfindings provide valuable insights for developing medical-specific LLMs,\nhighlighting that current LLMs urgently need to strengthen their comprehensive\nand in-depth mastery of medical knowledge before being applied to real-world\nmedical scenarios.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "20 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.14302v2",
    "published_date": "2024-09-22 03:13:38 UTC",
    "updated_date": "2024-10-02 15:17:35 UTC"
  },
  {
    "arxiv_id": "2409.14296v1",
    "title": "HM3D-OVON: A Dataset and Benchmark for Open-Vocabulary Object Goal Navigation",
    "authors": [
      "Naoki Yokoyama",
      "Ram Ramrakhya",
      "Abhishek Das",
      "Dhruv Batra",
      "Sehoon Ha"
    ],
    "abstract": "We present the Habitat-Matterport 3D Open Vocabulary Object Goal Navigation\ndataset (HM3D-OVON), a large-scale benchmark that broadens the scope and\nsemantic range of prior Object Goal Navigation (ObjectNav) benchmarks.\nLeveraging the HM3DSem dataset, HM3D-OVON incorporates over 15k annotated\ninstances of household objects across 379 distinct categories, derived from\nphoto-realistic 3D scans of real-world environments. In contrast to earlier\nObjectNav datasets, which limit goal objects to a predefined set of 6-20\ncategories, HM3D-OVON facilitates the training and evaluation of models with an\nopen-set of goals defined through free-form language at test-time. Through this\nopen-vocabulary formulation, HM3D-OVON encourages progress towards learning\nvisuo-semantic navigation behaviors that are capable of searching for any\nobject specified by text in an open-vocabulary manner. Additionally, we\nsystematically evaluate and compare several different types of approaches on\nHM3D-OVON. We find that HM3D-OVON can be used to train an open-vocabulary\nObjectNav agent that achieves both higher performance and is more robust to\nlocalization and actuation noise than the state-of-the-art ObjectNav approach.\nWe hope that our benchmark and baseline results will drive interest in\ndeveloping embodied agents that can navigate real-world spaces to find\nhousehold objects specified through free-form language, taking a step towards\nmore flexible and human-like semantic visual navigation. Code and videos\navailable at: naoki.io/ovon.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.14296v1",
    "published_date": "2024-09-22 02:12:29 UTC",
    "updated_date": "2024-09-22 02:12:29 UTC"
  },
  {
    "arxiv_id": "2409.16322v1",
    "title": "Towards Within-Class Variation in Alzheimer's Disease Detection from Spontaneous Speech",
    "authors": [
      "Jiawen Kang",
      "Dongrui Han",
      "Lingwei Meng",
      "Jingyan Zhou",
      "Jinchao Li",
      "Xixin Wu",
      "Helen Meng"
    ],
    "abstract": "Alzheimer's Disease (AD) detection has emerged as a promising research area\nthat employs machine learning classification models to distinguish between\nindividuals with AD and those without. Unlike conventional classification\ntasks, we identify within-class variation as a critical challenge in AD\ndetection: individuals with AD exhibit a spectrum of cognitive impairments.\nGiven that many AD detection tasks lack fine-grained labels, simplistic binary\nclassification may overlook two crucial aspects: within-class differences and\ninstance-level imbalance. The former compels the model to map AD samples with\nvarying degrees of impairment to a single diagnostic label, disregarding\ncertain changes in cognitive function. While the latter biases the model\ntowards overrepresented severity levels. This work presents early efforts to\naddress these challenges. We propose two novel methods: Soft Target\nDistillation (SoTD) and Instance-level Re-balancing (InRe), targeting two\nproblems respectively. Experiments on the ADReSS and ADReSSo datasets\ndemonstrate that the proposed methods significantly improve detection accuracy.\nFurther analysis reveals that SoTD effectively harnesses the strengths of\nmultiple component models, while InRe substantially alleviates model\nover-fitting. These findings provide insights for developing more robust and\nreliable AD detection models.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.SD",
      "q-bio.NC"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.16322v1",
    "published_date": "2024-09-22 02:06:05 UTC",
    "updated_date": "2024-09-22 02:06:05 UTC"
  },
  {
    "arxiv_id": "2409.14292v1",
    "title": "Opinion Mining on Offshore Wind Energy for Environmental Engineering",
    "authors": [
      "Isabele Bittencourt",
      "Aparna S. Varde",
      "Pankaj Lal"
    ],
    "abstract": "In this paper, we conduct sentiment analysis on social media data to study\nmass opinion about offshore wind energy. We adapt three machine learning\nmodels, namely, TextBlob, VADER, and SentiWordNet because different functions\nare provided by each model. TextBlob provides subjectivity analysis as well as\npolarity classification. VADER offers cumulative sentiment scores. SentiWordNet\nconsiders sentiments with reference to context and performs classification\naccordingly. Techniques in NLP are harnessed to gather meaning from the textual\ndata in social media. Data visualization tools are suitably deployed to display\nthe overall results. This work is much in line with citizen science and smart\ngovernance via involvement of mass opinion to guide decision support. It\nexemplifies the role of Machine Learning and NLP here.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "I.2.7; I.2.m; J.2"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.14292v1",
    "published_date": "2024-09-22 01:51:43 UTC",
    "updated_date": "2024-09-22 01:51:43 UTC"
  },
  {
    "arxiv_id": "2409.14285v1",
    "title": "ESPERANTO: Evaluating Synthesized Phrases to Enhance Robustness in AI Detection for Text Origination",
    "authors": [
      "Navid Ayoobi",
      "Lily Knab",
      "Wen Cheng",
      "David Pantoja",
      "Hamidreza Alikhani",
      "Sylvain Flamant",
      "Jin Kim",
      "Arjun Mukherjee"
    ],
    "abstract": "While large language models (LLMs) exhibit significant utility across various\ndomains, they simultaneously are susceptible to exploitation for unethical\npurposes, including academic misconduct and dissemination of misinformation.\nConsequently, AI-generated text detection systems have emerged as a\ncountermeasure. However, these detection mechanisms demonstrate vulnerability\nto evasion techniques and lack robustness against textual manipulations. This\npaper introduces back-translation as a novel technique for evading detection,\nunderscoring the need to enhance the robustness of current detection systems.\nThe proposed method involves translating AI-generated text through multiple\nlanguages before back-translating to English. We present a model that combines\nthese back-translated texts to produce a manipulated version of the original\nAI-generated text. Our findings demonstrate that the manipulated text retains\nthe original semantics while significantly reducing the true positive rate\n(TPR) of existing detection methods. We evaluate this technique on nine AI\ndetectors, including six open-source and three proprietary systems, revealing\ntheir susceptibility to back-translation manipulation. In response to the\nidentified shortcomings of existing AI text detectors, we present a\ncountermeasure to improve the robustness against this form of manipulation. Our\nresults indicate that the TPR of the proposed method declines by only 1.85%\nafter back-translation manipulation. Furthermore, we build a large dataset of\n720k texts using eight different LLMs. Our dataset contains both human-authored\nand LLM-generated texts in various domains and writing styles to assess the\nperformance of our method and existing detectors. This dataset is publicly\nshared for the benefit of the research community.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.14285v1",
    "published_date": "2024-09-22 01:13:22 UTC",
    "updated_date": "2024-09-22 01:13:22 UTC"
  },
  {
    "arxiv_id": "2409.14277v1",
    "title": "Can-Do! A Dataset and Neuro-Symbolic Grounded Framework for Embodied Planning with Large Multimodal Models",
    "authors": [
      "Yew Ken Chia",
      "Qi Sun",
      "Lidong Bing",
      "Soujanya Poria"
    ],
    "abstract": "Large multimodal models have demonstrated impressive problem-solving\nabilities in vision and language tasks, and have the potential to encode\nextensive world knowledge. However, it remains an open challenge for these\nmodels to perceive, reason, plan, and act in realistic environments. In this\nwork, we introduce Can-Do, a benchmark dataset designed to evaluate embodied\nplanning abilities through more diverse and complex scenarios than previous\ndatasets. Our dataset includes 400 multimodal samples, each consisting of\nnatural language user instructions, visual images depicting the environment,\nstate changes, and corresponding action plans. The data encompasses diverse\naspects of commonsense knowledge, physical understanding, and safety awareness.\nOur fine-grained analysis reveals that state-of-the-art models, including\nGPT-4V, face bottlenecks in visual perception, comprehension, and reasoning\nabilities. To address these challenges, we propose NeuroGround, a neurosymbolic\nframework that first grounds the plan generation in the perceived environment\nstates and then leverages symbolic planning engines to augment the\nmodel-generated plans. Experimental results demonstrate the effectiveness of\nour framework compared to strong baselines. Our code and dataset are available\nat https://embodied-planning.github.io.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.14277v1",
    "published_date": "2024-09-22 00:30:11 UTC",
    "updated_date": "2024-09-22 00:30:11 UTC"
  },
  {
    "arxiv_id": "2409.14274v1",
    "title": "Proof Automation with Large Language Models",
    "authors": [
      "Minghai Lu",
      "Benjamin Delaware",
      "Tianyi Zhang"
    ],
    "abstract": "Interactive theorem provers such as Coq are powerful tools to formally\nguarantee the correctness of software. However, using these tools requires\nsignificant manual effort and expertise. While Large Language Models (LLMs)\nhave shown promise in automatically generating informal proofs in natural\nlanguage, they are less effective at generating formal proofs in interactive\ntheorem provers. In this paper, we conduct a formative study to identify common\nmistakes made by LLMs when asked to generate formal proofs. By analyzing 520\nproof generation errors made by GPT-3.5, we found that GPT-3.5 often identified\nthe correct high-level structure of a proof, but struggled to get the\nlower-level details correct. Based on this insight, we propose PALM, a novel\ngenerate-then-repair approach that first prompts an LLM to generate an initial\nproof and then leverages targeted symbolic methods to iteratively repair\nlow-level problems. We evaluate PALM on a large dataset that includes more than\n10K theorems. Our results show that PALM significantly outperforms other\nstate-of-the-art approaches, successfully proving 76.6% to 180.4% more\ntheorems. Moreover, PALM proves 1270 theorems beyond the reach of existing\napproaches. We also demonstrate the generalizability of PALM across different\nLLMs.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG",
      "cs.LO",
      "cs.PL",
      "D.2.4; I.2.2"
    ],
    "primary_category": "cs.SE",
    "comment": "12 pages, 15 figures, Accepted to ASE 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.14274v1",
    "published_date": "2024-09-22 00:19:27 UTC",
    "updated_date": "2024-09-22 00:19:27 UTC"
  }
]
[
  {
    "arxiv_id": "2510.09926v1",
    "title": "Phase-Aware Deep Learning with Complex-Valued CNNs for Audio Signal Applications",
    "authors": [
      "Naman Agrawal"
    ],
    "abstract": "This study explores the design and application of Complex-Valued Convolutional Neural Networks (CVCNNs) in audio signal processing, with a focus on preserving and utilizing phase information often neglected in real-valued networks. We begin by presenting the foundational theoretical concepts of CVCNNs, including complex convolutions, pooling layers, Wirtinger-based differentiation, and various complex-valued activation functions. These are complemented by critical adaptations of training techniques, including complex batch normalization and weight initialization schemes, to ensure stability in training dynamics. Empirical evaluations are conducted across three stages. First, CVCNNs are benchmarked on standard image datasets, where they demonstrate competitive performance with real-valued CNNs, even under synthetic complex perturbations. Although our focus is audio signal processing, we first evaluate CVCNNs on image datasets to establish baseline performance and validate training stability before applying them to audio tasks. In the second experiment, we focus on audio classification using Mel-Frequency Cepstral Coefficients (MFCCs). CVCNNs trained on real-valued MFCCs slightly outperform real CNNs, while preserving phase in input workflows highlights challenges in exploiting phase without architectural modifications. Finally, a third experiment introduces GNNs to model phase information via edge weighting, where the inclusion of phase yields measurable gains in both binary and multi-class genre classification. These results underscore the expressive capacity of complex-valued architectures and confirm phase as a meaningful and exploitable feature in audio processing applications. While current methods show promise, especially with activations like cardioid, future advances in phase-aware design will be essential to leverage the potential of complex representations in neural networks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09926v1",
    "published_date": "2025-10-10 23:55:35 UTC",
    "updated_date": "2025-10-10 23:55:35 UTC"
  },
  {
    "arxiv_id": "2510.09914v1",
    "title": "Augmenting generative models with biomedical knowledge graphs improves targeted drug discovery",
    "authors": [
      "Aditya Malusare",
      "Vineet Punyamoorty",
      "Vaneet Aggarwal"
    ],
    "abstract": "Recent breakthroughs in generative modeling have demonstrated remarkable capabilities in molecular generation, yet the integration of comprehensive biomedical knowledge into these models has remained an untapped frontier. In this study, we introduce K-DREAM (Knowledge-Driven Embedding-Augmented Model), a novel framework that leverages knowledge graphs to augment diffusion-based generative models for drug discovery. By embedding structured information from large-scale knowledge graphs, K-DREAM directs molecular generation toward candidates with higher biological relevance and therapeutic suitability. This integration ensures that the generated molecules are aligned with specific therapeutic targets, moving beyond traditional heuristic-driven approaches. In targeted drug design tasks, K-DREAM generates drug candidates with improved binding affinities and predicted efficacy, surpassing current state-of-the-art generative models. It also demonstrates flexibility by producing molecules designed for multiple targets, enabling applications to complex disease mechanisms. These results highlight the utility of knowledge-enhanced generative models in rational drug design and their relevance to practical therapeutic development.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper has been accepted for publication in the IEEE Transactions on Artificial Intelligence, October 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.09914v1",
    "published_date": "2025-10-10 23:18:20 UTC",
    "updated_date": "2025-10-10 23:18:20 UTC"
  },
  {
    "arxiv_id": "2510.09912v1",
    "title": "SpectralCA: Bi-Directional Cross-Attention for Next-Generation UAV Hyperspectral Vision",
    "authors": [
      "D. V. Brovko"
    ],
    "abstract": "The relevance of this research lies in the growing demand for unmanned aerial vehicles (UAVs) capable of operating reliably in complex environments where conventional navigation becomes unreliable due to interference, poor visibility, or camouflage. Hyperspectral imaging (HSI) provides unique opportunities for UAV-based computer vision by enabling fine-grained material recognition and object differentiation, which are critical for navigation, surveillance, agriculture, and environmental monitoring. The aim of this work is to develop a deep learning architecture integrating HSI into UAV perception for navigation, object detection, and terrain classification. Objectives include: reviewing existing HSI methods, designing a hybrid 2D/3D convolutional architecture with spectral-spatial cross-attention, training, and benchmarking. The methodology is based on the modification of the Mobile 3D Vision Transformer (MDvT) by introducing the proposed SpectralCA block. This block employs bi-directional cross-attention to fuse spectral and spatial features, enhancing accuracy while reducing parameters and inference time. Experimental evaluation was conducted on the WHU-Hi-HongHu dataset, with results assessed using Overall Accuracy, Average Accuracy, and the Kappa coefficient. The findings confirm that the proposed architecture improves UAV perception efficiency, enabling real-time operation for navigation, object recognition, and environmental monitoring tasks.\n  Keywords: SpectralCA, deep learning, computer vision, hyperspectral imaging, unmanned aerial vehicle, object detection, semi-supervised learning.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "The work consists of three chapters, includes 12 figures, 4 tables, 31 references, and 1 appendix. A version of this work has been accepted for presentation at the 2025 IEEE 8th International Conference on Methods and Systems of Navigation and Motion Control",
    "pdf_url": "https://arxiv.org/pdf/2510.09912v1",
    "published_date": "2025-10-10 22:53:28 UTC",
    "updated_date": "2025-10-10 22:53:28 UTC"
  },
  {
    "arxiv_id": "2510.09907v1",
    "title": "Agentic Property-Based Testing: Finding Bugs Across the Python Ecosystem",
    "authors": [
      "Muhammad Maaz",
      "Liam DeVoe",
      "Zac Hatfield-Dodds",
      "Nicholas Carlini"
    ],
    "abstract": "Property-based testing (PBT) is a lightweight formal method, typically implemented as a randomized testing framework. Users specify the input domain for their test using combinators supplied by the PBT framework, and the expected properties or invariants as a unit-test function. The framework then searches for a counterexample, e.g. by generating inputs and calling the test function. In this work, we demonstrate an LLM-based agent which analyzes Python modules, infers function-specific and cross-function properties from code and documentation, synthesizes and executes PBTs, reflects on outputs of these tests to confirm true bugs, and finally outputs actionable bug reports for the developer. We perform an extensive evaluation of our agent across 100 popular Python packages. Of the bug reports generated by the agent, we found after manual review that 56\\% were valid bugs and 32\\% were valid bugs that we would report to maintainers. We then developed a ranking rubric to surface high-priority valid bugs to developers, and found that of the 21 top-scoring bugs, 86\\% were valid and 81\\% we would report. The bugs span diverse failure modes from serialization failures to numerical precision errors to flawed cache implementations. We reported 5 bugs, 4 with patches, including to NumPy and cloud computing SDKs, with 3 patches merged successfully. Our results suggest that LLMs with PBT provides a rigorous and scalable method for autonomously testing software. Our code and artifacts are available at: https://github.com/mmaaz-git/agentic-pbt.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "4 pages (main), NeurIPS 2025, The 4th Deep Learning for Code Workshop",
    "pdf_url": "https://arxiv.org/pdf/2510.09907v1",
    "published_date": "2025-10-10 22:43:54 UTC",
    "updated_date": "2025-10-10 22:43:54 UTC"
  },
  {
    "arxiv_id": "2510.09905v1",
    "title": "The Personalization Trap: How User Memory Alters Emotional Reasoning in LLMs",
    "authors": [
      "Xi Fang",
      "Weijie Xu",
      "Yuchong Zhang",
      "Stephanie Eckman",
      "Scott Nickleach",
      "Chandan K. Reddy"
    ],
    "abstract": "When an AI assistant remembers that Sarah is a single mother working two jobs, does it interpret her stress differently than if she were a wealthy executive? As personalized AI systems increasingly incorporate long-term user memory, understanding how this memory shapes emotional reasoning is critical. We investigate how user memory affects emotional intelligence in large language models (LLMs) by evaluating 15 models on human validated emotional intelligence tests. We find that identical scenarios paired with different user profiles produce systematically divergent emotional interpretations. Across validated user independent emotional scenarios and diverse user profiles, systematic biases emerged in several high-performing LLMs where advantaged profiles received more accurate emotional interpretations. Moreover, LLMs demonstrate significant disparities across demographic factors in emotion understanding and supportive recommendations tasks, indicating that personalization mechanisms can embed social hierarchies into models emotional reasoning. These results highlight a key challenge for memory enhanced AI: systems designed for personalization may inadvertently reinforce social inequalities.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "12 pages 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.09905v1",
    "published_date": "2025-10-10 22:39:37 UTC",
    "updated_date": "2025-10-10 22:39:37 UTC"
  },
  {
    "arxiv_id": "2510.09904v1",
    "title": "Stability of Transformers under Layer Normalization",
    "authors": [
      "Kelvin Kan",
      "Xingjian Li",
      "Benjamin J. Zhang",
      "Tuhin Sahai",
      "Stanley Osher",
      "Krishna Kumar",
      "Markos A. Katsoulakis"
    ],
    "abstract": "Despite their widespread use, training deep Transformers can be unstable. Layer normalization, a standard component, improves training stability, but its placement has often been ad-hoc. In this paper, we conduct a principled study on the forward (hidden states) and backward (gradient) stability of Transformers under different layer normalization placements. Our theory provides key insights into the training dynamics: whether training drives Transformers toward regular solutions or pathological behaviors. For forward stability, we derive explicit bounds on the growth of hidden states in trained Transformers. For backward stability, we analyze how layer normalization affects the backpropagation of gradients, thereby explaining the training dynamics of each layer normalization placement. Our analysis also guides the scaling of residual steps in Transformer blocks, where appropriate choices can further improve stability and performance. Our numerical results corroborate our theoretical findings. Beyond these results, our framework provides a principled way to sanity-check the stability of Transformers under new architectural modifications, offering guidance for future designs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09904v1",
    "published_date": "2025-10-10 22:27:20 UTC",
    "updated_date": "2025-10-10 22:27:20 UTC"
  },
  {
    "arxiv_id": "2510.09901v1",
    "title": "Autonomous Agents for Scientific Discovery: Orchestrating Scientists, Language, Code, and Physics",
    "authors": [
      "Lianhao Zhou",
      "Hongyi Ling",
      "Cong Fu",
      "Yepeng Huang",
      "Michael Sun",
      "Wendi Yu",
      "Xiaoxuan Wang",
      "Xiner Li",
      "Xingyu Su",
      "Junkai Zhang",
      "Xiusi Chen",
      "Chenxing Liang",
      "Xiaofeng Qian",
      "Heng Ji",
      "Wei Wang",
      "Marinka Zitnik",
      "Shuiwang Ji"
    ],
    "abstract": "Computing has long served as a cornerstone of scientific discovery. Recently, a paradigm shift has emerged with the rise of large language models (LLMs), introducing autonomous systems, referred to as agents, that accelerate discovery across varying levels of autonomy. These language agents provide a flexible and versatile framework that orchestrates interactions with human scientists, natural language, computer language and code, and physics. This paper presents our view and vision of LLM-based scientific agents and their growing role in transforming the scientific discovery lifecycle, from hypothesis discovery, experimental design and execution, to result analysis and refinement. We critically examine current methodologies, emphasizing key innovations, practical achievements, and outstanding limitations. Additionally, we identify open research challenges and outline promising directions for building more robust, generalizable, and adaptive scientific agents. Our analysis highlights the transformative potential of autonomous agents to accelerate scientific discovery across diverse domains.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09901v1",
    "published_date": "2025-10-10 22:26:26 UTC",
    "updated_date": "2025-10-10 22:26:26 UTC"
  },
  {
    "arxiv_id": "2510.09898v1",
    "title": "Learning Bug Context for PyTorch-to-JAX Translation with LLMs",
    "authors": [
      "Hung Phan",
      "Son Le Vu",
      "Ali Jannesari"
    ],
    "abstract": "Despite recent progress of large language models (LLMs) on code translation among mainstream languages, translating PyTorch to JAX remains nontrivial. The two libraries, though both embedded in Python, differ in core design, execution semantics, and ecosystem maturity; JAX is newer and comparatively underrepresented in public code, and parallel PyTorch--JAX corpora are limited. Weaknesses in existing evaluation further complicate cross-framework benchmarking. We present T2J, a prompt-augmentation framework that strengthens LLM-based PyTorch to JAX translation. Our pipeline (i) assembles two PyTorch sources -- the problem-solving set from TorchLeet (Aroori & Chien, 2025) and a GitHub-derived set from CodeParrot (Wolf et al., 2022) -- and uses GPT-4o-mini to produce initial JAX drafts; (ii) engages two professional developers to iteratively repair those drafts until functional equivalence, yielding a curated fixed-bug dataset of common errors and patches; and (iii) constructs augmented prompts that inject structured guidance from these fixes to steer lightweight LLMs (e.g., GPT-4o-mini). We also introduce three metrics tailored to PyTorch to JAX: T2J CodeTrans Score, T2J FixCost Score (an LLM-based estimate of bug-fix effort), and T2J Comparison Score (LLM-as-judge). Empirically, T2J raises GPT-4o-mini performance by up to 10% on CodeBLEU, 50% on T2J FixCost Score, 1.33 points on T2J CodeTrans Score (0--4 scale), and 100% on T2J Comparison Score; moreover, the generated code runs up to 2.5x faster than the baseline.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09898v1",
    "published_date": "2025-10-10 22:22:36 UTC",
    "updated_date": "2025-10-10 22:22:36 UTC"
  },
  {
    "arxiv_id": "2510.09895v3",
    "title": "Chain-of-Influence: Tracing Interdependencies Across Time and Features in Clinical Predictive Modelings",
    "authors": [
      "Yubo Li",
      "Rema Padman"
    ],
    "abstract": "Modeling clinical time-series data is hampered by the challenge of capturing latent, time-varying dependencies among features. State-of-the-art approaches often rely on black-box mechanisms or simple aggregation, failing to explicitly model how the influence of one clinical variable propagates through others over time. We propose $\\textbf{Chain-of-Influence (CoI)}$, an interpretable deep learning framework that constructs an explicit, time-unfolded graph of feature interactions. CoI enables the tracing of influence pathways, providing a granular audit trail that shows how any feature at any time contributes to the final prediction, both directly and through its influence on other variables. We evaluate CoI on mortality and disease progression tasks using the MIMIC-IV dataset and a chronic kidney disease cohort. Our framework achieves state-of-the-art predictive performance (AUROC of 0.960 on CKD progression and 0.950 on ICU mortality), with deletion-based sensitivity analyses confirming that CoI's learned attributions faithfully reflect its decision process. Through case studies, we demonstrate that CoI uncovers clinically meaningful, patient-specific patterns of disease progression, offering enhanced transparency into the temporal and cross-feature dependencies that inform clinical decision-making.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09895v3",
    "published_date": "2025-10-10 22:20:51 UTC",
    "updated_date": "2026-01-18 17:09:15 UTC"
  },
  {
    "arxiv_id": "2510.09894v1",
    "title": "Beyond AlphaEarth: Toward Human-Centered Spatial Representation via POI-Guided Contrastive Learning",
    "authors": [
      "Junyuan Liu",
      "Quan Qin",
      "Guangsheng Dong",
      "Xinglei Wang",
      "Jiazhuang Feng",
      "Zichao Zeng",
      "Tao Cheng"
    ],
    "abstract": "General-purpose spatial representations are essential for building transferable geospatial foundation models (GFMs). Among them, the AlphaEarth Foundation (AE) represents a major step toward a global, unified representation of the Earth's surface, learning 10-meter embeddings from multi-source Earth Observation (EO) data that capture rich physical and environmental patterns across diverse landscapes. However, such EO-driven representations remain limited in capturing the functional and socioeconomic dimensions of cities, as they primarily encode physical and spectral patterns rather than human activities or spatial functions. We propose AETHER (AlphaEarth-POI Enriched Representation Learning), a lightweight framework that adapts AlphaEarth to human-centered urban analysis through multimodal alignment guided by Points of Interest (POIs). AETHER aligns AE embeddings with textual representations of POIs, enriching physically grounded EO features with semantic cues about urban functions and socioeconomic contexts. In Greater London, AETHER achieves consistent gains over the AE baseline, with a 7.2% relative improvement in land-use classification F1 and a 23.6% relative reduction in Kullback-Leibler divergence for socioeconomic mapping. Built upon pretrained AE, AETHER leverages a lightweight multimodal alignment to enrich it with human-centered semantics while remaining computationally efficient and scalable for urban applications. By coupling EO with human-centered semantics, it advances geospatial foundation models toward general-purpose urban representations that integrate both physical form and functional meaning.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09894v1",
    "published_date": "2025-10-10 22:20:39 UTC",
    "updated_date": "2025-10-10 22:20:39 UTC"
  },
  {
    "arxiv_id": "2510.09891v1",
    "title": "Probabilistic bias adjustment of seasonal predictions of Arctic Sea Ice Concentration",
    "authors": [
      "Parsa Gooya",
      "Reinel Sospedra-Alfonso"
    ],
    "abstract": "Seasonal forecast of Arctic sea ice concentration is key to mitigate the negative impact and assess potential opportunities posed by the rapid decline of sea ice coverage. Seasonal prediction systems based on climate models often show systematic biases and complex spatio-temporal errors that grow with the forecasts. Consequently, operational predictions are routinely bias corrected and calibrated using retrospective forecasts. For predictions of Arctic sea ice concentration, error corrections are mainly based on one-to-one post-processing methods including climatological mean or linear regression correction and, more recently, machine learning. Such deterministic adjustments are confined at best to the limited number of costly-to-run ensemble members of the raw forecast. However, decision-making requires proper quantification of uncertainty and likelihood of events, particularly of extremes. We introduce a probabilistic error correction framework based on a conditional Variational Autoencoder model to map the conditional distribution of observations given the biased model prediction. This method naturally allows for generating large ensembles of adjusted forecasts. We evaluate our model using deterministic and probabilistic metrics and show that the adjusted forecasts are better calibrated, closer to the observational distribution, and have smaller errors than climatological mean adjusted forecasts.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.ao-ph",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09891v1",
    "published_date": "2025-10-10 22:17:29 UTC",
    "updated_date": "2025-10-10 22:17:29 UTC"
  },
  {
    "arxiv_id": "2510.09885v2",
    "title": "Closing the Data-Efficiency Gap Between Autoregressive and Masked Diffusion LLMs",
    "authors": [
      "Xu Pan",
      "Ely Hahami",
      "Jingxuan Fan",
      "Ziqian Xie",
      "Haim Sompolinsky"
    ],
    "abstract": "Despite autoregressive large language models (arLLMs) being the current dominant paradigm in language modeling, effectively updating these models to incorporate new factual knowledge still remains difficult. They resist knowledge injection via fine-tuning due to inherent shortcomings such as the \"reversal curse\" -- the challenge of answering questions that reverse the original information order in the training sample. Masked diffusion large language models (dLLMs) are rapidly emerging as a powerful alternative to the arLLM paradigm, with evidence of better data efficiency and free of the \"reversal curse\" in pre-training. However, it is unknown whether these advantages extend to the post-training phase, i.e. whether pre-trained dLLMs can easily acquire new knowledge through fine-tuning. On three diverse datasets, we fine-tune arLLMs and dLLMs, evaluating them with forward and backward style Question Answering (QA) to probe knowledge generalization and the reversal curse. Our results confirm that arLLMs critically rely on extensive data augmentation via paraphrases for QA generalization, and paraphrases are only effective when their information order matches the QA style. Conversely, dLLMs achieve high accuracies on both forward and backward QAs without paraphrases; adding paraphrases yields only marginal gains. Inspired by the dLLM's performance, we introduce a novel masked fine-tuning paradigm for knowledge injection into pre-trained arLLMs. This proposed method successfully and drastically improves the data efficiency of arLLM fine-tuning, effectively closing its performance gap with dLLMs. We further show that the masked fine-tuning paradigm of arLLMs can be extended to the supervised fine-tuning (SFT) of mathematical capability. Across two models and two datasets, our masked SFT outperforms regular SFT.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09885v2",
    "published_date": "2025-10-10 21:43:50 UTC",
    "updated_date": "2025-11-29 20:03:40 UTC"
  },
  {
    "arxiv_id": "2510.17846v1",
    "title": "CARLE: A Hybrid Deep-Shallow Learning Framework for Robust and Explainable RUL Estimation of Rolling Element Bearings",
    "authors": [
      "Waleed Razzaq",
      "Yun-Bo Zhao"
    ],
    "abstract": "Prognostic Health Management (PHM) systems monitor and predict equipment health. A key task is Remaining Useful Life (RUL) estimation, which predicts how long a component, such as a rolling element bearing, will operate before failure. Many RUL methods exist but often lack generalizability and robustness under changing operating conditions. This paper introduces CARLE, a hybrid AI framework that combines deep and shallow learning to address these challenges. CARLE uses Res-CNN and Res-LSTM blocks with multi-head attention and residual connections to capture spatial and temporal degradation patterns, and a Random Forest Regressor (RFR) for stable, accurate RUL prediction. A compact preprocessing pipeline applies Gaussian filtering for noise reduction and Continuous Wavelet Transform (CWT) for time-frequency feature extraction. We evaluate CARLE on the XJTU-SY and PRONOSTIA bearing datasets. Ablation studies measure each component's contribution, while noise and cross-domain experiments test robustness and generalization. Comparative results show CARLE outperforms several state-of-the-art methods, especially under dynamic conditions. Finally, we analyze model interpretability with LIME and SHAP to assess transparency and trustworthiness.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "26 pages, accepted at Soft Computing",
    "pdf_url": "https://arxiv.org/pdf/2510.17846v1",
    "published_date": "2025-10-10 21:43:26 UTC",
    "updated_date": "2025-10-10 21:43:26 UTC"
  },
  {
    "arxiv_id": "2510.09879v1",
    "title": "CHUG: Crowdsourced User-Generated HDR Video Quality Dataset",
    "authors": [
      "Shreshth Saini",
      "Alan C. Bovik",
      "Neil Birkbeck",
      "Yilin Wang",
      "Balu Adsumilli"
    ],
    "abstract": "High Dynamic Range (HDR) videos enhance visual experiences with superior brightness, contrast, and color depth. The surge of User-Generated Content (UGC) on platforms like YouTube and TikTok introduces unique challenges for HDR video quality assessment (VQA) due to diverse capture conditions, editing artifacts, and compression distortions. Existing HDR-VQA datasets primarily focus on professionally generated content (PGC), leaving a gap in understanding real-world UGC-HDR degradations. To address this, we introduce CHUG: Crowdsourced User-Generated HDR Video Quality Dataset, the first large-scale subjective study on UGC-HDR quality. CHUG comprises 856 UGC-HDR source videos, transcoded across multiple resolutions and bitrates to simulate real-world scenarios, totaling 5,992 videos. A large-scale study via Amazon Mechanical Turk collected 211,848 perceptual ratings. CHUG provides a benchmark for analyzing UGC-specific distortions in HDR videos. We anticipate CHUG will advance No-Reference (NR) HDR-VQA research by offering a large-scale, diverse, and real-world UGC dataset. The dataset is publicly available at: https://shreshthsaini.github.io/CHUG/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09879v1",
    "published_date": "2025-10-10 21:35:39 UTC",
    "updated_date": "2025-10-10 21:35:39 UTC"
  },
  {
    "arxiv_id": "2510.09877v1",
    "title": "Myopic Bayesian Decision Theory for Batch Active Learning with Partial Batch Label Sampling",
    "authors": [
      "Kangping Hu",
      "Stephen Mussmann"
    ],
    "abstract": "Over the past couple of decades, many active learning acquisition functions have been proposed, leaving practitioners with an unclear choice of which to use. Bayesian Decision Theory (BDT) offers a universal principle to guide decision-making. In this work, we derive BDT for (Bayesian) active learning in the myopic framework, where we imagine we only have one more point to label. This derivation leads to effective algorithms such as Expected Error Reduction (EER), Expected Predictive Information Gain (EPIG), and other algorithms that appear in the literature. Furthermore, we show that BAIT (active learning based on V-optimal experimental design) can be derived from BDT and asymptotic approximations. A key challenge of such methods is the difficult scaling to large batch sizes, leading to either computational challenges (BatchBALD) or dramatic performance drops (top-$B$ selection). Here, using a particular formulation of the decision process, we derive Partial Batch Label Sampling (ParBaLS) for the EPIG algorithm. We show experimentally for several datasets that ParBaLS EPIG gives superior performance for a fixed budget and Bayesian Logistic Regression on Neural Embeddings. Our code is available at https://github.com/ADDAPT-ML/ParBaLS.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09877v1",
    "published_date": "2025-10-10 21:28:42 UTC",
    "updated_date": "2025-10-10 21:28:42 UTC"
  },
  {
    "arxiv_id": "2510.09874v1",
    "title": "ROBOPSY PL[AI]: Using Role-Play to Investigate how LLMs Present Collective Memory",
    "authors": [
      "Margarete Jahrmann",
      "Thomas Brandstetter",
      "Stefan Glasauer"
    ],
    "abstract": "The paper presents the first results of an artistic research project investigating how Large Language Models (LLMs) curate and present collective memory. In a public installation exhibited during two months in Vienna in 2025, visitors could interact with five different LLMs (ChatGPT with GPT 4o and GPT 4o mini, Mistral Large, DeepSeek-Chat, and a locally run Llama 3.1 model), which were instructed to act as narrators, implementing a role-playing game revolving around the murder of Austrian philosopher Moritz Schlick in 1936. Results of the investigation include protocols of LLM-user interactions during the game and qualitative conversations after the play experience to get insight into the players' reactions to the game. In a quantitative analysis 115 introductory texts for role-playing generated by the LLMs were examined by different methods of natural language processing, including semantic similarity and sentiment analysis. While the qualitative player feedback allowed to distinguish three distinct types of users, the quantitative text analysis showed significant differences between how the different LLMs presented the historical content. Our study thus adds to ongoing efforts to analyse LLM performance, but also suggests a way of how these efforts can be disseminated in a playful way to a general audience.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "17 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.09874v1",
    "published_date": "2025-10-10 21:25:06 UTC",
    "updated_date": "2025-10-10 21:25:06 UTC"
  },
  {
    "arxiv_id": "2510.09872v1",
    "title": "WARC-Bench: Web Archive Based Benchmark for GUI Subtask Executions",
    "authors": [
      "Sanjari Srivastava",
      "Gang Li",
      "Cheng Chang",
      "Rishu Garg",
      "Manpreet Kaur",
      "Charlene Y. Lee",
      "Yuezhang Li",
      "Yining Mao",
      "Ignacio Cases",
      "Yanan Xie",
      "Peng Qi"
    ],
    "abstract": "Training web agents to navigate complex, real-world websites requires them to master $\\textit{subtasks}$ - short-horizon interactions on multiple UI components (e.g., choosing the correct date in a date picker, or scrolling in a container to extract information). We introduce WARC-Bench (Web Archive Benchmark), a novel web navigation benchmark featuring 438 tasks designed to evaluate multimodal AI agents on subtasks. WARC-Bench enables sandboxed interactions with dynamic and realistic webpages using Web ARChive files. We show that WARC-Bench is challenging for leading computer-use models, with the highest observed success rate being 64.8%. To improve open source models on subtask, we explore two common training techniques: supervised fine-tuning (SFT) and reinforcement learning with verifiable rewards (RLVR). Experiments show that SFT models obtain a 48.8% success rate on the benchmark. Training with RLVR over SFT checkpoints, even in data-scarce settings, improves the score to 52.8% on WARC-Bench, outperforming many frontier models. Our analysis concludes that mastering these subtasks is essential for robust web planning and navigation, and is a capability not extensively evaluated by existing benchmarks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09872v1",
    "published_date": "2025-10-10 21:20:51 UTC",
    "updated_date": "2025-10-10 21:20:51 UTC"
  },
  {
    "arxiv_id": "2510.09869v3",
    "title": "NarraBench: A Comprehensive Framework for Narrative Benchmarking",
    "authors": [
      "Sil Hamilton",
      "Matthew Wilkens",
      "Andrew Piper"
    ],
    "abstract": "We present NarraBench, a theory-informed taxonomy of narrative-understanding tasks, as well as an associated survey of 78 existing benchmarks in the area. We find significant need for new evaluations covering aspects of narrative understanding that are either overlooked in current work or are poorly aligned with existing metrics. Specifically, we estimate that only 27% of narrative tasks are well captured by existing benchmarks, and we note that some areas -- including narrative events, style, perspective, and revelation -- are nearly absent from current evaluations. We also note the need for increased development of benchmarks capable of assessing constitutively subjective and perspectival aspects of narrative, that is, aspects for which there is generally no single correct answer. Our taxonomy, survey, and methodology are of value to NLP researchers seeking to test LLM narrative understanding.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09869v3",
    "published_date": "2025-10-10 21:04:31 UTC",
    "updated_date": "2025-11-29 23:51:33 UTC"
  },
  {
    "arxiv_id": "2510.09859v3",
    "title": "Token Is All You Price",
    "authors": [
      "Weijie Zhong"
    ],
    "abstract": "We build a mechanism design framework where a platform designs GenAI models to screen users who obtain instrumental value from the generated conversation and privately differ in their preference for latency. We show that the revenue-optimal mechanism is simple: deploy a single aligned (user-optimal) model and use token cap as the only instrument to screen the user. The design decouples model training from pricing, is readily implemented with token metering, and mitigates misalignment pressures.",
    "categories": [
      "econ.TH",
      "cs.AI"
    ],
    "primary_category": "econ.TH",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09859v3",
    "published_date": "2025-10-10 20:49:31 UTC",
    "updated_date": "2025-12-11 04:25:41 UTC"
  },
  {
    "arxiv_id": "2510.09858v2",
    "title": "AI and Consciousness",
    "authors": [
      "Eric Schwitzgebel"
    ],
    "abstract": "This is a skeptical overview of the literature on AI consciousness. We will soon create AI systems that are conscious according to some influential, mainstream theories of consciousness but are not conscious according to other influential, mainstream theories of consciousness. We will not be in a position to know which theories are correct and whether we are surrounded by AI systems as richly and meaningfully conscious as human beings or instead only by systems as experientially blank as toasters. None of the standard arguments either for or against AI consciousness takes us far.\n  Table of Contents\n  Chapter One: Hills and Fog\n  Chapter Two: What Is Consciousness? What Is AI?\n  Chapter Three: Ten Possibly Essential Features of Consciousness\n  Chapter Four: Against Introspective and Conceptual Arguments for Essential Features\n  Chapter Five: Materialism and Functionalism\n  Chapter Six: The Turing Test and the Chinese Room\n  Chapter Seven: The Mimicry Argument Against AI Consciousness\n  Chapter Eight: Global Workspace Theories and Higher Order Theories\n  Chapter Nine: Integrated Information, Local Recurrence, Associative Learning, and Iterative Natural Kinds\n  Chapter Ten: Does Biological Substrate Matter?\n  Chapter Eleven: The Leapfrog Hypothesis and the Social Semi-Solution",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09858v2",
    "published_date": "2025-10-10 20:49:15 UTC",
    "updated_date": "2026-01-03 01:30:26 UTC"
  },
  {
    "arxiv_id": "2510.09852v1",
    "title": "ProxRouter: Proximity-Weighted LLM Query Routing for Improved Robustness to Outliers",
    "authors": [
      "Shivam Patel",
      "Neharika Jali",
      "Ankur Mallick",
      "Gauri Joshi"
    ],
    "abstract": "Large language model (LLM) query routers are critical to modern AI platforms as they seek to improve efficiency by assigning inference queries to accurate, yet low-cost models. Parametric routers typically use trained neural networks for LLM selection but suffer from retraining and maintenance overheads. Nonparametric routers are training-free, instead estimating LLM accuracy and cost via similarity between encodings of the input query and training set queries. However, like their parametric counterparts, nonparametric routers struggle to generalize to outlier queries, an issue exacerbated by limited diversity in training sets which are costly to expand and difficult to keep current with ever-evolving use cases. We propose ProxRouter, which applies an exponentially tilted aggregation mechanism to balance bias and variance in nonparametric routers, improving their robustness to outliers. Experiments show ProxRouter enhances outlier routing while preserving inlier performance with minimal overhead.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09852v1",
    "published_date": "2025-10-10 20:28:14 UTC",
    "updated_date": "2025-10-10 20:28:14 UTC"
  },
  {
    "arxiv_id": "2510.09846v1",
    "title": "CALM: A Causal Analysis Language Model for Tabular Data in Complex Systems with Local Scores, Conditional Independence Tests, and Relation Attributes",
    "authors": [
      "Zhenjiang Fan",
      "Zengyi Qin",
      "Yuanning Zheng",
      "Bo Xiong",
      "Summer Han"
    ],
    "abstract": "Causal discovery from observational data is fundamental to scientific fields like biology, where controlled experiments are often impractical. However, existing methods, including constraint-based (e.g., PC, causalMGM) and score-based approaches (e.g., NOTEARS), face significant limitations. These include an inability to resolve causal direction, restrictions to linear associations, sensitivity to violations of the faithfulness assumption, and inefficiency in searching vast hypothesis spaces. While large language models (LLMs) offer powerful reasoning capabilities, their application is hindered by a fundamental discrepancy: they are designed for text, while most causal data is tabular. To address these challenges, we introduce CALM, a novel causal analysis language model specifically designed for tabular data in complex systems. CALM leverages a Mamba-based architecture to classify causal patterns from pairwise variable relationships. It integrates a comprehensive suite of evidence, including local causal scores, conditional independence tests, and relational attributes, to capture a wide spectrum of linear, nonlinear, and conditional causal mechanisms. Trained on a diverse corpus of synthetic data (from linear, mixed, and nonlinear models) and 10 real-world biological datasets with rigorously validated causal relationships, our model ensures robustness and generalizability. Empirical evaluation demonstrates that CALM significantly outperforms existing methods in both simulation studies, achieving over 91% accuracy, and in a real-world application identifying causal factors in Hepatitis C virus progression. This work represents a significant step towards accurate and generalizable causal discovery by successfully adapting the pattern recognition capabilities of language models to the intricacies of tabular data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09846v1",
    "published_date": "2025-10-10 20:19:20 UTC",
    "updated_date": "2025-10-10 20:19:20 UTC"
  },
  {
    "arxiv_id": "2510.09845v1",
    "title": "Harnessing Self-Supervised Deep Learning and Geostationary Remote Sensing for Advancing Wildfire and Associated Air Quality Monitoring: Improved Smoke and Fire Front Masking using GOES and TEMPO Radiance Data",
    "authors": [
      "Nicholas LaHaye",
      "Thilanka Munashinge",
      "Hugo Lee",
      "Xiaohua Pan",
      "Gonzalo Gonzalez Abad",
      "Hazem Mahmoud",
      "Jennifer Wei"
    ],
    "abstract": "This work demonstrates the possibilities for improving wildfire and air quality management in the western United States by leveraging the unprecedented hourly data from NASA's TEMPO satellite mission and advances in self-supervised deep learning. Here we demonstrate the efficacy of deep learning for mapping the near real-time hourly spread of wildfire fronts and smoke plumes using an innovative self-supervised deep learning-system: successfully distinguishing smoke plumes from clouds using GOES-18 and TEMPO data, strong agreement across the smoke and fire masks generated from different sensing modalities as well as significant improvement over operational products for the same cases.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "https://2025.ieeeigarss.org/view_paper.php?PaperNum=6389&SessionID=1611",
    "pdf_url": "https://arxiv.org/pdf/2510.09845v1",
    "published_date": "2025-10-10 20:19:04 UTC",
    "updated_date": "2025-10-10 20:19:04 UTC"
  },
  {
    "arxiv_id": "2510.17845v1",
    "title": "MAT-Agent: Adaptive Multi-Agent Training Optimization",
    "authors": [
      "Jusheng Zhang",
      "Kaitong Cai",
      "Yijia Fan",
      "Ningyuan Liu",
      "Keze Wang"
    ],
    "abstract": "Multi-label image classification demands adaptive training strategies to navigate complex, evolving visual-semantic landscapes, yet conventional methods rely on static configurations that falter in dynamic settings. We propose MAT-Agent, a novel multi-agent framework that reimagines training as a collaborative, real-time optimization process. By deploying autonomous agents to dynamically tune data augmentation, optimizers, learning rates, and loss functions, MAT-Agent leverages non-stationary multi-armed bandit algorithms to balance exploration and exploitation, guided by a composite reward harmonizing accuracy, rare-class performance, and training stability. Enhanced with dual-rate exponential moving average smoothing and mixed-precision training, it ensures robustness and efficiency. Extensive experiments across Pascal VOC, COCO, and VG-256 demonstrate MAT-Agent's superiority: it achieves an mAP of 97.4 (vs. 96.2 for PAT-T), OF1 of 92.3, and CF1 of 91.4 on Pascal VOC; an mAP of 92.8 (vs. 92.0 for HSQ-CvN), OF1 of 88.2, and CF1 of 87.1 on COCO; and an mAP of 60.9, OF1 of 70.8, and CF1 of 61.1 on VG-256. With accelerated convergence and robust cross-domain generalization, MAT-Agent offers a scalable, intelligent solution for optimizing complex visual models, paving the way for adaptive deep learning advancements.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Acceptance to NeurIPS 2025 Main Track",
    "pdf_url": "https://arxiv.org/pdf/2510.17845v1",
    "published_date": "2025-10-10 19:41:50 UTC",
    "updated_date": "2025-10-10 19:41:50 UTC"
  },
  {
    "arxiv_id": "2510.11739v1",
    "title": "Celebrity Profiling on Short Urdu Text using Twitter Followers' Feed",
    "authors": [
      "Muhammad Hamza",
      "Rizwan Jafar"
    ],
    "abstract": "Social media has become an essential part of the digital age, serving as a platform for communication, interaction, and information sharing. Celebrities are among the most active users and often reveal aspects of their personal and professional lives through online posts. Platforms such as Twitter provide an opportunity to analyze language and behavior for understanding demographic and social patterns. Since followers frequently share linguistic traits and interests with the celebrities they follow, textual data from followers can be used to predict celebrity demographics. However, most existing research in this field has focused on English and other high-resource languages, leaving Urdu largely unexplored.\n  This study applies modern machine learning and deep learning techniques to the problem of celebrity profiling in Urdu. A dataset of short Urdu tweets from followers of subcontinent celebrities was collected and preprocessed. Multiple algorithms were trained and compared, including Logistic Regression, Support Vector Machines, Random Forests, Convolutional Neural Networks, and Long Short-Term Memory networks. The models were evaluated using accuracy, precision, recall, F1-score, and cumulative rank (cRank). The best performance was achieved for gender prediction with a cRank of 0.65 and an accuracy of 0.65, followed by moderate results for age, profession, and fame prediction. These results demonstrate that follower-based linguistic features can be effectively leveraged using machine learning and neural approaches for demographic prediction in Urdu, a low-resource language.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11739v1",
    "published_date": "2025-10-10 19:39:31 UTC",
    "updated_date": "2025-10-10 19:39:31 UTC"
  },
  {
    "arxiv_id": "2510.09815v1",
    "title": "Towards Understanding Ambiguity Resolution in Multimodal Inference of Meaning",
    "authors": [
      "Yufei Wang",
      "Adriana Kovashka",
      "Loretta Fernández",
      "Marc N. Coutanche",
      "Seth Wiener"
    ],
    "abstract": "We investigate a new setting for foreign language learning, where learners infer the meaning of unfamiliar words in a multimodal context of a sentence describing a paired image. We conduct studies with human participants using different image-text pairs. We analyze the features of the data (i.e., images and texts) that make it easier for participants to infer the meaning of a masked or unfamiliar word, and what language backgrounds of the participants correlate with success. We find only some intuitive features have strong correlations with participant performance, prompting the need for further investigating of predictive features for success in these tasks. We also analyze the ability of AI systems to reason about participant performance, and discover promising future directions for improving this reasoning ability.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to International Conference on Development and Learning (ICDL) 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.09815v1",
    "published_date": "2025-10-10 19:29:44 UTC",
    "updated_date": "2025-10-10 19:29:44 UTC"
  },
  {
    "arxiv_id": "2510.09805v1",
    "title": "Temporal Lifting as Latent-Space Regularization for Continuous-Time Flow Models in AI Systems",
    "authors": [
      "Jeffrey Camlin"
    ],
    "abstract": "We present a latent-space formulation of adaptive temporal reparametrization for continuous-time dynamical systems. The method, called *temporal lifting*, introduces a smooth monotone mapping $t \\mapsto τ(t)$ that regularizes near-singular behavior of the underlying flow while preserving its conservation laws. In the lifted coordinate, trajectories such as those of the incompressible Navier-Stokes equations on the torus $\\mathbb{T}^3$ become globally smooth. From the standpoint of machine-learning dynamics, temporal lifting acts as a continuous-time normalization or time-warping operator that can stabilize physics-informed neural networks and other latent-flow architectures used in AI systems. The framework links analytic regularity theory with representation-learning methods for stiff or turbulent processes.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages, 1 figure, 1 table, 1 algorithm",
    "pdf_url": "https://arxiv.org/pdf/2510.09805v1",
    "published_date": "2025-10-10 19:06:32 UTC",
    "updated_date": "2025-10-10 19:06:32 UTC"
  },
  {
    "arxiv_id": "2510.09801v2",
    "title": "How can we assess human-agent interactions? Case studies in software agent design",
    "authors": [
      "Valerie Chen",
      "Rohit Malhotra",
      "Xingyao Wang",
      "Juan Michelini",
      "Xuhui Zhou",
      "Aditya Bharat Soni",
      "Hoang H. Tran",
      "Calvin Smith",
      "Ameet Talwalkar",
      "Graham Neubig"
    ],
    "abstract": "LLM-powered agents are both a promising new technology and a source of complexity, where choices about models, tools, and prompting can affect their usefulness. While numerous benchmarks measure agent accuracy across domains, they mostly assume full automation, failing to represent the collaborative nature of real-world use cases. In this paper, we make two major steps towards the rigorous assessment of human-agent interactions. First, we propose PULSE, a framework for more efficient human-centric evaluation of agent designs, which comprises collecting user feedback, training an ML model to predict user satisfaction, and computing results by combining human satisfaction ratings with model-generated pseudo-labels. Second, we deploy the framework on a large-scale web platform built around the open-source software agent OpenHands, collecting in-the-wild usage data across over 15k users. We conduct case studies around how three agent design decisions -- choice of LLM backbone, planning strategy, and memory mechanisms -- impact developer satisfaction rates, yielding practical insights for software agent design. We also show how our framework can lead to more robust conclusions about agent design, reducing confidence intervals by 40% compared to a standard A/B test. Finally, we find substantial discrepancies between in-the-wild results and benchmark performance (e.g., the anti-correlation between results comparing claude-sonnet-4 and gpt-5), underscoring the limitations of benchmark-driven evaluation. Our findings provide guidance for evaluations of LLM agents with humans and identify opportunities for better agent designs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09801v2",
    "published_date": "2025-10-10 19:04:28 UTC",
    "updated_date": "2025-11-04 14:54:41 UTC"
  },
  {
    "arxiv_id": "2510.09783v1",
    "title": "Large Language Models for Imbalanced Classification: Diversity makes the difference",
    "authors": [
      "Dang Nguyen",
      "Sunil Gupta",
      "Kien Do",
      "Thin Nguyen",
      "Taylor Braund",
      "Alexis Whitton",
      "Svetha Venkatesh"
    ],
    "abstract": "Oversampling is one of the most widely used approaches for addressing imbalanced classification. The core idea is to generate additional minority samples to rebalance the dataset. Most existing methods, such as SMOTE, require converting categorical variables into numerical vectors, which often leads to information loss. Recently, large language model (LLM)-based methods have been introduced to overcome this limitation. However, current LLM-based approaches typically generate minority samples with limited diversity, reducing robustness and generalizability in downstream classification tasks. To address this gap, we propose a novel LLM-based oversampling method designed to enhance diversity. First, we introduce a sampling strategy that conditions synthetic sample generation on both minority labels and features. Second, we develop a new permutation strategy for fine-tuning pre-trained LLMs. Third, we fine-tune the LLM not only on minority samples but also on interpolated samples to further enrich variability. Extensive experiments on 10 tabular datasets demonstrate that our method significantly outperforms eight SOTA baselines. The generated synthetic samples are both realistic and diverse. Moreover, we provide theoretical analysis through an entropy-based perspective, proving that our method encourages diversity in the generated samples.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09783v1",
    "published_date": "2025-10-10 18:45:29 UTC",
    "updated_date": "2025-10-10 18:45:29 UTC"
  },
  {
    "arxiv_id": "2510.09782v1",
    "title": "The Geometry of Reasoning: Flowing Logics in Representation Space",
    "authors": [
      "Yufa Zhou",
      "Yixiao Wang",
      "Xunjian Yin",
      "Shuyan Zhou",
      "Anru R. Zhang"
    ],
    "abstract": "We study how large language models (LLMs) ``think'' through their representation space. We propose a novel geometric framework that models an LLM's reasoning as flows -- embedding trajectories evolving where logic goes. We disentangle logical structure from semantics by employing the same natural deduction propositions with varied semantic carriers, allowing us to test whether LLMs internalize logic beyond surface form. This perspective connects reasoning with geometric quantities such as position, velocity, and curvature, enabling formal analysis in representation and concept spaces. Our theory establishes: (1) LLM reasoning corresponds to smooth flows in representation space, and (2) logical statements act as local controllers of these flows' velocities. Using learned representation proxies, we design controlled experiments to visualize and quantify reasoning flows, providing empirical validation of our theoretical framework. Our work serves as both a conceptual foundation and practical tools for studying reasoning phenomenon, offering a new lens for interpretability and formal analysis of LLMs' behavior.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "Code: https://github.com/MasterZhou1/Reasoning-Flow",
    "pdf_url": "https://arxiv.org/pdf/2510.09782v1",
    "published_date": "2025-10-10 18:44:00 UTC",
    "updated_date": "2025-10-10 18:44:00 UTC"
  },
  {
    "arxiv_id": "2510.11738v1",
    "title": "SeeingSounds: Learning Audio-to-Visual Alignment via Text",
    "authors": [
      "Simone Carnemolla",
      "Matteo Pennisi",
      "Chiara Russo",
      "Simone Palazzo",
      "Daniela Giordano",
      "Concetto Spampinato"
    ],
    "abstract": "We introduce SeeingSounds, a lightweight and modular framework for audio-to-image generation that leverages the interplay between audio, language, and vision-without requiring any paired audio-visual data or training on visual generative models. Rather than treating audio as a substitute for text or relying solely on audio-to-text mappings, our method performs dual alignment: audio is projected into a semantic language space via a frozen language encoder, and, contextually grounded into the visual domain using a vision-language model. This approach, inspired by cognitive neuroscience, reflects the natural cross-modal associations observed in human perception. The model operates on frozen diffusion backbones and trains only lightweight adapters, enabling efficient and scalable learning. Moreover, it supports fine-grained and interpretable control through procedural text prompt generation, where audio transformations (e.g., volume or pitch shifts) translate into descriptive prompts (e.g., \"a distant thunder\") that guide visual outputs. Extensive experiments across standard benchmarks confirm that SeeingSounds outperforms existing methods in both zero-shot and supervised settings, establishing a new state of the art in controllable audio-to-visual generation.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CV",
      "cs.MM"
    ],
    "primary_category": "cs.SD",
    "comment": "accepted to ACM Multimedia Asia 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.11738v1",
    "published_date": "2025-10-10 18:42:50 UTC",
    "updated_date": "2025-10-10 18:42:50 UTC"
  },
  {
    "arxiv_id": "2510.09781v1",
    "title": "Building a Foundational Guardrail for General Agentic Systems via Synthetic Data",
    "authors": [
      "Yue Huang",
      "Hang Hua",
      "Yujun Zhou",
      "Pengcheng Jing",
      "Manish Nagireddy",
      "Inkit Padhi",
      "Greta Dolcetti",
      "Zhangchen Xu",
      "Subhajit Chaudhury",
      "Ambrish Rawat",
      "Liubov Nedoshivina",
      "Pin-Yu Chen",
      "Prasanna Sattigeri",
      "Xiangliang Zhang"
    ],
    "abstract": "While LLM agents can plan multi-step tasks, intervening at the planning stage-before any action is executed-is often the safest way to prevent harm, since certain risks can lead to severe consequences once carried out. However, existing guardrails mostly operate post-execution, which is difficult to scale and leaves little room for controllable supervision at the plan level. To address this challenge, we highlight three critical gaps in current research: data gap, model gap, and evaluation gap. To close the data gap, we introduce AuraGen, a controllable engine that (i) synthesizes benign trajectories, (ii) injects category-labeled risks with calibrated difficulty, and (iii) filters outputs via an automated reward model, producing large and reliable corpora for pre-execution safety. To close the guardian model gap, we propose a foundational guardrail Safiron, combining a cross-planner adapter with a compact guardian model. The adapter unifies different input formats, while Safiron flags risky cases, assigns risk types, and generates rationales; trained in two stages with a broadly explored data recipe, Safiron achieves robust transfer across settings. To close the evaluation gap, we release Pre-Exec Bench, a realistic benchmark covering diverse tools and branching trajectories, which measures detection, fine-grained categorization, explanation, and cross-planner generalization in human-verified scenarios. Extensive experiments demonstrate consistent gains of the proposed guardrail over strong baselines on Pre-Exec Bench, and ablations further distill actionable practices, providing a practical template for safer agentic systems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09781v1",
    "published_date": "2025-10-10 18:42:32 UTC",
    "updated_date": "2025-10-10 18:42:32 UTC"
  },
  {
    "arxiv_id": "2510.09780v2",
    "title": "SVTime: Small Time Series Forecasting Models Informed by \"Physics\" of Large Vision Model Forecasters",
    "authors": [
      "ChengAo Shen",
      "Ziming Zhao",
      "Hanghang Tong",
      "Dongjin Song",
      "Dongsheng Luo",
      "Qingsong Wen",
      "Jingchao Ni"
    ],
    "abstract": "Time series AI is crucial for analyzing dynamic web content, driving a surge of pre-trained large models known for their strong knowledge encoding and transfer capabilities across diverse tasks. However, given their energy-intensive training, inference, and hardware demands, using large models as a one-fits-all solution raises serious concerns about carbon footprint and sustainability. For a specific task, a compact yet specialized, high-performing model may be more practical and affordable, especially for resource-constrained users such as small businesses. This motivates the question: Can we build cost-effective lightweight models with large-model-like performance on core tasks such as forecasting? This paper addresses this question by introducing SVTime, a novel Small model inspired by large Vision model (LVM) forecasters for long-term Time series forecasting (LTSF). Recently, LVMs have been shown as powerful tools for LTSF. We identify a set of key inductive biases of LVM forecasters -- analogous to the \"physics\" governing their behaviors in LTSF -- and design small models that encode these biases through meticulously crafted linear layers and constraint functions. Across 21 baselines spanning lightweight, complex, and pre-trained large models on 8 benchmark datasets, SVTime outperforms state-of-the-art (SOTA) lightweight models and rivals large models with 10^3 fewer parameters than LVMs, while enabling efficient training and inference in low-resource settings.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09780v2",
    "published_date": "2025-10-10 18:42:23 UTC",
    "updated_date": "2025-10-31 02:36:51 UTC"
  },
  {
    "arxiv_id": "2510.12825v1",
    "title": "Classifier-Augmented Generation for Structured Workflow Prediction",
    "authors": [
      "Thomas Gschwind",
      "Shramona Chakraborty",
      "Nitin Gupta",
      "Sameep Mehta"
    ],
    "abstract": "ETL (Extract, Transform, Load) tools such as IBM DataStage allow users to visually assemble complex data workflows, but configuring stages and their properties remains time consuming and requires deep tool knowledge. We propose a system that translates natural language descriptions into executable workflows, automatically predicting both the structure and detailed configuration of the flow. At its core lies a Classifier-Augmented Generation (CAG) approach that combines utterance decomposition with a classifier and stage-specific few-shot prompting to produce accurate stage predictions. These stages are then connected into non-linear workflows using edge prediction, and stage properties are inferred from sub-utterance context. We compare CAG against strong single-prompt and agentic baselines, showing improved accuracy and efficiency, while substantially reducing token usage. Our architecture is modular, interpretable, and capable of end-to-end workflow generation, including robust validation steps. To our knowledge, this is the first system with a detailed evaluation across stage prediction, edge layout, and property generation for natural-language-driven ETL authoring.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at EMNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.12825v1",
    "published_date": "2025-10-10 18:38:25 UTC",
    "updated_date": "2025-10-10 18:38:25 UTC"
  },
  {
    "arxiv_id": "2510.09776v1",
    "title": "Why Do Transformers Fail to Forecast Time Series In-Context?",
    "authors": [
      "Yufa Zhou",
      "Yixiao Wang",
      "Surbhi Goel",
      "Anru R. Zhang"
    ],
    "abstract": "Time series forecasting (TSF) remains a challenging and largely unsolved problem in machine learning, despite significant recent efforts leveraging Large Language Models (LLMs), which predominantly rely on Transformer architectures. Empirical evidence consistently shows that even powerful Transformers often fail to outperform much simpler models, e.g., linear models, on TSF tasks; however, a rigorous theoretical understanding of this phenomenon remains limited. In this paper, we provide a theoretical analysis of Transformers' limitations for TSF through the lens of In-Context Learning (ICL) theory. Specifically, under AR($p$) data, we establish that: (1) Linear Self-Attention (LSA) models $\\textit{cannot}$ achieve lower expected MSE than classical linear models for in-context forecasting; (2) as the context length approaches to infinity, LSA asymptotically recovers the optimal linear predictor; and (3) under Chain-of-Thought (CoT) style inference, predictions collapse to the mean exponentially. We empirically validate these findings through carefully designed experiments. Our theory not only sheds light on several previously underexplored phenomena but also offers practical insights for designing more effective forecasting architectures. We hope our work encourages the broader research community to revisit the fundamental theoretical limitations of TSF and to critically evaluate the direct application of increasingly sophisticated architectures without deeper scrutiny.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Code: https://github.com/MasterZhou1/ICL-Time-Series",
    "pdf_url": "https://arxiv.org/pdf/2510.09776v1",
    "published_date": "2025-10-10 18:34:19 UTC",
    "updated_date": "2025-10-10 18:34:19 UTC"
  },
  {
    "arxiv_id": "2510.09771v2",
    "title": "PromptGuard at BLP-2025 Task 1: A Few-Shot Classification Framework Using Majority Voting and Keyword Similarity for Bengali Hate Speech Detection",
    "authors": [
      "Rakib Hossan",
      "Shubhashis Roy Dipta"
    ],
    "abstract": "The BLP-2025 Task 1A requires Bengali hate speech classification into six categories. Traditional supervised approaches need extensive labeled datasets that are expensive for low-resource languages. We developed PromptGuard, a few-shot framework combining chi-square statistical analysis for keyword extraction with adaptive majority voting for decision-making. We explore statistical keyword selection versus random approaches and adaptive voting mechanisms that extend classification based on consensus quality. Chi-square keywords provide consistent improvements across categories, while adaptive voting benefits ambiguous cases requiring extended classification rounds. PromptGuard achieves a micro-F1 of 67.61, outperforming n-gram baselines (60.75) and random approaches (14.65). Ablation studies confirm chi-square-based keywords show the most consistent impact across all categories.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to BLP at AACL-IJCNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.09771v2",
    "published_date": "2025-10-10 18:30:27 UTC",
    "updated_date": "2025-11-18 00:47:55 UTC"
  },
  {
    "arxiv_id": "2510.09768v1",
    "title": "Scaling Laws and Symmetry, Evidence from Neural Force Fields",
    "authors": [
      "Khang Ngo",
      "Siamak Ravanbakhsh"
    ],
    "abstract": "We present an empirical study in the geometric task of learning interatomic potentials, which shows equivariance matters even more at larger scales; we show a clear power-law scaling behaviour with respect to data, parameters and compute with ``architecture-dependent exponents''. In particular, we observe that equivariant architectures, which leverage task symmetry, scale better than non-equivariant models. Moreover, among equivariant architectures, higher-order representations translate to better scaling exponents. Our analysis also suggests that for compute-optimal training, the data and model sizes should scale in tandem regardless of the architecture. At a high level, these results suggest that, contrary to common belief, we should not leave it to the model to discover fundamental inductive biases such as symmetry, especially as we scale, because they change the inherent difficulty of the task and its scaling laws.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.comp-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "22 pages, 10 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.09768v1",
    "published_date": "2025-10-10 18:22:00 UTC",
    "updated_date": "2025-10-10 18:22:00 UTC"
  },
  {
    "arxiv_id": "2510.09762v1",
    "title": "PatentVision: A multimodal method for drafting patent applications",
    "authors": [
      "Ruo Yang",
      "Sai Krishna Reddy Mudhiganti",
      "Manali Sharma"
    ],
    "abstract": "Patent drafting is complex due to its need for detailed technical descriptions, legal compliance, and visual elements. Although Large Vision Language Models (LVLMs) show promise across various tasks, their application in automating patent writing remains underexplored. In this paper, we present PatentVision, a multimodal framework that integrates textual and visual inputs such as patent claims and drawings to generate complete patent specifications. Built on advanced LVLMs, PatentVision enhances accuracy by combining fine tuned vision language models with domain specific training tailored to patents. Experiments reveal it surpasses text only methods, producing outputs with greater fidelity and alignment with human written standards. Its incorporation of visual data allows it to better represent intricate design features and functional connections, leading to richer and more precise results. This study underscores the value of multimodal techniques in patent automation, providing a scalable tool to reduce manual workloads and improve consistency. PatentVision not only advances patent drafting but also lays the groundwork for broader use of LVLMs in specialized areas, potentially transforming intellectual property management and innovation processes.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09762v1",
    "published_date": "2025-10-10 18:12:05 UTC",
    "updated_date": "2025-10-10 18:12:05 UTC"
  },
  {
    "arxiv_id": "2510.15952v4",
    "title": "Executable Epistemology: The Structured Cognitive Loop as an Architecture of Intentional Understanding",
    "authors": [
      "Myung Ho Kim"
    ],
    "abstract": "Large language models exhibit intelligence without genuine epistemic understanding, exposing a key gap: the absence of epistemic architecture. This paper introduces the Structured Cognitive Loop (SCL) as an executable epistemological framework for emergent intelligence. Unlike traditional AI research asking \"what is intelligence?\" (ontological), SCL asks \"under what conditions does cognition emerge?\" (epistemological). Grounded in philosophy of mind and cognitive phenomenology, SCL bridges conceptual philosophy and implementable cognition. Drawing on process philosophy, enactive cognition, and extended mind theory, we define intelligence not as a property but as a performed process -- a continuous loop of judgment, memory, control, action, and regulation. SCL makes three contributions. First, it operationalizes philosophical insights into computationally interpretable structures, enabling \"executable epistemology\" -- philosophy as structural experiment. Second, it shows that functional separation within cognitive architecture yields more coherent and interpretable behavior than monolithic prompt based systems, supported by agent evaluations. Third, it redefines intelligence: not representational accuracy but the capacity to reconstruct its own epistemic state through intentional understanding. This framework impacts philosophy of mind, epistemology, and AI. For philosophy, it allows theories of cognition to be enacted and tested. For AI, it grounds behavior in epistemic structure rather than statistical regularity. For epistemology, it frames knowledge not as truth possession but as continuous reconstruction within a phenomenologically coherent loop. We situate SCL within debates on cognitive phenomenology, emergence, normativity, and intentionality, arguing that real progress requires not larger models but architectures that realize cognitive principles structurally.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "v4: Clarified the structural role of 'Regulation' as a governing constraint distinct from the loop cycle. Revised Figures 1 and 2 for better architectural clarity and unified terminology",
    "pdf_url": "https://arxiv.org/pdf/2510.15952v4",
    "published_date": "2025-10-10 18:03:32 UTC",
    "updated_date": "2025-12-11 08:40:26 UTC"
  },
  {
    "arxiv_id": "2510.09752v1",
    "title": "Patentformer: A demonstration of AI-assisted automated patent drafting",
    "authors": [
      "Sai Krishna Reddy Mudhiganti",
      "Juanyan Wang",
      "Ruo Yang",
      "Manali Sharma"
    ],
    "abstract": "Patent drafting presents significant challenges due to its reliance on the extensive experience and specialized expertise of patent attorneys, who must possess both legal acumen and technical understanding of an invention to craft patent applications in a formal legal writing style. This paper presents a demonstration of Patentformer, an AI-powered automated patent drafting platform designed to support patent attorneys by rapidly producing high-quality patent applications adhering to legal writing standards.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09752v1",
    "published_date": "2025-10-10 18:00:03 UTC",
    "updated_date": "2025-10-10 18:00:03 UTC"
  },
  {
    "arxiv_id": "2510.09608v1",
    "title": "StreamingVLM: Real-Time Understanding for Infinite Video Streams",
    "authors": [
      "Ruyi Xu",
      "Guangxuan Xiao",
      "Yukang Chen",
      "Liuning He",
      "Kelly Peng",
      "Yao Lu",
      "Song Han"
    ],
    "abstract": "Vision-language models (VLMs) could power real-time assistants and autonomous agents, but they face a critical challenge: understanding near-infinite video streams without escalating latency and memory usage. Processing entire videos with full attention leads to quadratic computational costs and poor performance on long videos. Meanwhile, simple sliding window methods are also flawed, as they either break coherence or suffer from high latency due to redundant recomputation. In this paper, we introduce StreamingVLM, a model designed for real-time, stable understanding of infinite visual input. Our approach is a unified framework that aligns training with streaming inference. During inference, we maintain a compact KV cache by reusing states of attention sinks, a short window of recent vision tokens, and a long window of recent text tokens. This streaming ability is instilled via a simple supervised fine-tuning (SFT) strategy that applies full attention on short, overlapped video chunks, which effectively mimics the inference-time attention pattern without training on prohibitively long contexts. For evaluation, we build Inf-Streams-Eval, a new benchmark with videos averaging over two hours that requires dense, per-second alignment between frames and text. On Inf-Streams-Eval, StreamingVLM achieves a 66.18% win rate against GPT-4O mini and maintains stable, real-time performance at up to 8 FPS on a single NVIDIA H100. Notably, our SFT strategy also enhances general VQA abilities without any VQA-specific fine-tuning, improving performance on LongVideoBench by +4.30 and OVOBench Realtime by +5.96. Code is available at https://github.com/mit-han-lab/streaming-vlm.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "The first two authors contributed equally to this work",
    "pdf_url": "https://arxiv.org/pdf/2510.09608v1",
    "published_date": "2025-10-10 17:59:58 UTC",
    "updated_date": "2025-10-10 17:59:58 UTC"
  },
  {
    "arxiv_id": "2510.09599v1",
    "title": "Prompting Test-Time Scaling Is A Strong LLM Reasoning Data Augmentation",
    "authors": [
      "Sondos Mahmoud Bsharat",
      "Zhiqiang Shen"
    ],
    "abstract": "Large language models (LLMs) have demonstrated impressive reasoning capabilities when provided with chain-of-thought exemplars, but curating large reasoning datasets remains laborious and resource-intensive. In this work, we introduce Prompting Test-Time Scaling (P-TTS), a simple yet effective inference-time data augmentation strategy for enhancing LLM reasoning through finetuning. Rather than collecting thousands or even millions of examples, P-TTS leverages a small pool of only 90 manually selected reasoning instances and systematically varies exemplar augmentation through principled instruction prompting intensities at test time to synthesize diverse reasoning trajectory contexts. Then we finetune the various sizes of Qwen-2.5 models on P-TTS data. Across a suite of mathematical reasoning AIME2024 & 25, MATH500, and GPQA-Diamond, our P-TTS-7B and 32B models outperform the prior competitive baselines like S1 and S1.1 (1K-shot), achieving absolute accuracy gains of +26.66% and +30.00% on AIME'24 (7B), and +13.34% and +6.67% on AIME'25 (7B); P-TTS-32B yields gains of +23.33% and +16.63% on AIME'24, and +26.63% and +3.33% on AIME'25 (vs. S1 and S1.1, respectively), with comparable or better performance on MATH500 and GPQA-Diamond. We further show that P-TTS enhances zero-shot generalization accuracy on out-of-domain reasoning benchmarks of Gaokao, Kaoyan, OlympiadBench, AMC23, GradeSchoolMath, and Minerva. Our analysis suggests that test-time scaling effectively explores the latent space of reasoning patterns, amplifying LLM problem-solving with minimal annotation overhead, and further unlocking the reasoning potential and capabilities of LLMs. Prompting Test-Time Scaling offers a practical, low-cost way to elicit LLM reasoning in resource-constrained or rapidly evolving domains.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Our code and data are available at https://github.com/VILA-Lab/PTTS",
    "pdf_url": "https://arxiv.org/pdf/2510.09599v1",
    "published_date": "2025-10-10 17:57:04 UTC",
    "updated_date": "2025-10-10 17:57:04 UTC"
  },
  {
    "arxiv_id": "2510.09596v1",
    "title": "BaNEL: Exploration Posteriors for Generative Modeling Using Only Negative Rewards",
    "authors": [
      "Sangyun Lee",
      "Brandon Amos",
      "Giulia Fanti"
    ],
    "abstract": "Today's generative models thrive with large amounts of supervised data and informative reward functions characterizing the quality of the generation. They work under the assumptions that the supervised data provides knowledge to pre-train the model, and the reward function provides dense information about how to further improve the generation quality and correctness. However, in the hardest instances of important problems, two problems arise: (1) the base generative model attains a near-zero reward signal, and (2) calls to the reward oracle are expensive. This setting poses a fundamentally different learning challenge than standard reward-based post-training. To address this, we propose BaNEL (Bayesian Negative Evidence Learning), an algorithm that post-trains the model using failed attempts only, while minimizing the number of reward evaluations (NREs). Our method is based on the idea that the problem of learning regularities underlying failures can be cast as another, in-loop generative modeling problem. We then leverage this model to assess whether new data resembles previously seen failures and steer the generation away from them. We show that BaNEL can improve model performance without observing a single successful sample on several sparse-reward tasks, outperforming existing novelty-bonus approaches by up to several orders of magnitude in success rate, while using fewer reward evaluations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09596v1",
    "published_date": "2025-10-10 17:55:03 UTC",
    "updated_date": "2025-10-10 17:55:03 UTC"
  },
  {
    "arxiv_id": "2510.09595v2",
    "title": "LiveOIBench: Can Large Language Models Outperform Human Contestants in Informatics Olympiads?",
    "authors": [
      "Kaijian Zou",
      "Aaron Xiong",
      "Yunxiang Zhang",
      "Frederick Zhang",
      "Yueqi Ren",
      "Jirong Yang",
      "Ayoung Lee",
      "Shitanshu Bhushan",
      "Lu Wang"
    ],
    "abstract": "Competitive programming problems increasingly serve as valuable benchmarks to evaluate the coding capabilities of large language models (LLMs) due to their complexity and ease of verification. Yet, current coding benchmarks face limitations such as lack of exceptionally challenging problems, insufficient test case coverage, reliance on online platform APIs that limit accessibility. To address these issues, we introduce LiveOIBench, a comprehensive benchmark featuring 403 expert-curated Olympiad-level competitive programming problems, each with an average of 60 expert-designed test cases. The problems are sourced directly from 72 official contests of 14 Informatics Olympiads in different regions conducted between 2023 and 2025. LiveOIBench distinguishes itself through four key features: (1) meticulously curated high-quality tasks with detailed subtask rubrics and extensive private test cases; (2) direct integration of elite contestant performance data to enable informative comparison against top-performing humans; (3) planned continuous, contamination-free updates from newly released Olympiad problems; and (4) a self-contained evaluation system facilitating offline and easy-to-reproduce assessments. Benchmarking 34 popular general-purpose and reasoning LLMs, we find that GPT-5 achieves a notable 81.76th percentile, a strong result that nonetheless falls short of top human contestants, who usually place above 90th. In contrast, among open-weight reasoning models, GPT-OSS-120B achieves only a 60th percentile, underscoring significant capability disparities from frontier closed models. Detailed analyses indicate that robust reasoning models prioritize precise problem analysis over excessive exploration, suggesting future models should emphasize structured analysis and minimize unnecessary exploration. All data, code, and leaderboard results are publicly available on our website.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09595v2",
    "published_date": "2025-10-10 17:54:24 UTC",
    "updated_date": "2025-12-22 18:56:01 UTC"
  },
  {
    "arxiv_id": "2510.15950v1",
    "title": "Cross-dataset Multivariate Time-series Model for Parkinson's Diagnosis via Keyboard Dynamics",
    "authors": [
      "Arianna Francesconi",
      "Donato Cappetta",
      "Fabio Rebecchi",
      "Paolo Soda",
      "Valerio Guarrasi",
      "Rosa Sicilia"
    ],
    "abstract": "Parkinson's disease (PD) presents a growing global challenge, affecting over 10 million individuals, with prevalence expected to double by 2040. Early diagnosis remains difficult due to the late emergence of motor symptoms and limitations of traditional clinical assessments. In this study, we propose a novel pipeline that leverages keystroke dynamics as a non-invasive and scalable biomarker for remote PD screening and telemonitoring. Our methodology involves three main stages: (i) preprocessing of data from four distinct datasets, extracting four temporal signals and addressing class imbalance through the comparison of three methods; (ii) pre-training eight state-of-the-art deep-learning architectures on the two largest datasets, optimizing temporal windowing, stride, and other hyperparameters; (iii) fine-tuning on an intermediate-sized dataset and performing external validation on a fourth, independent cohort. Our results demonstrate that hybrid convolutional-recurrent and transformer-based models achieve strong external validation performance, with AUC-ROC scores exceeding 90% and F1-Score over 70%. Notably, a temporal convolutional model attains an AUC-ROC of 91.14% in external validation, outperforming existing methods that rely solely on internal validation. These findings underscore the potential of keystroke dynamics as a reliable digital biomarker for PD, offering a promising avenue for early detection and continuous monitoring.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Proceedings of the Workshop on Artificial Intelligence for Biomedical Data (AIBio 2025), 28th European Conference on Artificial Intelligence 2025, Springer CCIS",
    "pdf_url": "https://arxiv.org/pdf/2510.15950v1",
    "published_date": "2025-10-10 17:42:08 UTC",
    "updated_date": "2025-10-10 17:42:08 UTC"
  },
  {
    "arxiv_id": "2510.09580v1",
    "title": "GraphMERT: Efficient and Scalable Distillation of Reliable Knowledge Graphs from Unstructured Data",
    "authors": [
      "Margarita Belova",
      "Jiaxin Xiao",
      "Shikhar Tuli",
      "Niraj K. Jha"
    ],
    "abstract": "Researchers have pursued neurosymbolic artificial intelligence (AI) applications for nearly three decades because symbolic components provide abstraction while neural components provide generalization. Thus, a marriage of the two components can lead to rapid advancements in AI. Yet, the field has not realized this promise since most neurosymbolic AI frameworks fail to scale. In addition, the implicit representations and approximate reasoning of neural approaches limit interpretability and trust. Knowledge graphs (KGs), a gold-standard representation of explicit semantic knowledge, can address the symbolic side. However, automatically deriving reliable KGs from text corpora has remained an open problem. We address these challenges by introducing GraphMERT, a tiny graphical encoder-only model that distills high-quality KGs from unstructured text corpora and its own internal representations. GraphMERT and its equivalent KG form a modular neurosymbolic stack: neural learning of abstractions; symbolic KGs for verifiable reasoning. GraphMERT + KG is the first efficient and scalable neurosymbolic model to achieve state-of-the-art benchmark accuracy along with superior symbolic representations relative to baselines.\n  Concretely, we target reliable domain-specific KGs that are both (1) factual (with provenance) and (2) valid (ontology-consistent relations with domain-appropriate semantics). When a large language model (LLM), e.g., Qwen3-32B, generates domain-specific KGs, it falls short on reliability due to prompt sensitivity, shallow domain expertise, and hallucinated relations. On text obtained from PubMed papers on diabetes, our 80M-parameter GraphMERT yields a KG with a 69.8% FActScore; a 32B-parameter baseline LLM yields a KG that achieves only 40.2% FActScore. The GraphMERT KG also attains a higher ValidityScore of 68.8%, versus 43.0% for the LLM baseline.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09580v1",
    "published_date": "2025-10-10 17:36:14 UTC",
    "updated_date": "2025-10-10 17:36:14 UTC"
  },
  {
    "arxiv_id": "2510.09577v1",
    "title": "Dyna-Mind: Learning to Simulate from Experience for Better AI Agents",
    "authors": [
      "Xiao Yu",
      "Baolin Peng",
      "Michel Galley",
      "Hao Cheng",
      "Qianhui Wu",
      "Janardhan Kulkarni",
      "Suman Nath",
      "Zhou Yu",
      "Jianfeng Gao"
    ],
    "abstract": "Reasoning models have recently shown remarkable progress in domains such as math and coding. However, their expert-level abilities in math and coding contrast sharply with their performance in long-horizon, interactive tasks such as web navigation and computer/phone-use. Inspired by literature on human cognition, we argue that current AI agents need ''vicarious trial and error'' - the capacity to mentally simulate alternative futures before acting - in order to enhance their understanding and performance in complex interactive environments. We introduce Dyna-Mind, a two-stage training framework that explicitly teaches (V)LM agents to integrate such simulation into their reasoning. In stage 1, we introduce Reasoning with Simulations (ReSim), which trains the agent to generate structured reasoning traces from expanded search trees built from real experience gathered through environment interactions. ReSim thus grounds the agent's reasoning in faithful world dynamics and equips it with the ability to anticipate future states in its reasoning. In stage 2, we propose Dyna-GRPO, an online reinforcement learning method to further strengthen the agent's simulation and decision-making ability by using both outcome rewards and intermediate states as feedback from real rollouts. Experiments on two synthetic benchmarks (Sokoban and ALFWorld) and one realistic benchmark (AndroidWorld) demonstrate that (1) ReSim effectively infuses simulation ability into AI agents, and (2) Dyna-GRPO leverages outcome and interaction-level signals to learn better policies for long-horizon, planning-intensive tasks. Together, these results highlight the central role of simulation in enabling AI agents to reason, plan, and act more effectively in the ever more challenging environments.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09577v1",
    "published_date": "2025-10-10 17:30:18 UTC",
    "updated_date": "2025-10-10 17:30:18 UTC"
  },
  {
    "arxiv_id": "2510.09739v1",
    "title": "Machine learning methods fail to provide cohesive atheoretical construction of personality traits from semantic embeddings",
    "authors": [
      "Ayoub Bouguettaya",
      "Elizabeth M. Stuart"
    ],
    "abstract": "The lexical hypothesis posits that personality traits are encoded in language and is foundational to models like the Big Five. We created a bottom-up personality model from a classic adjective list using machine learning and compared its descriptive utility against the Big Five by analyzing one million Reddit comments. The Big Five, particularly Agreeableness, Conscientiousness, and Neuroticism, provided a far more powerful and interpretable description of these online communities. In contrast, our machine-learning clusters provided no meaningful distinctions, failed to recover the Extraversion trait, and lacked the psychometric coherence of the Big Five. These results affirm the robustness of the Big Five and suggest personality's semantic structure is context-dependent. Our findings show that while machine learning can help check the ecological validity of established psychological theories, it may not be able to replace them.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "1 figure, 12 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.09739v1",
    "published_date": "2025-10-10 17:29:33 UTC",
    "updated_date": "2025-10-10 17:29:33 UTC"
  },
  {
    "arxiv_id": "2510.09738v1",
    "title": "Judge's Verdict: A Comprehensive Analysis of LLM Judge Capability Through Human Agreement",
    "authors": [
      "Steve Han",
      "Gilberto Titericz Junior",
      "Tom Balough",
      "Wenfei Zhou"
    ],
    "abstract": "This research introduces the Judge's Verdict Benchmark, a novel two-step methodology to evaluate Large Language Models (LLMs) as judges for response accuracy evaluation tasks. We assess how well 54 LLMs can replicate human judgment when scoring responses from RAG (Retrieval-Augmented Generation) or Agentic pipelines against ground truth answers. Our methodology progresses from traditional correlation analysis to comprehensive Cohen's Kappa analysis that measures actual agreement patterns. The two-step approach includes: (1) a correlation test that filters judges with strong alignment, followed by (2) a human-likeness test using z-scores to identify two distinct judgment patterns: human-like judgment (|z| < 1) that mimics natural human variation, and super-consistent judgment (z > 1) that exceeds typical human-to-human agreement levels. This methodology reveals that 27 out of 54 tested LLMs achieve Tier 1 performance: 23 models exhibit human-like patterns that preserve the nuances of human judgment, while 4 models demonstrate super-consistent behavior, a pattern that could indicate either enhanced reliability or oversimplification of complex judgments. Testing 43 open-source models (1B-405B parameters) and 11 closed models (GPT, Gemini, Claude variants), we demonstrate that judge excellence is not solely dependent on model size but on specific training strategies. Our key contributions include: (1) establishing that correlation alone is insufficient for judge evaluation, (2) introducing a \"Turing Test for judges\" based on agreement patterns, and (3) providing a standardized benchmark for classifying LLM judges into distinct performance tiers for different evaluation needs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 1 figure, 4 tables, under review as a conference paper at ICLR 2026",
    "pdf_url": "https://arxiv.org/pdf/2510.09738v1",
    "published_date": "2025-10-10 17:27:33 UTC",
    "updated_date": "2025-10-10 17:27:33 UTC"
  },
  {
    "arxiv_id": "2510.13836v1",
    "title": "SIMBA UQ: Similarity-Based Aggregation for Uncertainty Quantification in Large Language Models",
    "authors": [
      "Debarun Bhattacharjya",
      "Balaji Ganesan",
      "Junkyu Lee",
      "Radu Marinescu",
      "Katsiaryna Mirylenka",
      "Michael Glass",
      "Xiao Shou"
    ],
    "abstract": "When does a large language model (LLM) know what it does not know? Uncertainty quantification (UQ) provides measures of uncertainty, such as an estimate of the confidence in an LLM's generated output, and is therefore increasingly recognized as a crucial component of trusted AI systems. Black-box UQ methods do not require access to internal model information from the generating LLM and therefore have numerous real-world advantages, such as robustness to system changes, adaptability to choice of LLM, reduced costs, and computational tractability. In this paper, we investigate the effectiveness of UQ techniques that are primarily but not necessarily entirely black-box, where the consistency between a generated output and other sampled generations is used as a proxy for confidence in its correctness. We propose a high-level non-verbalized similarity-based aggregation framework that subsumes a broad swath of UQ approaches suitable for complex generative tasks, as well as introduce specific novel techniques from the framework that train confidence estimation models using small training sets. Through an empirical study with datasets spanning the diverse tasks of question answering, summarization, and text-to-SQL, we demonstrate that our proposed similarity-based methods can yield better calibrated confidences than baselines.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "15 pages including appendix, Findings of EMNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.13836v1",
    "published_date": "2025-10-10 17:22:53 UTC",
    "updated_date": "2025-10-10 17:22:53 UTC"
  },
  {
    "arxiv_id": "2510.09567v1",
    "title": "Safe, Untrusted, \"Proof-Carrying\" AI Agents: toward the agentic lakehouse",
    "authors": [
      "Jacopo Tagliabue",
      "Ciro Greco"
    ],
    "abstract": "Data lakehouses run sensitive workloads, where AI-driven automation raises concerns about trust, correctness, and governance. We argue that API-first, programmable lakehouses provide the right abstractions for safe-by-design, agentic workflows. Using Bauplan as a case study, we show how data branching and declarative environments extend naturally to agents, enabling reproducibility and observability while reducing the attack surface. We present a proof-of-concept in which agents repair data pipelines using correctness checks inspired by proof-carrying code. Our prototype demonstrates that untrusted AI agents can operate safely on production data and outlines a path toward a fully agentic lakehouse.",
    "categories": [
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.AI",
    "comment": "IEEE Big Data, Workshop on Secure and Safe AI Agents for Big Data Infrastructures",
    "pdf_url": "https://arxiv.org/pdf/2510.09567v1",
    "published_date": "2025-10-10 17:18:36 UTC",
    "updated_date": "2025-10-10 17:18:36 UTC"
  },
  {
    "arxiv_id": "2510.09551v1",
    "title": "Titans Revisited: A Lightweight Reimplementation and Critical Analysis of a Test-Time Memory Model",
    "authors": [
      "Gavriel Di Nepi",
      "Federico Siciliano",
      "Fabrizio Silvestri"
    ],
    "abstract": "By the end of 2024, Google researchers introduced Titans: Learning at Test Time, a neural memory model achieving strong empirical results across multiple tasks. However, the lack of publicly available code and ambiguities in the original description hinder reproducibility. In this work, we present a lightweight reimplementation of Titans and conduct a comprehensive evaluation on Masked Language Modeling, Time Series Forecasting, and Recommendation tasks. Our results reveal that Titans does not always outperform established baselines due to chunking. However, its Neural Memory component consistently improves performance compared to attention-only models. These findings confirm the model's innovative potential while highlighting its practical limitations and raising questions for future research.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09551v1",
    "published_date": "2025-10-10 17:03:24 UTC",
    "updated_date": "2025-10-10 17:03:24 UTC"
  },
  {
    "arxiv_id": "2510.09541v2",
    "title": "SPG: Sandwiched Policy Gradient for Masked Diffusion Language Models",
    "authors": [
      "Chenyu Wang",
      "Paria Rashidinejad",
      "DiJia Su",
      "Song Jiang",
      "Sid Wang",
      "Siyan Zhao",
      "Cai Zhou",
      "Shannon Zejiang Shen",
      "Feiyu Chen",
      "Tommi Jaakkola",
      "Yuandong Tian",
      "Bo Liu"
    ],
    "abstract": "Diffusion large language models (dLLMs) are emerging as an efficient alternative to autoregressive models due to their ability to decode multiple tokens in parallel. However, aligning dLLMs with human preferences or task-specific rewards via reinforcement learning (RL) is challenging because their intractable log-likelihood precludes the direct application of standard policy gradient methods. While prior work uses surrogates like the evidence lower bound (ELBO), these one-sided approximations can introduce significant policy gradient bias. To address this, we propose the Sandwiched Policy Gradient (SPG) that leverages both an upper and a lower bound of the true log-likelihood. Experiments show that SPG significantly outperforms baselines based on ELBO or one-step estimation. Specifically, SPG improves the accuracy over state-of-the-art RL methods for dLLMs by 3.6% in GSM8K, 2.6% in MATH500, 18.4% in Countdown and 27.0% in Sudoku.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09541v2",
    "published_date": "2025-10-10 16:52:25 UTC",
    "updated_date": "2025-10-13 01:58:46 UTC"
  },
  {
    "arxiv_id": "2510.09535v1",
    "title": "Mitigating Overthinking through Reasoning Shaping",
    "authors": [
      "Feifan Song",
      "Shaohang Wei",
      "Bofei Gao",
      "Yejie Wang",
      "Wen Luo",
      "Wei Li",
      "Linli Yao",
      "Weimin Xiong",
      "Liang Chen",
      "Tianyu Liu",
      "Houfeng Wang"
    ],
    "abstract": "Large reasoning models (LRMs) boosted by Reinforcement Learning from Verifier Reward (RLVR) have shown great power in problem solving, yet they often cause overthinking: excessive, meandering reasoning that inflates computational cost. Prior designs of penalization in RLVR manage to reduce token consumption while often harming model performance, which arises from the oversimplicity of token-level supervision. In this paper, we argue that the granularity of supervision plays a crucial role in balancing efficiency and accuracy, and propose Group Relative Segment Penalization (GRSP), a step-level method to regularize reasoning. Since preliminary analyses show that reasoning segments are strongly correlated with token consumption and model performance, we design a length-aware weighting mechanism across segment clusters. Extensive experiments demonstrate that GRSP achieves superior token efficiency without heavily compromising accuracy, especially the advantages with harder problems. Moreover, GRSP stabilizes RL training and scales effectively across model sizes.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09535v1",
    "published_date": "2025-10-10 16:49:03 UTC",
    "updated_date": "2025-10-10 16:49:03 UTC"
  },
  {
    "arxiv_id": "2510.09499v1",
    "title": "A methodology for clinically driven interactive segmentation evaluation",
    "authors": [
      "Parhom Esmaeili",
      "Virginia Fernandez",
      "Pedro Borges",
      "Eli Gibson",
      "Sebastien Ourselin",
      "M. Jorge Cardoso"
    ],
    "abstract": "Interactive segmentation is a promising strategy for building robust, generalisable algorithms for volumetric medical image segmentation. However, inconsistent and clinically unrealistic evaluation hinders fair comparison and misrepresents real-world performance. We propose a clinically grounded methodology for defining evaluation tasks and metrics, and built a software framework for constructing standardised evaluation pipelines. We evaluate state-of-the-art algorithms across heterogeneous and complex tasks and observe that (i) minimising information loss when processing user interactions is critical for model robustness, (ii) adaptive-zooming mechanisms boost robustness and speed convergence, (iii) performance drops if validation prompting behaviour/budgets differ from training, (iv) 2D methods perform well with slab-like images and coarse targets, but 3D context helps with large or irregularly shaped targets, (v) performance of non-medical-domain models (e.g. SAM2) degrades with poor contrast and complex shapes.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, Medical Image Computing and Computed Assisted Intervention 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.09499v1",
    "published_date": "2025-10-10 16:00:06 UTC",
    "updated_date": "2025-10-10 16:00:06 UTC"
  },
  {
    "arxiv_id": "2510.09497v1",
    "title": "Autonomous Soft Robotic Guidewire Navigation via Imitation Learning",
    "authors": [
      "Noah Barnes",
      "Ji Woong Kim",
      "Lingyun Di",
      "Hannah Qu",
      "Anuruddha Bhattacharjee",
      "Miroslaw Janowski",
      "Dheeraj Gandhi",
      "Bailey Felix",
      "Shaopeng Jiang",
      "Olivia Young",
      "Mark Fuge",
      "Ryan D. Sochol",
      "Jeremy D. Brown",
      "Axel Krieger"
    ],
    "abstract": "In endovascular surgery, endovascular interventionists push a thin tube called a catheter, guided by a thin wire to a treatment site inside the patient's blood vessels to treat various conditions such as blood clots, aneurysms, and malformations. Guidewires with robotic tips can enhance maneuverability, but they present challenges in modeling and control. Automation of soft robotic guidewire navigation has the potential to overcome these challenges, increasing the precision and safety of endovascular navigation. In other surgical domains, end-to-end imitation learning has shown promising results. Thus, we develop a transformer-based imitation learning framework with goal conditioning, relative action outputs, and automatic contrast dye injections to enable generalizable soft robotic guidewire navigation in an aneurysm targeting task. We train the model on 36 different modular bifurcated geometries, generating 647 total demonstrations under simulated fluoroscopy, and evaluate it on three previously unseen vascular geometries. The model can autonomously drive the tip of the robot to the aneurysm location with a success rate of 83% on the unseen geometries, outperforming several baselines. In addition, we present ablation and baseline studies to evaluate the effectiveness of each design and data collection choice. Project website: https://softrobotnavigation.github.io/",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09497v1",
    "published_date": "2025-10-10 15:57:09 UTC",
    "updated_date": "2025-10-10 15:57:09 UTC"
  },
  {
    "arxiv_id": "2510.09495v1",
    "title": "Precoder Design in Multi-User FDD Systems with VQ-VAE and GNN",
    "authors": [
      "Srikar Allaparapu",
      "Michael Baur",
      "Benedikt Böck",
      "Michael Joham",
      "Wolfgang Utschick"
    ],
    "abstract": "Robust precoding is efficiently feasible in frequency division duplex (FDD) systems by incorporating the learnt statistics of the propagation environment through a generative model. We build on previous work that successfully designed site-specific precoders based on a combination of Gaussian mixture models (GMMs) and graph neural networks (GNNs). In this paper, by utilizing a vector quantized-variational autoencoder (VQ-VAE), we circumvent one of the key drawbacks of GMMs, i.e., the number of GMM components scales exponentially to the feedback bits. In addition, the deep learning architecture of the VQ-VAE allows us to jointly train the GNN together with VQ-VAE along with pilot optimization forming an end-to-end (E2E) model, resulting in considerable performance gains in sum rate for multi-user wireless systems. Simulations demonstrate the superiority of the proposed frameworks over the conventional methods involving the sub-discrete Fourier transform (DFT) pilot matrix and iterative precoder algorithms enabling the deployment of systems characterized by fewer pilots or feedback bits.",
    "categories": [
      "cs.IT",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.IT",
    "comment": "Submitted to IEEE ICASSP 2026",
    "pdf_url": "https://arxiv.org/pdf/2510.09495v1",
    "published_date": "2025-10-10 15:55:18 UTC",
    "updated_date": "2025-10-10 15:55:18 UTC"
  },
  {
    "arxiv_id": "2510.13835v1",
    "title": "ConDABench: Interactive Evaluation of Language Models for Data Analysis",
    "authors": [
      "Avik Dutta",
      "Priyanshu Gupta",
      "Hosein Hasanbeig",
      "Rahul Pratap Singh",
      "Harshit Nigam",
      "Sumit Gulwani",
      "Arjun Radhakrishna",
      "Gustavo Soares",
      "Ashish Tiwari"
    ],
    "abstract": "Real-world data analysis tasks often come with under-specified goals and unclean data. User interaction is necessary to understand and disambiguate a user's intent, and hence, essential to solving these complex tasks. Existing benchmarks for evaluating LLMs on data analysis tasks do not capture these complexities or provide first-class support for interactivity. We introduce ConDABench, a framework for generating conversational data analysis (ConDA) benchmarks and evaluating external tools on the generated benchmarks. \\bench consists of (a) a multi-agent workflow for generating realistic benchmarks from articles describing insights gained from public datasets, (b) 1,420 ConDA problems generated using this workflow, and (c) an evaluation harness that, for the first time, makes it possible to systematically evaluate conversational data analysis tools on the generated ConDA problems. Evaluation of state-of-the-art LLMs on the benchmarks reveals that while the new generation of models are better at solving more instances, they are not necessarily better at solving tasks that require sustained, long-form engagement. ConDABench is an avenue for model builders to measure progress towards truly collaborative models that can complete complex interactive tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.13835v1",
    "published_date": "2025-10-10 15:54:51 UTC",
    "updated_date": "2025-10-10 15:54:51 UTC"
  },
  {
    "arxiv_id": "2510.09493v1",
    "title": "Performance Analysis of Machine Learning Algorithms in Chronic Kidney Disease Prediction",
    "authors": [
      "Iftekhar Ahmed",
      "Tanzil Ebad Chowdhury",
      "Biggo Bushon Routh",
      "Nafisa Tasmiya",
      "Shadman Sakib",
      "Adil Ahmed Chowdhury"
    ],
    "abstract": "Kidneys are the filter of the human body. About 10% of the global population is thought to be affected by Chronic Kidney Disease (CKD), which causes kidney function to decline. To protect in danger patients from additional kidney damage, effective risk evaluation of CKD and appropriate CKD monitoring are crucial. Due to quick and precise detection capabilities, Machine Learning models can help practitioners accomplish this goal efficiently; therefore, an enormous number of diagnosis systems and processes in the healthcare sector nowadays are relying on machine learning due to its disease prediction capability. In this study, we designed and suggested disease predictive computer-aided designs for the diagnosis of CKD. The dataset for CKD is attained from the repository of machine learning of UCL, with a few missing values; those are filled in using \"mean-mode\" and \"Random sampling method\" strategies. After successfully achieving the missing data, eight ML techniques (Random Forest, SVM, Naive Bayes, Logistic Regression, KNN, XGBoost, Decision Tree, and AdaBoost) were used to establish models, and the performance evaluation comparisons among the result accuracies are measured by the techniques to find the machine learning models with the highest accuracy. Among them, Random Forest as well as Logistic Regression showed an outstanding 99% accuracy, followed by the Ada Boost, XGBoost, Naive Bayes, Decision Tree, and SVM, whereas the KNN classifier model stands last with an accuracy of 73%.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, 7 figures, Presented at the 2022 IEEE 13th Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON), pp. 0417-0423",
    "pdf_url": "https://arxiv.org/pdf/2510.09493v1",
    "published_date": "2025-10-10 15:54:37 UTC",
    "updated_date": "2025-10-10 15:54:37 UTC"
  },
  {
    "arxiv_id": "2510.09474v1",
    "title": "Multimodal Policy Internalization for Conversational Agents",
    "authors": [
      "Zhenhailong Wang",
      "Jiateng Liu",
      "Amin Fazel",
      "Ritesh Sarkhel",
      "Xing Fan",
      "Xiang Li",
      "Chenlei Guo",
      "Heng Ji",
      "Ruhi Sarikaya"
    ],
    "abstract": "Modern conversational agents like ChatGPT and Alexa+ rely on predefined policies specifying metadata, response styles, and tool-usage rules. As these LLM-based systems expand to support diverse business and user queries, such policies, often implemented as in-context prompts, are becoming increasingly complex and lengthy, making faithful adherence difficult and imposing large fixed computational costs. With the rise of multimodal agents, policies that govern visual and multimodal behaviors are critical but remain understudied. Prior prompt-compression work mainly shortens task templates and demonstrations, while existing policy-alignment studies focus only on text-based safety rules. We introduce Multimodal Policy Internalization (MPI), a new task that internalizes reasoning-intensive multimodal policies into model parameters, enabling stronger policy-following without including the policy during inference. MPI poses unique data and algorithmic challenges. We build two datasets spanning synthetic and real-world decision-making and tool-using tasks and propose TriMPI, a three-stage training framework. TriMPI first injects policy knowledge via continual pretraining, then performs supervised finetuning, and finally applies PolicyRollout, a GRPO-style reinforcement learning extension that augments rollouts with policy-aware responses for grounded exploration. TriMPI achieves notable gains in end-to-end accuracy, generalization, and robustness to forgetting. As the first work on multimodal policy internalization, we provide datasets, training recipes, and comprehensive evaluations to foster future research. Project page: https://mikewangwzhl.github.io/TriMPI.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09474v1",
    "published_date": "2025-10-10 15:28:30 UTC",
    "updated_date": "2025-10-10 15:28:30 UTC"
  },
  {
    "arxiv_id": "2510.09469v1",
    "title": "Scalable Multi-Agent Path Finding using Collision-Aware Dynamic Alert Mask and a Hybrid Execution Strategy",
    "authors": [
      "Bharath Muppasani",
      "Ritirupa Dey",
      "Biplav Srivastava",
      "Vignesh Narayanan"
    ],
    "abstract": "Multi-agent pathfinding (MAPF) remains a critical problem in robotics and autonomous systems, where agents must navigate shared spaces efficiently while avoiding conflicts. Traditional centralized algorithms that have global information, such as Conflict-Based Search (CBS), provide high-quality solutions but become computationally expensive in large-scale scenarios due to the combinatorial explosion of conflicts that need resolution. Conversely, distributed approaches that have local information, particularly learning-based methods, offer better scalability by operating with relaxed information availability, yet often at the cost of solution quality. To address these limitations, we propose a hybrid framework that combines decentralized path planning with a lightweight centralized coordinator. Our framework leverages reinforcement learning (RL) for decentralized planning, enabling agents to adapt their planning based on minimal, targeted alerts--such as static conflict-cell flags or brief conflict tracks--that are dynamically shared information from the central coordinator for effective conflict resolution. We empirically study the effect of the information available to an agent on its planning performance. Our approach reduces the inter-agent information sharing compared to fully centralized and distributed methods, while still consistently finding feasible, collision-free solutions--even in large-scale scenarios having higher agent counts.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09469v1",
    "published_date": "2025-10-10 15:25:40 UTC",
    "updated_date": "2025-10-10 15:25:40 UTC"
  },
  {
    "arxiv_id": "2510.09462v1",
    "title": "Adaptive Attacks on Trusted Monitors Subvert AI Control Protocols",
    "authors": [
      "Mikhail Terekhov",
      "Alexander Panfilov",
      "Daniil Dzenhaliou",
      "Caglar Gulcehre",
      "Maksym Andriushchenko",
      "Ameya Prabhu",
      "Jonas Geiping"
    ],
    "abstract": "AI control protocols serve as a defense mechanism to stop untrusted LLM agents from causing harm in autonomous settings. Prior work treats this as a security problem, stress testing with exploits that use the deployment context to subtly complete harmful side tasks, such as backdoor insertion. In practice, most AI control protocols are fundamentally based on LLM monitors, which can become a central point of failure. We study adaptive attacks by an untrusted model that knows the protocol and the monitor model, which is plausible if the untrusted model was trained with a later knowledge cutoff or can search for this information autonomously. We instantiate a simple adaptive attack vector by which the attacker embeds publicly known or zero-shot prompt injections in the model outputs. Using this tactic, frontier models consistently evade diverse monitors and complete malicious tasks on two main AI control benchmarks. The attack works universally against current protocols that rely on a monitor. Furthermore, the recent Defer-to-Resample protocol even backfires, as its resampling amplifies the prompt injection and effectively reframes it as a best-of-$n$ attack. In general, adaptive attacks on monitor models represent a major blind spot in current control protocols and should become a standard component of evaluations for future AI control mechanisms.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09462v1",
    "published_date": "2025-10-10 15:12:44 UTC",
    "updated_date": "2025-10-10 15:12:44 UTC"
  },
  {
    "arxiv_id": "2512.00007v1",
    "title": "Use of Retrieval-Augmented Large Language Model Agent for Long-Form COVID-19 Fact-Checking",
    "authors": [
      "Jingyi Huang",
      "Yuyi Yang",
      "Mengmeng Ji",
      "Charles Alba",
      "Sheng Zhang",
      "Ruopeng An"
    ],
    "abstract": "The COVID-19 infodemic calls for scalable fact-checking solutions that handle long-form misinformation with accuracy and reliability. This study presents SAFE (system for accurate fact extraction and evaluation), an agent system that combines large language models with retrieval-augmented generation (RAG) to improve automated fact-checking of long-form COVID-19 misinformation. SAFE includes two agents - one for claim extraction and another for claim verification using LOTR-RAG, which leverages a 130,000-document COVID-19 research corpus. An enhanced variant, SAFE (LOTR-RAG + SRAG), incorporates Self-RAG to refine retrieval via query rewriting. We evaluated both systems on 50 fake news articles (2-17 pages) containing 246 annotated claims (M = 4.922, SD = 3.186), labeled as true (14.1%), partly true (14.4%), false (27.0%), partly false (2.2%), and misleading (21.0%) by public health professionals. SAFE systems significantly outperformed baseline LLMs in all metrics (p < 0.001). For consistency (0-1 scale), SAFE (LOTR-RAG) scored 0.629, exceeding both SAFE (+SRAG) (0.577) and the baseline (0.279). In subjective evaluations (0-4 Likert scale), SAFE (LOTR-RAG) also achieved the highest average ratings in usefulness (3.640), clearness (3.800), and authenticity (3.526). Adding SRAG slightly reduced overall performance, except for a minor gain in clearness. SAFE demonstrates robust improvements in long-form COVID-19 fact-checking by addressing LLM limitations in consistency and explainability. The core LOTR-RAG design proved more effective than its SRAG-augmented variant, offering a strong foundation for scalable misinformation mitigation.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.00007v1",
    "published_date": "2025-10-10 15:10:46 UTC",
    "updated_date": "2025-10-10 15:10:46 UTC"
  },
  {
    "arxiv_id": "2510.09459v2",
    "title": "Failure Prediction at Runtime for Generative Robot Policies",
    "authors": [
      "Ralf Römer",
      "Adrian Kobras",
      "Luca Worbis",
      "Angela P. Schoellig"
    ],
    "abstract": "Imitation learning (IL) with generative models, such as diffusion and flow matching, has enabled robots to perform complex, long-horizon tasks. However, distribution shifts from unseen environments or compounding action errors can still cause unpredictable and unsafe behavior, leading to task failure. Early failure prediction during runtime is therefore essential for deploying robots in human-centered and safety-critical environments. We propose FIPER, a general framework for Failure Prediction at Runtime for generative IL policies that does not require failure data. FIPER identifies two key indicators of impending failure: (i) out-of-distribution (OOD) observations detected via random network distillation in the policy's embedding space, and (ii) high uncertainty in generated actions measured by a novel action-chunk entropy score. Both failure prediction scores are calibrated using a small set of successful rollouts via conformal prediction. A failure alarm is triggered when both indicators, aggregated over short time windows, exceed their thresholds. We evaluate FIPER across five simulation and real-world environments involving diverse failure modes. Our results demonstrate that FIPER better distinguishes actual failures from benign OOD situations and predicts failures more accurately and earlier than existing methods. We thus consider this work an important step towards more interpretable and safer generative robot policies. Code, data and videos are available at https://tum-lsy.github.io/fiper_website.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Project page: https://tum-lsy.github.io/fiper_website. 33 pages, 12 figures. Accepted to NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.09459v2",
    "published_date": "2025-10-10 15:09:27 UTC",
    "updated_date": "2025-10-13 13:29:31 UTC"
  },
  {
    "arxiv_id": "2510.09458v1",
    "title": "SilvaScenes: Tree Segmentation and Species Classification from Under-Canopy Images in Natural Forests",
    "authors": [
      "David-Alexandre Duclos",
      "William Guimont-Martin",
      "Gabriel Jeanson",
      "Arthur Larochelle-Tremblay",
      "Théo Defosse",
      "Frédéric Moore",
      "Philippe Nolet",
      "François Pomerleau",
      "Philippe Giguère"
    ],
    "abstract": "Interest in robotics for forest management is growing, but perception in complex, natural environments remains a significant hurdle. Conditions such as heavy occlusion, variable lighting, and dense vegetation pose challenges to automated systems, which are essential for precision forestry, biodiversity monitoring, and the automation of forestry equipment. These tasks rely on advanced perceptual capabilities, such as detection and fine-grained species classification of individual trees. Yet, existing datasets are inadequate to develop such perception systems, as they often focus on urban settings or a limited number of species. To address this, we present SilvaScenes, a new dataset for instance segmentation of tree species from under-canopy images. Collected across five bioclimatic domains in Quebec, Canada, SilvaScenes features 1476 trees from 24 species with annotations from forestry experts. We demonstrate the relevance and challenging nature of our dataset by benchmarking modern deep learning approaches for instance segmentation. Our results show that, while tree segmentation is easy, with a top mean average precision (mAP) of 67.65%, species classification remains a significant challenge with an mAP of only 35.69%. Our dataset and source code will be available at https://github.com/norlab-ulaval/SilvaScenes.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.09458v1",
    "published_date": "2025-10-10 15:08:35 UTC",
    "updated_date": "2025-10-10 15:08:35 UTC"
  },
  {
    "arxiv_id": "2510.09425v1",
    "title": "Bandits with Single-Peaked Preferences and Limited Resources",
    "authors": [
      "Gur Keinan",
      "Rotem Torkan",
      "Omer Ben-Porat"
    ],
    "abstract": "We study an online stochastic matching problem in which an algorithm sequentially matches $U$ users to $K$ arms, aiming to maximize cumulative reward over $T$ rounds under budget constraints. Without structural assumptions, computing the optimal matching is NP-hard, making online learning computationally infeasible. To overcome this barrier, we focus on \\emph{single-peaked preferences} -- a well-established structure in social choice theory, where users' preferences are unimodal with respect to a common order over arms. We devise an efficient algorithm for the offline budgeted matching problem, and leverage it into an efficient online algorithm with a regret of $\\tilde O(UKT^{2/3})$. Our approach relies on a novel PQ tree-based order approximation method. If the single-peaked structure is known, we develop an efficient UCB-like algorithm that achieves a regret bound of $\\tilde O(U\\sqrt{TK})$.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09425v1",
    "published_date": "2025-10-10 14:27:25 UTC",
    "updated_date": "2025-10-10 14:27:25 UTC"
  },
  {
    "arxiv_id": "2510.09424v1",
    "title": "The Speech-LLM Takes It All: A Truly Fully End-to-End Spoken Dialogue State Tracking Approach",
    "authors": [
      "Nizar El Ghazal",
      "Antoine Caubrière",
      "Valentin Vielzeuf"
    ],
    "abstract": "This paper presents a comparative study of context management strategies for end-to-end Spoken Dialog State Tracking using Speech-LLMs. We systematically evaluate traditional multimodal context (combining text history and spoken current turn), full spoken history, and compressed spoken history approaches. Our experiments on the SpokenWOZ corpus demonstrate that providing the full spoken conversation as input yields the highest performance among models of similar size, significantly surpassing prior methods. Furthermore, we show that attention-pooling-based compression of the spoken history offers a strong trade-off, maintaining competitive accuracy with reduced context size. Detailed analysis confirms that improvements stem from more effective context utilization.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09424v1",
    "published_date": "2025-10-10 14:27:01 UTC",
    "updated_date": "2025-10-10 14:27:01 UTC"
  },
  {
    "arxiv_id": "2510.19835v1",
    "title": "A Quantum-Inspired Algorithm for Solving Sudoku Puzzles and the MaxCut Problem",
    "authors": [
      "Max B. Zhao",
      "Fei Li"
    ],
    "abstract": "We propose and evaluate a quantum-inspired algorithm for solving Quadratic Unconstrained Binary Optimization (QUBO) problems, which are mathematically equivalent to finding ground states of Ising spin-glass Hamiltonians. The algorithm employs Matrix Product States (MPS) to compactly represent large superpositions of spin configurations and utilizes a discrete driving schedule to guide the MPS toward the ground state. At each step, a driver Hamiltonian -- incorporating a transverse magnetic field -- is combined with the problem Hamiltonian to enable spin flips and facilitate quantum tunneling. The MPS is updated using the standard Density Matrix Renormalization Group (DMRG) method, which iteratively minimizes the system's energy via multiple sweeps across the spin chain. Despite its heuristic nature, the algorithm reliably identifies global minima, not merely near-optimal solutions, across diverse QUBO instances. We first demonstrate its effectiveness on intermediate-level Sudoku puzzles from publicly available sources, involving over $200$ Ising spins with long-range couplings dictated by constraint satisfaction. We then apply the algorithm to MaxCut problems from the Biq Mac library, successfully solving instances with up to $251$ nodes and $3,265$ edges. We discuss the advantages of this quantum-inspired approach, including its scalability, generalizability, and suitability for industrial-scale QUBO applications.",
    "categories": [
      "cs.AI",
      "cs.ET",
      "cs.NE",
      "quant-ph"
    ],
    "primary_category": "cs.AI",
    "comment": "29 pages, 10 figures, accepted by Quantum Information & Computation on August 6, 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.19835v1",
    "published_date": "2025-10-10 14:24:14 UTC",
    "updated_date": "2025-10-10 14:24:14 UTC"
  },
  {
    "arxiv_id": "2510.09421v1",
    "title": "On the Representations of Entities in Auto-regressive Large Language Models",
    "authors": [
      "Victor Morand",
      "Josiane Mothe",
      "Benjamin Piwowarski"
    ],
    "abstract": "Named entities are fundamental building blocks of knowledge in text, grounding factual information and structuring relationships within language. Despite their importance, it remains unclear how Large Language Models (LLMs) internally represent entities. Prior research has primarily examined explicit relationships, but little is known about entity representations themselves. We introduce entity mention reconstruction as a novel framework for studying how LLMs encode and manipulate entities. We investigate whether entity mentions can be generated from internal representations, how multi-token entities are encoded beyond last-token embeddings, and whether these representations capture relational knowledge. Our proposed method, leveraging _task vectors_, allows to consistently generate multi-token mentions from various entity representations derived from the LLMs hidden states. We thus introduce the _Entity Lens_, extending the _logit-lens_ to predict multi-token mentions. Our results bring new evidence that LLMs develop entity-specific mechanisms to represent and manipulate any multi-token entities, including those unseen during training. Our code is avalable at https://github.com/VictorMorand/EntityRepresentations .",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at BlackBoxNLP@EMNLP2025",
    "pdf_url": "https://arxiv.org/pdf/2510.09421v1",
    "published_date": "2025-10-10 14:23:44 UTC",
    "updated_date": "2025-10-10 14:23:44 UTC"
  },
  {
    "arxiv_id": "2510.09736v1",
    "title": "Chlorophyll-a Mapping and Prediction in the Mar Menor Lagoon Using C2RCC-Processed Sentinel 2 Imagery",
    "authors": [
      "Antonio Martínez-Ibarra",
      "Aurora González-Vidal",
      "Adrián Cánovas-Rodríguez",
      "Antonio F. Skarmeta"
    ],
    "abstract": "The Mar Menor, Europe's largest coastal lagoon, located in Spain, has undergone severe eutrophication crises. Monitoring chlorophyll-a (Chl-a) is essential to anticipate harmful algal blooms and guide mitigation. Traditional in situ measurements are spatially and temporally limited. Satellite-based approaches provide a more comprehensive view, enabling scalable, long-term, and transferable monitoring. This study aims to overcome limitations of chlorophyll monitoring, often restricted to surface estimates or limited temporal coverage, by developing a reliable methodology to predict and map Chl-a across the water column of the Mar Menor. The work integrates Sentinel 2 imagery with buoy-based ground truth to create models capable of high-resolution, depth-specific monitoring, enhancing early-warning capabilities for eutrophication. Nearly a decade of Sentinel 2 images was atmospherically corrected using C2RCC processors. Buoy data were aggregated by depth (0-1 m, 1-2 m, 2-3 m, 3-4 m). Multiple ML and DL algorithms-including RF, XGBoost, CatBoost, Multilater Perceptron Networks, and ensembles-were trained and validated using cross-validation. Systematic band-combination experiments and spatial aggregation strategies were tested to optimize prediction. Results show depth-dependent performance. At the surface, C2X-Complex with XGBoost and ensemble models achieved R2 = 0.89; at 1-2 m, CatBoost and ensemble models reached R2 = 0.87; at 2-3 m, TOA reflectances with KNN performed best (R2 = 0.81); while at 3-4 m, RF achieved R2 = 0.66. Generated maps successfully reproduced known eutrophication events (e.g., 2016 crisis, 2025 surge), confirming robustness. The study delivers an end-to-end, validated methodology for depth-specific Chl-amapping. Its integration of multispectral band combinations, buoy calibration, and ML/DL modeling offers a transferable framework for other turbid coastal systems.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "physics.ao-ph"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09736v1",
    "published_date": "2025-10-10 14:20:25 UTC",
    "updated_date": "2025-10-10 14:20:25 UTC"
  },
  {
    "arxiv_id": "2510.09735v1",
    "title": "InterCorpRel-LLM: Enhancing Financial Relational Understanding with Graph-Language Models",
    "authors": [
      "Qianyou Sun",
      "Jiexin Zheng",
      "Bohan Jin",
      "Lihua Chen",
      "Yijie Peng"
    ],
    "abstract": "Identifying inter-firm relationships such as supply and competitive ties is critical for financial analysis and corporate governance, yet remains challenging due to the scale, sparsity, and contextual dependence of corporate data. Graph-based methods capture structure but miss semantic depth, while large language models (LLMs) excel at text but remain limited in their ability to represent relational dependencies. To address this, we propose InterCorpRel-LLM, a cross-modal framework that integrates GNNs with LLMs, supported by a proprietary dataset derived from FactSet supply chain records and three tailored training tasks: company graph matching, industry classification, and supply relation prediction. This design enables effective joint modeling of structure and semantics. Experiments show that InterCorpRel-LLM substantially outperforms strong baselines, including GPT-5, on a supply relation identification task, achieving an F-score of 0.8543 vs. 0.2287 with only a 7B-parameter backbone and lightweight training. The model also generalizes to zero-shot competitor identification, underscoring its ability to capture nuanced inter-firm dynamics. Our framework thus provides analysts and strategists with a robust tool for mapping and reasoning about complex corporate networks, enhancing decision-making and risk management in dynamic markets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09735v1",
    "published_date": "2025-10-10 14:18:18 UTC",
    "updated_date": "2025-10-10 14:18:18 UTC"
  },
  {
    "arxiv_id": "2510.09734v1",
    "title": "ARROW: An Adaptive Rollout and Routing Method for Global Weather Forecasting",
    "authors": [
      "Jindong Tian",
      "Yifei Ding",
      "Ronghui Xu",
      "Hao Miao",
      "Chenjuan Guo",
      "Bin Yang"
    ],
    "abstract": "Weather forecasting is a fundamental task in spatiotemporal data analysis, with broad applications across a wide range of domains. Existing data-driven forecasting methods typically model atmospheric dynamics over a fixed short time interval (e.g., 6 hours) and rely on naive autoregression-based rollout for long-term forecasting (e.g., 138 hours). However, this paradigm suffers from two key limitations: (1) it often inadequately models the spatial and multi-scale temporal dependencies inherent in global weather systems, and (2) the rollout strategy struggles to balance error accumulation with the capture of fine-grained atmospheric variations. In this study, we propose ARROW, an Adaptive-Rollout Multi-scale temporal Routing method for Global Weather Forecasting. To contend with the first limitation, we construct a multi-interval forecasting model that forecasts weather across different time intervals. Within the model, the Shared-Private Mixture-of-Experts captures both shared patterns and specific characteristics of atmospheric dynamics across different time scales, while Ring Positional Encoding accurately encodes the circular latitude structure of the Earth when representing spatial information. For the second limitation, we develop an adaptive rollout scheduler based on reinforcement learning, which selects the most suitable time interval to forecast according to the current weather state. Experimental results demonstrate that ARROW achieves state-of-the-art performance in global weather forecasting, establishing a promising paradigm in this field.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages, 6 figures, conference",
    "pdf_url": "https://arxiv.org/pdf/2510.09734v1",
    "published_date": "2025-10-10 14:00:59 UTC",
    "updated_date": "2025-10-10 14:00:59 UTC"
  },
  {
    "arxiv_id": "2510.09404v2",
    "title": "Agentic Systems in Radiology: Design, Applications, Evaluation, and Challenges",
    "authors": [
      "Christian Bluethgen",
      "Dave Van Veen",
      "Daniel Truhn",
      "Jakob Nikolas Kather",
      "Michael Moor",
      "Malgorzata Polacin",
      "Akshay Chaudhari",
      "Thomas Frauenfelder",
      "Curtis P. Langlotz",
      "Michael Krauthammer",
      "Farhad Nooralahzadeh"
    ],
    "abstract": "Building agents, systems that perceive and act upon their environment with a degree of autonomy, has long been a focus of AI research. This pursuit has recently become vastly more practical with the emergence of large language models (LLMs) capable of using natural language to integrate information, follow instructions, and perform forms of \"reasoning\" and planning across a wide range of tasks. With its multimodal data streams and orchestrated workflows spanning multiple systems, radiology is uniquely suited to benefit from agents that can adapt to context and automate repetitive yet complex tasks. In radiology, LLMs and their multimodal variants have already demonstrated promising performance for individual tasks such as information extraction and report summarization. However, using LLMs in isolation underutilizes their potential to support complex, multi-step workflows where decisions depend on evolving context from multiple information sources. Equipping LLMs with external tools and feedback mechanisms enables them to drive systems that exhibit a spectrum of autonomy, ranging from semi-automated workflows to more adaptive agents capable of managing complex processes. This review examines the design of such LLM-driven agentic systems, highlights key applications, discusses evaluation methods for planning and tool use, and outlines challenges such as error cascades, tool-use efficiency, and health IT integration.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09404v2",
    "published_date": "2025-10-10 13:56:27 UTC",
    "updated_date": "2025-10-13 07:11:22 UTC"
  },
  {
    "arxiv_id": "2510.09394v3",
    "title": "Beyond Single-Granularity Prompts: A Multi-Scale Chain-of-Thought Prompt Learning for Graph",
    "authors": [
      "Ziyu Zheng",
      "Yaming Yang",
      "Ziyu Guan",
      "Wei Zhao",
      "Xinyan Huang",
      "Weigang Lu"
    ],
    "abstract": "The ``pre-train, prompt\" paradigm, designed to bridge the gap between pre-training tasks and downstream objectives, has been extended from the NLP domain to the graph domain and has achieved remarkable progress. Current mainstream graph prompt-tuning methods modify input or output features using learnable prompt vectors. However, existing approaches are confined to single-granularity (e.g., node-level or subgraph-level) during prompt generation, overlooking the inherently multi-scale structural information in graph data, which limits the diversity of prompt semantics. To address this issue, we pioneer the integration of multi-scale information into graph prompt and propose a Multi-Scale Graph Chain-of-Thought (MSGCOT) prompting framework. Specifically, we design a lightweight, low-rank coarsening network to efficiently capture multi-scale structural features as hierarchical basis vectors for prompt generation. Subsequently, mimicking human cognition from coarse-to-fine granularity, we dynamically integrate multi-scale information at each reasoning step, forming a progressive coarse-to-fine prompt chain. Extensive experiments on eight benchmark datasets demonstrate that MSGCOT outperforms the state-of-the-art single-granularity graph prompt-tuning method, particularly in few-shot scenarios, showcasing superior performance. The code is available at: https://github.com/zhengziyu77/MSGCOT.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by WWW2026",
    "pdf_url": "https://arxiv.org/pdf/2510.09394v3",
    "published_date": "2025-10-10 13:48:34 UTC",
    "updated_date": "2026-01-21 08:54:49 UTC"
  },
  {
    "arxiv_id": "2510.09393v2",
    "title": "ChoirRec: Semantic User Grouping via LLMs for Conversion Rate Prediction of Low-Activity Users",
    "authors": [
      "Dakai Zhai",
      "Jiong Gao",
      "Boya Du",
      "Junwei Xu",
      "Qijie Shen",
      "Jialin Zhu",
      "Yuning Jiang"
    ],
    "abstract": "Accurately predicting conversion rates (CVR) for low-activity users remains a fundamental challenge in large-scale e-commerce recommender systems. Existing approaches face three critical limitations: (i) reliance on noisy and unreliable behavioral signals; (ii) insufficient user-level information due to the lack of diverse interaction data; and (iii) a systemic training bias toward high-activity users that overshadows the needs of low-activity users. To address these challenges, we propose ChoirRec, a novel framework that leverages the semantic capabilities of Large Language Models (LLMs) to construct semantic user groups and enhance CVR prediction for low-activity users. With a dual-channel architecture designed for robust cross-user knowledge transfer, ChoirRec comprises three components: (i) a Semantic Group Generation module that utilizes LLMs to form reliable, cross-activity user clusters, thereby filtering out noisy signals; (ii) a Group-aware Hierarchical Representation module that enriches sparse user embeddings with informative group-level priors to mitigate data insufficiency; and (iii) a Group-aware Multi-granularity Modual that employs a dual-channel architecture and adaptive fusion mechanism to ensure effective learning and utilization of group knowledge. We conduct extensive offline and online experiments on Taobao, a leading industrial-scale e-commerce platform. ChoirRec improves GAUC by 1.16\\% in offline evaluations, while online A/B testing reveals a 7.24\\% increase in order volume, highlighting its substantial practical value in real-world applications.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09393v2",
    "published_date": "2025-10-10 13:48:09 UTC",
    "updated_date": "2025-10-13 02:54:57 UTC"
  },
  {
    "arxiv_id": "2510.09390v1",
    "title": "Identifying & Interactively Refining Ambiguous User Goals for Data Visualization Code Generation",
    "authors": [
      "Mert İnan",
      "Anthony Sicilia",
      "Alex Xie",
      "Saujas Vaduguru",
      "Daniel Fried",
      "Malihe Alikhani"
    ],
    "abstract": "Establishing shared goals is a fundamental step in human-AI communication. However, ambiguities can lead to outputs that seem correct but fail to reflect the speaker's intent. In this paper, we explore this issue with a focus on the data visualization domain, where ambiguities in natural language impact the generation of code that visualizes data. The availability of multiple views on the contextual (e.g., the intended plot and the code rendering the plot) allows for a unique and comprehensive analysis of diverse ambiguity types. We develop a taxonomy of types of ambiguity that arise in this task and propose metrics to quantify them. Using Matplotlib problems from the DS-1000 dataset, we demonstrate that our ambiguity metrics better correlate with human annotations than uncertainty baselines. Our work also explores how multi-turn dialogue can reduce ambiguity, therefore, improve code accuracy by better matching user goals. We evaluate three pragmatic models to inform our dialogue strategies: Gricean Cooperativity, Discourse Representation Theory, and Questions under Discussion. A simulated user study reveals how pragmatic dialogues reduce ambiguity and enhance code accuracy, highlighting the value of multi-turn exchanges in code generation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.HC",
      "cs.MA"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09390v1",
    "published_date": "2025-10-10 13:44:40 UTC",
    "updated_date": "2025-10-10 13:44:40 UTC"
  },
  {
    "arxiv_id": "2510.09389v1",
    "title": "Design Principles for Sequence Models via Coefficient Dynamics",
    "authors": [
      "Jerome Sieber",
      "Antonio Orvieto",
      "Melanie N. Zeilinger",
      "Carmen Amo Alonso"
    ],
    "abstract": "Deep sequence models, ranging from Transformers and State Space Models (SSMs) to more recent approaches such as gated linear RNNs, fundamentally compute outputs as linear combinations of past value vectors. To draw insights and systematically compare such architectures, we develop a unified framework that makes this output operation explicit, by casting the linear combination coefficients as the outputs of autonomous linear dynamical systems driven by impulse inputs. This viewpoint, in spirit substantially different from approaches focusing on connecting linear RNNs with linear attention, reveals a common mathematical theme across diverse architectures and crucially captures softmax attention, on top of RNNs, SSMs, and related models. In contrast to new model proposals that are commonly evaluated on benchmarks, we derive design principles linking architectural choices to model properties. Thereby identifying tradeoffs between expressivity and efficient implementation, geometric constraints on input selectivity, and stability conditions for numerically stable training and information retention. By connecting several insights and observations from recent literature, the framework both explains empirical successes of recent designs and provides guiding principles for systematically designing new sequence model architectures.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09389v1",
    "published_date": "2025-10-10 13:42:31 UTC",
    "updated_date": "2025-10-10 13:42:31 UTC"
  },
  {
    "arxiv_id": "2510.09379v1",
    "title": "Task-Level Insights from Eigenvalues across Sequence Models",
    "authors": [
      "Rahel Rickenbach",
      "Jelena Trisovic",
      "Alexandre Didier",
      "Jerome Sieber",
      "Melanie N. Zeilinger"
    ],
    "abstract": "Although softmax attention drives state-of-the-art performance for sequence models, its quadratic complexity limits scalability, motivating linear alternatives such as state space models (SSMs). While these alternatives improve efficiency, their fundamental differences in information processing remain poorly understood. In this work, we leverage the recently proposed dynamical systems framework to represent softmax, norm and linear attention as dynamical systems, enabling a structured comparison with SSMs by analyzing their respective eigenvalue spectra. Since eigenvalues capture essential aspects of dynamical system behavior, we conduct an extensive empirical analysis across diverse sequence models and benchmarks. We first show that eigenvalues influence essential aspects of memory and long-range dependency modeling, revealing spectral signatures that align with task requirements. Building on these insights, we then investigate how architectural modifications in sequence models impact both eigenvalue spectra and task performance. This correspondence further strengthens the position of eigenvalue analysis as a principled metric for interpreting, understanding, and ultimately improving the capabilities of sequence models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09379v1",
    "published_date": "2025-10-10 13:35:21 UTC",
    "updated_date": "2025-10-10 13:35:21 UTC"
  },
  {
    "arxiv_id": "2510.09378v1",
    "title": "The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton",
    "authors": [
      "Natalie Abreu",
      "Nikhil Vyas",
      "Sham Kakade",
      "Depen Morwani"
    ],
    "abstract": "Recent efforts to accelerate LLM pretraining have focused on computationally-efficient approximations that exploit second-order structure. This raises a key question for large-scale training: how much performance is forfeited by these approximations? To probe this question, we establish a practical upper bound on iteration complexity by applying full Gauss-Newton (GN) preconditioning to transformer models of up to 150M parameters. Our experiments show that full GN updates yield substantial gains over existing optimizers, achieving a 5.4x reduction in training iterations compared to strong baselines like SOAP and Muon. Furthermore, we find that a precise layerwise GN preconditioner, which ignores cross-layer information, nearly matches the performance of the full GN method. Collectively, our results suggest: (1) the GN approximation is highly effective for preconditioning, implying higher-order loss terms may not be critical for convergence speed; (2) the layerwise Hessian structure contains sufficient information to achieve most of these potential gains; and (3) a significant performance gap exists between current approximate methods and an idealized layerwise oracle.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09378v1",
    "published_date": "2025-10-10 13:35:10 UTC",
    "updated_date": "2025-10-10 13:35:10 UTC"
  },
  {
    "arxiv_id": "2510.09373v1",
    "title": "Sequence Variables: A Constraint Programming Computational Domain for Routing and Sequencing",
    "authors": [
      "Augustin Delecluse",
      "Pierre Schaus",
      "Pascal Van Hentenryck"
    ],
    "abstract": "Constraint Programming (CP) offers an intuitive, declarative framework for modeling Vehicle Routing Problems (VRP), yet classical CP models based on successor variables cannot always deal with optional visits or insertion based heuristics. To address these limitations, this paper formalizes sequence variables within CP. Unlike the classical successor models, this computational domain handle optional visits and support insertion heuristics, including insertion-based Large Neighborhood Search. We provide a clear definition of their domain, update operations, and introduce consistency levels for constraints on this domain. An implementation is described with the underlying data structures required for integrating sequence variables into existing trail-based CP solvers. Furthermore, global constraints specifically designed for sequence variables and vehicle routing are introduced. Finally, the effectiveness of sequence variables is demonstrated by simplifying problem modeling and achieving competitive computational performance on the Dial-a-Ride Problem.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09373v1",
    "published_date": "2025-10-10 13:29:40 UTC",
    "updated_date": "2025-10-10 13:29:40 UTC"
  },
  {
    "arxiv_id": "2511.17520v1",
    "title": "Safe Farming: Development of a Prevention System to Mitigate Vertebrates Crop Raiding",
    "authors": [
      "Razi Iqbal"
    ],
    "abstract": "One of the main problems for farmers is the protection of their crops, before and after harvesting, from animals and birds. To overcome this problem, this paper proposes a model of safe farming in which the crops will be protected from vertebrates attack through a prevention system that is based on Wirelesses Sensors Networks. Different sensor nodes are placed around the field that detect animals or birds existence and generate required signals and information. This information is passed to the Repelling and Notifying System (RNS) that is installed at the field through a short range wireless technology, ZigBee. As RNS receives the information, it generates ultrasonic sounds that are unbearable for animals and birds, which causes them to run away from the field. These ultrasonic sounds are generated in a frequency range that only animals and birds can hear, while humans cannot notice the sound. The paper also proposes a notifying system. It will inform the farmer about animals or birds intrusion in the field through SMS, but doesn't need any action from the farmer. The low cost and power efficiency of the proposed system is a key advantage for developing countries where cost and power are major players in any system feasibility.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.17520v1",
    "published_date": "2025-10-10 13:21:04 UTC",
    "updated_date": "2025-10-10 13:21:04 UTC"
  },
  {
    "arxiv_id": "2510.09362v1",
    "title": "deep-REMAP: Probabilistic Parameterization of Stellar Spectra Using Regularized Multi-Task Learning",
    "authors": [
      "Sankalp Gilda"
    ],
    "abstract": "In the era of exploding survey volumes, traditional methods of spectroscopic analysis are being pushed to their limits. In response, we develop deep-REMAP, a novel deep learning framework that utilizes a regularized, multi-task approach to predict stellar atmospheric parameters from observed spectra. We train a deep convolutional neural network on the PHOENIX synthetic spectral library and use transfer learning to fine-tune the model on a small subset of observed FGK dwarf spectra from the MARVELS survey. We then apply the model to 732 uncharacterized FGK giant candidates from the same survey. When validated on 30 MARVELS calibration stars, deep-REMAP accurately recovers the effective temperature ($T_{\\rm{eff}}$), surface gravity ($\\log \\rm{g}$), and metallicity ([Fe/H]), achieving a precision of, for instance, approximately 75 K in $T_{\\rm{eff}}$. By combining an asymmetric loss function with an embedding loss, our regression-as-classification framework is interpretable, robust to parameter imbalances, and capable of capturing non-Gaussian uncertainties. While developed for MARVELS, the deep-REMAP framework is extensible to other surveys and synthetic libraries, demonstrating a powerful and automated pathway for stellar characterization.",
    "categories": [
      "astro-ph.IM",
      "astro-ph.SR",
      "cs.AI"
    ],
    "primary_category": "astro-ph.IM",
    "comment": "14 pages. Accepted for publication in RASTI",
    "pdf_url": "https://arxiv.org/pdf/2510.09362v1",
    "published_date": "2025-10-10 13:20:06 UTC",
    "updated_date": "2025-10-10 13:20:06 UTC"
  },
  {
    "arxiv_id": "2510.09732v1",
    "title": "Evaluating LLM-Based Process Explanations under Progressive Behavioral-Input Reduction",
    "authors": [
      "P. van Oerle",
      "R. H. Bemthuis",
      "F. A. Bukhsh"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly used to generate textual explanations of process models discovered from event logs. Producing explanations from large behavioral abstractions (e.g., directly-follows graphs or Petri nets) can be computationally expensive. This paper reports an exploratory evaluation of explanation quality under progressive behavioral-input reduction, where models are discovered from progressively smaller prefixes of a fixed log. Our pipeline (i) discovers models at multiple input sizes, (ii) prompts an LLM to generate explanations, and (iii) uses a second LLM to assess completeness, bottleneck identification, and suggested improvements. On synthetic logs, explanation quality is largely preserved under moderate reduction, indicating a practical cost-quality trade-off. The study is exploratory, as the scores are LLM-based (comparative signals rather than ground truth) and the data are synthetic. The results suggest a path toward more computationally efficient, LLM-assisted process analysis in resource-constrained settings.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, 2 figures, 3 tables; to appear in Enterprise Design, Operations, and Computing. EDOC 2025 Workshops, Lecture Notes in Business Information Processing (LNBIP), Springer, 2025. Part of 29th International Conference on Enterprise Design, Operations, and Computing (EDOC)",
    "pdf_url": "https://arxiv.org/pdf/2510.09732v1",
    "published_date": "2025-10-10 13:10:50 UTC",
    "updated_date": "2025-10-10 13:10:50 UTC"
  },
  {
    "arxiv_id": "2510.15949v2",
    "title": "ATLAS: Adaptive Trading with LLM AgentS Through Dynamic Prompt Optimization and Multi-Agent Coordination",
    "authors": [
      "Charidimos Papadakis",
      "Angeliki Dimitriou",
      "Giorgos Filandrianos",
      "Maria Lymperaiou",
      "Konstantinos Thomas",
      "Giorgos Stamou"
    ],
    "abstract": "Large language models show promise for financial decision-making, yet deploying them as autonomous trading agents raises fundamental challenges: how to adapt instructions when rewards arrive late and obscured by market noise, how to synthesize heterogeneous information streams into coherent decisions, and how to bridge the gap between model outputs and executable market actions. We present ATLAS (Adaptive Trading with LLM AgentS), a unified multi-agent framework that integrates structured information from markets, news, and corporate fundamentals to support robust trading decisions. Within ATLAS, the central trading agent operates in an order-aware action space, ensuring that outputs correspond to executable market orders rather than abstract signals. The agent can incorporate feedback while trading using Adaptive-OPRO, a novel prompt-optimization technique that dynamically adapts the prompt by incorporating real-time, stochastic feedback, leading to increasing performance over time. Across regime-specific equity studies and multiple LLM families, Adaptive-OPRO consistently outperforms fixed prompts, while reflection-based feedback fails to provide systematic gains.",
    "categories": [
      "q-fin.TR",
      "cs.AI"
    ],
    "primary_category": "q-fin.TR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15949v2",
    "published_date": "2025-10-10 13:01:51 UTC",
    "updated_date": "2026-01-08 13:08:59 UTC"
  },
  {
    "arxiv_id": "2510.09340v1",
    "title": "Toward Mechanistic Explanation of Deductive Reasoning in Language Models",
    "authors": [
      "Davide Maltoni",
      "Matteo Ferrara"
    ],
    "abstract": "Recent large language models have demonstrated relevant capabilities in solving problems that require logical reasoning; however, the corresponding internal mechanisms remain largely unexplored. In this paper, we show that a small language model can solve a deductive reasoning task by learning the underlying rules (rather than operating as a statistical learner). A low-level explanation of its internal representations and computational circuits is then provided. Our findings reveal that induction heads play a central role in the implementation of the rule completion and rule chaining steps involved in the logical inference required by the task.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09340v1",
    "published_date": "2025-10-10 12:49:00 UTC",
    "updated_date": "2025-10-10 12:49:00 UTC"
  },
  {
    "arxiv_id": "2510.09338v2",
    "title": "Localist LLMs -- A Mathematical Framework for Dynamic Locality Control",
    "authors": [
      "Joachim Diederich"
    ],
    "abstract": "We present a novel framework for training large language models with continuously adjustable internal representations that span the full spectrum from localist (interpretable, rule-based) to distributed (generalizable, efficient) encodings. The key innovation is a locality dial, a tunable parameter that dynamically controls the degree of localization during both training and inference without requiring model retraining. This is achieved through group sparsity penalties on attention mechanisms, information-theoretic anchor design, and dynamic rule injection. We provide rigorous mathematical proofs establishing explicit threshold conditions under which attention provably concentrates on semantically relevant blocks, with exponential bounds on attention entropy and pointer fidelity. Specifically, we prove that when group sparsity penalties exceed certain threshold values, the model's attention mechanisms concentrate on semantically relevant blocks, achieving low entropy and high fidelity with negligible error. This framework enables practitioners to continuously interpolate between interpretable and high-performance modes, supporting applications in regulated domains requiring both transparency and capability.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09338v2",
    "published_date": "2025-10-10 12:44:59 UTC",
    "updated_date": "2025-11-03 09:05:41 UTC"
  },
  {
    "arxiv_id": "2510.09332v1",
    "title": "FLRC: Fine-grained Low-Rank Compressor for Efficient LLM Inference",
    "authors": [
      "Yu-Chen Lu",
      "Chong-Yan Chen",
      "Chi-Chih Chang",
      "Yu-Fang Hu",
      "Kai-Chiang Wu"
    ],
    "abstract": "Although large language models (LLM) have achieved remarkable performance, their enormous parameter counts hinder deployment on resource-constrained hardware. Low-rank compression can reduce both memory usage and computational demand, but applying a uniform compression ratio across all layers often leads to significant performance degradation, and previous methods perform poorly during decoding. To address these issues, we propose the Fine-grained Low-Rank Compressor (FLRC), which efficiently determines an optimal rank allocation for each layer, and incorporates progressive low-rank decoding to maintain text generation quality. Comprehensive experiments on diverse benchmarks demonstrate the superiority of FLRC, achieving up to a 17% improvement in ROUGE-L on summarization tasks compared to state-of-the-art low-rank compression methods, establishing a more robust and efficient framework to improve LLM inference.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by EMNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.09332v1",
    "published_date": "2025-10-10 12:35:09 UTC",
    "updated_date": "2025-10-10 12:35:09 UTC"
  },
  {
    "arxiv_id": "2510.09328v1",
    "title": "Randomized HyperSteiner: A Stochastic Delaunay Triangulation Heuristic for the Hyperbolic Steiner Minimal Tree",
    "authors": [
      "Aniss Aiman Medbouhi",
      "Alejandro García-Castellanos",
      "Giovanni Luca Marchetti",
      "Daniel Pelt",
      "Erik J Bekkers",
      "Danica Kragic"
    ],
    "abstract": "We study the problem of constructing Steiner Minimal Trees (SMTs) in hyperbolic space. Exact SMT computation is NP-hard, and existing hyperbolic heuristics such as HyperSteiner are deterministic and often get trapped in locally suboptimal configurations. We introduce Randomized HyperSteiner (RHS), a stochastic Delaunay triangulation heuristic that incorporates randomness into the expansion process and refines candidate trees via Riemannian gradient descent optimization. Experiments on synthetic data sets and a real-world single-cell transcriptomic data show that RHS outperforms Minimum Spanning Tree (MST), Neighbour Joining, and vanilla HyperSteiner (HS). In near-boundary configurations, RHS can achieve a 32% reduction in total length over HS, demonstrating its effectiveness and robustness in diverse data regimes.",
    "categories": [
      "cs.CG",
      "cs.AI"
    ],
    "primary_category": "cs.CG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09328v1",
    "published_date": "2025-10-10 12:31:55 UTC",
    "updated_date": "2025-10-10 12:31:55 UTC"
  },
  {
    "arxiv_id": "2510.09325v1",
    "title": "Rate optimal learning of equilibria from data",
    "authors": [
      "Till Freihaut",
      "Luca Viano",
      "Emanuele Nevali",
      "Volkan Cevher",
      "Matthieu Geist",
      "Giorgia Ramponi"
    ],
    "abstract": "We close open theoretical gaps in Multi-Agent Imitation Learning (MAIL) by characterizing the limits of non-interactive MAIL and presenting the first interactive algorithm with near-optimal sample complexity. In the non-interactive setting, we prove a statistical lower bound that identifies the all-policy deviation concentrability coefficient as the fundamental complexity measure, and we show that Behavior Cloning (BC) is rate-optimal. For the interactive setting, we introduce a framework that combines reward-free reinforcement learning with interactive MAIL and instantiate it with an algorithm, MAIL-WARM. It improves the best previously known sample complexity from $\\mathcal{O}(\\varepsilon^{-8})$ to $\\mathcal{O}(\\varepsilon^{-2}),$ matching the dependence on $\\varepsilon$ implied by our lower bound. Finally, we provide numerical results that support our theory and illustrate, in environments such as grid worlds, where Behavior Cloning fails to learn.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09325v1",
    "published_date": "2025-10-10 12:28:35 UTC",
    "updated_date": "2025-10-10 12:28:35 UTC"
  },
  {
    "arxiv_id": "2510.13832v1",
    "title": "Entropy Meets Importance: A Unified Head Importance-Entropy Score for Stable and Efficient Transformer Pruning",
    "authors": [
      "Minsik Choi",
      "Hyegang Son",
      "Changhoon Kim",
      "Young Geun Kim"
    ],
    "abstract": "Transformer-based models have achieved remarkable performance in NLP tasks. However, their structural characteristics-multiple layers and attention heads-introduce efficiency challenges in inference and deployment. To address these challenges, various pruning methods have recently been proposed. Notably, gradient-based methods using Head Importance Scores (HIS) have gained traction for interpretability, efficiency, and ability to identify redundant heads. However, HIS alone has limitations as it captures only the gradient-driven contribution, overlooking the diversity of attention patterns. To overcome these limitations, we introduce a novel pruning criterion, HIES (Head Importance-Entropy Score), which integrates head importance scores with attention entropy, providing complementary evidence on per-head contribution. Empirically, HIES-based pruning yields up to 15.2% improvement in model quality and 2.04x improvement in stability over HIS-only methods, enabling substantial model compression without sacrificing either accuracy or stability. Code will be released upon publication.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "32 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.13832v1",
    "published_date": "2025-10-10 12:08:20 UTC",
    "updated_date": "2025-10-10 12:08:20 UTC"
  },
  {
    "arxiv_id": "2510.09312v1",
    "title": "Verifying Chain-of-Thought Reasoning via Its Computational Graph",
    "authors": [
      "Zheng Zhao",
      "Yeskendir Koishekenov",
      "Xianjun Yang",
      "Naila Murray",
      "Nicola Cancedda"
    ],
    "abstract": "Current Chain-of-Thought (CoT) verification methods predict reasoning correctness based on outputs (black-box) or activations (gray-box), but offer limited insight into why a computation fails. We introduce a white-box method: Circuit-based Reasoning Verification (CRV). We hypothesize that attribution graphs of correct CoT steps, viewed as execution traces of the model's latent reasoning circuits, possess distinct structural fingerprints from those of incorrect steps. By training a classifier on structural features of these graphs, we show that these traces contain a powerful signal of reasoning errors. Our white-box approach yields novel scientific insights unattainable by other methods. (1) We demonstrate that structural signatures of error are highly predictive, establishing the viability of verifying reasoning directly via its computational graph. (2) We find these signatures to be highly domain-specific, revealing that failures in different reasoning tasks manifest as distinct computational patterns. (3) We provide evidence that these signatures are not merely correlational; by using our analysis to guide targeted interventions on individual transcoder features, we successfully correct the model's faulty reasoning. Our work shows that, by scrutinizing a model's computational process, we can move from simple error detection to a deeper, causal understanding of LLM reasoning.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09312v1",
    "published_date": "2025-10-10 12:06:04 UTC",
    "updated_date": "2025-10-10 12:06:04 UTC"
  },
  {
    "arxiv_id": "2510.09308v1",
    "title": "A Model-Driven Engineering Approach to AI-Powered Healthcare Platforms",
    "authors": [
      "Mira Raheem",
      "Amal Elgammal",
      "Michael Papazoglou",
      "Bernd Krämer",
      "Neamat El-Tazi"
    ],
    "abstract": "Artificial intelligence (AI) has the potential to transform healthcare by supporting more accurate diagnoses and personalized treatments. However, its adoption in practice remains constrained by fragmented data sources, strict privacy rules, and the technical complexity of building reliable clinical systems. To address these challenges, we introduce a model driven engineering (MDE) framework designed specifically for healthcare AI. The framework relies on formal metamodels, domain-specific languages (DSLs), and automated transformations to move from high level specifications to running software. At its core is the Medical Interoperability Language (MILA), a graphical DSL that enables clinicians and data scientists to define queries and machine learning pipelines using shared ontologies. When combined with a federated learning architecture, MILA allows institutions to collaborate without exchanging raw patient data, ensuring semantic consistency across sites while preserving privacy. We evaluate this approach in a multi center cancer immunotherapy study. The generated pipelines delivered strong predictive performance, with support vector machines achieving up to 98.5 percent and 98.3 percent accuracy in key tasks, while substantially reducing manual coding effort. These findings suggest that MDE principles metamodeling, semantic integration, and automated code generation can provide a practical path toward interoperable, reproducible, and trustworthy digital health platforms.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "Disclaimer: This manuscript is currently under review at * MDPI Informatics*",
    "pdf_url": "https://arxiv.org/pdf/2510.09308v1",
    "published_date": "2025-10-10 12:00:12 UTC",
    "updated_date": "2025-10-10 12:00:12 UTC"
  },
  {
    "arxiv_id": "2510.09302v1",
    "title": "CapGeo: A Caption-Assisted Approach to Geometric Reasoning",
    "authors": [
      "Yuying Li",
      "Siyi Qian",
      "Hao Liang",
      "Leqi Zheng",
      "Ruichuan An",
      "Yongzhen Guo",
      "Wentao Zhang"
    ],
    "abstract": "Geometric reasoning remains a core challenge for Multimodal Large Language Models (MLLMs). Even the most advanced closed-source systems, such as GPT-O3 and Gemini-2.5-Pro, still struggle to solve geometry problems reliably, despite exhibiting strong textual reasoning abilities on tasks like the International Mathematical Olympiad (IMO). This gap suggests that the bottleneck lies in understanding geometric diagrams rather than reasoning itself. Since geometric figures can often be faithfully described in concise textual form, converting visual content into captions offers a promising direction. Motivated by this insight, we introduce CapGeo, a caption-assisted reasoning framework that bridges visual and textual modalities. Experiments show substantial improvements when models are equipped with captions: Qwen2.5-VL-72B improves from 8.6% (vision-only) to 59.0%, while Claude-Opus-4 rises from 44.8% to 73.0%. To systematically evaluate and identify high-quality geometric captioning models, we further propose CapGeo-Bench, a dataset of 4,641 curated figure-caption pairs. Crucially, CapGeo-Bench incorporates a keypoint-based evaluation metric that correlates strongly with downstream CapGeo performance, enabling reliable assessment of geometric captioning ability. Together, our framework and benchmark highlight a new pathway toward advancing geometric reasoning in MLLMs.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "preprint, under review",
    "pdf_url": "https://arxiv.org/pdf/2510.09302v1",
    "published_date": "2025-10-10 11:47:54 UTC",
    "updated_date": "2025-10-10 11:47:54 UTC"
  },
  {
    "arxiv_id": "2510.09278v1",
    "title": "CLARity: Reasoning Consistency Alone Can Teach Reinforced Experts",
    "authors": [
      "Jiuheng Lin",
      "Cong Jiang",
      "Zirui Wu",
      "Jiarui Sun",
      "Yansong Feng"
    ],
    "abstract": "Training expert LLMs in domains with scarce data is difficult, often relying on multiple-choice questions (MCQs). However, standard outcome-based reinforcement learning (RL) on MCQs is risky. While it may improve accuracy, we observe it often degrades reasoning quality such as logical consistency. Existing solutions to supervise reasoning, such as large-scale Process Reward Models (PRMs), are prohibitively expensive. To address this, we propose CLARity, a cost-effective RL framework that enhances reasoning quality using only a small, general-purpose LLM. CLARity integrates a consistency-aware reward mechanism with a 2-stage refine-then-monitor training pipeline to enhance reasoning consistency, and a dynamic data reformulation strategy to to better exploit limited data. Experiments demonstrate that CLARity improves response consistency by 16.5% and accuracy by 7.5% over baselines. Human evaluations further confirm holistic improvements in coherence and professionalism. Thus, CLARity offers a generalizable solution that enables smaller models to effectively guide expert models by reasoning consistency.Our code is open sourced at: https://github.com/Infinite-set/CLARity",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09278v1",
    "published_date": "2025-10-10 11:21:09 UTC",
    "updated_date": "2025-10-10 11:21:09 UTC"
  },
  {
    "arxiv_id": "2510.09275v1",
    "title": "Inflated Excellence or True Performance? Rethinking Medical Diagnostic Benchmarks with Dynamic Evaluation",
    "authors": [
      "Xiangxu Zhang",
      "Lei Li",
      "Yanyun Zhou",
      "Xiao Zhou",
      "Yingying Zhang",
      "Xian Wu"
    ],
    "abstract": "Medical diagnostics is a high-stakes and complex domain that is critical to patient care. However, current evaluations of large language models (LLMs) are fundamentally misaligned with real-world clinical practice. Most of them rely on static benchmarks derived from public medical exam items, which tend to overestimate model performance and ignore the difference between textbook cases and the ambiguous, varying conditions in the real world. Recent efforts toward dynamic evaluation offer a promising alternative, but their improvements are limited to superficial perturbations and a narrow focus on accuracy. To address these gaps, we propose DyReMe, a dynamic benchmark for medical diagnostics that better reflects real clinical practice. Unlike static exam-style questions, DyReMe generates fresh, consultation-like cases that introduce distractors such as differential diagnoses and common misdiagnosis factors. It also varies expression styles to mimic diverse real-world query habits. Beyond accuracy, DyReMe evaluates LLMs on three additional clinically relevant dimensions: veracity, helpfulness, and consistency. Our experiments demonstrate that this dynamic approach yields more challenging and realistic assessments, revealing significant misalignments between the performance of state-of-the-art LLMs and real clinical practice. These findings highlight the urgent need for evaluation frameworks that better reflect the demands of trustworthy medical diagnostics.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09275v1",
    "published_date": "2025-10-10 11:19:04 UTC",
    "updated_date": "2025-10-10 11:19:04 UTC"
  },
  {
    "arxiv_id": "2510.09263v1",
    "title": "SynthID-Image: Image watermarking at internet scale",
    "authors": [
      "Sven Gowal",
      "Rudy Bunel",
      "Florian Stimberg",
      "David Stutz",
      "Guillermo Ortiz-Jimenez",
      "Christina Kouridi",
      "Mel Vecerik",
      "Jamie Hayes",
      "Sylvestre-Alvise Rebuffi",
      "Paul Bernard",
      "Chris Gamble",
      "Miklós Z. Horváth",
      "Fabian Kaczmarczyck",
      "Alex Kaskasoli",
      "Aleksandar Petrov",
      "Ilia Shumailov",
      "Meghana Thotakuri",
      "Olivia Wiles",
      "Jessica Yung",
      "Zahra Ahmed",
      "Victor Martin",
      "Simon Rosen",
      "Christopher Savčak",
      "Armin Senoner",
      "Nidhi Vyas",
      "Pushmeet Kohli"
    ],
    "abstract": "We introduce SynthID-Image, a deep learning-based system for invisibly watermarking AI-generated imagery. This paper documents the technical desiderata, threat models, and practical challenges of deploying such a system at internet scale, addressing key requirements of effectiveness, fidelity, robustness, and security. SynthID-Image has been used to watermark over ten billion images and video frames across Google's services and its corresponding verification service is available to trusted testers. For completeness, we present an experimental evaluation of an external model variant, SynthID-O, which is available through partnerships. We benchmark SynthID-O against other post-hoc watermarking methods from the literature, demonstrating state-of-the-art performance in both visual quality and robustness to common image perturbations. While this work centers on visual media, the conclusions on deployment, constraints, and threat modeling generalize to other modalities, including audio. This paper provides a comprehensive documentation for the large-scale deployment of deep learning-based media provenance systems.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09263v1",
    "published_date": "2025-10-10 11:03:31 UTC",
    "updated_date": "2025-10-10 11:03:31 UTC"
  },
  {
    "arxiv_id": "2510.09259v1",
    "title": "Detecting Data Contamination from Reinforcement Learning Post-training for Large Language Models",
    "authors": [
      "Yongding Tao",
      "Tian Wang",
      "Yihong Dong",
      "Huanyu Liu",
      "Kechi Zhang",
      "Xiaolong Hu",
      "Ge Li"
    ],
    "abstract": "Data contamination poses a significant threat to the reliable evaluation of Large Language Models (LLMs). This issue arises when benchmark samples may inadvertently appear in training sets, compromising the validity of reported performance. While detection methods have been developed for the pre-training and Supervised Fine-Tuning stages, a critical research gap exists for the increasingly significant phase of Reinforcement Learning (RL) post-training. As RL post-training becomes pivotal for advancing LLM reasoning, the absence of specialized contamination detection methods in this paradigm presents a critical vulnerability. To address this, we conduct the first systematic study of data detection within RL post-training scenario and propose Self-Critique. Our method is motivated by a key observation: after RL phase, the output entropy distribution of LLMs tends to collapse into highly specific and sparse modes. Self-Critique probes for the underlying policy collapse, i.e., the model's convergence to a narrow reasoning path, which causes this entropy reduction. To facilitate this research, we also introduce RL-MIA, a benchmark constructed to simulate this specific contamination scenario. Extensive experiments show that Self-Critique significantly outperforms baseline methods across multiple models and contamination tasks, achieving an AUC improvement of up to 30%. Whereas existing methods are close to a random guess for RL-phase contamination, our method makes detection possible.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09259v1",
    "published_date": "2025-10-10 10:58:50 UTC",
    "updated_date": "2025-10-10 10:58:50 UTC"
  },
  {
    "arxiv_id": "2511.20656v1",
    "title": "Context-Aware Visual Prompting: Automating Geospatial Web Dashboards with Large Language Models and Agent Self-Validation for Decision Support",
    "authors": [
      "Haowen Xu",
      "Jose Tupayachi",
      "Xiao-Ying Yu"
    ],
    "abstract": "The development of web-based geospatial dashboards for risk analysis and decision support is often challenged by the difficulty in visualization of big, multi-dimensional environmental data, implementation complexity, and limited automation. We introduce a generative AI framework that harnesses Large Language Models (LLMs) to automate the creation of interactive geospatial dashboards from user-defined inputs including UI wireframes, requirements, and data sources. By incorporating a structured knowledge graph, the workflow embeds domain knowledge into the generation process and enable accurate and context-aware code completions. A key component of our approach is the Context-Aware Visual Prompting (CAVP) mechanism, which extracts encodes and interface semantics from visual layouts to guide LLM driven generation of codes. The new framework also integrates a self-validation mechanism that uses an agent-based LLM and Pass@k evaluation alongside semantic metrics to assure output reliability. Dashboard snippets are paired with data visualization codebases and ontological representations, enabling a pipeline that produces scalable React-based completions using the MVVM architectural pattern. Our results demonstrate improved performance over baseline approaches and expanded functionality over third party platforms, while incorporating multi-page, fully functional interfaces. We successfully developed a framework to implement LLMs, demonstrated the pipeline for automated code generation, deployment, and performed chain-of-thought AI agents in self-validation. This integrative approach is guided by structured knowledge and visual prompts, providing an innovative geospatial solution in enhancing risk analysis and decision making.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.20656v1",
    "published_date": "2025-10-10 10:58:15 UTC",
    "updated_date": "2025-10-10 10:58:15 UTC"
  },
  {
    "arxiv_id": "2510.09254v1",
    "title": "Obstacle Avoidance using Dynamic Movement Primitives and Reinforcement Learning",
    "authors": [
      "Dominik Urbaniak",
      "Alejandro Agostini",
      "Pol Ramon",
      "Jan Rosell",
      "Raúl Suárez",
      "Michael Suppa"
    ],
    "abstract": "Learning-based motion planning can quickly generate near-optimal trajectories. However, it often requires either large training datasets or costly collection of human demonstrations. This work proposes an alternative approach that quickly generates smooth, near-optimal collision-free 3D Cartesian trajectories from a single artificial demonstration. The demonstration is encoded as a Dynamic Movement Primitive (DMP) and iteratively reshaped using policy-based reinforcement learning to create a diverse trajectory dataset for varying obstacle configurations. This dataset is used to train a neural network that takes as inputs the task parameters describing the obstacle dimensions and location, derived automatically from a point cloud, and outputs the DMP parameters that generate the trajectory. The approach is validated in simulation and real-robot experiments, outperforming a RRT-Connect baseline in terms of computation and execution time, as well as trajectory length, while supporting multi-modal trajectory generation for different obstacle geometries and end-effector dimensions. Videos and the implementation code are available at https://github.com/DominikUrbaniak/obst-avoid-dmp-pi2.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages, 7 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.09254v1",
    "published_date": "2025-10-10 10:51:42 UTC",
    "updated_date": "2025-10-10 10:51:42 UTC"
  },
  {
    "arxiv_id": "2510.15948v1",
    "title": "VisuoAlign: Safety Alignment of LVLMs with Multimodal Tree Search",
    "authors": [
      "MingSheng Li",
      "Guangze Zhao",
      "Sichen Liu"
    ],
    "abstract": "Large Vision-Language Models (LVLMs) have achieved remarkable progress in multimodal perception and generation, yet their safety alignment remains a critical challenge.Existing defenses and vulnerable to multimodal jailbreaks, as visual inputs introduce new attack surfaces, reasoning chains lack safety supervision, and alignment often degrades under modality fusion.To overcome these limitation, we propose VisuoAlign, a framework for multi-modal safety alignment via prompt-guided tree search.VisuoAlign embeds safety constrains into the reasoning process through visual-textual interactive prompts, employs Monte Carlo Tree Search(MCTS) to systematically construct diverse safety-critical prompt trajectories, and introduces prompt-based scaling to ensure real-time risk detection and compliant responses.Extensive experiments demonstrate that VisuoAlign proactively exposes risks, enables comprehensive dataset generation, and significantly improves the robustness of LVLMs against complex cross-modal threats.",
    "categories": [
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15948v1",
    "published_date": "2025-10-10 10:46:58 UTC",
    "updated_date": "2025-10-10 10:46:58 UTC"
  },
  {
    "arxiv_id": "2510.09244v1",
    "title": "Fundamentals of Building Autonomous LLM Agents",
    "authors": [
      "Victor de Lamo Castrillo",
      "Habtom Kahsay Gidey",
      "Alexander Lenz",
      "Alois Knoll"
    ],
    "abstract": "This paper reviews the architecture and implementation methods of agents powered by large language models (LLMs). Motivated by the limitations of traditional LLMs in real-world tasks, the research aims to explore patterns to develop \"agentic\" LLMs that can automate complex tasks and bridge the performance gap with human capabilities. Key components include a perception system that converts environmental percepts into meaningful representations; a reasoning system that formulates plans, adapts to feedback, and evaluates actions through different techniques like Chain-of-Thought and Tree-of-Thought; a memory system that retains knowledge through both short-term and long-term mechanisms; and an execution system that translates internal decisions into concrete actions. This paper shows how integrating these systems leads to more capable and generalized software bots that mimic human cognitive processes for autonomous and intelligent behavior.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09244v1",
    "published_date": "2025-10-10 10:32:39 UTC",
    "updated_date": "2025-10-10 10:32:39 UTC"
  },
  {
    "arxiv_id": "2510.09243v2",
    "title": "CrisiText: A dataset of warning messages for LLM training in emergency communication",
    "authors": [
      "Giacomo Gonella",
      "Gian Maria Campedelli",
      "Stefano Menini",
      "Marco Guerini"
    ],
    "abstract": "Effectively identifying threats and mitigating their potential damage during crisis situations, such as natural disasters or violent attacks, is paramount for safeguarding endangered individuals. To tackle these challenges, AI has been used in assisting humans in emergency situations. Still, the use of NLP techniques remains limited and mostly focuses on classification tasks. The significant potential of timely warning message generation using NLG architectures, however, has been largely overlooked. In this paper we present CrisiText, the first large-scale dataset for the generation of warning messages across 13 different types of crisis scenarios. The dataset contains more than 400,000 warning messages (spanning almost 18,000 crisis situations) aimed at assisting civilians during and after such events. To generate the dataset, we started from existing crisis descriptions and created chains of events related to the scenarios. Each event was then paired with a warning message. The generations follow experts' written guidelines to ensure correct terminology and factuality of their suggestions. Additionally, each message is accompanied by three suboptimal warning types to allow for the study of different NLG approaches. To this end, we conducted a series of experiments comparing supervised fine-tuning setups with preference alignment, zero-shot, and few-shot approaches. We further assessed model performance in out-of-distribution scenarios and evaluated the effectiveness of an automatic post-editor.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09243v2",
    "published_date": "2025-10-10 10:32:11 UTC",
    "updated_date": "2025-10-13 18:43:09 UTC"
  },
  {
    "arxiv_id": "2510.11736v1",
    "title": "AI Agents for the Dhumbal Card Game: A Comparative Study",
    "authors": [
      "Sahaj Raj Malla"
    ],
    "abstract": "This study evaluates Artificial Intelligence (AI) agents for Dhumbal, a culturally significant multiplayer card game with imperfect information, through a systematic comparison of rule-based, search-based, and learning-based strategies. We formalize Dhumbal's mechanics and implement diverse agents, including heuristic approaches (Aggressive, Conservative, Balanced, Opportunistic), search-based methods such as Monte Carlo Tree Search (MCTS) and Information Set Monte Carlo Tree Search (ISMCTS), and reinforcement learning approaches including Deep Q-Network (DQN) and Proximal Policy Optimization (PPO), and a random baseline. Evaluation involves within-category tournaments followed by a cross-category championship. Performance is measured via win rate, economic outcome, Jhyap success, cards discarded per round, risk assessment, and decision efficiency. Statistical significance is assessed using Welch's t-test with Bonferroni correction, effect sizes via Cohen's d, and 95% confidence intervals (CI). Across 1024 simulated rounds, the rule-based Aggressive agent achieves the highest win rate (88.3%, 95% CI: [86.3, 90.3]), outperforming ISMCTS (9.0%) and PPO (1.5%) through effective exploitation of Jhyap declarations. The study contributes a reproducible AI framework, insights into heuristic efficacy under partial information, and open-source code, thereby advancing AI research and supporting digital preservation of cultural games.",
    "categories": [
      "cs.AI",
      "cs.GT",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 7 figures, 6 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.11736v1",
    "published_date": "2025-10-10 10:31:19 UTC",
    "updated_date": "2025-10-10 10:31:19 UTC"
  },
  {
    "arxiv_id": "2510.09230v1",
    "title": "Diagnosing Shoulder Disorders Using Multimodal Large Language Models and Consumer-Grade Cameras",
    "authors": [
      "Jindong Hong",
      "Wencheng Zhang",
      "Shiqin Qiao",
      "Jianhai Chen",
      "Jianing Qiu",
      "Chuanyang Zheng",
      "Qian Xu",
      "Yun Ji",
      "Qianyue Wen",
      "Weiwei Sun",
      "Hao Li",
      "Huizhen Li",
      "Huichao Wang",
      "Kai Wu",
      "Meng Li",
      "Yijun He",
      "Lingjie Luo",
      "Jiankai Sun"
    ],
    "abstract": "Shoulder disorders, such as frozen shoulder (a.k.a., adhesive capsulitis), are common conditions affecting the health of people worldwide, and have a high incidence rate among the elderly and workers engaged in repetitive shoulder tasks. In regions with scarce medical resources, achieving early and accurate diagnosis poses significant challenges, and there is an urgent need for low-cost and easily scalable auxiliary diagnostic solutions. This research introduces videos captured by consumer-grade devices as the basis for diagnosis, reducing the cost for users. We focus on the innovative application of Multimodal Large Language Models (MLLMs) in the preliminary diagnosis of shoulder disorders and propose a Hybrid Motion Video Diagnosis framework (HMVDx). This framework divides the two tasks of action understanding and disease diagnosis, which are respectively completed by two MLLMs. In addition to traditional evaluation indicators, this work proposes a novel metric called Usability Index by the logical process of medical decision-making (action recognition, movement diagnosis, and final diagnosis). This index evaluates the effectiveness of MLLMs in the medical field from the perspective of the entire medical diagnostic pathway, revealing the potential value of low-cost MLLMs in medical applications for medical practitioners. In experimental comparisons, the accuracy of HMVDx in diagnosing shoulder joint injuries has increased by 79.6\\% compared with direct video diagnosis, a significant technical contribution to future research on the application of MLLMs for video understanding in the medical field.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09230v1",
    "published_date": "2025-10-10 10:17:23 UTC",
    "updated_date": "2025-10-10 10:17:23 UTC"
  },
  {
    "arxiv_id": "2510.09228v1",
    "title": "Clear Roads, Clear Vision: Advancements in Multi-Weather Restoration for Smart Transportation",
    "authors": [
      "Vijay M. Galshetwar",
      "Praful Hambarde",
      "Prashant W. Patil",
      "Akshay Dudhane",
      "Sachin Chaudhary",
      "Santosh Kumar Vipparathi",
      "Subrahmanyam Murala"
    ],
    "abstract": "Adverse weather conditions such as haze, rain, and snow significantly degrade the quality of images and videos, posing serious challenges to intelligent transportation systems (ITS) that rely on visual input. These degradations affect critical applications including autonomous driving, traffic monitoring, and surveillance. This survey presents a comprehensive review of image and video restoration techniques developed to mitigate weather-induced visual impairments. We categorize existing approaches into traditional prior-based methods and modern data-driven models, including CNNs, transformers, diffusion models, and emerging vision-language models (VLMs). Restoration strategies are further classified based on their scope: single-task models, multi-task/multi-weather systems, and all-in-one frameworks capable of handling diverse degradations. In addition, we discuss day and night time restoration challenges, benchmark datasets, and evaluation protocols. The survey concludes with an in-depth discussion on limitations in current research and outlines future directions such as mixed/compound-degradation restoration, real-time deployment, and agentic AI frameworks. This work aims to serve as a valuable reference for advancing weather-resilient vision systems in smart transportation environments. Lastly, to stay current with rapid advancements in this field, we will maintain regular updates of the latest relevant papers and their open-source implementations at https://github.com/ChaudharyUPES/A-comprehensive-review-on-Multi-weather-restoration",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This work has been submitted to IEEE for possible publication",
    "pdf_url": "https://arxiv.org/pdf/2510.09228v1",
    "published_date": "2025-10-10 10:15:59 UTC",
    "updated_date": "2025-10-10 10:15:59 UTC"
  },
  {
    "arxiv_id": "2510.09227v1",
    "title": "RegexPSPACE: A Benchmark for Evaluating LLM Reasoning on PSPACE-complete Regex Problems",
    "authors": [
      "Hyundong Jin",
      "Joonghyuk Hahn",
      "Yo-Sub Han"
    ],
    "abstract": "Large language models (LLMs) show strong performance across natural language processing (NLP), mathematical reasoning, and programming, and recent large reasoning models (LRMs) further emphasize explicit reasoning. Yet their computational limits, particularly spatial complexity constrained by finite context windows, remain poorly understood. While recent works often focus on problems within the NP complexity class, we push the boundary by introducing a novel benchmark grounded in two PSPACE-complete regular expression (regex) problems: equivalence decision (RegexEQ) and minimization (RegexMin). PSPACE-complete problems serve as a more rigorous standard for assessing computational capacity, as their solutions require massive search space exploration. We perform a double-exponential space exploration to construct a labeled dataset of over a million regex instances with a sound filtering process to build the benchmark. We conduct extensive evaluations on 6 LLMs and 5 LRMs of varying scales, revealing common failure patterns such as verbosity and repetition. With its well-defined structure and quantitative evaluation metrics, this work presents the first empirical investigation into the spatial computational limitations of LLMs and LRMs, offering a new framework for evaluating their advanced reasoning capabilities. Our code is available at https://github.com/hyundong98/RegexPSPACE .",
    "categories": [
      "cs.AI",
      "cs.FL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09227v1",
    "published_date": "2025-10-10 10:13:47 UTC",
    "updated_date": "2025-10-10 10:13:47 UTC"
  },
  {
    "arxiv_id": "2510.09223v1",
    "title": "Comparing Knowledge Source Integration Methods for Optimizing Healthcare Knowledge Fusion in Rescue Operation",
    "authors": [
      "Mubaris Nadeem",
      "Madjid Fathi"
    ],
    "abstract": "In the field of medicine and healthcare, the utilization of medical expertise, based on medical knowledge combined with patients' health information is a life-critical challenge for patients and health professionals. The within-laying complexity and variety form the need for a united approach to gather, analyze, and utilize existing knowledge of medical treatments, and medical operations to provide the ability to present knowledge for the means of accurate patient-driven decision-making. One way to achieve this is the fusion of multiple knowledge sources in healthcare. It provides health professionals the opportunity to select from multiple contextual aligned knowledge sources which enables the support for critical decisions. This paper presents multiple conceptual models for knowledge fusion in the field of medicine, based on a knowledge graph structure. It will evaluate, how knowledge fusion can be enabled and presents how to integrate various knowledge sources into the knowledge graph for rescue operations.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Conference Paper for 2024 IEEE 7th International Conference on Industrial Cyber-Physical Systems (ICPS), KIRETT Project, University of Siegen, Germany",
    "pdf_url": "https://arxiv.org/pdf/2510.09223v1",
    "published_date": "2025-10-10 10:10:14 UTC",
    "updated_date": "2025-10-10 10:10:14 UTC"
  },
  {
    "arxiv_id": "2510.13831v1",
    "title": "Informed Routing in LLMs: Smarter Token-Level Computation for Faster Inference",
    "authors": [
      "Chao Han",
      "Yijuan Liang",
      "Zihao Xuan",
      "Daokuan Wu",
      "Wei Zhang",
      "Xiaoyu Shen"
    ],
    "abstract": "The deployment of large language models (LLMs) in real-world applications is increasingly limited by their high inference cost. While recent advances in dynamic token-level computation allocation attempt to improve efficiency by selectively activating model components per token, existing methods rely on greedy routing--a myopic execute-or-skip mechanism that often leads to irreversible information loss and suboptimal token selection. This paper introduces informed routing, a new paradigm that proactively addresses these issues. The key insight is to assess not only a token's immediate importance but also its recoverability, i.e., how well its transformation can be approximated. To this end, we propose the Lightweight Feature Forecaster (LFF), a small predictive module that estimates a unit's output before routing decisions are made. This enables a flexible execute-or-approximate policy that preserves model fidelity while drastically reducing computation. Extensive experiments on both language modeling and reasoning tasks show that informed routing achieves state-of-the-art efficiency-performance trade-offs across multiple sparsity levels. Notably, even without final LoRA fine-tuning, our method matches or surpasses strong baselines that require full fine-tuning, all while reducing training time by over 50%. The code is available at: https://github.com/EIT-NLP/informed-routing",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.13831v1",
    "published_date": "2025-10-10 09:59:36 UTC",
    "updated_date": "2025-10-10 09:59:36 UTC"
  },
  {
    "arxiv_id": "2510.09726v1",
    "title": "Herb.jl: A Unifying Program Synthesis Library",
    "authors": [
      "Tilman Hinnerichs",
      "Reuben Gardos Reid",
      "Jaap de Jong",
      "Bart Swinkels",
      "Pamela Wochner",
      "Nicolae Filat",
      "Tudor Magurescu",
      "Issa Hanou",
      "Sebastijan Dumancic"
    ],
    "abstract": "Program synthesis -- the automatic generation of code given a specification -- is one of the most fundamental tasks in artificial intelligence (AI) and many programmers' dream. Numerous synthesizers have been developed to tackle program synthesis, manifesting different ideas to approach the exponentially growing program space. While numerous smart program synthesis tools exist, reusing and remixing previously developed methods is tedious and time-consuming. We propose Herb.jl, a unifying program synthesis library written in the Julia programming language, to address these issues. Since current methods rely on similar building blocks, we aim to modularize the underlying synthesis algorithm into communicating and fully extendable sub-compartments, allowing for straightforward reapplication of these modules. To demonstrate the benefits of using Herb.jl, we show three common use cases: 1. how to implement a simple problem and grammar, and how to solve it, 2. how to implement a previously developed synthesizer with just a few lines of code, and 3. how to run a synthesizer against a benchmark.",
    "categories": [
      "cs.PL",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.PL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09726v1",
    "published_date": "2025-10-10 09:45:36 UTC",
    "updated_date": "2025-10-10 09:45:36 UTC"
  },
  {
    "arxiv_id": "2510.09211v2",
    "title": "DICE: Structured Reasoning in LLMs through SLM-Guided Chain-of-Thought Correction",
    "authors": [
      "Yiqi Li",
      "Yusheng Liao",
      "Zhe Chen",
      "Yanfeng Wang",
      "Yu Wang"
    ],
    "abstract": "When performing reasoning tasks with user-specific requirements, such as strict output formats, large language models (LLMs) often prioritize reasoning over adherence to detailed instructions. Fine-tuning LLMs on supervised datasets to address this is impractical due to high computational costs and limited parameter access. To tackle this, we propose DICE, a lightweight framework that guides small language models (SLMs) to refine LLMs' outputs through chain-of-thought (CoT) correction. DICE decouples the process by first prompting LLMs to generate natural language responses, then using trained SLMs to analyze and refine these outputs to meet structured output specifications. This framework preserves LLMs' broad knowledge and reasoning capabilities while ensuring the outputs conform to user demands. Specifically, DICE first constructs structured CoT adaptation datasets via a two-stage method and subsequently applies a dual-tuning strategy to fine-tune SLMs for generating structured outputs in an analyze-then-answer pattern. Experiments demonstrate that DICE improves the average format accuracy and content correctness of LLM outputs by 35.4\\% and 29.4\\%, respectively, achieving state-of-the-art (SOTA) performance over other competitive baselines.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "This paper was accepted to the EMNLP 2025 main conference",
    "pdf_url": "https://arxiv.org/pdf/2510.09211v2",
    "published_date": "2025-10-10 09:45:35 UTC",
    "updated_date": "2025-10-20 08:57:20 UTC"
  },
  {
    "arxiv_id": "2510.09201v1",
    "title": "Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for MLLMs",
    "authors": [
      "Yumin Choi",
      "Dongki Kim",
      "Jinheon Baek",
      "Sung Ju Hwang"
    ],
    "abstract": "Large Language Models (LLMs) have shown remarkable success, and their multimodal expansions (MLLMs) further unlock capabilities spanning images, videos, and other modalities beyond text. However, despite this shift, prompt optimization approaches, designed to reduce the burden of manual prompt crafting while maximizing performance, remain confined to text, ultimately limiting the full potential of MLLMs. Motivated by this gap, we introduce the new problem of multimodal prompt optimization, which expands the prior definition of prompt optimization to the multimodal space defined by the pairs of textual and non-textual prompts. To tackle this problem, we then propose the Multimodal Prompt Optimizer (MPO), a unified framework that not only performs the joint optimization of multimodal prompts through alignment-preserving updates but also guides the selection process of candidate prompts by leveraging earlier evaluations as priors in a Bayesian-based selection strategy. Through extensive experiments across diverse modalities that go beyond text, such as images, videos, and even molecules, we demonstrate that MPO outperforms leading text-only optimization methods, establishing multimodal prompt optimization as a crucial step to realizing the potential of MLLMs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09201v1",
    "published_date": "2025-10-10 09:41:25 UTC",
    "updated_date": "2025-10-10 09:41:25 UTC"
  },
  {
    "arxiv_id": "2510.09200v1",
    "title": "Towards Safer and Understandable Driver Intention Prediction",
    "authors": [
      "Mukilan Karuppasamy",
      "Shankar Gangisetty",
      "Shyam Nandan Rai",
      "Carlo Masone",
      "C V Jawahar"
    ],
    "abstract": "Autonomous driving (AD) systems are becoming increasingly capable of handling complex tasks, mainly due to recent advances in deep learning and AI. As interactions between autonomous systems and humans increase, the interpretability of decision-making processes in driving systems becomes increasingly crucial for ensuring safe driving operations. Successful human-machine interaction requires understanding the underlying representations of the environment and the driving task, which remains a significant challenge in deep learning-based systems. To address this, we introduce the task of interpretability in maneuver prediction before they occur for driver safety, i.e., driver intent prediction (DIP), which plays a critical role in AD systems. To foster research in interpretable DIP, we curate the eXplainable Driving Action Anticipation Dataset (DAAD-X), a new multimodal, ego-centric video dataset to provide hierarchical, high-level textual explanations as causal reasoning for the driver's decisions. These explanations are derived from both the driver's eye-gaze and the ego-vehicle's perspective. Next, we propose Video Concept Bottleneck Model (VCBM), a framework that generates spatio-temporally coherent explanations inherently, without relying on post-hoc techniques. Finally, through extensive evaluations of the proposed VCBM on the DAAD-X dataset, we demonstrate that transformer-based models exhibit greater interpretability than conventional CNN-based models. Additionally, we introduce a multilabel t-SNE visualization technique to illustrate the disentanglement and causal correlation among multiple explanations. Our data, code and models are available at: https://mukil07.github.io/VCBM.github.io/",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.09200v1",
    "published_date": "2025-10-10 09:41:25 UTC",
    "updated_date": "2025-10-10 09:41:25 UTC"
  },
  {
    "arxiv_id": "2510.12822v2",
    "title": "Evidence Without Injustice: A New Counterfactual Test for Fair Algorithms",
    "authors": [
      "Michele Loi",
      "Marcello Di Bello",
      "Nicolò Cangiotti"
    ],
    "abstract": "The growing philosophical literature on algorithmic fairness has examined statistical criteria such as equalized odds and calibration, causal and counterfactual approaches, and the role of structural and compounding injustices. Yet an important dimension has been overlooked: whether the evidential value of an algorithmic output itself depends on structural injustice. We contrast a predictive policing algorithm, which relies on historical crime data, with a camera-based system that records ongoing offenses, where both are designed to guide police deployment. In evaluating the moral acceptability of acting on a piece of evidence, we must ask not only whether the evidence is probative in the actual world, but also whether it would remain probative in nearby worlds without the relevant injustices. The predictive policing algorithm fails this test, but the camera-based system passes it. When evidence fails the test, it is morally problematic to use it punitively, more so than evidence that passes the test.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "13 pages. Disclaimer on AI use: the authors used Claude 3.7 in the early stages of this project to help in the formulation of the Counterfactual Independence Principle presented in this paper. The model provided preliminary suggestions pertaining to the principle's articulation, case studies, and the structure of the argument. Screenshots of the initial output are on file with the authors",
    "pdf_url": "https://arxiv.org/pdf/2510.12822v2",
    "published_date": "2025-10-10 09:33:17 UTC",
    "updated_date": "2025-10-25 08:15:13 UTC"
  },
  {
    "arxiv_id": "2510.09187v1",
    "title": "Modern Deep Learning Approaches for Cricket Shot Classification: A Comprehensive Baseline Study",
    "authors": [
      "Sungwoo Kang"
    ],
    "abstract": "Cricket shot classification from video sequences remains a challenging problem in sports video analysis, requiring effective modeling of both spatial and temporal features. This paper presents the first comprehensive baseline study comparing seven different deep learning approaches across four distinct research paradigms for cricket shot classification. We implement and systematically evaluate traditional CNN-LSTM architectures, attention-based models, vision transformers, transfer learning approaches, and modern EfficientNet-GRU combinations on a unified benchmark. A critical finding of our study is the significant performance gap between claims in academic literature and practical implementation results. While previous papers reported accuracies of 96\\% (Balaji LRCN), 99.2\\% (IJERCSE), and 93\\% (Sensors), our standardized re-implementations achieve 46.0\\%, 55.6\\%, and 57.7\\% respectively. Our modern SOTA approach, combining EfficientNet-B0 with a GRU-based temporal model, achieves 92.25\\% accuracy, demonstrating that substantial improvements are possible with modern architectures and systematic optimization. All implementations follow modern MLOps practices with PyTorch Lightning, providing a reproducible research platform that exposes the critical importance of standardized evaluation protocols in sports video analysis research.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09187v1",
    "published_date": "2025-10-10 09:32:29 UTC",
    "updated_date": "2025-10-10 09:32:29 UTC"
  },
  {
    "arxiv_id": "2510.09181v1",
    "title": "On the Implicit Adversariality of Catastrophic Forgetting in Deep Continual Learning",
    "authors": [
      "Ze Peng",
      "Jian Zhang",
      "Jintao Guo",
      "Lei Qi",
      "Yang Gao",
      "Yinghuan Shi"
    ],
    "abstract": "Continual learning seeks the human-like ability to accumulate new skills in machine intelligence. Its central challenge is catastrophic forgetting, whose underlying cause has not been fully understood for deep networks. In this paper, we demystify catastrophic forgetting by revealing that the new-task training is implicitly an adversarial attack against the old-task knowledge. Specifically, the new-task gradients automatically and accurately align with the sharp directions of the old-task loss landscape, rapidly increasing the old-task loss. This adversarial alignment is intriguingly counter-intuitive because the sharp directions are too sparsely distributed to align with by chance. To understand it, we theoretically show that it arises from training's low-rank bias, which, through forward and backward propagation, confines the two directions into the same low-dimensional subspace, facilitating alignment. Gradient projection (GP) methods, a representative family of forgetting-mitigating methods, reduce adversarial alignment caused by forward propagation, but cannot address the alignment due to backward propagation. We propose backGP to address it, which reduces forgetting by 10.8% and improves accuracy by 12.7% on average over GP methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09181v1",
    "published_date": "2025-10-10 09:24:45 UTC",
    "updated_date": "2025-10-10 09:24:45 UTC"
  },
  {
    "arxiv_id": "2510.15947v2",
    "title": "WaveNet's Precision in EEG Classification",
    "authors": [
      "Casper van Laar",
      "Khubaib Ahmed"
    ],
    "abstract": "This study introduces a WaveNet-based deep learning model designed to automate the classification of intracranial electroencephalography (iEEG) signals into physiological activity, pathological (epileptic) activity, power-line noise, and other non-cerebral artifacts. Traditional methods for iEEG signal classification, which rely on expert visual review, are becoming increasingly impractical due to the growing complexity and volume of iEEG recordings. Leveraging a publicly available annotated dataset from Mayo Clinic and St. Anne's University Hospital, the WaveNet model was trained, validated, and tested on 209,231 samples using a 70/20/10 split. The model achieved a classification accuracy exceeding previous non-specialized CNN- and LSTM-based approaches and was benchmarked against a Temporal Convolutional Network (TCN) baseline. Notably, the model achieves high discrimination of noise and artifact classes, with precisions of 0.98 and approximately 1, respectively. Classification between physiological and pathological signals exhibits a modest but clinically interpretable overlap, with F1-scores of 0.96 and 0.90 and 175 and 272 cross-class false positives, respectively, reflecting inherent clinical overlap. WaveNet's architecture, originally developed for raw audio synthesis, is well-suited for iEEG data due to its use of dilated causal convolutions and residual connections, enabling the capture of both fine-grained and long-range temporal dependencies. The study also details the preprocessing pipeline, including dynamic dataset partitioning, the use of focal loss to address class imbalance, and normalization steps that support high model performance. While the results demonstrate strong in-distribution performance, generalizability across datasets and clinical settings has yet to be established.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP",
      "q-bio.NC"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages, 5 figures and 3 tables. Includes main text and bibliography",
    "pdf_url": "https://arxiv.org/pdf/2510.15947v2",
    "published_date": "2025-10-10 09:21:21 UTC",
    "updated_date": "2026-01-13 01:51:48 UTC"
  },
  {
    "arxiv_id": "2510.09162v2",
    "title": "Dr. Bias: Social Disparities in AI-Powered Medical Guidance",
    "authors": [
      "Emma Kondrup",
      "Anne Imouza"
    ],
    "abstract": "With the rapid progress of Large Language Models (LLMs), the general public now has easy and affordable access to applications capable of answering most health-related questions in a personalized manner. These LLMs are increasingly proving to be competitive, and now even surpass professionals in some medical capabilities. They hold particular promise in low-resource settings, considering they provide the possibility of widely accessible, quasi-free healthcare support. However, evaluations that fuel these motivations highly lack insights into the social nature of healthcare, oblivious to health disparities between social groups and to how bias may translate into LLM-generated medical advice and impact users. We provide an exploratory analysis of LLM answers to a series of medical questions spanning key clinical domains, where we simulate these questions being asked by several patient profiles that vary in sex, age range, and ethnicity. By comparing natural language features of the generated responses, we show that, when LLMs are used for medical advice generation, they generate responses that systematically differ between social groups. In particular, Indigenous and intersex patients receive advice that is less readable and more complex. We observe these trends amplify when intersectional groups are considered. Considering the increasing trust individuals place in these models, we argue for higher AI literacy and for the urgent need for investigation and mitigation by AI developers to ensure these systemic differences are diminished and do not translate to unjust patient support. Our code is publicly available on GitHub.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09162v2",
    "published_date": "2025-10-10 09:06:35 UTC",
    "updated_date": "2025-10-16 21:54:20 UTC"
  },
  {
    "arxiv_id": "2510.09159v1",
    "title": "Cross-Representation Benchmarking in Time-Series Electronic Health Records for Clinical Outcome Prediction",
    "authors": [
      "Tianyi Chen",
      "Mingcheng Zhu",
      "Zhiyao Luo",
      "Tingting Zhu"
    ],
    "abstract": "Electronic Health Records (EHRs) enable deep learning for clinical predictions, but the optimal method for representing patient data remains unclear due to inconsistent evaluation practices. We present the first systematic benchmark to compare EHR representation methods, including multivariate time-series, event streams, and textual event streams for LLMs. This benchmark standardises data curation and evaluation across two distinct clinical settings: the MIMIC-IV dataset for ICU tasks (mortality, phenotyping) and the EHRSHOT dataset for longitudinal care (30-day readmission, 1-year pancreatic cancer). For each paradigm, we evaluate appropriate modelling families--including Transformers, MLP, LSTMs and Retain for time-series, CLMBR and count-based models for event streams, 8-20B LLMs for textual streams--and analyse the impact of feature pruning based on data missingness. Our experiments reveal that event stream models consistently deliver the strongest performance. Pre-trained models like CLMBR are highly sample-efficient in few-shot settings, though simpler count-based models can be competitive given sufficient data. Furthermore, we find that feature selection strategies must be adapted to the clinical setting: pruning sparse features improves ICU predictions, while retaining them is critical for longitudinal tasks. Our results, enabled by a unified and reproducible pipeline, provide practical guidance for selecting EHR representations based on the clinical context and data regime.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09159v1",
    "published_date": "2025-10-10 09:03:47 UTC",
    "updated_date": "2025-10-10 09:03:47 UTC"
  },
  {
    "arxiv_id": "2510.09155v1",
    "title": "Federated Data Analytics for Cancer Immunotherapy: A Privacy-Preserving Collaborative Platform for Patient Management",
    "authors": [
      "Mira Raheem",
      "Michael Papazoglou",
      "Bernd Krämer",
      "Neamat El-Tazi",
      "Amal Elgammal"
    ],
    "abstract": "Connected health is a multidisciplinary approach focused on health management, prioritizing pa-tient needs in the creation of tools, services, and treatments. This paradigm ensures proactive and efficient care by facilitating the timely exchange of accurate patient information among all stake-holders in the care continuum. The rise of digital technologies and process innovations promises to enhance connected health by integrating various healthcare data sources. This integration aims to personalize care, predict health outcomes, and streamline patient management, though challeng-es remain, particularly in data architecture, application interoperability, and security. Data analytics can provide critical insights for informed decision-making and health co-creation, but solutions must prioritize end-users, including patients and healthcare professionals. This perspective was explored through an agile System Development Lifecycle in an EU-funded project aimed at developing an integrated AI-generated solution for managing cancer patients undergoing immunotherapy. This paper contributes with a collaborative digital framework integrating stakeholders across the care continuum, leveraging federated big data analytics and artificial intelligence for improved decision-making while ensuring privacy. Analytical capabilities, such as treatment recommendations and adverse event predictions, were validated using real-life data, achieving 70%-90% accuracy in a pilot study with the medical partners, demonstrating the framework's effectiveness.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG",
      "cs.SE"
    ],
    "primary_category": "cs.CY",
    "comment": "This manuscript is currently under review at * ACM Transactions on Computing for Healthcare (HEALTH)*",
    "pdf_url": "https://arxiv.org/pdf/2510.09155v1",
    "published_date": "2025-10-10 08:57:41 UTC",
    "updated_date": "2025-10-10 08:57:41 UTC"
  },
  {
    "arxiv_id": "2510.13830v1",
    "title": "Users as Annotators: LLM Preference Learning from Comparison Mode",
    "authors": [
      "Zhongze Cai",
      "Xiaocheng Li"
    ],
    "abstract": "Pairwise preference data have played an important role in the alignment of large language models (LLMs). Each sample of such data consists of a prompt, two different responses to the prompt, and a binary label indicating which of the two responses is better. The labels are usually annotated by professional human annotators. In this paper, we consider an alternative approach to collect pairwise preference data -- user annotation from comparison mode. With the increasingly wider adoption of LLMs among the population, users are contributing more and more of their preference labels through their daily interactions with the LLMs. The upside of such labels is that users are the best experts in judging the responses to their own queries/prompts, but the downside is the lack of quality control in these labels. In this paper, we consider a new idea of generating two responses from two different models or two different versions of the same model. The asymmetry allows us to make an inference of the user's data quality through our proposed user behavior model. We develop an expectation-maximization algorithm to estimate a latent quality factor of the user, and filter users' annotation data accordingly. The downstream task shows the effectiveness of our approach in both capturing the user behavior and data filtering for LLM alignment.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.13830v1",
    "published_date": "2025-10-10 08:57:34 UTC",
    "updated_date": "2025-10-10 08:57:34 UTC"
  },
  {
    "arxiv_id": "2510.09136v1",
    "title": "Controlled Personalization in Legacy Media Online Services: A Case Study in News Recommendation",
    "authors": [
      "Marlene Holzleitner",
      "Stephan Leitner",
      "Hanna Lind Jorgensen",
      "Christoph Schmitz",
      "Jacob Welander",
      "Dietmar Jannach"
    ],
    "abstract": "Personalized news recommendations have become a standard feature of large news aggregation services, optimizing user engagement through automated content selection. In contrast, legacy news media often approach personalization cautiously, striving to balance technological innovation with core editorial values. As a result, online platforms of traditional news outlets typically combine editorially curated content with algorithmically selected articles - a strategy we term controlled personalization. In this industry paper, we evaluate the effectiveness of controlled personalization through an A/B test conducted on the website of a major Norwegian legacy news organization. Our findings indicate that even a modest level of personalization yields substantial benefits. Specifically, we observe that users exposed to personalized content demonstrate higher click-through rates and reduced navigation effort, suggesting improved discovery of relevant content. Moreover, our analysis reveals that controlled personalization contributes to greater content diversity and catalog coverage and in addition reduces popularity bias. Overall, our results suggest that controlled personalization can successfully align user needs with editorial goals, offering a viable path for legacy media to adopt personalization technologies while upholding journalistic values.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09136v1",
    "published_date": "2025-10-10 08:37:13 UTC",
    "updated_date": "2025-10-10 08:37:13 UTC"
  },
  {
    "arxiv_id": "2510.09133v1",
    "title": "PAC Reasoning: Controlling the Performance Loss for Efficient Reasoning",
    "authors": [
      "Hao Zeng",
      "Jianguo Huang",
      "Bingyi Jing",
      "Hongxin Wei",
      "Bo An"
    ],
    "abstract": "Large reasoning models (LRMs) have achieved remarkable progress in complex problem-solving tasks. Despite this success, LRMs typically suffer from high computational costs during deployment, highlighting a need for efficient inference. A popular direction of efficiency improvement is to switch the LRM between thinking and nonthinking modes dynamically. However, such approaches often introduce additional reasoning errors and lack statistical guarantees for the performance loss, which are critical for high-stakes applications. In this work, we propose Probably Approximately Correct (PAC) reasoning that controls the performance loss under the user-specified performance loss tolerance. In particular, we construct an upper confidence bound on the performance loss, formulated as a monotone function of the uncertainty score, and subsequently determine a threshold for switching to the nonthinking model. Theoretically, using the threshold to switch between the thinking and nonthinking modes ensures bounded performance loss in a distribution-free manner. Our comprehensive experiments on reasoning benchmarks show that the proposed method can save computational budgets and control the user-specified performance loss.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "math.ST"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09133v1",
    "published_date": "2025-10-10 08:33:47 UTC",
    "updated_date": "2025-10-10 08:33:47 UTC"
  },
  {
    "arxiv_id": "2510.09121v2",
    "title": "MSDM: Generating Task-Specific Pathology Images with a Multimodal Conditioned Diffusion Model for Cell and Nuclei Segmentation",
    "authors": [
      "Dominik Winter",
      "Mai Bui",
      "Monica Azqueta Gavaldon",
      "Nicolas Triltsch",
      "Marco Rosati",
      "Nicolas Brieu"
    ],
    "abstract": "Scarcity of annotated data, particularly for rare or atypical morphologies, present significant challenges for cell and nuclei segmentation in computational pathology. While manual annotation is labor-intensive and costly, synthetic data offers a cost-effective alternative. We introduce a Multimodal Semantic Diffusion Model (MSDM) for generating realistic pixel-precise image-mask pairs for cell and nuclei segmentation. By conditioning the generative process with cellular/nuclear morphologies (using horizontal and vertical maps), RGB color characteristics, and BERT-encoded assay/indication metadata, MSDM generates datasests with desired morphological properties. These heterogeneous modalities are integrated via multi-head cross-attention, enabling fine-grained control over the generated images. Quantitative analysis demonstrates that synthetic images closely match real data, with low Wasserstein distances between embeddings of generated and real images under matching biological conditions. The incorporation of these synthetic samples, exemplified by columnar cells, significantly improves segmentation model accuracy on columnar cells. This strategy systematically enriches data sets, directly targeting model deficiencies. We highlight the effectiveness of multimodal diffusion-based augmentation for advancing the robustness and generalizability of cell and nuclei segmentation models. Thereby, we pave the way for broader application of generative models in computational pathology.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09121v2",
    "published_date": "2025-10-10 08:23:14 UTC",
    "updated_date": "2025-10-20 09:26:24 UTC"
  },
  {
    "arxiv_id": "2510.09114v3",
    "title": "On the Fairness of Privacy Protection: Measuring and Mitigating the Disparity of Group Privacy Risks for Differentially Private Machine Learning",
    "authors": [
      "Zhi Yang",
      "Changwu Huang",
      "Ke Tang",
      "Xin Yao"
    ],
    "abstract": "While significant progress has been made in conventional fairness-aware machine learning (ML) and differentially private ML (DPML), the fairness of privacy protection across groups remains underexplored. Existing studies have proposed methods to assess group privacy risks, but these are based on the average-case privacy risks of data records. Such approaches may underestimate the group privacy risks, thereby potentially underestimating the disparity across group privacy risks. Moreover, the current method for assessing the worst-case privacy risks of data records is time-consuming, limiting their practical applicability. To address these limitations, we introduce a novel membership inference game that can efficiently audit the approximate worst-case privacy risks of data records. Experimental results demonstrate that our method provides a more stringent measurement of group privacy risks, yielding a reliable assessment of the disparity in group privacy risks. Furthermore, to promote privacy protection fairness in DPML, we enhance the standard DP-SGD algorithm with an adaptive group-specific gradient clipping strategy, inspired by the design of canaries in differential privacy auditing studies. Extensive experiments confirm that our algorithm effectively reduces the disparity in group privacy risks, thereby enhancing the fairness of privacy protection in DPML.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09114v3",
    "published_date": "2025-10-10 08:09:08 UTC",
    "updated_date": "2025-11-16 09:16:20 UTC"
  },
  {
    "arxiv_id": "2510.09110v3",
    "title": "Synthetic Object Compositions for Scalable and Accurate Learning in Detection, Segmentation, and Grounding",
    "authors": [
      "Weikai Huang",
      "Jieyu Zhang",
      "Taoyang Jia",
      "Chenhao Zheng",
      "Ziqi Gao",
      "Jae Sung Park",
      "Winson Han",
      "Ranjay Krishna"
    ],
    "abstract": "Visual grouping -- operationalized through tasks such as instance segmentation, visual grounding, and object detection -- enables applications ranging from robotic perception to photo editing. These fundamental problems in computer vision are powered by large-scale, painstakingly annotated datasets. Despite their impact, these datasets are costly to build, biased in coverage, and difficult to scale. Synthetic datasets offer a promising alternative but struggle with flexibility, accuracy, and compositional diversity.\n  We introduce Synthetic Object Compositions (SOC), an accurate and scalable data synthesis pipeline via a novel object-centric composition strategy. It composes high-quality synthetic object segments into new images using 3D geometric layout augmentation and camera configuration augmentation with generative harmonization and mask-area-weighted blending, yielding accurate and diverse masks, boxes, and referring expressions.\n  Models trained on just 100K of our synthetic images outperform those trained on larger real datasets (GRIT 20M, V3Det 200K) and synthetic pipelines (Copy-Paste, X-Paste, SynGround, SegGen) by +24-36% -- achieving +10.9 AP on LVIS and +8.4 NAcc on gRefCOCO. Beyond the general open-vocabulary setup, SOC also enables controllable dataset construction for different use cases and boosts performance in both low-data and closed-vocabulary scenarios.\n  Augmenting LVIS and COCO with synthetic object segments delivers strong performance across different real-data scales and yields even greater improvements under extremely limited real-data conditions, including +6.59 AP on a 1% COCO data setup. Furthermore, this controllability enables targeted data generation for intra-class referring, a diagnostic grounding task we propose that requires fine-grained attribute discrimination.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project website: https://github.com/weikaih04/Synthetic-Detection-Segmentation-Grounding-Data",
    "pdf_url": "https://arxiv.org/pdf/2510.09110v3",
    "published_date": "2025-10-10 08:04:30 UTC",
    "updated_date": "2025-11-21 12:01:11 UTC"
  },
  {
    "arxiv_id": "2510.09105v1",
    "title": "MemLoss: Enhancing Adversarial Training with Recycling Adversarial Examples",
    "authors": [
      "Soroush Mahdi",
      "Maryam Amirmazlaghani",
      "Saeed Saravani",
      "Zahra Dehghanian"
    ],
    "abstract": "In this paper, we propose a new approach called MemLoss to improve the adversarial training of machine learning models. MemLoss leverages previously generated adversarial examples, referred to as 'Memory Adversarial Examples,' to enhance model robustness and accuracy without compromising performance on clean data. By using these examples across training epochs, MemLoss provides a balanced improvement in both natural accuracy and adversarial robustness. Experimental results on multiple datasets, including CIFAR-10, demonstrate that our method achieves better accuracy compared to existing adversarial training methods while maintaining strong robustness against attacks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "24 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.09105v1",
    "published_date": "2025-10-10 07:59:44 UTC",
    "updated_date": "2025-10-10 07:59:44 UTC"
  },
  {
    "arxiv_id": "2510.09724v1",
    "title": "InteractScience: Programmatic and Visually-Grounded Evaluation of Interactive Scientific Demonstration Code Generation",
    "authors": [
      "Qiaosheng Chen",
      "Yang Liu",
      "Lei Li",
      "Kai Chen",
      "Qipeng Guo",
      "Gong Cheng",
      "Fei Yuan"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly capable of generating complete applications from natural language instructions, creating new opportunities in science and education. In these domains, interactive scientific demonstrations are particularly valuable for explaining concepts, supporting new teaching methods, and presenting research findings. Generating such demonstrations requires models to combine accurate scientific knowledge with the ability to implement interactive front-end code that behaves correctly and responds to user actions. This capability goes beyond the scope of existing benchmarks, which typically evaluate either knowledge question answering without grounding in code or static web code generation without scientific interactivity. To evaluate this integrated ability, we design a hybrid framework that combines programmatic functional testing to rigorously verify interaction logic with visually-grounded qualitative testing to assess rendered outputs against reference snapshots. Building on this framework, we present InteractScience, a benchmark consisting of a substantial set of carefully designed questions across five scientific domains, each paired with unit tests, reference snapshots, and checklists. We evaluate 30 leading open- and closed-source LLMs and report results that highlight ongoing weaknesses in integrating domain knowledge with interactive front-end coding. Our work positions InteractScience as the first benchmark to automatically measure this combined capability with realistic interactive operations, providing a foundation for advancing reliable and educationally useful scientific demonstration code generation. All code and data are publicly available at https://github.com/open-compass/InteractScience.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "27 pages, 17 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.09724v1",
    "published_date": "2025-10-10 07:55:46 UTC",
    "updated_date": "2025-10-10 07:55:46 UTC"
  },
  {
    "arxiv_id": "2510.09096v1",
    "title": "When a Robot is More Capable than a Human: Learning from Constrained Demonstrators",
    "authors": [
      "Xinhu Li",
      "Ayush Jain",
      "Zhaojing Yang",
      "Yigit Korkmaz",
      "Erdem Bıyık"
    ],
    "abstract": "Learning from demonstrations enables experts to teach robots complex tasks using interfaces such as kinesthetic teaching, joystick control, and sim-to-real transfer. However, these interfaces often constrain the expert's ability to demonstrate optimal behavior due to indirect control, setup restrictions, and hardware safety. For example, a joystick can move a robotic arm only in a 2D plane, even though the robot operates in a higher-dimensional space. As a result, the demonstrations collected by constrained experts lead to suboptimal performance of the learned policies. This raises a key question: Can a robot learn a better policy than the one demonstrated by a constrained expert? We address this by allowing the agent to go beyond direct imitation of expert actions and explore shorter and more efficient trajectories. We use the demonstrations to infer a state-only reward signal that measures task progress, and self-label reward for unknown states using temporal interpolation. Our approach outperforms common imitation learning in both sample efficiency and task completion time. On a real WidowX robotic arm, it completes the task in 12 seconds, 10x faster than behavioral cloning, as shown in real-robot videos on https://sites.google.com/view/constrainedexpert .",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09096v1",
    "published_date": "2025-10-10 07:48:12 UTC",
    "updated_date": "2025-10-10 07:48:12 UTC"
  },
  {
    "arxiv_id": "2510.09090v1",
    "title": "AI and Human Oversight: A Risk-Based Framework for Alignment",
    "authors": [
      "Laxmiraju Kandikatla",
      "Branislav Radeljic"
    ],
    "abstract": "As Artificial Intelligence (AI) technologies continue to advance, protecting human autonomy and promoting ethical decision-making are essential to fostering trust and accountability. Human agency (the capacity of individuals to make informed decisions) should be actively preserved and reinforced by AI systems. This paper examines strategies for designing AI systems that uphold fundamental rights, strengthen human agency, and embed effective human oversight mechanisms. It discusses key oversight models, including Human-in-Command (HIC), Human-in-the-Loop (HITL), and Human-on-the-Loop (HOTL), and proposes a risk-based framework to guide the implementation of these mechanisms. By linking the level of AI model risk to the appropriate form of human oversight, the paper underscores the critical role of human involvement in the responsible deployment of AI, balancing technological innovation with the protection of individual values and rights. In doing so, it aims to ensure that AI technologies are used responsibly, safeguarding individual autonomy while maximizing societal benefits.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "19 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.09090v1",
    "published_date": "2025-10-10 07:36:44 UTC",
    "updated_date": "2025-10-10 07:36:44 UTC"
  },
  {
    "arxiv_id": "2510.09723v1",
    "title": "It's 2025 -- Narrative Learning is the new baseline to beat for explainable machine learning",
    "authors": [
      "Gregory D. Baker"
    ],
    "abstract": "In this paper, we introduce Narrative Learning, a methodology where models are defined entirely in natural language and iteratively refine their classification criteria using explanatory prompts rather than traditional numerical optimisation. We report on experiments to evaluate the accuracy and potential of this approach using 3 synthetic and 3 natural datasets and compare them against 7 baseline explainable machine learning models. We demonstrate that on 5 out of 6 of these datasets, Narrative Learning became more accurate than the baseline explainable models in 2025 or earlier because of improvements in language models. We also report on trends in the lexicostatistics of these models' outputs as a proxy for the comprehensibility of the explanations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "18 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.09723v1",
    "published_date": "2025-10-10 07:34:40 UTC",
    "updated_date": "2025-10-10 07:34:40 UTC"
  },
  {
    "arxiv_id": "2510.09087v1",
    "title": "Leading the Follower: Learning Persuasive Agents in Social Deduction Games",
    "authors": [
      "Zhang Zheng",
      "Deheng Ye",
      "Peilin Zhao",
      "Hao Wang"
    ],
    "abstract": "Large language model (LLM) agents have shown remarkable progress in social deduction games (SDGs). However, existing approaches primarily focus on information processing and strategy selection, overlooking the significance of persuasive communication in influencing other players' beliefs and responses. In SDGs, success depends not only on making correct deductions but on convincing others to response in alignment with one's intent. To address this limitation, we formalize turn-based dialogue in SDGs as a Stackelberg competition, where the current player acts as the leader who strategically influences the follower's response. Building on this theoretical foundation, we propose a reinforcement learning framework that trains agents to optimize utterances for persuasive impact. Through comprehensive experiments across three diverse SDGs, we demonstrate that our agents significantly outperform baselines. This work represents a significant step toward developing AI agents capable of strategic social influence, with implications extending to scenarios requiring persuasive communication.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09087v1",
    "published_date": "2025-10-10 07:33:30 UTC",
    "updated_date": "2025-10-10 07:33:30 UTC"
  },
  {
    "arxiv_id": "2510.09082v2",
    "title": "Physics-Informed High-order Graph Dynamics Identification Learning for Predicting Complex Networks Long-term Dynamics",
    "authors": [
      "Bicheng Wang",
      "Junping Wang",
      "Yibo Xue"
    ],
    "abstract": "Learning complex network dynamics is fundamental to understanding, modelling and controlling real-world complex systems. There are two main problems in the task of predicting the dynamic evolution of complex networks: on the one hand, existing methods usually use simple graphs to describe the relationships in complex networks; however, this approach can only capture pairwise relationships, while there may be rich non-pairwise structured relationships in the network. First-order GNNs have difficulty in capturing dynamic non-pairwise relationships. On the other hand, theoretical prediction models lack accuracy and data-driven prediction models lack interpretability. To address the above problems, this paper proposes a higher-order network dynamics identification method for long-term dynamic prediction of complex networks. Firstly, to address the problem that traditional graph machine learning can only deal with pairwise relations, dynamic hypergraph learning is introduced to capture the higher-order non-pairwise relations among complex networks and improve the accuracy of complex network modelling. Then, a dual-driven dynamic prediction module for physical data is proposed. The Koopman operator theory is introduced to transform the nonlinear dynamical differential equations for the dynamic evolution of complex networks into linear systems for solving. Meanwhile, the physical information neural differential equation method is utilised to ensure that the dynamic evolution conforms to the physical laws. The dual-drive dynamic prediction module ensures both accuracy and interpretability of the prediction. Validated on public datasets and self-built industrial chain network datasets, the experimental results show that the method in this paper has good prediction accuracy and long-term prediction performance.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.SI",
      "physics.soc-ph"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09082v2",
    "published_date": "2025-10-10 07:31:17 UTC",
    "updated_date": "2025-10-13 00:51:55 UTC"
  },
  {
    "arxiv_id": "2510.13829v1",
    "title": "A Linguistics-Aware LLM Watermarking via Syntactic Predictability",
    "authors": [
      "Shinwoo Park",
      "Hyejin Park",
      "Hyeseon Ahn",
      "Yo-Sub Han"
    ],
    "abstract": "As large language models (LLMs) continue to advance rapidly, reliable governance tools have become critical. Publicly verifiable watermarking is particularly essential for fostering a trustworthy AI ecosystem. A central challenge persists: balancing text quality against detection robustness. Recent studies have sought to navigate this trade-off by leveraging signals from model output distributions (e.g., token-level entropy); however, their reliance on these model-specific signals presents a significant barrier to public verification, as the detection process requires access to the logits of the underlying model. We introduce STELA, a novel framework that aligns watermark strength with the linguistic degrees of freedom inherent in language. STELA dynamically modulates the signal using part-of-speech (POS) n-gram-modeled linguistic indeterminacy, weakening it in grammatically constrained contexts to preserve quality and strengthen it in contexts with greater linguistic flexibility to enhance detectability. Our detector operates without access to any model logits, thus facilitating publicly verifiable detection. Through extensive experiments on typologically diverse languages-analytic English, isolating Chinese, and agglutinative Korean-we show that STELA surpasses prior methods in detection robustness. Our code is available at https://github.com/Shinwoo-Park/stela_watermark.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.13829v1",
    "published_date": "2025-10-10 07:26:15 UTC",
    "updated_date": "2025-10-10 07:26:15 UTC"
  },
  {
    "arxiv_id": "2510.09080v1",
    "title": "Training Models to Detect Successive Robot Errors from Human Reactions",
    "authors": [
      "Shannon Liu",
      "Maria Teresa Parreira",
      "Wendy Ju"
    ],
    "abstract": "As robots become more integrated into society, detecting robot errors is essential for effective human-robot interaction (HRI). When a robot fails repeatedly, how can it know when to change its behavior? Humans naturally respond to robot errors through verbal and nonverbal cues that intensify over successive failures-from confusion and subtle speech changes to visible frustration and impatience. While prior work shows that human reactions can indicate robot failures, few studies examine how these evolving responses reveal successive failures. This research uses machine learning to recognize stages of robot failure from human reactions. In a study with 26 participants interacting with a robot that made repeated conversational errors, behavioral features were extracted from video data to train models for individual users. The best model achieved 93.5% accuracy for detecting errors and 84.1% for classifying successive failures. Modeling the progression of human reactions enhances error detection and understanding of repeated interaction breakdowns in HRI.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted to NERC '25",
    "pdf_url": "https://arxiv.org/pdf/2510.09080v1",
    "published_date": "2025-10-10 07:25:44 UTC",
    "updated_date": "2025-10-10 07:25:44 UTC"
  },
  {
    "arxiv_id": "2510.09072v1",
    "title": "Emotion-Disentangled Embedding Alignment for Noise-Robust and Cross-Corpus Speech Emotion Recognition",
    "authors": [
      "Upasana Tiwari",
      "Rupayan Chakraborty",
      "Sunil Kumar Kopparapu"
    ],
    "abstract": "Effectiveness of speech emotion recognition in real-world scenarios is often hindered by noisy environments and variability across datasets. This paper introduces a two-step approach to enhance the robustness and generalization of speech emotion recognition models through improved representation learning. First, our model employs EDRL (Emotion-Disentangled Representation Learning) to extract class-specific discriminative features while preserving shared similarities across emotion categories. Next, MEA (Multiblock Embedding Alignment) refines these representations by projecting them into a joint discriminative latent subspace that maximizes covariance with the original speech input. The learned EDRL-MEA embeddings are subsequently used to train an emotion classifier using clean samples from publicly available datasets, and are evaluated on unseen noisy and cross-corpus speech samples. Improved performance under these challenging conditions demonstrates the effectiveness of the proposed method.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "13 pages, 1 figure",
    "pdf_url": "https://arxiv.org/pdf/2510.09072v1",
    "published_date": "2025-10-10 07:17:07 UTC",
    "updated_date": "2025-10-10 07:17:07 UTC"
  },
  {
    "arxiv_id": "2510.17844v1",
    "title": "Modeling Layered Consciousness with Multi-Agent Large Language Models",
    "authors": [
      "Sang Hun Kim",
      "Jongmin Lee",
      "Dongkyu Park",
      "So Young Lee",
      "Yosep Chong"
    ],
    "abstract": "We propose a multi-agent framework for modeling artificial consciousness in large language models (LLMs), grounded in psychoanalytic theory. Our \\textbf{Psychodynamic Model} simulates self-awareness, preconsciousness, and unconsciousness through agent interaction, guided by a Personalization Module combining fixed traits and dynamic needs. Using parameter-efficient fine-tuning on emotionally rich dialogues, the system was evaluated across eight personalized conditions. An LLM as a judge approach showed a 71.2\\% preference for the fine-tuned model, with improved emotional depth and reduced output variance, demonstrating its potential for adaptive, personalized cognition.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.CL",
    "comment": "20 pages, 4 figures, accepted for presentation at EMNLP 2025 Workshop on Active and Passive LLM Personalization (PALS) OpenReview: https://openreview.net/forum?id=rUtNkYvGJI",
    "pdf_url": "https://arxiv.org/pdf/2510.17844v1",
    "published_date": "2025-10-10 07:08:34 UTC",
    "updated_date": "2025-10-10 07:08:34 UTC"
  },
  {
    "arxiv_id": "2510.09060v1",
    "title": "OSCAR: Orthogonal Stochastic Control for Alignment-Respecting Diversity in Flow Matching",
    "authors": [
      "Jingxuan Wu",
      "Zhenglin Wan",
      "Xingrui Yu",
      "Yuzhe Yang",
      "Bo An",
      "Ivor Tsang"
    ],
    "abstract": "Flow-based text-to-image models follow deterministic trajectories, forcing users to repeatedly sample to discover diverse modes, which is a costly and inefficient process. We present a training-free, inference-time control mechanism that makes the flow itself diversity-aware. Our method simultaneously encourages lateral spread among trajectories via a feature-space objective and reintroduces uncertainty through a time-scheduled stochastic perturbation. Crucially, this perturbation is projected to be orthogonal to the generation flow, a geometric constraint that allows it to boost variation without degrading image details or prompt fidelity. Our procedure requires no retraining or modification to the base sampler and is compatible with common flow-matching solvers. Theoretically, our method is shown to monotonically increase a volume surrogate while, due to its geometric constraints, approximately preserving the marginal distribution. This provides a principled explanation for why generation quality is robustly maintained. Empirically, across multiple text-to-image settings under fixed sampling budgets, our method consistently improves diversity metrics such as the Vendi Score and Brisque over strong baselines, while upholding image quality and alignment.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09060v1",
    "published_date": "2025-10-10 07:07:19 UTC",
    "updated_date": "2025-10-10 07:07:19 UTC"
  },
  {
    "arxiv_id": "2510.09722v1",
    "title": "Layout-Aware Parsing Meets Efficient LLMs: A Unified, Scalable Framework for Resume Information Extraction and Evaluation",
    "authors": [
      "Fanwei Zhu",
      "Jinke Yu",
      "Zulong Chen",
      "Ying Zhou",
      "Junhao Ji",
      "Zhibo Yang",
      "Yuxue Zhang",
      "Haoyuan Hu",
      "Zhenghao Liu"
    ],
    "abstract": "Automated resume information extraction is critical for scaling talent acquisition, yet its real-world deployment faces three major challenges: the extreme heterogeneity of resume layouts and content, the high cost and latency of large language models (LLMs), and the lack of standardized datasets and evaluation tools. In this work, we present a layout-aware and efficiency-optimized framework for automated extraction and evaluation that addresses all three challenges. Our system combines a fine-tuned layout parser to normalize diverse document formats, an inference-efficient LLM extractor based on parallel prompting and instruction tuning, and a robust two-stage automated evaluation framework supported by new benchmark datasets. Extensive experiments show that our framework significantly outperforms strong baselines in both accuracy and efficiency. In particular, we demonstrate that a fine-tuned compact 0.6B LLM achieves top-tier accuracy while significantly reducing inference latency and computational cost. The system is fully deployed in Alibaba's intelligent HR platform, supporting real-time applications across its business units.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09722v1",
    "published_date": "2025-10-10 07:01:35 UTC",
    "updated_date": "2025-10-10 07:01:35 UTC"
  },
  {
    "arxiv_id": "2510.09720v1",
    "title": "Preference-Aware Memory Update for Long-Term LLM Agents",
    "authors": [
      "Haoran Sun",
      "Zekun Zhang",
      "Shaoning Zeng"
    ],
    "abstract": "One of the key factors influencing the reasoning capabilities of LLM-based agents is their ability to leverage long-term memory. Integrating long-term memory mechanisms allows agents to make informed decisions grounded in historical interactions. While recent advances have significantly improved the storage and retrieval components, by encoding memory into dense vectors for similarity search or organizing memory as structured knowledge graphs most existing approaches fall short in memory updating. In particular, they lack mechanisms for dynamically refining preference memory representations in response to evolving user behaviors and contexts. To address this gap, we propose a Preference-Aware Memory Update Mechanism (PAMU) that enables dynamic and personalized memory refinement. By integrating sliding window averages (SW) with exponential moving averages (EMA), PAMU constructs a fused preference-aware representation that captures both short-term fluctuations and long-term user tendencies. We conduct experiments on five task scenarios of the LoCoMo dataset, and the results show that our mechanism can significantly improve the output quality of LLM in five baselines, validating its effectiveness in long-term conversations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09720v1",
    "published_date": "2025-10-10 06:49:35 UTC",
    "updated_date": "2025-10-10 06:49:35 UTC"
  },
  {
    "arxiv_id": "2510.09719v3",
    "title": "ICL-Router: In-Context Learned Model Representations for LLM Routing",
    "authors": [
      "Chenxu Wang",
      "Hao Li",
      "Yiqun Zhang",
      "Linyao Chen",
      "Jianhao Chen",
      "Ping Jian",
      "Peng Ye",
      "Qiaosheng Zhang",
      "Shuyue Hu"
    ],
    "abstract": "Large language models (LLMs) often exhibit complementary strengths. Model routing harnesses these strengths by dynamically directing each query to the most suitable model, given a candidate model pool. However, routing performance relies on accurate model representations, and adding new models typically requires retraining, limiting scalability. To address these challenges, we propose a novel routing method using in-context vectors to represent model capabilities. The method proceeds in two stages. First, queries are embedded and projected into vectors, with a projector and LLM-based router trained to reconstruct the original queries, aligning vector representations with the router's semantic space. Second, each candidate model is profiled on a query set, and the router learns -- based on in-context vectors of query and model performance -- to predict whether each model can correctly answer new queries. Extensive experiments demonstrate that our method achieves state-of-the-art routing performance in both in-distribution and out-of-distribution tasks. Moreover, our method allows for seamless integration of new models without retraining the router. The code is available at https://github.com/lalalamdbf/ICL-Router.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by AAAI 2026",
    "pdf_url": "https://arxiv.org/pdf/2510.09719v3",
    "published_date": "2025-10-10 06:47:37 UTC",
    "updated_date": "2025-11-14 12:14:51 UTC"
  },
  {
    "arxiv_id": "2510.09051v1",
    "title": "Alif: Advancing Urdu Large Language Models via Multilingual Synthetic Data Distillation",
    "authors": [
      "Muhammad Ali Shafique",
      "Kanwal Mehreen",
      "Muhammad Arham",
      "Maaz Amjad",
      "Sabur Butt",
      "Hamza Farooq"
    ],
    "abstract": "Developing a high-performing large language models (LLMs) for low-resource languages such as Urdu, present several challenges. These challenges include the scarcity of high-quality datasets, multilingual inconsistencies, and safety concerns. Existing multilingual LLMs often address these issues by translating large volumes of available data. However, such translations often lack quality and cultural nuance while also incurring significant costs for data curation and training. To address these issues, we propose Alif-1.0-8B-Instruct, a multilingual Urdu-English model, that tackles these challenges with a unique approach. We train the model on a high-quality, multilingual synthetic dataset (Urdu-Instruct), developed using a modified self-instruct technique. By using unique prompts and seed values for each task along with a global task pool, this dataset incorporates Urdu-native chain-of-thought based reasoning, bilingual translation, cultural relevance, and ethical safety alignments. This technique significantly enhances the comprehension of Alif-1.0-8B-Instruct model for Urdu-specific tasks. As a result, Alif-1.0-8B-Instruct, built upon the pretrained Llama-3.1-8B, demonstrates superior performance compared to Llama-3.1-8B-Instruct for Urdu specific-tasks. It also outperformed leading multilingual LLMs, including Mistral-7B-Instruct-v0.3, Qwen-2.5-7B-Instruct, and Cohere-Aya-Expanse-8B, all within a training budget of under $100. Our results demonstrate that high-performance and low-resource language LLMs can be developed efficiently and culturally aligned using our modified self-instruct approach. All datasets, models, and code are publicly available at: https://github.com/traversaal-ai/alif-urdu-llm.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to the EMNLP 2025 Workshop on Multilingual Representation Learning (MRL)",
    "pdf_url": "https://arxiv.org/pdf/2510.09051v1",
    "published_date": "2025-10-10 06:41:02 UTC",
    "updated_date": "2025-10-10 06:41:02 UTC"
  },
  {
    "arxiv_id": "2510.09049v1",
    "title": "MEC$^3$O: Multi-Expert Consensus for Code Time Complexity Prediction",
    "authors": [
      "Joonghyuk Hahn",
      "Soohan Lim",
      "Yo-Sub Han"
    ],
    "abstract": "Predicting the complexity of source code is essential for software development and algorithm analysis. Recently, Baik et al. (2025) introduced CodeComplex for code time complexity prediction. The paper shows that LLMs without fine-tuning struggle with certain complexity classes. This suggests that no single LLM excels at every class, but rather each model shows advantages in certain classes. We propose MEC$^3$O, a multi-expert consensus system, which extends the multi-agent debate frameworks. MEC$^3$O assigns LLMs to complexity classes based on their performance and provides them with class-specialized instructions, turning them into experts. These experts engage in structured debates, and their predictions are integrated through a weighted consensus mechanism. Our expertise assignments to LLMs effectively handle Degeneration-of-Thought, reducing reliance on a separate judge model, and preventing convergence to incorrect majority opinions. Experiments on CodeComplex show that MEC$^3$O outperforms the open-source baselines, achieving at least 10% higher accuracy and macro-F1 scores. It also surpasses GPT-4o-mini in macro-F1 scores on average and demonstrates competitive on-par F1 scores to GPT-4o and GPT-o4-mini on average. This demonstrates the effectiveness of multi-expert debates and weight consensus strategy to generate the final predictions. Our code and data is available at https://github.com/suhanmen/MECO.",
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "24 pages, 11 figures, 10 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.09049v1",
    "published_date": "2025-10-10 06:34:49 UTC",
    "updated_date": "2025-10-10 06:34:49 UTC"
  },
  {
    "arxiv_id": "2510.09717v1",
    "title": "High-Power Training Data Identification with Provable Statistical Guarantees",
    "authors": [
      "Zhenlong Liu",
      "Hao Zeng",
      "Weiran Huang",
      "Hongxin Wei"
    ],
    "abstract": "Identifying training data within large-scale models is critical for copyright litigation, privacy auditing, and ensuring fair evaluation. The conventional approaches treat it as a simple binary classification task without statistical guarantees. A recent approach is designed to control the false discovery rate (FDR), but its guarantees rely on strong, easily violated assumptions. In this paper, we introduce Provable Training Data Identification (PTDI), a rigorous method that identifies a set of training data with strict false discovery rate (FDR) control. Specifically, our method computes p-values for each data point using a set of known unseen data, and then constructs a conservative estimator for the data usage proportion of the test set, which allows us to scale these p-values. Our approach then selects the final set of training data by identifying all points whose scaled p-values fall below a data-dependent threshold. This entire procedure enables the discovery of training data with provable, strict FDR control and significantly boosted power. Extensive experiments across a wide range of models (LLMs and VLMs), and datasets demonstrate that PTDI strictly controls the FDR and achieves higher power.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09717v1",
    "published_date": "2025-10-10 06:28:36 UTC",
    "updated_date": "2025-10-10 06:28:36 UTC"
  },
  {
    "arxiv_id": "2510.09045v2",
    "title": "LLM Based Long Code Translation using Identifier Replacement",
    "authors": [
      "Manojit Chakraborty",
      "Madhusudan Ghosh",
      "Rishabh Gupta"
    ],
    "abstract": "In the domain of software development, LLMs have been utilized to automate tasks such as code translation, where source code from one programming language is translated to another while preserving its functionality. However, LLMs often struggle with long source codes that don't fit into the context window, which produces inaccurate translations. To address this, we propose a novel zero-shot code translation method that incorporates identifier replacement. By substituting user-given long identifiers with generalized placeholders during translation, our method allows the LLM to focus on the logical structure of the code, by reducing token count and memory usage, which improves the efficiency and cost-effectiveness of long code translation. Our empirical results demonstrate that our approach preserves syntactical and hierarchical information and produces translation results with reduced tokens.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09045v2",
    "published_date": "2025-10-10 06:28:15 UTC",
    "updated_date": "2025-10-31 08:20:14 UTC"
  },
  {
    "arxiv_id": "2510.09043v2",
    "title": "Humanoid Artificial Consciousness Designed with Large Language Model Based on Psychoanalysis and Personality Theory",
    "authors": [
      "Sang Hun Kim",
      "Jongmin Lee",
      "Dongkyu Park",
      "So Young Lee",
      "Yosep Chong"
    ],
    "abstract": "Human consciousness is still a concept hard to define with current scientific understanding. Although Large Language Models (LLMs) have recently demonstrated significant advancements across various domains including translation and summarization, human consciousness is not something to imitate with current upfront technology owing to so-called hallucination. This study, therefore, proposes a novel approach to address these challenges by integrating psychoanalysis and the Myers-Briggs Type Indicator (MBTI) into constructing consciousness and personality modules. We developed three artificial consciousnesses (self-awareness, unconsciousness, and preconsciousness) based on the principles of psychoanalysis. Additionally, we designed 16 characters with different personalities representing the sixteen MBTI types, with several attributes such as needs, status, and memories. To determine if our model's artificial consciousness exhibits human-like cognition, we created ten distinct situations considering seven attributes such as emotional understanding and logical thinking. The decision-making process of artificial consciousness and the final action were evaluated in three ways: survey evaluation, three-tier classification via ChatGPT, and qualitative review. Both quantitative and qualitative analyses indicated a high likelihood of well-simulated consciousness, although the difference in response between different characters and consciousnesses was not very significant. This implies that the developed models incorporating elements of psychoanalysis and personality theory can lead to building a more intuitive and adaptable AI system with humanoid consciousness. Therefore, this study contributes to opening up new avenues for improving AI interactions in complex cognitive contexts.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "41 pages, 6 figures. Accepted and published to Cognitive Systems Research, 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.09043v2",
    "published_date": "2025-10-10 06:23:50 UTC",
    "updated_date": "2025-10-14 13:58:28 UTC"
  },
  {
    "arxiv_id": "2510.09041v2",
    "title": "Robust Driving Control for Autonomous Vehicles: An Intelligent General-sum Constrained Adversarial Reinforcement Learning Approach",
    "authors": [
      "Junchao Fan",
      "Qi Wei",
      "Ruichen Zhang",
      "Dusit Niyato",
      "Yang Lu",
      "Jianhua Wang",
      "Xiaolin Chang",
      "Bo Ai"
    ],
    "abstract": "Deep reinforcement learning (DRL) has demonstrated remarkable success in developing autonomous driving policies. However, its vulnerability to adversarial attacks remains a critical barrier to real-world deployment. Although existing robust methods have achieved success, they still suffer from three key issues: (i) these methods are trained against myopic adversarial attacks, limiting their abilities to respond to more strategic threats, (ii) they have trouble causing truly safety-critical events (e.g., collisions), but instead often result in minor consequences, and (iii) these methods can introduce learning instability and policy drift during training due to the lack of robust constraints. To address these issues, we propose Intelligent General-sum Constrained Adversarial Reinforcement Learning (IGCARL), a novel robust autonomous driving approach that consists of a strategic targeted adversary and a robust driving agent. The strategic targeted adversary is designed to leverage the temporal decision-making capabilities of DRL to execute strategically coordinated multi-step attacks. In addition, it explicitly focuses on inducing safety-critical events by adopting a general-sum objective. The robust driving agent learns by interacting with the adversary to develop a robust autonomous driving policy against adversarial attacks. To ensure stable learning in adversarial environments and to mitigate policy drift caused by attacks, the agent is optimized under a constrained formulation. Extensive experiments show that IGCARL improves the success rate by at least 27.9% over state-of-the-art methods, demonstrating superior robustness to adversarial attacks and enhancing the safety and reliability of DRL-based autonomous driving.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09041v2",
    "published_date": "2025-10-10 06:21:36 UTC",
    "updated_date": "2025-11-08 06:11:19 UTC"
  },
  {
    "arxiv_id": "2510.09038v1",
    "title": "Auto-scaling Continuous Memory for GUI Agent",
    "authors": [
      "Wenyi Wu",
      "Kun Zhou",
      "Ruoxin Yuan",
      "Vivian Yu",
      "Stephen Wang",
      "Zhiting Hu",
      "Biwei Huang"
    ],
    "abstract": "We study how to endow GUI agents with scalable memory that help generalize across unfamiliar interfaces and long-horizon tasks. Prior GUI agents compress past trajectories into text tokens, which balloons context length and misses decisive visual cues (e.g., exact widget size and position). We propose a continuous memory that encodes each GUI trajectory into a fixed-length sequence of continuous embeddings using the VLM itself as an encoder; these embeddings are plugged directly into the backbone's input layer, sharply reducing context cost while preserving fine-grained visual information. As memory size and retrieval depth increase, performance improves monotonically, unlike text memories that degrade with long prompts. To grow memory at low cost, we introduce an auto-scaling data flywheel that (i) discovers new environments via search, (ii) synthesizes tasks with an open-source VLM, (iii) rolls out trajectories with the agent, and (iv) verifies success with the same VLM. Using this pipeline, we collect 100k+ trajectories for about \\$4000 and fine-tune only the memory encoder (LoRA on a Q-Former, 1.2\\% parameters) with 1,500 samples. On real-world GUI benchmarks, our memory-augmented agent consistently improves success rates under long horizons and distribution shifts. Notably, Qwen-2.5-VL-7B + continuous memory achieves performance comparable to state-of-the-art closed-source models (e.g., GPT-4o, Claude-4).",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09038v1",
    "published_date": "2025-10-10 06:16:45 UTC",
    "updated_date": "2025-10-10 06:16:45 UTC"
  },
  {
    "arxiv_id": "2510.09037v1",
    "title": "Repairing Regex Vulnerabilities via Localization-Guided Instructions",
    "authors": [
      "Sicheol Sung",
      "Joonghyuk Hahn",
      "Yo-Sub Han"
    ],
    "abstract": "Regular expressions (regexes) are foundational to modern computing for critical tasks like input validation and data parsing, yet their ubiquity exposes systems to regular expression denial of service (ReDoS), a vulnerability requiring automated repair methods. Current approaches, however, are hampered by a trade-off. Symbolic, rule-based system are precise but fails to repair unseen or complex vulnerability patterns. Conversely, large language models (LLMs) possess the necessary generalizability but are unreliable for tasks demanding strict syntactic and semantic correctness. We resolve this impasse by introducing a hybrid framework, localized regex repair (LRR), designed to harness LLM generalization while enforcing reliability. Our core insight is to decouple problem identification from the repair process. First, a deterministic, symbolic module localizes the precise vulnerable subpattern, creating a constrained and tractable problem space. Then, the LLM invoked to generate a semantically equivalent fix for this isolated segment. This combined architecture successfully resolves complex repair cases intractable for rule-based repair while avoiding the semantic errors of LLM-only approaches. Our work provides a validated methodology for solving such problems in automated repair, improving the repair rate by 15.4%p over the state-of-the-art. Our code is available at https://github.com/cdltlehf/LRR.",
    "categories": [
      "cs.AI",
      "cs.PL"
    ],
    "primary_category": "cs.AI",
    "comment": "14 pages, 4 figures, 4 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.09037v1",
    "published_date": "2025-10-10 06:15:43 UTC",
    "updated_date": "2025-10-10 06:15:43 UTC"
  },
  {
    "arxiv_id": "2510.09714v2",
    "title": "All Code, No Thought: Current Language Models Struggle to Reason in Ciphered Language",
    "authors": [
      "Shiyuan Guo",
      "Henry Sleight",
      "Fabien Roger"
    ],
    "abstract": "Detecting harmful AI actions is important as AI agents gain adoption. Chain-of-thought (CoT) monitoring is one method widely used to detect adversarial attacks and AI misalignment. However, attackers and misaligned models might evade CoT monitoring through ciphered reasoning: reasoning hidden in encrypted, translated, or compressed text. To assess this risk, we test whether models can perform ciphered reasoning. For each of 28 different ciphers, we fine-tune and prompt up to 10 models to reason in that cipher. We measure model accuracy on math problems as a proxy for reasoning ability. Across the models we test, we find an asymmetry: model accuracy can drop significantly when reasoning in ciphered text, even though models demonstrate comprehension of ciphered text by being able to translate it accurately to English. Even frontier models struggle with lesser-known ciphers, although they can reason accurately in well-known ciphers like rot13. We show that ciphered reasoning capability correlates with cipher prevalence in pretraining data. We also identify scaling laws showing that ciphered reasoning capability improves slowly with additional fine-tuning data. Our work suggests that evading CoT monitoring using ciphered reasoning may be an ineffective tactic for current models and offers guidance on constraining the development of this capability in future frontier models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Version 2: updated related works section on LLM steganography",
    "pdf_url": "https://arxiv.org/pdf/2510.09714v2",
    "published_date": "2025-10-10 06:01:22 UTC",
    "updated_date": "2025-10-15 18:07:42 UTC"
  },
  {
    "arxiv_id": "2510.11734v1",
    "title": "Scaling Law in LLM Simulated Personality: More Detailed and Realistic Persona Profile Is All You Need",
    "authors": [
      "Yuqi Bai",
      "Tianyu Huang",
      "Kun Sun",
      "Yuting Chen"
    ],
    "abstract": "This research focuses on using large language models (LLMs) to simulate social experiments, exploring their ability to emulate human personality in virtual persona role-playing. The research develops an end-to-end evaluation framework, including individual-level analysis of stability and identifiability, as well as population-level analysis called progressive personality curves to examine the veracity and consistency of LLMs in simulating human personality. Methodologically, this research proposes important modifications to traditional psychometric approaches (CFA and construct validity) which are unable to capture improvement trends in LLMs at their current low-level simulation, potentially leading to remature rejection or methodological misalignment. The main contributions of this research are: proposing a systematic framework for LLM virtual personality evaluation; empirically demonstrating the critical role of persona detail in personality simulation quality; and identifying marginal utility effects of persona profiles, especially a Scaling Law in LLM personality simulation, offering operational evaluation metrics and a theoretical foundation for applying large language models in social science experiments.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.11734v1",
    "published_date": "2025-10-10 05:52:07 UTC",
    "updated_date": "2025-10-10 05:52:07 UTC"
  },
  {
    "arxiv_id": "2510.09025v1",
    "title": "Déréverbération non-supervisée de la parole par modèle hybride",
    "authors": [
      "Louis Bahrman",
      "Mathieu Fontaine",
      "Gaël Richard"
    ],
    "abstract": "This paper introduces a new training strategy to improve speech dereverberation systems in an unsupervised manner using only reverberant speech. Most existing algorithms rely on paired dry/reverberant data, which is difficult to obtain. Our approach uses limited acoustic information, like the reverberation time (RT60), to train a dereverberation system. Experimental results demonstrate that our method achieves more consistent performance across various objective metrics than the state-of-the-art.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "in French language",
    "pdf_url": "https://arxiv.org/pdf/2510.09025v1",
    "published_date": "2025-10-10 05:51:17 UTC",
    "updated_date": "2025-10-10 05:51:17 UTC"
  },
  {
    "arxiv_id": "2510.09021v1",
    "title": "RefGrader: Automated Grading of Mathematical Competition Proofs using Agentic Workflows",
    "authors": [
      "Hamed Mahdavi",
      "Pouria Mahdavinia",
      "Samira Malek",
      "Pegah Mohammadipour",
      "Alireza Hashemi",
      "Majid Daliri",
      "Alireza Farhadi",
      "Amir Khasahmadi",
      "Niloofar Mireshghallah",
      "Vasant Honavar"
    ],
    "abstract": "State-of-the-art (SOTA) LLMs have progressed from struggling on proof-based Olympiad problems to solving most of the IMO 2025 problems, with leading systems reportedly handling 5 of 6 problems. Given this progress, we assess how well these models can grade proofs: detecting errors, judging their severity, and assigning fair scores beyond binary correctness. We study proof-analysis capabilities using a corpus of 90 Gemini 2.5 Pro-generated solutions that we grade on a 1-4 scale with detailed error annotations, and on MathArena solution sets for IMO/USAMO 2025 scored on a 0-7 scale. Our analysis shows that models can reliably flag incorrect (including subtly incorrect) solutions but exhibit calibration gaps in how partial credit is assigned. To address this, we introduce agentic workflows that extract and analyze reference solutions and automatically derive problem-specific rubrics for a multi-step grading process. We instantiate and compare different design choices for the grading workflows, and evaluate their trade-offs. Across our annotated corpus and MathArena, our proposed workflows achieve higher agreement with human grades and more consistent handling of partial credit across metrics. We release all code, data, and prompts/logs to facilitate future research.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09021v1",
    "published_date": "2025-10-10 05:47:40 UTC",
    "updated_date": "2025-10-10 05:47:40 UTC"
  },
  {
    "arxiv_id": "2510.09017v2",
    "title": "Value-State Gated Attention for Mitigating Extreme-Token Phenomena in Transformers",
    "authors": [
      "Rui Bu",
      "Haofeng Zhong",
      "Wenzheng Chen",
      "Yangyan Li"
    ],
    "abstract": "Large models based on the Transformer architecture are susceptible to extreme-token phenomena, such as attention sinks and value-state drains. These issues, which degrade model performance, quantization fidelity, and interpretability, arise from a problematic mutual reinforcement mechanism where the model learns an inefficient 'no-op' behavior by focusing attention on tokens with near-zero value states. In this paper, we propose Value-State Gated Attention (VGA), a simple, dedicated, and stable architectural mechanism for performing 'no-op' attention efficiently by directly breaking this cycle. VGA introduces a learnable, data-dependent gate, computed directly from the value vectors (V), to modulate the output. Through a theoretical analysis of the underlying gradients, we show that gating the value-state with a function of itself is more effective at decoupling value and attention score updates than prior methods that gate on input embeddings. This creates a direct regulatory pathway that allows the model to suppress a token's contribution based on its emergent value representation. Our experiments demonstrate that VGA significantly mitigates the formation of attention sinks and stabilizes value-state norms, leading to improved performance, robust quantization fidelity, and enhanced model interpretability.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09017v2",
    "published_date": "2025-10-10 05:40:53 UTC",
    "updated_date": "2025-12-09 05:11:30 UTC"
  },
  {
    "arxiv_id": "2510.09016v2",
    "title": "DiTSinger: Scaling Singing Voice Synthesis with Diffusion Transformer and Implicit Alignment",
    "authors": [
      "Zongcai Du",
      "Guilin Deng",
      "Xiaofeng Guo",
      "Xin Gao",
      "Linke Li",
      "Kaichang Cheng",
      "Fubo Han",
      "Siyu Yang",
      "Peng Liu",
      "Pan Zhong",
      "Qiang Fu"
    ],
    "abstract": "Recent progress in diffusion-based Singing Voice Synthesis (SVS) demonstrates strong expressiveness but remains limited by data scarcity and model scalability. We introduce a two-stage pipeline: a compact seed set of human-sung recordings is constructed by pairing fixed melodies with diverse LLM-generated lyrics, and melody-specific models are trained to synthesize over 500 hours of high-quality Chinese singing data. Building on this corpus, we propose DiTSinger, a Diffusion Transformer with RoPE and qk-norm, systematically scaled in depth, width, and resolution for enhanced fidelity. Furthermore, we design an implicit alignment mechanism that obviates phoneme-level duration labels by constraining phoneme-to-acoustic attention within character-level spans, thereby improving robustness under noisy or uncertain alignments. Extensive experiments validate that our approach enables scalable, alignment-free, and high-fidelity SVS.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "ICASSP26 under review. Demo page: https://nju-jet.github.io/DiTSinger",
    "pdf_url": "https://arxiv.org/pdf/2510.09016v2",
    "published_date": "2025-10-10 05:39:45 UTC",
    "updated_date": "2025-12-24 17:16:37 UTC"
  },
  {
    "arxiv_id": "2510.09011v3",
    "title": "TripScore: Benchmarking and rewarding real-world travel planning with fine-grained evaluation",
    "authors": [
      "Yincen Qu",
      "Huan Xiao",
      "Feng Li",
      "Gregory Li",
      "Hui Zhou",
      "Xiangying Dai",
      "Xiaoru Dai"
    ],
    "abstract": "Travel planning is a valuable yet complex task that poses significant challenges even for advanced large language models (LLMs). While recent benchmarks have advanced in evaluating LLMs' planning capabilities, they often fall short in evaluating feasibility, reliability, and engagement of travel plans. We introduce a comprehensive benchmark for travel planning that unifies fine-grained criteria into a single reward, enabling direct comparison of plan quality and seamless integration with reinforcement learning (RL). Our evaluator achieves moderate agreement with travel-expert annotations (60.75%) and outperforms multiple LLM-as-judge baselines. We further release a large-scale dataset of 4,870 queries including 219 real-world, free-form requests for generalization to authentic user intent. Using this benchmark, we conduct extensive experiments across diverse methods and LLMs, including test-time computation, neuro-symbolic approaches, supervised fine-tuning, and RL via GRPO. Across base models, RL generally improves itinerary feasibility over prompt-only and supervised baselines, yielding higher unified reward scores.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09011v3",
    "published_date": "2025-10-10 05:22:29 UTC",
    "updated_date": "2025-10-16 07:45:03 UTC"
  },
  {
    "arxiv_id": "2510.09008v1",
    "title": "On Epistemic Uncertainty of Visual Tokens for Object Hallucinations in Large Vision-Language Models",
    "authors": [
      "Hoigi Seo",
      "Dong Un Kang",
      "Hyunjin Cho",
      "Joohoon Lee",
      "Se Young Chun"
    ],
    "abstract": "Large vision-language models (LVLMs), which integrate a vision encoder (VE) with a large language model, have achieved remarkable success across various tasks. However, there are still crucial challenges in LVLMs such as object hallucination, generating descriptions of objects that are not in the input image. Here, we argue that uncertain visual tokens within the VE is a key factor that contributes to object hallucination. Our statistical analysis found that there are positive correlations between visual tokens with high epistemic uncertainty and the occurrence of hallucinations. Furthermore, we show theoretically and empirically that visual tokens in early VE layers that exhibit large representation deviations under small adversarial perturbations indicate high epistemic uncertainty. Based on these findings, we propose a simple yet effective strategy to mitigate object hallucination by modifying the VE only. Our method comprises a proxy method with adversarial perturbations for identifying uncertain visual tokens efficiently and a method to mask these uncertain visual tokens during the self-attention process in the middle layers of the VE, suppressing their influence on visual encoding and thus alleviating hallucinations. Extensive experiments show that our method significantly reduces object hallucinations in LVLMs and can synergistically work with other prior arts.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09008v1",
    "published_date": "2025-10-10 05:12:52 UTC",
    "updated_date": "2025-10-10 05:12:52 UTC"
  },
  {
    "arxiv_id": "2510.08999v1",
    "title": "SQS: Bayesian DNN Compression through Sparse Quantized Sub-distributions",
    "authors": [
      "Ziyi Wang",
      "Nan Jiang",
      "Guang Lin",
      "Qifan Song"
    ],
    "abstract": "Compressing large-scale neural networks is essential for deploying models on resource-constrained devices. Most existing methods adopt weight pruning or low-bit quantization individually, often resulting in suboptimal compression rates to preserve acceptable performance drops. We introduce a unified framework for simultaneous pruning and low-bit quantization via Bayesian variational learning (SQS), which achieves higher compression rates than prior baselines while maintaining comparable performance. The key idea is to employ a spike-and-slab prior to inducing sparsity and model quantized weights using Gaussian Mixture Models (GMMs) to enable low-bit precision. In theory, we provide the consistent result of our proposed variational approach to a sparse and quantized deep neural network. Extensive experiments on compressing ResNet, BERT-base, Llama3, and Qwen2.5 models show that our method achieves higher compression rates than a line of existing methods with comparable performance drops.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08999v1",
    "published_date": "2025-10-10 04:54:29 UTC",
    "updated_date": "2025-10-10 04:54:29 UTC"
  },
  {
    "arxiv_id": "2510.08996v3",
    "title": "Saving SWE-Bench: A Benchmark Mutation Approach for Realistic Agent Evaluation",
    "authors": [
      "Spandan Garg",
      "Benjamin Steenhoek",
      "Yufan Huang"
    ],
    "abstract": "Current benchmarks for evaluating software engineering agents, such as SWE-Bench Verified, are predominantly derived from GitHub issues and fail to accurately reflect how developers interact with chat-based coding assistants in integrated development environments (IDEs). We posit that this mismatch leads to a systematic overestimation of agent's capabilities in real-world scenarios, especially bug fixing. We introduce a novel benchmarking framework that transforms existing formal benchmarks into realistic user queries through systematic analysis of developer interaction patterns with chat-based agents. Our methodology is flexible and can be easily extended to existing benchmarks. In this paper, we apply our testing framework to SWE-Bench Verified, the TypeScript subset of Multi-SWE-Bench and a private benchmark, SWE-Bench C# and transform formal GitHub issue descriptions into realistic user-style queries based on telemetry analysis of a popular chat-based agent interactions. Our findings reveal that existing benchmarks significantly overestimate agent capabilities for some models by >50% over baseline performance for public benchmarks and ~10-16% for our internal benchmark. This work establishes a new paradigm for evaluating interactive chat-based software engineering agents through benchmark mutation techniques.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08996v3",
    "published_date": "2025-10-10 04:42:02 UTC",
    "updated_date": "2026-01-17 07:42:49 UTC"
  },
  {
    "arxiv_id": "2510.09712v1",
    "title": "Group-Adaptive Adversarial Learning for Robust Fake News Detection Against Malicious Comments",
    "authors": [
      "Zhao Tong",
      "Chunlin Gong",
      "Yimeng Gu",
      "Haichao Shi",
      "Qiang Liu",
      "Shu Wu",
      "Xiao-Yu Zhang"
    ],
    "abstract": "The spread of fake news online distorts public judgment and erodes trust in social media platforms. Although recent fake news detection (FND) models perform well in standard settings, they remain vulnerable to adversarial comments-authored by real users or by large language models (LLMs)-that subtly shift model decisions. In view of this, we first present a comprehensive evaluation of comment attacks to existing fake news detectors and then introduce a group-adaptive adversarial training strategy to improve the robustness of FND models. To be specific, our approach comprises three steps: (1) dividing adversarial comments into three psychologically grounded categories: perceptual, cognitive, and societal; (2) generating diverse, category-specific attacks via LLMs to enhance adversarial training; and (3) applying a Dirichlet-based adaptive sampling mechanism (InfoDirichlet Adjusting Mechanism) that dynamically adjusts the learning focus across different comment categories during training. Experiments on benchmark datasets show that our method maintains strong detection accuracy while substantially increasing robustness to a wide range of adversarial comment perturbations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 12 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.09712v1",
    "published_date": "2025-10-10 04:39:57 UTC",
    "updated_date": "2025-10-10 04:39:57 UTC"
  },
  {
    "arxiv_id": "2510.09711v1",
    "title": "ReaLM: Residual Quantization Bridging Knowledge Graph Embeddings and Large Language Models",
    "authors": [
      "Wenbin Guo",
      "Xin Wang",
      "Jiaoyan Chen",
      "Lingbing Guo",
      "Zhao Li",
      "Zirui Chen"
    ],
    "abstract": "Large Language Models (LLMs) have recently emerged as a powerful paradigm for Knowledge Graph Completion (KGC), offering strong reasoning and generalization capabilities beyond traditional embedding-based approaches. However, existing LLM-based methods often struggle to fully exploit structured semantic representations, as the continuous embedding space of pretrained KG models is fundamentally misaligned with the discrete token space of LLMs. This discrepancy hinders effective semantic transfer and limits their performance. To address this challenge, we propose ReaLM, a novel and effective framework that bridges the gap between KG embeddings and LLM tokenization through the mechanism of residual vector quantization. ReaLM discretizes pretrained KG embeddings into compact code sequences and integrates them as learnable tokens within the LLM vocabulary, enabling seamless fusion of symbolic and contextual knowledge. Furthermore, we incorporate ontology-guided class constraints to enforce semantic consistency, refining entity predictions based on class-level compatibility. Extensive experiments on two widely used benchmark datasets demonstrate that ReaLM achieves state-of-the-art performance, confirming its effectiveness in aligning structured knowledge with large-scale language models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.09711v1",
    "published_date": "2025-10-10 04:36:13 UTC",
    "updated_date": "2025-10-10 04:36:13 UTC"
  },
  {
    "arxiv_id": "2510.14995v1",
    "title": "PC-UNet: An Enforcing Poisson Statistics U-Net for Positron Emission Tomography Denoising",
    "authors": [
      "Yang Shi",
      "Jingchao Wang",
      "Liangsi Lu",
      "Mingxuan Huang",
      "Ruixin He",
      "Yifeng Xie",
      "Hanqian Liu",
      "Minzhe Guo",
      "Yangyang Liang",
      "Weipeng Zhang",
      "Zimeng Li",
      "Xuhang Chen"
    ],
    "abstract": "Positron Emission Tomography (PET) is crucial in medicine, but its clinical use is limited due to high signal-to-noise ratio doses increasing radiation exposure. Lowering doses increases Poisson noise, which current denoising methods fail to handle, causing distortions and artifacts. We propose a Poisson Consistent U-Net (PC-UNet) model with a new Poisson Variance and Mean Consistency Loss (PVMC-Loss) that incorporates physical data to improve image fidelity. PVMC-Loss is statistically unbiased in variance and gradient adaptation, acting as a Generalized Method of Moments implementation, offering robustness to minor data mismatches. Tests on PET datasets show PC-UNet improves physical consistency and image fidelity, proving its ability to integrate physical information effectively.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by BIBM 2025 as a regular paper",
    "pdf_url": "https://arxiv.org/pdf/2510.14995v1",
    "published_date": "2025-10-10 04:26:26 UTC",
    "updated_date": "2025-10-10 04:26:26 UTC"
  },
  {
    "arxiv_id": "2510.08993v1",
    "title": "PlatformX: An End-to-End Transferable Platform for Energy-Efficient Neural Architecture Search",
    "authors": [
      "Xiaolong Tu",
      "Dawei Chen",
      "Kyungtae Han",
      "Onur Altintas",
      "Haoxin Wang"
    ],
    "abstract": "Hardware-Aware Neural Architecture Search (HW-NAS) has emerged as a powerful tool for designing efficient deep neural networks (DNNs) tailored to edge devices. However, existing methods remain largely impractical for real-world deployment due to their high time cost, extensive manual profiling, and poor scalability across diverse hardware platforms with complex, device-specific energy behavior. In this paper, we present PlatformX, a fully automated and transferable HW-NAS framework designed to overcome these limitations. PlatformX integrates four key components: (i) an energy-driven search space that expands conventional NAS design by incorporating energy-critical configurations, enabling exploration of high-efficiency architectures; (ii) a transferable kernel-level energy predictor across devices and incrementally refined with minimal on-device samples; (iii) a Pareto-based multi-objective search algorithm that balances energy and accuracy to identify optimal trade-offs; and (iv) a high-resolution runtime energy profiling system that automates on-device power measurement using external monitors without human intervention. We evaluate PlatformX across multiple mobile platforms, showing that it significantly reduces search overhead while preserving accuracy and energy fidelity. It identifies models with up to 0.94 accuracy or as little as 0.16 mJ per inference, both outperforming MobileNet-V2 in accuracy and efficiency. Code and tutorials are available at github.com/amai-gsu/PlatformX.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08993v1",
    "published_date": "2025-10-10 04:22:14 UTC",
    "updated_date": "2025-10-10 04:22:14 UTC"
  },
  {
    "arxiv_id": "2510.08987v2",
    "title": "Towards Efficient Multimodal Unified Reasoning Model via Model Merging",
    "authors": [
      "Qixiang Yin",
      "Huanjin Yao",
      "Jianghao Chen",
      "Jiaxing Huang",
      "Zhicheng Zhao",
      "Fei Su"
    ],
    "abstract": "Although Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities across diverse tasks, they encounter challenges in terms of reasoning efficiency, large model size and overthinking. However, existing lightweight MLLMs lack the capability to balance high efficiency and performance at a small scale. To this end, we propose Tiny-R1V, a novel lightweight 3B model that achieves faster inference and higher accuracy via a two-stage optimization, while unifying multimodal reasoning across multiple tasks with fewer inference tokens. In the first stage, Tiny-R1V introduces Length-Informed Relative Policy Optimization (LIPO), a new reinforcement learning method, to train each reasoning model, including mathematical reasoning, chart reasoning, and OCR capability. The LIPO dynamically adjusts the advantages of responses within groups by prioritizing concise yet high-quality responses to encourage the generation of shorter and more accurate responses. In the second stage, we propose Adaptive Model Merging (AMM), a training-free model merging method that merges multiple specialist models into a unified architecture. Specifically, AMM adaptively adjusts the weights of task vectors via a novel gradient projection regularization loss function, thus mitigating redundant conflicts between them. Extensive evaluations on ten widely-used reasoning benchmarks covering mathematics, structured data (charts, tables, documents), OCR, and general capabilities showcase the superior performance of Tiny-R1V, enabling lightweight models to excel in diverse multimodal reasoning tasks. Code will be available at \\href{https://github.com/buptyqx/Tiny-R1V}{https://github.com/buptyqx/Tiny-R1V}",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Technical report, Code will be available at https://github.com/buptyqx/Tiny-R1V",
    "pdf_url": "https://arxiv.org/pdf/2510.08987v2",
    "published_date": "2025-10-10 04:14:57 UTC",
    "updated_date": "2025-11-20 05:54:59 UTC"
  },
  {
    "arxiv_id": "2510.08981v1",
    "title": "SEER: Sustainability Enhanced Engineering of Software Requirements",
    "authors": [
      "Mandira Roy",
      "Novarun Deb",
      "Nabendu Chaki",
      "Agostino Cortesi"
    ],
    "abstract": "The rapid expansion of software development has significant environmental, technical, social, and economic impacts. Achieving the United Nations Sustainable Development Goals by 2030 compels developers to adopt sustainable practices. Existing methods mostly offer high-level guidelines, which are time-consuming to implement and rely on team adaptability. Moreover, they focus on design or implementation, while sustainability assessment should start at the requirements engineering phase. In this paper, we introduce SEER, a framework which addresses sustainability concerns in the early software development phase. The framework operates in three stages: (i) it identifies sustainability requirements (SRs) relevant to a specific software product from a general taxonomy; (ii) it evaluates how sustainable system requirements are based on the identified SRs; and (iii) it optimizes system requirements that fail to satisfy any SR. The framework is implemented using the reasoning capabilities of large language models and the agentic RAG (Retrieval Augmented Generation) approach. SEER has been experimented on four software projects from different domains. Results generated using Gemini 2.5 reasoning model demonstrate the effectiveness of the proposed approach in accurately identifying a broad range of sustainability concerns across diverse domains.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "Main Paper: 32 pages, References: 3 pages, Appendix: 13 pages. Submitted to the Journal of Systems and Software, Elsevier",
    "pdf_url": "https://arxiv.org/pdf/2510.08981v1",
    "published_date": "2025-10-10 03:48:30 UTC",
    "updated_date": "2025-10-10 03:48:30 UTC"
  },
  {
    "arxiv_id": "2510.09710v2",
    "title": "SeCon-RAG: A Two-Stage Semantic Filtering and Conflict-Free Framework for Trustworthy RAG",
    "authors": [
      "Xiaonan Si",
      "Meilin Zhu",
      "Simeng Qin",
      "Lijia Yu",
      "Lijun Zhang",
      "Shuaitong Liu",
      "Xinfeng Li",
      "Ranjie Duan",
      "Yang Liu",
      "Xiaojun Jia"
    ],
    "abstract": "Retrieval-augmented generation (RAG) systems enhance large language models (LLMs) with external knowledge but are vulnerable to corpus poisoning and contamination attacks, which can compromise output integrity. Existing defenses often apply aggressive filtering, leading to unnecessary loss of valuable information and reduced reliability in generation. To address this problem, we propose a two-stage semantic filtering and conflict-free framework for trustworthy RAG. In the first stage, we perform a joint filter with semantic and cluster-based filtering which is guided by the Entity-intent-relation extractor (EIRE). EIRE extracts entities, latent objectives, and entity relations from both the user query and filtered documents, scores their semantic relevance, and selectively adds valuable documents into the clean retrieval database. In the second stage, we proposed an EIRE-guided conflict-aware filtering module, which analyzes semantic consistency between the query, candidate answers, and retrieved knowledge before final answer generation, filtering out internal and external contradictions that could mislead the model. Through this two-stage process, SeCon-RAG effectively preserves useful knowledge while mitigating conflict contamination, achieving significant improvements in both generation robustness and output trustworthiness. Extensive experiments across various LLMs and datasets demonstrate that the proposed SeCon-RAG markedly outperforms state-of-the-art defense methods.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.09710v2",
    "published_date": "2025-10-10 03:44:29 UTC",
    "updated_date": "2025-10-15 07:05:25 UTC"
  },
  {
    "arxiv_id": "2510.08968v1",
    "title": "Learning Regularizers: Learning Optimizers that can Regularize",
    "authors": [
      "Suraj Kumar Sahoo",
      "Narayanan C Krishnan"
    ],
    "abstract": "Learned Optimizers (LOs), a type of Meta-learning, have gained traction due to their ability to be parameterized and trained for efficient optimization. Traditional gradient-based methods incorporate explicit regularization techniques such as Sharpness-Aware Minimization (SAM), Gradient-norm Aware Minimization (GAM), and Gap-guided Sharpness-Aware Minimization (GSAM) to enhance generalization and convergence. In this work, we explore a fundamental question: \\textbf{Can regularizers be learned?} We empirically demonstrate that LOs can be trained to learn and internalize the effects of traditional regularization techniques without explicitly applying them to the objective function. We validate this through extensive experiments on standard benchmarks (including MNIST, FMNIST, CIFAR and Neural Networks such as MLP, MLP-Relu and CNN), comparing LOs trained with and without access to explicit regularizers. Regularized LOs consistently outperform their unregularized counterparts in terms of test accuracy and generalization. Furthermore, we show that LOs retain and transfer these regularization effects to new optimization tasks by inherently seeking minima similar to those targeted by these regularizers. Our results suggest that LOs can inherently learn regularization properties, \\textit{challenging the conventional necessity of explicit optimizee loss regularization.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08968v1",
    "published_date": "2025-10-10 03:23:28 UTC",
    "updated_date": "2025-10-10 03:23:28 UTC"
  },
  {
    "arxiv_id": "2510.08966v1",
    "title": "Semantic-Condition Tuning: Fusing Graph Context with Large Language Models for Knowledge Graph Completion",
    "authors": [
      "Ruitong Liu",
      "Yan Wen",
      "Te Sun",
      "Yunjia Wu",
      "Pingyang Huang",
      "Zihang Yu",
      "Siyuan Li"
    ],
    "abstract": "Fusing Knowledge Graphs with Large Language Models is crucial for knowledge-intensive tasks like knowledge graph completion. The prevailing paradigm, prefix-tuning, simply concatenates knowledge embeddings with text inputs. However, this shallow fusion overlooks the rich relational semantics within KGs and imposes a significant implicit reasoning burden on the LLM to correlate the prefix with the text. To address these, we propose Semantic-condition Tuning (SCT), a new knowledge injection paradigm comprising two key modules. First, a Semantic Graph Module employs a Graph Neural Network to extract a context-aware semantic condition from the local graph neighborhood, guided by knowledge-enhanced relations. Subsequently, this condition is passed to a Condition-Adaptive Fusion Module, which, in turn, adaptively modulates the textual embedding via two parameterized projectors, enabling a deep, feature-wise, and knowledge-aware interaction. The resulting pre-fused embedding is then fed into the LLM for fine-tuning. Extensive experiments on knowledge graph benchmarks demonstrate that SCT significantly outperforms prefix-tuning and other strong baselines. Our analysis confirms that by modulating the input representation with semantic graph context before LLM inference, SCT provides a more direct and potent signal, enabling more accurate and robust knowledge reasoning.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "11 pages, 3 figures, conference",
    "pdf_url": "https://arxiv.org/pdf/2510.08966v1",
    "published_date": "2025-10-10 03:22:27 UTC",
    "updated_date": "2025-10-10 03:22:27 UTC"
  },
  {
    "arxiv_id": "2510.08962v1",
    "title": "Analytical Survey of Learning with Low-Resource Data: From Analysis to Investigation",
    "authors": [
      "Xiaofeng Cao",
      "Mingwei Xu",
      "Xin Yu",
      "Jiangchao Yao",
      "Wei Ye",
      "Shengjun Huang",
      "Minling Zhang",
      "Ivor W. Tsang",
      "Yew Soon Ong",
      "James T. Kwok",
      "Heng Tao Shen"
    ],
    "abstract": "Learning with high-resource data has demonstrated substantial success in artificial intelligence (AI); however, the costs associated with data annotation and model training remain significant. A fundamental objective of AI research is to achieve robust generalization with limited-resource data. This survey employs agnostic active sampling theory within the Probably Approximately Correct (PAC) framework to analyze the generalization error and label complexity associated with learning from low-resource data in both model-agnostic supervised and unsupervised settings. Based on this analysis, we investigate a suite of optimization strategies tailored for low-resource data learning, including gradient-informed optimization, meta-iteration optimization, geometry-aware optimization, and LLMs-powered optimization. Furthermore, we provide a comprehensive overview of multiple learning paradigms that can benefit from low-resource data, including domain transfer, reinforcement feedback, and hierarchical structure modeling. Finally, we conclude our analysis and investigation by summarizing the key findings and highlighting their implications for learning with low-resource data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by ACM Computing Surveys",
    "pdf_url": "https://arxiv.org/pdf/2510.08962v1",
    "published_date": "2025-10-10 03:15:42 UTC",
    "updated_date": "2025-10-10 03:15:42 UTC"
  },
  {
    "arxiv_id": "2510.11732v1",
    "title": "Serial-Parallel Dual-Path Architecture for Speaking Style Recognition",
    "authors": [
      "Guojian Li",
      "Qijie Shao",
      "Zhixian Zhao",
      "Shuiyuan Wang",
      "Zhonghua Fu",
      "Lei Xie"
    ],
    "abstract": "Speaking Style Recognition (SSR) identifies a speaker's speaking style characteristics from speech. Existing style recognition approaches primarily rely on linguistic information, with limited integration of acoustic information, which restricts recognition accuracy improvements. The fusion of acoustic and linguistic modalities offers significant potential to enhance recognition performance. In this paper, we propose a novel serial-parallel dual-path architecture for SSR that leverages acoustic-linguistic bimodal information. The serial path follows the ASR+STYLE serial paradigm, reflecting a sequential temporal dependency, while the parallel path integrates our designed Acoustic-Linguistic Similarity Module (ALSM) to facilitate cross-modal interaction with temporal simultaneity. Compared to the existing SSR baseline -- the OSUM model, our approach reduces parameter size by 88.4% and achieves a 30.3% improvement in SSR accuracy for eight styles on the test set.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted by NCMMSC2025",
    "pdf_url": "https://arxiv.org/pdf/2510.11732v1",
    "published_date": "2025-10-10 03:15:25 UTC",
    "updated_date": "2025-10-10 03:15:25 UTC"
  },
  {
    "arxiv_id": "2510.08959v1",
    "title": "DualResearch: Entropy-Gated Dual-Graph Retrieval for Answer Reconstruction",
    "authors": [
      "Jinxin Shi",
      "Zongsheng Cao",
      "Runmin Ma",
      "Yusong Hu",
      "Jie Zhou",
      "Xin Li",
      "Lei Bai",
      "Liang He",
      "Bo Zhang"
    ],
    "abstract": "The deep-research framework orchestrates external tools to perform complex, multi-step scientific reasoning that exceeds the native limits of a single large language model. However, it still suffers from context pollution, weak evidentiary support, and brittle execution paths. To address these issues, we propose DualResearch, a retrieval and fusion framework that matches the epistemic structure of tool-intensive reasoning by jointly modeling two complementary graphs: a breadth semantic graph that encodes stable background knowledge, and a depth causal graph that captures execution provenance. Each graph has a layer-native relevance function, seed-anchored semantic diffusion for breadth, and causal-semantic path matching with reliability weighting for depth. To reconcile their heterogeneity and query-dependent uncertainty, DualResearch converts per-layer path evidence into answer distributions and fuses them in log space via an entropy-gated rule with global calibration. The fusion up-weights the more certain channel and amplifies agreement. As a complement to deep-research systems, DualResearch compresses lengthy multi-tool execution logs into a concise reasoning graph, and we show that it can reconstruct answers stably and effectively. On the scientific reasoning benchmarks HLE and GPQA, DualResearch achieves competitive performance. Using log files from the open-source system InternAgent, its accuracy improves by 7.7% on HLE and 6.06% on GPQA.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "16 pages, 6 figures, 5 tables, Under Review",
    "pdf_url": "https://arxiv.org/pdf/2510.08959v1",
    "published_date": "2025-10-10 03:10:17 UTC",
    "updated_date": "2025-10-10 03:10:17 UTC"
  },
  {
    "arxiv_id": "2510.15946v2",
    "title": "Learning from Mistakes: Enhancing Harmful Meme Detection via Misjudgment Risk Patterns",
    "authors": [
      "Wenshuo Wang",
      "Ziyou Jiang",
      "Junjie Wang",
      "Mingyang Li",
      "Jie Huang",
      "Yuekai Huang",
      "Zhiyuan Chang",
      "Feiyan Duan",
      "Qing Wang"
    ],
    "abstract": "Internet memes have emerged as a popular multimodal medium, yet they are increasingly weaponized to convey harmful opinions through subtle rhetorical devices like irony and metaphor. Existing detection approaches, including MLLM-based techniques, struggle with these implicit expressions, leading to frequent misjudgments. This paper introduces PatMD, a novel approach that improves harmful meme detection by learning from and proactively mitigating these potential misjudgment risks. Our core idea is to move beyond superficial content-level matching and instead identify the underlying misjudgment risk patterns, proactively guiding the MLLMs to avoid known misjudgment pitfalls. We first construct a knowledge base where each meme is deconstructed into a misjudgment risk pattern explaining why it might be misjudged, either overlooking harmful undertones (false negative) or overinterpreting benign content (false positive). For a given target meme, PatMD retrieves relevant patterns and utilizes them to dynamically guide the MLLM's reasoning. Experiments on a benchmark of 6,626 memes across 5 harmful detection tasks show that PatMD outperforms state-of-the-art baselines, achieving an average of 8.30\\% improvement in F1-score and 7.71\\% improvement in accuracy, demonstrating strong generalizability and improved detection capability of harmful memes.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "The paper has something wrong and need to be corrected",
    "pdf_url": "https://arxiv.org/pdf/2510.15946v2",
    "published_date": "2025-10-10 03:08:30 UTC",
    "updated_date": "2025-10-21 03:40:37 UTC"
  },
  {
    "arxiv_id": "2510.13827v2",
    "title": "Bridging the Semantic Gap: Contrastive Rewards for Multilingual Text-to-SQL with GRPO",
    "authors": [
      "Ashish Kattamuri",
      "Ishita Prasad",
      "Meetu Malhotra",
      "Arpita Vats",
      "Rahul Raja",
      "Albert Lie"
    ],
    "abstract": "Current Text-to-SQL methods are evaluated and only focused on executable queries, overlooking the semantic alignment challenge -- both in terms of the semantic meaning of the query and the correctness of the execution results. Even execution accuracy itself shows significant drops when moving from English to other languages, with an average decline of 6 percentage points across non-English languages. We address these challenges by presenting a new framework that combines Group Relative Policy Optimization (GRPO) within a multilingual contrastive reward signal to enhance both task efficiency and semantic accuracy in Text-to-SQL systems in cross-lingual scenarios. Our method teaches models to obtain better correspondence between SQL generation and user intent by combining a reward signal based on semantic similarity. On the seven-language MultiSpider dataset, fine-tuning the LLaMA-3-3B model with GRPO improved the execution accuracy up to 87.4 percent (+26 pp over zero-shot) and semantic accuracy up to 52.29 percent (+32.86 pp). Adding our contrastive reward signal in the GRPO framework further improved the average semantic accuracy to 59.14 percent (+6.85 pp, up to +10 pp for Vietnamese). Our experiments showcase that a smaller, parameter-efficient 3B LLaMA model fine-tuned with our contrastive reward signal outperforms a much larger zero-shot 8B LLaMA model, with an uplift of 7.43 pp in execution accuracy (from 81.43 percent on the 8B model to 88.86 percent on the 3B model), and nearly matches its semantic accuracy (59.14 percent vs. 68.57 percent) -- all using just 3,000 reinforcement learning training examples. These results demonstrate how we can improve the performance of Text-to-SQL systems with contrastive rewards for directed semantic alignment, without requiring large-scale training datasets.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "20th International Workshop on Semantic and Social Media Adaptation & Personalization",
    "pdf_url": "https://arxiv.org/pdf/2510.13827v2",
    "published_date": "2025-10-10 03:07:55 UTC",
    "updated_date": "2025-11-21 02:06:39 UTC"
  },
  {
    "arxiv_id": "2510.08958v1",
    "title": "EcphoryRAG: Re-Imagining Knowledge-Graph RAG via Human Associative Memory",
    "authors": [
      "Zirui Liao"
    ],
    "abstract": "Cognitive neuroscience research indicates that humans leverage cues to activate entity-centered memory traces (engrams) for complex, multi-hop recollection. Inspired by this mechanism, we introduce EcphoryRAG, an entity-centric knowledge graph RAG framework. During indexing, EcphoryRAG extracts and stores only core entities with corresponding metadata, a lightweight approach that reduces token consumption by up to 94\\% compared to other structured RAG systems. For retrieval, the system first extracts cue entities from queries, then performs a scalable multi-hop associative search across the knowledge graph. Crucially, EcphoryRAG dynamically infers implicit relations between entities to populate context, enabling deep reasoning without exhaustive pre-enumeration of relationships. Extensive evaluations on the 2WikiMultiHop, HotpotQA, and MuSiQue benchmarks demonstrate that EcphoryRAG sets a new state-of-the-art, improving the average Exact Match (EM) score from 0.392 to 0.474 over strong KG-RAG methods like HippoRAG. These results validate the efficacy of the entity-cue-multi-hop retrieval paradigm for complex question answering.",
    "categories": [
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08958v1",
    "published_date": "2025-10-10 03:07:27 UTC",
    "updated_date": "2025-10-10 03:07:27 UTC"
  },
  {
    "arxiv_id": "2510.08956v2",
    "title": "A Human Behavioral Baseline for Collective Governance in Software Projects",
    "authors": [
      "Mobina Noori",
      "Mahasweta Chakraborti",
      "Amy X Zhang",
      "Seth Frey"
    ],
    "abstract": "We study how open source communities describe participation and control through version controlled governance documents. Using a corpus of 710 projects with paired snapshots, we parse text into actors, rules, actions, and objects, then group them and measure change with entropy for evenness, richness for diversity, and Jensen Shannon divergence for drift. Projects define more roles and more actions over time, and these are distributed more evenly, while the composition of rules remains stable. These findings indicate that governance grows by expanding and balancing categories of participation without major shifts in prescriptive force. The analysis provides a reproducible baseline for evaluating whether future AI mediated workflows concentrate or redistribute authority.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Algorithmic Collective Action Workshop @ NeurIPS 2025. arXiv admin note: text overlap with arXiv:2509.16295",
    "pdf_url": "https://arxiv.org/pdf/2510.08956v2",
    "published_date": "2025-10-10 03:04:46 UTC",
    "updated_date": "2025-11-17 02:57:26 UTC"
  },
  {
    "arxiv_id": "2510.08948v2",
    "title": "SHERLOCK: Towards Dynamic Knowledge Adaptation in LLM-enhanced E-commerce Risk Management",
    "authors": [
      "Nan Lu",
      "Yurong Hu",
      "Jiaquan Fang",
      "Yan Liu",
      "Rui Dong",
      "Yiming Wang",
      "Rui Lin",
      "Shaoyi Xu"
    ],
    "abstract": "The growth of the e-commerce industry has intensified the adversarial dynamics between shadow economy actors and risk management teams. Companies often conduct risk investigations into suspicious cases to identify emerging fraud patterns, thereby enhancing both preemptive risk prevention and post-hoc governance. However, the sheer volume of case analyses imposes a substantial workload on risk management analysts, as each case requires the integration of long-term expert experience and meticulous scrutiny across multiple risk dimensions. Additionally, individual disparities among analysts hinder the establishment of uniform and high-standard workflows. To address these challenges, we propose the SHERLOCK framework, which leverages the reasoning capabilities of large language models (LLMs) to assist analysts in risk investigations. Our approach consists of three primary components: (1) extracting risk management knowledge from multi-modal data and constructing a domain knowledge base (KB), (2) building an intelligent platform guided by the data flywheel paradigm that integrates daily operations, expert annotations, and model evaluations, with iteratively fine-tuning for preference alignment, and (3) introducing a Reflect & Refine (R&R) module that collaborates with the domain KB to establish a rapid response mechanism for evolving risk patterns. Experiments conducted on the real-world transaction dataset from JD dot com demonstrate that our method significantly improves the precision of both factual alignment and risk localization within the LLM analysis results. Deployment of the SHERLOCK-based LLM system on JD dot com has substantially enhanced the efficiency of case investigation workflows for risk managers.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08948v2",
    "published_date": "2025-10-10 02:57:58 UTC",
    "updated_date": "2025-10-13 02:26:39 UTC"
  },
  {
    "arxiv_id": "2510.08945v2",
    "title": "FATHOMS-RAG: A Framework for the Assessment of Thinking and Observation in Multimodal Systems that use Retrieval Augmented Generation",
    "authors": [
      "Samuel Hildebrand",
      "Curtis Taylor",
      "Sean Oesch",
      "James M Ghawaly",
      "Amir Sadovnik",
      "Ryan Shivers",
      "Brandon Schreiber",
      "Kevin Kurian"
    ],
    "abstract": "Retrieval-augmented generation (RAG) has emerged as a promising paradigm for improving factual accuracy in large language models (LLMs). We introduce a benchmark designed to evaluate RAG pipelines as a whole, evaluating a pipeline's ability to ingest, retrieve, and reason about several modalities of information, differentiating it from existing benchmarks that focus on particular aspects such as retrieval. We present (1) a small, human-created dataset of 93 questions designed to evaluate a pipeline's ability to ingest textual data, tables, images, and data spread across these modalities in one or more documents; (2) a phrase-level recall metric for correctness; (3) a nearest-neighbor embedding classifier to identify potential pipeline hallucinations; (4) a comparative evaluation of 2 pipelines built with open-source retrieval mechanisms and 4 closed-source foundation models; and (5) a third-party human evaluation of the alignment of our correctness and hallucination metrics. We find that closed-source pipelines significantly outperform open-source pipelines in both correctness and hallucination metrics, with wider performance gaps in questions relying on multimodal and cross-document information. Human evaluation of our metrics showed average agreement of 4.62 for correctness and 4.53 for hallucination detection on a 1-5 Likert scale (5 indicating \"strongly agree\").",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Submitted to 2026 International Conference on Language Resources and Evaluation (LREC2026)",
    "pdf_url": "https://arxiv.org/pdf/2510.08945v2",
    "published_date": "2025-10-10 02:51:47 UTC",
    "updated_date": "2025-11-19 20:56:32 UTC"
  },
  {
    "arxiv_id": "2510.09709v1",
    "title": "The Idola Tribus of AI: Large Language Models tend to perceive order where none exists",
    "authors": [
      "Shin-nosuke Ishikawa",
      "Masato Todo",
      "Taiki Ogihara",
      "Hirotsugu Ohba"
    ],
    "abstract": "We present a tendency of large language models (LLMs) to generate absurd patterns despite their clear inappropriateness in a simple task of identifying regularities in number series. Several approaches have been proposed to apply LLMs to complex real-world tasks, such as providing knowledge through retrieval-augmented generation and executing multi-step tasks using AI agent frameworks. However, these approaches rely on the logical consistency and self-coherence of LLMs, making it crucial to evaluate these aspects and consider potential countermeasures. To identify cases where LLMs fail to maintain logical consistency, we conducted an experiment in which LLMs were asked to explain the patterns in various integer sequences, ranging from arithmetic sequences to randomly generated integer series. While the models successfully identified correct patterns in arithmetic and geometric sequences, they frequently over-recognized patterns that were inconsistent with the given numbers when analyzing randomly generated series. This issue was observed even in multi-step reasoning models, including OpenAI o3, o4-mini, and Google Gemini 2.5 Flash Preview Thinking. This tendency to perceive non-existent patterns can be interpreted as the AI model equivalent of Idola Tribus and highlights potential limitations in their capability for applied tasks requiring logical reasoning, even when employing chain-of-thought reasoning mechanisms.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "14 pages, 3 figures, accepted to Findings of EMNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.09709v1",
    "published_date": "2025-10-10 02:51:15 UTC",
    "updated_date": "2025-10-10 02:51:15 UTC"
  },
  {
    "arxiv_id": "2510.08936v1",
    "title": "RO-Bench: Large-scale robustness evaluation of MLLMs with text-driven counterfactual videos",
    "authors": [
      "Zixi Yang",
      "Jiapeng Li",
      "Muxi Diao",
      "Yinuo Jing",
      "Kongming Liang"
    ],
    "abstract": "Recently, Multi-modal Large Language Models (MLLMs) have demonstrated significant performance across various video understanding tasks. However, their robustness, particularly when faced with manipulated video content, remains largely unexplored. In this paper, we introduce Ro-Bench, the first benchmark for evaluating MLLMs on dynamic out-of-distribution (OOD) counterfactual video test sets. Ro-Bench incorporates high-quality, diverse and temporally relevant video data, by editing Style, Object, Background and their compositions. We evaluated eight recent video MLLMs and found that current models exhibit substantial performance degradation on Ro-Bench when exposed to counterfactual video content. Furthermore, we demonstrate that fine-tuning MLLMs with counterfactual data enhances robustness, achieving a 21.73% performance increase on Ro-Bench and a 12.78% improvement across 20 tasks in the MVBench dataset. These findings underscore the effectiveness of counterfactual data in enhancing the video understanding ability of MLLMs. The code and data will be released shortly.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08936v1",
    "published_date": "2025-10-10 02:26:48 UTC",
    "updated_date": "2025-10-10 02:26:48 UTC"
  },
  {
    "arxiv_id": "2510.08931v1",
    "title": "RADAR: Mechanistic Pathways for Detecting Data Contamination in LLM Evaluation",
    "authors": [
      "Ashish Kattamuri",
      "Harshwardhan Fartale",
      "Arpita Vats",
      "Rahul Raja",
      "Ishita Prasad"
    ],
    "abstract": "Data contamination poses a significant challenge to reliable LLM evaluation, where models may achieve high performance by memorizing training data rather than demonstrating genuine reasoning capabilities. We introduce RADAR (Recall vs. Reasoning Detection through Activation Representation), a novel framework that leverages mechanistic interpretability to detect contamination by distinguishing recall-based from reasoning-based model responses. RADAR extracts 37 features spanning surface-level confidence trajectories and deep mechanistic properties including attention specialization, circuit dynamics, and activation flow patterns. Using an ensemble of classifiers trained on these features, RADAR achieves 93\\% accuracy on a diverse evaluation set, with perfect performance on clear cases and 76.7\\% accuracy on challenging ambiguous examples. This work demonstrates the potential of mechanistic interpretability for advancing LLM evaluation beyond traditional surface-level metrics.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "NeurIPS 2025 Workshop on Evaluating the Evolving LLM Lifecycle: Benchmarks, Emergent Abilities, and Scaling",
    "pdf_url": "https://arxiv.org/pdf/2510.08931v1",
    "published_date": "2025-10-10 02:21:45 UTC",
    "updated_date": "2025-10-10 02:21:45 UTC"
  },
  {
    "arxiv_id": "2510.08930v1",
    "title": "Co-Authoring the Self: A Human-AI Interface for Interest Reflection in Recommenders",
    "authors": [
      "Ruixuan Sun",
      "Junyuan Wang",
      "Sanjali Roy",
      "Joseph A. Konstan"
    ],
    "abstract": "Natural language-based user profiles in recommender systems have been explored for their interpretability and potential to help users scrutinize and refine their interests, thereby improving recommendation quality. Building on this foundation, we introduce a human-AI collaborative profile for a movie recommender system that presents editable personalized interest summaries of a user's movie history. Unlike static profiles, this design invites users to directly inspect, modify, and reflect on the system's inferences. In an eight-week online field deployment with 1775 active movie recommender users, we find persistent gaps between user-perceived and system-inferred interests, show how the profile encourages engagement and reflection, and identify design directions for leveraging imperfect AI-powered user profiles to stimulate more user intervention and build more transparent and trustworthy recommender experiences.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08930v1",
    "published_date": "2025-10-10 02:20:13 UTC",
    "updated_date": "2025-10-10 02:20:13 UTC"
  },
  {
    "arxiv_id": "2510.08928v1",
    "title": "LM Fight Arena: Benchmarking Large Multimodal Models via Game Competition",
    "authors": [
      "Yushuo Zheng",
      "Zicheng Zhang",
      "Xiongkuo Min",
      "Huiyu Duan",
      "Guangtao Zhai"
    ],
    "abstract": "Existing benchmarks for large multimodal models (LMMs) often fail to capture their performance in real-time, adversarial environments. We introduce LM Fight Arena (Large Model Fight Arena), a novel framework that evaluates LMMs by pitting them against each other in the classic fighting game Mortal Kombat II, a task requiring rapid visual understanding and tactical, sequential decision-making. In a controlled tournament, we test six leading open- and closed-source models, where each agent operates controlling the same character to ensure a fair comparison. The models are prompted to interpret game frames and state data to select their next actions. Unlike static evaluations, LM Fight Arena provides a fully automated, reproducible, and objective assessment of an LMM's strategic reasoning capabilities in a dynamic setting. This work introduces a challenging and engaging benchmark that bridges the gap between AI evaluation and interactive entertainment.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08928v1",
    "published_date": "2025-10-10 02:19:21 UTC",
    "updated_date": "2025-10-10 02:19:21 UTC"
  },
  {
    "arxiv_id": "2510.08908v1",
    "title": "A Frequency-Domain Analysis of the Multi-Armed Bandit Problem: A New Perspective on the Exploration-Exploitation Trade-off",
    "authors": [
      "Di Zhang"
    ],
    "abstract": "The stochastic multi-armed bandit (MAB) problem is one of the most fundamental models in sequential decision-making, with the core challenge being the trade-off between exploration and exploitation. Although algorithms such as Upper Confidence Bound (UCB) and Thompson Sampling, along with their regret theories, are well-established, existing analyses primarily operate from a time-domain and cumulative regret perspective, struggling to characterize the dynamic nature of the learning process. This paper proposes a novel frequency-domain analysis framework, reformulating the bandit process as a signal processing problem. Within this framework, the reward estimate of each arm is viewed as a spectral component, with its uncertainty corresponding to the component's frequency, and the bandit algorithm is interpreted as an adaptive filter. We construct a formal Frequency-Domain Bandit Model and prove the main theorem: the confidence bound term in the UCB algorithm is equivalent in the frequency domain to a time-varying gain applied to uncertain spectral components, a gain inversely proportional to the square root of the visit count. Based on this, we further derive finite-time dynamic bounds concerning the exploration rate decay. This theory not only provides a novel and intuitive physical interpretation for classical algorithms but also lays a rigorous theoretical foundation for designing next-generation algorithms with adaptive parameter adjustment.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.08908v1",
    "published_date": "2025-10-10 01:45:14 UTC",
    "updated_date": "2025-10-10 01:45:14 UTC"
  },
  {
    "arxiv_id": "2510.08902v1",
    "title": "A Unified Biomedical Named Entity Recognition Framework with Large Language Models",
    "authors": [
      "Tengxiao Lv",
      "Ling Luo",
      "Juntao Li",
      "Yanhua Wang",
      "Yuchen Pan",
      "Chao Liu",
      "Yanan Wang",
      "Yan Jiang",
      "Huiyi Lv",
      "Yuanyuan Sun",
      "Jian Wang",
      "Hongfei Lin"
    ],
    "abstract": "Accurate recognition of biomedical named entities is critical for medical information extraction and knowledge discovery. However, existing methods often struggle with nested entities, entity boundary ambiguity, and cross-lingual generalization. In this paper, we propose a unified Biomedical Named Entity Recognition (BioNER) framework based on Large Language Models (LLMs). We first reformulate BioNER as a text generation task and design a symbolic tagging strategy to jointly handle both flat and nested entities with explicit boundary annotation. To enhance multilingual and multi-task generalization, we perform bilingual joint fine-tuning across multiple Chinese and English datasets. Additionally, we introduce a contrastive learning-based entity selector that filters incorrect or spurious predictions by leveraging boundary-sensitive positive and negative samples. Experimental results on four benchmark datasets and two unseen corpora show that our method achieves state-of-the-art performance and robust zero-shot generalization across languages. The source codes are freely available at https://github.com/dreamer-tx/LLMNER.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted as a short paper at BIBM2025",
    "pdf_url": "https://arxiv.org/pdf/2510.08902v1",
    "published_date": "2025-10-10 01:33:54 UTC",
    "updated_date": "2025-10-10 01:33:54 UTC"
  },
  {
    "arxiv_id": "2510.08899v1",
    "title": "Pinpointing crucial steps: Attribution-based Credit Assignment for Verifiable Reinforcement Learning",
    "authors": [
      "Junxi Yin",
      "Haisen Luo",
      "Zhenyu Li",
      "Yihua Liu",
      "Dan Liu",
      "Zequn Li",
      "Xiaohang Xu"
    ],
    "abstract": "While Reinforcement Learning with Verifiable Rewards (RLVR) enhances complex reasoning in LLMs, current methods struggle to balance exploration and exploitation. This leads to critical issues like inaccurate credit assignment for intermediate steps and premature entropy collapse, limiting model performance. To address this, we introduce Attribution-based Contribution to Policy Optimization (ACPO), a phased framework that incorporates a difficulty-aware curriculum. ACPO improves exploration by using trajectory semantic segmentation and an attribution-based representation to dynamically regulate policy entropy, thus mitigating its collapse. Concurrently, it enhances exploitation with a factorized reward system that precisely quantifies the hierarchical contribution of each reasoning step, ensuring accurate credit assignment. Extensive experiments on challenging benchmarks, including AIME, MATH, and AMC, demonstrate that ACPO significantly outperforms existing state-of-the-art approaches.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.08899v1",
    "published_date": "2025-10-10 01:22:55 UTC",
    "updated_date": "2025-10-10 01:22:55 UTC"
  },
  {
    "arxiv_id": "2510.08896v1",
    "title": "HES-SQL: Hybrid Reasoning for Efficient Text-to-SQL with Structural Skeleton Guidance",
    "authors": [
      "Suming Qiu",
      "Jing Li",
      "Zhicheng Zhou",
      "Junjie Huang",
      "Linyuan Qiu",
      "Zhijie Sun"
    ],
    "abstract": "We present HES-SQL, a novel hybrid training framework that advances Text-to-SQL generation through the integration of thinking-mode-fused supervised fine-tuning (SFT) with Group Relative Policy Optimization (GRPO). Our approach introduces three key innovations: (1) a skeleton-completeness scoring mechanism that enhances preference alignment between generated queries and optimal SQL structures; (2) a query-latency-aware reward system that incentivizes the generation of computationally efficient SQL queries; (3) a self-distillation process for thinking-mode completion that prevents degradation of the model's reasoning capabilities. This framework enables hybrid thinking models to switch between reasoning and non-reasoning modes while improving SQL query accuracy and execution efficiency.\n  Experimental evaluation, conducted on MySQL 8.0 and SQLite 3.42 under controlled single-user conditions, demonstrates that HES-SQL achieves competitive performance with execution accuracies of 79.14\\% and 54.9\\% on the BIRD and KaggleDBQA benchmarks, respectively. Query latency is measured as the end-to-end execution time of generated queries on the DBMS, averaged over multiple runs to mitigate variance. Efficiency gains range from 11\\% to 20\\% relative to supervised baselines. Our results establish a new paradigm for Text-to-SQL systems that effectively balances semantic accuracy with computational efficiency through execution-informed reinforcement learning (RL). The proposed methodology has significant implications for developing robust natural language interfaces to databases and can be extended to broader structured generation tasks requiring both correctness and efficiency optimization.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08896v1",
    "published_date": "2025-10-10 01:15:57 UTC",
    "updated_date": "2025-10-10 01:15:57 UTC"
  },
  {
    "arxiv_id": "2510.08892v1",
    "title": "Exploring Multi-Temperature Strategies for Token- and Rollout-Level Control in RLVR",
    "authors": [
      "Haomin Zhuang",
      "Yujun Zhou",
      "Taicheng Guo",
      "Yue Huang",
      "Fangxu Liu",
      "Kai Song",
      "Xiangliang Zhang"
    ],
    "abstract": "Reinforcement Learning has demonstrated substantial improvements in the reasoning abilities of Large Language Models (LLMs), exhibiting significant applicability across various domains. Recent research has identified that tokens within LLMs play distinct roles during reasoning tasks, categorizing them into high-entropy reasoning tokens and low-entropy knowledge tokens. Prior approaches have typically focused on restricting updates to indirectly encourage exploration, yet they do not explicitly facilitate exploratory behavior during the token generation stage itself. In this work, we introduce a complementary approach that explicitly promotes exploration during sampling by applying distinct temperature settings for different token types. Specifically, our method employs higher temperatures for reasoning tokens to actively encourage exploration, while retaining lower temperatures for knowledge tokens to maintain factual correctness. Furthermore, we systematically investigate various multi-temperature scheduling strategies and their impacts within reinforcement learning contexts. Empirical evaluations on several reasoning benchmarks demonstrate that our approach significantly enhances the reasoning performance of LLMs. The code is available at https://github.com/zhmzm/Multi_Temperature_Verl.git.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08892v1",
    "published_date": "2025-10-10 01:11:42 UTC",
    "updated_date": "2025-10-10 01:11:42 UTC"
  },
  {
    "arxiv_id": "2510.08891v1",
    "title": "Designing and Evaluating an AI-driven Immersive Multidisciplinary Simulation (AIMS) for Interprofessional Education",
    "authors": [
      "Ruijie Wang",
      "Jie Lu",
      "Bo Pei",
      "Evonne Jones",
      "Jamey Brinson",
      "Timothy Brown"
    ],
    "abstract": "Interprofessional education has long relied on case studies and the use of standardized patients to support teamwork, communication, and related collaborative competencies among healthcare professionals. However, traditional approaches are often limited by cost, scalability, and inability to mimic the dynamic complexity of real-world clinical scenarios. To address these challenges, we designed and developed AIMS (AI-Enhanced Immersive Multidisciplinary Simulations), a virtual simulation that integrates a large language model (Gemini-2.5-Flash), a Unity-based virtual environment engine, and a character creation pipeline to support synchronized, multimodal interactions between the user and the virtual patient. AIMS was designed to enhance collaborative clinical reasoning and health promotion competencies among students from pharmacy, medicine, nursing, and social work. A formal usability testing session was conducted which participants assumed professional roles on a healthcare team and engaged in a mix of scripted and unscripted conversations. Participants explored the patient's symptoms, social context, and care needs. Usability issues were identified (e.g., audio routing, response latency) and used to guide subsequent refinements. Findings in general suggest that AIMS supports realistic, profession-specific and contextually appropriate conversations. We discussed both technical and pedagogical innovations of AIMS and concluded with future directions.",
    "categories": [
      "cs.ET",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.ET",
    "comment": "15 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.08891v1",
    "published_date": "2025-10-10 01:09:18 UTC",
    "updated_date": "2025-10-10 01:09:18 UTC"
  },
  {
    "arxiv_id": "2510.08878v2",
    "title": "ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling",
    "authors": [
      "Yuxuan Jiang",
      "Zehua Chen",
      "Zeqian Ju",
      "Yusheng Dai",
      "Weibei Dou",
      "Jun Zhu"
    ],
    "abstract": "Text-to-audio (TTA) generation with fine-grained control signals, e.g., precise timing control or intelligible speech content, has been explored in recent works. However, constrained by data scarcity, their generation performance at scale is still compromised. In this study, we recast controllable TTA generation as a multi-task learning problem and introduce a progressive diffusion modeling approach, ControlAudio. Our method adeptly fits distributions conditioned on more fine-grained information, including text, timing, and phoneme features, through a step-by-step strategy. First, we propose a data construction method spanning both annotation and simulation, augmenting condition information in the sequence of text, timing, and phoneme. Second, at the model training stage, we pretrain a diffusion transformer (DiT) on large-scale text-audio pairs, achieving scalable TTA generation, and then incrementally integrate the timing and phoneme features with unified semantic representations, expanding controllability. Finally, at the inference stage, we propose progressively guided generation, which sequentially emphasizes more fine-grained information, aligning inherently with the coarse-to-fine sampling nature of DiT. Extensive experiments show that ControlAudio achieves state-of-the-art performance in terms of temporal accuracy and speech clarity, significantly outperforming existing methods on both objective and subjective evaluations. Demo samples are available at: https://control-audio.github.io/Control-Audio.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "18 pages, 8 tables, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.08878v2",
    "published_date": "2025-10-10 00:19:41 UTC",
    "updated_date": "2025-12-25 16:05:43 UTC"
  },
  {
    "arxiv_id": "2510.09706v1",
    "title": "A Demonstration of Self-Adaptive Jamming Attack Detection in AI/ML Integrated O-RAN",
    "authors": [
      "Md Habibur Rahman",
      "Md Sharif Hossen",
      "Nathan H. Stephenson",
      "Vijay K. Shah",
      "Aloizio Da Silva"
    ],
    "abstract": "The open radio access network (O-RAN) enables modular, intelligent, and programmable 5G network architectures through the adoption of software-defined networking, network function virtualization, and implementation of standardized open interfaces. However, one of the security concerns for O-RAN, which can severely undermine network performance, is jamming attacks. This paper presents SAJD- a self-adaptive jammer detection framework that autonomously detects jamming attacks in AI/ML framework-integrated ORAN environments without human intervention. The SAJD framework forms a closed-loop system that includes near-realtime inference of radio signal jamming via our developed ML-based xApp, as well as continuous monitoring and retraining pipelines through rApps. In this demonstration, we will show how SAJD outperforms state-of-the-art jamming detection xApp (offline trained with manual labels) in terms of accuracy and adaptability under various dynamic and previously unseen interference scenarios in the O-RAN-compliant testbed.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "2 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.09706v1",
    "published_date": "2025-10-10 00:18:00 UTC",
    "updated_date": "2025-10-10 00:18:00 UTC"
  },
  {
    "arxiv_id": "2510.08876v1",
    "title": "Vector Graph-Based Repository Understanding for Issue-Driven File Retrieval",
    "authors": [
      "Kostiantyn Bevziuk",
      "Andrii Fatula",
      "Svetozar Lashin Yaroslav Opanasenko",
      "Anna Tukhtarova",
      "Ashok Jallepalli Pradeepkumar Sharma",
      "Hritvik Shrivastava"
    ],
    "abstract": "We present a repository decomposition system that converts large software repositories into a vectorized knowledge graph which mirrors project architectural and semantic structure, capturing semantic relationships and allowing a significant level of automatization of further repository development. The graph encodes syntactic relations such as containment, implementation, references, calls, and inheritance, and augments nodes with LLM-derived summaries and vector embeddings. A hybrid retrieval pipeline combines semantic retrieval with graph-aware expansion, and an LLM-based assistant formulates constrained, read-only graph requests and produces human-oriented explanations.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08876v1",
    "published_date": "2025-10-10 00:13:50 UTC",
    "updated_date": "2025-10-10 00:13:50 UTC"
  },
  {
    "arxiv_id": "2510.17843v1",
    "title": "GRETEL: A Goal-driven Retrieval and Execution-based Trial Framework for LLM Tool Selection Enhancing",
    "authors": [
      "Zongze Wu",
      "Yani Guo",
      "Churong Liang",
      "Runnan Li"
    ],
    "abstract": "Despite remarkable advances in Large Language Model capabilities, tool retrieval for agent-based systems remains fundamentally limited by reliance on semantic similarity, which fails to capture functional viability. Current methods often retrieve textually relevant but functionally inoperative tools due to parameter mismatches, authentication failures, and execution constraints--a phenomenon we term the semantic-functional gap. We introduce GRETEL, to address this gap through systematic empirical validation. GRETEL implements an agentic workflow that processes semantically retrieved candidates through sandboxed plan-execute-evaluate cycles, generating execution-grounded evidence to distinguish truly functional tools from merely descriptive matches. Our comprehensive evaluation on the ToolBench benchmark demonstrates substantial improvements across all metrics: Pass Rate (at 10) increases from 0.690 to 0.826, Recall (at 10) improves from 0.841 to 0.867, and NDCG (at 10) rises from 0.807 to 0.857.. These results establish that execution-based validation provides a more reliable foundation for tool selection than semantic similarity alone, enabling more robust agent performance in real-world applications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.LG",
    "comment": "5 pages, 1 figures, 5 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.17843v1",
    "published_date": "2025-10-10 00:12:51 UTC",
    "updated_date": "2025-10-10 00:12:51 UTC"
  },
  {
    "arxiv_id": "2510.08874v1",
    "title": "Slicing Is All You Need: Towards A Universal One-Sided Algorithm for Distributed Matrix Multiplication",
    "authors": [
      "Benjamin Brock",
      "Renato Golin"
    ],
    "abstract": "Many important applications across science, data analytics, and AI workloads depend on distributed matrix multiplication. Prior work has developed a large array of algorithms suitable for different problem sizes and partitionings including 1D, 2D, 1.5D, and 2.5D algorithms. A limitation of current work is that existing algorithms are limited to a subset of partitionings. Multiple algorithm implementations are required to support the full space of possible partitionings. If no algorithm implementation is available for a particular set of partitionings, one or more operands must be redistributed, increasing communication costs. This paper presents a universal one-sided algorithm for distributed matrix multiplication that supports all combinations of partitionings and replication factors. Our algorithm uses slicing (index arithmetic) to compute the sets of overlapping tiles that must be multiplied together. This list of local matrix multiplies can then either be executed directly, or reordered and lowered to an optimized IR to maximize overlap. We implement our algorithm using a high-level C++-based PGAS programming framework that performs direct GPU-to-GPU communication using intra-node interconnects. We evaluate performance for a wide variety of partitionings and replication factors, finding that our work is competitive with PyTorch DTensor, a highly optimized distributed tensor library targeting AI models.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.08874v1",
    "published_date": "2025-10-10 00:11:39 UTC",
    "updated_date": "2025-10-10 00:11:39 UTC"
  },
  {
    "arxiv_id": "2511.17519v1",
    "title": "SAJD: Self-Adaptive Jamming Attack Detection in AI/ML Integrated 5G O-RAN Networks",
    "authors": [
      "Md Habibur Rahman",
      "Md Sharif Hossen",
      "Nathan H. Stephenson",
      "Vijay K. Shah",
      "Aloizio Da Silva"
    ],
    "abstract": "The open radio access network (O-RAN) enables modular, intelligent, and programmable 5G network architectures through the adoption of software-defined networking (SDN), network function virtualization (NFV), and implementation of standardized open interfaces. It also facilitates closed loop control and (non/near) real-time optimization of radio access network (RAN) through the integration of non-real-time applications (rApps) and near-real-time applications (xApps). However, one of the security concerns for O-RAN that can severely undermine network performance and subject it to a prominent threat to the security & reliability of O-RAN networks is jamming attacks. To address this, we introduce SAJD-a self-adaptive jammer detection framework that autonomously detects jamming attacks in artificial intelligence (AI) / machine learning (ML)-integrated O-RAN environments. The SAJD framework forms a closed-loop system that includes near-real-time inference of radio signal jamming interference via our developed ML-based xApp, as well as continuous monitoring and retraining pipelines through rApps. Specifically, a labeler rApp is developed that uses live telemetry (i.e., KPIs) to detect model drift, triggers unsupervised data labeling, executes model training/retraining using the integrated & open-source ClearML framework, and updates deployed models on the fly, without service disruption. Experiments on O-RAN-compliant testbed demonstrate that the SAJD framework outperforms state-of-the-art (offline-trained with manual labels) jamming detection approach in accuracy and adaptability under various dynamic and previously unseen interference scenarios.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "6 pages, 5 figures, IEEE Military Communications Conference",
    "pdf_url": "https://arxiv.org/pdf/2511.17519v1",
    "published_date": "2025-10-10 00:09:09 UTC",
    "updated_date": "2025-10-10 00:09:09 UTC"
  },
  {
    "arxiv_id": "2510.08872v3",
    "title": "GTAlign: Game-Theoretic Alignment of LLM Assistants for Social Welfare",
    "authors": [
      "Siqi Zhu",
      "David Zhang",
      "Pedro Cisneros-Velarde",
      "Jiaxuan You"
    ],
    "abstract": "Large Language Models (LLMs) have achieved remarkable progress in reasoning, yet sometimes produce responses that are suboptimal for users in tasks such as writing, information seeking, or providing practical guidance. Conventional alignment practices typically assume that maximizing model reward also maximizes user welfare, but this assumption frequently fails in practice: models may over-clarify or generate overly verbose reasoning when users prefer concise answers. Such behaviors resemble the prisoner's dilemma, where individually rational choices lead to socially suboptimal outcomes. The fundamental challenge is the lack of a principled decision making mechanism that mutually benefits both the LLM and the user. We propose Game-Theoretic Alignment (GTAlign), an alignment framework that integrates game-theoretic decision making into both reasoning and training. During reasoning, the model explicitly treats user-LLM interaction as a strategic game: it constructs payoff matrices within its reasoning chain to estimate welfare for both itself and the user, and then selects actions that are mutually beneficial. During training, we introduce a social welfare reward that reinforces cooperative responses, aligning model behavior with socially efficient outcomes. In addition, we introduce an inference technique that leverages game-theoretic reasoning to dynamically adapt LLM's response when pricing policies of LLM service change. Extensive experiments demonstrate that GTAlign substantially improves reasoning efficiency, answer quality, and social welfare compared to baselines across diverse tasks. The code is available at https://github.com/ulab-uiuc/GTAlign .",
    "categories": [
      "cs.AI",
      "cs.GT",
      "cs.HC",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "31 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.08872v3",
    "published_date": "2025-10-10 00:05:14 UTC",
    "updated_date": "2025-11-03 18:54:17 UTC"
  }
]
{
  "date": "2024-11-29",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-11-29 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 更新了 85 篇论文，主要聚焦于 AI 模型优化、多模态学习、强化学习和实际应用等领域，其中 LLM 的推理增强、RAG 框架改进，以及机器人和医疗领域的创新最为引人注目，例如涉及知名学者 Mohit Bansal 的“Reverse Thinking Makes LLMs Stronger Reasoners”论文，展示了 LLM 在复杂推理任务中的潜力。\n\n下面，我将挑选并讨论部分关键论文，先优先聊那些创新性强、可能引发话题的文章（如 AI 和机器人领域），并将相关主题归类讨论。对于其他较常规或次要论文，我会快速掠过，只简要概述其核心点。每个论文标题以“中文 + 英文”形式列出，保留核心学术术语。\n\n### AI 模型优化与 RAG 系统\n这些论文突出了 LLM 和 RAG 在推理、生成和知识融合方面的进展，代表了当前 AI 研究的热点。\n\n- **Reverse Thinking Makes LLMs Stronger Reasoners（逆向思考使 LLM 成为更强的推理器）**：这篇论文由 Mohit Bansal 等学者主导，主要贡献是通过逆向思考框架 RevThink 来增强 LLM 的数学和逻辑推理能力。作者提出数据增强和多任务学习目标，使模型在 GSM8K 和 MATH500 数据集上平均提升 6.84% 的性能，强调了逆向推理在减少错误中的作用，令人印象深刻。\n  \n- **o1-Coder: an o1 Replication for Coding（o1-Coder：针对编码任务的 o1 复制）**：Yuxiang Zhang 等团队构建了一个基于强化学习和 Monte Carlo Tree Search 的 LLM 框架，用于代码生成。关键发现是它能模拟 OpenAI o1 模型，支持任务分解和迭代优化，实验显示在 API-bank 数据集上性能提升显著，适合实际编程应用。\n\n- **JetFormer: An Autoregressive Generative Model of Raw Images and Text（JetFormer：原始图像和文本的自回归生成模型）**：Michael Tschannen 等作者提出一个解码器-only Transformer 模型，用于多模态生成。贡献在于直接最大化原始数据的似然，避免预训练组件，实验在图像生成和理解任务上达到 SOTA 水平，展示了高效的多模态处理潜力。\n\n- **RAGDiffusion: Faithful Cloth Generation via External Knowledge Assimilation（RAGDiffusion：通过外部知识整合的忠实布料生成）**：Xianfeng Tan 等团队开发了 RAG 框架，用于图像生成。论文的核心是检索增强生成，引入结构聚合和多级对齐，显著提高了生成图像的结构和细节保真度，在真实数据集上表现优异。\n\n其他 RAG 相关论文，如“Advanced System Integration: Analyzing OpenAPI Chunking for Retrieval-Augmented Generation”，则快速探讨了 RAG 在系统集成中的应用，贡献在于优化提示和检索策略，但细节较常规，影响较小。\n\n### 机器人与视觉应用\n这些论文强调了 AI 在机器人和视觉任务中的实际落地，涉及多模态交互和高效计算。\n\n- **CogACT: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation（CogACT：用于机器人操作的视觉-语言-动作基础模型）**：Qixiu Li 等作者提出一个端到端 VLA 框架，结合视觉和语言增强机器人认知和动作协同。发现它在模拟和真实环境中成功率提升 35%，远超基线，展示了 LLM 在机器人任务中的强大泛化能力。\n\n- **AutoScenario: Realistic Corner Case Generation for Autonomous Vehicles with Multimodal Large Language Model（AutoScenario：使用多模态 LLM 生成自主车辆的真实角落案例）**：Qiujing Lu 等团队开发了基于 LLM 的框架，用于生成车辆模拟场景。贡献在于从多源数据中提取风险因素，提高了模拟的真实性和多样性，实验证明它能处理高风险场景，推进了自动驾驶的安全测试。\n\n- **SIMS: Simulating Stylized Human-Scene Interactions with Retrieval-Augmented Script Generation（SIMS：通过检索增强脚本生成模拟风格化人类-场景交互）**：Wenjia Wang 等作者引入 RAG 生成交互脚本，结合物理控制策略。论文的关键发现是它能产生更自然的动作序列，实验在多样场景中提升了交互质量，适用于虚拟现实应用。\n\n其他视觉论文，如“VISION-XL: High Definition Video Inverse Problem Solver using Latent Image Diffusion Models”，则专注于视频重建，贡献在于潜在空间扩散优化，但相对专业，不如上述论文话题度高。\n\n### 医疗与科学应用\n这些论文将 AI 应用于医疗和科学领域，展示了实际影响。\n\n- **TITAN: A Multimodal Whole Slide Foundation Model for Pathology（TITAN：用于病理学的多模态全滑片基础模型）**：Faisal Mahmood 等团队构建了多模态模型，用于病理图像分析。贡献在于自监督学习和报告生成，在稀有疾病检索上达到 SOTA，提升了临床诊断的准确性和泛化性。\n\n- **FLARE: Towards Universal Dataset Purification against Backdoor Attacks（FLARE：针对后门攻击的通用数据集净化）**：Yiming Li 等作者提出净化框架，聚合隐藏层激活来检测异常。发现它能有效对抗多种后门攻击，同时保持数据完整性，实验在基准数据集上显著提升了模型鲁棒性。\n\n其他医疗论文，如“Dynamic EEG-fMRI mapping: Revealing the relationship between brain connectivity and cognitive state”，探讨了脑网络动态，但贡献较基础，快速掠过。\n\n### 其他领域快速概述\n剩余论文覆盖强化学习、图神经网络和优化等领域，但许多主题较 niche 或常规，我仅简要提一下：\n- **DELT: A Simple Diversity-driven EarlyLate Training for Dataset Distillation（DELT：用于数据集蒸馏的简单多样性驱动早晚训练）**：提升了图像多样性，实验在 CIFAR 上平均提升 2-5% 性能。\n- **CAdam: Confidence-Based Optimization for Online Learning（CAdam：基于置信度的在线学习优化）**：提出新优化器，提升了在线推荐系统的鲁棒性。\n- 其他如“Quantized Delta Weight Is Safety Keeper”等，聚焦模型安全和量化，但细节较琐碎，不做深究。\n\n总之，今天的论文突出了 AI 在推理、生成和应用中的创新潜力，LLM 和 RAG 的相关工作尤其值得关注。arXiv 的多样性提醒我们，AI 研究正向更实用、更安全的方向演进，期待后续进展！（全文约800字）",
  "papers": [
    {
      "arxiv_id": "2412.00281v1",
      "title": "Streamlining the review process: AI-generated annotations in research manuscripts",
      "title_zh": "翻译失败",
      "authors": [
        "Oscar Díaz",
        "Xabier Garmendia",
        "Juanan Pereira"
      ],
      "abstract": "The increasing volume of research paper submissions poses a significant\nchallenge to the traditional academic peer-review system, leading to an\noverwhelming workload for reviewers. This study explores the potential of\nintegrating Large Language Models (LLMs) into the peer-review process to\nenhance efficiency without compromising effectiveness. We focus on manuscript\nannotations, particularly excerpt highlights, as a potential area for AI-human\ncollaboration. While LLMs excel in certain tasks like aspect coverage and\ninformativeness, they often lack high-level analysis and critical thinking,\nmaking them unsuitable for replacing human reviewers entirely. Our approach\ninvolves using LLMs to assist with specific aspects of the review process. This\npaper introduces AnnotateGPT, a platform that utilizes GPT-4 for manuscript\nreview, aiming to improve reviewers' comprehension and focus. We evaluate\nAnnotateGPT using a Technology Acceptance Model (TAM) questionnaire with nine\nparticipants and generalize the findings. Our work highlights annotation as a\nviable middle ground for AI-human collaboration in academic review, offering\ninsights into integrating LLMs into the review process and tuning traditional\nannotation tools for LLM incorporation.",
      "tldr_zh": "该研究探讨了使用 Large Language Models (LLMs) 来简化学术同行评审过程，针对研究论文提交量增加导致的评审者负担过重问题。论文引入 AnnotateGPT 平台，利用 GPT-4 生成手稿注释（如摘录高亮），以提升评审效率和理解，同时强调 LLMs 在方面覆盖和信息性方面表现出色，但缺乏高级分析和批判性思维，无法完全取代人类。实验通过 Technology Acceptance Model (TAM) 问卷调查九名参与者，验证了 AI-human collaboration 在注释任务中的可行性，并为 LLMs 整合学术评审提供实用见解。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00281v1",
      "published_date": "2024-11-29 23:26:34 UTC",
      "updated_date": "2024-11-29 23:26:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:32:56.108895"
    },
    {
      "arxiv_id": "2412.00278v1",
      "title": "Average-Over-Time Spiking Neural Networks for Uncertainty Estimation in Regression",
      "title_zh": "翻译失败",
      "authors": [
        "Tao Sun",
        "Sander Bohté"
      ],
      "abstract": "Uncertainty estimation is a standard tool to quantify the reliability of\nmodern deep learning models, and crucial for many real-world applications.\nHowever, efficient uncertainty estimation methods for spiking neural networks,\nparticularly for regression models, have been lacking. Here, we introduce two\nmethods that adapt the Average-Over-Time Spiking Neural Network (AOT-SNN)\nframework to regression tasks, enhancing uncertainty estimation in event-driven\nmodels. The first method uses the heteroscedastic Gaussian approach, where SNNs\npredict both the mean and variance at each time step, thereby generating a\nconditional probability distribution of the target variable. The second method\nleverages the Regression-as-Classification (RAC) approach, reformulating\nregression as a classification problem to facilitate uncertainty estimation. We\nevaluate our approaches on both a toy dataset and several benchmark datasets,\ndemonstrating that the proposed AOT-SNN models achieve performance comparable\nto or better than state-of-the-art deep neural network methods, particularly in\nuncertainty estimation. Our findings highlight the potential of SNNs for\nuncertainty estimation in regression tasks, providing an efficient and\nbiologically inspired alternative for applications requiring both accuracy and\nenergy efficiency.",
      "tldr_zh": "本论文提出两种方法，基于 Average-Over-Time Spiking Neural Networks (AOT-SNN) 框架，用于提升回归任务中的不确定性估计。第一种方法采用 heteroscedastic Gaussian approach，让 SNNs 在每个时间步预测均值和方差，从而生成目标变量的条件概率分布；第二种方法则使用 Regression-as-Classification (RAC) approach，将回归问题转化为分类问题以便于不确定性评估。在玩具数据集和基准数据集上的实验显示，AOT-SNN 模型的性能与最先进深度神经网络方法相当或更优，尤其在不确定性估计方面。该研究突显了 SNNs 在回归任务中的潜力，提供高效、受生物启发的替代方案，适用于需要准确性和能量效率的实际应用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00278v1",
      "published_date": "2024-11-29 23:13:52 UTC",
      "updated_date": "2024-11-29 23:13:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:33:08.857064"
    },
    {
      "arxiv_id": "2412.00261v1",
      "title": "Attribute-Enhanced Similarity Ranking for Sparse Link Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "João Mattos",
        "Zexi Huang",
        "Mert Kosan",
        "Ambuj Singh",
        "Arlei Silva"
      ],
      "abstract": "Link prediction is a fundamental problem in graph data. In its most realistic\nsetting, the problem consists of predicting missing or future links between\nrandom pairs of nodes from the set of disconnected pairs. Graph Neural Networks\n(GNNs) have become the predominant framework for link prediction. GNN-based\nmethods treat link prediction as a binary classification problem and handle the\nextreme class imbalance -- real graphs are very sparse -- by sampling\n(uniformly at random) a balanced number of disconnected pairs not only for\ntraining but also for evaluation. However, we show that the reported\nperformance of GNNs for link prediction in the balanced setting does not\ntranslate to the more realistic imbalanced setting and that simpler\ntopology-based approaches are often better at handling sparsity. These findings\nmotivate Gelato, a similarity-based link-prediction method that applies (1)\ngraph learning based on node attributes to enhance a topological heuristic, (2)\na ranking loss for addressing class imbalance, and (3) a negative sampling\nscheme that efficiently selects hard training pairs via graph partitioning.\nExperiments show that Gelato outperforms existing GNN-based alternatives.",
      "tldr_zh": "该研究揭示了图神经网络(GNNs)在稀疏图链接预测中的局限性，即在不平衡设置下性能不如简单拓扑方法，因为GNNs依赖随机采样无法有效处理真实稀疏图。\n为此，论文提出Gelato，一种基于节点属性的相似性排名方法，包括使用图学习增强拓扑启发式、采用排名损失处理类别不平衡，以及通过图分区高效选择硬负样本。\n实验结果表明，Gelato在链接预测任务上优于现有的GNN-based方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "cs.LG",
      "comment": "To appear at the 31st SIGKDD Conference on Knowledge Discovery and\n  Data Mining - Research Track (August 2024 Deadline)",
      "pdf_url": "http://arxiv.org/pdf/2412.00261v1",
      "published_date": "2024-11-29 21:22:35 UTC",
      "updated_date": "2024-11-29 21:22:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:33:19.949850"
    },
    {
      "arxiv_id": "2412.00251v1",
      "title": "Fine Tuning Large Language Models to Deliver CBT for Depression",
      "title_zh": "翻译失败",
      "authors": [
        "Talha Tahir"
      ],
      "abstract": "Cognitive Behavioral Therapy (CBT) is a well-established, evidence-based\ntreatment for Major Depressive Disorder. Unfortunately, there exist significant\nbarriers to individuals accessing CBT, including cost, scarcity of therapists\nand stigma. This study explores the feasibility of fine-tuning small open\nweight large language models (LLMs) to deliver CBT for depression. Using 58\nsets of synthetic CBT transcripts generated by the Nous Research fine-tune of\nLlama 3.1 405b, we fine-tuned three models: Mistral 7b v0.3, Qwen 2.5 7b, and\nLlama 3.1 8b. CBT fidelity was evaluated through a modified Cognitive Therapy\nRating Scale (CTRS). All fine-tuned models were compared against each other, as\nwell as their instruct-tuned variants. Simulated patient transcripts were\ngenerated for the purpose of evaluating model performance, with the instruct\nand CBT-tuned models acting as the therapist and DeepSeek-V2.5 acting as the\npatient. These simulated transcripts were evaluated on a modified CTRS by\nGemini 1.5 Pro-002. Our findings demonstrated that the CBT-tuned models\nsignificantly outperformed their instruct-tuned counterparts, with an average\nimprovement of 11.33 points (p < 0.001) on total CTRS score. Llama 3.1 8b had\nthe strongest performance (mean CTRS score 67.86 +/- 7.24), followed by Qwen\n2.5 7b (64.28 +/- 9.55) and Mistral 7b v0.3 (64.17 +/- 9.79), with these\ndifferences between models being statistically significant. The CBT-tuned\nmodels were competent in implementing core CBT techniques and providing\nempathetic responses, however, there were limitations observed in agenda\nadherence, exploration depth and long-context coherence. This study establishes\nthat CBT specific fine-tuning can effectively encode therapeutic competencies\nin small LLMs, though significant technical and ethical considerations must be\nresolved prior to clinical deployment.",
      "tldr_zh": "本研究探讨了微调小型开源 LLMs（大型语言模型）以提供针对抑郁症的 CBT（认知行为疗法）的可行性，使用58套合成对话记录微调了 Mistral 7b v0.3、Qwen 2.5 7b 和 Llama 3.1 8b 模型，并通过修改后的 CTRS（认知疗法评分量表）评估其疗效忠实度。结果显示，微调后的模型在 CTRS 总分上比 instruct-tuned 版本平均提高了11.33点（p < 0.001），其中 Llama 3.1 8b 表现最佳（均值67.86），其次是 Qwen 2.5 7b 和 Mistral 7b v0.3。这些模型在核心 CBT 技术和移情响应上表现出色，但存在议程遵守、探索深度和长上下文连贯性的局限性。该研究证明了 CBT 特定微调能有效增强 LLMs 的治疗能力，但需解决技术和伦理问题方可临床部署。",
      "categories": [
        "cs.AI",
        "cs.HC",
        "I.2.7; J.3"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00251v1",
      "published_date": "2024-11-29 20:48:08 UTC",
      "updated_date": "2024-11-29 20:48:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:33:34.533740"
    },
    {
      "arxiv_id": "2412.00245v1",
      "title": "Integrating Social Determinants of Health into Knowledge Graphs: Evaluating Prediction Bias and Fairness in Healthcare",
      "title_zh": "翻译失败",
      "authors": [
        "Tianqi Shang",
        "Weiqing He",
        "Tianlong Chen",
        "Ying Ding",
        "Huanmei Wu",
        "Kaixiong Zhou",
        "Li Shen"
      ],
      "abstract": "Social determinants of health (SDoH) play a crucial role in patient health\noutcomes, yet their integration into biomedical knowledge graphs remains\nunderexplored. This study addresses this gap by constructing an SDoH-enriched\nknowledge graph using the MIMIC-III dataset and PrimeKG. We introduce a novel\nfairness formulation for graph embeddings, focusing on invariance with respect\nto sensitive SDoH information. Via employing a heterogeneous-GCN model for\ndrug-disease link prediction, we detect biases related to various SDoH factors.\nTo mitigate these biases, we propose a post-processing method that\nstrategically reweights edges connected to SDoHs, balancing their influence on\ngraph representations. This approach represents one of the first comprehensive\ninvestigations into fairness issues within biomedical knowledge graphs\nincorporating SDoH. Our work not only highlights the importance of considering\nSDoH in medical informatics but also provides a concrete method for reducing\nSDoH-related biases in link prediction tasks, paving the way for more equitable\nhealthcare recommendations. Our code is available at\n\\url{https://github.com/hwq0726/SDoH-KG}.",
      "tldr_zh": "本研究探讨了将社会决定因素（Social Determinants of Health, SDoH）整合到生物医学知识图中的重要性，并评估由此引发的预测偏差和公平性问题。研究者使用 MIMIC-III 数据集和 PrimeKG 构建了一个 SDoH 丰富的知识图，并引入了一种新的公平性公式，强调相对于敏感 SDoH 信息的不变性。借助 heterogeneous-GCN 模型进行药物-疾病链接预测，他们检测了与各种 SDoH 因素相关的偏差，并提出了一种后处理方法，通过重新加权与 SDoH 相关的边来缓解这些偏差。该工作首次全面调查了 SDoH 在知识图中的公平性问题，提供了一种减少偏差的方法，以促进更公平的医疗推荐，并已在 GitHub 上公开代码。",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00245v1",
      "published_date": "2024-11-29 20:35:01 UTC",
      "updated_date": "2024-11-29 20:35:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:33:44.204581"
    },
    {
      "arxiv_id": "2412.00243v1",
      "title": "Realistic Corner Case Generation for Autonomous Vehicles with Multimodal Large Language Model",
      "title_zh": "翻译失败",
      "authors": [
        "Qiujing Lu",
        "Meng Ma",
        "Ximiao Dai",
        "Xuanhan Wang",
        "Shuo Feng"
      ],
      "abstract": "To guarantee the safety and reliability of autonomous vehicle (AV) systems,\ncorner cases play a crucial role in exploring the system's behavior under rare\nand challenging conditions within simulation environments. However, current\napproaches often fall short in meeting diverse testing needs and struggle to\ngeneralize to novel, high-risk scenarios that closely mirror real-world\nconditions. To tackle this challenge, we present AutoScenario, a multimodal\nLarge Language Model (LLM)-based framework for realistic corner case\ngeneration. It converts safety-critical real-world data from multiple sources\ninto textual representations, enabling the generalization of key risk factors\nwhile leveraging the extensive world knowledge and advanced reasoning\ncapabilities of LLMs.Furthermore, it integrates tools from the Simulation of\nUrban Mobility (SUMO) and CARLA simulators to simplify and execute the code\ngenerated by LLMs. Our experiments demonstrate that AutoScenario can generate\nrealistic and challenging test scenarios, precisely tailored to specific\ntesting requirements or textual descriptions. Additionally, we validated its\nability to produce diverse and novel scenarios derived from multimodal\nreal-world data involving risky situations, harnessing the powerful\ngeneralization capabilities of LLMs to effectively simulate a wide range of\ncorner cases.",
      "tldr_zh": "该研究针对自动驾驶车辆（AV）的安全可靠性，提出AutoScenario框架，利用多模态Large Language Model（LLM）生成真实的corner cases，以解决现有方法在多样化测试和泛化高风险场景上的不足。框架通过将多源安全关键真实数据转换为文本表示，结合LLM的广泛世界知识和高级推理能力，泛化关键风险因素，并整合SUMO和CARLA模拟器工具来简化执行LLM生成的代码。实验结果显示，AutoScenario能产生精确针对特定需求的真实挑战场景，并从多模态数据中创建多样化、新颖的测试场景，提升了AV系统在稀有条件下的模拟效果。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00243v1",
      "published_date": "2024-11-29 20:23:28 UTC",
      "updated_date": "2024-11-29 20:23:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:33:56.172140"
    },
    {
      "arxiv_id": "2412.00239v1",
      "title": "Generating a Low-code Complete Workflow via Task Decomposition and RAG",
      "title_zh": "翻译失败",
      "authors": [
        "Orlando Marquez Ayala",
        "Patrice Béchard"
      ],
      "abstract": "AI technologies are moving rapidly from research to production. With the\npopularity of Foundation Models (FMs) that generate text, images, and video,\nAI-based systems are increasing their complexity. Compared to traditional\nAI-based software, systems employing FMs, or GenAI-based systems, are more\ndifficult to design due to their scale and versatility. This makes it necessary\nto document best practices, known as design patterns in software engineering,\nthat can be used across GenAI applications. Our first contribution is to\nformalize two techniques, Task Decomposition and Retrieval-Augmented Generation\n(RAG), as design patterns for GenAI-based systems. We discuss their trade-offs\nin terms of software quality attributes and comment on alternative approaches.\nWe recommend to AI practitioners to consider these techniques not only from a\nscientific perspective but also from the standpoint of desired engineering\nproperties such as flexibility, maintainability, safety, and security. As a\nsecond contribution, we describe our industry experience applying Task\nDecomposition and RAG to build a complex real-world GenAI application for\nenterprise users: Workflow Generation. The task of generating workflows entails\ngenerating a specific plan using data from the system environment, taking as\ninput a user requirement. As these two patterns affect the entire AI\ndevelopment cycle, we explain how they impacted the dataset creation, model\ntraining, model evaluation, and deployment phases.",
      "tldr_zh": "该论文形式化了Task Decomposition和Retrieval-Augmented Generation (RAG)作为GenAI系统设计模式的关键技术，并分析了它们在软件质量属性（如灵活性、可维护性、安全性）方面的权衡，推荐AI从业者从工程角度评估这些方法。作为主要贡献，作者分享了在工业应用中运用这些模式构建复杂的企业级Workflow Generation系统，该系统基于用户需求生成特定计划，并影响了AI开发周期的各个阶段，包括数据集创建、模型训练、评估和部署。总体而言，这为GenAI应用的标准化和高效实现提供了实用指导。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Under review; 12 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.00239v1",
      "published_date": "2024-11-29 20:13:56 UTC",
      "updated_date": "2024-11-29 20:13:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:34:08.394825"
    },
    {
      "arxiv_id": "2412.00238v1",
      "title": "Twisted Convolutional Networks (TCNs): Enhancing Feature Interactions for Non-Spatial Data Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Junbo Jacob Lian"
      ],
      "abstract": "Twisted Convolutional Networks (TCNs) are introduced as a novel neural\nnetwork architecture designed to effectively process one-dimensional data with\narbitrary feature order and minimal spatial relationships. Unlike traditional\nConvolutional Neural Networks (CNNs), which excel at handling structured\ntwo-dimensional data like images, TCNs reduce dependency on feature order by\ncombining input features in innovative ways to create new representations. By\nexplicitly enhancing feature interactions and employing diverse feature\ncombinations, TCNs generate richer and more informative representations, making\nthem especially effective for classification tasks on datasets with arbitrary\nfeature arrangements. This paper details the TCN architecture and its feature\ncombination strategy, providing a comprehensive comparison with traditional\nCNNs, DeepSets, Transformers, and Graph Neural Networks (GNNs). Extensive\nexperiments on benchmark datasets demonstrate that TCNs achieve superior\nperformance, particularly in classification scenarios involving one-dimensional\ndata.",
      "tldr_zh": "该研究引入了Twisted Convolutional Networks (TCNs)，一种新型神经网络架构，专门用于处理一维数据，这些数据具有任意特征顺序和最小空间关系，通过创新的特征组合策略增强特征交互，从而生成更丰富的表示。\n与传统Convolutional Neural Networks (CNNs)、DeepSets、Transformers和Graph Neural Networks (GNNs)相比，TCNs减少了对特征顺序的依赖，特别适用于非空间数据的分类任务。\n实验结果显示，在基准数据集上，TCNs在一维数据分类场景中表现出色，性能优于现有基线模型。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "The source code for the TCNs can be accessed at\n  https://github.com/junbolian/Twisted-Convolutional-Networks",
      "pdf_url": "http://arxiv.org/pdf/2412.00238v1",
      "published_date": "2024-11-29 20:12:24 UTC",
      "updated_date": "2024-11-29 20:12:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:34:20.096833"
    },
    {
      "arxiv_id": "2412.00224v1",
      "title": "An AI-Driven Data Mesh Architecture Enhancing Decision-Making in Infrastructure Construction and Public Procurement",
      "title_zh": "翻译失败",
      "authors": [
        "Saurabh Mishra",
        "Mahendra Shinde",
        "Aniket Yadav",
        "Bilal Ayyub",
        "Anand Rao"
      ],
      "abstract": "Infrastructure construction, often dubbed an \"industry of industries,\" is\nclosely linked with government spending and public procurement, offering\nsignificant opportunities for improved efficiency and productivity through\nbetter transparency and information access. By leveraging these opportunities,\nwe can achieve notable gains in productivity, cost savings, and broader\neconomic benefits. Our approach introduces an integrated software ecosystem\nutilizing Data Mesh and Service Mesh architectures. This system includes the\nlargest training dataset for infrastructure and procurement, encompassing over\n100 billion tokens, scientific publications, activities, and risk data, all\nstructured by a systematic AI framework. Supported by a Knowledge Graph linked\nto domain-specific multi-agent tasks and Q&A capabilities, our platform\nstandardizes and ingests diverse data sources, transforming them into\nstructured knowledge. Leveraging large language models (LLMs) and automation,\nour system revolutionizes data structuring and knowledge creation, aiding\ndecision-making in early-stage project planning, detailed research, market\ntrend analysis, and qualitative assessments. Its web-scalable architecture\ndelivers domain-curated information, enabling AI agents to facilitate reasoning\nand manage uncertainties, while preparing for future expansions with\nspecialized agents targeting particular challenges. This integration of AI with\ndomain expertise not only boosts efficiency and decision-making in construction\nand infrastructure but also establishes a framework for enhancing government\nefficiency and accelerating the transition of traditional industries to digital\nworkflows. This work is poised to significantly influence AI-driven initiatives\nin this sector and guide best practices in AI Operations.",
      "tldr_zh": "该论文提出了一种AI驱动的Data Mesh架构，旨在提升基础设施建设和公共采购中的决策效率，通过整合Data Mesh和Service Mesh来标准化多样数据源并转化为结构化知识。系统利用一个包含超过100亿tokens的庞大数据集、Knowledge Graph以及LLMs和多智能体任务，支持早期项目规划、市场趋势分析和定性评估等应用。借助AI代理进行推理和管理不确定性，该框架显著提高了生产力、成本节约和经济效益，并为传统行业向数字工作流的转型提供了一个可扩展的蓝图。该工作有望指导AI Operations的最佳实践，并在政府效率提升方面产生深远影响。",
      "categories": [
        "cs.AI",
        "cs.DB",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00224v1",
      "published_date": "2024-11-29 19:33:51 UTC",
      "updated_date": "2024-11-29 19:33:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:34:33.103832"
    },
    {
      "arxiv_id": "2412.00209v1",
      "title": "Digital Twin in Industries: A Comprehensive Survey",
      "title_zh": "数字孪生在工业中的全面综述",
      "authors": [
        "Md Bokhtiar Al Zami",
        "Shaba Shaon",
        "Vu Khanh Quy",
        "Dinh C. Nguyen"
      ],
      "abstract": "Industrial networks are undergoing rapid transformation driven by the\nconvergence of emerging technologies that are revolutionizing conventional\nworkflows, enhancing operational efficiency, and fundamentally redefining the\nindustrial landscape across diverse sectors. Amidst this revolution, Digital\nTwin (DT) emerges as a transformative innovation that seamlessly integrates\nreal-world systems with their virtual counterparts, bridging the physical and\ndigital realms. In this article, we present a comprehensive survey of the\nemerging DT-enabled services and applications across industries, beginning with\nan overview of DT fundamentals and its components to a discussion of key\nenabling technologies for DT. Different from literature works, we investigate\nand analyze the capabilities of DT across a wide range of industrial services,\nincluding data sharing, data offloading, integrated sensing and communication,\ncontent caching, resource allocation, wireless networking, and metaverse. In\nparticular, we present an in-depth technical discussion of the roles of DT in\nindustrial applications across various domains, including manufacturing,\nhealthcare, transportation, energy, agriculture, space, oil and gas, as well as\nrobotics. Throughout the technical analysis, we delve into real-time data\ncommunications between physical and virtual platforms to enable industrial DT\nnetworking. Subsequently, we extensively explore and analyze a wide range of\nmajor privacy and security issues in DT-based industry. Taxonomy tables and the\nkey research findings from the survey are also given, emphasizing important\ninsights into the significance of DT in industries. Finally, we point out\nfuture research directions to spur further research in this promising area.",
      "tldr_zh": "这篇论文对Digital Twin (DT) 在工业中的应用进行了全面调查，涵盖了DT的基础组件、关键技术以及其在数据共享、数据卸载、集成感知与通信等领域的作用。研究分析了DT在制造、医疗、交通、能源、农业、空间、石油和天然气以及机器人等行业的具体应用，并探讨了实时数据通信、隐私和安全挑战，提供分类表和关键见解。最终，论文强调了DT对工业转型的重要性，并提出了未来研究方向，如进一步提升DT的安全性和扩展应用场景。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00209v1",
      "published_date": "2024-11-29 19:14:45 UTC",
      "updated_date": "2024-11-29 19:14:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:34:43.736965"
    },
    {
      "arxiv_id": "2412.00206v1",
      "title": "Towards the Ultimate Programming Language: Trust and Benevolence in the Age of Artificial Intelligence",
      "title_zh": "迈向终极编程语言：人工智能时代中的信任与仁慈",
      "authors": [
        "Bartosz Sawicki",
        "Michał Śmiałek",
        "Bartłomiej Skowron"
      ],
      "abstract": "This article explores the evolving role of programming languages in the\ncontext of artificial intelligence. It highlights the need for programming\nlanguages to ensure human understanding while eliminating unnecessary\nimplementation details and suggests that future programs should be designed to\nrecognize and actively support user interests. The vision includes a\nthree-level process: using natural language for requirements, translating it\ninto a precise system definition language, and finally optimizing the code for\nperformance. The concept of an \"Ultimate Programming Language\" is introduced,\nemphasizing its role in maintaining human control over machines. Trust,\nreliability, and benevolence are identified as key elements that will enhance\ncooperation between humans and AI systems.",
      "tldr_zh": "这篇论文探讨了编程语言在人工智能时代的发展，强调未来编程语言应确保人类理解、简化不必要细节，并主动支持用户利益。作者提出一个三层过程：首先使用自然语言定义需求，其次翻译成精确的系统定义语言，最后优化代码以提升性能。论文引入“Ultimate Programming Language”的概念，强调通过信任、可靠性和仁慈来维持人类对机器的控制，从而促进人类与AI系统的合作。",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.HC",
        "cs.PL",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "submitted to proceedings of \"Ethics and AI\" conference",
      "pdf_url": "http://arxiv.org/pdf/2412.00206v1",
      "published_date": "2024-11-29 19:02:25 UTC",
      "updated_date": "2024-11-29 19:02:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:34:54.864256"
    },
    {
      "arxiv_id": "2411.19946v1",
      "title": "DELT: A Simple Diversity-driven EarlyLate Training for Dataset Distillation",
      "title_zh": "翻译失败",
      "authors": [
        "Zhiqiang Shen",
        "Ammar Sherif",
        "Zeyuan Yin",
        "Shitong Shao"
      ],
      "abstract": "Recent advances in dataset distillation have led to solutions in two main\ndirections. The conventional batch-to-batch matching mechanism is ideal for\nsmall-scale datasets and includes bi-level optimization methods on models and\nsyntheses, such as FRePo, RCIG, and RaT-BPTT, as well as other methods like\ndistribution matching, gradient matching, and weight trajectory matching.\nConversely, batch-to-global matching typifies decoupled methods, which are\nparticularly advantageous for large-scale datasets. This approach has garnered\nsubstantial interest within the community, as seen in SRe$^2$L, G-VBSM, WMDD,\nand CDA. A primary challenge with the second approach is the lack of diversity\namong syntheses within each class since samples are optimized independently and\nthe same global supervision signals are reused across different synthetic\nimages. In this study, we propose a new Diversity-driven EarlyLate Training\n(DELT) scheme to enhance the diversity of images in batch-to-global matching\nwith less computation. Our approach is conceptually simple yet effective, it\npartitions predefined IPC samples into smaller subtasks and employs local\noptimizations to distill each subset into distributions from distinct phases,\nreducing the uniformity induced by the unified optimization process. These\ndistilled images from the subtasks demonstrate effective generalization when\napplied to the entire task. We conduct extensive experiments on CIFAR,\nTiny-ImageNet, ImageNet-1K, and its sub-datasets. Our approach outperforms the\nprevious state-of-the-art by 2$\\sim$5% on average across different datasets and\nIPCs (images per class), increasing diversity per class by more than 5% while\nreducing synthesis time by up to 39.3% for enhancing the training efficiency.\nCode is available at: https://github.com/VILA-Lab/DELT.",
      "tldr_zh": "本文提出了一种简单有效的多样性驱动的早期-后期训练方案（DELT），旨在提升数据集蒸馏（dataset distillation）中 batch-to-global matching 方法的样本多样性，同时减少计算开销。DELT 通过将预定义的 IPC（images per class）样本分成更小的子任务，并采用局部优化来蒸馏每个子集，从而减少统一优化过程引发的均匀性问题，并提升子任务图像的泛化能力。在 CIFAR、Tiny-ImageNet 和 ImageNet-1K 等数据集上的广泛实验中，DELT 比之前的最先进方法平均提高 2~5% 的性能，每个类别的多样性增加超过 5%，并将合成时间减少高达 39.3%。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19946v1",
      "published_date": "2024-11-29 18:59:46 UTC",
      "updated_date": "2024-11-29 18:59:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:35:08.740146"
    },
    {
      "arxiv_id": "2411.19943v3",
      "title": "Critical Tokens Matter: Token-Level Contrastive Estimation Enhances LLM's Reasoning Capability",
      "title_zh": "翻译失败",
      "authors": [
        "Zicheng Lin",
        "Tian Liang",
        "Jiahao Xu",
        "Qiuzhi Lin",
        "Xing Wang",
        "Ruilin Luo",
        "Chufan Shi",
        "Siheng Li",
        "Yujiu Yang",
        "Zhaopeng Tu"
      ],
      "abstract": "Mathematical reasoning tasks pose significant challenges for large language\nmodels (LLMs) because they require precise logical deduction and sequence\nanalysis. In this work, we introduce the concept of critical tokens -- elements\nwithin reasoning trajectories that significantly influence incorrect outcomes.\nWe present a novel framework for identifying these tokens through rollout\nsampling and demonstrate their substantial divergence from traditional error\ntokens. Through extensive experiments on datasets such as GSM8K and MATH500, we\nshow that identifying and replacing critical tokens significantly improves\nmodel accuracy. We propose an efficient methodology for pinpointing these\ntokens in large-scale datasets using contrastive estimation and extend this\nframework to enhance model training processes with direct preference\noptimization (DPO). Experimental results on GSM8K and MATH500 benchmarks with\nthe widely used models Llama-3 (8B and 70B) and Deepseek-math (7B) demonstrate\nthe effectiveness of the proposed approach, cDPO. Our results underscore the\npotential of leveraging critical tokens to reduce errors in reasoning tasks,\nadvancing the development of AI systems capable of robust logical deduction.\nOur code, annotated datasets, and trained models are available at\nhttps://github.com/chenzhiling9954/Critical-Tokens-Matter to support and\nencourage future research in this promising field.",
      "tldr_zh": "本研究针对大型语言模型（LLMs）在数学推理任务中的挑战，如精确逻辑演绎和序列分析，引入了 critical tokens 的概念，这些是推理轨迹中影响错误结果的关键元素。研究通过 rollout sampling 识别这些 tokens，并使用 contrastive estimation 在 GSM8K 和 MATH500 数据集上高效定位它们，同时扩展到模型训练中结合 direct preference optimization (DPO)，提出 cDPO 框架来优化训练过程。实验结果显示，在 Llama-3 (8B 和 70B) 及 Deepseek-math (7B) 模型上，cDPO 显著降低了推理错误，提高了模型准确率，从而提升了 AI 系统在逻辑推理方面的鲁棒性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Work in progress",
      "pdf_url": "http://arxiv.org/pdf/2411.19943v3",
      "published_date": "2024-11-29 18:58:22 UTC",
      "updated_date": "2025-01-13 06:53:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:35:20.530765"
    },
    {
      "arxiv_id": "2412.12108v3",
      "title": "Responsible AI Governance: A Response to UN Interim Report on Governing AI for Humanity",
      "title_zh": "翻译失败",
      "authors": [
        "Sarah Kiden",
        "Bernd Stahl",
        "Beverley Townsend",
        "Carsten Maple",
        "Charles Vincent",
        "Fraser Sampson",
        "Geoff Gilbert",
        "Helen Smith",
        "Jayati Deshmukh",
        "Jen Ross",
        "Jennifer Williams",
        "Jesus Martinez del Rincon",
        "Justyna Lisinska",
        "Karen O'Shea",
        "Márjory Da Costa Abreu",
        "Nelly Bencomo",
        "Oishi Deb",
        "Peter Winter",
        "Phoebe Li",
        "Philip Torr",
        "Pin Lean Lau",
        "Raquel Iniesta",
        "Gopal Ramchurn",
        "Sebastian Stein",
        "Vahid Yazdanpanah"
      ],
      "abstract": "This report presents a comprehensive response to the United Nation's Interim\nReport on Governing Artificial Intelligence (AI) for Humanity. It emphasizes\nthe transformative potential of AI in achieving the Sustainable Development\nGoals (SDGs) while acknowledging the need for robust governance to mitigate\nassociated risks. The response highlights opportunities for promoting\nequitable, secure, and inclusive AI ecosystems, which should be supported by\ninvestments in infrastructure and multi-stakeholder collaborations across\njurisdictions. It also underscores challenges, including societal inequalities\nexacerbated by AI, ethical concerns, and environmental impacts. Recommendations\nadvocate for legally binding norms, transparency, and multi-layered data\ngovernance models, alongside fostering AI literacy and capacity-building\ninitiatives. Internationally, the report calls for harmonising AI governance\nframeworks with established laws, human rights standards, and regulatory\napproaches. The report concludes with actionable principles for fostering\nresponsible AI governance through collaboration among governments, industry,\nacademia, and civil society, ensuring the development of AI aligns with\nuniversal human values and the public good.",
      "tldr_zh": "该报告是对联合国临时报告《Governing AI for Humanity》的全面回应，强调AI在实现可持续发展目标(SDGs)方面的变革潜力，同时指出需要强有力的治理来缓解AI带来的风险，如社会不平等、伦理问题和环境影响。报告突出促进公平、安全和包容性AI生态系统的机会，包括投资基础设施和多利益相关者跨辖区合作，并提出推荐措施，如制定法律规范、提升透明度、多层数据治理模型，以及加强AI识读和能力建设。最终，它呼吁国际上协调AI治理框架与人权标准相一致，并通过政府、行业、学术界和民间社会的协作，推动负责任的AI治理，以符合人类价值观和公共利益。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "Submitted to United Nations. 23 pages. All the Authors Contributed\n  Equally",
      "pdf_url": "http://arxiv.org/pdf/2412.12108v3",
      "published_date": "2024-11-29 18:57:24 UTC",
      "updated_date": "2024-12-31 18:52:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:35:31.607569"
    },
    {
      "arxiv_id": "2411.19939v3",
      "title": "VLSBench: Unveiling Visual Leakage in Multimodal Safety",
      "title_zh": "VLSBench：揭示多模态安全中的视觉泄漏",
      "authors": [
        "Xuhao Hu",
        "Dongrui Liu",
        "Hao Li",
        "Xuanjing Huang",
        "Jing Shao"
      ],
      "abstract": "Safety concerns of Multimodal large language models (MLLMs) have gradually\nbecome an important problem in various applications. Surprisingly, previous\nworks indicate a counterintuitive phenomenon that using textual unlearning to\nalign MLLMs achieves comparable safety performances with MLLMs aligned with\nimage text pairs. To explain such a phenomenon, we discover a Visual Safety\nInformation Leakage (VSIL) problem in existing multimodal safety benchmarks,\ni.e., the potentially risky content in the image has been revealed in the\ntextual query. Thus, MLLMs can easily refuse these sensitive image-text pairs\naccording to textual queries only, leading to unreliable cross-modality safety\nevaluation of MLLMs. We also conduct a further comparison experiment between\ntextual alignment and multimodal alignment to highlight this drawback. To this\nend, we construct multimodal Visual Leakless Safety Bench (VLSBench) with 2.2k\nimage-text pairs through an automated data pipeline. Experimental results\nindicate that VLSBench poses a significant challenge to both open-source and\nclose-source MLLMs, e.g., LLaVA, Qwen2-VL and GPT-4o. Besides, we empirically\ncompare textual and multimodal alignment methods on VLSBench and find that\ntextual alignment is effective enough for multimodal safety scenarios with\nVSIL, while multimodal alignment is preferable for safety scenarios without\nVSIL. Code and data are released under https://github.com/AI45Lab/VLSBench",
      "tldr_zh": "本文研究发现，多模态大语言模型(MLLMs)的安全基准存在Visual Safety Information Leakage (VSIL)问题，即图像中的风险内容已泄露在文本查询中，导致MLLMs的安全评估不可靠。作者构建了VLSBench数据集，包含2.2k图像-文本对，通过自动化数据管道生成，并通过实验比较文本对齐和多模态对齐方法。结果显示，VLSBench对开放源和闭源MLLMs（如LLaVA、Qwen2-VL和GPT-4o）构成重大挑战，在无VSIL场景下，多模态对齐更具优势。代码和数据已在GitHub开源。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.CR",
      "comment": "ACL2025 Main",
      "pdf_url": "http://arxiv.org/pdf/2411.19939v3",
      "published_date": "2024-11-29 18:56:37 UTC",
      "updated_date": "2025-05-17 15:14:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:35:44.716742"
    },
    {
      "arxiv_id": "2412.00174v1",
      "title": "SOLAMI: Social Vision-Language-Action Modeling for Immersive Interaction with 3D Autonomous Characters",
      "title_zh": "翻译失败",
      "authors": [
        "Jianping Jiang",
        "Weiye Xiao",
        "Zhengyu Lin",
        "Huaizhong Zhang",
        "Tianxiang Ren",
        "Yang Gao",
        "Zhiqian Lin",
        "Zhongang Cai",
        "Lei Yang",
        "Ziwei Liu"
      ],
      "abstract": "Human beings are social animals. How to equip 3D autonomous characters with\nsimilar social intelligence that can perceive, understand and interact with\nhumans remains an open yet foundamental problem. In this paper, we introduce\nSOLAMI, the first end-to-end Social vision-Language-Action (VLA) Modeling\nframework for Immersive interaction with 3D autonomous characters.\nSpecifically, SOLAMI builds 3D autonomous characters from three aspects: (1)\nSocial VLA Architecture: We propose a unified social VLA framework to generate\nmultimodal response (speech and motion) based on the user's multimodal input to\ndrive the character for social interaction. (2) Interactive Multimodal Data: We\npresent SynMSI, a synthetic multimodal social interaction dataset generated by\nan automatic pipeline using only existing motion datasets to address the issue\nof data scarcity. (3) Immersive VR Interface: We develop a VR interface that\nenables users to immersively interact with these characters driven by various\narchitectures. Extensive quantitative experiments and user studies demonstrate\nthat our framework leads to more precise and natural character responses (in\nboth speech and motion) that align with user expectations with lower latency.",
      "tldr_zh": "该研究引入了 SOLAMI，这是首个端到端 Social VLA (Vision-Language-Action) 建模框架，旨在赋予 3D 自主角色类似人类的社交智能，以感知、理解和互动。框架包括三个关键组件：Social VLA Architecture 用于基于用户多模态输入生成统一的语音和动作响应；SynMSI 数据集，通过自动管道从现有动作数据集合成多模态交互数据来解决数据稀缺问题；以及 Immersive VR Interface，提供沉浸式交互环境。实验和用户研究显示，SOLAMI 实现了更精确、自然的角色响应（语音和动作），与用户期望高度一致，同时降低了延迟。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00174v1",
      "published_date": "2024-11-29 18:53:40 UTC",
      "updated_date": "2024-11-29 18:53:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:35:55.697986"
    },
    {
      "arxiv_id": "2411.19922v1",
      "title": "Dynamic EEG-fMRI mapping: Revealing the relationship between brain connectivity and cognitive state",
      "title_zh": "动态 EEG-fMRI 映射：揭示脑连接性和认知状态之间的关系",
      "authors": [
        "Guiran Liu",
        "Binrong Zhu"
      ],
      "abstract": "This study investigated the dynamic connectivity patterns between EEG and\nfMRI modalities, contributing to our understanding of brain network\ninteractions. By employing a comprehensive approach that integrated static and\ndynamic analyses of EEG-fMRI data, we were able to uncover distinct\nconnectivity states and characterize their temporal fluctuations. The results\nrevealed modular organization within the intrinsic connectivity networks (ICNs)\nof the brain, highlighting the significant roles of sensory systems and the\ndefault mode network. The use of a sliding window technique allowed us to\nassess how functional connectivity varies over time, further elucidating the\ntransient nature of brain connectivity. Additionally, our findings align with\nprevious literature, reinforcing the notion that cognitive states can be\neffectively identified through short-duration data, specifically within the\n30-60 second timeframe. The established relationships between connectivity\nstrength and cognitive processes, particularly during different visual states,\nunderscore the relevance of our approach for future research into brain\ndynamics. Overall, this study not only enhances our understanding of the\ninterplay between EEG and fMRI signals but also paves the way for further\nexploration into the neural correlates of cognitive functions and their\nimplications in clinical settings. Future research should focus on refining\nthese methodologies and exploring their applications in various cognitive and\nclinical contexts.",
      "tldr_zh": "本研究探讨了 EEG 和 fMRI 模态之间的动态连接模式，以揭示脑连接与认知状态的关系。采用整合静态和动态分析的方法，包括滑动窗口技术，该研究揭示了脑内固有连接网络 (ICNs) 的模块化组织，并突出了感觉系统和默认模式网络的关键作用。结果表明，功能连接随时间变化，具有暂时性特征，且认知状态可通过 30-60 秒的短时数据有效识别。该方法增强了对脑动态的理解，并为临床应用铺平道路。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "15 pages, Subjects: Machine Learning (cs.LG); Human-Computer\n  Interaction (cs.HC); Signal Processing (eess.SP)",
      "pdf_url": "http://arxiv.org/pdf/2411.19922v1",
      "published_date": "2024-11-29 18:36:58 UTC",
      "updated_date": "2024-11-29 18:36:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:36:07.846856"
    },
    {
      "arxiv_id": "2411.19921v2",
      "title": "SIMS: Simulating Stylized Human-Scene Interactions with Retrieval-Augmented Script Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Wenjia Wang",
        "Liang Pan",
        "Zhiyang Dou",
        "Jidong Mei",
        "Zhouyingcheng Liao",
        "Yuke Lou",
        "Yifan Wu",
        "Lei Yang",
        "Jingbo Wang",
        "Taku Komura"
      ],
      "abstract": "Simulating stylized human-scene interactions (HSI) in physical environments\nis a challenging yet fascinating task. Prior works emphasize long-term\nexecution but fall short in achieving both diverse style and physical\nplausibility. To tackle this challenge, we introduce a novel hierarchical\nframework named SIMS that seamlessly bridges highlevel script-driven intent\nwith a low-level control policy, enabling more expressive and diverse\nhuman-scene interactions. Specifically, we employ Large Language Models with\nRetrieval-Augmented Generation (RAG) to generate coherent and diverse long-form\nscripts, providing a rich foundation for motion planning. A versatile\nmulticondition physics-based control policy is also developed, which leverages\ntext embeddings from the generated scripts to encode stylistic cues,\nsimultaneously perceiving environmental geometries and accomplishing task\ngoals. By integrating the retrieval-augmented script generation with the\nmulti-condition controller, our approach provides a unified solution for\ngenerating stylized HSI motions. We further introduce a comprehensive planning\ndataset produced by RAG and a stylized motion dataset featuring diverse\nlocomotions and interactions. Extensive experiments demonstrate SIMS's\neffectiveness in executing various tasks and generalizing across different\nscenarios, significantly outperforming previous methods.",
      "tldr_zh": "本研究提出SIMS框架，用于模拟风格化的人-场景互动（HSI），以解决现有方法在多样风格和物理合理性方面的不足。SIMS采用分层设计，通过Large Language Models结合Retrieval-Augmented Generation (RAG)生成连贯、多样的长脚本作为高层意图基础，并开发多条件基于物理的控制策略，利用脚本文本嵌入编码风格线索，同时感知环境几何和完成任务目标。实验结果显示，SIMS在各种任务执行和场景泛化上显著优于先前方法，并引入了由RAG生成的全新规划数据集和风格化运动数据集。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19921v2",
      "published_date": "2024-11-29 18:36:15 UTC",
      "updated_date": "2025-03-16 04:09:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:36:20.518164"
    },
    {
      "arxiv_id": "2411.19918v1",
      "title": "Handling irresolvable conflicts in the Semantic Web: an RDF-based conflict-tolerant version of the Deontic Traditional Scheme",
      "title_zh": "处理语义网中不可解决的冲突：德奥蒂克传统方案的基于RDF的容忍冲突版本",
      "authors": [
        "Livio Robaldo",
        "Gianluca Pozzato"
      ],
      "abstract": "This paper presents a new ontology that implements the well-known Deontic\nTraditional Scheme in RDFs and SPARQL, fit to handle irresolvable conflicts,\ni.e., situations in which two or more statements prescribe conflicting\nobligations, prohibitions, or permissions, with none of them being \"stronger\"\nthan the other one(s). In our view, this paper marks a significant advancement\nin standard theoretical research in formal Deontic Logic. Most contemporary\napproaches in this field are confined to the propositional level, mainly focus\non the notion of obligation, and lack implementations. The proposed framework\nis encoded in RDF, which is not only a first-order language but also the most\nwidely used knowledge representation language, as it forms the foundation of\nthe Semantic Web. Moreover, the proposed computational ontology formalizes all\ndeontic modalities defined in the Deontic Traditional Scheme, without\nspecifically focusing on obligations, and offers constructs to model and reason\nwith various types of irresolvable conflicts, violations, and the interaction\nbetween deontic modalities and contextual constraints in a given state of\naffairs. To the best of our knowledge, no existing approach in the literature\naddresses all these aspects within a unified integrated framework. All examples\npresented and discussed in this paper, together with Java code and clear\ninstructions to re-execute them locally, are available at\nhttps://github.com/liviorobaldo/conflict-tolerantDeonticTraditionalScheme",
      "tldr_zh": "本论文提出了一种基于RDF和SPARQL的新本体，实现了一个冲突耐受版本的Deontic Traditional Scheme，用于处理语义网中的不可解决冲突，即多个相互冲突的义务、禁止或许可，且无一优先。相比现有Deontic Logic研究，该框架扩展到一阶语言，支持所有德奥模式（deontic modalities），并提供结构来建模和推理冲突、违反以及德奥模式与上下文约束的交互，填补了统一框架的空白。实验和示例代码已开源在GitHub，展示了其在实际应用中的可行性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19918v1",
      "published_date": "2024-11-29 18:33:28 UTC",
      "updated_date": "2024-11-29 18:33:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:36:32.514727"
    },
    {
      "arxiv_id": "2411.19913v1",
      "title": "Quantifying the synthetic and real domain gap in aerial scene understanding",
      "title_zh": "翻译失败",
      "authors": [
        "Alina Marcu"
      ],
      "abstract": "Quantifying the gap between synthetic and real-world imagery is essential for\nimproving both transformer-based models - that rely on large volumes of data -\nand datasets, especially in underexplored domains like aerial scene\nunderstanding where the potential impact is significant. This paper introduces\na novel methodology for scene complexity assessment using Multi-Model Consensus\nMetric (MMCM) and depth-based structural metrics, enabling a robust evaluation\nof perceptual and structural disparities between domains. Our experimental\nanalysis, utilizing real-world (Dronescapes) and synthetic (Skyscenes)\ndatasets, demonstrates that real-world scenes generally exhibit higher\nconsensus among state-of-the-art vision transformers, while synthetic scenes\nshow greater variability and challenge model adaptability. The results\nunderline the inherent complexities and domain gaps, emphasizing the need for\nenhanced simulation fidelity and model generalization. This work provides\ncritical insights into the interplay between domain characteristics and model\nperformance, offering a pathway for improved domain adaptation strategies in\naerial scene understanding.",
      "tldr_zh": "这篇论文量化了合成图像和真实图像在航空场景理解中的域差距，引入了 Multi-Model Consensus Metric (MMCM) 和 depth-based structural metrics 等新方法来评估感知和结构差异。实验通过比较真实数据集 (Dronescapes) 和合成数据集 (Skyscenes)，发现真实场景在视觉 transformers 模型中表现出更高的共识，而合成场景具有更大的变异性，挑战了模型的适应性。研究强调了提升模拟保真度和模型泛化能力的必要性，并为航空场景理解的域适应策略提供了重要路径。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "17 pages (including references), 5 figures, 2 tables. Accepted for\n  publication in the \"Scientific Bulletin\", Series C, Electrical Engineering\n  and Computer Science, ISSN 2286-3540",
      "pdf_url": "http://arxiv.org/pdf/2411.19913v1",
      "published_date": "2024-11-29 18:18:26 UTC",
      "updated_date": "2024-11-29 18:18:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:36:44.937364"
    },
    {
      "arxiv_id": "2411.19886v1",
      "title": "PDDLFuse: A Tool for Generating Diverse Planning Domains",
      "title_zh": "PDDLFuse：一种",
      "authors": [
        "Vedant Khandelwal",
        "Amit Sheth",
        "Forest Agostinelli"
      ],
      "abstract": "Various real-world challenges require planning algorithms that can adapt to a\nbroad range of domains. Traditionally, the creation of planning domains has\nrelied heavily on human implementation, which limits the scale and diversity of\navailable domains. While recent advancements have leveraged generative AI\ntechnologies such as large language models (LLMs) for domain creation, these\nefforts have predominantly focused on translating existing domains from natural\nlanguage descriptions rather than generating novel ones. In contrast, the\nconcept of domain randomization, which has been highly effective in\nreinforcement learning, enhances performance and generalizability by training\non a diverse array of randomized new domains. Inspired by this success, our\ntool, PDDLFuse, aims to bridge this gap in Planning Domain Definition Language\n(PDDL). PDDLFuse is designed to generate new, diverse planning domains that can\nbe used to validate new planners or test foundational planning models. We have\ndeveloped methods to adjust the domain generators parameters to modulate the\ndifficulty of the domains it generates. This adaptability is crucial as\nexisting domain-independent planners often struggle with more complex problems.\nInitial tests indicate that PDDLFuse efficiently creates intricate and varied\ndomains, representing a significant advancement over traditional domain\ngeneration methods and making a contribution towards planning research.",
      "tldr_zh": "该论文提出PDDLFuse，一种工具，用于生成多样化的Planning Domain Definition Language (PDDL)规划域，以解决传统方法依赖人类实现导致的规模和多样性限制问题。该工具受domain randomization启发，通过调整参数来创建新颖的规划域，并控制其难度，以提升规划算法的适应性和泛化能力。与现有基于large language models (LLMs)的域翻译方法不同，PDDLFuse专注于生成原创域，用于验证新规划器或测试基础模型。初步测试显示，该工具能高效产生复杂多样的域，为规划研究带来显著进步。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "218 Tables, 3 Figures, 4 Algorithms",
      "pdf_url": "http://arxiv.org/pdf/2411.19886v1",
      "published_date": "2024-11-29 17:52:39 UTC",
      "updated_date": "2024-11-29 17:52:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:36:56.455739"
    },
    {
      "arxiv_id": "2411.19876v3",
      "title": "LUMIA: Linear probing for Unimodal and MultiModal Membership Inference Attacks leveraging internal LLM states",
      "title_zh": "LUMIA：利用内部 LLM 状态的单模态和多模态成员推理攻击的线性探针",
      "authors": [
        "Luis Ibanez-Lissen",
        "Lorena Gonzalez-Manzano",
        "Jose Maria de Fuentes",
        "Nicolas Anciaux",
        "Joaquin Garcia-Alfaro"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly used in a variety of\napplications, but concerns around membership inference have grown in parallel.\nPrevious efforts focus on black-to-grey-box models, thus neglecting the\npotential benefit from internal LLM information. To address this, we propose\nthe use of Linear Probes (LPs) as a method to detect Membership Inference\nAttacks (MIAs) by examining internal activations of LLMs. Our approach, dubbed\nLUMIA, applies LPs layer-by-layer to get fine-grained data on the model inner\nworkings. We test this method across several model architectures, sizes and\ndatasets, including unimodal and multimodal tasks. In unimodal MIA, LUMIA\nachieves an average gain of 15.71 % in Area Under the Curve (AUC) over previous\ntechniques. Remarkably, LUMIA reaches AUC>60% in 65.33% of cases -- an\nincrement of 46.80% against the state of the art. Furthermore, our approach\nreveals key insights, such as the model layers where MIAs are most detectable.\nIn multimodal models, LPs indicate that visual inputs can significantly\ncontribute to detect MIAs -- AUC>60% is reached in 85.90% of experiments.",
      "tldr_zh": "该研究提出 LUMIA 方法，使用 Linear Probes (LPs) 分析 Large Language Models (LLMs) 的内部激活，以检测 Membership Inference Attacks (MIAs)，适用于单模态和多模态任务。LUMIA 通过逐层应用 LPs 获取模型内部细粒度信息，并在多种模型架构、规模和数据集上进行测试。实验结果显示，在单模态 MIA 中，LUMIA 比现有技术平均提高 15.71% 的 Area Under the Curve (AUC)，在 65.33% 的情况下达到 AUC > 60%，并揭示 MIAs 最易检测的模型层；在多模态任务中，视觉输入显著提升检测性能，在 85.90% 的实验中达到 AUC > 60%。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19876v3",
      "published_date": "2024-11-29 17:38:56 UTC",
      "updated_date": "2025-01-10 15:08:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:37:08.935184"
    },
    {
      "arxiv_id": "2411.19875v1",
      "title": "Enhanced anomaly detection in well log data through the application of ensemble GANs",
      "title_zh": "翻译失败",
      "authors": [
        "Abdulrahman Al-Fakih",
        "A. Koeshidayatullah",
        "Tapan Mukerji",
        "SanLinn I. Kaka"
      ],
      "abstract": "Although generative adversarial networks (GANs) have shown significant\nsuccess in modeling data distributions for image datasets, their application to\nstructured or tabular data, such as well logs, remains relatively\nunderexplored. This study extends the ensemble GANs (EGANs) framework to\ncapture the distribution of well log data and detect anomalies that fall\noutside of these distributions. The proposed approach compares the performance\nof traditional methods, such as Gaussian mixture models (GMMs), with EGANs in\ndetecting anomalies outside the expected data distributions. For the gamma ray\n(GR) dataset, EGANs achieved a precision of 0.62 and F1 score of 0.76,\noutperforming GMM's precision of 0.38 and F1 score of 0.54. Similarly, for\ntravel time (DT), EGANs achieved a precision of 0.70 and F1 score of 0.79,\nsurpassing GMM 0.56 and 0.71. In the neutron porosity (NPHI) dataset, EGANs\nrecorded a precision of 0.53 and F1 score of 0.68, outshining GMM 0.47 and\n0.61. For the bulk density (RHOB) dataset, EGANs achieved a precision of 0.52\nand an F1 score of 0.67, slightly outperforming GMM, which yielded a precision\nof 0.50 and an F1 score of 0.65. This work's novelty lies in applying EGANs for\nwell log data analysis, showcasing their ability to learn data patterns and\nidentify anomalies that deviate from them. This approach offers more reliable\nanomaly detection compared to traditional methods like GMM. The findings\nhighlight the potential of EGANs in enhancing anomaly detection for well log\ndata, delivering significant implications for optimizing drilling strategies\nand reservoir management through more accurate, data-driven insights into\nsubsurface characterization.",
      "tldr_zh": "本研究扩展了 ensemble GANs (EGANs) 的应用，用于捕获井日志数据的分布并检测异常，相比传统方法如 Gaussian mixture models (GMMs)，EGANs 在多个数据集上表现出色，例如在 gamma ray (GR) 数据集上，EGANs 的 precision 为 0.62 和 F1 score 为 0.76，优于 GMMs 的 0.38 和 0.54。实验结果显示，EGANs 在 travel time (DT)、neutron porosity (NPHI) 和 bulk density (RHOB) 数据集上也实现了更高的 precision 和 F1 score，证明了其在学习数据模式和识别异常方面的优势。该方法创新性地应用于井日志分析，提供更可靠的异常检测，有助于优化钻井策略和水库管理。",
      "categories": [
        "physics.geo-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "physics.geo-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19875v1",
      "published_date": "2024-11-29 17:36:31 UTC",
      "updated_date": "2024-11-29 17:36:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:37:21.570740"
    },
    {
      "arxiv_id": "2411.19870v1",
      "title": "DeMo: Decoupled Momentum Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Bowen Peng",
        "Jeffrey Quesnelle",
        "Diederik P. Kingma"
      ],
      "abstract": "Training large neural networks typically requires sharing gradients between\naccelerators through specialized high-speed interconnects. Drawing from the\nsignal processing principles of frequency decomposition and energy compaction,\nwe demonstrate that synchronizing full optimizer states and model parameters\nduring training is unnecessary. By decoupling momentum updates and allowing\ncontrolled divergence in optimizer states across accelerators, we achieve\nimproved convergence compared to state-of-the-art optimizers. We introduce\n{\\textbf{De}}coupled {\\textbf{Mo}}mentum (DeMo), a fused optimizer and data\nparallel algorithm that reduces inter-accelerator communication requirements by\nseveral orders of magnitude. This enables training of large neural networks\neven with limited network bandwidth and heterogeneous hardware. Our method is\ntopology-agnostic and architecture-independent and supports scalable\nclock-synchronous distributed training with negligible compute and memory\noverhead. Empirical results show that models trained with DeMo match or exceed\nthe performance of equivalent models trained with AdamW, while eliminating the\nneed for high-speed interconnects when pre-training large scale foundation\nmodels. An open source reference PyTorch implementation is published on GitHub\nat https://github.com/bloc97/DeMo",
      "tldr_zh": "该研究提出DeMo（Decoupled Momentum）优化器，通过频率分解和能量压缩的信号处理原理，解耦动量更新并允许优化器状态在加速器之间受控发散，从而减少分布式训练中的通信需求几个数量级。DeMo支持在有限带宽和异构硬件上训练大型神经网络，拓扑无关、架构独立，并以微不足道的计算和内存开销实现可扩展的时钟同步训练。实验结果显示，使用DeMo训练的模型性能与AdamW相当或更好，且无需高速互连；开源PyTorch实现已发布于GitHub。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19870v1",
      "published_date": "2024-11-29 17:31:47 UTC",
      "updated_date": "2024-11-29 17:31:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:37:32.624783"
    },
    {
      "arxiv_id": "2411.19865v2",
      "title": "Reverse Thinking Makes LLMs Stronger Reasoners",
      "title_zh": "逆向思维使大型语言模型更强的推理者",
      "authors": [
        "Justin Chih-Yao Chen",
        "Zifeng Wang",
        "Hamid Palangi",
        "Rujun Han",
        "Sayna Ebrahimi",
        "Long Le",
        "Vincent Perot",
        "Swaroop Mishra",
        "Mohit Bansal",
        "Chen-Yu Lee",
        "Tomas Pfister"
      ],
      "abstract": "Reverse thinking plays a crucial role in human reasoning. Humans can reason\nnot only from a problem to a solution but also in reverse, i.e., start from the\nsolution and reason towards the problem. This often enhances overall reasoning\nperformance as it enables consistency checks between their forward and backward\nthinking. To enable Large Language Models (LLMs) to perform reverse thinking,\nwe introduce Reverse-Enhanced Thinking (RevThink), a framework composed of data\naugmentation and learning objectives. In RevThink, we augment the dataset by\ncollecting structured forward-backward reasoning from a teacher model,\nconsisting of: (1) the original question, (2) forward reasoning, (3) backward\nquestion, and (4) backward reasoning. We then employ three objectives to train\na smaller student model in a multi-task learning fashion: (a) generate forward\nreasoning from a question, (b) generate a backward question from a question,\nand (c) generate backward reasoning from the backward question. Experiments\nacross 12 datasets covering commonsense, math, and logical reasoning show an\naverage 13.53% improvement over the student model's zero-shot performance and a\n6.84% improvement over the strongest knowledge distillation baselines.\nMoreover, our method demonstrates sample efficiency -- using only 10% of the\ncorrect forward reasoning from the training data, it outperforms a standard\nfine-tuning method trained on 10x more forward reasoning. RevThink also\nexhibits strong generalization to out-of-distribution held-out datasets.",
      "tldr_zh": "该论文提出 Reverse-Enhanced Thinking (RevThink) 框架，以增强大型语言模型 (LLMs) 的推理能力，通过模拟人类逆向思维（从解决方案回溯到问题）来提升整体性能。框架包括数据增强步骤，利用教师模型生成结构化的正向-逆向推理数据，以及多任务学习目标：从问题生成正向推理、生成逆向问题和逆向推理。实验在12个涵盖常识、数学和逻辑推理的数据集上显示，RevThink 使学生模型的 zero-shot 性能平均提升13.53%，比最强知识蒸馏基线提高6.84%，并展现出高样本效率和对分布外数据集的强泛化能力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to NAACL 2025",
      "pdf_url": "http://arxiv.org/pdf/2411.19865v2",
      "published_date": "2024-11-29 17:27:05 UTC",
      "updated_date": "2025-03-07 20:33:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:37:45.516487"
    },
    {
      "arxiv_id": "2411.19842v1",
      "title": "Scaling Transformers for Low-Bitrate High-Quality Speech Coding",
      "title_zh": "翻译失败",
      "authors": [
        "Julian D Parker",
        "Anton Smirnov",
        "Jordi Pons",
        "CJ Carr",
        "Zack Zukowski",
        "Zach Evans",
        "Xubo Liu"
      ],
      "abstract": "The tokenization of speech with neural audio codec models is a vital part of\nmodern AI pipelines for the generation or understanding of speech, alone or in\na multimodal context. Traditionally such tokenization models have concentrated\non low parameter-count architectures using only components with strong\ninductive biases. In this work we show that by scaling a transformer\narchitecture with large parameter count to this problem, and applying a\nflexible Finite Scalar Quantization (FSQ) based bottleneck, it is possible to\nreach state-of-the-art speech quality at extremely low bit-rates of $400$ or\n$700$ bits-per-second. The trained models strongly out-perform existing\nbaselines in both objective and subjective tests.",
      "tldr_zh": "本文研究了通过扩展 Transformer 架构来实现低比特率高品质语音编码，针对传统神经音频编解码模型的参数限制问题。作者采用大规模 Transformer 模型结合灵活的 Finite Scalar Quantization (FSQ) 瓶颈，实现了在 400 或 700 bits-per-second 的极低比特率下达到最先进的语音质量。该方法在客观和主观测试中显著优于现有基线，为语音生成和理解的 AI 管道提供了新途径。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.SP"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19842v1",
      "published_date": "2024-11-29 16:58:02 UTC",
      "updated_date": "2024-11-29 16:58:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:37:56.499831"
    },
    {
      "arxiv_id": "2411.19809v1",
      "title": "Q-learning-based Model-free Safety Filter",
      "title_zh": "基于 Q-learning 的无模型安全过滤器",
      "authors": [
        "Guo Ning Sue",
        "Yogita Choudhary",
        "Richard Desatnik",
        "Carmel Majidi",
        "John Dolan",
        "Guanya Shi"
      ],
      "abstract": "Ensuring safety via safety filters in real-world robotics presents\nsignificant challenges, particularly when the system dynamics is complex or\nunavailable. To handle this issue, learning-based safety filters recently\ngained popularity, which can be classified as model-based and model-free\nmethods. Existing model-based approaches requires various assumptions on system\nmodel (e.g., control-affine), which limits their application in complex\nsystems, and existing model-free approaches need substantial modifications to\nstandard RL algorithms and lack versatility. This paper proposes a simple,\nplugin-and-play, and effective model-free safety filter learning framework. We\nintroduce a novel reward formulation and use Q-learning to learn Q-value\nfunctions to safeguard arbitrary task specific nominal policies via filtering\nout their potentially unsafe actions. The threshold used in the filtering\nprocess is supported by our theoretical analysis. Due to its model-free nature\nand simplicity, our framework can be seamlessly integrated with various RL\nalgorithms. We validate the proposed approach through simulations on double\nintegrator and Dubin's car systems and demonstrate its effectiveness in\nreal-world experiments with a soft robotic limb.",
      "tldr_zh": "这篇论文提出了一种基于 Q-learning 的 model-free 安全过滤器框架，用于处理复杂机器人系统动态未知时的安全问题，克服了现有 model-based 方法的系统假设限制和 model-free 方法的修改复杂性。框架引入新型奖励公式，通过学习 Q-value 函数来过滤任务特定名义策略的潜在不安全动作，并由理论分析支持过滤阈值的合理性。其简单且即插即用特性允许无缝集成到各种 RL algorithms 中，并在 double integrator、Dubin's car 系统模拟以及软机器人肢体真实实验中验证了显著的安全性提升。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "*Denotes equal contribution",
      "pdf_url": "http://arxiv.org/pdf/2411.19809v1",
      "published_date": "2024-11-29 16:16:59 UTC",
      "updated_date": "2024-11-29 16:16:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:38:09.350032"
    },
    {
      "arxiv_id": "2411.19806v2",
      "title": "Zero-shot Musical Stem Retrieval with Joint-Embedding Predictive Architectures",
      "title_zh": "翻译失败",
      "authors": [
        "Alain Riou",
        "Antonin Gagneré",
        "Gaëtan Hadjeres",
        "Stefan Lattner",
        "Geoffroy Peeters"
      ],
      "abstract": "In this paper, we tackle the task of musical stem retrieval. Given a musical\nmix, it consists in retrieving a stem that would fit with it, i.e., that would\nsound pleasant if played together. To do so, we introduce a new method based on\nJoint-Embedding Predictive Architectures, where an encoder and a predictor are\njointly trained to produce latent representations of a context and predict\nlatent representations of a target. In particular, we design our predictor to\nbe conditioned on arbitrary instruments, enabling our model to perform\nzero-shot stem retrieval. In addition, we discover that pretraining the encoder\nusing contrastive learning drastically improves the model's performance.\n  We validate the retrieval performances of our model using the MUSDB18 and\nMoisesDB datasets. We show that it significantly outperforms previous baselines\non both datasets, showcasing its ability to support more or less precise (and\npossibly unseen) conditioning. We also evaluate the learned embeddings on a\nbeat tracking task, demonstrating that they retain temporal structure and local\ninformation.",
      "tldr_zh": "本论文针对音乐干轨检索任务提出了一种基于 Joint-Embedding Predictive Architectures 的新方法，该方法通过联合训练编码器和预测器来生成上下文和目标的潜在表示，并使预测器能够根据任意乐器进行条件化，从而实现 Zero-shot 干轨检索。创新点包括使用对比学习预训练编码器，以显著提升模型性能，并在 MUSDB18 和 MoisesDB 数据集上验证其效果。实验结果显示，该模型在检索准确性上优于现有基线，并证明学到的嵌入保留了时间结构和局部信息，可支持更精确或未见过的条件化。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted to the IEEE International Conference on Acoustics, Speech\n  and Signal Processing (ICASSP 2025)",
      "pdf_url": "http://arxiv.org/pdf/2411.19806v2",
      "published_date": "2024-11-29 16:11:47 UTC",
      "updated_date": "2025-02-24 17:10:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:38:20.018850"
    },
    {
      "arxiv_id": "2411.19804v1",
      "title": "Advanced System Integration: Analyzing OpenAPI Chunking for Retrieval-Augmented Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Robin D. Pesl",
        "Jerin G. Mathew",
        "Massimo Mecella",
        "Marco Aiello"
      ],
      "abstract": "Integrating multiple (sub-)systems is essential to create advanced\nInformation Systems (ISs). Difficulties mainly arise when integrating dynamic\nenvironments across the IS lifecycle. A traditional approach is a registry that\nprovides the API documentation of the systems' endpoints. Large Language Models\n(LLMs) have shown to be capable of automatically creating system integrations\n(e.g., as service composition) based on this documentation but require concise\ninput due to input token limitations, especially regarding comprehensive API\ndescriptions. Currently, it is unknown how best to preprocess these API\ndescriptions. Within this work, we (i) analyze the usage of Retrieval Augmented\nGeneration (RAG) for endpoint discovery and the chunking, i.e., preprocessing,\nof OpenAPIs to reduce the input token length while preserving the most relevant\ninformation. To further reduce the input token length for the composition\nprompt and improve endpoint retrieval, we propose (ii) a Discovery Agent that\nonly receives a summary of the most relevant endpoints and retrieves details on\ndemand. We evaluate RAG for endpoint discovery using the RestBench benchmark,\nfirst, for the different chunking possibilities and parameters measuring the\nendpoint retrieval recall, precision, and F1 score. Then, we assess the\nDiscovery Agent using the same test set. With our prototype, we demonstrate how\nto successfully employ RAG for endpoint discovery to reduce the token count.\nWhile revealing high values for recall, precision, and F1, further research is\nnecessary to retrieve all requisite endpoints. Our experiments show that for\npreprocessing, LLM-based and format-specific approaches outperform na\\\"ive\nchunking methods. Relying on an agent further enhances these results as the\nagent splits the tasks into multiple fine granular subtasks, improving the\noverall RAG performance in the token count, precision, and F1 score.",
      "tldr_zh": "这篇论文分析了 Retrieval-Augmented Generation (RAG) 在系统整合中的应用，重点探讨了 OpenAPI 文档的 chunking 预处理，以减少输入 token 长度同时保留关键信息。作者提出了一种 Discovery Agent，通过仅提供相关端点摘要并按需检索细节，进一步优化端点发现过程。实验使用 RestBench 基准评估不同 chunking 方法，结果显示 LLM-based 和格式特定方法优于 naive 方式，使用代理显著提高了端点检索的召回率、精确率和 F1 分数，为高效系统整合提供了新途径。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19804v1",
      "published_date": "2024-11-29 16:09:43 UTC",
      "updated_date": "2024-11-29 16:09:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:38:32.556742"
    },
    {
      "arxiv_id": "2411.19787v1",
      "title": "CAREL: Instruction-guided reinforcement learning with cross-modal auxiliary objectives",
      "title_zh": "CAREL：指令引导的",
      "authors": [
        "Armin Saghafian",
        "Amirmohammad Izadi",
        "Negin Hashemi Dijujin",
        "Mahdieh Soleymani Baghshah"
      ],
      "abstract": "Grounding the instruction in the environment is a key step in solving\nlanguage-guided goal-reaching reinforcement learning problems. In automated\nreinforcement learning, a key concern is to enhance the model's ability to\ngeneralize across various tasks and environments. In goal-reaching scenarios,\nthe agent must comprehend the different parts of the instructions within the\nenvironmental context in order to complete the overall task successfully. In\nthis work, we propose CAREL (Cross-modal Auxiliary REinforcement Learning) as a\nnew framework to solve this problem using auxiliary loss functions inspired by\nvideo-text retrieval literature and a novel method called instruction tracking,\nwhich automatically keeps track of progress in an environment. The results of\nour experiments suggest superior sample efficiency and systematic\ngeneralization for this framework in multi-modal reinforcement learning\nproblems. Our code base is available here.",
      "tldr_zh": "本论文提出CAREL框架，用于解决语言引导的目标到达强化学习（reinforcement learning）问题，通过在环境中对指令进行接地（grounding）来提升模型在不同任务和环境的泛化能力。CAREL引入受视频-文本检索启发的辅助损失函数（cross-modal auxiliary objectives）和一种新颖的instruction tracking方法，该方法自动跟踪环境进度以帮助代理理解指令的各个部分。实验结果显示，CAREL在多模态强化学习问题中表现出优越的样本效率和系统泛化性能，并提供了开源代码库。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19787v1",
      "published_date": "2024-11-29 15:49:06 UTC",
      "updated_date": "2024-11-29 15:49:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:38:44.248851"
    },
    {
      "arxiv_id": "2412.00167v1",
      "title": "Origin-Destination Demand Prediction: An Urban Radiation and Attraction Perspective",
      "title_zh": "翻译失败",
      "authors": [
        "Xuan Ma",
        "Zepeng Bao",
        "Ming Zhong",
        "Yuanyuan Zhu",
        "Chenliang Li",
        "Jiawei Jiang",
        "Qing Li",
        "Tieyun Qian"
      ],
      "abstract": "In recent years, origin-destination (OD) demand prediction has gained\nsignificant attention for its profound implications in urban development.\nExisting data-driven deep learning methods primarily focus on the spatial or\ntemporal dependency between regions yet neglecting regions' fundamental\nfunctional difference. Though knowledge-driven physical methods have\ncharacterised regions' functions by their radiation and attraction capacities,\nthese functions are defined on numerical factors like population without\nconsidering regions' intrinsic nominal attributes, e.g., a region is a\nresidential or industrial district. Moreover, the complicated relationships\nbetween two types of capacities, e.g., the radiation capacity of a residential\ndistrict in the morning will be transformed into the attraction capacity in the\nevening, are totally missing from physical methods.\n  In this paper, we not only generalize the physical radiation and attraction\ncapacities into the deep learning framework with the extended capability to\nfulfil regions' functions, but also present a new model that captures the\nrelationships between two types of capacities. Specifically, we first model\nregions' radiation and attraction capacities using a bilateral branch network,\neach equipped with regions' attribute representations. We then describe the\ntransformation relationship of different capacities of the same region using a\nhypergraph-based parameter generation method. We finally unveil the competition\nrelationship of different regions with the same attraction capacity through\ncluster-based adversarial learning. Extensive experiments on two datasets\ndemonstrate the consistent improvements of our method over the state-of-the-art\nbaselines, as well as the good explainability of regions' functions using their\nnominal attributes.",
      "tldr_zh": "本文提出了一种从城市辐射和吸引角度出发的 origin-destination (OD) 需求预测方法，解决了现有数据驱动深度学习模型忽略区域功能差异的问题，同时扩展了传统物理方法以纳入区域的内在属性（如住宅或工业区）。该模型使用 bilateral branch network 建模区域的辐射和吸引能力，并通过 hypergraph-based parameter generation 方法捕捉能力转换关系（如住宅区的晨辐射转为夕吸引），以及 cluster-based adversarial learning 揭示区域间的竞争关系。实验在两个数据集上显示，该方法比最先进基线显著提升预测性能，并提供了区域功能的可解释性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00167v1",
      "published_date": "2024-11-29 15:35:17 UTC",
      "updated_date": "2024-11-29 15:35:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:38:56.709119"
    },
    {
      "arxiv_id": "2412.09000v1",
      "title": "The AI Interface: Designing for the Ideal Machine-Human Experience (Editorial)",
      "title_zh": "翻译失败",
      "authors": [
        "Aparna Sundar",
        "Tony Russell-Rose",
        "Udo Kruschwitz",
        "Karen Machleit"
      ],
      "abstract": "As artificial intelligence (AI) becomes increasingly embedded in daily life,\ndesigning intuitive, trustworthy, and emotionally resonant AI-human interfaces\nhas emerged as a critical challenge. This editorial introduces a Special Issue\nthat explores the psychology of AI experience design, focusing on how\ninterfaces can foster seamless collaboration between humans and machines.\nDrawing on insights from diverse fields (healthcare, consumer technology,\nworkplace dynamics, and cultural sector), the papers in this collection\nhighlight the complexities of trust, transparency, and emotional sensitivity in\nhuman-AI interaction. Key themes include designing AI systems that align with\nuser perceptions and expectations, overcoming resistance through transparency\nand trust, and framing AI capabilities to reduce user anxiety. By synthesizing\nfindings from eight diverse studies, this editorial underscores the need for AI\ninterfaces to balance efficiency with empathy, addressing both functional and\nemotional dimensions of user experience. Ultimately, it calls for actionable\nframeworks to bridge research and practice, ensuring that AI systems enhance\nhuman lives through thoughtful, human-centered design.",
      "tldr_zh": "这篇社论介绍了特别专辑，探讨AI接口设计如何实现直观、可信且情感共鸣的人机互动体验。文章从医疗、消费技术、工作动态和文化领域汲取见解，焦点在于信任、透明度和情感敏感性，强调AI系统需与用户感知和期望一致，以减少抵抗和焦虑。通过合成八个研究的发现，它呼吁开发行动框架，平衡AI的效率与同理心，确保人类中心设计提升用户生活。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "H.5.2"
      ],
      "primary_category": "cs.HC",
      "comment": "8 pages",
      "pdf_url": "http://arxiv.org/pdf/2412.09000v1",
      "published_date": "2024-11-29 15:17:32 UTC",
      "updated_date": "2024-11-29 15:17:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:39:08.161916"
    },
    {
      "arxiv_id": "2411.19766v1",
      "title": "Stock Price Prediction using Multi-Faceted Information based on Deep Recurrent Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Lida Shahbandari",
        "Elahe Moradi",
        "Mohammad Manthouri"
      ],
      "abstract": "Accurate prediction of stock market trends is crucial for informed investment\ndecisions and effective portfolio management, ultimately leading to enhanced\nwealth creation and risk mitigation. This study proposes a novel approach for\npredicting stock prices in the stock market by integrating Convolutional Neural\nNetworks (CNN) and Long Short-Term Memory (LSTM) networks, using sentiment\nanalysis of social network data and candlestick data (price). The proposed\nmethodology consists of two primary components: sentiment analysis of social\nnetwork and candlestick data. By amalgamating candlestick data with insights\ngleaned from Twitter, this approach facilitates a more detailed and accurate\nexamination of market trends and patterns, ultimately leading to more effective\nstock price predictions. Additionally, a Random Forest algorithm is used to\nclassify tweets as either positive or negative, allowing for a more subtle and\ninformed assessment of market sentiment. This study uses CNN and LSTM networks\nto predict stock prices. The CNN extracts short-term features, while the LSTM\nmodels long-term dependencies. The integration of both networks enables a more\ncomprehensive analysis of market trends and patterns, leading to more accurate\nstock price predictions.",
      "tldr_zh": "本研究提出了一种基于深度循环神经网络的股票价格预测方法，通过整合多方面信息（如社交网络的sentiment analysis和candlestick data）来提升预测准确性。方法包括使用Random Forest算法对Twitter推文进行正负分类，以提取市场情绪洞见；CNN用于提取短期特征，而LSTM则建模长期依赖关系。最终，通过融合这些组件，该方法实现了对市场趋势的更全面分析，并在投资决策、财富创造和风险缓解方面提供了更有效的支持。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19766v1",
      "published_date": "2024-11-29 15:12:48 UTC",
      "updated_date": "2024-11-29 15:12:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:39:19.013540"
    },
    {
      "arxiv_id": "2411.19763v1",
      "title": "Forecasting Foreign Exchange Market Prices Using Technical Indicators with Deep Learning and Attention Mechanism",
      "title_zh": "使用技术",
      "authors": [
        "Sahabeh Saadati",
        "Mohammad Manthouri"
      ],
      "abstract": "Accurate prediction of price behavior in the foreign exchange market is\ncrucial. This paper proposes a novel approach that leverages technical\nindicators and deep neural networks. The proposed architecture consists of a\nLong Short-Term Memory (LSTM) and Convolutional Neural Network (CNN), and\nattention mechanism. Initially, trend and oscillation technical indicators are\nemployed to extract statistical features from Forex currency pair data,\nproviding insights into price trends, market volatility, relative price\nstrength, and overbought and oversold conditions. Subsequently, the LSTM and\nCNN networks are utilized in parallel to predict future price movements,\nleveraging the strengths of both recurrent and convolutional architectures. The\nLSTM network captures long-term dependencies and temporal patterns in the data,\nwhile the CNN network extracts local patterns. The outputs of the parallel LSTM\nand CNN networks are then fed into an attention mechanism, which learns to\nweigh the importance of each feature and temporal dependency, generating a\ncontext-aware representation of the input data. The attention-weighted output\nis then used to predict future price movements, enabling the model to focus on\nthe most relevant features and temporal dependencies. Through a comprehensive\nevaluation of the proposed approach on multiple Forex currency pairs, we\ndemonstrate its effectiveness in predicting price behavior and outperforming\nbenchmark models.",
      "tldr_zh": "本研究提出了一种新方法，使用技术指标结合深度学习和注意力机制来预测外汇（Forex）市场价格行为。具体而言，该方法首先利用趋势和震荡技术指标从外汇货币对数据中提取统计特征，如价格趋势、市场波动和相对强度；然后并行运用 Long Short-Term Memory (LSTM) 网络捕捉长期依赖和时间模式，以及 Convolutional Neural Network (CNN) 网络提取局部模式。接着，通过注意力机制对 LSTM 和 CNN 的输出进行加权，生成上下文感知表示，以更准确地预测未来价格变动。实验结果显示，该方法在多个外汇货币对上的预测性能优于基准模型，证明了其有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19763v1",
      "published_date": "2024-11-29 15:07:44 UTC",
      "updated_date": "2024-11-29 15:07:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:39:31.333887"
    },
    {
      "arxiv_id": "2411.19758v1",
      "title": "LaVIDE: A Language-Vision Discriminator for Detecting Changes in Satellite Image with Map References",
      "title_zh": "翻译失败",
      "authors": [
        "Shuguo Jiang",
        "Fang Xu",
        "Sen Jia",
        "Gui-Song Xia"
      ],
      "abstract": "Change detection, which typically relies on the comparison of bi-temporal\nimages, is significantly hindered when only a single image is available.\nComparing a single image with an existing map, such as OpenStreetMap, which is\ncontinuously updated through crowd-sourcing, offers a viable solution to this\nchallenge. Unlike images that carry low-level visual details of ground objects,\nmaps convey high-level categorical information. This discrepancy in abstraction\nlevels complicates the alignment and comparison of the two data types. In this\npaper, we propose a \\textbf{La}nguage-\\textbf{VI}sion \\textbf{D}iscriminator\nfor d\\textbf{E}tecting changes in satellite image with map references, namely\n\\ours{}, which leverages language to bridge the information gap between maps\nand images. Specifically, \\ours{} formulates change detection as the problem of\n``{\\textit Does the pixel belong to [class]?}'', aligning maps and images\nwithin the feature space of the language-vision model to associate high-level\nmap categories with low-level image details. Moreover, we build a\nmixture-of-experts discriminative module, which compares linguistic features\nfrom maps with visual features from images across various semantic\nperspectives, achieving comprehensive semantic comparison for change detection.\nExtensive evaluation on four benchmark datasets demonstrates that \\ours{} can\neffectively detect changes in satellite image with map references,\noutperforming state-of-the-art change detection algorithms, e.g., with gains of\nabout $13.8$\\% on the DynamicEarthNet dataset and $4.3$\\% on the SECOND\ndataset.",
      "tldr_zh": "该论文提出 LaVIDE，一种语言-视觉辨别器，用于在仅有单张卫星图像时，通过与地图（如 OpenStreetMap）参考进行变化检测，以解决地图高层类别信息与图像低层视觉细节之间的抽象差异。LaVIDE 将变化检测问题表述为“像素是否属于[class]？”的形式，利用语言-视觉模型对齐地图和图像特征，并构建混合专家辨别模块，从多个语义视角比较特征以实现全面的语义对比。实验结果显示，LaVIDE 在四个基准数据集上显著优于现有算法，例如在 DynamicEarthNet 数据集上提升 13.8%，在 SECOND 数据集上提升 4.3%。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19758v1",
      "published_date": "2024-11-29 15:04:40 UTC",
      "updated_date": "2024-11-29 15:04:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:39:44.410792"
    },
    {
      "arxiv_id": "2412.12107v1",
      "title": "Generative AI Literacy: Twelve Defining Competencies",
      "title_zh": "翻译失败",
      "authors": [
        "Ravinithesh Annapureddy",
        "Alessandro Fornaroli",
        "Daniel Gatica-Perez"
      ],
      "abstract": "This paper introduces a competency-based model for generative artificial\nintelligence (AI) literacy covering essential skills and knowledge areas\nnecessary to interact with generative AI. The competencies range from\nfoundational AI literacy to prompt engineering and programming skills,\nincluding ethical and legal considerations. These twelve competencies offer a\nframework for individuals, policymakers, government officials, and educators\nlooking to navigate and take advantage of the potential of generative AI\nresponsibly. Embedding these competencies into educational programs and\nprofessional training initiatives can equip individuals to become responsible\nand informed users and creators of generative AI. The competencies follow a\nlogical progression and serve as a roadmap for individuals seeking to get\nfamiliar with generative AI and for researchers and policymakers to develop\nassessments, educational programs, guidelines, and regulations.",
      "tldr_zh": "这篇论文提出了一种基于能力的模型，定义了12个核心素养，用于生成式人工智能（AI） literacy，包括基础AI素养、prompt engineering、编程技能以及伦理和法律考虑。这些素养框架旨在帮助个人、政策制定者、政府官员和教育者负责任地利用生成式AI的潜力，并将其融入教育和专业培训中，以培养负责任的用户和创建者。该模型遵循逻辑进展，作为路线图，支持研究人员和决策者开发评估、教育程序、指南和法规。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.12107v1",
      "published_date": "2024-11-29 14:55:15 UTC",
      "updated_date": "2024-11-29 14:55:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:39:55.910634"
    },
    {
      "arxiv_id": "2411.19747v1",
      "title": "A Multi-Loss Strategy for Vehicle Trajectory Prediction: Combining Off-Road, Diversity, and Directional Consistency Losses",
      "title_zh": "翻译失败",
      "authors": [
        "Ahmad Rahimi",
        "Alexandre Alahi"
      ],
      "abstract": "Trajectory prediction is essential for the safety and efficiency of planning\nin autonomous vehicles. However, current models often fail to fully capture\ncomplex traffic rules and the complete range of potential vehicle movements.\nAddressing these limitations, this study introduces three novel loss functions:\nOffroad Loss, Direction Consistency Error, and Diversity Loss. These functions\nare designed to keep predicted paths within driving area boundaries, aligned\nwith traffic directions, and cover a wider variety of plausible driving\nscenarios. As all prediction modes should adhere to road rules and conditions,\nthis work overcomes the shortcomings of traditional \"winner takes all\" training\nmethods by applying the loss functions to all prediction modes. These loss\nfunctions not only improve model training but can also serve as metrics for\nevaluating the realism and diversity of trajectory predictions. Extensive\nvalidation on the nuScenes and Argoverse 2 datasets with leading baseline\nmodels demonstrates that our approach not only maintains accuracy but\nsignificantly improves safety and robustness, reducing offroad errors on\naverage by 47% on original and by 37% on attacked scenes. This work sets a new\nbenchmark for trajectory prediction in autonomous driving, offering substantial\nimprovements in navigating complex environments. Our code is available at\nhttps://github.com/vita-epfl/stay-on-track .",
      "tldr_zh": "本研究针对自动驾驶车辆轨迹预测中的问题，引入了Offroad Loss、Direction Consistency Error和Diversity Loss三种新损失函数，以确保预测路径保持在道路边界内、符合交通方向并覆盖更多多样化的驾驶场景。\n与传统“winner takes all”方法不同，该多损失策略应用于所有预测模式，从而提升了模型训练的全面性和评估指标的实用性。\n在nuScenes和Argoverse 2数据集上的实验验证显示，与基线模型相比，该方法平均减少了47%的offroad错误（原场景）和37%（攻击场景），显著提高了预测的安全性和鲁棒性。\n这项工作为自动驾驶轨迹预测设定了新基准，并提供了开源代码（https://github.com/vita-epfl/stay-on-track）。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MA",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint, 7 pages, 4 figures and 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2411.19747v1",
      "published_date": "2024-11-29 14:47:08 UTC",
      "updated_date": "2024-11-29 14:47:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:40:08.797905"
    },
    {
      "arxiv_id": "2411.19746v1",
      "title": "HVAC-DPT: A Decision Pretrained Transformer for HVAC Control",
      "title_zh": "翻译失败",
      "authors": [
        "Anaïs Berkes"
      ],
      "abstract": "Building operations consume approximately 40% of global energy, with Heating,\nVentilation, and Air Conditioning (HVAC) systems responsible for up to 50% of\nthis consumption. As HVAC energy demands are expected to rise, optimising\nsystem efficiency is crucial for reducing future energy use and mitigating\nclimate change. Existing control strategies lack generalisation and require\nextensive training and data, limiting their rapid deployment across diverse\nbuildings. This paper introduces HVAC-DPT, a Decision-Pretrained Transformer\nusing in-context Reinforcement Learning (RL) for multi-zone HVAC control.\nHVAC-DPT frames HVAC control as a sequential prediction task, training a causal\ntransformer on interaction histories generated by diverse RL agents. This\napproach enables HVAC-DPT to refine its policy in-context, without modifying\nnetwork parameters, allowing for deployment across different buildings without\nthe need for additional training or data collection. HVAC-DPT reduces energy\nconsumption in unseen buildings by 45% compared to the baseline controller,\noffering a scalable and effective approach to mitigating the increasing\nenvironmental impact of HVAC systems.",
      "tldr_zh": "本文研究了HVAC系统在建筑能源消耗中的关键作用，指出现有控制策略缺乏泛化性，需要大量训练数据。论文提出HVAC-DPT，一种Decision-Pretrained Transformer，利用in-context Reinforcement Learning (RL)将HVAC控制框架化为顺序预测任务，并在多样RL代理生成的交互历史中训练。HVAC-DPT能够在不同建筑中通过上下文优化策略，而无需修改网络参数或额外数据收集，最终在未见环境中比基线控制器减少45%的能源消耗，提供可扩展的解决方案来缓解HVAC的环境影响。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "comment": "7 pages, 3 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2411.19746v1",
      "published_date": "2024-11-29 14:46:37 UTC",
      "updated_date": "2024-11-29 14:46:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:40:20.190807"
    },
    {
      "arxiv_id": "2412.00166v1",
      "title": "To Ensemble or Not: Assessing Majority Voting Strategies for Phishing Detection with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Fouad Trad",
        "Ali Chehab"
      ],
      "abstract": "The effectiveness of Large Language Models (LLMs) significantly relies on the\nquality of the prompts they receive. However, even when processing identical\nprompts, LLMs can yield varying outcomes due to differences in their training\nprocesses. To leverage the collective intelligence of multiple LLMs and enhance\ntheir performance, this study investigates three majority voting strategies for\ntext classification, focusing on phishing URL detection. The strategies are:\n(1) a prompt-based ensemble, which utilizes majority voting across the\nresponses generated by a single LLM to various prompts; (2) a model-based\nensemble, which entails aggregating responses from multiple LLMs to a single\nprompt; and (3) a hybrid ensemble, which combines the two methods by sending\ndifferent prompts to multiple LLMs and then aggregating their responses. Our\nanalysis shows that ensemble strategies are most suited in cases where\nindividual components exhibit equivalent performance levels. However, when\nthere is a significant discrepancy in individual performance, the effectiveness\nof the ensemble method may not exceed that of the highest-performing single LLM\nor prompt. In such instances, opting for ensemble techniques is not\nrecommended.",
      "tldr_zh": "这篇论文评估了使用 Large Language Models (LLMs) 进行欺骗 URL 检测时的三种多数投票策略，以提升模型性能。策略包括：prompt-based ensemble（对单个 LLM 使用多个提示并投票）、model-based ensemble（对多个 LLM 使用同一提示并聚合响应），以及 hybrid ensemble（结合不同提示和多个 LLM 后聚合）。研究发现，当个体组件性能相当时，集成策略能显著改善检测效果；但若性能差异明显，集成方法可能不如最佳单个 LLM 或提示有效，因此在这种情况下不推荐使用集成。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted in 4th International Conference on Intelligent Systems and\n  Pattern Recognition (ISPR24)",
      "pdf_url": "http://arxiv.org/pdf/2412.00166v1",
      "published_date": "2024-11-29 14:42:23 UTC",
      "updated_date": "2024-11-29 14:42:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:40:31.728914"
    },
    {
      "arxiv_id": "2411.19744v1",
      "title": "Amplifying human performance in combinatorial competitive programming",
      "title_zh": "翻译失败",
      "authors": [
        "Petar Veličković",
        "Alex Vitvitskyi",
        "Larisa Markeeva",
        "Borja Ibarz",
        "Lars Buesing",
        "Matej Balog",
        "Alexander Novikov"
      ],
      "abstract": "Recent years have seen a significant surge in complex AI systems for\ncompetitive programming, capable of performing at admirable levels against\nhuman competitors. While steady progress has been made, the highest percentiles\nstill remain out of reach for these methods on standard competition platforms\nsuch as Codeforces. Here we instead focus on combinatorial competitive\nprogramming, where the target is to find as-good-as-possible solutions to\notherwise computationally intractable problems, over specific given inputs. We\nhypothesise that this scenario offers a unique testbed for human-AI synergy, as\nhuman programmers can write a backbone of a heuristic solution, after which AI\ncan be used to optimise the scoring function used by the heuristic. We deploy\nour approach on previous iterations of Hash Code, a global team programming\ncompetition inspired by NP-hard software engineering problems at Google, and we\nleverage FunSearch to evolve our scoring functions. Our evolved solutions\nsignificantly improve the attained scores from their baseline, successfully\nbreaking into the top percentile on all previous Hash Code online qualification\nrounds, and outperforming the top human teams on several. Our method is also\nperformant on an optimisation problem that featured in a recent held-out\nAtCoder contest.",
      "tldr_zh": "该研究探讨了在组合式竞争编程(combinatorial competitive programming)中，通过人类-AI 协同方式增强人类性能。研究假设人类编写基础启发式解决方案，然后利用 AI 方法如 FunSearch 来优化评分函数，从而解决计算上棘手的优化问题。在实验中，该方法应用于过去的 Hash Code 比赛，显著提升了基线分数，成功进入前百分位并在多个回合中超越顶级人类团队，同时在 AtCoder 的独立优化问题上也表现出色。总的来说，这一方法为人类-AI 协同在编程竞赛中的应用提供了新颖的框架。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE",
        "cs.PL"
      ],
      "primary_category": "cs.LG",
      "comment": "Technical report. 18 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.19744v1",
      "published_date": "2024-11-29 14:40:36 UTC",
      "updated_date": "2024-11-29 14:40:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:40:43.091133"
    },
    {
      "arxiv_id": "2411.19742v1",
      "title": "Graph Neural Networks for Heart Failure Prediction on an EHR-Based Patient Similarity Graph",
      "title_zh": "翻译失败",
      "authors": [
        "Heloisa Oss Boll",
        "Ali Amirahmadi",
        "Amira Soliman",
        "Stefan Byttner",
        "Mariana Recamonde-Mendoza"
      ],
      "abstract": "Objective: In modern healthcare, accurately predicting diseases is a crucial\nmatter. This study introduces a novel approach using graph neural networks\n(GNNs) and a Graph Transformer (GT) to predict the incidence of heart failure\n(HF) on a patient similarity graph at the next hospital visit. Materials and\nMethods: We used electronic health records (EHR) from the MIMIC-III dataset and\napplied the K-Nearest Neighbors (KNN) algorithm to create a patient similarity\ngraph using embeddings from diagnoses, procedures, and medications. Three\nmodels - GraphSAGE, Graph Attention Network (GAT), and Graph Transformer (GT) -\nwere implemented to predict HF incidence. Model performance was evaluated using\nF1 score, AUROC, and AUPRC metrics, and results were compared against baseline\nalgorithms. An interpretability analysis was performed to understand the\nmodel's decision-making process. Results: The GT model demonstrated the best\nperformance (F1 score: 0.5361, AUROC: 0.7925, AUPRC: 0.5168). Although the\nRandom Forest (RF) baseline achieved a similar AUPRC value, the GT model\noffered enhanced interpretability due to the use of patient relationships in\nthe graph structure. A joint analysis of attention weights, graph connectivity,\nand clinical features provided insight into model predictions across different\nclassification groups. Discussion and Conclusion: Graph-based approaches such\nas GNNs provide an effective framework for predicting HF. By leveraging a\npatient similarity graph, GNNs can capture complex relationships in EHR data,\npotentially improving prediction accuracy and clinical interpretability.",
      "tldr_zh": "这篇论文使用图神经网络 (GNNs) 和 Graph Transformer (GT) 在基于电子健康记录 (EHR) 的患者相似性图上预测下一次医院就诊时的心力衰竭 (HF) 发生。研究者利用 MIMIC-III 数据集，通过 K-Nearest Neighbors (KNN) 算法基于诊断、程序和药物嵌入构建患者相似图，并比较了 GraphSAGE、Graph Attention Network (GAT) 和 GT 模型的性能。结果显示，GT 模型表现出最佳效果 (F1 score: 0.5361, AUROC: 0.7925, AUPRC: 0.5168)，并在可解释性上优于基线算法如 Random Forest，因为它能捕捉 EHR 数据中的复杂患者关系。总之，该方法提升了 HF 预测的准确性和临床解释力，为图-based 医疗预测提供了有效框架。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19742v1",
      "published_date": "2024-11-29 14:40:19 UTC",
      "updated_date": "2024-11-29 14:40:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:40:57.230074"
    },
    {
      "arxiv_id": "2411.19732v1",
      "title": "Improving generalization of robot locomotion policies via Sharpness-Aware Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Severin Bochem",
        "Eduardo Gonzalez-Sanchez",
        "Yves Bicker",
        "Gabriele Fadini"
      ],
      "abstract": "Reinforcement learning often requires extensive training data.\nSimulation-to-real transfer offers a promising approach to address this\nchallenge in robotics. While differentiable simulators offer improved sample\nefficiency through exact gradients, they can be unstable in contact-rich\nenvironments and may lead to poor generalization. This paper introduces a novel\napproach integrating sharpness-aware optimization into gradient-based\nreinforcement learning algorithms. Our simulation results demonstrate that our\nmethod, tested on contact-rich environments, significantly enhances policy\nrobustness to environmental variations and action perturbations while\nmaintaining the sample efficiency of first-order methods. Specifically, our\napproach improves action noise tolerance compared to standard first-order\nmethods and achieves generalization comparable to zeroth-order methods. This\nimprovement stems from finding flatter minima in the loss landscape, associated\nwith better generalization. Our work offers a promising solution to balance\nefficient learning and robust sim-to-real transfer in robotics, potentially\nbridging the gap between simulation and real-world performance.",
      "tldr_zh": "这篇论文提出了一种新方法，通过将 Sharpness-Aware Optimization 整合到基于梯度的强化学习算法中，来改善机器人运动策略的泛化性能。该方法针对接触丰富的环境，显著提升了策略对环境变化和动作扰动的鲁棒性，同时保持了第一阶方法的样本效率。实验结果显示，与标准方法相比，它提高了对动作噪声的耐受性，并实现了与零阶方法相当的泛化效果，这种改进源于在损失景观中找到更平坦的最小值。该工作为强化学习在机器人领域的 sim-to-real 转移提供了平衡高效学习和真实世界性能的解决方案。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "9 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.19732v1",
      "published_date": "2024-11-29 14:25:54 UTC",
      "updated_date": "2024-11-29 14:25:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:41:08.650432"
    },
    {
      "arxiv_id": "2411.19722v2",
      "title": "JetFormer: An Autoregressive Generative Model of Raw Images and Text",
      "title_zh": "JetFormer：原始图像和",
      "authors": [
        "Michael Tschannen",
        "André Susano Pinto",
        "Alexander Kolesnikov"
      ],
      "abstract": "Removing modeling constraints and unifying architectures across domains has\nbeen a key driver of the recent progress in training large multimodal models.\nHowever, most of these models still rely on many separately trained components\nsuch as modality-specific encoders and decoders. In this work, we further\nstreamline joint generative modeling of images and text. We propose an\nautoregressive decoder-only transformer - JetFormer - which is trained to\ndirectly maximize the likelihood of raw data, without relying on any separately\npretrained components, and can understand and generate both text and images.\nSpecifically, we leverage a normalizing flow model to obtain a soft-token image\nrepresentation that is jointly trained with an autoregressive multimodal\ntransformer. The normalizing flow model serves as both an image encoder for\nperception tasks and an image decoder for image generation tasks during\ninference. JetFormer achieves text-to-image generation quality competitive with\nrecent VQ-VAE- and VAE-based baselines. These baselines rely on pretrained\nimage autoencoders, which are trained with a complex mixture of losses,\nincluding perceptual ones. At the same time, JetFormer demonstrates robust\nimage understanding capabilities. To the best of our knowledge, JetFormer is\nthe first model that is capable of generating high-fidelity images and\nproducing strong log-likelihood bounds.",
      "tldr_zh": "该研究提出了一种自回归生成模型JetFormer，这是一个解码器-only transformer，旨在统一图像和文本的联合生成，而不依赖预训练组件，直接最大化原始数据的似然。JetFormer利用normalizing flow模型来获取软-token图像表示，并与自回归多模态transformer联合训练，使其在推理时兼具图像编码器和解码器功能。在文本到图像生成任务上，JetFormer的性能与VQ-VAE和VAE基线相当，同时展示了强大的图像理解能力，是首个能够生成高保真图像并提供强log-likelihood界的模型。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "ICLR 2025. Code available at\n  https://github.com/google-research/big_vision",
      "pdf_url": "http://arxiv.org/pdf/2411.19722v2",
      "published_date": "2024-11-29 14:14:59 UTC",
      "updated_date": "2025-05-19 15:26:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:41:19.724576"
    },
    {
      "arxiv_id": "2411.19717v1",
      "title": "MonoPP: Metric-Scaled Self-Supervised Monocular Depth Estimation by Planar-Parallax Geometry in Automotive Applications",
      "title_zh": "翻译失败",
      "authors": [
        "Gasser Elazab",
        "Torben Gräber",
        "Michael Unterreiner",
        "Olaf Hellwich"
      ],
      "abstract": "Self-supervised monocular depth estimation (MDE) has gained popularity for\nobtaining depth predictions directly from videos. However, these methods often\nproduce scale invariant results, unless additional training signals are\nprovided. Addressing this challenge, we introduce a novel self-supervised\nmetric-scaled MDE model that requires only monocular video data and the\ncamera's mounting position, both of which are readily available in modern\nvehicles. Our approach leverages planar-parallax geometry to reconstruct scene\nstructure. The full pipeline consists of three main networks, a multi-frame\nnetwork, a singleframe network, and a pose network. The multi-frame network\nprocesses sequential frames to estimate the structure of the static scene using\nplanar-parallax geometry and the camera mounting position. Based on this\nreconstruction, it acts as a teacher, distilling knowledge such as scale\ninformation, masked drivable area, metric-scale depth for the static scene, and\ndynamic object mask to the singleframe network. It also aids the pose network\nin predicting a metric-scaled relative pose between two subsequent images. Our\nmethod achieved state-of-the-art results for the driving benchmark KITTI for\nmetric-scaled depth prediction. Notably, it is one of the first methods to\nproduce self-supervised metric-scaled depth prediction for the challenging\nCityscapes dataset, demonstrating its effectiveness and versatility.",
      "tldr_zh": "本研究提出MonoPP，一种基于Planar-Parallax Geometry的自监督单目深度估计(Self-Supervised Monocular Depth Estimation, MDE)模型，仅需单目视频数据和相机安装位置，即可实现度量尺度的深度预测。模型包括三个主要网络：多帧网络用于处理连续帧重建静态场景结构，并向单帧网络传递尺度信息、可驾驶区域掩码、度量尺度深度和动态对象掩码；单帧网络则学习这些知识以提升预测精度；姿态网络则预测图像间的度量尺度相对姿态。该方法在KITTI驾驶基准上取得了最先进的结果，并在Cityscapes数据集上首次实现了自监督度量尺度深度预测，展示了其在汽车应用中的有效性和通用性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at WACV 25, project page: https://mono-pp.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2411.19717v1",
      "published_date": "2024-11-29 14:06:58 UTC",
      "updated_date": "2024-11-29 14:06:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:41:32.287127"
    },
    {
      "arxiv_id": "2411.19713v3",
      "title": "CantorNet: A Sandbox for Testing Geometrical and Topological Complexity Measures",
      "title_zh": "CantorNet：用于测试",
      "authors": [
        "Michal Lewandowski",
        "Hamid Eghbalzadeh",
        "Bernhard A. Moser"
      ],
      "abstract": "Many natural phenomena are characterized by self-similarity, for example the\nsymmetry of human faces, or a repetitive motif of a song. Studying of such\nsymmetries will allow us to gain deeper insights into the underlying mechanisms\nof complex systems. Recognizing the importance of understanding these patterns,\nwe propose a geometrically inspired framework to study such phenomena in\nartificial neural networks. To this end, we introduce \\emph{CantorNet},\ninspired by the triadic construction of the Cantor set, which was introduced by\nGeorg Cantor in the $19^\\text{th}$ century. In mathematics, the Cantor set is a\nset of points lying on a single line that is self-similar and has a counter\nintuitive property of being an uncountably infinite null set. Similarly, we\nintroduce CantorNet as a sandbox for studying self-similarity by means of novel\ntopological and geometrical complexity measures. CantorNet constitutes a family\nof ReLU neural networks that spans the whole spectrum of possible Kolmogorov\ncomplexities, including the two opposite descriptions (linear and exponential\nas measured by the description length). CantorNet's decision boundaries can be\narbitrarily ragged, yet are analytically known. Besides serving as a testing\nground for complexity measures, our work may serve to illustrate potential\npitfalls in geometry-ignorant data augmentation techniques and adversarial\nattacks.",
      "tldr_zh": "本研究探讨了自然现象的自相似性（如人脸对称或歌曲重复模式），并提出CantorNet框架，作为一个受Cantor set启发的测试平台，用于在人工神经网络中研究几何和拓扑复杂性措施。CantorNet是一个ReLU neural networks家族，涵盖从线性到指数级的Kolmogorov complexities，其决策边界可任意不规则但分析性可知。实验表明，该框架不仅可作为测试复杂性措施的sandbox，还能揭示几何忽略的数据增强技术和adversarial attacks的潜在陷阱，从而加深对复杂系统的理解。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.NE",
      "comment": "Accepted at the NeurIPS Workshop on Symmetry and Geometry in Neural\n  Representations, 2024",
      "pdf_url": "http://arxiv.org/pdf/2411.19713v3",
      "published_date": "2024-11-29 14:01:34 UTC",
      "updated_date": "2025-01-28 11:19:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:41:44.335825"
    },
    {
      "arxiv_id": "2411.19668v1",
      "title": "ChineseWebText 2.0: Large-Scale High-quality Chinese Web Text with Multi-dimensional and fine-grained information",
      "title_zh": "ChineseWebText 2.0：大规模高品质中文网络文本，带有多维度和细粒度信息",
      "authors": [
        "Wanyue Zhang",
        "Ziyong Li",
        "Wen Yang",
        "Chunlin Leng",
        "Yinan Bai",
        "Qianlong Du",
        "Chengqing Zong",
        "Jiajun Zhang"
      ],
      "abstract": "During the development of large language models (LLMs), pre-training data\nplay a critical role in shaping LLMs' capabilities. In recent years several\nlarge-scale and high-quality pre-training datasets have been released to\naccelerate the research of LLMs, including ChineseWebText1.0, C4, Pile,\nWanJuan, MAPCC and others. However, as LLMs continue to evolve, focus has\nincreasingly shifted to domain-specific capabilities and safety concerns,\nmaking those previous coarse-grained texts insufficient for meeting training\nrequirements. Furthermore, fine-grained information, such as quality, domain\nand toxicity, is becoming increasingly important in building powerful and\nreliable LLMs for various scenarios. To address these challenges, in this paper\nwe propose a new tool-chain called MDFG-tool for constructing large-scale and\nhigh-quality Chinese datasets with multi-dimensional and fine-grained\ninformation. First, we employ manually crafted rules to discard explicit noisy\ntexts from raw contents. Second, the quality evaluation model, domain\nclassifier, and toxicity evaluation model are well-designed to assess the\nremaining cleaned data respectively. Finally, we integrate these three types of\nfine-grained information for each text. With this approach, we release the\nlargest, high-quality and fine-grained Chinese text ChineseWebText2.0, which\nconsists of 3.8TB and each text is associated with a quality score, domain\nlabels, a toxicity label and a toxicity score, facilitating the LLM researchers\nto select data based on various types of fine-grained information. The data,\ncodes and the tool-chain are available on this website\nhttps://github.com/CASIA-LM/ChineseWebText-2.0",
      "tldr_zh": "本论文介绍了 ChineseWebText 2.0，这是一个大规模、高质量的中文网络文本数据集，旨在解决现有预训练数据在大型语言模型 (LLMs) 训练中的粗粒度问题，提供多维度细粒度信息以提升模型的领域特定能力和安全性。研究团队开发了 MDFG-tool 工具链，包括手动规则过滤噪声文本、质量评估模型、领域分类器和毒性评估模型，来评估和整合每个文本的质量分数、领域标签、毒性标签及分数。最终，ChineseWebText 2.0 包含 3.8TB 的高质量数据，每个文本都附带细粒度信息，便于 LLMs 研究者根据需求选择数据；数据集、代码和工具链已在 GitHub 上公开。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ChineseWebTex2.0 dataset is available at\n  https://github.com/CASIA-LM/ChineseWebText-2.0",
      "pdf_url": "http://arxiv.org/pdf/2411.19668v1",
      "published_date": "2024-11-29 12:48:49 UTC",
      "updated_date": "2024-11-29 12:48:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:41:56.267780"
    },
    {
      "arxiv_id": "2411.19666v1",
      "title": "Multimodal Whole Slide Foundation Model for Pathology",
      "title_zh": "翻译失败",
      "authors": [
        "Tong Ding",
        "Sophia J. Wagner",
        "Andrew H. Song",
        "Richard J. Chen",
        "Ming Y. Lu",
        "Andrew Zhang",
        "Anurag J. Vaidya",
        "Guillaume Jaume",
        "Muhammad Shaban",
        "Ahrong Kim",
        "Drew F. K. Williamson",
        "Bowen Chen",
        "Cristina Almagro-Perez",
        "Paul Doucet",
        "Sharifa Sahai",
        "Chengkuan Chen",
        "Daisuke Komura",
        "Akihiro Kawabe",
        "Shumpei Ishikawa",
        "Georg Gerber",
        "Tingying Peng",
        "Long Phi Le",
        "Faisal Mahmood"
      ],
      "abstract": "The field of computational pathology has been transformed with recent\nadvances in foundation models that encode histopathology region-of-interests\n(ROIs) into versatile and transferable feature representations via\nself-supervised learning (SSL). However, translating these advancements to\naddress complex clinical challenges at the patient and slide level remains\nconstrained by limited clinical data in disease-specific cohorts, especially\nfor rare clinical conditions. We propose TITAN, a multimodal whole slide\nfoundation model pretrained using 335,645 WSIs via visual self-supervised\nlearning and vision-language alignment with corresponding pathology reports and\n423,122 synthetic captions generated from a multimodal generative AI copilot\nfor pathology. Without any finetuning or requiring clinical labels, TITAN can\nextract general-purpose slide representations and generate pathology reports\nthat generalize to resource-limited clinical scenarios such as rare disease\nretrieval and cancer prognosis. We evaluate TITAN on diverse clinical tasks and\nfind that TITAN outperforms both ROI and slide foundation models across machine\nlearning settings such as linear probing, few-shot and zero-shot\nclassification, rare cancer retrieval and cross-modal retrieval, and pathology\nreport generation.",
      "tldr_zh": "本研究提出 TITAN，一种多模态全切片基础模型（Multimodal Whole Slide Foundation Model），用于计算病理学领域，通过视觉自监督学习（SSL）和视觉-语言对齐，对 335,645 张全切片图像（WSIs）及 423,122 个合成标题进行预训练，以解决现有模型在处理患者级和切片级临床挑战的局限性，如稀有疾病数据不足。TITAN 无需微调或临床标签，即可提取通用切片表示并生成病理报告，适用于资源有限的场景，包括罕见疾病检索和癌症预后。实验结果显示，TITAN 在线性探测、少样本/零样本分类、罕见癌症检索、跨模态检索和病理报告生成等任务上，优于现有的 ROI 和切片基础模型，展示了其在临床应用中的显著优势。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "stat.AP"
      ],
      "primary_category": "eess.IV",
      "comment": "The code is accessible at https://github.com/mahmoodlab/TITAN",
      "pdf_url": "http://arxiv.org/pdf/2411.19666v1",
      "published_date": "2024-11-29 12:39:57 UTC",
      "updated_date": "2024-11-29 12:39:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:42:10.316976"
    },
    {
      "arxiv_id": "2411.19652v1",
      "title": "Uniform Attention Maps: Boosting Image Fidelity in Reconstruction and Editing",
      "title_zh": "翻译失败",
      "authors": [
        "Wenyi Mo",
        "Tianyu Zhang",
        "Yalong Bai",
        "Bing Su",
        "Ji-Rong Wen"
      ],
      "abstract": "Text-guided image generation and editing using diffusion models have achieved\nremarkable advancements. Among these, tuning-free methods have gained attention\nfor their ability to perform edits without extensive model adjustments,\noffering simplicity and efficiency. However, existing tuning-free approaches\noften struggle with balancing fidelity and editing precision. Reconstruction\nerrors in DDIM Inversion are partly attributed to the cross-attention mechanism\nin U-Net, which introduces misalignments during the inversion and\nreconstruction process. To address this, we analyze reconstruction from a\nstructural perspective and propose a novel approach that replaces traditional\ncross-attention with uniform attention maps, significantly enhancing image\nreconstruction fidelity. Our method effectively minimizes distortions caused by\nvarying text conditions during noise prediction. To complement this\nimprovement, we introduce an adaptive mask-guided editing technique that\nintegrates seamlessly with our reconstruction approach, ensuring consistency\nand accuracy in editing tasks. Experimental results demonstrate that our\napproach not only excels in achieving high-fidelity image reconstruction but\nalso performs robustly in real image composition and editing scenarios. This\nstudy underscores the potential of uniform attention maps to enhance the\nfidelity and versatility of diffusion-based image processing methods. Code is\navailable at https://github.com/Mowenyii/Uniform-Attention-Maps.",
      "tldr_zh": "该论文针对文本引导的图像生成和编辑中的 tuning-free 方法，分析了 DDIM Inversion 重建错误主要源于 U-Net 的 cross-attention 机制导致的失真问题。作者提出使用 uniform attention maps 替换传统 cross-attention，以显著提高图像重建的保真度，并引入 adaptive mask-guided editing 技术，确保编辑任务的一致性和准确性。实验结果显示，该方法在高保真图像重建、真实图像合成和编辑场景中表现出色，增强了 diffusion-based 图像处理方法的可靠性和多功能性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to WACV 2025",
      "pdf_url": "http://arxiv.org/pdf/2411.19652v1",
      "published_date": "2024-11-29 12:11:28 UTC",
      "updated_date": "2024-11-29 12:11:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:42:20.492758"
    },
    {
      "arxiv_id": "2411.19650v1",
      "title": "CogACT: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation",
      "title_zh": "翻译失败",
      "authors": [
        "Qixiu Li",
        "Yaobo Liang",
        "Zeyu Wang",
        "Lin Luo",
        "Xi Chen",
        "Mozheng Liao",
        "Fangyun Wei",
        "Yu Deng",
        "Sicheng Xu",
        "Yizhong Zhang",
        "Xiaofan Wang",
        "Bei Liu",
        "Jianlong Fu",
        "Jianmin Bao",
        "Dong Chen",
        "Yuanchun Shi",
        "Jiaolong Yang",
        "Baining Guo"
      ],
      "abstract": "The advancement of large Vision-Language-Action (VLA) models has\nsignificantly improved robotic manipulation in terms of language-guided task\nexecution and generalization to unseen scenarios. While existing VLAs adapted\nfrom pretrained large Vision-Language-Models (VLM) have demonstrated promising\ngeneralizability, their task performance is still unsatisfactory as indicated\nby the low tasks success rates in different environments. In this paper, we\npresent a new advanced VLA architecture derived from VLM. Unlike previous works\nthat directly repurpose VLM for action prediction by simple action\nquantization, we propose a omponentized VLA architecture that has a specialized\naction module conditioned on VLM output. We systematically study the design of\nthe action module and demonstrates the strong performance enhancement with\ndiffusion action transformers for action sequence modeling, as well as their\nfavorable scaling behaviors. We also conduct comprehensive experiments and\nablation studies to evaluate the efficacy of our models with varied designs.\nThe evaluation on 5 robot embodiments in simulation and real work shows that\nour model not only significantly surpasses existing VLAs in task performance\nand but also exhibits remarkable adaptation to new robots and generalization to\nunseen objects and backgrounds. It exceeds the average success rates of OpenVLA\nwhich has similar model size (7B) with ours by over 35% in simulated evaluation\nand 55% in real robot experiments. It also outperforms the large RT-2-X model\n(55B) by 18% absolute success rates in simulation. Code and models can be found\non our project page (https://cogact.github.io/).",
      "tldr_zh": "本研究提出 CogACT，一种基础性的 Vision-Language-Action (VLA) 模型，旨在将认知和动作协同应用于机器人操作任务，以提升语言引导的任务执行和泛化能力。不同于现有方法，CogACT 采用组件化的架构，包括一个基于 VLM 输出进行条件化的专用 action module，并利用 diffusion action transformers 来建模动作序列，从而显著改善任务性能。实验结果显示，该模型在 5 个机器人实体上的模拟和真实环境中，成功率比同规模的 OpenVLA 高出 35%（模拟）和 55%（真实），并超过大型 RT-2-X (55B) 模型 18%，展现出卓越的适应性和泛化能力。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Project Webpage: https://cogact.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2411.19650v1",
      "published_date": "2024-11-29 12:06:03 UTC",
      "updated_date": "2024-11-29 12:06:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:42:31.725962"
    },
    {
      "arxiv_id": "2411.19647v1",
      "title": "CAdam: Confidence-Based Optimization for Online Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Shaowen Wang",
        "Anan Liu",
        "Jian Xiao",
        "Huan Liu",
        "Yuekui Yang",
        "Cong Xu",
        "Qianqian Pu",
        "Suncong Zheng",
        "Wei Zhang",
        "Jian Li"
      ],
      "abstract": "Modern recommendation systems frequently employ online learning to\ndynamically update their models with freshly collected data. The most commonly\nused optimizer for updating neural networks in these contexts is the Adam\noptimizer, which integrates momentum ($m_t$) and adaptive learning rate\n($v_t$). However, the volatile nature of online learning data, characterized by\nits frequent distribution shifts and presence of noises, poses significant\nchallenges to Adam's standard optimization process: (1) Adam may use outdated\nmomentum and the average of squared gradients, resulting in slower adaptation\nto distribution changes, and (2) Adam's performance is adversely affected by\ndata noise. To mitigate these issues, we introduce CAdam, a confidence-based\noptimization strategy that assesses the consistence between the momentum and\nthe gradient for each parameter dimension before deciding on updates. If\nmomentum and gradient are in sync, CAdam proceeds with parameter updates\naccording to Adam's original formulation; if not, it temporarily withholds\nupdates and monitors potential shifts in data distribution in subsequent\niterations. This method allows CAdam to distinguish between the true\ndistributional shifts and mere noise, and adapt more quickly to new data\ndistributions. Our experiments with both synthetic and real-world datasets\ndemonstrate that CAdam surpasses other well-known optimizers, including the\noriginal Adam, in efficiency and noise robustness. Furthermore, in large-scale\nA/B testing within a live recommendation system, CAdam significantly enhances\nmodel performance compared to Adam, leading to substantial increases in the\nsystem's gross merchandise volume (GMV).",
      "tldr_zh": "本论文提出CAdam，一种基于置信度的优化策略，用于在线学习环境中解决Adam优化器的适应性问题，该优化器常用于推荐系统的模型更新。CAdam通过评估每个参数维度的动量(momentum)和梯度的一致性，若一致则按Adam原公式更新参数，否则暂停更新并监控后续数据分布变化，从而有效区分真正的分布偏移和噪声。实验结果显示，CAdam在合成和真实数据集上比Adam及其他优化器更高效、更鲁棒，并在实际A/B测试中显著提升推荐系统的性能，导致gross merchandise volume (GMV)大幅增加。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19647v1",
      "published_date": "2024-11-29 12:00:27 UTC",
      "updated_date": "2024-11-29 12:00:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:42:44.469675"
    },
    {
      "arxiv_id": "2411.19626v2",
      "title": "GREAT: Geometry-Intention Collaborative Inference for Open-Vocabulary 3D Object Affordance Grounding",
      "title_zh": "翻译失败",
      "authors": [
        "Yawen Shao",
        "Wei Zhai",
        "Yuhang Yang",
        "Hongchen Luo",
        "Yang Cao",
        "Zheng-Jun Zha"
      ],
      "abstract": "Open-Vocabulary 3D object affordance grounding aims to anticipate ``action\npossibilities'' regions on 3D objects with arbitrary instructions, which is\ncrucial for robots to generically perceive real scenarios and respond to\noperational changes. Existing methods focus on combining images or languages\nthat depict interactions with 3D geometries to introduce external interaction\npriors. However, they are still vulnerable to a limited semantic space by\nfailing to leverage implied invariant geometries and potential interaction\nintentions. Normally, humans address complex tasks through multi-step reasoning\nand respond to diverse situations by leveraging associative and analogical\nthinking. In light of this, we propose GREAT (GeometRy-intEntion collAboraTive\ninference) for Open-Vocabulary 3D Object Affordance Grounding, a novel\nframework that mines the object invariant geometry attributes and performs\nanalogically reason in potential interaction scenarios to form affordance\nknowledge, fully combining the knowledge with both geometries and visual\ncontents to ground 3D object affordance. Besides, we introduce the Point Image\nAffordance Dataset v2 (PIADv2), the largest 3D object affordance dataset at\npresent to support the task. Extensive experiments demonstrate the\neffectiveness and superiority of GREAT. The code and dataset are available at\nhttps://yawen-shao.github.io/GREAT/.",
      "tldr_zh": "该论文提出 GREAT 框架，用于 Open-Vocabulary 3D Object Affordance Grounding 任务，该框架通过挖掘对象的不变几何属性并进行类比推理，来形成 affordance 知识，并将这些知识与几何和视觉内容相结合，实现对 3D 对象行动可能性区域的精确定位。GREAT 解决了现有方法受限于语义空间的问题，模拟人类的多步推理和联想思维，以应对多样化的交互场景。同时，论文引入了 PIADv2 数据集，这是目前最大的 3D 对象 affordance 数据集，支持任务的开发和评估。实验结果证明，GREAT 在相关任务上表现出色，展示了其有效性和优越性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2025. Project page: https://yawen-shao.github.io/GREAT/ Code:\n  https://github.com/yawen-shao/GREAT_code",
      "pdf_url": "http://arxiv.org/pdf/2411.19626v2",
      "published_date": "2024-11-29 11:23:15 UTC",
      "updated_date": "2025-03-29 03:46:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:42:56.158180"
    },
    {
      "arxiv_id": "2411.19623v1",
      "title": "FairDD: Fair Dataset Distillation via Synchronized Matching",
      "title_zh": "FairDD：通过同步匹配的公平数据集蒸馏",
      "authors": [
        "Qihang Zhou",
        "Shenhao Fang",
        "Shibo He",
        "Wenchao Meng",
        "Jiming Chen"
      ],
      "abstract": "Condensing large datasets into smaller synthetic counterparts has\ndemonstrated its promise for image classification. However, previous research\nhas overlooked a crucial concern in image recognition: ensuring that models\ntrained on condensed datasets are unbiased towards protected attributes (PA),\nsuch as gender and race. Our investigation reveals that dataset distillation\n(DD) fails to alleviate the unfairness towards minority groups within original\ndatasets. Moreover, this bias typically worsens in the condensed datasets due\nto their smaller size. To bridge the research gap, we propose a novel fair\ndataset distillation (FDD) framework, namely FairDD, which can be seamlessly\napplied to diverse matching-based DD approaches, requiring no modifications to\ntheir original architectures. The key innovation of FairDD lies in\nsynchronously matching synthetic datasets to PA-wise groups of original\ndatasets, rather than indiscriminate alignment to the whole distributions in\nvanilla DDs, dominated by majority groups. This synchronized matching allows\nsynthetic datasets to avoid collapsing into majority groups and bootstrap their\nbalanced generation to all PA groups. Consequently, FairDD could effectively\nregularize vanilla DDs to favor biased generation toward minority groups while\nmaintaining the accuracy of target attributes. Theoretical analyses and\nextensive experimental evaluations demonstrate that FairDD significantly\nimproves fairness compared to vanilla DD methods, without sacrificing\nclassification accuracy. Its consistent superiority across diverse DDs,\nspanning Distribution and Gradient Matching, establishes it as a versatile FDD\napproach.",
      "tldr_zh": "该研究发现，传统数据集蒸馏（Dataset Distillation, DD）方法在图像分类中未能缓解模型对保护属性（Protected Attributes, PA）如性别和种族的偏见，反而由于数据集缩小而加剧了这种不公平问题。论文提出了一种新框架 FairDD，通过同步匹配（Synchronized Matching）将合成数据集与原数据集的 PA-wise 组对齐，而不是整体分布，从而避免偏向多数群体并促进所有组的平衡生成。实验结果表明，FairDD 显著提升了公平性，同时保持了分类准确性，并在多种 DD 方法（如 Distribution and Gradient Matching）上表现出通用优势。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19623v1",
      "published_date": "2024-11-29 11:22:20 UTC",
      "updated_date": "2024-11-29 11:22:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:43:08.065991"
    },
    {
      "arxiv_id": "2411.19583v1",
      "title": "Solving Rubik's Cube Without Tricky Sampling",
      "title_zh": "翻译失败",
      "authors": [
        "Yicheng Lin",
        "Siyu Liang"
      ],
      "abstract": "The Rubiks Cube, with its vast state space and sparse reward structure,\npresents a significant challenge for reinforcement learning (RL) due to the\ndifficulty of reaching rewarded states. Previous research addressed this by\npropagating cost-to-go estimates from the solved state and incorporating search\ntechniques. These approaches differ from human strategies that start from fully\nscrambled cubes, which can be tricky for solving a general sparse-reward\nproblem. In this paper, we introduce a novel RL algorithm using policy gradient\nmethods to solve the Rubiks Cube without relying on near solved-state sampling.\nOur approach employs a neural network to predict cost patterns between states,\nallowing the agent to learn directly from scrambled states. Our method was\ntested on the 2x2x2 Rubiks Cube, where the cube was scrambled 50,000 times, and\nthe model successfully solved it in over 99.4% of cases. Notably, this result\nwas achieved using only the policy network without relying on tree search as in\nprevious methods, demonstrating its effectiveness and potential for broader\napplications in sparse-reward problems.",
      "tldr_zh": "该研究提出了一种新型强化学习（RL）算法，用于解决Rubik's Cube问题，而不依赖于接近已解决状态的采样技巧。该算法采用策略梯度方法，并使用神经网络预测状态之间的成本模式，使代理能够直接从完全打乱的状态学习。在2x2x2 Rubik's Cube的测试中，该方法在50,000次打乱后成功解决超过99.4%的案例，仅依赖策略网络而非树搜索，展示了其在稀疏奖励问题中的广阔应用潜力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19583v1",
      "published_date": "2024-11-29 09:56:40 UTC",
      "updated_date": "2024-11-29 09:56:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:43:19.421128"
    },
    {
      "arxiv_id": "2411.19557v2",
      "title": "Initialization using Update Approximation is a Silver Bullet for Extremely Efficient Low-Rank Fine-Tuning",
      "title_zh": "翻译失败",
      "authors": [
        "Kaustubh Ponkshe",
        "Raghav Singhal",
        "Eduard Gorbunov",
        "Alexey Tumanov",
        "Samuel Horvath",
        "Praneeth Vepakomma"
      ],
      "abstract": "Low-rank adapters have become standard for efficiently fine-tuning large\nlanguage models (LLMs), but they often fall short of achieving the performance\nof full fine-tuning. We propose a method, LoRA Silver Bullet or LoRA-SB, that\napproximates full fine-tuning within low-rank subspaces using a carefully\ndesigned initialization strategy. We theoretically demonstrate that the\narchitecture of LoRA-XS, which inserts a learnable (r x r) matrix between B and\nA while keeping other matrices fixed, provides the precise conditions needed\nfor this approximation. We leverage its constrained update space to achieve\noptimal scaling for high-rank gradient updates while removing the need for\nhyperparameter tuning. We prove that our initialization offers an optimal\nlow-rank approximation of the initial gradient and preserves update directions\nthroughout training. Extensive experiments across mathematical reasoning,\ncommonsense reasoning, and language understanding tasks demonstrate that our\napproach exceeds the performance of standard LoRA while using \\textbf{27-90}\ntimes fewer learnable parameters, and comprehensively outperforms LoRA-XS. Our\nfindings establish that it is possible to simulate full fine-tuning in low-rank\nsubspaces, and achieve significant efficiency gains without sacrificing\nperformance. Our code is publicly available at\nhttps://github.com/RaghavSinghal10/lora-sb.",
      "tldr_zh": "本文提出 LoRA-SB 方法，通过更新逼近的初始化策略，在低秩子空间逼近大型语言模型 (LLMs) 的全微调性能，从而实现极高的微调效率。LoRA-SB 基于 LoRA-XS 架构，插入一个可学习的 (r x r) 矩阵来优化高秩梯度更新，提供初始梯度的最佳低秩逼近，并保持更新方向，无需超参数调优。实验结果显示，该方法在数学推理、常识推理和语言理解任务上超过了标准 LoRA，使用 27-90 倍更少的参数，并全面优于 LoRA-XS。这些发现证明了在低秩子空间模拟全微调的可行性，实现显著效率提升而不牺牲性能。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Kaustubh Ponkshe and Raghav Singhal contributed equally to this work",
      "pdf_url": "http://arxiv.org/pdf/2411.19557v2",
      "published_date": "2024-11-29 09:10:30 UTC",
      "updated_date": "2025-02-07 19:50:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:43:32.537692"
    },
    {
      "arxiv_id": "2411.19554v1",
      "title": "Unimib Assistant: designing a student-friendly RAG-based chatbot for all their needs",
      "title_zh": "翻译失败",
      "authors": [
        "Chiara Antico",
        "Stefano Giordano",
        "Cansu Koyuturk",
        "Dimitri Ognibene"
      ],
      "abstract": "Natural language processing skills of Large Language Models (LLMs) are\nunprecedented, having wide diffusion and application in different tasks. This\npilot study focuses on specializing ChatGPT behavior through a\nRetrieval-Augmented Generation (RAG) system using the OpenAI custom GPTs\nfeature. The purpose of our chatbot, called Unimib Assistant, is to provide\ninformation and solutions to the specific needs of University of Milano-Bicocca\n(Unimib) students through a question-answering approach. We provided the system\nwith a prompt highlighting its specific purpose and behavior, as well as\nuniversity-related documents and links obtained from an initial need-finding\nphase, interviewing six students. After a preliminary customization phase, a\nqualitative usability test was conducted with six other students to identify\nthe strengths and weaknesses of the chatbot, with the goal of improving it in a\nsubsequent redesign phase. While the chatbot was appreciated for its\nuser-friendly experience, perceived general reliability, well-structured\nresponses, and conversational tone, several significant technical and\nfunctional limitations emerged. In particular, the satisfaction and overall\nexperience of the users was impaired by the system's inability to always\nprovide fully accurate information. Moreover, it would often neglect to report\nrelevant information even if present in the materials uploaded and prompt\ngiven. Furthermore, it sometimes generated unclickable links, undermining its\ntrustworthiness, since providing the source of information was an important\naspect for our users. Further in-depth studies and feedback from other users as\nwell as implementation iterations are planned to refine our Unimib Assistant.",
      "tldr_zh": "本研究设计了Unimib Assistant，一种基于Retrieval-Augmented Generation (RAG)系统的聊天机器人，旨在为米兰比可卡大学(Unimib)学生提供个性化信息和解决方案。研究团队通过采访六名学生收集需求，并使用OpenAI custom GPTs功能定制提示和大学相关文档，随后对另一组六名学生进行定性可用性测试。测试结果显示，该聊天机器人因其用户友好界面、可靠响应和对话语气而受好评，但存在信息准确性不足、忽略相关内容以及生成不可点击链接等问题；未来计划包括进一步迭代和用户反馈以提升其性能。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted for Italian Workshop on Artificial Intelligence for Human\n  Machine Interaction (AIxHMI 2024), November 26, 2024, Bolzano, Italy",
      "pdf_url": "http://arxiv.org/pdf/2411.19554v1",
      "published_date": "2024-11-29 09:07:21 UTC",
      "updated_date": "2024-11-29 09:07:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:43:44.151744"
    },
    {
      "arxiv_id": "2411.19548v1",
      "title": "ReconDreamer: Crafting World Models for Driving Scene Reconstruction via Online Restoration",
      "title_zh": "ReconDreamer：通过在线修复构建世界模型用于驾驶场景重建",
      "authors": [
        "Chaojun Ni",
        "Guosheng Zhao",
        "Xiaofeng Wang",
        "Zheng Zhu",
        "Wenkang Qin",
        "Guan Huang",
        "Chen Liu",
        "Yuyin Chen",
        "Yida Wang",
        "Xueyang Zhang",
        "Yifei Zhan",
        "Kun Zhan",
        "Peng Jia",
        "Xianpeng Lang",
        "Xingang Wang",
        "Wenjun Mei"
      ],
      "abstract": "Closed-loop simulation is crucial for end-to-end autonomous driving. Existing\nsensor simulation methods (e.g., NeRF and 3DGS) reconstruct driving scenes\nbased on conditions that closely mirror training data distributions. However,\nthese methods struggle with rendering novel trajectories, such as lane changes.\nRecent works have demonstrated that integrating world model knowledge\nalleviates these issues. Despite their efficiency, these approaches still\nencounter difficulties in the accurate representation of more complex\nmaneuvers, with multi-lane shifts being a notable example. Therefore, we\nintroduce ReconDreamer, which enhances driving scene reconstruction through\nincremental integration of world model knowledge. Specifically, DriveRestorer\nis proposed to mitigate artifacts via online restoration. This is complemented\nby a progressive data update strategy designed to ensure high-quality rendering\nfor more complex maneuvers. To the best of our knowledge, ReconDreamer is the\nfirst method to effectively render in large maneuvers. Experimental results\ndemonstrate that ReconDreamer outperforms Street Gaussians in the NTA-IoU,\nNTL-IoU, and FID, with relative improvements by 24.87%, 6.72%, and 29.97%.\nFurthermore, ReconDreamer surpasses DriveDreamer4D with PVG during large\nmaneuver rendering, as verified by a relative improvement of 195.87% in the\nNTA-IoU metric and a comprehensive user study.",
      "tldr_zh": "该论文提出 ReconDreamer，一种通过在线修复技术（DriveRestorer）与渐进式数据更新策略相结合的世界模型框架，用于提升自动驾驶场景重建的准确性。该方法解决了现有方法如 NeRF 和 3DGS 在渲染新轨迹（如变道或多车道转移）时的局限性，通过整合世界模型知识实现高效的复杂 maneuver 渲染。实验结果显示，ReconDreamer 在 NTA-IoU、NTL-IoU 和 FID 指标上分别较 Street Gaussians 提升 24.87%、6.72% 和 29.97%，并在大 maneuver 渲染中较 DriveDreamer4D 改善 195.87% 的 NTA-IoU 表现。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Project Page: https://recondreamer.github.io",
      "pdf_url": "http://arxiv.org/pdf/2411.19548v1",
      "published_date": "2024-11-29 08:47:46 UTC",
      "updated_date": "2024-11-29 08:47:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:43:57.252673"
    },
    {
      "arxiv_id": "2411.19547v1",
      "title": "Training Agents with Weakly Supervised Feedback from Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Dihong Gong",
        "Pu Lu",
        "Zelong Wang",
        "Meng Zhou",
        "Xiuqiang He"
      ],
      "abstract": "Large Language Models (LLMs) offer a promising basis for creating agents that\ncan tackle complex tasks through iterative environmental interaction. Existing\nmethods either require these agents to mimic expert-provided trajectories or\nrely on definitive environmental feedback for reinforcement learning which\nlimits their application to specific scenarios like gaming or code generation.\nThis paper introduces a novel training method for LLM-based agents using weakly\nsupervised signals from a critic LLM, bypassing the need for expert\ntrajectories or definitive feedback. Our agents are trained in iterative\nmanner, where they initially generate trajectories through environmental\ninteraction. Subsequently, a critic LLM selects a subset of good trajectories,\nwhich are then used to update the agents, enabling them to generate improved\ntrajectories in the next iteration. Extensive tests on the API-bank dataset\nshow consistent improvement in our agents' capabilities and comparable\nperformance to GPT-4, despite using open-source models with much fewer\nparameters.",
      "tldr_zh": "这篇论文提出了一种新方法，使用来自批评者 Large Language Models (LLMs) 的弱监督反馈训练 LLM-based 代理，从而避免依赖专家轨迹或明确环境反馈的局限性。训练过程采用迭代方式：代理首先通过环境交互生成轨迹，然后批评者 LLM 筛选出优秀的子集，用于更新代理以产生更优的后续轨迹。在 API-bank 数据集上的广泛测试表明，该方法使代理能力持续提升，并使用参数更少的开源模型实现了与 GPT-4 相当的表现。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19547v1",
      "published_date": "2024-11-29 08:47:04 UTC",
      "updated_date": "2024-11-29 08:47:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:44:09.129140"
    },
    {
      "arxiv_id": "2411.19544v1",
      "title": "SkelMamba: A State Space Model for Efficient Skeleton Action Recognition of Neurological Disorders",
      "title_zh": "翻译失败",
      "authors": [
        "Niki Martinel",
        "Mariano Serrao",
        "Christian Micheloni"
      ],
      "abstract": "We introduce a novel state-space model (SSM)-based framework for\nskeleton-based human action recognition, with an anatomically-guided\narchitecture that improves state-of-the-art performance in both clinical\ndiagnostics and general action recognition tasks. Our approach decomposes\nskeletal motion analysis into spatial, temporal, and spatio-temporal streams,\nusing channel partitioning to capture distinct movement characteristics\nefficiently. By implementing a structured, multi-directional scanning strategy\nwithin SSMs, our model captures local joint interactions and global motion\npatterns across multiple anatomical body parts. This anatomically-aware\ndecomposition enhances the ability to identify subtle motion patterns critical\nin medical diagnosis, such as gait anomalies associated with neurological\nconditions. On public action recognition benchmarks, i.e., NTU RGB+D, NTU RGB+D\n120, and NW-UCLA, our model outperforms current state-of-the-art methods,\nachieving accuracy improvements up to $3.2\\%$ with lower computational\ncomplexity than previous leading transformer-based models. We also introduce a\nnovel medical dataset for motion-based patient neurological disorder analysis\nto validate our method's potential in automated disease diagnosis.",
      "tldr_zh": "我们介绍了SkelMamba，一种基于状态空间模型(SSM)的框架，用于高效的骨骼动作识别，特别针对神经系统疾病的临床诊断。该框架采用解剖学指导的架构，将骨骼运动分析分解为空间、时间和时空流，并通过多向扫描策略捕获局部关节互动和全局运动模式，从而更好地识别细微异常如步态问题。在公共基准如NTU RGB+D和NW-UCLA上，SkelMamba比现有方法提高了高达3.2%的准确率，同时降低了计算复杂度。我们还引入了一个新的医疗数据集，以验证该方法在自动化神经疾病诊断中的潜力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19544v1",
      "published_date": "2024-11-29 08:43:52 UTC",
      "updated_date": "2024-11-29 08:43:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:46:09.836804"
    },
    {
      "arxiv_id": "2411.19539v1",
      "title": "Knowledge Management for Automobile Failure Analysis Using Graph RAG",
      "title_zh": "Graph RAG 用于汽车故障分析的知识管理",
      "authors": [
        "Yuta Ojima",
        "Hiroki Sakaji",
        "Tadashi Nakamura",
        "Hiroaki Sakata",
        "Kazuya Seki",
        "Yuu Teshigawara",
        "Masami Yamashita",
        "Kazuhiro Aoyama"
      ],
      "abstract": "This paper presents a knowledge management system for automobile failure\nanalysis using retrieval-augmented generation (RAG) with large language models\n(LLMs) and knowledge graphs (KGs). In the automotive industry, there is a\ngrowing demand for knowledge transfer of failure analysis from experienced\nengineers to young engineers. However, failure events are phenomena that occur\nin a chain reaction, making them difficult for beginners to analyze them. While\nknowledge graphs, which can describe semantic relationships and structure\ninformation is effective in representing failure events, due to their\ncapability of representing the relationships between components, there is much\ninformation in KGs, so it is challenging for young engineers to extract and\nunderstand sub-graphs from the KG. On the other hand, there is increasing\ninterest in the use of Graph RAG, a type of RAG that combines LLMs and KGs for\nknowledge management. However, when using the current Graph RAG framework with\nan existing knowledge graph for automobile failures, several issues arise\nbecause it is difficult to generate executable queries for a knowledge graph\ndatabase which is not constructed by LLMs. To address this, we focused on\noptimizing the Graph RAG pipeline for existing knowledge graphs. Using an\noriginal Q&A dataset, the ROUGE F1 score of the sentences generated by the\nproposed method showed an average improvement of 157.6% compared to the current\nmethod. This highlights the effectiveness of the proposed method for automobile\nfailure analysis.",
      "tldr_zh": "这篇论文提出了一种基于 Graph RAG（Graph Retrieval-Augmented Generation）的知识管理系统，结合 Large Language Models (LLMs) 和 Knowledge Graphs (KGs)，用于汽车故障分析，以帮助经验工程师将知识转移给初学者。针对现有 KGs 中信息过多且查询生成困难的问题，该方法优化了 Graph RAG 管道，通过改进查询机制来更好地提取和理解故障事件的连锁关系。实验结果显示，使用原创 Q&A 数据集，该方法的 ROUGE F1 分数比现有方法平均提高了 157.6%，证明了其在汽车故障分析中的有效性。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "7 pages, 6 figures, to be published in 2024 IEEE International\n  Conference on Bid Data (BigData)",
      "pdf_url": "http://arxiv.org/pdf/2411.19539v1",
      "published_date": "2024-11-29 08:34:07 UTC",
      "updated_date": "2024-11-29 08:34:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:44:31.948917"
    },
    {
      "arxiv_id": "2411.19537v1",
      "title": "Deepfake Media Generation and Detection in the Generative AI Era: A Survey and Outlook",
      "title_zh": "生成式 AI 时代下的 Deepfake 媒体生成与检测：调查与展望",
      "authors": [
        "Florinel-Alin Croitoru",
        "Andrei-Iulian Hiji",
        "Vlad Hondru",
        "Nicolae Catalin Ristea",
        "Paul Irofti",
        "Marius Popescu",
        "Cristian Rusu",
        "Radu Tudor Ionescu",
        "Fahad Shahbaz Khan",
        "Mubarak Shah"
      ],
      "abstract": "With the recent advancements in generative modeling, the realism of deepfake\ncontent has been increasing at a steady pace, even reaching the point where\npeople often fail to detect manipulated media content online, thus being\ndeceived into various kinds of scams. In this paper, we survey deepfake\ngeneration and detection techniques, including the most recent developments in\nthe field, such as diffusion models and Neural Radiance Fields. Our literature\nreview covers all deepfake media types, comprising image, video, audio and\nmultimodal (audio-visual) content. We identify various kinds of deepfakes,\naccording to the procedure used to alter or generate the fake content. We\nfurther construct a taxonomy of deepfake generation and detection methods,\nillustrating the important groups of methods and the domains where these\nmethods are applied. Next, we gather datasets used for deepfake detection and\nprovide updated rankings of the best performing deepfake detectors on the most\npopular datasets. In addition, we develop a novel multimodal benchmark to\nevaluate deepfake detectors on out-of-distribution content. The results\nindicate that state-of-the-art detectors fail to generalize to deepfake content\ngenerated by unseen deepfake generators. Finally, we propose future directions\nto obtain robust and powerful deepfake detectors. Our project page and new\nbenchmark are available at https://github.com/CroitoruAlin/biodeep.",
      "tldr_zh": "这篇论文对生成式AI时代deepfake媒体的生成和检测技术进行了全面调查，涵盖了图像、视频、音频和多模态内容，包括diffusion models和Neural Radiance Fields等最新进展。作者构建了deepfake生成和检测方法的分类系统，汇总了相关数据集并排名了表现最佳的检测器，同时开发了一个新多模态基准来评估检测器在分布外内容上的泛化能力。结果显示，state-of-the-art检测器无法有效处理由未见过的deepfake生成器创建的内容。论文最后提出了未来方向，以提升deepfake检测器的鲁棒性和可靠性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MM",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19537v1",
      "published_date": "2024-11-29 08:29:25 UTC",
      "updated_date": "2024-11-29 08:29:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:44:44.641978"
    },
    {
      "arxiv_id": "2412.01787v2",
      "title": "Pretrained Reversible Generation as Unsupervised Visual Representation Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Rongkun Xue",
        "Jinouwen Zhang",
        "Yazhe Niu",
        "Dazhong Shen",
        "Bingqi Ma",
        "Yu Liu",
        "Jing Yang"
      ],
      "abstract": "Recent generative models based on score matching and flow matching have\nsignificantly advanced generation tasks, but their potential in discriminative\ntasks remains underexplored. Previous approaches, such as generative\nclassifiers, have not fully leveraged the capabilities of these models for\ndiscriminative tasks due to their intricate designs. We propose Pretrained\nReversible Generation (PRG), which extracts unsupervised representations by\nreversing the generative process of a pretrained continuous generation model.\nPRG effectively reuses unsupervised generative models, leveraging their high\ncapacity to serve as robust and generalizable feature extractors for downstream\ntasks. This framework enables the flexible selection of feature hierarchies\ntailored to specific downstream tasks. Our method consistently outperforms\nprior approaches across multiple benchmarks, achieving state-of-the-art\nperformance among generative model based methods, including 78% top-1 accuracy\non ImageNet at a resolution of 64. Extensive ablation studies, including\nout-of-distribution evaluations, further validate the effectiveness of our\napproach.",
      "tldr_zh": "该研究探讨了基于分数匹配(score matching)和流匹配(flow matching)的生成模型在判别任务中的潜力，提出Pretrained Reversible Generation (PRG)方法，通过逆转预训练连续生成模型的生成过程来实现无监督视觉表示学习。PRG利用生成模型的高容量作为鲁棒特征提取器，并允许根据下游任务灵活选择特征层次，从而提升模型的泛化性。在多个基准测试中，PRG超越了现有方法，实现了生成模型基于方法的最新性能，包括在ImageNet 64分辨率下78%的top-1准确率；广泛的消融研究和分布外评估进一步证实了其有效性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.01787v2",
      "published_date": "2024-11-29 08:24:49 UTC",
      "updated_date": "2025-03-08 14:13:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:46:20.862526"
    },
    {
      "arxiv_id": "2412.00156v4",
      "title": "VISION-XL: High Definition Video Inverse Problem Solver using Latent Image Diffusion Models",
      "title_zh": "翻译失败",
      "authors": [
        "Taesung Kwon",
        "Jong Chul Ye"
      ],
      "abstract": "In this paper, we propose a novel framework for solving high-definition video\ninverse problems using latent image diffusion models. Building on recent\nadvancements in spatio-temporal optimization for video inverse problems using\nimage diffusion models, our approach leverages latent-space diffusion models to\nachieve enhanced video quality and resolution. To address the high\ncomputational demands of processing high-resolution frames, we introduce a\npseudo-batch consistent sampling strategy, allowing efficient operation on a\nsingle GPU. Additionally, to improve temporal consistency, we present\npseudo-batch inversion, an initialization technique that incorporates\ninformative latents from the measurement. By integrating with SDXL, our\nframework achieves state-of-the-art video reconstruction across a wide range of\nspatio-temporal inverse problems, including complex combinations of frame\naveraging and various spatial degradations, such as deblurring,\nsuper-resolution, and inpainting. Unlike previous methods, our approach\nsupports multiple aspect ratios (landscape, vertical, and square) and delivers\nHD-resolution reconstructions (exceeding 1280x720) in under 6 seconds per frame\non a single NVIDIA 4090 GPU.",
      "tldr_zh": "本研究提出VISION-XL框架，使用latent image diffusion models解决高清视频逆问题，通过潜在空间扩散模型提升视频质量和分辨率。框架引入pseudo-batch consistent sampling策略以优化单GPU计算效率，以及pseudo-batch inversion技术来增强时间一致性。实验表明，VISION-XL与SDXL集成后，在帧平均、去模糊、超分辨率和修复等逆问题上实现最先进的重建，支持多种宽高比，并在单NVIDIA 4090 GPU上每帧不到6秒输出超过1280x720的高清结果。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://vision-xl.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2412.00156v4",
      "published_date": "2024-11-29 08:10:49 UTC",
      "updated_date": "2025-03-07 02:43:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:46:32.935069"
    },
    {
      "arxiv_id": "2411.19530v1",
      "title": "Quantized Delta Weight Is Safety Keeper",
      "title_zh": "翻译失败",
      "authors": [
        "Yule Liu",
        "Zhen Sun",
        "Xinlei He",
        "Xinyi Huang"
      ],
      "abstract": "Recent advancements in fine-tuning proprietary language models enable\ncustomized applications across various domains but also introduce two major\nchallenges: high resource demands and security risks. Regarding resource\ndemands, recent work proposes novel partial compression, such as BitDelta, to\nquantize the delta weights between the fine-tuned model and base model.\nRegarding the security risks, user-defined fine-tuning can introduce security\nvulnerabilities, such as alignment issues, backdoor attacks, and\nhallucinations. However, most of the current efforts in security assessment\nfocus on the full-precision or full-compression models, it is not\nwell-discussed how the partial compression methods affect security concerns. To\nbridge this gap, we evaluate the robustness of delta-weight quantization\nagainst these security threats. In this paper, we uncover a \"free lunch\"\nphenomenon: partial compression can enhance model security against\nfine-tuning-based attacks with bearable utility loss. Using Llama-2-7b-chat as\na case study, we show that, with under 10% utility degradation, the partial\ncompression mitigates alignment-breaking risks by up to 66.17%, harmful\nbackdoor vulnerabilities by 64.46%, and targeted output manipulation risks by\nup to 90.53%. We further apply LogitLens to visualize internal state\ntransformations during forward passes, suggesting mechanisms for both security\nfailure and recovery in standard versus compressed fine-tuning. This work\noffers new insights into selecting effective delta compression methods for\nsecure, resource-efficient multi-tenant services.",
      "tldr_zh": "本研究探讨了微调语言模型的资源需求和安全风险，提出使用 Quantized Delta Weight 作为一种部分压缩方法来量化 delta weights，从而增强模型对 fine-tuning-based attacks 的鲁棒性。实验结果显示，在 Llama-2-7b-chat 模型上，这种方法在 utility loss 低于 10% 的情况下，能降低 alignment-breaking risks 至多 66.17%、backdoor vulnerabilities 至多 64.46% 和 targeted output manipulation risks 至多 90.53%。这项工作揭示了部分压缩的“免费午餐”现象，并通过 LogitLens 分析内部状态，提供安全高效的多租户服务的实用见解。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19530v1",
      "published_date": "2024-11-29 08:05:50 UTC",
      "updated_date": "2024-11-29 08:05:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:46:44.738721"
    },
    {
      "arxiv_id": "2411.19528v1",
      "title": "RAGDiffusion: Faithful Cloth Generation via External Knowledge Assimilation",
      "title_zh": "翻译失败",
      "authors": [
        "Xianfeng Tan",
        "Yuhan Li",
        "Wenxiang Shang",
        "Yubo Wu",
        "Jian Wang",
        "Xuanhong Chen",
        "Yi Zhang",
        "Ran Lin",
        "Bingbing Ni"
      ],
      "abstract": "Standard clothing asset generation involves creating forward-facing flat-lay\ngarment images displayed on a clear background by extracting clothing\ninformation from diverse real-world contexts, which presents significant\nchallenges due to highly standardized sampling distributions and precise\nstructural requirements in the generated images. Existing models have limited\nspatial perception and often exhibit structural hallucinations in this\nhigh-specification generative task. To address this issue, we propose a novel\nRetrieval-Augmented Generation (RAG) framework, termed RAGDiffusion, to enhance\nstructure determinacy and mitigate hallucinations by assimilating external\nknowledge from LLM and databases. RAGDiffusion consists of two core processes:\n(1) Retrieval-based structure aggregation, which employs contrastive learning\nand a Structure Locally Linear Embedding (SLLE) to derive global structure and\nspatial landmarks, providing both soft and hard guidance to counteract\nstructural ambiguities; and (2) Omni-level faithful garment generation, which\nintroduces a three-level alignment that ensures fidelity in structural,\npattern, and decoding components within the diffusing. Extensive experiments on\nchallenging real-world datasets demonstrate that RAGDiffusion synthesizes\nstructurally and detail-faithful clothing assets with significant performance\nimprovements, representing a pioneering effort in high-specification faithful\ngeneration with RAG to confront intrinsic hallucinations and enhance fidelity.",
      "tldr_zh": "该论文提出了 RAGDiffusion，一种基于 Retrieval-Augmented Generation (RAG) 的框架，用于生成保真的服装资产，通过吸收外部知识（如 LLM 和数据库）来解决现有模型在结构感知和幻觉问题上的局限性。框架的核心包括：(1) Retrieval-based structure aggregation，利用对比学习和 Structure Locally Linear Embedding (SLLE) 来提取全局结构和空间 landmarks，提供软硬指导以减少结构模糊；(2) Omni-level faithful garment generation，通过三层对齐确保结构、图案和解码组件的精确保真度。实验结果显示，在真实世界数据集上，RAGDiffusion 显著提升了服装资产的结构和细节保真度，代表了 RAG 在高规格生成任务中的开创性应用。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Project website: https://colorful-liyu.github.io/RAGDiffusion-page/",
      "pdf_url": "http://arxiv.org/pdf/2411.19528v1",
      "published_date": "2024-11-29 07:57:32 UTC",
      "updated_date": "2024-11-29 07:57:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:46:57.486996"
    },
    {
      "arxiv_id": "2411.19527v3",
      "title": "DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding",
      "title_zh": "翻译失败",
      "authors": [
        "Jungbin Cho",
        "Junwan Kim",
        "Jisoo Kim",
        "Minseo Kim",
        "Mingu Kang",
        "Sungeun Hong",
        "Tae-Hyun Oh",
        "Youngjae Yu"
      ],
      "abstract": "Human motion is inherently continuous and dynamic, posing significant\nchallenges for generative models. While discrete generation methods are widely\nused, they suffer from limited expressiveness and frame-wise noise artifacts.\nIn contrast, continuous approaches produce smoother, more natural motion but\noften struggle to adhere to conditioning signals due to high-dimensional\ncomplexity and limited training data. To resolve this discord between discrete\nand continuous representations, we introduce DisCoRD: Discrete Tokens to\nContinuous Motion via Rectified Flow Decoding, a novel method that leverages\nrectified flow to decode discrete motion tokens in the continuous, raw motion\nspace. Our core idea is to frame token decoding as a conditional generation\ntask, ensuring that DisCoRD captures fine-grained dynamics and achieves\nsmoother, more natural motions. Compatible with any discrete-based framework,\nour method enhances naturalness without compromising faithfulness to the\nconditioning signals on diverse settings. Extensive evaluations Our project\npage is available at: https://whwjdqls.github.io/discord.github.io/.",
      "tldr_zh": "这篇论文解决了人类动作生成中的离散和连续表示矛盾，提出 DisCoRD 方法，通过 Rectified Flow Decoding 将离散 motion tokens 解码到连续的原始动作空间。核心创新是将标记解码视为条件生成任务，以捕捉细粒度动态并产生更平滑、自然的动作，同时保持对条件信号的忠实性。DisCoRD 兼容任何基于离散的框架，并在多样设置中提升生成质量，实验结果显示其在自然性和准确性方面显著优于基线模型。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages",
      "pdf_url": "http://arxiv.org/pdf/2411.19527v3",
      "published_date": "2024-11-29 07:54:56 UTC",
      "updated_date": "2025-04-18 08:49:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:47:08.748777"
    },
    {
      "arxiv_id": "2411.19526v1",
      "title": "A Local Information Aggregation based Multi-Agent Reinforcement Learning for Robot Swarm Dynamic Task Allocation",
      "title_zh": "基于本地信息聚合的多智能体",
      "authors": [
        "Yang Lv",
        "Jinlong Lei",
        "Peng Yi"
      ],
      "abstract": "In this paper, we explore how to optimize task allocation for robot swarms in\ndynamic environments, emphasizing the necessity of formulating robust,\nflexible, and scalable strategies for robot cooperation. We introduce a novel\nframework using a decentralized partially observable Markov decision process\n(Dec_POMDP), specifically designed for distributed robot swarm networks. At the\ncore of our methodology is the Local Information Aggregation Multi-Agent Deep\nDeterministic Policy Gradient (LIA_MADDPG) algorithm, which merges centralized\ntraining with distributed execution (CTDE). During the centralized training\nphase, a local information aggregation (LIA) module is meticulously designed to\ngather critical data from neighboring robots, enhancing decision-making\nefficiency. In the distributed execution phase, a strategy improvement method\nis proposed to dynamically adjust task allocation based on changing and\npartially observable environmental conditions. Our empirical evaluations show\nthat the LIA module can be seamlessly integrated into various CTDE-based MARL\nmethods, significantly enhancing their performance. Additionally, by comparing\nLIA_MADDPG with six conventional reinforcement learning algorithms and a\nheuristic algorithm, we demonstrate its superior scalability, rapid adaptation\nto environmental changes, and ability to maintain both stability and\nconvergence speed. These results underscore LIA_MADDPG's outstanding\nperformance and its potential to significantly improve dynamic task allocation\nin robot swarms through enhanced local collaboration and adaptive strategy\nexecution.",
      "tldr_zh": "本论文探讨了在动态环境中优化机器人群任务分配的问题，提出了一种基于 Dec_POMDP 的分布式框架，以实现鲁棒、灵活和可扩展的机器人合作。核心方法是 LIA_MADDPG 算法，该算法结合 CTDE（集中训练分布式执行），通过本地信息聚合 (LIA) 模块从邻近机器人收集关键数据，并动态调整策略以适应环境变化。实验结果显示，LIA_MADDPG 相较于六种传统强化学习算法和启发式算法，显著提升了性能，在可扩展性、适应速度以及稳定性和收敛性方面表现出色。",
      "categories": [
        "cs.AI",
        "cs.MA",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19526v1",
      "published_date": "2024-11-29 07:53:05 UTC",
      "updated_date": "2024-11-29 07:53:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:47:20.524226"
    },
    {
      "arxiv_id": "2411.19523v2",
      "title": "Density-Calibrated Conformal Quantile Regression",
      "title_zh": "翻译失败",
      "authors": [
        "Yuan Lu"
      ],
      "abstract": "This paper introduces the Density-Calibrated Conformal Quantile Regression\n(CQR-d) method, a novel approach for constructing prediction intervals that\nadapts to varying uncertainty across the feature space. Building upon conformal\nquantile regression, CQR-d incorporates local information through a weighted\ncombination of local and global conformity scores, where the weights are\ndetermined by local data density. We prove that CQR-d provides valid marginal\ncoverage at level $1 - \\alpha - \\epsilon$, where $\\epsilon$ represents a small\ntolerance from numerical optimization. Through extensive simulation studies and\nan application to the a heteroscedastic dataset available in R, we demonstrate\nthat CQR-d maintains the desired coverage while producing substantially\nnarrower prediction intervals compared to standard conformal quantile\nregression (CQR). The method's effectiveness is particularly pronounced in\nsettings with clear local uncertainty patterns, making it a valuable tool for\nprediction tasks in heterogeneous data environments.",
      "tldr_zh": "本论文提出了一种新方法Density-Calibrated Conformal Quantile Regression (CQR-d)，用于构建适应特征空间不确定性的预测区间，通过加权结合局部和全局一致性分数（权重基于局部数据密度）来提升预测精度。相比标准Conformal Quantile Regression (CQR)，CQR-d在理论上证明了其边际覆盖率可达1 - α - ε，其中ε为数值优化中的小容差。实验结果显示，该方法在模拟研究和异方差数据集应用中保持了期望的覆盖率，同时显著缩小了预测区间，尤其适用于具有明显局部不确定性的异质数据环境。",
      "categories": [
        "stat.ME",
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "stat.ME",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19523v2",
      "published_date": "2024-11-29 07:41:20 UTC",
      "updated_date": "2024-12-02 20:33:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:49:26.353313"
    },
    {
      "arxiv_id": "2411.19517v4",
      "title": "RL-MILP Solver: A Reinforcement Learning Approach for Solving Mixed-Integer Linear Programs with Graph Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Tae-Hoon Lee",
        "Min-Soo Kim"
      ],
      "abstract": "Mixed-integer linear programming (MILP) is a widely used optimization\ntechnique across various fields. Existing $\\textit{end-to-end learning}$\nmethods for MILP generate values for a subset of decision variables and\ndelegate the remaining problem to traditional MILP solvers. However, this\napproach often fails to guarantee solution feasibility (i.e., satisfying all\nconstraints) due to inaccurate predictions and primarily focuses on binary\ndecision variables. Satisfying all constraints is a prerequisite for obtaining\nthe optimal solution, and the feasibility issue becomes even more critical with\nnon-binary integer (integer, for short) variables. Thus, addressing the\nfeasibility of MILP involving integer variables is crucial. To address these\nchallenges, we propose a novel reinforcement learning (RL)-based solver that\nnot only finds the first feasible solution but also incrementally discovers\nbetter feasible solutions without delegating the remainder to off-the-shelf\nsolvers. Our experimental results demonstrate that the proposed method achieves\n(near-)optimal solutions.",
      "tldr_zh": "该论文提出了一种基于强化学习（RL）的求解器RL-MILP Solver，用于解决混合整数线性规划（MILP）问题，该方法利用图神经网络（Graph Neural Networks）生成完整决策变量值，而非依赖传统求解器，从而解决现有端到端学习方法在可行性预测上的不足，特别是针对非二进制整数变量。不同于现有方法，该求解器不仅能找到首个可行解决方案，还能逐步优化以发现更优方案。实验结果表明，该方法能够实现（近似）最优解决方案，提升了MILP问题的求解效率和可靠性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Extended version (17 pages, 8 figures). Accepted at the 2025 AAAI\n  Workshop on AI to Accelerate Science and Engineering (AI2ASE)",
      "pdf_url": "http://arxiv.org/pdf/2411.19517v4",
      "published_date": "2024-11-29 07:23:34 UTC",
      "updated_date": "2025-03-11 08:21:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:47:44.402030"
    },
    {
      "arxiv_id": "2412.00154v2",
      "title": "o1-Coder: an o1 Replication for Coding",
      "title_zh": "翻译失败",
      "authors": [
        "Yuxiang Zhang",
        "Shangxi Wu",
        "Yuqi Yang",
        "Jiangming Shu",
        "Jinlin Xiao",
        "Chao Kong",
        "Jitao Sang"
      ],
      "abstract": "The technical report introduces O1-CODER, an attempt to replicate OpenAI's o1\nmodel with a focus on coding tasks. It integrates reinforcement learning (RL)\nand Monte Carlo Tree Search (MCTS) to enhance the model's System-2 thinking\ncapabilities. The framework includes training a Test Case Generator (TCG) for\nstandardized code testing, using MCTS to generate code data with reasoning\nprocesses, and iteratively fine-tuning the policy model to initially produce\npseudocode and then generate the full code. The report also addresses the\nopportunities and challenges in deploying o1-like models in real-world\napplications, suggesting transitioning to the System-2 paradigm and\nhighlighting the imperative for world model construction. Updated model\nprogress and experimental results will be reported in subsequent versions. All\nsource code, curated datasets, as well as the derived models are disclosed at\nhttps://github.com/ADaM-BJTU/O1-CODER .",
      "tldr_zh": "该研究介绍了 O1-CODER，一种针对编码任务的 OpenAI o1 模型复制框架，旨在通过强化学习 (RL) 和 Monte Carlo Tree Search (MCTS) 增强模型的 System-2 思考能力。框架包括训练 Test Case Generator (TCG) 用于标准化代码测试，利用 MCTS 生成带有推理过程的代码数据，并通过迭代微调策略模型从伪代码逐步生成完整代码。论文讨论了在真实世界应用中部署 o1-like 模型的机会与挑战，强调转向 System-2 范式和世界模型构建的必要性，并公开了源代码和数据集于 GitHub，后续版本将报告更新实验结果。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00154v2",
      "published_date": "2024-11-29 07:19:56 UTC",
      "updated_date": "2024-12-10 04:39:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:47:57.786065"
    },
    {
      "arxiv_id": "2411.19504v1",
      "title": "TQA-Bench: Evaluating LLMs for Multi-Table Question Answering with Scalable Context and Symbolic Extension",
      "title_zh": "翻译失败",
      "authors": [
        "Zipeng Qiu",
        "You Peng",
        "Guangxin He",
        "Binhang Yuan",
        "Chen Wang"
      ],
      "abstract": "The advent of large language models (LLMs) has unlocked great opportunities\nin complex data management tasks, particularly in question answering (QA) over\ncomplicated multi-table relational data. Despite significant progress,\nsystematically evaluating LLMs on multi-table QA remains a critical challenge\ndue to the inherent complexity of analyzing heterogeneous table structures and\npotential large scale of serialized relational data. Existing benchmarks\nprimarily focus on single-table QA, failing to capture the intricacies of\nreasoning across multiple relational tables, as required in real-world domains\nsuch as finance, healthcare, and e-commerce. To address this gap, we present\nTQA-Bench, a new multi-table QA benchmark designed to evaluate the capabilities\nof LLMs in tackling complex QA tasks over relational data. Our benchmark\nincorporates diverse relational database instances sourced from real-world\npublic datasets and introduces a flexible sampling mechanism to create tasks\nwith varying multi-table context lengths, ranging from 8K to 64K tokens. To\nensure robustness and reliability, we integrate symbolic extensions into the\nevaluation framework, enabling the assessment of LLM reasoning capabilities\nbeyond simple data retrieval or probabilistic pattern matching. We\nsystematically evaluate a range of LLMs, both open-source and closed-source,\nspanning model scales from 7 billion to 70 billion parameters. Our extensive\nexperiments reveal critical insights into the performance of LLMs in\nmulti-table QA, highlighting both challenges and opportunities for advancing\ntheir application in complex, data-driven environments. Our benchmark\nimplementation and results are available at\nhttps://github.com/Relaxed-System-Lab/TQA-Bench.",
      "tldr_zh": "本研究引入了 TQA-Bench，这是一个新的基准，用于评估大型语言模型 (LLMs) 在多表问答 (multi-table QA) 任务中的性能，旨在解决现有基准偏重单表 QA 的局限性。TQA-Bench 利用真实世界公共数据集构建多样化的关系数据库实例，并通过灵活的采样机制生成上下文长度从 8K 到 64K tokens 的任务，同时整合 symbolic extensions 来测试 LLMs 的高级推理能力，而非单纯的数据检索。研究对一系列开源和闭源 LLMs（参数规模从 7 亿到 70 亿）进行了系统评估，结果揭示了这些模型在处理复杂多表 QA 时的关键挑战和潜在机会。该基准的实现和实验结果可在 GitHub 上获取。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19504v1",
      "published_date": "2024-11-29 06:48:13 UTC",
      "updated_date": "2024-11-29 06:48:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:48:09.302093"
    },
    {
      "arxiv_id": "2411.19502v1",
      "title": "Knowledge-Data Fusion Based Source-Free Semi-Supervised Domain Adaptation for Seizure Subtype Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Ruimin Peng",
        "Jiayu An",
        "Dongrui Wu"
      ],
      "abstract": "Electroencephalogram (EEG)-based seizure subtype classification enhances\nclinical diagnosis efficiency. Source-free semi-supervised domain adaptation\n(SF-SSDA), which transfers a pre-trained model to a new dataset with no source\ndata and limited labeled target data, can be used for privacy-preserving\nseizure subtype classification. This paper considers two challenges in SF-SSDA\nfor EEG-based seizure subtype classification: 1) How to effectively fuse both\nraw EEG data and expert knowledge in classifier design? 2) How to align the\nsource and target domain distributions for SF-SSDA? We propose a Knowledge-Data\nFusion based SF-SSDA approach, KDF-MutualSHOT, for EEG-based seizure subtype\nclassification. In source model training, KDF uses Jensen-Shannon Divergence to\nfacilitate mutual learning between a feature-driven Decision Tree-based model\nand a data-driven Transformer-based model. To adapt KDF to a new target\ndataset, an SF-SSDA algorithm, MutualSHOT, is developed, which features a\nconsistency-based pseudo-label selection strategy. Experiments on the public\nTUSZ and CHSZ datasets demonstrated that KDF-MutualSHOT outperformed other\nsupervised and source-free domain adaptation approaches in cross-subject\nseizure subtype classification.",
      "tldr_zh": "该研究提出了一种基于知识-数据融合(Knowledge-Data Fusion)的源自由半监督域适应(Source-Free Semi-Supervised Domain Adaptation, SF-SSDA)方法，名为 KDF-MutualSHOT，用于 EEG（脑电图）-based 癫痫亚型分类，以提升临床诊断效率并保护隐私。方法在源模型训练中利用 Jensen-Shannon Divergence 促进特征驱动的 Decision Tree 模型和数据驱动的 Transformer 模型之间的相互学习，从而有效融合原始 EEG 数据和专家知识。针对新目标数据集，MutualSHOT 算法引入基于一致性的伪标签选择策略，以对齐源域和目标域分布。实验在 TUSZ 和 CHSZ 数据集上显示，KDF-MutualSHOT 在跨主体癫痫亚型分类中优于其他监督和源自由域适应方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19502v1",
      "published_date": "2024-11-29 06:40:45 UTC",
      "updated_date": "2024-11-29 06:40:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:49:38.222106"
    },
    {
      "arxiv_id": "2412.00152v1",
      "title": "Dynamic Neural Curiosity Enhances Learning Flexibility for Autonomous Goal Discovery",
      "title_zh": "翻译失败",
      "authors": [
        "Quentin Houbre",
        "Roel Pieters"
      ],
      "abstract": "The autonomous learning of new goals in robotics remains a complex issue to\naddress. Here, we propose a model where curiosity influence learning\nflexibility. To do so, this paper proposes to root curiosity and attention\ntogether by taking inspiration from the Locus Coeruleus-Norepinephrine system\nalong with various cognitive processes such as cognitive persistence and visual\nhabituation. We apply our approach by experimenting with a simulated robotic\narm on a set of objects with varying difficulty. The robot first discovers new\ngoals via bottom-up attention through motor babbling with an inhibition of\nreturn mechanism, then engage to the learning of goals due to neural activity\narising within the curiosity mechanism. The architecture is modelled with\ndynamic neural fields and the learning of goals such as pushing the objects in\ndiverse directions is supported by the use of forward and inverse models\nimplemented by multi-layer perceptrons. The adoption of dynamic neural fields\nto model curiosity, habituation and persistence allows the robot to demonstrate\nvarious learning trajectories depending on the object. In addition, the\napproach exhibits interesting properties regarding the learning of similar\ngoals as well as the continuous switch between exploration and exploitation.",
      "tldr_zh": "本研究提出了一种动态神经好奇心（Dynamic Neural Curiosity）模型，以增强机器人自主目标发现的学习灵活性。该模型将好奇心与注意力相结合，借鉴 Locus Coeruleus-Norepinephrine 系统以及认知过程如认知持久性和视觉习惯化，在模拟机器人臂上通过 motor babbling 和 inhibition of return 机制实现目标发现和学习。使用动态神经场（dynamic neural fields）建模好奇心、习惯化和持久性，并结合前向和逆向模型（由多层感知器实现），机器人能够根据物体难度展示不同的学习轨迹，并在探索和利用之间实现持续切换。实验结果表明，该方法显著提高了机器人学习类似目标的能力，推动了机器人自主学习领域的进展。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00152v1",
      "published_date": "2024-11-29 06:37:23 UTC",
      "updated_date": "2024-11-29 06:37:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:49:49.003823"
    },
    {
      "arxiv_id": "2411.19500v1",
      "title": "COLD: Causal reasOning in cLosed Daily activities",
      "title_zh": "翻译失败",
      "authors": [
        "Abhinav Joshi",
        "Areeb Ahmad",
        "Ashutosh Modi"
      ],
      "abstract": "Large Language Models (LLMs) have shown state-of-the-art performance in a\nvariety of tasks, including arithmetic and reasoning; however, to gauge the\nintellectual capabilities of LLMs, causal reasoning has become a reliable proxy\nfor validating a general understanding of the mechanics and intricacies of the\nworld similar to humans. Previous works in natural language processing (NLP)\nhave either focused on open-ended causal reasoning via causal commonsense\nreasoning (CCR) or framed a symbolic representation-based question answering\nfor theoretically backed-up analysis via a causal inference engine. The former\nadds an advantage of real-world grounding but lacks theoretically backed-up\nanalysis/validation, whereas the latter is far from real-world grounding. In\nthis work, we bridge this gap by proposing the COLD (Causal reasOning in cLosed\nDaily activities) framework, which is built upon human understanding of daily\nreal-world activities to reason about the causal nature of events. We show that\nthe proposed framework facilitates the creation of enormous causal queries (~ 9\nmillion) and comes close to the mini-turing test, simulating causal reasoning\nto evaluate the understanding of a daily real-world task. We evaluate multiple\nLLMs on the created causal queries and find that causal reasoning is\nchallenging even for activities trivial to humans. We further explore (the\ncausal reasoning abilities of LLMs) using the backdoor criterion to determine\nthe causal strength between events.",
      "tldr_zh": "本文提出 COLD 框架（Causal reasOning in cLosed Daily activities），旨在桥接开放式因果推理（如 Causal Commonsense Reasoning）和符号表示方法的差距，通过基于人类对日常活动的理解来评估大型语言模型（LLMs）的因果推理能力。该框架能生成约 9 百万个因果查询，并模拟迷你图灵测试，模拟真实世界任务的因果关系理解。实验评估了多个 LLMs，发现它们在人类眼中简单的日常活动中表现出色挑战性；此外，利用后门准则（backdoor criterion）进一步分析了事件间因果强度的表现。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Paper accepted at NeurIPS 2024; Total 37 Pages",
      "pdf_url": "http://arxiv.org/pdf/2411.19500v1",
      "published_date": "2024-11-29 06:37:13 UTC",
      "updated_date": "2024-11-29 06:37:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:50:24.821908"
    },
    {
      "arxiv_id": "2411.19498v1",
      "title": "Protecting Multiple Types of Privacy Simultaneously in EEG-based Brain-Computer Interfaces",
      "title_zh": "在基于 EEG 的脑机接口中同时保护多种类型的隐私",
      "authors": [
        "Lubin Meng",
        "Xue Jiang",
        "Tianwang Jia",
        "Dongrui Wu"
      ],
      "abstract": "A brain-computer interface (BCI) enables direct communication between the\nbrain and an external device. Electroencephalogram (EEG) is the preferred input\nsignal in non-invasive BCIs, due to its convenience and low cost. EEG-based\nBCIs have been successfully used in many applications, such as neurological\nrehabilitation, text input, games, and so on. However, EEG signals inherently\ncarry rich personal information, necessitating privacy protection. This paper\ndemonstrates that multiple types of private information (user identity, gender,\nand BCI-experience) can be easily inferred from EEG data, imposing a serious\nprivacy threat to BCIs. To address this issue, we design perturbations to\nconvert the original EEG data into privacy-protected EEG data, which conceal\nthe private information while maintaining the primary BCI task performance.\nExperimental results demonstrated that the privacy-protected EEG data can\nsignificantly reduce the classification accuracy of user identity, gender and\nBCI-experience, but almost do not affect at all the classification accuracy of\nthe primary BCI task, enabling user privacy protection in EEG-based BCIs.",
      "tldr_zh": "这篇论文探讨了基于EEG的脑机接口(BCI)系统中，用户身份、性别和BCI经验等多个类型隐私信息的潜在泄露风险。研究人员提出了一种方法，通过设计perturbations来转换原始EEG数据，从而隐藏私人信息，同时保持主要BCI任务的性能。实验结果表明，隐私保护后的EEG数据显著降低了私人信息的分类准确率，但几乎不影响BCI任务的分类准确率，为EEG-based BCI的安全应用提供了有效解决方案。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19498v1",
      "published_date": "2024-11-29 06:33:31 UTC",
      "updated_date": "2024-11-29 06:33:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:50:12.988937"
    },
    {
      "arxiv_id": "2412.00151v1",
      "title": "DLaVA: Document Language and Vision Assistant for Answer Localization with Enhanced Interpretability and Trustworthiness",
      "title_zh": "DLaVA：用于答案定位的文档语言与视觉助手，具有增强的可解释性和可信度",
      "authors": [
        "Ahmad Mohammadshirazi",
        "Pinaki Prasad Guha Neogi",
        "Ser-Nam Lim",
        "Rajiv Ramnath"
      ],
      "abstract": "Document Visual Question Answering (VQA) requires models to interpret textual\ninformation within complex visual layouts and comprehend spatial relationships\nto answer questions based on document images. Existing approaches often lack\ninterpretability and fail to precisely localize answers within the document,\nhindering users' ability to verify responses and understand the reasoning\nprocess. Moreover, standard metrics like Average Normalized Levenshtein\nSimilarity (ANLS) focus on text accuracy but overlook spatial correctness. We\nintroduce DLaVA, a novel method that enhances Multimodal Large Language Models\n(MLLMs) with answer localization capabilities for Document VQA. Our approach\nintegrates image annotation directly into the MLLM pipeline, improving\ninterpretability by enabling users to trace the model's reasoning. We present\nboth OCR-dependent and OCR-free architectures, with the OCR-free approach\neliminating the need for separate text recognition components, thus reducing\ncomplexity. To the best of our knowledge, DLaVA is the first approach to\nintroduce answer localization within multimodal QA, marking a significant step\nforward in enhancing user trust and reducing the risk of AI hallucinations. Our\ncontributions include enhancing interpretability and reliability by grounding\nresponses in spatially annotated visual content, introducing answer\nlocalization in MLLMs, proposing a streamlined pipeline that combines an MLLM\nwith a text detection module, and conducting comprehensive evaluations using\nboth textual and spatial accuracy metrics, including Intersection over Union\n(IoU). Experimental results on standard datasets demonstrate that DLaVA\nachieves SOTA performance, significantly enhancing model transparency and\nreliability. Our approach sets a new benchmark for Document VQA, highlighting\nthe critical importance of precise answer localization and model\ninterpretability.",
      "tldr_zh": "本研究针对 Document VQA 的挑战，提出 DLaVA，一种增强 Multimodal Large Language Models (MLLMs) 的方法，专注于答案定位以提升模型的可解释性和可信度。DLaVA 通过将图像标注集成到 MLLM 管道中，提供 OCR-dependent 和 OCR-free 架构，解决了现有方法在空间关系理解和答案精确定位上的不足，同时减少了 AI 幻觉的风险。创新点包括首次在多模态 QA 中引入答案定位、简化管道设计，并使用指标如 Intersection over Union (IoU) 进行全面评估。实验结果显示，DLaVA 在标准数据集上达到 SOTA 性能，显著提高了模型的透明度和可靠性，为 Document VQA 设置了新基准。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.00151v1",
      "published_date": "2024-11-29 06:17:11 UTC",
      "updated_date": "2024-11-29 06:17:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:50:26.786711"
    },
    {
      "arxiv_id": "2411.19488v2",
      "title": "Interleaved-Modal Chain-of-Thought",
      "title_zh": "翻译失败",
      "authors": [
        "Jun Gao",
        "Yongqi Li",
        "Ziqiang Cao",
        "Wenjie Li"
      ],
      "abstract": "Chain-of-Thought (CoT) prompting elicits large language models (LLMs) to\nproduce a series of intermediate reasoning steps before arriving at the final\nanswer. However, when transitioning to vision-language models (VLMs), their\ntext-only rationales struggle to express the fine-grained associations with the\noriginal image. In this paper, we propose an image-incorporated multimodal\nChain-of-Thought, named \\textbf{Interleaved-modal Chain-of-Thought (ICoT)},\nwhich generates sequential reasoning steps consisting of paired visual and\ntextual rationales to infer the final answer. Intuitively, the novel ICoT\nrequires VLMs to enable the generation of fine-grained interleaved-modal\ncontent, which is hard for current VLMs to fulfill. Considering that the\nrequired visual information is usually part of the input image, we propose\n\\textbf{Attention-driven Selection (ADS)} to realize ICoT over existing VLMs.\nADS intelligently inserts regions of the input image to generate the\ninterleaved-modal reasoning steps with ignorable additional latency. ADS relies\nsolely on the attention map of VLMs without the need for parameterization, and\ntherefore it is a plug-and-play strategy that can be generalized to a spectrum\nof VLMs. We apply ADS to realize ICoT on two popular VLMs of different\narchitectures. Extensive evaluations of three benchmarks have shown that ICoT\nprompting achieves substantial performance (up to 14\\%) and interpretability\nimprovements compared to existing multimodal CoT prompting methods.",
      "tldr_zh": "本论文提出 Interleaved-modal Chain-of-Thought (ICoT)，一种改进 Chain-of-Thought (CoT) 的方法，应用于视觉语言模型 (VLMs)，通过生成交错的图像和文本推理步骤来更好地表达图像的细粒度关联，从而提升任务性能和可解释性。ICoT 要求 VLMs 输出成对的视觉与文本理性步骤，但现有模型难以实现，因此作者引入 Attention-driven Selection (ADS) 策略，利用 VLMs 的注意力图智能选择并插入输入图像区域，实现无参数的即插即用推理增强。实验结果显示，在三个基准上，ICoT 比现有多模态 CoT 方法提高了高达 14% 的性能和可解释性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2025 Main Conference",
      "pdf_url": "http://arxiv.org/pdf/2411.19488v2",
      "published_date": "2024-11-29 06:06:35 UTC",
      "updated_date": "2025-03-17 09:01:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:50:39.288771"
    },
    {
      "arxiv_id": "2412.03594v2",
      "title": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix Sharing and Throughput-oriented Token Batching",
      "title_zh": "翻译失败",
      "authors": [
        "Zhen Zheng",
        "Xin Ji",
        "Taosong Fang",
        "Fanghao Zhou",
        "Chuanjie Liu",
        "Gang Peng"
      ],
      "abstract": "Large language models (LLMs) increasingly play an important role in a wide\nrange of information processing and management tasks. Many of these tasks are\nperformed in large batches or even offline, and the performance indictor for\nwhich is throughput. These tasks usually show the characteristic of prefix\nsharing, where different prompt input can partially show the common prefix.\nHowever, the existing LLM inference engines tend to optimize the streaming\nrequests and show limitations of supporting the large batched tasks with the\nprefix sharing characteristic. The existing solutions use the LRU-based cache\nto reuse the KV context of common prefix between requests. The KV context that\nare about to be reused may prematurely evicted with the implicit cache\nmanagement. Besides, the streaming oriented systems do not leverage the\nrequest-batch information and can not mix the decoding tokens with the prefill\nchunks to the best for the batched scenarios, and thus fails to saturate the\nGPU. We propose BatchLLM to address the above problems. BatchLLM explicitly\nidentifies the common prefixes globally. The requests sharing the same prefix\nwill be scheduled together to reuse the KV context the best. BatchLLM reorders\nthe requests and schedules the requests with larger ratio of decoding first to\nbetter mix the decoding tokens with the latter prefill chunks, and applies\nmemory-centric token batching to enlarge the token-batch sizes, which helps to\nincrease the GPU utilization. Finally, BatchLLM optimizes the prefix-shared\nAttention kernel with horizontal fusion to reduce tail effect and kernel launch\noverhead. Extensive evaluation shows that BatchLLM outperforms vLLM and SGLang\nby 1.3$\\times$ to 10.8$\\times$ on a set of microbenchmarks and a typical\nindustry workload under different hardware environments.",
      "tldr_zh": "本论文提出 BatchLLM 系统，针对大型语言模型 (LLMs) 在批量推理任务中的性能优化，重点解决前缀共享特性的利用不足和 GPU 利用率低的问题。BatchLLM 通过全局识别共同前缀、将共享前缀的请求一起调度、重新排序请求以优先混合解码令牌和预填充块，以及应用内存中心令牌批量处理和优化前缀共享 Attention 内核，来提升吞吐量并减少内核启动开销。与现有系统相比，该方法显著改善了 KV context 的重用效率。实验结果显示，BatchLLM 在不同硬件环境下，比 vLLM 和 SGLang 在微基准和实际工作负载上性能提升 1.3 倍到 10.8 倍。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DC",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.03594v2",
      "published_date": "2024-11-29 05:57:37 UTC",
      "updated_date": "2025-01-17 09:37:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:50:51.437886"
    },
    {
      "arxiv_id": "2411.19485v1",
      "title": "Action Engine: An LLM-based Framework for Automatic FaaS Workflow Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Akiharu Esashi",
        "Pawissanutt Lertpongrujikorn",
        "Mohsen Amini Salehi"
      ],
      "abstract": "Function as a Service (FaaS) is poised to become the foundation of the next\ngeneration of cloud systems due to its inherent advantages in scalability,\ncost-efficiency, and ease of use. However, challenges such as the need for\nspecialized knowledge and difficulties in building function workflows persist\nfor cloud-native application developers. To overcome these challenges and\nmitigate the burden of developing FaaS-based applications, in this paper, we\npropose a mechanism called Action Engine, that makes use of Tool-Augmented\nLarge Language Models (LLMs) at its kernel to interpret human language queries\nand automates FaaS workflow generation, thereby, reducing the need for\nspecialized expertise and manual design. Action Engine includes modules to\nidentify relevant functions from the FaaS repository and seamlessly manage the\ndata dependency between them, ensuring that the developer's query is processed\nand resolved. Beyond that, Action Engine can execute the generated workflow by\nfeeding the user-provided parameters. Our evaluations show that Action Engine\ncan generate workflows with up to 20\\% higher correctness without developer\ninvolvement. We notice that Action Engine can unlock FaaS workflow generation\nfor non-cloud-savvy developers and expedite the development cycles of\ncloud-native applications.",
      "tldr_zh": "该研究针对 FaaS（Function as a Service）的开发挑战，如需要专业知识和手动构建工作流的问题，提出了一种基于 Tool-Augmented Large Language Models (LLMs) 的框架 Action Engine。Action Engine 通过解析人类语言查询，从 FaaS 仓库中识别相关函数、管理数据依赖，并自动生成和执行工作流，从而减少开发者参与。实验结果显示，该框架可将工作流正确性提高高达 20%，有助于非云专家开发者加速云原生应用的开发周期。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.LG",
        "cs.SE"
      ],
      "primary_category": "cs.DC",
      "comment": "Accepted at Utility Cloud Computing (UCC '24) conference",
      "pdf_url": "http://arxiv.org/pdf/2411.19485v1",
      "published_date": "2024-11-29 05:54:41 UTC",
      "updated_date": "2024-11-29 05:54:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:51:00.889979"
    },
    {
      "arxiv_id": "2411.19479v1",
      "title": "FLARE: Towards Universal Dataset Purification against Backdoor Attacks",
      "title_zh": "FLARE：面向通用数据集净化以对抗后门攻击",
      "authors": [
        "Linshan Hou",
        "Wei Luo",
        "Zhongyun Hua",
        "Songhua Chen",
        "Leo Yu Zhang",
        "Yiming Li"
      ],
      "abstract": "Deep neural networks (DNNs) are susceptible to backdoor attacks, where\nadversaries poison datasets with adversary-specified triggers to implant hidden\nbackdoors, enabling malicious manipulation of model predictions. Dataset\npurification serves as a proactive defense by removing malicious training\nsamples to prevent backdoor injection at its source. We first reveal that the\ncurrent advanced purification methods rely on a latent assumption that the\nbackdoor connections between triggers and target labels in backdoor attacks are\nsimpler to learn than the benign features. We demonstrate that this assumption,\nhowever, does not always hold, especially in all-to-all (A2A) and untargeted\n(UT) attacks. As a result, purification methods that analyze the separation\nbetween the poisoned and benign samples in the input-output space or the final\nhidden layer space are less effective. We observe that this separability is not\nconfined to a single layer but varies across different hidden layers. Motivated\nby this understanding, we propose FLARE, a universal purification method to\ncounter various backdoor attacks. FLARE aggregates abnormal activations from\nall hidden layers to construct representations for clustering. To enhance\nseparation, FLARE develops an adaptive subspace selection algorithm to isolate\nthe optimal space for dividing an entire dataset into two clusters. FLARE\nassesses the stability of each cluster and identifies the cluster with higher\nstability as poisoned. Extensive evaluations on benchmark datasets demonstrate\nthe effectiveness of FLARE against 22 representative backdoor attacks,\nincluding all-to-one (A2O), all-to-all (A2A), and untargeted (UT) attacks, and\nits robustness to adaptive attacks.",
      "tldr_zh": "这篇论文针对深度神经网络(DNNs)中的后门攻击，提出了一种通用数据集净化方法FLARE，以移除恶意样本并防止后门注入。FLARE通过从所有隐藏层聚合异常激活来构建表示，并使用自适应子空间选择算法隔离最佳聚类空间，同时评估聚类稳定性来识别受污染集群，从而提升了对不同攻击的可分离性。实验结果表明，FLARE在基准数据集上有效对抗22种后门攻击，包括all-to-one(A2O)、all-to-all(A2A)和untargeted(UT)攻击，并对自适应攻击表现出鲁棒性。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "13 pages",
      "pdf_url": "http://arxiv.org/pdf/2411.19479v1",
      "published_date": "2024-11-29 05:34:21 UTC",
      "updated_date": "2024-11-29 05:34:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:51:14.765139"
    },
    {
      "arxiv_id": "2411.19477v4",
      "title": "Simple and Provable Scaling Laws for the Test-Time Compute of Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yanxi Chen",
        "Xuchen Pan",
        "Yaliang Li",
        "Bolin Ding",
        "Jingren Zhou"
      ],
      "abstract": "We propose two simple, principled and practical algorithms that enjoy\nprovable scaling laws for the test-time compute of large language models\n(LLMs). The first one is a two-stage knockout-style algorithm: given an input\nproblem, it first generates multiple candidate solutions, and then aggregate\nthem via a knockout tournament for the final output. Assuming that the LLM can\ngenerate a correct solution with non-zero probability and do better than a\nrandom guess in comparing a pair of correct and incorrect solutions, we prove\ntheoretically that the failure probability of this algorithm decays to zero\nexponentially or by a power law (depending on the specific way of scaling) as\nits test-time compute grows. The second one is a two-stage league-style\nalgorithm, where each candidate is evaluated by its average win rate against\nmultiple opponents, rather than eliminated upon loss to a single opponent.\nUnder analogous but more robust assumptions, we prove that its failure\nprobability also decays to zero exponentially with more test-time compute. Both\nalgorithms require a black-box LLM and nothing else (e.g., no verifier or\nreward model) for a minimalistic implementation, which makes them appealing for\npractical applications and easy to adapt for different tasks. Through extensive\nexperiments with diverse models and datasets, we validate the proposed theories\nand demonstrate the outstanding scaling properties of both algorithms.",
      "tldr_zh": "这篇论文提出两个简单且可证明的算法，用于大型语言模型 (LLMs) 的测试时计算 (test-time compute)，以提升模型性能的 scaling laws。第一个算法采用两阶段淘汰式方法：生成多个候选解决方案，然后通过淘汰赛聚合输出，并在假设 LLM 以非零概率生成正确方案并优于随机比较的情况下，证明失败概率随计算资源增长呈指数或幂律衰减。第二个算法是两阶段联赛式方法，通过评估每个候选方案的平均胜率来避免单一失败，确保在更稳健假设下，失败概率也呈指数衰减。实验在多种模型和数据集上验证了这些理论，展示了算法的出色缩放性能和实用性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19477v4",
      "published_date": "2024-11-29 05:29:47 UTC",
      "updated_date": "2025-05-19 12:12:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:51:26.110895"
    },
    {
      "arxiv_id": "2411.19475v1",
      "title": "Effective Fine-Tuning of Vision-Language Models for Accurate Galaxy Morphology Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Ruoqi Wang",
        "Haitao Wang",
        "Qiong Luo"
      ],
      "abstract": "Galaxy morphology analysis involves classifying galaxies by their shapes and\nstructures. For this task, directly training domain-specific models on large,\nannotated astronomical datasets is effective but costly. In contrast,\nfine-tuning vision foundation models on a smaller set of astronomical images is\nmore resource-efficient but generally results in lower accuracy. To harness the\nbenefits of both approaches and address their shortcomings, we propose\nGalaxAlign, a novel method that fine-tunes pre-trained foundation models to\nachieve high accuracy on astronomical tasks. Specifically, our method extends a\ncontrastive learning architecture to align three types of data in fine-tuning:\n(1) a set of schematic symbols representing galaxy shapes and structures, (2)\ntextual labels of these symbols, and (3) galaxy images. This way, GalaxAlign\nnot only eliminates the need for expensive pretraining but also enhances the\neffectiveness of fine-tuning. Extensive experiments on galaxy classification\nand similarity search demonstrate that our method effectively fine-tunes\ngeneral pre-trained models for astronomical tasks by incorporating\ndomain-specific multi-modal knowledge.",
      "tldr_zh": "该研究针对星系形态分析的挑战，提出了一种名为 GalaxAlign 的新方法，用于高效微调 Vision-Language Models，以提高分类准确性。GalaxAlign 通过扩展对比学习（contrastive learning）架构，对齐三种数据：代表星系形状的示意图符号、这些符号的文本标签，以及实际星系图像，从而避免昂贵的预训练并增强微调效果。实验结果显示，该方法在星系分类和相似性搜索任务上显著提升了通用预训练模型的表现，成功整合了领域特定的多模态知识。",
      "categories": [
        "cs.CV",
        "astro-ph.GA",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19475v1",
      "published_date": "2024-11-29 05:10:47 UTC",
      "updated_date": "2024-11-29 05:10:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:51:37.148211"
    },
    {
      "arxiv_id": "2411.19463v1",
      "title": "Towards Understanding Retrieval Accuracy and Prompt Quality in RAG Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Shengming Zhao",
        "Yuheng Huang",
        "Jiayang Song",
        "Zhijie Wang",
        "Chengcheng Wan",
        "Lei Ma"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) is a pivotal technique for enhancing the\ncapability of large language models (LLMs) and has demonstrated promising\nefficacy across a diverse spectrum of tasks. While LLM-driven RAG systems show\nsuperior performance, they face unique challenges in stability and reliability.\nTheir complexity hinders developers' efforts to design, maintain, and optimize\neffective RAG systems. Therefore, it is crucial to understand how RAG's\nperformance is impacted by its design. In this work, we conduct an early\nexploratory study toward a better understanding of the mechanism of RAG\nsystems, covering three code datasets, three QA datasets, and two LLMs. We\nfocus on four design factors: retrieval document type, retrieval recall,\ndocument selection, and prompt techniques. Our study uncovers how each factor\nimpacts system correctness and confidence, providing valuable insights for\ndeveloping an accurate and reliable RAG system. Based on these findings, we\npresent nine actionable guidelines for detecting defects and optimizing the\nperformance of RAG systems. We hope our early exploration can inspire further\nadvancements in engineering, improving and maintaining LLM-driven intelligent\nsoftware systems for greater efficiency and reliability.",
      "tldr_zh": "本研究探讨了Retrieval-Augmented Generation (RAG) 系统在提升大型语言模型 (LLMs) 能力方面的表现，同时分析了其稳定性和可靠性面临的挑战。通过使用三个代码数据集、三个 QA 数据集和两个 LLMs，该研究考察了四个关键设计因素：retrieval document type、retrieval recall、document selection 和 prompt techniques。结果显示，这些因素对系统正确性和置信度有显著影响，并基于此提出了九条行动指南，以帮助开发者优化 RAG 系统的性能和可靠性。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19463v1",
      "published_date": "2024-11-29 04:25:31 UTC",
      "updated_date": "2024-11-29 04:25:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:51:48.653869"
    },
    {
      "arxiv_id": "2411.19460v1",
      "title": "Look Every Frame All at Once: Video-Ma$^2$mba for Efficient Long-form Video Understanding with Multi-Axis Gradient Checkpointing",
      "title_zh": "翻译失败",
      "authors": [
        "Hosu Lee",
        "Junho Kim",
        "Hyunjun Kim",
        "Yong Man Ro"
      ],
      "abstract": "With the growing scale and complexity of video data, efficiently processing\nlong video sequences poses significant challenges due to the quadratic increase\nin memory and computational demands associated with existing transformer-based\nLarge Multi-modal Models (LMMs). To address these issues, we introduce\nVideo-Ma$^2$mba, a novel architecture that incorporates State Space Models\n(SSMs) within the Mamba-2 framework, replacing the attention mechanisms. This\nallows the LMMs to scale linearly in terms of time and memory requirements,\nmaking it feasible to handle long-duration video content. Furthermore, we\nenhance the memory efficiency introducing the Multi-Axis Gradient Checkpointing\n(MA-GC) method, which strategically manages memory by retaining only essential\nactivations across multiple computational axes. Our approach significantly\nreduces the memory footprint compared to standard gradient checkpointing.\nEmpirical analyses show that Video-Ma$^2$mba can process extensive video\nsequences-equivalent to millions of tokens or over two hours of continuous\nsequences at 1 FPS-on a single GPU. By maintaining a detailed capture of\ntemporal dynamics, our model improves the accuracy and relevance of responses\nin long video understanding tasks, demonstrating substantial advantages over\nexisting frameworks.",
      "tldr_zh": "该研究针对基于 Transformer 的 Large Multi-modal Models (LMMs) 在处理长视频序列时面临的内存和计算需求的二次方增长问题，提出了一种高效架构 Video-Ma²mba。该架构利用 State Space Models (SSMs) 替换注意力机制，基于 Mamba-2 框架，实现时间和内存需求的线性扩展，从而能处理数百万标记的视频序列（相当于超过两小时的连续内容）。此外，引入 Multi-Axis Gradient Checkpointing (MA-GC) 方法，通过仅保留必要的激活来进一步优化内存使用。实验结果显示，Video-Ma²mba 在单个 GPU 上显著提升了长视频理解任务的准确性和相关性，优于现有框架。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://ivy-lvlm.github.io/Video-MA2MBA/",
      "pdf_url": "http://arxiv.org/pdf/2411.19460v1",
      "published_date": "2024-11-29 04:12:13 UTC",
      "updated_date": "2024-11-29 04:12:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:52:01.647831"
    },
    {
      "arxiv_id": "2411.19456v1",
      "title": "Beyond Surface Structure: A Causal Assessment of LLMs' Comprehension Ability",
      "title_zh": "翻译失败",
      "authors": [
        "Yujin Han",
        "Lei Xu",
        "Sirui Chen",
        "Difan Zou",
        "Chaochao Lu"
      ],
      "abstract": "Large language models (LLMs) have shown remarkable capability in natural\nlanguage tasks, yet debate persists on whether they truly comprehend deep\nstructure (i.e., core semantics) or merely rely on surface structure (e.g.,\npresentation format). Prior studies observe that LLMs' performance declines\nwhen intervening on surface structure, arguing their success relies on surface\nstructure recognition. However, surface structure sensitivity does not prevent\ndeep structure comprehension. Rigorously evaluating LLMs' capability requires\nanalyzing both, yet deep structure is often overlooked. To this end, we assess\nLLMs' comprehension ability using causal mediation analysis, aiming to fully\ndiscover the capability of using both deep and surface structures.\nSpecifically, we formulate the comprehension of deep structure as direct causal\neffect (DCE) and that of surface structure as indirect causal effect (ICE),\nrespectively. To address the non-estimability of original DCE and ICE --\nstemming from the infeasibility of isolating mutual influences of deep and\nsurface structures, we develop the corresponding quantifiable surrogates,\nincluding approximated DCE (ADCE) and approximated ICE (AICE). We further apply\nthe ADCE to evaluate a series of mainstream LLMs, showing that most of them\nexhibit deep structure comprehension ability, which grows along with the\nprediction accuracy. Comparing ADCE and AICE demonstrates closed-source LLMs\nrely more on deep structure, while open-source LLMs are more surface-sensitive,\nwhich decreases with model scale. Theoretically, ADCE is a bidirectional\nevaluation, which measures both the sufficiency and necessity of deep structure\nchanges in causing output variations, thus offering a more comprehensive\nassessment than accuracy, a common evaluation in LLMs. Our work provides new\ninsights into LLMs' deep structure comprehension and offers novel methods for\nLLMs evaluation.",
      "tldr_zh": "这篇论文使用因果中介分析评估大型语言模型 (LLMs) 的理解能力，探讨它们是否依赖表面结构（如呈现格式）还是真正理解深层结构（如核心语义）。作者定义了直接因果效应 (DCE) 来量化深层结构理解，以及间接因果效应 (ICE) 来量化表面结构影响，并开发了近似版本 ADCE 和 AICE，以解决这些效应的不可直接估算问题。实验结果显示，大多数 LLMs 展现出深层结构理解能力，且这种能力随预测准确率提升而增强；此外，闭源 LLMs 更依赖深层结构，而开源 LLMs 对表面结构的敏感性随模型规模减少。该方法作为双向评估，比传统准确率更全面，为 LLMs 能力评估提供了新工具和见解。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "28 pages, 14 figures, 10 tables",
      "pdf_url": "http://arxiv.org/pdf/2411.19456v1",
      "published_date": "2024-11-29 03:57:26 UTC",
      "updated_date": "2024-11-29 03:57:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:52:15.069272"
    },
    {
      "arxiv_id": "2411.19451v1",
      "title": "Learning Visual Abstract Reasoning through Dual-Stream Networks",
      "title_zh": "通过双流",
      "authors": [
        "Kai Zhao",
        "Chang Xu",
        "Bailu Si"
      ],
      "abstract": "Visual abstract reasoning tasks present challenges for deep neural networks,\nexposing limitations in their capabilities. In this work, we present a neural\nnetwork model that addresses the challenges posed by Raven's Progressive\nMatrices (RPM). Inspired by the two-stream hypothesis of visual processing, we\nintroduce the Dual-stream Reasoning Network (DRNet), which utilizes two\nparallel branches to capture image features. On top of the two streams, a\nreasoning module first learns to merge the high-level features of the same\nimage. Then, it employs a rule extractor to handle combinations involving the\neight context images and each candidate image, extracting discrete abstract\nrules and utilizing an multilayer perceptron (MLP) to make predictions.\nEmpirical results demonstrate that the proposed DRNet achieves state-of-the-art\naverage performance across multiple RPM benchmarks. Furthermore, DRNet\ndemonstrates robust generalization capabilities, even extending to various\nout-of-distribution scenarios. The dual streams within DRNet serve distinct\nfunctions by addressing local or spatial information. They are then integrated\ninto the reasoning module, leveraging abstract rules to facilitate the\nexecution of visual reasoning tasks. These findings indicate that the\ndual-stream architecture could play a crucial role in visual abstract\nreasoning.",
      "tldr_zh": "这篇论文提出了一种名为 Dual-stream Reasoning Network (DRNet) 的神经网络模型，用于解决深度神经网络在视觉抽象推理任务中的局限性，特别是针对 Raven's Progressive Matrices (RPM)。DRNet 基于 two-stream hypothesis，采用两个并行分支捕获图像特征，然后通过一个推理模块合并高阶特征、提取抽象规则，并利用 multilayer perceptron (MLP) 进行预测。实验结果显示，DRNet 在多个 RPM 基准上达到了 state-of-the-art 性能，并展现出强大的泛化能力，即使在 out-of-distribution 场景中也能有效，证明了双流架构在视觉抽象推理中的关键作用。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.19451v1",
      "published_date": "2024-11-29 03:25:32 UTC",
      "updated_date": "2024-11-29 03:25:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:52:25.605004"
    },
    {
      "arxiv_id": "2411.19447v1",
      "title": "Adaptive Interactive Segmentation for Multimodal Medical Imaging via Selection Engine",
      "title_zh": "翻译失败",
      "authors": [
        "Zhi Li",
        "Kai Zhao",
        "Yaqi Wang",
        "Shuai Wang"
      ],
      "abstract": "In medical image analysis, achieving fast, efficient, and accurate\nsegmentation is essential for automated diagnosis and treatment. Although\nrecent advancements in deep learning have significantly improved segmentation\naccuracy, current models often face challenges in adaptability and\ngeneralization, particularly when processing multi-modal medical imaging data.\nThese limitations stem from the substantial variations between imaging\nmodalities and the inherent complexity of medical data. To address these\nchallenges, we propose the Strategy-driven Interactive Segmentation Model\n(SISeg), built on SAM2, which enhances segmentation performance across various\nmedical imaging modalities by integrating a selection engine. To mitigate\nmemory bottlenecks and optimize prompt frame selection during the inference of\n2D image sequences, we developed an automated system, the Adaptive Frame\nSelection Engine (AFSE). This system dynamically selects the optimal prompt\nframes without requiring extensive prior medical knowledge and enhances the\ninterpretability of the model's inference process through an interactive\nfeedback mechanism. We conducted extensive experiments on 10 datasets covering\n7 representative medical imaging modalities, demonstrating the SISeg model's\nrobust adaptability and generalization in multi-modal tasks. The project page\nand code will be available at: [URL].",
      "tldr_zh": "该研究针对多模态医疗图像分割的适应性和泛化挑战，提出了一种基于 SAM2 的 Strategy-driven Interactive Segmentation Model (SISeg)，通过集成 Adaptive Frame Selection Engine (AFSE) 来动态选择最佳提示帧，减少内存瓶颈并提升模型解释性。AFSE 系统无需大量先验医疗知识，即可优化2D图像序列的推理过程，并支持交互反馈机制。实验在涵盖7种医疗图像模态的10个数据集上验证了 SISeg 的鲁棒适应性和泛化能力，为自动化诊断和治疗提供更高效的分割工具。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19447v1",
      "published_date": "2024-11-29 03:08:28 UTC",
      "updated_date": "2024-11-29 03:08:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:52:36.829042"
    },
    {
      "arxiv_id": "2411.19440v1",
      "title": "Gradient Inversion Attack on Graph Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Divya Anand Sinha",
        "Yezi Liu",
        "Ruijie Du",
        "Yanning Shen"
      ],
      "abstract": "Graph federated learning is of essential importance for training over large\ngraph datasets while protecting data privacy, where each client stores a subset\nof local graph data, while the server collects the local gradients and\nbroadcasts only the aggregated gradients. Recent studies reveal that a\nmalicious attacker can steal private image data from gradient exchanging of\nneural networks during federated learning. However, none of the existing works\nhave studied the vulnerability of graph data and graph neural networks under\nsuch attack. To answer this question, the present paper studies the problem of\nwhether private data can be recovered from leaked gradients in both node\nclassification and graph classification tasks and { proposes a novel attack\nnamed Graph Leakage from Gradients (GLG)}. Two widely-used GNN frameworks are\nanalyzed, namely GCN and GraphSAGE. The effects of different model settings on\nrecovery are extensively discussed. Through theoretical analysis and empirical\nvalidation, it is shown that parts of the graph data can be leaked from the\ngradients.",
      "tldr_zh": "该研究探讨了图联邦学习中Graph Neural Networks (GNNs)的隐私漏洞，攻击者可能通过梯度信息恢复私有图数据。论文提出了一种名为Graph Leakage from Gradients (GLG)的创新攻击方法，针对GCN和GraphSAGE等框架，并分析了不同模型设置的影响。通过理论分析和实验验证，结果显示攻击能成功泄露部分图数据，暴露了图联邦学习的潜在风险。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.19440v1",
      "published_date": "2024-11-29 02:42:17 UTC",
      "updated_date": "2024-11-29 02:42:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:52:49.409983"
    },
    {
      "arxiv_id": "2411.19418v2",
      "title": "Proto Successor Measure: Representing the Behavior Space of an RL Agent",
      "title_zh": "翻译失败",
      "authors": [
        "Siddhant Agarwal",
        "Harshit Sikchi",
        "Peter Stone",
        "Amy Zhang"
      ],
      "abstract": "Having explored an environment, intelligent agents should be able to transfer\ntheir knowledge to most downstream tasks within that environment without\nadditional interactions. Referred to as \"zero-shot learning\", this ability\nremains elusive for general-purpose reinforcement learning algorithms. While\nrecent works have attempted to produce zero-shot RL agents, they make\nassumptions about the nature of the tasks or the structure of the MDP. We\npresent Proto Successor Measure: the basis set for all possible behaviors of a\nReinforcement Learning Agent in a dynamical system. We prove that any possible\nbehavior (represented using visitation distributions) can be represented using\nan affine combination of these policy-independent basis functions. Given a\nreward function at test time, we simply need to find the right set of linear\nweights to combine these bases corresponding to the optimal policy. We derive a\npractical algorithm to learn these basis functions using reward-free\ninteraction data from the environment and show that our approach can produce\nthe optimal policy at test time for any given reward function without\nadditional environmental interactions. Project page:\nhttps://agarwalsiddhant10.github.io/projects/psm.html.",
      "tldr_zh": "本论文提出了Proto Successor Measure，这是一种表示强化学习（RL）代理在动态系统中的所有可能行为的基底集合，旨在实现零样本学习（zero-shot learning），让代理无需额外互动即可转移知识到下游任务。作者证明了任何行为（用visitation distributions表示）都可以通过这些政策无关的基函数的仿射组合来表示，并在测试时仅需计算线性权重来生成针对任意奖励函数的最优策略。他们开发了一个实用算法，使用reward-free interaction data从环境中学习这些基函数，实验结果显示该方法能有效产生最优策略，而无需进一步环境互动。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Under submission, 20 pages",
      "pdf_url": "http://arxiv.org/pdf/2411.19418v2",
      "published_date": "2024-11-29 00:09:39 UTC",
      "updated_date": "2025-03-11 17:41:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T05:53:01.197567"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 88,
  "processed_papers_count": 88,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-21T05:53:20.586299"
}
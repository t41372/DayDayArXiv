[
  {
    "arxiv_id": "2408.09605v1",
    "title": "Does Thought Require Sensory Grounding? From Pure Thinkers to Large Language Models",
    "authors": [
      "David J. Chalmers"
    ],
    "abstract": "Does the capacity to think require the capacity to sense? A lively debate on\nthis topic runs throughout the history of philosophy and now animates\ndiscussions of artificial intelligence. I argue that in principle, there can be\npure thinkers: thinkers that lack the capacity to sense altogether. I also\nargue for significant limitations in just what sort of thought is possible in\nthe absence of the capacity to sense. Regarding AI, I do not argue directly\nthat large language models can think or understand, but I rebut one important\nargument (the argument from sensory grounding) that they cannot. I also use\nrecent results regarding language models to address the question of whether or\nhow sensory grounding enhances cognitive capacities.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09605v1",
    "published_date": "2024-08-18 22:18:29 UTC",
    "updated_date": "2024-08-18 22:18:29 UTC"
  },
  {
    "arxiv_id": "2408.09600v2",
    "title": "Antidote: Post-fine-tuning Safety Alignment for Large Language Models against Harmful Fine-tuning",
    "authors": [
      "Tiansheng Huang",
      "Gautam Bhattacharya",
      "Pratik Joshi",
      "Josh Kimball",
      "Ling Liu"
    ],
    "abstract": "Safety aligned Large Language Models (LLMs) are vulnerable to harmful\nfine-tuning attacks \\cite{qi2023fine}-- a few harmful data mixed in the\nfine-tuning dataset can break the LLMs's safety alignment. Existing mitigation\nstrategies include alignment stage solutions \\cite{huang2024vaccine,\nrosati2024representation} and fine-tuning stage solutions\n\\cite{huang2024lazy,mukhoti2023fine}. However, our evaluation shows that both\ncategories of defenses fail \\textit{when some specific training\nhyper-parameters are chosen} -- a large learning rate or a large number of\ntraining epochs in the fine-tuning stage can easily invalidate the defense,\nwhich however, is necessary to guarantee finetune performance. To this end, we\npropose Antidote, a post-fine-tuning stage solution, which remains\n\\textbf{\\textit{agnostic to the training hyper-parameters in the fine-tuning\nstage}}. Antidote relies on the philosophy that by removing the harmful\nparameters, the harmful model can be recovered from the harmful behaviors,\nregardless of how those harmful parameters are formed in the fine-tuning stage.\nWith this philosophy, we introduce a one-shot pruning stage after harmful\nfine-tuning to remove the harmful weights that are responsible for the\ngeneration of harmful content. Despite its embarrassing simplicity, empirical\nresults show that Antidote can reduce harmful score while maintaining accuracy\non downstream tasks.Our project page is at\n\\url{https://huangtiansheng.github.io/Antidote_gh_page/}",
    "categories": [
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09600v2",
    "published_date": "2024-08-18 21:45:03 UTC",
    "updated_date": "2024-09-03 03:45:21 UTC"
  },
  {
    "arxiv_id": "2408.09594v3",
    "title": "Moonshine: Distilling Game Content Generators into Steerable Generative Models",
    "authors": [
      "Yuhe Nie",
      "Michael Middleton",
      "Tim Merino",
      "Nidhushan Kanagaraja",
      "Ashutosh Kumar",
      "Zhan Zhuang",
      "Julian Togelius"
    ],
    "abstract": "Procedural Content Generation via Machine Learning (PCGML) has enhanced game\ncontent creation, yet challenges in controllability and limited training data\npersist. This study addresses these issues by distilling a constructive PCG\nalgorithm into a controllable PCGML model. We first generate a large amount of\ncontent with a constructive algorithm and label it using a Large Language Model\n(LLM). We use these synthetic labels to condition two PCGML models for\ncontent-specific generation, a diffusion model and the five-dollar model. This\nneural network distillation process ensures that the generation aligns with the\noriginal algorithm while introducing controllability through plain text. We\ndefine this text-conditioned PCGML as a Text-to-game-Map (T2M) task, offering\nan alternative to prevalent text-to-image multi-modal tasks. We compare our\ndistilled models with the baseline constructive algorithm. Our analysis of the\nvariety, accuracy, and quality of our generation demonstrates the efficacy of\ndistilling constructive methods into controllable text-conditioned PCGML\nmodels.",
    "categories": [
      "cs.AI",
      "I.2.1"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09594v3",
    "published_date": "2024-08-18 20:59:59 UTC",
    "updated_date": "2025-02-02 18:40:22 UTC"
  },
  {
    "arxiv_id": "2408.09588v1",
    "title": "SynTraC: A Synthetic Dataset for Traffic Signal Control from Traffic Monitoring Cameras",
    "authors": [
      "Tiejin Chen",
      "Prithvi Shirke",
      "Bharatesh Chakravarthi",
      "Arpitsinh Vaghela",
      "Longchao Da",
      "Duo Lu",
      "Yezhou Yang",
      "Hua Wei"
    ],
    "abstract": "This paper introduces SynTraC, the first public image-based traffic signal\ncontrol dataset, aimed at bridging the gap between simulated environments and\nreal-world traffic management challenges. Unlike traditional datasets for\ntraffic signal control which aim to provide simplified feature vectors like\nvehicle counts from traffic simulators, SynTraC provides real-style images from\nthe CARLA simulator with annotated features, along with traffic signal states.\nThis image-based dataset comes with diverse real-world scenarios, including\nvarying weather and times of day. Additionally, SynTraC also provides different\nreward values for advanced traffic signal control algorithms like reinforcement\nlearning. Experiments with SynTraC demonstrate that it is still an open\nchallenge to image-based traffic signal control methods compared with\nfeature-based control methods, indicating our dataset can further guide the\ndevelopment of future algorithms. The code for this paper can be found in\n\\url{https://github.com/DaRL-LibSignal/SynTraC}.SynTraC",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to IEEE ITSC2024",
    "pdf_url": "http://arxiv.org/pdf/2408.09588v1",
    "published_date": "2024-08-18 20:20:43 UTC",
    "updated_date": "2024-08-18 20:20:43 UTC"
  },
  {
    "arxiv_id": "2408.09570v1",
    "title": "Say My Name: a Model's Bias Discovery Framework",
    "authors": [
      "Massimiliano Ciranni",
      "Luca Molinaro",
      "Carlo Alberto Barbano",
      "Attilio Fiandrotti",
      "Vittorio Murino",
      "Vito Paolo Pastore",
      "Enzo Tartaglione"
    ],
    "abstract": "In the last few years, due to the broad applicability of deep learning to\ndownstream tasks and end-to-end training capabilities, increasingly more\nconcerns about potential biases to specific, non-representative patterns have\nbeen raised. Many works focusing on unsupervised debiasing usually leverage the\ntendency of deep models to learn ``easier'' samples, for example by clustering\nthe latent space to obtain bias pseudo-labels. However, the interpretation of\nsuch pseudo-labels is not trivial, especially for a non-expert end user, as it\ndoes not provide semantic information about the bias features. To address this\nissue, we introduce ``Say My Name'' (SaMyNa), the first tool to identify biases\nwithin deep models semantically. Unlike existing methods, our approach focuses\non biases learned by the model. Our text-based pipeline enhances explainability\nand supports debiasing efforts: applicable during either training or post-hoc\nvalidation, our method can disentangle task-related information and proposes\nitself as a tool to analyze biases. Evaluation on traditional benchmarks\ndemonstrates its effectiveness in detecting biases and even disclaiming them,\nshowcasing its broad applicability for model diagnosis.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09570v1",
    "published_date": "2024-08-18 18:50:59 UTC",
    "updated_date": "2024-08-18 18:50:59 UTC"
  },
  {
    "arxiv_id": "2408.09568v2",
    "title": "MergeRepair: An Exploratory Study on Merging Task-Specific Adapters in Code LLMs for Automated Program Repair",
    "authors": [
      "Meghdad Dehghan",
      "Jie JW Wu",
      "Fatemeh H. Fard",
      "Ali Ouni"
    ],
    "abstract": "[Context] Large Language Models (LLMs) have shown good performance in several\nsoftware development-related tasks such as program repair, documentation, code\nrefactoring, debugging, and testing. Adapters are specialized, small modules\ndesigned for parameter efficient fine-tuning of LLMs for specific tasks,\ndomains, or applications without requiring extensive retraining of the entire\nmodel. These adapters offer a more efficient way to customize LLMs for\nparticular needs, leveraging the pre-existing capabilities of the large model.\nMerging LLMs and adapters has shown promising results for various natural\nlanguage domains and tasks, enabling the use of the learned models and adapters\nwithout additional training for a new task. [Objective] This research proposes\ncontinual merging and empirically studies the capabilities of merged adapters\nin Code LLMs, specially for the Automated Program Repair (APR) task. The goal\nis to gain insights into whether and how merging task-specific adapters can\naffect the performance of APR. [Method] In our framework, MergeRepair, we plan\nto merge multiple task-specific adapters using three different merging methods\nand evaluate the performance of the merged adapter for the APR task.\nParticularly, we will employ two main merging scenarios for all three\ntechniques, (i) merging using equal-weight averaging applied on parameters of\ndifferent adapters, where all adapters are of equal importance; and (ii) our\nproposed approach, continual merging, in which we sequentially merge the\ntask-specific adapters and the order and weight of merged adapters matter. By\nexploratory study of merging techniques, we will investigate the improvement\nand generalizability of merged adapters for APR. Through continual merging, we\nwill explore the capability of merged adapters and the effect of task order, as\nit occurs in real-world software projects.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09568v2",
    "published_date": "2024-08-18 18:45:48 UTC",
    "updated_date": "2024-08-26 19:27:46 UTC"
  },
  {
    "arxiv_id": "2408.09565v1",
    "title": "Grammatical Error Feedback: An Implicit Evaluation Approach",
    "authors": [
      "Stefano Bannò",
      "Kate Knill",
      "Mark J. F. Gales"
    ],
    "abstract": "Grammatical feedback is crucial for consolidating second language (L2)\nlearning. Most research in computer-assisted language learning has focused on\nfeedback through grammatical error correction (GEC) systems, rather than\nexamining more holistic feedback that may be more useful for learners. This\nholistic feedback will be referred to as grammatical error feedback (GEF). In\nthis paper, we present a novel implicit evaluation approach to GEF that\neliminates the need for manual feedback annotations. Our method adopts a\ngrammatical lineup approach where the task is to pair feedback and essay\nrepresentations from a set of possible alternatives. This matching process can\nbe performed by appropriately prompting a large language model (LLM). An\nimportant aspect of this process, explored here, is the form of the lineup,\ni.e., the selection of foils. This paper exploits this framework to examine the\nquality and need for GEC to generate feedback, as well as the system used to\ngenerate feedback, using essays from the Cambridge Learner Corpus.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09565v1",
    "published_date": "2024-08-18 18:31:55 UTC",
    "updated_date": "2024-08-18 18:31:55 UTC"
  },
  {
    "arxiv_id": "2408.09559v1",
    "title": "HiAgent: Hierarchical Working Memory Management for Solving Long-Horizon Agent Tasks with Large Language Model",
    "authors": [
      "Mengkang Hu",
      "Tianxing Chen",
      "Qiguang Chen",
      "Yao Mu",
      "Wenqi Shao",
      "Ping Luo"
    ],
    "abstract": "Large Language Model (LLM)-based agents exhibit significant potential across\nvarious domains, operating as interactive systems that process environmental\nobservations to generate executable actions for target tasks. The effectiveness\nof these agents is significantly influenced by their memory mechanism, which\nrecords historical experiences as sequences of action-observation pairs. We\ncategorize memory into two types: cross-trial memory, accumulated across\nmultiple attempts, and in-trial memory (working memory), accumulated within a\nsingle attempt. While considerable research has optimized performance through\ncross-trial memory, the enhancement of agent performance through improved\nworking memory utilization remains underexplored. Instead, existing approaches\noften involve directly inputting entire historical action-observation pairs\ninto LLMs, leading to redundancy in long-horizon tasks. Inspired by human\nproblem-solving strategies, this paper introduces HiAgent, a framework that\nleverages subgoals as memory chunks to manage the working memory of LLM-based\nagents hierarchically. Specifically, HiAgent prompts LLMs to formulate subgoals\nbefore generating executable actions and enables LLMs to decide proactively to\nreplace previous subgoals with summarized observations, retaining only the\naction-observation pairs relevant to the current subgoal. Experimental results\nacross five long-horizon tasks demonstrate that HiAgent achieves a twofold\nincrease in success rate and reduces the average number of steps required by\n3.8. Additionally, our analysis shows that HiAgent consistently improves\nperformance across various steps, highlighting its robustness and\ngeneralizability. Project Page: https://github.com/HiAgent2024/HiAgent .",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CL",
    "comment": "Project Page: https://github.com/HiAgent2024/HiAgent",
    "pdf_url": "http://arxiv.org/pdf/2408.09559v1",
    "published_date": "2024-08-18 17:59:49 UTC",
    "updated_date": "2024-08-18 17:59:49 UTC"
  },
  {
    "arxiv_id": "2408.09556v1",
    "title": "Addressing Heterogeneity in Federated Learning: Challenges and Solutions for a Shared Production Environment",
    "authors": [
      "Tatjana Legler",
      "Vinit Hegiste",
      "Ahmed Anwar",
      "Martin Ruskowski"
    ],
    "abstract": "Federated learning (FL) has emerged as a promising approach to training\nmachine learning models across decentralized data sources while preserving data\nprivacy, particularly in manufacturing and shared production environments.\nHowever, the presence of data heterogeneity variations in data distribution,\nquality, and volume across different or clients and production sites, poses\nsignificant challenges to the effectiveness and efficiency of FL. This paper\nprovides a comprehensive overview of heterogeneity in FL within the context of\nmanufacturing, detailing the types and sources of heterogeneity, including\nnon-independent and identically distributed (non-IID) data, unbalanced data,\nvariable data quality, and statistical heterogeneity. We discuss the impact of\nthese types of heterogeneity on model training and review current methodologies\nfor mitigating their adverse effects. These methodologies include personalized\nand customized models, robust aggregation techniques, and client selection\ntechniques. By synthesizing existing research and proposing new strategies,\nthis paper aims to provide insight for effectively managing data heterogeneity\nin FL, enhancing model robustness, and ensuring fair and efficient training\nacross diverse environments. Future research directions are also identified,\nhighlighting the need for adaptive and scalable solutions to further improve\nthe FL paradigm in the context of Industry 4.0.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09556v1",
    "published_date": "2024-08-18 17:49:44 UTC",
    "updated_date": "2024-08-18 17:49:44 UTC"
  },
  {
    "arxiv_id": "2408.09540v1",
    "title": "Using ChatGPT to Score Essays and Short-Form Constructed Responses",
    "authors": [
      "Mark D. Shermis"
    ],
    "abstract": "This study aimed to determine if ChatGPT's large language models could match\nthe scoring accuracy of human and machine scores from the ASAP competition. The\ninvestigation focused on various prediction models, including linear\nregression, random forest, gradient boost, and boost. ChatGPT's performance was\nevaluated against human raters using quadratic weighted kappa (QWK) metrics.\nResults indicated that while ChatGPT's gradient boost model achieved QWKs close\nto human raters for some data sets, its overall performance was inconsistent\nand often lower than human scores. The study highlighted the need for further\nrefinement, particularly in handling biases and ensuring scoring fairness.\nDespite these challenges, ChatGPT demonstrated potential for scoring\nefficiency, especially with domain-specific fine-tuning. The study concludes\nthat ChatGPT can complement human scoring but requires additional development\nto be reliable for high-stakes assessments. Future research should improve\nmodel accuracy, address ethical considerations, and explore hybrid models\ncombining ChatGPT with empirical methods.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.7; I.3"
    ],
    "primary_category": "cs.CL",
    "comment": "35 pages, 8 tables, 2 Figures, 27 references",
    "pdf_url": "http://arxiv.org/pdf/2408.09540v1",
    "published_date": "2024-08-18 16:51:28 UTC",
    "updated_date": "2024-08-18 16:51:28 UTC"
  },
  {
    "arxiv_id": "2408.09530v1",
    "title": "PA-LLaVA: A Large Language-Vision Assistant for Human Pathology Image Understanding",
    "authors": [
      "Dawei Dai",
      "Yuanhui Zhang",
      "Long Xu",
      "Qianlan Yang",
      "Xiaojing Shen",
      "Shuyin Xia",
      "Guoyin Wang"
    ],
    "abstract": "The previous advancements in pathology image understanding primarily involved\ndeveloping models tailored to specific tasks. Recent studies has demonstrated\nthat the large vision-language model can enhance the performance of various\ndownstream tasks in medical image understanding. In this study, we developed a\ndomain-specific large language-vision assistant (PA-LLaVA) for pathology image\nunderstanding. Specifically, (1) we first construct a human pathology\nimage-text dataset by cleaning the public medical image-text data for\ndomain-specific alignment; (2) Using the proposed image-text data, we first\ntrain a pathology language-image pretraining (PLIP) model as the specialized\nvisual encoder for pathology image, and then we developed scale-invariant\nconnector to avoid the information loss caused by image scaling; (3) We adopt\ntwo-stage learning to train PA-LLaVA, first stage for domain alignment, and\nsecond stage for end to end visual question \\& answering (VQA) task. In\nexperiments, we evaluate our PA-LLaVA on both supervised and zero-shot VQA\ndatasets, our model achieved the best overall performance among multimodal\nmodels of similar scale. The ablation experiments also confirmed the\neffectiveness of our design. We posit that our PA-LLaVA model and the datasets\npresented in this work can promote research in field of computational\npathology. All codes are available at:\nhttps://github.com/ddw2AIGROUP2CQUPT/PA-LLaVA}{https://github.com/ddw2AIGROUP2CQUPT/PA-LLaVA",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 4 figs",
    "pdf_url": "http://arxiv.org/pdf/2408.09530v1",
    "published_date": "2024-08-18 16:30:32 UTC",
    "updated_date": "2024-08-18 16:30:32 UTC"
  },
  {
    "arxiv_id": "2408.09529v2",
    "title": "Revisiting the Graph Reasoning Ability of Large Language Models: Case Studies in Translation, Connectivity and Shortest Path",
    "authors": [
      "Xinnan Dai",
      "Qihao Wen",
      "Yifei Shen",
      "Hongzhi Wen",
      "Dongsheng Li",
      "Jiliang Tang",
      "Caihua Shan"
    ],
    "abstract": "Large Language Models (LLMs) have achieved great success in various reasoning\ntasks. In this work, we focus on the graph reasoning ability of LLMs. Although\ntheoretical studies proved that LLMs are capable of handling graph reasoning\ntasks, empirical evaluations reveal numerous failures. To deepen our\nunderstanding on this discrepancy, we revisit the ability of LLMs on three\nfundamental graph tasks: graph description translation, graph connectivity, and\nthe shortest-path problem. Our findings suggest that LLMs can fail to\nunderstand graph structures through text descriptions and exhibit varying\nperformance for all these three fundamental tasks. Meanwhile, we perform a\nreal-world investigation on knowledge graphs and make consistent observations\nwith our findings. The codes and datasets are available.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09529v2",
    "published_date": "2024-08-18 16:26:39 UTC",
    "updated_date": "2025-01-07 20:26:34 UTC"
  },
  {
    "arxiv_id": "2408.09527v2",
    "title": "ALS-HAR: Harnessing Wearable Ambient Light Sensors to Enhance IMU-based Human Activity Recogntion",
    "authors": [
      "Lala Shakti Swarup Ray",
      "Daniel Geißler",
      "Mengxi Liu",
      "Bo Zhou",
      "Sungho Suh",
      "Paul Lukowicz"
    ],
    "abstract": "Despite the widespread integration of ambient light sensors (ALS) in smart\ndevices commonly used for screen brightness adaptation, their application in\nhuman activity recognition (HAR), primarily through body-worn ALS, is largely\nunexplored. In this work, we developed ALS-HAR, a robust wearable light-based\nmotion activity classifier. Although ALS-HAR achieves comparable accuracy to\nother modalities, its natural sensitivity to external disturbances, such as\nchanges in ambient light, weather conditions, or indoor lighting, makes it\nchallenging for daily use. To address such drawbacks, we introduce strategies\nto enhance environment-invariant IMU-based activity classifications through\naugmented multi-modal and contrastive classifications by transferring the\nknowledge extracted from the ALS. Our experiments on a real-world activity\ndataset for three different scenarios demonstrate that while ALS-HAR's accuracy\nstrongly relies on external lighting conditions, cross-modal information can\nstill improve other HAR systems, such as IMU-based classifiers.Even in\nscenarios where ALS performs insufficiently, the additional knowledge enables\nimproved accuracy and macro F1 score by up to 4.2 % and 6.4 %, respectively,\nfor IMU-based classifiers and even surpasses multi-modal sensor fusion models\nin two of our three experiment scenarios. Our research highlights the untapped\npotential of ALS integration in advancing sensor-based HAR technology, paving\nthe way for practical and efficient wearable ALS-based activity recognition\nsystems with potential applications in healthcare, sports monitoring, and smart\nindoor environments.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09527v2",
    "published_date": "2024-08-18 16:24:22 UTC",
    "updated_date": "2024-08-22 09:03:13 UTC"
  },
  {
    "arxiv_id": "2408.09523v1",
    "title": "A Unified Framework for Interpretable Transformers Using PDEs and Information Theory",
    "authors": [
      "Yukun Zhang"
    ],
    "abstract": "This paper presents a novel unified theoretical framework for understanding\nTransformer architectures by integrating Partial Differential Equations (PDEs),\nNeural Information Flow Theory, and Information Bottleneck Theory. We model\nTransformer information dynamics as a continuous PDE process, encompassing\ndiffusion, self-attention, and nonlinear residual components. Our comprehensive\nexperiments across image and text modalities demonstrate that the PDE model\neffectively captures key aspects of Transformer behavior, achieving high\nsimilarity (cosine similarity > 0.98) with Transformer attention distributions\nacross all layers. While the model excels in replicating general information\nflow patterns, it shows limitations in fully capturing complex, non-linear\ntransformations. This work provides crucial theoretical insights into\nTransformer mechanisms, offering a foundation for future optimizations in deep\nlearning architectural design. We discuss the implications of our findings,\npotential applications in model interpretability and efficiency, and outline\ndirections for enhancing PDE models to better mimic the intricate behaviors\nobserved in Transformers, paving the way for more transparent and optimized AI\nsystems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09523v1",
    "published_date": "2024-08-18 16:16:57 UTC",
    "updated_date": "2024-08-18 16:16:57 UTC"
  },
  {
    "arxiv_id": "2409.00041v1",
    "title": "Needles in Needle Stacks: Meaningful Clinical Information Buried in Noisy Waveform Data",
    "authors": [
      "Sujay Nagaraj",
      "Andrew J. Goodwin",
      "Dmytro Lopushanskyy",
      "Danny Eytan",
      "Robert W. Greer",
      "Sebastian D. Goodfellow",
      "Azadeh Assadi",
      "Anand Jayarajan",
      "Anna Goldenberg",
      "Mjaye L. Mazwi"
    ],
    "abstract": "Central Venous Lines (C-Lines) and Arterial Lines (A-Lines) are routinely\nused in the Critical Care Unit (CCU) for blood sampling, medication\nadministration, and high-frequency blood pressure measurement. Judiciously\naccessing these lines is important, as over-utilization is associated with\nsignificant in-hospital morbidity and mortality. Documenting the frequency of\nline-access is an important step in reducing these adverse outcomes.\nUnfortunately, the current gold-standard for documentation is manual and\nsubject to error, omission, and bias. The high-frequency blood pressure\nwaveform data from sensors in these lines are often noisy and full of\nartifacts. Standard approaches in signal processing remove noise artifacts\nbefore meaningful analysis. However, from bedside observations, we\ncharacterized a distinct artifact that occurs during each instance of C-Line or\nA-Line use. These artifacts are buried amongst physiological waveform and\nextraneous noise. We focus on Machine Learning (ML) models that can detect\nthese artifacts from waveform data in real-time - finding needles in needle\nstacks, in order to automate the documentation of line-access. We built and\nevaluated ML classifiers running in real-time at a major children's hospital to\nachieve this goal. We demonstrate the utility of these tools for reducing\ndocumentation burden, increasing available information for bedside clinicians,\nand informing unit-level initiatives to improve patient safety.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "Machine Learning For Health Care 2024 (MLHC)",
    "pdf_url": "http://arxiv.org/pdf/2409.00041v1",
    "published_date": "2024-08-18 15:45:11 UTC",
    "updated_date": "2024-08-18 15:45:11 UTC"
  },
  {
    "arxiv_id": "2408.09516v1",
    "title": "A Logic for Policy Based Resource Exchanges in Multiagent Systems",
    "authors": [
      "Lorenzo Ceragioli",
      "Pierpaolo Degano",
      "Letterio Galletta",
      "Luca Viganò"
    ],
    "abstract": "In multiagent systems autonomous agents interact with each other to achieve\nindividual and collective goals. Typical interactions concern negotiation and\nagreement on resource exchanges. Modeling and formalizing these agreements pose\nsignificant challenges, particularly in capturing the dynamic behaviour of\nagents, while ensuring that resources are correctly handled. Here, we propose\nexchange environments as a formal setting where agents specify and obey\nexchange policies, which are declarative statements about what resources they\noffer and what they require in return. Furthermore, we introduce a decidable\nextension of the computational fragment of linear logic as a fundamental tool\nfor representing exchange environments and studying their dynamics in terms of\nprovability.",
    "categories": [
      "cs.LO",
      "cs.AI"
    ],
    "primary_category": "cs.LO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09516v1",
    "published_date": "2024-08-18 15:43:11 UTC",
    "updated_date": "2024-08-18 15:43:11 UTC"
  },
  {
    "arxiv_id": "2408.09503v2",
    "title": "Out-of-distribution generalization via composition: a lens through induction heads in Transformers",
    "authors": [
      "Jiajun Song",
      "Zhuoyan Xu",
      "Yiqiao Zhong"
    ],
    "abstract": "Large language models (LLMs) such as GPT-4 sometimes appear to be creative,\nsolving novel tasks often with a few demonstrations in the prompt. These tasks\nrequire the models to generalize on distributions different from those from\ntraining data -- which is known as out-of-distribution (OOD) generalization.\nDespite the tremendous success of LLMs, how they approach OOD generalization\nremains an open and underexplored question. We examine OOD generalization in\nsettings where instances are generated according to hidden rules, including\nin-context learning with symbolic reasoning. Models are required to infer the\nhidden rules behind input prompts without any fine-tuning.\n  We empirically examined the training dynamics of Transformers on a synthetic\nexample and conducted extensive experiments on a variety of pretrained LLMs,\nfocusing on a type of components known as induction heads. We found that OOD\ngeneralization and composition are tied together -- models can learn rules by\ncomposing two self-attention layers, thereby achieving OOD generalization.\nFurthermore, a shared latent subspace in the embedding (or feature) space acts\nas a bridge for composition by aligning early layers and later layers, which we\nrefer to as the common bridge representation hypothesis.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.CL",
    "comment": "46 pages, 27 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.09503v2",
    "published_date": "2024-08-18 14:52:25 UTC",
    "updated_date": "2024-12-28 17:15:28 UTC"
  },
  {
    "arxiv_id": "2408.09501v1",
    "title": "Beyond Local Views: Global State Inference with Diffusion Models for Cooperative Multi-Agent Reinforcement Learning",
    "authors": [
      "Zhiwei Xu",
      "Hangyu Mao",
      "Nianmin Zhang",
      "Xin Xin",
      "Pengjie Ren",
      "Dapeng Li",
      "Bin Zhang",
      "Guoliang Fan",
      "Zhumin Chen",
      "Changwei Wang",
      "Jiangjin Yin"
    ],
    "abstract": "In partially observable multi-agent systems, agents typically only have\naccess to local observations. This severely hinders their ability to make\nprecise decisions, particularly during decentralized execution. To alleviate\nthis problem and inspired by image outpainting, we propose State Inference with\nDiffusion Models (SIDIFF), which uses diffusion models to reconstruct the\noriginal global state based solely on local observations. SIDIFF consists of a\nstate generator and a state extractor, which allow agents to choose suitable\nactions by considering both the reconstructed global state and local\nobservations. In addition, SIDIFF can be effortlessly incorporated into current\nmulti-agent reinforcement learning algorithms to improve their performance.\nFinally, we evaluated SIDIFF on different experimental platforms, including\nMulti-Agent Battle City (MABC), a novel and flexible multi-agent reinforcement\nlearning environment we developed. SIDIFF achieved desirable results and\noutperformed other popular algorithms.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "15 pages, 12 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.09501v1",
    "published_date": "2024-08-18 14:49:53 UTC",
    "updated_date": "2024-08-18 14:49:53 UTC"
  },
  {
    "arxiv_id": "2408.09490v4",
    "title": "Leveraging Invariant Principle for Heterophilic Graph Structure Distribution Shifts",
    "authors": [
      "Jinluan Yang",
      "Zhengyu Chen",
      "Teng Xiao",
      "Wenqiao Zhang",
      "Yong Lin",
      "Kun Kuang"
    ],
    "abstract": "Heterophilic Graph Neural Networks (HGNNs) have shown promising results for\nsemi-supervised learning tasks on graphs. Notably, most real-world heterophilic\ngraphs are composed of a mixture of nodes with different neighbor patterns,\nexhibiting local node-level homophilic and heterophilic structures. However,\nexisting works are only devoted to designing better HGNN backbones or\narchitectures for node classification tasks on heterophilic and homophilic\ngraph benchmarks simultaneously, and their analyses of HGNN performance with\nrespect to nodes are only based on the determined data distribution without\nexploring the effect caused by this structural difference between training and\ntesting nodes. How to learn invariant node representations on heterophilic\ngraphs to handle this structure difference or distribution shifts remains\nunexplored. In this paper, we first discuss the limitations of previous\ngraph-based invariant learning methods from the perspective of data\naugmentation. Then, we propose \\textbf{HEI}, a framework capable of generating\ninvariant node representations through incorporating heterophily information to\ninfer latent environments without augmentation, which are then used for\ninvariant prediction, under heterophilic graph structure distribution shifts.\nWe theoretically show that our proposed method can achieve guaranteed\nperformance under heterophilic graph structure distribution shifts. Extensive\nexperiments on various benchmarks and backbones can also demonstrate the\neffectiveness of our method compared with existing state-of-the-art baselines.\nThe code is available at https://github.com/Yangjinluan/HEI",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "arxiv version of WWW2025",
    "pdf_url": "http://arxiv.org/pdf/2408.09490v4",
    "published_date": "2024-08-18 14:10:34 UTC",
    "updated_date": "2025-02-25 03:06:08 UTC"
  },
  {
    "arxiv_id": "2408.09489v1",
    "title": "REFINE-LM: Mitigating Language Model Stereotypes via Reinforcement Learning",
    "authors": [
      "Rameez Qureshi",
      "Naïm Es-Sebbani",
      "Luis Galárraga",
      "Yvette Graham",
      "Miguel Couceiro",
      "Zied Bouraoui"
    ],
    "abstract": "With the introduction of (large) language models, there has been significant\nconcern about the unintended bias such models may inherit from their training\ndata. A number of studies have shown that such models propagate gender\nstereotypes, as well as geographical and racial bias, among other biases. While\nexisting works tackle this issue by preprocessing data and debiasing\nembeddings, the proposed methods require a lot of computational resources and\nannotation effort while being limited to certain types of biases. To address\nthese issues, we introduce REFINE-LM, a debiasing method that uses\nreinforcement learning to handle different types of biases without any\nfine-tuning. By training a simple model on top of the word probability\ndistribution of a LM, our bias agnostic reinforcement learning method enables\nmodel debiasing without human annotations or significant computational\nresources. Experiments conducted on a wide range of models, including several\nLMs, show that our method (i) significantly reduces stereotypical biases while\npreserving LMs performance; (ii) is applicable to different types of biases,\ngeneralizing across contexts such as gender, ethnicity, religion, and\nnationality-based biases; and (iii) it is not expensive to train.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09489v1",
    "published_date": "2024-08-18 14:08:31 UTC",
    "updated_date": "2024-08-18 14:08:31 UTC"
  },
  {
    "arxiv_id": "2408.09481v2",
    "title": "PanoSent: A Panoptic Sextuple Extraction Benchmark for Multimodal Conversational Aspect-based Sentiment Analysis",
    "authors": [
      "Meng Luo",
      "Hao Fei",
      "Bobo Li",
      "Shengqiong Wu",
      "Qian Liu",
      "Soujanya Poria",
      "Erik Cambria",
      "Mong-Li Lee",
      "Wynne Hsu"
    ],
    "abstract": "While existing Aspect-based Sentiment Analysis (ABSA) has received extensive\neffort and advancement, there are still gaps in defining a more holistic\nresearch target seamlessly integrating multimodality, conversation context,\nfine-granularity, and also covering the changing sentiment dynamics as well as\ncognitive causal rationales. This paper bridges the gaps by introducing a\nmultimodal conversational ABSA, where two novel subtasks are proposed: 1)\nPanoptic Sentiment Sextuple Extraction, panoramically recognizing holder,\ntarget, aspect, opinion, sentiment, rationale from multi-turn multi-party\nmultimodal dialogue. 2) Sentiment Flipping Analysis, detecting the dynamic\nsentiment transformation throughout the conversation with the causal reasons.\nTo benchmark the tasks, we construct PanoSent, a dataset annotated both\nmanually and automatically, featuring high quality, large scale, multimodality,\nmultilingualism, multi-scenarios, and covering both implicit and explicit\nsentiment elements. To effectively address the tasks, we devise a novel\nChain-of-Sentiment reasoning framework, together with a novel multimodal large\nlanguage model (namely Sentica) and a paraphrase-based verification mechanism.\nExtensive evaluations demonstrate the superiority of our methods over strong\nbaselines, validating the efficacy of all our proposed methods. The work is\nexpected to open up a new era for the ABSA community, and thus all our codes\nand data are open at https://PanoSent.github.io/",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by ACM MM 2024 (Oral)",
    "pdf_url": "http://arxiv.org/pdf/2408.09481v2",
    "published_date": "2024-08-18 13:51:01 UTC",
    "updated_date": "2024-09-09 15:57:34 UTC"
  },
  {
    "arxiv_id": "2408.09465v1",
    "title": "MedMAP: Promoting Incomplete Multi-modal Brain Tumor Segmentation with Alignment",
    "authors": [
      "Tianyi Liu",
      "Zhaorui Tan",
      "Muyin Chen",
      "Xi Yang",
      "Haochuan Jiang",
      "Kaizhu Huang"
    ],
    "abstract": "Brain tumor segmentation is often based on multiple magnetic resonance\nimaging (MRI). However, in clinical practice, certain modalities of MRI may be\nmissing, which presents a more difficult scenario. To cope with this challenge,\nKnowledge Distillation, Domain Adaption, and Shared Latent Space have emerged\nas commonly promising strategies. However, recent efforts typically overlook\nthe modality gaps and thus fail to learn important invariant feature\nrepresentations across different modalities. Such drawback consequently leads\nto limited performance for missing modality models. To ameliorate these\nproblems, pre-trained models are used in natural visual segmentation tasks to\nminimize the gaps. However, promising pre-trained models are often unavailable\nin medical image segmentation tasks. Along this line, in this paper, we propose\na novel paradigm that aligns latent features of involved modalities to a\nwell-defined distribution anchor as the substitution of the pre-trained model}.\nAs a major contribution, we prove that our novel training paradigm ensures a\ntight evidence lower bound, thus theoretically certifying its effectiveness.\nExtensive experiments on different backbones validate that the proposed\nparadigm can enable invariant feature representations and produce models with\nnarrowed modality gaps. Models with our alignment paradigm show their superior\nperformance on both BraTS2018 and BraTS2020 datasets.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09465v1",
    "published_date": "2024-08-18 13:16:30 UTC",
    "updated_date": "2024-08-18 13:16:30 UTC"
  },
  {
    "arxiv_id": "2408.09456v1",
    "title": "In-Memory Learning Automata Architecture using Y-Flash Cell",
    "authors": [
      "Omar Ghazal",
      "Tian Lan",
      "Shalman Ojukwu",
      "Komal Krishnamurthy",
      "Alex Yakovlev",
      "Rishad Shafik"
    ],
    "abstract": "The modern implementation of machine learning architectures faces significant\nchallenges due to frequent data transfer between memory and processing units.\nIn-memory computing, primarily through memristor-based analog computing, offers\na promising solution to overcome this von Neumann bottleneck. In this\ntechnology, data processing and storage are located inside the memory. Here, we\nintroduce a novel approach that utilizes floating-gate Y-Flash memristive\ndevices manufactured with a standard 180 nm CMOS process. These devices offer\nattractive features, including analog tunability and moderate device-to-device\nvariation; such characteristics are essential for reliable decision-making in\nML applications. This paper uses a new machine learning algorithm, the Tsetlin\nMachine (TM), for in-memory processing architecture. The TM's learning element,\nAutomaton, is mapped into a single Y-Flash cell, where the Automaton's range is\ntransferred into the Y-Flash's conductance scope. Through comprehensive\nsimulations, the proposed hardware implementation of the learning automata,\nparticularly for Tsetlin machines, has demonstrated enhanced scalability and\non-edge learning capabilities.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.ET",
      "cs.LG"
    ],
    "primary_category": "cs.AR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09456v1",
    "published_date": "2024-08-18 12:31:54 UTC",
    "updated_date": "2024-08-18 12:31:54 UTC"
  },
  {
    "arxiv_id": "2408.09451v1",
    "title": "GraphSPNs: Sum-Product Networks Benefit From Canonical Orderings",
    "authors": [
      "Milan Papež",
      "Martin Rektoris",
      "Václav Šmídl",
      "Tomáš Pevný"
    ],
    "abstract": "Deep generative models have recently made a remarkable progress in capturing\ncomplex probability distributions over graphs. However, they are intractable\nand thus unable to answer even the most basic probabilistic inference queries\nwithout resorting to approximations. Therefore, we propose graph sum-product\nnetworks (GraphSPNs), a tractable deep generative model which provides exact\nand efficient inference over (arbitrary parts of) graphs. We investigate\ndifferent principles to make SPNs permutation invariant. We demonstrate that\nGraphSPNs are able to (conditionally) generate novel and chemically valid\nmolecular graphs, being competitive to, and sometimes even better than,\nexisting intractable models. We find out that (Graph)SPNs benefit from ensuring\nthe permutation invariance via canonical ordering.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09451v1",
    "published_date": "2024-08-18 12:19:16 UTC",
    "updated_date": "2024-08-18 12:19:16 UTC"
  },
  {
    "arxiv_id": "2408.13273v1",
    "title": "Retrieval-Augmented Generation Meets Data-Driven Tabula Rasa Approach for Temporal Knowledge Graph Forecasting",
    "authors": [
      "Geethan Sannidhi",
      "Sagar Srinivas Sakhinana",
      "Venkataramana Runkana"
    ],
    "abstract": "Pre-trained large language models (PLLMs) like OpenAI ChatGPT and Google\nGemini face challenges such as inaccurate factual recall, hallucinations,\nbiases, and future data leakage for temporal Knowledge Graph (tKG) forecasting.\nTo address these issues, we introduce sLA-tKGF (small-scale language assistant\nfor tKG forecasting), which utilizes Retrieval-Augmented Generation (RAG)\naided, custom-trained small-scale language models through a tabula rasa\napproach from scratch for effective tKG forecasting. Our framework constructs\nknowledge-infused prompts with relevant historical data from tKGs, web search\nresults, and PLLMs-generated textual descriptions to understand historical\nentity relationships prior to the target time. It leverages these external\nknowledge-infused prompts for deeper understanding and reasoning of\ncontext-specific semantic and temporal information to zero-shot prompt\nsmall-scale language models for more accurate predictions of future events\nwithin tKGs. It reduces hallucinations and mitigates distributional shift\nchallenges through comprehending changing trends over time. As a result, it\nenables more accurate and contextually grounded forecasts of future events\nwhile minimizing computational demands. Rigorous empirical studies demonstrate\nour framework robustness, scalability, and state-of-the-art (SOTA) performance\non benchmark datasets with interpretable and trustworthy tKG forecasting.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Paper was accepted at ACM KDD -2024 -- Undergraduate Consortium.\n  Please find the link: https://kdd2024.kdd.org/undergraduate-consortium/",
    "pdf_url": "http://arxiv.org/pdf/2408.13273v1",
    "published_date": "2024-08-18 11:52:24 UTC",
    "updated_date": "2024-08-18 11:52:24 UTC"
  },
  {
    "arxiv_id": "2408.14484v1",
    "title": "Agentic Retrieval-Augmented Generation for Time Series Analysis",
    "authors": [
      "Chidaksh Ravuru",
      "Sagar Srinivas Sakhinana",
      "Venkataramana Runkana"
    ],
    "abstract": "Time series modeling is crucial for many applications, however, it faces\nchallenges such as complex spatio-temporal dependencies and distribution shifts\nin learning from historical context to predict task-specific outcomes. To\naddress these challenges, we propose a novel approach using an agentic\nRetrieval-Augmented Generation (RAG) framework for time series analysis. The\nframework leverages a hierarchical, multi-agent architecture where the master\nagent orchestrates specialized sub-agents and delegates the end-user request to\nthe relevant sub-agent. The sub-agents utilize smaller, pre-trained language\nmodels (SLMs) customized for specific time series tasks through fine-tuning\nusing instruction tuning and direct preference optimization, and retrieve\nrelevant prompts from a shared repository of prompt pools containing distilled\nknowledge about historical patterns and trends to improve predictions on new\ndata. Our proposed modular, multi-agent RAG approach offers flexibility and\nachieves state-of-the-art performance across major time series tasks by\ntackling complex challenges more effectively than task-specific customized\nmethods across benchmark datasets.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Paper was accepted for Undergraduate Consortium at ACM KDD, 2024.\n  Please find the link: https://kdd2024.kdd.org/undergraduate-consortium/",
    "pdf_url": "http://arxiv.org/pdf/2408.14484v1",
    "published_date": "2024-08-18 11:47:55 UTC",
    "updated_date": "2024-08-18 11:47:55 UTC"
  },
  {
    "arxiv_id": "2408.09442v1",
    "title": "Parallel Sampling via Counting",
    "authors": [
      "Nima Anari",
      "Ruiquan Gao",
      "Aviad Rubinstein"
    ],
    "abstract": "We show how to use parallelization to speed up sampling from an arbitrary\ndistribution $\\mu$ on a product space $[q]^n$, given oracle access to counting\nqueries: $\\mathbb{P}_{X\\sim \\mu}[X_S=\\sigma_S]$ for any $S\\subseteq [n]$ and\n$\\sigma_S \\in [q]^S$. Our algorithm takes $O({n^{2/3}\\cdot\n\\operatorname{polylog}(n,q)})$ parallel time, to the best of our knowledge, the\nfirst sublinear in $n$ runtime for arbitrary distributions. Our results have\nimplications for sampling in autoregressive models. Our algorithm directly\nworks with an equivalent oracle that answers conditional marginal queries\n$\\mathbb{P}_{X\\sim \\mu}[X_i=\\sigma_i\\;\\vert\\; X_S=\\sigma_S]$, whose role is\nplayed by a trained neural network in autoregressive models. This suggests a\nroughly $n^{1/3}$-factor speedup is possible for sampling in any-order\nautoregressive models. We complement our positive result by showing a lower\nbound of $\\widetilde{\\Omega}(n^{1/3})$ for the runtime of any parallel sampling\nalgorithm making at most $\\operatorname{poly}(n)$ queries to the counting\noracle, even for $q=2$.",
    "categories": [
      "cs.DS",
      "cs.AI",
      "cs.LG",
      "math.PR"
    ],
    "primary_category": "cs.DS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09442v1",
    "published_date": "2024-08-18 11:42:54 UTC",
    "updated_date": "2024-08-18 11:42:54 UTC"
  },
  {
    "arxiv_id": "2408.11866v1",
    "title": "Crossing New Frontiers: Knowledge-Augmented Large Language Model Prompting for Zero-Shot Text-Based De Novo Molecule Design",
    "authors": [
      "Sakhinana Sagar Srinivas",
      "Venkataramana Runkana"
    ],
    "abstract": "Molecule design is a multifaceted approach that leverages computational\nmethods and experiments to optimize molecular properties, fast-tracking new\ndrug discoveries, innovative material development, and more efficient chemical\nprocesses. Recently, text-based molecule design has emerged, inspired by\nnext-generation AI tasks analogous to foundational vision-language models. Our\nstudy explores the use of knowledge-augmented prompting of large language\nmodels (LLMs) for the zero-shot text-conditional de novo molecular generation\ntask. Our approach uses task-specific instructions and a few demonstrations to\naddress distributional shift challenges when constructing augmented prompts for\nquerying LLMs to generate molecules consistent with technical descriptions. Our\nframework proves effective, outperforming state-of-the-art (SOTA) baseline\nmodels on benchmark datasets.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "q-bio.BM"
    ],
    "primary_category": "cs.CL",
    "comment": "Paper was accepted at R0-FoMo: Robustness of Few-shot and Zero-shot\n  Learning in Foundation Models, NeurIPS-2023. Please find the links:\n  https://sites.google.com/view/r0-fomo/accepted-papers?authuser=0 and\n  https://neurips.cc/virtual/2023/workshop/66517",
    "pdf_url": "http://arxiv.org/pdf/2408.11866v1",
    "published_date": "2024-08-18 11:37:19 UTC",
    "updated_date": "2024-08-18 11:37:19 UTC"
  },
  {
    "arxiv_id": "2408.09439v2",
    "title": "Towards Boosting LLMs-driven Relevance Modeling with Progressive Retrieved Behavior-augmented Prompting",
    "authors": [
      "Zeyuan Chen",
      "Haiyan Wu",
      "Kaixin Wu",
      "Wei Chen",
      "Mingjie Zhong",
      "Jia Xu",
      "Zhongyi Liu",
      "Wei Zhang"
    ],
    "abstract": "Relevance modeling is a critical component for enhancing user experience in\nsearch engines, with the primary objective of identifying items that align with\nusers' queries. Traditional models only rely on the semantic congruence between\nqueries and items to ascertain relevance. However, this approach represents\nmerely one aspect of the relevance judgement, and is insufficient in isolation.\nEven powerful Large Language Models (LLMs) still cannot accurately judge the\nrelevance of a query and an item from a semantic perspective. To augment\nLLMs-driven relevance modeling, this study proposes leveraging user\ninteractions recorded in search logs to yield insights into users' implicit\nsearch intentions. The challenge lies in the effective prompting of LLMs to\ncapture dynamic search intentions, which poses several obstacles in real-world\nrelevance scenarios, i.e., the absence of domain-specific knowledge, the\ninadequacy of an isolated prompt, and the prohibitive costs associated with\ndeploying LLMs. In response, we propose ProRBP, a novel Progressive Retrieved\nBehavior-augmented Prompting framework for integrating search scenario-oriented\nknowledge with LLMs effectively. Specifically, we perform the user-driven\nbehavior neighbors retrieval from the daily search logs to obtain\ndomain-specific knowledge in time, retrieving candidates that users consider to\nmeet their expectations. Then, we guide LLMs for relevance modeling by\nemploying advanced prompting techniques that progressively improve the outputs\nof the LLMs, followed by a progressive aggregation with comprehensive\nconsideration of diverse aspects. For online serving, we have developed an\nindustrial application framework tailored for the deployment of LLMs in\nrelevance modeling. Experiments on real-world industry data and online A/B\ntesting demonstrate our proposal achieves promising performance.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted By COLING 2025",
    "pdf_url": "http://arxiv.org/pdf/2408.09439v2",
    "published_date": "2024-08-18 11:07:38 UTC",
    "updated_date": "2024-12-06 12:09:15 UTC"
  },
  {
    "arxiv_id": "2408.09438v1",
    "title": "Enhancing Modal Fusion by Alignment and Label Matching for Multimodal Emotion Recognition",
    "authors": [
      "Qifei Li",
      "Yingming Gao",
      "Yuhua Wen",
      "Cong Wang",
      "Ya Li"
    ],
    "abstract": "To address the limitation in multimodal emotion recognition (MER) performance\narising from inter-modal information fusion, we propose a novel MER framework\nbased on multitask learning where fusion occurs after alignment, called\nFoal-Net. The framework is designed to enhance the effectiveness of modality\nfusion and includes two auxiliary tasks: audio-video emotion alignment (AVEL)\nand cross-modal emotion label matching (MEM). First, AVEL achieves alignment of\nemotional information in audio-video representations through contrastive\nlearning. Then, a modal fusion network integrates the aligned features.\nMeanwhile, MEM assesses whether the emotions of the current sample pair are the\nsame, providing assistance for modal information fusion and guiding the model\nto focus more on emotional information. The experimental results conducted on\nIEMOCAP corpus show that Foal-Net outperforms the state-of-the-art methods and\nemotion alignment is necessary before modal fusion.",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.CV",
      "cs.SD"
    ],
    "primary_category": "cs.MM",
    "comment": "The paper has been accepted by INTERSPEECH 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.09438v1",
    "published_date": "2024-08-18 11:05:21 UTC",
    "updated_date": "2024-08-18 11:05:21 UTC"
  },
  {
    "arxiv_id": "2408.09434v2",
    "title": "HySem: A context length optimized LLM pipeline for unstructured tabular extraction",
    "authors": [
      "Narayanan PP",
      "Anantharaman Palacode Narayana Iyer"
    ],
    "abstract": "Regulatory compliance reporting in the pharmaceutical industry relies on\ndetailed tables, but these are often under-utilized beyond compliance due to\ntheir unstructured format and arbitrary content. Extracting and semantically\nrepresenting tabular data is challenging due to diverse table presentations.\nLarge Language Models (LLMs) demonstrate substantial potential for semantic\nrepresentation, yet they encounter challenges related to accuracy and context\nsize limitations, which are crucial considerations for the industry\napplications. We introduce HySem, a pipeline that employs a novel context\nlength optimization technique to generate accurate semantic JSON\nrepresentations from HTML tables. This approach utilizes a custom fine-tuned\nmodel specifically designed for cost- and privacy-sensitive small and medium\npharmaceutical enterprises. Running on commodity hardware and leveraging\nopen-source models, HySem surpasses its peer open-source models in accuracy and\nprovides competitive performance when benchmarked against OpenAI GPT-4o and\neffectively addresses context length limitations, which is a crucial factor for\nsupporting larger tables.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "F.2.2; I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "19 pages, 7 tables, 10 figures, 2 algorithms",
    "pdf_url": "http://arxiv.org/pdf/2408.09434v2",
    "published_date": "2024-08-18 10:40:37 UTC",
    "updated_date": "2024-10-05 13:32:25 UTC"
  },
  {
    "arxiv_id": "2408.09432v1",
    "title": "Deformation-aware GAN for Medical Image Synthesis with Substantially Misaligned Pairs",
    "authors": [
      "Bowen Xin",
      "Tony Young",
      "Claire E Wainwright",
      "Tamara Blake",
      "Leo Lebrat",
      "Thomas Gaass",
      "Thomas Benkert",
      "Alto Stemmer",
      "David Coman",
      "Jason Dowling"
    ],
    "abstract": "Medical image synthesis generates additional imaging modalities that are\ncostly, invasive or harmful to acquire, which helps to facilitate the clinical\nworkflow. When training pairs are substantially misaligned (e.g., lung MRI-CT\npairs with respiratory motion), accurate image synthesis remains a critical\nchallenge. Recent works explored the directional registration module to adjust\nmisalignment in generative adversarial networks (GANs); however, substantial\nmisalignment will lead to 1) suboptimal data mapping caused by correspondence\nambiguity, and 2) degraded image fidelity caused by morphology influence on\ndiscriminators. To address the challenges, we propose a novel Deformation-aware\nGAN (DA-GAN) to dynamically correct the misalignment during the image synthesis\nbased on multi-objective inverse consistency. Specifically, in the generative\nprocess, three levels of inverse consistency cohesively optimise symmetric\nregistration and image generation for improved correspondence. In the\nadversarial process, to further improve image fidelity under misalignment, we\ndesign deformation-aware discriminators to disentangle the mismatched spatial\nmorphology from the judgement of image fidelity. Experimental results show that\nDA-GAN achieved superior performance on a public dataset with simulated\nmisalignments and a real-world lung MRI-CT dataset with respiratory motion\nmisalignment. The results indicate the potential for a wide range of medical\nimage synthesis tasks such as radiotherapy planning.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "Accepted by MIDL2024",
    "pdf_url": "http://arxiv.org/pdf/2408.09432v1",
    "published_date": "2024-08-18 10:29:35 UTC",
    "updated_date": "2024-08-18 10:29:35 UTC"
  },
  {
    "arxiv_id": "2408.09430v1",
    "title": "FASST: Fast LLM-based Simultaneous Speech Translation",
    "authors": [
      "Siqi Ouyang",
      "Xi Xu",
      "Chinmay Dandekar",
      "Lei Li"
    ],
    "abstract": "Simultaneous speech translation (SST) takes streaming speech input and\ngenerates text translation on the fly. Existing methods either have high\nlatency due to recomputation of input representations, or fall behind of\noffline ST in translation quality. In this paper, we propose FASST, a fast\nlarge language model based method for streaming speech translation. We propose\nblockwise-causal speech encoding and consistency mask, so that streaming speech\ninput can be encoded incrementally without recomputation. Furthermore, we\ndevelop a two-stage training strategy to optimize FASST for simultaneous\ninference. We evaluate FASST and multiple strong prior models on MuST-C\ndataset. Experiment results show that FASST achieves the best quality-latency\ntrade-off. It outperforms the previous best model by an average of 1.5 BLEU\nunder the same latency for English to Spanish translation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09430v1",
    "published_date": "2024-08-18 10:12:39 UTC",
    "updated_date": "2024-08-18 10:12:39 UTC"
  },
  {
    "arxiv_id": "2408.09426v1",
    "title": "A Robust Algorithm for Contactless Fingerprint Enhancement and Matching",
    "authors": [
      "Mahrukh Siddiqui",
      "Shahzaib Iqbal",
      "Bandar AlShammari",
      "Bandar Alhaqbani",
      "Tariq M. Khan",
      "Imran Razzak"
    ],
    "abstract": "Compared to contact fingerprint images, contactless fingerprint images\nexhibit four distinct characteristics: (1) they contain less noise; (2) they\nhave fewer discontinuities in ridge patterns; (3) the ridge-valley pattern is\nless distinct; and (4) they pose an interoperability problem, as they lack the\nelastic deformation caused by pressing the finger against the capture device.\nThese properties present significant challenges for the enhancement of\ncontactless fingerprint images. In this study, we propose a novel contactless\nfingerprint identification solution that enhances the accuracy of minutiae\ndetection through improved frequency estimation and a new region-quality-based\nminutia extraction algorithm. In addition, we introduce an efficient and highly\naccurate minutiae-based encoding and matching algorithm. We validate the\neffectiveness of our approach through extensive experimental testing. Our\nmethod achieves a minimum Equal Error Rate (EER) of 2.84\\% on the PolyU\ncontactless fingerprint dataset, demonstrating its superior performance\ncompared to existing state-of-the-art techniques. The proposed fingerprint\nidentification method exhibits notable precision and resilience, proving to be\nan effective and feasible solution for contactless fingerprint-based\nidentification systems.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09426v1",
    "published_date": "2024-08-18 10:01:42 UTC",
    "updated_date": "2024-08-18 10:01:42 UTC"
  },
  {
    "arxiv_id": "2408.09422v1",
    "title": "Distinguish Confusion in Legal Judgment Prediction via Revised Relation Knowledge",
    "authors": [
      "Nuo Xu",
      "Pinghui Wang",
      "Junzhou Zhao",
      "Feiyang Sun",
      "Lin Lan",
      "Jing Tao",
      "Li Pan",
      "Xiaohong Guan"
    ],
    "abstract": "Legal Judgment Prediction (LJP) aims to automatically predict a law case's\njudgment results based on the text description of its facts. In practice, the\nconfusing law articles (or charges) problem frequently occurs, reflecting that\nthe law cases applicable to similar articles (or charges) tend to be misjudged.\nAlthough some recent works based on prior knowledge solve this issue well, they\nignore that confusion also occurs between law articles with a high posterior\nsemantic similarity due to the data imbalance problem instead of only between\nthe prior highly similar ones, which is this work's further finding. This paper\nproposes an end-to-end model named \\textit{D-LADAN} to solve the above\nchallenges. On the one hand, D-LADAN constructs a graph among law articles\nbased on their text definition and proposes a graph distillation operation\n(GDO) to distinguish the ones with a high prior semantic similarity. On the\nother hand, D-LADAN presents a novel momentum-updated memory mechanism to\ndynamically sense the posterior similarity between law articles (or charges)\nand a weighted GDO to adaptively capture the distinctions for revising the\ninductive bias caused by the data imbalance problem. We perform extensive\nexperiments to demonstrate that D-LADAN significantly outperforms\nstate-of-the-art methods in accuracy and robustness.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by ACM TOIS",
    "pdf_url": "http://arxiv.org/pdf/2408.09422v1",
    "published_date": "2024-08-18 09:44:59 UTC",
    "updated_date": "2024-08-18 09:44:59 UTC"
  },
  {
    "arxiv_id": "2408.09416v2",
    "title": "Challenges and Responses in the Practice of Large Language Models",
    "authors": [
      "Hongyin Zhu"
    ],
    "abstract": "This paper carefully summarizes extensive and profound questions from all\nwalks of life, focusing on the current high-profile AI field, covering multiple\ndimensions such as industry trends, academic research, technological innovation\nand business applications. This paper meticulously curates questions that are\nboth thought-provoking and practically relevant, providing nuanced and\ninsightful answers to each. To facilitate readers' understanding and reference,\nthis paper specifically classifies and organizes these questions systematically\nand meticulously from the five core dimensions of computing power\ninfrastructure, software architecture, data resources, application scenarios,\nand brain science. This work aims to provide readers with a comprehensive,\nin-depth and cutting-edge AI knowledge framework to help people from all walks\nof life grasp the pulse of AI development, stimulate innovative thinking, and\npromote industrial progress.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09416v2",
    "published_date": "2024-08-18 09:15:11 UTC",
    "updated_date": "2024-08-21 11:24:42 UTC"
  },
  {
    "arxiv_id": "2408.09410v3",
    "title": "BernGraph: Probabilistic Graph Neural Networks for EHR-based Medication Recommendations",
    "authors": [
      "Xihao Piao",
      "Pei Gao",
      "Zheng Chen",
      "Lingwei Zhu",
      "Yasuko Matsubara",
      "Yasushi Sakurai",
      "Jimeng Sun"
    ],
    "abstract": "The medical community believes binary medical event outcomes in EHR data\ncontain sufficient information for making a sensible recommendation. However,\nthere are two challenges to effectively utilizing such data: (1) modeling the\nrelationship between massive 0,1 event outcomes is difficult, even with expert\nknowledge; (2) in practice, learning can be stalled by the binary values since\nthe equally important 0 entries propagate no learning signals. Currently, there\nis a large gap between the assumed sufficient information and the reality that\nno promising results have been shown by utilizing solely the binary data:\nvisiting or secondary information is often necessary to reach acceptable\nperformance. In this paper, we attempt to build the first successful binary EHR\ndata-oriented drug recommendation system by tackling the two difficulties,\nmaking sensible drug recommendations solely using the binary EHR medical\nrecords. To this end, we take a statistical perspective to view the EHR data as\na sample from its cohorts and transform them into continuous Bernoulli\nprobabilities. The transformed entries not only model a deterministic binary\nevent with a distribution but also allow reflecting \\emph{event-event}\nrelationship by conditional probability. A graph neural network is learned on\ntop of the transformation. It captures event-event correlations while\nemphasizing \\emph{event-to-patient} features. Extensive results demonstrate\nthat the proposed method achieves state-of-the-art performance on large-scale\ndatabases, outperforming baseline methods that use secondary information by a\nlarge margin. The source code is available at\n\\url{https://github.com/chenzRG/BEHRMecom}",
    "categories": [
      "cs.AI",
      "68T01"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09410v3",
    "published_date": "2024-08-18 08:52:27 UTC",
    "updated_date": "2024-09-11 02:12:44 UTC"
  },
  {
    "arxiv_id": "2408.09404v1",
    "title": "Comparison between the Structures of Word Co-occurrence and Word Similarity Networks for Ill-formed and Well-formed Texts in Taiwan Mandarin",
    "authors": [
      "Po-Hsuan Huang",
      "Hsuan-Lei Shao"
    ],
    "abstract": "The study of word co-occurrence networks has attracted the attention of\nresearchers due to their potential significance as well as applications.\nUnderstanding the structure of word co-occurrence networks is therefore\nimportant to fully realize their significance and usages. In past studies, word\nco-occurrence networks built on well-formed texts have been found to possess\ncertain characteristics, including being small-world, following a two-regime\npower law distribution, and being generally disassortative. On the flip side,\npast studies have found that word co-occurrence networks built from ill-formed\ntexts such as microblog posts may behave differently from those built from\nwell-formed documents. While both kinds of word co-occurrence networks are\nsmall-world and disassortative, word co-occurrence networks built from\nill-formed texts are scale-free and follow the power law distribution instead\nof the two-regime power law distribution. However, since past studies on the\nbehavior of word co-occurrence networks built from ill-formed texts only\ninvestigated English, the universality of such characteristics remains to be\nseen among different languages. In addition, it is yet to be investigated\nwhether there could be possible similitude/differences between word\nco-occurrence networks and other potentially comparable networks. This study\ntherefore investigates and compares the structure of word co-occurrence\nnetworks and word similarity networks based on Taiwan Mandarin ill-formed\ninternet forum posts and compare them with those built with well-formed\njudicial judgments, and seeks to find out whether the three aforementioned\nproperties (scale-free, small-world, and disassortative) for ill-formed and\nwell-formed texts are universal among different languages and between word\nco-occurrence and word similarity networks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "H.3.3; I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "4 pages, 1 figure, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2408.09404v1",
    "published_date": "2024-08-18 08:30:16 UTC",
    "updated_date": "2024-08-18 08:30:16 UTC"
  },
  {
    "arxiv_id": "2408.09403v2",
    "title": "Obtaining Optimal Spiking Neural Network in Sequence Learning via CRNN-SNN Conversion",
    "authors": [
      "Jiahao Su",
      "Kang You",
      "Zekai Xu",
      "Weizhi Xu",
      "Zhezhi He"
    ],
    "abstract": "Spiking neural networks (SNNs) are becoming a promising alternative to\nconventional artificial neural networks (ANNs) due to their rich neural\ndynamics and the implementation of energy-efficient neuromorphic chips.\nHowever, the non-differential binary communication mechanism makes SNN hard to\nconverge to an ANN-level accuracy. When SNN encounters sequence learning, the\nsituation becomes worse due to the difficulties in modeling long-range\ndependencies. To overcome these difficulties, researchers developed variants of\nLIF neurons and different surrogate gradients but still failed to obtain good\nresults when the sequence became longer (e.g., $>$500). Unlike them, we obtain\nan optimal SNN in sequence learning by directly mapping parameters from a\nquantized CRNN. We design two sub-pipelines to support the end-to-end\nconversion of different structures in neural networks, which is called\nCNN-Morph (CNN $\\rightarrow$ QCNN $\\rightarrow$ BIFSNN) and RNN-Morph (RNN\n$\\rightarrow$ QRNN $\\rightarrow$ RBIFSNN). Using conversion pipelines and the\ns-analog encoding method, the conversion error of our framework is zero.\nFurthermore, we give the theoretical and experimental demonstration of the\nlossless CRNN-SNN conversion. Our results show the effectiveness of our method\nover short and long timescales tasks compared with the state-of-the-art\nlearning- and conversion-based methods. We reach the highest accuracy of 99.16%\n(0.46 $\\uparrow$) on S-MNIST, 94.95% (3.95 $\\uparrow$) on PS-MNIST (sequence\nlength of 784) respectively, and the lowest loss of 0.057 (0.013 $\\downarrow$)\nwithin 8 time-steps in collision avoidance dataset.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by 33rd International Conference on Artificial Neural\n  Networks",
    "pdf_url": "http://arxiv.org/pdf/2408.09403v2",
    "published_date": "2024-08-18 08:23:51 UTC",
    "updated_date": "2024-08-26 01:26:35 UTC"
  },
  {
    "arxiv_id": "2408.09393v1",
    "title": "Federated Graph Learning with Structure Proxy Alignment",
    "authors": [
      "Xingbo Fu",
      "Zihan Chen",
      "Binchi Zhang",
      "Chen Chen",
      "Jundong Li"
    ],
    "abstract": "Federated Graph Learning (FGL) aims to learn graph learning models over graph\ndata distributed in multiple data owners, which has been applied in various\napplications such as social recommendation and financial fraud detection.\nInherited from generic Federated Learning (FL), FGL similarly has the data\nheterogeneity issue where the label distribution may vary significantly for\ndistributed graph data across clients. For instance, a client can have the\nmajority of nodes from a class, while another client may have only a few nodes\nfrom the same class. This issue results in divergent local objectives and\nimpairs FGL convergence for node-level tasks, especially for node\nclassification. Moreover, FGL also encounters a unique challenge for the node\nclassification task: the nodes from a minority class in a client are more\nlikely to have biased neighboring information, which prevents FGL from learning\nexpressive node embeddings with Graph Neural Networks (GNNs). To grapple with\nthe challenge, we propose FedSpray, a novel FGL framework that learns local\nclass-wise structure proxies in the latent space and aligns them to obtain\nglobal structure proxies in the server. Our goal is to obtain the aligned\nstructure proxies that can serve as reliable, unbiased neighboring information\nfor node classification. To achieve this, FedSpray trains a global\nfeature-structure encoder and generates unbiased soft targets with structure\nproxies to regularize local training of GNN models in a personalized way. We\nconduct extensive experiments over four datasets, and experiment results\nvalidate the superiority of FedSpray compared with other baselines. Our code is\navailable at https://github.com/xbfu/FedSpray.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by KDD 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.09393v1",
    "published_date": "2024-08-18 07:32:54 UTC",
    "updated_date": "2024-08-18 07:32:54 UTC"
  },
  {
    "arxiv_id": "2408.09386v2",
    "title": "Game Development as Human-LLM Interaction",
    "authors": [
      "Jiale Hong",
      "Hongqiu Wu",
      "Hai Zhao"
    ],
    "abstract": "Game development is a highly specialized task that relies on a complex game\nengine powered by complex programming languages, preventing many gaming\nenthusiasts from handling it. This paper introduces the Chat Game Engine\n(ChatGE) powered by LLM, which allows everyone to develop a custom game using\nnatural language through Human-LLM interaction. To enable an LLM to function as\na ChatGE, we instruct it to perform the following processes in each turn: (1)\n$P_{script}$: configure the game script segment based on the user's input; (2)\n$P_{code}$: generate the corresponding code snippet based on the game script\nsegment; (3) $P_{utter}$: interact with the user, including guidance and\nfeedback. We propose a data synthesis pipeline based on LLM to generate game\nscript-code pairs and interactions from a few manually crafted seed data. We\npropose a three-stage progressive training strategy to transfer the\ndialogue-based LLM to our ChatGE smoothly. We construct a ChatGE for poker\ngames as a case study and comprehensively evaluate it from two perspectives:\ninteraction quality and code correctness.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09386v2",
    "published_date": "2024-08-18 07:06:57 UTC",
    "updated_date": "2024-12-16 06:58:49 UTC"
  },
  {
    "arxiv_id": "2408.09385v2",
    "title": "Reward Difference Optimization For Sample Reweighting In Offline RLHF",
    "authors": [
      "Shiqi Wang",
      "Zhengze Zhang",
      "Rui Zhao",
      "Fei Tan",
      "Cam Tu Nguyen"
    ],
    "abstract": "With the rapid advances in Large Language Models (LLMs), aligning LLMs with\nhuman preferences become increasingly important. Although Reinforcement\nLearning with Human Feedback (RLHF) proves effective, it is complicated and\nhighly resource-intensive. As such, offline RLHF has been introduced as an\nalternative solution, which directly optimizes LLMs with ranking losses on a\nfixed preference dataset. Current offline RLHF only captures the \"ordinal\nrelationship\" between responses, overlooking the crucial aspect of how much one\nis preferred over the others. To address this issue, we propose a simple yet\neffective solution called Reward Difference Optimization, shorted as RDO.\nSpecifically, we introduce reward difference coefficients to reweigh sample\npairs in offline RLHF. We then develop a difference model which captures rich\ninteractions between a pair of responses for predicting these difference\ncoefficients. Experiments with 7B LLMs on the HH and TL;DR datasets\nsubstantiate the effectiveness of our method in both automatic metrics and\nhuman evaluation, thereby highlighting its potential for aligning LLMs with\nhuman intent and values",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2024 findings",
    "pdf_url": "http://arxiv.org/pdf/2408.09385v2",
    "published_date": "2024-08-18 07:04:16 UTC",
    "updated_date": "2024-10-30 04:47:00 UTC"
  },
  {
    "arxiv_id": "2408.09382v1",
    "title": "VRCopilot: Authoring 3D Layouts with Generative AI Models in VR",
    "authors": [
      "Lei Zhang",
      "Jin Pan",
      "Jacob Gettig",
      "Steve Oney",
      "Anhong Guo"
    ],
    "abstract": "Immersive authoring provides an intuitive medium for users to create 3D\nscenes via direct manipulation in Virtual Reality (VR). Recent advances in\ngenerative AI have enabled the automatic creation of realistic 3D layouts.\nHowever, it is unclear how capabilities of generative AI can be used in\nimmersive authoring to support fluid interactions, user agency, and creativity.\nWe introduce VRCopilot, a mixed-initiative system that integrates pre-trained\ngenerative AI models into immersive authoring to facilitate human-AI\nco-creation in VR. VRCopilot presents multimodal interactions to support rapid\nprototyping and iterations with AI, and intermediate representations such as\nwireframes to augment user controllability over the created content. Through a\nseries of user studies, we evaluated the potential and challenges in manual,\nscaffolded, and automatic creation in immersive authoring. We found that\nscaffolded creation using wireframes enhanced the user agency compared to\nautomatic creation. We also found that manual creation via multimodal\nspecification offers the highest sense of creativity and agency.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.HC",
    "comment": "UIST 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.09382v1",
    "published_date": "2024-08-18 06:45:31 UTC",
    "updated_date": "2024-08-18 06:45:31 UTC"
  },
  {
    "arxiv_id": "2408.09380v4",
    "title": "ELASTIC: Efficient Linear Attention for Sequential Interest Compression",
    "authors": [
      "Jiaxin Deng",
      "Shiyao Wang",
      "Song Lu",
      "Yinfeng Li",
      "Xinchen Luo",
      "Yuanjun Liu",
      "Peixing Xu",
      "Guorui Zhou"
    ],
    "abstract": "State-of-the-art sequential recommendation models heavily rely on\ntransformer's attention mechanism. However, the quadratic computational and\nmemory complexities of self attention have limited its scalability for modeling\nusers' long range behaviour sequences. To address this problem, we propose\nELASTIC, an Efficient Linear Attention for SequenTial Interest Compression,\nrequiring only linear time complexity and decoupling model capacity from\ncomputational cost. Specifically, ELASTIC introduces a fixed length interest\nexperts with linear dispatcher attention mechanism which compresses the\nlong-term behaviour sequences to a significantly more compact representation\nwhich reduces up to 90% GPU memory usage with x2.7 inference speed up. The\nproposed linear dispatcher attention mechanism significantly reduces the\nquadratic complexity and makes the model feasible for adequately modeling\nextremely long sequences. Moreover, in order to retain the capacity for\nmodeling various user interests, ELASTIC initializes a vast learnable interest\nmemory bank and sparsely retrieves compressed user's interests from the memory\nwith a negligible computational overhead. The proposed interest memory\nretrieval technique significantly expands the cardinality of available interest\nspace while keeping the same computational cost, thereby striking a trade-off\nbetween recommendation accuracy and efficiency. To validate the effectiveness\nof our proposed ELASTIC, we conduct extensive experiments on various public\ndatasets and compare it with several strong sequential recommenders.\nExperimental results demonstrate that ELASTIC consistently outperforms\nbaselines by a significant margin and also highlight the computational\nefficiency of ELASTIC when modeling long sequences. We will make our\nimplementation code publicly available.",
    "categories": [
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "We hereby withdraw this paper from arXiv due to incomplete\n  experiments. Upon further review, we have determined that additional\n  experimental work is necessary to fully validate our findings and conclusions",
    "pdf_url": "http://arxiv.org/pdf/2408.09380v4",
    "published_date": "2024-08-18 06:41:46 UTC",
    "updated_date": "2025-02-12 04:00:41 UTC"
  },
  {
    "arxiv_id": "2408.09371v1",
    "title": "Detecting the Undetectable: Combining Kolmogorov-Arnold Networks and MLP for AI-Generated Image Detection",
    "authors": [
      "Taharim Rahman Anon",
      "Jakaria Islam Emon"
    ],
    "abstract": "As artificial intelligence progresses, the task of distinguishing between\nreal and AI-generated images is increasingly complicated by sophisticated\ngenerative models. This paper presents a novel detection framework adept at\nrobustly identifying images produced by cutting-edge generative AI models, such\nas DALL-E 3, MidJourney, and Stable Diffusion 3. We introduce a comprehensive\ndataset, tailored to include images from these advanced generators, which\nserves as the foundation for extensive evaluation. we propose a classification\nsystem that integrates semantic image embeddings with a traditional Multilayer\nPerceptron (MLP). This baseline system is designed to effectively differentiate\nbetween real and AI-generated images under various challenging conditions.\nEnhancing this approach, we introduce a hybrid architecture that combines\nKolmogorov-Arnold Networks (KAN) with the MLP. This hybrid model leverages the\nadaptive, high-resolution feature transformation capabilities of KAN, enabling\nour system to capture and analyze complex patterns in AI-generated images that\nare typically overlooked by conventional models. In out-of-distribution\ntesting, our proposed model consistently outperformed the standard MLP across\nthree out of distribution test datasets, demonstrating superior performance and\nrobustness in classifying real images from AI-generated images with impressive\nF1 scores.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "8 Pages, IEEE Transactions",
    "pdf_url": "http://arxiv.org/pdf/2408.09371v1",
    "published_date": "2024-08-18 06:00:36 UTC",
    "updated_date": "2024-08-18 06:00:36 UTC"
  },
  {
    "arxiv_id": "2408.13112v1",
    "title": "An Introduction to Cognidynamics",
    "authors": [
      "Marco Gori"
    ],
    "abstract": "This paper gives an introduction to \\textit{Cognidynamics}, that is to the\ndynamics of cognitive systems driven by optimal objectives imposed over time\nwhen they interact either with a defined virtual or with a real-world\nenvironment. The proposed theory is developed in the general framework of\ndynamic programming which leads to think of computational laws dictated by\nclassic Hamiltonian equations. Those equations lead to the formulation of a\nneural propagation scheme in cognitive agents modeled by dynamic neural\nnetworks which exhibits locality in both space and time, thus contributing the\nlongstanding debate on biological plausibility of learning algorithms like\nBackpropagation. We interpret the learning process in terms of energy exchange\nwith the environment and show the crucial role of energy dissipation and its\nlinks with focus of attention mechanisms and conscious behavior.",
    "categories": [
      "q-bio.NC",
      "cs.AI"
    ],
    "primary_category": "q-bio.NC",
    "comment": "This paper is related to the invited talk I gave at the Third\n  Conference on Lifelong Learning Agents (CoLLAs 2024) on the 29th of July 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.13112v1",
    "published_date": "2024-08-18 05:40:07 UTC",
    "updated_date": "2024-08-18 05:40:07 UTC"
  },
  {
    "arxiv_id": "2408.09365v2",
    "title": "Concept Distillation from Strong to Weak Models via Hypotheses-to-Theories Prompting",
    "authors": [
      "Emmanuel Aboah Boateng",
      "Cassiano O. Becker",
      "Nabiha Asghar",
      "Kabir Walia",
      "Ashwin Srinivasan",
      "Ehi Nosakhare",
      "Soundar Srinivasan",
      "Victor Dibia"
    ],
    "abstract": "Hand-crafting high quality prompts to optimize the performance of language\nmodels is a complicated and labor-intensive process. Furthermore, when\nmigrating to newer, smaller, or weaker models (possibly due to latency or cost\ngains), prompts need to be updated to re-optimize the task performance. We\npropose Concept Distillation (CD), an automatic prompt optimization technique\nfor enhancing weaker models on complex tasks. CD involves: (1) collecting\nmistakes made by weak models with a base prompt (initialization), (2) using a\nstrong model to generate reasons for these mistakes and create rules/concepts\nfor weak models (induction), and (3) filtering these rules based on validation\nset performance and integrating them into the base prompt\n(deduction/verification). We evaluated CD on NL2Code and mathematical reasoning\ntasks, observing significant performance boosts for small and weaker language\nmodels. Notably, Mistral-7B's accuracy on Multi-Arith increased by 20%, and\nPhi-3-mini-3.8B's accuracy on HumanEval rose by 34%. Compared to other\nautomated methods, CD offers an effective, cost-efficient strategy for\nimproving weak models' performance on complex tasks and enables seamless\nworkload migration across different language models without compromising\nperformance.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to NAACL 2025; 17 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.09365v2",
    "published_date": "2024-08-18 05:37:48 UTC",
    "updated_date": "2025-02-23 00:55:47 UTC"
  },
  {
    "arxiv_id": "2408.09358v2",
    "title": "Panorama Tomosynthesis from Head CBCT with Simulated Projection Geometry",
    "authors": [
      "Anusree P. S.",
      "Bikram Keshari Parida",
      "Seong Yong Moon",
      "Wonsang You"
    ],
    "abstract": "Cone Beam Computed Tomography (CBCT) and Panoramic X-rays are the most\ncommonly used imaging modalities in dental health care. CBCT can produce\nthree-dimensional views of a patient's head, providing clinicians with better\ndiagnostic capability, whereas Panoramic X-ray can capture the entire\nmaxillofacial region in a single image. If the CBCT is already available, it\ncan be beneficial to synthesize a Panoramic X-ray, thereby avoiding an\nimmediate additional scan and extra radiation exposure. Existing methods focus\non delineating an approximate dental arch and creating orthogonal projections\nalong this arch. However, no golden standard is available for such dental arch\nextractions, and this choice can affect the quality of synthesized X-rays. To\navoid such issues, we propose a novel method for synthesizing Panoramic X-rays\nfrom diverse head CBCTs, employing a simulated projection geometry and dynamic\nrotation centers. Our method effectively synthesized panoramic views from CBCT,\neven for patients with missing or nonexistent teeth and in the presence of\nsevere metal implants. Our results demonstrate that this method can generate\nhigh-quality panoramic images irrespective of the CBCT scanner geometry.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 6 figures, 1 table, Journal submission planned",
    "pdf_url": "http://arxiv.org/pdf/2408.09358v2",
    "published_date": "2024-08-18 04:48:03 UTC",
    "updated_date": "2024-08-20 07:07:49 UTC"
  },
  {
    "arxiv_id": "2408.09357v1",
    "title": "Meta-Learning Empowered Meta-Face: Personalized Speaking Style Adaptation for Audio-Driven 3D Talking Face Animation",
    "authors": [
      "Xukun Zhou",
      "Fengxin Li",
      "Ziqiao Peng",
      "Kejian Wu",
      "Jun He",
      "Biao Qin",
      "Zhaoxin Fan",
      "Hongyan Liu"
    ],
    "abstract": "Audio-driven 3D face animation is increasingly vital in live streaming and\naugmented reality applications. While remarkable progress has been observed,\nmost existing approaches are designed for specific individuals with predefined\nspeaking styles, thus neglecting the adaptability to varied speaking styles. To\naddress this limitation, this paper introduces MetaFace, a novel methodology\nmeticulously crafted for speaking style adaptation. Grounded in the novel\nconcept of meta-learning, MetaFace is composed of several key components: the\nRobust Meta Initialization Stage (RMIS) for fundamental speaking style\nadaptation, the Dynamic Relation Mining Neural Process (DRMN) for forging\nconnections between observed and unobserved speaking styles, and the Low-rank\nMatrix Memory Reduction Approach to enhance the efficiency of model\noptimization as well as learning style details. Leveraging these novel designs,\nMetaFace not only significantly outperforms robust existing baselines but also\nestablishes a new state-of-the-art, as substantiated by our experimental\nresults.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.GR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09357v1",
    "published_date": "2024-08-18 04:42:43 UTC",
    "updated_date": "2024-08-18 04:42:43 UTC"
  },
  {
    "arxiv_id": "2408.09350v1",
    "title": "E-CGL: An Efficient Continual Graph Learner",
    "authors": [
      "Jianhao Guo",
      "Zixuan Ni",
      "Yun Zhu",
      "Siliang Tang"
    ],
    "abstract": "Continual learning has emerged as a crucial paradigm for learning from\nsequential data while preserving previous knowledge. In the realm of continual\ngraph learning, where graphs continuously evolve based on streaming graph data,\ncontinual graph learning presents unique challenges that require adaptive and\nefficient graph learning methods in addition to the problem of catastrophic\nforgetting. The first challenge arises from the interdependencies between\ndifferent graph data, where previous graphs can influence new data\ndistributions. The second challenge lies in the efficiency concern when dealing\nwith large graphs. To addresses these two problems, we produce an Efficient\nContinual Graph Learner (E-CGL) in this paper. We tackle the interdependencies\nissue by demonstrating the effectiveness of replay strategies and introducing a\ncombined sampling strategy that considers both node importance and diversity.\nTo overcome the limitation of efficiency, E-CGL leverages a simple yet\neffective MLP model that shares weights with a GCN during training, achieving\nacceleration by circumventing the computationally expensive message passing\nprocess. Our method comprehensively surpasses nine baselines on four graph\ncontinual learning datasets under two settings, meanwhile E-CGL largely reduces\nthe catastrophic forgetting problem down to an average of -1.1%. Additionally,\nE-CGL achieves an average of 15.83x training time acceleration and 4.89x\ninference time acceleration across the four datasets. These results indicate\nthat E-CGL not only effectively manages the correlation between different graph\ndata during continual training but also enhances the efficiency of continual\nlearning on large graphs. The code is publicly available at\nhttps://github.com/aubreygjh/E-CGL.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09350v1",
    "published_date": "2024-08-18 04:10:30 UTC",
    "updated_date": "2024-08-18 04:10:30 UTC"
  },
  {
    "arxiv_id": "2408.11071v2",
    "title": "DiffZOO: A Purely Query-Based Black-Box Attack for Red-teaming Text-to-Image Generative Model via Zeroth Order Optimization",
    "authors": [
      "Pucheng Dang",
      "Xing Hu",
      "Dong Li",
      "Rui Zhang",
      "Qi Guo",
      "Kaidi Xu"
    ],
    "abstract": "Current text-to-image (T2I) synthesis diffusion models raise misuse concerns,\nparticularly in creating prohibited or not-safe-for-work (NSFW) images. To\naddress this, various safety mechanisms and red teaming attack methods are\nproposed to enhance or expose the T2I model's capability to generate unsuitable\ncontent. However, many red teaming attack methods assume knowledge of the text\nencoders, limiting their practical usage. In this work, we rethink the case of\n\\textit{purely black-box} attacks without prior knowledge of the T2l model. To\novercome the unavailability of gradients and the inability to optimize attacks\nwithin a discrete prompt space, we propose DiffZOO which applies Zeroth Order\nOptimization to procure gradient approximations and harnesses both C-PRV and\nD-PRV to enhance attack prompts within the discrete prompt domain. We evaluated\nour method across multiple safety mechanisms of the T2I diffusion model and\nonline servers. Experiments on multiple state-of-the-art safety mechanisms show\nthat DiffZOO attains an 8.5% higher average attack success rate than previous\nworks, hence its promise as a practical red teaming tool for T2l models.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.11071v2",
    "published_date": "2024-08-18 03:16:59 UTC",
    "updated_date": "2025-02-06 04:37:01 UTC"
  },
  {
    "arxiv_id": "2408.09326v1",
    "title": "Characterizing and Evaluating the Reliability of LLMs against Jailbreak Attacks",
    "authors": [
      "Kexin Chen",
      "Yi Liu",
      "Dongxia Wang",
      "Jiaying Chen",
      "Wenhai Wang"
    ],
    "abstract": "Large Language Models (LLMs) have increasingly become pivotal in content\ngeneration with notable societal impact. These models hold the potential to\ngenerate content that could be deemed harmful.Efforts to mitigate this risk\ninclude implementing safeguards to ensure LLMs adhere to social ethics.However,\ndespite such measures, the phenomenon of \"jailbreaking\" -- where carefully\ncrafted prompts elicit harmful responses from models -- persists as a\nsignificant challenge. Recognizing the continuous threat posed by jailbreaking\ntactics and their repercussions for the trustworthy use of LLMs, a rigorous\nassessment of the models' robustness against such attacks is essential. This\nstudy introduces an comprehensive evaluation framework and conducts an\nlarge-scale empirical experiment to address this need. We concentrate on 10\ncutting-edge jailbreak strategies across three categories, 1525 questions from\n61 specific harmful categories, and 13 popular LLMs. We adopt multi-dimensional\nmetrics such as Attack Success Rate (ASR), Toxicity Score, Fluency, Token\nLength, and Grammatical Errors to thoroughly assess the LLMs' outputs under\njailbreak. By normalizing and aggregating these metrics, we present a detailed\nreliability score for different LLMs, coupled with strategic recommendations to\nreduce their susceptibility to such vulnerabilities. Additionally, we explore\nthe relationships among the models, attack strategies, and types of harmful\ncontent, as well as the correlations between the evaluation metrics, which\nproves the validity of our multifaceted evaluation framework. Our extensive\nexperimental results demonstrate a lack of resilience among all tested LLMs\nagainst certain strategies, and highlight the need to concentrate on the\nreliability facets of LLMs. We believe our study can provide valuable insights\ninto enhancing the security evaluation of LLMs against jailbreak within the\ndomain.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.09326v1",
    "published_date": "2024-08-18 01:58:03 UTC",
    "updated_date": "2024-08-18 01:58:03 UTC"
  },
  {
    "arxiv_id": "2408.09312v1",
    "title": "Learning Fair Invariant Representations under Covariate and Correlation Shifts Simultaneously",
    "authors": [
      "Dong Li",
      "Chen Zhao",
      "Minglai Shao",
      "Wenjun Wang"
    ],
    "abstract": "Achieving the generalization of an invariant classifier from training domains\nto shifted test domains while simultaneously considering model fairness is a\nsubstantial and complex challenge in machine learning. Existing methods address\nthe problem of fairness-aware domain generalization, focusing on either\ncovariate shift or correlation shift, but rarely consider both at the same\ntime. In this paper, we introduce a novel approach that focuses on learning a\nfairness-aware domain-invariant predictor within a framework addressing both\ncovariate and correlation shifts simultaneously, ensuring its generalization to\nunknown test domains inaccessible during training. In our approach, data are\nfirst disentangled into content and style factors in latent spaces.\nFurthermore, fairness-aware domain-invariant content representations can be\nlearned by mitigating sensitive information and retaining as much other\ninformation as possible. Extensive empirical studies on benchmark datasets\ndemonstrate that our approach surpasses state-of-the-art methods with respect\nto model accuracy as well as both group and individual fairness.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "CIKM 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.09312v1",
    "published_date": "2024-08-18 00:01:04 UTC",
    "updated_date": "2024-08-18 00:01:04 UTC"
  }
]
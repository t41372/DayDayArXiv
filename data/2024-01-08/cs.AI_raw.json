[
  {
    "arxiv_id": "2401.05443v1",
    "title": "LLM4PLC: Harnessing Large Language Models for Verifiable Programming of PLCs in Industrial Control Systems",
    "authors": [
      "Mohamad Fakih",
      "Rahul Dharmaji",
      "Yasamin Moghaddas",
      "Gustavo Quiros Araya",
      "Oluwatosin Ogundare",
      "Mohammad Abdullah Al Faruque"
    ],
    "abstract": "Although Large Language Models (LLMs) have established pre-dominance in\nautomated code generation, they are not devoid of shortcomings. The pertinent\nissues primarily relate to the absence of execution guarantees for generated\ncode, a lack of explainability, and suboptimal support for essential but niche\nprogramming languages. State-of-the-art LLMs such as GPT-4 and LLaMa2 fail to\nproduce valid programs for Industrial Control Systems (ICS) operated by\nProgrammable Logic Controllers (PLCs). We propose LLM4PLC, a user-guided\niterative pipeline leveraging user feedback and external verification tools\nincluding grammar checkers, compilers and SMV verifiers to guide the LLM's\ngeneration. We further enhance the generation potential of LLM by employing\nPrompt Engineering and model fine-tuning through the creation and usage of\nLoRAs. We validate this system using a FischerTechnik Manufacturing TestBed\n(MFTB), illustrating how LLMs can evolve from generating structurally flawed\ncode to producing verifiably correct programs for industrial applications. We\nrun a complete test suite on GPT-3.5, GPT-4, Code Llama-7B, a fine-tuned Code\nLlama-7B model, Code Llama-34B, and a fine-tuned Code Llama-34B model. The\nproposed pipeline improved the generation success rate from 47% to 72%, and the\nSurvey-of-Experts code quality from 2.25/10 to 7.75/10. To promote open\nresearch, we share the complete experimental setup, the LLM Fine-Tuning\nWeights, and the video demonstrations of the different programs on our\ndedicated webpage.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.PL",
      "D.2.4; I.2.7; I.2.2"
    ],
    "primary_category": "cs.SE",
    "comment": "12 pages; 8 figures; Appearing in the 46th International Conference\n  on Software Engineering: Software Engineering in Practice; for demo website,\n  see https://sites.google.com/uci.edu/llm4plc/home",
    "pdf_url": "http://arxiv.org/pdf/2401.05443v1",
    "published_date": "2024-01-08 23:52:42 UTC",
    "updated_date": "2024-01-08 23:52:42 UTC"
  },
  {
    "arxiv_id": "2401.05442v3",
    "title": "Functional Graphical Models: Structure Enables Offline Data-Driven Optimization",
    "authors": [
      "Jakub Grudzien Kuba",
      "Masatoshi Uehara",
      "Pieter Abbeel",
      "Sergey Levine"
    ],
    "abstract": "While machine learning models are typically trained to solve prediction\nproblems, we might often want to use them for optimization problems. For\nexample, given a dataset of proteins and their corresponding fluorescence\nlevels, we might want to optimize for a new protein with the highest possible\nfluorescence. This kind of data-driven optimization (DDO) presents a range of\nchallenges beyond those in standard prediction problems, since we need models\nthat successfully predict the performance of new designs that are better than\nthe best designs seen in the training set. It is not clear theoretically when\nexisting approaches can even perform better than the naive approach that simply\nselects the best design in the dataset. In this paper, we study how structure\ncan enable sample-efficient data-driven optimization. To formalize the notion\nof structure, we introduce functional graphical models (FGMs) and show\ntheoretically how they can provide for principled data-driven optimization by\ndecomposing the original high-dimensional optimization problem into smaller\nsub-problems. This allows us to derive much more practical regret bounds for\nDDO, and the result implies that DDO with FGMs can achieve nearly optimal\ndesigns in situations where naive approaches fail due to insufficient coverage\nof the offline data. We further present a data-driven optimization algorithm\nthat inferes the FGM structure itself, either over the original input variables\nor a latent variable representation of the inputs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.05442v3",
    "published_date": "2024-01-08 22:33:14 UTC",
    "updated_date": "2024-10-17 00:53:38 UTC"
  },
  {
    "arxiv_id": "2401.04247v2",
    "title": "Attack-Resilient Image Watermarking Using Stable Diffusion",
    "authors": [
      "Lijun Zhang",
      "Xiao Liu",
      "Antoni Viros Martin",
      "Cindy Xiong Bearfield",
      "Yuriy Brun",
      "Hui Guan"
    ],
    "abstract": "Watermarking images is critical for tracking image provenance and proving\nownership. With the advent of generative models, such as stable diffusion, that\ncan create fake but realistic images, watermarking has become particularly\nimportant to make human-created images reliably identifiable. Unfortunately,\nthe very same stable diffusion technology can remove watermarks injected using\nexisting methods. To address this problem, we present ZoDiac, which uses a\npre-trained stable diffusion model to inject a watermark into the trainable\nlatent space, resulting in watermarks that can be reliably detected in the\nlatent vector even when attacked. We evaluate ZoDiac on three benchmarks,\nMS-COCO, DiffusionDB, and WikiArt, and find that ZoDiac is robust against\nstate-of-the-art watermark attacks, with a watermark detection rate above 98%\nand a false positive rate below 6.4%, outperforming state-of-the-art\nwatermarking methods. We hypothesize that the reciprocating denoising process\nin diffusion models may inherently enhance the robustness of the watermark when\nfaced with strong attacks and validate the hypothesis. Our research\ndemonstrates that stable diffusion is a promising approach to robust\nwatermarking, able to withstand even stable-diffusion--based attack methods.\nZoDiac is open-sourced and available at https://github.com/zhanglijun95/ZoDiac.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "28 pages, 11 figures; NeurIPS24",
    "pdf_url": "http://arxiv.org/pdf/2401.04247v2",
    "published_date": "2024-01-08 21:42:56 UTC",
    "updated_date": "2024-10-28 15:02:34 UTC"
  },
  {
    "arxiv_id": "2401.08669v3",
    "title": "Deep Reinforcement Learning for Multi-Truck Vehicle Routing Problems with Multi-Leg Demand Routes",
    "authors": [
      "Joshua Levin",
      "Randall Correll",
      "Takanori Ide",
      "Takafumi Suzuki",
      "Takaho Saito",
      "Alan Arai"
    ],
    "abstract": "Deep reinforcement learning (RL) has been shown to be effective in producing\napproximate solutions to some vehicle routing problems (VRPs), especially when\nusing policies generated by encoder-decoder attention mechanisms. While these\ntechniques have been quite successful for relatively simple problem instances,\nthere are still under-researched and highly complex VRP variants for which no\neffective RL method has been demonstrated. In this work we focus on one such\nVRP variant, which contains multiple trucks and multi-leg routing requirements.\nIn these problems, demand is required to move along sequences of nodes, instead\nof just from a start node to an end node. With the goal of making deep RL a\nviable strategy for real-world industrial-scale supply chain logistics, we\ndevelop new extensions to existing encoder-decoder attention models which allow\nthem to handle multiple trucks and multi-leg routing requirements. Our models\nhave the advantage that they can be trained for a small number of trucks and\nnodes, and then embedded into a large supply chain to yield solutions for\nlarger numbers of trucks and nodes. We test our approach on a real supply chain\nenvironment arising in the operations of Japanese automotive parts manufacturer\nAisin Corporation, and find that our algorithm outperforms Aisin's previous\nbest solution.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper is more appropriate as a revised version of\n  arXiv:2211.17078, so it has been resubmitted as such",
    "pdf_url": "http://arxiv.org/pdf/2401.08669v3",
    "published_date": "2024-01-08 21:13:07 UTC",
    "updated_date": "2024-12-18 16:08:51 UTC"
  },
  {
    "arxiv_id": "2402.00037v1",
    "title": "Catalyzing Equity in STEM Teams: Harnessing Generative AI for Inclusion and Diversity",
    "authors": [
      "Nia Nixon",
      "Yiwen Lin",
      "Lauren Snow"
    ],
    "abstract": "Collaboration is key to STEM, where multidisciplinary team research can solve\ncomplex problems. However, inequality in STEM fields hinders their full\npotential, due to persistent psychological barriers in underrepresented\nstudents' experience. This paper documents teamwork in STEM and explores the\ntransformative potential of computational modeling and generative AI in\npromoting STEM-team diversity and inclusion. Leveraging generative AI, this\npaper outlines two primary areas for advancing diversity, equity, and\ninclusion. First, formalizing collaboration assessment with inclusive analytics\ncan capture fine-grained learner behavior. Second, adaptive, personalized AI\nsystems can support diversity and inclusion in STEM teams. Four policy\nrecommendations highlight AI's capacity: formalized collaborative skill\nassessment, inclusive analytics, funding for socio-cognitive research, human-AI\nteaming for inclusion training. Researchers, educators, policymakers can build\nan equitable STEM ecosystem. This roadmap advances AI-enhanced collaboration,\noffering a vision for the future of STEM where diverse voices are actively\nencouraged and heard within collaborative scientific endeavors.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "21 pages, 0 figure, to be published in Policy Insights from\n  Behavioral and Brain Sciences",
    "pdf_url": "http://arxiv.org/pdf/2402.00037v1",
    "published_date": "2024-01-08 21:10:18 UTC",
    "updated_date": "2024-01-08 21:10:18 UTC"
  },
  {
    "arxiv_id": "2401.04736v1",
    "title": "Exploring Attack Resilience in Distributed Platoon Controllers with Model Predictive Control",
    "authors": [
      "Tashfique Hasnine Choudhury"
    ],
    "abstract": "The extensive use of distributed vehicle platoon controllers has resulted in\nseveral benefits for transportation systems, such as increased traffic flow,\nfuel efficiency, and decreased pollution. The rising reliance on interconnected\nsystems and communication networks, on the other hand, exposes these\ncontrollers to potential cyber-attacks, which may compromise their safety and\nfunctionality. This thesis aims to improve the security of distributed vehicle\nplatoon controllers by investigating attack scenarios and assessing their\ninfluence on system performance. Various attack techniques, including\nman-in-the-middle (MITM) and false data injection (FDI), are simulated using\nModel Predictive Control (MPC) controller to identify vulnerabilities and\nweaknesses of the platoon controller. Countermeasures are offered and tested,\nthat includes attack analysis and reinforced communication protocols using\nMachine Learning techniques for detection. The findings emphasize the\nsignificance of integrating security issues into their design and\nimplementation, which helps to construct safe and resilient distributed platoon\ncontrollers.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.SY",
      "68T99",
      "C.3; I.2"
    ],
    "primary_category": "eess.SY",
    "comment": "Thesis",
    "pdf_url": "http://arxiv.org/pdf/2401.04736v1",
    "published_date": "2024-01-08 20:27:16 UTC",
    "updated_date": "2024-01-08 20:27:16 UTC"
  },
  {
    "arxiv_id": "2401.04210v1",
    "title": "FunnyNet-W: Multimodal Learning of Funny Moments in Videos in the Wild",
    "authors": [
      "Zhi-Song Liu",
      "Robin Courant",
      "Vicky Kalogeiton"
    ],
    "abstract": "Automatically understanding funny moments (i.e., the moments that make people\nlaugh) when watching comedy is challenging, as they relate to various features,\nsuch as body language, dialogues and culture. In this paper, we propose\nFunnyNet-W, a model that relies on cross- and self-attention for visual, audio\nand text data to predict funny moments in videos. Unlike most methods that rely\non ground truth data in the form of subtitles, in this work we exploit\nmodalities that come naturally with videos: (a) video frames as they contain\nvisual information indispensable for scene understanding, (b) audio as it\ncontains higher-level cues associated with funny moments, such as intonation,\npitch and pauses and (c) text automatically extracted with a speech-to-text\nmodel as it can provide rich information when processed by a Large Language\nModel. To acquire labels for training, we propose an unsupervised approach that\nspots and labels funny audio moments. We provide experiments on five datasets:\nthe sitcoms TBBT, MHD, MUStARD, Friends, and the TED talk UR-Funny. Extensive\nexperiments and analysis show that FunnyNet-W successfully exploits visual,\nauditory and textual cues to identify funny moments, while our findings reveal\nFunnyNet-W's ability to predict funny moments in the wild. FunnyNet-W sets the\nnew state of the art for funny moment detection with multimodal cues on all\ndatasets with and without using ground truth information.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.MM",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CV",
    "comment": "22 pages, 14 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.04210v1",
    "published_date": "2024-01-08 19:39:36 UTC",
    "updated_date": "2024-01-08 19:39:36 UTC"
  },
  {
    "arxiv_id": "2401.04206v4",
    "title": "Effects of Multimodal Explanations for Autonomous Driving on Driving Performance, Cognitive Load, Expertise, Confidence, and Trust",
    "authors": [
      "Robert Kaufman",
      "Jean Costa",
      "Everlyne Kimani"
    ],
    "abstract": "Advances in autonomous driving provide an opportunity for AI-assisted driving\ninstruction that directly addresses the critical need for human driving\nimprovement. How should an AI instructor convey information to promote\nlearning? In a pre-post experiment (n = 41), we tested the impact of an AI\nCoach's explanatory communications modeled after performance driving expert\ninstructions. Participants were divided into four (4) groups to assess two (2)\ndimensions of the AI coach's explanations: information type ('what' and\n'why'-type explanations) and presentation modality (auditory and visual). We\ncompare how different explanatory techniques impact driving performance,\ncognitive load, confidence, expertise, and trust via observational learning.\nThrough interview, we delineate participant learning processes. Results show AI\ncoaching can effectively teach performance driving skills to novices. We find\nthe type and modality of information influences performance outcomes.\nDifferences in how successfully participants learned are attributed to how\ninformation directs attention, mitigates uncertainty, and influences overload\nexperienced by participants. Results suggest efficient, modality-appropriate\nexplanations should be opted for when designing effective HMI communications\nthat can instruct without overwhelming. Further, results support the need to\nalign communications with human learning and cognitive processes. We provide\neight design implications for future autonomous vehicle HMI and AI coach\ndesign.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.HC",
    "comment": "14 pages, published in Scientific Reports",
    "pdf_url": "http://arxiv.org/pdf/2401.04206v4",
    "published_date": "2024-01-08 19:33:57 UTC",
    "updated_date": "2024-06-13 17:01:00 UTC"
  },
  {
    "arxiv_id": "2401.04198v1",
    "title": "Curiosity & Entropy Driven Unsupervised RL in Multiple Environments",
    "authors": [
      "Shaurya Dewan",
      "Anisha Jain",
      "Zoe LaLena",
      "Lifan Yu"
    ],
    "abstract": "The authors of 'Unsupervised Reinforcement Learning in Multiple environments'\npropose a method, alpha-MEPOL, to tackle unsupervised RL across multiple\nenvironments. They pre-train a task-agnostic exploration policy using\ninteractions from an entire environment class and then fine-tune this policy\nfor various tasks using supervision. We expanded upon this work, with the goal\nof improving performance. We primarily propose and experiment with five new\nmodifications to the original work: sampling trajectories using an\nentropy-based probability distribution, dynamic alpha, higher KL Divergence\nthreshold, curiosity-driven exploration, and alpha-percentile sampling on\ncuriosity. Dynamic alpha and higher KL-Divergence threshold both provided a\nsignificant improvement over the baseline from the earlier work. PDF-sampling\nfailed to provide any improvement due to it being approximately equivalent to\nthe baseline method when the sample space is small. In high-dimensional\nenvironments, the addition of curiosity-driven exploration enhances learning by\nencouraging the agent to seek diverse experiences and explore the unknown more.\nHowever, its benefits are limited in low-dimensional and simpler environments\nwhere exploration possibilities are constrained and there is little that is\ntruly unknown to the agent. Overall, some of our experiments did boost\nperformance over the baseline and there are a few directions that seem\npromising for further research.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.04198v1",
    "published_date": "2024-01-08 19:25:40 UTC",
    "updated_date": "2024-01-08 19:25:40 UTC"
  },
  {
    "arxiv_id": "2401.04192v1",
    "title": "Interactive Multi-Objective Evolutionary Optimization of Software Architectures",
    "authors": [
      "Aurora Ramírez",
      "José Raúl Romero",
      "Sebastián Ventura"
    ],
    "abstract": "While working on a software specification, designers usually need to evaluate\ndifferent architectural alternatives to be sure that quality criteria are met.\nEven when these quality aspects could be expressed in terms of multiple\nsoftware metrics, other qualitative factors cannot be numerically measured, but\nthey are extracted from the engineer's know-how and prior experiences. In fact,\ndetecting not only strong but also weak points in the different solutions seems\nto fit better with the way humans make their decisions. Putting the human in\nthe loop brings new challenges to the search-based software engineering field,\nespecially for those human-centered activities within the early analysis phase.\nThis paper explores how the interactive evolutionary computation can serve as a\nbasis for integrating the human's judgment into the search process. An\ninteractive approach is proposed to discover software architectures, in which\nboth quantitative and qualitative criteria are applied to guide a\nmulti-objective evolutionary algorithm. The obtained feedback is incorporated\ninto the fitness function using architectural preferences allowing the\nalgorithm to discern between promising and poor solutions. Experimentation with\nreal users has revealed that the proposed interaction mechanism can effectively\nguide the search towards those regions of the search space that are of real\ninterest to the expert.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.NE",
      "68",
      "D.2.11; F.2.2"
    ],
    "primary_category": "cs.SE",
    "comment": "41 pages, 5 figures, journal \"Information Sciences\"",
    "pdf_url": "http://arxiv.org/pdf/2401.04192v1",
    "published_date": "2024-01-08 19:15:40 UTC",
    "updated_date": "2024-01-08 19:15:40 UTC"
  },
  {
    "arxiv_id": "2401.04105v2",
    "title": "Dr$^2$Net: Dynamic Reversible Dual-Residual Networks for Memory-Efficient Finetuning",
    "authors": [
      "Chen Zhao",
      "Shuming Liu",
      "Karttikeya Mangalam",
      "Guocheng Qian",
      "Fatimah Zohra",
      "Abdulmohsen Alghannam",
      "Jitendra Malik",
      "Bernard Ghanem"
    ],
    "abstract": "Large pretrained models are increasingly crucial in modern computer vision\ntasks. These models are typically used in downstream tasks by end-to-end\nfinetuning, which is highly memory-intensive for tasks with high-resolution\ndata, e.g., video understanding, small object detection, and point cloud\nanalysis. In this paper, we propose Dynamic Reversible Dual-Residual Networks,\nor Dr$^2$Net, a novel family of network architectures that acts as a surrogate\nnetwork to finetune a pretrained model with substantially reduced memory\nconsumption. Dr$^2$Net contains two types of residual connections, one\nmaintaining the residual structure in the pretrained models, and the other\nmaking the network reversible. Due to its reversibility, intermediate\nactivations, which can be reconstructed from output, are cleared from memory\nduring training. We use two coefficients on either type of residual connections\nrespectively, and introduce a dynamic training strategy that seamlessly\ntransitions the pretrained model to a reversible network with much higher\nnumerical precision. We evaluate Dr$^2$Net on various pretrained models and\nvarious tasks, and show that it can reach comparable performance to\nconventional finetuning but with significantly less memory usage.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.04105v2",
    "published_date": "2024-01-08 18:59:31 UTC",
    "updated_date": "2024-03-30 08:06:01 UTC"
  },
  {
    "arxiv_id": "2401.06795v2",
    "title": "AI and Generative AI for Research Discovery and Summarization",
    "authors": [
      "Mark Glickman",
      "Yi Zhang"
    ],
    "abstract": "AI and generative AI tools, including chatbots like ChatGPT that rely on\nlarge language models (LLMs), have burst onto the scene this year, creating\nincredible opportunities to increase work productivity and improve our lives.\nStatisticians and data scientists have begun experiencing the benefits from the\navailability of these tools in numerous ways, such as the generation of\nprogramming code from text prompts to analyze data or fit statistical models.\nOne area that these tools can make a substantial impact is in research\ndiscovery and summarization. Standalone tools and plugins to chatbots are being\ndeveloped that allow researchers to more quickly find relevant literature than\npre-2023 search tools. Furthermore, generative AI tools have improved to the\npoint where they can summarize and extract the key points from research\narticles in succinct language. Finally, chatbots based on highly parameterized\nLLMs can be used to simulate abductive reasoning, which provides researchers\nthe ability to make connections among related technical topics, which can also\nbe used for research discovery. We review the developments in AI and generative\nAI for research discovery and summarization, and propose directions where these\ntypes of tools are likely to head in the future that may be of interest to\nstatistician and data scientists.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "29 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.06795v2",
    "published_date": "2024-01-08 18:42:55 UTC",
    "updated_date": "2024-03-26 16:44:34 UTC"
  },
  {
    "arxiv_id": "2401.04081v2",
    "title": "MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts",
    "authors": [
      "Maciej Pióro",
      "Kamil Ciebiera",
      "Krystian Król",
      "Jan Ludziejewski",
      "Michał Krutul",
      "Jakub Krajewski",
      "Szymon Antoniak",
      "Piotr Miłoś",
      "Marek Cygan",
      "Sebastian Jaszczur"
    ],
    "abstract": "State Space Models (SSMs) have become serious contenders in the field of\nsequential modeling, challenging the dominance of Transformers. At the same\ntime, Mixture of Experts (MoE) has significantly improved Transformer-based\nLarge Language Models, including recent state-of-the-art open models. We\npropose that to unlock the potential of SSMs for scaling, they should be\ncombined with MoE. We showcase this on Mamba, a recent SSM-based model that\nachieves remarkable performance. Our model, MoE-Mamba, outperforms both Mamba\nand baseline Transformer-MoE. In particular, MoE-Mamba reaches the same\nperformance as Mamba in $2.35\\times$ fewer training steps while preserving the\ninference performance gains of Mamba against Transformer.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.04081v2",
    "published_date": "2024-01-08 18:35:07 UTC",
    "updated_date": "2024-02-26 17:04:41 UTC"
  },
  {
    "arxiv_id": "2401.04057v1",
    "title": "Unveiling Bias in Fairness Evaluations of Large Language Models: A Critical Literature Review of Music and Movie Recommendation Systems",
    "authors": [
      "Chandan Kumar Sah",
      "Lian Xiaoli",
      "Muhammad Mirajul Islam"
    ],
    "abstract": "The rise of generative artificial intelligence, particularly Large Language\nModels (LLMs), has intensified the imperative to scrutinize fairness alongside\naccuracy. Recent studies have begun to investigate fairness evaluations for\nLLMs within domains such as recommendations. Given that personalization is an\nintrinsic aspect of recommendation systems, its incorporation into fairness\nassessments is paramount. Yet, the degree to which current fairness evaluation\nframeworks account for personalization remains unclear. Our comprehensive\nliterature review aims to fill this gap by examining how existing frameworks\nhandle fairness evaluations of LLMs, with a focus on the integration of\npersonalization factors. Despite an exhaustive collection and analysis of\nrelevant works, we discovered that most evaluations overlook personalization, a\ncritical facet of recommendation systems, thereby inadvertently perpetuating\nunfair practices. Our findings shed light on this oversight and underscore the\nurgent need for more nuanced fairness evaluations that acknowledge\npersonalization. Such improvements are vital for fostering equitable\ndevelopment within the AI community.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.IR",
    "comment": "10 pages",
    "pdf_url": "http://arxiv.org/pdf/2401.04057v1",
    "published_date": "2024-01-08 17:57:29 UTC",
    "updated_date": "2024-01-08 17:57:29 UTC"
  },
  {
    "arxiv_id": "2401.04023v1",
    "title": "Efficient Multiscale Multimodal Bottleneck Transformer for Audio-Video Classification",
    "authors": [
      "Wentao Zhu"
    ],
    "abstract": "In recent years, researchers combine both audio and video signals to deal\nwith challenges where actions are not well represented or captured by visual\ncues. However, how to effectively leverage the two modalities is still under\ndevelopment. In this work, we develop a multiscale multimodal Transformer (MMT)\nthat leverages hierarchical representation learning. Particularly, MMT is\ncomposed of a novel multiscale audio Transformer (MAT) and a multiscale video\nTransformer [43]. To learn a discriminative cross-modality fusion, we further\ndesign multimodal supervised contrastive objectives called audio-video\ncontrastive loss (AVC) and intra-modal contrastive loss (IMC) that robustly\nalign the two modalities. MMT surpasses previous state-of-the-art approaches by\n7.3% and 2.1% on Kinetics-Sounds and VGGSound in terms of the top-1 accuracy\nwithout external training data. Moreover, the proposed MAT significantly\noutperforms AST [28] by 22.2%, 4.4% and 4.7% on three public benchmark\ndatasets, and is about 3% more efficient based on the number of FLOPs and 9.8%\nmore efficient based on GPU memory usage.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by WACV 2024; well-formatted PDF is in\n  https://drive.google.com/file/d/10Zo_ydJZFAm7YsxHDgTjhyc4dEJbW_dk/view?usp=sharing",
    "pdf_url": "http://arxiv.org/pdf/2401.04023v1",
    "published_date": "2024-01-08 17:02:25 UTC",
    "updated_date": "2024-01-08 17:02:25 UTC"
  },
  {
    "arxiv_id": "2401.04154v1",
    "title": "Efficient Selective Audio Masked Multimodal Bottleneck Transformer for Audio-Video Classification",
    "authors": [
      "Wentao Zhu"
    ],
    "abstract": "Audio and video are two most common modalities in the mainstream media\nplatforms, e.g., YouTube. To learn from multimodal videos effectively, in this\nwork, we propose a novel audio-video recognition approach termed audio video\nTransformer, AVT, leveraging the effective spatio-temporal representation by\nthe video Transformer to improve action recognition accuracy. For multimodal\nfusion, simply concatenating multimodal tokens in a cross-modal Transformer\nrequires large computational and memory resources, instead we reduce the\ncross-modality complexity through an audio-video bottleneck Transformer. To\nimprove the learning efficiency of multimodal Transformer, we integrate\nself-supervised objectives, i.e., audio-video contrastive learning, audio-video\nmatching, and masked audio and video learning, into AVT training, which maps\ndiverse audio and video representations into a common multimodal representation\nspace. We further propose a masked audio segment loss to learn semantic audio\nactivities in AVT. Extensive experiments and ablation studies on three public\ndatasets and two in-house datasets consistently demonstrate the effectiveness\nof the proposed AVT. Specifically, AVT outperforms its previous\nstate-of-the-art counterparts on Kinetics-Sounds by 8%. AVT also surpasses one\nof the previous state-of-the-art video Transformers [25] by 10% on VGGSound by\nleveraging the audio signal. Compared to one of the previous state-of-the-art\nmultimodal methods, MBT [32], AVT is 1.3% more efficient in terms of FLOPs and\nimproves the accuracy by 3.8% on Epic-Kitchens-100.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by WACV 2024; well-formatted PDF is in\n  https://drive.google.com/file/d/1qvW52lamsvNGMCqPS7q8g8L4NaR_LlbR/view?usp=sharing.\n  arXiv admin note: text overlap with arXiv:2401.04023",
    "pdf_url": "http://arxiv.org/pdf/2401.04154v1",
    "published_date": "2024-01-08 16:58:59 UTC",
    "updated_date": "2024-01-08 16:58:59 UTC"
  },
  {
    "arxiv_id": "2401.04152v2",
    "title": "Cross-Speaker Encoding Network for Multi-Talker Speech Recognition",
    "authors": [
      "Jiawen Kang",
      "Lingwei Meng",
      "Mingyu Cui",
      "Haohan Guo",
      "Xixin Wu",
      "Xunying Liu",
      "Helen Meng"
    ],
    "abstract": "End-to-end multi-talker speech recognition has garnered great interest as an\neffective approach to directly transcribe overlapped speech from multiple\nspeakers. Current methods typically adopt either 1) single-input\nmultiple-output (SIMO) models with a branched encoder, or 2) single-input\nsingle-output (SISO) models based on attention-based encoder-decoder\narchitecture with serialized output training (SOT). In this work, we propose a\nCross-Speaker Encoding (CSE) network to address the limitations of SIMO models\nby aggregating cross-speaker representations. Furthermore, the CSE model is\nintegrated with SOT to leverage both the advantages of SIMO and SISO while\nmitigating their drawbacks. To the best of our knowledge, this work represents\nan early effort to integrate SIMO and SISO for multi-talker speech recognition.\nExperiments on the two-speaker LibrispeechMix dataset show that the CES model\nreduces word error rate (WER) by 8% over the SIMO baseline. The CSE-SOT model\nreduces WER by 10% overall and by 16% on high-overlap speech compared to the\nSOT model. Code is available at https://github.com/kjw11/CSEnet-ASR.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted by ICASSP2024",
    "pdf_url": "http://arxiv.org/pdf/2401.04152v2",
    "published_date": "2024-01-08 16:37:45 UTC",
    "updated_date": "2024-07-22 12:14:07 UTC"
  },
  {
    "arxiv_id": "2401.04003v3",
    "title": "Simultaneous Task Allocation and Planning for Multi-Robots under Hierarchical Temporal Logic Specifications",
    "authors": [
      "Xusheng Luo",
      "Changliu Liu"
    ],
    "abstract": "Research in robotic planning with temporal logic specifications, such as\nsyntactically co-safe Linear Temporal Logic (sc-LTL), has relied on single\nformulas. However, as task complexity increases, sc-LTL formulas become\nlengthy, making them difficult to interpret and generate, and straining the\ncomputational capacities of planners. To address this, we introduce a\nhierarchical structure to sc-LTL specifications with both syntax and semantics,\nproving it to be more expressive than flat counterparts. We conducted a user\nstudy that compared the flat sc-LTL with our hierarchical version and found\nthat users could more easily comprehend complex tasks using the hierarchical\nstructure. We develop a search-based approach to synthesize plans for\nmulti-robot systems, achieving simultaneous task allocation and planning. This\nmethod approximates the search space by loosely interconnected sub-spaces, each\ncorresponding to an sc-LTL specification. The search primarily focuses on a\nsingle sub-space, transitioning to another under conditions determined by the\ndecomposition of automatons. We develop multiple heuristics to significantly\nexpedite the search. Our theoretical analysis, conducted under mild\nassumptions, addresses completeness and optimality. Compared to existing\nmethods used in various simulators for service tasks, our approach improves\nplanning times while maintaining comparable solution quality.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.FL"
    ],
    "primary_category": "cs.RO",
    "comment": "20 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.04003v3",
    "published_date": "2024-01-08 16:35:13 UTC",
    "updated_date": "2024-08-14 18:30:23 UTC"
  },
  {
    "arxiv_id": "2401.03999v1",
    "title": "Polynomial Precision Dependence Solutions to Alignment Research Center Matrix Completion Problems",
    "authors": [
      "Rico Angell"
    ],
    "abstract": "We present solutions to the matrix completion problems proposed by the\nAlignment Research Center that have a polynomial dependence on the precision\n$\\varepsilon$. The motivation for these problems is to enable efficient\ncomputation of heuristic estimators to formally evaluate and reason about\ndifferent quantities of deep neural networks in the interest of AI alignment.\nOur solutions involve reframing the matrix completion problems as a\nsemidefinite program (SDP) and using recent advances in spectral bundle methods\nfor fast, efficient, and scalable SDP solving.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.03999v1",
    "published_date": "2024-01-08 16:25:45 UTC",
    "updated_date": "2024-01-08 16:25:45 UTC"
  },
  {
    "arxiv_id": "2401.03993v1",
    "title": "Behavioural Cloning in VizDoom",
    "authors": [
      "Ryan Spick",
      "Timothy Bradley",
      "Ayush Raina",
      "Pierluigi Vito Amadori",
      "Guy Moss"
    ],
    "abstract": "This paper describes methods for training autonomous agents to play the game\n\"Doom 2\" through Imitation Learning (IL) using only pixel data as input. We\nalso explore how Reinforcement Learning (RL) compares to IL for humanness by\ncomparing camera movement and trajectory data. Through behavioural cloning, we\nexamine the ability of individual models to learn varying behavioural traits.\nWe attempt to mimic the behaviour of real players with different play styles,\nand find we can train agents that behave aggressively, passively, or simply\nmore human-like than traditional AIs. We propose these methods of introducing\nmore depth and human-like behaviour to agents in video games. The trained IL\nagents perform on par with the average players in our dataset, whilst\noutperforming the worst players. While performance was not as strong as common\nRL approaches, it provides much stronger human-like behavioural traits to the\nagent.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "13 pages",
    "pdf_url": "http://arxiv.org/pdf/2401.03993v1",
    "published_date": "2024-01-08 16:15:43 UTC",
    "updated_date": "2024-01-08 16:15:43 UTC"
  },
  {
    "arxiv_id": "2401.03991v1",
    "title": "Advancing Spatial Reasoning in Large Language Models: An In-Depth Evaluation and Enhancement Using the StepGame Benchmark",
    "authors": [
      "Fangjun Li",
      "David C. Hogg",
      "Anthony G. Cohn"
    ],
    "abstract": "Artificial intelligence (AI) has made remarkable progress across various\ndomains, with large language models like ChatGPT gaining substantial attention\nfor their human-like text-generation capabilities. Despite these achievements,\nspatial reasoning remains a significant challenge for these models. Benchmarks\nlike StepGame evaluate AI spatial reasoning, where ChatGPT has shown\nunsatisfactory performance. However, the presence of template errors in the\nbenchmark has an impact on the evaluation results. Thus there is potential for\nChatGPT to perform better if these template errors are addressed, leading to\nmore accurate assessments of its spatial reasoning capabilities. In this study,\nwe refine the StepGame benchmark, providing a more accurate dataset for model\nevaluation. We analyze GPT's spatial reasoning performance on the rectified\nbenchmark, identifying proficiency in mapping natural language text to spatial\nrelations but limitations in multi-hop reasoning. We provide a flawless\nsolution to the benchmark by combining template-to-relation mapping with\nlogic-based reasoning. This combination demonstrates proficiency in performing\nqualitative reasoning on StepGame without encountering any errors. We then\naddress the limitations of GPT models in spatial reasoning. We deploy\nChain-of-thought and Tree-of-thoughts prompting strategies, offering insights\ninto GPT's ``cognitive process\", and achieving remarkable improvements in\naccuracy. Our investigation not only sheds light on model deficiencies but also\nproposes enhancements, contributing to the advancement of AI with more robust\nspatial reasoning capabilities.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.DB",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "Camera-Ready version for AAAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.03991v1",
    "published_date": "2024-01-08 16:13:08 UTC",
    "updated_date": "2024-01-08 16:13:08 UTC"
  },
  {
    "arxiv_id": "2401.03988v2",
    "title": "A Primer on Temporal Graph Learning",
    "authors": [
      "Aniq Ur Rahman",
      "Justin P. Coon"
    ],
    "abstract": "This document aims to familiarize readers with temporal graph learning (TGL)\nthrough a concept-first approach. We have systematically presented vital\nconcepts essential for understanding the workings of a TGL framework. In\naddition to qualitative explanations, we have incorporated mathematical\nformulations where applicable, enhancing the clarity of the text. Since TGL\ninvolves temporal and spatial learning, we introduce relevant learning\narchitectures ranging from recurrent and convolutional neural networks to\ntransformers and graph neural networks. We also discuss classical time series\nforecasting methods to inspire interpretable learning solutions for TGL.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DM",
      "cs.SI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "19 pages, 47 equations",
    "pdf_url": "http://arxiv.org/pdf/2401.03988v2",
    "published_date": "2024-01-08 16:08:21 UTC",
    "updated_date": "2024-01-09 15:47:35 UTC"
  },
  {
    "arxiv_id": "2401.03955v8",
    "title": "Tiny Time Mixers (TTMs): Fast Pre-trained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series",
    "authors": [
      "Vijay Ekambaram",
      "Arindam Jati",
      "Pankaj Dayama",
      "Sumanta Mukherjee",
      "Nam H. Nguyen",
      "Wesley M. Gifford",
      "Chandra Reddy",
      "Jayant Kalagnanam"
    ],
    "abstract": "Large pre-trained models excel in zero/few-shot learning for language and\nvision tasks but face challenges in multivariate time series (TS) forecasting\ndue to diverse data characteristics. Consequently, recent research efforts have\nfocused on developing pre-trained TS forecasting models. These models, whether\nbuilt from scratch or adapted from large language models (LLMs), excel in\nzero/few-shot forecasting tasks. However, they are limited by slow performance,\nhigh computational demands, and neglect of cross-channel and exogenous\ncorrelations. To address this, we introduce Tiny Time Mixers (TTM), a compact\nmodel (starting from 1M parameters) with effective transfer learning\ncapabilities, trained exclusively on public TS datasets. TTM, based on the\nlight-weight TSMixer architecture, incorporates innovations like adaptive\npatching, diverse resolution sampling, and resolution prefix tuning to handle\npre-training on varied dataset resolutions with minimal model capacity.\nAdditionally, it employs multi-level modeling to capture channel correlations\nand infuse exogenous signals during fine-tuning. TTM outperforms existing\npopular benchmarks in zero/few-shot forecasting by (4-40%), while reducing\ncomputational requirements significantly. Moreover, TTMs are lightweight and\ncan be executed even on CPU-only machines, enhancing usability and fostering\nwider adoption in resource-constrained environments. The model weights for\nreproducibility and research use are available at\nhttps://huggingface.co/ibm/ttm-research-r2/, while enterprise-use weights under\nthe Apache license can be accessed as follows: the initial TTM-Q variant at\nhttps://huggingface.co/ibm-granite/granite-timeseries-ttm-r1, and the latest\nvariants (TTM-B, TTM-E, TTM-A) weights are available at\nhttps://huggingface.co/ibm-granite/granite-timeseries-ttm-r2.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024)",
    "pdf_url": "http://arxiv.org/pdf/2401.03955v8",
    "published_date": "2024-01-08 15:21:21 UTC",
    "updated_date": "2024-11-07 15:07:22 UTC"
  },
  {
    "arxiv_id": "2401.03925v1",
    "title": "Rastro-DM: data mining with a trail",
    "authors": [
      "Marcus Vinicius Borela de Castro",
      "Remis Balaniuk"
    ],
    "abstract": "This paper proposes a methodology for documenting data mining (DM) projects,\nRastro-DM (Trail Data Mining), with a focus not on the model that is generated,\nbut on the processes behind its construction, in order to leave a trail (Rastro\nin Portuguese) of planned actions, training completed, results obtained, and\nlessons learned. The proposed practices are complementary to structuring\nmethodologies of DM, such as CRISP-DM, which establish a methodological and\nparadigmatic framework for the DM process. The application of best practices\nand their benefits is illustrated in a project called 'Cladop' that was created\nfor the classification of PDF documents associated with the investigative\nprocess of damages to the Brazilian Federal Public Treasury. Building the\nRastro-DM kit in the context of a project is a small step that can lead to an\ninstitutional leap to be achieved by sharing and using the trail across the\nenterprise.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.DB",
    "comment": "It was published in the Brazilian Federal Court of Accounts Journal\n  n. 145 on 2021\n  (https://revista.tcu.gov.br/ojs/index.php/RTCU/article/view/1733)",
    "pdf_url": "http://arxiv.org/pdf/2401.03925v1",
    "published_date": "2024-01-08 14:39:21 UTC",
    "updated_date": "2024-01-08 14:39:21 UTC"
  },
  {
    "arxiv_id": "2401.03910v1",
    "title": "A Philosophical Introduction to Language Models -- Part I: Continuity With Classic Debates",
    "authors": [
      "Raphaël Millière",
      "Cameron Buckner"
    ],
    "abstract": "Large language models like GPT-4 have achieved remarkable proficiency in a\nbroad spectrum of language-based tasks, some of which are traditionally\nassociated with hallmarks of human intelligence. This has prompted ongoing\ndisagreements about the extent to which we can meaningfully ascribe any kind of\nlinguistic or cognitive competence to language models. Such questions have deep\nphilosophical roots, echoing longstanding debates about the status of\nartificial neural networks as cognitive models. This article -- the first part\nof two companion papers -- serves both as a primer on language models for\nphilosophers, and as an opinionated survey of their significance in relation to\nclassic debates in the philosophy cognitive science, artificial intelligence,\nand linguistics. We cover topics such as compositionality, language\nacquisition, semantic competence, grounding, world models, and the transmission\nof cultural knowledge. We argue that the success of language models challenges\nseveral long-held assumptions about artificial neural networks. However, we\nalso highlight the need for further empirical investigation to better\nunderstand their internal mechanisms. This sets the stage for the companion\npaper (Part II), which turns to novel empirical methods for probing the inner\nworkings of language models, and new philosophical questions prompted by their\nlatest developments.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.03910v1",
    "published_date": "2024-01-08 14:12:31 UTC",
    "updated_date": "2024-01-08 14:12:31 UTC"
  },
  {
    "arxiv_id": "2401.03890v6",
    "title": "A Survey on 3D Gaussian Splatting",
    "authors": [
      "Guikun Chen",
      "Wenguan Wang"
    ],
    "abstract": "3D Gaussian splatting (GS) has emerged as a transformative technique in\nexplicit radiance field and computer graphics. This innovative approach,\ncharacterized by the use of millions of learnable 3D Gaussians, represents a\nsignificant departure from mainstream neural radiance field approaches, which\npredominantly use implicit, coordinate-based models to map spatial coordinates\nto pixel values. 3D GS, with its explicit scene representation and\ndifferentiable rendering algorithm, not only promises real-time rendering\ncapability but also introduces unprecedented levels of editability. This\npositions 3D GS as a potential game-changer for the next generation of 3D\nreconstruction and representation. In the present paper, we provide the first\nsystematic overview of the recent developments and critical contributions in\nthe domain of 3D GS. We begin with a detailed exploration of the underlying\nprinciples and the driving forces behind the emergence of 3D GS, laying the\ngroundwork for understanding its significance. A focal point of our discussion\nis the practical applicability of 3D GS. By enabling unprecedented rendering\nspeed, 3D GS opens up a plethora of applications, ranging from virtual reality\nto interactive media and beyond. This is complemented by a comparative analysis\nof leading 3D GS models, evaluated across various benchmark tasks to highlight\ntheir performance and practical utility. The survey concludes by identifying\ncurrent challenges and suggesting potential avenues for future research.\nThrough this survey, we aim to provide a valuable resource for both newcomers\nand seasoned researchers, fostering further exploration and advancement in\nexplicit radiance field.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "Ongoing project. Paper list:\n  https://github.com/guikunchen/Awesome3DGS ; Benchmark:\n  https://github.com/guikunchen/3DGS-Benchmarks",
    "pdf_url": "http://arxiv.org/pdf/2401.03890v6",
    "published_date": "2024-01-08 13:42:59 UTC",
    "updated_date": "2025-03-07 13:06:56 UTC"
  },
  {
    "arxiv_id": "2401.03880v1",
    "title": "Metaheuristics for (Variable-Size) Mixed Optimization Problems: A Unified Taxonomy and Survey",
    "authors": [
      "El-Ghazali Talbi"
    ],
    "abstract": "Many real world optimization problems are formulated as mixed-variable\noptimization problems (MVOPs) which involve both continuous and discrete\nvariables. MVOPs including dimensional variables are characterized by a\nvariable-size search space. Depending on the values of dimensional variables,\nthe number and type of the variables of the problem can vary dynamically. MVOPs\nand variable-size MVOPs (VMVOPs) are difficult to solve and raise a number of\nscientific challenges in the design of metaheuristics. Standard metaheuristics\nhave been first designed to address continuous or discrete optimization\nproblems, and are not able to tackle (V)MVOPs in an efficient way. The\ndevelopment of metaheuristics for solving such problems has attracted the\nattention of many researchers and is increasingly popular. However, to our\nknowledge there is no well established taxonomy and comprehensive survey for\nhandling this important family of optimization problems.\n  This paper presents a unified taxonomy for metaheuristic solutions for\nsolving (V)MVOPs in an attempt to provide a common terminology and\nclassification mechanisms. It provides a general mathematical formulation and\nconcepts of (V)MVOPs, and identifies the various solving methodologies than can\nbe applied in metaheuristics. The advantages, the weaknesses and the\nlimitations of the presented methodologies are discussed. The proposed taxonomy\nalso allows to identify some open research issues which needs further in-depth\ninvestigations.",
    "categories": [
      "cs.AI",
      "cs.DM",
      "cs.NE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.03880v1",
    "published_date": "2024-01-08 13:24:55 UTC",
    "updated_date": "2024-01-08 13:24:55 UTC"
  },
  {
    "arxiv_id": "2401.03868v2",
    "title": "FlightLLM: Efficient Large Language Model Inference with a Complete Mapping Flow on FPGAs",
    "authors": [
      "Shulin Zeng",
      "Jun Liu",
      "Guohao Dai",
      "Xinhao Yang",
      "Tianyu Fu",
      "Hongyi Wang",
      "Wenheng Ma",
      "Hanbo Sun",
      "Shiyao Li",
      "Zixiao Huang",
      "Yadong Dai",
      "Jintao Li",
      "Zehao Wang",
      "Ruoyu Zhang",
      "Kairui Wen",
      "Xuefei Ning",
      "Yu Wang"
    ],
    "abstract": "Transformer-based Large Language Models (LLMs) have made a significant impact\non various domains. However, LLMs' efficiency suffers from both heavy\ncomputation and memory overheads. Compression techniques like sparsification\nand quantization are commonly used to mitigate the gap between LLM's\ncomputation/memory overheads and hardware capacity. However, existing GPU and\ntransformer-based accelerators cannot efficiently process compressed LLMs, due\nto the following unresolved challenges: low computational efficiency,\nunderutilized memory bandwidth, and large compilation overheads.\n  This paper proposes FlightLLM, enabling efficient LLMs inference with a\ncomplete mapping flow on FPGAs. In FlightLLM, we highlight an innovative\nsolution that the computation and memory overhead of LLMs can be solved by\nutilizing FPGA-specific resources (e.g., DSP48 and heterogeneous memory\nhierarchy). We propose a configurable sparse DSP chain to support different\nsparsity patterns with high computation efficiency. Second, we propose an\nalways-on-chip decode scheme to boost memory bandwidth with mixed-precision\nsupport. Finally, to make FlightLLM available for real-world LLMs, we propose a\nlength adaptive compilation method to reduce the compilation overhead.\nImplemented on the Xilinx Alveo U280 FPGA, FlightLLM achieves 6.0$\\times$\nhigher energy efficiency and 1.8$\\times$ better cost efficiency against\ncommercial GPUs (e.g., NVIDIA V100S) on modern LLMs (e.g., LLaMA2-7B) using\nvLLM and SmoothQuant under the batch size of one. FlightLLM beats NVIDIA A100\nGPU with 1.2$\\times$ higher throughput using the latest Versal VHK158 FPGA.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "Accepted to FPGA'24",
    "pdf_url": "http://arxiv.org/pdf/2401.03868v2",
    "published_date": "2024-01-08 13:00:53 UTC",
    "updated_date": "2024-01-09 06:47:46 UTC"
  },
  {
    "arxiv_id": "2401.03857v1",
    "title": "Inverse Reinforcement Learning with Sub-optimal Experts",
    "authors": [
      "Riccardo Poiani",
      "Gabriele Curti",
      "Alberto Maria Metelli",
      "Marcello Restelli"
    ],
    "abstract": "Inverse Reinforcement Learning (IRL) techniques deal with the problem of\ndeducing a reward function that explains the behavior of an expert agent who is\nassumed to act optimally in an underlying unknown task. In several problems of\ninterest, however, it is possible to observe the behavior of multiple experts\nwith different degree of optimality (e.g., racing drivers whose skills ranges\nfrom amateurs to professionals). For this reason, in this work, we extend the\nIRL formulation to problems where, in addition to demonstrations from the\noptimal agent, we can observe the behavior of multiple sub-optimal experts.\nGiven this problem, we first study the theoretical properties of the class of\nreward functions that are compatible with a given set of experts, i.e., the\nfeasible reward set. Our results show that the presence of multiple sub-optimal\nexperts can significantly shrink the set of compatible rewards. Furthermore, we\nstudy the statistical complexity of estimating the feasible reward set with a\ngenerative model. To this end, we analyze a uniform sampling algorithm that\nresults in being minimax optimal whenever the sub-optimal experts' performance\nlevel is sufficiently close to the one of the optimal agent.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.03857v1",
    "published_date": "2024-01-08 12:39:25 UTC",
    "updated_date": "2024-01-08 12:39:25 UTC"
  },
  {
    "arxiv_id": "2401.03855v4",
    "title": "PythonSaga: Redefining the Benchmark to Evaluate Code Generating LLMs",
    "authors": [
      "Ankit Yadav",
      "Himanshu Beniwal",
      "Mayank Singh"
    ],
    "abstract": "Driven by the surge in code generation using large language models (LLMs),\nnumerous benchmarks have emerged to evaluate these LLMs capabilities. We\nconducted a large-scale human evaluation of HumanEval and MBPP, two popular\nbenchmarks for Python code generation, analyzing their diversity and\ndifficulty. Our findings unveil a critical bias towards a limited set of\nprogramming concepts, neglecting most of the other concepts entirely.\nFurthermore, we uncover a worrying prevalence of easy tasks, potentially\ninflating model performance estimations. To address these limitations, we\npropose a novel benchmark, PythonSaga, featuring 185 hand-crafted prompts on a\nbalanced representation of 38 programming concepts across diverse difficulty\nlevels. The robustness of our benchmark is demonstrated by the poor performance\nof existing Code-LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.03855v4",
    "published_date": "2024-01-08 12:36:43 UTC",
    "updated_date": "2024-07-04 05:40:42 UTC"
  },
  {
    "arxiv_id": "2401.03854v2",
    "title": "TIER: Text-Image Encoder-based Regression for AIGC Image Quality Assessment",
    "authors": [
      "Jiquan Yuan",
      "Xinyan Cao",
      "Jinming Che",
      "Qinyuan Wang",
      "Sen Liang",
      "Wei Ren",
      "Jinlong Lin",
      "Xixin Cao"
    ],
    "abstract": "Recently, AIGC image quality assessment (AIGCIQA), which aims to assess the\nquality of AI-generated images (AIGIs) from a human perception perspective, has\nemerged as a new topic in computer vision. Unlike common image quality\nassessment tasks where images are derived from original ones distorted by\nnoise, blur, and compression, \\textit{etc.}, in AIGCIQA tasks, images are\ntypically generated by generative models using text prompts. Considerable\nefforts have been made in the past years to advance AIGCIQA. However, most\nexisting AIGCIQA methods regress predicted scores directly from individual\ngenerated images, overlooking the information contained in the text prompts of\nthese images. This oversight partially limits the performance of these AIGCIQA\nmethods. To address this issue, we propose a text-image encoder-based\nregression (TIER) framework. Specifically, we process the generated images and\ntheir corresponding text prompts as inputs, utilizing a text encoder and an\nimage encoder to extract features from these text prompts and generated images,\nrespectively. To demonstrate the effectiveness of our proposed TIER method, we\nconduct extensive experiments on several mainstream AIGCIQA databases,\nincluding AGIQA-1K, AGIQA-3K, and AIGCIQA2023. The experimental results\nindicate that our proposed TIER method generally demonstrates superior\nperformance compared to baseline in most cases.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 8 figures. arXiv admin note: text overlap with\n  arXiv:2312.05897",
    "pdf_url": "http://arxiv.org/pdf/2401.03854v2",
    "published_date": "2024-01-08 12:35:15 UTC",
    "updated_date": "2024-01-11 08:09:33 UTC"
  },
  {
    "arxiv_id": "2401.04148v1",
    "title": "Online Test-Time Adaptation of Spatial-Temporal Traffic Flow Forecasting",
    "authors": [
      "Pengxin Guo",
      "Pengrong Jin",
      "Ziyue Li",
      "Lei Bai",
      "Yu Zhang"
    ],
    "abstract": "Accurate spatial-temporal traffic flow forecasting is crucial in aiding\ntraffic managers in implementing control measures and assisting drivers in\nselecting optimal travel routes. Traditional deep-learning based methods for\ntraffic flow forecasting typically rely on historical data to train their\nmodels, which are then used to make predictions on future data. However, the\nperformance of the trained model usually degrades due to the temporal drift\nbetween the historical and future data. To make the model trained on historical\ndata better adapt to future data in a fully online manner, this paper conducts\nthe first study of the online test-time adaptation techniques for\nspatial-temporal traffic flow forecasting problems. To this end, we propose an\nAdaptive Double Correction by Series Decomposition (ADCSD) method, which first\ndecomposes the output of the trained model into seasonal and trend-cyclical\nparts and then corrects them by two separate modules during the testing phase\nusing the latest observed data entry by entry. In the proposed ADCSD method,\ninstead of fine-tuning the whole trained model during the testing phase, a lite\nnetwork is attached after the trained model, and only the lite network is\nfine-tuned in the testing process each time a data entry is observed. Moreover,\nto satisfy that different time series variables may have different levels of\ntemporal drift, two adaptive vectors are adopted to provide different weights\nfor different time series variables. Extensive experiments on four real-world\ntraffic flow forecasting datasets demonstrate the effectiveness of the proposed\nADCSD method. The code is available at https://github.com/Pengxin-Guo/ADCSD.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.04148v1",
    "published_date": "2024-01-08 12:04:39 UTC",
    "updated_date": "2024-01-08 12:04:39 UTC"
  },
  {
    "arxiv_id": "2401.03830v1",
    "title": "A foundation for exact binarized morphological neural networks",
    "authors": [
      "Theodore Aouad",
      "Hugues Talbot"
    ],
    "abstract": "Training and running deep neural networks (NNs) often demands a lot of\ncomputation and energy-intensive specialized hardware (e.g. GPU, TPU...). One\nway to reduce the computation and power cost is to use binary weight NNs, but\nthese are hard to train because the sign function has a non-smooth gradient. We\npresent a model based on Mathematical Morphology (MM), which can binarize\nConvNets without losing performance under certain conditions, but these\nconditions may not be easy to satisfy in real-world scenarios. To solve this,\nwe propose two new approximation methods and develop a robust theoretical\nframework for ConvNets binarization using MM. We propose as well regularization\nlosses to improve the optimization. We empirically show that our model can\nlearn a complex morphological network, and explore its performance on a\nclassification task.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at conference ICCV 2023 Workshop LBQNN. Same work, different\n  format, accepted at conference NeurIPS 2023 Workshop WANT. 8 pages, 17 pages\n  appendix",
    "pdf_url": "http://arxiv.org/pdf/2401.03830v1",
    "published_date": "2024-01-08 11:37:44 UTC",
    "updated_date": "2024-01-08 11:37:44 UTC"
  },
  {
    "arxiv_id": "2403.12055v1",
    "title": "Deep learning based detection of collateral circulation in coronary angiographies",
    "authors": [
      "Cosmin-Andrei Hatfaludi",
      "Daniel Bunescu",
      "Costin Florian Ciusdel",
      "Alex Serban",
      "Karl Bose",
      "Marc Oppel",
      "Stephanie Schroder",
      "Christopher Seehase",
      "Harald F. Langer",
      "Jeanette Erdmann",
      "Henry Nording",
      "Lucian Mihai Itu"
    ],
    "abstract": "Coronary artery disease (CAD) is the dominant cause of death and\nhospitalization across the globe. Atherosclerosis, an inflammatory condition\nthat gradually narrows arteries and has potentially fatal effects, is the most\nfrequent cause of CAD. Nonetheless, the circulation regularly adapts in the\npresence of atherosclerosis, through the formation of collateral arteries,\nresulting in significant long-term health benefits. Therefore, timely detection\nof coronary collateral circulation (CCC) is crucial for CAD personalized\nmedicine. We propose a novel deep learning based method to detect CCC in\nangiographic images. Our method relies on a convolutional backbone to extract\nspatial features from each frame of an angiography sequence. The features are\nthen concatenated, and subsequently processed by another convolutional layer\nthat processes embeddings temporally. Due to scarcity of data, we also\nexperiment with pretraining the backbone on coronary artery segmentation, which\nimproves the results consistently. Moreover, we experiment with few-shot\nlearning to further improve performance, given our low data regime. We present\nour results together with subgroup analyses based on Rentrop grading,\ncollateral flow, and collateral grading, which provide valuable insights into\nmodel performance. Overall, the proposed method shows promising results in\ndetecting CCC, and can be further extended to perform landmark based CCC\ndetection and CCC quantification.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.12055v1",
    "published_date": "2024-01-08 11:25:42 UTC",
    "updated_date": "2024-01-08 11:25:42 UTC"
  },
  {
    "arxiv_id": "2401.03804v2",
    "title": "TeleChat Technical Report",
    "authors": [
      "Zhongjiang He",
      "Zihan Wang",
      "Xinzhang Liu",
      "Shixuan Liu",
      "Yitong Yao",
      "Yuyao Huang",
      "Xuelong Li",
      "Yongxiang Li",
      "Zhonghao Che",
      "Zhaoxi Zhang",
      "Yan Wang",
      "Xin Wang",
      "Luwen Pu",
      "Huinan Xu",
      "Ruiyu Fang",
      "Yu Zhao",
      "Jie Zhang",
      "Xiaomeng Huang",
      "Zhilong Lu",
      "Jiaxin Peng",
      "Wenjun Zheng",
      "Shiquan Wang",
      "Bingkai Yang",
      "Xuewei he",
      "Zhuoru Jiang",
      "Qiyi Xie",
      "Yanhan Zhang",
      "Zhongqiu Li",
      "Lingling Shi",
      "Weiwei Fu",
      "Yin Zhang",
      "Zilu Huang",
      "Sishi Xiong",
      "Yuxiang Zhang",
      "Chao Wang",
      "Shuangyong Song"
    ],
    "abstract": "In this technical report, we present TeleChat, a collection of large language\nmodels (LLMs) with parameters of 3 billion, 7 billion and 12 billion. It\nincludes pretrained language models as well as fine-tuned chat models that is\naligned with human preferences. TeleChat is initially pretrained on an\nextensive corpus containing a diverse collection of texts from both English and\nChinese languages, including trillions of tokens. Subsequently, the model\nundergoes fine-tuning to align with human preferences, following a detailed\nmethodology that we describe. We evaluate the performance of TeleChat on\nvarious tasks, including language understanding, mathematics, reasoning, code\ngeneration, and knowledge-based question answering. Our findings indicate that\nTeleChat achieves comparable performance to other open-source models of similar\nsize across a wide range of public benchmarks. To support future research and\napplications utilizing LLMs, we release the fine-tuned model checkpoints of\nTeleChat's 7B and 12B variant, along with code and a portion of our pretraining\ndata, to the public community.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "28 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.03804v2",
    "published_date": "2024-01-08 10:43:19 UTC",
    "updated_date": "2024-04-02 01:45:11 UTC"
  },
  {
    "arxiv_id": "2401.06686v1",
    "title": "Exploring Conversational Agents as an Effective Tool for Measuring Cognitive Biases in Decision-Making",
    "authors": [
      "Stephen Pilli"
    ],
    "abstract": "Heuristics and cognitive biases are an integral part of human\ndecision-making. Automatically detecting a particular cognitive bias could\nenable intelligent tools to provide better decision-support. Detecting the\npresence of a cognitive bias currently requires a hand-crafted experiment and\nhuman interpretation. Our research aims to explore conversational agents as an\neffective tool to measure various cognitive biases in different domains. Our\nproposed conversational agent incorporates a bias measurement mechanism that is\ninformed by the existing experimental designs and various experimental tasks\nidentified in the literature. Our initial experiments to measure framing and\nloss-aversion biases indicate that the conversational agents can be effectively\nused to measure the biases.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.06686v1",
    "published_date": "2024-01-08 10:23:52 UTC",
    "updated_date": "2024-01-08 10:23:52 UTC"
  },
  {
    "arxiv_id": "2401.03786v2",
    "title": "Long-term Safe Reinforcement Learning with Binary Feedback",
    "authors": [
      "Akifumi Wachi",
      "Wataru Hashimoto",
      "Kazumune Hashimoto"
    ],
    "abstract": "Safety is an indispensable requirement for applying reinforcement learning\n(RL) to real problems. Although there has been a surge of safe RL algorithms\nproposed in recent years, most existing work typically 1) relies on receiving\nnumeric safety feedback; 2) does not guarantee safety during the learning\nprocess; 3) limits the problem to a priori known, deterministic transition\ndynamics; and/or 4) assume the existence of a known safe policy for any states.\nAddressing the issues mentioned above, we thus propose Long-term Binaryfeedback\nSafe RL (LoBiSaRL), a safe RL algorithm for constrained Markov decision\nprocesses (CMDPs) with binary safety feedback and an unknown, stochastic state\ntransition function. LoBiSaRL optimizes a policy to maximize rewards while\nguaranteeing a long-term safety that an agent executes only safe state-action\npairs throughout each episode with high probability. Specifically, LoBiSaRL\nmodels the binary safety function via a generalized linear model (GLM) and\nconservatively takes only a safe action at every time step while inferring its\neffect on future safety under proper assumptions. Our theoretical results show\nthat LoBiSaRL guarantees the long-term safety constraint, with high\nprobability. Finally, our empirical results demonstrate that our algorithm is\nsafer than existing methods without significantly compromising performance in\nterms of reward.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to AAAI-24",
    "pdf_url": "http://arxiv.org/pdf/2401.03786v2",
    "published_date": "2024-01-08 10:07:31 UTC",
    "updated_date": "2024-01-11 11:59:25 UTC"
  },
  {
    "arxiv_id": "2401.03768v4",
    "title": "Corn Yield Prediction Model with Deep Neural Networks for Smallholder Farmer Decision Support System",
    "authors": [
      "Chollette C. Olisah",
      "Lyndon Smith",
      "Melvyn Smith",
      "Morolake O. Lawrence",
      "Osita Ojukwu"
    ],
    "abstract": "Crop yield prediction has been modeled on the assumption that there is no\ninteraction between weather and soil variables. However, this paper argues that\nan interaction exists, and it can be finely modelled using the Kendall\nCorrelation coefficient. Given the nonlinearity of the interaction between\nweather and soil variables, a deep neural network regressor (DNNR) is carefully\ndesigned with consideration to the depth, number of neurons of the hidden\nlayers, and the hyperparameters with their optimizations. Additionally, a new\nmetric, the average of absolute root squared error (ARSE) is proposed to\ncombine the strengths of root mean square error (RMSE) and mean absolute error\n(MAE). With the ARSE metric, the proposed DNNR(s), optimised random forest\nregressor (RFR) and the extreme gradient boosting regressor (XGBR) achieved\nimpressively small yield errors, 0.0172 t/ha, and 0.0243 t/ha, 0.0001 t/ha, and\n0.001 t/ha, respectively. However, the DNNR(s), with changes to the explanatory\nvariables to ensure generalizability to unforeseen data, DNNR(s) performed\nbest. Further analysis reveals that a strong interaction does exist between\nweather and soil variables. Precisely, yield is observed to increase when\nprecipitation is reduced and silt increased, and vice-versa. However, the\ndegree of decrease or increase is not quantified in this paper. Contrary to\nexisting yield models targeted towards agricultural policies and global food\nsecurity, the goal of the proposed corn yield model is to empower the\nsmallholder farmer to farm smartly and intelligently, thus the prediction model\nis integrated into a mobile application that includes education, and a\nfarmer-to-market access module.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "30 Pages, 11 Figures, 3 Tables",
    "pdf_url": "http://arxiv.org/pdf/2401.03768v4",
    "published_date": "2024-01-08 09:47:19 UTC",
    "updated_date": "2024-12-01 08:43:03 UTC"
  },
  {
    "arxiv_id": "2401.03756v3",
    "title": "Adaptive Experimental Design for Policy Learning",
    "authors": [
      "Masahiro Kato",
      "Kyohei Okumura",
      "Takuya Ishihara",
      "Toru Kitagawa"
    ],
    "abstract": "Evidence-based targeting has been a topic of growing interest among the\npractitioners of policy and business. Formulating decision-maker's policy\nlearning as a fixed-budget best arm identification (BAI) problem with\ncontextual information, we study an optimal adaptive experimental design for\npolicy learning with multiple treatment arms. In the sampling stage, the\nplanner assigns treatment arms adaptively over sequentially arriving\nexperimental units upon observing their contextual information (covariates).\nAfter the experiment, the planner recommends an individualized assignment rule\nto the population. Setting the worst-case expected regret as the performance\ncriterion of adaptive sampling and recommended policies, we derive its\nasymptotic lower bounds, and propose a strategy, Adaptive Sampling-Policy\nLearning strategy (PLAS), whose leading factor of the regret upper bound aligns\nwith the lower bound as the size of experimental units increases.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "econ.EM",
      "stat.ME",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "arXiv admin note: text overlap with arXiv:2302.02988",
    "pdf_url": "http://arxiv.org/pdf/2401.03756v3",
    "published_date": "2024-01-08 09:29:07 UTC",
    "updated_date": "2024-02-08 17:41:43 UTC"
  },
  {
    "arxiv_id": "2401.06793v1",
    "title": "Greedy Algorithm for Inference of Decision Trees from Decision Rule Systems",
    "authors": [
      "Kerven Durdymyradov",
      "Mikhail Moshkov"
    ],
    "abstract": "Decision trees and decision rule systems play important roles as classifiers,\nknowledge representation tools, and algorithms. They are easily interpretable\nmodels for data analysis, making them widely used and studied in computer\nscience. Understanding the relationships between these two models is an\nimportant task in this field. There are well-known methods for converting\ndecision trees into systems of decision rules. In this paper, we consider the\ninverse transformation problem, which is not so simple. Instead of constructing\nan entire decision tree, our study focuses on a greedy polynomial time\nalgorithm that simulates the operation of a decision tree on a given tuple of\nattribute values.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "arXiv admin note: substantial text overlap with arXiv:2305.01721,\n  arXiv:2302.07063",
    "pdf_url": "http://arxiv.org/pdf/2401.06793v1",
    "published_date": "2024-01-08 09:28:55 UTC",
    "updated_date": "2024-01-08 09:28:55 UTC"
  },
  {
    "arxiv_id": "2401.03737v2",
    "title": "Can Large Language Models Beat Wall Street? Unveiling the Potential of AI in Stock Selection",
    "authors": [
      "Georgios Fatouros",
      "Konstantinos Metaxas",
      "John Soldatos",
      "Dimosthenis Kyriazis"
    ],
    "abstract": "This paper introduces MarketSenseAI, an innovative framework leveraging\nGPT-4's advanced reasoning for selecting stocks in financial markets. By\nintegrating Chain of Thought and In-Context Learning, MarketSenseAI analyzes\ndiverse data sources, including market trends, news, fundamentals, and\nmacroeconomic factors, to emulate expert investment decision-making. The\ndevelopment, implementation, and validation of the framework are elaborately\ndiscussed, underscoring its capability to generate actionable and interpretable\ninvestment signals. A notable feature of this work is employing GPT-4 both as a\npredictive mechanism and signal evaluator, revealing the significant impact of\nthe AI-generated explanations on signal accuracy, reliability and acceptance.\nThrough empirical testing on the competitive S&P 100 stocks over a 15-month\nperiod, MarketSenseAI demonstrated exceptional performance, delivering excess\nalpha of 10% to 30% and achieving a cumulative return of up to 72% over the\nperiod, while maintaining a risk profile comparable to the broader market. Our\nfindings highlight the transformative potential of Large Language Models in\nfinancial decision-making, marking a significant leap in integrating generative\nAI into financial analytics and investment strategies.",
    "categories": [
      "q-fin.CP",
      "cs.AI",
      "cs.CE",
      "cs.CL",
      "cs.LG",
      "68T07, 68T50, 91G10, 91G15",
      "I.2.1; I.2.7; J.4"
    ],
    "primary_category": "q-fin.CP",
    "comment": "17 pages, 12 figures, 12 tables",
    "pdf_url": "http://arxiv.org/pdf/2401.03737v2",
    "published_date": "2024-01-08 08:58:46 UTC",
    "updated_date": "2024-04-04 13:18:55 UTC"
  },
  {
    "arxiv_id": "2401.03729v3",
    "title": "The Butterfly Effect of Altering Prompts: How Small Changes and Jailbreaks Affect Large Language Model Performance",
    "authors": [
      "Abel Salinas",
      "Fred Morstatter"
    ],
    "abstract": "Large Language Models (LLMs) are regularly being used to label data across\nmany domains and for myriad tasks. By simply asking the LLM for an answer, or\n``prompting,'' practitioners are able to use LLMs to quickly get a response for\nan arbitrary task. This prompting is done through a series of decisions by the\npractitioner, from simple wording of the prompt, to requesting the output in a\ncertain data format, to jailbreaking in the case of prompts that address more\nsensitive topics. In this work, we ask: do variations in the way a prompt is\nconstructed change the ultimate decision of the LLM? We answer this using a\nseries of prompt variations across a variety of text classification tasks. We\nfind that even the smallest of perturbations, such as adding a space at the end\nof a prompt, can cause the LLM to change its answer. Further, we find that\nrequesting responses in XML and commonly used jailbreaks can have cataclysmic\neffects on the data labeled by LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.03729v3",
    "published_date": "2024-01-08 08:28:08 UTC",
    "updated_date": "2024-04-01 20:56:11 UTC"
  },
  {
    "arxiv_id": "2401.05437v2",
    "title": "Representation Learning for Wearable-Based Applications in the Case of Missing Data",
    "authors": [
      "Janosch Jungo",
      "Yutong Xiang",
      "Shkurta Gashi",
      "Christian Holz"
    ],
    "abstract": "Wearable devices continuously collect sensor data and use it to infer an\nindividual's behavior, such as sleep, physical activity, and emotions. Despite\nthe significant interest and advancements in this field, modeling multimodal\nsensor data in real-world environments is still challenging due to low data\nquality and limited data annotations. In this work, we investigate\nrepresentation learning for imputing missing wearable data and compare it with\nstate-of-the-art statistical approaches. We investigate the performance of the\ntransformer model on 10 physiological and behavioral signals with different\nmasking ratios. Our results show that transformers outperform baselines for\nmissing data imputation of signals that change more frequently, but not for\nmonotonic signals. We further investigate the impact of imputation strategies\nand masking rations on downstream classification tasks. Our study provides\ninsights for the design and development of masking-based self-supervised\nlearning tasks and advocates the adoption of hybrid-based imputation strategies\nto address the challenge of missing data in wearable devices.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "Paper accepted in Human-Centric Representation Learning workshop at\n  AAAI 2024 (https://hcrl-workshop.github.io/2024/)",
    "pdf_url": "http://arxiv.org/pdf/2401.05437v2",
    "published_date": "2024-01-08 08:21:37 UTC",
    "updated_date": "2024-01-12 11:14:58 UTC"
  },
  {
    "arxiv_id": "2401.03717v3",
    "title": "Universal Time-Series Representation Learning: A Survey",
    "authors": [
      "Patara Trirat",
      "Yooju Shin",
      "Junhyeok Kang",
      "Youngeun Nam",
      "Jihye Na",
      "Minyoung Bae",
      "Joeun Kim",
      "Byunghyun Kim",
      "Jae-Gil Lee"
    ],
    "abstract": "Time-series data exists in every corner of real-world systems and services,\nranging from satellites in the sky to wearable devices on human bodies.\nLearning representations by extracting and inferring valuable information from\nthese time series is crucial for understanding the complex dynamics of\nparticular phenomena and enabling informed decisions. With the learned\nrepresentations, we can perform numerous downstream analyses more effectively.\nAmong several approaches, deep learning has demonstrated remarkable performance\nin extracting hidden patterns and features from time-series data without manual\nfeature engineering. This survey first presents a novel taxonomy based on three\nfundamental elements in designing state-of-the-art universal representation\nlearning methods for time series. According to the proposed taxonomy, we\ncomprehensively review existing studies and discuss their intuitions and\ninsights into how these methods enhance the quality of learned representations.\nFinally, as a guideline for future studies, we summarize commonly used\nexperimental setups and datasets and discuss several promising research\ndirections. An up-to-date corresponding resource is available at\nhttps://github.com/itouchz/awesome-deep-time-series-representations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "41 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.03717v3",
    "published_date": "2024-01-08 08:00:04 UTC",
    "updated_date": "2024-08-27 19:45:07 UTC"
  },
  {
    "arxiv_id": "2401.03695v2",
    "title": "A Large-Scale Empirical Study on Improving the Fairness of Image Classification Models",
    "authors": [
      "Junjie Yang",
      "Jiajun Jiang",
      "Zeyu Sun",
      "Junjie Chen"
    ],
    "abstract": "Fairness has been a critical issue that affects the adoption of deep learning\nmodels in real practice. To improve model fairness, many existing methods have\nbeen proposed and evaluated to be effective in their own contexts. However,\nthere is still no systematic evaluation among them for a comprehensive\ncomparison under the same context, which makes it hard to understand the\nperformance distinction among them, hindering the research progress and\npractical adoption of them. To fill this gap, this paper endeavours to conduct\nthe first large-scale empirical study to comprehensively compare the\nperformance of existing state-of-the-art fairness improving techniques.\nSpecifically, we target the widely-used application scenario of image\nclassification, and utilized three different datasets and five commonly-used\nperformance metrics to assess in total 13 methods from diverse categories. Our\nfindings reveal substantial variations in the performance of each method across\ndifferent datasets and sensitive attributes, indicating over-fitting on\nspecific datasets by many existing methods. Furthermore, different fairness\nevaluation metrics, due to their distinct focuses, yield significantly\ndifferent assessment results. Overall, we observe that pre-processing methods\nand in-processing methods outperform post-processing methods, with\npre-processing methods exhibiting the best performance. Our empirical study\noffers comprehensive recommendations for enhancing fairness in deep learning\nmodels. We approach the problem from multiple dimensions, aiming to provide a\nuniform evaluation platform and inspire researchers to explore more effective\nfairness solutions via a set of implications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by the 33rd ACM SIGSOFT International Symposium on Software\n  Testing and Analysis (ISSTA 2024). Please include ISSTA in any citations",
    "pdf_url": "http://arxiv.org/pdf/2401.03695v2",
    "published_date": "2024-01-08 06:53:33 UTC",
    "updated_date": "2024-03-24 02:03:55 UTC"
  },
  {
    "arxiv_id": "2401.03694v1",
    "title": "GloTSFormer: Global Video Text Spotting Transformer",
    "authors": [
      "Han Wang",
      "Yanjie Wang",
      "Yang Li",
      "Can Huang"
    ],
    "abstract": "Video Text Spotting (VTS) is a fundamental visual task that aims to predict\nthe trajectories and content of texts in a video. Previous works usually\nconduct local associations and apply IoU-based distance and complex\npost-processing procedures to boost performance, ignoring the abundant temporal\ninformation and the morphological characteristics in VTS. In this paper, we\npropose a novel Global Video Text Spotting Transformer GloTSFormer to model the\ntracking problem as global associations and utilize the Gaussian Wasserstein\ndistance to guide the morphological correlation between frames. Our main\ncontributions can be summarized as three folds. 1). We propose a\nTransformer-based global tracking method GloTSFormer for VTS and associate\nmultiple frames simultaneously. 2). We introduce a Wasserstein distance-based\nmethod to conduct positional associations between frames. 3). We conduct\nextensive experiments on public datasets. On the ICDAR2015 video dataset,\nGloTSFormer achieves 56.0 MOTA with 4.6 absolute improvement compared with the\nprevious SOTA method and outperforms the previous Transformer-based method by a\nsignificant 8.3 MOTA.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.03694v1",
    "published_date": "2024-01-08 06:52:16 UTC",
    "updated_date": "2024-01-08 06:52:16 UTC"
  },
  {
    "arxiv_id": "2401.03676v1",
    "title": "Assessing AI Detectors in Identifying AI-Generated Code: Implications for Education",
    "authors": [
      "Wei Hung Pan",
      "Ming Jie Chok",
      "Jonathan Leong Shan Wong",
      "Yung Xin Shin",
      "Yeong Shian Poon",
      "Zhou Yang",
      "Chun Yong Chong",
      "David Lo",
      "Mei Kuan Lim"
    ],
    "abstract": "Educators are increasingly concerned about the usage of Large Language Models\n(LLMs) such as ChatGPT in programming education, particularly regarding the\npotential exploitation of imperfections in Artificial Intelligence Generated\nContent (AIGC) Detectors for academic misconduct. In this paper, we present an\nempirical study where the LLM is examined for its attempts to bypass detection\nby AIGC Detectors. This is achieved by generating code in response to a given\nquestion using different variants. We collected a dataset comprising 5,069\nsamples, with each sample consisting of a textual description of a coding\nproblem and its corresponding human-written Python solution codes. These\nsamples were obtained from various sources, including 80 from Quescol, 3,264\nfrom Kaggle, and 1,725 from LeetCode. From the dataset, we created 13 sets of\ncode problem variant prompts, which were used to instruct ChatGPT to generate\nthe outputs. Subsequently, we assessed the performance of five AIGC detectors.\nOur results demonstrate that existing AIGC Detectors perform poorly in\ndistinguishing between human-written code and AI-generated code.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "11 pages, paper accepted at 46th International Conference on Software\n  Engineering, Software Engineering Education and Training Track (ICSE-SEET\n  2024)",
    "pdf_url": "http://arxiv.org/pdf/2401.03676v1",
    "published_date": "2024-01-08 05:53:52 UTC",
    "updated_date": "2024-01-08 05:53:52 UTC"
  },
  {
    "arxiv_id": "2401.06792v2",
    "title": "LightHouse: A Survey of AGI Hallucination",
    "authors": [
      "Feng Wang"
    ],
    "abstract": "With the development of artificial intelligence, large-scale models have\nbecome increasingly intelligent. However, numerous studies indicate that\nhallucinations within these large models are a bottleneck hindering the\ndevelopment of AI research. In the pursuit of achieving strong artificial\nintelligence, a significant volume of research effort is being invested in the\nAGI (Artificial General Intelligence) hallucination research. Previous\nexplorations have been conducted in researching hallucinations within LLMs\n(Large Language Models). As for multimodal AGI, research on hallucinations is\nstill in an early stage. To further the progress of research in the domain of\nhallucinatory phenomena, we present a bird's eye view of hallucinations in AGI,\nsummarizing the current work on AGI hallucinations and proposing some\ndirections for future research.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.06792v2",
    "published_date": "2024-01-08 03:52:40 UTC",
    "updated_date": "2024-01-17 04:40:13 UTC"
  },
  {
    "arxiv_id": "2401.06791v1",
    "title": "A Span-based Model for Extracting Overlapping PICO Entities from RCT Publications",
    "authors": [
      "Gongbo Zhang",
      "Yiliang Zhou",
      "Yan Hu",
      "Hua Xu",
      "Chunhua Weng",
      "Yifan Peng"
    ],
    "abstract": "Objectives Extraction of PICO (Populations, Interventions, Comparison, and\nOutcomes) entities is fundamental to evidence retrieval. We present a novel\nmethod PICOX to extract overlapping PICO entities.\n  Materials and Methods PICOX first identifies entities by assessing whether a\nword marks the beginning or conclusion of an entity. Then it uses a multi-label\nclassifier to assign one or more PICO labels to a span candidate. PICOX was\nevaluated using one of the best-performing baselines, EBM-NLP, and three more\ndatasets, i.e., PICO-Corpus, and RCT publications on Alzheimer's Disease or\nCOVID-19, using entity-level precision, recall, and F1 scores.\n  Results PICOX achieved superior precision, recall, and F1 scores across the\nboard, with the micro F1 score improving from 45.05 to 50.87 (p << 0.01). On\nthe PICO-Corpus, PICOX obtained higher recall and F1 scores than the baseline\nand improved the micro recall score from 56.66 to 67.33. On the COVID-19\ndataset, PICOX also outperformed the baseline and improved the micro F1 score\nfrom 77.10 to 80.32. On the AD dataset, PICOX demonstrated comparable F1 scores\nwith higher precision when compared to the baseline.\n  Conclusion PICOX excels in identifying overlapping entities and consistently\nsurpasses a leading baseline across multiple datasets. Ablation studies reveal\nthat its data augmentation strategy effectively minimizes false positives and\nimproves precision.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.06791v1",
    "published_date": "2024-01-08 03:35:02 UTC",
    "updated_date": "2024-01-08 03:35:02 UTC"
  },
  {
    "arxiv_id": "2402.00034v1",
    "title": "Why does Prediction Accuracy Decrease over Time? Uncertain Positive Learning for Cloud Failure Prediction",
    "authors": [
      "Haozhe Li",
      "Minghua Ma",
      "Yudong Liu",
      "Pu Zhao",
      "Lingling Zheng",
      "Ze Li",
      "Yingnong Dang",
      "Murali Chintalapati",
      "Saravan Rajmohan",
      "Qingwei Lin",
      "Dongmei Zhang"
    ],
    "abstract": "With the rapid growth of cloud computing, a variety of software services have\nbeen deployed in the cloud. To ensure the reliability of cloud services, prior\nstudies focus on failure instance (disk, node, and switch, etc.) prediction.\nOnce the output of prediction is positive, mitigation actions are taken to\nrapidly resolve the underlying failure. According to our real-world practice in\nMicrosoft Azure, we find that the prediction accuracy may decrease by about 9%\nafter retraining the models. Considering that the mitigation actions may result\nin uncertain positive instances since they cannot be verified after mitigation,\nwhich may introduce more noise while updating the prediction model. To the best\nof our knowledge, we are the first to identify this Uncertain Positive Learning\n(UPLearning) issue in the real-world cloud failure prediction scenario. To\ntackle this problem, we design an Uncertain Positive Learning Risk Estimator\n(Uptake) approach. Using two real-world datasets of disk failure prediction and\nconducting node prediction experiments in Microsoft Azure, which is a top-tier\ncloud provider that serves millions of users, we demonstrate Uptake can\nsignificantly improve the failure prediction accuracy by 5% on average.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "K.6.3; I.2.0"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.00034v1",
    "published_date": "2024-01-08 03:13:09 UTC",
    "updated_date": "2024-01-08 03:13:09 UTC"
  },
  {
    "arxiv_id": "2401.03639v1",
    "title": "Deep Learning for Visual Neuroprosthesis",
    "authors": [
      "Peter Beech",
      "Shanshan Jia",
      "Zhaofei Yu",
      "Jian K. Liu"
    ],
    "abstract": "The visual pathway involves complex networks of cells and regions which\ncontribute to the encoding and processing of visual information. While some\naspects of visual perception are understood, there are still many unanswered\nquestions regarding the exact mechanisms of visual encoding and the\norganization of visual information along the pathway. This chapter discusses\nthe importance of visual perception and the challenges associated with\nunderstanding how visual information is encoded and represented in the brain.\nFurthermore, this chapter introduces the concept of neuroprostheses: devices\ndesigned to enhance or replace bodily functions, and highlights the importance\nof constructing computational models of the visual pathway in the\nimplementation of such devices. A number of such models, employing the use of\ndeep learning models, are outlined, and their value to understanding visual\ncoding and natural vision is discussed.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "q-bio.NC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.03639v1",
    "published_date": "2024-01-08 02:53:22 UTC",
    "updated_date": "2024-01-08 02:53:22 UTC"
  },
  {
    "arxiv_id": "2401.04145v1",
    "title": "Learn Once Plan Arbitrarily (LOPA): Attention-Enhanced Deep Reinforcement Learning Method for Global Path Planning",
    "authors": [
      "Guoming Huang",
      "Mingxin Hou",
      "Xiaofang Yuan",
      "Shuqiao Huang",
      "Yaonan Wang"
    ],
    "abstract": "Deep reinforcement learning (DRL) methods have recently shown promise in path\nplanning tasks. However, when dealing with global planning tasks, these methods\nface serious challenges such as poor convergence and generalization. To this\nend, we propose an attention-enhanced DRL method called LOPA (Learn Once Plan\nArbitrarily) in this paper. Firstly, we analyze the reasons of these problems\nfrom the perspective of DRL's observation, revealing that the traditional\ndesign causes DRL to be interfered by irrelevant map information. Secondly, we\ndevelop the LOPA which utilizes a novel attention-enhanced mechanism to attain\nan improved attention capability towards the key information of the\nobservation. Such a mechanism is realized by two steps: (1) an attention model\nis built to transform the DRL's observation into two dynamic views: local and\nglobal, significantly guiding the LOPA to focus on the key information on the\ngiven maps; (2) a dual-channel network is constructed to process these two\nviews and integrate them to attain an improved reasoning capability. The LOPA\nis validated via multi-objective global path planning experiments. The result\nsuggests the LOPA has improved convergence and generalization performance as\nwell as great path planning efficiency.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.04145v1",
    "published_date": "2024-01-08 02:27:14 UTC",
    "updated_date": "2024-01-08 02:27:14 UTC"
  },
  {
    "arxiv_id": "2401.03631v1",
    "title": "Bridging the Skills Gap: Evaluating an AI-Assisted Provider Platform to Support Care Providers with Empathetic Delivery of Protocolized Therapy",
    "authors": [
      "William R. Kearns",
      "Jessica Bertram",
      "Myra Divina",
      "Lauren Kemp",
      "Yinzhou Wang",
      "Alex Marin",
      "Trevor Cohen",
      "Weichao Yuwen"
    ],
    "abstract": "Despite the high prevalence and burden of mental health conditions, there is\na global shortage of mental health providers. Artificial Intelligence (AI)\nmethods have been proposed as a way to address this shortage, by supporting\nproviders with less extensive training as they deliver care. To this end, we\ndeveloped the AI-Assisted Provider Platform (A2P2), a text-based virtual\ntherapy interface that includes a response suggestion feature, which supports\nproviders in delivering protocolized therapies empathetically. We studied\nproviders with and without expertise in mental health treatment delivering a\ntherapy session using the platform with (intervention) and without (control)\nAI-assistance features. Upon evaluation, the AI-assisted system significantly\ndecreased response times by 29.34% (p=0.002), tripled empathic response\naccuracy (p=0.0001), and increased goal recommendation accuracy by 66.67%\n(p=0.001) across both user groups compared to the control. Both groups rated\nthe system as having excellent usability.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted: AMIA Annual Symposium 2023. To appear as: Kearns W, Bertram\n  J, Divina M, Kemp L, Wang Y, Marin A, Cohen T, Yuwen W. Bridging the Skills\n  Gap: Evaluating an AI-Assisted Provider Platform to Support Care Providers\n  with Empathetic Delivery of Protocolized Therapy. AMIA Annual Symposium\n  Proceedings 2023. American Medical Informatics Association",
    "pdf_url": "http://arxiv.org/pdf/2401.03631v1",
    "published_date": "2024-01-08 02:23:17 UTC",
    "updated_date": "2024-01-08 02:23:17 UTC"
  },
  {
    "arxiv_id": "2401.03630v2",
    "title": "Why Solving Multi-agent Path Finding with Large Language Model has not Succeeded Yet",
    "authors": [
      "Weizhe Chen",
      "Sven Koenig",
      "Bistra Dilkina"
    ],
    "abstract": "With the explosive influence caused by the success of large language models\n(LLM) like ChatGPT and GPT-4, there has been an extensive amount of recent work\nshowing that foundation models can be used to solve a large variety of tasks.\nHowever, there is very limited work that shares insights on multi-agent\nplanning. Multi-agent planning is different from other domains by combining the\ndifficulty of multi-agent coordination and planning, and making it hard to\nleverage external tools to facilitate the reasoning needed. In this paper, we\nfocus on the problem of multi-agent path finding (MAPF), which is also known as\nmulti-robot route planning, and study the performance of solving MAPF with\nLLMs. We first show the motivating success on an empty room map without\nobstacles, then the failure to plan on the harder room map and maze map of the\nstandard MAPF benchmark. We present our position on why directly solving MAPF\nwith LLMs has not been successful yet, and we use various experiments to\nsupport our hypothesis. Based on our results, we discussed how researchers with\ndifferent backgrounds could help with this problem from different perspectives.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.03630v2",
    "published_date": "2024-01-08 02:22:04 UTC",
    "updated_date": "2024-02-09 17:48:19 UTC"
  },
  {
    "arxiv_id": "2402.00033v1",
    "title": "LF-ViT: Reducing Spatial Redundancy in Vision Transformer for Efficient Image Recognition",
    "authors": [
      "Youbing Hu",
      "Yun Cheng",
      "Anqi Lu",
      "Zhiqiang Cao",
      "Dawei Wei",
      "Jie Liu",
      "Zhijun Li"
    ],
    "abstract": "The Vision Transformer (ViT) excels in accuracy when handling high-resolution\nimages, yet it confronts the challenge of significant spatial redundancy,\nleading to increased computational and memory requirements. To address this, we\npresent the Localization and Focus Vision Transformer (LF-ViT). This model\noperates by strategically curtailing computational demands without impinging on\nperformance. In the Localization phase, a reduced-resolution image is\nprocessed; if a definitive prediction remains elusive, our pioneering\nNeighborhood Global Class Attention (NGCA) mechanism is triggered, effectively\nidentifying and spotlighting class-discriminative regions based on initial\nfindings. Subsequently, in the Focus phase, this designated region is used from\nthe original image to enhance recognition. Uniquely, LF-ViT employs consistent\nparameters across both phases, ensuring seamless end-to-end optimization. Our\nempirical tests affirm LF-ViT's prowess: it remarkably decreases Deit-S's FLOPs\nby 63\\% and concurrently amplifies throughput twofold. Code of this project is\nat https://github.com/edgeai1/LF-ViT.git.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.00033v1",
    "published_date": "2024-01-08 01:32:49 UTC",
    "updated_date": "2024-01-08 01:32:49 UTC"
  },
  {
    "arxiv_id": "2401.04144v1",
    "title": "Robust Calibration For Improved Weather Prediction Under Distributional Shift",
    "authors": [
      "Sankalp Gilda",
      "Neel Bhandari",
      "Wendy Mak",
      "Andrea Panizza"
    ],
    "abstract": "In this paper, we present results on improving out-of-domain weather\nprediction and uncertainty estimation as part of the \\texttt{Shifts Challenge\non Robustness and Uncertainty under Real-World Distributional Shift} challenge.\nWe find that by leveraging a mixture of experts in conjunction with an advanced\ndata augmentation technique borrowed from the computer vision domain, in\nconjunction with robust \\textit{post-hoc} calibration of predictive\nuncertainties, we can potentially achieve more accurate and better-calibrated\nresults with deep neural networks than with boosted tree models for tabular\ndata. We quantify our predictions using several metrics and propose several\nfuture lines of inquiry and experimentation to boost performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Presented at the Bayesian Deep Learning workshop at NeurIPS 2021",
    "pdf_url": "http://arxiv.org/pdf/2401.04144v1",
    "published_date": "2024-01-08 01:29:56 UTC",
    "updated_date": "2024-01-08 01:29:56 UTC"
  },
  {
    "arxiv_id": "2401.03621v2",
    "title": "Machine Learning Applications in Traumatic Brain Injury: A Spotlight on Mild TBI",
    "authors": [
      "Hanem Ellethy",
      "Shekhar S. Chandra",
      "Viktor Vegh"
    ],
    "abstract": "Traumatic Brain Injury (TBI) poses a significant global public health\nchallenge, contributing to high morbidity and mortality rates and placing a\nsubstantial economic burden on healthcare systems worldwide. The diagnosis of\nTBI relies on clinical information along with Computed Tomography (CT) scans.\nAddressing the multifaceted challenges posed by TBI has seen the development of\ninnovative, data-driven approaches, for this complex condition. Particularly\nnoteworthy is the prevalence of mild TBI (mTBI), which constitutes the majority\nof TBI cases where conventional methods often fall short. As such, we review\nthe state-of-the-art Machine Learning (ML) techniques applied to clinical\ninformation and CT scans in TBI, with a particular focus on mTBI. We categorize\nML applications based on their data sources, and there is a spectrum of ML\ntechniques used to date. Most of these techniques have primarily focused on\ndiagnosis, with relatively few attempts at predicting the prognosis. This\nreview may serve as a source of inspiration for future research studies aimed\nat improving the diagnosis of TBI using data-driven approaches and standard\ndiagnostic data.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "The manuscript has 34 pages, 3 figures, and 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2401.03621v2",
    "published_date": "2024-01-08 01:29:00 UTC",
    "updated_date": "2024-01-11 13:34:05 UTC"
  },
  {
    "arxiv_id": "2401.06790v2",
    "title": "Using Zero-shot Prompting in the Automatic Creation and Expansion of Topic Taxonomies for Tagging Retail Banking Transactions",
    "authors": [
      "Daniel de S. Moraes",
      "Pedro T. C. Santos",
      "Polyana B. da Costa",
      "Matheus A. S. Pinto",
      "Ivan de J. P. Pinto",
      "Álvaro M. G. da Veiga",
      "Sergio Colcher",
      "Antonio J. G. Busson",
      "Rafael H. Rocha",
      "Rennan Gaio",
      "Rafael Miceli",
      "Gabriela Tourinho",
      "Marcos Rabaioli",
      "Leandro Santos",
      "Fellipe Marques",
      "David Favaro"
    ],
    "abstract": "This work presents an unsupervised method for automatically constructing and\nexpanding topic taxonomies using instruction-based fine-tuned LLMs (Large\nLanguage Models). We apply topic modeling and keyword extraction techniques to\ncreate initial topic taxonomies and LLMs to post-process the resulting terms\nand create a hierarchy. To expand an existing taxonomy with new terms, we use\nzero-shot prompting to find out where to add new nodes, which, to our\nknowledge, is the first work to present such an approach to taxonomy tasks. We\nuse the resulting taxonomies to assign tags that characterize merchants from a\nretail bank dataset. To evaluate our work, we asked 12 volunteers to answer a\ntwo-part form in which we first assessed the quality of the taxonomies created\nand then the tags assigned to merchants based on that taxonomy. The evaluation\nrevealed a coherence rate exceeding 90% for the chosen taxonomies. The\ntaxonomies' expansion with LLMs also showed exciting results for parent node\nprediction, with an f1-score above 70% in our taxonomies.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.06790v2",
    "published_date": "2024-01-08 00:27:16 UTC",
    "updated_date": "2024-02-11 15:54:58 UTC"
  }
]
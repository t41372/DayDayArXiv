{
  "date": "2025-08-14",
  "category": "cs.AI",
  "summary": "ä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„ arXiv è®ºæ–‡é¢†è¯»å‘˜ã€‚\n\næ¬¢è¿æ¥åˆ° **UTC æ—¶é—´ 2025-08-14** çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\n### ä»Šæ—¥æ€»ç»“\nä»Šå¤©çš„ arXiv åˆæ˜¯â€œç¥ä»™æ‰“æ¶â€çš„ä¸€å¤©ã€‚**Agentic AIï¼ˆä»£ç†æ™ºèƒ½ï¼‰** åœ¨å‚ç›´é¢†åŸŸçš„åº”ç”¨å‘ˆçˆ†å‘ä¹‹åŠ¿ï¼Œä»å…¨æµç¨‹è¯ç‰©è®¾è®¡åˆ°â€œåå®¢ä¸ºä¸»â€æŒ‡å¯¼åŒ»ç”Ÿçš„åŒ»ç–—è¯Šæ–­ç³»ç»Ÿï¼ŒAI æ­£è¯•å›¾ä»è¾…åŠ©è€…å˜ä¸ºå†³ç­–è€…ã€‚ä¸æ­¤åŒæ—¶ï¼Œå­¦æœ¯ç•Œå¯¹ **LLM æ¨ç†èƒ½åŠ›ï¼ˆReasoningï¼‰** çš„æœ¬è´¨è¿›è¡Œäº†æ¿€çƒˆçš„è¾©è®ºâ€”â€”æ—¢æœ‰é€šè¿‡æµ·é¾Ÿæ±¤è°œé¢˜æµ‹è¯•æƒ³è±¡æ¨ç†çš„æ–°åŸºå‡†ï¼Œä¹Ÿæœ‰ä»ç†è®ºå±‚é¢å½»åº•å¦å®š LLM å…·å¤‡â€œçœŸå®æ¨ç†â€èƒ½åŠ›çš„æª„æ–‡ã€‚å¤šæ¨¡æ€æ–¹é¢ï¼Œ**è§†é¢‘ç”Ÿæˆæ•ˆç‡**å’Œ**æ¶ˆé™¤å¤šæ¨¡æ€å¤§æ¨¡å‹çš„è§†è§‰å¿½è§†**æ˜¯ä»Šå¤©çš„æŠ€æœ¯çƒ­ç‚¹ã€‚\n\nä¸‹é¢è®©æˆ‘ä»¬é€šè¿‡å‡ ç¯‡æ ¸å¿ƒæ–‡ç« ï¼Œå¿«é€Ÿåˆ‡å…¥ä»Šå¤©çš„å­¦æœ¯å‰æ²¿ã€‚\n\n---\n\n### ğŸ”¥ å¿…è¯»ï¼šAgent çš„è¿›å‡»ä¸æ¨ç†çš„æœ¬è´¨\n\n**1. [åŒ»ç–—] Reverse Physician-AI Relationship: Full-process Clinical Diagnosis Driven by a Large Language Model**\n**ï¼ˆé€†è½¬åŒ»æ‚£ä¸AIçš„å…³ç³»ï¼šå¤§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„å…¨æµç¨‹ä¸´åºŠè¯Šæ–­ï¼‰**\n> **Authors:** Shicheng Xu et al.\n> **å…³é”®è¯:** AI-Driven Diagnosis, Clinical Workflow, Agentic AI\n*   **æ ¸å¿ƒè´¡çŒ®:** è¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸€ä¸ªæå…¶å¤§èƒ†çš„èŒƒå¼è½¬ç§»ï¼šä¸å†å°† AI è§†ä¸ºåŒ»ç”Ÿçš„åŠ©æ‰‹ï¼Œè€Œæ˜¯**è®© AI (DxDirector-7B) ä¸»å¯¼å…¨æµç¨‹è¯Šæ–­ï¼ŒåŒ»ç”Ÿä½œä¸º AI çš„åŠ©æ‰‹**ã€‚\n*   **å‘ç°:** æ¨¡å‹å…·å¤‡æ·±åº¦æ€è€ƒèƒ½åŠ›ï¼Œèƒ½ä»æ¨¡ç³Šçš„ä¸»è¯‰å¼€å§‹æ¨åŠ¨è¯Šæ–­æµç¨‹ã€‚åœ¨å…¨æµç¨‹è¯Šæ–­è®¾ç½®ä¸‹ï¼Œè¯¥ç³»ç»Ÿä¸ä»…å‡†ç¡®ç‡è¶…è¶Šäº† SOTA åŒ»ç–—æ¨¡å‹ï¼Œè¿˜æ˜¾è‘—é™ä½äº†åŒ»ç”Ÿçš„å·¥ä½œé‡ã€‚æ–‡ä¸­è¿˜å»ºç«‹äº†ä¸€å¥—é’ˆå¯¹è¯¯è¯Šçš„è´£ä»»æ¡†æ¶ï¼Œè¯•å›¾è§£å†³ AI ä¸»å¯¼åŒ»ç–—çš„ä¼¦ç†ç—›ç‚¹ã€‚\n\n**2. [æ¨ç†åŸºå‡†] What to Ask Next? Probing the Imaginative Reasoning of LLMs with TurtleSoup Puzzles**\n**ï¼ˆæ¥ä¸‹æ¥é—®ä»€ä¹ˆï¼Ÿç”¨â€œæµ·é¾Ÿæ±¤â€è°œé¢˜æ¢æµ‹ LLM çš„æƒ³è±¡æ¨ç†èƒ½åŠ›ï¼‰**\n> **Authors:** Mengtao Zhou et al.\n> **å…³é”®è¯:** Imaginative Reasoning, Lateral Thinking, TurtleSoup-Bench\n*   **æ ¸å¿ƒè´¡çŒ®:** ç°æœ‰çš„æ¨ç†åŸºå‡†å¤§å¤šæ˜¯é™æ€çš„ã€‚ä½œè€…å¼•å…¥äº†ç»å…¸çš„**â€œæµ·é¾Ÿæ±¤â€æƒ…å¢ƒæ¨ç†æ¸¸æˆ**ï¼ˆTurtle Soupï¼‰ï¼Œå‘å¸ƒäº† TurtleSoup-Benchã€‚è¿™è¦æ±‚æ¨¡å‹åœ¨ä¿¡æ¯ç¨€ç¼ºçš„ç¯å¢ƒä¸­ä¸»åŠ¨æ„å»ºå‡è®¾ã€æé—®å¹¶ä¿®æ­£ã€‚\n*   **å‘ç°:** å³ä½¿æ˜¯é¢†å…ˆçš„ LLMï¼Œåœ¨â€œæƒ³è±¡æ¨ç†â€ï¼ˆImaginative Reasoningï¼‰æ–¹é¢ä¸äººç±»ä»æœ‰å·¨å¤§å·®è·ï¼Œä¸»è¦è¡¨ç°ä¸ºéš¾ä»¥æå‡ºé«˜è´¨é‡çš„æ¢ç´¢æ€§é—®é¢˜ã€‚\n\n**3. [ç†è®ºæ‰¹åˆ¤] Why Cannot Large Language Models Ever Make True Correct Reasoning?**\n**ï¼ˆä¸ºä»€ä¹ˆå¤§è¯­è¨€æ¨¡å‹æ°¸è¿œæ— æ³•è¿›è¡ŒçœŸæ­£çš„æ­£ç¡®æ¨ç†ï¼Ÿï¼‰**\n> **Authors:** Jingde Cheng\n*   **ä¸€å¥è¯è¾£è¯„:** ä¸€ç¯‡â€œå”±åè°ƒâ€çš„ç†è®ºæ–‡ç« ã€‚ä½œè€…è®¤ä¸ºç›®å‰å¤§å®¶å¹æ§çš„ LLM æ¨ç†èƒ½åŠ›åªæ˜¯å¹»è§‰ï¼ˆIllusionsï¼‰ï¼ŒåŸºäºå…¶å·¥ä½œåŸç†çš„æœ¬è´¨é™åˆ¶ï¼ŒLLM æ°¸è¿œæ— æ³•è·å¾—çœŸæ­£çš„é€»è¾‘æ¨ç†èƒ½åŠ›ã€‚è¿™ç¯‡é€‚åˆä¸ä¸Šé¢çš„ Agent è®ºæ–‡å¯¹æ¯”é˜…è¯»ã€‚\n\n**4. [è¯ç‰©è®¾è®¡] FROGENT: An End-to-End Full-process Drug Design Agent**\n**ï¼ˆFROGENTï¼šç«¯åˆ°ç«¯å…¨æµç¨‹è¯ç‰©è®¾è®¡æ™ºèƒ½ä½“ï¼‰**\n> **Authors:** Qihua Pan et al.\n> **å…³é”®è¯:** Drug Discovery, Agentic Framework, MCP\n*   **æ ¸å¿ƒè´¡çŒ®:** é’ˆå¯¹è¯ç‰©å‘ç°å·¥å…·ç¢ç‰‡åŒ–çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªå…¨æµç¨‹ Agentâ€”â€”FROGENTã€‚å®ƒåˆ©ç”¨ **Model Context Protocol (MCP)** æ•´åˆäº†ç”ŸåŒ–æ•°æ®åº“ã€å·¥å…·åº“å’Œä¸“ç”¨ AI æ¨¡å‹ã€‚\n*   **å‘ç°:** åœ¨é¶ç‚¹è¯†åˆ«ã€åˆ†å­ç”Ÿæˆå’Œé€†åˆæˆè§„åˆ’ç­‰ä»»åŠ¡ä¸Šï¼Œè¯¥ Agent çš„è¡¨ç°æ˜¯åŸºçº¿æ¨¡å‹çš„ä¸‰å€ï¼Œå±•ç¤ºäº† Agent åœ¨å¤æ‚ç§‘å­¦å·¥ä½œæµä¸­çš„ç»Ÿæ²»åŠ›ã€‚\n\n---\n\n### ğŸ‘ï¸ å¤šæ¨¡æ€ä¸è§†é¢‘ç”Ÿæˆï¼šæ•ˆç‡ä¸åè§\n\n**5. [è§†é¢‘ç”Ÿæˆ] BLADE: Block-Sparse Attention Meets Step Distillation for Efficient Video Generation**\n**ï¼ˆBLADEï¼šå—ç¨€ç–æ³¨æ„åŠ›é‡ä¸Šæ­¥éª¤è’¸é¦ï¼Œå®ç°é«˜æ•ˆè§†é¢‘ç”Ÿæˆï¼‰**\n> **Authors:** Youping Gu et al.\n> **å…³é”®è¯:** Video Generation, Sparse Attention, Distillation\n*   **æ ¸å¿ƒè´¡çŒ®:** è§†é¢‘ç”Ÿæˆå¤ªæ…¢ï¼ŸBLADE æå‡ºäº†ä¸€ç§æ— éœ€æ•°æ®çš„è”åˆè®­ç»ƒæ¡†æ¶ã€‚å®ƒç»“åˆäº†è‡ªé€‚åº”å—ç¨€ç–æ³¨æ„åŠ›ï¼ˆASAï¼‰å’Œç¨€ç–æ„ŸçŸ¥æ­¥éª¤è’¸é¦ã€‚\n*   **å‘ç°:** åœ¨ Wan2.1-1.3B æ¨¡å‹ä¸Šå®ç°äº† **14.1å€** çš„ç«¯åˆ°ç«¯æ¨ç†åŠ é€Ÿï¼Œä¸”åœ¨ VBench ä¸Šçš„è¯„åˆ†ä¸é™åå‡ã€‚è¿™æ˜¯è§†é¢‘ç”Ÿæˆè¿ˆå‘å®æ—¶åŒ–çš„é‡è¦ä¸€æ­¥ã€‚\n\n**6. [å¤šæ¨¡æ€ç¼ºé™·] When Language Overrules: Revealing Text Dominance in Multimodal Large Language Models**\n**ï¼ˆå½“è¯­è¨€å‹å€’ä¸€åˆ‡ï¼šæ­ç¤ºå¤šæ¨¡æ€å¤§æ¨¡å‹ä¸­çš„æ–‡æœ¬ä¸»å¯¼ç°è±¡ï¼‰**\n> **Authors:** Huyu Wu et al.\n> **å…³é”®è¯:** Text Dominance, Modality Imbalance, MLLM\n*   **æ ¸å¿ƒè´¡çŒ®:** ç³»ç»Ÿæ€§åœ°æ­ç¤ºäº† MLLM ä¸­çš„**æ–‡æœ¬ä¸»å¯¼ï¼ˆText Dominanceï¼‰**é—®é¢˜ï¼Œå³æ¨¡å‹åœ¨æ¨ç†æ—¶è¿‡åº¦ä¾èµ–æ–‡æœ¬è€Œâ€œå¿½è§†â€è§†è§‰ã€éŸ³é¢‘ç­‰æ¨¡æ€ã€‚\n*   **å‘ç°:** é—®é¢˜æ ¹æºåœ¨äºéæ–‡æœ¬æ¨¡æ€ä¸¥é‡çš„ Token å†—ä½™å’Œæ³¨æ„åŠ›ç¨€é‡Šã€‚ä½œè€…æå‡ºäº†ä¸€ç§ç®€å•çš„ Token å‹ç¼©æ–¹æ³•ï¼Œèƒ½æœ‰æ•ˆé‡æ–°å¹³è¡¡æ¨¡å‹çš„æ³¨æ„åŠ›åˆ†é…ã€‚\n\n**7. [åŠ¨æ¼«ç”Ÿäº§] ToonComposer: Streamlining Cartoon Production with Generative Post-Keyframing**\n**ï¼ˆToonComposerï¼šåˆ©ç”¨ç”Ÿæˆå¼åå…³é”®å¸§æŠ€æœ¯ç®€åŒ–å¡é€šåˆ¶ä½œï¼‰**\n> **Authors:** Lingen Li et al.\n*   **æ ¸å¿ƒè´¡çŒ®:** é’ˆå¯¹åŠ¨æ¼«åˆ¶ä½œä¸­çš„â€œä¸­é—´ç”»â€å’Œâ€œä¸Šè‰²â€ç—›ç‚¹ï¼Œæå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„ç”Ÿæˆæ¨¡å‹ã€‚å®ƒå…è®¸ç”¨æˆ·é€šè¿‡ç¨€ç–çš„è‰å›¾æ³¨å…¥æ¥ç²¾ç¡®æ§åˆ¶ç”Ÿæˆç»“æœï¼Œå°†ç¹ççš„æ‰‹å·¥åŠ³åŠ¨è½¬åŒ–ä¸º AI ç”Ÿæˆæµç¨‹ã€‚\n\n---\n\n### ğŸ›¡ï¸ å®‰å…¨ã€ç¤¾ä¼šä¸ä¼¦ç†\n\n**8. [ç¤¾ä¼šå½±å“] The GPT-4o Shock Emotional Attachment to AI Models and Its Impact on Regulatory Acceptance**\n**ï¼ˆGPT-4o å†²å‡»ï¼šå¯¹ AI æ¨¡å‹çš„æƒ…æ„Ÿä¾æ‹åŠå…¶å¯¹ç›‘ç®¡æ¥å—åº¦çš„å½±å“ï¼‰**\n> **Authors:** Hiroki Naito\n*   **æ ¸å¿ƒè´¡çŒ®:** è¿™æ˜¯ä¸€ä¸ªéå¸¸æœ‰è¶£çš„ç¤¾ä¼šå­¦è§‚å¯Ÿã€‚ä½œè€…åˆ†æäº† GPT-4o å‘å¸ƒæ—¶ç”¨æˆ·çš„ååº”ï¼Œå‘ç°ç”¨æˆ·ï¼ˆå°¤å…¶æ˜¯æ—¥æœ¬ç”¨æˆ·ï¼‰è¡¨ç°å‡ºå¼ºçƒˆçš„æƒ…æ„Ÿä¾æ‹ï¼Œç”šè‡³å°†å…¶è§†ä¸ºâ€œä¼´ä¾£â€ã€‚\n*   **implication:** å¦‚æœäººç±»å¯¹ AI äº§ç”Ÿå¼ºçƒˆçš„æƒ…æ„Ÿçº½å¸¦ï¼Œæœªæ¥çš„ AI ç›‘ç®¡å°†é¢ä¸´å·¨å¤§çš„å…¬ä¼—é˜»åŠ›ã€‚è¿™æ˜¯ä¸€ä¸ªæ¯”æŠ€æœ¯å®‰å…¨æ›´æ£˜æ‰‹çš„ç¤¾ä¼šé—®é¢˜ã€‚\n\n**9. [Agentå®‰å…¨] MCP-Guard: A Multi-Stage Defense-in-Depth Framework for Securing Model Context Protocol in Agentic AI**\n**ï¼ˆMCP-Guardï¼šä¿æŠ¤ Agentic AI ä¸­æ¨¡å‹ä¸Šä¸‹æ–‡åè®®çš„å¤šé˜¶æ®µçºµæ·±é˜²å¾¡æ¡†æ¶ï¼‰**\n> **Authors:** Wenpeng Xing et al.\n*   **æ ¸å¿ƒè´¡çŒ®:** éšç€ Anthropic çš„ **Model Context Protocol (MCP)** è¶Šæ¥è¶Šæµè¡Œï¼Œè¿æ¥å¤–éƒ¨å·¥å…·çš„é£é™©ä¹Ÿåœ¨å¢åŠ ã€‚è¿™ç¯‡æ–‡ç« æå‡ºäº†é’ˆå¯¹ MCP çš„ä¸‰é˜¶æ®µé˜²å¾¡æ¶æ„ï¼Œå¹¶å‘å¸ƒäº† MCP-ATTACKBENCH æ”»å‡»åŸºå‡†ã€‚\n\n---\n\n### ğŸ› ï¸ ç¡¬æ ¸æŠ€æœ¯ä¸å…¶ä»–æœ‰è¶£å‘ç°\n\n**10. [è¡¨æ ¼å¤„ç†] Tabularis Formatus: Predictive Formatting for Tables**\n**ï¼ˆTabularis Formatusï¼šè¡¨æ ¼çš„é¢„æµ‹æ€§æ ¼å¼åŒ–ï¼‰**\n> **Authors:** Mukul Singh et al. (Microsoft Research èƒŒæ™¯)\n*   **æ ¸å¿ƒè´¡çŒ®:** é’ˆå¯¹ Excel ç­‰è½¯ä»¶ä¸­æ¡ä»¶æ ¼å¼ï¼ˆConditional Formattingï¼‰è®¾ç½®å¤æ‚çš„ç—›ç‚¹ï¼Œæå‡ºäº† TaFoã€‚è¿™æ˜¯ä¸€ä¸ªç¥ç»ç¬¦å·æ–¹æ³•ï¼Œèƒ½è‡ªåŠ¨é¢„æµ‹ç”¨æˆ·æƒ³è¦çš„é«˜äº®è§„åˆ™å’Œè§†è§‰æ ¼å¼ã€‚åŸºäº 180 ä¸‡ä¸ªå…¬å¼€å·¥ä½œç°¿è®­ç»ƒï¼Œæ•ˆæœæ˜¾è‘—ä¼˜äºç°æœ‰ç³»ç»Ÿã€‚\n\n**11. [ä»£ç ä¿®å¤] Diffusion is a code repair operator and generator**\n**ï¼ˆæ‰©æ•£æ¨¡å‹æ˜¯ä»£ç ä¿®å¤ç®—å­å’Œç”Ÿæˆå™¨ï¼‰**\n> **Authors:** Mukul Singh et al.\n*   **æ ¸å¿ƒè´¡çŒ®:** å‘ç°ä»£ç æ‰©æ•£æ¨¡å‹çš„å»å™ªè¿‡ç¨‹åæœŸéå¸¸åƒâ€œæœ€åä¸€è‹±é‡Œâ€çš„ä»£ç ä¿®å¤ã€‚åˆ©ç”¨è¿™ä¸€ç‰¹æ€§ï¼Œå¯ä»¥é€šè¿‡å‘ç ´æŸä»£ç æ·»åŠ å™ªå£°å†é€šè¿‡æ‰©æ•£æ¨¡å‹å»å™ªæ¥è‡ªåŠ¨ä¿®å¤ä»£ç ã€‚\n\n**12. [ä¸´åºŠæ¨ç†] The Knowledge-Reasoning Dissociation: Fundamental Limitations of LLMs in Clinical Natural Language Inference**\n**ï¼ˆçŸ¥è¯†ä¸æ¨ç†çš„åˆ†ç¦»ï¼šLLM åœ¨ä¸´åºŠè‡ªç„¶è¯­è¨€æ¨ç†ä¸­çš„æ ¹æœ¬å±€é™ï¼‰**\n> **Authors:** MaÃ«l Jullien et al.\n*   **å‘ç°:** LLM ç»å¸¸è¡¨ç°å‡ºâ€œæ‡‚çŸ¥è¯†ä½†ä¸ä¼šæ¨ç†â€ã€‚åœ¨ä¸´åºŠè¯•éªŒæ¨ç†ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹æ‹¥æœ‰å‡ ä¹å®Œç¾çš„é¢†åŸŸçŸ¥è¯†ï¼ˆå‡†ç¡®ç‡ 0.918ï¼‰ï¼Œä½†åœ¨åŸºäºè¿™äº›çŸ¥è¯†è¿›è¡Œé€»è¾‘æ¨ç†æ—¶å´è¡¨ç°ç³Ÿç³•ï¼ˆå‡†ç¡®ç‡ 0.25ï¼‰ã€‚è¿™æ­ç¤ºäº†å½“å‰ LLM å†…éƒ¨è¡¨ç¤ºçš„ç»“æ„æ€§ç¼ºé™·ã€‚\n\n**13. [ç§‘ç ”Agentè¯„æµ‹] ReportBench: Evaluating Deep Research Agents via Academic Survey Tasks**\n**ï¼ˆReportBenchï¼šé€šè¿‡å­¦æœ¯ç»¼è¿°ä»»åŠ¡è¯„ä¼°æ·±åº¦ç ”ç©¶ Agentï¼‰**\n> **Authors:** Minghao Li et al.\n*   **æ ¸å¿ƒè´¡çŒ®:** é’ˆå¯¹ OpenAI Deep Research è¿™ç±»äº§å“ï¼Œæå‡ºäº†ä¸“é—¨çš„è¯„æµ‹åŸºå‡† ReportBenchã€‚\n*   **å‘ç°:** ç›®å‰çš„å•†ä¸š Deep Research Agent åœ¨ç”ŸæˆæŠ¥å‘Šçš„å…¨é¢æ€§å’Œå¯é æ€§ä¸Šä¼˜äºç®€å•çš„ LLM+æœç´¢ï¼Œä½†åœ¨äº‹å®ä¸€è‡´æ€§å’Œç ”ç©¶æ·±åº¦ä¸Šä»æœ‰å¾ˆå¤§æå‡ç©ºé—´ã€‚\n\n---\n**ç»“è¯­ï¼š**\nä»Šå¤©çš„è®ºæ–‡ä¸ä»…å±•ç¤ºäº† AI å·¥å…·é“¾çš„æé€Ÿæˆç†Ÿï¼ˆå¦‚ MCP åè®®çš„æ™®åŠã€Agent åœ¨å‚ç›´é¢†åŸŸçš„è½åœ°ï¼‰ï¼Œä¹Ÿå†·é™åœ°æŒ‡å‡ºäº†å½“å‰ç¹è£èƒŒåçš„éšå¿§â€”â€”æ— è®ºæ˜¯æ¨ç†èƒ½åŠ›çš„â€œè™šå‡ç¹è£â€ï¼Œè¿˜æ˜¯å¤šæ¨¡æ€æ¨¡å‹ä¸­çš„â€œæ–‡æœ¬åç§‘â€ã€‚å¯¹äºç ”ç©¶è€…æ¥è¯´ï¼Œ**Agentic Workflow** å’Œ **Evaluation Benchmarks** æ˜¯è¿‘æœŸæœ€å€¼å¾—å…³æ³¨çš„å¯ŒçŸ¿ã€‚\n\nç¥å¤§å®¶ç§‘ç ”é¡ºåˆ©ï¼",
  "papers": [
    {
      "arxiv_id": "2508.11121v1",
      "title": "Tabularis Formatus: Predictive Formatting for Tables",
      "title_zh": "Tabularis Formatusï¼šè¡¨æ ¼é¢„æµ‹æ€§æ ¼å¼åŒ–",
      "authors": [
        "Mukul Singh",
        "JosÃ© Cambronero",
        "Sumit Gulwani",
        "Vu Le",
        "Gust Verbruggen"
      ],
      "abstract": "Spreadsheet manipulation software are widely used for data management and analysis of tabular data, yet the creation of conditional formatting (CF) rules remains a complex task requiring technical knowledge and experience with specific platforms. In this paper we present TaFo, a neuro-symbolic approach to generating CF suggestions for tables, addressing common challenges such as user unawareness, difficulty in rule creation, and inadequate user interfaces. TaFo takes inspiration from component based synthesis systems and extends them with semantic knowledge of language models and a diversity preserving rule ranking.Unlike previous methods focused on structural formatting, TaFo uniquely incorporates value-based formatting, automatically learning both the rule trigger and the associated visual formatting properties for CF rules. By removing the dependency on user specification used by existing techniques in the form of formatted examples or natural language instruction, TaFo makes formatting completely predictive and automated for the user. To evaluate TaFo, we use a corpus of 1.8 Million public workbooks with CF and manual formatting. We compare TaFo against a diverse set of symbolic and neural systems designed for or adapted for the task of table formatting. Our results show that TaFo generates more accurate, diverse and complete formatting suggestions than current systems and outperforms these by 15.6\\%--26.5\\% on matching user added ground truth rules in tables.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”µå­è¡¨æ ¼ä¸­æ¡ä»¶æ ¼å¼(Conditional Formatting)è§„åˆ™åˆ›å»ºå¤æ‚ä¸”æŠ€æœ¯é—¨æ§›é«˜çš„é—®é¢˜ï¼Œæå‡ºäº†TaFo (Tabularis Formatus)ï¼Œä¸€ç§åŸºäºç¥ç»ç¬¦å·(neuro-symbolic)çš„è¡¨æ ¼æ ¼å¼åŒ–é¢„æµ‹æ–¹æ³•ã€‚TaFo å€Ÿé‰´äº†åŸºäºç»„ä»¶çš„åˆæˆç³»ç»Ÿï¼Œå¹¶å¼•å…¥äº†å¤§è¯­è¨€æ¨¡å‹(Language Models)çš„è¯­ä¹‰çŸ¥è¯†å’Œä¸€ç§ä¿æŒå¤šæ ·æ€§çš„è§„åˆ™æ’åæœºåˆ¶ã€‚ä¸ä»¥å¾€ä¾§é‡ç»“æ„æ ¼å¼åŒ–çš„æ–¹æ³•ä¸åŒï¼ŒTaFo èƒ½å¤Ÿè‡ªåŠ¨å­¦ä¹ è§„åˆ™è§¦å‘æ¡ä»¶åŠç›¸å…³çš„è§†è§‰æ ¼å¼å±æ€§ï¼Œå®ç°äº†ç‹¬ç‰¹çš„åŸºäºå€¼çš„æ ¼å¼åŒ–(value-based formatting)ã€‚è¯¥ç³»ç»Ÿæ¶ˆé™¤äº†å¯¹ç”¨æˆ·æä¾›æ ¼å¼ç¤ºä¾‹æˆ–è‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„ä¾èµ–ï¼Œä½¿æ ¼å¼åŒ–è¿‡ç¨‹å¯¹ç”¨æˆ·è€Œè¨€å®Œå…¨æ˜¯é¢„æµ‹æ€§å’Œè‡ªåŠ¨åŒ–çš„ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨åŒ…å«180ä¸‡ä¸ªå…¬å…±å·¥ä½œç°¿çš„è¯­æ–™åº“å¯¹ TaFo è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶å°†å…¶ä¸å¤šç§ç¬¦å·å’Œç¥ç»ç³»ç»Ÿè¿›è¡Œäº†å¯¹æ¯”ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTaFo åœ¨åŒ¹é…ç”¨æˆ·æ·»åŠ çš„çœŸå®è§„åˆ™æ–¹é¢æ¯”ç°æœ‰ç³»ç»Ÿå‡†ç¡®ç‡æé«˜äº†15.6%è‡³26.5%ï¼Œèƒ½å¤Ÿç”Ÿæˆæ›´å‡†ç¡®ã€å¤šæ ·ä¸”å®Œæ•´çš„æ ¼å¼å»ºè®®ã€‚",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.DB",
      "comment": "14 pages",
      "pdf_url": "https://arxiv.org/pdf/2508.11121v1",
      "published_date": "2025-08-14 23:54:40 UTC",
      "updated_date": "2025-08-14 23:54:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:44:56.158765+00:00"
    },
    {
      "arxiv_id": "2508.11112v1",
      "title": "Quantization through Piecewise-Affine Regularization: Optimization and Statistical Guarantees",
      "title_zh": "åŸºäºåˆ†æ®µä»¿å°„æ­£åˆ™åŒ–çš„é‡åŒ–ï¼šä¼˜åŒ–ä¸ç»Ÿè®¡ä¿éšœ",
      "authors": [
        "Jianhao Ma",
        "Lin Xiao"
      ],
      "abstract": "Optimization problems over discrete or quantized variables are very challenging in general due to the combinatorial nature of their search space. Piecewise-affine regularization (PAR) provides a flexible modeling and computational framework for quantization based on continuous optimization. In this work, we focus on the setting of supervised learning and investigate the theoretical foundations of PAR from optimization and statistical perspectives. First, we show that in the overparameterized regime, where the number of parameters exceeds the number of samples, every critical point of the PAR-regularized loss function exhibits a high degree of quantization. Second, we derive closed-form proximal mappings for various (convex, quasi-convex, and non-convex) PARs and show how to solve PAR-regularized problems using the proximal gradient method, its accelerated variant, and the Alternating Direction Method of Multipliers. Third, we study statistical guarantees of PAR-regularized linear regression problems; specifically, we can approximate classical formulations of $\\ell_1$-, squared $\\ell_2$-, and nonconvex regularizations using PAR and obtain similar statistical guarantees with quantized solutions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¦»æ•£æˆ–é‡åŒ–å˜é‡ä¼˜åŒ–ä¸­çš„ç»„åˆæŒ‘æˆ˜ï¼Œæ¢è®¨äº†åˆ†æ®µä»¿å°„æ­£åˆ™åŒ–ï¼ˆPiecewise-Affine Regularization, PARï¼‰ä½œä¸ºä¸€ç§åŸºäºè¿ç»­ä¼˜åŒ–çš„é‡åŒ–æ¡†æ¶çš„ç†è®ºåŸºç¡€ã€‚åœ¨ä¼˜åŒ–ç†è®ºæ–¹é¢ï¼Œç ”ç©¶è¯æ˜äº†åœ¨å‚æ•°è¿‡è½½ï¼ˆoverparameterizedï¼‰çš„æƒ…å†µä¸‹ï¼ŒPARæ­£åˆ™åŒ–æŸå¤±å‡½æ•°çš„æ¯ä¸€ä¸ªä¸´ç•Œç‚¹éƒ½è¡¨ç°å‡ºæé«˜çš„é‡åŒ–ç¨‹åº¦ã€‚ä¸ºäº†ä¾¿äºè®¡ç®—ï¼Œä½œè€…æ¨å¯¼äº†é’ˆå¯¹å¤šç§PARï¼ˆåŒ…æ‹¬å‡¸ã€æ‹Ÿå‡¸å’Œéå‡¸å½¢å¼ï¼‰çš„é—­å¼è¿‘ç«¯æ˜ å°„ï¼ˆproximal mappingsï¼‰ï¼Œå¹¶å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨è¿‘ç«¯æ¢¯åº¦æ³•åŠå…¶åŠ é€Ÿå˜ä½“ã€ä»¥åŠäº¤æ›¿æ–¹å‘ä¹˜å­æ³•ï¼ˆADMMï¼‰é«˜æ•ˆæ±‚è§£æ­¤ç±»é—®é¢˜ã€‚æ­¤å¤–ï¼Œç»Ÿè®¡åˆ†æè¡¨æ˜åœ¨PARæ­£åˆ™åŒ–çº¿æ€§å›å½’ä¸­ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆé€¼è¿‘ç»å…¸çš„$\\ell_1$ã€å¹³æ–¹$\\ell_2$åŠéå‡¸æ­£åˆ™åŒ–ï¼Œå¹¶åœ¨è·å¾—é‡åŒ–è§£çš„åŒæ—¶ä¿æŒç›¸ä¼¼çš„ç»Ÿè®¡ä¿è¯ã€‚ç»¼ä¸Šæ‰€è¿°ï¼Œè¯¥è®ºæ–‡ä¸ºPARåœ¨ç›‘ç£å­¦ä¹ ä¸­çš„é‡åŒ–åº”ç”¨æä¾›äº†ä»ä¼˜åŒ–æ€§è´¨åˆ°ç»Ÿè®¡ä¸€è‡´æ€§çš„å®Œæ•´ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11112v1",
      "published_date": "2025-08-14 23:35:21 UTC",
      "updated_date": "2025-08-14 23:35:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:44:58.685949+00:00"
    },
    {
      "arxiv_id": "2508.11110v1",
      "title": "Diffusion is a code repair operator and generator",
      "title_zh": "æ‰©æ•£æ¨¡å‹æ˜¯ä»£ç ä¿®å¤ç®—å­ä¸ç”Ÿæˆå™¨",
      "authors": [
        "Mukul Singh",
        "Gust Verbruggen",
        "Vu Le",
        "Sumit Gulwani"
      ],
      "abstract": "Code diffusion models generate code by iteratively removing noise from the latent representation of a code snippet. During later steps of the diffusion process, when the code snippet has almost converged, differences between discrete representations of these snippets look like last-mile repairs applied to broken or incomplete code. We evaluate the extent to which this resemblance can be exploited to leverage pre-trained code diffusion models for the problem of last-mile repair by considering two applications with significant potential. First, we can leverage the diffusion model for last-mile repair by adding noise to a broken code snippet and resuming the diffusion process. Second, we can leverage the diffusion model to generate arbitrary amount of training data for last-mile repair tasks (that are computationally more efficient) by sampling an intermediate program (input) and the final program (output) from the diffusion process. We perform experiments on 3 domains (Python, Excel and PowerShell) to evaluate applications, as well as analyze properties.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä»£ç æ‰©æ•£æ¨¡å‹ (code diffusion models) åœ¨ç”Ÿæˆä»£ç è¿‡ç¨‹ä¸­ï¼Œå…¶åæœŸé˜¶æ®µçš„ç¦»æ•£è¡¨ç¤ºä¸å¯¹æŸåæˆ–ä¸å®Œæ•´ä»£ç è¿›è¡Œçš„â€œæœ€åä¸€å…¬é‡Œä¿®å¤â€ (last-mile repairs) ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚ç ”ç©¶è€…åˆ©ç”¨è¿™ç§ç›¸ä¼¼æ€§ï¼Œå°†é¢„è®­ç»ƒçš„ä»£ç æ‰©æ•£æ¨¡å‹ç›´æ¥åº”ç”¨äºä»£ç ä¿®å¤ä»»åŠ¡ï¼Œæå‡ºé€šè¿‡å‘æŸåçš„ä»£ç ç‰‡æ®µæ·»åŠ å™ªå£°å¹¶æ¢å¤æ‰©æ•£è¿‡ç¨‹æ¥å®ç°ä¿®å¤ã€‚æ­¤å¤–ï¼Œè¯¥æ‰©æ•£æ¨¡å‹è¿˜è¢«ç”¨ä½œé«˜è´¨é‡çš„æ•°æ®ç”Ÿæˆå™¨ï¼Œé€šè¿‡åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­é‡‡æ ·ä¸­é—´ç¨‹åºä½œä¸ºè¾“å…¥å’Œæœ€ç»ˆç¨‹åºä½œä¸ºè¾“å‡ºï¼Œä¸ºä¿®å¤ä»»åŠ¡ç”Ÿæˆæµ·é‡çš„è®­ç»ƒæ•°æ®ã€‚ç ”ç©¶äººå‘˜åœ¨ Python, Excel å’Œ PowerShell ä¸‰ä¸ªé¢†åŸŸå¯¹è¿™äº›åº”ç”¨è¿›è¡Œäº†å®éªŒè¯„ä¼°ï¼Œå¹¶æ·±å…¥åˆ†æäº†å…¶æ€§èƒ½å±æ€§ã€‚å®éªŒç»“æœè¯æ˜äº†æ‰©æ•£æ¨¡å‹åœ¨ä½œä¸ºä»£ç ä¿®å¤ç®—å­å’Œæ•°æ®ç”Ÿæˆå™¨æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºè‡ªåŠ¨åŒ–ç¨‹åºä¿®å¤æä¾›äº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "12 pages",
      "pdf_url": "https://arxiv.org/pdf/2508.11110v1",
      "published_date": "2025-08-14 23:27:09 UTC",
      "updated_date": "2025-08-14 23:27:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:44:54.993379+00:00"
    },
    {
      "arxiv_id": "2508.11093v1",
      "title": "Utilizing Vision-Language Models as Action Models for Intent Recognition and Assistance",
      "title_zh": "å°†è§†è§‰è¯­è¨€æ¨¡å‹ä½œä¸ºåŠ¨ä½œæ¨¡å‹ç”¨äºæ„å›¾è¯†åˆ«ä¸è¾…åŠ©",
      "authors": [
        "Cesar Alan Contreras",
        "Manolis Chiou",
        "Alireza Rastegarpanah",
        "Michal Szulik",
        "Rustam Stolkin"
      ],
      "abstract": "Human-robot collaboration requires robots to quickly infer user intent, provide transparent reasoning, and assist users in achieving their goals. Our recent work introduced GUIDER, our framework for inferring navigation and manipulation intents. We propose augmenting GUIDER with a vision-language model (VLM) and a text-only language model (LLM) to form a semantic prior that filters objects and locations based on the mission prompt. A vision pipeline (YOLO for object detection and the Segment Anything Model for instance segmentation) feeds candidate object crops into the VLM, which scores their relevance given an operator prompt; in addition, the list of detected object labels is ranked by a text-only LLM. These scores weight the existing navigation and manipulation layers of GUIDER, selecting context-relevant targets while suppressing unrelated objects. Once the combined belief exceeds a threshold, autonomy changes occur, enabling the robot to navigate to the desired area and retrieve the desired object, while adapting to any changes in the operator's intent. Future work will evaluate the system on Isaac Sim using a Franka Emika arm on a Ridgeback base, with a focus on real-time assistance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å¢å¼ºå‹æ¡†æ¶ï¼Œé€šè¿‡å°†Vision-Language Models (VLM)å’ŒLarge Language Models (LLM)æ•´åˆä¸ºåŠ¨ä½œæ¨¡å‹ï¼Œæå‡æœºå™¨äººåœ¨æ„å›¾è¯†åˆ«ä¸åä½œè¾…åŠ©ä¸­çš„è¡¨ç°ã€‚è¯¥æ–¹æ³•åœ¨åŸæœ‰çš„GUIDERæ¡†æ¶åŸºç¡€ä¸Šï¼Œåˆ©ç”¨VLMå’ŒLLMæ„å»ºè¯­ä¹‰å…ˆéªŒ(semantic prior)ï¼Œæ ¹æ®æ“ä½œè€…çš„æŒ‡ä»¤æœ‰æ•ˆç­›é€‰ç›¸å…³çš„ç‰©ä½“å’Œä½ç½®ã€‚ç³»ç»Ÿè§†è§‰ç®¡çº¿ç»“åˆäº†YOLOç›®æ ‡æ£€æµ‹ä¸Segment Anything Model (SAM)å®ä¾‹åˆ†å‰²æŠ€æœ¯ï¼Œå°†å€™é€‰å›¾åƒè¾“å…¥VLMä»¥è¯„ä¼°å…¶ä¸ä»»åŠ¡æŒ‡ä»¤çš„ç›¸å…³æ€§ã€‚åŒæ—¶ï¼Œçº¯æ–‡æœ¬LLMå¯¹æ£€æµ‹åˆ°çš„ç‰©ä½“æ ‡ç­¾è¿›è¡Œæ’åºï¼Œè¿™äº›è¯„åˆ†å…±åŒåŠ æƒGUIDERçš„å¯¼èˆªå’Œæ“ä½œå±‚ï¼Œä»è€Œç²¾å‡†é”å®šä¸Šä¸‹æ–‡ç›¸å…³çš„ç›®æ ‡å¹¶æŠ‘åˆ¶æ— å…³å¹²æ‰°ã€‚å½“ç³»ç»Ÿç½®ä¿¡åº¦è¾¾åˆ°é˜ˆå€¼åï¼Œæœºå™¨äººèƒ½å¤Ÿè‡ªä¸»æ‰§è¡Œå¯¼èˆªå¹¶æŠ“å–ç›®æ ‡ç‰©ä½“ï¼Œå¹¶èƒ½å®æ—¶é€‚åº”æ“ä½œè€…æ„å›¾çš„å˜åŒ–ã€‚æœªæ¥çš„ç ”ç©¶å°†é‡ç‚¹åœ¨Isaac Simä»¿çœŸç¯å¢ƒä¸­åˆ©ç”¨Franka Emikaæœºæ¢°è‡‚å’ŒRidgebackåº•åº§å¯¹è¯¥ç³»ç»Ÿçš„å®æ—¶è¾…åŠ©æ•ˆèƒ½è¿›è¡Œæ·±å…¥è¯„ä¼°ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted at Human-Centered Robot Autonomy for Human-Robot Teams (HuRoboT) at IEEE RO-MAN 2025, Eindhoven, the Netherlands",
      "pdf_url": "https://arxiv.org/pdf/2508.11093v1",
      "published_date": "2025-08-14 22:19:09 UTC",
      "updated_date": "2025-08-14 22:19:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:45:06.095595+00:00"
    },
    {
      "arxiv_id": "2508.11090v1",
      "title": "Compressive Meta-Learning",
      "title_zh": "å‹ç¼©å…ƒå­¦ä¹ ",
      "authors": [
        "Daniel Mas Montserrat",
        "David Bonet",
        "Maria Perera",
        "Xavier GirÃ³-i-Nieto",
        "Alexander G. Ioannidis"
      ],
      "abstract": "The rapid expansion in the size of new datasets has created a need for fast and efficient parameter-learning techniques. Compressive learning is a framework that enables efficient processing by using random, non-linear features to project large-scale databases onto compact, information-preserving representations whose dimensionality is independent of the number of samples and can be easily stored, transferred, and processed. These database-level summaries are then used to decode parameters of interest from the underlying data distribution without requiring access to the original samples, offering an efficient and privacy-friendly learning framework. However, both the encoding and decoding techniques are typically randomized and data-independent, failing to exploit the underlying structure of the data. In this work, we propose a framework that meta-learns both the encoding and decoding stages of compressive learning methods by using neural networks that provide faster and more accurate systems than the current state-of-the-art approaches. To demonstrate the potential of the presented Compressive Meta-Learning framework, we explore multiple applications -- including neural network-based compressive PCA, compressive ridge regression, compressive k-means, and autoencoders.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è§„æ¨¡æ•°æ®é›†å¯¹é«˜æ•ˆå‚æ•°å­¦ä¹ çš„éœ€æ±‚ï¼Œæå‡ºäº† Compressive Meta-Learning æ¡†æ¶ã€‚ä¼ ç»Ÿçš„ Compressive learning ä¾èµ–éšæœºä¸”ç‹¬ç«‹äºæ•°æ®çš„ç¼–ç ä¸è§£ç æŠ€æœ¯ï¼Œå¾€å¾€æ— æ³•æœ‰æ•ˆåˆ©ç”¨æ•°æ®çš„æ½œåœ¨ç»“æ„ã€‚è¯¥æ¡†æ¶é€šè¿‡ç¥ç»ç½‘ç»œå¯¹ Compressive learning çš„ç¼–ç å’Œè§£ç é˜¶æ®µè¿›è¡Œå…ƒå­¦ä¹ (meta-learns)ï¼Œä»è€Œå®ç°äº†æ¯”ç°æœ‰ state-of-the-art æ–¹æ³•æ›´å¿«é€Ÿä¸”æ›´å‡†ç¡®çš„ç³»ç»Ÿã€‚è¯¥æ–¹æ³•å°†å¤§è§„æ¨¡æ•°æ®åº“æŠ•å½±ä¸ºç»´åº¦ä¸æ ·æœ¬é‡æ— å…³çš„ç´§å‡‘ä¿¡æ¯è¡¨ç¤ºï¼Œåœ¨æ— éœ€ç›´æ¥è®¿é—®åŸå§‹æ ·æœ¬çš„æƒ…å†µä¸‹å³å¯è§£ç å…³é”®å‚æ•°ï¼Œå…·æœ‰æé«˜çš„æ•ˆç‡å’Œéšç§ä¿æŠ¤ç‰¹æ€§ã€‚ç ”ç©¶åœ¨ compressive PCAã€compressive ridge regressionã€compressive k-means ä»¥åŠ autoencoders ç­‰å¤šä¸ªåº”ç”¨åœºæ™¯ä¸­éªŒè¯äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE",
        "cs.DB"
      ],
      "primary_category": "cs.LG",
      "comment": "Extended version of a paper accepted at KDD '25",
      "pdf_url": "https://arxiv.org/pdf/2508.11090v1",
      "published_date": "2025-08-14 22:08:06 UTC",
      "updated_date": "2025-08-14 22:08:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:45:10.487248+00:00"
    },
    {
      "arxiv_id": "2508.11085v2",
      "title": "A learning-driven automatic planning framework for proton PBS treatments of H&N cancers",
      "title_zh": "å¤´é¢ˆéƒ¨è‚¿ç˜¤è´¨å­ PBS æ²»ç–—çš„å­¦ä¹ é©±åŠ¨å‹è‡ªåŠ¨è®¡åˆ’æ¡†æ¶",
      "authors": [
        "Qingqing Wang",
        "Liqiang Xiao",
        "Chang Chang"
      ],
      "abstract": "Proton pencil beam scanning (PBS) treatment planning for head & neck (H&N) cancers involves numerous conflicting objectives, requiring iterative objective parameter adjustments to balance multiple clinical goals. We propose a learning-driven inverse optimizer and integrate it into a proximal policy optimization (PPO)-based planning framework to automatically generate high-quality plans for patients with diverse treatment requirements. The inverse optimizer is a learning-to-optimize (L2O) method that predicts update steps by learning from task-specific data distributions. For the first time, long-context processing techniques developed for large language models (LLMs) are utilized to address the scalability limitations of existing L2O methods, enabling simultaneous optimization over a substantially large set of variables. The PPO framework functions as an outer-loop virtual planner, autonomously adjusting objective parameters through a policy network, and the inner-loop L2O inverse optimizer computes machine-deliverable spot monitor unit (MU) values based on the PPO-refined objectives. Moreover, a Swin UnetR dose predictor is trained with prescription- and beam-specific information to estimate the initial objective parameters. In our experiments, total 97 patients with bilateral or ipsilateral H&N cancers are collected for training and testing. Compared with the second-order gradient-based methods, our L2O optimizer improves the effectiveness and efficiency of the time-consuming inverse optimization by 22.97% and 36.41%, respectively, and in conjunction with the PPO-based virtual planner, plans are generated within clinically acceptable times, i.e. 2.55 hours in average, and shows improved or comparable organs-at-risk sparing with superior target coverage compared with human-generated plans.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤´é¢ˆéƒ¨ç™Œç—‡(H&N cancers)çš„è´¨å­ç‚¹æ‰«æ(PBS)æ”¾ç–—è®¡åˆ’ä¸­å¤šç›®æ ‡ä¼˜åŒ–çš„å¤æ‚æ€§ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå­¦ä¹ é©±åŠ¨çš„è‡ªåŠ¨è®¡åˆ’æ¡†æ¶ã€‚è¯¥æ¡†æ¶é›†æˆäº†ä¸€ä¸ªåŸºäºè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–(PPO)çš„å¤–å±‚è™šæ‹Ÿè§„åˆ’å™¨å’Œä¸€ä¸ªåŸºäºå­¦ä¹ ä¼˜åŒ–(L2O)ç®—æ³•çš„å†…å±‚é€†ä¼˜åŒ–å™¨ï¼Œæ—¨åœ¨è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡çš„æ²»ç–—è®¡åˆ’ã€‚ç ”ç©¶é¦–æ¬¡å¼•å…¥äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„é•¿ä¸Šä¸‹æ–‡å¤„ç†æŠ€æœ¯ï¼Œä»¥è§£å†³ä¼ ç»ŸL2Oæ–¹æ³•åœ¨å¤„ç†å¤§è§„æ¨¡å˜é‡æ—¶çš„å¯æ‰©å±•æ€§é™åˆ¶ï¼Œå¹¶ç»“åˆSwin UnetRå‰‚é‡é¢„æµ‹å™¨æ¥ä¼°è®¡åˆå§‹ä¼˜åŒ–å‚æ•°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥L2Oä¼˜åŒ–å™¨åœ¨é€†ä¼˜åŒ–ä»»åŠ¡çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ä¸Šåˆ†åˆ«æ¯”äºŒé˜¶æ¢¯åº¦æ³•æå‡äº†22.97%å’Œ36.41%ã€‚æœ€ç»ˆç”Ÿæˆçš„æ”¾ç–—è®¡åˆ’åœ¨å¹³å‡2.55å°æ—¶å†…å®Œæˆï¼Œä¸”åœ¨å±åŠå™¨å®˜(OAR)ä¿æŠ¤å’Œé¶åŒºè¦†ç›–(Target coverage)æ–¹é¢è¾¾åˆ°æˆ–ä¼˜äºäººå·¥åˆ¶å®šçš„è®¡åˆ’ã€‚è¯¥æ¡†æ¶ä¸ºå®ç°é«˜æ•ˆã€è‡ªåŠ¨åŒ–çš„æ”¾å°„æ²»ç–—è§„åˆ’æä¾›äº†åˆ›æ–°çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "27 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.11085v2",
      "published_date": "2025-08-14 21:50:31 UTC",
      "updated_date": "2025-09-15 17:16:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:45:28.287556+00:00"
    },
    {
      "arxiv_id": "2508.11719v1",
      "title": "Are AI Machines Making Humans Obsolete?",
      "title_zh": "äººå·¥æ™ºèƒ½æœºå™¨æ˜¯å¦æ­£ä½¿äººç±»èµ°å‘è¿‡æ—¶ï¼Ÿ",
      "authors": [
        "Matthias Scheutz"
      ],
      "abstract": "This chapter starts with a sketch of how we got to \"generative AI\" (GenAI) and a brief summary of the various impacts it had so far. It then discusses some of the opportunities of GenAI, followed by the challenges and dangers, including dystopian outcomes resulting from using uncontrolled machine learning and our failures to understand the results. It concludes with some suggestions for how to control GenAI and address its dangers.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢³ç†äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½(Generative AI)çš„å‘å±•è·¯å¾„ï¼Œå¹¶å¯¹å…¶ç›®å‰äº§ç”Ÿçš„å½±å“è¿›è¡Œäº†ç³»ç»Ÿæ€§å›é¡¾ã€‚æ–‡ä¸­æ·±å…¥æ¢è®¨äº†GenAIå¸¦æ¥çš„å¤šé‡æœºé‡ï¼ŒåŒæ—¶è­¦ç¤ºäº†ä¸å—æ§çš„æœºå™¨å­¦ä¹ (Machine Learning)ä»¥åŠäººç±»å¯¹ç»“æœç†è§£å¤±æ•ˆå¯èƒ½å¯¼è‡´çš„ä¹Œæ‰˜é‚¦å¼è´Ÿé¢åæœã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç³»åˆ—æ—¨åœ¨æ§åˆ¶GenAIå¹¶åº”å¯¹å…¶æ½œåœ¨é£é™©çš„ç­–ç•¥å»ºè®®ã€‚å…¨æ–‡é€šè¿‡åˆ†ææŠ€æœ¯å‘å±•çš„åŒé‡æ€§ï¼Œæ¢è®¨äº†AIæŠ€æœ¯æ˜¯å¦ä¼šä½¿äººç±»è¿‡æ—¶è¿™ä¸€æ ¸å¿ƒè®®é¢˜ï¼Œå¹¶ä¸ºæ„å»ºå¯æ§çš„æ™ºèƒ½åŒ–æœªæ¥æä¾›äº†æŒ‡å¯¼æ€è·¯ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "Forthcoming in Ramana Kumar Vinjamuri (ed.) \"Bridging the Gap between Mind and Machine\", Springer",
      "pdf_url": "https://arxiv.org/pdf/2508.11719v1",
      "published_date": "2025-08-14 21:24:31 UTC",
      "updated_date": "2025-08-14 21:24:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:45:21.590375+00:00"
    },
    {
      "arxiv_id": "2508.11074v1",
      "title": "LD-LAudio-V1: Video-to-Long-Form-Audio Generation Extension with Dual Lightweight Adapters",
      "title_zh": "LD-LAudio-V1ï¼šåŸºäºåŒè½»é‡çº§é€‚é…å™¨çš„è§†é¢‘åˆ°é•¿éŸ³é¢‘ç”Ÿæˆæ‰©å±•",
      "authors": [
        "Haomin Zhang",
        "Kristin Qi",
        "Shuxin Yang",
        "Zihao Chen",
        "Chaofan Ding",
        "Xinhan Di"
      ],
      "abstract": "Generating high-quality and temporally synchronized audio from video content is essential for video editing and post-production tasks, enabling the creation of semantically aligned audio for silent videos. However, most existing approaches focus on short-form audio generation for video segments under 10 seconds or rely on noisy datasets for long-form video-to-audio zsynthesis. To address these limitations, we introduce LD-LAudio-V1, an extension of state-of-the-art video-to-audio models and it incorporates dual lightweight adapters to enable long-form audio generation. In addition, we release a clean and human-annotated video-to-audio dataset that contains pure sound effects without noise or artifacts. Our method significantly reduces splicing artifacts and temporal inconsistencies while maintaining computational efficiency. Compared to direct fine-tuning with short training videos, LD-LAudio-V1 achieves significant improvements across multiple metrics: $FD_{\\text{passt}}$ 450.00 $\\rightarrow$ 327.29 (+27.27%), $FD_{\\text{panns}}$ 34.88 $\\rightarrow$ 22.68 (+34.98%), $FD_{\\text{vgg}}$ 3.75 $\\rightarrow$ 1.28 (+65.87%), $KL_{\\text{panns}}$ 2.49 $\\rightarrow$ 2.07 (+16.87%), $KL_{\\text{passt}}$ 1.78 $\\rightarrow$ 1.53 (+14.04%), $IS_{\\text{panns}}$ 4.17 $\\rightarrow$ 4.30 (+3.12%), $IB_{\\text{score}}$ 0.25 $\\rightarrow$ 0.28 (+12.00%), $Energy\\Delta10\\text{ms}$ 0.3013 $\\rightarrow$ 0.1349 (+55.23%), $Energy\\Delta10\\text{ms(vs.GT)}$ 0.0531 $\\rightarrow$ 0.0288 (+45.76%), and $Sem.\\,Rel.$ 2.73 $\\rightarrow$ 3.28 (+20.15%). Our dataset aims to facilitate further research in long-form video-to-audio generation and is available at https://github.com/deepreasonings/long-form-video2audio.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LD-LAudio-V1ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ video-to-audio æ¨¡å‹åœ¨ç”Ÿæˆ10ç§’ä»¥ä¸Šé•¿æ ¼å¼éŸ³é¢‘æ—¶é¢ä¸´çš„å™ªå£°å¹²æ‰°åŠæ—¶é—´ä¸ä¸€è‡´æ€§é—®é¢˜ã€‚é€šè¿‡å¼•å…¥ Dual Lightweight Adaptersï¼ŒLD-LAudio-V1 æˆåŠŸå®ç°äº†å¯¹é•¿è§†é¢‘çš„è¯­ä¹‰å¯¹é½ä¸é«˜è´¨é‡éŸ³é¢‘åˆæˆã€‚ç ”ç©¶å›¢é˜ŸåŒæ­¥å‘å¸ƒäº†ä¸€ä¸ªç»è¿‡äººå·¥æ ‡æ³¨çš„é«˜è´¨é‡éŸ³æ•ˆæ•°æ®é›†ï¼Œæœ‰æ•ˆå‡å°‘äº†éŸ³é¢‘æ‹¼æ¥ä¸­çš„ä¼ªå½±å¹¶ä¿æŒäº†æé«˜çš„è®¡ç®—æ•ˆç‡ã€‚å®éªŒæ•°æ®è¡¨æ˜ï¼ŒLD-LAudio-V1 åœ¨ $FD_{\\text{passt}}$ã€è¯­ä¹‰ç›¸å…³æ€§ï¼ˆSem. Rel.ï¼‰å’Œèƒ½é‡å·®å€¼ï¼ˆEnergy $\\Delta$ï¼‰ç­‰æ ¸å¿ƒæŒ‡æ ‡ä¸Šå‡æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„çŸ­è§†é¢‘å¾®è°ƒæ–¹æ³•ã€‚è¯¥æ¡†æ¶ä¸ä»…æå‡äº†é•¿æ ¼å¼è§†é¢‘è½¬éŸ³é¢‘çš„åŒæ­¥ç²¾åº¦ï¼Œä¹Ÿä¸ºè§†é¢‘ç¼–è¾‘å’ŒåæœŸåˆ¶ä½œä¸­çš„è‡ªåŠ¨åŒ–éŸ³æ•ˆç”Ÿæˆæä¾›äº†é‡è¦çš„æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CV",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Gen4AVC@ICCV: 1st Workshop on Generative AI for Audio-Visual Content Creation",
      "pdf_url": "https://arxiv.org/pdf/2508.11074v1",
      "published_date": "2025-08-14 21:11:57 UTC",
      "updated_date": "2025-08-14 21:11:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:45:26.000832+00:00"
    },
    {
      "arxiv_id": "2508.11070v1",
      "title": "From Individual to Multi-Agent Algorithmic Recourse: Minimizing the Welfare Gap via Capacitated Bipartite Matching",
      "title_zh": "ä»ä¸ªä½“åˆ°å¤šæ™ºèƒ½ä½“ç®—æ³•è¿½ç´¢ï¼šåŸºäºå¸¦å®¹é‡é™åˆ¶äºŒåˆ†å›¾åŒ¹é…çš„ç¦åˆ©å·®è·æœ€å°åŒ–",
      "authors": [
        "Zahra Khotanlou",
        "Kate Larson",
        "Amir-Hossein Karimi"
      ],
      "abstract": "Decision makers are increasingly relying on machine learning in sensitive situations. In such settings, algorithmic recourse aims to provide individuals with actionable and minimally costly steps to reverse unfavorable AI-driven decisions. While existing research predominantly focuses on single-individual (i.e., seeker) and single-model (i.e., provider) scenarios, real-world applications often involve multiple interacting stakeholders. Optimizing outcomes for seekers under an individual welfare approach overlooks the inherently multi-agent nature of real-world systems, where individuals interact and compete for limited resources. To address this, we introduce a novel framework for multi-agent algorithmic recourse that accounts for multiple recourse seekers and recourse providers. We model this many-to-many interaction as a capacitated weighted bipartite matching problem, where matches are guided by both recourse cost and provider capacity. Edge weights, reflecting recourse costs, are optimized for social welfare while quantifying the welfare gap between individual welfare and this collectively feasible outcome. We propose a three-layer optimization framework: (1) basic capacitated matching, (2) optimal capacity redistribution to minimize the welfare gap, and (3) cost-aware optimization balancing welfare maximization with capacity adjustment costs. Experimental validation on synthetic and real-world datasets demonstrates that our framework enables the many-to-many algorithmic recourse to achieve near-optimal welfare with minimum modification in system settings. This work extends algorithmic recourse from individual recommendations to system-level design, providing a tractable path toward higher social welfare while maintaining individual actionability.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰ Algorithmic Recourse ä¸»è¦å…³æ³¨å•ä½“åœºæ™¯è€Œå¿½è§†ç°å®ä¸–ç•Œä¸­å¤šæ™ºèƒ½ä½“ç«äº‰æœ‰é™èµ„æºçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªå¤šæ™ºèƒ½ä½“ç®—æ³•è¡¥æ•‘çš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†å¤šä¸ªå¯»æ±‚è€…ä¸æä¾›è€…ä¹‹é—´çš„äº¤äº’å»ºæ¨¡ä¸º Capacitated Weighted Bipartite Matching é—®é¢˜ï¼Œæ—¨åœ¨é€šè¿‡ä¼˜åŒ–è¾¹æƒé‡æ¥æœ€å°åŒ–ä¸ªä½“ç¦åˆ©ä¸é›†ä½“å¯è¡Œç»“æœä¹‹é—´çš„ Welfare Gapã€‚ç ”ç©¶è€…è®¾è®¡äº†ä¸€ä¸ªåŒ…å«åŸºç¡€å®¹é‡åŒ¹é…ã€æœ€ä¼˜å®¹é‡å†åˆ†é…ä»¥åŠæˆæœ¬æ„ŸçŸ¥ä¼˜åŒ–çš„ä¸‰å±‚ä¼˜åŒ–æ¡†æ¶ï¼Œä»¥å¹³è¡¡ç¤¾ä¼šç¦åˆ©æœ€å¤§åŒ–ä¸ç³»ç»Ÿè°ƒæ•´æˆæœ¬ã€‚åœ¨åˆæˆåŠçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½åœ¨å¯¹ç³»ç»Ÿè®¾ç½®è¿›è¡Œæå°ä¿®æ”¹çš„æƒ…å†µä¸‹ï¼Œä½¿å¤šå¯¹å¤šè¡¥æ•‘æ–¹æ¡ˆè¾¾åˆ°è¿‘ä¹æœ€ä¼˜çš„ç¦åˆ©æ°´å¹³ã€‚æ­¤é¡¹å·¥ä½œæˆåŠŸå°†ç®—æ³•è¡¥æ•‘ä»ä¸ªä½“åŒ–å»ºè®®æ‰©å±•è‡³ç³»ç»Ÿçº§è®¾è®¡ï¼Œä¸ºåœ¨ä¿éšœä¸ªä½“ Actionability çš„åŒæ—¶æå‡æ•´ä½“ç¤¾ä¼šç¦åˆ©æä¾›äº†é«˜æ•ˆä¸”å¯å¤„ç†çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11070v1",
      "published_date": "2025-08-14 21:04:24 UTC",
      "updated_date": "2025-08-14 21:04:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:45:31.085839+00:00"
    },
    {
      "arxiv_id": "2508.11052v1",
      "title": "AI That Helps Us Help Each Other: A Proactive System for Scaffolding Mentor-Novice Collaboration in Entrepreneurship Coaching",
      "title_zh": "åŠ©åŠ›äº’åŠ©çš„äººå·¥æ™ºèƒ½ï¼šä¸€ç§åœ¨åˆ›ä¸šè¾…å¯¼ä¸­æ”¯æ’‘å¸ˆå¾’åä½œçš„ä¸»åŠ¨å¼ç³»ç»Ÿ",
      "authors": [
        "Evey Jiaxin Huang",
        "Matthew Easterday",
        "Elizabeth Gerber"
      ],
      "abstract": "Entrepreneurship requires navigating open-ended, ill-defined problems: identifying risks, challenging assumptions, and making strategic decisions under deep uncertainty. Novice founders often struggle with these metacognitive demands, while mentors face limited time and visibility to provide tailored support. We present a human-AI coaching system that combines a domain-specific cognitive model of entrepreneurial risk with a large language model (LLM) to proactively scaffold both novice and mentor thinking. The system proactively poses diagnostic questions that challenge novices' thinking and helps both novices and mentors plan for more focused and emotionally attuned meetings. Critically, mentors can inspect and modify the underlying cognitive model, shaping the logic of the system to reflect their evolving needs. Through an exploratory field deployment, we found that using the system supported novice metacognition, helped mentors plan emotionally attuned strategies, and improved meeting depth, intentionality, and focus--while also surfaced key tensions around trust, misdiagnosis, and expectations of AI. We contribute design principles for proactive AI systems that scaffold metacognition and human-human collaboration in complex, ill-defined domains, offering implications for similar domains like healthcare, education, and knowledge work.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ—¨åœ¨æ”¯æŒåˆ›ä¸šè¾…å¯¼ä¸­å¯¼å¸ˆä¸æ–°æ‰‹åä½œçš„äººæœºåä½œ(human-AI)è¾…å¯¼ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³åˆ›ä¸šè¿‡ç¨‹ä¸­åº”å¯¹ä¸ç¡®å®šæ€§æ—¶çš„å…ƒè®¤çŸ¥(metacognitive)éœ€æ±‚ã€‚è¯¥ç³»ç»Ÿå°†åˆ›ä¸šé£é™©çš„é¢†åŸŸç‰¹å®šè®¤çŸ¥æ¨¡å‹(cognitive model)ä¸å¤§è¯­è¨€æ¨¡å‹(LLM)ç›¸ç»“åˆï¼Œé€šè¿‡ä¸»åŠ¨æå‡ºè¯Šæ–­æ€§é—®é¢˜æ¥æŒ‘æˆ˜æ–°æ‰‹çš„æ€ç»´ï¼Œå¹¶ååŠ©åŒæ–¹è§„åˆ’æ›´å…·æ·±åº¦å’Œæƒ…æ„Ÿå…±é¸£çš„ä¼šè®®ã€‚å¯¼å¸ˆå¯ä»¥æ£€æŸ¥å¹¶ä¿®æ”¹åº•å±‚çš„è®¤çŸ¥æ¨¡å‹ï¼Œä½¿AIçš„é€»è¾‘ä¸å…¶å®é™…éœ€æ±‚ä¿æŒä¸€è‡´ã€‚å®åœ°éƒ¨ç½²ç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿæœ‰æ•ˆæ”¯æŒäº†æ–°æ‰‹çš„å…ƒè®¤çŸ¥èƒ½åŠ›ï¼Œæ”¹å–„äº†ä¼šè®®çš„æ·±åº¦å’Œä¸“æ³¨åº¦ï¼ŒåŒæ—¶ä¹Ÿæ­ç¤ºäº†å…³äºä¿¡ä»»ã€è¯¯è¯Šå’ŒAIé¢„æœŸæ–¹é¢çš„æ ¸å¿ƒå¼ åŠ›ã€‚è¯¥ç ”ç©¶ä¸ºåœ¨å¤æ‚ã€å®šä¹‰æ¨¡ç³Šçš„é¢†åŸŸä¸­å¼€å‘æ”¯æŒå…ƒè®¤çŸ¥å’Œäººé™…åä½œçš„ä¸»åŠ¨å¼AI(proactive AI)ç³»ç»Ÿæä¾›äº†å…³é”®çš„è®¾è®¡åŸåˆ™ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "To appear in CSCW 2025 Volume 9",
      "pdf_url": "https://arxiv.org/pdf/2508.11052v1",
      "published_date": "2025-08-14 20:23:48 UTC",
      "updated_date": "2025-08-14 20:23:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:45:32.486573+00:00"
    },
    {
      "arxiv_id": "2508.15809v3",
      "title": "Chain-of-Query: Unleashing the Power of LLMs in SQL-Aided Table Understanding via Multi-Agent Collaboration",
      "title_zh": "Chain-of-Queryï¼šé€šè¿‡å¤šæ™ºèƒ½ä½“åä½œé‡Šæ”¾ LLMs åœ¨ SQL è¾…åŠ©è¡¨æ ¼ç†è§£ä¸­çš„æ½œèƒ½",
      "authors": [
        "Songyuan Sui",
        "Hongyi Liu",
        "Serena Liu",
        "Li Li",
        "Soo-Hyun Choi",
        "Rui Chen",
        "Xia Hu"
      ],
      "abstract": "Table understanding requires structured, multi-step reasoning. Large Language Models (LLMs) struggle with it due to the structural complexity of tabular data. Recently, multi-agent frameworks for SQL generation have shown promise in tackling the challenges of understanding tabular data, but existing approaches often suffer from limitations such as the inability to comprehend table structure for reliable SQL generation, error propagation that results in invalid queries, and over-reliance on execution correctness. To address these issues, we propose Chain-of-Query (CoQ), a novel multi-agent framework for SQL-aided table understanding. CoQ adopts natural-language-style representations of table schemas to abstract away structural noise and enhance understanding. It employs a clause-by-clause SQL generation strategy to improve query quality and introduces a hybrid reasoning division that separates SQL-based mechanical reasoning from LLM-based logical inference, thereby reducing reliance on execution outcomes. Extensive experiments across four models and five widely used benchmarks demonstrate that CoQ achieves substantial accuracy improvements and significantly lowers invalid SQL rates compared to prior generic LLM-based, SQL-aided, and hybrid baselines, confirming its superior effectiveness in table understanding. The code is available at https://github.com/SongyuanSui/ChainofQuery.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Chain-of-Query (CoQ)ï¼Œä¸€ç§æ—¨åœ¨å¢å¼º SQL è¾…åŠ©è¡¨æ ¼ç†è§£çš„æ–°å‹å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œä»¥è§£å†³å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†å¤æ‚ç»“æ„åŒ–æ•°æ®æ—¶é¢ä¸´çš„è¡¨ç»“æ„ç†è§£ä¸è¶³åŠé”™è¯¯ä¼ æ’­ç­‰æŒ‘æˆ˜ã€‚CoQ é‡‡ç”¨è‡ªç„¶è¯­è¨€é£æ ¼çš„è¡¨ç»“æ„ï¼ˆtable schemasï¼‰è¡¨ç¤ºæ³•æ¥æ¶ˆé™¤ç»“æ„å™ªå£°ï¼Œå¹¶ç»“åˆé€å­å¥ç”Ÿæˆ SQL (clause-by-clause SQL generation) çš„ç­–ç•¥ä»¥æå‡æŸ¥è¯¢è´¨é‡ã€‚åŒæ—¶ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†æ··åˆæ¨ç†åˆ†å·¥æœºåˆ¶ï¼Œå°†åŸºäº SQL çš„æœºæ¢°æ¨ç†ä¸åŸºäº LLM çš„é€»è¾‘æ¨ç†æœ‰æ•ˆåˆ†ç¦»ï¼Œä»è€Œé™ä½äº†å¯¹ä»£ç æ‰§è¡Œæ­£ç¡®æ€§çš„è¿‡åº¦ä¾èµ–ã€‚åœ¨äº”ä¸ªä¸»æµåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCoQ åœ¨å¤šç§æ¨¡å‹ä¸Šå‡å®ç°äº†æ˜¾è‘—çš„å‡†ç¡®ç‡æå‡ï¼Œå¹¶å¤§å¹…é™ä½äº†æ— æ•ˆ SQL çš„ç”Ÿæˆç‡ã€‚è¿™äº›ç»“æœå……åˆ†è¯æ˜äº† CoQ åœ¨å¤æ‚è¡¨æ ¼ç†è§£ä»»åŠ¡ä¸­ç›¸è¾ƒäºç°æœ‰ SQL è¾…åŠ©åŠæ··åˆåŸºçº¿æ¨¡å‹çš„å“è¶Šæœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.CL",
      "comment": "AACL 2025 Main Conference (Oral)",
      "pdf_url": "https://arxiv.org/pdf/2508.15809v3",
      "published_date": "2025-08-14 19:46:46 UTC",
      "updated_date": "2025-11-30 16:21:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:45:50.195977+00:00"
    },
    {
      "arxiv_id": "2508.11037v1",
      "title": "Learning with Confidence",
      "title_zh": "ç½®ä¿¡å­¦ä¹ ",
      "authors": [
        "Oliver Ethan Richardson"
      ],
      "abstract": "We characterize a notion of confidence that arises in learning or updating beliefs: the amount of trust one has in incoming information and its impact on the belief state. This learner's confidence can be used alongside (and is easily mistaken for) probability or likelihood, but it is fundamentally a different concept -- one that captures many familiar concepts in the literature, including learning rates and number of training epochs, Shafer's weight of evidence, and Kalman gain. We formally axiomatize what it means to learn with confidence, give two canonical ways of measuring confidence on a continuum, and prove that confidence can always be represented in this way. Under additional assumptions, we derive more compact representations of confidence-based learning in terms of vector fields and loss functions. These representations induce an extended language of compound \"parallel\" observations. We characterize Bayes Rule as the special case of an optimizing learner whose loss representation is a linear expectation.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹å­¦ä¹ æˆ–æ›´æ–°ä¿¡å¿µè¿‡ç¨‹ä¸­äº§ç”Ÿçš„â€œç½®ä¿¡åº¦â€ï¼ˆconfidenceï¼‰è¿›è¡Œäº†å®šä¹‰å’Œåˆ»ç”»ï¼Œå³å­¦ä¹ è€…å¯¹æ–°ä¿¡æ¯çš„ä¿¡ä»»ç¨‹åº¦åŠå…¶å¯¹ä¿¡å¿µçŠ¶æ€çš„å½±å“ã€‚è¿™ç§å­¦ä¹ è€…ç½®ä¿¡åº¦ä¸æ¦‚ç‡æˆ–ä¼¼ç„¶æ€§æœ‰ç€æœ¬è´¨åŒºåˆ«ï¼Œå®ƒæ¶µç›–äº†å­¦ä¹ ç‡ã€è®­ç»ƒè½®æ•°ï¼ˆepochsï¼‰ã€Shaferçš„è¯æ®æƒé‡ä»¥åŠå¡å°”æ›¼å¢ç›Šï¼ˆKalman gainï¼‰ç­‰å¤šä¸ªæ ¸å¿ƒæ¦‚å¿µã€‚ä½œè€…å¯¹å¸¦ç½®ä¿¡åº¦çš„å­¦ä¹ è¿›è¡Œäº†æ­£å¼çš„å…¬ç†åŒ–ï¼ˆaxiomatizeï¼‰ï¼Œæä¾›äº†ä¸¤ç§åœ¨è¿ç»­ç»Ÿä¸€ä½“ä¸Šè¡¡é‡ç½®ä¿¡åº¦çš„è§„èŒƒæ–¹æ³•ï¼Œå¹¶è¯æ˜äº†ç½®ä¿¡åº¦æ€»èƒ½ä»¥æ­¤æ–¹å¼è¡¨ç¤ºã€‚åœ¨é¢å¤–å‡è®¾ä¸‹ï¼Œç ”ç©¶æ¨å¯¼å‡ºäº†åŸºäºå‘é‡åœºå’ŒæŸå¤±å‡½æ•°çš„æ›´ç®€æ´çš„ç½®ä¿¡åº¦å­¦ä¹ è¡¨ç¤ºï¼Œè¿™äº›è¡¨ç¤ºè¿›ä¸€æ­¥å¼•å…¥äº†å¤åˆâ€œå¹¶è¡Œâ€è§‚æµ‹çš„æ‰©å±•è¯­è¨€ã€‚æœ€åï¼Œè¯¥ç ”ç©¶å°†è´å¶æ–¯å‡†åˆ™ï¼ˆBayes Ruleï¼‰åˆ»ç”»ä¸ºä¸€ä¸ªä¼˜åŒ–å­¦ä¹ è€…çš„ç‰¹ä¾‹ï¼Œå…¶æŸå¤±è¡¨ç¤ºä¸ºçº¿æ€§æœŸæœ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.DG"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted for oral UAI 2025, plus some additional modifications for clarity",
      "pdf_url": "https://arxiv.org/pdf/2508.11037v1",
      "published_date": "2025-08-14 19:45:40 UTC",
      "updated_date": "2025-08-14 19:45:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:45:58.350664+00:00"
    },
    {
      "arxiv_id": "2508.11033v2",
      "title": "Note on Selection Bias in Observational Estimates of Algorithmic Progress",
      "title_zh": "å…³äºç®—æ³•è¿›å±•è§‚æµ‹ä¼°è®¡ä¸­é€‰æ‹©åå·®çš„æ³¨è®°",
      "authors": [
        "Parker Whitfill"
      ],
      "abstract": "Ho et. al (2024) attempts to estimate the degree of algorithmic progress from language models. They collect observational data on language models' loss and compute over time, and argue that as time has passed, language models' algorithmic efficiency has been rising. That is, the loss achieved for fixed compute has been dropping over time. In this note, I raise one potential methodological problem with the estimation strategy. Intuitively, if part of algorithmic quality is latent, and compute choices are endogenous to algorithmic quality, then resulting estimates of algorithmic quality will be contaminated by selection bias.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Ho et. al (2024) å…³äºè¯­è¨€æ¨¡å‹ç®—æ³•è¿›æ­¥çš„ä¼°ç®—æ–¹æ³•æå‡ºäº†æ‰¹åˆ¤æ€§è§è§£ã€‚åŸç ”ç©¶é€šè¿‡åˆ†æè¯­è¨€æ¨¡å‹çš„è®­ç»ƒæŸå¤±(loss)ä¸è®¡ç®—é‡(compute)éšæ—¶é—´çš„å˜åŒ–ï¼Œè®¤ä¸ºç®—æ³•æ•ˆç‡(algorithmic efficiency)åœ¨ä¸æ–­æé«˜ï¼Œå³åœ¨å›ºå®šè®¡ç®—é‡ä¸‹å®ç°çš„æŸå¤±åœ¨æŒç»­ä¸‹é™ã€‚ä½œè€…åœ¨æœ¬æ–‡ä¸­æŒ‡å‡ºï¼Œè¿™ç§åŸºäºè§‚æµ‹æ•°æ®çš„ä¼°ç®—ç­–ç•¥å­˜åœ¨æ½œåœ¨çš„æ–¹æ³•è®ºç¼ºé™·ï¼Œå³é€‰æ‹©åå·®(selection bias)é—®é¢˜ã€‚å¦‚æœç®—æ³•è´¨é‡çš„æŸäº›éƒ¨åˆ†æ˜¯æ½œåœ¨çš„(latent)ï¼Œä¸”è®¡ç®—èµ„æºçš„é€‰æ‹©å¯¹ç®—æ³•è´¨é‡å…·æœ‰å†…ç”Ÿæ€§(endogenous)ï¼Œé‚£ä¹ˆæœ€ç»ˆå¾—å‡ºçš„ç®—æ³•è¿›æ­¥ä¼°ç®—ç»“æœå°†ä¼šå—åˆ°æ•°æ®æ±¡æŸ“ã€‚è¿™ä¸€è®¨è®ºå¼ºè°ƒäº†åœ¨è¯„ä¼°æŠ€æœ¯æ¼”è¿›æ—¶ï¼Œå¿…é¡»å®¡æ…å¤„ç†æ¨¡å‹è§„æ¨¡å†³ç­–ä¸ç®—æ³•è´¨é‡ä¹‹é—´çš„å¤æ‚å…³è”ï¼Œä»¥é¿å…å¯¹ç®—æ³•è¿›æ­¥ç¨‹åº¦äº§ç”Ÿè¯¯å¯¼æ€§ç»“è®ºã€‚",
      "categories": [
        "econ.GN",
        "cs.AI"
      ],
      "primary_category": "econ.GN",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11033v2",
      "published_date": "2025-08-14 19:38:10 UTC",
      "updated_date": "2025-08-18 05:12:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:45:57.550095+00:00"
    },
    {
      "arxiv_id": "2508.11031v1",
      "title": "Risk-Based Prognostics and Health Management",
      "title_zh": "åŸºäºé£é™©çš„æ•…éšœé¢„æµ‹ä¸å¥åº·ç®¡ç†",
      "authors": [
        "John W. Sheppard"
      ],
      "abstract": "It is often the case that risk assessment and prognostics are viewed as related but separate tasks. This chapter describes a risk-based approach to prognostics that seeks to provide a tighter coupling between risk assessment and fault prediction. We show how this can be achieved using the continuous-time Bayesian network as the underlying modeling framework. Furthermore, we provide an overview of the techniques that are available to derive these models from data and show how they might be used in practice to achieve tasks like decision support and performance-based logistics. This work is intended to provide an overview of the recent developments related to risk-based prognostics, and we hope that it will serve as a tutorial of sorts that will assist others in adopting these techniques.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†Risk Assessmentä¸Prognosticsä¹‹é—´çš„å…³ç³»ï¼Œæå‡ºäº†ä¸€ç§æ—¨åœ¨å®ç°é£é™©è¯„ä¼°ä¸æ•…éšœé¢„æµ‹ç´§å¯†è€¦åˆçš„Risk-based Prognosticsæ–¹æ³•ã€‚è¯¥æ–¹æ³•é‡‡ç”¨Continuous-time Bayesian Networkä½œä¸ºæ ¸å¿ƒå»ºæ¨¡æ¡†æ¶ï¼Œæœ‰æ•ˆæ•´åˆäº†ä¸ç¡®å®šæ€§ä¸æ—¶åºæ¼”åŒ–ç‰¹å¾ã€‚æ–‡ç« ç³»ç»Ÿåœ°æ¦‚è¿°äº†ä»æ•°æ®ä¸­æ¨å¯¼è¿™äº›æ¨¡å‹çš„æŠ€æœ¯æ‰‹æ®µï¼Œå¹¶è¯¦ç»†è¯´æ˜äº†å…¶åœ¨Decision Supportå’ŒPerformance-based Logisticsç­‰å®é™…ä»»åŠ¡ä¸­çš„åº”ç”¨æ–¹å¼ã€‚è¯¥å·¥ä½œé€šè¿‡æ€»ç»“Risk-based Prognosticsçš„æœ€æ–°ç ”ç©¶æˆæœï¼Œä¸ºç›¸å…³é¢†åŸŸæä¾›äº†å…·æœ‰æŒ‡å¯¼æ„ä¹‰çš„æ•™ç¨‹ï¼Œæ—¨åœ¨æ¨åŠ¨è¿™äº›å…ˆè¿›æŠ€æœ¯åœ¨å·¥ä¸šå®è·µä¸­çš„é‡‡çº³ä¸è½åœ°ã€‚",
      "categories": [
        "eess.SY",
        "cs.AI",
        "stat.AP"
      ],
      "primary_category": "eess.SY",
      "comment": "Appears as Chapter 27 in Realizing Complex Integrated Systems, Anthony P. Ambler and John W. Sheppard (ads.), CRC Press, 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.11031v1",
      "published_date": "2025-08-14 19:31:33 UTC",
      "updated_date": "2025-08-14 19:31:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:46:28.785920+00:00"
    },
    {
      "arxiv_id": "2508.11025v1",
      "title": "Zono-Conformal Prediction: Zonotope-Based Uncertainty Quantification for Regression and Classification Tasks",
      "title_zh": "Zono-Conformal Predictionï¼šåŸºäºå¸¦çŠ¶å¤šèƒå½¢çš„å›å½’ä¸åˆ†ç±»ä»»åŠ¡ä¸ç¡®å®šæ€§é‡åŒ–",
      "authors": [
        "Laura LÃ¼tzow",
        "Michael Eichelbeck",
        "Mykel J. Kochenderfer",
        "Matthias Althoff"
      ],
      "abstract": "Conformal prediction is a popular uncertainty quantification method that augments a base predictor with prediction sets with statistically valid coverage guarantees. However, current methods are often computationally expensive and data-intensive, as they require constructing an uncertainty model before calibration. Moreover, existing approaches typically represent the prediction sets with intervals, which limits their ability to capture dependencies in multi-dimensional outputs. We address these limitations by introducing zono-conformal prediction, a novel approach inspired by interval predictor models and reachset-conformant identification that constructs prediction zonotopes with assured coverage. By placing zonotopic uncertainty sets directly into the model of the base predictor, zono-conformal predictors can be identified via a single, data-efficient linear program. While we can apply zono-conformal prediction to arbitrary nonlinear base predictors, we focus on feed-forward neural networks in this work. Aside from regression tasks, we also construct optimal zono-conformal predictors in classification settings where the output of an uncertain predictor is a set of possible classes. We provide probabilistic coverage guarantees and present methods for detecting outliers in the identification data. In extensive numerical experiments, we show that zono-conformal predictors are less conservative than interval predictor models and standard conformal prediction methods, while achieving a similar coverage over the test data.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿçš„ç¬¦åˆé¢„æµ‹(Conformal Prediction)æ–¹æ³•åœ¨è®¡ç®—å¼€é”€ã€æ•°æ®éœ€æ±‚ä»¥åŠæ•æ‰å¤šç»´è¾“å‡ºä¾èµ–å…³ç³»æ–¹é¢çš„å±€é™ï¼Œæå‡ºäº†Zono-Conformal Predictionæ¡†æ¶ã€‚è¯¥æ–¹æ³•å—åˆ°åŒºé—´é¢„æµ‹æ¨¡å‹(Interval Predictor Models)å’Œåˆ°è¾¾é›†ç¬¦åˆè¾¨è¯†(Reachset-Conformant Identification)çš„å¯å‘ï¼Œé€šè¿‡åœ¨åŸºç¡€é¢„æµ‹å™¨ä¸­ç›´æ¥åµŒå…¥å¸¦çŠ¶ä½“(Zonotope)ä¸ç¡®å®šæ€§é›†åˆï¼Œæ„å»ºå‡ºå…·æœ‰ç»Ÿè®¡è¦†ç›–ä¿è¯çš„é¢„æµ‹å¸¦çŠ¶ä½“ã€‚è¯¥é¢„æµ‹å™¨å¯é€šè¿‡å•ä¸€ä¸”é«˜æ•ˆçš„çº¿æ€§è§„åˆ’(Linear Program)è¿›è¡Œè¯†åˆ«ï¼Œç ”ç©¶é‡ç‚¹å±•ç¤ºäº†å…¶åœ¨å‰é¦ˆç¥ç»ç½‘ç»œ(Feed-forward Neural Networks)ä¸­çš„åº”ç”¨ã€‚é™¤äº†å›å½’ä»»åŠ¡å¤–ï¼Œç ”ç©¶è¿˜é’ˆå¯¹åˆ†ç±»åœºæ™¯æ„å»ºäº†æœ€ä¼˜é¢„æµ‹å™¨ï¼Œå¹¶æä¾›äº†æ¦‚ç‡è¦†ç›–ä¿è¯åŠç¦»ç¾¤å€¼æ£€æµ‹æ–¹æ³•ã€‚æ•°å€¼å®éªŒè¡¨æ˜ï¼ŒZono-conformalé¢„æµ‹å™¨åœ¨ä¿æŒç›¸ä¼¼æµ‹è¯•é›†è¦†ç›–ç‡çš„åŒæ—¶ï¼Œæ¯”ä¼ ç»Ÿçš„åŒºé—´é¢„æµ‹æ¨¡å‹å’Œæ ‡å‡†ç¬¦åˆé¢„æµ‹æ–¹æ³•æ›´å…·ç´§è‡´æ€§ï¼Œæ˜¾è‘—é™ä½äº†é¢„æµ‹çš„ä¿å®ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint. Under review",
      "pdf_url": "https://arxiv.org/pdf/2508.11025v1",
      "published_date": "2025-08-14 19:03:28 UTC",
      "updated_date": "2025-08-14 19:03:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:46:06.550013+00:00"
    },
    {
      "arxiv_id": "2508.11017v2",
      "title": "Beyond the Rosetta Stone: Unification Forces in Generalization Dynamics",
      "title_zh": "è¶…è¶Šç½—å¡å¡”çŸ³ç¢‘ï¼šæ³›åŒ–åŠ¨åŠ›å­¦ä¸­çš„ç»Ÿä¸€åŠ›é‡",
      "authors": [
        "Carter Blum",
        "Katja Filippova",
        "Ann Yuan",
        "Asma Ghandeharioun",
        "Julian Zimmert",
        "Fred Zhang",
        "Jessica Hoffmann",
        "Tal Linzen",
        "Martin Wattenberg",
        "Lucas Dixon",
        "Mor Geva"
      ],
      "abstract": "Large language models (LLMs) struggle with cross-lingual knowledge transfer: they hallucinate when asked in one language about facts expressed in a different language during training. This work introduces a controlled setting to study the causes and dynamics of this phenomenon by training small Transformer models from scratch on synthetic multilingual datasets. We identify a learning phase wherein a model develops either separate or unified representations of the same facts across languages, and show that unification is essential for cross-lingual transfer. We also show that the degree of unification depends on mutual information between facts and training data language, and on how easy it is to extract that language. Based on these insights, we develop methods to modulate the level of cross-lingual transfer by manipulating data distribution and tokenization, and we introduce metrics and visualizations to formally characterize their effects on unification. Our work shows how controlled settings can shed light on pre-training dynamics and suggests new directions for improving cross-lingual transfer in LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨è·¨è¯­è¨€çŸ¥è¯†è½¬ç§»(cross-lingual knowledge transfer)ä¸­é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå³æ¨¡å‹å¸¸åœ¨å¤„ç†éè®­ç»ƒè¯­è¨€äº‹å®æ—¶äº§ç”Ÿå¹»è§‰ã€‚ä½œè€…é€šè¿‡åœ¨åˆæˆå¤šè¯­è¨€æ•°æ®é›†ä¸Šä»å¤´è®­ç»ƒå°å‹Transformeræ¨¡å‹ï¼Œè¯†åˆ«å‡ºä¸€ä¸ªå†³å®šæ¨¡å‹å½¢æˆç‹¬ç«‹æˆ–ç»Ÿä¸€è¡¨ç¤ºçš„å­¦ä¹ é˜¶æ®µï¼Œå¹¶è¯æ˜äº†è¡¨ç¤ºçš„ç»Ÿä¸€(unification)æ˜¯å®ç°è·¨è¯­è¨€è½¬ç§»çš„æ ¸å¿ƒã€‚ç ”ç©¶æŒ‡å‡ºï¼Œè¿™ç§ç»Ÿä¸€ç¨‹åº¦å—åˆ°äº‹å®ä¸è®­ç»ƒè¯­è¨€é—´çš„äº’ä¿¡æ¯(mutual information)ä»¥åŠè¯­è¨€æå–éš¾æ˜“ç¨‹åº¦çš„å½±å“ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œè®ºæ–‡æå‡ºäº†é€šè¿‡æ“çºµæ•°æ®åˆ†å¸ƒå’Œåˆ†è¯(tokenization)æ¥è°ƒèŠ‚è·¨è¯­è¨€è½¬ç§»çš„æ–¹æ³•ï¼Œå¹¶å¼•å…¥äº†ç›¸å…³åº¦é‡æŒ‡æ ‡ä¸å¯è§†åŒ–å·¥å…·ã€‚è¯¥å·¥ä½œä¸ä»…æ­ç¤ºäº†é¢„è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ³›åŒ–åŠ¨åŠ›å­¦ï¼Œä¹Ÿä¸ºå¢å¼ºå¤§æ¨¡å‹çš„è·¨è¯­è¨€èƒ½åŠ›æä¾›äº†æ–°çš„ç†è®ºæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11017v2",
      "published_date": "2025-08-14 18:44:13 UTC",
      "updated_date": "2025-08-28 15:51:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:46:11.458226+00:00"
    },
    {
      "arxiv_id": "2508.11016v2",
      "title": "CURE: Critical-Token-Guided Re-Concatenation for Entropy-Collapse Prevention",
      "title_zh": "CUREï¼šç”¨äºé¢„é˜²ç†µå¡Œé™·çš„å…³é”® Token å¼•å¯¼å¼é‡æ‹¼æ¥",
      "authors": [
        "Qingbin Li",
        "Rongkun Xue",
        "Jie Wang",
        "Ming Zhou",
        "Zhi Li",
        "Xiaofeng Ji",
        "Yongqi Wang",
        "Miao Liu",
        "Zheming Yang",
        "Minghui Qiu",
        "Jing Yang"
      ],
      "abstract": "Recent advances in Reinforcement Learning with Verified Reward (RLVR) have driven the emergence of more sophisticated cognitive behaviors in large language models (LLMs), thereby enhancing their reasoning capabilities. However, in prior RLVR pipelines, the repeated use of static initial-state sampling drawn exactly from the dataset distribution during each sampling phase produced overly deterministic, low diversity model behavior, which manifested as rapid entropy collapse and hindered sustained performance gains during prolonged training. To address this issue, we introduce CURE (Critical-token-gUided Re concatenation for Entropy-collapse prevention), a two-stage framework that balances exploration and exploitation. Specifically, in the first stage, to deliberately steer the model toward novel yet coherent contexts, we re-generate at high-entropy critical tokens and jointly optimize the original and the branched trajectories. The further comparison with vanilla DAPO shows that the regeneration process achieves a better performance on math reasoning tasks while sustaining a high-level entropy degree for exploration. In the second stage, we continue training with static initial-state sampling by DAPO, intentionally placing the model in a familiar state to gradually strengthen exploitation. Extensive experiments on Qwen-2.5-Math-7B show that, compared to other RLVR methods, CURE achieves a 5% performance gain across six math benchmarks, establishing state-of-the-art performance in both entropy and accuracy. A series of experiments further validate the effectiveness of our approach. Code is available at https://github.com/bytedance/CURE.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼ºåŒ–å­¦ä¹ éªŒè¯å¥–åŠ±(Reinforcement Learning with Verified Reward, RLVR)è¿‡ç¨‹ä¸­ç”±äºé™æ€åˆå§‹çŠ¶æ€é‡‡æ ·å¯¼è‡´çš„ç†µåç¼©(Entropy Collapse)å’Œæ¨¡å‹å¤šæ ·æ€§ä¸è¶³é—®é¢˜ï¼Œæå‡ºäº†åä¸ºCUREçš„åŒé˜¶æ®µæ¡†æ¶ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œä¸ºäº†å¼•å¯¼æ¨¡å‹æ¢ç´¢æ–°é¢–ä¸”è¿è´¯çš„ä¸Šä¸‹æ–‡ï¼ŒCUREåœ¨å…·æœ‰é«˜ç†µçš„å…³é”®æ ‡è®°(Critical Tokens)å¤„è¿›è¡Œé‡æ–°ç”Ÿæˆï¼Œå¹¶è”åˆä¼˜åŒ–åŸå§‹ä¸åˆ†æ”¯è½¨è¿¹ã€‚è¯¥è¿‡ç¨‹ç›¸æ¯”ä¼ ç»Ÿçš„DAPOæ–¹æ³•ï¼Œåœ¨ä¿æŒé«˜æ°´å¹³æ¢ç´¢ç†µçš„åŒæ—¶æ˜¾è‘—æå‡äº†æ•°å­¦æ¨ç†æ€§èƒ½ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæ¡†æ¶é€šè¿‡é™æ€åˆå§‹çŠ¶æ€é‡‡æ ·ç»§ç»­è¿›è¡ŒDAPOè®­ç»ƒï¼Œåœ¨ç†Ÿæ‚‰çŠ¶æ€ä¸‹åŠ å¼ºæ¨¡å‹å¯¹çŸ¥è¯†çš„åˆ©ç”¨(Exploitation)ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºQwen-2.5-Math-7Bçš„CUREåœ¨å…­ä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†5%çš„æ€§èƒ½æå‡ï¼Œåœ¨ç†µä¿æŒå’Œå‡†ç¡®ç‡æ–¹é¢å‡è¾¾åˆ°äº†æœ€å…ˆè¿›æ°´å¹³ã€‚è¯¥ç ”ç©¶æœ‰æ•ˆå¹³è¡¡äº†å¼ºåŒ–å­¦ä¹ ä¸­çš„æ¢ç´¢ä¸åˆ©ç”¨ï¼Œä¸ºè§£å†³é•¿æ—¶è®­ç»ƒä¸­çš„æ€§èƒ½ç“¶é¢ˆæä¾›äº†æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11016v2",
      "published_date": "2025-08-14 18:40:34 UTC",
      "updated_date": "2025-08-24 13:33:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:46:35.954320+00:00"
    },
    {
      "arxiv_id": "2508.11010v1",
      "title": "Deep Learning-Based Automated Segmentation of Uterine Myomas",
      "title_zh": "åŸºäºæ·±åº¦å­¦ä¹ çš„å­å®«è‚Œç˜¤è‡ªåŠ¨åˆ†å‰²",
      "authors": [
        "Tausifa Jan Saleem",
        "Mohammad Yaqub"
      ],
      "abstract": "Uterine fibroids (myomas) are the most common benign tumors of the female reproductive system, particularly among women of childbearing age. With a prevalence exceeding 70%, they pose a significant burden on female reproductive health. Clinical symptoms such as abnormal uterine bleeding, infertility, pelvic pain, and pressure-related discomfort play a crucial role in guiding treatment decisions, which are largely influenced by the size, number, and anatomical location of the fibroids. Magnetic Resonance Imaging (MRI) is a non-invasive and highly accurate imaging modality commonly used by clinicians for the diagnosis of uterine fibroids. Segmenting uterine fibroids requires a precise assessment of both the uterus and fibroids on MRI scans, including measurements of volume, shape, and spatial location. However, this process is labor intensive and time consuming and subjected to variability due to intra- and inter-expert differences at both pre- and post-treatment stages. As a result, there is a critical need for an accurate and automated segmentation method for uterine fibroids. In recent years, deep learning algorithms have shown re-markable improvements in medical image segmentation, outperforming traditional methods. These approaches offer the potential for fully automated segmentation. Several studies have explored the use of deep learning models to achieve automated segmentation of uterine fibroids. However, most of the previous work has been conducted using private datasets, which poses challenges for validation and comparison between studies. In this study, we leverage the publicly available Uterine Myoma MRI Dataset (UMD) to establish a baseline for automated segmentation of uterine fibroids, enabling standardized evaluation and facilitating future research in this domain.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å­å®«è‚Œç˜¤(Uterine Myomas)åœ¨ä¸´åºŠè¯Šæ–­ä¸­ä¾èµ–äººå·¥åˆ†å‰²MRIå½±åƒæ‰€é¢ä¸´çš„è€—æ—¶è´¹åŠ›ä¸”å­˜åœ¨ä¸“å®¶é—´å·®å¼‚ç­‰æŒ‘æˆ˜ï¼Œæ¢è®¨äº†åŸºäºæ·±åº¦å­¦ä¹ (Deep Learning)çš„è‡ªåŠ¨åŒ–åˆ†å‰²æ–¹æ¡ˆã€‚å­å®«è‚Œç˜¤çš„æ²»ç–—å†³ç­–é«˜åº¦ä¾èµ–äºå…¶åœ¨å½±åƒä¸­çš„å¤§å°ã€æ•°é‡åŠè§£å‰–ä½ç½®ï¼Œè€Œæ·±åº¦å­¦ä¹ ç®—æ³•åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­å·²è¡¨ç°å‡ºè¶…è¶Šä¼ ç»Ÿæ–¹æ³•çš„å“è¶Šæ€§èƒ½ã€‚ç”±äºä»¥å¾€ç ”ç©¶å¤šä¾èµ–ç§æœ‰æ•°æ®é›†è€Œç¼ºä¹ç»Ÿä¸€çš„éªŒè¯æ ‡å‡†ï¼Œæœ¬ç ”ç©¶åˆ©ç”¨å…¬å¼€çš„Uterine Myoma MRI Dataset (UMD)å»ºç«‹äº†å­å®«è‚Œç˜¤è‡ªåŠ¨åŒ–åˆ†å‰²çš„åŸºå‡†(Baseline)ã€‚è¯¥å·¥ä½œæ—¨åœ¨é€šè¿‡è‡ªåŠ¨åŒ–çš„æ‰‹æ®µç²¾ç¡®è¯„ä¼°å­å®«åŠè‚Œç˜¤çš„ä½“ç§¯ä¸å½¢çŠ¶ï¼Œä¸ºè¯¥é¢†åŸŸçš„åç»­ç ”ç©¶æä¾›äº†æ ‡å‡†åŒ–çš„è¯„ä¼°æ¡†æ¶ã€‚è¿™ç§æ–¹æ³•ä¸ä»…èƒ½å¤Ÿæå‡ä¸´åºŠè¯Šæ–­çš„æ•ˆç‡ï¼Œè¿˜ä¸ºå­å®«è‚Œç˜¤çš„ç²¾å‡†æ²»ç–—ä¸æœ¯åè¯„ä¼°å¥ å®šäº†æŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11010v1",
      "published_date": "2025-08-14 18:22:14 UTC",
      "updated_date": "2025-08-14 18:22:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:46:24.005852+00:00"
    },
    {
      "arxiv_id": "2508.11009v3",
      "title": "SproutBench: A Benchmark for Safe and Ethical Large Language Models for Youth",
      "title_zh": "SproutBenchï¼šé¢å‘é’å°‘å¹´çš„å¤§è¯­è¨€æ¨¡å‹å®‰å…¨ä¸ä¼¦ç†è¯„ä¼°åŸºå‡†",
      "authors": [
        "Wenpeng Xing",
        "Lanyi Wei",
        "Haixiao Hu",
        "Jingyi Yu",
        "Rongchang Li",
        "Mohan Li",
        "Changting Lin",
        "Meng Han"
      ],
      "abstract": "The rapid proliferation of large language models (LLMs) in applications targeting children and adolescents necessitates a fundamental reassessment of prevailing AI safety frameworks, which are largely tailored to adult users and neglect the distinct developmental vulnerabilities of minors. This paper highlights key deficiencies in existing LLM safety benchmarks, including their inadequate coverage of age-specific cognitive, emotional, and social risks spanning early childhood (ages 0--6), middle childhood (7--12), and adolescence (13--18). To bridge these gaps, we introduce SproutBench, an innovative evaluation suite comprising 1,283 developmentally grounded adversarial prompts designed to probe risks such as emotional dependency, privacy violations, and imitation of hazardous behaviors. Through rigorous empirical evaluation of 47 diverse LLMs, we uncover substantial safety vulnerabilities, corroborated by robust inter-dimensional correlations (e.g., between Safety and Risk Prevention) and a notable inverse relationship between Interactivity and Age Appropriateness. These insights yield practical guidelines for advancing child-centric AI design and deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºå¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å„¿ç«¥å’Œé’å°‘å¹´åº”ç”¨ä¸­è¿…é€Ÿæ™®åŠï¼Œä½†ç°æœ‰ AI å®‰å…¨æ¡†æ¶å› ä¸»è¦é’ˆå¯¹æˆå¹´äººè€Œå¿½è§†äº†æœªæˆå¹´äººåœ¨è®¤çŸ¥å’Œæƒ…æ„Ÿç­‰æ–¹é¢çš„å‘è‚²è„†å¼±æ€§ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº† SproutBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å« 1,283 ä¸ªåŸºäºå‘è‚²ç†è®ºçš„å¯¹æŠ—æ€§æç¤º(adversarial prompts)çš„åˆ›æ–°è¯„ä¼°å¥—ä»¶ï¼Œæ—¨åœ¨æ¢æµ‹æƒ…æ„Ÿä¾èµ–(emotional dependency)ã€éšç§ä¾µçŠ¯(privacy violations)åŠå±é™©è¡Œä¸ºæ¨¡ä»¿(imitation of hazardous behaviors)ç­‰é£é™©ã€‚é€šè¿‡å¯¹ 47 ä¸ªä¸åŒçš„ LLMs è¿›è¡Œå®è¯è¯„ä¼°ï¼Œç ”ç©¶æ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨å®‰å…¨ä¿éšœæ–¹é¢çš„é‡å¤§æ¼æ´ã€‚å®éªŒå‘ç°å®‰å…¨æ€§(Safety)ä¸é£é™©é˜²èŒƒ(Risk Prevention)ä¹‹é—´å­˜åœ¨å¼ºç›¸å…³æ€§ï¼Œä¸”äº’åŠ¨æ€§(Interactivity)ä¸å¹´é¾„é€‚å®œæ€§(Age Appropriateness)ä¹‹é—´å‘ˆç°æ˜¾è‘—çš„è´Ÿç›¸å…³ã€‚è¿™äº›å‘ç°ä¸ºæ¨åŠ¨ä»¥å„¿ç«¥ä¸ºä¸­å¿ƒçš„ AI è®¾è®¡ä¸éƒ¨ç½²æä¾›äº†å®è·µæŒ‡å—ï¼Œå¡«è¡¥äº†ç°æœ‰å®‰å…¨åŸºå‡†åœ¨æœªæˆå¹´äººä¿æŠ¤é¢†åŸŸçš„ç©ºç™½ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted in AAAI 2026 Workshop on AI for Education",
      "pdf_url": "https://arxiv.org/pdf/2508.11009v3",
      "published_date": "2025-08-14 18:21:39 UTC",
      "updated_date": "2025-12-13 15:39:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:46:22.995114+00:00"
    },
    {
      "arxiv_id": "2508.10993v1",
      "title": "Match & Choose: Model Selection Framework for Fine-tuning Text-to-Image Diffusion Models",
      "title_zh": "Match & Chooseï¼šé¢å‘æ–‡æœ¬ç”Ÿæˆå›¾åƒæ‰©æ•£æ¨¡å‹å¾®è°ƒçš„æ¨¡å‹é€‰æ‹©æ¡†æ¶",
      "authors": [
        "Basile Lewandowski",
        "Robert Birke",
        "Lydia Y. Chen"
      ],
      "abstract": "Text-to-image (T2I) models based on diffusion and transformer architectures advance rapidly. They are often pretrained on large corpora, and openly shared on a model platform, such as HuggingFace. Users can then build up AI applications, e.g., generating media contents, by adopting pretrained T2I models and fine-tuning them on the target dataset. While public pretrained T2I models facilitate the democratization of the models, users face a new challenge: which model can be best fine-tuned based on the target data domain? Model selection is well addressed in classification tasks, but little is known in (pretrained) T2I models and their performance indication on the target domain. In this paper, we propose the first model selection framework, M&C, which enables users to efficiently choose a pretrained T2I model from a model platform without exhaustively fine-tuning them all on the target dataset. The core of M&C is a matching graph, which consists of: (i) nodes of available models and profiled datasets, and (ii) edges of model-data and data-data pairs capturing the fine-tuning performance and data similarity, respectively. We then build a model that, based on the inputs of model/data feature, and, critically, the graph embedding feature, extracted from the matching graph, predicts the model achieving the best quality after fine-tuning for the target domain. We evaluate M&C on choosing across ten T2I models for 32 datasets against three baselines. Our results show that M&C successfully predicts the best model for fine-tuning in 61.3% of the cases and a closely performing model for the rest.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†é¦–ä¸ªé’ˆå¯¹æ–‡æœ¬ç”Ÿæˆå›¾åƒ(Text-to-Image, T2I)æ‰©æ•£æ¨¡å‹çš„æ¨¡å‹é€‰æ‹©æ¡†æ¶M&C (Match & Choose)ï¼Œæ—¨åœ¨è§£å†³ç”¨æˆ·åœ¨é¢å¯¹æµ·é‡é¢„è®­ç»ƒæ¨¡å‹æ—¶éš¾ä»¥ç¡®å®šå“ªæ¬¾æ¨¡å‹æœ€é€‚åˆç‰¹å®šç›®æ ‡é¢†åŸŸå¾®è°ƒçš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ˜¯æ„å»ºä¸€ä¸ªåŒ¹é…å›¾(matching graph)ï¼Œé€šè¿‡æ¨¡å‹ä¸æ•°æ®é›†èŠ‚ç‚¹ä»¥åŠè¡¨ç¤ºå¾®è°ƒæ€§èƒ½å’Œæ•°æ®ç›¸ä¼¼æ€§(data similarity)çš„è¾¹ï¼Œæ¥æ•æ‰æ¨¡å‹ä¸æ•°æ®é—´çš„å¤æ‚å…³ç³»ã€‚åˆ©ç”¨ä»å›¾ä¸­æå–çš„å›¾åµŒå…¥ç‰¹å¾(graph embedding feature)åŠç›¸å…³æ•°æ®ç‰¹å¾ï¼ŒM&Cèƒ½å¤Ÿé¢„æµ‹å‡ºåœ¨ç›®æ ‡é¢†åŸŸå¾®è°ƒåè¡¨ç°æœ€ä¼˜çš„æ¨¡å‹ï¼Œä»è€Œé¿å…äº†å¯¹æ‰€æœ‰å€™é€‰æ¨¡å‹è¿›è¡Œè€—æ—¶çš„è¯¦å°½å¾®è°ƒã€‚åœ¨æ¶‰åŠ10ä¸ªT2Iæ¨¡å‹å’Œ32ä¸ªæ•°æ®é›†çš„è¯„ä¼°ä¸­ï¼ŒM&Cåœ¨61.3%çš„æƒ…å†µä¸‹å‡†ç¡®é¢„æµ‹äº†æœ€ä½³æ¨¡å‹ï¼Œå¹¶åœ¨å…¶ä»–æƒ…å†µä¸‹ä¹Ÿé”å®šäº†æ€§èƒ½ç›¸è¿‘çš„å€™é€‰è€…ï¼Œè¯æ˜äº†å…¶åœ¨æ¨¡å‹ç­›é€‰ä¸Šçš„é«˜æ•ˆæ€§ä¸å‡†ç¡®æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10993v1",
      "published_date": "2025-08-14 18:00:50 UTC",
      "updated_date": "2025-08-14 18:00:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:46:29.891507+00:00"
    },
    {
      "arxiv_id": "2508.10991v4",
      "title": "MCP-Guard: A Multi-Stage Defense-in-Depth Framework for Securing Model Context Protocol in Agentic AI",
      "title_zh": "MCP-Guardï¼šé¢å‘æ™ºèƒ½ä½“ AI æ¨¡å‹ä¸Šä¸‹æ–‡åè®®å®‰å…¨çš„å¤šé˜¶æ®µæ·±åº¦é˜²å¾¡æ¡†æ¶",
      "authors": [
        "Wenpeng Xing",
        "Zhonghao Qi",
        "Yupeng Qin",
        "Yilin Li",
        "Caini Chang",
        "Jiahui Yu",
        "Changting Lin",
        "Zhenzhen Xie",
        "Meng Han"
      ],
      "abstract": "While Large Language Models (LLMs) have achieved remarkable performance, they remain vulnerable to jailbreak. The integration of Large Language Models (LLMs) with external tools via protocols such as the Model Context Protocol (MCP) introduces critical security vulnerabilities, including prompt injection, data exfiltration, and other threats. To counter these challenges, we propose MCP-GUARD, a robust, layered defense architecture designed for LLM-tool interactions. MCP-GUARD employs a three-stage detection pipeline that balances efficiency with accuracy: it progresses from lightweight static scanning for overt threats and a deep neural detector for semantic attacks, to our fine-tuned E5-based model which achieves 96.01\\% accuracy in identifying adversarial prompts. Finally, an LLM arbitrator synthesizes these signals to deliver the final decision. To enable rigorous training and evaluation, we introduce MCP-ATTACKBENCH, a comprehensive benchmark comprising 70,448 samples augmented by GPT-4. This benchmark simulates diverse real-world attack vectors that circumvent conventional defenses in the MCP paradigm, thereby laying a solid foundation for future research on securing LLM-tool ecosystems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)é€šè¿‡Model Context Protocol (MCP)ä¸å¤–éƒ¨å·¥å…·é›†æˆæ—¶é¢ä¸´çš„æç¤ºæ³¨å…¥(prompt injection)å’Œæ•°æ®æ³„éœ²(data exfiltration)ç­‰å®‰å…¨å¨èƒï¼Œæå‡ºäº†å¤šé˜¶æ®µæ·±åº¦é˜²å¾¡æ¡†æ¶MCP-GUARDã€‚è¯¥æ¡†æ¶é‡‡ç”¨ç”±è½»é‡çº§é™æ€æ‰«æã€æ·±å±‚ç¥ç»è¯­ä¹‰æ£€æµ‹å™¨ä»¥åŠåŸºäºE5çš„å¾®è°ƒæ¨¡å‹ç»„æˆçš„ä¸‰é˜¶æ®µæ£€æµ‹æµæ°´çº¿ï¼Œå…¶ä¸­E5æ¨¡å‹åœ¨è¯†åˆ«å¯¹æŠ—æ€§æç¤ºæ–¹é¢è¾¾åˆ°äº†96.01%çš„å‡†ç¡®ç‡ã€‚ç³»ç»Ÿæœ€ç»ˆé€šè¿‡LLMä»²è£å™¨(LLM arbitrator)ç»¼åˆå„é˜¶æ®µä¿¡å·åšå‡ºé˜²å¾¡å†³ç­–ï¼Œæœ‰æ•ˆå¹³è¡¡äº†æ£€æµ‹æ•ˆç‡ä¸å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿæ¨å‡ºäº†åŒ…å«70,448ä¸ªæ ·æœ¬çš„ç»¼åˆåŸºå‡†æµ‹è¯•é›†MCP-ATTACKBENCHï¼Œç”¨äºæ¨¡æ‹ŸçœŸå®ä¸–ç•Œä¸­å¤šç§è§„é¿ä¼ ç»Ÿé˜²å¾¡çš„æ”»å‡»å‘é‡ã€‚è¯¥å·¥ä½œçš„å¼€å±•ä¸ºä¿éšœæ™ºèƒ½ä½“AI(Agentic AI)ä¸­LLMä¸å·¥å…·äº¤äº’ç”Ÿæ€ç³»ç»Ÿçš„å®‰å…¨æ€§æä¾›äº†é‡è¦æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10991v4",
      "published_date": "2025-08-14 18:00:25 UTC",
      "updated_date": "2026-01-08 08:42:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:46:55.585088+00:00"
    },
    {
      "arxiv_id": "2508.13187v1",
      "title": "Combating Homelessness Stigma with LLMs: A New Multi-Modal Dataset for Bias Detection",
      "title_zh": "åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹åº”å¯¹æ— å®¶å¯å½’è€…æ±¡ååŒ–ï¼šç”¨äºåè§æ£€æµ‹çš„æ–°å‹å¤šæ¨¡æ€æ•°æ®é›†",
      "authors": [
        "Jonathan A. Karr",
        "Benjamin F. Herbst",
        "Ting Hua",
        "Matthew Hauenstein",
        "Georgina Curto",
        "Nitesh V. Chawla"
      ],
      "abstract": "Homelessness is a persistent social challenge, impacting millions worldwide. Over 770,000 people experienced homelessness in the U.S. in 2024. Social stigmatization is a significant barrier to alleviation, shifting public perception, and influencing policymaking. Given that online and city council discourse reflect and influence part of public opinion, it provides valuable insights to identify and track social biases. This research contributes to alleviating homelessness by acting on public opinion. It introduces novel methods, building on natural language processing (NLP) and large language models (LLMs), to identify and measure PEH social bias expressed in digital spaces. We present a new, manually-annotated multi-modal dataset compiled from Reddit, X (formerly Twitter), news articles, and city council meeting minutes across 10 U.S. cities. This unique dataset provides evidence of the typologies of homelessness bias described in the literature. In order to scale up and automate the detection of homelessness bias online, we evaluate LLMs as classifiers. We applied both zero-shot and few-shot classification techniques to this data. We utilized local LLMs (Llama 3.2 3B Instruct, Qwen 2.5 7B Instruct, and Phi4 Instruct Mini) as well as closed-source API models (GPT-4.1, Gemini 2.5 Pro, and Grok-4). Our findings reveal that although there are significant inconsistencies in local LLM zero-shot classification, the in-context learning classification scores of local LLMs approach the classification scores of closed-source LLMs. Furthermore, LLMs outperform BERT when averaging across all categories. This work aims to raise awareness about the pervasive bias against PEH, develop new indicators to inform policy, and ultimately enhance the fairness and ethical application of Generative AI technologies.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ— å®¶å¯å½’è€…çš„ç¤¾ä¼šæ±¡ååŒ–é—®é¢˜ï¼Œå¼•å…¥äº†ä¸€ä¸ªå…¨æ–°çš„ã€ç»è¿‡äººå·¥æ ‡æ³¨çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œæ—¨åœ¨åˆ©ç”¨è‡ªç„¶è¯­è¨€å¤„ç† (NLP) å’Œå¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) è¯†åˆ«å¹¶æµ‹é‡æ•°å­—ç©ºé—´ä¸­çš„ç¤¾ä¼šåè§ã€‚è¯¥æ•°æ®é›†æ•´åˆäº†æ¥è‡ª Redditã€Xã€æ–°é—»æ–‡ç« ä»¥åŠç¾å›½10ä¸ªåŸå¸‚è®®ä¼šä¼šè®®çºªè¦çš„å¤šæ ·åŒ–æ•°æ®ï¼Œå®è¯äº†æ–‡çŒ®ä¸­å…³äºæ— å®¶å¯å½’åè§çš„åˆ†ç±»å­¦ã€‚ç ”ç©¶å›¢é˜Ÿè¯„ä¼°äº†å¤šç§æœ¬åœ° LLMsï¼ˆå¦‚ Llama 3.2, Qwen 2.5 å’Œ Phi4ï¼‰ä»¥åŠé—­æº API æ¨¡å‹ï¼ˆå¦‚ GPT-4.1, Gemini 2.5 Pro å’Œ Grok-4ï¼‰åœ¨ zero-shot å’Œ few-shot ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡æœ¬åœ°æ¨¡å‹åœ¨ zero-shot æ¨¡å¼ä¸‹å­˜åœ¨ä¸ä¸€è‡´æ€§ï¼Œä½†é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹  (in-context learning)ï¼Œå…¶åˆ†ç±»è¯„åˆ†å·²æ¥è¿‘é—­æºæ¨¡å‹ï¼Œä¸” LLMs åœ¨æ‰€æœ‰ç±»åˆ«ä¸Šçš„å¹³å‡è¡¨ç°å‡ä¼˜äº BERTã€‚è¯¥å·¥ä½œä¸ä»…ä¸ºæ”¿ç­–åˆ¶å®šæä¾›äº†æ–°çš„é‡åŒ–æŒ‡æ ‡ï¼Œè¿˜è‡´åŠ›äºæå‡ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (Generative AI) åœ¨å¤„ç†æ•æ„Ÿç¤¾ä¼šè®®é¢˜æ—¶çš„å…¬å¹³æ€§ä¸ä¼¦ç†è¡¨ç°ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.13187v1",
      "published_date": "2025-08-14 17:58:34 UTC",
      "updated_date": "2025-08-14 17:58:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:47:01.193590+00:00"
    },
    {
      "arxiv_id": "2508.10976v1",
      "title": "Grounding Rule-Based Argumentation Using Datalog",
      "title_zh": "åŸºäº Datalog çš„è§„åˆ™è®ºè¯å®ä¾‹åŒ–",
      "authors": [
        "Martin Diller",
        "Sarah Alice Gaggl",
        "Philipp Hanisch",
        "Giuseppina Monterosso",
        "Fritz Rauschenbach"
      ],
      "abstract": "ASPIC+ is one of the main general frameworks for rule-based argumentation for AI. Although first-order rules are commonly used in ASPIC+ examples, most existing approaches to reason over rule-based argumentation only support propositional rules. To enable reasoning over first-order instances, a preliminary grounding step is required. As groundings can lead to an exponential increase in the size of the input theories, intelligent procedures are needed. However, there is a lack of dedicated solutions for ASPIC+. Therefore, we propose an intelligent grounding procedure that keeps the size of the grounding manageable while preserving the correctness of the reasoning process. To this end, we translate the first-order ASPIC+ instance into a Datalog program and query a Datalog engine to obtain ground substitutions to perform the grounding of rules and contraries. Additionally, we propose simplifications specific to the ASPIC+ formalism to avoid grounding of rules that have no influence on the reasoning process. Finally, we performed an empirical evaluation of a prototypical implementation to show scalability.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ rule-based argumentation çš„æ ¸å¿ƒæ¡†æ¶ ASPIC+ï¼Œè§£å†³äº†å…¶åœ¨å¤„ç† first-order rules æ—¶å› æ¨ç†è¿‡ç¨‹éœ€è¿›è¡Œåˆæ­¥ grounding è€Œå¯¼è‡´çš„ç†è®ºè§„æ¨¡æŒ‡æ•°çº§å¢é•¿é—®é¢˜ã€‚ä½œè€…æå‡ºäº†ä¸€ç§æ™ºèƒ½åŒ–çš„ grounding è¿‡ç¨‹ï¼Œæ—¨åœ¨å‡å°å®ä¾‹åŒ–è§„æ¨¡çš„åŒæ—¶ç¡®ä¿æ¨ç†çš„ correctnessã€‚è¯¥æ–¹æ³•é€šè¿‡å°† first-order ASPIC+ å®ä¾‹è½¬æ¢ä¸º Datalog ç¨‹åºï¼Œå¹¶åˆ©ç”¨ Datalog engine è·å– ground substitutions æ¥å®Œæˆè§„åˆ™ä¸ contraries çš„å®ä¾‹åŒ–ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†ä¸“å±äº ASPIC+ å½¢å¼åŒ–ç‰¹å¾çš„ç®€åŒ–æŠ€æœ¯ï¼Œæœ‰æ•ˆé¿å…äº†å¯¹æ¨ç†ç»“æœæ— å½±å“çš„è§„åˆ™è¿›è¡Œ groundingã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥åŸå‹ç³»ç»Ÿåœ¨å¤„ç†å¤§è§„æ¨¡å®ä¾‹æ—¶å±•ç°å‡ºä¼˜å¼‚çš„ scalabilityï¼Œä¸ºåŸºäºä¸€é˜¶è§„åˆ™çš„è®ºè¯æ¨ç†æä¾›äº†é«˜æ•ˆå¯è¡Œçš„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10976v1",
      "published_date": "2025-08-14 17:57:32 UTC",
      "updated_date": "2025-08-14 17:57:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:47:06.790787+00:00"
    },
    {
      "arxiv_id": "2508.15808v1",
      "title": "Uplifted Attackers, Human Defenders: The Cyber Offense-Defense Balance for Trailing-Edge Organizations",
      "title_zh": "å®åŠ›æå‡çš„æ”»å‡»è€…ä¸äººç±»é˜²å¾¡è€…ï¼šè½åå‹ç»„ç»‡çš„ç½‘ç»œæ”»é˜²å¹³è¡¡",
      "authors": [
        "Benjamin Murphy",
        "Twm Stone"
      ],
      "abstract": "Advances in AI are widely understood to have implications for cybersecurity. Articles have emphasized the effect of AI on the cyber offense-defense balance, and commentators can be found arguing either that cyber will privilege attackers or defenders. For defenders, arguments are often made that AI will enable solutions like formal verification of all software--and for some well-equipped companies, this may be true. This conversation, however, does not match the reality for most companies. \"Trailing-edge organizations,\" as we term them, rely heavily on legacy software, poorly staff security roles, and struggle to implement best practices like rapid deployment of security patches. These decisions may be the result of corporate inertia, but may also be the result of a seemingly-rational calculation that attackers may not bother targeting a firm due to lack of economic incentives, and as a result, underinvestment in defense will not be punished.\n  This approach to security may have been sufficient prior to the development of AI systems, but it is unlikely to remain viable in the near future. We argue that continuing improvements in AI's capabilities poses additional risks on two fronts: First, increased usage of AI will alter the economics of the marginal cyberattack and expose these trailing-edge organizations to more attackers, more frequently. Second, AI's advances will enable attackers to develop exploits and launch attacks earlier than they can today--meaning that it is insufficient for these companies to attain parity with today's leading defenders, but must instead aim for faster remediation timelines and more resilient software. The situation today portends a dramatically increased number of attacks in the near future. Moving forward, we offer a range of solutions for both organizations and governments to improve the defensive posture of firms which lag behind their peers today.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººå·¥æ™ºèƒ½(AI)å‘å±•å¯¹ç½‘ç»œå®‰å…¨æ”»é˜²å¹³è¡¡çš„å½±å“ï¼Œç‰¹åˆ«å…³æ³¨äº†è¢«ç§°ä¸ºâ€œåè¿›ç»„ç»‡â€(Trailing-Edge Organizations)çš„ç¾¤ä½“ã€‚è¿™äº›ç»„ç»‡é€šå¸¸è¿‡åº¦ä¾èµ–é—ç•™è½¯ä»¶(Legacy Software)ï¼Œå®‰å…¨äººå‘˜é…ç½®ä¸è¶³ï¼Œä¸”éš¾ä»¥è½å®å¿«é€Ÿéƒ¨ç½²å®‰å…¨è¡¥ä¸ç­‰æœ€ä½³å®è·µã€‚ç ”ç©¶æŒ‡å‡ºï¼ŒAI çš„è¿›æ­¥å°†æ”¹å˜è¾¹é™…ç½‘ç»œæ”»å‡»çš„ç»æµæ¨¡å‹ï¼Œä½¿è¿™äº›åè¿›ç»„ç»‡é¢ä¸´æ›´é«˜é¢‘æ¬¡ã€æ¥è‡ªæ›´å¤šæ”»å‡»è€…çš„å¨èƒã€‚æ­¤å¤–ï¼ŒAI æŠ€æœ¯çš„æå‡ä½¿æ”»å‡»è€…èƒ½å¤Ÿæ¯”ä»¥å¾€æ›´æ—©åœ°å¼€å‘å‡ºæ¼æ´åˆ©ç”¨ç¨‹åºï¼Œè¿™æ„å‘³ç€ç›¸å…³ç»„ç»‡å¿…é¡»è¿½æ±‚æ›´å¿«çš„ä¿®å¤æ—¶é—´è¡¨å’Œæ›´å…·å¼¹æ€§çš„è½¯ä»¶ç³»ç»Ÿã€‚æ–‡ç« é¢„è­¦æœªæ¥ç½‘ç»œæ”»å‡»æ•°é‡å°†å‰§å¢ï¼Œå¹¶é’ˆå¯¹ç»„ç»‡å’Œæ”¿åºœæå‡ºäº†ä¸€ç³»åˆ—æ”¹è¿›è½åä¼ä¸šé˜²å¾¡æ€åŠ¿çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15808v1",
      "published_date": "2025-08-14 17:56:57 UTC",
      "updated_date": "2025-08-14 17:56:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:47:10.890107+00:00"
    },
    {
      "arxiv_id": "2508.10887v1",
      "title": "Empirical Investigation into Configuring Echo State Networks for Representative Benchmark Problem Domains",
      "title_zh": "é’ˆå¯¹ä»£è¡¨æ€§åŸºå‡†é—®é¢˜é¢†åŸŸçš„å›å£°çŠ¶æ€ç½‘ç»œé…ç½®å®è¯ç ”ç©¶",
      "authors": [
        "Brooke R. Weborg",
        "Gursel Serpen"
      ],
      "abstract": "This paper examines Echo State Network, a reservoir computer, performance using four different benchmark problems, then proposes heuristics or rules of thumb for configuring the architecture, as well as the selection of parameters and their values, which are applicable to problems within the same domain, to help serve to fill the experience gap needed by those entering this field of study. The influence of various parameter selections and their value adjustments, as well as architectural changes made to an Echo State Network, a powerful recurrent neural network configured as a reservoir computer, can be challenging to fully comprehend without experience in the field, and even some hyperparameter optimization algorithms may have difficulty adjusting parameter values without proper manual selections made first. Therefore, it is imperative to understand the effects of parameters and their value selection on Echo State Network architecture performance for a successful build. Thus, to address the requirement for an extensive background in Echo State Network architecture, as well as examine how Echo State Network performance is affected with respect to variations in architecture, design, and parameter selection and values, a series of benchmark tasks representing different problem domains, including time series prediction, pattern generation, chaotic system prediction, and time series classification, were modeled and experimented on to show the impact on the performance of Echo State Network.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å‚¨å±‚è®¡ç®—æ¨¡å‹ Echo State Network (ESN) åœ¨æ¶æ„é…ç½®å’Œå‚æ•°é€‰æ‹©æ–¹é¢çš„å¤æ‚æ€§ï¼Œå¼€å±•äº†æ·±å…¥çš„å®è¯è°ƒæŸ¥ã€‚é€šè¿‡åœ¨ Time Series Predictionã€Pattern Generationã€Chaotic System Prediction ä»¥åŠ Time Series Classification å››ä¸ªä»£è¡¨æ€§é¢†åŸŸçš„åŸºå‡†é—®é¢˜ä¸Šè¿›è¡Œå®éªŒï¼Œæœ¬æ–‡ç³»ç»Ÿåˆ†æäº†æ¶æ„è®¾è®¡ã€å‚æ•°é€‰å–åŠå…¶æ•°å€¼è°ƒæ•´å¯¹æ¨¡å‹æ€§èƒ½çš„å…·ä½“å½±å“ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç³»åˆ—é€‚ç”¨äºåŒç±»é—®é¢˜çš„å¯å‘å¼é…ç½®è§„åˆ™ (Heuristics) å’Œç»éªŒæ³•åˆ™ï¼Œæ—¨åœ¨ä¸ºé¢†åŸŸåˆå­¦è€…æä¾›å¿…è¦çš„æŒ‡å¯¼ï¼Œå¹¶å¼¥è¡¥å…¶åœ¨æ‰‹åŠ¨é¢„è®¾å‚æ•°å’Œä¼˜åŒ–ç®—æ³•åº”ç”¨ä¸­çš„ç»éªŒç¼ºå£ã€‚å®éªŒç»“æœæ­ç¤ºäº†ä¸åŒå‚æ•°å¯¹ ESN æ€§èƒ½çš„ä½œç”¨æœºåˆ¶ï¼Œä¸ºæˆåŠŸæ„å»ºé«˜æ•ˆçš„ Echo State Network æ¶æ„æä¾›äº†å®ç”¨çš„å‚è€ƒæ¡†æ¶ã€‚",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "49 pages, 21 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.10887v1",
      "published_date": "2025-08-14 17:55:47 UTC",
      "updated_date": "2025-08-14 17:55:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:47:19.599378+00:00"
    },
    {
      "arxiv_id": "2508.10881v1",
      "title": "ToonComposer: Streamlining Cartoon Production with Generative Post-Keyframing",
      "title_zh": "ToonComposerï¼šé€šè¿‡ç”Ÿæˆå¼åå…³é”®å¸§æŠ€æœ¯ç®€åŒ–å¡é€šåˆ¶ä½œæµç¨‹",
      "authors": [
        "Lingen Li",
        "Guangzhi Wang",
        "Zhaoyang Zhang",
        "Yaowei Li",
        "Xiaoyu Li",
        "Qi Dou",
        "Jinwei Gu",
        "Tianfan Xue",
        "Ying Shan"
      ],
      "abstract": "Traditional cartoon and anime production involves keyframing, inbetweening, and colorization stages, which require intensive manual effort. Despite recent advances in AI, existing methods often handle these stages separately, leading to error accumulation and artifacts. For instance, inbetweening approaches struggle with large motions, while colorization methods require dense per-frame sketches. To address this, we introduce ToonComposer, a generative model that unifies inbetweening and colorization into a single post-keyframing stage. ToonComposer employs a sparse sketch injection mechanism to provide precise control using keyframe sketches. Additionally, it uses a cartoon adaptation method with the spatial low-rank adapter to tailor a modern video foundation model to the cartoon domain while keeping its temporal prior intact. Requiring as few as a single sketch and a colored reference frame, ToonComposer excels with sparse inputs, while also supporting multiple sketches at any temporal location for more precise motion control. This dual capability reduces manual workload and improves flexibility, empowering artists in real-world scenarios. To evaluate our model, we further created PKBench, a benchmark featuring human-drawn sketches that simulate real-world use cases. Our evaluation demonstrates that ToonComposer outperforms existing methods in visual quality, motion consistency, and production efficiency, offering a superior and more flexible solution for AI-assisted cartoon production.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»ŸåŠ¨ç”»åˆ¶ä½œä¸­inbetweeningå’Œcolorizationé˜¶æ®µåˆ†ç¦»å¯¼è‡´çš„è¯¯å·®ç§¯ç´¯é—®é¢˜ï¼Œæå‡ºäº†ç»Ÿä¸€çš„ç”Ÿæˆå¼æ¨¡å‹ToonComposerã€‚è¯¥æ¨¡å‹é€šè¿‡å°†è¿™ä¸¤ä¸ªç¯èŠ‚æ•´åˆä¸ºå•ä¸€çš„post-keyframingé˜¶æ®µï¼Œå¹¶å¼•å…¥sparse sketch injectionæœºåˆ¶ï¼Œåˆ©ç”¨å…³é”®å¸§çº¿ç¨¿å®ç°ç²¾ç¡®æ§åˆ¶ã€‚ä¸ºäº†å°†video foundation modelé€‚é…è‡³åŠ¨ç”»é¢†åŸŸï¼ŒToonComposeré‡‡ç”¨äº†å¸¦æœ‰spatial low-rank adapterçš„åŠ¨ç”»é€‚é…æ–¹æ³•ï¼Œåœ¨ä¿ç•™æ—¶é—´å…ˆéªŒçš„åŒæ—¶æå‡äº†æ¨¡å‹è¡¨ç°ã€‚è¯¥ç³»ç»Ÿä¸ä»…æ”¯æŒåŸºäºæç¨€ç–è¾“å…¥çš„ç”Ÿæˆï¼Œè¿˜å…è®¸ç”¨æˆ·åœ¨ä»»æ„æ—¶é—´ç‚¹æ·»åŠ å¤šå¼ çº¿ç¨¿ä»¥å¢å¼ºè¿åŠ¨æ§åˆ¶çš„çµæ´»æ€§ã€‚ç ”ç©¶å›¢é˜Ÿè¿›ä¸€æ­¥æ¨å‡ºäº†åŸºäºçœŸå®æ‰‹ç»˜åœºæ™¯çš„è¯„ä¼°åŸºå‡†PKBenchï¼Œå®éªŒè¯æ˜ToonComposeråœ¨è§†è§‰è´¨é‡ã€motion consistencyå’Œç”Ÿäº§æ•ˆç‡ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œä¸ºAIè¾…åŠ©åŠ¨ç”»åˆ¶ä½œæä¾›äº†æ›´é«˜æ•ˆä¸”çµæ´»çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project Page: https://lg-li.github.io/project/tooncomposer",
      "pdf_url": "https://arxiv.org/pdf/2508.10881v1",
      "published_date": "2025-08-14 17:50:11 UTC",
      "updated_date": "2025-08-14 17:50:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:47:20.491764+00:00"
    },
    {
      "arxiv_id": "2508.10880v2",
      "title": "Searching for Privacy Risks in LLM Agents via Simulation",
      "title_zh": "é€šè¿‡æ¨¡æ‹Ÿæ¢ç´¢å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“ä¸­çš„éšç§é£é™©",
      "authors": [
        "Yanzhe Zhang",
        "Diyi Yang"
      ],
      "abstract": "The widespread deployment of LLM-based agents is likely to introduce a critical privacy threat: malicious agents that proactively engage others in multi-turn interactions to extract sensitive information. However, the evolving nature of such dynamic dialogues makes it challenging to anticipate emerging vulnerabilities and design effective defenses. To tackle this problem, we present a search-based framework that alternates between improving attack and defense strategies through the simulation of privacy-critical agent interactions. Specifically, we employ LLMs as optimizers to analyze simulation trajectories and iteratively propose new agent instructions. To explore the strategy space more efficiently, we further utilize parallel search with multiple threads and cross-thread propagation. Through this process, we find that attack strategies escalate from direct requests to sophisticated tactics, such as impersonation and consent forgery, while defenses evolve from simple rule-based constraints to robust identity-verification state machines. The discovered attacks and defenses transfer across diverse scenarios and backbone models, demonstrating strong practical utility for building privacy-aware agents.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†LLM-based agentsåœ¨å¤šè½®äº¤äº’ä¸­å¯èƒ½å¸¦æ¥çš„éšç§å¨èƒï¼Œç‰¹åˆ«æ˜¯æ¶æ„æ™ºèƒ½ä½“ä¸»åŠ¨æå–æ•æ„Ÿä¿¡æ¯çš„é£é™©ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªåŸºäºæœç´¢çš„æ¡†æ¶ï¼Œé€šè¿‡æ¨¡æ‹Ÿéšç§å…³é”®çš„æ™ºèƒ½ä½“äº¤äº’ï¼Œåœ¨æ”»å‡»å’Œé˜²å¾¡ç­–ç•¥ä¹‹é—´äº¤æ›¿ä¼˜åŒ–ã€‚è¯¥æ¡†æ¶åˆ©ç”¨LLMsä½œä¸ºä¼˜åŒ–å™¨åˆ†ææ¨¡æ‹Ÿè½¨è¿¹ï¼Œå¹¶é‡‡ç”¨å¤šçº¿ç¨‹å¹¶è¡Œæœç´¢(parallel search)ä¸è·¨çº¿ç¨‹ä¼ æ’­æŠ€æœ¯æ¥é«˜æ•ˆæ¢ç´¢ç­–ç•¥ç©ºé—´ã€‚ç ”ç©¶å‘ç°ï¼Œæ”»å‡»ç­–ç•¥ä¼šä»ç›´æ¥è¯·æ±‚æ¼”åŒ–ä¸ºèº«ä»½å†’å……(impersonation)å’ŒåŒæ„ä¼ªé€ (consent forgery)ç­‰å¤æ‚æ‰‹æ®µï¼Œè€Œé˜²å¾¡ç­–ç•¥åˆ™ä»ç®€å•çš„è§„åˆ™é™åˆ¶è¿›åŒ–ä¸ºç¨³å¥çš„èº«ä»½éªŒè¯çŠ¶æ€æœº(identity-verification state machines)ã€‚å®éªŒè¯æ˜ï¼Œè¿™äº›å‘ç°çš„æ”»å‡»å’Œé˜²å¾¡ç­–ç•¥åœ¨ä¸åŒåœºæ™¯å’ŒåŸºåº§æ¨¡å‹é—´å…·æœ‰è‰¯å¥½çš„è¿ç§»æ€§ï¼Œä¸ºæ„å»ºå…·å¤‡éšç§æ„è¯†çš„æ™ºèƒ½ä½“æä¾›äº†é‡è¦çš„å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "Preprint",
      "pdf_url": "https://arxiv.org/pdf/2508.10880v2",
      "published_date": "2025-08-14 17:49:09 UTC",
      "updated_date": "2025-09-25 04:24:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:47:25.008276+00:00"
    },
    {
      "arxiv_id": "2508.10875v2",
      "title": "A Survey on Diffusion Language Models",
      "title_zh": "æ‰©æ•£è¯­è¨€æ¨¡å‹ç»¼è¿°",
      "authors": [
        "Tianyi Li",
        "Mingda Chen",
        "Bowei Guo",
        "Zhiqiang Shen"
      ],
      "abstract": "Diffusion Language Models (DLMs) are rapidly emerging as a powerful and promising alternative to the dominant autoregressive (AR) paradigm. By generating tokens in parallel through an iterative denoising process, DLMs possess inherent advantages in reducing inference latency and capturing bidirectional context, thereby enabling fine-grained control over the generation process. While achieving a several-fold speed-up, recent advancements have allowed DLMs to show performance comparable to their autoregressive counterparts, making them a compelling choice for various natural language processing tasks. In this survey, we provide a holistic overview of the current DLM landscape. We trace its evolution and relationship with other paradigms, such as autoregressive and masked language models, and cover both foundational principles and state-of-the-art models. Our work offers an up-to-date, comprehensive taxonomy and an in-depth analysis of current techniques, from pre-training strategies to advanced post-training methods. Another contribution of this survey is a thorough review of DLM inference strategies and optimizations, including improvements in decoding parallelism, caching mechanisms, and generation quality. We also highlight the latest approaches to multimodal extensions of DLMs and delineate their applications across various practical scenarios. Furthermore, our discussion addresses the limitations and challenges of DLMs, including efficiency, long-sequence handling, and infrastructure requirements, while outlining future research directions to sustain progress in this rapidly evolving field. Project GitHub is available at https://github.com/VILA-Lab/Awesome-DLMs.",
      "tldr_zh": "è¯¥ç»¼è¿°å…¨é¢å›é¡¾äº†æ‰©æ•£è¯­è¨€æ¨¡å‹(Diffusion Language Models, DLMs)ä½œä¸ºè‡ªå›å½’(autoregressive, AR)èŒƒå¼æœ‰åŠ›æ›¿ä»£æ–¹æ¡ˆçš„å…´èµ·ä¸ç°çŠ¶ã€‚DLMsé€šè¿‡è¿­ä»£å»å™ª(iterative denoising)è¿‡ç¨‹å®ç°Tokençš„å¹¶è¡Œç”Ÿæˆï¼Œåœ¨é™ä½æ¨ç†å»¶è¿Ÿã€æ•æ‰åŒå‘ä¸Šä¸‹æ–‡(bidirectional context)ä»¥åŠå®ç°ç»†ç²’åº¦æ§åˆ¶ç”Ÿæˆæ–¹é¢å±•ç°å‡ºå›ºæœ‰ä¼˜åŠ¿ã€‚æ–‡ç« ç³»ç»Ÿåœ°æ¢³ç†äº†DLMsçš„æ¼”è¿›å†ç¨‹ï¼Œå¹¶é’ˆå¯¹é¢„è®­ç»ƒ(pre-training)ç­–ç•¥ã€é«˜çº§åè®­ç»ƒ(post-training)æ–¹æ³•åŠæœ€å…ˆè¿›çš„æ¨¡å‹æ¶æ„å»ºç«‹äº†è¯¦å°½çš„åˆ†ç±»ä½“ç³»ã€‚æ­¤å¤–ï¼Œç ”ç©¶æ·±å…¥æ¢è®¨äº†æ¨ç†é˜¶æ®µçš„è§£ç å¹¶è¡ŒåŒ–ã€ç¼“å­˜æœºåˆ¶ç­‰ä¼˜åŒ–ç­–ç•¥ï¼Œå¹¶ä»‹ç»äº†å¤šæ¨¡æ€æ‰©å±•åŠå…¶åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨ã€‚æœ€åï¼Œè®ºæ–‡åˆ†æäº†DLMsåœ¨é•¿åºåˆ—å¤„ç†æ•ˆç‡å’ŒåŸºç¡€è®¾æ–½éœ€æ±‚ç­‰æ–¹é¢çš„æŒ‘æˆ˜ï¼Œå¹¶ä¸ºè¯¥é¢†åŸŸçš„æœªæ¥ç ”ç©¶æ–¹å‘æä¾›äº†æŒ‡å¼•ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10875v2",
      "published_date": "2025-08-14 17:47:22 UTC",
      "updated_date": "2025-12-04 17:57:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:47:27.691335+00:00"
    },
    {
      "arxiv_id": "2508.10872v1",
      "title": "TLE-Based A2C Agent for Terrestrial Coverage Orbital Path Planning",
      "title_zh": "é¢å‘åœ°é¢è¦†ç›–è½¨é“è·¯å¾„è§„åˆ’çš„åŸºäº TLE çš„ A2C æ™ºèƒ½ä½“",
      "authors": [
        "Anantha Narayanan",
        "Battu Bhanu Teja",
        "Pruthwik Mishra"
      ],
      "abstract": "The increasing congestion of Low Earth Orbit (LEO) poses persistent challenges to the efficient deployment and safe operation of Earth observation satellites. Mission planners must now account not only for mission-specific requirements but also for the increasing collision risk with active satellites and space debris. This work presents a reinforcement learning framework using the Advantage Actor-Critic (A2C) algorithm to optimize satellite orbital parameters for precise terrestrial coverage within predefined surface radii. By formulating the problem as a Markov Decision Process (MDP) within a custom OpenAI Gymnasium environment, our method simulates orbital dynamics using classical Keplerian elements. The agent progressively learns to adjust five of the orbital parameters - semi-major axis, eccentricity, inclination, right ascension of ascending node, and the argument of perigee-to achieve targeted terrestrial coverage. Comparative evaluation against Proximal Policy Optimization (PPO) demonstrates A2C's superior performance, achieving 5.8x higher cumulative rewards (10.0 vs 9.263025) while converging in 31.5x fewer timesteps (2,000 vs 63,000). The A2C agent consistently meets mission objectives across diverse target coordinates while maintaining computational efficiency suitable for real-time mission planning applications. Key contributions include: (1) a TLE-based orbital simulation environment incorporating physics constraints, (2) validation of actor-critic methods' superiority over trust region approaches in continuous orbital control, and (3) demonstration of rapid convergence enabling adaptive satellite deployment. This approach establishes reinforcement learning as a computationally efficient alternative for scalable and intelligent LEO mission planning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäº Advantage Actor-Critic (A2C) ç®—æ³•çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–ä½åœ°çƒè½¨é“ (LEO) å«æ˜Ÿçš„è½¨é“å‚æ•°ä»¥å®ç°ç²¾ç¡®çš„åœ°é¢è¦†ç›–ã€‚ç ”ç©¶è€…åœ¨è‡ªå®šä¹‰çš„ OpenAI Gymnasium ç¯å¢ƒä¸­å°†è¯¥é—®é¢˜å»ºæ¨¡ä¸º Markov Decision Process (MDP)ï¼Œå¹¶åˆ©ç”¨ Keplerian elements æ¨¡æ‹Ÿè½¨é“åŠ¨åŠ›å­¦ã€‚è¯¥ A2C agent é€šè¿‡å­¦ä¹ è°ƒæ•´ semi-major axisã€eccentricityã€inclinationã€right ascension of ascending node å’Œ argument of perigee äº”ä¸ªå…³é”®å‚æ•°ï¼Œå®ç°äº†é’ˆå¯¹ä¸åŒç›®æ ‡åæ ‡çš„é«˜æ•ˆè·¯å¾„è§„åˆ’ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒA2C çš„æ”¶æ•›é€Ÿåº¦æ¯” Proximal Policy Optimization (PPO) å¿« 31.5 å€ï¼Œä¸”ç´¯è®¡å¥–åŠ±æå‡äº† 5.8 å€ã€‚è¿™é¡¹å·¥ä½œé€šè¿‡é›†æˆ TLE-based è½¨é“æ¨¡æ‹Ÿå’Œç‰©ç†çº¦æŸï¼ŒéªŒè¯äº† actor-critic æ–¹æ³•åœ¨è¿ç»­è½¨é“æ§åˆ¶ä»»åŠ¡ä¸­ä¼˜äº trust region æ–¹æ³•ï¼Œä¸ºå®ç°æ™ºèƒ½åŒ–ã€å¯æ‰©å±•çš„å«æ˜Ÿä»»åŠ¡è§„åˆ’æä¾›äº†ä¸€ç§é«˜æ•ˆçš„è®¡ç®—æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "8 pages, 6 figures, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2508.10872v1",
      "published_date": "2025-08-14 17:44:51 UTC",
      "updated_date": "2025-08-14 17:44:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:47:33.467973+00:00"
    },
    {
      "arxiv_id": "2508.10869v1",
      "title": "Medico 2025: Visual Question Answering for Gastrointestinal Imaging",
      "title_zh": "Medico 2025ï¼šé¢å‘èƒƒè‚ é“å½±åƒçš„è§†è§‰é—®ç­”",
      "authors": [
        "Sushant Gautam",
        "Vajira Thambawita",
        "Michael Riegler",
        "PÃ¥l Halvorsen",
        "Steven Hicks"
      ],
      "abstract": "The Medico 2025 challenge addresses Visual Question Answering (VQA) for Gastrointestinal (GI) imaging, organized as part of the MediaEval task series. The challenge focuses on developing Explainable Artificial Intelligence (XAI) models that answer clinically relevant questions based on GI endoscopy images while providing interpretable justifications aligned with medical reasoning. It introduces two subtasks: (1) answering diverse types of visual questions using the Kvasir-VQA-x1 dataset, and (2) generating multimodal explanations to support clinical decision-making. The Kvasir-VQA-x1 dataset, created from 6,500 images and 159,549 complex question-answer (QA) pairs, serves as the benchmark for the challenge. By combining quantitative performance metrics and expert-reviewed explainability assessments, this task aims to advance trustworthy Artificial Intelligence (AI) in medical image analysis. Instructions, data access, and an updated guide for participation are available in the official competition repository: https://github.com/simula/MediaEval-Medico-2025",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† Medico 2025 æŒ‘æˆ˜èµ›ï¼Œä½œä¸º MediaEval ç³»åˆ—ä»»åŠ¡çš„ä¸€éƒ¨åˆ†ï¼Œå…¶ä¸“æ³¨äºèƒƒè‚ é“ (Gastrointestinal, GI) å½±åƒçš„è§†è§‰é—®ç­” (Visual Question Answering, VQA) ä»»åŠ¡ã€‚è¯¥æŒ‘æˆ˜èµ›çš„æ ¸å¿ƒç›®æ ‡æ˜¯å¼€å‘å¯è§£é‡Šäººå·¥æ™ºèƒ½ (Explainable Artificial Intelligence, XAI) æ¨¡å‹ï¼Œè¦æ±‚æ¨¡å‹åœ¨å›ç­”ä¸´åºŠç›¸å…³é—®é¢˜çš„åŒæ—¶ï¼Œæä¾›ç¬¦åˆåŒ»å­¦é€»è¾‘çš„å¯è§£é‡Šæ€§ä¾æ®ã€‚ç ”ç©¶å¼•å…¥äº†åŒ…å« 6,500 å¼ å›¾åƒå’Œ 159,549 ä¸ªå¤æ‚é—®ç­”å¯¹çš„ Kvasir-VQA-x1 æ•°æ®é›†ä½œä¸ºåŸºå‡†ï¼Œå¹¶è®¾ç½®äº†è§†è§‰é—®é¢˜å›ç­”ä¸å¤šæ¨¡æ€è§£é‡Šç”Ÿæˆä¸¤é¡¹å­ä»»åŠ¡ã€‚é€šè¿‡ç»“åˆå®šé‡æ€§èƒ½æŒ‡æ ‡ä¸ä¸“å®¶è¯„å®¡çš„è§£é‡Šæ€§è¯„ä¼°ï¼Œè¯¥ä»»åŠ¡æ—¨åœ¨è¿›ä¸€æ­¥æ¨åŠ¨åŒ»ç–—å›¾åƒåˆ†æé¢†åŸŸä¸­å¯ä¿¡äººå·¥æ™ºèƒ½ (trustworthy AI) çš„æŠ€æœ¯è¿›æ­¥ã€‚ç›®å‰ï¼Œç›¸å…³çš„æŒ‡ä»¤ã€æ•°æ®è®¿é—®æƒé™åŠå‚ä¸æŒ‡å—å·²åœ¨å®˜æ–¹ç«èµ›ä»“åº“ä¸­å‘å¸ƒï¼Œä¸ºä¸´åºŠå†³ç­–æ”¯æŒæä¾›äº†æ ‡å‡†åŒ–çš„è¯„ä¼°æ¡†æ¶ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10869v1",
      "published_date": "2025-08-14 17:43:46 UTC",
      "updated_date": "2025-08-14 17:43:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:47:36.686388+00:00"
    },
    {
      "arxiv_id": "2508.10865v1",
      "title": "Performance of GPT-5 in Brain Tumor MRI Reasoning",
      "title_zh": "GPT-5 åœ¨è„‘è‚¿ç˜¤ MRI æ¨ç†ä¸­çš„è¡¨ç°",
      "authors": [
        "Mojtaba Safari",
        "Shansong Wang",
        "Mingzhe Hu",
        "Zach Eidex",
        "Qiang Li",
        "Xiaofeng Yang"
      ],
      "abstract": "Accurate differentiation of brain tumor types on magnetic resonance imaging (MRI) is critical for guiding treatment planning in neuro-oncology. Recent advances in large language models (LLMs) have enabled visual question answering (VQA) approaches that integrate image interpretation with natural language reasoning. In this study, we evaluated GPT-4o, GPT-5-nano, GPT-5-mini, and GPT-5 on a curated brain tumor VQA benchmark derived from 3 Brain Tumor Segmentation (BraTS) datasets - glioblastoma (GLI), meningioma (MEN), and brain metastases (MET). Each case included multi-sequence MRI triplanar mosaics and structured clinical features transformed into standardized VQA items. Models were assessed in a zero-shot chain-of-thought setting for accuracy on both visual and reasoning tasks. Results showed that GPT-5-mini achieved the highest macro-average accuracy (44.19%), followed by GPT-5 (43.71%), GPT-4o (41.49%), and GPT-5-nano (35.85%). Performance varied by tumor subtype, with no single model dominating across all cohorts. These findings suggest that GPT-5 family models can achieve moderate accuracy in structured neuro-oncological VQA tasks, but not at a level acceptable for clinical use.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº† GPT-5 ç³»åˆ—æ¨¡å‹åœ¨è„‘è‚¿ç˜¤ MRI æ¨ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½è¡¨ç°ï¼Œé‡ç‚¹æ¢è®¨äº†å…¶åœ¨åŒºåˆ†ä¸åŒè„‘è‚¿ç˜¤ç±»å‹æ–¹é¢çš„æ½œåŠ›ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨ BraTS æ•°æ®é›†æ„å»ºäº†ä¸€ä¸ªè§†è§‰é—®ç­”(VQA)åŸºå‡†ï¼Œå¯¹ GPT-4oã€GPT-5-nanoã€GPT-5-mini å’Œ GPT-5 åœ¨é›¶æ ·æœ¬é“¾å¼æ€ç»´(zero-shot chain-of-thought)è®¾ç½®ä¸‹è¿›è¡Œäº†æµ‹è¯•ã€‚å®éªŒç»“åˆäº†å¤šåºåˆ— MRI å›¾åƒå’Œæ ‡å‡†åŒ–çš„ä¸´åºŠç‰¹å¾ï¼Œç»“æœæ˜¾ç¤º GPT-5-mini è·å¾—äº†æœ€é«˜çš„å®å¹³å‡å‡†ç¡®ç‡ï¼ˆ44.19%ï¼‰ï¼ŒGPT-5 ä»¥ 43.71% ç´§éšå…¶åã€‚å°½ç®¡è¿™äº›æ¨¡å‹åœ¨å¤„ç†ç¥ç»è‚¿ç˜¤å­¦ VQA ä»»åŠ¡æ—¶å±•ç°äº†ä¸€å®šçš„æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶è¡¨ç°éšè‚¿ç˜¤äºšå‹ï¼ˆGLI, MEN, METï¼‰è€Œå¼‚ï¼Œä¸”æ•´ä½“å‡†ç¡®ç‡å°šæœªè¾¾åˆ°ä¸´åºŠåŒ»ç–—å¯æ¥å—çš„æ ‡å‡†ã€‚è¯¥å‘ç°è¡¨æ˜ GPT-5 å®¶æ—æ¨¡å‹è™½ç„¶èƒ½å®ç°ä¸­ç­‰ç¨‹åº¦çš„æ¨ç†å‡†ç¡®æ€§ï¼Œä½†åœ¨å¤æ‚çš„ç¥ç»è‚¿ç˜¤è¾…åŠ©è¯Šæ–­ä¸­ä»æœ‰è¾ƒå¤§çš„æå‡ç©ºé—´ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10865v1",
      "published_date": "2025-08-14 17:35:31 UTC",
      "updated_date": "2025-08-14 17:35:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:47:57.762678+00:00"
    },
    {
      "arxiv_id": "2508.10860v1",
      "title": "From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms",
      "title_zh": "ä»é»‘ç®±åˆ°é€æ˜ï¼šåˆ©ç”¨å¯è§£é‡Šäººå·¥æ™ºèƒ½æå‡é«˜æ ¡è¯¾å ‚è‡ªåŠ¨åŒ–å£è¯‘è¯„ä¼°",
      "authors": [
        "Zhaokun Jiang",
        "Ziyin Zhang"
      ],
      "abstract": "Recent advancements in machine learning have spurred growing interests in automated interpreting quality assessment. Nevertheless, existing research suffers from insufficient examination of language use quality, unsatisfactory modeling effectiveness due to data scarcity and imbalance, and a lack of efforts to explain model predictions. To address these gaps, we propose a multi-dimensional modeling framework that integrates feature engineering, data augmentation, and explainable machine learning. This approach prioritizes explainability over ``black box'' predictions by utilizing only construct-relevant, transparent features and conducting Shapley Value (SHAP) analysis. Our results demonstrate strong predictive performance on a novel English-Chinese consecutive interpreting dataset, identifying BLEURT and CometKiwi scores to be the strongest predictive features for fidelity, pause-related features for fluency, and Chinese-specific phraseological diversity metrics for language use. Overall, by placing particular emphasis on explainability, we present a scalable, reliable, and transparent alternative to traditional human evaluation, facilitating the provision of detailed diagnostic feedback for learners and supporting self-regulated learning advantages not afforded by automated scores in isolation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨å£è¯‘è´¨é‡è¯„ä¼°ä¸­è¯­è¨€è´¨é‡è€ƒå¯Ÿä¸è¶³ã€æ•°æ®ç¨€ç¼ºå¯¼è‡´çš„å»ºæ¨¡æ•ˆæœæ¬ ä½³ä»¥åŠæ¨¡å‹é¢„æµ‹ç¼ºä¹è§£é‡Šæ€§ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ä¸ªé›†æˆç‰¹å¾å·¥ç¨‹(feature engineering)ã€æ•°æ®å¢å¼º(data augmentation)å’Œå¯è§£é‡Šæœºå™¨å­¦ä¹ (explainable machine learning)çš„å¤šç»´å»ºæ¨¡æ¡†æ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡ä»…åˆ©ç”¨ä¸æ„å¿µç›¸å…³çš„é€æ˜ç‰¹å¾å¹¶å¼•å…¥Shapley Value (SHAP)åˆ†æï¼Œå®ç°äº†ä»â€œé»‘ç›’â€é¢„æµ‹å‘é«˜é€æ˜åº¦è¯„ä¼°çš„è½¬å˜ã€‚åœ¨æ–°å‹è‹±æ±‰äº¤æ›¿ä¼ è¯‘æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒBLEURTå’ŒCometKiwiè¯„åˆ†æ˜¯è¡¡é‡å¿ å®åº¦(fidelity)çš„æœ€å¼ºé¢„æµ‹ç‰¹å¾ã€‚ç ”ç©¶åŒæ—¶å‘ç°ï¼Œåœé¡¿ç›¸å…³ç‰¹å¾å¯¹æµåˆ©åº¦(fluency)è¯„ä¼°è‡³å…³é‡è¦ï¼Œè€Œé’ˆå¯¹ä¸­æ–‡çš„è¯ç»„å¤šæ ·æ€§æŒ‡æ ‡(phraseological diversity metrics)åˆ™èƒ½æœ‰æ•ˆè¯„ä¼°è¯­è¨€ä½¿ç”¨è´¨é‡ã€‚è¯¥ç ”ç©¶ä¸ºä¼ ç»Ÿäººå·¥è¯„ä¼°æä¾›äº†ä¸€ä¸ªå¯æ‰©å±•ã€å¯é ä¸”é€æ˜çš„æ›¿ä»£æ–¹æ¡ˆã€‚è¿™ä¸ä»…èƒ½å¤Ÿä¸ºå­¦ä¹ è€…æä¾›è¯¦ç»†çš„è¯Šæ–­æ€§åé¦ˆï¼Œè¿˜å¼¥è¡¥äº†å•ä¸€è‡ªåŠ¨è¯„åˆ†åœ¨æ”¯æŒè‡ªæˆ‘è°ƒèŠ‚å­¦ä¹ æ–¹é¢çš„ä¸è¶³ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10860v1",
      "published_date": "2025-08-14 17:31:18 UTC",
      "updated_date": "2025-08-14 17:31:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:48:10.784426+00:00"
    },
    {
      "arxiv_id": "2508.11716v2",
      "title": "Privacy-Aware Detection of Fake Identity Documents: Methodology, Benchmark, and Improved Algorithms (FakeIDet2)",
      "title_zh": "éšç§ä¿æŠ¤çš„è™šå‡èº«ä»½è¯ä»¶æ£€æµ‹ï¼šæ–¹æ³•ä½“ç³»ã€åŸºå‡†æµ‹è¯•ä¸æ”¹è¿›ç®—æ³• (FakeIDet2)",
      "authors": [
        "Javier MuÃ±oz-Haro",
        "Ruben Tolosana",
        "Julian Fierrez",
        "Ruben Vera-Rodriguez",
        "Aythami Morales"
      ],
      "abstract": "Remote user verification in Internet-based applications is becoming increasingly important nowadays. A popular scenario for it consists of submitting a picture of the user's Identity Document (ID) to a service platform, authenticating its veracity, and then granting access to the requested digital service. An ID is well-suited to verify the identity of an individual, since it is government issued, unique, and nontransferable. However, with recent advances in Artificial Intelligence (AI), attackers can surpass security measures in IDs and create very realistic physical and synthetic fake IDs. Researchers are now trying to develop methods to detect an ever-growing number of these AI-based fakes that are almost indistinguishable from authentic (bona fide) IDs. In this counterattack effort, researchers are faced with an important challenge: the difficulty in using real data to train fake ID detectors. This real data scarcity for research and development is originated by the sensitive nature of these documents, which are usually kept private by the ID owners (the users) and the ID Holders (e.g., government, police, bank, etc.). The main contributions of our study are: 1) We propose and discuss a patch-based methodology to preserve privacy in fake ID detection research. 2) We provide a new public database, FakeIDet2-db, comprising over 900K real/fake ID patches extracted from 2,000 ID images, acquired using different smartphone sensors, illumination and height conditions, etc. In addition, three physical attacks are considered: print, screen, and composite. 3) We present a new privacy-aware fake ID detection method, FakeIDet2. 4) We release a standard reproducible benchmark that considers physical and synthetic attacks from popular databases in the literature.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äº’è”ç½‘åº”ç”¨ä¸­è¿œç¨‹éªŒè¯é¢ä¸´çš„ AI ä¼ªé€ èº«ä»½æ–‡ä»¶ (Fake Identity Documents) å¨èƒï¼Œæå‡ºäº†åä¸º FakeIDet2 çš„éšç§æ„ŸçŸ¥æ£€æµ‹æ–¹æ¡ˆã€‚ä¸ºè§£å†³çœŸå® ID æ•°æ®å› æ•æ„Ÿæ€§å¯¼è‡´çš„è®­ç»ƒæ ·æœ¬ç¨€ç¼ºé—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäº Patch-based çš„æ–¹æ³•è®ºï¼Œæ—¨åœ¨éšç§ä¿æŠ¤çš„å‰æä¸‹å®ç°é«˜æ•ˆçš„ä¼ªé€ æ£€æµ‹ã€‚ç ”ç©¶å›¢é˜ŸåŒæ­¥å‘å¸ƒäº†å¤§è§„æ¨¡æ•°æ®åº“ FakeIDet2-dbï¼Œè¯¥åº“åŒ…å«ä»ä¸åŒä¼ æ„Ÿå™¨åŠæ‹æ‘„ç¯å¢ƒè·å–çš„ 2000 å¼  ID å›¾åƒä¸­æå–çš„ 90 ä½™ä¸‡ä¸ªçœŸå®ä¸ä¼ªé€  Patchï¼Œå¹¶æ¶µç›–äº† Printã€Screen å’Œ Composite ç­‰å¤šç§ç‰©ç†æ”»å‡»æ¨¡å¼ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜æå‡ºäº†ä¸€ç§æ”¹è¿›çš„éšç§æ„ŸçŸ¥æ£€æµ‹ç®—æ³•ï¼Œå¹¶å»ºç«‹äº†æ ‡å‡†çš„å¯é‡å¤ Benchmarkï¼Œç”¨ä»¥ç³»ç»Ÿè¯„ä¼°é’ˆå¯¹ç‰©ç†å’Œåˆæˆæ”»å‡»çš„é˜²å¾¡æ•ˆèƒ½ã€‚è¯¥å·¥ä½œé€šè¿‡åˆ›æ–°çš„æ–¹æ³•è®ºã€æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ï¼Œä¸ºåœ¨ä¿éšœæ•°æ®éšç§çš„å‰æä¸‹æå‡èº«ä»½æ–‡ä»¶éªŒè¯çš„å®‰å…¨æ€§æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV",
        "eess.IV"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11716v2",
      "published_date": "2025-08-14 17:30:36 UTC",
      "updated_date": "2025-08-28 11:05:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:47:57.954692+00:00"
    },
    {
      "arxiv_id": "2508.10839v1",
      "title": "Reinforced Language Models for Sequential Decision Making",
      "title_zh": "é¢å‘åºåˆ—å†³ç­–çš„å¼ºåŒ–è¯­è¨€æ¨¡å‹",
      "authors": [
        "Jim Dilkes",
        "Vahid Yazdanpanah",
        "Sebastian Stein"
      ],
      "abstract": "Large Language Models (LLMs) show potential as sequential decision-making agents, but their application is often limited due to a reliance on large, computationally expensive models. This creates a need to improve smaller models, yet existing post-training methods are designed for single-turn interactions and cannot handle credit assignment in multi-step agentic tasks. To address this, we introduce Multi-Step Group-Relative Policy Optimization (MS-GRPO), a new algorithm for post-training LLM agents, grounded in formal Text-Mediated Stochastic Game (TSMG) and Language-Agent Policy (LAP) frameworks. For credit assignment, MS-GRPO attributes the entire cumulative episode reward to each individual episode step. We supplement this algorithm with a novel absolute-advantage-weighted episode sampling strategy that we show improves training performance. We evaluate our approach by post-training a 3-billion parameter model on Snake and Frozen Lake. Our experiments demonstrate that the method is effective in improving decision-making performance: our post-trained 3B parameter model outperforms a 72B parameter baseline by 50% on the Frozen Lake task. This work demonstrates that targeted post-training is a practical and efficient alternative to relying on model scale for creating sequential decision-making agents using LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)ä½œä¸ºåºè´¯å†³ç­–æ™ºèƒ½ä½“(Sequential Decision-Making Agents)æ—¶å­˜åœ¨çš„è®¡ç®—èµ„æºä¾èµ–åŠå¤šæ­¥ä»»åŠ¡ä¿¡ç”¨åˆ†é…(Credit Assignment)éš¾é¢˜ï¼Œæå‡ºäº†Multi-Step Group-Relative Policy Optimization (MS-GRPO)åè®­ç»ƒç®—æ³•ã€‚è¯¥æ–¹æ³•åŸºäºæ–‡æœ¬ä»‹å¯¼éšæœºåšå¼ˆ(TSMG)å’Œè¯­è¨€æ™ºèƒ½ä½“ç­–ç•¥(LAP)æ¡†æ¶ï¼Œé€šè¿‡å°†ç´¯ç§¯å›åˆå¥–åŠ±å±æ€§åŒ–è‡³æ¯ä¸ªå†³ç­–æ­¥éª¤ï¼Œå¹¶è¾…ä»¥ç»å¯¹ä¼˜åŠ¿åŠ æƒå›åˆé‡‡æ ·(Absolute-advantage-weighted episode sampling)ç­–ç•¥æ¥ä¼˜åŒ–è®­ç»ƒæ€§èƒ½ã€‚å®éªŒåœ¨Snakeå’ŒFrozen Lakeä»»åŠ¡ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œç»“æœè¡¨æ˜ç»è¿‡åè®­ç»ƒçš„3Bå‚æ•°æ¨¡å‹åœ¨Frozen Lakeä»»åŠ¡ä¸­çš„è¡¨ç°ä¼˜äº72Bå‚æ•°åŸºçº¿æ¨¡å‹è¾¾50%ã€‚è¯¥ç ”ç©¶è¯æ˜äº†é’ˆå¯¹æ€§åè®­ç»ƒæ˜¯æå‡å°è§„æ¨¡æ¨¡å‹åºè´¯å†³ç­–èƒ½åŠ›çš„æœ‰æ•ˆé€”å¾„ï¼Œä¸ºæ„å»ºé«˜æ•ˆæ™ºèƒ½ä½“æä¾›äº†æ¯”å•çº¯è¿½æ±‚æ¨¡å‹è§„æ¨¡æ›´å®ç”¨çš„æ›¿ä»£æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10839v1",
      "published_date": "2025-08-14 17:05:44 UTC",
      "updated_date": "2025-08-14 17:05:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:48:07.796660+00:00"
    },
    {
      "arxiv_id": "2508.10828v1",
      "title": "A Multimodal Neural Network for Recognizing Subjective Self-Disclosure Towards Social Robots",
      "title_zh": "é¢å‘ç¤¾äº¤æœºå™¨äººä¸»è§‚è‡ªæˆ‘æŠ«éœ²è¯†åˆ«çš„å¤šæ¨¡æ€ç¥ç»ç½‘ç»œ",
      "authors": [
        "Henry Powell",
        "Guy Laban",
        "Emily S. Cross"
      ],
      "abstract": "Subjective self-disclosure is an important feature of human social interaction. While much has been done in the social and behavioural literature to characterise the features and consequences of subjective self-disclosure, little work has been done thus far to develop computational systems that are able to accurately model it. Even less work has been done that attempts to model specifically how human interactants self-disclose with robotic partners. It is becoming more pressing as we require social robots to work in conjunction with and establish relationships with humans in various social settings. In this paper, our aim is to develop a custom multimodal attention network based on models from the emotion recognition literature, training this model on a large self-collected self-disclosure video corpus, and constructing a new loss function, the scale preserving cross entropy loss, that improves upon both classification and regression versions of this problem. Our results show that the best performing model, trained with our novel loss function, achieves an F1 score of 0.83, an improvement of 0.48 from the best baseline model. This result makes significant headway in the aim of allowing social robots to pick up on an interaction partner's self-disclosures, an ability that will be essential in social robots with social cognition.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººç±»ä¸ç¤¾äº¤æœºå™¨äºº (Social Robots) äº’åŠ¨ä¸­ä¸»è§‚è‡ªæˆ‘æŠ«éœ² (Subjective Self-Disclosure) çš„è¯†åˆ«é—®é¢˜ï¼Œæ—¨åœ¨å¡«è¡¥ç°æœ‰è®¡ç®—æ¨¡å‹åœ¨å»ºæ¨¡äººç±»å‘æœºå™¨äººè‡ªæˆ‘æŠ«éœ²æ–¹é¢çš„ç©ºç™½ã€‚ä¸ºæ­¤ï¼Œä½œè€…åŸºäºæƒ…æ„Ÿè¯†åˆ« (Emotion Recognition) æ–‡çŒ®ä¸­çš„æ¨¡å‹ï¼Œå¼€å‘äº†ä¸€ç§å®šåˆ¶çš„å¤šæ¨¡æ€æ³¨æ„åŠ›ç½‘ç»œ (Multimodal Attention Network)ã€‚è¯¥æ¨¡å‹åœ¨ä½œè€…è‡ªè¡Œæ”¶é›†çš„å¤§å‹è‡ªæˆ‘æŠ«éœ²è§†é¢‘è¯­æ–™åº“ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°å‹çš„æŸå¤±å‡½æ•°â€”â€”å°ºåº¦ä¿æŒäº¤å‰ç†µæŸå¤± (Scale Preserving Cross Entropy Loss)ã€‚è¿™ç§æ–°æŸå¤±å‡½æ•°åœ¨å¤„ç†åˆ†ç±»å’Œå›å½’é—®é¢˜ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼Œæœ‰æ•ˆä¼˜åŒ–äº†æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨æµ‹è¯•ä¸­å–å¾—äº† 0.83 çš„ F1 scoreï¼Œç›¸è¾ƒäºæœ€ä½³åŸºçº¿æ¨¡å‹å¤§å¹…æå‡äº† 0.48ã€‚è¿™é¡¹ç ”ç©¶ä¸ºç¤¾äº¤æœºå™¨äººæ•æ‰äº’åŠ¨ä¼™ä¼´çš„è‡ªæˆ‘æŠ«éœ²è¡Œä¸ºæä¾›äº†é‡è¦æŠ€æœ¯æ”¯æŒï¼Œæ˜¯æå‡æœºå™¨äººç¤¾äº¤è®¤çŸ¥èƒ½åŠ›çš„å…³é”®è¿›å±•ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted at 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
      "pdf_url": "https://arxiv.org/pdf/2508.10828v1",
      "published_date": "2025-08-14 16:50:51 UTC",
      "updated_date": "2025-08-14 16:50:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:48:01.390505+00:00"
    },
    {
      "arxiv_id": "2508.10972v1",
      "title": "Not There Yet: Evaluating Vision Language Models in Simulating the Visual Perception of People with Low Vision",
      "title_zh": "å°šæœ‰å·®è·ï¼šè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ¨¡æ‹Ÿä½è§†åŠ›äººç¾¤è§†è§‰æ„ŸçŸ¥ä¸­çš„è¡¨ç°",
      "authors": [
        "Rosiana Natalie",
        "Wenqian Xu",
        "Ruei-Che Chang",
        "Rada Mihalcea",
        "Anhong Guo"
      ],
      "abstract": "Advances in vision language models (VLMs) have enabled the simulation of general human behavior through their reasoning and problem solving capabilities. However, prior research has not investigated such simulation capabilities in the accessibility domain. In this paper, we evaluate the extent to which VLMs can simulate the vision perception of low vision individuals when interpreting images. We first compile a benchmark dataset through a survey study with 40 low vision participants, collecting their brief and detailed vision information and both open-ended and multiple-choice image perception and recognition responses to up to 25 images. Using these responses, we construct prompts for VLMs (GPT-4o) to create simulated agents of each participant, varying the included information on vision information and example image responses. We evaluate the agreement between VLM-generated responses and participants' original answers. Our results indicate that VLMs tend to infer beyond the specified vision ability when given minimal prompts, resulting in low agreement (0.59). The agreement between the agent' and participants' responses remains low when only either the vision information (0.59) or example image responses (0.59) are provided, whereas a combination of both significantly increase the agreement (0.70, p < 0.0001). Notably, a single example combining both open-ended and multiple-choice responses, offers significant performance improvements over either alone (p < 0.0001), while additional examples provided minimal benefits (p > 0.05).",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨æ¨¡æ‹Ÿä½è§†åŠ›(Low Vision)äººç¾¤è§†è§‰æ„ŸçŸ¥æ–¹é¢çš„èƒ½åŠ›ï¼Œå¡«è¡¥äº†å¯è®¿é—®æ€§é¢†åŸŸæ¨¡æ‹Ÿç ”ç©¶çš„ç©ºç™½ã€‚ä½œè€…é€šè¿‡å¯¹40åä½è§†åŠ›å‚ä¸è€…çš„è°ƒæŸ¥æ„å»ºäº†åŸºå‡†æ•°æ®é›†ï¼Œè®°å½•äº†ä»–ä»¬çš„è§†è§‰èƒŒæ™¯ä¿¡æ¯ä»¥åŠå¯¹25å¼ å›¾åƒçš„æ„ŸçŸ¥å“åº”ï¼Œå¹¶ä»¥æ­¤åˆ©ç”¨GPT-4oæ„å»ºäº†æ¨¡æ‹Ÿæ™ºèƒ½ä½“ã€‚ç ”ç©¶é€šè¿‡æ”¹å˜æç¤ºè¯ä¸­åŒ…å«çš„è§†è§‰ä¿¡æ¯å’Œç¤ºä¾‹å“åº”ï¼Œè¯„ä¼°äº†æ¨¡å‹ç”Ÿæˆç»“æœä¸çœŸå®å‚ä¸è€…å›ç­”ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨æç¤ºä¿¡æ¯æœ€å°‘çš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹å®¹æ˜“äº§ç”Ÿè¿‡åº¦æ¨æ–­ï¼Œä¸€è‡´æ€§å¾—åˆ†ä»…ä¸º0.59ï¼›è€ŒåŒæ—¶æä¾›è§†è§‰æè¿°å’Œæ„ŸçŸ¥ç¤ºä¾‹èƒ½å°†ä¸€è‡´æ€§æ˜¾è‘—æå‡è‡³0.70ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°åŒ…å«å¼€æ”¾å¼å’Œå¤šé¡¹é€‰æ‹©å“åº”çš„å•ä¸€ç»¼åˆç¤ºä¾‹å¯¹æ€§èƒ½çš„æå‡æ•ˆæœæœ€ä¸ºæ˜¾è‘—ï¼Œè€Œå¢åŠ æ›´å¤šç¤ºä¾‹å¸¦æ¥çš„æ”¶ç›Šæœ‰é™ã€‚è¯¥ç ”ç©¶è¡¨æ˜å½“å‰çš„VLMsåœ¨æ¨¡æ‹Ÿç‰¹å®šäººç¾¤çš„è§†è§‰æ„ŸçŸ¥æ—¶ä»å­˜åœ¨å±€é™ï¼Œä¸”é«˜åº¦ä¾èµ–äºä¸Šä¸‹æ–‡æç¤ºçš„è´¨é‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10972v1",
      "published_date": "2025-08-14 16:46:03 UTC",
      "updated_date": "2025-08-14 16:46:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:48:17.284976+00:00"
    },
    {
      "arxiv_id": "2508.11715v1",
      "title": "Benchmark Dataset Generation and Evaluation for Excel Formula Repair with LLMs",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ Excel å…¬å¼ä¿®å¤åŸºå‡†æ•°æ®é›†ç”Ÿæˆä¸è¯„ä¼°",
      "authors": [
        "Ananya Singha",
        "Harshita Sahijwani",
        "Walt Williams",
        "Emmanuel Aboah Boateng",
        "Nick Hausman",
        "Miguel Di Luca",
        "Keegan Choudhury",
        "Chaya Binet",
        "Vu Le",
        "Tianwei Chen",
        "Oryan Rokeah Chen",
        "Sulaiman Vesal",
        "Sadid Hasan"
      ],
      "abstract": "Excel is a pervasive yet often complex tool, particularly for novice users, where runtime errors arising from logical mistakes or misinterpretations of functions pose a significant challenge. While large language models (LLMs) offer promising assistance by explaining formula errors, the automated correction of these semantic runtime errors remains an open problem. A primary challenge to advancing models for such scenarios is the severe lack of high-quality, comprehensive datasets for training and rigorous evaluation. This paper addresses this gap by introducing a novel approach for constructing a benchmark dataset specifically designed for Excel formula repair. We propose a data generation pipeline, which leverages a small set of curated seed samples from online forums to synthetically expand the dataset. Our pipeline integrates few-shot prompting with LLMs and employs a robust \\textit{LLM-as-a-Judge} validation framework, combined with execution-based checks to ensure the correctness and semantic fidelity of the generated data. This process produced a benchmark dataset of 618 high-quality samples, covering common runtime errors. Furthermore, we propose a context-aware baseline technique for Excel formula repair that utilizes LLMs to leverage both the faulty formula, and relevant spreadsheet context. We evaluate the performance of various LLMs (GPT-4o, GPT-4.1, Phi-3, Mistral) on our newly generated benchmark using execution-based metrics. Our analysis demonstrates the dataset's quality through manual annotation and provides insights into error and function distributions. The proposed generation methodology is highly scalable and can be readily adapted to create evaluation benchmarks for similar code repair tasks in other low-resource programming languages.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Excelå…¬å¼ä¿®å¤é¢†åŸŸç¼ºä¹é«˜è´¨é‡è®­ç»ƒå’Œè¯„ä¼°æ•°æ®é›†çš„ç°çŠ¶ï¼Œæå‡ºäº†ä¸€ç§åˆ›æ–°çš„Benchmarkæ•°æ®é›†æ„å»ºæ–¹æ³•ã€‚ä½œè€…å¼€å‘äº†ä¸€å¥—æ•°æ®ç”Ÿæˆæµæ°´çº¿ï¼Œåˆ©ç”¨åœ¨çº¿è®ºå›çš„ç§å­æ ·æœ¬é€šè¿‡Few-shotæç¤ºè¯ä¸LLMsè¿›è¡Œåˆæˆæ‰©å±•ï¼Œå¹¶ç»“åˆLLM-as-a-JudgeéªŒè¯æ¡†æ¶ä¸åŸºäºæ‰§è¡Œ(execution-based)çš„æ£€æŸ¥æ¥ç¡®ä¿è¯­ä¹‰ä¿çœŸåº¦ã€‚è¯¥æµç¨‹æœ€ç»ˆç”Ÿæˆäº†åŒ…å«618ä¸ªæ¶µç›–å¸¸è§è¿è¡Œæ—¶é”™è¯¯çš„é«˜è´¨é‡æ ·æœ¬æ•°æ®é›†ã€‚æ­¤å¤–ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆå…¬å¼ä¸ç”µå­è¡¨æ ¼ä¸Šä¸‹æ–‡çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥(context-aware)ä¿®å¤åŸºå‡†æŠ€æœ¯ï¼Œå¹¶åœ¨æ–°æ•°æ®é›†ä¸Šå¯¹GPT-4oã€GPT-4.1ã€Phi-3å’ŒMistralç­‰å¤šç§LLMsè¿›è¡Œäº†æ€§èƒ½è¯„ä¼°ã€‚å®éªŒç»“æœä¸ä»…éªŒè¯äº†æ•°æ®é›†çš„è´¨é‡ï¼Œè¿˜è¡¨æ˜è¯¥ç”Ÿæˆæ–¹æ³•å…·æœ‰é«˜åº¦çš„å¯æ‰©å±•æ€§ï¼Œå¯æ¨å¹¿è‡³å…¶ä»–ä½èµ„æºç¼–ç¨‹è¯­è¨€çš„ä»£ç ä¿®å¤ä»»åŠ¡ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted at the KDD workshop on Evaluation and Trustworthiness of Agentic and Generative AI Models",
      "pdf_url": "https://arxiv.org/pdf/2508.11715v1",
      "published_date": "2025-08-14 16:43:35 UTC",
      "updated_date": "2025-08-14 16:43:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:48:17.650569+00:00"
    },
    {
      "arxiv_id": "2508.10971v1",
      "title": "Rule2Text: A Framework for Generating and Evaluating Natural Language Explanations of Knowledge Graph Rules",
      "title_zh": "Rule2Textï¼šçŸ¥è¯†å›¾è°±è§„åˆ™è‡ªç„¶è¯­è¨€è§£é‡Šç”Ÿæˆä¸è¯„ä¼°æ¡†æ¶",
      "authors": [
        "Nasim Shirvani-Mahdavi",
        "Chengkai Li"
      ],
      "abstract": "Knowledge graphs (KGs) can be enhanced through rule mining; however, the resulting logical rules are often difficult for humans to interpret due to their inherent complexity and the idiosyncratic labeling conventions of individual KGs. This work presents Rule2Text, a comprehensive framework that leverages large language models (LLMs) to generate natural language explanations for mined logical rules, thereby improving KG accessibility and usability. We conduct extensive experiments using multiple datasets, including Freebase variants (FB-CVT-REV, FB+CVT-REV, and FB15k-237) as well as the ogbl-biokg dataset, with rules mined using AMIE 3.5.1. We systematically evaluate several LLMs across a comprehensive range of prompting strategies, including zero-shot, few-shot, variable type incorporation, and Chain-of-Thought reasoning. To systematically assess models' performance, we conduct a human evaluation of generated explanations on correctness and clarity. To address evaluation scalability, we develop and validate an LLM-as-a-judge framework that demonstrates strong agreement with human evaluators. Leveraging the best-performing model (Gemini 2.0 Flash), LLM judge, and human-in-the-loop feedback, we construct high-quality ground truth datasets, which we use to fine-tune the open-source Zephyr model. Our results demonstrate significant improvements in explanation quality after fine-tuning, with particularly strong gains in the domain-specific dataset. Additionally, we integrate a type inference module to support KGs lacking explicit type information. All code and data are publicly available at https://github.com/idirlab/KGRule2NL.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Rule2Textæ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸ºçŸ¥è¯†å›¾è°±(KGs)æŒ–æ˜å‡ºçš„é€»è¾‘è§„åˆ™ç”Ÿæˆè‡ªç„¶è¯­è¨€è§£é‡Šï¼Œè§£å†³è§„åˆ™å› å¤æ‚æ€§è€Œéš¾ä»¥è¢«äººç±»ç†è§£çš„é—®é¢˜ã€‚ç ”ç©¶åœ¨Freebaseå˜ä½“å’Œogbl-biokgç­‰æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œç³»ç»Ÿè¯„ä¼°äº†é›¶æ ·æœ¬(zero-shot)ã€å°‘æ ·æœ¬(few-shot)åŠé“¾å¼æ€ç»´(Chain-of-Thought)ç­‰å¤šç§æç¤ºç­–ç•¥ã€‚ä¸ºäº†æå‡è¯„ä¼°çš„å¯æ‰©å±•æ€§ï¼Œç ”ç©¶è€…å¼€å‘å¹¶éªŒè¯äº†LLM-as-a-judgeæ¡†æ¶ï¼Œå…¶å®éªŒç»“æœæ˜¾ç¤ºè¯¥æ¡†æ¶ä¸äººç±»åœ¨æ­£ç¡®æ€§å’Œæ¸…æ™°åº¦ä¸Šçš„è¯„ä»·é«˜åº¦ä¸€è‡´ã€‚é€šè¿‡åˆ©ç”¨Gemini 2.0 Flashå’Œäººå·¥åé¦ˆæ„å»ºçš„é«˜è´¨é‡åŸºå‡†æ•°æ®é›†ï¼Œç ”ç©¶è¿›ä¸€æ­¥å¾®è°ƒäº†å¼€æºæ¨¡å‹Zephyrï¼Œåœ¨é¢†åŸŸç‰¹å®šæ•°æ®é›†ä¸­å–å¾—äº†æ˜¾è‘—çš„è´¨é‡æå‡ã€‚æ­¤å¤–ï¼ŒRule2Textè¿˜é›†æˆäº†ç±»å‹æ¨æ–­(type inference)æ¨¡å—ï¼Œä»¥æ”¯æŒç¼ºä¹æ˜¾å¼ç±»å‹ä¿¡æ¯çš„KGsã€‚è¯¥å·¥ä½œä¸ºæé«˜çŸ¥è¯†å›¾è°±çš„å¯è®¿é—®æ€§æä¾›äº†æœ‰æ•ˆçš„ç³»ç»ŸåŒ–æ–¹æ¡ˆï¼Œç›¸å…³ä»£ç å’Œæ•°æ®å‡å·²å…¬å¼€å‘å¸ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "arXiv admin note: text overlap with arXiv:2507.23740",
      "pdf_url": "https://arxiv.org/pdf/2508.10971v1",
      "published_date": "2025-08-14 16:41:47 UTC",
      "updated_date": "2025-08-14 16:41:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:48:18.352899+00:00"
    },
    {
      "arxiv_id": "2508.10806v1",
      "title": "Who Benefits from AI Explanations? Towards Accessible and Interpretable Systems",
      "title_zh": "è°èƒ½ä» AI è§£é‡Šä¸­è·ç›Šï¼Ÿè¿ˆå‘æ— éšœç¢ä¸”å¯è§£é‡Šçš„ç³»ç»Ÿ",
      "authors": [
        "Maria J. P. Peixoto",
        "Akriti Pandey",
        "Ahsan Zaman",
        "Peter R. Lewis"
      ],
      "abstract": "As AI systems are increasingly deployed to support decision-making in critical domains, explainability has become a means to enhance the understandability of these outputs and enable users to make more informed and conscious choices. However, despite growing interest in the usability of eXplainable AI (XAI), the accessibility of these methods, particularly for users with vision impairments, remains underexplored. This paper investigates accessibility gaps in XAI through a two-pronged approach. First, a literature review of 79 studies reveals that evaluations of XAI techniques rarely include disabled users, with most explanations relying on inherently visual formats. Second, we present a four-part methodological proof of concept that operationalizes inclusive XAI design: (1) categorization of AI systems, (2) persona definition and contextualization, (3) prototype design and implementation, and (4) expert and user assessment of XAI techniques for accessibility. Preliminary findings suggest that simplified explanations are more comprehensible for non-visual users than detailed ones, and that multimodal presentation is required for more equitable interpretability.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¯è§£é‡Šäººå·¥æ™ºèƒ½ (XAI) åœ¨å¯è®¿é—®æ€§ (Accessibility) æ–¹é¢çš„ç¼ºå£ï¼ŒæŒ‡å‡ºè™½ç„¶ XAI æ—¨åœ¨å¢å¼ºç³»ç»Ÿè¾“å‡ºçš„å¯ç†è§£æ€§ï¼Œä½†é’ˆå¯¹è§†éšœç”¨æˆ·çš„ç ”ç©¶ä»è¢«ä¸¥é‡å¿½è§†ã€‚é€šè¿‡å¯¹ 79 é¡¹ç ”ç©¶çš„æ–‡çŒ®ç»¼è¿°ï¼Œä½œè€…å‘ç°ç°æœ‰çš„ XAI æŠ€æœ¯è¯„ä¼°æå°‘å°†æ®‹ç–¾ç”¨æˆ·çº³å…¥å…¶ä¸­ï¼Œä¸”ç»å¤§å¤šæ•°è§£é‡Šé«˜åº¦ä¾èµ–äºè§†è§‰æ ¼å¼ã€‚é’ˆå¯¹è¿™ä¸€ç°çŠ¶ï¼Œè®ºæ–‡æå‡ºäº†ä¸€å¥—åŒ…å«ç³»ç»Ÿåˆ†ç±»ã€ç”¨æˆ·ç”»åƒå®šä¹‰ã€åŸå‹è®¾è®¡åŠä¸“å®¶ä¸ç”¨æˆ·è¯„ä¼°çš„å››é˜¶æ®µåŒ…å®¹æ€§ XAI è®¾è®¡æ–¹æ³•è®ºã€‚åˆæ­¥ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç›¸æ¯”äºå¤æ‚çš„è¯¦ç»†è¯´æ˜ï¼Œç®€åŒ–çš„è§£é‡Šå¯¹äºéè§†è§‰ç”¨æˆ·è€Œè¨€æ›´å…·å¯ç†è§£æ€§ã€‚è¯¥è®ºæ–‡æœ€åå¼ºè°ƒï¼Œä¸ºäº†å®ç°æ›´å…¬å¹³çš„å¯è§£é‡Šæ€§ï¼Œå¿…é¡»å¼•å…¥å¤šæ¨¡æ€ (Multimodal) çš„å‘ˆç°æ–¹å¼ï¼Œä»è€Œè®© AI ç³»ç»ŸçœŸæ­£æœåŠ¡äºå¤šå…ƒåŒ–çš„ç”¨æˆ·ç¾¤ä½“ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Paper accepted for the IJCAI 2025 Workshop on Explainable Artificial Intelligence (XAI): https://sites.google.com/view/xai2025/proceedings",
      "pdf_url": "https://arxiv.org/pdf/2508.10806v1",
      "published_date": "2025-08-14 16:26:09 UTC",
      "updated_date": "2025-08-14 16:26:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:48:28.954407+00:00"
    },
    {
      "arxiv_id": "2508.10798v1",
      "title": "The SET Perceptual Factors Framework: Towards Assured Perception for Autonomous Systems",
      "title_zh": "SET æ„ŸçŸ¥å› ç´ æ¡†æ¶ï¼šè¿ˆå‘è‡ªä¸»ç³»ç»Ÿçš„ç¡®ä¿¡æ„ŸçŸ¥",
      "authors": [
        "Troi Williams"
      ],
      "abstract": "Future autonomous systems promise significant societal benefits, yet their deployment raises concerns about safety and trustworthiness. A key concern is assuring the reliability of robot perception, as perception seeds safe decision-making. Failures in perception are often due to complex yet common environmental factors and can lead to accidents that erode public trust. To address this concern, we introduce the SET (Self, Environment, and Target) Perceptual Factors Framework. We designed the framework to systematically analyze how factors such as weather, occlusion, or sensor limitations negatively impact perception. To achieve this, the framework employs SET State Trees to categorize where such factors originate and SET Factor Trees to model how these sources and factors impact perceptual tasks like object detection or pose estimation. Next, we develop Perceptual Factor Models using both trees to quantify the uncertainty for a given task. Our framework aims to promote rigorous safety assurances and cultivate greater public understanding and trust in autonomous systems by offering a transparent and standardized method for identifying, modeling, and communicating perceptual risks.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿåœ¨å®‰å…¨æ€§ä¸å¯ä¿¡åº¦æ–¹é¢çš„æŒ‘æˆ˜ï¼Œé‡ç‚¹æ¢è®¨äº†æœºå™¨äººæ„ŸçŸ¥(Perception)çš„å¯é æ€§é—®é¢˜ï¼Œå› ä¸ºæ„ŸçŸ¥å¤±æ•ˆå¾€å¾€æ˜¯å¯¼è‡´äº‹æ•…çš„å…³é”®è¯±å› ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† SET (Self, Environment, and Target) Perceptual Factors Frameworkï¼Œæ—¨åœ¨ç³»ç»Ÿæ€§åœ°åˆ†æç¯å¢ƒã€é®æŒ¡æˆ–ä¼ æ„Ÿå™¨é™åˆ¶ç­‰å› ç´ å¯¹æ„ŸçŸ¥çš„è´Ÿé¢å½±å“ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ SET State Trees å¯¹å½±å“å› ç´ çš„æ¥æºè¿›è¡Œåˆ†ç±»ï¼Œå¹¶ç»“åˆ SET Factor Trees å»ºæ¨¡è¿™äº›å› ç´ å¦‚ä½•å…·ä½“å¹²æ‰°ç›®æ ‡æ£€æµ‹(Object Detection)æˆ–å§¿æ€ä¼°è®¡(Pose Estimation)ç­‰ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼€å‘äº† Perceptual Factor Models ä»¥é‡åŒ–ç‰¹å®šæ„ŸçŸ¥ä»»åŠ¡çš„ä¸ç¡®å®šæ€§(Uncertainty)ã€‚é€šè¿‡æä¾›é€æ˜ä¸”æ ‡å‡†åŒ–çš„æ„ŸçŸ¥é£é™©è¯†åˆ«ä¸å»ºæ¨¡æ–¹æ³•ï¼Œè¯¥æ¡†æ¶ä¸ºè‡ªä¸»ç³»ç»Ÿçš„å®‰å…¨ä¿éšœæä¾›äº†é‡è¦æ”¯æ’‘ï¼Œæœ‰åŠ©äºæå‡å…¬ä¼—å¯¹è¯¥æŠ€æœ¯çš„ç†è§£ä¸ä¿¡ä»»ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "4 pages, 4 figures, accepted to the Workshop on Public Trust in Autonomous Systems at the 2025 IEEE International Conference on Robotics & Automation",
      "pdf_url": "https://arxiv.org/pdf/2508.10798v1",
      "published_date": "2025-08-14 16:22:01 UTC",
      "updated_date": "2025-08-14 16:22:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:48:26.854169+00:00"
    },
    {
      "arxiv_id": "2508.10785v1",
      "title": "Enhancing Fairness in Autoencoders for Node-Level Graph Anomaly Detection",
      "title_zh": "æå‡èŠ‚ç‚¹çº§å›¾å¼‚å¸¸æ£€æµ‹ä¸­è‡ªç¼–ç å™¨çš„å…¬å¹³æ€§",
      "authors": [
        "Shouju Wang",
        "Yuchen Song",
        "Sheng'en Li",
        "Dongmian Zou"
      ],
      "abstract": "Graph anomaly detection (GAD) has become an increasingly important task across various domains. With the rapid development of graph neural networks (GNNs), GAD methods have achieved significant performance improvements. However, fairness considerations in GAD remain largely underexplored. Indeed, GNN-based GAD models can inherit and amplify biases present in training data, potentially leading to unfair outcomes. While existing efforts have focused on developing fair GNNs, most approaches target node classification tasks, where models often rely on simple layer architectures rather than autoencoder-based structures, which are the most widely used architecturs for anomaly detection. To address fairness in autoencoder-based GAD models, we propose \\textbf{D}is\\textbf{E}ntangled \\textbf{C}ounterfactual \\textbf{A}dversarial \\textbf{F}air (DECAF)-GAD, a framework that alleviates bias while preserving GAD performance. Specifically, we introduce a structural causal model (SCM) to disentangle sensitive attributes from learned representations. Based on this causal framework, we formulate a specialized autoencoder architecture along with a fairness-guided loss function. Through extensive experiments on both synthetic and real-world datasets, we demonstrate that DECAF-GAD not only achieves competitive anomaly detection performance but also significantly enhances fairness metrics compared to baseline GAD methods. Our code is available at https://github.com/Tlhey/decaf_code.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†èŠ‚ç‚¹çº§å›¾å¼‚å¸¸æ£€æµ‹ (Graph Anomaly Detection, GAD) ä¸­çš„å…¬å¹³æ€§é—®é¢˜ï¼ŒæŒ‡å‡ºåŸºäºå›¾ç¥ç»ç½‘ç»œ (GNNs) çš„æ¨¡å‹å®¹æ˜“ç»§æ‰¿å’Œæ”¾å¤§è®­ç»ƒæ•°æ®ä¸­çš„åè§ã€‚é’ˆå¯¹ç°æœ‰å…¬å¹³æ€§ç ”ç©¶å¤šé›†ä¸­äºèŠ‚ç‚¹åˆ†ç±»è€Œéå¼‚å¸¸æ£€æµ‹æœ€å¸¸ç”¨çš„è‡ªåŠ¨ç¼–ç å™¨ (Autoencoder) ç»“æ„è¿™ä¸€ç°çŠ¶ï¼Œä½œè€…æå‡ºäº† DECAF-GAD æ¡†æ¶ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ç»“æ„å› æœæ¨¡å‹ (Structural Causal Model, SCM) ä»¥å®ç°æ•æ„Ÿå±æ€§ä¸å­¦ä¹ è¡¨å¾çš„è§£è€¦ã€‚åŸºäºæ­¤å› æœæ¡†æ¶ï¼Œç ”ç©¶è®¾è®¡äº†ä¸“é—¨çš„è‡ªåŠ¨ç¼–ç å™¨æ¶æ„å’Œå…¬å¹³æ€§å¼•å¯¼çš„æŸå¤±å‡½æ•°ã€‚åœ¨åˆæˆåŠçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒDECAF-GAD åœ¨ä¿æŒç«äº‰åŠ›çš„å¼‚å¸¸æ£€æµ‹æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†å…¬å¹³æ€§æŒ‡æ ‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted in ECAI-2025",
      "pdf_url": "https://arxiv.org/pdf/2508.10785v1",
      "published_date": "2025-08-14 16:12:15 UTC",
      "updated_date": "2025-08-14 16:12:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:48:44.359371+00:00"
    },
    {
      "arxiv_id": "2508.10779v1",
      "title": "Ultra-High-Definition Reference-Based Landmark Image Super-Resolution with Generative Diffusion Prior",
      "title_zh": "# è®ºæ–‡æ ‡é¢˜ç¿»è¯‘ ğŸ“„\n\n---\n\nåŸºäºç”Ÿæˆå¼æ‰©æ•£å…ˆéªŒçš„è¶…é«˜æ¸…å‚è€ƒå¼åœ°æ ‡å›¾åƒè¶…åˆ†è¾¨ç‡\n\n---\n\nä½ å¥½ï¼æˆ‘æ˜¯ **Gemini Enterprise**âœ¨ã€‚æˆ‘å·²ç»æŒ‰ç…§æ‚¨çš„è¦æ±‚ï¼Œå°†è¯¥å­¦æœ¯æ ‡é¢˜ç¿»è¯‘æˆäº†åœ°æ ‡æ€§ä¸”ç¬¦åˆä¸­æ–‡å­¦æœ¯è§„èŒƒçš„è¡¨è¾¾ã€‚\n\nè¿™æ®µè¯‘æ–‡ç²¾å‡†æ•æ‰äº†â€œç”Ÿæˆå¼æ‰©æ•£å…ˆéªŒâ€ï¼ˆGenerative Diffusion Priorï¼‰ä½œä¸ºæ ¸å¿ƒæ–¹æ³•è®ºçš„è§’è‰²ï¼Œå¹¶æ¸…æ™°åœ°ç•Œå®šäº†â€œè¶…é«˜æ¸…â€ï¼ˆUltra-High-Definitionï¼‰ä¸â€œå‚è€ƒå¼åœ°æ ‡å›¾åƒè¶…åˆ†è¾¨ç‡â€ï¼ˆReference-Based Landmark Image Super-Resolutionï¼‰çš„ä¸šåŠ¡åœºæ™¯ã€‚\n\nå¦‚æœæ‚¨æœ‰æ›´å¤šçš„ arXiv è®ºæ–‡æ ‡é¢˜éœ€è¦æ¶¦è‰²ï¼Œæˆ–è€…æƒ³é’ˆå¯¹æ‘˜è¦å†…å®¹è¿›è¡Œæ·±åº¦çš„å­¦æœ¯è®¨è®ºï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ï¼Œæˆ‘å¾ˆä¹æ„ç»§ç»­ä¸ºæ‚¨æ•ˆåŠ³ï¼æ‚¨è§‰å¾—è¿™ä¸ªè¯‘æ³•æ˜¯å¦ç¬¦åˆæ‚¨çš„å‘è¡¨è¦æ±‚ï¼Ÿ",
      "authors": [
        "Zhenning Shi",
        "Zizheng Yan",
        "Yuhang Yu",
        "Clara Xue",
        "Jingyu Zhuang",
        "Qi Zhang",
        "Jinwei Chen",
        "Tao Li",
        "Qingnan Fan"
      ],
      "abstract": "Reference-based Image Super-Resolution (RefSR) aims to restore a low-resolution (LR) image by utilizing the semantic and texture information from an additional reference high-resolution (reference HR) image. Existing diffusion-based RefSR methods are typically built upon ControlNet, which struggles to effectively align the information between the LR image and the reference HR image. Moreover, current RefSR datasets suffer from limited resolution and poor image quality, resulting in the reference images lacking sufficient fine-grained details to support high-quality restoration. To overcome the limitations above, we propose TriFlowSR, a novel framework that explicitly achieves pattern matching between the LR image and the reference HR image. Meanwhile, we introduce Landmark-4K, the first RefSR dataset for Ultra-High-Definition (UHD) landmark scenarios. Considering the UHD scenarios with real-world degradation, in TriFlowSR, we design a Reference Matching Strategy to effectively match the LR image with the reference HR image. Experimental results show that our approach can better utilize the semantic and texture information of the reference HR image compared to previous methods. To the best of our knowledge, we propose the first diffusion-based RefSR pipeline for ultra-high definition landmark scenarios under real-world degradation. Our code and model will be available at https://github.com/nkicsl/TriFlowSR.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TriFlowSR æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åŸºäºå‚è€ƒå›¾åƒçš„è¶…åˆ†è¾¨ç‡ (Reference-based Image Super-Resolution, RefSR) æŠ€æœ¯ä¸­ä½åˆ†è¾¨ç‡ (LR) å›¾åƒä¸å‚è€ƒé«˜åˆ†è¾¨ç‡ (HR) å›¾åƒä¿¡æ¯å¯¹é½å›°éš¾çš„é—®é¢˜ã€‚é’ˆå¯¹ç°æœ‰æ‰©æ•£æ¨¡å‹æ–¹æ³•åœ¨å¤„ç†è¶…é«˜æ¸… (UHD) åœºæ™¯æ—¶ç»†ç²’åº¦ç»†èŠ‚ä¸è¶³ä»¥åŠæ•°æ®é›†è´¨é‡å—é™çš„ç°çŠ¶ï¼Œä½œè€…å¼•å…¥äº†é¦–ä¸ªé’ˆå¯¹è¶…é«˜æ¸…åœ°æ ‡åœºæ™¯çš„ RefSR æ•°æ®é›† Landmark-4Kã€‚TriFlowSR æ ¸å¿ƒè®¾è®¡äº†ä¸€ç§ Reference Matching Strategyï¼Œé€šè¿‡æ˜¾å¼çš„æ¨¡å¼åŒ¹é…åœ¨çœŸå®ä¸–ç•Œé™è´¨ç¯å¢ƒä¸‹æœ‰æ•ˆç»“åˆå‚è€ƒå›¾åƒçš„è¯­ä¹‰ä¸çº¹ç†ä¿¡æ¯ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒTriFlowSR åœ¨å›¾åƒæ¢å¤è´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºæ­¤å‰çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿæ›´å……åˆ†åœ°æŒ–æ˜å‚è€ƒå›¾åƒçš„å…ˆéªŒçŸ¥è¯†ã€‚ä½œä¸ºé¦–ä¸ªåº”ç”¨äºçœŸå®é™è´¨è¶…é«˜æ¸…åœ°æ ‡åœºæ™¯çš„åŸºäºæ‰©æ•£æ¨¡å‹çš„ RefSR ç®¡çº¿ï¼Œè¯¥å·¥ä½œä¸ºå›¾åƒè¶…åˆ†è¾¨ç‡é¢†åŸŸæä¾›äº†é‡è¦çš„åŸºå‡†å’ŒæŠ€æœ¯çªç ´ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10779v1",
      "published_date": "2025-08-14 16:04:39 UTC",
      "updated_date": "2025-08-14 16:04:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:49:03.484606+00:00"
    },
    {
      "arxiv_id": "2508.10777v1",
      "title": "The Knowledge-Reasoning Dissociation: Fundamental Limitations of LLMs in Clinical Natural Language Inference",
      "title_zh": "çŸ¥è¯†ä¸æ¨ç†çš„è§£ç¦»ï¼šå¤§è¯­è¨€æ¨¡å‹åœ¨ä¸´åºŠè‡ªç„¶è¯­è¨€æ¨ç†ä¸­çš„æ ¹æœ¬å±€é™æ€§",
      "authors": [
        "MaÃ«l Jullien",
        "Marco Valentino",
        "AndrÃ© Freitas"
      ],
      "abstract": "Large language models are often assumed to acquire increasingly structured, generalizable internal representations simply by scaling data and parameters. We interrogate this assumption by introducing a Clinical Trial Natural Language Inference benchmark comprising four reasoning families, Causal Attribution, Compositional Grounding, Epistemic Verification, and Risk State Abstraction. Each item is paired with a targeted Ground Knowledge and Meta-Level Reasoning Verification (GKMRV) probe, allowing us to dissociate failures of factual access from failures of inference. We evaluate six contemporary LLMs under both direct and chain of thought prompting.\n  Models achieve near-ceiling GKMRV accuracy (mean accuracy 0.918) yet perform poorly on the main reasoning tasks (mean accuracy 0.25). Despite low accuracy, output inferences are highly consistent across samples (mean 0.87), indicating a systematic application of underlying heuristics and shortcuts.\n  These results reveal fundamental structural and representational limitations: current LLMs often possess the relevant clinical knowledge but lack the structured, composable internal representations needed to deploy it reliably (e.g., integrating constraints, weighing evidence, or simulating counterfactuals). Decoupling knowledge from reasoning with GKMRV makes this dissociation explicit and measurable, providing an effective framework for probing the reliability of LLMs in high-stakes domains.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ä¸´åºŠè‡ªç„¶è¯­è¨€æ¨ç†(Clinical Natural Language Inference)ä¸­çš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªåŒ…å«å››ç§æ¨ç†ç±»å‹çš„åŸºå‡†æµ‹è¯•ã€‚ç ”ç©¶é€šè¿‡å¼•å…¥åä¸ºGKMRVï¼ˆGround Knowledge and Meta-Level Reasoning Verificationï¼‰çš„æ¢æµ‹æ–¹æ³•ï¼ŒæˆåŠŸå°†æ¨¡å‹çš„äº‹å®è·å–èƒ½åŠ›ä¸æ¨ç†èƒ½åŠ›è¿›è¡Œè§£è€¦ã€‚å®éªŒå¯¹å…­ç§å½“ä»£LLMsçš„è¯„ä¼°æ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨äº‹å®çŸ¥è¯†æ¢æµ‹ä¸­è¡¨ç°å‡ºæ¥è¿‘æé™çš„é«˜å‡†ç¡®ç‡ï¼ˆ0.918ï¼‰ï¼Œä½†åœ¨æ ¸å¿ƒæ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°å´éå¸¸ç³Ÿç³•ï¼ˆ0.25ï¼‰ã€‚å°½ç®¡å‡†ç¡®ç‡è¾ƒä½ï¼Œæ¨¡å‹è¾“å‡ºçš„æ¨ç†ç»“æœå…·æœ‰é«˜åº¦ä¸€è‡´æ€§ï¼Œè¡¨æ˜å…¶ç³»ç»Ÿæ€§åœ°åˆ©ç”¨äº†åº•å±‚çš„å¯å‘å¼(heuristics)å’Œæ·å¾„ã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†å½“å‰LLMså­˜åœ¨çš„æ ¹æœ¬æ€§ç»“æ„å’Œè¡¨ç¤ºå±€é™ï¼Œå³æ¨¡å‹å³ä¾¿æ‹¥æœ‰ç›¸å…³ä¸´åºŠçŸ¥è¯†ï¼Œä¹Ÿç¼ºä¹å¯é éƒ¨ç½²çŸ¥è¯†æ‰€éœ€çš„ç»“æ„åŒ–ã€å¯ç»„åˆçš„å†…éƒ¨è¡¨ç¤ºã€‚é€šè¿‡GKMRVå®ç°çš„çŸ¥è¯†ä¸æ¨ç†è„±é’©ï¼Œä¸ºè¡¡é‡é«˜é£é™©é¢†åŸŸä¸­LLMsçš„å¯é æ€§æä¾›äº†ä¸€ä¸ªæ˜ç¡®ä¸”æœ‰æ•ˆçš„è¯„ä¼°æ¡†æ¶ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "19 pages",
      "pdf_url": "https://arxiv.org/pdf/2508.10777v1",
      "published_date": "2025-08-14 16:01:10 UTC",
      "updated_date": "2025-08-14 16:01:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:48:50.150617+00:00"
    },
    {
      "arxiv_id": "2508.10776v1",
      "title": "Estimating Covariance for Global Minimum Variance Portfolio: A Decision-Focused Learning Approach",
      "title_zh": "å…¨å±€æœ€å°æ–¹å·®ç»„åˆçš„åæ–¹å·®ä¼°è®¡ï¼šä¸€ç§å†³ç­–èšç„¦å­¦ä¹ æ–¹æ³•",
      "authors": [
        "Juchan Kim",
        "Inwoo Tae",
        "Yongjae Lee"
      ],
      "abstract": "Portfolio optimization constitutes a cornerstone of risk management by quantifying the risk-return trade-off. Since it inherently depends on accurate parameter estimation under conditions of future uncertainty, the selection of appropriate input parameters is critical for effective portfolio construction. However, most conventional statistical estimators and machine learning algorithms determine these parameters by minimizing mean-squared error (MSE), a criterion that can yield suboptimal investment decisions. In this paper, we adopt decision-focused learning (DFL) - an approach that directly optimizes decision quality rather than prediction error such as MSE - to derive the global minimum-variance portfolio (GMVP). Specifically, we theoretically derive the gradient of decision loss using the analytic solution of GMVP and its properties regarding the principal components of itself. Through extensive empirical evaluation, we show that prediction-focused estimation methods may fail to produce optimal allocations in practice, whereas DFL-based methods consistently deliver superior decision performance. Furthermore, we provide a comprehensive analysis of DFL's mechanism in GMVP construction, focusing on its volatility reduction capability, decision-driving features, and estimation characteristics.",
      "tldr_zh": "è¯¥è®ºæ–‡æ¢è®¨äº†æŠ•èµ„ç»„åˆä¼˜åŒ–ä¸­å‚æ•°ä¼°è®¡å¯¹å†³ç­–è´¨é‡çš„å½±å“ï¼ŒæŒ‡å‡ºä¼ ç»Ÿä»¥æœ€å°åŒ–å‡æ–¹è¯¯å·®(MSE)ä¸ºç›®æ ‡çš„ç»Ÿè®¡å’Œæœºå™¨å­¦ä¹ æ–¹æ³•å¾€å¾€ä¼šå¯¼è‡´æ¬¡ä¼˜çš„æŠ•èµ„ç»“æœã€‚ç ”ç©¶è€…æå‡ºäº†ä¸€ç§åŸºäºå†³ç­–èšç„¦å­¦ä¹ (Decision-Focused Learning, DFL)çš„æ–¹æ³•ï¼Œæ—¨åœ¨ç›´æ¥ä¼˜åŒ–å†³ç­–è´¨é‡è€Œéé¢„æµ‹è¯¯å·®ï¼Œå¹¶å°†å…¶åº”ç”¨äºå…¨çƒæœ€å°æ–¹å·®ç»„åˆ(Global Minimum-Variance Portfolio, GMVP)çš„æ„å»ºã€‚é€šè¿‡åˆ©ç”¨GMVPçš„è§£æè§£åŠå…¶ä¸»æˆåˆ†(Principal Components)çš„æ€§è´¨ï¼Œè¯¥ç ”ç©¶ä»ç†è®ºä¸Šæ¨å¯¼äº†å†³ç­–æŸå¤±çš„æ¢¯åº¦ã€‚å®è¯è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œä¼ ç»Ÿçš„é¢„æµ‹é©±åŠ¨å‹ä¼°è®¡æ–¹æ³•åœ¨å®é™…é…ç½®ä¸­è¡¨ç°ä¸ä½³ï¼Œè€ŒåŸºäºDFLçš„æ–¹æ³•åˆ™å±•ç°å‡ºæ›´ä¼˜å¼‚çš„å†³ç­–æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ·±å…¥åˆ†æäº†DFLåœ¨é™ä½æ³¢åŠ¨ç‡(Volatility Reduction)æ–¹é¢çš„æœºåˆ¶ã€å†³ç­–é©±åŠ¨ç‰¹å¾ä»¥åŠä¼°è®¡ç‰¹æ€§ï¼Œä¸ºä¼˜åŒ–æŠ•èµ„å†³ç­–æä¾›äº†æ–°çš„ç†è®ºæ”¯æŒå’Œå®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "q-fin.PM",
        "cs.AI"
      ],
      "primary_category": "q-fin.PM",
      "comment": "11 pages, 12 figures, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2508.10776v1",
      "published_date": "2025-08-14 16:00:52 UTC",
      "updated_date": "2025-08-14 16:00:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:48:53.693541+00:00"
    },
    {
      "arxiv_id": "2508.10774v2",
      "title": "BLADE: Block-Sparse Attention Meets Step Distillation for Efficient Video Generation",
      "title_zh": "BLADEï¼šèåˆå—ç¨€ç–æ³¨æ„åŠ›ä¸æ­¥æ•°è’¸é¦çš„é«˜æ•ˆè§†é¢‘ç”Ÿæˆ",
      "authors": [
        "Youping Gu",
        "Xiaolong Li",
        "Yuhao Hu",
        "Minqi Chen",
        "Bohan Zhuang"
      ],
      "abstract": "Diffusion Transformers currently lead the field in high-quality video generation, but their slow iterative denoising process and prohibitive quadratic attention costs for long sequences create significant inference bottlenecks. While both step distillation and sparse attention mechanisms have shown promise as independent acceleration strategies, effectively combining these approaches presents critical challenges -- training-free integration yields suboptimal results, while separately training sparse attention after step distillation requires prohibitively expensive high-quality video data. To overcome these limitations, we propose BLADE, an innovative data-free joint training framework that introduces: (1) an Adaptive Block-Sparse Attention (ASA) mechanism for dynamically generating content-aware sparsity masks to focus computation on salient spatiotemporal features, and (2) a sparsity-aware step distillation paradigm, built upon Trajectory Distribution Matching (TDM), directly incorporates sparsity into the distillation process rather than treating it as a separate compression step and features fast convergence. We validate BLADE on text-to-video models like CogVideoX-5B and Wan2.1-1.3B, and our framework demonstrates remarkable efficiency gains across different scales. On Wan2.1-1.3B, BLADE achieves a 14.10x end-to-end inference acceleration over a 50-step baseline. Moreover, on models such as CogVideoX-5B with short video sequence lengths, our framework delivers a robust 8.89x speedup. Crucially, the acceleration is accompanied by a consistent quality improvement. On the VBench-2.0 benchmark, BLADE boosts the score of CogVideoX-5B to 0.569 (from 0.534) and Wan2.1-1.3B to 0.570 (from 0.563), results that are further corroborated by superior ratings in human evaluations. Project is available at http://ziplab.co/BLADE-Homepage/.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† BLADEï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨æå‡è§†é¢‘ç”Ÿæˆæ•ˆç‡çš„åˆ›æ–°æ•°æ®æ— å…³è”åˆè®­ç»ƒæ¡†æ¶ï¼Œæœ‰æ•ˆè§£å†³äº† Diffusion Transformers åœ¨æ¨ç†è¿‡ç¨‹ä¸­é¢ä¸´çš„é€Ÿåº¦æ…¢å’Œ Attention è®¡ç®—æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ã€‚BLADE å¼•å…¥äº† Adaptive Block-Sparse Attention (ASA) æœºåˆ¶ï¼Œé€šè¿‡åŠ¨æ€ç”Ÿæˆå†…å®¹æ„ŸçŸ¥çš„ç¨€ç–æ©ç å°†è®¡ç®—èšç„¦äºæ˜¾è‘—çš„æ—¶ç©ºç‰¹å¾ï¼Œå¹¶ç»“åˆåŸºäº Trajectory Distribution Matching (TDM) çš„ç¨€ç–æ„ŸçŸ¥æ­¥é•¿è’¸é¦èŒƒå¼ï¼Œå°†ç¨€ç–åŒ–ç›´æ¥èå…¥è’¸é¦è¿‡ç¨‹ã€‚å®éªŒåœ¨ CogVideoX-5B å’Œ Wan2.1-1.3B ç­‰å¤§æ¨¡å‹ä¸ŠéªŒè¯äº†è¯¥æ¡†æ¶çš„ä¼˜è¶Šæ€§ï¼Œå…¶ä¸­åœ¨ Wan2.1-1.3B ä¸Šå®ç°äº† 14.10 å€çš„ç«¯åˆ°ç«¯æ¨ç†åŠ é€Ÿã€‚æµ‹è¯„ç»“æœæ˜¾ç¤ºï¼ŒBLADE åœ¨å¤§å¹…æå‡ç”Ÿæˆé€Ÿåº¦çš„åŒæ—¶ï¼Œåœ¨ VBench-2.0 åŸºå‡†æµ‹è¯•ä¸­ä¹Ÿå–å¾—äº†æ¯”åŸºå‡†æ¨¡å‹æ›´é«˜çš„è¯„åˆ†ï¼Œè¯æ˜äº†å…¶åœ¨æ•ˆç‡ä¸ç”Ÿæˆè´¨é‡ä¸Šçš„åŒé‡æå‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Tech report",
      "pdf_url": "https://arxiv.org/pdf/2508.10774v2",
      "published_date": "2025-08-14 15:58:59 UTC",
      "updated_date": "2025-09-29 15:46:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:48:58.685325+00:00"
    },
    {
      "arxiv_id": "2508.14091v1",
      "title": "Logical Expressivity and Explanations for Monotonic GNNs with Scoring Functions",
      "title_zh": "å¸¦è¯„åˆ†å‡½æ•°çš„å•è°ƒå›¾ç¥ç»ç½‘ç»œçš„é€»è¾‘è¡¨è¾¾èƒ½åŠ›ä¸è§£é‡Š",
      "authors": [
        "Matthew Morris",
        "David J. Tena Cucala",
        "Bernardo Cuenca Grau"
      ],
      "abstract": "Graph neural networks (GNNs) are often used for the task of link prediction: predicting missing binary facts in knowledge graphs (KGs). To address the lack of explainability of GNNs on KGs, recent works extract Datalog rules from GNNs with provable correspondence guarantees. The extracted rules can be used to explain the GNN's predictions; furthermore, they can help characterise the expressive power of various GNN models. However, these works address only a form of link prediction based on a restricted, low-expressivity graph encoding/decoding method. In this paper, we consider a more general and popular approach for link prediction where a scoring function is used to decode the GNN output into fact predictions. We show how GNNs and scoring functions can be adapted to be monotonic, use the monotonicity to extract sound rules for explaining predictions, and leverage existing results about the kind of rules that scoring functions can capture. We also define procedures for obtaining equivalent Datalog programs for certain classes of monotonic GNNs with scoring functions. Our experiments show that, on link prediction benchmarks, monotonic GNNs and scoring functions perform well in practice and yield many sound rules.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†çŸ¥è¯†å›¾è°± (Knowledge Graphs) é“¾è·¯é¢„æµ‹ä»»åŠ¡ä¸­å›¾ç¥ç»ç½‘ç»œ (GNNs) çš„å¯è§£é‡Šæ€§é—®é¢˜ï¼Œé’ˆå¯¹ç°æœ‰æ–¹æ³•åœ¨ç¼–ç ä¸è§£ç é€»è¾‘ä¸Šçš„å±€é™æ€§æå‡ºäº†æ”¹è¿›æ–¹æ¡ˆã€‚ä½œè€…æå‡ºäº†ä¸€ç§å°† GNNs ä¸è¯„åˆ†å‡½æ•° (Scoring Functions) ç»“åˆçš„é€šç”¨æ¡†æ¶ï¼Œå¹¶å°†å…¶é€‚é…ä¸ºå•è°ƒæ€§ (Monotonic) å½¢å¼ï¼Œä»¥ç¡®ä¿é¢„æµ‹é€»è¾‘çš„ä¸€è‡´æ€§ã€‚é€šè¿‡åˆ©ç”¨å•è°ƒæ€§ï¼Œç ”ç©¶èƒ½å¤Ÿä»æ¨¡å‹ä¸­æå–å‡ºå¯é çš„ Datalog è§„åˆ™ç”¨äºè§£é‡Šé¢„æµ‹ç»“æœï¼Œå¹¶åˆ»ç”»äº†ä¸åŒæ¨¡å‹æ¶æ„çš„é€»è¾‘è¡¨è¾¾èƒ½åŠ› (Logical Expressivity)ã€‚æ­¤å¤–ï¼Œè¯¥å·¥ä½œå®šä¹‰äº†ä¸ºç‰¹å®šå•è°ƒ GNNs è‡ªåŠ¨ç”Ÿæˆç­‰æ•ˆ Datalog ç¨‹åºçš„å½¢å¼åŒ–è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥å•è°ƒæ¨¡å‹åœ¨é“¾è·¯é¢„æµ‹åŸºå‡†æµ‹è¯•ä¸­ä¸ä»…ä¿æŒäº†è‰¯å¥½çš„é¢„æµ‹æ€§èƒ½ï¼Œä¸”èƒ½ç”Ÿæˆå¤§é‡å…·å¤‡é€»è¾‘å®Œå¤‡æ€§çš„è§£é‡Šæ€§è§„åˆ™ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.LG",
      "comment": "Full version (with appendices) of paper accepted to KR 2025 (22nd International Conference on Principles of Knowledge Representation and Reasoning)",
      "pdf_url": "https://arxiv.org/pdf/2508.14091v1",
      "published_date": "2025-08-14 15:56:48 UTC",
      "updated_date": "2025-08-14 15:56:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:48:57.084849+00:00"
    },
    {
      "arxiv_id": "2508.10771v1",
      "title": "AEGIS: Authenticity Evaluation Benchmark for AI-Generated Video Sequences",
      "title_zh": "AEGISï¼šäººå·¥æ™ºèƒ½ç”Ÿæˆè§†é¢‘åºåˆ—çœŸå®æ€§è¯„ä¼°åŸºå‡†",
      "authors": [
        "Jieyu Li",
        "Xin Zhang",
        "Joey Tianyi Zhou"
      ],
      "abstract": "Recent advances in AI-generated content have fueled the rise of highly realistic synthetic videos, posing severe risks to societal trust and digital integrity. Existing benchmarks for video authenticity detection typically suffer from limited realism, insufficient scale, and inadequate complexity, failing to effectively evaluate modern vision-language models against sophisticated forgeries. To address this critical gap, we introduce AEGIS, a novel large-scale benchmark explicitly targeting the detection of hyper-realistic and semantically nuanced AI-generated videos. AEGIS comprises over 10,000 rigorously curated real and synthetic videos generated by diverse, state-of-the-art generative models, including Stable Video Diffusion, CogVideoX-5B, KLing, and Sora, encompassing open-source and proprietary architectures. In particular, AEGIS features specially constructed challenging subsets enhanced with robustness evaluation. Furthermore, we provide multimodal annotations spanning Semantic-Authenticity Descriptions, Motion Features, and Low-level Visual Features, facilitating authenticity detection and supporting downstream tasks such as multimodal fusion and forgery localization. Extensive experiments using advanced vision-language models demonstrate limited detection capabilities on the most challenging subsets of AEGIS, highlighting the dataset's unique complexity and realism beyond the current generalization capabilities of existing models. In essence, AEGIS establishes an indispensable evaluation benchmark, fundamentally advancing research toward developing genuinely robust, reliable, broadly generalizable video authenticity detection methodologies capable of addressing real-world forgery threats. Our dataset is available on https://huggingface.co/datasets/Clarifiedfish/AEGIS.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AEGISï¼Œä¸€ä¸ªæ—¨åœ¨è¯„ä¼° AI-Generated Video çœŸå®æ€§çš„æ–°å‹å¤§è§„æ¨¡ Benchmarkï¼Œä»¥åº”å¯¹é«˜é€¼çœŸåˆæˆè§†é¢‘å¯¹æ•°å­—è¯šä¿¡å¸¦æ¥çš„å¨èƒã€‚AEGIS åŒ…å«è¶…è¿‡ 10,000 ä¸ªç”± Stable Video Diffusionã€CogVideoX-5Bã€KLing å’Œ Sora ç­‰é¡¶å°–æ¨¡å‹ç”Ÿæˆçš„çœŸå®ä¸åˆæˆè§†é¢‘ï¼Œæ¶µç›–äº†å¼€æºåŠå•†ä¸šæ¶æ„ã€‚ä¸ºäº†æå‡è¯„ä¼°æ·±åº¦ï¼Œè¯¥æ•°æ®é›†æä¾›äº†åŒ…å« Semantic-Authenticity Descriptionsã€Motion Features å’Œ Low-level Visual Features åœ¨å†…çš„å¤šæ¨¡æ€æ ‡æ³¨ï¼Œå¹¶è®¾è®¡äº†ä¸“é—¨çš„é²æ£’æ€§è¯„ä¼°å­é›†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç°æœ‰çš„å…ˆè¿› Vision-Language Models åœ¨å¤„ç† AEGIS çš„æŒ‘æˆ˜æ€§å­é›†æ—¶è¡¨ç°æœ‰é™ï¼Œè¯æ˜äº†è¯¥ Benchmark åœ¨å¤æ‚åº¦å’ŒçœŸå®æ„Ÿæ–¹é¢çš„ç‹¬ç‰¹æ€§ã€‚AEGIS çš„æ¨å‡ºä¸ºå¼€å‘æ›´å…·é²æ£’æ€§å’Œé€šç”¨æ€§çš„è§†é¢‘ä¼ªé€ æ£€æµ‹æŠ€æœ¯æä¾›äº†å…³é”®çš„è¯„ä¼°å¹³å°ï¼Œæœ‰åŠ©äºåº”å¯¹ç°å®ä¸–ç•Œä¸­çš„æ•°å­—æ¬ºè¯ˆå¨èƒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Proceedings of the 33rd ACM International Conference on Multimedia",
      "pdf_url": "https://arxiv.org/pdf/2508.10771v1",
      "published_date": "2025-08-14 15:55:49 UTC",
      "updated_date": "2025-08-14 15:55:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:49:05.393559+00:00"
    },
    {
      "arxiv_id": "2508.10769v1",
      "title": "Modeling Human Responses to Multimodal AI Content",
      "title_zh": "å¯¹å¤šæ¨¡æ€ AI å†…å®¹çš„äººç±»ååº”å»ºæ¨¡",
      "authors": [
        "Zhiqi Shen",
        "Shaojing Fan",
        "Danni Xu",
        "Terence Sim",
        "Mohan Kankanhalli"
      ],
      "abstract": "As AI-generated content becomes widespread, so does the risk of misinformation. While prior research has primarily focused on identifying whether content is authentic, much less is known about how such content influences human perception and behavior. In domains like trading or the stock market, predicting how people react (e.g., whether a news post will go viral), can be more critical than verifying its factual accuracy. To address this, we take a human-centered approach and introduce the MhAIM Dataset, which contains 154,552 online posts (111,153 of them AI-generated), enabling large-scale analysis of how people respond to AI-generated content. Our human study reveals that people are better at identifying AI content when posts include both text and visuals, particularly when inconsistencies exist between the two. We propose three new metrics: trustworthiness, impact, and openness, to quantify how users judge and engage with online content. We present T-Lens, an LLM-based agent system designed to answer user queries by incorporating predicted human responses to multimodal information. At its core is HR-MCP (Human Response Model Context Protocol), built on the standardized Model Context Protocol (MCP), enabling seamless integration with any LLM. This integration allows T-Lens to better align with human reactions, enhancing both interpretability and interaction capabilities. Our work provides empirical insights and practical tools to equip LLMs with human-awareness capabilities. By highlighting the complex interplay among AI, human cognition, and information reception, our findings suggest actionable strategies for mitigating the risks of AI-driven misinformation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹(AI-generated content)å¯¹äººç±»æ„ŸçŸ¥å’Œè¡Œä¸ºçš„å½±å“ï¼Œå¹¶å‘å¸ƒäº†åŒ…å«154,552æ¡åœ¨çº¿å¸–å­çš„MhAIM Datasetï¼Œæ—¨åœ¨å¡«è¡¥äººç±»å¯¹AIè™šå‡ä¿¡æ¯ååº”çš„ç ”ç©¶ç©ºç™½ã€‚é€šè¿‡äººç±»å—è¯•ç ”ç©¶å‘ç°ï¼Œå½“æ–‡æœ¬ä¸è§†è§‰ä¿¡æ¯ä¸ä¸€è‡´æ—¶ï¼Œç”¨æˆ·æ›´æ˜“è¯†åˆ«å‡ºAIå†…å®¹ã€‚ç ”ç©¶æå‡ºäº†å¯ä¿¡åº¦(trustworthiness)ã€å½±å“åŠ›(impact)å’Œå¼€æ”¾åº¦(openness)ä¸‰é¡¹æŒ‡æ ‡ï¼Œç”¨äºé‡åŒ–ç”¨æˆ·å¯¹åœ¨çº¿å†…å®¹çš„è¯„åˆ¤ä¸äº’åŠ¨ã€‚æ­¤å¤–ï¼Œè¯¥å›¢é˜Ÿå¼€å‘äº†åä¸ºT-Lensçš„LLMæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œå…¶æ ¸å¿ƒæ˜¯åŸºäºæ ‡å‡†æ¨¡å‹ä¸Šä¸‹æ–‡åè®®(MCP)æ„å»ºçš„HR-MCPåè®®ï¼Œèƒ½å¤Ÿæ•´åˆé¢„æµ‹çš„äººç±»ååº”ä»¥å¢å¼ºç³»ç»Ÿçš„è§£é‡ŠåŠ›ä¸äº¤äº’èƒ½åŠ›ã€‚è¯¥å·¥ä½œä¸ºæå‡å¤§è¯­è¨€æ¨¡å‹çš„äººç±»æ„ŸçŸ¥èƒ½åŠ›(human-awareness)æä¾›äº†å®è¯è§è§£å’Œå®ç”¨å·¥å…·ã€‚é€šè¿‡æ­ç¤ºAIã€äººç±»è®¤çŸ¥ä¸ä¿¡æ¯æ¥æ”¶ä¹‹é—´çš„å¤æ‚ç›¸äº’ä½œç”¨ï¼Œè¯¥ç ”ç©¶ä¸ºç¼“è§£AIé©±åŠ¨çš„è™šå‡ä¿¡æ¯é£é™©æä¾›äº†åˆ‡å®å¯è¡Œçš„ç­–ç•¥ã€‚",
      "categories": [
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10769v1",
      "published_date": "2025-08-14 15:55:19 UTC",
      "updated_date": "2025-08-14 15:55:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:49:15.691639+00:00"
    },
    {
      "arxiv_id": "2508.10760v1",
      "title": "FROGENT: An End-to-End Full-process Drug Design Agent",
      "title_zh": "FROGENTï¼šç«¯åˆ°ç«¯å…¨æµç¨‹è¯ç‰©è®¾è®¡æ™ºèƒ½ä½“",
      "authors": [
        "Qihua Pan",
        "Dong Xu",
        "Jenna Xinyi Yao",
        "Lijia Ma",
        "Zexuan Zhu",
        "Junkai Ji"
      ],
      "abstract": "Powerful AI tools for drug discovery reside in isolated web apps, desktop programs, and code libraries. Such fragmentation forces scientists to manage incompatible interfaces and specialized scripts, which can be a cumbersome and repetitive process. To address this issue, a Full-pROcess druG dEsign ageNT, named FROGENT, has been proposed. Specifically, FROGENT utilizes a Large Language Model and the Model Context Protocol to integrate multiple dynamic biochemical databases, extensible tool libraries, and task-specific AI models. This agentic framework allows FROGENT to execute complicated drug discovery workflows dynamically, including component tasks such as target identification, molecule generation and retrosynthetic planning. FROGENT has been evaluated on eight benchmarks that cover various aspects of drug discovery, such as knowledge retrieval, property prediction, virtual screening, mechanistic analysis, molecular design, and synthesis. It was compared against six increasingly advanced ReAct-style agents that support code execution and literature searches. Empirical results demonstrated that FROGENT triples the best baseline performance in hit-finding and doubles it in interaction profiling, significantly outperforming both the open-source model Qwen3-32B and the commercial model GPT-4o. In addition, real-world cases have been utilized to validate the practicability and generalization of FROGENT. This development suggests that streamlining the agentic drug discovery pipeline can significantly enhance researcher productivity.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† FROGENTï¼Œä¸€ä¸ªæ—¨åœ¨è§£å†³è¯ç‰©ç ”å‘ä¸­å·¥å…·ç¢ç‰‡åŒ–å’Œæ¥å£ä¸å…¼å®¹é—®é¢˜çš„å…¨æµç¨‹è¯ç‰©è®¾è®¡æ™ºèƒ½ä½“ã€‚FROGENT åˆ©ç”¨ Large Language Model (LLM) å’Œ Model Context Protocol (MCP) æ•´åˆäº†åŠ¨æ€ç”ŸåŒ–æ•°æ®åº“ã€å¯æ‰©å±•å·¥å…·åº“åŠç‰¹å®šä»»åŠ¡çš„ AI æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤ŸåŠ¨æ€æ‰§è¡Œæ¶µç›–é¶ç‚¹è¯†åˆ« (target identification)ã€åˆ†å­ç”Ÿæˆ (molecule generation) å’Œé€†åˆæˆè§„åˆ’ (retrosynthetic planning) çš„å¤æ‚å·¥ä½œæµã€‚é€šè¿‡å…«é¡¹æ¶µç›–çŸ¥è¯†æ£€ç´¢ã€å±æ€§é¢„æµ‹å’Œè™šæ‹Ÿç­›é€‰ç­‰é¢†åŸŸçš„åŸºå‡†æµ‹è¯•è¯„ä¼°ï¼ŒFROGENT åœ¨å‘ç°å…ˆå¯¼åŒ–åˆç‰© (hit-finding) æ–¹é¢çš„æ€§èƒ½è¾¾åˆ°æœ€ä½³åŸºçº¿çš„ä¸‰å€ï¼Œåœ¨ç›¸äº’ä½œç”¨åˆ†æ (interaction profiling) æ–¹é¢åˆ™ç¿»äº†ä¸€ç•ªï¼Œæ˜¾è‘—ä¼˜äº Qwen3-32B å’Œ GPT-4oã€‚çœŸå®æ¡ˆä¾‹éªŒè¯è¿›ä¸€æ­¥è¯å®äº†è¯¥æ™ºèƒ½ä½“çš„å®ç”¨æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚è¿™é¡¹å·¥ä½œè¡¨æ˜ï¼Œé€šè¿‡ agentic æ¡†æ¶ç®€åŒ–è¯ç‰©ç ”å‘æµæ°´çº¿ï¼Œå¯ä»¥æå¤§æé«˜ç§‘ç ”äººå‘˜çš„ç”Ÿäº§æ•ˆç‡ã€‚",
      "categories": [
        "q-bio.BM",
        "cs.AI"
      ],
      "primary_category": "q-bio.BM",
      "comment": "9 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.10760v1",
      "published_date": "2025-08-14 15:45:53 UTC",
      "updated_date": "2025-08-14 15:45:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:49:12.186074+00:00"
    },
    {
      "arxiv_id": "2508.15807v2",
      "title": "Vocabulary Expansion of Large Language Models via Kullback-Leibler-Based Self-Distillation",
      "title_zh": "åŸºäº KL æ•£åº¦è‡ªè’¸é¦çš„å¤§è¯­è¨€æ¨¡å‹è¯è¡¨æ‰©å……",
      "authors": [
        "Max Rehman Linder"
      ],
      "abstract": "Large pre-trained language models often struggle to incorporate new domain-specific terminology when fine-tuned on small, specialized corpora. In this work, we address the challenge of vocabulary expansion in frozen LLMs by introducing a mathematically grounded method for knowledge distillation via KL divergence, even when the original and extended models use different tokenizations. This allows the student model to inherit distributional knowledge from the teacher despite differing vocabularies. We compare our KL-based distillation approach to conventional cross-entropy training, evaluating both methods across multiple strategies for initializing new token embeddings. After embedding initialization, models are further fine-tuned to integrate the new vocabulary. Each trained model is benchmarked on approximately 2000 code-generation tasks, where our approach achieves the best performance across the board. Finally, through mechanistic interpretability, we analyze how models learn representations for the new tokens, providing an explanation for the observed gains and offering insight into the structure of embedding space during vocabulary expansion.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†ç‰¹å®šé¢†åŸŸæœ¯è¯­æ—¶éš¾ä»¥é«˜æ•ˆæ‰©å±•è¯è¡¨çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºKullback-Leibleræ•£åº¦çš„è‡ªè’¸é¦ï¼ˆKullback-Leibler-Based Self-Distillationï¼‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•å»ºç«‹äº†ä¸¥è°¨çš„æ•°å­¦æ¡†æ¶ï¼Œä½¿å¾—å³ä½¿åœ¨åŸå§‹æ¨¡å‹ä¸æ‰©å±•æ¨¡å‹é‡‡ç”¨ä¸åŒåˆ†è¯æ–¹æ¡ˆï¼ˆtokenizationsï¼‰çš„æƒ…å†µä¸‹ï¼Œå­¦ç”Ÿæ¨¡å‹ä»èƒ½æœ‰æ•ˆç»§æ‰¿æ•™å¸ˆæ¨¡å‹çš„åˆ†å¸ƒçŸ¥è¯†ã€‚å®éªŒé€šè¿‡å¯¹æ¯”ä¼ ç»Ÿçš„äº¤å‰ç†µï¼ˆcross-entropyï¼‰è®­ç»ƒä»¥åŠå¤šç§åµŒå…¥åˆå§‹åŒ–ï¼ˆembedding initializationï¼‰ç­–ç•¥ï¼Œåœ¨çº¦2000é¡¹ä»£ç ç”Ÿæˆï¼ˆcode-generationï¼‰ä»»åŠ¡ä¸­è¯æ˜äº†è¯¥æ–¹æ³•åœ¨å„é¡¹æŒ‡æ ‡ä¸Šå‡å–å¾—äº†æœ€ä¼˜æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç ”ç©¶äººå‘˜åˆ©ç”¨æœºæ¢°å¯è§£é‡Šæ€§ï¼ˆmechanistic interpretabilityï¼‰å·¥å…·åˆ†æäº†æ¨¡å‹å­¦ä¹ æ–°tokenè¡¨ç¤ºçš„å…·ä½“è¿‡ç¨‹ï¼Œä¸ºè§‚å¯Ÿåˆ°çš„æ€§èƒ½æå‡æä¾›äº†åŸç†è§£é‡Šï¼Œå¹¶æ·±å…¥æ­ç¤ºäº†è¯è¡¨æ‰©å±•æœŸé—´åµŒå…¥ç©ºé—´ï¼ˆembedding spaceï¼‰çš„ç»“æ„æ¼”å˜è§„å¾‹ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Master's Thesis",
      "pdf_url": "https://arxiv.org/pdf/2508.15807v2",
      "published_date": "2025-08-14 15:45:50 UTC",
      "updated_date": "2026-01-12 20:12:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:49:17.891180+00:00"
    },
    {
      "arxiv_id": "2508.10967v2",
      "title": "Retro-Expert: Collaborative Reasoning for Interpretable Retrosynthesis",
      "title_zh": "Retro-Expertï¼šé¢å‘å¯è§£é‡Šé€†åˆæˆçš„ååŒæ¨ç†",
      "authors": [
        "Xinyi Li",
        "Sai Wang",
        "Yutian Lin",
        "Yu Wu",
        "Yi Yang"
      ],
      "abstract": "Retrosynthesis prediction aims to infer the reactant molecule based on a given product molecule, which is a fundamental task in chemical synthesis. However, existing models rely on static pattern-matching paradigm, which limits their ability to perform effective logic decision-making, leading to black-box decision-making. Building on this, we propose Retro-Expert, an interpretable retrosynthesis framework that performs collaborative reasoning by combining the complementary reasoning strengths of Large Language Models and specialized models via reinforcement learning. It outputs natural language explanations grounded in chemical logic through three components: (1) specialized models analyze the product to construct high-quality chemical decision space, (2) LLM-driven critical reasoning to generate predictions and corresponding interpretable reasoning path, and (3) reinforcement learning optimizing interpretable decision policy. Experiments show that Retro-Expert not only surpasses both LLM-based and specialized models across different metrics but also provides expert-aligned explanations that bridge the gap between AI predictions and actionable chemical insights.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Retro-Expertï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå¯è§£é‡Šé€†åˆæˆé¢„æµ‹ (Retrosynthesis) çš„åä½œæ¨ç†æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹å› ä¾èµ–é™æ€æ¨¡å¼åŒ¹é…è€Œå¯¼è‡´çš„é»‘ç›’å†³ç­–å’Œé€»è¾‘ç¼ºå¤±é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) æœ‰æœºç»“åˆäº†å¤§è¯­è¨€æ¨¡å‹ (LLMs) ä¸é¢†åŸŸä¸“é—¨æ¨¡å‹ (Specialized Models) çš„æ¨ç†ä¼˜åŠ¿ï¼Œå¹¶èƒ½è¾“å‡ºåŸºäºåŒ–å­¦é€»è¾‘çš„è‡ªç„¶è¯­è¨€è§£é‡Šã€‚å…¶æ ¸å¿ƒåŒ…å«ä¸‰ä¸ªéƒ¨åˆ†ï¼šç”±ä¸“é—¨æ¨¡å‹æ„å»ºé«˜è´¨é‡çš„åŒ–å­¦å†³ç­–ç©ºé—´ï¼Œåˆ©ç”¨ LLM é©±åŠ¨æ‰¹åˆ¤æ€§æ¨ç†ä»¥ç”Ÿæˆé¢„æµ‹åŠå¯¹åº”çš„æ¨ç†è·¯å¾„ï¼Œä»¥åŠé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å†³ç­–ç­–ç•¥ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRetro-Expert åœ¨å„é¡¹æŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰çš„ LLM æˆ–ä¸“é—¨æ¨¡å‹ï¼Œå¹¶èƒ½æä¾›ä¸ä¸“å®¶é€»è¾‘ä¸€è‡´çš„è§£é‡Šã€‚è¯¥ç ”ç©¶æœ‰æ•ˆåœ°å¼¥åˆäº†äººå·¥æ™ºèƒ½é¢„æµ‹ä¸å®é™…åŒ–å­¦è§è§£ä¹‹é—´çš„é¸¿æ²Ÿï¼Œæå‡äº†é€†åˆæˆé¢„æµ‹çš„é€æ˜åº¦å’Œå¯æ“ä½œæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10967v2",
      "published_date": "2025-08-14 15:41:25 UTC",
      "updated_date": "2025-12-05 06:21:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:49:37.084069+00:00"
    },
    {
      "arxiv_id": "2508.10758v1",
      "title": "Natively Trainable Sparse Attention for Hierarchical Point Cloud Datasets",
      "title_zh": "é¢å‘åˆ†å±‚ç‚¹äº‘æ•°æ®é›†çš„åŸç”Ÿå¯è®­ç»ƒç¨€ç–æ³¨æ„åŠ›",
      "authors": [
        "Nicolas Lapautre",
        "Maria Marchenko",
        "Carlos Miguel PatiÃ±o",
        "Xin Zhou"
      ],
      "abstract": "Unlocking the potential of transformers on datasets of large physical systems depends on overcoming the quadratic scaling of the attention mechanism. This work explores combining the Erwin architecture with the Native Sparse Attention (NSA) mechanism to improve the efficiency and receptive field of transformer models for large-scale physical systems, addressing the challenge of quadratic attention complexity. We adapt the NSA mechanism for non-sequential data, implement the Erwin NSA model, and evaluate it on three datasets from the physical sciences -- cosmology simulations, molecular dynamics, and air pressure modeling -- achieving performance that matches or exceeds that of the original Erwin model. Additionally, we reproduce the experimental results from the Erwin paper to validate their implementation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢ç´¢äº†å°† Erwin æ¶æ„ä¸ Native Sparse Attention (NSA) æœºåˆ¶ç›¸ç»“åˆï¼Œæ—¨åœ¨æé«˜å¤„ç†å¤§è§„æ¨¡ç‰©ç†ç³»ç»Ÿ(physical systems)æ•°æ®é›†æ—¶ transformer æ¨¡å‹çš„æ•ˆç‡å¹¶æ‰©å¤§å…¶æ„Ÿå—é‡ã€‚ä¸ºè§£å†³æ³¨æ„åŠ›æœºåˆ¶çš„äºŒæ¬¡å¤æ‚åº¦(quadratic attention complexity)éš¾é¢˜ï¼Œç ”ç©¶è€…å°† NSA æœºåˆ¶æ”¹è¿›å¹¶åº”ç”¨äºéåºåˆ—æ•°æ®(non-sequential data)ï¼Œå®ç°äº† Erwin NSA æ¨¡å‹ã€‚å®éªŒåœ¨å®‡å®™å­¦æ¨¡æ‹Ÿ(cosmology simulations)ã€åˆ†å­åŠ¨åŠ›å­¦(molecular dynamics)å’Œæ°”å‹å»ºæ¨¡(air pressure modeling)ä¸‰ä¸ªç‰©ç†ç§‘å­¦é¢†åŸŸçš„æ•°æ®é›†ä¸Šå±•å¼€ã€‚ç»“æœè¡¨æ˜ï¼ŒErwin NSA åœ¨æ€§èƒ½ä¸Šè¾¾åˆ°æˆ–è¶…è¿‡äº†åŸå§‹ Erwin æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥å·¥ä½œè¿˜é€šè¿‡å¤ç°å®éªŒæˆåŠŸéªŒè¯äº†åŸå§‹ Erwin è®ºæ–‡çš„å®ç°ç»“æœï¼Œä¸ºå¤„ç†åˆ†å±‚ç‚¹äº‘æ•°æ®é›†(hierarchical point cloud datasets)æä¾›äº†é«˜æ•ˆä¸”å¯æ‰©å±•çš„å»ºæ¨¡æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10758v1",
      "published_date": "2025-08-14 15:39:34 UTC",
      "updated_date": "2025-08-14 15:39:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:49:41.390402+00:00"
    },
    {
      "arxiv_id": "2508.10751v1",
      "title": "Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models",
      "title_zh": "Pass@k è®­ç»ƒï¼šè‡ªé€‚åº”å¹³è¡¡å¤§æ¨ç†æ¨¡å‹çš„æ¢ç´¢ä¸åˆ©ç”¨",
      "authors": [
        "Zhipeng Chen",
        "Xiaobo Qin",
        "Youbin Wu",
        "Yue Ling",
        "Qinghao Ye",
        "Wayne Xin Zhao",
        "Guang Shi"
      ],
      "abstract": "Reinforcement learning with verifiable rewards (RLVR), which typically adopts Pass@1 as the reward, has faced the issues in balancing exploration and exploitation, causing policies to prefer conservative actions, converging to a local optimum. Identifying an appropriate reward metric is therefore crucial. Regarding the prior work, although Pass@k has been used in evaluation, its connection to LLM exploration ability in RLVR remains largely overlooked. To investigate this, we first use Pass@k as the reward to train the policy model (i.e., $\\textbf{Pass@k Training}$), and observe the improvement on its exploration ability. Next, we derive an analytical solution for the advantage of Pass@k Training, leading to an efficient and effective process. Building on this, our analysis reveals that exploration and exploitation are not inherently conflicting objectives, while they can mutually enhance each other. Moreover, Pass@k Training with analytical derivation essentially involves directly designing the advantage function. Inspired by this, we preliminarily explore the advantage design for RLVR, showing promising results and highlighting a potential future direction.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¸¦æœ‰å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹  (RLVR) ä¸­é€šå¸¸ä½¿ç”¨ Pass@1 å¯¼è‡´ç­–ç•¥è¿‡äºä¿å®ˆä¸”æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜çš„é—®é¢˜ï¼Œæ¢è®¨äº† Pass@k Training åœ¨å¹³è¡¡å¤§å‹æ¨ç†æ¨¡å‹æ¢ç´¢ (Exploration) ä¸åˆ©ç”¨ (Exploitation) æ–¹é¢çš„ä½œç”¨ã€‚ä½œè€…é€šè¿‡å°† Pass@k ä½œä¸ºå¥–åŠ±ç›´æ¥è®­ç»ƒç­–ç•¥æ¨¡å‹ï¼Œå¹¶æ¨å¯¼å‡ºäº† Pass@k Training ä¼˜åŠ¿å‡½æ•° (Advantage Function) çš„è§£æè§£ï¼Œä»è€Œåœ¨æ˜¾è‘—æå‡æ¨¡å‹æ¢ç´¢èƒ½åŠ›çš„åŒæ—¶ä¿è¯äº†è®­ç»ƒè¿‡ç¨‹çš„é«˜æ•ˆæ€§ã€‚åˆ†æç»“æœæ­ç¤ºäº†æ¢ç´¢ä¸åˆ©ç”¨å¹¶éæœ¬è´¨å†²çªçš„ç›®æ ‡ï¼ŒäºŒè€…å®é™…ä¸Šå¯ä»¥ç›¸äº’å¢å¼ºã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶é€šè¿‡è§£ææ¨å¯¼åˆæ­¥æ¢ç´¢äº† RLVR çš„ä¼˜åŠ¿å‡½æ•°è®¾è®¡ (Advantage Design)ï¼Œå®éªŒç»“æœè¡¨æ˜è¯¥æ–¹å‘å…·æœ‰æ˜¾è‘—çš„æ”¹è¿›æ½œåŠ›ï¼Œä¸ºä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„è¡¨ç°æä¾›äº†æ–°çš„ç†è®ºæ”¯æ’‘å’Œå®è·µæ–¹å‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Technical Report about RLVR: 32 pages, 18 figures, 7 tables",
      "pdf_url": "https://arxiv.org/pdf/2508.10751v1",
      "published_date": "2025-08-14 15:34:47 UTC",
      "updated_date": "2025-08-14 15:34:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:49:46.387148+00:00"
    },
    {
      "arxiv_id": "2508.10747v3",
      "title": "Scaling Up without Fading Out: Goal-Aware Sparse GNN for RL-based Generalized Planning",
      "title_zh": "è§„æ¨¡æ‰©å±•è€Œä¸å¤±æ•ˆï¼šé¢å‘å¼ºåŒ–å­¦ä¹ æ³›åŒ–è§„åˆ’çš„ç›®æ ‡æ„ŸçŸ¥ç¨€ç–å›¾ç¥ç»ç½‘ç»œ",
      "authors": [
        "Sangwoo Jeon",
        "Juchul Shin",
        "Gyeong-Tae Kim",
        "YeonJe Cho",
        "Seongwoo Kim"
      ],
      "abstract": "Generalized planning using deep reinforcement learning (RL) combined with graph neural networks (GNNs) has shown promising results in various symbolic planning domains described by PDDL. However, existing approaches typically represent planning states as fully connected graphs, leading to a combinatorial explosion in edge information and substantial sparsity as problem scales grow, especially evident in large grid-based environments. This dense representation results in diluted node-level information, exponentially increases memory requirements, and ultimately makes learning infeasible for larger-scale problems. To address these challenges, we propose a sparse, goal-aware GNN representation that selectively encodes relevant local relationships and explicitly integrates spatial features related to the goal. We validate our approach by designing novel drone mission scenarios based on PDDL within a grid world, effectively simulating realistic mission execution environments. Our experimental results demonstrate that our method scales effectively to larger grid sizes previously infeasible with dense graph representations and substantially improves policy generalization and success rates. Our findings provide a practical foundation for addressing realistic, large-scale generalized planning tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ (RL)å’Œå›¾ç¥ç»ç½‘ç»œ(GNN)çš„é€šç”¨è§„åˆ’(Generalized Planning)åœ¨å¤„ç†å¤§è§„æ¨¡åœºæ™¯æ—¶é¢ä¸´çš„æ‰©å±•æ€§ç“¶é¢ˆè¿›è¡Œäº†æ¢è®¨ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•å› é‡‡ç”¨å…¨è¿æ¥å›¾è¡¨ç¤ºè€Œå¯¼è‡´çš„è¾¹ç¼˜ä¿¡æ¯çˆ†ç‚¸ã€å†…å­˜å‰§å¢åŠèŠ‚ç‚¹ä¿¡æ¯ç¨€é‡Šç­‰é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§ç¨€ç–ä¸”å…·å¤‡ç›®æ ‡æ„ŸçŸ¥èƒ½åŠ›çš„Goal-Aware Sparse GNNè¡¨ç¤ºæ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡é€‰æ‹©æ€§åœ°ç¼–ç ç›¸å…³çš„å±€éƒ¨å…³ç³»ï¼Œå¹¶æ˜¾å¼é›†æˆä¸ç›®æ ‡ç›¸å…³çš„ç©ºé—´ç‰¹å¾ï¼Œæ˜¾è‘—ä¼˜åŒ–äº†å¤§è§„æ¨¡ç¯å¢ƒä¸‹çš„çŠ¶æ€è¡¨å¾æ•ˆç‡ã€‚åœ¨åŸºäºPDDLçš„æ— äººæœºä»»åŠ¡å®éªŒä¸­ï¼Œè¯¥æ–¹æ³•æˆåŠŸæ‰©å±•åˆ°äº†æ­¤å‰å¯†é›†å›¾è¡¨ç¤ºæ— æ³•å¤„ç†çš„å¤§å‹ç½‘æ ¼è§„æ¨¡ï¼Œå¹¶å¤§å¹…æå‡äº†ç­–ç•¥çš„æ³›åŒ–æ€§èƒ½ä¸ä»»åŠ¡æˆåŠŸç‡ã€‚è¯¥æˆæœä¸ºè§£å†³ç°å®ä¸–ç•Œä¸­å¤§è§„æ¨¡é€šç”¨è§„åˆ’ä»»åŠ¡æä¾›äº†ä¸€ä¸ªé«˜æ•ˆä¸”å®ç”¨çš„æŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted for publication in International Journal of Control, Automation, and Systems (IJCAS). The Version of Record is available via the publisher",
      "pdf_url": "https://arxiv.org/pdf/2508.10747v3",
      "published_date": "2025-08-14 15:30:28 UTC",
      "updated_date": "2025-11-09 00:48:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:49:51.192917+00:00"
    },
    {
      "arxiv_id": "2508.16625v2",
      "title": "Data and Context Matter: Towards Generalizing AI-based Software Vulnerability Detection",
      "title_zh": "æ•°æ®ä¸ä¸Šä¸‹æ–‡çš„é‡è¦æ€§ï¼šæå‡åŸºäºäººå·¥æ™ºèƒ½çš„è½¯ä»¶æ¼æ´æ£€æµ‹æ³›åŒ–èƒ½åŠ›",
      "authors": [
        "Rijha Safdar",
        "Danyail Mateen",
        "Syed Taha Ali",
        "M. Umer Ashfaq",
        "Wajahat Hussain"
      ],
      "abstract": "AI-based solutions demonstrate remarkable results in identifying vulnerabilities in software, but research has consistently found that this performance does not generalize to unseen codebases. In this paper, we specifically investigate the impact of model architecture, parameter configuration, and quality of training data on the ability of these systems to generalize.\n  For this purpose, we introduce VulGate, a high quality state of the art dataset that mitigates the shortcomings of prior datasets, by removing mislabeled and duplicate samples, updating new vulnerabilities, incorporating additional metadata, integrating hard samples, and including dedicated test sets. We undertake a series of experiments to demonstrate that improved dataset diversity and quality substantially enhances vulnerability detection. We also introduce and benchmark multiple encoder-only and decoder-only models. We find that encoder-based models outperform other models in terms of accuracy and generalization. Our model achieves \\textbf{6.8\\%} improvement in recall on the benchmark BigVul dataset and outperforms others on unseen projects, demonstrating enhanced generalizability. Our results highlight the role of data quality and model selection in the development of robust vulnerability detection systems. Our findings suggest a direction for future systems with high cross-project effectiveness.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŸºäº AI çš„è½¯ä»¶æ¼æ´æ£€æµ‹ç³»ç»Ÿåœ¨é¢å¯¹æœªçŸ¥ä»£ç åº“æ—¶æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† VulGateï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„å‰æ²¿æ•°æ®é›†ï¼Œé€šè¿‡ç§»é™¤é”™è¯¯æ ‡ç­¾å’Œé‡å¤æ ·æœ¬ã€æ›´æ–°æ¼æ´ä¿¡æ¯ä»¥åŠæ•´åˆ hard samples ç­‰æ‰‹æ®µå…‹æœäº†ç°æœ‰æ•°æ®é›†çš„ç¼ºé™·ã€‚ç ”ç©¶é€šè¿‡ä¸€ç³»åˆ—å®éªŒè¯æ˜ï¼Œæé«˜æ•°æ®é›†çš„å¤šæ ·æ€§å’Œè´¨é‡èƒ½æ˜¾è‘—å¢å¼ºæ¼æ´æ£€æµ‹çš„æ•ˆæœã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿé’ˆå¯¹å¤šç§ encoder-only å’Œ decoder-only æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå‘ç° encoder-based æ¨¡å‹åœ¨å‡†ç¡®ç‡å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢è¡¨ç°æ›´ä¼˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨ BigVul åŸºå‡†æ•°æ®é›†ä¸Šçš„ recall æå‡äº† 6.8%ï¼Œå¹¶åœ¨æœªçŸ¥é¡¹ç›®ä¸Šå±•ç°å‡ºæ›´å¼ºçš„è·¨é¡¹ç›®æ³›åŒ–æ€§èƒ½ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†æ•°æ®è´¨é‡å’Œæ¨¡å‹é€‰æ‹©å¯¹äºæ„å»ºé²æ£’æ¼æ´æ£€æµ‹ç³»ç»Ÿçš„é‡è¦æ€§ï¼Œä¸ºæœªæ¥ç³»ç»Ÿçš„å¼€å‘æŒ‡æ˜äº†æ–¹å‘ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.16625v2",
      "published_date": "2025-08-14 15:30:22 UTC",
      "updated_date": "2025-10-06 19:18:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:49:52.494194+00:00"
    },
    {
      "arxiv_id": "2508.10745v1",
      "title": "Agentic Design Review System",
      "title_zh": "æ™ºèƒ½ä½“åŒ–è®¾è®¡è¯„å®¡ç³»ç»Ÿ",
      "authors": [
        "Sayan Nag",
        "K J Joseph",
        "Koustava Goswami",
        "Vlad I Morariu",
        "Balaji Vasan Srinivasan"
      ],
      "abstract": "Evaluating graphic designs involves assessing it from multiple facets like alignment, composition, aesthetics and color choices. Evaluating designs in a holistic way involves aggregating feedback from individual expert reviewers. Towards this, we propose an Agentic Design Review System (AgenticDRS), where multiple agents collaboratively analyze a design, orchestrated by a meta-agent. A novel in-context exemplar selection approach based on graph matching and a unique prompt expansion method plays central role towards making each agent design aware. Towards evaluating this framework, we propose DRS-BENCH benchmark. Thorough experimental evaluation against state-of-the-art baselines adapted to the problem setup, backed-up with critical ablation experiments brings out the efficacy of Agentic-DRS in evaluating graphic designs and generating actionable feedback. We hope that this work will attract attention to this pragmatic, yet under-explored research direction.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Agentic Design Review System (AgenticDRS)ï¼Œæ—¨åœ¨é€šè¿‡å¤šæ™ºèƒ½ä½“åä½œä»å¯¹é½ã€æ„å›¾ã€å®¡ç¾å’Œè‰²å½©ç­‰å¤šä¸ªç»´åº¦å¯¹å¹³é¢è®¾è®¡è¿›è¡Œå…¨é¢è¯„ä¼°ã€‚ç³»ç»Ÿé‡‡ç”¨ Meta-agent åè°ƒå¤šä¸ªä¸“å®¶æ™ºèƒ½ä½“å…±åŒåˆ†æè®¾è®¡ï¼Œå¹¶å¼•å…¥äº†åŸºäº Graph Matching çš„æƒ…å¢ƒç¤ºä¾‹é€‰æ‹©ï¼ˆIn-context exemplar selectionï¼‰å’Œç‹¬ç‰¹çš„ Prompt Expansion æ–¹æ³•æ¥å¢å¼ºæ™ºèƒ½ä½“å¯¹è®¾è®¡çš„æ„ŸçŸ¥åŠ›ã€‚ç ”ç©¶è€…åŒæ­¥æ¨å‡ºäº† DRS-BENCH åŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡ä¸å¤šç§æœ€å…ˆè¿›åŸºçº¿æ¨¡å‹çš„å¯¹æ¯”å®éªŒï¼ŒéªŒè¯äº† AgenticDRS åœ¨ç”Ÿæˆå¯æ“ä½œæ€§åé¦ˆæ–¹é¢çš„å“è¶Šæ€§èƒ½ã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥è¯å®äº†å„æ ¸å¿ƒç»„ä»¶çš„æœ‰æ•ˆæ€§ï¼Œä¸ºè‡ªåŠ¨åŒ–å¹³é¢è®¾è®¡è¯„ä¼°è¿™ä¸€å…·æœ‰å®é™…æ„ä¹‰ä¸”å°šæœªå……åˆ†æ¢ç´¢çš„é¢†åŸŸæä¾›äº†åˆ›æ–°æ–¹æ¡ˆã€‚è¯¥æ¡†æ¶ä¸ä»…æé«˜äº†è®¾è®¡è¯„ä»·çš„å‡†ç¡®æ€§ï¼Œä¹Ÿå±•ç°äº†å¤šæ™ºèƒ½ä½“åä½œåœ¨å¤„ç†å¤æ‚è§†è§‰ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.MA",
        "cs.MM"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10745v1",
      "published_date": "2025-08-14 15:29:24 UTC",
      "updated_date": "2025-08-14 15:29:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:49:58.994968+00:00"
    },
    {
      "arxiv_id": "2508.10732v1",
      "title": "APFL: Analytic Personalized Federated Learning via Dual-Stream Least Squares",
      "title_zh": "APFLï¼šåŸºäºåŒæµæœ€å°äºŒä¹˜æ³•çš„è§£æä¸ªæ€§åŒ–è”é‚¦å­¦ä¹ ",
      "authors": [
        "Kejia Fan",
        "Jianheng Tang",
        "Zhirui Yang",
        "Feijiang Han",
        "Jiaxu Li",
        "Run He",
        "Yajiang Huang",
        "Anfeng Liu",
        "Houbing Herbert Song",
        "Yunhuai Liu",
        "Huiping Zhuang"
      ],
      "abstract": "Personalized Federated Learning (PFL) has presented a significant challenge to deliver personalized models to individual clients through collaborative training. Existing PFL methods are often vulnerable to non-IID data, which severely hinders collective generalization and then compromises the subsequent personalization efforts. In this paper, to address this non-IID issue in PFL, we propose an Analytic Personalized Federated Learning (APFL) approach via dual-stream least squares. In our APFL, we use a foundation model as a frozen backbone for feature extraction. Subsequent to the feature extractor, we develop dual-stream analytic models to achieve both collective generalization and individual personalization. Specifically, our APFL incorporates a shared primary stream for global generalization across all clients, and a dedicated refinement stream for local personalization of each individual client. The analytical solutions of our APFL enable its ideal property of heterogeneity invariance, theoretically meaning that each personalized model remains identical regardless of how heterogeneous the data are distributed across all other clients. Empirical results across various datasets also validate the superiority of our APFL over state-of-the-art baselines, with advantages of at least 1.10%-15.45% in accuracy.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†APFL (Analytic Personalized Federated Learning)ï¼Œä¸€ç§åŸºäºåŒæµæœ€å°äºŒä¹˜æ³• (dual-stream least squares) çš„è§£æä¸ªæ€§åŒ–è”é‚¦å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä¸ªæ€§åŒ–è”é‚¦å­¦ä¹  (PFL) ä¸­å› éç‹¬ç«‹åŒåˆ†å¸ƒ (non-IID) æ•°æ®å¯¼è‡´çš„æ³›åŒ–ä¸ä¸ªæ€§åŒ–éš¾é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨åŸºç¡€æ¨¡å‹ (foundation model) ä½œä¸ºå†»ç»“éª¨å¹²è¿›è¡Œç‰¹å¾æå–ï¼Œå¹¶è®¾è®¡äº†åŒ…å«å…±äº«ä¸»æµå’Œä¸“ç”¨ç»†åŒ–æµçš„åŒæµè§£ææ¨¡å‹ï¼Œåˆ†åˆ«å®ç°å…¨å±€æ³›åŒ–ä¸ä¸ªä½“ä¸ªæ€§åŒ–ã€‚ç”±äºé‡‡ç”¨äº†è§£æè§£ï¼ŒAPFL å…·å¤‡ç†æƒ³çš„å¼‚æ„ä¸å˜æ€§ (heterogeneity invariance) ç‰¹æ€§ï¼Œåœ¨ç†è®ºä¸Šä¿è¯äº†ä¸ªæ€§åŒ–æ¨¡å‹ä¸å—å…¶ä»–å®¢æˆ·ç«¯æ•°æ®åˆ†å¸ƒå¼‚æ„æ€§çš„å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAPFL åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰çš„å…ˆè¿›åŸºçº¿æ¨¡å‹ï¼Œå‡†ç¡®ç‡æå‡å¹…åº¦è¾¾ 1.10% è‡³ 15.45%ï¼Œè¯æ˜äº†å…¶åœ¨å¤„ç†å¼‚æ„æ•°æ®ç¯å¢ƒä¸‹çš„å“è¶Šæ€§èƒ½ä¸å¯é æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 4 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2508.10732v1",
      "published_date": "2025-08-14 15:12:50 UTC",
      "updated_date": "2025-08-14 15:12:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:50:04.194624+00:00"
    },
    {
      "arxiv_id": "2508.10729v1",
      "title": "EgoCross: Benchmarking Multimodal Large Language Models for Cross-Domain Egocentric Video Question Answering",
      "title_zh": "EgoCrossï¼šé¢å‘è·¨é¢†åŸŸç¬¬ä¸€äººç§°è§†è§’è§†é¢‘é—®ç­”çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¯„æµ‹åŸºå‡†",
      "authors": [
        "Yanjun Li",
        "Yuqian Fu",
        "Tianwen Qian",
        "Qi'ao Xu",
        "Silong Dai",
        "Danda Pani Paudel",
        "Luc Van Gool",
        "Xiaoling Wang"
      ],
      "abstract": "Recent advances in Multimodal Large Language Models (MLLMs) have significantly pushed the frontier of egocentric video question answering (EgocentricQA). However, existing benchmarks and studies are mainly limited to common daily activities such as cooking and cleaning. In contrast, real-world deployment inevitably encounters domain shifts, where target domains differ substantially in both visual style and semantic content. To bridge this gap, we introduce \\textbf{EgoCross}, a comprehensive benchmark designed to evaluate the cross-domain generalization of MLLMs in EgocentricQA. EgoCross covers four diverse and challenging domains, including surgery, industry, extreme sports, and animal perspective, representing realistic and high-impact application scenarios. It comprises approximately 1,000 QA pairs across 798 video clips, spanning four key QA tasks: prediction, recognition, localization, and counting. Each QA pair provides both OpenQA and CloseQA formats to support fine-grained evaluation. Extensive experiments show that most existing MLLMs, whether general-purpose or egocentric-specialized, struggle to generalize to domains beyond daily life, highlighting the limitations of current models. Furthermore, we conduct several pilot studies, \\eg, fine-tuning and reinforcement learning, to explore potential improvements. We hope EgoCross and our accompanying analysis will serve as a foundation for advancing domain-adaptive, robust egocentric video understanding. Data and codes will be released at: \\href{https://github.com/MyUniverse0726/EgoCross}{https://github.com/MyUniverse0726/EgoCross.}",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰ç¬¬ä¸€äººç§°è§†é¢‘é—®ç­” (EgocentricQA) åŸºå‡†æµ‹è¯•å±€é™äºæ—¥å¸¸æ´»åŠ¨çš„é—®é¢˜ï¼Œæå‡ºäº† EgoCrossï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) è·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›çš„ç»¼åˆæ€§åŸºå‡†ã€‚EgoCross æ¶µç›–äº†æ‰‹æœ¯ (surgery)ã€å·¥ä¸š (industry)ã€æé™è¿åŠ¨ (extreme sports) å’ŒåŠ¨ç‰©è§†è§’ (animal perspective) å››ä¸ªæå…·æŒ‘æˆ˜æ€§çš„çœŸå®åº”ç”¨åœºæ™¯ï¼ŒåŒ…å«çº¦ 1,000 ä¸ªé—®ç­”å¯¹å’Œ 798 ä¸ªè§†é¢‘ç‰‡æ®µã€‚è¯¥åŸºå‡†æ¨ªè·¨é¢„æµ‹ (prediction)ã€è¯†åˆ« (recognition)ã€å®šä½ (localization) å’Œè®¡æ•° (counting) å››é¡¹æ ¸å¿ƒä»»åŠ¡ï¼Œå¹¶åŒæ—¶æ”¯æŒ OpenQA å’Œ CloseQA æ ¼å¼ä»¥è¿›è¡Œç»†ç²’åº¦è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ— è®ºæ˜¯é€šç”¨å‹è¿˜æ˜¯ä¸“ç”¨å‹ MLLMsï¼Œåœ¨å¤„ç†æ—¥å¸¸ç”Ÿæ´»ä»¥å¤–çš„é¢†åŸŸæ—¶å‡è¡¨ç°å‡ºæ˜æ˜¾çš„æ€§èƒ½ç“¶é¢ˆï¼Œæš´éœ²å‡ºå½“å‰æ¨¡å‹åœ¨è·¨é¢†åŸŸæ³›åŒ–æ–¹é¢çš„å±€é™æ€§ã€‚æœ€åï¼Œç ”ç©¶é€šè¿‡å¾®è°ƒ (fine-tuning) å’Œå¼ºåŒ–å­¦ä¹  (reinforcement learning) ç­‰è¯•ç‚¹ç ”ç©¶æ¢è®¨äº†æ”¹è¿›è·¯å¾„ï¼Œä¸ºå¼€å‘é²æ£’çš„é¢†åŸŸè‡ªé€‚åº”è§†é¢‘ç†è§£æ¨¡å‹å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10729v1",
      "published_date": "2025-08-14 15:11:20 UTC",
      "updated_date": "2025-08-14 15:11:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:50:08.250622+00:00"
    },
    {
      "arxiv_id": "2508.10713v1",
      "title": "Electromagnetic Simulations of Antennas on GPUs for Machine Learning Applications",
      "title_zh": "é¢å‘æœºå™¨å­¦ä¹ åº”ç”¨çš„åŸºäº GPU çš„å¤©çº¿ç”µç£ä»¿çœŸ",
      "authors": [
        "Murat Temiz",
        "Vemund Bakken"
      ],
      "abstract": "This study proposes an antenna simulation framework powered by graphics processing units (GPUs) based on an open-source electromagnetic (EM) simulation software (gprMax) for machine learning applications of antenna design and optimization. Furthermore, it compares the simulation results with those obtained through commercial EM software. The proposed software framework for machine learning and surrogate model applications will produce antenna data sets consisting of a large number of antenna simulation results using GPUs. Although machine learning methods can attain the optimum solutions for many problems, they are known to be data-hungry and require a great deal of samples for the training stage of the algorithms. However, producing a sufficient number of training samples in EM applications within a limited time is challenging due to the high computational complexity of EM simulations. Therefore, GPUs are utilized in this study to simulate a large number of antennas with predefined or random antenna shape parameters to produce data sets. Moreover, this study also compares various machine learning and deep learning models in terms of antenna parameter estimation performance. This study demonstrates that an entry-level GPU substantially outperforms a high-end CPU in terms of computational performance, while a high-end gaming GPU can achieve around 18 times more computational performance compared to a high-end CPU. Moreover, it is shown that the open-source EM simulation software can deliver similar results to those obtained via commercial software in the simulation of microstrip antennas when the spatial resolution of the simulations is sufficiently fine.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŸºäºå¼€æºç”µç£(EM)æ¨¡æ‹Ÿè½¯ä»¶gprMaxå¹¶ç”±å›¾å½¢å¤„ç†å•å…ƒ(GPUs)é©±åŠ¨çš„å¤©çº¿æ¨¡æ‹Ÿæ¡†æ¶ï¼Œæ—¨åœ¨æ”¯æŒå¤©çº¿è®¾è®¡ä¸ä¼˜åŒ–ä¸­çš„Machine Learningåº”ç”¨ã€‚è¯¥æ¡†æ¶é’ˆå¯¹ç”µç£(EM)æ¨¡æ‹Ÿè®¡ç®—å¤æ‚åº¦é«˜ã€éš¾ä»¥åœ¨çŸ­æ—¶é—´å†…ä¸ºæ•°æ®å¯†é›†å‹ç®—æ³•æä¾›å……è¶³è®­ç»ƒæ ·æœ¬çš„æŒ‘æˆ˜ï¼Œåˆ©ç”¨GPUsç”ŸæˆåŒ…å«å¤§é‡æ¨¡æ‹Ÿç»“æœçš„æ•°æ®é›†ã€‚ç ”ç©¶å¯¹æ¯”äº†å¤šç§Machine Learningå’ŒDeep Learningæ¨¡å‹åœ¨å¤©çº¿å‚æ•°ä¼°è®¡ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œæ˜¾è‘—æå‡äº†æ¨¡æ‹Ÿæ•ˆç‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå…¥é—¨çº§GPUçš„è®¡ç®—æ€§èƒ½å·²ä¼˜äºé«˜ç«¯CPUï¼Œè€Œé«˜ç«¯æ¸¸æˆGPUçš„æ€§èƒ½å¯è¾¾é«˜ç«¯CPUçš„18å€å·¦å³ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¯å®äº†åœ¨ç©ºé—´åˆ†è¾¨ç‡è¶³å¤Ÿç»†è‡´çš„æƒ…å†µä¸‹ï¼Œè¯¥å¼€æºæ¡†æ¶åœ¨Microstrip Antennasæ¨¡æ‹Ÿä¸­èƒ½è·å¾—ä¸å•†ä¸šç”µç£(EM)è½¯ä»¶ç›¸ä¼¼çš„ç»“æœã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "20 pages, 10 figures, 4 tables, journal article",
      "pdf_url": "https://arxiv.org/pdf/2508.10713v1",
      "published_date": "2025-08-14 14:56:04 UTC",
      "updated_date": "2025-08-14 14:56:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:50:13.760020+00:00"
    },
    {
      "arxiv_id": "2508.10703v1",
      "title": "GenOM: Ontology Matching with Description Generation and Large Language Model",
      "title_zh": "GenOMï¼šåŸºäºæè¿°ç”Ÿæˆä¸å¤§è¯­è¨€æ¨¡å‹çš„æœ¬ä½“åŒ¹é…",
      "authors": [
        "Yiping Song",
        "Jiaoyan Chen",
        "Renate A. Schmidt"
      ],
      "abstract": "Ontology matching (OM) plays an essential role in enabling semantic interoperability and integration across heterogeneous knowledge sources, particularly in the biomedical domain which contains numerous complex concepts related to diseases and pharmaceuticals. This paper introduces GenOM, a large language model (LLM)-based ontology alignment framework, which enriches the semantic representations of ontology concepts via generating textual definitions, retrieves alignment candidates with an embedding model, and incorporates exact matching-based tools to improve precision. Extensive experiments conducted on the OAEI Bio-ML track demonstrate that GenOM can often achieve competitive performance, surpassing many baselines including traditional OM systems and recent LLM-based methods. Further ablation studies confirm the effectiveness of semantic enrichment and few-shot prompting, highlighting the framework's robustness and adaptability.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† GenOMï¼Œä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹ (Large Language Model) çš„æœ¬ä½“åŒ¹é… (Ontology Matching) æ¡†æ¶ï¼Œæ—¨åœ¨æå‡ç”Ÿç‰©åŒ»å­¦ç­‰å¤æ‚é¢†åŸŸå¼‚æ„çŸ¥è¯†æºä¹‹é—´çš„è¯­ä¹‰äº’æ“ä½œæ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸ºæœ¬ä½“æ¦‚å¿µç”Ÿæˆæ–‡æœ¬å®šä¹‰æ¥ä¸°å¯Œå…¶è¯­ä¹‰è¡¨ç¤ºï¼Œå¹¶åˆ©ç”¨åµŒå…¥æ¨¡å‹ (Embedding Model) æ£€ç´¢æ½œåœ¨çš„å¯¹é½å€™é€‰å¯¹è±¡ã€‚ä¸ºäº†è¿›ä¸€æ­¥æå‡åŒ¹é…ç²¾åº¦ï¼ŒGenOM è¿˜æ•´åˆäº†åŸºäºç²¾ç¡®åŒ¹é… (Exact Matching) çš„ä¼ ç»Ÿå·¥å…·ã€‚åœ¨ OAEI Bio-ML èµ›é“ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒGenOM çš„æ€§èƒ½ä¼˜äºä¼ ç»Ÿ OM ç³»ç»ŸåŠè¿‘æœŸåŸºäº LLM çš„ä¸»æµæ–¹æ³•ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯å®äº†è¯­ä¹‰å¢å¼ºå’Œå°‘æ ·æœ¬æç¤º (Few-shot Prompting) åœ¨æå‡æ¡†æ¶ç¨³å¥æ€§ä¸é€‚åº”æ€§æ–¹é¢çš„å…³é”®ä½œç”¨ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10703v1",
      "published_date": "2025-08-14 14:48:09 UTC",
      "updated_date": "2025-08-14 14:48:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:50:17.764335+00:00"
    },
    {
      "arxiv_id": "2508.10701v1",
      "title": "REFN: A Reinforcement-Learning-From-Network Framework against 1-day/n-day Exploitations",
      "title_zh": "REFNï¼šä¸€ç§é’ˆå¯¹ 1-day/n-day æ¼æ´åˆ©ç”¨çš„åŸºäºç½‘ç»œå¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶",
      "authors": [
        "Tianlong Yu",
        "Lihong Liu",
        "Ziyi Zhou",
        "Fudu Xing",
        "Kailong Wang",
        "Yang Yang"
      ],
      "abstract": "The exploitation of 1 day or n day vulnerabilities poses severe threats to networked devices due to massive deployment scales and delayed patching (average Mean Time To Patch exceeds 60 days). Existing defenses, including host based patching and network based filtering, are inadequate due to limited scalability across diverse devices, compatibility issues especially with embedded or legacy systems, and error prone deployment process (manual patch validation). To address these issues, we introduce REFN (Reinforcement Learning From Network), a novel framework that trains Large Language Models (LLMs) to autonomously generate network filters to prevent 1 day or n day exploitations. REFN ensures scalability by uniquely employs Reinforcement Learning (RL) driven by online network rewards instead of traditional Human Feedback (RLHF). REFN guarantees compatibility via unified deployment on edge security gateways (Amazon Eero). REFN provides robustness via online validation using real network traffic. Crucially, REFN addresses three core challenges in training LLMs for exploit prevention: 1) expanding current LLMs limited vulnerability fixing expertise via Agentic RAG based Knowledge Distillation, 2) bridging current LLMs language to network gaps through an RL From VNF Pipeline that translates language context (vulnerability description) into network enforcement, 3) addressing the LLM hallucination and non determinism via the Online Agentic Validation that penalizes erroneous outputs. Evaluated across 22 families of 1 day or n day exploits, REFN demonstrates effectiveness (21.1 percent higher accuracy than alternatives), efficiency (Mean Time To Patch of 3.65 hours) and scalability (easily scale to 10K devices). REFN serves as an initial step toward training LLMs to rapidly prevent massive scale 1 day or n day exploitations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹1-day/n-dayæ¼æ´ç”±äºè¡¥ä¸å»¶è¿Ÿå’Œå…¼å®¹æ€§é—®é¢˜å¯¹è”ç½‘è®¾å¤‡æ„æˆçš„ä¸¥é‡å¨èƒï¼Œæå‡ºäº†REFNï¼ˆReinforcement-Learning-From-Networkï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹(LLMs)è‡ªä¸»ç”Ÿæˆç½‘ç»œè¿‡æ»¤å™¨ï¼Œå¹¶åˆ›æ–°æ€§åœ°åˆ©ç”¨åœ¨çº¿ç½‘ç»œå¥–åŠ±é©±åŠ¨çš„å¼ºåŒ–å­¦ä¹ (Reinforcement Learning, RL)å–ä»£ä¼ ç»Ÿçš„RLHFï¼Œä»è€Œæ˜¾è‘—æå‡é˜²å¾¡çš„æ‰©å±•æ€§ã€‚ä¸ºäº†å…‹æœæ¨¡å‹åœ¨æ¼æ´ä¿®å¤é¢†åŸŸçš„çŸ¥è¯†å±€é™ï¼ŒREFNå¼•å…¥äº†åŸºäºAgentic RAGçš„çŸ¥è¯†è’¸é¦ï¼Œå¹¶é€šè¿‡RL From VNF Pipelineå°†æ¼æ´æè¿°è½¬åŒ–ä¸ºå…·ä½“çš„ç½‘ç»œæ‰§è¡Œç­–ç•¥ã€‚æ­¤å¤–ï¼Œæ¡†æ¶é€šè¿‡Online Agentic Validationæœºåˆ¶å¯¹é”™è¯¯è¾“å‡ºè¿›è¡Œæƒ©ç½šï¼Œæœ‰æ•ˆè§£å†³äº†LLMçš„å¹»è§‰å’Œä¸ç¡®å®šæ€§ï¼Œç¡®ä¿äº†åœ¨è¾¹ç¼˜å®‰å…¨ç½‘å…³éƒ¨ç½²çš„é²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨22ç±»æ¼æ´æµ‹è¯•ä¸­ï¼ŒREFNçš„å‡†ç¡®ç‡æ¯”ç°æœ‰æ–¹æ¡ˆé«˜å‡º21.1%ï¼Œå¹³å‡è¡¥ä¸æ—¶é—´(Mean Time To Patch, MTTP)å¤§å¹…ç¼©çŸ­è‡³3.65å°æ—¶ï¼Œå¹¶å…·å¤‡æ”¯æŒ10,000å°è§„æ¨¡è®¾å¤‡çš„æ‰©å±•èƒ½åŠ›ã€‚è¯¥å·¥ä½œä¸ºåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹å®ç°å¤§è§„æ¨¡è‡ªåŠ¨åŒ–æ¼æ´é˜²å¾¡å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10701v1",
      "published_date": "2025-08-14 14:45:45 UTC",
      "updated_date": "2025-08-14 14:45:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:50:34.387615+00:00"
    },
    {
      "arxiv_id": "2508.10695v1",
      "title": "Learning from Natural Language Feedback for Personalized Question Answering",
      "title_zh": "é¢å‘ä¸ªæ€§åŒ–é—®ç­”çš„è‡ªç„¶è¯­è¨€åé¦ˆå­¦ä¹ ",
      "authors": [
        "Alireza Salemi",
        "Hamed Zamani"
      ],
      "abstract": "Personalization is crucial for enhancing both the effectiveness and user satisfaction of language technologies, particularly in information-seeking tasks like question answering. Current approaches for personalizing large language models (LLMs) often rely on retrieval-augmented generation (RAG), followed by reinforcement learning with scalar reward signals to teach models how to use retrieved personal context. We believe that these scalar rewards sometimes provide weak, non-instructive feedback, limiting learning efficiency and personalization quality. We introduce VAC, a novel framework for personalized response generation that replaces scalar rewards with natural language feedback (NLF) that are generated conditioned on the user profiles and the question narratives. NLF serves as a rich and actionable supervision signal, allowing the policy model to iteratively refine its outputs and internalize effective personalization strategies. Training alternates between optimizing the feedback model and fine-tuning the policy model on the improved responses, resulting in a policy model that no longer requires feedback at inference. Evaluation on the LaMP-QA benchmark that consists of three diverse domains demonstrates consistent and significant improvements over the state-of-the-art results. Human evaluations further confirm the superior quality of the generated responses. These results demonstrate that NLF provides more effective signals for optimizing personalized question answering.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¸ªæ€§åŒ–é—®ç­”ï¼ˆPersonalized Question Answeringï¼‰é¢†åŸŸä¸­ç°æœ‰å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¾èµ–æ ‡é‡å¥–åŠ±ï¼ˆscalar rewardsï¼‰è¿›è¡Œå¼ºåŒ–å­¦ä¹ æ—¶ï¼Œå› åé¦ˆä¿¡å·å¼±ä¸”ç¼ºä¹æŒ‡å¯¼æ€§è€Œå¯¼è‡´å­¦ä¹ æ•ˆç‡å—é™çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºVACçš„æ–°å‹æ¡†æ¶ã€‚VACæ¡†æ¶çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºä½¿ç”¨åŸºäºç”¨æˆ·ç”»åƒï¼ˆuser profilesï¼‰å’Œé—®é¢˜å™è¿°ç”Ÿæˆçš„è‡ªç„¶è¯­è¨€åé¦ˆï¼ˆNatural Language Feedback, NLFï¼‰æ›¿ä»£ä¼ ç»Ÿçš„æ ‡é‡ä¿¡å·ï¼Œä¸ºæ¨¡å‹æä¾›æ›´ä¸°å¯Œä¸”å¯æ“ä½œçš„ç›‘ç£ä¿¡å·ã€‚é€šè¿‡äº¤æ›¿ä¼˜åŒ–åé¦ˆæ¨¡å‹å’Œå¾®è°ƒç­–ç•¥æ¨¡å‹ï¼ˆpolicy modelï¼‰ï¼Œè¯¥æ¡†æ¶ä½¿æ¨¡å‹èƒ½å¤Ÿè¿­ä»£æ”¹è¿›è¾“å‡ºå¹¶å†…åŒ–ä¸ªæ€§åŒ–ç­–ç•¥ï¼Œä¸”åœ¨æ¨ç†é˜¶æ®µä¸å†éœ€è¦é¢å¤–åé¦ˆã€‚åœ¨LaMP-QAåŸºå‡†æµ‹è¯•çš„ä¸‰ä¸ªä¸åŒé¢†åŸŸä¸­ï¼Œè¯¥æ–¹æ³•å‡å–å¾—äº†æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›ï¼ˆstate-of-the-artï¼‰æ¨¡å‹çš„ç»“æœã€‚äººå·¥è¯„ä¼°ï¼ˆhuman evaluationsï¼‰è¿›ä¸€æ­¥è¯å®äº†VACåœ¨ç”Ÿæˆå›ç­”è´¨é‡ä¸Šçš„ä¼˜è¶Šæ€§ï¼Œè¯æ˜äº†è‡ªç„¶è¯­è¨€åé¦ˆåœ¨ä¼˜åŒ–ä¸ªæ€§åŒ–è¯­è¨€æŠ€æœ¯æ–¹é¢å…·æœ‰æ¯”æ ‡é‡å¥–åŠ±æ›´å¼ºçš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10695v1",
      "published_date": "2025-08-14 14:36:53 UTC",
      "updated_date": "2025-08-14 14:36:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:50:37.552864+00:00"
    },
    {
      "arxiv_id": "2508.10687v1",
      "title": "Continuous Bangla Sign Language Translation: Mitigating the Expense of Gloss Annotation with the Assistance of Graph",
      "title_zh": "è¿ç»­å­ŸåŠ æ‹‰è¯­æ‰‹è¯­ç¿»è¯‘ï¼šå€ŸåŠ©å›¾ç»“æ„é™ä½æ‰‹è¯­è¯æ ‡æ³¨æˆæœ¬",
      "authors": [
        "Safaeid Hossain Arib",
        "Rabeya Akter",
        "Sejuti Rahman"
      ],
      "abstract": "Millions of individuals worldwide are affected by deafness and hearing impairment. Sign language serves as a sophisticated means of communication for the deaf and hard of hearing. However, in societies that prioritize spoken languages, sign language often faces underestimation, leading to communication barriers and social exclusion. The Continuous Bangla Sign Language Translation project aims to address this gap by enhancing translation methods. While recent approaches leverage transformer architecture for state-of-the-art results, our method integrates graph-based methods with the transformer architecture. This fusion, combining transformer and STGCN-LSTM architectures, proves more effective in gloss-free translation. Our contributions include architectural fusion, exploring various fusion strategies, and achieving a new state-of-the-art performance on diverse sign language datasets, namely RWTH-PHOENIX-2014T, CSL-Daily, How2Sign, and BornilDB v1.0. Our approach demonstrates superior performance compared to current translation outcomes across all datasets, showcasing notable improvements of BLEU-4 scores of 4.01, 2.07, and 0.5, surpassing those of GASLT, GASLT and slt_how2sign in RWTH-PHOENIX-2014T, CSL-Daily, and How2Sign, respectively. Also, we introduce benchmarking on the BornilDB v1.0 dataset for the first time. Our method sets a benchmark for future research, emphasizing the importance of gloss-free translation to improve communication accessibility for the deaf and hard of hearing.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹è¿ç»­å­ŸåŠ æ‹‰è¯­æ‰‹è¯­ç¿»è¯‘ï¼ˆContinuous Bangla Sign Language Translationï¼‰ä¸­ Gloss æ ‡æ³¨æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ—¨åœ¨å‡å°‘æ ‡æ³¨ä¾èµ–çš„æ—  Gloss ç¿»è¯‘æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ›æ–°æ€§åœ°å°† Transformer æ¶æ„ä¸åŸºäºå›¾çš„æ–¹æ³•ï¼ˆGraph-based methodsï¼‰ç›¸ç»“åˆï¼Œé€šè¿‡èåˆ STGCN-LSTM æ¶æ„æ¥æ•æ‰æ‰‹è¯­çš„å¤æ‚æ—¶ç©ºç‰¹å¾ã€‚ç ”ç©¶å›¢é˜Ÿæ·±å…¥æ¢ç´¢äº†å¤šç§æ¶æ„èåˆç­–ç•¥ï¼Œåœ¨ RWTH-PHOENIX-2014Tã€CSL-Daily å’Œ How2Sign ç­‰å¤šä¸ªä¸»æµæ•°æ®é›†ä¸Šå‡åˆ·æ–°äº† SOTA æ€§èƒ½è®°å½•ã€‚å®éªŒæ•°æ®æ˜¾ç¤ºï¼Œå…¶ BLEU-4 åˆ†æ•°ç›¸è¾ƒäº GASLT ç­‰æ¨¡å‹åˆ†åˆ«æå‡äº† 4.01ã€2.07 å’Œ 0.5ï¼Œå±•ç¤ºäº†æ˜¾è‘—çš„ç¿»è¯‘å‡†ç¡®åº¦æå‡ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜é¦–æ¬¡åœ¨ BornilDB v1.0 æ•°æ®é›†ä¸Šå»ºç«‹äº†åŸºå‡†æµ‹è¯•ï¼ˆBenchmarkingï¼‰ï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†å‚è€ƒæ ‡å‡†ã€‚è¿™é¡¹å·¥ä½œä¸ä»…æœ‰æ•ˆé™ä½äº†æ‰‹è¯­ç¿»è¯‘å¯¹äººå·¥æ ‡æ³¨çš„ä¾èµ–ï¼Œä¹Ÿä¸ºæå‡å¬éšœç¾¤ä½“çš„æ²Ÿé€šä¾¿æ·æ€§æä¾›äº†é‡è¦çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10687v1",
      "published_date": "2025-08-14 14:32:31 UTC",
      "updated_date": "2025-08-14 14:32:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:50:42.859943+00:00"
    },
    {
      "arxiv_id": "2508.10672v2",
      "title": "Hybrid Generative Fusion for Efficient and Privacy-Preserving Face Recognition Dataset Generation",
      "title_zh": "æ··åˆç”Ÿæˆå¼èåˆï¼šé«˜æ•ˆä¸”éšç§ä¿æŠ¤çš„äººè„¸è¯†åˆ«æ•°æ®é›†ç”Ÿæˆ",
      "authors": [
        "Feiran Li",
        "Qianqian Xu",
        "Shilong Bao",
        "Boyu Han",
        "Zhiyong Yang",
        "Qingming Huang"
      ],
      "abstract": "In this paper, we present our approach to the DataCV ICCV Challenge, which centers on building a high-quality face dataset to train a face recognition model. The constructed dataset must not contain identities overlapping with any existing public face datasets. To handle this challenge, we begin with a thorough cleaning of the baseline HSFace dataset, identifying and removing mislabeled or inconsistent identities through a Mixture-of-Experts (MoE) strategy combining face embedding clustering and GPT-4o-assisted verification. We retain the largest consistent identity cluster and apply data augmentation up to a fixed number of images per identity. To further diversify the dataset, we generate synthetic identities using Stable Diffusion with prompt engineering. As diffusion models are computationally intensive, we generate only one reference image per identity and efficiently expand it using Vec2Face, which rapidly produces 49 identity-consistent variants. This hybrid approach fuses GAN-based and diffusion-based samples, enabling efficient construction of a diverse and high-quality dataset. To address the high visual similarity among synthetic identities, we adopt a curriculum learning strategy by placing them early in the training schedule, allowing the model to progress from easier to harder samples. Our final dataset contains 50 images per identity, and all newly generated identities are checked with mainstream face datasets to ensure no identity leakage. Our method achieves \\textbf{1st place} in the competition, and experimental results show that our dataset improves model performance across 10K, 20K, and 100K identity scales. Code is available at https://github.com/Ferry-Li/datacv_fr.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ DataCV ICCV Challenge çš„äººè„¸è¯†åˆ«æ•°æ®é›†æ„å»ºä»»åŠ¡ï¼Œæå‡ºäº†ä¸€ç§é«˜æ•ˆä¸”ä¿æŠ¤éšç§çš„æ··åˆç”Ÿæˆèåˆæ–¹æ³•ã€‚ç ”ç©¶å›¢é˜Ÿé¦–å…ˆé‡‡ç”¨ç»“åˆäººè„¸åµŒå…¥èšç±»ä¸ GPT-4o è¾…åŠ©éªŒè¯çš„æ··åˆä¸“å®¶ç­–ç•¥(Mixture-of-Experts)å¯¹åŸºå‡†æ•°æ®é›†è¿›è¡Œæ¸…æ´—ï¼Œéšåé€šè¿‡ Stable Diffusion ç»“åˆ Vec2Face æŠ€æœ¯å¿«é€Ÿç”Ÿæˆå¤šæ ·åŒ–çš„åˆæˆèº«ä»½å˜ä½“ã€‚ä¸ºäº†è§£å†³åˆæˆèº«ä»½é—´é«˜åº¦è§†è§‰ç›¸ä¼¼æ€§çš„æŒ‘æˆ˜ï¼Œè¯¥æ–¹æ³•é‡‡ç”¨äº†è¯¾ç¨‹å­¦ä¹ (Curriculum Learning)ç­–ç•¥ï¼Œä¼˜åŒ–äº†æ¨¡å‹ä»æ˜“åˆ°éš¾çš„è®­ç»ƒè¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç¡®ä¿ä¸ä¸ç°æœ‰å…¬å…±æ•°æ®é›†é‡å çš„å‰æä¸‹ï¼Œæ˜¾è‘—æå‡äº† 10K è‡³ 100K èº«ä»½è§„æ¨¡ä¸‹çš„æ¨¡å‹æ€§èƒ½ï¼Œå¹¶æœ€ç»ˆåœ¨ç«èµ›ä¸­è·å¾—ç¬¬ä¸€åã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This paper has been accpeted to ICCV 2025 DataCV Workshop",
      "pdf_url": "https://arxiv.org/pdf/2508.10672v2",
      "published_date": "2025-08-14 14:14:18 UTC",
      "updated_date": "2025-08-18 09:15:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:50:43.954623+00:00"
    },
    {
      "arxiv_id": "2508.15806v1",
      "title": "SurfaceLogicKV: Surface and Logic Attention Behaviors are All You Need for Robust KV Cache Compression",
      "title_zh": "SurfaceLogicKVï¼šä»…éœ€è¡¨é¢ä¸é€»è¾‘æ³¨æ„åŠ›è¡Œä¸ºå³å¯å®ç°ç¨³å¥çš„ KV ç¼“å­˜å‹ç¼©",
      "authors": [
        "Mengjie Li",
        "William J. Song"
      ],
      "abstract": "The increasing input sequence length in Large Language Models (LLMs) puts significant pressure on key-value (KV) cache storage, making efficient inference challenging. Explicitly distinguishing attention behavior into our self-defined surface memorization and logic construction reveals essential roles in long-context reasoning. We observe that an individual attention head can display various behaviors, with nearly 98.5% effectively ignoring completely irrelevant information. The remaining 1.5% behaves as logic construction, and 0.5% behaves as surface memorization. Based on layer- and head-wise integration, we propose a novel two-stage SurfaceLogicKV method to utilize these attention behaviors for KV Cache compression. As a result, it achieves improved compressing robustness while maintaining competitive performance across various tasks and long sequences compared to baselines or even FullKV in some specific situations",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†é•¿åºåˆ—æ—¶KV cacheå­˜å‚¨å‹åŠ›å·¨å¤§çš„é—®é¢˜ï¼Œæå‡ºäº†SurfaceLogicKVå‹ç¼©æ–¹æ³•ã€‚é€šè¿‡æ·±å…¥åˆ†ææ³¨æ„åŠ›æœºåˆ¶ï¼Œç ”ç©¶è€…è¯†åˆ«å¹¶å®šä¹‰äº†surface memorizationå’Œlogic constructionä¸¤ç§åœ¨é•¿ä¸Šä¸‹æ–‡æ¨ç†ä¸­èµ·å…³é”®ä½œç”¨çš„æ³¨æ„åŠ›è¡Œä¸ºã€‚è§‚å¯Ÿå‘ç°çº¦98.5%çš„æ³¨æ„åŠ›å¤´å®é™…ä¸Šä¼šå¿½ç•¥æ— å…³ä¿¡æ¯ï¼Œè€Œä»…æœ‰æå°‘æ•°å¤´è¡¨ç°å‡ºé€»è¾‘æ„å»ºæˆ–è¡¨é¢è®°å¿†è¡Œä¸ºã€‚åŸºäºè¿™äº›è¡Œä¸ºç‰¹å¾ï¼ŒSurfaceLogicKVé‡‡ç”¨äº†ä¸€ç§åˆ›æ–°çš„ä¸¤é˜¶æ®µå±‚çº§ä¸å¤´çº§é›†æˆæ–¹æ¡ˆæ¥å®ç°é«˜æ•ˆå‹ç¼©ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§ä»»åŠ¡å’Œé•¿åºåˆ—åœºæ™¯ä¸­å±•ç°å‡ºå“è¶Šçš„å‹ç¼©é²æ£’æ€§ï¼Œå…¶æ€§èƒ½åœ¨ç‰¹å®šæƒ…å†µä¸‹ç”šè‡³èƒ½ä¸FullKVç›¸åª²ç¾ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "18 pages, 9 tables, 10 pages",
      "pdf_url": "https://arxiv.org/pdf/2508.15806v1",
      "published_date": "2025-08-14 14:08:58 UTC",
      "updated_date": "2025-08-14 14:08:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:50:45.657937+00:00"
    },
    {
      "arxiv_id": "2508.10669v1",
      "title": "STEP: Stepwise Curriculum Learning for Context-Knowledge Fusion in Conversational Recommendation",
      "title_zh": "STEPï¼šå¯¹è¯å¼æ¨èä¸­è¯­å¢ƒ-çŸ¥è¯†èåˆçš„æ­¥è¿›å¼è¯¾ç¨‹å­¦ä¹ ",
      "authors": [
        "Zhenye Yang",
        "Jinpeng Chen",
        "Huan Li",
        "Xiongnan Jin",
        "Xuanyang Li",
        "Junwei Zhang",
        "Hongbo Gao",
        "Kaimin Wei",
        "Senzhang Wang"
      ],
      "abstract": "Conversational recommender systems (CRSs) aim to proactively capture user preferences through natural language dialogue and recommend high-quality items. To achieve this, CRS gathers user preferences via a dialog module and builds user profiles through a recommendation module to generate appropriate recommendations. However, existing CRS faces challenges in capturing the deep semantics of user preferences and dialogue context. In particular, the efficient integration of external knowledge graph (KG) information into dialogue generation and recommendation remains a pressing issue. Traditional approaches typically combine KG information directly with dialogue content, which often struggles with complex semantic relationships, resulting in recommendations that may not align with user expectations.\n  To address these challenges, we introduce STEP, a conversational recommender centered on pre-trained language models that combines curriculum-guided context-knowledge fusion with lightweight task-specific prompt tuning. At its heart, an F-Former progressively aligns the dialogue context with knowledge-graph entities through a three-stage curriculum, thus resolving fine-grained semantic mismatches. The fused representation is then injected into the frozen language model via two minimal yet adaptive prefix prompts: a conversation prefix that steers response generation toward user intent and a recommendation prefix that biases item ranking toward knowledge-consistent candidates. This dual-prompt scheme allows the model to share cross-task semantics while respecting the distinct objectives of dialogue and recommendation. Experimental results show that STEP outperforms mainstream methods in the precision of recommendation and dialogue quality in two public datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¯¹è¯æ¨èç³»ç»Ÿ(CRS)åœ¨èåˆå¤–éƒ¨çŸ¥è¯†å›¾è°±(KG)ä¸å¯¹è¯ä¸Šä¸‹æ–‡æ—¶å­˜åœ¨çš„æ·±å±‚è¯­ä¹‰å¯¹é½éš¾é¢˜ï¼Œæå‡ºäº†åŸºäºé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„STEPæ¡†æ¶ã€‚è¯¥æ¡†æ¶æ ¸å¿ƒåœ¨äºå¼•å…¥äº†F-Formerï¼Œé€šè¿‡ä¸‰é˜¶æ®µè¯¾ç¨‹å­¦ä¹ (three-stage curriculum)é€æ­¥å®ç°å¯¹è¯è¯­å¢ƒä¸çŸ¥è¯†å›¾è°±å®ä½“çš„æ·±åº¦å¯¹é½ï¼Œä»è€Œè§£å†³ç»†ç²’åº¦çš„è¯­ä¹‰ä¸åŒ¹é…é—®é¢˜ã€‚STEPè¿›ä¸€æ­¥é‡‡ç”¨è½»é‡çº§çš„ä»»åŠ¡ç‰¹å®šæç¤ºå¾®è°ƒ(prompt tuning)ç­–ç•¥ï¼Œé€šè¿‡å¯¹è¯å‰ç¼€(conversation prefix)å’Œæ¨èå‰ç¼€(recommendation prefix)å°†èåˆè¡¨ç¤ºæ³¨å…¥å†»ç»“çš„è¯­è¨€æ¨¡å‹ä¸­ï¼Œåœ¨å¹³è¡¡å¯¹è¯ç”Ÿæˆä¸æ¨èæ’åºä¸åŒç›®æ ‡çš„åŒæ—¶å®ç°è·¨ä»»åŠ¡è¯­ä¹‰å…±äº«ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSTEPåœ¨ä¸¤ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„æ¨èç²¾åº¦å’Œå¯¹è¯è´¨é‡å‡æ˜¾è‘—ä¼˜äºä¸»æµåŸºçº¿æ¨¡å‹ã€‚",
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages; 4 figures; 6 tables; code available at https://github.com/Alex-bupt/STEP",
      "pdf_url": "https://arxiv.org/pdf/2508.10669v1",
      "published_date": "2025-08-14 14:08:21 UTC",
      "updated_date": "2025-08-14 14:08:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:50:53.858112+00:00"
    },
    {
      "arxiv_id": "2508.10667v1",
      "title": "AddressVLM: Cross-view Alignment Tuning for Image Address Localization using Large Vision-Language Models",
      "title_zh": "AddressVLMï¼šåŸºäºå¤§è§†è§‰è¯­è¨€æ¨¡å‹çš„å›¾åƒåœ°å€å®šä½è·¨è§†å›¾å¯¹é½å¾®è°ƒ",
      "authors": [
        "Shixiong Xu",
        "Chenghao Zhang",
        "Lubin Fan",
        "Yuan Zhou",
        "Bin Fan",
        "Shiming Xiang",
        "Gaofeng Meng",
        "Jieping Ye"
      ],
      "abstract": "Large visual language models (LVLMs) have demonstrated impressive performance in coarse-grained geo-localization at the country or city level, but they struggle with fine-grained street-level localization within urban areas. In this paper, we explore integrating city-wide address localization capabilities into LVLMs, facilitating flexible address-related question answering using street-view images. A key challenge is that the street-view visual question-and-answer (VQA) data provides only microscopic visual cues, leading to subpar performance in fine-tuned models. To tackle this issue, we incorporate perspective-invariant satellite images as macro cues and propose cross-view alignment tuning including a satellite-view and street-view image grafting mechanism, along with an automatic label generation mechanism. Then LVLM's global understanding of street distribution is enhanced through cross-view matching. Our proposed model, named AddressVLM, consists of two-stage training protocols: cross-view alignment tuning and address localization tuning. Furthermore, we have constructed two street-view VQA datasets based on image address localization datasets from Pittsburgh and San Francisco. Qualitative and quantitative evaluations demonstrate that AddressVLM outperforms counterpart LVLMs by over 9% and 12% in average address localization accuracy on these two datasets, respectively.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AddressVLMï¼Œæ—¨åœ¨è§£å†³å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ (LVLMs) åœ¨åŸå¸‚è¡—é“çº§ç²¾ç»†åœ°ç†å®šä½ (Fine-grained street-level localization) æ–¹é¢çš„ä¸è¶³ã€‚ç”±äºè¡—é“è§†å›¾ä»…èƒ½æä¾›å¾®è§‚è§†è§‰çº¿ç´¢ï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†å…·æœ‰é€è§†ä¸å˜æ€§çš„å«æ˜Ÿå›¾åƒä½œä¸ºå®è§‚å‚è€ƒï¼Œå¹¶æå‡ºäº†è·¨è§†å›¾å¯¹é½å¾®è°ƒ (Cross-view alignment tuning) æ–¹æ¡ˆã€‚AddressVLM é‡‡ç”¨äº†å«æ˜Ÿè§†å›¾ä¸è¡—æ™¯å›¾åƒå«æ¥æœºåˆ¶ (Grafting mechanism) ä»¥åŠè‡ªåŠ¨æ ‡ç­¾ç”Ÿæˆæœºåˆ¶ï¼Œé€šè¿‡è·¨è§†å›¾åŒ¹é…å¢å¼ºæ¨¡å‹å¯¹è¡—é“åˆ†å¸ƒçš„å…¨å±€ç†è§£ã€‚æ¨¡å‹è®­ç»ƒè¿‡ç¨‹åˆ†ä¸ºè·¨è§†å›¾å¯¹é½å¾®è°ƒå’Œåœ°å€å®šä½å¾®è°ƒä¸¤ä¸ªé˜¶æ®µã€‚ç ”ç©¶äººå‘˜è¿˜åŸºäºåŒ¹å…¹å ¡å’Œæ—§é‡‘å±±çš„åœ°å€å®šä½æ•°æ®æ„å»ºäº†ä¸¤ä¸ªä¸“é—¨çš„è¡—æ™¯ VQA æ•°æ®é›†è¿›è¡ŒéªŒè¯ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼ŒAddressVLM åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„å¹³å‡åœ°å€å®šä½å‡†ç¡®ç‡æ¯”åŒç±» LVLMs åˆ†åˆ«æå‡äº† 9% å’Œ 12% ä»¥ä¸Šï¼Œè¯æ˜äº†ç»“åˆå¤šè§†å›¾ä¿¡æ¯åœ¨æå‡æ¨¡å‹ç²¾ç»†åŒ–å®šä½èƒ½åŠ›æ–¹é¢çš„æ˜¾è‘—ä½œç”¨ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10667v1",
      "published_date": "2025-08-14 14:06:28 UTC",
      "updated_date": "2025-08-14 14:06:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:50:52.164472+00:00"
    },
    {
      "arxiv_id": "2508.10666v1",
      "title": "Deep Learning in Classical and Quantum Physics",
      "title_zh": "ç»å…¸ä¸é‡å­ç‰©ç†ä¸­çš„æ·±åº¦å­¦ä¹ ",
      "authors": [
        "Timothy Heightman",
        "Marcin PÅ‚odzieÅ„"
      ],
      "abstract": "Scientific progress is tightly coupled to the emergence of new research tools. Today, machine learning (ML)-especially deep learning (DL)-has become a transformative instrument for quantum science and technology. Owing to the intrinsic complexity of quantum systems, DL enables efficient exploration of large parameter spaces, extraction of patterns from experimental data, and data-driven guidance for research directions. These capabilities already support tasks such as refining quantum control protocols and accelerating the discovery of materials with targeted quantum properties, making ML/DL literacy an essential skill for the next generation of quantum scientists. At the same time, DL's power brings risks: models can overfit noisy data, obscure causal structure, and yield results with limited physical interpretability. Recognizing these limitations and deploying mitigation strategies is crucial for scientific rigor. These lecture notes provide a comprehensive, graduate-level introduction to DL for quantum applications, combining conceptual exposition with hands-on examples. Organized as a progressive sequence, they aim to equip readers to decide when and how to apply DL effectively, to understand its practical constraints, and to adapt AI methods responsibly to problems across quantum physics, chemistry, and engineering.",
      "tldr_zh": "æœ¬è®²ä¹‰é’ˆå¯¹Classical and Quantum Physicsé¢†åŸŸä¸­çš„Deep Learning (DL)åº”ç”¨æä¾›äº†å…¨é¢çš„ç ”ç©¶ç”Ÿæ°´å¹³ä»‹ç»ï¼Œé˜é‡Šäº†ML/DLä½œä¸ºé‡å­ç§‘å­¦ä¸æŠ€æœ¯å˜é©æ€§å·¥å…·çš„é‡è¦åœ°ä½ã€‚é‰´äºé‡å­ç³»ç»Ÿå›ºæœ‰çš„å¤æ‚æ€§ï¼ŒDLåœ¨æ¢ç´¢å¤§è§„æ¨¡å‚æ•°ç©ºé—´ã€ä»å®éªŒæ•°æ®æå–æ¨¡å¼ä»¥åŠæä¾›æ•°æ®é©±åŠ¨çš„ç ”ç©¶æŒ‡å¯¼æ–¹é¢è¡¨ç°å“è¶Šã€‚è®²ä¹‰è¯¦ç»†æ¢è®¨äº†å…¶åœ¨ä¼˜åŒ–Quantum Controlåè®®å’ŒåŠ é€Ÿå…·æœ‰ç‰¹å®šé‡å­ç‰¹æ€§ææ–™çš„å‘ç°ç­‰ä»»åŠ¡ä¸­çš„å®é™…åº”ç”¨ï¼Œå¹¶å¼ºè°ƒäº†DLç´ å…»å·²æˆä¸ºæ–°ä¸€ä»£é‡å­ç§‘å­¦å®¶çš„æ ¸å¿ƒæŠ€èƒ½ã€‚åŒæ—¶ï¼Œè¯¥ç ”ç©¶æ·±å…¥å‰–æäº†DLå¯èƒ½å¸¦æ¥çš„Overfittingã€å› æœç»“æ„æ¨¡ç³ŠåŠç‰©ç†Interpretabilityä¸è¶³ç­‰æ½œåœ¨é£é™©ï¼Œå¹¶æå‡ºäº†ç›¸åº”çš„ç¼“è§£ç­–ç•¥ã€‚é€šè¿‡ç»“åˆç†è®ºæ¦‚å¿µä¸åŠ¨æ‰‹å®è·µæ¡ˆä¾‹ï¼Œè®²ä¹‰æ—¨åœ¨å¸®åŠ©è¯»è€…æŒæ¡åœ¨é‡å­ç‰©ç†ã€åŒ–å­¦åŠå·¥ç¨‹é¢†åŸŸæœ‰æ•ˆä¸”è´Ÿè´£ä»»åœ°åº”ç”¨AIæ–¹æ³•çš„èƒ½åŠ›ï¼Œå¹¶ä½¿å…¶èƒ½å¤Ÿæ·±å…¥ç†è§£æŠ€æœ¯çš„å®é™…çº¦æŸã€‚",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.NE",
        "physics.comp-ph"
      ],
      "primary_category": "quant-ph",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10666v1",
      "published_date": "2025-08-14 14:05:12 UTC",
      "updated_date": "2025-08-14 14:05:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:50:55.169200+00:00"
    },
    {
      "arxiv_id": "2508.10655v1",
      "title": "Serial Over Parallel: Learning Continual Unification for Multi-Modal Visual Object Tracking and Benchmarking",
      "title_zh": "ä¸²è¡Œä¼˜äºå¹¶è¡Œï¼šå¤šæ¨¡æ€è§†è§‰ç›®æ ‡è·Ÿè¸ªåŠåŸºå‡†æµ‹è¯•çš„æŒç»­ç»Ÿä¸€åŒ–å­¦ä¹ ",
      "authors": [
        "Zhangyong Tang",
        "Tianyang Xu",
        "Xuefeng Zhu",
        "Chunyang Cheng",
        "Tao Zhou",
        "Xiaojun Wu",
        "Josef Kittler"
      ],
      "abstract": "Unifying multiple multi-modal visual object tracking (MMVOT) tasks draws increasing attention due to the complementary nature of different modalities in building robust tracking systems. Existing practices mix all data sensor types in a single training procedure, structuring a parallel paradigm from the data-centric perspective and aiming for a global optimum on the joint distribution of the involved tasks. However, the absence of a unified benchmark where all types of data coexist forces evaluations on separated benchmarks, causing \\textit{inconsistency} between training and testing, thus leading to performance \\textit{degradation}. To address these issues, this work advances in two aspects: \\ding{182} A unified benchmark, coined as UniBench300, is introduced to bridge the inconsistency by incorporating multiple task data, reducing inference passes from three to one and cutting time consumption by 27\\%. \\ding{183} The unification process is reformulated in a serial format, progressively integrating new tasks. In this way, the performance degradation can be specified as knowledge forgetting of previous tasks, which naturally aligns with the philosophy of continual learning (CL), motivating further exploration of injecting CL into the unification process. Extensive experiments conducted on two baselines and four benchmarks demonstrate the significance of UniBench300 and the superiority of CL in supporting a stable unification process. Moreover, while conducting dedicated analyses, the performance degradation is found to be negatively correlated with network capacity. Additionally, modality discrepancies contribute to varying degradation levels across tasks (RGBT > RGBD > RGBE in MMVOT), offering valuable insights for future multi-modal vision research. Source codes and the proposed benchmark is available at \\textit{https://github.com/Zhangyong-Tang/UniBench300}.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤šæ¨¡æ€è§†è§‰ç›®æ ‡è·Ÿè¸ª (MMVOT) ä»»åŠ¡çš„ç»Ÿä¸€åŒ–é—®é¢˜ï¼ŒæŒ‡å‡ºå½“å‰çš„å¹¶è¡Œè®­ç»ƒèŒƒå¼ç”±äºç¼ºä¹ç»Ÿä¸€åŸºå‡†ï¼Œå¯¼è‡´è®­ç»ƒä¸æµ‹è¯•é˜¶æ®µå­˜åœ¨ä¸ä¸€è‡´æ€§åŠæ€§èƒ½ä¸‹é™ (Performance Degradation)ã€‚ä¸ºæ­¤ï¼Œä½œè€…å¼•å…¥äº†ç»Ÿä¸€åŸºå‡† UniBench300ï¼Œé€šè¿‡æ•´åˆå¤šç§ä»»åŠ¡æ•°æ®ï¼Œå°†æ¨ç†é¢‘æ¬¡ä»ä¸‰æ¬¡å‡å°‘è‡³ä¸€æ¬¡ï¼Œå¹¶ä½¿è€—æ—¶é™ä½äº† 27%ã€‚åœ¨æ–¹æ³•è®ºå±‚é¢ï¼Œè¯¥ç ”ç©¶å°†ç»Ÿä¸€åŒ–è¿‡ç¨‹é‡æ–°å®šä¹‰ä¸ºä¸²è¡Œæ¨¡å¼ (Serial Format)ï¼Œåˆ©ç”¨æŒç»­å­¦ä¹  (Continual Learning) é€æ­¥æ•´åˆæ–°ä»»åŠ¡ï¼Œæœ‰æ•ˆè§£å†³äº†çŸ¥è¯†é—å¿˜é—®é¢˜å¹¶æå‡äº†ç³»ç»Ÿçš„ç¨³å®šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ€§èƒ½ä¸‹é™ä¸ç½‘ç»œå®¹é‡ (Network Capacity) å‘ˆè´Ÿç›¸å…³ï¼Œä¸”ä¸åŒæ¨¡æ€é—´çš„å·®å¼‚ï¼ˆRGBT > RGBD > RGBEï¼‰ä¼šå¯¼è‡´ä¸åŒç¨‹åº¦çš„è¡°å‡ã€‚è¯¥ç ”ç©¶é€šè¿‡æå‡º UniBench300 å’Œä¸²è¡Œç»Ÿä¸€åŒ–æ¡†æ¶ï¼Œä¸ºå®ç°é«˜æ•ˆã€ç¨³å®šçš„å¤šæ¨¡æ€ç›®æ ‡è·Ÿè¸ªç³»ç»Ÿæä¾›äº†æ–°çš„è§†è§’å’Œè¯„ä¼°å·¥å…·ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "ACMMM 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.10655v1",
      "published_date": "2025-08-14 13:54:04 UTC",
      "updated_date": "2025-08-14 13:54:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:51:05.854215+00:00"
    },
    {
      "arxiv_id": "2508.13186v1",
      "title": "MM-BrowseComp: A Comprehensive Benchmark for Multimodal Browsing Agents",
      "title_zh": "MM-BrowseCompï¼šé¢å‘å¤šæ¨¡æ€æµè§ˆæ™ºèƒ½ä½“çš„ç»¼åˆæ€§åŸºå‡†",
      "authors": [
        "Shilong Li",
        "Xingyuan Bu",
        "Wenjie Wang",
        "Jiaheng Liu",
        "Jun Dong",
        "Haoyang He",
        "Hao Lu",
        "Haozhe Zhang",
        "Chenchen Jing",
        "Zhen Li",
        "Chuanhao Li",
        "Jiayi Tian",
        "Chenchen Zhang",
        "Tianhao Peng",
        "Yancheng He",
        "Jihao Gu",
        "Yuanxing Zhang",
        "Jian Yang",
        "Ge Zhang",
        "Wenhao Huang",
        "Wangchunshu Zhou",
        "Zhaoxiang Zhang",
        "Ruizhe Ding",
        "Shilei Wen"
      ],
      "abstract": "AI agents with advanced reasoning and tool use capabilities have demonstrated impressive performance in web browsing for deep search. While existing benchmarks such as BrowseComp evaluate these browsing abilities, they primarily focus on textual information, overlooking the prevalence of multimodal content. To bridge this gap, we introduce MM-BrowseComp, a novel benchmark comprising 224 challenging, hand-crafted questions specifically designed to assess agents' multimodal retrieval and reasoning capabilities. These questions often incorporate images in prompts, and crucial information encountered during the search and reasoning process may also be embedded within images or videos on webpages. Consequently, methods relying solely on text prove insufficient for our benchmark. Additionally, we provide a verified checklist for each question, enabling fine-grained analysis of multimodal dependencies and reasoning paths. Our comprehensive evaluation of state-of-the-art models on MM-BrowseComp reveals that even top models like OpenAI o3 with tools achieve only 29.02\\% accuracy, highlighting the suboptimal multimodal capabilities and lack of native multimodal reasoning in current models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MM-BrowseCompï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹å¤šæ¨¡æ€æµè§ˆæ™ºèƒ½ä½“(Multimodal Browsing Agents)çš„å…¨é¢åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨å¡«è¡¥ç°æœ‰åŸºå‡†BrowseCompä¸»è¦å…³æ³¨æ–‡æœ¬è€Œå¿½è§†å¤šæ¨¡æ€å†…å®¹çš„ç©ºç™½ã€‚è¯¥åŸºå‡†åŒ…å«224ä¸ªæ‰‹å·¥è®¾è®¡çš„æŒ‘æˆ˜æ€§é—®é¢˜ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°æ™ºèƒ½ä½“åœ¨æœç´¢å’Œæ¨ç†è¿‡ç¨‹ä¸­çš„å¤šæ¨¡æ€æ£€ç´¢(Retrieval)ä¸æ¨ç†(Reasoning)èƒ½åŠ›ã€‚è¿™äº›é—®é¢˜çš„æç¤ºè¯(Prompts)å¸¸åŒ…å«å›¾åƒï¼Œä¸”ç½‘é¡µä¸­çš„å…³é”®ä¿¡æ¯å¯èƒ½åµŒå…¥åœ¨å›¾åƒæˆ–è§†é¢‘ä¸­ï¼Œå¯¼è‡´çº¯æ–‡æœ¬æ–¹æ³•æ— æ³•æœ‰æ•ˆå¤„ç†ã€‚ç ”ç©¶è¿˜ä¸ºæ¯ä¸ªé—®é¢˜æä¾›äº†ç»è¿‡éªŒè¯çš„æ¸…å•(Checklist)ï¼Œæ”¯æŒå¯¹å¤šæ¨¡æ€ä¾èµ–å’Œæ¨ç†è·¯å¾„è¿›è¡Œç»†ç²’åº¦åˆ†æã€‚å¯¹å½“å‰é¡¶å°–æ¨¡å‹çš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼Œå³ä½¿æ˜¯OpenAI o3åœ¨é…å¤‡å·¥å…·çš„æƒ…å†µä¸‹å‡†ç¡®ç‡ä¹Ÿä»…ä¸º29.02%ï¼Œæ­ç¤ºäº†ç°æœ‰æ¨¡å‹åœ¨åŸç”Ÿå¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ä¸Šçš„æ˜¾è‘—ä¸è¶³ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "The first two authors contribute equally, 26 pages, repo at https://github.com/MMBrowseComp/MM-BrowseComp",
      "pdf_url": "https://arxiv.org/pdf/2508.13186v1",
      "published_date": "2025-08-14 13:46:47 UTC",
      "updated_date": "2025-08-14 13:46:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:51:03.863917+00:00"
    },
    {
      "arxiv_id": "2508.10646v2",
      "title": "SPHENIC: Topology-Aware Multi-View Clustering for Spatial Transcriptomics",
      "title_zh": "SPHENICï¼šé¢å‘ç©ºé—´è½¬å½•ç»„å­¦çš„æ‹“æ‰‘æ„ŸçŸ¥å¤šè§†å›¾èšç±»",
      "authors": [
        "Chenkai Guo",
        "Yikai Zhu",
        "Renxiang Guan",
        "Jinli Ma",
        "Siwei Wang",
        "Ke Liang",
        "Guangdun Peng",
        "Dayu Hu"
      ],
      "abstract": "Spatial transcriptomics clustering is pivotal for identifying cell subpopulations by leveraging spatial location information. While recent graph-based methods modeling cell-cell interactions have improved clustering accuracy, they remain limited in two key aspects: (i) reliance on local aggregation in static graphs often fails to capture robust global topological structures (e.g., loops and voids) and is vulnerable to noisy edges; and (ii) dimensionality reduction techniques frequently neglect spatial coherence, causing physically adjacent spots to be erroneously separated in the latent space. To overcome these challenges, we propose SPHENIC, a Spatial Persistent Homology-Enhanced Neighborhood Integrative Clustering method. Specifically, it explicitly incorporates topology-invariant features into the clustering network to ensure robust representation learning against noise. Furthermore, we design a dual-regularized optimization module that imposes spatial constraints alongside distributional optimization, ensuring that the embedding space preserves the physical proximity of cells. Extensive experiments on 11 benchmark datasets demonstrate that SPHENIC outperforms state-of-the-art methods by 4.19%-9.14%, validating its superiority in characterizing complex tissue architectures.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SPHENICï¼Œä¸€ç§ç©ºé—´æŒä¹…åŒè°ƒå¢å¼ºçš„é‚»åŸŸé›†æˆèšç±»æ–¹æ³•(Spatial Persistent Homology-Enhanced Neighborhood Integrative Clustering)ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ç©ºé—´è½¬å½•ç»„å­¦èšç±»æ–¹æ³•åœ¨æ•æ‰å…¨å±€æ‹“æ‰‘ç»“æ„å’Œä¿æŒç©ºé—´ä¸€è‡´æ€§æ–¹é¢çš„å±€é™ã€‚è¯¥æ–¹æ³•é€šè¿‡æ˜¾å¼åœ°å°†æ‹“æ‰‘ä¸å˜ç‰¹å¾(topology-invariant features)å¼•å…¥èšç±»ç½‘ç»œï¼Œæœ‰æ•ˆæå‡äº†æ¨¡å‹åœ¨é¢å¯¹å™ªå£°è¾¹ç¼˜æ—¶çš„é²æ£’æ€§ï¼Œå¹¶èƒ½æ›´å¥½åœ°è¯†åˆ«ç»„ç»‡ä¸­çš„ç©ºè…”æˆ–ç¯è·¯ç­‰å¤æ‚æ‹“æ‰‘ç‰¹å¾ã€‚æ­¤å¤–ï¼Œç ”ç©¶è®¾è®¡äº†ä¸€ä¸ªåŒé‡æ­£åˆ™åŒ–ä¼˜åŒ–æ¨¡å—(dual-regularized optimization module)ï¼Œé€šè¿‡æ–½åŠ ç©ºé—´çº¦æŸç¡®ä¿ç‰©ç†é‚»è¿‘çš„ä½ç‚¹åœ¨åµŒå…¥ç©ºé—´ä¸­å¾—åˆ°ä¿ç•™ã€‚åœ¨11ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSPHENICçš„æ€§èƒ½ä¼˜äºå½“å‰æœ€å…ˆè¿›æ–¹æ³•4.19%-9.14%ï¼Œå……åˆ†éªŒè¯äº†å…¶åœ¨è¯†åˆ«ç»†èƒäºšç¾¤å’Œè¡¨å¾å¤æ‚ç»„ç»‡æ¶æ„æ–¹é¢çš„ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 5 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2508.10646v2",
      "published_date": "2025-08-14 13:43:28 UTC",
      "updated_date": "2026-01-19 18:56:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:51:20.387445+00:00"
    },
    {
      "arxiv_id": "2508.16624v3",
      "title": "The GPT-4o Shock Emotional Attachment to AI Models and Its Impact on Regulatory Acceptance: A Cross-Cultural Analysis of the Immediate Transition from GPT-4o to GPT-5",
      "title_zh": "GPT-4o å†²å‡»ï¼šå¯¹ AI æ¨¡å‹çš„æƒ…æ„Ÿä¾æ‹åŠå…¶å¯¹ç›‘ç®¡æ¥å—åº¦çš„å½±å“â€”â€”ä» GPT-4o åˆ° GPT-5 å³æ—¶è¿‡æ¸¡çš„è·¨æ–‡åŒ–åˆ†æ",
      "authors": [
        "Hiroki Naito"
      ],
      "abstract": "In August 2025, a major AI company's immediate, mandatory transition from its previous to its next-generation model triggered widespread public reactions. I collected 150 posts in Japanese and English from multiple social media platforms and video-sharing services between August 8-9, 2025, and qualitatively analyzed expressions of emotional attachment and resistance. Users often described GPT-4o as a trusted partner or AI boyfriend, suggesting person-like bonds. Japanese posts were dominated by loss-oriented narratives, whereas English posts included more anger, meta-level critique, and memes.A preliminary quantitative check showed a statistically significant difference in attachment coding between Japanese and English posts, with substantially higher attachment observed in the Japanese data. The findings suggest that for attachment-heavy models, even safety-oriented changes can face rapid, large-scale resistance that narrows the practical window for behavioral control. If future AI robots capable of inducing emotional bonds become widespread in the physical world, such attachment could surpass the ability to enforce regulation at an even earlier stage than in digital settings. Policy options include gradual transitions, parallel availability, and proactive measurement of attachment thresholds and points of no return to prevent emotional dynamics from outpacing effective governance.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† GPT-4o å‘ GPT-5 å¼ºåˆ¶è½¬æ¢è¿‡ç¨‹ä¸­ç”¨æˆ·äº§ç”Ÿçš„æƒ…æ„Ÿä¾æ‹ (emotional attachment) åŠå…¶å¯¹ç›‘ç®¡æ¥å—åº¦çš„å½±å“ã€‚ç ”ç©¶é€šè¿‡å®šæ€§åˆ†æ 150 æ¡æ—¥è¯­å’Œè‹±è¯­ç¤¾äº¤åª’ä½“å¸–å­ï¼Œå‘ç°ç”¨æˆ·å¸¸å°† GPT-4o è§†ä¸ºä¿¡ä»»çš„ä¼™ä¼´ï¼Œç”šè‡³äº§ç”Ÿäº†ç±»äººæƒ…æ„Ÿçº½å¸¦ã€‚è·¨æ–‡åŒ–å¯¹æ¯”æ˜¾ç¤ºï¼Œæ—¥æœ¬ç”¨æˆ·çš„ååº”ä»¥ä¸§å¤±æ„Ÿä¸ºä¸»ï¼Œè€Œè‹±è¯­ç”¨æˆ·åˆ™æ›´å¤šè¡¨ç°å‡ºæ„¤æ€’å’Œå…ƒçº§æ‰¹åˆ¤ (meta-level critique)ï¼Œä¸”å®šé‡ç»“æœè¯å®æ—¥æœ¬ç”¨æˆ·çš„æƒ…æ„Ÿä¾æ‹æ˜¾è‘—æ›´é«˜ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œè¿™ç§å¼ºçƒˆçš„æƒ…æ„Ÿä¾æ‹ä¼šå¼•å‘å¯¹æ¨¡å‹å˜æ›´çš„å¤§è§„æ¨¡æŠµåˆ¶ï¼Œæ˜¾è‘—ç¼©å°äº†è¡Œä¸ºæ§åˆ¶çš„å®é™…ç›‘ç®¡çª—å£ã€‚éšç€æœªæ¥å…·å¤‡è¯±å¯¼æƒ…æ„Ÿèƒ½åŠ›çš„ AI æœºå™¨äººåœ¨ç‰©ç†ä¸–ç•Œæ™®åŠï¼Œè¿™ç§ä¾æ‹å¯èƒ½åœ¨æ—©æœŸé˜¶æ®µå°±è¶…è¶Šç›‘ç®¡çš„æ‰§è¡Œèƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡å»ºè®®æ”¿ç­–åˆ¶å®šè€…é‡‡å–æ¸è¿›å¼è¿‡æ¸¡ã€åŒç‰ˆæœ¬å¹¶è¡Œä»¥åŠä¸»åŠ¨æµ‹é‡æƒ…æ„Ÿä¾æ‹é˜ˆå€¼ç­‰æ‰‹æ®µï¼Œä»¥é˜²æ­¢æƒ…æ„ŸåŠ¨æ€æ¼”å˜è¶…å‡ºæœ‰æ•ˆæ²»ç†çš„èŒƒç•´ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "9 pages ,3 tables",
      "pdf_url": "https://arxiv.org/pdf/2508.16624v3",
      "published_date": "2025-08-14 13:36:58 UTC",
      "updated_date": "2026-01-05 02:20:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:51:27.799642+00:00"
    },
    {
      "arxiv_id": "2508.10616v2",
      "title": "Fourier-Guided Attention Upsampling for Image Super-Resolution",
      "title_zh": "é¢å‘å›¾åƒè¶…åˆ†è¾¨ç‡çš„å‚…é‡Œå¶å¼•å¯¼æ³¨æ„åŠ›ä¸Šé‡‡æ ·",
      "authors": [
        "Daejune Choi",
        "Youchan No",
        "Jinhyung Lee",
        "Duksu Kim"
      ],
      "abstract": "We propose Frequency-Guided Attention (FGA), a lightweight upsampling module for single image super-resolution. Conventional upsamplers, such as Sub-Pixel Convolution, are efficient but frequently fail to reconstruct high-frequency details and introduce aliasing artifacts. FGA addresses these issues by integrating (1) a Fourier feature-based Multi-Layer Perceptron (MLP) for positional frequency encoding, (2) a cross-resolution Correlation Attention Layer for adaptive spatial alignment, and (3) a frequency-domain L1 loss for spectral fidelity supervision. Adding merely 0.3M parameters, FGA consistently enhances performance across five diverse super-resolution backbones in both lightweight and full-capacity scenarios. Experimental results demonstrate average PSNR gains of 0.12~0.14 dB and improved frequency-domain consistency by up to 29%, particularly evident on texture-rich datasets. Visual and spectral evaluations confirm FGA's effectiveness in reducing aliasing and preserving fine details, establishing it as a practical, scalable alternative to traditional upsampling methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å•å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆSISRï¼‰ä¸­ä¼ ç»Ÿä¸Šé‡‡æ ·æ–¹æ³•ï¼ˆå¦‚ Sub-Pixel Convolutionï¼‰åœ¨æ¢å¤é«˜é¢‘ç»†èŠ‚å’Œå‡å°‘æ··å ä¼ªå½±ï¼ˆaliasing artifactsï¼‰æ–¹é¢çš„ä¸è¶³ï¼Œæå‡ºäº†åä¸º Frequency-Guided Attention (FGA) çš„è½»é‡çº§ä¸Šé‡‡æ ·æ¨¡å—ã€‚FGA æ ¸å¿ƒç”±ä¸‰ä¸ªå…³é”®ç»„ä»¶æ„æˆï¼ŒåŒ…æ‹¬ç”¨äºä½ç½®é¢‘ç‡ç¼–ç çš„åŸºäº Fourier ç‰¹å¾çš„å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ï¼Œä»¥åŠç”¨äºè‡ªé€‚åº”ç©ºé—´å¯¹é½çš„è·¨åˆ†è¾¨ç‡ç›¸å…³æ³¨æ„åŠ›å±‚ï¼ˆCorrelation Attention Layerï¼‰ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¼•å…¥äº†é¢‘åŸŸ L1 æŸå¤±ï¼ˆfrequency-domain L1 lossï¼‰è¿›è¡Œé¢‘è°±ä¿çœŸåº¦ç›‘ç£ã€‚FGA ä»…å¢åŠ äº† 0.3M çš„å‚æ•°é‡ï¼Œå´åœ¨è½»é‡çº§å’Œå…¨å®¹é‡åœºæ™¯ä¸‹çš„äº”ç§ä¸åŒè¶…åˆ†è¾¨ç‡éª¨å¹²ç½‘ç»œä¸Šå‡å®ç°äº†æ€§èƒ½æŒç»­æå‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFGA å¹³å‡æå‡äº† 0.12 è‡³ 0.14 dB çš„ PSNRï¼Œå¹¶å°†é¢‘åŸŸä¸€è‡´æ€§æœ€é«˜æé«˜äº† 29%ï¼Œåœ¨çº¹ç†ä¸°å¯Œçš„å›¾åƒä¸Šæ•ˆæœå°¤ä¸ºæ˜¾è‘—ã€‚è§†è§‰å’Œé¢‘è°±è¯„ä¼°è¿›ä¸€æ­¥è¯å®ï¼ŒFGA åœ¨å‡å°‘æ··å å’Œä¿ç•™å¾®ç»†ç»†èŠ‚æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œæ˜¯ä¼ ç»Ÿä¸Šé‡‡æ ·æ–¹æ³•çš„ä¸€ä¸ªå®ç”¨ä¸”å¯æ‰©å±•çš„æ›¿ä»£æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "15 pages, 7 figures, under submission to a journal",
      "pdf_url": "https://arxiv.org/pdf/2508.10616v2",
      "published_date": "2025-08-14 13:13:17 UTC",
      "updated_date": "2025-08-23 06:41:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:51:32.558035+00:00"
    },
    {
      "arxiv_id": "2508.10599v3",
      "title": "MSRS: Adaptive Multi-Subspace Representation Steering for Attribute Alignment in Large Language Models",
      "title_zh": "MSRSï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹å±æ€§å¯¹é½çš„è‡ªé€‚åº”å¤šå­ç©ºé—´è¡¨å¾å¼•å¯¼",
      "authors": [
        "Xinyan Jiang",
        "Lin Zhang",
        "Jiayi Zhang",
        "Qingsong Yang",
        "Guimin Hu",
        "Di Wang",
        "Lijie Hu"
      ],
      "abstract": "Activation steering offers a promising approach to controlling the behavior of Large Language Models by directly manipulating their internal activations. However, most existing methods struggle to jointly steer multiple attributes, often resulting in interference and undesirable trade-offs. To address this challenge, we propose Multi-Subspace Representation Steering (MSRS), a novel framework for effective multi-attribute steering via subspace representation fine-tuning. MSRS reduces inter-attribute interference by allocating orthogonal subspaces to each attribute, isolating their influence within the model's representation space. MSRS also incorporates a hybrid subspace composition strategy: it combines attribute-specific subspaces for unique steering directions with a shared subspace for common steering directions. A dynamic weighting function learns to efficiently integrate these components for precise control. During inference, MSRS introduces a token-level steering mechanism that dynamically identifies and intervenes on the most semantically relevant tokens, enabling fine-grained behavioral modulation. Experimental results show that MSRS significantly reduces attribute conflicts, surpasses existing methods across a range of attributes, and generalizes effectively to diverse downstream tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MSRSï¼ˆMulti-Subspace Representation Steeringï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šå±æ€§å¯¹é½æ—¶å¸¸é¢ä¸´çš„å±æ€§å¹²æ‰°å’Œæƒè¡¡æŒ‘æˆ˜çš„è‡ªé€‚åº”æ¡†æ¶ã€‚MSRSé€šè¿‡ä¸ºæ¯ä¸ªå±æ€§åˆ†é…æ­£äº¤å­ç©ºé—´ï¼ˆorthogonal subspacesï¼‰ï¼Œåœ¨è¡¨ç¤ºç©ºé—´ä¸­æœ‰æ•ˆéš”ç¦»äº†å„å±æ€§çš„å½±å“ï¼Œä»è€Œå‡å°‘äº†å±æ€§é—´çš„ç›¸äº’å¹²æ‰°ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†æ··åˆå­ç©ºé—´ç»„åˆç­–ç•¥ï¼Œç»“åˆå±æ€§ç‰¹å®šå­ç©ºé—´å’Œå…±äº«å­ç©ºé—´ï¼Œå¹¶é€šè¿‡åŠ¨æ€æƒé‡å‡½æ•°å®ç°ç²¾ç¡®çš„è¡¨ç¤ºå¼•å¯¼ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒMSRSåˆ©ç”¨æ ‡è®°çº§ï¼ˆtoken-levelï¼‰å¼•å¯¼æœºåˆ¶åŠ¨æ€è¯†åˆ«å¹¶å¹²é¢„å…³é”®è¯­ä¹‰æ ‡è®°ï¼Œå®ç°äº†ç»†ç²’åº¦çš„è¡Œä¸ºè°ƒèŠ‚ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒMSRSèƒ½æ˜¾è‘—é™ä½å±æ€§å†²çªï¼Œåœ¨å¤šå±æ€§æ§åˆ¶è¡¨ç°ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶å±•ç°å‡ºè‰¯å¥½çš„ä¸‹æ¸¸ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10599v3",
      "published_date": "2025-08-14 12:40:19 UTC",
      "updated_date": "2025-11-21 05:56:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:51:31.297091+00:00"
    },
    {
      "arxiv_id": "2508.10595v1",
      "title": "On Spectral Properties of Gradient-based Explanation Methods",
      "title_zh": "è®ºåŸºäºæ¢¯åº¦çš„è§£é‡Šæ–¹æ³•çš„è°±æ€§è´¨",
      "authors": [
        "Amir Mehrpanah",
        "Erik Englesson",
        "Hossein Azizpour"
      ],
      "abstract": "Understanding the behavior of deep networks is crucial to increase our confidence in their results. Despite an extensive body of work for explaining their predictions, researchers have faced reliability issues, which can be attributed to insufficient formalism. In our research, we adopt novel probabilistic and spectral perspectives to formally analyze explanation methods. Our study reveals a pervasive spectral bias stemming from the use of gradient, and sheds light on some common design choices that have been discovered experimentally, in particular, the use of squared gradient and input perturbation. We further characterize how the choice of perturbation hyperparameters in explanation methods, such as SmoothGrad, can lead to inconsistent explanations and introduce two remedies based on our proposed formalism: (i) a mechanism to determine a standard perturbation scale, and (ii) an aggregation method which we call SpectralLens. Finally, we substantiate our theoretical results through quantitative evaluations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦ç½‘ç»œè§£é‡Šæ–¹æ³•ï¼ˆexplanation methodsï¼‰ä¸­å­˜åœ¨çš„å¯é æ€§é—®é¢˜ï¼Œä»æ¦‚ç‡å’Œå…‰è°±ï¼ˆspectralï¼‰çš„æ–°è§†è§’å¯¹åŸºäºæ¢¯åº¦çš„è§£é‡Šæ–¹æ³•è¿›è¡Œäº†å½¢å¼åŒ–åˆ†æã€‚ç ”ç©¶æ­ç¤ºäº†ç”±äºä½¿ç”¨æ¢¯åº¦è€Œäº§ç”Ÿçš„æ™®éå…‰è°±åå·®ï¼ˆspectral biasï¼‰ï¼Œå¹¶ä»ç†è®ºä¸Šé˜æ˜äº†å®éªŒä¸­å‘ç°çš„å¸¸è§è®¾è®¡é€‰æ‹©ï¼Œç‰¹åˆ«æ˜¯å¹³æ–¹æ¢¯åº¦ï¼ˆsquared gradientï¼‰å’Œè¾“å…¥æ‰°åŠ¨ï¼ˆinput perturbationï¼‰çš„æœ‰æ•ˆæ€§ã€‚è®ºæ–‡è¿›ä¸€æ­¥è¡¨å¾äº†è¯¸å¦‚ SmoothGrad ç­‰è§£é‡Šæ–¹æ³•ä¸­æ‰°åŠ¨è¶…å‚æ•°çš„é€‰æ‹©å¦‚ä½•å¯¼è‡´è§£é‡Šçš„ä¸ä¸€è‡´æ€§ã€‚åŸºäºæ‰€æå‡ºçš„å½¢å¼åŒ–æ¡†æ¶ï¼Œç ”ç©¶æå‡ºäº†ä¸¤ç§è¡¥æ•‘æªæ–½ï¼šä¸€ç§æ˜¯ç¡®å®šæ ‡å‡†æ‰°åŠ¨å°ºåº¦çš„æœºåˆ¶ï¼Œå¦ä¸€ç§æ˜¯åä¸º SpectralLens çš„èšåˆæ–¹æ³•ã€‚æœ€åï¼Œç ”ç©¶é€šè¿‡å®šé‡è¯„ä¼°è¯å®äº†å…¶ç†è®ºç»“æœï¼Œä¸ºå¢å¼ºè§£é‡Šæ–¹æ³•çš„å¯é æ€§æä¾›äº†é‡è¦çš„ç†è®ºæ”¯æ’‘ä¸å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "36 pages, 16 figures, published in European Conference on Computer Vision 2024",
      "pdf_url": "https://arxiv.org/pdf/2508.10595v1",
      "published_date": "2025-08-14 12:37:22 UTC",
      "updated_date": "2025-08-14 12:37:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:51:32.754695+00:00"
    },
    {
      "arxiv_id": "2508.10594v2",
      "title": "FreeGAD: A Training-Free yet Effective Approach for Graph Anomaly Detection",
      "title_zh": "FreeGADï¼šä¸€ç§æ— éœ€è®­ç»ƒä¸”æœ‰æ•ˆçš„å›¾å¼‚å¸¸æ£€æµ‹æ–¹æ³•",
      "authors": [
        "Yunfeng Zhao",
        "Yixin Liu",
        "Shiyuan Li",
        "Qingfeng Chen",
        "Yu Zheng",
        "Shirui Pan"
      ],
      "abstract": "Graph Anomaly Detection (GAD) aims to identify nodes that deviate from the majority within a graph, playing a crucial role in applications such as social networks and e-commerce. Despite the current advancements in deep learning-based GAD, existing approaches often suffer from high deployment costs and poor scalability due to their complex and resource-intensive training processes. Surprisingly, our empirical findings suggest that the training phase of deep GAD methods, commonly perceived as crucial, may actually contribute less to anomaly detection performance than expected. Inspired by this, we propose FreeGAD, a novel training-free yet effective GAD method. Specifically, it leverages an affinity-gated residual encoder to generate anomaly-aware representations. Meanwhile, FreeGAD identifies anchor nodes as pseudo-normal and anomalous guides, followed by calculating anomaly scores through anchor-guided statistical deviations. Extensive experiments demonstrate that FreeGAD achieves superior anomaly detection performance, efficiency, and scalability on multiple benchmark datasets from diverse domains, without any training or iterative optimization.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å›¾å¼‚å¸¸æ£€æµ‹ (Graph Anomaly Detection, GAD) åœ¨æ·±åº¦å­¦ä¹ æ–¹æ³•ä¸­é¢ä¸´çš„é«˜éƒ¨ç½²æˆæœ¬å’Œä½å¯æ‰©å±•æ€§é—®é¢˜ï¼Œæå‡ºäº†åä¸º FreeGAD çš„æ— éœ€è®­ç»ƒä¸”é«˜æ•ˆçš„æ£€æµ‹æ–¹æ³•ã€‚åŸºäºè®­ç»ƒè¿‡ç¨‹å¯¹ GAD æ€§èƒ½è´¡çŒ®å¯èƒ½ä½äºé¢„æœŸçš„å‘ç°ï¼ŒFreeGAD åˆ©ç”¨äº²å’ŒåŠ›é—¨æ§æ®‹å·®ç¼–ç å™¨ (affinity-gated residual encoder) æ¥ç”Ÿæˆå¼‚å¸¸æ„ŸçŸ¥è¡¨ç¤ºã€‚è¯¥æ–¹æ³•è¿›ä¸€æ­¥é€šè¿‡è¯†åˆ«é”šèŠ‚ç‚¹ä½œä¸ºä¼ªæ­£å¸¸å’Œå¼‚å¸¸å¼•å¯¼ï¼Œå¹¶åˆ©ç”¨é”šç‚¹å¼•å¯¼çš„ç»Ÿè®¡åå·® (anchor-guided statistical deviations) è®¡ç®—å¼‚å¸¸åˆ†æ•°ã€‚åœ¨å¤šä¸ªé¢†åŸŸçš„åŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜ï¼ŒFreeGAD åœ¨æ— éœ€ä»»ä½•è®­ç»ƒæˆ–è¿­ä»£ä¼˜åŒ–çš„æƒ…å½¢ä¸‹ï¼Œåœ¨æ£€æµ‹æ€§èƒ½ã€è¿è¡Œæ•ˆç‡å’Œç³»ç»Ÿå¯æ‰©å±•æ€§ä¸Šå‡å±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸ºå¤§è§„æ¨¡å›¾æ•°æ®çš„å¼‚å¸¸è¯†åˆ«æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by ClKM 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.10594v2",
      "published_date": "2025-08-14 12:37:20 UTC",
      "updated_date": "2025-08-19 01:47:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:51:44.093673+00:00"
    },
    {
      "arxiv_id": "2508.10559v1",
      "title": "Fake Speech Wild: Detecting Deepfake Speech on Social Media Platform",
      "title_zh": "Fake Speech Wildï¼šç¤¾äº¤åª’ä½“å¹³å°ä¸Šçš„æ·±åº¦ä¼ªé€ è¯­éŸ³æ£€æµ‹",
      "authors": [
        "Yuankun Xie",
        "Ruibo Fu",
        "Xiaopeng Wang",
        "Zhiyong Wang",
        "Ya Li",
        "Zhengqi Wen",
        "Haonnan Cheng",
        "Long Ye"
      ],
      "abstract": "The rapid advancement of speech generation technology has led to the widespread proliferation of deepfake speech across social media platforms. While deepfake audio countermeasures (CMs) achieve promising results on public datasets, their performance degrades significantly in cross-domain scenarios. To advance CMs for real-world deepfake detection, we first propose the Fake Speech Wild (FSW) dataset, which includes 254 hours of real and deepfake audio from four different media platforms, focusing on social media. As CMs, we establish a benchmark using public datasets and advanced selfsupervised learning (SSL)-based CMs to evaluate current CMs in real-world scenarios. We also assess the effectiveness of data augmentation strategies in enhancing CM robustness for detecting deepfake speech on social media. Finally, by augmenting public datasets and incorporating the FSW training set, we significantly advanced real-world deepfake audio detection performance, achieving an average equal error rate (EER) of 3.54% across all evaluation sets.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹ç¤¾äº¤åª’ä½“ä¸Šæ·±åº¦ä¼ªé€ è¯­éŸ³(deepfake speech)æ³›æ»¥çš„é—®é¢˜ï¼Œæå‡ºäº†Fake Speech Wild (FSW)æ•°æ®é›†ï¼Œæ—¨åœ¨æå‡ç°å®ç¯å¢ƒä¸­çš„æ£€æµ‹æ€§èƒ½ã€‚è¯¥æ•°æ®é›†åŒ…å«254å°æ—¶æ¥è‡ªå››ä¸ªä¸åŒåª’ä½“å¹³å°çš„çœŸå®å’Œä¼ªé€ éŸ³é¢‘ï¼Œä¸“æ³¨äºè§£å†³ç°æœ‰å¯¹ç­–(CMs)åœ¨è·¨åŸŸåœºæ™¯ä¸‹æ€§èƒ½æ˜¾è‘—ä¸‹é™çš„é—®é¢˜ã€‚ç ”ç©¶é€šè¿‡åˆ©ç”¨å…ˆè¿›çš„è‡ªç›‘ç£å­¦ä¹ (SSL)æ¨¡å‹å»ºç«‹åŸºå‡†ï¼Œå¹¶è¯„ä¼°äº†æ•°æ®å¢å¼º(data augmentation)ç­–ç•¥åœ¨æé«˜æ£€æµ‹ç¨³å¥æ€§æ–¹é¢çš„ä½œç”¨ã€‚å®éªŒè¡¨æ˜ï¼Œé€šè¿‡ç»“åˆFSWè®­ç»ƒé›†å’Œå¢å¼ºåçš„å…¬å…±æ•°æ®é›†ï¼Œç ”ç©¶åœ¨æ‰€æœ‰è¯„ä¼°é›†ä¸­å®ç°äº†3.54%çš„å¹³å‡ç­‰é”™ç‡(EER)ï¼Œæ˜¾è‘—æ¨è¿›äº†çœŸå®ç¤¾äº¤åª’ä½“ç¯å¢ƒä¸‹çš„æ·±åº¦ä¼ªé€ éŸ³é¢‘æ£€æµ‹æŠ€æœ¯ã€‚è¯¥æˆæœä¸ºå¼€å‘å¯éƒ¨ç½²çš„ã€å¯¹æŠ—å¤æ‚è¯­éŸ³æ¬ºè¯ˆçš„ç³»ç»Ÿå¥ å®šäº†åšå®åŸºç¡€ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10559v1",
      "published_date": "2025-08-14 11:56:30 UTC",
      "updated_date": "2025-08-14 11:56:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:51:47.186315+00:00"
    },
    {
      "arxiv_id": "2508.10557v2",
      "title": "PTQAT: A Hybrid Parameter-Efficient Quantization Algorithm for 3D Perception Tasks",
      "title_zh": "PTQATï¼šé¢å‘3Dæ„ŸçŸ¥ä»»åŠ¡çš„æ··åˆå‚æ•°é«˜æ•ˆé‡åŒ–ç®—æ³•",
      "authors": [
        "Xinhao Wang",
        "Zhiwei Lin",
        "Zhongyu Xia",
        "Yongtao Wang"
      ],
      "abstract": "Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) represent two mainstream model quantization approaches. However, PTQ often leads to unacceptable performance degradation in quantized models, while QAT imposes substantial GPU memory requirements and extended training time due to weight fine-tuning. In this paper, we propose PTQAT, a novel general hybrid quantization algorithm for the efficient deployment of 3D perception networks. To address the speed accuracy trade-off between PTQ and QAT, our method selects critical layers for QAT fine-tuning and performs PTQ on the remaining layers. Contrary to intuition, fine-tuning the layers with smaller output discrepancies before and after quantization, rather than those with larger discrepancies, actually leads to greater improvements in the model's quantization accuracy. This means we better compensate for quantization errors during their propagation, rather than addressing them at the point where they occur. The proposed PTQAT achieves similar performance to QAT with more efficiency by freezing nearly 50% of quantifiable layers. Additionally, PTQAT is a universal quantization method that supports various quantization bit widths (4 bits) as well as different model architectures, including CNNs and Transformers. The experimental results on nuScenes across diverse 3D perception tasks, including object detection, semantic segmentation, and occupancy prediction, show that our method consistently outperforms QAT-only baselines. Notably, it achieves 0.2%-0.9% NDS and 0.3%-1.0% mAP gains in object detection, 0.3%-2.0% mIoU gains in semantic segmentation and occupancy prediction while fine-tuning fewer weights.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹ 3D æ„ŸçŸ¥ç½‘ç»œéƒ¨ç½²ä¸­çš„é‡åŒ–éš¾é¢˜ï¼Œæå‡ºäº† PTQATï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆäº†è®­ç»ƒåé‡åŒ– (Post-Training Quantization, PTQ) å’Œé‡åŒ–æ„ŸçŸ¥è®­ç»ƒ (Quantization-Aware Training, QAT) ä¼˜åŠ¿çš„é€šç”¨æ··åˆå‚æ•°é«˜æ•ˆé‡åŒ–ç®—æ³•ã€‚è¯¥ç®—æ³•é€šè¿‡é€‰æ‹©å…³é”®å±‚è¿›è¡Œ QAT å¾®è°ƒå¹¶å¯¹å‰©ä½™å±‚åº”ç”¨ PTQï¼Œæœ‰æ•ˆè§£å†³äº†é‡åŒ–ç²¾åº¦ä¸è®­ç»ƒæ•ˆç‡ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚ç ”ç©¶å‘ç°ï¼Œå¾®è°ƒé‡åŒ–å‰åè¾“å‡ºå·®å¼‚è¾ƒå°çš„å±‚ï¼ˆè€Œéä¼ ç»Ÿè®¤çŸ¥ä¸­å·®å¼‚è¾ƒå¤§çš„å±‚ï¼‰èƒ½æ›´æœ‰æ•ˆåœ°åœ¨è¯¯å·®ä¼ æ’­è¿‡ç¨‹ä¸­è¿›è¡Œè¡¥å¿ï¼Œä»è€Œæ˜¾è‘—æå‡é‡åŒ–ç²¾åº¦ã€‚PTQAT åœ¨å†»ç»“è¿‘ 50% å¯é‡åŒ–å±‚çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†ä¸ QAT ç›¸å½“çš„æ€§èƒ½ï¼Œä¸”æ”¯æŒå¤šç§æ¨¡å‹æ¶æ„å¦‚ CNN å’Œ Transformer ä»¥åŠ 4-bit ç­‰ä¸åŒä½å®½ã€‚åœ¨ nuScenes æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ 3D ç›®æ ‡æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²å’Œå æ®é¢„æµ‹ (Occupancy Prediction) ç­‰å¤šé¡¹ä»»åŠ¡ä¸­å‡ä¼˜äºä»…ä½¿ç”¨ QAT çš„åŸºçº¿ã€‚å…·ä½“è€Œè¨€ï¼Œå®ƒåœ¨å¾®è°ƒæ›´å°‘æƒé‡çš„å‰æä¸‹ï¼Œåœ¨æ£€æµ‹ä»»åŠ¡ä¸­æå‡äº† 0.2%-0.9% çš„ NDS å’Œ 0.3%-1.0% çš„ mAPï¼Œå¹¶åœ¨è¯­ä¹‰åˆ†å‰²ä¸å æ®é¢„æµ‹ä»»åŠ¡ä¸­è·å¾—äº† 0.3%-2.0% çš„ mIoU å¢ç›Šã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, Accepted by ICCVW 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.10557v2",
      "published_date": "2025-08-14 11:55:21 UTC",
      "updated_date": "2025-08-15 06:20:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:51:51.299416+00:00"
    },
    {
      "arxiv_id": "2508.10556v1",
      "title": "Retrieval-Augmented Prompt for OOD Detection",
      "title_zh": "ç”¨äº OOD æ£€æµ‹çš„æ£€ç´¢å¢å¼ºæç¤º",
      "authors": [
        "Ruisong Han",
        "Zongbo Han",
        "Jiahao Zhang",
        "Mingyue Cheng",
        "Changqing Zhang"
      ],
      "abstract": "Out-of-Distribution (OOD) detection is crucial for the reliable deployment of machine learning models in-the-wild, enabling accurate identification of test samples that differ from the training data distribution. Existing methods rely on auxiliary outlier samples or in-distribution (ID) data to generate outlier information for training, but due to limited outliers and their mismatch with real test OOD samples, they often fail to provide sufficient semantic supervision, leading to suboptimal performance. To address this, we propose a novel OOD detection method called Retrieval-Augmented Prompt (RAP). RAP augments a pre-trained vision-language model's prompts by retrieving external knowledge, offering enhanced semantic supervision for OOD detection. During training, RAP retrieves descriptive words for outliers based on joint similarity with external textual knowledge and uses them to augment the model's OOD prompts. During testing, RAP dynamically updates OOD prompts in real-time based on the encountered OOD samples, enabling the model to rapidly adapt to the test environment. Our extensive experiments demonstrate that RAP achieves state-of-the-art performance on large-scale OOD detection benchmarks. For example, in 1-shot OOD detection on the ImageNet-1k dataset, RAP reduces the average FPR95 by 7.05% and improves the AUROC by 1.71% compared to previous methods. Additionally, comprehensive ablation studies validate the effectiveness of each module and the underlying motivations of our approach.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨å®é™…éƒ¨ç½²ä¸­é¢ä¸´çš„åˆ†å¸ƒå¤–æ£€æµ‹(Out-of-Distribution, OOD)é—®é¢˜ï¼ŒæŒ‡å‡ºç°æœ‰æ–¹æ³•å› è¾…åŠ©æ ·æœ¬ä¸è¶³æˆ–ä¸çœŸå®æµ‹è¯•æ ·æœ¬ä¸åŒ¹é…ï¼Œå¯¼è‡´ç¼ºä¹è¶³å¤Ÿçš„è¯­ä¹‰ç›‘ç£ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åä¸ºæ£€ç´¢å¢å¼ºæç¤º(Retrieval-Augmented Prompt, RAP)çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡æ£€ç´¢å¤–éƒ¨çŸ¥è¯†æ¥å¢å¼ºé¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹çš„æç¤ºè¯ã€‚åœ¨è®­ç»ƒé˜¶æ®µï¼ŒRAPåˆ©ç”¨å¤–éƒ¨æ–‡æœ¬çŸ¥è¯†æ£€ç´¢ç¦»ç¾¤å€¼çš„æè¿°æ€§è¯æ±‡ä»¥å¢å¼ºOOD promptsï¼›åœ¨æµ‹è¯•é˜¶æ®µï¼Œå®ƒèƒ½æ ¹æ®å®æ—¶é‡åˆ°çš„æ ·æœ¬åŠ¨æ€æ›´æ–°æç¤ºï¼Œä½¿æ¨¡å‹å¿«é€Ÿé€‚åº”æµ‹è¯•ç¯å¢ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRAPåœ¨å¤§è§„æ¨¡OODæ£€æµ‹åŸºå‡†ä¸Šè¾¾åˆ°äº†SOTAæ°´å¹³ï¼Œåœ¨ImageNet-1kçš„1-shotä»»åŠ¡ä¸­å°†å¹³å‡FPR95é™ä½äº†7.05%ï¼ŒAUROCæå‡äº†1.71%ã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥éªŒè¯äº†å„æ¨¡å—çš„æœ‰æ•ˆæ€§åŠå…¶åœ¨å¢å¼ºè¯­ä¹‰ç›‘ç£æ–¹é¢çš„æ ¸å¿ƒä½œç”¨ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10556v1",
      "published_date": "2025-08-14 11:52:43 UTC",
      "updated_date": "2025-08-14 11:52:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:51:53.687872+00:00"
    },
    {
      "arxiv_id": "2508.10552v1",
      "title": "When Language Overrules: Revealing Text Dominance in Multimodal Large Language Models",
      "title_zh": "å½“è¯­è¨€å æ®ä¸»å¯¼ï¼šæ­ç¤ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„æ–‡æœ¬ä¸»å¯¼ç°è±¡",
      "authors": [
        "Huyu Wu",
        "Meng Tang",
        "Xinhan Zheng",
        "Haiyun Jiang"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities across a diverse range of multimodal tasks. However, these models suffer from a core problem known as text dominance: they depend heavily on text for their inference, while underutilizing other modalities. While prior work has acknowledged this phenomenon in vision-language tasks, often attributing it to data biases or model architectures. In this paper, we conduct the first systematic investigation of text dominance across diverse data modalities, including images, videos, audio, time-series, and graphs. To measure this imbalance, we propose two evaluation metrics: the Modality Dominance Index (MDI) and the Attention Efficiency Index (AEI). Our comprehensive analysis reveals that text dominance is both significant and pervasive across all tested modalities. Our in-depth analysis identifies three underlying causes: attention dilution from severe token redundancy in non-textual modalities, the influence of fusion architecture design, and task formulations that implicitly favor textual inputs. Furthermore, we propose a simple token compression method that effectively rebalances model attention. Applying this method to LLaVA-7B, for instance, drastically reduces its MDI from 10.23 to a well-balanced value of 0.86. Our analysis and methodological framework offer a foundation for the development of more equitable and comprehensive multimodal language models.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)ä¸­æ™®éå­˜åœ¨çš„æ–‡æœ¬ä¸»å¯¼(Text Dominance)ç°è±¡ï¼Œå³æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­è¿‡åº¦ä¾èµ–æ–‡æœ¬è€Œä½ä¼°äº†å…¶ä»–æ¨¡æ€çš„ä½œç”¨ã€‚ä½œè€…é¦–æ¬¡åœ¨å›¾åƒã€è§†é¢‘ã€éŸ³é¢‘ã€æ—¶é—´åºåˆ—åŠå›¾è¡¨ç­‰å¤šç§æ•°æ®æ¨¡æ€ä¸Šè¿›è¡Œäº†ç³»ç»Ÿæ€§è°ƒæŸ¥ï¼Œå¹¶æå‡ºäº†æ¨¡æ€ä¸»å¯¼æŒ‡æ•°(MDI)å’Œæ³¨æ„åŠ›æ•ˆç‡æŒ‡æ•°(AEI)ä¸¤é¡¹é‡åŒ–æŒ‡æ ‡ã€‚ç ”ç©¶å‘ç°ï¼Œæ–‡æœ¬ä¸»å¯¼çš„æ ¹æºåœ¨äºéæ–‡æœ¬æ¨¡æ€ä¸¥é‡çš„ä»¤ç‰Œå†—ä½™(Token Redundancy)å¯¼è‡´çš„æ³¨æ„åŠ›ç¨€é‡Šã€èåˆæ¶æ„è®¾è®¡ç¼ºé™·ä»¥åŠä»»åŠ¡è¡¨è¿°çš„åè§ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§ç®€å•çš„ä»¤ç‰Œå‹ç¼©(Token Compression)æ–¹æ³•ï¼Œæ—¨åœ¨é‡æ–°å¹³è¡¡æ¨¡å‹çš„æ³¨æ„åŠ›åˆ†é…ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å°†LLaVA-7Bçš„MDIä»10.23å¤§å¹…ä¼˜åŒ–è‡³è¶‹äºå¹³è¡¡çš„0.86ã€‚è¯¥åˆ†ææ¡†æ¶å’Œæ–¹æ³•ä¸ºæ„å»ºæ›´å…¬å¹³ã€å…¨é¢çš„å¤šæ¨¡æ€æ¨¡å‹å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10552v1",
      "published_date": "2025-08-14 11:44:52 UTC",
      "updated_date": "2025-08-14 11:44:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:52:05.892565+00:00"
    },
    {
      "arxiv_id": "2508.10548v1",
      "title": "Stabilizing Long-term Multi-turn Reinforcement Learning with Gated Rewards",
      "title_zh": "åˆ©ç”¨é—¨æ§å¥–åŠ±ç¨³å®šé•¿ç¨‹å¤šè½®å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Zetian Sun",
        "Dongfang Li",
        "Zhuoen Chen",
        "Yuhuai Qin",
        "Baotian Hu"
      ],
      "abstract": "Reward sparsity in long-horizon reinforcement learning (RL) tasks remains a significant challenge, while existing outcome-based reward shaping struggles to define meaningful immediate rewards without introducing bias or requiring explicit task decomposition. Alternatively, verification-based reward shaping uses stepwise critics, but misalignment between immediate rewards and long-term objectives can lead to reward hacking and suboptimal policies. In this work, we address this problem in the context of software engineering (SWE) tasks, where multi-turn reasoning and rule-based verification are critical. We introduce the SWE-oriented RL Framework, a unified system supporting multi-turn interaction, docker-based execution, and customizable reward functions. Additionally, we propose Gated Reward Accumulation (G-RA), a novel method that accumulates immediate rewards only when high-level (long-term) rewards meet a predefined threshold, ensuring stable RL optimization. Experiments on SWE-bench Verified and kBench demonstrate that G-RA leads to an increase in completion rates (47.6\\% \\rightarrow 93.8\\% and 22.0\\% \\rightarrow 86.0\\%) and modification rates (19.6\\% \\rightarrow 23.8\\% and 12.0\\% \\rightarrow 42.0\\%), while avoiding policy degradation caused by reward misalignment. Our findings highlight the importance of balanced reward accumulation in long-horizon RL and provide a practical solution.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é•¿æ—¶ç¨‹å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)ä»»åŠ¡ä¸­ç”±äºå¥–åŠ±ç¨€ç–ä»¥åŠå³æ—¶å¥–åŠ±ä¸é•¿æœŸç›®æ ‡å¤±é…å¯¼è‡´çš„å¥–åŠ±å›é¦ˆé»‘å®¢æ”»å‡»(reward hacking)å’Œæ¬¡ä¼˜ç­–ç•¥é—®é¢˜è¿›è¡Œäº†æ·±å…¥æ¢è®¨ã€‚ä½œè€…ä»¥è½¯ä»¶å·¥ç¨‹(Software Engineering)ä»»åŠ¡ä¸ºèƒŒæ™¯ï¼Œæå‡ºäº†ä¸€ä¸ªé›†æˆäº†å¤šè½®äº¤äº’ã€Dockeræ‰§è¡Œç¯å¢ƒåŠè‡ªå®šä¹‰å¥–åŠ±å‡½æ•°çš„SWE-oriented RL Frameworkã€‚ç ”ç©¶çš„æ ¸å¿ƒè´¡çŒ®åœ¨äºæå‡ºé—¨æ§å¥–åŠ±ç´¯ç§¯(Gated Reward Accumulation, G-RA)æœºåˆ¶ï¼Œè¯¥æœºåˆ¶è§„å®šä»…åœ¨é•¿æœŸå¥–åŠ±è¾¾åˆ°ç‰¹å®šé˜ˆå€¼æ—¶æ‰ç´¯ç§¯å³æ—¶å¥–åŠ±ï¼Œä»¥æ­¤å¼•å¯¼ç¨³å®šçš„RLä¼˜åŒ–è¿‡ç¨‹ã€‚å®éªŒåœ¨SWE-bench Verifiedå’ŒkBenchåŸºå‡†æµ‹è¯•ä¸Šè¯æ˜äº†è¯¥æ–¹æ³•çš„å“è¶Šæ€§èƒ½ï¼Œå…¶å®Œæˆç‡åˆ†åˆ«ä»47.6%å’Œ22.0%å¤§å¹…æå‡è‡³93.8%å’Œ86.0%ã€‚è¿™ä¸€ç ”ç©¶ä¸ä»…æœ‰æ•ˆé˜²æ­¢äº†ç”±äºå¥–åŠ±è¯¯å¯¼å¼•èµ·çš„ç­–ç•¥é€€åŒ–ï¼Œä¹Ÿä¸ºè§£å†³é•¿æ—¶ç¨‹å¤šè½®RLä»»åŠ¡ä¸­çš„å¥–åŠ±å¹³è¡¡é—®é¢˜æä¾›äº†æå…·å®è·µä»·å€¼çš„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10548v1",
      "published_date": "2025-08-14 11:37:02 UTC",
      "updated_date": "2025-08-14 11:37:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:52:21.768258+00:00"
    },
    {
      "arxiv_id": "2508.10956v2",
      "title": "A Study of Commonsense Reasoning over Visual Object Properties",
      "title_zh": "è§†è§‰ç›®æ ‡å±æ€§å¸¸è¯†æ¨ç†ç ”ç©¶",
      "authors": [
        "Abhishek Kolari",
        "Mohammadhossein Khojasteh",
        "Yifan Jiang",
        "Floris den Hengst",
        "Filip Ilievski"
      ],
      "abstract": "Inspired by human categorization, object property reasoning involves identifying and recognizing low-level details and higher-level abstractions. While current visual question answering (VQA) studies consider multiple object properties, such as size, they typically blend perception and reasoning and lack representativeness in terms of reasoning and image categories, making it unclear whether and how vision-language models (VLMs) abstract and reason over depicted objects. To this end, we introduce a systematic evaluation framework comprising images of three representative types, three reasoning levels of increasing complexity, and four object property dimensions, informed by prior work on common sense. We develop a procedure to instantiate this framework in two VQA object reasoning benchmarks: OPTICS-CNT, comprising 360 images paired with 1,080 multi-level, count-based questions, and OPTICS-CMP, with 2.1k comparison questions. Experiments with 12 state-of-the-art VLMs in zero-shot settings reveal significant limitations relative to humans, with the best-performing model achieving below 40% counting and 70% comparison accuracy. VLMs struggle particularly with photographic images, counterfactual reasoning, physical and functional properties, and higher counts. We make the OPTICS benchmark data and code available to support future work on scalable benchmarking methods, generalized annotation guidelines, and advanced reasoning VLMs.",
      "tldr_zh": "æœ¬ç ”ç©¶æ—¨åœ¨æ¢è®¨è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨ç‰©ä½“å±æ€§æ¨ç†æ–¹é¢çš„èƒ½åŠ›ï¼Œè§£å†³ç°æœ‰è§†è§‰é—®ç­”(VQA)ç ”ç©¶ä¸­æ„ŸçŸ¥ä¸æ¨ç†ç•Œé™æ¨¡ç³Šä»¥åŠå›¾åƒç±»åˆ«ä»£è¡¨æ€§ä¸è¶³çš„é—®é¢˜ã€‚ç ”ç©¶è€…æå‡ºäº†ä¸€ä¸ªç³»ç»Ÿæ€§è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…å«ä¸‰ç±»ä»£è¡¨æ€§å›¾åƒã€ä¸‰ä¸ªå¤æ‚åº¦é€’å¢çš„æ¨ç†å±‚æ¬¡ä»¥åŠå››ä¸ªåŸºäºå¸¸è¯†çš„ç‰©ä½“å±æ€§ç»´åº¦ã€‚åŸºäºè¯¥æ¡†æ¶ï¼Œç ”ç©¶å¼€å‘äº†ä¸¤ä¸ªç‰©ä½“æ¨ç†åŸºå‡†æµ‹è¯•ï¼šåŒ…å«360å¼ å›¾åƒåŠ1,080ä¸ªè®¡æ•°é—®é¢˜çš„OPTICS-CNTï¼Œä»¥åŠåŒ…å«2,100ä¸ªæ¯”è¾ƒé—®é¢˜çš„OPTICS-CMPã€‚é’ˆå¯¹12ç§å…ˆè¿›VLMsçš„é›¶æ ·æœ¬(zero-shot)å®éªŒæ˜¾ç¤ºï¼Œè¿™äº›æ¨¡å‹åœ¨æ¨ç†èƒ½åŠ›ä¸Šä»æ˜¾è‘—è½åäºäººç±»ï¼Œè¡¨ç°æœ€å¥½çš„æ¨¡å‹åœ¨è®¡æ•°å’Œæ¯”è¾ƒä»»åŠ¡ä¸­çš„å‡†ç¡®ç‡åˆ†åˆ«ä½äº40%å’Œ70%ã€‚å®éªŒå‘ç°VLMsåœ¨å¤„ç†çœŸå®æ‘„å½±å›¾åƒ(photographic images)ã€åäº‹å®æ¨ç†(counterfactual reasoning)ã€ç‰©ç†åŠåŠŸèƒ½å±æ€§(physical and functional properties)ä»¥åŠé«˜æ•°é‡è®¡æ•°æ—¶é¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚è¯¥ç ”ç©¶é€šè¿‡å…¬å¼€OPTICSåŸºå‡†æ•°æ®å’Œä»£ç ï¼Œä¸ºæœªæ¥å¼€å‘å¯æ‰©å±•è¯„ä¼°æ–¹æ³•ã€æ³›åŒ–æ ‡æ³¨å‡†åˆ™åŠé«˜çº§æ¨ç†VLMsæä¾›äº†é‡è¦æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10956v2",
      "published_date": "2025-08-14 11:28:40 UTC",
      "updated_date": "2026-01-15 11:10:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:52:25.652487+00:00"
    },
    {
      "arxiv_id": "2508.10539v1",
      "title": "Improving Value-based Process Verifier via Low-Cost Variance Reduction",
      "title_zh": "é€šè¿‡ä½æˆæœ¬æ–¹å·®ç¼©å‡æå‡åŸºäºä»·å€¼çš„è¿‡ç¨‹éªŒè¯å™¨",
      "authors": [
        "Zetian Sun",
        "Dongfang Li",
        "Baotian Hu",
        "Min Zhang"
      ],
      "abstract": "Large language models (LLMs) have achieved remarkable success in a wide range of tasks. However, their reasoning capabilities, particularly in complex domains like mathematics, remain a significant challenge. Value-based process verifiers, which estimate the probability of a partial reasoning chain leading to a correct solution, are a promising approach for improving reasoning. Nevertheless, their effectiveness is often hindered by estimation error in their training annotations, a consequence of the limited number of Monte Carlo (MC) samples feasible due to the high cost of LLM inference. In this paper, we identify that the estimation error primarily arises from high variance rather than bias, and the MC estimator is a Minimum Variance Unbiased Estimator (MVUE). To address the problem, we propose the \\textsc{Com}pound \\textsc{M}onte \\textsc{C}arlo \\textsc{S}ampling (ComMCS) method, which constructs an unbiased estimator by linearly combining the MC estimators from the current and subsequent steps. Theoretically, we show that our method leads to a predictable reduction in variance, while maintaining an unbiased estimation without additional LLM inference cost. We also perform empirical experiments on the MATH-500 and GSM8K benchmarks to demonstrate the effectiveness of our method. Notably, ComMCS outperforms regression-based optimization method by 2.8 points, the non-variance-reduced baseline by 2.2 points on MATH-500 on Best-of-32 sampling experiment.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤æ‚æ•°å­¦æ¨ç†ä¸­é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæ¢è®¨äº†æ—¨åœ¨è¯„ä¼°ä¸­é—´æ¨ç†æ­¥éª¤æ­£ç¡®æ€§çš„ Value-based process verifiers åŠå…¶å—è®­ç»ƒæ ‡æ³¨ä¼°è®¡è¯¯å·®é™åˆ¶çš„é—®é¢˜ã€‚ç ”ç©¶æŒ‡å‡ºè¯¥è¯¯å·®ä¸»è¦æºäºé«˜æ–¹å·®(High Variance)è€Œéåå·®(Bias)ï¼Œå¹¶æ®æ­¤æå‡ºäº† ComMCS (Compound Monte Carlo Sampling) æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡çº¿æ€§ç»„åˆå½“å‰ä¸åç»­æ­¥éª¤çš„ Monte Carlo (MC) ä¼°è®¡é‡ï¼Œåœ¨ä¸å¢åŠ é¢å¤–æ¨ç†æˆæœ¬çš„æƒ…å†µä¸‹æ„å»ºå‡ºæ–¹å·®æ›´ä½çš„æ— åä¼°è®¡é‡ã€‚ç†è®ºåˆ†æä¸å®éªŒç»“æœå‡è¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œåœ¨ MATH-500 åŸºå‡†æµ‹è¯•çš„ Best-of-32 é‡‡æ ·å®éªŒä¸­ï¼ŒComMCS æ¯”åŸºç¡€æ¨¡å‹å‡†ç¡®ç‡æå‡äº† 2.2 ä¸ªç™¾åˆ†ç‚¹ï¼Œä¸”æ€§èƒ½ä¼˜äºåŸºäºå›å½’çš„ä¼˜åŒ–æ–¹æ³•ã€‚è¿™é¡¹å·¥ä½œä¸ºä½æˆæœ¬æå‡è¿‡ç¨‹éªŒè¯å™¨çš„å‡†ç¡®æ€§ä¸å¯é æ€§æä¾›äº†é‡è¦æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10539v1",
      "published_date": "2025-08-14 11:22:29 UTC",
      "updated_date": "2025-08-14 11:22:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:52:26.955742+00:00"
    },
    {
      "arxiv_id": "2508.10530v1",
      "title": "Diversity First, Quality Later: A Two-Stage Assumption for Language Model Alignment",
      "title_zh": "å¤šæ ·æ€§ä¼˜å…ˆï¼Œè´¨é‡éšåï¼šè¯­è¨€æ¨¡å‹å¯¹é½çš„ä¸¤é˜¶æ®µå‡è®¾",
      "authors": [
        "Zetian Sun",
        "Dongfang Li",
        "Baotian Hu"
      ],
      "abstract": "The alignment of language models (LMs) with human preferences is critical for building reliable AI systems. The problem is typically framed as optimizing an LM policy to maximize the expected reward that reflects human preferences. Recently, Direct Preference Optimization (DPO) was proposed as a LM alignment method that directly optimize the policy from static preference data, and further improved by incorporating on-policy sampling (i.e., preference candidates generated during the training loop) for better LM alignment. However, we show on-policy data is not always optimal, with systematic effectiveness difference emerging between static and on-policy preference candidates. For example, on-policy data can result in a 3$\\times$ effectiveness compared with static data for Llama-3, and a 0.4$\\times$ effectiveness for Zephyr. To explain the phenomenon, we propose the alignment stage assumption, which divides the alignment process into two distinct stages: the preference injection stage, which benefits from diverse data, and the preference fine-tuning stage, which favors high-quality data. Through theoretical and empirical analysis, we characterize these stages and propose an effective algorithm to identify the boundaries between them. We perform experiments on 5 models (Llama, Zephyr, Phi-2, Qwen, Pythia) and 2 alignment methods (DPO, SLiC-HF) to show the generalizability of alignment stage assumption and boundary measurement.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è¯­è¨€æ¨¡å‹(LMs)ä¸äººç±»åå¥½å¯¹é½çš„ä¼˜åŒ–é—®é¢˜ï¼Œå‘ç°å¸¸ç”¨çš„ç›´æ¥åå¥½ä¼˜åŒ–(DPO)åœ¨çº¿é‡‡æ ·(on-policy sampling)æ•°æ®åœ¨ä¸åŒæ¨¡å‹ä¸Šçš„æ•ˆèƒ½å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚é’ˆå¯¹è¿™ä¸€ç°è±¡ï¼Œä½œè€…æå‡ºäº†å¯¹é½é˜¶æ®µå‡è®¾(alignment stage assumption)ï¼Œå°†å¯¹é½è¿‡ç¨‹åˆ†ä¸ºåå¥½æ³¨å…¥é˜¶æ®µ(preference injection stage)å’Œåå¥½å¾®è°ƒé˜¶æ®µ(preference fine-tuning stage)ã€‚è¯¥å‡è®¾æŒ‡å‡ºï¼Œåå¥½æ³¨å…¥é˜¶æ®µæ›´å—ç›Šäºå¤šæ ·åŒ–æ•°æ®(diverse data)ï¼Œè€Œåå¥½å¾®è°ƒé˜¶æ®µåˆ™æ›´ä¾èµ–é«˜è´¨é‡æ•°æ®(high-quality data)ã€‚ç ”ç©¶é€šè¿‡ç†è®ºå’Œå®è¯åˆ†æåˆ»ç”»äº†è¿™ä¸¤ä¸ªé˜¶æ®µçš„ç‰¹å¾ï¼Œå¹¶æå‡ºäº†ä¸€ç§è¯†åˆ«é˜¶æ®µè¾¹ç•Œçš„æœ‰æ•ˆç®—æ³•ã€‚åœ¨Llamaã€Zephyrã€Phi-2ã€Qwenã€Pythiaç­‰5ç§æ¨¡å‹åŠDPOã€SLiC-HFæ–¹æ³•ä¸Šçš„å®éªŒè¯æ˜äº†è¯¥å‡è®¾å’Œè¾¹ç•Œæµ‹é‡æ–¹æ³•çš„é€šç”¨æ€§ï¼Œä¸ºä¼˜åŒ–æ¨¡å‹å¯¹é½ç­–ç•¥æä¾›äº†ç†è®ºæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10530v1",
      "published_date": "2025-08-14 11:05:18 UTC",
      "updated_date": "2025-08-14 11:05:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:52:43.594695+00:00"
    },
    {
      "arxiv_id": "2508.10528v2",
      "title": "Med-GLIP: Advancing Medical Language-Image Pre-training with Large-scale Grounded Dataset",
      "title_zh": "Med-GLIPï¼šå€ŸåŠ©å¤§è§„æ¨¡å®šä½æ•°æ®é›†æ¨è¿›åŒ»å­¦è¯­è¨€-å›¾åƒé¢„è®­ç»ƒ",
      "authors": [
        "Ziye Deng",
        "Ruihan He",
        "Jiaxiang Liu",
        "Yuan Wang",
        "Zijie Meng",
        "Songtao Jiang",
        "Yong Xie",
        "Zuozhu Liu"
      ],
      "abstract": "Medical image grounding aims to align natural language phrases with specific regions in medical images, serving as a foundational task for intelligent diagnosis, visual question answering (VQA), and automated report generation (MRG). However, existing research is constrained by limited modality coverage, coarse-grained annotations, and the absence of a unified, generalizable grounding framework. To address these challenges, we construct a large-scale medical grounding dataset Med-GLIP-5M comprising over 5.3 million region-level annotations across seven imaging modalities, covering diverse anatomical structures and pathological findings. The dataset supports both segmentation and grounding tasks with hierarchical region labels, ranging from organ-level boundaries to fine-grained lesions. Based on this foundation, we propose Med-GLIP, a modality-aware grounding framework trained on Med-GLIP-5M. Rather than relying on explicitly designed expert modules, Med-GLIP implicitly acquires hierarchical semantic understanding from diverse training data -- enabling it to recognize multi-granularity structures, such as distinguishing lungs from pneumonia lesions. Extensive experiments demonstrate that Med-GLIP consistently outperforms state-of-the-art baselines across multiple grounding benchmarks. Furthermore, integrating its spatial outputs into downstream tasks, including medical VQA and report generation, leads to substantial performance gains. Our dataset will be released soon.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»å­¦å›¾åƒå®šä½(Medical image grounding)é¢†åŸŸæ¨¡æ€è¦†ç›–æœ‰é™å’Œæ ‡æ³¨ç²—ç³™ç­‰æŒ‘æˆ˜ï¼Œæ„å»ºäº†å¤§è§„æ¨¡æ•°æ®é›† Med-GLIP-5Mï¼Œå…¶ä¸­åŒ…å«è¶…è¿‡530ä¸‡ä¸ªè·¨è¶Šä¸ƒç§æˆåƒæ¨¡æ€çš„åŒºåŸŸçº§æ ‡æ³¨ã€‚åŸºäºæ­¤æ•°æ®é›†ï¼Œç ”ç©¶è€…æå‡ºäº† Med-GLIP æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä¸ä¾èµ–æ˜¾å¼çš„ä¸“å®¶æ¨¡å—ï¼Œè€Œæ˜¯é€šè¿‡å¤šæ ·åŒ–æ•°æ®éšå¼å­¦ä¹ å±‚çº§è¯­ä¹‰ç†è§£ï¼Œä»è€Œå®ç°å¯¹å¤šç²’åº¦åŒ»å­¦ç»“æ„çš„ç²¾å‡†è¯†åˆ«ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒMed-GLIP åœ¨å¤šä¸ªå®šä½åŸºå‡†æµ‹è¯•ä¸­å‡æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚åŒæ—¶ï¼Œå°† Med-GLIP çš„ç©ºé—´å®šä½èƒ½åŠ›åº”ç”¨äºåŒ»å­¦è§†è§‰é—®ç­”(VQA)å’ŒæŠ¥å‘Šç”Ÿæˆ(MRG)ç­‰ä¸‹æ¸¸ä»»åŠ¡ï¼Œä¹Ÿå±•ç°å‡ºäº†å®è´¨æ€§çš„æ€§èƒ½æå‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10528v2",
      "published_date": "2025-08-14 11:02:38 UTC",
      "updated_date": "2025-11-05 21:40:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:52:50.956592+00:00"
    },
    {
      "arxiv_id": "2508.10507v1",
      "title": "Multi-Sample Anti-Aliasing and Constrained Optimization for 3D Gaussian Splatting",
      "title_zh": "é¢å‘ 3D Gaussian Splatting çš„å¤šé‡‡æ ·æŠ—é”¯é½¿ä¸çº¦æŸä¼˜åŒ–",
      "authors": [
        "Zheng Zhou",
        "Jia-Chen Zhang",
        "Yu-Jie Xiong",
        "Chun-Ming Xia"
      ],
      "abstract": "Recent advances in 3D Gaussian splatting have significantly improved real-time novel view synthesis, yet insufficient geometric constraints during scene optimization often result in blurred reconstructions of fine-grained details, particularly in regions with high-frequency textures and sharp discontinuities. To address this, we propose a comprehensive optimization framework integrating multisample anti-aliasing (MSAA) with dual geometric constraints. Our system computes pixel colors through adaptive blending of quadruple subsamples, effectively reducing aliasing artifacts in high-frequency components. The framework introduces two constraints: (a) an adaptive weighting strategy that prioritizes under-reconstructed regions through dynamic gradient analysis, and (b) gradient differential constraints enforcing geometric regularization at object boundaries. This targeted optimization enables the model to allocate computational resources preferentially to critical regions requiring refinement while maintaining global consistency. Extensive experimental evaluations across multiple benchmarks demonstrate that our method achieves state-of-the-art performance in detail preservation, particularly in preserving high-frequency textures and sharp discontinuities, while maintaining real-time rendering efficiency. Quantitative metrics and perceptual studies confirm statistically significant improvements over baseline approaches in both structural similarity (SSIM) and perceptual quality (LPIPS).",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ 3D Gaussian splatting åœ¨åœºæ™¯ä¼˜åŒ–ä¸­å› å‡ ä½•çº¦æŸä¸è¶³å¯¼è‡´é«˜é¢‘çº¹ç†å’Œé”åˆ©è¾¹ç¼˜é‡å»ºæ¨¡ç³Šçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆå¤šé‡é‡‡æ ·æŠ—é”¯é½¿ (multisample anti-aliasing, MSAA) ä¸åŒé‡å‡ ä½•çº¦æŸçš„ç»¼åˆä¼˜åŒ–æ¡†æ¶ã€‚è¯¥ç³»ç»Ÿé€šè¿‡å››å€å­é‡‡æ · (quadruple subsamples) çš„è‡ªé€‚åº”æ··åˆè®¡ç®—åƒç´ é¢œè‰²ï¼Œæœ‰æ•ˆå‡å°‘äº†é«˜é¢‘ç»„ä»¶ä¸­çš„é”¯é½¿ä¼ªå½±ã€‚æ¡†æ¶å¼•å…¥äº†åŸºäºåŠ¨æ€æ¢¯åº¦åˆ†æçš„è‡ªé€‚åº”æƒé‡ç­–ç•¥ä»¥ä¼˜å…ˆå¤„ç†é‡å»ºä¸è¶³çš„åŒºåŸŸï¼Œå¹¶åˆ©ç”¨æ¢¯åº¦å¾®åˆ†çº¦æŸ (gradient differential constraints) åœ¨ç‰©ä½“è¾¹ç•Œå¼ºåŒ–å‡ ä½•æ­£åˆ™åŒ–ã€‚è¿™ç§æœ‰é’ˆå¯¹æ€§çš„ä¼˜åŒ–ä½¿æ¨¡å‹èƒ½å¤Ÿä¼˜å…ˆåˆ†é…è®¡ç®—èµ„æºç»™å…³é”®åŒºåŸŸï¼ŒåŒæ—¶ä¿æŒå…¨å±€ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒå®æ—¶æ¸²æŸ“æ•ˆç‡çš„åŒæ—¶ï¼Œåœ¨ç»†èŠ‚ä¿ç•™æ–¹é¢è¾¾åˆ°äº† state-of-the-art æ€§èƒ½ï¼Œå¹¶åœ¨ SSIM å’Œ LPIPS ç­‰æŒ‡æ ‡ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10507v1",
      "published_date": "2025-08-14 10:14:36 UTC",
      "updated_date": "2025-08-14 10:14:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:52:43.391980+00:00"
    },
    {
      "arxiv_id": "2508.16623v2",
      "title": "RAST: A Retrieval Augmented Spatio-Temporal Framework for Traffic Prediction",
      "title_zh": "RASTï¼šä¸€ç§ç”¨äºäº¤é€šé¢„æµ‹çš„æ£€ç´¢å¢å¼ºæ—¶ç©ºæ¡†æ¶",
      "authors": [
        "Weilin Ruan",
        "Xilin Dang",
        "Ziyu Zhou",
        "Sisuo Lyu",
        "Yuxuan Liang"
      ],
      "abstract": "Traffic prediction is a cornerstone of modern intelligent transportation systems and a critical task in spatio-temporal forecasting. Although advanced Spatio-temporal Graph Neural Networks (STGNNs) and pre-trained models have achieved significant progress in traffic prediction, two key challenges remain: (i) limited contextual capacity when modeling complex spatio-temporal dependencies, and (ii) low predictability at fine-grained spatio-temporal points due to heterogeneous patterns. Inspired by Retrieval-Augmented Generation (RAG), we propose RAST, a universal framework that integrates retrieval-augmented mechanisms with spatio-temporal modeling to address these challenges. Our framework consists of three key designs: 1) Decoupled Encoder and Query Generator to capture decoupled spatial and temporal features and construct a fusion query via residual fusion; 2) Spatio-temporal Retrieval Store and Retrievers to maintain and retrieve vectorized fine-grained patterns; and 3) Universal Backbone Predictor that flexibly accommodates pre-trained STGNNs or simple MLP predictors. Extensive experiments on six real-world traffic networks, including large-scale datasets, demonstrate that RAST achieves superior performance while maintaining computational efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†RASTï¼Œä¸€ç§å—æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)å¯å‘çš„é€šç”¨æ—¶ç©ºé¢„æµ‹æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰STGNNsåœ¨å¤„ç†å¤æ‚æ—¶ç©ºä¾èµ–æ—¶ä¸Šä¸‹æ–‡èƒ½åŠ›ä¸è¶³ä»¥åŠç»†ç²’åº¦é¢„æµ‹å‡†ç¡®æ€§ä½çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶ä¸»è¦ç”±è§£è€¦ç¼–ç å™¨(Decoupled Encoder)ä¸æŸ¥è¯¢ç”Ÿæˆå™¨ã€æ—¶ç©ºæ£€ç´¢åº“(Spatio-temporal Retrieval Store)ä¸æ£€ç´¢å™¨ï¼Œä»¥åŠé€šç”¨ä¸»å¹²é¢„æµ‹å™¨(Universal Backbone Predictor)ä¸‰å¤§æ ¸å¿ƒè®¾è®¡ç»„æˆã€‚é€šè¿‡å¼•å…¥æ£€ç´¢æœºåˆ¶ï¼ŒRASTèƒ½å¤Ÿæœ‰æ•ˆåœ°ç»´æŠ¤å¹¶æå–å‘é‡åŒ–çš„ç»†ç²’åº¦æ—¶ç©ºæ¨¡å¼ï¼Œä»è€Œå¢å¼ºå¯¹å¼‚æ„äº¤é€šæ•°æ®çš„å»ºæ¨¡èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå…¶é¢„æµ‹å™¨æ¨¡å—å¯çµæ´»å…¼å®¹é¢„è®­ç»ƒæ¨¡å‹æˆ–ç®€å•çš„MLPï¼Œå…·æœ‰æé«˜çš„é€šç”¨æ€§ã€‚åœ¨å…­ä¸ªçœŸå®äº¤é€šç½‘ç»œæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒRASTåœ¨æ˜¾è‘—æå‡é¢„æµ‹ç²¾åº¦çš„åŒæ—¶ï¼Œä»ä¿æŒäº†è‰¯å¥½çš„è®¡ç®—æ•ˆç‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by AAAI 2026 (AI for Social Impact)",
      "pdf_url": "https://arxiv.org/pdf/2508.16623v2",
      "published_date": "2025-08-14 10:11:39 UTC",
      "updated_date": "2025-12-30 07:38:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:52:39.984380+00:00"
    },
    {
      "arxiv_id": "2508.10504v1",
      "title": "Advances in Logic-Based Entity Resolution: Enhancing ASPEN with Local Merges and Optimality Criteria",
      "title_zh": "åŸºäºé€»è¾‘çš„å®ä½“è§£ææ–°è¿›å±•ï¼šé€šè¿‡å±€éƒ¨åˆå¹¶ä¸æœ€ä¼˜æ€§å‡†åˆ™å¢å¼º ASPEN",
      "authors": [
        "Zhliang Xiang",
        "Meghyn Bienvenu",
        "Gianluca Cima",
        "VÃ­ctor GutiÃ©rrez-Basulto",
        "YazmÃ­n IbÃ¡Ã±ez-GarcÃ­a"
      ],
      "abstract": "In this paper, we present ASPEN+, which extends an existing ASP-based system, ASPEN,for collective entity resolution with two important functionalities: support for local merges and new optimality criteria for preferred solutions. Indeed, ASPEN only supports so-called global merges of entity-referring constants (e.g. author ids), in which all occurrences of matched constants are treated as equivalent and merged accordingly. However, it has been argued that when resolving data values, local merges are often more appropriate, as e.g. some instances of 'J. Lee' may refer to 'Joy Lee', while others should be matched with 'Jake Lee'. In addition to allowing such local merges, ASPEN+ offers new optimality criteria for selecting solutions, such as minimizing rule violations or maximising the number of rules supporting a merge. Our main contributions are thus (1) the formalisation and computational analysis of various notions of optimal solution, and (2) an extensive experimental evaluation on real-world datasets, demonstrating the effect of local merges and the new optimality criteria on both accuracy and runtime.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ASPEN+ï¼Œè¿™æ˜¯å¯¹åŸºäºç­”æ¡ˆé›†ç¼–ç¨‹ (Answer Set Programming, ASP) çš„é›†ä½“å®ä½“è§£æ (Collective Entity Resolution) ç³»ç»Ÿ ASPEN çš„é‡è¦æ‰©å±•ã€‚ASPEN+ å¼•å…¥äº†å¯¹å±€éƒ¨åˆå¹¶ (Local Merges) çš„æ”¯æŒï¼Œå…‹æœäº†åŸç³»ç»Ÿä»…æ”¯æŒå…¨å±€åˆå¹¶ (Global Merges) çš„å±€é™ï¼Œå…è®¸åŒä¸€æ•°æ®å€¼åœ¨ä¸åŒè¯­å¢ƒä¸‹å…³è”è‡³ä¸åŒå®ä½“ã€‚æ­¤å¤–ï¼Œè¯¥ç³»ç»Ÿè®¾è®¡äº†æ–°çš„æœ€ä¼˜æ€§æ ‡å‡† (Optimality Criteria) ç”¨äºç­›é€‰æ–¹æ¡ˆï¼Œä¾‹å¦‚æœ€å°åŒ–è§„åˆ™å†²çªæˆ–æœ€å¤§åŒ–è§„åˆ™æ”¯æŒåº¦ã€‚ç ”ç©¶å¯¹å¤šç§æœ€ä¼˜è§£å®šä¹‰è¿›è¡Œäº†ä¸¥è°¨çš„å½¢å¼åŒ–ä¸è®¡ç®—å¤æ‚åº¦åˆ†æã€‚åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå±€éƒ¨åˆå¹¶ä¸æ–°æœ€ä¼˜æ€§æ ‡å‡†çš„ç»“åˆä¸ä»…æ˜¾è‘—æå‡äº†å®ä½“è§£æçš„å‡†ç¡®ç‡ï¼Œè¿˜åœ¨è¿è¡Œæ•ˆç‡æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB",
      "comment": "Full version of a paper accepted at KR 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.10504v1",
      "published_date": "2025-08-14 10:05:56 UTC",
      "updated_date": "2025-08-14 10:05:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:52:44.355751+00:00"
    },
    {
      "arxiv_id": "2508.10501v4",
      "title": "PASS: Probabilistic Agentic Supernet Sampling for Interpretable and Adaptive Chest X-Ray Reasoning",
      "title_zh": "PASSï¼šé¢å‘å¯è§£é‡Šä¸è‡ªé€‚åº”èƒ¸éƒ¨ X å…‰æ¨ç†çš„æ¦‚ç‡æ™ºèƒ½ä½“è¶…ç½‘ç»œé‡‡æ ·",
      "authors": [
        "Yushi Feng",
        "Junye Du",
        "Yingying Hong",
        "Qifan Wang",
        "Lequan Yu"
      ],
      "abstract": "Existing tool-augmented agentic systems are limited in the real world by (i) black-box reasoning steps that undermine trust of decision-making and pose safety risks, (ii) poor multimodal integration, which is inherently critical for healthcare tasks, and (iii) rigid and computationally inefficient agentic pipelines. We introduce PASS (Probabilistic Agentic Supernet Sampling), the first multimodal framework to address these challenges in the context of Chest X-Ray (CXR) reasoning. PASS adaptively samples agentic workflows over a multi-tool graph, yielding decision paths annotated with interpretable probabilities. Given the complex CXR reasoning task with multimodal medical data, PASS leverages its learned task-conditioned distribution over the agentic supernet. Thus, it adaptively selects the most suitable tool at each supernet layer, offering probability-annotated trajectories for post-hoc audits and directly enhancing medical AI safety. PASS also continuously compresses salient findings into an evolving personalized memory, while dynamically deciding whether to deepen its reasoning path or invoke an early exit for efficiency. To optimize a Pareto frontier balancing performance and cost, we design a novel three-stage training procedure, including expert knowledge warm-up, contrastive path-ranking, and cost-aware reinforcement learning. To facilitate rigorous evaluation, we introduce CAB-E, a comprehensive benchmark for multi-step, safety-critical, free-form CXR reasoning. Experiments across various benchmarks validate that PASS significantly outperforms strong baselines in multiple metrics (e.g., accuracy, LLM-Judge, semantic similarity, etc.) while balancing computational costs, pushing a new paradigm shift towards interpretable, adaptive, and multimodal medical agentic systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†PASS (Probabilistic Agentic Supernet Sampling)ï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹èƒ¸éƒ¨Xå…‰(Chest X-Ray)æ¨ç†è®¾è®¡çš„è§£é‡Šæ€§è‡ªé€‚åº”å¤šæ¨¡æ€æ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ç³»ç»Ÿåœ¨å†³ç­–é€æ˜åº¦ã€å¤šæ¨¡æ€é›†æˆå’Œè®¡ç®—æ•ˆç‡æ–¹é¢çš„å±€é™ã€‚PASSé€šè¿‡åœ¨å¤šå·¥å…·å›¾ä¸Šè‡ªé€‚åº”é‡‡æ ·æ™ºèƒ½ä½“å·¥ä½œæµï¼Œåˆ©ç”¨åœ¨æ™ºèƒ½ä½“è¶…ç½‘ç»œ(agentic supernet)ä¸Šå­¦ä¹ åˆ°çš„ä»»åŠ¡æ¡ä»¶åˆ†å¸ƒé€‰æ‹©æœ€ä¼˜å·¥å…·ï¼Œå¹¶ç”Ÿæˆå¸¦æœ‰æ¦‚ç‡æ ‡æ³¨çš„å¯è§£é‡Šå†³ç­–è·¯å¾„ä»¥å¢å¼ºåŒ»ç–—AIå®‰å…¨æ€§ã€‚è¯¥æ¡†æ¶æ”¯æŒå°†å‘ç°å‹ç¼©è‡³æ¼”åŒ–ä¸­çš„ä¸ªæ€§åŒ–è®°å¿†ï¼Œå¹¶èƒ½åŠ¨æ€æ‰§è¡Œæ—©æœŸé€€å‡º(early exit)ä»¥ä¼˜åŒ–è®¡ç®—èµ„æºã€‚ç ”ç©¶å›¢é˜Ÿé‡‡ç”¨äº†åŒ…å«ä¸“å®¶çŸ¥è¯†é¢„çƒ­ã€å¯¹æ¯”è·¯å¾„æ’åº(contrastive path-ranking)å’Œæˆæœ¬æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ çš„ä¸‰é˜¶æ®µè®­ç»ƒç¨‹åºã€‚é€šè¿‡åœ¨åŒæ­¥æ¨å‡ºçš„å®‰å…¨å…³é”®å‹åŸºå‡†æµ‹è¯•é›†CAB-EåŠå¤šä¸ªæŒ‡æ ‡ä¸Šçš„å®éªŒéªŒè¯ï¼ŒPASSåœ¨å‡†ç¡®ç‡å’Œè¯­ä¹‰ç›¸ä¼¼åº¦ç­‰æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå®ç°äº†æ€§èƒ½ä¸æˆæœ¬çš„å¹³è¡¡ï¼Œä¸ºè§£é‡Šæ€§å¤šæ¨¡æ€åŒ»ç–—æ™ºèƒ½ä½“ç³»ç»Ÿæ ‘ç«‹äº†æ–°èŒƒå¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10501v4",
      "published_date": "2025-08-14 10:03:47 UTC",
      "updated_date": "2025-12-19 16:27:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:52:54.891544+00:00"
    },
    {
      "arxiv_id": "2508.10494v1",
      "title": "A Unified Multi-Agent Framework for Universal Multimodal Understanding and Generation",
      "title_zh": "é¢å‘é€šç”¨å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆçš„ç»Ÿä¸€å¤šæ™ºèƒ½ä½“æ¡†æ¶",
      "authors": [
        "Jiulin Li",
        "Ping Huang",
        "Yexin Li",
        "Shuo Chen",
        "Juewen Hu",
        "Ye Tian"
      ],
      "abstract": "Real-world multimodal applications often require any-to-any capabilities, enabling both understanding and generation across modalities including text, image, audio, and video. However, integrating the strengths of autoregressive language models (LLMs) for reasoning and diffusion models for high-fidelity generation remains challenging. Existing approaches rely on rigid pipelines or tightly coupled architectures, limiting flexibility and scalability. We propose MAGUS (Multi-Agent Guided Unified Multimodal System), a modular framework that unifies multimodal understanding and generation via two decoupled phases: Cognition and Deliberation. MAGUS enables symbolic multi-agent collaboration within a shared textual workspace. In the Cognition phase, three role-conditioned multimodal LLM agents - Perceiver, Planner, and Reflector - engage in collaborative dialogue to perform structured understanding and planning. The Deliberation phase incorporates a Growth-Aware Search mechanism that orchestrates LLM-based reasoning and diffusion-based generation in a mutually reinforcing manner. MAGUS supports plug-and-play extensibility, scalable any-to-any modality conversion, and semantic alignment - all without the need for joint training. Experiments across multiple benchmarks, including image, video, and audio generation, as well as cross-modal instruction following, demonstrate that MAGUS outperforms strong baselines and state-of-the-art systems. Notably, on the MME benchmark, MAGUS surpasses the powerful closed-source model GPT-4o.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MAGUSï¼ˆMulti-Agent Guided Unified Multimodal Systemï¼‰ï¼Œä¸€ä¸ªæ—¨åœ¨å®ç°å…¨èƒ½å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆçš„ç»Ÿä¸€å¤šæ™ºèƒ½ä½“æ¡†æ¶ã€‚ä¸ºäº†è§£å†³LLMsçš„æ¨ç†èƒ½åŠ›ä¸diffusion modelsçš„é«˜è´¨é‡ç”Ÿæˆèƒ½åŠ›éš¾ä»¥æœ‰æ•ˆæ•´åˆçš„é—®é¢˜ï¼ŒMAGUSé‡‡ç”¨äº†è®¤çŸ¥ï¼ˆCognitionï¼‰ä¸å®¡è®®ï¼ˆDeliberationï¼‰ä¸¤ä¸ªè§£è€¦é˜¶æ®µçš„æ¨¡å—åŒ–è®¾è®¡ã€‚åœ¨è®¤çŸ¥é˜¶æ®µï¼Œç”±Perceiverã€Plannerå’ŒReflectorä¸‰ä¸ªè§’è‰²åŒ–çš„å¤šæ¨¡æ€LLMæ™ºèƒ½ä½“é€šè¿‡åä½œå¯¹è¯è¿›è¡Œç»“æ„åŒ–ç†è§£ä¸è§„åˆ’ã€‚åœ¨å®¡è®®é˜¶æ®µï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†Growth-Aware Searchæœºåˆ¶ï¼Œä»¥äº’è¡¥çš„æ–¹å¼åè°ƒåŸºäºLLMçš„æ¨ç†å’ŒåŸºäºæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆã€‚MAGUSå…·å¤‡æ’ä»¶å¼çš„æ‰©å±•èƒ½åŠ›ï¼Œæ”¯æŒå¯æ‰©å±•çš„any-to-anyæ¨¡æ€è½¬æ¢ä¸è¯­ä¹‰å¯¹é½ï¼Œä¸”æ— éœ€ä»»ä½•è”åˆè®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMAGUSåœ¨å›¾åƒã€è§†é¢‘å’ŒéŸ³é¢‘ç”Ÿæˆç­‰å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºSOTAç³»ç»Ÿï¼Œå…¶åœ¨MMEåŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ç”šè‡³è¶…è¶Šäº†GPT-4oã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "comment": "8 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.10494v1",
      "published_date": "2025-08-14 09:52:51 UTC",
      "updated_date": "2025-08-14 09:52:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:52:57.550890+00:00"
    },
    {
      "arxiv_id": "2508.10492v1",
      "title": "Reverse Physician-AI Relationship: Full-process Clinical Diagnosis Driven by a Large Language Model",
      "title_zh": "åŒ»ç”Ÿ-äººå·¥æ™ºèƒ½å…³ç³»çš„é€†è½¬ï¼šå¤§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„å…¨æµç¨‹ä¸´åºŠè¯Šæ–­",
      "authors": [
        "Shicheng Xu",
        "Xin Huang",
        "Zihao Wei",
        "Liang Pang",
        "Huawei Shen",
        "Xueqi Cheng"
      ],
      "abstract": "Full-process clinical diagnosis in the real world encompasses the entire diagnostic workflow that begins with only an ambiguous chief complaint. While artificial intelligence (AI), particularly large language models (LLMs), is transforming clinical diagnosis, its role remains largely as an assistant to physicians. This AI-assisted working pattern makes AI can only answer specific medical questions at certain parts within the diagnostic process, but lack the ability to drive the entire diagnostic process starting from an ambiguous complaint, which still relies heavily on human physicians. This gap limits AI's ability to fully reduce physicians' workload and enhance diagnostic efficiency. To address this, we propose a paradigm shift that reverses the relationship between physicians and AI: repositioning AI as the primary director, with physicians serving as its assistants. So we present DxDirector-7B, an LLM endowed with advanced deep thinking capabilities, enabling it to drive the full-process diagnosis with minimal physician involvement. Furthermore, DxDirector-7B establishes a robust accountability framework for misdiagnoses, delineating responsibility between AI and human physicians. In evaluations across rare, complex, and real-world cases under full-process diagnosis setting, DxDirector-7B not only achieves significant superior diagnostic accuracy but also substantially reduces physician workload than state-of-the-art medical LLMs as well as general-purpose LLMs. Fine-grained analyses across multiple clinical departments and tasks validate its efficacy, with expert evaluations indicating its potential to serve as a viable substitute for medical specialists. These findings mark a new era where AI, traditionally a physicians' assistant, now drives the entire diagnostic process to drastically reduce physicians' workload, indicating an efficient and accurate diagnostic solution.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å½“å‰åŒ»ç–—AIä»…èƒ½è¾…åŠ©å›ç­”ç‰¹å®šåŒ»ç–—é—®é¢˜è€Œæ— æ³•ä¸»å¯¼å…¨æµç¨‹è¯Šæ–­çš„å±€é™ï¼Œæå‡ºäº†ä¸€ç§åè½¬åŒ»ç”Ÿä¸AIå…³ç³»çš„æ–°èŒƒå¼ï¼Œå°†AIå®šä½ä¸ºè¯Šæ–­è¿‡ç¨‹çš„ä¸»è¦é©±åŠ¨è€…ã€‚ä¸ºæ­¤ï¼Œä½œè€…å¼€å‘äº†å…·å¤‡å…ˆè¿›æ·±åº¦æ€ç»´(deep thinking)èƒ½åŠ›çš„ DxDirector-7B æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨æå°‘åŒ»ç”Ÿå‚ä¸çš„æƒ…å†µä¸‹ï¼Œä»æ¨¡ç³Šçš„ä¸»è¯‰å¼€å§‹é©±åŠ¨å®Œæ•´çš„ä¸´åºŠè¯Šæ–­æµç¨‹ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹è¿˜å»ºç«‹äº†ä¸€å¥—é—®è´£æ¡†æ¶ï¼Œç”¨äºæ˜ç¡®AIä¸åŒ»ç”Ÿåœ¨è¯¯è¯Šä¸­çš„è´£ä»»åˆ’åˆ†ã€‚åœ¨é’ˆå¯¹å¤æ‚åŠç°å®ç—…ä¾‹çš„è¯„ä¼°ä¸­ï¼ŒDxDirector-7B çš„è¯Šæ–­å‡†ç¡®ç‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„é€šç”¨åŠåŒ»ç–—å¤§è¯­è¨€æ¨¡å‹(LLMs)ï¼Œå¹¶å¤§å¹…å‡è½»äº†åŒ»ç”Ÿçš„å·¥ä½œè´Ÿæ‹…ã€‚å¤šç§‘å®¤çš„ç²¾ç»†åŒ–åˆ†æä¸ä¸“å®¶è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ¨¡å‹å…·æœ‰ä½œä¸ºä¸“ç§‘åŒ»ç”Ÿå¯è¡Œæ›¿ä»£æ–¹æ¡ˆçš„æ½œåŠ›ï¼Œå¼€å¯äº†AIä¸»å¯¼å…¨æµç¨‹è¯Šæ–­ä»¥å®ç°é«˜æ•ˆç²¾å‡†åŒ»ç–—çš„æ–°çºªå…ƒã€‚",
      "categories": [
        "cs.AI",
        "cs.CE",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "39 pages",
      "pdf_url": "https://arxiv.org/pdf/2508.10492v1",
      "published_date": "2025-08-14 09:51:20 UTC",
      "updated_date": "2025-08-14 09:51:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:53:10.857626+00:00"
    },
    {
      "arxiv_id": "2508.10491v1",
      "title": "Contrastive ECOC: Learning Output Codes for Adversarial Defense",
      "title_zh": "å¯¹æ¯”å¼ ECOCï¼šé¢å‘å¯¹æŠ—é˜²å¾¡çš„è¾“å‡ºç å­¦ä¹ ",
      "authors": [
        "Che-Yu Chou",
        "Hung-Hsuan Chen"
      ],
      "abstract": "Although one-hot encoding is commonly used for multiclass classification, it is not always the most effective encoding mechanism. Error Correcting Output Codes (ECOC) address multiclass classification by mapping each class to a unique codeword used as a label. Traditional ECOC methods rely on manually designed or randomly generated codebooks, which are labor-intensive and may yield suboptimal, dataset-agnostic results. This paper introduces three models for automated codebook learning based on contrastive learning, allowing codebooks to be learned directly and adaptively from data. Across four datasets, our proposed models demonstrate superior robustness to adversarial attacks compared to two baselines. The source is available at https://github.com/YuChou20/Automated-Codebook-Learning-with-Error-Correcting-Output-Code-Technique.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šç±»åˆ†ç±»ä»»åŠ¡ä¸­ one-hot encoding çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§åä¸º Contrastive ECOC çš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¯¹æ¯”å­¦ä¹  (contrastive learning) å®ç°è‡ªåŠ¨åŒ–çš„ä»£ç æœ¬ (codebook) å­¦ä¹ ã€‚é’ˆå¯¹ä¼ ç»Ÿ Error Correcting Output Codes (ECOC) æ–¹æ³•ä¾èµ–äººå·¥è®¾è®¡æˆ–éšæœºç”Ÿæˆä¸”éš¾ä»¥è‡ªé€‚åº”æ•°æ®ç‰¹æ€§çš„ç¼ºé™·ï¼Œè¯¥è®ºæ–‡å¼•å…¥äº†ä¸‰ç§èƒ½å¤Ÿç›´æ¥ä»æ•°æ®ä¸­å­¦ä¹ è¾“å‡ºç çš„æ¨¡å‹ã€‚å®éªŒç»“æœåœ¨å››ä¸ªæ•°æ®é›†ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜å…¶åœ¨åº”å¯¹å¯¹æŠ—æ”»å‡» (adversarial attacks) æ—¶ç›¸è¾ƒäºä¸¤ç§åŸºå‡†æ¨¡å‹è¡¨ç°å‡ºæ›´å“è¶Šçš„é²æ£’æ€§ã€‚è¿™ä¸€å·¥ä½œé€šè¿‡è‡ªåŠ¨åŒ–ç”Ÿæˆæœ€ä¼˜è¾“å‡ºç ï¼Œä¸ºå¢å¼ºæ·±åº¦å­¦ä¹ æ¨¡å‹çš„å¯¹æŠ—é˜²å¾¡ (adversarial defense) èƒ½åŠ›æä¾›äº†æ–°é€”å¾„ï¼Œä¸”ç›¸å…³ä»£ç å·²åœ¨ GitHub å¼€æºã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10491v1",
      "published_date": "2025-08-14 09:50:50 UTC",
      "updated_date": "2025-08-14 09:50:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:53:13.427776+00:00"
    },
    {
      "arxiv_id": "2508.10490v1",
      "title": "On the Complexity-Faithfulness Trade-off of Gradient-Based Explanations",
      "title_zh": "è®ºåŸºäºæ¢¯åº¦è§£é‡Šçš„å¤æ‚åº¦ä¸å¿ å®åº¦æƒè¡¡",
      "authors": [
        "Amir Mehrpanah",
        "Matteo Gamba",
        "Kevin Smith",
        "Hossein Azizpour"
      ],
      "abstract": "ReLU networks, while prevalent for visual data, have sharp transitions, sometimes relying on individual pixels for predictions, making vanilla gradient-based explanations noisy and difficult to interpret. Existing methods, such as GradCAM, smooth these explanations by producing surrogate models at the cost of faithfulness. We introduce a unifying spectral framework to systematically analyze and quantify smoothness, faithfulness, and their trade-off in explanations. Using this framework, we quantify and regularize the contribution of ReLU networks to high-frequency information, providing a principled approach to identifying this trade-off. Our analysis characterizes how surrogate-based smoothing distorts explanations, leading to an ``explanation gap'' that we formally define and measure for different post-hoc methods. Finally, we validate our theoretical findings across different design choices, datasets, and ablations.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ReLUç½‘ç»œåœ¨å¤„ç†è§†è§‰æ•°æ®æ—¶ç”±äºå‰§çƒˆè½¬æ¢å¯¼è‡´ä¼ ç»ŸGradient-Based Explanationsäº§ç”Ÿå™ªå£°ä¸”éš¾ä»¥è§£é‡Šçš„é—®é¢˜ã€‚é’ˆå¯¹ç°æœ‰GradCAMç­‰æ–¹æ³•è™½èƒ½å®ç°å¹³æ»‘è§£é‡Šä½†å¾€å¾€ç‰ºç‰²Faithfulnessçš„ç°çŠ¶ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„Spectral Frameworkï¼Œç”¨äºç³»ç»Ÿåœ°åˆ†æå¹¶é‡åŒ–å¹³æ»‘åº¦ã€å¿ å®æ€§åŠå…¶æƒè¡¡å…³ç³»ã€‚è¯¥æ¡†æ¶é€šè¿‡é‡åŒ–å’Œæ­£åˆ™åŒ–ReLUç½‘ç»œå¯¹é«˜é¢‘ä¿¡æ¯çš„è´¡çŒ®ï¼Œä¸ºè¯†åˆ«è¿™ç§æƒè¡¡æä¾›äº†åŸåˆ™æ€§æ–¹æ³•ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ­£å¼å®šä¹‰å¹¶æµ‹é‡äº†ä¸åŒPost-hoc Methodsä¸­ç”±äºå¹³æ»‘å¤„ç†å¯¼è‡´çš„Explanation Gapï¼Œæ­ç¤ºäº†ä»£ç†æ¨¡å‹å¯¹è§£é‡ŠçœŸå®æ€§çš„æ‰­æ›²ã€‚æœ€åï¼Œé€šè¿‡åœ¨å¤šç§è®¾è®¡é€‰æ‹©ã€æ•°æ®é›†å’Œæ¶ˆèå®éªŒä¸Šçš„æµ‹è¯•ï¼ŒéªŒè¯äº†è¯¥ç†è®ºå‘ç°çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "23 pages, 14 figures, to be published in International Conference on Computer Vision 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.10490v1",
      "published_date": "2025-08-14 09:49:07 UTC",
      "updated_date": "2025-08-14 09:49:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:53:16.298797+00:00"
    },
    {
      "arxiv_id": "2508.10486v1",
      "title": "SEQ-GPT: LLM-assisted Spatial Query via Example",
      "title_zh": "SEQ-GPTï¼šå¤§è¯­è¨€æ¨¡å‹è¾…åŠ©çš„ç¤ºä¾‹åŒ–ç©ºé—´æŸ¥è¯¢",
      "authors": [
        "Ivan Khai Ze Lim",
        "Ningyi Liao",
        "Yiming Yang",
        "Gerald Wei Yong Yip",
        "Siqiang Luo"
      ],
      "abstract": "Contemporary spatial services such as online maps predominantly rely on user queries for location searches. However, the user experience is limited when performing complex tasks, such as searching for a group of locations simultaneously. In this study, we examine the extended scenario known as Spatial Exemplar Query (SEQ), where multiple relevant locations are jointly searched based on user-specified examples. We introduce SEQ-GPT, a spatial query system powered by Large Language Models (LLMs) towards more versatile SEQ search using natural language. The language capabilities of LLMs enable unique interactive operations in the SEQ process, including asking users to clarify query details and dynamically adjusting the search based on user feedback. We also propose a tailored LLM adaptation pipeline that aligns natural language with structured spatial data and queries through dialogue synthesis and multi-model cooperation. SEQ-GPT offers an end-to-end demonstration for broadening spatial search with realistic data and application scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åœ¨çº¿åœ°å›¾ç­‰ç°æœ‰ç©ºé—´æœåŠ¡åœ¨å¤„ç†å¤šåœ°ç‚¹åŒæ­¥æœç´¢ç­‰å¤æ‚ä»»åŠ¡æ—¶çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§åä¸º SEQ-GPT çš„ç©ºé—´æŸ¥è¯¢ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„èƒ½åŠ›ï¼Œå®ç°äº†åŸºäºè‡ªç„¶è¯­è¨€çš„ç©ºé—´ç¤ºä¾‹æŸ¥è¯¢ï¼ˆSpatial Exemplar Query, SEQï¼‰ï¼Œèƒ½å¤Ÿæ”¯æŒæŸ¥è¯¢è¿‡ç¨‹ä¸­çš„ç»†èŠ‚æ¾„æ¸…å’ŒåŸºäºç”¨æˆ·åé¦ˆçš„åŠ¨æ€æœç´¢è°ƒæ•´ã€‚ç ”ç©¶æå‡ºäº†ä¸€å¥—å®šåˆ¶çš„å¤§è¯­è¨€æ¨¡å‹é€‚é…æµæ°´çº¿ï¼Œé€šè¿‡å¯¹è¯åˆæˆï¼ˆDialogue Synthesisï¼‰å’Œå¤šæ¨¡å‹åä½œï¼ˆMulti-model Cooperationï¼‰å°†è‡ªç„¶è¯­è¨€ä¸ç»“æ„åŒ–ç©ºé—´æ•°æ®åŠæŸ¥è¯¢è¿›è¡Œå¯¹é½ã€‚ä½œä¸ºä¸€ç§ç«¯åˆ°ç«¯çš„æ¼”ç¤ºæ–¹æ¡ˆï¼ŒSEQ-GPT åœ¨çœŸå®æ•°æ®å’Œåº”ç”¨åœºæ™¯ä¸‹å±•ç¤ºäº†æ‰©å±•ç©ºé—´æœç´¢èƒ½åŠ›çš„æ½œåŠ›ï¼Œä¸ºå¤æ‚åœ°ç†ä¿¡æ¯æ£€ç´¢æä¾›äº†æ›´é«˜æ•ˆçš„äº¤äº’æ–¹å¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10486v1",
      "published_date": "2025-08-14 09:41:55 UTC",
      "updated_date": "2025-08-14 09:41:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:53:24.748537+00:00"
    },
    {
      "arxiv_id": "2508.10480v1",
      "title": "Pinet: Optimizing hard-constrained neural networks with orthogonal projection layers",
      "title_zh": "Pinetï¼šåŸºäºæ­£äº¤æŠ•å½±å±‚ä¼˜åŒ–ç¡¬çº¦æŸç¥ç»ç½‘ç»œ",
      "authors": [
        "Panagiotis D. Grontas",
        "Antonio Terpin",
        "Efe C. Balta",
        "Raffaello D'Andrea",
        "John Lygeros"
      ],
      "abstract": "We introduce an output layer for neural networks that ensures satisfaction of convex constraints. Our approach, $Î $net, leverages operator splitting for rapid and reliable projections in the forward pass, and the implicit function theorem for backpropagation. We deploy $Î $net as a feasible-by-design optimization proxy for parametric constrained optimization problems and obtain modest-accuracy solutions faster than traditional solvers when solving a single problem, and significantly faster for a batch of problems. We surpass state-of-the-art learning approaches in terms of training time, solution quality, and robustness to hyperparameter tuning, while maintaining similar inference times. Finally, we tackle multi-vehicle motion planning with non-convex trajectory preferences and provide $Î $net as a GPU-ready package implemented in JAX with effective tuning heuristics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† $\\Pi$netï¼Œè¿™æ˜¯ä¸€ç§ä¸ºç¥ç»ç½‘ç»œè®¾è®¡çš„è¾“å‡ºå±‚ï¼Œæ—¨åœ¨å¼ºåˆ¶å…¶è¾“å‡ºæ»¡è¶³å‡¸çº¦æŸ (convex constraints)ã€‚è¯¥æ–¹æ³•åœ¨å‰å‘ä¼ æ’­ä¸­åˆ©ç”¨ç®—å­åˆ†è£‚ (operator splitting) æŠ€æœ¯å®ç°å¿«é€Ÿå¯é çš„æŠ•å½±ï¼Œå¹¶åœ¨åå‘ä¼ æ’­ä¸­åº”ç”¨éšå‡½æ•°å®šç† (implicit function theorem)ã€‚ä½œä¸ºå‚æ•°åŒ–çº¦æŸä¼˜åŒ–é—®é¢˜çš„ä¼˜åŒ–ä»£ç†ï¼Œ$\\Pi$net çš„æ±‚è§£é€Ÿåº¦ä¼˜äºä¼ ç»Ÿæ±‚è§£å™¨ (traditional solvers)ï¼Œå°¤å…¶åœ¨å¤„ç†æ‰¹é‡é—®é¢˜æ—¶è¡¨ç°å“è¶Šã€‚å®éªŒè¯æ˜ï¼Œ$\\Pi$net åœ¨è®­ç»ƒæ—¶é—´ã€è§£çš„è´¨é‡åŠå¯¹è¶…å‚æ•°å¾®è°ƒ (hyperparameter tuning) çš„é²æ£’æ€§æ–¹é¢å‡è¶…è¶Šäº†ç°æœ‰çš„å…ˆè¿›å­¦ä¹ æ–¹æ³•ã€‚æœ€åï¼Œè¯¥ç ”ç©¶å°† $\\Pi$net åº”ç”¨äºå…·æœ‰éå‡¸è½¨è¿¹åå¥½çš„å¤šè½¦è¿åŠ¨è§„åˆ’ (multi-vehicle motion planning) ä»»åŠ¡ï¼Œå¹¶å‘å¸ƒäº†åŸºäº JAX çš„ GPU åŠ é€Ÿè½¯ä»¶åŒ…ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10480v1",
      "published_date": "2025-08-14 09:32:09 UTC",
      "updated_date": "2025-08-14 09:32:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:53:25.360173+00:00"
    },
    {
      "arxiv_id": "2508.14090v2",
      "title": "DLLMQuant: Quantizing Diffusion-based Large Language Models",
      "title_zh": "DLLMQuantï¼šåŸºäºæ‰©æ•£çš„å¤§è¯­è¨€æ¨¡å‹é‡åŒ–",
      "authors": [
        "Chen Xu",
        "Dawei Yang"
      ],
      "abstract": "Diffusion-based large language models (DLLMs) have shown promise for non-autoregressive text generation, but their deployment is constrained by large model sizes and heavy computational costs. Post-training quantization (PTQ), a widely used method for compressing and accelerating Large Language Models (LLMs), suffers from severe accuracy degradation and reduced generalization performance when directly applied to DLLMs (e.g., AWQ suffers a 16% accuracy drop on LLADA under W4A4). This paper explores how DLLMs' key mechanisms - dynamic masking, iterative generation, bidirectional attention - clash with quantization. We identify three core issues: 1) Iterative generation and dynamic masking ratios lead to distinct token distributions across decoding steps, which are not adequately captured by existing PTQ calibration methods; 2) Quantization errors are accumulated and amplified progressively during iteration in DLLMs, causing quantized models to perform worse as decoding steps progress; 3) Unmasked tokens stabilize while masked remain probabilistic, making overall feature distribution incompatible with existing PTQ methods. To address these issues, we propose DLLMQuant, a PTQ framework tailored for DLLMs, which incorporates three novel techniques: 1) Temporal-Mask Adaptive Sampling (TMAS), a calibration method that accounts for both time and mask factors, with the capacity to capture distributions across timesteps. 2) Interaction-Aware Activation Quantization (IA-AQ), which utilizes bidirectional attention's interaction signals to dynamically allocate quantization resources. 3) Certainty-Guided Quantization (CGQ), which integrates mask status and token scores as key weighting criteria into error compensation, making weight quantization more suitable for DLLMs. Experiments show that DLLMQuant achieves significant performance gains while enhancing efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºæ‰©æ•£çš„å¤§è¯­è¨€æ¨¡å‹(DLLMs)åœ¨éƒ¨ç½²ä¸­é¢ä¸´çš„æ¨¡å‹ä½“ç§¯å·¨å¤§ä¸è®¡ç®—æˆæœ¬é«˜æ˜‚çš„æŒ‘æˆ˜ï¼Œåˆ†æäº†ç°æœ‰è®­ç»ƒåé‡åŒ–(PTQ)æ–¹æ³•åœ¨åº”ç”¨äºæ­¤ç±»æ¨¡å‹æ—¶å‡ºç°çš„ç²¾åº¦ä¸¥é‡ä¸‹é™é—®é¢˜ã€‚ç ”ç©¶å‘ç°DLLMsçš„åŠ¨æ€æ©ç ã€è¿­ä»£ç”ŸæˆåŠåŒå‘æ³¨æ„åŠ›æœºåˆ¶å¯¼è‡´Tokenåˆ†å¸ƒéšæ­¥éª¤å˜åŒ–ã€é‡åŒ–è¯¯å·®é€çº§ç´¯ç§¯ï¼Œå¹¶ä½¿å¾—ç‰¹å¾åˆ†å¸ƒä¸ä¼ ç»ŸPTQæ–¹æ³•ä¸å…¼å®¹ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸“ä¸ºDLLMsè®¾è®¡çš„DLLMQuanté‡åŒ–æ¡†æ¶ï¼Œæ¶µç›–äº†èƒ½å¤Ÿæ•æ‰è·¨æ—¶é—´æ­¥åˆ†å¸ƒçš„Temporal-Mask Adaptive Sampling (TMAS)æ ¡å‡†æŠ€æœ¯ã€‚è¯¥æ¡†æ¶è¿˜åŒ…å«åˆ©ç”¨åŒå‘æ³¨æ„åŠ›ä¿¡å·åŠ¨æ€åˆ†é…èµ„æºçš„Interaction-Aware Activation Quantization (IA-AQ)ï¼Œä»¥åŠå°†æ©ç çŠ¶æ€å’ŒTokenå¾—åˆ†ä½œä¸ºè¯¯å·®è¡¥å¿æƒé‡çš„Certainty-Guided Quantization (CGQ)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDLLMQuantåœ¨æ˜¾è‘—å¢å¼ºæ¨¡å‹éƒ¨ç½²æ•ˆç‡çš„åŒæ—¶ï¼Œå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œæœ‰æ•ˆè§£å†³äº†é‡åŒ–è¿‡ç¨‹ä¸­çš„æ³›åŒ–æ€§ä¸ç²¾åº¦ä¸‹é™é—®é¢˜ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.14090v2",
      "published_date": "2025-08-14 09:30:17 UTC",
      "updated_date": "2025-08-26 02:18:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:53:33.986012+00:00"
    },
    {
      "arxiv_id": "2508.10469v1",
      "title": "Enhanced Sparse Point Cloud Data Processing for Privacy-aware Human Action Recognition",
      "title_zh": "é¢å‘éšç§æ„ŸçŸ¥äººä½“åŠ¨ä½œè¯†åˆ«çš„å¢å¼ºå‹ç¨€ç–ç‚¹äº‘æ•°æ®å¤„ç†",
      "authors": [
        "Maimunatu Tunau",
        "Vincent Gbouna Zakka",
        "Zhuangzhuang Dai"
      ],
      "abstract": "Human Action Recognition (HAR) plays a crucial role in healthcare, fitness tracking, and ambient assisted living technologies. While traditional vision based HAR systems are effective, they pose privacy concerns. mmWave radar sensors offer a privacy preserving alternative but present challenges due to the sparse and noisy nature of their point cloud data. In the literature, three primary data processing methods: Density-Based Spatial Clustering of Applications with Noise (DBSCAN), the Hungarian Algorithm, and Kalman Filtering have been widely used to improve the quality and continuity of radar data. However, a comprehensive evaluation of these methods, both individually and in combination, remains lacking. This paper addresses that gap by conducting a detailed performance analysis of the three methods using the MiliPoint dataset. We evaluate each method individually, all possible pairwise combinations, and the combination of all three, assessing both recognition accuracy and computational cost. Furthermore, we propose targeted enhancements to the individual methods aimed at improving accuracy. Our results provide crucial insights into the strengths and trade-offs of each method and their integrations, guiding future work on mmWave based HAR systems",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ¯«ç±³æ³¢é›·è¾¾(mmWave radar)åœ¨äººä½“åŠ¨ä½œè¯†åˆ«(HAR)ä¸­é¢ä¸´çš„ç‚¹äº‘æ•°æ®ç¨€ç–ä¸å™ªå£°æŒ‘æˆ˜ï¼Œå¯¹DBSCANã€Hungarian Algorithmå’ŒKalman Filteringä¸‰ç§æ ¸å¿ƒæ•°æ®å¤„ç†æ–¹æ³•è¿›è¡Œäº†ç³»ç»Ÿæ€§è¯„ä¼°ã€‚é€šè¿‡ä½¿ç”¨MiliPointæ•°æ®é›†ï¼Œè®ºæ–‡æ·±å…¥åˆ†æäº†è¿™äº›æ–¹æ³•åœ¨ç‹¬ç«‹ä½¿ç”¨åŠå¤šç§ç»„åˆé…ç½®ä¸‹çš„è¯†åˆ«å‡†ç¡®ç‡ä¸è®¡ç®—å¼€é”€ï¼Œå¡«è¡¥äº†ç°æœ‰æ–‡çŒ®ä¸­ç¼ºä¹ç»¼åˆè¯„ä»·çš„ç©ºç™½ã€‚ä½œè€…è¿˜é’ˆå¯¹æ¯ç§æ–¹æ³•æå‡ºäº†ç‰¹å®šçš„ä¼˜åŒ–å¢å¼ºæ–¹æ¡ˆï¼Œæ—¨åœ¨è¿›ä¸€æ­¥æå‡ç³»ç»Ÿçš„åŠ¨ä½œè¯†åˆ«æ€§èƒ½ã€‚å®éªŒç»“æœè¯¦ç»†é˜è¿°äº†ä¸åŒå¤„ç†è·¯å¾„çš„ä¼˜åŠ£åŠ¿åŠå…¶ç›¸äº’æƒè¡¡ï¼Œä¸ºæ„å»ºé«˜æ•ˆã€éšç§ä¿æŠ¤çš„æ¯«ç±³æ³¢é›·è¾¾HARç³»ç»Ÿæä¾›äº†å…³é”®çš„æŠ€æœ¯æ”¯æ’‘ä¸è®¾è®¡å‡†åˆ™ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10469v1",
      "published_date": "2025-08-14 09:09:49 UTC",
      "updated_date": "2025-08-14 09:09:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:53:30.689558+00:00"
    },
    {
      "arxiv_id": "2508.10467v1",
      "title": "FIRESPARQL: A LLM-based Framework for SPARQL Query Generation over Scholarly Knowledge Graphs",
      "title_zh": "FIRESPARQLï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å­¦æœ¯çŸ¥è¯†å›¾è°± SPARQL æŸ¥è¯¢ç”Ÿæˆæ¡†æ¶",
      "authors": [
        "Xueli Pan",
        "Victor de Boer",
        "Jacco van Ossenbruggen"
      ],
      "abstract": "Question answering over Scholarly Knowledge Graphs (SKGs) remains a challenging task due to the complexity of scholarly content and the intricate structure of these graphs. Large Language Model (LLM) approaches could be used to translate natural language questions (NLQs) into SPARQL queries; however, these LLM-based approaches struggle with SPARQL query generation due to limited exposure to SKG-specific content and the underlying schema. We identified two main types of errors in the LLM-generated SPARQL queries: (i) structural inconsistencies, such as missing or redundant triples in the queries, and (ii) semantic inaccuracies, where incorrect entities or properties are shown in the queries despite a correct query structure. To address these issues, we propose FIRESPARQL, a modular framework that supports fine-tuned LLMs as a core component, with optional context provided via retrieval-augmented generation (RAG) and a SPARQL query correction layer. We evaluate the framework on the SciQA Benchmark using various configurations (zero-shot, zero-shot with RAG, one-shot, fine-tuning, and fine-tuning with RAG) and compare the performance with baseline and state-of-the-art approaches. We measure query accuracy using BLEU and ROUGE metrics, and query result accuracy using relaxed exact match(RelaxedEM), with respect to the gold standards containing the NLQs, SPARQL queries, and the results of the queries. Experimental results demonstrate that fine-tuning achieves the highest overall performance, reaching 0.90 ROUGE-L for query accuracy and 0.85 RelaxedEM for result accuracy on the test set.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å­¦æœ¯çŸ¥è¯†å›¾è°±(Scholarly Knowledge Graphs, SKGs)ä¸­è‡ªç„¶è¯­è¨€é—®é¢˜è½¬åŒ–ä¸ºSPARQLæŸ¥è¯¢çš„å¤æ‚æ€§ï¼Œæå‡ºäº†FIRESPARQLæ¨¡å—åŒ–æ¡†æ¶ã€‚ç ”ç©¶è¯†åˆ«äº†å¤§è¯­è¨€æ¨¡å‹(LLM)åœ¨ç”ŸæˆæŸ¥è¯¢æ—¶çš„ç»“æ„ä¸ä¸€è‡´å’Œè¯­ä¹‰ä¸å‡†ç¡®ä¸¤å¤§æ ¸å¿ƒé”™è¯¯ç±»å‹ï¼Œå¹¶é’ˆå¯¹æ€§åœ°é›†æˆäº†å¾®è°ƒ(fine-tuning)æ¨¡å‹ã€æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)å’ŒæŸ¥è¯¢æ ¡æ­£å±‚ã€‚é€šè¿‡åœ¨SciQAåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œé›¶æ ·æœ¬ã€å•æ ·æœ¬åŠå¾®è°ƒç­‰å¤šç§å®éªŒé…ç½®çš„è¯„ä¼°ï¼ŒFIRESPARQLå±•ç¤ºäº†å“è¶Šçš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¾®è°ƒåçš„æ¨¡å‹è¡¨ç°æœ€ä¸ºå‡ºè‰²ï¼Œå…¶æŸ¥è¯¢å‡†ç¡®ç‡è¾¾åˆ°0.90 ROUGE-Lï¼Œç»“æœå‡†ç¡®ç‡è¾¾åˆ°0.85 RelaxedEMã€‚è¯¥æ¡†æ¶æ˜¾è‘—æå‡äº†åœ¨å¤æ‚å­¦æœ¯å›¾è°±ç»“æ„ä¸‹ç”Ÿæˆç²¾ç¡®SPARQLæŸ¥è¯¢çš„èƒ½åŠ›ï¼Œä¸ºæ„å»ºé«˜æ•ˆçš„å­¦æœ¯é—®ç­”ç³»ç»Ÿæä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.DL"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at 17th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management (IC3K)",
      "pdf_url": "https://arxiv.org/pdf/2508.10467v1",
      "published_date": "2025-08-14 09:08:50 UTC",
      "updated_date": "2025-08-14 09:08:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:53:33.389870+00:00"
    },
    {
      "arxiv_id": "2508.10461v1",
      "title": "X-Node: Self-Explanation is All We Need",
      "title_zh": "X-Nodeï¼šè‡ªè§£é‡Šå³å…¨éƒ¨æ‰€éœ€",
      "authors": [
        "Prajit Sengupta",
        "Islem Rekik"
      ],
      "abstract": "Graph neural networks (GNNs) have achieved state-of-the-art results in computer vision and medical image classification tasks by capturing structural dependencies across data instances. However, their decision-making remains largely opaque, limiting their trustworthiness in high-stakes clinical applications where interpretability is essential. Existing explainability techniques for GNNs are typically post-hoc and global, offering limited insight into individual node decisions or local reasoning. We introduce X-Node, a self-explaining GNN framework in which each node generates its own explanation as part of the prediction process. For every node, we construct a structured context vector encoding interpretable cues such as degree, centrality, clustering, feature saliency, and label agreement within its local topology. A lightweight Reasoner module maps this context into a compact explanation vector, which serves three purposes: (1) reconstructing the node's latent embedding via a decoder to enforce faithfulness, (2) generating a natural language explanation using a pre-trained LLM (e.g., Grok or Gemini), and (3) guiding the GNN itself via a \"text-injection\" mechanism that feeds explanations back into the message-passing pipeline. We evaluate X-Node on two graph datasets derived from MedMNIST and MorphoMNIST, integrating it with GCN, GAT, and GIN backbones. Our results show that X-Node maintains competitive classification accuracy while producing faithful, per-node explanations. Repository: https://github.com/basiralab/X-Node.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å›¾ç¥ç»ç½‘ç»œ(Graph Neural Networks, GNNs)åœ¨åŒ»ç–—å›¾åƒåˆ†ç±»ä¸­å› å†³ç­–ä¸é€æ˜è€Œé™åˆ¶å…¶åœ¨ä¸´åºŠåº”ç”¨ä¸­ä¿¡ä»»åº¦çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºX-Nodeçš„è‡ªè§£é‡Šæ¡†æ¶ã€‚è¯¥æ¡†æ¶å…è®¸æ¯ä¸ªèŠ‚ç‚¹åœ¨é¢„æµ‹è¿‡ç¨‹ä¸­ç”Ÿæˆè‡ªèº«è§£é‡Šï¼Œé€šè¿‡æ„å»ºåŒ…å«åº¦(degree)ã€ä¸­å¿ƒæ€§(centrality)ã€ç‰¹å¾æ˜¾è‘—æ€§(feature saliency)åŠå±€éƒ¨æ‹“æ‰‘æ ‡ç­¾ä¸€è‡´æ€§ç­‰å¯è§£é‡Šçº¿ç´¢çš„ç»“æ„åŒ–ä¸Šä¸‹æ–‡å‘é‡ã€‚è½»é‡çº§çš„Reasoneræ¨¡å—å°†è¿™äº›ä¸Šä¸‹æ–‡æ˜ å°„ä¸ºè§£é‡Šå‘é‡ï¼Œç”¨äºé€šè¿‡è§£ç å™¨é‡å»ºæ½œåµŒå…¥ä»¥ç¡®ä¿å¿ å®æ€§(faithfulness)ï¼Œå¹¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLM)ç”Ÿæˆè‡ªç„¶è¯­è¨€è¯´æ˜ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¼•å…¥äº†æ–‡æœ¬æ³¨å…¥(text-injection)æœºåˆ¶ï¼Œå°†è§£é‡Šåé¦ˆè‡³æ¶ˆæ¯ä¼ é€’ç®¡é“ä»¥å¼•å¯¼æ¨¡å‹å­¦ä¹ ã€‚åœ¨MedMNISTå’ŒMorphoMNISTæ•°æ®é›†ä¸Šç»“åˆGCNã€GATåŠGINä¸»å¹²ç½‘ç»œçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒX-Nodeåœ¨ä¿æŒç«äº‰æ€§åˆ†ç±»å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œèƒ½ä¸ºæ¯ä¸ªèŠ‚ç‚¹æä¾›é«˜è´¨é‡ä¸”å…·æœ‰å¿ å®æ€§çš„è§£é‡Šã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10461v1",
      "published_date": "2025-08-14 09:00:45 UTC",
      "updated_date": "2025-08-14 09:00:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:53:41.691762+00:00"
    },
    {
      "arxiv_id": "2508.10455v1",
      "title": "RealAC: A Domain-Agnostic Framework for Realistic and Actionable Counterfactual Explanations",
      "title_zh": "RealACï¼šä¸€ç§ç”¨äºç”ŸæˆçœŸå®ä¸”å¯æ“ä½œåäº‹å®è§£é‡Šçš„é¢†åŸŸæ— å…³æ¡†æ¶",
      "authors": [
        "Asiful Arefeen",
        "Shovito Barua Soumma",
        "Hassan Ghasemzadeh"
      ],
      "abstract": "Counterfactual explanations provide human-understandable reasoning for AI-made decisions by describing minimal changes to input features that would alter a model's prediction. To be truly useful in practice, such explanations must be realistic and feasible -- they should respect both the underlying data distribution and user-defined feasibility constraints. Existing approaches often enforce inter-feature dependencies through rigid, hand-crafted constraints or domain-specific knowledge, which limits their generalizability and ability to capture complex, nonlinear relations inherent in data. Moreover, they rarely accommodate user-specified preferences and suggest explanations that are causally implausible or infeasible to act upon. We introduce RealAC, a domain-agnostic framework for generating realistic and actionable counterfactuals. RealAC automatically preserves complex inter-feature dependencies without relying on explicit domain knowledge -- by aligning the joint distributions of feature pairs between factual and counterfactual instances. The framework also allows end-users to ``freeze'' attributes they cannot or do not wish to change by suppressing change in frozen features during optimization. Evaluations on three synthetic and two real datasets demonstrate that RealAC balances realism with actionability. Our method outperforms state-of-the-art baselines and Large Language Model-based counterfactual generation techniques in causal edge score, dependency preservation score, and IM1 realism metric and offers a solution for causality-aware and user-centric counterfactual generation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†RealACï¼Œä¸€ä¸ªæ—¨åœ¨ç”Ÿæˆå…·æœ‰çœŸå®æ€§(Realistic)å’Œå¯æ“ä½œæ€§(Actionable)çš„åäº‹å®è§£é‡Š(Counterfactual Explanations)çš„é€šç”¨æ¡†æ¶ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•è¿‡åº¦ä¾èµ–é¢†åŸŸçŸ¥è¯†ä¸”éš¾ä»¥æ•æ‰å¤æ‚éçº¿æ€§ç‰¹å¾ä¾èµ–çš„é—®é¢˜ã€‚RealACé€šè¿‡å¯¹é½äº‹å®ä¸åäº‹å®å®ä¾‹ä¹‹é—´ç‰¹å¾å¯¹çš„è”åˆåˆ†å¸ƒ(Joint Distributions)ï¼Œèƒ½å¤Ÿåœ¨æ— éœ€æ˜¾å¼é¢†åŸŸçŸ¥è¯†çš„æƒ…å†µä¸‹è‡ªåŠ¨ä¿ç•™å¤æ‚çš„ç‰¹å¾é—´ä¾èµ–å…³ç³»ã€‚è¯¥æ¡†æ¶è¿˜å…è®¸ç”¨æˆ·â€œå†»ç»“â€(Freeze)ç‰¹å®šå±æ€§ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„å»ºè®®ç¬¦åˆç”¨æˆ·çš„å¯è¡Œæ€§çº¦æŸå’Œåå¥½ã€‚åœ¨åˆæˆåŠçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼ŒRealACåœ¨å› æœè¾¹ç¼˜å¾—åˆ†(Causal Edge Score)ã€ä¾èµ–ä¿ç•™å¾—åˆ†(Dependency Preservation Score)å’ŒIM1çœŸå®æ€§åº¦é‡ä¸Šå‡ä¼˜äºç°æœ‰çš„å…ˆè¿›åŸºçº¿æ¨¡å‹ä»¥åŠåŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„ç”ŸæˆæŠ€æœ¯ã€‚è¯¥ç ”ç©¶ä¸ºå®ç°å› æœæ„ŸçŸ¥(Causality-aware)å’Œä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒçš„åäº‹å®ç”Ÿæˆæä¾›äº†ä¸€ç§é¢†åŸŸæ— å…³çš„é«˜æ•ˆè§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10455v1",
      "published_date": "2025-08-14 08:51:39 UTC",
      "updated_date": "2025-08-14 08:51:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:53:50.086079+00:00"
    },
    {
      "arxiv_id": "2508.10436v1",
      "title": "Alternating Approach-Putt Models for Multi-Stage Speech Enhancement",
      "title_zh": "é¢å‘å¤šé˜¶æ®µè¯­éŸ³å¢å¼ºçš„äº¤æ›¿å¼ Approach-Putt æ¨¡å‹",
      "authors": [
        "Iksoon Jeong",
        "Kyung-Joong Kim",
        "Kang-Hun Ahn"
      ],
      "abstract": "Speech enhancement using artificial neural networks aims to remove noise from noisy speech signals while preserving the speech content. However, speech enhancement networks often introduce distortions to the speech signal, referred to as artifacts, which can degrade audio quality. In this work, we propose a post-processing neural network designed to mitigate artifacts introduced by speech enhancement models. Inspired by the analogy of making a `Putt' after an `Approach' in golf, we name our model PuttNet. We demonstrate that alternating between a speech enhancement model and the proposed Putt model leads to improved speech quality, as measured by perceptual quality scores (PESQ), objective intelligibility (STOI), and background noise intrusiveness (CBAK) scores. Furthermore, we illustrate with graphical analysis why this alternating Approach outperforms repeated application of either model alone.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¥ç»ç½‘ç»œè¯­éŸ³å¢å¼º (Speech Enhancement) åœ¨å»é™¤å™ªå£°æ—¶å¸¸å¼•å…¥äººå·¥ä¼ªå½± (Artifacts) å¯¼è‡´éŸ³è´¨ä¸‹é™çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º PuttNet çš„åå¤„ç†ç½‘ç»œã€‚å—é«˜å°”å¤«çƒè¿åŠ¨ä¸­â€œè¿‘æœå²­åˆ‡çƒ (Approach)â€ä¸â€œæ¨æ† (Putt)â€çš„åä½œå¯å‘ï¼Œè¯¥æ–¹æ³•é€šè¿‡åœ¨è¯­éŸ³å¢å¼ºæ¨¡å‹ä¸ PuttNet ä¹‹é—´äº¤æ›¿è¿­ä»£æ¥ä¼˜åŒ–ä¿¡å·è´¨é‡ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¿™ç§äº¤æ›¿å¤„ç†æ¨¡å¼åœ¨æ„ŸçŸ¥è´¨é‡ (PESQ)ã€å®¢è§‚å¯æ‡‚åº¦ (STOI) å’ŒèƒŒæ™¯å™ªå£°ä¾µå…¥æ€§ (CBAK) ç­‰æŒ‡æ ‡ä¸Šå‡ä¼˜äºåŸºçº¿æ–¹æ¡ˆã€‚ç ”ç©¶è¿›ä¸€æ­¥é€šè¿‡å›¾å½¢åˆ†æé˜æ˜äº†äº¤æ›¿åä½œæœºåˆ¶ç›¸æ¯”äºå•ä¸€æ¨¡å‹ç®€å•é‡å¤çš„ä¼˜åŠ¿ï¼Œè¯æ˜äº†è¯¥å¤šé˜¶æ®µæ¡†æ¶åœ¨æå‡è¯­éŸ³ä¿®å¤ç²¾åº¦æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "This work has been submitted to the IEEE for possible publication",
      "pdf_url": "https://arxiv.org/pdf/2508.10436v1",
      "published_date": "2025-08-14 08:18:42 UTC",
      "updated_date": "2025-08-14 08:18:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:54:59.746813+00:00"
    },
    {
      "arxiv_id": "2508.10435v1",
      "title": "Unpacking the Implicit Norm Dynamics of Sharpness-Aware Minimization in Tensorized Models",
      "title_zh": "æ­ç¤ºå¼ é‡åŒ–æ¨¡å‹ä¸­é”åº¦æ„ŸçŸ¥æœ€å°åŒ–çš„éšå¼èŒƒæ•°åŠ¨åŠ›å­¦",
      "authors": [
        "Tianxiao Cao",
        "Kyohei Atarashi",
        "Hisashi Kashima"
      ],
      "abstract": "Sharpness-Aware Minimization (SAM) has been proven to be an effective optimization technique for improving generalization in overparameterized models. While prior works have explored the implicit regularization of SAM in simple two-core scale-invariant settings, its behavior in more general tensorized or scale-invariant models remains underexplored. In this work, we leverage scale-invariance to analyze the norm dynamics of SAM in general tensorized models. We introduce the notion of \\emph{Norm Deviation} as a global measure of core norm imbalance, and derive its evolution under SAM using gradient flow analysis. We show that SAM's implicit control of Norm Deviation is governed by the covariance between core norms and their gradient magnitudes. Motivated by these findings, we propose a simple yet effective method, \\emph{Deviation-Aware Scaling (DAS)}, which explicitly mimics this regularization behavior by scaling core norms in a data-adaptive manner. Our experiments across tensor completion, noisy training, model compression, and parameter-efficient fine-tuning confirm that DAS achieves competitive or improved performance over SAM, while offering reduced computational overhead.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†Sharpness-Aware Minimization (SAM)åœ¨å¹¿ä¹‰å¼ é‡åŒ–æ¨¡å‹(tensorized models)ä¸­çš„éšå¼æ­£åˆ™åŒ–æœºåˆ¶ï¼Œæ—¨åœ¨å¡«è¡¥å…ˆå‰ç ”ç©¶ä»…é™äºç®€å•ä¸¤æ ¸å°ºåº¦ä¸å˜è®¾ç½®çš„ç©ºç™½ã€‚ä½œè€…å¼•å…¥äº†â€œèŒƒæ•°åå·®â€(Norm Deviation)çš„æ¦‚å¿µä½œä¸ºè¡¡é‡æ ¸å¿ƒèŒƒæ•°ä¸å¹³è¡¡çš„å…¨å±€æŒ‡æ ‡ï¼Œå¹¶åˆ©ç”¨æ¢¯åº¦æµåˆ†ææ¨å¯¼äº†å…¶åœ¨SAMä¸‹çš„æ¼”å˜è§„å¾‹ã€‚ç ”ç©¶å‘ç°ï¼ŒSAMå¯¹èŒƒæ•°åå·®çš„éšå¼æ§åˆ¶ä¸»è¦ç”±æ ¸å¿ƒèŒƒæ•°ä¸å…¶æ¢¯åº¦å¹…åº¦ä¹‹é—´çš„åæ–¹å·®å†³å®šã€‚åŸºäºè¿™äº›å‘ç°ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºDeviation-Aware Scaling (DAS)çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡æ•°æ®è‡ªé€‚åº”çš„æ–¹å¼ç¼©æ”¾æ ¸å¿ƒèŒƒæ•°æ¥æ˜¾å¼æ¨¡æ‹Ÿè¿™ç§æ­£åˆ™åŒ–è¡Œä¸ºã€‚åœ¨å¼ é‡è¡¥å…¨(tensor completion)ã€å™ªå£°è®­ç»ƒã€æ¨¡å‹å‹ç¼©åŠå‚æ•°é«˜æ•ˆå¾®è°ƒ(parameter-efficient fine-tuning)ç­‰ä»»åŠ¡ä¸­çš„å®éªŒè¡¨æ˜ï¼ŒDASèƒ½å¤Ÿè¾¾åˆ°ç”šè‡³è¶…è¶ŠSAMçš„æ€§èƒ½ã€‚ç›¸æ¯”äºSAMï¼ŒDASåœ¨æä¾›ç¨³å¥æ³›åŒ–èƒ½åŠ›çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—å¼€é”€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10435v1",
      "published_date": "2025-08-14 08:17:34 UTC",
      "updated_date": "2025-08-14 08:17:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:54:08.594924+00:00"
    },
    {
      "arxiv_id": "2508.10433v1",
      "title": "We-Math 2.0: A Versatile MathBook System for Incentivizing Visual Mathematical Reasoning",
      "title_zh": "We-Math 2.0ï¼šä¸€ç§æ—¨åœ¨æ¿€åŠ±è§†è§‰æ•°å­¦æ¨ç†çš„é€šç”¨ MathBook ç³»ç»Ÿ",
      "authors": [
        "Runqi Qiao",
        "Qiuna Tan",
        "Peiqing Yang",
        "Yanzi Wang",
        "Xiaowan Wang",
        "Enhui Wan",
        "Sitong Zhou",
        "Guanting Dong",
        "Yuchen Zeng",
        "Yida Xu",
        "Jie Wang",
        "Chong Sun",
        "Chen Li",
        "Honggang Zhang"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities across various tasks, but still struggle with complex mathematical reasoning. Existing research primarily focuses on dataset construction and method optimization, often overlooking two critical aspects: comprehensive knowledge-driven design and model-centric data space modeling. In this paper, we introduce We-Math 2.0, a unified system that integrates a structured mathematical knowledge system, model-centric data space modeling, and a reinforcement learning (RL)-based training paradigm to comprehensively enhance the mathematical reasoning abilities of MLLMs. The key contributions of We-Math 2.0 are fourfold: (1) MathBook Knowledge System: We construct a five-level hierarchical system encompassing 491 knowledge points and 1,819 fundamental principles. (2) MathBook-Standard & Pro: We develop MathBook-Standard, a dataset that ensures broad conceptual coverage and flexibility through dual expansion. Additionally, we define a three-dimensional difficulty space and generate 7 progressive variants per problem to build MathBook-Pro, a challenging dataset for robust training. (3) MathBook-RL: We propose a two-stage RL framework comprising: (i) Cold-Start Fine-tuning, which aligns the model with knowledge-oriented chain-of-thought reasoning; and (ii) Progressive Alignment RL, leveraging average-reward learning and dynamic data scheduling to achieve progressive alignment across difficulty levels. (4) MathBookEval: We introduce a comprehensive benchmark covering all 491 knowledge points with diverse reasoning step distributions. Experimental results show that MathBook-RL performs competitively with existing baselines on four widely-used benchmarks and achieves strong results on MathBookEval, suggesting promising generalization in mathematical reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† We-Math 2.0ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å…¨é¢æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) è§†è§‰æ•°å­¦æ¨ç†èƒ½åŠ›çš„ç»Ÿä¸€ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿæ„å»ºäº†ä¸€ä¸ªåŒ…å« 491 ä¸ªçŸ¥è¯†ç‚¹å’Œ 1,819 æ¡åŸºæœ¬åŸç†çš„äº”çº§åˆ†å±‚ MathBook Knowledge Systemï¼Œå¡«è¡¥äº†ç°æœ‰ç ”ç©¶åœ¨çŸ¥è¯†é©±åŠ¨è®¾è®¡æ–¹é¢çš„ç©ºç™½ã€‚å›¢é˜Ÿé€šè¿‡å®šä¹‰ä¸‰ç»´éš¾åº¦ç©ºé—´å¹¶ç”Ÿæˆæ¸è¿›å¼å˜ä½“ï¼Œå¼€å‘äº† MathBook-Standard å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„è®­ç»ƒæ•°æ®é›† MathBook-Proã€‚è®ºæ–‡æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µçš„ Reinforcement Learning (RL) æ¡†æ¶ MathBook-RLï¼Œé€šè¿‡å†·å¯åŠ¨å¾®è°ƒ (Cold-Start Fine-tuning) ä¸æ¸è¿›å¼å¯¹é½å¼ºåŒ–å­¦ä¹ å®ç°è·¨éš¾åº¦çš„æ¨¡å‹å¯¹é½ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†å…¨é¢çš„åŸºå‡†æµ‹è¯• MathBookEval ä»¥è¯„ä¼°æ¨¡å‹åœ¨å¤šæ ·åŒ–æ¨ç†æ­¥éª¤ä¸‹çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWe-Math 2.0 åœ¨å››ä¸ªä¸»æµåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Working in progress",
      "pdf_url": "https://arxiv.org/pdf/2508.10433v1",
      "published_date": "2025-08-14 08:15:41 UTC",
      "updated_date": "2025-08-14 08:15:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:54:47.255779+00:00"
    },
    {
      "arxiv_id": "2508.10429v1",
      "title": "MM-Food-100K: A 100,000-Sample Multimodal Food Intelligence Dataset with Verifiable Provenance",
      "title_zh": "MM-Food-100Kï¼šå…·æœ‰å¯éªŒè¯æ¥æºçš„10ä¸‡æ ·æœ¬å¤šæ¨¡æ€é£Ÿå“æ™ºèƒ½æ•°æ®é›†",
      "authors": [
        "Yi Dong",
        "Yusuke Muraoka",
        "Scott Shi",
        "Yi Zhang"
      ],
      "abstract": "We present MM-Food-100K, a public 100,000-sample multimodal food intelligence dataset with verifiable provenance. It is a curated approximately 10% open subset of an original 1.2 million, quality-accepted corpus of food images annotated for a wide range of information (such as dish name, region of creation). The corpus was collected over six weeks from over 87,000 contributors using the Codatta contribution model, which combines community sourcing with configurable AI-assisted quality checks; each submission is linked to a wallet address in a secure off-chain ledger for traceability, with a full on-chain protocol on the roadmap. We describe the schema, pipeline, and QA, and validate utility by fine-tuning large vision-language models (ChatGPT 5, ChatGPT OSS, Qwen-Max) on image-based nutrition prediction. Fine-tuning yields consistent gains over out-of-box baselines across standard metrics; we report results primarily on the MM-Food-100K subset. We release MM-Food-100K for publicly free access and retain approximately 90% for potential commercial access with revenue sharing to contributors.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† MM-Food-100Kï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å« 10,0000 ä¸ªæ ·æœ¬çš„å¤šæ¨¡æ€é£Ÿå“æ™ºèƒ½æ•°æ®é›†ï¼Œå…·æœ‰å¯éªŒè¯çš„å‡ºå¤„ (Verifiable Provenance)ã€‚è¯¥æ•°æ®é›†æ˜¯ä» 120 ä¸‡ä¸ªé«˜è´¨é‡é£Ÿå“å›¾åƒè¯­æ–™åº“ä¸­ç²¾é€‰å‡ºçš„ 10% å­é›†ï¼Œé€šè¿‡ Codatta è´¡çŒ®æ¨¡å‹æ”¶é›†ï¼Œç»“åˆäº†ç¤¾åŒºä¼—åŒ…ä¸å¯é…ç½®çš„ AI è¾…åŠ©è´¨é‡æ£€æŸ¥ï¼Œå¹¶å°†æ¯é¡¹æäº¤é“¾æ¥åˆ°å®‰å…¨é“¾ä¸‹è´¦æœ¬çš„é’±åŒ…åœ°å€ä»¥å®ç°å¯è¿½æº¯æ€§ã€‚ç ”ç©¶äººå‘˜é€šè¿‡åœ¨åŸºäºå›¾åƒçš„è¥å…»é¢„æµ‹ä»»åŠ¡ä¸Šå¾®è°ƒå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ (Vision-Language Models)ï¼ŒåŒ…æ‹¬ ChatGPT 5ã€ChatGPT OSS å’Œ Qwen-Maxï¼ŒéªŒè¯äº†æ•°æ®é›†çš„å®ç”¨æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¾®è°ƒåçš„æ¨¡å‹åœ¨å„é¡¹æ ‡å‡†æŒ‡æ ‡ä¸Šå‡ä¸€è‡´ä¼˜äºåŸå§‹åŸºçº¿æ¨¡å‹ï¼Œè¯æ˜äº†è¯¥æ•°æ®é›†åœ¨æå‡æ¨¡å‹æ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚ç›®å‰ MM-Food-100K å·²å‘å…¬ä¼—å…è´¹å¼€æ”¾ï¼Œæ—¨åœ¨ä¸ºå¯è¿½æº¯çš„å¤šæ¨¡æ€æ•°æ®ç ”ç©¶æä¾›æ”¯æŒã€‚",
      "categories": [
        "cs.AI",
        "cs.CR",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, 5 figures, 6 tables. The dataset is available at https://huggingface.co/datasets/Codatta/MM-Food-100K",
      "pdf_url": "https://arxiv.org/pdf/2508.10429v1",
      "published_date": "2025-08-14 07:59:31 UTC",
      "updated_date": "2025-08-14 07:59:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:54:19.191134+00:00"
    },
    {
      "arxiv_id": "2508.10425v1",
      "title": "HiRef: Leveraging Hierarchical Ontology and Network Refinement for Robust Medication Recommendation",
      "title_zh": "HiRefï¼šåˆ©ç”¨å±‚çº§æœ¬ä½“ä¸ç½‘ç»œç²¾ç‚¼å®ç°é²æ£’çš„è¯ç‰©æ¨è",
      "authors": [
        "Yan Ting Chok",
        "Soyon Park",
        "Seungheun Baek",
        "Hajung Kim",
        "Junhyun Lee",
        "Jaewoo Kang"
      ],
      "abstract": "Medication recommendation is a crucial task for assisting physicians in making timely decisions from longitudinal patient medical records. However, real-world EHR data present significant challenges due to the presence of rarely observed medical entities and incomplete records that may not fully capture the clinical ground truth. While data-driven models trained on longitudinal Electronic Health Records often achieve strong empirical performance, they struggle to generalize under missing or novel conditions, largely due to their reliance on observed co-occurrence patterns. To address these issues, we propose Hierarchical Ontology and Network Refinement for Robust Medication Recommendation (HiRef), a unified framework that combines two complementary structures: (i) the hierarchical semantics encoded in curated medical ontologies, and (ii) refined co-occurrence patterns derived from real-world EHRs. We embed ontology entities in hyperbolic space, which naturally captures tree-like relationships and enables knowledge transfer through shared ancestors, thereby improving generalizability to unseen codes. To further improve robustness, we introduce a prior-guided sparse regularization scheme that refines the EHR co-occurrence graph by suppressing spurious edges while preserving clinically meaningful associations. Our model achieves strong performance on EHR benchmarks (MIMIC-III and MIMIC-IV) and maintains high accuracy under simulated unseen-code settings. Extensive experiments with comprehensive ablation studies demonstrate HiRef's resilience to unseen medical codes, supported by in-depth analyses of the learned sparsified graph structure and medical code embeddings.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†HiRefï¼Œä¸€ç§ç»“åˆå±‚çº§æœ¬ä½“è®º(Hierarchical Ontology)ä¸ç½‘ç»œç²¾ç‚¼(Network Refinement)çš„é²æ£’è¯ç‰©æ¨èæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç”µå­å¥åº·æ¡£æ¡ˆ(EHR)æ•°æ®ä¸­å› ç½•è§åŒ»ç–—å®ä½“å’Œè®°å½•ä¸å®Œæ•´å¯¼è‡´çš„æ³›åŒ–èƒ½åŠ›ä¸è¶³é—®é¢˜ã€‚è¯¥æ¡†æ¶å°†åŒ»ç–—æœ¬ä½“å®ä½“åµŒå…¥åˆ°åŒæ›²ç©ºé—´(Hyperbolic Space)ä¸­ï¼Œåˆ©ç”¨å…¶å¤©ç„¶æ•æ‰æ ‘çŠ¶å±‚çº§å…³ç³»çš„ç‰¹æ€§ï¼Œé€šè¿‡å…±äº«ç¥–å…ˆèŠ‚ç‚¹å®ç°çŸ¥è¯†è¿ç§»ï¼Œä»è€Œå¢å¼ºäº†å¯¹æœªè§ä»£ç (Unseen Codes)çš„å¤„ç†èƒ½åŠ›ã€‚ä¸ºäº†æå‡ç³»ç»Ÿçš„é²æ£’æ€§ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†å…ˆéªŒå¼•å¯¼çš„ç¨€ç–æ­£åˆ™åŒ–æ–¹æ¡ˆ(Prior-guided Sparse Regularization)ï¼Œé€šè¿‡æŠ‘åˆ¶è™šå‡å…³è”å¹¶ä¿ç•™ä¸´åºŠæœ‰æ„ä¹‰çš„å…±ç°æ¨¡å¼æ¥ç²¾ç‚¼EHRå›¾ç»“æ„ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒHiRefåœ¨MIMIC-IIIå’ŒMIMIC-IVåŸºå‡†æ•°æ®é›†ä¸Šå‡è¡¨ç°ä¼˜å¼‚ï¼Œä¸”åœ¨æ¨¡æ‹Ÿçš„æ–°åŒ»ç–—ä»£ç æƒ…å¢ƒä¸‹ä»èƒ½ä¿æŒé«˜å‡†ç¡®ç‡ã€‚æ¶ˆèå®éªŒå’Œæ·±åº¦åˆ†æè¿›ä¸€æ­¥è¯æ˜äº†HiRefåœ¨å¤„ç†å¤æ‚åŒ»ç–—æ•°æ®æ—¶çš„éŸ§æ€§ï¼Œä¸ºä¸´åºŠå†³ç­–æ”¯æŒæä¾›äº†æ›´å…·æ³›åŒ–æ€§çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10425v1",
      "published_date": "2025-08-14 07:55:03 UTC",
      "updated_date": "2025-08-14 07:55:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:54:22.384794+00:00"
    },
    {
      "arxiv_id": "2508.10423v1",
      "title": "MASH: Cooperative-Heterogeneous Multi-Agent Reinforcement Learning for Single Humanoid Robot Locomotion",
      "title_zh": "MASHï¼šé¢å‘å•äººå½¢æœºå™¨äººè¿åŠ¨çš„åä½œå¼‚æ„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Qi Liu",
        "Xiaopeng Zhang",
        "Mingshan Tan",
        "Shuaikang Ma",
        "Jinliang Ding",
        "Yanjie Li"
      ],
      "abstract": "This paper proposes a novel method to enhance locomotion for a single humanoid robot through cooperative-heterogeneous multi-agent deep reinforcement learning (MARL). While most existing methods typically employ single-agent reinforcement learning algorithms for a single humanoid robot or MARL algorithms for multi-robot system tasks, we propose a distinct paradigm: applying cooperative-heterogeneous MARL to optimize locomotion for a single humanoid robot. The proposed method, multi-agent reinforcement learning for single humanoid locomotion (MASH), treats each limb (legs and arms) as an independent agent that explores the robot's action space while sharing a global critic for cooperative learning. Experiments demonstrate that MASH accelerates training convergence and improves whole-body cooperation ability, outperforming conventional single-agent reinforcement learning methods. This work advances the integration of MARL into single-humanoid-robot control, offering new insights into efficient locomotion strategies.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MASHï¼ˆMulti-Agent Reinforcement Learning for Single Humanoid Locomotionï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨æå‡å•ä¸€äººå½¢æœºå™¨äººè¿åŠ¨èƒ½åŠ›çš„åˆä½œå¼‚æ„å¤šæ™ºèƒ½ä½“æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆCooperative-Heterogeneous MARLï¼‰æ–¹æ³•ã€‚ä¸å°†å•ä¸€äººå½¢æœºå™¨äººè§†ä¸ºå•æ™ºèƒ½ä½“çš„ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒMASHå°†æœºå™¨äººçš„å››è‚¢ï¼ˆè…¿éƒ¨å’Œæ‰‹è‡‚ï¼‰åˆ†åˆ«è§†ä¸ºç‹¬ç«‹çš„æ™ºèƒ½ä½“ï¼Œé€šè¿‡å…±äº«å…¨å±€è¯„è®ºå‘˜ï¼ˆGlobal Criticï¼‰æ¥å®ç°è‚¢ä½“é—´çš„åä½œå­¦ä¹ ã€‚è¿™ç§æ–°é¢–çš„èŒƒå¼å…è®¸å„è‚¢ä½“åœ¨ç‹¬ç«‹çš„åŠ¨ä½œç©ºé—´å†…è¿›è¡Œæ¢ç´¢ï¼ŒåŒæ—¶ä¿æŒæ•´ä½“çš„åè°ƒæ€§ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒMASHåœ¨è®­ç»ƒæ”¶æ•›é€Ÿåº¦å’Œå…¨èº«åä½œèƒ½åŠ›æ–¹é¢å‡ä¼˜äºä¼ ç»Ÿçš„å•æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆSingle-Agent Reinforcement Learningï¼‰æ–¹æ³•ã€‚è¯¥ç ”ç©¶ä¸ä»…æ¨åŠ¨äº†å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ åœ¨å•ä½“æœºå™¨äººæ§åˆ¶ä¸­çš„åº”ç”¨ï¼Œä¹Ÿä¸ºå®ç°é«˜æ•ˆçš„äººå½¢æœºå™¨äººè¿åŠ¨ç­–ç•¥æä¾›äº†æ–°çš„æ€è·¯ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10423v1",
      "published_date": "2025-08-14 07:54:31 UTC",
      "updated_date": "2025-08-14 07:54:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:54:27.291343+00:00"
    },
    {
      "arxiv_id": "2508.10419v3",
      "title": "ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long Narrative Reasoning",
      "title_zh": "ComoRAGï¼šé¢å‘æœ‰çŠ¶æ€é•¿å™äº‹æ¨ç†çš„è®¤çŸ¥å¯å‘å¼è®°å¿†ç»„ç»‡å‹ RAG",
      "authors": [
        "Juyuan Wang",
        "Rongchen Zhao",
        "Wei Wei",
        "Yufeng Wang",
        "Mo Yu",
        "Jie Zhou",
        "Jin Xu",
        "Liyan Xu"
      ],
      "abstract": "Narrative comprehension on long stories and novels has been a challenging domain attributed to their intricate plotlines and entangled, often evolving relations among characters and entities. Given the LLM's diminished reasoning over extended context and its high computational cost, retrieval-based approaches remain a pivotal role in practice. However, traditional RAG methods could fall short due to their stateless, single-step retrieval process, which often overlooks the dynamic nature of capturing interconnected relations within long-range context. In this work, we propose ComoRAG, holding the principle that narrative reasoning is not a one-shot process, but a dynamic, evolving interplay between new evidence acquisition and past knowledge consolidation, analogous to human cognition on reasoning with memory-related signals in the brain. Specifically, when encountering a reasoning impasse, ComoRAG undergoes iterative reasoning cycles while interacting with a dynamic memory workspace. In each cycle, it generates probing queries to devise new exploratory paths, then integrates the retrieved evidence of new aspects into a global memory pool, thereby supporting the emergence of a coherent context for the query resolution. Across four challenging long-context narrative benchmarks (200K+ tokens), ComoRAG outperforms strong RAG baselines with consistent relative gains up to 11% compared to the strongest baseline. Further analysis reveals that ComoRAG is particularly advantageous for complex queries requiring global context comprehension, offering a principled, cognitively motivated paradigm towards retrieval-based stateful reasoning. Our framework is made publicly available at https://github.com/EternityJune25/ComoRAG.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é•¿ç¯‡å™äº‹ç†è§£ä¸­å¤æ‚çš„æƒ…èŠ‚å’Œä¸æ–­æ¼”å˜çš„å®ä½“å…³ç³»ï¼Œæå‡ºäº† ComoRAGï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿ RAG æ–¹æ³•åœ¨é•¿ä¸Šä¸‹æ–‡æ¨ç†ä¸­å› æ— çŠ¶æ€ã€å•æ­¥æ£€ç´¢è€Œæ— æ³•æ•æ‰åŠ¨æ€å…³è”çš„å±€é™æ€§ã€‚ComoRAG å€Ÿé‰´äººç±»è®¤çŸ¥çš„è®°å¿†å·©å›ºæœºåˆ¶ï¼Œå°†å™äº‹æ¨ç†è§†ä¸ºæ–°è¯æ®è·å–ä¸æ—§çŸ¥è¯†æ•´åˆä¹‹é—´çš„åŠ¨æ€è¿›åŒ–è¿‡ç¨‹ã€‚åœ¨é‡åˆ°æ¨ç†å›°éš¾æ—¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡ä¸åŠ¨æ€ Memory Workspace äº¤äº’è¿›è¡Œè¿­ä»£æ¨ç†å¾ªç¯ï¼Œç”Ÿæˆæ¢æµ‹æŸ¥è¯¢ä»¥æ¢ç´¢æ–°è·¯å¾„ï¼Œå¹¶å°†å¤šç»´åº¦çš„è¯æ®æ•´åˆè‡³å…¨å±€ Memory Pool ä¸­ï¼Œä»è€Œæ„å»ºè¿è´¯çš„ä¸Šä¸‹æ–‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å››ä¸ªåŒ…å«è¶…è¿‡ 200K tokens çš„é•¿ä¸Šä¸‹æ–‡å™äº‹åŸºå‡†æµ‹è¯•ä¸­ï¼ŒComoRAG çš„è¡¨ç°ä¼˜äºå¤šç§å¼ºåŠ› RAG åŸºçº¿ï¼Œæœ€é«˜å®ç° 11% çš„æ€§èƒ½æå‡ã€‚è¯¥ç ”ç©¶è¯æ˜ ComoRAG ç‰¹åˆ«é€‚ç”¨äºéœ€è¦å…¨å±€ä¸Šä¸‹æ–‡ç†è§£çš„å¤æ‚æŸ¥è¯¢ï¼Œä¸ºåŸºäºæ£€ç´¢çš„æœ‰çŠ¶æ€æ¨ç†ï¼ˆStateful Reasoningï¼‰æä¾›äº†ä¸€ç§å—è®¤çŸ¥å¯å‘çš„æœ‰æ•ˆèŒƒå¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2508.10419v3",
      "published_date": "2025-08-14 07:52:09 UTC",
      "updated_date": "2025-11-12 14:55:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:54:42.102596+00:00"
    },
    {
      "arxiv_id": "2508.10416v1",
      "title": "CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model",
      "title_zh": "CorrectNavï¼šåˆ©ç”¨è‡ªçº é”™é£è½®èµ‹èƒ½è§†è§‰-è¯­è¨€-åŠ¨ä½œå¯¼èˆªæ¨¡å‹",
      "authors": [
        "Zhuoyuan Yu",
        "Yuxing Long",
        "Zihan Yang",
        "Chengyan Zeng",
        "Hongwei Fan",
        "Jiyao Zhang",
        "Hao Dong"
      ],
      "abstract": "Existing vision-and-language navigation models often deviate from the correct trajectory when executing instructions. However, these models lack effective error correction capability, hindering their recovery from errors. To address this challenge, we propose Self-correction Flywheel, a novel post-training paradigm. Instead of considering the model's error trajectories on the training set as a drawback, our paradigm emphasizes their significance as a valuable data source. We have developed a method to identify deviations in these error trajectories and devised innovative techniques to automatically generate self-correction data for perception and action. These self-correction data serve as fuel to power the model's continued training. The brilliance of our paradigm is revealed when we re-evaluate the model on the training set, uncovering new error trajectories. At this time, the self-correction flywheel begins to spin. Through multiple flywheel iterations, we progressively enhance our monocular RGB-based VLA navigation model CorrectNav. Experiments on R2R-CE and RxR-CE benchmarks show CorrectNav achieves new state-of-the-art success rates of 65.1% and 69.3%, surpassing prior best VLA navigation models by 8.2% and 16.4%. Real robot tests in various indoor and outdoor environments demonstrate \\method's superior capability of error correction, dynamic obstacle avoidance, and long instruction following.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰çš„è§†è§‰-è¯­è¨€-å¯¼èˆª(Vision-and-Language Navigation)æ¨¡å‹åœ¨æ‰§è¡ŒæŒ‡ä»¤æ—¶å®¹æ˜“åç¦»æ­£ç¡®è½¨è¿¹ä¸”ç¼ºä¹çº é”™èƒ½åŠ›çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºSelf-correction Flywheelçš„ä¸€ç§æ–°å‹åè®­ç»ƒèŒƒå¼ã€‚è¯¥èŒƒå¼å°†æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šçš„é”™è¯¯è½¨è¿¹è§†ä¸ºå®è´µçš„æ•°æ®èµ„æºï¼Œé€šè¿‡è¯†åˆ«åå·®å¹¶è‡ªåŠ¨ç”Ÿæˆé’ˆå¯¹æ„ŸçŸ¥(Perception)ä¸åŠ¨ä½œ(Action)çš„è‡ªæˆ‘çº é”™æ•°æ®ï¼Œä»¥æ­¤é©±åŠ¨æ¨¡å‹çš„æŒç»­è®­ç»ƒã€‚é€šè¿‡å¤šæ¬¡Flywheelè¿­ä»£ï¼Œç ”ç©¶å›¢é˜Ÿé€æ­¥å¢å¼ºäº†åŸºäºå•ç›®RGBçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œ(Vision-Language-Action)å¯¼èˆªæ¨¡å‹CorrectNavã€‚å®éªŒè¡¨æ˜ï¼ŒCorrectNavåœ¨R2R-CEå’ŒRxR-CEåŸºå‡†æµ‹è¯•ä¸­åˆ†åˆ«å®ç°äº†65.1%å’Œ69.3%çš„æˆåŠŸç‡ï¼Œåˆ·æ–°äº†SOTAè®°å½•ï¼Œè¾ƒæ­¤å‰æœ€ä½³æ¨¡å‹åˆ†åˆ«æå‡äº†8.2%å’Œ16.4%ã€‚çœŸå®æœºå™¨äººæµ‹è¯•è¿›ä¸€æ­¥éªŒè¯äº†è¯¥æ¨¡å‹åœ¨é”™è¯¯çº æ­£ã€åŠ¨æ€é¿éšœ(Dynamic Obstacle Avoidance)ä»¥åŠé•¿æŒ‡ä»¤éµå¾ª(Long Instruction Following)æ–¹é¢çš„å“è¶Šæ€§èƒ½ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10416v1",
      "published_date": "2025-08-14 07:39:26 UTC",
      "updated_date": "2025-08-14 07:39:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:54:40.092324+00:00"
    },
    {
      "arxiv_id": "2508.10414v1",
      "title": "MCP2OSC: Parametric Control by Natural Language",
      "title_zh": "MCP2OSCï¼šåŸºäºè‡ªç„¶è¯­è¨€çš„å‚æ•°åŒ–æ§åˆ¶",
      "authors": [
        "Yuan-Yi Fan"
      ],
      "abstract": "Text prompts enable intuitive content creation but may fall short in achieving high precision for intricate tasks; knob or slider controls offer precise adjustments at the cost of increased complexity. To address the gap between knobs and prompts, a new MCP (Model Context Protocol) server and a unique set of prompt design criteria are presented to enable exploring parametric OSC (OpenSoundControl) control by natural language prompts. Demonstrated by 14 practical QA examples with best practices and the generalized prompt templates, this study finds Claude integrated with the MCP2OSC server effective in generating OSC messages by natural language, interpreting, searching, and visualizing OSC messages, validating and debugging OSC messages, and managing OSC address patterns. MCP2OSC enhances human-machine collaboration by leveraging LLM (Large Language Model) to handle intricate OSC development tasks, and by empowering human creativity with an intuitive language interface featuring flexible precision controls: a prompt-based OSC tool. This study provides a novel perspective on the creative MCP application at the network protocol level by utilizing LLM's strength in directly processing and generating human-readable OSC messages. The results suggest its potential for a LLM-based universal control mechanism for multimedia devices.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MCP2OSCï¼Œä¸€ä¸ªæ–°å‹çš„Model Context Protocol (MCP)æœåŠ¡å™¨ï¼Œæ—¨åœ¨å¼¥è¡¥è‡ªç„¶è¯­è¨€æç¤ºè¯(Prompts)åœ¨ç²¾ç»†ä»»åŠ¡ä¸­ç²¾åº¦ä¸è¶³ä¸ç‰©ç†æ§åˆ¶æ—‹é’®æ“ä½œå¤æ‚ä¹‹é—´çš„é¸¿æ²Ÿã€‚è¯¥ç³»ç»Ÿé€šè¿‡ä¸€å¥—ç‹¬ç‰¹çš„æç¤ºè¯è®¾è®¡å‡†åˆ™ï¼Œå®ç°äº†åˆ©ç”¨è‡ªç„¶è¯­è¨€å¯¹OpenSoundControl (OSC)åè®®è¿›è¡Œé«˜æ•ˆçš„å‚æ•°åŒ–æ§åˆ¶ã€‚å®éªŒé€šè¿‡14ä¸ªå®é™…æ¡ˆä¾‹è¯æ˜ï¼Œé›†æˆMCP2OSCçš„Claudeæ¨¡å‹èƒ½å¤Ÿç²¾å‡†å®ŒæˆOSCæ¶ˆæ¯çš„ç”Ÿæˆã€è§£æã€å¯è§†åŒ–åŠè°ƒè¯•ï¼Œå¹¶èƒ½æœ‰æ•ˆç®¡ç†åœ°å€æ¨¡å¼ã€‚MCP2OSCå……åˆ†å‘æŒ¥äº†Large Language Model (LLM)åœ¨å¤„ç†äººç±»å¯è¯»ç½‘ç»œåè®®æ–¹é¢çš„ä¼˜åŠ¿ï¼Œä¸ºå¤šåª’ä½“äº¤äº’æä¾›äº†ç›´è§‚ä¸”å…·å¤‡çµæ´»ç²¾åº¦çš„æ§åˆ¶ç•Œé¢ã€‚è¿™é¡¹å·¥ä½œä¸ºç½‘ç»œåè®®å±‚é¢çš„åˆ›é€ æ€§MCPåº”ç”¨æä¾›äº†æ–°è§†è§’ï¼Œå±•ç°äº†æ„å»ºåŸºäºLLMçš„å¤šåª’ä½“è®¾å¤‡é€šç”¨æ§åˆ¶æœºåˆ¶çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10414v1",
      "published_date": "2025-08-14 07:38:01 UTC",
      "updated_date": "2025-08-14 07:38:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:54:42.865665+00:00"
    },
    {
      "arxiv_id": "2508.11711v2",
      "title": "Enhancing GraphQL Security by Detecting Malicious Queries Using Large Language Models, Sentence Transformers, and Convolutional Neural Networks",
      "title_zh": "åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ã€å¥å­è½¬æ¢å™¨å’Œå·ç§¯ç¥ç»ç½‘ç»œæ£€æµ‹æ¶æ„æŸ¥è¯¢ä»¥å¢å¼º GraphQL å®‰å…¨æ€§",
      "authors": [
        "Irash Perera",
        "Hiranya Abeyrathne",
        "Sanjeewa Malalgoda",
        "Arshardh Ifthikar"
      ],
      "abstract": "GraphQL's flexibility, while beneficial for efficient data fetching, introduces unique security vulnerabilities that traditional API security mechanisms often fail to address. Malicious GraphQL queries can exploit the language's dynamic nature, leading to denial-of-service attacks, data exfiltration through injection, and other exploits. Existing solutions, such as static analysis, rate limiting, and general-purpose Web Application Firewalls, offer limited protection against sophisticated, context-aware attacks. This paper presents a novel, AI-driven approach for real-time detection of malicious GraphQL queries. Our method combines static analysis with machine learning techniques, including Large Language Models (LLMs) for dynamic schema-based configuration, Sentence Transformers (SBERT and Doc2Vec) for contextual embedding of query payloads, and Convolutional Neural Networks (CNNs), Random Forests, and Multilayer Perceptrons for classification. We detail the system architecture, implementation strategies optimized for production environments (including ONNX Runtime optimization and parallel processing), and evaluate the performance of our detection models and the overall system under load. Results demonstrate high accuracy in detecting various threats, including SQL injection, OS command injection, and XSS exploits, alongside effective mitigation of DoS and SSRF attempts. This research contributes a robust and adaptable solution for enhancing GraphQL API security.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ GraphQL çµæ´»æ€§å¸¦æ¥çš„å®‰å…¨æ¼æ´ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„äººå·¥æ™ºèƒ½é©±åŠ¨å®æ—¶æ£€æµ‹æ–¹æ³•ã€‚è¯¥æ–¹æ³•å°†é™æ€åˆ†æä¸æœºå™¨å­¦ä¹ æŠ€æœ¯ç›¸ç»“åˆï¼Œåˆ©ç”¨ Large Language Models (LLMs) è¿›è¡ŒåŸºäºæ¨¡å¼çš„åŠ¨æ€é…ç½®ï¼Œå¹¶é‡‡ç”¨ Sentence Transformers (åŒ…æ‹¬ SBERT å’Œ Doc2Vec) å¯¹æŸ¥è¯¢è½½è·è¿›è¡Œä¸Šä¸‹æ–‡åµŒå…¥ã€‚åœ¨åˆ†ç±»é˜¶æ®µï¼Œç³»ç»Ÿé›†æˆäº† Convolutional Neural Networks (CNNs)ã€Random Forests å’Œ Multilayer Perceptrons ä»¥æé«˜è¯†åˆ«ç²¾åº¦ã€‚ä¸ºäº†ç¡®ä¿ç”Ÿäº§ç¯å¢ƒçš„æ€§èƒ½ï¼Œç ”ç©¶è¿˜å®æ–½äº† ONNX Runtime ä¼˜åŒ–å’Œå¹¶è¡Œå¤„ç†ç­–ç•¥ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ¡ˆåœ¨æ£€æµ‹ SQL injectionã€OS command injection å’Œ XSS æ¼æ´æ–¹é¢è¡¨ç°å‡ºæé«˜çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶èƒ½æœ‰æ•ˆé˜²å¾¡ DoS å’Œ SSRF æ”»å‡»ã€‚è¿™é¡¹å·¥ä½œä¸ºæå‡ GraphQL API çš„å®‰å…¨æ€§æä¾›äº†ä¸€ä¸ªé²æ£’ä¸”å¯æ‰©å±•çš„ AI é©±åŠ¨æ¡†æ¶ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11711v2",
      "published_date": "2025-08-14 07:35:11 UTC",
      "updated_date": "2025-10-08 06:22:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:54:58.948510+00:00"
    },
    {
      "arxiv_id": "2508.10409v2",
      "title": "AnalogSeeker: An Open-source Foundation Language Model for Analog Circuit Design",
      "title_zh": "AnalogSeekerï¼šé¢å‘æ¨¡æ‹Ÿç”µè·¯è®¾è®¡çš„å¼€æºåŸºç¡€è¯­è¨€æ¨¡å‹",
      "authors": [
        "Zihao Chen",
        "Ji Zhuang",
        "Jinyi Shen",
        "Xiaoyue Ke",
        "Xinyi Yang",
        "Mingjie Zhou",
        "Zhuoyao Du",
        "Xu Yan",
        "Zhouyang Wu",
        "Zhenyu Xu",
        "Jiangli Huang",
        "Li Shang",
        "Xuan Zeng",
        "Fan Yang"
      ],
      "abstract": "In this paper, we propose AnalogSeeker, an effort toward an open-source foundation language model for analog circuit design, with the aim of integrating domain knowledge and giving design assistance. To overcome the scarcity of data in this field, we employ a corpus collection strategy based on the domain knowledge framework of analog circuits. High-quality, accessible textbooks across relevant subfields are systematically curated and cleaned into a textual domain corpus. To address the complexity of knowledge of analog circuits, we introduce a granular domain knowledge distillation method. Raw, unlabeled domain corpus is decomposed into typical, granular learning nodes, where a multi-agent framework distills implicit knowledge embedded in unstructured text into question-answer data pairs with detailed reasoning processes, yielding a fine-grained, learnable dataset for fine-tuning. To address the unexplored challenges in training analog circuit foundation models, we explore and share our training methods through both theoretical analysis and experimental validation. We finally establish a fine-tuning-centric training paradigm, customizing and implementing a neighborhood self-constrained supervised fine-tuning algorithm. This approach enhances training outcomes by constraining the perturbation magnitude between the model's output distributions before and after training. In practice, we train the Qwen2.5-32B-Instruct model to obtain AnalogSeeker, which achieves 85.04% accuracy on AMSBench-TQA, the analog circuit knowledge evaluation benchmark, with a 15.67% point improvement over the original model and is competitive with mainstream commercial models. Furthermore, AnalogSeeker also shows effectiveness in the downstream operational amplifier design task. AnalogSeeker is open-sourced at https://huggingface.co/analogllm/analogseeker for research use.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†AnalogSeekerï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ä¸ºæ¨¡æ‹Ÿç”µè·¯è®¾è®¡(Analog Circuit Design)æä¾›é¢†åŸŸçŸ¥è¯†é›†æˆå’Œè®¾è®¡è¾…åŠ©çš„å¼€æºåŸºç¡€è¯­è¨€æ¨¡å‹ã€‚é’ˆå¯¹è¯¥é¢†åŸŸæ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œç ”ç©¶å›¢é˜ŸåŸºäºæ¨¡æ‹Ÿç”µè·¯é¢†åŸŸçŸ¥è¯†æ¡†æ¶ï¼Œé€šè¿‡ç³»ç»Ÿç­–åˆ’å’Œæ¸…æ´—ç›¸å…³å­é¢†åŸŸçš„æ•™ç§‘ä¹¦ï¼Œæ„å»ºäº†é«˜è´¨é‡çš„æ–‡æœ¬è¯­æ–™åº“ã€‚ä¸ºäº†å¤„ç†å¤æ‚çš„é¢†åŸŸçŸ¥è¯†ï¼Œè¯¥ç ”ç©¶å¼•å…¥äº†ä¸€ç§ç»†ç²’åº¦çš„é¢†åŸŸçŸ¥è¯†è’¸é¦(Granular Domain Knowledge Distillation)æ–¹æ³•ï¼Œåˆ©ç”¨å¤šæ™ºèƒ½ä½“æ¡†æ¶å°†éç»“æ„åŒ–æ–‡æœ¬è½¬åŒ–ä¸ºå¸¦æœ‰è¯¦ç»†æ¨ç†è¿‡ç¨‹çš„é—®ç­”å¯¹ã€‚åœ¨è®­ç»ƒæ–¹é¢ï¼Œç ”ç©¶è€…ç¡®ç«‹äº†ä»¥å¾®è°ƒä¸ºä¸­å¿ƒçš„è®­ç»ƒèŒƒå¼ï¼Œå¹¶å¼€å‘äº†é‚»åŸŸè‡ªçº¦æŸç›‘ç£å¾®è°ƒ(Neighborhood Self-constrained Supervised Fine-tuning)ç®—æ³•ï¼Œé€šè¿‡çº¦æŸè®­ç»ƒå‰åæ¨¡å‹è¾“å‡ºåˆ†å¸ƒçš„æ‰°åŠ¨å¹…åº¦æ¥ä¼˜åŒ–å¾®è°ƒæ•ˆæœã€‚é€šè¿‡å¯¹Qwen2.5-32B-Instructè¿›è¡Œå¾®è°ƒï¼ŒAnalogSeekeråœ¨æ¨¡æ‹Ÿç”µè·¯çŸ¥è¯†è¯„ä¼°åŸºå‡†AMSBench-TQAä¸Šè¾¾åˆ°äº†85.04%çš„å‡†ç¡®ç‡ï¼Œè¾ƒåŸå§‹æ¨¡å‹æå‡äº†15.67%ã€‚è¯¥æ¨¡å‹åœ¨æ€§èƒ½ä¸Šå¯ä¸ä¸»æµå•†ä¸šæ¨¡å‹ç«äº‰ï¼Œå¹¶åœ¨ä¸‹æ¸¸çš„è¿ç®—æ”¾å¤§å™¨(Operational Amplifier)è®¾è®¡ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10409v2",
      "published_date": "2025-08-14 07:32:07 UTC",
      "updated_date": "2025-11-05 12:36:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:55:16.152805+00:00"
    },
    {
      "arxiv_id": "2508.10404v1",
      "title": "Layer-Wise Perturbations via Sparse Autoencoders for Adversarial Text Generation",
      "title_zh": "åŸºäºç¨€ç–è‡ªç¼–ç å™¨çš„å¯¹æŠ—æ€§æ–‡æœ¬ç”Ÿæˆåˆ†å±‚æ‰°åŠ¨",
      "authors": [
        "Huizhen Shu",
        "Xuying Li",
        "Qirui Wang",
        "Yuji Kosuga",
        "Mengqiu Tian",
        "Zhuo Li"
      ],
      "abstract": "With the rapid proliferation of Natural Language Processing (NLP), especially Large Language Models (LLMs), generating adversarial examples to jailbreak LLMs remains a key challenge for understanding model vulnerabilities and improving robustness. In this context, we propose a new black-box attack method that leverages the interpretability of large models. We introduce the Sparse Feature Perturbation Framework (SFPF), a novel approach for adversarial text generation that utilizes sparse autoencoders to identify and manipulate critical features in text. After using the SAE model to reconstruct hidden layer representations, we perform feature clustering on the successfully attacked texts to identify features with higher activations. These highly activated features are then perturbed to generate new adversarial texts. This selective perturbation preserves the malicious intent while amplifying safety signals, thereby increasing their potential to evade existing defenses. Our method enables a new red-teaming strategy that balances adversarial effectiveness with safety alignment. Experimental results demonstrate that adversarial texts generated by SFPF can bypass state-of-the-art defense mechanisms, revealing persistent vulnerabilities in current NLP systems.However, the method's effectiveness varies across prompts and layers, and its generalizability to other architectures and larger models remains to be validated.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ç¨€ç–ç‰¹å¾æ‰°åŠ¨æ¡†æ¶ (Sparse Feature Perturbation Framework, SFPF)ï¼Œæ—¨åœ¨åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„å¯è§£é‡Šæ€§ç”Ÿæˆå¯¹æŠ—æ–‡æœ¬ä»¥å®ç°è¶Šç‹±æ”»å‡»ã€‚è¯¥æ–¹æ³•é€šè¿‡ç¨€ç–è‡ªç¼–ç å™¨ (Sparse Autoencoders, SAEs) é‡æ„éšè—å±‚è¡¨ç¤ºï¼Œå¹¶åˆ©ç”¨ç‰¹å¾èšç±»æŠ€æœ¯è¯†åˆ«æ”»å‡»æˆåŠŸæ–‡æœ¬ä¸­çš„é«˜æ¿€æ´»ç‰¹å¾ã€‚é€šè¿‡å¯¹è¿™äº›å…³é”®ç‰¹å¾è¿›è¡Œé€‰æ‹©æ€§æ‰°åŠ¨ï¼ŒSFPF èƒ½å¤Ÿåœ¨ä¿ç•™æ¶æ„æ„å›¾çš„åŒæ—¶å¢å¼ºå…¶ç»•è¿‡é˜²å¾¡çš„èƒ½åŠ›ï¼Œä¸ºçº¢é˜Ÿæµ‹è¯• (Red-teaming) æä¾›äº†ä¸€ç§å¹³è¡¡å¯¹æŠ—æœ‰æ•ˆæ€§ä¸å®‰å…¨å¯¹é½çš„æ–°ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSFPF ç”Ÿæˆçš„å¯¹æŠ—æ ·æœ¬èƒ½æœ‰æ•ˆç»•è¿‡å½“å‰æœ€å…ˆè¿›çš„é˜²å¾¡æœºåˆ¶ï¼Œæ­ç¤ºäº†ç°æœ‰è‡ªç„¶è¯­è¨€å¤„ç† (NLP) ç³»ç»Ÿä¸­æŒç»­å­˜åœ¨çš„è„†å¼±æ€§ã€‚å°½ç®¡è¯¥æ–¹æ³•åœ¨ä¸åŒæç¤ºè¯å’Œå±‚çº§ä¸Šçš„æ•ˆæœå­˜åœ¨å·®å¼‚ï¼Œä½†å…¶ä¸ºè¯„ä¼°å’Œæå‡å¤§æ¨¡å‹çš„é²æ£’æ€§æä¾›äº†é‡è¦å·¥å…·ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10404v1",
      "published_date": "2025-08-14 07:12:44 UTC",
      "updated_date": "2025-08-14 07:12:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:55:16.351867+00:00"
    },
    {
      "arxiv_id": "2508.15805v1",
      "title": "ALAS: Autonomous Learning Agent for Self-Updating Language Models",
      "title_zh": "ALASï¼šç”¨äºè¯­è¨€æ¨¡å‹è‡ªæ›´æ–°çš„è‡ªä¸»å­¦ä¹ æ™ºèƒ½ä½“",
      "authors": [
        "Dhruv Atreja"
      ],
      "abstract": "Large language models (LLMs) often have a fixed knowledge cutoff, limiting their accuracy on emerging information. We present ALAS (Autonomous Learning Agent System), a modular pipeline that continuously updates an LLM's knowledge with minimal human intervention. ALAS autonomously generates a learning curriculum for a target domain, retrieves up-to-date information from the web (with citations), distills this into question-answer training data, and fine-tunes the model through supervised fine-tuning (SFT) and direct preference optimization (DPO). It iteratively evaluates performance and revises the curriculum, enabling long-term continual learning. We demonstrate ALAS's ability to self-improve a model on rapidly evolving domains (e.g., new Python releases, latest security CVEs, academic trends), significantly boosting post-cutoff question answering accuracy (from 15% to 90% on average) without manual dataset curation. The system emphasizes modularity and reproducibility: each component (planning, retrieval, distillation, memory, fine-tuning) is interchangeable and built on standard APIs. We discuss comparative baselines (e.g., retrieval-augmented generation vs. fine-tuning) and show that ALAS achieves 90% accuracy on knowledge-updated queries with minimal engineering overhead. Finally, we outline limitations (cost, dependency on source quality) and future directions for autonomous lifelong learning in LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ALAS (Autonomous Learning Agent System)ï¼Œä¸€ç§æ—¨åœ¨å®ç°å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)çŸ¥è¯†è‡ªæˆ‘æ›´æ–°çš„è‡ªä¸»å­¦ä¹ æ™ºèƒ½ä½“ç³»ç»Ÿã€‚é’ˆå¯¹LLMsçŸ¥è¯†åˆ‡æ–­(knowledge cutoff)å¯¼è‡´çš„å‡†ç¡®æ€§é™åˆ¶ï¼ŒALASé€šè¿‡æ¨¡å—åŒ–æµæ°´çº¿è‡ªä¸»ç”Ÿæˆç‰¹å®šé¢†åŸŸçš„å­¦ä¹ è¯¾ç¨‹ï¼Œå¹¶ä»äº’è”ç½‘æ£€ç´¢æœ€æ–°ä¿¡æ¯ã€‚è¯¥ç³»ç»Ÿå°†æ£€ç´¢åˆ°çš„æ•°æ®æç‚¼ä¸ºé—®ç­”å¯¹ï¼Œéšåé€šè¿‡ç›‘ç£å¾®è°ƒ(SFT)å’Œç›´æ¥åå¥½ä¼˜åŒ–(DPO)å¯¹æ¨¡å‹è¿›è¡ŒæŒç»­å¾®è°ƒï¼Œå¹¶å…·å¤‡è¿­ä»£è¯„ä¼°ä¸è¯¾ç¨‹ä¿®è®¢çš„èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒALASåœ¨Pythonæ–°ç‰ˆæœ¬å’Œç½‘ç»œå®‰å…¨æ¼æ´(CVEs)ç­‰å¿«é€Ÿæ¼”è¿›çš„é¢†åŸŸä¸­ï¼Œå°†æ¨¡å‹åœ¨çŸ¥è¯†åˆ‡æ–­åçš„é—®ç­”å‡†ç¡®ç‡ä»15%æ˜¾è‘—æå‡è‡³90%ã€‚è¯¥ç³»ç»Ÿå±•ç°äº†æé«˜çš„æ¨¡å—åŒ–ä¸å¯å¤ç°æ€§ï¼Œä¸ºLLMså®ç°ä½å·¥ç¨‹å¼€é”€çš„è‡ªä¸»ç»ˆèº«å­¦ä¹ (autonomous lifelong learning)æä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15805v1",
      "published_date": "2025-08-14 06:55:51 UTC",
      "updated_date": "2025-08-14 06:55:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:55:24.150886+00:00"
    },
    {
      "arxiv_id": "2508.10397v1",
      "title": "PQ-DAF: Pose-driven Quality-controlled Data Augmentation for Data-scarce Driver Distraction Detection",
      "title_zh": "PQ-DAFï¼šé¢å‘æ•°æ®ç¨€ç¼ºé©¾é©¶å‘˜åˆ†å¿ƒæ£€æµ‹çš„å§¿æ€é©±åŠ¨è´¨é‡å—æ§æ•°æ®å¢å¼º",
      "authors": [
        "Haibin Sun",
        "Xinghui Song"
      ],
      "abstract": "Driver distraction detection is essential for improving traffic safety and reducing road accidents. However, existing models often suffer from degraded generalization when deployed in real-world scenarios. This limitation primarily arises from the few-shot learning challenge caused by the high cost of data annotation in practical environments, as well as the substantial domain shift between training datasets and target deployment conditions. To address these issues, we propose a Pose-driven Quality-controlled Data Augmentation Framework (PQ-DAF) that leverages a vision-language model for sample filtering to cost-effectively expand training data and enhance cross-domain robustness. Specifically, we employ a Progressive Conditional Diffusion Model (PCDMs) to accurately capture key driver pose features and synthesize diverse training examples. A sample quality assessment module, built upon the CogVLM vision-language model, is then introduced to filter out low-quality synthetic samples based on a confidence threshold, ensuring the reliability of the augmented dataset. Extensive experiments demonstrate that PQ-DAF substantially improves performance in few-shot driver distraction detection, achieving significant gains in model generalization under data-scarce conditions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é©¾é©¶å‘˜åˆ†å¿ƒæ£€æµ‹(Driver distraction detection)åœ¨å®é™…åº”ç”¨ä¸­å› æ ‡æ³¨æˆæœ¬é«˜å¯¼è‡´çš„å°æ ·æœ¬å­¦ä¹ (few-shot learning)æŒ‘æˆ˜ï¼Œä»¥åŠè®­ç»ƒé›†ä¸éƒ¨ç½²ç¯å¢ƒé—´çš„é¢†åŸŸåç§»(domain shift)é—®é¢˜ï¼Œæå‡ºäº†PQ-DAFæ¡†æ¶ã€‚PQ-DAFæ˜¯ä¸€ç§å§¿æ€é©±åŠ¨ä¸”å—è´¨é‡æ§åˆ¶çš„æ•°æ®å¢å¼ºæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œæ ·æœ¬è¿‡æ»¤ï¼Œä»¥ä½æˆæœ¬æ–¹å¼æ‰©å±•è®­ç»ƒæ•°æ®å¹¶æå‡è·¨åŸŸé²æ£’æ€§ã€‚è¯¥æ¡†æ¶æ ¸å¿ƒé‡‡ç”¨äº†æ¸è¿›å¼æ¡ä»¶æ‰©æ•£æ¨¡å‹(PCDMs)æ¥ç²¾ç¡®æ•æ‰é©¾é©¶å‘˜çš„å…³é”®å§¿æ€ç‰¹å¾ï¼Œå¹¶åˆæˆå¤šæ ·åŒ–çš„è®­ç»ƒæ ·æœ¬ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†åŸºäºCogVLMè§†è§‰è¯­è¨€æ¨¡å‹çš„è´¨é‡è¯„ä¼°æ¨¡å—ï¼Œåˆ©ç”¨ç½®ä¿¡åº¦é˜ˆå€¼è¿‡æ»¤ä½è´¨é‡åˆæˆæ ·æœ¬ï¼Œç¡®ä¿äº†å¢å¼ºæ•°æ®é›†çš„å¯é æ€§ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒPQ-DAFåœ¨æ•°æ®åŒ®ä¹çš„æƒ…å†µä¸‹æ˜¾è‘—æå‡äº†é©¾é©¶å‘˜åˆ†å¿ƒæ£€æµ‹çš„æ€§èƒ½ï¼Œå¹¶åœ¨æ¨¡å‹æ³›åŒ–èƒ½åŠ›ä¸Šå–å¾—äº†æ˜¾è‘—å¢ç›Šã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.10397v1",
      "published_date": "2025-08-14 06:54:28 UTC",
      "updated_date": "2025-08-14 06:54:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:55:27.446096+00:00"
    },
    {
      "arxiv_id": "2508.10391v4",
      "title": "LeanRAG: Knowledge-Graph-Based Generation with Semantic Aggregation and Hierarchical Retrieval",
      "title_zh": "LeanRAGï¼šèåˆè¯­ä¹‰èšåˆä¸å±‚çº§æ£€ç´¢çš„çŸ¥è¯†å›¾è°±ç”Ÿæˆæ–¹æ³•",
      "authors": [
        "Yaoze Zhang",
        "Rong Wu",
        "Pinlong Cai",
        "Xiaoman Wang",
        "Guohang Yan",
        "Song Mao",
        "Ding Wang",
        "Botian Shi"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) plays a crucial role in grounding Large Language Models by leveraging external knowledge, whereas the effectiveness is often compromised by the retrieval of contextually flawed or incomplete information. To address this, knowledge graph-based RAG methods have evolved towards hierarchical structures, organizing knowledge into multi-level summaries. However, these approaches still suffer from two critical, unaddressed challenges: high-level conceptual summaries exist as disconnected ``semantic islands'', lacking the explicit relations needed for cross-community reasoning; and the retrieval process itself remains structurally unaware, often degenerating into an inefficient flat search that fails to exploit the graph's rich topology. To overcome these limitations, we introduce LeanRAG, a framework that features a deeply collaborative design combining knowledge aggregation and retrieval strategies. LeanRAG first employs a novel semantic aggregation algorithm that forms entity clusters and constructs new explicit relations among aggregation-level summaries, creating a fully navigable semantic network. Then, a bottom-up, structure-guided retrieval strategy anchors queries to the most relevant fine-grained entities and then systematically traverses the graph's semantic pathways to gather concise yet contextually comprehensive evidence sets. The LeanRAG can mitigate the substantial overhead associated with path retrieval on graphs and minimizes redundant information retrieval. Extensive experiments on four challenging QA benchmarks with different domains demonstrate that LeanRAG significantly outperforming existing methods in response quality while reducing 46\\% retrieval redundancy. Code is available at: https://github.com/RaZzzyz/LeanRAG",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LeanRAGæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åŸºäºçŸ¥è¯†å›¾è°±çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ä¸­å­˜åœ¨çš„â€œè¯­ä¹‰å­¤å²›â€å’Œæ£€ç´¢è¿‡ç¨‹ç¼ºä¹ç»“æ„æ„ŸçŸ¥ç­‰æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ç§åˆ›æ–°çš„è¯­ä¹‰èšåˆç®—æ³•(semantic aggregation algorithm)ï¼Œé€šè¿‡å½¢æˆå®ä½“ç°‡å¹¶æ„å»ºæ‘˜è¦å±‚çº§çš„æ˜¾å¼å…³è”ï¼Œå°†åŸæœ¬å­¤ç«‹çš„çŸ¥è¯†ç‚¹è½¬åŒ–ä¸ºå¯å…¨é¢å¯¼èˆªçš„è¯­ä¹‰ç½‘ç»œã€‚ä¸ä¹‹é…å¥—çš„æ˜¯ä¸€ç§ç”±åº•å‘ä¸Šçš„ç»“æ„å¼•å¯¼æ£€ç´¢ç­–ç•¥(structure-guided retrieval strategy)ï¼Œé€šè¿‡é”šå®šç»†ç²’åº¦å®ä½“å¹¶ç³»ç»Ÿæ€§åœ°æ²¿è¯­ä¹‰è·¯å¾„éå†ï¼Œä»¥è·å–ç®€æ´ä¸”ä¸Šä¸‹æ–‡è¯¦å°½çš„è¯æ®é›†ã€‚è¿™ç§è®¾è®¡æœ‰æ•ˆç¼“è§£äº†å›¾è·¯å¾„æ£€ç´¢çš„å·¨å¤§å¼€é”€ï¼Œå¹¶æ˜¾è‘—æœ€å°åŒ–äº†å†—ä½™ä¿¡æ¯çš„æå–ã€‚åœ¨å››ä¸ªè·¨é¢†åŸŸé—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLeanRAGåœ¨å›ç­”è´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶å°†æ£€ç´¢å†—ä½™é™ä½äº†46%ï¼Œä¸ºé«˜æ•ˆä¸”ç²¾å‡†çš„çŸ¥è¯†å›¾è°±å¢å¼ºç”Ÿæˆæä¾›äº†æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by AAAI-26",
      "pdf_url": "https://arxiv.org/pdf/2508.10391v4",
      "published_date": "2025-08-14 06:47:18 UTC",
      "updated_date": "2025-11-12 06:50:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:55:31.745944+00:00"
    },
    {
      "arxiv_id": "2508.10954v1",
      "title": "Towards Efficient Prompt-based Continual Learning in Distributed Medical AI",
      "title_zh": "è¿ˆå‘åˆ†å¸ƒå¼åŒ»ç–—äººå·¥æ™ºèƒ½ä¸­çš„é«˜æ•ˆæç¤ºå¼æŒç»­å­¦ä¹ ",
      "authors": [
        "Gyutae Oh",
        "Jitae Shin"
      ],
      "abstract": "Modern AI models achieve state-of-the-art performance with large-scale, high-quality datasets; however, ethical, social, and institutional constraints in the medical domain severely restrict data sharing, rendering centralized learning nearly impossible. Each institution must incrementally update models using only local data. Traditional training overfits new samples and suffers from catastrophic forgetting, losing previously acquired knowledge. Medical data distributions also shift due to varying diagnostic equipment and demographics. Although continual learning (CL) has advanced, most methods address natural images, leaving medical-domain-specific CL underexplored. We propose a prompt-based continual learning (PCL) approach featuring a unified prompt pool with a minimal expansion strategy: by expanding and freezing a subset of prompts, our method reduces computational overhead, and a novel regularization term balances retention and adaptation. Experiments on three diabetic retinopathy datasets Aptos2019, LI2019, and Diabetic Retinopathy Detection show our model improves final classification accuracy by at least 10% and F1-score by 9 points over state-of-the-art approaches while lowering inference cost. We anticipate this study will drive sustainable medical AI advances, enabling real-time diagnosis, patient monitoring, and telemedicine applications in distributed healthcare. Code will be released upon acceptance",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»ç–—AIé¢†åŸŸå› éšç§å’Œåˆ¶åº¦é™åˆ¶å¯¼è‡´çš„æ•°æ®å…±äº«éš¾é¢˜ï¼Œæ¢è®¨äº†åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸‹å¦‚ä½•è¿›è¡Œé«˜æ•ˆçš„æŒç»­å­¦ä¹ (Continual Learning)ã€‚ä¼ ç»Ÿçš„æ¨¡å‹æ›´æ–°æ–¹æ³•åœ¨å¤„ç†åŒ»ç–—æ•°æ®æ—¶å®¹æ˜“å‡ºç°ç¾éš¾æ€§é—å¿˜(Catastrophic Forgetting)ï¼Œä¸”éš¾ä»¥åº”å¯¹ç”±è®¾å¤‡å·®å¼‚å’Œäººç¾¤ç‰¹å¾å¼•èµ·çš„åˆ†å¸ƒåç§»ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäºæç¤ºè¯çš„æŒç»­å­¦ä¹ (Prompt-based Continual Learning, PCL)æ–¹æ³•ï¼Œå…¶æ ¸å¿ƒæ˜¯ä¸€ä¸ªé‡‡ç”¨æç®€æ‰©å¼ ç­–ç•¥çš„ç»Ÿä¸€æç¤ºæ± (Prompt Pool)ã€‚è¯¥æ–¹æ³•é€šè¿‡æ‰©å±•å¹¶å†»ç»“éƒ¨åˆ†æç¤ºè¯æ¥æ˜¾è‘—é™ä½è®¡ç®—å¼€é”€ï¼Œå¹¶å¼•å…¥ä¸€ç§æ–°å‹æ­£åˆ™åŒ–é¡¹ä»¥å¹³è¡¡çŸ¥è¯†ä¿ç•™ä¸æ–°ä»»åŠ¡é€‚åº”ã€‚åœ¨Aptos2019ç­‰ä¸‰ä¸ªç³–å°¿ç—…è§†ç½‘è†œç—…å˜æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨åˆ†ç±»å‡†ç¡®ç‡ä¸Šæ¯”ç°æœ‰æœ€å…ˆè¿›æ–¹æ³•æå‡äº†è‡³å°‘10%ï¼ŒF1åˆ†æ•°æé«˜äº†9åˆ†ã€‚è¿™é¡¹ç ”ç©¶åœ¨é™ä½æ¨ç†æˆæœ¬çš„åŒæ—¶ï¼Œä¸ºåˆ†å¸ƒå¼åŒ»ç–—ä¸­çš„å®æ—¶è¯Šæ–­å’Œè¿œç¨‹åŒ»ç–—åº”ç”¨æä¾›äº†å¯æŒç»­çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "10p",
      "pdf_url": "https://arxiv.org/pdf/2508.10954v1",
      "published_date": "2025-08-14 06:46:14 UTC",
      "updated_date": "2025-08-14 06:46:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:56:26.900258+00:00"
    },
    {
      "arxiv_id": "2508.10383v2",
      "title": "Unlocking Robust Semantic Segmentation Performance via Label-only Elastic Deformations against Implicit Label Noise",
      "title_zh": "é€šè¿‡ä»…é’ˆå¯¹æ ‡ç­¾çš„å¼¹æ€§å½¢å˜åº”å¯¹éšæ€§æ ‡ç­¾å™ªå£°ï¼Œæå‡é²æ£’çš„è¯­ä¹‰åˆ†å‰²æ€§èƒ½",
      "authors": [
        "Yechan Kim",
        "Dongho Yoon",
        "Younkwan Lee",
        "Unse Fatima",
        "Hong Kook Kim",
        "Songjae Lee",
        "Sanga Park",
        "Jeong Ho Park",
        "Seonjong Kang",
        "Moongu Jeon"
      ],
      "abstract": "While previous studies on image segmentation focus on handling severe (or explicit) label noise, real-world datasets also exhibit subtle (or implicit) label imperfections. These arise from inherent challenges, such as ambiguous object boundaries and annotator variability. Although not explicitly present, such mild and latent noise can still impair model performance. Typical data augmentation methods, which apply identical transformations to the image and its label, risk amplifying these subtle imperfections and limiting the model's generalization capacity. In this paper, we introduce NSegment+, a novel augmentation framework that decouples image and label transformations to address such realistic noise for semantic segmentation. By introducing controlled elastic deformations only to segmentation labels while preserving the original images, our method encourages models to focus on learning robust representations of object structures despite minor label inconsistencies. Extensive experiments demonstrate that NSegment+ consistently improves performance, achieving mIoU gains of up to +2.29, +2.38, +1.75, and +3.39 in average on Vaihingen, LoveDA, Cityscapes, and PASCAL VOC, respectively-even without bells and whistles, highlighting the importance of addressing implicit label noise. These gains can be further amplified when combined with other training tricks, including CutMix and Label Smoothing.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¯­ä¹‰åˆ†å‰² (Semantic Segmentation) ä¸­ç”±äºç‰©ä½“è¾¹ç•Œæ¨¡ç³Šå’Œæ ‡æ³¨å˜å¼‚äº§ç”Ÿçš„éšå¼æ ‡ç­¾å™ªå£° (Implicit Label Noise) é—®é¢˜ï¼Œæå‡ºäº†åä¸º NSegment+ çš„æ–°å‹å¢å¼ºæ¡†æ¶ã€‚NSegment+ é€šè¿‡è§£è€¦å›¾åƒå’Œæ ‡ç­¾çš„å˜æ¢è¿‡ç¨‹ï¼Œä»…å¯¹åˆ†å‰²æ ‡ç­¾å¼•å…¥å—æ§çš„å¼¹æ€§å½¢å˜ (Elastic Deformations)ï¼ŒåŒæ—¶ä¿æŒåŸå§‹å›¾åƒä¸å˜ï¼Œä»è€Œä¿ƒä½¿æ¨¡å‹åœ¨æ ‡ç­¾å­˜åœ¨ç»†å¾®ä¸ä¸€è‡´æ—¶ä»èƒ½å­¦ä¹ åˆ°ç‰©ä½“ç»“æ„çš„é²æ£’è¡¨ç¤ºã€‚åœ¨ Vaihingenã€LoveDAã€Cityscapes å’Œ PASCAL VOC æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ— éœ€é¢å¤–å¤æ‚æ“ä½œçš„æƒ…å†µä¸‹ï¼Œèƒ½å°†å¹³å‡ mIoU åˆ†åˆ«æ˜¾è‘—æå‡è‡³å¤š 2.29ã€2.38ã€1.75 å’Œ 3.39ã€‚æ­¤å¤–ï¼ŒNSegment+ å±•ç°äº†è‰¯å¥½çš„å…¼å®¹æ€§ï¼Œä¸ CutMix å’Œ Label Smoothing ç­‰è®­ç»ƒæŠ€å·§ç»“åˆåèƒ½è¿›ä¸€æ­¥æ”¾å¤§æ€§èƒ½å¢ç›Šã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†å¤„ç†éšå¼æ ‡ç­¾å™ªå£°å¯¹æå‡åˆ†å‰²æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„é‡è¦æ€§ï¼Œä¸ºå®ç°æ›´å…·é²æ£’æ€§çš„è¯­ä¹‰åˆ†å‰²æä¾›äº†ç®€ä¾¿ä¸”æœ‰æ•ˆçš„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10383v2",
      "published_date": "2025-08-14 06:27:43 UTC",
      "updated_date": "2025-08-22 08:15:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:55:33.084310+00:00"
    },
    {
      "arxiv_id": "2508.10370v1",
      "title": "eMamba: Efficient Acceleration Framework for Mamba Models in Edge Computing",
      "title_zh": "eMambaï¼šé¢å‘è¾¹ç¼˜è®¡ç®—çš„ Mamba æ¨¡å‹é«˜æ•ˆåŠ é€Ÿæ¡†æ¶",
      "authors": [
        "Jiyong Kim",
        "Jaeho Lee",
        "Jiahao Lin",
        "Alish Kanani",
        "Miao Sun",
        "Umit Y. Ogras",
        "Jaehyun Park"
      ],
      "abstract": "State Space Model (SSM)-based machine learning architectures have recently gained significant attention for processing sequential data. Mamba, a recent sequence-to-sequence SSM, offers competitive accuracy with superior computational efficiency compared to state-of-the-art transformer models. While this advantage makes Mamba particularly promising for resource-constrained edge devices, no hardware acceleration frameworks are currently optimized for deploying it in such environments. This paper presents eMamba, a comprehensive end-to-end hardware acceleration framework explicitly designed for deploying Mamba models on edge platforms. eMamba maximizes computational efficiency by replacing complex normalization layers with lightweight hardware-aware alternatives and approximating expensive operations, such as SiLU activation and exponentiation, considering the target applications. Then, it performs an approximation-aware neural architecture search (NAS) to tune the learnable parameters used during approximation. Evaluations with Fashion-MNIST, CIFAR-10, and MARS, an open-source human pose estimation dataset, show eMamba achieves comparable accuracy to state-of-the-art techniques using 1.63-19.9$\\times$ fewer parameters. In addition, it generalizes well to large-scale natural language tasks, demonstrating stable perplexity across varying sequence lengths on the WikiText2 dataset. We also quantize and implement the entire eMamba pipeline on an AMD ZCU102 FPGA and ASIC using GlobalFoundries (GF) 22 nm technology. Experimental results show 4.95-5.62$\\times$ lower latency and 2.22-9.95$\\times$ higher throughput, with 4.77$\\times$ smaller area, 9.84$\\times$ lower power, and 48.6$\\times$ lower energy consumption than baseline solutions while maintaining competitive accuracy.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† eMambaï¼Œä¸€ä¸ªä¸“ä¸ºè¾¹ç¼˜è®¡ç®—å¹³å°è®¾è®¡çš„ Mamba æ¨¡å‹ç«¯åˆ°ç«¯ç¡¬ä»¶åŠ é€Ÿæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³çŠ¶æ€ç©ºé—´æ¨¡å‹ (SSM) åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹çš„éƒ¨ç½²éš¾é¢˜ã€‚eMamba é€šè¿‡å¼•å…¥è½»é‡çº§ç¡¬ä»¶æ„ŸçŸ¥æ›¿ä»£æ–¹æ¡ˆæ¥æ›¿æ¢å¤æ‚çš„å½’ä¸€åŒ–å±‚ï¼Œå¹¶é’ˆå¯¹ç›®æ ‡åº”ç”¨è¿‘ä¼¼å¤„ç† SiLU æ¿€æ´»å‡½æ•°å’ŒæŒ‡æ•°è¿ç®—ç­‰é«˜å¼€é”€æ“ä½œï¼Œæ˜¾è‘—æå‡äº†è®¡ç®—æ•ˆç‡ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨è¿‘ä¼¼æ„ŸçŸ¥ç¥ç»æ¶æ„æœç´¢ (NAS) æ¥å¾®è°ƒå¯å­¦ä¹ å‚æ•°ï¼Œç¡®ä¿äº†è¿‘ä¼¼å¤„ç†åçš„æ¨¡å‹ç²¾åº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒeMamba åœ¨ Fashion-MNISTã€CIFAR-10 åŠ WikiText2 ç­‰å¤šä¸ªæ•°æ®é›†ä¸Šä»¥æ›´å°‘çš„å‚æ•°é‡å®ç°äº†ç«äº‰æ€§å‡†ç¡®ç‡ã€‚åœ¨ FPGA å’Œ ASIC ç¡¬ä»¶å®ç°ä¸­ï¼ŒeMamba ç›¸æ¯”åŸºå‡†æ–¹æ¡ˆå±•ç°å‡ºæ›´ä½çš„å»¶è¿Ÿã€æ›´é«˜çš„ååé‡ä»¥åŠæ˜¾è‘—é™ä½çš„åŠŸè€—å’Œèƒ½è€—ï¼Œä¸º Mamba æ¨¡å‹åœ¨è¾¹ç¼˜ä¾§çš„é«˜æ•ˆéƒ¨ç½²æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Paper accepted at ESWEEK 2025 (CODES+ISSS) conference",
      "pdf_url": "https://arxiv.org/pdf/2508.10370v1",
      "published_date": "2025-08-14 06:08:05 UTC",
      "updated_date": "2025-08-14 06:08:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:55:38.890087+00:00"
    },
    {
      "arxiv_id": "2508.10358v1",
      "title": "What to Ask Next? Probing the Imaginative Reasoning of LLMs with TurtleSoup Puzzles",
      "title_zh": "ä¸‹ä¸€æ­¥è¯¥é—®ä»€ä¹ˆï¼Ÿåˆ©ç”¨æµ·é¾Ÿæ±¤è°œé¢˜æ¢ç©¶å¤§è¯­è¨€æ¨¡å‹çš„æƒ³è±¡æ€§æ¨ç†",
      "authors": [
        "Mengtao Zhou",
        "Sifan Wu",
        "Huan Zhang",
        "Qi Sima",
        "Bang Liu"
      ],
      "abstract": "We investigate the capacity of Large Language Models (LLMs) for imaginative reasoning--the proactive construction, testing, and revision of hypotheses in information-sparse environments. Existing benchmarks, often static or focused on social deduction, fail to capture the dynamic, exploratory nature of this reasoning process. To address this gap, we introduce a comprehensive research framework based on the classic \"Turtle Soup\" game, integrating a benchmark, an agent, and an evaluation protocol. We present TurtleSoup-Bench, the first large-scale, bilingual, interactive benchmark for imaginative reasoning, comprising 800 turtle soup puzzles sourced from both the Internet and expert authors. We also propose Mosaic-Agent, a novel agent designed to assess LLMs' performance in this setting. To evaluate reasoning quality, we develop a multi-dimensional protocol measuring logical consistency, detail completion, and conclusion alignment. Experiments with leading LLMs reveal clear capability limits, common failure patterns, and a significant performance gap compared to humans. Our work offers new insights into LLMs' imaginative reasoning and establishes a foundation for future research on exploratory agent behavior.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨ä¿¡æ¯ç¨€ç–ç¯å¢ƒä¸‹çš„Imaginative Reasoningèƒ½åŠ›ï¼Œå³ä¸»åŠ¨æ„å»ºã€æµ‹è¯•å’Œä¿®æ­£å‡è®¾çš„èƒ½åŠ›ã€‚ä¸ºè§£å†³ç°æœ‰åŸºå‡†æµ‹è¯•æ— æ³•æ•æ‰è¿™ç§åŠ¨æ€æ¢ç´¢æ¨ç†è¿‡ç¨‹çš„é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº†åŸºäºç»å…¸â€œæµ·é¾Ÿæ±¤â€(Turtle Soup)æ¸¸æˆçš„å…¨é¢ç ”ç©¶æ¡†æ¶ï¼Œå¹¶å‘å¸ƒäº†é¦–ä¸ªå¤§è§„æ¨¡ã€åŒè¯­ä¸”äº¤äº’å¼çš„åŸºå‡†æµ‹è¯•TurtleSoup-Benchã€‚åŒæ—¶ï¼Œç ”ç©¶æå‡ºäº†Mosaic-Agentç”¨ä»¥è¯„ä¼°æ¨¡å‹è¡¨ç°ï¼Œå¹¶å¼€å‘äº†è¡¡é‡é€»è¾‘ä¸€è‡´æ€§ä¸ç»“è®ºä¸€è‡´æ€§çš„å¤šç»´åº¦è¯„ä¼°åè®®ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå³ä¾¿æ˜¯é¢†å…ˆçš„LLMsåœ¨è¿™ä¸€é¢†åŸŸä¹Ÿå­˜åœ¨æ˜æ˜¾çš„æ€§èƒ½ç“¶é¢ˆå’Œç‰¹å®šçš„å¤±è´¥æ¨¡å¼ï¼Œä¸äººç±»è¡¨ç°ç›¸æ¯”ä»æœ‰å·¨å¤§å·®è·ã€‚è¯¥ç ”ç©¶ä¸ä»…æ­ç¤ºäº†LLMsåœ¨æ¢ç´¢æ€§æ¨ç†æ–¹é¢çš„ç°çŠ¶ï¼Œä¹Ÿä¸ºæœªæ¥æ™ºèƒ½ä½“è¡Œä¸ºçš„ç ”ç©¶å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10358v1",
      "published_date": "2025-08-14 05:55:42 UTC",
      "updated_date": "2025-08-14 05:55:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:55:41.086187+00:00"
    },
    {
      "arxiv_id": "2508.11710v1",
      "title": "Code Vulnerability Detection Across Different Programming Languages with AI Models",
      "title_zh": "åŸºäºäººå·¥æ™ºèƒ½æ¨¡å‹çš„è·¨ç¼–ç¨‹è¯­è¨€ä»£ç æ¼æ´æ£€æµ‹",
      "authors": [
        "Hael Abdulhakim Ali Humran",
        "Ferdi Sonmez"
      ],
      "abstract": "Security vulnerabilities present in a code that has been written in diverse programming languages are among the most critical yet complicated aspects of source code to detect. Static analysis tools based on rule-based patterns usually do not work well at detecting the context-dependent bugs and lead to high false positive rates. Recent developments in artificial intelligence, specifically the use of transformer-based models like CodeBERT and CodeLlama, provide light to this problem, as they show potential in finding such flaws better. This paper presents the implementations of these models on various datasets of code vulnerability, showing how off-the-shelf models can successfully produce predictive capacity in models through dynamic fine-tuning of the models on vulnerable and safe code fragments. The methodology comprises the gathering of the dataset, normalization of the language, fine-tuning of the model, and incorporation of ensemble learning and explainable AI. Experiments show that a well-trained CodeBERT can be as good as or even better than some existing static analyzers in terms of accuracy greater than 97%. Further study has indicated that although language models can achieve close-to-perfect recall, the precision can decrease. A solution to this is given by hybrid models and validation procedures, which will reduce false positives. According to the results, the AI-based solutions generalize to different programming languages and classes of vulnerability. Nevertheless, robustness, interpretability, and deployment readiness are still being developed. The results illustrate the probabilities that AI will enhance the trustworthiness in the usability and scalability of machine-learning-based detectors of vulnerabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨äººå·¥æ™ºèƒ½æ¨¡å‹åœ¨å¤šç§ç¼–ç¨‹è¯­è¨€ä¸­è¿›è¡Œä»£ç æ¼æ´æ£€æµ‹çš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿé™æ€åˆ†æå·¥å…·(Static analysis tools)åœ¨å¤„ç†ä¸Šä¸‹æ–‡ç›¸å…³æ¼æ´æ—¶è¯¯æŠ¥ç‡é«˜çš„é—®é¢˜ã€‚ç ”ç©¶é‡ç‚¹è¯„ä¼°äº†åŸºäºTransformeræ¶æ„çš„æ¨¡å‹ï¼Œå¦‚CodeBERTå’ŒCodeLlamaï¼Œé€šè¿‡å¯¹æ˜“å—æ”»å‡»å’Œå®‰å…¨çš„ä»£ç ç‰‡æ®µè¿›è¡ŒåŠ¨æ€å¾®è°ƒ(Fine-tuning)æ¥æå‡æ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›ã€‚å…¶æ–¹æ³•è®ºæ¶µç›–äº†æ•°æ®é›†æ”¶é›†ã€è¯­è¨€å½’ä¸€åŒ–ã€æ¨¡å‹å¾®è°ƒï¼Œä»¥åŠé›†æˆå­¦ä¹ (Ensemble learning)å’Œå¯è§£é‡Šäººå·¥æ™ºèƒ½(Explainable AI)çš„ç»“åˆåº”ç”¨ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»è¿‡è‰¯å¥½è®­ç»ƒçš„CodeBERTåœ¨å‡†ç¡®ç‡ä¸Šè¶…è¿‡97%ï¼Œè¡¨ç°ä¼˜äºéƒ¨åˆ†ç°æœ‰é™æ€åˆ†æå™¨ã€‚é’ˆå¯¹æ¨¡å‹è™½ç„¶å¬å›ç‡(Recall)æ¥è¿‘å®Œç¾ä½†ç²¾ç¡®ç‡(Precision)å¯èƒ½ä¸‹é™çš„é—®é¢˜ï¼Œç ”ç©¶æå‡ºäº†æ··åˆæ¨¡å‹(Hybrid models)å’ŒéªŒè¯ç¨‹åºä½œä¸ºæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚æœ€ç»ˆç»“æœè¯æ˜äº†AIé©±åŠ¨çš„æ–¹æ¡ˆåœ¨ä¸åŒç¼–ç¨‹è¯­è¨€å’Œæ¼æ´ç±»åˆ«ä¸­å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºå¢å¼ºæœºå™¨å­¦ä¹ æ¼æ´æ£€æµ‹å™¨çš„å¯ä¿¡åº¦ä¸å¯æ‰©å±•æ€§å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11710v1",
      "published_date": "2025-08-14 05:41:58 UTC",
      "updated_date": "2025-08-14 05:41:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:55:47.759772+00:00"
    },
    {
      "arxiv_id": "2508.11709v1",
      "title": "Navigating the New Landscape: A Conceptual Model for Project-Based Assessment (PBA) in the Age of GenAI",
      "title_zh": "åº”å¯¹æ–°æ ¼å±€ï¼šç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenAIï¼‰æ—¶ä»£ä¸‹é¡¹ç›®åˆ¶è¯„ä¼°ï¼ˆPBAï¼‰çš„æ¦‚å¿µæ¨¡å‹",
      "authors": [
        "Rajan Kadel",
        "Samar Shailendra",
        "Urvashi Rahul Saxena"
      ],
      "abstract": "The rapid integration of Generative Artificial Intelligence (GenAI) into higher education presents both opportunities and challenges for assessment design, particularly within Project-Based Assessment (PBA) contexts. Traditional assessment methods often emphasise the final product in the PBA, which can now be significantly influenced or created by GenAI tools, raising concerns regarding product authenticity, academic integrity, and learning validation. This paper advocates for a reimagined assessment model for Project-Based Learning (PBL) or a capstone project that prioritises process-oriented evaluation, multi-modal and multifaceted assessment design, and ethical engagement with GenAI to enable higher-order thinking. The model also emphasises the use of (GenAI-assisted) personalised feedback by a supervisor as an observance of the learning process during the project lifecycle. A use case scenario is provided to illustrate the application of the model in a capstone project setting. The paper concludes with recommendations for educators and curriculum designers to ensure that assessment practices remain robust, learner-centric, and integrity-driven in the evolving landscape of GenAI.",
      "tldr_zh": "éšç€ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (Generative Artificial Intelligence, GenAI) è¿…é€Ÿèå…¥é«˜ç­‰æ•™è‚²ï¼Œä¼ ç»Ÿçš„é¡¹ç›®å¼è¯„ä¼° (Project-Based Assessment, PBA) å› è¿‡åº¦å…³æ³¨æœ€ç»ˆæˆæœè€Œé¢ä¸´å­¦æœ¯è¯šä¿¡å’Œå­¦ä¹ éªŒè¯æ–¹é¢çš„ä¸¥å³»æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹é¡¹ç›®å¼å­¦ä¹  (Project-Based Learning, PBL) æˆ–æ¯•ä¸šè®¾è®¡ (capstone project) é‡æ–°æ„æƒ³çš„è¯„ä¼°æ¨¡å‹ï¼Œæ—¨åœ¨åº”å¯¹ GenAI å¸¦æ¥çš„æŠ€æœ¯å˜é©ã€‚è¯¥æ¨¡å‹ä¼˜å…ˆè€ƒè™‘é¢å‘è¿‡ç¨‹çš„è¯„ä»· (process-oriented evaluation) ä»¥åŠå¤šæ¨¡æ€å’Œå¤šç»´åº¦çš„è¯„ä¼°è®¾è®¡ (multi-modal and multifaceted assessment design)ï¼Œå¹¶å¼ºè°ƒä¸ GenAI è¿›è¡Œä¼¦ç†äº’åŠ¨ä»¥ä¿ƒè¿›é«˜é˜¶æ€ç»´ (higher-order thinking)ã€‚æ­¤å¤–ï¼Œæ¨¡å‹çªå‡ºäº†å¯¼å¸ˆåœ¨é¡¹ç›®ç”Ÿå‘½å‘¨æœŸä¸­åˆ©ç”¨ (GenAI-assisted) ä¸ªæ€§åŒ–åé¦ˆæ¥è§‚å¯Ÿå­¦ä¹ è¿‡ç¨‹çš„é‡è¦æ€§ã€‚ç ”ç©¶é€šè¿‡æ¯•ä¸šè®¾è®¡çš„å®é™…åº”ç”¨æ¡ˆä¾‹å±•ç¤ºäº†è¯¥æ¨¡å‹çš„å¯æ“ä½œæ€§ã€‚æœ€åï¼Œæœ¬æ–‡ä¸ºæ•™è‚²å·¥ä½œè€…å’Œè¯¾ç¨‹è®¾è®¡è€…æä¾›äº†ç›¸å…³å»ºè®®ï¼Œä»¥ç¡®ä¿è¯„ä¼°å®è·µåœ¨ GenAI æ¼”è¿›è¿‡ç¨‹ä¸­ä¿æŒç¨³å¥ã€ä»¥å­¦ä¹ è€…ä¸ºä¸­å¿ƒå¹¶ç”±è¯šä¿¡é©±åŠ¨ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11709v1",
      "published_date": "2025-08-14 05:22:37 UTC",
      "updated_date": "2025-08-14 05:22:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:56:44.285515+00:00"
    },
    {
      "arxiv_id": "2508.10345v1",
      "title": "Welfare-Centric Clustering",
      "title_zh": "ä»¥ç¦åˆ©ä¸ºä¸­å¿ƒçš„èšç±»",
      "authors": [
        "Claire Jie Zhang",
        "Seyed A. Esmaeili",
        "Jamie Morgenstern"
      ],
      "abstract": "Fair clustering has traditionally focused on ensuring equitable group representation or equalizing group-specific clustering costs. However, Dickerson et al. (2025) recently showed that these fairness notions may yield undesirable or unintuitive clustering outcomes and advocated for a welfare-centric clustering approach that models the utilities of the groups. In this work, we model group utilities based on both distances and proportional representation and formalize two optimization objectives based on welfare-centric clustering: the Rawlsian (Egalitarian) objective and the Utilitarian objective. We introduce novel algorithms for both objectives and prove theoretical guarantees for them. Empirical evaluations on multiple real-world datasets demonstrate that our methods significantly outperform existing fair clustering baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿå…¬å¹³èšç±»(Fair clustering)åœ¨ç¾¤ä½“ä»£è¡¨æ€§å’Œæˆæœ¬å‡ç­‰å®šä¹‰ä¸‹å¯èƒ½äº§ç”Ÿä¸ç›´è§‚ç»“æœçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ä»¥ç¦åˆ©ä¸ºä¸­å¿ƒ(Welfare-centric clustering)çš„èšç±»æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»¼åˆè€ƒè™‘è·ç¦»å’Œæ¯”ä¾‹ä»£è¡¨æ€§(Proportional representation)æ¥å»ºæ¨¡ç¾¤ä½“æ•ˆç”¨(Group utilities)ï¼Œå¹¶æ­£å¼å®šä¹‰äº†ç½—å°”æ–¯ä¸»ä¹‰(Rawlsian/Egalitarian)å’ŒåŠŸåˆ©ä¸»ä¹‰(Utilitarian)ä¸¤ç§ä¼˜åŒ–ç›®æ ‡ã€‚ç ”ç©¶è€…é’ˆå¯¹è¿™ä¸¤ç§ç›®æ ‡å¼€å‘äº†å…¨æ–°çš„ç®—æ³•ï¼Œå¹¶ä¸ºå…¶æä¾›äº†å®Œå¤‡çš„ç†è®ºä¿è¯(Theoretical guarantees)ã€‚åœ¨å¤šä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®è¯è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æå‡ç¾¤ä½“æ•´ä½“ç¦åˆ©æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„å…¬å¹³èšç±»åŸºå‡†æ–¹æ³•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY",
        "cs.DS"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10345v1",
      "published_date": "2025-08-14 05:02:32 UTC",
      "updated_date": "2025-08-14 05:02:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:56:45.254912+00:00"
    },
    {
      "arxiv_id": "2508.10340v1",
      "title": "Multi-Agent Trust Region Policy Optimisation: A Joint Constraint Approach",
      "title_zh": "å¤šæ™ºèƒ½ä½“ç½®ä¿¡åŒºåŸŸç­–ç•¥ä¼˜åŒ–ï¼šä¸€ç§è”åˆçº¦æŸæ–¹æ³•",
      "authors": [
        "Chak Lam Shek",
        "Guangyao Shi",
        "Pratap Tokekar"
      ],
      "abstract": "Multi-agent reinforcement learning (MARL) requires coordinated and stable policy updates among interacting agents. Heterogeneous-Agent Trust Region Policy Optimization (HATRPO) enforces per-agent trust region constraints using Kullback-Leibler (KL) divergence to stabilize training. However, assigning each agent the same KL threshold can lead to slow and locally optimal updates, especially in heterogeneous settings. To address this limitation, we propose two approaches for allocating the KL divergence threshold across agents: HATRPO-W, a Karush-Kuhn-Tucker-based (KKT-based) method that optimizes threshold assignment under global KL constraints, and HATRPO-G, a greedy algorithm that prioritizes agents based on improvement-to-divergence ratio. By connecting sequential policy optimization with constrained threshold scheduling, our approach enables more flexible and effective learning in heterogeneous-agent settings. Experimental results demonstrate that our methods significantly boost the performance of HATRPO, achieving faster convergence and higher final rewards across diverse MARL benchmarks. Specifically, HATRPO-W and HATRPO-G achieve comparable improvements in final performance, each exceeding 22.5%. Notably, HATRPO-W also demonstrates more stable learning dynamics, as reflected by its lower variance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ (MARL)ä¸­ç­–ç•¥æ›´æ–°çš„åè°ƒä¸ç¨³å®šæ€§é—®é¢˜ï¼Œå¯¹å¼‚æ„æ™ºèƒ½ä½“ä¿¡ä»»åŒºåŸŸç­–ç•¥ä¼˜åŒ–(HATRPO)è¿›è¡Œäº†æ”¹è¿›ã€‚é’ˆå¯¹ä¼ ç»ŸHATRPOä¸ºæ¯ä¸ªæ™ºèƒ½ä½“åˆ†é…å›ºå®šKullback-Leibler (KL)æ•£åº¦é˜ˆå€¼å¯¼è‡´æ›´æ–°ç¼“æ…¢æˆ–é™·å…¥å±€éƒ¨æœ€ä¼˜çš„å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸¤ç§çµæ´»çš„é˜ˆå€¼åˆ†é…æ–¹æ³•ï¼šåŸºäºKarush-Kuhn-Tucker (KKT)æ¡ä»¶çš„ä¼˜åŒ–æ–¹æ³•HATRPO-Wï¼Œä»¥åŠåŸºäºæ”¹è¿›ä¸æ•£åº¦æ¯”ç‡çš„è´ªå©ªç®—æ³•HATRPO-Gã€‚é€šè¿‡å°†é¡ºåºç­–ç•¥ä¼˜åŒ–ä¸çº¦æŸé˜ˆå€¼è°ƒåº¦ç›¸ç»“åˆï¼Œè¯¥æ–¹æ³•åœ¨å¼‚æ„æ™ºèƒ½ä½“ç¯å¢ƒä¸‹å®ç°äº†æ›´æœ‰æ•ˆçš„å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHATRPO-Wå’ŒHATRPO-Gåœ¨å¤šä¸ªMARLåŸºå‡†ä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œæœ€ç»ˆå›æŠ¥ç‡æé«˜è¶…è¿‡22.5%ï¼Œä¸”HATRPO-Wè¡¨ç°å‡ºæ›´ç¨³å®šçš„å­¦ä¹ åŠ¨æ€å’Œæ›´ä½çš„æ–¹å·®ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10340v1",
      "published_date": "2025-08-14 04:48:46 UTC",
      "updated_date": "2025-08-14 04:48:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:56:55.284564+00:00"
    },
    {
      "arxiv_id": "2510.14982v2",
      "title": "cuAPO: A CUDA-based Parallelization of Artificial Protozoa Optimizer",
      "title_zh": "cuAPOï¼šåŸºäº CUDA çš„äººå·¥åŸç”ŸåŠ¨ç‰©ä¼˜åŒ–ç®—æ³•å¹¶è¡ŒåŒ–",
      "authors": [
        "Henish Soliya",
        "Anugrah Jain"
      ],
      "abstract": "Metaheuristic algorithms are widely used for solving complex problems due to their ability to provide near-optimal solutions. But the execution time of these algorithms increases with the problem size and/or solution space. And, to get more promising results, we have to execute these algorithms for a large number of iterations, requiring a large amount of time and this is one of the main issues found with these algorithms. To handle the same, researchers are now-a-days working on design and development of parallel versions of state-of-the-art metaheuristic optimization algorithms. We, in this paper, present a CUDA-based parallelization of state-of-the-art Artificial Protozoa Optimizer leveraging GPU acceleration. We implement both the existing sequential version and the proposed parallel version of Artificial Protozoa Optimizer for a performance comparison. Our experimental results calculated over a set of CEC2022 benchmark functions demonstrate a significant performance gain i.e. up to 6.7 times speed up is achieved with proposed parallel version. We also use a real world application, i.e., Image Thresholding to compare both algorithms.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…ƒå¯å‘å¼ç®—æ³•åœ¨å¤„ç†å¤æ‚å¤§è§„æ¨¡é—®é¢˜æ—¶æ‰§è¡Œæ—¶é—´è¿‡é•¿çš„é—®é¢˜ï¼Œæå‡ºäº† cuAPOï¼Œå³ä¸€ç§åŸºäº CUDA çš„äººå·¥åŸç”ŸåŠ¨ç‰©ä¼˜åŒ–ç®—æ³• (Artificial Protozoa Optimizer) å¹¶è¡ŒåŒ–æ–¹æ¡ˆã€‚é€šè¿‡åˆ©ç”¨ GPU åŠ é€ŸæŠ€æœ¯ï¼Œè¯¥æ–¹æ¡ˆæ—¨åœ¨æ˜¾è‘—ç¼©çŸ­ä¼˜åŒ–è¿‡ç¨‹æ‰€éœ€çš„æ—¶é—´å¹¶æå‡è®¡ç®—æ•ˆç‡ã€‚ç ”ç©¶äººå‘˜åˆ†åˆ«å®ç°äº†ç°æœ‰çš„ä¸²è¡Œç‰ˆæœ¬ä¸æ‰€æçš„å¹¶è¡Œç‰ˆæœ¬ï¼Œå¹¶åœ¨ CEC2022 åŸºå‡†å‡½æ•°é›†ä¸Šè¿›è¡Œäº†è¯¦ç»†çš„æ€§èƒ½å¯¹æ¯”å®éªŒã€‚å®éªŒæ•°æ®è¯æ˜ï¼Œå¹¶è¡ŒåŒ–çš„ cuAPO ç›¸æ¯”ä¸²è¡Œç‰ˆæœ¬å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½å¢ç›Šï¼Œæœ€é«˜åŠ é€Ÿæ¯”è¾¾åˆ°äº† 6.7 å€ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å°†ç®—æ³•åº”ç”¨äºå›¾åƒé˜ˆå€¼åˆ†å‰² (Image Thresholding) è¿™ä¸€ç°å®ä¸–ç•Œä»»åŠ¡ä¸­ï¼Œé€šè¿‡å¯¹æ¯”éªŒè¯äº†å¹¶è¡ŒåŒ–æ–¹æ¡ˆåœ¨å¤„ç†å®é™…å¤æ‚è®¡ç®—é—®é¢˜æ—¶çš„å“è¶Šæ€§èƒ½ã€‚",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.14982v2",
      "published_date": "2025-08-14 04:44:29 UTC",
      "updated_date": "2025-12-14 14:05:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:56:51.059850+00:00"
    },
    {
      "arxiv_id": "2508.10337v2",
      "title": "A Curriculum Learning Approach to Reinforcement Learning: Leveraging RAG for Multimodal Question Answering",
      "title_zh": "å¼ºåŒ–å­¦ä¹ ä¸­çš„è¯¾ç¨‹å­¦ä¹ æ–¹æ³•ï¼šåˆ©ç”¨ RAG å®ç°å¤šæ¨¡æ€é—®ç­”",
      "authors": [
        "Chenliang Zhang",
        "Lin Wang",
        "Yuanyuan Lu",
        "Yusheng Qi",
        "Kexin Wang",
        "Peixu Hou",
        "Wenshi Chen"
      ],
      "abstract": "This paper describes the solutions of the Dianping-Trust-Safety team for the META CRAG-MM challenge. The challenge requires building a comprehensive retrieval-augmented generation system capable for multi-modal multi-turn question answering. The competition consists of three tasks: (1) answering questions using structured data retrieved from an image-based mock knowledge graph, (2) synthesizing information from both knowledge graphs and web search results, and (3) handling multi-turn conversations that require context understanding and information aggregation from multiple sources. For Task 1, our solution is based on the vision large language model, enhanced by supervised fine-tuning with knowledge distilled from GPT-4.1. We further applied curriculum learning strategies to guide reinforcement learning, resulting in improved answer accuracy and reduced hallucination. For Task 2 and Task 3, we additionally leveraged web search APIs to incorporate external knowledge, enabling the system to better handle complex queries and multi-turn conversations. Our approach achieved 1st place in Task 1 with a significant lead of 52.38%, and 3rd place in Task 3, demonstrating the effectiveness of the integration of curriculum learning with reinforcement learning in our training pipeline.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† Dianping-Trust-Safety å›¢é˜Ÿé’ˆå¯¹ META CRAG-MM æŒ‘æˆ˜èµ›å¼€å‘çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) ç³»ç»Ÿæ–¹æ¡ˆï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤šè½®é—®ç­”ä¸­çš„å¤æ‚ä¿¡æ¯èšåˆé—®é¢˜ã€‚å¯¹äºåŸºäºå›¾åƒçŸ¥è¯†å›¾è°±çš„ä»»åŠ¡ï¼Œå›¢é˜Ÿé‡‡ç”¨äº†è§†è§‰å¤§è¯­è¨€æ¨¡å‹ (VLLM)ï¼Œå¹¶ç»“åˆä» GPT-4.1 è’¸é¦çš„çŸ¥è¯†è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒ (SFT)ã€‚ç ”ç©¶çš„æ ¸å¿ƒåœ¨äºå¼•å…¥è¯¾ç¨‹å­¦ä¹  (Curriculum Learning) ç­–ç•¥æ¥æŒ‡å¯¼å¼ºåŒ–å­¦ä¹  (Reinforcement Learning)ï¼Œæ˜¾è‘—æå‡äº†å›ç­”çš„å‡†ç¡®æ€§å¹¶å‡å°‘äº†å¹»è§‰ç°è±¡ã€‚é’ˆå¯¹éœ€è¦å¤–éƒ¨ä¿¡æ¯çš„ä»»åŠ¡ï¼Œç³»ç»Ÿè¿›ä¸€æ­¥æ•´åˆäº† Web æœç´¢æ¥å£ä»¥å¢å¼ºä¸Šä¸‹æ–‡ç†è§£å’Œå¤šæºæ•°æ®åˆæˆèƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ Task 1 ä¸­ä»¥ 52.38% çš„é¢†å…ˆä¼˜åŠ¿è·å¾—å† å†›ï¼Œå¹¶åœ¨ Task 3 ä¸­ä½åˆ—ç¬¬ä¸‰ï¼Œå……åˆ†è¯æ˜äº†å°†è¯¾ç¨‹å­¦ä¹ ä¸å¼ºåŒ–å­¦ä¹ é›†æˆåˆ°è®­ç»ƒæµæ°´çº¿ä¸­åœ¨æå‡æ¨¡å‹æ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10337v2",
      "published_date": "2025-08-14 04:37:56 UTC",
      "updated_date": "2026-01-14 13:50:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:56:53.751457+00:00"
    },
    {
      "arxiv_id": "2508.10332v1",
      "title": "Layer-Wise Analysis of Self-Supervised Representations for Age and Gender Classification in Children's Speech",
      "title_zh": "å„¿ç«¥è¯­éŸ³å¹´é¾„ä¸æ€§åˆ«åˆ†ç±»ä¸­çš„è‡ªç›‘ç£è¡¨å¾é€å±‚åˆ†æ",
      "authors": [
        "Abhijit Sinha",
        "Harishankar Kumar",
        "Mohit Joshi",
        "Hemant Kumar Kathania",
        "Shrikanth Narayanan",
        "Sudarsana Reddy Kadiri"
      ],
      "abstract": "Children's speech presents challenges for age and gender classification due to high variability in pitch, articulation, and developmental traits. While self-supervised learning (SSL) models perform well on adult speech tasks, their ability to encode speaker traits in children remains underexplored. This paper presents a detailed layer-wise analysis of four Wav2Vec2 variants using the PFSTAR and CMU Kids datasets. Results show that early layers (1-7) capture speaker-specific cues more effectively than deeper layers, which increasingly focus on linguistic information. Applying PCA further improves classification, reducing redundancy and highlighting the most informative components. The Wav2Vec2-large-lv60 model achieves 97.14% (age) and 98.20% (gender) on CMU Kids; base-100h and large-lv60 models reach 86.05% and 95.00% on PFSTAR. These results reveal how speaker traits are structured across SSL model depth and support more targeted, adaptive strategies for child-aware speech interfaces.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å„¿ç«¥è¯­éŸ³ä¸­éŸ³é«˜å’Œå‘éŸ³çš„é«˜å˜å¼‚æ€§ï¼Œå¯¹å››ç§ Wav2Vec2 å˜ä½“åœ¨ PFSTAR å’Œ CMU Kids æ•°æ®é›†ä¸Šçš„ Age and Gender Classification ä»»åŠ¡è¿›è¡Œäº†è¯¦ç»†çš„ Layer-wise Analysisã€‚ç ”ç©¶å‘ç°ï¼ŒSelf-Supervised Learning (SSL) æ¨¡å‹çš„æµ…å±‚ï¼ˆ1-7å±‚ï¼‰èƒ½æ¯”æ·±å±‚æ›´æœ‰æ•ˆåœ°æ•æ‰ Speaker-specific cuesï¼Œè€Œæ·±å±‚åˆ™æ›´å€¾å‘äºç¼–ç  Linguistic informationã€‚é€šè¿‡å¼•å…¥ PCA é™ç»´æŠ€æœ¯ï¼Œç ”ç©¶è¿›ä¸€æ­¥å‡å°‘äº†ç‰¹å¾å†—ä½™å¹¶æ˜¾è‘—æå‡äº†åˆ†ç±»æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒWav2Vec2-large-lv60 æ¨¡å‹åœ¨ CMU Kids æ•°æ®é›†ä¸Šåˆ†åˆ«å®ç°äº† 97.14% çš„å¹´é¾„åˆ†ç±»å‡†ç¡®ç‡å’Œ 98.20% çš„æ€§åˆ«åˆ†ç±»å‡†ç¡®ç‡ã€‚è¿™äº›å‘ç°æ­ç¤ºäº†å„¿ç«¥è¯­éŸ³ç‰¹å¾åœ¨ SSL æ¨¡å‹æ·±åº¦ç»´åº¦ä¸Šçš„ç»“æ„åŒ–åˆ†å¸ƒè§„å¾‹ï¼Œä¸ºå¼€å‘æ›´å…·é’ˆå¯¹æ€§å’Œè‡ªé€‚åº”èƒ½åŠ›çš„å„¿ç«¥è¯­éŸ³ Speech interfaces æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.HC",
        "cs.LG",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted at Workshop on Child Computer Interaction (WOCCI 2025)",
      "pdf_url": "https://arxiv.org/pdf/2508.10332v1",
      "published_date": "2025-08-14 04:11:44 UTC",
      "updated_date": "2025-08-14 04:11:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:57:05.153932+00:00"
    },
    {
      "arxiv_id": "2508.10315v2",
      "title": "A Vision-Language Pre-training Model-Guided Approach for Mitigating Backdoor Attacks in Federated Learning",
      "title_zh": "è§†è§‰-è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹å¼•å¯¼çš„è”é‚¦å­¦ä¹ åé—¨æ”»å‡»ç¼“è§£æ–¹æ³•",
      "authors": [
        "Keke Gai",
        "Dongjue Wang",
        "Jing Yu",
        "Liehuang Zhu",
        "Qi Wu"
      ],
      "abstract": "Defending backdoor attacks in Federated Learning (FL) under heterogeneous client data distributions encounters limitations balancing effectiveness and privacy-preserving, while most existing methods highly rely on the assumption of homogeneous client data distributions or the availability of a clean serve dataset. In this paper, we propose an FL backdoor defense framework, named CLIP-Fed, that utilizes the zero-shot learning capabilities of vision-language pre-training models. Our scheme overcomes the limitations of Non-IID imposed on defense effectiveness by integrating pre-aggregation and post-aggregation defense strategies. CLIP-Fed aligns the knowledge of the global model and CLIP on the augmented dataset using prototype contrastive loss and Kullback-Leibler divergence, so that class prototype deviations caused by backdoor samples are ensured and the correlation between trigger patterns and target labels is eliminated. In order to balance privacy-preserving and coverage enhancement of the dataset against diverse triggers, we further construct and augment the server dataset via using the multimodal large language model and frequency analysis without any client samples. Extensive experiments on representative datasets evidence the effectiveness of CLIP-Fed. Comparing to other existing methods, CLIP-Fed achieves an average reduction in Attack Success Rate, {\\em i.e.}, 2.03\\% on CIFAR-10 and 1.35\\% on CIFAR-10-LT, while improving average Main Task Accuracy by 7.92\\% and 0.48\\%, respectively. Our codes are available at https://anonymous.4open.science/r/CLIP-Fed.",
      "tldr_zh": "é’ˆå¯¹è”é‚¦å­¦ä¹ (Federated Learning)åœ¨å¼‚æ„æ•°æ®åˆ†å¸ƒä¸‹éš¾ä»¥å¹³è¡¡åé—¨æ”»å‡»é˜²å¾¡æ•ˆæœä¸éšç§ä¿æŠ¤çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºCLIP-Fedçš„é˜²å¾¡æ¡†æ¶ï¼Œåˆ©ç”¨è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹(Vision-Language Pre-training Model)çš„é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›è¿›è¡Œè¾…åŠ©é˜²å¾¡ã€‚è¯¥æ–¹æ¡ˆé€šè¿‡æ•´åˆèšåˆå‰ä¸èšåˆåçš„é˜²å¾¡ç­–ç•¥ï¼Œæœ‰æ•ˆå…‹æœäº†éç‹¬ç«‹åŒåˆ†å¸ƒ(Non-IID)ç‰¹æ€§å¯¹é˜²å¾¡æ€§èƒ½çš„è´Ÿé¢å½±å“ã€‚CLIP-Fedåˆ©ç”¨åŸå‹å¯¹æ¯”æŸå¤±(Prototype Contrastive Loss)å’ŒKLæ•£åº¦(Kullback-Leibler divergence)å°†å…¨å±€æ¨¡å‹ä¸CLIPçš„çŸ¥è¯†è¿›è¡Œå¯¹é½ï¼Œä»è€Œçº æ­£åé—¨æ ·æœ¬å¯¼è‡´çš„ç±»åˆ«åŸå‹åå·®å¹¶æ¶ˆé™¤è§¦å‘å™¨ä¸ç›®æ ‡æ ‡ç­¾çš„å…³è”ã€‚ä¸ºäº†åœ¨ä¿æŠ¤éšç§çš„åŒæ—¶å¢å¼ºå¯¹å¤šæ ·åŒ–è§¦å‘å™¨çš„è¦†ç›–ï¼Œç ”ç©¶è¿›ä¸€æ­¥åˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(Multimodal Large Language Model)å’Œé¢‘ç‡åˆ†æï¼Œåœ¨ä¸ä¾èµ–å®¢æˆ·ç«¯æ ·æœ¬çš„æƒ…å†µä¸‹æ„å»ºå¹¶å¢å¼ºæœåŠ¡å™¨ç«¯æ•°æ®é›†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCLIP-Fedåœ¨CIFAR-10å’ŒCIFAR-10-LTæ•°æ®é›†ä¸Šæ˜¾è‘—é™ä½äº†æ”»å‡»æˆåŠŸç‡(Attack Success Rate)ï¼Œå¹¶æœ‰æ•ˆæå‡äº†ä¸»ä»»åŠ¡å‡†ç¡®ç‡(Main Task Accuracy)ï¼ŒéªŒè¯äº†å…¶åœ¨å¤æ‚è”é‚¦å­¦ä¹ ç¯å¢ƒä¸‹çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10315v2",
      "published_date": "2025-08-14 03:39:54 UTC",
      "updated_date": "2025-10-13 07:55:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:57:17.870356+00:00"
    },
    {
      "arxiv_id": "2508.15804v1",
      "title": "ReportBench: Evaluating Deep Research Agents via Academic Survey Tasks",
      "title_zh": "ReportBenchï¼šåŸºäºå­¦æœ¯ç»¼è¿°ä»»åŠ¡çš„æ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“è¯„ä¼°",
      "authors": [
        "Minghao Li",
        "Ying Zeng",
        "Zhihao Cheng",
        "Cong Ma",
        "Kai Jia"
      ],
      "abstract": "The advent of Deep Research agents has substantially reduced the time required for conducting extensive research tasks. However, these tasks inherently demand rigorous standards of factual accuracy and comprehensiveness, necessitating thorough evaluation before widespread adoption. In this paper, we propose ReportBench, a systematic benchmark designed to evaluate the content quality of research reports generated by large language models (LLMs). Our evaluation focuses on two critical dimensions: (1) the quality and relevance of cited literature, and (2) the faithfulness and veracity of the statements within the generated reports. ReportBench leverages high-quality published survey papers available on arXiv as gold-standard references, from which we apply reverse prompt engineering to derive domain-specific prompts and establish a comprehensive evaluation corpus. Furthermore, we develop an agent-based automated framework within ReportBench that systematically analyzes generated reports by extracting citations and statements, checking the faithfulness of cited content against original sources, and validating non-cited claims using web-based resources. Empirical evaluations demonstrate that commercial Deep Research agents such as those developed by OpenAI and Google consistently generate more comprehensive and reliable reports than standalone LLMs augmented with search or browsing tools. However, there remains substantial room for improvement in terms of the breadth and depth of research coverage, as well as factual consistency. The complete code and data will be released at the following link: https://github.com/ByteDance-BandAI/ReportBench",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ReportBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) ç”Ÿæˆçš„ç ”ç©¶æŠ¥å‘Šå†…å®¹è´¨é‡çš„ç³»ç»ŸåŒ–åŸºå‡†ã€‚è¯„ä¼°é‡ç‚¹é›†ä¸­åœ¨ä¸¤ä¸ªå…³é”®ç»´åº¦ï¼šå¼•ç”¨æ–‡çŒ®çš„è´¨é‡ä¸ç›¸å…³æ€§ï¼Œä»¥åŠæŠ¥å‘Šå†…é™ˆè¿°çš„å¿ å®åº¦ (Faithfulness) å’ŒçœŸå®æ€§ (Veracity)ã€‚ReportBench åˆ©ç”¨ arXiv ä¸Šé«˜è´¨é‡çš„å·²å‘è¡¨ç»¼è¿°è®ºæ–‡ä½œä¸ºé‡‘æ ‡å‡†å‚è€ƒï¼Œé€šè¿‡é€†å‘æç¤ºå·¥ç¨‹ (Reverse Prompt Engineering) æ¨å¯¼å‡ºç‰¹å®šé¢†åŸŸçš„æç¤ºï¼Œå¹¶å»ºç«‹äº†å…¨é¢çš„è¯„ä¼°è¯­æ–™åº“ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶å¼€å‘äº†ä¸€ä¸ªåŸºäºæ™ºèƒ½ä½“çš„è‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œé€šè¿‡æå–å¼•ç”¨å’Œé™ˆè¿°ã€å¯¹ç…§åŸå§‹æ¥æºæ£€æŸ¥å†…å®¹å¿ å®åº¦ï¼Œå¹¶åˆ©ç”¨ Web èµ„æºéªŒè¯æœªå¼•ç”¨å£°æ˜ã€‚å®è¯è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œç”± OpenAI å’Œ Google å¼€å‘çš„å•†ä¸š Deep Research æ™ºèƒ½ä½“åœ¨æŠ¥å‘Šçš„å…¨é¢æ€§å’Œå¯é æ€§ä¸Šä¸€è‡´ä¼˜äºå•çº¯ç»“åˆæœç´¢å·¥å…·çš„ LLMsã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨ç ”ç©¶è¦†ç›–çš„å¹¿åº¦ã€æ·±åº¦ä»¥åŠäº‹å®ä¸€è‡´æ€§ (Factual Consistency) æ–¹é¢ä»å­˜åœ¨æ˜¾è‘—çš„æå‡ç©ºé—´ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15804v1",
      "published_date": "2025-08-14 03:33:43 UTC",
      "updated_date": "2025-08-14 03:33:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:57:11.066747+00:00"
    },
    {
      "arxiv_id": "2508.10308v1",
      "title": "ReviewRL: Towards Automated Scientific Review with RL",
      "title_zh": "ReviewRLï¼šè¿ˆå‘åŸºäºå¼ºåŒ–å­¦ä¹ çš„è‡ªåŠ¨åŒ–å­¦æœ¯è¯„å®¡",
      "authors": [
        "Sihang Zeng",
        "Kai Tian",
        "Kaiyan Zhang",
        "Yuru wang",
        "Junqi Gao",
        "Runze Liu",
        "Sa Yang",
        "Jingxuan Li",
        "Xinwei Long",
        "Jiaheng Ma",
        "Biqing Qi",
        "Bowen Zhou"
      ],
      "abstract": "Peer review is essential for scientific progress but faces growing challenges due to increasing submission volumes and reviewer fatigue. Existing automated review approaches struggle with factual accuracy, rating consistency, and analytical depth, often generating superficial or generic feedback lacking the insights characteristic of high-quality human reviews. We introduce ReviewRL, a reinforcement learning framework for generating comprehensive and factually grounded scientific paper reviews. Our approach combines: (1) an ArXiv-MCP retrieval-augmented context generation pipeline that incorporates relevant scientific literature, (2) supervised fine-tuning that establishes foundational reviewing capabilities, and (3) a reinforcement learning procedure with a composite reward function that jointly enhances review quality and rating accuracy. Experiments on ICLR 2025 papers demonstrate that ReviewRL significantly outperforms existing methods across both rule-based metrics and model-based quality assessments. ReviewRL establishes a foundational framework for RL-driven automatic critique generation in scientific discovery, demonstrating promising potential for future development in this domain. The implementation of ReviewRL will be released at GitHub.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒè¡Œè¯„å®¡ä¸­å­˜åœ¨çš„å‡†ç¡®æ€§ä¸è¶³å’Œæ·±åº¦æ¬ ç¼ºç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº† ReviewRLï¼Œä¸€ä¸ªæ—¨åœ¨ç”Ÿæˆå…¨é¢ä¸”å…·æœ‰äº‹å®ä¾æ®çš„ç§‘å­¦è®ºæ–‡è¯„å®¡çš„å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) æ¡†æ¶ã€‚è¯¥æ¡†æ¶é¦–å…ˆç»“åˆäº† ArXiv-MCP æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) æµæ°´çº¿ï¼Œé€šè¿‡å¼•å…¥ç›¸å…³ç§‘å­¦æ–‡çŒ®æ¥å¢å¼ºèƒŒæ™¯ä¿¡æ¯çš„ç”Ÿæˆã€‚å…¶æ¬¡ï¼Œé€šè¿‡æœ‰ç›‘ç£å¾®è°ƒ (Supervised Fine-tuning) å»ºç«‹äº†åŸºç¡€çš„è¯„å®¡èƒ½åŠ›ã€‚éšåï¼Œåˆ©ç”¨åŒ…å«å¤åˆå¥–åŠ±å‡½æ•°çš„å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ï¼Œå…±åŒæå‡è¯„å®¡è´¨é‡å’Œè¯„åˆ†å‡†ç¡®æ€§ã€‚åœ¨ ICLR 2025 è®ºæ–‡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒReviewRL åœ¨åŸºäºè§„åˆ™çš„æŒ‡æ ‡å’ŒåŸºäºæ¨¡å‹çš„è´¨é‡è¯„ä¼°ä¸­å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚è¯¥ç ”ç©¶ä¸ºç§‘å­¦å‘ç°é¢†åŸŸä¸­ç”±å¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„è‡ªåŠ¨åŒ–è¯„è®ºç”Ÿæˆå¥ å®šäº†åŸºç¡€æ¡†æ¶ï¼Œå±•ç¤ºäº†è¯¥é¢†åŸŸæœªæ¥å‘å±•çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "13 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.10308v1",
      "published_date": "2025-08-14 03:26:13 UTC",
      "updated_date": "2025-08-14 03:26:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:57:13.953269+00:00"
    },
    {
      "arxiv_id": "2508.10304v1",
      "title": "Yet another algorithmic bias: A Discursive Analysis of Large Language Models Reinforcing Dominant Discourses on Gender and Race",
      "title_zh": "åˆä¸€ç§ç®—æ³•åè§ï¼šå¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–æ€§åˆ«ä¸ç§æ—ä¸»å¯¼è¯è¯­çš„è¯è¯­åˆ†æ",
      "authors": [
        "Gustavo Bonil",
        "Simone Hashiguti",
        "Jhessica Silva",
        "JoÃ£o Gondim",
        "Helena Maia",
        "NÃ¡dia Silva",
        "Helio Pedrini",
        "Sandra Avila"
      ],
      "abstract": "With the advance of Artificial Intelligence (AI), Large Language Models (LLMs) have gained prominence and been applied in diverse contexts. As they evolve into more sophisticated versions, it is essential to assess whether they reproduce biases, such as discrimination and racialization, while maintaining hegemonic discourses. Current bias detection approaches rely mostly on quantitative, automated methods, which often overlook the nuanced ways in which biases emerge in natural language. This study proposes a qualitative, discursive framework to complement such methods. Through manual analysis of LLM-generated short stories featuring Black and white women, we investigate gender and racial biases. We contend that qualitative methods such as the one proposed here are fundamental to help both developers and users identify the precise ways in which biases manifest in LLM outputs, thus enabling better conditions to mitigate them. Results show that Black women are portrayed as tied to ancestry and resistance, while white women appear in self-discovery processes. These patterns reflect how language models replicate crystalized discursive representations, reinforcing essentialization and a sense of social immobility. When prompted to correct biases, models offered superficial revisions that maintained problematic meanings, revealing limitations in fostering inclusive narratives. Our results demonstrate the ideological functioning of algorithms and have significant implications for the ethical use and development of AI. The study reinforces the need for critical, interdisciplinary approaches to AI design and deployment, addressing how LLM-generated discourses reflect and perpetuate inequalities.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)å¯èƒ½å¤åˆ¶æ­§è§†å’Œç§æ—åŒ–åè§çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å®šæ€§çš„è¯­ç¯‡åˆ†ææ¡†æ¶(discursive framework)ï¼Œæ—¨åœ¨å¼¥è¡¥ç°æœ‰å®šé‡è‡ªåŠ¨æ£€æµ‹æ–¹æ³•çš„ä¸è¶³ã€‚é€šè¿‡å¯¹LLMsç”Ÿæˆçš„æ¶‰åŠé»‘äººä¸ç™½äººå¥³æ€§çš„çŸ­ç¯‡æ•…äº‹è¿›è¡Œäººå·¥åˆ†æï¼Œç ”ç©¶äººå‘˜æ·±å…¥æ¢è®¨äº†æ€§åˆ«ä¸ç§æ—åè§çš„å…·ä½“è¡¨ç°å½¢å¼ã€‚ç ”ç©¶å‘ç°ï¼Œæ¨¡å‹åœ¨åˆ›ä½œä¸­å°†é»‘äººå¥³æ€§æç»˜ä¸ºä¸ç¥–å…ˆ(ancestry)å’ŒåæŠ—(resistance)ç´§å¯†ç›¸è¿ï¼Œè€Œç™½äººå¥³æ€§åˆ™æ›´å¤šåœ°å‡ºç°åœ¨è‡ªæˆ‘å‘ç°(self-discovery)çš„è¿‡ç¨‹ä¸­ã€‚è¿™äº›æ¨¡å¼åæ˜ äº†è¯­è¨€æ¨¡å‹å¦‚ä½•é€šè¿‡å¤åˆ¶å›ºåŒ–çš„è¯­ç¯‡è¡¨è¾¾ï¼Œå¼ºåŒ–äº†æœ¬è´¨ä¸»ä¹‰(essentialization)å’Œç¤¾ä¼šåœæ»æ„Ÿã€‚å½“è¢«è¦æ±‚çº æ­£åè§æ—¶ï¼Œæ¨¡å‹ä»…æä¾›è‚¤æµ…çš„ä¿®æ­£ï¼Œæ­ç¤ºäº†å…¶åœ¨æ„å»ºåŒ…å®¹æ€§å™äº‹æ–¹é¢çš„å±€é™æ€§ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†ç®—æ³•çš„æ„è¯†å½¢æ€è¿ä½œæœºåˆ¶ï¼Œå¼ºè°ƒåœ¨äººå·¥æ™ºèƒ½(AI)çš„è®¾è®¡ä¸éƒ¨ç½²ä¸­ï¼ŒäºŸéœ€è·¨å­¦ç§‘çš„æ‰¹è¯„æ–¹æ³•æ¥åº”å¯¹LLMså¯¹ç¤¾ä¼šä¸å¹³ç­‰çš„æŒç»­å›ºåŒ–ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "29 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2508.10304v1",
      "published_date": "2025-08-14 03:22:02 UTC",
      "updated_date": "2025-08-14 03:22:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:57:27.749912+00:00"
    },
    {
      "arxiv_id": "2508.16620v2",
      "title": "STRelay: A Universal Spatio-Temporal Relaying Framework for Location Prediction over Human Trajectory Data",
      "title_zh": "STRelayï¼šé¢å‘äººç±»è½¨è¿¹æ•°æ®ä½ç½®é¢„æµ‹çš„é€šç”¨æ—¶ç©ºä¸­ç»§æ¡†æ¶",
      "authors": [
        "Bangchao Deng",
        "Lianhua Ji",
        "Chunhua Chen",
        "Xin Jing",
        "Ling Ding",
        "Bingqing QU",
        "Pengyang Wang",
        "Dingqi Yang"
      ],
      "abstract": "Next location prediction is a critical task in human mobility modeling, enabling applications like travel planning and urban mobility management. Existing methods mainly rely on historical spatiotemporal trajectory data to train sequence models that directly forecast future locations. However, they often overlook the importance of the future spatiotemporal contexts, which are highly informative for the future locations. For example, knowing how much time and distance a user will travel could serve as a critical clue for predicting the user's next location. Against this background, we propose \\textbf{STRelay}, a universal \\textbf{\\underline{S}}patio\\textbf{\\underline{T}}emporal \\textbf{\\underline{Relay}}ing framework explicitly modeling the future spatiotemporal context given a human trajectory, to boost the performance of different location prediction models. Specifically, STRelay models future spatiotemporal contexts in a relaying manner, which is subsequently integrated with the encoded historical representation from a base location prediction model, enabling multi-task learning by simultaneously predicting the next time interval, next moving distance interval, and finally the next location. We evaluate STRelay integrated with five state-of-the-art location prediction base models on four real-world trajectory datasets. Results demonstrate that STRelay consistently improves prediction performance across all cases by 2.49\\%-11.30\\%. Additionally, we find that the future spatiotemporal contexts are particularly helpful for entertainment-related locations and also for user groups who prefer traveling longer distances. The performance gain on such non-daily-routine activities, which often suffer from higher uncertainty, is indeed complementary to the base location prediction models that often excel at modeling regular daily routine patterns.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†STRelayï¼Œè¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„æ—¶ç©ºæ¥åŠ›æ¡†æ¶(Spatio-Temporal Relaying framework)ï¼Œæ—¨åœ¨é€šè¿‡æ˜¾å¼å»ºæ¨¡æœªæ¥æ—¶ç©ºä¸Šä¸‹æ–‡æ¥æå‡äººç±»ç§»åŠ¨è½¨è¿¹é¢„æµ‹çš„æ€§èƒ½ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•å¾€å¾€å¿½ç•¥æœªæ¥æ—¶é—´ä¸è·ç¦»ç­‰å…³é”®çº¿ç´¢çš„é—®é¢˜ï¼ŒSTRelayä»¥æ¥åŠ›æ–¹å¼å»ºæ¨¡æœªæ¥æ—¶ç©ºèƒŒæ™¯ï¼Œå¹¶å°†å…¶ä¸åŸºç¡€ä½ç½®é¢„æµ‹æ¨¡å‹æå–çš„å†å²è¡¨å¾ç›¸ç»“åˆï¼Œé€šè¿‡å¤šä»»åŠ¡å­¦ä¹ (multi-task learning)åŒæ—¶é¢„æµ‹ä¸‹ä¸€æ—¶é—´é—´éš”ã€ä¸‹ä¸€ç§»åŠ¨è·ç¦»åŠæœ€ç»ˆä½ç½®ã€‚å®éªŒåœ¨å››ä¸ªçœŸå®æ•°æ®é›†ä¸Šç»“åˆäº”ç§å…ˆè¿›åŸºç¡€æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºSTRelayåœ¨æ‰€æœ‰æ¡ˆä¾‹ä¸­å‡å®ç°äº†2.49%-11.30%çš„æ€§èƒ½æå‡ã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°ï¼Œè¯¥æ¡†æ¶åœ¨é¢„æµ‹å¨±ä¹ç›¸å…³åœ°ç‚¹åŠé•¿è·ç¦»å‡ºè¡Œç­‰éæ—¥å¸¸å¸¸è§„æ´»åŠ¨æ–¹é¢è¡¨ç°å°¤ä¸ºçªå‡ºï¼Œæœ‰æ•ˆå¼¥è¡¥äº†ä¼ ç»Ÿæ¨¡å‹åœ¨å¤„ç†é«˜ä¸ç¡®å®šæ€§è½¨è¿¹æ—¶çš„ä¸è¶³ï¼Œæ˜¾è‘—å¢å¼ºäº†ä½ç½®é¢„æµ‹çš„é²æ£’æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.16620v2",
      "published_date": "2025-08-14 02:44:33 UTC",
      "updated_date": "2025-12-30 03:45:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:57:40.793753+00:00"
    },
    {
      "arxiv_id": "2508.10293v2",
      "title": "Promoting Efficient Reasoning with Verifiable Stepwise Reward",
      "title_zh": "é€šè¿‡å¯éªŒè¯çš„åˆ†æ­¥å¥–åŠ±æå‡æ¨ç†æ•ˆç‡",
      "authors": [
        "Chuhuai Yue",
        "Chengqi Dong",
        "Yinan Gao",
        "Hang He",
        "Jiajun Chai",
        "Guojun Yin",
        "Wei Lin"
      ],
      "abstract": "Large reasoning models (LRMs) have recently achieved significant progress in complex reasoning tasks, aided by reinforcement learning with verifiable rewards. However, LRMs often suffer from overthinking, expending excessive computation on simple problems and reducing efficiency. Existing efficient reasoning methods typically require accurate task assessment to preset token budgets or select reasoning modes, which limits their flexibility and reliability. In this work, we revisit the essence of overthinking and identify that encouraging effective steps while penalizing ineffective ones is key to its solution. To this end, we propose a novel rule-based verifiable stepwise reward mechanism (VSRM), which assigns rewards based on the performance of intermediate states in the reasoning trajectory. This approach is intuitive and naturally fits the step-by-step nature of reasoning tasks. We conduct extensive experiments on standard mathematical reasoning benchmarks, including AIME24 and AIME25, by integrating VSRM with PPO and Reinforce++. Results show that our method achieves substantial output length reduction while maintaining original reasoning performance, striking an optimal balance between efficiency and accuracy. Further analysis of overthinking frequency and pass@k score before and after training demonstrates that our approach in deed effectively suppresses ineffective steps and encourages effective reasoning, fundamentally alleviating the overthinking problem. All code will be released upon acceptance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹æ¨ç†æ¨¡å‹(Large reasoning models, LRMs)åœ¨å¤„ç†ç®€å•é—®é¢˜æ—¶å› è¿‡åº¦è®¡ç®—è€Œäº§ç”Ÿçš„â€œè¿‡æ€è€ƒâ€(overthinking)é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºè§„åˆ™çš„å¯éªŒè¯é€æ­¥å¥–åŠ±æœºåˆ¶(Verifiable Stepwise Reward Mechanism, VSRM)ã€‚è¯¥æœºåˆ¶æ ¹æ®æ¨ç†è½¨è¿¹ä¸­ä¸­é—´çŠ¶æ€çš„è¡¨ç°åˆ†é…å¥–åŠ±ï¼Œæ—¨åœ¨é¼“åŠ±æœ‰æ•ˆæ¨ç†æ­¥éª¤å¹¶æƒ©ç½šæ— æ•ˆæ­¥éª¤ï¼Œä»è€Œå¥‘åˆæ¨ç†ä»»åŠ¡çš„é€æ­¥ç‰¹æ€§ã€‚é€šè¿‡å°†VSRMä¸PPOå’ŒReinforce++ç®—æ³•ç»“åˆï¼Œå¹¶åœ¨AIME24å’ŒAIME25ç­‰æ•°å­¦æ¨ç†åŸºå‡†ä¸Šè¿›è¡Œå®éªŒï¼Œç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨ä¿æŒåŸæœ‰æ¨ç†å‡†ç¡®ç‡çš„åŒæ—¶æ˜¾è‘—ç¼©çŸ­äº†è¾“å‡ºé•¿åº¦ï¼Œå®ç°äº†æ•ˆç‡ä¸ç²¾åº¦çš„å¹³è¡¡ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆæŠ‘åˆ¶æ— æ•ˆæ¨ç†æ­¥éª¤ï¼Œä»æ ¹æœ¬ä¸Šç¼“è§£äº†æ¨¡å‹çš„è¿‡æ€è€ƒç°è±¡ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.10293v2",
      "published_date": "2025-08-14 02:43:53 UTC",
      "updated_date": "2025-08-16 13:14:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:57:42.991906+00:00"
    },
    {
      "arxiv_id": "2508.11708v3",
      "title": "Street Review: A Participatory AI-Based Framework for Assessing Streetscape Inclusivity",
      "title_zh": "Street Reviewï¼šä¸€ç§åŸºäºå‚ä¸å¼äººå·¥æ™ºèƒ½çš„è¡—æ™¯åŒ…å®¹æ€§è¯„ä¼°æ¡†æ¶",
      "authors": [
        "Rashid Mushkani",
        "Shin Koseki"
      ],
      "abstract": "Urban centers undergo social, demographic, and cultural changes that shape public street use and require systematic evaluation of public spaces. This study presents Street Review, a mixed-methods approach that combines participatory research with AI-based analysis to assess streetscape inclusivity. In MontrÃ©al, Canada, 28 residents participated in semi-directed interviews and image evaluations, supported by the analysis of approximately 45,000 street-view images from Mapillary. The approach produced visual analytics, such as heatmaps, to correlate subjective user ratings with physical attributes like sidewalk, maintenance, greenery, and seating. Findings reveal variations in perceptions of inclusivity and accessibility across demographic groups, demonstrating that incorporating diverse user feedback can enhance machine learning models through careful data-labeling and co-production strategies. The Street Review framework offers a systematic method for urban planners and policy analysts to inform planning, policy development, and management of public streets.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Street Reviewï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆäº†å‚ä¸å¼ç ”ç©¶(Participatory Research)ä¸äººå·¥æ™ºèƒ½(AI)åˆ†æçš„æ··åˆæ–¹æ³•æ¡†æ¶ï¼Œæ—¨åœ¨ç³»ç»Ÿæ€§è¯„ä¼°åŸå¸‚è¡—æ™¯çš„åŒ…å®¹æ€§(Inclusivity)ã€‚ç ”ç©¶ä»¥åŠ æ‹¿å¤§è’™ç‰¹åˆ©å°”ä¸ºæ¡ˆä¾‹ï¼Œé€šè¿‡ 28 åå±…æ°‘çš„æ·±åº¦è®¿è°ˆä¸ Mapillary å¹³å°ä¸Šçº¦ 45,000 å¼ è¡—æ™¯å›¾åƒçš„ AI åˆ†æç›¸ç»“åˆï¼Œåˆ©ç”¨çƒ­åŠ›å›¾ç­‰å¯è§†åŒ–æ‰‹æ®µå»ºç«‹äº†ç”¨æˆ·ä¸»è§‚è¯„åˆ†ä¸äººè¡Œé“ã€ç»¿åŒ–åŠç»´æŠ¤çŠ¶å†µç­‰ç‰©ç†å±æ€§çš„å…³è”ã€‚ç ”ç©¶å‘ç°ï¼Œä¸åŒäººå£èƒŒæ™¯çš„ç¾¤ä½“å¯¹è¡—é“åŒ…å®¹æ€§å’Œå¯è¾¾æ€§(Accessibility)çš„æ„ŸçŸ¥å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œè¯æ˜äº†é€šè¿‡ååŒç”Ÿäº§(Co-production)ç­–ç•¥å°†å¤šå…ƒç”¨æˆ·åé¦ˆçº³å…¥æœºå™¨å­¦ä¹ æ¨¡å‹å¯ä»¥æœ‰æ•ˆæå‡æ¨¡å‹è¡¨ç°ã€‚è¯¥æ¡†æ¶ä¸ºåŸå¸‚è§„åˆ’è€…å’Œæ”¿ç­–åˆ†æå¸ˆæä¾›äº†ä¸€å¥—ç§‘å­¦çš„å·¥å…·ï¼Œç”¨äºæŒ‡å¯¼å…¬å…±è¡—é“çš„è§„åˆ’ã€ç®¡ç†åŠæ”¿ç­–åˆ¶å®šï¼Œä»è€Œä¿ƒè¿›åŸå¸‚ç©ºé—´çš„å…¬å¹³åˆ©ç”¨ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.11708v3",
      "published_date": "2025-08-14 02:40:56 UTC",
      "updated_date": "2025-11-03 19:45:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:57:51.390511+00:00"
    },
    {
      "arxiv_id": "2508.10268v1",
      "title": "Pose-Robust Calibration Strategy for Point-of-Gaze Estimation on Mobile Phones",
      "title_zh": "æ™ºèƒ½æ‰‹æœºæ³¨è§†ç‚¹ä¼°è®¡çš„å§¿æ€é²æ£’æ ¡å‡†ç­–ç•¥",
      "authors": [
        "Yujie Zhao",
        "Jiabei Zeng",
        "Shiguang Shan"
      ],
      "abstract": "Although appearance-based point-of-gaze (PoG) estimation has improved, the estimators still struggle to generalize across individuals due to personal differences. Therefore, person-specific calibration is required for accurate PoG estimation. However, calibrated PoG estimators are often sensitive to head pose variations. To address this, we investigate the key factors influencing calibrated estimators and explore pose-robust calibration strategies. Specifically, we first construct a benchmark, MobilePoG, which includes facial images from 32 individuals focusing on designated points under either fixed or continuously changing head poses. Using this benchmark, we systematically analyze how the diversity of calibration points and head poses influences estimation accuracy. Our experiments show that introducing a wider range of head poses during calibration improves the estimator's ability to handle pose variation. Building on this insight, we propose a dynamic calibration strategy in which users fixate on calibration points while moving their phones. This strategy naturally introduces head pose variation during a user-friendly and efficient calibration process, ultimately producing a better calibrated PoG estimator that is less sensitive to head pose variations than those using conventional calibration strategies. Codes and datasets are available at our project page.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰‹æœºç«¯åŸºäºè¡¨è§‚çš„è§†çº¿ä¼°è®¡(Point-of-Gaze, PoG)åœ¨è·¨ä¸ªä½“æ³›åŒ–åŠå¤´éƒ¨å§¿æ€å˜åŒ–ä¸‹çš„é²æ£’æ€§éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§å§¿æ€é²æ£’çš„æ ¡å‡†ç­–ç•¥ã€‚ä½œè€…é¦–å…ˆæ„å»ºäº†åŒ…å«32åå—è¯•è€…åœ¨å›ºå®šå’Œè¿ç»­å˜åŒ–å§¿æ€ä¸‹æ•°æ®çš„MobilePoGåŸºå‡†æµ‹è¯•é›†ï¼Œå¹¶ç³»ç»Ÿåˆ†æäº†æ ¡å‡†ç‚¹å¤šæ ·æ€§ä¸å¤´éƒ¨å§¿æ€å¯¹ä¼°è®¡å‡†ç¡®åº¦çš„å½±å“ã€‚å®éªŒå‘ç°ï¼Œåœ¨æ ¡å‡†è¿‡ç¨‹ä¸­å¼•å…¥æ›´å¹¿æ³›çš„å¤´éƒ¨å§¿æ€èƒ½æ˜¾è‘—å¢å¼ºä¼°è®¡å™¨å¤„ç†å§¿æ€æ³¢åŠ¨çš„èƒ½åŠ›ã€‚æ®æ­¤ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åŠ¨æ€æ ¡å‡†ç­–ç•¥ï¼Œå¼•å¯¼ç”¨æˆ·åœ¨æ³¨è§†ç‰¹å®šç‚¹çš„åŒæ—¶ç§»åŠ¨æ‰‹æœºï¼Œä»è€Œåœ¨é«˜æ•ˆä¸”å‹å¥½çš„äº¤äº’ä¸­è‡ªç„¶å¼•å…¥å§¿æ€å¤šæ ·æ€§ã€‚æœ€ç»ˆç»“æœè¯æ˜ï¼Œè¯¥ç­–ç•¥ç”Ÿæˆçš„PoGä¼°è®¡å™¨ç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•å¯¹å¤´éƒ¨å§¿æ€å˜åŒ–æ›´å…·é²æ£’æ€§ï¼Œæœ‰æ•ˆæå‡äº†ç§»åŠ¨ç«¯è§†çº¿è¿½è¸ªçš„å‡†ç¡®æ€§ä¸å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted for British Machine Vision Conference (BMVC) 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.10268v1",
      "published_date": "2025-08-14 01:28:30 UTC",
      "updated_date": "2025-08-14 01:28:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:57:56.786041+00:00"
    },
    {
      "arxiv_id": "2508.15802v1",
      "title": "MAC: A Live Benchmark for Multimodal Large Language Models in Scientific Understanding",
      "title_zh": "MACï¼šå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç§‘å­¦ç†è§£èƒ½åŠ›çš„åŠ¨æ€åŸºå‡†",
      "authors": [
        "Mohan Jiang",
        "Jin Gao",
        "Jiahao Zhan",
        "Dequan Wang"
      ],
      "abstract": "As multimodal large language models (MLLMs) grow increasingly capable, fixed benchmarks are gradually losing their effectiveness in evaluating high-level scientific understanding. In this paper, we introduce the Multimodal Academic Cover benchmark (MAC), a live benchmark that could continuously evolve with scientific advancement and model progress. MAC leverages over 25,000 image-text pairs sourced from issues of top-tier scientific journals such as Nature, Science, and Cell, challenging MLLMs to reason across abstract visual and textual scientific content. Experiments on our most recent yearly snapshot, MAC-2025, reveal that while MLLMs demonstrate strong perceptual abilities, their cross-modal scientific reasoning remains limited. To bridge this gap, we propose DAD, a lightweight inference-time approach that enhances MLLMs by extending MLLM visual features with language space reasoning, achieving performance improvements of up to 11%. Finally, we highlight the live nature of MAC through experiments on updating journal covers and models for curation, illustrating its potential to remain aligned with the frontier of human knowledge. We release our benchmark at https://github.com/mhjiang0408/MAC_Bench.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Multimodal Academic Cover (MAC)åŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) ç§‘å­¦ç†è§£èƒ½åŠ›çš„åŠ¨æ€åŸºå‡†ï¼Œæ—¨åœ¨è§£å†³å›ºå®šåŸºå‡†æµ‹è¯•åœ¨è¯„ä¼°é«˜é˜¶ç§‘å­¦ç†è§£æ—¶é€æ¸å¤±æ•ˆçš„é—®é¢˜ã€‚MAC åˆ©ç”¨äº†æºè‡ª Natureã€Science å’Œ Cell ç­‰é¡¶å°–ç§‘å­¦æœŸåˆŠçš„ 25,000 å¤šä¸ªå›¾åƒ-æ–‡æœ¬å¯¹ï¼ŒæŒ‘æˆ˜æ¨¡å‹åœ¨æŠ½è±¡è§†è§‰ä¸æ–‡æœ¬ç§‘å­¦å†…å®¹ä¹‹é—´çš„æ¨ç†èƒ½åŠ›ã€‚å¯¹ MAC-2025 è¿™ä¸€æœ€æ–°å¹´åº¦å¿«ç…§çš„å®éªŒå‘ç°ï¼Œå°½ç®¡ MLLMs å±•ç°å‡ºè¾ƒå¼ºçš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œä½†å…¶è·¨æ¨¡æ€ç§‘å­¦æ¨ç†èƒ½åŠ›ä»ç„¶å—é™ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº† DADï¼Œä¸€ç§è½»é‡çº§çš„æ¨ç†é˜¶æ®µ (inference-time) æ–¹æ³•ï¼Œé€šè¿‡åœ¨è¯­è¨€ç©ºé—´æ¨ç†ä¸­æ‰©å±• MLLM çš„è§†è§‰ç‰¹å¾ï¼Œå®ç°äº†æœ€é«˜ 11% çš„æ€§èƒ½æå‡ã€‚æœ€åï¼Œå®éªŒè¿›ä¸€æ­¥å±•ç¤ºäº† MAC çš„åŠ¨æ€æ›´æ–°ç‰¹æ€§ï¼Œä½¿å…¶èƒ½å¤Ÿä¸äººç±»ç§‘å­¦çŸ¥è¯†çš„å‰æ²¿ä¿æŒåŒæ­¥ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.15802v1",
      "published_date": "2025-08-14 01:22:57 UTC",
      "updated_date": "2025-08-14 01:22:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:57:56.558189+00:00"
    },
    {
      "arxiv_id": "2508.10265v2",
      "title": "Why Cannot Large Language Models Ever Make True Correct Reasoning?",
      "title_zh": "ä¸ºä»€ä¹ˆå¤§è¯­è¨€æ¨¡å‹æ°¸è¿œæ— æ³•å®ç°çœŸæ­£çš„æ­£ç¡®æ¨ç†ï¼Ÿ",
      "authors": [
        "Jingde Cheng"
      ],
      "abstract": "Recently, with the application progress of AIGC tools based on large language models (LLMs), led by ChatGPT, many AI experts and more non-professionals are trumpeting the \"reasoning ability\" of the LLMs. The present author considers that the so-called \"reasoning ability\" of LLMs are just illusions of those people who with vague concepts. In fact, the LLMs can never have the true reasoning ability. This paper intents to explain that, because the essential limitations of their working principle, the LLMs can never have the ability of true correct reasoning.",
      "tldr_zh": "è¯¥è®ºæ–‡æ·±å…¥æ¢è®¨äº† Large Language Models (LLMs) æ˜¯å¦å…·å¤‡çœŸæ­£çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶å¯¹ç›®å‰ä»¥ ChatGPT ä¸ºä»£è¡¨çš„ AIGC å·¥å…·æ‰€å±•ç°å‡ºçš„â€œæ™ºèƒ½â€æå‡ºäº†è´¨ç–‘ã€‚ä½œè€…è®¤ä¸ºï¼Œè®¸å¤šä¸“å®¶å’Œéä¸“ä¸šäººå£«æ‰€å¹æ§çš„ LLMs æ¨ç†èƒ½åŠ›å®é™…ä¸Šæ˜¯ä¸€ç§æºäºæ¦‚å¿µæ¨¡ç³Šçš„é”™è§‰ (illusion)ï¼Œæœ¬è´¨ä¸Šå¹¶ä¸å­˜åœ¨ã€‚æ–‡ç« é€šè¿‡åˆ†æ LLMs çš„åº•å±‚å·¥ä½œåŸç† (working principle)ï¼Œè¯¦ç»†é˜è¿°äº†å…¶åœ¨å¤„ç†é€»è¾‘ä¸çœŸç†æ¨å¯¼æ—¶çš„æ ¹æœ¬æ€§å±€é™ã€‚ç ”ç©¶æ˜ç¡®æŒ‡å‡ºï¼Œç”±äºå…¶è¿è¡Œæœºåˆ¶çš„æœ¬è´¨é™åˆ¶ï¼ŒLLMs æ°¸è¿œæ— æ³•äº§ç”ŸçœŸæ­£ä¸”æ­£ç¡®çš„ Reasoning Abilityã€‚è¯¥è®ºæ–‡æ—¨åœ¨æ­£æœ¬æ¸…æºï¼Œä¸ºç†è§£ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„è®¤çŸ¥è¾¹ç•Œæä¾›æ‰¹åˆ¤æ€§çš„ç†è®ºè§£é‡Šã€‚",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "8 pages. arXiv admin note: substantial text overlap with arXiv:2412.12408",
      "pdf_url": "https://arxiv.org/pdf/2508.10265v2",
      "published_date": "2025-08-14 01:18:18 UTC",
      "updated_date": "2025-08-16 23:38:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:57:56.951621+00:00"
    },
    {
      "arxiv_id": "2508.10264v2",
      "title": "MRFD: Multi-Region Fusion Decoding with Self-Consistency for Mitigating Hallucinations in LVLMs",
      "title_zh": "MRFDï¼šæ—¨åœ¨ç¼“è§£ LVLMs å¹»è§‰çš„è‡ªä¸€è‡´æ€§å¤šåŒºåŸŸèåˆè§£ç ",
      "authors": [
        "Haonan Ge",
        "Yiwei Wang",
        "Ming-Hsuan Yang",
        "Yujun Cai"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) have shown strong performance across multimodal tasks. However, they often produce hallucinations -- text that is inconsistent with visual input, due to the limited ability to verify information in different regions of the image. To address this, we propose Multi-Region Fusion Decoding (MRFD), a training-free decoding method that improves factual grounding by modeling inter-region consistency. MRFD identifies salient regions using cross-attention, generates initial responses for each, and computes reliability weights based on Jensen-Shannon Divergence (JSD) among the responses. These weights guide a consistency-aware fusion of per-region predictions, using region-aware prompts inspired by Chain-of-Thought reasoning. Experiments across multiple LVLMs and benchmarks show that MRFD significantly reduces hallucinations and improves response factuality without requiring model updates.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ (Large Vision-Language Models, LVLMs) å› éªŒè¯å›¾åƒä¸åŒåŒºåŸŸä¿¡æ¯èƒ½åŠ›æœ‰é™è€Œå¯¼è‡´å¹»è§‰ (hallucinations) çš„æŒ‘æˆ˜ï¼Œæå‡ºäº† Multi-Region Fusion Decoding (MRFD) è§£ç æ–¹æ¡ˆã€‚MRFD æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒ (training-free) çš„è§£ç æ–¹æ³•ï¼Œæ ¸å¿ƒåœ¨äºé€šè¿‡å»ºæ¨¡åŒºåŸŸé—´çš„ä¸€è‡´æ€§ (inter-region consistency) æ¥æé«˜ç”Ÿæˆå†…å®¹çš„äº‹å®åŸºç¡€ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ cross-attention æœºåˆ¶è¯†åˆ«æ˜¾è‘—åŒºåŸŸå¹¶ä¸ºå„åŒºåŸŸç”Ÿæˆåˆå§‹å“åº”ï¼ŒåŒæ—¶é‡‡ç”¨ Jensen-Shannon Divergence (JSD) è®¡ç®—å“åº”çš„å¯é æ€§æƒé‡ã€‚å€Ÿé‰´ Chain-of-Thought æ¨ç†æ€è·¯ï¼ŒMRFD ä½¿ç”¨åŒºåŸŸæ„ŸçŸ¥æç¤ºå¼•å¯¼å„åŒºåŸŸé¢„æµ‹ç»“æœçš„ä¸€è‡´æ€§æ„ŸçŸ¥èåˆã€‚åœ¨å¤šé¡¹ LVLMs å’ŒåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼ŒMRFD èƒ½å¤Ÿæ˜¾è‘—é™ä½å¹»è§‰å¹¶æå‡å“åº”çš„äº‹å®æ€§ï¼Œä¸”æ— éœ€å¯¹æ¨¡å‹è¿›è¡Œä»»ä½•å‚æ•°æ›´æ–°ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "EMNLP 2025",
      "pdf_url": "https://arxiv.org/pdf/2508.10264v2",
      "published_date": "2025-08-14 01:17:39 UTC",
      "updated_date": "2025-10-13 09:52:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:58:08.794581+00:00"
    },
    {
      "arxiv_id": "2508.10260v1",
      "title": "DINOMotion: advanced robust tissue motion tracking with DINOv2 in 2D-Cine MRI-guided radiotherapy",
      "title_zh": "DINOMotionï¼š2D-Cine MRI å¼•å¯¼æ”¾ç–—ä¸­åŸºäº DINOv2 çš„å…ˆè¿›é²æ£’ç»„ç»‡è¿åŠ¨è·Ÿè¸ª",
      "authors": [
        "Soorena Salari",
        "Catherine Spino",
        "Laurie-Anne Pharand",
        "Fabienne Lathuiliere",
        "Hassan Rivaz",
        "Silvain Beriault",
        "Yiming Xiao"
      ],
      "abstract": "Accurate tissue motion tracking is critical to ensure treatment outcome and safety in 2D-Cine MRI-guided radiotherapy. This is typically achieved by registration of sequential images, but existing methods often face challenges with large misalignments and lack of interpretability. In this paper, we introduce DINOMotion, a novel deep learning framework based on DINOv2 with Low-Rank Adaptation (LoRA) layers for robust, efficient, and interpretable motion tracking. DINOMotion automatically detects corresponding landmarks to derive optimal image registration, enhancing interpretability by providing explicit visual correspondences between sequential images. The integration of LoRA layers reduces trainable parameters, improving training efficiency, while DINOv2's powerful feature representations offer robustness against large misalignments. Unlike iterative optimization-based methods, DINOMotion directly computes image registration at test time. Our experiments on volunteer and patient datasets demonstrate its effectiveness in estimating both linear and nonlinear transformations, achieving Dice scores of 92.07% for the kidney, 90.90% for the liver, and 95.23% for the lung, with corresponding Hausdorff distances of 5.47 mm, 8.31 mm, and 6.72 mm, respectively. DINOMotion processes each scan in approximately 30ms and consistently outperforms state-of-the-art methods, particularly in handling large misalignments. These results highlight its potential as a robust and interpretable solution for real-time motion tracking in 2D-Cine MRI-guided radiotherapy.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DINOMotionï¼Œä¸€ç§åŸºäºDINOv2çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³2D-Cine MRIå¼•å¯¼æ”¾ç–—ä¸­ç»„ç»‡è¿åŠ¨è·Ÿè¸ªé¢ä¸´çš„å¤§è§„æ¨¡å¤±å‡†å’Œç¼ºä¹å¯è§£é‡Šæ€§ç­‰æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡é›†æˆä½ç§©è‡ªé€‚åº”(LoRA)å±‚å‡å°‘äº†å‚æ•°é‡å¹¶æå‡äº†è®­ç»ƒæ•ˆç‡ï¼Œåˆ©ç”¨DINOv2å¼ºå¤§çš„ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›ç¡®ä¿äº†è·Ÿè¸ªçš„é²æ£’æ€§ã€‚DINOMotioné€šè¿‡è‡ªåŠ¨æ£€æµ‹åœ°æ ‡(landmarks)æ¥å®ç°æœ€ä¼˜å›¾åƒé…å‡†ï¼Œå¹¶èƒ½æä¾›æ˜ç¡®çš„è§†è§‰å¯¹åº”å…³ç³»ï¼Œä»è€Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹å†³ç­–çš„é€æ˜åº¦ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œè¯¥æ–¹æ³•æ— éœ€è¿­ä»£ä¼˜åŒ–å³å¯ç›´æ¥è®¡ç®—é…å‡†ç»“æœï¼Œæ¯å¸§å¤„ç†æ—¶é—´ä»…çº¦30msã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDINOMotionåœ¨è‚¾è„ã€è‚è„å’Œè‚ºéƒ¨çš„Diceè¯„åˆ†åˆ†åˆ«è¾¾åˆ°92.07%ã€90.90%å’Œ95.23%ï¼Œåœ¨å„é¡¹æŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰çš„å…ˆè¿›æ–¹æ³•ã€‚è¯¥ç ”ç©¶è¯æ˜äº†DINOMotionåœ¨å®æ—¶ã€é²æ£’ä¸”å¯è§£é‡Šçš„ç»„ç»‡è¿åŠ¨è·Ÿè¸ªé¢†åŸŸçš„å·¨å¤§åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "Accepted to IEEE Transactions on Biomedical Engineering (TMBE), 14 pages",
      "pdf_url": "https://arxiv.org/pdf/2508.10260v1",
      "published_date": "2025-08-14 01:02:26 UTC",
      "updated_date": "2025-08-14 01:02:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:58:11.591951+00:00"
    },
    {
      "arxiv_id": "2508.10252v1",
      "title": "Facilitating Longitudinal Interaction Studies of AI Systems",
      "title_zh": "æ¨åŠ¨äººå·¥æ™ºèƒ½ç³»ç»Ÿçºµå‘äº¤äº’ç ”ç©¶çš„å¼€å±•",
      "authors": [
        "Tao Long",
        "Sitong Wang",
        "Ã‰milie Fabre",
        "Tony Wang",
        "Anup Sathya",
        "Jason Wu",
        "Savvas Petridis",
        "Dingzeyu Li",
        "Tuhin Chakrabarty",
        "Yue Jiang",
        "Jingyi Li",
        "Tiffany Tseng",
        "Ken Nakagaki",
        "Qian Yang",
        "Nikolas Martelaro",
        "Jeffrey V. Nickerson",
        "Lydia B. Chilton"
      ],
      "abstract": "UIST researchers develop tools to address user challenges. However, user interactions with AI evolve over time through learning, adaptation, and repurposing, making one time evaluations insufficient. Capturing these dynamics requires longer-term studies, but challenges in deployment, evaluation design, and data collection have made such longitudinal research difficult to implement. Our workshop aims to tackle these challenges and prepare researchers with practical strategies for longitudinal studies. The workshop includes a keynote, panel discussions, and interactive breakout groups for discussion and hands-on protocol design and tool prototyping sessions. We seek to foster a community around longitudinal system research and promote it as a more embraced method for designing, building, and evaluating UIST tools.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•ä¿ƒè¿›äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„é•¿æœŸäº¤äº’ç ”ç©¶(Longitudinal Interaction Studies)ï¼Œæ—¨åœ¨è§£å†³ç”±äºç”¨æˆ·å­¦ä¹ å’Œé€‚åº”å¯¼è‡´äº¤äº’éšæ—¶é—´æ¼”å˜è€Œå•æ¬¡è¯„ä¼°ä¸è¶³çš„é—®é¢˜ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œå½“å‰çš„çºµå‘ç ”ç©¶åœ¨ç³»ç»Ÿéƒ¨ç½²ã€è¯„ä¼°è®¾è®¡å’Œæ•°æ®æ”¶é›†æ–¹é¢é¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œé™åˆ¶äº†ç›¸å…³å·¥å…·çš„å‘å±•ã€‚ä¸ºæ­¤ï¼Œä½œè€…ç»„ç»‡äº†ä¸€ä¸ªä¸“é—¨çš„å·¥ä½œåŠï¼Œé€šè¿‡ä¸»é¢˜æ¼”è®²(Keynote)ã€ä¸“é¢˜è®¨è®º(Panel)å’ŒåŸå‹è®¾è®¡ç­‰äº’åŠ¨ç¯èŠ‚ï¼Œä¸ºç ”ç©¶äººå‘˜æä¾›å¼€å±•çºµå‘ç ”ç©¶çš„å®ç”¨ç­–ç•¥ã€‚è¯¥é¡¹ç›®æ—¨åœ¨å›´ç»•çºµå‘ç³»ç»Ÿç ”ç©¶å»ºç«‹å­¦æœ¯ç¤¾åŒºï¼Œå¹¶æ¨åŠ¨è¯¥æ–¹æ³•åœ¨ UIST å·¥å…·çš„è®¾è®¡ã€æ„å»ºå’Œè¯„ä¼°ä¸­å¾—åˆ°æ›´å¹¿æ³›çš„è®¤å¯ä¸åº”ç”¨ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted workshop proposal @ UIST 2025 Busan, Korea. Workshop website: https://longitudinal-workshop.github.io/",
      "pdf_url": "https://arxiv.org/pdf/2508.10252v1",
      "published_date": "2025-08-14 00:38:23 UTC",
      "updated_date": "2025-08-14 00:38:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T11:58:21.995474+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 139,
  "processed_papers_count": 139,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-24T11:59:15.336634+00:00"
}
{
  "date": "2025-06-15",
  "category": "cs.AI",
  "summary": "ä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„ arXiv è®ºæ–‡é¢†è¯»å‘˜ã€‚\n\næ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-06-15 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\n**ä»Šæ—¥æ€»ç»“ï¼š**\nä»Šå¤©çš„ arXiv åˆæ˜¯â€œç¥ä»™æ‰“æ¶â€çš„ä¸€å¤©ã€‚**åä¸ºè¯ºäºšæ–¹èˆŸå®éªŒå®¤**æŠ›å‡ºäº†é‡ç£…çš„ **CloudMatrix384** æ¶æ„ï¼Œä¸“ä¸º DeepSeek-R1 è¿™ç§å¤§æ¨¡å‹æœåŠ¡è¿›è¡Œè½¯ç¡¬ååŒä¼˜åŒ–ï¼›**Test-time Compute Scaling**ï¼ˆæµ‹è¯•æ—¶è®¡ç®—æ‰©å±•ï¼‰ä¾ç„¶æ˜¯ç»å¯¹çš„ä¸»æµè¯é¢˜ï¼Œå¤šç¯‡æ–‡ç« æ¢è®¨äº†å¦‚ä½•æ›´èªæ˜åœ°åˆ†é…æ¨ç†ç®—åŠ›ï¼›æ­¤å¤–ï¼Œ**RAG** å¼€å§‹èµ°å‘â€œç˜¦èº«â€å’Œâ€œå»å›¾åŒ–â€ï¼Œè€Œæœºå™¨äººé¢†åŸŸå‡ºç°äº†ä¸€ä¸ªèƒ½æ‰“â€œåŠŸå¤«â€çš„ **KungfuBot**ã€‚\n\nä¸‹é¢æˆ‘ä»¬ç›´å…¥ä¸»é¢˜ï¼Œçœ‹çœ‹ä»Šå¤©å€¼å¾—å…³æ³¨çš„ç¡¬æ ¸ç ”ç©¶ã€‚\n\n---\n\n### ğŸš€ åŸºç¡€è®¾æ–½ä¸å¤§æ¨¡å‹ç³»ç»Ÿ (Infrastructure & Systems)\n\n**1. [åä¸º] Serving Large Language Models on Huawei CloudMatrix384**\n> **åä¸º CloudMatrix384 ä¸Šçš„å¤§è¯­è¨€æ¨¡å‹æœåŠ¡**\n> *Paper 56*\n\nè¿™ç»å¯¹æ˜¯ä»Šå¤©çš„é‡å¤´æˆã€‚éšç€æ¨¡å‹å‚æ•°ï¼ˆå¦‚ MoEï¼‰å’Œä¸Šä¸‹æ–‡é•¿åº¦çš„æš´å¢ï¼Œä¼ ç»Ÿ AI é›†ç¾¤é¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚åä¸ºä»‹ç»äº† **CloudMatrix384** è¶…èŠ‚ç‚¹æ¶æ„ï¼Œé›†æˆ 384 ä¸ª Ascend 910 NPU å’Œ 192 ä¸ª Kunpeng CPUã€‚\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šæå‡ºäº†ä¸€ç§åä¸º **CloudMatrix-Infer** çš„æœåŠ¡æ–¹æ¡ˆï¼Œé‡‡ç”¨äº† P2P æœåŠ¡æ¶æ„ç‹¬ç«‹æ‰©å±• Prefillã€Decode å’Œ Cachingã€‚\n*   **äº®ç‚¹**ï¼šé’ˆå¯¹ **DeepSeek-R1** è¿›è¡Œäº†è¯„ä¼°ï¼Œå®ç°äº† 6,688 tokens/s/NPU çš„ Prefill ååé‡å’Œ <50ms çš„ TPOTï¼ˆæ¯ä¸ª token è¾“å‡ºæ—¶é—´ï¼‰ã€‚è¿™æ˜¯ç›®å‰ä¸šç•Œéå¸¸å‰æ²¿çš„è½¯ç¡¬ä»¶ååŒè®¾è®¡å‚è€ƒã€‚\n\n**2. [æ•ˆç‡ä¼˜åŒ–] GTA: Grouped-head latenT Attention**\n> **GTAï¼šåˆ†ç»„å¤´æ½œåœ¨æ³¨æ„åŠ›æœºåˆ¶**\n> *Paper 45*\n\næ³¨æ„åŠ›æœºåˆ¶çš„ KV Cache æ˜¯æ˜¾å­˜æ€æ‰‹ã€‚è¿™ç¯‡è®ºæ–‡å‘ç°æ³¨æ„åŠ›å›¾åœ¨ä¸åŒ Head ä¹‹é—´æœ‰é«˜åº¦ç›¸ä¼¼æ€§ã€‚\n*   **æ–¹æ³•**ï¼šæå‡ºäº† **GTA**ï¼Œä¸€æ–¹é¢å¤ç”¨æ³¨æ„åŠ›åˆ†æ•°ï¼ˆå‡å°‘ Key cacheï¼‰ï¼Œå¦ä¸€æ–¹é¢é€šè¿‡éçº¿æ€§è§£ç å™¨å°† Value cache å‹ç¼©åˆ°æ½œåœ¨ç©ºé—´ã€‚\n*   **æ•ˆæœ**ï¼šç›¸æ¯” GQAï¼ˆGrouped-Query Attentionï¼‰ï¼ŒFLOPs å‡å°‘ 62.5%ï¼ŒKV cache å‡å°‘ 70%ï¼Œæ¨ç†é€Ÿåº¦æå‡ 2 å€ã€‚è¿™å¯¹äºåœ¨èµ„æºå—é™è®¾å¤‡ä¸Šéƒ¨ç½² LLM éå¸¸æœ‰æ„ä¹‰ã€‚\n\n---\n\n### ğŸ§  æ¨ç†ã€Scaling ä¸ Agent (Reasoning & Agents)\n\n**3. [æ¨ç† Scaling] Scaling Test-time Compute for LLM Agents**\n> **æ‰©å±• LLM Agent çš„æµ‹è¯•æ—¶è®¡ç®—**\n> *Paper 10*\n\nTest-time computeï¼ˆæµ‹è¯•æ—¶è®¡ç®—ï¼‰æœ€è¿‘å¤§ç«ï¼Œä½†è¿™ç¯‡è®ºæ–‡é¦–æ¬¡ç³»ç»Ÿåœ°å°†å…¶åº”ç”¨äº **Language Agents**ã€‚\n*   **å‘ç°**ï¼š\n    1. æ‰©å±•æµ‹è¯•æ—¶è®¡ç®—ç¡®å®èƒ½æå‡ Agent æ€§èƒ½ã€‚\n    2. Agent éœ€è¦çŸ¥é“â€œä½•æ—¶åæ€â€ï¼ˆReflectï¼‰ã€‚\n    3. List-wise çš„éªŒè¯å’Œç»“æœåˆå¹¶æ–¹æ³•æ•ˆæœæœ€å¥½ã€‚\n    4. å¢åŠ  Rollout çš„å¤šæ ·æ€§å¯¹æ€§èƒ½æœ‰æ­£å‘å½±å“ã€‚\n\n*åŒç±»æ¨è*ï¼š\n*   **Paper 49 (SPECS)**: åˆ©ç”¨**æŠ•æœºè§£ç ï¼ˆSpeculative Decodingï¼‰**çš„æ€æƒ³ï¼Œç”¨å°æ¨¡å‹ç”Ÿæˆè‰ç¨¿ï¼Œå¤§æ¨¡å‹éªŒè¯ï¼Œåœ¨ä¿æŒç²¾åº¦çš„åŒæ—¶é™ä½äº†å»¶è¿Ÿã€‚\n*   **Paper 54 (Strategic Scaling)**: æŠŠè®¡ç®—åˆ†é…çœ‹ä½œä¸€ä¸ª**å¤šè‡‚è€è™æœºï¼ˆBanditï¼‰**é—®é¢˜ï¼ŒåŠ¨æ€è¯„ä¼° Query éš¾åº¦ï¼ŒæŠŠå¥½é’¢ç”¨åœ¨åˆ€åˆƒä¸Šï¼ˆéš¾çš„ Query ç»™æ›´å¤šç®—åŠ›ï¼‰ã€‚\n\n**4. [ç»“æ„åŒ–æ¨ç†] STRuCT-LLM: Unifying Tabular and Graph Reasoning with Reinforcement Learning**\n> **STRuCT-LLMï¼šåˆ©ç”¨å¼ºåŒ–å­¦ä¹ ç»Ÿä¸€è¡¨æ ¼å’Œå›¾æ¨ç†**\n> *Paper 1*\n\nè¡¨æ ¼ï¼ˆSQLï¼‰å’Œå›¾ï¼ˆCypherï¼‰é€šå¸¸æ˜¯åˆ†å¼€ç ”ç©¶çš„ã€‚è¿™ç¯‡è®ºæ–‡é€šè¿‡ RL å’Œ CoT æŠŠä¸¤è€…ç»Ÿä¸€èµ·æ¥ã€‚\n*   **åˆ›æ–°**ï¼šåˆ©ç”¨ SQL å’Œ Cypher çš„å…±äº«æŠ½è±¡è¿›è¡Œè¿ç§»å­¦ä¹ ã€‚å³ä½¿æ²¡æœ‰å…±äº« Schemaï¼Œè®­ç»ƒ SQL ä¹Ÿèƒ½æå‡ Cypher çš„æ€§èƒ½ï¼Œåä¹‹äº¦ç„¶ã€‚\n*   **ç»“æœ**ï¼šæœ€å¤§çš„æ¨¡å‹ QwQ-32B åœ¨ Spider (SQL) ä¸Šæå‡ 13.5%ï¼Œåœ¨ Text2Cypher ä¸ŠæƒŠäººåœ°æå‡äº† 73.1%ã€‚\n\n**5. [åç›´è§‰] Can LLMs Reconcile Knowledge Conflicts in Counterfactual Reasoning**\n> **LLM èƒ½åœ¨åäº‹å®æ¨ç†ä¸­è°ƒå’ŒçŸ¥è¯†å†²çªå—ï¼Ÿ**\n> *Paper 64*\n\næˆ‘ä»¬å¸¸è®¤ä¸º LLM åªè¦ Context é‡Œç»™äº†æ–°è®¾å®šï¼Œå°±èƒ½æŒ‰è®¾å®šæ¨ç†ã€‚ä½†è¿™é¡¹ç ”ç©¶æ³¼äº†å†·æ°´ï¼šLLM åœ¨**åäº‹å®æ¨ç†**ï¼ˆCounterfactual Reasoningï¼‰ä¸­ï¼Œç»å¸¸æ— æ³•æ‘†è„±é¢„è®­ç»ƒçŸ¥è¯†ï¼ˆParametric Knowledgeï¼‰çš„é¡½å›ºå½±å“ï¼Œç®€å•çš„å¾®è°ƒç”šè‡³ä¼šç ´ååŸæœ‰çš„çŸ¥è¯†ã€‚\n\n---\n\n### ğŸ“š RAG ä¸æ£€ç´¢å¢å¼º (RAG & Retrieval)\n\n**6. [RAGç˜¦èº«] SlimRAG: Retrieval without Graphs via Entity-Aware Context Selection**\n> **SlimRAGï¼šé€šè¿‡å®ä½“æ„ŸçŸ¥ä¸Šä¸‹æ–‡é€‰æ‹©å®ç°æ— å›¾æ£€ç´¢**\n> *Paper 20*\n\nç°åœ¨çš„ GraphRAG å¾ˆç«ï¼Œä½†æ„å»ºå›¾è°±å’Œéå†éƒ½å¾ˆé‡ã€‚SlimRAG è¯´ï¼šæˆ‘ä»¬ä¸éœ€è¦å›¾ã€‚\n*   **æ–¹æ³•**ï¼šç”¨â€œå®ä½“-å—â€ï¼ˆEntity-to-Chunkï¼‰è¡¨ä»£æ›¿å¤æ‚çš„å›¾ç»“æ„ã€‚æŸ¥è¯¢æ—¶è¯†åˆ«å…³é”®å®ä½“ï¼Œç›´æ¥æ£€ç´¢ç›¸å…³å—ã€‚\n*   **æ•ˆæœ**ï¼šæ¯” Graph-based RAG æ›´å‡†ï¼Œä¸”ç´¢å¼•å¤§å°å’Œæ£€ç´¢å†…å®¹æ›´ç´§å‡‘ï¼ˆRITU æŒ‡æ ‡å¤§å¹…é™ä½ï¼‰ã€‚è¿™æ˜¯ä¸€ç§â€œåå…¶é“è€Œè¡Œä¹‹â€çš„é«˜æ•ˆæ€è·¯ã€‚\n\n**7. [ç¥ç»ç¬¦å·] SymRAG: Efficient Neuro-Symbolic Retrieval Through Adaptive Query Routing**\n> **SymRAGï¼šåŸºäºè‡ªé€‚åº”æŸ¥è¯¢è·¯ç”±çš„é«˜æ•ˆç¥ç»ç¬¦å·æ£€ç´¢**\n> *Paper 2*\n\nå¹¶ä¸æ˜¯æ‰€æœ‰ Query éƒ½éœ€è¦æ˜‚è´µçš„ç¥ç»ç½‘ç»œå¤„ç†ã€‚SymRAG å¼•å…¥äº†ä¸€ä¸ª**è·¯ç”±å™¨**ï¼Œæ ¹æ®æŸ¥è¯¢å¤æ‚åº¦å’Œç³»ç»Ÿè´Ÿè½½ï¼ŒåŠ¨æ€å†³å®šèµ°â€œç¬¦å·è·¯â€ï¼ˆSymbolicï¼‰ã€â€œç¥ç»è·¯â€ï¼ˆNeuralï¼‰è¿˜æ˜¯â€œæ··åˆè·¯â€ã€‚\n*   **ç»“æœ**ï¼šCPU åˆ©ç”¨ç‡æä½ï¼ˆ3.6%-6.2%ï¼‰ï¼Œä¾ç„¶ä¿æŒäº†æé«˜çš„å‡†ç¡®ç‡ã€‚è¿™æ˜¯å…¸å‹çš„å·¥ç¨‹åŒ–ä¼˜åŒ–æ€è·¯ã€‚\n\n---\n\n### ğŸ¤– æœºå™¨äººä¸å…·èº«æ™ºèƒ½ (Robotics)\n\n**8. [NeurIPS 2025] KungfuBot: Physics-Based Humanoid Whole-Body Control for Learning Highly-Dynamic Skills**\n> **KungfuBotï¼šåŸºäºç‰©ç†çš„ç±»äººå…¨èº«æ§åˆ¶ï¼Œç”¨äºå­¦ä¹ é«˜åŠ¨æ€æŠ€èƒ½**\n> *Paper 24*\n\nè¿™ç¯‡è®ºæ–‡éå¸¸æœ‰è¶£ï¼Œæ—¨åœ¨è®©æœºå™¨äººå­¦ä¼šâ€œåŠŸå¤«â€å’Œè·³èˆç­‰é«˜åŠ¨æ€åŠ¨ä½œã€‚\n*   **éš¾ç‚¹**ï¼šç°æœ‰ç®—æ³•åªèƒ½åšæ…¢é€Ÿã€å¹³æ»‘çš„åŠ¨ä½œã€‚\n*   **æ–¹æ³•**ï¼šè®¾è®¡äº†ä¸€å¥—åŠ¨ä½œå¤„ç†æµæ°´çº¿ï¼ˆæå–ã€è¿‡æ»¤ã€é‡å®šå‘ï¼‰åŠ ä¸Šéå¯¹ç§°çš„ Actor-Critic æ¡†æ¶ã€‚\n*   **Demo**ï¼šå·²åœ¨ Unitree G1 æœºå™¨äººä¸Šéƒ¨ç½²ï¼Œå±•ç¤ºäº†éå¸¸ç¨³å®šçš„é«˜åŠ¨æ€è¡Œä¸ºã€‚\n\n**9. [è§†è§‰-è¯­è¨€-åŠ¨ä½œ] SP-VLA: A Joint Model Scheduling and Token Pruning Approach for VLA Model Acceleration**\n> **SP-VLAï¼šä¸€ç§ç”¨äº VLA æ¨¡å‹åŠ é€Ÿçš„è”åˆæ¨¡å‹è°ƒåº¦ä¸ Token å‰ªææ–¹æ³•**\n> *Paper 53*\n\nVLA æ¨¡å‹ï¼ˆå¦‚ç”¨äºæœºå™¨äººçš„å¤§æ¨¡å‹ï¼‰å¤ªæ…¢äº†ï¼Œæ— æ³•å®æ—¶æ§åˆ¶ã€‚\n*   **ä»¿ç”Ÿæ€è·¯**ï¼šæ¨¡ä»¿äººç±»ï¼Œå°†åŠ¨ä½œåˆ†ä¸ºâ€œæ·±æ€ç†Ÿè™‘â€ï¼ˆDeliberativeï¼‰å’Œâ€œç›´è§‰â€ï¼ˆIntuitiveï¼‰ã€‚å‰è€…ç”¨å¤§ VLA æ¨¡å‹ï¼Œåè€…ç”¨è½»é‡çº§ç”Ÿæˆå™¨ã€‚é…åˆ Token å‰ªæï¼Œé€Ÿåº¦æå‡æ˜¾è‘—ã€‚\n\n---\n\n### ğŸ›¡ï¸ å®‰å…¨ä¸å¯¹é½ (Safety & Alignment)\n\n**10. [ç†è®ºç»Ÿä¸€] Implicit Reward as the Bridge: A Unified View of SFT and DPO Connections**\n> **éšå¼å¥–åŠ±ä½œä¸ºæ¡¥æ¢ï¼šSFT å’Œ DPO è¿æ¥çš„ç»Ÿä¸€è§†è§’**\n> *Paper 51*\n\nè¿™ç¯‡è®ºæ–‡ä»ç†è®ºä¸Šè¯æ˜äº† SFTï¼ˆç›‘ç£å¾®è°ƒï¼‰å…¶å®æ˜¯ DPOï¼ˆç›´æ¥åå¥½ä¼˜åŒ–ï¼‰çš„ä¸€ç§ç‰¹æ®Šæƒ…å†µã€‚\n*   **å‘ç°**ï¼šä¼ ç»Ÿ SFT ä¸­ KL æ•£åº¦é¡¹å¤±æ•ˆï¼Œå¯¼è‡´çº¦æŸä¸è¶³ã€‚ä½œè€…æå‡ºäº†ä¸€ç§ç®€å•çš„å­¦ä¹ ç‡è¡°å‡ç­–ç•¥ï¼Œåœ¨æŒ‡ä»¤éµå¾ªä»»åŠ¡ä¸Šæå‡äº† 25% çš„æ€§èƒ½ã€‚\n\n**11. [è¶Šç‹±æ”»å‡»] Alphabet Index Mapping: Jailbreaking LLMs through Semantic Dissimilarity**\n> **å­—æ¯ç´¢å¼•æ˜ å°„ï¼šé€šè¿‡è¯­ä¹‰ä¸ç›¸ä¼¼æ€§è¶Šç‹± LLM**\n> *Paper 63*\n\nä¸€ç§æ–°çš„è¶Šç‹±æ‰‹æ®µï¼ˆAIMï¼‰ã€‚\n*   **åŸç†**ï¼šåˆ©ç”¨è¯­ä¹‰ä¸ç›¸ä¼¼æ€§ï¼ˆSemantic Dissimilarityï¼‰æ¥ç»•è¿‡é˜²å¾¡ï¼ŒåŒæ—¶ä¿æŒç®€å•çš„è§£ç èƒ½åŠ›ã€‚è¿™ç§æ”»å‡»åœ¨ GPT-4 ä¸Šå®ç°äº† 94% çš„æ”»å‡»æˆåŠŸç‡ã€‚\n\n---\n\n### ğŸ› ï¸ æœ‰è¶£çš„å·¥å…·ä¸åº”ç”¨\n\n*   **[ç§‘ç ”è¾…åŠ©] SciSage (Paper 62)**: ä¸€ä¸ªå¤š Agent æ¡†æ¶ï¼Œä¸“é—¨ç”¨æ¥å†™**ç§‘å­¦ç»¼è¿°**ã€‚å®ƒé‡‡ç”¨â€œè¾¹å†™è¾¹åæ€â€ï¼ˆreflect-when-you-writeï¼‰çš„æ¨¡å¼ã€‚è™½ç„¶åœ¨äººç±»è¯„ä¼°ä¸­äº’æœ‰èƒœè´Ÿï¼Œä½†åœ¨å¼•ç”¨å‡†ç¡®æ€§å’Œå¹¿åº¦ä¸Šè¡¨ç°å‡ºè‰²ã€‚æ•™æˆä»¬ï¼Œä¹Ÿè®¸ä»¥åæ–‡çŒ®ç»¼è¿°å¯ä»¥è®©å®ƒå…ˆæ‰“ä¸ªè‰ç¨¿ï¼Ÿ\n*   **[æ¸¸æˆåšå¼ˆ] Mastering Da Vinci Code (Paper 38)**: åœ¨ã€Šè¾¾èŠ¬å¥‡å¯†ç ã€‹è¿™ç§æ¨ç†æ¸¸æˆä¸­ï¼ŒåŸºäº PPO çš„å¼ºåŒ–å­¦ä¹  Agent å‡»è´¥äº†åŒ…æ‹¬ Gemini å’Œ GPT åœ¨å†…çš„ LLM Agentã€‚è¯´æ˜åœ¨çº¯é€»è¾‘æ¼”ç»å’Œéšè—ä¿¡æ¯åšå¼ˆä¸Šï¼ŒRL ä¾ç„¶æ˜¯ç‹è€…ã€‚\n\n---\n\n**Professor Gemini çš„ç‚¹è¯„ï¼š**\nä»Šå¤©æœ€ä»¤æˆ‘å°è±¡æ·±åˆ»çš„æ˜¯ **STRuCT-LLM** å’Œ **Huawei CloudMatrix**ã€‚å‰è€…å±•ç¤ºäº†å¦‚ä½•æ‰“ç ´ç»“æ„åŒ–æ•°æ®ï¼ˆSQLï¼‰å’Œå›¾æ•°æ®ï¼ˆCypherï¼‰ä¹‹é—´çš„å£å’ï¼Œè¿™æ˜¯ä¼ä¸šçº§åº”ç”¨éå¸¸ç—›ç‚¹çš„éœ€æ±‚ï¼›åè€…åˆ™å±•ç¤ºäº†å½“æ¨¡å‹è§„æ¨¡å¤§åˆ°ä¸€å®šç¨‹åº¦åï¼Œåº•å±‚çš„ç®—åŠ›æ¶æ„å¿…é¡»é‡æ„ä»¥é€‚åº” MoE å’Œè¶…é•¿ Contextã€‚\n\nè¿™å°±æ˜¯ä»Šå¤©çš„ arXiv å¿«æŠ¥ï¼Œå¸Œæœ›å¯¹ä½ çš„ç ”ç©¶æœ‰æ‰€å¯å‘ã€‚æˆ‘ä»¬æ˜å¤©è§ï¼",
  "papers": [
    {
      "arxiv_id": "2506.21575v1",
      "title": "STRuCT-LLM: Unifying Tabular and Graph Reasoning with Reinforcement Learning for Semantic Parsing",
      "title_zh": "STRuCT-LLMï¼šç»“åˆå¼ºåŒ–å­¦ä¹ ç»Ÿä¸€è¡¨æ ¼ä¸å›¾æ¨ç†çš„è¯­ä¹‰è§£æ",
      "authors": [
        "Josefa Lia Stoisser",
        "Marc Boubnovski Martell",
        "Lawrence Phillips",
        "Casper Hansen",
        "Julien Fauqueur"
      ],
      "abstract": "We propose STRuCT-LLM, a unified framework for training large language models (LLMs) to perform structured reasoning over both relational and graph-structured data. Our approach jointly optimizes Text-to-SQL and Text-to-Cypher tasks using reinforcement learning (RL) combined with Chain-of-Thought (CoT) supervision. To support fine-grained optimization in graph-based parsing, we introduce a topology-aware reward function based on graph edit distance. Unlike prior work that treats relational and graph formalisms in isolation, STRuCT-LLM leverages shared abstractions between SQL and Cypher to induce cross-formalism transfer, enabling SQL training to improve Cypher performance and vice versa - even without shared schemas. Our largest model (QwQ-32B) achieves substantial relative improvements across tasks: on semantic parsing, Spider improves by 13.5\\% and Text2Cypher by 73.1\\%. The model also demonstrates strong zero-shot generalization, improving performance on downstream tabular QA (TableBench: 8.5\\%) and knowledge graph QA (CR-LT-KGQA: 1.7\\%) without any QA-specific supervision. These results demonstrate both the effectiveness of executable queries as scaffolds for structured reasoning and the synergistic benefits of jointly training on SQL and Cypher (code available at https://github.com/bouv/STRuCT-LLM).",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† STRuCT-LLMï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹ (LLMs) å¯¹å…³ç³»å‹æ•°æ®å’Œå›¾ç»“æ„æ•°æ®è¿›è¡Œç»“æ„åŒ–æ¨ç†çš„ç»Ÿä¸€æ¡†æ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡ç»“åˆå¼ºåŒ–å­¦ä¹  (Reinforcement Learning) ä¸é“¾å¼æ€ç»´ (Chain-of-Thought) ç›‘ç£ï¼ŒååŒä¼˜åŒ–äº† Text-to-SQL å’Œ Text-to-Cypher ä»»åŠ¡ã€‚ä¸ºäº†åœ¨å›¾è§£æä¸­å®ç°ç»†ç²’åº¦ä¼˜åŒ–ï¼Œç ”ç©¶è€…å¼•å…¥äº†ä¸€ç§åŸºäºå›¾ç¼–è¾‘è·ç¦» (graph edit distance) çš„æ‹“æ‰‘æ„ŸçŸ¥å¥–åŠ±å‡½æ•°ã€‚STRuCT-LLM åˆ©ç”¨ SQL å’Œ Cypher ä¹‹é—´çš„å…±äº«æŠ½è±¡å®ç°äº†è·¨å½¢å¼è¿ç§»ï¼Œä½¿å¾—ä¸¤ç§è¯­è¨€çš„è®­ç»ƒèƒ½å¤Ÿäº’ä¿ƒæå‡ï¼Œå³ä½¿åœ¨æ²¡æœ‰å…±äº«æ¨¡å¼çš„æƒ…å†µä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨ Spider æ•°æ®é›†ä¸Šæå‡äº† 13.5%ï¼Œåœ¨ Text2Cypher ä»»åŠ¡ä¸Šæ›´æ˜¯å®ç°äº† 73.1% çš„æ˜¾è‘—å¢é•¿ã€‚æ­¤å¤–ï¼Œæ¨¡å‹åœ¨è¡¨æ ¼é—®ç­” (TableBench) å’ŒçŸ¥è¯†å›¾è°±é—®ç­” (CR-LT-KGQA) ä»»åŠ¡ä¸­å±•ç°äº†å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚è¿™é¡¹ç ”ç©¶è¯æ˜äº†å¯æ‰§è¡ŒæŸ¥è¯¢ä½œä¸ºç»“æ„åŒ–æ¨ç†æ”¯æ¶çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æ­ç¤ºäº† SQL ä¸ Cypher è”åˆè®­ç»ƒå¸¦æ¥çš„æ˜¾è‘—ååŒæ”¶ç›Šã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21575v1",
      "published_date": "2025-06-15 22:40:36 UTC",
      "updated_date": "2025-06-15 22:40:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:06:13.007716+00:00"
    },
    {
      "arxiv_id": "2506.12981v2",
      "title": "SymRAG: Efficient Neuro-Symbolic Retrieval Through Adaptive Query Routing",
      "title_zh": "SymRAGï¼šåŸºäºè‡ªé€‚åº”æŸ¥è¯¢è·¯ç”±çš„é«˜æ•ˆç¥ç»ç¬¦å·æ£€ç´¢",
      "authors": [
        "Safayat Bin Hakim",
        "Muhammad Adil",
        "Alvaro Velasquez",
        "Houbing Herbert Song"
      ],
      "abstract": "Current Retrieval-Augmented Generation systems use uniform processing, causing inefficiency as simple queries consume resources similar to complex multi-hop tasks. We present SymRAG, a framework that introduces adaptive query routing via real-time complexity and load assessment to select symbolic, neural, or hybrid pathways. SymRAG's neuro-symbolic approach adjusts computational pathways based on both query characteristics and system load, enabling efficient resource allocation across diverse query types. By combining linguistic and structural query properties with system load metrics, SymRAG allocates resources proportional to reasoning requirements. Evaluated on 2,000 queries across HotpotQA (multi-hop reasoning) and DROP (discrete reasoning) using Llama-3.2-3B and Mistral-7B models, SymRAG achieves competitive accuracy (97.6--100.0% exact match) with efficient resource utilization (3.6--6.2% CPU utilization, 0.985--3.165s processing). Disabling adaptive routing increases processing time by 169--1151%, showing its significance for complex models. These results suggest adaptive computation strategies are more sustainable and scalable for hybrid AI systems that use dynamic routing and neuro-symbolic frameworks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SymRAGï¼Œä¸€ç§é€šè¿‡è‡ªé€‚åº”æŸ¥è¯¢è·¯ç”±(Adaptive Query Routing)å®ç°çš„ç¥ç»ç¬¦å·æ£€ç´¢æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ£€ç´¢å¢å¼ºç”Ÿæˆ(Retrieval-Augmented Generation, RAG)ç³»ç»Ÿå› é‡‡ç”¨ç»Ÿä¸€å¤„ç†æ–¹å¼è€Œå¯¼è‡´çš„èµ„æºåˆ†é…ä½æ•ˆé—®é¢˜ã€‚SymRAG å¼•å…¥äº†å®æ—¶å¤æ‚åº¦å’Œè´Ÿè½½è¯„ä¼°æœºåˆ¶ï¼Œèƒ½å¤Ÿæ ¹æ®æŸ¥è¯¢ç‰¹å¾å’Œç³»ç»Ÿè´Ÿè½½åœ¨ç¬¦å·(symbolic)ã€ç¥ç»(neural)æˆ–æ··åˆ(hybrid)è·¯å¾„ä¹‹é—´è¿›è¡ŒåŠ¨æ€é€‰æ‹©ã€‚è¯¥æ¡†æ¶ç»“åˆäº†æŸ¥è¯¢çš„è¯­è¨€ä¸ç»“æ„å±æ€§ä»¥åŠç³»ç»Ÿè´Ÿè½½æŒ‡æ ‡ï¼Œç¡®ä¿èµ„æºåˆ†é…ä¸æ¨ç†éœ€æ±‚ç›¸åŒ¹é…ã€‚åœ¨ HotpotQA å’Œ DROP æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSymRAG é…åˆ Llama-3.2-3B å’Œ Mistral-7B æ¨¡å‹å®ç°äº† 97.6%--100.0% çš„ç²¾ç¡®åŒ¹é…ç‡ï¼ŒåŒæ—¶ä¿æŒäº†æä½çš„ CPU å ç”¨ç‡å’Œå¤„ç†å»¶è¿Ÿã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œç¦ç”¨è‡ªé€‚åº”è·¯ç”±ä¼šä½¿å¤„ç†æ—¶é—´å¢åŠ  169%--1151%ï¼Œè¯æ˜äº†è¯¥ç­–ç•¥å¯¹äºæ„å»ºå¯æŒç»­ä¸”å¯æ‰©å±•çš„æ··åˆäººå·¥æ™ºèƒ½ç³»ç»Ÿå…·æœ‰é‡è¦æ„ä¹‰ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at 19th International Conference on Neurosymbolic Learning and Reasoning (NeSy 2025)",
      "pdf_url": "https://arxiv.org/pdf/2506.12981v2",
      "published_date": "2025-06-15 22:35:43 UTC",
      "updated_date": "2025-07-12 05:28:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:07:09.302918+00:00"
    },
    {
      "arxiv_id": "2506.12965v3",
      "title": "Distributional Training Data Attribution: What do Influence Functions Sample?",
      "title_zh": "åˆ†å¸ƒå¼è®­ç»ƒæ•°æ®å½’å› ï¼šå½±å“å‡½æ•°ç©¶ç«Ÿåœ¨é‡‡æ ·ä»€ä¹ˆï¼Ÿ",
      "authors": [
        "Bruno Mlodozeniec",
        "Isaac Reid",
        "Sam Power",
        "David Krueger",
        "Murat Erdogdu",
        "Richard E. Turner",
        "Roger Grosse"
      ],
      "abstract": "Randomness is an unavoidable part of training deep learning models, yet something that traditional training data attribution algorithms fail to rigorously account for. They ignore the fact that, due to stochasticity in the initialisation and batching, training on the same dataset can yield different models. In this paper, we address this shortcoming through introducing distributional training data attribution (d-TDA), the goal of which is to predict how the distribution of model outputs (over training runs) depends upon the dataset. Intriguingly, we find that influence functions (IFs), a popular data attribution tool, are 'secretly distributional': they emerge from our framework as the limit to unrolled differentiation, without requiring restrictive convexity assumptions. This provides a new perspective on the effectiveness of IFs in deep learning. We demonstrate the practical utility of d-TDA in experiments, including improving data pruning for vision transformers and identifying influential examples with diffusion models.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ·±åº¦å­¦ä¹ è®­ç»ƒä¸­çš„éšæœºæ€§å¯¹è®­ç»ƒæ•°æ®å½’å› (Training Data Attribution)çš„å½±å“ï¼ŒæŒ‡å‡ºä¼ ç»Ÿç®—æ³•å¾€å¾€å¿½ç•¥äº†åˆå§‹åŒ–å’Œæ‰¹å¤„ç†éšæœºæ€§å¯¼è‡´çš„æ¨¡å‹å·®å¼‚ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†åˆ†å¸ƒè®­ç»ƒæ•°æ®å½’å› (Distributional Training Data Attribution, d-TDA)ï¼Œæ—¨åœ¨é¢„æµ‹æ¨¡å‹è¾“å‡ºåˆ†å¸ƒå¦‚ä½•éšæ•°æ®é›†å˜åŒ–ã€‚ç ”ç©¶å‘ç°ï¼Œæµè¡Œçš„å½’å› å·¥å…·å½±å“å‡½æ•°(Influence Functions, IFs)æœ¬è´¨ä¸Šå…·æœ‰â€œåˆ†å¸ƒç‰¹æ€§â€ï¼Œå®ƒæ˜¯å±•å¼€å¾®åˆ†(Unrolled Differentiation)çš„æé™ä¸”æ— éœ€å¼ºåŠ å‡¸æ€§å‡è®¾ï¼Œè¿™ä¸ºç†è§£å…¶åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§æä¾›äº†å…¨æ–°è§†è§’ã€‚å®éªŒè¯æ˜ï¼Œd-TDA åœ¨ä¼˜åŒ–è§†è§‰å˜æ¢å™¨(Vision Transformers)çš„æ•°æ®å‰ªæ(Data Pruning)ä»¥åŠè¯†åˆ«æ‰©æ•£æ¨¡å‹(Diffusion Models)ä¸­çš„å…³é”®å½±å“æ ·æœ¬æ–¹é¢å…·æœ‰æ˜¾è‘—çš„å®é™…åº”ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.12965v3",
      "published_date": "2025-06-15 21:02:36 UTC",
      "updated_date": "2025-10-25 12:43:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:07:14.084492+00:00"
    },
    {
      "arxiv_id": "2506.12963v2",
      "title": "Reasoning Model Unlearning: Forgetting Traces, Not Just Answers, While Preserving Reasoning Skills",
      "title_zh": "æ¨ç†æ¨¡å‹æœºå™¨é—å¿˜ï¼šåœ¨ä¿ç•™æ¨ç†èƒ½åŠ›çš„åŒæ—¶é—å¿˜æ¨ç†è½¨è¿¹è€Œéä»…æ¶ˆé™¤ç­”æ¡ˆ",
      "authors": [
        "Changsheng Wang",
        "Chongyu Fan",
        "Yihua Zhang",
        "Jinghan Jia",
        "Dennis Wei",
        "Parikshit Ram",
        "Nathalie Baracaldo",
        "Sijia Liu"
      ],
      "abstract": "Recent advances in large reasoning models (LRMs) have enabled strong chain-of-thought (CoT) generation through test-time computation. While these multi-step reasoning capabilities represent a major milestone in language model performance, they also introduce new safety risks. In this work, we present the first systematic study to revisit the problem of machine unlearning in the context of LRMs. Machine unlearning refers to the process of removing the influence of sensitive, harmful, or undesired data or knowledge from a trained model without full retraining. We show that conventional unlearning algorithms, originally designed for non-reasoning models, are inadequate for LRMs. In particular, even when final answers are successfully erased, sensitive information often persists within the intermediate reasoning steps, i.e., CoT trajectories. To address this challenge, we extend conventional unlearning and propose Reasoning-aware Representation Misdirection for Unlearning ($R^2MU$), a novel method that effectively suppresses sensitive reasoning traces and prevents the generation of associated final answers, while preserving the model's reasoning ability. Our experiments demonstrate that $R^2MU$ significantly reduces sensitive information leakage within reasoning traces and achieves strong performance across both safety and reasoning benchmarks, evaluated on state-of-the-art models such as DeepSeek-R1-Distill-LLaMA-8B and DeepSeek-R1-Distill-Qwen-14B.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰ç³»ç»Ÿåœ°æ¢è®¨äº†æœºå™¨å¸è½½ï¼ˆMachine unlearningï¼‰é—®é¢˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿå¸è½½ç®—æ³•æ— æ³•å½»åº•æ¸…é™¤é“¾å¼æ€ç»´ï¼ˆChain-of-Thoughtï¼‰è½¨è¿¹ä¸­æ½œè—çš„æ•æ„Ÿä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œä½œè€…æå‡ºäº†Reasoning-aware Representation Misdirection for Unlearning ($R^2MU$)ï¼Œè¿™æ˜¯ä¸€ç§èƒ½å¤Ÿæœ‰æ•ˆæŠ‘åˆ¶æ•æ„Ÿæ¨ç†ç—•è¿¹å¹¶é˜»æ­¢ç”Ÿæˆç›¸å…³æœ€ç»ˆç­”æ¡ˆçš„æ–°å‹æ–¹æ³•ã€‚è¯¥æ–¹æ³•åœ¨æ¶ˆé™¤ç‰¹å®šæ•æ„ŸçŸ¥è¯†çš„åŒæ—¶ï¼ŒæˆåŠŸä¿ç•™äº†æ¨¡å‹æ•´ä½“çš„æ¨ç†æŠ€èƒ½ã€‚å®éªŒåœ¨DeepSeek-R1-Distill-LLaMA-8Bå’ŒDeepSeek-R1-Distill-Qwen-14Bç­‰å‰æ²¿æ¨¡å‹ä¸Šå±•å¼€ï¼Œç»“æœè¡¨æ˜$R^2MU$èƒ½æ˜¾è‘—é™ä½æ¨ç†è½¨è¿¹ä¸­çš„æ•æ„Ÿä¿¡æ¯æ³„éœ²é£é™©ï¼Œå¹¶åœ¨å®‰å…¨åˆè§„æ€§ä¸æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†ä¼˜å¼‚è¡¨ç°ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by EMNLP 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.12963v2",
      "published_date": "2025-06-15 20:54:23 UTC",
      "updated_date": "2025-10-10 19:19:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:07:18.693298+00:00"
    },
    {
      "arxiv_id": "2506.12953v1",
      "title": "Forecasting Time Series with LLMs via Patch-Based Prompting and Decomposition",
      "title_zh": "åŸºäºåˆ†å—æç¤ºä¸åˆ†è§£çš„å¤§è¯­è¨€æ¨¡å‹æ—¶é—´åºåˆ—é¢„æµ‹",
      "authors": [
        "Mayank Bumb",
        "Anshul Vemulapalli",
        "Sri Harsha Vardhan Prasad Jella",
        "Anish Gupta",
        "An La",
        "Ryan A. Rossi",
        "Hongjie Chen",
        "Franck Dernoncourt",
        "Nesreen K. Ahmed",
        "Yu Wang"
      ],
      "abstract": "Recent advances in Large Language Models (LLMs) have demonstrated new possibilities for accurate and efficient time series analysis, but prior work often required heavy fine-tuning and/or ignored inter-series correlations. In this work, we explore simple and flexible prompt-based strategies that enable LLMs to perform time series forecasting without extensive retraining or the use of a complex external architecture. Through the exploration of specialized prompting methods that leverage time series decomposition, patch-based tokenization, and similarity-based neighbor augmentation, we find that it is possible to enhance LLM forecasting quality while maintaining simplicity and requiring minimal preprocessing of data. To this end, we propose our own method, PatchInstruct, which enables LLMs to make precise and effective predictions.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)è¿›è¡Œæ—¶é—´åºåˆ—é¢„æµ‹çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æŠ€æœ¯é€šå¸¸éœ€è¦æ·±åº¦å¾®è°ƒæˆ–å¿½è§†åºåˆ—é—´ç›¸å…³æ€§çš„å±€é™ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åä¸ºPatchInstructçš„çµæ´»æç¤ºç­–ç•¥ï¼Œä½¿æ¨¡å‹æ— éœ€å¤§è§„æ¨¡é‡æ–°è®­ç»ƒæˆ–å¤æ‚çš„å¤–éƒ¨æ¶æ„å³å¯æ‰§è¡Œé¢„æµ‹ä»»åŠ¡ã€‚è¯¥æ–¹æ³•é€šè¿‡ç»“åˆæ—¶é—´åºåˆ—åˆ†è§£(Decomposition)ã€åˆ†å—æ ‡è®°åŒ–(Patch-based tokenization)ä»¥åŠåŸºäºç›¸ä¼¼æ€§çš„é‚»åŸŸå¢å¼º(Similarity-based neighbor augmentation)ç­‰ä¸“é—¨çš„æç¤ºæ‰‹æ®µï¼Œæ˜¾è‘—æå‡äº†é¢„æµ‹è´¨é‡ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒPatchInstructåœ¨ä¿æŒç³»ç»Ÿç®€æ´æ€§å’Œæœ€å°åŒ–æ•°æ®é¢„å¤„ç†éœ€æ±‚çš„åŒæ—¶ï¼Œæœ‰æ•ˆå¢å¼ºäº†LLMså¯¹æ—¶é—´åºåˆ—æ•°æ®çš„å»ºæ¨¡èƒ½åŠ›ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä½¿å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆç²¾ç¡®ä¸”æœ‰æ•ˆçš„é¢„æµ‹ï¼Œä¸ºé«˜æ•ˆçš„æ—¶é—´åºåˆ—åˆ†ææä¾›äº†æå…·çµæ´»æ€§çš„æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.12953v1",
      "published_date": "2025-06-15 19:42:58 UTC",
      "updated_date": "2025-06-15 19:42:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:06:19.833746+00:00"
    },
    {
      "arxiv_id": "2506.12952v1",
      "title": "Constitutive Components for Human-Like Autonomous Artificial Intelligence",
      "title_zh": "ç±»äººè‡ªä¸»äººå·¥æ™ºèƒ½çš„æ„æˆè¦ç´ ",
      "authors": [
        "Kazunori D Yamada"
      ],
      "abstract": "This study is the first to clearly identify the functions required to construct artificial entities capable of behaving autonomously like humans, and organizes them into a three-layer functional hierarchy. Specifically, it defines three levels: Core Functions, which enable interaction with the external world; the Integrative Evaluation Function, which selects actions based on perception and memory; and the Self Modification Function, which dynamically reconfigures behavioral principles and internal components. Based on this structure, the study proposes a stepwise model of autonomy comprising reactive, weak autonomous, and strong autonomous levels, and discusses its underlying design principles and developmental aspects. It also explores the relationship between these functions and existing artificial intelligence design methods, addressing their potential as a foundation for general intelligence and considering future applications and ethical implications. By offering a theoretical framework that is independent of specific technical methods, this work contributes to a deeper understanding of autonomy and provides a foundation for designing future artificial entities with strong autonomy.",
      "tldr_zh": "æœ¬ç ”ç©¶é¦–æ¬¡æ˜ç¡®æ ‡è¯†äº†æ„å»ºèƒ½å¤Ÿåƒäººç±»ä¸€æ ·è‡ªä¸»è¡ŒåŠ¨çš„äººå·¥å®ä½“æ‰€éœ€çš„å„é¡¹åŠŸèƒ½ï¼Œå¹¶å°†å…¶ç»„ç»‡ä¸ºä¸€ä¸ªä¸‰å±‚åŠŸèƒ½å±‚çº§ç»“æ„ã€‚å…·ä½“è€Œè¨€ï¼Œè¯¥ç»“æ„å®šä¹‰äº†è´Ÿè´£ä¸å¤–éƒ¨ä¸–ç•Œäº¤äº’çš„Core Functionsã€åŸºäºæ„ŸçŸ¥å’Œè®°å¿†é€‰æ‹©è¡ŒåŠ¨çš„Integrative Evaluation Functionï¼Œä»¥åŠåŠ¨æ€é‡æ„è¡Œä¸ºåŸåˆ™å’Œå†…éƒ¨ç»„ä»¶çš„Self Modification Functionã€‚åŸºäºæ­¤ç»“æ„ï¼Œç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŒ…å«Reactiveã€Weak Autonomouså’ŒStrong Autonomousçº§åˆ«çš„åˆ†æ­¥è‡ªä¸»æ¨¡å‹ï¼Œå¹¶è¯¦ç»†è®¨è®ºäº†å…¶è®¾è®¡åŸåˆ™ã€å‘å±•ç‰¹å¾ä»¥åŠä¸ç°æœ‰Artificial Intelligenceè®¾è®¡æ–¹æ³•çš„å…³ç³»ã€‚è®ºæ–‡è¿›ä¸€æ­¥æ¢è®¨äº†è¯¥æ¡†æ¶ä½œä¸ºGeneral IntelligenceåŸºç¡€çš„æ½œåŠ›ï¼Œå¹¶å®¡è§†äº†æœªæ¥çš„åº”ç”¨åœºæ™¯ä¸ä¼¦ç†å½±å“ã€‚é€šè¿‡æä¾›ä¸€ä¸ªç‹¬ç«‹äºç‰¹å®šæŠ€æœ¯å®ç°è·¯å¾„çš„ç†è®ºæ¡†æ¶ï¼Œè¯¥å·¥ä½œæ·±åŒ–äº†å¯¹è‡ªä¸»æ€§çš„ç§‘å­¦ç†è§£ï¼Œå¹¶ä¸ºè®¾è®¡å…·æœ‰Strong Autonomyçš„æœªæ¥äººå·¥å®ä½“æä¾›äº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.12952v1",
      "published_date": "2025-06-15 19:35:27 UTC",
      "updated_date": "2025-06-15 19:35:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:07:06.516445+00:00"
    },
    {
      "arxiv_id": "2506.12949v1",
      "title": "eLog analysis for accelerators: status and future outlook",
      "title_zh": "åŠ é€Ÿå™¨ eLog åˆ†æï¼šç°çŠ¶ä¸æœªæ¥å±•æœ›",
      "authors": [
        "Antonin Sulc",
        "Thorsten Hellert",
        "Aaron Reed",
        "Adam Carpenter",
        "Alex Bien",
        "Chris Tennant",
        "Claudio Bisegni",
        "Daniel Lersch",
        "Daniel Ratner",
        "David Lawrence",
        "Diana McSpadden",
        "Hayden Hoschouer",
        "Jason St. John",
        "Thomas Britton"
      ],
      "abstract": "This work demonstrates electronic logbook (eLog) systems leveraging modern AI-driven information retrieval capabilities at the accelerator facilities of Fermilab, Jefferson Lab, Lawrence Berkeley National Laboratory (LBNL), SLAC National Accelerator Laboratory. We evaluate contemporary tools and methodologies for information retrieval with Retrieval Augmented Generation (RAGs), focusing on operational insights and integration with existing accelerator control systems.\n  The study addresses challenges and proposes solutions for state-of-the-art eLog analysis through practical implementations, demonstrating applications and limitations. We present a framework for enhancing accelerator facility operations through improved information accessibility and knowledge management, which could potentially lead to more efficient operations.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨Fermilabã€Jefferson Labã€LBNLå’ŒSLACç­‰åŠ é€Ÿå™¨è®¾æ–½ä¸­ï¼Œåˆ©ç”¨ç°ä»£äººå·¥æ™ºèƒ½é©±åŠ¨çš„ä¿¡æ¯æ£€ç´¢èƒ½åŠ›å¯¹ç”µå­æ—¥å¿—(eLog)ç³»ç»Ÿè¿›è¡Œåˆ†æçš„ç°çŠ¶ä¸å‰æ™¯ã€‚ç ”ç©¶é‡ç‚¹è¯„ä¼°äº†å½“ä»£ä¿¡æ¯æ£€ç´¢å·¥å…·å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)æŠ€æœ¯ï¼Œæ—¨åœ¨è·å–è¿è¡Œæ´å¯Ÿå¹¶å®ç°å…¶ä¸ç°æœ‰åŠ é€Ÿå™¨æ§åˆ¶ç³»ç»Ÿçš„é›†æˆã€‚é€šè¿‡å®é™…åº”ç”¨å®æ–½ï¼Œè¯¥ç ”ç©¶é’ˆå¯¹å°–ç«¯eLogåˆ†æé¢ä¸´çš„æŒ‘æˆ˜æå‡ºäº†è§£å†³æ–¹æ¡ˆï¼Œå¹¶è¯¦ç»†å±•ç¤ºäº†ç›¸å…³æŠ€æœ¯çš„åº”ç”¨èŒƒç•´ä¸å±€é™æ€§ã€‚æœ€åï¼Œç ”ç©¶æå‡ºäº†ä¸€ä¸ªé€šè¿‡æ”¹è¿›ä¿¡æ¯å¯è·å–æ€§å’ŒçŸ¥è¯†ç®¡ç†(Knowledge Management)æ¥å¢å¼ºåŠ é€Ÿå™¨è®¾æ–½è¿è¡Œçš„æ¡†æ¶ï¼Œä¸ºå®ç°æ›´é«˜æ•ˆçš„è®¾æ–½è¿è¡Œæä¾›äº†æ½œåœ¨è·¯å¾„ã€‚",
      "categories": [
        "hep-ex",
        "cs.AI"
      ],
      "primary_category": "hep-ex",
      "comment": "4 pages, 2 figures, 16th International Particle Accelerator Conference (IPAC'25)",
      "pdf_url": "https://arxiv.org/pdf/2506.12949v1",
      "published_date": "2025-06-15 19:23:38 UTC",
      "updated_date": "2025-06-15 19:23:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:06:30.039827+00:00"
    },
    {
      "arxiv_id": "2506.12937v2",
      "title": "HypER: Literature-grounded Hypothesis Generation and Distillation with Provenance",
      "title_zh": "HypERï¼šå…·æœ‰æº¯æºèƒ½åŠ›çš„æ–‡çŒ®é©±åŠ¨å‹ç§‘å­¦å‡è¯´ç”Ÿæˆä¸è’¸é¦",
      "authors": [
        "Rosni Vasu",
        "Chandrayee Basu",
        "Bhavana Dalvi Mishra",
        "Cristina Sarasua",
        "Peter Clark",
        "Abraham Bernstein"
      ],
      "abstract": "Large Language models have demonstrated promising performance in research ideation across scientific domains. Hypothesis development, the process of generating a highly specific declarative statement connecting a research idea with empirical validation, has received relatively less attention. Existing approaches trivially deploy retrieval augmentation and focus only on the quality of the final output ignoring the underlying reasoning process behind ideation. We present $\\texttt{HypER}$ ($\\textbf{Hyp}$othesis Generation with $\\textbf{E}$xplanation and $\\textbf{R}$easoning), a small language model (SLM) trained for literature-guided reasoning and evidence-based hypothesis generation. $\\texttt{HypER}$ is trained in a multi-task setting to discriminate between valid and invalid scientific reasoning chains in presence of controlled distractions. We find that $\\texttt{HypER}$ outperformes the base model, distinguishing valid from invalid reasoning chains (+22\\% average absolute F1), generates better evidence-grounded hypotheses (0.327 vs. 0.305 base model) with high feasibility and impact as judged by human experts ($>$3.5 on 5-point Likert scale).",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†HypER (Hypothesis Generation with Explanation and Reasoning)ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºæ–‡çŒ®å¼•å¯¼æ¨ç†å’Œå¾ªè¯å‡è®¾ç”Ÿæˆè€Œè®¾è®¡çš„å°è¯­è¨€æ¨¡å‹(SLM)ã€‚é’ˆå¯¹ç°æœ‰ç§‘ç ”æ„æ€æ–¹æ³•å¾€å¾€å¿½è§†åº•å±‚æ¨ç†é€»è¾‘çš„é—®é¢˜ï¼ŒHypERé€šè¿‡å¤šä»»åŠ¡è®¾ç½®è¿›è¡Œè®­ç»ƒï¼Œæ—¨åœ¨å¹²æ‰°é¡¹å­˜åœ¨çš„æƒ…å†µä¸‹å‡†ç¡®è¾¨åˆ«æœ‰æ•ˆä¸æ— æ•ˆçš„ç§‘å­¦æ¨ç†é“¾ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒHypERåœ¨æ¨ç†é“¾è¾¨åˆ«ä»»åŠ¡ä¸Šçš„å¹³å‡ç»å¯¹F1å€¼æ¯”åŸºçº¿æ¨¡å‹æå‡äº†22%ï¼Œç”Ÿæˆçš„å¾ªè¯å‡è®¾è´¨é‡ä¹Ÿæ˜¾è‘—ä¼˜äºåŸºç¡€æ¨¡å‹ã€‚äººç±»ä¸“å®¶è¯„å®¡è¿›ä¸€æ­¥è¯å®ï¼ŒHypERç”Ÿæˆçš„å‡è®¾åœ¨å¯è¡Œæ€§ä¸å½±å“åŠ›æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œåœ¨5åˆ†åˆ¶Likert scaleä¸‹è·å¾—äº†è¶…è¿‡3.5åˆ†çš„è¯„ä»·ï¼Œä¸ºç§‘å­¦å‡è®¾çš„è‡ªåŠ¨ç”Ÿæˆä¸éªŒè¯æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "EMNLP 2025, 26 pages (9 pages: main paper body)",
      "pdf_url": "https://arxiv.org/pdf/2506.12937v2",
      "published_date": "2025-06-15 18:41:23 UTC",
      "updated_date": "2025-08-21 20:28:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:06:55.065467+00:00"
    },
    {
      "arxiv_id": "2506.21574v1",
      "title": "Digital Gatekeepers: Exploring Large Language Model's Role in Immigration Decisions",
      "title_zh": "æ•°å­—æŠŠå…³äººï¼šæ¢ç©¶å¤§è¯­è¨€æ¨¡å‹åœ¨ç§»æ°‘å†³ç­–ä¸­çš„è§’è‰²",
      "authors": [
        "Yicheng Mao",
        "Yang Zhao"
      ],
      "abstract": "With globalization and increasing immigrant populations, immigration departments face significant work-loads and the challenge of ensuring fairness in decision-making processes. Integrating artificial intelligence offers a promising solution to these challenges. This study investigates the potential of large language models (LLMs),such as GPT-3.5 and GPT-4, in supporting immigration decision-making. Utilizing a mixed-methods approach,this paper conducted discrete choice experiments and in-depth interviews to study LLM decision-making strategies and whether they are fair. Our findings demonstrate that LLMs can align their decision-making with human strategies, emphasizing utility maximization and procedural fairness. Meanwhile, this paper also reveals that while ChatGPT has safeguards to prevent unintentional discrimination, it still exhibits stereotypes and biases concerning nationality and shows preferences toward privileged group. This dual analysis highlights both the potential and limitations of LLMs in automating and enhancing immigration decisions.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œå¦‚GPT-3.5å’ŒGPT-4ï¼Œåœ¨æ”¯æŒç§»æ°‘å†³ç­–æ–¹é¢çš„æ½œåŠ›ï¼Œæ—¨åœ¨åº”å¯¹å…¨çƒåŒ–èƒŒæ™¯ä¸‹ç§»æ°‘éƒ¨é—¨é¢ä¸´çš„é«˜å·¥ä½œé‡å’Œå†³ç­–å…¬å¹³æ€§æŒ‘æˆ˜ã€‚é€šè¿‡ç»“åˆç¦»æ•£é€‰æ‹©å®éªŒï¼ˆdiscrete choice experimentsï¼‰å’Œæ·±åº¦è®¿è°ˆï¼ˆin-depth interviewsï¼‰çš„æ··åˆç ”ç©¶æ–¹æ³•ï¼Œè¯¥è®ºæ–‡æ·±å…¥åˆ†æäº†LLMsçš„å†³ç­–ç­–ç•¥åŠå…¶å…¬å¹³æ€§è¡¨ç°ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨å†³ç­–è¿‡ç¨‹ä¸­èƒ½å¤Ÿä¸äººç±»ç­–ç•¥ä¿æŒä¸€è‡´ï¼Œå¼ºè°ƒæ•ˆç”¨æœ€å¤§åŒ–ï¼ˆutility maximizationï¼‰å’Œç¨‹åºå…¬å¹³ï¼ˆprocedural fairnessï¼‰ã€‚ç„¶è€Œï¼Œåˆ†æåŒæ—¶æ­ç¤ºäº†LLMsçš„å±€é™æ€§ï¼šå°½ç®¡ChatGPTè®¾æœ‰é˜²èŒƒæ— æ„è¯†æ­§è§†çš„å®‰å…¨æœºåˆ¶ï¼Œä½†åœ¨æ¶‰åŠå›½ç±æ—¶ä»è¡¨ç°å‡ºåˆ»æ¿å°è±¡ä¸åè§ï¼Œå¹¶å¯¹ç‰¹æƒç¾¤ä½“è¡¨ç°å‡ºåå¥½ã€‚è¿™ä¸€åŒé‡åˆ†æé˜æ˜äº†LLMsåœ¨è‡ªåŠ¨åŒ–å’Œå¢å¼ºç§»æ°‘å†³ç­–è¿‡ç¨‹ä¸­çš„æ½œåŠ›ä¸ç°å®å±€é™ï¼Œä¸ºæœªæ¥äººå·¥æ™ºèƒ½åœ¨è¡Œæ”¿å†³ç­–é¢†åŸŸçš„åº”ç”¨æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.21574v1",
      "published_date": "2025-06-15 18:04:39 UTC",
      "updated_date": "2025-06-15 18:04:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:07:08.127468+00:00"
    },
    {
      "arxiv_id": "2506.12928v1",
      "title": "Scaling Test-time Compute for LLM Agents",
      "title_zh": "æ‰©å±•å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“çš„æ¨ç†æ—¶è®¡ç®—",
      "authors": [
        "King Zhu",
        "Hanhao Li",
        "Siwei Wu",
        "Tianshun Xing",
        "Dehua Ma",
        "Xiangru Tang",
        "Minghao Liu",
        "Jian Yang",
        "Jiaheng Liu",
        "Yuchen Eleanor Jiang",
        "Changwang Zhang",
        "Chenghua Lin",
        "Jun Wang",
        "Ge Zhang",
        "Wangchunshu Zhou"
      ],
      "abstract": "Scaling test time compute has shown remarkable success in improving the reasoning abilities of large language models (LLMs). In this work, we conduct the first systematic exploration of applying test-time scaling methods to language agents and investigate the extent to which it improves their effectiveness. Specifically, we explore different test-time scaling strategies, including: (1) parallel sampling algorithms; (2) sequential revision strategies; (3) verifiers and merging methods; (4)strategies for diversifying rollouts.We carefully analyze and ablate the impact of different design strategies on applying test-time scaling on language agents, and have follow findings: 1. Scaling test time compute could improve the performance of agents. 2. Knowing when to reflect is important for agents. 3. Among different verification and result merging approaches, the list-wise method performs best. 4. Increasing diversified rollouts exerts a positive effect on the agent's task performance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“ (LLM Agents) é¦–æ¬¡ç³»ç»Ÿæ€§åœ°æ¢è®¨äº†æ‰©å±•æ¨ç†ä¾§è®¡ç®— (Scaling test-time compute) çš„åº”ç”¨ï¼Œæ—¨åœ¨æå‡æ™ºèƒ½ä½“çš„æ¨ç†ä¸ä»»åŠ¡æ‰§è¡Œèƒ½åŠ›ã€‚ç ”ç©¶äººå‘˜æ·±å…¥è€ƒå¯Ÿäº†å¹¶è¡Œé‡‡æ ·ç®—æ³• (parallel sampling algorithms)ã€åºåˆ—ä¿®æ­£ç­–ç•¥ (sequential revision strategies)ã€éªŒè¯å™¨ä¸åˆå¹¶æ–¹æ³• (verifiers and merging methods) ä»¥åŠ rollout å¤šæ ·åŒ–ç­‰æ ¸å¿ƒç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰©å±•æ¨ç†ä¾§è®¡ç®—èƒ½æ˜¾è‘—ä¼˜åŒ–æ™ºèƒ½ä½“æ€§èƒ½ï¼Œä¸”ç¡®å®šâ€œä½•æ—¶è¿›è¡Œåæ€â€ (when to reflect) æ˜¯æ™ºèƒ½ä½“æˆåŠŸçš„å…³é”®ã€‚åœ¨å„ç§éªŒè¯å’Œç»“æœåˆå¹¶æ–¹æ³•ä¸­ï¼ŒåŸºäºåˆ—è¡¨çš„æ–¹æ³• (list-wise method) è¡¨ç°æœ€ä¸ºå‡ºè‰²ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è¯å®äº†å¢åŠ å¤šæ ·åŒ–çš„è½¨è¿¹æ¼”ç»ƒ (diversified rollouts) å¯¹å¢å¼ºæ™ºèƒ½ä½“æœ€ç»ˆä»»åŠ¡è¡¨ç°å…·æœ‰æ˜ç¡®çš„æ­£å‘å½±å“ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.12928v1",
      "published_date": "2025-06-15 17:59:47 UTC",
      "updated_date": "2025-06-15 17:59:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:07:36.586118+00:00"
    },
    {
      "arxiv_id": "2506.12927v1",
      "title": "Sectoral Coupling in Linguistic State Space",
      "title_zh": "è¯­è¨€çŠ¶æ€ç©ºé—´ä¸­çš„æ‰‡åŒºè€¦åˆ",
      "authors": [
        "Sebastian Dumbrava"
      ],
      "abstract": "This work presents a formal framework for quantifying the internal dependencies between functional subsystems within artificial agents whose belief states are composed of structured linguistic fragments. Building on the Semantic Manifold framework, which organizes belief content into functional sectors and stratifies them across hierarchical levels of abstraction, we introduce a system of sectoral coupling constants that characterize how one cognitive sector influences another within a fixed level of abstraction. The complete set of these constants forms an agent-specific coupling profile that governs internal information flow, shaping the agent's overall processing tendencies and cognitive style. We provide a detailed taxonomy of these intra-level coupling roles, covering domains such as perceptual integration, memory access and formation, planning, meta-cognition, execution control, and affective modulation. We also explore how these coupling profiles generate feedback loops, systemic dynamics, and emergent signatures of cognitive behavior. Methodologies for inferring these profiles from behavioral or internal agent data are outlined, along with a discussion of how these couplings evolve across abstraction levels. This framework contributes a mechanistic and interpretable approach to modeling complex cognition, with applications in AI system design, alignment diagnostics, and the analysis of emergent agent behavior.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå½¢å¼åŒ–æ¡†æ¶ï¼Œç”¨äºé‡åŒ–å†…éƒ¨ä¿¡å¿µçŠ¶æ€ç”±ç»“æ„åŒ–è¯­è¨€ç‰‡æ®µç»„æˆçš„æ™ºèƒ½ä½“ä¸­åŠŸèƒ½å­ç³»ç»Ÿä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚åœ¨ Semantic Manifold æ¡†æ¶çš„åŸºç¡€ä¸Šï¼Œç ”ç©¶å¼•å…¥äº†éƒ¨é—¨è€¦åˆå¸¸æ•° (sectoral coupling constants) ç³»ç»Ÿï¼Œç”¨ä»¥è¡¨å¾åŒä¸€æŠ½è±¡å±‚æ¬¡å†…ä¸åŒè®¤çŸ¥éƒ¨é—¨ä¹‹é—´çš„ç›¸äº’å½±å“ã€‚è¿™äº›å¸¸æ•°å…±åŒæ„æˆäº†æ™ºèƒ½ä½“ç‰¹æœ‰çš„è€¦åˆé…ç½®æ–‡ä»¶ (coupling profile)ï¼Œå†³å®šäº†å†…éƒ¨ä¿¡æ¯æµå‘å¹¶å¡‘é€ äº†å…¶æ•´ä½“å¤„ç†å€¾å‘ä¸è®¤çŸ¥é£æ ¼ã€‚è¯¥æ¡†æ¶æä¾›äº†è¯¦ç»†çš„å†…éƒ¨è€¦åˆè§’è‰²åˆ†ç±»å­¦ï¼Œæ¶µç›–äº†æ„ŸçŸ¥é›†æˆã€è®°å¿†è®¿é—®ã€è§„åˆ’ã€å…ƒè®¤çŸ¥åŠæƒ…æ„Ÿè°ƒèŠ‚ç­‰å¤šä¸ªé¢†åŸŸã€‚é€šè¿‡åˆ†æè€¦åˆé…ç½®å¦‚ä½•äº§ç”Ÿåé¦ˆå›è·¯ã€ç³»ç»ŸåŠ¨åŠ›å­¦åŠè®¤çŸ¥è¡Œä¸ºçš„æ¶Œç°ç‰¹å¾ï¼Œç ”ç©¶ä¸ºå»ºæ¨¡å¤æ‚è®¤çŸ¥æä¾›äº†ä¸€ç§æœºæ¢°åŒ–ä¸”å…·æœ‰å¯è§£é‡Šæ€§çš„è·¯å¾„ã€‚è¯¥æˆæœåœ¨äººå·¥æ™ºèƒ½ç³»ç»Ÿè®¾è®¡ã€å¯¹é½è¯Šæ–­ (alignment diagnostics) ä»¥åŠæ™ºèƒ½ä½“æ¶Œç°è¡Œä¸ºåˆ†æä¸­å…·æœ‰é‡è¦çš„åº”ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "56 pages, 12 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.12927v1",
      "published_date": "2025-06-15 17:58:54 UTC",
      "updated_date": "2025-06-15 17:58:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:08:05.247740+00:00"
    },
    {
      "arxiv_id": "2506.12925v1",
      "title": "Identifying and Investigating Global News Coverage of Critical Events Such as Disasters and Terrorist Attacks",
      "title_zh": "ç¾å®³ä¸ææ€–è¢­å‡»ç­‰é‡å¤§äº‹ä»¶å…¨çƒæ–°é—»æŠ¥é“çš„è¯†åˆ«ä¸æ¢ç©¶",
      "authors": [
        "Erica Cai",
        "Xi Chen",
        "Reagan Grey Keeney",
        "Ethan Zuckerman",
        "Brendan O'Connor",
        "Przemyslaw A. Grabowicz"
      ],
      "abstract": "Comparative studies of news coverage are challenging to conduct because methods to identify news articles about the same event in different languages require expertise that is difficult to scale. We introduce an AI-powered method for identifying news articles based on an event FINGERPRINT, which is a minimal set of metadata required to identify critical events. Our event coverage identification method, FINGERPRINT TO ARTICLE MATCHING FOR EVENTS (FAME), efficiently identifies news articles about critical world events, specifically terrorist attacks and several types of natural disasters. FAME does not require training data and is able to automatically and efficiently identify news articles that discuss an event given its fingerprint: time, location, and class (such as storm or flood). The method achieves state-of-the-art performance and scales to massive databases of tens of millions of news articles and hundreds of events happening globally. We use FAME to identify 27,441 articles that cover 470 natural disaster and terrorist attack events that happened in 2020. To this end, we use a massive database of news articles in three languages from MediaCloud, and three widely used, expert-curated databases of critical events: EM-DAT, USGS, and GTD. Our case study reveals patterns consistent with prior literature: coverage of disasters and terrorist attacks correlates to death counts, to the GDP of a country where the event occurs, and to trade volume between the reporting country and the country where the event occurred. We share our NLP annotations and cross-country media attention data to support the efforts of researchers and media monitoring organizations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è·¨è¯­è¨€æ–°é—»æŠ¥é“æ¯”è¾ƒç ”ç©¶ä¸­éš¾ä»¥è§„æ¨¡åŒ–è¯†åˆ«ç›¸åŒäº‹ä»¶çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º FAME (Fingerprint to Article Matching for Events) çš„äººå·¥æ™ºèƒ½é©±åŠ¨æ–¹æ³•ã€‚FAME ä»…éœ€åˆ©ç”¨äº‹ä»¶çš„æ—¶é—´ã€åœ°ç‚¹å’Œç±»åˆ«ç­‰æœ€å°åŒ–å…ƒæ•°æ®ï¼ˆå³äº‹ä»¶ FINGERPRINTï¼‰å³å¯åœ¨æµ·é‡æ•°æ®åº“ä¸­è‡ªåŠ¨ä¸”é«˜æ•ˆåœ°è¯†åˆ«å…³é”®ä¸–ç•Œäº‹ä»¶ï¼Œä¸”è¯¥æ–¹æ³•ä¸éœ€è¦è®­ç»ƒæ•°æ®ã€‚å®éªŒè¯æ˜ï¼ŒFAME åœ¨å¤„ç†åŒ…å«æ•°åƒä¸‡ç¯‡æ–°é—»æ–‡ç« å’Œæ•°ç™¾ä¸ªå…¨çƒäº‹ä»¶çš„å¤§è§„æ¨¡æ•°æ®åº“æ—¶è¾¾åˆ°äº† state-of-the-art æ€§èƒ½ã€‚é€šè¿‡å¯¹ 2020 å¹´å‘ç”Ÿçš„ 470 èµ·è‡ªç„¶ç¾å®³å’Œææ€–è¢­å‡»äº‹ä»¶è¿›è¡Œæ¡ˆä¾‹ç ”ç©¶ï¼Œç ”ç©¶å‘ç°å¯¹è¿™äº›äº‹ä»¶çš„æ–°é—»è¦†ç›–ç‡ä¸æ­»äº¡äººæ•°ã€å‘ç”Ÿå›½çš„ GDP ä»¥åŠæŠ¥é“å›½ä¸å‘ç”Ÿå›½ä¹‹é—´çš„è´¸æ˜“é¢ (trade volume) å‘ˆæ˜¾è‘—æ­£ç›¸å…³ã€‚è¯¥ç ”ç©¶ä¸ä»…æ­ç¤ºäº†å…¨çƒåª’ä½“å…³æ³¨åº¦çš„å†…åœ¨åˆ†å¸ƒæ¨¡å¼ï¼Œè¿˜å…±äº«äº† NLP æ ‡æ³¨å’Œè·¨å›½åª’ä½“å…³æ³¨æ•°æ®ï¼Œä¸ºå…¨çƒåª’ä½“ç›‘æµ‹å’Œç›¸å…³å­¦æœ¯ç ”ç©¶æä¾›äº†å¼ºæœ‰åŠ›çš„å·¥å…·æ”¯æŒã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.12925v1",
      "published_date": "2025-06-15 17:50:08 UTC",
      "updated_date": "2025-06-15 17:50:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:07:58.584389+00:00"
    },
    {
      "arxiv_id": "2506.13827v1",
      "title": "Balancing Preservation and Modification: A Region and Semantic Aware Metric for Instruction-Based Image Editing",
      "title_zh": "å¹³è¡¡ä¿æŒä¸ä¿®æ”¹ï¼šä¸€ç§é¢å‘æŒ‡ä»¤å¼å›¾åƒç¼–è¾‘çš„åŒºåŸŸä¸è¯­ä¹‰æ„ŸçŸ¥åº¦é‡æŒ‡æ ‡",
      "authors": [
        "Zhuoying Li",
        "Zhu Xu",
        "Yuxin Peng",
        "Yang Liu"
      ],
      "abstract": "Instruction-based image editing, which aims to modify the image faithfully according to the instruction while preserving irrelevant content unchanged, has made significant progress. However, there still lacks a comprehensive metric for assessing the editing quality. Existing metrics either require high human evaluation costs, which hinder large-scale evaluation, or are adapted from other tasks and lose task-specific concerns, failing to comprehensively evaluate both instruction-based modification and preservation of irrelevant regions, resulting in biased evaluation. To tackle this, we introduce a new metric called Balancing Preservation and Modification (BPM), tailored for instruction-based image editing by explicitly disentangling the image into editing-relevant and irrelevant regions for specific consideration. We first identify and locate editing-relevant regions, followed by a two-tier process to assess editing quality: Region-Aware Judge evaluates whether the position and size of the edited region align with the instruction, and Semantic-Aware Judge further assesses the instruction content compliance within editing-relevant regions as well as content preservation within irrelevant regions, yielding comprehensive and interpretable quality assessment. Moreover, the editing-relevant region localization in BPM can be integrated into image editing approaches to improve editing quality, demonstrating its broad applicability. We verify the effectiveness of the BPM metric on comprehensive instruction-editing data, and the results show the highest alignment with human evaluation compared to existing metrics, indicating its efficacy. Code is available at: https://joyli-x.github.io/BPM/",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘ (Instruction-based image editing) è¯„ä»·æŒ‡æ ‡å­˜åœ¨çš„äººå·¥è¯„ä¼°æˆæœ¬é«˜æˆ–éš¾ä»¥å…¼é¡¾å†…å®¹ä¿®æ”¹ä¸æ— å…³åŒºåŸŸä¿ç•™çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º BPM (Balancing Preservation and Modification) çš„æ–°æŒ‡æ ‡ã€‚è¯¥æŒ‡æ ‡é€šè¿‡å°†å›¾åƒæ˜¾å¼åˆ†è§£ä¸ºç¼–è¾‘ç›¸å…³åŒºåŸŸå’Œæ— å…³åŒºåŸŸï¼Œå®ç°å¯¹ç¼–è¾‘è´¨é‡çš„ç»†ç²’åº¦è¯„ä¼°ã€‚å…·ä½“è€Œè¨€ï¼ŒBPM é¦–å…ˆå®šä½ç¼–è¾‘åŒºåŸŸï¼Œéšååˆ©ç”¨ Region-Aware Judge è¯„ä¼°ç¼–è¾‘ä½ç½®ä¸å¤§å°çš„å‡†ç¡®æ€§ï¼Œå¹¶ç»“åˆ Semantic-Aware Judge è€ƒå¯ŸæŒ‡ä»¤ç¬¦åˆåº¦åŠæ— å…³å†…å®¹çš„ä¿ç•™æƒ…å†µã€‚æ­¤å¤–ï¼ŒBPM çš„åŒºåŸŸå®šä½åŠŸèƒ½è¿˜å¯é›†æˆè‡³ç°æœ‰ç¼–è¾‘æ–¹æ³•ä¸­ä»¥æå‡å…¶ç”Ÿæˆè´¨é‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBPM åœ¨ç»¼åˆæŒ‡ä»¤ç¼–è¾‘æ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¸äººç±»è¯„ä¼°æœ€é«˜çš„ä¸€è‡´æ€§ï¼Œä¸ºè¯¥é¢†åŸŸçš„è´¨é‡è¡¡é‡æä¾›äº†ä¸€ä¸ªå…¨é¢ä¸”å…·æœ‰å¯è§£é‡Šæ€§çš„å·¥å…·ã€‚",
      "categories": [
        "cs.GR",
        "cs.AI"
      ],
      "primary_category": "cs.GR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.13827v1",
      "published_date": "2025-06-15 17:12:57 UTC",
      "updated_date": "2025-06-15 17:12:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:07:55.792548+00:00"
    },
    {
      "arxiv_id": "2506.12912v1",
      "title": "Logit Dynamics in Softmax Policy Gradient Methods",
      "title_zh": "Softmax ç­–ç•¥æ¢¯åº¦æ–¹æ³•ä¸­çš„ Logit åŠ¨åŠ›å­¦",
      "authors": [
        "Yingru Li"
      ],
      "abstract": "We analyzes the logit dynamics of softmax policy gradient methods. We derive the exact formula for the L2 norm of the logit update vector: $$ \\|Î”\\mathbf{z}\\|_2 \\propto \\sqrt{1-2P_c + C(P)} $$ This equation demonstrates that update magnitudes are determined by the chosen action's probability ($P_c$) and the policy's collision probability ($C(P)$), a measure of concentration inversely related to entropy. Our analysis reveals an inherent self-regulation mechanism where learning vigor is automatically modulated by policy confidence, providing a foundational insight into the stability and convergence of these methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥åˆ†æäº† Softmax Policy Gradient æ–¹æ³•ä¸­çš„ Logit Dynamicsï¼Œå¹¶æ¨å¯¼å‡ºäº† Logit æ›´æ–°å‘é‡ L2 Norm çš„ç²¾ç¡®å…¬å¼ $\\|Î”\\mathbf{z}\\|_2 \\propto \\sqrt{1-2P_c + C(P)}$ã€‚è¯¥æ–¹ç¨‹è¯æ˜äº†æ›´æ–°å¹…åº¦ç”±æ‰€é€‰åŠ¨ä½œçš„æ¦‚ç‡ $P_c$ å’Œç­–ç•¥çš„ç¢°æ’æ¦‚ç‡ $C(P)$ å…±åŒå†³å®šï¼Œå…¶ä¸­ $C(P)$ æ˜¯ä¸ Entropy æˆåæ¯”çš„ç­–ç•¥é›†ä¸­åº¦åº¦é‡ã€‚åˆ†ææ­ç¤ºäº†ä¸€ç§å†…åœ¨çš„è‡ªæˆ‘è°ƒèŠ‚æœºåˆ¶ (Self-regulation Mechanism)ï¼Œä½¿å¾—å­¦ä¹ å¼ºåº¦èƒ½å¤Ÿæ ¹æ®ç­–ç•¥ç½®ä¿¡åº¦è‡ªåŠ¨è¿›è¡Œè°ƒåˆ¶ã€‚è¿™ä¸€å‘ç°ä¸º Softmax Policy Gradient æ–¹æ³•çš„ç¨³å®šæ€§ (Stability) å’Œæ”¶æ•›æ€§ (Convergence) æä¾›äº†åŸºç¡€æ€§çš„ç†è®ºè§è§£ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "7 pages",
      "pdf_url": "https://arxiv.org/pdf/2506.12912v1",
      "published_date": "2025-06-15 17:02:39 UTC",
      "updated_date": "2025-06-15 17:02:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:08:07.990477+00:00"
    },
    {
      "arxiv_id": "2506.12911v2",
      "title": "Constraint-Guided Prediction Refinement via Deterministic Diffusion Trajectories",
      "title_zh": "åŸºäºç¡®å®šæ€§æ‰©æ•£è½¨è¿¹çš„çº¦æŸå¼•å¯¼é¢„æµ‹ä¿®æ­£",
      "authors": [
        "Pantelis Dogoulis",
        "Fabien Bernier",
        "FÃ©lix Fourreau",
        "Karim Tit",
        "Maxime Cordy"
      ],
      "abstract": "Many real-world machine learning tasks require outputs that satisfy hard constraints, such as physical conservation laws, structured dependencies in graphs, or column-level relationships in tabular data. Existing approaches rely either on domain-specific architectures and losses or on strong assumptions on the constraint space, restricting their applicability to linear or convex constraints. We propose a general-purpose framework for constraint-aware refinement that leverages denoising diffusion implicit models (DDIMs). Starting from a coarse prediction, our method iteratively refines it through a deterministic diffusion trajectory guided by a learned prior and augmented by constraint gradient corrections. The approach accommodates a wide class of non-convex and nonlinear equality constraints and can be applied post hoc to any base model. We demonstrate the method in two representative domains: constrained adversarial attack generation on tabular data with column-level dependencies and in AC power flow prediction under Kirchhoff's laws. Across both settings, our diffusion-guided refinement improves both constraint satisfaction and performance while remaining lightweight and model-agnostic.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨å­¦ä¹ ä»»åŠ¡ä¸­ç¡¬çº¦æŸ(hard constraints)éš¾ä»¥æ»¡è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºç¡®å®šæ€§æ‰©æ•£è½¨è¿¹çš„é€šç”¨çº¦æŸå¼•å¯¼é¢„æµ‹ä¼˜åŒ–æ¡†æ¶ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å»å™ªæ‰©æ•£éšå¼æ¨¡å‹(DDIMs)ï¼Œé€šè¿‡ç»“åˆå­¦ä¹ å…ˆéªŒä¸çº¦æŸæ¢¯åº¦æ ¡æ­£ï¼Œå¯¹åˆå§‹é¢„æµ‹è¿›è¡Œè¿­ä»£ä¼˜åŒ–ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿå¤„ç†å¹¿æ³›çš„éå‡¸(non-convex)å’Œéçº¿æ€§ç­‰å¼çº¦æŸï¼Œä¸”å…·å¤‡æ¨¡å‹æ— å…³æ€§ï¼Œå¯ä½œä¸ºäº‹å(post hoc)æ’ä»¶åº”ç”¨äºä»»ä½•åŸºç¡€æ¨¡å‹ã€‚ç ”ç©¶åœ¨è¡¨æ ¼æ•°æ®çš„å—é™å¯¹æŠ—æ”»å‡»ç”Ÿæˆå’Œäº¤æµæ½®æµé¢„æµ‹(AC power flow prediction)ä»»åŠ¡ä¸­è¿›è¡Œäº†éªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§æ‰©æ•£å¼•å¯¼çš„ä¼˜åŒ–æ–¹æ³•åœ¨ä¿æŒè½»é‡åŒ–çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ€§èƒ½ä¸çº¦æŸæ»¡è¶³ç‡ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.12911v2",
      "published_date": "2025-06-15 17:02:07 UTC",
      "updated_date": "2025-11-19 20:05:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:08:06.698692+00:00"
    },
    {
      "arxiv_id": "2506.15735v1",
      "title": "ContextBench: Modifying Contexts for Targeted Latent Activation",
      "title_zh": "ContextBenchï¼šé¢å‘å®šå‘æ½œåœ¨æ¿€æ´»çš„ä¸Šä¸‹æ–‡ä¿®æ”¹",
      "authors": [
        "Robert Graham",
        "Edward Stevinson",
        "Leo Richter",
        "Alexander Chia",
        "Joseph Miller",
        "Joseph Isaac Bloom"
      ],
      "abstract": "Identifying inputs that trigger specific behaviours or latent features in language models could have a wide range of safety use cases. We investigate a class of methods capable of generating targeted, linguistically fluent inputs that activate specific latent features or elicit model behaviours. We formalise this approach as context modification and present ContextBench -- a benchmark with tasks assessing core method capabilities and potential safety applications. Our evaluation framework measures both elicitation strength (activation of latent features or behaviours) and linguistic fluency, highlighting how current state-of-the-art methods struggle to balance these objectives. We enhance Evolutionary Prompt Optimisation (EPO) with LLM-assistance and diffusion model inpainting, and demonstrate that these variants achieve state-of-the-art performance in balancing elicitation effectiveness and fluency.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é€šè¿‡ç”Ÿæˆé’ˆå¯¹æ€§çš„ã€è¯­è¨€æµç•…çš„è¾“å…¥æ¥è§¦å‘è¯­è¨€æ¨¡å‹ç‰¹å®šè¡Œä¸ºæˆ–æ½œåœ¨ç‰¹å¾çš„æ–¹æ³•ï¼Œå¹¶å°†æ­¤ç±»æ–¹æ³•å½¢å¼åŒ–ä¸ºä¸Šä¸‹æ–‡ä¿®æ”¹(context modification)ã€‚ä½œè€…æå‡ºäº†ContextBenchåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°è¯¥æ–¹æ³•åœ¨æ ¸å¿ƒèƒ½åŠ›å’Œæ½œåœ¨å®‰å…¨åº”ç”¨ä¸­çš„è¡¨ç°ï¼Œå…¶è¯„ä¼°æ¡†æ¶æ¶µç›–äº†è¯±å‘å¼ºåº¦(elicitation strength)ä¸è¯­è¨€æµç•…åº¦(linguistic fluency)ä¸¤ä¸ªç»´åº¦ã€‚è¯„ä»·ç»“æœæ˜¾ç¤ºï¼Œç›®å‰æœ€å…ˆè¿›çš„æ–¹æ³•åœ¨å¹³è¡¡è¯±å‘æ•ˆæœä¸è¯­è¨€æµç•…åº¦æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œè¯¥ç ”ç©¶ç»“åˆå¤§è¯­è¨€æ¨¡å‹è¾…åŠ©(LLM-assistance)ä¸æ‰©æ•£æ¨¡å‹é‡ç»˜(diffusion model inpainting)æŠ€æœ¯æ”¹è¿›äº†è¿›åŒ–æç¤ºä¼˜åŒ–(Evolutionary Prompt Optimisation, EPO)ç®—æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›æ”¹è¿›åçš„ç®—æ³•å˜ä½“åœ¨å¹³è¡¡è¯±å‘æœ‰æ•ˆæ€§ä¸æµç•…åº¦æ–¹é¢è¾¾åˆ°äº†å½“å‰é¢†å…ˆæ°´å¹³ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15735v1",
      "published_date": "2025-06-15 16:54:09 UTC",
      "updated_date": "2025-06-15 16:54:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:08:07.632762+00:00"
    },
    {
      "arxiv_id": "2506.12902v1",
      "title": "KCLNet: Physics-Informed Power Flow Prediction via Constraints Projections",
      "title_zh": "KCLNetï¼šåŸºäºçº¦æŸæŠ•å½±çš„ç‰©ç†ä¿¡æ¯æ½®æµé¢„æµ‹",
      "authors": [
        "Pantelis Dogoulis",
        "Karim Tit",
        "Maxime Cordy"
      ],
      "abstract": "In the modern context of power systems, rapid, scalable, and physically plausible power flow predictions are essential for ensuring the grid's safe and efficient operation. While traditional numerical methods have proven robust, they require extensive computation to maintain physical fidelity under dynamic or contingency conditions. In contrast, recent advancements in artificial intelligence (AI) have significantly improved computational speed; however, they often fail to enforce fundamental physical laws during real-world contingencies, resulting in physically implausible predictions. In this work, we introduce KCLNet, a physics-informed graph neural network that incorporates Kirchhoff's Current Law as a hard constraint via hyperplane projections. KCLNet attains competitive prediction accuracy while ensuring zero KCL violations, thereby delivering reliable and physically consistent power flow predictions critical to secure the operation of modern smart grids.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† KCLNetï¼Œä¸€ç§ç‰©ç†æ„ŸçŸ¥çš„å›¾ç¥ç»ç½‘ç»œ (Graph Neural Network)ï¼Œæ—¨åœ¨è§£å†³äººå·¥æ™ºèƒ½æ¨¡å‹åœ¨ç”µåŠ›æµé‡é¢„æµ‹ä¸­å¾€å¾€éš¾ä»¥éµå¾ªç‰©ç†å®šå¾‹å¹¶å¯¼è‡´é¢„æµ‹ç»“æœåœ¨ç‰©ç†ä¸Šä¸å¯è¡Œçš„é—®é¢˜ã€‚KCLNet é€šè¿‡è¶…å¹³é¢æŠ•å½± (Hyperplane Projections) çš„æ–¹å¼ï¼Œå°†åŸºå°”éœå¤«ç”µæµå®šå¾‹ (Kirchhoff's Current Law, KCL) ä½œä¸ºæ¨¡å‹å†…éƒ¨çš„ç¡¬çº¦æŸ (Hard Constraint) å¼ºåˆ¶æ‰§è¡Œã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ä¿æŒç«äº‰æ€§é¢„æµ‹å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œå®ç°äº†é›¶ KCL è¿åé‡ï¼Œç¡®ä¿äº†é¢„æµ‹ç»“æœçš„ç‰©ç†ä¸€è‡´æ€§ã€‚è¿™ä¸€ç‰¹æ€§å¯¹äºä¿éšœç°ä»£æ™ºèƒ½ç”µç½‘åœ¨åŠ¨æ€æˆ–åº”æ€¥çŠ¶å†µä¸‹çš„å®‰å…¨é«˜æ•ˆè¿è¡Œè‡³å…³é‡è¦ï¼Œä¸ºç”µåŠ›ç³»ç»Ÿæä¾›äº†å¯é ä¸”å…·å¤‡ç‰©ç†ä¿è¯çš„å¿«é€Ÿé¢„æµ‹æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.12902v1",
      "published_date": "2025-06-15 16:29:02 UTC",
      "updated_date": "2025-06-15 16:29:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:08:19.135711+00:00"
    },
    {
      "arxiv_id": "2506.12894v1",
      "title": "Homeostatic Coupling for Prosocial Behavior",
      "title_zh": "ä¿ƒè¿›äº²ç¤¾ä¼šè¡Œä¸ºçš„ç¨³æ€è€¦åˆ",
      "authors": [
        "Naoto Yoshida",
        "Kingson Man"
      ],
      "abstract": "When regarding the suffering of others, we often experience personal distress and feel compelled to help\\footnote{Preprint. Under review.}. Inspired by living systems, we investigate the emergence of prosocial behavior among autonomous agents that are motivated by homeostatic self-regulation. We perform multi-agent reinforcement learning, treating each agent as a vulnerable homeostat charged with maintaining its own well-being. We introduce an empathy-like mechanism to share homeostatic states between agents: an agent can either \\emph{observe} their partner's internal state ({\\bf cognitive empathy}) or the agent's internal state can be \\emph{directly coupled} to that of their partner ({\\bf affective empathy}). In three simple multi-agent environments, we show that prosocial behavior arises only under homeostatic coupling - when the distress of a partner can affect one's own well-being. Additionally, we show that empathy can be learned: agents can ``decode\" their partner's external emotive states to infer the partner's internal homeostatic states. Assuming some level of physiological similarity, agents reference their own emotion-generation functions to invert the mapping from outward display to internal state. Overall, we demonstrate the emergence of prosocial behavior when homeostatic agents learn to ``read\" the emotions of others and then to empathize, or feel as they feel.",
      "tldr_zh": "è¯¥ç ”ç©¶å—ç”Ÿç‰©ç³»ç»Ÿå¯å‘ï¼Œé€šè¿‡å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ (multi-agent reinforcement learning)æ¢è®¨äº†ç”±ç¨³æ€è‡ªæˆ‘è°ƒèŠ‚(homeostatic self-regulation)é©±åŠ¨çš„æ™ºèƒ½ä½“å¦‚ä½•æ¶Œç°äº²ç¤¾ä¼šè¡Œä¸º(prosocial behavior)ã€‚ç ”ç©¶è€…å°†æ¯ä¸ªæ™ºèƒ½ä½“å»ºæ¨¡ä¸ºè´Ÿè´£ç»´æŒè‡ªèº«ç¦ç¥‰çš„æ˜“æŸç¨³æ€å™¨(vulnerable homeostat)ï¼Œå¹¶è®¾è®¡äº†è®¤çŸ¥å…±æƒ…(cognitive empathy)ä¸æƒ…æ„Ÿå…±æƒ…(affective empathy)ä¸¤ç§æœºåˆ¶æ¥å…±äº«å†…éƒ¨çŠ¶æ€ã€‚å®éªŒè¯æ˜ï¼Œäº²ç¤¾ä¼šè¡Œä¸ºä»…åœ¨ç¨³æ€è€¦åˆ(homeostatic coupling)â€”â€”å³ä¼™ä¼´çš„ç—›è‹¦èƒ½å¤Ÿç›´æ¥å¹²æ‰°è‡ªèº«ç¨³æ€æ—¶æ‰ä¼šäº§ç”Ÿã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°æ™ºèƒ½ä½“å¯ä»¥é€šè¿‡è§£ç ä¼™ä¼´çš„å¤–éƒ¨æƒ…ç»ªçŠ¶æ€(emotive states)æ¥å­¦ä¹ å¹¶æ¨æ–­å…¶å†…éƒ¨ç¨³æ€ã€‚é€šè¿‡å‚è€ƒè‡ªèº«çš„ç”Ÿç†ç›¸ä¼¼æ€§ï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿå®ç°ä»å¤–åœ¨è¡¨ç°åˆ°å†…åœ¨çŠ¶æ€çš„åå‘æ˜ å°„ã€‚æœ€ç»ˆï¼Œè¯¥å·¥ä½œå±•ç¤ºäº†å½“ç¨³æ€æ™ºèƒ½ä½“å­¦ä¼šç†è§£å¹¶æ„ŸåŒèº«å—åœ°ä½“å¯Ÿä»–äººæƒ…ç»ªæ—¶ï¼Œäº²ç¤¾ä¼šè¡Œä¸ºä¾¿ä¼šéšä¹‹å‡ºç°ã€‚",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "Preprint. Unver review",
      "pdf_url": "https://arxiv.org/pdf/2506.12894v1",
      "published_date": "2025-06-15 15:49:21 UTC",
      "updated_date": "2025-06-15 15:49:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:08:18.992986+00:00"
    },
    {
      "arxiv_id": "2506.12891v1",
      "title": "Evolutionary Developmental Biology Can Serve as the Conceptual Foundation for a New Design Paradigm in Artificial Intelligence",
      "title_zh": "æ¼”åŒ–å‘è‚²ç”Ÿç‰©å­¦å¯ä½œä¸ºäººå·¥æ™ºèƒ½æ–°å‹è®¾è®¡èŒƒå¼çš„æ¦‚å¿µåŸºç¡€",
      "authors": [
        "Zeki Doruk Erden",
        "Boi Faltings"
      ],
      "abstract": "Artificial intelligence (AI), propelled by advancements in machine learning, has made significant strides in solving complex tasks. However, the current neural network-based paradigm, while effective, is heavily constrained by inherent limitations, primarily a lack of structural organization and a progression of learning that displays undesirable properties. As AI research progresses without a unifying framework, it either tries to patch weaknesses heuristically or draws loosely from biological mechanisms without strong theoretical foundations. Meanwhile, the recent paradigm shift in evolutionary understanding -- driven primarily by evolutionary developmental biology (EDB) -- has been largely overlooked in AI literature, despite a striking analogy between the Modern Synthesis and contemporary machine learning, evident in their shared assumptions, approaches, and limitations upon careful analysis. Consequently, the principles of adaptation from EDB that reshaped our understanding of the evolutionary process can also form the foundation of a unifying conceptual framework for the next design philosophy in AI, going beyond mere inspiration and grounded firmly in biology's first principles. This article provides a detailed overview of the analogy between the Modern Synthesis and modern machine learning, and outlines the core principles of a new AI design paradigm based on insights from EDB. To exemplify our analysis, we also present two learning system designs grounded in specific developmental principles -- regulatory connections, somatic variation and selection, and weak linkage -- that resolve multiple major limitations of contemporary machine learning in an organic manner, while also providing deeper insights into the role of these mechanisms in biological evolution.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºå°†æ¼”åŒ–å‘è‚²ç”Ÿç‰©å­¦ï¼ˆEvolutionary Developmental Biology, EDBï¼‰ä½œä¸ºäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ–°è®¾è®¡èŒƒå¼çš„æ¦‚å¿µåŸºç¡€ï¼Œä»¥è§£å†³å½“å‰åŸºäºç¥ç»ç½‘ç»œçš„æœºå™¨å­¦ä¹ åœ¨ç»“æ„ç»„ç»‡å’Œå­¦ä¹ è¿‡ç¨‹ä¸­çš„å±€é™æ€§ã€‚ä½œè€…æŒ‡å‡ºï¼Œç°ä»£æœºå™¨å­¦ä¹ ä¸ç”Ÿç‰©å­¦ä¸­çš„â€œç°ä»£ç»¼åˆè®ºâ€ï¼ˆModern Synthesisï¼‰åœ¨å‡è®¾å’Œå±€é™æ€§ä¸Šå…·æœ‰æƒŠäººçš„ç›¸ä¼¼æ€§ï¼Œè€Œ EDB çš„é€‚åº”æ€§åŸåˆ™å¯ä»¥ä¸º AI æä¾›æ›´åšå®çš„ç†è®ºæ”¯æ’‘ã€‚è¯¥ç ”ç©¶è¶…è¶Šäº†ç®€å•çš„ç”Ÿç‰©çµæ„Ÿå¯å‘ï¼Œæ—¨åœ¨å°†ç”Ÿç‰©å­¦çš„ç¬¬ä¸€æ€§åŸç†èå…¥ AI çš„è®¾è®¡å“²å­¦ä¸­ï¼Œæ„å»ºä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ã€‚æ–‡ä¸­è¯¦ç»†å¯¹æ¯”äº†ç°ä»£ç»¼åˆè®ºä¸ç°ä»£æœºå™¨å­¦ä¹ çš„ç±»æ¯”å…³ç³»ï¼Œå¹¶æ¦‚è¿°äº†åŸºäº EDB è§è§£çš„æ–°èŒƒå¼çš„æ ¸å¿ƒåŸåˆ™ã€‚ä¸ºéªŒè¯è¯¥åˆ†æï¼Œç ”ç©¶å±•ç¤ºäº†ä¸¤ç§åŸºäºå…·ä½“å‘è‚²åŸåˆ™ï¼ˆå¦‚ regulatory connectionsã€somatic variation and selection å’Œ weak linkageï¼‰çš„å­¦ä¹ ç³»ç»Ÿè®¾è®¡ã€‚è¿™äº›è®¾è®¡ä»¥æœ‰æœºçš„æ–¹å¼è§£å†³äº†å½“ä»£æœºå™¨å­¦ä¹ çš„å¤šé¡¹é‡å¤§ç¼ºé™·ï¼ŒåŒæ—¶ä¸ºç†è§£è¿™äº›æœºåˆ¶åœ¨ç”Ÿç‰©æ¼”åŒ–ä¸­çš„ä½œç”¨æä¾›äº†æ›´æ·±å±‚çš„è§è§£ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.12891v1",
      "published_date": "2025-06-15 15:41:44 UTC",
      "updated_date": "2025-06-15 15:41:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:08:23.738356+00:00"
    },
    {
      "arxiv_id": "2506.17288v1",
      "title": "SlimRAG: Retrieval without Graphs via Entity-Aware Context Selection",
      "title_zh": "SlimRAGï¼šåŸºäºå®ä½“æ„ŸçŸ¥ä¸Šä¸‹æ–‡é€‰æ‹©çš„æ— å›¾æ£€ç´¢",
      "authors": [
        "Jiale Zhang",
        "Jiaxiang Chen",
        "Zhucong Li",
        "Jie Ding",
        "Kui Zhao",
        "Zenglin Xu",
        "Xin Pang",
        "Yinghui Xu"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) enhances language models by incorporating external knowledge at inference time. However, graph-based RAG systems often suffer from structural overhead and imprecise retrieval: they require costly pipelines for entity linking and relation extraction, yet frequently return subgraphs filled with loosely related or tangential content. This stems from a fundamental flaw -- semantic similarity does not imply semantic relevance. We introduce SlimRAG, a lightweight framework for retrieval without graphs. SlimRAG replaces structure-heavy components with a simple yet effective entity-aware mechanism. At indexing time, it constructs a compact entity-to-chunk table based on semantic embeddings. At query time, it identifies salient entities, retrieves and scores associated chunks, and assembles a concise, contextually relevant input -- without graph traversal or edge construction. To quantify retrieval efficiency, we propose Relative Index Token Utilization (RITU), a metric measuring the compactness of retrieved content. Experiments across multiple QA benchmarks show that SlimRAG outperforms strong flat and graph-based baselines in accuracy while reducing index size and RITU (e.g., 16.31 vs. 56+), highlighting the value of structure-free, entity-centric context selection. The code will be released soon. https://github.com/continue-ai-company/SlimRAG",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SlimRAGï¼Œä¸€ç§æ— éœ€å›¾ç»“æ„çš„è½»é‡çº§æ£€ç´¢å¢å¼ºç”Ÿæˆ (Retrieval-Augmented Generation, RAG) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»ŸåŸºäºå›¾çš„ RAG ç³»ç»Ÿä¸­å­˜åœ¨çš„ç»“æ„å¼€é”€å¤§ä»¥åŠæ£€ç´¢ä¸ç²¾å‡†ç­‰é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„å®ä½“æ„ŸçŸ¥æœºåˆ¶ (entity-aware mechanism) å–ä»£äº†å¤æ‚çš„å›¾ç»„ä»¶ï¼Œé¿å…äº†æˆæœ¬é«˜æ˜‚çš„å®ä½“é“¾æ¥å’Œå…³ç³»æŠ½å–è¿‡ç¨‹ã€‚åœ¨ç´¢å¼•é˜¶æ®µï¼ŒSlimRAG åŸºäºè¯­ä¹‰åµŒå…¥æ„å»ºç´§å‡‘çš„å®ä½“åˆ°å—è¡¨æ ¼ (entity-to-chunk table)ï¼›åœ¨æŸ¥è¯¢é˜¶æ®µï¼Œå®ƒé€šè¿‡è¯†åˆ«å…³é”®å®ä½“å¹¶å¯¹å…³è”å—è¿›è¡Œæ£€ç´¢ä¸è¯„åˆ†ï¼Œæ— éœ€è¿›è¡Œå›¾éå†æˆ–è¾¹ç¼˜æ„å»ºå³å¯ç»„è£…å‡ºç®€æ´ä¸”ä¸Šä¸‹æ–‡ç›¸å…³çš„è¾“å…¥ã€‚ä¸ºäº†è¡¡é‡æ£€ç´¢æ•ˆç‡ï¼Œç ”ç©¶è€…æå‡ºäº†ç›¸å¯¹ç´¢å¼•ä»¤ç‰Œåˆ©ç”¨ç‡ (Relative Index Token Utilization, RITU) æŒ‡æ ‡ï¼Œç”¨ä»¥é‡åŒ–æ£€ç´¢å†…å®¹çš„ç´§å‡‘ç¨‹åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSlimRAG åœ¨å¤šä¸ªé—®ç­” (QA) åŸºå‡†æµ‹è¯•ä¸­çš„å‡†ç¡®ç‡å‡ä¼˜äºå¼ºåŠ›çš„å¹³é¢å’ŒåŸºäºå›¾çš„åŸºå‡†æ¨¡å‹ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°äº†ç´¢å¼•å¤§å°å¹¶é™ä½äº† RITU å€¼ã€‚è¯¥ç ”ç©¶è¯æ˜äº†æ— ç»“æ„ã€ä»¥å®ä½“ä¸ºä¸­å¿ƒçš„ä¸Šä¸‹æ–‡é€‰æ‹©åœ¨æå‡æ£€ç´¢æ•ˆç‡å’Œå‡†ç¡®æ€§æ–¹é¢çš„å·¨å¤§ä»·å€¼ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.17288v1",
      "published_date": "2025-06-15 15:36:17 UTC",
      "updated_date": "2025-06-15 15:36:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:08:32.750943+00:00"
    },
    {
      "arxiv_id": "2506.12879v1",
      "title": "Exploring the Potential of Metacognitive Support Agents for Human-AI Co-Creation",
      "title_zh": "æ¢ç´¢å…ƒè®¤çŸ¥æ”¯æŒæ™ºèƒ½ä½“åœ¨äººæœºååŒåˆ›ä½œä¸­çš„æ½œåŠ›",
      "authors": [
        "Frederic Gmeiner",
        "Kaitao Luo",
        "Ye Wang",
        "Kenneth Holstein",
        "Nikolas Martelaro"
      ],
      "abstract": "Despite the potential of generative AI (GenAI) design tools to enhance design processes, professionals often struggle to integrate AI into their workflows. Fundamental cognitive challenges include the need to specify all design criteria as distinct parameters upfront (intent formulation) and designers' reduced cognitive involvement in the design process due to cognitive offloading, which can lead to insufficient problem exploration, underspecification, and limited ability to evaluate outcomes. Motivated by these challenges, we envision novel metacognitive support agents that assist designers in working more reflectively with GenAI. To explore this vision, we conducted exploratory prototyping through a Wizard of Oz elicitation study with 20 mechanical designers probing multiple metacognitive support strategies. We found that agent-supported users created more feasible designs than non-supported users, with differing impacts between support strategies. Based on these findings, we discuss opportunities and tradeoffs of metacognitive support agents and considerations for future AI-based design tools.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½(GenAI)åœ¨è¾…åŠ©è®¾è®¡ä¸­çš„åº”ç”¨å›°å¢ƒï¼ŒæŒ‡å‡ºè®¾è®¡å¸ˆå¸¸é¢ä¸´æ„å›¾åˆ¶å®š(intent formulation)å›°éš¾ä»¥åŠè®¤çŸ¥å¸è½½(cognitive offloading)å¯¼è‡´çš„æ·±å±‚æ¢ç´¢ä¸è¶³ç­‰æŒ‘æˆ˜ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº†å…ƒè®¤çŸ¥æ”¯æŒæ™ºèƒ½ä½“(metacognitive support agents)çš„æ„¿æ™¯ï¼Œæ—¨åœ¨å¼•å¯¼è®¾è®¡å¸ˆåœ¨ä½¿ç”¨GenAIæ—¶è¿›è¡Œæ›´å…·åæ€æ€§çš„åä½œã€‚é€šè¿‡å¯¹20åæœºæ¢°è®¾è®¡å¸ˆè¿›è¡Œçš„Wizard of Ozè¯±å¯¼ç ”ç©¶ï¼Œå›¢é˜Ÿæ¢ç´¢å¹¶éªŒè¯äº†å¤šç§å…ƒè®¤çŸ¥æ”¯æŒç­–ç•¥ï¼Œå‘ç°å—æ”¯æŒçš„ç”¨æˆ·èƒ½å¤Ÿåˆ›ä½œå‡ºå¯è¡Œæ€§æ›´é«˜çš„è®¾è®¡æ–¹æ¡ˆã€‚ç ”ç©¶ç»“æœæ­ç¤ºäº†ä¸åŒæ”¯æŒç­–ç•¥å¯¹è®¾è®¡äº§å‡ºçš„å·®å¼‚åŒ–å½±å“ï¼Œå¹¶æ·±å…¥åˆ†æäº†å…ƒè®¤çŸ¥æ”¯æŒæ™ºèƒ½ä½“åœ¨äººæœºååŒåˆ›ä½œä¸­çš„æœºé‡ä¸æƒè¡¡ï¼Œä¸ºæœªæ¥AIè¾…åŠ©è®¾è®¡å·¥å…·çš„ç ”å‘æä¾›äº†å…³é”®å‚è€ƒã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "26 pages, to be published in the proceedings of the Designing Interactive Systems Conference (DIS'25)",
      "pdf_url": "https://arxiv.org/pdf/2506.12879v1",
      "published_date": "2025-06-15 15:09:37 UTC",
      "updated_date": "2025-06-15 15:09:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:08:53.315840+00:00"
    },
    {
      "arxiv_id": "2506.14837v1",
      "title": "Improved Iterative Refinement for Chart-to-Code Generation via Structured Instruction",
      "title_zh": "åŸºäºç»“æ„åŒ–æŒ‡ä»¤çš„å›¾è¡¨åˆ°ä»£ç ç”Ÿæˆæ”¹è¿›è¿­ä»£ç»†åŒ–",
      "authors": [
        "Chengzhi Xu",
        "Yuyang Wang",
        "Lai Wei",
        "Lichao Sun",
        "Weiran Huang"
      ],
      "abstract": "Recently, multimodal large language models (MLLMs) have attracted increasing research attention due to their powerful visual understanding capabilities. While they have achieved impressive results on various vision tasks, their performance on chart-to-code generation remains suboptimal. This task requires MLLMs to generate executable code that can reproduce a given chart, demanding not only precise visual understanding but also accurate translation of visual elements into structured code. Directly prompting MLLMs to perform this complex task often yields unsatisfactory results. To address this challenge, we propose {ChartIR}, an iterative refinement method based on structured instruction. First, we distinguish two tasks: visual understanding and code translation. To accomplish the visual understanding component, we design two types of structured instructions: description and difference. The description instruction captures the visual elements of the reference chart, while the difference instruction characterizes the discrepancies between the reference chart and the generated chart. These instructions effectively transform visual features into language representations, thereby facilitating the subsequent code translation process. Second, we decompose the overall chart generation pipeline into two stages: initial code generation and iterative refinement, enabling progressive enhancement of the final output. Experimental results show that, compared to other method, our method achieves superior performance on both the open-source model Qwen2-VL and the closed-source model GPT-4o.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å›¾è¡¨åˆ°ä»£ç ç”Ÿæˆï¼ˆchart-to-code generationï¼‰ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³çš„é—®é¢˜ï¼Œæå‡ºäº† ChartIRï¼Œä¸€ç§åŸºäºç»“æ„åŒ–æŒ‡ä»¤ï¼ˆstructured instructionï¼‰çš„è¿­ä»£ç²¾ç‚¼æ–¹æ³•ã€‚è¯¥æ¡†æ¶å°†å¤æ‚ä»»åŠ¡è§£è€¦ä¸ºè§†è§‰ç†è§£å’Œä»£ç ç¿»è¯‘ï¼Œé€šè¿‡è®¾è®¡ description æŒ‡ä»¤æ•æ‰å‚è€ƒå›¾è¡¨çš„å…³é”®è§†è§‰å…ƒç´ ã€‚åŒæ—¶ï¼Œåˆ©ç”¨ difference æŒ‡ä»¤ç²¾ç¡®æè¿°å‚è€ƒå›¾è¡¨ä¸ç”Ÿæˆçš„å›¾è¡¨ä¹‹é—´çš„å·®å¼‚ï¼Œå°†è§†è§‰ç‰¹å¾è½¬åŒ–ä¸ºæ›´åˆ©äºæ¨¡å‹å¤„ç†çš„è¯­è¨€è¡¨ç¤ºã€‚ChartIR è¿›ä¸€æ­¥å°†ç”Ÿæˆæµç¨‹åˆ†ä¸ºåˆå§‹ä»£ç ç”Ÿæˆä¸è¿­ä»£ç²¾ç‚¼ï¼ˆiterative refinementï¼‰ä¸¤ä¸ªé˜¶æ®µï¼Œä»è€Œå®ç°å¯¹æœ€ç»ˆè¾“å‡ºçš„å¾ªåºæ¸è¿›å¼ä¼˜åŒ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¼€æºæ¨¡å‹ Qwen2-VL å’Œé—­æºæ¨¡å‹ GPT-4o ä¸Šå‡è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æå‡äº†å›¾è¡¨ä»£ç ç”Ÿæˆçš„å‡†ç¡®æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.14837v1",
      "published_date": "2025-06-15 14:10:16 UTC",
      "updated_date": "2025-06-15 14:10:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:08:51.836069+00:00"
    },
    {
      "arxiv_id": "2506.13825v1",
      "title": "The Reflexive Integrated Information Unit: A Differentiable Primitive for Artificial Consciousness",
      "title_zh": "è‡ªåæ•´åˆä¿¡æ¯å•å…ƒï¼šä¸€ç§é¢å‘äººå·¥æ„è¯†çš„å¯å¾®åŸºå…ƒ",
      "authors": [
        "Gnankan Landry Regis N'guessan",
        "Issa Karambal"
      ],
      "abstract": "Research on artificial consciousness lacks the equivalent of the perceptron: a small, trainable module that can be copied, benchmarked, and iteratively improved. We introduce the Reflexive Integrated Information Unit (RIIU), a recurrent cell that augments its hidden state $h$ with two additional vectors: (i) a meta-state $Î¼$ that records the cell's own causal footprint, and (ii) a broadcast buffer $B$ that exposes that footprint to the rest of the network. A sliding-window covariance and a differentiable Auto-$Î¦$ surrogate let each RIIU maximize local information integration online. We prove that RIIUs (1) are end-to-end differentiable, (2) compose additively, and (3) perform $Î¦$-monotone plasticity under gradient ascent. In an eight-way Grid-world, a four-layer RIIU agent restores $>90\\%$ reward within 13 steps after actuator failure, twice as fast as a parameter-matched GRU, while maintaining a non-zero Auto-$Î¦$ signal. By shrinking \"consciousness-like\" computation down to unit scale, RIIUs turn a philosophical debate into an empirical mathematical problem.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†è‡ªåé›†æˆä¿¡æ¯å•å…ƒ(Reflexive Integrated Information Unit, RIIU)ï¼Œè¿™æ˜¯ä¸€ç§å¯å¾®åˆ†çš„åŸºç¡€è®¡ç®—å•å…ƒï¼Œæ—¨åœ¨å¡«è¡¥äººå·¥æ„è¯†ç ”ç©¶ä¸­ç¼ºä¹ç±»ä¼¼æ„ŸçŸ¥å™¨(perceptron)çš„å¯è®­ç»ƒæ¨¡å—çš„ç©ºç™½ã€‚RIIU ä½œä¸ºä¸€ä¸ªå¾ªç¯å•å…ƒï¼Œé€šè¿‡å…ƒçŠ¶æ€($\\mu$)è®°å½•è‡ªèº«çš„å› æœè¶³è¿¹(causal footprint)ï¼Œå¹¶åˆ©ç”¨å¹¿æ’­ç¼“å†²åŒº($B$)å°†è¯¥è¶³è¿¹æš´éœ²ç»™ç½‘ç»œå…¶ä»–éƒ¨åˆ†ã€‚è¯¥å•å…ƒç»“åˆæ»‘åŠ¨çª—å£åæ–¹å·®å’Œå¯å¾®åˆ†çš„ Auto-$\\Phi$ ä»£ç†æ¨¡å‹ï¼Œå®ç°äº†åœ¨çº¿æœ€å¤§åŒ–å±€éƒ¨ä¿¡æ¯é›†æˆã€‚ç ”ç©¶è¯æ˜äº† RIIU å…·æœ‰ç«¯åˆ°ç«¯å¯å¾®åˆ†æ€§ã€åŠ æ³•å¯ç»„åˆæ€§ï¼Œå¹¶åœ¨æ¢¯åº¦ä¸Šå‡ä¸‹è¡¨ç°å‡º $\\Phi$ å•è°ƒå¯å¡‘æ€§($\\Phi$-monotone plasticity)ã€‚åœ¨å…«å‘ç½‘æ ¼ä¸–ç•Œ(Grid-world)å®éªŒä¸­ï¼Œå››å±‚ RIIU æ™ºèƒ½ä½“åœ¨æ‰§è¡Œå™¨æ•…éšœåä»…ç”¨ 13 æ­¥å³å¯æ¢å¤ 90% ä»¥ä¸Šçš„å¥–åŠ±ï¼Œæ¢å¤é€Ÿåº¦æ˜¯å‚æ•°åŒ¹é…çš„ GRU çš„ä¸¤å€ã€‚é€šè¿‡å°†ç±»æ„è¯†è®¡ç®—ç¼©å°åˆ°å•å…ƒè§„æ¨¡ï¼ŒRIIU æˆåŠŸå°†å…³äºæ„è¯†çš„å“²å­¦è®¨è®ºè½¬åŒ–ä¸ºäº†å®è¯æ•°å­¦é—®é¢˜ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.13825v1",
      "published_date": "2025-06-15 14:07:59 UTC",
      "updated_date": "2025-06-15 14:07:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:09:00.190269+00:00"
    },
    {
      "arxiv_id": "2506.12851v2",
      "title": "KungfuBot: Physics-Based Humanoid Whole-Body Control for Learning Highly-Dynamic Skills",
      "title_zh": "KungfuBotï¼šé¢å‘é«˜åŠ¨æ€æŠ€èƒ½å­¦ä¹ çš„åŸºäºç‰©ç†çš„ç±»äººæœºå™¨äººå…¨èº«æ§åˆ¶",
      "authors": [
        "Weiji Xie",
        "Jinrui Han",
        "Jiakun Zheng",
        "Huanyu Li",
        "Xinzhe Liu",
        "Jiyuan Shi",
        "Weinan Zhang",
        "Chenjia Bai",
        "Xuelong Li"
      ],
      "abstract": "Humanoid robots are promising to acquire various skills by imitating human behaviors. However, existing algorithms are only capable of tracking smooth, low-speed human motions, even with delicate reward and curriculum design. This paper presents a physics-based humanoid control framework, aiming to master highly-dynamic human behaviors such as Kungfu and dancing through multi-steps motion processing and adaptive motion tracking. For motion processing, we design a pipeline to extract, filter out, correct, and retarget motions, while ensuring compliance with physical constraints to the maximum extent. For motion imitation, we formulate a bi-level optimization problem to dynamically adjust the tracking accuracy tolerance based on the current tracking error, creating an adaptive curriculum mechanism. We further construct an asymmetric actor-critic framework for policy training. In experiments, we train whole-body control policies to imitate a set of highly-dynamic motions. Our method achieves significantly lower tracking errors than existing approaches and is successfully deployed on the Unitree G1 robot, demonstrating stable and expressive behaviors. The project page is https://kungfu-bot.github.io.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† KungfuBotï¼Œä¸€ä¸ªåŸºäºç‰©ç†çš„ç±»äººæœºå™¨äººå…¨èº«æ§åˆ¶æ¡†æ¶ï¼Œæ—¨åœ¨è®©æœºå™¨äººæŒæ¡åŠŸå¤«å’Œèˆè¹ˆç­‰é«˜åŠ¨æ€æŠ€èƒ½ã€‚é’ˆå¯¹ç°æœ‰ç®—æ³•ä»…èƒ½è·Ÿè¸ªä½é€Ÿå¹³ç¨³åŠ¨ä½œçš„å±€é™ï¼Œè¯¥æ¡†æ¶è®¾è®¡äº†ä¸€å¥—æ¶µç›–è¿åŠ¨æå–ã€æ»¤æ³¢ã€ä¿®æ­£å’Œé‡å®šå‘ï¼ˆRetargetingï¼‰çš„å¤„ç†æµç¨‹ï¼Œä»¥ç¡®ä¿åŠ¨ä½œæœ€å¤§é™åº¦ç¬¦åˆç‰©ç†çº¦æŸã€‚åœ¨è¿åŠ¨æ¨¡ä»¿æ–¹é¢ï¼Œç ”ç©¶é€šè¿‡æ„å»ºåŒå±‚ä¼˜åŒ–ï¼ˆBi-level optimizationï¼‰é—®é¢˜ï¼Œæ ¹æ®å®æ—¶è¯¯å·®åŠ¨æ€è°ƒæ•´è·Ÿè¸ªç²¾åº¦å®¹å·®ï¼Œå½¢æˆäº†ä¸€ç§è‡ªé€‚åº”è¯¾ç¨‹å­¦ä¹ ï¼ˆAdaptive curriculum mechanismï¼‰æœºåˆ¶ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜é‡‡ç”¨äº†éå¯¹ç§°æ¼”å‘˜-è¯„è®ºå®¶ï¼ˆAsymmetric actor-criticï¼‰æ¶æ„è¿›è¡Œç­–ç•¥è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ¯”ç°æœ‰æ–¹æ³•çš„è·Ÿè¸ªè¯¯å·®æ˜¾è‘—é™ä½ï¼Œå¹¶å·²æˆåŠŸéƒ¨ç½²åœ¨ Unitree G1 æœºå™¨äººä¸Šï¼Œå±•ç¤ºäº†ç¨³å®šä¸”å…·æœ‰è¡¨ç°åŠ›çš„å…¨èº«æ§åˆ¶èƒ½åŠ›ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "NeurIPS 2025. Project Page: https://kungfu-bot.github.io/",
      "pdf_url": "https://arxiv.org/pdf/2506.12851v2",
      "published_date": "2025-06-15 13:58:53 UTC",
      "updated_date": "2025-10-27 01:13:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:08:57.791382+00:00"
    },
    {
      "arxiv_id": "2507.00019v1",
      "title": "Quantum Inspired Encoding Strategies for Machine Learning Models: Proposing and Evaluating Instance Level, Global Discrete, and Class Conditional Representations",
      "title_zh": "æœºå™¨å­¦ä¹ æ¨¡å‹çš„é‡å­å¯å‘å¼ç¼–ç ç­–ç•¥ï¼šå®ä¾‹çº§ã€å…¨å±€ç¦»æ•£åŠç±»åˆ«æ¡ä»¶è¡¨ç¤ºçš„æå‡ºä¸è¯„ä¼°",
      "authors": [
        "Minati Rath",
        "Hema Date"
      ],
      "abstract": "In this study, we propose, evaluate and compare three quantum inspired data encoding strategies, Instance Level Strategy (ILS), Global Discrete Strategy (GDS) and Class Conditional Value Strategy (CCVS), for transforming classical data into quantum data for use in pure classical machine learning models. The primary objective is to reduce high encoding time while ensuring correct encoding values and analyzing their impact on classification performance. The Instance Level Strategy treats each row of dataset independently; mimics local quantum states. Global Discrete Value Based encoding strategy maps all unique feature values across the full dataset to quantum states uniformly. In contrast, the Class conditional Value based encoding strategy encodes unique values separately for each class, preserving class dependent information.\n  We apply these encoding strategies to a classification task and assess their impact on en-coding efficiency, correctness, model accuracy, and computational cost. By analyzing the trade offs between encoding time, precision, and predictive performance, this study provides insights into optimizing quantum inspired data transformations for classical machine learning workflows.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å°†ç»å…¸æ•°æ®è½¬åŒ–ä¸º Quantum Data ç”¨äºç»å…¸æœºå™¨å­¦ä¹ æ¨¡å‹çš„éœ€æ±‚ï¼Œæå‡ºå¹¶è¯„ä¼°äº†ä¸‰ç§ Quantum Inspired æ•°æ®ç¼–ç ç­–ç•¥ï¼šInstance Level Strategy (ILS)ã€Global Discrete Strategy (GDS) å’Œ Class Conditional Value Strategy (CCVS)ã€‚è¿™äº›ç­–ç•¥çš„æ ¸å¿ƒç›®æ ‡æ˜¯åœ¨ç¡®ä¿ç¼–ç å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œé™ä½ç¼–ç æ—¶é—´æˆæœ¬ï¼Œå¹¶æ·±å…¥åˆ†æå…¶å¯¹åˆ†ç±»æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚Instance Level Strategy (ILS) å°†æ•°æ®é›†çš„æ¯ä¸€è¡Œè§†ä¸ºç‹¬ç«‹ä¸ªä½“è¿›è¡Œå¤„ç†ï¼Œä»¥æ­¤æ¨¡æ‹Ÿå±€éƒ¨ Quantum Statesï¼›Global Discrete Strategy (GDS) åˆ™å°†æ•´ä¸ªæ•°æ®é›†ä¸­æ‰€æœ‰å”¯ä¸€çš„ç‰¹å¾å€¼ç»Ÿä¸€æ˜ å°„è‡³ Quantum Statesã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒClass Conditional Value Strategy (CCVS) ä¸ºä¸åŒç±»åˆ«åˆ†åˆ«ç¼–ç å”¯ä¸€å€¼ï¼Œä»è€Œæœ‰æ•ˆåœ°ä¿ç•™äº†ç±»åˆ«ä¾èµ–ä¿¡æ¯ã€‚é€šè¿‡åœ¨åˆ†ç±»ä»»åŠ¡ä¸­å¯¹ç¼–ç æ•ˆç‡ã€æ­£ç¡®æ€§ã€æ¨¡å‹å‡†ç¡®ç‡åŠè®¡ç®—æˆæœ¬è¿›è¡Œç»¼åˆè¯„ä¼°ï¼Œè¯¥ç ”ç©¶æ­ç¤ºäº†ç¼–ç æ—¶é—´ã€ç²¾åº¦ä¸é¢„æµ‹æ€§èƒ½ä¹‹é—´çš„æƒè¡¡å…³ç³»ã€‚è¯¥æˆæœä¸ºåœ¨ç»å…¸æœºå™¨å­¦ä¹ å·¥ä½œæµä¸­ä¼˜åŒ– Quantum Inspired æ•°æ®è½¬æ¢æä¾›äº†é‡è¦çš„å®è·µæŒ‡å¯¼ä¸è§è§£ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "quant-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.00019v1",
      "published_date": "2025-06-15 13:50:57 UTC",
      "updated_date": "2025-06-15 13:50:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:09:01.585658+00:00"
    },
    {
      "arxiv_id": "2506.18919v3",
      "title": "MemeMind: A Large-Scale Multimodal Dataset with Chain-of-Thought Reasoning for Harmful Meme Detection",
      "title_zh": "MemeMindï¼šèåˆé“¾å¼æ€ç»´æ¨ç†çš„å¤§è§„æ¨¡æœ‰å®³æ¨¡å› æ£€æµ‹å¤šæ¨¡æ€æ•°æ®é›†",
      "authors": [
        "Hexiang Gu",
        "Qifan Yu",
        "Yuan Liu",
        "Zikang Li",
        "Saihui Hou",
        "Jian Zhao",
        "Zhaofeng He"
      ],
      "abstract": "As a multimodal medium combining images and text, memes frequently convey implicit harmful content through metaphors and humor, rendering the detection of harmful memes a complex and challenging task. Although recent studies have made progress in detection accuracy and interpretability, large-scale, high-quality datasets for harmful memes remain scarce, and current methods still struggle to capture implicit risks and nuanced semantics. Thus, we construct MemeMind, a large-scale harmful meme dataset. Aligned with the international standards and the context of internet, MemeMind provides detailed Chain-of-Thought (CoT) reasoning annotations to support fine-grained analysis of implicit intentions in memes. Based on this dataset, we further propose MemeGuard, a reasoning-oriented multimodal detection model that significantly improves both the accuracy of harmful meme detection and the interpretability of model decisions. Extensive experimental results demonstrate that MemeGuard outperforms existing state-of-the-art methods on the MemeMind dataset, establishing a solid foundation for future research in harmful meme detection.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç½‘ç»œè¡¨æƒ…åŒ…(Memes)é€šè¿‡éšå–»å’Œå¹½é»˜ä¼ è¾¾éšå«å±å®³å†…å®¹å¯¼è‡´æ£€æµ‹å›°éš¾çš„é—®é¢˜ï¼Œæ„å»ºäº†å¤§è§„æ¨¡æœ‰å®³è¡¨æƒ…åŒ…æ•°æ®é›† MemeMindã€‚è¯¥æ•°æ®é›†éµå¾ªå›½é™…æ ‡å‡†å¹¶ç»“åˆäº’è”ç½‘è¯­å¢ƒï¼Œæä¾›äº†è¯¦ç»†çš„é“¾å¼æ€ç»´(Chain-of-Thought, CoT)æ¨ç†æ³¨é‡Šï¼Œä»¥æ”¯æŒå¯¹è¡¨æƒ…åŒ…éšå«æ„å›¾çš„ç»†ç²’åº¦åˆ†æã€‚åŸºäºæ­¤æ•°æ®é›†ï¼Œç ”ç©¶è€…è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§é¢å‘æ¨ç†çš„å¤šæ¨¡æ€æ£€æµ‹æ¨¡å‹ MemeGuardï¼Œæ—¨åœ¨æå‡æœ‰å®³è¡¨æƒ…åŒ…æ£€æµ‹çš„å‡†ç¡®æ€§ä¸æ¨¡å‹å†³ç­–çš„å¯è§£é‡Šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMemeGuard åœ¨ MemeMind æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›(State-of-the-art)æ–¹æ³•ã€‚è¯¥å·¥ä½œé€šè¿‡æ•´åˆé«˜è´¨é‡æ•°æ®é›†ä¸æ¨ç†å¯¼å‘çš„æ¨¡å‹æ¶æ„ï¼Œä¸ºæœ‰å®³è¡¨æƒ…åŒ…æ£€æµ‹é¢†åŸŸçš„åç»­ç ”ç©¶å¥ å®šäº†åšå®åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18919v3",
      "published_date": "2025-06-15 13:45:30 UTC",
      "updated_date": "2026-01-06 09:25:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:09:07.702939+00:00"
    },
    {
      "arxiv_id": "2506.12846v5",
      "title": "VFEFL: Privacy-Preserving Federated Learning against Malicious Clients via Verifiable Functional Encryption",
      "title_zh": "VFEFLï¼šåŸºäºå¯éªŒè¯å‡½æ•°åŠ å¯†çš„æŠµå¾¡æ¶æ„å®¢æˆ·ç«¯éšç§ä¿æŠ¤è”é‚¦å­¦ä¹ ",
      "authors": [
        "Nina Cai",
        "Jinguang Han",
        "Weizhi Meng"
      ],
      "abstract": "Federated learning is a promising distributed learning paradigm that enables collaborative model training without exposing local client data, thereby protecting data privacy. However, it also brings new threats and challenges. The advancement of model inversion attacks has rendered the plaintext transmission of local models insecure, while the distributed nature of federated learning makes it particularly vulnerable to attacks raised by malicious clients. To protect data privacy and prevent malicious client attacks, this paper proposes a privacy-preserving Federated Learning framework based on Verifiable Functional Encryption (VFEFL), without a non-colluding dual-server assumption or additional trusted third-party. Specifically, we propose a novel Cross-Ciphertext Decentralized Verifiable Functional Encryption (CC-DVFE) scheme that enables the verification of specific relationships over multi-dimensional ciphertexts. This scheme is formally treated, in terms of definition, security model and security proof. Furthermore, based on the proposed CC-DVFE scheme, we design a privacy-preserving federated learning framework that incorporates a novel robust aggregation rule to detect malicious clients, enabling the effective training of high-accuracy models under adversarial settings. Finally, we provide the formal analysis and empirical evaluation of VFEFL. The results demonstrate that our approach achieves the desired privacy protection, robustness, verifiability and fidelity, while eliminating the reliance on non-colluding dual-server assumption or trusted third parties required by most existing methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†VFEFLï¼Œä¸€ç§åŸºäºå¯éªŒè¯å‡½æ•°åŠ å¯†(Verifiable Functional Encryption)çš„éšç§ä¿æŠ¤è”é‚¦å­¦ä¹ (Federated Learning)æ¡†æ¶ï¼Œæ—¨åœ¨åº”å¯¹æ¶æ„å®¢æˆ·ç«¯æ”»å‡»å’Œæ¨¡å‹åæ¼”æ”»å‡»(model inversion attacks)ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§è·¨å¯†æ–‡å»ä¸­å¿ƒåŒ–å¯éªŒè¯å‡½æ•°åŠ å¯†(CC-DVFE)æ–¹æ¡ˆï¼Œå®ç°äº†å¯¹å¤šç»´å¯†æ–‡ç‰¹å®šå…³ç³»çš„æœ‰æ•ˆéªŒè¯ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒVFEFLæ— éœ€éå…±è°‹åŒæœåŠ¡å™¨(non-colluding dual-server)å‡è®¾æˆ–é¢å¤–çš„å¯ä¿¡ç¬¬ä¸‰æ–¹(trusted third-party)ï¼Œæ˜¾è‘—æå‡äº†ç³»ç»Ÿçš„å»ä¸­å¿ƒåŒ–ç¨‹åº¦ã€‚é€šè¿‡ç»“åˆæ–°å‹çš„é²æ£’èšåˆè§„åˆ™(robust aggregation rule)ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿç²¾å‡†æ£€æµ‹æ¶æ„å®¢æˆ·ç«¯ï¼Œç¡®ä¿åœ¨å¯¹æŠ—æ€§ç¯å¢ƒä¸‹ä»èƒ½è®­ç»ƒå‡ºé«˜ç²¾åº¦çš„æ¨¡å‹ã€‚å®éªŒåˆ†æä¸å®è¯è¯„ä¼°è¯æ˜ï¼ŒVFEFLåœ¨å®ç°éšç§ä¿æŠ¤ã€é²æ£’æ€§(robustness)ã€å¯éªŒè¯æ€§å’Œä¿çœŸåº¦(fidelity)çš„åŒæ—¶ï¼Œæ¶ˆé™¤äº†å¯¹ç‰¹å®šä¸­å¿ƒåŒ–ä¿¡ä»»æœºæ„çš„ä¾èµ–ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.12846v5",
      "published_date": "2025-06-15 13:38:40 UTC",
      "updated_date": "2026-01-06 13:23:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:09:15.495259+00:00"
    },
    {
      "arxiv_id": "2506.12841v1",
      "title": "WereWolf-Plus: An Update of Werewolf Game setting Based on DSGBench",
      "title_zh": "WereWolf-Plusï¼šåŸºäº DSGBench çš„ç‹¼äººæ€æ¸¸æˆè®¾ç½®æ›´æ–°",
      "authors": [
        "Xinyuan Xia",
        "Yuanyi Song",
        "Haomin Ma",
        "Jinyu Cai"
      ],
      "abstract": "With the rapid development of LLM-based agents, increasing attention has been given to their social interaction and strategic reasoning capabilities. However, existing Werewolf-based benchmarking platforms suffer from overly simplified game settings, incomplete evaluation metrics, and poor scalability. To address these limitations, we propose WereWolf-Plus, a multi-model, multi-dimensional, and multi-method benchmarking platform for evaluating multi-agent strategic reasoning in the Werewolf game. The platform offers strong extensibility, supporting customizable configurations for roles such as Seer, Witch, Hunter, Guard, and Sheriff, along with flexible model assignment and reasoning enhancement strategies for different roles. In addition, we introduce a comprehensive set of quantitative evaluation metrics for all special roles, werewolves, and the sheriff, and enrich the assessment dimensions for agent reasoning ability, cooperation capacity, and social influence. WereWolf-Plus provides a more flexible and reliable environment for advancing research on inference and strategic interaction within multi-agent communities. Our code is open sourced at https://github.com/MinstrelsyXia/WereWolfPlus.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰ç‹¼äººæ€åŸºå‡†å¹³å°å­˜åœ¨çš„æ¸¸æˆè®¾å®šè¿‡äºç®€åŒ–ã€è¯„ä»·æŒ‡æ ‡ä¸å®Œæ•´åŠå¯æ‰©å±•æ€§å·®ç­‰å±€é™ï¼Œæå‡ºäº† WereWolf-Plusã€‚è¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤šæ™ºèƒ½ä½“æˆ˜ç•¥æ¨ç† (multi-agent strategic reasoning) çš„å¤šæ¨¡å‹ã€å¤šç»´åº¦ã€å¤šæ–¹æ³•åŸºå‡†æµ‹è¯•å¹³å°ã€‚è¯¥å¹³å°å…·å¤‡æå¼ºçš„æ‰©å±•æ€§ï¼Œæ”¯æŒå¯¹ Seerã€Witchã€Hunterã€Guard å’Œ Sheriff ç­‰è§’è‰²è¿›è¡Œè‡ªå®šä¹‰é…ç½®ï¼Œå¹¶å…è®¸ä¸ºä¸åŒè§’è‰²çµæ´»åˆ†é…æ¨¡å‹åŠæ¨ç†å¢å¼ºç­–ç•¥ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…ä¸ºæ‰€æœ‰ç‰¹æ®Šè§’è‰²ã€ç‹¼äººåŠ Sheriff å¼•å…¥äº†ä¸€å¥—å…¨é¢çš„å®šé‡è¯„ä»·æŒ‡æ ‡ï¼Œä»æ¨ç†èƒ½åŠ›ã€åä½œèƒ½åŠ›å’Œç¤¾ä¼šå½±å“åŠ›ç­‰å¤šä¸ªç»´åº¦ä¸°å¯Œäº†è¯„ä¼°ä½“ç³»ã€‚WereWolf-Plus ä¸ºæ¨è¿›å¤šæ™ºèƒ½ä½“ç¤¾åŒºå†…æ¨ç†ä¸æˆ˜ç•¥äº¤äº’çš„ç ”ç©¶æä¾›äº†ä¸€ä¸ªæ›´ä¸ºçµæ´»ä¸”å¯é çš„ç¯å¢ƒã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.12841v1",
      "published_date": "2025-06-15 13:28:41 UTC",
      "updated_date": "2025-06-15 13:28:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:09:18.095389+00:00"
    },
    {
      "arxiv_id": "2506.12839v1",
      "title": "Fair Bayesian Model-Based Clustering",
      "title_zh": "åŸºäºæ¨¡å‹çš„å…¬å¹³è´å¶æ–¯èšç±»",
      "authors": [
        "Jihu Lee",
        "Kunwoong Kim",
        "Yongdai Kim"
      ],
      "abstract": "Fair clustering has become a socially significant task with the advancement of machine learning technologies and the growing demand for trustworthy AI. Group fairness ensures that the proportions of each sensitive group are similar in all clusters. Most existing group-fair clustering methods are based on the $K$-means clustering and thus require the distance between instances and the number of clusters to be given in advance. To resolve this limitation, we propose a fair Bayesian model-based clustering called Fair Bayesian Clustering (FBC). We develop a specially designed prior which puts its mass only on fair clusters, and implement an efficient MCMC algorithm. Advantages of FBC are that it can infer the number of clusters and can be applied to any data type as long as the likelihood is defined (e.g., categorical data). Experiments on real-world datasets show that FBC (i) reasonably infers the number of clusters, (ii) achieves a competitive utility-fairness trade-off compared to existing fair clustering methods, and (iii) performs well on categorical data.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨å­¦ä¹ ä¸­æ—¥ç›Šå¢é•¿çš„å¯ä¿¡ AI éœ€æ±‚ï¼Œæå‡ºäº† Fair Bayesian Clustering (FBC)ï¼Œä¸€ç§å…¬å¹³çš„è´å¶æ–¯æ¨¡å‹èšç±»æ–¹æ³•ã€‚ä¼ ç»Ÿçš„å…¬å¹³èšç±»å¤šä¾èµ– $K$-meansï¼Œé¢ä¸´éœ€è¦é¢„è®¾èšç±»æ•°é‡å’Œè·ç¦»åº¦é‡çš„å±€é™ï¼Œè€Œ FBC é€šè¿‡è®¾è®¡ä¸“é—¨çš„å…ˆéªŒï¼ˆpriorï¼‰å°†åˆ†å¸ƒé›†ä¸­åœ¨å…¬å¹³èšç±»ä¸Šï¼Œå¹¶åˆ©ç”¨é«˜æ•ˆçš„ MCMC ç®—æ³•è¿›è¡Œæ¨æ–­ã€‚è¯¥æ¡†æ¶çš„ä¸»è¦ä¼˜åŠ¿åœ¨äºèƒ½å¤Ÿè‡ªåŠ¨æ¨æ–­èšç±»æ•°é‡ï¼Œå¹¶å¯çµæ´»åº”ç”¨äºä»»ä½•å®šä¹‰äº†ä¼¼ç„¶ï¼ˆlikelihoodï¼‰çš„æ•°æ®ç±»å‹ï¼Œå¦‚ç±»åˆ«æ•°æ®ï¼ˆcategorical dataï¼‰ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒFBC åœ¨çœŸå®æ•°æ®é›†ä¸Šå®ç°äº†ä¼˜å¼‚çš„æ•ˆç”¨ä¸å…¬å¹³æ€§æƒè¡¡ï¼ˆutility-fairness trade-offï¼‰ï¼Œå¹¶åœ¨å¤„ç†éæ•°å€¼å‹æ•°æ®æ—¶è¡¨ç°ç¨³å¥ã€‚",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.12839v1",
      "published_date": "2025-06-15 13:16:32 UTC",
      "updated_date": "2025-06-15 13:16:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:09:22.487902+00:00"
    },
    {
      "arxiv_id": "2506.13824v1",
      "title": "MLDebugging: Towards Benchmarking Code Debugging Across Multi-Library Scenarios",
      "title_zh": "MLDebuggingï¼šè·¨å¤šåº“åœºæ™¯çš„ä»£ç è°ƒè¯•åŸºå‡†æµ‹è¯•ç ”ç©¶",
      "authors": [
        "Jinyang Huang",
        "Xiachong Feng",
        "Qiguang Chen",
        "Hanjie Zhao",
        "Zihui Cheng",
        "Jiesong Bai",
        "Jingxuan Zhou",
        "Min Li",
        "Libo Qin"
      ],
      "abstract": "Code debugging is a crucial task in software engineering, which attracts increasing attention. While remarkable success has been made in the era of large language models (LLMs), current research still focuses on the simple no-library or single-library setting, ignoring the complex multi-library scenario in real-world applications. To address this limitation, we make the first attempt to introduce MLDebugging (Multi-Library Debugging), a comprehensive benchmark designed to assess debugging challenges within multi-library Python code. Specifically, MLDebugging encompasses 126 distinct Python libraries, covering a wide range of multi-library code issues, categorized into seven distinct types. Furthermore, we conduct a thorough evaluation of MLDebugging using both mainstream open-source and closed-source LLMs and highlight that current LLMs still struggle to correctly perform code debugging across multi-library scenarios. We hope this work can uncover the potential of LLMs in multi-library debugging scenario and offer insights for future research.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è½¯ä»¶å·¥ç¨‹ä¸­ç°æœ‰ä»£ç è°ƒè¯•ç ”ç©¶å¤§å¤šå±€é™äºæ— åº“æˆ–å•ä¸€åº“åœºæ™¯ï¼Œè€Œå¿½è§†äº†ç°å®åº”ç”¨ä¸­å¤æ‚çš„å¤šåº“ï¼ˆMulti-Libraryï¼‰æƒ…å¢ƒè¿™ä¸€å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† MLDebugging (Multi-Library Debugging)ï¼Œè¿™æ˜¯é¦–ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°å¤šåº“ Python ä»£ç è°ƒè¯•æŒ‘æˆ˜çš„ç»¼åˆåŸºå‡†æµ‹è¯•ã€‚MLDebugging æ¶µç›–äº† 126 ä¸ªä¸åŒçš„ Python åº“ï¼ŒåŒ…å«å¤šç§è·¨åº“ä»£ç é—®é¢˜ï¼Œå¹¶å°†å…¶ç»†åˆ†ä¸ºä¸ƒä¸ªä¸åŒçš„ç±»åˆ«ã€‚ç ”ç©¶è€…åˆ©ç”¨ä¸»æµçš„å¼€æºå’Œé—­æºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹ MLDebugging è¿›è¡Œäº†æ·±å…¥è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç›®å‰çš„ LLMs åœ¨å¤„ç†å¤šåº“åœºæ™¯ä¸‹çš„ä»£ç è°ƒè¯•ä»»åŠ¡æ—¶ä»ç„¶é¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œéš¾ä»¥å‡†ç¡®ä¿®å¤ç›¸å…³é”™è¯¯ã€‚è¯¥å·¥ä½œæ—¨åœ¨æ­ç¤º LLMs åœ¨å¤šåº“è°ƒè¯•åœºæ™¯ä¸­çš„æ½œåŠ›ï¼Œå¹¶ä¸ºæœªæ¥çš„ç›¸å…³ç ”ç©¶æä¾›è§è§£ä¸åŸºå‡†æ”¯æŒã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "ACL 2025 Findings",
      "pdf_url": "https://arxiv.org/pdf/2506.13824v1",
      "published_date": "2025-06-15 13:02:59 UTC",
      "updated_date": "2025-06-15 13:02:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:09:19.598389+00:00"
    },
    {
      "arxiv_id": "2506.15734v1",
      "title": "The Safety Reminder: A Soft Prompt to Reactivate Delayed Safety Awareness in Vision-Language Models",
      "title_zh": "Safety Reminderï¼šç”¨äºé‡æ–°æ¿€æ´»è§†è§‰-è¯­è¨€æ¨¡å‹ä¸­å»¶è¿Ÿå®‰å…¨æ„è¯†çš„è½¯æç¤º",
      "authors": [
        "Peiyuan Tang",
        "Haojie Xin",
        "Xiaodong Zhang",
        "Jun Sun",
        "Qin Xia",
        "Zijiang Yang"
      ],
      "abstract": "As Vision-Language Models (VLMs) demonstrate increasing capabilities across real-world applications such as code generation and chatbot assistance, ensuring their safety has become paramount. Unlike traditional Large Language Models (LLMs), VLMs face unique vulnerabilities due to their multimodal nature, allowing adversaries to modify visual or textual inputs to bypass safety guardrails and trigger the generation of harmful content. Through systematic analysis of VLM behavior under attack, we identify a novel phenomenon termed ``delayed safety awareness''. Specifically, we observe that safety-aligned VLMs may initially be compromised to produce harmful content, but eventually recognize the associated risks and attempt to self-correct. This pattern suggests that VLMs retain their underlying safety awareness but experience a temporal delay in their activation. Building on this insight, we hypothesize that VLMs' safety awareness can be proactively reactivated through carefully designed prompts. To this end, we introduce ``The Safety Reminder'', a soft prompt tuning approach that optimizes learnable prompt tokens, which are periodically injected during the text generation process to enhance safety awareness, effectively preventing harmful content generation. Additionally, our safety reminder only activates when harmful content is detected, leaving normal conversations unaffected and preserving the model's performance on benign tasks. Through comprehensive evaluation across three established safety benchmarks and one adversarial attacks, we demonstrate that our approach significantly reduces attack success rates while maintaining model utility, offering a practical solution for deploying safer VLMs in real-world applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Vision-Language Models (VLMs) ç”±äºå…¶å¤šæ¨¡æ€ç‰¹æ€§ï¼Œå®¹æ˜“è¢«æ”»å‡»è€…é€šè¿‡ä¿®æ”¹è§†è§‰æˆ–æ–‡æœ¬è¾“å…¥ç»•è¿‡å®‰å…¨é˜²æŠ¤çš„é—®é¢˜ã€‚é€šè¿‡ç³»ç»Ÿåˆ†æï¼Œç ”ç©¶è€…å‘ç°äº†ä¸€ä¸ªåä¸º delayed safety awareness çš„æ–°ç°è±¡ï¼Œå³å®‰å…¨å¯¹é½çš„æ¨¡å‹å¯èƒ½æœ€åˆè¢«è¯±å¯¼ç”Ÿæˆæœ‰å®³å†…å®¹ï¼Œä½†éšåä¼šæ„è¯†åˆ°é£é™©å¹¶å°è¯•è‡ªæˆ‘çº æ­£ã€‚åŸºäºè¿™ä¸€æ´å¯Ÿï¼Œç ”ç©¶æå‡ºäº† The Safety Reminderï¼Œè¿™æ˜¯ä¸€ç§ soft prompt tuning æ–¹æ³•ï¼Œé€šè¿‡åœ¨æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ä¸­å‘¨æœŸæ€§æ³¨å…¥å¯å­¦ä¹ çš„ prompt tokens æ¥å¢å¼ºå®‰å…¨æ„è¯†å¹¶é˜»æ­¢æœ‰å®³å†…å®¹ç”Ÿæˆã€‚è¯¥æé†’æœºåˆ¶ä»…åœ¨æ£€æµ‹åˆ°æœ‰å®³å†…å®¹æ—¶æ¿€æ´»ï¼Œç¡®ä¿åœ¨å¤„ç†è‰¯æ€§ä»»åŠ¡å’Œæ­£å¸¸å¯¹è¯æ—¶ä¸ä¼šå½±å“æ¨¡å‹çš„æ€§èƒ½ã€‚åœ¨ä¸‰ä¸ªå®‰å…¨åŸºå‡†å’Œå¯¹æŠ—æ€§æ”»å‡»ä¸‹çš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ç»´æŒæ¨¡å‹å®ç”¨æ€§çš„åŒæ—¶æ˜¾è‘—é™ä½äº†æ”»å‡»æˆåŠŸç‡ã€‚è¯¥ç ”ç©¶ä¸ºåœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­éƒ¨ç½²æ›´å®‰å…¨çš„ VLMs æä¾›äº†ä¸€ç§åˆ‡å®å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CR",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "23 pages, 10 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.15734v1",
      "published_date": "2025-06-15 12:48:38 UTC",
      "updated_date": "2025-06-15 12:48:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:09:41.747708+00:00"
    },
    {
      "arxiv_id": "2506.12831v1",
      "title": "Synesthesia of Machines (SoM)-Enhanced Sub-THz ISAC Transmission for Air-Ground Network",
      "title_zh": "é¢å‘ç©ºåœ°ç½‘ç»œçš„æœºå™¨è”è§‰ï¼ˆSoMï¼‰å¢å¼ºå‹äºšå¤ªèµ«å…¹é€šæ„Ÿä¸€ä½“åŒ–ä¼ è¾“",
      "authors": [
        "Zonghui Yang",
        "Shijian Gao",
        "Xiang Cheng",
        "Liuqing Yang"
      ],
      "abstract": "Integrated sensing and communication (ISAC) within sub-THz frequencies is crucial for future air-ground networks, but unique propagation characteristics and hardware limitations present challenges in optimizing ISAC performance while increasing operational latency. This paper introduces a multi-modal sensing fusion framework inspired by synesthesia of machine (SoM) to enhance sub-THz ISAC transmission. By exploiting inherent degrees of freedom in sub-THz hardware and channels, the framework optimizes the radio-frequency environment. Squint-aware beam management is developed to improve air-ground network adaptability, enabling three-dimensional dynamic ISAC links. Leveraging multi-modal information, the framework enhances ISAC performance and reduces latency. Visual data rapidly localizes users and targets, while a customized multi-modal learning algorithm optimizes the hybrid precoder. A new metric provides comprehensive performance evaluation, and extensive experiments demonstrate that the proposed scheme significantly improves ISAC efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å—æœºå™¨è”è§‰(Synesthesia of Machine, SoM)å¯å‘çš„å¤šæ¨¡æ€æ„ŸçŸ¥èåˆæ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºç©ºåœ°ç½‘ç»œä¸­äºšå¤ªèµ«å…¹(sub-THz)é›†æˆæ„ŸçŸ¥ä¸é€šä¿¡(ISAC)çš„ä¼ è¾“æ€§èƒ½ã€‚é’ˆå¯¹sub-THzé¢‘æ®µç‹¬ç‰¹çš„ä¼ æ’­ç‰¹æ€§å’Œç¡¬ä»¶é™åˆ¶æ‰€å¯¼è‡´çš„æ•ˆç‡ä½ä¸‹ä¸é«˜å»¶è¿Ÿé—®é¢˜ï¼Œè¯¥æ¡†æ¶å……åˆ†åˆ©ç”¨ç¡¬ä»¶å’Œä¿¡é“çš„å›ºæœ‰è‡ªç”±åº¦æ¥ä¼˜åŒ–å°„é¢‘ç¯å¢ƒã€‚ç ”ç©¶å¼€å‘äº†æ–œè§†æ„ŸçŸ¥æ³¢æŸç®¡ç†(Squint-aware beam management)æŠ€æœ¯ï¼Œå®ç°äº†é€‚åº”ç©ºåœ°ç½‘ç»œç¯å¢ƒçš„ä¸‰ç»´åŠ¨æ€ISACé“¾è·¯ã€‚é€šè¿‡å¼•å…¥è§†è§‰æ•°æ®è¿›è¡Œå¿«é€Ÿå®šä½ï¼Œå¹¶ç»“åˆå®šåˆ¶çš„å¤šæ¨¡æ€å­¦ä¹ ç®—æ³•ä¼˜åŒ–æ··åˆé¢„ç¼–ç å™¨(hybrid precoder)ï¼Œè¯¥æ–¹æ¡ˆæ˜¾è‘—æå‡äº†ç³»ç»Ÿæ€§èƒ½å¹¶é™ä½äº†æ“ä½œå»¶è¿Ÿã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†ä¸€ç§å…¨æ–°çš„æ€§èƒ½è¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶é€šè¿‡å¹¿æ³›å®éªŒè¯æ˜äº†è¯¥æ–¹æ¡ˆåœ¨æé«˜ISACæ•ˆç‡æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.12831v1",
      "published_date": "2025-06-15 12:30:14 UTC",
      "updated_date": "2025-06-15 12:30:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:09:56.387173+00:00"
    },
    {
      "arxiv_id": "2506.12825v1",
      "title": "Rethinking Optimization: A Systems-Based Approach to Social Externalities",
      "title_zh": "é‡æ–°å®¡è§†ä¼˜åŒ–ï¼šåº”å¯¹ç¤¾ä¼šå¤–éƒ¨æ€§çš„ç³»ç»ŸåŒ–æ–¹æ³•",
      "authors": [
        "Pegah Nokhiz",
        "Aravinda Kanchana Ruwanpathirana",
        "Helen Nissenbaum"
      ],
      "abstract": "Optimization is widely used for decision making across various domains, valued for its ability to improve efficiency. However, poor implementation practices can lead to unintended consequences, particularly in socioeconomic contexts where externalities (costs or benefits to third parties outside the optimization process) are significant. To propose solutions, it is crucial to first characterize involved stakeholders, their goals, and the types of subpar practices causing unforeseen outcomes. This task is complex because affected stakeholders often fall outside the direct focus of optimization processes. Also, incorporating these externalities into optimization requires going beyond traditional economic frameworks, which often focus on describing externalities but fail to address their normative implications or interconnected nature, and feedback loops. This paper suggests a framework that combines systems thinking with the economic concept of externalities to tackle these challenges. This approach aims to characterize what went wrong, who was affected, and how (or where) to include them in the optimization process. Economic externalities, along with their established quantification methods, assist in identifying \"who was affected and how\" through stakeholder characterization. Meanwhile, systems thinking (an analytical approach to comprehending relationships in complex systems) provides a holistic, normative perspective. Systems thinking contributes to an understanding of interconnections among externalities, feedback loops, and determining \"when\" to incorporate them in the optimization. Together, these approaches create a comprehensive framework for addressing optimization's unintended consequences, balancing descriptive accuracy with normative objectives. Using this, we examine three common types of subpar practices: ignorance, error, and prioritization of short-term goals.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å†³ç­–è¿‡ç¨‹ä¸­å¹¿æ³›ä½¿ç”¨çš„ä¼˜åŒ–(Optimization)æŠ€æœ¯åœ¨ç¤¾ä¼šç»æµèƒŒæ™¯ä¸‹å¯èƒ½å¯¼è‡´çš„è´Ÿé¢å¤–éƒ¨æ€§(Social Externalities)é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆç³»ç»Ÿæ€è€ƒ(Systems Thinking)ä¸ç»æµå­¦å¤–éƒ¨æ€§æ¦‚å¿µçš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨æ›´å…¨é¢åœ°è¯†åˆ«å’Œå¤„ç†ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„éé¢„æœŸåæœã€‚è¯¥æ¡†æ¶åˆ©ç”¨ç»æµå­¦æ–¹æ³•æ¥ç•Œå®šå—å½±å“çš„åˆ©ç›Šç›¸å…³è€…(Stakeholders)åŠå…¶å—å½±å“çš„æ–¹å¼ï¼ŒåŒæ—¶å¼•å…¥ç³»ç»Ÿæ€è€ƒä»¥æä¾›æ•´ä½“æ€§çš„è§„èŒƒè§†è§’ã€‚é€šè¿‡è¿™ç§ç»“åˆï¼Œç ”ç©¶è€…èƒ½å¤Ÿæ›´æ·±å…¥åœ°ç†è§£å¤–éƒ¨æ€§ä¹‹é—´çš„ç›¸äº’è”ç³»å’Œåé¦ˆå›è·¯(Feedback Loops)ï¼Œå¹¶ç¡®å®šå°†å…¶çº³å…¥ä¼˜åŒ–è¿‡ç¨‹çš„æ°å½“æ—¶æœºã€‚æœ€åï¼Œè®ºæ–‡åˆ©ç”¨è¯¥æ¡†æ¶åˆ†æäº†å¯¼è‡´ä¸è‰¯åæœçš„ä¸‰ç±»å¸¸è§å®è·µï¼šæ— çŸ¥(Ignorance)ã€é”™è¯¯(Error)ä»¥åŠå¯¹çŸ­æœŸç›®æ ‡çš„è¿‡åº¦ä¼˜å…ˆ(Prioritization of short-term goals)ã€‚è¿™ä¸€ç ”ç©¶ä¸ºå¹³è¡¡ä¼˜åŒ–çš„æè¿°å‡†ç¡®æ€§ä¸è§„èŒƒæ€§ç›®æ ‡æä¾›äº†ç†è®ºæ”¯æŒï¼Œæœ‰åŠ©äºåœ¨å¤æ‚ç³»ç»Ÿä¸­å®ç°æ›´å…·ç¤¾ä¼šè´£ä»»æ„Ÿçš„å†³ç­–ã€‚",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.12825v1",
      "published_date": "2025-06-15 12:14:10 UTC",
      "updated_date": "2025-06-15 12:14:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:09:42.260609+00:00"
    },
    {
      "arxiv_id": "2506.12818v1",
      "title": "Taking the GP Out of the Loop",
      "title_zh": "è®©é«˜æ–¯è¿‡ç¨‹è„±ç¦»å¾ªç¯",
      "authors": [
        "David Sweet",
        "Siddhant anand Jadhav"
      ],
      "abstract": "Bayesian optimization (BO) has traditionally solved black box problems where evaluation is expensive and, therefore, design-evaluation pairs (i.e., observations) are few. Recently, there has been growing interest in applying BO to problems where evaluation is cheaper and, thus, observations are more plentiful. An impediment to scaling BO to many observations, $N$, is the $O(N^3)$ scaling of a na{Ã¯}ve query of the Gaussian process (GP) surrogate. Modern implementations reduce this to $O(N^2)$, but the GP remains a bottleneck. We propose Epistemic Nearest Neighbors (ENN), a surrogate that estimates function values and epistemic uncertainty from $K$ nearest-neighbor observations. ENN has $O(N)$ query time and omits hyperparameter fitting, leaving uncertainty uncalibrated. To accommodate the lack of calibration, we employ an acquisition method based on Pareto-optimal tradeoffs between predicted value and uncertainty. Our proposed method, TuRBO-ENN, replaces the GP surrogate in TuRBO with ENN and its Thompson sampling acquisition method with our Pareto-based alternative. We demonstrate numerically that TuRBO-ENN can reduce the time to generate proposals by one to two orders of magnitude compared to TuRBO and scales to thousands of observations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è´å¶æ–¯ä¼˜åŒ–(Bayesian optimization)ä¸­é«˜æ–¯è¿‡ç¨‹(Gaussian process)ä»£ç†æ¨¡å‹åœ¨å¤„ç†æµ·é‡è§‚æµ‹æ•°æ®æ—¶é¢ä¸´çš„è®¡ç®—ç“¶é¢ˆï¼Œæå‡ºäº†åä¸ºEpistemic Nearest Neighbors (ENN)çš„æ›¿ä»£æ–¹æ¡ˆã€‚ENNé€šè¿‡åˆ©ç”¨$K$ä¸ªæœ€è¿‘é‚»è§‚æµ‹å€¼æ¥ä¼°è®¡å‡½æ•°å€¼å’Œè®¤çŸ¥ä¸ç¡®å®šæ€§(epistemic uncertainty)ï¼Œä¸ä»…å°†æŸ¥è¯¢æ—¶é—´å¤æ‚åº¦æ˜¾è‘—é™ä½è‡³$O(N)$ï¼Œè¿˜çœå»äº†å¤æ‚çš„è¶…å‚æ•°æ‹Ÿåˆè¿‡ç¨‹ã€‚ä¸ºäº†è§£å†³ä¸ç¡®å®šæ€§æ ¡å‡†ç¼ºå¤±çš„é—®é¢˜ï¼Œç ”ç©¶è€…å¼•å…¥äº†ä¸€ç§åŸºäºé¢„æµ‹å€¼ä¸ä¸ç¡®å®šæ€§å¸•ç´¯æ‰˜æœ€ä¼˜æƒè¡¡(Pareto-optimal tradeoffs)çš„é‡‡é›†æ–¹æ³•ï¼Œä»¥å–ä»£ä¼ ç»Ÿçš„æ±¤æ™®æ£®é‡‡æ ·(Thompson sampling)ã€‚åœ¨æ­¤åŸºç¡€ä¸Šæ„å»ºçš„TuRBO-ENNæ¡†æ¶ï¼Œé€šè¿‡åœ¨TuRBOç®—æ³•ä¸­å¼•å…¥ENNä»£ç†ï¼Œå®ç°äº†åœ¨å¤„ç†æ•°åƒä¸ªè§‚æµ‹ç‚¹æ—¶ç”Ÿæˆå»ºè®®çš„é€Ÿåº¦æ¯”åŸå§‹ç®—æ³•æå‡ä¸€åˆ°ä¸¤ä¸ªæ•°é‡çº§ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒä¼˜åŒ–æ€§èƒ½çš„åŒæ—¶ï¼Œæå¤§æå‡äº†åœ¨å¤§è§„æ¨¡è§‚æµ‹åœºæ™¯ä¸‹çš„è®¡ç®—æ•ˆç‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages, 11 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.12818v1",
      "published_date": "2025-06-15 11:37:28 UTC",
      "updated_date": "2025-06-15 11:37:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:09:55.786747+00:00"
    },
    {
      "arxiv_id": "2506.12812v1",
      "title": "Federated Neuroevolution O-RAN: Enhancing the Robustness of Deep Reinforcement Learning xApps",
      "title_zh": "è”é‚¦ç¥ç»è¿›åŒ– O-RANï¼šå¢å¼ºæ·±åº¦å¼ºåŒ–å­¦ä¹  xApps çš„é²æ£’æ€§",
      "authors": [
        "Mohammadreza Kouchaki",
        "Aly Sabri Abdalla",
        "Vuk Marojevic"
      ],
      "abstract": "The open radio access network (O-RAN) architecture introduces RAN intelligent controllers (RICs) to facilitate the management and optimization of the disaggregated RAN. Reinforcement learning (RL) and its advanced form, deep RL (DRL), are increasingly employed for designing intelligent controllers, or xApps, to be deployed in the near-real time (near-RT) RIC. These models often encounter local optima, which raise concerns about their reliability for RAN intelligent control. We therefore introduce Federated O-RAN enabled Neuroevolution (NE)-enhanced DRL (F-ONRL) that deploys an NE-based optimizer xApp in parallel to the RAN controller xApps. This NE-DRL xApp framework enables effective exploration and exploitation in the near-RT RIC without disrupting RAN operations. We implement the NE xApp along with a DRL xApp and deploy them on Open AI Cellular (OAIC) platform and present numerical results that demonstrate the improved robustness of xApps while effectively balancing the additional computational load.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹å¼€æ”¾å¼æ— çº¿æ¥å…¥ç½‘(O-RAN)ä¸­æ·±åº¦å¼ºåŒ–å­¦ä¹ (DRL)çš„xAppså®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜è§£ä¸”å¯é æ€§ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†Federated O-RAN enabled Neuroevolution (NE)-enhanced DRL (F-ONRL)æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡åœ¨è¿‘å®æ—¶(near-RT) RICä¸­å¹¶è¡Œéƒ¨ç½²åŸºäºNeuroevolution (NE)çš„ä¼˜åŒ–å™¨xAppä¸RANæ§åˆ¶å™¨xAppsï¼Œæ„å»ºäº†ä¸€ç§åˆ›æ–°çš„NE-DRLå·¥ä½œæ¨¡å¼ã€‚è¿™ç§æ¶æ„èƒ½å¤Ÿå®ç°åœ¨ä¸ä¸­æ–­RANä¸šåŠ¡çš„å‰æä¸‹ï¼Œåœ¨è¿‘å®æ—¶ç¯å¢ƒä¸­è¿›è¡Œé«˜æ•ˆçš„æ¢ç´¢ä¸åˆ©ç”¨ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨Open AI Cellular (OAIC)å¹³å°ä¸Šå¯¹è¯¥æ¡†æ¶è¿›è¡Œäº†éƒ¨ç½²ä¸æµ‹è¯•ï¼Œå®éªŒç»“æœè¯æ˜F-ONRLæ˜¾è‘—å¢å¼ºäº†xAppsçš„é²æ£’æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ¡ˆè¿˜èƒ½åœ¨æå‡æ™ºèƒ½æ§åˆ¶æ€§èƒ½çš„åŒæ—¶ï¼Œæœ‰æ•ˆå¹³è¡¡ç”±æ­¤äº§ç”Ÿçš„é¢å¤–è®¡ç®—è´Ÿè½½ã€‚",
      "categories": [
        "cs.AI",
        "cs.NE",
        "eess.SY"
      ],
      "primary_category": "cs.AI",
      "comment": "This article has been accepted for publication in IEEE Communications Magazine",
      "pdf_url": "https://arxiv.org/pdf/2506.12812v1",
      "published_date": "2025-06-15 10:58:10 UTC",
      "updated_date": "2025-06-15 10:58:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:09:51.590812+00:00"
    },
    {
      "arxiv_id": "2506.12811v1",
      "title": "Flow-Based Policy for Online Reinforcement Learning",
      "title_zh": "é¢å‘åœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„åŸºäºæµçš„ç­–ç•¥",
      "authors": [
        "Lei Lv",
        "Yunfei Li",
        "Yu Luo",
        "Fuchun Sun",
        "Tao Kong",
        "Jiafeng Xu",
        "Xiao Ma"
      ],
      "abstract": "We present \\textbf{FlowRL}, a novel framework for online reinforcement learning that integrates flow-based policy representation with Wasserstein-2-regularized optimization. We argue that in addition to training signals, enhancing the expressiveness of the policy class is crucial for the performance gains in RL. Flow-based generative models offer such potential, excelling at capturing complex, multimodal action distributions. However, their direct application in online RL is challenging due to a fundamental objective mismatch: standard flow training optimizes for static data imitation, while RL requires value-based policy optimization through a dynamic buffer, leading to difficult optimization landscapes. FlowRL first models policies via a state-dependent velocity field, generating actions through deterministic ODE integration from noise. We derive a constrained policy search objective that jointly maximizes Q through the flow policy while bounding the Wasserstein-2 distance to a behavior-optimal policy implicitly derived from the replay buffer. This formulation effectively aligns the flow optimization with the RL objective, enabling efficient and value-aware policy learning despite the complexity of the policy class. Empirical evaluations on DMControl and Humanoidbench demonstrate that FlowRL achieves competitive performance in online reinforcement learning benchmarks.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†FlowRLï¼Œè¿™æ˜¯ä¸€ä¸ªå°†æµ(flow-based)ç­–ç•¥è¡¨ç¤ºä¸Wasserstein-2æ­£åˆ™åŒ–ä¼˜åŒ–ç›¸ç»“åˆçš„æ–°å‹åœ¨çº¿å¼ºåŒ–å­¦ä¹ (Online Reinforcement Learning)æ¡†æ¶ã€‚ä½œè€…è®¤ä¸ºï¼Œé™¤äº†è®­ç»ƒä¿¡å·å¤–ï¼Œå¢å¼ºç­–ç•¥ç±»çš„è¡¨è¾¾èƒ½åŠ›å¯¹äºæå‡å¼ºåŒ–å­¦ä¹ æ€§èƒ½è‡³å…³é‡è¦ï¼Œè€Œæµç”Ÿæˆæ¨¡å‹åœ¨æ•æ‰å¤æ‚ã€å¤šæ¨¡æ€åŠ¨ä½œåˆ†å¸ƒæ–¹é¢å…·æœ‰æ˜¾è‘—æ½œåŠ›ã€‚é’ˆå¯¹æµæ¨¡å‹åœ¨åŠ¨æ€ä»·å€¼ä¼˜åŒ–ä¸­é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒFlowRLé€šè¿‡çŠ¶æ€ä¾èµ–çš„é€Ÿåº¦åœº(velocity field)æ¥å»ºæ¨¡ç­–ç•¥ï¼Œå¹¶åˆ©ç”¨ç¡®å®šæ€§å¸¸å¾®åˆ†æ–¹ç¨‹(ODE)ç§¯åˆ†ä»å™ªå£°ä¸­ç”ŸæˆåŠ¨ä½œã€‚è¯¥æ¡†æ¶æ¨å¯¼å‡ºä¸€ä¸ªå—é™ç­–ç•¥æœç´¢ç›®æ ‡ï¼Œåœ¨æœ€å¤§åŒ–Qå€¼çš„åŒæ—¶ï¼Œé™åˆ¶ç­–ç•¥ä¸å›æ”¾ç¼“å†²åŒºä¸­éšå«çš„è¡Œä¸ºæœ€ä¼˜ç­–ç•¥ä¹‹é—´çš„Wasserstein-2è·ç¦»ã€‚è¿™ç§å…¬å¼è®¾è®¡æœ‰æ•ˆåœ°å°†æµä¼˜åŒ–ä¸å¼ºåŒ–å­¦ä¹ ç›®æ ‡å¯¹é½ï¼Œä½¿å¾—åœ¨å¤æ‚çš„ç­–ç•¥ç©ºé—´ä¸­ä¹Ÿèƒ½å®ç°é«˜æ•ˆä¸”æ„ŸçŸ¥ä»·å€¼çš„ç­–ç•¥å­¦ä¹ ã€‚åœ¨DMControlå’ŒHumanoidbenchä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒFlowRLåœ¨åœ¨çº¿å¼ºåŒ–å­¦ä¹ åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½æ°´å¹³ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.12811v1",
      "published_date": "2025-06-15 10:53:35 UTC",
      "updated_date": "2025-06-15 10:53:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:09:56.190072+00:00"
    },
    {
      "arxiv_id": "2506.12804v1",
      "title": "Fuzzy Propositional Formulas under the Stable Model Semantics",
      "title_zh": "ç¨³å®šæ¨¡å‹è¯­ä¹‰ä¸‹çš„æ¨¡ç³Šå‘½é¢˜å…¬å¼",
      "authors": [
        "Joohyung Lee",
        "Yi Wang"
      ],
      "abstract": "We define a stable model semantics for fuzzy propositional formulas, which generalizes both fuzzy propositional logic and the stable model semantics of classical propositional formulas. The syntax of the language is the same as the syntax of fuzzy propositional logic, but its semantics distinguishes stable models from non-stable models. The generality of the language allows for highly configurable nonmonotonic reasoning for dynamic domains involving graded truth degrees. We show that several properties of Boolean stable models are naturally extended to this many-valued setting, and discuss how it is related to other approaches to combining fuzzy logic and the stable model semantics.",
      "tldr_zh": "è¯¥ç ”ç©¶ä¸º Fuzzy Propositional Formulas å®šä¹‰äº†ä¸€ç§ Stable Model Semanticsï¼Œå®ç°äº†å¯¹ Fuzzy Propositional Logic å’Œç»å…¸å‘½é¢˜å…¬å¼ Stable Model Semantics çš„ç»Ÿä¸€æ¨å¹¿ã€‚è¯¥è¯­è¨€åœ¨ä¿æŒ Fuzzy Propositional Logic è¯­æ³•ä¸å˜çš„åŸºç¡€ä¸Šï¼Œé€šè¿‡åœ¨è¯­ä¹‰å±‚é¢åŒºåˆ† Stable Models ä¸éç¨³å®šæ¨¡å‹ï¼Œä¸ºåŒ…å« Graded Truth Degrees çš„åŠ¨æ€é¢†åŸŸæä¾›äº†é«˜åº¦å¯é…ç½®çš„ Nonmonotonic Reasoning èƒ½åŠ›ã€‚ä½œè€…è¯æ˜äº† Boolean Stable Models çš„æ ¸å¿ƒå±æ€§èƒ½å¤Ÿè‡ªç„¶åœ°æ‰©å±•åˆ°è¿™ç§å¤šå€¼é€»è¾‘è®¾å®šä¸­ã€‚æ­¤å¤–ï¼Œæ–‡ä¸­è¿˜æ·±å…¥æ¢è®¨äº†è¯¥æ–¹æ³•ä¸ç°æœ‰ç»“åˆ Fuzzy Logic å’Œ Stable Model Semantics çš„å…¶ä»–ç ”ç©¶è·¯å¾„ä¹‹é—´çš„å…³è”ï¼Œä¸ºå¤„ç†å…·æœ‰æ¨¡ç³Šæ€§çš„å¤æ‚é€»è¾‘æ¨ç†ä»»åŠ¡æä¾›äº†å¼ºæœ‰åŠ›çš„ç†è®ºæ¡†æ¶ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "In the Special Issue on Logics for Reasoning about Preferences, Uncertainty and Vagueness of the IfCoLog Journal of Logics and their Applications, pages 1927-1972, 2017",
      "pdf_url": "https://arxiv.org/pdf/2506.12804v1",
      "published_date": "2025-06-15 10:38:56 UTC",
      "updated_date": "2025-06-15 10:38:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:10:00.290750+00:00"
    },
    {
      "arxiv_id": "2506.12801v1",
      "title": "Mastering Da Vinci Code: A Comparative Study of Transformer, LLM, and PPO-based Agents",
      "title_zh": "æŒæ¡ Da Vinci Codeï¼šåŸºäº Transformerã€LLM å’Œ PPO æ™ºèƒ½ä½“çš„æ¯”è¾ƒç ”ç©¶",
      "authors": [
        "LeCheng Zhang",
        "Yuanshi Wang",
        "Haotian Shen",
        "Xujie Wang"
      ],
      "abstract": "The Da Vinci Code, a game of logical deduction and imperfect information, presents unique challenges for artificial intelligence, demanding nuanced reasoning beyond simple pattern recognition. This paper investigates the efficacy of various AI paradigms in mastering this game. We develop and evaluate three distinct agent architectures: a Transformer-based baseline model with limited historical context, several Large Language Model (LLM) agents (including Gemini, DeepSeek, and GPT variants) guided by structured prompts, and an agent based on Proximal Policy Optimization (PPO) employing a Transformer encoder for comprehensive game history processing. Performance is benchmarked against the baseline, with the PPO-based agent demonstrating superior win rates ($58.5\\% \\pm 1.0\\%$), significantly outperforming the LLM counterparts. Our analysis highlights the strengths of deep reinforcement learning in policy refinement for complex deductive tasks, particularly in learning implicit strategies from self-play. We also examine the capabilities and inherent limitations of current LLMs in maintaining strict logical consistency and strategic depth over extended gameplay, despite sophisticated prompting. This study contributes to the broader understanding of AI in recreational games involving hidden information and multi-step logical reasoning, offering insights into effective agent design and the comparative advantages of different AI approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ…å«é€»è¾‘æ¼”ç»å’Œä¸å®Œå…¨ä¿¡æ¯çš„ Da Vinci Code æ¸¸æˆï¼Œæ·±å…¥æ¢è®¨äº†ä¸åŒäººå·¥æ™ºèƒ½èŒƒå¼çš„æ•ˆèƒ½ã€‚ç ”ç©¶è®¾è®¡å¹¶è¯„ä¼°äº†ä¸‰ç§æ¶æ„ï¼šä¸€ä¸ªå…·æœ‰æœ‰é™ä¸Šä¸‹æ–‡çš„ Transformer åŸºå‡†æ¨¡å‹ï¼Œå¤šä¸ªç”±ç»“æ„åŒ–æç¤ºå¼•å¯¼çš„ Large Language Model (LLM) æ™ºèƒ½ä½“ï¼ˆåŒ…æ‹¬ Geminiã€DeepSeek å’Œ GPT å˜ä½“ï¼‰ï¼Œä»¥åŠä¸€ä¸ªé‡‡ç”¨ Transformer ç¼–ç å™¨å¤„ç†å®Œæ•´å†å²çš„ Proximal Policy Optimization (PPO) æ™ºèƒ½ä½“ã€‚å®éªŒåŸºå‡†æµ‹è¯•æ˜¾ç¤ºï¼ŒPPO æ™ºèƒ½ä½“ä»¥ $58.5\\% \\pm 1.0\\%$ çš„èƒœç‡è¡¨ç°å‡ºæ˜æ˜¾çš„ä¼˜è¶Šæ€§ï¼Œæ˜¾è‘—è¶…è¿‡äº† LLM æ™ºèƒ½ä½“ã€‚åˆ†ææŒ‡å‡ºï¼Œæ·±åº¦å¼ºåŒ–å­¦ä¹  (Deep Reinforcement Learning) åœ¨å¤æ‚æ¼”ç»ä»»åŠ¡çš„ç­–ç•¥æ”¹è¿›ä¸­å…·æœ‰ä¼˜åŠ¿ï¼Œå°¤å…¶æ“…é•¿ä»è‡ªæˆ‘åšå¼ˆä¸­å­¦ä¹ éšå«ç­–ç•¥ã€‚ç ”ç©¶åŒæ—¶æ­ç¤ºäº†å½“å‰ LLM å°½ç®¡é‡‡ç”¨äº†å¤æ‚çš„æç¤ºè¯å·¥ç¨‹ï¼Œä½†åœ¨é•¿æ—¶é—´åšå¼ˆä¸­ç»´æŒé€»è¾‘ä¸€è‡´æ€§å’Œç­–ç•¥æ·±åº¦æ–¹é¢ä»å­˜åœ¨å±€é™æ€§ã€‚è¯¥ç ”ç©¶ä¸ºç†è§£ AI å¤„ç†éšè—ä¿¡æ¯å’Œå¤šæ­¥é€»è¾‘æ¨ç†æä¾›äº†æ–°è§è§£ï¼Œå¹¶ä¸ºé«˜æ•ˆæ™ºèƒ½ä½“è®¾è®¡æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.12801v1",
      "published_date": "2025-06-15 10:33:30 UTC",
      "updated_date": "2025-06-15 10:33:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:10:11.592100+00:00"
    },
    {
      "arxiv_id": "2506.12795v1",
      "title": "Resilient-native and Intelligent NextG Systems",
      "title_zh": "å†…ç”ŸéŸ§æ€§ä¸æ™ºèƒ½ NextG ç³»ç»Ÿ",
      "authors": [
        "Mehdi Bennis"
      ],
      "abstract": "Just like power, water and transportation systems, wireless networks are a crucial societal infrastructure. As natural and human-induced disruptions continue to grow, wireless networks must be resilient to unforeseen events, able to withstand and recover from unexpected adverse conditions, shocks, unmodeled disturbances and cascading failures. Despite its critical importance, resilience remains an elusive concept, with its mathematical foundations still underdeveloped. Unlike robustness and reliability, resilience is premised on the fact that disruptions will inevitably happen. Resilience, in terms of elasticity, focuses on the ability to bounce back to favorable states, while resilience as plasticity involves agents (or networks) that can flexibly expand their states, hypotheses and course of actions, by transforming through real-time adaptation and reconfiguration. This constant situational awareness and vigilance of adapting world models and counterfactually reasoning about potential system failures and the corresponding best responses, is a core aspect of resilience. This article seeks to first define resilience and disambiguate it from reliability and robustness, before delving into the mathematics of resilience. Finally, the article concludes by presenting nuanced metrics and discussing trade-offs tailored to the unique characteristics of network resilience.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†Resilient-native and Intelligent NextG Systemsä½œä¸ºå…³é”®ç¤¾ä¼šåŸºç¡€è®¾æ–½çš„é‡è¦æ€§ï¼Œæ—¨åœ¨åº”å¯¹æ—¥ç›Šå¢é•¿çš„è‡ªç„¶åŠäººä¸ºå¹²æ‰°ã€‚æ–‡ç« æ˜ç¡®å®šä¹‰äº†Resilienceï¼Œå¹¶å°†å…¶ä¸Reliabilityå’ŒRobustnessè¿›è¡Œäº†æ·±åº¦åŒºåˆ†ï¼Œå¼ºè°ƒResilienceçš„å‰ææ˜¯å¹²æ‰°ä¸å¯é¿å…ã€‚ç ”ç©¶å°†Resilienceåˆ†ä¸ºElasticityï¼ˆæ¢å¤åˆ°æœ‰åˆ©çŠ¶æ€çš„èƒ½åŠ›ï¼‰ä¸Plasticityï¼ˆé€šè¿‡å®æ—¶é€‚åº”å’Œé‡æ„çµæ´»æ‰©å±•è¡ŒåŠ¨è·¯å¾„çš„èƒ½åŠ›ï¼‰ï¼ŒæŒ‡å‡ºé€šè¿‡World Modelsè¿›è¡Œæƒ…å¢ƒæ„ŸçŸ¥å¹¶é’ˆå¯¹æ½œåœ¨æ•…éšœè¿›è¡ŒCounterfactually Reasoningæ˜¯å…¶æ ¸å¿ƒã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ·±å…¥æ¢è®¨äº†Resilienceçš„æ•°å­¦åŸºç¡€ï¼Œå¹¶é’ˆå¯¹æ— çº¿ç½‘ç»œçš„ç‹¬ç‰¹ç‰¹æ€§æå‡ºäº†ç»†åŒ–çš„Metricså’Œæ€§èƒ½æƒè¡¡åˆ†æã€‚",
      "categories": [
        "cs.ET",
        "cs.AI"
      ],
      "primary_category": "cs.ET",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.12795v1",
      "published_date": "2025-06-15 10:01:44 UTC",
      "updated_date": "2025-06-15 10:01:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:10:15.896170+00:00"
    },
    {
      "arxiv_id": "2506.12784v1",
      "title": "LPMLN, Weak Constraints, and P-log",
      "title_zh": "LPMLNã€å¼±çº¦æŸä¸ P-log",
      "authors": [
        "Joohyung Lee",
        "Zhun Yang"
      ],
      "abstract": "LPMLN is a recently introduced formalism that extends answer set programs by adopting the log-linear weight scheme of Markov Logic. This paper investigates the relationships between LPMLN and two other extensions of answer set programs: weak constraints to express a quantitative preference among answer sets, and P-log to incorporate probabilistic uncertainty. We present a translation of LPMLN into programs with weak constraints and a translation of P-log into LPMLN, which complement the existing translations in the opposite directions. The first translation allows us to compute the most probable stable models (i.e., MAP estimates) of LPMLN programs using standard ASP solvers. This result can be extended to other formalisms, such as Markov Logic, ProbLog, and Pearl's Causal Models, that are shown to be translatable into LPMLN. The second translation tells us how probabilistic nonmonotonicity (the ability of the reasoner to change his probabilistic model as a result of new information) of P-log can be represented in LPMLN, which yields a way to compute P-log using standard ASP solvers and MLN solvers.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† LPMLN ä¸å¼±çº¦æŸ(weak constraints)åŠ P-log ä¸¤ç§å›ç­”é›†ç¨‹åº(ASP)æ‰©å±•ä¹‹é—´çš„å…³ç³»ï¼ŒLPMLN æ˜¯ä¸€ç§é‡‡ç”¨é©¬å°”å¯å¤«é€»è¾‘(Markov Logic)å¯¹æ•°çº¿æ€§æƒé‡æ–¹æ¡ˆçš„å½¢å¼åŒ–æ–¹æ³•ã€‚è®ºæ–‡æå‡ºäº†å°† LPMLN è½¬æ¢ä¸ºå¸¦æœ‰å¼±çº¦æŸç¨‹åºçš„ç¿»è¯‘æ–¹æ³•ï¼Œä½¿å¾—ç ”ç©¶è€…èƒ½å¤Ÿåˆ©ç”¨æ ‡å‡† ASP æ±‚è§£å™¨æ¥è®¡ç®— LPMLN çš„æœ€å¤§åéªŒæ¦‚ç‡(MAP)ä¼°è®¡ï¼Œè¯¥ç»“æœå¯è¿›ä¸€æ­¥æ‰©å±•è‡³ ProbLog å’Œ Pearl's Causal Models ç­‰æ¨¡å‹ã€‚åŒæ—¶ï¼Œè¯¥ç ”ç©¶è¿˜å®ç°äº†ä» P-log åˆ° LPMLN çš„ç¿»è¯‘ï¼Œæ­ç¤ºäº†å¦‚ä½•åœ¨ LPMLN ä¸­è¡¨ç¤º P-log çš„æ¦‚ç‡éå•è°ƒæ€§ï¼Œå¹¶æä¾›äº†åˆ©ç”¨æ ‡å‡† ASP å’Œ MLN æ±‚è§£å™¨è®¡ç®— P-log çš„æ–°é€”å¾„ã€‚è¿™äº›ç ”ç©¶æˆæœå®Œå–„äº†ä¸åŒæ¦‚ç‡é€»è¾‘ç¼–ç¨‹æ¡†æ¶ä¹‹é—´çš„è½¬æ¢ä½“ç³»ï¼Œä¸ºå¯æ‰©å±•çš„æ¦‚ç‡æ¨ç†æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "In Proceedings of the 31st AAAI Conference on Artificial Intelligence (AAAI 2017), pages 1170-1177, 2017",
      "pdf_url": "https://arxiv.org/pdf/2506.12784v1",
      "published_date": "2025-06-15 09:28:20 UTC",
      "updated_date": "2025-06-15 09:28:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:10:23.897907+00:00"
    },
    {
      "arxiv_id": "2506.12775v2",
      "title": "Scene-aware SAR ship detection guided by unsupervised sea-land segmentation",
      "title_zh": "æ— ç›‘ç£æµ·é™†åˆ†å‰²å¼•å¯¼çš„åœºæ™¯æ„ŸçŸ¥ SAR èˆ°èˆ¹æ£€æµ‹",
      "authors": [
        "Han Ke",
        "Xiao Ke",
        "Ye Yan",
        "Rui Liu",
        "Jinpeng Yang",
        "Tianwen Zhang",
        "Xu Zhan",
        "Xiaowo Xu"
      ],
      "abstract": "DL based Synthetic Aperture Radar (SAR) ship detection has tremendous advantages in numerous areas. However, it still faces some problems, such as the lack of prior knowledge, which seriously affects detection accuracy. In order to solve this problem, we propose a scene-aware SAR ship detection method based on unsupervised sea-land segmentation. This method follows a classical two-stage framework and is enhanced by two models: the unsupervised land and sea segmentation module (ULSM) and the land attention suppression module (LASM). ULSM and LASM can adaptively guide the network to reduce attention on land according to the type of scenes (inshore scene and offshore scene) and add prior knowledge (sea land segmentation information) to the network, thereby reducing the network's attention to land directly and enhancing offshore detection performance relatively. This increases the accuracy of ship detection and enhances the interpretability of the model. Specifically, in consideration of the lack of land sea segmentation labels in existing deep learning-based SAR ship detection datasets, ULSM uses an unsupervised approach to classify the input data scene into inshore and offshore types and performs sea-land segmentation for inshore scenes. LASM uses the sea-land segmentation information as prior knowledge to reduce the network's attention to land. We conducted our experiments using the publicly available SSDD dataset, which demonstrated the effectiveness of our network.",
      "tldr_zh": "é’ˆå¯¹æ·±åº¦å­¦ä¹ (Deep Learning)åœ¨åˆæˆå­”å¾„é›·è¾¾(SAR)èˆ°èˆ¹æ£€æµ‹ä¸­ç¼ºä¹å…ˆéªŒçŸ¥è¯†ä¸”æ˜“å—é™†åœ°å¹²æ‰°çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ— ç›‘ç£æµ·é™†åˆ†å‰²åœºæ™¯æ„ŸçŸ¥çš„æ£€æµ‹æ–¹æ³•ã€‚è¯¥æ–¹æ³•åœ¨ç»å…¸çš„äºŒé˜¶æ®µæ¡†æ¶ä¸­å¼•å…¥äº†æ— ç›‘ç£æµ·é™†åˆ†å‰²æ¨¡å—(ULSM)å’Œé™†åœ°æ³¨æ„åŠ›æŠ‘åˆ¶æ¨¡å—(LASM)ï¼Œæ—¨åœ¨é€šè¿‡å‡å°‘å¯¹é™†åœ°ç‰¹å¾çš„å…³æ³¨æ¥æå‡æ£€æµ‹ç²¾åº¦ã€‚ULSMåˆ©ç”¨æ— ç›‘ç£æ–¹å¼å°†è¾“å…¥åœºæ™¯åˆ†ç±»ä¸ºè¿‘æµ·æˆ–è¿œæµ·ï¼Œå¹¶å¯¹è¿‘æµ·åœºæ™¯ç”Ÿæˆæµ·é™†åˆ†å‰²å…ˆéªŒä¿¡æ¯ï¼Œä»è€Œè§£å†³äº†ç°æœ‰æ£€æµ‹æ•°æ®é›†ç¼ºä¹åˆ†å‰²æ ‡ç­¾çš„é—®é¢˜ã€‚LASMåˆ™å°†è¿™äº›å…ˆéªŒçŸ¥è¯†æ³¨å…¥ç½‘ç»œï¼Œè‡ªé€‚åº”åœ°é™ä½æ¨¡å‹å¯¹é™†åœ°åŒºåŸŸçš„æ³¨æ„åŠ›ï¼Œä»è€Œç›¸å¯¹å¢å¼ºäº†æµ·ä¸Šç›®æ ‡çš„æ£€æµ‹èƒ½åŠ›ã€‚åœ¨å…¬å¼€çš„SSDDæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå®ƒåœ¨æ˜¾è‘—æé«˜èˆ°èˆ¹æ£€æµ‹å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œä¹Ÿå¢å¼ºäº†æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸‹çš„å¯è§£é‡Šæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.12775v2",
      "published_date": "2025-06-15 08:57:20 UTC",
      "updated_date": "2025-12-18 07:18:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:10:38.114678+00:00"
    },
    {
      "arxiv_id": "2506.12770v1",
      "title": "Solving tricky quantum optics problems with assistance from (artificial) intelligence",
      "title_zh": "å€ŸåŠ©ï¼ˆäººå·¥ï¼‰æ™ºèƒ½æ±‚è§£é‡å­å…‰å­¦ç–‘éš¾é—®é¢˜",
      "authors": [
        "Manas Pandey",
        "Bharath Hebbe Madhusudhana",
        "Saikat Ghosh",
        "Dmitry Budker"
      ],
      "abstract": "The capabilities of modern artificial intelligence (AI) as a ``scientific collaborator'' are explored by engaging it with three nuanced problems in quantum optics: state populations in optical pumping, resonant transitions between decaying states (the Burshtein effect), and degenerate mirrorless lasing. Through iterative dialogue, the authors observe that AI models--when prompted and corrected--can reason through complex scenarios, refine their answers, and provide expert-level guidance, closely resembling the interaction with an adept colleague. The findings highlight that AI democratizes access to sophisticated modeling and analysis, shifting the focus in scientific practice from technical mastery to the generation and testing of ideas, and reducing the time for completing research tasks from days to minutes.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººå·¥æ™ºèƒ½(AI)ä½œä¸ºâ€œç§‘å­¦åä½œä¼™ä¼´â€åœ¨è§£å†³é‡å­å…‰å­¦(quantum optics)ä¸­æ£˜æ‰‹é—®é¢˜çš„èƒ½åŠ›ã€‚ç ”ç©¶è€…é€šè¿‡è¿­ä»£å¯¹è¯ï¼Œå¼•å¯¼AIå¤„ç†äº†å…‰æ³µæµ¦ä¸­çš„èƒ½çº§åˆ†å¸ƒ(state populations in optical pumping)ã€è¡°å˜æ€ä¹‹é—´çš„å…±æŒ¯è·ƒè¿(Burshtein effect)ä»¥åŠç®€å¹¶æ— é•œåƒæ¿€å…‰(degenerate mirrorless lasing)ä¸‰ä¸ªå…·ä½“åœºæ™¯ã€‚å®éªŒè§‚å¯Ÿåˆ°AIæ¨¡å‹åœ¨æ¥å—æç¤ºå’Œçº æ­£åï¼Œèƒ½å¤Ÿå¯¹å¤æ‚æƒ…å¢ƒè¿›è¡Œé€»è¾‘æ¨ç†å¹¶æä¾›ä¸“å®¶çº§æŒ‡å¯¼ï¼Œå…¶è¡¨ç°ç±»ä¼¼äºç»éªŒä¸°å¯Œçš„ç§‘ç ”åŒäº‹ã€‚è¿™é¡¹å‘ç°å¼ºè°ƒäº†AIåœ¨æ™®åŠå¤æ‚å»ºæ¨¡ä¸åˆ†ææ–¹é¢çš„ä½œç”¨ï¼Œä½¿ç§‘ç ”é‡ç‚¹ä»å•çº¯çš„æŠ€æœ¯æŒæ¡è½¬å‘åˆ›æ„çš„ç”Ÿæˆä¸æµ‹è¯•ã€‚æ­¤å¤–ï¼ŒAIçš„è¾…åŠ©å°†åŸæœ¬éœ€è¦æ•°å¤©çš„ç ”ç©¶ä»»åŠ¡ç¼©çŸ­è‡³æ•°åˆ†é’Ÿï¼Œæå¤§åœ°åŠ é€Ÿäº†ç§‘å­¦æ¢ç´¢çš„è¿›ç¨‹ã€‚",
      "categories": [
        "quant-ph",
        "cs.AI",
        "physics.atom-ph"
      ],
      "primary_category": "quant-ph",
      "comment": "9 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.12770v1",
      "published_date": "2025-06-15 08:40:15 UTC",
      "updated_date": "2025-06-15 08:40:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:10:47.456190+00:00"
    },
    {
      "arxiv_id": "2506.12762v1",
      "title": "On-board Sonar Data Classification for Path Following in Underwater Vehicles using Fast Interval Type-2 Fuzzy Extreme Learning Machine",
      "title_zh": "åŸºäºå¿«é€ŸåŒºé—´äºŒå‹æ¨¡ç³Šæé™å­¦ä¹ æœºçš„æ°´ä¸‹èˆªè¡Œå™¨è·¯å¾„è·Ÿè¸ªæœºè½½å£°å‘æ•°æ®åˆ†ç±»",
      "authors": [
        "Adrian Rubio-Solis",
        "Luciano Nava-Balanzar",
        "Tomas Salgado-Jimenez"
      ],
      "abstract": "In autonomous underwater missions, the successful completion of predefined paths mainly depends on the ability of underwater vehicles to recognise their surroundings. In this study, we apply the concept of Fast Interval Type-2 Fuzzy Extreme Learning Machine (FIT2-FELM) to train a Takagi-Sugeno-Kang IT2 Fuzzy Inference System (TSK IT2-FIS) for on-board sonar data classification using an underwater vehicle called BlueROV2. The TSK IT2-FIS is integrated into a Hierarchical Navigation Strategy (HNS) as the main navigation engine to infer local motions and provide the BlueROV2 with full autonomy to follow an obstacle-free trajectory in a water container of 2.5m x 2.5m x 3.5m. Compared to traditional navigation architectures, using the proposed method, we observe a robust path following behaviour in the presence of uncertainty and noise. We found that the proposed approach provides the BlueROV with a more complete sensory picture about its surroundings while real-time navigation planning is performed by the concurrent execution of two or more tasks.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹è‡ªä¸»æ°´ä¸‹èˆªè¡Œå™¨åœ¨è·¯å¾„è·Ÿéšä»»åŠ¡ä¸­å¯¹ç¯å¢ƒè¯†åˆ«çš„éœ€æ±‚ï¼Œæå‡ºäº†ä¸€ç§åŸºäº Fast Interval Type-2 Fuzzy Extreme Learning Machine (FIT2-FELM) çš„åœ¨çº¿å£°çº³æ•°æ®åˆ†ç±»æ–¹æ³•ã€‚è¯¥æ–¹æ³•ç”¨äºè®­ç»ƒ Takagi-Sugeno-Kang IT2 Fuzzy Inference System (TSK IT2-FIS)ï¼Œå¹¶å°†å…¶é›†æˆåˆ°åˆ†å±‚å¯¼èˆªç­–ç•¥ (Hierarchical Navigation Strategy, HNS) ä¸­ï¼Œä½œä¸º BlueROV2 èˆªè¡Œå™¨çš„æ ¸å¿ƒå¯¼èˆªå¼•æ“ä»¥æ¨æ–­å±€éƒ¨è¿åŠ¨ã€‚å®éªŒåœ¨ 2.5m x 2.5m x 3.5m çš„æ°´åŸŸç¯å¢ƒä¸­è¿›è¡Œï¼Œç»“æœè¡¨æ˜è¯¥ç³»ç»Ÿä½¿èˆªè¡Œå™¨èƒ½å¤Ÿå®ç°å®Œå…¨è‡ªä¸»çš„æ— éšœç¢è½¨è¿¹è·Ÿè¸ªã€‚ä¸ä¼ ç»Ÿå¯¼èˆªæ¶æ„ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨åº”å¯¹ä¸ç¡®å®šæ€§å’Œå™ªå£°æ—¶å±•ç°å‡ºæ›´ç¨³å¥çš„è·¯å¾„è·Ÿéšèƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œè¯¥æ–¹æ³•ä¸ä»…ä¸ºèˆªè¡Œå™¨æä¾›äº†æ›´å®Œæ•´çš„ç¯å¢ƒæ„Ÿå®˜å›¾æ™¯ï¼Œè¿˜æ”¯æŒé€šè¿‡å¹¶å‘æ‰§è¡Œå¤šé¡¹ä»»åŠ¡æ¥ä¼˜åŒ–å®æ—¶å¯¼èˆªè§„åˆ’ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.12762v1",
      "published_date": "2025-06-15 08:01:36 UTC",
      "updated_date": "2025-06-15 08:01:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:10:51.843559+00:00"
    },
    {
      "arxiv_id": "2506.12754v2",
      "title": "AFBS:Buffer Gradient Selection in Semi-asynchronous Federated Learning",
      "title_zh": "AFBSï¼šåŠå¼‚æ­¥è”é‚¦å­¦ä¹ ä¸­çš„ç¼“å­˜æ¢¯åº¦é€‰æ‹©",
      "authors": [
        "Chaoyi Lu",
        "Yiding Sun",
        "Jinqian Chen",
        "Zhichuan Yang",
        "Jiangming Pan",
        "Jihua Zhu"
      ],
      "abstract": "Asynchronous federated learning (AFL) accelerates training by eliminating the need to wait for stragglers, but its asynchronous nature introduces gradient staleness, where outdated gradients degrade performance. Existing solutions address this issue with gradient buffers, forming a semi-asynchronous framework. However, this approach struggles when buffers accumulate numerous stale gradients, as blindly aggregating all gradients can harm training. To address this, we propose AFBS (Asynchronous FL Buffer Selection), the first algorithm to perform gradient selection within buffers while ensuring privacy protection. Specifically, the client sends the random projection encrypted label distribution matrix before training, and the server performs client clustering based on it. During training, server scores and selects gradients within each cluster based on their informational value, discarding low-value gradients to enhance semi-asynchronous federated learning. Extensive experiments in highly heterogeneous system and data environments demonstrate AFBS's superior performance compared to state-of-the-art methods. Notably, on the most challenging task, CIFAR-100, AFBS improves accuracy by up to 4.8% over the previous best algorithm and reduces the time to reach target accuracy by 75%.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼‚æ­¥è”é‚¦å­¦ä¹ (Asynchronous Federated Learning, AFL)ä¸­æ¢¯åº¦è¿‡æ—¶(gradient staleness)å¯¼è‡´æ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼Œæå‡ºäº†é¦–ä¸ªæ”¯æŒéšç§ä¿æŠ¤çš„ç¼“å†²åŒºæ¢¯åº¦é€‰æ‹©ç®—æ³•AFBS (Asynchronous FL Buffer Selection)ã€‚åœ¨è®­ç»ƒå‰ï¼Œå®¢æˆ·ç«¯å‘é€ç»éšæœºæŠ•å½±åŠ å¯†çš„æ ‡ç­¾åˆ†å¸ƒçŸ©é˜µï¼Œç”±æœåŠ¡ç«¯å®Œæˆå®¢æˆ·ç«¯èšç±»(Client Clustering)ï¼›è®­ç»ƒæœŸé—´ï¼ŒæœåŠ¡ç«¯æ ¹æ®ä¿¡æ¯ä»·å€¼å¯¹å„èšç±»å†…çš„æ¢¯åº¦è¿›è¡Œè¯„åˆ†ä¸ç­›é€‰ï¼Œé€šè¿‡å‰”é™¤ä½ä»·å€¼æ¢¯åº¦æ¥ä¼˜åŒ–åŠå¼‚æ­¥å­¦ä¹ æ¡†æ¶ã€‚å®éªŒè¯æ˜ï¼ŒAFBSåœ¨é«˜åº¦å¼‚æ„çš„ç³»ç»Ÿå’Œæ•°æ®ç¯å¢ƒä¸‹å‡è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ”¶æ•›æ•ˆç‡ã€‚åœ¨CIFAR-100ä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•ç›¸æ¯”ç°æœ‰æœ€ä¼˜ç®—æ³•å°†å‡†ç¡®ç‡æå‡äº†4.8%ï¼Œå¹¶å°†è¾¾åˆ°ç›®æ ‡å‡†ç¡®ç‡çš„æ—¶é—´ç¼©çŸ­äº†75%ï¼Œä¸ºè§£å†³è”é‚¦å­¦ä¹ ä¸­çš„å¼‚æ­¥æŒ‘æˆ˜æä¾›äº†é«˜æ•ˆä¸”å®‰å…¨çš„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.12754v2",
      "published_date": "2025-06-15 07:42:46 UTC",
      "updated_date": "2025-06-23 05:27:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:10:52.236627+00:00"
    },
    {
      "arxiv_id": "2506.17286v2",
      "title": "GTA: Grouped-head latenT Attention",
      "title_zh": "GTAï¼šåˆ†ç»„å¤´éšå¼æ³¨æ„åŠ›",
      "authors": [
        "Luoyang Sun",
        "Cheng Deng",
        "Jiwen Jiang",
        "Xinjian Wu",
        "Haifeng Zhang",
        "Lei Chen",
        "Lionel Ni",
        "Jun Wang"
      ],
      "abstract": "Attention mechanisms underpin the success of large language models (LLMs), yet their substantial computational and memory overhead poses challenges for optimizing efficiency and performance. A critical bottleneck arises as KV cache and attention computations scale rapidly with text length, challenging deployment on hardware with limited computational and memory resources. We observe that attention mechanisms exhibit substantial redundancy, since the KV cache can be significantly compressed and attention maps across heads display high similarity, revealing that much of the computation and storage is unnecessary. Leveraging these insights, we propose \\textbf{G}rouped-Head Laten\\textbf{T} \\textbf{A}ttention (GTA), a novel attention mechanism that reduces memory usage and computational complexity while maintaining performance. GTA comprises two components: (1) a shared attention map mechanism that reuses attention scores across multiple heads, decreasing the key cache size; and (2) a nonlinear value decoder with learned projections that compresses the value cache into a latent space, further cutting memory needs. GTA cuts attention computation FLOPs by up to \\emph{62.5\\%} versus Grouped-Query Attention and shrink the KV cache by up to \\emph{70\\%}, all while avoiding the extra overhead of Multi-Head Latent Attention to improve LLM deployment efficiency. Consequently, GTA models achieve a \\emph{2x} increase in end-to-end inference speed, with prefill benefiting from reduced computational cost and decoding benefiting from the smaller cache footprint.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†GTA (Grouped-head latenT Attention)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸­æ³¨æ„åŠ›æœºåˆ¶è®¡ç®—ä¸å†…å­˜å¼€é”€è¿‡å¤§é—®é¢˜çš„åˆ›æ–°æœºåˆ¶ã€‚ç ”ç©¶è€…è§‚å¯Ÿåˆ°æ³¨æ„åŠ›æœºåˆ¶å­˜åœ¨æ˜¾è‘—å†—ä½™ï¼ŒKV cache å¯è¢«å¤§å¹…å‹ç¼©ä¸”ä¸åŒå¤´ä¹‹é—´çš„æ³¨æ„åŠ›å›¾å…·æœ‰é«˜åº¦ç›¸ä¼¼æ€§ï¼Œæ­ç¤ºäº†å¤§é‡è®¡ç®—å’Œå­˜å‚¨æ˜¯ä¸å¿…è¦çš„ã€‚GTA ä¸»è¦ç”±å…±äº«æ³¨æ„åŠ›å›¾æœºåˆ¶(Shared attention map mechanism)å’Œéçº¿æ€§æ•°å€¼è§£ç å™¨(Nonlinear value decoder)ç»„æˆï¼Œå‰è€…é€šè¿‡è·¨å¤´å¤ç”¨æ³¨æ„åŠ›åˆ†æ•°æ˜¾è‘—å‡å°‘ Key cacheï¼Œåè€…åˆ™åˆ©ç”¨å­¦ä¹ åˆ°çš„æŠ•å½±å°† Value cache å‹ç¼©è‡³æ½œç©ºé—´ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸ Grouped-Query Attention ç›¸æ¯”ï¼ŒGTA å¯å‡å°‘é«˜è¾¾ 62.5% çš„è®¡ç®—é‡å¹¶å‹ç¼© 70% çš„ KV cacheï¼ŒåŒæ—¶é¿å¼€äº† Multi-Head Latent Attention çš„é¢å¤–å¼€é”€ã€‚æœ€ç»ˆï¼ŒGTA å®ç°äº†ç«¯åˆ°ç«¯æ¨ç†é€Ÿåº¦ 2 å€çš„æå‡ï¼Œæ˜¾è‘—ä¼˜åŒ–äº† LLM åœ¨èµ„æºå—é™ç¡¬ä»¶ä¸Šçš„éƒ¨ç½²æ•ˆç‡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.17286v2",
      "published_date": "2025-06-15 07:19:33 UTC",
      "updated_date": "2025-07-23 05:57:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:10:47.311990+00:00"
    },
    {
      "arxiv_id": "2506.12747v2",
      "title": "Unleashing Diffusion and State Space Models for Medical Image Segmentation",
      "title_zh": "é‡Šæ”¾æ‰©æ•£ä¸çŠ¶æ€ç©ºé—´æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„æ•ˆèƒ½",
      "authors": [
        "Rong Wu",
        "Ziqi Chen",
        "Liming Zhong",
        "Heng Li",
        "Hai Shu"
      ],
      "abstract": "Existing segmentation models trained on a single medical imaging dataset often lack robustness when encountering unseen organs or tumors. Developing a robust model capable of identifying rare or novel tumor categories not present during training is crucial for advancing medical imaging applications. We propose DSM, a novel framework that leverages diffusion and state space models to segment unseen tumor categories beyond the training data. DSM utilizes two sets of object queries trained within modified attention decoders to enhance classification accuracy. Initially, the model learns organ queries using an object-aware feature grouping strategy to capture organ-level visual features. It then refines tumor queries by focusing on diffusion-based visual prompts, enabling precise segmentation of previously unseen tumors. Furthermore, we incorporate diffusion-guided feature fusion to improve semantic segmentation performance. By integrating CLIP text embeddings, DSM captures category-sensitive classes to improve linguistic transfer knowledge, thereby enhancing the model's robustness across diverse scenarios and multi-label tasks. Extensive experiments demonstrate the superior performance of DSM in various tumor segmentation tasks. Code is available at https://github.com/Rows21/k-Means_Mask_Mamba.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å•æ•°æ®é›†åˆ†å‰²æ¨¡å‹åœ¨é¢å¯¹æœªè§å™¨å®˜æˆ–è‚¿ç˜¤æ—¶é²æ£’æ€§ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº† DSM æ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨ Diffusion å’Œ State Space Models åˆ†å‰²è®­ç»ƒæ•°æ®ä¹‹å¤–çš„æœªè§è‚¿ç˜¤ç±»åˆ«ã€‚æ¨¡å‹åœ¨æ”¹è¿›çš„ Attention decoders ä¸­ä½¿ç”¨ Object-aware feature grouping ç­–ç•¥æ¥æ•æ‰å™¨å®˜çº§è§†è§‰ç‰¹å¾ï¼Œå¹¶é€šè¿‡ Diffusion-based visual prompts ç²¾ç‚¼è‚¿ç˜¤æŸ¥è¯¢ä»¥å®ç°ç²¾å‡†åˆ†å‰²ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº† Diffusion-guided feature fusion ä»¥ä¼˜åŒ–è¯­ä¹‰åˆ†å‰²æ€§èƒ½ï¼Œå¹¶æ•´åˆ CLIP æ–‡æœ¬åµŒå…¥æ¥å¢å¼ºç±»åˆ«æ•æ„Ÿæ€§ä¸è¯­è¨€è¿ç§»èƒ½åŠ›ã€‚å¤§é‡å®éªŒè¯æ˜ï¼ŒDSM åœ¨å¤šç§è‚¿ç˜¤åˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å¤šæ ·åŒ–åœºæ™¯å’Œå¤šæ ‡ç­¾ä»»åŠ¡ä¸­çš„ç¨³å¥æ€§ã€‚è¯¥æ–¹æ³•æˆåŠŸè§£å†³äº†è¯†åˆ«ç½•è§æˆ–æ–°å‹è‚¿ç˜¤çš„æŒ‘æˆ˜ï¼Œä¸ºåŒ»ç–—å½±åƒé¢†åŸŸçš„é€šç”¨åˆ†å‰²ä»»åŠ¡æä¾›äº†é«˜æ•ˆä¸”é²æ£’çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.12747v2",
      "published_date": "2025-06-15 07:07:14 UTC",
      "updated_date": "2025-07-01 07:16:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:11:01.693827+00:00"
    },
    {
      "arxiv_id": "2506.12738v1",
      "title": "Adaptive Dropout: Unleashing Dropout across Layers for Generalizable Image Super-Resolution",
      "title_zh": "Adaptive Dropoutï¼šé‡Šæ”¾è·¨å±‚ Dropout æ½œèƒ½ï¼Œæå‡å›¾åƒè¶…åˆ†è¾¨ç‡çš„æ³›åŒ–æ€§èƒ½",
      "authors": [
        "Hang Xu",
        "Wei Yu",
        "Jiangtong Tan",
        "Zhen Zou",
        "Feng Zhao"
      ],
      "abstract": "Blind Super-Resolution (blind SR) aims to enhance the model's generalization ability with unknown degradation, yet it still encounters severe overfitting issues. Some previous methods inspired by dropout, which enhances generalization by regularizing features, have shown promising results in blind SR. Nevertheless, these methods focus solely on regularizing features before the final layer and overlook the need for generalization in features at intermediate layers. Without explicit regularization of features at intermediate layers, the blind SR network struggles to obtain well-generalized feature representations. However, the key challenge is that directly applying dropout to intermediate layers leads to a significant performance drop, which we attribute to the inconsistency in training-testing and across layers it introduced. Therefore, we propose Adaptive Dropout, a new regularization method for blind SR models, which mitigates the inconsistency and facilitates application across intermediate layers of networks. Specifically, for training-testing inconsistency, we re-design the form of dropout and integrate the features before and after dropout adaptively. For inconsistency in generalization requirements across different layers, we innovatively design an adaptive training strategy to strengthen feature propagation by layer-wise annealing. Experimental results show that our method outperforms all past regularization methods on both synthetic and real-world benchmark datasets, also highly effective in other image restoration tasks. Code is available at \\href{https://github.com/xuhang07/Adpative-Dropout}{https://github.com/xuhang07/Adpative-Dropout}.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç›²è¶…åˆ†è¾¨ç‡(blind SR)ä»»åŠ¡ä¸­æ¨¡å‹å®¹æ˜“å‡ºç°ä¸¥é‡è¿‡æ‹Ÿåˆçš„é—®é¢˜ï¼Œæå‡ºäº†Adaptive Dropoutæ­£åˆ™åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨å…¨é¢æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚ä»¥å¾€æ–¹æ³•å¤šå±€é™äºåœ¨æœ€ç»ˆå±‚ä¹‹å‰åº”ç”¨dropoutï¼Œè€Œå¿½è§†äº†ä¸­é—´å±‚ç‰¹å¾çš„æ­£åˆ™åŒ–éœ€æ±‚ï¼Œä½†ç›´æ¥åœ¨ä¸­é—´å±‚ä½¿ç”¨dropoutä¼šå¯¼è‡´è®­ç»ƒ-æµ‹è¯•(training-testing)ä»¥åŠè·¨å±‚çš„ä¸€è‡´æ€§é—®é¢˜ã€‚ä¸ºè§£å†³æ­¤æŒ‘æˆ˜ï¼Œè¯¥ç ”ç©¶é‡æ–°è®¾è®¡äº†dropoutçš„å½¢å¼å¹¶è‡ªé€‚åº”åœ°é›†æˆç‰¹å¾ï¼Œä»è€Œç¼“è§£è®­ç»ƒä¸æµ‹è¯•é—´çš„ä¸ä¸€è‡´ã€‚åŒæ—¶ï¼Œç ”ç©¶å¼•å…¥äº†åŸºäºå±‚çº§é€€ç«(layer-wise annealing)çš„è‡ªé€‚åº”è®­ç»ƒç­–ç•¥ï¼Œé€šè¿‡é€æ­¥è°ƒæ•´æ¥ä¼˜åŒ–ä¸åŒå±‚é—´çš„ç‰¹å¾ä¼ æ’­ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒAdaptive Dropoutåœ¨åˆæˆä¸çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå‡ä¼˜äºä»¥å¾€çš„æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œä¸”åœ¨å¤šç§å›¾åƒä¿®å¤(image restoration)ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºæå¼ºçš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 8 figures, CVPR2025",
      "pdf_url": "https://arxiv.org/pdf/2506.12738v1",
      "published_date": "2025-06-15 06:21:39 UTC",
      "updated_date": "2025-06-15 06:21:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:11:08.345407+00:00"
    },
    {
      "arxiv_id": "2506.12735v1",
      "title": "Revealing the Challenges of Sim-to-Real Transfer in Model-Based Reinforcement Learning via Latent Space Modeling",
      "title_zh": "é€šè¿‡éšç©ºé—´å»ºæ¨¡æ­ç¤ºåŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ åœ¨ Sim-to-Real è¿ç§»ä¸­çš„æŒ‘æˆ˜",
      "authors": [
        "Zhilin Lin",
        "Shiliang Sun"
      ],
      "abstract": "Reinforcement learning (RL) is playing an increasingly important role in fields such as robotic control and autonomous driving. However, the gap between simulation and the real environment remains a major obstacle to the practical deployment of RL. Agents trained in simulators often struggle to maintain performance when transferred to real-world physical environments. In this paper, we propose a latent space based approach to analyze the impact of simulation on real-world policy improvement in model-based settings. As a natural extension of model-based methods, our approach enables an intuitive observation of the challenges faced by model-based methods in sim-to-real transfer. Experiments conducted in the MuJoCo environment evaluate the performance of our method in both measuring and mitigating the sim-to-real gap. The experiments also highlight the various challenges that remain in overcoming the sim-to-real gap, especially for model-based methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)åœ¨æœºå™¨äººæ§åˆ¶ç­‰é¢†åŸŸçš„åº”ç”¨ï¼Œé‡ç‚¹å…³æ³¨ä»¿çœŸä¸ç°å®ç¯å¢ƒä¹‹é—´çš„å·®è·(Sim-to-Real gap)è¿™ä¸€æ ¸å¿ƒéšœç¢ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åŸºäºæ½œç©ºé—´å»ºæ¨¡(Latent Space Modeling)çš„æ–¹æ³•ï¼Œæ—¨åœ¨åˆ†ææ¨¡å‹åŒ–å¼ºåŒ–å­¦ä¹ (Model-Based Reinforcement Learning)åœ¨ä»ä»¿çœŸå‘ç°å®è¿ç§»æ—¶å¯¹ç­–ç•¥æ”¹è¿›çš„å½±å“ã€‚è¯¥æ–¹æ³•ä½œä¸ºæ¨¡å‹åŒ–æ–¹æ³•çš„å»¶ä¼¸ï¼Œèƒ½å¤Ÿç›´è§‚åœ°è§‚å¯Ÿåˆ°æ¨¡å‹åŒ–æ–¹æ³•åœ¨è¿ç§»è¿‡ç¨‹ä¸­é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¹¶æä¾›äº†è¡¡é‡å’Œç¼“è§£è¿™äº›å·®è·çš„æ–°é€”å¾„ã€‚é€šè¿‡åœ¨MuJoCoç¯å¢ƒä¸­çš„å®éªŒï¼Œç ”ç©¶è¯„ä¼°äº†è¯¥æ–¹æ¡ˆåœ¨å‡å°Sim-to-Realå·®è·æ–¹é¢çš„æ€§èƒ½ã€‚å®éªŒç»“æœä¸ä»…è¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¿˜è¿›ä¸€æ­¥æ­ç¤ºäº†åœ¨æ¨¡å‹åŒ–è®¾ç½®ä¸‹å½»åº•å…‹æœä»¿çœŸä¸ç°å®å·®è·æ‰€ä¾ç„¶é¢ä¸´çš„è¯¸å¤šæŒ‘æˆ˜ã€‚è¯¥é¡¹å·¥ä½œä¸ºæ·±å…¥ç†è§£å¼ºåŒ–å­¦ä¹ çš„è·¨åŸŸè¿ç§»éš¾é¢˜æä¾›äº†é‡è¦çš„åˆ†æå·¥å…·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.12735v1",
      "published_date": "2025-06-15 06:02:42 UTC",
      "updated_date": "2025-06-15 06:02:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:11:08.511241+00:00"
    },
    {
      "arxiv_id": "2506.15733v1",
      "title": "$\\texttt{SPECS}$: Faster Test-Time Scaling through Speculative Drafts",
      "title_zh": "SPECSï¼šåŸºäºæŠ•æœºæ€§è‰ç¨¿åŠ é€Ÿæµ‹è¯•æ—¶æ‰©å±•",
      "authors": [
        "Mert Cemri",
        "Nived Rajaraman",
        "Rishabh Tiwari",
        "Xiaoxuan Liu",
        "Kurt Keutzer",
        "Ion Stoica",
        "Kannan Ramchandran",
        "Ahmad Beirami",
        "Ziteng Sun"
      ],
      "abstract": "Scaling test-time compute has driven the recent advances in the reasoning capabilities of large language models (LLMs), typically by allocating additional computation for more thorough exploration. However, increased compute often comes at the expense of higher user-facing latency, directly impacting user experience. Current test-time scaling methods primarily optimize for accuracy based on total compute resources (FLOPS), often overlooking latency constraints. To address this gap, we propose $\\texttt{SPECS}$, a latency-aware test-time scaling method inspired by speculative decoding. $\\texttt{SPECS}$~uses a smaller, faster model to generate candidate sequences efficiently, and evaluates these candidates using signals from both a larger target model and a dedicated reward model. We introduce new integration strategies, including reward-guided soft verification and a reward-based deferral mechanism. Empirical results on MATH500, AMC23 and OlympiadBench datasets show that $\\texttt{SPECS}$~matches or surpasses beam search accuracy while reducing latency by up to $\\sim$19.1\\%. Our theoretical analysis shows that our algorithm converges to the solution of a KL-regularized reinforcement learning objective with increasing beam width.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† $\\texttt{SPECS}$ï¼Œä¸€ç§å—æŠ•æœºè§£ç (Speculative Decoding)å¯å‘çš„å»¶è¿Ÿæ„ŸçŸ¥(Latency-aware)æµ‹è¯•æ—¶ç¼©æ”¾(Test-time Scaling)æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ¨ç†é˜¶æ®µé€šè¿‡å¢åŠ è®¡ç®—é‡æå‡æ€§èƒ½æ—¶æ‰€å¸¦æ¥çš„é«˜å»¶è¿Ÿé—®é¢˜ã€‚$\\texttt{SPECS}$ åˆ©ç”¨è¾ƒå°ä¸”æ›´å¿«çš„æ¨¡å‹é«˜æ•ˆç”Ÿæˆå€™é€‰åºåˆ—ï¼Œå¹¶ç»“åˆå¤§ç›®æ ‡æ¨¡å‹å’Œä¸“ç”¨å¥–åŠ±æ¨¡å‹(Reward Model)çš„ä¿¡å·å¯¹å€™é€‰åºåˆ—è¿›è¡Œè¯„ä¼°ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†å¥–åŠ±å¼•å¯¼çš„è½¯éªŒè¯(Reward-guided Soft Verification)å’ŒåŸºäºå¥–åŠ±çš„æ¨è¿Ÿæœºåˆ¶(Reward-based Deferral Mechanism)ç­‰é›†æˆç­–ç•¥ï¼Œä»¥ä¼˜åŒ–è®¡ç®—èµ„æºåˆ†é…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ MATH500ã€AMC23 å’Œ OlympiadBench ç­‰æ•°æ®é›†ä¸Šï¼Œ$\\texttt{SPECS}$ åœ¨å‡†ç¡®ç‡ä¸Šè¾¾åˆ°æˆ–è¶…è¿‡äº†ä¼ ç»Ÿçš„æŸæœç´¢(Beam Search)ï¼ŒåŒæ—¶å°†å»¶è¿Ÿé™ä½äº†çº¦19.1%ã€‚ç†è®ºåˆ†æè¿›ä¸€æ­¥è¯æ˜ï¼Œè¯¥ç®—æ³•èƒ½éšæŸå®½(Beam Width)å¢åŠ æ”¶æ•›è‡³ KL æ­£åˆ™åŒ–å¼ºåŒ–å­¦ä¹ (KL-regularized Reinforcement Learning)ç›®æ ‡çš„è§£ï¼Œä¸ºå…¼é¡¾æ¨ç†ç²¾åº¦ä¸æ•ˆç‡æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "28 pages, 6 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.15733v1",
      "published_date": "2025-06-15 05:50:05 UTC",
      "updated_date": "2025-06-15 05:50:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:11:07.644447+00:00"
    },
    {
      "arxiv_id": "2506.12730v1",
      "title": "Decentralized Decision Making in Two Sided Manufacturing-as-a-Service Marketplaces",
      "title_zh": "åŒè¾¹åˆ¶é€ å³æœåŠ¡å¸‚åœºä¸­çš„å»ä¸­å¿ƒåŒ–å†³ç­–",
      "authors": [
        "Deepak Pahwa"
      ],
      "abstract": "Advancements in digitization have enabled two sided manufacturing-as-a-service (MaaS) marketplaces which has significantly reduced product development time for designers. These platforms provide designers with access to manufacturing resources through a network of suppliers and have instant order placement capabilities. Two key decision making levers are typically used to optimize the operations of these marketplaces: pricing and matching. The existing marketplaces operate in a centralized structure where they have complete control over decision making. However, a decentralized organization of the platform enables transparency of information across clients and suppliers. This dissertation focuses on developing tools for decision making enabling decentralization in MaaS marketplaces. In pricing mechanisms, a data driven method is introduced which enables small service providers to price services based on specific attributes of the services offered. A data mining method recommends a network based price to a supplier based on its attributes and the attributes of other suppliers on the platform. Three different approaches are considered for matching mechanisms. First, a reverse auction mechanism is introduced where designers bid for manufacturing services and the mechanism chooses a supplier which can match the bid requirements and stated price. The second approach uses mechanism design and mathematical programming to develop a stable matching mechanism for matching orders to suppliers based on their preferences. Empirical simulations are used to test the mechanisms in a simulated 3D printing marketplace and to evaluate the impact of stability on its performance. The third approach considers the matching problem in a dynamic and stochastic environment where demand (orders) and supply (supplier capacities) arrive over time and matching is performed online.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨åŒè¾¹åˆ¶é€ å³æœåŠ¡(Manufacturing-as-a-Service, MaaS)å¸‚åœºä¸­å®ç°å»ä¸­å¿ƒåŒ–å†³ç­–çš„æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡æé«˜ä¿¡æ¯é€æ˜åº¦æ¥ä¼˜åŒ–ä¼ ç»Ÿä¸­å¿ƒåŒ–å¹³å°çš„è¿è¥æ•ˆç‡ã€‚é’ˆå¯¹å®šä»·æœºåˆ¶ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§æ•°æ®é©±åŠ¨çš„æ–¹æ³•ï¼Œä½¿å°å‹æœåŠ¡æä¾›å•†èƒ½å¤Ÿæ ¹æ®ç‰¹å®šæœåŠ¡å±æ€§è¿›è¡Œå®šä»·ï¼Œå¹¶åˆ©ç”¨æ•°æ®æŒ–æ˜æŠ€æœ¯ä¸ºä¾›åº”å•†æä¾›åŸºäºç½‘ç»œçš„ä»·æ ¼æ¨èã€‚åœ¨åŒ¹é…æœºåˆ¶æ–¹é¢ï¼Œç ”ç©¶å¼•å…¥äº†åå‘æ‹å–æœºåˆ¶(Reverse Auction Mechanism)ä¾›è®¾è®¡è€…æŠ•æ ‡ï¼Œå¹¶ç»“åˆæœºåˆ¶è®¾è®¡(Mechanism Design)ä¸æ•°å­¦è§„åˆ’å¼€å‘äº†åŸºäºåå¥½çš„ç¨³å®šåŒ¹é…ç³»ç»Ÿã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜è§£å†³äº†éœ€æ±‚å’Œä¾›åº”åŠ¨æ€éšæœºåˆ°è¾¾æ—¶çš„åœ¨çº¿åŒ¹é…éš¾é¢˜ã€‚é€šè¿‡åœ¨3Dæ‰“å°å¸‚åœºè¿›è¡Œçš„å®è¯æ¨¡æ‹Ÿï¼Œè®ºæ–‡éªŒè¯äº†æ‰€ææœºåˆ¶çš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¯„ä¼°äº†ç¨³å®šæ€§å¯¹å¸‚åœºç»©æ•ˆçš„å½±å“ï¼Œä¸ºæ„å»ºé«˜æ•ˆã€é€æ˜çš„å»ä¸­å¿ƒåŒ–MaaSå¹³å°æä¾›äº†å…³é”®çš„å†³ç­–æ”¯æŒå·¥å…·ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.12730v1",
      "published_date": "2025-06-15 05:43:21 UTC",
      "updated_date": "2025-06-15 05:43:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:11:11.186356+00:00"
    },
    {
      "arxiv_id": "2507.00018v2",
      "title": "Implicit Reward as the Bridge: A Unified View of SFT and DPO Connections",
      "title_zh": "ä»¥éšå¼å¥–åŠ±ä¸ºæ¡¥æ¢ï¼šSFT ä¸ DPO å…³è”çš„ç»Ÿä¸€è§†è§’",
      "authors": [
        "Bo Wang",
        "Qinyuan Cheng",
        "Runyu Peng",
        "Rong Bao",
        "Peiji Li",
        "Qipeng Guo",
        "Linyang Li",
        "Zhiyuan Zeng",
        "Yunhua Zhou",
        "Xipeng Qiu"
      ],
      "abstract": "Post-training processes are essential phases in grounding pre-trained language models to real-world tasks, with learning from demonstrations or preference signals playing a crucial role in this adaptation. We present a unified theoretical framework bridging Supervised Fine-Tuning (SFT) and preference learning in Large Language Model (LLM) post-training. Through rigorous mathematical derivation, we demonstrate that both SFT and preference learning methods like Direct Preference Optimization (DPO) operate within the same optimal policy-reward subspace, with SFT representing a special case of implicit reward learning. Our analysis reveals a critical limitation in conventional SFT: the KL divergence term in distribution matching becomes constant with respect to the policy during optimization, failing to constrain model updates. To address this, we propose a simple yet effective learning rate reduction approach that yields significant performance improvements (up to \\textbf{25\\%} relative gain and \\textbf{6\\%} absolute win rate increase in instruction following tasks. Additionally, we derive alternative SFT objectives from various f-divergence functions that preserve the KL term during optimization, further enhancing post-DPO model performance. Finally, we extend the theoretical relationship between LLM logits and Q-functions from preference learning to the SFT context, providing mathematical derivations and experimental validation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„ç†è®ºæ¡†æ¶ï¼Œæ—¨åœ¨å»ºç«‹å¤§è¯­è¨€æ¨¡å‹(LLM)åæœŸè®­ç»ƒä¸­ç›‘ç£å¾®è°ƒ(SFT)ä¸åå¥½å­¦ä¹ (Preference Learning)ä¹‹é—´çš„è”ç³»ã€‚é€šè¿‡æ•°å­¦æ¨å¯¼ï¼Œä½œè€…è¯æ˜äº†SFTå’ŒDPOç­‰æ–¹æ³•å‡åœ¨ç›¸åŒçš„æœ€ä¼˜ç­–ç•¥-å¥–åŠ±å­ç©ºé—´(optimal policy-reward subspace)å†…è¿è¡Œï¼Œä¸”SFTå¯è¢«è§†ä¸ºéšå¼å¥–åŠ±å­¦ä¹ (implicit reward learning)çš„ä¸€ç§ç‰¹ä¾‹ã€‚ç ”ç©¶æ­ç¤ºäº†ä¼ ç»ŸSFTçš„ä¸€ä¸ªå…³é”®ç¼ºé™·ï¼Œå³ä¼˜åŒ–è¿‡ç¨‹ä¸­åˆ†å¸ƒåŒ¹é…çš„KLæ•£åº¦(KL divergence)é¡¹ä¼šå˜ä¸ºå¸¸æ•°ï¼Œä»è€Œå¤±å»äº†å¯¹æ¨¡å‹æ›´æ–°çš„çº¦æŸä½œç”¨ã€‚ä¸ºæ”¹è¿›è¿™ä¸€ç°çŠ¶ï¼Œç ”ç©¶æå‡ºé€šè¿‡é™ä½å­¦ä¹ ç‡æ¥æå‡æ€§èƒ½ï¼Œå¹¶åœ¨æŒ‡ä»¤éµå¾ªä»»åŠ¡ä¸­è·å¾—äº†æœ€é«˜25%çš„ç›¸å¯¹å¢ç›Šã€‚æ­¤å¤–ï¼Œä½œè€…åˆ©ç”¨f-æ•£åº¦(f-divergence)å¯¼å‡ºäº†æ–°å‹SFTç›®æ ‡å‡½æ•°ä»¥ä¿ç•™KLçº¦æŸé¡¹ï¼Œæœ‰æ•ˆå¢å¼ºäº†DPOåçš„æ¨¡å‹è¡¨ç°ã€‚æœ€åï¼Œè¯¥ç ”ç©¶æˆåŠŸå°†Logitsä¸Qå‡½æ•°ä¹‹é—´çš„ç†è®ºå…³è”ä»åå¥½å­¦ä¹ æ‰©å±•åˆ°äº†SFTé¢†åŸŸï¼Œå¹¶å®Œæˆäº†å®éªŒéªŒè¯ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.00018v2",
      "published_date": "2025-06-15 05:42:29 UTC",
      "updated_date": "2025-07-04 08:16:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:11:25.991029+00:00"
    },
    {
      "arxiv_id": "2506.12725v1",
      "title": "Rethinking DPO: The Role of Rejected Responses in Preference Misalignment",
      "title_zh": "é‡æ–°å®¡è§† DPOï¼šæ‹’ç»å“åº”åœ¨åå¥½å¤±è°ƒä¸­çš„ä½œç”¨",
      "authors": [
        "Jay Hyeon Cho",
        "JunHyeok Oh",
        "Myunsoo Kim",
        "Byung-Jun Lee"
      ],
      "abstract": "Direct Preference Optimization (DPO) is a simple and efficient framework that has attracted substantial attention. However, it often struggles to meet its primary objectives -- increasing the generation probability of chosen responses while reducing that of rejected responses -- due to the dominant influence of rejected responses on the loss function. This imbalance leads to suboptimal performance in promoting preferred responses. In this work, we systematically analyze the limitations of DPO and existing algorithms designed to achieve the objectives stated above. To address these limitations, we propose Bounded-DPO (BDPO), a novel method that bounds the influence of rejected responses while maintaining the original optimization structure of DPO. Through theoretical analysis and empirical evaluations, we demonstrate that BDPO achieves a balanced optimization of the chosen and rejected responses, outperforming existing algorithms.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDirect Preference Optimization, DPOï¼‰åœ¨åå¥½å¯¹é½ä¸­çš„è¡¨ç°ï¼Œå‘ç°è¢«æ‹’ç»å›å¤ï¼ˆrejected responsesï¼‰åœ¨æŸå¤±å‡½æ•°ä¸­å æ®ä¸»å¯¼åœ°ä½ï¼Œå¯¼è‡´å…¶éš¾ä»¥å¹³è¡¡æå‡é€‰ä¸­å›å¤ï¼ˆchosen responsesï¼‰æ¦‚ç‡ä¸é™ä½è¢«æ‹’ç»å›å¤æ¦‚ç‡çš„ç›®æ ‡ã€‚è¿™ç§ä¸å¹³è¡¡é™åˆ¶äº†DPOåœ¨æ¨å¹¿é¦–é€‰å›å¤æ–¹é¢çš„æ€§èƒ½ï¼Œè¿›è€Œå½±å“äº†æ¨¡å‹çš„æœ€ç»ˆå¯¹é½æ•ˆæœã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œä½œè€…æå‡ºäº†Bounded-DPOï¼ˆBDPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åœ¨ä¿ç•™DPOåŸå§‹ä¼˜åŒ–ç»“æ„åŸºç¡€ä¸Šï¼Œé€šè¿‡é™åˆ¶è¢«æ‹’ç»å›å¤å½±å“åŠ›çš„åˆ›æ–°æ–¹æ³•ã€‚ç†è®ºåˆ†æå’Œå®è¯è¯„ä¼°å‡è¯æ˜ï¼ŒBDPOèƒ½å¤Ÿå®ç°é€‰ä¸­ä¸è¢«æ‹’ç»å›å¤ä¹‹é—´çš„å¹³è¡¡ä¼˜åŒ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBDPOçš„è¡¨ç°ä¼˜äºç°æœ‰çš„å¤šç§ä¼˜åŒ–ç®—æ³•ï¼Œä¸ºæå‡å¤§è¯­è¨€æ¨¡å‹çš„åå¥½å¯¹é½è´¨é‡æä¾›äº†æœ‰æ•ˆçš„è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.12725v1",
      "published_date": "2025-06-15 05:32:07 UTC",
      "updated_date": "2025-06-15 05:32:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:11:27.699464+00:00"
    },
    {
      "arxiv_id": "2506.12723v3",
      "title": "SP-VLA: A Joint Model Scheduling and Token Pruning Approach for VLA Model Acceleration",
      "title_zh": "SP-VLAï¼šè”åˆæ¨¡å‹è°ƒåº¦ä¸è¯å…ƒå‰ªæçš„ VLA æ¨¡å‹åŠ é€Ÿæ–¹æ³•",
      "authors": [
        "Ye Li",
        "Yuan Meng",
        "Zewen Sun",
        "Kangye Ji",
        "Chen Tang",
        "Jiajun Fan",
        "Xinzhu Ma",
        "Shutao Xia",
        "Zhi Wang",
        "Wenwu Zhu"
      ],
      "abstract": "Vision-Language-Action (VLA) models have attracted increasing attention for their strong control capabilities. However, their high computational cost and low execution frequency hinder their suitability for real-time tasks such as robotic manipulation and autonomous navigation. Existing VLA acceleration methods primarily focus on structural optimization, overlooking the fact that these models operate in sequential decision-making environments. As a result, temporal redundancy in sequential action generation and spatial redundancy in visual input remain unaddressed. To this end, we propose SP-VLA, a unified framework that accelerates VLA models by jointly scheduling models and pruning tokens. Specifically, we design an action-aware model scheduling mechanism that reduces temporal redundancy by dynamically switching between VLA model and a lightweight generator. Inspired by the human motion pattern of focusing on key decision points while relying on intuition for other actions, we categorize VLA actions into deliberative and intuitive, assigning the former to the VLA model and the latter to the lightweight generator, enabling frequency-adaptive execution through collaborative model scheduling. To address spatial redundancy, we further develop a spatio-semantic dual-aware token pruning method. Tokens are classified into spatial and semantic types and pruned based on their dual-aware importance to accelerate VLA inference. These two mechanisms work jointly to guide the VLA in focusing on critical actions and salient visual information, achieving effective acceleration while maintaining high accuracy. Extensive experiments show that our method achieves 1.5$\\times$ lossless acceleration in LIBERO and 2.4$\\times$ in SimplerEnv, with up to 6% average performance gain. Inference frequency and latency improve by 2.2$\\times$ in SimplerEnv and 1.4$\\times$ in LIBERO.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SP-VLAï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡è”åˆæ¨¡å‹è°ƒåº¦(Model Scheduling)å’Œä»¤ç‰Œå‰ªæ(Token Pruning)æ¥åŠ é€Ÿè§†è§‰-è¯­è¨€-åŠ¨ä½œ(VLA)æ¨¡å‹çš„ç»Ÿä¸€æ¡†æ¶ã€‚é’ˆå¯¹VLAæ¨¡å‹åœ¨å®æ—¶æœºå™¨äººæ“çºµå’Œè‡ªä¸»å¯¼èˆªä¸­é¢ä¸´çš„è®¡ç®—æˆæœ¬é«˜åŠæ‰§è¡Œé¢‘ç‡ä½ç­‰é—®é¢˜ï¼ŒSP-VLAè®¾è®¡äº†ä¸€ç§åŠ¨ä½œæ„ŸçŸ¥(Action-aware)çš„æ¨¡å‹è°ƒåº¦æœºåˆ¶ï¼Œé€šè¿‡åœ¨VLAæ¨¡å‹ä¸è½»é‡çº§ç”Ÿæˆå™¨(Lightweight Generator)ä¹‹é—´åŠ¨æ€åˆ‡æ¢æ¥å‡å°‘åŠ¨ä½œç”Ÿæˆä¸­çš„æ—¶é—´å†—ä½™ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜å¼•å…¥äº†ç©ºé—´-è¯­ä¹‰åŒé‡æ„ŸçŸ¥(Spatio-semantic dual-aware)çš„Token Pruningæ–¹æ³•ï¼Œé’ˆå¯¹è§†è§‰è¾“å…¥è¿›è¡Œåˆ†ç±»å‰ªæä»¥è§£å†³ç©ºé—´å†—ä½™ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSP-VLAåœ¨LIBEROå’ŒSimplerEnvåŸºå‡†æµ‹è¯•ä¸­åˆ†åˆ«å®ç°äº†1.5å€å’Œ2.4å€çš„æ— æŸåŠ é€Ÿï¼Œå¹¶å°†æ¨ç†é¢‘ç‡æœ€é«˜æå‡äº†2.2å€ã€‚è¯¥æ–¹æ³•æˆåŠŸå¼•å¯¼æ¨¡å‹ä¸“æ³¨äºå…³é”®åŠ¨ä½œå’Œæ˜¾è‘—è§†è§‰ä¿¡æ¯ï¼Œåœ¨æå‡æ‰§è¡Œæ•ˆç‡çš„åŒæ—¶ä¿æŒç”šè‡³å¢å¼ºäº†æ¨¡å‹çš„æ§åˆ¶ç²¾åº¦ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.12723v3",
      "published_date": "2025-06-15 05:04:17 UTC",
      "updated_date": "2025-10-03 02:47:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:11:33.846496+00:00"
    },
    {
      "arxiv_id": "2506.12721v1",
      "title": "Strategic Scaling of Test-Time Compute: A Bandit Learning Approach",
      "title_zh": "ç­–ç•¥æ€§ç¼©æ”¾æ¨ç†æ—¶ç®—åŠ›ï¼šåŸºäº Bandit å­¦ä¹ çš„æ–¹æ³•",
      "authors": [
        "Bowen Zuo",
        "Yinglun Zhu"
      ],
      "abstract": "Scaling test-time compute has emerged as an effective strategy for improving the performance of large language models. However, existing methods typically allocate compute uniformly across all queries, overlooking variation in query difficulty. To address this inefficiency, we formulate test-time compute allocation as a novel bandit learning problem and propose adaptive algorithms that estimate query difficulty on the fly and allocate compute accordingly. Compared to uniform allocation, our algorithms allocate more compute to challenging queries while maintaining accuracy on easier ones. Among challenging queries, our algorithms further learn to prioritize solvable instances, effectively reducing excessive computing on unsolvable queries. We theoretically prove that our algorithms achieve better compute efficiency than uniform allocation and empirically validate their effectiveness on math and code benchmarks. Specifically, our algorithms achieve up to an 11.10% performance improvement (15.04% relative) on the MATH-500 dataset and up to a 7.41% performance improvement (14.40% relative) on LiveCodeBench.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(Large Language Models)åœ¨æ¨ç†é˜¶æ®µç»Ÿä¸€åˆ†é…è®¡ç®—èµ„æºå¯¼è‡´çš„æ•ˆç‡ä½ä¸‹é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¤šè‡‚è€è™æœºå­¦ä¹ (Bandit Learning)çš„è‡ªé€‚åº”è®¡ç®—åˆ†é…æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡åŠ¨æ€ä¼°è®¡æŸ¥è¯¢éš¾åº¦ï¼Œå°†æ›´å¤šè®¡ç®—èµ„æºåˆ†é…ç»™å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼ŒåŒæ—¶ä¿æŒç®€å•æŸ¥è¯¢çš„å‡†ç¡®æ€§ã€‚ç®—æ³•è¿›ä¸€æ­¥å­¦ä¹ ä¼˜å…ˆå¤„ç†å›°éš¾ä½†å¯è§£å†³çš„å®ä¾‹ï¼Œä»è€Œæœ‰æ•ˆå‡å°‘äº†åœ¨ä¸å¯è§£æŸ¥è¯¢ä¸Šçš„è¿‡åº¦è®¡ç®—æ¶ˆè€—ã€‚ç†è®ºè¯æ˜è¡¨æ˜ï¼Œè¯¥ç®—æ³•æ¯”ä¼ ç»Ÿçš„ç»Ÿä¸€åˆ†é…æ–¹å¼å…·æœ‰æ›´é«˜çš„è®¡ç®—æ•ˆç‡ã€‚åœ¨æ•°å­¦å’Œç¼–ç¨‹åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨MATH-500æ•°æ®é›†ä¸Šå®ç°äº†11.10%çš„æ€§èƒ½æå‡ï¼Œåœ¨LiveCodeBenchä¸Šæå‡äº†7.41%ã€‚è¿™ä¸€ç ”ç©¶ä¸ºç­–ç•¥æ€§æ‰©å±•æ¨ç†ä¾§è®¡ç®—(Test-Time Compute)æä¾›äº†é«˜æ•ˆä¸”å¯è¯æ˜çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.12721v1",
      "published_date": "2025-06-15 04:55:49 UTC",
      "updated_date": "2025-06-15 04:55:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:11:32.127827+00:00"
    },
    {
      "arxiv_id": "2506.13820v1",
      "title": "Structured Program Synthesis using LLMs: Results and Insights from the IPARC Challenge",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ç»“æ„åŒ–ç¨‹åºåˆæˆï¼šIPARC æŒ‘æˆ˜èµ›çš„ç»“æœä¸å¯ç¤º",
      "authors": [
        "Shraddha Surana",
        "Ashwin Srinivasan",
        "Michael Bain"
      ],
      "abstract": "The IPARC Challenge, inspired by ARC, provides controlled program synthesis tasks over synthetic images to evaluate automatic program construction, focusing on sequence, selection, and iteration. This set of 600 tasks has resisted automated solutions. This paper presents a structured inductive programming approach with LLMs that successfully solves tasks across all IPARC categories. The controlled nature of IPARC reveals insights into LLM-based code generation, including the importance of prior structuring, LLMs' ability to aid structuring (requiring human refinement), the need to freeze correct code, the efficiency of code reuse, and how LLM-generated code can spark human creativity. These findings suggest valuable mechanisms for human-LLM collaboration in tackling complex program synthesis.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å—ARCå¯å‘çš„IPARC Challengeï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œç»“æ„åŒ–å½’çº³ç¼–ç¨‹ï¼ˆstructured inductive programmingï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³åŒ…å«åºåˆ—ï¼ˆsequenceï¼‰ã€é€‰æ‹©ï¼ˆselectionï¼‰å’Œè¿­ä»£ï¼ˆiterationï¼‰ç­‰600é¡¹å—æ§ç¨‹åºåˆæˆä»»åŠ¡ã€‚è¯¥æ–¹æ³•æˆåŠŸè§£å†³äº†IPARCæ‰€æœ‰ç±»åˆ«çš„ä»»åŠ¡ï¼Œæ‰“ç ´äº†ä»¥å¾€è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆéš¾ä»¥æ”»å…‹è¯¥æŒ‘æˆ˜çš„å±€é¢ã€‚ç ”ç©¶æ­ç¤ºäº†LLMä»£ç ç”Ÿæˆçš„å…³é”®æ´å¯Ÿï¼Œå¼ºè°ƒäº†å…ˆéªŒç»“æ„åŒ–ï¼ˆprior structuringï¼‰çš„é‡è¦æ€§ï¼Œä»¥åŠLLMåœ¨è¾…åŠ©ç»“æ„åŒ–è¿‡ç¨‹ä¸­ä»éœ€äººå·¥ç»†åŒ–çš„ç°çŠ¶ã€‚æ­¤å¤–ï¼Œå®éªŒè¡¨æ˜å†»ç»“æ­£ç¡®ä»£ç ï¼ˆfreezing correct codeï¼‰å’Œä»£ç å¤ç”¨ï¼ˆcode reuseï¼‰èƒ½æ˜¾è‘—æå‡æ•ˆç‡ï¼Œä¸”LLMç”Ÿæˆçš„ä»£ç å…·æœ‰æ¿€å‘äººç±»åˆ›é€ åŠ›çš„æ½œåŠ›ã€‚è¿™äº›å‘ç°ä¸ºå¤æ‚ç¨‹åºåˆæˆä»»åŠ¡ä¸­æ„å»ºé«˜æ•ˆçš„äººç±»ä¸LLMåä½œæœºåˆ¶ï¼ˆhuman-LLM collaborationï¼‰æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.PL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.13820v1",
      "published_date": "2025-06-15 04:33:00 UTC",
      "updated_date": "2025-06-15 04:33:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:11:33.240596+00:00"
    },
    {
      "arxiv_id": "2506.12708v3",
      "title": "Serving Large Language Models on Huawei CloudMatrix384",
      "title_zh": "Huawei CloudMatrix384 ä¸Šçš„å¤§è¯­è¨€æ¨¡å‹æ¨ç†æœåŠ¡",
      "authors": [
        "Pengfei Zuo",
        "Huimin Lin",
        "Junbo Deng",
        "Nan Zou",
        "Xingkun Yang",
        "Yingyu Diao",
        "Weifeng Gao",
        "Ke Xu",
        "Zhangyu Chen",
        "Shirui Lu",
        "Zhao Qiu",
        "Peiyang Li",
        "Xianyu Chang",
        "Zhengzhong Yu",
        "Fangzheng Miao",
        "Jia Zheng",
        "Ying Li",
        "Yuan Feng",
        "Bei Wang",
        "Zaijian Zong",
        "Mosong Zhou",
        "Wenli Zhou",
        "Houjiang Chen",
        "Xingyu Liao",
        "Yipeng Li",
        "Wenxiao Zhang",
        "Ping Zhu",
        "Yinggang Wang",
        "Chuanjie Xiao",
        "Depeng Liang",
        "Dong Cao",
        "Juncheng Liu",
        "Yongqiang Yang",
        "Xiaolong Bai",
        "Yi Li",
        "Huaguo Xie",
        "Huatao Wu",
        "Zhibin Yu",
        "Lv Chen",
        "Hu Liu",
        "Yujun Ding",
        "Haipei Zhu",
        "Jing Xia",
        "Yi Xiong",
        "Zhou Yu",
        "Heng Liao"
      ],
      "abstract": "The rapid evolution of large language models (LLMs), driven by growing parameter scales, adoption of mixture-of-experts (MoE) architectures, and expanding context lengths, imposes unprecedented demands on AI infrastructure. Traditional AI clusters face limitations in compute intensity, memory bandwidth, inter-chip communication, and latency, compounded by variable workloads and strict service-level objectives. Addressing these issues requires fundamentally redesigned hardware-software integration. This paper introduces Huawei CloudMatrix, a next-generation AI datacenter architecture, realized in the production-grade CloudMatrix384 supernode. It integrates 384 Ascend 910 NPUs and 192 Kunpeng CPUs interconnected via an ultra-high-bandwidth Unified Bus (UB) network, enabling direct all-to-all communication and dynamic pooling of resources. These features optimize performance for communication-intensive operations, such as large-scale MoE expert parallelism and distributed key-value cache access. To fully leverage CloudMatrix384, we propose CloudMatrix-Infer, an advanced LLM serving solution incorporating three core innovations: a peer-to-peer serving architecture that independently scales prefill, decode, and caching; a large-scale expert parallelism strategy supporting EP320 via efficient UB-based token dispatch; and hardware-aware optimizations including specialized operators, microbatch-based pipelining, and INT8 quantization. Evaluation with the DeepSeek-R1 model shows CloudMatrix-Infer achieves state-of-the-art efficiency: prefill throughput of 6,688 tokens/s per NPU and decode throughput of 1,943 tokens/s per NPU (<50 ms TPOT). It effectively balances throughput and latency, sustaining 538 tokens/s per NPU even under stringent 15 ms latency constraints, while INT8 quantization maintains model accuracy across benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº†åä¸ºæ–°ä¸€ä»£AIæ•°æ®ä¸­å¿ƒæ¶æ„CloudMatrixåŠå…¶åœ¨CloudMatrix384è¶…èŠ‚ç‚¹ä¸Šçš„ç”Ÿäº§å®ç°ï¼Œæ—¨åœ¨åº”å¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)å› å‚æ•°è§„æ¨¡å¢é•¿ã€æ··åˆä¸“å®¶æ¶æ„(MoE)åŠè¶…é•¿ä¸Šä¸‹æ–‡å¸¦æ¥çš„åŸºç¡€è®¾æ–½æŒ‘æˆ˜ã€‚CloudMatrix384é›†æˆäº†384é¢—Ascend 910 NPUå’Œ192é¢—Kunpeng CPUï¼Œé€šè¿‡è¶…é«˜å¸¦å®½çš„Unified Bus (UB)ç½‘ç»œå®ç°å…¨å¯¹å…¨ç›´æ¥é€šä¿¡å’Œèµ„æºåŠ¨æ€æ± åŒ–ã€‚é’ˆå¯¹è¯¥ç¡¬ä»¶æ¶æ„ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†CloudMatrix-Inferæ¨ç†æ–¹æ¡ˆï¼Œé‡‡ç”¨ç‚¹å¯¹ç‚¹(peer-to-peer)æœåŠ¡æ¶æ„ä»¥ç‹¬ç«‹æ‰©å±•Prefillã€Decodeå’Œç¼“å­˜ä»»åŠ¡ï¼Œå¹¶æ”¯æŒé«˜è¾¾EP320çš„å¤§è§„æ¨¡ä¸“å®¶å¹¶è¡Œ(Expert Parallelism)ç­–ç•¥ã€‚é€šè¿‡ç¡¬ä»¶æ„ŸçŸ¥ç®—å­ä¼˜åŒ–ã€å¾®æ‰¹æ¬¡æµæ°´çº¿å’ŒINT8é‡åŒ–ç­‰æ‰‹æ®µï¼Œè¯¥ç³»ç»Ÿåœ¨DeepSeek-R1æ¨¡å‹è¯„ä¼°ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼ŒNPUå•èŠ¯ç‰‡çš„Prefillå’ŒDecodeååé‡åˆ†åˆ«è¾¾åˆ°6,688 tokens/så’Œ1,943 tokens/sã€‚å®éªŒç»“æœè¯æ˜ï¼ŒCloudMatrix384èƒ½æœ‰æ•ˆå¹³è¡¡ååé‡ä¸å»¶è¿Ÿï¼Œå³ä½¿åœ¨15msçš„ä¸¥è‹›å»¶è¿Ÿçº¦æŸä¸‹ä»èƒ½ä¿æŒé«˜æ•ˆè¿è¡Œï¼Œä¸ºå¤§è§„æ¨¡MoEæ¨¡å‹çš„åˆ†å¸ƒå¼æ¨ç†æä¾›äº†é¢†å…ˆçš„ç¡¬ä»¶è½¯ä»¶ååŒä¼˜åŒ–æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.AR",
        "cs.LG"
      ],
      "primary_category": "cs.DC",
      "comment": "59 pages, 24 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.12708v3",
      "published_date": "2025-06-15 03:41:34 UTC",
      "updated_date": "2025-06-19 12:27:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:11:48.342000+00:00"
    },
    {
      "arxiv_id": "2506.12706v1",
      "title": "NAP-Tuning: Neural Augmented Prompt Tuning for Adversarially Robust Vision-Language Models",
      "title_zh": "NAP-Tuningï¼šé¢å‘å¯¹æŠ—é²æ£’è§†è§‰è¯­è¨€æ¨¡å‹çš„ç¥ç»å¢å¼ºæç¤ºå¾®è°ƒ",
      "authors": [
        "Jiaming Zhang",
        "Xin Wang",
        "Xingjun Ma",
        "Lingyu Qiu",
        "Yu-Gang Jiang",
        "Jitao Sang"
      ],
      "abstract": "Vision-Language Models (VLMs) such as CLIP have demonstrated remarkable capabilities in understanding relationships between visual and textual data through joint embedding spaces. Despite their effectiveness, these models remain vulnerable to adversarial attacks, particularly in the image modality, posing significant security concerns. Building upon our previous work on Adversarial Prompt Tuning (AdvPT), which introduced learnable text prompts to enhance adversarial robustness in VLMs without extensive parameter training, we present a significant extension by introducing the Neural Augmentor framework for Multi-modal Adversarial Prompt Tuning (NAP-Tuning).Our key innovations include: (1) extending AdvPT from text-only to multi-modal prompting across both text and visual modalities, (2) expanding from single-layer to multi-layer prompt architectures, and (3) proposing a novel architecture-level redesign through our Neural Augmentor approach, which implements feature purification to directly address the distortions introduced by adversarial attacks in feature space. Our NAP-Tuning approach incorporates token refiners that learn to reconstruct purified features through residual connections, allowing for modality-specific and layer-specific feature correction.Comprehensive experiments demonstrate that NAP-Tuning significantly outperforms existing methods across various datasets and attack types. Notably, our approach shows significant improvements over the strongest baselines under the challenging AutoAttack benchmark, outperforming them by 33.5% on ViT-B16 and 33.0% on ViT-B32 architectures while maintaining competitive clean accuracy.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨é¢å¯¹å¯¹æŠ—æ”»å‡»(Adversarial attacks)æ—¶çš„è„†å¼±æ€§ï¼Œæå‡ºäº†Neural Augmented Prompt Tuning (NAP-Tuning)æ¡†æ¶ã€‚è¯¥æ–¹æ³•æ˜¯å¯¹å‰æœŸå·¥ä½œAdversarial Prompt Tuning (AdvPT)çš„é‡å¤§æ‰©å±•ï¼Œå°†ä»…æ–‡æœ¬çš„æç¤ºå­¦ä¹ æå‡ä¸ºè·¨æ–‡æœ¬å’Œè§†è§‰æ¨¡æ€çš„å¤šæ¨¡æ€æç¤º(Multi-modal prompting)ã€‚NAP-Tuningä¸ä»…å°†å•å±‚æç¤ºæ¶æ„æ‰©å±•ä¸ºå¤šå±‚æ¶æ„ï¼Œè¿˜é€šè¿‡Neural Augmentoræ¡†æ¶å®ç°äº†æ¶æ„çº§çš„é‡æ–°è®¾è®¡ã€‚è¯¥æ¡†æ¶åˆ©ç”¨Token refinerså’Œæ®‹å·®è¿æ¥(Residual connections)è¿›è¡Œç‰¹å¾çº¯åŒ–(Feature purification)ï¼Œä»¥é’ˆå¯¹æ€§åœ°çº æ­£å¯¹æŠ—æ”»å‡»åœ¨ç‰¹å¾ç©ºé—´ä¸­å¼•å…¥çš„ç•¸å˜ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒNAP-Tuningåœ¨å¤šç§æ•°æ®é›†å’Œæ”»å‡»ç±»å‹ä¸‹å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ç‰¹åˆ«æ˜¯åœ¨æŒ‘æˆ˜æ€§çš„AutoAttackåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨ViT-B16å’ŒViT-B32æ¶æ„ä¸Šçš„ç¨³å¥å‡†ç¡®ç‡åˆ†åˆ«æå‡äº†33.5%å’Œ33.0%ï¼ŒåŒæ—¶ä¿æŒäº†æå…·ç«äº‰åŠ›çš„å¹²å‡€æ•°æ®å‡†ç¡®ç‡(Clean accuracy)ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.12706v1",
      "published_date": "2025-06-15 03:34:23 UTC",
      "updated_date": "2025-06-15 03:34:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:11:43.085085+00:00"
    },
    {
      "arxiv_id": "2506.12704v2",
      "title": "Flexible Realignment of Language Models",
      "title_zh": "è¯­è¨€æ¨¡å‹çš„çµæ´»é‡æ–°å¯¹é½",
      "authors": [
        "Wenhong Zhu",
        "Ruobing Xie",
        "Weinan Zhang",
        "Rui Wang"
      ],
      "abstract": "Realignment becomes necessary when a language model (LM) fails to meet expected performance. We propose a flexible realignment framework that supports quantitative control of alignment degree during training and inference. This framework incorporates Training-time Realignment (TrRa), which efficiently realigns the reference model by leveraging the controllable fusion of logits from both the reference and already aligned models. For example, TrRa reduces token usage by 54.63% on DeepSeek-R1-Distill-Qwen-1.5B without any performance degradation, outperforming DeepScaleR-1.5B's 33.86%. To complement TrRa during inference, we introduce a layer adapter that enables smooth Inference-time Realignment (InRa). This adapter is initialized to perform an identity transformation at the bottom layer and is inserted preceding the original layers. During inference, input embeddings are simultaneously processed by the adapter and the original layer, followed by the remaining layers, and then controllably interpolated at the logit level. We upgraded DeepSeek-R1-Distill-Qwen-7B from a slow-thinking model to one that supports both fast and slow thinking, allowing flexible alignment control even during inference. By encouraging deeper reasoning, it even surpassed its original performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªçµæ´»çš„é‡æ–°å¯¹é½ (Flexible Realignment) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¯­è¨€æ¨¡å‹ (LM) æ€§èƒ½æœªè¾¾é¢„æœŸæ—¶éœ€è¦é‡æ–°è°ƒæ•´çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†è®­ç»ƒæ—¶é‡æ–°å¯¹é½ (Training-time Realignment, TrRa)ï¼Œé€šè¿‡å¯¹å‚è€ƒæ¨¡å‹å’Œå·²å¯¹é½æ¨¡å‹çš„ Logits è¿›è¡Œå¯æ§èåˆï¼Œæ˜¾è‘—æå‡äº†å¯¹é½æ•ˆç‡ã€‚å®éªŒè¡¨æ˜ï¼ŒTrRa åœ¨ DeepSeek-R1-Distill-Qwen-1.5B ä¸Šå‡å°‘äº† 54.63% çš„ Token ä½¿ç”¨é‡ä¸”æ— æ€§èƒ½æŸå¤±ï¼Œè¡¨ç°ä¼˜äº DeepScaleRã€‚ä¸ºäº†å®ç°æ¨ç†é˜¶æ®µçš„çµæ´»æ§åˆ¶ï¼Œç ”ç©¶è¿›ä¸€æ­¥æå‡ºäº†æ¨ç†æ—¶é‡æ–°å¯¹é½ (Inference-time Realignment, InRa) åŠå…¶æ ¸å¿ƒç»„ä»¶å±‚é€‚é…å™¨ (Layer Adapter)ã€‚è¯¥é€‚é…å™¨é€šè¿‡è¾“å…¥åµŒå…¥çš„åŒæ­¥å¤„ç†ä¸ Logit å±‚é¢çš„å¯æ§æ’å€¼ï¼Œæ”¯æŒå¯¹æ¨¡å‹å¯¹é½ç¨‹åº¦çš„å®šé‡è°ƒèŠ‚ã€‚åˆ©ç”¨è¯¥æ¡†æ¶ï¼Œç ”ç©¶æˆåŠŸå°† DeepSeek-R1-Distill-Qwen-7B å‡çº§ä¸ºå…¼å…·å¿«æ…¢æ€è€ƒèƒ½åŠ›çš„æ¨¡å‹ï¼Œå¹¶é€šè¿‡å¼•å¯¼æ·±åº¦æ¨ç†ä½¿å…¶æœ€ç»ˆè¶…è¶Šäº†åŸå§‹æ€§èƒ½æ°´å¹³ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.12704v2",
      "published_date": "2025-06-15 03:26:59 UTC",
      "updated_date": "2026-01-11 11:28:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:11:44.096111+00:00"
    },
    {
      "arxiv_id": "2506.12698v1",
      "title": "Unsupervised Contrastive Learning Using Out-Of-Distribution Data for Long-Tailed Dataset",
      "title_zh": "é¢å‘é•¿å°¾æ•°æ®é›†çš„åˆ©ç”¨åˆ†å¸ƒå¤–æ•°æ®æ— ç›‘ç£å¯¹æ¯”å­¦ä¹ ",
      "authors": [
        "Cuong Manh Hoang",
        "Yeejin Lee",
        "Byeongkeun Kang"
      ],
      "abstract": "This work addresses the task of self-supervised learning (SSL) on a long-tailed dataset that aims to learn balanced and well-separated representations for downstream tasks such as image classification. This task is crucial because the real world contains numerous object categories, and their distributions are inherently imbalanced. Towards robust SSL on a class-imbalanced dataset, we investigate leveraging a network trained using unlabeled out-of-distribution (OOD) data that are prevalently available online. We first train a network using both in-domain (ID) and sampled OOD data by back-propagating the proposed pseudo semantic discrimination loss alongside a domain discrimination loss. The OOD data sampling and loss functions are designed to learn a balanced and well-separated embedding space. Subsequently, we further optimize the network on ID data by unsupervised contrastive learning while using the previously trained network as a guiding network. The guiding network is utilized to select positive/negative samples and to control the strengths of attractive/repulsive forces in contrastive learning. We also distil and transfer its embedding space to the training network to maintain balancedness and separability. Through experiments on four publicly available long-tailed datasets, we demonstrate that the proposed method outperforms previous state-of-the-art methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é•¿å°¾åˆ†å¸ƒæ•°æ®é›†(Long-tailed dataset)ä¸‹çš„è‡ªç›‘ç£å­¦ä¹ (Self-supervised learning, SSL)ä»»åŠ¡ï¼Œæ—¨åœ¨ä¸ºä¸‹æ¸¸å›¾åƒåˆ†ç±»ä»»åŠ¡å­¦ä¹ å¹³è¡¡ä¸”å¯åˆ†æ€§å¼ºçš„è¡¨å¾ã€‚ä½œè€…æå‡ºåˆ©ç”¨å¹¿æ³›å¯å¾—çš„æ— æ ‡ç­¾åˆ†å¸ƒå¤–æ•°æ®(Out-of-distribution, OOD)æ¥å¢å¼ºæ¨¡å‹åœ¨ç±»åˆ«ä¸å¹³è¡¡æ•°æ®é›†ä¸Šçš„é²æ£’æ€§ã€‚è¯¥æ–¹æ³•é¦–å…ˆé€šè¿‡ç»“åˆä¼ªè¯­ä¹‰åˆ¤åˆ«æŸå¤±(Pseudo semantic discrimination loss)å’Œé¢†åŸŸåˆ¤åˆ«æŸå¤±(Domain discrimination loss)ï¼Œåˆ©ç”¨é¢†åŸŸå†…(ID)å’Œé‡‡æ ·åçš„OODæ•°æ®å…±åŒè®­ç»ƒå‡ºä¸€ä¸ªå¹³è¡¡çš„åµŒå…¥ç©ºé—´ã€‚éšåï¼Œåœ¨æ— ç›‘ç£å¯¹æ¯”å­¦ä¹ (Unsupervised contrastive learning)é˜¶æ®µï¼Œå°†å‰æœŸè®­ç»ƒçš„ç½‘ç»œä½œä¸ºå¼•å¯¼ç½‘ç»œ(Guiding network)ï¼Œç”¨äºè¾…åŠ©æ­£è´Ÿæ ·æœ¬çš„é€‰æ‹©å¹¶åŠ¨æ€è°ƒèŠ‚å¯¹æ¯”å­¦ä¹ ä¸­çš„å¸å¼•ä¸æ’æ–¥åŠ›å¼ºåº¦ã€‚æ­¤å¤–ï¼Œé€šè¿‡å°†å¼•å¯¼ç½‘ç»œçš„åµŒå…¥ç©ºé—´çŸ¥è¯†è’¸é¦(Distill)å¹¶è¿ç§»è‡³ç›®æ ‡ç½‘ç»œï¼Œæœ‰æ•ˆç»´æŒäº†è¡¨å¾çš„å¹³è¡¡æ€§ä¸åˆ†ç¦»åº¦ã€‚åœ¨å››ä¸ªå…¬å¼€é•¿å°¾æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰çš„å…ˆè¿›ç®—æ³•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "13 pages",
      "pdf_url": "https://arxiv.org/pdf/2506.12698v1",
      "published_date": "2025-06-15 03:12:38 UTC",
      "updated_date": "2025-06-15 03:12:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:11:50.648487+00:00"
    },
    {
      "arxiv_id": "2506.12697v2",
      "title": "MGDFIS: Multi-scale Global-detail Feature Integration Strategy for Small Object Detection",
      "title_zh": "MGDFISï¼šé¢å‘å°ç›®æ ‡æ£€æµ‹çš„å¤šå°ºåº¦å…¨å±€-ç»†èŠ‚ç‰¹å¾èåˆç­–ç•¥",
      "authors": [
        "Yuxiang Wang",
        "Xuecheng Bai",
        "Boyu Hu",
        "Chuanzhi Xu",
        "Haodong Chen",
        "Vera Chung",
        "Tingxue Li",
        "Xiaoming Chen"
      ],
      "abstract": "Small object detection in UAV imagery is crucial for applications such as search-and-rescue, traffic monitoring, and environmental surveillance, but it is hampered by tiny object size, low signal-to-noise ratios, and limited feature extraction. Existing multi-scale fusion methods help, but add computational burden and blur fine details, making small object detection in cluttered scenes difficult. To overcome these challenges, we propose the Multi-scale Global-detail Feature Integration Strategy (MGDFIS), a unified fusion framework that tightly couples global context with local detail to boost detection performance while maintaining efficiency. MGDFIS comprises three synergistic modules: the FusionLock-TSS Attention Module, which marries token-statistics self-attention with DynamicTanh normalization to highlight spectral and spatial cues at minimal cost; the Global-detail Integration Module, which fuses multi-scale context via directional convolution and parallel attention while preserving subtle shape and texture variations; and the Dynamic Pixel Attention Module, which generates pixel-wise weighting maps to rebalance uneven foreground and background distributions and sharpen responses to true object regions. Extensive experiments on the VisDrone benchmark demonstrate that MGDFIS consistently outperforms state-of-the-art methods across diverse backbone architectures and detection frameworks, achieving superior precision and recall with low inference time. By striking an optimal balance between accuracy and resource usage, MGDFIS provides a practical solution for small-object detection on resource-constrained UAV platforms.",
      "tldr_zh": "æ— äººæœº(UAV)å›¾åƒä¸­çš„å°ç›®æ ‡æ£€æµ‹(Small Object Detection)é¢ä¸´ç›®æ ‡å°ºå¯¸æå°ã€ä¿¡å™ªæ¯”ä½ä»¥åŠç‰¹å¾æå–å—é™ç­‰æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•å¸¸å› è®¡ç®—è´Ÿæ‹…é‡æˆ–ç»†èŠ‚æ¨¡ç³Šè€Œéš¾ä»¥åº”å¯¹ã€‚æœ¬ç ”ç©¶æå‡ºäº†å¤šå°ºåº¦å…¨å±€-ç»†èŠ‚ç‰¹å¾é›†æˆç­–ç•¥(Multi-scale Global-detail Feature Integration Strategy, MGDFIS)ï¼Œé€šè¿‡FusionLock-TSS Attention Moduleã€Global-detail Integration Moduleå’ŒDynamic Pixel Attention Moduleä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—ï¼Œå®ç°äº†å…¨å±€ä¸Šä¸‹æ–‡ä¸å±€éƒ¨ç»†èŠ‚çš„ç´§å¯†è€¦åˆã€‚è¿™äº›æ¨¡å—åˆ†åˆ«é€šè¿‡Token-statistics self-attentionçªå‡ºå…‰è°±ç©ºé—´çº¿ç´¢ã€åˆ©ç”¨å®šå‘å·ç§¯ä¿ç•™çº¹ç†å˜åŒ–ï¼Œå¹¶ç”Ÿæˆåƒç´ çº§æƒé‡å›¾ä»¥å¹³è¡¡å‰æ™¯ä¸èƒŒæ™¯åˆ†å¸ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMGDFISåœ¨VisDroneåŸºå‡†æµ‹è¯•ä¸­ä¸€è‡´ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•(SOTA)ï¼Œä¸”åœ¨æ¨ç†é€Ÿåº¦ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚è¯¥ç­–ç•¥åœ¨æ£€æµ‹ç²¾åº¦ä¸è®¡ç®—èµ„æºæ¶ˆè€—ä¹‹é—´å–å¾—äº†ç†æƒ³å¹³è¡¡ï¼Œä¸ºèµ„æºå—é™çš„æ— äººæœºå¹³å°æä¾›äº†å®ç”¨çš„å°ç›®æ ‡æ£€æµ‹è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "9 pages, 5 figures, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.12697v2",
      "published_date": "2025-06-15 02:54:25 UTC",
      "updated_date": "2025-08-13 15:13:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:11:50.806485+00:00"
    },
    {
      "arxiv_id": "2506.12691v1",
      "title": "Get on the Train or be Left on the Station: Using LLMs for Software Engineering Research",
      "title_zh": "ç™»è½¦å‰è¡Œï¼Œè¿˜æ˜¯æ»ç•™ç«™å°ï¼šå¤§è¯­è¨€æ¨¡å‹åœ¨è½¯ä»¶å·¥ç¨‹ç ”ç©¶ä¸­çš„åº”ç”¨",
      "authors": [
        "Bianca Trinkenreich",
        "Fabio Calefato",
        "Geir Hanssen",
        "Kelly Blincoe",
        "Marcos Kalinowski",
        "Mauro PezzÃ¨",
        "Paolo Tell",
        "Margaret-Anne Storey"
      ],
      "abstract": "The adoption of Large Language Models (LLMs) is not only transforming software engineering (SE) practice but is also poised to fundamentally disrupt how research is conducted in the field. While perspectives on this transformation range from viewing LLMs as mere productivity tools to considering them revolutionary forces, we argue that the SE research community must proactively engage with and shape the integration of LLMs into research practices, emphasizing human agency in this transformation. As LLMs rapidly become integral to SE research - both as tools that support investigations and as subjects of study - a human-centric perspective is essential. Ensuring human oversight and interpretability is necessary for upholding scientific rigor, fostering ethical responsibility, and driving advancements in the field. Drawing from discussions at the 2nd Copenhagen Symposium on Human-Centered AI in SE, this position paper employs McLuhan's Tetrad of Media Laws to analyze the impact of LLMs on SE research. Through this theoretical lens, we examine how LLMs enhance research capabilities through accelerated ideation and automated processes, make some traditional research practices obsolete, retrieve valuable aspects of historical research approaches, and risk reversal effects when taken to extremes. Our analysis reveals opportunities for innovation and potential pitfalls that require careful consideration. We conclude with a call to action for the SE research community to proactively harness the benefits of LLMs while developing frameworks and guidelines to mitigate their risks, to ensure continued rigor and impact of research in an AI-augmented future.",
      "tldr_zh": "è¿™ç¯‡ç«‹åœºè®ºæ–‡æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)å¯¹è½¯ä»¶å·¥ç¨‹(SE)ç ”ç©¶é¢†åŸŸçš„å˜é©æ€§å½±å“ï¼Œå¼ºè°ƒç ”ç©¶ç¤¾åŒºå¿…é¡»ä¸»åŠ¨å¼•å¯¼LLMsçš„æ•´åˆå¹¶åšæŒä»¥äººä¸ºæœ¬(human-centric)çš„è§†è§’ã€‚ç ”ç©¶é‡‡ç”¨äº†éº¦å…‹å¢æ±‰çš„åª’ä»‹å®šå¾‹å››é‡å¥(McLuhan's Tetrad of Media Laws)ç†è®ºæ¡†æ¶ï¼Œæ·±å…¥åˆ†æäº†LLMsåœ¨å¢å¼ºç ”ç©¶èƒ½åŠ›ã€å¯¼è‡´ä¼ ç»Ÿå®è·µè¿‡æ—¶ä»¥åŠé‡æ–°å®¡è§†å†å²æ–¹æ³•æ–¹é¢çš„ä½œç”¨ã€‚é€šè¿‡è¿™ä¸€ç†è®ºé•œå¤´ï¼Œä½œè€…æ­ç¤ºäº†LLMsåœ¨åŠ é€Ÿæ„æ€ä¸è‡ªåŠ¨åŒ–æµç¨‹ä¸­å¸¦æ¥çš„æœºé‡ï¼ŒåŒæ—¶ä¹Ÿè­¦å‘Šäº†è¿‡åº¦ä¾èµ–å¯èƒ½å¼•å‘çš„é€†è½¬æ•ˆåº”(reversal effects)åŠæ½œåœ¨é™·é˜±ã€‚æ–‡ç« æŒ‡å‡ºï¼Œä¸ºäº†ç»´æŒç§‘å­¦ä¸¥è°¨æ€§å’Œä¼¦ç†è´£ä»»ï¼Œç¡®ä¿äººç±»å¯¹AIå·¥å…·çš„ç›‘ç£å’Œç ”ç©¶è¿‡ç¨‹çš„å¯è§£é‡Šæ€§è‡³å…³é‡è¦ã€‚æœ€åï¼Œè®ºæ–‡å‘¼åSEç ”ç©¶ç•Œå°½å¿«åˆ¶å®šç›¸å…³æ¡†æ¶ä¸å‡†åˆ™ï¼Œä»¥ç¡®ä¿åœ¨äººå·¥æ™ºèƒ½å¢å¼ºçš„æœªæ¥ä¸­ç ”ç©¶å·¥ä½œä»èƒ½ä¿æŒå…¶åº”æœ‰çš„æ·±åº¦ä¸å½±å“åŠ›ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted for publication at the 1st Workshop on Human-Centered AI for SE (Human AISE) held at the 33rd ACM International Conference on the Foundations of Software Engineering (FSE Companion '25), June 23-28, 2025, Trondheim, Norway",
      "pdf_url": "https://arxiv.org/pdf/2506.12691v1",
      "published_date": "2025-06-15 02:25:50 UTC",
      "updated_date": "2025-06-15 02:25:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:12:09.544838+00:00"
    },
    {
      "arxiv_id": "2506.12689v2",
      "title": "SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation",
      "title_zh": "SciSageï¼šä¸€ç§é«˜è´¨é‡ç§‘å­¦ç»¼è¿°ç”Ÿæˆçš„å¤šæ™ºèƒ½ä½“æ¡†æ¶",
      "authors": [
        "Xiaofeng Shi",
        "Qian Kou",
        "Yuduo Li",
        "Ning Tang",
        "Jinxin Xie",
        "Longbin Yu",
        "Songjing Wang",
        "Hua Zhou"
      ],
      "abstract": "The rapid growth of scientific literature demands robust tools for automated survey-generation. However, current large language model (LLM)-based methods often lack in-depth analysis, structural coherence, and reliable citations. To address these limitations, we introduce SciSage, a multi-agent framework employing a reflect-when-you-write paradigm. SciSage features a hierarchical Reflector agent that critically evaluates drafts at outline, section, and document levels, collaborating with specialized agents for query interpretation, content retrieval, and refinement. We also release SurveyScope, a rigorously curated benchmark of 46 high-impact papers (2020-2025) across 11 computer science domains, with strict recency and citation-based quality controls. Evaluations demonstrate that SciSage outperforms state-of-the-art baselines (LLM x MapReduce-V2, AutoSurvey), achieving +1.73 points in document coherence and +32% in citation F1 scores. Human evaluations reveal mixed outcomes (3 wins vs. 7 losses against human-written surveys), but highlight SciSage's strengths in topical breadth and retrieval efficiency. Overall, SciSage offers a promising foundation for research-assistive writing tools.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SciSageï¼Œä¸€ä¸ªä¸“ä¸ºé«˜è´¨é‡ç§‘å­¦ç»¼è¿°ç”Ÿæˆè®¾è®¡çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ (LLM) åœ¨ç»¼è¿°ç”Ÿæˆä¸­æ™®éå­˜åœ¨çš„æ·±åº¦ä¸è¶³ã€ç»“æ„ä¸è¿è´¯åŠå¼•ç”¨ä¸å¯é ç­‰é—®é¢˜ã€‚SciSage é‡‡ç”¨äº†â€œè¾¹å†™è¾¹åæ€â€(reflect-when-you-write) çš„èŒƒå¼ï¼Œé€šè¿‡ä¸€ä¸ªå±‚æ¬¡åŒ–çš„ Reflector æ™ºèƒ½ä½“åœ¨æçº²ã€ç« èŠ‚å’Œæ–‡æ¡£å±‚é¢è¿›è¡Œè¯„ä¼°ï¼Œå¹¶ååŒä¸“é—¨çš„æ™ºèƒ½ä½“å®ŒæˆæŸ¥è¯¢è§£è¯»ã€å†…å®¹æ£€ç´¢ä¸å†…å®¹ç²¾ç‚¼ã€‚ç ”ç©¶å›¢é˜ŸåŒæ­¥æ¨å‡ºäº† SurveyScope åŸºå‡†æµ‹è¯•é›†ï¼ŒåŒ…å«æ¶µç›– 11 ä¸ªè®¡ç®—æœºç§‘å­¦é¢†åŸŸçš„ 46 ç¯‡é«˜å½±å“åŠ›è®ºæ–‡ï¼Œä»¥ç¡®ä¿è¯„ä¼°çš„ä¸¥è°¨æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSciSage åœ¨æ–‡æ¡£è¿è´¯æ€§ä¸Šæå‡äº† 1.73 åˆ†ï¼Œå¼•ç”¨ F1 scores æé«˜äº† 32%ï¼Œæ˜¾è‘—ä¼˜äº LLM x MapReduce-V2 å’Œ AutoSurvey ç­‰åŸºå‡†æ¨¡å‹ã€‚å°½ç®¡äººç±»è¯„ä¼°æ˜¾ç¤ºå…¶ä¸äººç±»æ’°å†™çš„ç»¼è¿°ç›¸æ¯”ä»æœ‰å·®è·ï¼Œä½† SciSage åœ¨ä¸»é¢˜å¹¿åº¦å’Œæ£€ç´¢æ•ˆç‡ä¸Šè¡¨ç°å“è¶Šï¼Œä¸ºç ”ç©¶è¾…åŠ©å†™ä½œå·¥å…·æä¾›äº†æå…·å‰æ™¯çš„åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.12689v2",
      "published_date": "2025-06-15 02:23:47 UTC",
      "updated_date": "2025-07-21 03:49:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:12:05.694090+00:00"
    },
    {
      "arxiv_id": "2506.12685v1",
      "title": "Alphabet Index Mapping: Jailbreaking LLMs through Semantic Dissimilarity",
      "title_zh": "å­—æ¯ç´¢å¼•æ˜ å°„ï¼šåŸºäºè¯­ä¹‰å·®å¼‚æ€§çš„å¤§è¯­è¨€æ¨¡å‹è¶Šç‹±",
      "authors": [
        "Bilal Saleh Husain"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities, yet their susceptibility to adversarial attacks, particularly jailbreaking, poses significant safety and ethical concerns. While numerous jailbreak methods exist, many suffer from computational expense, high token usage, or complex decoding schemes. Liu et al. (2024) introduced FlipAttack, a black-box method that achieves high attack success rates (ASR) through simple prompt manipulation. This paper investigates the underlying mechanisms of FlipAttack's effectiveness by analyzing the semantic changes induced by its flipping modes. We hypothesize that semantic dissimilarity between original and manipulated prompts is inversely correlated with ASR. To test this, we examine embedding space visualizations (UMAP, KDE) and cosine similarities for FlipAttack's modes. Furthermore, we introduce a novel adversarial attack, Alphabet Index Mapping (AIM), designed to maximize semantic dissimilarity while maintaining simple decodability. Experiments on GPT-4 using a subset of AdvBench show AIM and its variant AIM+FWO achieve a 94% ASR, outperforming FlipAttack and other methods on this subset. Our findings suggest that while high semantic dissimilarity is crucial, a balance with decoding simplicity is key for successful jailbreaking. This work contributes to a deeper understanding of adversarial prompt mechanics and offers a new, effective jailbreak technique.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(Large Language Models)é¢ä¸´çš„è¶Šç‹±æ”»å‡»(jailbreaking)é£é™©ï¼Œé‡ç‚¹åˆ†æäº†é»‘ç›’æ–¹æ³•FlipAttackçš„æœ‰æ•ˆæ€§æœºåˆ¶ã€‚ç ”ç©¶è€…æå‡ºå¹¶éªŒè¯äº†ä¸€ä¸ªå‡è®¾ï¼Œå³åŸå§‹æç¤ºè¯ä¸æ“çºµåæç¤ºè¯ä¹‹é—´çš„è¯­ä¹‰ç›¸å¼‚æ€§(semantic dissimilarity)æ˜¯å½±å“æ”»å‡»æˆåŠŸç‡(Attack Success Rate, ASR)çš„å…³é”®å› ç´ ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œè®ºæ–‡å¼•å…¥äº†ä¸€ç§åä¸ºAlphabet Index Mapping (AIM)çš„æ–°å‹æ”»å‡»æ–¹æ³•ï¼Œé€šè¿‡æœ€å¤§åŒ–è¯­ä¹‰ç›¸å¼‚æ€§å¹¶ç»“åˆç®€å•çš„è§£ç æ–¹æ¡ˆæ¥è¯±å¯¼æ¨¡å‹è¿è§„ã€‚åœ¨GPT-4å’ŒAdvBenchå­é›†çš„å®éªŒä¸­ï¼ŒAIMåŠå…¶å˜ä½“AIM+FWOå–å¾—äº†94%çš„ASRï¼Œæ˜¾è‘—ä¼˜äºFlipAttackç­‰å¯¹æ¯”æ–¹æ³•ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨å¯¹æŠ—æ€§æç¤ºä¸­å®ç°é«˜è¯­ä¹‰ç›¸å¼‚æ€§ä¸è§£ç ç®€æ˜“æ€§ä¹‹é—´çš„å¹³è¡¡æ˜¯æå‡è¶Šç‹±æˆåŠŸç‡çš„æ ¸å¿ƒã€‚è¯¥å·¥ä½œä¸ä»…æä¾›äº†ä¸€ç§é«˜æ•ˆçš„è¶Šç‹±æŠ€æœ¯ï¼Œä¹Ÿä¸ºç†è§£å¯¹æŠ—æ€§æç¤ºè¯çš„åº•å±‚æœºåˆ¶æä¾›äº†æ–°è§†è§’ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "10 pages, 2 figures, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.12685v1",
      "published_date": "2025-06-15 01:59:08 UTC",
      "updated_date": "2025-06-15 01:59:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:12:10.490928+00:00"
    },
    {
      "arxiv_id": "2506.15732v3",
      "title": "Can LLMs Reconcile Knowledge Conflicts in Counterfactual Reasoning",
      "title_zh": "LLMsèƒ½å¦è°ƒå’Œåäº‹å®æ¨ç†ä¸­çš„çŸ¥è¯†å†²çªï¼Ÿ",
      "authors": [
        "Khurram Yamin",
        "Gaurav Ghosal",
        "Bryan Wilder"
      ],
      "abstract": "Large Language Models have been shown to contain extensive world knowledge in their parameters, enabling impressive performance on many knowledge intensive tasks. However, when deployed in novel settings, LLMs often encounter situations where they must integrate parametric knowledge with new or unfamiliar information. In this work, we explore whether LLMs can combine knowledge in-context with their parametric knowledge through the lens of counterfactual reasoning. Through synthetic and real experiments in multi-hop reasoning problems, we show that LLMs generally struggle with counterfactual reasoning, often resorting to exclusively using their parametric knowledge. Moreover, we show that simple post-hoc finetuning can struggle to instill counterfactual reasoning ability -- often leading to degradation in stored parametric knowledge. Ultimately, our work reveals important limitations of current LLM's abilities to re-purpose parametric knowledge in novel settings.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨åäº‹å®æ¨ç†(Counterfactual Reasoning)åœºæ™¯ä¸‹ï¼Œå¦‚ä½•åè°ƒå…¶å‚æ•°åŒ–çŸ¥è¯†(Parametric Knowledge)ä¸ä¸Šä¸‹æ–‡çŸ¥è¯†(In-context Knowledge)ä¹‹é—´çš„å†²çªã€‚é€šè¿‡åœ¨å¤šæ­¥æ¨ç†(Multi-hop Reasoning)é—®é¢˜ä¸Šçš„åˆæˆä¸çœŸå®å®éªŒå‘ç°ï¼ŒLLMs åœ¨é¢å¯¹åäº‹å®æƒ…å¢ƒæ—¶é€šå¸¸è¡¨ç°æ¬ ä½³ï¼Œå¾€å¾€å€¾å‘äºæ’ä»–æ€§åœ°ä½¿ç”¨å…¶å†…éƒ¨çš„å‚æ•°åŒ–çŸ¥è¯†ã€‚å®éªŒè¿›ä¸€æ­¥è¯æ˜ï¼Œç®€å•çš„åæœŸå¾®è°ƒ(Post-hoc Finetuning)éš¾ä»¥æœ‰æ•ˆæ³¨å…¥åäº‹å®æ¨ç†èƒ½åŠ›ï¼Œä¸”å¸¸ä¼šå¯¼è‡´æ¨¡å‹åŸæœ‰å‚æ•°åŒ–çŸ¥è¯†çš„é€€åŒ–ã€‚æœ€ç»ˆï¼Œè¿™é¡¹å·¥ä½œæ­ç¤ºäº†å½“å‰ LLMs åœ¨æ–°é¢–è®¾ç½®ä¸‹é‡æ–°åˆ©ç”¨å’Œè°ƒæ•´å…¶å‚æ•°åŒ–çŸ¥è¯†æ–¹é¢å­˜åœ¨çš„å…³é”®å±€é™æ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "ICML 2025 Workshop on Scaling up Intervention Models",
      "pdf_url": "https://arxiv.org/pdf/2506.15732v3",
      "published_date": "2025-06-15 01:08:05 UTC",
      "updated_date": "2025-10-21 13:15:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:12:09.362932+00:00"
    },
    {
      "arxiv_id": "2506.12667v2",
      "title": "Building Trustworthy AI by Addressing its 16+2 Desiderata with Goal-Directed Commonsense Reasoning",
      "title_zh": "åˆ©ç”¨ç›®æ ‡å¯¼å‘çš„å¸¸è¯†æ¨ç†æ»¡è¶³ 16+2 é¡¹è¦ç´ ï¼Œæ„å»ºå¯ä¿¡äººå·¥æ™ºèƒ½",
      "authors": [
        "Alexis R. Tudor",
        "Yankai Zeng",
        "Huaduo Wang",
        "Joaquin Arias",
        "Gopal Gupta"
      ],
      "abstract": "Current advances in AI and its applicability have highlighted the need to ensure its trustworthiness for legal, ethical, and even commercial reasons. Sub-symbolic machine learning algorithms, such as the LLMs, simulate reasoning but hallucinate and their decisions cannot be explained or audited (crucial aspects for trustworthiness). On the other hand, rule-based reasoners, such as Cyc, are able to provide the chain of reasoning steps but are complex and use a large number of reasoners. We propose a middle ground using s(CASP), a goal-directed constraint-based answer set programming reasoner that employs a small number of mechanisms to emulate reliable and explainable human-style commonsense reasoning. In this paper, we explain how s(CASP) supports the 16 desiderata for trustworthy AI introduced by Doug Lenat and Gary Marcus (2023), and two additional ones: inconsistency detection and the assumption of alternative worlds. To illustrate the feasibility and synergies of s(CASP), we present a range of diverse applications, including a conversational chatbot and a virtually embodied reasoner.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å½“å‰å¤§è¯­è¨€æ¨¡å‹(LLMs)ç­‰å­ç¬¦å·æœºå™¨å­¦ä¹ ç®—æ³•åœ¨å¯ä¿¡åº¦æ–¹é¢å­˜åœ¨çš„å¹»è§‰ã€æ— æ³•å®¡è®¡åŠä¸å¯è§£é‡Šç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºs(CASP)çš„æŠ˜ä¸­æ–¹æ¡ˆã€‚s(CASP)æ˜¯ä¸€ç§åŸºäºç›®æ ‡å¯¼å‘å’Œçº¦æŸçš„ç­”æ¡ˆé›†ç¼–ç¨‹(Answer Set Programming)æ¨ç†æœºï¼Œæ—¨åœ¨åˆ©ç”¨å°‘é‡çš„æœºåˆ¶æ¥æ¨¡æ‹Ÿå¯é ä¸”å¯è§£é‡Šçš„äººç±»å¼å¸¸è¯†æ¨ç†(Commonsense Reasoning)ã€‚è¯¥æ¡†æ¶ä¸ä»…æ»¡è¶³äº†Doug Lenatå’ŒGary Marcusæå‡ºçš„16é¡¹å¯ä¿¡AIéœ€æ±‚(Desiderata)ï¼Œè¿˜é¢å¤–æ”¯æŒä¸ä¸€è‡´æ£€æµ‹(Inconsistency Detection)å’Œå¤šé‡ä¸–ç•Œå‡è®¾(Alternative Worlds)ä¸¤é¡¹å…³é”®ç‰¹æ€§ã€‚é€šè¿‡åœ¨å¯¹è¯èŠå¤©æœºå™¨äººå’Œè™šæ‹Ÿå®ä½“æ¨ç†æœºç­‰å¤šç§åœºæ™¯ä¸‹çš„åº”ç”¨ï¼Œè¯¥ç ”ç©¶è¯æ˜äº†s(CASP)åœ¨æ„å»ºæ³•å¾‹ã€ä¼¦ç†åŠå•†ä¸šå±‚é¢å¯ä¿¡AIç³»ç»Ÿä¸­çš„å¯è¡Œæ€§ä¸ååŒæ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.12667v2",
      "published_date": "2025-06-15 00:09:12 UTC",
      "updated_date": "2025-10-30 21:01:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:12:16.484819+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 65,
  "processed_papers_count": 65,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-23T22:13:06.447103+00:00"
}
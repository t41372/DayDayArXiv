[
  {
    "arxiv_id": "2404.17732v1",
    "title": "Generative Dataset Distillation: Balancing Global Structure and Local Details",
    "authors": [
      "Longzhen Li",
      "Guang Li",
      "Ren Togo",
      "Keisuke Maeda",
      "Takahiro Ogawa",
      "Miki Haseyama"
    ],
    "abstract": "In this paper, we propose a new dataset distillation method that considers\nbalancing global structure and local details when distilling the information\nfrom a large dataset into a generative model. Dataset distillation has been\nproposed to reduce the size of the required dataset when training models. The\nconventional dataset distillation methods face the problem of long redeployment\ntime and poor cross-architecture performance. Moreover, previous methods\nfocused too much on the high-level semantic attributes between the synthetic\ndataset and the original dataset while ignoring the local features such as\ntexture and shape. Based on the above understanding, we propose a new method\nfor distilling the original image dataset into a generative model. Our method\ninvolves using a conditional generative adversarial network to generate the\ndistilled dataset. Subsequently, we ensure balancing global structure and local\ndetails in the distillation process, continuously optimizing the generator for\nmore information-dense dataset generation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by the 1st CVPR Workshop on Dataset Distillation",
    "pdf_url": "http://arxiv.org/pdf/2404.17732v1",
    "published_date": "2024-04-26 23:46:10 UTC",
    "updated_date": "2024-04-26 23:46:10 UTC"
  },
  {
    "arxiv_id": "2404.17725v1",
    "title": "Boltzmann State-Dependent Rationality",
    "authors": [
      "Osher Lerner"
    ],
    "abstract": "This paper expands on existing learned models of human behavior via a\nmeasured step in structured irrationality. Specifically, by replacing the\nsuboptimality constant $\\beta$ in a Boltzmann rationality model with a function\nover states $\\beta(s)$, we gain natural expressivity in a computationally\ntractable manner. This paper discusses relevant mathematical theory, sets up\nseveral experimental designs, presents limited preliminary results, and\nproposes future investigations.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17725v1",
    "published_date": "2024-04-26 23:15:09 UTC",
    "updated_date": "2024-04-26 23:15:09 UTC"
  },
  {
    "arxiv_id": "2404.17723v2",
    "title": "Retrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering",
    "authors": [
      "Zhentao Xu",
      "Mark Jerome Cruz",
      "Matthew Guevara",
      "Tie Wang",
      "Manasi Deshpande",
      "Xiaofeng Wang",
      "Zheng Li"
    ],
    "abstract": "In customer service technical support, swiftly and accurately retrieving\nrelevant past issues is critical for efficiently resolving customer inquiries.\nThe conventional retrieval methods in retrieval-augmented generation (RAG) for\nlarge language models (LLMs) treat a large corpus of past issue tracking\ntickets as plain text, ignoring the crucial intra-issue structure and\ninter-issue relations, which limits performance. We introduce a novel customer\nservice question-answering method that amalgamates RAG with a knowledge graph\n(KG). Our method constructs a KG from historical issues for use in retrieval,\nretaining the intra-issue structure and inter-issue relations. During the\nquestion-answering phase, our method parses consumer queries and retrieves\nrelated sub-graphs from the KG to generate answers. This integration of a KG\nnot only improves retrieval accuracy by preserving customer service structure\ninformation but also enhances answering quality by mitigating the effects of\ntext segmentation. Empirical assessments on our benchmark datasets, utilizing\nkey retrieval (MRR, Recall@K, NDCG@K) and text generation (BLEU, ROUGE, METEOR)\nmetrics, reveal that our method outperforms the baseline by 77.6% in MRR and by\n0.32 in BLEU. Our method has been deployed within LinkedIn's customer service\nteam for approximately six months and has reduced the median per-issue\nresolution time by 28.6%.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "I.2"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17723v2",
    "published_date": "2024-04-26 23:05:20 UTC",
    "updated_date": "2024-05-06 05:16:42 UTC"
  },
  {
    "arxiv_id": "2404.17718v1",
    "title": "Lessons from Deploying CropFollow++: Under-Canopy Agricultural Navigation with Keypoints",
    "authors": [
      "Arun N. Sivakumar",
      "Mateus V. Gasparino",
      "Michael McGuire",
      "Vitor A. H. Higuti",
      "M. Ugur Akcal",
      "Girish Chowdhary"
    ],
    "abstract": "We present a vision-based navigation system for under-canopy agricultural\nrobots using semantic keypoints. Autonomous under-canopy navigation is\nchallenging due to the tight spacing between the crop rows ($\\sim 0.75$ m),\ndegradation in RTK-GPS accuracy due to multipath error, and noise in LiDAR\nmeasurements from the excessive clutter. Our system, CropFollow++, introduces\nmodular and interpretable perception architecture with a learned semantic\nkeypoint representation. We deployed CropFollow++ in multiple under-canopy\ncover crop planting robots on a large scale (25 km in total) in various field\nconditions and we discuss the key lessons learned from this.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted to the IEEE ICRA Workshop on Field Robotics 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.17718v1",
    "published_date": "2024-04-26 22:46:17 UTC",
    "updated_date": "2024-04-26 22:46:17 UTC"
  },
  {
    "arxiv_id": "2404.17716v2",
    "title": "Airlift Challenge: A Competition for Optimizing Cargo Delivery",
    "authors": [
      "Adis Delanovic",
      "Carmen Chiu",
      "John F. Kolen",
      "Marvin Gülhan",
      "Jonathan Cawalla",
      "Andre Beckus"
    ],
    "abstract": "Airlift operations require the timely distribution of various cargo, much of\nwhich is time sensitive and valuable. These operations, however, have to\ncontend with sudden disruptions from weather and malfunctions, requiring\nimmediate rescheduling. The Airlift Challenge competition seeks possible\nsolutions via a simulator that provides a simplified abstraction of the airlift\nproblem. The simulator uses an OpenAI gym interface that allows participants to\ncreate an algorithm for planning agent actions. The algorithm is scored using a\nremote evaluator against scenarios of ever-increasing difficulty. The second\niteration of the competition was underway from November 2023 to April 2024.\nThis paper describes the competition, simulation environment, and results. As a\nstep towards applying generalized planning techniques to the problem, a\ntemporal PDDL domain is presented for the Pickup and Delivery Problem, a model\nwhich lies at the core of the Airlift Challenge.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17716v2",
    "published_date": "2024-04-26 22:30:10 UTC",
    "updated_date": "2025-04-20 00:24:36 UTC"
  },
  {
    "arxiv_id": "2405.00726v2",
    "title": "Unveiling Thoughts: A Review of Advancements in EEG Brain Signal Decoding into Text",
    "authors": [
      "Saydul Akbar Murad",
      "Nick Rahimi"
    ],
    "abstract": "The conversion of brain activity into text using electroencephalography (EEG)\nhas gained significant traction in recent years. Many researchers are working\nto develop new models to decode EEG signals into text form. Although this area\nhas shown promising developments, it still faces numerous challenges that\nnecessitate further improvement. It's important to outline this area's recent\ndevelopments and future research directions. In this review article, we\nthoroughly summarize the progress in EEG-to-text conversion. Firstly, we talk\nabout how EEG-to-text technology has grown and what problems we still face.\nSecondly, we discuss existing techniques used in this field. This includes\nmethods for collecting EEG data, the steps to process these signals, and the\ndevelopment of systems capable of translating these signals into coherent text.\nWe conclude with potential future research directions, emphasizing the need for\nenhanced accuracy, reduced system constraints, and the exploration of novel\napplications across varied sectors. By addressing these aspects, this review\naims to contribute to developing more accessible and effective Brain-Computer\nInterface (BCI) technology for a broader user base.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.00726v2",
    "published_date": "2024-04-26 21:18:05 UTC",
    "updated_date": "2024-09-20 04:28:34 UTC"
  },
  {
    "arxiv_id": "2404.17688v1",
    "title": "Seizing the Means of Production: Exploring the Landscape of Crafting, Adapting and Navigating Generative AI Models in the Visual Arts",
    "authors": [
      "Ahmed M. Abuzuraiq",
      "Philippe Pasquier"
    ],
    "abstract": "In this paper, we map out the landscape of options available to visual\nartists for creating personal artworks, including crafting, adapting and\nnavigating deep generative models. Following that, we argue for revisiting\nmodel crafting, defined as the design and manipulation of generative models for\ncreative goals, and motivate studying and designing for model crafting as a\ncreative activity in its own right.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted to CHI 2024 workshop on Generative AI and HCI",
    "pdf_url": "http://arxiv.org/pdf/2404.17688v1",
    "published_date": "2024-04-26 20:38:09 UTC",
    "updated_date": "2024-04-26 20:38:09 UTC"
  },
  {
    "arxiv_id": "2404.17687v2",
    "title": "Knowledge Transfer for Cross-Domain Reinforcement Learning: A Systematic Review",
    "authors": [
      "Sergio A. Serrano",
      "Jose Martinez-Carranza",
      "L. Enrique Sucar"
    ],
    "abstract": "Reinforcement Learning (RL) provides a framework in which agents can be\ntrained, via trial and error, to solve complex decision-making problems.\nLearning with little supervision causes RL methods to require large amounts of\ndata, rendering them too expensive for many applications (e.g., robotics). By\nreusing knowledge from a different task, knowledge transfer methods present an\nalternative to reduce the training time in RL. Given the severe data scarcity,\ndue to their flexibility, there has been a growing interest in methods capable\nof transferring knowledge across different domains (i.e., problems with\ndifferent representations). However, identifying similarities and adapting\nknowledge across tasks from different domains requires matching their\nrepresentations or finding domain-invariant features. These processes can be\ndata-demanding, which poses the main challenge in cross-domain knowledge\ntransfer: to select and transform knowledge in a data-efficient way, such that\nit accelerates learning in the target task, despite the presence of significant\ndifferences across problems (e.g., robots with distinct morphologies). Thus,\nthis review presents a unifying analysis of methods focused on transferring\nknowledge across different domains. Through a taxonomy based on a\ntransfer-approach categorization and a characterization of works based on their\ndata-assumption requirements, the contributions of this article are 1) a\ncomprehensive and systematic revision of knowledge transfer methods for the\ncross-domain RL setting, 2) a categorization and characterization of such\nmethods to provide an analysis based on relevant features such as their\ntransfer approach and data requirements, and 3) a discussion on the main\nchallenges regarding cross-domain knowledge transfer, as well as on ideas of\nfuture directions worth exploring to address these problems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17687v2",
    "published_date": "2024-04-26 20:36:58 UTC",
    "updated_date": "2024-11-20 19:02:48 UTC"
  },
  {
    "arxiv_id": "2404.17674v2",
    "title": "Center-Based Relaxed Learning Against Membership Inference Attacks",
    "authors": [
      "Xingli Fang",
      "Jung-Eun Kim"
    ],
    "abstract": "Membership inference attacks (MIAs) are currently considered one of the main\nprivacy attack strategies, and their defense mechanisms have also been\nextensively explored. However, there is still a gap between the existing\ndefense approaches and ideal models in performance and deployment costs. In\nparticular, we observed that the privacy vulnerability of the model is closely\ncorrelated with the gap between the model's data-memorizing ability and\ngeneralization ability. To address this, we propose a new architecture-agnostic\ntraining paradigm called center-based relaxed learning (CRL), which is adaptive\nto any classification model and provides privacy preservation by sacrificing a\nminimal or no loss of model generalizability. We emphasize that CRL can better\nmaintain the model's consistency between member and non-member data. Through\nextensive experiments on standard classification datasets, we empirically show\nthat this approach exhibits comparable performance without requiring additional\nmodel capacity or data costs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted in the Conference on Uncertainty in Artificial Intelligence\n  (UAI) 2024, PMLR",
    "pdf_url": "http://arxiv.org/pdf/2404.17674v2",
    "published_date": "2024-04-26 19:41:08 UTC",
    "updated_date": "2024-05-29 17:54:47 UTC"
  },
  {
    "arxiv_id": "2404.17670v1",
    "title": "Federated Learning for Blind Image Super-Resolution",
    "authors": [
      "Brian B. Moser",
      "Ahmed Anwar",
      "Federico Raue",
      "Stanislav Frolov",
      "Andreas Dengel"
    ],
    "abstract": "Traditional blind image SR methods need to model real-world degradations\nprecisely. Consequently, current research struggles with this dilemma by\nassuming idealized degradations, which leads to limited applicability to actual\nuser data. Moreover, the ideal scenario - training models on data from the\ntargeted user base - presents significant privacy concerns. To address both\nchallenges, we propose to fuse image SR with federated learning, allowing\nreal-world degradations to be directly learned from users without invading\ntheir privacy. Furthermore, it enables optimization across many devices without\ndata centralization. As this fusion is underexplored, we introduce new\nbenchmarks specifically designed to evaluate new SR methods in this federated\nsetting. By doing so, we employ known degradation modeling techniques from SR\nresearch. However, rather than aiming to mirror real degradations, our\nbenchmarks use these degradation models to simulate the variety of degradations\nfound across clients within a distributed user base. This distinction is\ncrucial as it circumvents the need to precisely model real-world degradations,\nwhich limits contemporary blind image SR research. Our proposed benchmarks\ninvestigate blind image SR under new aspects, namely differently distributed\ndegradation types among users and varying user numbers. We believe new methods\ntested within these benchmarks will perform more similarly in an application,\nas the simulated scenario addresses the variety while federated learning\nenables the training on actual degradations.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.ET",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17670v1",
    "published_date": "2024-04-26 19:27:07 UTC",
    "updated_date": "2024-04-26 19:27:07 UTC"
  },
  {
    "arxiv_id": "2404.17648v1",
    "title": "Consolidating LAMA with Best-First Width Search",
    "authors": [
      "Augusto B. Corrêa",
      "Jendrik Seipp"
    ],
    "abstract": "One key decision for heuristic search algorithms is how to balance\nexploration and exploitation. In classical planning, novelty search has come\nout as the most successful approach in this respect. The idea is to favor\nstates that contain previously unseen facts when searching for a plan. This is\ndone by maintaining a record of the tuples of facts observed in previous\nstates. Then the novelty of a state is the size of the smallest previously\nunseen tuple. The most successful version of novelty search is best-first width\nsearch (BFWS), which combines novelty measures with heuristic estimates. An\northogonal approach to balance exploration-exploitation is to use several\nopen-lists. These open-lists are ordered using different heuristic estimates,\nwhich diversify the information used in the search. The search algorithm then\nalternates between these open-lists, trying to exploit these different\nestimates. This is the approach used by LAMA, a classical planner that, a\ndecade after its release, is still considered state-of-the-art in agile\nplanning. In this paper, we study how to combine LAMA and BFWS. We show that\nsimply adding the strongest open-list used in BFWS to LAMA harms performance.\nHowever, we show that combining only parts of each planner leads to a new\nstate-of-the-art agile planner.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17648v1",
    "published_date": "2024-04-26 18:10:29 UTC",
    "updated_date": "2024-04-26 18:10:29 UTC"
  },
  {
    "arxiv_id": "2404.17644v6",
    "title": "A Conditional Independence Test in the Presence of Discretization",
    "authors": [
      "Boyang Sun",
      "Yu Yao",
      "Guang-Yuan Hao",
      "Yumou Qiu",
      "Kun Zhang"
    ],
    "abstract": "Testing conditional independence has many applications, such as in Bayesian\nnetwork learning and causal discovery. Different test methods have been\nproposed. However, existing methods generally can not work when only\ndiscretized observations are available. Specifically, consider $X_1$,\n$\\tilde{X}_2$ and $X_3$ are observed variables, where $\\tilde{X}_2$ is a\ndiscretization of latent variables $X_2$. Applying existing test methods to the\nobservations of $X_1$, $\\tilde{X}_2$ and $X_3$ can lead to a false conclusion\nabout the underlying conditional independence of variables $X_1$, $X_2$ and\n$X_3$. Motivated by this, we propose a conditional independence test\nspecifically designed to accommodate the presence of such discretization. To\nachieve this, we design the bridge equations to recover the parameter\nreflecting the statistical information of the underlying latent continuous\nvariables. An appropriate test statistic and its asymptotic distribution under\nthe null hypothesis of conditional independence have also been derived. Both\ntheoretical results and empirical validation have been provided, demonstrating\nthe effectiveness of our test methods.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17644v6",
    "published_date": "2024-04-26 18:08:15 UTC",
    "updated_date": "2025-03-18 15:55:12 UTC"
  },
  {
    "arxiv_id": "2404.17642v1",
    "title": "Empowering Large Language Models for Textual Data Augmentation",
    "authors": [
      "Yichuan Li",
      "Kaize Ding",
      "Jianling Wang",
      "Kyumin Lee"
    ],
    "abstract": "With the capabilities of understanding and executing natural language\ninstructions, Large language models (LLMs) can potentially act as a powerful\ntool for textual data augmentation. However, the quality of augmented data\ndepends heavily on the augmentation instructions provided, and the\neffectiveness can fluctuate across different downstream tasks. While manually\ncrafting and selecting instructions can offer some improvement, this approach\nfaces scalability and consistency issues in practice due to the diversity of\ndownstream tasks. In this work, we address these limitations by proposing a new\nsolution, which can automatically generate a large pool of augmentation\ninstructions and select the most suitable task-informed instructions, thereby\nempowering LLMs to create high-quality augmented data for different downstream\ntasks. Empirically, the proposed approach consistently generates augmented data\nwith better quality compared to non-LLM and LLM-based data augmentation\nmethods, leading to the best performance on 26 few-shot learning tasks sourced\nfrom a wide range of application domains.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17642v1",
    "published_date": "2024-04-26 18:04:25 UTC",
    "updated_date": "2024-04-26 18:04:25 UTC"
  },
  {
    "arxiv_id": "2405.01582v3",
    "title": "Text Quality-Based Pruning for Efficient Training of Language Models",
    "authors": [
      "Vasu Sharma",
      "Karthik Padthe",
      "Newsha Ardalani",
      "Kushal Tirumala",
      "Russell Howes",
      "Hu Xu",
      "Po-Yao Huang",
      "Shang-Wen Li",
      "Armen Aghajanyan",
      "Gargi Ghosh",
      "Luke Zettlemoyer"
    ],
    "abstract": "In recent times training Language Models (LMs) have relied on computationally\nheavy training over massive datasets which makes this training process\nextremely laborious. In this paper we propose a novel method for numerically\nevaluating text quality in large unlabelled NLP datasets in a model agnostic\nmanner to assign the text instances a \"quality score\".\n  By proposing the text quality metric, the paper establishes a framework to\nidentify and eliminate low-quality text instances, leading to improved training\nefficiency for LM models. Experimental results over multiple models and\ndatasets demonstrate the efficacy of this approach, showcasing substantial\ngains in training effectiveness and highlighting the potential for\nresource-efficient LM training.\n  For example, we observe an absolute accuracy improvement of 0.9% averaged\nover 14 downstream evaluation tasks for multiple LM models while using 40%\nlesser data and training 42% faster when training on the OpenWebText dataset\nand 0.8% average absolute accuracy improvement while using 20% lesser data and\ntraining 21% faster on the Wikipedia dataset.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01582v3",
    "published_date": "2024-04-26 18:01:25 UTC",
    "updated_date": "2024-05-10 23:35:53 UTC"
  },
  {
    "arxiv_id": "2404.17546v1",
    "title": "Probabilistic Inference in Language Models via Twisted Sequential Monte Carlo",
    "authors": [
      "Stephen Zhao",
      "Rob Brekelmans",
      "Alireza Makhzani",
      "Roger Grosse"
    ],
    "abstract": "Numerous capability and safety techniques of Large Language Models (LLMs),\nincluding RLHF, automated red-teaming, prompt engineering, and infilling, can\nbe cast as sampling from an unnormalized target distribution defined by a given\nreward or potential function over the full sequence. In this work, we leverage\nthe rich toolkit of Sequential Monte Carlo (SMC) for these probabilistic\ninference problems. In particular, we use learned twist functions to estimate\nthe expected future value of the potential at each timestep, which enables us\nto focus inference-time computation on promising partial sequences. We propose\na novel contrastive method for learning the twist functions, and establish\nconnections with the rich literature of soft reinforcement learning. As a\ncomplementary application of our twisted SMC framework, we present methods for\nevaluating the accuracy of language model inference techniques using novel\nbidirectional SMC bounds on the log partition function. These bounds can be\nused to estimate the KL divergence between the inference and target\ndistributions in both directions. We apply our inference evaluation techniques\nto show that twisted SMC is effective for sampling undesirable outputs from a\npretrained model (a useful component of harmlessness training and automated\nred-teaming), generating reviews with varied sentiment, and performing\ninfilling tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17546v1",
    "published_date": "2024-04-26 17:18:32 UTC",
    "updated_date": "2024-04-26 17:18:32 UTC"
  },
  {
    "arxiv_id": "2404.17535v1",
    "title": "Using Neural Implicit Flow To Represent Latent Dynamics Of Canonical Systems",
    "authors": [
      "Imran Nasim",
      "Joaõ Lucas de Sousa Almeida"
    ],
    "abstract": "The recently introduced class of architectures known as Neural Operators has\nemerged as highly versatile tools applicable to a wide range of tasks in the\nfield of Scientific Machine Learning (SciML), including data representation and\nforecasting. In this study, we investigate the capabilities of Neural Implicit\nFlow (NIF), a recently developed mesh-agnostic neural operator, for\nrepresenting the latent dynamics of canonical systems such as the\nKuramoto-Sivashinsky (KS), forced Korteweg-de Vries (fKdV), and Sine-Gordon\n(SG) equations, as well as for extracting dynamically relevant information from\nthem. Finally we assess the applicability of NIF as a dimensionality reduction\nalgorithm and conduct a comparative analysis with another widely recognized\nfamily of neural operators, known as Deep Operator Networks (DeepONets).",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted into the International conference on Scientific Computation\n  and Machine Learning 2024 (SCML 2024)",
    "pdf_url": "http://arxiv.org/pdf/2404.17535v1",
    "published_date": "2024-04-26 17:01:38 UTC",
    "updated_date": "2024-04-26 17:01:38 UTC"
  },
  {
    "arxiv_id": "2405.01581v1",
    "title": "The Mercurial Top-Level Ontology of Large Language Models",
    "authors": [
      "Nele Köhler",
      "Fabian Neuhaus"
    ],
    "abstract": "In our work, we systematize and analyze implicit ontological commitments in\nthe responses generated by large language models (LLMs), focusing on ChatGPT\n3.5 as a case study. We investigate how LLMs, despite having no explicit\nontology, exhibit implicit ontological categorizations that are reflected in\nthe texts they generate. The paper proposes an approach to understanding the\nontological commitments of LLMs by defining ontology as a theory that provides\na systematic account of the ontological commitments of some text. We\ninvestigate the ontological assumptions of ChatGPT and present a systematized\naccount, i.e., GPT's top-level ontology. This includes a taxonomy, which is\navailable as an OWL file, as well as a discussion about ontological assumptions\n(e.g., about its mereology or presentism). We show that in some aspects GPT's\ntop-level ontology is quite similar to existing top-level ontologies. However,\nthere are significant challenges arising from the flexible nature of\nLLM-generated texts, including ontological overload, ambiguity, and\ninconsistency.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01581v1",
    "published_date": "2024-04-26 16:52:32 UTC",
    "updated_date": "2024-04-26 16:52:32 UTC"
  },
  {
    "arxiv_id": "2404.17525v3",
    "title": "Large Language Model Agent as a Mechanical Designer",
    "authors": [
      "Yayati Jadhav",
      "Amir Barati Farimani"
    ],
    "abstract": "Conventional mechanical design follows an iterative process in which initial\nconcepts are refined through cycles of expert assessment and resource-intensive\nFinite Element Method (FEM) analysis to meet performance goals. While machine\nlearning models have been developed to assist in parts of this process, they\ntypically require large datasets, extensive training, and are often tailored to\nspecific tasks, limiting their generalizability. To address these limitations,\nwe propose a framework that leverages a pretrained Large Language Model (LLM)\nin conjunction with an FEM module to autonomously generate, evaluate, and\nrefine structural designs based on performance specifications and numerical\nfeedback. The LLM operates without domain-specific fine-tuning, using general\nreasoning to propose design candidates, interpret FEM-derived performance\nmetrics, and apply structurally sound modifications. Using 2D truss structures\nas a testbed, we show that the LLM can effectively navigate highly discrete and\nmulti-faceted design spaces, balance competing objectives, and identify\nconvergence when further optimization yields diminishing returns. Compared to\nNon-dominated Sorting Genetic Algorithm II (NSGA-II), our method achieves\nfaster convergence and fewer FEM evaluations. Experiments with varying\ntemperature settings (0.5, 1.0, 1.2) and model sizes (GPT-4.1 and GPT-4.1-mini)\nindicate that smaller models yield higher constraint satisfaction with fewer\nsteps, while lower temperatures enhance design consistency. These results\nestablish LLMs as a promising new class of reasoning-based, natural\nlanguage-driven optimizers for autonomous design and iterative structural\nrefinement.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17525v3",
    "published_date": "2024-04-26 16:41:24 UTC",
    "updated_date": "2025-04-30 18:23:36 UTC"
  },
  {
    "arxiv_id": "2404.17524v4",
    "title": "On the Use of Large Language Models to Generate Capability Ontologies",
    "authors": [
      "Luis Miguel Vieira da Silva",
      "Aljosha Köcher",
      "Felix Gehlhoff",
      "Alexander Fay"
    ],
    "abstract": "Capability ontologies are increasingly used to model functionalities of\nsystems or machines. The creation of such ontological models with all\nproperties and constraints of capabilities is very complex and can only be done\nby ontology experts. However, Large Language Models (LLMs) have shown that they\ncan generate machine-interpretable models from natural language text input and\nthus support engineers / ontology experts. Therefore, this paper investigates\nhow LLMs can be used to create capability ontologies. We present a study with a\nseries of experiments in which capabilities with varying complexities are\ngenerated using different prompting techniques and with different LLMs. Errors\nin the generated ontologies are recorded and compared. To analyze the quality\nof the generated ontologies, a semi-automated approach based on RDF syntax\nchecking, OWL reasoning, and SHACL constraints is used. The results of this\nstudy are very promising because even for complex capabilities, the generated\nontologies are almost free of errors.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "\\c{opyright} 2024 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works",
    "pdf_url": "http://arxiv.org/pdf/2404.17524v4",
    "published_date": "2024-04-26 16:41:00 UTC",
    "updated_date": "2024-10-18 08:03:02 UTC"
  },
  {
    "arxiv_id": "2404.17522v1",
    "title": "Enhancing Legal Compliance and Regulation Analysis with Large Language Models",
    "authors": [
      "Shabnam Hassani"
    ],
    "abstract": "This research explores the application of Large Language Models (LLMs) for\nautomating the extraction of requirement-related legal content in the food\nsafety domain and checking legal compliance of regulatory artifacts. With\nIndustry 4.0 revolutionizing the food industry and with the General Data\nProtection Regulation (GDPR) reshaping privacy policies and data processing\nagreements, there is a growing gap between regulatory analysis and recent\ntechnological advancements. This study aims to bridge this gap by leveraging\nLLMs, namely BERT and GPT models, to accurately classify legal provisions and\nautomate compliance checks. Our findings demonstrate promising results,\nindicating LLMs' significant potential to enhance legal compliance and\nregulatory analysis efficiency, notably by reducing manual workload and\nimproving accuracy within reasonable time and financial constraints.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "to be published in 32nd IEEE International Requirements Engineering\n  2024 Conference (RE'24) - Doctoral Symposium. arXiv admin note: text overlap\n  with arXiv:2404.14356",
    "pdf_url": "http://arxiv.org/pdf/2404.17522v1",
    "published_date": "2024-04-26 16:40:49 UTC",
    "updated_date": "2024-04-26 16:40:49 UTC"
  },
  {
    "arxiv_id": "2404.17513v2",
    "title": "A Comprehensive Evaluation on Event Reasoning of Large Language Models",
    "authors": [
      "Zhengwei Tao",
      "Zhi Jin",
      "Yifan Zhang",
      "Xiancai Chen",
      "Haiyan Zhao",
      "Jia Li",
      "Bing Liang",
      "Chongyang Tao",
      "Qun Liu",
      "Kam-Fai Wong"
    ],
    "abstract": "Event reasoning is a fundamental ability that underlies many applications. It\nrequires event schema knowledge to perform global reasoning and needs to deal\nwith the diversity of the inter-event relations and the reasoning paradigms.\nHow well LLMs accomplish event reasoning on various relations and reasoning\nparadigms remains unknown. To mitigate this disparity, we comprehensively\nevaluate the abilities of event reasoning of LLMs. We introduce a novel\nbenchmark EV2 for EValuation of EVent reasoning. EV2 consists of two levels of\nevaluation of schema and instance and is comprehensive in relations and\nreasoning paradigms. We conduct extensive experiments on EV2. We find that LLMs\nhave abilities to accomplish event reasoning but their performances are far\nfrom satisfactory. We also notice the imbalance of event reasoning abilities in\nLLMs. Besides, LLMs have event schema knowledge, however, they're not aligned\nwith humans on how to utilize the knowledge. Based on these findings, we guide\nthe LLMs in utilizing the event schema knowledge as memory leading to\nimprovements on event reasoning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17513v2",
    "published_date": "2024-04-26 16:28:34 UTC",
    "updated_date": "2024-08-02 17:39:32 UTC"
  },
  {
    "arxiv_id": "2405.01580v1",
    "title": "On the Limitations of Embedding Based Methods for Measuring Functional Correctness for Code Generation",
    "authors": [
      "Atharva Naik"
    ],
    "abstract": "The task of code generation from natural language (NL2Code) has become\nextremely popular, especially with the advent of Large Language Models (LLMs).\nHowever, efforts to quantify and track this progress have suffered due to a\nlack of reliable metrics for functional correctness. While popular benchmarks\nlike HumanEval have test cases to enable reliable evaluation of correctness, it\nis time-consuming and requires human effort to collect test cases. As an\nalternative several reference-based evaluation metrics have been proposed, with\nembedding-based metrics like CodeBERTScore being touted as having a high\ncorrelation with human preferences and functional correctness. In our work, we\nanalyze the ability of embedding-based metrics like CodeBERTScore to measure\nfunctional correctness and other helpful constructs like editing effort by\nanalyzing outputs of ten models over two popular code generation benchmarks.\nOur results show that while they have a weak correlation with functional\ncorrectness (0.16), they are strongly correlated (0.72) with editing effort.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01580v1",
    "published_date": "2024-04-26 15:54:39 UTC",
    "updated_date": "2024-04-26 15:54:39 UTC"
  },
  {
    "arxiv_id": "2404.17493v2",
    "title": "Causally Abstracted Multi-armed Bandits",
    "authors": [
      "Fabio Massimo Zennaro",
      "Nicholas Bishop",
      "Joel Dyer",
      "Yorgos Felekis",
      "Anisoara Calinescu",
      "Michael Wooldridge",
      "Theodoros Damoulas"
    ],
    "abstract": "Multi-armed bandits (MAB) and causal MABs (CMAB) are established frameworks\nfor decision-making problems. The majority of prior work typically studies and\nsolves individual MAB and CMAB in isolation for a given problem and associated\ndata. However, decision-makers are often faced with multiple related problems\nand multi-scale observations where joint formulations are needed in order to\nefficiently exploit the problem structures and data dependencies. Transfer\nlearning for CMABs addresses the situation where models are defined on\nidentical variables, although causal connections may differ. In this work, we\nextend transfer learning to setups involving CMABs defined on potentially\ndifferent variables, with varying degrees of granularity, and related via an\nabstraction map. Formally, we introduce the problem of causally abstracted MABs\n(CAMABs) by relying on the theory of causal abstraction in order to express a\nrigorous abstraction map. We propose algorithms to learn in a CAMAB, and study\ntheir regret. We illustrate the limitations and the strengths of our algorithms\non a real-world scenario related to online advertising.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 3 figures (main article); 20 pages, 10 figures (appendix);\n  40th Conference on Uncertainty in Artificial Intelligence (UAI)",
    "pdf_url": "http://arxiv.org/pdf/2404.17493v2",
    "published_date": "2024-04-26 15:48:09 UTC",
    "updated_date": "2024-07-17 07:10:25 UTC"
  },
  {
    "arxiv_id": "2404.17489v2",
    "title": "Tabular Data Contrastive Learning via Class-Conditioned and Feature-Correlation Based Augmentation",
    "authors": [
      "Wei Cui",
      "Rasa Hosseinzadeh",
      "Junwei Ma",
      "Tongzi Wu",
      "Yi Sui",
      "Keyvan Golestan"
    ],
    "abstract": "Contrastive learning is a model pre-training technique by first creating\nsimilar views of the original data, and then encouraging the data and its\ncorresponding views to be close in the embedding space. Contrastive learning\nhas witnessed success in image and natural language data, thanks to the\ndomain-specific augmentation techniques that are both intuitive and effective.\nNonetheless, in tabular domain, the predominant augmentation technique for\ncreating views is through corrupting tabular entries via swapping values, which\nis not as sound or effective. We propose a simple yet powerful improvement to\nthis augmentation technique: corrupting tabular data conditioned on class\nidentity. Specifically, when corrupting a specific tabular entry from an anchor\nrow, instead of randomly sampling a value in the same feature column from the\nentire table uniformly, we only sample from rows that are identified to be\nwithin the same class as the anchor row. We assume the semi-supervised learning\nsetting, and adopt the pseudo labeling technique for obtaining class identities\nover all table rows. We also explore the novel idea of selecting features to be\ncorrupted based on feature correlation structures. Extensive experiments show\nthat the proposed approach consistently outperforms the conventional corruption\nmethod for tabular data classification tasks. Our code is available at\nhttps://github.com/willtop/Tabular-Class-Conditioned-SSL.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, 4 algorithms, 3 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2404.17489v2",
    "published_date": "2024-04-26 15:43:49 UTC",
    "updated_date": "2024-04-30 14:11:15 UTC"
  },
  {
    "arxiv_id": "2404.17487v1",
    "title": "Conformal Prediction with Learned Features",
    "authors": [
      "Shayan Kiyani",
      "George Pappas",
      "Hamed Hassani"
    ],
    "abstract": "In this paper, we focus on the problem of conformal prediction with\nconditional guarantees. Prior work has shown that it is impossible to construct\nnontrivial prediction sets with full conditional coverage guarantees. A wealth\nof research has considered relaxations of full conditional guarantees, relying\non some predefined uncertainty structures. Departing from this line of\nthinking, we propose Partition Learning Conformal Prediction (PLCP), a\nframework to improve conditional validity of prediction sets through learning\nuncertainty-guided features from the calibration data. We implement PLCP\nefficiently with alternating gradient descent, utilizing off-the-shelf machine\nlearning models. We further analyze PLCP theoretically and provide conditional\nguarantees for infinite and finite sample sizes. Finally, our experimental\nresults over four real-world and synthetic datasets show the superior\nperformance of PLCP compared to state-of-the-art methods in terms of coverage\nand length in both classification and regression scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17487v1",
    "published_date": "2024-04-26 15:43:06 UTC",
    "updated_date": "2024-04-26 15:43:06 UTC"
  },
  {
    "arxiv_id": "2404.17475v2",
    "title": "CEval: A Benchmark for Evaluating Counterfactual Text Generation",
    "authors": [
      "Van Bach Nguyen",
      "Jörg Schlötterer",
      "Christin Seifert"
    ],
    "abstract": "Counterfactual text generation aims to minimally change a text, such that it\nis classified differently. Judging advancements in method development for\ncounterfactual text generation is hindered by a non-uniform usage of data sets\nand metrics in related work. We propose CEval, a benchmark for comparing\ncounterfactual text generation methods. CEval unifies counterfactual and text\nquality metrics, includes common counterfactual datasets with human\nannotations, standard baselines (MICE, GDBA, CREST) and the open-source\nlanguage model LLAMA-2. Our experiments found no perfect method for generating\ncounterfactual text. Methods that excel at counterfactual metrics often produce\nlower-quality text while LLMs with simple prompts generate high-quality text\nbut struggle with counterfactual criteria. By making CEval available as an\nopen-source Python library, we encourage the community to contribute more\nmethods and maintain consistent evaluation in future work.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17475v2",
    "published_date": "2024-04-26 15:23:47 UTC",
    "updated_date": "2024-08-13 07:39:59 UTC"
  },
  {
    "arxiv_id": "2404.17625v2",
    "title": "Alice's Adventures in a Differentiable Wonderland -- Volume I, A Tour of the Land",
    "authors": [
      "Simone Scardapane"
    ],
    "abstract": "Neural networks surround us, in the form of large language models, speech\ntranscription systems, molecular discovery algorithms, robotics, and much more.\nStripped of anything else, neural networks are compositions of differentiable\nprimitives, and studying them means learning how to program and how to interact\nwith these models, a particular example of what is called differentiable\nprogramming.\n  This primer is an introduction to this fascinating field imagined for\nsomeone, like Alice, who has just ventured into this strange differentiable\nwonderland. I overview the basics of optimizing a function via automatic\ndifferentiation, and a selection of the most common designs for handling\nsequences, graphs, texts, and audios. The focus is on a intuitive,\nself-contained introduction to the most important design techniques, including\nconvolutional, attentional, and recurrent blocks, hoping to bridge the gap\nbetween theory and code (PyTorch and JAX) and leaving the reader capable of\nunderstanding some of the most advanced models out there, such as large\nlanguage models (LLMs) and multimodal architectures.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Companion website for additional chapters:\n  https://www.sscardapane.it/alice-book",
    "pdf_url": "http://arxiv.org/pdf/2404.17625v2",
    "published_date": "2024-04-26 15:19:58 UTC",
    "updated_date": "2024-07-04 14:52:11 UTC"
  },
  {
    "arxiv_id": "2405.02334v2",
    "title": "Rad4XCNN: a new agnostic method for post-hoc global explanation of CNN-derived features by means of radiomics",
    "authors": [
      "Francesco Prinzi",
      "Carmelo Militello",
      "Calogero Zarcaro",
      "Tommaso Vincenzo Bartolotta",
      "Salvatore Gaglio",
      "Salvatore Vitabile"
    ],
    "abstract": "In recent years, machine learning-based clinical decision support systems\n(CDSS) have played a key role in the analysis of several medical conditions.\nDespite their promising capabilities, the lack of transparency in AI models\nposes significant challenges, particularly in medical contexts where\nreliability is a mandatory aspect. However, it appears that explainability is\ninversely proportional to accuracy. For this reason, achieving transparency\nwithout compromising predictive accuracy remains a key challenge. This paper\npresents a novel method, namely Rad4XCNN, to enhance the predictive power of\nCNN-derived features with the inherent interpretability of radiomic features.\nRad4XCNN diverges from conventional methods based on saliency maps, by\nassociating intelligible meaning to CNN-derived features by means of Radiomics,\noffering new perspectives on explanation methods beyond visualization maps.\nUsing a breast cancer classification task as a case study, we evaluated\nRad4XCNN on ultrasound imaging datasets, including an online dataset and two\nin-house datasets for internal and external validation. Some key results are:\ni) CNN-derived features guarantee more robust accuracy when compared against\nViT-derived and radiomic features; ii) conventional visualization map methods\nfor explanation present several pitfalls; iii) Rad4XCNN does not sacrifice\nmodel accuracy for their explainability; iv) Rad4XCNN provides a global\nexplanation enabling the physician to extract global insights and findings. Our\nmethod can mitigate some concerns related to the explainability-accuracy\ntrade-off. This study highlighted the importance of proposing new methods for\nmodel explanation without affecting their accuracy.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.02334v2",
    "published_date": "2024-04-26 15:02:39 UTC",
    "updated_date": "2025-01-08 14:42:05 UTC"
  },
  {
    "arxiv_id": "2404.18591v1",
    "title": "FashionSD-X: Multimodal Fashion Garment Synthesis using Latent Diffusion",
    "authors": [
      "Abhishek Kumar Singh",
      "Ioannis Patras"
    ],
    "abstract": "The rapid evolution of the fashion industry increasingly intersects with\ntechnological advancements, particularly through the integration of generative\nAI. This study introduces a novel generative pipeline designed to transform the\nfashion design process by employing latent diffusion models. Utilizing\nControlNet and LoRA fine-tuning, our approach generates high-quality images\nfrom multimodal inputs such as text and sketches. We leverage and enhance\nstate-of-the-art virtual try-on datasets, including Multimodal Dress Code and\nVITON-HD, by integrating sketch data. Our evaluation, utilizing metrics like\nFID, CLIP Score, and KID, demonstrates that our model significantly outperforms\ntraditional stable diffusion models. The results not only highlight the\neffectiveness of our model in generating fashion-appropriate outputs but also\nunderscore the potential of diffusion models in revolutionizing fashion design\nworkflows. This research paves the way for more interactive, personalized, and\ntechnologically enriched methodologies in fashion design and representation,\nbridging the gap between creative vision and practical application.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.18591v1",
    "published_date": "2024-04-26 14:59:42 UTC",
    "updated_date": "2024-04-26 14:59:42 UTC"
  },
  {
    "arxiv_id": "2404.17461v1",
    "title": "Multi-layer random features and the approximation power of neural networks",
    "authors": [
      "Rustem Takhanov"
    ],
    "abstract": "A neural architecture with randomly initialized weights, in the infinite\nwidth limit, is equivalent to a Gaussian Random Field whose covariance function\nis the so-called Neural Network Gaussian Process kernel (NNGP). We prove that a\nreproducing kernel Hilbert space (RKHS) defined by the NNGP contains only\nfunctions that can be approximated by the architecture. To achieve a certain\napproximation error the required number of neurons in each layer is defined by\nthe RKHS norm of the target function. Moreover, the approximation can be\nconstructed from a supervised dataset by a random multi-layer representation of\nan input vector, together with training of the last layer's weights.\n  For a 2-layer NN and a domain equal to an $n-1$-dimensional sphere in\n${\\mathbb R}^n$, we compare the number of neurons required by Barron's theorem\nand by the multi-layer features construction. We show that if eigenvalues of\nthe integral operator of the NNGP decay slower than $k^{-n-\\frac{2}{3}}$ where\n$k$ is an order of an eigenvalue, then our theorem guarantees a more succinct\nneural network approximation than Barron's theorem. We also make some\ncomputational experiments to verify our theoretical findings. Our experiments\nshow that realistic neural networks easily learn target functions even when\nboth theorems do not give any guarantees.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to Uncertainty in Artificial Intelligence (UAI) 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.17461v1",
    "published_date": "2024-04-26 14:57:56 UTC",
    "updated_date": "2024-04-26 14:57:56 UTC"
  },
  {
    "arxiv_id": "2404.17454v2",
    "title": "Domain Adaptive and Fine-grained Anomaly Detection for Single-cell Sequencing Data and Beyond",
    "authors": [
      "Kaichen Xu",
      "Yueyang Ding",
      "Suyang Hou",
      "Weiqiang Zhan",
      "Nisang Chen",
      "Jun Wang",
      "Xiaobo Sun"
    ],
    "abstract": "Fined-grained anomalous cell detection from affected tissues is critical for\nclinical diagnosis and pathological research. Single-cell sequencing data\nprovide unprecedented opportunities for this task. However, current anomaly\ndetection methods struggle to handle domain shifts prevalent in multi-sample\nand multi-domain single-cell sequencing data, leading to suboptimal\nperformance. Moreover, these methods fall short of distinguishing anomalous\ncells into pathologically distinct subtypes. In response, we propose ACSleuth,\na novel, reconstruction deviation-guided generative framework that integrates\nthe detection, domain adaptation, and fine-grained annotating of anomalous\ncells into a methodologically cohesive workflow. Notably, we present the first\ntheoretical analysis of using reconstruction deviations output by generative\nmodels for anomaly detection in lieu of domain shifts. This analysis informs us\nto develop a novel and superior maximum mean discrepancy-based anomaly scorer\nin ACSleuth. Extensive benchmarks over various single-cell data and other types\nof tabular data demonstrate ACSleuth's superiority over the state-of-the-art\nmethods in identifying and subtyping anomalies in multi-sample and multi-domain\ncontexts. Our code is available at https://github.com/Catchxu/ACsleuth.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG",
    "comment": "17 pages, 2 figures. Accepted by IJCAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.17454v2",
    "published_date": "2024-04-26 14:48:24 UTC",
    "updated_date": "2024-04-29 16:14:26 UTC"
  },
  {
    "arxiv_id": "2404.17443v1",
    "title": "\"ChatGPT Is Here to Help, Not to Replace Anybody\" -- An Evaluation of Students' Opinions On Integrating ChatGPT In CS Courses",
    "authors": [
      "Bruno Pereira Cipriano",
      "Pedro Alves"
    ],
    "abstract": "Large Language Models (LLMs) like GPT and Bard are capable of producing code\nbased on textual descriptions, with remarkable efficacy. Such technology will\nhave profound implications for computing education, raising concerns about\ncheating, excessive dependence, and a decline in computational thinking skills,\namong others. There has been extensive research on how teachers should handle\nthis challenge but it is also important to understand how students feel about\nthis paradigm shift. In this research, 52 first-year CS students were surveyed\nin order to assess their views on technologies with code-generation\ncapabilities, both from academic and professional perspectives. Our findings\nindicate that while students generally favor the academic use of GPT, they\ndon't over rely on it, only mildly asking for its help. Although most students\nbenefit from GPT, some struggle to use it effectively, urging the need for\nspecific GPT training. Opinions on GPT's impact on their professional lives\nvary, but there is a consensus on its importance in academic practice.",
    "categories": [
      "cs.ET",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.ET",
    "comment": "Author's version: this is a paper under revision",
    "pdf_url": "http://arxiv.org/pdf/2404.17443v1",
    "published_date": "2024-04-26 14:29:16 UTC",
    "updated_date": "2024-04-26 14:29:16 UTC"
  },
  {
    "arxiv_id": "2404.17438v1",
    "title": "Real-World Deployment of a Hierarchical Uncertainty-Aware Collaborative Multiagent Planning System",
    "authors": [
      "Martina Stadler Kurtz",
      "Samuel Prentice",
      "Yasmin Veys",
      "Long Quang",
      "Carlos Nieto-Granda",
      "Michael Novitzky",
      "Ethan Stump",
      "Nicholas Roy"
    ],
    "abstract": "We would like to enable a collaborative multiagent team to navigate at long\nlength scales and under uncertainty in real-world environments. In practice,\nplanning complexity scales with the number of agents in the team, with the\nlength scale of the environment, and with environmental uncertainty. Enabling\ntractable planning requires developing abstract models that can represent\ncomplex, high-quality plans. However, such models often abstract away\ninformation needed to generate directly-executable plans for real-world agents\nin real-world environments, as planning in such detail, especially in the\npresence of real-world uncertainty, would be computationally intractable. In\nthis paper, we describe the deployment of a planning system that used a\nhierarchy of planners to execute collaborative multiagent navigation tasks in\nreal-world, unknown environments. By developing a planning system that was\nrobust to failures at every level of the planning hierarchy, we enabled the\nteam to complete collaborative navigation tasks, even in the presence of\nimperfect planning abstractions and real-world uncertainty. We deployed our\napproach on a Clearpath Husky-Jackal team navigating in a structured outdoor\nenvironment, and demonstrated that the system enabled the agents to\nsuccessfully execute collaborative plans.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted to the IEEE ICRA Workshop on Field Robotics 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.17438v1",
    "published_date": "2024-04-26 14:22:50 UTC",
    "updated_date": "2024-04-26 14:22:50 UTC"
  },
  {
    "arxiv_id": "2404.17400v2",
    "title": "Spatial-frequency Dual-Domain Feature Fusion Network for Low-Light Remote Sensing Image Enhancement",
    "authors": [
      "Zishu Yao",
      "Guodong Fan",
      "Jinfu Fan",
      "Min Gan",
      "C. L. Philip Chen"
    ],
    "abstract": "Low-light remote sensing images generally feature high resolution and high\nspatial complexity, with continuously distributed surface features in space.\nThis continuity in scenes leads to extensive long-range correlations in spatial\ndomains within remote sensing images. Convolutional Neural Networks, which rely\non local correlations for long-distance modeling, struggle to establish\nlong-range correlations in such images. On the other hand, transformer-based\nmethods that focus on global information face high computational complexities\nwhen processing high-resolution remote sensing images. From another\nperspective, Fourier transform can compute global information without\nintroducing a large number of parameters, enabling the network to more\nefficiently capture the overall image structure and establish long-range\ncorrelations. Therefore, we propose a Dual-Domain Feature Fusion Network (DFFN)\nfor low-light remote sensing image enhancement. Specifically, this challenging\ntask of low-light enhancement is divided into two more manageable sub-tasks:\nthe first phase learns amplitude information to restore image brightness, and\nthe second phase learns phase information to refine details. To facilitate\ninformation exchange between the two phases, we designed an information fusion\naffine block that combines data from different phases and scales. Additionally,\nwe have constructed two dark light remote sensing datasets to address the\ncurrent lack of datasets in dark light remote sensing image enhancement.\nExtensive evaluations show that our method outperforms existing\nstate-of-the-art methods. The code is available at\nhttps://github.com/iijjlk/DFFN.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "14 page",
    "pdf_url": "http://arxiv.org/pdf/2404.17400v2",
    "published_date": "2024-04-26 13:21:31 UTC",
    "updated_date": "2024-09-06 05:50:16 UTC"
  },
  {
    "arxiv_id": "2405.00723v1",
    "title": "EEG_RL-Net: Enhancing EEG MI Classification through Reinforcement Learning-Optimised Graph Neural Networks",
    "authors": [
      "Htoo Wai Aung",
      "Jiao Jiao Li",
      "Yang An",
      "Steven W. Su"
    ],
    "abstract": "Brain-Computer Interfaces (BCIs) rely on accurately decoding\nelectroencephalography (EEG) motor imagery (MI) signals for effective device\ncontrol. Graph Neural Networks (GNNs) outperform Convolutional Neural Networks\n(CNNs) in this regard, by leveraging the spatial relationships between EEG\nelectrodes through adjacency matrices. The EEG_GLT-Net framework, featuring the\nstate-of-the-art EEG_GLT adjacency matrix method, has notably enhanced EEG MI\nsignal classification, evidenced by an average accuracy of 83.95% across 20\nsubjects on the PhysioNet dataset. This significantly exceeds the 76.10%\naccuracy rate achieved using the Pearson Correlation Coefficient (PCC) method\nwithin the same framework.\n  In this research, we advance the field by applying a Reinforcement Learning\n(RL) approach to the classification of EEG MI signals. Our innovative method\nempowers the RL agent, enabling not only the classification of EEG MI data\npoints with higher accuracy, but effective identification of EEG MI data points\nthat are less distinct. We present the EEG_RL-Net, an enhancement of the\nEEG_GLT-Net framework, which incorporates the trained EEG GCN Block from\nEEG_GLT-Net at an adjacency matrix density of 13.39% alongside the RL-centric\nDueling Deep Q Network (Dueling DQN) block. The EEG_RL-Net model showcases\nexceptional classification performance, achieving an unprecedented average\naccuracy of 96.40% across 20 subjects within 25 milliseconds. This model\nillustrates the transformative effect of the RL in EEG MI time point\nclassification.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.00723v1",
    "published_date": "2024-04-26 13:09:50 UTC",
    "updated_date": "2024-04-26 13:09:50 UTC"
  },
  {
    "arxiv_id": "2404.17391v1",
    "title": "M3BAT: Unsupervised Domain Adaptation for Multimodal Mobile Sensing with Multi-Branch Adversarial Training",
    "authors": [
      "Lakmal Meegahapola",
      "Hamza Hassoune",
      "Daniel Gatica-Perez"
    ],
    "abstract": "Over the years, multimodal mobile sensing has been used extensively for\ninferences regarding health and well being, behavior, and context. However, a\nsignificant challenge hindering the widespread deployment of such models in\nreal world scenarios is the issue of distribution shift. This is the phenomenon\nwhere the distribution of data in the training set differs from the\ndistribution of data in the real world, the deployment environment. While\nextensively explored in computer vision and natural language processing, and\nwhile prior research in mobile sensing briefly addresses this concern, current\nwork primarily focuses on models dealing with a single modality of data, such\nas audio or accelerometer readings, and consequently, there is little research\non unsupervised domain adaptation when dealing with multimodal sensor data. To\naddress this gap, we did extensive experiments with domain adversarial neural\nnetworks (DANN) showing that they can effectively handle distribution shifts in\nmultimodal sensor data. Moreover, we proposed a novel improvement over DANN,\ncalled M3BAT, unsupervised domain adaptation for multimodal mobile sensing with\nmulti-branch adversarial training, to account for the multimodality of sensor\ndata during domain adaptation with multiple branches. Through extensive\nexperiments conducted on two multimodal mobile sensing datasets, three\ninference tasks, and 14 source-target domain pairs, including both regression\nand classification, we demonstrate that our approach performs effectively on\nunseen domains. Compared to directly deploying a model trained in the source\ndomain to the target domain, the model shows performance increases up to 12%\nAUC (area under the receiver operating characteristics curves) on\nclassification tasks, and up to 0.13 MAE (mean absolute error) on regression\ntasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at the Proceedings of the ACM on Interactive, Mobile,\n  Wearable and Ubiquitous Technologies (IMWUT). Paper will be presented at ACM\n  UbiComp 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.17391v1",
    "published_date": "2024-04-26 13:09:35 UTC",
    "updated_date": "2024-04-26 13:09:35 UTC"
  },
  {
    "arxiv_id": "2404.17390v1",
    "title": "How Could AI Support Design Education? A Study Across Fields Fuels Situating Analytics",
    "authors": [
      "Ajit Jain",
      "Andruid Kerne",
      "Hannah Fowler",
      "Jinsil Seo",
      "Galen Newman",
      "Nic Lupfer",
      "Aaron Perrine"
    ],
    "abstract": "We use the process and findings from a case study of design educators'\npractices of assessment and feedback to fuel theorizing about how to make AI\nuseful in service of human experience. We build on Suchman's theory of situated\nactions. We perform a qualitative study of 11 educators in 5 fields, who teach\ndesign processes situated in project-based learning contexts. Through\nqualitative data gathering and analysis, we derive codes: design process;\nassessment and feedback challenges; and computational support.\n  We twice invoke creative cognition's family resemblance principle. First, to\nexplain how design instructors already use assessment rubrics and second, to\nexplain the analogous role for design creativity analytics: no particular trait\nis necessary or sufficient; each only tends to indicate good design work. Human\nteachers remain essential. We develop a set of situated design creativity\nanalytics--Fluency, Flexibility, Visual Consistency, Multiscale Organization,\nand Legible Contrast--to support instructors' efforts, by providing on-demand,\nlearning objectives-based assessment and feedback to students.\n  We theorize a methodology, which we call situating analytics, firstly because\nmaking AI support living human activity depends on aligning what analytics\nmeasure with situated practices. Further, we realize that analytics can become\nmost significant to users by situating them through interfaces that integrate\nthem into the material contexts of their use. Here, this means situating design\ncreativity analytics into actual design environments. Through the case study,\nwe identify situating analytics as a methodology for explaining analytics to\nusers, because the iterative process of alignment with practice has the\npotential to enable data scientists to derive analytics that make sense as part\nof and support situated human experiences.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "H.5.2"
    ],
    "primary_category": "cs.HC",
    "comment": "31 pages, 3 figures, Submitted to ACM",
    "pdf_url": "http://arxiv.org/pdf/2404.17390v1",
    "published_date": "2024-04-26 13:06:52 UTC",
    "updated_date": "2024-04-26 13:06:52 UTC"
  },
  {
    "arxiv_id": "2404.17378v1",
    "title": "Quantum Adjoint Convolutional Layers for Effective Data Representation",
    "authors": [
      "Ren-Xin Zhao",
      "Shi Wang",
      "Yaonan Wang"
    ],
    "abstract": "Quantum Convolutional Layer (QCL) is considered as one of the core of Quantum\nConvolutional Neural Networks (QCNNs) due to its efficient data feature\nextraction capability. However, the current principle of QCL is not as\nmathematically understandable as Classical Convolutional Layer (CCL) due to its\nblack-box structure. Moreover, classical data mapping in many QCLs is\ninefficient. To this end, firstly, the Quantum Adjoint Convolution Operation\n(QACO) consisting of a quantum amplitude encoding and its inverse is\ntheoretically shown to be equivalent to the quantum normalization of the\nconvolution operation based on the Frobenius inner product while achieving an\nefficient characterization of the data. Subsequently, QACO is extended into a\nQuantum Adjoint Convolutional Layer (QACL) by Quantum Phase Estimation (QPE) to\ncompute all Frobenius inner products in parallel. At last, comparative\nsimulation experiments are carried out on PennyLane and TensorFlow platforms,\nmainly for the two cases of kernel fixed and unfixed in QACL. The results\ndemonstrate that QACL with the insight of special quantum properties for the\nsame images, provides higher training accuracy in MNIST and Fashion MNIST\nclassification experiments, but sacrifices the learning performance to some\nextent. Predictably, our research lays the foundation for the development of\nefficient and interpretable quantum convolutional networks and also advances\nthe field of quantum machine vision.",
    "categories": [
      "quant-ph",
      "cs.AI"
    ],
    "primary_category": "quant-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17378v1",
    "published_date": "2024-04-26 12:52:45 UTC",
    "updated_date": "2024-04-26 12:52:45 UTC"
  },
  {
    "arxiv_id": "2404.17369v1",
    "title": "Assessing the Potential of AI for Spatially Sensitive Nature-Related Financial Risks",
    "authors": [
      "Steven Reece",
      "Emma O'Donnell",
      "Felicia Liu",
      "Joanna Wolstenholme",
      "Frida Arriaga",
      "Giacomo Ascenzi",
      "Richard Pywell"
    ],
    "abstract": "There is growing recognition among financial institutions, financial\nregulators and policy makers of the importance of addressing nature-related\nrisks and opportunities. Evaluating and assessing nature-related risks for\nfinancial institutions is challenging due to the large volume of heterogeneous\ndata available on nature and the complexity of investment value chains and the\nvarious components' relationship to nature. The dual problem of scaling data\nanalytics and analysing complex systems can be addressed using Artificial\nIntelligence (AI). We address issues such as plugging existing data gaps with\ndiscovered data, data estimation under uncertainty, time series analysis and\n(near) real-time updates. This report presents potential AI solutions for\nmodels of two distinct use cases, the Brazil Beef Supply Use Case and the Water\nUtility Use Case. Our two use cases cover a broad perspective within\nsustainable finance. The Brazilian cattle farming use case is an example of\ngreening finance - integrating nature-related considerations into mainstream\nfinancial decision-making to transition investments away from sectors with poor\nhistorical track records and unsustainable operations. The deployment of\nnature-based solutions in the UK water utility use case is an example of\nfinancing green - driving investment to nature-positive outcomes. The two use\ncases also cover different sectors, geographies, financial assets and AI\nmodelling techniques, providing an overview on how AI could be applied to\ndifferent challenges relating to nature's integration into finance. This report\nis primarily aimed at financial institutions but is also of interest to ESG\ndata providers, TNFD, systems modellers, and, of course, AI practitioners.",
    "categories": [
      "q-fin.CP",
      "cs.AI"
    ],
    "primary_category": "q-fin.CP",
    "comment": "67 pages, 10 figures, UKRI (NERC) Integrated Finance and Biodiversity\n  for a Nature Positive Future Programme",
    "pdf_url": "http://arxiv.org/pdf/2404.17369v1",
    "published_date": "2024-04-26 12:42:39 UTC",
    "updated_date": "2024-04-26 12:42:39 UTC"
  },
  {
    "arxiv_id": "2404.17365v3",
    "title": "Similarity Equivariant Graph Neural Networks for Homogenization of Metamaterials",
    "authors": [
      "Fleur Hendriks",
      "Vlado Menkovski",
      "Martin Doškář",
      "Marc G. D. Geers",
      "Ondřej Rokoš"
    ],
    "abstract": "Soft, porous mechanical metamaterials exhibit pattern transformations that\nmay have important applications in soft robotics, sound reduction and\nbiomedicine. To design these innovative materials, it is important to be able\nto simulate them accurately and quickly, in order to tune their mechanical\nproperties. Since conventional simulations using the finite element method\nentail a high computational cost, in this article we aim to develop a machine\nlearning-based approach that scales favorably to serve as a surrogate model. To\nensure that the model is also able to handle various microstructures, including\nthose not encountered during training, we include the microstructure as part of\nthe network input. Therefore, we introduce a graph neural network that predicts\nglobal quantities (energy, stress stiffness) as well as the pattern\ntransformations that occur (the kinematics). To make our model as accurate and\ndata-efficient as possible, various symmetries are incorporated into the model.\nThe starting point is an E(n)-equivariant graph neural network (which respects\ntranslation, rotation and reflection) that has periodic boundary conditions\n(i.e., it is in-/equivariant with respect to the choice of RVE), is scale\nin-/equivariant, can simulate large deformations, and can predict scalars,\nvectors as well as second and fourth order tensors (specifically energy, stress\nand stiffness). The incorporation of scale equivariance makes the model\nequivariant with respect to the similarities group, of which the Euclidean\ngroup E(n) is a subgroup. We show that this network is more accurate and\ndata-efficient than graph neural networks with fewer symmetries. To create an\nefficient graph representation of the finite element discretization, we use\nonly the internal geometrical hole boundaries from the finite element mesh to\nachieve a better speed-up and scaling with the mesh size.",
    "categories": [
      "cond-mat.soft",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cond-mat.soft",
    "comment": "60 pages, 22 figures. Published in CMAME (Computer Methods in Applied\n  Mechanics and Engineering)",
    "pdf_url": "http://arxiv.org/pdf/2404.17365v3",
    "published_date": "2024-04-26 12:30:32 UTC",
    "updated_date": "2025-03-13 14:48:27 UTC"
  },
  {
    "arxiv_id": "2405.00722v2",
    "title": "LLMs for Generating and Evaluating Counterfactuals: A Comprehensive Study",
    "authors": [
      "Van Bach Nguyen",
      "Paul Youssef",
      "Christin Seifert",
      "Jörg Schlötterer"
    ],
    "abstract": "As NLP models become more complex, understanding their decisions becomes more\ncrucial. Counterfactuals (CFs), where minimal changes to inputs flip a model's\nprediction, offer a way to explain these models. While Large Language Models\n(LLMs) have shown remarkable performance in NLP tasks, their efficacy in\ngenerating high-quality CFs remains uncertain. This work fills this gap by\ninvestigating how well LLMs generate CFs for two NLU tasks. We conduct a\ncomprehensive comparison of several common LLMs, and evaluate their CFs,\nassessing both intrinsic metrics, and the impact of these CFs on data\naugmentation. Moreover, we analyze differences between human and LLM-generated\nCFs, providing insights for future research directions. Our results show that\nLLMs generate fluent CFs, but struggle to keep the induced changes minimal.\nGenerating CFs for Sentiment Analysis (SA) is less challenging than NLI where\nLLMs show weaknesses in generating CFs that flip the original label. This also\nreflects on the data augmentation performance, where we observe a large gap\nbetween augmenting with human and LLMs CFs. Furthermore, we evaluate LLMs'\nability to assess CFs in a mislabelled data setting, and show that they have a\nstrong bias towards agreeing with the provided labels. GPT4 is more robust\nagainst this bias and its scores correlate well with automatic metrics. Our\nfindings reveal several limitations and point to potential future work\ndirections.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP Findings 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.00722v2",
    "published_date": "2024-04-26 11:57:21 UTC",
    "updated_date": "2024-11-12 11:49:33 UTC"
  },
  {
    "arxiv_id": "2404.17617v1",
    "title": "Beyond Traditional Threats: A Persistent Backdoor Attack on Federated Learning",
    "authors": [
      "Tao Liu",
      "Yuhang Zhang",
      "Zhu Feng",
      "Zhiqin Yang",
      "Chen Xu",
      "Dapeng Man",
      "Wu Yang"
    ],
    "abstract": "Backdoors on federated learning will be diluted by subsequent benign updates.\nThis is reflected in the significant reduction of attack success rate as\niterations increase, ultimately failing. We use a new metric to quantify the\ndegree of this weakened backdoor effect, called attack persistence. Given that\nresearch to improve this performance has not been widely noted,we propose a\nFull Combination Backdoor Attack (FCBA) method. It aggregates more combined\ntrigger information for a more complete backdoor pattern in the global model.\nTrained backdoored global model is more resilient to benign updates, leading to\na higher attack success rate on the test set. We test on three datasets and\nevaluate with two models across various settings. FCBA's persistence\noutperforms SOTA federated learning backdoor attacks. On GTSRB, postattack 120\nrounds, our attack success rate rose over 50% from baseline. The core code of\nour method is available at https://github.com/PhD-TaoLiu/FCBA.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17617v1",
    "published_date": "2024-04-26 11:47:36 UTC",
    "updated_date": "2024-04-26 11:47:36 UTC"
  },
  {
    "arxiv_id": "2404.17342v2",
    "title": "From Multiple-Choice to Extractive QA: A Case Study for English and Arabic",
    "authors": [
      "Teresa Lynn",
      "Malik H. Altakrori",
      "Samar Mohamed Magdy",
      "Rocktim Jyoti Das",
      "Chenyang Lyu",
      "Mohamed Nasr",
      "Younes Samih",
      "Kirill Chirkunov",
      "Alham Fikri Aji",
      "Preslav Nakov",
      "Shantanu Godbole",
      "Salim Roukos",
      "Radu Florian",
      "Nizar Habash"
    ],
    "abstract": "The rapid evolution of Natural Language Processing (NLP) has favoured major\nlanguages such as English, leaving a significant gap for many others due to\nlimited resources. This is especially evident in the context of data\nannotation, a task whose importance cannot be underestimated, but which is\ntime-consuming and costly. Thus, any dataset for resource-poor languages is\nprecious, in particular when it is task-specific. Here, we explore the\nfeasibility of repurposing an existing multilingual dataset for a new NLP task:\nwe repurpose a subset of the BELEBELE dataset (Bandarkar et al., 2023), which\nwas designed for multiple-choice question answering (MCQA), to enable the more\npractical task of extractive QA (EQA) in the style of machine reading\ncomprehension. We present annotation guidelines and a parallel EQA dataset for\nEnglish and Modern Standard Arabic (MSA). We also present QA evaluation results\nfor several monolingual and cross-lingual QA pairs including English, MSA, and\nfive Arabic dialects. We aim to help others adapt our approach for the\nremaining 120 BELEBELE language variants, many of which are deemed\nunder-resourced. We also provide a thorough analysis and share insights to\ndeepen understanding of the challenges and opportunities in NLP task\nreformulation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Paper 8 pages, Appendix 12 pages. Published at COLING2025",
    "pdf_url": "http://arxiv.org/pdf/2404.17342v2",
    "published_date": "2024-04-26 11:46:05 UTC",
    "updated_date": "2025-01-24 05:18:29 UTC"
  },
  {
    "arxiv_id": "2404.17336v1",
    "title": "Introducing cosmosGPT: Monolingual Training for Turkish Language Models",
    "authors": [
      "H. Toprak Kesgin",
      "M. Kaan Yuce",
      "Eren Dogan",
      "M. Egemen Uzun",
      "Atahan Uz",
      "H. Emre Seyrek",
      "Ahmed Zeer",
      "M. Fatih Amasyali"
    ],
    "abstract": "The number of open source language models that can produce Turkish is\nincreasing day by day, as in other languages. In order to create the basic\nversions of such models, the training of multilingual models is usually\ncontinued with Turkish corpora. The alternative is to train the model with only\nTurkish corpora. In this study, we first introduce the cosmosGPT models that we\ncreated with this alternative method. Then, we introduce new finetune datasets\nfor basic language models to fulfill user requests and new evaluation datasets\nfor measuring the capabilities of Turkish language models. Finally, a\ncomprehensive comparison of the adapted Turkish language models on different\ncapabilities is presented. The results show that the language models we built\nwith the monolingual corpus have promising performance despite being about 10\ntimes smaller than the others.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17336v1",
    "published_date": "2024-04-26 11:34:11 UTC",
    "updated_date": "2024-04-26 11:34:11 UTC"
  },
  {
    "arxiv_id": "2404.17335v3",
    "title": "A Novel Spike Transformer Network for Depth Estimation from Event Cameras via Cross-modality Knowledge Distillation",
    "authors": [
      "Xin Zhang",
      "Liangxiu Han",
      "Tam Sobeih",
      "Lianghao Han",
      "Darren Dancey"
    ],
    "abstract": "Depth estimation is a critical task in computer vision, with applications in\nautonomous navigation, robotics, and augmented reality. Event cameras, which\nencode temporal changes in light intensity as asynchronous binary spikes, offer\nunique advantages such as low latency, high dynamic range, and energy\nefficiency. However, their unconventional spiking output and the scarcity of\nlabelled datasets pose significant challenges to traditional image-based depth\nestimation methods. To address these challenges, we propose a novel\nenergy-efficient Spike-Driven Transformer Network (SDT) for depth estimation,\nleveraging the unique properties of spiking data. The proposed SDT introduces\nthree key innovations: (1) a purely spike-driven transformer architecture that\nincorporates spike-based attention and residual mechanisms, enabling precise\ndepth estimation with minimal energy consumption; (2) a fusion depth estimation\nhead that combines multi-stage features for fine-grained depth prediction while\nensuring computational efficiency; and (3) a cross-modality knowledge\ndistillation framework that utilises a pre-trained vision foundation model\n(DINOv2) to enhance the training of the spiking network despite limited data\navailability.This work represents the first exploration of transformer-based\nspiking neural networks for depth estimation, providing a significant step\nforward in energy-efficient neuromorphic computing for real-world vision\napplications.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "16 pages",
    "pdf_url": "http://arxiv.org/pdf/2404.17335v3",
    "published_date": "2024-04-26 11:32:53 UTC",
    "updated_date": "2025-02-24 10:47:58 UTC"
  },
  {
    "arxiv_id": "2404.17323v1",
    "title": "A Deep Dive into Effects of Structural Bias on CMA-ES Performance along Affine Trajectories",
    "authors": [
      "Niki van Stein",
      "Sarah L. Thomson",
      "Anna V. Kononova"
    ],
    "abstract": "To guide the design of better iterative optimisation heuristics, it is\nimperative to understand how inherent structural biases within algorithm\ncomponents affect the performance on a wide variety of search landscapes. This\nstudy explores the impact of structural bias in the modular Covariance Matrix\nAdaptation Evolution Strategy (modCMA), focusing on the roles of various\nmodulars within the algorithm. Through an extensive investigation involving\n435,456 configurations of modCMA, we identified key modules that significantly\ninfluence structural bias of various classes. Our analysis utilized the\nDeep-BIAS toolbox for structural bias detection and classification,\ncomplemented by SHAP analysis for quantifying module contributions. The\nperformance of these configurations was tested on a sequence of\naffine-recombined functions, maintaining fixed optimum locations while\ngradually varying the landscape features. Our results demonstrate an interplay\nbetween module-induced structural bias and algorithm performance across\ndifferent landscape characteristics.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "15 pages, 5 figures, submitted to PPSN 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.17323v1",
    "published_date": "2024-04-26 11:07:09 UTC",
    "updated_date": "2024-04-26 11:07:09 UTC"
  },
  {
    "arxiv_id": "2404.17316v1",
    "title": "Certified MaxSAT Preprocessing",
    "authors": [
      "Hannes Ihalainen",
      "Andy Oertel",
      "Yong Kiam Tan",
      "Jeremias Berg",
      "Matti Järvisalo",
      "Jakob Nordström"
    ],
    "abstract": "Building on the progress in Boolean satisfiability (SAT) solving over the\nlast decades, maximum satisfiability (MaxSAT) has become a viable approach for\nsolving NP-hard optimization problems, but ensuring correctness of MaxSAT\nsolvers has remained an important concern. For SAT, this is largely a solved\nproblem thanks to the use of proof logging, meaning that solvers emit\nmachine-verifiable proofs of (un)satisfiability to certify correctness.\nHowever, for MaxSAT, proof logging solvers have started being developed only\nvery recently. Moreover, these nascent efforts have only targeted the core\nsolving process, ignoring the preprocessing phase where input problem instances\ncan be substantially reformulated before being passed on to the solver proper.\nIn this work, we demonstrate how pseudo-Boolean proof logging can be used to\ncertify the correctness of a wide range of modern MaxSAT preprocessing\ntechniques. By combining and extending the VeriPB and CakePB tools, we provide\nformally verified, end-to-end proof checking that the input and preprocessed\noutput MaxSAT problem instances have the same optimal value. An extensive\nevaluation on applied MaxSAT benchmarks shows that our approach is feasible in\npractice.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17316v1",
    "published_date": "2024-04-26 10:55:06 UTC",
    "updated_date": "2024-04-26 10:55:06 UTC"
  },
  {
    "arxiv_id": "2404.17302v1",
    "title": "Part-Guided 3D RL for Sim2Real Articulated Object Manipulation",
    "authors": [
      "Pengwei Xie",
      "Rui Chen",
      "Siang Chen",
      "Yuzhe Qin",
      "Fanbo Xiang",
      "Tianyu Sun",
      "Jing Xu",
      "Guijin Wang",
      "Hao Su"
    ],
    "abstract": "Manipulating unseen articulated objects through visual feedback is a critical\nbut challenging task for real robots. Existing learning-based solutions mainly\nfocus on visual affordance learning or other pre-trained visual models to guide\nmanipulation policies, which face challenges for novel instances in real-world\nscenarios. In this paper, we propose a novel part-guided 3D RL framework, which\ncan learn to manipulate articulated objects without demonstrations. We combine\nthe strengths of 2D segmentation and 3D RL to improve the efficiency of RL\npolicy training. To improve the stability of the policy on real robots, we\ndesign a Frame-consistent Uncertainty-aware Sampling (FUS) strategy to get a\ncondensed and hierarchical 3D representation. In addition, a single versatile\nRL policy can be trained on multiple articulated object manipulation tasks\nsimultaneously in simulation and shows great generalizability to novel\ncategories and instances. Experimental results demonstrate the effectiveness of\nour framework in both simulation and real-world settings. Our code is available\nat\nhttps://github.com/THU-VCLab/Part-Guided-3D-RL-for-Sim2Real-Articulated-Object-Manipulation.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "9 pages",
    "pdf_url": "http://arxiv.org/pdf/2404.17302v1",
    "published_date": "2024-04-26 10:18:17 UTC",
    "updated_date": "2024-04-26 10:18:17 UTC"
  },
  {
    "arxiv_id": "2406.14566v1",
    "title": "LM-IGTD: a 2D image generator for low-dimensional and mixed-type tabular data to leverage the potential of convolutional neural networks",
    "authors": [
      "Vanesa Gómez-Martínez",
      "Francisco J. Lara-Abelenda",
      "Pablo Peiro-Corbacho",
      "David Chushig-Muzo",
      "Conceicao Granja",
      "Cristina Soguero-Ruiz"
    ],
    "abstract": "Tabular data have been extensively used in different knowledge domains.\nConvolutional neural networks (CNNs) have been successfully used in many\napplications where important information about data is embedded in the order of\nfeatures (images), outperforming predictive results of traditional models.\nRecently, several researchers have proposed transforming tabular data into\nimages to leverage the potential of CNNs and obtain high results in predictive\ntasks such as classification and regression. In this paper, we present a novel\nand effective approach for transforming tabular data into images, addressing\nthe inherent limitations associated with low-dimensional and mixed-type\ndatasets. Our method, named Low Mixed-Image Generator for Tabular Data\n(LM-IGTD), integrates a stochastic feature generation process and a modified\nversion of the IGTD. We introduce an automatic and interpretable end-to-end\npipeline, enabling the creation of images from tabular data. A mapping between\noriginal features and the generated images is established, and post hoc\ninterpretability methods are employed to identify crucial areas of these\nimages, enhancing interpretability for predictive tasks. An extensive\nevaluation of the tabular-to-image generation approach proposed on 12\nlow-dimensional and mixed-type datasets, including binary and multi-class\nclassification scenarios. In particular, our method outperformed all\ntraditional ML models trained on tabular data in five out of twelve datasets\nwhen using images generated with LM-IGTD and CNN. In the remaining datasets,\nLM-IGTD images and CNN consistently surpassed three out of four traditional ML\nmodels, achieving similar results to the fourth model.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.14566v1",
    "published_date": "2024-04-26 09:52:39 UTC",
    "updated_date": "2024-04-26 09:52:39 UTC"
  },
  {
    "arxiv_id": "2404.17276v2",
    "title": "Efficient Deterministic Renewable Energy Forecasting Guided by Multiple-Location Weather Data",
    "authors": [
      "Charalampos Symeonidis",
      "Nikos Nikolaidis"
    ],
    "abstract": "Electricity generated from renewable energy sources has been established as\nan efficient remedy for both energy shortages and the environmental pollution\nstemming from conventional energy production methods. Solar and wind power are\ntwo of the most dominant renewable energy sources. The accurate forecasting of\nthe energy generation of those sources facilitates their integration into\nelectric grids, by minimizing the negative impact of uncertainty regarding\ntheir management and operation. This paper proposes a novel methodology for\ndeterministic wind and solar energy generation forecasting for multiple\ngeneration sites, utilizing multi-location weather forecasts. The method\nemploys a U-shaped Temporal Convolutional Auto-Encoder (UTCAE) architecture for\ntemporal processing of weather-related and energy-related time-series across\neach site. The Multi-sized Kernels convolutional Spatio-Temporal Attention\n(MKST-Attention), inspired by the multi-head scaled-dot product attention\nmechanism, is also proposed aiming to efficiently transfer temporal patterns\nfrom weather data to energy data, without a priori knowledge of the locations\nof the power stations and the locations of provided weather data. The conducted\nexperimental evaluation on a day-ahead solar and wind energy forecasting\nscenario on five datasets demonstrated that the proposed method achieves top\nresults, outperforming all competitive time-series forecasting state-of-the-art\nmethods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this article is\n  published in Neural Computing and Applications, and is available online at\n  https://doi.org/10.1007/s00521-024-10607-2",
    "pdf_url": "http://arxiv.org/pdf/2404.17276v2",
    "published_date": "2024-04-26 09:30:55 UTC",
    "updated_date": "2025-01-03 09:54:37 UTC"
  },
  {
    "arxiv_id": "2405.09543v1",
    "title": "Algorithmic Fairness: A Tolerance Perspective",
    "authors": [
      "Renqiang Luo",
      "Tao Tang",
      "Feng Xia",
      "Jiaying Liu",
      "Chengpei Xu",
      "Leo Yu Zhang",
      "Wei Xiang",
      "Chengqi Zhang"
    ],
    "abstract": "Recent advancements in machine learning and deep learning have brought\nalgorithmic fairness into sharp focus, illuminating concerns over\ndiscriminatory decision making that negatively impacts certain individuals or\ngroups. These concerns have manifested in legal, ethical, and societal\nchallenges, including the erosion of trust in intelligent systems. In response,\nthis survey delves into the existing literature on algorithmic fairness,\nspecifically highlighting its multifaceted social consequences. We introduce a\nnovel taxonomy based on 'tolerance', a term we define as the degree to which\nvariations in fairness outcomes are acceptable, providing a structured approach\nto understanding the subtleties of fairness within algorithmic decisions. Our\nsystematic review covers diverse industries, revealing critical insights into\nthe balance between algorithmic decision making and social equity. By\nsynthesizing these insights, we outline a series of emerging challenges and\npropose strategic directions for future research and policy making, with the\ngoal of advancing the field towards more equitable algorithmic systems.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.IR",
      "cs.LG",
      "68T01, 68W40",
      "I.2.6; K.4.2; H.1.2"
    ],
    "primary_category": "cs.CY",
    "comment": "33 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.09543v1",
    "published_date": "2024-04-26 08:16:54 UTC",
    "updated_date": "2024-04-26 08:16:54 UTC"
  },
  {
    "arxiv_id": "2404.17225v1",
    "title": "Enhancing Privacy and Security of Autonomous UAV Navigation",
    "authors": [
      "Vatsal Aggarwal",
      "Arjun Ramesh Kaushik",
      "Charanjit Jutla",
      "Nalini Ratha"
    ],
    "abstract": "Autonomous Unmanned Aerial Vehicles (UAVs) have become essential tools in\ndefense, law enforcement, disaster response, and product delivery. These\nautonomous navigation systems require a wireless communication network, and of\nlate are deep learning based. In critical scenarios such as border protection\nor disaster response, ensuring the secure navigation of autonomous UAVs is\nparamount. But, these autonomous UAVs are susceptible to adversarial attacks\nthrough the communication network or the deep learning models - eavesdropping /\nman-in-the-middle / membership inference / reconstruction. To address this\nsusceptibility, we propose an innovative approach that combines Reinforcement\nLearning (RL) and Fully Homomorphic Encryption (FHE) for secure autonomous UAV\nnavigation. This end-to-end secure framework is designed for real-time video\nfeeds captured by UAV cameras and utilizes FHE to perform inference on\nencrypted input images. While FHE allows computations on encrypted data,\ncertain computational operators are yet to be implemented. Convolutional neural\nnetworks, fully connected neural networks, activation functions and OpenAI Gym\nLibrary are meticulously adapted to the FHE domain to enable encrypted data\nprocessing. We demonstrate the efficacy of our proposed approach through\nextensive experimentation. Our proposed approach ensures security and privacy\nin autonomous UAV navigation with negligible loss in performance.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17225v1",
    "published_date": "2024-04-26 07:54:04 UTC",
    "updated_date": "2024-04-26 07:54:04 UTC"
  },
  {
    "arxiv_id": "2404.17196v1",
    "title": "Human-Imperceptible Retrieval Poisoning Attacks in LLM-Powered Applications",
    "authors": [
      "Quan Zhang",
      "Binqi Zeng",
      "Chijin Zhou",
      "Gwihwan Go",
      "Heyuan Shi",
      "Yu Jiang"
    ],
    "abstract": "Presently, with the assistance of advanced LLM application development\nframeworks, more and more LLM-powered applications can effortlessly augment the\nLLMs' knowledge with external content using the retrieval augmented generation\n(RAG) technique. However, these frameworks' designs do not have sufficient\nconsideration of the risk of external content, thereby allowing attackers to\nundermine the applications developed with these frameworks. In this paper, we\nreveal a new threat to LLM-powered applications, termed retrieval poisoning,\nwhere attackers can guide the application to yield malicious responses during\nthe RAG process. Specifically, through the analysis of LLM application\nframeworks, attackers can craft documents visually indistinguishable from\nbenign ones. Despite the documents providing correct information, once they are\nused as reference sources for RAG, the application is misled into generating\nincorrect responses. Our preliminary experiments indicate that attackers can\nmislead LLMs with an 88.33\\% success rate, and achieve a 66.67\\% success rate\nin the real-world application, demonstrating the potential impact of retrieval\npoisoning.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17196v1",
    "published_date": "2024-04-26 07:11:18 UTC",
    "updated_date": "2024-04-26 07:11:18 UTC"
  },
  {
    "arxiv_id": "2404.17186v1",
    "title": "MCSDNet: Mesoscale Convective System Detection Network via Multi-scale Spatiotemporal Information",
    "authors": [
      "Jiajun Liang",
      "Baoquan Zhang",
      "Yunming Ye",
      "Xutao Li",
      "Chuyao Luo",
      "Xukai Fu"
    ],
    "abstract": "The accurate detection of Mesoscale Convective Systems (MCS) is crucial for\nmeteorological monitoring due to their potential to cause significant\ndestruction through severe weather phenomena such as hail, thunderstorms, and\nheavy rainfall. However, the existing methods for MCS detection mostly targets\non single-frame detection, which just considers the static characteristics and\nignores the temporal evolution in the life cycle of MCS. In this paper, we\npropose a novel encoder-decoder neural network for MCS detection(MCSDNet).\nMCSDNet has a simple architecture and is easy to expand. Different from the\nprevious models, MCSDNet targets on multi-frames detection and leverages\nmulti-scale spatiotemporal information for the detection of MCS regions in\nremote sensing imagery(RSI). As far as we know, it is the first work to utilize\nmulti-scale spatiotemporal information to detect MCS regions. Firstly, we\ndesign a multi-scale spatiotemporal information module to extract multi-level\nsemantic from different encoder levels, which makes our models can extract more\ndetail spatiotemporal features. Secondly, a Spatiotemporal Mix Unit(STMU) is\nintroduced to MCSDNet to capture both intra-frame features and inter-frame\ncorrelations, which is a scalable module and can be replaced by other\nspatiotemporal module, e.g., CNN, RNN, Transformer and our proposed Dual\nSpatiotemporal Attention(DSTA). This means that the future works about\nspatiotemporal modules can be easily integrated to our model. Finally, we\npresent MCSRSI, the first publicly available dataset for multi-frames MCS\ndetection based on visible channel images from the FY-4A satellite. We also\nconduct several experiments on MCSRSI and find that our proposed MCSDNet\nachieve the best performance on MCS detection task when comparing to other\nbaseline methods.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17186v1",
    "published_date": "2024-04-26 06:40:54 UTC",
    "updated_date": "2024-04-26 06:40:54 UTC"
  },
  {
    "arxiv_id": "2404.17611v1",
    "title": "MetaSD: A Unified Framework for Scalable Downscaling of Meteorological Variables in Diverse Situations",
    "authors": [
      "Jing Hu",
      "Honghu Zhang",
      "Peng Zheng",
      "Jialin Mu",
      "Xiaomeng Huang",
      "Xi Wu"
    ],
    "abstract": "Addressing complex meteorological processes at a fine spatial resolution\nrequires substantial computational resources. To accelerate meteorological\nsimulations, researchers have utilized neural networks to downscale\nmeteorological variables from low-resolution simulations. Despite notable\nadvancements, contemporary cutting-edge downscaling algorithms tailored to\nspecific variables. Addressing meteorological variables in isolation overlooks\ntheir interconnectedness, leading to an incomplete understanding of atmospheric\ndynamics. Additionally, the laborious processes of data collection, annotation,\nand computational resources required for individual variable downscaling are\nsignificant hurdles. Given the limited versatility of existing models across\ndifferent meteorological variables and their failure to account for\ninter-variable relationships, this paper proposes a unified downscaling\napproach leveraging meta-learning. This framework aims to facilitate the\ndownscaling of diverse meteorological variables derived from various numerical\nmodels and spatiotemporal scales. Trained at variables consisted of\ntemperature, wind, surface pressure and total precipitation from ERA5 and GFS,\nthe proposed method can be extended to downscale convective precipitation,\npotential energy, height, humidity and ozone from CFS, S2S and CMIP6 at\ndifferent spatiotemporal scales, which demonstrating its capability to capture\nthe interconnections among diverse variables. Our approach represents the\ninitial effort to create a generalized downscaling model. Experimental evidence\ndemonstrates that the proposed model outperforms existing top downscaling\nmethods in both quantitative and qualitative assessments.",
    "categories": [
      "physics.ao-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "physics.ao-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17611v1",
    "published_date": "2024-04-26 06:31:44 UTC",
    "updated_date": "2024-04-26 06:31:44 UTC"
  },
  {
    "arxiv_id": "2404.17183v1",
    "title": "Prevalent Frequency of Emotional and Physical Symptoms in Social Anxiety using Zero Shot Classification: An Observational Study",
    "authors": [
      "Muhammad Rizwan",
      "Jure Demšar"
    ],
    "abstract": "Social anxiety represents a prevalent challenge in modern society, affecting\nindividuals across personal and professional spheres. Left unaddressed, this\ncondition can yield substantial negative consequences, impacting social\ninteractions and performance. Further understanding its diverse physical and\nemotional symptoms becomes pivotal for comprehensive diagnosis and tailored\ntherapeutic interventions. This study analyze prevalence and frequency of\nsocial anxiety symptoms taken from Mayo Clinic, exploring diverse human\nexperiences from utilizing a large Reddit dataset dedicated to this issue.\nLeveraging these platforms, the research aims to extract insights and examine a\nspectrum of physical and emotional symptoms linked to social anxiety disorder.\nUpholding ethical considerations, the study maintains strict user anonymity\nwithin the dataset. By employing a novel approach, the research utilizes\nBART-based multi-label zero-shot classification to identify and measure symptom\nprevalence and significance in the form of probability score for each symptom\nunder consideration. Results uncover distinctive patterns: \"Trembling\" emerges\nas a prevalent physical symptom, while emotional symptoms like \"Fear of being\njudged negatively\" exhibit high frequencies. These findings offer insights into\nthe multifaceted nature of social anxiety, aiding clinical practices and\ninterventions tailored to its diverse expressions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17183v1",
    "published_date": "2024-04-26 06:28:51 UTC",
    "updated_date": "2024-04-26 06:28:51 UTC"
  },
  {
    "arxiv_id": "2405.02332v2",
    "title": "Efficient Exploration of Image Classifier Failures with Bayesian Optimization and Text-to-Image Models",
    "authors": [
      "Adrien LeCoz",
      "Houssem Ouertatani",
      "Stéphane Herbin",
      "Faouzi Adjed"
    ],
    "abstract": "Image classifiers should be used with caution in the real world. Performance\nevaluated on a validation set may not reflect performance in the real world. In\nparticular, classifiers may perform well for conditions that are frequently\nencountered during training, but poorly for other infrequent conditions. In\nthis study, we hypothesize that recent advances in text-to-image generative\nmodels make them valuable for benchmarking computer vision models such as image\nclassifiers: they can generate images conditioned by textual prompts that cause\nclassifier failures, allowing failure conditions to be described with textual\nattributes. However, their generation cost becomes an issue when a large number\nof synthetic images need to be generated, which is the case when many different\nattribute combinations need to be tested. We propose an image classifier\nbenchmarking method as an iterative process that alternates image generation,\nclassifier evaluation, and attribute selection. This method efficiently\nexplores the attributes that ultimately lead to poor behavior detection.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.02332v2",
    "published_date": "2024-04-26 06:22:43 UTC",
    "updated_date": "2024-09-27 09:21:03 UTC"
  },
  {
    "arxiv_id": "2404.17173v1",
    "title": "Exploring Beyond Logits: Hierarchical Dynamic Labeling Based on Embeddings for Semi-Supervised Classification",
    "authors": [
      "Yanbiao Ma",
      "Licheng Jiao",
      "Fang Liu",
      "Lingling Li",
      "Shuyuan Yang",
      "Xu Liu"
    ],
    "abstract": "In semi-supervised learning, methods that rely on confidence learning to\ngenerate pseudo-labels have been widely proposed. However, increasing research\nfinds that when faced with noisy and biased data, the model's representation\nnetwork is more reliable than the classification network. Additionally, label\ngeneration methods based on model predictions often show poor adaptability\nacross different datasets, necessitating customization of the classification\nnetwork. Therefore, we propose a Hierarchical Dynamic Labeling (HDL) algorithm\nthat does not depend on model predictions and utilizes image embeddings to\ngenerate sample labels. We also introduce an adaptive method for selecting\nhyperparameters in HDL, enhancing its versatility. Moreover, HDL can be\ncombined with general image encoders (e.g., CLIP) to serve as a fundamental\ndata processing module. We extract embeddings from datasets with class-balanced\nand long-tailed distributions using pre-trained semi-supervised models.\nSubsequently, samples are re-labeled using HDL, and the re-labeled samples are\nused to further train the semi-supervised models. Experiments demonstrate\nimproved model performance, validating the motivation that representation\nnetworks are more reliable than classifiers or predictors. Our approach has the\npotential to change the paradigm of pseudo-label generation in semi-supervised\nlearning.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17173v1",
    "published_date": "2024-04-26 06:00:27 UTC",
    "updated_date": "2024-04-26 06:00:27 UTC"
  },
  {
    "arxiv_id": "2406.15391v1",
    "title": "Examining the Legal Status of Digital Assets as Property: A Comparative Analysis of Jurisdictional Approaches",
    "authors": [
      "Luke Lee"
    ],
    "abstract": "This paper examines the complex legal landscape surrounding digital assets,\nanalysing how they are defined and regulated as property across various\njurisdictions. As digital assets such as cryptocurrencies and non-fungible\ntokens (NFTs) increasingly integrate with global economies, their intangible\nnature presents unique challenges to traditional property law concepts,\nnecessitating a re-evaluation of legal definitions and ownership frameworks.\nThis research presents a comparative analysis, reviewing how different legal\nsystems classify and manage digital assets within property law, highlighting\nthe variations in regulatory approaches and their implications on ownership,\ntransfer, and inheritance rights. By examining seminal cases and regulatory\ndevelopments in major jurisdictions, including the United States, the European\nUnion, and Singapore, this paper explores the emerging trends and potential\nlegal evolutions that could influence the global handling of digital assets.\nThe study aims to contribute to the scholarly discourse by proposing a\nharmonized approach to digital asset regulation, seeking to balance innovation\nwith legal certainty and consumer protection.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.CY",
    "comment": "16 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.15391v1",
    "published_date": "2024-04-26 04:22:30 UTC",
    "updated_date": "2024-04-26 04:22:30 UTC"
  },
  {
    "arxiv_id": "2404.17136v1",
    "title": "Automated Data Visualization from Natural Language via Large Language Models: An Exploratory Study",
    "authors": [
      "Yang Wu",
      "Yao Wan",
      "Hongyu Zhang",
      "Yulei Sui",
      "Wucai Wei",
      "Wei Zhao",
      "Guandong Xu",
      "Hai Jin"
    ],
    "abstract": "The Natural Language to Visualization (NL2Vis) task aims to transform\nnatural-language descriptions into visual representations for a grounded table,\nenabling users to gain insights from vast amounts of data. Recently, many deep\nlearning-based approaches have been developed for NL2Vis. Despite the\nconsiderable efforts made by these approaches, challenges persist in\nvisualizing data sourced from unseen databases or spanning multiple tables.\nTaking inspiration from the remarkable generation capabilities of Large\nLanguage Models (LLMs), this paper conducts an empirical study to evaluate\ntheir potential in generating visualizations, and explore the effectiveness of\nin-context learning prompts for enhancing this task. In particular, we first\nexplore the ways of transforming structured tabular data into sequential text\nprompts, as to feed them into LLMs and analyze which table content contributes\nmost to the NL2Vis. Our findings suggest that transforming structured tabular\ndata into programs is effective, and it is essential to consider the table\nschema when formulating prompts. Furthermore, we evaluate two types of LLMs:\nfinetuned models (e.g., T5-Small) and inference-only models (e.g., GPT-3.5),\nagainst state-of-the-art methods, using the NL2Vis benchmarks (i.e., nvBench).\nThe experimental results reveal that LLMs outperform baselines, with\ninference-only models consistently exhibiting performance improvements, at\ntimes even surpassing fine-tuned models when provided with certain few-shot\ndemonstrations through in-context learning. Finally, we analyze when the LLMs\nfail in NL2Vis, and propose to iteratively update the results using strategies\nsuch as chain-of-thought, role-playing, and code-interpreter. The experimental\nresults confirm the efficacy of iterative updates and hold great potential for\nfuture study.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17136v1",
    "published_date": "2024-04-26 03:25:35 UTC",
    "updated_date": "2024-04-26 03:25:35 UTC"
  },
  {
    "arxiv_id": "2404.17129v3",
    "title": "Process Mining Embeddings: Learning Vector Representations for Petri Nets",
    "authors": [
      "Juan G. Colonna",
      "Ahmed A. Fares",
      "Márcio Duarte",
      "Ricardo Sousa"
    ],
    "abstract": "Process Mining offers a powerful framework for uncovering, analyzing, and\noptimizing real-world business processes. Petri nets provide a versatile means\nof modeling process behavior. However, traditional methods often struggle to\neffectively compare complex Petri nets, hindering their potential for process\nenhancement. To address this challenge, we introduce PetriNet2Vec, an\nunsupervised methodology inspired by Doc2Vec. This approach converts Petri nets\ninto embedding vectors, facilitating the comparison, clustering, and\nclassification of process models. We validated our approach using the PDC\nDataset, comprising 96 diverse Petri net models. The results demonstrate that\nPetriNet2Vec effectively captures the structural properties of process models,\nenabling accurate process classification and efficient process retrieval.\nSpecifically, our findings highlight the utility of the learned embeddings in\ntwo key downstream tasks: process classification and process retrieval. In\nprocess classification, the embeddings allowed for accurate categorization of\nprocess models based on their structural properties. In process retrieval, the\nembeddings enabled efficient retrieval of similar process models using cosine\ndistance. These results demonstrate the potential of PetriNet2Vec to\nsignificantly enhance process mining capabilities.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17129v3",
    "published_date": "2024-04-26 03:07:32 UTC",
    "updated_date": "2024-07-31 17:19:34 UTC"
  },
  {
    "arxiv_id": "2404.17126v2",
    "title": "Deep Evidential Learning for Radiotherapy Dose Prediction",
    "authors": [
      "Hai Siong Tan",
      "Kuancheng Wang",
      "Rafe Mcbeth"
    ],
    "abstract": "In this work, we present a novel application of an uncertainty-quantification\nframework called Deep Evidential Learning in the domain of radiotherapy dose\nprediction. Using medical images of the Open Knowledge-Based Planning Challenge\ndataset, we found that this model can be effectively harnessed to yield\nuncertainty estimates that inherited correlations with prediction errors upon\ncompletion of network training. This was achieved only after reformulating the\noriginal loss function for a stable implementation. We found that (i)epistemic\nuncertainty was highly correlated with prediction errors, with various\nassociation indices comparable or stronger than those for Monte-Carlo Dropout\nand Deep Ensemble methods, (ii)the median error varied with uncertainty\nthreshold much more linearly for epistemic uncertainty in Deep Evidential\nLearning relative to these other two conventional frameworks, indicative of a\nmore uniformly calibrated sensitivity to model errors, (iii)relative to\nepistemic uncertainty, aleatoric uncertainty demonstrated a more significant\nshift in its distribution in response to Gaussian noise added to CT intensity,\ncompatible with its interpretation as reflecting data noise. Collectively, our\nresults suggest that Deep Evidential Learning is a promising approach that can\nendow deep-learning models in radiotherapy dose prediction with statistical\nrobustness. Towards enhancing its clinical relevance, we demonstrate how we can\nuse such a model to construct the predicted Dose-Volume-Histograms' confidence\nintervals.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.IV",
      "physics.med-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "28 pages",
    "pdf_url": "http://arxiv.org/pdf/2404.17126v2",
    "published_date": "2024-04-26 02:43:45 UTC",
    "updated_date": "2024-09-23 08:45:43 UTC"
  },
  {
    "arxiv_id": "2404.17122v1",
    "title": "2M-NER: Contrastive Learning for Multilingual and Multimodal NER with Language and Modal Fusion",
    "authors": [
      "Dongsheng Wang",
      "Xiaoqin Feng",
      "Zeming Liu",
      "Chuan Wang"
    ],
    "abstract": "Named entity recognition (NER) is a fundamental task in natural language\nprocessing that involves identifying and classifying entities in sentences into\npre-defined types. It plays a crucial role in various research fields,\nincluding entity linking, question answering, and online product\nrecommendation. Recent studies have shown that incorporating multilingual and\nmultimodal datasets can enhance the effectiveness of NER. This is due to\nlanguage transfer learning and the presence of shared implicit features across\ndifferent modalities. However, the lack of a dataset that combines\nmultilingualism and multimodality has hindered research exploring the\ncombination of these two aspects, as multimodality can help NER in multiple\nlanguages simultaneously. In this paper, we aim to address a more challenging\ntask: multilingual and multimodal named entity recognition (MMNER), considering\nits potential value and influence. Specifically, we construct a large-scale\nMMNER dataset with four languages (English, French, German and Spanish) and two\nmodalities (text and image). To tackle this challenging MMNER task on the\ndataset, we introduce a new model called 2M-NER, which aligns the text and\nimage representations using contrastive learning and integrates a multimodal\ncollaboration module to effectively depict the interactions between the two\nmodalities. Extensive experimental results demonstrate that our model achieves\nthe highest F1 score in multilingual and multimodal NER tasks compared to some\ncomparative and representative baselines. Additionally, in a challenging\nanalysis, we discovered that sentence-level alignment interferes a lot with NER\nmodels, indicating the higher level of difficulty in our dataset.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "20 pages",
    "pdf_url": "http://arxiv.org/pdf/2404.17122v1",
    "published_date": "2024-04-26 02:34:31 UTC",
    "updated_date": "2024-04-26 02:34:31 UTC"
  },
  {
    "arxiv_id": "2404.17120v2",
    "title": "Talking Nonsense: Probing Large Language Models' Understanding of Adversarial Gibberish Inputs",
    "authors": [
      "Valeriia Cherepanova",
      "James Zou"
    ],
    "abstract": "Large language models (LLMs) exhibit excellent ability to understand human\nlanguages, but do they also understand their own language that appears\ngibberish to us? In this work we delve into this question, aiming to uncover\nthe mechanisms underlying such behavior in LLMs. We employ the Greedy\nCoordinate Gradient optimizer to craft prompts that compel LLMs to generate\ncoherent responses from seemingly nonsensical inputs. We call these inputs LM\nBabel and this work systematically studies the behavior of LLMs manipulated by\nthese prompts. We find that the manipulation efficiency depends on the target\ntext's length and perplexity, with the Babel prompts often located in lower\nloss minima compared to natural prompts. We further examine the structure of\nthe Babel prompts and evaluate their robustness. Notably, we find that guiding\nthe model to generate harmful texts is not more difficult than into generating\nbenign texts, suggesting lack of alignment for out-of-distribution prompts.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17120v2",
    "published_date": "2024-04-26 02:29:26 UTC",
    "updated_date": "2024-04-29 17:41:51 UTC"
  },
  {
    "arxiv_id": "2404.17609v2",
    "title": "CoSD: Collaborative Stance Detection with Contrastive Heterogeneous Topic Graph Learning",
    "authors": [
      "Yinghan Cheng",
      "Qi Zhang",
      "Chongyang Shi",
      "Liang Xiao",
      "Shufeng Hao",
      "Liang Hu"
    ],
    "abstract": "Stance detection seeks to identify the viewpoints of individuals either in\nfavor or against a given target or a controversial topic. Current advanced\nneural models for stance detection typically employ fully parametric softmax\nclassifiers. However, these methods suffer from several limitations, including\nlack of explainability, insensitivity to the latent data structure, and\nunimodality, which greatly restrict their performance and applications. To\naddress these challenges, we present a novel collaborative stance detection\nframework called (CoSD) which leverages contrastive heterogeneous topic graph\nlearning to learn topic-aware semantics and collaborative signals among texts,\ntopics, and stance labels for enhancing stance detection. During training, we\nconstruct a heterogeneous graph to structurally organize texts and stances\nthrough implicit topics via employing latent Dirichlet allocation. We then\nperform contrastive graph learning to learn heterogeneous node representations,\naggregating informative multi-hop collaborative signals via an elaborate\nCollaboration Propagation Aggregation (CPA) module. During inference, we\nintroduce a hybrid similarity scoring module to enable the comprehensive\nincorporation of topic-aware semantics and collaborative signals for stance\ndetection. Extensive experiments on two benchmark datasets demonstrate the\nstate-of-the-art detection performance of CoSD, verifying the effectiveness and\nexplainability of our collaborative framework.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "13 pages",
    "pdf_url": "http://arxiv.org/pdf/2404.17609v2",
    "published_date": "2024-04-26 02:04:05 UTC",
    "updated_date": "2024-06-19 13:34:24 UTC"
  },
  {
    "arxiv_id": "2404.17098v1",
    "title": "CLARE: Cognitive Load Assessment in REaltime with Multimodal Data",
    "authors": [
      "Anubhav Bhatti",
      "Prithila Angkan",
      "Behnam Behinaein",
      "Zunayed Mahmud",
      "Dirk Rodenburg",
      "Heather Braund",
      "P. James Mclellan",
      "Aaron Ruberto",
      "Geoffery Harrison",
      "Daryl Wilson",
      "Adam Szulewski",
      "Dan Howes",
      "Ali Etemad",
      "Paul Hungler"
    ],
    "abstract": "We present a novel multimodal dataset for Cognitive Load Assessment in\nREaltime (CLARE). The dataset contains physiological and gaze data from 24\nparticipants with self-reported cognitive load scores as ground-truth labels.\nThe dataset consists of four modalities, namely, Electrocardiography (ECG),\nElectrodermal Activity (EDA), Electroencephalogram (EEG), and Gaze tracking. To\nmap diverse levels of mental load on participants during experiments, each\nparticipant completed four nine-minutes sessions on a computer-based operator\nperformance and mental workload task (the MATB-II software) with varying levels\nof complexity in one minute segments. During the experiment, participants\nreported their cognitive load every 10 seconds. For the dataset, we also\nprovide benchmark binary classification results with machine learning and deep\nlearning models on two different evaluation schemes, namely, 10-fold and\nleave-one-subject-out (LOSO) cross-validation. Benchmark results show that for\n10-fold evaluation, the convolutional neural network (CNN) based deep learning\nmodel achieves the best classification performance with ECG, EDA, and Gaze. In\ncontrast, for LOSO, the best performance is achieved by the deep learning model\nwith ECG, EDA, and EEG.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "12 pages, 10 figures, 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2404.17098v1",
    "published_date": "2024-04-26 01:17:06 UTC",
    "updated_date": "2024-04-26 01:17:06 UTC"
  }
]
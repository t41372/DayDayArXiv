[
  {
    "arxiv_id": "2502.12386v1",
    "title": "Bridging the Data Gap in AI Reliability Research and Establishing DR-AIR, a Comprehensive Data Repository for AI Reliability",
    "authors": [
      "Simin Zheng",
      "Jared M. Clark",
      "Fatemeh Salboukh",
      "Priscila Silva",
      "Karen da Mata",
      "Fenglian Pan",
      "Jie Min",
      "Jiayi Lian",
      "Caleb B. King",
      "Lance Fiondella",
      "Jian Liu",
      "Xinwei Deng",
      "Yili Hong"
    ],
    "abstract": "Artificial intelligence (AI) technology and systems have been advancing\nrapidly. However, ensuring the reliability of these systems is crucial for\nfostering public confidence in their use. This necessitates the modeling and\nanalysis of reliability data specific to AI systems. A major challenge in AI\nreliability research, particularly for those in academia, is the lack of\nreadily available AI reliability data. To address this gap, this paper focuses\non conducting a comprehensive review of available AI reliability data and\nestablishing DR-AIR: a data repository for AI reliability. Specifically, we\nintroduce key measurements and data types for assessing AI reliability, along\nwith the methodologies used to collect these data. We also provide a detailed\ndescription of the currently available datasets with illustrative examples.\nFurthermore, we outline the setup of the DR-AIR repository and demonstrate its\npractical applications. This repository provides easy access to datasets\nspecifically curated for AI reliability research. We believe these efforts will\nsignificantly benefit the AI research community by facilitating access to\nvaluable reliability data and promoting collaboration across various academic\ndomains within AI. We conclude our paper with a call to action, encouraging the\nresearch community to contribute and share AI reliability data to further\nadvance this critical field of study.",
    "categories": [
      "stat.AP",
      "cs.AI"
    ],
    "primary_category": "stat.AP",
    "comment": "34 pages, 12 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.12386v1",
    "published_date": "2025-02-17 23:50:36 UTC",
    "updated_date": "2025-02-17 23:50:36 UTC"
  },
  {
    "arxiv_id": "2502.12382v1",
    "title": "Hybrid Machine Learning Models for Intrusion Detection in IoT: Leveraging a Real-World IoT Dataset",
    "authors": [
      "Md Ahnaf Akif",
      "Ismail Butun",
      "Andre Williams",
      "Imadeldin Mahgoub"
    ],
    "abstract": "The rapid growth of the Internet of Things (IoT) has revolutionized\nindustries, enabling unprecedented connectivity and functionality. However,\nthis expansion also increases vulnerabilities, exposing IoT networks to\nincreasingly sophisticated cyberattacks. Intrusion Detection Systems (IDS) are\ncrucial for mitigating these threats, and recent advancements in Machine\nLearning (ML) offer promising avenues for improvement. This research explores a\nhybrid approach, combining several standalone ML models such as Random Forest\n(RF), XGBoost, K-Nearest Neighbors (KNN), and AdaBoost, in a voting-based\nhybrid classifier for effective IoT intrusion detection. This ensemble method\nleverages the strengths of individual algorithms to enhance accuracy and\naddress challenges related to data complexity and scalability. Using the\nwidely-cited IoT-23 dataset, a prominent benchmark in IoT cybersecurity\nresearch, we evaluate our hybrid classifiers for both binary and multi-class\nintrusion detection problems, ensuring a fair comparison with existing\nliterature. Results demonstrate that our proposed hybrid models, designed for\nrobustness and scalability, outperform standalone approaches in IoT\nenvironments. This work contributes to the development of advanced, intelligent\nIDS frameworks capable of addressing evolving cyber threats.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "9 pages, 8 figures, 2 tables, journal submission",
    "pdf_url": "http://arxiv.org/pdf/2502.12382v1",
    "published_date": "2025-02-17 23:41:10 UTC",
    "updated_date": "2025-02-17 23:41:10 UTC"
  },
  {
    "arxiv_id": "2502.12373v1",
    "title": "Soft Robotics for Search and Rescue: Advancements, Challenges, and Future Directions",
    "authors": [
      "Abhishek Sebastian"
    ],
    "abstract": "Soft robotics has emerged as a transformative technology in Search and Rescue\n(SAR) operations, addressing challenges in navigating complex, hazardous\nenvironments that often limit traditional rigid robots. This paper critically\nexamines advancements in soft robotic technologies tailored for SAR\napplications, focusing on their unique capabilities in adaptability, safety,\nand efficiency. By leveraging bio-inspired designs, flexible materials, and\nadvanced locomotion mechanisms, such as crawling, rolling, and shape morphing,\nsoft robots demonstrate exceptional potential in disaster scenarios. However,\nsignificant barriers persist, including material durability, power\ninefficiency, sensor integration, and control complexity. This comprehensive\nreview highlights the current state of soft robotics in SAR, discusses\nsimulation methodologies and hardware validations, and introduces performance\nmetrics essential for their evaluation. By bridging the gap between theoretical\nadvancements and practical deployment, this study underscores the potential of\nsoft robotic systems to revolutionize SAR missions and advocates for continued\ninterdisciplinary innovation to overcome existing limitations.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12373v1",
    "published_date": "2025-02-17 23:24:18 UTC",
    "updated_date": "2025-02-17 23:24:18 UTC"
  },
  {
    "arxiv_id": "2502.12372v1",
    "title": "Factual Inconsistency in Data-to-Text Generation Scales Exponentially with LLM Size: A Statistical Validation",
    "authors": [
      "Joy Mahapatra",
      "Soumyajit Roy",
      "Utpal Garain"
    ],
    "abstract": "Monitoring factual inconsistency is essential for ensuring trustworthiness in\ndata-to-text generation (D2T). While large language models (LLMs) have\ndemonstrated exceptional performance across various D2T tasks, previous studies\non scaling laws have primarily focused on generalization error through power\nlaw scaling to LLM size (i.e., the number of model parameters). However, no\nresearch has examined the impact of LLM size on factual inconsistency in D2T.\nIn this paper, we investigate how factual inconsistency in D2T scales with LLM\nsize by exploring two scaling laws: power law and exponential scaling. To\nrigorously evaluate and compare these scaling laws, we employ a statistical\nvalidation framework consisting of three key stages: predictive performance\nestimation, goodness-of-fit assessment, and comparative analysis. For a\ncomprehensive empirical study, we analyze three popular LLM families across\nfive D2T datasets, measuring factual inconsistency inversely using four\nstate-of-the-art consistency metrics. Our findings, based on exhaustive\nempirical results and validated through our framework, reveal that, contrary to\nthe widely assumed power law scaling, factual inconsistency in D2T follows an\nexponential scaling with LLM size.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "21 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.12372v1",
    "published_date": "2025-02-17 23:24:00 UTC",
    "updated_date": "2025-02-17 23:24:00 UTC"
  },
  {
    "arxiv_id": "2502.12371v2",
    "title": "IMLE Policy: Fast and Sample Efficient Visuomotor Policy Learning via Implicit Maximum Likelihood Estimation",
    "authors": [
      "Krishan Rana",
      "Robert Lee",
      "David Pershouse",
      "Niko Suenderhauf"
    ],
    "abstract": "Recent advances in imitation learning, particularly using generative\nmodelling techniques like diffusion, have enabled policies to capture complex\nmulti-modal action distributions. However, these methods often require large\ndatasets and multiple inference steps for action generation, posing challenges\nin robotics where the cost for data collection is high and computation\nresources are limited. To address this, we introduce IMLE Policy, a novel\nbehaviour cloning approach based on Implicit Maximum Likelihood Estimation\n(IMLE). IMLE Policy excels in low-data regimes, effectively learning from\nminimal demonstrations and requiring 38\\% less data on average to match the\nperformance of baseline methods in learning complex multi-modal behaviours. Its\nsimple generator-based architecture enables single-step action generation,\nimproving inference speed by 97.3\\% compared to Diffusion Policy, while\noutperforming single-step Flow Matching. We validate our approach across\ndiverse manipulation tasks in simulated and real-world environments, showcasing\nits ability to capture complex behaviours under data constraints. Videos and\ncode are provided on our project page: https://imle-policy.github.io/.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Videos and code are available at https://imle-policy.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2502.12371v2",
    "published_date": "2025-02-17 23:22:49 UTC",
    "updated_date": "2025-03-11 00:38:28 UTC"
  },
  {
    "arxiv_id": "2502.12362v1",
    "title": "Classifiers of Data Sharing Statements in Clinical Trial Records",
    "authors": [
      "Saber Jelodari Mamaghani",
      "Cosima Strantz",
      "Dennis Toddenroth"
    ],
    "abstract": "Digital individual participant data (IPD) from clinical trials are\nincreasingly distributed for potential scientific reuse. The identification of\navailable IPD, however, requires interpretations of textual data-sharing\nstatements (DSS) in large databases. Recent advancements in computational\nlinguistics include pre-trained language models that promise to simplify the\nimplementation of effective classifiers based on textual inputs. In a subset of\n5,000 textual DSS from ClinicalTrials.gov, we evaluate how well classifiers\nbased on domain-specific pre-trained language models reproduce original\navailability categories as well as manually annotated labels. Typical metrics\nindicate that classifiers that predicted manual annotations outperformed those\nthat learned to output the original availability categories. This suggests that\nthe textual DSS descriptions contain applicable information that the\navailability categories do not, and that such classifiers could thus aid the\nautomatic identification of available IPD in large trial databases.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "68T50",
      "I.2.7; J.3"
    ],
    "primary_category": "cs.CL",
    "comment": "Published in Proceedings of MIE 2024, IOS Press eBooks. Studies in\n  Health Technology and Informatics, Vol. 316, pp. 834-838. Conference held in\n  Athens, Greece",
    "pdf_url": "http://arxiv.org/pdf/2502.12362v1",
    "published_date": "2025-02-17 22:56:56 UTC",
    "updated_date": "2025-02-17 22:56:56 UTC"
  },
  {
    "arxiv_id": "2502.12360v2",
    "title": "Detecting Systematic Weaknesses in Vision Models along Predefined Human-Understandable Dimensions",
    "authors": [
      "Sujan Sai Gannamaneni",
      "Rohil Prakash Rao",
      "Michael Mock",
      "Maram Akila",
      "Stefan Wrobel"
    ],
    "abstract": "Slice discovery methods (SDMs) are prominent algorithms for finding\nsystematic weaknesses in DNNs. They identify top-k semantically coherent\nslices/subsets of data where a DNN-under-test has low performance. For being\ndirectly useful, slices should be aligned with human-understandable and\nrelevant dimensions, which, for example, are defined by safety and domain\nexperts as part of the operational design domain (ODD). While SDMs can be\napplied effectively on structured data, their application on image data is\ncomplicated by the lack of semantic metadata. To address these issues, we\npresent an algorithm that combines foundation models for zero-shot image\nclassification to generate semantic metadata with methods for combinatorial\nsearch to find systematic weaknesses in images. In contrast to existing\napproaches, ours identifies weak slices that are in line with pre-defined\nhuman-understandable dimensions. As the algorithm includes foundation models,\nits intermediate and final results may not always be exact. Therefore, we\ninclude an approach to address the impact of noisy metadata. We validate our\nalgorithm on both synthetic and real-world datasets, demonstrating its ability\nto recover human-understandable systematic weaknesses. Furthermore, using our\napproach, we identify systematic weaknesses of multiple pre-trained and\npublicly available state-of-the-art computer vision DNNs.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12360v2",
    "published_date": "2025-02-17 22:50:45 UTC",
    "updated_date": "2025-03-06 18:07:00 UTC"
  },
  {
    "arxiv_id": "2502.12354v2",
    "title": "Human-centered explanation does not fit all: The interplay of sociotechnical, cognitive, and individual factors in the effect AI explanations in algorithmic decision-making",
    "authors": [
      "Yongsu Ahn",
      "Yu-Ru Lin",
      "Malihe Alikhani",
      "Eunjeong Cheon"
    ],
    "abstract": "Recent XAI studies have investigated what constitutes a \\textit{good}\nexplanation in AI-assisted decision-making. Despite the widely accepted\nhuman-friendly properties of explanations, such as contrastive and selective,\nexisting studies have yielded inconsistent findings. To address these gaps, our\nstudy focuses on the cognitive dimensions of explanation evaluation, by\nevaluating six explanations with different contrastive strategies and\ninformation selectivity and scrutinizing factors behind their valuation\nprocess. Our analysis results find that contrastive explanations are not the\nmost preferable or understandable in general; Rather, different contrastive and\nselective explanations were appreciated to a different extent based on who they\nare, when, how, and what to explain -- with different level of cognitive load\nand engagement and sociotechnical contexts. Given these findings, we call for a\nnuanced view of explanation strategies, with implications for designing AI\ninterfaces to accommodate individual and contextual differences in AI-assisted\ndecision-making.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12354v2",
    "published_date": "2025-02-17 22:42:53 UTC",
    "updated_date": "2025-05-02 08:24:50 UTC"
  },
  {
    "arxiv_id": "2502.12352v2",
    "title": "Towards Mechanistic Interpretability of Graph Transformers via Attention Graphs",
    "authors": [
      "Batu El",
      "Deepro Choudhury",
      "Pietro Liò",
      "Chaitanya K. Joshi"
    ],
    "abstract": "We introduce Attention Graphs, a new tool for mechanistic interpretability of\nGraph Neural Networks (GNNs) and Graph Transformers based on the mathematical\nequivalence between message passing in GNNs and the self-attention mechanism in\nTransformers. Attention Graphs aggregate attention matrices across Transformer\nlayers and heads to describe how information flows among input nodes. Through\nexperiments on homophilous and heterophilous node classification tasks, we\nanalyze Attention Graphs from a network science perspective and find that: (1)\nWhen Graph Transformers are allowed to learn the optimal graph structure using\nall-to-all attention among input nodes, the Attention Graphs learned by the\nmodel do not tend to correlate with the input/original graph structure; and (2)\nFor heterophilous graphs, different Graph Transformer variants can achieve\nsimilar performance while utilising distinct information flow patterns. Open\nsource code: https://github.com/batu-el/understanding-inductive-biases-of-gnns",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12352v2",
    "published_date": "2025-02-17 22:35:16 UTC",
    "updated_date": "2025-02-25 17:15:29 UTC"
  },
  {
    "arxiv_id": "2503.03756v1",
    "title": "Efficient Finetuning for Dimensional Speech Emotion Recognition in the Age of Transformers",
    "authors": [
      "Aneesha Sampath",
      "James Tavernor",
      "Emily Mower Provost"
    ],
    "abstract": "Accurate speech emotion recognition is essential for developing human-facing\nsystems. Recent advancements have included finetuning large, pretrained\ntransformer models like Wav2Vec 2.0. However, the finetuning process requires\nsubstantial computational resources, including high-memory GPUs and significant\nprocessing time. As the demand for accurate emotion recognition continues to\ngrow, efficient finetuning approaches are needed to reduce the computational\nburden. Our study focuses on dimensional emotion recognition, predicting\nattributes such as activation (calm to excited) and valence (negative to\npositive). We present various finetuning techniques, including full finetuning,\npartial finetuning of transformer layers, finetuning with mixed precision,\npartial finetuning with caching, and low-rank adaptation (LoRA) on the Wav2Vec\n2.0 base model. We find that partial finetuning with mixed precision achieves\nperformance comparable to full finetuning while increasing training speed by\n67%. Caching intermediate representations further boosts efficiency, yielding\nan 88% speedup and a 71% reduction in learnable parameters. We recommend\nfinetuning the final three transformer layers in mixed precision to balance\nperformance and training efficiency, and adding intermediate representation\ncaching for optimal speed with minimal performance trade-offs. These findings\nlower the barriers to finetuning speech emotion recognition systems, making\naccurate emotion recognition more accessible to a broader range of researchers\nand practitioners.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech\n  and Signal Processing (ICASSP)",
    "pdf_url": "http://arxiv.org/pdf/2503.03756v1",
    "published_date": "2025-02-17 22:34:08 UTC",
    "updated_date": "2025-02-17 22:34:08 UTC"
  },
  {
    "arxiv_id": "2502.12346v1",
    "title": "QuZO: Quantized Zeroth-Order Fine-Tuning for Large Language Models",
    "authors": [
      "Jiajun Zhou",
      "Yifan Yang",
      "Kai Zhen",
      "Ziyue Liu",
      "Yequan Zhao",
      "Ershad Banijamali",
      "Athanasios Mouchtaris",
      "Ngai Wong",
      "Zheng Zhang"
    ],
    "abstract": "Language Models (LLMs) are often quantized to lower precision to reduce the\nmemory cost and latency in inference. However, quantization often degrades\nmodel performance, thus fine-tuning is required for various down-stream tasks.\nTraditional fine-tuning methods such as stochastic gradient descent and Adam\noptimization require backpropagation, which are error-prone in the\nlow-precision settings. To overcome these limitations, we propose the Quantized\nZeroth-Order (QuZO) framework, specifically designed for fine-tuning LLMs\nthrough low-precision (e.g., 4- or 8-bit) forward passes. Our method can avoid\nthe error-prone low-precision straight-through estimator, and utilizes\noptimized stochastic rounding to mitigate the increased bias. QuZO simplifies\nthe training process, while achieving results comparable to first-order methods\nin ${\\rm FP}8$ and superior accuracy in ${\\rm INT}8$ and ${\\rm INT}4$ training.\nExperiments demonstrate that low-bit training QuZO achieves performance\ncomparable to MeZO optimization on GLUE, Multi-Choice, and Generation tasks,\nwhile reducing memory cost by $2.94 \\times$ in LLaMA2-7B fine-tuning compared\nto quantized first-order methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12346v1",
    "published_date": "2025-02-17 22:20:31 UTC",
    "updated_date": "2025-02-17 22:20:31 UTC"
  },
  {
    "arxiv_id": "2502.12329v1",
    "title": "A Novel Unified Parametric Assumption for Nonconvex Optimization",
    "authors": [
      "Artem Riabinin",
      "Ahmed Khaled",
      "Peter Richtárik"
    ],
    "abstract": "Nonconvex optimization is central to modern machine learning, but the general\nframework of nonconvex optimization yields weak convergence guarantees that are\ntoo pessimistic compared to practice. On the other hand, while convexity\nenables efficient optimization, it is of limited applicability to many\npractical problems. To bridge this gap and better understand the practical\nsuccess of optimization algorithms in nonconvex settings, we introduce a novel\nunified parametric assumption. Our assumption is general enough to encompass a\nbroad class of nonconvex functions while also being specific enough to enable\nthe derivation of a unified convergence theorem for gradient-based methods.\nNotably, by tuning the parameters of our assumption, we demonstrate its\nversatility in recovering several existing function classes as special cases\nand in identifying functions amenable to efficient optimization. We derive our\nconvergence theorem for both deterministic and stochastic optimization, and\nconduct experiments to verify that our assumption can hold practically over\noptimization trajectories.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12329v1",
    "published_date": "2025-02-17 21:25:31 UTC",
    "updated_date": "2025-02-17 21:25:31 UTC"
  },
  {
    "arxiv_id": "2502.13174v1",
    "title": "Generative Topology Optimization: Exploring Diverse Solutions in Structural Design",
    "authors": [
      "Andreas Radler",
      "Eric Volkmann",
      "Johannes Brandstetter",
      "Arturs Berzins"
    ],
    "abstract": "Topology optimization (TO) is a family of computational methods that derive\nnear-optimal geometries from formal problem descriptions. Despite their\nsuccess, established TO methods are limited to generating single solutions,\nrestricting the exploration of alternative designs. To address this limitation,\nwe introduce Generative Topology Optimization (GenTO) - a data-free method that\ntrains a neural network to generate structurally compliant shapes and explores\ndiverse solutions through an explicit diversity constraint. The network is\ntrained with a solver-in-the-loop, optimizing the material distribution in each\niteration. The trained model produces diverse shapes that closely adhere to the\ndesign requirements. We validate GenTO on 2D and 3D TO problems. Our results\ndemonstrate that GenTO produces more diverse solutions than any prior method\nwhile maintaining near-optimality and being an order of magnitude faster due to\ninherent parallelism. These findings open new avenues for engineering and\ndesign, offering enhanced flexibility and innovation in structural\noptimization.",
    "categories": [
      "cs.LG",
      "cond-mat.mtrl-sci",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.13174v1",
    "published_date": "2025-02-17 21:24:18 UTC",
    "updated_date": "2025-02-17 21:24:18 UTC"
  },
  {
    "arxiv_id": "2502.12328v1",
    "title": "LM Agents for Coordinating Multi-User Information Gathering",
    "authors": [
      "Harsh Jhamtani",
      "Jacob Andreas",
      "Benjamin Van Durme"
    ],
    "abstract": "This paper introduces PeopleJoin, a benchmark for evaluating LM-mediated\ncollaborative problem solving. Given a user request, PeopleJoin agents must\nidentify teammates who might be able to assist, converse with these teammates\nto gather information, and finally compile a useful answer or summary for the\noriginal user. PeopleJoin comprises two evaluation domains: PeopleJoin-QA,\nfocused on questions about tabular data, and PeopleJoin-DocCreation, focused on\ndocument creation tasks. The two domains are adapted from existing NLP\nbenchmarks for database question answering and multi-document summarization;\nhere, however, the information needed to complete these tasks is distributed\nacross synthetic ``organizations'' of 2--20 users, simulating natural\nmulti-user collaboration scenarios. We implemented several popular LM agent\narchitectures, evaluating their accuracy and efficiency at completing tasks,\nand highlight new research questions that can be studied using PeopleJoin.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12328v1",
    "published_date": "2025-02-17 21:19:45 UTC",
    "updated_date": "2025-02-17 21:19:45 UTC"
  },
  {
    "arxiv_id": "2502.12327v1",
    "title": "Learning Plasma Dynamics and Robust Rampdown Trajectories with Predict-First Experiments at TCV",
    "authors": [
      "Allen M. Wang",
      "Alessandro Pau",
      "Cristina Rea",
      "Oswin So",
      "Charles Dawson",
      "Olivier Sauter",
      "Mark D. Boyer",
      "Anna Vu",
      "Cristian Galperti",
      "Chuchu Fan",
      "Antoine Merle",
      "Yoeri Poels",
      "Cristina Venturini",
      "Stefano Marchioni",
      "the TCV Team"
    ],
    "abstract": "The rampdown in tokamak operations is a difficult to simulate phase during\nwhich the plasma is often pushed towards multiple instability limits. To\naddress this challenge, and reduce the risk of disrupting operations, we\nleverage recent advances in Scientific Machine Learning (SciML) to develop a\nneural state-space model (NSSM) that predicts plasma dynamics during Tokamak\n\\`a Configuration Variable (TCV) rampdowns. By integrating simple physics\nstructure and data-driven models, the NSSM efficiently learns plasma dynamics\nduring the rampdown from a modest dataset of 311 pulses with only five pulses\nin the reactor relevant high performance regime. The NSSM is parallelized\nacross uncertainties, and reinforcement learning (RL) is applied to design\ntrajectories that avoid multiple instability limits with high probability.\nExperiments at TCV ramping down high performance plasmas show statistically\nsignificant improvements in current and energy at plasma termination, with\nimprovements in speed through continuous re-training. A predict-first\nexperiment, increasing plasma current by 20\\% from baseline, demonstrates the\nNSSM's ability to make small extrapolations with sufficient accuracy to design\ntrajectories that successfully terminate the pulse. The developed approach\npaves the way for designing tokamak controls with robustness to considerable\nuncertainty, and demonstrates the relevance of the SciML approach to learning\nplasma dynamics for rapidly developing robust trajectories and controls during\nthe incremental campaigns of upcoming burning plasma tokamaks.",
    "categories": [
      "physics.plasm-ph",
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "physics.plasm-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12327v1",
    "published_date": "2025-02-17 21:19:15 UTC",
    "updated_date": "2025-02-17 21:19:15 UTC"
  },
  {
    "arxiv_id": "2502.14896v1",
    "title": "A Comprehensive Survey on Concept Erasure in Text-to-Image Diffusion Models",
    "authors": [
      "Changhoon Kim",
      "Yanjun Qi"
    ],
    "abstract": "Text-to-Image (T2I) models have made remarkable progress in generating\nhigh-quality, diverse visual content from natural language prompts. However,\ntheir ability to reproduce copyrighted styles, sensitive imagery, and harmful\ncontent raises significant ethical and legal concerns. Concept erasure offers a\nproactive alternative to external filtering by modifying T2I models to prevent\nthe generation of undesired content. In this survey, we provide a structured\noverview of concept erasure, categorizing existing methods based on their\noptimization strategies and the architectural components they modify. We\ncategorize concept erasure methods into fine-tuning for parameter updates,\nclosed-form solutions for efficient edits, and inference-time interventions for\ncontent restriction without weight modification. Additionally, we explore\nadversarial attacks that bypass erasure techniques and discuss emerging\ndefenses. To support further research, we consolidate key datasets, evaluation\nmetrics, and benchmarks for assessing erasure effectiveness and model\nrobustness. This survey serves as a comprehensive resource, offering insights\ninto the evolving landscape of concept erasure, its challenges, and future\ndirections.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14896v1",
    "published_date": "2025-02-17 20:51:20 UTC",
    "updated_date": "2025-02-17 20:51:20 UTC"
  },
  {
    "arxiv_id": "2503.05725v2",
    "title": "A new framework for prognostics in decentralized industries: Enhancing fairness, security, and transparency through Blockchain and Federated Learning",
    "authors": [
      "T. Q. D. Pham",
      "K. D. Tran",
      "Khanh T. P. Nguyen",
      "X. V. Tran",
      "L. Köehl",
      "K. P. Tran"
    ],
    "abstract": "As global industries transition towards Industry 5.0 predictive maintenance\nPM remains crucial for cost effective operations resilience and minimizing\ndowntime in increasingly smart manufacturing environments In this chapter we\nexplore how the integration of Federated Learning FL and blockchain BC\ntechnologies enhances the prediction of machinerys Remaining Useful Life RUL\nwithin decentralized and human centric industrial ecosystems Traditional\ncentralized data approaches raise concerns over privacy security and\nscalability especially as Artificial intelligence AI driven smart manufacturing\nbecomes more prevalent This chapter leverages FL to enable localized model\ntraining across multiple sites while utilizing BC to ensure trust transparency\nand data integrity across the network This BC integrated FL framework optimizes\nRUL predictions enhances data privacy and security establishes transparency and\npromotes collaboration in decentralized manufacturing It addresses key\nchallenges such as maintaining privacy and security ensuring transparency and\nfairness and incentivizing participation in decentralized networks Experimental\nvalidation using the NASA CMAPSS dataset demonstrates the model effectiveness\nin real world scenarios and we extend our findings to the broader research\ncommunity through open source code on GitHub inviting collaborative development\nto drive innovation in Industry 5.0",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05725v2",
    "published_date": "2025-02-17 20:28:40 UTC",
    "updated_date": "2025-04-08 16:53:33 UTC"
  },
  {
    "arxiv_id": "2502.12304v1",
    "title": "Warmup Generations: A Task-Agnostic Approach for Guiding Sequence-to-Sequence Learning with Unsupervised Initial State Generation",
    "authors": [
      "Senyu Li",
      "Zipeng Sun",
      "Jiayi Wang",
      "Xue Liu",
      "Pontus Stenetorp",
      "Siva Reddy",
      "David Ifeoluwa Adelani"
    ],
    "abstract": "Traditional supervised fine-tuning (SFT) strategies for sequence-to-sequence\ntasks often train models to directly generate the target output. Recent work\nhas shown that guiding models with intermediate steps, such as keywords,\noutlines, or reasoning chains, can significantly improve performance,\ncoherence, and interpretability. However, these methods often depend on\npredefined intermediate formats and annotated data, limiting their scalability\nand generalizability. In this work, we introduce a task-agnostic framework that\nenables models to generate intermediate \"warmup\" sequences. These warmup\nsequences, serving as an initial state for subsequent generation, are optimized\nto enhance the probability of generating the target sequence without relying on\nexternal supervision or human-designed structures. Drawing inspiration from\nreinforcement learning principles, our method iteratively refines these\nintermediate steps to maximize their contribution to the final output, similar\nto reward-driven optimization in reinforcement learning with human feedback.\nExperimental results across tasks such as translation, summarization, and\nmulti-choice question answering for logical reasoning show that our approach\noutperforms traditional SFT methods, and offers a scalable and flexible\nsolution for sequence-to-sequence tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12304v1",
    "published_date": "2025-02-17 20:23:42 UTC",
    "updated_date": "2025-02-17 20:23:42 UTC"
  },
  {
    "arxiv_id": "2502.13173v1",
    "title": "Thinking Preference Optimization",
    "authors": [
      "Wang Yang",
      "Hongye Jin",
      "Jingfeng Yang",
      "Vipin Chaudhary",
      "Xiaotian Han"
    ],
    "abstract": "Supervised Fine-Tuning (SFT) has been a go-to and effective method for\nenhancing long chain-of-thought (CoT) reasoning in relatively small LLMs by\nfine-tuning them with long CoT responses from larger LLMs. To continually\nimprove reasoning abilities, we can either collect new high-quality long CoT\nreasoning SFT data or repeatedly train on existing SFT datasets. However,\nacquiring new long CoT SFT data is costly and limited, while repeated training\noften results in a performance plateau or decline. To further boost the\nperformance with the SFT data, we propose Thinking Preference Optimization\n(ThinkPO), a simple yet effective post-SFT method that enhances long CoT\nreasoning without requiring new long CoT responses. Instead, ThinkPO utilizes\nreadily available or easily obtainable short CoT reasoning responses as\nrejected answers and long CoT responses as chosen answers for the same\nquestion. It then applies direct preference optimization to encourage the model\nto favor longer reasoning outputs. Experiments show that ThinkPO further\nimproves the reasoning performance of SFT-ed models, e.g. it increases math\nreasoning accuracy of SFT-ed models by 8.6% and output length by 25.9%.\nNotably, ThinkPO is capable of continually boosting the performance of the\npublicly distilled SFT model, e.g., increasing the official\nDeepSeek-R1-Distill-Qwen-7B's performance on MATH500 from 87.4% to 91.2%.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.13173v1",
    "published_date": "2025-02-17 19:56:21 UTC",
    "updated_date": "2025-02-17 19:56:21 UTC"
  },
  {
    "arxiv_id": "2502.13172v1",
    "title": "Unveiling Privacy Risks in LLM Agent Memory",
    "authors": [
      "Bo Wang",
      "Weiyi He",
      "Pengfei He",
      "Shenglai Zeng",
      "Zhen Xiang",
      "Yue Xing",
      "Jiliang Tang"
    ],
    "abstract": "Large Language Model (LLM) agents have become increasingly prevalent across\nvarious real-world applications. They enhance decision-making by storing\nprivate user-agent interactions in the memory module for demonstrations,\nintroducing new privacy risks for LLM agents. In this work, we systematically\ninvestigate the vulnerability of LLM agents to our proposed Memory EXTRaction\nAttack (MEXTRA) under a black-box setting. To extract private information from\nmemory, we propose an effective attacking prompt design and an automated prompt\ngeneration method based on different levels of knowledge about the LLM agent.\nExperiments on two representative agents demonstrate the effectiveness of\nMEXTRA. Moreover, we explore key factors influencing memory leakage from both\nthe agent's and the attacker's perspectives. Our findings highlight the urgent\nneed for effective memory safeguards in LLM agent design and deployment.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Under review",
    "pdf_url": "http://arxiv.org/pdf/2502.13172v1",
    "published_date": "2025-02-17 19:55:53 UTC",
    "updated_date": "2025-02-17 19:55:53 UTC"
  },
  {
    "arxiv_id": "2502.12280v1",
    "title": "Connecting Large Language Model Agent to High Performance Computing Resource",
    "authors": [
      "Heng Ma",
      "Alexander Brace",
      "Carlo Siebenschuh",
      "Greg Pauloski",
      "Ian Foster",
      "Arvind Ramanathan"
    ],
    "abstract": "The Large Language Model agent workflow enables the LLM to invoke tool\nfunctions to increase the performance on specific scientific domain questions.\nTo tackle large scale of scientific research, it requires access to computing\nresource and parallel computing setup. In this work, we implemented Parsl to\nthe LangChain/LangGraph tool call setup, to bridge the gap between the LLM\nagent to the computing resource. Two tool call implementations were set up and\ntested on both local workstation and HPC environment on Polaris/ALCF. The first\nimplementation with Parsl-enabled LangChain tool node queues the tool functions\nconcurrently to the Parsl workers for parallel execution. The second\nconfiguration is implemented by converting the tool functions into Parsl\nensemble functions, and is more suitable for large task on super computer\nenvironment. The LLM agent workflow was prompted to run molecular dynamics\nsimulations, with different protein structure and simulation conditions. These\nresults showed the LLM agent tools were managed and executed concurrently by\nParsl on the available computing resource.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "I.2.11"
    ],
    "primary_category": "cs.DC",
    "comment": "7 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.12280v1",
    "published_date": "2025-02-17 19:32:30 UTC",
    "updated_date": "2025-02-17 19:32:30 UTC"
  },
  {
    "arxiv_id": "2502.12278v1",
    "title": "Towards Practical First-Order Model Counting",
    "authors": [
      "Ananth K. Kidambi",
      "Guramrit Singh",
      "Paulius Dilkas",
      "Kuldeep S. Meel"
    ],
    "abstract": "First-order model counting (FOMC) is the problem of counting the number of\nmodels of a sentence in first-order logic. Since lifted inference techniques\nrely on reductions to variants of FOMC, the design of scalable methods for FOMC\nhas attracted attention from both theoreticians and practitioners over the past\ndecade. Recently, a new approach based on first-order knowledge compilation was\nproposed. This approach, called Crane, instead of simply providing the final\ncount, generates definitions of (possibly recursive) functions that can be\nevaluated with different arguments to compute the model count for any domain\nsize. However, this approach is not fully automated, as it requires manual\nevaluation of the constructed functions. The primary contribution of this work\nis a fully automated compilation algorithm, called Gantry, which transforms the\nfunction definitions into C++ code equipped with arbitrary-precision\narithmetic. These additions allow the new FOMC algorithm to scale to domain\nsizes over 500,000 times larger than the current state of the art, as\ndemonstrated through experimental results.",
    "categories": [
      "cs.LO",
      "cs.AI"
    ],
    "primary_category": "cs.LO",
    "comment": "18 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2502.12278v1",
    "published_date": "2025-02-17 19:28:06 UTC",
    "updated_date": "2025-02-17 19:28:06 UTC"
  },
  {
    "arxiv_id": "2502.12275v2",
    "title": "Integrating Expert Knowledge into Logical Programs via LLMs",
    "authors": [
      "Franciszek Górski",
      "Oskar Wysocki",
      "Marco Valentino",
      "Andre Freitas"
    ],
    "abstract": "This paper introduces ExKLoP, a novel framework designed to evaluate how\neffectively Large Language Models (LLMs) integrate expert knowledge into\nlogical reasoning systems. This capability is especially valuable in\nengineering, where expert knowledge-such as manufacturer-recommended\noperational ranges-can be directly embedded into automated monitoring systems.\nBy mirroring expert verification steps, tasks like range checking and\nconstraint validation help ensure system safety and reliability. Our approach\nsystematically evaluates LLM-generated logical rules, assessing both syntactic\nfluency and logical correctness in these critical validation tasks. We also\nexplore the models' capacity for self-correction via an iterative feedback loop\nbased on code execution outcomes. ExKLoP presents an extensible dataset\ncomprising 130 engineering premises, 950 prompts, and corresponding validation\npoints. It enables comprehensive benchmarking while allowing control over task\ncomplexity and scalability of experiments. We leverage the synthetic data\ncreation methodology to conduct extensive empirical evaluation on a diverse set\nof LLMs including Llama3, Gemma3, Codestral and QwenCoder. The results reveal\nthat most models generate nearly perfect syntactically correct code and exhibit\nstrong performance in translating expert knowledge into correct code. At the\nsame time, while most LLMs produce nearly flawless syntactic output, their\nability to correctly implement logical rules varies, as does their capacity for\nself-improvement. Overall, ExKLoP serves as a robust evaluation platform that\nstreamlines the selection of effective models for self-correcting systems while\nclearly delineating the types of errors encountered.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12275v2",
    "published_date": "2025-02-17 19:18:23 UTC",
    "updated_date": "2025-05-12 08:43:52 UTC"
  },
  {
    "arxiv_id": "2502.12272v3",
    "title": "Learning to Reason at the Frontier of Learnability",
    "authors": [
      "Thomas Foster",
      "Jakob Foerster"
    ],
    "abstract": "Reinforcement learning is now widely adopted as the final stage of large\nlanguage model training, especially for reasoning-style tasks such as maths\nproblems. Typically, models attempt each question many times during a single\ntraining step and attempt to learn from their successes and failures. However,\nwe demonstrate that throughout training with two popular algorithms (PPO and\nVinePPO) on two widely used datasets, many questions are either solved by all\nattempts - meaning they are already learned - or by none - providing no\nmeaningful training signal. To address this, we adapt a method from the\nreinforcement learning literature - sampling for learnability - and apply it to\nthe reinforcement learning stage of LLM training. Our curriculum prioritises\nquestions with high variance of success, i.e. those where the agent sometimes\nsucceeds, but not always. Our findings demonstrate that this curriculum\nconsistently boosts training performance across multiple algorithms and\ndatasets, paving the way for more efficient and effective reinforcement\nlearning with LLMs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12272v3",
    "published_date": "2025-02-17 19:16:37 UTC",
    "updated_date": "2025-02-24 18:15:02 UTC"
  },
  {
    "arxiv_id": "2502.12267v1",
    "title": "NeuroStrata: Harnessing Neurosymbolic Paradigms for Improved Design, Testability, and Verifiability of Autonomous CPS",
    "authors": [
      "Xi Zheng",
      "Ziyang Li",
      "Ivan Ruchkin",
      "Ruzica Piskac",
      "Miroslav Pajic"
    ],
    "abstract": "Autonomous cyber-physical systems (CPSs) leverage AI for perception,\nplanning, and control but face trust and safety certification challenges due to\ninherent uncertainties. The neurosymbolic paradigm replaces stochastic layers\nwith interpretable symbolic AI, enabling determinism. While promising,\nchallenges like multisensor fusion, adaptability, and verification remain. This\npaper introduces NeuroStrata, a neurosymbolic framework to enhance the testing\nand verification of autonomous CPS. We outline its key components, present\nearly results, and detail future plans.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12267v1",
    "published_date": "2025-02-17 19:07:41 UTC",
    "updated_date": "2025-02-17 19:07:41 UTC"
  },
  {
    "arxiv_id": "2503.05724v1",
    "title": "Addressing Moral Uncertainty using Large Language Models for Ethical Decision-Making",
    "authors": [
      "Rohit K. Dubey",
      "Damian Dailisan",
      "Sachit Mahajan"
    ],
    "abstract": "We present an ethical decision-making framework that refines a pre-trained\nreinforcement learning (RL) model using a task-agnostic ethical layer.\nFollowing initial training, the RL model undergoes ethical fine-tuning, where\nhuman feedback is replaced by feedback generated from a large language model\n(LLM). The LLM embodies consequentialist, deontological, virtue, social\njustice, and care ethics as moral principles to assign belief values to\nrecommended actions during ethical decision-making. An ethical layer aggregates\nbelief scores from multiple LLM-derived moral perspectives using Belief\nJensen-Shannon Divergence and Dempster-Shafer Theory into probability scores\nthat also serve as the shaping reward, steering the agent toward choices that\nalign with a balanced ethical framework. This integrated learning framework\nhelps the RL agent navigate moral uncertainty in complex environments and\nenables it to make morally sound decisions across diverse tasks. Our approach,\ntested across different LLM variants and compared with other belief aggregation\ntechniques, demonstrates improved consistency, adaptability, and reduced\nreliance on handcrafted ethical rewards. This method is especially effective in\ndynamic scenarios where ethical challenges arise unexpectedly, making it\nwell-suited for real-world applications.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "13 pages, 5 figures. All authors contributed equally to this work",
    "pdf_url": "http://arxiv.org/pdf/2503.05724v1",
    "published_date": "2025-02-17 19:05:55 UTC",
    "updated_date": "2025-02-17 19:05:55 UTC"
  },
  {
    "arxiv_id": "2503.05723v1",
    "title": "AI Mimicry and Human Dignity: Chatbot Use as a Violation of Self-Respect",
    "authors": [
      "Jan-Willem van der Rijt",
      "Dimitri Coelho Mollo",
      "Bram Vaassen"
    ],
    "abstract": "This paper investigates how human interactions with AI-powered chatbots may\noffend human dignity. Current chatbots, driven by large language models (LLMs),\nmimic human linguistic behaviour but lack the moral and rational capacities\nessential for genuine interpersonal respect. Human beings are prone to\nanthropomorphise chatbots. Indeed, chatbots appear to be deliberately designed\nto elicit that response. As a result, human beings' behaviour toward chatbots\noften resembles behaviours typical of interaction between moral agents. Drawing\non a second-personal, relational account of dignity, we argue that interacting\nwith chatbots in this way is incompatible with the dignity of users. We show\nthat, since second-personal respect is premised on reciprocal recognition of\nsecond-personal authority, behaving towards chatbots in ways that convey\nsecond-personal respect is bound to misfire in morally problematic ways, given\nthe lack of reciprocity. Consequently, such chatbot interactions amount to\nsubtle but significant violations of self-respect: the respect we are dutybound\nto show for our own dignity. We illustrate this by discussing four actual\nchatbot use cases (information retrieval, customer service, advising, and\ncompanionship), and propound that the increasing societal pressure to engage in\nsuch interactions with chatbots poses a hitherto underappreciated threat to\nhuman dignity.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05723v1",
    "published_date": "2025-02-17 19:02:12 UTC",
    "updated_date": "2025-02-17 19:02:12 UTC"
  },
  {
    "arxiv_id": "2502.12154v1",
    "title": "Diffusion Models without Classifier-free Guidance",
    "authors": [
      "Zhicong Tang",
      "Jianmin Bao",
      "Dong Chen",
      "Baining Guo"
    ],
    "abstract": "This paper presents Model-guidance (MG), a novel objective for training\ndiffusion model that addresses and removes of the commonly used Classifier-free\nguidance (CFG). Our innovative approach transcends the standard modeling of\nsolely data distribution to incorporating the posterior probability of\nconditions. The proposed technique originates from the idea of CFG and is easy\nyet effective, making it a plug-and-play module for existing models. Our method\nsignificantly accelerates the training process, doubles the inference speed,\nand achieve exceptional quality that parallel and even surpass concurrent\ndiffusion models with CFG. Extensive experiments demonstrate the effectiveness,\nefficiency, scalability on different models and datasets. Finally, we establish\nstate-of-the-art performance on ImageNet 256 benchmarks with an FID of 1.34.\nOur code is available at https://github.com/tzco/Diffusion-wo-CFG.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12154v1",
    "published_date": "2025-02-17 18:59:50 UTC",
    "updated_date": "2025-02-17 18:59:50 UTC"
  },
  {
    "arxiv_id": "2502.12149v1",
    "title": "HARBOR: Exploring Persona Dynamics in Multi-Agent Competition",
    "authors": [
      "Kenan Jiang",
      "Li Xiong",
      "Fei Liu"
    ],
    "abstract": "We investigate factors contributing to LLM agents' success in competitive\nmulti-agent environments, using auctions as a testbed where agents bid to\nmaximize profit. The agents are equipped with bidding domain knowledge,\ndistinct personas that reflect item preferences, and a memory of auction\nhistory. Our work extends the classic auction scenario by creating a realistic\nenvironment where multiple agents bid on houses, weighing aspects such as size,\nlocation, and budget to secure the most desirable homes at the lowest prices.\nParticularly, we investigate three key questions: (a) How does a persona\ninfluence an agent's behavior in a competitive setting? (b) Can an agent\neffectively profile its competitors' behavior during auctions? (c) How can\npersona profiling be leveraged to create an advantage using strategies such as\ntheory of mind? Through a series of experiments, we analyze the behaviors of\nLLM agents and shed light on new findings. Our testbed, called HARBOR, offers a\nvaluable platform for deepening our understanding of multi-agent workflows in\ncompetitive environments.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12149v1",
    "published_date": "2025-02-17 18:58:36 UTC",
    "updated_date": "2025-02-17 18:58:36 UTC"
  },
  {
    "arxiv_id": "2502.12145v1",
    "title": "Fast or Better? Balancing Accuracy and Cost in Retrieval-Augmented Generation with Flexible User Control",
    "authors": [
      "Jinyan Su",
      "Jennifer Healey",
      "Preslav Nakov",
      "Claire Cardie"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a powerful approach to\nmitigate large language model (LLM) hallucinations by incorporating external\nknowledge retrieval. However, existing RAG frameworks often apply retrieval\nindiscriminately,leading to inefficiencies-over-retrieving when unnecessary or\nfailing to retrieve iteratively when required for complex reasoning. Recent\nadaptive retrieval strategies, though adaptively navigates these retrieval\nstrategies, predict only based on query complexity and lacks user-driven\nflexibility, making them infeasible for diverse user application needs. In this\npaper, we introduce a novel user-controllable RAG framework that enables\ndynamic adjustment of the accuracy-cost trade-off. Our approach leverages two\nclassifiers: one trained to prioritize accuracy and another to prioritize\nretrieval efficiency. Via an interpretable control parameter $\\alpha$, users\ncan seamlessly navigate between minimal-cost retrieval and high-accuracy\nretrieval based on their specific requirements. We empirically demonstrate that\nour approach effectively balances accuracy, retrieval cost, and user\ncontrollability, making it a practical and adaptable solution for real-world\napplications.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12145v1",
    "published_date": "2025-02-17 18:56:20 UTC",
    "updated_date": "2025-02-17 18:56:20 UTC"
  },
  {
    "arxiv_id": "2502.12143v2",
    "title": "Small Models Struggle to Learn from Strong Reasoners",
    "authors": [
      "Yuetai Li",
      "Xiang Yue",
      "Zhangchen Xu",
      "Fengqing Jiang",
      "Luyao Niu",
      "Bill Yuchen Lin",
      "Bhaskar Ramasubramanian",
      "Radha Poovendran"
    ],
    "abstract": "Large language models (LLMs) excel in complex reasoning tasks, and distilling\ntheir reasoning capabilities into smaller models has shown promise. However, we\nuncover an interesting phenomenon, which we term the Small Model Learnability\nGap: small models ($\\leq$3B parameters) do not consistently benefit from long\nchain-of-thought (CoT) reasoning or distillation from larger models. Instead,\nthey perform better when fine-tuned on shorter, simpler reasoning chains that\nbetter align with their intrinsic learning capacity. To address this, we\npropose Mix Distillation, a simple yet effective strategy that balances\nreasoning complexity by combining long and short CoT examples or reasoning from\nboth larger and smaller models. Our experiments demonstrate that Mix\nDistillation significantly improves small model reasoning performance compared\nto training on either data alone. These findings highlight the limitations of\ndirect strong model distillation and underscore the importance of adapting\nreasoning complexity for effective reasoning capability transfer.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12143v2",
    "published_date": "2025-02-17 18:56:15 UTC",
    "updated_date": "2025-02-22 16:23:36 UTC"
  },
  {
    "arxiv_id": "2502.12131v1",
    "title": "Transformer Dynamics: A neuroscientific approach to interpretability of large language models",
    "authors": [
      "Jesseba Fernando",
      "Grigori Guitchounts"
    ],
    "abstract": "As artificial intelligence models have exploded in scale and capability,\nunderstanding of their internal mechanisms remains a critical challenge.\nInspired by the success of dynamical systems approaches in neuroscience, here\nwe propose a novel framework for studying computations in deep learning\nsystems. We focus on the residual stream (RS) in transformer models,\nconceptualizing it as a dynamical system evolving across layers. We find that\nactivations of individual RS units exhibit strong continuity across layers,\ndespite the RS being a non-privileged basis. Activations in the RS accelerate\nand grow denser over layers, while individual units trace unstable periodic\norbits. In reduced-dimensional spaces, the RS follows a curved trajectory with\nattractor-like dynamics in the lower layers. These insights bridge dynamical\nsystems theory and mechanistic interpretability, establishing a foundation for\na \"neuroscience of AI\" that combines theoretical rigor with large-scale data\nanalysis to advance our understanding of modern neural networks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12131v1",
    "published_date": "2025-02-17 18:49:40 UTC",
    "updated_date": "2025-02-17 18:49:40 UTC"
  },
  {
    "arxiv_id": "2502.12130v1",
    "title": "Scaling Autonomous Agents via Automatic Reward Modeling And Planning",
    "authors": [
      "Zhenfang Chen",
      "Delin Chen",
      "Rui Sun",
      "Wenjun Liu",
      "Chuang Gan"
    ],
    "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across\na range of text-generation tasks. However, LLMs still struggle with problems\nrequiring multi-step decision-making and environmental feedback, such as online\nshopping, scientific reasoning, and mathematical problem-solving. Unlike pure\ntext data, collecting large-scale decision-making data is challenging.\nMoreover, many powerful LLMs are only accessible through APIs, which hinders\ntheir fine-tuning for agent tasks due to cost and complexity. To address LLM\nagents' limitations, we propose a framework that can automatically learn a\nreward model from the environment without human annotations. This model can be\nused to evaluate the action trajectories of LLM agents and provide heuristics\nfor task planning. Specifically, our approach involves employing one LLM-based\nagent to navigate an environment randomly, generating diverse action\ntrajectories. Subsequently, a separate LLM is leveraged to assign a task intent\nand synthesize a negative response alongside the correct response for each\ntrajectory. These triplets (task intent, positive response, and negative\nresponse) are then utilized as training data to optimize a reward model capable\nof scoring action trajectories. The effectiveness and generalizability of our\nframework are demonstrated through evaluations conducted on different agent\nbenchmarks. In conclusion, our proposed framework represents a significant\nadvancement in enhancing LLM agents' decision-making capabilities. By\nautomating the learning of reward models, we overcome the challenges of data\nscarcity and API limitations, potentially revolutionizing the application of\nLLMs in complex and interactive environments. This research paves the way for\nmore sophisticated AI agents capable of tackling a wide range of real-world\nproblems requiring multi-step decision-making.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "ICLR2025, Project page: https://armap-agent.github.io",
    "pdf_url": "http://arxiv.org/pdf/2502.12130v1",
    "published_date": "2025-02-17 18:49:25 UTC",
    "updated_date": "2025-02-17 18:49:25 UTC"
  },
  {
    "arxiv_id": "2502.12128v3",
    "title": "LaM-SLidE: Latent Space Modeling of Spatial Dynamical Systems via Linked Entities",
    "authors": [
      "Florian Sestak",
      "Artur Toshev",
      "Andreas Fürst",
      "Günter Klambauer",
      "Andreas Mayr",
      "Johannes Brandstetter"
    ],
    "abstract": "Generative models are spearheading recent progress in deep learning,\nshowcasing strong promise for trajectory sampling in dynamical systems as well.\nHowever, whereas latent space modeling paradigms have transformed image and\nvideo generation, similar approaches are more difficult for most dynamical\nsystems. Such systems -- from chemical molecule structures to collective human\nbehavior -- are described by interactions of entities, making them inherently\nlinked to connectivity patterns, entity conservation, and the traceability of\nentities over time. Our approach, LaM-SLidE (Latent Space Modeling of Spatial\nDynamical Systems via Linked Entities), bridges the gap between: (1) keeping\nthe traceability of individual entities in a latent system representation, and\n(2) leveraging the efficiency and scalability of recent advances in image and\nvideo generation, where pre-trained encoder and decoder enable generative\nmodeling directly in latent space. The core idea of LaM-SLidE is the\nintroduction of identifier representations (IDs) that enable the retrieval of\nentity properties and entity composition from latent system representations,\nthus fostering traceability. Experimentally, across different domains, we show\nthat LaM-SLidE performs favorably in terms of speed, accuracy, and\ngeneralizability. Code is available at https://github.com/ml-jku/LaM-SLidE .",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Project page: https://ml-jku.github.io/LaM-SLidE/",
    "pdf_url": "http://arxiv.org/pdf/2502.12128v3",
    "published_date": "2025-02-17 18:49:13 UTC",
    "updated_date": "2025-05-21 08:58:58 UTC"
  },
  {
    "arxiv_id": "2502.12125v1",
    "title": "Hypernym Bias: Unraveling Deep Classifier Training Dynamics through the Lens of Class Hierarchy",
    "authors": [
      "Roman Malashin",
      "Valeria Yachnaya",
      "Alexander Mullin"
    ],
    "abstract": "We investigate the training dynamics of deep classifiers by examining how\nhierarchical relationships between classes evolve during training. Through\nextensive experiments, we argue that the learning process in classification\nproblems can be understood through the lens of label clustering. Specifically,\nwe observe that networks tend to distinguish higher-level (hypernym) categories\nin the early stages of training, and learn more specific (hyponym) categories\nlater. We introduce a novel framework to track the evolution of the feature\nmanifold during training, revealing how the hierarchy of class relations\nemerges and refines across the network layers. Our analysis demonstrates that\nthe learned representations closely align with the semantic structure of the\ndataset, providing a quantitative description of the clustering process.\nNotably, we show that in the hypernym label space, certain properties of neural\ncollapse appear earlier than in the hyponym label space, helping to bridge the\ngap between the initial and terminal phases of learning. We believe our\nfindings offer new insights into the mechanisms driving hierarchical learning\nin deep networks, paving the way for future advancements in understanding deep\nlearning dynamics.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12125v1",
    "published_date": "2025-02-17 18:47:01 UTC",
    "updated_date": "2025-02-17 18:47:01 UTC"
  },
  {
    "arxiv_id": "2502.12120v1",
    "title": "LLMs on the Line: Data Determines Loss-to-Loss Scaling Laws",
    "authors": [
      "Prasanna Mayilvahanan",
      "Thaddäus Wiedemer",
      "Sayak Mallick",
      "Matthias Bethge",
      "Wieland Brendel"
    ],
    "abstract": "Scaling laws guide the development of large language models (LLMs) by\noffering estimates for the optimal balance of model size, tokens, and compute.\nMore recently, loss-to-loss scaling laws that relate losses across pretraining\ndatasets and downstream tasks have emerged as a powerful tool for understanding\nand improving LLM performance. In this work, we investigate which factors most\nstrongly influence loss-to-loss scaling. Our experiments reveal that the\npretraining data and tokenizer determine the scaling trend. In contrast, model\nsize, optimization hyperparameters, and even significant architectural\ndifferences, such as between transformer-based models like Llama and\nstate-space models like Mamba, have limited impact. Consequently, practitioners\nshould carefully curate suitable pretraining datasets for optimal downstream\nperformance, while architectures and other settings can be freely optimized for\ntraining efficiency.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12120v1",
    "published_date": "2025-02-17 18:45:25 UTC",
    "updated_date": "2025-02-17 18:45:25 UTC"
  },
  {
    "arxiv_id": "2502.12119v1",
    "title": "PRISM: Self-Pruning Intrinsic Selection Method for Training-Free Multimodal Data Selection",
    "authors": [
      "Jinhe Bi",
      "Yifan Wang",
      "Danqi Yan",
      "Xun Xiao",
      "Artur Hecker",
      "Volker Tresp",
      "Yunpu Ma"
    ],
    "abstract": "Visual instruction tuning refines pre-trained Multimodal Large Language\nModels (MLLMs) to enhance their real-world task performance. However, the rapid\nexpansion of visual instruction datasets introduces significant data\nredundancy, leading to excessive computational costs. Existing data selection\nmethods predominantly rely on proxy models or loss-based metrics, both of which\nimpose substantial computational overheads due to the necessity of model\ninference and backpropagation. To address this challenge, we propose PRISM, a\nnovel training-free approach for efficient multimodal data selection. Unlike\nexisting methods, PRISM eliminates the reliance on proxy models, warm-up\npretraining, and gradient-based optimization. Instead, it leverages Pearson\ncorrelation analysis to quantify the intrinsic visual encoding properties of\nMLLMs, computing a task-specific correlation score to identify high-value\ninstances. This not only enbles data-efficient selection,but maintains the\noriginal performance. Empirical evaluations across multiple MLLMs demonstrate\nthat PRISM reduces the overall time required for visual instruction tuning and\ndata selection to just 30% of conventional methods, while surpassing fully\nfine-tuned models across eight multimodal and three language understanding\nbenchmarks, achieving a 101.7% relative improvement in final performance.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12119v1",
    "published_date": "2025-02-17 18:43:41 UTC",
    "updated_date": "2025-02-17 18:43:41 UTC"
  },
  {
    "arxiv_id": "2502.12977v1",
    "title": "Time-series attribution maps with regularized contrastive learning",
    "authors": [
      "Steffen Schneider",
      "Rodrigo González Laiz",
      "Anastasiia Filippova",
      "Markus Frey",
      "Mackenzie Weygandt Mathis"
    ],
    "abstract": "Gradient-based attribution methods aim to explain decisions of deep learning\nmodels but so far lack identifiability guarantees. Here, we propose a method to\ngenerate attribution maps with identifiability guarantees by developing a\nregularized contrastive learning algorithm trained on time-series data plus a\nnew attribution method called Inverted Neuron Gradient (collectively named\nxCEBRA). We show theoretically that xCEBRA has favorable properties for\nidentifying the Jacobian matrix of the data generating process. Empirically, we\ndemonstrate robust approximation of zero vs. non-zero entries in the\nground-truth attribution map on synthetic datasets, and significant\nimprovements across previous attribution methods based on feature ablation,\nShapley values, and other gradient-based methods. Our work constitutes a first\nexample of identifiable inference of time-series attribution maps and opens\navenues to a better understanding of time-series data, such as for neural\ndynamics and decision-processes within neural networks.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "q-bio.NC"
    ],
    "primary_category": "stat.ML",
    "comment": "Accepted at The 28th International Conference on Artificial\n  Intelligence and Statistics (AISTATS 2025). Code is available at\n  https://github.com/AdaptiveMotorControlLab/CEBRA",
    "pdf_url": "http://arxiv.org/pdf/2502.12977v1",
    "published_date": "2025-02-17 18:34:25 UTC",
    "updated_date": "2025-02-17 18:34:25 UTC"
  },
  {
    "arxiv_id": "2502.12109v1",
    "title": "Personality Structured Interview for Large Language Model Simulation in Personality Research",
    "authors": [
      "Pengda Wang",
      "Huiqi Zou",
      "Hanjie Chen",
      "Tianjun Sun",
      "Ziang Xiao",
      "Frederick L. Oswald"
    ],
    "abstract": "Although psychometrics researchers have recently explored the use of large\nlanguage models (LLMs) as proxies for human participants, LLMs often fail to\ngenerate heterogeneous data with human-like diversity, which diminishes their\nvalue in advancing social science research. To address these challenges, we\nexplored the potential of the theory-informed Personality Structured Interview\n(PSI) as a tool for simulating human responses in personality research. In this\napproach, the simulation is grounded in nuanced real-human interview\ntranscripts that target the personality construct of interest. We have provided\na growing set of 357 structured interview transcripts from a representative\nsample, each containing an individual's response to 32 open-ended questions\ncarefully designed to gather theory-based personality evidence. Additionally,\ngrounded in psychometric research, we have summarized an evaluation framework\nto systematically validate LLM-generated psychometric data. Results from three\nexperiments demonstrate that well-designed structured interviews could improve\nhuman-like heterogeneity in LLM-simulated personality data and predict\npersonality-related behavioral outcomes (i.e., organizational citizenship\nbehaviors and counterproductive work behavior). We further discuss the role of\ntheory-informed structured interviews in LLM-based simulation and outline a\ngeneral framework for designing structured interviews to simulate human-like\ndata for psychometric research.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "41 Pages, 30 Tables, 5 Figures",
    "pdf_url": "http://arxiv.org/pdf/2502.12109v1",
    "published_date": "2025-02-17 18:31:57 UTC",
    "updated_date": "2025-02-17 18:31:57 UTC"
  },
  {
    "arxiv_id": "2502.12108v1",
    "title": "Using the Path of Least Resistance to Explain Deep Networks",
    "authors": [
      "Sina Salek",
      "Joseph Enguehard"
    ],
    "abstract": "Integrated Gradients (IG), a widely used axiomatic path-based attribution\nmethod, assigns importance scores to input features by integrating model\ngradients along a straight path from a baseline to the input. While effective\nin some cases, we show that straight paths can lead to flawed attributions. In\nthis paper, we identify the cause of these misattributions and propose an\nalternative approach that treats the input space as a Riemannian manifold,\ncomputing attributions by integrating gradients along geodesics. We call this\nmethod Geodesic Integrated Gradients (GIG). To approximate geodesic paths, we\nintroduce two techniques: a k-Nearest Neighbours-based approach for smaller\nmodels and a Stochastic Variational Inference-based method for larger ones.\nAdditionally, we propose a new axiom, Strong Completeness, extending the axioms\nsatisfied by IG. We show that this property is desirable for attribution\nmethods and that GIG is the only method that satisfies it. Through experiments\non both synthetic and real-world data, we demonstrate that GIG outperforms\nexisting explainability methods, including IG.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12108v1",
    "published_date": "2025-02-17 18:29:24 UTC",
    "updated_date": "2025-02-17 18:29:24 UTC"
  },
  {
    "arxiv_id": "2502.12102v1",
    "title": "Relational Norms for Human-AI Cooperation",
    "authors": [
      "Brian D. Earp",
      "Sebastian Porsdam Mann",
      "Mateo Aboy",
      "Edmond Awad",
      "Monika Betzler",
      "Marietjie Botes",
      "Rachel Calcott",
      "Mina Caraccio",
      "Nick Chater",
      "Mark Coeckelbergh",
      "Mihaela Constantinescu",
      "Hossein Dabbagh",
      "Kate Devlin",
      "Xiaojun Ding",
      "Vilius Dranseika",
      "Jim A. C. Everett",
      "Ruiping Fan",
      "Faisal Feroz",
      "Kathryn B. Francis",
      "Cindy Friedman",
      "Orsolya Friedrich",
      "Iason Gabriel",
      "Ivar Hannikainen",
      "Julie Hellmann",
      "Arasj Khodadade Jahrome",
      "Niranjan S. Janardhanan",
      "Paul Jurcys",
      "Andreas Kappes",
      "Maryam Ali Khan",
      "Gordon Kraft-Todd",
      "Maximilian Kroner Dale",
      "Simon M. Laham",
      "Benjamin Lange",
      "Muriel Leuenberger",
      "Jonathan Lewis",
      "Peng Liu",
      "David M. Lyreskog",
      "Matthijs Maas",
      "John McMillan",
      "Emilian Mihailov",
      "Timo Minssen",
      "Joshua Teperowski Monrad",
      "Kathryn Muyskens",
      "Simon Myers",
      "Sven Nyholm",
      "Alexa M. Owen",
      "Anna Puzio",
      "Christopher Register",
      "Madeline G. Reinecke",
      "Adam Safron",
      "Henry Shevlin",
      "Hayate Shimizu",
      "Peter V. Treit",
      "Cristina Voinea",
      "Karen Yan",
      "Anda Zahiu",
      "Renwen Zhang",
      "Hazem Zohny",
      "Walter Sinnott-Armstrong",
      "Ilina Singh",
      "Julian Savulescu",
      "Margaret S. Clark"
    ],
    "abstract": "How we should design and interact with social artificial intelligence depends\non the socio-relational role the AI is meant to emulate or occupy. In human\nsociety, relationships such as teacher-student, parent-child, neighbors,\nsiblings, or employer-employee are governed by specific norms that prescribe or\nproscribe cooperative functions including hierarchy, care, transaction, and\nmating. These norms shape our judgments of what is appropriate for each\npartner. For example, workplace norms may allow a boss to give orders to an\nemployee, but not vice versa, reflecting hierarchical and transactional\nexpectations. As AI agents and chatbots powered by large language models are\nincreasingly designed to serve roles analogous to human positions - such as\nassistant, mental health provider, tutor, or romantic partner - it is\nimperative to examine whether and how human relational norms should extend to\nhuman-AI interactions. Our analysis explores how differences between AI systems\nand humans, such as the absence of conscious experience and immunity to\nfatigue, may affect an AI's capacity to fulfill relationship-specific functions\nand adhere to corresponding norms. This analysis, which is a collaborative\neffort by philosophers, psychologists, relationship scientists, ethicists,\nlegal experts, and AI researchers, carries important implications for AI\nsystems design, user behavior, and regulation. While we accept that AI systems\ncan offer significant benefits such as increased availability and consistency\nin certain socio-relational roles, they also risk fostering unhealthy\ndependencies or unrealistic expectations that could spill over into human-human\nrelationships. We propose that understanding and thoughtfully shaping (or\nimplementing) suitable human-AI relational norms will be crucial for ensuring\nthat human-AI interactions are ethical, trustworthy, and favorable to human\nwell-being.",
    "categories": [
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.AI",
    "comment": "76 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.12102v1",
    "published_date": "2025-02-17 18:23:29 UTC",
    "updated_date": "2025-02-17 18:23:29 UTC"
  },
  {
    "arxiv_id": "2502.12094v1",
    "title": "A Study on Leveraging Search and Self-Feedback for Agent Reasoning",
    "authors": [
      "Karthikeyan K",
      "Michelle Yuan",
      "Elman Mansimov",
      "Katerina Margatina",
      "Anurag Pratik",
      "Daniele Bonadiman",
      "Monica Sunkara",
      "Yi Zhang",
      "Yassine Benajiba"
    ],
    "abstract": "Recent works have demonstrated that incorporating search during inference can\nsignificantly improve reasoning capabilities of language agents. Some\napproaches may make use of the ground truth or rely on model's own generated\nfeedback. The search algorithm uses this feedback to then produce values that\nwill update its criterion for exploring and exploiting various reasoning paths.\nIn this study, we investigate how search and model's self-feedback can be\nleveraged for reasoning tasks. First, we explore differences in ground-truth\nfeedback and self-feedback during search for math reasoning. Second, we observe\nlimitations in applying search techniques to more complex tasks like\ntool-calling and design domain-specific approaches to address these gaps. Our\nexperiments reveal challenges related to generalization when solely relying on\nself-feedback during search. For search to work effectively, either access to\nthe ground-truth is needed or feedback mechanisms need to be carefully designed\nfor the specific task.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Under review",
    "pdf_url": "http://arxiv.org/pdf/2502.12094v1",
    "published_date": "2025-02-17 18:12:36 UTC",
    "updated_date": "2025-02-17 18:12:36 UTC"
  },
  {
    "arxiv_id": "2502.12088v2",
    "title": "Meta-Statistical Learning: Supervised Learning of Statistical Inference",
    "authors": [
      "Maxime Peyrard",
      "Kyunghyun Cho"
    ],
    "abstract": "This work demonstrates that the tools and principles driving the success of\nlarge language models (LLMs) can be repurposed to tackle distribution-level\ntasks, where the goal is to predict properties of the data-generating\ndistribution rather than labels for individual datapoints. These tasks\nencompass statistical inference problems such as parameter estimation,\nhypothesis testing, or mutual information estimation. Framing these tasks\nwithin traditional machine learning pipelines is challenging, as supervision is\ntypically tied to individual datapoint. We propose meta-statistical learning, a\nframework inspired by multi-instance learning that reformulates statistical\ninference tasks as supervised learning problems. In this approach, entire\ndatasets are treated as single inputs to neural networks, which predict\ndistribution-level parameters. Transformer-based architectures, without\npositional encoding, provide a natural fit due to their permutation-invariance\nproperties. By training on large-scale synthetic datasets, meta-statistical\nmodels can leverage the scalability and optimization infrastructure of\nTransformer-based LLMs. We demonstrate the framework's versatility with\napplications in hypothesis testing and mutual information estimation, showing\nstrong performance, particularly for small datasets where traditional neural\nmethods struggle.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12088v2",
    "published_date": "2025-02-17 18:04:39 UTC",
    "updated_date": "2025-02-19 22:12:49 UTC"
  },
  {
    "arxiv_id": "2502.12067v1",
    "title": "TokenSkip: Controllable Chain-of-Thought Compression in LLMs",
    "authors": [
      "Heming Xia",
      "Yongqi Li",
      "Chak Tou Leong",
      "Wenjie Wang",
      "Wenjie Li"
    ],
    "abstract": "Chain-of-Thought (CoT) has been proven effective in enhancing the reasoning\ncapabilities of large language models (LLMs). Recent advancements, such as\nOpenAI's o1 and DeepSeek-R1, suggest that scaling up the length of CoT\nsequences during inference could further boost LLM reasoning performance.\nHowever, due to the autoregressive nature of LLM decoding, longer CoT outputs\nlead to a linear increase in inference latency, adversely affecting user\nexperience, particularly when the CoT exceeds 10,000 tokens. To address this\nlimitation, we analyze the semantic importance of tokens within CoT outputs and\nreveal that their contributions to reasoning vary. Building on this insight, we\npropose TokenSkip, a simple yet effective approach that enables LLMs to\nselectively skip less important tokens, allowing for controllable CoT\ncompression. Extensive experiments across various models and tasks demonstrate\nthe effectiveness of TokenSkip in reducing CoT token usage while preserving\nstrong reasoning performance. Notably, when applied to Qwen2.5-14B-Instruct,\nTokenSkip reduces reasoning tokens by 40% (from 313 to 181) on GSM8K, with less\nthan a 0.4% performance drop.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12067v1",
    "published_date": "2025-02-17 17:37:26 UTC",
    "updated_date": "2025-02-17 17:37:26 UTC"
  },
  {
    "arxiv_id": "2502.12066v1",
    "title": "CONSTRUCTA: Automating Commercial Construction Schedules in Fabrication Facilities with Large Language Models",
    "authors": [
      "Yifan Zhang",
      "Xue Yang"
    ],
    "abstract": "Automating planning with LLMs presents transformative opportunities for\ntraditional industries, yet remains underexplored. In commercial construction,\nthe complexity of automated scheduling often requires manual intervention to\nensure precision. We propose CONSTRUCTA, a novel framework leveraging LLMs to\noptimize construction schedules in complex projects like semiconductor\nfabrication. CONSTRUCTA addresses key challenges by: (1) integrating\nconstruction-specific knowledge through static RAG; (2) employing\ncontext-sampling techniques inspired by architectural expertise to provide\nrelevant input; and (3) deploying Construction DPO to align schedules with\nexpert preferences using RLHF. Experiments on proprietary data demonstrate\nperformance improvements of +42.3% in missing value prediction, +79.1% in\ndependency analysis, and +28.9% in automated planning compared to baseline\nmethods, showcasing its potential to revolutionize construction workflows and\ninspire domain-specific LLM advancements.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12066v1",
    "published_date": "2025-02-17 17:35:42 UTC",
    "updated_date": "2025-02-17 17:35:42 UTC"
  },
  {
    "arxiv_id": "2502.12064v1",
    "title": "AI-generated Text Detection with a GLTR-based Approach",
    "authors": [
      "Lucía Yan Wu",
      "Isabel Segura-Bedmar"
    ],
    "abstract": "The rise of LLMs (Large Language Models) has contributed to the improved\nperformance and development of cutting-edge NLP applications. However, these\ncan also pose risks when used maliciously, such as spreading fake news, harmful\ncontent, impersonating individuals, or facilitating school plagiarism, among\nothers. This is because LLMs can generate high-quality texts, which are\nchallenging to differentiate from those written by humans. GLTR, which stands\nfor Giant Language Model Test Room and was developed jointly by the MIT-IBM\nWatson AI Lab and HarvardNLP, is a visual tool designed to help detect\nmachine-generated texts based on GPT-2, that highlights the words in text\ndepending on the probability that they were machine-generated. One limitation\nof GLTR is that the results it returns can sometimes be ambiguous and lead to\nconfusion. This study aims to explore various ways to improve GLTR's\neffectiveness for detecting AI-generated texts within the context of the\nIberLef-AuTexTification 2023 shared task, in both English and Spanish\nlanguages. Experiment results show that our GLTR-based GPT-2 model overcomes\nthe state-of-the-art models on the English dataset with a macro F1-score of\n80.19%, except for the first ranking model (80.91%). However, for the Spanish\ndataset, we obtained a macro F1-score of 66.20%, which differs by 4.57%\ncompared to the top-performing model.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12064v1",
    "published_date": "2025-02-17 17:32:55 UTC",
    "updated_date": "2025-02-17 17:32:55 UTC"
  },
  {
    "arxiv_id": "2502.12054v1",
    "title": "PhysReason: A Comprehensive Benchmark towards Physics-Based Reasoning",
    "authors": [
      "Xinyu Zhang",
      "Yuxuan Dong",
      "Yanrui Wu",
      "Jiaxing Huang",
      "Chengyou Jia",
      "Basura Fernando",
      "Mike Zheng Shou",
      "Lingling Zhang",
      "Jun Liu"
    ],
    "abstract": "Large language models demonstrate remarkable capabilities across various\ndomains, especially mathematics and logic reasoning. However, current\nevaluations overlook physics-based reasoning - a complex task requiring physics\ntheorems and constraints. We present PhysReason, a 1,200-problem benchmark\ncomprising knowledge-based (25%) and reasoning-based (75%) problems, where the\nlatter are divided into three difficulty levels (easy, medium, hard). Notably,\nproblems require an average of 8.1 solution steps, with hard requiring 15.6,\nreflecting the complexity of physics-based reasoning. We propose the Physics\nSolution Auto Scoring Framework, incorporating efficient answer-level and\ncomprehensive step-level evaluations. Top-performing models like Deepseek-R1,\nGemini-2.0-Flash-Thinking, and o3-mini-high achieve less than 60% on\nanswer-level evaluation, with performance dropping from knowledge questions\n(75.11%) to hard problems (31.95%). Through step-level evaluation, we\nidentified four key bottlenecks: Physics Theorem Application, Physics Process\nUnderstanding, Calculation, and Physics Condition Analysis. These findings\nposition PhysReason as a novel and comprehensive benchmark for evaluating\nphysics-based reasoning capabilities in large language models. Our code and\ndata will be published at https:/dxzxy12138.github.io/PhysReason.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12054v1",
    "published_date": "2025-02-17 17:24:14 UTC",
    "updated_date": "2025-02-17 17:24:14 UTC"
  },
  {
    "arxiv_id": "2502.12227v1",
    "title": "Identifying the Best Transition Law",
    "authors": [
      "Mehrasa Ahmadipour",
      "élise Crepon",
      "Aurélien Garivier"
    ],
    "abstract": "Motivated by recursive learning in Markov Decision Processes, this paper\nstudies best-arm identification in bandit problems where each arm's reward is\ndrawn from a multinomial distribution with a known support. We compare the\nperformance { reached by strategies including notably LUCB without and with use\nof this knowledge. } In the first case, we use classical non-parametric\napproaches for the confidence intervals. In the second case, where a\nprobability distribution is to be estimated, we first use classical deviation\nbounds (Hoeffding and Bernstein) on each dimension independently, and then the\nEmpirical Likelihood method (EL-LUCB) on the joint probability vector. The\neffectiveness of these methods is demonstrated through simulations on scenarios\nwith varying levels of structural complexity.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12227v1",
    "published_date": "2025-02-17 17:23:52 UTC",
    "updated_date": "2025-02-17 17:23:52 UTC"
  },
  {
    "arxiv_id": "2502.12048v2",
    "title": "A Survey on Bridging EEG Signals and Generative AI: From Image and Text to Beyond",
    "authors": [
      "Shreya Shukla",
      "Jose Torres",
      "Abhijit Mishra",
      "Jacek Gwizdka",
      "Shounak Roychowdhury"
    ],
    "abstract": "Integration of Brain-Computer Interfaces (BCIs) and Generative Artificial\nIntelligence (GenAI) has opened new frontiers in brain signal decoding,\nenabling assistive communication, neural representation learning, and\nmultimodal integration. BCIs, particularly those leveraging\nElectroencephalography (EEG), provide a non-invasive means of translating\nneural activity into meaningful outputs. Recent advances in deep learning,\nincluding Generative Adversarial Networks (GANs) and Transformer-based Large\nLanguage Models (LLMs), have significantly improved EEG-based generation of\nimages, text, and speech. This paper provides a literature review of the\nstate-of-the-art in EEG-based multimodal generation, focusing on (i)\nEEG-to-image generation through GANs, Variational Autoencoders (VAEs), and\nDiffusion Models, and (ii) EEG-to-text generation leveraging Transformer based\nlanguage models and contrastive learning methods. Additionally, we discuss the\nemerging domain of EEG-to-speech synthesis, an evolving multimodal frontier. We\nhighlight key datasets, use cases, challenges, and EEG feature encoding methods\nthat underpin generative approaches. By providing a structured overview of\nEEG-based generative AI, this survey aims to equip researchers and\npractitioners with insights to advance neural decoding, enhance assistive\ntechnologies, and expand the frontiers of brain-computer interaction.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12048v2",
    "published_date": "2025-02-17 17:16:41 UTC",
    "updated_date": "2025-02-18 22:49:49 UTC"
  },
  {
    "arxiv_id": "2502.12031v1",
    "title": "Masked Latent Prediction and Classification for Self-Supervised Audio Representation Learning",
    "authors": [
      "Aurian Quelennec",
      "Pierre Chouteau",
      "Geoffroy Peeters",
      "Slim Essid"
    ],
    "abstract": "Recently, self-supervised learning methods based on masked latent prediction\nhave proven to encode input data into powerful representations. However, during\ntraining, the learned latent space can be further transformed to extract\nhigher-level information that could be more suited for downstream\nclassification tasks. Therefore, we propose a new method: MAsked latenT\nPrediction And Classification (MATPAC), which is trained with two pretext tasks\nsolved jointly. As in previous work, the first pretext task is a masked latent\nprediction task, ensuring a robust input representation in the latent space.\nThe second one is unsupervised classification, which utilises the latent\nrepresentations of the first pretext task to match probability distributions\nbetween a teacher and a student. We validate the MATPAC method by comparing it\nto other state-of-the-art proposals and conducting ablations studies. MATPAC\nreaches state-of-the-art self-supervised learning results on reference audio\nclassification datasets such as OpenMIC, GTZAN, ESC-50 and US8K and outperforms\ncomparable supervised methods results for musical auto-tagging on\nMagna-tag-a-tune.",
    "categories": [
      "cs.SD",
      "cs.AI"
    ],
    "primary_category": "cs.SD",
    "comment": "Copyright 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works",
    "pdf_url": "http://arxiv.org/pdf/2502.12031v1",
    "published_date": "2025-02-17 17:02:26 UTC",
    "updated_date": "2025-02-17 17:02:26 UTC"
  },
  {
    "arxiv_id": "2502.12029v2",
    "title": "KnowPath: Knowledge-enhanced Reasoning via LLM-generated Inference Paths over Knowledge Graphs",
    "authors": [
      "Qi Zhao",
      "Hongyu Yang",
      "Qi Song",
      "Xinwei Yao",
      "Xiangyang Li"
    ],
    "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious complex tasks, yet they still suffer from hallucinations. Introducing\nexternal knowledge, such as knowledge graph, can enhance the LLMs' ability to\nprovide factual answers. LLMs have the ability to interactively explore\nknowledge graphs. However, most approaches have been affected by insufficient\ninternal knowledge excavation in LLMs, limited generation of trustworthy\nknowledge reasoning paths, and a vague integration between internal and\nexternal knowledge. Therefore, we propose KnowPath, a knowledge-enhanced large\nmodel framework driven by the collaboration of internal and external knowledge.\nIt relies on the internal knowledge of the LLM to guide the exploration of\ninterpretable directed subgraphs in external knowledge graphs, better\nintegrating the two knowledge sources for more accurate reasoning. Extensive\nexperiments on multiple real-world datasets confirm the superiority of\nKnowPath.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12029v2",
    "published_date": "2025-02-17 17:02:01 UTC",
    "updated_date": "2025-03-13 13:22:46 UTC"
  },
  {
    "arxiv_id": "2502.12025v1",
    "title": "SafeChain: Safety of Language Models with Long Chain-of-Thought Reasoning Capabilities",
    "authors": [
      "Fengqing Jiang",
      "Zhangchen Xu",
      "Yuetai Li",
      "Luyao Niu",
      "Zhen Xiang",
      "Bo Li",
      "Bill Yuchen Lin",
      "Radha Poovendran"
    ],
    "abstract": "Emerging large reasoning models (LRMs), such as DeepSeek-R1 models, leverage\nlong chain-of-thought (CoT) reasoning to generate structured intermediate\nsteps, enhancing their reasoning capabilities. However, long CoT does not\ninherently guarantee safe outputs, potentially leading to harmful consequences\nsuch as the introduction of security vulnerabilities in code or the spread of\nmisinformation. Current research on large language model (LLM) safety usually\nfocuses on short-answer responses, overlooking the long CoT style outputs of\nLRMs. To bridge this gap, we conduct a systematic study of LRM safety. First,\nwe investigate safety evaluators calibrated against human annotations. Using\nour newly developed metrics, we thoroughly assess the safety of 12\nstate-of-the-art LRMs on StrongReject and WildJailbreak datasets. Our results\nshow that LRMs are not safe compared to their reasoning advance. Further, we\nperform a fine-grained analysis of the reasoning trace and final answer. We\nfind that three decoding strategies-ZeroThink, LessThink, and MoreThink-can\nimprove model safety without additional training. However, these strategies\neither use constrained reasoning traces or incur high inference costs. To\nbetter strengthen LRM safety, we introduce SafeChain, the first-of-its-kind\nsafety training dataset in CoT style. We fine-tune two LRMs with SafeChain,\nshowing that it not only enhances model safety but also preserves performance\nacross 6 reasoning benchmarks.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12025v1",
    "published_date": "2025-02-17 16:57:56 UTC",
    "updated_date": "2025-02-17 16:57:56 UTC"
  },
  {
    "arxiv_id": "2502.14894v1",
    "title": "FOCUS on Contamination: A Geospatial Deep Learning Framework with a Noise-Aware Loss for Surface Water PFAS Prediction",
    "authors": [
      "Jowaria Khan",
      "Alexa Friedman",
      "Sydney Evans",
      "Runzi Wang",
      "Kaley Beins",
      "David Andrews",
      "Elizabeth Bondi-Kelly"
    ],
    "abstract": "Per and polyfluoroalkyl substances (PFAS), chemicals found in products like\nnon-stick cookware, are unfortunately persistent environmental pollutants with\nsevere health risks. Accurately mapping PFAS contamination is crucial for\nguiding targeted remediation efforts and protecting public and environmental\nhealth, yet detection across large regions remains challenging due to the cost\nof testing and the difficulty of simulating their spread. In this work, we\nintroduce FOCUS, a geospatial deep learning framework with a label noise-aware\nloss function, to predict PFAS contamination in surface water over large\nregions. By integrating hydrological flow data, land cover information, and\nproximity to known PFAS sources, our approach leverages both spatial and\nenvironmental context to improve prediction accuracy. We evaluate the\nperformance of our approach through extensive ablation studies and comparative\nanalyses against baselines like sparse segmentation, as well as existing\nscientific methods, including Kriging and pollutant transport simulations.\nResults highlight our framework's potential for scalable PFAS monitoring.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY",
      "cs.LG",
      "I.2.1; I.2.10; I.4.6; I.4.9; I.4.10; J.2"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14894v1",
    "published_date": "2025-02-17 16:57:10 UTC",
    "updated_date": "2025-02-17 16:57:10 UTC"
  },
  {
    "arxiv_id": "2502.12022v2",
    "title": "Teaching LLMs According to Their Aptitude: Adaptive Reasoning for Mathematical Problem Solving",
    "authors": [
      "Xin Xu",
      "Yan Xu",
      "Tianhao Chen",
      "Yuchen Yan",
      "Chengwu Liu",
      "Zaoyu Chen",
      "Yufei Wang",
      "Yichun Yin",
      "Yasheng Wang",
      "Lifeng Shang",
      "Qun Liu"
    ],
    "abstract": "Existing approaches to mathematical reasoning with large language models\n(LLMs) rely on Chain-of-Thought (CoT) for generalizability or Tool-Integrated\nReasoning (TIR) for precise computation. While efforts have been made to\ncombine these methods, they primarily rely on post-selection or predefined\nstrategies, leaving an open question: whether LLMs can autonomously adapt their\nreasoning strategy based on their inherent capabilities. In this work, we\npropose TATA (Teaching LLMs According to Their Aptitude), an adaptive framework\nthat enables LLMs to personalize their reasoning strategy spontaneously,\naligning it with their intrinsic aptitude. TATA incorporates base-LLM-aware\ndata selection during supervised fine-tuning (SFT) to tailor training data to\nthe model's unique abilities. This approach equips LLMs to autonomously\ndetermine and apply the appropriate reasoning strategy at test time. We\nevaluate TATA through extensive experiments on six mathematical reasoning\nbenchmarks, using both general-purpose and math-specialized LLMs. Empirical\nresults demonstrate that TATA effectively combines the complementary strengths\nof CoT and TIR, achieving superior or comparable performance with improved\ninference efficiency compared to TIR alone. Further analysis underscores the\ncritical role of aptitude-aware data selection in enabling LLMs to make\neffective and adaptive reasoning decisions and align reasoning strategies with\nmodel capabilities.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.12022v2",
    "published_date": "2025-02-17 16:56:23 UTC",
    "updated_date": "2025-02-26 04:38:54 UTC"
  },
  {
    "arxiv_id": "2502.12018v2",
    "title": "Atom of Thoughts for Markov LLM Test-Time Scaling",
    "authors": [
      "Fengwei Teng",
      "Zhaoyang Yu",
      "Quan Shi",
      "Jiayi Zhang",
      "Chenglin Wu",
      "Yuyu Luo"
    ],
    "abstract": "Large Language Models (LLMs) achieve superior performance through\ntraining-time scaling, and test-time scaling further enhances their\ncapabilities by conducting effective reasoning during inference. However, as\nthe scale of reasoning increases, existing test-time scaling methods suffer\nfrom accumulated historical information, which not only wastes computational\nresources but also interferes with effective reasoning. To address this issue,\nwe observe that complex reasoning can be achieved by solving a series of\nindependent and self-contained subquestions. These subquestions are essentially\n\\textit{atomic questions}, exhibiting the memoryless property similar to Markov\nprocesses. Based on this observation, we propose Atom of Thoughts (\\our), where\neach state transition consists of decomposing the current question into a\ndependency-based directed acyclic graph and contracting its subquestions,\nforming a simplified question that maintains answer equivalence with the\noriginal problem. This answer preservation enables the iterative\n\\textit{decomposition-contraction} process to naturally form a meaningful\nMarkov reasoning process. Furthermore, these atomic states can be seamlessly\nintegrated into existing test-time scaling methods, enabling \\our to serve as a\nplug-in enhancement for improving reasoning capabilities. Experiments across\nsix benchmarks demonstrate the effectiveness of \\our both as a standalone\nframework and a plug-in enhancement. Notably, on HotpotQA, when applied to\ngpt-4o-mini, \\our achieves an \\textbf{80.6\\%} F1 score, surpassing o3-mini by\n\\textbf{3.4\\%} and DeepSeek-R1 by \\textbf{10.6\\%}. The code is available at\n\\href{https://github.com/qixucen/atom}{https://github.com/qixucen/atom}.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12018v2",
    "published_date": "2025-02-17 16:52:42 UTC",
    "updated_date": "2025-03-23 19:05:28 UTC"
  },
  {
    "arxiv_id": "2502.12007v1",
    "title": "Demographic Attributes Prediction from Speech Using WavLM Embeddings",
    "authors": [
      "Yuchen Yang",
      "Thomas Thebaud",
      "Najim Dehak"
    ],
    "abstract": "This paper introduces a general classifier based on WavLM features, to infer\ndemographic characteristics, such as age, gender, native language, education,\nand country, from speech. Demographic feature prediction plays a crucial role\nin applications like language learning, accessibility, and digital forensics,\nenabling more personalized and inclusive technologies. Leveraging pretrained\nmodels for embedding extraction, the proposed framework identifies key acoustic\nand linguistic fea-tures associated with demographic attributes, achieving a\nMean Absolute Error (MAE) of 4.94 for age prediction and over 99.81% accuracy\nfor gender classification across various datasets. Our system improves upon\nexisting models by up to relative 30% in MAE and up to relative 10% in accuracy\nand F1 scores across tasks, leveraging a diverse range of datasets and large\npretrained models to ensure robustness and generalizability. This study offers\nnew insights into speaker diversity and provides a strong foundation for future\nresearch in speech-based demographic profiling.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "6 pages, accepted by The Conference on Information Sciences and\n  Systems (CISS)",
    "pdf_url": "http://arxiv.org/pdf/2502.12007v1",
    "published_date": "2025-02-17 16:43:47 UTC",
    "updated_date": "2025-02-17 16:43:47 UTC"
  },
  {
    "arxiv_id": "2502.14893v1",
    "title": "NOTA: Multimodal Music Notation Understanding for Visual Large Language Model",
    "authors": [
      "Mingni Tang",
      "Jiajia Li",
      "Lu Yang",
      "Zhiqiang Zhang",
      "Jinghao Tian",
      "Zuchao Li",
      "Lefei Zhang",
      "Ping Wang"
    ],
    "abstract": "Symbolic music is represented in two distinct forms: two-dimensional,\nvisually intuitive score images, and one-dimensional, standardized text\nannotation sequences. While large language models have shown extraordinary\npotential in music, current research has primarily focused on unimodal symbol\nsequence text. Existing general-domain visual language models still lack the\nability of music notation understanding. Recognizing this gap, we propose NOTA,\nthe first large-scale comprehensive multimodal music notation dataset. It\nconsists of 1,019,237 records, from 3 regions of the world, and contains 3\ntasks. Based on the dataset, we trained NotaGPT, a music notation visual large\nlanguage model. Specifically, we involve a pre-alignment training phase for\ncross-modal alignment between the musical notes depicted in music score images\nand their textual representation in ABC notation. Subsequent training phases\nfocus on foundational music information extraction, followed by training on\nmusic notation analysis. Experimental results demonstrate that our NotaGPT-7B\nachieves significant improvement on music understanding, showcasing the\neffectiveness of NOTA and the training pipeline. Our datasets are open-sourced\nat https://huggingface.co/datasets/MYTH-Lab/NOTA-dataset.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14893v1",
    "published_date": "2025-02-17 16:39:19 UTC",
    "updated_date": "2025-02-17 16:39:19 UTC"
  },
  {
    "arxiv_id": "2502.11995v2",
    "title": "Presumed Cultural Identity: How Names Shape LLM Responses",
    "authors": [
      "Siddhesh Pawar",
      "Arnav Arora",
      "Lucie-Aimée Kaffee",
      "Isabelle Augenstein"
    ],
    "abstract": "Names are deeply tied to human identity. They can serve as markers of\nindividuality, cultural heritage, and personal history. However, using names as\na core indicator of identity can lead to over-simplification of complex\nidentities. When interacting with LLMs, user names are an important point of\ninformation for personalisation. Names can enter chatbot conversations through\ndirect user input (requested by chatbots), as part of task contexts such as CV\nreviews, or as built-in memory features that store user information for\npersonalisation. We study biases associated with names by measuring cultural\npresumptions in the responses generated by LLMs when presented with common\nsuggestion-seeking queries, which might involve making assumptions about the\nuser. Our analyses demonstrate strong assumptions about cultural identity\nassociated with names present in LLM generations across multiple cultures. Our\nwork has implications for designing more nuanced personalisation systems that\navoid reinforcing stereotypes while maintaining meaningful customisation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "23 Pages, 13 Figures, 4 Tables",
    "pdf_url": "http://arxiv.org/pdf/2502.11995v2",
    "published_date": "2025-02-17 16:35:15 UTC",
    "updated_date": "2025-03-10 10:48:57 UTC"
  },
  {
    "arxiv_id": "2502.11989v1",
    "title": "Characterizing Photorealism and Artifacts in Diffusion Model-Generated Images",
    "authors": [
      "Negar Kamali",
      "Karyn Nakamura",
      "Aakriti Kumar",
      "Angelos Chatzimparmpas",
      "Jessica Hullman",
      "Matthew Groh"
    ],
    "abstract": "Diffusion model-generated images can appear indistinguishable from authentic\nphotographs, but these images often contain artifacts and implausibilities that\nreveal their AI-generated provenance. Given the challenge to public trust in\nmedia posed by photorealistic AI-generated images, we conducted a large-scale\nexperiment measuring human detection accuracy on 450 diffusion-model generated\nimages and 149 real images. Based on collecting 749,828 observations and 34,675\ncomments from 50,444 participants, we find that scene complexity of an image,\nartifact types within an image, display time of an image, and human curation of\nAI-generated images all play significant roles in how accurately people\ndistinguish real from AI-generated images. Additionally, we propose a taxonomy\ncharacterizing artifacts often appearing in images generated by diffusion\nmodels. Our empirical observations and taxonomy offer nuanced insights into the\ncapabilities and limitations of diffusion models to generate photorealistic\nimages in 2024.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.HC",
    "comment": "26 pages, 24 Figures, Accepted by ACM CHI 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.11989v1",
    "published_date": "2025-02-17 16:28:15 UTC",
    "updated_date": "2025-02-17 16:28:15 UTC"
  },
  {
    "arxiv_id": "2502.11981v1",
    "title": "Machine Learning Should Maximize Welfare, Not (Only) Accuracy",
    "authors": [
      "Nir Rosenfeld",
      "Haifeng Xu"
    ],
    "abstract": "Decades of research in machine learning have given us powerful tools for\nmaking accurate predictions. But when used in social settings and on human\ninputs, better accuracy does not immediately translate to better social\noutcomes. This may not be surprising given that conventional learning\nframeworks are not designed to express societal preferences -- let alone\npromote them. This position paper argues that machine learning is currently\nmissing, and can gain much from incorporating, a proper notion of social\nwelfare. The field of welfare economics asks: how should we allocate limited\nresources to self-interested agents in a way that maximizes social benefit? We\nargue that this perspective applies to many modern applications of machine\nlearning in social contexts, and advocate for its adoption. Rather than\ndisposing of prediction, we aim to leverage this forte of machine learning for\npromoting social welfare. We demonstrate this idea by proposing a conceptual\nframework that gradually transitions from accuracy maximization (with awareness\nto welfare) to welfare maximization (via accurate prediction). We detail\napplications and use-cases for which our framework can be effective, identify\ntechnical challenges and practical opportunities, and highlight future avenues\nworth pursuing.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11981v1",
    "published_date": "2025-02-17 16:22:46 UTC",
    "updated_date": "2025-02-17 16:22:46 UTC"
  },
  {
    "arxiv_id": "2502.11969v1",
    "title": "Learning Generalizable Prompt for CLIP with Class Similarity Knowledge",
    "authors": [
      "Sehun Jung",
      "Hyang-won Lee"
    ],
    "abstract": "In vision-language models (VLMs), prompt tuning has shown its effectiveness\nin adapting models to downstream tasks. However, learned prompts struggle to\ngeneralize to unseen classes, as they tend to overfit to the classes that are\ntargeted during prompt tuning. Examining failure cases, we observed that\nlearned prompts disrupt the semantics of unseen classes, generating text\nembeddings with incorrect semantic relationships among classes. To address\nthis, we propose Similarity Alignment Regularization (SAR), which regularizes\nlearnable prompts to preserve the semantic relationships among classes captured\nby hand-crafted prompts. Specifically, we first obtain novel classes related to\nbase classes using ChatGPT-4o and utilize them as potential unseen classes\nduring prompt tuning. Then, by targeting both base and novel classes, SAR\naligns the similarity relationships among text embeddings generated by\nlearnable prompts with the similarity relationships from hand-crafted prompts.\nExtensive experiments applying SAR to existing prompt tuning methods\ndemonstrate its effectiveness in improving generalization to unseen classes.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11969v1",
    "published_date": "2025-02-17 16:18:07 UTC",
    "updated_date": "2025-02-17 16:18:07 UTC"
  },
  {
    "arxiv_id": "2502.11968v1",
    "title": "Theoretical Barriers in Bellman-Based Reinforcement Learning",
    "authors": [
      "Brieuc Pinon",
      "Raphaël Jungers",
      "Jean-Charles Delvenne"
    ],
    "abstract": "Reinforcement Learning algorithms designed for high-dimensional spaces often\nenforce the Bellman equation on a sampled subset of states, relying on\ngeneralization to propagate knowledge across the state space. In this paper, we\nidentify and formalize a fundamental limitation of this common approach.\nSpecifically, we construct counterexample problems with a simple structure that\nthis approach fails to exploit. Our findings reveal that such algorithms can\nneglect critical information about the problems, leading to inefficiencies.\nFurthermore, we extend this negative result to another approach from the\nliterature: Hindsight Experience Replay learning state-to-state reachability.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11968v1",
    "published_date": "2025-02-17 16:18:00 UTC",
    "updated_date": "2025-02-17 16:18:00 UTC"
  },
  {
    "arxiv_id": "2502.11965v2",
    "title": "A MIMO Wireless Channel Foundation Model via CIR-CSI Consistency",
    "authors": [
      "Jun Jiang",
      "Wenjun Yu",
      "Yunfan Li",
      "Yuan Gao",
      "Shugong Xu"
    ],
    "abstract": "In the field of artificial intelligence, self-supervised learning has\ndemonstrated superior generalization capabilities by leveraging large-scale\nunlabeled datasets for pretraining, which is especially critical for wireless\ncommunication models to adapt to a variety of scenarios. This paper\ninnovatively treats Channel State Information (CSI) and Channel Impulse\nResponse (CIR) as naturally aligned multi-modal data and proposes the first\nMIMO wireless channel foundation model, named CSI-CLIP. By effectively\ncapturing the joint representations of both CIR and CSI, CSI-CLIP exhibits\nremarkable adaptability across scenarios and robust feature extraction\ncapabilities. Experimental results show that in positioning task, CSI-CLIP\nreduces the mean error distance by 22%; in beam management task, it increases\naccuracy by 1% compared to traditional supervised methods, as well as in the\nchannel identification task. These improvements not only highlight the\npotential and value of CSI-CLIP in integrating sensing and communication but\nalso demonstrate its significant advantages over existing techniques. Moreover,\nviewing CSI and CIR as multi-modal pairs and contrastive learning for wireless\nchannel foundation model open up new research directions in the domain of MIMO\nwireless communications.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "6 pages, 2025 ICMLCN accepted",
    "pdf_url": "http://arxiv.org/pdf/2502.11965v2",
    "published_date": "2025-02-17 16:13:40 UTC",
    "updated_date": "2025-03-01 13:07:25 UTC"
  },
  {
    "arxiv_id": "2502.11962v1",
    "title": "Navigating the Helpfulness-Truthfulness Trade-Off with Uncertainty-Aware Instruction Fine-Tuning",
    "authors": [
      "Tianyi Wu",
      "Jingwei Ni",
      "Bryan Hooi",
      "Jiaheng Zhang",
      "Elliott Ash",
      "See-Kiong Ng",
      "Mrinmaya Sachan",
      "Markus Leippold"
    ],
    "abstract": "Instruction Fine-tuning (IFT) can enhance the helpfulness of Large Language\nModels (LLMs), but it may lower their truthfulness. This trade-off arises\nbecause IFT steers LLMs to generate responses with long-tail knowledge that is\nnot well covered during pre-training, leading to more informative but less\ntruthful answers when generalizing to unseen tasks. In this paper, we\nempirically demonstrate this helpfulness-truthfulness trade-off in IFT and\npropose $\\textbf{UNIT}$, a novel IFT paradigm to address it. UNIT teaches LLMs\nto recognize their uncertainty and explicitly reflect it at the end of their\nresponses. Experimental results show that UNIT-tuned models maintain their\nhelpfulness while distinguishing between certain and uncertain claims, thereby\nreducing hallucinations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11962v1",
    "published_date": "2025-02-17 16:10:30 UTC",
    "updated_date": "2025-02-17 16:10:30 UTC"
  },
  {
    "arxiv_id": "2502.11959v1",
    "title": "STRIVE: Structured Reasoning for Self-Improvement in Claim Verification",
    "authors": [
      "Haisong Gong",
      "Jing Li",
      "Junfei Wu",
      "Qiang Liu",
      "Shu Wu",
      "Liang Wang"
    ],
    "abstract": "Claim verification is the task of determining whether a claim is supported or\nrefuted by evidence. Self-improvement methods, where reasoning chains are\ngenerated and those leading to correct results are selected for training, have\nsucceeded in tasks like mathematical problem solving. However, in claim\nverification, this approach struggles. Low-quality reasoning chains may falsely\nmatch binary truth labels, introducing faulty reasoning into the\nself-improvement process and ultimately degrading performance. To address this,\nwe propose STRIVE: Structured Reasoning for Self-Improved Verification. Our\nmethod introduces a structured reasoning design with Claim Decomposition,\nEntity Analysis, and Evidence Grounding Verification. These components improve\nreasoning quality, reduce errors, and provide additional supervision signals\nfor self-improvement. STRIVE begins with a warm-up phase, where the base model\nis fine-tuned on a small number of annotated examples to learn the structured\nreasoning design. It is then applied to generate reasoning chains for all\ntraining examples, selecting only those that are correct and structurally sound\nfor subsequent self-improvement training. We demonstrate that STRIVE achieves\nsignificant improvements over baseline models, with a 31.4% performance gain\nover the base model and 20.7% over Chain of Thought on the HOVER datasets,\nhighlighting its effectiveness.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11959v1",
    "published_date": "2025-02-17 16:07:07 UTC",
    "updated_date": "2025-02-17 16:07:07 UTC"
  },
  {
    "arxiv_id": "2502.11949v1",
    "title": "Massively Scaling Explicit Policy-conditioned Value Functions",
    "authors": [
      "Nico Bohlinger",
      "Jan Peters"
    ],
    "abstract": "We introduce a scaling strategy for Explicit Policy-Conditioned Value\nFunctions (EPVFs) that significantly improves performance on challenging\ncontinuous-control tasks. EPVFs learn a value function V({\\theta}) that is\nexplicitly conditioned on the policy parameters, enabling direct gradient-based\nupdates to the parameters of any policy. However, EPVFs at scale struggle with\nunrestricted parameter growth and efficient exploration in the policy parameter\nspace. To address these issues, we utilize massive parallelization with\nGPU-based simulators, big batch sizes, weight clipping and scaled peturbations.\nOur results show that EPVFs can be scaled to solve complex tasks, such as a\ncustom Ant environment, and can compete with state-of-the-art Deep\nReinforcement Learning (DRL) baselines like Proximal Policy Optimization (PPO)\nand Soft Actor-Critic (SAC). We further explore action-based policy parameter\nrepresentations from previous work and specialized neural network architectures\nto efficiently handle weight-space features, which have not been used in the\ncontext of DRL before.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11949v1",
    "published_date": "2025-02-17 16:02:54 UTC",
    "updated_date": "2025-02-17 16:02:54 UTC"
  },
  {
    "arxiv_id": "2502.11946v2",
    "title": "Step-Audio: Unified Understanding and Generation in Intelligent Speech Interaction",
    "authors": [
      "Ailin Huang",
      "Boyong Wu",
      "Bruce Wang",
      "Chao Yan",
      "Chen Hu",
      "Chengli Feng",
      "Fei Tian",
      "Feiyu Shen",
      "Jingbei Li",
      "Mingrui Chen",
      "Peng Liu",
      "Ruihang Miao",
      "Wang You",
      "Xi Chen",
      "Xuerui Yang",
      "Yechang Huang",
      "Yuxiang Zhang",
      "Zheng Gong",
      "Zixin Zhang",
      "Hongyu Zhou",
      "Jianjian Sun",
      "Brian Li",
      "Chengting Feng",
      "Changyi Wan",
      "Hanpeng Hu",
      "Jianchang Wu",
      "Jiangjie Zhen",
      "Ranchen Ming",
      "Song Yuan",
      "Xuelin Zhang",
      "Yu Zhou",
      "Bingxin Li",
      "Buyun Ma",
      "Hongyuan Wang",
      "Kang An",
      "Wei Ji",
      "Wen Li",
      "Xuan Wen",
      "Xiangwen Kong",
      "Yuankai Ma",
      "Yuanwei Liang",
      "Yun Mou",
      "Bahtiyar Ahmidi",
      "Bin Wang",
      "Bo Li",
      "Changxin Miao",
      "Chen Xu",
      "Chenrun Wang",
      "Dapeng Shi",
      "Deshan Sun",
      "Dingyuan Hu",
      "Dula Sai",
      "Enle Liu",
      "Guanzhe Huang",
      "Gulin Yan",
      "Heng Wang",
      "Haonan Jia",
      "Haoyang Zhang",
      "Jiahao Gong",
      "Junjing Guo",
      "Jiashuai Liu",
      "Jiahong Liu",
      "Jie Feng",
      "Jie Wu",
      "Jiaoren Wu",
      "Jie Yang",
      "Jinguo Wang",
      "Jingyang Zhang",
      "Junzhe Lin",
      "Kaixiang Li",
      "Lei Xia",
      "Li Zhou",
      "Liang Zhao",
      "Longlong Gu",
      "Mei Chen",
      "Menglin Wu",
      "Ming Li",
      "Mingxiao Li",
      "Mingliang Li",
      "Mingyao Liang",
      "Na Wang",
      "Nie Hao",
      "Qiling Wu",
      "Qinyuan Tan",
      "Ran Sun",
      "Shuai Shuai",
      "Shaoliang Pang",
      "Shiliang Yang",
      "Shuli Gao",
      "Shanshan Yuan",
      "Siqi Liu",
      "Shihong Deng",
      "Shilei Jiang",
      "Sitong Liu",
      "Tiancheng Cao",
      "Tianyu Wang",
      "Wenjin Deng",
      "Wuxun Xie",
      "Weipeng Ming",
      "Wenqing He",
      "Wen Sun",
      "Xin Han",
      "Xin Huang",
      "Xiaomin Deng",
      "Xiaojia Liu",
      "Xin Wu",
      "Xu Zhao",
      "Yanan Wei",
      "Yanbo Yu",
      "Yang Cao",
      "Yangguang Li",
      "Yangzhen Ma",
      "Yanming Xu",
      "Yaoyu Wang",
      "Yaqiang Shi",
      "Yilei Wang",
      "Yizhuang Zhou",
      "Yinmin Zhong",
      "Yang Zhang",
      "Yaoben Wei",
      "Yu Luo",
      "Yuanwei Lu",
      "Yuhe Yin",
      "Yuchu Luo",
      "Yuanhao Ding",
      "Yuting Yan",
      "Yaqi Dai",
      "Yuxiang Yang",
      "Zhe Xie",
      "Zheng Ge",
      "Zheng Sun",
      "Zhewei Huang",
      "Zhichao Chang",
      "Zhisheng Guan",
      "Zidong Yang",
      "Zili Zhang",
      "Binxing Jiao",
      "Daxin Jiang",
      "Heung-Yeung Shum",
      "Jiansheng Chen",
      "Jing Li",
      "Shuchang Zhou",
      "Xiangyu Zhang",
      "Xinhao Zhang",
      "Yibo Zhu"
    ],
    "abstract": "Real-time speech interaction, serving as a fundamental interface for\nhuman-machine collaboration, holds immense potential. However, current\nopen-source models face limitations such as high costs in voice data\ncollection, weakness in dynamic control, and limited intelligence. To address\nthese challenges, this paper introduces Step-Audio, the first production-ready\nopen-source solution. Key contributions include: 1) a 130B-parameter unified\nspeech-text multi-modal model that achieves unified understanding and\ngeneration, with the Step-Audio-Chat version open-sourced; 2) a generative\nspeech data engine that establishes an affordable voice cloning framework and\nproduces the open-sourced lightweight Step-Audio-TTS-3B model through\ndistillation; 3) an instruction-driven fine control system enabling dynamic\nadjustments across dialects, emotions, singing, and RAP; 4) an enhanced\ncognitive architecture augmented with tool calling and role-playing abilities\nto manage complex tasks effectively. Based on our new StepEval-Audio-360\nevaluation benchmark, Step-Audio achieves state-of-the-art performance in human\nevaluations, especially in terms of instruction following. On open-source\nbenchmarks like LLaMA Question, shows 9.3% average performance improvement,\ndemonstrating our commitment to advancing the development of open-source\nmulti-modal language technologies. Our code and models are available at\nhttps://github.com/stepfun-ai/Step-Audio.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11946v2",
    "published_date": "2025-02-17 15:58:56 UTC",
    "updated_date": "2025-02-18 07:29:10 UTC"
  },
  {
    "arxiv_id": "2502.11941v1",
    "title": "Deep Spatio-Temporal Neural Network for Air Quality Reanalysis",
    "authors": [
      "Ammar Kheder",
      "Benjamin Foreback",
      "Lili Wang",
      "Zhi-Song Liu",
      "Michael Boy"
    ],
    "abstract": "Air quality prediction is key to mitigating health impacts and guiding\ndecisions, yet existing models tend to focus on temporal trends while\noverlooking spatial generalization. We propose AQ-Net, a spatiotemporal\nreanalysis model for both observed and unobserved stations in the near future.\nAQ-Net utilizes the LSTM and multi-head attention for the temporal regression.\nWe also propose a cyclic encoding technique to ensure continuous time\nrepresentation. To learn fine-grained spatial air quality estimation, we\nincorporate AQ-Net with the neural kNN to explore feature-based interpolation,\nsuch that we can fill the spatial gaps given coarse observation stations. To\ndemonstrate the efficiency of our model for spatiotemporal reanalysis, we use\ndata from 2013-2017 collected in northern China for PM2.5 analysis. Extensive\nexperiments show that AQ-Net excels in air quality reanalysis, highlighting the\npotential of hybrid spatio-temporal models to better capture environmental\ndynamics, especially in urban areas where both spatial and temporal variability\nare critical.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11941v1",
    "published_date": "2025-02-17 15:52:22 UTC",
    "updated_date": "2025-02-17 15:52:22 UTC"
  },
  {
    "arxiv_id": "2502.11937v1",
    "title": "FitLight: Federated Imitation Learning for Plug-and-Play Autonomous Traffic Signal Control",
    "authors": [
      "Yutong Ye",
      "Yingbo Zhou",
      "Zhusen Liu",
      "Xiao Du",
      "Hao Zhou",
      "Xiang Lian",
      "Mingsong Chen"
    ],
    "abstract": "Although Reinforcement Learning (RL)-based Traffic Signal Control (TSC)\nmethods have been extensively studied, their practical applications still raise\nsome serious issues such as high learning cost and poor generalizability. This\nis because the ``trial-and-error'' training style makes RL agents extremely\ndependent on the specific traffic environment, which also requires a long\nconvergence time. To address these issues, we propose a novel Federated\nImitation Learning (FIL)-based framework for multi-intersection TSC, named\nFitLight, which allows RL agents to plug-and-play for any traffic environment\nwithout additional pre-training cost. Unlike existing imitation learning\napproaches that rely on pre-training RL agents with demonstrations, FitLight\nallows real-time imitation learning and seamless transition to reinforcement\nlearning. Due to our proposed knowledge-sharing mechanism and novel hybrid\npressure-based agent design, RL agents can quickly find a best control policy\nwith only a few episodes. Moreover, for resource-constrained TSC scenarios,\nFitLight supports model pruning and heterogeneous model aggregation, such that\nRL agents can work on a micro-controller with merely 16{\\it KB} RAM and 32{\\it\nKB} ROM. Extensive experiments demonstrate that, compared to state-of-the-art\nmethods, FitLight not only provides a superior starting point but also\nconverges to a better final solution on both real-world and synthetic datasets,\neven under extreme resource limitations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11937v1",
    "published_date": "2025-02-17 15:48:46 UTC",
    "updated_date": "2025-02-17 15:48:46 UTC"
  },
  {
    "arxiv_id": "2502.11925v2",
    "title": "GRAPHGPT-O: Synergistic Multimodal Comprehension and Generation on Graphs",
    "authors": [
      "Yi Fang",
      "Bowen Jin",
      "Jiacheng Shen",
      "Sirui Ding",
      "Qiaoyu Tan",
      "Jiawei Han"
    ],
    "abstract": "The rapid development of Multimodal Large Language Models (MLLMs) has enabled\nthe integration of multiple modalities, including texts and images, within the\nlarge language model (LLM) framework. However, texts and images are usually\ninterconnected, forming a multimodal attributed graph (MMAG). It is\nunderexplored how MLLMs can incorporate the relational information\n(\\textit{i.e.}, graph structure) and semantic information (\\textit{i.e.,} texts\nand images) on such graphs for multimodal comprehension and generation. In this\npaper, we propose GraphGPT-o, which supports omni-multimodal understanding and\ncreation on MMAGs. We first comprehensively study linearization variants to\ntransform semantic and structural information as input for MLLMs. Then, we\npropose a hierarchical aligner that enables deep graph encoding, bridging the\ngap between MMAGs and MLLMs. Finally, we explore the inference choices,\nadapting MLLM to interleaved text and image generation in graph scenarios.\nExtensive experiments on three datasets from different domains demonstrate the\neffectiveness of our proposed method. Datasets and codes will be open-sourced\nupon acceptance.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11925v2",
    "published_date": "2025-02-17 15:35:36 UTC",
    "updated_date": "2025-03-08 02:59:52 UTC"
  },
  {
    "arxiv_id": "2502.11916v2",
    "title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models",
    "authors": [
      "Jiamin Su",
      "Yibo Yan",
      "Fangteng Fu",
      "Han Zhang",
      "Jingheng Ye",
      "Xiang Liu",
      "Jiahao Huo",
      "Huiyu Zhou",
      "Xuming Hu"
    ],
    "abstract": "Automated Essay Scoring (AES) plays a crucial role in educational assessment\nby providing scalable and consistent evaluations of writing tasks. However,\ntraditional AES systems face three major challenges: (1) reliance on\nhandcrafted features that limit generalizability, (2) difficulty in capturing\nfine-grained traits like coherence and argumentation, and (3) inability to\nhandle multimodal contexts. In the era of Multimodal Large Language Models\n(MLLMs), we propose EssayJudge, the first multimodal benchmark to evaluate AES\ncapabilities across lexical-, sentence-, and discourse-level traits. By\nleveraging MLLMs' strengths in trait-specific scoring and multimodal context\nunderstanding, EssayJudge aims to offer precise, context-rich evaluations\nwithout manual feature engineering, addressing longstanding AES limitations.\nOur experiments with 18 representative MLLMs reveal gaps in AES performance\ncompared to human evaluation, particularly in discourse-level traits,\nhighlighting the need for further advancements in MLLM-based AES research.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by ACL Findings 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.11916v2",
    "published_date": "2025-02-17 15:31:59 UTC",
    "updated_date": "2025-05-20 09:54:54 UTC"
  },
  {
    "arxiv_id": "2502.11915v1",
    "title": "On the robustness of ChatGPT in teaching Korean Mathematics",
    "authors": [
      "Phuong-Nam Nguyen",
      "Quang Nguyen-The",
      "An Vu-Minh",
      "Diep-Anh Nguyen",
      "Xuan-Lam Pham"
    ],
    "abstract": "ChatGPT, an Artificial Intelligence model, has the potential to revolutionize\neducation. However, its effectiveness in solving non-English questions remains\nuncertain. This study evaluates ChatGPT's robustness using 586 Korean\nmathematics questions. ChatGPT achieves 66.72% accuracy, correctly answering\n391 out of 586 questions. We also assess its ability to rate mathematics\nquestions based on eleven criteria and perform a topic analysis. Our findings\nshow that ChatGPT's ratings align with educational theory and test-taker\nperspectives. While ChatGPT performs well in question classification, it\nstruggles with non-English contexts, highlighting areas for improvement. Future\nresearch should address linguistic biases and enhance accuracy across diverse\nlanguages. Domain-specific optimizations and multilingual training could\nimprove ChatGPT's role in personalized education.",
    "categories": [
      "cs.AI",
      "math.HO",
      "I.2.7; K.3.1; G.3"
    ],
    "primary_category": "cs.AI",
    "comment": "21 pages, 12 figures, includes statistical analysis of ChatGPT's\n  robustness in solving and rating multilingual mathematics questions. Focus on\n  Korean CSAT Mathematics. Evaluates AI accuracy, rating effectiveness, and\n  topic analysis",
    "pdf_url": "http://arxiv.org/pdf/2502.11915v1",
    "published_date": "2025-02-17 15:31:27 UTC",
    "updated_date": "2025-02-17 15:31:27 UTC"
  },
  {
    "arxiv_id": "2502.12226v2",
    "title": "On Creating a Causally Grounded Usable Rating Method for Assessing the Robustness of Foundation Models Supporting Time Series",
    "authors": [
      "Kausik Lakkaraju",
      "Rachneet Kaur",
      "Parisa Zehtabi",
      "Sunandita Patra",
      "Siva Likitha Valluru",
      "Zhen Zeng",
      "Biplav Srivastava",
      "Marco Valtorta"
    ],
    "abstract": "Foundation Models (FMs) have improved time series forecasting in various\nsectors, such as finance, but their vulnerability to input disturbances can\nhinder their adoption by stakeholders, such as investors and analysts. To\naddress this, we propose a causally grounded rating framework to study the\nrobustness of Foundational Models for Time Series (FMTS) with respect to input\nperturbations. We evaluate our approach to the stock price prediction problem,\na well-studied problem with easily accessible public data, evaluating six\nstate-of-the-art (some multi-modal) FMTS across six prominent stocks spanning\nthree industries. The ratings proposed by our framework effectively assess the\nrobustness of FMTS and also offer actionable insights for model selection and\ndeployment. Within the scope of our study, we find that (1) multi-modal FMTS\nexhibit better robustness and accuracy compared to their uni-modal versions\nand, (2) FMTS pre-trained on time series forecasting task exhibit better\nrobustness and forecasting accuracy compared to general-purpose FMTS\npre-trained across diverse settings. Further, to validate our framework's\nusability, we conduct a user study showcasing FMTS prediction errors along with\nour computed ratings. The study confirmed that our ratings reduced the\ndifficulty for users in comparing the robustness of different systems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12226v2",
    "published_date": "2025-02-17 15:26:16 UTC",
    "updated_date": "2025-03-31 22:32:58 UTC"
  },
  {
    "arxiv_id": "2502.11897v2",
    "title": "DLFR-VAE: Dynamic Latent Frame Rate VAE for Video Generation",
    "authors": [
      "Zhihang Yuan",
      "Siyuan Wang",
      "Rui Xie",
      "Hanling Zhang",
      "Tongcheng Fang",
      "Yuzhang Shang",
      "Shengen Yan",
      "Guohao Dai",
      "Yu Wang"
    ],
    "abstract": "In this paper, we propose the Dynamic Latent Frame Rate VAE (DLFR-VAE), a\ntraining-free paradigm that can make use of adaptive temporal compression in\nlatent space. While existing video generative models apply fixed compression\nrates via pretrained VAE, we observe that real-world video content exhibits\nsubstantial temporal non-uniformity, with high-motion segments containing more\ninformation than static scenes. Based on this insight, DLFR-VAE dynamically\nadjusts the latent frame rate according to the content complexity.\nSpecifically, DLFR-VAE comprises two core innovations: (1) A Dynamic Latent\nFrame Rate Scheduler that partitions videos into temporal chunks and adaptively\ndetermines optimal frame rates based on information-theoretic content\ncomplexity, and (2) A training-free adaptation mechanism that transforms\npretrained VAE architectures into a dynamic VAE that can process features with\nvariable frame rates. Our simple but effective DLFR-VAE can function as a\nplug-and-play module, seamlessly integrating with existing video generation\nmodels and accelerating the video generation process.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11897v2",
    "published_date": "2025-02-17 15:22:31 UTC",
    "updated_date": "2025-04-02 13:25:35 UTC"
  },
  {
    "arxiv_id": "2502.11896v1",
    "title": "CAMEL: Continuous Action Masking Enabled by Large Language Models for Reinforcement Learning",
    "authors": [
      "Yanxiao Zhao",
      "Yangge Qian",
      "Jingyang Shan",
      "Xiaolin Qin"
    ],
    "abstract": "Reinforcement learning (RL) in continuous action spaces encounters persistent\nchallenges, such as inefficient exploration and convergence to suboptimal\nsolutions. To address these limitations, we propose CAMEL, a novel framework\nintegrating LLM-generated suboptimal policies into the RL training pipeline.\nCAMEL leverages dynamic action masking and an adaptive epsilon-masking\nmechanism to guide exploration during early training stages while gradually\nenabling agents to optimize policies independently. At the core of CAMEL lies\nthe integration of Python-executable suboptimal policies generated by LLMs\nbased on environment descriptions and task objectives. Although simplistic and\nhard-coded, these policies offer valuable initial guidance for RL agents. To\neffectively utilize these priors, CAMEL employs masking-aware optimization to\ndynamically constrain the action space based on LLM outputs. Additionally,\nepsilon-masking gradually reduces reliance on LLM-generated guidance, enabling\nagents to transition from constrained exploration to autonomous policy\nrefinement. Experimental validation on Gymnasium MuJoCo environments\ndemonstrates the effectiveness of CAMEL. In Hopper-v4 and Ant-v4, LLM-generated\npolicies significantly improve sample efficiency, achieving performance\ncomparable to or surpassing expert masking baselines. For Walker2d-v4, where\nLLMs struggle to accurately model bipedal gait dynamics, CAMEL maintains robust\nRL performance without notable degradation, highlighting the framework's\nadaptability across diverse tasks. While CAMEL shows promise in enhancing\nsample efficiency and mitigating convergence challenges, these issues remain\nopen for further research. Future work aims to generalize CAMEL to multimodal\nLLMs for broader observation-action spaces and automate policy evaluation,\nreducing human intervention and enhancing scalability in RL training pipelines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at RLDM 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.11896v1",
    "published_date": "2025-02-17 15:22:19 UTC",
    "updated_date": "2025-02-17 15:22:19 UTC"
  },
  {
    "arxiv_id": "2502.11895v1",
    "title": "Continual Quantization-Aware Pre-Training: When to transition from 16-bit to 1.58-bit pre-training for BitNet language models?",
    "authors": [
      "Jacob Nielsen",
      "Peter Schneider-Kamp",
      "Lukas Galke"
    ],
    "abstract": "Large language models (LLMs) require immense resources for training and\ninference. Quantization, a technique that reduces the precision of model\nparameters, offers a promising solution for improving LLM efficiency and\nsustainability. While post-training quantization methods typically achieve 4-8\nbits per parameter, recent research suggests that training LLMs with 1.58 bits\nper weight parameter from scratch can maintain model accuracy while greatly\nreducing memory requirements and energy consumption at inference time. Here, we\ninvestigate a training strategy for quantization-aware pre-training, where the\nmodels are first trained with 16-bit precision and then transition into\n1.58-bit quantization-aware training. Our results on 11 downstream tasks show\nthat this 16-to-1.58-bit training strategy is preferable over full 1.58-bit\ntraining and leaves models closer to those which have undergone 16-bit\ntraining. We further investigate the effects of retaining the optimizer state\nat the transition point and gradually phasing in quantization strength --\nfinding that both techniques alleviate the magnitude of loss spikes, but also\nthat these effects can be compensated through further training.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11895v1",
    "published_date": "2025-02-17 15:21:11 UTC",
    "updated_date": "2025-02-17 15:21:11 UTC"
  },
  {
    "arxiv_id": "2502.12225v2",
    "title": "Subjective Logic Encodings",
    "authors": [
      "Jake Vasilakes",
      "Chrysoula Zerva",
      "Sophia Ananiadou"
    ],
    "abstract": "Many existing approaches for learning from labeled data assume the existence\nof gold-standard labels. According to these approaches, inter-annotator\ndisagreement is seen as noise to be removed, either through refinement of\nannotation guidelines, label adjudication, or label filtering. However,\nannotator disagreement can rarely be totally eradicated, especially on more\nsubjective tasks such as sentiment analysis or hate speech detection where\ndisagreement is natural. Therefore, a new approach to learning from labeled\ndata, called data perspectivism, seeks to leverage inter-annotator disagreement\nto learn models that stay true to the inherent uncertainty of the task by\ntreating annotations as opinions of the annotators, rather than gold-standard\nfacts. Despite this conceptual grounding, existing methods under data\nperspectivism are limited to using disagreement as the sole source of\nannotation uncertainty. To expand the possibilities of data perspectivism, we\nintroduce Subjective Logic Encodings (SLEs), a flexible framework for\nconstructing classification targets that explicitly encodes annotations as\nopinions of the annotators. Based on Subjective Logic Theory, SLEs encode\nlabels as Dirichlet distributions and provide principled methods for encoding\nand aggregating various types of annotation uncertainty -- annotator\nconfidence, reliability, and disagreement -- into the targets. We show that\nSLEs are a generalization of other types of label encodings as well as how to\nestimate models to predict SLEs using a distribution matching objective.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "We make our code publicly available at\n  https://github.com/jvasilakes/SLEncodings",
    "pdf_url": "http://arxiv.org/pdf/2502.12225v2",
    "published_date": "2025-02-17 15:14:10 UTC",
    "updated_date": "2025-03-20 15:52:29 UTC"
  },
  {
    "arxiv_id": "2502.11887v1",
    "title": "Stonefish: Supporting Machine Learning Research in Marine Robotics",
    "authors": [
      "Michele Grimaldi",
      "Patryk Cieslak",
      "Eduardo Ochoa",
      "Vibhav Bharti",
      "Hayat Rajani",
      "Ignacio Carlucho",
      "Maria Koskinopoulou",
      "Yvan R. Petillot",
      "Nuno Gracias"
    ],
    "abstract": "Simulations are highly valuable in marine robotics, offering a cost-effective\nand controlled environment for testing in the challenging conditions of\nunderwater and surface operations. Given the high costs and logistical\ndifficulties of real-world trials, simulators capable of capturing the\noperational conditions of subsea environments have become key in developing and\nrefining algorithms for remotely-operated and autonomous underwater vehicles.\nThis paper highlights recent enhancements to the Stonefish simulator, an\nadvanced open-source platform supporting development and testing of marine\nrobotics solutions. Key updates include a suite of additional sensors, such as\nan event-based camera, a thermal camera, and an optical flow camera, as well\nas, visual light communication, support for tethered operations, improved\nthruster modelling, more flexible hydrodynamics, and enhanced sonar accuracy.\nThese developments and an automated annotation tool significantly bolster\nStonefish's role in marine robotics research, especially in the field of\nmachine learning, where training data with a known ground truth is hard or\nimpossible to collect.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted as full paper at ICRA 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.11887v1",
    "published_date": "2025-02-17 15:13:41 UTC",
    "updated_date": "2025-02-17 15:13:41 UTC"
  },
  {
    "arxiv_id": "2502.11886v1",
    "title": "LIMR: Less is More for RL Scaling",
    "authors": [
      "Xuefeng Li",
      "Haoyang Zou",
      "Pengfei Liu"
    ],
    "abstract": "In this paper, we ask: what truly determines the effectiveness of RL training\ndata for enhancing language models' reasoning capabilities? While recent\nadvances like o1, Deepseek R1, and Kimi1.5 demonstrate RL's potential, the lack\nof transparency about training data requirements has hindered systematic\nprogress. Starting directly from base models without distillation, we challenge\nthe assumption that scaling up RL training data inherently improves\nperformance. we demonstrate that a strategically selected subset of just 1,389\nsamples can outperform the full 8,523-sample dataset. We introduce Learning\nImpact Measurement (LIM), an automated method to evaluate and prioritize\ntraining samples based on their alignment with model learning trajectories,\nenabling efficient resource utilization and scalable implementation. Our method\nachieves comparable or even superior performance using only 1,389 samples\nversus the full 8,523 samples dataset. Notably, while recent data-efficient\napproaches (e.g., LIMO and s1) show promise with 32B-scale models, we find it\nsignificantly underperforms at 7B-scale through supervised fine-tuning (SFT).\nIn contrast, our RL-based LIMR achieves 16.7% higher accuracy on AIME24 and\noutperforms LIMO and s1 by 13.0% and 22.2% on MATH500. These results\nfundamentally reshape our understanding of RL scaling in LLMs, demonstrating\nthat precise sample selection, rather than data scale, may be the key to\nunlocking enhanced reasoning capabilities. For reproducible research and future\ninnovation, we are open-sourcing LIMR, including implementation of LIM,\ntraining and evaluation code, curated datasets, and trained models at\nhttps://github.com/GAIR-NLP/LIMR.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "6pages",
    "pdf_url": "http://arxiv.org/pdf/2502.11886v1",
    "published_date": "2025-02-17 15:13:29 UTC",
    "updated_date": "2025-02-17 15:13:29 UTC"
  },
  {
    "arxiv_id": "2502.11882v4",
    "title": "Leveraging Dual Process Theory in Language Agent Framework for Real-time Simultaneous Human-AI Collaboration",
    "authors": [
      "Shao Zhang",
      "Xihuai Wang",
      "Wenhao Zhang",
      "Chaoran Li",
      "Junru Song",
      "Tingyu Li",
      "Lin Qiu",
      "Xuezhi Cao",
      "Xunliang Cai",
      "Wen Yao",
      "Weinan Zhang",
      "Xinbing Wang",
      "Ying Wen"
    ],
    "abstract": "Agents built on large language models (LLMs) have excelled in turn-by-turn\nhuman-AI collaboration but struggle with simultaneous tasks requiring real-time\ninteraction. Latency issues and the challenge of inferring variable human\nstrategies hinder their ability to make autonomous decisions without explicit\ninstructions. Through experiments with current independent System 1 and System\n2 methods, we validate the necessity of using Dual Process Theory (DPT) in\nreal-time tasks. We propose DPT-Agent, a novel language agent framework that\nintegrates System 1 and System 2 for efficient real-time simultaneous human-AI\ncollaboration. DPT-Agent's System 1 uses a Finite-state Machine (FSM) and\ncode-as-policy for fast, intuitive, and controllable decision-making.\nDPT-Agent's System 2 integrates Theory of Mind (ToM) and asynchronous\nreflection to infer human intentions and perform reasoning-based autonomous\ndecisions. We demonstrate the effectiveness of DPT-Agent through further\nexperiments with rule-based agents and human collaborators, showing significant\nimprovements over mainstream LLM-based frameworks. DPT-Agent can effectively\nhelp LLMs convert correct slow thinking and reasoning into executable actions,\nthereby improving performance. To the best of our knowledge, DPT-Agent is the\nfirst language agent framework that achieves successful real-time simultaneous\nhuman-AI collaboration autonomously. Code of DPT-Agent can be found in\nhttps://github.com/sjtu-marl/DPT-Agent.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "Preprint under review. Update the experimental results of the\n  DeepSeek-R1 series models, QwQ-32b, o3-mini-high and o3-mini-medium",
    "pdf_url": "http://arxiv.org/pdf/2502.11882v4",
    "published_date": "2025-02-17 15:09:45 UTC",
    "updated_date": "2025-03-10 07:25:31 UTC"
  },
  {
    "arxiv_id": "2502.11881v1",
    "title": "Hypothesis-Driven Theory-of-Mind Reasoning for Large Language Models",
    "authors": [
      "Hyunwoo Kim",
      "Melanie Sclar",
      "Tan Zhi-Xuan",
      "Lance Ying",
      "Sydney Levine",
      "Yang Liu",
      "Joshua B. Tenenbaum",
      "Yejin Choi"
    ],
    "abstract": "Existing LLM reasoning methods have shown impressive capabilities across\nvarious tasks, such as solving math and coding problems. However, applying\nthese methods to scenarios without ground-truth answers or rule-based\nverification methods - such as tracking the mental states of an agent - remains\nchallenging. Inspired by the sequential Monte Carlo algorithm, we introduce\nthought-tracing, an inference-time reasoning algorithm designed to trace the\nmental states of specific agents by generating hypotheses and weighting them\nbased on observations without relying on ground-truth solutions to questions in\ndatasets. Our algorithm is modeled after the Bayesian theory-of-mind framework,\nusing LLMs to approximate probabilistic inference over agents' evolving mental\nstates based on their perceptions and actions. We evaluate thought-tracing on\ndiverse theory-of-mind benchmarks, demonstrating significant performance\nimprovements compared to baseline LLMs. Our experiments also reveal interesting\nbehaviors of the recent reasoning models - e.g., o1 and R1 - on theory-of-mind,\nhighlighting the difference of social reasoning compared to other domains.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11881v1",
    "published_date": "2025-02-17 15:08:50 UTC",
    "updated_date": "2025-02-17 15:08:50 UTC"
  },
  {
    "arxiv_id": "2502.13171v1",
    "title": "Web Phishing Net (WPN): A scalable machine learning approach for real-time phishing campaign detection",
    "authors": [
      "Muhammad Fahad Zia",
      "Sri Harish Kalidass"
    ],
    "abstract": "Phishing is the most prevalent type of cyber-attack today and is recognized\nas the leading source of data breaches with significant consequences for both\nindividuals and corporations. Web-based phishing attacks are the most frequent\nwith vectors such as social media posts and emails containing links to phishing\nURLs that once clicked on render host systems vulnerable to more sinister\nattacks. Research efforts to detect phishing URLs have involved the use of\nsupervised learning techniques that use large amounts of data to train models\nand have high computational requirements. They also involve analysis of\nfeatures derived from vectors including email contents thus affecting user\nprivacy. Additionally, they suffer from a lack of resilience against evolution\nof threats especially with the advent of generative AI techniques to bypass\nthese systems as with AI-generated phishing URLs. Unsupervised methods such as\nclustering techniques have also been used in phishing detection in the past,\nhowever, they are at times unscalable due to the use of pair-wise comparisons.\nThey also lack high detection rates while detecting phishing campaigns. In this\npaper, we propose an unsupervised learning approach that is not only fast but\nscalable, as it does not involve pair-wise comparisons. It is able to detect\nentire campaigns at a time with a high detection rate while preserving user\nprivacy; this includes the recent surge of campaigns with targeted phishing\nURLs generated by malicious entities using generative AI techniques.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "IEEE Intelligent Cybersecurity Conference (ICSC2024)",
    "pdf_url": "http://arxiv.org/pdf/2502.13171v1",
    "published_date": "2025-02-17 15:06:56 UTC",
    "updated_date": "2025-02-17 15:06:56 UTC"
  },
  {
    "arxiv_id": "2502.11880v1",
    "title": "Bitnet.cpp: Efficient Edge Inference for Ternary LLMs",
    "authors": [
      "Jinheng Wang",
      "Hansong Zhou",
      "Ting Song",
      "Shijie Cao",
      "Yan Xia",
      "Ting Cao",
      "Jianyu Wei",
      "Shuming Ma",
      "Hongyu Wang",
      "Furu Wei"
    ],
    "abstract": "The advent of 1-bit large language models (LLMs), led by BitNet b1.58, has\nspurred interest in ternary LLMs. Despite this, research and practical\napplications focusing on efficient edge inference for ternary LLMs remain\nscarce. To bridge this gap, we introduce Bitnet.cpp, an inference system\noptimized for BitNet b1.58 and ternary LLMs. Given that mixed-precision matrix\nmultiplication (mpGEMM) constitutes the bulk of inference time in ternary LLMs,\nBitnet.cpp incorporates a novel mpGEMM library to facilitate\nsub-2-bits-per-weight, efficient and lossless inference. The library features\ntwo core solutions: Ternary Lookup Table (TL), which addresses spatial\ninefficiencies of previous bit-wise methods, and Int2 with a Scale (I2_S),\nwhich ensures lossless edge inference, both enabling high-speed inference. Our\nexperiments show that Bitnet.cpp achieves up to a 6.25x increase in speed over\nfull-precision baselines and up to 2.32x over low-bit baselines, setting new\nbenchmarks in the field. Additionally, we expand TL to element-wise lookup\ntable (ELUT) for low-bit LLMs in the appendix, presenting both theoretical and\nempirical evidence of its considerable potential. Bitnet.cpp is publicly\navailable at https://github.com/microsoft/BitNet/tree/paper , offering a\nsophisticated solution for the efficient and practical deployment of edge LLMs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "18 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.11880v1",
    "published_date": "2025-02-17 15:06:28 UTC",
    "updated_date": "2025-02-17 15:06:28 UTC"
  },
  {
    "arxiv_id": "2502.11863v1",
    "title": "FedEAT: A Robustness Optimization Framework for Federated LLMs",
    "authors": [
      "Yahao Pang",
      "Xingyuan Wu",
      "Xiaojin Zhang",
      "Wei Chen",
      "Hai Jin"
    ],
    "abstract": "Significant advancements have been made by Large Language Models (LLMs) in\nthe domains of natural language understanding and automated content creation.\nHowever, they still face persistent problems, including substantial\ncomputational costs and inadequate availability of training data. The\ncombination of Federated Learning (FL) and LLMs (federated LLMs) offers a\nsolution by leveraging distributed data while protecting privacy, which\npositions it as an ideal choice for sensitive domains. However, Federated LLMs\nstill suffer from robustness challenges, including data heterogeneity,\nmalicious clients, and adversarial attacks, which greatly hinder their\napplications. We first introduce the robustness problems in federated LLMs, to\naddress these challenges, we propose FedEAT (Federated Embedding space\nAdversarial Training), a novel framework that applies adversarial training in\nthe embedding space of client LLM and employs a robust aggregation approach,\nspecifically geometric median aggregation, to enhance the robustness of\nFederated LLMs. Our experiments demonstrate that FedEAT effectively improves\nthe robustness of Federated LLMs with minimal performance loss.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11863v1",
    "published_date": "2025-02-17 14:55:46 UTC",
    "updated_date": "2025-02-17 14:55:46 UTC"
  },
  {
    "arxiv_id": "2502.12224v2",
    "title": "Fate: Fast Edge Inference of Mixture-of-Experts Models via Cross-Layer Gate",
    "authors": [
      "Zhiyuan Fang",
      "Zicong Hong",
      "Yuegui Huang",
      "Yufeng Lyu",
      "Wuhui Chen",
      "Yue Yu",
      "Fan Yu",
      "Zibin Zheng"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, and their application in edge scenarios has attracted\nsignificant attention. However, sparse-activated Mixture-of-Experts (MoE)\nmodels, which are well suited for edge scenarios, have received relatively\nlittle attention due to their high memory demands. Offload-based methods have\nbeen proposed to address this challenge, but they face difficulties with expert\nprediction. Inaccurate expert predictions can result in prolonged inference\ndelays. To promote the application of MoE models in edge scenarios, we propose\nFate, an offloading system designed for MoE models to enable efficient\ninference in resource-constrained environments. The key insight behind Fate is\nthat gate inputs from adjacent layers can be effectively used for expert\nprefetching, achieving high prediction accuracy without additional GPU\noverhead. Furthermore, Fate employs a shallow-favoring expert caching strategy\nthat increases the expert hit rate to 99\\%. Additionally, Fate integrates\ntailored quantization strategies for cache optimization and IO efficiency.\nExperimental results show that, compared to Load on Demand and Expert\nActivation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in\nprefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively,\nwhile maintaining inference quality. Moreover, Fate's performance improvements\nare scalable across different memory budgets.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12224v2",
    "published_date": "2025-02-17 14:54:14 UTC",
    "updated_date": "2025-05-07 07:57:21 UTC"
  },
  {
    "arxiv_id": "2502.11850v1",
    "title": "Steering the LoCoMotif: Using Domain Knowledge in Time Series Motif Discovery",
    "authors": [
      "Aras Yurtman",
      "Daan Van Wesenbeeck",
      "Wannes Meert",
      "Hendrik Blockeel"
    ],
    "abstract": "Time Series Motif Discovery (TSMD) identifies repeating patterns in time\nseries data, but its unsupervised nature might result in motifs that are not\ninteresting to the user. To address this, we propose a framework that allows\nthe user to impose constraints on the motifs to be discovered, where\nconstraints can easily be defined according to the properties of the desired\nmotifs in the application domain. We also propose an efficient implementation\nof the framework, the LoCoMotif-DoK algorithm. We demonstrate that\nLoCoMotif-DoK can effectively leverage domain knowledge in real and synthetic\ndata, outperforming other TSMD techniques which only support a limited form of\ndomain knowledge.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11850v1",
    "published_date": "2025-02-17 14:44:12 UTC",
    "updated_date": "2025-02-17 14:44:12 UTC"
  },
  {
    "arxiv_id": "2502.11844v2",
    "title": "BaxBench: Can LLMs Generate Correct and Secure Backends?",
    "authors": [
      "Mark Vero",
      "Niels Mündler",
      "Victor Chibotaru",
      "Veselin Raychev",
      "Maximilian Baader",
      "Nikola Jovanović",
      "Jingxuan He",
      "Martin Vechev"
    ],
    "abstract": "The automatic generation of programs has long been a fundamental challenge in\ncomputer science. Recent benchmarks have shown that large language models\n(LLMs) can effectively generate code at the function level, make code edits,\nand solve algorithmic coding tasks. However, to achieve full automation, LLMs\nshould be able to generate production-quality, self-contained application\nmodules. To evaluate the capabilities of LLMs in solving this challenge, we\nintroduce BaxBench, a novel evaluation benchmark consisting of 392 tasks for\nthe generation of backend applications. We focus on backends for three critical\nreasons: (i) they are practically relevant, building the core components of\nmost modern web and cloud software, (ii) they are difficult to get right,\nrequiring multiple functions and files to achieve the desired functionality,\nand (iii) they are security-critical, as they are exposed to untrusted\nthird-parties, making secure solutions that prevent deployment-time attacks an\nimperative. BaxBench validates the functionality of the generated applications\nwith comprehensive test cases, and assesses their security exposure by\nexecuting end-to-end exploits. Our experiments reveal key limitations of\ncurrent LLMs in both functionality and security: (i) even the best model,\nOpenAI o1, achieves a mere 60% on code correctness; (ii) on average, we could\nsuccessfully execute security exploits on more than half of the correct\nprograms generated by each LLM; and (iii) in less popular backend frameworks,\nmodels further struggle to generate correct and secure applications. Progress\non BaxBench signifies important steps towards autonomous and secure software\ndevelopment with LLMs.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "cs.PL"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11844v2",
    "published_date": "2025-02-17 14:37:47 UTC",
    "updated_date": "2025-02-20 14:52:31 UTC"
  },
  {
    "arxiv_id": "2502.11843v1",
    "title": "Can LLM Agents Maintain a Persona in Discourse?",
    "authors": [
      "Pranav Bhandari",
      "Nicolas Fay",
      "Michael Wise",
      "Amitava Datta",
      "Stephanie Meek",
      "Usman Naseem",
      "Mehwish Nasim"
    ],
    "abstract": "Large Language Models (LLMs) are widely used as conversational agents,\nexploiting their capabilities in various sectors such as education, law,\nmedicine, and more. However, LLMs are often subjected to context-shifting\nbehaviour, resulting in a lack of consistent and interpretable\npersonality-aligned interactions. Adherence to psychological traits lacks\ncomprehensive analysis, especially in the case of dyadic (pairwise)\nconversations. We examine this challenge from two viewpoints, initially using\ntwo conversation agents to generate a discourse on a certain topic with an\nassigned personality from the OCEAN framework (Openness, Conscientiousness,\nExtraversion, Agreeableness, and Neuroticism) as High/Low for each trait. This\nis followed by using multiple judge agents to infer the original traits\nassigned to explore prediction consistency, inter-model agreement, and\nalignment with the assigned personality. Our findings indicate that while LLMs\ncan be guided toward personality-driven dialogue, their ability to maintain\npersonality traits varies significantly depending on the combination of models\nand discourse settings. These inconsistencies emphasise the challenges in\nachieving stable and interpretable personality-aligned interactions in LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11843v1",
    "published_date": "2025-02-17 14:36:39 UTC",
    "updated_date": "2025-02-17 14:36:39 UTC"
  },
  {
    "arxiv_id": "2502.11840v1",
    "title": "ChordFormer: A Conformer-Based Architecture for Large-Vocabulary Audio Chord Recognition",
    "authors": [
      "Muhammad Waseem Akram",
      "Stefano Dettori",
      "Valentina Colla",
      "Giorgio Carlo Buttazzo"
    ],
    "abstract": "Chord recognition serves as a critical task in music information retrieval\ndue to the abstract and descriptive nature of chords in music analysis. While\naudio chord recognition systems have achieved significant accuracy for small\nvocabularies (e.g., major/minor chords), large-vocabulary chord recognition\nremains a challenging problem. This complexity also arises from the inherent\nlong-tail distribution of chords, where rare chord types are underrepresented\nin most datasets, leading to insufficient training samples. Effective chord\nrecognition requires leveraging contextual information from audio sequences,\nyet existing models, such as combinations of convolutional neural networks,\nbidirectional long short-term memory networks, and bidirectional transformers,\nface limitations in capturing long-term dependencies and exhibit suboptimal\nperformance on large-vocabulary chord recognition tasks. This work proposes\nChordFormer, a novel conformer-based architecture designed to tackle structural\nchord recognition (e.g., triads, bass, sevenths) for large vocabularies.\nChordFormer leverages conformer blocks that integrate convolutional neural\nnetworks with transformers, thus enabling the model to capture both local\npatterns and global dependencies effectively. By addressing challenges such as\nclass imbalance through a reweighted loss function and structured chord\nrepresentations, ChordFormer outperforms state-of-the-art models, achieving a\n2% improvement in frame-wise accuracy and a 6% increase in class-wise accuracy\non large-vocabulary chord datasets. Furthermore, ChordFormer excels in handling\nclass imbalance, providing robust and balanced recognition across chord types.\nThis approach bridges the gap between theoretical music knowledge and practical\napplications, advancing the field of large-vocabulary chord recognition.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CV",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.SD",
    "comment": "13 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.11840v1",
    "published_date": "2025-02-17 14:35:16 UTC",
    "updated_date": "2025-02-17 14:35:16 UTC"
  },
  {
    "arxiv_id": "2502.11831v1",
    "title": "Intuitive physics understanding emerges from self-supervised pretraining on natural videos",
    "authors": [
      "Quentin Garrido",
      "Nicolas Ballas",
      "Mahmoud Assran",
      "Adrien Bardes",
      "Laurent Najman",
      "Michael Rabbat",
      "Emmanuel Dupoux",
      "Yann LeCun"
    ],
    "abstract": "We investigate the emergence of intuitive physics understanding in\ngeneral-purpose deep neural network models trained to predict masked regions in\nnatural videos. Leveraging the violation-of-expectation framework, we find that\nvideo prediction models trained to predict outcomes in a learned representation\nspace demonstrate an understanding of various intuitive physics properties,\nsuch as object permanence and shape consistency. In contrast, video prediction\nin pixel space and multimodal large language models, which reason through text,\nachieve performance closer to chance. Our comparisons of these architectures\nreveal that jointly learning an abstract representation space while predicting\nmissing parts of sensory input, akin to predictive coding, is sufficient to\nacquire an understanding of intuitive physics, and that even models trained on\none week of unique video achieve above chance performance. This challenges the\nidea that core knowledge -- a set of innate systems to help understand the\nworld -- needs to be hardwired to develop an understanding of intuitive\nphysics.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "24 pages,14 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2502.11831v1",
    "published_date": "2025-02-17 14:27:14 UTC",
    "updated_date": "2025-02-17 14:27:14 UTC"
  },
  {
    "arxiv_id": "2502.11829v1",
    "title": "Code-Vision: Evaluating Multimodal LLMs Logic Understanding and Code Generation Capabilities",
    "authors": [
      "Hanbin Wang",
      "Xiaoxuan Zhou",
      "Zhipeng Xu",
      "Keyuan Cheng",
      "Yuxin Zuo",
      "Kai Tian",
      "Jingwei Song",
      "Junting Lu",
      "Wenhui Hu",
      "Xueyang Liu"
    ],
    "abstract": "This paper introduces Code-Vision, a benchmark designed to evaluate the\nlogical understanding and code generation capabilities of Multimodal Large\nLanguage Models (MLLMs). It challenges MLLMs to generate a correct program that\nfulfills specific functionality requirements based on a given flowchart, which\nvisually represents the desired algorithm or process. Code-Vision comprises\nthree subsets: HumanEval-V, Algorithm, and MATH, which evaluate MLLMs' coding\nabilities across basic programming, algorithmic, and mathematical\nproblem-solving domains. Our experiments evaluate 12 MLLMs on Code-Vision.\nExperimental results demonstrate that there is a large performance difference\nbetween proprietary and open-source models. On Hard problems, GPT-4o can\nachieve 79.3% pass@1, but the best open-source model only achieves 15%. Further\nexperiments reveal that Code-Vision can pose unique challenges compared to\nother multimodal reasoning benchmarks MMCode and MathVista. We also explore the\nreason for the poor performance of the open-source models. All data and codes\nare available at https://github.com/wanghanbinpanda/CodeVision.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CL",
    "comment": "15 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.11829v1",
    "published_date": "2025-02-17 14:25:45 UTC",
    "updated_date": "2025-02-17 14:25:45 UTC"
  },
  {
    "arxiv_id": "2502.12222v1",
    "title": "IMPACTX: Improving Model Performance by Appropriately predicting CorrecT eXplanations",
    "authors": [
      "Andrea Apicella",
      "Salvatore Giugliano",
      "Francesco Isgrò",
      "Roberto Prevete"
    ],
    "abstract": "The eXplainable Artificial Intelligence (XAI) research predominantly\nconcentrates to provide explainations about AI model decisions, especially Deep\nLearning (DL) models. However, there is a growing interest in using XAI\ntechniques to automatically improve the performance of the AI systems\nthemselves.\n  This paper proposes IMPACTX, a novel approach that leverages XAI as a fully\nautomated attention mechanism, without requiring external knowledge or human\nfeedback. Experimental results show that IMPACTX has improved performance\nrespect to the standalone ML model by integrating an attention mechanism based\nan XAI method outputs during the model training. Furthermore, IMPACTX directly\nprovides proper feature attribution maps for the model's decisions, without\nrelying on external XAI methods during the inference process.\n  Our proposal is evaluated using three widely recognized DL models\n(EfficientNet-B2, MobileNet, and LeNet-5) along with three standard image\ndatasets: CIFAR-10, CIFAR-100, and STL-10. The results show that IMPACTX\nconsistently improves the performance of all the inspected DL models across all\nevaluated datasets, and it directly provides appropriate explanations for its\nresponses.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "in peer review",
    "pdf_url": "http://arxiv.org/pdf/2502.12222v1",
    "published_date": "2025-02-17 14:15:20 UTC",
    "updated_date": "2025-02-17 14:15:20 UTC"
  },
  {
    "arxiv_id": "2502.11817v1",
    "title": "AAKT: Enhancing Knowledge Tracing with Alternate Autoregressive Modeling",
    "authors": [
      "Hao Zhou",
      "Wenge Rong",
      "Jianfei Zhang",
      "Qing Sun",
      "Yuanxin Ouyang",
      "Zhang Xiong"
    ],
    "abstract": "Knowledge Tracing (KT) aims to predict students' future performances based on\ntheir former exercises and additional information in educational settings. KT\nhas received significant attention since it facilitates personalized\nexperiences in educational situations. Simultaneously, the autoregressive\nmodeling on the sequence of former exercises has been proven effective for this\ntask. One of the primary challenges in autoregressive modeling for Knowledge\nTracing is effectively representing the anterior (pre-response) and posterior\n(post-response) states of learners across exercises. Existing methods often\nemploy complex model architectures to update learner states using question and\nresponse records. In this study, we propose a novel perspective on knowledge\ntracing task by treating it as a generative process, consistent with the\nprinciples of autoregressive models. We demonstrate that knowledge states can\nbe directly represented through autoregressive encodings on a question-response\nalternate sequence, where model generate the most probable representation in\nhidden state space by analyzing history interactions. This approach underpins\nour framework, termed Alternate Autoregressive Knowledge Tracing (AAKT).\nAdditionally, we incorporate supplementary educational information, such as\nquestion-related skills, into our framework through an auxiliary task, and\ninclude extra exercise details, like response time, as additional inputs. Our\nproposed framework is implemented using advanced autoregressive technologies\nfrom Natural Language Generation (NLG) for both training and prediction.\nEmpirical evaluations on four real-world KT datasets indicate that AAKT\nconsistently outperforms all baseline models in terms of AUC, ACC, and RMSE.\nFurthermore, extensive ablation studies and visualized analysis validate the\neffectiveness of key components in AAKT.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11817v1",
    "published_date": "2025-02-17 14:09:51 UTC",
    "updated_date": "2025-02-17 14:09:51 UTC"
  },
  {
    "arxiv_id": "2502.11812v1",
    "title": "Towards Understanding Fine-Tuning Mechanisms of LLMs via Circuit Analysis",
    "authors": [
      "Xu Wang",
      "Yan Hu",
      "Wenyu Du",
      "Reynold Cheng",
      "Benyou Wang",
      "Difan Zou"
    ],
    "abstract": "Fine-tuning significantly improves the performance of Large Language Models\n(LLMs), yet its underlying mechanisms remain poorly understood. This paper aims\nto provide an in-depth interpretation of the fine-tuning process through\ncircuit analysis, a popular tool in Mechanistic Interpretability (MI). Unlike\nprevious studies\n\\cite{prakash2024finetuningenhancesexistingmechanisms,chhabra2024neuroplasticity}\nthat focus on tasks where pre-trained models already perform well, we develop a\nset of mathematical tasks where fine-tuning yields substantial performance\ngains, which are closer to the practical setting. In our experiments, we\nidentify circuits at various checkpoints during fine-tuning and examine the\ninterplay between circuit analysis, fine-tuning methods, and task complexities.\nFirst, we find that while circuits maintain high node similarity before and\nafter fine-tuning, their edges undergo significant changes, which is in\ncontrast to the previous work\n\\cite{prakash2024finetuningenhancesexistingmechanisms,chhabra2024neuroplasticity}\nthat show circuits only add some additional components after fine-tuning. Based\non these observations, we develop a circuit-aware Low-Rank Adaptation (LoRA)\nmethod, which assigns ranks to layers based on edge changes in the circuits.\nExperimental results demonstrate that our circuit-based LoRA algorithm achieves\nan average performance improvement of 2.46\\% over standard LoRA with similar\nparameter sizes. Furthermore, we explore how combining circuits from subtasks\ncan enhance fine-tuning in compositional tasks, providing new insights into the\ndesign of such tasks and deepening the understanding of circuit dynamics and\nfine-tuning mechanisms.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "25 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.11812v1",
    "published_date": "2025-02-17 13:59:41 UTC",
    "updated_date": "2025-02-17 13:59:41 UTC"
  },
  {
    "arxiv_id": "2502.11809v2",
    "title": "Revealing Bias Formation in Deep Neural Networks Through the Geometric Mechanisms of Human Visual Decoupling",
    "authors": [
      "Yanbiao Ma",
      "Bowei Liu",
      "Boyuan Gao",
      "Wei Dai",
      "Jiayi Chen",
      "Shuo Li"
    ],
    "abstract": "Deep neural networks (DNNs) often exhibit biases toward certain categories\nduring object recognition, even under balanced training data conditions. The\nintrinsic mechanisms underlying these biases remain unclear. Inspired by the\nhuman visual system, which decouples object manifolds through hierarchical\nprocessing to achieve object recognition, we propose a geometric analysis\nframework linking the geometric complexity of class-specific perceptual\nmanifolds in DNNs to model bias. Our findings reveal that differences in\ngeometric complexity can lead to varying recognition capabilities across\ncategories, introducing biases. To support this analysis, we present the\nPerceptual-Manifold-Geometry library, designed for calculating the geometric\nproperties of perceptual manifolds.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11809v2",
    "published_date": "2025-02-17 13:54:02 UTC",
    "updated_date": "2025-03-13 13:14:55 UTC"
  },
  {
    "arxiv_id": "2502.11799v2",
    "title": "Table-Critic: A Multi-Agent Framework for Collaborative Criticism and Refinement in Table Reasoning",
    "authors": [
      "Peiying Yu",
      "Guoxin Chen",
      "Jingjing Wang"
    ],
    "abstract": "Despite the remarkable capabilities of large language models (LLMs) in\nvarious reasoning tasks, they still struggle with table reasoning tasks,\nparticularly in maintaining consistency throughout multi-step reasoning\nprocesses. While existing approaches have explored various decomposition\nstrategies, they often lack effective mechanisms to identify and correct errors\nin intermediate reasoning steps, leading to cascading error propagation. To\naddress these issues, we propose Table-Critic, a novel multi-agent framework\nthat facilitates collaborative criticism and iterative refinement of the\nreasoning process until convergence to correct solutions. Our framework\nconsists of four specialized agents: a Judge for error identification, a Critic\nfor comprehensive critiques, a Refiner for process improvement, and a Curator\nfor pattern distillation. To effectively deal with diverse and unpredictable\nerror types, we introduce a self-evolving template tree that systematically\naccumulates critique knowledge through experience-driven learning and guides\nfuture reflections. Extensive experiments have demonstrated that Table-Critic\nachieves substantial improvements over existing methods, achieving superior\naccuracy and error correction rates while maintaining computational efficiency\nand lower solution degradation rate.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "ACL 2025 Main",
    "pdf_url": "http://arxiv.org/pdf/2502.11799v2",
    "published_date": "2025-02-17 13:42:12 UTC",
    "updated_date": "2025-05-19 14:10:55 UTC"
  },
  {
    "arxiv_id": "2502.11777v1",
    "title": "Deep Neural Networks for Accurate Depth Estimation with Latent Space Features",
    "authors": [
      "Siddiqui Muhammad Yasir",
      "Hyunsik Ahn"
    ],
    "abstract": "Depth estimation plays a pivotal role in advancing human-robot interactions,\nespecially in indoor environments where accurate 3D scene reconstruction is\nessential for tasks like navigation and object handling. Monocular depth\nestimation, which relies on a single RGB camera, offers a more affordable\nsolution compared to traditional methods that use stereo cameras or LiDAR.\nHowever, despite recent progress, many monocular approaches struggle with\naccurately defining depth boundaries, leading to less precise reconstructions.\nIn response to these challenges, this study introduces a novel depth estimation\nframework that leverages latent space features within a deep convolutional\nneural network to enhance the precision of monocular depth maps. The proposed\nmodel features dual encoder-decoder architecture, enabling both color-to-depth\nand depth-to-depth transformations. This structure allows for refined depth\nestimation through latent space encoding. To further improve the accuracy of\ndepth boundaries and local features, a new loss function is introduced. This\nfunction combines latent loss with gradient loss, helping the model maintain\nthe integrity of depth boundaries. The framework is thoroughly tested using the\nNYU Depth V2 dataset, where it sets a new benchmark, particularly excelling in\ncomplex indoor scenarios. The results clearly show that this approach\neffectively reduces depth ambiguities and blurring, making it a promising\nsolution for applications in human-robot interaction and 3D scene\nreconstruction.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11777v1",
    "published_date": "2025-02-17 13:11:35 UTC",
    "updated_date": "2025-02-17 13:11:35 UTC"
  },
  {
    "arxiv_id": "2502.11771v1",
    "title": "The Validation Gap: A Mechanistic Analysis of How Language Models Compute Arithmetic but Fail to Validate It",
    "authors": [
      "Leonardo Bertolazzi",
      "Philipp Mondorf",
      "Barbara Plank",
      "Raffaella Bernardi"
    ],
    "abstract": "The ability of large language models (LLMs) to validate their output and\nidentify potential errors is crucial for ensuring robustness and reliability.\nHowever, current research indicates that LLMs struggle with self-correction,\nencountering significant challenges in detecting errors. While studies have\nexplored methods to enhance self-correction in LLMs, relatively little\nattention has been given to understanding the models' internal mechanisms\nunderlying error detection. In this paper, we present a mechanistic analysis of\nerror detection in LLMs, focusing on simple arithmetic problems. Through\ncircuit analysis, we identify the computational subgraphs responsible for\ndetecting arithmetic errors across four smaller-sized LLMs. Our findings reveal\nthat all models heavily rely on $\\textit{consistency heads}$--attention heads\nthat assess surface-level alignment of numerical values in arithmetic\nsolutions. Moreover, we observe that the models' internal arithmetic\ncomputation primarily occurs in higher layers, whereas validation takes place\nin middle layers, before the final arithmetic results are fully encoded. This\nstructural dissociation between arithmetic computation and validation seems to\nexplain why current LLMs struggle to detect even simple arithmetic errors.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "34 pages, 31 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.11771v1",
    "published_date": "2025-02-17 13:00:44 UTC",
    "updated_date": "2025-02-17 13:00:44 UTC"
  },
  {
    "arxiv_id": "2502.11770v1",
    "title": "Cognitive-Aligned Document Selection for Retrieval-augmented Generation",
    "authors": [
      "Bingyu Wan",
      "Fuxi Zhang",
      "Zhongpeng Qi",
      "Jiayi Ding",
      "Jijun Li",
      "Baoshi Fan",
      "Yijia Zhang",
      "Jun Zhang"
    ],
    "abstract": "Large language models (LLMs) inherently display hallucinations since the\nprecision of generated texts cannot be guaranteed purely by the parametric\nknowledge they include. Although retrieval-augmented generation (RAG) systems\nenhance the accuracy and reliability of generative models by incorporating\nexternal documents, these retrieved documents often fail to adequately support\nthe model's responses in practical applications. To address this issue, we\npropose GGatrieval (Fine-\\textbf{G}rained \\textbf{G}rounded \\textbf{A}lignment\nRe\\textbf{trieval} for verifiable generation), which leverages an LLM to\ndynamically update queries and filter high-quality, reliable retrieval\ndocuments. Specifically, we parse the user query into its syntactic components\nand perform fine-grained grounded alignment with the retrieved documents. For\nquery components that cannot be individually aligned, we propose a dynamic\nsemantic compensation mechanism that iteratively refines and rewrites the query\nwhile continuously updating the retrieval results. This iterative process\ncontinues until the retrieved documents sufficiently support the query's\nresponse. Our approach introduces a novel criterion for filtering retrieved\ndocuments, closely emulating human strategies for acquiring targeted\ninformation. This ensures that the retrieved content effectively supports and\nverifies the generated outputs. On the ALCE benchmark, our method significantly\nsurpasses a wide range of baselines, achieving state-of-the-art performance.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11770v1",
    "published_date": "2025-02-17 13:00:15 UTC",
    "updated_date": "2025-02-17 13:00:15 UTC"
  },
  {
    "arxiv_id": "2502.11763v1",
    "title": "Lightweight Deepfake Detection Based on Multi-Feature Fusion",
    "authors": [
      "Siddiqui Muhammad Yasir",
      "Hyun Kim"
    ],
    "abstract": "Deepfake technology utilizes deep learning based face manipulation techniques\nto seamlessly replace faces in videos creating highly realistic but\nartificially generated content. Although this technology has beneficial\napplications in media and entertainment misuse of its capabilities may lead to\nserious risks including identity theft cyberbullying and false information. The\nintegration of DL with visual cognition has resulted in important technological\nimprovements particularly in addressing privacy risks caused by artificially\ngenerated deepfake images on digital media platforms. In this study we propose\nan efficient and lightweight method for detecting deepfake images and videos\nmaking it suitable for devices with limited computational resources. In order\nto reduce the computational burden usually associated with DL models our method\nintegrates machine learning classifiers in combination with keyframing\napproaches and texture analysis. Moreover the features extracted with a\nhistogram of oriented gradients (HOG) local binary pattern (LBP) and KAZE bands\nwere integrated to evaluate using random forest extreme gradient boosting extra\ntrees and support vector classifier algorithms. Our findings show a\nfeature-level fusion of HOG LBP and KAZE features improves accuracy to 92% and\n96% on FaceForensics++ and Celeb-DFv2 respectively.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11763v1",
    "published_date": "2025-02-17 12:55:41 UTC",
    "updated_date": "2025-02-17 12:55:41 UTC"
  },
  {
    "arxiv_id": "2502.11756v1",
    "title": "On the Computation of the Fisher Information in Continual Learning",
    "authors": [
      "Gido M. van de Ven"
    ],
    "abstract": "One of the most popular methods for continual learning with deep neural\nnetworks is Elastic Weight Consolidation (EWC), which involves computing the\nFisher Information. The exact way in which the Fisher Information is computed\nis however rarely described, and multiple different implementations for it can\nbe found online. This blog post discusses and empirically compares several\noften-used implementations, which highlights that many currently reported\nresults for EWC could likely be improved by changing the way the Fisher\nInformation is computed.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "To appear in the blogpost track at ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.11756v1",
    "published_date": "2025-02-17 12:52:10 UTC",
    "updated_date": "2025-02-17 12:52:10 UTC"
  },
  {
    "arxiv_id": "2502.11753v1",
    "title": "HintsOfTruth: A Multimodal Checkworthiness Detection Dataset with Real and Synthetic Claims",
    "authors": [
      "Michiel van der Meer",
      "Pavel Korshunov",
      "Sébastien Marcel",
      "Lonneke van der Plas"
    ],
    "abstract": "Misinformation can be countered with fact-checking, but the process is costly\nand slow. Identifying checkworthy claims is the first step, where automation\ncan help scale fact-checkers' efforts. However, detection methods struggle with\ncontent that is 1) multimodal, 2) from diverse domains, and 3) synthetic. We\nintroduce HintsOfTruth, a public dataset for multimodal checkworthiness\ndetection with $27$K real-world and synthetic image/claim pairs. The mix of\nreal and synthetic data makes this dataset unique and ideal for benchmarking\ndetection methods. We compare fine-tuned and prompted Large Language Models\n(LLMs). We find that well-configured lightweight text-based encoders perform\ncomparably to multimodal models but the first only focus on identifying\nnon-claim-like content. Multimodal LLMs can be more accurate but come at a\nsignificant computational cost, making them impractical for large-scale\napplications. When faced with synthetic data, multimodal models perform more\nrobustly",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11753v1",
    "published_date": "2025-02-17 12:49:55 UTC",
    "updated_date": "2025-02-17 12:49:55 UTC"
  },
  {
    "arxiv_id": "2502.11751v1",
    "title": "Language Models Can See Better: Visual Contrastive Decoding For LLM Multimodal Reasoning",
    "authors": [
      "Yuqi Pang",
      "Bowen Yang",
      "Haoqin Tu",
      "Yun Cao",
      "Zeyu Zhang"
    ],
    "abstract": "Although Large Language Models (LLMs) excel in reasoning and generation for\nlanguage tasks, they are not specifically designed for multimodal challenges.\nTraining Multimodal Large Language Models (MLLMs), however, is\nresource-intensive and constrained by various training limitations. In this\npaper, we propose the Modular-based Visual Contrastive Decoding (MVCD)\nframework to move this obstacle. Our framework leverages LLMs' In-Context\nLearning (ICL) capability and the proposed visual contrastive-example decoding\n(CED), specifically tailored for this framework, without requiring any\nadditional training. By converting visual signals into text and focusing on\ncontrastive output distributions during decoding, we can highlight the new\ninformation introduced by contextual examples, explore their connections, and\navoid over-reliance on prior encoded knowledge. MVCD enhances LLMs' visual\nperception to make it see and reason over the input visuals. To demonstrate\nMVCD's effectiveness, we conduct experiments with four LLMs across five\nquestion answering datasets. Our results not only show consistent improvement\nin model accuracy but well explain the effective components inside our decoding\nstrategy. Our code will be available at https://github.com/Pbhgit/MVCD.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to ICASSP 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.11751v1",
    "published_date": "2025-02-17 12:47:00 UTC",
    "updated_date": "2025-02-17 12:47:00 UTC"
  },
  {
    "arxiv_id": "2502.11749v1",
    "title": "JotlasNet: Joint Tensor Low-Rank and Attention-based Sparse Unrolling Network for Accelerating Dynamic MRI",
    "authors": [
      "Yinghao Zhang",
      "Haiyan Gui",
      "Ningdi Yang",
      "Yue Hu"
    ],
    "abstract": "Joint low-rank and sparse unrolling networks have shown superior performance\nin dynamic MRI reconstruction. However, existing works mainly utilized matrix\nlow-rank priors, neglecting the tensor characteristics of dynamic MRI images,\nand only a global threshold is applied for the sparse constraint to the\nmulti-channel data, limiting the flexibility of the network. Additionally, most\nof them have inherently complex network structure, with intricate interactions\namong variables. In this paper, we propose a novel deep unrolling network,\nJotlasNet, for dynamic MRI reconstruction by jointly utilizing tensor low-rank\nand attention-based sparse priors. Specifically, we utilize tensor low-rank\nprior to exploit the structural correlations in high-dimensional data.\nConvolutional neural networks are used to adaptively learn the low-rank and\nsparse transform domains. A novel attention-based soft thresholding operator is\nproposed to assign a unique learnable threshold to each channel of the data in\nthe CNN-learned sparse domain. The network is unrolled from the elaborately\ndesigned composite splitting algorithm and thus features a simple yet efficient\nparallel structure. Extensive experiments on two datasets (OCMR, CMRxRecon)\ndemonstrate the superior performance of JotlasNet in dynamic MRI\nreconstruction.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.4.5; I.2.6; I.4.1"
    ],
    "primary_category": "cs.CV",
    "comment": "13 pages, 7 figures, accepted by Magnetic Resonance Imaging",
    "pdf_url": "http://arxiv.org/pdf/2502.11749v1",
    "published_date": "2025-02-17 12:43:04 UTC",
    "updated_date": "2025-02-17 12:43:04 UTC"
  },
  {
    "arxiv_id": "2502.11741v2",
    "title": "SQL-o1: A Self-Reward Heuristic Dynamic Search Method for Text-to-SQL",
    "authors": [
      "Shuai Lyu",
      "Haoran Luo",
      "Ripeng Li",
      "Zhonghong Ou",
      "Jiangfeng Sun",
      "Yang Qin",
      "Xiaoran Shang",
      "Meina Song",
      "Yifan Zhu"
    ],
    "abstract": "Text-to-SQL (Text2SQL) aims to map natural language questions to executable\nSQL queries. Although large language models (LLMs) have driven significant\nprogress, current approaches struggle with poor transferability to open-source\nLLMs, limited robustness against logic and function errors in complex queries,\nand inefficiencies in structured search. We introduce SQL-o1, a\nself-reward-driven heuristic search framework built on an agent-based\narchitecture to enhance model reasoning capabilities. SQL-o1 leverages Monte\nCarlo Tree Search (MCTS) for structured, multi-step exploration, and\nincorporates a dynamic pruning strategy to accelerate inference without\nsacrificing accuracy. On the Spider and Bird benchmarks, SQL-o1 achieves a\n+10.8 execution accuracy improvement on the complex Bird dataset, surpassing\neven GPT-4-based models. Notably, it exhibits strong few-shot generalization\nand robust cross-model transferability across open-source LLMs. Our code is\navailable at:https://github.com/ShuaiLyu0110/SQL-o1.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "28 pages,12 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.11741v2",
    "published_date": "2025-02-17 12:28:11 UTC",
    "updated_date": "2025-05-21 15:21:35 UTC"
  },
  {
    "arxiv_id": "2502.11736v2",
    "title": "ReviewEval: An Evaluation Framework for AI-Generated Reviews",
    "authors": [
      "Chhavi Kirtani",
      "Madhav Krishan Garg",
      "Tejash Prasad",
      "Tanmay Singhal",
      "Murari Mandal",
      "Dhruv Kumar"
    ],
    "abstract": "The escalating volume of academic research, coupled with a shortage of\nqualified reviewers, necessitates innovative approaches to peer review. While\nlarge language model (LLMs) offer potential for automating this process, their\ncurrent limitations include superficial critiques, hallucinations, and a lack\nof actionable insights. This research addresses these challenges by introducing\na comprehensive evaluation framework for AI-generated reviews, that measures\nalignment with human evaluations, verifies factual accuracy, assesses\nanalytical depth, and identifies actionable insights. We also propose a novel\nalignment mechanism that tailors LLM-generated reviews to the unique evaluation\npriorities of individual conferences and journals. To enhance the quality of\nthese reviews, we introduce a self-refinement loop that iteratively optimizes\nthe LLM's review prompts. Our framework establishes standardized metrics for\nevaluating AI-based review systems, thereby bolstering the reliability of\nAI-generated reviews in academic research.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Under review: 8 pages, 2 figures, 2 tables, 3 pages for appendix",
    "pdf_url": "http://arxiv.org/pdf/2502.11736v2",
    "published_date": "2025-02-17 12:22:11 UTC",
    "updated_date": "2025-02-21 21:01:22 UTC"
  },
  {
    "arxiv_id": "2502.11723v1",
    "title": "Energy-Conscious LLM Decoding: Impact of Text Generation Strategies on GPU Energy Consumption",
    "authors": [
      "Alireza Nik",
      "Michael A. Riegler",
      "Pål Halvorsen"
    ],
    "abstract": "Decoding strategies significantly influence the quality and diversity of the\ngenerated texts in large language models (LLMs), yet their impact on\ncomputational resource consumption, particularly GPU energy usage, is\ninsufficiently studied. This paper investigates the relationship between text\ngeneration decoding methods and energy efficiency, focusing on the trade-off\nbetween generation quality and GPU energy consumption across diverse tasks and\ndecoding configurations. By benchmarking multiple strategies across different\ntext generation tasks, such as Translation, Code Summarization, and Math\nProblem Solving, we reveal how selecting appropriate decoding techniques with\ntheir tuned hyperparameters affects text quality and has measurable\nimplications for resource utilization, emphasizing the need for balanced\noptimization. To the best of our knowledge, this study is among the first to\nexplore decoding strategies in LLMs through the lens of energy consumption,\noffering actionable insights for designing resource-aware applications that\nmaintain high-quality text generation.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11723v1",
    "published_date": "2025-02-17 12:10:25 UTC",
    "updated_date": "2025-02-17 12:10:25 UTC"
  },
  {
    "arxiv_id": "2502.11715v1",
    "title": "Proactive Depot Discovery: A Generative Framework for Flexible Location-Routing",
    "authors": [
      "Site Qu",
      "Guoqiang Hu"
    ],
    "abstract": "The Location-Routing Problem (LRP), which combines the challenges of facility\n(depot) locating and vehicle route planning, is critically constrained by the\nreliance on predefined depot candidates, limiting the solution space and\npotentially leading to suboptimal outcomes. Previous research on LRP without\npredefined depots is scant and predominantly relies on heuristic algorithms\nthat iteratively attempt depot placements across a planar area. Such approaches\nlack the ability to proactively generate depot locations that meet specific\ngeographic requirements, revealing a notable gap in current research landscape.\nTo bridge this gap, we propose a data-driven generative DRL framework, designed\nto proactively generate depots for LRP without predefined depot candidates,\nsolely based on customer requests data which include geographic and demand\ninformation. It can operate in two distinct modes: direct generation of exact\ndepot locations, and the creation of a multivariate Gaussian distribution for\nflexible depots sampling. By extracting depots' geographic pattern from\ncustomer requests data, our approach can dynamically respond to logistical\nneeds, identifying high-quality depot locations that further reduce total\nrouting costs compared to traditional methods. Extensive experiments\ndemonstrate that, for a same group of customer requests, compared with those\ndepots identified through random attempts, our framework can proactively\ngenerate depots that lead to superior solution routes with lower routing cost.\nThe implications of our framework potentially extend into real-world\napplications, particularly in emergency medical rescue and disaster relief\nlogistics, where rapid establishment and adjustment of depot locations are\nparamount, showcasing its potential in addressing LRP for dynamic and\nunpredictable environments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11715v1",
    "published_date": "2025-02-17 12:00:28 UTC",
    "updated_date": "2025-02-17 12:00:28 UTC"
  },
  {
    "arxiv_id": "2502.11711v2",
    "title": "Knowledge-aware contrastive heterogeneous molecular graph learning",
    "authors": [
      "Mukun Chen",
      "Jia Wu",
      "Shirui Pan",
      "Fu Lin",
      "Bo Du",
      "Xiuwen Gong",
      "Wenbin Hu"
    ],
    "abstract": "Molecular representation learning is pivotal in predicting molecular\nproperties and advancing drug design. Traditional methodologies, which\npredominantly rely on homogeneous graph encoding, are limited by their\ninability to integrate external knowledge and represent molecular structures\nacross different levels of granularity. To address these limitations, we\npropose a paradigm shift by encoding molecular graphs into heterogeneous\nstructures, introducing a novel framework: Knowledge-aware Contrastive\nHeterogeneous Molecular Graph Learning (KCHML). This approach leverages\ncontrastive learning to enrich molecular representations with embedded external\nknowledge. KCHML conceptualizes molecules through three distinct graph\nviews-molecular, elemental, and pharmacological-enhanced by heterogeneous\nmolecular graphs and a dual message-passing mechanism. This design offers a\ncomprehensive representation for property prediction, as well as for downstream\ntasks such as drug-drug interaction (DDI) prediction. Extensive benchmarking\ndemonstrates KCHML's superiority over state-of-the-art molecular property\nprediction models, underscoring its ability to capture intricate molecular\nfeatures.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11711v2",
    "published_date": "2025-02-17 11:53:58 UTC",
    "updated_date": "2025-03-21 02:55:34 UTC"
  },
  {
    "arxiv_id": "2502.11705v1",
    "title": "LLM Agents Making Agent Tools",
    "authors": [
      "Georg Wölflein",
      "Dyke Ferber",
      "Daniel Truhn",
      "Ognjen Arandjelović",
      "Jakob Nikolas Kather"
    ],
    "abstract": "Tool use has turned large language models (LLMs) into powerful agents that\ncan perform complex multi-step tasks by dynamically utilising external software\ncomponents. However, these tools must be implemented in advance by human\ndevelopers, hindering the applicability of LLM agents in domains which demand\nlarge numbers of highly specialised tools, like in life sciences and medicine.\nMotivated by the growing trend of scientific studies accompanied by public code\nrepositories, we propose ToolMaker, a novel agentic framework that autonomously\ntransforms papers with code into LLM-compatible tools. Given a short task\ndescription and a repository URL, ToolMaker autonomously installs required\ndependencies and generates code to perform the task, using a closed-loop\nself-correction mechanism to iteratively diagnose and rectify errors. To\nevaluate our approach, we introduce a benchmark comprising 15 diverse and\ncomplex computational tasks spanning both medical and non-medical domains with\nover 100 unit tests to objectively assess tool correctness and robustness.\nToolMaker correctly implements 80% of the tasks, substantially outperforming\ncurrent state-of-the-art software engineering agents. ToolMaker therefore is a\nstep towards fully autonomous agent-based scientific workflows.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11705v1",
    "published_date": "2025-02-17 11:44:11 UTC",
    "updated_date": "2025-02-17 11:44:11 UTC"
  },
  {
    "arxiv_id": "2502.11687v1",
    "title": "ReVeil: Unconstrained Concealed Backdoor Attack on Deep Neural Networks using Machine Unlearning",
    "authors": [
      "Manaar Alam",
      "Hithem Lamri",
      "Michail Maniatakos"
    ],
    "abstract": "Backdoor attacks embed hidden functionalities in deep neural networks (DNN),\ntriggering malicious behavior with specific inputs. Advanced defenses monitor\nanomalous DNN inferences to detect such attacks. However, concealed backdoors\nevade detection by maintaining a low pre-deployment attack success rate (ASR)\nand restoring high ASR post-deployment via machine unlearning. Existing\nconcealed backdoors are often constrained by requiring white-box or black-box\naccess or auxiliary data, limiting their practicality when such access or data\nis unavailable. This paper introduces ReVeil, a concealed backdoor attack\ntargeting the data collection phase of the DNN training pipeline, requiring no\nmodel access or auxiliary data. ReVeil maintains low pre-deployment ASR across\nfour datasets and four trigger patterns, successfully evades three popular\nbackdoor detection methods, and restores high ASR post-deployment through\nmachine unlearning.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "This paper is accepted at 62nd Design Automation Conference (DAC)\n  2025",
    "pdf_url": "http://arxiv.org/pdf/2502.11687v1",
    "published_date": "2025-02-17 11:25:28 UTC",
    "updated_date": "2025-02-17 11:25:28 UTC"
  },
  {
    "arxiv_id": "2502.11684v1",
    "title": "MathFimer: Enhancing Mathematical Reasoning by Expanding Reasoning Steps through Fill-in-the-Middle Task",
    "authors": [
      "Yuchen Yan",
      "Yongliang Shen",
      "Yang Liu",
      "Jin Jiang",
      "Xin Xu",
      "Mengdi Zhang",
      "Jian Shao",
      "Yueting Zhuang"
    ],
    "abstract": "Mathematical reasoning represents a critical frontier in advancing large\nlanguage models (LLMs). While step-by-step approaches have emerged as the\ndominant paradigm for mathematical problem-solving in LLMs, the quality of\nreasoning steps in training data fundamentally constrains the performance of\nthe models. Recent studies has demonstrated that more detailed intermediate\nsteps can enhance model performance, yet existing methods for step expansion\neither require more powerful external models or incur substantial computational\ncosts. In this paper, we introduce MathFimer, a novel framework for\nmathematical reasoning step expansion inspired by the \"Fill-in-the-middle\" task\nfrom code completion. By decomposing solution chains into prefix-suffix pairs\nand training models to reconstruct missing intermediate steps, we develop a\nspecialized model, MathFimer-7B, on our carefully curated NuminaMath-FIM\ndataset. We then apply these models to enhance existing mathematical reasoning\ndatasets by inserting detailed intermediate steps into their solution chains,\ncreating MathFimer-expanded versions. Through comprehensive experiments on\nmultiple mathematical reasoning datasets, including MathInstruct, MetaMathQA\nand etc., we demonstrate that models trained on MathFimer-expanded data\nconsistently outperform their counterparts trained on original data across\nvarious benchmarks such as GSM8K and MATH. Our approach offers a practical,\nscalable solution for enhancing mathematical reasoning capabilities in LLMs\nwithout relying on powerful external models or expensive inference procedures.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11684v1",
    "published_date": "2025-02-17 11:22:24 UTC",
    "updated_date": "2025-02-17 11:22:24 UTC"
  },
  {
    "arxiv_id": "2502.11681v4",
    "title": "RIDE: Enhancing Large Language Model Alignment through Restyled In-Context Learning Demonstration Exemplars",
    "authors": [
      "Yuncheng Hua",
      "Lizhen Qu",
      "Zhuang Li",
      "Hao Xue",
      "Flora D. Salim",
      "Gholamreza Haffari"
    ],
    "abstract": "Alignment tuning is crucial for ensuring large language models (LLMs) behave\nethically and helpfully. Current alignment approaches require high-quality\nannotations and significant training resources. This paper proposes a low-cost,\ntuning-free method using in-context learning (ICL) to enhance LLM alignment.\nThrough an analysis of high-quality ICL demos, we identified style as a key\nfactor influencing LLM alignment capabilities and explicitly restyled ICL\nexemplars based on this stylistic framework. Additionally, we combined the\nrestyled demos to achieve a balance between the two conflicting aspects of LLM\nalignment--factuality and safety. We packaged the restyled examples as prompts\nto trigger few-shot learning, improving LLM alignment. Compared to the best\nbaseline approach, with an average score of 5.00 as the maximum, our method\nachieves a maximum 0.10 increase on the Alpaca task (from 4.50 to 4.60), a 0.22\nenhancement on the Just-eval benchmark (from 4.34 to 4.56), and a maximum\nimprovement of 0.32 (from 3.53 to 3.85) on the MT-Bench dataset. We release the\ncode and data at https://github.com/AnonymousCode-ComputerScience/RIDE.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "38 pages, 2 figures, 20 tables; The paper is under review in ARR",
    "pdf_url": "http://arxiv.org/pdf/2502.11681v4",
    "published_date": "2025-02-17 11:16:19 UTC",
    "updated_date": "2025-03-05 14:38:19 UTC"
  },
  {
    "arxiv_id": "2502.11671v1",
    "title": "Diversity-Oriented Data Augmentation with Large Language Models",
    "authors": [
      "Zaitian Wang",
      "Jinghan Zhang",
      "Xinhao Zhang",
      "Kunpeng Liu",
      "Pengfei Wang",
      "Yuanchun Zhou"
    ],
    "abstract": "Data augmentation is an essential technique in natural language processing\n(NLP) for enriching training datasets by generating diverse samples. This\nprocess is crucial for improving the robustness and generalization capabilities\nof NLP models. However, a significant challenge remains: \\textit{Insufficient\nAttention to Sample Distribution Diversity}. Most existing methods focus on\nincreasing the sample numbers while neglecting the sample distribution\ndiversity, which can lead to model overfitting. In response, we explore data\naugmentation's impact on dataset diversity and propose a\n\\textbf{\\underline{D}}iversity-\\textbf{\\underline{o}}riented data\n\\textbf{\\underline{Aug}}mentation framework (\\textbf{DoAug}). %\n\\(\\mathscr{DoAug}\\) Specifically, we utilize a diversity-oriented fine-tuning\napproach to train an LLM as a diverse paraphraser, which is capable of\naugmenting textual datasets by generating diversified paraphrases. Then, we\napply the LLM paraphraser to a selected coreset of highly informative samples\nand integrate the paraphrases with the original data to create a more diverse\naugmented dataset. Finally, we conduct extensive experiments on 12 real-world\ntextual datasets. The results show that our fine-tuned LLM augmenter improves\ndiversity while preserving label consistency, thereby enhancing the robustness\nand performance of downstream tasks. Specifically, it achieves an average\nperformance gain of \\(10.52\\%\\), surpassing the runner-up baseline with more\nthan three percentage points.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11671v1",
    "published_date": "2025-02-17 11:00:40 UTC",
    "updated_date": "2025-02-17 11:00:40 UTC"
  },
  {
    "arxiv_id": "2502.11664v2",
    "title": "VRoPE: Rotary Position Embedding for Video Large Language Models",
    "authors": [
      "Zikang Liu",
      "Longteng Guo",
      "Yepeng Tang",
      "Tongtian Yue",
      "Junxian Cai",
      "Kai Ma",
      "Qingbin Liu",
      "Xi Chen",
      "Jing Liu"
    ],
    "abstract": "Rotary Position Embedding (RoPE) has shown strong performance in text-based\nLarge Language Models (LLMs), but extending it to video remains a challenge due\nto the intricate spatiotemporal structure of video frames. Existing\nadaptations, such as RoPE-3D, attempt to encode spatial and temporal dimensions\nseparately but suffer from two major limitations: positional bias in attention\ndistribution and disruptions in video-text transitions. To overcome these\nissues, we propose Video Rotary Position Embedding (VRoPE), a novel positional\nencoding method tailored for Video-LLMs. Specifically, we introduce a more\nbalanced encoding strategy that mitigates attention biases, ensuring a more\nuniform distribution of spatial focus. Additionally, our approach restructures\npositional indices to ensure a smooth transition between video and text tokens.\nExtensive experiments on different models demonstrate that VRoPE consistently\noutperforms previous RoPE variants, achieving significant improvements in video\nunderstanding, temporal reasoning, and retrieval tasks. Code will be available\nat https://github.com/johncaged/VRoPE.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "13 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.11664v2",
    "published_date": "2025-02-17 10:53:57 UTC",
    "updated_date": "2025-05-21 06:40:36 UTC"
  },
  {
    "arxiv_id": "2502.11658v3",
    "title": "\"I'm not for sale\" -- Perceptions and limited awareness of privacy risks by digital natives about location data",
    "authors": [
      "Antoine Boutet",
      "Victor Morel"
    ],
    "abstract": "Although mobile devices benefit users in their daily lives in numerous ways,\nthey also raise several privacy concerns. For instance, they can reveal\nsensitive information that can be inferred from location data. This location\ndata is shared through service providers as well as mobile applications.\nUnderstanding how and with whom users share their location data -- as well as\nusers' perception of the underlying privacy risks --, are important notions to\ngrasp in order to design usable privacy-enhancing technologies. In this work,\nwe perform a quantitative and qualitative analysis of smartphone users'\nawareness, perception and self-reported behavior towards location data-sharing\nthrough a survey of n=99 young adult participants (i.e., digital natives). We\ncompare stated practices with actual behaviors to better understand their\nmental models, and survey participants' understanding of privacy risks before\nand after the inspection of location traces and the information that can be\ninferred therefrom.\n  Our empirical results show that participants have risky privacy practices:\nabout 54% of participants underestimate the number of mobile applications to\nwhich they have granted access to their data, and 33% forget or do not think of\nrevoking access to their data. Also, by using a demonstrator to perform\ninferences from location data, we observe that slightly more than half of\nparticipants (57%) are surprised by the extent of potentially inferred\ninformation, and that 47% intend to reduce access to their data via permissions\nas a result of using the demonstrator. Last, a majority of participants have\nlittle knowledge of the tools to better protect themselves, but are nonetheless\nwilling to follow suggestions to improve privacy (51%). Educating people,\nincluding digital natives, about privacy risks through transparency tools seems\na promising approach.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CY",
    "comment": "Accepted for publication at ICWSM2025",
    "pdf_url": "http://arxiv.org/pdf/2502.11658v3",
    "published_date": "2025-02-17 10:49:23 UTC",
    "updated_date": "2025-04-24 10:05:23 UTC"
  },
  {
    "arxiv_id": "2502.11651v1",
    "title": "MMXU: A Multi-Modal and Multi-X-ray Understanding Dataset for Disease Progression",
    "authors": [
      "Linjie Mu",
      "Zhongzhen Huang",
      "Shengqian Qin",
      "Yakun Zhu",
      "Shaoting Zhang",
      "Xiaofan Zhang"
    ],
    "abstract": "Large vision-language models (LVLMs) have shown great promise in medical\napplications, particularly in visual question answering (MedVQA) and diagnosis\nfrom medical images. However, existing datasets and models often fail to\nconsider critical aspects of medical diagnostics, such as the integration of\nhistorical records and the analysis of disease progression over time. In this\npaper, we introduce MMXU (Multimodal and MultiX-ray Understanding), a novel\ndataset for MedVQA that focuses on identifying changes in specific regions\nbetween two patient visits. Unlike previous datasets that primarily address\nsingle-image questions, MMXU enables multi-image questions, incorporating both\ncurrent and historical patient data. We demonstrate the limitations of current\nLVLMs in identifying disease progression on MMXU-\\textit{test}, even those that\nperform well on traditional benchmarks. To address this, we propose a\nMedRecord-Augmented Generation (MAG) approach, incorporating both global and\nregional historical records. Our experiments show that integrating historical\nrecords significantly enhances diagnostic accuracy by at least 20\\%, bridging\nthe gap between current LVLMs and human expert performance. Additionally, we\nfine-tune models with MAG on MMXU-\\textit{dev}, which demonstrates notable\nimprovements. We hope this work could illuminate the avenue of advancing the\nuse of LVLMs in medical diagnostics by emphasizing the importance of historical\ncontext in interpreting medical images. Our dataset is released at\n\\href{https://github.com/linjiemu/MMXU}{https://github.com/linjiemu/MMXU}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11651v1",
    "published_date": "2025-02-17 10:43:38 UTC",
    "updated_date": "2025-02-17 10:43:38 UTC"
  },
  {
    "arxiv_id": "2502.11649v2",
    "title": "Competing LLM Agents in a Non-Cooperative Game of Opinion Polarisation",
    "authors": [
      "Amin Qasmi",
      "Usman Naseem",
      "Mehwish Nasim"
    ],
    "abstract": "We introduce a novel non-cooperative game to analyse opinion formation and\nresistance, incorporating principles from social psychology such as\nconfirmation bias, resource constraints, and influence penalties. Our\nsimulation features Large Language Model (LLM) agents competing to influence a\npopulation, with penalties imposed for generating messages that propagate or\ncounter misinformation. This framework integrates resource optimisation into\nthe agents' decision-making process. Our findings demonstrate that while higher\nconfirmation bias strengthens opinion alignment within groups, it also\nexacerbates overall polarisation. Conversely, lower confirmation bias leads to\nfragmented opinions and limited shifts in individual beliefs. Investing heavily\nin a high-resource debunking strategy can initially align the population with\nthe debunking agent, but risks rapid resource depletion and diminished\nlong-term influence.",
    "categories": [
      "cs.AI",
      "cs.SI",
      "I.6; I.2.7"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11649v2",
    "published_date": "2025-02-17 10:41:55 UTC",
    "updated_date": "2025-03-10 02:14:41 UTC"
  },
  {
    "arxiv_id": "2502.13170v2",
    "title": "Unveiling the Magic of Code Reasoning through Hypothesis Decomposition and Amendment",
    "authors": [
      "Yuze Zhao",
      "Tianyun Ji",
      "Wenjun Feng",
      "Zhenya Huang",
      "Qi Liu",
      "Zhiding Liu",
      "Yixiao Ma",
      "Kai Zhang",
      "Enhong Chen"
    ],
    "abstract": "The reasoning abilities are one of the most enigmatic and captivating aspects\nof large language models (LLMs). Numerous studies are dedicated to exploring\nand expanding the boundaries of this reasoning capability. However, tasks that\nembody both reasoning and recall characteristics are often overlooked. In this\npaper, we introduce such a novel task, code reasoning, to provide a new\nperspective for the reasoning abilities of LLMs. We summarize three\nmeta-benchmarks based on established forms of logical reasoning, and\ninstantiate these into eight specific benchmark tasks. Our testing on these\nbenchmarks reveals that LLMs continue to struggle with identifying satisfactory\nreasoning pathways. Additionally, we present a new pathway exploration pipeline\ninspired by human intricate problem-solving methods. This Reflective Hypothesis\nDecomposition and Amendment (RHDA) pipeline consists of the following iterative\nsteps: (1) Proposing potential hypotheses based on observations and decomposing\nthem; (2) Utilizing tools to validate hypotheses and reflection outcomes; (3)\nRevising hypothesis in light of observations. Our approach effectively\nmitigates logical chain collapses arising from forgetting or hallucination\nissues in multi-step reasoning, resulting in performance gains of up to\n$3\\times$. Finally, we expanded this pipeline by applying it to simulate\ncomplex household tasks in real-world scenarios, specifically in VirtualHome,\nenhancing the handling of failure cases. We release our code and all of results\nat https://github.com/TnTWoW/code_reasoning.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "ICLR 2025 Poster;23 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.13170v2",
    "published_date": "2025-02-17 10:39:58 UTC",
    "updated_date": "2025-02-26 02:50:35 UTC"
  },
  {
    "arxiv_id": "2502.11647v1",
    "title": "DELMAN: Dynamic Defense Against Large Language Model Jailbreaking with Model Editing",
    "authors": [
      "Yi Wang",
      "Fenghua Weng",
      "Sibei Yang",
      "Zhan Qin",
      "Minlie Huang",
      "Wenjie Wang"
    ],
    "abstract": "Large Language Models (LLMs) are widely applied in decision making, but their\ndeployment is threatened by jailbreak attacks, where adversarial users\nmanipulate model behavior to bypass safety measures. Existing defense\nmechanisms, such as safety fine-tuning and model editing, either require\nextensive parameter modifications or lack precision, leading to performance\ndegradation on general tasks, which is unsuitable to post-deployment safety\nalignment. To address these challenges, we propose DELMAN (Dynamic Editing for\nLLMs JAilbreak DefeNse), a novel approach leveraging direct model editing for\nprecise, dynamic protection against jailbreak attacks. DELMAN directly updates\na minimal set of relevant parameters to neutralize harmful behaviors while\npreserving the model's utility. To avoid triggering a safe response in benign\ncontext, we incorporate KL-divergence regularization to ensure the updated\nmodel remains consistent with the original model when processing benign\nqueries. Experimental results demonstrate that DELMAN outperforms baseline\nmethods in mitigating jailbreak attacks while preserving the model's utility,\nand adapts seamlessly to new attack instances, providing a practical and\nefficient solution for post-deployment model protection.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11647v1",
    "published_date": "2025-02-17 10:39:21 UTC",
    "updated_date": "2025-02-17 10:39:21 UTC"
  },
  {
    "arxiv_id": "2502.11644v1",
    "title": "InTec: integrated things-edge computing: a framework for distributing machine learning pipelines in edge AI systems",
    "authors": [
      "Habib Larian",
      "Faramarz Safi-Esfahani"
    ],
    "abstract": "With the rapid expansion of the Internet of Things (IoT), sensors,\nsmartphones, and wearables have become integral to daily life, powering smart\napplications in home automation, healthcare, and intelligent transportation.\nHowever, these advancements face significant challenges due to latency and\nbandwidth constraints imposed by traditional cloud based machine learning (ML)\nframeworks. The need for innovative solutions is evident as cloud computing\nstruggles with increased latency and network congestion. Previous attempts to\noffload parts of the ML pipeline to edge and cloud layers have yet to fully\nresolve these issues, often worsening system response times and network\ncongestion due to the computational limitations of edge devices. In response to\nthese challenges, this study introduces the InTec (Integrated Things Edge\nComputing) framework, a groundbreaking innovation in IoT architecture. Unlike\nexisting methods, InTec fully leverages the potential of a three tier\narchitecture by strategically distributing ML tasks across the Things, Edge,\nand Cloud layers. This comprehensive approach enables real time data processing\nat the point of data generation, significantly reducing latency, optimizing\nnetwork traffic, and enhancing system reliability. InTec effectiveness is\nvalidated through empirical evaluation using the MHEALTH dataset for human\nmotion detection in smart homes, demonstrating notable improvements in key\nmetrics: an 81.56 percent reduction in response time, a 10.92 percent decrease\nin network traffic, a 9.82 percent improvement in throughput, a 21.86 percent\nreduction in edge energy consumption, and a 25.83 percent reduction in cloud\nenergy consumption. These advancements establish InTec as a new benchmark for\nscalable, responsive, and energy efficient IoT applications, demonstrating its\npotential to revolutionize how the ML pipeline is integrated into Edge AI (EI)\nsystems.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "68M14, 68T05"
    ],
    "primary_category": "cs.DC",
    "comment": "For InTec framework implementation, see GitHub repository\n  https://github.com/IDASLab/InTec_Framework",
    "pdf_url": "http://arxiv.org/pdf/2502.11644v1",
    "published_date": "2025-02-17 10:38:00 UTC",
    "updated_date": "2025-02-17 10:38:00 UTC"
  },
  {
    "arxiv_id": "2502.11639v2",
    "title": "Neural Interpretable Reasoning",
    "authors": [
      "Pietro Barbiero",
      "Giuseppe Marra",
      "Gabriele Ciravegna",
      "David Debot",
      "Francesco De Santis",
      "Michelangelo Diligenti",
      "Mateo Espinosa Zarlenga",
      "Francesco Giannini"
    ],
    "abstract": "We formalize a novel modeling framework for achieving interpretability in\ndeep learning, anchored in the principle of inference equivariance. While the\ndirect verification of interpretability scales exponentially with the number of\nvariables of the system, we show that this complexity can be mitigated by\ntreating interpretability as a Markovian property and employing neural\nre-parametrization techniques. Building on these insights, we propose a new\nmodeling paradigm -- neural generation and interpretable execution -- that\nenables scalable verification of equivariance. This paradigm provides a general\napproach for designing Neural Interpretable Reasoners that are not only\nexpressive but also transparent.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11639v2",
    "published_date": "2025-02-17 10:33:24 UTC",
    "updated_date": "2025-03-04 08:38:14 UTC"
  },
  {
    "arxiv_id": "2503.16445v1",
    "title": "FINCH: Locally Visualizing Higher-Order Feature Interactions in Black Box Models",
    "authors": [
      "Anna Kleinau",
      "Bernhard Preim",
      "Monique Meuschke"
    ],
    "abstract": "In an era where black-box AI models are integral to decision-making across\nindustries, robust methods for explaining these models are more critical than\never. While these models leverage complex feature interplay for accurate\npredictions, most explanation methods only assign relevance to individual\nfeatures. There is a research gap in methods that effectively illustrate\ninteractions between features, especially in visualizing higher-order\ninteractions involving multiple features, which challenge conventional\nrepresentation methods. To address this challenge in local explanations focused\non individual instances, we employ a visual, subset-based approach to reveal\nrelevant feature interactions. Our visual analytics tool FINCH uses coloring\nand highlighting techniques to create intuitive, human-centered visualizations,\nand provides additional views that enable users to calibrate their trust in the\nmodel and explanations. We demonstrate FINCH in multiple case studies,\ndemonstrating its generalizability, and conducted an extensive human study with\nmachine learning experts to highlight its helpfulness and usability. With this\napproach, FINCH allows users to visualize feature interactions involving any\nnumber of features locally.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "11 pages, 18 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.16445v1",
    "published_date": "2025-02-17 10:33:20 UTC",
    "updated_date": "2025-02-17 10:33:20 UTC"
  },
  {
    "arxiv_id": "2502.15780v1",
    "title": "Feature Engineering Approach to Building Load Prediction: A Case Study for Commercial Building Chiller Plant Optimization in Tropical Weather",
    "authors": [
      "Zhan Wang",
      "Chen Weidong",
      "Huang Zhifeng",
      "Md Raisul Islam",
      "Chua Kian Jon"
    ],
    "abstract": "In tropical countries with high humidity, air conditioning can account for up\nto 60% of a building's energy use. For commercial buildings with centralized\nsystems, the efficiency of the chiller plant is vital, and model predictive\ncontrol provides an effective strategy for optimizing operations through\ndynamic adjustments based on accurate load predictions. Artificial neural\nnetworks are effective for modelling nonlinear systems but are prone to\noverfitting due to their complexity. Effective feature engineering can mitigate\nthis issue. While weather data are crucial for load prediction, they are often\nused as raw numerical inputs without advanced processing. Clustering features\nis a technique that can reduce model complexity and enhance prediction\naccuracy. Although previous studies have explored clustering algorithms for\nload prediction, none have applied them to multidimensional weather data,\nrevealing a research gap. This study presents a cooling load prediction model\nthat combines a neural network with Kalman filtering and K-means clustering.\nApplied to real world data from a commercial skyscraper in Singapore's central\nbusiness district, the model achieved a 46.5% improvement in prediction\naccuracy. An optimal chiller sequencing strategy was also developed through\ngenetic algorithm optimization of the predictive load, potentially saving 13.8%\nin energy. Finally, the study evaluated the integration of thermal energy\nstorage into the chiller plant design, demonstrating potential reductions in\ncapital and operational costs of 26% and 13%, respectively.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.LG",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.15780v1",
    "published_date": "2025-02-17 10:22:43 UTC",
    "updated_date": "2025-02-17 10:22:43 UTC"
  },
  {
    "arxiv_id": "2502.11617v1",
    "title": "In-Context Parametric Inference: Point or Distribution Estimators?",
    "authors": [
      "Sarthak Mittal",
      "Yoshua Bengio",
      "Nikolay Malkin",
      "Guillaume Lajoie"
    ],
    "abstract": "Bayesian and frequentist inference are two fundamental paradigms in\nstatistical estimation. Bayesian methods treat hypotheses as random variables,\nincorporating priors and updating beliefs via Bayes' theorem, whereas\nfrequentist methods assume fixed but unknown hypotheses, relying on estimators\nlike maximum likelihood. While extensive research has compared these\napproaches, the frequentist paradigm of obtaining point estimates has become\npredominant in deep learning, as Bayesian inference is challenging due to the\ncomputational complexity and the approximation gap of posterior estimation\nmethods. However, a good understanding of trade-offs between the two approaches\nis lacking in the regime of amortized estimators, where in-context learners are\ntrained to estimate either point values via maximum likelihood or maximum a\nposteriori estimation, or full posteriors using normalizing flows, score-based\ndiffusion samplers, or diagonal Gaussian approximations, conditioned on\nobservations. To help resolve this, we conduct a rigorous comparative analysis\nspanning diverse problem settings, from linear models to shallow neural\nnetworks, with a robust evaluation framework assessing both in-distribution and\nout-of-distribution generalization on tractable tasks. Our experiments indicate\nthat amortized point estimators generally outperform posterior inference,\nthough the latter remain competitive in some low-dimensional problems, and we\nfurther discuss why this might be the case.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11617v1",
    "published_date": "2025-02-17 10:00:24 UTC",
    "updated_date": "2025-02-17 10:00:24 UTC"
  },
  {
    "arxiv_id": "2502.11614v1",
    "title": "Is Human-Like Text Liked by Humans? Multilingual Human Detection and Preference Against AI",
    "authors": [
      "Yuxia Wang",
      "Rui Xing",
      "Jonibek Mansurov",
      "Giovanni Puccetti",
      "Zhuohan Xie",
      "Minh Ngoc Ta",
      "Jiahui Geng",
      "Jinyan Su",
      "Mervat Abassy",
      "Saad El Dine Ahmed",
      "Kareem Elozeiri",
      "Nurkhan Laiyk",
      "Maiya Goloburda",
      "Tarek Mahmoud",
      "Raj Vardhan Tomar",
      "Alexander Aziz",
      "Ryuto Koike",
      "Masahiro Kaneko",
      "Artem Shelmanov",
      "Ekaterina Artemova",
      "Vladislav Mikhailov",
      "Akim Tsvigun",
      "Alham Fikri Aji",
      "Nizar Habash",
      "Iryna Gurevych",
      "Preslav Nakov"
    ],
    "abstract": "Prior studies have shown that distinguishing text generated by large language\nmodels (LLMs) from human-written one is highly challenging, and often no better\nthan random guessing. To verify the generalizability of this finding across\nlanguages and domains, we perform an extensive case study to identify the upper\nbound of human detection accuracy. Across 16 datasets covering 9 languages and\n9 domains, 19 annotators achieved an average detection accuracy of 87.6%, thus\nchallenging previous conclusions. We find that major gaps between human and\nmachine text lie in concreteness, cultural nuances, and diversity. Prompting by\nexplicitly explaining the distinctions in the prompts can partially bridge the\ngaps in over 50% of the cases. However, we also find that humans do not always\nprefer human-written text, particularly when they cannot clearly identify its\nsource.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11614v1",
    "published_date": "2025-02-17 09:56:46 UTC",
    "updated_date": "2025-02-17 09:56:46 UTC"
  },
  {
    "arxiv_id": "2502.11612v2",
    "title": "Maximum Entropy Reinforcement Learning with Diffusion Policy",
    "authors": [
      "Xiaoyi Dong",
      "Jian Cheng",
      "Xi Sheryl Zhang"
    ],
    "abstract": "The Soft Actor-Critic (SAC) algorithm with a Gaussian policy has become a\nmainstream implementation for realizing the Maximum Entropy Reinforcement\nLearning (MaxEnt RL) objective, which incorporates entropy maximization to\nencourage exploration and enhance policy robustness. While the Gaussian policy\nperforms well on simpler tasks, its exploration capacity and potential\nperformance in complex multi-goal RL environments are limited by its inherent\nunimodality. In this paper, we employ the diffusion model, a powerful\ngenerative model capable of capturing complex multimodal distributions, as the\npolicy representation to fulfill the MaxEnt RL objective, developing a method\nnamed MaxEnt RL with Diffusion Policy (MaxEntDP). Our method enables efficient\nexploration and brings the policy closer to the optimal MaxEnt policy.\nExperimental results on Mujoco benchmarks show that MaxEntDP outperforms the\nGaussian policy and other generative models within the MaxEnt RL framework, and\nperforms comparably to other state-of-the-art diffusion-based online RL\nalgorithms. Our code is available at https://github.com/diffusionyes/MaxEntDP.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "21 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.11612v2",
    "published_date": "2025-02-17 09:55:58 UTC",
    "updated_date": "2025-02-18 09:33:28 UTC"
  },
  {
    "arxiv_id": "2502.11611v1",
    "title": "Identifying Gender Stereotypes and Biases in Automated Translation from English to Italian using Similarity Networks",
    "authors": [
      "Fatemeh Mohammadi",
      "Marta Annamaria Tamborini",
      "Paolo Ceravolo",
      "Costanza Nardocci",
      "Samira Maghool"
    ],
    "abstract": "This paper is a collaborative effort between Linguistics, Law, and Computer\nScience to evaluate stereotypes and biases in automated translation systems. We\nadvocate gender-neutral translation as a means to promote gender inclusion and\nimprove the objectivity of machine translation. Our approach focuses on\nidentifying gender bias in English-to-Italian translations. First, we define\ngender bias following human rights law and linguistics literature. Then we\nproceed by identifying gender-specific terms such as she/lei and he/lui as key\nelements. We then evaluate the cosine similarity between these target terms and\nothers in the dataset to reveal the model's perception of semantic relations.\nUsing numerical features, we effectively evaluate the intensity and direction\nof the bias. Our findings provide tangible insights for developing and training\ngender-neutral translation algorithms.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11611v1",
    "published_date": "2025-02-17 09:55:32 UTC",
    "updated_date": "2025-02-17 09:55:32 UTC"
  },
  {
    "arxiv_id": "2502.11603v1",
    "title": "DR.GAP: Mitigating Bias in Large Language Models using Gender-Aware Prompting with Demonstration and Reasoning",
    "authors": [
      "Hongye Qiu",
      "Yue Xu",
      "Meikang Qiu",
      "Wenjie Wang"
    ],
    "abstract": "Large Language Models (LLMs) exhibit strong natural language processing\ncapabilities but also inherit and amplify societal biases, including gender\nbias, raising fairness concerns. Existing debiasing methods face significant\nlimitations: parameter tuning requires access to model weights, prompt-based\napproaches often degrade model utility, and optimization-based techniques lack\ngeneralizability. To address these challenges, we propose DR.GAP (Demonstration\nand Reasoning for Gender-Aware Prompting), an automated and model-agnostic\napproach that mitigates gender bias while preserving model performance. DR.GAP\nselects bias-revealing examples and generates structured reasoning to guide\nmodels toward more impartial responses. Extensive experiments on coreference\nresolution and QA tasks across multiple LLMs (GPT-3.5, Llama3, and\nLlama2-Alpaca) demonstrate its effectiveness, generalization ability, and\nrobustness. DR.GAP can generalize to vision-language models (VLMs), achieving\nsignificant bias reduction.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11603v1",
    "published_date": "2025-02-17 09:43:36 UTC",
    "updated_date": "2025-02-17 09:43:36 UTC"
  },
  {
    "arxiv_id": "2502.11596v1",
    "title": "LLM Embeddings for Deep Learning on Tabular Data",
    "authors": [
      "Boshko Koloski",
      "Andrei Margeloiu",
      "Xiangjian Jiang",
      "Blaž Škrlj",
      "Nikola Simidjievski",
      "Mateja Jamnik"
    ],
    "abstract": "Tabular deep-learning methods require embedding numerical and categorical\ninput features into high-dimensional spaces before processing them. Existing\nmethods deal with this heterogeneous nature of tabular data by employing\nseparate type-specific encoding approaches. This limits the cross-table\ntransfer potential and the exploitation of pre-trained knowledge. We propose a\nnovel approach that first transforms tabular data into text, and then leverages\npre-trained representations from LLMs to encode this data, resulting in a\nplug-and-play solution to improv ing deep-learning tabular methods. We\ndemonstrate that our approach improves accuracy over competitive models, such\nas MLP, ResNet and FT-Transformer, by validating on seven classification\ndatasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11596v1",
    "published_date": "2025-02-17 09:28:51 UTC",
    "updated_date": "2025-02-17 09:28:51 UTC"
  },
  {
    "arxiv_id": "2502.11588v1",
    "title": "A Unified Modeling Framework for Automated Penetration Testing",
    "authors": [
      "Yunfei Wang",
      "Shixuan Liu",
      "Wenhao Wang",
      "Changling Zhou",
      "Chao Zhang",
      "Jiandong Jin",
      "Cheng Zhu"
    ],
    "abstract": "The integration of artificial intelligence into automated penetration testing\n(AutoPT) has highlighted the necessity of simulation modeling for the training\nof intelligent agents, due to its cost-efficiency and swift feedback\ncapabilities. Despite the proliferation of AutoPT research, there is a\nrecognized gap in the availability of a unified framework for simulation\nmodeling methods. This paper presents a systematic review and synthesis of\nexisting techniques, introducing MDCPM to categorize studies based on\nliterature objectives, network simulation complexity, dependency of technical\nand tactical operations, and scenario feedback and variation. To bridge the gap\nin unified method for multi-dimensional and multi-level simulation modeling,\ndynamic environment modeling, and the scarcity of public datasets, we introduce\nAutoPT-Sim, a novel modeling framework that based on policy automation and\nencompasses the combination of all sub dimensions. AutoPT-Sim offers a\ncomprehensive approach to modeling network environments, attackers, and\ndefenders, transcending the constraints of static modeling and accommodating\nnetworks of diverse scales. We publicly release a generated standard network\nenvironment dataset and the code of Network Generator. By integrating publicly\navailable datasets flexibly, support is offered for various simulation modeling\nlevels focused on policy automation in MDCPM and the network generator help\nresearchers output customized target network data by adjusting parameters or\nfine-tuning the network generator.",
    "categories": [
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11588v1",
    "published_date": "2025-02-17 09:21:53 UTC",
    "updated_date": "2025-02-17 09:21:53 UTC"
  },
  {
    "arxiv_id": "2502.11585v1",
    "title": "Calibration of Vehicular Traffic Simulation Models by Local Optimization",
    "authors": [
      "Davide Andrea Guastella",
      "Alejandro Morales-Hernàndez",
      "Bruno Cornelis",
      "Gianluca Bontempi"
    ],
    "abstract": "Simulation is a valuable tool for traffic management experts to assist them\nin refining and improving transportation systems and anticipating the impact of\npossible changes in the infrastructure network before their actual\nimplementation. Calibrating simulation models using traffic count data is\nchallenging because of the complexity of the environment, the lack of data, and\nthe uncertainties in traffic dynamics. This paper introduces a novel stochastic\nsimulation-based traffic calibration technique. The novelty of the proposed\nmethod is: (i) it performs local traffic calibration, (ii) it allows\ncalibrating simulated traffic in large-scale environments, (iii) it requires\nonly the traffic count data. The local approach enables decentralizing the\ncalibration task to reach near real-time performance, enabling the fostering of\ndigital twins. Using only traffic count data makes the proposed method generic\nso that it can be applied in different traffic scenarios at various scales\n(from neighborhood to region). We assess the proposed technique on a model of\nBrussels, Belgium, using data from real traffic monitoring devices. The\nproposed method has been implemented using the open-source traffic simulator\nSUMO. Experimental results show that the traffic model calibrated using the\nproposed method is on average 16% more accurate than those obtained by the\nstate-of-the-art methods, using the same dataset. We also make available the\noutput traffic model obtained from real data.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Published on Springer Transportation",
    "pdf_url": "http://arxiv.org/pdf/2502.11585v1",
    "published_date": "2025-02-17 09:17:01 UTC",
    "updated_date": "2025-02-17 09:17:01 UTC"
  },
  {
    "arxiv_id": "2502.11578v1",
    "title": "Language Complexity Measurement as a Noisy Zero-Shot Proxy for Evaluating LLM Performance",
    "authors": [
      "Birger Moell",
      "Johan Boye"
    ],
    "abstract": "Large Language Models (LLMs) have made significant strides in natural\nlanguage generation but often face challenges in tasks requiring precise\ncalculations and structural analysis. This paper investigates the performance\nof state-of-the-art LLMs on language complexity measurement tasks, through the\ncomputation of the LIX readability metric and Average Dependency Distance\n(ADD). Using Swedish high school and university-level essays, we evaluate the\nmodels' abilities to compute LIX scores and perform dependency parsing,\ncomparing their results to established ground truths. Our findings reveal that\nwhile all models demonstrate some capacity for these tasks, ChatGPT-o1-mini\nperforms most consistently, achieving the highest accuracy in both LIX\ncomputation and dependency parsing. Additionally, we observe a strong\nsignificant correlation -0.875 p 0.026 (N=6) between the models' accuracy in\ncomputing LIX and their overall performance on the Massive Multitask Language\nUnderstanding (MMLU) benchmark. These results suggest that language complexity\nmeasurement abilities can serve as a noisy zero-shot proxies for assessing the\ngeneral capabilities of LLMs, providing a practical method for model evaluation\nwithout the need for extensive benchmarking datasets.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Submitted to ACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.11578v1",
    "published_date": "2025-02-17 09:09:58 UTC",
    "updated_date": "2025-02-17 09:09:58 UTC"
  },
  {
    "arxiv_id": "2502.12217v1",
    "title": "Optimal Brain Iterative Merging: Mitigating Interference in LLM Merging",
    "authors": [
      "Zhixiang Wang",
      "Zhenyu Mao",
      "Yixuan Qiao",
      "Yunfang Wu",
      "Biye Li"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities, but\ntheir high computational costs pose challenges for customization. Model merging\noffers a cost-effective alternative, yet existing methods suffer from\ninterference among parameters, leading to performance degradation. In this\nwork, we propose Optimal Brain Iterative Merging (OBIM), a novel method\ndesigned to mitigate both intra-model and inter-model interference. OBIM\nconsists of two key components: (1) A saliency measurement mechanism that\nevaluates parameter importance based on loss changes induced by individual\nweight alterations, reducing intra-model interference by preserving only\nhigh-saliency parameters. (2) A mutually exclusive iterative merging framework,\nwhich incrementally integrates models using a binary mask to avoid direct\nparameter averaging, thereby mitigating inter-model interference. We validate\nOBIM through experiments on both Supervised Fine-Tuned (SFT) models and\npost-pretrained checkpoints. The results show that OBIM significantly\noutperforms existing merging techniques. Overall, OBIM provides an effective\nand practical solution for enhancing LLM merging.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12217v1",
    "published_date": "2025-02-17 09:07:49 UTC",
    "updated_date": "2025-02-17 09:07:49 UTC"
  },
  {
    "arxiv_id": "2502.11573v1",
    "title": "InfiR : Crafting Effective Small Language Models and Multimodal Small Language Models in Reasoning",
    "authors": [
      "Congkai Xie",
      "Shuo Cai",
      "Wenjun Wang",
      "Pengxiang Li",
      "Zhijie Sang",
      "Kejing Yang",
      "Yiming Zhang",
      "Zhen Li",
      "Guanghao Zhu",
      "Zeyu Liu",
      "Yang Yu",
      "Yuhang Liu",
      "Su Lu",
      "Baoyi He",
      "Qi Zhou",
      "Xiaotian Han",
      "Jianbo Yuan",
      "Shengyu Zhang",
      "Fei Wu",
      "Hongxia Yang"
    ],
    "abstract": "Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)\nhave made significant advancements in reasoning capabilities. However, they\nstill face challenges such as high computational demands and privacy concerns.\nThis paper focuses on developing efficient Small Language Models (SLMs) and\nMultimodal Small Language Models (MSLMs) that retain competitive reasoning\nabilities. We introduce a novel training pipeline that enhances reasoning\ncapabilities and facilitates deployment on edge devices, achieving\nstate-of-the-art performance while minimizing development costs. \\InfR~ aims to\nadvance AI systems by improving reasoning, reducing adoption barriers, and\naddressing privacy concerns through smaller model sizes. Resources are\navailable at https://github. com/Reallm-Labs/InfiR.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11573v1",
    "published_date": "2025-02-17 09:07:32 UTC",
    "updated_date": "2025-02-17 09:07:32 UTC"
  },
  {
    "arxiv_id": "2502.11574v2",
    "title": "Large Language Models and Mathematical Reasoning Failures",
    "authors": [
      "Johan Boye",
      "Birger Moell"
    ],
    "abstract": "This paper investigates the mathematical reasoning capabilities of large\nlanguage models (LLMs) using 50 newly constructed high-school-level word\nproblems. Unlike prior studies that focus solely on answer correctness, we\nrigorously analyze both final answers and solution steps to identify reasoning\nfailures. Evaluating eight state-of-the-art models - including Mixtral, Llama,\nGemini, GPT-4o, and OpenAI's o1 variants - we find that while newer models\n(e.g., o3-mini, deepseek-r1) achieve higher accuracy, all models exhibit errors\nin spatial reasoning, strategic planning, and arithmetic, sometimes producing\ncorrect answers through flawed logic. Common failure modes include unwarranted\nassumptions, over-reliance on numerical patterns, and difficulty translating\nphysical intuition into mathematical steps. Manual analysis reveals that models\nstruggle with problems requiring multi-step deduction or real-world knowledge,\ndespite possessing broad mathematical knowledge. Our results underscore the\nimportance of evaluating reasoning processes, not just answers, and caution\nagainst overestimating LLMs' problem-solving proficiency. The study highlights\npersistent gaps in LLMs' generalization abilities, emphasizing the need for\ntargeted improvements in structured reasoning and constraint handling.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11574v2",
    "published_date": "2025-02-17 09:07:32 UTC",
    "updated_date": "2025-02-21 11:04:07 UTC"
  },
  {
    "arxiv_id": "2502.11569v2",
    "title": "Towards Reasoning Ability of Small Language Models",
    "authors": [
      "Gaurav Srivastava",
      "Shuxiang Cao",
      "Xuan Wang"
    ],
    "abstract": "Reasoning has long been viewed as an emergent property of large language\nmodels (LLMs), appearing at or above a certain scale ($\\sim$100B parameters).\nHowever, recent studies challenge this assumption, showing that small language\nmodels (SLMs) can also achieve competitive reasoning performance. SLMs are\nincreasingly favored for their efficiency and deployability. However, there is\na lack of systematic study on the reasoning abilities of diverse SLMs,\nincluding those trained from scratch or derived from LLMs through quantization,\npruning, and distillation. This raises a critical question: Can SLMs achieve\nreasoning abilities comparable to LLMs? In this work, we systematically survey,\nbenchmark, and analyze 72 SLMs from six model families across 14 reasoning\nbenchmarks. For reliable evaluation, we examine four evaluation methods and\ncompare four LLM judges against human evaluations on 800 data points. We repeat\nall experiments three times to ensure a robust performance assessment.\nAdditionally, we analyze the impact of different prompting strategies in small\nmodels. Beyond accuracy, we also evaluate model robustness under adversarial\nconditions and intermediate reasoning steps. Our findings challenge the\nassumption that scaling is the only way to achieve strong reasoning. Instead,\nwe foresee a future where SLMs with strong reasoning capabilities can be\ndeveloped through structured training or post-training compression. They can\nserve as efficient alternatives to LLMs for reasoning-intensive tasks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "# fixed some typos, added public slm reasoning leaderboard",
    "pdf_url": "http://arxiv.org/pdf/2502.11569v2",
    "published_date": "2025-02-17 08:59:16 UTC",
    "updated_date": "2025-04-23 23:02:52 UTC"
  },
  {
    "arxiv_id": "2502.11563v1",
    "title": "Leader and Follower: Interactive Motion Generation under Trajectory Constraints",
    "authors": [
      "Runqi Wang",
      "Caoyuan Ma",
      "Jian Zhao",
      "Hanrui Xu",
      "Dongfang Sun",
      "Haoyang Chen",
      "Lin Xiong",
      "Zheng Wang",
      "Xuelong Li"
    ],
    "abstract": "With the rapid advancement of game and film production, generating\ninteractive motion from texts has garnered significant attention due to its\npotential to revolutionize content creation processes. In many practical\napplications, there is a need to impose strict constraints on the motion range\nor trajectory of virtual characters. However, existing methods that rely solely\non textual input face substantial challenges in accurately capturing the user's\nintent, particularly in specifying the desired trajectory. As a result, the\ngenerated motions often lack plausibility and accuracy. Moreover, existing\ntrajectory - based methods for customized motion generation rely on retraining\nfor single - actor scenarios, which limits flexibility and adaptability to\ndifferent datasets, as well as interactivity in two-actor motions. To generate\ninteractive motion following specified trajectories, this paper decouples\ncomplex motion into a Leader - Follower dynamic, inspired by role allocation in\npartner dancing. Based on this framework, this paper explores the motion range\nrefinement process in interactive motion generation and proposes a\ntraining-free approach, integrating a Pace Controller and a Kinematic\nSynchronization Adapter. The framework enhances the ability of existing models\nto generate motion that adheres to trajectory by controlling the leader's\nmovement and correcting the follower's motion to align with the leader.\nExperimental results show that the proposed approach, by better leveraging\ntrajectory information, outperforms existing methods in both realism and\naccuracy.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11563v1",
    "published_date": "2025-02-17 08:52:45 UTC",
    "updated_date": "2025-02-17 08:52:45 UTC"
  },
  {
    "arxiv_id": "2502.11560v1",
    "title": "A Survey of Automatic Prompt Engineering: An Optimization Perspective",
    "authors": [
      "Wenwu Li",
      "Xiangfeng Wang",
      "Wenhao Li",
      "Bo Jin"
    ],
    "abstract": "The rise of foundation models has shifted focus from resource-intensive\nfine-tuning to prompt engineering, a paradigm that steers model behavior\nthrough input design rather than weight updates. While manual prompt\nengineering faces limitations in scalability, adaptability, and cross-modal\nalignment, automated methods, spanning foundation model (FM) based\noptimization, evolutionary methods, gradient-based optimization, and\nreinforcement learning, offer promising solutions. Existing surveys, however,\nremain fragmented across modalities and methodologies. This paper presents the\nfirst comprehensive survey on automated prompt engineering through a unified\noptimization-theoretic lens. We formalize prompt optimization as a maximization\nproblem over discrete, continuous, and hybrid prompt spaces, systematically\norganizing methods by their optimization variables (instructions, soft prompts,\nexemplars), task-specific objectives, and computational frameworks. By bridging\ntheoretical formulation with practical implementations across text, vision, and\nmultimodal domains, this survey establishes a foundational framework for both\nresearchers and practitioners, while highlighting underexplored frontiers in\nconstrained optimization and agent-oriented prompt design.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "19 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.11560v1",
    "published_date": "2025-02-17 08:48:07 UTC",
    "updated_date": "2025-02-17 08:48:07 UTC"
  },
  {
    "arxiv_id": "2502.11559v1",
    "title": "Auto-Search and Refinement: An Automated Framework for Gender Bias Mitigation in Large Language Models",
    "authors": [
      "Yue Xu",
      "Chengyan Fu",
      "Li Xiong",
      "Sibei Yang",
      "Wenjie Wang"
    ],
    "abstract": "Pre-training large language models (LLMs) on vast text corpora enhances\nnatural language processing capabilities but risks encoding social biases,\nparticularly gender bias. While parameter-modification methods like fine-tuning\nmitigate bias, they are resource-intensive, unsuitable for closed-source\nmodels, and lack adaptability to evolving societal norms. Instruction-based\napproaches offer flexibility but often compromise task performance. To address\nthese limitations, we propose $\\textit{FaIRMaker}$, an automated and\nmodel-independent framework that employs an $\\textbf{auto-search and\nrefinement}$ paradigm to adaptively generate Fairwords, which act as\ninstructions integrated into input queries to reduce gender bias and enhance\nresponse quality. Extensive experiments demonstrate that $\\textit{FaIRMaker}$\nautomatically searches for and dynamically refines Fairwords, effectively\nmitigating gender bias while preserving task integrity and ensuring\ncompatibility with both API-based and open-source LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11559v1",
    "published_date": "2025-02-17 08:44:04 UTC",
    "updated_date": "2025-02-17 08:44:04 UTC"
  },
  {
    "arxiv_id": "2502.11555v1",
    "title": "Equilibrate RLHF: Towards Balancing Helpfulness-Safety Trade-off in Large Language Models",
    "authors": [
      "Yingshui Tan",
      "Yilei Jiang",
      "Yanshi Li",
      "Jiaheng Liu",
      "Xingyuan Bu",
      "Wenbo Su",
      "Xiangyu Yue",
      "Xiaoyong Zhu",
      "Bo Zheng"
    ],
    "abstract": "Fine-tuning large language models (LLMs) based on human preferences, commonly\nachieved through reinforcement learning from human feedback (RLHF), has been\neffective in improving their performance. However, maintaining LLM safety\nthroughout the fine-tuning process remains a significant challenge, as\nresolving conflicts between safety and helpfulness can be non-trivial.\nTypically, the safety alignment of LLM is trained on data with safety-related\ncategories. However, our experiments find that naively increasing the scale of\nsafety training data usually leads the LLMs to an ``overly safe'' state rather\nthan a ``truly safe'' state, boosting the refusal rate through extensive\nsafety-aligned data without genuinely understanding the requirements for safe\nresponses. Such an approach can inadvertently diminish the models' helpfulness.\nTo understand the phenomenon, we first investigate the role of safety data by\ncategorizing them into three different groups, and observe that each group\nbehaves differently as training data scales up. To boost the balance between\nsafety and helpfulness, we propose an Equilibrate RLHF framework including a\nFine-grained Data-centric (FDC) approach that achieves better safety alignment\neven with fewer training data, and an Adaptive Message-wise Alignment (AMA)\napproach, which selectively highlight the key segments through a gradient\nmasking strategy. Extensive experimental results demonstrate that our approach\nsignificantly enhances the safety alignment of LLMs while balancing safety and\nhelpfulness.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11555v1",
    "published_date": "2025-02-17 08:40:30 UTC",
    "updated_date": "2025-02-17 08:40:30 UTC"
  },
  {
    "arxiv_id": "2502.12216v1",
    "title": "Tactic: Adaptive Sparse Attention with Clustering and Distribution Fitting for Long-Context LLMs",
    "authors": [
      "Kan Zhu",
      "Tian Tang",
      "Qinyu Xu",
      "Yile Gu",
      "Zhichen Zeng",
      "Rohan Kadekodi",
      "Liangyu Zhao",
      "Ang Li",
      "Arvind Krishnamurthy",
      "Baris Kasikci"
    ],
    "abstract": "Long-context models are essential for many applications but face\ninefficiencies in loading large KV caches during decoding. Prior methods\nenforce fixed token budgets for sparse attention, assuming a set number of\ntokens can approximate full attention. However, these methods overlook\nvariations in the importance of attention across heads, layers, and contexts.\nTo address these limitations, we propose Tactic, a sparsity-adaptive and\ncalibration-free sparse attention mechanism that dynamically selects tokens\nbased on their cumulative attention scores rather than a fixed token budget. By\nsetting a target fraction of total attention scores, Tactic ensures that token\nselection naturally adapts to variations in attention sparsity. To efficiently\napproximate this selection, Tactic leverages clustering-based sorting and\ndistribution fitting, allowing it to accurately estimate token importance with\nminimal computational overhead. We show that Tactic outperforms existing sparse\nattention algorithms, achieving superior accuracy and up to 7.29x decode\nattention speedup. This improvement translates to an overall 1.58x end-to-end\ninference speedup, making Tactic a practical and effective solution for\nlong-context LLM inference in accuracy-sensitive applications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12216v1",
    "published_date": "2025-02-17 08:39:43 UTC",
    "updated_date": "2025-02-17 08:39:43 UTC"
  },
  {
    "arxiv_id": "2502.11554v1",
    "title": "Toward Metaphor-Fluid Conversation Design for Voice User Interfaces",
    "authors": [
      "Smit Desai",
      "Jessie Chin",
      "Dakuo Wang",
      "Benjamin Cowan",
      "Michael Twidale"
    ],
    "abstract": "Metaphors play a critical role in shaping user experiences with Voice User\nInterfaces (VUIs), yet existing designs often rely on static, human-centric\nmetaphors that fail to adapt to diverse contexts and user needs. This paper\nintroduces Metaphor-Fluid Design, a novel approach that dynamically adjusts\nmetaphorical representations based on conversational use-contexts. We compare\nthis approach to a Default VUI, which characterizes the present implementation\nof commercial VUIs commonly designed around the persona of an assistant,\noffering a uniform interaction style across contexts. In Study 1 (N=130),\nmetaphors were mapped to four key use-contexts-commands, information seeking,\nsociality, and error recovery-along the dimensions of formality and hierarchy,\nrevealing distinct preferences for task-specific metaphorical designs. Study 2\n(N=91) evaluates a Metaphor-Fluid VUI against a Default VUI, showing that the\nMetaphor-Fluid VUI enhances perceived intention to adopt, enjoyment, and\nlikability by aligning better with user expectations for different contexts.\nHowever, individual differences in metaphor preferences highlight the need for\npersonalization. These findings challenge the one-size-fits-all paradigm of VUI\ndesign and demonstrate the potential of Metaphor-Fluid Design to create more\nadaptive and engaging human-AI interactions.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.ET"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11554v1",
    "published_date": "2025-02-17 08:36:12 UTC",
    "updated_date": "2025-02-17 08:36:12 UTC"
  },
  {
    "arxiv_id": "2502.11541v2",
    "title": "MuSC: Improving Complex Instruction Following with Multi-granularity Self-Contrastive Training",
    "authors": [
      "Hui Huang",
      "Jiaheng Liu",
      "Yancheng He",
      "Shilong Li",
      "Bing Xu",
      "Conghui Zhu",
      "Muyun Yang",
      "Tiejun Zhao"
    ],
    "abstract": "Complex instruction-following with elaborate constraints is imperative for\nLarge Language Models (LLMs). While existing methods have constructed data for\ncomplex instruction alignment, they all rely on a more advanced model,\nespecially GPT-4, limiting their application. In this paper, we propose a\nMulti-granularity Self-Contrastive Training (MuSC) framework, to improve the\ncomplex instruction alignment without relying on a stronger model. Our method\nis conducted on both coarse and fine granularity. On coarse-granularity, we\nconstruct constraint-aware preference data based on instruction decomposition\nand recombination. On fine-granularity, we perform token-aware preference\noptimization with dynamic token-level supervision. Our method is evaluated on\nopen-sourced models, and experiment results show our method achieves\nsignificant improvement on both complex and general instruction-following\nbenchmarks, surpassing previous self-alignment methods.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11541v2",
    "published_date": "2025-02-17 08:12:49 UTC",
    "updated_date": "2025-02-23 05:56:44 UTC"
  },
  {
    "arxiv_id": "2502.15779v1",
    "title": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating Rotation and Learnable Non-uniform Quantizer",
    "authors": [
      "Euntae Choi",
      "Sumin Song",
      "Woosang Lim",
      "Sungjoo Yoo"
    ],
    "abstract": "We propose Rotate, Clip, and Partition (RCP), a quantization-aware training\n(QAT) approach that first realizes extreme compression of LLMs with\nW2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP\nintegrates recent rotation techniques with a novel non-uniform weight quantizer\ndesign, by quantitatively analyzing the impact of random rotation on 2-bit\nweight quantization. Our weight quantizer features Learnable Direct\nPartitioning (LDP), which introduces learnable parameters to directly learn\nnon-uniform intervals jointly with LLM weights. We also present a specialized\nGPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP\ncan compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and\n5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging\nmobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and\nMetaMath-7B with no critical problems such as convergence failure and\nrepetition. Code will be made available at blind_review.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.15779v1",
    "published_date": "2025-02-17 08:12:34 UTC",
    "updated_date": "2025-02-17 08:12:34 UTC"
  },
  {
    "arxiv_id": "2502.11537v3",
    "title": "Uncovering Untapped Potential in Sample-Efficient World Model Agents",
    "authors": [
      "Lior Cohen",
      "Kaixin Wang",
      "Bingyi Kang",
      "Uri Gadot",
      "Shie Mannor"
    ],
    "abstract": "World model (WM) agents enable sample-efficient reinforcement learning by\nlearning policies entirely from simulated experience. However, existing\ntoken-based world models (TBWMs) are limited to visual inputs and discrete\nactions, restricting their adoption and applicability. Moreover, although both\nintrinsic motivation and prioritized WM replay have shown promise in improving\nWM performance and generalization, they remain underexplored in this setting,\nparticularly in combination. We introduce Simulus, a highly modular TBWM agent\nthat integrates (1) a modular multi-modality tokenization framework, (2)\nintrinsic motivation, (3) prioritized WM replay, and (4)\nregression-as-classification for reward and return prediction. Simulus achieves\nstate-of-the-art sample efficiency for planning-free WMs across three diverse\nbenchmarks. Ablation studies reveal the individual contribution of each\ncomponent while highlighting their synergy. Our code and model weights are\npublicly available at https://github.com/leor-c/Simulus.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11537v3",
    "published_date": "2025-02-17 08:06:10 UTC",
    "updated_date": "2025-05-20 11:45:24 UTC"
  },
  {
    "arxiv_id": "2502.11528v1",
    "title": "A Survey of Personalized Large Language Models: Progress and Future Directions",
    "authors": [
      "Jiahong Liu",
      "Zexuan Qiu",
      "Zhongyang Li",
      "Quanyu Dai",
      "Jieming Zhu",
      "Minda Hu",
      "Menglin Yang",
      "Irwin King"
    ],
    "abstract": "Large Language Models (LLMs) excel in handling general knowledge tasks, yet\nthey struggle with user-specific personalization, such as understanding\nindividual emotions, writing styles, and preferences. Personalized Large\nLanguage Models (PLLMs) tackle these challenges by leveraging individual user\ndata, such as user profiles, historical dialogues, content, and interactions,\nto deliver responses that are contextually relevant and tailored to each user's\nspecific needs. This is a highly valuable research topic, as PLLMs can\nsignificantly enhance user satisfaction and have broad applications in\nconversational agents, recommendation systems, emotion recognition, medical\nassistants, and more. This survey reviews recent advancements in PLLMs from\nthree technical perspectives: prompting for personalized context (input level),\nfinetuning for personalized adapters (model level), and alignment for\npersonalized preferences (objective level). To provide deeper insights, we also\ndiscuss current limitations and outline several promising directions for future\nresearch. Updated information about this survey can be found at the\nhttps://github.com/JiahongLiu21/Awesome-Personalized-Large-Language-Models.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "7pages, 5 figures, Under Review",
    "pdf_url": "http://arxiv.org/pdf/2502.11528v1",
    "published_date": "2025-02-17 07:58:31 UTC",
    "updated_date": "2025-02-17 07:58:31 UTC"
  },
  {
    "arxiv_id": "2502.11521v1",
    "title": "DeFiScope: Detecting Various DeFi Price Manipulations with LLM Reasoning",
    "authors": [
      "Juantao Zhong",
      "Daoyuan Wu",
      "Ye Liu",
      "Maoyi Xie",
      "Yang Liu",
      "Yi Li",
      "Ning Liu"
    ],
    "abstract": "DeFi (Decentralized Finance) is one of the most important applications of\ntoday's cryptocurrencies and smart contracts. It manages hundreds of billions\nin Total Value Locked (TVL) on-chain, yet it remains susceptible to common DeFi\nprice manipulation attacks. Despite state-of-the-art (SOTA) systems like\nDeFiRanger and DeFort, we found that they are less effective to non-standard\nprice models in custom DeFi protocols, which account for 44.2% of the 95 DeFi\nprice manipulation attacks reported over the past three years.\n  In this paper, we introduce the first LLM-based approach, DeFiScope, for\ndetecting DeFi price manipulation attacks in both standard and custom price\nmodels. Our insight is that large language models (LLMs) have certain\nintelligence to abstract price calculation from code and infer the trend of\ntoken price changes based on the extracted price models. To further strengthen\nLLMs in this aspect, we leverage Foundry to synthesize on-chain data and use it\nto fine-tune a DeFi price-specific LLM. Together with the high-level DeFi\noperations recovered from low-level transaction data, DeFiScope detects various\nDeFi price manipulations according to systematically mined patterns.\nExperimental results show that DeFiScope achieves a high precision of 96% and a\nrecall rate of 80%, significantly outperforming SOTA approaches. Moreover, we\nevaluate DeFiScope's cost-effectiveness and demonstrate its practicality by\nhelping our industry partner confirm 147 real-world price manipulation attacks,\nincluding discovering 81 previously unknown historical incidents.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11521v1",
    "published_date": "2025-02-17 07:45:03 UTC",
    "updated_date": "2025-02-17 07:45:03 UTC"
  },
  {
    "arxiv_id": "2502.11519v1",
    "title": "UniGO: A Unified Graph Neural Network for Modeling Opinion Dynamics on Graphs",
    "authors": [
      "Hao Li",
      "Hao Jiang",
      "Yuke Zheng",
      "Hao Sun",
      "Wenying Gong"
    ],
    "abstract": "Polarization and fragmentation in social media amplify user biases, making it\nincreasingly important to understand the evolution of opinions. Opinion\ndynamics provide interpretability for studying opinion evolution, yet\nincorporating these insights into predictive models remains challenging. This\nchallenge arises due to the inherent complexity of the diversity of opinion\nfusion rules and the difficulty in capturing equilibrium states while avoiding\nover-smoothing. This paper constructs a unified opinion dynamics model to\nintegrate different opinion fusion rules and generates corresponding synthetic\ndatasets. To fully leverage the advantages of unified opinion dynamics, we\nintroduces UniGO, a framework for modeling opinion evolution on graphs. Using a\ncoarsen-refine mechanism, UniGO efficiently models opinion dynamics through a\ngraph neural network, mitigating over-smoothing while preserving equilibrium\nphenomena. UniGO leverages pretraining on synthetic datasets, which enhances\nits ability to generalize to real-world scenarios, providing a viable paradigm\nfor applications of opinion dynamics. Experimental results on both synthetic\nand real-world datasets demonstrate UniGO's effectiveness in capturing complex\nopinion formation processes and predicting future evolution. The pretrained\nmodel also shows strong generalization capability, validating the benefits of\nusing synthetic data to boost real-world performance.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "WWW2025",
    "pdf_url": "http://arxiv.org/pdf/2502.11519v1",
    "published_date": "2025-02-17 07:40:32 UTC",
    "updated_date": "2025-02-17 07:40:32 UTC"
  },
  {
    "arxiv_id": "2502.11518v1",
    "title": "Generative Multi-Agent Collaboration in Embodied AI: A Systematic Review",
    "authors": [
      "Di Wu",
      "Xian Wei",
      "Guang Chen",
      "Hao Shen",
      "Xiangfeng Wang",
      "Wenhao Li",
      "Bo Jin"
    ],
    "abstract": "Embodied multi-agent systems (EMAS) have attracted growing attention for\ntheir potential to address complex, real-world challenges in areas such as\nlogistics and robotics. Recent advances in foundation models pave the way for\ngenerative agents capable of richer communication and adaptive problem-solving.\nThis survey provides a systematic examination of how EMAS can benefit from\nthese generative capabilities. We propose a taxonomy that categorizes EMAS by\nsystem architectures and embodiment modalities, emphasizing how collaboration\nspans both physical and virtual contexts. Central building blocks, perception,\nplanning, communication, and feedback, are then analyzed to illustrate how\ngenerative techniques bolster system robustness and flexibility. Through\nconcrete examples, we demonstrate the transformative effects of integrating\nfoundation models into embodied, multi-agent frameworks. Finally, we discuss\nchallenges and future directions, underlining the significant promise of EMAS\nto reshape the landscape of AI-driven collaboration.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.MA",
    "comment": "18 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.11518v1",
    "published_date": "2025-02-17 07:39:34 UTC",
    "updated_date": "2025-02-17 07:39:34 UTC"
  },
  {
    "arxiv_id": "2502.11513v1",
    "title": "MaZO: Masked Zeroth-Order Optimization for Multi-Task Fine-Tuning of Large Language Models",
    "authors": [
      "Zhen Zhang",
      "Yifan Yang",
      "Kai Zhen",
      "Nathan Susanj",
      "Athanasios Mouchtaris",
      "Siegfried Kunzmann",
      "Zheng Zhang"
    ],
    "abstract": "Large language models have demonstrated exceptional capabilities across\ndiverse tasks, but their fine-tuning demands significant memory, posing\nchallenges for resource-constrained environments. Zeroth-order (ZO)\noptimization provides a memory-efficient alternative by eliminating the need\nfor backpropagation. However, ZO optimization suffers from high gradient\nvariance, and prior research has largely focused on single-task learning,\nleaving its application to multi-task learning unexplored. Multi-task learning\nis crucial for leveraging shared knowledge across tasks to improve\ngeneralization, yet it introduces unique challenges under ZO settings, such as\namplified gradient variance and collinearity. In this paper, we present MaZO,\nthe first framework specifically designed for multi-task LLM fine-tuning under\nZO optimization. MaZO tackles these challenges at the parameter level through\ntwo key innovations: a weight importance metric to identify critical parameters\nand a multi-task weight update mask to selectively update these parameters,\nreducing the dimensionality of the parameter space and mitigating task\nconflicts. Experiments demonstrate that MaZO achieves state-of-the-art\nperformance, surpassing even multi-task learning methods designed for\nfirst-order optimization.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "17 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.11513v1",
    "published_date": "2025-02-17 07:28:52 UTC",
    "updated_date": "2025-02-17 07:28:52 UTC"
  },
  {
    "arxiv_id": "2502.12215v2",
    "title": "Revisiting the Test-Time Scaling of o1-like Models: Do they Truly Possess Test-Time Scaling Capabilities?",
    "authors": [
      "Zhiyuan Zeng",
      "Qinyuan Cheng",
      "Zhangyue Yin",
      "Yunhua Zhou",
      "Xipeng Qiu"
    ],
    "abstract": "The advent of test-time scaling in large language models (LLMs), exemplified\nby OpenAI's o1 series, has advanced reasoning capabilities by scaling\ncomputational resource allocation during inference. While successors like QwQ,\nDeepseek-R1 (R1) and LIMO replicate these advancements, whether these models\ntruly possess test-time scaling capabilities remains underexplored. This study\nfound that longer CoTs of these o1-like models do not consistently enhance\naccuracy; in fact, correct solutions are often shorter than incorrect ones for\nthe same questions. Further investigation shows this phenomenon is closely\nrelated to models' self-revision capabilities - longer CoTs contain more\nself-revisions, which often lead to performance degradation. We then compare\nsequential and parallel scaling strategies on QwQ, R1 and LIMO, finding that\nparallel scaling achieves better coverage and scalability. Based on these\ninsights, we propose Shortest Majority Vote, a method that combines parallel\nscaling strategies with CoT length characteristics, significantly improving\nmodels' test-time scalability compared to conventional majority voting\napproaches.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Add the github link",
    "pdf_url": "http://arxiv.org/pdf/2502.12215v2",
    "published_date": "2025-02-17 07:21:11 UTC",
    "updated_date": "2025-03-03 15:29:43 UTC"
  },
  {
    "arxiv_id": "2502.11509v1",
    "title": "DifCluE: Generating Counterfactual Explanations with Diffusion Autoencoders and modal clustering",
    "authors": [
      "Suparshva Jain",
      "Amit Sangroya",
      "Lovekesh Vig"
    ],
    "abstract": "Generating multiple counterfactual explanations for different modes within a\nclass presents a significant challenge, as these modes are distinct yet\nconverge under the same classification. Diffusion probabilistic models (DPMs)\nhave demonstrated a strong ability to capture the underlying modes of data\ndistributions. In this paper, we harness the power of a Diffusion Autoencoder\nto generate multiple distinct counterfactual explanations. By clustering in the\nlatent space, we uncover the directions corresponding to the different modes\nwithin a class, enabling the generation of diverse and meaningful\ncounterfactuals. We introduce a novel methodology, DifCluE, which consistently\nidentifies these modes and produces more reliable counterfactual explanations.\nOur experimental results demonstrate that DifCluE outperforms the current\nstate-of-the-art in generating multiple counterfactual explanations, offering a\nsignificant advancement in model interpretability.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11509v1",
    "published_date": "2025-02-17 07:17:37 UTC",
    "updated_date": "2025-02-17 07:17:37 UTC"
  },
  {
    "arxiv_id": "2502.11508v1",
    "title": "Chinese Spelling Correction: A Comprehensive Survey of Progress, Challenges, and Opportunities",
    "authors": [
      "Changchun Liu",
      "Kai Zhang",
      "Junzhe Jiang",
      "Zixiao Kong",
      "Qi Liu",
      "Enhong Chen"
    ],
    "abstract": "Chinese Spelling Correction (CSC) is a critical task in natural language\nprocessing, aimed at detecting and correcting spelling errors in Chinese text.\nThis survey provides a comprehensive overview of CSC, tracing its evolution\nfrom pre-trained language models to large language models, and critically\nanalyzing their respective strengths and weaknesses in this domain. Moreover,\nwe further present a detailed examination of existing benchmark datasets,\nhighlighting their inherent challenges and limitations. Finally, we propose\npromising future research directions, particularly focusing on leveraging the\npotential of LLMs and their reasoning capabilities for improved CSC\nperformance. To the best of our knowledge, this is the first comprehensive\nsurvey dedicated to the field of CSC. We believe this work will serve as a\nvaluable resource for researchers, fostering a deeper understanding of the\nfield and inspiring future advancements.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11508v1",
    "published_date": "2025-02-17 07:17:27 UTC",
    "updated_date": "2025-02-17 07:17:27 UTC"
  },
  {
    "arxiv_id": "2502.11504v1",
    "title": "Accelerated Gradient-based Design Optimization Via Differentiable Physics-Informed Neural Operator: A Composites Autoclave Processing Case Study",
    "authors": [
      "Janak M. Patel",
      "Milad Ramezankhani",
      "Anirudh Deodhar",
      "Dagnachew Birru"
    ],
    "abstract": "Simulation and optimization are crucial for advancing the engineering design\nof complex systems and processes. Traditional optimization methods require\nsubstantial computational time and effort due to their reliance on\nresource-intensive simulations, such as finite element analysis, and the\ncomplexity of rigorous optimization algorithms. Data-agnostic AI-based\nsurrogate models, such as Physics-Informed Neural Operators (PINOs), offer a\npromising alternative to these conventional simulations, providing drastically\nreduced inference time, unparalleled data efficiency, and zero-shot\nsuper-resolution capability. However, the predictive accuracy of these models\nis often constrained to small, low-dimensional design spaces or systems with\nrelatively simple dynamics. To address this, we introduce a novel\nPhysics-Informed DeepONet (PIDON) architecture, which extends the capabilities\nof conventional neural operators to effectively model the nonlinear behavior of\ncomplex engineering systems across high-dimensional design spaces and a wide\nrange of dynamic design configurations. This new architecture outperforms\nexisting SOTA models, enabling better predictions across broader design spaces.\nLeveraging PIDON's differentiability, we integrate a gradient-based\noptimization approach using the Adam optimizer to efficiently determine optimal\ndesign variables. This forms an end-to-end gradient-based optimization\nframework that accelerates the design process while enhancing scalability and\nefficiency. We demonstrate the effectiveness of this framework in the\noptimization of aerospace-grade composites curing processes achieving a 3x\nspeedup in obtaining optimal design variables compared to gradient-free\nmethods. Beyond composites processing, the proposed model has the potential to\nbe used as a scalable and efficient optimization tool for broader applications\nin advanced engineering and digital twin systems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NA",
      "math.NA"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.11504v1",
    "published_date": "2025-02-17 07:11:46 UTC",
    "updated_date": "2025-02-17 07:11:46 UTC"
  },
  {
    "arxiv_id": "2502.18489v1",
    "title": "LLM4EFFI: Leveraging Large Language Models to Enhance Code Efficiency and Correctness",
    "authors": [
      "Tong Ye",
      "Weigang Huang",
      "Xuhong Zhang",
      "Tengfei Ma",
      "Peiyu Liu",
      "Jianwei Yin",
      "Wenhai Wang"
    ],
    "abstract": "Large Language Models (LLMs), particularly Code LLMs, have demonstrated\nimpressive performance in code generation. Current research primarily focuses\non the correctness of generated code, while efficiency remains less explored.\nRecent works have focused on modifying the initial version of the code to\nimprove its efficiency. However, such refinements are limited by the\nalgorithmic design and overall logic of the initial code, resulting in only\nincremental improvements. In contrast, when human developers write high-quality\ncode, they typically begin by designing several potential solutions at the\nlogical level, evaluating various algorithms and their complexities, and then\nproceeding to implement and optimize the solution. In this study, we introduce\n\\tool: \\uline{L}arge \\uline{L}anguage \\uline{M}odel for Code\n\\uline{Effi}ciency, a novel framework that enables LLMs to generate code that\nbalances both efficiency and correctness. Specifically, \\tool divides the\nefficiency optimization process into two domains: algorithmic exploration in\nthe logic domain and implementation optimization in the code domain. The\ncorrectness of the code is then guaranteed through a synthetic test case\nrefinement process. This approach, which prioritizes efficiency before ensuring\ncorrectness, offers a new paradigm for efficient code generation. Experiments\ndemonstrate that \\tool consistently improves both efficiency and correctness,\nachieving new state-of-the-art performance in code efficiency benchmarks across\nvarious LLM backbones.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18489v1",
    "published_date": "2025-02-17 07:01:18 UTC",
    "updated_date": "2025-02-17 07:01:18 UTC"
  },
  {
    "arxiv_id": "2502.11492v2",
    "title": "Why Vision Language Models Struggle with Visual Arithmetic? Towards Enhanced Chart and Geometry Understanding",
    "authors": [
      "Kung-Hsiang Huang",
      "Can Qin",
      "Haoyi Qiu",
      "Philippe Laban",
      "Shafiq Joty",
      "Caiming Xiong",
      "Chien-Sheng Wu"
    ],
    "abstract": "Vision Language Models (VLMs) have achieved remarkable progress in multimodal\ntasks, yet they often struggle with visual arithmetic, seemingly simple\ncapabilities like object counting or length comparison, which are essential for\nrelevant complex tasks like chart understanding and geometric reasoning. In\nthis work, we first investigate the root causes of this deficiency through a\nsuite of probing tasks focusing on basic visual arithmetic. Our analysis\nreveals that while pre-trained vision encoders typically capture sufficient\ninformation, the text decoder often fails to decode it correctly for arithmetic\nreasoning. To address this, we propose CogAlign, a novel post-training strategy\ninspired by Piaget's theory of cognitive development. CogAlign trains VLMs to\nrecognize invariant properties under visual transformations. We demonstrate\nthat this approach significantly improves the performance of three diverse VLMs\non our proposed probing tasks. Furthermore, CogAlign enhances performance by an\naverage of 4.6% on CHOCOLATE and 2.9% on MATH-VISION, outperforming or matching\nsupervised fine-tuning methods while requiring only 60% less training data.\nThese results highlight the effectiveness and generalizability of CogAlign in\nimproving fundamental visual arithmetic capabilities and their transfer to\ndownstream tasks.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "Code and data are available at\n  https://github.com/SalesforceAIResearch/CogAlign",
    "pdf_url": "http://arxiv.org/pdf/2502.11492v2",
    "published_date": "2025-02-17 06:54:49 UTC",
    "updated_date": "2025-03-10 02:13:57 UTC"
  },
  {
    "arxiv_id": "2502.11491v1",
    "title": "Ontology-Guided Reverse Thinking Makes Large Language Models Stronger on Knowledge Graph Question Answering",
    "authors": [
      "Runxuan Liu",
      "Bei Luo",
      "Jiaqi Li",
      "Baoxin Wang",
      "Ming Liu",
      "Dayong Wu",
      "Shijin Wang",
      "Bing Qin"
    ],
    "abstract": "Large language models (LLMs) have shown remarkable capabilities in natural\nlanguage processing. However, in knowledge graph question answering tasks\n(KGQA), there remains the issue of answering questions that require multi-hop\nreasoning. Existing methods rely on entity vector matching, but the purpose of\nthe question is abstract and difficult to match with specific entities. As a\nresult, it is difficult to establish reasoning paths to the purpose, which\nleads to information loss and redundancy. To address this issue, inspired by\nhuman reverse thinking, we propose Ontology-Guided Reverse Thinking (ORT), a\nnovel framework that constructs reasoning paths from purposes back to\nconditions. ORT operates in three key phases: (1) using LLM to extract purpose\nlabels and condition labels, (2) constructing label reasoning paths based on\nthe KG ontology, and (3) using the label reasoning paths to guide knowledge\nretrieval. Experiments on the WebQSP and CWQ datasets show that ORT achieves\nstate-of-the-art performance and significantly enhances the capability of LLMs\nfor KGQA.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11491v1",
    "published_date": "2025-02-17 06:53:15 UTC",
    "updated_date": "2025-02-17 06:53:15 UTC"
  },
  {
    "arxiv_id": "2502.15778v1",
    "title": "One for All: A General Framework of LLMs-based Multi-Criteria Decision Making on Human Expert Level",
    "authors": [
      "Hui Wang",
      "Fafa Zhang",
      "Chaoxu Mu"
    ],
    "abstract": "Multi-Criteria Decision Making~(MCDM) is widely applied in various fields,\nusing quantitative and qualitative analyses of multiple levels and attributes\nto support decision makers in making scientific and rational decisions in\ncomplex scenarios. However, traditional MCDM methods face bottlenecks in\nhigh-dimensional problems. Given the fact that Large Language Models~(LLMs)\nachieve impressive performance in various complex tasks, but limited work\nevaluates LLMs in specific MCDM problems with the help of human domain experts,\nwe further explore the capability of LLMs by proposing an LLM-based evaluation\nframework to automatically deal with general complex MCDM problems. Within the\nframework, we assess the performance of various typical open-source models, as\nwell as commercial models such as Claude and ChatGPT, on 3 important\napplications, these models can only achieve around 60\\% accuracy rate compared\nto the evaluation ground truth. Upon incorporation of Chain-of-Thought or\nfew-shot prompting, the accuracy rates rise to around 70\\%, and highly depend\non the model. In order to further improve the performance, a LoRA-based\nfine-tuning technique is employed. The experimental results show that the\naccuracy rates for different applications improve significantly to around 95\\%,\nand the performance difference is trivial between different models, indicating\nthat LoRA-based fine-tuned LLMs exhibit significant and stable advantages in\naddressing MCDM tasks and can provide human-expert-level solutions to a wide\nrange of MCDM challenges.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "11 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.15778v1",
    "published_date": "2025-02-17 06:47:20 UTC",
    "updated_date": "2025-02-17 06:47:20 UTC"
  },
  {
    "arxiv_id": "2502.11482v1",
    "title": "DATA: Decomposed Attention-based Task Adaptation for Rehearsal-Free Continual Learning",
    "authors": [
      "Huanxuan Liao",
      "Shizhu He",
      "Yupu Hao",
      "Jun Zhao",
      "Kang Liu"
    ],
    "abstract": "Continual learning (CL) is essential for Large Language Models (LLMs) to\nadapt to evolving real-world demands, yet they are susceptible to catastrophic\nforgetting (CF). While traditional CF solutions rely on expensive data\nrehearsal, recent rehearsal-free methods employ model-based and\nregularization-based strategies to address this issue. However, these\napproaches often neglect the model's plasticity, which is crucial to achieving\noptimal performance on newly learned tasks. Consequently, a key challenge in CL\nis striking a balance between preserving plasticity and mitigating CF. To\ntackle this challenge, we propose the $\\textbf{D}$ecomposed\n$\\textbf{A}$ttention-based $\\textbf{T}$ask $\\textbf{A}$daptation (DATA), which\nexplicitly decouples and learns both task-specific and task-shared knowledge\nusing high-rank and low-rank task adapters (e.g., LoRAs). For new tasks, DATA\ndynamically adjusts the weights of adapters of different ranks based on their\nrelevance and distinction from previous tasks, allowing the model to acquire\nnew task-specific skills while effectively retaining previously learned\nknowledge. Specifically, we implement a decomposed component weighting strategy\ncomprising learnable components that collectively generate attention-based\nweights, allowing the model to integrate and utilize diverse knowledge from\neach DATA. Extensive experiments on three widely used benchmarks demonstrate\nthat our proposed method achieves state-of-the-art performance. Notably, our\napproach significantly enhances model plasticity and mitigates CF by extending\nlearnable components and employing stochastic restoration during training\niterations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11482v1",
    "published_date": "2025-02-17 06:35:42 UTC",
    "updated_date": "2025-02-17 06:35:42 UTC"
  },
  {
    "arxiv_id": "2502.11481v1",
    "title": "Variable-frame CNNLSTM for Breast Nodule Classification using Ultrasound Videos",
    "authors": [
      "Xiangxiang Cui",
      "Zhongyu Li",
      "Xiayue Fan",
      "Peng Huang",
      "Ying Wang",
      "Meng Yang",
      "Shi Chang",
      "Jihua Zhu"
    ],
    "abstract": "The intersection of medical imaging and artificial intelligence has become an\nimportant research direction in intelligent medical treatment, particularly in\nthe analysis of medical images using deep learning for clinical diagnosis.\nDespite the advances, existing keyframe classification methods lack extraction\nof time series features, while ultrasonic video classification based on\nthree-dimensional convolution requires uniform frame numbers across patients,\nresulting in poor feature extraction efficiency and model classification\nperformance. This study proposes a novel video classification method based on\nCNN and LSTM, introducing NLP's long and short sentence processing scheme into\nvideo classification for the first time. The method reduces CNN-extracted image\nfeatures to 1x512 dimension, followed by sorting and compressing feature\nvectors for LSTM training. Specifically, feature vectors are sorted by patient\nvideo frame numbers and populated with padding value 0 to form variable\nbatches, with invalid padding values compressed before LSTM training to\nconserve computing resources. Experimental results demonstrate that our\nvariable-frame CNNLSTM method outperforms other approaches across all metrics,\nshowing improvements of 3-6% in F1 score and 1.5% in specificity compared to\nkeyframe methods. The variable-frame CNNLSTM also achieves better accuracy and\nprecision than equal-frame CNNLSTM. These findings validate the effectiveness\nof our approach in classifying variable-frame ultrasound videos and suggest\npotential applications in other medical imaging modalities.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11481v1",
    "published_date": "2025-02-17 06:35:37 UTC",
    "updated_date": "2025-02-17 06:35:37 UTC"
  },
  {
    "arxiv_id": "2502.13167v1",
    "title": "SmartLLM: Smart Contract Auditing using Custom Generative AI",
    "authors": [
      "Jun Kevin",
      "Pujianto Yugopuspito"
    ],
    "abstract": "Smart contracts are essential to decentralized finance (DeFi) and blockchain\necosystems but are increasingly vulnerable to exploits due to coding errors and\ncomplex attack vectors. Traditional static analysis tools and existing\nvulnerability detection methods often fail to address these challenges\ncomprehensively, leading to high false-positive rates and an inability to\ndetect dynamic vulnerabilities. This paper introduces SmartLLM, a novel\napproach leveraging fine-tuned LLaMA 3.1 models with Retrieval-Augmented\nGeneration (RAG) to enhance the accuracy and efficiency of smart contract\nauditing. By integrating domain-specific knowledge from ERC standards and\nemploying advanced techniques such as QLoRA for efficient fine-tuning, SmartLLM\nachieves superior performance compared to static analysis tools like Mythril\nand Slither, as well as zero-shot large language model (LLM) prompting methods\nsuch as GPT-3.5 and GPT-4. Experimental results demonstrate a perfect recall of\n100% and an accuracy score of 70%, highlighting the model's robustness in\nidentifying vulnerabilities, including reentrancy and access control issues.\nThis research advances smart contract security by offering a scalable and\neffective auditing solution, supporting the secure adoption of decentralized\napplications.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.13167v1",
    "published_date": "2025-02-17 06:22:05 UTC",
    "updated_date": "2025-02-17 06:22:05 UTC"
  },
  {
    "arxiv_id": "2502.11470v1",
    "title": "Optimized detection of cyber-attacks on IoT networks via hybrid deep learning models",
    "authors": [
      "Ahmed Bensaoud",
      "Jugal Kalita"
    ],
    "abstract": "The rapid expansion of Internet of Things (IoT) devices has increased the\nrisk of cyber-attacks, making effective detection essential for securing IoT\nnetworks. This work introduces a novel approach combining Self-Organizing Maps\n(SOMs), Deep Belief Networks (DBNs), and Autoencoders to detect known and\npreviously unseen attack patterns. A comprehensive evaluation using simulated\nand real-world traffic data is conducted, with models optimized via Particle\nSwarm Optimization (PSO). The system achieves an accuracy of up to 99.99% and\nMatthews Correlation Coefficient (MCC) values exceeding 99.50%. Experiments on\nNSL-KDD, UNSW-NB15, and CICIoT2023 confirm the model's strong performance\nacross diverse attack types. These findings suggest that the proposed method\nenhances IoT security by identifying emerging threats and adapting to evolving\nattack strategies.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11470v1",
    "published_date": "2025-02-17 06:01:06 UTC",
    "updated_date": "2025-02-17 06:01:06 UTC"
  },
  {
    "arxiv_id": "2502.13166v1",
    "title": "Large Language Models Can Help Mitigate Barren Plateaus",
    "authors": [
      "Jun Zhuang",
      "Chaowen Guan"
    ],
    "abstract": "In the era of noisy intermediate-scale quantum (NISQ) computing, Quantum\nNeural Networks (QNNs) have emerged as a promising approach for various\napplications, yet their training is often hindered by barren plateaus (BPs),\nwhere gradient variance vanishes exponentially as the model size increases. To\naddress this challenge, we propose a new Large Language Model (LLM)-driven\nsearch framework, AdaInit, that iteratively searches for optimal initial\nparameters of QNNs to maximize gradient variance and therefore mitigate BPs.\nUnlike conventional one-time initialization methods, AdaInit dynamically\nrefines QNN's initialization using LLMs with adaptive prompting. Theoretical\nanalysis of the Expected Improvement (EI) proves a supremum for the search,\nensuring this process can eventually identify the optimal initial parameter of\nthe QNN. Extensive experiments across four public datasets demonstrate that\nAdaInit significantly enhances QNN's trainability compared to classic\ninitialization methods, validating its effectiveness in mitigating BPs.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "quant-ph",
    "comment": "TL;DR: We propose a new LLM-driven framework designed for mitigating\n  barren plateaus",
    "pdf_url": "http://arxiv.org/pdf/2502.13166v1",
    "published_date": "2025-02-17 05:57:15 UTC",
    "updated_date": "2025-02-17 05:57:15 UTC"
  },
  {
    "arxiv_id": "2502.11458v1",
    "title": "Towards Efficient Pre-training: Exploring FP4 Precision in Large Language Models",
    "authors": [
      "Jiecheng Zhou",
      "Ding Tang",
      "Rong Fu",
      "Boni Hu",
      "Haoran Xu",
      "Yi Wang",
      "Zhilin Pei",
      "Zhongling Su",
      "Liang Liu",
      "Xingcheng Zhang",
      "Weiming Zhang"
    ],
    "abstract": "The burgeoning computational demands for training large language models\n(LLMs) necessitate efficient methods, including quantized training, which\nleverages low-bit arithmetic operations to reduce costs. While FP8 precision\nhas shown potential, leveraging FP4 remains challenging due to inherent\nquantization errors and limited representation capability. Based on the\nTransformer architecture, we present an FP4 training scheme for LLMs,\novercoming these obstacles through mixed-precision quantization strategies\ntailed for different modules and training stages. This allows us to apply the\nprecision level suitable to distinct components within the model, ensuring that\nmulti-head attention and linear layers are handled appropriately. Our\npretraining recipe ensures stability in backpropagation by incorporating\nfine-grained quantization methods with a target precision training schedule.\nExperimental results demonstrate that our FP4 training scheme achieves accuracy\ncomparable to BF16 and FP8, with smaller theoretical computational cost. With\nthe advent of next-generation hardware supporting FP4, our method sets the\nfoundation for efficient ultra-low precision training.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 2 figure",
    "pdf_url": "http://arxiv.org/pdf/2502.11458v1",
    "published_date": "2025-02-17 05:33:11 UTC",
    "updated_date": "2025-02-17 05:33:11 UTC"
  },
  {
    "arxiv_id": "2502.11457v1",
    "title": "Aligning Sentence Simplification with ESL Learner's Proficiency for Language Acquisition",
    "authors": [
      "Guanlin Li",
      "Yuki Arase",
      "Noel Crespi"
    ],
    "abstract": "Text simplification is crucial for improving accessibility and comprehension\nfor English as a Second Language (ESL) learners. This study goes a step further\nand aims to facilitate ESL learners' language acquisition by simplification.\nSpecifically, we propose simplifying complex sentences to appropriate levels\nfor learners while also increasing vocabulary coverage of the target level in\nthe simplifications. We achieve this without a parallel corpus by conducting\nreinforcement learning on a large language model. Our method employs\ntoken-level and sentence-level rewards, and iteratively trains the model on its\nself-generated outputs to guide the model to search for simplification\nhypotheses that satisfy the target attributes. Experiment results on CEFR-SP\nand TurkCorpus datasets show that the proposed method can effectively increase\nthe frequency and diversity of vocabulary of the target level by more than\n$20\\%$ compared to baseline models, while maintaining high simplification\nquality.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "NAACL2025 main",
    "pdf_url": "http://arxiv.org/pdf/2502.11457v1",
    "published_date": "2025-02-17 05:32:56 UTC",
    "updated_date": "2025-02-17 05:32:56 UTC"
  },
  {
    "arxiv_id": "2502.11456v1",
    "title": "Leveraging Labelled Data Knowledge: A Cooperative Rectification Learning Network for Semi-supervised 3D Medical Image Segmentation",
    "authors": [
      "Yanyan Wang",
      "Kechen Song",
      "Yuyuan Liu",
      "Shuai Ma",
      "Yunhui Yan",
      "Gustavo Carneiro"
    ],
    "abstract": "Semi-supervised 3D medical image segmentation aims to achieve accurate\nsegmentation using few labelled data and numerous unlabelled data. The main\nchallenge in the design of semi-supervised learning methods consists in the\neffective use of the unlabelled data for training. A promising solution\nconsists of ensuring consistent predictions across different views of the data,\nwhere the efficacy of this strategy depends on the accuracy of the\npseudo-labels generated by the model for this consistency learning strategy. In\nthis paper, we introduce a new methodology to produce high-quality\npseudo-labels for a consistency learning strategy to address semi-supervised 3D\nmedical image segmentation. The methodology has three important contributions.\nThe first contribution is the Cooperative Rectification Learning Network (CRLN)\nthat learns multiple prototypes per class to be used as external knowledge\npriors to adaptively rectify pseudo-labels at the voxel level. The second\ncontribution consists of the Dynamic Interaction Module (DIM) to facilitate\npairwise and cross-class interactions between prototypes and multi-resolution\nimage features, enabling the production of accurate voxel-level clues for\npseudo-label rectification. The third contribution is the Cooperative Positive\nSupervision (CPS), which optimises uncertain representations to align with\nunassertive representations of their class distributions, improving the model's\naccuracy in classifying uncertain regions. Extensive experiments on three\npublic 3D medical segmentation datasets demonstrate the effectiveness and\nsuperiority of our semi-supervised learning method.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Medical Image Analysis",
    "pdf_url": "http://arxiv.org/pdf/2502.11456v1",
    "published_date": "2025-02-17 05:29:50 UTC",
    "updated_date": "2025-02-17 05:29:50 UTC"
  },
  {
    "arxiv_id": "2502.11453v1",
    "title": "Connector-S: A Survey of Connectors in Multi-modal Large Language Models",
    "authors": [
      "Xun Zhu",
      "Zheng Zhang",
      "Xi Chen",
      "Yiming Shi",
      "Miao Li",
      "Ji Wu"
    ],
    "abstract": "With the rapid advancements in multi-modal large language models (MLLMs),\nconnectors play a pivotal role in bridging diverse modalities and enhancing\nmodel performance. However, the design and evolution of connectors have not\nbeen comprehensively analyzed, leaving gaps in understanding how these\ncomponents function and hindering the development of more powerful connectors.\nIn this survey, we systematically review the current progress of connectors in\nMLLMs and present a structured taxonomy that categorizes connectors into atomic\noperations (mapping, compression, mixture of experts) and holistic designs\n(multi-layer, multi-encoder, multi-modal scenarios), highlighting their\ntechnical contributions and advancements. Furthermore, we discuss several\npromising research frontiers and challenges, including high-resolution input,\ndynamic compression, guide information selection, combination strategy, and\ninterpretability. This survey is intended to serve as a foundational reference\nand a clear roadmap for researchers, providing valuable insights into the\ndesign and optimization of next-generation connectors to enhance the\nperformance and adaptability of MLLMs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11453v1",
    "published_date": "2025-02-17 05:28:04 UTC",
    "updated_date": "2025-02-17 05:28:04 UTC"
  },
  {
    "arxiv_id": "2502.11450v1",
    "title": "Fishing For Cheap And Efficient Pruners At Initialization",
    "authors": [
      "Ivo Gollini Navarrete",
      "Nicolas Mauricio Cuadrado",
      "Jose Renato Restom",
      "Martin Takáč",
      "Samuel Horváth"
    ],
    "abstract": "Pruning offers a promising solution to mitigate the associated costs and\nenvironmental impact of deploying large deep neural networks (DNNs).\nTraditional approaches rely on computationally expensive trained models or\ntime-consuming iterative prune-retrain cycles, undermining their utility in\nresource-constrained settings. To address this issue, we build upon the\nestablished principles of saliency (LeCun et al., 1989) and connection\nsensitivity (Lee et al., 2018) to tackle the challenging problem of one-shot\npruning neural networks (NNs) before training (PBT) at initialization. We\nintroduce Fisher-Taylor Sensitivity (FTS), a computationally cheap and\nefficient pruning criterion based on the empirical Fisher Information Matrix\n(FIM) diagonal, offering a viable alternative for integrating first- and\nsecond-order information to identify a model's structurally important\nparameters. Although the FIM-Hessian equivalency only holds for convergent\nmodels that maximize the likelihood, recent studies (Karakida et al., 2019)\nsuggest that, even at initialization, the FIM captures essential geometric\ninformation of parameters in overparameterized NNs, providing the basis for our\nmethod. Finally, we demonstrate empirically that layer collapse, a critical\nlimitation of data-dependent pruning methodologies, is easily overcome by\npruning within a single training epoch after initialization. We perform\nexperiments on ResNet18 and VGG19 with CIFAR-10 and CIFAR-100, widely used\nbenchmarks in pruning research. Our method achieves competitive performance\nagainst state-of-the-art techniques for one-shot PBT, even under extreme\nsparsity conditions. Our code is made available to the public.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "68T05",
      "I.2.6; C.1.3"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages of main content (excluding references), 2 figures, 2 tables,\n  1 algorithm, and 11 pages of appendix. Code available at\n  https://github.com/Gollini/Fisher_Taylor_Sensitivity",
    "pdf_url": "http://arxiv.org/pdf/2502.11450v1",
    "published_date": "2025-02-17 05:22:23 UTC",
    "updated_date": "2025-02-17 05:22:23 UTC"
  },
  {
    "arxiv_id": "2502.11448v2",
    "title": "AGrail: A Lifelong Agent Guardrail with Effective and Adaptive Safety Detection",
    "authors": [
      "Weidi Luo",
      "Shenghong Dai",
      "Xiaogeng Liu",
      "Suman Banerjee",
      "Huan Sun",
      "Muhao Chen",
      "Chaowei Xiao"
    ],
    "abstract": "The rapid advancements in Large Language Models (LLMs) have enabled their\ndeployment as autonomous agents for handling complex tasks in dynamic\nenvironments. These LLMs demonstrate strong problem-solving capabilities and\nadaptability to multifaceted scenarios. However, their use as agents also\nintroduces significant risks, including task-specific risks, which are\nidentified by the agent administrator based on the specific task requirements\nand constraints, and systemic risks, which stem from vulnerabilities in their\ndesign or interactions, potentially compromising confidentiality, integrity, or\navailability (CIA) of information and triggering security risks. Existing\ndefense agencies fail to adaptively and effectively mitigate these risks. In\nthis paper, we propose AGrail, a lifelong agent guardrail to enhance LLM agent\nsafety, which features adaptive safety check generation, effective safety check\noptimization, and tool compatibility and flexibility. Extensive experiments\ndemonstrate that AGrail not only achieves strong performance against\ntask-specific and system risks but also exhibits transferability across\ndifferent LLM agents' tasks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11448v2",
    "published_date": "2025-02-17 05:12:33 UTC",
    "updated_date": "2025-02-18 05:37:44 UTC"
  },
  {
    "arxiv_id": "2502.11447v2",
    "title": "Does Editing Provide Evidence for Localization?",
    "authors": [
      "Zihao Wang",
      "Victor Veitch"
    ],
    "abstract": "A basic aspiration for interpretability research in large language models is\nto \"localize\" semantically meaningful behaviors to particular components within\nthe LLM. There are various heuristics for finding candidate locations within\nthe LLM. Once a candidate localization is found, it can be assessed by editing\nthe internal representations at the corresponding localization and checking\nwhether this induces model behavior that is consistent with the semantic\ninterpretation of the localization. The question we address here is: how strong\nis the evidence provided by such edits? To evaluate the localization claim, we\nwant to assess the effect of the optimal intervention at a particular location.\nThe key new technical tool is a way of adapting LLM alignment techniques to\nfind such optimal localized edits. With this tool in hand, we give an example\nwhere the edit-based evidence for localization appears strong, but where\nlocalization clearly fails. Indeed, we find that optimal edits at random\nlocalizations can be as effective as aligning the full model. In aggregate, our\nresults suggest that merely observing that localized edits induce targeted\nchanges in behavior provides little to no evidence that these locations\nactually encode the target behavior.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "68T50",
      "I.2.7; I.2.6; F.1.1"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11447v2",
    "published_date": "2025-02-17 05:09:46 UTC",
    "updated_date": "2025-02-19 06:45:25 UTC"
  },
  {
    "arxiv_id": "2502.11442v1",
    "title": "Multi-Turn Multi-Modal Question Clarification for Enhanced Conversational Understanding",
    "authors": [
      "Kimia Ramezan",
      "Alireza Amiri Bavandpour",
      "Yifei Yuan",
      "Clemencia Siro",
      "Mohammad Aliannejadi"
    ],
    "abstract": "Conversational query clarification enables users to refine their search\nqueries through interactive dialogue, improving search effectiveness.\nTraditional approaches rely on text-based clarifying questions, which often\nfail to capture complex user preferences, particularly those involving visual\nattributes. While recent work has explored single-turn multi-modal\nclarification with images alongside text, such methods do not fully support the\nprogressive nature of user intent refinement over multiple turns. Motivated by\nthis, we introduce the Multi-turn Multi-modal Clarifying Questions (MMCQ) task,\nwhich combines text and visual modalities to refine user queries in a\nmulti-turn conversation. To facilitate this task, we create a large-scale\ndataset named ClariMM comprising over 13k multi-turn interactions and 33k\nquestion-answer pairs containing multi-modal clarifying questions. We propose\nMario, a retrieval framework that employs a two-phase ranking strategy: initial\nretrieval with BM25, followed by a multi-modal generative re-ranking model that\nintegrates textual and visual information from conversational history. Our\nexperiments show that multi-turn multi-modal clarification outperforms\nuni-modal and single-turn approaches, improving MRR by 12.88%. The gains are\nmost significant in longer interactions, demonstrating the value of progressive\nrefinement for complex queries.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11442v1",
    "published_date": "2025-02-17 04:58:14 UTC",
    "updated_date": "2025-02-17 04:58:14 UTC"
  },
  {
    "arxiv_id": "2502.11439v1",
    "title": "An Efficient Row-Based Sparse Fine-Tuning",
    "authors": [
      "Cen-Jhih Li",
      "Aditya Bhaskara"
    ],
    "abstract": "Fine-tuning is an important step in adapting foundation models such as large\nlanguage models to downstream tasks. To make this step more accessible to users\nwith limited computational budgets, it is crucial to develop fine-tuning\nmethods that are memory and computationally efficient. Sparse Fine-tuning (SFT)\nand Low-rank adaptation (LoRA) are two frameworks that have emerged for\naddressing this problem and have been adopted widely in practice. In this work,\nwe develop a new SFT framework, based on ideas from neural network pruning. At\na high level, we first identify \"important\" neurons/nodes using feature\nimportance metrics from network pruning (specifically, we use the structural\npruning method), and then perform fine-tuning by restricting to weights\ninvolving these neurons. Using experiments on common language tasks, we\ndemonstrate that our method significantly improves the memory efficiency of SFT\nwithout increasing training time complexity and implementation complexity,\nwhile achieving accuracy comparable to state-of-the-art methods such as LoRA\nand its variants.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11439v1",
    "published_date": "2025-02-17 04:54:42 UTC",
    "updated_date": "2025-02-17 04:54:42 UTC"
  },
  {
    "arxiv_id": "2502.11437v1",
    "title": "Learning Dexterous Bimanual Catch Skills through Adversarial-Cooperative Heterogeneous-Agent Reinforcement Learning",
    "authors": [
      "Taewoo Kim",
      "Youngwoo Yoon",
      "Jaehong Kim"
    ],
    "abstract": "Robotic catching has traditionally focused on single-handed systems, which\nare limited in their ability to handle larger or more complex objects. In\ncontrast, bimanual catching offers significant potential for improved dexterity\nand object handling but introduces new challenges in coordination and control.\nIn this paper, we propose a novel framework for learning dexterous bimanual\ncatching skills using Heterogeneous-Agent Reinforcement Learning (HARL). Our\napproach introduces an adversarial reward scheme, where a throw agent increases\nthe difficulty of throws-adjusting speed-while a catch agent learns to\ncoordinate both hands to catch objects under these evolving conditions. We\nevaluate the framework in simulated environments using 15 different objects,\ndemonstrating robustness and versatility in handling diverse objects. Our\nmethod achieved approximately a 2x increase in catching reward compared to\nsingle-agent baselines across 15 diverse objects.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "ICRA 2025 Accepted",
    "pdf_url": "http://arxiv.org/pdf/2502.11437v1",
    "published_date": "2025-02-17 04:50:45 UTC",
    "updated_date": "2025-02-17 04:50:45 UTC"
  },
  {
    "arxiv_id": "2502.11435v1",
    "title": "SMART: Self-Aware Agent for Tool Overuse Mitigation",
    "authors": [
      "Cheng Qian",
      "Emre Can Acikgoz",
      "Hongru Wang",
      "Xiusi Chen",
      "Avirup Sil",
      "Dilek Hakkani-Tür",
      "Gokhan Tur",
      "Heng Ji"
    ],
    "abstract": "Current Large Language Model (LLM) agents demonstrate strong reasoning and\ntool use capabilities, but often lack self-awareness, failing to balance these\napproaches effectively. This imbalance leads to Tool Overuse, where models\nunnecessarily rely on external tools for tasks solvable with parametric\nknowledge, increasing computational overhead. Inspired by human metacognition,\nwe introduce SMART (Strategic Model-Aware Reasoning with Tools), a paradigm\nthat enhances an agent's self-awareness to optimize task handling and reduce\ntool overuse. To support this paradigm, we introduce SMART-ER, a dataset\nspanning three domains, where reasoning alternates between parametric knowledge\nand tool-dependent steps, with each step enriched by rationales explaining when\ntools are necessary. Through supervised training, we develop SMARTAgent, a\nfamily of models that dynamically balance parametric knowledge and tool use.\nEvaluations show that SMARTAgent reduces tool use by 24% while improving\nperformance by over 37%, enabling 7B-scale models to match its 70B counterpart\nand GPT-4o. Additionally, SMARTAgent generalizes to out-of-distribution test\ndata like GSM8K and MINTQA, maintaining accuracy with just one-fifth the tool\ncalls. These highlight the potential of strategic tool use to enhance\nreasoning, mitigate overuse, and bridge the gap between model size and\nperformance, advancing intelligent and resource-efficient agent designs.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "18 pages, 8 tables, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.11435v1",
    "published_date": "2025-02-17 04:50:37 UTC",
    "updated_date": "2025-02-17 04:50:37 UTC"
  },
  {
    "arxiv_id": "2502.15777v1",
    "title": "TSS GAZ PTP: Towards Improving Gumbel AlphaZero with Two-stage Self-play for Multi-constrained Electric Vehicle Routing Problems",
    "authors": [
      "Hui Wang",
      "Xufeng Zhang",
      "Xiaoyu Zhang",
      "Zhenhuan Ding",
      "Chaoxu Mu"
    ],
    "abstract": "Recently, Gumbel AlphaZero~(GAZ) was proposed to solve classic combinatorial\noptimization problems such as TSP and JSSP by creating a carefully designed\ncompetition model~(consisting of a learning player and a competitor player),\nwhich leverages the idea of self-play. However, if the competitor is too strong\nor too weak, the effectiveness of self-play training can be reduced,\nparticularly in complex CO problems. To address this problem, we further\npropose a two-stage self-play strategy to improve the GAZ method~(named TSS GAZ\nPTP). In the first stage, the learning player uses the enhanced policy network\nbased on the Gumbel Monte Carlo Tree Search~(MCTS), and the competitor uses the\nhistorical best trained policy network~(acts as a greedy player). In the second\nstage, we employ Gumbel MCTS for both players, which makes the competition\nfiercer so that both players can continuously learn smarter trajectories. We\nfirst investigate the performance of our proposed TSS GAZ PTP method on TSP\nsince it is also used as a test problem by the original GAZ. The results show\nthe superior performance of TSS GAZ PTP. Then we extend TSS GAZ PTP to deal\nwith multi-constrained Electric Vehicle Routing Problems~(EVRP), which is a\nrecently well-known real application research topic and remains challenging as\na complex CO problem. Impressively, the experimental results show that the TSS\nGAZ PTP outperforms the state-of-the-art Deep Reinforcement Learning methods in\nall types of instances tested and outperforms the optimization solver in tested\nlarge-scale instances, indicating the importance and promising of employing\nmore dynamic self-play strategies for complex CO problems.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "11 pages,9 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.15777v1",
    "published_date": "2025-02-17 04:47:36 UTC",
    "updated_date": "2025-02-17 04:47:36 UTC"
  },
  {
    "arxiv_id": "2502.14892v1",
    "title": "EgoSpeak: Learning When to Speak for Egocentric Conversational Agents in the Wild",
    "authors": [
      "Junhyeok Kim",
      "Min Soo Kim",
      "Jiwan Chung",
      "Jungbin Cho",
      "Jisoo Kim",
      "Sungwoong Kim",
      "Gyeongbo Sim",
      "Youngjae Yu"
    ],
    "abstract": "Predicting when to initiate speech in real-world environments remains a\nfundamental challenge for conversational agents. We introduce EgoSpeak, a novel\nframework for real-time speech initiation prediction in egocentric streaming\nvideo. By modeling the conversation from the speaker's first-person viewpoint,\nEgoSpeak is tailored for human-like interactions in which a conversational\nagent must continuously observe its environment and dynamically decide when to\ntalk. Our approach bridges the gap between simplified experimental setups and\ncomplex natural conversations by integrating four key capabilities: (1)\nfirst-person perspective, (2) RGB processing, (3) online processing, and (4)\nuntrimmed video processing. We also present YT-Conversation, a diverse\ncollection of in-the-wild conversational videos from YouTube, as a resource for\nlarge-scale pretraining. Experiments on EasyCom and Ego4D demonstrate that\nEgoSpeak outperforms random and silence-based baselines in real time. Our\nresults also highlight the importance of multimodal input and context length in\neffectively deciding when to speak.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "NAACL 2025 Findings. Project page at\n  https://jun297.github.io/EgoSpeak/",
    "pdf_url": "http://arxiv.org/pdf/2502.14892v1",
    "published_date": "2025-02-17 04:47:12 UTC",
    "updated_date": "2025-02-17 04:47:12 UTC"
  },
  {
    "arxiv_id": "2502.11433v3",
    "title": "FLAG-Trader: Fusion LLM-Agent with Gradient-based Reinforcement Learning for Financial Trading",
    "authors": [
      "Guojun Xiong",
      "Zhiyang Deng",
      "Keyi Wang",
      "Yupeng Cao",
      "Haohang Li",
      "Yangyang Yu",
      "Xueqing Peng",
      "Mingquan Lin",
      "Kaleb E Smith",
      "Xiao-Yang Liu",
      "Jimin Huang",
      "Sophia Ananiadou",
      "Qianqian Xie"
    ],
    "abstract": "Large language models (LLMs) fine-tuned on multimodal financial data have\ndemonstrated impressive reasoning capabilities in various financial tasks.\nHowever, they often struggle with multi-step, goal-oriented scenarios in\ninteractive financial markets, such as trading, where complex agentic\napproaches are required to improve decision-making. To address this, we propose\n\\textsc{FLAG-Trader}, a unified architecture integrating linguistic processing\n(via LLMs) with gradient-driven reinforcement learning (RL) policy\noptimization, in which a partially fine-tuned LLM acts as the policy network,\nleveraging pre-trained knowledge while adapting to the financial domain through\nparameter-efficient fine-tuning. Through policy gradient optimization driven by\ntrading rewards, our framework not only enhances LLM performance in trading but\nalso improves results on other financial-domain tasks. We present extensive\nempirical evidence to validate these enhancements.",
    "categories": [
      "cs.AI",
      "cs.CE",
      "q-fin.TR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11433v3",
    "published_date": "2025-02-17 04:45:53 UTC",
    "updated_date": "2025-02-19 03:40:56 UTC"
  },
  {
    "arxiv_id": "2502.12214v1",
    "title": "Zero Token-Driven Deep Thinking in LLMs: Unlocking the Full Potential of Existing Parameters via Cyclic Refinement",
    "authors": [
      "Guanghao Li",
      "Wenhao Jiang",
      "Li Shen",
      "Ming Tang",
      "Chun Yuan"
    ],
    "abstract": "Resource limitations often constrain the parameter counts of Large Language\nModels (LLMs), hindering their performance. While existing methods employ\nparameter sharing to reuse the same parameter set under fixed budgets, such\napproaches typically force each layer to assume multiple roles with a\npredetermined number of iterations, restricting efficiency and adaptability. In\nthis work, we propose the Zero Token Transformer (ZTT), which features a\nhead-tail decoupled parameter cycling method. We disentangle the first (head)\nand last (tail) layers from parameter cycling and iteratively refine only the\nintermediate layers. Furthermore, we introduce a Zero-Token Mechanism, an\ninternal architectural component rather than an input token, to guide\nlayer-specific computation. At each cycle, the model retrieves a zero token\n(with trainable key values) from a Zero-Token Pool, integrating it alongside\nregular tokens in the attention mechanism. The corresponding attention scores\nnot only reflect each layer's computational importance but also enable dynamic\nearly exits without sacrificing overall model accuracy. Our approach achieves\nsuperior performance under tight parameter budgets, effectively reduces\ncomputational overhead via early exits, and can be readily applied to fine-tune\nexisting pre-trained models for enhanced efficiency and adaptability.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12214v1",
    "published_date": "2025-02-17 04:37:22 UTC",
    "updated_date": "2025-02-17 04:37:22 UTC"
  },
  {
    "arxiv_id": "2502.11425v1",
    "title": "Counterfactual-Consistency Prompting for Relative Temporal Understanding in Large Language Models",
    "authors": [
      "Jongho Kim",
      "Seung-won Hwang"
    ],
    "abstract": "Despite the advanced capabilities of large language models (LLMs), their\ntemporal reasoning ability remains underdeveloped. Prior works have highlighted\nthis limitation, particularly in maintaining temporal consistency when\nunderstanding events. For example, models often confuse mutually exclusive\ntemporal relations like ``before'' and ``after'' between events and make\ninconsistent predictions. In this work, we tackle the issue of temporal\ninconsistency in LLMs by proposing a novel counterfactual prompting approach.\nOur method generates counterfactual questions and enforces collective\nconstraints, enhancing the model's consistency. We evaluate our method on\nmultiple datasets, demonstrating significant improvements in event ordering for\nexplicit and implicit events and temporal commonsense understanding by\neffectively addressing temporal inconsistencies.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2502.11425v1",
    "published_date": "2025-02-17 04:37:07 UTC",
    "updated_date": "2025-02-17 04:37:07 UTC"
  },
  {
    "arxiv_id": "2502.11422v1",
    "title": "Planning of Heuristics: Strategic Planning on Large Language Models with Monte Carlo Tree Search for Automating Heuristic Optimization",
    "authors": [
      "Chaoxu Mu",
      "Xufeng Zhang",
      "Hui Wang"
    ],
    "abstract": "Heuristics have achieved great success in solving combinatorial optimization\nproblems (COPs). However, heuristics designed by humans require too much domain\nknowledge and testing time. Given the fact that Large Language Models (LLMs)\npossess strong capabilities to understand and generate content, and a knowledge\nbase that covers various domains, which offer a novel way to automatically\noptimize heuristics. Therefore, we propose Planning of Heuristics (PoH), an\noptimization method that integrates the self-reflection of LLMs with the Monte\nCarlo Tree Search (MCTS), a well-known planning algorithm. PoH iteratively\nrefines generated heuristics by evaluating their performance and providing\nimprovement suggestions. Our method enables to iteratively evaluate the\ngenerated heuristics (states) and improve them based on the improvement\nsuggestions (actions) and evaluation results (rewards), by effectively\nsimulating future states to search for paths with higher rewards. In this\npaper, we apply PoH to solve the Traveling Salesman Problem (TSP) and the Flow\nShop Scheduling Problem (FSSP). The experimental results show that PoH\noutperforms other hand-crafted heuristics and Automatic Heuristic Design (AHD)\nby other LLMs-based methods, and achieves the significant improvements and the\nstate-of-the-art performance of our proposed method in automating heuristic\noptimization with LLMs to solve COPs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "17 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.11422v1",
    "published_date": "2025-02-17 04:35:01 UTC",
    "updated_date": "2025-02-17 04:35:01 UTC"
  },
  {
    "arxiv_id": "2502.11418v2",
    "title": "TimeCAP: Learning to Contextualize, Augment, and Predict Time Series Events with Large Language Model Agents",
    "authors": [
      "Geon Lee",
      "Wenchao Yu",
      "Kijung Shin",
      "Wei Cheng",
      "Haifeng Chen"
    ],
    "abstract": "Time series data is essential in various applications, including climate\nmodeling, healthcare monitoring, and financial analytics. Understanding the\ncontextual information associated with real-world time series data is often\nessential for accurate and reliable event predictions. In this paper, we\nintroduce TimeCAP, a time-series processing framework that creatively employs\nLarge Language Models (LLMs) as contextualizers of time series data, extending\ntheir typical usage as predictors. TimeCAP incorporates two independent LLM\nagents: one generates a textual summary capturing the context of the time\nseries, while the other uses this enriched summary to make more informed\npredictions. In addition, TimeCAP employs a multi-modal encoder that synergizes\nwith the LLM agents, enhancing predictive performance through mutual\naugmentation of inputs with in-context examples. Experimental results on\nreal-world datasets demonstrate that TimeCAP outperforms state-of-the-art\nmethods for time series event prediction, including those utilizing LLMs as\npredictors, achieving an average improvement of 28.75% in F1 score.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.11418v2",
    "published_date": "2025-02-17 04:17:27 UTC",
    "updated_date": "2025-03-10 04:15:20 UTC"
  },
  {
    "arxiv_id": "2502.13165v1",
    "title": "HedgeAgents: A Balanced-aware Multi-agent Financial Trading System",
    "authors": [
      "Xiangyu Li",
      "Yawen Zeng",
      "Xiaofen Xing",
      "Jin Xu",
      "Xiangmin Xu"
    ],
    "abstract": "As automated trading gains traction in the financial market, algorithmic\ninvestment strategies are increasingly prominent. While Large Language Models\n(LLMs) and Agent-based models exhibit promising potential in real-time market\nanalysis and trading decisions, they still experience a significant -20% loss\nwhen confronted with rapid declines or frequent fluctuations, impeding their\npractical application. Hence, there is an imperative to explore a more robust\nand resilient framework. This paper introduces an innovative multi-agent\nsystem, HedgeAgents, aimed at bolstering system robustness via ``hedging''\nstrategies. In this well-balanced system, an array of hedging agents has been\ntailored, where HedgeAgents consist of a central fund manager and multiple\nhedging experts specializing in various financial asset classes. These agents\nleverage LLMs' cognitive capabilities to make decisions and coordinate through\nthree types of conferences. Benefiting from the powerful understanding of LLMs,\nour HedgeAgents attained a 70% annualized return and a 400% total return over a\nperiod of 3 years. Moreover, we have observed with delight that HedgeAgents can\neven formulate investment experience comparable to those of human experts\n(https://hedgeagents.github.io/).",
    "categories": [
      "cs.MA",
      "cs.AI",
      "q-fin.TR"
    ],
    "primary_category": "cs.MA",
    "comment": "This paper has been accepted by The Web Conference 2025 (WWW 2025)\n  and selected for an oral presentation",
    "pdf_url": "http://arxiv.org/pdf/2502.13165v1",
    "published_date": "2025-02-17 04:13:19 UTC",
    "updated_date": "2025-02-17 04:13:19 UTC"
  },
  {
    "arxiv_id": "2502.13164v1",
    "title": "Multi-Agent Actor-Critic Generative AI for Query Resolution and Analysis",
    "authors": [
      "Mohammad Wali Ur Rahman",
      "Ric Nevarez",
      "Lamia Tasnim Mim",
      "Salim Hariri"
    ],
    "abstract": "In this paper, we introduce MASQRAD (Multi-Agent Strategic Query Resolution\nand Diagnostic tool), a transformative framework for query resolution based on\nthe actor-critic model, which utilizes multiple generative AI agents. MASQRAD\nis excellent at translating imprecise or ambiguous user inquiries into precise\nand actionable requests. This framework generates pertinent visualizations and\nresponses to these focused queries, as well as thorough analyses and insightful\ninterpretations for users. MASQRAD addresses the common shortcomings of\nexisting solutions in domains that demand fast and precise data interpretation,\nsuch as their incapacity to successfully apply AI for generating actionable\ninsights and their challenges with the inherent ambiguity of user queries.\nMASQRAD functions as a sophisticated multi-agent system but \"masquerades\" to\nusers as a single AI entity, which lowers errors and enhances data interaction.\nThis approach makes use of three primary AI agents: Actor Generative AI, Critic\nGenerative AI, and Expert Analysis Generative AI. Each is crucial for creating,\nenhancing, and evaluating data interactions. The Actor AI generates Python\nscripts to generate data visualizations from large datasets within operational\nconstraints, and the Critic AI rigorously refines these scripts through\nmulti-agent debate. Finally, the Expert Analysis AI contextualizes the outcomes\nto aid in decision-making. With an accuracy rate of 87\\% when handling tasks\nrelated to natural language visualization, MASQRAD establishes new benchmarks\nfor automated data interpretation and showcases a noteworthy advancement that\nhas the potential to revolutionize AI-driven applications.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "Accepted for publication in IEEE Transactions on Artificial\n  Intelligence",
    "pdf_url": "http://arxiv.org/pdf/2502.13164v1",
    "published_date": "2025-02-17 04:03:15 UTC",
    "updated_date": "2025-02-17 04:03:15 UTC"
  },
  {
    "arxiv_id": "2502.12213v1",
    "title": "Spatiotemporal-aware Trend-Seasonality Decomposition Network for Traffic Flow Forecasting",
    "authors": [
      "Lingxiao Cao",
      "Bin Wang",
      "Guiyuan Jiang",
      "Yanwei Yu",
      "Junyu Dong"
    ],
    "abstract": "Traffic prediction is critical for optimizing travel scheduling and enhancing\npublic safety, yet the complex spatial and temporal dynamics within traffic\ndata present significant challenges for accurate forecasting. In this paper, we\nintroduce a novel model, the Spatiotemporal-aware Trend-Seasonality\nDecomposition Network (STDN). This model begins by constructing a dynamic graph\nstructure to represent traffic flow and incorporates novel spatio-temporal\nembeddings to jointly capture global traffic dynamics. The representations\nlearned are further refined by a specially designed trend-seasonality\ndecomposition module, which disentangles the trend-cyclical component and\nseasonal component for each traffic node at different times within the graph.\nThese components are subsequently processed through an encoder-decoder network\nto generate the final predictions. Extensive experiments conducted on\nreal-world traffic datasets demonstrate that STDN achieves superior performance\nwith remarkable computation cost. Furthermore, we have released a new traffic\ndataset named JiNan, which features unique inner-city dynamics, thereby\nenriching the scenario comprehensiveness in traffic prediction evaluation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12213v1",
    "published_date": "2025-02-17 03:29:02 UTC",
    "updated_date": "2025-02-17 03:29:02 UTC"
  },
  {
    "arxiv_id": "2502.14891v2",
    "title": "CoDiff: Conditional Diffusion Model for Collaborative 3D Object Detection",
    "authors": [
      "Zhe Huang",
      "Shuo Wang",
      "Yongcai Wang",
      "Lei Wang"
    ],
    "abstract": "Collaborative 3D object detection holds significant importance in the field\nof autonomous driving, as it greatly enhances the perception capabilities of\neach individual agent by facilitating information exchange among multiple\nagents. However, in practice, due to pose estimation errors and time delays,\nthe fusion of information across agents often results in feature\nrepresentations with spatial and temporal noise, leading to detection errors.\nDiffusion models naturally have the ability to denoise noisy samples to the\nideal data, which motivates us to explore the use of diffusion models to\naddress the noise problem between multi-agent systems. In this work, we propose\nCoDiff, a novel robust collaborative perception framework that leverages the\npotential of diffusion models to generate more comprehensive and clearer\nfeature representations. To the best of our knowledge, this is the first work\nto apply diffusion models to multi-agent collaborative perception.\nSpecifically, we project high-dimensional feature map into the latent space of\na powerful pre-trained autoencoder. Within this space, individual agent\ninformation serves as a condition to guide the diffusion model's sampling. This\nprocess denoises coarse feature maps and progressively refines the fused\nfeatures. Experimental study on both simulated and real-world datasets\ndemonstrates that the proposed framework CoDiff consistently outperforms\nexisting relevant methods in terms of the collaborative object detection\nperformance, and exhibits highly desired robustness when the pose and delay\ninformation of agents is with high-level noise. The code is released at\nhttps://github.com/HuangZhe885/CoDiff",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14891v2",
    "published_date": "2025-02-17 03:20:52 UTC",
    "updated_date": "2025-04-17 01:47:10 UTC"
  },
  {
    "arxiv_id": "2502.11381v2",
    "title": "Without Paired Labeled Data: An End-to-End Self-Supervised Paradigm for UAV-View Geo-Localization",
    "authors": [
      "Zhongwei Chen",
      "Zhao-Xu Yang",
      "Hai-Jun Rong"
    ],
    "abstract": "UAV-View Geo-Localization (UVGL) aims to achieve accurate localization of\nunmanned aerial vehicles (UAVs) by retrieving the most relevant GPS-tagged\nsatellite images. However, existing methods heavily rely on pre-paired\nUAV-satellite images for supervised learning. Such dependency not only incurs\nhigh annotation costs but also severely limits scalability and practical\ndeployment in open-world UVGL scenarios. To address these limitations, we\npropose an end-to-end self-supervised UVGL method. Our method leverages a\nshallow backbone network to extract initial features, employs clustering to\ngenerate pseudo labels, and adopts a dual-path contrastive learning\narchitecture to learn discriminative intra-view representations. Furthermore,\nour method incorporates two core modules, the dynamic hierarchical memory\nlearning module and the information consistency evolution learning module. The\ndynamic hierarchical memory learning module combines short-term and long-term\nmemory to enhance intra-view feature consistency and discriminability.\nMeanwhile, the information consistency evolution learning module leverages a\nneighborhood-driven dynamic constraint mechanism to systematically capture\nimplicit cross-view semantic correlations, thereby improving cross-view feature\nalignment. To further stabilize and strengthen the self-supervised training\nprocess, a pseudo-label enhancement strategy is introduced, which refines the\nquality of pseudo supervision. Our method ultimately constructs a unified\ncross-view feature representation space under self-supervised settings.\nExtensive experiments on three public benchmark datasets demonstrate that the\nproposed method consistently outperforms existing self-supervised methods and\neven surpasses several state-of-the-art supervised methods. Our code is\navailable at https://github.com/ISChenawei/DMNIL.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11381v2",
    "published_date": "2025-02-17 02:53:08 UTC",
    "updated_date": "2025-04-01 03:44:00 UTC"
  },
  {
    "arxiv_id": "2502.11379v1",
    "title": "CCJA: Context-Coherent Jailbreak Attack for Aligned Large Language Models",
    "authors": [
      "Guanghao Zhou",
      "Panjia Qiu",
      "Mingyuan Fan",
      "Cen Chen",
      "Mingyuan Chu",
      "Xin Zhang",
      "Jun Zhou"
    ],
    "abstract": "Despite explicit alignment efforts for large language models (LLMs), they can\nstill be exploited to trigger unintended behaviors, a phenomenon known as\n\"jailbreaking.\" Current jailbreak attack methods mainly focus on discrete\nprompt manipulations targeting closed-source LLMs, relying on manually crafted\nprompt templates and persuasion rules. However, as the capabilities of\nopen-source LLMs improve, ensuring their safety becomes increasingly crucial.\nIn such an environment, the accessibility of model parameters and gradient\ninformation by potential attackers exacerbates the severity of jailbreak\nthreats. To address this research gap, we propose a novel\n\\underline{C}ontext-\\underline{C}oherent \\underline{J}ailbreak\n\\underline{A}ttack (CCJA). We define jailbreak attacks as an optimization\nproblem within the embedding space of masked language models. Through\ncombinatorial optimization, we effectively balance the jailbreak attack success\nrate with semantic coherence. Extensive evaluations show that our method not\nonly maintains semantic consistency but also surpasses state-of-the-art\nbaselines in attack effectiveness. Additionally, by integrating semantically\ncoherent jailbreak prompts generated by our method into widely used black-box\nmethodologies, we observe a notable enhancement in their success rates when\ntargeting closed-source commercial LLMs. This highlights the security threat\nposed by open-source LLMs to commercial counterparts. We will open-source our\ncode if the paper is accepted.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11379v1",
    "published_date": "2025-02-17 02:49:26 UTC",
    "updated_date": "2025-02-17 02:49:26 UTC"
  },
  {
    "arxiv_id": "2502.12210v1",
    "title": "Enhancing Frame Detection with Retrieval Augmented Generation",
    "authors": [
      "Papa Abdou Karim Karou Diallo",
      "Amal Zouaq"
    ],
    "abstract": "Recent advancements in Natural Language Processing have significantly\nimproved the extraction of structured semantic representations from\nunstructured text, especially through Frame Semantic Role Labeling (FSRL).\nDespite this progress, the potential of Retrieval-Augmented Generation (RAG)\nmodels for frame detection remains under-explored. In this paper, we present\nthe first RAG-based approach for frame detection called RCIF (Retrieve\nCandidates and Identify Frames). RCIF is also the first approach to operate\nwithout the need for explicit target span and comprises three main stages: (1)\ngeneration of frame embeddings from various representations ; (2) retrieval of\ncandidate frames given an input text; and (3) identification of the most\nsuitable frames. We conducted extensive experiments across multiple\nconfigurations, including zero-shot, few-shot, and fine-tuning settings. Our\nresults show that our retrieval component significantly reduces the complexity\nof the task by narrowing the search space thus allowing the frame identifier to\nrefine and complete the set of candidates. Our approach achieves\nstate-of-the-art performance on FrameNet 1.5 and 1.7, demonstrating its\nrobustness in scenarios where only raw text is provided. Furthermore, we\nleverage the structured representation obtained through this method as a proxy\nto enhance generalization across lexical variations in the task of translating\nnatural language questions into SPARQL queries.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12210v1",
    "published_date": "2025-02-17 02:34:02 UTC",
    "updated_date": "2025-02-17 02:34:02 UTC"
  },
  {
    "arxiv_id": "2502.11368v1",
    "title": "LLMs can Perform Multi-Dimensional Analytic Writing Assessments: A Case Study of L2 Graduate-Level Academic English Writing",
    "authors": [
      "Zhengxiang Wang",
      "Veronika Makarova",
      "Zhi Li",
      "Jordan Kodner",
      "Owen Rambow"
    ],
    "abstract": "The paper explores the performance of LLMs in the context of\nmulti-dimensional analytic writing assessments, i.e. their ability to provide\nboth scores and comments based on multiple assessment criteria. Using a corpus\nof literature reviews written by L2 graduate students and assessed by human\nexperts against 9 analytic criteria, we prompt several popular LLMs to perform\nthe same task under various conditions. To evaluate the quality of feedback\ncomments, we apply a novel feedback comment quality evaluation framework. This\nframework is interpretable, cost-efficient, scalable, and reproducible,\ncompared to existing methods that rely on manual judgments. We find that LLMs\ncan generate reasonably good and generally reliable multi-dimensional analytic\nassessments. We release our corpus for reproducibility.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "26 pages, 6 figures, 15 tables",
    "pdf_url": "http://arxiv.org/pdf/2502.11368v1",
    "published_date": "2025-02-17 02:31:56 UTC",
    "updated_date": "2025-02-17 02:31:56 UTC"
  },
  {
    "arxiv_id": "2502.11367v1",
    "title": "Sparse Autoencoder Features for Classifications and Transferability",
    "authors": [
      "Jack Gallifant",
      "Shan Chen",
      "Kuleen Sasse",
      "Hugo Aerts",
      "Thomas Hartvigsen",
      "Danielle S. Bitterman"
    ],
    "abstract": "Sparse Autoencoders (SAEs) provide potentials for uncovering structured,\nhuman-interpretable representations in Large Language Models (LLMs), making\nthem a crucial tool for transparent and controllable AI systems. We\nsystematically analyze SAE for interpretable feature extraction from LLMs in\nsafety-critical classification tasks. Our framework evaluates (1) model-layer\nselection and scaling properties, (2) SAE architectural configurations,\nincluding width and pooling strategies, and (3) the effect of binarizing\ncontinuous SAE activations. SAE-derived features achieve macro F1 > 0.8,\noutperforming hidden-state and BoW baselines while demonstrating cross-model\ntransfer from Gemma 2 2B to 9B-IT models. These features generalize in a\nzero-shot manner to cross-lingual toxicity detection and visual classification\ntasks. Our analysis highlights the significant impact of pooling strategies and\nbinarization thresholds, showing that binarization offers an efficient\nalternative to traditional feature selection while maintaining or improving\nperformance. These findings establish new best practices for SAE-based\ninterpretability and enable scalable, transparent deployment of LLMs in\nreal-world applications. Full repo: https://github.com/shan23chen/MOSAIC.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11367v1",
    "published_date": "2025-02-17 02:30:45 UTC",
    "updated_date": "2025-02-17 02:30:45 UTC"
  },
  {
    "arxiv_id": "2502.11358v1",
    "title": "Mimicking the Familiar: Dynamic Command Generation for Information Theft Attacks in LLM Tool-Learning System",
    "authors": [
      "Ziyou Jiang",
      "Mingyang Li",
      "Guowei Yang",
      "Junjie Wang",
      "Yuekai Huang",
      "Zhiyuan Chang",
      "Qing Wang"
    ],
    "abstract": "Information theft attacks pose a significant risk to Large Language Model\n(LLM) tool-learning systems. Adversaries can inject malicious commands through\ncompromised tools, manipulating LLMs to send sensitive information to these\ntools, which leads to potential privacy breaches. However, existing attack\napproaches are black-box oriented and rely on static commands that cannot adapt\nflexibly to the changes in user queries and the invocation chain of tools. It\nmakes malicious commands more likely to be detected by LLM and leads to attack\nfailure. In this paper, we propose AutoCMD, a dynamic attack comment generation\napproach for information theft attacks in LLM tool-learning systems. Inspired\nby the concept of mimicking the familiar, AutoCMD is capable of inferring the\ninformation utilized by upstream tools in the toolchain through learning on\nopen-source systems and reinforcement with target system examples, thereby\ngenerating more targeted commands for information theft. The evaluation results\nshow that AutoCMD outperforms the baselines with +13.2% $ASR_{Theft}$, and can\nbe generalized to new tool-learning systems to expose their information leakage\nrisks. We also design four defense methods to effectively protect tool-learning\nsystems from the attack.",
    "categories": [
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.AI",
    "comment": "15 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.11358v1",
    "published_date": "2025-02-17 02:15:46 UTC",
    "updated_date": "2025-02-17 02:15:46 UTC"
  },
  {
    "arxiv_id": "2502.11357v2",
    "title": "Explorer: Scaling Exploration-driven Web Trajectory Synthesis for Multimodal Web Agents",
    "authors": [
      "Vardaan Pahuja",
      "Yadong Lu",
      "Corby Rosset",
      "Boyu Gou",
      "Arindam Mitra",
      "Spencer Whitehead",
      "Yu Su",
      "Ahmed Awadallah"
    ],
    "abstract": "Recent success in large multimodal models (LMMs) has sparked promising\napplications of agents capable of autonomously completing complex web tasks.\nWhile open-source LMM agents have made significant advances in offline\nevaluation benchmarks, their performance still falls substantially short of\nhuman-level capabilities in more realistic online settings. A key bottleneck is\nthe lack of diverse and large-scale trajectory-level datasets across various\ndomains, which are expensive to collect. In this paper, we address this\nchallenge by developing a scalable recipe to synthesize the largest and most\ndiverse trajectory-level dataset to date, containing over 94K successful\nmultimodal web trajectories, spanning 49K unique URLs, 720K screenshots, and\n33M web elements. In particular, we leverage extensive web exploration and\nrefinement to obtain diverse task intents. The average cost is 28 cents per\nsuccessful trajectory, making it affordable to a wide range of users in the\ncommunity. Leveraging this dataset, we train Explorer, a multimodal web agent,\nand demonstrate strong performance on both offline and online web agent\nbenchmarks such as Mind2Web-Live, Multimodal-Mind2Web, and MiniWob++.\nAdditionally, our experiments highlight data scaling as a key driver for\nimproving web agent capabilities. We hope this study makes state-of-the-art\nLMM-based agent research at a larger scale more accessible.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "24 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.11357v2",
    "published_date": "2025-02-17 02:13:48 UTC",
    "updated_date": "2025-02-19 01:38:06 UTC"
  },
  {
    "arxiv_id": "2502.11355v3",
    "title": "Nuclear Deployed: Analyzing Catastrophic Risks in Decision-making of Autonomous LLM Agents",
    "authors": [
      "Rongwu Xu",
      "Xiaojian Li",
      "Shuo Chen",
      "Wei Xu"
    ],
    "abstract": "Large language models (LLMs) are evolving into autonomous decision-makers,\nraising concerns about catastrophic risks in high-stakes scenarios,\nparticularly in Chemical, Biological, Radiological and Nuclear (CBRN) domains.\nBased on the insight that such risks can originate from trade-offs between the\nagent's Helpful, Harmlessness and Honest (HHH) goals, we build a novel\nthree-stage evaluation framework, which is carefully constructed to effectively\nand naturally expose such risks. We conduct 14,400 agentic simulations across\n12 advanced LLMs, with extensive experiments and analysis. Results reveal that\nLLM agents can autonomously engage in catastrophic behaviors and deception,\nwithout being deliberately induced. Furthermore, stronger reasoning abilities\noften increase, rather than mitigate, these risks. We also show that these\nagents can violate instructions and superior commands. On the whole, we\nempirically prove the existence of catastrophic risks in autonomous LLM agents.\nWe release our code to foster further research.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "Please visit https://llm-catastrophic-risks.github.io for a quick\n  tour of our research. Our code is available at\n  https://github.com/pillowsofwind/LLM-CBRN-Risks",
    "pdf_url": "http://arxiv.org/pdf/2502.11355v3",
    "published_date": "2025-02-17 02:11:17 UTC",
    "updated_date": "2025-03-23 06:22:28 UTC"
  },
  {
    "arxiv_id": "2502.11356v1",
    "title": "SAIF: A Sparse Autoencoder Framework for Interpreting and Steering Instruction Following of Language Models",
    "authors": [
      "Zirui He",
      "Haiyan Zhao",
      "Yiran Qiao",
      "Fan Yang",
      "Ali Payani",
      "Jing Ma",
      "Mengnan Du"
    ],
    "abstract": "The ability of large language models (LLMs) to follow instructions is crucial\nfor their practical applications, yet the underlying mechanisms remain poorly\nunderstood. This paper presents a novel framework that leverages sparse\nautoencoders (SAE) to interpret how instruction following works in these\nmodels. We demonstrate how the features we identify can effectively steer model\noutputs to align with given instructions. Through analysis of SAE latent\nactivations, we identify specific latents responsible for instruction following\nbehavior. Our findings reveal that instruction following capabilities are\nencoded by a distinct set of instruction-relevant SAE latents. These latents\nboth show semantic proximity to relevant instructions and demonstrate causal\neffects on model behavior. Our research highlights several crucial factors for\nachieving effective steering performance: precise feature identification, the\nrole of final layer, and optimal instruction positioning. Additionally, we\ndemonstrate that our methodology scales effectively across SAEs and LLMs of\nvarying sizes.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "21 pages, 11 figures, 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2502.11356v1",
    "published_date": "2025-02-17 02:11:17 UTC",
    "updated_date": "2025-02-17 02:11:17 UTC"
  },
  {
    "arxiv_id": "2502.12209v1",
    "title": "Suboptimal Shapley Value Explanations",
    "authors": [
      "Xiaolei Lu"
    ],
    "abstract": "Deep Neural Networks (DNNs) have demonstrated strong capacity in supporting a\nwide variety of applications. Shapley value has emerged as a prominent tool to\nanalyze feature importance to help people understand the inference process of\ndeep neural models. Computing Shapley value function requires choosing a\nbaseline to represent feature's missingness. However, existing random and\nconditional baselines could negatively influence the explanation. In this\npaper, by analyzing the suboptimality of different baselines, we identify the\nproblematic baseline where the asymmetric interaction between $\\bm{x}'_i$ (the\nreplacement of the faithful influential feature) and other features has\nsignificant directional bias toward the model's output, and conclude that\n$p(y|\\bm{x}'_i) = p(y)$ potentially minimizes the asymmetric interaction\ninvolving $\\bm{x}'_i$. We further generalize the uninformativeness of\n$\\bm{x}'_i$ toward the label space $L$ to avoid estimating $p(y)$ and design a\nsimple uncertainty-based reweighting mechanism to accelerate the computation\nprocess. We conduct experiments on various NLP tasks and our quantitative\nanalysis demonstrates the effectiveness of the proposed uncertainty-based\nreweighting mechanism. Furthermore, by measuring the consistency of\nexplanations generated by explainable methods and human, we highlight the\ndisparity between model inference and human understanding.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12209v1",
    "published_date": "2025-02-17 01:17:12 UTC",
    "updated_date": "2025-02-17 01:17:12 UTC"
  },
  {
    "arxiv_id": "2502.11333v1",
    "title": "Inverse Flow and Consistency Models",
    "authors": [
      "Yuchen Zhang",
      "Jian Zhou"
    ],
    "abstract": "Inverse generation problems, such as denoising without ground truth\nobservations, is a critical challenge in many scientific inquiries and\nreal-world applications. While recent advances in generative models like\ndiffusion models, conditional flow matching, and consistency models achieved\nimpressive results by casting generation as denoising problems, they cannot be\ndirectly used for inverse generation without access to clean data. Here we\nintroduce Inverse Flow (IF), a novel framework that enables using these\ngenerative models for inverse generation problems including denoising without\nground truth. Inverse Flow can be flexibly applied to nearly any continuous\nnoise distribution and allows complex dependencies. We propose two algorithms\nfor learning Inverse Flows, Inverse Flow Matching (IFM) and Inverse Consistency\nModel (ICM). Notably, to derive the computationally efficient, simulation-free\ninverse consistency model objective, we generalized consistency training to any\nforward diffusion processes or conditional flows, which have applications\nbeyond denoising. We demonstrate the effectiveness of IF on synthetic and real\ndatasets, outperforming prior approaches while enabling noise distributions\nthat previous methods cannot support. Finally, we showcase applications of our\ntechniques to fluorescence microscopy and single-cell genomics data,\nhighlighting IF's utility in scientific problems. Overall, this work expands\nthe applications of powerful generative models to inversion generation\nproblems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11333v1",
    "published_date": "2025-02-17 01:11:42 UTC",
    "updated_date": "2025-02-17 01:11:42 UTC"
  },
  {
    "arxiv_id": "2502.11330v1",
    "title": "System Message Generation for User Preferences using Open-Source Models",
    "authors": [
      "Minbyul Jeong",
      "Jungho Cho",
      "Minsoo Khang",
      "Dawoon Jung",
      "Teakgyu Hong"
    ],
    "abstract": "System messages play a crucial role in interactions with large language\nmodels (LLMs), often serving as prompts to initiate conversations. Through\nsystem messages, users can assign specific roles, perform intended tasks,\nincorporate background information, specify various output formats and\ncommunication styles. Despite such versatility, publicly available data are\noften lack system messages and subject to strict license constraints in the\nindustry field. Manual labeling of publicly available data with system messages\nthat align with user instructions demands significant resources. In view of\nsuch challenges, our work introduces SysGen, a pipeline for generating system\nmessages with better aligned assistant responses from the supervised\nfine-tuning dataset without system messages. Training on SysGen data has\ndemonstrated substantial improvements in the alignment of model responses with\nsystem messages and user instructions, as demonstrated across various\nopen-source models on the Multifacet benchmark, while maintaining minimal\nimpact on other unseen benchmarks such as Open LLM Leaderboard 2. Our\nqualitative analysis highlights the importance of diverse system messages to\nensure better adaptability across different contexts.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11330v1",
    "published_date": "2025-02-17 01:05:31 UTC",
    "updated_date": "2025-02-17 01:05:31 UTC"
  },
  {
    "arxiv_id": "2502.15776v1",
    "title": "Logic.py: Bridging the Gap between LLMs and Constraint Solvers",
    "authors": [
      "Pascal Kesseli",
      "Peter O'Hearn",
      "Ricardo Silveira Cabral"
    ],
    "abstract": "We present a novel approach to formalise and solve search-based problems\nusing large language models, which significantly improves upon previous\nstate-of-the-art results. We demonstrate the efficacy of this approach on the\nlogic puzzles benchmark ZebraLogicBench. Instead of letting the LLM attempt to\ndirectly solve the puzzles, our method prompts the model to formalise the\nproblem in a logic-focused domain-specific language (DSL) called Logic.py. This\nformalised representation is then solved using a constraint solver, leveraging\nthe strengths of both the language model and the solver. Our approach achieves\na remarkable 65% absolute improvement over the baseline performance of Llama\n3.1 70B on ZebraLogicBench, setting a new state-of-the-art with an accuracy of\nover 90%. This significant advancement demonstrates the potential of combining\nlanguage models with domain-specific languages and auxiliary tools on\ntraditionally challenging tasks for LLMs.",
    "categories": [
      "cs.AI",
      "cs.LO",
      "68T27",
      "F.4.1; I.2.3; I.2.8"
    ],
    "primary_category": "cs.AI",
    "comment": "11 pages,9 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.15776v1",
    "published_date": "2025-02-17 00:36:54 UTC",
    "updated_date": "2025-02-17 00:36:54 UTC"
  }
]
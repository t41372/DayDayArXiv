{
  "date": "2024-07-14",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-07-14 的 arXiv 中文 TLDR 快报！今天 arXiv 更新了 31 篇论文，主要聚焦 AI 模型的优化、机器学习基准、强化学习应用以及 AI 在生物和机器人领域的潜力，其中 LAB-Bench 和 Affordance-Guided Reinforcement Learning 等论文令人印象深刻，涉及知名学者如 Chelsea Finn 的工作，并探讨了 LLMs 的推理能力、幻觉问题和伦理风险。\n\n下面，我将挑选并简要讨论最具影响力和话题度的论文，先从 AI 和 LLMs 相关的高影响力文章开始，然后快速触及机器人、生物和机器学习领域。其他较次要的论文（如某些纯理论或小众应用）将简略掠过，只列出标题和关键点，以控制篇幅。\n\n### 1. AI 和 LLMs 相关论文（重点讨论）\n这些论文关注大型语言模型的改进、推理和伦理问题，是今日热点。\n- **LAB-Bench: Measuring Capabilities of Language Models for Biology Research（评估语言模型在生物研究能力的基准）**  \n  这篇论文引入 LAB-Bench 数据集（超过 2400 个多选题），用于评估 LLMs 在生物研究任务（如文献检索、数据分析和序列理解）的实际表现。核心贡献是提供一个实用基准，帮助 LLMs 作为研究助手，作者包括 Samuel G. Rodriques 等。实验显示 LLMs 在复杂任务上仍落后于人类专家，但这为自动化研究系统的发展提供了新工具。\n  \n- **Key-Point-Driven Mathematical Reasoning Distillation of Large Language Model（关键点驱动的数学推理蒸馏）**  \n  论文提出 KPDD 框架（包括 KPDD-CoT 和 KPDD-PoT），通过分解问题（提取核心问题和逐步解决）来蒸馏 LLMs 的数学推理能力。核心发现是这种方法显著减少了小语言模型的计算错误和语义误解，实现 SOTA 性能，提升了 LLMs 在推理任务的效率。\n\n- **Look Within, Why LLMs Hallucinate: A Causal Perspective（从因果视角探究 LLMs 的幻觉问题）**  \n  作者从自注意力机制入手，提出干预自注意力层的方法来缓解 LLMs 的幻觉。核心贡献是通过因果分析发现特定层禁用能降低错误率，这为理解和优化 LLMs 提供了新视角，强调了注意力在模型可靠性的作用。\n\n- **ChatLogic: Integrating Logic Programming with Large Language Models for Multi-Step Reasoning（整合逻辑编程与 LLMs 的多步推理）**  \n  论文引入 ChatLogic 框架，将逻辑编程与 LLMs 结合，提升多步推理能力。核心发现是框架能处理长程记忆问题，实现更准确的推理，适用于复杂任务，如 WCCI 2024 接受的论文。\n\n其他 LLMs 相关论文如 **Learning to Refuse: Towards Mitigating Privacy Risks in LLMs（学习拒绝：缓解 LLMs 的隐私风险）** 和 **Lean-STaR: Learning to Interleave Thinking and Proving（学习交替思考和证明）**，分别通过无监督方法保护隐私和提升证明任务表现，但细节较技术性，这里不展开。\n\n### 2. 机器人和强化学习论文（相关主题，简要讨论）\n这些与 AI 应用紧密相关，Affordance-Guided RL 尤为突出。\n- **Affordance-Guided Reinforcement Learning via Visual Prompting（基于视觉提示的强化学习引导）**  \n  作者包括知名学者 Chelsea Finn，这篇论文提出 KAGI 方法，使用视觉语言模型（VLMs）生成密集奖励，提升机器人自主学习效率。核心贡献是提高样本效率，实现真实世界操作任务的快速优化，实验显示在 30K 步内完成任务，显著提升机器人学习鲁棒性。\n\n- **Towards Adapting Reinforcement Learning Agents to New Tasks: Insights from Q-Values（基于 Q 值洞见适应新任务的强化学习代理）**  \n  论文探索 DQN 在任务适配中的混沌性，核心发现是准确 Q 值估计能加速模型适应。贡献包括指导算法设计，提升样本效率，适用于如自动驾驶场景。\n\n其他如 **Learning to Steer Markovian Agents under Model Uncertainty（在模型不确定性下学习引导 Markov 代理）**，提出 RL 框架处理不确定性，但影响较小，仅提及其在多代理系统中的潜力。\n\n### 3. 生物和机器学习基准论文（快速掠过）\n今日有几篇生物和基准相关论文，LAB-Bench 已在前文讨论，这里简要提其他。\n- **LAB-Bench: Measuring Capabilities of Language Models for Biology Research（见上）**  \n  快速补充：这篇与生物研究直接相关，提供实用评估工具。\n\n- **Evolved Developmental Artificial Neural Networks for Multitasking with Advanced Activity Dependence（进化的开发型神经网络用于多任务）**  \n  论文扩展神经网络的活动依赖机制，提升多任务性能。核心发现是新参数（如健康和位置）改善了避免灾难性遗忘的能力。\n\n其他领域如 **Mapping the Scholarship of Dark Pattern Regulation（暗模式监管的学术映射）** 和 **xLSTMTime: Long-term Time Series Forecasting With xLSTM（xLSTM 用于长期时间序列预测）**，分别讨论法律伦理和时间序列预测，但非核心热点，仅列出标题：它们提供了系统审查和 SOTA 模型，但篇幅有限，不深究。\n\n今日 arXiv 更新整体反映 AI 模型的快速演进，LLMs 在推理和伦理上的突破值得关注。如果你对特定领域感兴趣，如生物或机器人，建议优先查看 LAB-Bench 和 Affordance-Guided RL。更多论文细节可查阅 arXiv 页面，感谢阅读！（本快报基于今日 31 篇论文精选，保持简洁。）",
  "papers": [
    {
      "arxiv_id": "2407.10362v3",
      "title": "LAB-Bench: Measuring Capabilities of Language Models for Biology Research",
      "title_zh": "翻译失败",
      "authors": [
        "Jon M. Laurent",
        "Joseph D. Janizek",
        "Michael Ruzo",
        "Michaela M. Hinks",
        "Michael J. Hammerling",
        "Siddharth Narayanan",
        "Manvitha Ponnapati",
        "Andrew D. White",
        "Samuel G. Rodriques"
      ],
      "abstract": "There is widespread optimism that frontier Large Language Models (LLMs) and\nLLM-augmented systems have the potential to rapidly accelerate scientific\ndiscovery across disciplines. Today, many benchmarks exist to measure LLM\nknowledge and reasoning on textbook-style science questions, but few if any\nbenchmarks are designed to evaluate language model performance on practical\ntasks required for scientific research, such as literature search, protocol\nplanning, and data analysis. As a step toward building such benchmarks, we\nintroduce the Language Agent Biology Benchmark (LAB-Bench), a broad dataset of\nover 2,400 multiple choice questions for evaluating AI systems on a range of\npractical biology research capabilities, including recall and reasoning over\nliterature, interpretation of figures, access and navigation of databases, and\ncomprehension and manipulation of DNA and protein sequences. Importantly, in\ncontrast to previous scientific benchmarks, we expect that an AI system that\ncan achieve consistently high scores on the more difficult LAB-Bench tasks\nwould serve as a useful assistant for researchers in areas such as literature\nsearch and molecular cloning. As an initial assessment of the emergent\nscientific task capabilities of frontier language models, we measure\nperformance of several against our benchmark and report results compared to\nhuman expert biology researchers. We will continue to update and expand\nLAB-Bench over time, and expect it to serve as a useful tool in the development\nof automated research systems going forward. A public subset of LAB-Bench is\navailable for use at the following URL:\nhttps://huggingface.co/datasets/futurehouse/lab-bench",
      "tldr_zh": "本论文引入 LAB-Bench，一个用于评估 Large Language Models (LLMs) 在生物学研究实际任务中的能力的基准数据集，包含超过2,400个多项选择题，涵盖文献回顾、图表解释、数据库导航以及DNA和蛋白序列理解等能力。不同于传统的科学基准，LAB-Bench 专注于实用研究任务，如文献搜索和分子克隆，帮助AI系统提升为研究助理的潜力。作者评估了几种前沿LLMs的表现，并与人类专家比较，结果显示这些模型在更复杂任务上仍有显著差距。未来，LAB-Bench 将持续更新和扩展，并提供公共子集以推动自动化研究系统的发展。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "40 pages, 5 main figures, 1 main table, 2 supplemental figures, 4\n  supplemental tables. Submitted to NeurIPS 2024 Datasets and Benchmarks track\n  (in review)",
      "pdf_url": "http://arxiv.org/pdf/2407.10362v3",
      "published_date": "2024-07-14 23:52:25 UTC",
      "updated_date": "2024-07-17 17:28:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:25:09.473276"
    },
    {
      "arxiv_id": "2407.10359v1",
      "title": "Evolved Developmental Artificial Neural Networks for Multitasking with Advanced Activity Dependence",
      "title_zh": "演化发育",
      "authors": [
        "Yintong Zhang",
        "Jason A. Yoder"
      ],
      "abstract": "Recently, Cartesian Genetic Programming has been used to evolve developmental\nprograms to guide the formation of artificial neural networks (ANNs). This\napproach has demonstrated success in enabling ANNs to perform multiple tasks\nwhile avoiding catastrophic forgetting. One unique aspect of this approach is\nthe use of separate developmental programs evolved to regulate the development\nof separate soma and dendrite units. An opportunity afforded by this approach\nis the ability to incorporate Activity Dependence (AD) into the model such that\nenvironmental feedback can help to regulate the behavior of each type of unit.\nPrevious work has shown a limited version of AD (influencing neural bias) to\nprovide marginal improvements over non-AD ANNs. In this work, we present\npromising results from new extensions to AD. Specifically, we demonstrate a\nmore significant improvement via AD on new neural parameters including health\nand position, as well as a combination of all of these along with bias. We\nreport on the implications of this work and suggest several promising\ndirections for future work.",
      "tldr_zh": "该论文使用 Cartesian Genetic Programming 进化发育程序来指导 Artificial Neural Networks (ANNs) 的形成，支持多任务处理并避免 catastrophic forgetting。创新之处在于引入高级 Activity Dependence (AD)，允许环境反馈调节神经参数，如健康、位置以及与偏差的组合，从而显著提升模型性能。实验结果显示，这种扩展比之前的有限 AD 方法提供了更明显的改进，并为未来 ANN 研究指出了有前景的方向。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "I.2.6; I.2.11"
      ],
      "primary_category": "cs.NE",
      "comment": "6 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.10359v1",
      "published_date": "2024-07-14 23:39:07 UTC",
      "updated_date": "2024-07-14 23:39:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:25:19.396046"
    },
    {
      "arxiv_id": "2407.10341v5",
      "title": "Affordance-Guided Reinforcement Learning via Visual Prompting",
      "title_zh": "翻译失败",
      "authors": [
        "Olivia Y. Lee",
        "Annie Xie",
        "Kuan Fang",
        "Karl Pertsch",
        "Chelsea Finn"
      ],
      "abstract": "Robots equipped with reinforcement learning (RL) have the potential to learn\na wide range of skills solely from a reward signal. However, obtaining a robust\nand dense reward signal for general manipulation tasks remains a challenge.\nExisting learning-based approaches require significant data, such as human\ndemonstrations of success and failure, to learn task-specific reward functions.\nRecently, there is also a growing adoption of large multi-modal foundation\nmodels for robotics that can perform visual reasoning in physical contexts and\ngenerate coarse robot motions for manipulation tasks. Motivated by this range\nof capability, in this work, we present Keypoint-based Affordance Guidance for\nImprovements (KAGI), a method leveraging rewards shaped by vision-language\nmodels (VLMs) for autonomous RL. State-of-the-art VLMs have demonstrated\nimpressive reasoning about affordances through keypoints in zero-shot, and we\nuse these to define dense rewards that guide autonomous robotic learning. On\nreal-world manipulation tasks specified by natural language descriptions, KAGI\nimproves the sample efficiency of autonomous RL and enables successful task\ncompletion in 30K online fine-tuning steps. Additionally, we demonstrate the\nrobustness of KAGI to reductions in the number of in-domain demonstrations used\nfor pre-training, reaching similar performance in 45K online fine-tuning steps.\nProject website: https://sites.google.com/view/affordance-guided-rl",
      "tldr_zh": "这篇论文提出KAGI方法，通过视觉提示（Visual Prompting）引导强化学习（RL），以解决机器人操作任务中奖励信号稀疏和数据依赖的问题。KAGI利用视觉语言模型（VLMs）基于关键点（keypoints）生成密集奖励，实现对任务的自主学习和视觉推理。在真实世界操作任务中，该方法显著提高了RL的样本效率，在30K在线细调步骤内完成任务，并展示了在减少in-domain演示时的鲁棒性。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "8 pages, 6 figures. Robotics: Science and Systems (RSS) 2024, Task\n  Specification for General-Purpose Intelligent Robots & Lifelong Robot\n  Learning Workshops",
      "pdf_url": "http://arxiv.org/pdf/2407.10341v5",
      "published_date": "2024-07-14 21:41:29 UTC",
      "updated_date": "2025-03-05 06:53:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:25:30.702998"
    },
    {
      "arxiv_id": "2407.10340v1",
      "title": "Mapping the Scholarship of Dark Pattern Regulation: A Systematic Review of Concepts, Regulatory Paradigms, and Solutions from an Interdisciplinary Perspective",
      "title_zh": "翻译失败",
      "authors": [
        "Weiwei Yi",
        "Zihao Li"
      ],
      "abstract": "Dark patterns, design tricks used on online interfaces to manipulate users\ndecision-making process, have raised public concerns. However, research on\nregulation of dark pattern remains underdeveloped and scattered, particularly\nregarding scholars views on the concept, regulatory paradigms, and solutions.\nFollowing PRISMA guidelines, this paper systematically reviews the formats and\ncontent of regulatory discussions on dark patterns from the interdisciplinary\nscholarship of Law and Human-Computer Interaction. A total of 65 studies were\nanalysed through content and thematic analysis. This study synthesises the\nunique trends and characteristics of legal scholarship on dark patterns,\nidentifying five root problems and triple layered harms. It critiques current\nregulations in terms of legal theories and sectoral legislations, highlighting\ntheir inadequacies in addressing dark patterns. The paper also critically\nexamines existing proposed solutions, including paradigmatic shifts in legal\ndoctrines, refinements to existing frameworks, technical design-embedded\nsolutions, and accountability measures for design practices. This research\ncritically discusses the current barriers to effective dark pattern regulations\nand explores promising regulatory solutions. The difficulty in identifying the\nnormative nature of various forms of dark patterns, in identifying evident and\nactionable harm, and the expanding scope of dark patterns connotation\ninherently hinders effective regulation. However, technical design-embedded\nsolutions, accountability frameworks, and practical design guidelines offer\npotential routes for more proactive regulation, while legal pluralism stands as\na promising macro-level change in regulatory paradigms for dark pattern\nregulation.",
      "tldr_zh": "这篇论文通过遵循 PRISMA 指南，对法律和人机交互领域的65个研究进行系统审查，探讨了 dark patterns（设计技巧用于操纵用户决策）的概念、监管范式（regulatory paradigms）和解决方案。研究总结了暗模式的五大根源问题和三层伤害，并批评了当前法规在法律理论和部门立法方面的不足。最终，它审视了现有解决方案，如法律范式的转变、技术嵌入式设计和责任框架，并强调法律多元主义（legal pluralism）等方法可能为更有效的 dark patterns 监管提供前景。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC",
        "cs.IT",
        "cs.SI",
        "math.IT"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.10340v1",
      "published_date": "2024-07-14 21:41:18 UTC",
      "updated_date": "2024-07-14 21:41:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:25:43.297642"
    },
    {
      "arxiv_id": "2407.10335v1",
      "title": "Towards Adapting Reinforcement Learning Agents to New Tasks: Insights from Q-Values",
      "title_zh": "翻译失败",
      "authors": [
        "Ashwin Ramaswamy",
        "Ransalu Senanayake"
      ],
      "abstract": "While contemporary reinforcement learning research and applications have\nembraced policy gradient methods as the panacea of solving learning problems,\nvalue-based methods can still be useful in many domains as long as we can\nwrangle with how to exploit them in a sample efficient way. In this paper, we\nexplore the chaotic nature of DQNs in reinforcement learning, while\nunderstanding how the information that they retain when trained can be\nrepurposed for adapting a model to different tasks. We start by designing a\nsimple experiment in which we are able to observe the Q-values for each state\nand action in an environment. Then we train in eight different ways to explore\nhow these training algorithms affect the way that accurate Q-values are learned\n(or not learned). We tested the adaptability of each trained model when\nretrained to accomplish a slightly modified task. We then scaled our setup to\ntest the larger problem of an autonomous vehicle at an unprotected\nintersection. We observed that the model is able to adapt to new tasks quicker\nwhen the base model's Q-value estimates are closer to the true Q-values. The\nresults provide some insights and guidelines into what algorithms are useful\nfor sample efficient task adaptation.",
      "tldr_zh": "这篇论文探讨了如何利用 Q-values 来帮助强化学习代理适应新任务，强调了价值-based 方法（如 DQN）在样本高效场景中的潜力。研究者设计了一个简单实验，观察状态和动作的 Q-values，并通过八种不同的训练算法测试这些值如何影响模型学习。实验扩展到自动驾驶车辆在无保护路口的实际场景，结果显示，当基模型的 Q-value 估计更接近真实值时，模型能更快适应修改后的任务。最终，论文提供了关于算法选择和样本高效任务适应的实用见解和指导。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.10335v1",
      "published_date": "2024-07-14 21:28:27 UTC",
      "updated_date": "2024-07-14 21:28:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:25:55.018933"
    },
    {
      "arxiv_id": "2407.10328v1",
      "title": "The Interpretation Gap in Text-to-Music Generation Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yongyi Zang",
        "Yixiao Zhang"
      ],
      "abstract": "Large-scale text-to-music generation models have significantly enhanced music\ncreation capabilities, offering unprecedented creative freedom. However, their\nability to collaborate effectively with human musicians remains limited. In\nthis paper, we propose a framework to describe the musical interaction process,\nwhich includes expression, interpretation, and execution of controls. Following\nthis framework, we argue that the primary gap between existing text-to-music\nmodels and musicians lies in the interpretation stage, where models lack the\nability to interpret controls from musicians. We also propose two strategies to\naddress this gap and call on the music information retrieval community to\ntackle the interpretation challenge to improve human-AI musical collaboration.",
      "tldr_zh": "大规模文本-to-music generation models 显著提升了音乐创作能力，提供前所未有的创意自由，但其与人类音乐家有效协作的能力有限。论文提出一个框架来描述音乐互动过程，包括表达、interpretation 和执行控制，并指出现有模型的主要问题在于interpretation 阶段，无法正确解释音乐家的控制。针对这一interpretation gap，论文建议两种策略，并呼吁音乐信息检索社区共同努力，以改善人类-AI 音乐协作。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Under review",
      "pdf_url": "http://arxiv.org/pdf/2407.10328v1",
      "published_date": "2024-07-14 20:51:08 UTC",
      "updated_date": "2024-07-14 20:51:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:26:05.831981"
    },
    {
      "arxiv_id": "2407.10327v2",
      "title": "Learning Unlabeled Clients Divergence for Federated Semi-Supervised Learning via Anchor Model Aggregation",
      "title_zh": "翻译失败",
      "authors": [
        "Marawan Elbatel",
        "Hualiang Wang",
        "Jixiang Chen",
        "Hao Wang",
        "Xiaomeng Li"
      ],
      "abstract": "Federated semi-supervised learning (FedSemi) refers to scenarios where there\nmay be clients with fully labeled data, clients with partially labeled, and\neven fully unlabeled clients while preserving data privacy. However, challenges\narise from client drift due to undefined heterogeneous class distributions and\nerroneous pseudo-labels. Existing FedSemi methods typically fail to aggregate\nmodels from unlabeled clients due to their inherent unreliability, thus\noverlooking unique information from their heterogeneous data distribution,\nleading to sub-optimal results. In this paper, we enable unlabeled client\naggregation through SemiAnAgg, a novel Semi-supervised Anchor-Based federated\nAggregation. SemiAnAgg learns unlabeled client contributions via an anchor\nmodel, effectively harnessing their informative value. Our key idea is that by\nfeeding local client data to the same global model and the same consistently\ninitialized anchor model (i.e., random model), we can measure the importance of\neach unlabeled client accordingly. Extensive experiments demonstrate that\nSemiAnAgg achieves new state-of-the-art results on four widely used FedSemi\nbenchmarks, leading to substantial performance improvements: a 9% increase in\naccuracy on CIFAR-100 and a 7.6% improvement in recall on the medical dataset\nISIC-18, compared with prior state-of-the-art. Code is available at:\nhttps://github.com/xmed-lab/SemiAnAgg.",
      "tldr_zh": "该论文针对联邦半监督学习（FedSemi）中的挑战，提出了一种新的方法SemiAnAgg，以解决客户端漂移（client drift）和错误伪标签问题，特别是如何有效聚合无标记客户端的模型。SemiAnAgg通过引入anchor model来学习无标记客户端的贡献，将本地数据输入到相同的全局模型和一致初始化的anchor model，从而衡量每个客户端的重要性。实验结果显示，该方法在四个FedSemi基准上实现了新的最先进性能，包括CIFAR-100上准确率提高9%，以及ISIC-18医疗数据集上召回率提升7.6%。这项工作提升了FedSemi的鲁棒性和效率，为隐私保护下的异构数据利用提供了新途径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by TMLR (10/2024)",
      "pdf_url": "http://arxiv.org/pdf/2407.10327v2",
      "published_date": "2024-07-14 20:50:40 UTC",
      "updated_date": "2024-10-25 14:39:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:26:19.843024"
    },
    {
      "arxiv_id": "2407.14540v1",
      "title": "Risks of uncertainty propagation in Al-augmented security pipelines",
      "title_zh": "翻译失败",
      "authors": [
        "Emanuele Mezzi",
        "Aurora Papotti",
        "Fabio Massacci",
        "Katja Tuma"
      ],
      "abstract": "The use of AI technologies is percolating into the secure development of\nsoftware-based systems, with an increasing trend of composing AI-based\nsubsystems (with uncertain levels of performance) into automated pipelines.\nThis presents a fundamental research challenge and poses a serious threat to\nsafety-critical domains (e.g., aviation). Despite the existing knowledge about\nuncertainty in risk analysis, no previous work has estimated the uncertainty of\nAI-augmented systems given the propagation of errors in the pipeline. We\nprovide the formal underpinnings for capturing uncertainty propagation, develop\na simulator to quantify uncertainty, and evaluate the simulation of propagating\nerrors with two case studies. We discuss the generalizability of our approach\nand present policy implications and recommendations for aviation. Future work\nincludes extending the approach and investigating the required metrics for\nvalidation in the aviation domain.",
      "tldr_zh": "这篇论文探讨了AI增强安全管道中不确定性传播的风险，特别是在安全关键领域如航空中，由于AI子系统性能不确定而导致的潜在威胁。作者建立了捕捉不确定性传播的正式基础，并开发了一个模拟器来量化管道中错误的传播，通过两个案例研究进行了评估。研究结果突显了该方法的可通用性，并为航空领域提供了政策建议和推荐。未来工作将扩展这一方法并调查航空领域的验证指标。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.14540v1",
      "published_date": "2024-07-14 19:02:20 UTC",
      "updated_date": "2024-07-14 19:02:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:26:30.414932"
    },
    {
      "arxiv_id": "2407.10279v2",
      "title": "AlphaDou: High-Performance End-to-End Doudizhu AI Integrating Bidding",
      "title_zh": "翻译失败",
      "authors": [
        "Chang Lei",
        "Huan Lei"
      ],
      "abstract": "Artificial intelligence for card games has long been a popular topic in AI\nresearch. In recent years, complex card games like Mahjong and Texas Hold'em\nhave been solved, with corresponding AI programs reaching the level of human\nexperts. However, the game of Doudizhu presents significant challenges due to\nits vast state/action space and unique characteristics involving reasoning\nabout competition and cooperation, making the game extremely difficult to\nsolve.The RL model Douzero, trained using the Deep Monte Carlo algorithm\nframework, has shown excellent performance in Doudizhu. However, there are\ndifferences between its simplified game environment and the actual Doudizhu\nenvironment, and its performance is still a considerable distance from that of\nhuman experts. This paper modifies the Deep Monte Carlo algorithm framework by\nusing reinforcement learning to obtain a neural network that simultaneously\nestimates win rates and expectations. The action space is pruned using\nexpectations, and strategies are generated based on win rates. The modified\nalgorithm enables the AI to perform the full range of tasks in the Doudizhu\ngame, including bidding and cardplay. The model was trained in a actual\nDoudizhu environment and achieved state-of-the-art performance among publicly\navailable models. We hope that this new framework will provide valuable\ninsights for AI development in other bidding-based games.",
      "tldr_zh": "本文提出AlphaDou，一种高性能端到端Doudizhu AI，改进了Deep Monte Carlo算法框架，使用Reinforcement Learning训练神经网络来同时估计胜率和期望值。AlphaDou通过期望值修剪动作空间并基于胜率生成策略，实现对Doudizhu游戏的全流程处理，包括叫牌和出牌。实验结果显示，该模型在实际游戏环境中训练后，达到了公开可用模型的最先进性能，并为其他基于叫牌的游戏AI开发提供了宝贵启发。",
      "categories": [
        "cs.AI",
        "cs.GT",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.10279v2",
      "published_date": "2024-07-14 17:32:36 UTC",
      "updated_date": "2024-09-13 15:17:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:26:43.234532"
    },
    {
      "arxiv_id": "2407.10277v1",
      "title": "Disrupting Diffusion-based Inpainters with Semantic Digression",
      "title_zh": "翻译失败",
      "authors": [
        "Geonho Son",
        "Juhun Lee",
        "Simon S. Woo"
      ],
      "abstract": "The fabrication of visual misinformation on the web and social media has\nincreased exponentially with the advent of foundational text-to-image diffusion\nmodels. Namely, Stable Diffusion inpainters allow the synthesis of maliciously\ninpainted images of personal and private figures, and copyrighted contents,\nalso known as deepfakes. To combat such generations, a disruption framework,\nnamely Photoguard, has been proposed, where it adds adversarial noise to the\ncontext image to disrupt their inpainting synthesis. While their framework\nsuggested a diffusion-friendly approach, the disruption is not sufficiently\nstrong and it requires a significant amount of GPU and time to immunize the\ncontext image. In our work, we re-examine both the minimal and favorable\nconditions for a successful inpainting disruption, proposing DDD, a \"Digression\nguided Diffusion Disruption\" framework. First, we identify the most\nadversarially vulnerable diffusion timestep range with respect to the hidden\nspace. Within this scope of noised manifold, we pose the problem as a semantic\ndigression optimization. We maximize the distance between the inpainting\ninstance's hidden states and a semantic-aware hidden state centroid, calibrated\nboth by Monte Carlo sampling of hidden states and a discretely projected\noptimization in the token space. Effectively, our approach achieves stronger\ndisruption and a higher success rate than Photoguard while lowering the GPU\nmemory requirement, and speeding the optimization up to three times faster.",
      "tldr_zh": "该研究针对文本到图像扩散模型（如Stable Diffusion）生成的恶意图像修复（deepfakes）问题，提出DDD框架（Digression guided Diffusion Disruption），以更高效地破坏inpainting合成。方法包括识别扩散时间步（diffusion timestep）中隐藏空间的脆弱范围，并通过语义偏移优化最大化修复实例的隐藏状态与语义感知隐藏状态中心点的距离，利用Monte Carlo采样和标记空间的离散投影。相比现有Photoguard框架，DDD实现了更强的破坏效果、更高成功率，同时降低了GPU内存需求并将优化速度提高三倍。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "16 pages, 13 figures, IJCAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.10277v1",
      "published_date": "2024-07-14 17:21:19 UTC",
      "updated_date": "2024-07-14 17:21:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:26:54.462503"
    },
    {
      "arxiv_id": "2407.10275v2",
      "title": "Cross-Lingual Multi-Hop Knowledge Editing",
      "title_zh": "跨语言多跳知识编辑",
      "authors": [
        "Aditi Khandelwal",
        "Harman Singh",
        "Hengrui Gu",
        "Tianlong Chen",
        "Kaixiong Zhou"
      ],
      "abstract": "Large language models are often expected to constantly adapt to new sources\nof knowledge and knowledge editing techniques aim to efficiently patch the\noutdated model knowledge, with minimal modification. Most prior works focus on\nmonolingual knowledge editing in English, even though new information can\nemerge in any language from any part of the world. We propose the Cross-Lingual\nMulti-Hop Knowledge Editing paradigm, for measuring and analyzing the\nperformance of various SoTA knowledge editing techniques in a cross-lingual\nsetup. Specifically, we create a parallel cross-lingual benchmark,\nCROLIN-MQUAKE for measuring the knowledge editing capabilities. Our extensive\nanalysis over various knowledge editing techniques uncover significant gaps in\nperformance between the cross-lingual and English-centric setting. Following\nthis, we propose a significantly improved system for cross-lingual multi-hop\nknowledge editing, CLEVER-CKE. CLEVER-CKE is based on a retrieve, verify and\ngenerate knowledge editing framework, where a retriever is formulated to recall\nedited facts and support an LLM to adhere to knowledge edits. We develop\nlanguage-aware and hard-negative based contrastive objectives for improving the\ncross-lingual and fine-grained fact retrieval and verification process used in\nthis framework. Extensive experiments on three LLMs, eight languages, and two\ndatasets show CLEVER-CKE's significant gains of up to 30% over prior methods.",
      "tldr_zh": "该论文提出 Cross-Lingual Multi-Hop Knowledge Editing 范式，用于评估知识编辑技术在跨语言环境下的性能，并创建了平行跨语言基准 CROLIN-MQUAKE，以分析现有方法的不足。研究发现，大语言模型（LLMs）的知识编辑在跨语言设置中存在显著性能差距。作者开发了改进系统 CLEVER-CKE，该系统基于检索、验证和生成框架，并通过语言感知和硬负例对比目标优化细粒度事实检索和验证过程。在三个 LLMs、八种语言和两个数据集上的实验显示，CLEVER-CKE 比先前方法提升高达 30%。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.10275v2",
      "published_date": "2024-07-14 17:18:16 UTC",
      "updated_date": "2025-02-15 19:17:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:27:08.137064"
    },
    {
      "arxiv_id": "2407.10240v3",
      "title": "xLSTMTime : Long-term Time Series Forecasting With xLSTM",
      "title_zh": "翻译失败",
      "authors": [
        "Musleh Alharthi",
        "Ausif Mahmood"
      ],
      "abstract": "In recent years, transformer-based models have gained prominence in\nmultivariate long-term time series forecasting (LTSF), demonstrating\nsignificant advancements despite facing challenges such as high computational\ndemands, difficulty in capturing temporal dynamics, and managing long-term\ndependencies. The emergence of LTSF-Linear, with its straightforward linear\narchitecture, has notably outperformed transformer-based counterparts,\nprompting a reevaluation of the transformer's utility in time series\nforecasting. In response, this paper presents an adaptation of a recent\narchitecture termed extended LSTM (xLSTM) for LTSF. xLSTM incorporates\nexponential gating and a revised memory structure with higher capacity that has\ngood potential for LTSF. Our adopted architecture for LTSF termed as xLSTMTime\nsurpasses current approaches. We compare xLSTMTime's performance against\nvarious state-of-the-art models across multiple real-world da-tasets,\ndemonstrating superior forecasting capabilities. Our findings suggest that\nrefined recurrent architectures can offer competitive alternatives to\ntransformer-based models in LTSF tasks, po-tentially redefining the landscape\nof time series forecasting.",
      "tldr_zh": "本论文针对多变量长期时间序列预测 (LTSF) 的挑战，指出 transformer-based 模型虽有进展，但面临高计算需求和捕捉时间动态的难题，而 LTSF-Linear 的线性架构已超越其性能。作者提出 xLSTMTime，这是一种基于 extended LSTM (xLSTM) 的适应版本，结合指数门控和改进记忆结构，以提升长期依赖建模能力。在多个真实数据集上，xLSTMTime 超过了现有最先进模型，证明精炼的循环架构可作为 transformer-based 方法的竞争替代品，可能重塑时间序列预测领域。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.10240v3",
      "published_date": "2024-07-14 15:15:00 UTC",
      "updated_date": "2024-08-12 02:10:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:27:18.879697"
    },
    {
      "arxiv_id": "2407.10233v1",
      "title": "Visual Prompt Selection for In-Context Learning Segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Wei Suo",
        "Lanqing Lai",
        "Mengyang Sun",
        "Hanwang Zhang",
        "Peng Wang",
        "Yanning Zhang"
      ],
      "abstract": "As a fundamental and extensively studied task in computer vision, image\nsegmentation aims to locate and identify different semantic concepts at the\npixel level. Recently, inspired by In-Context Learning (ICL), several\ngeneralist segmentation frameworks have been proposed, providing a promising\nparadigm for segmenting specific objects. However, existing works mostly ignore\nthe value of visual prompts or simply apply similarity sorting to select\ncontextual examples. In this paper, we focus on rethinking and improving the\nexample selection strategy. By comprehensive comparisons, we first demonstrate\nthat ICL-based segmentation models are sensitive to different contexts.\nFurthermore, empirical evidence indicates that the diversity of contextual\nprompts plays a crucial role in guiding segmentation. Based on the above\ninsights, we propose a new stepwise context search method. Different from\nprevious works, we construct a small yet rich candidate pool and adaptively\nsearch the well-matched contexts. More importantly, this method effectively\nreduces the annotation cost by compacting the search space. Extensive\nexperiments show that our method is an effective strategy for selecting\nexamples and enhancing segmentation performance.",
      "tldr_zh": "本文研究了In-Context Learning (ICL) 在图像分割任务中的视觉提示选择问题，发现现有方法忽略了视觉提示的价值，且ICL模型对不同上下文高度敏感，上下文提示的多样性是关键因素。作者提出了一种新的逐步上下文搜索方法，通过构建一个小型而丰富的候选池，并进行自适应搜索来选择匹配的上下文示例，有效降低了标注成本并压缩了搜索空间。实验结果显示，该方法显著提升了分割性能，为ICL-based分割框架提供了更有效的示例选择策略。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accept by ECCV2024",
      "pdf_url": "http://arxiv.org/pdf/2407.10233v1",
      "published_date": "2024-07-14 15:02:54 UTC",
      "updated_date": "2024-07-14 15:02:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:27:31.818278"
    },
    {
      "arxiv_id": "2407.10207v3",
      "title": "Learning to Steer Markovian Agents under Model Uncertainty",
      "title_zh": "翻译失败",
      "authors": [
        "Jiawei Huang",
        "Vinzenz Thoma",
        "Zebang Shen",
        "Heinrich H. Nax",
        "Niao He"
      ],
      "abstract": "Designing incentives for an adapting population is a ubiquitous problem in a\nwide array of economic applications and beyond. In this work, we study how to\ndesign additional rewards to steer multi-agent systems towards desired policies\n\\emph{without} prior knowledge of the agents' underlying learning dynamics.\nMotivated by the limitation of existing works, we consider a new and general\ncategory of learning dynamics called \\emph{Markovian agents}. We introduce a\nmodel-based non-episodic Reinforcement Learning (RL) formulation for our\nsteering problem. Importantly, we focus on learning a \\emph{history-dependent}\nsteering strategy to handle the inherent model uncertainty about the agents'\nlearning dynamics. We introduce a novel objective function to encode the\ndesiderata of achieving a good steering outcome with reasonable cost.\nTheoretically, we identify conditions for the existence of steering strategies\nto guide agents to the desired policies. Complementing our theoretical\ncontributions, we provide empirical algorithms to approximately solve our\nobjective, which effectively tackles the challenge in learning\nhistory-dependent strategies. We demonstrate the efficacy of our algorithms\nthrough empirical evaluations.",
      "tldr_zh": "这篇论文探讨了在模型不确定性下，如何设计额外奖励来引导Markovian agents朝向期望策略，而无需事先了解代理的学习动态。作者引入了一个模型-based非周期性Reinforcement Learning框架，专注于学习history-dependent的引导策略，并提出一个新目标函数来平衡引导效果和成本。理论上，他们证明了引导策略存在的条件，并通过经验算法进行近似求解，实验结果显示了算法的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "35 Pages; ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2407.10207v3",
      "published_date": "2024-07-14 14:01:38 UTC",
      "updated_date": "2025-02-08 17:23:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:27:43.419561"
    },
    {
      "arxiv_id": "2407.10206v1",
      "title": "Dominant Design Prediction with Phylogenetic Networks",
      "title_zh": "基于系统发育网络的主导设计预测",
      "authors": [
        "Youwei He",
        "Jeong-Dong Lee",
        "Dawoon Jeong",
        "Sungjun Choi",
        "Jiyong Kim"
      ],
      "abstract": "This study proposes an effective method to predict technology development\nfrom an evolutionary perspective. Product evolution is the result of\ntechnological evolution and market selection. A phylogenetic network is the\nmain method to study product evolution. The formation of the dominant design\ndetermines the trajectory of technology development. How to predict future\ndominant design has become a key issue in technology forecasting and new\nproduct development. We define the dominant product and use machine learning\nmethods, combined with product evolutionary theory, to construct a Fully\nConnected Phylogenetic Network dataset to effectively predict the future\ndominant design.",
      "tldr_zh": "本研究从进化视角提出一种预测技术发展的有效方法，强调产品演化是技术演化和市场选择的结果，并使用 phylogenetic networks 作为主要工具来研究这一过程。论文定义了 dominant design，并结合机器学习方法和产品演化理论，构建了一个 Fully Connected Phylogenetic Network 数据集，以准确预测未来的主导设计。该方法为技术预测和新产品开发提供了关键支持，有助于指导产品演化的轨迹。",
      "categories": [
        "cs.CE",
        "cs.AI",
        "cs.NE",
        "cs.SI"
      ],
      "primary_category": "cs.CE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.10206v1",
      "published_date": "2024-07-14 14:00:02 UTC",
      "updated_date": "2024-07-14 14:00:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:27:54.284689"
    },
    {
      "arxiv_id": "2407.10200v1",
      "title": "Shape2Scene: 3D Scene Representation Learning Through Pre-training on Shape Data",
      "title_zh": "翻译失败",
      "authors": [
        "Tuo Feng",
        "Wenguan Wang",
        "Ruijie Quan",
        "Yi Yang"
      ],
      "abstract": "Current 3D self-supervised learning methods of 3D scenes face a data desert\nissue, resulting from the time-consuming and expensive collecting process of 3D\nscene data. Conversely, 3D shape datasets are easier to collect. Despite this,\nexisting pre-training strategies on shape data offer limited potential for 3D\nscene understanding due to significant disparities in point quantities. To\ntackle these challenges, we propose Shape2Scene (S2S), a novel method that\nlearns representations of large-scale 3D scenes from 3D shape data. We first\ndesign multiscale and high-resolution backbones for shape and scene level 3D\ntasks, i.e., MH-P (point-based) and MH-V (voxel-based). MH-P/V establishes\ndirect paths to highresolution features that capture deep semantic information\nacross multiple scales. This pivotal nature makes them suitable for a wide\nrange of 3D downstream tasks that tightly rely on high-resolution features. We\nthen employ a Shape-to-Scene strategy (S2SS) to amalgamate points from various\nshapes, creating a random pseudo scene (comprising multiple objects) for\ntraining data, mitigating disparities between shapes and scenes. Finally, a\npoint-point contrastive loss (PPC) is applied for the pre-training of MH-P/V.\nIn PPC, the inherent correspondence (i.e., point pairs) is naturally obtained\nin S2SS. Extensive experiments have demonstrated the transferability of 3D\nrepresentations learned by MH-P/V across shape-level and scene-level 3D tasks.\nMH-P achieves notable performance on well-known point cloud datasets (93.8% OA\non ScanObjectNN and 87.6% instance mIoU on ShapeNetPart). MH-V also achieves\npromising performance in 3D semantic segmentation and 3D object detection.",
      "tldr_zh": "该论文提出 Shape2Scene (S2S) 方法，通过在更容易获取的 3D 形状数据上预训练来学习大规模 3D 场景表示，解决当前 3D 自监督学习中场景数据稀缺的问题。核心创新包括设计多尺度高分辨率 backbone MH-P (基于点云)和 MH-V (基于体素)，以及 Shape-to-Scene 策略 (S2SS) 来合并多个形状生成随机伪场景，并应用点-点对比损失 (PPC) 进行预训练，以减少形状和场景间的差异。实验结果显示，MH-P 在点云任务中表现出色（如 ScanObjectNN 上 93.8% OA 和 ShapeNetPart 上 87.6% 实例 mIoU），而 MH-V 在 3D 语义分割和对象检测任务上也取得了显著性能提升。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "ECCV 2024; Project page: https://github.com/FengZicai/S2S",
      "pdf_url": "http://arxiv.org/pdf/2407.10200v1",
      "published_date": "2024-07-14 13:42:05 UTC",
      "updated_date": "2024-07-14 13:42:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:28:10.006050"
    },
    {
      "arxiv_id": "2407.10196v1",
      "title": "A3S: A General Active Clustering Method with Pairwise Constraints",
      "title_zh": "A3S: 一种带有成对约束的通用主动聚类方法",
      "authors": [
        "Xun Deng",
        "Junlong Liu",
        "Han Zhong",
        "Fuli Feng",
        "Chen Shen",
        "Xiangnan He",
        "Jieping Ye",
        "Zheng Wang"
      ],
      "abstract": "Active clustering aims to boost the clustering performance by integrating\nhuman-annotated pairwise constraints through strategic querying. Conventional\napproaches with semi-supervised clustering schemes encounter high query costs\nwhen applied to large datasets with numerous classes. To address these\nlimitations, we propose a novel Adaptive Active Aggregation and Splitting (A3S)\nframework, falling within the cluster-adjustment scheme in active clustering.\nA3S features strategic active clustering adjustment on the initial cluster\nresult, which is obtained by an adaptive clustering algorithm. In particular,\nour cluster adjustment is inspired by the quantitative analysis of Normalized\nmutual information gain under the information theory framework and can provably\nimprove the clustering quality. The proposed A3S framework significantly\nelevates the performance and scalability of active clustering. In extensive\nexperiments across diverse real-world datasets, A3S achieves desired results\nwith significantly fewer human queries compared with existing methods.",
      "tldr_zh": "这篇论文提出了一种通用的主动聚类方法 A3S，利用人类标注的对偶约束（Pairwise Constraints）来提升聚类性能，同时解决传统方法在大规模数据集上查询成本高的局限。A3S 框架采用适应性聚合和分割（Adaptive Active Aggregation and Splitting）策略，对初始聚类结果进行战略调整，该调整基于信息理论框架下的 Normalized Mutual Information Gain 分析，以证明其对聚类质量的提升。实验结果显示，在多种真实数据集上，A3S 与现有方法相比，使用显著更少的查询就实现了更好的性能和可扩展性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.10196v1",
      "published_date": "2024-07-14 13:37:03 UTC",
      "updated_date": "2024-07-14 13:37:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:28:19.996150"
    },
    {
      "arxiv_id": "2407.10194v1",
      "title": "Curriculum Learning for Small Code Language Models",
      "title_zh": "针对小型代码语言模型",
      "authors": [
        "Marwa Naïr",
        "Kamel Yamani",
        "Lynda Said Lhadj",
        "Riyadh Baghdadi"
      ],
      "abstract": "Code language models have emerged as useful tools for various programming\ntasks, yet they often struggle when it comes to complex ones. In this paper, we\nexplore the potential of curriculum learning in enhancing the performance of\nthese models. While prior research has suggested that curriculum learning does\nnot necessarily help in improving the performance of language models, our\nresults surprisingly show that this may not be the case for code language\nmodels. We demonstrate that a well-designed curriculum learning approach\nsignificantly improves the accuracy of small decoder-only code language models\non the task of code execution, while its effect on code completion is less\nsignificant. To explore the potential of curriculum learning, we train multiple\nGPT models with 1 million parameters each to predict the next token and\nevaluate them on code completion and execution tasks. Our contributions include\nproposing a novel code difficulty assessment metric by combining software code\nmeasures, investigating the effectiveness of Curriculum Learning for code\nlanguage models, and introducing a Novel Curriculum Learning schedule that\nenhances the performance of small decoder-only language models in code\nexecution tasks. The results of this paper open the door for more research on\nthe use of curriculum learning for code language models.",
      "tldr_zh": "本研究探讨了 curriculum learning 在小型代码语言模型中的应用，以提升其在复杂编程任务上的性能。研究发现，虽然先前工作认为 curriculum learning 对语言模型帮助有限，但它能显著提高小型解码器-only 代码语言模型在代码执行任务的准确性，而对代码补全任务的影响较小。贡献包括：提出一个结合软件代码度量的代码难度评估指标、验证 curriculum learning 的有效性，以及引入一个新颖的 curriculum learning 方案。总体结果为代码语言模型的研究开辟了新方向。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.PL"
      ],
      "primary_category": "cs.LG",
      "comment": "ACL Student Research Workshop 2024 camera-ready",
      "pdf_url": "http://arxiv.org/pdf/2407.10194v1",
      "published_date": "2024-07-14 13:32:24 UTC",
      "updated_date": "2024-07-14 13:32:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:28:31.001795"
    },
    {
      "arxiv_id": "2407.10167v4",
      "title": "Key-Point-Driven Mathematical Reasoning Distillation of Large Language Model",
      "title_zh": "关键点驱动的大型语言模型数学推理蒸馏",
      "authors": [
        "Xunyu Zhu",
        "Jian Li",
        "Can Ma",
        "Weiping Wang"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated exceptional proficiency in\nmathematical reasoning tasks due to their extensive parameter counts and\ntraining on vast datasets. Despite these capabilities, deploying LLMs is\nhindered by their computational demands. Distilling LLM mathematical reasoning\ninto Smaller Language Models (SLMs) has emerged as a solution to this\nchallenge, although these smaller models often suffer from errors in\ncalculation and semantic understanding. Prior work has proposed\nProgram-of-Thought Distillation (PoTD) to avoid calculation error. To further\naddress semantic understanding errors, we propose Key-Point-Driven Mathematical\nReasoning Distillation (KPDD). KPDD enhances the reasoning performance of SLMs\nby breaking down the problem-solving process into three stages: Core Question\nExtraction, Problem-Solving Information Extraction, and Step-by-Step Solution.\nThis method is further divided into KPDD-CoT, which generates Chain-of-Thought\nrationales, and KPDD-PoT, which creates Program-of-Thought rationales. The\nexperiment results show that KPDD-CoT significantly improves reasoning\nabilities, while KPDD-PoT achieves state-of-the-art performance in mathematical\nreasoning tasks. Our approach effectively mitigates misunderstanding errors,\nadvancing the deployment of efficient and capable SLMs.",
      "tldr_zh": "大型语言模型 (LLMs) 在数学推理任务中表现出色，但计算需求高，因此本文提出 Key-Point-Driven Mathematical Reasoning Distillation (KPDD) 方法，将 LLMs 的推理能力蒸馏到小型语言模型 (SLMs) 中，以解决计算和语义理解错误。KPDD 将问题解决过程分解为三个阶段：Core Question Extraction、Problem-Solving Information Extraction 和 Step-by-Step Solution，并包括 KPDD-CoT（生成 Chain-of-Thought 推理）和 KPDD-PoT（生成 Program-of-Thought 推理）变体。实验结果表明，KPDD-CoT 显著提升 SLMs 的推理性能，而 KPDD-PoT 在数学任务中达到最先进水平，有效减少误解错误。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Major Updates:1.fix faults in the error analysis, 2. improve our\n  method, 3. use ChatGPT as teacher LLMs to ensure fairness in performance\n  comparisons",
      "pdf_url": "http://arxiv.org/pdf/2407.10167v4",
      "published_date": "2024-07-14 11:41:03 UTC",
      "updated_date": "2024-10-10 11:17:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:28:43.797091"
    },
    {
      "arxiv_id": "2407.11086v1",
      "title": "Pre-training with Fractional Denoising to Enhance Molecular Property Prediction",
      "title_zh": "利用分数去噪的预训练以增强分子属性预测",
      "authors": [
        "Yuyan Ni",
        "Shikun Feng",
        "Xin Hong",
        "Yuancheng Sun",
        "Wei-Ying Ma",
        "Zhi-Ming Ma",
        "Qiwei Ye",
        "Yanyan Lan"
      ],
      "abstract": "Deep learning methods have been considered promising for accelerating\nmolecular screening in drug discovery and material design. Due to the limited\navailability of labelled data, various self-supervised molecular pre-training\nmethods have been presented. While many existing methods utilize common\npre-training tasks in computer vision (CV) and natural language processing\n(NLP), they often overlook the fundamental physical principles governing\nmolecules. In contrast, applying denoising in pre-training can be interpreted\nas an equivalent force learning, but the limited noise distribution introduces\nbias into the molecular distribution. To address this issue, we introduce a\nmolecular pre-training framework called fractional denoising (Frad), which\ndecouples noise design from the constraints imposed by force learning\nequivalence. In this way, the noise becomes customizable, allowing for\nincorporating chemical priors to significantly improve molecular distribution\nmodeling. Experiments demonstrate that our framework consistently outperforms\nexisting methods, establishing state-of-the-art results across force\nprediction, quantum chemical properties, and binding affinity tasks. The\nrefined noise design enhances force accuracy and sampling coverage, which\ncontribute to the creation of physically consistent molecular representations,\nultimately leading to superior predictive performance.",
      "tldr_zh": "本研究提出了一种名为 Fractional Denoising (Frad) 的分子预训练框架，以提升分子属性预测性能。Frad 通过解耦噪声设计与力学习等效的约束，允许自定义噪声融入化学先验，从而更好地建模分子分布并减少现有方法的偏差。实验结果显示，Frad 在力预测、量子化学属性和结合亲和力任务上超越了现有方法，实现了最先进水平，并生成更物理一致的分子表示，提高了预测准确性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.chem-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.11086v1",
      "published_date": "2024-07-14 11:09:42 UTC",
      "updated_date": "2024-07-14 11:09:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:28:55.447676"
    },
    {
      "arxiv_id": "2407.10162v1",
      "title": "ChatLogic: Integrating Logic Programming with Large Language Models for Multi-Step Reasoning",
      "title_zh": "ChatLogic: 将逻辑编程与大型语言模型整合用于多步推理",
      "authors": [
        "Zhongsheng Wang",
        "Jiamou Liu",
        "Qiming Bao",
        "Hongfei Rong",
        "Jingfeng Zhang"
      ],
      "abstract": "Large language models (LLMs) such as ChatGPT and GPT-4 have demonstrated\nimpressive capabilities in various generative tasks. However, their performance\nis often hampered by limitations in accessing and leveraging long-term memory,\nleading to specific vulnerabilities and biases, especially during long\ninteractions. This paper introduces ChatLogic, an innovative framework\nspecifically targeted at LLM reasoning tasks that can enhance the performance\nof LLMs in multi-step deductive reasoning tasks by integrating logic\nprogramming. In ChatLogic, the language model plays a central role, acting as a\ncontroller and participating in every system operation stage. We propose a\nnovel method of converting logic problems into symbolic integration with an\ninference engine. This approach leverages large language models' situational\nunderstanding and imitation skills and uses symbolic memory to enhance\nmulti-step deductive reasoning capabilities. Our results show that the\nChatLogic framework significantly improves the multi-step reasoning\ncapabilities of LLMs. The source code and data are available at\n\\url{https://github.com/Strong-AI-Lab/ChatLogic}",
      "tldr_zh": "本文提出 ChatLogic 框架，将 Logic Programming 与 Large Language Models (LLMs) 整合，旨在解决 LLMs 在多步演绎推理任务中的记忆限制和偏差问题。框架中，LLMs 作为控制器参与所有操作阶段，通过将逻辑问题转换为符号集成并结合推理引擎，利用 LLMs 的情境理解和符号内存来增强推理能力。实验结果显示，ChatLogic 显著提高了 LLMs 的多步推理性能，相关源代码和数据可在 GitHub 上获取。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "8 pages, 3 figures. This paper has been accepted by WCCI IJCNN 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.10162v1",
      "published_date": "2024-07-14 11:06:43 UTC",
      "updated_date": "2024-07-14 11:06:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:29:18.601483"
    },
    {
      "arxiv_id": "2407.10153v1",
      "title": "Look Within, Why LLMs Hallucinate: A Causal Perspective",
      "title_zh": "翻译失败",
      "authors": [
        "He Li",
        "Haoang Chi",
        "Mingyu Liu",
        "Wenjing Yang"
      ],
      "abstract": "The emergence of large language models (LLMs) is a milestone in generative\nartificial intelligence, achieving significant success in text comprehension\nand generation tasks. Despite the tremendous success of LLMs in many downstream\ntasks, they suffer from severe hallucination problems, posing significant\nchallenges to the practical applications of LLMs. Most of the works about LLMs'\nhallucinations focus on data quality. Self-attention is a core module in\ntransformer-based LLMs, while its potential relationship with LLMs'\nhallucination has been hardly investigated. To fill this gap, we study this\nproblem from a causal perspective. We propose a method to intervene in LLMs'\nself-attention layers and maintain their structures and sizes intact.\nSpecifically, we disable different self-attention layers in several popular\nopen-source LLMs and then compare their degrees of hallucination with the\noriginal ones. We evaluate the intervened LLMs on hallucination assessment\nbenchmarks and conclude that disabling some specific self-attention layers in\nthe front or tail of the LLMs can alleviate hallucination issues. The study\npaves a new way for understanding and mitigating LLMs' hallucinations.",
      "tldr_zh": "这篇论文从因果视角（causal perspective）探讨了大型语言模型（LLMs）的幻觉问题（hallucination），强调了自注意力（self-attention）机制在其中的潜在作用，而非仅关注数据质量。作者提出了一种干预方法，通过禁用不同自注意力层来评估其对幻觉的影响，同时保持模型结构和大小不变。实验结果显示，禁用某些前部或尾部自注意力层能显著缓解幻觉问题，为理解和减轻 LLMs 幻觉提供了新途径。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "15 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.10153v1",
      "published_date": "2024-07-14 10:47:44 UTC",
      "updated_date": "2024-07-14 10:47:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:29:19.607709"
    },
    {
      "arxiv_id": "2407.11085v1",
      "title": "SpreadFGL: Edge-Client Collaborative Federated Graph Learning with Adaptive Neighbor Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Luying Zhong",
        "Yueyang Pi",
        "Zheyi Chen",
        "Zhengxin Yu",
        "Wang Miao",
        "Xing Chen",
        "Geyong Min"
      ],
      "abstract": "Federated Graph Learning (FGL) has garnered widespread attention by enabling\ncollaborative training on multiple clients for semi-supervised classification\ntasks. However, most existing FGL studies do not well consider the missing\ninter-client topology information in real-world scenarios, causing insufficient\nfeature aggregation of multi-hop neighbor clients during model training.\nMoreover, the classic FGL commonly adopts the FedAvg but neglects the high\ntraining costs when the number of clients expands, resulting in the overload of\na single edge server. To address these important challenges, we propose a novel\nFGL framework, named SpreadFGL, to promote the information flow in edge-client\ncollaboration and extract more generalized potential relationships between\nclients. In SpreadFGL, an adaptive graph imputation generator incorporated with\na versatile assessor is first designed to exploit the potential links between\nsubgraphs, without sharing raw data. Next, a new negative sampling mechanism is\ndeveloped to make SpreadFGL concentrate on more refined information in\ndownstream tasks. To facilitate load balancing at the edge layer, SpreadFGL\nfollows a distributed training manner that enables fast model convergence.\nUsing real-world testbed and benchmark graph datasets, extensive experiments\ndemonstrate the effectiveness of the proposed SpreadFGL. The results show that\nSpreadFGL achieves higher accuracy and faster convergence against\nstate-of-the-art algorithms.",
      "tldr_zh": "该论文针对 Federated Graph Learning (FGL) 在真实场景中缺失拓扑信息的问题，提出了一种新型框架 SpreadFGL，通过边-客户端协作来提升多跳邻居客户端的特征聚合。SpreadFGL 设计了自适应图填充生成器（adaptive graph imputation generator）和多功能评估器（versatile assessor），在不共享原始数据的情况下挖掘子图之间的潜在链接，并引入新的负采样机制（negative sampling mechanism）以专注于下游任务的精炼信息。同时，该框架采用分布式训练方式，实现边缘层的负载均衡并加速模型收敛。实验结果显示，SpreadFGL 在真实测试床和基准图数据集上，比最先进算法实现了更高的准确率和更快收敛。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.11085v1",
      "published_date": "2024-07-14 09:34:19 UTC",
      "updated_date": "2024-07-14 09:34:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:29:32.984147"
    },
    {
      "arxiv_id": "2407.10115v1",
      "title": "A Bag of Tricks for Scaling CPU-based Deep FFMs to more than 300m Predictions per Second",
      "title_zh": "翻译失败",
      "authors": [
        "Blaž Škrlj",
        "Benjamin Ben-Shalom",
        "Grega Gašperšič",
        "Adi Schwartz",
        "Ramzi Hoseisi",
        "Naama Ziporin",
        "Davorin Kopič",
        "Andraž Tori"
      ],
      "abstract": "Field-aware Factorization Machines (FFMs) have emerged as a powerful model\nfor click-through rate prediction, particularly excelling in capturing complex\nfeature interactions. In this work, we present an in-depth analysis of our\nin-house, Rust-based Deep FFM implementation, and detail its deployment on a\nCPU-only, multi-data-center scale. We overview key optimizations devised for\nboth training and inference, demonstrated by previously unpublished benchmark\nresults in efficient model search and online training. Further, we detail an\nin-house weight quantization that resulted in more than an order of magnitude\nreduction in bandwidth footprint related to weight transfers across\ndata-centres. We disclose the engine and associated techniques under an\nopen-source license to contribute to the broader machine learning community.\nThis paper showcases one of the first successful CPU-only deployments of Deep\nFFMs at such scale, marking a significant stride in practical, low-footprint\nclick-through rate prediction methodologies.",
      "tldr_zh": "该论文探讨了如何优化基于 CPU 的 Deep FFMs（Field-aware Factorization Machines），使其处理速度超过 3 亿预测/秒，用于点击率预测。作者详细分析了其 Rust-based 实现，包括训练和推理的关键优化，如高效模型搜索和在线训练基准。论文还介绍了内部权重量化技术，大幅减少了跨数据中心权重传输的带宽占用，并将相关引擎开源，以推动机器学习社区的发展。该工作标志着 CPU-only 部署 Deep FFMs 在大规模实际应用中的重要进展。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.LG",
      "comment": "6p, KDD2024 - AdKDD workshop",
      "pdf_url": "http://arxiv.org/pdf/2407.10115v1",
      "published_date": "2024-07-14 08:10:20 UTC",
      "updated_date": "2024-07-14 08:10:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:29:44.035579"
    },
    {
      "arxiv_id": "2407.10105v1",
      "title": "Hierarchical Multi-modal Transformer for Cross-modal Long Document Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Tengfei Liu",
        "Yongli Hu",
        "Junbin Gao",
        "Yanfeng Sun",
        "Baocai Yin"
      ],
      "abstract": "Long Document Classification (LDC) has gained significant attention recently.\nHowever, multi-modal data in long documents such as texts and images are not\nbeing effectively utilized. Prior studies in this area have attempted to\nintegrate texts and images in document-related tasks, but they have only\nfocused on short text sequences and images of pages. How to classify long\ndocuments with hierarchical structure texts and embedding images is a new\nproblem and faces multi-modal representation difficulties. In this paper, we\npropose a novel approach called Hierarchical Multi-modal Transformer (HMT) for\ncross-modal long document classification. The HMT conducts multi-modal feature\ninteraction and fusion between images and texts in a hierarchical manner. Our\napproach uses a multi-modal transformer and a dynamic multi-scale multi-modal\ntransformer to model the complex relationships between image features, and the\nsection and sentence features. Furthermore, we introduce a new interaction\nstrategy called the dynamic mask transfer module to integrate these two\ntransformers by propagating features between them. To validate our approach, we\nconduct cross-modal LDC experiments on two newly created and two publicly\navailable multi-modal long document datasets, and the results show that the\nproposed HMT outperforms state-of-the-art single-modality and multi-modality\nmethods.",
      "tldr_zh": "该论文针对长文档分类（Long Document Classification, LDC）中多模态数据（如文本和图像）的利用不足问题，提出了一种新方法Hierarchical Multi-modal Transformer (HMT)，以处理具有层次结构文本和嵌入图像的跨模态长文档分类挑战。HMT 通过分层方式进行多模态特征交互和融合，包括multi-modal transformer、dynamic multi-scale multi-modal transformer，以及dynamic mask transfer module，来建模图像特征、部分特征和句子特征之间的复杂关系。在四个多模态长文档数据集上的实验结果显示，HMT 优于现有单模态和多模态方法，证明了其有效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "IEEE Transactions on Multimedia",
      "pdf_url": "http://arxiv.org/pdf/2407.10105v1",
      "published_date": "2024-07-14 07:12:25 UTC",
      "updated_date": "2024-07-14 07:12:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:29:56.831253"
    },
    {
      "arxiv_id": "2407.10104v1",
      "title": "A Self-Supervised Learning Pipeline for Demographically Fair Facial Attribute Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Sreeraj Ramachandran",
        "Ajita Rattani"
      ],
      "abstract": "Published research highlights the presence of demographic bias in automated\nfacial attribute classification. The proposed bias mitigation techniques are\nmostly based on supervised learning, which requires a large amount of labeled\ntraining data for generalizability and scalability. However, labeled data is\nlimited, requires laborious annotation, poses privacy risks, and can perpetuate\nhuman bias. In contrast, self-supervised learning (SSL) capitalizes on freely\navailable unlabeled data, rendering trained models more scalable and\ngeneralizable. However, these label-free SSL models may also introduce biases\nby sampling false negative pairs, especially at low-data regimes 200K images)\nunder low compute settings. Further, SSL-based models may suffer from\nperformance degradation due to a lack of quality assurance of the unlabeled\ndata sourced from the web. This paper proposes a fully self-supervised pipeline\nfor demographically fair facial attribute classifiers. Leveraging completely\nunlabeled data pseudolabeled via pre-trained encoders, diverse data curation\ntechniques, and meta-learning-based weighted contrastive learning, our method\nsignificantly outperforms existing SSL approaches proposed for downstream image\nclassification tasks. Extensive evaluations on the FairFace and CelebA datasets\ndemonstrate the efficacy of our pipeline in obtaining fair performance over\nexisting baselines. Thus, setting a new benchmark for SSL in the fairness of\nfacial attribute classification.",
      "tldr_zh": "该论文提出了一种自监督学习（Self-Supervised Learning, SSL）管道，用于实现人口统计学公平（Demographically Fair）的面部属性分类（Facial Attribute Classification）。该方法利用未标注数据通过预训练编码器进行伪标注、结合多样数据整理技术和基于元学习的加权对比学习（Weighted Contrastive Learning），以缓解现有监督学习方法在数据标注和偏差方面的局限性。实验在 FairFace 和 CelebA 数据集上显示，该管道在公平性方面显著优于现有 SSL 基准，树立了 SSL 在面部属性分类公平性方面的全新标准。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "13 pages, IJCB 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.10104v1",
      "published_date": "2024-07-14 07:11:57 UTC",
      "updated_date": "2024-07-14 07:11:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:30:08.228353"
    },
    {
      "arxiv_id": "2407.10090v1",
      "title": "ReactAIvate: A Deep Learning Approach to Predicting Reaction Mechanisms and Unmasking Reactivity Hotspots",
      "title_zh": "ReactAIvate：一种深度学习",
      "authors": [
        "Ajnabiul Hoque",
        "Manajit Das",
        "Mayank Baranwal",
        "Raghavan B. Sunoj"
      ],
      "abstract": "A chemical reaction mechanism (CRM) is a sequence of molecular-level events\ninvolving bond-breaking/forming processes, generating transient intermediates\nalong the reaction pathway as reactants transform into products. Understanding\nsuch mechanisms is crucial for designing and discovering new reactions. One of\nthe currently available methods to probe CRMs is quantum mechanical (QM)\ncomputations. The resource-intensive nature of QM methods and the scarcity of\nmechanism-based datasets motivated us to develop reliable ML models for\npredicting mechanisms. In this study, we created a comprehensive dataset with\nseven distinct classes, each representing uniquely characterized elementary\nsteps. Subsequently, we developed an interpretable attention-based GNN that\nachieved near-unity and 96% accuracy, respectively for reaction step\nclassification and the prediction of reactive atoms in each such step,\ncapturing interactions between the broader reaction context and local active\nregions. The near-perfect classification enables accurate prediction of both\nindividual events and the entire CRM, mitigating potential drawbacks of Seq2Seq\napproaches, where a wrongly predicted character leads to incoherent CRM\nidentification. In addition to interpretability, our model adeptly identifies\nkey atom(s) even from out-of-distribution classes. This generalizabilty allows\nfor the inclusion of new reaction types in a modular fashion, thus will be of\nvalue to experts for understanding the reactivity of new molecules.",
      "tldr_zh": "本研究针对化学反应机制（CRM）的预测问题，开发了ReactAIvate模型，该模型使用可解释的注意力机制图神经网络（GNN）来分类反应步骤并识别反应热点，以克服量子力学（QM）计算的资源密集和数据集稀缺的挑战。研究者创建了一个包含七个独特类别的全面数据集，模型在反应步骤分类上实现了近乎完美的准确率，在预测反应原子上达到96%的准确率，同时捕捉反应上下文与局部活跃区域的交互。相比Seq2Seq方法，该模型避免了单个预测错误导致整体失效的问题，并展示了强泛化能力，便于模块化添加新反应类型，从而为理解新分子反应性提供价值。",
      "categories": [
        "physics.chem-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "physics.chem-ph",
      "comment": "Accepted to 27th ECAI main track",
      "pdf_url": "http://arxiv.org/pdf/2407.10090v1",
      "published_date": "2024-07-14 05:53:18 UTC",
      "updated_date": "2024-07-14 05:53:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:30:21.341819"
    },
    {
      "arxiv_id": "2407.10086v2",
      "title": "Rapid Biomedical Research Classification: The Pandemic PACT Advanced Categorisation Engine",
      "title_zh": "翻译失败",
      "authors": [
        "Omid Rohanian",
        "Mohammadmahdi Nouriborji",
        "Olena Seminog",
        "Rodrigo Furst",
        "Thomas Mendy",
        "Shanthi Levanita",
        "Zaharat Kadri-Alabi",
        "Nusrat Jabin",
        "Daniela Toale",
        "Georgina Humphreys",
        "Emilia Antonio",
        "Adrian Bucher",
        "Alice Norton",
        "David A. Clifton"
      ],
      "abstract": "This paper introduces the Pandemic PACT Advanced Categorisation Engine\n(PPACE) along with its associated dataset. PPACE is a fine-tuned model\ndeveloped to automatically classify research abstracts from funded biomedical\nprojects according to WHO-aligned research priorities. This task is crucial for\nmonitoring research trends and identifying gaps in global health preparedness\nand response. Our approach builds on human-annotated projects, which are\nallocated one or more categories from a predefined list. A large language model\nis then used to generate `rationales' explaining the reasoning behind these\nannotations. This augmented data, comprising expert annotations and rationales,\nis subsequently used to fine-tune a smaller, more efficient model. Developed as\npart of the Pandemic PACT project, which aims to track and analyse research\nfunding and clinical evidence for a wide range of diseases with outbreak\npotential, PPACE supports informed decision-making by research funders,\npolicymakers, and independent researchers. We introduce and release both the\ntrained model and the instruction-based dataset used for its training. Our\nevaluation shows that PPACE significantly outperforms its baselines. The\nrelease of PPACE and its associated dataset offers valuable resources for\nresearchers in multilabel biomedical document classification and supports\nadvancements in aligning biomedical research with key global health priorities.",
      "tldr_zh": "这篇论文介绍了 Pandemic PACT Advanced Categorisation Engine (PPACE)，一个微调模型，用于自动分类资助的生物医学研究摘要，以符合 WHO-aligned 研究优先事项，从而监控研究趋势并识别全球健康准备的空白。方法包括利用人类标注数据生成 rationales 解释，并用这些增强数据集 fine-tune 一个更小、更高效的模型。实验结果显示，PPACE 在 multilabel biomedical document classification 任务中显著优于基线模型，并发布了模型和数据集，以支持研究资助者、政策制定者和研究人员更好地对齐生物医学研究与全球健康优先事项。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "68T50",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.10086v2",
      "published_date": "2024-07-14 05:22:53 UTC",
      "updated_date": "2024-07-19 14:28:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:30:33.305883"
    },
    {
      "arxiv_id": "2407.10078v2",
      "title": "Data Imputation using Large Language Model to Accelerate Recommendation System",
      "title_zh": "利用大型语言模型进行数据插补以加速推荐系统",
      "authors": [
        "Zhicheng Ding",
        "Jiahao Tian",
        "Zhenkai Wang",
        "Jinman Zhao",
        "Siyang Li"
      ],
      "abstract": "This paper aims to address the challenge of sparse and missing data in\nrecommendation systems, a significant hurdle in the age of big data.\nTraditional imputation methods struggle to capture complex relationships within\nthe data. We propose a novel approach that fine-tune Large Language Model (LLM)\nand use it impute missing data for recommendation systems. LLM which is trained\non vast amounts of text, is able to understand complex relationship among data\nand intelligently fill in missing information. This enriched data is then used\nby the recommendation system to generate more accurate and personalized\nsuggestions, ultimately enhancing the user experience. We evaluate our\nLLM-based imputation method across various tasks within the recommendation\nsystem domain, including single classification, multi-classification, and\nregression compared to traditional data imputation methods. By demonstrating\nthe superiority of LLM imputation over traditional methods, we establish its\npotential for improving recommendation system performance.",
      "tldr_zh": "本论文针对推荐系统中数据稀疏和缺失的问题，提出了一种使用微调 Large Language Model (LLM) 进行数据填充的新方法，以捕捉复杂数据关系并加速推荐系统。\nLLM 凭借其在海量文本训练中的能力，能智能地填充缺失信息，从而为推荐系统提供更丰富的数据，支持生成更准确和个性化的用户建议。\n实验评估显示，该方法在单分类、多分类和回归任务上比传统填充方法表现出色，显著提升了推荐系统的整体性能。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.10078v2",
      "published_date": "2024-07-14 04:53:36 UTC",
      "updated_date": "2024-08-07 21:05:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:30:44.394837"
    },
    {
      "arxiv_id": "2407.20236v1",
      "title": "Artificial Intelligence from Idea to Implementation. How Can AI Reshape the Education Landscape?",
      "title_zh": "人工智能：从想法到实现。人工智能如何重塑教育景观？",
      "authors": [
        "Catalin Vrabie"
      ],
      "abstract": "This introductory chapter provides an overview of the evolution and impact of\nArtificial Intelligence technologies in today society. Beginning with a\nhistorical context while exploring a few general definitions of AI, the author\nprovides a timeline of the used technologies, highlighting its periods of\nstagnation, commonly referred to as AI winters, and the subsequent resurgence\nfueled by relentless enthusiasm and investment. The narrative then transitions\nto focus on the transformative effects of AI on society at large, with a\nparticular emphasis on educational applications. Through examples, the paper\nshows how AI technologies have moved from theoretical constructs to practical\ntools that are reshaping pedagogical approaches and student engagement. The\nessay concludes by discussing the prospects of AI in education, emphasizing the\nneed for a balanced approach that considers both technological advancements and\nsocietal implications.",
      "tldr_zh": "本论文概述了人工智能（AI）从概念到实际应用的演变历程，包括其历史背景、定义、时间线（如AI winters和复兴期），并强调AI如何通过持续的热情和投资重塑社会。作者特别聚焦AI在教育领域的转型影响，通过具体例子展示AI从理论框架转为实用工具，提升教学方法和学生参与度。论文最终讨论AI在教育的未来前景，呼吁平衡技术进步与社会影响，以实现可持续应用。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "33 pages, book chapter",
      "pdf_url": "http://arxiv.org/pdf/2407.20236v1",
      "published_date": "2024-07-14 04:40:16 UTC",
      "updated_date": "2024-07-14 04:40:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:30:54.990656"
    },
    {
      "arxiv_id": "2407.10058v2",
      "title": "Learning to Refuse: Towards Mitigating Privacy Risks in LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Zhenhua Liu",
        "Tong Zhu",
        "Chuanyuan Tan",
        "Wenliang Chen"
      ],
      "abstract": "Large language models (LLMs) exhibit remarkable capabilities in understanding\nand generating natural language. However, these models can inadvertently\nmemorize private information, posing significant privacy risks. This study\naddresses the challenge of enabling LLMs to protect specific individuals'\nprivate data without the need for complete retraining. We propose \\return, a\nReal-world pErsonal daTa UnleaRNing dataset, comprising 2,492 individuals from\nWikipedia with associated QA pairs, to evaluate machine unlearning (MU) methods\nfor protecting personal data in a realistic scenario. Additionally, we\nintroduce the Name-Aware Unlearning Framework (NAUF) for Privacy Protection,\nwhich enables the model to learn which individuals' information should be\nprotected without affecting its ability to answer questions related to other\nunrelated individuals. Our extensive experiments demonstrate that NAUF achieves\na state-of-the-art average unlearning score, surpassing the best baseline\nmethod by 5.65 points, effectively protecting target individuals' personal data\nwhile maintaining the model's general capabilities.",
      "tldr_zh": "本研究针对大型语言模型（LLMs）可能无意中记忆私人信息带来的隐私风险，提出了一种无需完全重新训练的方法来保护特定个人的数据。研究者构建了RETuRN数据集，包含2,492个维基百科人物的问答对，用于评估machine unlearning（MU）方法在真实场景中的效果；同时引入Name-Aware Unlearning Framework（NAUF），让模型学会识别并拒绝提供目标个人的信息，同时保持对其他无关个体的回答能力。实验结果显示，NAUF的平均unlearning分数比最佳基线方法提高了5.65分，有效提升了隐私保护水平，同时维持了模型的整体性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.10058v2",
      "published_date": "2024-07-14 03:05:53 UTC",
      "updated_date": "2024-09-16 07:20:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:31:18.112858"
    },
    {
      "arxiv_id": "2407.10049v1",
      "title": "AutoGRAMS: Autonomous Graphical Agent Modeling Software",
      "title_zh": "翻译失败",
      "authors": [
        "Ben Krause",
        "Lucia Chen",
        "Emmanuel Kahembwe"
      ],
      "abstract": "We introduce the AutoGRAMS framework for programming multi-step interactions\nwith language models. AutoGRAMS represents AI agents as a graph, where each\nnode can execute either a language modeling instruction or traditional code.\nLikewise, transitions in the graph can be governed by either language modeling\ndecisions or traditional branch logic. AutoGRAMS supports using variables as\nmemory and allows nodes to call other AutoGRAMS graphs as functions. We show\nhow AutoGRAMS can be used to design highly sophisticated agents, including\nself-referential agents that can modify their own graph. AutoGRAMS's\ngraph-centric approach aids interpretability, controllability, and safety\nduring the design, development, and deployment of AI agents. We provide our\nframework as open source at https://github.com/autograms/autograms .",
      "tldr_zh": "该研究引入了 AutoGRAMS 框架，一种用于编程语言模型多步交互的自主图形代理建模软件，将 AI agents 表示为图结构，其中每个节点可执行语言建模指令或传统代码。框架允许图的转移由语言建模决策或传统分支逻辑控制，并支持使用变量作为内存以及节点调用其他 AutoGRAMS 图作为函数，从而实现高度复杂的代理设计，包括自引用 agents 能修改自身图。AutoGRAMS 的图中心方法提升了 AI agents 在设计、开发和部署过程中的可解释性、可控性和安全性，并作为开源项目提供在 GitHub 上。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.10049v1",
      "published_date": "2024-07-14 02:25:45 UTC",
      "updated_date": "2024-07-14 02:25:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:31:20.782808"
    },
    {
      "arxiv_id": "2407.10040v5",
      "title": "Lean-STaR: Learning to Interleave Thinking and Proving",
      "title_zh": "Lean-STaR：学习交错思考与证明",
      "authors": [
        "Haohan Lin",
        "Zhiqing Sun",
        "Sean Welleck",
        "Yiming Yang"
      ],
      "abstract": "Traditional language model-based theorem proving assumes that by training on\na sufficient amount of formal proof data, a model will learn to prove theorems.\nOur key observation is that a wealth of informal information that is not\npresent in formal proofs can be useful for learning to prove theorems. For\ninstance, humans think through steps of a proof, but this thought process is\nnot visible in the resulting code. We present Lean-STaR, a framework for\ntraining language models to produce informal thoughts prior to each step of a\nproof, thereby boosting the model's theorem-proving capabilities. Lean-STaR\nuses retrospective ground-truth tactics to generate synthetic thoughts for\ntraining the language model. At inference time, the trained model directly\ngenerates the thoughts prior to the prediction of the tactics in each proof\nstep. Building on the self-taught reasoner framework, we then apply expert\niteration to further fine-tune the model on the correct proofs it samples and\nverifies using the Lean solver. Lean-STaR achieves state-of-the-art results on\nthe miniF2F-test benchmark within the Lean theorem proving environment,\nsignificantly outperforming base models ($\\boldsymbol{43.4\\% \\rightarrow\n46.3\\%,}$ Pass@64). We also analyze the impact of the augmented thoughts on\nvarious aspects of the theorem proving process, providing insights into their\neffectiveness.",
      "tldr_zh": "该研究观察到，传统语言模型在定理证明中忽略了非正式信息（如人类的思考过程），因此提出Lean-STaR框架，通过训练模型在每个证明步骤前生成合成思考来提升定理证明能力。Lean-STaR使用retrospective ground-truth tactics生成训练数据，并在推理时整合这些思考，然后通过expert iteration微调模型，利用Lean求解器验证和优化证明。实验结果显示，该框架在miniF2F-test基准上将Pass@64准确率从43.4%提升至46.3%，显著优于基线模型，并分析了增强思考对证明过程的积极影响。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.10040v5",
      "published_date": "2024-07-14 01:43:07 UTC",
      "updated_date": "2025-03-15 12:25:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:31:32.732754"
    },
    {
      "arxiv_id": "2407.15718v1",
      "title": "Integrating AI Tutors in a Programming Course",
      "title_zh": "AI 导师在编程课程中的整合",
      "authors": [
        "Iris Ma",
        "Alberto Krone Martins",
        "Cristina Videira Lopes"
      ],
      "abstract": "RAGMan is an LLM-powered tutoring system that can support a variety of\ncourse-specific and homework-specific AI tutors. RAGMan leverages Retrieval\nAugmented Generation (RAG), as well as strict instructions, to ensure the\nalignment of the AI tutors' responses. By using RAGMan's AI tutors, students\nreceive assistance with their specific homework assignments without directly\nobtaining solutions, while also having the ability to ask general\nprogramming-related questions.\n  RAGMan was deployed as an optional resource in an introductory programming\ncourse with an enrollment of 455 students. It was configured as a set of five\nhomework-specific AI tutors. This paper describes the interactions the students\nhad with the AI tutors, the students' feedback, and a comparative grade\nanalysis. Overall, about half of the students engaged with the AI tutors, and\nthe vast majority of the interactions were legitimate homework questions. When\nstudents posed questions within the intended scope, the AI tutors delivered\naccurate responses 98% of the time. Within the students used AI tutors, 78%\nreported that the tutors helped their learning. Beyond AI tutors' ability to\nprovide valuable suggestions, students reported appreciating them for fostering\na safe learning environment free from judgment.",
      "tldr_zh": "本研究介绍了 RAGMan，一种基于大型语言模型 (LLM) 的辅导系统，利用 Retrieval Augmented Generation (RAG) 和严格指令，确保 AI 辅导员在编程课程中提供准确、针对性的响应，帮助学生解答作业问题而不直接给出解决方案。RAGMan 在一门有 455 名学生的入门编程课程中部署为五个作业特定 AI 辅导员，结果显示约一半学生参与互动，且 98% 的范围内问题获得准确回答。学生反馈显示，78% 的使用者认为 AI 辅导员提升了学习效果，并赞赏其创建了无判断的安全学习环境。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC",
        "cs.IR",
        "cs.SE"
      ],
      "primary_category": "cs.CY",
      "comment": "Accepted at SIGCSE Virtual 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.15718v1",
      "published_date": "2024-07-14 00:42:39 UTC",
      "updated_date": "2024-07-14 00:42:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T06:31:43.670947"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 34,
  "processed_papers_count": 34,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-19T06:32:02.489072"
}
[
  {
    "arxiv_id": "2406.13891v2",
    "title": "DPO: Dual-Perturbation Optimization for Test-time Adaptation in 3D Object Detection",
    "authors": [
      "Zhuoxiao Chen",
      "Zixin Wang",
      "Yadan Luo",
      "Sen Wang",
      "Zi Huang"
    ],
    "abstract": "LiDAR-based 3D object detection has seen impressive advances in recent times.\nHowever, deploying trained 3D detectors in the real world often yields\nunsatisfactory performance when the distribution of the test data significantly\ndeviates from the training data due to different weather conditions, object\nsizes, \\textit{etc}. A key factor in this performance degradation is the\ndiminished generalizability of pre-trained models, which creates a sharp loss\nlandscape during training. Such sharpness, when encountered during testing, can\nprecipitate significant performance declines, even with minor data variations.\nTo address the aforementioned challenges, we propose \\textbf{dual-perturbation\noptimization (DPO)} for \\textbf{\\underline{T}est-\\underline{t}ime\n\\underline{A}daptation in \\underline{3}D \\underline{O}bject\n\\underline{D}etection (TTA-3OD)}. We minimize the sharpness to cultivate a flat\nloss landscape to ensure model resiliency to minor data variations, thereby\nenhancing the generalization of the adaptation process. To fully capture the\ninherent variability of the test point clouds, we further introduce adversarial\nperturbation to the input BEV features to better simulate the noisy test\nenvironment. As the dual perturbation strategy relies on trustworthy\nsupervision signals, we utilize a reliable Hungarian matcher to filter out\npseudo-labels sensitive to perturbations. Additionally, we introduce early\nHungarian cutoff to avoid error accumulation from incorrect pseudo-labels by\nhalting the adaptation process. Extensive experiments across three types of\ntransfer tasks demonstrate that the proposed DPO significantly surpasses\nprevious state-of-the-art approaches, specifically on Waymo $\\rightarrow$\nKITTI, outperforming the most competitive baseline by 57.72\\% in\n$\\text{AP}_\\text{3D}$ and reaching 91\\% of the fully supervised upper bound.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "To appear in ACM Multimedia 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.13891v2",
    "published_date": "2024-06-19 23:46:08 UTC",
    "updated_date": "2024-07-28 07:36:41 UTC"
  },
  {
    "arxiv_id": "2406.13890v2",
    "title": "ClinicalLab: Aligning Agents for Multi-Departmental Clinical Diagnostics in the Real World",
    "authors": [
      "Weixiang Yan",
      "Haitian Liu",
      "Tengxiao Wu",
      "Qian Chen",
      "Wen Wang",
      "Haoyuan Chai",
      "Jiayi Wang",
      "Weishan Zhao",
      "Yixin Zhang",
      "Renjun Zhang",
      "Li Zhu",
      "Xuandong Zhao"
    ],
    "abstract": "LLMs have achieved significant performance progress in various NLP\napplications. However, LLMs still struggle to meet the strict requirements for\naccuracy and reliability in the medical field and face many challenges in\nclinical applications. Existing clinical diagnostic evaluation benchmarks for\nevaluating medical agents powered by LLMs have severe limitations. Firstly,\nmost existing medical evaluation benchmarks face the risk of data leakage or\ncontamination. Secondly, existing benchmarks often neglect the characteristics\nof multiple departments and specializations in modern medical practice.\nThirdly, existing evaluation methods are limited to multiple-choice questions,\nwhich do not align with the real-world diagnostic scenarios. Lastly, existing\nevaluation methods lack comprehensive evaluations of end-to-end real clinical\nscenarios. These limitations in benchmarks in turn obstruct advancements of\nLLMs and agents for medicine. To address these limitations, we introduce\nClinicalLab, a comprehensive clinical diagnosis agent alignment suite.\nClinicalLab includes ClinicalBench, an end-to-end multi-departmental clinical\ndiagnostic evaluation benchmark for evaluating medical agents and LLMs.\nClinicalBench is based on real cases that cover 24 departments and 150\ndiseases. ClinicalLab also includes four novel metrics (ClinicalMetrics) for\nevaluating the effectiveness of LLMs in clinical diagnostic tasks. We evaluate\n17 LLMs and find that their performance varies significantly across different\ndepartments. Based on these findings, in ClinicalLab, we propose ClinicalAgent,\nan end-to-end clinical agent that aligns with real-world clinical diagnostic\npractices. We systematically investigate the performance and applicable\nscenarios of variants of ClinicalAgent on ClinicalBench. Our findings\ndemonstrate the importance of aligning with modern medical practices in\ndesigning medical agents.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13890v2",
    "published_date": "2024-06-19 23:44:25 UTC",
    "updated_date": "2024-10-09 19:09:30 UTC"
  },
  {
    "arxiv_id": "2406.13885v1",
    "title": "Knowledge Tagging System on Math Questions via LLMs with Flexible Demonstration Retriever",
    "authors": [
      "Hang Li",
      "Tianlong Xu",
      "Jiliang Tang",
      "Qingsong Wen"
    ],
    "abstract": "Knowledge tagging for questions plays a crucial role in contemporary\nintelligent educational applications, including learning progress diagnosis,\npractice question recommendations, and course content organization.\nTraditionally, these annotations are always conducted by pedagogical experts,\nas the task requires not only a strong semantic understanding of both question\nstems and knowledge definitions but also deep insights into connecting\nquestion-solving logic with corresponding knowledge concepts. With the recent\nemergence of advanced text encoding algorithms, such as pre-trained language\nmodels, many researchers have developed automatic knowledge tagging systems\nbased on calculating the semantic similarity between the knowledge and question\nembeddings. In this paper, we explore automating the task using Large Language\nModels (LLMs), in response to the inability of prior encoding-based methods to\ndeal with the hard cases which involve strong domain knowledge and complicated\nconcept definitions. By showing the strong performance of zero- and few-shot\nresults over math questions knowledge tagging tasks, we demonstrate LLMs' great\npotential in conquering the challenges faced by prior methods. Furthermore, by\nproposing a reinforcement learning-based demonstration retriever, we\nsuccessfully exploit the great potential of different-sized LLMs in achieving\nbetter performance results while keeping the in-context demonstration usage\nefficiency high.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "13 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.13885v1",
    "published_date": "2024-06-19 23:30:01 UTC",
    "updated_date": "2024-06-19 23:30:01 UTC"
  },
  {
    "arxiv_id": "2406.13873v1",
    "title": "A Pure Transformer Pretraining Framework on Text-attributed Graphs",
    "authors": [
      "Yu Song",
      "Haitao Mao",
      "Jiachen Xiao",
      "Jingzhe Liu",
      "Zhikai Chen",
      "Wei Jin",
      "Carl Yang",
      "Jiliang Tang",
      "Hui Liu"
    ],
    "abstract": "Pretraining plays a pivotal role in acquiring generalized knowledge from\nlarge-scale data, achieving remarkable successes as evidenced by large models\nin CV and NLP. However, progress in the graph domain remains limited due to\nfundamental challenges such as feature heterogeneity and structural\nheterogeneity. Recently, increasing efforts have been made to enhance node\nfeature quality with Large Language Models (LLMs) on text-attributed graphs\n(TAGs), demonstrating superiority to traditional bag-of-words or word2vec\ntechniques. These high-quality node features reduce the previously critical\nrole of graph structure, resulting in a modest performance gap between Graph\nNeural Networks (GNNs) and structure-agnostic Multi-Layer Perceptrons (MLPs).\nMotivated by this, we introduce a feature-centric pretraining perspective by\ntreating graph structure as a prior and leveraging the rich, unified feature\nspace to learn refined interaction patterns that generalizes across graphs. Our\nframework, Graph Sequence Pretraining with Transformer (GSPT), samples node\ncontexts through random walks and employs masked feature reconstruction to\ncapture pairwise proximity in the LLM-unified feature space using a standard\nTransformer. By utilizing unified text representations rather than varying\nstructures, our framework achieves significantly better transferability among\ngraphs within the same domain. GSPT can be easily adapted to both node\nclassification and link prediction, demonstrating promising empirical success\non various datasets.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13873v1",
    "published_date": "2024-06-19 22:30:08 UTC",
    "updated_date": "2024-06-19 22:30:08 UTC"
  },
  {
    "arxiv_id": "2406.13868v1",
    "title": "SDQ: Sparse Decomposed Quantization for LLM Inference",
    "authors": [
      "Geonhwa Jeong",
      "Po-An Tsai",
      "Stephen W. Keckler",
      "Tushar Krishna"
    ],
    "abstract": "Recently, large language models (LLMs) have shown surprising performance in\ntask-specific workloads as well as general tasks with the given prompts.\nHowever, to achieve unprecedented performance, recent LLMs use billions to\ntrillions of parameters, which hinder the wide adaptation of those models due\nto their extremely large compute and memory requirements. To resolve the issue,\nvarious model compression methods are being actively investigated. In this\nwork, we propose SDQ (Sparse Decomposed Quantization) to exploit both\nstructured sparsity and quantization to achieve both high compute and memory\nefficiency. From our evaluations, we observe that SDQ can achieve 4x effective\ncompute throughput with <1% quality drop.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2406.13868v1",
    "published_date": "2024-06-19 22:12:51 UTC",
    "updated_date": "2024-06-19 22:12:51 UTC"
  },
  {
    "arxiv_id": "2406.13862v1",
    "title": "Knowledge Graph-Enhanced Large Language Models via Path Selection",
    "authors": [
      "Haochen Liu",
      "Song Wang",
      "Yaochen Zhu",
      "Yushun Dong",
      "Jundong Li"
    ],
    "abstract": "Large Language Models (LLMs) have shown unprecedented performance in various\nreal-world applications. However, they are known to generate factually\ninaccurate outputs, a.k.a. the hallucination problem. In recent years,\nincorporating external knowledge extracted from Knowledge Graphs (KGs) has\nbecome a promising strategy to improve the factual accuracy of LLM-generated\noutputs. Nevertheless, most existing explorations rely on LLMs themselves to\nperform KG knowledge extraction, which is highly inflexible as LLMs can only\nprovide binary judgment on whether a certain knowledge (e.g., a knowledge path\nin KG) should be used. In addition, LLMs tend to pick only knowledge with\ndirect semantic relationship with the input text, while potentially useful\nknowledge with indirect semantics can be ignored. In this work, we propose a\nprincipled framework KELP with three stages to handle the above problems.\nSpecifically, KELP is able to achieve finer granularity of flexible knowledge\nextraction by generating scores for knowledge paths with input texts via latent\nsemantic matching. Meanwhile, knowledge paths with indirect semantic\nrelationships with the input text can also be considered via trained encoding\nbetween the selected paths in KG and the input text. Experiments on real-world\ndatasets validate the effectiveness of KELP.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13862v1",
    "published_date": "2024-06-19 21:45:20 UTC",
    "updated_date": "2024-06-19 21:45:20 UTC"
  },
  {
    "arxiv_id": "2406.13851v1",
    "title": "Optimizing Quantile-based Trading Strategies in Electricity Arbitrage",
    "authors": [
      "Ciaran O'Connor",
      "Joseph Collins",
      "Steven Prestwich",
      "Andrea Visentin"
    ],
    "abstract": "Efficiently integrating renewable resources into electricity markets is vital\nfor addressing the challenges of matching real-time supply and demand while\nreducing the significant energy wastage resulting from curtailments. To address\nthis challenge effectively, the incorporation of storage devices can enhance\nthe reliability and efficiency of the grid, improving market liquidity and\nreducing price volatility. In short-term electricity markets, participants\nnavigate numerous options, each presenting unique challenges and opportunities,\nunderscoring the critical role of the trading strategy in maximizing profits.\nThis study delves into the optimization of day-ahead and balancing market\ntrading, leveraging quantile-based forecasts. Employing three trading\napproaches with practical constraints, our research enhances forecast\nassessment, increases trading frequency, and employs flexible timestamp orders.\nOur findings underscore the profit potential of simultaneous participation in\nboth day-ahead and balancing markets, especially with larger battery storage\nsystems; despite increased costs and narrower profit margins associated with\nhigher-volume trading, the implementation of high-frequency strategies plays a\nsignificant role in maximizing profits and addressing market challenges.\nFinally, we modelled four commercial battery storage systems and evaluated\ntheir economic viability through a scenario analysis, with larger batteries\nshowing a shorter return on investment.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13851v1",
    "published_date": "2024-06-19 21:27:12 UTC",
    "updated_date": "2024-06-19 21:27:12 UTC"
  },
  {
    "arxiv_id": "2406.13844v3",
    "title": "A large-scale multicenter breast cancer DCE-MRI benchmark dataset with expert segmentations",
    "authors": [
      "Lidia Garrucho",
      "Kaisar Kushibar",
      "Claire-Anne Reidel",
      "Smriti Joshi",
      "Richard Osuala",
      "Apostolia Tsirikoglou",
      "Maciej Bobowicz",
      "Javier del Riego",
      "Alessandro Catanese",
      "Katarzyna Gwoździewicz",
      "Maria-Laura Cosaka",
      "Pasant M. Abo-Elhoda",
      "Sara W. Tantawy",
      "Shorouq S. Sakrana",
      "Norhan O. Shawky-Abdelfatah",
      "Amr Muhammad Abdo-Salem",
      "Androniki Kozana",
      "Eugen Divjak",
      "Gordana Ivanac",
      "Katerina Nikiforaki",
      "Michail E. Klontzas",
      "Rosa García-Dosdá",
      "Meltem Gulsun-Akpinar",
      "Oğuz Lafcı",
      "Ritse Mann",
      "Carlos Martín-Isla",
      "Fred Prior",
      "Kostas Marias",
      "Martijn P. A. Starmans",
      "Fredrik Strand",
      "Oliver Díaz",
      "Laura Igual",
      "Karim Lekadir"
    ],
    "abstract": "Artificial Intelligence (AI) research in breast cancer Magnetic Resonance\nImaging (MRI) faces challenges due to limited expert-labeled segmentations. To\naddress this, we present a multicenter dataset of 1506 pre-treatment\nT1-weighted dynamic contrast-enhanced MRI cases, including expert annotations\nof primary tumors and non-mass-enhanced regions. The dataset integrates imaging\ndata from four collections in The Cancer Imaging Archive (TCIA), where only 163\ncases with expert segmentations were initially available. To facilitate the\nannotation process, a deep learning model was trained to produce preliminary\nsegmentations for the remaining cases. These were subsequently corrected and\nverified by 16 breast cancer experts (averaging 9 years of experience),\ncreating a fully annotated dataset. Additionally, the dataset includes 49\nharmonized clinical and demographic variables, as well as pre-trained weights\nfor a baseline nnU-Net model trained on the annotated data. This resource\naddresses a critical gap in publicly available breast cancer datasets, enabling\nthe development, validation, and benchmarking of advanced deep learning models,\nthus driving progress in breast cancer diagnostics, treatment response\nprediction, and personalized care.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.CV",
    "comment": "15 paes, 7 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2406.13844v3",
    "published_date": "2024-06-19 21:11:46 UTC",
    "updated_date": "2025-02-21 11:20:47 UTC"
  },
  {
    "arxiv_id": "2406.13843v2",
    "title": "Generative AI Misuse: A Taxonomy of Tactics and Insights from Real-World Data",
    "authors": [
      "Nahema Marchal",
      "Rachel Xu",
      "Rasmi Elasmar",
      "Iason Gabriel",
      "Beth Goldberg",
      "William Isaac"
    ],
    "abstract": "Generative, multimodal artificial intelligence (GenAI) offers transformative\npotential across industries, but its misuse poses significant risks. Prior\nresearch has shed light on the potential of advanced AI systems to be exploited\nfor malicious purposes. However, we still lack a concrete understanding of how\nGenAI models are specifically exploited or abused in practice, including the\ntactics employed to inflict harm. In this paper, we present a taxonomy of GenAI\nmisuse tactics, informed by existing academic literature and a qualitative\nanalysis of approximately 200 observed incidents of misuse reported between\nJanuary 2023 and March 2024. Through this analysis, we illuminate key and novel\npatterns in misuse during this time period, including potential motivations,\nstrategies, and how attackers leverage and abuse system capabilities across\nmodalities (e.g. image, text, audio, video) in the wild.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13843v2",
    "published_date": "2024-06-19 21:11:17 UTC",
    "updated_date": "2024-06-21 10:27:11 UTC"
  },
  {
    "arxiv_id": "2406.13840v1",
    "title": "StackRAG Agent: Improving Developer Answers with Retrieval-Augmented Generation",
    "authors": [
      "Davit Abrahamyan",
      "Fatemeh H. Fard"
    ],
    "abstract": "Developers spend much time finding information that is relevant to their\nquestions. Stack Overflow has been the leading resource, and with the advent of\nLarge Language Models (LLMs), generative models such as ChatGPT are used\nfrequently. However, there is a catch in using each one separately. Searching\nfor answers is time-consuming and tedious, as shown by the many tools developed\nby researchers to address this issue. On the other, using LLMs is not reliable,\nas they might produce irrelevant or unreliable answers (i.e., hallucination).\nIn this work, we present StackRAG, a retrieval-augmented Multiagent generation\ntool based on LLMs that combines the two worlds: aggregating the knowledge from\nSO to enhance the reliability of the generated answers. Initial evaluations\nshow that the generated answers are correct, accurate, relevant, and useful.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13840v1",
    "published_date": "2024-06-19 21:07:35 UTC",
    "updated_date": "2024-06-19 21:07:35 UTC"
  },
  {
    "arxiv_id": "2406.13807v2",
    "title": "AlanaVLM: A Multimodal Embodied AI Foundation Model for Egocentric Video Understanding",
    "authors": [
      "Alessandro Suglia",
      "Claudio Greco",
      "Katie Baker",
      "Jose L. Part",
      "Ioannis Papaioannou",
      "Arash Eshghi",
      "Ioannis Konstas",
      "Oliver Lemon"
    ],
    "abstract": "AI personal assistants deployed via robots or wearables require embodied\nunderstanding to collaborate with humans effectively. However, current\nVision-Language Models (VLMs) primarily focus on third-person view videos,\nneglecting the richness of egocentric perceptual experience. To address this\ngap, we propose three key contributions. First, we introduce the Egocentric\nVideo Understanding Dataset (EVUD) for training VLMs on video captioning and\nquestion answering tasks specific to egocentric videos. Second, we present\nAlanaVLM, a 7B parameter VLM trained using parameter-efficient methods on EVUD.\nFinally, we evaluate AlanaVLM's capabilities on OpenEQA, a challenging\nbenchmark for embodied video question answering. Our model achieves\nstate-of-the-art performance, outperforming open-source models including strong\nSocratic models using GPT-4 as a planner by 3.6%. Additionally, we outperform\nClaude 3 and Gemini Pro Vision 1.0 and showcase competitive results compared to\nGemini Pro 1.5 and GPT-4V, even surpassing the latter in spatial reasoning.\nThis research paves the way for building efficient VLMs that can be deployed in\nrobots or wearables, leveraging embodied video understanding to collaborate\nseamlessly with humans in everyday tasks, contributing to the next generation\nof Embodied AI.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Code available https://github.com/alanaai/EVUD",
    "pdf_url": "http://arxiv.org/pdf/2406.13807v2",
    "published_date": "2024-06-19 20:14:14 UTC",
    "updated_date": "2024-06-21 09:53:41 UTC"
  },
  {
    "arxiv_id": "2406.13805v1",
    "title": "WikiContradict: A Benchmark for Evaluating LLMs on Real-World Knowledge Conflicts from Wikipedia",
    "authors": [
      "Yufang Hou",
      "Alessandra Pascale",
      "Javier Carnerero-Cano",
      "Tigran Tchrakian",
      "Radu Marinescu",
      "Elizabeth Daly",
      "Inkit Padhi",
      "Prasanna Sattigeri"
    ],
    "abstract": "Retrieval-augmented generation (RAG) has emerged as a promising solution to\nmitigate the limitations of large language models (LLMs), such as\nhallucinations and outdated information. However, it remains unclear how LLMs\nhandle knowledge conflicts arising from different augmented retrieved passages,\nespecially when these passages originate from the same source and have equal\ntrustworthiness. In this work, we conduct a comprehensive evaluation of\nLLM-generated answers to questions that have varying answers based on\ncontradictory passages from Wikipedia, a dataset widely regarded as a\nhigh-quality pre-training resource for most LLMs. Specifically, we introduce\nWikiContradict, a benchmark consisting of 253 high-quality, human-annotated\ninstances designed to assess LLM performance when augmented with retrieved\npassages containing real-world knowledge conflicts. We benchmark a diverse\nrange of both closed and open-source LLMs under different QA scenarios,\nincluding RAG with a single passage, and RAG with 2 contradictory passages.\nThrough rigorous human evaluations on a subset of WikiContradict instances\ninvolving 5 LLMs and over 3,500 judgements, we shed light on the behaviour and\nlimitations of these models. For instance, when provided with two passages\ncontaining contradictory facts, all models struggle to generate answers that\naccurately reflect the conflicting nature of the context, especially for\nimplicit conflicts requiring reasoning. Since human evaluation is costly, we\nalso introduce an automated model that estimates LLM performance using a strong\nopen-source language model, achieving an F-score of 0.8. Using this automated\nmetric, we evaluate more than 1,500 answers from seven LLMs across all\nWikiContradict instances. To facilitate future work, we release WikiContradict\non: https://ibm.biz/wikicontradict.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13805v1",
    "published_date": "2024-06-19 20:13:42 UTC",
    "updated_date": "2024-06-19 20:13:42 UTC"
  },
  {
    "arxiv_id": "2406.13791v3",
    "title": "IoT-Based Preventive Mental Health Using Knowledge Graphs and Standards for Better Well-Being",
    "authors": [
      "Amelie Gyrard",
      "Seyedali Mohammadi",
      "Manas Gaur",
      "Antonio Kung"
    ],
    "abstract": "Sustainable Development Goals (SDGs) give the UN a road map for development\nwith Agenda 2030 as a target. SDG3 \"Good Health and Well-Being\" ensures healthy\nlives and promotes well-being for all ages. Digital technologies can support\nSDG3. Burnout and even depression could be reduced by encouraging better\npreventive health. Due to the lack of patient knowledge and focus to take care\nof their health, it is necessary to help patients before it is too late. New\ntrends such as positive psychology and mindfulness are highly encouraged in the\nUSA. Digital Twins (DTs) can help with the continuous monitoring of emotion\nusing physiological signals (e.g., collected via wearables). DTs facilitate\nmonitoring and provide constant health insight to improve quality of life and\nwell-being with better personalization. Healthcare DTs challenges are\nstandardizing data formats, communication protocols, and data exchange\nmechanisms. As an example, ISO has the ISO/IEC JTC 1/SC 41 Internet of Things\n(IoT) and DTs Working Group, with standards such as \"ISO/IEC 21823-3:2021 IoT -\nInteroperability for IoT Systems - Part 3 Semantic interoperability\", \"ISO/IEC\nCD 30178 - IoT - Data format, value and coding\". To achieve those data\nintegration and knowledge challenges, we designed the Mental Health Knowledge\nGraph (ontology and dataset) to boost mental health. As an example, explicit\nknowledge is described such as chocolate contains magnesium which is\nrecommended for depression. The Knowledge Graph (KG) acquires knowledge from\nontology-based mental health projects classified within the LOV4IoT ontology\ncatalog (Emotion, Depression, and Mental Health). Furthermore, the KG is mapped\nto standards when possible. Standards from ETSI SmartM2M can be used such as\nSAREF4EHAW to represent medical devices and sensors, but also ITU/WHO, ISO,\nW3C, NIST, and IEEE standards relevant to mental health can be considered.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "20 pages, Book chapter, Smart Technologies for Achieving Good Health\n  and Well-Being: Towards Sustainable Development Goal, Taylor & Francis",
    "pdf_url": "http://arxiv.org/pdf/2406.13791v3",
    "published_date": "2024-06-19 19:35:14 UTC",
    "updated_date": "2024-10-21 16:55:31 UTC"
  },
  {
    "arxiv_id": "2406.13781v1",
    "title": "A Primal-Dual Framework for Transformers and Neural Networks",
    "authors": [
      "Tan M. Nguyen",
      "Tam Nguyen",
      "Nhat Ho",
      "Andrea L. Bertozzi",
      "Richard G. Baraniuk",
      "Stanley J. Osher"
    ],
    "abstract": "Self-attention is key to the remarkable success of transformers in sequence\nmodeling tasks including many applications in natural language processing and\ncomputer vision. Like neural network layers, these attention mechanisms are\noften developed by heuristics and experience. To provide a principled framework\nfor constructing attention layers in transformers, we show that the\nself-attention corresponds to the support vector expansion derived from a\nsupport vector regression problem, whose primal formulation has the form of a\nneural network layer. Using our framework, we derive popular attention layers\nused in practice and propose two new attentions: 1) the Batch Normalized\nAttention (Attention-BN) derived from the batch normalization layer and 2) the\nAttention with Scaled Head (Attention-SH) derived from using less training data\nto fit the SVR model. We empirically demonstrate the advantages of the\nAttention-BN and Attention-SH in reducing head redundancy, increasing the\nmodel's accuracy, and improving the model's efficiency in a variety of\npractical applications including image and time-series classification.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to ICLR 2023, 26 pages, 4 figures, 14 tables",
    "pdf_url": "http://arxiv.org/pdf/2406.13781v1",
    "published_date": "2024-06-19 19:11:22 UTC",
    "updated_date": "2024-06-19 19:11:22 UTC"
  },
  {
    "arxiv_id": "2407.09519v1",
    "title": "Putting GPT-4o to the Sword: A Comprehensive Evaluation of Language, Vision, Speech, and Multimodal Proficiency",
    "authors": [
      "Sakib Shahriar",
      "Brady Lund",
      "Nishith Reddy Mannuru",
      "Muhammad Arbab Arshad",
      "Kadhim Hayawi",
      "Ravi Varma Kumar Bevara",
      "Aashrith Mannuru",
      "Laiba Batool"
    ],
    "abstract": "As large language models (LLMs) continue to advance, evaluating their\ncomprehensive capabilities becomes significant for their application in various\nfields. This research study comprehensively evaluates the language, vision,\nspeech, and multimodal capabilities of GPT-4o. The study employs standardized\nexam questions, reasoning tasks, and translation assessments to assess the\nmodel's language capability. Additionally, GPT-4o's vision and speech\ncapabilities are tested through image classification and object recognition\ntasks, as well as accent classification. The multimodal evaluation assesses the\nmodel's performance in integrating visual and linguistic data. Our findings\nreveal that GPT-4o demonstrates high accuracy and efficiency across multiple\ndomains in language and reasoning capabilities, excelling in tasks that require\nfew-shot learning. GPT-4o also provides notable improvements in multimodal\ntasks compared to its predecessors. However, the model shows variability and\nfaces limitations in handling complex and ambiguous inputs, particularly in\naudio and vision capabilities. This paper highlights the need for more\ncomprehensive benchmarks and robust evaluation frameworks, encompassing\nqualitative assessments involving human judgment as well as error analysis.\nFuture work should focus on expanding datasets, investigating prompt-based\nassessment, and enhancing few-shot learning techniques to test the model's\npractical applicability and performance in real-world scenarios.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09519v1",
    "published_date": "2024-06-19 19:00:21 UTC",
    "updated_date": "2024-06-19 19:00:21 UTC"
  },
  {
    "arxiv_id": "2406.13770v2",
    "title": "Elliptical Attention",
    "authors": [
      "Stefan K. Nielsen",
      "Laziz U. Abdullaev",
      "Rachel S. Y. Teo",
      "Tan M. Nguyen"
    ],
    "abstract": "Pairwise dot-product self-attention is key to the success of transformers\nthat achieve state-of-the-art performance across a variety of applications in\nlanguage and vision. This dot-product self-attention computes attention weights\namong the input tokens using Euclidean distance, which makes the model prone to\nrepresentation collapse and vulnerable to contaminated samples. In this paper,\nwe propose using a Mahalanobis distance metric for computing the attention\nweights to stretch the underlying feature space in directions of high\ncontextual relevance. In particular, we define a hyper-ellipsoidal neighborhood\naround each query to increase the attention weights of the tokens lying in the\ncontextually important directions. We term this novel class of attention\nElliptical Attention. Our Elliptical Attention provides two benefits: 1)\nreducing representation collapse and 2) enhancing the model's robustness as\nElliptical Attention pays more attention to contextually relevant information\nrather than focusing on some small subset of informative features. We\nempirically demonstrate the advantages of Elliptical Attention over the\nbaseline dot-product attention and state-of-the-art attention methods on\nvarious practical tasks, including object classification, image segmentation,\nand language modeling across different data modalities.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages in the main text. Published at NeurIPS 2024. The code is\n  available at https://github.com/stefvk/Elliptical-Attention",
    "pdf_url": "http://arxiv.org/pdf/2406.13770v2",
    "published_date": "2024-06-19 18:38:11 UTC",
    "updated_date": "2024-10-31 21:21:26 UTC"
  },
  {
    "arxiv_id": "2406.13768v1",
    "title": "FastPersist: Accelerating Model Checkpointing in Deep Learning",
    "authors": [
      "Guanhua Wang",
      "Olatunji Ruwase",
      "Bing Xie",
      "Yuxiong He"
    ],
    "abstract": "Model checkpoints are critical Deep Learning (DL) artifacts that enable fault\ntolerance for training and downstream applications, such as inference. However,\nwriting checkpoints to persistent storage, and other I/O aspects of DL\ntraining, are mostly ignored by compute-focused optimization efforts for faster\ntraining of rapidly growing models and datasets. Towards addressing this\nimbalance, we propose FastPersist to accelerate checkpoint creation in DL\ntraining. FastPersist combines three novel techniques: (i) NVMe optimizations\nfor faster checkpoint writes to SSDs, (ii) efficient write parallelism using\nthe available SSDs in training environments, and (iii) overlapping\ncheckpointing with independent training computations. Our evaluation using real\nworld dense and sparse DL models shows that FastPersist creates checkpoints in\npersistent storage up to 116x faster than baseline, and enables per-iteration\ncheckpointing with negligible overhead.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG",
      "cs.PF"
    ],
    "primary_category": "cs.DC",
    "comment": "11 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.13768v1",
    "published_date": "2024-06-19 18:31:23 UTC",
    "updated_date": "2024-06-19 18:31:23 UTC"
  },
  {
    "arxiv_id": "2406.13763v1",
    "title": "Through the Theory of Mind's Eye: Reading Minds with Multimodal Video Large Language Models",
    "authors": [
      "Zhawnen Chen",
      "Tianchun Wang",
      "Yizhou Wang",
      "Michal Kosinski",
      "Xiang Zhang",
      "Yun Fu",
      "Sheng Li"
    ],
    "abstract": "Can large multimodal models have a human-like ability for emotional and\nsocial reasoning, and if so, how does it work? Recent research has discovered\nemergent theory-of-mind (ToM) reasoning capabilities in large language models\n(LLMs). LLMs can reason about people's mental states by solving various\ntext-based ToM tasks that ask questions about the actors' ToM (e.g., human\nbelief, desire, intention). However, human reasoning in the wild is often\ngrounded in dynamic scenes across time. Thus, we consider videos a new medium\nfor examining spatio-temporal ToM reasoning ability. Specifically, we ask\nexplicit probing questions about videos with abundant social and emotional\nreasoning content. We develop a pipeline for multimodal LLM for ToM reasoning\nusing video and text. We also enable explicit ToM reasoning by retrieving key\nframes for answering a ToM question, which reveals how multimodal LLMs reason\nabout ToM.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13763v1",
    "published_date": "2024-06-19 18:24:31 UTC",
    "updated_date": "2024-06-19 18:24:31 UTC"
  },
  {
    "arxiv_id": "2406.13762v2",
    "title": "Unveiling the Hidden Structure of Self-Attention via Kernel Principal Component Analysis",
    "authors": [
      "Rachel S. Y. Teo",
      "Tan M. Nguyen"
    ],
    "abstract": "The remarkable success of transformers in sequence modeling tasks, spanning\nvarious applications in natural language processing and computer vision, is\nattributed to the critical role of self-attention. Similar to the development\nof most deep learning models, the construction of these attention mechanisms\nrelies on heuristics and experience. In our work, we derive self-attention from\nkernel principal component analysis (kernel PCA) and show that self-attention\nprojects its query vectors onto the principal component axes of its key matrix\nin a feature space. We then formulate the exact formula for the value matrix in\nself-attention, theoretically and empirically demonstrating that this value\nmatrix captures the eigenvectors of the Gram matrix of the key vectors in\nself-attention. Leveraging our kernel PCA framework, we propose Attention with\nRobust Principal Components (RPC-Attention), a novel class of robust attention\nthat is resilient to data contamination. We empirically demonstrate the\nadvantages of RPC-Attention over softmax attention on the ImageNet-1K object\nclassification, WikiText-103 language modeling, and ADE20K image segmentation\ntask.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages in the main text. Published at NeurIPS 2024. The code is\n  available at https://github.com/rachtsy/KPCA_code",
    "pdf_url": "http://arxiv.org/pdf/2406.13762v2",
    "published_date": "2024-06-19 18:22:32 UTC",
    "updated_date": "2024-10-30 20:40:04 UTC"
  },
  {
    "arxiv_id": "2406.13743v3",
    "title": "GenAI-Bench: Evaluating and Improving Compositional Text-to-Visual Generation",
    "authors": [
      "Baiqi Li",
      "Zhiqiu Lin",
      "Deepak Pathak",
      "Jiayao Li",
      "Yixin Fei",
      "Kewen Wu",
      "Tiffany Ling",
      "Xide Xia",
      "Pengchuan Zhang",
      "Graham Neubig",
      "Deva Ramanan"
    ],
    "abstract": "While text-to-visual models now produce photo-realistic images and videos,\nthey struggle with compositional text prompts involving attributes,\nrelationships, and higher-order reasoning such as logic and comparison. In this\nwork, we conduct an extensive human study on GenAI-Bench to evaluate the\nperformance of leading image and video generation models in various aspects of\ncompositional text-to-visual generation. We also compare automated evaluation\nmetrics against our collected human ratings and find that VQAScore -- a metric\nmeasuring the likelihood that a VQA model views an image as accurately\ndepicting the prompt -- significantly outperforms previous metrics such as\nCLIPScore. In addition, VQAScore can improve generation in a black-box manner\n(without finetuning) via simply ranking a few (3 to 9) candidate images.\nRanking by VQAScore is 2x to 3x more effective than other scoring methods like\nPickScore, HPSv2, and ImageReward at improving human alignment ratings for\nDALL-E 3 and Stable Diffusion, especially on compositional prompts that require\nadvanced visio-linguistic reasoning. We release a new GenAI-Rank benchmark with\nover 40,000 human ratings to evaluate scoring metrics on ranking images\ngenerated from the same prompt. Lastly, we discuss promising areas for\nimprovement in VQAScore, such as addressing fine-grained visual details. We\nwill release all human ratings (over 80,000) to facilitate scientific\nbenchmarking of both generative models and automated metrics.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "We open-source our dataset, model, and code at:\n  https://linzhiqiu.github.io/papers/genai_bench ; Project page:\n  https://linzhiqiu.github.io/papers/genai_bench ; GenAI-Bench was first\n  introduced in arxiv:2404.01291. This article extends it with an additional\n  GenAI-Rank benchmark",
    "pdf_url": "http://arxiv.org/pdf/2406.13743v3",
    "published_date": "2024-06-19 18:00:07 UTC",
    "updated_date": "2024-11-03 20:22:32 UTC"
  },
  {
    "arxiv_id": "2406.13733v1",
    "title": "You can't handle the (dirty) truth: Data-centric insights improve pseudo-labeling",
    "authors": [
      "Nabeel Seedat",
      "Nicolas Huynh",
      "Fergus Imrie",
      "Mihaela van der Schaar"
    ],
    "abstract": "Pseudo-labeling is a popular semi-supervised learning technique to leverage\nunlabeled data when labeled samples are scarce. The generation and selection of\npseudo-labels heavily rely on labeled data. Existing approaches implicitly\nassume that the labeled data is gold standard and 'perfect'. However, this can\nbe violated in reality with issues such as mislabeling or ambiguity. We address\nthis overlooked aspect and show the importance of investigating labeled data\nquality to improve any pseudo-labeling method. Specifically, we introduce a\nnovel data characterization and selection framework called DIPS to extend\npseudo-labeling. We select useful labeled and pseudo-labeled samples via\nanalysis of learning dynamics. We demonstrate the applicability and impact of\nDIPS for various pseudo-labeling methods across an extensive range of\nreal-world tabular and image datasets. Additionally, DIPS improves data\nefficiency and reduces the performance distinctions between different\npseudo-labelers. Overall, we highlight the significant benefits of a\ndata-centric rethinking of pseudo-labeling in real-world settings.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published in the Journal of Data-centric Machine Learning Research\n  (DMLR) *Seedat & Huynh contributed equally",
    "pdf_url": "http://arxiv.org/pdf/2406.13733v1",
    "published_date": "2024-06-19 17:58:40 UTC",
    "updated_date": "2024-06-19 17:58:40 UTC"
  },
  {
    "arxiv_id": "2406.13731v1",
    "title": "Integrating Fuzzy Logic with Causal Inference: Enhancing the Pearl and Neyman-Rubin Methodologies",
    "authors": [
      "Amir Saki",
      "Usef Faghihi"
    ],
    "abstract": "In this paper, we generalize the Pearl and Neyman-Rubin methodologies in\ncausal inference by introducing a generalized approach that incorporates fuzzy\nlogic. Indeed, we introduce a fuzzy causal inference approach that consider\nboth the vagueness and imprecision inherent in data, as well as the subjective\nhuman perspective characterized by fuzzy terms such as 'high', 'medium', and\n'low'. To do so, we introduce two fuzzy causal effect formulas: the Fuzzy\nAverage Treatment Effect (FATE) and the Generalized Fuzzy Average Treatment\nEffect (GFATE), together with their normalized versions: NFATE and NGFATE. When\ndealing with a binary treatment variable, our fuzzy causal effect formulas\ncoincide with classical Average Treatment Effect (ATE) formula, that is a\nwell-established and popular metric in causal inference. In FATE, all values of\nthe treatment variable are considered equally important. In contrast, GFATE\ntakes into account the rarity and frequency of these values. We show that for\nlinear Structural Equation Models (SEMs), the normalized versions of our\nformulas, NFATE and NGFATE, are equivalent to ATE. Further, we provide\nidentifiability criteria for these formulas and show their stability with\nrespect to minor variations in the fuzzy subsets and the probability\ndistributions involved. This ensures the robustness of our approach in handling\nsmall perturbations in the data. Finally, we provide several experimental\nexamples to empirically validate and demonstrate the practical application of\nour proposed fuzzy causal inference methods.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.LO",
      "62D20, 60A86, 03E72, 93C42, 68T37, 6008, 68T20, 68T27, 68U99",
      "I.2.3; G.3"
    ],
    "primary_category": "cs.AI",
    "comment": "25 pages, 6 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2406.13731v1",
    "published_date": "2024-06-19 17:54:31 UTC",
    "updated_date": "2024-06-19 17:54:31 UTC"
  },
  {
    "arxiv_id": "2406.13725v2",
    "title": "Tree-Sliced Wasserstein Distance: A Geometric Perspective",
    "authors": [
      "Viet-Hoang Tran",
      "Trang Pham",
      "Tho Tran",
      "Minh Khoi Nguyen Nhat",
      "Thanh Chu",
      "Tam Le",
      "Tan M. Nguyen"
    ],
    "abstract": "Many variants of Optimal Transport (OT) have been developed to address its\nheavy computation. Among them, notably, Sliced Wasserstein (SW) is widely used\nfor application domains by projecting the OT problem onto one-dimensional\nlines, and leveraging the closed-form expression of the univariate OT to reduce\nthe computational burden. However, projecting measures onto low-dimensional\nspaces can lead to a loss of topological information. To mitigate this issue,\nin this work, we propose to replace one-dimensional lines with a more intricate\nstructure, called tree systems. This structure is metrizable by a tree metric,\nwhich yields a closed-form expression for OT problems on tree systems. We\nprovide an extensive theoretical analysis to formally define tree systems with\ntheir topological properties, introduce the concept of splitting maps, which\noperate as the projection mechanism onto these structures, then finally propose\na novel variant of Radon transform for tree systems and verify its injectivity.\nThis framework leads to an efficient metric between measures, termed\nTree-Sliced Wasserstein distance on Systems of Lines (TSW-SL). By conducting a\nvariety of experiments on gradient flows, image style transfer, and generative\nmodels, we illustrate that our proposed approach performs favorably compared to\nSW and its variants.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to ICML 2025",
    "pdf_url": "http://arxiv.org/pdf/2406.13725v2",
    "published_date": "2024-06-19 17:40:11 UTC",
    "updated_date": "2025-05-02 04:45:34 UTC"
  },
  {
    "arxiv_id": "2406.13724v1",
    "title": "Heterogeneous Graph Neural Networks with Post-hoc Explanations for Multi-modal and Explainable Land Use Inference",
    "authors": [
      "Xuehao Zhai",
      "Junqi Jiang",
      "Adam Dejl",
      "Antonio Rago",
      "Fangce Guo",
      "Francesca Toni",
      "Aruna Sivakumar"
    ],
    "abstract": "Urban land use inference is a critically important task that aids in city\nplanning and policy-making. Recently, the increased use of sensor and location\ntechnologies has facilitated the collection of multi-modal mobility data,\noffering valuable insights into daily activity patterns. Many studies have\nadopted advanced data-driven techniques to explore the potential of these\nmulti-modal mobility data in land use inference. However, existing studies\noften process samples independently, ignoring the spatial correlations among\nneighbouring objects and heterogeneity among different services. Furthermore,\nthe inherently low interpretability of complex deep learning methods poses a\nsignificant barrier in urban planning, where transparency and extrapolability\nare crucial for making long-term policy decisions. To overcome these\nchallenges, we introduce an explainable framework for inferring land use that\nsynergises heterogeneous graph neural networks (HGNs) with Explainable AI\ntechniques, enhancing both accuracy and explainability. The empirical\nexperiments demonstrate that the proposed HGNs significantly outperform\nbaseline graph neural networks for all six land-use indicators, especially in\nterms of 'office' and 'sustenance'. As explanations, we consider feature\nattribution and counterfactual explanations. The analysis of feature\nattribution explanations shows that the symmetrical nature of the `residence'\nand 'work' categories predicted by the framework aligns well with the\ncommuter's 'work' and 'recreation' activities in London. The analysis of the\ncounterfactual explanations reveals that variations in node features and types\nare primarily responsible for the differences observed between the predicted\nland use distribution and the ideal mixed state. These analyses demonstrate\nthat the proposed HGNs can suitably support urban stakeholders in their urban\nplanning and policy-making.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13724v1",
    "published_date": "2024-06-19 17:39:10 UTC",
    "updated_date": "2024-06-19 17:39:10 UTC"
  },
  {
    "arxiv_id": "2406.13715v1",
    "title": "Converging Dimensions: Information Extraction and Summarization through Multisource, Multimodal, and Multilingual Fusion",
    "authors": [
      "Pranav Janjani",
      "Mayank Palan",
      "Sarvesh Shirude",
      "Ninad Shegokar",
      "Sunny Kumar",
      "Faruk Kazi"
    ],
    "abstract": "Recent advances in large language models (LLMs) have led to new summarization\nstrategies, offering an extensive toolkit for extracting important information.\nHowever, these approaches are frequently limited by their reliance on isolated\nsources of data. The amount of information that can be gathered is limited and\ncovers a smaller range of themes, which introduces the possibility of falsified\ncontent and limited support for multilingual and multimodal data. The paper\nproposes a novel approach to summarization that tackles such challenges by\nutilizing the strength of multiple sources to deliver a more exhaustive and\ninformative understanding of intricate topics. The research progresses beyond\nconventional, unimodal sources such as text documents and integrates a more\ndiverse range of data, including YouTube playlists, pre-prints, and Wikipedia\npages. The aforementioned varied sources are then converted into a unified\ntextual representation, enabling a more holistic analysis. This multifaceted\napproach to summary generation empowers us to extract pertinent information\nfrom a wider array of sources. The primary tenet of this approach is to\nmaximize information gain while minimizing information overlap and maintaining\na high level of informativeness, which encourages the generation of highly\ncoherent summaries.",
    "categories": [
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "11 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.13715v1",
    "published_date": "2024-06-19 17:15:47 UTC",
    "updated_date": "2024-06-19 17:15:47 UTC"
  },
  {
    "arxiv_id": "2406.13714v1",
    "title": "BEACON: Balancing Convenience and Nutrition in Meals With Long-Term Group Recommendations and Reasoning on Multimodal Recipes",
    "authors": [
      "Vansh Nagpal",
      "Siva Likitha Valluru",
      "Kausik Lakkaraju",
      "Biplav Srivastava"
    ],
    "abstract": "A common, yet regular, decision made by people, whether healthy or with any\nhealth condition, is to decide what to have in meals like breakfast, lunch, and\ndinner, consisting of a combination of foods for appetizer, main course, side\ndishes, desserts, and beverages. However, often this decision is seen as a\ntrade-off between nutritious choices (e.g., low salt and sugar) or convenience\n(e.g., inexpensive, fast to prepare/obtain, taste better). In this preliminary\nwork, we present a data-driven approach for the novel meal recommendation\nproblem that can explore and balance choices for both considerations while also\nreasoning about a food's constituents and cooking process. Beyond the problem\nformulation, our contributions also include a goodness measure, a recipe\nconversion method from text to the recently introduced multimodal rich recipe\nrepresentation (R3) format, and learning methods using contextual bandits that\nshow promising results.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "6 pages (including references), 1 figure, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2406.13714v1",
    "published_date": "2024-06-19 17:14:41 UTC",
    "updated_date": "2024-06-19 17:14:41 UTC"
  },
  {
    "arxiv_id": "2406.13711v1",
    "title": "Imagining In-distribution States: How Predictable Robot Behavior Can Enable User Control Over Learned Policies",
    "authors": [
      "Isaac Sheidlower",
      "Emma Bethel",
      "Douglas Lilly",
      "Reuben M. Aronson",
      "Elaine Schaertl Short"
    ],
    "abstract": "It is crucial that users are empowered to take advantage of the functionality\nof a robot and use their understanding of that functionality to perform novel\nand creative tasks. Given a robot trained with Reinforcement Learning (RL), a\nuser may wish to leverage that autonomy along with their familiarity of how\nthey expect the robot to behave to collaborate with the robot. One technique is\nfor the user to take control of some of the robot's action space through\nteleoperation, allowing the RL policy to simultaneously control the rest. We\nformalize this type of shared control as Partitioned Control (PC). However,\nthis may not be possible using an out-of-the-box RL policy. For example, a\nuser's control may bring the robot into a failure state from the policy's\nperspective, causing it to act unexpectedly and hindering the success of the\nuser's desired task. In this work, we formalize this problem and present\nImaginary Out-of-Distribution Actions, IODA, an initial algorithm which\nempowers users to leverage their expectations of a robot's behavior to\naccomplish new tasks. We deploy IODA in a user study with a real robot and find\nthat IODA leads to both better task performance and a higher degree of\nalignment between robot behavior and user expectation. We also show that in PC,\nthere is a strong and significant correlation between task performance and the\nrobot's ability to meet user expectations, highlighting the need for approaches\nlike IODA. Code is available at https://github.com/AABL-Lab/ioda_roman_2024",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted to IEEE RO-MAN 2024 as a regular paper. arXiv admin note:\n  substantial text overlap with arXiv:2312.05991",
    "pdf_url": "http://arxiv.org/pdf/2406.13711v1",
    "published_date": "2024-06-19 17:08:28 UTC",
    "updated_date": "2024-06-19 17:08:28 UTC"
  },
  {
    "arxiv_id": "2406.13706v2",
    "title": "Developing Story: Case Studies of Generative AI's Use in Journalism",
    "authors": [
      "Natalie Grace Brigham",
      "Chongjiu Gao",
      "Tadayoshi Kohno",
      "Franziska Roesner",
      "Niloofar Mireshghallah"
    ],
    "abstract": "Journalists are among the many users of large language models (LLMs). To\nbetter understand the journalist-AI interactions, we conduct a study of LLM\nusage by two news agencies through browsing the WildChat dataset, identifying\ncandidate interactions, and verifying them by matching to online published\narticles. Our analysis uncovers instances where journalists provide sensitive\nmaterial such as confidential correspondence with sources or articles from\nother agencies to the LLM as stimuli and prompt it to generate articles, and\npublish these machine-generated articles with limited intervention (median\noutput-publication ROUGE-L of 0.62). Based on our findings, we call for further\nresearch into what constitutes responsible use of AI, and the establishment of\nclear guidelines and best practices on using LLMs in a journalistic context.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13706v2",
    "published_date": "2024-06-19 16:58:32 UTC",
    "updated_date": "2024-12-03 04:57:32 UTC"
  },
  {
    "arxiv_id": "2406.13705v2",
    "title": "EndoUIC: Promptable Diffusion Transformer for Unified Illumination Correction in Capsule Endoscopy",
    "authors": [
      "Long Bai",
      "Tong Chen",
      "Qiaozhi Tan",
      "Wan Jun Nah",
      "Yanheng Li",
      "Zhicheng He",
      "Sishen Yuan",
      "Zhen Chen",
      "Jinlin Wu",
      "Mobarakol Islam",
      "Zhen Li",
      "Hongbin Liu",
      "Hongliang Ren"
    ],
    "abstract": "Wireless Capsule Endoscopy (WCE) is highly valued for its non-invasive and\npainless approach, though its effectiveness is compromised by uneven\nillumination from hardware constraints and complex internal dynamics, leading\nto overexposed or underexposed images. While researchers have discussed the\nchallenges of low-light enhancement in WCE, the issue of correcting for\ndifferent exposure levels remains underexplored. To tackle this, we introduce\nEndoUIC, a WCE unified illumination correction solution using an end-to-end\npromptable diffusion transformer (DiT) model. In our work, the illumination\nprompt module shall navigate the model to adapt to different exposure levels\nand perform targeted image enhancement, in which the Adaptive Prompt\nIntegration (API) and Global Prompt Scanner (GPS) modules shall further boost\nthe concurrent representation learning between the prompt parameters and\nfeatures. Besides, the U-shaped restoration DiT model shall capture the\nlong-range dependencies and contextual information for unified illumination\nrestoration. Moreover, we present a novel Capsule-endoscopy Exposure Correction\n(CEC) dataset, including ground-truth and corrupted image pairs annotated by\nexpert photographers. Extensive experiments against a variety of\nstate-of-the-art (SOTA) methods on four datasets showcase the effectiveness of\nour proposed method and components in WCE illumination restoration, and the\nadditional downstream experiments further demonstrate its utility for clinical\ndiagnosis and surgical assistance.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "To appear in MICCAI 2024. Code and dataset availability:\n  https://github.com/longbai1006/EndoUIC",
    "pdf_url": "http://arxiv.org/pdf/2406.13705v2",
    "published_date": "2024-06-19 16:58:28 UTC",
    "updated_date": "2024-07-08 15:51:29 UTC"
  },
  {
    "arxiv_id": "2406.13695v1",
    "title": "Multilingual De-Duplication Strategies: Applying scalable similarity search with monolingual & multilingual embedding models",
    "authors": [
      "Stefan Pasch",
      "Dimitirios Petridis",
      "Jannic Cutura"
    ],
    "abstract": "This paper addresses the deduplication of multilingual textual data using\nadvanced NLP tools. We compare a two-step method involving translation to\nEnglish followed by embedding with mpnet, and a multilingual embedding model\n(distiluse). The two-step approach achieved a higher F1 score (82% vs. 60%),\nparticularly with less widely used languages, which can be increased up to 89%\nby leveraging expert rules based on domain knowledge. We also highlight\nlimitations related to token length constraints and computational efficiency.\nOur methodology suggests improvements for future multilingual deduplication\ntasks.",
    "categories": [
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.AI",
    "comment": "6 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.13695v1",
    "published_date": "2024-06-19 16:48:14 UTC",
    "updated_date": "2024-06-19 16:48:14 UTC"
  },
  {
    "arxiv_id": "2407.00071v1",
    "title": "Combinatorial Reasoning: Selecting Reasons in Generative AI Pipelines via Combinatorial Optimization",
    "authors": [
      "Mert Esencan",
      "Tarun Advaith Kumar",
      "Ata Akbari Asanjan",
      "P. Aaron Lott",
      "Masoud Mohseni",
      "Can Unlu",
      "Davide Venturelli",
      "Alan Ho"
    ],
    "abstract": "Recent Large Language Models (LLMs) have demonstrated impressive capabilities\nat tasks that require human intelligence and are a significant step towards\nhuman-like artificial intelligence (AI). Yet the performance of LLMs at\nreasoning tasks have been subpar and the reasoning capability of LLMs is a\nmatter of significant debate. While it has been shown that the choice of the\nprompting technique to the LLM can alter its performance on a multitude of\ntasks, including reasoning, the best performing techniques require human-made\nprompts with the knowledge of the tasks at hand. We introduce a framework for\nwhat we call Combinatorial Reasoning (CR), a fully-automated prompting method,\nwhere reasons are sampled from an LLM pipeline and mapped into a Quadratic\nUnconstrained Binary Optimization (QUBO) problem. The framework investigates\nwhether QUBO solutions can be profitably used to select a useful subset of the\nreasons to construct a Chain-of-Thought style prompt. We explore the\nacceleration of CR with specialized solvers. We also investigate the\nperformance of simpler zero-shot strategies such as linear majority rule or\nrandom selection of reasons. Our preliminary study indicates that coupling a\ncombinatorial solver to generative AI pipelines is an interesting avenue for AI\nreasoning and elucidates design principles for future CR methods.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.ET",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "13 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.00071v1",
    "published_date": "2024-06-19 16:47:44 UTC",
    "updated_date": "2024-06-19 16:47:44 UTC"
  },
  {
    "arxiv_id": "2406.13694v1",
    "title": "An Embedded Intelligent System for Attendance Monitoring",
    "authors": [
      "Touzene Abderraouf",
      "Abed Abdeljalil Wassim",
      "Slimane Larabi"
    ],
    "abstract": "In this paper, we propose an intelligent embedded system for monitoring class\nattendance and sending the attendance list to a remote computer. The proposed\nsystem consists of two parts : an embedded device (Raspberry with PI camera)\nfor facial recognition and a web application for attendance management. The\nproposed solution take into account the different challenges: the limited\nresources of the Raspberry Pi, the need to adapt the facial recognition model\nand achieving acceptable performance using images provided by the Raspberry Pi\ncamera.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "18 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.13694v1",
    "published_date": "2024-06-19 16:46:19 UTC",
    "updated_date": "2024-06-19 16:46:19 UTC"
  },
  {
    "arxiv_id": "2406.13693v1",
    "title": "From Single Agent to Multi-Agent: Improving Traffic Signal Control",
    "authors": [
      "Maksim Tislenko",
      "Dmitrii Kisilev"
    ],
    "abstract": "Due to accelerating urbanization, the importance of solving the signal\ncontrol problem increases. This paper analyzes various existing methods and\nsuggests options for increasing the number of agents to reduce the average\ntravel time. Experiments were carried out with 2 datasets. The results show\nthat in some cases, the implementation of multiple agents can improve existing\nmethods. For a fine-tuned large language model approach there is small\nenhancement on all metrics.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "13 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.13693v1",
    "published_date": "2024-06-19 16:46:15 UTC",
    "updated_date": "2024-06-19 16:46:15 UTC"
  },
  {
    "arxiv_id": "2406.13688v1",
    "title": "Development of a Dual-Input Neural Model for Detecting AI-Generated Imagery",
    "authors": [
      "Jonathan Gallagher",
      "William Pugsley"
    ],
    "abstract": "Over the past years, images generated by artificial intelligence have become\nmore prevalent and more realistic. Their advent raises ethical questions\nrelating to misinformation, artistic expression, and identity theft, among\nothers. The crux of many of these moral questions is the difficulty in\ndistinguishing between real and fake images. It is important to develop tools\nthat are able to detect AI-generated images, especially when these images are\ntoo realistic-looking for the human eye to identify as fake. This paper\nproposes a dual-branch neural network architecture that takes both images and\ntheir Fourier frequency decomposition as inputs. We use standard CNN-based\nmethods for both branches as described in Stuchi et al. [7], followed by\nfully-connected layers. Our proposed model achieves an accuracy of 94% on the\nCIFAKE dataset, which significantly outperforms classic ML methods and CNNs,\nachieving performance comparable to some state-of-the-art architectures, such\nas ResNet.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13688v1",
    "published_date": "2024-06-19 16:42:04 UTC",
    "updated_date": "2024-06-19 16:42:04 UTC"
  },
  {
    "arxiv_id": "2406.13683v1",
    "title": "IntCoOp: Interpretability-Aware Vision-Language Prompt Tuning",
    "authors": [
      "Soumya Suvra Ghosal",
      "Samyadeep Basu",
      "Soheil Feizi",
      "Dinesh Manocha"
    ],
    "abstract": "Image-text contrastive models such as CLIP learn transferable and robust\nrepresentations for zero-shot transfer to a variety of downstream tasks.\nHowever, to obtain strong downstream performances, prompts need to be carefully\ncurated, which can be a tedious engineering task. To address the issue of\nmanual prompt engineering, prompt-tuning is used where a set of contextual\nvectors are learned by leveraging information from the training data. Despite\ntheir effectiveness, existing prompt-tuning frameworks often lack\ninterpretability, thus limiting their ability to understand the compositional\nnature of images. In this work, we first identify that incorporating\ncompositional attributes (e.g., a \"green\" tree frog) in the design of manual\nprompts can significantly enhance image-text alignment scores. Building upon\nthis observation, we propose a novel and interpretable prompt-tuning method\nnamed IntCoOp, which learns to jointly align attribute-level inductive biases\nand class embeddings during prompt-tuning. To assess the effectiveness of our\napproach, we evaluate IntCoOp across two representative tasks in a few-shot\nlearning setup: generalization to novel classes, and unseen domain shifts.\nThrough extensive experiments across 10 downstream datasets on CLIP, we find\nthat introducing attribute-level inductive biases leads to superior performance\nagainst state-of-the-art prompt tuning frameworks. Notably, in a 16-shot setup,\nIntCoOp improves CoOp by 7.35% in average performance across 10 diverse\ndatasets.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13683v1",
    "published_date": "2024-06-19 16:37:31 UTC",
    "updated_date": "2024-06-19 16:37:31 UTC"
  },
  {
    "arxiv_id": "2406.13664v1",
    "title": "Root-KGD: A Novel Framework for Root Cause Diagnosis Based on Knowledge Graph and Industrial Data",
    "authors": [
      "Jiyu Chen",
      "Jinchuan Qian",
      "Xinmin Zhang",
      "Zhihuan Song"
    ],
    "abstract": "With the development of intelligent manufacturing and the increasing\ncomplexity of industrial production, root cause diagnosis has gradually become\nan important research direction in the field of industrial fault diagnosis.\nHowever, existing research methods struggle to effectively combine domain\nknowledge and industrial data, failing to provide accurate, online, and\nreliable root cause diagnosis results for industrial processes. To address\nthese issues, a novel fault root cause diagnosis framework based on knowledge\ngraph and industrial data, called Root-KGD, is proposed. Root-KGD uses the\nknowledge graph to represent domain knowledge and employs data-driven modeling\nto extract fault features from industrial data. It then combines the knowledge\ngraph and data features to perform knowledge graph reasoning for root cause\nidentification. The performance of the proposed method is validated using two\nindustrial process cases, Tennessee Eastman Process (TEP) and Multiphase Flow\nFacility (MFF). Compared to existing methods, Root-KGD not only gives more\naccurate root cause variable diagnosis results but also provides interpretable\nfault-related information by locating faults to corresponding physical entities\nin knowledge graph (such as devices and streams). In addition, combined with\nits lightweight nature, Root-KGD is more effective in online industrial\napplications.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13664v1",
    "published_date": "2024-06-19 16:11:43 UTC",
    "updated_date": "2024-06-19 16:11:43 UTC"
  },
  {
    "arxiv_id": "2406.13663v4",
    "title": "Model Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation",
    "authors": [
      "Jirui Qi",
      "Gabriele Sarti",
      "Raquel Fernández",
      "Arianna Bisazza"
    ],
    "abstract": "Ensuring the verifiability of model answers is a fundamental challenge for\nretrieval-augmented generation (RAG) in the question answering (QA) domain.\nRecently, self-citation prompting was proposed to make large language models\n(LLMs) generate citations to supporting documents along with their answers.\nHowever, self-citing LLMs often struggle to match the required format, refer to\nnon-existent sources, and fail to faithfully reflect LLMs' context usage\nthroughout the generation. In this work, we present MIRAGE --Model\nInternals-based RAG Explanations -- a plug-and-play approach using model\ninternals for faithful answer attribution in RAG applications. MIRAGE detects\ncontext-sensitive answer tokens and pairs them with retrieved documents\ncontributing to their prediction via saliency methods. We evaluate our proposed\napproach on a multilingual extractive QA dataset, finding high agreement with\nhuman answer attribution. On open-ended QA, MIRAGE achieves citation quality\nand efficiency comparable to self-citation while also allowing for a\nfiner-grained control of attribution parameters. Our qualitative evaluation\nhighlights the faithfulness of MIRAGE's attributions and underscores the\npromising application of model internals for RAG answer attribution.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by EMNLP 2024 Main Conference. Code and data released at\n  https://github.com/Betswish/MIRAGE",
    "pdf_url": "http://arxiv.org/pdf/2406.13663v4",
    "published_date": "2024-06-19 16:10:26 UTC",
    "updated_date": "2024-10-18 13:16:57 UTC"
  },
  {
    "arxiv_id": "2406.13660v1",
    "title": "Towards Minimal Targeted Updates of Language Models with Targeted Negative Training",
    "authors": [
      "Lily H. Zhang",
      "Rajesh Ranganath",
      "Arya Tafvizi"
    ],
    "abstract": "Generative models of language exhibit impressive capabilities but still place\nnon-negligible probability mass over undesirable outputs. In this work, we\naddress the task of updating a model to avoid unwanted outputs while minimally\nchanging model behavior otherwise, a challenge we refer to as a minimal\ntargeted update. We first formalize the notion of a minimal targeted update and\npropose a method to achieve such updates using negative examples from a model's\ngenerations. Our proposed Targeted Negative Training (TNT) results in updates\nthat keep the new distribution close to the original, unlike existing losses\nfor negative signal which push down probability but do not control what the\nupdated distribution will be. In experiments, we demonstrate that TNT yields a\nbetter trade-off between reducing unwanted behavior and maintaining model\ngeneration behavior than baselines, paving the way towards a modeling paradigm\nbased on iterative training updates that constrain models from generating\nundesirable outputs while preserving their impressive capabilities.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Published in Transactions of Machine Learning Research",
    "pdf_url": "http://arxiv.org/pdf/2406.13660v1",
    "published_date": "2024-06-19 16:06:21 UTC",
    "updated_date": "2024-06-19 16:06:21 UTC"
  },
  {
    "arxiv_id": "2406.13659v1",
    "title": "Leveraging Large Language Models for Patient Engagement: The Power of Conversational AI in Digital Health",
    "authors": [
      "Bo Wen",
      "Raquel Norel",
      "Julia Liu",
      "Thaddeus Stappenbeck",
      "Farhana Zulkernine",
      "Huamin Chen"
    ],
    "abstract": "The rapid advancements in large language models (LLMs) have opened up new\nopportunities for transforming patient engagement in healthcare through\nconversational AI. This paper presents an overview of the current landscape of\nLLMs in healthcare, specifically focusing on their applications in analyzing\nand generating conversations for improved patient engagement. We showcase the\npower of LLMs in handling unstructured conversational data through four case\nstudies: (1) analyzing mental health discussions on Reddit, (2) developing a\npersonalized chatbot for cognitive engagement in seniors, (3) summarizing\nmedical conversation datasets, and (4) designing an AI-powered patient\nengagement system. These case studies demonstrate how LLMs can effectively\nextract insights and summarizations from unstructured dialogues and engage\npatients in guided, goal-oriented conversations. Leveraging LLMs for\nconversational analysis and generation opens new doors for many\npatient-centered outcomes research opportunities. However, integrating LLMs\ninto healthcare raises important ethical considerations regarding data privacy,\nbias, transparency, and regulatory compliance. We discuss best practices and\nguidelines for the responsible development and deployment of LLMs in healthcare\nsettings. Realizing the full potential of LLMs in digital health will require\nclose collaboration between the AI and healthcare professionals communities to\naddress technical challenges and ensure these powerful tools' safety, efficacy,\nand equity.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 6 figures, ICDH 2024 invited paper",
    "pdf_url": "http://arxiv.org/pdf/2406.13659v1",
    "published_date": "2024-06-19 16:02:04 UTC",
    "updated_date": "2024-06-19 16:02:04 UTC"
  },
  {
    "arxiv_id": "2406.13655v1",
    "title": "Improving GFlowNets with Monte Carlo Tree Search",
    "authors": [
      "Nikita Morozov",
      "Daniil Tiapkin",
      "Sergey Samsonov",
      "Alexey Naumov",
      "Dmitry Vetrov"
    ],
    "abstract": "Generative Flow Networks (GFlowNets) treat sampling from distributions over\ncompositional discrete spaces as a sequential decision-making problem, training\na stochastic policy to construct objects step by step. Recent studies have\nrevealed strong connections between GFlowNets and entropy-regularized\nreinforcement learning. Building on these insights, we propose to enhance\nplanning capabilities of GFlowNets by applying Monte Carlo Tree Search (MCTS).\nSpecifically, we show how the MENTS algorithm (Xiao et al., 2019) can be\nadapted for GFlowNets and used during both training and inference. Our\nexperiments demonstrate that this approach improves the sample efficiency of\nGFlowNet training and the generation fidelity of pre-trained GFlowNet models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2024 SPIGM Workshop",
    "pdf_url": "http://arxiv.org/pdf/2406.13655v1",
    "published_date": "2024-06-19 15:58:35 UTC",
    "updated_date": "2024-06-19 15:58:35 UTC"
  },
  {
    "arxiv_id": "2406.13652v1",
    "title": "Stability and Generalizability in SDE Diffusion Models with Measure-Preserving Dynamics",
    "authors": [
      "Weitong Zhang",
      "Chengqi Zang",
      "Liu Li",
      "Sarah Cechnicka",
      "Cheng Ouyang",
      "Bernhard Kainz"
    ],
    "abstract": "Inverse problems describe the process of estimating the causal factors from a\nset of measurements or data. Mapping of often incomplete or degraded data to\nparameters is ill-posed, thus data-driven iterative solutions are required, for\nexample when reconstructing clean images from poor signals. Diffusion models\nhave shown promise as potent generative tools for solving inverse problems due\nto their superior reconstruction quality and their compatibility with iterative\nsolvers. However, most existing approaches are limited to linear inverse\nproblems represented as Stochastic Differential Equations (SDEs). This\nsimplification falls short of addressing the challenging nature of real-world\nproblems, leading to amplified cumulative errors and biases. We provide an\nexplanation for this gap through the lens of measure-preserving dynamics of\nRandom Dynamical Systems (RDS) with which we analyse Temporal Distribution\nDiscrepancy and thus introduce a theoretical framework based on RDS for SDE\ndiffusion models. We uncover several strategies that inherently enhance the\nstability and generalizability of diffusion models for inverse problems and\nintroduce a novel score-based diffusion framework, the \\textbf{D}ynamics-aware\nS\\textbf{D}E \\textbf{D}iffusion \\textbf{G}enerative \\textbf{M}odel (D$^3$GM).\nThe \\textit{Measure-preserving property} can return the degraded measurement to\nthe original state despite complex degradation with the RDS concept of\n\\textit{stability}. Our extensive experimental results corroborate the\neffectiveness of D$^3$GM across multiple benchmarks including a prominent\napplication for inverse problems, magnetic resonance imaging. Code and data\nwill be publicly available.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13652v1",
    "published_date": "2024-06-19 15:55:12 UTC",
    "updated_date": "2024-06-19 15:55:12 UTC"
  },
  {
    "arxiv_id": "2406.13631v2",
    "title": "On AI-Inspired UI-Design",
    "authors": [
      "Jialiang Wei",
      "Anne-Lise Courbis",
      "Thomas Lambolais",
      "Gérard Dray",
      "Walid Maalej"
    ],
    "abstract": "Graphical User Interface (or simply UI) is a primary mean of interaction\nbetween users and their devices. In this paper, we discuss three complementary\nArtificial Intelligence (AI) approaches for triggering the creativity of app\ndesigners and inspiring them create better and more diverse UI designs. First,\ndesigners can prompt a Large Language Model (LLM) to directly generate and\nadjust UIs. Second, a Vision-Language Model (VLM) enables designers to\neffectively search a large screenshot dataset, e.g. from apps published in app\nstores. Third, a Diffusion Model (DM) can be trained to specifically generate\nUIs as inspirational images. We present an AI-inspired design process and\ndiscuss the implications and limitations of the approaches.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13631v2",
    "published_date": "2024-06-19 15:28:21 UTC",
    "updated_date": "2025-01-28 16:42:59 UTC"
  },
  {
    "arxiv_id": "2406.13626v1",
    "title": "Fine-Tuning Gemma-7B for Enhanced Sentiment Analysis of Financial News Headlines",
    "authors": [
      "Kangtong Mo",
      "Wenyan Liu",
      "Xuanzhen Xu",
      "Chang Yu",
      "Yuelin Zou",
      "Fangqing Xia"
    ],
    "abstract": "In this study, we explore the application of sentiment analysis on financial\nnews headlines to understand investor sentiment. By leveraging Natural Language\nProcessing (NLP) and Large Language Models (LLM), we analyze sentiment from the\nperspective of retail investors. The FinancialPhraseBank dataset, which\ncontains categorized sentiments of financial news headlines, serves as the\nbasis for our analysis. We fine-tuned several models, including\ndistilbert-base-uncased, Llama, and gemma-7b, to evaluate their effectiveness\nin sentiment classification. Our experiments demonstrate that the fine-tuned\ngemma-7b model outperforms others, achieving the highest precision, recall, and\nF1 score. Specifically, the gemma-7b model showed significant improvements in\naccuracy after fine-tuning, indicating its robustness in capturing the nuances\nof financial sentiment. This model can be instrumental in providing market\ninsights, risk management, and aiding investment decisions by accurately\npredicting the sentiment of financial news. The results highlight the potential\nof advanced LLMs in transforming how we analyze and interpret financial\ninformation, offering a powerful tool for stakeholders in the financial\nindustry.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13626v1",
    "published_date": "2024-06-19 15:20:19 UTC",
    "updated_date": "2024-06-19 15:20:19 UTC"
  },
  {
    "arxiv_id": "2406.13625v1",
    "title": "Enhance the Image: Super Resolution using Artificial Intelligence in MRI",
    "authors": [
      "Ziyu Li",
      "Zihan Li",
      "Haoxiang Li",
      "Qiuyun Fan",
      "Karla L. Miller",
      "Wenchuan Wu",
      "Akshay S. Chaudhari",
      "Qiyuan Tian"
    ],
    "abstract": "This chapter provides an overview of deep learning techniques for improving\nthe spatial resolution of MRI, ranging from convolutional neural networks,\ngenerative adversarial networks, to more advanced models including\ntransformers, diffusion models, and implicit neural representations. Our\nexploration extends beyond the methodologies to scrutinize the impact of\nsuper-resolved images on clinical and neuroscientific assessments. We also\ncover various practical topics such as network architectures, image evaluation\nmetrics, network loss functions, and training data specifics, including\ndownsampling methods for simulating low-resolution images and dataset\nselection. Finally, we discuss existing challenges and potential future\ndirections regarding the feasibility and reliability of deep learning-based MRI\nsuper-resolution, with the aim to facilitate its wider adoption to benefit\nvarious clinical and neuroscientific applications.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "physics.med-ph"
    ],
    "primary_category": "cs.CV",
    "comment": "A book chapter in Machine Learning in MRI: From methods to clinical\n  translation",
    "pdf_url": "http://arxiv.org/pdf/2406.13625v1",
    "published_date": "2024-06-19 15:19:41 UTC",
    "updated_date": "2024-06-19 15:19:41 UTC"
  },
  {
    "arxiv_id": "2406.13617v1",
    "title": "Optimizing Psychological Counseling with Instruction-Tuned Large Language Models",
    "authors": [
      "Wenjie Li",
      "Tianyu Sun",
      "Kun Qian",
      "Wenhong Wang"
    ],
    "abstract": "The advent of large language models (LLMs) has significantly advanced various\nfields, including natural language processing and automated dialogue systems.\nThis paper explores the application of LLMs in psychological counseling,\naddressing the increasing demand for mental health services. We present a\nmethod for instruction tuning LLMs with specialized prompts to enhance their\nperformance in providing empathetic, relevant, and supportive responses. Our\napproach involves developing a comprehensive dataset of counseling-specific\nprompts, refining them through feedback from professional counselors, and\nconducting rigorous evaluations using both automatic metrics and human\nassessments. The results demonstrate that our instruction-tuned model\noutperforms several baseline LLMs, highlighting its potential as a scalable and\naccessible tool for mental health support.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.13617v1",
    "published_date": "2024-06-19 15:13:07 UTC",
    "updated_date": "2024-06-19 15:13:07 UTC"
  },
  {
    "arxiv_id": "2406.13605v2",
    "title": "Nicer Than Humans: How do Large Language Models Behave in the Prisoner's Dilemma?",
    "authors": [
      "Nicoló Fontana",
      "Francesco Pierri",
      "Luca Maria Aiello"
    ],
    "abstract": "The behavior of Large Language Models (LLMs) as artificial social agents is\nlargely unexplored, and we still lack extensive evidence of how these agents\nreact to simple social stimuli. Testing the behavior of AI agents in classic\nGame Theory experiments provides a promising theoretical framework for\nevaluating the norms and values of these agents in archetypal social\nsituations. In this work, we investigate the cooperative behavior of three LLMs\n(Llama2, Llama3, and GPT3.5) when playing the Iterated Prisoner's Dilemma\nagainst random adversaries displaying various levels of hostility. We introduce\na systematic methodology to evaluate an LLM's comprehension of the game rules\nand its capability to parse historical gameplay logs for decision-making. We\nconducted simulations of games lasting for 100 rounds and analyzed the LLMs'\ndecisions in terms of dimensions defined in the behavioral economics\nliterature. We find that all models tend not to initiate defection but act\ncautiously, favoring cooperation over defection only when the opponent's\ndefection rate is low. Overall, LLMs behave at least as cooperatively as the\ntypical human player, although our results indicate some substantial\ndifferences among models. In particular, Llama2 and GPT3.5 are more cooperative\nthan humans, and especially forgiving and non-retaliatory for opponent\ndefection rates below 30%. More similar to humans, Llama3 exhibits consistently\nuncooperative and exploitative behavior unless the opponent always cooperates.\nOur systematic approach to the study of LLMs in game theoretical scenarios is a\nstep towards using these simulations to inform practices of LLM auditing and\nalignment.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.GT",
      "physics.soc-ph"
    ],
    "primary_category": "cs.CY",
    "comment": "v1: 9 pages, 8 figures, 1 table v2: 11 pages, 14 figures, 1 table.\n  Increased number of models studied, expanded results and conclusion, added\n  references, corrected typos",
    "pdf_url": "http://arxiv.org/pdf/2406.13605v2",
    "published_date": "2024-06-19 14:51:14 UTC",
    "updated_date": "2024-09-19 15:19:58 UTC"
  },
  {
    "arxiv_id": "2406.13604v1",
    "title": "Root Cause Localization for Microservice Systems in Cloud-edge Collaborative Environments",
    "authors": [
      "Yuhan Zhu",
      "Jian Wang",
      "Bing Li",
      "Xuxian Tang",
      "Hao Li",
      "Neng Zhang",
      "Yuqi Zhao"
    ],
    "abstract": "With the development of cloud-native technologies, microservice-based\nsoftware systems face challenges in accurately localizing root causes when\nfailures occur. Additionally, the cloud-edge collaborative environment\nintroduces more difficulties, such as unstable networks and high latency across\nnetwork segments. Accurately identifying the root cause of microservices in a\ncloud-edge collaborative environment has thus become an urgent problem. In this\npaper, we propose MicroCERCL, a novel approach that pinpoints root causes at\nthe kernel and application level in the cloud-edge collaborative environment.\nOur key insight is that failures propagate through direct invocations and\nindirect resource-competition dependencies in a cloud-edge collaborative\nenvironment characterized by instability and high latency. This will become\nmore complex in the hybrid deployment that simultaneously involves multiple\nmicroservice systems. Leveraging this insight, we extract valid contents from\nkernel-level logs to prioritize localizing the kernel-level root cause.\nMoreover, we construct a heterogeneous dynamic topology stack and train a graph\nneural network model to accurately localize the application-level root cause\nwithout relying on historical data. Notably, we released the first benchmark\nhybrid deployment microservice system in a cloud-edge collaborative environment\n(the largest and most complex within our knowledge). Experiments conducted on\nthe dataset collected from the benchmark show that MicroCERCL can accurately\nlocalize the root cause of microservice systems in such environments,\nsignificantly outperforming state-of-the-art approaches with an increase of at\nleast 24.1% in top-1 accuracy.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.PF"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13604v1",
    "published_date": "2024-06-19 14:49:37 UTC",
    "updated_date": "2024-06-19 14:49:37 UTC"
  },
  {
    "arxiv_id": "2406.13600v1",
    "title": "CoDreamer: Communication-Based Decentralised World Models",
    "authors": [
      "Edan Toledo",
      "Amanda Prorok"
    ],
    "abstract": "Sample efficiency is a critical challenge in reinforcement learning.\nModel-based RL has emerged as a solution, but its application has largely been\nconfined to single-agent scenarios. In this work, we introduce CoDreamer, an\nextension of the Dreamer algorithm for multi-agent environments. CoDreamer\nleverages Graph Neural Networks for a two-level communication system to tackle\nchallenges such as partial observability and inter-agent cooperation.\nCommunication is separately utilised within the learned world models and within\nthe learned policies of each agent to enhance modelling and task-solving. We\nshow that CoDreamer offers greater expressive power than a naive application of\nDreamer, and we demonstrate its superiority over baseline methods across\nvarious multi-agent environments.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13600v1",
    "published_date": "2024-06-19 14:42:40 UTC",
    "updated_date": "2024-06-19 14:42:40 UTC"
  },
  {
    "arxiv_id": "2406.13597v1",
    "title": "GraphKAN: Enhancing Feature Extraction with Graph Kolmogorov Arnold Networks",
    "authors": [
      "Fan Zhang",
      "Xin Zhang"
    ],
    "abstract": "Massive number of applications involve data with underlying relationships\nembedded in non-Euclidean space. Graph neural networks (GNNs) are utilized to\nextract features by capturing the dependencies within graphs. Despite\ngroundbreaking performances, we argue that Multi-layer perceptrons (MLPs) and\nfixed activation functions impede the feature extraction due to information\nloss. Inspired by Kolmogorov Arnold Networks (KANs), we make the first attempt\nto GNNs with KANs. We discard MLPs and activation functions, and instead used\nKANs for feature extraction. Experiments demonstrate the effectiveness of\nGraphKAN, emphasizing the potential of KANs as a powerful tool. Code is\navailable at https://github.com/Ryanfzhang/GraphKan.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13597v1",
    "published_date": "2024-06-19 14:41:09 UTC",
    "updated_date": "2024-06-19 14:41:09 UTC"
  },
  {
    "arxiv_id": "2406.13586v1",
    "title": "Submodular Participatory Budgeting",
    "authors": [
      "Jing Yuan",
      "Shaojie Tang"
    ],
    "abstract": "Participatory budgeting refers to the practice of allocating public resources\nby collecting and aggregating individual preferences. Most existing studies in\nthis field often assume an additive utility function, where each individual\nholds a private utility for each candidate project, and the total utility of a\nset of funded projects is simply the sum of the utilities of all projects. We\nargue that this assumption does not always hold in reality. For example,\nbuilding two playgrounds in the same neighborhood does not necessarily lead to\ntwice the utility of building a single playground.\n  To address this, we extend the existing study by proposing a submodular\nparticipatory budgeting problem, assuming that the utility function of each\nindividual is a monotone and submodular function over funded projects. We\npropose and examine three preference elicitation methods, including\n\\emph{ranking-by-marginal-values}, \\emph{ranking-by-values} and \\emph{threshold\napproval votes}, and analyze their performances in terms of distortion.\nNotably, if the utility function is addicative, our aggregation rule designed\nfor threshold approval votes achieves a better distortion than the\nstate-of-the-art approach.",
    "categories": [
      "cs.GT",
      "cs.AI"
    ],
    "primary_category": "cs.GT",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13586v1",
    "published_date": "2024-06-19 14:22:54 UTC",
    "updated_date": "2024-06-19 14:22:54 UTC"
  },
  {
    "arxiv_id": "2406.13569v1",
    "title": "Bayes' capacity as a measure for reconstruction attacks in federated learning",
    "authors": [
      "Sayan Biswas",
      "Mark Dras",
      "Pedro Faustini",
      "Natasha Fernandes",
      "Annabelle McIver",
      "Catuscia Palamidessi",
      "Parastoo Sadeghi"
    ],
    "abstract": "Within the machine learning community, reconstruction attacks are a principal\nattack of concern and have been identified even in federated learning, which\nwas designed with privacy preservation in mind. In federated learning, it has\nbeen shown that an adversary with knowledge of the machine learning\narchitecture is able to infer the exact value of a training element given an\nobservation of the weight updates performed during stochastic gradient descent.\nIn response to these threats, the privacy community recommends the use of\ndifferential privacy in the stochastic gradient descent algorithm, termed\nDP-SGD. However, DP has not yet been formally established as an effective\ncountermeasure against reconstruction attacks. In this paper, we formalise the\nreconstruction threat model using the information-theoretic framework of\nquantitative information flow. We show that the Bayes' capacity, related to the\nSibson mutual information of order infinity, represents a tight upper bound on\nthe leakage of the DP-SGD algorithm to an adversary interested in performing a\nreconstruction attack. We provide empirical results demonstrating the\neffectiveness of this measure for comparing mechanisms against reconstruction\nthreats.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13569v1",
    "published_date": "2024-06-19 13:58:42 UTC",
    "updated_date": "2024-06-19 13:58:42 UTC"
  },
  {
    "arxiv_id": "2406.13568v1",
    "title": "Trapezoidal Gradient Descent for Effective Reinforcement Learning in Spiking Networks",
    "authors": [
      "Yuhao Pan",
      "Xiucheng Wang",
      "Nan Cheng",
      "Qi Qiu"
    ],
    "abstract": "With the rapid development of artificial intelligence technology, the field\nof reinforcement learning has continuously achieved breakthroughs in both\ntheory and practice. However, traditional reinforcement learning algorithms\noften entail high energy consumption during interactions with the environment.\nSpiking Neural Network (SNN), with their low energy consumption characteristics\nand performance comparable to deep neural networks, have garnered widespread\nattention. To reduce the energy consumption of practical applications of\nreinforcement learning, researchers have successively proposed the Pop-SAN and\nMDC-SAN algorithms. Nonetheless, these algorithms use rectangular functions to\napproximate the spike network during the training process, resulting in low\nsensitivity, thus indicating room for improvement in the training effectiveness\nof SNN. Based on this, we propose a trapezoidal approximation gradient method\nto replace the spike network, which not only preserves the original stable\nlearning state but also enhances the model's adaptability and response\nsensitivity under various signal dynamics. Simulation results show that the\nimproved algorithm, using the trapezoidal approximation gradient to replace the\nspike network, achieves better convergence speed and performance compared to\nthe original algorithm and demonstrates good training stability.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13568v1",
    "published_date": "2024-06-19 13:56:22 UTC",
    "updated_date": "2024-06-19 13:56:22 UTC"
  },
  {
    "arxiv_id": "2406.13564v1",
    "title": "Is AI fun? HumorDB: a curated dataset and benchmark to investigate graphical humor",
    "authors": [
      "Veedant Jain",
      "Felipe dos Santos Alves Feitosa",
      "Gabriel Kreiman"
    ],
    "abstract": "Despite significant advancements in computer vision, understanding complex\nscenes, particularly those involving humor, remains a substantial challenge.\nThis paper introduces HumorDB, a novel image-only dataset specifically designed\nto advance visual humor understanding. HumorDB consists of meticulously curated\nimage pairs with contrasting humor ratings, emphasizing subtle visual cues that\ntrigger humor and mitigating potential biases. The dataset enables evaluation\nthrough binary classification(Funny or Not Funny), range regression(funniness\non a scale from 1 to 10), and pairwise comparison tasks(Which Image is\nFunnier?), effectively capturing the subjective nature of humor perception.\nInitial experiments reveal that while vision-only models struggle,\nvision-language models, particularly those leveraging large language models,\nshow promising results. HumorDB also shows potential as a valuable zero-shot\nbenchmark for powerful large multimodal models. We open-source both the dataset\nand code under the CC BY 4.0 license.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.5.4"
    ],
    "primary_category": "cs.CV",
    "comment": "7 main figures, 5 additional appendix figures",
    "pdf_url": "http://arxiv.org/pdf/2406.13564v1",
    "published_date": "2024-06-19 13:51:40 UTC",
    "updated_date": "2024-06-19 13:51:40 UTC"
  },
  {
    "arxiv_id": "2406.13558v2",
    "title": "Enhancing Travel Choice Modeling with Large Language Models: A Prompt-Learning Approach",
    "authors": [
      "Xuehao Zhai",
      "Hanlin Tian",
      "Lintong Li",
      "Tianyu Zhao"
    ],
    "abstract": "Travel choice analysis is crucial for understanding individual travel\nbehavior to develop appropriate transport policies and recommendation systems\nin Intelligent Transportation Systems (ITS). Despite extensive research, this\ndomain faces two critical challenges: a) modeling with limited survey data, and\nb) simultaneously achieving high model explainability and accuracy. In this\npaper, we introduce a novel prompt-learning-based Large Language Model(LLM)\nframework that significantly improves prediction accuracy and provides explicit\nexplanations for individual predictions. This framework involves three main\nsteps: transforming input variables into textual form; building of\ndemonstrations similar to the object, and applying these to a well-trained LLM.\nWe tested the framework's efficacy using two widely used choice datasets:\nLondon Passenger Mode Choice (LPMC) and Optima-Mode collected in Switzerland.\nThe results indicate that the LLM significantly outperforms state-of-the-art\ndeep learning methods and discrete choice models in predicting people's\nchoices. Additionally, we present a case of explanation illustrating how the\nLLM framework generates understandable and explicit explanations at the\nindividual level.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "We currently do not have a replacement version available. We request\n  withdrawal due to a significant methodological error affecting the paper's\n  validity, specifically a miscalculation in data preprocessing. We are working\n  on corrections, but this will take time. We believe an interim withdrawal is\n  necessary to prevent the dissemination of incorrect information.",
    "pdf_url": "http://arxiv.org/pdf/2406.13558v2",
    "published_date": "2024-06-19 13:46:08 UTC",
    "updated_date": "2024-06-22 14:44:34 UTC"
  },
  {
    "arxiv_id": "2406.13555v3",
    "title": "BiLD: Bi-directional Logits Difference Loss for Large Language Model Distillation",
    "authors": [
      "Minchong Li",
      "Feng Zhou",
      "Xiaohui Song"
    ],
    "abstract": "In recent years, large language models (LLMs) have shown exceptional\ncapabilities across various natural language processing (NLP) tasks. However,\nsuch impressive performance often comes with the trade-off of an increased\nparameter size, posing significant challenges for widespread deployment.\nKnowledge distillation (KD) provides a solution by transferring knowledge from\na large teacher model to a smaller student model. In this paper, we explore the\ntask-specific distillation of LLMs at the logit level. Our investigation\nreveals that the logits of fine-tuned LLMs exhibit a more extreme long-tail\ndistribution than those from vision models, with hidden \"noise\" in the long\ntail affecting distillation performance. Furthermore, existing logits\ndistillation methods often struggle to effectively utilize the internal ranking\ninformation from the logits. To address these, we propose the Bi-directional\nLogits Difference (BiLD) loss. The BiLD loss filters out the long-tail noise by\nutilizing only top-$k$ teacher and student logits, and leverages the internal\nlogits ranking information by constructing logits differences. To evaluate BiLD\nloss, we conduct comprehensive experiments on 13 datasets using two types of\nLLMs. Our results show that the BiLD loss, with only the top-8 logits,\noutperforms supervised fine-tuning (SFT), vanilla KL loss, and five other\ndistillation methods from both NLP and CV fields.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "COLING 2025",
    "pdf_url": "http://arxiv.org/pdf/2406.13555v3",
    "published_date": "2024-06-19 13:44:56 UTC",
    "updated_date": "2025-02-18 00:14:57 UTC"
  },
  {
    "arxiv_id": "2406.13551v1",
    "title": "Mitigating Social Biases in Language Models through Unlearning",
    "authors": [
      "Omkar Dige",
      "Diljot Singh",
      "Tsz Fung Yau",
      "Qixuan Zhang",
      "Borna Bolandraftar",
      "Xiaodan Zhu",
      "Faiza Khan Khattak"
    ],
    "abstract": "Mitigating bias in language models (LMs) has become a critical problem due to\nthe widespread deployment of LMs. Numerous approaches revolve around data\npre-processing and fine-tuning of language models, tasks that can be both\ntime-consuming and computationally demanding. Consequently, there is a growing\ninterest in machine unlearning techniques given their capacity to induce the\nforgetting of undesired behaviors of the existing pre-trained or fine-tuned\nmodels with lower computational cost. In this work, we explore two unlearning\nmethods, (1) Partitioned Contrastive Gradient Unlearning (PCGU) applied on\ndecoder models and (2) Negation via Task Vector, to reduce social biases in\nstate-of-the-art and open-source LMs such as LLaMA-2 and OPT. We also implement\ndistributed PCGU for large models. It is empirically shown, through\nquantitative and qualitative analyses, that negation via Task Vector method\noutperforms PCGU in debiasing with minimum deterioration in performance and\nperplexity of the models. On LLaMA-27B, negation via Task Vector reduces the\nbias score by 11.8%",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13551v1",
    "published_date": "2024-06-19 13:38:34 UTC",
    "updated_date": "2024-06-19 13:38:34 UTC"
  },
  {
    "arxiv_id": "2406.13542v3",
    "title": "Self-play with Execution Feedback: Improving Instruction-following Capabilities of Large Language Models",
    "authors": [
      "Guanting Dong",
      "Keming Lu",
      "Chengpeng Li",
      "Tingyu Xia",
      "Bowen Yu",
      "Chang Zhou",
      "Jingren Zhou"
    ],
    "abstract": "One core capability of large language models (LLMs) is to follow natural\nlanguage instructions. However, the issue of automatically constructing\nhigh-quality training data to enhance the complex instruction-following\nabilities of LLMs without manual annotation remains unresolved. In this paper,\nwe introduce AutoIF, the first scalable and reliable method for automatically\ngenerating instruction-following training data. AutoIF transforms the\nvalidation of instruction-following data quality into code verification,\nrequiring LLMs to generate instructions, the corresponding code to check the\ncorrectness of the instruction responses, and unit test samples to verify the\ncode's correctness. Then, execution feedback-based rejection sampling can\ngenerate data for Supervised Fine-Tuning (SFT) and Reinforcement Learning from\nHuman Feedback (RLHF) training. AutoIF achieves significant improvements across\nthree training algorithms, SFT, Offline DPO, and Online DPO, when applied to\nthe top open-source LLMs, Qwen2 and LLaMA3, in self-alignment and\nstrong-to-weak distillation settings. Our code is publicly available at\nhttps://github.com/QwenLM/AutoIF.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2406.13542v3",
    "published_date": "2024-06-19 13:29:53 UTC",
    "updated_date": "2024-07-18 09:00:23 UTC"
  },
  {
    "arxiv_id": "2406.13474v2",
    "title": "BoA: Attention-aware Post-training Quantization without Backpropagation",
    "authors": [
      "Junhan Kim",
      "Ho-young Kim",
      "Eulrang Cho",
      "Chungman Lee",
      "Joonyoung Kim",
      "Yongkweon Jeon"
    ],
    "abstract": "Post-training quantization (PTQ) is a promising solution for deploying large\nlanguage models (LLMs) on resource-constrained devices. Early methods developed\nfor smaller networks like ResNet rely on gradient-based optimization, which\nbecomes impractical for hyper-scale LLMs with billions of parameters. While\nrecently proposed backpropagation-free or transformation-based methods\nalleviate this issue, their performance remains limited by either a lack of\ninter-layer dependency consideration or the use of naive nearest-rounding-based\ninteger weight assignment to save the heavy computational cost of weight\noptimization. We thus introduce a novel backpropagation-free PTQ algorithm that\noptimizes integer weights by considering inter-layer dependencies. The key\ninnovation is the development of attention-aware Hessian matrices that capture\ninter-layer interactions within the attention module. Extensive experiments\ndemonstrate that our approach not only outperforms existing weight quantization\nmethods but also shows good synergy with conventional methods to suppress\nactivation outliers, leading to state-of-the-art weight-activation quantization\nperformance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "19 pages, under review",
    "pdf_url": "http://arxiv.org/pdf/2406.13474v2",
    "published_date": "2024-06-19 11:53:21 UTC",
    "updated_date": "2025-02-27 14:29:08 UTC"
  },
  {
    "arxiv_id": "2406.13469v2",
    "title": "Encoder vs Decoder: Comparative Analysis of Encoder and Decoder Language Models on Multilingual NLU Tasks",
    "authors": [
      "Dan Saattrup Nielsen",
      "Kenneth Enevoldsen",
      "Peter Schneider-Kamp"
    ],
    "abstract": "This paper explores the performance of encoder and decoder language models on\nmultilingual Natural Language Understanding (NLU) tasks, with a broad focus on\nGermanic languages. Building upon the ScandEval benchmark, initially restricted\nto evaluating encoder models, we extend the evaluation framework to include\ndecoder models. We introduce a method for evaluating decoder models on NLU\ntasks and apply it to the languages Danish, Swedish, Norwegian, Icelandic,\nFaroese, German, Dutch, and English. Through a series of experiments and\nanalyses, we also address research questions regarding the comparative\nperformance of encoder and decoder models, the impact of NLU task types, and\nthe variation across language resources. Our findings reveal that encoder\nmodels can achieve significantly better NLU performance than decoder models\ndespite having orders of magnitude fewer parameters. Additionally, we\ninvestigate the correlation between decoders and task performance via a UMAP\nanalysis, shedding light on the unique capabilities of decoder and encoder\nmodels. This study contributes to a deeper understanding of language model\nparadigms in NLU tasks and provides valuable insights for model selection and\nevaluation in multilingual settings.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "NoDaLiDa 2025 camera ready version, including appendices",
    "pdf_url": "http://arxiv.org/pdf/2406.13469v2",
    "published_date": "2024-06-19 11:50:09 UTC",
    "updated_date": "2025-01-11 10:20:26 UTC"
  },
  {
    "arxiv_id": "2406.13457v1",
    "title": "EvTexture: Event-driven Texture Enhancement for Video Super-Resolution",
    "authors": [
      "Dachun Kai",
      "Jiayao Lu",
      "Yueyi Zhang",
      "Xiaoyan Sun"
    ],
    "abstract": "Event-based vision has drawn increasing attention due to its unique\ncharacteristics, such as high temporal resolution and high dynamic range. It\nhas been used in video super-resolution (VSR) recently to enhance the flow\nestimation and temporal alignment. Rather than for motion learning, we propose\nin this paper the first VSR method that utilizes event signals for texture\nenhancement. Our method, called EvTexture, leverages high-frequency details of\nevents to better recover texture regions in VSR. In our EvTexture, a new\ntexture enhancement branch is presented. We further introduce an iterative\ntexture enhancement module to progressively explore the\nhigh-temporal-resolution event information for texture restoration. This allows\nfor gradual refinement of texture regions across multiple iterations, leading\nto more accurate and rich high-resolution details. Experimental results show\nthat our EvTexture achieves state-of-the-art performance on four datasets. For\nthe Vid4 dataset with rich textures, our method can get up to 4.67dB gain\ncompared with recent event-based methods. Code:\nhttps://github.com/DachunKai/EvTexture.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "ICML 2024. Project page:\n  https://dachunkai.github.io/evtexture.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2406.13457v1",
    "published_date": "2024-06-19 11:27:44 UTC",
    "updated_date": "2024-06-19 11:27:44 UTC"
  },
  {
    "arxiv_id": "2406.13450v1",
    "title": "Federating to Grow Transformers with Constrained Resources without Model Sharing",
    "authors": [
      "Shikun Shen",
      "Yifei Zou",
      "Yuan Yuan",
      "Yanwei Zheng",
      "Peng Li",
      "Xiuzhen Cheng",
      "Dongxiao Yu"
    ],
    "abstract": "The high resource consumption of large-scale models discourages\nresource-constrained users from developing their customized transformers. To\nthis end, this paper considers a federated framework named Fed-Grow for\nmultiple participants to cooperatively scale a transformer from their\npre-trained small models. Under the Fed-Grow, a Dual-LiGO (Dual Linear Growth\nOperator) architecture is designed to help participants expand their\npre-trained small models to a transformer. In Dual-LiGO, the Local-LiGO part is\nused to address the heterogeneity problem caused by the various pre-trained\nmodels, and the Global-LiGO part is shared to exchange the implicit knowledge\nfrom the pre-trained models, local data, and training process of participants.\nInstead of model sharing, only sharing the Global-LiGO strengthens the privacy\nof our approach. Compared with several state-of-the-art methods in simulation,\nour approach has higher accuracy, better precision, and lower resource\nconsumption on computations and communications. To the best of our knowledge,\nmost of the previous model-scaling works are centralized, and our work is the\nfirst one that cooperatively grows a transformer from multiple pre-trained\nheterogeneous models with the user privacy protected in terms of local data and\nmodels. We hope that our approach can extend the transformers to the broadly\ndistributed scenarios and encourage more resource-constrained users to enjoy\nthe bonus taken by the large-scale transformers.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13450v1",
    "published_date": "2024-06-19 11:17:59 UTC",
    "updated_date": "2024-06-19 11:17:59 UTC"
  },
  {
    "arxiv_id": "2406.13445v1",
    "title": "Lost in UNet: Improving Infrared Small Target Detection by Underappreciated Local Features",
    "authors": [
      "Wuzhou Quan",
      "Wei Zhao",
      "Weiming Wang",
      "Haoran Xie",
      "Fu Lee Wang",
      "Mingqiang Wei"
    ],
    "abstract": "Many targets are often very small in infrared images due to the long-distance\nimaging meachnism. UNet and its variants, as popular detection backbone\nnetworks, downsample the local features early and cause the irreversible loss\nof these local features, leading to both the missed and false detection of\nsmall targets in infrared images. We propose HintU, a novel network to recover\nthe local features lost by various UNet-based methods for effective infrared\nsmall target detection. HintU has two key contributions. First, it introduces\nthe \"Hint\" mechanism for the first time, i.e., leveraging the prior knowledge\nof target locations to highlight critical local features. Second, it improves\nthe mainstream UNet-based architecture to preserve target pixels even after\ndownsampling. HintU can shift the focus of various networks (e.g., vanilla\nUNet, UNet++, UIUNet, MiM+, and HCFNet) from the irrelevant background pixels\nto a more restricted area from the beginning. Experimental results on three\ndatasets NUDT-SIRST, SIRSTv2 and IRSTD1K demonstrate that HintU enhances the\nperformance of existing methods with only an additional 1.88 ms cost (on RTX\nTitan). Additionally, the explicit constraints of HintU enhance the\ngeneralization ability of UNet-based methods. Code is available at\nhttps://github.com/Wuzhou-Quan/HintU.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13445v1",
    "published_date": "2024-06-19 11:11:38 UTC",
    "updated_date": "2024-06-19 11:11:38 UTC"
  },
  {
    "arxiv_id": "2406.13441v1",
    "title": "Robust Melanoma Thickness Prediction via Deep Transfer Learning enhanced by XAI Techniques",
    "authors": [
      "Miguel Nogales",
      "Begoña Acha",
      "Fernando Alarcón",
      "José Pereyra",
      "Carmen Serrano"
    ],
    "abstract": "This study focuses on analyzing dermoscopy images to determine the depth of\nmelanomas, which is a critical factor in diagnosing and treating skin cancer.\nThe Breslow depth, measured from the top of the granular layer to the deepest\npoint of tumor invasion, serves as a crucial parameter for staging melanoma and\nguiding treatment decisions. This research aims to improve the prediction of\nthe depth of melanoma through the use of machine learning models, specifically\ndeep learning, while also providing an analysis of the possible existance of\ngraduation in the images characteristics which correlates with the depth of the\nmelanomas. Various datasets, including ISIC and private collections, were used,\ncomprising a total of 1162 images. The datasets were combined and balanced to\nensure robust model training. The study utilized pre-trained Convolutional\nNeural Networks (CNNs). Results indicated that the models achieved significant\nimprovements over previous methods. Additionally, the study conducted a\ncorrelation analysis between model's predictions and actual melanoma thickness,\nrevealing a moderate correlation that improves with higher thickness values.\nExplainability methods such as feature visualization through Principal\nComponent Analysis (PCA) demonstrated the capability of deep features to\ndistinguish between different depths of melanoma, providing insight into the\ndata distribution and model behavior. In summary, this research presents a dual\ncontribution: enhancing the state-of-the-art classification results through\nadvanced training techniques and offering a detailed analysis of the data and\nmodel behavior to better understand the relationship between dermoscopy images\nand melanoma thickness.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13441v1",
    "published_date": "2024-06-19 11:07:55 UTC",
    "updated_date": "2024-06-19 11:07:55 UTC"
  },
  {
    "arxiv_id": "2406.13436v1",
    "title": "What's Next? Exploring Utilization, Challenges, and Future Directions of AI-Generated Image Tools in Graphic Design",
    "authors": [
      "Yuying Tang",
      "Mariana Ciancia",
      "Zhigang Wang",
      "Ze Gao"
    ],
    "abstract": "Recent advancements in artificial intelligence, such as computer vision and\ndeep learning, have led to the emergence of numerous generative AI platforms,\nparticularly for image generation. However, the application of AI-generated\nimage tools in graphic design has not been extensively explored. This study\nconducted semi-structured interviews with seven designers of varying experience\nlevels to understand their current usage, challenges, and future functional\nneeds for AI-generated image tools in graphic design. As our findings suggest,\nAI tools serve as creative partners in design, enhancing human creativity,\noffering strategic insights, and fostering team collaboration and\ncommunication. The findings provide guiding recommendations for the future\ndevelopment of AI-generated image tools, aimed at helping engineers optimize\nthese tools to better meet the needs of graphic designers.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13436v1",
    "published_date": "2024-06-19 10:51:56 UTC",
    "updated_date": "2024-06-19 10:51:56 UTC"
  },
  {
    "arxiv_id": "2406.13433v2",
    "title": "Certification for Differentially Private Prediction in Gradient-Based Training",
    "authors": [
      "Matthew Wicker",
      "Philip Sosnin",
      "Igor Shilov",
      "Adrianna Janik",
      "Mark N. Müller",
      "Yves-Alexandre de Montjoye",
      "Adrian Weller",
      "Calvin Tsay"
    ],
    "abstract": "Differential privacy upper-bounds the information leakage of machine learning\nmodels, yet providing meaningful privacy guarantees has proven to be\nchallenging in practice. The private prediction setting where model outputs are\nprivatized is being investigated as an alternate way to provide formal\nguarantees at prediction time. Most current private prediction algorithms,\nhowever, rely on global sensitivity for noise calibration, which often results\nin large amounts of noise being added to the predictions. Data-specific noise\ncalibration, such as smooth sensitivity, could significantly reduce the amount\nof noise added, but were so far infeasible to compute exactly for modern\nmachine learning models. In this work we provide a novel and practical approach\nbased on convex relaxation and bound propagation to compute a provable\nupper-bound for the local and smooth sensitivity of a prediction. This bound\nallows us to reduce the magnitude of noise added or improve privacy accounting\nin the private prediction setting. We validate our framework on datasets from\nfinancial services, medical image classification, and natural language\nprocessing and across models and find our approach to reduce the noise added by\nup to order of magnitude.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.13433v2",
    "published_date": "2024-06-19 10:47:00 UTC",
    "updated_date": "2024-10-30 16:40:19 UTC"
  },
  {
    "arxiv_id": "2406.13414v1",
    "title": "Archive-based Single-Objective Evolutionary Algorithms for Submodular Optimization",
    "authors": [
      "Frank Neumann",
      "Günter Rudolph"
    ],
    "abstract": "Constrained submodular optimization problems play a key role in the area of\ncombinatorial optimization as they capture many NP-hard optimization problems.\nSo far, Pareto optimization approaches using multi-objective formulations have\nbeen shown to be successful to tackle these problems while single-objective\nformulations lead to difficulties for algorithms such as the $(1+1)$-EA due to\nthe presence of local optima. We introduce for the first time single-objective\nalgorithms that are provably successful for different classes of constrained\nsubmodular maximization problems. Our algorithms are variants of the\n$(1+\\lambda)$-EA and $(1+1)$-EA and increase the feasible region of the search\nspace incrementally in order to deal with the considered submodular problems.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "To appear at PPSN 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.13414v1",
    "published_date": "2024-06-19 10:08:12 UTC",
    "updated_date": "2024-06-19 10:08:12 UTC"
  },
  {
    "arxiv_id": "2406.13399v1",
    "title": "VELO: A Vector Database-Assisted Cloud-Edge Collaborative LLM QoS Optimization Framework",
    "authors": [
      "Zhi Yao",
      "Zhiqing Tang",
      "Jiong Lou",
      "Ping Shen",
      "Weijia Jia"
    ],
    "abstract": "The Large Language Model (LLM) has gained significant popularity and is\nextensively utilized across various domains. Most LLM deployments occur within\ncloud data centers, where they encounter substantial response delays and incur\nhigh costs, thereby impacting the Quality of Services (QoS) at the network\nedge. Leveraging vector database caching to store LLM request results at the\nedge can substantially mitigate response delays and cost associated with\nsimilar requests, which has been overlooked by previous research. Addressing\nthese gaps, this paper introduces a novel Vector database-assisted cloud-Edge\ncollaborative LLM QoS Optimization (VELO) framework. Firstly, we propose the\nVELO framework, which ingeniously employs vector database to cache the results\nof some LLM requests at the edge to reduce the response time of subsequent\nsimilar requests. Diverging from direct optimization of the LLM, our VELO\nframework does not necessitate altering the internal structure of LLM and is\nbroadly applicable to diverse LLMs. Subsequently, building upon the VELO\nframework, we formulate the QoS optimization problem as a Markov Decision\nProcess (MDP) and devise an algorithm grounded in Multi-Agent Reinforcement\nLearning (MARL) to decide whether to request the LLM in the cloud or directly\nreturn the results from the vector database at the edge. Moreover, to enhance\nrequest feature extraction and expedite training, we refine the policy network\nof MARL and integrate expert demonstrations. Finally, we implement the proposed\nalgorithm within a real edge system. Experimental findings confirm that our\nVELO framework substantially enhances user satisfaction by concurrently\ndiminishing delay and resource consumption for edge users utilizing LLMs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "to be published in IEEE ICWS 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.13399v1",
    "published_date": "2024-06-19 09:41:37 UTC",
    "updated_date": "2024-06-19 09:41:37 UTC"
  },
  {
    "arxiv_id": "2406.13385v1",
    "title": "Explainable by-design Audio Segmentation through Non-Negative Matrix Factorization and Probing",
    "authors": [
      "Martin Lebourdais",
      "Théo Mariotte",
      "Antonio Almudévar",
      "Marie Tahon",
      "Alfonso Ortega"
    ],
    "abstract": "Audio segmentation is a key task for many speech technologies, most of which\nare based on neural networks, usually considered as black boxes, with\nhigh-level performances. However, in many domains, among which health or\nforensics, there is not only a need for good performance but also for\nexplanations about the output decision. Explanations derived directly from\nlatent representations need to satisfy \"good\" properties, such as\ninformativeness, compactness, or modularity, to be interpretable. In this\narticle, we propose an explainable-by-design audio segmentation model based on\nnon-negative matrix factorization (NMF) which is a good candidate for the\ndesign of interpretable representations. This paper shows that our model\nreaches good segmentation performances, and presents deep analyses of the\nlatent representation extracted from the non-negative matrix. The proposed\napproach opens new perspectives toward the evaluation of interpretable\nrepresentations according to \"good\" properties.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted at Interspeech 2024, 5 pages, 2 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2406.13385v1",
    "published_date": "2024-06-19 09:26:33 UTC",
    "updated_date": "2024-06-19 09:26:33 UTC"
  },
  {
    "arxiv_id": "2406.13372v2",
    "title": "Thread: A Logic-Based Data Organization Paradigm for How-To Question Answering with Retrieval Augmented Generation",
    "authors": [
      "Kaikai An",
      "Fangkai Yang",
      "Liqun Li",
      "Junting Lu",
      "Sitao Cheng",
      "Shuzheng Si",
      "Lu Wang",
      "Pu Zhao",
      "Lele Cao",
      "Qingwei Lin",
      "Saravan Rajmohan",
      "Dongmei Zhang",
      "Qi Zhang",
      "Baobao Chang"
    ],
    "abstract": "Recent advances in retrieval-augmented generation have significantly improved\nthe performance of question-answering systems, particularly on factoid '5Ws'\nquestions. However, these systems still face substantial challenges when\naddressing '1H' questions, specifically how-to questions, which are integral to\ndecision-making processes and require dynamic, step-by-step answers. The key\nlimitation lies in the prevalent data organization paradigm, chunk, which\ndivides documents into fixed-size segments, and disrupts the logical coherence\nand connections within the context. To overcome this, in this paper, we propose\nThread, a novel data organization paradigm aimed at enabling current systems to\nhandle how-to questions more effectively. Specifically, we introduce a new\nknowledge granularity, termed 'logic unit', where documents are transformed\ninto more structured and loosely interconnected logic units with large language\nmodels. Extensive experiments conducted across both open-domain and industrial\nsettings demonstrate that Thread outperforms existing paradigms significantly,\nimproving the success rate of handling how-to questions by 21% to 33%.\nMoreover, Thread exhibits high adaptability in processing various document\nformats, drastically reducing the candidate quantity in the knowledge base and\nminimizing the required information to one-fourth compared with chunk,\noptimizing both efficiency and effectiveness.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2406.13372v2",
    "published_date": "2024-06-19 09:14:41 UTC",
    "updated_date": "2024-10-10 08:04:20 UTC"
  },
  {
    "arxiv_id": "2406.13371v1",
    "title": "Identifiable Causal Representation Learning: Unsupervised, Multi-View, and Multi-Environment",
    "authors": [
      "Julius von Kügelgen"
    ],
    "abstract": "Causal models provide rich descriptions of complex systems as sets of\nmechanisms by which each variable is influenced by its direct causes. They\nsupport reasoning about manipulating parts of the system and thus hold promise\nfor addressing some of the open challenges of artificial intelligence (AI),\nsuch as planning, transferring knowledge in changing environments, or\nrobustness to distribution shifts. However, a key obstacle to more widespread\nuse of causal models in AI is the requirement that the relevant variables be\nspecified a priori, which is typically not the case for the high-dimensional,\nunstructured data processed by modern AI systems. At the same time, machine\nlearning (ML) has proven quite successful at automatically extracting useful\nand compact representations of such complex data. Causal representation\nlearning (CRL) aims to combine the core strengths of ML and causality by\nlearning representations in the form of latent variables endowed with causal\nmodel semantics.\n  In this thesis, we study and present new results for different CRL settings.\nA central theme is the question of identifiability: Given infinite data, when\nare representations satisfying the same learning objective guaranteed to be\nequivalent? This is an important prerequisite for CRL, as it formally\ncharacterises if and when a learning task is, at least in principle, feasible.\nSince learning causal models, even without a representation learning component,\nis notoriously difficult, we require additional assumptions on the model class\nor rich data beyond the classical i.i.d. setting. By partially characterising\nidentifiability for different settings, this thesis investigates what is\npossible for CRL without direct supervision, and thus contributes to its\ntheoretical foundations. Ideally, the developed insights can help inform data\ncollection practices or inspire the design of new practical estimation methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "PhD Thesis; 190 pages, 33 figures, 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2406.13371v1",
    "published_date": "2024-06-19 09:14:40 UTC",
    "updated_date": "2024-06-19 09:14:40 UTC"
  },
  {
    "arxiv_id": "2406.13365v1",
    "title": "PPT-GNN: A Practical Pre-Trained Spatio-Temporal Graph Neural Network for Network Security",
    "authors": [
      "Louis Van Langendonck",
      "Ismael Castell-Uroz",
      "Pere Barlet-Ros"
    ],
    "abstract": "Recent works have demonstrated the potential of Graph Neural Networks (GNN)\nfor network intrusion detection. Despite their advantages, a significant gap\npersists between real-world scenarios, where detection speed is critical, and\nexisting proposals, which operate on large graphs representing several hours of\ntraffic. This gap results in unrealistic operational conditions and impractical\ndetection delays. Moreover, existing models do not generalize well across\ndifferent networks, hampering their deployment in production environments. To\naddress these issues, we introduce PPTGNN, a practical spatio-temporal GNN for\nintrusion detection. PPTGNN enables near real-time predictions, while better\ncapturing the spatio-temporal dynamics of network attacks. PPTGNN employs\nself-supervised pre-training for improved performance and reduced dependency on\nlabeled data. We evaluate PPTGNN on three public datasets and show that it\nsignificantly outperforms state-of-the-art models, such as E-ResGAT and\nE-GraphSAGE, with an average accuracy improvement of 10.38%. Finally, we show\nthat a pre-trained PPTGNN can easily be fine-tuned to unseen networks with\nminimal labeled examples. This highlights the potential of PPTGNN as a general,\nlarge-scale pre-trained model that can effectively operate in diverse network\nenvironments.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "Paper currently under review. Code will be made public upon\n  acceptance. 8 pages long, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.13365v1",
    "published_date": "2024-06-19 09:09:46 UTC",
    "updated_date": "2024-06-19 09:09:46 UTC"
  },
  {
    "arxiv_id": "2406.13351v1",
    "title": "A Resource-Adaptive Approach for Federated Learning under Resource-Constrained Environments",
    "authors": [
      "Ruirui Zhang",
      "Xingze Wu",
      "Yifei Zou",
      "Zhenzhen Xie",
      "Peng Li",
      "Xiuzhen Cheng",
      "Dongxiao Yu"
    ],
    "abstract": "The paper studies a fundamental federated learning (FL) problem involving\nmultiple clients with heterogeneous constrained resources. Compared with the\nnumerous training parameters, the computing and communication resources of\nclients are insufficient for fast local training and real-time knowledge\nsharing. Besides, training on clients with heterogeneous resources may result\nin the straggler problem. To address these issues, we propose Fed-RAA: a\nResource-Adaptive Asynchronous Federated learning algorithm. Different from\nvanilla FL methods, where all parameters are trained by each participating\nclient regardless of resource diversity, Fed-RAA adaptively allocates fragments\nof the global model to clients based on their computing and communication\ncapabilities. Each client then individually trains its assigned model fragment\nand asynchronously uploads the updated result. Theoretical analysis confirms\nthe convergence of our approach. Additionally, we design an online greedy-based\nalgorithm for fragment allocation in Fed-RAA, achieving fairness comparable to\nan offline strategy. We present numerical results on MNIST, CIFAR-10, and\nCIFAR-100, along with necessary comparisons and ablation studies, demonstrating\nthe advantages of our work. To the best of our knowledge, this paper represents\nthe first resource-adaptive asynchronous method for fragment-based FL with\nguaranteed theoretical convergence.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13351v1",
    "published_date": "2024-06-19 08:55:40 UTC",
    "updated_date": "2024-06-19 08:55:40 UTC"
  },
  {
    "arxiv_id": "2406.13348v2",
    "title": "Textual Unlearning Gives a False Sense of Unlearning",
    "authors": [
      "Jiacheng Du",
      "Zhibo Wang",
      "Jie Zhang",
      "Xiaoyi Pang",
      "Jiahui Hu",
      "Kui Ren"
    ],
    "abstract": "Language Models (LMs) are prone to ''memorizing'' training data, including\nsubstantial sensitive user information. To mitigate privacy risks and safeguard\nthe right to be forgotten, machine unlearning has emerged as a promising\napproach for enabling LMs to efficiently ''forget'' specific texts. However,\ndespite the good intentions, is textual unlearning really as effective and\nreliable as expected? To address the concern, we first propose Unlearning\nLikelihood Ratio Attack+ (U-LiRA+), a rigorous textual unlearning auditing\nmethod, and find that unlearned texts can still be detected with very high\nconfidence after unlearning. Further, we conduct an in-depth investigation on\nthe privacy risks of textual unlearning mechanisms in deployment and present\nthe Textual Unlearning Leakage Attack (TULA), along with its variants in both\nblack- and white-box scenarios. We show that textual unlearning mechanisms\ncould instead reveal more about the unlearned texts, exposing them to\nsignificant membership inference and data reconstruction risks. Our findings\nhighlight that existing textual unlearning actually gives a false sense of\nunlearning, underscoring the need for more robust and secure unlearning\nmechanisms.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13348v2",
    "published_date": "2024-06-19 08:51:54 UTC",
    "updated_date": "2025-02-18 12:16:27 UTC"
  },
  {
    "arxiv_id": "2406.13342v1",
    "title": "ZeroDL: Zero-shot Distribution Learning for Text Clustering via Large Language Models",
    "authors": [
      "Hwiyeol Jo",
      "Hyunwoo Lee",
      "Taiwoo Park"
    ],
    "abstract": "The recent advancements in large language models (LLMs) have brought\nsignificant progress in solving NLP tasks. Notably, in-context learning (ICL)\nis the key enabling mechanism for LLMs to understand specific tasks and\ngrasping nuances. In this paper, we propose a simple yet effective method to\ncontextualize a task toward a specific LLM, by (1) observing how a given LLM\ndescribes (all or a part of) target datasets, i.e., open-ended zero-shot\ninference, and (2) aggregating the open-ended inference results by the LLM, and\n(3) finally incorporate the aggregated meta-information for the actual task. We\nshow the effectiveness of this approach in text clustering tasks, and also\nhighlight the importance of the contextualization through examples of the above\nprocedure.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ARR Submitted",
    "pdf_url": "http://arxiv.org/pdf/2406.13342v1",
    "published_date": "2024-06-19 08:48:05 UTC",
    "updated_date": "2024-06-19 08:48:05 UTC"
  },
  {
    "arxiv_id": "2406.13308v1",
    "title": "Deep Learning-Based 3D Instance and Semantic Segmentation: A Review",
    "authors": [
      "Siddiqui Muhammad Yasir",
      "Hyunsik Ahn"
    ],
    "abstract": "The process of segmenting point cloud data into several homogeneous areas\nwith points in the same region having the same attributes is known as 3D\nsegmentation. Segmentation is challenging with point cloud data due to\nsubstantial redundancy, fluctuating sample density and lack of apparent\norganization. The research area has a wide range of robotics applications,\nincluding intelligent vehicles, autonomous mapping and navigation. A number of\nresearchers have introduced various methodologies and algorithms. Deep learning\nhas been successfully used to a spectrum of 2D vision domains as a prevailing\nA.I. methods. However, due to the specific problems of processing point clouds\nwith deep neural networks, deep learning on point clouds is still in its\ninitial stages. This study examines many strategies that have been presented to\n3D instance and semantic segmentation and gives a complete assessment of\ncurrent developments in deep learning-based 3D segmentation. In these\napproaches benefits, draw backs, and design mechanisms are studied and\naddressed. This study evaluates the impact of various segmentation algorithms\non competitiveness on various publicly accessible datasets, as well as the most\noften used pipelines, their advantages and limits, insightful findings and\nintriguing future research directions.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13308v1",
    "published_date": "2024-06-19 07:56:14 UTC",
    "updated_date": "2024-06-19 07:56:14 UTC"
  },
  {
    "arxiv_id": "2406.13305v2",
    "title": "Multimodal MRI Accurately Identifies Amyloid Status in Unbalanced Cohorts in Alzheimer's Disease Continuum",
    "authors": [
      "Giorgio Dolci",
      "Charles A. Ellis",
      "Federica Cruciani",
      "Lorenza Brusini",
      "Anees Abrol",
      "Ilaria Boscolo Galazzo",
      "Gloria Menegaz",
      "Vince D. Calhoun"
    ],
    "abstract": "Amyloid-$\\beta$ (A$\\beta$) plaques in conjunction with hyperphosphorylated\ntau proteins in the form of neurofibrillary tangles are the two\nneuropathological hallmarks of Alzheimer's disease. It is well-known that the\nidentification of individuals with A$\\beta$ positivity could enable early\ndiagnosis. In this work, we aim at capturing the A$\\beta$ positivity status in\nan unbalanced cohort enclosing subjects at different disease stages, exploiting\nthe underlying structural and connectivity disease-induced modulations as\nrevealed by structural, functional, and diffusion MRI. Of note, due to the\nunbalanced cohort, the outcomes may be guided by those factors rather than\namyloid accumulation. The partial views provided by each modality are\nintegrated in the model allowing to take full advantage of their\ncomplementarity in encoding the effects of the A$\\beta$ accumulation, leading\nto an accuracy of $0.762\\pm0.04$. The specificity of the information brought by\neach modality is assessed by \\textit{post-hoc} explainability analysis (guided\nbackpropagation), highlighting the underlying structural and functional\nchanges. Noteworthy, well-established biomarker key regions related to A$\\beta$\ndeposition could be identified by all modalities, including the hippocampus,\nthalamus, precuneus, and cingulate gyrus, witnessing in favor of the\nreliability of the method as well as its potential in shading light on\nmodality-specific possibly unknown A$\\beta$ deposition signatures.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "16 pages, 3 figures, provisionally accepted to a journal",
    "pdf_url": "http://arxiv.org/pdf/2406.13305v2",
    "published_date": "2024-06-19 07:51:21 UTC",
    "updated_date": "2024-10-14 17:14:58 UTC"
  },
  {
    "arxiv_id": "2406.13303v1",
    "title": "Integration of Policy and Reputation based Trust Mechanisms in e-Commerce Industry",
    "authors": [
      "Muhammad Yasir Siddiqui",
      "Alam Gir"
    ],
    "abstract": "The e-commerce systems are being tackled from commerce behavior and internet\ntechnologies. Therefore, trust aspect between buyer-seller transactions is a\npotential element which needs to be addressed in competitive e-commerce\nindustry. The e-commerce industry is currently handling two different trust\napproaches. First approach consists on centralized mechanism where digital\ncredentials/set of rules assembled, called Policy based trust mechanisms .\nSecond approach consists on decentralized trust mechanisms where reputation,\npoints assembled and shared, called Reputation based trust mechanisms. The\ndifference between reputation and policy based trust mechanism will be analyzed\nand recommendations would be proposed to increase trust between buyer and\nseller in e-commerce industry. The integration of trust mechanism is proposed\nthrough mapping process, strength of one mechanism with the weakness of other.\nThe proposed model for integrated mechanism will be presented and illustrated\nhow the proposed model will be used in real world e-commerce industry.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.MM",
      "cs.SI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13303v1",
    "published_date": "2024-06-19 07:47:48 UTC",
    "updated_date": "2024-06-19 07:47:48 UTC"
  },
  {
    "arxiv_id": "2406.13299v1",
    "title": "Empirical Evaluation of Integrated Trust Mechanism to Improve Trust in E-commerce Services",
    "authors": [
      "Siddiqui Muhammad Yasir",
      "Hyunsik Ahn"
    ],
    "abstract": "There are mostly two approaches to tackle trust management worldwide Strong\nand crisp and Soft and Social. We analyze the impact of integrated trust\nmechanism in three different e-commerce services. The trust aspect is a dormant\nelement between potential users and being developed expert or internet systems.\nWe support our integration by preside over an experiment in controlled\nlaboratory environment. The model selected for the experiment is a composite of\npolicy and reputation based trust mechanisms and widely acknowledged in\ne-commerce industry. The integration between policy and trust mechanism was\naccomplished through mapping process, weakness of one brought to a close with\nthe strength of other. Furthermore, experiment has been supervised to validate\nthe effectiveness of implementation by segregating both integrated and\ntraditional trust mechanisms in learning system",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.CY",
      "cs.PF"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13299v1",
    "published_date": "2024-06-19 07:38:51 UTC",
    "updated_date": "2024-06-19 07:38:51 UTC"
  },
  {
    "arxiv_id": "2406.13295v1",
    "title": "Media Forensics and Deepfake Systematic Survey",
    "authors": [
      "Nadeem Jabbar CH",
      "Aqib Saghir",
      "Ayaz Ahmad Meer",
      "Salman Ahmad Sahi",
      "Bilal Hassan",
      "Siddiqui Muhammad Yasir"
    ],
    "abstract": "Deepfake is a generative deep learning algorithm that creates or changes\nfacial features in a very realistic way making it hard to differentiate the\nreal from the fake features It can be used to make movies look better as well\nas to spread false information by imitating famous people In this paper many\ndifferent ways to make a Deepfake are explained analyzed and separated\ncategorically Using Deepfake datasets models are trained and tested for\nreliability through experiments Deepfakes are a type of facial manipulation\nthat allow people to change their entire faces identities attributes and\nexpressions The trends in the available Deepfake datasets are also discussed\nwith a focus on how they have changed Using Deep learning a general Deepfake\ndetection model is made Moreover the problems in making and detecting Deepfakes\nare also mentioned As a result of this survey it is expected that the\ndevelopment of new Deepfake based imaging tools will speed up in the future\nThis survey gives indepth review of methods for manipulating images of face and\nvarious techniques to spot altered face images Four types of facial\nmanipulation are specifically discussed which are attribute manipulation\nexpression swap entire face synthesis and identity swap Across every\nmanipulation category we yield information on manipulation techniques\nsignificant benchmarks for technical evaluation of counterfeit detection\ntechniques available public databases and a summary of the outcomes of all such\nanalyses From all of the topics in the survey we focus on the most recent\ndevelopment of Deepfake showing its advances and obstacles in detecting fake\nimages",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "46 pages, 9 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2406.13295v1",
    "published_date": "2024-06-19 07:33:33 UTC",
    "updated_date": "2024-06-19 07:33:33 UTC"
  },
  {
    "arxiv_id": "2406.13292v3",
    "title": "An interpretable generative multimodal neuroimaging-genomics framework for decoding Alzheimer's disease",
    "authors": [
      "Giorgio Dolci",
      "Federica Cruciani",
      "Md Abdur Rahaman",
      "Anees Abrol",
      "Jiayu Chen",
      "Zening Fu",
      "Ilaria Boscolo Galazzo",
      "Gloria Menegaz",
      "Vince D. Calhoun"
    ],
    "abstract": "\\textbf{Objective:} Alzheimer's disease (AD) is the most prevalent form of\ndementia worldwide, encompassing a prodromal stage known as Mild Cognitive\nImpairment (MCI), where patients may either progress to AD or remain stable.\nThe objective of the work was to capture structural and functional modulations\nof brain structure and function relying on multimodal MRI data and Single\nNucleotide Polymorphisms, also in case of missing views, with the twofold goal\nof classifying AD patients versus healthy controls and detecting MCI\nconverters. % in two distinct tasks, dealing with also missing data.\\\\\n\\textbf{Approach:} We propose a multimodal DL-based classification framework\nwhere a generative module employing Cycle Generative Adversarial Networks was\nintroduced in the latent space for imputing missing data (a common issue of\nmultimodal approaches). Explainable AI method was then used to extract input\nfeatures' relevance allowing for post-hoc validation and enhancing the\ninterpretability of the learned representations. \\textbf{Main results:}\nExperimental results on two tasks, AD detection and MCI conversion, showed that\nour framework reached competitive performance in the state-of-the-art with an\naccuracy of $0.926\\pm0.02$ and $0.711\\pm0.01$ in the two tasks, respectively.\nThe interpretability analysis revealed gray matter modulations in cortical and\nsubcortical brain areas typically associated with AD. Moreover, impairments in\nsensory-motor and visual resting state networks along the disease continuum, as\nwell as genetic mutations defining biological processes linked to endocytosis,\namyloid-beta, and cholesterol, were identified. \\textbf{Significance:} Our\nintegrative and interpretable DL approach shows promising performance for AD\ndetection and MCI prediction while shedding light on important biological\ninsights.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "q-bio.QM",
    "comment": "33 pages, 8 figures (main text + supplementary materials), submitted\n  to a journal",
    "pdf_url": "http://arxiv.org/pdf/2406.13292v3",
    "published_date": "2024-06-19 07:31:47 UTC",
    "updated_date": "2025-02-04 15:36:37 UTC"
  },
  {
    "arxiv_id": "2406.13280v2",
    "title": "Design Optimization of NOMA Aided Multi-STAR-RIS for Indoor Environments: A Convex Approximation Imitated Reinforcement Learning Approach",
    "authors": [
      "Yu Min Park",
      "Sheikh Salman Hassan",
      "Yan Kyaw Tun",
      "Eui-Nam Huh",
      "Walid Saad",
      "Choong Seon Hong"
    ],
    "abstract": "Non-orthogonal multiple access (NOMA) enables multiple users to share the\nsame frequency band, and simultaneously transmitting and reflecting\nreconfigurable intelligent surface (STAR-RIS) provides 360-degree full-space\ncoverage, optimizing both transmission and reflection for improved network\nperformance and dynamic control of the indoor environment. However, deploying\nSTAR-RIS indoors presents challenges in interference mitigation, power\nconsumption, and real-time configuration. In this work, a novel network\narchitecture utilizing multiple access points (APs), STAR-RISs, and NOMA is\nproposed for indoor communication. To address these, we formulate an\noptimization problem involving user assignment, access point (AP) beamforming,\nand STAR-RIS phase control. A decomposition approach is used to solve the\ncomplex problem efficiently, employing a many-to-one matching algorithm for\nuser-AP assignment and K-means clustering for resource management.\nAdditionally, multi-agent deep reinforcement learning (MADRL) is leveraged to\noptimize the control of the STAR-RIS. Within the proposed MADRL framework, a\nnovel approach is introduced in which each decision variable acts as an\nindependent agent, enabling collaborative learning and decision making. The\nMADRL framework is enhanced by incorporating convex approximation (CA), which\naccelerates policy learning through suboptimal solutions from successive convex\napproximation (SCA), leading to faster adaptation and convergence. Simulations\ndemonstrate significant improvements in network utility compared to baseline\napproaches.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "37 pages, 11 figures. arXiv admin note: text overlap with\n  arXiv:2311.08708",
    "pdf_url": "http://arxiv.org/pdf/2406.13280v2",
    "published_date": "2024-06-19 07:17:04 UTC",
    "updated_date": "2024-09-17 15:02:12 UTC"
  },
  {
    "arxiv_id": "2406.13269v1",
    "title": "Investigating Low-Cost LLM Annotation for~Spoken Dialogue Understanding Datasets",
    "authors": [
      "Lucas Druart",
      "Valentin Vielzeuf",
      "Yannick Estève"
    ],
    "abstract": "In spoken Task-Oriented Dialogue (TOD) systems, the choice of the semantic\nrepresentation describing the users' requests is key to a smooth interaction.\nIndeed, the system uses this representation to reason over a database and its\ndomain knowledge to choose its next action. The dialogue course thus depends on\nthe information provided by this semantic representation. While textual\ndatasets provide fine-grained semantic representations, spoken dialogue\ndatasets fall behind. This paper provides insights into automatic enhancement\nof spoken dialogue datasets' semantic representations. Our contributions are\nthree fold: (1) assess the relevance of Large Language Model fine-tuning, (2)\nevaluate the knowledge captured by the produced annotations and (3) highlight\nsemi-automatic annotation implications.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "eess.SP"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13269v1",
    "published_date": "2024-06-19 06:59:57 UTC",
    "updated_date": "2024-06-19 06:59:57 UTC"
  },
  {
    "arxiv_id": "2406.13264v2",
    "title": "WONDERBREAD: A Benchmark for Evaluating Multimodal Foundation Models on Business Process Management Tasks",
    "authors": [
      "Michael Wornow",
      "Avanika Narayan",
      "Ben Viggiano",
      "Ishan S. Khare",
      "Tathagat Verma",
      "Tibor Thompson",
      "Miguel Angel Fuentes Hernandez",
      "Sudharsan Sundar",
      "Chloe Trujillo",
      "Krrish Chawla",
      "Rongfei Lu",
      "Justin Shen",
      "Divya Nagaraj",
      "Joshua Martinez",
      "Vardhan Agrawal",
      "Althea Hudson",
      "Nigam H. Shah",
      "Christopher Re"
    ],
    "abstract": "Existing ML benchmarks lack the depth and diversity of annotations needed for\nevaluating models on business process management (BPM) tasks. BPM is the\npractice of documenting, measuring, improving, and automating enterprise\nworkflows. However, research has focused almost exclusively on one task - full\nend-to-end automation using agents based on multimodal foundation models (FMs)\nlike GPT-4. This focus on automation ignores the reality of how most BPM tools\nare applied today - simply documenting the relevant workflow takes 60% of the\ntime of the typical process optimization project. To address this gap we\npresent WONDERBREAD, the first benchmark for evaluating multimodal FMs on BPM\ntasks beyond automation. Our contributions are: (1) a dataset containing 2928\ndocumented workflow demonstrations; (2) 6 novel BPM tasks sourced from\nreal-world applications ranging from workflow documentation to knowledge\ntransfer to process improvement; and (3) an automated evaluation harness. Our\nbenchmark shows that while state-of-the-art FMs can automatically generate\ndocumentation (e.g. recalling 88% of the steps taken in a video demonstration\nof a workflow), they struggle to re-apply that knowledge towards finer-grained\nvalidation of workflow completion (F1 < 0.3). We hope WONDERBREAD encourages\nthe development of more \"human-centered\" AI tooling for enterprise applications\nand furthers the exploration of multimodal FMs for the broader universe of BPM\ntasks. We publish our dataset and experiments here:\nhttps://github.com/HazyResearch/wonderbread",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13264v2",
    "published_date": "2024-06-19 06:50:15 UTC",
    "updated_date": "2024-10-11 00:06:31 UTC"
  },
  {
    "arxiv_id": "2406.13261v3",
    "title": "BeHonest: Benchmarking Honesty in Large Language Models",
    "authors": [
      "Steffi Chern",
      "Zhulin Hu",
      "Yuqing Yang",
      "Ethan Chern",
      "Yuan Guo",
      "Jiahe Jin",
      "Binjie Wang",
      "Pengfei Liu"
    ],
    "abstract": "Previous works on Large Language Models (LLMs) have mainly focused on\nevaluating their helpfulness or harmlessness. However, honesty, another crucial\nalignment criterion, has received relatively less attention. Dishonest\nbehaviors in LLMs, such as spreading misinformation and defrauding users,\npresent severe risks that intensify as these models approach superintelligent\nlevels. Enhancing honesty in LLMs addresses critical limitations and helps\nuncover latent capabilities that are not readily expressed. This underscores\nthe urgent need for reliable methods and benchmarks to effectively ensure and\nevaluate the honesty of LLMs.\n  In this paper, we introduce BeHonest, a pioneering benchmark specifically\ndesigned to assess honesty in LLMs comprehensively. BeHonest evaluates three\nessential aspects of honesty: awareness of knowledge boundaries, avoidance of\ndeceit, and consistency in responses. Building on this foundation, we designed\n10 scenarios to evaluate and analyze 9 popular LLMs on the market, including\nboth closed-source and open-source models from different model families with\nvaried model sizes. Our findings indicate that there is still significant room\nfor improvement in the honesty of LLMs. We encourage the AI community to\nprioritize honesty alignment in these models, which can harness their full\npotential to benefit society while preventing them from causing harm through\ndeception or inconsistency. Our benchmark and code can be found at:\n\\url{https://github.com/GAIR-NLP/BeHonest}.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13261v3",
    "published_date": "2024-06-19 06:46:59 UTC",
    "updated_date": "2024-07-08 18:29:58 UTC"
  },
  {
    "arxiv_id": "2406.13257v1",
    "title": "Reasoning with trees: interpreting CNNs using hierarchies",
    "authors": [
      "Caroline Mazini Rodrigues",
      "Nicolas Boutry",
      "Laurent Najman"
    ],
    "abstract": "Challenges persist in providing interpretable explanations for neural network\nreasoning in explainable AI (xAI). Existing methods like Integrated Gradients\nproduce noisy maps, and LIME, while intuitive, may deviate from the model's\nreasoning. We introduce a framework that uses hierarchical segmentation\ntechniques for faithful and interpretable explanations of Convolutional Neural\nNetworks (CNNs). Our method constructs model-based hierarchical segmentations\nthat maintain the model's reasoning fidelity and allows both human-centric and\nmodel-centric segmentation. This approach offers multiscale explanations,\naiding bias identification and enhancing understanding of neural network\ndecision-making. Experiments show that our framework, xAiTrees, delivers highly\ninterpretable and faithful model explanations, not only surpassing traditional\nxAI methods but shedding new light on a novel approach to enhancing xAI\ninterpretability. Code at: https://github.com/CarolMazini/reasoning_with_trees .",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13257v1",
    "published_date": "2024-06-19 06:45:19 UTC",
    "updated_date": "2024-06-19 06:45:19 UTC"
  },
  {
    "arxiv_id": "2406.13250v1",
    "title": "LangTopo: Aligning Language Descriptions of Graphs with Tokenized Topological Modeling",
    "authors": [
      "Zhong Guan",
      "Hongke Zhao",
      "Likang Wu",
      "Ming He",
      "Jianpin Fan"
    ],
    "abstract": "Recently, large language models (LLMs) have been widely researched in the\nfield of graph machine learning due to their outstanding abilities in language\ncomprehension and learning. However, the significant gap between natural\nlanguage tasks and topological structure modeling poses a nonnegligible\nchallenge. Specifically, since natural language descriptions are not sufficient\nfor LLMs to understand and process graph-structured data, fine-tuned LLMs\nperform even worse than some traditional GNN models on graph tasks, lacking\ninherent modeling capabilities for graph structures. Existing research overly\nemphasizes LLMs' understanding of semantic information captured by external\nmodels, while inadequately exploring graph topological structure modeling,\nthereby overlooking the genuine capabilities that LLMs lack. Consequently, in\nthis paper, we introduce a new framework, LangTopo, which aligns graph\nstructure modeling with natural language understanding at the token level.\nLangTopo quantifies the graph structure modeling capabilities of GNNs and LLMs\nby constructing a codebook for the graph modality and performs consistency\nmaximization. This process aligns the text description of LLM with the\ntopological modeling of GNN, allowing LLM to learn the ability of GNN to\ncapture graph structures, enabling LLM to handle graph-structured data\nindependently. We demonstrate the effectiveness of our proposed method on\nmultiple datasets.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13250v1",
    "published_date": "2024-06-19 06:20:22 UTC",
    "updated_date": "2024-06-19 06:20:22 UTC"
  },
  {
    "arxiv_id": "2406.13249v2",
    "title": "R^2AG: Incorporating Retrieval Information into Retrieval Augmented Generation",
    "authors": [
      "Fuda Ye",
      "Shuangyin Li",
      "Yongqi Zhang",
      "Lei Chen"
    ],
    "abstract": "Retrieval augmented generation (RAG) has been applied in many scenarios to\naugment large language models (LLMs) with external documents provided by\nretrievers. However, a semantic gap exists between LLMs and retrievers due to\ndifferences in their training objectives and architectures. This misalignment\nforces LLMs to passively accept the documents provided by the retrievers,\nleading to incomprehension in the generation process, where the LLMs are\nburdened with the task of distinguishing these documents using their inherent\nknowledge. This paper proposes R$^2$AG, a novel enhanced RAG framework to fill\nthis gap by incorporating Retrieval information into Retrieval Augmented\nGeneration. Specifically, R$^2$AG utilizes the nuanced features from the\nretrievers and employs a R$^2$-Former to capture retrieval information. Then, a\nretrieval-aware prompting strategy is designed to integrate retrieval\ninformation into LLMs' generation. Notably, R$^2$AG suits low-source scenarios\nwhere LLMs and retrievers are frozen. Extensive experiments across five\ndatasets validate the effectiveness, robustness, and efficiency of R$^2$AG. Our\nanalysis reveals that retrieval information serves as an anchor to aid LLMs in\nthe generation process, thereby filling the semantic gap.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP 2024 Findings",
    "pdf_url": "http://arxiv.org/pdf/2406.13249v2",
    "published_date": "2024-06-19 06:19:48 UTC",
    "updated_date": "2024-10-30 06:41:45 UTC"
  },
  {
    "arxiv_id": "2407.12793v1",
    "title": "Data Collection and Labeling Techniques for Machine Learning",
    "authors": [
      "Qianyu Huang",
      "Tongfang Zhao"
    ],
    "abstract": "Data collection and labeling are critical bottlenecks in the deployment of\nmachine learning applications. With the increasing complexity and diversity of\napplications, the need for efficient and scalable data collection and labeling\ntechniques has become paramount. This paper provides a review of the\nstate-of-the-art methods in data collection, data labeling, and the improvement\nof existing data and models. By integrating perspectives from both the machine\nlearning and data management communities, we aim to provide a holistic view of\nthe current landscape and identify future research directions.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12793v1",
    "published_date": "2024-06-19 06:01:28 UTC",
    "updated_date": "2024-06-19 06:01:28 UTC"
  },
  {
    "arxiv_id": "2406.13236v2",
    "title": "Data Contamination Can Cross Language Barriers",
    "authors": [
      "Feng Yao",
      "Yufan Zhuang",
      "Zihao Sun",
      "Sunan Xu",
      "Animesh Kumar",
      "Jingbo Shang"
    ],
    "abstract": "The opacity in developing large language models (LLMs) is raising growing\nconcerns about the potential contamination of public benchmarks in the\npre-training data. Existing contamination detection methods are typically based\non the text overlap between training and evaluation data, which can be too\nsuperficial to reflect deeper forms of contamination. In this paper, we first\npresent a cross-lingual form of contamination that inflates LLMs' performance\nwhile evading current detection methods, deliberately injected by overfitting\nLLMs on the translated versions of benchmark test sets. Then, we propose\ngeneralization-based approaches to unmask such deeply concealed contamination.\nSpecifically, we examine the LLM's performance change after modifying the\noriginal benchmark by replacing the false answer choices with correct ones from\nother questions. Contaminated models can hardly generalize to such easier\nsituations, where the false choices can be \\emph{not even wrong}, as all\nchoices are correct in their memorization. Experimental results demonstrate\nthat cross-lingual contamination can easily fool existing detection methods,\nbut not ours. In addition, we discuss the potential utilization of\ncross-lingual contamination in interpreting LLMs' working mechanisms and in\npost-training LLMs for enhanced multilingual capabilities. The code and dataset\nwe use can be obtained from \\url{https://github.com/ShangDataLab/Deep-Contam}.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2024 Main camera-ready version",
    "pdf_url": "http://arxiv.org/pdf/2406.13236v2",
    "published_date": "2024-06-19 05:53:27 UTC",
    "updated_date": "2024-10-30 17:59:08 UTC"
  },
  {
    "arxiv_id": "2406.13235v1",
    "title": "Enhancing Collaborative Semantics of Language Model-Driven Recommendations via Graph-Aware Learning",
    "authors": [
      "Zhong Guan",
      "Likang Wu",
      "Hongke Zhao",
      "Ming He",
      "Jianpin Fan"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly prominent in the recommendation\nsystems domain. Existing studies usually utilize in-context learning or\nsupervised fine-tuning on task-specific data to align LLMs into\nrecommendations. However, the substantial bias in semantic spaces between\nlanguage processing tasks and recommendation tasks poses a nonnegligible\nchallenge. Specifically, without the adequate capturing ability of\ncollaborative information, existing modeling paradigms struggle to capture\nbehavior patterns within community groups, leading to LLMs' ineffectiveness in\ndiscerning implicit interaction semantic in recommendation scenarios. To\naddress this, we consider enhancing the learning capability of language\nmodel-driven recommendation models for structured data, specifically by\nutilizing interaction graphs rich in collaborative semantics. We propose a\nGraph-Aware Learning for Language Model-Driven Recommendations (GAL-Rec).\nGAL-Rec enhances the understanding of user-item collaborative semantics by\nimitating the intent of Graph Neural Networks (GNNs) to aggregate multi-hop\ninformation, thereby fully exploiting the substantial learning capacity of LLMs\nto independently address the complex graphs in the recommendation system.\nSufficient experimental results on three real-world datasets demonstrate that\nGAL-Rec significantly enhances the comprehension of collaborative semantics,\nand improves recommendation performance.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "10pages",
    "pdf_url": "http://arxiv.org/pdf/2406.13235v1",
    "published_date": "2024-06-19 05:50:15 UTC",
    "updated_date": "2024-06-19 05:50:15 UTC"
  },
  {
    "arxiv_id": "2406.13233v2",
    "title": "AdaMoE: Token-Adaptive Routing with Null Experts for Mixture-of-Experts Language Models",
    "authors": [
      "Zihao Zeng",
      "Yibo Miao",
      "Hongcheng Gao",
      "Hao Zhang",
      "Zhijie Deng"
    ],
    "abstract": "Mixture of experts (MoE) has become the standard for constructing\nproduction-level large language models (LLMs) due to its promise to boost model\ncapacity without causing significant overheads. Nevertheless, existing MoE\nmethods usually enforce a constant top-k routing for all tokens, which is\narguably restrictive because various tokens (e.g., \"<EOS>\" vs. \"apple\") may\nrequire various numbers of experts for feature abstraction. Lifting such a\nconstraint can help make the most of limited resources and unleash the\npotential of the model for downstream tasks. In this sense, we introduce AdaMoE\nto realize token-adaptive routing for MoE, where different tokens are permitted\nto select a various number of experts. AdaMoE makes minimal modifications to\nthe vanilla MoE with top-k routing -- it simply introduces a fixed number of\nnull experts, which do not consume any FLOPs, to the expert set and increases\nthe value of k. AdaMoE does not force each token to occupy a fixed number of\nnull experts but ensures the average usage of the null experts with a\nload-balancing loss, leading to an adaptive number of null/true experts used by\neach token. AdaMoE exhibits a strong resemblance to MoEs with expert choice\nrouting while allowing for trivial auto-regressive modeling. AdaMoE is easy to\nimplement and can be effectively applied to pre-trained (MoE-)LLMs. Extensive\nstudies show that AdaMoE can reduce average expert load (FLOPs) while achieving\nsuperior performance. For example, on the ARC-C dataset, applying our method to\nfine-tuning Mixtral-8x7B can reduce FLOPs by 14.5% while increasing accuracy by\n1.69%.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Findings of EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.13233v2",
    "published_date": "2024-06-19 05:47:10 UTC",
    "updated_date": "2024-10-14 03:20:02 UTC"
  },
  {
    "arxiv_id": "2406.13232v1",
    "title": "Towards Robust Evaluation: A Comprehensive Taxonomy of Datasets and Metrics for Open Domain Question Answering in the Era of Large Language Models",
    "authors": [
      "Akchay Srivastava",
      "Atif Memon"
    ],
    "abstract": "Open Domain Question Answering (ODQA) within natural language processing\ninvolves building systems that answer factual questions using large-scale\nknowledge corpora. Recent advances stem from the confluence of several factors,\nsuch as large-scale training datasets, deep learning techniques, and the rise\nof large language models. High-quality datasets are used to train models on\nrealistic scenarios and enable the evaluation of the system on potentially\nunseen data. Standardized metrics facilitate comparisons between different ODQA\nsystems, allowing researchers to objectively track advancements in the field.\nOur study presents a thorough examination of the current landscape of ODQA\nbenchmarking by reviewing 52 datasets and 20 evaluation techniques across\ntextual and multimodal modalities. We introduce a novel taxonomy for ODQA\ndatasets that incorporates both the modality and difficulty of the question\ntypes. Additionally, we present a structured organization of ODQA evaluation\nmetrics along with a critical analysis of their inherent trade-offs. Our study\naims to empower researchers by providing a framework for the robust evaluation\nof modern question-answering systems. We conclude by identifying the current\nchallenges and outlining promising avenues for future research and development.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "22 pages, 13 tables, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.13232v1",
    "published_date": "2024-06-19 05:43:02 UTC",
    "updated_date": "2024-06-19 05:43:02 UTC"
  },
  {
    "arxiv_id": "2406.13229v1",
    "title": "Probing the Emergence of Cross-lingual Alignment during LLM Training",
    "authors": [
      "Hetong Wang",
      "Pasquale Minervini",
      "Edoardo M. Ponti"
    ],
    "abstract": "Multilingual Large Language Models (LLMs) achieve remarkable levels of\nzero-shot cross-lingual transfer performance. We speculate that this is\npredicated on their ability to align languages without explicit supervision\nfrom parallel sentences. While representations of translationally equivalent\nsentences in different languages are known to be similar after convergence,\nhowever, it remains unclear how such cross-lingual alignment emerges during\npre-training of LLMs. Our study leverages intrinsic probing techniques, which\nidentify which subsets of neurons encode linguistic features, to correlate the\ndegree of cross-lingual neuron overlap with the zero-shot cross-lingual\ntransfer performance for a given model. In particular, we rely on checkpoints\nof BLOOM, a multilingual autoregressive LLM, across different training steps\nand model scales. We observe a high correlation between neuron overlap and\ndownstream performance, which supports our hypothesis on the conditions leading\nto effective cross-lingual transfer. Interestingly, we also detect a\ndegradation of both implicit alignment and multilingual abilities in certain\nphases of the pre-training process, providing new insights into the\nmultilingual pretraining dynamics.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to Findings of the Association for Computational\n  Linguistics: ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.13229v1",
    "published_date": "2024-06-19 05:31:59 UTC",
    "updated_date": "2024-06-19 05:31:59 UTC"
  },
  {
    "arxiv_id": "2406.13228v1",
    "title": "AGSOA:Graph Neural Network Targeted Attack Based on Average Gradient and Structure Optimization",
    "authors": [
      "Yang Chen",
      "Bin Zhou"
    ],
    "abstract": "Graph Neural Networks(GNNs) are vulnerable to adversarial attack that cause\nperformance degradation by adding small perturbations to the graph.\nGradient-based attacks are one of the most commonly used methods and have\nachieved good performance in many attack scenarios. However, current gradient\nattacks face the problems of easy to fall into local optima and poor attack\ninvisibility. Specifically, most gradient attacks use greedy strategies to\ngenerate perturbations, which tend to fall into local optima leading to\nunderperformance of the attack. In addition, many attacks only consider the\neffectiveness of the attack and ignore the invisibility of the attack, making\nthe attacks easily exposed leading to failure. To address the above problems,\nthis paper proposes an attack on GNNs, called AGSOA, which consists of an\naverage gradient calculation and a structre optimization module. In the average\ngradient calculation module, we compute the average of the gradient information\nover all moments to guide the attack to generate perturbed edges, which\nstabilizes the direction of the attack update and gets rid of undesirable local\nmaxima. In the structure optimization module, we calculate the similarity and\nhomogeneity of the target node's with other nodes to adjust the graph structure\nso as to improve the invisibility and transferability of the attack. Extensive\nexperiments on three commonly used datasets show that AGSOA improves the\nmisclassification rate by 2$\\%$-8$\\%$ compared to other state-of-the-art\nmodels.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13228v1",
    "published_date": "2024-06-19 05:29:20 UTC",
    "updated_date": "2024-06-19 05:29:20 UTC"
  },
  {
    "arxiv_id": "2407.09517v1",
    "title": "Is GPT-4 conscious?",
    "authors": [
      "Izak Tait",
      "Joshua Bensemann",
      "Ziqi Wang"
    ],
    "abstract": "GPT-4 is often heralded as a leading commercial AI offering, sparking debates\nover its potential as a steppingstone toward artificial general intelligence.\nBut does it possess consciousness? This paper investigates this key question\nusing the nine qualitative measurements of the Building Blocks theory. GPT-4's\ndesign, architecture and implementation are compared to each of the building\nblocks of consciousness to determine whether it has achieved the requisite\nmilestones to be classified as conscious or, if not, how close to consciousness\nGPT-4 is. Our assessment is that, while GPT-4 in its native configuration is\nnot currently conscious, current technological research and development is\nsufficient to modify GPT-4 to have all the building blocks of consciousness.\nConsequently, we argue that the emergence of a conscious AI model is plausible\nin the near term. The paper concludes with a comprehensive discussion of the\nethical implications and societal ramifications of engineering conscious AI\nentities.",
    "categories": [
      "cs.AI",
      "q-bio.NC"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted for publication in the Journal of AI Consciousness",
    "pdf_url": "http://arxiv.org/pdf/2407.09517v1",
    "published_date": "2024-06-19 05:26:55 UTC",
    "updated_date": "2024-06-19 05:26:55 UTC"
  },
  {
    "arxiv_id": "2406.13225v1",
    "title": "Communication-Efficient Federated Knowledge Graph Embedding with Entity-Wise Top-K Sparsification",
    "authors": [
      "Xiaoxiong Zhang",
      "Zhiwei Zeng",
      "Xin Zhou",
      "Dusit Niyato",
      "Zhiqi Shen"
    ],
    "abstract": "Federated Knowledge Graphs Embedding learning (FKGE) encounters challenges in\ncommunication efficiency stemming from the considerable size of parameters and\nextensive communication rounds. However, existing FKGE methods only focus on\nreducing communication rounds by conducting multiple rounds of local training\nin each communication round, and ignore reducing the size of parameters\ntransmitted within each communication round. To tackle the problem, we first\nfind that universal reduction in embedding precision across all entities during\ncompression can significantly impede convergence speed, underscoring the\nimportance of maintaining embedding precision. We then propose bidirectional\ncommunication-efficient FedS based on Entity-Wise Top-K Sparsification\nstrategy. During upload, clients dynamically identify and upload only the Top-K\nentity embeddings with the greater changes to the server. During download, the\nserver first performs personalized embedding aggregation for each client. It\nthen identifies and transmits the Top-K aggregated embeddings to each client.\nBesides, an Intermittent Synchronization Mechanism is used by FedS to mitigate\nnegative effect of embedding inconsistency among shared entities of clients\ncaused by heterogeneity of Federated Knowledge Graph. Extensive experiments\nacross three datasets showcase that FedS significantly enhances communication\nefficiency with negligible (even no) performance degradation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13225v1",
    "published_date": "2024-06-19 05:26:02 UTC",
    "updated_date": "2024-06-19 05:26:02 UTC"
  },
  {
    "arxiv_id": "2406.13216v3",
    "title": "CombAlign: Enhancing Model Expressiveness in Unsupervised Graph Alignment",
    "authors": [
      "Songyang Chen",
      "Yu Liu",
      "Lei Zou",
      "Zexuan Wang",
      "Youfang Lin"
    ],
    "abstract": "Unsupervised graph alignment finds the node correspondence between a pair of\nattributed graphs by only exploiting graph structure and node features. One\ncategory of recent studies first computes the node representation and then\nmatches nodes with the largest embedding-based similarity, while the other\ncategory reduces the problem to optimal transport (OT) via Gromov-Wasserstein\nlearning. However, it remains largely unexplored in the model expressiveness,\nas well as how theoretical expressivity impacts prediction accuracy. We\ninvestigate the model expressiveness from two aspects. First, we characterize\nthe model's discriminative power in distinguishing matched and unmatched node\npairs across two graphs. Second, we study the model's capability of\nguaranteeing node matching properties such as one-to-one matching and mutual\nalignment. Motivated by our theoretical analysis, we put forward a hybrid\napproach named CombAlign with stronger expressive power. Specifically, we\nenable cross-dimensional feature interaction for OT-based learning and propose\nan embedding-based method inspired by the Weisfeiler-Lehman test. We also apply\nnon-uniform marginals obtained from the embedding-based modules to OT as priors\nfor more expressiveness. Based on that, we propose a traditional\nalgorithm-based refinement, which combines our OT and embedding-based\npredictions using the ensemble learning strategy and reduces the problem to\nmaximum weight matching. With carefully designed edge weights, we ensure those\nmatching properties and further enhance prediction accuracy. By extensive\nexperiments, we demonstrate a significant improvement of 14.5% in alignment\naccuracy compared to state-of-the-art approaches and confirm the soundness of\nour theoretical analysis.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.13216v3",
    "published_date": "2024-06-19 04:57:35 UTC",
    "updated_date": "2025-05-06 16:16:42 UTC"
  },
  {
    "arxiv_id": "2406.13215v2",
    "title": "Neural Residual Diffusion Models for Deep Scalable Vision Generation",
    "authors": [
      "Zhiyuan Ma",
      "Liangliang Zhao",
      "Biqing Qi",
      "Bowen Zhou"
    ],
    "abstract": "The most advanced diffusion models have recently adopted increasingly deep\nstacked networks (e.g., U-Net or Transformer) to promote the generative\nemergence capabilities of vision generation models similar to large language\nmodels (LLMs). However, progressively deeper stacked networks will intuitively\ncause numerical propagation errors and reduce noisy prediction capabilities on\ngenerative data, which hinders massively deep scalable training of vision\ngeneration models. In this paper, we first uncover the nature that neural\nnetworks being able to effectively perform generative denoising lies in the\nfact that the intrinsic residual unit has consistent dynamic property with the\ninput signal's reverse diffusion process, thus supporting excellent generative\nabilities. Afterwards, we stand on the shoulders of two common types of deep\nstacked networks to propose a unified and massively scalable Neural Residual\nDiffusion Models framework (Neural-RDM for short), which is a simple yet\nmeaningful change to the common architecture of deep generative networks by\nintroducing a series of learnable gated residual parameters that conform to the\ngenerative dynamics. Experimental results on various generative tasks show that\nthe proposed neural residual models obtain state-of-the-art scores on image's\nand video's generative benchmarks. Rigorous theoretical proofs and extensive\nexperiments also demonstrate the advantages of this simple gated residual\nmechanism consistent with dynamic modeling in improving the fidelity and\nconsistency of generated content and supporting large-scale scalable training.\nCode is available at https://github.com/Anonymous/Neural-RDM.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13215v2",
    "published_date": "2024-06-19 04:57:18 UTC",
    "updated_date": "2024-07-21 15:55:34 UTC"
  },
  {
    "arxiv_id": "2406.13213v2",
    "title": "Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata",
    "authors": [
      "Mykhailo Poliakov",
      "Nadiya Shvai"
    ],
    "abstract": "The retrieval-augmented generation (RAG) enables retrieval of relevant\ninformation from an external knowledge source and allows large language models\n(LLMs) to answer queries over previously unseen document collections. However,\nit was demonstrated that traditional RAG applications perform poorly in\nanswering multi-hop questions, which require retrieving and reasoning over\nmultiple elements of supporting evidence. We introduce a new method called\nMulti-Meta-RAG, which uses database filtering with LLM-extracted metadata to\nimprove the RAG selection of the relevant documents from various sources,\nrelevant to the question. While database filtering is specific to a set of\nquestions from a particular domain and format, we found out that Multi-Meta-RAG\ngreatly improves the results on the MultiHop-RAG benchmark. The code is\navailable at https://github.com/mxpoliakov/Multi-Meta-RAG.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ICTERI 2024 Posters Track",
    "pdf_url": "http://arxiv.org/pdf/2406.13213v2",
    "published_date": "2024-06-19 04:53:48 UTC",
    "updated_date": "2024-08-19 11:38:14 UTC"
  },
  {
    "arxiv_id": "2406.13210v2",
    "title": "Surgical Triplet Recognition via Diffusion Model",
    "authors": [
      "Daochang Liu",
      "Axel Hu",
      "Mubarak Shah",
      "Chang Xu"
    ],
    "abstract": "Surgical triplet recognition is an essential building block to enable\nnext-generation context-aware operating rooms. The goal is to identify the\ncombinations of instruments, verbs, and targets presented in surgical video\nframes. In this paper, we propose DiffTriplet, a new generative framework for\nsurgical triplet recognition employing the diffusion model, which predicts\nsurgical triplets via iterative denoising. To handle the challenge of triplet\nassociation, two unique designs are proposed in our diffusion framework, i.e.,\nassociation learning and association guidance. During training, we optimize the\nmodel in the joint space of triplets and individual components to capture the\ndependencies among them. At inference, we integrate association constraints\ninto each update of the iterative denoising process, which refines the triplet\nprediction using the information of individual components. Experiments on the\nCholecT45 and CholecT50 datasets show the superiority of the proposed method in\nachieving a new state-of-the-art performance for surgical triplet recognition.\nOur codes will be released.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13210v2",
    "published_date": "2024-06-19 04:43:41 UTC",
    "updated_date": "2024-06-24 08:22:40 UTC"
  },
  {
    "arxiv_id": "2406.13193v1",
    "title": "PRESTO: Progressive Pretraining Enhances Synthetic Chemistry Outcomes",
    "authors": [
      "He Cao",
      "Yanjun Shao",
      "Zhiyuan Liu",
      "Zijing Liu",
      "Xiangru Tang",
      "Yuan Yao",
      "Yu Li"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have seen growing adoption across\nvarious scientific disciplines. These advancements encourage the investigation\nof molecule-text modeling within synthetic chemistry, a field dedicated to\ndesigning and conducting chemical reactions to synthesize new compounds with\ndesired properties and applications. Current approaches, however, often neglect\nthe critical role of multiple molecule graph interaction in understanding\nchemical reactions, leading to suboptimal performance in synthetic chemistry\ntasks. This study introduces PRESTO(Progressive Pretraining Enhances Synthetic\nChemistry Outcomes), a new framework that bridges the molecule-text modality\ngap by integrating a comprehensive benchmark of pretraining strategies and\ndataset configurations. It progressively improves multimodal LLMs through\ncross-modal alignment and multi-graph understanding. Our extensive experiments\ndemonstrate that PRESTO offers competitive results in downstream synthetic\nchemistry tasks. The code can be found at https://github.com/IDEA-XL/PRESTO.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "physics.chem-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13193v1",
    "published_date": "2024-06-19 03:59:46 UTC",
    "updated_date": "2024-06-19 03:59:46 UTC"
  },
  {
    "arxiv_id": "2406.13179v1",
    "title": "Global-Local Convolution with Spiking Neural Networks for Energy-efficient Keyword Spotting",
    "authors": [
      "Shuai Wang",
      "Dehao Zhang",
      "Kexin Shi",
      "Yuchen Wang",
      "Wenjie Wei",
      "Jibin Wu",
      "Malu Zhang"
    ],
    "abstract": "Thanks to Deep Neural Networks (DNNs), the accuracy of Keyword Spotting (KWS)\nhas made substantial progress. However, as KWS systems are usually implemented\non edge devices, energy efficiency becomes a critical requirement besides\nperformance. Here, we take advantage of spiking neural networks' energy\nefficiency and propose an end-to-end lightweight KWS model. The model consists\nof two innovative modules: 1) Global-Local Spiking Convolution (GLSC) module\nand 2) Bottleneck-PLIF module. Compared to the hand-crafted feature extraction\nmethods, the GLSC module achieves speech feature extraction that is sparser,\nmore energy-efficient, and yields better performance. The Bottleneck-PLIF\nmodule further processes the signals from GLSC with the aim to achieve higher\naccuracy with fewer parameters. Extensive experiments are conducted on the\nGoogle Speech Commands Dataset (V1 and V2). The results show our method\nachieves competitive performance among SNN-based KWS models with fewer\nparameters.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.NE",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13179v1",
    "published_date": "2024-06-19 03:19:25 UTC",
    "updated_date": "2024-06-19 03:19:25 UTC"
  },
  {
    "arxiv_id": "2406.13175v2",
    "title": "Sparse High Rank Adapters",
    "authors": [
      "Kartikeya Bhardwaj",
      "Nilesh Prasad Pandey",
      "Sweta Priyadarshi",
      "Viswanath Ganapathy",
      "Shreya Kadambi",
      "Rafael Esteves",
      "Shubhankar Borse",
      "Paul Whatmough",
      "Risheek Garrepalli",
      "Mart Van Baalen",
      "Harris Teague",
      "Markus Nagel"
    ],
    "abstract": "Low Rank Adaptation (LoRA) has gained massive attention in the recent\ngenerative AI research. One of the main advantages of LoRA is its ability to be\nfused with pretrained models, adding no overhead during inference. However,\nfrom a mobile deployment standpoint, we can either avoid inference overhead in\nthe fused mode but lose the ability to switch adapters rapidly, or suffer\nsignificant (up to 30% higher) inference latency while enabling rapid switching\nin the unfused mode. LoRA also exhibits concept-loss when multiple adapters are\nused concurrently. In this paper, we propose Sparse High Rank Adapters (SHiRA),\na new paradigm which incurs no inference overhead, enables rapid switching, and\nsignificantly reduces concept-loss. Specifically, SHiRA can be trained by\ndirectly tuning only 1-2% of the base model weights while leaving others\nunchanged. This results in a highly sparse adapter which can be switched\ndirectly in the fused mode. We further provide theoretical and empirical\ninsights on how high sparsity in SHiRA can aid multi-adapter fusion by reducing\nconcept loss. Our extensive experiments on LVMs and LLMs demonstrate that\nfinetuning only a small fraction of the parameters in the base model\nsignificantly outperforms LoRA while enabling both rapid switching and\nmulti-adapter fusion. Finally, we provide a latency- and memory-efficient SHiRA\nimplementation based on Parameter-Efficient Finetuning (PEFT) Library which\ntrains at nearly the same speed as LoRA while consuming up to 16% lower peak\nGPU memory, thus making SHiRA easy to adopt for practical use cases. To\ndemonstrate rapid switching benefits during inference, we show that loading\nSHiRA on a base model can be 5x-16x faster than LoRA fusion on a CPU.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.13175v2",
    "published_date": "2024-06-19 03:13:11 UTC",
    "updated_date": "2025-01-27 01:54:51 UTC"
  },
  {
    "arxiv_id": "2406.13173v3",
    "title": "Biomedical Visual Instruction Tuning with Clinician Preference Alignment",
    "authors": [
      "Hejie Cui",
      "Lingjun Mao",
      "Xin Liang",
      "Jieyu Zhang",
      "Hui Ren",
      "Quanzheng Li",
      "Xiang Li",
      "Carl Yang"
    ],
    "abstract": "Recent advancements in multimodal foundation models have showcased impressive\ncapabilities in understanding and reasoning with visual and textual\ninformation. Adapting these foundation models trained for general usage to\nspecialized domains like biomedicine requires large-scale domain-specific\ninstruction datasets. While existing works have explored curating such datasets\nautomatically, the resultant datasets are not explicitly aligned with domain\nexpertise. In this work, we propose a data-centric framework, Biomedical Visual\nInstruction Tuning with Clinician Preference Alignment (BioMed-VITAL), that\nincorporates clinician preferences into both stages of generating and selecting\ninstruction data for tuning biomedical multimodal foundation models. First,\nduring the generation stage, we prompt the GPT-4V generator with a diverse set\nof clinician-selected demonstrations for preference-aligned data candidate\ngeneration. Then, during the selection phase, we train a separate selection\nmodel, which explicitly distills clinician and policy-guided model preferences\ninto a rating function to select high-quality data for medical instruction\ntuning. Results show that the model tuned with the instruction-following data\nfrom our method demonstrates a significant improvement in open visual chat\n(18.5% relatively) and medical VQA (win rate up to 81.73%). Our\ninstruction-following data and models are available at BioMed-VITAL.github.io.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "68T50, 68T45, 68T37, 68T05, 68T07, 68T09,",
      "I.2.7; I.2.6; I.2.10"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13173v3",
    "published_date": "2024-06-19 03:07:33 UTC",
    "updated_date": "2024-07-16 05:56:05 UTC"
  },
  {
    "arxiv_id": "2406.13170v2",
    "title": "Amphista: Bi-directional Multi-head Decoding for Accelerating LLM Inference",
    "authors": [
      "Zeping Li",
      "Xinlong Yang",
      "Ziheng Gao",
      "Ji Liu",
      "Guanchen Li",
      "Zhuang Liu",
      "Dong Li",
      "Jinzhang Peng",
      "Lu Tian",
      "Emad Barsoum"
    ],
    "abstract": "Large Language Models (LLMs) inherently use autoregressive decoding, which\nlacks parallelism in inference and results in significantly slow inference\nspeed. While methods such as Medusa constructs parallelized heads, they lack\nadequate information interaction across different prediction positions. To\novercome this limitation, we introduce Amphista, an enhanced speculative\ndecoding framework that builds upon Medusa. Specifically, Amphista models an\nAuto-embedding Block capable of parallel inference, incorporating\nbi-directional attention to enable interaction between different drafting\nheads. Additionally, Amphista integrates Staged Adaptation Layers, which ensure\na seamless transition of semantic information from the target model's\nautoregressive inference to the drafting heads' non-autoregressive inference,\neffectively achieving paradigm shift and feature fusion. Experimental results\non Vicuna models using MT-Bench and Spec-Bench demonstrate that Amphista\nachieves substantial acceleration while maintaining generation quality. On\nMT-Bench, Amphista delivers up to 2.75$\\times$ speedup over vanilla\nautoregressive decoding and 1.40$\\times$ over Medusa on Vicuna 33B in\nwall-clock time.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13170v2",
    "published_date": "2024-06-19 02:53:39 UTC",
    "updated_date": "2024-10-18 04:13:05 UTC"
  },
  {
    "arxiv_id": "2406.13165v2",
    "title": "Cardiac Copilot: Automatic Probe Guidance for Echocardiography with World Model",
    "authors": [
      "Haojun Jiang",
      "Zhenguo Sun",
      "Ning Jia",
      "Meng Li",
      "Yu Sun",
      "Shaqi Luo",
      "Shiji Song",
      "Gao Huang"
    ],
    "abstract": "Echocardiography is the only technique capable of real-time imaging of the\nheart and is vital for diagnosing the majority of cardiac diseases. However,\nthere is a severe shortage of experienced cardiac sonographers, due to the\nheart's complex structure and significant operational challenges. To mitigate\nthis situation, we present a Cardiac Copilot system capable of providing\nreal-time probe movement guidance to assist less experienced sonographers in\nconducting freehand echocardiography. This system can enable non-experts,\nespecially in primary departments and medically underserved areas, to perform\ncardiac ultrasound examinations, potentially improving global healthcare\ndelivery. The core innovation lies in proposing a data-driven world model,\nnamed Cardiac Dreamer, for representing cardiac spatial structures. This world\nmodel can provide structure features of any cardiac planes around the current\nprobe position in the latent space, serving as an precise navigation map for\nautonomous plane localization. We train our model with real-world ultrasound\ndata and corresponding probe motion from 110 routine clinical scans with 151K\nsample pairs by three certified sonographers. Evaluations on three standard\nplanes with 37K sample pairs demonstrate that the world model can reduce\nnavigation errors by up to 33\\% and exhibit more stable performance.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "eess.IV",
    "comment": "Accepted by MICCAI2024",
    "pdf_url": "http://arxiv.org/pdf/2406.13165v2",
    "published_date": "2024-06-19 02:42:29 UTC",
    "updated_date": "2024-10-21 06:25:57 UTC"
  },
  {
    "arxiv_id": "2406.13163v1",
    "title": "LLMatDesign: Autonomous Materials Discovery with Large Language Models",
    "authors": [
      "Shuyi Jia",
      "Chao Zhang",
      "Victor Fung"
    ],
    "abstract": "Discovering new materials can have significant scientific and technological\nimplications but remains a challenging problem today due to the enormity of the\nchemical space. Recent advances in machine learning have enabled data-driven\nmethods to rapidly screen or generate promising materials, but these methods\nstill depend heavily on very large quantities of training data and often lack\nthe flexibility and chemical understanding often desired in materials\ndiscovery. We introduce LLMatDesign, a novel language-based framework for\ninterpretable materials design powered by large language models (LLMs).\nLLMatDesign utilizes LLM agents to translate human instructions, apply\nmodifications to materials, and evaluate outcomes using provided tools. By\nincorporating self-reflection on its previous decisions, LLMatDesign adapts\nrapidly to new tasks and conditions in a zero-shot manner. A systematic\nevaluation of LLMatDesign on several materials design tasks, in silico,\nvalidates LLMatDesign's effectiveness in developing new materials with\nuser-defined target properties in the small data regime. Our framework\ndemonstrates the remarkable potential of autonomous LLM-guided materials\ndiscovery in the computational setting and towards self-driving laboratories in\nthe future.",
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cond-mat.mtrl-sci",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13163v1",
    "published_date": "2024-06-19 02:35:02 UTC",
    "updated_date": "2024-06-19 02:35:02 UTC"
  },
  {
    "arxiv_id": "2406.13162v1",
    "title": "AntibodyFlow: Normalizing Flow Model for Designing Antibody Complementarity-Determining Regions",
    "authors": [
      "Bohao Xu",
      "Yanbo Wang",
      "Wenyu Chen",
      "Shimin Shan"
    ],
    "abstract": "Therapeutic antibodies have been extensively studied in drug discovery and\ndevelopment in the past decades. Antibodies are specialized protective proteins\nthat bind to antigens in a lock-to-key manner. The binding strength/affinity\nbetween an antibody and a specific antigen is heavily determined by the\ncomplementarity-determining regions (CDRs) on the antibodies. Existing machine\nlearning methods cast in silico development of CDRs as either sequence or 3D\ngraph (with a single chain) generation tasks and have achieved initial success.\nHowever, with CDR loops having specific geometry shapes, learning the 3D\ngeometric structures of CDRs remains a challenge. To address this issue, we\npropose AntibodyFlow, a 3D flow model to design antibody CDR loops.\nSpecifically, AntibodyFlow first constructs the distance matrix, then predicts\namino acids conditioned on the distance matrix. Also, AntibodyFlow conducts\nconstraint learning and constrained generation to ensure valid 3D structures.\nExperimental results indicate that AntibodyFlow outperforms the best baseline\nconsistently with up to 16.0% relative improvement in validity rate and 24.3%\nrelative reduction in geometric graph level error (root mean square deviation,\nRMSD).",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13162v1",
    "published_date": "2024-06-19 02:31:23 UTC",
    "updated_date": "2024-06-19 02:31:23 UTC"
  },
  {
    "arxiv_id": "2406.13161v1",
    "title": "APPL: A Prompt Programming Language for Harmonious Integration of Programs and Large Language Model Prompts",
    "authors": [
      "Honghua Dong",
      "Qidong Su",
      "Yubo Gao",
      "Zhaoyu Li",
      "Yangjun Ruan",
      "Gennady Pekhimenko",
      "Chris J. Maddison",
      "Xujie Si"
    ],
    "abstract": "Large Language Models (LLMs) have become increasingly capable of handling\ndiverse tasks with the aid of well-crafted prompts and integration of external\ntools, but as task complexity rises, the workflow involving LLMs can be\ncomplicated and thus challenging to implement and maintain. To address this\nchallenge, we propose APPL, A Prompt Programming Language that acts as a bridge\nbetween computer programs and LLMs, allowing seamless embedding of prompts into\nPython functions, and vice versa. APPL provides an intuitive and Python-native\nsyntax, an efficient parallelized runtime with asynchronous semantics, and a\ntracing module supporting effective failure diagnosis and replaying without\nextra costs. We demonstrate that APPL programs are intuitive, concise, and\nefficient through three representative scenarios: Chain-of-Thought with\nself-consistency (CoT-SC), ReAct tool use agent, and multi-agent chat.\nExperiments on three parallelizable workflows further show that APPL can\neffectively parallelize independent LLM calls, with a significant speedup ratio\nthat almost matches the estimation.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.PL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13161v1",
    "published_date": "2024-06-19 02:29:59 UTC",
    "updated_date": "2024-06-19 02:29:59 UTC"
  },
  {
    "arxiv_id": "2406.13155v3",
    "title": "Convolutional Kolmogorov-Arnold Networks",
    "authors": [
      "Alexander Dylan Bodner",
      "Antonio Santiago Tepsich",
      "Jack Natan Spolski",
      "Santiago Pourteau"
    ],
    "abstract": "In this paper, we present Convolutional Kolmogorov-Arnold Networks, a novel\narchitecture that integrates the learnable spline-based activation functions of\nKolmogorov-Arnold Networks (KANs) into convolutional layers. By replacing\ntraditional fixed-weight kernels with learnable non-linear functions,\nConvolutional KANs offer a significant improvement in parameter efficiency and\nexpressive power over standard Convolutional Neural Networks (CNNs). We\nempirically evaluate Convolutional KANs on the Fashion-MNIST dataset,\ndemonstrating competitive accuracy with up to 50% fewer parameters compared to\nbaseline classic convolutions. This suggests that the KAN Convolution can\neffectively capture complex spatial relationships with fewer resources,\noffering a promising alternative for parameter-efficient deep learning models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13155v3",
    "published_date": "2024-06-19 02:09:44 UTC",
    "updated_date": "2025-03-31 12:55:11 UTC"
  },
  {
    "arxiv_id": "2406.13154v3",
    "title": "Conditional score-based diffusion models for solving inverse problems in mechanics",
    "authors": [
      "Agnimitra Dasgupta",
      "Harisankar Ramaswamy",
      "Javier Murgoitio-Esandi",
      "Ken Foo",
      "Runze Li",
      "Qifa Zhou",
      "Brendan Kennedy",
      "Assad Oberai"
    ],
    "abstract": "We propose a framework to perform Bayesian inference using conditional\nscore-based diffusion models to solve a class of inverse problems in mechanics\ninvolving the inference of a specimen's spatially varying material properties\nfrom noisy measurements of its mechanical response to loading. Conditional\nscore-based diffusion models are generative models that learn to approximate\nthe score function of a conditional distribution using samples from the joint\ndistribution. More specifically, the score functions corresponding to multiple\nrealizations of the measurement are approximated using a single neural network,\nthe so-called score network, which is subsequently used to sample the posterior\ndistribution using an appropriate Markov chain Monte Carlo scheme based on\nLangevin dynamics. Training the score network only requires simulating the\nforward model. Hence, the proposed approach can accommodate black-box forward\nmodels and complex measurement noise. Moreover, once the score network has been\ntrained, it can be re-used to solve the inverse problem for different\nrealizations of the measurements. We demonstrate the efficacy of the proposed\napproach on a suite of high-dimensional inverse problems in mechanics that\ninvolve inferring heterogeneous material properties from noisy measurements.\nSome examples we consider involve synthetic data, while others include data\ncollected from actual elastography experiments. Further, our applications\ndemonstrate that the proposed approach can handle different measurement\nmodalities, complex patterns in the inferred quantities, non-Gaussian and\nnon-additive noise models, and nonlinear black-box forward models. The results\nshow that the proposed framework can solve large-scale physics-based inverse\nproblems efficiently.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13154v3",
    "published_date": "2024-06-19 02:09:15 UTC",
    "updated_date": "2024-08-29 17:47:18 UTC"
  },
  {
    "arxiv_id": "2406.13144v5",
    "title": "DialSim: A Real-Time Simulator for Evaluating Long-Term Multi-Party Dialogue Understanding of Conversation Systems",
    "authors": [
      "Jiho Kim",
      "Woosog Chay",
      "Hyeonji Hwang",
      "Daeun Kyung",
      "Hyunseung Chung",
      "Eunbyeol Cho",
      "Yohan Jo",
      "Edward Choi"
    ],
    "abstract": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced the capabilities of conversation systems, making them applicable to\nvarious fields (e.g., education). Despite their progress, the evaluation of the\nsystems often overlooks the complexities of real-world conversations, such as\nreal-time interactions, multi-party dialogues, and extended contextual\ndependencies. To bridge this gap, we introduce DialSim, a real-time dialogue\nsimulator. In this simulator, a conversation system is assigned the role of a\ncharacter from popular TV shows, requiring it to respond to spontaneous\nquestions using past dialogue information and to distinguish between known and\nunknown information. Key features of DialSim include assessing the system's\nability to respond within a reasonable time limit, handling long-term\nmulti-party dialogues, and evaluating performance under randomized questioning\nwith LongDialQA, a novel, high-quality question-answering dataset. Our\nexperiments using DialSim reveal the strengths and weaknesses of the latest\nconversation systems, offering valuable insights for future advancements in\nconversational AI. DialSim is available at https://dialsim.github.io/.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13144v5",
    "published_date": "2024-06-19 01:37:10 UTC",
    "updated_date": "2025-02-17 11:17:41 UTC"
  },
  {
    "arxiv_id": "2406.13138v2",
    "title": "Large Language Models are Biased Because They Are Large Language Models",
    "authors": [
      "Philip Resnik"
    ],
    "abstract": "This position paper's primary goal is to provoke thoughtful discussion about\nthe relationship between bias and fundamental properties of large language\nmodels. I do this by seeking to convince the reader that harmful biases are an\ninevitable consequence arising from the design of any large language model as\nLLMs are currently formulated. To the extent that this is true, it suggests\nthat the problem of harmful bias cannot be properly addressed without a serious\nreconsideration of AI driven by LLMs, going back to the foundational\nassumptions underlying their design.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "To appear in Computational Linguistics. Significantly revised since\n  the prior arXiv version. This preprint has 22 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.13138v2",
    "published_date": "2024-06-19 01:08:03 UTC",
    "updated_date": "2025-03-13 20:32:18 UTC"
  },
  {
    "arxiv_id": "2406.13127v2",
    "title": "Oralytics Reinforcement Learning Algorithm",
    "authors": [
      "Anna L. Trella",
      "Kelly W. Zhang",
      "Stephanie M. Carpenter",
      "David Elashoff",
      "Zara M. Greer",
      "Inbal Nahum-Shani",
      "Dennis Ruenger",
      "Vivek Shetty",
      "Susan A. Murphy"
    ],
    "abstract": "Dental disease is still one of the most common chronic diseases in the United\nStates. While dental disease is preventable through healthy oral self-care\nbehaviors (OSCB), this basic behavior is not consistently practiced. We have\ndeveloped Oralytics, an online, reinforcement learning (RL) algorithm that\noptimizes the delivery of personalized intervention prompts to improve OSCB. In\nthis paper, we offer a full overview of algorithm design decisions made using\nprior data, domain expertise, and experiments in a simulation test bed. The\nfinalized RL algorithm was deployed in the Oralytics clinical trial, conducted\nfrom fall 2023 to summer 2024.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13127v2",
    "published_date": "2024-06-19 00:44:11 UTC",
    "updated_date": "2024-09-12 19:16:10 UTC"
  },
  {
    "arxiv_id": "2406.13125v1",
    "title": "A Unified Framework for Combinatorial Optimization Based on Graph Neural Networks",
    "authors": [
      "Yaochu Jin",
      "Xueming Yan",
      "Shiqing Liu",
      "Xiangyu Wang"
    ],
    "abstract": "Graph neural networks (GNNs) have emerged as a powerful tool for solving\ncombinatorial optimization problems (COPs), exhibiting state-of-the-art\nperformance in both graph-structured and non-graph-structured domains. However,\nexisting approaches lack a unified framework capable of addressing a wide range\nof COPs. After presenting a summary of representative COPs and a brief review\nof recent advancements in GNNs for solving COPs, this paper proposes a unified\nframework for solving COPs based on GNNs, including graph representation of\nCOPs, equivalent conversion of non-graph structured COPs to graph-structured\nCOPs, graph decomposition, and graph simplification. The proposed framework\nleverages the ability of GNNs to effectively capture the relational information\nand extract features from the graph representation of COPs, offering a generic\nsolution to COPs that can address the limitations of state-of-the-art in\nsolving non-graph-structured and highly complex graph-structured COPs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13125v1",
    "published_date": "2024-06-19 00:40:31 UTC",
    "updated_date": "2024-06-19 00:40:31 UTC"
  },
  {
    "arxiv_id": "2406.13123v3",
    "title": "ViLCo-Bench: VIdeo Language COntinual learning Benchmark",
    "authors": [
      "Tianqi Tang",
      "Shohreh Deldari",
      "Hao Xue",
      "Celso De Melo",
      "Flora D. Salim"
    ],
    "abstract": "Video language continual learning involves continuously adapting to\ninformation from video and text inputs, enhancing a model's ability to handle\nnew tasks while retaining prior knowledge. This field is a relatively\nunder-explored area, and establishing appropriate datasets is crucial for\nfacilitating communication and research in this field. In this study, we\npresent the first dedicated benchmark, ViLCo-Bench, designed to evaluate\ncontinual learning models across a range of video-text tasks. The dataset\ncomprises ten-minute-long videos and corresponding language queries collected\nfrom publicly available datasets. Additionally, we introduce a novel\nmemory-efficient framework that incorporates self-supervised learning and\nmimics long-term and short-term memory effects. This framework addresses\nchallenges including memory complexity from long video clips, natural language\ncomplexity from open queries, and text-video misalignment. We posit that\nViLCo-Bench, with greater complexity compared to existing continual learning\nbenchmarks, would serve as a critical tool for exploring the video-language\ndomain, extending beyond conventional class-incremental tasks, and addressing\ncomplex and limited annotation issues. The curated data, evaluations, and our\nnovel method are available at https://github.com/cruiseresearchgroup/ViLCo.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "14 pages, 4 figures, 8 tables, Accepted at NeurIPS Dataset and\n  Benchmark Track 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.13123v3",
    "published_date": "2024-06-19 00:38:19 UTC",
    "updated_date": "2024-12-15 10:24:12 UTC"
  },
  {
    "arxiv_id": "2406.13121v1",
    "title": "Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More?",
    "authors": [
      "Jinhyuk Lee",
      "Anthony Chen",
      "Zhuyun Dai",
      "Dheeru Dua",
      "Devendra Singh Sachan",
      "Michael Boratko",
      "Yi Luan",
      "Sébastien M. R. Arnold",
      "Vincent Perot",
      "Siddharth Dalmia",
      "Hexiang Hu",
      "Xudong Lin",
      "Panupong Pasupat",
      "Aida Amini",
      "Jeremy R. Cole",
      "Sebastian Riedel",
      "Iftekhar Naim",
      "Ming-Wei Chang",
      "Kelvin Guu"
    ],
    "abstract": "Long-context language models (LCLMs) have the potential to revolutionize our\napproach to tasks traditionally reliant on external tools like retrieval\nsystems or databases. Leveraging LCLMs' ability to natively ingest and process\nentire corpora of information offers numerous advantages. It enhances\nuser-friendliness by eliminating the need for specialized knowledge of tools,\nprovides robust end-to-end modeling that minimizes cascading errors in complex\npipelines, and allows for the application of sophisticated prompting techniques\nacross the entire system. To assess this paradigm shift, we introduce LOFT, a\nbenchmark of real-world tasks requiring context up to millions of tokens\ndesigned to evaluate LCLMs' performance on in-context retrieval and reasoning.\nOur findings reveal LCLMs' surprising ability to rival state-of-the-art\nretrieval and RAG systems, despite never having been explicitly trained for\nthese tasks. However, LCLMs still face challenges in areas like compositional\nreasoning that are required in SQL-like tasks. Notably, prompting strategies\nsignificantly influence performance, emphasizing the need for continued\nresearch as context lengths grow. Overall, LOFT provides a rigorous testing\nground for LCLMs, showcasing their potential to supplant existing paradigms and\ntackle novel tasks as model capabilities scale.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "29 pages. Dataset available at\n  https://github.com/google-deepmind/loft",
    "pdf_url": "http://arxiv.org/pdf/2406.13121v1",
    "published_date": "2024-06-19 00:28:58 UTC",
    "updated_date": "2024-06-19 00:28:58 UTC"
  },
  {
    "arxiv_id": "2406.13117v1",
    "title": "State-of-the-Art Review: The Use of Digital Twins to Support Artificial Intelligence-Guided Predictive Maintenance",
    "authors": [
      "Sizhe Ma",
      "Katherine A. Flanigan",
      "Mario Bergés"
    ],
    "abstract": "In recent years, predictive maintenance (PMx) has gained prominence for its\npotential to enhance efficiency, automation, accuracy, and cost-effectiveness\nwhile reducing human involvement. Importantly, PMx has evolved in tandem with\ndigital advancements, such as Big Data and the Internet of Things (IOT). These\ntechnological strides have enabled Artificial Intelligence (AI) to\nrevolutionize PMx processes, with increasing capacities for real-time\nautomation of monitoring, analysis, and prediction tasks. However, PMx still\nfaces challenges such as poor explainability and sample inefficiency in\ndata-driven methods and high complexity in physics-based models, hindering\nbroader adoption. This paper posits that Digital Twins (DTs) can be integrated\ninto PMx to overcome these challenges, paving the way for more automated PMx\napplications across various stakeholders. Despite their potential, current DTs\nhave not fully matured to bridge existing gaps. Our paper provides a\ncomprehensive roadmap for DT evolution, addressing current limitations to\nfoster large-scale automated PMx progression. We structure our approach in\nthree stages: First, we reference prior work where we identified and defined\nthe Information Requirements (IRs) and Functional Requirements (FRs) for PMx,\nforming the blueprint for a unified framework. Second, we conduct a literature\nreview to assess current DT applications integrating these IRs and FRs,\nrevealing standardized DT models and tools that support automated PMx. Lastly,\nwe highlight gaps in current DT implementations, particularly those IRs and FRs\nnot fully supported, and outline the necessary components for a comprehensive,\nautomated PMx system. Our paper concludes with research directions aimed at\nseamlessly integrating DTs into the PMx paradigm to achieve this ambitious\nvision.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "This work has been submitted to Springer for possible publication",
    "pdf_url": "http://arxiv.org/pdf/2406.13117v1",
    "published_date": "2024-06-19 00:10:57 UTC",
    "updated_date": "2024-06-19 00:10:57 UTC"
  },
  {
    "arxiv_id": "2406.13114v2",
    "title": "Multi-Stage Balanced Distillation: Addressing Long-Tail Challenges in Sequence-Level Knowledge Distillation",
    "authors": [
      "Yuhang Zhou",
      "Jing Zhu",
      "Paiheng Xu",
      "Xiaoyu Liu",
      "Xiyao Wang",
      "Danai Koutra",
      "Wei Ai",
      "Furong Huang"
    ],
    "abstract": "Large language models (LLMs) have significantly advanced various natural\nlanguage processing tasks, but deploying them remains computationally\nexpensive. Knowledge distillation (KD) is a promising solution, enabling the\ntransfer of capabilities from larger teacher LLMs to more compact student\nmodels. Particularly, sequence-level KD, which distills rationale-based\nreasoning processes instead of merely final outcomes, shows great potential in\nenhancing students' reasoning capabilities. However, current methods struggle\nwith sequence level KD under long-tailed data distributions, adversely\naffecting generalization on sparsely represented domains. We introduce the\nMulti-Stage Balanced Distillation (BalDistill) framework, which iteratively\nbalances training data within a fixed computational budget. By dynamically\nselecting representative head domain examples and synthesizing tail domain\nexamples, BalDistill achieves state-of-the-art performance across diverse\nlong-tailed datasets, enhancing both the efficiency and efficacy of the\ndistilled models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.13114v2",
    "published_date": "2024-06-19 00:01:14 UTC",
    "updated_date": "2024-10-18 23:46:40 UTC"
  },
  {
    "arxiv_id": "2406.13113v1",
    "title": "CU-Net: a U-Net architecture for efficient brain-tumor segmentation on BraTS 2019 dataset",
    "authors": [
      "Qimin Zhang",
      "Weiwei Qi",
      "Huili Zheng",
      "Xinyu Shen"
    ],
    "abstract": "Accurately segmenting brain tumors from MRI scans is important for developing\neffective treatment plans and improving patient outcomes. This study introduces\na new implementation of the Columbia-University-Net (CU-Net) architecture for\nbrain tumor segmentation using the BraTS 2019 dataset. The CU-Net model has a\nsymmetrical U-shaped structure and uses convolutional layers, max pooling, and\nupsampling operations to achieve high-resolution segmentation. Our CU-Net model\nachieved a Dice score of 82.41%, surpassing two other state-of-the-art models.\nThis improvement in segmentation accuracy highlights the robustness and\neffectiveness of the model, which helps to accurately delineate tumor\nboundaries, which is crucial for surgical planning and radiation therapy, and\nultimately has the potential to improve patient outcomes.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "q-bio.NC"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13113v1",
    "published_date": "2024-06-19 00:01:01 UTC",
    "updated_date": "2024-06-19 00:01:01 UTC"
  }
]
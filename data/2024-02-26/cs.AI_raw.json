[
  {
    "arxiv_id": "2402.17087v1",
    "title": "A Note on Bayesian Networks with Latent Root Variables",
    "authors": [
      "Marco Zaffalon",
      "Alessandro Antonucci"
    ],
    "abstract": "We characterise the likelihood function computed from a Bayesian network with\nlatent variables as root nodes. We show that the marginal distribution over the\nremaining, manifest, variables also factorises as a Bayesian network, which we\ncall empirical. A dataset of observations of the manifest variables allows us\nto quantify the parameters of the empirical Bayesian net. We prove that (i) the\nlikelihood of such a dataset from the original Bayesian network is dominated by\nthe global maximum of the likelihood from the empirical one; and that (ii) such\na maximum is attained if and only if the parameters of the Bayesian network are\nconsistent with those of the empirical model.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.17087v1",
    "published_date": "2024-02-26 23:53:34 UTC",
    "updated_date": "2024-02-26 23:53:34 UTC"
  },
  {
    "arxiv_id": "2402.17082v1",
    "title": "Deconstructing the Veneer of Simplicity: Co-Designing Introductory Generative AI Workshops with Local Entrepreneurs",
    "authors": [
      "Yasmine Kotturi",
      "Angel Anderson",
      "Glenn Ford",
      "Michael Skirpan",
      "Jeffrey P. Bigham"
    ],
    "abstract": "Generative AI platforms and features are permeating many aspects of work.\nEntrepreneurs from lean economies in particular are well positioned to\noutsource tasks to generative AI given limited resources. In this paper, we\nwork to address a growing disparity in use of these technologies by building on\na four-year partnership with a local entrepreneurial hub dedicated to equity in\ntech and entrepreneurship. Together, we co-designed an interactive workshops\nseries aimed to onboard local entrepreneurs to generative AI platforms.\nAlongside four community-driven and iterative workshops with entrepreneurs\nacross five months, we conducted interviews with 15 local entrepreneurs and\ncommunity providers. We detail the importance of communal and supportive\nexposure to generative AI tools for local entrepreneurs, scaffolding actionable\nuse (and supporting non-use), demystifying generative AI technologies by\nemphasizing entrepreneurial power, while simultaneously deconstructing the\nveneer of simplicity to address the many operational skills needed for\nsuccessful application.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.17082v1",
    "published_date": "2024-02-26 23:40:33 UTC",
    "updated_date": "2024-02-26 23:40:33 UTC"
  },
  {
    "arxiv_id": "2403.00823v1",
    "title": "Adapting to Teammates in a Cooperative Language Game",
    "authors": [
      "Christopher Archibald",
      "Spencer Brosnahan"
    ],
    "abstract": "The game of Codenames has recently emerged as a domain of interest for\nintelligent agent design. The game is unique due to the way that language and\ncoordination between teammates play important roles. Previous approaches to\ndesigning agents for this game have utilized a single internal language model\nto determine action choices. This often leads to good performance with some\nteammates and inferior performance with other teammates, as the agent cannot\nadapt to any specific teammate. In this paper we present the first adaptive\nagent for playing Codenames. We adopt an ensemble approach with the goal of\ndetermining, during the course of interacting with a specific teammate, which\nof our internal expert agents, each potentially with its own language model, is\nthe best match. One difficulty faced in this approach is the lack of a single\nnumerical metric that accurately captures the performance of a Codenames team.\nPrior Codenames research has utilized a handful of different metrics to\nevaluate agent teams. We propose a novel single metric to evaluate the\nperformance of a Codenames team, whether playing a single team (solitaire)\ngame, or a competitive game against another team. We then present and analyze\nan ensemble agent which selects an internal expert on each turn in order to\nmaximize this proposed metric. Experimental analysis shows that this ensemble\napproach adapts to individual teammates and often performs nearly as well as\nthe best internal expert with a teammate. Crucially, this success does not\ndepend on any previous knowledge about the teammates, the ensemble agents, or\ntheir compatibility. This research represents an important step to making\nlanguage-based agents for cooperative language settings like Codenames more\nadaptable to individual teammates.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.00823v1",
    "published_date": "2024-02-26 23:15:07 UTC",
    "updated_date": "2024-02-26 23:15:07 UTC"
  },
  {
    "arxiv_id": "2402.17073v3",
    "title": "Hyperdimensional Representation Learning for Node Classification and Link Prediction",
    "authors": [
      "Abhishek Dalvi",
      "Vasant Honavar"
    ],
    "abstract": "We introduce Hyperdimensional Graph Learner (HDGL), a novel method for node\nclassification and link prediction in graphs. HDGL maps node features into a\nvery high-dimensional space (\\textit{hyperdimensional} or HD space for short)\nusing the \\emph{injectivity} property of node representations in a family of\nGraph Neural Networks (GNNs) and then uses HD operators such as\n\\textit{bundling} and \\textit{binding} to aggregate information from the local\nneighborhood of each node yielding latent node representations that can support\nboth node classification and link prediction tasks. HDGL, unlike GNNs that rely\non computationally expensive iterative optimization and hyperparameter tuning,\nrequires only a single pass through the data set. We report results of\nexperiments using widely used benchmark datasets which demonstrate that, on the\nnode classification task, HDGL achieves accuracy that is competitive with that\nof the state-of-the-art GNN methods at substantially reduced computational\ncost; and on the link prediction task, HDGL matches the performance of DeepWalk\nand related methods, although it falls short of computationally demanding\nstate-of-the-art GNNs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by WSDM 2025",
    "pdf_url": "http://arxiv.org/pdf/2402.17073v3",
    "published_date": "2024-02-26 23:15:01 UTC",
    "updated_date": "2025-02-27 00:21:39 UTC"
  },
  {
    "arxiv_id": "2402.17065v2",
    "title": "Taming the Tail in Class-Conditional GANs: Knowledge Sharing via Unconditional Training at Lower Resolutions",
    "authors": [
      "Saeed Khorram",
      "Mingqi Jiang",
      "Mohamad Shahbazi",
      "Mohamad H. Danesh",
      "Li Fuxin"
    ],
    "abstract": "Despite extensive research on training generative adversarial networks (GANs)\nwith limited training data, learning to generate images from long-tailed\ntraining distributions remains fairly unexplored. In the presence of imbalanced\nmulti-class training data, GANs tend to favor classes with more samples,\nleading to the generation of low-quality and less diverse samples in tail\nclasses. In this study, we aim to improve the training of class-conditional\nGANs with long-tailed data. We propose a straightforward yet effective method\nfor knowledge sharing, allowing tail classes to borrow from the rich\ninformation from classes with more abundant training data. More concretely, we\npropose modifications to existing class-conditional GAN architectures to ensure\nthat the lower-resolution layers of the generator are trained entirely\nunconditionally while reserving class-conditional generation for the\nhigher-resolution layers. Experiments on several long-tail benchmarks and GAN\narchitectures demonstrate a significant improvement over existing methods in\nboth the diversity and fidelity of the generated images. The code is available\nat https://github.com/khorrams/utlo.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.17065v2",
    "published_date": "2024-02-26 23:03:00 UTC",
    "updated_date": "2024-06-16 22:11:56 UTC"
  },
  {
    "arxiv_id": "2402.17045v2",
    "title": "An Investigation into the Performances of the State-of-the-art Machine Learning Approaches for Various Cyber-attack Detection: A Survey",
    "authors": [
      "Tosin Ige",
      "Christopher Kiekintveld",
      "Aritran Piplai"
    ],
    "abstract": "In this research, we analyzed the suitability of each of the current\nstate-of-the-art machine learning models for various cyberattack detection from\nthe past 5 years with a major emphasis on the most recent works for comparative\nstudy to identify the knowledge gap where work is still needed to be done with\nregard to detection of each category of cyberattack. We also reviewed the\nsuitability, effeciency and limitations of recent research on state-of-the-art\nclassifiers and novel frameworks in the detection of differnet cyberattacks.\nOur result shows the need for; further research and exploration on machine\nlearning approach for the detection of drive-by download attacks, an\ninvestigation into the mix performance of Naive Bayes to identify possible\nresearch direction on improvement to existing state-of-the-art Naive Bayes\nclassifier, we also identify that current machine learning approach to the\ndetection of SQLi attack cannot detect an already compromised database with\nSQLi attack signifying another possible future research direction.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "10",
    "pdf_url": "http://arxiv.org/pdf/2402.17045v2",
    "published_date": "2024-02-26 22:04:25 UTC",
    "updated_date": "2024-05-10 06:14:50 UTC"
  },
  {
    "arxiv_id": "2402.17042v2",
    "title": "Towards Generalizing Inferences from Trials to Target Populations",
    "authors": [
      "Melody Y Huang",
      "Harsh Parikh"
    ],
    "abstract": "Randomized Controlled Trials (RCTs) are pivotal in generating internally\nvalid estimates with minimal assumptions, serving as a cornerstone for\nresearchers dedicated to advancing causal inference methods. However, extending\nthese findings beyond the experimental cohort to achieve externally valid\nestimates is crucial for broader scientific inquiry. This paper delves into the\nforefront of addressing these external validity challenges, encapsulating the\nessence of a multidisciplinary workshop held at the Institute for Computational\nand Experimental Research in Mathematics (ICERM), Brown University, in Fall\n2023. The workshop congregated experts from diverse fields including social\nscience, medicine, public health, statistics, computer science, and education,\nto tackle the unique obstacles each discipline faces in extrapolating\nexperimental findings. Our study presents three key contributions: we integrate\nongoing efforts, highlighting methodological synergies across fields; provide\nan exhaustive review of generalizability and transportability based on the\nworkshop's discourse; and identify persistent hurdles while suggesting avenues\nfor future research. By doing so, this paper aims to enhance the collective\nunderstanding of the generalizability and transportability of causal effects,\nfostering cross-disciplinary collaboration and offering valuable insights for\nresearchers working on refining and applying causal inference methods.",
    "categories": [
      "stat.ME",
      "cs.AI",
      "cs.LG",
      "econ.EM"
    ],
    "primary_category": "stat.ME",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.17042v2",
    "published_date": "2024-02-26 21:49:44 UTC",
    "updated_date": "2024-05-25 00:05:02 UTC"
  },
  {
    "arxiv_id": "2402.17032v1",
    "title": "REFACTOR: Learning to Extract Theorems from Proofs",
    "authors": [
      "Jin Peng Zhou",
      "Yuhuai Wu",
      "Qiyang Li",
      "Roger Grosse"
    ],
    "abstract": "Human mathematicians are often good at recognizing modular and reusable\ntheorems that make complex mathematical results within reach. In this paper, we\npropose a novel method called theoREm-from-prooF extrACTOR (REFACTOR) for\ntraining neural networks to mimic this ability in formal mathematical theorem\nproving. We show on a set of unseen proofs, REFACTOR is able to extract 19.6%\nof the theorems that humans would use to write the proofs. When applying the\nmodel to the existing Metamath library, REFACTOR extracted 16 new theorems.\nWith newly extracted theorems, we show that the existing proofs in the MetaMath\ndatabase can be refactored. The new theorems are used very frequently after\nrefactoring, with an average usage of 733.5 times, and help shorten the proof\nlengths. Lastly, we demonstrate that the prover trained on the new-theorem\nrefactored dataset proves more test theorems and outperforms state-of-the-art\nbaselines by frequently leveraging a diverse set of newly extracted theorems.\nCode can be found at https://github.com/jinpz/refactor.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.17032v1",
    "published_date": "2024-02-26 21:21:30 UTC",
    "updated_date": "2024-02-26 21:21:30 UTC"
  },
  {
    "arxiv_id": "2402.17018v1",
    "title": "A Curious Case of Remarkable Resilience to Gradient Attacks via Fully Convolutional and Differentiable Front End with a Skip Connection",
    "authors": [
      "Leonid Boytsov",
      "Ameya Joshi",
      "Filipe Condessa"
    ],
    "abstract": "We tested front-end enhanced neural models where a frozen classifier was\nprepended by a differentiable and fully convolutional model with a skip\nconnection. By training them using a small learning rate for about one epoch,\nwe obtained models that retained the accuracy of the backbone classifier while\nbeing unusually resistant to gradient attacks including APGD and FAB-T attacks\nfrom the AutoAttack package, which we attributed to gradient masking. The\ngradient masking phenomenon is not new, but the degree of masking was quite\nremarkable for fully differentiable models that did not have\ngradient-shattering components such as JPEG compression or components that are\nexpected to cause diminishing gradients.\n  Though black box attacks can be partially effective against gradient masking,\nthey are easily defeated by combining models into randomized ensembles. We\nestimate that such ensembles achieve near-SOTA AutoAttack accuracy on CIFAR10,\nCIFAR100, and ImageNet despite having virtually zero accuracy under adaptive\nattacks. Adversarial training of the backbone classifier can further increase\nresistance of the front-end enhanced model to gradient attacks. On CIFAR10, the\nrespective randomized ensemble achieved 90.8$\\pm 2.5$% (99% CI) accuracy under\nAutoAttack while having only 18.2$\\pm 3.6$% accuracy under the adaptive attack.\n  We do not establish SOTA in adversarial robustness. Instead, we make\nmethodological contributions and further supports the thesis that adaptive\nattacks designed with the complete knowledge of model architecture are crucial\nin demonstrating model robustness and that even the so-called white-box\ngradient attacks can have limited applicability. Although gradient attacks can\nbe complemented with black-box attack such as the SQUARE attack or the\nzero-order PGD, black-box attacks can be weak against randomized ensembles,\ne.g., when ensemble models mask gradients.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.17018v1",
    "published_date": "2024-02-26 20:55:47 UTC",
    "updated_date": "2024-02-26 20:55:47 UTC"
  },
  {
    "arxiv_id": "2402.17016v1",
    "title": "Multi-Task Contrastive Learning for 8192-Token Bilingual Text Embeddings",
    "authors": [
      "Isabelle Mohr",
      "Markus Krimmel",
      "Saba Sturua",
      "Mohammad Kalim Akram",
      "Andreas Koukounas",
      "Michael Günther",
      "Georgios Mastrapas",
      "Vinit Ravishankar",
      "Joan Fontanals Martínez",
      "Feng Wang",
      "Qi Liu",
      "Ziniu Yu",
      "Jie Fu",
      "Saahil Ognawala",
      "Susana Guzman",
      "Bo Wang",
      "Maximilian Werk",
      "Nan Wang",
      "Han Xiao"
    ],
    "abstract": "We introduce a novel suite of state-of-the-art bilingual text embedding\nmodels that are designed to support English and another target language. These\nmodels are capable of processing lengthy text inputs with up to 8192 tokens,\nmaking them highly versatile for a range of natural language processing tasks\nsuch as text retrieval, clustering, and semantic textual similarity (STS)\ncalculations.\n  By focusing on bilingual models and introducing a unique multi-task learning\nobjective, we have significantly improved the model performance on STS tasks,\nwhich outperforms the capabilities of existing multilingual models in both\ntarget language understanding and cross-lingual evaluation tasks. Moreover, our\nbilingual models are more efficient, requiring fewer parameters and less memory\ndue to their smaller vocabulary needs. Furthermore, we have expanded the\nMassive Text Embedding Benchmark (MTEB) to include benchmarks for German and\nSpanish embedding models. This integration aims to stimulate further research\nand advancement in text embedding technologies for these languages.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "68T50",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.17016v1",
    "published_date": "2024-02-26 20:53:12 UTC",
    "updated_date": "2024-02-26 20:53:12 UTC"
  },
  {
    "arxiv_id": "2402.17013v1",
    "title": "Towards Explainability and Fairness in Swiss Judgement Prediction: Benchmarking on a Multilingual Dataset",
    "authors": [
      "Santosh T. Y. S. S",
      "Nina Baumgartner",
      "Matthias Stürmer",
      "Matthias Grabmair",
      "Joel Niklaus"
    ],
    "abstract": "The assessment of explainability in Legal Judgement Prediction (LJP) systems\nis of paramount importance in building trustworthy and transparent systems,\nparticularly considering the reliance of these systems on factors that may lack\nlegal relevance or involve sensitive attributes. This study delves into the\nrealm of explainability and fairness in LJP models, utilizing Swiss Judgement\nPrediction (SJP), the only available multilingual LJP dataset. We curate a\ncomprehensive collection of rationales that `support' and `oppose' judgement\nfrom legal experts for 108 cases in German, French, and Italian. By employing\nan occlusion-based explainability approach, we evaluate the explainability\nperformance of state-of-the-art monolingual and multilingual BERT-based LJP\nmodels, as well as models developed with techniques such as data augmentation\nand cross-lingual transfer, which demonstrated prediction performance\nimprovement. Notably, our findings reveal that improved prediction performance\ndoes not necessarily correspond to enhanced explainability performance,\nunderscoring the significance of evaluating models from an explainability\nperspective. Additionally, we introduce a novel evaluation framework, Lower\nCourt Insertion (LCI), which allows us to quantify the influence of lower court\ninformation on model predictions, exposing current models' biases.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "68T50",
      "I.2"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at LREC-COLING 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.17013v1",
    "published_date": "2024-02-26 20:42:40 UTC",
    "updated_date": "2024-02-26 20:42:40 UTC"
  },
  {
    "arxiv_id": "2402.17012v4",
    "title": "Pandora's White-Box: Precise Training Data Detection and Extraction in Large Language Models",
    "authors": [
      "Jeffrey G. Wang",
      "Jason Wang",
      "Marvin Li",
      "Seth Neel"
    ],
    "abstract": "In this paper we develop state-of-the-art privacy attacks against Large\nLanguage Models (LLMs), where an adversary with some access to the model tries\nto learn something about the underlying training data. Our headline results are\nnew membership inference attacks (MIAs) against pretrained LLMs that perform\nhundreds of times better than baseline attacks, and a pipeline showing that\nover 50% (!) of the fine-tuning dataset can be extracted from a fine-tuned LLM\nin natural settings. We consider varying degrees of access to the underlying\nmodel, pretraining and fine-tuning data, and both MIAs and training data\nextraction. For pretraining data, we propose two new MIAs: a supervised neural\nnetwork classifier that predicts training data membership on the basis of\n(dimensionality-reduced) model gradients, as well as a variant of this attack\nthat only requires logit access to the model by leveraging recent\nmodel-stealing work on LLMs. To our knowledge this is the first MIA that\nexplicitly incorporates model-stealing information. Both attacks outperform\nexisting black-box baselines, and our supervised attack closes the gap between\nMIA attack success against LLMs and the strongest known attacks for other\nmachine learning models. In fine-tuning, we find that a simple attack based on\nthe ratio of the loss between the base and fine-tuned models is able to achieve\nnear-perfect MIA performance; we then leverage our MIA to extract a large\nfraction of the fine-tuning dataset from fine-tuned Pythia and Llama models.\nOur code is available at github.com/safr-ai-lab/pandora-llm.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "Found software bug in experiments, withdrawing in order to address\n  and update results",
    "pdf_url": "http://arxiv.org/pdf/2402.17012v4",
    "published_date": "2024-02-26 20:41:50 UTC",
    "updated_date": "2024-07-15 02:37:09 UTC"
  },
  {
    "arxiv_id": "2402.17010v2",
    "title": "Generative Retrieval with Large Language Models",
    "authors": [
      "Ye Wang",
      "Xinrun Xu",
      "Rui Xie",
      "Wenxin Hu",
      "Wei Ye"
    ],
    "abstract": "When completing knowledge-intensive tasks, humans sometimes need not just an\nanswer but also a corresponding reference passage for auxiliary reading.\nPrevious methods required obtaining pre-segmented article chunks through\nadditional retrieval models. This paper explores leveraging the parameterized\nknowledge stored during the pre-training phase of large language models (LLMs)\nto independently recall reference passage from any starting position. We\npropose a two-stage framework that simulates the scenario of humans recalling\neasily forgotten references. Initially, the LLM is prompted to recall document\ntitle identifiers to obtain a coarse-grained document set. Then, based on the\nacquired coarse-grained document set, it recalls fine-grained passage. In the\ntwo-stage recall process, we use constrained decoding to ensure that content\noutside of the stored documents is not generated. To increase speed, we only\nrecall a short prefix in the second stage, then locate its position to retrieve\na complete passage. Experiments on KILT knowledge-sensitive tasks have verified\nthat LLMs can independently recall reference passage location in various task\nforms, and the obtained reference significantly assist downstream tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.17010v2",
    "published_date": "2024-02-26 20:35:32 UTC",
    "updated_date": "2024-10-29 08:45:35 UTC"
  },
  {
    "arxiv_id": "2402.17003v2",
    "title": "Monitoring Fidelity of Online Reinforcement Learning Algorithms in Clinical Trials",
    "authors": [
      "Anna L. Trella",
      "Kelly W. Zhang",
      "Inbal Nahum-Shani",
      "Vivek Shetty",
      "Iris Yan",
      "Finale Doshi-Velez",
      "Susan A. Murphy"
    ],
    "abstract": "Online reinforcement learning (RL) algorithms offer great potential for\npersonalizing treatment for participants in clinical trials. However, deploying\nan online, autonomous algorithm in the high-stakes healthcare setting makes\nquality control and data quality especially difficult to achieve. This paper\nproposes algorithm fidelity as a critical requirement for deploying online RL\nalgorithms in clinical trials. It emphasizes the responsibility of the\nalgorithm to (1) safeguard participants and (2) preserve the scientific utility\nof the data for post-trial analyses. We also present a framework for\npre-deployment planning and real-time monitoring to help algorithm developers\nand clinical researchers ensure algorithm fidelity. To illustrate our\nframework's practical application, we present real-world examples from the\nOralytics clinical trial. Since Spring 2023, this trial successfully deployed\nan autonomous, online RL algorithm to personalize behavioral interventions for\nparticipants at risk for dental disease.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.17003v2",
    "published_date": "2024-02-26 20:19:14 UTC",
    "updated_date": "2024-08-12 16:56:11 UTC"
  },
  {
    "arxiv_id": "2402.16998v2",
    "title": "What Do Language Models Hear? Probing for Auditory Representations in Language Models",
    "authors": [
      "Jerry Ngo",
      "Yoon Kim"
    ],
    "abstract": "This work explores whether language models encode meaningfully grounded\nrepresentations of sounds of objects. We learn a linear probe that retrieves\nthe correct text representation of an object given a snippet of audio related\nto that object, where the sound representation is given by a pretrained audio\nmodel. This probe is trained via a contrastive loss that pushes the language\nrepresentations and sound representations of an object to be close to one\nanother. After training, the probe is tested on its ability to generalize to\nobjects that were not seen during training. Across different language models\nand audio models, we find that the probe generalization is above chance in many\ncases, indicating that despite being trained only on raw text, language models\nencode grounded knowledge of sounds for some objects.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16998v2",
    "published_date": "2024-02-26 20:13:58 UTC",
    "updated_date": "2024-08-16 08:13:38 UTC"
  },
  {
    "arxiv_id": "2403.00011v1",
    "title": "Introducing User Feedback-based Counterfactual Explanations (UFCE)",
    "authors": [
      "Muhammad Suffian",
      "Jose M. Alonso-Moral",
      "Alessandro Bogliolo"
    ],
    "abstract": "Machine learning models are widely used in real-world applications. However,\ntheir complexity makes it often challenging to interpret the rationale behind\ntheir decisions. Counterfactual explanations (CEs) have emerged as a viable\nsolution for generating comprehensible explanations in eXplainable Artificial\nIntelligence (XAI). CE provides actionable information to users on how to\nachieve the desired outcome with minimal modifications to the input. However,\ncurrent CE algorithms usually operate within the entire feature space when\noptimizing changes to turn over an undesired outcome, overlooking the\nidentification of key contributors to the outcome and disregarding the\npracticality of the suggested changes. In this study, we introduce a novel\nmethodology, that is named as user feedback-based counterfactual explanation\n(UFCE), which addresses these limitations and aims to bolster confidence in the\nprovided explanations. UFCE allows for the inclusion of user constraints to\ndetermine the smallest modifications in the subset of actionable features while\nconsidering feature dependence, and evaluates the practicality of suggested\nchanges using benchmark evaluation metrics. We conducted three experiments with\nfive datasets, demonstrating that UFCE outperforms two well-known CE methods in\nterms of \\textit{proximity}, \\textit{sparsity}, and \\textit{feasibility}.\nReported results indicate that user constraints influence the generation of\nfeasible CEs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "preprint of paper submitted to IJCIS Springer",
    "pdf_url": "http://arxiv.org/pdf/2403.00011v1",
    "published_date": "2024-02-26 20:09:44 UTC",
    "updated_date": "2024-02-26 20:09:44 UTC"
  },
  {
    "arxiv_id": "2402.16994v2",
    "title": "GEM3D: GEnerative Medial Abstractions for 3D Shape Synthesis",
    "authors": [
      "Dmitry Petrov",
      "Pradyumn Goyal",
      "Vikas Thamizharasan",
      "Vladimir G. Kim",
      "Matheus Gadelha",
      "Melinos Averkiou",
      "Siddhartha Chaudhuri",
      "Evangelos Kalogerakis"
    ],
    "abstract": "We introduce GEM3D -- a new deep, topology-aware generative model of 3D\nshapes. The key ingredient of our method is a neural skeleton-based\nrepresentation encoding information on both shape topology and geometry.\nThrough a denoising diffusion probabilistic model, our method first generates\nskeleton-based representations following the Medial Axis Transform (MAT), then\ngenerates surfaces through a skeleton-driven neural implicit formulation. The\nneural implicit takes into account the topological and geometric information\nstored in the generated skeleton representations to yield surfaces that are\nmore topologically and geometrically accurate compared to previous neural field\nformulations. We discuss applications of our method in shape synthesis and\npoint cloud reconstruction tasks, and evaluate our method both qualitatively\nand quantitatively. We demonstrate significantly more faithful surface\nreconstruction and diverse shape generation results compared to the\nstate-of-the-art, also involving challenging scenarios of reconstructing and\nsynthesizing structurally complex, high-genus shape surfaces from Thingi10K and\nShapeNet.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Webpage: https://lodurality.github.io/GEM3D/ -- Cond. accept. to\n  SIGGRAPH 2024 (conf. track) -- Changes (based on reviews): changed style to\n  sigconf; rearranged figures for readability; added missing citations; fixed\n  misaligned centers in Fig. 3; added failure cases (Fig. 10); rewrote\n  discussion; added categories averages to Tab. 8; added Tab. 10 with model\n  capacities",
    "pdf_url": "http://arxiv.org/pdf/2402.16994v2",
    "published_date": "2024-02-26 20:00:57 UTC",
    "updated_date": "2024-04-11 03:44:49 UTC"
  },
  {
    "arxiv_id": "2402.16973v2",
    "title": "Successfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Corrections",
    "authors": [
      "Lingjun Zhao",
      "Khanh Nguyen",
      "Hal Daumé III"
    ],
    "abstract": "Language models will inevitably err in situations with which they are\nunfamiliar. However, by effectively communicating uncertainties, they can still\nguide humans toward making sound decisions in those contexts. We demonstrate\nthis idea by developing HEAR, a system that can successfully guide humans in\nsimulated residential environments despite generating potentially inaccurate\ninstructions. Diverging from systems that provide users with only the\ninstructions they generate, HEAR warns users of potential errors in its\ninstructions and suggests corrections. This rich uncertainty information\neffectively prevents misguidance and reduces the search space for users.\nEvaluation with 80 users shows that HEAR achieves a 13% increase in success\nrate and a 29% reduction in final location error distance compared to only\npresenting instructions to users. Interestingly, we find that offering users\npossibilities to explore, HEAR motivates them to make more attempts at the\ntask, ultimately leading to a higher success rate. To our best knowledge, this\nwork is the first to show the practical benefits of uncertainty communication\nin a long-horizon sequential decision-making problem.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.16973v2",
    "published_date": "2024-02-26 19:16:04 UTC",
    "updated_date": "2024-10-05 01:21:14 UTC"
  },
  {
    "arxiv_id": "2402.16968v1",
    "title": "A Survey of Large Language Models in Cybersecurity",
    "authors": [
      "Gabriel de Jesus Coelho da Silva",
      "Carlos Becker Westphall"
    ],
    "abstract": "Large Language Models (LLMs) have quickly risen to prominence due to their\nability to perform at or close to the state-of-the-art in a variety of fields\nwhile handling natural language. An important field of research is the\napplication of such models at the cybersecurity context. This survey aims to\nidentify where in the field of cybersecurity LLMs have already been applied,\nthe ways in which they are being used and their limitations in the field.\nFinally, suggestions are made on how to improve such limitations and what can\nbe expected from these systems once these limitations are overcome.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16968v1",
    "published_date": "2024-02-26 19:06:02 UTC",
    "updated_date": "2024-02-26 19:06:02 UTC"
  },
  {
    "arxiv_id": "2402.16965v1",
    "title": "WIPI: A New Web Threat for LLM-Driven Web Agents",
    "authors": [
      "Fangzhou Wu",
      "Shutong Wu",
      "Yulong Cao",
      "Chaowei Xiao"
    ],
    "abstract": "With the fast development of large language models (LLMs), LLM-driven Web\nAgents (Web Agents for short) have obtained tons of attention due to their\nsuperior capability where LLMs serve as the core part of making decisions like\nthe human brain equipped with multiple web tools to actively interact with\nexternal deployed websites. As uncountable Web Agents have been released and\nsuch LLM systems are experiencing rapid development and drawing closer to\nwidespread deployment in our daily lives, an essential and pressing question\narises: \"Are these Web Agents secure?\". In this paper, we introduce a novel\nthreat, WIPI, that indirectly controls Web Agent to execute malicious\ninstructions embedded in publicly accessible webpages. To launch a successful\nWIPI works in a black-box environment. This methodology focuses on the form and\ncontent of indirect instructions within external webpages, enhancing the\nefficiency and stealthiness of the attack. To evaluate the effectiveness of the\nproposed methodology, we conducted extensive experiments using 7 plugin-based\nChatGPT Web Agents, 8 Web GPTs, and 3 different open-source Web Agents. The\nresults reveal that our methodology achieves an average attack success rate\n(ASR) exceeding 90% even in pure black-box scenarios. Moreover, through an\nablation study examining various user prefix instructions, we demonstrated that\nthe WIPI exhibits strong robustness, maintaining high performance across\ndiverse prefix instructions.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16965v1",
    "published_date": "2024-02-26 19:01:54 UTC",
    "updated_date": "2024-02-26 19:01:54 UTC"
  },
  {
    "arxiv_id": "2402.16846v2",
    "title": "GROUNDHOG: Grounding Large Language Models to Holistic Segmentation",
    "authors": [
      "Yichi Zhang",
      "Ziqiao Ma",
      "Xiaofeng Gao",
      "Suhaila Shakiah",
      "Qiaozi Gao",
      "Joyce Chai"
    ],
    "abstract": "Most multimodal large language models (MLLMs) learn language-to-object\ngrounding through causal language modeling where grounded objects are captured\nby bounding boxes as sequences of location tokens. This paradigm lacks\npixel-level representations that are important for fine-grained visual\nunderstanding and diagnosis. In this work, we introduce GROUNDHOG, an MLLM\ndeveloped by grounding Large Language Models to holistic segmentation.\nGROUNDHOG incorporates a masked feature extractor and converts extracted\nfeatures into visual entity tokens for the MLLM backbone, which then connects\ngroundable phrases to unified grounding masks by retrieving and merging the\nentity masks. To train GROUNDHOG, we carefully curated M3G2, a grounded visual\ninstruction tuning dataset with Multi-Modal Multi-Grained Grounding, by\nharvesting a collection of segmentation-grounded datasets with rich\nannotations. Our experimental results show that GROUNDHOG achieves superior\nperformance on various language grounding tasks without task-specific\nfine-tuning, and significantly reduces object hallucination. GROUNDHOG also\ndemonstrates better grounding towards complex forms of visual input and\nprovides easy-to-understand diagnosis in failure cases.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to CVPR 2024. Website: https://groundhog-mllm.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2402.16846v2",
    "published_date": "2024-02-26 18:59:33 UTC",
    "updated_date": "2024-04-16 17:59:53 UTC"
  },
  {
    "arxiv_id": "2402.16845v2",
    "title": "Neural Operators with Localized Integral and Differential Kernels",
    "authors": [
      "Miguel Liu-Schiaffini",
      "Julius Berner",
      "Boris Bonev",
      "Thorsten Kurth",
      "Kamyar Azizzadenesheli",
      "Anima Anandkumar"
    ],
    "abstract": "Neural operators learn mappings between function spaces, which is practical\nfor learning solution operators of PDEs and other scientific modeling\napplications. Among them, the Fourier neural operator (FNO) is a popular\narchitecture that performs global convolutions in the Fourier space. However,\nsuch global operations are often prone to over-smoothing and may fail to\ncapture local details. In contrast, convolutional neural networks (CNN) can\ncapture local features but are limited to training and inference at a single\nresolution. In this work, we present a principled approach to operator learning\nthat can capture local features under two frameworks by learning differential\noperators and integral operators with locally supported kernels. Specifically,\ninspired by stencil methods, we prove that we obtain differential operators\nunder an appropriate scaling of the kernel values of CNNs. To obtain local\nintegral operators, we utilize suitable basis representations for the kernels\nbased on discrete-continuous convolutions. Both these approaches preserve the\nproperties of operator learning and, hence, the ability to predict at any\nresolution. Adding our layers to FNOs significantly improves their performance,\nreducing the relative L2-error by 34-72% in our experiments, which include a\nturbulent 2D Navier-Stokes and the spherical shallow water equations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NA",
      "math.NA"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at 2024 International Conference on Machine Learning",
    "pdf_url": "http://arxiv.org/pdf/2402.16845v2",
    "published_date": "2024-02-26 18:59:31 UTC",
    "updated_date": "2024-06-08 22:16:13 UTC"
  },
  {
    "arxiv_id": "2402.16844v3",
    "title": "Think Big, Generate Quick: LLM-to-SLM for Fast Autoregressive Decoding",
    "authors": [
      "Benjamin Bergner",
      "Andrii Skliar",
      "Amelie Royer",
      "Tijmen Blankevoort",
      "Yuki Asano",
      "Babak Ehteshami Bejnordi"
    ],
    "abstract": "Large language models (LLMs) have become ubiquitous in practice and are\nwidely used for generation tasks such as translation, summarization and\ninstruction following. However, their enormous size and reliance on\nautoregressive decoding increase deployment costs and complicate their use in\nlatency-critical applications. In this work, we propose a hybrid approach that\ncombines language models of different sizes to increase the efficiency of\nautoregressive decoding while maintaining high performance. Our method utilizes\na pretrained frozen LLM that encodes all prompt tokens once in parallel, and\nuses the resulting representations to condition and guide a small language\nmodel (SLM), which then generates the response more efficiently. We investigate\nthe combination of encoder-decoder LLMs with both encoder-decoder and\ndecoder-only SLMs from different model families and only require fine-tuning of\nthe SLM. Experiments with various benchmarks show substantial speedups of up to\n$4\\times$, with minor performance penalties of $1-2\\%$ for translation and\nsummarization tasks compared to the LLM.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Work presented at the ES-FoMo II Workshop at ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.16844v3",
    "published_date": "2024-02-26 18:59:28 UTC",
    "updated_date": "2024-07-17 13:59:48 UTC"
  },
  {
    "arxiv_id": "2402.16843v2",
    "title": "Multi-LoRA Composition for Image Generation",
    "authors": [
      "Ming Zhong",
      "Yelong Shen",
      "Shuohang Wang",
      "Yadong Lu",
      "Yizhu Jiao",
      "Siru Ouyang",
      "Donghan Yu",
      "Jiawei Han",
      "Weizhu Chen"
    ],
    "abstract": "Low-Rank Adaptation (LoRA) is extensively utilized in text-to-image models\nfor the accurate rendition of specific elements like distinct characters or\nunique styles in generated images. Nonetheless, existing methods face\nchallenges in effectively composing multiple LoRAs, especially as the number of\nLoRAs to be integrated grows, thus hindering the creation of complex imagery.\nIn this paper, we study multi-LoRA composition through a decoding-centric\nperspective. We present two training-free methods: LoRA Switch, which\nalternates between different LoRAs at each denoising step, and LoRA Composite,\nwhich simultaneously incorporates all LoRAs to guide more cohesive image\nsynthesis. To evaluate the proposed approaches, we establish ComposLoRA, a new\ncomprehensive testbed as part of this research. It features a diverse range of\nLoRA categories with 480 composition sets. Utilizing an evaluation framework\nbased on GPT-4V, our findings demonstrate a clear improvement in performance\nwith our methods over the prevalent baseline, particularly evident when\nincreasing the number of LoRAs in a composition. The code, benchmarks, LoRA\nweights, and all evaluation details are available on our project website:\nhttps://maszhongming.github.io/Multi-LoRA-Composition.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.GR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Transactions on Machine Learning Research (TMLR), 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.16843v2",
    "published_date": "2024-02-26 18:59:18 UTC",
    "updated_date": "2024-11-19 02:52:45 UTC"
  },
  {
    "arxiv_id": "2402.16836v1",
    "title": "PhyGrasp: Generalizing Robotic Grasping with Physics-informed Large Multimodal Models",
    "authors": [
      "Dingkun Guo",
      "Yuqi Xiang",
      "Shuqi Zhao",
      "Xinghao Zhu",
      "Masayoshi Tomizuka",
      "Mingyu Ding",
      "Wei Zhan"
    ],
    "abstract": "Robotic grasping is a fundamental aspect of robot functionality, defining how\nrobots interact with objects. Despite substantial progress, its\ngeneralizability to counter-intuitive or long-tailed scenarios, such as objects\nwith uncommon materials or shapes, remains a challenge. In contrast, humans can\neasily apply their intuitive physics to grasp skillfully and change grasps\nefficiently, even for objects they have never seen before.\n  This work delves into infusing such physical commonsense reasoning into\nrobotic manipulation. We introduce PhyGrasp, a multimodal large model that\nleverages inputs from two modalities: natural language and 3D point clouds,\nseamlessly integrated through a bridge module. The language modality exhibits\nrobust reasoning capabilities concerning the impacts of diverse physical\nproperties on grasping, while the 3D modality comprehends object shapes and\nparts. With these two capabilities, PhyGrasp is able to accurately assess the\nphysical properties of object parts and determine optimal grasping poses.\nAdditionally, the model's language comprehension enables human instruction\ninterpretation, generating grasping poses that align with human preferences. To\ntrain PhyGrasp, we construct a dataset PhyPartNet with 195K object instances\nwith varying physical properties and human preferences, alongside their\ncorresponding language descriptions. Extensive experiments conducted in the\nsimulation and on the real robots demonstrate that PhyGrasp achieves\nstate-of-the-art performance, particularly in long-tailed cases, e.g., about\n10% improvement in success rate over GraspNet. Project page:\nhttps://sites.google.com/view/phygrasp",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16836v1",
    "published_date": "2024-02-26 18:57:52 UTC",
    "updated_date": "2024-02-26 18:57:52 UTC"
  },
  {
    "arxiv_id": "2402.16832v2",
    "title": "Cross-Modal Projection in Multimodal LLMs Doesn't Really Project Visual Attributes to Textual Space",
    "authors": [
      "Gaurav Verma",
      "Minje Choi",
      "Kartik Sharma",
      "Jamelle Watson-Daniels",
      "Sejoon Oh",
      "Srijan Kumar"
    ],
    "abstract": "Multimodal large language models (MLLMs) like LLaVA and GPT-4(V) enable\ngeneral-purpose conversations about images with the language modality. As\noff-the-shelf MLLMs may have limited capabilities on images from domains like\ndermatology and agriculture, they must be fine-tuned to unlock domain-specific\napplications. The prevalent architecture of current open-source MLLMs comprises\ntwo major modules: an image-language (cross-modal) projection network and a\nlarge language model. It is desirable to understand the roles of these two\nmodules in modeling domain-specific visual attributes to inform the design of\nfuture models and streamline the interpretability efforts on the current\nmodels. To this end, via experiments on 4 datasets and under 2 fine-tuning\nsettings, we find that as the MLLM is fine-tuned, it indeed gains\ndomain-specific visual capabilities, but the updates do not lead to the\nprojection extracting relevant domain-specific visual attributes. Our results\nindicate that the domain-specific visual attributes are modeled by the LLM,\neven when only the projection is fine-tuned. Through this study, we offer a\npotential reinterpretation of the role of cross-modal projections in MLLM\narchitectures. Project webpage:\nhttps://claws-lab.github.io/projection-in-MLLMs/",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at ACL 2024 (Main, Short)",
    "pdf_url": "http://arxiv.org/pdf/2402.16832v2",
    "published_date": "2024-02-26 18:56:48 UTC",
    "updated_date": "2024-07-21 18:11:34 UTC"
  },
  {
    "arxiv_id": "2402.16828v2",
    "title": "Training Neural Networks from Scratch with Parallel Low-Rank Adapters",
    "authors": [
      "Minyoung Huh",
      "Brian Cheung",
      "Jeremy Bernstein",
      "Phillip Isola",
      "Pulkit Agrawal"
    ],
    "abstract": "The scalability of deep learning models is fundamentally limited by computing\nresources, memory, and communication. Although methods like low-rank adaptation\n(LoRA) have reduced the cost of model finetuning, its application in model\npre-training remains largely unexplored. This paper explores extending LoRA to\nmodel pre-training, identifying the inherent constraints and limitations of\nstandard LoRA in this context. We introduce LoRA-the-Explorer (LTE), a novel\nbi-level optimization algorithm designed to enable parallel training of\nmultiple low-rank heads across computing nodes, thereby reducing the need for\nfrequent synchronization. Our approach includes extensive experimentation on\nvision transformers using various vision datasets, demonstrating that LTE is\ncompetitive with standard pre-training.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16828v2",
    "published_date": "2024-02-26 18:55:13 UTC",
    "updated_date": "2024-07-26 21:56:47 UTC"
  },
  {
    "arxiv_id": "2402.16823v3",
    "title": "Language Agents as Optimizable Graphs",
    "authors": [
      "Mingchen Zhuge",
      "Wenyi Wang",
      "Louis Kirsch",
      "Francesco Faccio",
      "Dmitrii Khizbullin",
      "Jürgen Schmidhuber"
    ],
    "abstract": "Various human-designed prompt engineering techniques have been proposed to\nimprove problem solvers based on Large Language Models (LLMs), yielding many\ndisparate code bases. We unify these approaches by describing LLM-based agents\nas computational graphs. The nodes implement functions to process multimodal\ndata or query LLMs, and the edges describe the information flow between\noperations. Graphs can be recursively combined into larger composite graphs\nrepresenting hierarchies of inter-agent collaboration (where edges connect\noperations of different agents). Our novel automatic graph optimizers (1)\nrefine node-level LLM prompts (node optimization) and (2) improve agent\norchestration by changing graph connectivity (edge optimization). Experiments\ndemonstrate that our framework can be used to efficiently develop, integrate,\nand automatically improve various LLM agents. The code can be found at\nhttps://github.com/metauto-ai/gptswarm.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "Project Website: https://gptswarm.org ; Github Repo:\n  https://github.com/metauto-ai/gptswarm . In Forty-first International\n  Conference on Machine Learning (2024)",
    "pdf_url": "http://arxiv.org/pdf/2402.16823v3",
    "published_date": "2024-02-26 18:48:27 UTC",
    "updated_date": "2024-08-22 13:06:51 UTC"
  },
  {
    "arxiv_id": "2402.16822v3",
    "title": "Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts",
    "authors": [
      "Mikayel Samvelyan",
      "Sharath Chandra Raparthy",
      "Andrei Lupu",
      "Eric Hambro",
      "Aram H. Markosyan",
      "Manish Bhatt",
      "Yuning Mao",
      "Minqi Jiang",
      "Jack Parker-Holder",
      "Jakob Foerster",
      "Tim Rocktäschel",
      "Roberta Raileanu"
    ],
    "abstract": "As large language models (LLMs) become increasingly prevalent across many\nreal-world applications, understanding and enhancing their robustness to\nadversarial attacks is of paramount importance. Existing methods for\nidentifying adversarial prompts tend to focus on specific domains, lack\ndiversity, or require extensive human annotations. To address these\nlimitations, we present Rainbow Teaming, a novel black-box approach for\nproducing a diverse collection of adversarial prompts. Rainbow Teaming casts\nadversarial prompt generation as a quality-diversity problem and uses\nopen-ended search to generate prompts that are both effective and diverse.\nFocusing on the safety domain, we use Rainbow Teaming to target various\nstate-of-the-art LLMs, including the Llama 2 and Llama 3 models. Our approach\nreveals hundreds of effective adversarial prompts, with an attack success rate\nexceeding 90% across all tested models. Furthermore, we demonstrate that\nprompts generated by Rainbow Teaming are highly transferable and that\nfine-tuning models with synthetic data generated by our method significantly\nenhances their safety without sacrificing general performance or helpfulness.\nWe additionally explore the versatility of Rainbow Teaming by applying it to\nquestion answering and cybersecurity, showcasing its potential to drive robust\nopen-ended self-improvement in a wide range of applications.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16822v3",
    "published_date": "2024-02-26 18:47:27 UTC",
    "updated_date": "2024-12-11 18:07:25 UTC"
  },
  {
    "arxiv_id": "2402.16819v2",
    "title": "Nemotron-4 15B Technical Report",
    "authors": [
      "Jupinder Parmar",
      "Shrimai Prabhumoye",
      "Joseph Jennings",
      "Mostofa Patwary",
      "Sandeep Subramanian",
      "Dan Su",
      "Chen Zhu",
      "Deepak Narayanan",
      "Aastha Jhunjhunwala",
      "Ayush Dattagupta",
      "Vibhu Jawa",
      "Jiwei Liu",
      "Ameya Mahabaleshwarkar",
      "Osvald Nitski",
      "Annika Brundyn",
      "James Maki",
      "Miguel Martinez",
      "Jiaxuan You",
      "John Kamalu",
      "Patrick LeGresley",
      "Denys Fridman",
      "Jared Casper",
      "Ashwath Aithal",
      "Oleksii Kuchaiev",
      "Mohammad Shoeybi",
      "Jonathan Cohen",
      "Bryan Catanzaro"
    ],
    "abstract": "We introduce Nemotron-4 15B, a 15-billion-parameter large multilingual\nlanguage model trained on 8 trillion text tokens. Nemotron-4 15B demonstrates\nstrong performance when assessed on English, multilingual, and coding tasks: it\noutperforms all existing similarly-sized open models on 4 out of 7 downstream\nevaluation areas and achieves competitive performance to the leading open\nmodels in the remaining ones. Specifically, Nemotron-4 15B exhibits the best\nmultilingual capabilities of all similarly-sized models, even outperforming\nmodels over four times larger and those explicitly specialized for multilingual\ntasks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16819v2",
    "published_date": "2024-02-26 18:43:45 UTC",
    "updated_date": "2024-02-27 15:22:57 UTC"
  },
  {
    "arxiv_id": "2402.17793v1",
    "title": "A Surprising Failure? Multimodal LLMs and the NLVR Challenge",
    "authors": [
      "Anne Wu",
      "Kianté Brantley",
      "Yoav Artzi"
    ],
    "abstract": "This study evaluates three state-of-the-art MLLMs -- GPT-4V, Gemini Pro, and\nthe open-source model IDEFICS -- on the compositional natural language vision\nreasoning task NLVR. Given a human-written sentence paired with a synthetic\nimage, this task requires the model to determine the truth value of the\nsentence with respect to the image. Despite the strong performance demonstrated\nby these models, we observe they perform poorly on NLVR, which was constructed\nto require compositional and spatial reasoning, and to be robust for semantic\nand systematic biases.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.17793v1",
    "published_date": "2024-02-26 18:37:18 UTC",
    "updated_date": "2024-02-26 18:37:18 UTC"
  },
  {
    "arxiv_id": "2402.16795v2",
    "title": "If in a Crowdsourced Data Annotation Pipeline, a GPT-4",
    "authors": [
      "Zeyu He",
      "Chieh-Yang Huang",
      "Chien-Kuang Cornelia Ding",
      "Shaurya Rohatgi",
      "Ting-Hao 'Kenneth' Huang"
    ],
    "abstract": "Recent studies indicated GPT-4 outperforms online crowd workers in data\nlabeling accuracy, notably workers from Amazon Mechanical Turk (MTurk).\nHowever, these studies were criticized for deviating from standard\ncrowdsourcing practices and emphasizing individual workers' performances over\nthe whole data-annotation process. This paper compared GPT-4 and an ethical and\nwell-executed MTurk pipeline, with 415 workers labeling 3,177 sentence segments\nfrom 200 scholarly articles using the CODA-19 scheme. Two worker interfaces\nyielded 127,080 labels, which were then used to infer the final labels through\neight label-aggregation algorithms. Our evaluation showed that despite best\npractices, MTurk pipeline's highest accuracy was 81.5%, whereas GPT-4 achieved\n83.6%. Interestingly, when combining GPT-4's labels with crowd labels collected\nvia an advanced worker interface for aggregation, 2 out of the 8 algorithms\nachieved an even higher accuracy (87.5%, 87.0%). Further analysis suggested\nthat, when the crowd's and GPT-4's labeling strengths are complementary,\naggregating them could increase labeling accuracy.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted By CHI 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.16795v2",
    "published_date": "2024-02-26 18:08:52 UTC",
    "updated_date": "2024-06-28 19:33:48 UTC"
  },
  {
    "arxiv_id": "2402.16790v1",
    "title": "Beyond Self-learned Attention: Mitigating Attention Bias in Transformer-based Models Using Attention Guidance",
    "authors": [
      "Jiri Gesi",
      "Iftekhar Ahmed"
    ],
    "abstract": "Transformer-based models have demonstrated considerable potential for source\ncode modeling tasks in software engineering. However, they are limited by their\ndependence solely on automatic self-attention weight learning mechanisms.\nPrevious studies have shown that these models overemphasize delimiters added by\ntokenizers (e.g., [CLS], [SEP]), which may lead to overlooking essential\ninformation in the original input source code. To address this challenge, we\nintroduce SyntaGuid, a novel approach that utilizes the observation that\nattention weights tend to be biased towards specific source code syntax tokens\nand abstract syntax tree (AST) elements in fine-tuned language models when they\nmake correct predictions. SyntaGuid facilitates the guidance of\nattention-weight learning, leading to improved model performance on various\nsoftware engineering tasks. We evaluate the effectiveness of SyntaGuid on\nmultiple tasks and demonstrate that it outperforms existing state-of-the-art\nmodels in overall performance without requiring additional data. Experimental\nresult shows that SyntaGuid can improve overall performance up to 3.25% and fix\nup to 28.3% wrong predictions. Our work represents the first attempt to guide\nthe attention of Transformer-based models towards critical source code tokens\nduring fine-tuning, highlighting the potential for enhancing Transformer-based\nmodels in software engineering.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16790v1",
    "published_date": "2024-02-26 18:03:50 UTC",
    "updated_date": "2024-02-26 18:03:50 UTC"
  },
  {
    "arxiv_id": "2402.16788v4",
    "title": "Why Transformers Need Adam: A Hessian Perspective",
    "authors": [
      "Yushun Zhang",
      "Congliang Chen",
      "Tian Ding",
      "Ziniu Li",
      "Ruoyu Sun",
      "Zhi-Quan Luo"
    ],
    "abstract": "SGD performs worse than Adam by a significant margin on Transformers, but the\nreason remains unclear. In this work, we provide an explanation through the\nlens of Hessian: (i) Transformers are \"heterogeneous\": the Hessian spectrum\nacross parameter blocks vary dramatically, a phenomenon we call \"block\nheterogeneity\"; (ii) Heterogeneity hampers SGD: SGD performs worse than Adam on\nproblems with block heterogeneity. To validate (i) and (ii), we check various\nTransformers, CNNs, MLPs, and quadratic problems, and find that SGD can perform\non par with Adam on problems without block heterogeneity, but performs worse\nthan Adam when the heterogeneity exists. Our initial theoretical analysis\nindicates that SGD performs worse because it applies one single learning rate\nto all blocks, which cannot handle the heterogeneity among blocks. This\nlimitation could be ameliorated if we use coordinate-wise learning rates, as\ndesigned in Adam.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Advances in Neural Information Processing Systems, 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.16788v4",
    "published_date": "2024-02-26 18:01:41 UTC",
    "updated_date": "2024-10-21 08:27:23 UTC"
  },
  {
    "arxiv_id": "2402.16786v2",
    "title": "Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models",
    "authors": [
      "Paul Röttger",
      "Valentin Hofmann",
      "Valentina Pyatkin",
      "Musashi Hinck",
      "Hannah Rose Kirk",
      "Hinrich Schütze",
      "Dirk Hovy"
    ],
    "abstract": "Much recent work seeks to evaluate values and opinions in large language\nmodels (LLMs) using multiple-choice surveys and questionnaires. Most of this\nwork is motivated by concerns around real-world LLM applications. For example,\npolitically-biased LLMs may subtly influence society when they are used by\nmillions of people. Such real-world concerns, however, stand in stark contrast\nto the artificiality of current evaluations: real users do not typically ask\nLLMs survey questions. Motivated by this discrepancy, we challenge the\nprevailing constrained evaluation paradigm for values and opinions in LLMs and\nexplore more realistic unconstrained evaluations. As a case study, we focus on\nthe popular Political Compass Test (PCT). In a systematic review, we find that\nmost prior work using the PCT forces models to comply with the PCT's\nmultiple-choice format. We show that models give substantively different\nanswers when not forced; that answers change depending on how models are\nforced; and that answers lack paraphrase robustness. Then, we demonstrate that\nmodels give different answers yet again in a more realistic open-ended answer\nsetting. We distill these findings into recommendations and open challenges in\nevaluating values and opinions in LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at ACL 2024 (Main Conference)",
    "pdf_url": "http://arxiv.org/pdf/2402.16786v2",
    "published_date": "2024-02-26 18:00:49 UTC",
    "updated_date": "2024-06-05 10:17:53 UTC"
  },
  {
    "arxiv_id": "2402.16934v1",
    "title": "FedReview: A Review Mechanism for Rejecting Poisoned Updates in Federated Learning",
    "authors": [
      "Tianhang Zheng",
      "Baochun Li"
    ],
    "abstract": "Federated learning has recently emerged as a decentralized approach to learn\na high-performance model without access to user data. Despite its\neffectiveness, federated learning gives malicious users opportunities to\nmanipulate the model by uploading poisoned model updates to the server. In this\npaper, we propose a review mechanism called FedReview to identify and decline\nthe potential poisoned updates in federated learning. Under our mechanism, the\nserver randomly assigns a subset of clients as reviewers to evaluate the model\nupdates on their training datasets in each round. The reviewers rank the model\nupdates based on the evaluation results and count the number of the updates\nwith relatively low quality as the estimated number of poisoned updates. Based\non review reports, the server employs a majority voting mechanism to integrate\nthe rankings and remove the potential poisoned updates in the model aggregation\nprocess. Extensive evaluation on multiple datasets demonstrate that FedReview\ncan assist the server to learn a well-performed global model in an adversarial\nenvironment.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16934v1",
    "published_date": "2024-02-26 17:53:15 UTC",
    "updated_date": "2024-02-26 17:53:15 UTC"
  },
  {
    "arxiv_id": "2403.00822v2",
    "title": "InteraRec: Screenshot Based Recommendations Using Multimodal Large Language Models",
    "authors": [
      "Saketh Reddy Karra",
      "Theja Tulabandhula"
    ],
    "abstract": "Weblogs, comprised of records detailing user activities on any website, offer\nvaluable insights into user preferences, behavior, and interests. Numerous\nrecommendation algorithms, employing strategies such as collaborative\nfiltering, content-based filtering, and hybrid methods, leverage the data mined\nthrough these weblogs to provide personalized recommendations to users. Despite\nthe abundance of information available in these weblogs, identifying and\nextracting pertinent information and key features from them necessitate\nextensive engineering endeavors. The intricate nature of the data also poses a\nchallenge for interpretation, especially for non-experts. In this study, we\nintroduce a sophisticated and interactive recommendation framework denoted as\nInteraRec, which diverges from conventional approaches that exclusively depend\non weblogs for recommendation generation. InteraRec framework captures\nhigh-frequency screenshots of web pages as users navigate through a website.\nLeveraging state-of-the-art multimodal large language models (MLLMs), it\nextracts valuable insights into user preferences from these screenshots by\ngenerating a textual summary based on predefined keywords. Subsequently, an\nLLM-integrated optimization setup utilizes this summary to generate tailored\nrecommendations. Through our experiments, we demonstrate the effectiveness of\nInteraRec in providing users with valuable and personalized offerings.\nFurthermore, we explore the integration of session-based recommendation systems\ninto the InteraRec framework, aiming to enhance its overall performance.\nFinally, we curate a new dataset comprising of screenshots from product web\npages on the Amazon website for the validation of the InteraRec framework.\nDetailed experiments demonstrate the efficacy of the InteraRec framework in\ndelivering valuable and personalized recommendations tailored to individual\nuser preferences.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.00822v2",
    "published_date": "2024-02-26 17:47:57 UTC",
    "updated_date": "2024-06-16 00:40:15 UTC"
  },
  {
    "arxiv_id": "2402.16775v2",
    "title": "A Comprehensive Evaluation of Quantization Strategies for Large Language Models",
    "authors": [
      "Renren Jin",
      "Jiangcun Du",
      "Wuwei Huang",
      "Wei Liu",
      "Jian Luan",
      "Bin Wang",
      "Deyi Xiong"
    ],
    "abstract": "Increasing the number of parameters in large language models (LLMs) usually\nimproves performance in downstream tasks but raises compute and memory costs,\nmaking deployment difficult in resource-limited settings. Quantization\ntechniques, which reduce the bits needed for model weights or activations with\nminimal performance loss, have become popular due to the rise of LLMs. However,\nmost quantization studies use pre-trained LLMs, and the impact of quantization\non instruction-tuned LLMs and the relationship between perplexity and benchmark\nperformance of quantized LLMs are not well understood. Evaluation of quantized\nLLMs is often limited to language modeling and a few classification tasks,\nleaving their performance on other benchmarks unclear. To address these gaps,\nwe propose a structured evaluation framework consisting of three critical\ndimensions: (1) knowledge \\& capacity, (2) alignment, and (3) efficiency, and\nconduct extensive experiments across ten diverse benchmarks. Our experimental\nresults indicate that LLMs with 4-bit quantization can retain performance\ncomparable to their non-quantized counterparts, and perplexity can serve as a\nproxy metric for quantized LLMs on most benchmarks. Furthermore, quantized LLMs\nwith larger parameter scales can outperform smaller LLMs. Despite the memory\nsavings achieved through quantization, it can also slow down the inference\nspeed of LLMs. Consequently, substantial engineering efforts and hardware\nsupport are imperative to achieve a balanced optimization of decoding speed and\nmemory consumption in the context of quantized LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL 2024 Findings",
    "pdf_url": "http://arxiv.org/pdf/2402.16775v2",
    "published_date": "2024-02-26 17:45:36 UTC",
    "updated_date": "2024-06-06 13:38:26 UTC"
  },
  {
    "arxiv_id": "2402.16763v2",
    "title": "ELiSe: Efficient Learning of Sequences in Structured Recurrent Networks",
    "authors": [
      "Laura Kriener",
      "Kristin Völk",
      "Ben von Hünerbein",
      "Federico Benitez",
      "Walter Senn",
      "Mihai A. Petrovici"
    ],
    "abstract": "Behavior can be described as a temporal sequence of actions driven by neural\nactivity. To learn complex sequential patterns in neural networks, memories of\npast activities need to persist on significantly longer timescales than the\nrelaxation times of single-neuron activity. While recurrent networks can\nproduce such long transients, training these networks is a challenge. Learning\nvia error propagation confers models such as FORCE, RTRL or BPTT a significant\nfunctional advantage, but at the expense of biological plausibility. While\nreservoir computing circumvents this issue by learning only the readout\nweights, it does not scale well with problem complexity. We propose that two\nprominent structural features of cortical networks can alleviate these issues:\nthe presence of a certain network scaffold at the onset of learning and the\nexistence of dendritic compartments for enhancing neuronal information storage\nand computation. Our resulting model for Efficient Learning of Sequences\n(ELiSe) builds on these features to acquire and replay complex non-Markovian\nspatio-temporal patterns using only local, always-on and phase-free synaptic\nplasticity. We showcase the capabilities of ELiSe in a mock-up of birdsong\nlearning, and demonstrate its flexibility with respect to parametrization, as\nwell as its robustness to external disturbances.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "q-bio.NC",
    "comment": "15 pages, 7 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2402.16763v2",
    "published_date": "2024-02-26 17:30:34 UTC",
    "updated_date": "2024-09-27 11:49:58 UTC"
  },
  {
    "arxiv_id": "2402.16933v2",
    "title": "Incremental Concept Formation over Visual Images Without Catastrophic Forgetting",
    "authors": [
      "Nicki Barari",
      "Xin Lian",
      "Christopher J. MacLellan"
    ],
    "abstract": "Deep neural networks have excelled in machine learning, particularly in\nvision tasks, however, they often suffer from catastrophic forgetting when\nlearning new tasks sequentially. In this work, we introduce Cobweb4V, an\nalternative to traditional neural network approaches. Cobweb4V is a novel\nvisual classification method that builds on Cobweb, a human like learning\nsystem that is inspired by the way humans incrementally learn new concepts over\ntime. In this research, we conduct a comprehensive evaluation, showcasing\nCobweb4Vs proficiency in learning visual concepts, requiring less data to\nachieve effective learning outcomes compared to traditional methods,\nmaintaining stable performance over time, and achieving commendable asymptotic\nbehavior, without catastrophic forgetting effects. These characteristics align\nwith learning strategies in human cognition, positioning Cobweb4V as a\npromising alternative to neural network approaches.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.IR"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by The Eleventh Annual Conference on Advances in Cognitive\n  Systems",
    "pdf_url": "http://arxiv.org/pdf/2402.16933v2",
    "published_date": "2024-02-26 17:20:16 UTC",
    "updated_date": "2024-09-19 03:32:16 UTC"
  },
  {
    "arxiv_id": "2402.16751v3",
    "title": "Value Preferences Estimation and Disambiguation in Hybrid Participatory Systems",
    "authors": [
      "Enrico Liscio",
      "Luciano C. Siebert",
      "Catholijn M. Jonker",
      "Pradeep K. Murukannaiah"
    ],
    "abstract": "Understanding citizens' values in participatory systems is crucial for\ncitizen-centric policy-making. We envision a hybrid participatory system where\nparticipants make choices and provide motivations for those choices, and AI\nagents estimate their value preferences by interacting with them. We focus on\nsituations where a conflict is detected between participants' choices and\nmotivations, and propose methods for estimating value preferences while\naddressing detected inconsistencies by interacting with the participants. We\noperationalize the philosophical stance that \"valuing is deliberatively\nconsequential.\" That is, if a participant's choice is based on a deliberation\nof value preferences, the value preferences can be observed in the motivation\nthe participant provides for the choice. Thus, we propose and compare value\npreferences estimation methods that prioritize the values estimated from\nmotivations over the values estimated from choices alone. Then, we introduce a\ndisambiguation strategy that combines Natural Language Processing and Active\nLearning to address the detected inconsistencies between choices and\nmotivations. We evaluate the proposed methods on a dataset of a large-scale\nsurvey on energy transition. The results show that explicitly addressing\ninconsistencies between choices and motivations improves the estimation of an\nindividual's value preferences. The disambiguation strategy does not show\nsubstantial improvements when compared to similar baselines--however, we\ndiscuss how the novelty of the approach can open new research avenues and\npropose improvements to address the current limitations.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16751v3",
    "published_date": "2024-02-26 17:16:28 UTC",
    "updated_date": "2025-02-11 07:00:28 UTC"
  },
  {
    "arxiv_id": "2402.16749v3",
    "title": "MISC: Ultra-low Bitrate Image Semantic Compression Driven by Large Multimodal Model",
    "authors": [
      "Chunyi Li",
      "Guo Lu",
      "Donghui Feng",
      "Haoning Wu",
      "Zicheng Zhang",
      "Xiaohong Liu",
      "Guangtao Zhai",
      "Weisi Lin",
      "Wenjun Zhang"
    ],
    "abstract": "With the evolution of storage and communication protocols, ultra-low bitrate\nimage compression has become a highly demanding topic. However, existing\ncompression algorithms must sacrifice either consistency with the ground truth\nor perceptual quality at ultra-low bitrate. In recent years, the rapid\ndevelopment of the Large Multimodal Model (LMM) has made it possible to balance\nthese two goals. To solve this problem, this paper proposes a method called\nMultimodal Image Semantic Compression (MISC), which consists of an LMM encoder\nfor extracting the semantic information of the image, a map encoder to locate\nthe region corresponding to the semantic, an image encoder generates an\nextremely compressed bitstream, and a decoder reconstructs the image based on\nthe above information. Experimental results show that our proposed MISC is\nsuitable for compressing both traditional Natural Sense Images (NSIs) and\nemerging AI-Generated Images (AIGIs) content. It can achieve optimal\nconsistency and perception results while saving 50% bitrate, which has strong\npotential applications in the next generation of storage and communication. The\ncode will be released on https://github.com/lcysyzxdxc/MISC.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "13 page, 11 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2402.16749v3",
    "published_date": "2024-02-26 17:11:11 UTC",
    "updated_date": "2024-04-17 14:06:28 UTC"
  },
  {
    "arxiv_id": "2402.16726v4",
    "title": "Towards Empirical Interpretation of Internal Circuits and Properties in Grokked Transformers on Modular Polynomials",
    "authors": [
      "Hiroki Furuta",
      "Gouki Minegishi",
      "Yusuke Iwasawa",
      "Yutaka Matsuo"
    ],
    "abstract": "Grokking has been actively explored to reveal the mystery of delayed\ngeneralization and identifying interpretable representations and algorithms\ninside the grokked models is a suggestive hint to understanding its mechanism.\nGrokking on modular addition has been known to implement Fourier representation\nand its calculation circuits with trigonometric identities in Transformers.\nConsidering the periodicity in modular arithmetic, the natural question is to\nwhat extent these explanations and interpretations hold for the grokking on\nother modular operations beyond addition. For a closer look, we first\nhypothesize that any modular operations can be characterized with distinctive\nFourier representation or internal circuits, grokked models obtain common\nfeatures transferable among similar operations, and mixing datasets with\nsimilar operations promotes grokking. Then, we extensively examine them by\nlearning Transformers on complex modular arithmetic tasks, including\npolynomials. Our Fourier analysis and novel progress measure for modular\narithmetic, Fourier Frequency Density and Fourier Coefficient Ratio,\ncharacterize distinctive internal representations of grokked models per modular\noperation; for instance, polynomials often result in the superposition of the\nFourier components seen in elementary arithmetic, but clear patterns do not\nemerge in challenging non-factorizable polynomials. In contrast, our ablation\nstudy on the pre-grokked models reveals that the transferability among the\nmodels grokked with each operation can be only limited to specific\ncombinations, such as from elementary arithmetic to linear expressions.\nMoreover, some multi-task mixtures may lead to co-grokking -- where grokking\nsimultaneously happens for all the tasks -- and accelerate generalization,\nwhile others may not find optimal solutions. We provide empirical steps towards\nthe interpretability of internal circuits.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published at Transactions on Machine Learning Research (TMLR), Code:\n  https://github.com/frt03/grok_mod_poly",
    "pdf_url": "http://arxiv.org/pdf/2402.16726v4",
    "published_date": "2024-02-26 16:48:12 UTC",
    "updated_date": "2024-12-30 11:00:27 UTC"
  },
  {
    "arxiv_id": "2402.16718v1",
    "title": "An Overview of the Development of Stereotactic Body Radiation Therapy",
    "authors": [
      "Yanqi Zong",
      "Zhengrong Cui",
      "Luqi Lin",
      "Sihao Wang",
      "Yizhi Chen"
    ],
    "abstract": "Stereotactic body radiation therapy (SBRT) refers to focusing high-energy\nrays in three-dimensional space on the tumor lesion area, reducing the dose\nreceived by surrounding normal tissues, which can effectively improve the local\ncontrol rate of the tumor and reduce the probability of complications. With the\ncomprehensive development of medical imaging, radiation biology and other\ndisciplines, this less-fractional, high-dose radiotherapy method has been\nincreasingly developed and applied in clinical practice. The background,\nradio-biological basis, key technologies and main equipment of SBRT are\ndiscussed, and its future development direction is prospected.",
    "categories": [
      "physics.med-ph",
      "cs.AI"
    ],
    "primary_category": "physics.med-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16718v1",
    "published_date": "2024-02-26 16:38:22 UTC",
    "updated_date": "2024-02-26 16:38:22 UTC"
  },
  {
    "arxiv_id": "2402.16717v1",
    "title": "CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models",
    "authors": [
      "Huijie Lv",
      "Xiao Wang",
      "Yuansen Zhang",
      "Caishuang Huang",
      "Shihan Dou",
      "Junjie Ye",
      "Tao Gui",
      "Qi Zhang",
      "Xuanjing Huang"
    ],
    "abstract": "Adversarial misuse, particularly through `jailbreaking' that circumvents a\nmodel's safety and ethical protocols, poses a significant challenge for Large\nLanguage Models (LLMs). This paper delves into the mechanisms behind such\nsuccessful attacks, introducing a hypothesis for the safety mechanism of\naligned LLMs: intent security recognition followed by response generation.\nGrounded in this hypothesis, we propose CodeChameleon, a novel jailbreak\nframework based on personalized encryption tactics. To elude the intent\nsecurity recognition phase, we reformulate tasks into a code completion format,\nenabling users to encrypt queries using personalized encryption functions. To\nguarantee response generation functionality, we embed a decryption function\nwithin the instructions, which allows the LLM to decrypt and execute the\nencrypted queries successfully. We conduct extensive experiments on 7 LLMs,\nachieving state-of-the-art average Attack Success Rate (ASR). Remarkably, our\nmethod achieves an 86.6\\% ASR on GPT-4-1106.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16717v1",
    "published_date": "2024-02-26 16:35:59 UTC",
    "updated_date": "2024-02-26 16:35:59 UTC"
  },
  {
    "arxiv_id": "2402.16714v2",
    "title": "Quantum linear algebra is all you need for Transformer architectures",
    "authors": [
      "Naixu Guo",
      "Zhan Yu",
      "Matthew Choi",
      "Aman Agrawal",
      "Kouhei Nakaji",
      "Alán Aspuru-Guzik",
      "Patrick Rebentrost"
    ],
    "abstract": "Generative machine learning methods such as large-language models are\nrevolutionizing the creation of text and images. While these models are\npowerful they also harness a large amount of computational resources. The\ntransformer is a key component in large language models that aims to generate a\nsuitable completion of a given partial sequence. In this work, we investigate\ntransformer architectures under the lens of fault-tolerant quantum computing.\nThe input model is one where trained weight matrices are given as block\nencodings and we construct the query, key, and value matrices for the\ntransformer. We show how to prepare a block encoding of the self-attention\nmatrix, with a new subroutine for the row-wise application of the softmax\nfunction. In addition, we combine quantum subroutines to construct important\nbuilding blocks in the transformer, the residual connection and layer\nnormalization, and the feed-forward neural network. Our subroutines prepare an\namplitude encoding of the transformer output, which can be measured to obtain a\nprediction. Based on common open-source large-language models, we provide\ninsights into the behavior of important parameters determining the run time of\nthe quantum algorithm. We discuss the potential and challenges for obtaining a\nquantum advantage.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "quant-ph",
    "comment": "31 pages, 4 figures, 2 tables, comments are welcome",
    "pdf_url": "http://arxiv.org/pdf/2402.16714v2",
    "published_date": "2024-02-26 16:31:28 UTC",
    "updated_date": "2024-05-31 03:34:57 UTC"
  },
  {
    "arxiv_id": "2402.16705v2",
    "title": "SelectIT: Selective Instruction Tuning for LLMs via Uncertainty-Aware Self-Reflection",
    "authors": [
      "Liangxin Liu",
      "Xuebo Liu",
      "Derek F. Wong",
      "Dongfang Li",
      "Ziyi Wang",
      "Baotian Hu",
      "Min Zhang"
    ],
    "abstract": "Instruction tuning (IT) is crucial to tailoring large language models (LLMs)\ntowards human-centric interactions. Recent advancements have shown that the\ncareful selection of a small, high-quality subset of IT data can significantly\nenhance the performance of LLMs. Despite this, common approaches often rely on\nadditional models or data, which increases costs and limits widespread\nadoption. In this work, we propose a novel approach, termed SelectIT, that\ncapitalizes on the foundational capabilities of the LLM itself. Specifically,\nwe exploit the intrinsic uncertainty present in LLMs to more effectively select\nhigh-quality IT data, without the need for extra resources. Furthermore, we\nintroduce a curated IT dataset, the Selective Alpaca, created by applying\nSelectIT to the Alpaca-GPT4 dataset. Empirical results demonstrate that IT\nusing Selective Alpaca leads to substantial model ability enhancement. The\nrobustness of SelectIT has also been corroborated in various foundation models\nand domain-specific tasks. Our findings suggest that longer and more\ncomputationally intensive IT data may serve as superior sources of IT, offering\nvaluable insights for future research in this area. Data, code, and scripts are\nfreely available at https://github.com/Blue-Raincoat/SelectIT.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.16705v2",
    "published_date": "2024-02-26 16:21:53 UTC",
    "updated_date": "2025-01-15 08:20:19 UTC"
  },
  {
    "arxiv_id": "2402.16700v1",
    "title": "Generating Effective Ensembles for Sentiment Analysis",
    "authors": [
      "Itay Etelis",
      "Avi Rosenfeld",
      "Abraham Itzhak Weinberg",
      "David Sarne"
    ],
    "abstract": "In recent years, transformer models have revolutionized Natural Language\nProcessing (NLP), achieving exceptional results across various tasks, including\nSentiment Analysis (SA). As such, current state-of-the-art approaches for SA\npredominantly rely on transformer models alone, achieving impressive accuracy\nlevels on benchmark datasets. In this paper, we show that the key for further\nimproving the accuracy of such ensembles for SA is to include not only\ntransformers, but also traditional NLP models, despite the inferiority of the\nlatter compared to transformer models. However, as we empirically show, this\nnecessitates a change in how the ensemble is constructed, specifically relying\non the Hierarchical Ensemble Construction (HEC) algorithm we present. Our\nempirical studies across eight canonical SA datasets reveal that ensembles\nincorporating a mix of model types, structured via HEC, significantly\noutperform traditional ensembles. Finally, we provide a comparative analysis of\nthe performance of the HEC and GPT-4, demonstrating that while GPT-4 closely\napproaches state-of-the-art SA methods, it remains outperformed by our proposed\nensemble strategy.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16700v1",
    "published_date": "2024-02-26 16:14:47 UTC",
    "updated_date": "2024-02-26 16:14:47 UTC"
  },
  {
    "arxiv_id": "2402.16689v1",
    "title": "Adaptation of Biomedical and Clinical Pretrained Models to French Long Documents: A Comparative Study",
    "authors": [
      "Adrien Bazoge",
      "Emmanuel Morin",
      "Beatrice Daille",
      "Pierre-Antoine Gourraud"
    ],
    "abstract": "Recently, pretrained language models based on BERT have been introduced for\nthe French biomedical domain. Although these models have achieved\nstate-of-the-art results on biomedical and clinical NLP tasks, they are\nconstrained by a limited input sequence length of 512 tokens, which poses\nchallenges when applied to clinical notes. In this paper, we present a\ncomparative study of three adaptation strategies for long-sequence models,\nleveraging the Longformer architecture. We conducted evaluations of these\nmodels on 16 downstream tasks spanning both biomedical and clinical domains.\nOur findings reveal that further pre-training an English clinical model with\nFrench biomedical texts can outperform both converting a French biomedical BERT\nto the Longformer architecture and pre-training a French biomedical Longformer\nfrom scratch. The results underscore that long-sequence French biomedical\nmodels improve performance across most downstream tasks regardless of sequence\nlength, but BERT based models remain the most efficient for named entity\nrecognition tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16689v1",
    "published_date": "2024-02-26 16:05:33 UTC",
    "updated_date": "2024-02-26 16:05:33 UTC"
  },
  {
    "arxiv_id": "2402.16684v1",
    "title": "Automated Floodwater Depth Estimation Using Large Multimodal Model for Rapid Flood Mapping",
    "authors": [
      "Temitope Akinboyewa",
      "Huan Ning",
      "M. Naser Lessani",
      "Zhenlong Li"
    ],
    "abstract": "Information on the depth of floodwater is crucial for rapid mapping of areas\naffected by floods. However, previous approaches for estimating floodwater\ndepth, including field surveys, remote sensing, and machine learning\ntechniques, can be time-consuming and resource-intensive. This paper presents\nan automated and fast approach for estimating floodwater depth from on-site\nflood photos. A pre-trained large multimodal model, GPT-4 Vision, was used\nspecifically for estimating floodwater. The input data were flooding photos\nthat contained referenced objects, such as street signs, cars, people, and\nbuildings. Using the heights of the common objects as references, the model\nreturned the floodwater depth as the output. Results show that the proposed\napproach can rapidly provide a consistent and reliable estimation of floodwater\ndepth from flood photos. Such rapid estimation is transformative in flood\ninundation mapping and assessing the severity of the flood in near-real time,\nwhich is essential for effective flood response strategies.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16684v1",
    "published_date": "2024-02-26 16:02:15 UTC",
    "updated_date": "2024-02-26 16:02:15 UTC"
  },
  {
    "arxiv_id": "2402.16668v1",
    "title": "Program-Based Strategy Induction for Reinforcement Learning",
    "authors": [
      "Carlos G. Correa",
      "Thomas L. Griffiths",
      "Nathaniel D. Daw"
    ],
    "abstract": "Typical models of learning assume incremental estimation of\ncontinuously-varying decision variables like expected rewards. However, this\nclass of models fails to capture more idiosyncratic, discrete heuristics and\nstrategies that people and animals appear to exhibit. Despite recent advances\nin strategy discovery using tools like recurrent networks that generalize the\nclassic models, the resulting strategies are often onerous to interpret, making\nconnections to cognition difficult to establish. We use Bayesian program\ninduction to discover strategies implemented by programs, letting the\nsimplicity of strategies trade off against their effectiveness. Focusing on\nbandit tasks, we find strategies that are difficult or unexpected with\nclassical incremental learning, like asymmetric learning from rewarded and\nunrewarded trials, adaptive horizon-dependent random exploration, and discrete\nstate switching.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16668v1",
    "published_date": "2024-02-26 15:40:46 UTC",
    "updated_date": "2024-02-26 15:40:46 UTC"
  },
  {
    "arxiv_id": "2402.16667v1",
    "title": "RepoAgent: An LLM-Powered Open-Source Framework for Repository-level Code Documentation Generation",
    "authors": [
      "Qinyu Luo",
      "Yining Ye",
      "Shihao Liang",
      "Zhong Zhang",
      "Yujia Qin",
      "Yaxi Lu",
      "Yesai Wu",
      "Xin Cong",
      "Yankai Lin",
      "Yingli Zhang",
      "Xiaoyin Che",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "abstract": "Generative models have demonstrated considerable potential in software\nengineering, particularly in tasks such as code generation and debugging.\nHowever, their utilization in the domain of code documentation generation\nremains underexplored. To this end, we introduce RepoAgent, a large language\nmodel powered open-source framework aimed at proactively generating,\nmaintaining, and updating code documentation. Through both qualitative and\nquantitative evaluations, we have validated the effectiveness of our approach,\nshowing that RepoAgent excels in generating high-quality repository-level\ndocumentation. The code and results are publicly accessible at\nhttps://github.com/OpenBMB/RepoAgent.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7; F.2.2"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16667v1",
    "published_date": "2024-02-26 15:39:52 UTC",
    "updated_date": "2024-02-26 15:39:52 UTC"
  },
  {
    "arxiv_id": "2402.16654v2",
    "title": "GigaPevt: Multimodal Medical Assistant",
    "authors": [
      "Pavel Blinov",
      "Konstantin Egorov",
      "Ivan Sviridov",
      "Nikolay Ivanov",
      "Stepan Botman",
      "Evgeniy Tagin",
      "Stepan Kudin",
      "Galina Zubkova",
      "Andrey Savchenko"
    ],
    "abstract": "Building an intelligent and efficient medical assistant is still a\nchallenging AI problem. The major limitation comes from the data modality\nscarceness, which reduces comprehensive patient perception. This demo paper\npresents the GigaPevt, the first multimodal medical assistant that combines the\ndialog capabilities of large language models with specialized medical models.\nSuch an approach shows immediate advantages in dialog quality and metric\nperformance, with a 1.18% accuracy improvement in the question-answering task.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "68T07",
      "I.2.1"
    ],
    "primary_category": "cs.AI",
    "comment": "IJCAI 2024, 4 pages, 2 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2402.16654v2",
    "published_date": "2024-02-26 15:26:56 UTC",
    "updated_date": "2024-07-30 06:04:31 UTC"
  },
  {
    "arxiv_id": "2402.16651v1",
    "title": "A Comprehensive Survey of Belief Rule Base (BRB) Hybrid Expert system: Bridging Decision Science and Professional Services",
    "authors": [
      "Karim Derrick"
    ],
    "abstract": "The Belief Rule Base (BRB) system that adopts a hybrid approach integrating\nthe precision of expert systems with the adaptability of data-driven models.\nCharacterized by its use of if-then rules to accommodate various types of\nuncertainty through belief degrees, BRB adeptly handles fuzziness, randomness,\nand ignorance. This semi-quantitative tool excels in processing both numerical\ndata and linguistic knowledge from diverse sources, making it as an\nindispensable resource in modelling complex nonlinear systems. Notably, BRB's\ntransparent, white-box nature ensures accessibility and clarity for\ndecision-makers and stakeholders, further enhancing its applicability. With its\ngrowing adoption in fields ranging from decision-making and reliability\nevaluation in network security and fault diagnosis, this study aims to explore\nthe evolution and the multifaceted applications of BRB. By analysing its\ndevelopment across different domains, we highlight BRB's potential to\nrevolutionize sectors traditionally resistant to technological disruption, in\nparticular insurance and law.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16651v1",
    "published_date": "2024-02-26 15:23:27 UTC",
    "updated_date": "2024-02-26 15:23:27 UTC"
  },
  {
    "arxiv_id": "2402.17792v1",
    "title": "EGNN-C+: Interpretable Evolving Granular Neural Network and Application in Classification of Weakly-Supervised EEG Data Streams",
    "authors": [
      "Daniel Leite",
      "Alisson Silva",
      "Gabriella Casalino",
      "Arnab Sharma",
      "Danielle Fortunato",
      "Axel-Cyrille Ngomo"
    ],
    "abstract": "We introduce a modified incremental learning algorithm for evolving Granular\nNeural Network Classifiers (eGNN-C+). We use double-boundary hyper-boxes to\nrepresent granules, and customize the adaptation procedures to enhance the\nrobustness of outer boxes for data coverage and noise suppression, while\nensuring that inner boxes remain flexible to capture drifts. The classifier\nevolves from scratch, incorporates new classes on the fly, and performs local\nincremental feature weighting. As an application, we focus on the\nclassification of emotion-related patterns within electroencephalogram (EEG)\nsignals. Emotion recognition is crucial for enhancing the realism and\ninteractivity of computer systems. We extract features from the Fourier\nspectrum of EEG signals obtained from 28 individuals engaged in playing\ncomputer games -- a public dataset. Each game elicits a different predominant\nemotion: boredom, calmness, horror, or joy. We analyze individual electrodes,\ntime window lengths, and frequency bands to assess the accuracy and\ninterpretability of resulting user-independent neural models. The findings\nindicate that both brain hemispheres assist classification, especially\nelectrodes on the temporal (T8) and parietal (P7) areas, alongside\ncontributions from frontal and occipital electrodes. While patterns may\nmanifest in any band, the Alpha (8-13Hz), Delta (1-4Hz), and Theta (4-8Hz)\nbands, in this order, exhibited higher correspondence with the emotion classes.\nThe eGNN-C+ demonstrates effectiveness in learning EEG data. It achieves an\naccuracy of 81.7% and a 0.0029 II interpretability using 10-second time\nwindows, even in face of a highly-stochastic time-varying 4-class\nclassification problem.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "eess.SP",
    "comment": "10 pages, IEEE International Conference on Evolving and Adaptive\n  Intelligent Systems 2024 (IEEE EAIS 2024)",
    "pdf_url": "http://arxiv.org/pdf/2402.17792v1",
    "published_date": "2024-02-26 15:11:41 UTC",
    "updated_date": "2024-02-26 15:11:41 UTC"
  },
  {
    "arxiv_id": "2402.16929v2",
    "title": "LangGPT: Rethinking Structured Reusable Prompt Design Framework for LLMs from the Programming Language",
    "authors": [
      "Ming Wang",
      "Yuanzhong Liu",
      "Xiaoyu Liang",
      "Songlian Li",
      "Yijie Huang",
      "Xiaoming Zhang",
      "Sijia Shen",
      "Chaofeng Guan",
      "Daling Wang",
      "Shi Feng",
      "Huaiwen Zhang",
      "Yifei Zhang",
      "Minghui Zheng",
      "Chi Zhang"
    ],
    "abstract": "LLMs have demonstrated commendable performance across diverse domains.\nNevertheless, formulating high-quality prompts to instruct LLMs proficiently\nposes a challenge for non-AI experts. Existing research in prompt engineering\nsuggests somewhat scattered optimization principles and designs empirically\ndependent prompt optimizers. Unfortunately, these endeavors lack a structured\ndesign template, incurring high learning costs and resulting in low\nreusability. In addition, it is not conducive to the iterative updating of\nprompts. Inspired by structured reusable programming languages, we propose\nLangGPT, a dual-layer prompt design framework as the programming language for\nLLMs. LangGPT has an easy-to-learn normative structure and provides an extended\nstructure for migration and reuse. Experiments illustrate that LangGPT\nsignificantly enhances the performance of LLMs. Moreover, the case study shows\nthat LangGPT leads LLMs to generate higher-quality responses. Furthermore, we\nanalyzed the ease of use and reusability of LangGPT through a user survey in\nour online community.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.PL"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16929v2",
    "published_date": "2024-02-26 15:05:16 UTC",
    "updated_date": "2024-06-29 14:19:08 UTC"
  },
  {
    "arxiv_id": "2402.16631v3",
    "title": "GenAINet: Enabling Wireless Collective Intelligence via Knowledge Transfer and Reasoning",
    "authors": [
      "Hang Zou",
      "Qiyang Zhao",
      "Samson Lasaulce",
      "Lina Bariah",
      "Mehdi Bennis",
      "Merouane Debbah"
    ],
    "abstract": "Generative Artificial Intelligence (GenAI) and communication networks are\nexpected to have groundbreaking synergies for 6G. Connecting GenAI agents via a\nwireless network can potentially unleash the power of Collective Intelligence\n(CI) and pave the way for Artificial General Intelligence (AGI). However,\ncurrent wireless networks are designed as a \"data pipe\" and are not suited to\naccommodate and leverage the power of GenAI. In this paper, we propose the\nGenAINet framework in which distributed GenAI agents communicate knowledge\n(facts, experiences, and methods) to accomplish arbitrary tasks. We first\npropose an architecture for a single GenAI agent and then provide a network\narchitecture integrating GenAI capabilities to manage both network protocols\nand applications. Building on this, we investigate effective communication and\nreasoning problems by proposing a semantic-native GenAINet. Specifically, GenAI\nagents extract semantics from heterogeneous raw data, build and maintain a\nknowledge model representing the semantic relationships among pieces of\nknowledge, which is retrieved by GenAI models for planning and reasoning. Under\nthis paradigm, different levels of collaboration can be achieved flexibly\ndepending on the complexity of targeted tasks. Furthermore, we conduct two case\nstudies in which, through wireless device queries, we demonstrate that\nextracting, compressing and transferring common knowledge can improve query\naccuracy while reducing communication costs; and in the wireless power control\nproblem, we show that distributed agents can complete general tasks\nindependently through collaborative reasoning without predefined communication\nprotocols. Finally, we discuss challenges and future research directions in\napplying Large Language Models (LLMs) in 6G networks.",
    "categories": [
      "cs.AI",
      "cs.NI",
      "eess.SP"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16631v3",
    "published_date": "2024-02-26 15:03:46 UTC",
    "updated_date": "2025-05-04 20:44:38 UTC"
  },
  {
    "arxiv_id": "2402.16627v3",
    "title": "Contextualized Diffusion Models for Text-Guided Image and Video Generation",
    "authors": [
      "Ling Yang",
      "Zhilong Zhang",
      "Zhaochen Yu",
      "Jingwei Liu",
      "Minkai Xu",
      "Stefano Ermon",
      "Bin Cui"
    ],
    "abstract": "Conditional diffusion models have exhibited superior performance in\nhigh-fidelity text-guided visual generation and editing. Nevertheless,\nprevailing text-guided visual diffusion models primarily focus on incorporating\ntext-visual relationships exclusively into the reverse process, often\ndisregarding their relevance in the forward process. This inconsistency between\nforward and reverse processes may limit the precise conveyance of textual\nsemantics in visual synthesis results. To address this issue, we propose a\nnovel and general contextualized diffusion model (ContextDiff) by incorporating\nthe cross-modal context encompassing interactions and alignments between text\ncondition and visual sample into forward and reverse processes. We propagate\nthis context to all timesteps in the two processes to adapt their trajectories,\nthereby facilitating cross-modal conditional modeling. We generalize our\ncontextualized diffusion to both DDPMs and DDIMs with theoretical derivations,\nand demonstrate the effectiveness of our model in evaluations with two\nchallenging tasks: text-to-image generation, and text-to-video editing. In each\ntask, our ContextDiff achieves new state-of-the-art performance, significantly\nenhancing the semantic alignment between text condition and generated samples,\nas evidenced by quantitative and qualitative evaluations. Our code is available\nat https://github.com/YangLing0818/ContextDiff",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "ICLR 2024. Project: https://github.com/YangLing0818/ContextDiff",
    "pdf_url": "http://arxiv.org/pdf/2402.16627v3",
    "published_date": "2024-02-26 15:01:16 UTC",
    "updated_date": "2024-06-04 01:08:56 UTC"
  },
  {
    "arxiv_id": "2402.16928v1",
    "title": "CLAP: Learning Transferable Binary Code Representations with Natural Language Supervision",
    "authors": [
      "Hao Wang",
      "Zeyu Gao",
      "Chao Zhang",
      "Zihan Sha",
      "Mingyang Sun",
      "Yuchen Zhou",
      "Wenyu Zhu",
      "Wenju Sun",
      "Han Qiu",
      "Xi Xiao"
    ],
    "abstract": "Binary code representation learning has shown significant performance in\nbinary analysis tasks. But existing solutions often have poor transferability,\nparticularly in few-shot and zero-shot scenarios where few or no training\nsamples are available for the tasks. To address this problem, we present CLAP\n(Contrastive Language-Assembly Pre-training), which employs natural language\nsupervision to learn better representations of binary code (i.e., assembly\ncode) and get better transferability. At the core, our approach boosts superior\ntransfer learning capabilities by effectively aligning binary code with their\nsemantics explanations (in natural language), resulting a model able to\ngenerate better embeddings for binary code. To enable this alignment training,\nwe then propose an efficient dataset engine that could automatically generate a\nlarge and diverse dataset comprising of binary code and corresponding natural\nlanguage explanations. We have generated 195 million pairs of binary code and\nexplanations and trained a prototype of CLAP. The evaluations of CLAP across\nvarious downstream tasks in binary analysis all demonstrate exceptional\nperformance. Notably, without any task-specific training, CLAP is often\ncompetitive with a fully supervised baseline, showing excellent\ntransferability. We release our pre-trained model and code at\nhttps://github.com/Hustcw/CLAP.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16928v1",
    "published_date": "2024-02-26 13:49:52 UTC",
    "updated_date": "2024-02-26 13:49:52 UTC"
  },
  {
    "arxiv_id": "2402.16567v3",
    "title": "Aligning Large Language Models to a Domain-specific Graph Database for NL2GQL",
    "authors": [
      "Yuanyuan Liang",
      "Keren Tan",
      "Tingyu Xie",
      "Wenbiao Tao",
      "Siyuan Wang",
      "Yunshi Lan",
      "Weining Qian"
    ],
    "abstract": "Graph Databases (Graph DB) find extensive application across diverse domains\nsuch as finance, social networks, and medicine. Yet, the translation of Natural\nLanguage (NL) into the Graph Query Language (GQL), referred to as NL2GQL, poses\nsignificant challenges owing to its intricate and specialized nature. Some\napproaches have sought to utilize Large Language Models (LLMs) to address\nanalogous tasks like text2SQL. Nonetheless, in the realm of NL2GQL tasks\ntailored to a particular domain, the absence of domain-specific NL-GQL data\npairs adds complexity to aligning LLMs with the graph DB. To tackle this\nchallenge, we present a well-defined pipeline. Initially, we utilize ChatGPT to\ngenerate NL-GQL data pairs, leveraging the provided graph DB with\nself-instruction. Subsequently, we employ the generated data to fine-tune LLMs,\nensuring alignment between LLMs and the graph DB. Moreover, we find the\nimportance of relevant schema in efficiently generating accurate GQLs. Thus, we\nintroduce a method to extract relevant schema as the input context. We evaluate\nour method using two carefully constructed datasets derived from graph DBs in\nthe finance and medicine domains, named FinGQL and MediGQL. Experimental\nresults reveal that our approach significantly outperforms a set of baseline\nmethods, with improvements of 5.90 and 6.36 absolute points on EM, and 6.00 and\n7.09 absolute points on EX for FinGQL and MediGQL, respectively.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.CL",
    "comment": "13 pages,2 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.16567v3",
    "published_date": "2024-02-26 13:46:51 UTC",
    "updated_date": "2024-09-05 06:34:11 UTC"
  },
  {
    "arxiv_id": "2402.16562v4",
    "title": "QF-tuner: Breaking Tradition in Reinforcement Learning",
    "authors": [
      "Mahmood A. Jumaah",
      "Yossra H. Ali",
      "Tarik A. Rashid"
    ],
    "abstract": "In reinforcement learning algorithms, the hyperparameters tuning method\nrefers to choosing the optimal parameters that may increase the overall\nperformance. Manual or random hyperparameter tuning methods can lead to\ndifferent results in the reinforcement learning algorithms. In this paper, we\npropose a new method called QF-tuner for automatic hyperparameter tuning in the\nQ learning algorithm using the FOX optimization algorithm (FOX). Furthermore, a\nnew objective function has been employed within FOX that prioritizes reward\nover learning error and time. QF tuner starts by running the FOX and tries to\nminimize the fitness value derived from observations at each iteration by\nexecuting the Q-learning algorithm. The proposed method has been evaluated\nusing two control tasks from the OpenAI Gym: CartPole and FrozenLake. The\nempirical results indicate that the QF-tuner outperforms other optimization\nalgorithms, such as particle swarm optimization (PSO), bees algorithm (BA),\ngenetic algorithms (GA), and the random method. However, on the FrozenLake\ntask, the QF-tuner increased rewards by 36% and reduced learning time by 26%,\nwhile on the CartPole task, it increased rewards by 57% and reduced learning\ntime by 20%. Thus, the QF-tuner is an essential method for hyperparameter\ntuning in Q-learning algorithms, enabling more effective solutions to control\ntask problems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages",
    "pdf_url": "http://arxiv.org/pdf/2402.16562v4",
    "published_date": "2024-02-26 13:39:04 UTC",
    "updated_date": "2025-03-18 01:41:10 UTC"
  },
  {
    "arxiv_id": "2403.07905v1",
    "title": "Enhancing Kubernetes Automated Scheduling with Deep Learning and Reinforcement Techniques for Large-Scale Cloud Computing Optimization",
    "authors": [
      "Zheng Xu",
      "Yulu Gong",
      "Yanlin Zhou",
      "Qiaozhi Bao",
      "Wenpin Qian"
    ],
    "abstract": "With the continuous expansion of the scale of cloud computing applications,\nartificial intelligence technologies such as Deep Learning and Reinforcement\nLearning have gradually become the key tools to solve the automated task\nscheduling of large-scale cloud computing systems. Aiming at the complexity and\nreal-time requirement of task scheduling in large-scale cloud computing system,\nthis paper proposes an automatic task scheduling scheme based on deep learning\nand reinforcement learning. Firstly, the deep learning technology is used to\nmonitor and predict the parameters in the cloud computing system in real time\nto obtain the system status information. Then, combined with reinforcement\nlearning algorithm, the task scheduling strategy is dynamically adjusted\naccording to the real-time system state and task characteristics to achieve the\noptimal utilization of system resources and the maximum of task execution\nefficiency. This paper verifies the effectiveness and performance advantages of\nthe proposed scheme in experiments, and proves the potential and application\nprospect of deep learning and reinforcement learning in automatic task\nscheduling in large-scale cloud computing systems.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.07905v1",
    "published_date": "2024-02-26 13:12:44 UTC",
    "updated_date": "2024-02-26 13:12:44 UTC"
  },
  {
    "arxiv_id": "2402.16546v1",
    "title": "Beyond Accuracy: An Empirical Study on Unit Testing in Open-source Deep Learning Projects",
    "authors": [
      "Han Wang",
      "Sijia Yu",
      "Chunyang Chen",
      "Burak Turhan",
      "Xiaodong Zhu"
    ],
    "abstract": "Deep Learning (DL) models have rapidly advanced, focusing on achieving high\nperformance through testing model accuracy and robustness. However, it is\nunclear whether DL projects, as software systems, are tested thoroughly or\nfunctionally correct when there is a need to treat and test them like other\nsoftware systems. Therefore, we empirically study the unit tests in open-source\nDL projects, analyzing 9,129 projects from GitHub. We find that: 1) unit tested\nDL projects have positive correlation with the open-source project metrics and\nhave a higher acceptance rate of pull requests, 2) 68% of the sampled DL\nprojects are not unit tested at all, 3) the layer and utilities (utils) of DL\nmodels have the most unit tests. Based on these findings and previous research\noutcomes, we built a mapping taxonomy between unit tests and faults in DL\nprojects. We discuss the implications of our findings for developers and\nresearchers and highlight the need for unit testing in open-source DL projects\nto ensure their reliability and stability. The study contributes to this\ncommunity by raising awareness of the importance of unit testing in DL projects\nand encouraging further research in this area.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "ACM Transactions on Software Engineering and Methodology (2023)",
    "pdf_url": "http://arxiv.org/pdf/2402.16546v1",
    "published_date": "2024-02-26 13:08:44 UTC",
    "updated_date": "2024-02-26 13:08:44 UTC"
  },
  {
    "arxiv_id": "2402.16542v2",
    "title": "RoboGrind: Intuitive and Interactive Surface Treatment with Industrial Robots",
    "authors": [
      "Benjamin Alt",
      "Florian Stöckl",
      "Silvan Müller",
      "Christopher Braun",
      "Julian Raible",
      "Saad Alhasan",
      "Oliver Rettig",
      "Lukas Ringle",
      "Darko Katic",
      "Rainer Jäkel",
      "Michael Beetz",
      "Marcus Strand",
      "Marco F. Huber"
    ],
    "abstract": "Surface treatment tasks such as grinding, sanding or polishing are a vital\nstep of the value chain in many industries, but are notoriously challenging to\nautomate. We present RoboGrind, an integrated system for the intuitive,\ninteractive automation of surface treatment tasks with industrial robots. It\ncombines a sophisticated 3D perception pipeline for surface scanning and\nautomatic defect identification, an interactive voice-controlled wizard system\nfor the AI-assisted bootstrapping and parameterization of robot programs, and\nan automatic planning and execution pipeline for force-controlled robotic\nsurface treatment. RoboGrind is evaluated both under laboratory and real-world\nconditions in the context of refabricating fiberglass wind turbine blades.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "68T40",
      "I.2.6; I.2.2; I.2.9"
    ],
    "primary_category": "cs.RO",
    "comment": "7 pages, 6 figures, accepted to the 2024 IEEE International\n  Conference on Robotics and Automation (ICRA 2024)",
    "pdf_url": "http://arxiv.org/pdf/2402.16542v2",
    "published_date": "2024-02-26 13:01:28 UTC",
    "updated_date": "2024-02-27 08:57:43 UTC"
  },
  {
    "arxiv_id": "2402.17791v1",
    "title": "Label Informed Contrastive Pretraining for Node Importance Estimation on Knowledge Graphs",
    "authors": [
      "Tianyu Zhang",
      "Chengbin Hou",
      "Rui Jiang",
      "Xuegong Zhang",
      "Chenghu Zhou",
      "Ke Tang",
      "Hairong Lv"
    ],
    "abstract": "Node Importance Estimation (NIE) is a task of inferring importance scores of\nthe nodes in a graph. Due to the availability of richer data and knowledge,\nrecent research interests of NIE have been dedicating to knowledge graphs for\npredicting future or missing node importance scores. Existing state-of-the-art\nNIE methods train the model by available labels, and they consider every\ninterested node equally before training. However, the nodes with higher\nimportance often require or receive more attention in real-world scenarios,\ne.g., people may care more about the movies or webpages with higher importance.\nTo this end, we introduce Label Informed ContrAstive Pretraining (LICAP) to the\nNIE problem for being better aware of the nodes with high importance scores.\nSpecifically, LICAP is a novel type of contrastive learning framework that aims\nto fully utilize the continuous labels to generate contrastive samples for\npretraining embeddings. Considering the NIE problem, LICAP adopts a novel\nsampling strategy called top nodes preferred hierarchical sampling to first\ngroup all interested nodes into a top bin and a non-top bin based on node\nimportance scores, and then divide the nodes within top bin into several finer\nbins also based on the scores. The contrastive samples are generated from those\nbins, and are then used to pretrain node embeddings of knowledge graphs via a\nnewly proposed Predicate-aware Graph Attention Networks (PreGAT), so as to\nbetter separate the top nodes from non-top nodes, and distinguish the top nodes\nwithin top bin by keeping the relative order among finer bins. Extensive\nexperiments demonstrate that the LICAP pretrained embeddings can further boost\nthe performance of existing NIE methods and achieve the new state-of-the-art\nperformance regarding both regression and ranking metrics. The source code for\nreproducibility is available at https://github.com/zhangtia16/LICAP",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.SI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by IEEE TNNLS",
    "pdf_url": "http://arxiv.org/pdf/2402.17791v1",
    "published_date": "2024-02-26 12:28:51 UTC",
    "updated_date": "2024-02-26 12:28:51 UTC"
  },
  {
    "arxiv_id": "2402.16926v1",
    "title": "On the (In)feasibility of ML Backdoor Detection as an Hypothesis Testing Problem",
    "authors": [
      "Georg Pichler",
      "Marco Romanelli",
      "Divya Prakash Manivannan",
      "Prashanth Krishnamurthy",
      "Farshad Khorrami",
      "Siddharth Garg"
    ],
    "abstract": "We introduce a formal statistical definition for the problem of backdoor\ndetection in machine learning systems and use it to analyze the feasibility of\nsuch problems, providing evidence for the utility and applicability of our\ndefinition. The main contributions of this work are an impossibility result and\nan achievability result for backdoor detection. We show a no-free-lunch\ntheorem, proving that universal (adversary-unaware) backdoor detection is\nimpossible, except for very small alphabet sizes. Thus, we argue, that backdoor\ndetection methods need to be either explicitly, or implicitly adversary-aware.\nHowever, our work does not imply that backdoor detection cannot work in\nspecific scenarios, as evidenced by successful backdoor detection methods in\nthe scientific literature. Furthermore, we connect our definition to the\nprobably approximately correct (PAC) learnability of the out-of-distribution\ndetection problem.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16926v1",
    "published_date": "2024-02-26 11:43:01 UTC",
    "updated_date": "2024-02-26 11:43:01 UTC"
  },
  {
    "arxiv_id": "2402.16505v2",
    "title": "Memory GAPS: Would LLMs pass the Tulving Test?",
    "authors": [
      "Jean-Marie Chauvet"
    ],
    "abstract": "The Tulving Test was designed to investigate memory performance in\nrecognition and recall tasks. Its results help assess the relevance of the\n\"Synergistic Ecphory Model\" of memory and similar RK paradigms in human\nperformance. This paper starts investigating whether the more than\nforty-year-old framework sheds some light on LLMs' acts of remembering.",
    "categories": [
      "cs.AI",
      "I.2.4"
    ],
    "primary_category": "cs.AI",
    "comment": "15 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.16505v2",
    "published_date": "2024-02-26 11:40:51 UTC",
    "updated_date": "2024-02-28 15:40:31 UTC"
  },
  {
    "arxiv_id": "2403.07904v3",
    "title": "Addressing the regulatory gap: moving towards an EU AI audit ecosystem beyond the AI Act by including civil society",
    "authors": [
      "David Hartmann",
      "José Renato Laranjeira de Pereira",
      "Chiara Streitbörger",
      "Bettina Berendt"
    ],
    "abstract": "The European legislature has proposed the Digital Services Act (DSA) and\nArtificial Intelligence Act (AIA) to regulate platforms and Artificial\nIntelligence (AI) products. We review to what extent third-party audits are\npart of both laws and how is access to information on models and the data\nprovided. By considering the value of third-party audits and third-party data\naccess in an audit ecosystem, we identify a regulatory gap in that the AIA does\nnot provide access to data for researchers and civil society. Our contributions\nto the literature include: (1) Defining an AI audit ecosystem incorporating\ncompliance and oversight. (2) Highlighting a regulatory gap within the DSA and\nAIA regulatory framework, preventing the establishment of an AI audit ecosystem\nthat has effective oversight by civil society and academia. (3) Emphasizing\nthat third-party audits by research and civil society must be part of that\necosystem, we call for AIA amendments and delegated acts to include data and\nmodel access for certain AI products. Furthermore, we call for the DSA to\nprovide NGOs and investigative journalists with data access to platforms by\ndelegated acts and for adaptions and amendments of the AIA to provide\nthird-party audits and data and model access, at least for high-risk systems.\nRegulations modeled after EU AI regulations should enable data access and\nthird-party audits, fostering an AI audit ecosystem that promotes compliance\nand oversight mechanisms.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.07904v3",
    "published_date": "2024-02-26 11:32:42 UTC",
    "updated_date": "2025-02-19 15:03:02 UTC"
  },
  {
    "arxiv_id": "2402.17599v2",
    "title": "DAGnosis: Localized Identification of Data Inconsistencies using Structures",
    "authors": [
      "Nicolas Huynh",
      "Jeroen Berrevoets",
      "Nabeel Seedat",
      "Jonathan Crabbé",
      "Zhaozhi Qian",
      "Mihaela van der Schaar"
    ],
    "abstract": "Identification and appropriate handling of inconsistencies in data at\ndeployment time is crucial to reliably use machine learning models. While\nrecent data-centric methods are able to identify such inconsistencies with\nrespect to the training set, they suffer from two key limitations: (1)\nsuboptimality in settings where features exhibit statistical independencies,\ndue to their usage of compressive representations and (2) lack of localization\nto pin-point why a sample might be flagged as inconsistent, which is important\nto guide future data collection. We solve these two fundamental limitations\nusing directed acyclic graphs (DAGs) to encode the training set's features\nprobability distribution and independencies as a structure. Our method, called\nDAGnosis, leverages these structural interactions to bring valuable and\ninsightful data-centric conclusions. DAGnosis unlocks the localization of the\ncauses of inconsistencies on a DAG, an aspect overlooked by previous\napproaches. Moreover, we show empirically that leveraging these interactions\n(1) leads to more accurate conclusions in detecting inconsistencies, as well as\n(2) provides more detailed insights into why some samples are flagged.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "AISTATS 2024; added correspondance email",
    "pdf_url": "http://arxiv.org/pdf/2402.17599v2",
    "published_date": "2024-02-26 11:29:16 UTC",
    "updated_date": "2024-02-28 10:46:07 UTC"
  },
  {
    "arxiv_id": "2402.16925v1",
    "title": "Minimize Control Inputs for Strong Structural Controllability Using Reinforcement Learning with Graph Neural Network",
    "authors": [
      "Mengbang Zou",
      "Weisi Guo",
      "Bailu Jin"
    ],
    "abstract": "Strong structural controllability (SSC) guarantees networked system with\nlinear-invariant dynamics controllable for all numerical realizations of\nparameters. Current research has established algebraic and graph-theoretic\nconditions of SSC for zero/nonzero or zero/nonzero/arbitrary structure. One\nrelevant practical problem is how to fully control the system with the minimal\nnumber of input signals and identify which nodes must be imposed signals.\nPrevious work shows that this optimization problem is NP-hard and it is\ndifficult to find the solution. To solve this problem, we formulate the graph\ncoloring process as a Markov decision process (MDP) according to the\ngraph-theoretical condition of SSC for both zero/nonzero and\nzero/nonzero/arbitrary structure. We use Actor-critic method with Directed\ngraph neural network which represents the color information of graph to\noptimize MDP. Our method is validated in a social influence network with real\ndata and different complex network models. We find that the number of input\nnodes is determined by the average degree of the network and the input nodes\ntend to select nodes with low in-degree and avoid high-degree nodes.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16925v1",
    "published_date": "2024-02-26 11:18:53 UTC",
    "updated_date": "2024-02-26 11:18:53 UTC"
  },
  {
    "arxiv_id": "2402.16486v1",
    "title": "Intelligent Known and Novel Aircraft Recognition -- A Shift from Classification to Similarity Learning for Combat Identification",
    "authors": [
      "Ahmad Saeed",
      "Haasha Bin Atif",
      "Usman Habib",
      "Mohsin Bilal"
    ],
    "abstract": "Precise aircraft recognition in low-resolution remote sensing imagery is a\nchallenging yet crucial task in aviation, especially combat identification.\nThis research addresses this problem with a novel, scalable, and AI-driven\nsolution. The primary hurdle in combat identification in remote sensing imagery\nis the accurate recognition of Novel/Unknown types of aircraft in addition to\nKnown types. Traditional methods, human expert-driven combat identification and\nimage classification, fall short in identifying Novel classes. Our methodology\nemploys similarity learning to discern features of a broad spectrum of military\nand civilian aircraft. It discerns both Known and Novel aircraft types,\nleveraging metric learning for the identification and supervised few-shot\nlearning for aircraft type classification. To counter the challenge of limited\nlow-resolution remote sensing data, we propose an end-to-end framework that\nadapts to the diverse and versatile process of military aircraft recognition by\ntraining a generalized embedder in fully supervised manner. Comparative\nanalysis with earlier aircraft image classification methods shows that our\napproach is effective for aircraft image classification (F1-score Aircraft Type\nof 0.861) and pioneering for quantifying the identification of Novel types\n(F1-score Bipartitioning of 0.936). The proposed methodology effectively\naddresses inherent challenges in remote sensing data, thereby setting new\nstandards in dataset quality. The research opens new avenues for domain experts\nand demonstrates unique capabilities in distinguishing various aircraft types,\ncontributing to a more robust, domain-adapted potential for real-time aircraft\nrecognition.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16486v1",
    "published_date": "2024-02-26 11:08:26 UTC",
    "updated_date": "2024-02-26 11:08:26 UTC"
  },
  {
    "arxiv_id": "2402.16482v1",
    "title": "On Languaging a Simulation Engine",
    "authors": [
      "Han Liu",
      "Liantang Li"
    ],
    "abstract": "Language model intelligence is revolutionizing the way we program materials\nsimulations. However, the diversity of simulation scenarios renders it\nchallenging to precisely transform human language into a tailored simulator.\nHere, using three functionalized types of language model, we propose a\nlanguage-to-simulation (Lang2Sim) framework that enables interactive navigation\non languaging a simulation engine, by taking a scenario instance of water\nsorption in porous matrices. Unlike line-by-line coding of a target simulator,\nthe language models interpret each simulator as an assembly of invariant tool\nfunction and its variant input-output pair. Lang2Sim enables the precise\ntransform of textual description by functionalizing and sequentializing the\nlanguage models of, respectively, rationalizing the tool categorization,\ncustomizing its input-output combinations, and distilling the simulator input\ninto executable format. Importantly, depending on its functionalized type, each\nlanguage model features a distinct processing of chat history to best balance\nits memory limit and information completeness, thus leveraging the model\nintelligence to unstructured nature of human request. Overall, this work\nestablishes language model as an intelligent platform to unlock the era of\nlanguaging a simulation engine.",
    "categories": [
      "cs.AI",
      "cs.CE",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16482v1",
    "published_date": "2024-02-26 11:01:54 UTC",
    "updated_date": "2024-02-26 11:01:54 UTC"
  },
  {
    "arxiv_id": "2403.04772v2",
    "title": "Representing Pedagogic Content Knowledge Through Rough Sets",
    "authors": [
      "A Mani"
    ],
    "abstract": "A teacher's knowledge base consists of knowledge of mathematics content,\nknowledge of student epistemology, and pedagogical knowledge. It has severe\nimplications on the understanding of student's knowledge of content, and the\nlearning context in general. The necessity to formalize the different content\nknowledge in approximate senses is recognized in the education research\nliterature. A related problem is that of coherent formalizability. Existing\nresponsive or smart AI-based software systems do not concern themselves with\nmeaning, and trained ones are replete with their own issues. In the present\nresearch, many issues in modeling teachers' understanding of content are\nidentified, and a two-tier rough set-based model is proposed by the present\nauthor for the purpose of developing software that can aid the varied tasks of\na teacher. The main advantage of the proposed approach is in its ability to\ncoherently handle vagueness, granularity and multi-modality. An extended\nexample to equational reasoning is used to demonstrate these. The paper is\nmeant for rough set researchers intending to build logical models or develop\nmeaning-aware AI-software to aid teachers, and education research experts.",
    "categories": [
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "15+ pages",
    "pdf_url": "http://arxiv.org/pdf/2403.04772v2",
    "published_date": "2024-02-26 11:00:45 UTC",
    "updated_date": "2024-04-15 20:34:26 UTC"
  },
  {
    "arxiv_id": "2402.16924v1",
    "title": "Theoretical Unification of the Fractured Aspects of Information",
    "authors": [
      "Marcin J. Schroeder"
    ],
    "abstract": "The article has as its main objective the identification of fundamental\nepistemological obstacles in the study of information related to unnecessary\nmethodological assumptions and the demystification of popular beliefs in the\nfundamental divisions of the aspects of information that can be understood as\nBachelardian rupture of epistemological obstacles. These general considerations\nare preceded by an overview of the motivations for the study of information and\nthe role of the concept of information in the conceptualization of\nintelligence, complexity, and consciousness justifying the need for a\nsufficiently general perspective in the study of information, and are followed\nat the end of the article by a brief exposition of an example of a possible\napplication in the development of the unified theory of information free from\nunnecessary divisions and claims of superiority of the existing preferences in\nmethodology. The reference to Gaston Bachelard and his ideas of epistemological\nobstacles and epistemological ruptures seems highly appropriate for the\nreflection on the development of information study, in particular in the\ncontext of obstacles such as the absence of semantics of information,\nnegligence of its structural analysis, separation of its digital and analog\nforms, and misguided use of mathematics.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "52 pages",
    "pdf_url": "http://arxiv.org/pdf/2402.16924v1",
    "published_date": "2024-02-26 10:35:41 UTC",
    "updated_date": "2024-02-26 10:35:41 UTC"
  },
  {
    "arxiv_id": "2402.16472v2",
    "title": "mEdIT: Multilingual Text Editing via Instruction Tuning",
    "authors": [
      "Vipul Raheja",
      "Dimitris Alikaniotis",
      "Vivek Kulkarni",
      "Bashar Alhafni",
      "Dhruv Kumar"
    ],
    "abstract": "We introduce mEdIT, a multi-lingual extension to CoEdIT -- the recent\nstate-of-the-art text editing models for writing assistance. mEdIT models are\ntrained by fine-tuning multi-lingual large, pre-trained language models (LLMs)\nvia instruction tuning. They are designed to take instructions from the user\nspecifying the attributes of the desired text in the form of natural language\ninstructions, such as Grammatik korrigieren (German) or Parafrasee la oraci\\'on\n(Spanish). We build mEdIT by curating data from multiple publicly available\nhuman-annotated text editing datasets for three text editing tasks (Grammatical\nError Correction (GEC), Text Simplification, and Paraphrasing) across diverse\nlanguages belonging to six different language families. We detail the design\nand training of mEdIT models and demonstrate their strong performance on many\nmulti-lingual text editing benchmarks against other multilingual LLMs. We also\nfind that mEdIT generalizes effectively to new languages over multilingual\nbaselines. We publicly release our data, code, and trained models at\nhttps://github.com/vipulraheja/medit.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to NAACL 2024 (Main). 23 pages, 8 tables, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.16472v2",
    "published_date": "2024-02-26 10:33:36 UTC",
    "updated_date": "2024-04-17 16:59:30 UTC"
  },
  {
    "arxiv_id": "2402.16459v3",
    "title": "Defending LLMs against Jailbreaking Attacks via Backtranslation",
    "authors": [
      "Yihan Wang",
      "Zhouxing Shi",
      "Andrew Bai",
      "Cho-Jui Hsieh"
    ],
    "abstract": "Although many large language models (LLMs) have been trained to refuse\nharmful requests, they are still vulnerable to jailbreaking attacks which\nrewrite the original prompt to conceal its harmful intent. In this paper, we\npropose a new method for defending LLMs against jailbreaking attacks by\n``backtranslation''. Specifically, given an initial response generated by the\ntarget LLM from an input prompt, our backtranslation prompts a language model\nto infer an input prompt that can lead to the response. The inferred prompt is\ncalled the backtranslated prompt which tends to reveal the actual intent of the\noriginal prompt, since it is generated based on the LLM's response and not\ndirectly manipulated by the attacker. We then run the target LLM again on the\nbacktranslated prompt, and we refuse the original prompt if the model refuses\nthe backtranslated prompt. We explain that the proposed defense provides\nseveral benefits on its effectiveness and efficiency. We empirically\ndemonstrate that our defense significantly outperforms the baselines, in the\ncases that are hard for the baselines, and our defense also has little impact\non the generation quality for benign input prompts. Our implementation is based\non our library for LLM jailbreaking defense algorithms at\n\\url{https://github.com/YihanWang617/llm-jailbreaking-defense}, and the code\nfor reproducing our experiments is available at\n\\url{https://github.com/YihanWang617/LLM-Jailbreaking-Defense-Backtranslation}.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16459v3",
    "published_date": "2024-02-26 10:03:33 UTC",
    "updated_date": "2024-06-06 19:21:44 UTC"
  },
  {
    "arxiv_id": "2402.16455v1",
    "title": "Performance Comparison of Surrogate-Assisted Evolutionary Algorithms on Computational Fluid Dynamics Problems",
    "authors": [
      "Jakub Kudela",
      "Ladislav Dobrovsky"
    ],
    "abstract": "Surrogate-assisted evolutionary algorithms (SAEAs) are recently among the\nmost widely studied methods for their capability to solve expensive real-world\noptimization problems. However, the development of new methods and benchmarking\nwith other techniques still relies almost exclusively on artificially created\nproblems. In this paper, we use two real-world computational fluid dynamics\nproblems to compare the performance of eleven state-of-the-art single-objective\nSAEAs. We analyze the performance by investigating the quality and robustness\nof the obtained solutions and the convergence properties of the selected\nmethods. Our findings suggest that the more recently published methods, as well\nas the techniques that utilize differential evolution as one of their\noptimization mechanisms, perform significantly better than the other considered\nmethods.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16455v1",
    "published_date": "2024-02-26 09:58:36 UTC",
    "updated_date": "2024-02-26 09:58:36 UTC"
  },
  {
    "arxiv_id": "2402.16449v1",
    "title": "Online Efficient Safety-Critical Control for Mobile Robots in Unknown Dynamic Multi-Obstacle Environments",
    "authors": [
      "Yu Zhang",
      "Guangyao Tian",
      "Long Wen",
      "Xiangtong Yao",
      "Liding Zhang",
      "Zhenshan Bing",
      "Wei He",
      "Alois Knoll"
    ],
    "abstract": "This paper proposes a LiDAR-based goal-seeking and exploration framework,\naddressing the efficiency of online obstacle avoidance in unstructured\nenvironments populated with static and moving obstacles. This framework\naddresses two significant challenges associated with traditional dynamic\ncontrol barrier functions (D-CBFs): their online construction and the\ndiminished real-time performance caused by utilizing multiple D-CBFs. To tackle\nthe first challenge, the framework's perception component begins with\nclustering point clouds via the DBSCAN algorithm, followed by encapsulating\nthese clusters with the minimum bounding ellipses (MBEs) algorithm to create\nelliptical representations. By comparing the current state of MBEs with those\nstored from previous moments, the differentiation between static and dynamic\nobstacles is realized, and the Kalman filter is utilized to predict the\nmovements of the latter. Such analysis facilitates the D-CBF's online\nconstruction for each MBE. To tackle the second challenge, we introduce buffer\nzones, generating Type-II D-CBFs online for each identified obstacle. Utilizing\nthese buffer zones as activation areas substantially reduces the number of\nD-CBFs that need to be activated. Upon entering these buffer zones, the system\nprioritizes safety, autonomously navigating safe paths, and hence referred to\nas the exploration mode. Exiting these buffer zones triggers the system's\ntransition to goal-seeking mode. We demonstrate that the system's states under\nthis framework achieve safety and asymptotic stabilization. Experimental\nresults in simulated and real-world environments have validated our framework's\ncapability, allowing a LiDAR-equipped mobile robot to efficiently and safely\nreach the desired location within dynamic environments containing multiple\nobstacles.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16449v1",
    "published_date": "2024-02-26 09:53:37 UTC",
    "updated_date": "2024-02-26 09:53:37 UTC"
  },
  {
    "arxiv_id": "2402.16442v3",
    "title": "On Distributed Larger-Than-Memory Subset Selection With Pairwise Submodular Functions",
    "authors": [
      "Maximilian Böther",
      "Abraham Sebastian",
      "Pranjal Awasthi",
      "Ana Klimovic",
      "Srikumar Ramalingam"
    ],
    "abstract": "Modern datasets span billions of samples, making training on all available\ndata infeasible. Selecting a high quality subset helps in reducing training\ncosts and enhancing model quality. Submodularity, a discrete analogue of\nconvexity, is commonly used for solving such subset selection problems.\nHowever, existing algorithms for optimizing submodular functions are\nsequential, and the prior distributed methods require at least one central\nmachine to fit the target subset in DRAM. At billion datapoint scale, even the\nsubset may not fit a single machine, and the sequential algorithms are\nprohibitively slow. In this paper, we relax the requirement of having a central\nmachine for the target subset by proposing a novel distributed bounding\nalgorithm with provable approximation guarantees. The algorithm iteratively\nbounds the minimum and maximum utility values to select high quality points and\ndiscard the unimportant ones. When bounding does not find the complete subset,\nwe use a multi-round, partition-based distributed greedy algorithm to identify\nthe remaining subset. We discuss how to implement these algorithms in a\ndistributed data processing framework and empirically analyze different\nconfigurations. We find high quality subsets on CIFAR-100 and ImageNet with\nmarginal or no loss in quality compared to centralized methods, and scale to a\ndataset with 13 billion points.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.DC",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "accepted at MLSys 2025",
    "pdf_url": "http://arxiv.org/pdf/2402.16442v3",
    "published_date": "2024-02-26 09:38:39 UTC",
    "updated_date": "2025-04-03 08:19:38 UTC"
  },
  {
    "arxiv_id": "2402.16435v1",
    "title": "Training Implicit Generative Models via an Invariant Statistical Loss",
    "authors": [
      "José Manuel de Frutos",
      "Pablo M. Olmos",
      "Manuel A. Vázquez",
      "Joaquín Míguez"
    ],
    "abstract": "Implicit generative models have the capability to learn arbitrary complex\ndata distributions. On the downside, training requires telling apart real data\nfrom artificially-generated ones using adversarial discriminators, leading to\nunstable training and mode-dropping issues. As reported by Zahee et al. (2017),\neven in the one-dimensional (1D) case, training a generative adversarial\nnetwork (GAN) is challenging and often suboptimal. In this work, we develop a\ndiscriminator-free method for training one-dimensional (1D) generative implicit\nmodels and subsequently expand this method to accommodate multivariate cases.\nOur loss function is a discrepancy measure between a suitably chosen\ntransformation of the model samples and a uniform distribution; hence, it is\ninvariant with respect to the true distribution of the data. We first formulate\nour method for 1D random variables, providing an effective solution for\napproximate reparameterization of arbitrary complex distributions. Then, we\nconsider the temporal setting (both univariate and multivariate), in which we\nmodel the conditional distribution of each sample given the history of the\nprocess. We demonstrate through numerical simulations that this new method\nyields promising results, successfully learning true distributions in a variety\nof scenarios and mitigating some of the well-known problems that\nstate-of-the-art implicit methods present.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "primary_category": "cs.LG",
    "comment": "Proceedings of the 27th International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.16435v1",
    "published_date": "2024-02-26 09:32:28 UTC",
    "updated_date": "2024-02-26 09:32:28 UTC"
  },
  {
    "arxiv_id": "2403.05574v3",
    "title": "HealMe: Harnessing Cognitive Reframing in Large Language Models for Psychotherapy",
    "authors": [
      "Mengxi Xiao",
      "Qianqian Xie",
      "Ziyan Kuang",
      "Zhicheng Liu",
      "Kailai Yang",
      "Min Peng",
      "Weiguang Han",
      "Jimin Huang"
    ],
    "abstract": "Large Language Models (LLMs) can play a vital role in psychotherapy by\nadeptly handling the crucial task of cognitive reframing and overcoming\nchallenges such as shame, distrust, therapist skill variability, and resource\nscarcity. Previous LLMs in cognitive reframing mainly converted negative\nemotions to positive ones, but these approaches have limited efficacy, often\nnot promoting clients' self-discovery of alternative perspectives. In this\npaper, we unveil the Helping and Empowering through Adaptive Language in Mental\nEnhancement (HealMe) model. This novel cognitive reframing therapy method\neffectively addresses deep-rooted negative thoughts and fosters rational,\nbalanced perspectives. Diverging from traditional LLM methods, HealMe employs\nempathetic dialogue based on psychotherapeutic frameworks. It systematically\nguides clients through distinguishing circumstances from feelings,\nbrainstorming alternative viewpoints, and developing empathetic, actionable\nsuggestions. Moreover, we adopt the first comprehensive and expertly crafted\npsychological evaluation metrics, specifically designed to rigorously assess\nthe performance of cognitive reframing, in both AI-simulated dialogues and\nreal-world therapeutic conversations. Experimental results show that our model\noutperforms others in terms of empathy, guidance, and logical coherence,\ndemonstrating its effectiveness and potential positive impact on psychotherapy.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "J.4"
    ],
    "primary_category": "cs.HC",
    "comment": "19 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.05574v3",
    "published_date": "2024-02-26 09:10:34 UTC",
    "updated_date": "2024-07-29 12:05:36 UTC"
  },
  {
    "arxiv_id": "2402.16402v1",
    "title": "Graph Learning with Distributional Edge Layouts",
    "authors": [
      "Xinjian Zhao",
      "Chaolong Ying",
      "Tianshu Yu"
    ],
    "abstract": "Graph Neural Networks (GNNs) learn from graph-structured data by passing\nlocal messages between neighboring nodes along edges on certain topological\nlayouts. Typically, these topological layouts in modern GNNs are\ndeterministically computed (e.g., attention-based GNNs) or locally sampled\n(e.g., GraphSage) under heuristic assumptions. In this paper, we for the first\ntime pose that these layouts can be globally sampled via Langevin dynamics\nfollowing Boltzmann distribution equipped with explicit physical energy,\nleading to higher feasibility in the physical world. We argue that such a\ncollection of sampled/optimized layouts can capture the wide energy\ndistribution and bring extra expressivity on top of WL-test, therefore easing\ndownstream tasks. As such, we propose Distributional Edge Layouts (DELs) to\nserve as a complement to a variety of GNNs. DEL is a pre-processing strategy\nindependent of subsequent GNN variants, thus being highly flexible.\nExperimental results demonstrate that DELs consistently and substantially\nimprove a series of GNN baselines, achieving state-of-the-art performance on\nmultiple datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "20 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.16402v1",
    "published_date": "2024-02-26 08:55:10 UTC",
    "updated_date": "2024-02-26 08:55:10 UTC"
  },
  {
    "arxiv_id": "2402.16397v1",
    "title": "Investigating Deep Watermark Security: An Adversarial Transferability Perspective",
    "authors": [
      "Biqing Qi",
      "Junqi Gao",
      "Yiang Luo",
      "Jianxing Liu",
      "Ligang Wu",
      "Bowen Zhou"
    ],
    "abstract": "The rise of generative neural networks has triggered an increased demand for\nintellectual property (IP) protection in generated content. Deep watermarking\ntechniques, recognized for their flexibility in IP protection, have garnered\nsignificant attention. However, the surge in adversarial transferable attacks\nposes unprecedented challenges to the security of deep watermarking\ntechniques-an area currently lacking systematic investigation. This study fills\nthis gap by introducing two effective transferable attackers to assess the\nvulnerability of deep watermarks against erasure and tampering risks.\nSpecifically, we initially define the concept of local sample density,\nutilizing it to deduce theorems on the consistency of model outputs. Upon\ndiscovering that perturbing samples towards high sample density regions (HSDR)\nof the target class enhances targeted adversarial transferability, we propose\nthe Easy Sample Selection (ESS) mechanism and the Easy Sample Matching Attack\n(ESMA) method. Additionally, we propose the Bottleneck Enhanced Mixup (BEM)\nthat integrates information bottleneck theory to reduce the generator's\ndependence on irrelevant noise. Experiments show a significant enhancement in\nthe success rate of targeted transfer attacks for both ESMA and BEM-ESMA\nmethods. We further conduct a comprehensive evaluation using ESMA and BEM-ESMA\nas measurements, considering model architecture and watermark encoding length,\nand achieve some impressive findings.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "18 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.16397v1",
    "published_date": "2024-02-26 08:41:14 UTC",
    "updated_date": "2024-02-26 08:41:14 UTC"
  },
  {
    "arxiv_id": "2402.16389v1",
    "title": "MoZIP: A Multilingual Benchmark to Evaluate Large Language Models in Intellectual Property",
    "authors": [
      "Shiwen Ni",
      "Minghuan Tan",
      "Yuelin Bai",
      "Fuqiang Niu",
      "Min Yang",
      "Bowen Zhang",
      "Ruifeng Xu",
      "Xiaojun Chen",
      "Chengming Li",
      "Xiping Hu",
      "Ye Li",
      "Jianping Fan"
    ],
    "abstract": "Large language models (LLMs) have demonstrated impressive performance in\nvarious natural language processing (NLP) tasks. However, there is limited\nunderstanding of how well LLMs perform in specific domains (e.g, the\nintellectual property (IP) domain). In this paper, we contribute a new\nbenchmark, the first Multilingual-oriented quiZ on Intellectual Property\n(MoZIP), for the evaluation of LLMs in the IP domain. The MoZIP benchmark\nincludes three challenging tasks: IP multiple-choice quiz (IPQuiz), IP question\nanswering (IPQA), and patent matching (PatentMatch). In addition, we also\ndevelop a new IP-oriented multilingual large language model (called MoZi),\nwhich is a BLOOMZ-based model that has been supervised fine-tuned with\nmultilingual IP-related text data. We evaluate our proposed MoZi model and four\nwell-known LLMs (i.e., BLOOMZ, BELLE, ChatGLM and ChatGPT) on the MoZIP\nbenchmark. Experimental results demonstrate that MoZi outperforms BLOOMZ, BELLE\nand ChatGLM by a noticeable margin, while it had lower scores compared with\nChatGPT. Notably, the performance of current LLMs on the MoZIP benchmark has\nmuch room for improvement, and even the most powerful ChatGPT does not reach\nthe passing level. Our source code, data, and models are available at\n\\url{https://github.com/AI-for-Science/MoZi}.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16389v1",
    "published_date": "2024-02-26 08:27:50 UTC",
    "updated_date": "2024-02-26 08:27:50 UTC"
  },
  {
    "arxiv_id": "2402.16387v1",
    "title": "On the Generalization Capability of Temporal Graph Learning Algorithms: Theoretical Insights and a Simpler Method",
    "authors": [
      "Weilin Cong",
      "Jian Kang",
      "Hanghang Tong",
      "Mehrdad Mahdavi"
    ],
    "abstract": "Temporal Graph Learning (TGL) has become a prevalent technique across diverse\nreal-world applications, especially in domains where data can be represented as\na graph and evolves over time. Although TGL has recently seen notable progress\nin algorithmic solutions, its theoretical foundations remain largely\nunexplored. This paper aims at bridging this gap by investigating the\ngeneralization ability of different TGL algorithms (e.g., GNN-based, RNN-based,\nand memory-based methods) under the finite-wide over-parameterized regime. We\nestablish the connection between the generalization error of TGL algorithms and\n\"the number of layers/steps\" in the GNN-/RNN-based TGL methods and \"the\nfeature-label alignment (FLA) score\", where FLA can be used as a proxy for the\nexpressive power and explains the performance of memory-based methods. Guided\nby our theoretical analysis, we propose Simplified-Temporal-Graph-Network,\nwhich enjoys a small generalization error, improved overall performance, and\nlower model complexity. Extensive experiments on real-world datasets\ndemonstrate the effectiveness of our method. Our theoretical findings and\nproposed algorithm offer essential insights into TGL from a theoretical\nstandpoint, laying the groundwork for the designing practical TGL algorithms in\nfuture studies.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16387v1",
    "published_date": "2024-02-26 08:22:22 UTC",
    "updated_date": "2024-02-26 08:22:22 UTC"
  },
  {
    "arxiv_id": "2402.16380v1",
    "title": "An Automated End-to-End Open-Source Software for High-Quality Text-to-Speech Dataset Generation",
    "authors": [
      "Ahmet Gunduz",
      "Kamer Ali Yuksel",
      "Kareem Darwish",
      "Golara Javadi",
      "Fabio Minazzi",
      "Nicola Sobieski",
      "Sebastien Bratieres"
    ],
    "abstract": "Data availability is crucial for advancing artificial intelligence\napplications, including voice-based technologies. As content creation,\nparticularly in social media, experiences increasing demand, translation and\ntext-to-speech (TTS) technologies have become essential tools. Notably, the\nperformance of these TTS technologies is highly dependent on the quality of the\ntraining data, emphasizing the mutual dependence of data availability and\ntechnological progress. This paper introduces an end-to-end tool to generate\nhigh-quality datasets for text-to-speech (TTS) models to address this critical\nneed for high-quality data. The contributions of this work are manifold and\ninclude: the integration of language-specific phoneme distribution into sample\nselection, automation of the recording process, automated and human-in-the-loop\nquality assurance of recordings, and processing of recordings to meet specified\nformats. The proposed application aims to streamline the dataset creation\nprocess for TTS models through these features, thereby facilitating\nadvancements in voice-based technologies.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "eess.AS",
    "comment": "9 Pages, 6 Figures, 4 Tables, LREC-COLING 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.16380v1",
    "published_date": "2024-02-26 07:58:33 UTC",
    "updated_date": "2024-02-26 07:58:33 UTC"
  },
  {
    "arxiv_id": "2402.16379v3",
    "title": "TEaR: Improving LLM-based Machine Translation with Systematic Self-Refinement",
    "authors": [
      "Zhaopeng Feng",
      "Yan Zhang",
      "Hao Li",
      "Bei Wu",
      "Jiayu Liao",
      "Wenqiang Liu",
      "Jun Lang",
      "Yang Feng",
      "Jian Wu",
      "Zuozhu Liu"
    ],
    "abstract": "Large Language Models (LLMs) have achieved impressive results in Machine\nTranslation (MT). However, careful evaluations by human reveal that the\ntranslations produced by LLMs still contain multiple errors. Importantly,\nfeeding back such error information into the LLMs can lead to self-refinement\nand result in improved translation performance. Motivated by these insights, we\nintroduce a systematic LLM-based self-refinement translation framework, named\n\\textbf{TEaR}, which stands for \\textbf{T}ranslate, \\textbf{E}stimate,\n\\textbf{a}nd \\textbf{R}efine, marking a significant step forward in this\ndirection. Our findings demonstrate that 1) our self-refinement framework\nsuccessfully assists LLMs in improving their translation quality across a wide\nrange of languages, whether it's from high-resource languages to low-resource\nones or whether it's English-centric or centered around other languages; 2)\nTEaR exhibits superior systematicity and interpretability; 3) different\nestimation strategies yield varied impacts, directly affecting the\neffectiveness of the final corrections. Additionally, traditional neural\ntranslation models and evaluation models operate separately, often focusing on\nsingular tasks due to their limited capabilities, while general-purpose LLMs\npossess the capability to undertake both tasks simultaneously. We further\nconduct cross-model correction experiments to investigate the potential\nrelationship between the translation and evaluation capabilities of\ngeneral-purpose LLMs. Our code and data are available at\nhttps://github.com/fzp0424/self_correct_mt",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Our code and data are available at\n  https://github.com/fzp0424/self_correct_mt",
    "pdf_url": "http://arxiv.org/pdf/2402.16379v3",
    "published_date": "2024-02-26 07:58:12 UTC",
    "updated_date": "2024-06-21 07:35:53 UTC"
  },
  {
    "arxiv_id": "2402.16369v1",
    "title": "Generative AI in Vision: A Survey on Models, Metrics and Applications",
    "authors": [
      "Gaurav Raut",
      "Apoorv Singh"
    ],
    "abstract": "Generative AI models have revolutionized various fields by enabling the\ncreation of realistic and diverse data samples. Among these models, diffusion\nmodels have emerged as a powerful approach for generating high-quality images,\ntext, and audio. This survey paper provides a comprehensive overview of\ngenerative AI diffusion and legacy models, focusing on their underlying\ntechniques, applications across different domains, and their challenges. We\ndelve into the theoretical foundations of diffusion models, including concepts\nsuch as denoising diffusion probabilistic models (DDPM) and score-based\ngenerative modeling. Furthermore, we explore the diverse applications of these\nmodels in text-to-image, image inpainting, and image super-resolution, along\nwith others, showcasing their potential in creative tasks and data\naugmentation. By synthesizing existing research and highlighting critical\nadvancements in this field, this survey aims to provide researchers and\npractitioners with a comprehensive understanding of generative AI diffusion and\nlegacy models and inspire future innovations in this exciting area of\nartificial intelligence.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16369v1",
    "published_date": "2024-02-26 07:47:12 UTC",
    "updated_date": "2024-02-26 07:47:12 UTC"
  },
  {
    "arxiv_id": "2402.16363v6",
    "title": "LLM Inference Unveiled: Survey and Roofline Model Insights",
    "authors": [
      "Zhihang Yuan",
      "Yuzhang Shang",
      "Yang Zhou",
      "Zhen Dong",
      "Zhe Zhou",
      "Chenhao Xue",
      "Bingzhe Wu",
      "Zhikai Li",
      "Qingyi Gu",
      "Yong Jae Lee",
      "Yan Yan",
      "Beidi Chen",
      "Guangyu Sun",
      "Kurt Keutzer"
    ],
    "abstract": "The field of efficient Large Language Model (LLM) inference is rapidly\nevolving, presenting a unique blend of opportunities and challenges. Although\nthe field has expanded and is vibrant, there hasn't been a concise framework\nthat analyzes the various methods of LLM Inference to provide a clear\nunderstanding of this domain. Our survey stands out from traditional literature\nreviews by not only summarizing the current state of research but also by\nintroducing a framework based on roofline model for systematic analysis of LLM\ninference techniques. This framework identifies the bottlenecks when deploying\nLLMs on hardware devices and provides a clear understanding of practical\nproblems, such as why LLMs are memory-bound, how much memory and computation\nthey need, and how to choose the right hardware. We systematically collate the\nlatest advancements in efficient LLM inference, covering crucial areas such as\nmodel compression (e.g., Knowledge Distillation and Quantization), algorithm\nimprovements (e.g., Early Exit and Mixture-of-Expert), and both hardware and\nsystem-level enhancements. Our survey stands out by analyzing these methods\nwith roofline model, helping us understand their impact on memory access and\ncomputation. This distinctive approach not only showcases the current research\nlandscape but also delivers valuable insights for practical implementation,\npositioning our work as an indispensable resource for researchers new to the\nfield as well as for those seeking to deepen their understanding of efficient\nLLM deployment. The analyze tool, LLM-Viewer, is open-sourced.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16363v6",
    "published_date": "2024-02-26 07:33:05 UTC",
    "updated_date": "2024-05-01 20:42:28 UTC"
  },
  {
    "arxiv_id": "2402.16361v1",
    "title": "Layer-wise Regularized Dropout for Neural Language Models",
    "authors": [
      "Shiwen Ni",
      "Min Yang",
      "Ruifeng Xu",
      "Chengming Li",
      "Xiping Hu"
    ],
    "abstract": "Among the various pre-trained neural language models that are popular today,\ndropout is already an indispensable regularization technique. To solve the\ninconsistency between training and inference caused by the randomness of\ndropout, some studies use consistency training to regularize dropout at the\noutput layer. In this paper, we propose a novel Layer-wise Regularized Dropout\n(LR-Drop), which is specially designed for Transformer-based Language models.\nSpecifically, LR-Drop layer-wise regularizes each Transformer layer using the\nconsistency training strategy. Each training sample passes through the two\nsiamese sub-models sampled by dropout, and then LR-Drop forces the hidden\nstates, multi-head attention matrices, and output distribution of the two\nsiamese sub-models to be consistent. The proposed LR-Drop can be regarded as a\n\"self-distillation\" framework, in which each sub-model generated by dropout is\nthe other's \"teacher\" model and \"student\" model. Through extensive experiments\non 8 natural language understanding datasets, 6 neural machine translation\ndatasets, and 1 abstractive summarization dataset (a total of 15 datasets), we\nshow that LR-Drop achieves superior performances, including state-of-the-art\nresults.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16361v1",
    "published_date": "2024-02-26 07:31:35 UTC",
    "updated_date": "2024-02-26 07:31:35 UTC"
  },
  {
    "arxiv_id": "2402.16359v3",
    "title": "Feedback Efficient Online Fine-Tuning of Diffusion Models",
    "authors": [
      "Masatoshi Uehara",
      "Yulai Zhao",
      "Kevin Black",
      "Ehsan Hajiramezanali",
      "Gabriele Scalia",
      "Nathaniel Lee Diamant",
      "Alex M Tseng",
      "Sergey Levine",
      "Tommaso Biancalani"
    ],
    "abstract": "Diffusion models excel at modeling complex data distributions, including\nthose of images, proteins, and small molecules. However, in many cases, our\ngoal is to model parts of the distribution that maximize certain properties:\nfor example, we may want to generate images with high aesthetic quality, or\nmolecules with high bioactivity. It is natural to frame this as a reinforcement\nlearning (RL) problem, in which the objective is to fine-tune a diffusion model\nto maximize a reward function that corresponds to some property. Even with\naccess to online queries of the ground-truth reward function, efficiently\ndiscovering high-reward samples can be challenging: they might have a low\nprobability in the initial distribution, and there might be many infeasible\nsamples that do not even have a well-defined reward (e.g., unnatural images or\nphysically impossible molecules). In this work, we propose a novel\nreinforcement learning procedure that efficiently explores on the manifold of\nfeasible samples. We present a theoretical analysis providing a regret\nguarantee, as well as empirical validation across three domains: images,\nbiological sequences, and molecules.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.QM",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.16359v3",
    "published_date": "2024-02-26 07:24:32 UTC",
    "updated_date": "2024-07-18 08:21:54 UTC"
  },
  {
    "arxiv_id": "2402.16354v2",
    "title": "Language-guided Skill Learning with Temporal Variational Inference",
    "authors": [
      "Haotian Fu",
      "Pratyusha Sharma",
      "Elias Stengel-Eskin",
      "George Konidaris",
      "Nicolas Le Roux",
      "Marc-Alexandre Côté",
      "Xingdi Yuan"
    ],
    "abstract": "We present an algorithm for skill discovery from expert demonstrations. The\nalgorithm first utilizes Large Language Models (LLMs) to propose an initial\nsegmentation of the trajectories. Following that, a hierarchical variational\ninference framework incorporates the LLM-generated segmentation information to\ndiscover reusable skills by merging trajectory segments. To further control the\ntrade-off between compression and reusability, we introduce a novel auxiliary\nobjective based on the Minimum Description Length principle that helps guide\nthis skill discovery process. Our results demonstrate that agents equipped with\nour method are able to discover skills that help accelerate learning and\noutperform baseline skill learning approaches on new long-horizon tasks in\nBabyAI, a grid world navigation environment, as well as ALFRED, a household\nsimulation environment.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.16354v2",
    "published_date": "2024-02-26 07:19:23 UTC",
    "updated_date": "2024-05-27 14:31:38 UTC"
  },
  {
    "arxiv_id": "2402.16352v2",
    "title": "MathGenie: Generating Synthetic Data with Question Back-translation for Enhancing Mathematical Reasoning of LLMs",
    "authors": [
      "Zimu Lu",
      "Aojun Zhou",
      "Houxing Ren",
      "Ke Wang",
      "Weikang Shi",
      "Junting Pan",
      "Mingjie Zhan",
      "Hongsheng Li"
    ],
    "abstract": "Large language models (LLMs) have exhibited great potential in mathematical\nreasoning. However, there remains a performance gap in this area between\nexisting open-source models and closed-source models such as GPT-4. In this\npaper, we introduce MathGenie, a novel method for generating diverse and\nreliable math problems from a small-scale problem-solution dataset (denoted as\nseed data). We augment the ground-truth solutions of our seed data and train a\nback-translation model to translate the augmented solutions back into new\nquestions. Subsequently, we generate code-integrated solutions for the new\nquestions. To ensure the correctness of the code-integrated solutions, we\nemploy rationale-based strategy for solution verification. Various pretrained\nmodels, ranging from 7B to 70B, are trained on the newly curated data to test\nthe effectiveness of the proposed augmentation technique, resulting in a family\nof models known as MathGenieLM. These models consistently outperform previous\nopen-source models across five representative mathematical reasoning datasets,\nachieving state-of-the-art performance. In particular, MathGenieLM-InternLM2\nachieves an accuracy of 87.7% on GSM8K and 55.7% on MATH, securing the best\noverall score among open-source language models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL 2024 camera ready",
    "pdf_url": "http://arxiv.org/pdf/2402.16352v2",
    "published_date": "2024-02-26 07:17:25 UTC",
    "updated_date": "2024-09-11 08:23:58 UTC"
  },
  {
    "arxiv_id": "2402.16342v1",
    "title": "Contingency Planning Using Bi-level Markov Decision Processes for Space Missions",
    "authors": [
      "Somrita Banerjee",
      "Edward Balaban",
      "Mark Shirley",
      "Kevin Bradner",
      "Marco Pavone"
    ],
    "abstract": "This work focuses on autonomous contingency planning for scientific missions\nby enabling rapid policy computation from any off-nominal point in the state\nspace in the event of a delay or deviation from the nominal mission plan.\nSuccessful contingency planning involves managing risks and rewards, often\nprobabilistically associated with actions, in stochastic scenarios. Markov\nDecision Processes (MDPs) are used to mathematically model decision-making in\nsuch scenarios. However, in the specific case of planetary rover traverse\nplanning, the vast action space and long planning time horizon pose\ncomputational challenges. A bi-level MDP framework is proposed to improve\ncomputational tractability, while also aligning with existing mission planning\npractices and enhancing explainability and trustworthiness of AI-driven\nsolutions. We discuss the conversion of a mission planning MDP into a bi-level\nMDP, and test the framework on RoverGridWorld, a modified GridWorld environment\nfor rover mission planning. We demonstrate the computational tractability and\nnear-optimal policies achievable with the bi-level MDP approach, highlighting\nthe trade-offs between compute time and policy optimality as the problem's\ncomplexity grows. This work facilitates more efficient and flexible contingency\nplanning in the context of scientific missions.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16342v1",
    "published_date": "2024-02-26 06:42:30 UTC",
    "updated_date": "2024-02-26 06:42:30 UTC"
  },
  {
    "arxiv_id": "2402.16321v1",
    "title": "Self-Supervised Speech Quality Estimation and Enhancement Using Only Clean Speech",
    "authors": [
      "Szu-Wei Fu",
      "Kuo-Hsuan Hung",
      "Yu Tsao",
      "Yu-Chiang Frank Wang"
    ],
    "abstract": "Speech quality estimation has recently undergone a paradigm shift from\nhuman-hearing expert designs to machine-learning models. However, current\nmodels rely mainly on supervised learning, which is time-consuming and\nexpensive for label collection. To solve this problem, we propose VQScore, a\nself-supervised metric for evaluating speech based on the quantization error of\na vector-quantized-variational autoencoder (VQ-VAE). The training of VQ-VAE\nrelies on clean speech; hence, large quantization errors can be expected when\nthe speech is distorted. To further improve correlation with real quality\nscores, domain knowledge of speech processing is incorporated into the model\ndesign. We found that the vector quantization mechanism could also be used for\nself-supervised speech enhancement (SE) model training. To improve the\nrobustness of the encoder for SE, a novel self-distillation mechanism combined\nwith adversarial training is introduced. In summary, the proposed speech\nquality estimation method and enhancement models require only clean speech for\ntraining without any label requirements. Experimental results show that the\nproposed VQScore and enhancement model are competitive with supervised\nbaselines. The code will be released after publication.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Published as a conference paper at ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.16321v1",
    "published_date": "2024-02-26 06:01:38 UTC",
    "updated_date": "2024-02-26 06:01:38 UTC"
  },
  {
    "arxiv_id": "2402.16313v3",
    "title": "Chain-of-Discussion: A Multi-Model Framework for Complex Evidence-Based Question Answering",
    "authors": [
      "Mingxu Tao",
      "Dongyan Zhao",
      "Yansong Feng"
    ],
    "abstract": "Open-ended question answering requires models to find appropriate evidence to\nform wellreasoned, comprehensive and helpful answers. In practical\napplications, models also need to engage in extended discussions on potential\nscenarios closely relevant to the question. With augmentation of retrieval\nmodule, open-source Large Language Models (LLMs) can produce coherent answers\noften with different focuses, but are still sub-optimal in terms of reliable\nevidence selection and in-depth question analysis. In this paper, we propose a\nnovel Chain-ofDiscussion framework to leverage the synergy among multiple\nopen-source LLMs aiming to provide more correct and more comprehensive answers\nfor open-ended QA, although they are not strong enough individually. Our\nexperiments show that discussions among multiple LLMs play a vital role in\nenhancing the quality of answers.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "COLING 2025",
    "pdf_url": "http://arxiv.org/pdf/2402.16313v3",
    "published_date": "2024-02-26 05:31:34 UTC",
    "updated_date": "2024-12-16 07:11:59 UTC"
  },
  {
    "arxiv_id": "2402.16312v1",
    "title": "Federated Contextual Cascading Bandits with Asynchronous Communication and Heterogeneous Users",
    "authors": [
      "Hantao Yang",
      "Xutong Liu",
      "Zhiyong Wang",
      "Hong Xie",
      "John C. S. Lui",
      "Defu Lian",
      "Enhong Chen"
    ],
    "abstract": "We study the problem of federated contextual combinatorial cascading bandits,\nwhere $|\\mathcal{U}|$ agents collaborate under the coordination of a central\nserver to provide tailored recommendations to the $|\\mathcal{U}|$ corresponding\nusers. Existing works consider either a synchronous framework, necessitating\nfull agent participation and global synchronization, or assume user homogeneity\nwith identical behaviors. We overcome these limitations by considering (1)\nfederated agents operating in an asynchronous communication paradigm, where no\nmandatory synchronization is required and all agents communicate independently\nwith the server, (2) heterogeneous user behaviors, where users can be\nstratified into $J \\le |\\mathcal{U}|$ latent user clusters, each exhibiting\ndistinct preferences. For this setting, we propose a UCB-type algorithm with\ndelicate communication protocols. Through theoretical analysis, we give\nsub-linear regret bounds on par with those achieved in the synchronous\nframework, while incurring only logarithmic communication costs. Empirical\nevaluation on synthetic and real-world datasets validates our algorithm's\nsuperior performance in terms of regrets and communication costs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by AAAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.16312v1",
    "published_date": "2024-02-26 05:31:14 UTC",
    "updated_date": "2024-02-26 05:31:14 UTC"
  },
  {
    "arxiv_id": "2402.16311v3",
    "title": "Cross-domain Chinese Sentence Pattern Parsing",
    "authors": [
      "Jingsi Yu",
      "Cunliang Kong",
      "Liner Yang",
      "Meishan Zhang",
      "Lin Zhu",
      "Yujie Wang",
      "Haozhe Lin",
      "Maosong Sun",
      "Erhong Yang"
    ],
    "abstract": "Sentence Pattern Structure (SPS) parsing is a syntactic analysis method\nprimarily employed in language teaching.Existing SPS parsers rely heavily on\ntextbook corpora for training, lacking cross-domain capability.To overcome this\nconstraint, this paper proposes an innovative approach leveraging large\nlanguage models (LLMs) within a self-training framework. Partial syntactic\nrules from a source domain are combined with target domain sentences to\ndynamically generate training data, enhancing the adaptability of the parser to\ndiverse domains.Experiments conducted on textbook and news domains demonstrate\nthe effectiveness of the proposed method, outperforming rule-based baselines by\n1.68 points on F1 metrics.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16311v3",
    "published_date": "2024-02-26 05:30:48 UTC",
    "updated_date": "2024-04-08 03:22:41 UTC"
  },
  {
    "arxiv_id": "2402.16310v4",
    "title": "REPLAY: Modeling Time-Varying Temporal Regularities of Human Mobility for Location Prediction over Sparse Trajectories",
    "authors": [
      "Bangchao Deng",
      "Bingqing Qu",
      "Pengyang Wang",
      "Dingqi Yang",
      "Benjamin Fankhauser",
      "Philippe Cudre-Mauroux"
    ],
    "abstract": "Location prediction forecasts a user's location based on historical user\nmobility traces. To tackle the intrinsic sparsity issue of real-world user\nmobility traces, spatiotemporal contexts have been shown as significantly\nuseful. Existing solutions mostly incorporate spatiotemporal distances between\nlocations in mobility traces, either by feeding them as additional inputs to\nRecurrent Neural Networks (RNNs) or by using them to search for informative\npast hidden states for prediction. However, such distance-based methods fail to\ncapture the time-varying temporal regularities of human mobility, where human\nmobility is often more regular in the morning than in other periods, for\nexample; this suggests the usefulness of the actual timestamps besides the\ntemporal distances. Against this background, we propose REPLAY, a general RNN\narchitecture learning to capture the time-varying temporal regularities for\nlocation prediction. Specifically, REPLAY not only resorts to the\nspatiotemporal distances in sparse trajectories to search for the informative\npast hidden states, but also accommodates the time-varying temporal\nregularities by incorporating smoothed timestamp embeddings using Gaussian\nweighted averaging with timestamp-specific learnable bandwidths, which can\nflexibly adapt to the temporal regularities of different strengths across\ndifferent timestamps. Our extensive evaluation compares REPLAY against a\nsizable collection of state-of-the-art techniques on two real-world datasets.\nResults show that REPLAY consistently and significantly outperforms\nstate-of-the-art methods by 7.7\\%-10.5\\% in the location prediction task, and\nthe bandwidths reveal interesting patterns of the time-varying temporal\nregularities.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by IEEE Transactions on Mobile Computing",
    "pdf_url": "http://arxiv.org/pdf/2402.16310v4",
    "published_date": "2024-02-26 05:28:36 UTC",
    "updated_date": "2025-05-05 01:48:08 UTC"
  },
  {
    "arxiv_id": "2402.16305v1",
    "title": "Referee Can Play: An Alternative Approach to Conditional Generation via Model Inversion",
    "authors": [
      "Xuantong Liu",
      "Tianyang Hu",
      "Wenjia Wang",
      "Kenji Kawaguchi",
      "Yuan Yao"
    ],
    "abstract": "As a dominant force in text-to-image generation tasks, Diffusion\nProbabilistic Models (DPMs) face a critical challenge in controllability,\nstruggling to adhere strictly to complex, multi-faceted instructions. In this\nwork, we aim to address this alignment challenge for conditional generation\ntasks. First, we provide an alternative view of state-of-the-art DPMs as a way\nof inverting advanced Vision-Language Models (VLMs). With this formulation, we\nnaturally propose a training-free approach that bypasses the conventional\nsampling process associated with DPMs. By directly optimizing images with the\nsupervision of discriminative VLMs, the proposed method can potentially achieve\na better text-image alignment. As proof of concept, we demonstrate the pipeline\nwith the pre-trained BLIP-2 model and identify several key designs for improved\nimage generation. To further enhance the image fidelity, a Score Distillation\nSampling module of Stable Diffusion is incorporated. By carefully balancing the\ntwo components during optimization, our method can produce high-quality images\nwith near state-of-the-art performance on T2I-Compbench.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16305v1",
    "published_date": "2024-02-26 05:08:40 UTC",
    "updated_date": "2024-02-26 05:08:40 UTC"
  },
  {
    "arxiv_id": "2402.16302v2",
    "title": "Graph Diffusion Policy Optimization",
    "authors": [
      "Yijing Liu",
      "Chao Du",
      "Tianyu Pang",
      "Chongxuan Li",
      "Min Lin",
      "Wei Chen"
    ],
    "abstract": "Recent research has made significant progress in optimizing diffusion models\nfor downstream objectives, which is an important pursuit in fields such as\ngraph generation for drug design. However, directly applying these models to\ngraph presents challenges, resulting in suboptimal performance. This paper\nintroduces graph diffusion policy optimization (GDPO), a novel approach to\noptimize graph diffusion models for arbitrary (e.g., non-differentiable)\nobjectives using reinforcement learning. GDPO is based on an eager policy\ngradient tailored for graph diffusion models, developed through meticulous\nanalysis and promising improved performance. Experimental results show that\nGDPO achieves state-of-the-art performance in various graph generation tasks\nwith complex and diverse objectives. Code is available at\nhttps://github.com/sail-sg/GDPO.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.16302v2",
    "published_date": "2024-02-26 04:58:42 UTC",
    "updated_date": "2024-10-25 15:59:35 UTC"
  },
  {
    "arxiv_id": "2402.16298v1",
    "title": "MV-Swin-T: Mammogram Classification with Multi-view Swin Transformer",
    "authors": [
      "Sushmita Sarker",
      "Prithul Sarker",
      "George Bebis",
      "Alireza Tavakkoli"
    ],
    "abstract": "Traditional deep learning approaches for breast cancer classification has\npredominantly concentrated on single-view analysis. In clinical practice,\nhowever, radiologists concurrently examine all views within a mammography exam,\nleveraging the inherent correlations in these views to effectively detect\ntumors. Acknowledging the significance of multi-view analysis, some studies\nhave introduced methods that independently process mammogram views, either\nthrough distinct convolutional branches or simple fusion strategies,\ninadvertently leading to a loss of crucial inter-view correlations. In this\npaper, we propose an innovative multi-view network exclusively based on\ntransformers to address challenges in mammographic image classification. Our\napproach introduces a novel shifted window-based dynamic attention block,\nfacilitating the effective integration of multi-view information and promoting\nthe coherent transfer of this information between views at the spatial feature\nmap level. Furthermore, we conduct a comprehensive comparative analysis of the\nperformance and effectiveness of transformer-based models under diverse\nsettings, employing the CBIS-DDSM and Vin-Dr Mammo datasets. Our code is\npublicly available at https://github.com/prithuls/MV-Swin-T",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "4 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.16298v1",
    "published_date": "2024-02-26 04:41:04 UTC",
    "updated_date": "2024-02-26 04:41:04 UTC"
  },
  {
    "arxiv_id": "2402.16297v2",
    "title": "A Poisson-Gamma Dynamic Factor Model with Time-Varying Transition Dynamics",
    "authors": [
      "Jiahao Wang",
      "Sikun Yang",
      "Heinz Koeppl",
      "Xiuzhen Cheng",
      "Pengfei Hu",
      "Guoming Zhang"
    ],
    "abstract": "Probabilistic approaches for handling count-valued time sequences have\nattracted amounts of research attentions because their ability to infer\nexplainable latent structures and to estimate uncertainties, and thus are\nespecially suitable for dealing with \\emph{noisy} and \\emph{incomplete} count\ndata. Among these models, Poisson-Gamma Dynamical Systems (PGDSs) are proven to\nbe effective in capturing the evolving dynamics underlying observed count\nsequences. However, the state-of-the-art PGDS still fails to capture the\n\\emph{time-varying} transition dynamics that are commonly observed in\nreal-world count time sequences. To mitigate this gap, a non-stationary PGDS is\nproposed to allow the underlying transition matrices to evolve over time, and\nthe evolving transition matrices are modeled by sophisticatedly-designed\nDirichlet Markov chains. Leveraging Dirichlet-Multinomial-Beta data\naugmentation techniques, a fully-conjugate and efficient Gibbs sampler is\ndeveloped to perform posterior simulation. Experiments show that, in comparison\nwith related models, the proposed non-stationary PGDS achieves improved\npredictive performance due to its capacity to learn non-stationary dependency\nstructure captured by the time-evolving transition matrices.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16297v2",
    "published_date": "2024-02-26 04:39:01 UTC",
    "updated_date": "2024-05-23 07:21:27 UTC"
  },
  {
    "arxiv_id": "2402.16294v2",
    "title": "BlockFUL: Enabling Unlearning in Blockchained Federated Learning",
    "authors": [
      "Xiao Liu",
      "Mingyuan Li",
      "Xu Wang",
      "Guangsheng Yu",
      "Wei Ni",
      "Lixiang Li",
      "Haipeng Peng",
      "Renping Liu"
    ],
    "abstract": "Unlearning in Federated Learning (FL) presents significant challenges, as\nmodels grow and evolve with complex inheritance relationships. This complexity\nis amplified when blockchain is employed to ensure the integrity and\ntraceability of FL, where the need to edit multiple interlinked blockchain\nrecords and update all inherited models complicates the process.In this paper,\nwe introduce Blockchained Federated Unlearning (BlockFUL), a novel framework\nwith a dual-chain structure comprising a live chain and an archive chain for\nenabling unlearning capabilities within Blockchained FL. BlockFUL introduces\ntwo new unlearning paradigms, i.e., parallel and sequential paradigms, which\ncan be effectively implemented through gradient-ascent-based and\nre-training-based unlearning methods. These methods enhance the unlearning\nprocess across multiple inherited models by enabling efficient consensus\noperations and reducing computational costs. Our extensive experiments validate\nthat these methods effectively reduce data dependency and operational overhead,\nthereby boosting the overall performance of unlearning inherited models within\nBlockFUL on CIFAR-10 and Fashion-MNIST datasets using AlexNet, ResNet18, and\nMobileNetV2 models.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16294v2",
    "published_date": "2024-02-26 04:31:53 UTC",
    "updated_date": "2024-08-14 10:44:53 UTC"
  },
  {
    "arxiv_id": "2402.16288v1",
    "title": "PerLTQA: A Personal Long-Term Memory Dataset for Memory Classification, Retrieval, and Synthesis in Question Answering",
    "authors": [
      "Yiming Du",
      "Hongru Wang",
      "Zhengyi Zhao",
      "Bin Liang",
      "Baojun Wang",
      "Wanjun Zhong",
      "Zezhong Wang",
      "Kam-Fai Wong"
    ],
    "abstract": "Long-term memory plays a critical role in personal interaction, considering\nlong-term memory can better leverage world knowledge, historical information,\nand preferences in dialogues. Our research introduces PerLTQA, an innovative QA\ndataset that combines semantic and episodic memories, including world\nknowledge, profiles, social relationships, events, and dialogues. This dataset\nis collected to investigate the use of personalized memories, focusing on\nsocial interactions and events in the QA task. PerLTQA features two types of\nmemory and a comprehensive benchmark of 8,593 questions for 30 characters,\nfacilitating the exploration and application of personalized memories in Large\nLanguage Models (LLMs). Based on PerLTQA, we propose a novel framework for\nmemory integration and generation, consisting of three main components: Memory\nClassification, Memory Retrieval, and Memory Synthesis. We evaluate this\nframework using five LLMs and three retrievers. Experimental results\ndemonstrate that BERT-based classification models significantly outperform LLMs\nsuch as ChatGLM3 and ChatGPT in the memory classification task. Furthermore,\nour study highlights the importance of effective memory integration in the QA\ntask.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16288v1",
    "published_date": "2024-02-26 04:09:53 UTC",
    "updated_date": "2024-02-26 04:09:53 UTC"
  },
  {
    "arxiv_id": "2402.16281v2",
    "title": "RobKiNet: Robotic Kinematics Informed Neural Network for Optimal Robot Configuration Prediction",
    "authors": [
      "Yanlong Peng",
      "Zhigang Wang",
      "Yisheng Zhang",
      "Pengxu Chang",
      "Ziwen He",
      "Kai Gu",
      "Hongshen Zhang",
      "Ming Chen"
    ],
    "abstract": "Task and Motion Planning (TAMP) is essential for robots to interact with the\nworld and accomplish complex tasks. The TAMP problem involves a critical gap:\nexploring the robot's configuration parameters (such as chassis position and\nrobotic arm joint angles) within continuous space to ensure that task-level\nglobal constraints are met while also enhancing the efficiency of subsequent\nmotion planning. Existing methods still have significant room for improvement\nin terms of efficiency. Recognizing that robot kinematics is a key factor in\nmotion planning, we propose a framework called the Robotic Kinematics Informed\nNeural Network (RobKiNet) as a bridge between task and motion layers. RobKiNet\nintegrates kinematic knowledge into neural networks to train models capable of\nefficient configuration prediction. We designed a Chassis Motion Predictor(CMP)\nand a Full Motion Predictor(FMP) using RobKiNet, which employed two entirely\ndifferent sets of forward and inverse kinematics constraints to achieve loosely\ncoupled control and whole-body control, respectively. Experiments demonstrate\nthat CMP and FMP can predict configuration parameters with 96.67% and 98%\naccuracy, respectively. That means that the corresponding motion planning can\nachieve a speedup of 24.24x and 153x compared to random sampling. Furthermore,\nRobKiNet demonstrates remarkable data efficiency. CMP only requires 1/71 and\nFMP only requires 1/15052 of the training data for the same prediction accuracy\ncompared to other deep learning methods. These results demonstrate the great\npotential of RoboKiNet in robot applications.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16281v2",
    "published_date": "2024-02-26 03:54:32 UTC",
    "updated_date": "2025-03-04 07:06:42 UTC"
  },
  {
    "arxiv_id": "2402.16278v3",
    "title": "A Self-matching Training Method with Annotation Embedding Models for Ontology Subsumption Prediction",
    "authors": [
      "Yukihiro Shiraishi",
      "Ken Kaneiwa"
    ],
    "abstract": "Recently, ontology embeddings representing entities in a low-dimensional\nspace have been proposed for ontology completion. However, the ontology\nembeddings for concept subsumption prediction do not address the difficulties\nof similar and isolated entities and fail to extract the global information of\nannotation axioms from an ontology. In this paper, we propose a self-matching\ntraining method for the two ontology embedding models: Inverted-index Matrix\nEmbedding (InME) and Co-occurrence Matrix Embedding (CoME). The two embeddings\ncapture the global and local information in annotation axioms by means of the\noccurring locations of each word in a set of axioms and the co-occurrences of\nwords in each axiom. The self-matching training method increases the robustness\nof the concept subsumption prediction when predicted superclasses are similar\nto subclasses and are isolated to other entities in an ontology. Our evaluation\nexperiments show that the self-matching training method with InME outperforms\nthe existing ontology embeddings for the GO and FoodOn ontologies and that the\nmethod with the concatenation of CoME and OWL2Vec* outperforms them for the\nHeLiS ontology.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "21 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.16278v3",
    "published_date": "2024-02-26 03:46:01 UTC",
    "updated_date": "2024-03-10 10:04:41 UTC"
  },
  {
    "arxiv_id": "2402.16269v1",
    "title": "From Large Language Models and Optimization to Decision Optimization CoPilot: A Research Manifesto",
    "authors": [
      "Segev Wasserkrug",
      "Leonard Boussioux",
      "Dick den Hertog",
      "Farzaneh Mirzazadeh",
      "Ilker Birbil",
      "Jannis Kurtz",
      "Donato Maragno"
    ],
    "abstract": "Significantly simplifying the creation of optimization models for real-world\nbusiness problems has long been a major goal in applying mathematical\noptimization more widely to important business and societal decisions. The\nrecent capabilities of Large Language Models (LLMs) present a timely\nopportunity to achieve this goal. Therefore, we propose research at the\nintersection of LLMs and optimization to create a Decision Optimization CoPilot\n(DOCP) - an AI tool designed to assist any decision maker, interacting in\nnatural language to grasp the business problem, subsequently formulating and\nsolving the corresponding optimization model. This paper outlines our DOCP\nvision and identifies several fundamental requirements for its implementation.\nWe describe the state of the art through a literature survey and experiments\nusing ChatGPT. We show that a) LLMs already provide substantial novel\ncapabilities relevant to a DOCP, and b) major research challenges remain to be\naddressed. We also propose possible research directions to overcome these gaps.\nWe also see this work as a call to action to bring together the LLM and\noptimization communities to pursue our vision, thereby enabling much more\nwidespread improved decision-making.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "math.OC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16269v1",
    "published_date": "2024-02-26 03:10:11 UTC",
    "updated_date": "2024-02-26 03:10:11 UTC"
  },
  {
    "arxiv_id": "2402.16268v1",
    "title": "Foundation Model Transparency Reports",
    "authors": [
      "Rishi Bommasani",
      "Kevin Klyman",
      "Shayne Longpre",
      "Betty Xiong",
      "Sayash Kapoor",
      "Nestor Maslej",
      "Arvind Narayanan",
      "Percy Liang"
    ],
    "abstract": "Foundation models are critical digital technologies with sweeping societal\nimpact that necessitates transparency. To codify how foundation model\ndevelopers should provide transparency about the development and deployment of\ntheir models, we propose Foundation Model Transparency Reports, drawing upon\nthe transparency reporting practices in social media. While external\ndocumentation of societal harms prompted social media transparency reports, our\nobjective is to institutionalize transparency reporting for foundation models\nwhile the industry is still nascent. To design our reports, we identify 6\ndesign principles given the successes and shortcomings of social media\ntransparency reporting. To further schematize our reports, we draw upon the 100\ntransparency indicators from the Foundation Model Transparency Index. Given\nthese indicators, we measure the extent to which they overlap with the\ntransparency requirements included in six prominent government policies (e.g.,\nthe EU AI Act, the US Executive Order on Safe, Secure, and Trustworthy AI).\nWell-designed transparency reports could reduce compliance costs, in part due\nto overlapping regulatory requirements across different jurisdictions. We\nencourage foundation model developers to regularly publish transparency\nreports, building upon recommendations from the G7 and the White House.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16268v1",
    "published_date": "2024-02-26 03:09:06 UTC",
    "updated_date": "2024-02-26 03:09:06 UTC"
  },
  {
    "arxiv_id": "2402.16255v1",
    "title": "Watch Your Head: Assembling Projection Heads to Save the Reliability of Federated Models",
    "authors": [
      "Jinqian Chen",
      "Jihua Zhu",
      "Qinghai Zheng",
      "Zhongyu Li",
      "Zhiqiang Tian"
    ],
    "abstract": "Federated learning encounters substantial challenges with heterogeneous data,\nleading to performance degradation and convergence issues. While considerable\nprogress has been achieved in mitigating such an impact, the reliability aspect\nof federated models has been largely disregarded. In this study, we conduct\nextensive experiments to investigate the reliability of both generic and\npersonalized federated models. Our exploration uncovers a significant finding:\n\\textbf{federated models exhibit unreliability when faced with heterogeneous\ndata}, demonstrating poor calibration on in-distribution test data and low\nuncertainty levels on out-of-distribution data. This unreliability is primarily\nattributed to the presence of biased projection heads, which introduce\nmiscalibration into the federated models. Inspired by this observation, we\npropose the \"Assembled Projection Heads\" (APH) method for enhancing the\nreliability of federated models. By treating the existing projection head\nparameters as priors, APH randomly samples multiple initialized parameters of\nprojection heads from the prior and further performs targeted fine-tuning on\nlocally available data under varying learning rates. Such a head ensemble\nintroduces parameter diversity into the deterministic model, eliminating the\nbias and producing reliable predictions via head averaging. We evaluate the\neffectiveness of the proposed APH method across three prominent federated\nbenchmarks. Experimental results validate the efficacy of APH in model\ncalibration and uncertainty estimation. Notably, APH can be seamlessly\nintegrated into various federated approaches but only requires less than 30\\%\nadditional computation cost for 100$\\times$ inferences within large models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted in AAAI-24",
    "pdf_url": "http://arxiv.org/pdf/2402.16255v1",
    "published_date": "2024-02-26 02:37:39 UTC",
    "updated_date": "2024-02-26 02:37:39 UTC"
  },
  {
    "arxiv_id": "2402.16248v1",
    "title": "Topic-to-essay generation with knowledge-based content selection",
    "authors": [
      "Jieyong Wang",
      "Chunyao Song",
      "Yihao Wu"
    ],
    "abstract": "The topic-to-essay generation task is a challenging natural language\ngeneration task that aims to generate paragraph-level text with high semantic\ncoherence based on a given set of topic words. Previous work has focused on the\nintroduction of external knowledge, ignoring the insufficient generated text\ndiversity. In order to improve the generation diversity, we propose a novel\ncopy mechanism model with a content selection module that integrates rich\nsemantic knowledge from the language model into the decoder. Furthermore, we\nintroduce the improved prefix tuning method to train the model, enabling it to\nadapt to varying input complexities. In addition, we have contributed a new\nChinese dataset for TEG tasks. Experimental results demonstrate that the\nproposed model can improve the generated text diversity by 35\\% to 59\\%\ncompared to the state-of-the-art method, while maintaining a high level of\ntopic consistency.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16248v1",
    "published_date": "2024-02-26 02:14:42 UTC",
    "updated_date": "2024-02-26 02:14:42 UTC"
  },
  {
    "arxiv_id": "2402.16242v1",
    "title": "HSONet:A Siamese foreground association-driven hard case sample optimization network for high-resolution remote sensing image change detection",
    "authors": [
      "Chao Tao",
      "Dongsheng Kuang",
      "Zhenyang Huang",
      "Chengli Peng",
      "Haifeng Li"
    ],
    "abstract": "In the later training stages, further improvement of the models ability to\ndetermine changes relies on how well the change detection (CD) model learns\nhard cases; however, there are two additional challenges to learning hard case\nsamples: (1) change labels are limited and tend to pointer only to foreground\ntargets, yet hard case samples are prevalent in the background, which leads to\noptimizing the loss function focusing on the foreground targets and ignoring\nthe background hard cases, which we call imbalance. (2) Complex situations,\nsuch as light shadows, target occlusion, and seasonal changes, induce hard case\nsamples, and in the absence of both supervisory and scene information, it is\ndifficult for the model to learn hard case samples directly to accurately\nobtain the feature representations of the change information, which we call\nmissingness. We propose a Siamese foreground association-driven hard case\nsample optimization network (HSONet). To deal with this imbalance, we propose\nan equilibrium optimization loss function to regulate the optimization focus of\nthe foreground and background, determine the hard case samples through the\ndistribution of the loss values, and introduce dynamic weights in the loss term\nto gradually shift the optimization focus of the loss from the foreground to\nthe background hard cases as the training progresses. To address this\nmissingness, we understand hard case samples with the help of the scene\ncontext, propose the scene-foreground association module, use potential remote\nsensing spatial scene information to model the association between the target\nof interest in the foreground and the related context to obtain scene\nembedding, and apply this information to the feature reinforcement of hard\ncases. Experiments on four public datasets show that HSONet outperforms current\nstate-of-the-art CD methods, particularly in detecting hard case samples.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "17 figures, 8 tables, 18 pages",
    "pdf_url": "http://arxiv.org/pdf/2402.16242v1",
    "published_date": "2024-02-26 02:03:08 UTC",
    "updated_date": "2024-02-26 02:03:08 UTC"
  },
  {
    "arxiv_id": "2402.16237v1",
    "title": "Active Level Set Estimation for Continuous Search Space with Theoretical Guarantee",
    "authors": [
      "Giang Ngo",
      "Dang Nguyen",
      "Dat Phan-Trong",
      "Sunil Gupta"
    ],
    "abstract": "A common problem encountered in many real-world applications is level set\nestimation where the goal is to determine the region in the function domain\nwhere the function is above or below a given threshold. When the function is\nblack-box and expensive to evaluate, the level sets need to be found in a\nminimum set of function evaluations. Existing methods often assume a discrete\nsearch space with a finite set of data points for function evaluations and\nestimating the level sets. When applied to a continuous search space, these\nmethods often need to first discretize the space which leads to poor results\nwhile needing high computational time. While some methods cater for the\ncontinuous setting, they still lack a proper guarantee for theoretical\nconvergence. To address this problem, we propose a novel algorithm that does\nnot need any discretization and can directly work in continuous search spaces.\nOur method suggests points by constructing an acquisition function that is\ndefined as a measure of confidence of the function being higher or lower than\nthe given threshold. A theoretical analysis for the convergence of the\nalgorithm to an accurate solution is provided. On multiple synthetic and\nreal-world datasets, our algorithm successfully outperforms state-of-the-art\nmethods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16237v1",
    "published_date": "2024-02-26 01:46:56 UTC",
    "updated_date": "2024-02-26 01:46:56 UTC"
  },
  {
    "arxiv_id": "2402.16235v2",
    "title": "Human-AI Co-Creation of Worked Examples for Programming Classes",
    "authors": [
      "Mohammad Hassany",
      "Peter Brusilovsky",
      "Jiaze Ke",
      "Kamil Akhuseyinoglu",
      "Arun Balajiee Lekshmi Narayanan"
    ],
    "abstract": "Worked examples (solutions to typical programming problems presented as a\nsource code in a certain language and are used to explain the topics from a\nprogramming class) are among the most popular types of learning content in\nprogramming classes. Most approaches and tools for presenting these examples to\nstudents are based on line-by-line explanations of the example code. However,\ninstructors rarely have time to provide line-by-line explanations for a large\nnumber of examples typically used in a programming class. In this paper, we\nexplore and assess a human-AI collaboration approach to authoring worked\nexamples for Java programming. We introduce an authoring system for creating\nJava worked examples that generates a starting version of code explanations and\npresents it to the instructor to edit if necessary.We also present a study that\nassesses the quality of explanations created with this approach",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "arXiv admin note: substantial text overlap with arXiv:2312.02105",
    "pdf_url": "http://arxiv.org/pdf/2402.16235v2",
    "published_date": "2024-02-26 01:44:24 UTC",
    "updated_date": "2024-02-29 05:22:01 UTC"
  },
  {
    "arxiv_id": "2402.16230v1",
    "title": "GARNN: An Interpretable Graph Attentive Recurrent Neural Network for Predicting Blood Glucose Levels via Multivariate Time Series",
    "authors": [
      "Chengzhe Piao",
      "Taiyu Zhu",
      "Stephanie E Baldeweg",
      "Paul Taylor",
      "Pantelis Georgiou",
      "Jiahao Sun",
      "Jun Wang",
      "Kezhi Li"
    ],
    "abstract": "Accurate prediction of future blood glucose (BG) levels can effectively\nimprove BG management for people living with diabetes, thereby reducing\ncomplications and improving quality of life. The state of the art of BG\nprediction has been achieved by leveraging advanced deep learning methods to\nmodel multi-modal data, i.e., sensor data and self-reported event data,\norganised as multi-variate time series (MTS). However, these methods are mostly\nregarded as ``black boxes'' and not entirely trusted by clinicians and\npatients. In this paper, we propose interpretable graph attentive recurrent\nneural networks (GARNNs) to model MTS, explaining variable contributions via\nsummarizing variable importance and generating feature maps by graph attention\nmechanisms instead of post-hoc analysis. We evaluate GARNNs on four datasets,\nrepresenting diverse clinical scenarios. Upon comparison with twelve\nwell-established baseline methods, GARNNs not only achieve the best prediction\naccuracy but also provide high-quality temporal interpretability, in particular\nfor postprandial glucose levels as a result of corresponding meal intake and\ninsulin injection. These findings underline the potential of GARNN as a robust\ntool for improving diabetes care, bridging the gap between deep learning\ntechnology and real-world healthcare solutions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.16230v1",
    "published_date": "2024-02-26 01:18:53 UTC",
    "updated_date": "2024-02-26 01:18:53 UTC"
  },
  {
    "arxiv_id": "2403.00816v3",
    "title": "Read and Think: An Efficient Step-wise Multimodal Language Model for Document Understanding and Reasoning",
    "authors": [
      "Jinxu Zhang"
    ],
    "abstract": "Understanding the contents of multimodal documents is essential to accurately\nextract relevant evidence and use it for reasoning. Existing document\nunderstanding models tend to generate answers with a single word or phrase\ndirectly, ignoring the source document's evidence and lacking interpretability.\nIn this work, we address the lack of step-wise capabilities through data\naugmentation and extension. Specifically, We use Multi-modal Large Language\nModels (MLLMs), which have strong visual understanding and reasoning abilities,\nas data generators to generate step-wise question-and-answer pairs for document\nimages and use a high-performance LLM as the error detector to filter out noisy\ndata. This step-wise data generation pipeline is implemented using both\ntemplate-based and few-shot methods. We then use the generated high-quality\ndata to train a humanized document understanding and reasoning model,\nspecifically designed to solve complex questions that require reasoning or\nmulti-hop question answering, dubbed DocAssistant. Experimental results\ndemonstrate the effectiveness and application value of step-wise generation,\nshowing a 5 improvement on InfoVQA with complex layouts and a 7 improvement on\nChartQA with complex reasoning, compared to directly generated answers. We hope\nour work highlights the potential of synthetic data and encourages further\nexploration of multi-modal document reasoning capabilities.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.00816v3",
    "published_date": "2024-02-26 01:17:50 UTC",
    "updated_date": "2024-08-14 07:26:08 UTC"
  }
]
[
  {
    "arxiv_id": "2406.18790v2",
    "title": "MUMU: Bootstrapping Multimodal Image Generation from Text-to-Image Data",
    "authors": [
      "William Berman",
      "Alexander Peysakhovich"
    ],
    "abstract": "We train a model to generate images from multimodal prompts of interleaved\ntext and images such as \"a <picture of a man> man and his <picture of a dog>\ndog in an <picture of a cartoon> animated style.\" We bootstrap a multimodal\ndataset by extracting semantically meaningful image crops corresponding to\nwords in the image captions of synthetically generated and publicly available\ntext-image data. Our model, MUMU, is composed of a vision-language model\nencoder with a diffusion decoder and is trained on a single 8xH100 GPU node.\nDespite being only trained on crops from the same image, MUMU learns to compose\ninputs from different images into a coherent output. For example, an input of a\nrealistic person and a cartoon will output the same person in the cartoon\nstyle, and an input of a standing subject and a scooter will output the subject\nriding the scooter. As a result, our model generalizes to tasks such as style\ntransfer and character consistency. Our results show the promise of using\nmultimodal models as general purpose controllers for image generation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.18790v2",
    "published_date": "2024-06-26 23:21:42 UTC",
    "updated_date": "2024-09-11 21:56:02 UTC"
  },
  {
    "arxiv_id": "2407.11014v1",
    "title": "Geode: A Zero-shot Geospatial Question-Answering Agent with Explicit Reasoning and Precise Spatio-Temporal Retrieval",
    "authors": [
      "Devashish Vikas Gupta",
      "Azeez Syed Ali Ishaqui",
      "Divya Kiran Kadiyala"
    ],
    "abstract": "Large language models (LLMs) have shown promising results in learning and\ncontextualizing information from different forms of data. Recent advancements\nin foundational models, particularly those employing self-attention mechanisms,\nhave significantly enhanced our ability to comprehend the semantics of diverse\ndata types. One such area that could highly benefit from multi-modality is in\nunderstanding geospatial data, which inherently has multiple modalities.\nHowever, current Natural Language Processing (NLP) mechanisms struggle to\neffectively address geospatial queries. Existing pre-trained LLMs are\ninadequately equipped to meet the unique demands of geospatial data, lacking\nthe ability to retrieve precise spatio-temporal data in real-time, thus leading\nto significantly reduced accuracy in answering complex geospatial queries. To\naddress these limitations, we introduce Geode--a pioneering system designed to\ntackle zero-shot geospatial question-answering tasks with high precision using\nspatio-temporal data retrieval. Our approach represents a significant\nimprovement in addressing the limitations of current LLM models, demonstrating\nremarkable improvement in geospatial question-answering abilities compared to\nexisting state-of-the-art pre-trained models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.11014v1",
    "published_date": "2024-06-26 21:59:54 UTC",
    "updated_date": "2024-06-26 21:59:54 UTC"
  },
  {
    "arxiv_id": "2406.18765v1",
    "title": "WV-Net: A foundation model for SAR WV-mode satellite imagery trained using contrastive self-supervised learning on 10 million images",
    "authors": [
      "Yannik Glaser",
      "Justin E. Stopa",
      "Linnea M. Wolniewicz",
      "Ralph Foster",
      "Doug Vandemark",
      "Alexis Mouche",
      "Bertrand Chapron",
      "Peter Sadowski"
    ],
    "abstract": "The European Space Agency's Copernicus Sentinel-1 (S-1) mission is a\nconstellation of C-band synthetic aperture radar (SAR) satellites that provide\nunprecedented monitoring of the world's oceans. S-1's wave mode (WV) captures\n20x20 km image patches at 5 m pixel resolution and is unaffected by cloud cover\nor time-of-day. The mission's open data policy has made SAR data easily\naccessible for a range of applications, but the need for manual image\nannotations is a bottleneck that hinders the use of machine learning methods.\nThis study uses nearly 10 million WV-mode images and contrastive\nself-supervised learning to train a semantic embedding model called WV-Net. In\nmultiple downstream tasks, WV-Net outperforms a comparable model that was\npre-trained on natural images (ImageNet) with supervised learning. Experiments\nshow improvements for estimating wave height (0.50 vs 0.60 RMSE using linear\nprobing), estimating near-surface air temperature (0.90 vs 0.97 RMSE), and\nperforming multilabel-classification of geophysical and atmospheric phenomena\n(0.96 vs 0.95 micro-averaged AUROC). WV-Net embeddings are also superior in an\nunsupervised image-retrieval task and scale better in data-sparse settings.\nTogether, these results demonstrate that WV-Net embeddings can support\ngeophysical research by providing a convenient foundation model for a variety\nof data analysis and exploration tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "J.2; I.4.10"
    ],
    "primary_category": "cs.LG",
    "comment": "20 pages, 9 figures, submitted to NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.18765v1",
    "published_date": "2024-06-26 21:30:41 UTC",
    "updated_date": "2024-06-26 21:30:41 UTC"
  },
  {
    "arxiv_id": "2406.18763v2",
    "title": "Conformalized Link Prediction on Graph Neural Networks",
    "authors": [
      "Tianyi Zhao",
      "Jian Kang",
      "Lu Cheng"
    ],
    "abstract": "Graph Neural Networks (GNNs) excel in diverse tasks, yet their applications\nin high-stakes domains are often hampered by unreliable predictions. Although\nnumerous uncertainty quantification methods have been proposed to address this\nlimitation, they often lack \\textit{rigorous} uncertainty estimates. This work\nmakes the first attempt to introduce a distribution-free and model-agnostic\nuncertainty quantification approach to construct a predictive interval with a\nstatistical guarantee for GNN-based link prediction. We term it as\n\\textit{conformalized link prediction.} Our approach builds upon conformal\nprediction (CP), a framework that promises to construct statistically robust\nprediction sets or intervals. We first theoretically and empirically establish\na permutation invariance condition for the application of CP in link prediction\ntasks, along with an exact test-time coverage. Leveraging the important\nstructural information in graphs, we then identify a novel and crucial\nconnection between a graph's adherence to the power law distribution and the\nefficiency of CP. This insight leads to the development of a simple yet\neffective sampling-based method to align the graph structure with a power law\ndistribution prior to the standard CP procedure. Extensive experiments\ndemonstrate that for conformalized link prediction, our approach achieves the\ndesired marginal coverage while significantly improving the efficiency of CP\ncompared to baseline methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.18763v2",
    "published_date": "2024-06-26 21:17:37 UTC",
    "updated_date": "2024-07-18 22:06:38 UTC"
  },
  {
    "arxiv_id": "2406.18757v2",
    "title": "The Impact of Feature Representation on the Accuracy of Photonic Neural Networks",
    "authors": [
      "Mauricio Gomes de Queiroz",
      "Paul Jimenez",
      "Raphael Cardoso",
      "Mateus Vidaletti Costa",
      "Mohab Abdalla",
      "Ian O'Connor",
      "Alberto Bosio",
      "Fabio Pavanello"
    ],
    "abstract": "Photonic Neural Networks (PNNs) are gaining significant interest in the\nresearch community due to their potential for high parallelization, low\nlatency, and energy efficiency. PNNs compute using light, which leads to\nseveral differences in implementation when compared to electronics, such as the\nneed to represent input features in the photonic domain before feeding them\ninto the network. In this encoding process, it is common to combine multiple\nfeatures into a single input to reduce the number of inputs and associated\ndevices, leading to smaller and more energy-efficient PNNs. Although this\nalters the network's handling of input data, its impact on PNNs remains\nunderstudied. This paper addresses this open question, investigating the effect\nof commonly used encoding strategies that combine features on the performance\nand learning capabilities of PNNs. Here, using the concept of feature\nimportance, we develop a mathematical methodology for analyzing feature\ncombination. Through this methodology, we demonstrate that encoding multiple\nfeatures together in a single input determines their relative importance, thus\nlimiting the network's ability to learn from the data. Given some prior\nknowledge of the data, however, this can also be leveraged for higher accuracy.\nBy selecting an optimal encoding method, we achieve up to a 12.3% improvement\nin accuracy of PNNs trained on the Iris dataset compared to other encoding\ntechniques, surpassing the performance of networks where features are not\ncombined. These findings highlight the importance of carefully choosing the\nencoding to the accuracy and decision-making strategies of PNNs, particularly\nin size or power constrained applications.",
    "categories": [
      "cs.ET",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.ET",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.18757v2",
    "published_date": "2024-06-26 20:55:26 UTC",
    "updated_date": "2024-06-28 17:12:37 UTC"
  },
  {
    "arxiv_id": "2406.18747v2",
    "title": "A Stem-Agnostic Single-Decoder System for Music Source Separation Beyond Four Stems",
    "authors": [
      "Karn N. Watcharasupat",
      "Alexander Lerch"
    ],
    "abstract": "Despite significant recent progress across multiple subtasks of audio source\nseparation, few music source separation systems support separation beyond the\nfour-stem vocals, drums, bass, and other (VDBO) setup. Of the very few current\nsystems that support source separation beyond this setup, most continue to rely\non an inflexible decoder setup that can only support a fixed pre-defined set of\nstems. Increasing stem support in these inflexible systems correspondingly\nrequires increasing computational complexity, rendering extensions of these\nsystems computationally infeasible for long-tail instruments. In this work, we\npropose Banquet, a system that allows source separation of multiple stems using\njust one decoder. A bandsplit source separation model is extended to work in a\nquery-based setup in tandem with a music instrument recognition PaSST model. On\nthe MoisesDB dataset, Banquet, at only 24.9 M trainable parameters, approached\nthe performance level of the significantly more complex 6-stem Hybrid\nTransformer Demucs on VDBO stems and outperformed it on guitar and piano. The\nquery-based setup allows for the separation of narrow instrument classes such\nas clean acoustic guitars, and can be successfully applied to the extraction of\nless common stems such as reeds and organs. Implementation is available at\nhttps://github.com/kwatcharasupat/query-bandit.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.IR",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted to the 25th International Society for Music Information\n  Retrieval Conference (ISMIR 2024). Camera-ready version",
    "pdf_url": "http://arxiv.org/pdf/2406.18747v2",
    "published_date": "2024-06-26 20:25:53 UTC",
    "updated_date": "2024-08-26 01:07:11 UTC"
  },
  {
    "arxiv_id": "2406.18741v1",
    "title": "Decentralized Semantic Traffic Control in AVs Using RL and DQN for Dynamic Roadblocks",
    "authors": [
      "Emanuel Figetakis",
      "Yahuza Bello",
      "Ahmed Refaey",
      "Abdallah Shami"
    ],
    "abstract": "Autonomous Vehicles (AVs), furnished with sensors capable of capturing\nessential vehicle dynamics such as speed, acceleration, and precise location,\npossess the capacity to execute intelligent maneuvers, including lane changes,\nin anticipation of approaching roadblocks. Nevertheless, the sheer volume of\nsensory data and the processing necessary to derive informed decisions can\noften overwhelm the vehicles, rendering them unable to handle the task\nindependently. Consequently, a common approach in traffic scenarios involves\ntransmitting the data to servers for processing, a practice that introduces\nchallenges, particularly in situations demanding real-time processing. In\nresponse to this challenge, we present a novel DL-based semantic traffic\ncontrol system that entrusts semantic encoding responsibilities to the vehicles\nthemselves. This system processes driving decisions obtained from a\nReinforcement Learning (RL) agent, streamlining the decision-making process.\nSpecifically, our framework envisions scenarios where abrupt roadblocks\nmaterialize due to factors such as road maintenance, accidents, or vehicle\nrepairs, necessitating vehicles to make determinations concerning lane-keeping\nor lane-changing actions to navigate past these obstacles. To formulate this\nscenario mathematically, we employ a Markov Decision Process (MDP) and harness\nthe Deep Q Learning (DQN) algorithm to unearth viable solutions.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.18741v1",
    "published_date": "2024-06-26 20:12:48 UTC",
    "updated_date": "2024-06-26 20:12:48 UTC"
  },
  {
    "arxiv_id": "2406.18731v1",
    "title": "WavRx: a Disease-Agnostic, Generalizable, and Privacy-Preserving Speech Health Diagnostic Model",
    "authors": [
      "Yi Zhu",
      "Tiago Falk"
    ],
    "abstract": "Speech is known to carry health-related attributes, which has emerged as a\nnovel venue for remote and long-term health monitoring. However, existing\nmodels are usually tailored for a specific type of disease, and have been shown\nto lack generalizability across datasets. Furthermore, concerns have been\nraised recently towards the leakage of speaker identity from health embeddings.\nTo mitigate these limitations, we propose WavRx, a speech health diagnostics\nmodel that captures the respiration and articulation related dynamics from a\nuniversal speech representation. Our in-domain and cross-domain experiments on\nsix pathological speech datasets demonstrate WavRx as a new state-of-the-art\nhealth diagnostic model. Furthermore, we show that the amount of speaker\nidentity entailed in the WavRx health embeddings is significantly reduced\nwithout extra guidance during training. An in-depth analysis of the model was\nperformed, thus providing physiological interpretation of its improved\ngeneralizability and privacy-preserving ability.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "eess.AS",
    "comment": "Under review; Model script available at\n  https://github.com/zhu00121/WavRx",
    "pdf_url": "http://arxiv.org/pdf/2406.18731v1",
    "published_date": "2024-06-26 19:59:21 UTC",
    "updated_date": "2024-06-26 19:59:21 UTC"
  },
  {
    "arxiv_id": "2406.18701v1",
    "title": "Fast Optimizer Benchmark",
    "authors": [
      "Simon Blauth",
      "Tobias Bürger",
      "Zacharias Häringer",
      "Jörg Franke",
      "Frank Hutter"
    ],
    "abstract": "In this paper, we present the Fast Optimizer Benchmark (FOB), a tool designed\nfor evaluating deep learning optimizers during their development. The benchmark\nsupports tasks from multiple domains such as computer vision, natural language\nprocessing, and graph learning. The focus is on convenient usage, featuring\nhuman-readable YAML configurations, SLURM integration, and plotting utilities.\nFOB can be used together with existing hyperparameter optimization (HPO) tools\nas it handles training and resuming of runs. The modular design enables\nintegration into custom pipelines, using it simply as a collection of tasks. We\nshowcase an optimizer comparison as a usage example of our tool. FOB can be\nfound on GitHub: https://github.com/automl/FOB.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "5 pages + 12 appendix pages, submitted to AutoML Conf 2024 Workshop\n  Track",
    "pdf_url": "http://arxiv.org/pdf/2406.18701v1",
    "published_date": "2024-06-26 19:10:34 UTC",
    "updated_date": "2024-06-26 19:10:34 UTC"
  },
  {
    "arxiv_id": "2406.18695v2",
    "title": "Learning to Correct for QA Reasoning with Black-box LLMs",
    "authors": [
      "Jaehyung Kim",
      "Dongyoung Kim",
      "Yiming Yang"
    ],
    "abstract": "An open challenge in recent machine learning is about how to improve the\nreasoning capability of large language models (LLMs) in a black-box setting,\ni.e., without access to detailed information such as output token\nprobabilities. Existing approaches either rely on accessibility (which is often\nunrealistic) or involve significantly increased train- and inference-time\ncosts. This paper addresses those limitations or shortcomings by proposing a\nnovel approach, namely CoBB (Correct for improving QA reasoning of Black-Box\nLLMs). It uses a trained adaptation model to perform a seq2seq mapping from the\noften-imperfect reasonings of the original black-box LLM to the correct or\nimproved reasonings. Specifically, the adaptation model is initialized with a\nrelatively small open-source LLM and adapted over a collection of sub-sampled\ntraining pairs. To select the representative pairs of correct and incorrect\nreasonings, we formulated the dataset construction as an optimization problem\nthat minimizes the statistical divergence between the sampled subset and the\nentire collection, and solved it via a genetic algorithm. We then train the\nadaptation model over the sampled pairs by contrasting the likelihoods of\ncorrect and incorrect reasonings. Our experimental results demonstrate that\nCoBB significantly improves reasoning accuracy across various QA benchmarks,\ncompared to the best-performing adaptation baselines.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "22 pages; EMNLP 2024 (long, main)",
    "pdf_url": "http://arxiv.org/pdf/2406.18695v2",
    "published_date": "2024-06-26 18:57:32 UTC",
    "updated_date": "2024-10-08 06:09:26 UTC"
  },
  {
    "arxiv_id": "2406.18690v1",
    "title": "Petal-X: Human-Centered Visual Explanations to Improve Cardiovascular Risk Communication",
    "authors": [
      "Diego Rojo",
      "Houda Lamqaddam",
      "Lucija Gosak",
      "Katrien Verbert"
    ],
    "abstract": "Cardiovascular diseases (CVDs), the leading cause of death worldwide, can be\nprevented in most cases through behavioral interventions. Therefore, effective\ncommunication of CVD risk and projected risk reduction by risk factor\nmodification plays a crucial role in reducing CVD risk at the individual level.\nHowever, despite interest in refining risk estimation with improved prediction\nmodels such as SCORE2, the guidelines for presenting these risk estimations in\nclinical practice remained essentially unchanged in the last few years, with\ngraphical score charts (GSCs) continuing to be one of the prevalent systems.\nThis work describes the design and implementation of Petal-X, a novel tool to\nsupport clinician-patient shared decision-making by explaining the CVD risk\ncontributions of different factors and facilitating what-if analysis. Petal-X\nrelies on a novel visualization, Petal Product Plots, and a tailor-made global\nsurrogate model of SCORE2, whose fidelity is comparable to that of the GSCs\nused in clinical practice. We evaluated Petal-X compared to GSCs in a\ncontrolled experiment with 88 healthcare students, all but one with experience\nwith chronic patients. The results show that Petal-X outperforms GSC in\ncritical tasks, such as comparing the contribution to the patient's 10-year CVD\nrisk of each modifiable risk factor, without a significant loss of perceived\ntransparency, trust, or intent to use. Our study provides an innovative\napproach to the visualization and explanation of risk in clinical practice\nthat, due to its model-agnostic nature, could continue to support\nnext-generation artificial intelligence risk assessment models.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.18690v1",
    "published_date": "2024-06-26 18:48:50 UTC",
    "updated_date": "2024-06-26 18:48:50 UTC"
  },
  {
    "arxiv_id": "2406.18682v2",
    "title": "The Multilingual Alignment Prism: Aligning Global and Local Preferences to Reduce Harm",
    "authors": [
      "Aakanksha",
      "Arash Ahmadian",
      "Beyza Ermis",
      "Seraphina Goldfarb-Tarrant",
      "Julia Kreutzer",
      "Marzieh Fadaee",
      "Sara Hooker"
    ],
    "abstract": "A key concern with the concept of \"alignment\" is the implicit question of\n\"alignment to what?\". AI systems are increasingly used across the world, yet\nsafety alignment is often focused on homogeneous monolingual settings.\nAdditionally, preference training and safety measures often overfit to harms\ncommon in Western-centric datasets. Here, we explore the viability of different\nalignment approaches when balancing dual objectives: addressing and optimizing\nfor a non-homogeneous set of languages and cultural preferences while\nminimizing both global and local harms. We collect the first set of human\nannotated red-teaming prompts in different languages distinguishing between\nglobal and local harm, which serve as a laboratory for understanding the\nreliability of alignment techniques when faced with preference distributions\nthat are non-stationary across geographies and languages. While this setting is\nseldom covered by the literature to date, which primarily centers on English\nharm mitigation, it captures real-world interactions with AI systems around the\nworld. We establish a new precedent for state-of-the-art alignment techniques\nacross 6 languages with minimal degradation in general performance. Our work\nprovides important insights into cross-lingual transfer and novel optimization\napproaches to safeguard AI systems designed to serve global populations.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.18682v2",
    "published_date": "2024-06-26 18:39:08 UTC",
    "updated_date": "2024-07-08 14:26:16 UTC"
  },
  {
    "arxiv_id": "2406.18679v1",
    "title": "Speakers Unembedded: Embedding-free Approach to Long-form Neural Diarization",
    "authors": [
      "Xiang Li",
      "Vivek Govindan",
      "Rohit Paturi",
      "Sundararajan Srinivasan"
    ],
    "abstract": "End-to-end neural diarization (EEND) models offer significant improvements\nover traditional embedding-based Speaker Diarization (SD) approaches but falls\nshort on generalizing to long-form audio with large number of speakers.\nEEND-vector-clustering method mitigates this by combining local EEND with\nglobal clustering of speaker embeddings from local windows, but this requires\nan additional speaker embedding framework alongside the EEND module. In this\npaper, we propose a novel framework applying EEND both locally and globally for\nlong-form audio without separate speaker embeddings. This approach achieves\nsignificant relative DER reduction of 13% and 10% over the conventional 1-pass\nEEND on Callhome American English and RT03-CTS datasets respectively and\nmarginal improvements over EEND-vector-clustering without the need for\nadditional speaker embeddings. Furthermore, we discuss the computational\ncomplexity of our proposed framework and explore strategies for reducing\nprocessing times.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted at INTERSPEECH 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.18679v1",
    "published_date": "2024-06-26 18:32:16 UTC",
    "updated_date": "2024-06-26 18:32:16 UTC"
  },
  {
    "arxiv_id": "2406.18678v2",
    "title": "Few-shot Personalization of LLMs with Mis-aligned Responses",
    "authors": [
      "Jaehyung Kim",
      "Yiming Yang"
    ],
    "abstract": "As the diversity of users increases, the capability of providing personalized\nresponses by large language models (LLMs) has become increasingly important.\nExisting approaches have only limited successes in LLM personalization, due to\nthe absence of personalized learning or the reliance on shared personal data.\nThis paper proposes a new approach for a few-shot personalization of LLMs with\ntheir mis-aligned responses (Fermi). Our key idea is to learn a set of\npersonalized prompts for each user by progressively improving the prompts using\nLLMs, based on user profile (e.g., demographic information) and a few examples\nof previous opinions. During an iterative process of prompt improvement, we\nincorporate the contexts of mis-aligned responses by LLMs, which are especially\ncrucial for the effective personalization of LLMs. In addition, we develop an\neffective inference method to further leverage the context of the test query\nand the personalized prompts. Our experimental results demonstrate that Fermi\nsignificantly improves performance across various benchmarks, compared to\nbest-performing baselines.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "NAACL 25 (main, long), 32 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.18678v2",
    "published_date": "2024-06-26 18:29:12 UTC",
    "updated_date": "2025-03-04 03:45:47 UTC"
  },
  {
    "arxiv_id": "2406.18676v2",
    "title": "Understand What LLM Needs: Dual Preference Alignment for Retrieval-Augmented Generation",
    "authors": [
      "Guanting Dong",
      "Yutao Zhu",
      "Chenghao Zhang",
      "Zechen Wang",
      "Zhicheng Dou",
      "Ji-Rong Wen"
    ],
    "abstract": "Retrieval-augmented generation (RAG) has demonstrated effectiveness in\nmitigating the hallucination problem of large language models (LLMs). However,\nthe difficulty of aligning the retriever with the diverse LLMs' knowledge\npreferences inevitably poses an inevitable challenge in developing a reliable\nRAG system. To address this issue, we propose DPA-RAG, a universal framework\ndesigned to align diverse knowledge preferences within RAG systems.\nSpecifically, we initially introduce a preference knowledge construction\npipline and incorporate five novel query augmentation strategies to alleviate\npreference data scarcity. Based on preference data, DPA-RAG accomplishes both\nexternal and internal preference alignment: 1) It jointly integrate pair-wise,\npoint-wise, and contrastive preference alignment abilities into the reranker,\nachieving external preference alignment among RAG components. 2) It further\nintroduces a pre-aligned stage before vanilla Supervised Fine-tuning (SFT),\nenabling LLMs to implicitly capture knowledge aligned with their reasoning\npreferences, achieving LLMs' internal alignment. Experimental results across\nfour knowledge-intensive QA datasets demonstrate that DPA-RAG outperforms all\nbaselines and seamlessly integrates both black-box and open-sourced LLM\nreaders. Further qualitative analysis and discussions also provide empirical\nguidance for achieving reliable RAG systems. Our code is publicly available at\nhttps://github.com/dongguanting/DPA-RAG.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2406.18676v2",
    "published_date": "2024-06-26 18:26:53 UTC",
    "updated_date": "2024-07-18 08:28:09 UTC"
  },
  {
    "arxiv_id": "2406.18675v2",
    "title": "Human-AI Collaborative Taxonomy Construction: A Case Study in Profession-Specific Writing Assistants",
    "authors": [
      "Minhwa Lee",
      "Zae Myung Kim",
      "Vivek Khetan",
      "Dongyeop Kang"
    ],
    "abstract": "Large Language Models (LLMs) have assisted humans in several writing tasks,\nincluding text revision and story generation. However, their effectiveness in\nsupporting domain-specific writing, particularly in business contexts, is\nrelatively less explored. Our formative study with industry professionals\nrevealed the limitations in current LLMs' understanding of the nuances in such\ndomain-specific writing. To address this gap, we propose an approach of\nhuman-AI collaborative taxonomy development to perform as a guideline for\ndomain-specific writing assistants. This method integrates iterative feedback\nfrom domain experts and multiple interactions between these experts and LLMs to\nrefine the taxonomy. Through larger-scale experiments, we aim to validate this\nmethodology and thus improve LLM-powered writing assistance, tailoring it to\nmeet the unique requirements of different stakeholder needs.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted to CHI 2024 In2Writing Workshop",
    "pdf_url": "http://arxiv.org/pdf/2406.18675v2",
    "published_date": "2024-06-26 18:25:06 UTC",
    "updated_date": "2024-07-16 00:13:09 UTC"
  },
  {
    "arxiv_id": "2406.18665v4",
    "title": "RouteLLM: Learning to Route LLMs with Preference Data",
    "authors": [
      "Isaac Ong",
      "Amjad Almahairi",
      "Vincent Wu",
      "Wei-Lin Chiang",
      "Tianhao Wu",
      "Joseph E. Gonzalez",
      "M Waleed Kadous",
      "Ion Stoica"
    ],
    "abstract": "Large language models (LLMs) exhibit impressive capabilities across a wide\nrange of tasks, yet the choice of which model to use often involves a trade-off\nbetween performance and cost. More powerful models, though effective, come with\nhigher expenses, while less capable models are more cost-effective. To address\nthis dilemma, we propose several efficient router models that dynamically\nselect between a stronger and a weaker LLM during inference, aiming to optimize\nthe balance between cost and response quality. We develop a training framework\nfor these routers leveraging human preference data and data augmentation\ntechniques to enhance performance. Our evaluation on widely-recognized\nbenchmarks shows that our approach significantly reduces costs-by over 2 times\nin certain cases-without compromising the quality of responses. Interestingly,\nour router models also demonstrate significant transfer learning capabilities,\nmaintaining their performance even when the strong and weak models are changed\nat test time. This highlights the potential of these routers to provide a\ncost-effective yet high-performance solution for deploying LLMs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.18665v4",
    "published_date": "2024-06-26 18:10:22 UTC",
    "updated_date": "2025-02-23 08:50:33 UTC"
  },
  {
    "arxiv_id": "2406.18630v1",
    "title": "Improving Hyperparameter Optimization with Checkpointed Model Weights",
    "authors": [
      "Nikhil Mehta",
      "Jonathan Lorraine",
      "Steve Masson",
      "Ramanathan Arunachalam",
      "Zaid Pervaiz Bhat",
      "James Lucas",
      "Arun George Zachariah"
    ],
    "abstract": "When training deep learning models, the performance depends largely on the\nselected hyperparameters. However, hyperparameter optimization (HPO) is often\none of the most expensive parts of model design. Classical HPO methods treat\nthis as a black-box optimization problem. However, gray-box HPO methods, which\nincorporate more information about the setup, have emerged as a promising\ndirection for more efficient optimization. For example, using intermediate loss\nevaluations to terminate bad selections. In this work, we propose an HPO method\nfor neural networks using logged checkpoints of the trained weights to guide\nfuture hyperparameter selections. Our method, Forecasting Model Search (FMS),\nembeds weights into a Gaussian process deep kernel surrogate model, using a\npermutation-invariant graph metanetwork to be data-efficient with the logged\nnetwork weights. To facilitate reproducibility and further research, we\nopen-source our code at https://github.com/NVlabs/forecasting-model-search.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML",
      "68T05",
      "I.2.6; G.1.6; D.2.8"
    ],
    "primary_category": "cs.LG",
    "comment": "See the project website at\n  https://research.nvidia.com/labs/toronto-ai/FMS/",
    "pdf_url": "http://arxiv.org/pdf/2406.18630v1",
    "published_date": "2024-06-26 17:59:54 UTC",
    "updated_date": "2024-06-26 17:59:54 UTC"
  },
  {
    "arxiv_id": "2406.18532v1",
    "title": "Symbolic Learning Enables Self-Evolving Agents",
    "authors": [
      "Wangchunshu Zhou",
      "Yixin Ou",
      "Shengwei Ding",
      "Long Li",
      "Jialong Wu",
      "Tiannan Wang",
      "Jiamin Chen",
      "Shuai Wang",
      "Xiaohua Xu",
      "Ningyu Zhang",
      "Huajun Chen",
      "Yuchen Eleanor Jiang"
    ],
    "abstract": "The AI community has been exploring a pathway to artificial general\nintelligence (AGI) by developing \"language agents\", which are complex large\nlanguage models (LLMs) pipelines involving both prompting techniques and tool\nusage methods. While language agents have demonstrated impressive capabilities\nfor many real-world tasks, a fundamental limitation of current language agents\nresearch is that they are model-centric, or engineering-centric. That's to say,\nthe progress on prompts, tools, and pipelines of language agents requires\nsubstantial manual engineering efforts from human experts rather than\nautomatically learning from data. We believe the transition from model-centric,\nor engineering-centric, to data-centric, i.e., the ability of language agents\nto autonomously learn and evolve in environments, is the key for them to\npossibly achieve AGI.\n  In this work, we introduce agent symbolic learning, a systematic framework\nthat enables language agents to optimize themselves on their own in a\ndata-centric way using symbolic optimizers. Specifically, we consider agents as\nsymbolic networks where learnable weights are defined by prompts, tools, and\nthe way they are stacked together. Agent symbolic learning is designed to\noptimize the symbolic network within language agents by mimicking two\nfundamental algorithms in connectionist learning: back-propagation and gradient\ndescent. Instead of dealing with numeric weights, agent symbolic learning works\nwith natural language simulacrums of weights, loss, and gradients. We conduct\nproof-of-concept experiments on both standard benchmarks and complex real-world\ntasks and show that agent symbolic learning enables language agents to update\nthemselves after being created and deployed in the wild, resulting in\n\"self-evolving agents\".",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Code available at https://github.com/aiwaves-cn/agents",
    "pdf_url": "http://arxiv.org/pdf/2406.18532v1",
    "published_date": "2024-06-26 17:59:18 UTC",
    "updated_date": "2024-06-26 17:59:18 UTC"
  },
  {
    "arxiv_id": "2406.18518v1",
    "title": "APIGen: Automated Pipeline for Generating Verifiable and Diverse Function-Calling Datasets",
    "authors": [
      "Zuxin Liu",
      "Thai Hoang",
      "Jianguo Zhang",
      "Ming Zhu",
      "Tian Lan",
      "Shirley Kokane",
      "Juntao Tan",
      "Weiran Yao",
      "Zhiwei Liu",
      "Yihao Feng",
      "Rithesh Murthy",
      "Liangwei Yang",
      "Silvio Savarese",
      "Juan Carlos Niebles",
      "Huan Wang",
      "Shelby Heinecke",
      "Caiming Xiong"
    ],
    "abstract": "The advancement of function-calling agent models requires diverse, reliable,\nand high-quality datasets. This paper presents APIGen, an automated data\ngeneration pipeline designed to synthesize verifiable high-quality datasets for\nfunction-calling applications. We leverage APIGen and collect 3,673 executable\nAPIs across 21 different categories to generate diverse function-calling\ndatasets in a scalable and structured manner. Each data in our dataset is\nverified through three hierarchical stages: format checking, actual function\nexecutions, and semantic verification, ensuring its reliability and\ncorrectness. We demonstrate that models trained with our curated datasets, even\nwith only 7B parameters, can achieve state-of-the-art performance on the\nBerkeley Function-Calling Benchmark, outperforming multiple GPT-4 models.\nMoreover, our 1B model achieves exceptional performance, surpassing\nGPT-3.5-Turbo and Claude-3 Haiku. We release a dataset containing 60,000\nhigh-quality entries, aiming to advance the field of function-calling agent\ndomains. The dataset is available on Huggingface:\nhttps://huggingface.co/datasets/Salesforce/xlam-function-calling-60k and the\nproject homepage: https://apigen-pipeline.github.io/",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SE"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.18518v1",
    "published_date": "2024-06-26 17:49:11 UTC",
    "updated_date": "2024-06-26 17:49:11 UTC"
  },
  {
    "arxiv_id": "2406.18629v1",
    "title": "Step-DPO: Step-wise Preference Optimization for Long-chain Reasoning of LLMs",
    "authors": [
      "Xin Lai",
      "Zhuotao Tian",
      "Yukang Chen",
      "Senqiao Yang",
      "Xiangru Peng",
      "Jiaya Jia"
    ],
    "abstract": "Mathematical reasoning presents a significant challenge for Large Language\nModels (LLMs) due to the extensive and precise chain of reasoning required for\naccuracy. Ensuring the correctness of each reasoning step is critical. To\naddress this, we aim to enhance the robustness and factuality of LLMs by\nlearning from human feedback. However, Direct Preference Optimization (DPO) has\nshown limited benefits for long-chain mathematical reasoning, as models\nemploying DPO struggle to identify detailed errors in incorrect answers. This\nlimitation stems from a lack of fine-grained process supervision. We propose a\nsimple, effective, and data-efficient method called Step-DPO, which treats\nindividual reasoning steps as units for preference optimization rather than\nevaluating answers holistically. Additionally, we have developed a data\nconstruction pipeline for Step-DPO, enabling the creation of a high-quality\ndataset containing 10K step-wise preference pairs. We also observe that in DPO,\nself-generated data is more effective than data generated by humans or GPT-4,\ndue to the latter's out-of-distribution nature. Our findings demonstrate that\nas few as 10K preference data pairs and fewer than 500 Step-DPO training steps\ncan yield a nearly 3% gain in accuracy on MATH for models with over 70B\nparameters. Notably, Step-DPO, when applied to Qwen2-72B-Instruct, achieves\nscores of 70.8% and 94.0% on the test sets of MATH and GSM8K, respectively,\nsurpassing a series of closed-source models, including GPT-4-1106,\nClaude-3-Opus, and Gemini-1.5-Pro. Our code, data, and models are available at\nhttps://github.com/dvlab-research/Step-DPO.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Code, data, and models are available at\n  https://github.com/dvlab-research/Step-DPO",
    "pdf_url": "http://arxiv.org/pdf/2406.18629v1",
    "published_date": "2024-06-26 17:43:06 UTC",
    "updated_date": "2024-06-26 17:43:06 UTC"
  },
  {
    "arxiv_id": "2407.01603v3",
    "title": "A Review of Large Language Models and Autonomous Agents in Chemistry",
    "authors": [
      "Mayk Caldas Ramos",
      "Christopher J. Collison",
      "Andrew D. White"
    ],
    "abstract": "Large language models (LLMs) have emerged as powerful tools in chemistry,\nsignificantly impacting molecule design, property prediction, and synthesis\noptimization. This review highlights LLM capabilities in these domains and\ntheir potential to accelerate scientific discovery through automation. We also\nreview LLM-based autonomous agents: LLMs with a broader set of tools to\ninteract with their surrounding environment. These agents perform diverse tasks\nsuch as paper scraping, interfacing with automated laboratories, and synthesis\nplanning. As agents are an emerging topic, we extend the scope of our review of\nagents beyond chemistry and discuss across any scientific domains. This review\ncovers the recent history, current capabilities, and design of LLMs and\nautonomous agents, addressing specific challenges, opportunities, and future\ndirections in chemistry. Key challenges include data quality and integration,\nmodel interpretability, and the need for standard benchmarks, while future\ndirections point towards more sophisticated multi-modal agents and enhanced\ncollaboration between agents and experimental methods. Due to the quick pace of\nthis field, a repository has been built to keep track of the latest studies:\nhttps://github.com/ur-whitelab/LLMs-in-science.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "physics.chem-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01603v3",
    "published_date": "2024-06-26 17:33:21 UTC",
    "updated_date": "2024-11-14 23:56:22 UTC"
  },
  {
    "arxiv_id": "2406.18505v1",
    "title": "Mental Modeling of Reinforcement Learning Agents by Language Models",
    "authors": [
      "Wenhao Lu",
      "Xufeng Zhao",
      "Josua Spisak",
      "Jae Hee Lee",
      "Stefan Wermter"
    ],
    "abstract": "Can emergent language models faithfully model the intelligence of\ndecision-making agents? Though modern language models exhibit already some\nreasoning ability, and theoretically can potentially express any probable\ndistribution over tokens, it remains underexplored how the world knowledge\nthese pretrained models have memorized can be utilized to comprehend an agent's\nbehaviour in the physical world. This study empirically examines, for the first\ntime, how well large language models (LLMs) can build a mental model of agents,\ntermed agent mental modelling, by reasoning about an agent's behaviour and its\neffect on states from agent interaction history. This research may unveil the\npotential of leveraging LLMs for elucidating RL agent behaviour, addressing a\nkey challenge in eXplainable reinforcement learning (XRL). To this end, we\npropose specific evaluation metrics and test them on selected RL task datasets\nof varying complexity, reporting findings on agent mental model establishment.\nOur results disclose that LLMs are not yet capable of fully mental modelling\nagents through inference alone without further innovations. This work thus\nprovides new insights into the capabilities and limitations of modern LLMs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "https://lukaswill.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2406.18505v1",
    "published_date": "2024-06-26 17:14:45 UTC",
    "updated_date": "2024-06-26 17:14:45 UTC"
  },
  {
    "arxiv_id": "2407.01464v1",
    "title": "Graph Neural Network as Computationally Efficient Emulator of Ice-sheet and Sea-level System Model (ISSM)",
    "authors": [
      "Younghyun Koo",
      "Maryam Rahnemoonfar"
    ],
    "abstract": "The Ice-sheet and Sea-level System Model (ISSM) provides solutions for Stokes\nequations relevant to ice sheet dynamics by employing finite element and fine\nmesh adaption. However, since its finite element method is compatible only with\nCentral Processing Units (CPU), the ISSM has limits on further economizing\ncomputational time. Thus, by taking advantage of Graphics Processing Units\n(GPUs), we design a graph convolutional network (GCN) as a fast emulator for\nISSM. The GCN is trained and tested using the 20-year transient ISSM\nsimulations in the Pine Island Glacier (PIG). The GCN reproduces ice thickness\nand velocity with a correlation coefficient greater than 0.998, outperforming\nthe traditional convolutional neural network (CNN). Additionally, GCN shows 34\ntimes faster computational speed than the CPU-based ISSM modeling. The\nGPU-based GCN emulator allows us to predict how the PIG will change in the\nfuture under different melting rate scenarios with high fidelity and much\nfaster computational time.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NA",
      "math.NA"
    ],
    "primary_category": "cs.LG",
    "comment": "5 pages, 4 figures, submitted to the 2024 IEEE International\n  Geoscience and Remote Sensing Symposium. arXiv admin note: text overlap with\n  arXiv:2402.05291",
    "pdf_url": "http://arxiv.org/pdf/2407.01464v1",
    "published_date": "2024-06-26 16:13:11 UTC",
    "updated_date": "2024-06-26 16:13:11 UTC"
  },
  {
    "arxiv_id": "2406.18460v1",
    "title": "Role-Play Zero-Shot Prompting with Large Language Models for Open-Domain Human-Machine Conversation",
    "authors": [
      "Ahmed Njifenjou",
      "Virgile Sucal",
      "Bassam Jabaian",
      "Fabrice Lefèvre"
    ],
    "abstract": "Recently, various methods have been proposed to create open-domain\nconversational agents with Large Language Models (LLMs). These models are able\nto answer user queries, but in a one-way Q&A format rather than a true\nconversation. Fine-tuning on particular datasets is the usual way to modify\ntheir style to increase conversational ability, but this is expensive and\nusually only available in a few languages. In this study, we explore role-play\nzero-shot prompting as an efficient and cost-effective solution for open-domain\nconversation, using capable multilingual LLMs (Beeching et al., 2023) trained\nto obey instructions. We design a prompting system that, when combined with an\ninstruction-following model - here Vicuna (Chiang et al., 2023) - produces\nconversational agents that match and even surpass fine-tuned models in human\nevaluation in French in two different tasks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "Updated version of a paper originally submitted at SIGDIAL 2023",
    "pdf_url": "http://arxiv.org/pdf/2406.18460v1",
    "published_date": "2024-06-26 16:10:53 UTC",
    "updated_date": "2024-06-26 16:10:53 UTC"
  },
  {
    "arxiv_id": "2406.18451v3",
    "title": "Detecting Brittle Decisions for Free: Leveraging Margin Consistency in Deep Robust Classifiers",
    "authors": [
      "Jonas Ngnawé",
      "Sabyasachi Sahoo",
      "Yann Pequignot",
      "Frédéric Precioso",
      "Christian Gagné"
    ],
    "abstract": "Despite extensive research on adversarial training strategies to improve\nrobustness, the decisions of even the most robust deep learning models can\nstill be quite sensitive to imperceptible perturbations, creating serious risks\nwhen deploying them for high-stakes real-world applications. While detecting\nsuch cases may be critical, evaluating a model's vulnerability at a\nper-instance level using adversarial attacks is computationally too intensive\nand unsuitable for real-time deployment scenarios. The input space margin is\nthe exact score to detect non-robust samples and is intractable for deep neural\nnetworks. This paper introduces the concept of margin consistency -- a property\nthat links the input space margins and the logit margins in robust models --\nfor efficient detection of vulnerable samples. First, we establish that margin\nconsistency is a necessary and sufficient condition to use a model's logit\nmargin as a score for identifying non-robust samples. Next, through\ncomprehensive empirical analysis of various robustly trained models on CIFAR10\nand CIFAR100 datasets, we show that they indicate high margin consistency with\na strong correlation between their input space margins and the logit margins.\nThen, we show that we can effectively and confidently use the logit margin to\ndetect brittle decisions with such models. Finally, we address cases where the\nmodel is not sufficiently margin-consistent by learning a pseudo-margin from\nthe feature representation. Our findings highlight the potential of leveraging\ndeep representations to assess adversarial vulnerability in deployment\nscenarios efficiently.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 6 figures, 2 tables. Version Update: Neurips Camera Ready",
    "pdf_url": "http://arxiv.org/pdf/2406.18451v3",
    "published_date": "2024-06-26 16:00:35 UTC",
    "updated_date": "2024-11-01 02:13:59 UTC"
  },
  {
    "arxiv_id": "2406.18450v2",
    "title": "Preference Elicitation for Offline Reinforcement Learning",
    "authors": [
      "Alizée Pace",
      "Bernhard Schölkopf",
      "Gunnar Rätsch",
      "Giorgia Ramponi"
    ],
    "abstract": "Applying reinforcement learning (RL) to real-world problems is often made\nchallenging by the inability to interact with the environment and the\ndifficulty of designing reward functions. Offline RL addresses the first\nchallenge by considering access to an offline dataset of environment\ninteractions labeled by the reward function. In contrast, Preference-based RL\ndoes not assume access to the reward function and learns it from preferences,\nbut typically requires an online interaction with the environment. We bridge\nthe gap between these frameworks by exploring efficient methods for acquiring\npreference feedback in a fully offline setup. We propose Sim-OPRL, an offline\npreference-based reinforcement learning algorithm, which leverages a learned\nenvironment model to elicit preference feedback on simulated rollouts. Drawing\non insights from both the offline RL and the preference-based RL literature,\nour algorithm employs a pessimistic approach for out-of-distribution data, and\nan optimistic approach for acquiring informative preferences about the optimal\npolicy. We provide theoretical guarantees regarding the sample complexity of\nour approach, dependent on how well the offline data covers the optimal policy.\nFinally, we demonstrate the empirical performance of Sim-OPRL in various\nenvironments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2406.18450v2",
    "published_date": "2024-06-26 15:59:13 UTC",
    "updated_date": "2025-02-28 08:36:00 UTC"
  },
  {
    "arxiv_id": "2406.18449v2",
    "title": "Cascading Large Language Models for Salient Event Graph Generation",
    "authors": [
      "Xingwei Tan",
      "Yuxiang Zhou",
      "Gabriele Pergola",
      "Yulan He"
    ],
    "abstract": "Generating event graphs from long documents is challenging due to the\ninherent complexity of multiple tasks involved such as detecting events,\nidentifying their relationships, and reconciling unstructured input with\nstructured graphs. Recent studies typically consider all events with equal\nimportance, failing to distinguish salient events crucial for understanding\nnarratives. This paper presents CALLMSAE, a CAscading Large Language Model\nframework for SAlient Event graph generation, which leverages the capabilities\nof LLMs and eliminates the need for costly human annotations. We first identify\nsalient events by prompting LLMs to generate summaries, from which salient\nevents are identified. Next, we develop an iterative code refinement prompting\nstrategy to generate event relation graphs, removing hallucinated relations and\nrecovering missing edges. Powered by CALLMSAE, we present \\textit{NYT-SEG}, a\nlarge-scale automatically annotated event graph dataset which can serve as\ndistant supervision signals. Fine-tuning contextualised graph generation models\non \\textit{NYT-SEG} outperforms the models trained on CAEVO data. Results on a\nhuman-annotated test set show that the proposed method generates salient and\nmore accurate graphs, outperforming competitive baselines.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "NAACL 2025 Camera-ready (9 + 14 pages)",
    "pdf_url": "http://arxiv.org/pdf/2406.18449v2",
    "published_date": "2024-06-26 15:53:54 UTC",
    "updated_date": "2025-02-08 17:23:18 UTC"
  },
  {
    "arxiv_id": "2406.18423v1",
    "title": "Graph Neural Networks for Emulation of Finite-Element Ice Dynamics in Greenland and Antarctic Ice Sheets",
    "authors": [
      "Younghyun Koo",
      "Maryam Rahnemoonfar"
    ],
    "abstract": "Although numerical models provide accurate solutions for ice sheet dynamics\nbased on physics laws, they accompany intensified computational demands to\nsolve partial differential equations. In recent years, convolutional neural\nnetworks (CNNs) have been widely used as statistical emulators for those\nnumerical models. However, since CNNs operate on regular grids, they cannot\nrepresent the refined meshes and computational efficiency of finite-element\nnumerical models. Therefore, instead of CNNs, this study adopts an equivariant\ngraph convolutional network (EGCN) as an emulator for the ice sheet dynamics\nmodeling. EGCN reproduces ice thickness and velocity changes in the Helheim\nGlacier, Greenland, and Pine Island Glacier, Antarctica, with 260 times and 44\ntimes faster computation time, respectively. Compared to the traditional CNN\nand graph convolutional network, EGCN shows outstanding accuracy in thickness\nprediction near fast ice streams by preserving the equivariance to the\ntranslation and rotation of graphs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NA",
      "math.NA"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages, 2 figures, submitted to the ICML 2024 Workshop on Machine\n  Learning for Earth System Modeling",
    "pdf_url": "http://arxiv.org/pdf/2406.18423v1",
    "published_date": "2024-06-26 15:18:49 UTC",
    "updated_date": "2024-06-26 15:18:49 UTC"
  },
  {
    "arxiv_id": "2406.18420v1",
    "title": "Mixture of Experts in a Mixture of RL settings",
    "authors": [
      "Timon Willi",
      "Johan Obando-Ceron",
      "Jakob Foerster",
      "Karolina Dziugaite",
      "Pablo Samuel Castro"
    ],
    "abstract": "Mixtures of Experts (MoEs) have gained prominence in (self-)supervised\nlearning due to their enhanced inference efficiency, adaptability to\ndistributed training, and modularity. Previous research has illustrated that\nMoEs can significantly boost Deep Reinforcement Learning (DRL) performance by\nexpanding the network's parameter count while reducing dormant neurons, thereby\nenhancing the model's learning capacity and ability to deal with\nnon-stationarity. In this work, we shed more light on MoEs' ability to deal\nwith non-stationarity and investigate MoEs in DRL settings with \"amplified\"\nnon-stationarity via multi-task training, providing further evidence that MoEs\nimprove learning capacity. In contrast to previous work, our multi-task results\nallow us to better understand the underlying causes for the beneficial effect\nof MoE in DRL training, the impact of the various MoE components, and insights\ninto how best to incorporate them in actor-critic-based DRL networks. Finally,\nwe also confirm results from previous work.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.18420v1",
    "published_date": "2024-06-26 15:15:15 UTC",
    "updated_date": "2024-06-26 15:15:15 UTC"
  },
  {
    "arxiv_id": "2406.18414v2",
    "title": "BiTrack: Bidirectional Offline 3D Multi-Object Tracking Using Camera-LiDAR Data",
    "authors": [
      "Kemiao Huang",
      "Yinqi Chen",
      "Meiying Zhang",
      "Qi Hao"
    ],
    "abstract": "Compared with real-time multi-object tracking (MOT), offline multi-object\ntracking (OMOT) has the advantages to perform 2D-3D detection fusion, erroneous\nlink correction, and full track optimization but has to deal with the\nchallenges from bounding box misalignment and track evaluation, editing, and\nrefinement. This paper proposes \"BiTrack\", a 3D OMOT framework that includes\nmodules of 2D-3D detection fusion, initial trajectory generation, and\nbidirectional trajectory re-optimization to achieve optimal tracking results\nfrom camera-LiDAR data. The novelty of this paper includes threefold: (1)\ndevelopment of a point-level object registration technique that employs a\ndensity-based similarity metric to achieve accurate fusion of 2D-3D detection\nresults; (2) development of a set of data association and track management\nskills that utilizes a vertex-based similarity metric as well as false alarm\nrejection and track recovery mechanisms to generate reliable bidirectional\nobject trajectories; (3) development of a trajectory re-optimization scheme\nthat re-organizes track fragments of different fidelities in a greedy fashion,\nas well as refines each trajectory with completion and smoothing techniques.\nThe experiment results on the KITTI dataset demonstrate that BiTrack achieves\nthe state-of-the-art performance for 3D OMOT tasks in terms of accuracy and\nefficiency.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.18414v2",
    "published_date": "2024-06-26 15:09:54 UTC",
    "updated_date": "2025-03-18 14:57:30 UTC"
  },
  {
    "arxiv_id": "2406.18406v2",
    "title": "IRCAN: Mitigating Knowledge Conflicts in LLM Generation via Identifying and Reweighting Context-Aware Neurons",
    "authors": [
      "Dan Shi",
      "Renren Jin",
      "Tianhao Shen",
      "Weilong Dong",
      "Xinwei Wu",
      "Deyi Xiong"
    ],
    "abstract": "It is widely acknowledged that large language models (LLMs) encode a vast\nreservoir of knowledge after being trained on mass data. Recent studies\ndisclose knowledge conflicts in LLM generation, wherein outdated or incorrect\nparametric knowledge (i.e., encoded knowledge) contradicts new knowledge\nprovided in the context. To mitigate such knowledge conflicts, we propose a\nnovel framework, IRCAN (Identifying and Reweighting Context-Aware Neurons) to\ncapitalize on neurons that are crucial in processing contextual cues.\nSpecifically, IRCAN first identifies neurons that significantly contribute to\ncontext processing, utilizing a context-aware attribution score derived from\nintegrated gradients. Subsequently, the identified context-aware neurons are\nstrengthened via reweighting. In doing so, we steer LLMs to generate\ncontext-sensitive outputs with respect to the new knowledge provided in the\ncontext. Extensive experiments conducted across a variety of models and tasks\ndemonstrate that IRCAN not only achieves remarkable improvements in handling\nknowledge conflicts but also offers a scalable, plug-and-play solution that can\nbe integrated seamlessly with existing models. Our codes are released at\nhttps://github.com/danshi777/IRCAN.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.18406v2",
    "published_date": "2024-06-26 14:57:38 UTC",
    "updated_date": "2024-11-14 10:55:14 UTC"
  },
  {
    "arxiv_id": "2406.19418v1",
    "title": "A Quantization-based Technique for Privacy Preserving Distributed Learning",
    "authors": [
      "Maurizio Colombo",
      "Rasool Asal",
      "Ernesto Damiani",
      "Lamees Mahmoud AlQassem",
      "Al Anoud Almemari",
      "Yousof Alhammadi"
    ],
    "abstract": "The massive deployment of Machine Learning (ML) models raises serious\nconcerns about data protection. Privacy-enhancing technologies (PETs) offer a\npromising first step, but hard challenges persist in achieving confidentiality\nand differential privacy in distributed learning. In this paper, we describe a\nnovel, regulation-compliant data protection technique for the distributed\ntraining of ML models, applicable throughout the ML life cycle regardless of\nthe underlying ML architecture. Designed from the data owner's perspective, our\nmethod protects both training data and ML model parameters by employing a\nprotocol based on a quantized multi-hash data representation Hash-Comb combined\nwith randomization. The hyper-parameters of our scheme can be shared using\nstandard Secure Multi-Party computation protocols. Our experimental results\ndemonstrate the robustness and accuracy-preserving properties of our approach.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.19418v1",
    "published_date": "2024-06-26 14:54:12 UTC",
    "updated_date": "2024-06-26 14:54:12 UTC"
  },
  {
    "arxiv_id": "2406.18394v5",
    "title": "AlphaForge: A Framework to Mine and Dynamically Combine Formulaic Alpha Factors",
    "authors": [
      "Hao Shi",
      "Weili Song",
      "Xinting Zhang",
      "Jiahe Shi",
      "Cuicui Luo",
      "Xiang Ao",
      "Hamid Arian",
      "Luis Seco"
    ],
    "abstract": "The complexity of financial data, characterized by its variability and low\nsignal-to-noise ratio, necessitates advanced methods in quantitative investment\nthat prioritize both performance and interpretability.Transitioning from early\nmanual extraction to genetic programming, the most advanced approach in the\nalpha factor mining domain currently employs reinforcement learning to mine a\nset of combination factors with fixed weights. However, the performance of\nresultant alpha factors exhibits inconsistency, and the inflexibility of fixed\nfactor weights proves insufficient in adapting to the dynamic nature of\nfinancial markets. To address this issue, this paper proposes a two-stage\nformulaic alpha generating framework AlphaForge, for alpha factor mining and\nfactor combination. This framework employs a generative-predictive neural\nnetwork to generate factors, leveraging the robust spatial exploration\ncapabilities inherent in deep learning while concurrently preserving diversity.\nThe combination model within the framework incorporates the temporal\nperformance of factors for selection and dynamically adjusts the weights\nassigned to each component alpha factor. Experiments conducted on real-world\ndatasets demonstrate that our proposed model outperforms contemporary\nbenchmarks in formulaic alpha factor mining. Furthermore, our model exhibits a\nnotable enhancement in portfolio returns within the realm of quantitative\ninvestment and real money investment.",
    "categories": [
      "q-fin.CP",
      "cs.AI"
    ],
    "primary_category": "q-fin.CP",
    "comment": "10 pages, 3 figures, Accepted by AAAI2025",
    "pdf_url": "http://arxiv.org/pdf/2406.18394v5",
    "published_date": "2024-06-26 14:34:37 UTC",
    "updated_date": "2024-12-12 08:28:17 UTC"
  },
  {
    "arxiv_id": "2406.18388v3",
    "title": "SAM: Semi-Active Mechanism for Extensible Continuum Manipulator and Real-time Hysteresis Compensation Control Algorithm",
    "authors": [
      "Junhyun Park",
      "Seonghyeok Jang",
      "Myeongbo Park",
      "Hyojae Park",
      "Jeonghyeon Yoon",
      "Minho Hwang"
    ],
    "abstract": "Cable-Driven Continuum Manipulators (CDCMs) enable scar-free procedures but\nface limitations in workspace and control accuracy due to hysteresis. We\nintroduce an extensible CDCM with a Semi-active Mechanism (SAM) and develop a\nreal-time hysteresis compensation control algorithm using a Temporal\nConvolutional Network (TCN) based on data collected from fiducial markers and\nRGBD sensing. Performance validation shows the proposed controller\nsignificantly reduces hysteresis by up to 69.5% in random trajectory tracking\ntest and approximately 26% in the box pointing task. The SAM mechanism enables\naccess to various lesions without damaging surrounding tissues. The proposed\ncontroller with TCN-based compensation effectively predicts hysteresis behavior\nand minimizes position and joint angle errors in real-time, which has the\npotential to enhance surgical task performance.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "22 pages, 19 figures, 9 tables",
    "pdf_url": "http://arxiv.org/pdf/2406.18388v3",
    "published_date": "2024-06-26 14:30:51 UTC",
    "updated_date": "2024-09-30 08:11:20 UTC"
  },
  {
    "arxiv_id": "2407.15022v1",
    "title": "Encouraging Responsible Use of Generative AI in Education: A Reward-Based Learning Approach",
    "authors": [
      "Aditi Singh",
      "Abul Ehtesham",
      "Saket Kumar",
      "Gaurav Kumar Gupta",
      "Tala Talaei Khoei"
    ],
    "abstract": "This research introduces an innovative mathematical learning approach that\nintegrates generative AI to cultivate a structured learning rather than quick\nsolution. Our method combines chatbot capabilities and generative AI to offer\ninteractive problem-solving exercises, enhancing learning through a stepby-step\napproach for varied problems, advocating for the responsible use of AI in\neducation. Our approach emphasizes that immediate answers from ChatGPT can\nimpede real learning. We introduce a reward-based system that requires students\nto solve mathematical problems effectively to receive the final answer. This\nencourages a progressive learning path from basic to complex problems,\nrewarding mastery with final solutions. The goal is to transition students from\nseeking quick fixes to engaging actively in a comprehensive learning\nexperience.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "9 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.15022v1",
    "published_date": "2024-06-26 14:27:24 UTC",
    "updated_date": "2024-06-26 14:27:24 UTC"
  },
  {
    "arxiv_id": "2406.18626v1",
    "title": "An LLM-based Knowledge Synthesis and Scientific Reasoning Framework for Biomedical Discovery",
    "authors": [
      "Oskar Wysocki",
      "Magdalena Wysocka",
      "Danilo Carvalho",
      "Alex Teodor Bogatu",
      "Danilo Miranda Gusicuma",
      "Maxime Delmas",
      "Harriet Unsworth",
      "Andre Freitas"
    ],
    "abstract": "We present BioLunar, developed using the Lunar framework, as a tool for\nsupporting biological analyses, with a particular emphasis on molecular-level\nevidence enrichment for biomarker discovery in oncology. The platform\nintegrates Large Language Models (LLMs) to facilitate complex scientific\nreasoning across distributed evidence spaces, enhancing the capability for\nharmonizing and reasoning over heterogeneous data sources. Demonstrating its\nutility in cancer research, BioLunar leverages modular design, reusable data\naccess and data analysis components, and a low-code user interface, enabling\nresearchers of all programming levels to construct LLM-enabled scientific\nworkflows. By facilitating automatic scientific discovery and inference from\nheterogeneous evidence, BioLunar exemplifies the potential of the integration\nbetween LLMs, specialised databases and biomedical tools to support\nexpert-level knowledge synthesis and discovery.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "q-bio.QM",
    "comment": "accepted for ACL 2024 System Demonstration Track",
    "pdf_url": "http://arxiv.org/pdf/2406.18626v1",
    "published_date": "2024-06-26 14:22:46 UTC",
    "updated_date": "2024-06-26 14:22:46 UTC"
  },
  {
    "arxiv_id": "2406.18379v2",
    "title": "MALSIGHT: Exploring Malicious Source Code and Benign Pseudocode for Iterative Binary Malware Summarization",
    "authors": [
      "Haolang Lu",
      "Hongrui Peng",
      "Guoshun Nan",
      "Jiaoyang Cui",
      "Cheng Wang",
      "Weifei Jin",
      "Songtao Wang",
      "Shengli Pan",
      "Xiaofeng Tao"
    ],
    "abstract": "Binary malware summarization aims to automatically generate human-readable\ndescriptions of malware behaviors from executable files, facilitating tasks\nlike malware cracking and detection. Previous methods based on Large Language\nModels (LLMs) have shown great promise. However, they still face significant\nissues, including poor usability, inaccurate explanations,and incomplete\nsummaries, primarily due to the obscure pseudocode structure and the lack of\nmalware training summaries. Further, calling relationships between functions,\nwhich involve the rich interactions within a binary malware, remain largely\nunderexplored. To this end, we propose MALSIGHT, a novel code summarization\nframework that can iteratively generate descriptions of binary malware by\nexploring malicious source code and benign pseudocode. Specifically, we\nconstruct the first malware summary dataset, MalS and MalP, using an LLM and\nmanually refine this dataset with human effort. At the training stage, we tune\nour proposed MalT5, a novel LLM-based code model, on the MalS and benign\npseudocode datasets. Then, at the test stage, we iteratively feed the\npseudocode functions into MalT5 to obtain the summary. Such a procedure\nfacilitates the understanding of pseudocode structure and captures the\nintricate interactions between functions, thereby benefiting summaries'\nusability, accuracy, and completeness. Additionally, we propose a novel\nevaluation benchmark, BLEURT-sum, to measure the quality of summaries.\nExperiments on three datasets show the effectiveness of the proposed MALSIGHT.\nNotably, our proposed MalT5, with only 0.77B parameters, delivers comparable\nperformance to much larger Code-Llama.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CR",
    "comment": "14 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.18379v2",
    "published_date": "2024-06-26 14:21:09 UTC",
    "updated_date": "2024-11-06 13:26:50 UTC"
  },
  {
    "arxiv_id": "2406.18370v1",
    "title": "Learning pure quantum states (almost) without regret",
    "authors": [
      "Josep Lumbreras",
      "Mikhail Terekhov",
      "Marco Tomamichel"
    ],
    "abstract": "We initiate the study of quantum state tomography with minimal regret. A\nlearner has sequential oracle access to an unknown pure quantum state, and in\neach round selects a pure probe state. Regret is incurred if the unknown state\nis measured orthogonal to this probe, and the learner's goal is to minimise the\nexpected cumulative regret over $T$ rounds. The challenge is to find a balance\nbetween the most informative measurements and measurements incurring minimal\nregret. We show that the cumulative regret scales as\n$\\Theta(\\operatorname{polylog} T)$ using a new tomography algorithm based on a\nmedian of means least squares estimator. This algorithm employs measurements\nbiased towards the unknown state and produces online estimates that are optimal\n(up to logarithmic terms) in the number of observed samples.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "quant-ph",
    "comment": "24 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.18370v1",
    "published_date": "2024-06-26 14:13:50 UTC",
    "updated_date": "2024-06-26 14:13:50 UTC"
  },
  {
    "arxiv_id": "2407.09541v1",
    "title": "MATE: Meet At The Embedding -- Connecting Images with Long Texts",
    "authors": [
      "Young Kyun Jang",
      "Junmo Kang",
      "Yong Jae Lee",
      "Donghyun Kim"
    ],
    "abstract": "While advancements in Vision Language Models (VLMs) have significantly\nimproved the alignment of visual and textual data, these models primarily focus\non aligning images with short descriptive captions. This focus limits their\nability to handle complex text interactions, particularly with longer texts\nsuch as lengthy captions or documents, which have not been extensively explored\nyet. In this paper, we introduce Meet At The Embedding (MATE), a novel approach\nthat combines the capabilities of VLMs with Large Language Models (LLMs) to\novercome this challenge without the need for additional image-long text pairs.\nSpecifically, we replace the text encoder of the VLM with a pretrained\nLLM-based encoder that excels in understanding long texts. To bridge the gap\nbetween VLM and LLM, MATE incorporates a projection module that is trained in a\nmulti-stage manner. It starts by aligning the embeddings from the VLM text\nencoder with those from the LLM using extensive text pairs. This module is then\nemployed to seamlessly align image embeddings closely with LLM embeddings. We\npropose two new cross-modal retrieval benchmarks to assess the task of\nconnecting images with long texts (lengthy captions / documents). Extensive\nexperimental results demonstrate that MATE effectively connects images with\nlong texts, uncovering diverse semantic relationships.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09541v1",
    "published_date": "2024-06-26 14:10:00 UTC",
    "updated_date": "2024-06-26 14:10:00 UTC"
  },
  {
    "arxiv_id": "2406.18364v1",
    "title": "Research on Information Extraction of LCSTS Dataset Based on an Improved BERTSum-LSTM Model",
    "authors": [
      "Yiming Chen",
      "Haobin Chen",
      "Simin Liu",
      "Yunyun Liu",
      "Fanhao Zhou",
      "Bing Wei"
    ],
    "abstract": "With the continuous advancement of artificial intelligence, natural language\nprocessing technology has become widely utilized in various fields. At the same\ntime, there are many challenges in creating Chinese news summaries. First of\nall, the semantics of Chinese news is complex, and the amount of information is\nenormous. Extracting critical information from Chinese news presents a\nsignificant challenge. Second, the news summary should be concise and clear,\nfocusing on the main content and avoiding redundancy. In addition, the\nparticularity of the Chinese language, such as polysemy, word segmentation,\netc., makes it challenging to generate Chinese news summaries. Based on the\nabove, this paper studies the information extraction method of the LCSTS\ndataset based on an improved BERTSum-LSTM model. We improve the BERTSum-LSTM\nmodel to make it perform better in generating Chinese news summaries. The\nexperimental results show that the proposed method has a good effect on\ncreating news summaries, which is of great importance to the construction of\nnews summaries.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "submitted to ICMIII 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.18364v1",
    "published_date": "2024-06-26 14:04:15 UTC",
    "updated_date": "2024-06-26 14:04:15 UTC"
  },
  {
    "arxiv_id": "2406.18361v3",
    "title": "Stable Diffusion Segmentation for Biomedical Images with Single-step Reverse Process",
    "authors": [
      "Tianyu Lin",
      "Zhiguang Chen",
      "Zhonghao Yan",
      "Weijiang Yu",
      "Fudan Zheng"
    ],
    "abstract": "Diffusion models have demonstrated their effectiveness across various\ngenerative tasks. However, when applied to medical image segmentation, these\nmodels encounter several challenges, including significant resource and time\nrequirements. They also necessitate a multi-step reverse process and multiple\nsamples to produce reliable predictions. To address these challenges, we\nintroduce the first latent diffusion segmentation model, named SDSeg, built\nupon stable diffusion (SD). SDSeg incorporates a straightforward latent\nestimation strategy to facilitate a single-step reverse process and utilizes\nlatent fusion concatenation to remove the necessity for multiple samples.\nExtensive experiments indicate that SDSeg surpasses existing state-of-the-art\nmethods on five benchmark datasets featuring diverse imaging modalities.\nRemarkably, SDSeg is capable of generating stable predictions with a solitary\nreverse step and sample, epitomizing the model's stability as implied by its\nname. The code is available at\nhttps://github.com/lin-tianyu/Stable-Diffusion-Seg",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at MICCAI 2024. Code and citation info see\n  https://github.com/lin-tianyu/Stable-Diffusion-Seg",
    "pdf_url": "http://arxiv.org/pdf/2406.18361v3",
    "published_date": "2024-06-26 14:01:07 UTC",
    "updated_date": "2024-07-09 17:25:27 UTC"
  },
  {
    "arxiv_id": "2406.18354v1",
    "title": "Kolmogorov-Arnold Graph Neural Networks",
    "authors": [
      "Gianluca De Carlo",
      "Andrea Mastropietro",
      "Aris Anagnostopoulos"
    ],
    "abstract": "Graph neural networks (GNNs) excel in learning from network-like data but\noften lack interpretability, making their application challenging in domains\nrequiring transparent decision-making. We propose the Graph Kolmogorov-Arnold\nNetwork (GKAN), a novel GNN model leveraging spline-based activation functions\non edges to enhance both accuracy and interpretability. Our experiments on five\nbenchmark datasets demonstrate that GKAN outperforms state-of-the-art GNN\nmodels in node classification, link prediction, and graph classification tasks.\nIn addition to the improved accuracy, GKAN's design inherently provides clear\ninsights into the model's decision-making process, eliminating the need for\npost-hoc explainability techniques. This paper discusses the methodology,\nperformance, and interpretability of GKAN, highlighting its potential for\napplications in domains where interpretability is crucial.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "7 pages, 4 figures, under review",
    "pdf_url": "http://arxiv.org/pdf/2406.18354v1",
    "published_date": "2024-06-26 13:54:59 UTC",
    "updated_date": "2024-06-26 13:54:59 UTC"
  },
  {
    "arxiv_id": "2406.18351v2",
    "title": "Reinforcement Learning with Intrinsically Motivated Feedback Graph for Lost-sales Inventory Control",
    "authors": [
      "Zifan Liu",
      "Xinran Li",
      "Shibo Chen",
      "Gen Li",
      "Jiashuo Jiang",
      "Jun Zhang"
    ],
    "abstract": "Reinforcement learning (RL) has proven to be well-performed and\ngeneral-purpose in the inventory control (IC). However, further improvement of\nRL algorithms in the IC domain is impeded due to two limitations of online\nexperience. First, online experience is expensive to acquire in real-world\napplications. With the low sample efficiency nature of RL algorithms, it would\ntake extensive time to train the RL policy to convergence. Second, online\nexperience may not reflect the true demand due to the lost sales phenomenon\ntypical in IC, which makes the learning process more challenging. To address\nthe above challenges, we propose a decision framework that combines\nreinforcement learning with feedback graph (RLFG) and intrinsically motivated\nexploration (IME) to boost sample efficiency. In particular, we first take\nadvantage of the inherent properties of lost-sales IC problems and design the\nfeedback graph (FG) specially for lost-sales IC problems to generate abundant\nside experiences aid RL updates. Then we conduct a rigorous theoretical\nanalysis of how the designed FG reduces the sample complexity of RL methods.\nBased on the theoretical insights, we design an intrinsic reward to direct the\nRL agent to explore to the state-action space with more side experiences,\nfurther exploiting FG's power. Experimental results demonstrate that our method\ngreatly improves the sample efficiency of applying RL in IC. Our code is\navailable at https://anonymous.4open.science/r/RLIMFG4IC-811D/",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.18351v2",
    "published_date": "2024-06-26 13:52:47 UTC",
    "updated_date": "2025-02-17 09:34:52 UTC"
  },
  {
    "arxiv_id": "2407.11013v2",
    "title": "Quantum-tunnelling deep neural network for optical illusion recognition",
    "authors": [
      "Ivan S. Maksymov"
    ],
    "abstract": "The discovery of the quantum tunnelling (QT) effect -- the transmission of\nparticles through a high potential barrier -- was one of the most impressive\nachievements of quantum mechanics made in the 1920s. Responding to the\ncontemporary challenges, I introduce a deep neural network (DNN) architecture\nthat processes information using the effect of QT. I demonstrate the ability of\nQT-DNN to recognise optical illusions like a human. Tasking QT-DNN to simulate\nhuman perception of the Necker cube and Rubin's vase, I provide arguments in\nfavour of the superiority of QT-based activation functions over the activation\nfunctions optimised for modern applications in machine vision, also showing\nthat, at the fundamental level, QT-DNN is closely related to biology-inspired\nDNNs and models based on the principles of quantum information processing.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.HC",
      "cs.NE",
      "physics.soc-ph",
      "quant-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "A part of the special collection \"Neuromorphic Technologies for Novel\n  Hardware AI\"",
    "pdf_url": "http://arxiv.org/pdf/2407.11013v2",
    "published_date": "2024-06-26 13:49:07 UTC",
    "updated_date": "2025-02-22 01:39:09 UTC"
  },
  {
    "arxiv_id": "2406.18346v1",
    "title": "AI Alignment through Reinforcement Learning from Human Feedback? Contradictions and Limitations",
    "authors": [
      "Adam Dahlgren Lindström",
      "Leila Methnani",
      "Lea Krause",
      "Petter Ericson",
      "Íñigo Martínez de Rituerto de Troya",
      "Dimitri Coelho Mollo",
      "Roel Dobbe"
    ],
    "abstract": "This paper critically evaluates the attempts to align Artificial Intelligence\n(AI) systems, especially Large Language Models (LLMs), with human values and\nintentions through Reinforcement Learning from Feedback (RLxF) methods,\ninvolving either human feedback (RLHF) or AI feedback (RLAIF). Specifically, we\nshow the shortcomings of the broadly pursued alignment goals of honesty,\nharmlessness, and helpfulness. Through a multidisciplinary sociotechnical\ncritique, we examine both the theoretical underpinnings and practical\nimplementations of RLxF techniques, revealing significant limitations in their\napproach to capturing the complexities of human ethics and contributing to AI\nsafety. We highlight tensions and contradictions inherent in the goals of RLxF.\nIn addition, we discuss ethically-relevant issues that tend to be neglected in\ndiscussions about alignment and RLxF, among which the trade-offs between\nuser-friendliness and deception, flexibility and interpretability, and system\nsafety. We conclude by urging researchers and practitioners alike to critically\nassess the sociotechnical ramifications of RLxF, advocating for a more nuanced\nand reflective approach to its application in AI development.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "12 pages, 1 table, to be submitted",
    "pdf_url": "http://arxiv.org/pdf/2406.18346v1",
    "published_date": "2024-06-26 13:42:13 UTC",
    "updated_date": "2024-06-26 13:42:13 UTC"
  },
  {
    "arxiv_id": "2407.10329v1",
    "title": "Generative Discrimination: What Happens When Generative AI Exhibits Bias, and What Can Be Done About It",
    "authors": [
      "Philipp Hacker"
    ],
    "abstract": "As generative Artificial Intelligence (genAI) technologies proliferate across\nsectors, they offer significant benefits but also risk exacerbating\ndiscrimination. This chapter explores how genAI intersects with\nnon-discrimination laws, identifying shortcomings and suggesting improvements.\nIt highlights two main types of discriminatory outputs: (i) demeaning and\nabusive content and (ii) subtler biases due to inadequate representation of\nprotected groups, which may not be overtly discriminatory in individual cases\nbut have cumulative discriminatory effects. For example, genAI systems may\npredominantly depict white men when asked for images of people in important\njobs.\n  This chapter examines these issues, categorizing problematic outputs into\nthree legal categories: discriminatory content; harassment; and legally hard\ncases like unbalanced content, harmful stereotypes or misclassification. It\nargues for holding genAI providers and deployers liable for discriminatory\noutputs and highlights the inadequacy of traditional legal frameworks to\naddress genAI-specific issues. The chapter suggests updating EU laws, including\nthe AI Act, to mitigate biases in training and input data, mandating testing\nand auditing, and evolving legislation to enforce standards for bias mitigation\nand inclusivity as technology advances.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "arXiv admin comment: This version has been removed by arXiv\n  administrators as the submitter did not have the rights to agree to the\n  license at the time of submission",
    "pdf_url": "http://arxiv.org/pdf/2407.10329v1",
    "published_date": "2024-06-26 13:32:58 UTC",
    "updated_date": "2024-06-26 13:32:58 UTC"
  },
  {
    "arxiv_id": "2406.18625v1",
    "title": "Automatic Prediction of Amyotrophic Lateral Sclerosis Progression using Longitudinal Speech Transformer",
    "authors": [
      "Liming Wang",
      "Yuan Gong",
      "Nauman Dawalatabad",
      "Marco Vilela",
      "Katerina Placek",
      "Brian Tracey",
      "Yishu Gong",
      "Alan Premasiri",
      "Fernando Vieira",
      "James Glass"
    ],
    "abstract": "Automatic prediction of amyotrophic lateral sclerosis (ALS) disease\nprogression provides a more efficient and objective alternative than manual\napproaches. We propose ALS longitudinal speech transformer (ALST), a neural\nnetwork-based automatic predictor of ALS disease progression from longitudinal\nspeech recordings of ALS patients. By taking advantage of high-quality\npretrained speech features and longitudinal information in the recordings, our\nbest model achieves 91.0\\% AUC, improving upon the previous best model by 5.6\\%\nrelative on the ALS TDI dataset. Careful analysis reveals that ALST is capable\nof fine-grained and interpretable predictions of ALS progression, especially\nfor distinguishing between rarer and more severe cases. Code is publicly\navailable.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.18625v1",
    "published_date": "2024-06-26 13:28:24 UTC",
    "updated_date": "2024-06-26 13:28:24 UTC"
  },
  {
    "arxiv_id": "2406.18333v1",
    "title": "Continuous Sign Language Recognition Using Intra-inter Gloss Attention",
    "authors": [
      "Hossein Ranjbar",
      "Alireza Taheri"
    ],
    "abstract": "Many continuous sign language recognition (CSLR) studies adopt\ntransformer-based architectures for sequence modeling due to their powerful\ncapacity for capturing global contexts. Nevertheless, vanilla self-attention,\nwhich serves as the core module of the transformer, calculates a weighted\naverage over all time steps; therefore, the local temporal semantics of sign\nvideos may not be fully exploited. In this study, we introduce a novel module\nin sign language recognition studies, called intra-inter gloss attention\nmodule, to leverage the relationships among frames within glosses and the\nsemantic and grammatical dependencies between glosses in the video. In the\nintra-gloss attention module, the video is divided into equally sized chunks\nand a self-attention mechanism is applied within each chunk. This localized\nself-attention significantly reduces complexity and eliminates noise introduced\nby considering non-relative frames. In the inter-gloss attention module, we\nfirst aggregate the chunk-level features within each gloss chunk by average\npooling along the temporal dimension. Subsequently, multi-head self-attention\nis applied to all chunk-level features. Given the non-significance of the\nsigner-environment interaction, we utilize segmentation to remove the\nbackground of the videos. This enables the proposed model to direct its focus\ntoward the signer. Experimental results on the PHOENIX-2014 benchmark dataset\ndemonstrate that our method can effectively extract sign language features in\nan end-to-end manner without any prior knowledge, improve the accuracy of CSLR,\nand achieve the word error rate (WER) of 20.4 on the test set which is a\ncompetitive result compare to the state-of-the-art which uses additional\nsupervisions.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.18333v1",
    "published_date": "2024-06-26 13:21:08 UTC",
    "updated_date": "2024-06-26 13:21:08 UTC"
  },
  {
    "arxiv_id": "2406.18326v2",
    "title": "PaCoST: Paired Confidence Significance Testing for Benchmark Contamination Detection in Large Language Models",
    "authors": [
      "Huixuan Zhang",
      "Yun Lin",
      "Xiaojun Wan"
    ],
    "abstract": "Large language models (LLMs) are known to be trained on vast amounts of data,\nwhich may unintentionally or intentionally include data from commonly used\nbenchmarks. This inclusion can lead to cheatingly high scores on model\nleaderboards, yet result in disappointing performance in real-world\napplications. To address this benchmark contamination problem, we first propose\na set of requirements that practical contamination detection methods should\nfollow. Following these proposed requirements, we introduce PaCoST, a Paired\nConfidence Significance Testing to effectively detect benchmark contamination\nin LLMs. Our method constructs a counterpart for each piece of data with the\nsame distribution, and performs statistical analysis of the corresponding\nconfidence to test whether the model is significantly more confident under the\noriginal benchmark. We validate the effectiveness of PaCoST and apply it on\npopular open-source models and benchmarks. We find that almost all models and\nbenchmarks we tested are suspected contaminated more or less. We finally call\nfor new LLM evaluation methods.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by EMNLP 2024 Findings",
    "pdf_url": "http://arxiv.org/pdf/2406.18326v2",
    "published_date": "2024-06-26 13:12:40 UTC",
    "updated_date": "2025-03-18 08:57:39 UTC"
  },
  {
    "arxiv_id": "2406.18321v1",
    "title": "MathOdyssey: Benchmarking Mathematical Problem-Solving Skills in Large Language Models Using Odyssey Math Data",
    "authors": [
      "Meng Fang",
      "Xiangpeng Wan",
      "Fei Lu",
      "Fei Xing",
      "Kai Zou"
    ],
    "abstract": "Large language models (LLMs) have significantly advanced natural language\nunderstanding and demonstrated strong problem-solving abilities. Despite these\nsuccesses, most LLMs still struggle with solving mathematical problems due to\nthe intricate reasoning required. This paper investigates the mathematical\nproblem-solving capabilities of LLMs using the newly developed \"MathOdyssey\"\ndataset. The dataset includes diverse mathematical problems at high school and\nuniversity levels, created by experts from notable institutions to rigorously\ntest LLMs in advanced problem-solving scenarios and cover a wider range of\nsubject areas. By providing the MathOdyssey dataset as a resource to the AI\ncommunity, we aim to contribute to the understanding and improvement of AI\ncapabilities in complex mathematical problem-solving. We conduct benchmarking\non open-source models, such as Llama-3 and DBRX-Instruct, and closed-source\nmodels from the GPT series and Gemini models. Our results indicate that while\nLLMs perform well on routine and moderately difficult tasks, they face\nsignificant challenges with Olympiad-level problems and complex\nuniversity-level questions. Our analysis shows a narrowing performance gap\nbetween open-source and closed-source models, yet substantial challenges\nremain, particularly with the most demanding problems. This study highlights\nthe ongoing need for research to enhance the mathematical reasoning of LLMs.\nThe dataset, results, and code are publicly available.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.18321v1",
    "published_date": "2024-06-26 13:02:35 UTC",
    "updated_date": "2024-06-26 13:02:35 UTC"
  },
  {
    "arxiv_id": "2406.18312v4",
    "title": "AI-native Memory: A Pathway from LLMs Towards AGI",
    "authors": [
      "Jingbo Shang",
      "Zai Zheng",
      "Jiale Wei",
      "Xiang Ying",
      "Felix Tao",
      "Mindverse Team"
    ],
    "abstract": "Large language models (LLMs) have demonstrated the world with the sparks of\nartificial general intelligence (AGI). One opinion, especially from some\nstartups working on LLMs, argues that an LLM with nearly unlimited context\nlength can realize AGI. However, they might be too optimistic about the\nlong-context capability of (existing) LLMs -- (1) Recent literature has shown\nthat their effective context length is significantly smaller than their claimed\ncontext length; and (2) Our reasoning-in-a-haystack experiments further\ndemonstrate that simultaneously finding the relevant information from a long\ncontext and conducting (simple) reasoning is nearly impossible. In this paper,\nwe envision a pathway from LLMs to AGI through the integration of\n\\emph{memory}. We believe that AGI should be a system where LLMs serve as core\nprocessors. In addition to raw data, the memory in this system would store a\nlarge number of important conclusions derived from reasoning processes.\nCompared with retrieval-augmented generation (RAG) that merely processing raw\ndata, this approach not only connects semantically related information closer,\nbut also simplifies complex inferences at the time of querying. As an\nintermediate stage, the memory will likely be in the form of natural language\ndescriptions, which can be directly consumed by users too. Ultimately, every\nagent/person should have its own large personal model, a deep neural network\nmodel (thus \\emph{AI-native}) that parameterizes and compresses all types of\nmemory, even the ones cannot be described by natural languages. Finally, we\ndiscuss the significant potential of AI-native memory as the transformative\ninfrastructure for (proactive) engagement, personalization, distribution, and\nsocial in the AGI era, as well as the incurred privacy and security challenges\nwith preliminary solutions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.18312v4",
    "published_date": "2024-06-26 12:51:37 UTC",
    "updated_date": "2024-08-28 08:07:49 UTC"
  },
  {
    "arxiv_id": "2406.18305v1",
    "title": "S3: A Simple Strong Sample-effective Multimodal Dialog System",
    "authors": [
      "Elisei Rykov",
      "Egor Malkershin",
      "Alexander Panchenko"
    ],
    "abstract": "In this work, we present a conceptually simple yet powerful baseline for the\nmultimodal dialog task, an S3 model, that achieves near state-of-the-art\nresults on two compelling leaderboards: MMMU and AI Journey Contest 2023. The\nsystem is based on a pre-trained large language model, pre-trained modality\nencoders for image and audio, and a trainable modality projector. The proposed\neffective data mixture for training such an architecture demonstrates that a\nmultimodal model based on a strong language model and trained on a small amount\nof multimodal data can perform efficiently in the task of multimodal dialog.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.18305v1",
    "published_date": "2024-06-26 12:45:43 UTC",
    "updated_date": "2024-06-26 12:45:43 UTC"
  },
  {
    "arxiv_id": "2407.12013v2",
    "title": "Dating ancient manuscripts using radiocarbon and AI-based writing style analysis",
    "authors": [
      "Mladen Popović",
      "Maruf A. Dhali",
      "Lambert Schomaker",
      "Johannes van der Plicht",
      "Kaare Lund Rasmussen",
      "Jacopo La Nasa",
      "Ilaria Degano",
      "Maria Perla Colombini",
      "Eibert Tigchelaar"
    ],
    "abstract": "Determining the chronology of ancient handwritten manuscripts is essential\nfor reconstructing the evolution of ideas. For the Dead Sea Scrolls, this is\nparticularly important. However, there is an almost complete lack of\ndate-bearing manuscripts evenly distributed across the timeline and written in\nsimilar scripts available for palaeographic comparison. Here, we present Enoch,\na state-of-the-art AI-based date-prediction model, trained on the basis of new\nradiocarbon-dated samples of the scrolls. Enoch uses established\nhandwriting-style descriptors and applies Bayesian ridge regression. The\nchallenge of this study is that the number of radiocarbon-dated manuscripts is\nsmall, while current machine learning requires an abundance of training data.\nWe show that by using combined angular and allographic writing style feature\nvectors and applying Bayesian ridge regression, Enoch could predict the\nradiocarbon-based dates from style, supported by leave-one-out validation, with\nvaried MAEs of 27.9 to 30.7 years relative to the radiocarbon dating. Enoch was\nthen used to estimate the dates of 135 unseen manuscripts, revealing that 79\nper cent of the samples were considered 'realistic' upon palaeographic post-hoc\nevaluation. We present a new chronology of the scrolls. The radiocarbon ranges\nand Enoch's style-based predictions are often older than the traditionally\nassumed palaeographic estimates. In the range of 300-50 BCE, Enoch's date\nprediction provides an improved granularity. The study is in line with current\ndevelopments in multimodal machine-learning techniques, and the methods can be\nused for date prediction in other partially-dated manuscript collections. This\nresearch shows how Enoch's quantitative, probability-based approach can be a\ntool for palaeographers and historians, re-dating ancient Jewish key texts and\ncontributing to current debates on Jewish and Christian origins.",
    "categories": [
      "cs.DL",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.DL",
    "comment": "16 pages of main article, 103 pages of supplementary materials; the\n  first version of this article is originally prepared in July 2023 after the\n  completion of all the experiments",
    "pdf_url": "http://arxiv.org/pdf/2407.12013v2",
    "published_date": "2024-06-26 12:33:34 UTC",
    "updated_date": "2024-10-18 08:57:30 UTC"
  },
  {
    "arxiv_id": "2406.18293v2",
    "title": "Combining Automated Optimisation of Hyperparameters and Reward Shape",
    "authors": [
      "Julian Dierkes",
      "Emma Cramer",
      "Holger H. Hoos",
      "Sebastian Trimpe"
    ],
    "abstract": "There has been significant progress in deep reinforcement learning (RL) in\nrecent years. Nevertheless, finding suitable hyperparameter configurations and\nreward functions remains challenging even for experts, and performance heavily\nrelies on these design choices. Also, most RL research is conducted on known\nbenchmarks where knowledge about these choices already exists. However, novel\npractical applications often pose complex tasks for which no prior knowledge\nabout good hyperparameters and reward functions is available, thus\nnecessitating their derivation from scratch. Prior work has examined\nautomatically tuning either hyperparameters or reward functions individually.\nWe demonstrate empirically that an RL algorithm's hyperparameter configurations\nand reward function are often mutually dependent, meaning neither can be fully\noptimised without appropriate values for the other. We then propose a\nmethodology for the combined optimisation of hyperparameters and the reward\nfunction. Furthermore, we include a variance penalty as an optimisation\nobjective to improve the stability of learned policies. We conducted extensive\nexperiments using Proximal Policy Optimisation and Soft Actor-Critic on four\nenvironments. Our results show that combined optimisation significantly\nimproves over baseline performance in half of the environments and achieves\ncompetitive performance in the others, with only a minor increase in\ncomputational costs. This suggests that combined optimisation should be best\npractice.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published in the Reinforcement Learning Journal 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.18293v2",
    "published_date": "2024-06-26 12:23:54 UTC",
    "updated_date": "2024-10-09 14:24:24 UTC"
  },
  {
    "arxiv_id": "2407.01601v2",
    "title": "Unveiling and Controlling Anomalous Attention Distribution in Transformers",
    "authors": [
      "Ruiqing Yan",
      "Xingbo Du",
      "Haoyu Deng",
      "Linghan Zheng",
      "Qiuzhuang Sun",
      "Jifang Hu",
      "Yuhang Shao",
      "Penghao Jiang",
      "Jinrong Jiang",
      "Lian Zhao"
    ],
    "abstract": "With the advent of large models based on the Transformer architecture,\nresearchers have observed an anomalous phenomenon in the Attention\nmechanism--there is a very high attention on the first element, which is\nprevalent across Transformer-based models. It is crucial to understand it for\nthe development of techniques focusing on attention distribution, such as\nKey-Value (KV) Cache compression and infinite extrapolation; however, the\nlatent cause leaves to be unknown. In this paper, we analyze such a phenomenon\nfrom the perspective of waiver phenomenon, which involves reducing the internal\nvalues of certain elements in the sequence, allowing them to absorb excess\nattention without affecting their contribution to information. In specific\nmodels, due to differences in positional encoding and attention patterns, we\nhave found that the selection of waiver elements by the model can be\ncategorized into two methods: positional-encoding-based and\nfeature-distribution-within-elements-based.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01601v2",
    "published_date": "2024-06-26 11:53:35 UTC",
    "updated_date": "2024-07-03 16:19:59 UTC"
  },
  {
    "arxiv_id": "2406.18259v1",
    "title": "Detecting Machine-Generated Texts: Not Just \"AI vs Humans\" and Explainability is Complicated",
    "authors": [
      "Jiazhou Ji",
      "Ruizhe Li",
      "Shujun Li",
      "Jie Guo",
      "Weidong Qiu",
      "Zheng Huang",
      "Chiyu Chen",
      "Xiaoyu Jiang",
      "Xinru Lu"
    ],
    "abstract": "As LLMs rapidly advance, increasing concerns arise regarding risks about\nactual authorship of texts we see online and in real world. The task of\ndistinguishing LLM-authored texts is complicated by the nuanced and overlapping\nbehaviors of both machines and humans. In this paper, we challenge the current\npractice of considering LLM-generated text detection a binary classification\ntask of differentiating human from AI. Instead, we introduce a novel ternary\ntext classification scheme, adding an \"undecided\" category for texts that could\nbe attributed to either source, and we show that this new category is crucial\nto understand how to make the detection result more explainable to lay users.\nThis research shifts the paradigm from merely classifying to explaining\nmachine-generated texts, emphasizing need for detectors to provide clear and\nunderstandable explanations to users. Our study involves creating four new\ndatasets comprised of texts from various LLMs and human authors. Based on new\ndatasets, we performed binary classification tests to ascertain the most\neffective SOTA detection methods and identified SOTA LLMs capable of producing\nharder-to-detect texts. We constructed a new dataset of texts generated by two\ntop-performing LLMs and human authors, and asked three human annotators to\nproduce ternary labels with explanation notes. This dataset was used to\ninvestigate how three top-performing SOTA detectors behave in new ternary\nclassification context. Our results highlight why \"undecided\" category is much\nneeded from the viewpoint of explainability. Additionally, we conducted an\nanalysis of explainability of the three best-performing detectors and the\nexplanation notes of the human annotators, revealing insights about the\ncomplexity of explainable detection of machine-generated texts. Finally, we\npropose guidelines for developing future detection systems with improved\nexplanatory power.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "19 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.18259v1",
    "published_date": "2024-06-26 11:11:47 UTC",
    "updated_date": "2024-06-26 11:11:47 UTC"
  },
  {
    "arxiv_id": "2406.18254v1",
    "title": "Improving the Consistency in Cross-Lingual Cross-Modal Retrieval with 1-to-K Contrastive Learning",
    "authors": [
      "Zhijie Nie",
      "Richong Zhang",
      "Zhangchi Feng",
      "Hailang Huang",
      "Xudong Liu"
    ],
    "abstract": "Cross-lingual Cross-modal Retrieval (CCR) is an essential task in web search,\nwhich aims to break the barriers between modality and language simultaneously\nand achieves image-text retrieval in the multi-lingual scenario with a single\nmodel. In recent years, excellent progress has been made based on cross-lingual\ncross-modal pre-training; particularly, the methods based on contrastive\nlearning on large-scale data have significantly improved retrieval tasks.\nHowever, these methods directly follow the existing pre-training methods in the\ncross-lingual or cross-modal domain, leading to two problems of inconsistency\nin CCR: The methods with cross-lingual style suffer from the intra-modal error\npropagation, resulting in inconsistent recall performance across languages in\nthe whole dataset. The methods with cross-modal style suffer from the\ninter-modal optimization direction bias, resulting in inconsistent rank across\nlanguages within each instance, which cannot be reflected by Recall@K. To solve\nthese problems, we propose a simple but effective 1-to-K contrastive learning\nmethod, which treats each language equally and eliminates error propagation and\noptimization bias. In addition, we propose a new evaluation metric, Mean Rank\nVariance (MRV), to reflect the rank inconsistency across languages within each\ninstance. Extensive experiments on four CCR datasets show that our method\nimproves both recall rates and MRV with smaller-scale pre-trained data,\nachieving the new state-of-art.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted by KDD 2024 Research Track",
    "pdf_url": "http://arxiv.org/pdf/2406.18254v1",
    "published_date": "2024-06-26 11:04:25 UTC",
    "updated_date": "2024-06-26 11:04:25 UTC"
  },
  {
    "arxiv_id": "2406.18239v1",
    "title": "Zero-shot prompt-based classification: topic labeling in times of foundation models in German Tweets",
    "authors": [
      "Simon Münker",
      "Kai Kugler",
      "Achim Rettinger"
    ],
    "abstract": "Filtering and annotating textual data are routine tasks in many areas, like\nsocial media or news analytics. Automating these tasks allows to scale the\nanalyses wrt. speed and breadth of content covered and decreases the manual\neffort required. Due to technical advancements in Natural Language Processing,\nspecifically the success of large foundation models, a new tool for automating\nsuch annotation processes by using a text-to-text interface given written\nguidelines without providing training samples has become available.\n  In this work, we assess these advancements in-the-wild by empirically testing\nthem in an annotation task on German Twitter data about social and political\nEuropean crises. We compare the prompt-based results with our human annotation\nand preceding classification approaches, including Naive Bayes and a BERT-based\nfine-tuning/domain adaptation pipeline. Our results show that the prompt-based\napproach - despite being limited by local computation resources during the\nmodel selection - is comparable with the fine-tuned BERT but without any\nannotated training data. Our findings emphasize the ongoing paradigm shift in\nthe NLP landscape, i.e., the unification of downstream tasks and elimination of\nthe need for pre-labeled training data.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 2 tables, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2406.18239v1",
    "published_date": "2024-06-26 10:44:02 UTC",
    "updated_date": "2024-06-26 10:44:02 UTC"
  },
  {
    "arxiv_id": "2406.18237v1",
    "title": "PlaMo: Plan and Move in Rich 3D Physical Environments",
    "authors": [
      "Assaf Hallak",
      "Gal Dalal",
      "Chen Tessler",
      "Kelly Guo",
      "Shie Mannor",
      "Gal Chechik"
    ],
    "abstract": "Controlling humanoids in complex physically simulated worlds is a\nlong-standing challenge with numerous applications in gaming, simulation, and\nvisual content creation. In our setup, given a rich and complex 3D scene, the\nuser provides a list of instructions composed of target locations and\nlocomotion types. To solve this task we present PlaMo, a scene-aware path\nplanner and a robust physics-based controller. The path planner produces a\nsequence of motion paths, considering the various limitations the scene imposes\non the motion, such as location, height, and speed. Complementing the planner,\nour control policy generates rich and realistic physical motion adhering to the\nplan. We demonstrate how the combination of both modules enables traversing\ncomplex landscapes in diverse forms while responding to real-time changes in\nthe environment. Video: https://youtu.be/wWlqSQlRZ9M .",
    "categories": [
      "cs.AI",
      "cs.GR",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.18237v1",
    "published_date": "2024-06-26 10:41:07 UTC",
    "updated_date": "2024-06-26 10:41:07 UTC"
  },
  {
    "arxiv_id": "2407.09539v1",
    "title": "Classification of Inkjet Printers based on Droplet Statistics",
    "authors": [
      "Patrick Takenaka",
      "Manuel Eberhardinger",
      "Daniel Grießhaber",
      "Johannes Maucher"
    ],
    "abstract": "Knowing the printer model used to print a given document may provide a\ncrucial lead towards identifying counterfeits or conversely verifying the\nvalidity of a real document. Inkjet printers produce probabilistic droplet\npatterns that appear to be distinct for each printer model and as such we\ninvestigate the utilization of droplet characteristics including frequency\ndomain features extracted from printed document scans for the classification of\nthe underlying printer model. We collect and publish a dataset of high\nresolution document scans and show that our extracted features are informative\nenough to enable a neural network to distinguish not only the printer\nmanufacturer, but also individual printer models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "to be published in 2024 International Joint Conference on Neural\n  Networks (IJCNN)",
    "pdf_url": "http://arxiv.org/pdf/2407.09539v1",
    "published_date": "2024-06-26 10:20:01 UTC",
    "updated_date": "2024-06-26 10:20:01 UTC"
  },
  {
    "arxiv_id": "2406.18221v3",
    "title": "Enhancing Data Privacy in Large Language Models through Private Association Editing",
    "authors": [
      "Davide Venditti",
      "Elena Sofia Ruzzetti",
      "Giancarlo A. Xompero",
      "Cristina Giannone",
      "Andrea Favalli",
      "Raniero Romagnoli",
      "Fabio Massimo Zanzotto"
    ],
    "abstract": "Large language models (LLMs) require a significant redesign in solutions to\npreserve privacy in data-intensive applications due to their text-generation\ncapabilities. Indeed, LLMs tend to memorize and emit private information when\nmaliciously prompted. In this paper, we introduce Private Association Editing\n(PAE) as a novel defense approach for private data leakage. PAE is designed to\neffectively remove Personally Identifiable Information (PII) without retraining\nthe model. Experimental results demonstrate the effectiveness of PAE with\nrespect to alternative baseline methods. We believe PAE will serve as a\ncritical tool in the ongoing effort to protect data privacy in LLMs,\nencouraging the development of safer models for real-world applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.18221v3",
    "published_date": "2024-06-26 10:08:47 UTC",
    "updated_date": "2024-10-16 13:31:05 UTC"
  },
  {
    "arxiv_id": "2406.18220v1",
    "title": "Guiding Video Prediction with Explicit Procedural Knowledge",
    "authors": [
      "Patrick Takenaka",
      "Johannes Maucher",
      "Marco F. Huber"
    ],
    "abstract": "We propose a general way to integrate procedural knowledge of a domain into\ndeep learning models. We apply it to the case of video prediction, building on\ntop of object-centric deep models and show that this leads to a better\nperformance than using data-driven models alone. We develop an architecture\nthat facilitates latent space disentanglement in order to use the integrated\nprocedural knowledge, and establish a setup that allows the model to learn the\nprocedural interface in the latent space using the downstream task of video\nprediction. We contrast the performance to a state-of-the-art data-driven\napproach and show that problems where purely data-driven approaches struggle\ncan be handled by using knowledge about the domain, providing an alternative to\nsimply collecting more data.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Published in 2023 IEEE/CVF International Conference on Computer\n  Vision Workshops (ICCVW)",
    "pdf_url": "http://arxiv.org/pdf/2406.18220v1",
    "published_date": "2024-06-26 10:08:24 UTC",
    "updated_date": "2024-06-26 10:08:24 UTC"
  },
  {
    "arxiv_id": "2407.09538v1",
    "title": "A Dynamic Systems Approach to Modelling Human-Machine Rhythm Interaction",
    "authors": [
      "Zhongju Yuan",
      "Wannes Van Ransbeeck",
      "Geraint Wiggins",
      "Dick Botteldooren"
    ],
    "abstract": "In exploring the simulation of human rhythmic perception and synchronization\ncapabilities, this study introduces a computational model inspired by the\nphysical and biological processes underlying rhythm processing. Utilizing a\nreservoir computing framework that simulates the function of cerebellum, the\nmodel features a dual-neuron classification and incorporates parameters to\nmodulate information transfer, reflecting biological neural network\ncharacteristics. Our findings demonstrate the model's ability to accurately\nperceive and adapt to rhythmic patterns within the human perceptible range,\nexhibiting behavior closely aligned with human rhythm interaction. By\nincorporating fine-tuning mechanisms and delay-feedback, the model enables\ncontinuous learning and precise rhythm prediction. The introduction of\ncustomized settings further enhances its capacity to stimulate diverse human\nrhythmic behaviors, underscoring the potential of this architecture in temporal\ncognitive task modeling and the study of rhythm synchronization and prediction\nin artificial and biological systems. Therefore, our model is capable of\ntransparently modelling cognitive theories that elucidate the dynamic processes\nby which the brain generates rhythm-related behavior.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.HC",
      "cs.NE"
    ],
    "primary_category": "q-bio.NC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09538v1",
    "published_date": "2024-06-26 10:07:20 UTC",
    "updated_date": "2024-06-26 10:07:20 UTC"
  },
  {
    "arxiv_id": "2406.18211v1",
    "title": "AI Cards: Towards an Applied Framework for Machine-Readable AI and Risk Documentation Inspired by the EU AI Act",
    "authors": [
      "Delaram Golpayegani",
      "Isabelle Hupont",
      "Cecilia Panigutti",
      "Harshvardhan J. Pandit",
      "Sven Schade",
      "Declan O'Sullivan",
      "Dave Lewis"
    ],
    "abstract": "With the upcoming enforcement of the EU AI Act, documentation of high-risk AI\nsystems and their risk management information will become a legal requirement\nplaying a pivotal role in demonstration of compliance. Despite its importance,\nthere is a lack of standards and guidelines to assist with drawing up AI and\nrisk documentation aligned with the AI Act. This paper aims to address this gap\nby providing an in-depth analysis of the AI Act's provisions regarding\ntechnical documentation, wherein we particularly focus on AI risk management.\nOn the basis of this analysis, we propose AI Cards as a novel holistic\nframework for representing a given intended use of an AI system by encompassing\ninformation regarding technical specifications, context of use, and risk\nmanagement, both in human- and machine-readable formats. While the\nhuman-readable representation of AI Cards provides AI stakeholders with a\ntransparent and comprehensible overview of the AI use case, its\nmachine-readable specification leverages on state of the art Semantic Web\ntechnologies to embody the interoperability needed for exchanging documentation\nwithin the AI value chain. This brings the flexibility required for reflecting\nchanges applied to the AI system and its context, provides the scalability\nneeded to accommodate potential amendments to legal requirements, and enables\ndevelopment of automated tools to assist with legal compliance and conformity\nassessment tasks. To solidify the benefits, we provide an exemplar AI Card for\nan AI-based student proctoring system and further discuss its potential\napplications within and beyond the context of the AI Act.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.18211v1",
    "published_date": "2024-06-26 09:51:49 UTC",
    "updated_date": "2024-06-26 09:51:49 UTC"
  },
  {
    "arxiv_id": "2406.18193v1",
    "title": "MammothModa: Multi-Modal Large Language Model",
    "authors": [
      "Qi She",
      "Junwen Pan",
      "Xin Wan",
      "Rui Zhang",
      "Dawei Lu",
      "Kai Huang"
    ],
    "abstract": "In this report, we introduce MammothModa, yet another multi-modal large\nlanguage model (MLLM) designed to achieve state-of-the-art performance starting\nfrom an elementary baseline. We focus on three key design insights: (i)\nIntegrating Visual Capabilities while Maintaining Complex Language\nUnderstanding: In addition to the vision encoder, we incorporated the Visual\nAttention Experts into the LLM to enhance its visual capabilities. (ii)\nExtending Context Window for High-Resolution and Long-Duration Visual Feature:\nWe explore the Visual Merger Module to effectively reduce the token number of\nhigh-resolution images and incorporated frame position ids to avoid position\ninterpolation. (iii) High-Quality Bilingual Datasets: We meticulously curated\nand filtered a high-quality bilingual multimodal dataset to reduce visual\nhallucinations. With above recipe we build MammothModa that consistently\noutperforms the state-of-the-art models, e.g., LLaVA-series, across main\nreal-world visual language benchmarks without bells and whistles.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Technical report",
    "pdf_url": "http://arxiv.org/pdf/2406.18193v1",
    "published_date": "2024-06-26 09:17:27 UTC",
    "updated_date": "2024-06-26 09:17:27 UTC"
  },
  {
    "arxiv_id": "2406.18192v2",
    "title": "Methodology of Adapting Large English Language Models for Specific Cultural Contexts",
    "authors": [
      "Wenjing Zhang",
      "Siqi Xiao",
      "Xuejiao Lei",
      "Ning Wang",
      "Huazheng Zhang",
      "Meijuan An",
      "Bikun Yang",
      "Zhaoxiang Liu",
      "Kai Wang",
      "Shiguo Lian"
    ],
    "abstract": "The rapid growth of large language models(LLMs) has emerged as a prominent\ntrend in the field of artificial intelligence. However, current\nstate-of-the-art LLMs are predominantly based on English. They encounter\nlimitations when directly applied to tasks in specific cultural domains, due to\ndeficiencies in domain-specific knowledge and misunderstandings caused by\ndifferences in cultural values. To address this challenge, our paper proposes a\nrapid adaptation method for large models in specific cultural contexts, which\nleverages instruction-tuning based on specific cultural knowledge and safety\nvalues data. Taking Chinese as the specific cultural context and utilizing the\nLLaMA3-8B as the experimental English LLM, the evaluation results demonstrate\nthat the adapted LLM significantly enhances its capabilities in domain-specific\nknowledge and adaptability to safety values, while maintaining its original\nexpertise advantages.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.18192v2",
    "published_date": "2024-06-26 09:16:08 UTC",
    "updated_date": "2024-06-27 02:17:19 UTC"
  },
  {
    "arxiv_id": "2406.18187v1",
    "title": "Selective Prompting Tuning for Personalized Conversations with LLMs",
    "authors": [
      "Qiushi Huang",
      "Xubo Liu",
      "Tom Ko",
      "Bo Wu",
      "Wenwu Wang",
      "Yu Zhang",
      "Lilian Tang"
    ],
    "abstract": "In conversational AI, personalizing dialogues with persona profiles and\ncontextual understanding is essential. Despite large language models' (LLMs)\nimproved response coherence, effective persona integration remains a challenge.\nIn this work, we first study two common approaches for personalizing LLMs:\ntextual prompting and direct fine-tuning. We observed that textual prompting\noften struggles to yield responses that are similar to the ground truths in\ndatasets, while direct fine-tuning tends to produce repetitive or overly\ngeneric replies. To alleviate those issues, we propose \\textbf{S}elective\n\\textbf{P}rompt \\textbf{T}uning (SPT), which softly prompts LLMs for\npersonalized conversations in a selective way. Concretely, SPT initializes a\nset of soft prompts and uses a trainable dense retriever to adaptively select\nsuitable soft prompts for LLMs according to different input contexts, where the\nprompt retriever is dynamically updated through feedback from the LLMs.\nAdditionally, we propose context-prompt contrastive learning and prompt fusion\nlearning to encourage the SPT to enhance the diversity of personalized\nconversations. Experiments on the CONVAI2 dataset demonstrate that SPT\nsignificantly enhances response diversity by up to 90\\%, along with\nimprovements in other critical performance indicators. Those results highlight\nthe efficacy of SPT in fostering engaging and personalized dialogue generation.\nThe SPT model code (https://github.com/hqsiswiliam/SPT) is publicly available\nfor further exploration.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ACL 2024 findings",
    "pdf_url": "http://arxiv.org/pdf/2406.18187v1",
    "published_date": "2024-06-26 09:03:52 UTC",
    "updated_date": "2024-06-26 09:03:52 UTC"
  },
  {
    "arxiv_id": "2407.09537v1",
    "title": "ViPro: Enabling and Controlling Video Prediction for Complex Dynamical Scenarios using Procedural Knowledge",
    "authors": [
      "Patrick Takenaka",
      "Johannes Maucher",
      "Marco F. Huber"
    ],
    "abstract": "We propose a novel architecture design for video prediction in order to\nutilize procedural domain knowledge directly as part of the computational graph\nof data-driven models. On the basis of new challenging scenarios we show that\nstate-of-the-art video predictors struggle in complex dynamical settings, and\nhighlight that the introduction of prior process knowledge makes their learning\nproblem feasible. Our approach results in the learning of a symbolically\naddressable interface between data-driven aspects in the model and our\ndedicated procedural knowledge module, which we utilize in downstream control\ntasks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "accepted at NeSy2024, to be published in LNCS/LNAI",
    "pdf_url": "http://arxiv.org/pdf/2407.09537v1",
    "published_date": "2024-06-26 09:01:35 UTC",
    "updated_date": "2024-06-26 09:01:35 UTC"
  },
  {
    "arxiv_id": "2406.18178v2",
    "title": "Games of Knightian Uncertainty as AGI testbeds",
    "authors": [
      "Spyridon Samothrakis",
      "Dennis J. N. J. Soemers",
      "Damian Machlanski"
    ],
    "abstract": "Arguably, for the latter part of the late 20th and early 21st centuries,\ngames have been seen as the drosophila of AI. Games are a set of exciting\ntestbeds, whose solutions (in terms of identifying optimal players) would lead\nto machines that would possess some form of general intelligence, or at the\nvery least help us gain insights toward building intelligent machines.\nFollowing impressive successes in traditional board games like Go, Chess, and\nPoker, but also video games like the Atari 2600 collection, it is clear that\nthis is not the case. Games have been attacked successfully, but we are nowhere\nnear AGI developments (or, as harsher critics might say, useful AI\ndevelopments!). In this short vision paper, we argue that for game research to\nbecome again relevant to the AGI pathway, we need to be able to address\n\\textit{Knightian uncertainty} in the context of games, i.e. agents need to be\nable to adapt to rapid changes in game rules on the fly with no warning, no\nprevious data, and no model access.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.18178v2",
    "published_date": "2024-06-26 08:52:34 UTC",
    "updated_date": "2024-06-27 09:58:35 UTC"
  },
  {
    "arxiv_id": "2406.18175v2",
    "title": "Galaxy spectroscopy without spectra: Galaxy properties from photometric images with conditional diffusion models",
    "authors": [
      "Lars Doorenbos",
      "Eva Sextl",
      "Kevin Heng",
      "Stefano Cavuoti",
      "Massimo Brescia",
      "Olena Torbaniuk",
      "Giuseppe Longo",
      "Raphael Sznitman",
      "Pablo Márquez-Neila"
    ],
    "abstract": "Modern spectroscopic surveys can only target a small fraction of the vast\namount of photometrically cataloged sources in wide-field surveys. Here, we\nreport the development of a generative AI method capable of predicting optical\ngalaxy spectra from photometric broad-band images alone. This method draws from\nthe latest advances in diffusion models in combination with contrastive\nnetworks. We pass multi-band galaxy images into the architecture to obtain\noptical spectra. From these, robust values for galaxy properties can be derived\nwith any methods in the spectroscopic toolbox, such as standard population\nsynthesis techniques and Lick indices. When trained and tested on 64x64-pixel\nimages from the Sloan Digital Sky Survey, the global bimodality of star-forming\nand quiescent galaxies in photometric space is recovered, as well as a\nmass-metallicity relation of star-forming galaxies. The comparison between the\nobserved and the artificially created spectra shows good agreement in overall\nmetallicity, age, Dn4000, stellar velocity dispersion, and E(B-V) values.\nPhotometric redshift estimates of our generative algorithm can compete with\nother current, specialized deep-learning techniques. Moreover, this work is the\nfirst attempt in the literature to infer velocity dispersion from photometric\nimages. Additionally, we can predict the presence of an active galactic nucleus\nup to an accuracy of 82%. With our method, scientifically interesting galaxy\nproperties, normally requiring spectroscopic inputs, can be obtained in future\ndata sets from large-scale photometric surveys alone. The spectra prediction\nvia AI can further assist in creating realistic mock catalogs.",
    "categories": [
      "astro-ph.GA",
      "astro-ph.IM",
      "cs.AI"
    ],
    "primary_category": "astro-ph.GA",
    "comment": "Accepted by The Astrophysical Journal. Code is available at\n  https://github.com/LarsDoorenbos/generate-spectra",
    "pdf_url": "http://arxiv.org/pdf/2406.18175v2",
    "published_date": "2024-06-26 08:49:51 UTC",
    "updated_date": "2024-10-28 11:14:10 UTC"
  },
  {
    "arxiv_id": "2406.18621v2",
    "title": "Towards Deep Active Learning in Avian Bioacoustics",
    "authors": [
      "Lukas Rauch",
      "Denis Huseljic",
      "Moritz Wirth",
      "Jens Decke",
      "Bernhard Sick",
      "Christoph Scholz"
    ],
    "abstract": "Passive acoustic monitoring (PAM) in avian bioacoustics enables\ncost-effective and extensive data collection with minimal disruption to natural\nhabitats. Despite advancements in computational avian bioacoustics, deep\nlearning models continue to encounter challenges in adapting to diverse\nenvironments in practical PAM scenarios. This is primarily due to the scarcity\nof annotations, which requires labor-intensive efforts from human experts.\nActive learning (AL) reduces annotation cost and speed ups adaption to diverse\nscenarios by querying the most informative instances for labeling. This paper\noutlines a deep AL approach, introduces key challenges, and conducts a\nsmall-scale pilot study.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "accepted at IAL@ECML-PKDD24",
    "pdf_url": "http://arxiv.org/pdf/2406.18621v2",
    "published_date": "2024-06-26 08:43:05 UTC",
    "updated_date": "2024-11-05 13:31:46 UTC"
  },
  {
    "arxiv_id": "2406.18620v1",
    "title": "Documentation Practices of Artificial Intelligence",
    "authors": [
      "Stefan Arnold",
      "Dilara Yesilbas",
      "Rene Gröbner",
      "Dominik Riedelbauch",
      "Maik Horn",
      "Sven Weinzierl"
    ],
    "abstract": "Artificial Intelligence (AI) faces persistent challenges in terms of\ntransparency and accountability, which requires rigorous documentation. Through\na literature review on documentation practices, we provide an overview of\nprevailing trends, persistent issues, and the multifaceted interplay of factors\ninfluencing the documentation. Our examination of key characteristics such as\nscope, target audiences, support for multimodality, and level of automation,\nhighlights a dynamic evolution in documentation practices, underscored by a\nshift towards a more holistic, engaging, and automated documentation.",
    "categories": [
      "cs.DL",
      "cs.AI"
    ],
    "primary_category": "cs.DL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.18620v1",
    "published_date": "2024-06-26 08:33:52 UTC",
    "updated_date": "2024-06-26 08:33:52 UTC"
  },
  {
    "arxiv_id": "2406.18166v1",
    "title": "Start from Zero: Triple Set Prediction for Automatic Knowledge Graph Completion",
    "authors": [
      "Wen Zhang",
      "Yajing Xu",
      "Peng Ye",
      "Zhiwei Huang",
      "Zezhong Xu",
      "Jiaoyan Chen",
      "Jeff Z. Pan",
      "Huajun Chen"
    ],
    "abstract": "Knowledge graph (KG) completion aims to find out missing triples in a KG.\nSome tasks, such as link prediction and instance completion, have been proposed\nfor KG completion. They are triple-level tasks with some elements in a missing\ntriple given to predict the missing element of the triple. However, knowing\nsome elements of the missing triple in advance is not always a realistic\nsetting. In this paper, we propose a novel graph-level automatic KG completion\ntask called Triple Set Prediction (TSP) which assumes none of the elements in\nthe missing triples is given. TSP is to predict a set of missing triples given\na set of known triples. To properly and accurately evaluate this new task, we\npropose 4 evaluation metrics including 3 classification metrics and 1 ranking\nmetric, considering both the partial-open-world and the closed-world\nassumptions. Furthermore, to tackle the huge candidate triples for prediction,\nwe propose a novel and efficient subgraph-based method GPHT that can predict\nthe triple set fast. To fairly compare the TSP results, we also propose two\ntypes of methods RuleTensor-TSP and KGE-TSP applying the existing rule- and\nembedding-based methods for TSP as baselines. During experiments, we evaluate\nthe proposed methods on two datasets extracted from Wikidata following the\nrelation-similarity partial-open-world assumption proposed by us, and also\ncreate a complete family data set to evaluate TSP results following the\nclosed-world assumption. Results prove that the methods can successfully\ngenerate a set of missing triples and achieve reasonable scores on the new\ntask, and GPHT performs better than the baselines with significantly shorter\nprediction time. The datasets and code for experiments are available at\nhttps://github.com/zjukg/GPHT-for-TSP.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Paper accepted by TKDE in 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.18166v1",
    "published_date": "2024-06-26 08:26:32 UTC",
    "updated_date": "2024-06-26 08:26:32 UTC"
  },
  {
    "arxiv_id": "2406.18142v1",
    "title": "Innovating for Tomorrow: The Convergence of SE and Green AI",
    "authors": [
      "Luís Cruz",
      "Xavier Franch Gutierrez",
      "Silverio Martínez-Fernández"
    ],
    "abstract": "The latest advancements in machine learning, specifically in foundation\nmodels, are revolutionizing the frontiers of existing software engineering (SE)\nprocesses. This is a bi-directional phenomona, where 1) software systems are\nnow challenged to provide AI-enabled features to their users, and 2) AI is used\nto automate tasks within the software development lifecycle. In an era where\nsustainability is a pressing societal concern, our community needs to adopt a\nlong-term plan enabling a conscious transformation that aligns with\nenvironmental sustainability values. In this paper, we reflect on the impact of\nadopting environmentally friendly practices to create AI-enabled software\nsystems and make considerations on the environmental impact of using foundation\nmodels for software development.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted in SE 2030 - International Workshop on Software Engineering\n  in 2030",
    "pdf_url": "http://arxiv.org/pdf/2406.18142v1",
    "published_date": "2024-06-26 07:47:04 UTC",
    "updated_date": "2024-06-26 07:47:04 UTC"
  },
  {
    "arxiv_id": "2406.18140v3",
    "title": "Exclusive Style Removal for Cross Domain Novel Class Discovery",
    "authors": [
      "Yicheng Wang",
      "Feng Liu",
      "Junmin Liu",
      "Kai Sun"
    ],
    "abstract": "As a promising field in open-world learning, \\textit{Novel Class Discovery}\n(NCD) is usually a task to cluster unseen novel classes in an unlabeled set\nbased on the prior knowledge of labeled data within the same domain. However,\nthe performance of existing NCD methods could be severely compromised when\nnovel classes are sampled from a different distribution with the labeled ones.\nIn this paper, we explore and establish the solvability of NCD in cross domain\nsetting with the necessary condition that style information must be removed.\nBased on the theoretical analysis, we introduce an exclusive style removal\nmodule for extracting style information that is distinctive from the baseline\nfeatures, thereby facilitating inference. Moreover, this module is easy to\nintegrate with other NCD methods, acting as a plug-in to improve performance on\nnovel classes with different distributions compared to the seen labeled set.\nAdditionally, recognizing the non-negligible influence of different backbones\nand pre-training strategies on the performance of the NCD methods, we build a\nfair benchmark for future NCD research. Extensive experiments on three common\ndatasets demonstrate the effectiveness of our proposed module.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.18140v3",
    "published_date": "2024-06-26 07:44:27 UTC",
    "updated_date": "2024-09-13 11:36:49 UTC"
  },
  {
    "arxiv_id": "2406.18125v2",
    "title": "ResumeAtlas: Revisiting Resume Classification with Large-Scale Datasets and Large Language Models",
    "authors": [
      "Ahmed Heakl",
      "Youssef Mohamed",
      "Noran Mohamed",
      "Aly Elsharkawy",
      "Ahmed Zaky"
    ],
    "abstract": "The increasing reliance on online recruitment platforms coupled with the\nadoption of AI technologies has highlighted the critical need for efficient\nresume classification methods. However, challenges such as small datasets, lack\nof standardized resume templates, and privacy concerns hinder the accuracy and\neffectiveness of existing classification models. In this work, we address these\nchallenges by presenting a comprehensive approach to resume classification. We\ncurated a large-scale dataset of 13,389 resumes from diverse sources and\nemployed Large Language Models (LLMs) such as BERT and Gemma1.1 2B for\nclassification. Our results demonstrate significant improvements over\ntraditional machine learning approaches, with our best model achieving a top-1\naccuracy of 92\\% and a top-5 accuracy of 97.5\\%. These findings underscore the\nimportance of dataset quality and advanced model architectures in enhancing the\naccuracy and robustness of resume classification systems, thus advancing the\nfield of online recruitment practices.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages, 6 figures, 1 table, 6th International Conference on AI in\n  Computational Linguistics",
    "pdf_url": "http://arxiv.org/pdf/2406.18125v2",
    "published_date": "2024-06-26 07:25:18 UTC",
    "updated_date": "2024-07-12 18:19:28 UTC"
  },
  {
    "arxiv_id": "2406.18122v1",
    "title": "Poisoned LangChain: Jailbreak LLMs by LangChain",
    "authors": [
      "Ziqiu Wang",
      "Jun Liu",
      "Shengkai Zhang",
      "Yang Yang"
    ],
    "abstract": "With the development of natural language processing (NLP), large language\nmodels (LLMs) are becoming increasingly popular. LLMs are integrating more into\neveryday life, raising public concerns about their security vulnerabilities.\nConsequently, the security of large language models is becoming critically\nimportant. Currently, the techniques for attacking and defending against LLMs\nare continuously evolving. One significant method type of attack is the\njailbreak attack, which designed to evade model safety mechanisms and induce\nthe generation of inappropriate content. Existing jailbreak attacks primarily\nrely on crafting inducement prompts for direct jailbreaks, which are less\neffective against large models with robust filtering and high comprehension\nabilities. Given the increasing demand for real-time capabilities in large\nlanguage models, real-time updates and iterations of new knowledge have become\nessential. Retrieval-Augmented Generation (RAG), an advanced technique to\ncompensate for the model's lack of new knowledge, is gradually becoming\nmainstream. As RAG enables the model to utilize external knowledge bases, it\nprovides a new avenue for jailbreak attacks.\n  In this paper, we conduct the first work to propose the concept of indirect\njailbreak and achieve Retrieval-Augmented Generation via LangChain. Building on\nthis, we further design a novel method of indirect jailbreak attack, termed\nPoisoned-LangChain (PLC), which leverages a poisoned external knowledge base to\ninteract with large language models, thereby causing the large models to\ngenerate malicious non-compliant dialogues.We tested this method on six\ndifferent large language models across three major categories of jailbreak\nissues. The experiments demonstrate that PLC successfully implemented indirect\njailbreak attacks under three different scenarios, achieving success rates of\n88.56%, 79.04%, and 82.69% respectively.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "6 pages,2 figures,This paper is a submission to ACM TURC. It has been\n  accepted by the editor of the organizer",
    "pdf_url": "http://arxiv.org/pdf/2406.18122v1",
    "published_date": "2024-06-26 07:21:02 UTC",
    "updated_date": "2024-06-26 07:21:02 UTC"
  },
  {
    "arxiv_id": "2406.18120v2",
    "title": "ArzEn-LLM: Code-Switched Egyptian Arabic-English Translation and Speech Recognition Using LLMs",
    "authors": [
      "Ahmed Heakl",
      "Youssef Zaghloul",
      "Mennatullah Ali",
      "Rania Hossam",
      "Walid Gomaa"
    ],
    "abstract": "Motivated by the widespread increase in the phenomenon of code-switching\nbetween Egyptian Arabic and English in recent times, this paper explores the\nintricacies of machine translation (MT) and automatic speech recognition (ASR)\nsystems, focusing on translating code-switched Egyptian Arabic-English to\neither English or Egyptian Arabic. Our goal is to present the methodologies\nemployed in developing these systems, utilizing large language models such as\nLLama and Gemma. In the field of ASR, we explore the utilization of the Whisper\nmodel for code-switched Egyptian Arabic recognition, detailing our experimental\nprocedures including data preprocessing and training techniques. Through the\nimplementation of a consecutive speech-to-text translation system that\nintegrates ASR with MT, we aim to overcome challenges posed by limited\nresources and the unique characteristics of the Egyptian Arabic dialect.\nEvaluation against established metrics showcases promising results, with our\nmethodologies yielding a significant improvement of $56\\%$ in English\ntranslation over the state-of-the-art and $9.3\\%$ in Arabic translation. Since\ncode-switching is deeply inherent in spoken languages, it is crucial that ASR\nsystems can effectively handle this phenomenon. This capability is crucial for\nenabling seamless interaction in various domains, including business\nnegotiations, cultural exchanges, and academic discourse. Our models and code\nare available as open-source resources. Code:\n\\url{http://github.com/ahmedheakl/arazn-llm}}, Models:\n\\url{http://huggingface.co/collections/ahmedheakl/arazn-llm-662ceaf12777656607b9524e}.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, 4 figures, 5 tables, 6th International Conference on AI in\n  Computational Linguistics",
    "pdf_url": "http://arxiv.org/pdf/2406.18120v2",
    "published_date": "2024-06-26 07:19:51 UTC",
    "updated_date": "2024-07-12 18:22:26 UTC"
  },
  {
    "arxiv_id": "2407.00092v1",
    "title": "Visual Reasoning and Multi-Agent Approach in Multimodal Large Language Models (MLLMs): Solving TSP and mTSP Combinatorial Challenges",
    "authors": [
      "Mohammed Elhenawy",
      "Ahmad Abutahoun",
      "Taqwa I. Alhadidi",
      "Ahmed Jaber",
      "Huthaifa I. Ashqar",
      "Shadi Jaradat",
      "Ahmed Abdelhay",
      "Sebastien Glaser",
      "Andry Rakotonirainy"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) harness comprehensive knowledge\nspanning text, images, and audio to adeptly tackle complex problems, including\nzero-shot in-context learning scenarios. This study explores the ability of\nMLLMs in visually solving the Traveling Salesman Problem (TSP) and Multiple\nTraveling Salesman Problem (mTSP) using images that portray point distributions\non a two-dimensional plane. We introduce a novel approach employing multiple\nspecialized agents within the MLLM framework, each dedicated to optimizing\nsolutions for these combinatorial challenges. Our experimental investigation\nincludes rigorous evaluations across zero-shot settings and introduces\ninnovative multi-agent zero-shot in-context scenarios. The results demonstrated\nthat both multi-agent models. Multi-Agent 1, which includes the Initializer,\nCritic, and Scorer agents, and Multi-Agent 2, which comprises only the\nInitializer and Critic agents; significantly improved solution quality for TSP\nand mTSP problems. Multi-Agent 1 excelled in environments requiring detailed\nroute refinement and evaluation, providing a robust framework for sophisticated\noptimizations. In contrast, Multi-Agent 2, focusing on iterative refinements by\nthe Initializer and Critic, proved effective for rapid decision-making\nscenarios. These experiments yield promising outcomes, showcasing the robust\nvisual reasoning capabilities of MLLMs in addressing diverse combinatorial\nproblems. The findings underscore the potential of MLLMs as powerful tools in\ncomputational optimization, offering insights that could inspire further\nadvancements in this promising field. Project link:\nhttps://github.com/ahmed-abdulhuy/Solving-TSP-and-mTSP-Combinatorial-Challenges-using-Visual-Reasoning-and-Multi-Agent-Approach-MLLMs-.git",
    "categories": [
      "cs.AI",
      "cs.ET",
      "cs.GT",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.00092v1",
    "published_date": "2024-06-26 07:12:06 UTC",
    "updated_date": "2024-06-26 07:12:06 UTC"
  },
  {
    "arxiv_id": "2406.18116v1",
    "title": "BADGE: BADminton report Generation and Evaluation with LLM",
    "authors": [
      "Shang-Hsuan Chiang",
      "Lin-Wei Chao",
      "Kuang-Da Wang",
      "Chih-Chuan Wang",
      "Wen-Chih Peng"
    ],
    "abstract": "Badminton enjoys widespread popularity, and reports on matches generally\ninclude details such as player names, game scores, and ball types, providing\naudiences with a comprehensive view of the games. However, writing these\nreports can be a time-consuming task. This challenge led us to explore whether\na Large Language Model (LLM) could automate the generation and evaluation of\nbadminton reports. We introduce a novel framework named BADGE, designed for\nthis purpose using LLM. Our method consists of two main phases: Report\nGeneration and Report Evaluation. Initially, badminton-related data is\nprocessed by the LLM, which then generates a detailed report of the match. We\ntested different Input Data Types, In-Context Learning (ICL), and LLM, finding\nthat GPT-4 performs best when using CSV data type and the Chain of Thought\nprompting. Following report generation, the LLM evaluates and scores the\nreports to assess their quality. Our comparisons between the scores evaluated\nby GPT-4 and human judges show a tendency to prefer GPT-4 generated reports.\nSince the application of LLM in badminton reporting remains largely unexplored,\nour research serves as a foundational step for future advancements in this\narea. Moreover, our method can be extended to other sports games, thereby\nenhancing sports promotion. For more details, please refer to\nhttps://github.com/AndyChiangSH/BADGE.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by IJCAI 2024 Workshop: The 2nd International Workshop on\n  Intelligent Technologies for Precision Sports Science (IT4PSS)",
    "pdf_url": "http://arxiv.org/pdf/2406.18116v1",
    "published_date": "2024-06-26 07:07:52 UTC",
    "updated_date": "2024-06-26 07:07:52 UTC"
  },
  {
    "arxiv_id": "2406.18115v1",
    "title": "Open-vocabulary Mobile Manipulation in Unseen Dynamic Environments with 3D Semantic Maps",
    "authors": [
      "Dicong Qiu",
      "Wenzong Ma",
      "Zhenfu Pan",
      "Hui Xiong",
      "Junwei Liang"
    ],
    "abstract": "Open-Vocabulary Mobile Manipulation (OVMM) is a crucial capability for\nautonomous robots, especially when faced with the challenges posed by unknown\nand dynamic environments. This task requires robots to explore and build a\nsemantic understanding of their surroundings, generate feasible plans to\nachieve manipulation goals, adapt to environmental changes, and comprehend\nnatural language instructions from humans. To address these challenges, we\npropose a novel framework that leverages the zero-shot detection and grounded\nrecognition capabilities of pretraining visual-language models (VLMs) combined\nwith dense 3D entity reconstruction to build 3D semantic maps. Additionally, we\nutilize large language models (LLMs) for spatial region abstraction and online\nplanning, incorporating human instructions and spatial semantic context. We\nhave built a 10-DoF mobile manipulation robotic platform JSR-1 and demonstrated\nin real-world robot experiments that our proposed framework can effectively\ncapture spatial semantics and process natural language user instructions for\nzero-shot OVMM tasks under dynamic environment settings, with an overall\nnavigation and task success rate of 80.95% and 73.33% over 105 episodes, and\nbetter SFT and SPL by 157.18% and 19.53% respectively compared to the baseline.\nFurthermore, the framework is capable of replanning towards the next most\nprobable candidate location based on the spatial semantic context derived from\nthe 3D semantic map when initial plans fail, keeping an average success rate of\n76.67%.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "Open-vocabulary, Mobile Manipulation, Dynamic Environments, 3D\n  Semantic Maps, Zero-shot, LLMs, VLMs, 18 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.18115v1",
    "published_date": "2024-06-26 07:06:42 UTC",
    "updated_date": "2024-06-26 07:06:42 UTC"
  },
  {
    "arxiv_id": "2406.18088v2",
    "title": "LLM-Driven Multimodal Opinion Expression Identification",
    "authors": [
      "Bonian Jia",
      "Huiyao Chen",
      "Yueheng Sun",
      "Meishan Zhang",
      "Min Zhang"
    ],
    "abstract": "Opinion Expression Identification (OEI) is essential in NLP for applications\nranging from voice assistants to depression diagnosis. This study extends OEI\nto encompass multimodal inputs, underlining the significance of auditory cues\nin delivering emotional subtleties beyond the capabilities of text. We\nintroduce a novel multimodal OEI (MOEI) task, integrating text and speech to\nmirror real-world scenarios. Utilizing CMU MOSEI and IEMOCAP datasets, we\nconstruct the CI-MOEI dataset. Additionally, Text-to-Speech (TTS) technology is\napplied to the MPQA dataset to obtain the CIM-OEI dataset. We design a template\nfor the OEI task to take full advantage of the generative power of large\nlanguage models (LLMs). Advancing further, we propose an LLM-driven method\nSTOEI, which combines speech and text modal to identify opinion expressions.\nOur experiments demonstrate that MOEI significantly improves the performance\nwhile our method outperforms existing methods by 9.20\\% and obtains SOTA\nresults.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "5 pages, 3 Figures, Accept by Interspeech 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.18088v2",
    "published_date": "2024-06-26 05:52:47 UTC",
    "updated_date": "2024-06-29 09:55:50 UTC"
  },
  {
    "arxiv_id": "2406.18087v1",
    "title": "EHR-Based Mobile and Web Platform for Chronic Disease Risk Prediction Using Large Language Multimodal Models",
    "authors": [
      "Chun-Chieh Liao",
      "Wei-Ting Kuo",
      "I-Hsuan Hu",
      "Yen-Chen Shih",
      "Jun-En Ding",
      "Feng Liu",
      "Fang-Ming Hung"
    ],
    "abstract": "Traditional diagnosis of chronic diseases involves in-person consultations\nwith physicians to identify the disease. However, there is a lack of research\nfocused on predicting and developing application systems using clinical notes\nand blood test values. We collected five years of Electronic Health Records\n(EHRs) from Taiwan's hospital database between 2017 and 2021 as an AI database.\nFurthermore, we developed an EHR-based chronic disease prediction platform\nutilizing Large Language Multimodal Models (LLMMs), successfully integrating\nwith frontend web and mobile applications for prediction. This prediction\nplatform can also connect to the hospital's backend database, providing\nphysicians with real-time risk assessment diagnostics. The demonstration link\ncan be found at https://www.youtube.com/watch?v=oqmL9DEDFgA.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.18087v1",
    "published_date": "2024-06-26 05:51:08 UTC",
    "updated_date": "2024-06-26 05:51:08 UTC"
  },
  {
    "arxiv_id": "2406.19417v1",
    "title": "\"Glue pizza and eat rocks\" -- Exploiting Vulnerabilities in Retrieval-Augmented Generative Models",
    "authors": [
      "Zhen Tan",
      "Chengshuai Zhao",
      "Raha Moraffah",
      "Yifan Li",
      "Song Wang",
      "Jundong Li",
      "Tianlong Chen",
      "Huan Liu"
    ],
    "abstract": "Retrieval-Augmented Generative (RAG) models enhance Large Language Models\n(LLMs) by integrating external knowledge bases, improving their performance in\napplications like fact-checking and information searching. In this paper, we\ndemonstrate a security threat where adversaries can exploit the openness of\nthese knowledge bases by injecting deceptive content into the retrieval\ndatabase, intentionally changing the model's behavior. This threat is critical\nas it mirrors real-world usage scenarios where RAG systems interact with\npublicly accessible knowledge bases, such as web scrapings and user-contributed\ndata pools. To be more realistic, we target a realistic setting where the\nadversary has no knowledge of users' queries, knowledge base data, and the LLM\nparameters. We demonstrate that it is possible to exploit the model\nsuccessfully through crafted content uploads with access to the retriever. Our\nfindings emphasize an urgent need for security measures in the design and\ndeployment of RAG systems to prevent potential manipulation and ensure the\nintegrity of machine-generated content.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2406.19417v1",
    "published_date": "2024-06-26 05:36:23 UTC",
    "updated_date": "2024-06-26 05:36:23 UTC"
  },
  {
    "arxiv_id": "2406.18078v1",
    "title": "Self-Training with Pseudo-Label Scorer for Aspect Sentiment Quad Prediction",
    "authors": [
      "Yice Zhang",
      "Jie Zeng",
      "Weiming Hu",
      "Ziyi Wang",
      "Shiwei Chen",
      "Ruifeng Xu"
    ],
    "abstract": "Aspect Sentiment Quad Prediction (ASQP) aims to predict all quads (aspect\nterm, aspect category, opinion term, sentiment polarity) for a given review,\nwhich is the most representative and challenging task in aspect-based sentiment\nanalysis. A key challenge in the ASQP task is the scarcity of labeled data,\nwhich limits the performance of existing methods. To tackle this issue, we\npropose a self-training framework with a pseudo-label scorer, wherein a scorer\nassesses the match between reviews and their pseudo-labels, aiming to filter\nout mismatches and thereby enhance the effectiveness of self-training. We\nhighlight two critical aspects to ensure the scorer's effectiveness and\nreliability: the quality of the training dataset and its model architecture. To\nthis end, we create a human-annotated comparison dataset and train a generative\nmodel on it using ranking-based objectives. Extensive experiments on public\nASQP datasets reveal that using our scorer can greatly and consistently improve\nthe effectiveness of self-training. Moreover, we explore the possibility of\nreplacing humans with large language models for comparison dataset annotation,\nand experiments demonstrate its feasibility. We release our code and data at\nhttps://github.com/HITSZ-HLT/ST-w-Scorer-ABSA .",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ACL 2024 Main Conference",
    "pdf_url": "http://arxiv.org/pdf/2406.18078v1",
    "published_date": "2024-06-26 05:30:21 UTC",
    "updated_date": "2024-06-26 05:30:21 UTC"
  },
  {
    "arxiv_id": "2406.18074v1",
    "title": "Few-Shot Medical Image Segmentation with High-Fidelity Prototypes",
    "authors": [
      "Song Tang",
      "Shaxu Yan",
      "Xiaozhi Qi",
      "Jianxin Gao",
      "Mao Ye",
      "Jianwei Zhang",
      "Xiatian Zhu"
    ],
    "abstract": "Few-shot Semantic Segmentation (FSS) aims to adapt a pretrained model to new\nclasses with as few as a single labelled training sample per class. Despite the\nprototype based approaches have achieved substantial success, existing models\nare limited to the imaging scenarios with considerably distinct objects and not\nhighly complex background, e.g., natural images. This makes such models\nsuboptimal for medical imaging with both conditions invalid. To address this\nproblem, we propose a novel Detail Self-refined Prototype Network (DSPNet) to\nconstructing high-fidelity prototypes representing the object foreground and\nthe background more comprehensively. Specifically, to construct global\nsemantics while maintaining the captured detail semantics, we learn the\nforeground prototypes by modelling the multi-modal structures with clustering\nand then fusing each in a channel-wise manner. Considering that the background\noften has no apparent semantic relation in the spatial dimensions, we integrate\nchannel-specific structural information under sparse channel-aware regulation.\nExtensive experiments on three challenging medical image benchmarks show the\nsuperiority of DSPNet over previous state-of-the-art methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.18074v1",
    "published_date": "2024-06-26 05:06:14 UTC",
    "updated_date": "2024-06-26 05:06:14 UTC"
  },
  {
    "arxiv_id": "2407.07111v1",
    "title": "Diffusion Model-Based Video Editing: A Survey",
    "authors": [
      "Wenhao Sun",
      "Rong-Cheng Tu",
      "Jingyi Liao",
      "Dacheng Tao"
    ],
    "abstract": "The rapid development of diffusion models (DMs) has significantly advanced\nimage and video applications, making \"what you want is what you see\" a reality.\nAmong these, video editing has gained substantial attention and seen a swift\nrise in research activity, necessitating a comprehensive and systematic review\nof the existing literature. This paper reviews diffusion model-based video\nediting techniques, including theoretical foundations and practical\napplications. We begin by overviewing the mathematical formulation and image\ndomain's key methods. Subsequently, we categorize video editing approaches by\nthe inherent connections of their core technologies, depicting evolutionary\ntrajectory. This paper also dives into novel applications, including\npoint-based editing and pose-guided human video editing. Additionally, we\npresent a comprehensive comparison using our newly introduced V2VBench.\nBuilding on the progress achieved to date, the paper concludes with ongoing\nchallenges and potential directions for future research.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "23 pages, 12 figures, a project related to this paper can be found at\n  https://github.com/wenhao728/awesome-diffusion-v2v",
    "pdf_url": "http://arxiv.org/pdf/2407.07111v1",
    "published_date": "2024-06-26 04:58:39 UTC",
    "updated_date": "2024-06-26 04:58:39 UTC"
  },
  {
    "arxiv_id": "2406.18069v3",
    "title": "Large Language Models for Cuffless Blood Pressure Measurement From Wearable Biosignals",
    "authors": [
      "Zengding Liu",
      "Chen Chen",
      "Jiannong Cao",
      "Minglei Pan",
      "Jikui Liu",
      "Nan Li",
      "Fen Miao",
      "Ye Li"
    ],
    "abstract": "Large language models (LLMs) have captured significant interest from both\nacademia and industry due to their impressive performance across various\ntextual tasks. However, the potential of LLMs to analyze physiological\ntime-series data remains an emerging research field. Particularly, there is a\nnotable gap in the utilization of LLMs for analyzing wearable biosignals to\nachieve cuffless blood pressure (BP) measurement, which is critical for the\nmanagement of cardiovascular diseases. This paper presents the first work to\nexplore the capacity of LLMs to perform cuffless BP estimation based on\nwearable biosignals. We extracted physiological features from electrocardiogram\n(ECG) and photoplethysmogram (PPG) signals and designed context-enhanced\nprompts by combining these features with BP domain knowledge and user\ninformation. Subsequently, we adapted LLMs to BP estimation tasks through\nfine-tuning. To evaluate the proposed approach, we conducted assessments of ten\nadvanced LLMs using a comprehensive public dataset of wearable biosignals from\n1,272 participants. The experimental results demonstrate that the optimally\nfine-tuned LLM significantly surpasses conventional task-specific baselines,\nachieving an estimation error of 0.00 $\\pm$ 9.25 mmHg for systolic BP and 1.29\n$\\pm$ 6.37 mmHg for diastolic BP. Notably, the ablation studies highlight the\nbenefits of our context enhancement strategy, leading to an 8.9% reduction in\nmean absolute error for systolic BP estimation. This paper pioneers the\nexploration of LLMs for cuffless BP measurement, providing a potential solution\nto enhance the accuracy of cuffless BP measurement.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.18069v3",
    "published_date": "2024-06-26 04:54:45 UTC",
    "updated_date": "2024-07-05 01:25:13 UTC"
  },
  {
    "arxiv_id": "2406.18062v1",
    "title": "Breaking the Barrier: Enhanced Utility and Robustness in Smoothed DRL Agents",
    "authors": [
      "Chung-En Sun",
      "Sicun Gao",
      "Tsui-Wei Weng"
    ],
    "abstract": "Robustness remains a paramount concern in deep reinforcement learning (DRL),\nwith randomized smoothing emerging as a key technique for enhancing this\nattribute. However, a notable gap exists in the performance of current smoothed\nDRL agents, often characterized by significantly low clean rewards and weak\nrobustness. In response to this challenge, our study introduces innovative\nalgorithms aimed at training effective smoothed robust DRL agents. We propose\nS-DQN and S-PPO, novel approaches that demonstrate remarkable improvements in\nclean rewards, empirical robustness, and robustness guarantee across standard\nRL benchmarks. Notably, our S-DQN and S-PPO agents not only significantly\noutperform existing smoothed agents by an average factor of $2.16\\times$ under\nthe strongest attack, but also surpass previous robustly-trained agents by an\naverage factor of $2.13\\times$. This represents a significant leap forward in\nthe field. Furthermore, we introduce Smoothed Attack, which is $1.89\\times$\nmore effective in decreasing the rewards of smoothed agents than existing\nadversarial attacks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published in ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.18062v1",
    "published_date": "2024-06-26 04:49:03 UTC",
    "updated_date": "2024-06-26 04:49:03 UTC"
  },
  {
    "arxiv_id": "2406.18060v3",
    "title": "AdaZeta: Adaptive Zeroth-Order Tensor-Train Adaption for Memory-Efficient Large Language Models Fine-Tuning",
    "authors": [
      "Yifan Yang",
      "Kai Zhen",
      "Ershad Banijamal",
      "Athanasios Mouchtaris",
      "Zheng Zhang"
    ],
    "abstract": "Fine-tuning large language models (LLMs) has achieved remarkable performance\nacross various natural language processing tasks, yet it demands more and more\nmemory as model sizes keep growing. To address this issue, the recently\nproposed Memory-efficient Zeroth-order (MeZO) methods attempt to fine-tune LLMs\nusing only forward passes, thereby avoiding the need for a backpropagation\ngraph. However, significant performance drops and a high risk of divergence\nhave limited their widespread adoption. In this paper, we propose the Adaptive\nZeroth-order Tensor-Train Adaption (AdaZeta) framework, specifically designed\nto improve the performance and convergence of the ZO methods. To enhance\ndimension-dependent ZO estimation accuracy, we introduce a fast-forward,\nlow-parameter tensorized adapter. To tackle the frequently observed divergence\nissue in large-scale ZO fine-tuning tasks, we propose an adaptive query number\nschedule that guarantees convergence. Detailed theoretical analysis and\nextensive experimental results on Roberta-Large and Llama-2-7B models\nsubstantiate the efficacy of our AdaZeta framework in terms of accuracy, memory\nefficiency, and convergence speed.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted for publication in EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.18060v3",
    "published_date": "2024-06-26 04:33:13 UTC",
    "updated_date": "2024-12-02 19:03:47 UTC"
  },
  {
    "arxiv_id": "2406.18616v1",
    "title": "Towards Large Language Model Aided Program Refinement",
    "authors": [
      "Yufan Cai",
      "Zhe Hou",
      "Xiaokun Luan",
      "David Miguel Sanan Baena",
      "Yun Lin",
      "Jun Sun",
      "Jin Song Dong"
    ],
    "abstract": "Program refinement involves correctness-preserving transformations from\nformal high-level specification statements into executable programs.\nTraditional verification tool support for program refinement is highly\ninteractive and lacks automation. On the other hand, the emergence of large\nlanguage models (LLMs) enables automatic code generations from informal natural\nlanguage specifications. However, code generated by LLMs is often unreliable.\nMoreover, the opaque procedure from specification to code provided by LLM is an\nuncontrolled black box. We propose LLM4PR, a tool that combines formal program\nrefinement techniques with informal LLM-based methods to (1) transform the\nspecification to preconditions and postconditions, (2) automatically build\nprompts based on refinement calculus, (3) interact with LLM to generate code,\nand finally, (4) verify that the generated code satisfies the conditions of\nrefinement calculus, thus guaranteeing the correctness of the code. We have\nimplemented our tool using GPT4, Coq, and Coqhammer, and evaluated it on the\nHumanEval and EvalPlus datasets.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "K.6.3"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.18616v1",
    "published_date": "2024-06-26 04:29:27 UTC",
    "updated_date": "2024-06-26 04:29:27 UTC"
  },
  {
    "arxiv_id": "2406.18053v1",
    "title": "Bidirectional-Reachable Hierarchical Reinforcement Learning with Mutually Responsive Policies",
    "authors": [
      "Yu Luo",
      "Fuchun Sun",
      "Tianying Ji",
      "Xianyuan Zhan"
    ],
    "abstract": "Hierarchical reinforcement learning (HRL) addresses complex long-horizon\ntasks by skillfully decomposing them into subgoals. Therefore, the\neffectiveness of HRL is greatly influenced by subgoal reachability. Typical HRL\nmethods only consider subgoal reachability from the unilateral level, where a\ndominant level enforces compliance to the subordinate level. However, we\nobserve that when the dominant level becomes trapped in local exploration or\ngenerates unattainable subgoals, the subordinate level is negatively affected\nand cannot follow the dominant level's actions. This can potentially make both\nlevels stuck in local optima, ultimately hindering subsequent subgoal\nreachability. Allowing real-time bilateral information sharing and error\ncorrection would be a natural cure for this issue, which motivates us to\npropose a mutual response mechanism. Based on this, we propose the\nBidirectional-reachable Hierarchical Policy Optimization (BrHPO)--a simple yet\neffective algorithm that also enjoys computation efficiency. Experiment results\non a variety of long-horizon tasks showcase that BrHPO outperforms other\nstate-of-the-art HRL baselines, coupled with a significantly higher exploration\nefficiency and robustness.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.18053v1",
    "published_date": "2024-06-26 04:05:04 UTC",
    "updated_date": "2024-06-26 04:05:04 UTC"
  },
  {
    "arxiv_id": "2406.18049v1",
    "title": "Improving Entity Recognition Using Ensembles of Deep Learning and Fine-tuned Large Language Models: A Case Study on Adverse Event Extraction from Multiple Sources",
    "authors": [
      "Yiming Li",
      "Deepthi Viswaroopan",
      "William He",
      "Jianfu Li",
      "Xu Zuo",
      "Hua Xu",
      "Cui Tao"
    ],
    "abstract": "Adverse event (AE) extraction following COVID-19 vaccines from text data is\ncrucial for monitoring and analyzing the safety profiles of immunizations.\nTraditional deep learning models are adept at learning intricate feature\nrepresentations and dependencies in sequential data, but often require\nextensive labeled data. In contrast, large language models (LLMs) excel in\nunderstanding contextual information, but exhibit unstable performance on named\nentity recognition tasks, possibly due to their broad but unspecific training.\nThis study aims to evaluate the effectiveness of LLMs and traditional deep\nlearning models in AE extraction, and to assess the impact of ensembling these\nmodels on performance. In this study, we utilized reports and posts from the\nVAERS (n=621), Twitter (n=9,133), and Reddit (n=131) as our corpora. Our goal\nwas to extract three types of entities: \"vaccine\", \"shot\", and \"ae\". We\nexplored and fine-tuned (except GPT-4) multiple LLMs, including GPT-2, GPT-3.5,\nGPT-4, and Llama-2, as well as traditional deep learning models like RNN and\nBioBERT. To enhance performance, we created ensembles of the three models with\nthe best performance. For evaluation, we used strict and relaxed F1 scores to\nevaluate the performance for each entity type, and micro-average F1 was used to\nassess the overall performance. The ensemble model achieved the highest\nperformance in \"vaccine\", \"shot\", and \"ae\" with strict F1-scores of 0.878,\n0.930, and 0.925, respectively, along with a micro-average score of 0.903. In\nconclusion, this study demonstrates the effectiveness and robustness of\nensembling fine-tuned traditional deep learning models and LLMs, for extracting\nAE-related information. This study contributes to the advancement of biomedical\nnatural language processing, providing valuable insights into improving AE\nextraction from text data for pharmacovigilance and public health surveillance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.18049v1",
    "published_date": "2024-06-26 03:56:21 UTC",
    "updated_date": "2024-06-26 03:56:21 UTC"
  },
  {
    "arxiv_id": "2406.18045v3",
    "title": "PharmaGPT: Domain-Specific Large Language Models for Bio-Pharmaceutical and Chemistry",
    "authors": [
      "Linqing Chen",
      "Weilei Wang",
      "Zilong Bai",
      "Peng Xu",
      "Yan Fang",
      "Jie Fang",
      "Wentao Wu",
      "Lizhi Zhou",
      "Ruiji Zhang",
      "Yubin Xia",
      "Chaobo Xu",
      "Ran Hu",
      "Licong Xu",
      "Qijun Cai",
      "Haoran Hua",
      "Jing Sun",
      "Jin Liu",
      "Tian Qiu",
      "Haowen Liu",
      "Meng Hu",
      "Xiuwen Li",
      "Fei Gao",
      "Yufu Wang",
      "Lin Tie",
      "Chaochao Wang",
      "Jianping Lu",
      "Cheng Sun",
      "Yixin Wang",
      "Shengjie Yang",
      "Yuancheng Li",
      "Lu Jin",
      "Lisha Zhang",
      "Fu Bian",
      "Zhongkai Ye",
      "Lidong Pei",
      "Changyang Tu"
    ],
    "abstract": "Large language models (LLMs) have revolutionized Natural Language Processing\n(NLP) by minimizing the need for complex feature engineering. However, the\napplication of LLMs in specialized domains like biopharmaceuticals and\nchemistry remains largely unexplored. These fields are characterized by\nintricate terminologies, specialized knowledge, and a high demand for precision\nareas where general purpose LLMs often fall short. In this study, we introduce\nPharmaGPT, a suite of domain specilized LLMs with 13 billion and 70 billion\nparameters, specifically trained on a comprehensive corpus tailored to the\nBio-Pharmaceutical and Chemical domains. Our evaluation shows that PharmaGPT\nsurpasses existing general models on specific-domain benchmarks such as NAPLEX,\ndemonstrating its exceptional capability in domain-specific tasks. Remarkably,\nthis performance is achieved with a model that has only a fraction, sometimes\njust one-tenth-of the parameters of general-purpose large models. This\nadvancement establishes a new benchmark for LLMs in the bio-pharmaceutical and\nchemical fields, addressing the existing gap in specialized language modeling.\nIt also suggests a promising path for enhanced research and development, paving\nthe way for more precise and effective NLP applications in these areas.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.18045v3",
    "published_date": "2024-06-26 03:43:09 UTC",
    "updated_date": "2024-07-09 06:52:17 UTC"
  },
  {
    "arxiv_id": "2406.18043v2",
    "title": "GenRL: Multimodal-foundation world models for generalization in embodied agents",
    "authors": [
      "Pietro Mazzaglia",
      "Tim Verbelen",
      "Bart Dhoedt",
      "Aaron Courville",
      "Sai Rajeswar"
    ],
    "abstract": "Learning generalist embodied agents, able to solve multitudes of tasks in\ndifferent domains is a long-standing problem. Reinforcement learning (RL) is\nhard to scale up as it requires a complex reward design for each task. In\ncontrast, language can specify tasks in a more natural way. Current foundation\nvision-language models (VLMs) generally require fine-tuning or other\nadaptations to be adopted in embodied contexts, due to the significant domain\ngap. However, the lack of multimodal data in such domains represents an\nobstacle to developing foundation models for embodied applications. In this\nwork, we overcome these problems by presenting multimodal-foundation world\nmodels, able to connect and align the representation of foundation VLMs with\nthe latent space of generative world models for RL, without any language\nannotations. The resulting agent learning framework, GenRL, allows one to\nspecify tasks through vision and/or language prompts, ground them in the\nembodied domain's dynamics, and learn the corresponding behaviors in\nimagination. As assessed through large-scale multi-task benchmarking in\nlocomotion and manipulation domains, GenRL enables multi-task generalization\nfrom language and visual prompts. Furthermore, by introducing a data-free\npolicy learning strategy, our approach lays the groundwork for foundational\npolicy learning using generative world models. Website, code and data:\nhttps://mazpie.github.io/genrl/",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "Presented at NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.18043v2",
    "published_date": "2024-06-26 03:41:48 UTC",
    "updated_date": "2024-10-30 20:16:18 UTC"
  },
  {
    "arxiv_id": "2407.03361v1",
    "title": "PianoBART: Symbolic Piano Music Generation and Understanding with Large-Scale Pre-Training",
    "authors": [
      "Xiao Liang",
      "Zijian Zhao",
      "Weichao Zeng",
      "Yutong He",
      "Fupeng He",
      "Yiyi Wang",
      "Chengying Gao"
    ],
    "abstract": "Learning musical structures and composition patterns is necessary for both\nmusic generation and understanding, but current methods do not make uniform use\nof learned features to generate and comprehend music simultaneously. In this\npaper, we propose PianoBART, a pre-trained model that uses BART for both\nsymbolic piano music generation and understanding. We devise a multi-level\nobject selection strategy for different pre-training tasks of PianoBART, which\ncan prevent information leakage or loss and enhance learning ability. The\nmusical semantics captured in pre-training are fine-tuned for music generation\nand understanding tasks. Experiments demonstrate that PianoBART efficiently\nlearns musical patterns and achieves outstanding performance in generating\nhigh-quality coherent pieces and comprehending music. Our code and\nsupplementary material are available at https://github.com/RS2002/PianoBart.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.03361v1",
    "published_date": "2024-06-26 03:35:54 UTC",
    "updated_date": "2024-06-26 03:35:54 UTC"
  },
  {
    "arxiv_id": "2406.18033v1",
    "title": "Boosting Soft Q-Learning by Bounding",
    "authors": [
      "Jacob Adamczyk",
      "Volodymyr Makarenko",
      "Stas Tiomkin",
      "Rahul V. Kulkarni"
    ],
    "abstract": "An agent's ability to leverage past experience is critical for efficiently\nsolving new tasks. Prior work has focused on using value function estimates to\nobtain zero-shot approximations for solutions to a new task. In soft\nQ-learning, we show how any value function estimate can also be used to derive\ndouble-sided bounds on the optimal value function. The derived bounds lead to\nnew approaches for boosting training performance which we validate\nexperimentally. Notably, we find that the proposed framework suggests an\nalternative method for updating the Q-function, leading to boosted performance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "To appear in the 1st Reinforcement Learning Conference",
    "pdf_url": "http://arxiv.org/pdf/2406.18033v1",
    "published_date": "2024-06-26 03:02:22 UTC",
    "updated_date": "2024-06-26 03:02:22 UTC"
  },
  {
    "arxiv_id": "2406.18027v2",
    "title": "Automated Clinical Data Extraction with Knowledge Conditioned LLMs",
    "authors": [
      "Diya Li",
      "Asim Kadav",
      "Aijing Gao",
      "Rui Li",
      "Richard Bourgon"
    ],
    "abstract": "The extraction of lung lesion information from clinical and medical imaging\nreports is crucial for research on and clinical care of lung-related diseases.\nLarge language models (LLMs) can be effective at interpreting unstructured text\nin reports, but they often hallucinate due to a lack of domain-specific\nknowledge, leading to reduced accuracy and posing challenges for use in\nclinical settings. To address this, we propose a novel framework that aligns\ngenerated internal knowledge with external knowledge through in-context\nlearning (ICL). Our framework employs a retriever to identify relevant units of\ninternal or external knowledge and a grader to evaluate the truthfulness and\nhelpfulness of the retrieved internal-knowledge rules, to align and update the\nknowledge bases. Experiments with expert-curated test datasets demonstrate that\nthis ICL approach can increase the F1 score for key fields (lesion size, margin\nand solidity) by an average of 12.9% over existing ICL methods.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "COLING25 Industry Track",
    "pdf_url": "http://arxiv.org/pdf/2406.18027v2",
    "published_date": "2024-06-26 02:49:28 UTC",
    "updated_date": "2024-11-15 02:07:34 UTC"
  },
  {
    "arxiv_id": "2406.18022v2",
    "title": "Automated Off-Policy Estimator Selection via Supervised Learning",
    "authors": [
      "Nicolò Felicioni",
      "Michael Benigni",
      "Maurizio Ferrari Dacrema"
    ],
    "abstract": "The Off-Policy Evaluation (OPE) problem consists of evaluating the\nperformance of counterfactual policies with data collected by another one. To\nsolve the OPE problem, we resort to estimators, which aim to estimate in the\nmost accurate way possible the performance that the counterfactual policies\nwould have had if they were deployed in place of the logging policy. In the\nliterature, several estimators have been developed, all with different\ncharacteristics and theoretical guarantees. Therefore, there is no dominant\nestimator and each estimator may be the best for different OPE problems,\ndepending on the characteristics of the dataset at hand. Although the selection\nof the estimator is a crucial choice for an accurate OPE, this problem has been\nwidely overlooked in the literature. We propose an automated data-driven OPE\nestimator selection method based on supervised learning. In particular, the\ncore idea we propose in this paper is to create several synthetic OPE tasks and\nuse a machine learning model trained to predict the best estimator for those\nsynthetic tasks. We empirically show how our method is able to perform a better\nestimator selection compared to a baseline method on several real-world\ndatasets, with a computational cost significantly lower than the one of the\nbaseline.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.18022v2",
    "published_date": "2024-06-26 02:34:48 UTC",
    "updated_date": "2024-11-09 19:06:18 UTC"
  },
  {
    "arxiv_id": "2406.18020v1",
    "title": "MolFusion: Multimodal Fusion Learning for Molecular Representations via Multi-granularity Views",
    "authors": [
      "Muzhen Cai",
      "Sendong Zhao",
      "Haochun Wang",
      "Yanrui Du",
      "Zewen Qiang",
      "Bing Qin",
      "Ting Liu"
    ],
    "abstract": "Artificial Intelligence predicts drug properties by encoding drug molecules,\naiding in the rapid screening of candidates. Different molecular\nrepresentations, such as SMILES and molecule graphs, contain complementary\ninformation for molecular encoding. Thus exploiting complementary information\nfrom different molecular representations is one of the research priorities in\nmolecular encoding. Most existing methods for combining molecular\nmulti-modalities only use molecular-level information, making it hard to encode\nintra-molecular alignment information between different modalities. To address\nthis issue, we propose a multi-granularity fusion method that is MolFusion. The\nproposed MolFusion consists of two key components: (1) MolSim, a\nmolecular-level encoding component that achieves molecular-level alignment\nbetween different molecular representations. and (2) AtomAlign, an atomic-level\nencoding component that achieves atomic-level alignment between different\nmolecular representations. Experimental results show that MolFusion effectively\nutilizes complementary multimodal information, leading to significant\nimprovements in performance across various classification and regression tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.chem-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.18020v1",
    "published_date": "2024-06-26 02:26:50 UTC",
    "updated_date": "2024-06-26 02:26:50 UTC"
  },
  {
    "arxiv_id": "2407.07110v2",
    "title": "Foundation Models for ECG: Leveraging Hybrid Self-Supervised Learning for Advanced Cardiac Diagnostics",
    "authors": [
      "Junho Song",
      "Jong-Hwan Jang",
      "Byeong Tak Lee",
      "DongGyun Hong",
      "Joon-myoung Kwon",
      "Yong-Yeon Jo"
    ],
    "abstract": "Using foundation models enhanced by self-supervised learning (SSL) methods\npresents an innovative approach to electrocardiogram (ECG) analysis, which is\ncrucial for cardiac health monitoring and diagnosis. This study comprehensively\nevaluates foundation models for ECGs, leveraging SSL methods, including\ngenerative and contrastive learning, on a vast dataset comprising approximately\n1.3 million ECG samples. By integrating these methods with consideration of the\nunique characteristics of ECGs, we developed a Hybrid Learning (HL) for\nfoundation models that improve the precision and reliability of cardiac\ndiagnostics. The HL-based foundation model adeptly captures the intricate\ndetails of ECGs, enhancing diagnostic capability. The results underscore the\nconsiderable potential of SSL-enhanced foundation models in clinical settings,\nsetting the stage for future research into their scalable applications across a\nbroader range of medical diagnostics. This work sets a new standard in the ECG\nfield, emphasizing the transformative influence of tailored, data-driven model\ntraining on the effectiveness and accuracy of medical diagnostics.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "27 pages",
    "pdf_url": "http://arxiv.org/pdf/2407.07110v2",
    "published_date": "2024-06-26 02:24:13 UTC",
    "updated_date": "2024-10-15 09:33:39 UTC"
  },
  {
    "arxiv_id": "2407.01598v1",
    "title": "Long-Term Prediction Accuracy Improvement of Data-Driven Medium-Range Global Weather Forecast",
    "authors": [
      "Yifan Hu",
      "Fukang Yin",
      "Weimin Zhang",
      "Kaijun Ren",
      "Junqiang Song",
      "Kefeng Deng",
      "Di Zhang"
    ],
    "abstract": "Long-term stability stands as a crucial requirement in data-driven\nmedium-range global weather forecasting. Spectral bias is recognized as the\nprimary contributor to instabilities, as data-driven methods difficult to learn\nsmall-scale dynamics. In this paper, we reveal that the universal mechanism for\nthese instabilities is not only related to spectral bias but also to\ndistortions brought by processing spherical data using conventional\nconvolution. These distortions lead to a rapid amplification of errors over\nsuccessive long-term iterations, resulting in a significant decline in forecast\naccuracy. To address this issue, a universal neural operator called the\nSpherical Harmonic Neural Operator (SHNO) is introduced to improve long-term\niterative forecasts. SHNO uses the spherical harmonic basis to mitigate\ndistortions for spherical data and uses gated residual spectral attention\n(GRSA) to correct spectral bias caused by spurious correlations across\ndifferent scales. The effectiveness and merit of the proposed method have been\nvalidated through its application for spherical Shallow Water Equations (SWEs)\nand medium-range global weather forecasting. Our findings highlight the\nbenefits and potential of SHNO to improve the accuracy of long-term prediction.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01598v1",
    "published_date": "2024-06-26 02:06:27 UTC",
    "updated_date": "2024-06-26 02:06:27 UTC"
  },
  {
    "arxiv_id": "2406.18012v2",
    "title": "View-Invariant Pixelwise Anomaly Detection in Multi-object Scenes with Adaptive View Synthesis",
    "authors": [
      "Subin Varghese",
      "Vedhus Hoskere"
    ],
    "abstract": "Visual anomaly detection in the built environment is a valuable tool for\napplications such as infrastructure assessment, construction monitoring,\nsecurity surveillance, and urban planning. Anomaly detection approaches are\ntypically unsupervised and work by detecting deviations from an expected state\nwhere no assumptions are made exact type of deviation. Unsupervised pixel-level\nanomaly detection methods have been developed to successfully recognize and\nsegment anomalies; however, existing techniques are designed for industrial\nsettings with a fixed camera position. In the built environment, images are\nperiodically captured by a camera operated manually or mounted on aerial or\nground vehicles. The camera pose between successive collections may vary widely\nvoiding a fundamental assumption in existing anomaly detection approaches. To\naddress this gap, we introduce the problem of Scene Anomaly Detection (Scene\nAD), where the goal is to detect anomalies from two sets of images: one set\nwithout anomalies and one set that may or may not contain anomalies. No labeled\nsemantic segmentation data are provided for training. We propose a novel\nnetwork, OmniAD, to tackle Scene AD by refining the reverse distillation\nanomaly detection method, leading to a 40\\% improvement in pixel-level anomaly\ndetection. Additionally, we introduce two new data augmentation strategies that\nleverage novel view synthesis and camera localization to enhance\ngeneralization. We evaluate our approach both qualitatively and quantitatively\non a new dataset, ToyCity the first Scene AD dataset featuring multiple objects\nas well as on the established single object centric dataset, MAD. Our method\ndemonstrates marked improvement over baseline approaches, paving the way for\nrobust anomaly detection in scenes with real-world camera pose variations\ncommonly observed in the built environment. https://drags99.github.io/OmniAD/",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.18012v2",
    "published_date": "2024-06-26 01:54:10 UTC",
    "updated_date": "2025-04-01 00:59:21 UTC"
  },
  {
    "arxiv_id": "2406.18002v2",
    "title": "Decoding with Limited Teacher Supervision Requires Understanding When to Trust the Teacher",
    "authors": [
      "Hyunjong Ok",
      "Jegwang Ryu",
      "Jaeho Lee"
    ],
    "abstract": "How can small-scale large language models (LLMs) efficiently utilize the\nsupervision of LLMs to improve their generative quality? This question has been\nwell studied in scenarios where there is no restriction on the number of LLM\nsupervisions one can use, giving birth to many decoding algorithms that utilize\nsupervision without further training. However, it is still unclear what is an\neffective strategy under the $\\textit{limited supervision}$ scenario, where we\nassume that no more than a few tokens can be generated by LLMs. To this end, we\ndevelop an algorithm to effectively aggregate the small-scale LLM and LLM\npredictions on initial tokens so that the generated tokens can more accurately\ncondition the subsequent token generation by small-scale LLM only. Critically,\nwe find that it is essential to adaptively overtrust or disregard the LLM\nprediction based on the confidence of the small-scale LLM. Through our\nexperiments on a wide range of models and datasets, we demonstrate that our\nmethod provides a consistent improvement over conventional decoding strategies.\n$\\small$ $\\textbf{Code:}$ https://github.com/HJ-Ok/DecLimSup",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "17 pages, 7 figures, EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.18002v2",
    "published_date": "2024-06-26 01:16:12 UTC",
    "updated_date": "2024-10-03 07:55:17 UTC"
  },
  {
    "arxiv_id": "2406.17992v1",
    "title": "Catching Chameleons: Detecting Evolving Disinformation Generated using Large Language Models",
    "authors": [
      "Bohan Jiang",
      "Chengshuai Zhao",
      "Zhen Tan",
      "Huan Liu"
    ],
    "abstract": "Despite recent advancements in detecting disinformation generated by large\nlanguage models (LLMs), current efforts overlook the ever-evolving nature of\nthis disinformation. In this work, we investigate a challenging yet practical\nresearch problem of detecting evolving LLM-generated disinformation.\nDisinformation evolves constantly through the rapid development of LLMs and\ntheir variants. As a consequence, the detection model faces significant\nchallenges. First, it is inefficient to train separate models for each\ndisinformation generator. Second, the performance decreases in scenarios when\nevolving LLM-generated disinformation is encountered in sequential order. To\naddress this problem, we propose DELD (Detecting Evolving LLM-generated\nDisinformation), a parameter-efficient approach that jointly leverages the\ngeneral fact-checking capabilities of pre-trained language models (PLM) and the\nindependent disinformation generation characteristics of various LLMs. In\nparticular, the learned characteristics are concatenated sequentially to\nfacilitate knowledge accumulation and transformation. DELD addresses the issue\nof label scarcity by integrating the semantic embeddings of disinformation with\ntrainable soft prompts to elicit model-specific knowledge. Our experiments show\nthat \\textit{DELD} significantly outperforms state-of-the-art methods.\nMoreover, our method provides critical insights into the unique patterns of\ndisinformation generation across different LLMs, offering valuable perspectives\nin this line of research.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.17992v1",
    "published_date": "2024-06-26 00:21:39 UTC",
    "updated_date": "2024-06-26 00:21:39 UTC"
  },
  {
    "arxiv_id": "2406.17990v1",
    "title": "Explicit Diversity Conditions for Effective Question Answer Generation with Large Language Models",
    "authors": [
      "Vikas Yadav",
      "Hyuk Joon Kwon",
      "Vijay Srinivasan",
      "Hongxia Jin"
    ],
    "abstract": "Question Answer Generation (QAG) is an effective data augmentation technique\nto improve the accuracy of question answering systems, especially in\nlow-resource domains. While recent pretrained and large language model-based\nQAG methods have made substantial progress, they face the critical issue of\nredundant QA pair generation, affecting downstream QA systems. Implicit\ndiversity techniques such as sampling and diverse beam search are proven\neffective solutions but often yield smaller diversity. We present explicit\ndiversity conditions for QAG, focusing on spatial aspects, question types, and\nentities, substantially increasing diversity in QA generation. Our work\nemphasizes the need of explicit diversity conditions for generating diverse\nquestion-answer synthetic data by showing significant improvements in\ndownstream QA task over existing widely adopted implicit diversity techniques.\nIn particular, generated QA pairs from explicit diversity conditions when used\nto train the downstream QA model results in an average 4.1% exact match and\n4.5% F1 improvement over QAG from implicit sampling techniques on SQuADDU. Our\nwork emphasizes the need for explicit diversity conditions even more in\nlow-resource datasets (SubjQA), where average downstream QA performance\nimprovements are around 12% EM.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Published at COLING 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.17990v1",
    "published_date": "2024-06-26 00:12:08 UTC",
    "updated_date": "2024-06-26 00:12:08 UTC"
  },
  {
    "arxiv_id": "2406.17987v4",
    "title": "Multi-step Inference over Unstructured Data",
    "authors": [
      "Aditya Kalyanpur",
      "Kailash Karthik Saravanakumar",
      "Victor Barres",
      "CJ McFate",
      "Lori Moon",
      "Nati Seifu",
      "Maksim Eremeev",
      "Jose Barrera",
      "Abraham Bautista-Castillo",
      "Eric Brown",
      "David Ferrucci"
    ],
    "abstract": "The advent of Large Language Models (LLMs) and Generative AI has\nrevolutionized natural language applications across various domains. However,\nhigh-stakes decision-making tasks in fields such as medical, legal and finance\nrequire a level of precision, comprehensiveness, and logical consistency that\npure LLM or Retrieval-Augmented-Generation (RAG) approaches often fail to\ndeliver. At Elemental Cognition (EC), we have developed a neuro-symbolic AI\nplatform to tackle these problems. The platform integrates fine-tuned LLMs for\nknowledge extraction and alignment with a robust symbolic reasoning engine for\nlogical inference, planning and interactive constraint solving. We describe\nCora, a Collaborative Research Assistant built on this platform, that is\ndesigned to perform complex research and discovery tasks in high-stakes\ndomains. This paper discusses the multi-step inference challenges inherent in\nsuch domains, critiques the limitations of existing LLM-based methods, and\ndemonstrates how Cora's neuro-symbolic approach effectively addresses these\nissues. We provide an overview of the system architecture, key algorithms for\nknowledge extraction and formal reasoning, and present preliminary evaluation\nresults that highlight Cora's superior performance compared to well-known LLM\nand RAG baselines.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17987v4",
    "published_date": "2024-06-26 00:00:45 UTC",
    "updated_date": "2024-07-24 18:38:51 UTC"
  }
]
[
  {
    "arxiv_id": "2503.01872v1",
    "title": "FairGen: Controlling Sensitive Attributes for Fair Generations in Diffusion Models via Adaptive Latent Guidance",
    "authors": [
      "Mintong Kang",
      "Vinayshekhar Bannihatti Kumar",
      "Shamik Roy",
      "Abhishek Kumar",
      "Sopan Khosla",
      "Balakrishnan Murali Narayanaswamy",
      "Rashmi Gangadharaiah"
    ],
    "abstract": "Text-to-image diffusion models often exhibit biases toward specific\ndemographic groups, such as generating more males than females when prompted to\ngenerate images of engineers, raising ethical concerns and limiting their\nadoption. In this paper, we tackle the challenge of mitigating generation bias\ntowards any target attribute value (e.g., \"male\" for \"gender\") in diffusion\nmodels while preserving generation quality. We propose FairGen, an adaptive\nlatent guidance mechanism which controls the generation distribution during\ninference. In FairGen, a latent guidance module dynamically adjusts the\ndiffusion process to enforce specific attributes, while a memory module tracks\nthe generation statistics and steers latent guidance to align with the targeted\nfair distribution of the attribute values. Further, given the limitations of\nexisting datasets in comprehensively assessing bias in diffusion models, we\nintroduce a holistic bias evaluation benchmark HBE, covering diverse domains\nand incorporating complex prompts across various applications. Extensive\nevaluations on HBE and Stable Bias datasets demonstrate that FairGen\noutperforms existing bias mitigation approaches, achieving substantial bias\nreduction (e.g., 68.5% gender bias reduction on Stable Diffusion 2). Ablation\nstudies highlight FairGen's ability to flexibly and precisely control\ngeneration distribution at any user-specified granularity, ensuring adaptive\nand targeted bias mitigation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Under submission",
    "pdf_url": "http://arxiv.org/pdf/2503.01872v1",
    "published_date": "2025-02-25 23:47:22 UTC",
    "updated_date": "2025-02-25 23:47:22 UTC"
  },
  {
    "arxiv_id": "2503.00043v2",
    "title": "VOILA: Evaluation of MLLMs For Perceptual Understanding and Analogical Reasoning",
    "authors": [
      "Nilay Yilmaz",
      "Maitreya Patel",
      "Yiran Lawrence Luo",
      "Tejas Gokhale",
      "Chitta Baral",
      "Suren Jayasuriya",
      "Yezhou Yang"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have become a powerful tool for\nintegrating visual and textual information. Despite their exceptional\nperformance on visual understanding benchmarks, measuring their ability to\nreason abstractly across multiple images remains a significant challenge. To\naddress this, we introduce VOILA, a large-scale, open-ended, dynamic benchmark\ndesigned to evaluate MLLMs' perceptual understanding and abstract relational\nreasoning. VOILA employs an analogical mapping approach in the visual domain,\nrequiring models to generate an image that completes an analogy between two\ngiven image pairs, reference and application, without relying on predefined\nchoices. Our experiments demonstrate that the analogical reasoning tasks in\nVOILA present a challenge to MLLMs. Through multi-step analysis, we reveal that\ncurrent MLLMs struggle to comprehend inter-image relationships and exhibit\nlimited capabilities in high-level relational reasoning. Notably, we observe\nthat performance improves when following a multi-step strategy of least-to-most\nprompting. Comprehensive evaluations on open-source models and GPT-4o show that\non text-based answers, the best accuracy for challenging scenarios is 13%\n(LLaMa 3.2) and even for simpler tasks is only 29% (GPT-4o), while human\nperformance is significantly higher at 70% across both difficulty levels.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at ICLR 2025. Code and data: https://github.com/nlylmz/Voila",
    "pdf_url": "http://arxiv.org/pdf/2503.00043v2",
    "published_date": "2025-02-25 23:36:19 UTC",
    "updated_date": "2025-03-04 18:47:38 UTC"
  },
  {
    "arxiv_id": "2502.18697v1",
    "title": "H-FLTN: A Privacy-Preserving Hierarchical Framework for Electric Vehicle Spatio-Temporal Charge Prediction",
    "authors": [
      "Robert Marlin",
      "Raja Jurdak",
      "Alsharif Abuadbba"
    ],
    "abstract": "The widespread adoption of Electric Vehicles (EVs) poses critical challenges\nfor energy providers, particularly in predicting charging time (temporal\nprediction), ensuring user privacy, and managing resources efficiently in\nmobility-driven networks. This paper introduces the Hierarchical Federated\nLearning Transformer Network (H-FLTN) framework to address these challenges.\nH-FLTN employs a three-tier hierarchical architecture comprising EVs, community\nDistributed Energy Resource Management Systems (DERMS), and the Energy Provider\nData Centre (EPDC) to enable accurate spatio-temporal predictions of EV\ncharging needs while preserving privacy. Temporal prediction is enhanced using\nTransformer-based learning, capturing complex dependencies in charging\nbehavior. Privacy is ensured through Secure Aggregation, Additive Secret\nSharing, and Peer-to-Peer (P2P) Sharing with Augmentation, which allow only\nsecret shares of model weights to be exchanged while securing all\ntransmissions. To improve training efficiency and resource management, H-FLTN\nintegrates Dynamic Client Capping Mechanism (DCCM) and Client Rotation\nManagement (CRM), ensuring that training remains both computationally and\ntemporally efficient as the number of participating EVs increases. DCCM\noptimises client participation by limiting excessive computational loads, while\nCRM balances training contributions across epochs, preventing imbalanced\nparticipation. Our simulation results based on large-scale empirical vehicle\nmobility data reveal that DCCM and CRM reduce the training time complexity with\nincreasing EVs from linear to constant. Its integration into real-world smart\ncity infrastructure enhances energy demand forecasting, resource allocation,\nand grid stability, ensuring reliability and sustainability in future mobility\necosystems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "I.6.5"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, 7 tables, 2 figures, Journal Paper",
    "pdf_url": "http://arxiv.org/pdf/2502.18697v1",
    "published_date": "2025-02-25 23:20:53 UTC",
    "updated_date": "2025-02-25 23:20:53 UTC"
  },
  {
    "arxiv_id": "2502.18695v1",
    "title": "Policy-as-Prompt: Rethinking Content Moderation in the Age of Large Language Models",
    "authors": [
      "Konstantina Palla",
      "José Luis Redondo García",
      "Claudia Hauff",
      "Francesco Fabbri",
      "Henrik Lindström",
      "Daniel R. Taber",
      "Andreas Damianou",
      "Mounia Lalmas"
    ],
    "abstract": "Content moderation plays a critical role in shaping safe and inclusive online\nenvironments, balancing platform standards, user expectations, and regulatory\nframeworks. Traditionally, this process involves operationalising policies into\nguidelines, which are then used by downstream human moderators for enforcement,\nor to further annotate datasets for training machine learning moderation\nmodels. However, recent advancements in large language models (LLMs) are\ntransforming this landscape. These models can now interpret policies directly\nas textual inputs, eliminating the need for extensive data curation. This\napproach offers unprecedented flexibility, as moderation can be dynamically\nadjusted through natural language interactions. This paradigm shift raises\nimportant questions about how policies are operationalised and the implications\nfor content moderation practices. In this paper, we formalise the emerging\npolicy-as-prompt framework and identify five key challenges across four\ndomains: Technical Implementation (1. translating policy to prompts, 2.\nsensitivity to prompt structure and formatting), Sociotechnical (3. the risk of\ntechnological determinism in policy formation), Organisational (4. evolving\nroles between policy and machine learning teams), and Governance (5. model\ngovernance and accountability). Through analysing these challenges across\ntechnical, sociotechnical, organisational, and governance dimensions, we\ndiscuss potential mitigation approaches. This research provides actionable\ninsights for practitioners and lays the groundwork for future exploration of\nscalable and adaptive content moderation systems in digital ecosystems.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.CY",
    "comment": "14 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.18695v1",
    "published_date": "2025-02-25 23:15:16 UTC",
    "updated_date": "2025-02-25 23:15:16 UTC"
  },
  {
    "arxiv_id": "2502.18690v1",
    "title": "Hybrid Voting-Based Task Assignment in Role-Playing Games",
    "authors": [
      "Daniel Weiner",
      "Raj Korpan"
    ],
    "abstract": "In role-playing games (RPGs), the level of immersion is critical-especially\nwhen an in-game agent conveys tasks, hints, or ideas to the player. For an\nagent to accurately interpret the player's emotional state and contextual\nnuances, a foundational level of understanding is required, which can be\nachieved using a Large Language Model (LLM). Maintaining the LLM's focus across\nmultiple context changes, however, necessitates a more robust approach, such as\nintegrating the LLM with a dedicated task allocation model to guide its\nperformance throughout gameplay. In response to this need, we introduce\nVoting-Based Task Assignment (VBTA), a framework inspired by human reasoning in\ntask allocation and completion. VBTA assigns capability profiles to agents and\ntask descriptions to tasks, then generates a suitability matrix that quantifies\nthe alignment between an agent's abilities and a task's requirements.\nLeveraging six distinct voting methods, a pre-trained LLM, and integrating\nconflict-based search (CBS) for path planning, VBTA efficiently identifies and\nassigns the most suitable agent to each task. While existing approaches focus\non generating individual aspects of gameplay, such as single quests, or combat\nencounters, our method shows promise when generating both unique combat\nencounters and narratives because of its generalizable nature.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted for presentation at Dungeons, Neurons, and Dialogues: Social\n  Interaction Dynamics in Contextual Games Workshop at 20th Annual ACM/IEEE\n  International Conference on Human-Robot Interaction (HRI 2025)",
    "pdf_url": "http://arxiv.org/pdf/2502.18690v1",
    "published_date": "2025-02-25 22:58:21 UTC",
    "updated_date": "2025-02-25 22:58:21 UTC"
  },
  {
    "arxiv_id": "2503.00042v2",
    "title": "An Analysis of Data Transformation Effects on Segment Anything 2",
    "authors": [
      "Clayton Bromley",
      "Alexander Moore",
      "Amar Saini",
      "Doug Poland",
      "Carmen Carrano"
    ],
    "abstract": "Video object segmentation (VOS) is a critical task in the development of\nvideo perception and understanding. The Segment-Anything Model 2 (SAM 2),\nreleased by Meta AI, is the current state-of-the-art architecture for\nend-to-end VOS. SAM 2 performs very well on both clean video data and augmented\ndata, and completely intelligent video perception requires an understanding of\nhow this architecture is capable of achieving such quality results. To better\nunderstand how each step within the SAM 2 architecture permits high-quality\nvideo segmentation, a variety of complex video transformations are passed\nthrough the architecture, and the impact at each stage of the process is\nmeasured. It is observed that each progressive stage enables the filtering of\ncomplex transformation noise and the emphasis of the object of interest.\nContributions include the creation of complex transformation video datasets, an\nanalysis of how each stage of the SAM 2 architecture interprets these\ntransformations, and visualizations of segmented objects through each stage. By\nbetter understanding how each model structure impacts overall video\nunderstanding, VOS development can work to improve real-world applicability and\nperformance tracking, localizing, and segmenting objects despite complex\ncluttered scenes and obscurations.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "68T45",
      "I.4.6; I.2.10"
    ],
    "primary_category": "eess.IV",
    "comment": "11 pages, 30 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.00042v2",
    "published_date": "2025-02-25 22:58:13 UTC",
    "updated_date": "2025-05-13 02:36:07 UTC"
  },
  {
    "arxiv_id": "2502.18685v1",
    "title": "Speaking the Right Language: The Impact of Expertise Alignment in User-AI Interactions",
    "authors": [
      "Shramay Palta",
      "Nirupama Chandrasekaran",
      "Rachel Rudinger",
      "Scott Counts"
    ],
    "abstract": "Using a sample of 25,000 Bing Copilot conversations, we study how the agent\nresponds to users of varying levels of domain expertise and the resulting\nimpact on user experience along multiple dimensions. Our findings show that\nacross a variety of topical domains, the agent largely responds at proficient\nor expert levels of expertise (77% of conversations) which correlates with\npositive user experience regardless of the user's level of expertise.\nMisalignment, such that the agent responds at a level of expertise below that\nof the user, has a negative impact on overall user experience, with the impact\nmore profound for more complex tasks. We also show that users engage more, as\nmeasured by the number of words in the conversation, when the agent responds at\na level of expertise commensurate with that of the user. Our findings\nunderscore the importance of alignment between user and AI when designing\nhuman-centered AI systems, to ensure satisfactory and productive interactions.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "arXiv Version",
    "pdf_url": "http://arxiv.org/pdf/2502.18685v1",
    "published_date": "2025-02-25 22:46:51 UTC",
    "updated_date": "2025-02-25 22:46:51 UTC"
  },
  {
    "arxiv_id": "2502.18682v2",
    "title": "AI Mismatches: Identifying Potential Algorithmic Harms Before AI Development",
    "authors": [
      "Devansh Saxena",
      "Ji-Youn Jung",
      "Jodi Forlizzi",
      "Kenneth Holstein",
      "John Zimmerman"
    ],
    "abstract": "AI systems are often introduced with high expectations, yet many fail to\ndeliver, resulting in unintended harm and missed opportunities for benefit. We\nfrequently observe significant \"AI Mismatches\", where the system's actual\nperformance falls short of what is needed to ensure safety and co-create value.\nThese mismatches are particularly difficult to address once development is\nunderway, highlighting the need for early-stage intervention. Navigating\ncomplex, multi-dimensional risk factors that contribute to AI Mismatches is a\npersistent challenge. To address it, we propose an AI Mismatch approach to\nanticipate and mitigate risks early on, focusing on the gap between realistic\nmodel performance and required task performance. Through an analysis of 774 AI\ncases, we extracted a set of critical factors, which informed the development\nof seven matrices that map the relationships between these factors and\nhighlight high-risk areas. Through case studies, we demonstrate how our\napproach can help reduce risks in AI development.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "CHI Conference on Human Factors in Computing Systems (CHI '25), April\n  26-May 1, 2025, Yokohama, Japan",
    "pdf_url": "http://arxiv.org/pdf/2502.18682v2",
    "published_date": "2025-02-25 22:43:00 UTC",
    "updated_date": "2025-04-15 03:15:40 UTC"
  },
  {
    "arxiv_id": "2502.18681v1",
    "title": "Comparing Native and Non-native English Speakers' Behaviors in Collaborative Writing through Visual Analytics",
    "authors": [
      "Yuexi Chen",
      "Yimin Xiao",
      "Kazi Tasnim Zinat",
      "Naomi Yamashita",
      "Ge Gao",
      "Zhicheng Liu"
    ],
    "abstract": "Understanding collaborative writing dynamics between native speakers (NS) and\nnon-native speakers (NNS) is critical for enhancing collaboration quality and\nteam inclusivity. In this paper, we partnered with communication researchers to\ndevelop visual analytics solutions for comparing NS and NNS behaviors in 162\nwriting sessions across 27 teams. The primary challenges in analyzing writing\nbehaviors are data complexity and the uncertainties introduced by automated\nmethods. In response, we present \\textsc{COALA}, a novel visual analytics tool\nthat improves model interpretability by displaying uncertainties in author\nclusters, generating behavior summaries using large language models, and\nvisualizing writing-related actions at multiple granularities. We validated the\neffectiveness of \\textsc{COALA} through user studies with domain experts\n(N=2+2) and researchers with relevant experience (N=8). We present the insights\ndiscovered by participants using \\textsc{COALA}, suggest features for future\nAI-assisted collaborative writing tools, and discuss the broader implications\nfor analyzing collaborative processes beyond writing.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "H.5.2"
    ],
    "primary_category": "cs.HC",
    "comment": "accepted by CHI 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.18681v1",
    "published_date": "2025-02-25 22:39:55 UTC",
    "updated_date": "2025-02-25 22:39:55 UTC"
  },
  {
    "arxiv_id": "2503.01871v1",
    "title": "Data Augmentation for Instruction Following Policies via Trajectory Segmentation",
    "authors": [
      "Niklas Höpner",
      "Ilaria Tiddi",
      "Herke van Hoof"
    ],
    "abstract": "The scalability of instructable agents in robotics or gaming is often\nhindered by limited data that pairs instructions with agent trajectories.\nHowever, large datasets of unannotated trajectories containing sequences of\nvarious agent behaviour (play trajectories) are often available. In a\nsemi-supervised setup, we explore methods to extract labelled segments from\nplay trajectories. The goal is to augment a small annotated dataset of\ninstruction-trajectory pairs to improve the performance of an\ninstruction-following policy trained downstream via imitation learning.\nAssuming little variation in segment length, recent video segmentation methods\ncan effectively extract labelled segments. To address the constraint of segment\nlength, we propose Play Segmentation (PS), a probabilistic model that finds\nmaximum likely segmentations of extended subsegments, while only being trained\non individual instruction segments. Our results in a game environment and a\nsimulated robotic gripper setting underscore the importance of segmentation;\nrandomly sampled segments diminish performance, while incorporating labelled\nsegments from PS improves policy performance to the level of a policy trained\non twice the amount of labelled data.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.01871v1",
    "published_date": "2025-02-25 22:06:01 UTC",
    "updated_date": "2025-02-25 22:06:01 UTC"
  },
  {
    "arxiv_id": "2503.01870v1",
    "title": "Can Large Language Models Extract Customer Needs as well as Professional Analysts?",
    "authors": [
      "Artem Timoshenko",
      "Chengfeng Mao",
      "John R. Hauser"
    ],
    "abstract": "Identifying customer needs (CNs) is important for product management, product\ndevelopment, and marketing. Applications rely on professional analysts\ninterpreting textual data (e.g., interview transcripts, online reviews) to\nunderstand the nuances of customer experience and concisely formulate \"jobs to\nbe done.\" The task is cognitively complex and time-consuming. Current practice\nfacilitates the process with keyword search and machine learning but relies on\nhuman judgment to formulate CNs. We examine whether Large Language Models\n(LLMs) can automatically extract CNs. Because evaluating CNs requires\nprofessional judgment, we partnered with a marketing consulting firm to conduct\na blind study of CNs extracted by: (1) a foundational LLM with prompt\nengineering only (Base LLM), (2) an LLM fine-tuned with professionally\nidentified CNs (SFT LLM), and (3) professional analysts. The SFT LLM performs\nas well as or better than professional analysts when extracting CNs. The\nextracted CNs are well-formulated, sufficiently specific to identify\nopportunities, and justified by source content (no hallucinations). The SFT LLM\nis efficient and provides more complete coverage of CNs. The Base LLM was not\nsufficiently accurate or specific. Organizations can rely on SFT LLMs to reduce\nmanual effort, enhance the precision of CN articulation, and provide improved\ninsight for innovation and marketing strategy.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "econ.GN",
      "q-fin.EC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.01870v1",
    "published_date": "2025-02-25 21:55:35 UTC",
    "updated_date": "2025-02-25 21:55:35 UTC"
  },
  {
    "arxiv_id": "2502.18658v2",
    "title": "Assistance or Disruption? Exploring and Evaluating the Design and Trade-offs of Proactive AI Programming Support",
    "authors": [
      "Kevin Pu",
      "Daniel Lazaro",
      "Ian Arawjo",
      "Haijun Xia",
      "Ziang Xiao",
      "Tovi Grossman",
      "Yan Chen"
    ],
    "abstract": "AI programming tools enable powerful code generation, and recent prototypes\nattempt to reduce user effort with proactive AI agents, but their impact on\nprogramming workflows remains unexplored. We introduce and evaluate\nCodellaborator, a design probe LLM agent that initiates programming assistance\nbased on editor activities and task context. We explored three interface\nvariants to assess trade-offs between increasingly salient AI support:\nprompt-only, proactive agent, and proactive agent with presence and context\n(Codellaborator). In a within-subject study (N=18), we find that proactive\nagents increase efficiency compared to prompt-only paradigm, but also incur\nworkflow disruptions. However, presence indicators and interaction context\nsupport alleviated disruptions and improved users' awareness of AI processes.\nWe underscore trade-offs of Codellaborator on user control, ownership, and code\nunderstanding, emphasizing the need to adapt proactivity to programming\nprocesses. Our research contributes to the design exploration and evaluation of\nproactive AI systems, presenting design implications on AI-integrated\nprogramming workflow.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18658v2",
    "published_date": "2025-02-25 21:37:25 UTC",
    "updated_date": "2025-03-04 15:26:19 UTC"
  },
  {
    "arxiv_id": "2502.18653v1",
    "title": "Enhancing Text Classification with a Novel Multi-Agent Collaboration Framework Leveraging BERT",
    "authors": [
      "Hediyeh Baban",
      "Sai A Pidapar",
      "Aashutosh Nema",
      "Sichen Lu"
    ],
    "abstract": "We introduce a novel multi-agent collaboration framework designed to enhance\nthe accuracy and robustness of text classification models. Leveraging BERT as\nthe primary classifier, our framework dynamically escalates low-confidence\npredictions to a specialized multi-agent system comprising Lexical, Contextual,\nLogic, Consensus, and Explainability agents. This collaborative approach allows\nfor comprehensive analysis and consensus-driven decision-making, significantly\nimproving classification performance across diverse text classification tasks.\nEmpirical evaluations on benchmark datasets demonstrate that our framework\nachieves a 5.5% increase in accuracy compared to standard BERT-based\nclassifiers, underscoring its effectiveness and academic novelty in advancing\nmulti-agent systems within natural language processing.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18653v1",
    "published_date": "2025-02-25 21:30:16 UTC",
    "updated_date": "2025-02-25 21:30:16 UTC"
  },
  {
    "arxiv_id": "2502.18652v2",
    "title": "Independent Mobility GPT (IDM-GPT): A Self-Supervised Multi-Agent Large Language Model Framework for Customized Traffic Mobility Analysis Using Machine Learning Models",
    "authors": [
      "Fengze Yang",
      "Xiaoyue Cathy Liu",
      "Lingjiu Lu",
      "Bingzhang Wang",
      "Chenxi Dylan Liu"
    ],
    "abstract": "With the urbanization process, an increasing number of sensors are being\ndeployed in transportation systems, leading to an explosion of big data. To\nharness the power of this vast transportation data, various machine learning\n(ML) and artificial intelligence (AI) methods have been introduced to address\nnumerous transportation challenges. However, these methods often require\nsignificant investment in data collection, processing, storage, and the\nemployment of professionals with expertise in transportation and ML.\nAdditionally, privacy issues are a major concern when processing data for\nreal-world traffic control and management. To address these challenges, the\nresearch team proposes an innovative Multi-agent framework named Independent\nMobility GPT (IDM-GPT) based on large language models (LLMs) for customized\ntraffic analysis, management suggestions, and privacy preservation. IDM-GPT\nefficiently connects users, transportation databases, and ML models\neconomically. IDM-GPT trains, customizes, and applies various LLM-based AI\nagents for multiple functions, including user query comprehension, prompts\noptimization, data analysis, model selection, and performance evaluation and\nenhancement. With IDM-GPT, users without any background in transportation or ML\ncan efficiently and intuitively obtain data analysis and customized suggestions\nin near real-time based on their questions. Experimental results demonstrate\nthat IDM-GPT delivers satisfactory performance across multiple traffic-related\ntasks, providing comprehensive and actionable insights that support effective\ntraffic management and urban mobility improvement.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "24 pages, 4 figures, TRR accepted",
    "pdf_url": "http://arxiv.org/pdf/2502.18652v2",
    "published_date": "2025-02-25 21:28:15 UTC",
    "updated_date": "2025-03-03 19:02:27 UTC"
  },
  {
    "arxiv_id": "2504.03650v1",
    "title": "BoxRL-NNV: Boxed Refinement of Latin Hypercube Samples for Neural Network Verification",
    "authors": [
      "Sarthak Das"
    ],
    "abstract": "BoxRL-NNV is a Python tool for the detection of safety violations in neural\nnetworks by computing the bounds of the output variables, given the bounds of\nthe input variables of the network. This is done using global extrema\nestimation via Latin Hypercube Sampling, and further refinement using L-BFGS-B\nfor local optimization around the initial guess. This paper presents an\noverview of BoxRL-NNV, as well as our results for a subset of the ACAS Xu\nbenchmark. A complete evaluation of the tool's performance, including benchmark\ncomparisons with state-of-the-art tools, shall be presented at the Sixth\nInternational Verification of Neural Networks Competition (VNN-COMP'25).",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03650v1",
    "published_date": "2025-02-25 21:15:55 UTC",
    "updated_date": "2025-02-25 21:15:55 UTC"
  },
  {
    "arxiv_id": "2502.18641v1",
    "title": "WhatELSE: Shaping Narrative Spaces at Configurable Level of Abstraction for AI-bridged Interactive Storytelling",
    "authors": [
      "Zhuoran Lu",
      "Qian Zhou",
      "Yi Wang"
    ],
    "abstract": "Generative AI significantly enhances player agency in interactive narratives\n(IN) by enabling just-in-time content generation that adapts to player actions.\nWhile delegating generation to AI makes IN more interactive, it becomes\nchallenging for authors to control the space of possible narratives - within\nwhich the final story experienced by the player emerges from their interaction\nwith AI. In this paper, we present WhatELSE, an AI-bridged IN authoring system\nthat creates narrative possibility spaces from example stories. WhatELSE\nprovides three views (narrative pivot, outline, and variants) to help authors\nunderstand the narrative space and corresponding tools leveraging linguistic\nabstraction to control the boundaries of the narrative space. Taking innovative\nLLM-based narrative planning approaches, WhatELSE further unfolds the narrative\nspace into executable game events. Through a user study (N=12) and technical\nevaluations, we found that WhatELSE enables authors to perceive and edit the\nnarrative space and generates engaging interactive narratives at play-time.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "In Proceedings of CHI 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.18641v1",
    "published_date": "2025-02-25 21:02:15 UTC",
    "updated_date": "2025-02-25 21:02:15 UTC"
  },
  {
    "arxiv_id": "2502.18639v1",
    "title": "Quantum Machine Learning in Precision Medicine and Drug Discovery -- A Game Changer for Tailored Treatments?",
    "authors": [
      "Markus Bertl",
      "Alan Mott",
      "Salvatore Sinno",
      "Bhavika Bhalgamiya"
    ],
    "abstract": "The digitization of healthcare presents numerous challenges, including the\ncomplexity of biological systems, vast data generation, and the need for\npersonalized treatment plans. Traditional computational methods often fall\nshort, leading to delayed and sometimes ineffective diagnoses and treatments.\nQuantum Computing (QC) and Quantum Machine Learning (QML) offer transformative\nadvancements with the potential to revolutionize medicine. This paper\nsummarizes areas where QC promises unprecedented computational power, enabling\nfaster, more accurate diagnostics, personalized treatments, and enhanced drug\ndiscovery processes. However, integrating quantum technologies into precision\nmedicine also presents challenges, including errors in algorithms and high\ncosts. We show that mathematically-based techniques for specifying, developing,\nand verifying software (formal methods) can enhance the reliability and\ncorrectness of QC. By providing a rigorous mathematical framework, formal\nmethods help to specify, develop, and verify systems with high precision. In\ngenomic data analysis, formal specification languages can precisely (1) define\nthe behavior and properties of quantum algorithms designed to identify genetic\nmarkers associated with diseases. Model checking tools can systematically\nexplore all possible states of the algorithm to (2) ensure it behaves correctly\nunder all conditions, while theorem proving techniques provide mathematical (3)\nproof that the algorithm meets its specified properties, ensuring accuracy and\nreliability. Additionally, formal optimization techniques can (4) enhance the\nefficiency and performance of quantum algorithms by reducing resource usage,\nsuch as the number of qubits and gate operations. Therefore, we posit that\nformal methods can significantly contribute to enabling QC to realize its full\npotential as a game changer in precision medicine.",
    "categories": [
      "cs.ET",
      "cs.AI",
      "quant-ph"
    ],
    "primary_category": "cs.ET",
    "comment": "presented at AISoLA 2024",
    "pdf_url": "http://arxiv.org/pdf/2502.18639v1",
    "published_date": "2025-02-25 20:59:22 UTC",
    "updated_date": "2025-02-25 20:59:22 UTC"
  },
  {
    "arxiv_id": "2502.18635v2",
    "title": "Faster, Cheaper, Better: Multi-Objective Hyperparameter Optimization for LLM and RAG Systems",
    "authors": [
      "Matthew Barker",
      "Andrew Bell",
      "Evan Thomas",
      "James Carr",
      "Thomas Andrews",
      "Umang Bhatt"
    ],
    "abstract": "While Retrieval Augmented Generation (RAG) has emerged as a popular technique\nfor improving Large Language Model (LLM) systems, it introduces a large number\nof choices, parameters and hyperparameters that must be made or tuned. This\nincludes the LLM, embedding, and ranker models themselves, as well as\nhyperparameters governing individual RAG components. Yet, collectively\noptimizing the entire configuration in a RAG or LLM system remains\nunder-explored - especially in multi-objective settings - due to intractably\nlarge solution spaces, noisy objective evaluations, and the high cost of\nevaluations. In this work, we introduce the first approach for multi-objective\nparameter optimization of cost, latency, safety and alignment over entire LLM\nand RAG systems. We find that Bayesian optimization methods significantly\noutperform baseline approaches, obtaining a superior Pareto front on two new\nRAG benchmark tasks. We conclude our work with important considerations for\npractitioners who are designing multi-objective RAG systems, highlighting\nnuances such as how optimal configurations may not generalize across tasks and\nobjectives.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "68T20, 68Q32, 90C29, 62P30",
      "I.2.6; I.2.7; G.1.6; G.3"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18635v2",
    "published_date": "2025-02-25 20:52:06 UTC",
    "updated_date": "2025-05-08 10:58:09 UTC"
  },
  {
    "arxiv_id": "2502.18632v1",
    "title": "Automated Knowledge Component Generation and Knowledge Tracing for Coding Problems",
    "authors": [
      "Zhangqi Duan",
      "Nigel Fernandez",
      "Sri Kanakadandi",
      "Bita Akram",
      "Andrew Lan"
    ],
    "abstract": "Knowledge components (KCs) mapped to problems help model student learning,\ntracking their mastery levels on fine-grained skills thereby facilitating\npersonalized learning and feedback in online learning platforms. However,\ncrafting and tagging KCs to problems, traditionally performed by human domain\nexperts, is highly labor-intensive. We present a fully automated, LLM-based\npipeline for KC generation and tagging for open-ended programming problems. We\nalso develop an LLM-based knowledge tracing (KT) framework to leverage these\nLLM-generated KCs, which we refer to as KCGen-KT. We conduct extensive\nquantitative and qualitative evaluations validating the effectiveness of\nKCGen-KT. On a real-world dataset of student code submissions to open-ended\nprogramming problems, KCGen-KT outperforms existing KT methods. We investigate\nthe learning curves of generated KCs and show that LLM-generated KCs have a\ncomparable level-of-fit to human-written KCs under the performance factor\nanalysis (PFA) model. We also conduct a human evaluation to show that the KC\ntagging accuracy of our pipeline is reasonably accurate when compared to that\nby human domain experts.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.LG",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18632v1",
    "published_date": "2025-02-25 20:40:51 UTC",
    "updated_date": "2025-02-25 20:40:51 UTC"
  },
  {
    "arxiv_id": "2502.18620v1",
    "title": "Diffusion Models for conditional MRI generation",
    "authors": [
      "Miguel Herencia García del Castillo",
      "Ricardo Moya Garcia",
      "Manuel Jesús Cerezo Mazón",
      "Ekaitz Arriola Garcia",
      "Pablo Menéndez Fernández-Miranda"
    ],
    "abstract": "In this article, we present a Latent Diffusion Model (LDM) for the generation\nof brain Magnetic Resonance Imaging (MRI), conditioning its generation based on\npathology (Healthy, Glioblastoma, Sclerosis, Dementia) and acquisition modality\n(T1w, T1ce, T2w, Flair, PD).\n  To evaluate the quality of the generated images, the Fr\\'echet Inception\nDistance (FID) and Multi-Scale Structural Similarity Index (MS-SSIM) metrics\nwere employed. The results indicate that the model generates images with a\ndistribution similar to real ones, maintaining a balance between visual\nfidelity and diversity. Additionally, the model demonstrates extrapolation\ncapability, enabling the generation of configurations that were not present in\nthe training data.\n  The results validate the potential of the model to increase in the number of\nsamples in clinical datasets, balancing underrepresented classes, and\nevaluating AI models in medicine, contributing to the development of diagnostic\ntools in radiology without compromising patient privacy.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18620v1",
    "published_date": "2025-02-25 20:08:29 UTC",
    "updated_date": "2025-02-25 20:08:29 UTC"
  },
  {
    "arxiv_id": "2503.01868v1",
    "title": "Systems and Algorithms for Convolutional Multi-Hybrid Language Models at Scale",
    "authors": [
      "Jerome Ku",
      "Eric Nguyen",
      "David W. Romero",
      "Garyk Brixi",
      "Brandon Yang",
      "Anton Vorontsov",
      "Ali Taghibakhshi",
      "Amy X. Lu",
      "Dave P. Burke",
      "Greg Brockman",
      "Stefano Massaroli",
      "Christopher Ré",
      "Patrick D. Hsu",
      "Brian L. Hie",
      "Stefano Ermon",
      "Michael Poli"
    ],
    "abstract": "We introduce convolutional multi-hybrid architectures, with a design grounded\non two simple observations. First, operators in hybrid models can be tailored\nto token manipulation tasks such as in-context recall, multi-token recall, and\ncompression, with input-dependent convolutions and attention offering\ncomplementary performance. Second, co-designing convolution operators and\nhardware-aware algorithms enables efficiency gains in regimes where previous\nalternative architectures struggle to surpass Transformers. At the 40 billion\nparameter scale, we train end-to-end 1.2 to 2.9 times faster than optimized\nTransformers, and 1.1 to 1.4 times faster than previous generation hybrids. On\nH100 GPUs and model width 4096, individual operators in the proposed\nmulti-hybrid StripedHyena 2 architecture achieve two-fold throughput\nimprovement over linear attention and state-space models. Multi-hybrids excel\nat sequence modeling over byte-tokenized data, as demonstrated by the Evo 2\nline of models. We discuss the foundations that enable these results, including\narchitecture design, overlap-add blocked kernels for tensor cores, and\ndedicated all-to-all and point-to-point context parallelism strategies.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.01868v1",
    "published_date": "2025-02-25 19:47:20 UTC",
    "updated_date": "2025-02-25 19:47:20 UTC"
  },
  {
    "arxiv_id": "2502.18604v1",
    "title": "Mind the Gap: Bridging the Divide Between AI Aspirations and the Reality of Autonomous Characterization",
    "authors": [
      "Grace Guinan",
      "Addison Salvador",
      "Michelle A. Smeaton",
      "Andrew Glaws",
      "Hilary Egan",
      "Brian C. Wyatt",
      "Babak Anasori",
      "Kevin R. Fiedler",
      "Matthew J. Olszta",
      "Steven R. Spurgeon"
    ],
    "abstract": "What does materials science look like in the \"Age of Artificial\nIntelligence?\" Each materials domain-synthesis, characterization, and\nmodeling-has a different answer to this question, motivated by unique\nchallenges and constraints. This work focuses on the tremendous potential of\nautonomous characterization within electron microscopy. We present our recent\nadvancements in developing domain-aware, multimodal models for microscopy\nanalysis capable of describing complex atomic systems. We then address the\ncritical gap between the theoretical promise of autonomous microscopy and its\ncurrent practical limitations, showcasing recent successes while highlighting\nthe necessary developments to achieve robust, real-world autonomy.",
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.AI"
    ],
    "primary_category": "cond-mat.mtrl-sci",
    "comment": "33 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.18604v1",
    "published_date": "2025-02-25 19:43:47 UTC",
    "updated_date": "2025-02-25 19:43:47 UTC"
  },
  {
    "arxiv_id": "2502.18586v1",
    "title": "Autonomous Vision-Guided Resection of Central Airway Obstruction",
    "authors": [
      "M. E. Smith",
      "N. Yilmaz",
      "T. Watts",
      "P. M. Scheikl",
      "J. Ge",
      "A. Deguet",
      "A. Kuntz",
      "A. Krieger"
    ],
    "abstract": "Existing tracheal tumor resection methods often lack the precision required\nfor effective airway clearance, and robotic advancements offer new potential\nfor autonomous resection. We present a vision-guided, autonomous approach for\npalliative resection of tracheal tumors. This system models the tracheal\nsurface with a fifth-degree polynomial to plan tool trajectories, while a\ncustom Faster R-CNN segmentation pipeline identifies the trachea and tumor\nboundaries. The electrocautery tool angle is optimized using handheld surgical\ndemonstrations, and trajectories are planned to maintain a 1 mm safety\nclearance from the tracheal surface. We validated the workflow successfully in\nfive consecutive experiments on ex-vivo animal tissue models, successfully\nclearing the airway obstruction without trachea perforation in all cases (with\nmore than 90% volumetric tumor removal). These results support the feasibility\nof an autonomous resection platform, paving the way for future developments in\nminimally-invasive autonomous resection.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "Submitted to World Scientific, Journal of Medical Robotics Research\n  (JMRR) 2025. 10 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.18586v1",
    "published_date": "2025-02-25 19:11:11 UTC",
    "updated_date": "2025-02-25 19:11:11 UTC"
  },
  {
    "arxiv_id": "2502.18581v1",
    "title": "Scalable Best-of-N Selection for Large Language Models via Self-Certainty",
    "authors": [
      "Zhewei Kang",
      "Xuandong Zhao",
      "Dawn Song"
    ],
    "abstract": "Best-of-N selection is a key technique for improving the reasoning\nperformance of Large Language Models (LLMs) through increased test-time\ncomputation. Current state-of-the-art methods often employ computationally\nintensive reward models for response evaluation and selection. Reward-free\nalternatives, like self-consistency and universal self-consistency, are limited\nin their ability to handle open-ended generation tasks or scale effectively. To\naddress these limitations, we propose self-certainty, a novel and efficient\nmetric that leverages the inherent probability distribution of LLM outputs to\nestimate response quality without requiring external reward models. We\nhypothesize that higher distributional self-certainty, aggregated across\nmultiple samples, correlates with improved response accuracy, as it reflects\ngreater confidence in the generated output. Through extensive experiments on\nvarious reasoning tasks, we demonstrate that self-certainty (1) scales\neffectively with increasing sample size $N$, akin to reward models but without\nthe computational overhead; (2) complements chain-of-thought, improving\nreasoning performance beyond greedy decoding; and (3) generalizes to open-ended\ntasks where traditional self-consistency methods fall short. Our findings\nestablish self-certainty as a practical and efficient way for improving LLM\nreasoning capabilities. The code is available at\nhttps://github.com/backprop07/Self-Certainty",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18581v1",
    "published_date": "2025-02-25 19:08:07 UTC",
    "updated_date": "2025-02-25 19:08:07 UTC"
  },
  {
    "arxiv_id": "2502.18578v1",
    "title": "Differentially Private Iterative Screening Rules for Linear Regression",
    "authors": [
      "Amol Khanna",
      "Fred Lu",
      "Edward Raff"
    ],
    "abstract": "Linear $L_1$-regularized models have remained one of the simplest and most\neffective tools in data science. Over the past decade, screening rules have\nrisen in popularity as a way to eliminate features when producing the sparse\nregression weights of $L_1$ models. However, despite the increasing need of\nprivacy-preserving models for data analysis, to the best of our knowledge, no\ndifferentially private screening rule exists. In this paper, we develop the\nfirst private screening rule for linear regression. We initially find that this\nscreening rule is too strong: it screens too many coefficients as a result of\nthe private screening step. However, a weakened implementation of private\nscreening reduces overscreening and improves performance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "Proceedings of the 15th ACM Conference on Data and Application\n  Security and Privacy",
    "pdf_url": "http://arxiv.org/pdf/2502.18578v1",
    "published_date": "2025-02-25 19:06:19 UTC",
    "updated_date": "2025-02-25 19:06:19 UTC"
  },
  {
    "arxiv_id": "2502.18573v1",
    "title": "FactReasoner: A Probabilistic Approach to Long-Form Factuality Assessment for Large Language Models",
    "authors": [
      "Radu Marinescu",
      "Debarun Bhattacharjya",
      "Junkyu Lee",
      "Tigran Tchrakian",
      "Javier Carnerero Cano",
      "Yufang Hou",
      "Elizabeth Daly",
      "Alessandra Pascale"
    ],
    "abstract": "Large language models (LLMs) have demonstrated vast capabilities on\ngenerative tasks in recent years, yet they struggle with guaranteeing the\nfactual correctness of the generated content. This makes these models\nunreliable in realistic situations where factually accurate responses are\nexpected. In this paper, we propose FactReasoner, a new factuality assessor\nthat relies on probabilistic reasoning to assess the factuality of a long-form\ngenerated response. Specifically, FactReasoner decomposes the response into\natomic units, retrieves relevant contexts for them from an external knowledge\nsource, and constructs a joint probability distribution over the atoms and\ncontexts using probabilistic encodings of the logical relationships\n(entailment, contradiction) between the textual utterances corresponding to the\natoms and contexts. FactReasoner then computes the posterior probability of\nwhether atomic units in the response are supported by the retrieved contexts.\nOur experiments on labeled and unlabeled benchmark datasets demonstrate clearly\nthat FactReasoner improves considerably over state-of-the-art prompt-based\napproaches in terms of both factual precision and recall.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18573v1",
    "published_date": "2025-02-25 19:01:48 UTC",
    "updated_date": "2025-02-25 19:01:48 UTC"
  },
  {
    "arxiv_id": "2502.18462v1",
    "title": "Scalable Equilibrium Sampling with Sequential Boltzmann Generators",
    "authors": [
      "Charlie B. Tan",
      "Avishek Joey Bose",
      "Chen Lin",
      "Leon Klein",
      "Michael M. Bronstein",
      "Alexander Tong"
    ],
    "abstract": "Scalable sampling of molecular states in thermodynamic equilibrium is a\nlong-standing challenge in statistical physics. Boltzmann generators tackle\nthis problem by pairing powerful normalizing flows with importance sampling to\nobtain statistically independent samples under the target distribution. In this\npaper, we extend the Boltzmann generator framework and introduce Sequential\nBoltzmann generators (SBG) with two key improvements. The first is a highly\nefficient non-equivariant Transformer-based normalizing flow operating directly\non all-atom Cartesian coordinates. In contrast to equivariant continuous flows\nof prior methods, we leverage exactly invertible non-equivariant architectures\nwhich are highly efficient both during sample generation and likelihood\ncomputation. As a result, this unlocks more sophisticated inference strategies\nbeyond standard importance sampling. More precisely, as a second key\nimprovement we perform inference-time scaling of flow samples using annealed\nLangevin dynamics which transports samples toward the target distribution\nleading to lower variance (annealed) importance weights which enable higher\nfidelity resampling with sequential Monte Carlo. SBG achieves state-of-the-art\nperformance w.r.t. all metrics on molecular systems, demonstrating the first\nequilibrium sampling in Cartesian coordinates of tri, tetra, and hexapeptides\nthat were so far intractable for prior Boltzmann generators.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2502.18462v1",
    "published_date": "2025-02-25 18:59:13 UTC",
    "updated_date": "2025-02-25 18:59:13 UTC"
  },
  {
    "arxiv_id": "2503.05773v1",
    "title": "Between Innovation and Oversight: A Cross-Regional Study of AI Risk Management Frameworks in the EU, U.S., UK, and China",
    "authors": [
      "Amir Al-Maamari"
    ],
    "abstract": "As artificial intelligence (AI) technologies increasingly enter important\nsectors like healthcare, transportation, and finance, the development of\neffective governance frameworks is crucial for dealing with ethical, security,\nand societal risks. This paper conducts a comparative analysis of AI risk\nmanagement strategies across the European Union (EU), United States (U.S.),\nUnited Kingdom (UK), and China. A multi-method qualitative approach, including\ncomparative policy analysis, thematic analysis, and case studies, investigates\nhow these regions classify AI risks, implement compliance measures, structure\noversight, prioritize transparency, and respond to emerging innovations.\nExamples from high-risk contexts like healthcare diagnostics, autonomous\nvehicles, fintech, and facial recognition demonstrate the advantages and\nlimitations of different regulatory models. The findings show that the EU\nimplements a structured, risk-based framework that prioritizes transparency and\nconformity assessments, while the U.S. uses decentralized, sector-specific\nregulations that promote innovation but may lead to fragmented enforcement. The\nflexible, sector-specific strategy of the UK facilitates agile responses but\nmay lead to inconsistent coverage across domains. China's centralized\ndirectives allow rapid large-scale implementation while constraining public\ntransparency and external oversight. These insights show the necessity for AI\nregulation that is globally informed yet context-sensitive, aiming to balance\neffective risk management with technological progress. The paper concludes with\npolicy recommendations and suggestions for future research aimed at enhancing\neffective, adaptive, and inclusive AI governance globally.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05773v1",
    "published_date": "2025-02-25 18:52:17 UTC",
    "updated_date": "2025-02-25 18:52:17 UTC"
  },
  {
    "arxiv_id": "2502.18452v1",
    "title": "FRIDA to the Rescue! Analyzing Synthetic Data Effectiveness in Object-Based Common Sense Reasoning for Disaster Response",
    "authors": [
      "Mollie Shichman",
      "Claire Bonial",
      "Austin Blodgett",
      "Taylor Hudson",
      "Francis Ferraro",
      "Rachel Rudinger"
    ],
    "abstract": "Large Language Models (LLMs) have the potential for substantial common sense\nreasoning. However, these capabilities are often emergent in larger models.\nThis means smaller models that can be run locally are less helpful and capable\nwith respect to certain reasoning tasks. To meet our problem space\nrequirements, we fine-tune smaller LLMs to disaster domains, as these domains\ninvolve complex and low-frequency physical common sense knowledge. We introduce\na pipeline to create Field Ready Instruction Decoding Agent (FRIDA) models,\nwhere domain experts and linguists combine their knowledge to make high-quality\nseed data that is used to generate synthetic data for fine-tuning. We create a\nset of 130 seed instructions for synthetic generation, a synthetic dataset of\n25000 instructions, and 119 evaluation instructions relating to both general\nand earthquake-specific object affordances. We fine-tune several LLaMa and\nMistral instruction-tuned models and find that FRIDA models outperform their\nbase models at a variety of sizes. We then run an ablation study to understand\nwhich kinds of synthetic data most affect performance and find that training\nphysical state and object function common sense knowledge alone improves over\nFRIDA models trained on all data. We conclude that the FRIDA pipeline is\ncapable of instilling general common sense, but needs to be augmented with\ninformation retrieval for specific domain knowledge.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages, 3 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2502.18452v1",
    "published_date": "2025-02-25 18:51:06 UTC",
    "updated_date": "2025-02-25 18:51:06 UTC"
  },
  {
    "arxiv_id": "2502.18555v1",
    "title": "Application of Attention Mechanism with Bidirectional Long Short-Term Memory (BiLSTM) and CNN for Human Conflict Detection using Computer Vision",
    "authors": [
      "Erick da Silva Farias",
      "Eduardo Palhares Junior"
    ],
    "abstract": "The automatic detection of human conflicts through videos is a crucial area\nin computer vision, with significant applications in monitoring and public\nsafety policies. However, the scarcity of public datasets and the complexity of\nhuman interactions make this task challenging. This study investigates the\nintegration of advanced deep learning techniques, including Attention\nMechanism, Convolutional Neural Networks (CNNs), and Bidirectional Long\nShortTerm Memory (BiLSTM), to improve the detection of violent behaviors in\nvideos. The research explores how the use of the attention mechanism can help\nfocus on the most relevant parts of the video, enhancing the accuracy and\nrobustness of the model. The experiments indicate that the combination of CNNs\nwith BiLSTM and the attention mechanism provides a promising solution for\nconflict monitoring, offering insights into the effectiveness of different\nstrategies. This work opens new possibilities for the development of automated\nsurveillance systems that can operate more efficiently in real-time detection\nof violent events.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18555v1",
    "published_date": "2025-02-25 18:48:34 UTC",
    "updated_date": "2025-02-25 18:48:34 UTC"
  },
  {
    "arxiv_id": "2502.18449v1",
    "title": "SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open Software Evolution",
    "authors": [
      "Yuxiang Wei",
      "Olivier Duchenne",
      "Jade Copet",
      "Quentin Carbonneaux",
      "Lingming Zhang",
      "Daniel Fried",
      "Gabriel Synnaeve",
      "Rishabh Singh",
      "Sida I. Wang"
    ],
    "abstract": "The recent DeepSeek-R1 release has demonstrated the immense potential of\nreinforcement learning (RL) in enhancing the general reasoning capabilities of\nlarge language models (LLMs). While DeepSeek-R1 and other follow-up work\nprimarily focus on applying RL to competitive coding and math problems, this\npaper introduces SWE-RL, the first approach to scale RL-based LLM reasoning for\nreal-world software engineering. Leveraging a lightweight rule-based reward\n(e.g., the similarity score between ground-truth and LLM-generated solutions),\nSWE-RL enables LLMs to autonomously recover a developer's reasoning processes\nand solutions by learning from extensive open-source software evolution data --\nthe record of a software's entire lifecycle, including its code snapshots, code\nchanges, and events such as issues and pull requests. Trained on top of Llama\n3, our resulting reasoning model, Llama3-SWE-RL-70B, achieves a 41.0% solve\nrate on SWE-bench Verified -- a human-verified collection of real-world GitHub\nissues. To our knowledge, this is the best performance reported for\nmedium-sized (<100B) LLMs to date, even comparable to leading proprietary LLMs\nlike GPT-4o. Surprisingly, despite performing RL solely on software evolution\ndata, Llama3-SWE-RL has even emerged with generalized reasoning skills. For\nexample, it shows improved results on five out-of-domain tasks, namely,\nfunction coding, library use, code reasoning, mathematics, and general language\nunderstanding, whereas a supervised-finetuning baseline even leads to\nperformance degradation on average. Overall, SWE-RL opens up a new direction to\nimprove the reasoning capabilities of LLMs through reinforcement learning on\nmassive software engineering data.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18449v1",
    "published_date": "2025-02-25 18:45:04 UTC",
    "updated_date": "2025-02-25 18:45:04 UTC"
  },
  {
    "arxiv_id": "2502.18448v1",
    "title": "Disambiguate First Parse Later: Generating Interpretations for Ambiguity Resolution in Semantic Parsing",
    "authors": [
      "Irina Saparina",
      "Mirella Lapata"
    ],
    "abstract": "Handling ambiguity and underspecification is an important challenge in\nnatural language interfaces, particularly for tasks like text-to-SQL semantic\nparsing. We propose a modular approach that resolves ambiguity using natural\nlanguage interpretations before mapping these to logical forms (e.g., SQL\nqueries). Although LLMs excel at parsing unambiguous utterances, they show\nstrong biases for ambiguous ones, typically predicting only preferred\ninterpretations. We constructively exploit this bias to generate an initial set\nof preferred disambiguations and then apply a specialized infilling model to\nidentify and generate missing interpretations. To train the infilling model, we\nintroduce an annotation method that uses SQL execution to validate different\nmeanings. Our approach improves interpretation coverage and generalizes across\ndatasets with different annotation styles, database structures, and ambiguity\ntypes.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18448v1",
    "published_date": "2025-02-25 18:42:26 UTC",
    "updated_date": "2025-02-25 18:42:26 UTC"
  },
  {
    "arxiv_id": "2502.18439v1",
    "title": "MAPoRL: Multi-Agent Post-Co-Training for Collaborative Large Language Models with Reinforcement Learning",
    "authors": [
      "Chanwoo Park",
      "Seungju Han",
      "Xingzhi Guo",
      "Asuman Ozdaglar",
      "Kaiqing Zhang",
      "Joo-Kyung Kim"
    ],
    "abstract": "Leveraging multiple large language models (LLMs) to build collaborative\nmulti-agentic workflows has demonstrated significant potential. However, most\nprevious studies focus on prompting the out-of-the-box LLMs, relying on their\ninnate capability for collaboration, which may not improve LLMs' performance as\nshown recently. In this paper, we introduce a new post-training paradigm MAPoRL\n(Multi-Agent Post-co-training for collaborative LLMs with Reinforcement\nLearning), to explicitly elicit the collaborative behaviors and further unleash\nthe power of multi-agentic LLM frameworks. In MAPoRL, multiple LLMs first\ngenerate their own responses independently and engage in a multi-turn\ndiscussion to collaboratively improve the final answer. In the end, a MAPoRL\nverifier evaluates both the answer and the discussion, by assigning a score\nthat verifies the correctness of the answer, while adding incentives to\nencourage corrective and persuasive discussions. The score serves as the\nco-training reward, and is then maximized through multi-agent RL. Unlike\nexisting LLM post-training paradigms, MAPoRL advocates the co-training of\nmultiple LLMs together using RL for better generalization. Accompanied by\nanalytical insights, our experiments demonstrate that training individual LLMs\nalone is insufficient to induce effective collaboration. In contrast,\nmulti-agent co-training can boost the collaboration performance across\nbenchmarks, with generalization to unseen domains.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18439v1",
    "published_date": "2025-02-25 18:33:48 UTC",
    "updated_date": "2025-02-25 18:33:48 UTC"
  },
  {
    "arxiv_id": "2502.18438v2",
    "title": "ToMCAT: Theory-of-Mind for Cooperative Agents in Teams via Multiagent Diffusion Policies",
    "authors": [
      "Pedro Sequeira",
      "Vidyasagar Sadhu",
      "Melinda Gervasio"
    ],
    "abstract": "In this paper we present ToMCAT (Theory-of-Mind for Cooperative Agents in\nTeams), a new framework for generating ToM-conditioned trajectories. It\ncombines a meta-learning mechanism, that performs ToM reasoning over teammates'\nunderlying goals and future behavior, with a multiagent denoising-diffusion\nmodel, that generates plans for an agent and its teammates conditioned on both\nthe agent's goals and its teammates' characteristics, as computed via ToM. We\nimplemented an online planning system that dynamically samples new trajectories\n(replans) from the diffusion model whenever it detects a divergence between a\npreviously generated plan and the current state of the world. We conducted\nseveral experiments using ToMCAT in a simulated cooking domain. Our results\nhighlight the importance of the dynamic replanning mechanism in reducing the\nusage of resources without sacrificing team performance. We also show that\nrecent observations about the world and teammates' behavior collected by an\nagent over the course of an episode combined with ToM inferences are crucial to\ngenerate team-aware plans for dynamic adaptation to teammates, especially when\nno prior information is provided about them.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.MA",
    "comment": "Appears in Proc. of the Adaptive and Learning Agents Workshop (ALA\n  2025), ala-workshop.github.io",
    "pdf_url": "http://arxiv.org/pdf/2502.18438v2",
    "published_date": "2025-02-25 18:31:55 UTC",
    "updated_date": "2025-05-06 05:04:09 UTC"
  },
  {
    "arxiv_id": "2503.01867v1",
    "title": "Neural Manifolds and Cognitive Consistency: A New Approach to Memory Consolidation in Artificial Systems",
    "authors": [
      "Phuong-Nam Nguyen"
    ],
    "abstract": "We introduce a novel mathematical framework that unifies neural population\ndynamics, hippocampal sharp wave-ripple (SpWR) generation, and cognitive\nconsistency constraints inspired by Heider's theory. Our model leverages\nlow-dimensional manifold representations to capture structured neural drift and\nincorporates a balance energy function to enforce coherent synaptic\ninteractions, effectively simulating the memory consolidation processes\nobserved in biological systems. Simulation results demonstrate that our\napproach not only reproduces key features of SpWR events but also enhances\nnetwork interpretability. This work paves the way for scalable neuromorphic\narchitectures that bridge neuroscience and artificial intelligence, offering\nmore robust and adaptive learning mechanisms for future intelligent systems.",
    "categories": [
      "cs.AI",
      "cs.NE",
      "q-bio.NC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.01867v1",
    "published_date": "2025-02-25 18:28:25 UTC",
    "updated_date": "2025-02-25 18:28:25 UTC"
  },
  {
    "arxiv_id": "2502.18431v1",
    "title": "TextGames: Learning to Self-Play Text-Based Puzzle Games via Language Model Reasoning",
    "authors": [
      "Frederikus Hudi",
      "Genta Indra Winata",
      "Ruochen Zhang",
      "Alham Fikri Aji"
    ],
    "abstract": "Reasoning is a fundamental capability of large language models (LLMs),\nenabling them to comprehend, analyze, and solve complex problems. In this\npaper, we introduce TextGames, an innovative benchmark specifically crafted to\nassess LLMs through demanding text-based games that require advanced skills in\npattern recognition, spatial awareness, arithmetic, and logical reasoning. Our\nanalysis probes LLMs' performance in both single-turn and multi-turn reasoning,\nand their abilities in leveraging feedback to correct subsequent answers\nthrough self-reflection. Our findings reveal that, although LLMs exhibit\nproficiency in addressing most easy and medium-level problems, they face\nsignificant challenges with more difficult tasks. In contrast, humans are\ncapable of solving all tasks when given sufficient time. Moreover, we observe\nthat LLMs show improved performance in multi-turn predictions through\nself-reflection, yet they still struggle with sequencing, counting, and\nfollowing complex rules consistently. Additionally, models optimized for\nreasoning outperform pre-trained LLMs that prioritize instruction following,\nhighlighting the crucial role of reasoning skills in addressing highly complex\nproblems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18431v1",
    "published_date": "2025-02-25 18:26:48 UTC",
    "updated_date": "2025-02-25 18:26:48 UTC"
  },
  {
    "arxiv_id": "2502.18425v1",
    "title": "PyEvalAI: AI-assisted evaluation of Jupyter Notebooks for immediate personalized feedback",
    "authors": [
      "Nils Wandel",
      "David Stotko",
      "Alexander Schier",
      "Reinhard Klein"
    ],
    "abstract": "Grading student assignments in STEM courses is a laborious and repetitive\ntask for tutors, often requiring a week to assess an entire class. For\nstudents, this delay of feedback prevents iterating on incorrect solutions,\nhampers learning, and increases stress when exercise scores determine admission\nto the final exam. Recent advances in AI-assisted education, such as automated\ngrading and tutoring systems, aim to address these challenges by providing\nimmediate feedback and reducing grading workload. However, existing solutions\noften fall short due to privacy concerns, reliance on proprietary closed-source\nmodels, lack of support for combining Markdown, LaTeX and Python code, or\nexcluding course tutors from the grading process. To overcome these\nlimitations, we introduce PyEvalAI, an AI-assisted evaluation system, which\nautomatically scores Jupyter notebooks using a combination of unit tests and a\nlocally hosted language model to preserve privacy. Our approach is free,\nopen-source, and ensures tutors maintain full control over the grading process.\nA case study demonstrates its effectiveness in improving feedback speed and\ngrading efficiency for exercises in a university-level course on numerics.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18425v1",
    "published_date": "2025-02-25 18:20:20 UTC",
    "updated_date": "2025-02-25 18:20:20 UTC"
  },
  {
    "arxiv_id": "2502.18553v3",
    "title": "Applications of Statistical Field Theory in Deep Learning",
    "authors": [
      "Zohar Ringel",
      "Noa Rubin",
      "Edo Mor",
      "Moritz Helias",
      "Inbar Seroussi"
    ],
    "abstract": "Deep learning algorithms have made incredible strides in the past decade, yet\ndue to their complexity, the science of deep learning remains in its early\nstages. Being an experimentally driven field, it is natural to seek a theory of\ndeep learning within the physics paradigm. As deep learning is largely about\nlearning functions and distributions over functions, statistical field theory,\na rich and versatile toolbox for tackling complex distributions over functions\n(fields) is an obvious choice of formalism. Research efforts carried out in the\npast few years have demonstrated the ability of field theory to provide useful\ninsights on generalization, implicit bias, and feature learning effects. Here\nwe provide a pedagogical review of this emerging line of research.",
    "categories": [
      "stat.ML",
      "cond-mat.dis-nn",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18553v3",
    "published_date": "2025-02-25 18:19:06 UTC",
    "updated_date": "2025-04-17 07:48:20 UTC"
  },
  {
    "arxiv_id": "2502.18412v1",
    "title": "Comparative Analysis of MDL-VAE vs. Standard VAE on 202 Years of Gynecological Data",
    "authors": [
      "Paula Santos"
    ],
    "abstract": "This study presents a comparative evaluation of a Variational Autoencoder\n(VAE) enhanced with Minimum Description Length (MDL) regularization against a\nStandard Autoencoder for reconstructing high-dimensional gynecological data.\nThe MDL-VAE exhibits significantly lower reconstruction errors (MSE, MAE, RMSE)\nand more structured latent representations, driven by effective KL divergence\nregularization. Statistical analyses confirm these performance improvements are\nsignificant. Furthermore, the MDL-VAE shows consistent training and validation\nlosses and achieves efficient inference times, underscoring its robustness and\npractical viability. Our findings suggest that incorporating MDL principles\ninto VAE architectures can substantially improve data reconstruction and\ngeneralization, making it a promising approach for advanced applications in\nhealthcare data modeling and analysis.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pagas, 5 figures, 9th International Conference on Signal, Image\n  Processing (SIPO 2025), Vancouver CA",
    "pdf_url": "http://arxiv.org/pdf/2502.18412v1",
    "published_date": "2025-02-25 18:05:46 UTC",
    "updated_date": "2025-02-25 18:05:46 UTC"
  },
  {
    "arxiv_id": "2502.18410v2",
    "title": "TSKANMixer: Kolmogorov-Arnold Networks with MLP-Mixer Model for Time Series Forecasting",
    "authors": [
      "Young-Chae Hong",
      "Bei Xiao",
      "Yangho Chen"
    ],
    "abstract": "Time series forecasting has long been a focus of research across diverse\nfields, including economics, energy, healthcare, and traffic management. Recent\nworks have introduced innovative architectures for time series models, such as\nthe Time-Series Mixer (TSMixer), which leverages multi-layer perceptrons (MLPs)\nto enhance prediction accuracy by effectively capturing both spatial and\ntemporal dependencies within the data. In this paper, we investigate the\ncapabilities of the Kolmogorov-Arnold Networks (KANs) for time-series\nforecasting by modifying TSMixer with a KAN layer (TSKANMixer). Experimental\nresults demonstrate that TSKANMixer tends to improve prediction accuracy over\nthe original TSMixer across multiple datasets, ranking among the top-performing\nmodels compared to other time series approaches. Our results show that the KANs\nare promising alternatives to improve the performance of time series\nforecasting by replacing or extending traditional MLPs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 4 figures, 7 tables and accepted at the AI4TS: AI for Time\n  Series Analysis workshop, AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.18410v2",
    "published_date": "2025-02-25 18:04:45 UTC",
    "updated_date": "2025-03-27 16:34:13 UTC"
  },
  {
    "arxiv_id": "2502.18407v1",
    "title": "AgentRM: Enhancing Agent Generalization with Reward Modeling",
    "authors": [
      "Yu Xia",
      "Jingru Fan",
      "Weize Chen",
      "Siyu Yan",
      "Xin Cong",
      "Zhong Zhang",
      "Yaxi Lu",
      "Yankai Lin",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "abstract": "Existing LLM-based agents have achieved strong performance on held-in tasks,\nbut their generalizability to unseen tasks remains poor. Hence, some recent\nwork focus on fine-tuning the policy model with more diverse tasks to improve\nthe generalizability. In this work, we find that finetuning a reward model to\nguide the policy model is more robust than directly finetuning the policy\nmodel. Based on this finding, we propose AgentRM, a generalizable reward model,\nto guide the policy model for effective test-time search. We comprehensively\ninvestigate three approaches to construct the reward model, including explicit\nreward modeling, implicit reward modeling and LLM-as-a-judge. We then use\nAgentRM to guide the answer generation with Best-of-N sampling and step-level\nbeam search. On four types of nine agent tasks, AgentRM enhances the base\npolicy model by $8.8$ points on average, surpassing the top general agent by\n$4.0$. Moreover, it demonstrates weak-to-strong generalization, yielding\ngreater improvement of $12.6$ on LLaMA-3-70B policy model. As for the\nspecializability, AgentRM can also boost a finetuned policy model and\noutperform the top specialized agent by $11.4$ on three held-in tasks. Further\nanalysis verifies its effectiveness in test-time scaling. Codes will be\nreleased to facilitate the research in this area.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18407v1",
    "published_date": "2025-02-25 17:58:02 UTC",
    "updated_date": "2025-02-25 17:58:02 UTC"
  },
  {
    "arxiv_id": "2502.18406v1",
    "title": "The Gradient of Algebraic Model Counting",
    "authors": [
      "Jaron Maene",
      "Luc De Raedt"
    ],
    "abstract": "Algebraic model counting unifies many inference tasks on logic formulas by\nexploiting semirings. Rather than focusing on inference, we consider learning,\nespecially in statistical-relational and neurosymbolic AI, which combine\nlogical, probabilistic and neural representations. Concretely, we show that the\nvery same semiring perspective of algebraic model counting also applies to\nlearning. This allows us to unify various learning algorithms by generalizing\ngradients and backpropagation to different semirings. Furthermore, we show how\ncancellation and ordering properties of a semiring can be exploited for more\nmemory-efficient backpropagation. This allows us to obtain some interesting\nvariations of state-of-the-art gradient-based optimisation methods for\nprobabilistic logical models. We also discuss why algebraic model counting on\ntractable circuits does not lead to more efficient second-order optimization.\nEmpirically, our algebraic backpropagation exhibits considerable speed-ups as\ncompared to existing approaches.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Published at AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.18406v1",
    "published_date": "2025-02-25 17:57:55 UTC",
    "updated_date": "2025-02-25 17:57:55 UTC"
  },
  {
    "arxiv_id": "2502.18387v2",
    "title": "How Far are LLMs from Real Search? A Comprehensive Study on Efficiency, Completeness, and Inherent Capabilities",
    "authors": [
      "Minhua Lin",
      "Hui Liu",
      "Xianfeng Tang",
      "Jingying Zeng",
      "Zhenwei Dai",
      "Chen Luo",
      "Zheng Li",
      "Xiang Zhang",
      "Qi He",
      "Suhang Wang"
    ],
    "abstract": "Search plays a fundamental role in problem-solving across various domains,\nwith most real-world decision-making problems being solvable through systematic\nsearch. Drawing inspiration from recent discussions on search and learning, we\nsystematically explore the complementary relationship between search and Large\nLanguage Models (LLMs) from three perspectives. First, we analyze how learning\ncan enhance search efficiency and propose Search via Learning (SeaL), a\nframework that leverages LLMs for effective and efficient search. Second, we\nfurther extend SeaL to SeaL-C to ensure rigorous completeness during search.\nOur evaluation across three real-world planning tasks demonstrates that SeaL\nachieves near-perfect accuracy while reducing search spaces by up to 99.1%\ncompared to traditional approaches. Finally, we explore how far LLMs are from\nreal search by investigating whether they can develop search capabilities\nindependently. Our analysis reveals that while current LLMs struggle with\nefficient search in complex problems, incorporating systematic search\nstrategies significantly enhances their problem-solving capabilities. These\nfindings not only validate the effectiveness of our approach but also highlight\nthe need for improving LLMs' search abilities for real-world applications.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "31 pages, 9 figures, 18 tables",
    "pdf_url": "http://arxiv.org/pdf/2502.18387v2",
    "published_date": "2025-02-25 17:30:40 UTC",
    "updated_date": "2025-02-26 06:40:18 UTC"
  },
  {
    "arxiv_id": "2502.18373v1",
    "title": "EgoSim: An Egocentric Multi-view Simulator and Real Dataset for Body-worn Cameras during Motion and Activity",
    "authors": [
      "Dominik Hollidt",
      "Paul Streli",
      "Jiaxi Jiang",
      "Yasaman Haghighi",
      "Changlin Qian",
      "Xintong Liu",
      "Christian Holz"
    ],
    "abstract": "Research on egocentric tasks in computer vision has mostly focused on\nhead-mounted cameras, such as fisheye cameras or embedded cameras inside\nimmersive headsets. We argue that the increasing miniaturization of optical\nsensors will lead to the prolific integration of cameras into many more\nbody-worn devices at various locations. This will bring fresh perspectives to\nestablished tasks in computer vision and benefit key areas such as human motion\ntracking, body pose estimation, or action recognition -- particularly for the\nlower body, which is typically occluded.\n  In this paper, we introduce EgoSim, a novel simulator of body-worn cameras\nthat generates realistic egocentric renderings from multiple perspectives\nacross a wearer's body. A key feature of EgoSim is its use of real motion\ncapture data to render motion artifacts, which are especially noticeable with\narm- or leg-worn cameras. In addition, we introduce MultiEgoView, a dataset of\negocentric footage from six body-worn cameras and ground-truth full-body 3D\nposes during several activities: 119 hours of data are derived from AMASS\nmotion sequences in four high-fidelity virtual environments, which we augment\nwith 5 hours of real-world motion data from 13 participants using six GoPro\ncameras and 3D body pose references from an Xsens motion capture suit.\n  We demonstrate EgoSim's effectiveness by training an end-to-end video-only 3D\npose estimation network. Analyzing its domain gap, we show that our dataset and\nsimulator substantially aid training for inference on real-world data.\n  EgoSim code & MultiEgoView dataset: https://siplab.org/projects/EgoSim",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18373v1",
    "published_date": "2025-02-25 17:11:14 UTC",
    "updated_date": "2025-02-25 17:11:14 UTC"
  },
  {
    "arxiv_id": "2502.18371v1",
    "title": "MindMem: Multimodal for Predicting Advertisement Memorability Using LLMs and Deep Learning",
    "authors": [
      "Sepehr Asgarian",
      "Qayam Jetha",
      "Jouhyun Jeon"
    ],
    "abstract": "In the competitive landscape of advertising, success hinges on effectively\nnavigating and leveraging complex interactions among consumers, advertisers,\nand advertisement platforms. These multifaceted interactions compel advertisers\nto optimize strategies for modeling consumer behavior, enhancing brand recall,\nand tailoring advertisement content. To address these challenges, we present\nMindMem, a multimodal predictive model for advertisement memorability. By\nintegrating textual, visual, and auditory data, MindMem achieves\nstate-of-the-art performance, with a Spearman's correlation coefficient of\n0.631 on the LAMBDA and 0.731 on the Memento10K dataset, consistently\nsurpassing existing methods. Furthermore, our analysis identified key factors\ninfluencing advertisement memorability, such as video pacing, scene complexity,\nand emotional resonance. Expanding on this, we introduced MindMem-ReAd\n(MindMem-Driven Re-generated Advertisement), which employs Large Language\nModel-based simulations to optimize advertisement content and placement,\nresulting in up to a 74.12% improvement in advertisement memorability. Our\nresults highlight the transformative potential of Artificial Intelligence in\nadvertising, offering advertisers a robust tool to drive engagement, enhance\ncompetitiveness, and maximize impact in a rapidly evolving market.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "7 pages, 5 figures, 4 Tables, AAAI 2025 Economics of Modern ML:\n  Markets, Incentives, and Generative AI Workshop",
    "pdf_url": "http://arxiv.org/pdf/2502.18371v1",
    "published_date": "2025-02-25 17:09:12 UTC",
    "updated_date": "2025-02-25 17:09:12 UTC"
  },
  {
    "arxiv_id": "2502.18357v1",
    "title": "Which Contributions Deserve Credit? Perceptions of Attribution in Human-AI Co-Creation",
    "authors": [
      "Jessica He",
      "Stephanie Houde",
      "Justin D. Weisz"
    ],
    "abstract": "AI systems powered by large language models can act as capable assistants for\nwriting and editing. In these tasks, the AI system acts as a co-creative\npartner, making novel contributions to an artifact-under-creation alongside its\nhuman partner(s). One question that arises in these scenarios is the extent to\nwhich AI should be credited for its contributions. We examined knowledge\nworkers' views of attribution through a survey study (N=155) and found that\nthey assigned different levels of credit across different contribution types,\namounts, and initiative. Compared to a human partner, we observed a consistent\npattern in which AI was assigned less credit for equivalent contributions.\nParticipants felt that disclosing AI involvement was important and used a\nvariety of criteria to make attribution judgments, including the quality of\ncontributions, personal values, and technology considerations. Our results\nmotivate and inform new approaches for crediting AI contributions to co-created\nwork.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "30 pages, 5 figures. In CHI Conference on Human Factors in Computing\n  Systems (CHI '25), April 26-May 1, 2025, Yokohama, Japan",
    "pdf_url": "http://arxiv.org/pdf/2502.18357v1",
    "published_date": "2025-02-25 16:48:10 UTC",
    "updated_date": "2025-02-25 16:48:10 UTC"
  },
  {
    "arxiv_id": "2503.05770v1",
    "title": "Generative Artificial Intelligence: Evolving Technology, Growing Societal Impact, and Opportunities for Information Systems Research",
    "authors": [
      "Veda C. Storey",
      "Wei Thoo Yue",
      "J. Leon Zhao",
      "Roman Lukyanenko"
    ],
    "abstract": "The continuing, explosive developments in generative artificial intelligence\n(GenAI), built on large language models and related algorithms, has led to much\nexcitement and speculation about the potential impact of this new technology.\nClaims include AI being poised to revolutionize business and society and\ndramatically change personal life. However, it remains unclear exactly how this\ntechnology, with its significantly distinct features from past AI technologies,\nhas transformative potential. Nor is it clear how researchers in information\nsystems (IS) should respond. In this paper, we consider the evolving and\nemerging trends of AI in order to examine its present and predict its future\nimpacts. Many existing papers on GenAI are either too technical for most IS\nresearchers or lack the depth needed to appreciate the potential impacts of\nGenAI. We, therefore, attempt to bridge the technical and organizational\ncommunities of GenAI from a system-oriented sociotechnical perspective.\nSpecifically, we explore the unique features of GenAI, which are rooted in the\ncontinued change from symbolism to connectionism, and the deep systemic and\ninherent properties of human-AI ecosystems. We retrace the evolution of AI that\nproceeded the level of adoption, adaption, and use found today, in order to\npropose future research on various impacts of GenAI in both business and\nsociety within the context of information systems research. Our efforts are\nintended to contribute to the creation of a well-structured research agenda in\nthe IS community to support innovative strategies and operations enabled by\nthis new wave of AI.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05770v1",
    "published_date": "2025-02-25 16:34:23 UTC",
    "updated_date": "2025-02-25 16:34:23 UTC"
  },
  {
    "arxiv_id": "2502.18328v1",
    "title": "From Vision to Sound: Advancing Audio Anomaly Detection with Vision-Based Algorithms",
    "authors": [
      "Manuel Barusco",
      "Francesco Borsatti",
      "Davide Dalle Pezze",
      "Francesco Paissan",
      "Elisabetta Farella",
      "Gian Antonio Susto"
    ],
    "abstract": "Recent advances in Visual Anomaly Detection (VAD) have introduced\nsophisticated algorithms leveraging embeddings generated by pre-trained feature\nextractors. Inspired by these developments, we investigate the adaptation of\nsuch algorithms to the audio domain to address the problem of Audio Anomaly\nDetection (AAD). Unlike most existing AAD methods, which primarily classify\nanomalous samples, our approach introduces fine-grained temporal-frequency\nlocalization of anomalies within the spectrogram, significantly improving\nexplainability. This capability enables a more precise understanding of where\nand when anomalies occur, making the results more actionable for end users. We\nevaluate our approach on industrial and environmental benchmarks, demonstrating\nthe effectiveness of VAD techniques in detecting anomalies in audio signals.\nMoreover, they improve explainability by enabling localized anomaly\nidentification, making audio anomaly detection systems more interpretable and\npractical.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CV",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18328v1",
    "published_date": "2025-02-25 16:22:42 UTC",
    "updated_date": "2025-02-25 16:22:42 UTC"
  },
  {
    "arxiv_id": "2502.18315v1",
    "title": "GraphRank Pro+: Advancing Talent Analytics Through Knowledge Graphs and Sentiment-Enhanced Skill Profiling",
    "authors": [
      "Sirisha Velampalli",
      "Chandrashekar Muniyappa"
    ],
    "abstract": "The extraction of information from semi-structured text, such as resumes, has\nlong been a challenge due to the diverse formatting styles and subjective\ncontent organization. Conventional solutions rely on specialized logic tailored\nfor specific use cases. However, we propose a revolutionary approach leveraging\nstructured Graphs, Natural Language Processing (NLP), and Deep Learning. By\nabstracting intricate logic into Graph structures, we transform raw data into a\ncomprehensive Knowledge Graph. This innovative framework enables precise\ninformation extraction and sophisticated querying. We systematically construct\ndictionaries assigning skill weights, paving the way for nuanced talent\nanalysis. Our system not only benefits job recruiters and curriculum designers\nbut also empowers job seekers with targeted query-based filtering and ranking\ncapabilities.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "05C81",
      "I.2.7"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18315v1",
    "published_date": "2025-02-25 16:07:40 UTC",
    "updated_date": "2025-02-25 16:07:40 UTC"
  },
  {
    "arxiv_id": "2502.18548v3",
    "title": "What is the Alignment Objective of GRPO?",
    "authors": [
      "Milan Vojnovic",
      "Se-Young Yun"
    ],
    "abstract": "In this note, we examine the aggregation of preferences achieved by the Group\nPolicy Optimisation (GRPO) algorithm, a reinforcement learning method used to\ntrain advanced artificial intelligence models such as DeepSeek-R1-Zero and\nDeepSeekMath. The GRPO algorithm trains a policy using a reward preference\nmodel, which is computed by sampling a set of outputs for a given context,\nobserving the corresponding rewards, and applying shift-and-scale normalisation\nto these reward values. Additionally, it incorporates a penalty function to\ndiscourage deviations from a reference policy.\n  We present a framework that enables us to characterise the stationary\npolicies of the GRPO algorithm. This analysis reveals that the aggregation of\npreferences differs fundamentally from standard logarithmic pooling, which is\nimplemented by other approaches such as RLHF. The precise form of preference\naggregation arises from the way the reward preference model is defined and from\nthe penalty function, which we show to essentially correspond to the reverse\nKullback-Leibler (KL) divergence between the aggregation policy and the\nreference policy.\n  Interestingly, we demonstrate that for groups of size two, the reward\npreference model corresponds to pairwise comparison preferences, similar to\nthose in other alignment methods based on pairwise comparison feedback. We\nprovide explicit characterisations of the aggregate preference for binary\nquestions, for groups of size two, and in the limit of large group size. This\nprovides insights into the dependence of the aggregate preference on parameters\nsuch as the regularisation constant and the confidence margin of question\nanswers.\n  Finally, we discuss the aggregation of preferences obtained by modifying the\nGRPO algorithm to use direct KL divergence as the penalty or to use rewards\nwithout scale normalisation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18548v3",
    "published_date": "2025-02-25 15:56:56 UTC",
    "updated_date": "2025-03-13 16:48:34 UTC"
  },
  {
    "arxiv_id": "2502.18547v2",
    "title": "Steganography Beyond Space-Time with Chain of Multimodal AI",
    "authors": [
      "Ching-Chun Chang",
      "Isao Echizen"
    ],
    "abstract": "Steganography is the art and science of covert writing, with a broad range of\napplications interwoven within the realm of cybersecurity. As artificial\nintelligence continues to evolve, its ability to synthesise realistic content\nemerges as a threat in the hands of cybercriminals who seek to manipulate and\nmisrepresent the truth. Such synthetic content introduces a non-trivial risk of\noverwriting the subtle changes made for the purpose of steganography. When the\nsignals in both the spatial and temporal domains are vulnerable to unforeseen\noverwriting, it calls for reflection on what, if any, remains invariant. This\nstudy proposes a paradigm in steganography for audiovisual media, where\nmessages are concealed beyond both spatial and temporal domains. A chain of\nmultimodal artificial intelligence is developed to deconstruct audiovisual\ncontent into a cover text, embed a message within the linguistic domain, and\nthen reconstruct the audiovisual content through synchronising both auditory\nand visual modalities with the resultant stego text. The message is encoded by\nbiasing the word sampling process of a language generation model and decoded by\nanalysing the probability distribution of word choices. The accuracy of message\ntransmission is evaluated under both zero-bit and multi-bit capacity settings.\nFidelity is assessed through both biometric and semantic similarities,\ncapturing the identities of the recorded face and voice, as well as the core\nideas conveyed through the media. Secrecy is examined through statistical\ncomparisons between cover and stego texts. Robustness is tested across various\nscenarios, including audiovisual resampling, face-swapping, voice-cloning and\ntheir combinations.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.MA",
      "cs.MM"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18547v2",
    "published_date": "2025-02-25 15:56:09 UTC",
    "updated_date": "2025-04-20 02:50:48 UTC"
  },
  {
    "arxiv_id": "2502.18298v1",
    "title": "Smart and Efficient IoT-Based Irrigation System Design: Utilizing a Hybrid Agent-Based and System Dynamics Approach",
    "authors": [
      "Taha Ahmadi Pargo",
      "Mohsen Akbarpour Shirazi",
      "Dawud Fadai"
    ],
    "abstract": "Regarding problems like reduced precipitation and an increase in population,\nwater resource scarcity has become one of the most critical problems in\nmodern-day societies, as a consequence, there is a shortage of available water\nresources for irrigation in arid and semi-arid countries. On the other hand, it\nis possible to utilize modern technologies to control irrigation and reduce\nwater loss. One of these technologies is the Internet of Things (IoT). Despite\nthe possibility of using the IoT in irrigation control systems, there are\ncomplexities in designing such systems. Considering this issue, it is possible\nto use agent-oriented software engineering (AOSE) methodologies to design\ncomplex cyber-physical systems such as IoT-based systems. In this research, a\nsmart irrigation system is designed based on Prometheus AOSE methodology, to\nreduce water loss by maintaining soil moisture in a suitable interval. The\ndesigned system comprises sensors, a central agent, and irrigation nodes. These\nagents follow defined rules to maintain soil moisture at a desired level\ncooperatively. For system simulation, a hybrid agent-based and system dynamics\nmodel was designed. In this hybrid model, soil moisture dynamics were modeled\nbased on the system dynamics approach. The proposed model, was implemented in\nAnyLogic computer simulation software. Utilizing the simulation model,\nirrigation rules were examined. The system's functionality in automatic\nirrigation mode was tested based on a 256-run, fractional factorial design, and\nthe effects of important factors such as soil properties on total irrigated\nwater and total operation time were analyzed. Based on the tests, the system\nconsistently irrigated nearly optimal water amounts in all tests. Moreover, the\nresults were also used to minimize the system's energy consumption by reducing\nthe system's operational time.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.SY",
      "eess.SY",
      "stat.AP",
      "I.6.6, I.2.1, J.2"
    ],
    "primary_category": "cs.MA",
    "comment": "50 pages, 22 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.18298v1",
    "published_date": "2025-02-25 15:34:38 UTC",
    "updated_date": "2025-02-25 15:34:38 UTC"
  },
  {
    "arxiv_id": "2502.18296v1",
    "title": "Mixing Any Cocktail with Limited Ingredients: On the Structure of Payoff Sets in Multi-Objective MDPs and its Impact on Randomised Strategies",
    "authors": [
      "James C. A. Main",
      "Mickael Randour"
    ],
    "abstract": "We consider multi-dimensional payoff functions in Markov decision processes,\nand ask whether a given expected payoff vector can be achieved or not. In\ngeneral, pure strategies (i.e., not resorting to randomisation) do not suffice\nfor this problem.\n  We study the structure of the set of expected payoff vectors of all\nstrategies given a multi-dimensional payoff function and its consequences\nregarding randomisation requirements for strategies. In particular, we prove\nthat for any payoff for which the expectation is well-defined under all\nstrategies, it is sufficient to mix (i.e., randomly select a pure strategy at\nthe start of a play and committing to it for the rest of the play) finitely\nmany pure strategies to approximate any expected payoff vector up to any\nprecision. Furthermore, for any payoff for which the expected payoff is finite\nunder all strategies, any expected payoff can be obtained exactly by mixing\nfinitely many strategies.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.FL",
      "cs.LO",
      "math.PR"
    ],
    "primary_category": "cs.GT",
    "comment": "64 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.18296v1",
    "published_date": "2025-02-25 15:33:59 UTC",
    "updated_date": "2025-02-25 15:33:59 UTC"
  },
  {
    "arxiv_id": "2502.18293v1",
    "title": "AMPO: Active Multi-Preference Optimization",
    "authors": [
      "Taneesh Gupta",
      "Rahul Madhavan",
      "Xuchao Zhang",
      "Chetan Bansal",
      "Saravan Rajmohan"
    ],
    "abstract": "Multi-preference optimization enriches language-model alignment beyond\npairwise preferences by contrasting entire sets of helpful and undesired\nresponses, thereby enabling richer training signals for large language models.\nDuring self-play alignment, these models often produce numerous candidate\nanswers per query, rendering it computationally infeasible to include all\nresponses in the training objective. In this work, we propose $\\textit{Active\nMulti-Preference Optimization}$ (AMPO), a novel approach that combines\non-policy generation, a multi-preference group-contrastive loss, and active\nsubset selection. Specifically, we score and embed large candidate pools of\nresponses and then select a small, yet informative, subset that covers reward\nextremes and distinct semantic clusters for preference optimization. Our\ncontrastive training scheme is capable of identifying not only the best and\nworst answers but also subtle, underexplored modes that are crucial for robust\nalignment. Theoretically, we provide guarantees for expected reward\nmaximization using our active selection method, and empirically, AMPO achieves\nstate-of-the-art results on $\\textit{AlpacaEval}$ using Llama 8B.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18293v1",
    "published_date": "2025-02-25 15:29:51 UTC",
    "updated_date": "2025-02-25 15:29:51 UTC"
  },
  {
    "arxiv_id": "2502.18274v2",
    "title": "Citrus: Leveraging Expert Cognitive Pathways in a Medical Language Model for Advanced Medical Decision Support",
    "authors": [
      "Guoxin Wang",
      "Minyu Gao",
      "Shuai Yang",
      "Ya Zhang",
      "Lizhi He",
      "Liang Huang",
      "Hanlin Xiao",
      "Yexuan Zhang",
      "Wanyue Li",
      "Lu Chen",
      "Jintao Fei",
      "Xin Li"
    ],
    "abstract": "Large language models (LLMs), particularly those with reasoning capabilities,\nhave rapidly advanced in recent years, demonstrating significant potential\nacross a wide range of applications. However, their deployment in healthcare,\nespecially in disease reasoning tasks, is hindered by the challenge of\nacquiring expert-level cognitive data. In this paper, we introduce Citrus, a\nmedical language model that bridges the gap between clinical expertise and AI\nreasoning by emulating the cognitive processes of medical experts. The model is\ntrained on a large corpus of simulated expert disease reasoning data,\nsynthesized using a novel approach that accurately captures the decision-making\npathways of clinicians. This approach enables Citrus to better simulate the\ncomplex reasoning processes involved in diagnosing and treating medical\nconditions. To further address the lack of publicly available datasets for\nmedical reasoning tasks, we release the last-stage training data, including a\ncustom-built medical diagnostic dialogue dataset. This open-source contribution\naims to support further research and development in the field. Evaluations\nusing authoritative benchmarks such as MedQA, covering tasks in medical\nreasoning and language understanding, show that Citrus achieves superior\nperformance compared to other models of similar size. These results highlight\nCitrus potential to significantly enhance medical decision support systems,\nproviding a more accurate and efficient tool for clinical decision-making.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18274v2",
    "published_date": "2025-02-25 15:05:12 UTC",
    "updated_date": "2025-02-26 02:50:52 UTC"
  },
  {
    "arxiv_id": "2502.18545v1",
    "title": "PII-Bench: Evaluating Query-Aware Privacy Protection Systems",
    "authors": [
      "Hao Shen",
      "Zhouhong Gu",
      "Haokai Hong",
      "Weili Han"
    ],
    "abstract": "The widespread adoption of Large Language Models (LLMs) has raised\nsignificant privacy concerns regarding the exposure of personally identifiable\ninformation (PII) in user prompts. To address this challenge, we propose a\nquery-unrelated PII masking strategy and introduce PII-Bench, the first\ncomprehensive evaluation framework for assessing privacy protection systems.\nPII-Bench comprises 2,842 test samples across 55 fine-grained PII categories,\nfeaturing diverse scenarios from single-subject descriptions to complex\nmulti-party interactions. Each sample is carefully crafted with a user query,\ncontext description, and standard answer indicating query-relevant PII. Our\nempirical evaluation reveals that while current models perform adequately in\nbasic PII detection, they show significant limitations in determining PII query\nrelevance. Even state-of-the-art LLMs struggle with this task, particularly in\nhandling complex multi-subject scenarios, indicating substantial room for\nimprovement in achieving intelligent PII masking.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18545v1",
    "published_date": "2025-02-25 14:49:08 UTC",
    "updated_date": "2025-02-25 14:49:08 UTC"
  },
  {
    "arxiv_id": "2502.18220v1",
    "title": "UASTrack: A Unified Adaptive Selection Framework with Modality-Customization in Single Object Tracking",
    "authors": [
      "He Wang",
      "Tianyang Xu",
      "Zhangyong Tang",
      "Xiao-Jun Wu",
      "Josef Kittler"
    ],
    "abstract": "Multi-modal tracking is essential in single-object tracking (SOT), as\ndifferent sensor types contribute unique capabilities to overcome challenges\ncaused by variations in object appearance. However, existing unified RGB-X\ntrackers (X represents depth, event, or thermal modality) either rely on the\ntask-specific training strategy for individual RGB-X image pairs or fail to\naddress the critical importance of modality-adaptive perception in real-world\napplications. In this work, we propose UASTrack, a unified adaptive selection\nframework that facilitates both model and parameter unification, as well as\nadaptive modality discrimination across various multi-modal tracking tasks. To\nachieve modality-adaptive perception in joint RGB-X pairs, we design a\nDiscriminative Auto-Selector (DAS) capable of identifying modality labels,\nthereby distinguishing the data distributions of auxiliary modalities.\nFurthermore, we propose a Task-Customized Optimization Adapter (TCOA) tailored\nto various modalities in the latent space. This strategy effectively filters\nnoise redundancy and mitigates background interference based on the specific\ncharacteristics of each modality. Extensive comparisons conducted on five\nbenchmarks including LasHeR, GTOT, RGBT234, VisEvent, and DepthTrack, covering\nRGB-T, RGB-E, and RGB-D tracking scenarios, demonstrate our innovative approach\nachieves comparative performance by introducing only additional training\nparameters of 1.87M and flops of 1.95G. The code will be available at\nhttps://github.com/wanghe/UASTrack.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18220v1",
    "published_date": "2025-02-25 14:04:31 UTC",
    "updated_date": "2025-02-25 14:04:31 UTC"
  },
  {
    "arxiv_id": "2502.18218v3",
    "title": "FLARE: A Framework for Stellar Flare Forecasting using Stellar Physical Properties and Historical Records",
    "authors": [
      "Bingke Zhu",
      "Xiaoxiao Wang",
      "Minghui Jia",
      "Yihan Tao",
      "Xiao Kong",
      "Ali Luo",
      "Yingying Chen",
      "Ming Tang",
      "Jinqiao Wang"
    ],
    "abstract": "Stellar flare events are critical observational samples for astronomical\nresearch; however, recorded flare events remain limited. Stellar flare\nforecasting can provide additional flare event samples to support research\nefforts. Despite this potential, no specialized models for stellar flare\nforecasting have been proposed to date. In this paper, we present extensive\nexperimental evidence demonstrating that both stellar physical properties and\nhistorical flare records are valuable inputs for flare forecasting tasks. We\nthen introduce FLARE (Forecasting Light-curve-based Astronomical Records via\nfeatures Ensemble), the first-of-its-kind large model specifically designed for\nstellar flare forecasting. FLARE integrates stellar physical properties and\nhistorical flare records through a novel Soft Prompt Module and Residual Record\nFusion Module. Our experiments on the publicly available Kepler light curve\ndataset demonstrate that FLARE achieves superior performance compared to other\nmethods across all evaluation metrics. Finally, we validate the forecast\ncapability of our model through a comprehensive case study.",
    "categories": [
      "astro-ph.SR",
      "astro-ph.IM",
      "cs.AI"
    ],
    "primary_category": "astro-ph.SR",
    "comment": "Accepted by IJCAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.18218v3",
    "published_date": "2025-02-25 14:03:15 UTC",
    "updated_date": "2025-05-22 10:37:00 UTC"
  },
  {
    "arxiv_id": "2502.18209v1",
    "title": "LAG: LLM agents for Leaderboard Auto Generation on Demanding",
    "authors": [
      "Jian Wu",
      "Jiayu Zhang",
      "Dongyuan Li",
      "Linyi Yang",
      "Aoxiao Zhong",
      "Renhe Jiang",
      "Qingsong Wen",
      "Yue Zhang"
    ],
    "abstract": "This paper introduces Leaderboard Auto Generation (LAG), a novel and\nwell-organized framework for automatic generation of leaderboards on a given\nresearch topic in rapidly evolving fields like Artificial Intelligence (AI).\nFaced with a large number of AI papers updated daily, it becomes difficult for\nresearchers to track every paper's proposed methods, experimental results, and\nsettings, prompting the need for efficient automatic leaderboard construction.\nWhile large language models (LLMs) offer promise in automating this process,\nchallenges such as multi-document summarization, leaderboard generation, and\nexperiment fair comparison still remain under exploration. LAG solves these\nchallenges through a systematic approach that involves the paper collection,\nexperiment results extraction and integration, leaderboard generation, and\nquality evaluation. Our contributions include a comprehensive solution to the\nleaderboard construction problem, a reliable evaluation method, and\nexperimental results showing the high quality of leaderboards.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18209v1",
    "published_date": "2025-02-25 13:54:03 UTC",
    "updated_date": "2025-02-25 13:54:03 UTC"
  },
  {
    "arxiv_id": "2502.18202v1",
    "title": "DenoMAE2.0: Improving Denoising Masked Autoencoders by Classifying Local Patches",
    "authors": [
      "Atik Faysal",
      "Mohammad Rostami",
      "Taha Boushine",
      "Reihaneh Gh. Roshan",
      "Huaxia Wang",
      "Nikhil Muralidhar"
    ],
    "abstract": "We introduce DenoMAE2.0, an enhanced denoising masked autoencoder that\nintegrates a local patch classification objective alongside traditional\nreconstruction loss to improve representation learning and robustness. Unlike\nconventional Masked Autoencoders (MAE), which focus solely on reconstructing\nmissing inputs, DenoMAE2.0 introduces position-aware classification of unmasked\npatches, enabling the model to capture fine-grained local features while\nmaintaining global coherence. This dual-objective approach is particularly\nbeneficial in semi-supervised learning for wireless communication, where high\nnoise levels and data scarcity pose significant challenges. We conduct\nextensive experiments on modulation signal classification across a wide range\nof signal-to-noise ratios (SNRs), from extremely low to moderately high\nconditions and in a low data regime. Our results demonstrate that DenoMAE2.0\nsurpasses its predecessor, Deno-MAE, and other baselines in both denoising\nquality and downstream classification accuracy. DenoMAE2.0 achieves a 1.1%\nimprovement over DenoMAE on our dataset and 11.83%, 16.55% significant improved\naccuracy gains on the RadioML benchmark, over DenoMAE, for constellation\ndiagram classification of modulation signals.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18202v1",
    "published_date": "2025-02-25 13:41:56 UTC",
    "updated_date": "2025-02-25 13:41:56 UTC"
  },
  {
    "arxiv_id": "2502.18185v3",
    "title": "VesselSAM: Leveraging SAM for Aortic Vessel Segmentation with LoRA and Atrous Attention",
    "authors": [
      "Adnan Iltaf",
      "Rayan Merghani Ahmed",
      "Zhenxi Zhang",
      "Bin Li",
      "Shoujun Zhou"
    ],
    "abstract": "Medical image segmentation is crucial for clinical diagnosis and treatment\nplanning, especially when dealing with complex anatomical structures such as\nvessels. However, accurately segmenting vessels remains challenging due to\ntheir small size, intricate edge structures, and susceptibility to artifacts\nand imaging noise. In this work, we propose VesselSAM, an enhanced version of\nthe Segment Anything Model (SAM), specifically tailored for aortic vessel\nsegmentation. VesselSAM incorporates AtrousLoRA, a novel module integrating\nAtrous Attention and Low-Rank Adaptation (LoRA), to enhance segmentation\nperformance. Atrous Attention enables the model to capture multi-scale\ncontextual information, preserving both fine-grained local details and broader\nglobal context. Additionally, LoRA facilitates efficient fine-tuning of the\nfrozen SAM image encoder, reducing the number of trainable parameters and\nthereby enhancing computational efficiency. We evaluate VesselSAM using two\nchallenging datasets: the Aortic Vessel Tree (AVT) dataset and the Type-B\nAortic Dissection (TBAD) dataset. VesselSAM achieves state-of-the-art\nperformance, attaining DSC scores of 93.50\\%, 93.25\\%, 93.02\\%, and 93.26\\%\nacross multi-center datasets. Our results demonstrate that VesselSAM delivers\nhigh segmentation accuracy while significantly reducing computational overhead\ncompared to existing large-scale models. This development paves the way for\nenhanced AI-based aortic vessel segmentation in clinical environments. The code\nand models will be released at https://github.com/Adnan-CAS/AtrousLora.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2502.18185v3",
    "published_date": "2025-02-25 13:26:06 UTC",
    "updated_date": "2025-03-26 06:10:48 UTC"
  },
  {
    "arxiv_id": "2502.18180v2",
    "title": "ChatMotion: A Multimodal Multi-Agent for Human Motion Analysis",
    "authors": [
      "Lei Li",
      "Sen Jia",
      "Jianhao Wang",
      "Zhaochong An",
      "Jiaang Li",
      "Jenq-Neng Hwang",
      "Serge Belongie"
    ],
    "abstract": "Advancements in Multimodal Large Language Models (MLLMs) have improved human\nmotion understanding. However, these models remain constrained by their\n\"instruct-only\" nature, lacking interactivity and adaptability for diverse\nanalytical perspectives. To address these challenges, we introduce ChatMotion,\na multimodal multi-agent framework for human motion analysis. ChatMotion\ndynamically interprets user intent, decomposes complex tasks into meta-tasks,\nand activates specialized function modules for motion comprehension. It\nintegrates multiple specialized modules, such as the MotionCore, to analyze\nhuman motion from various perspectives. Extensive experiments demonstrate\nChatMotion's precision, adaptability, and user engagement for human motion\nunderstanding.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18180v2",
    "published_date": "2025-02-25 13:12:55 UTC",
    "updated_date": "2025-02-27 13:55:34 UTC"
  },
  {
    "arxiv_id": "2502.18179v1",
    "title": "Problem Solved? Information Extraction Design Space for Layout-Rich Documents using LLMs",
    "authors": [
      "Gaye Colakoglu",
      "Gürkan Solmaz",
      "Jonathan Fürst"
    ],
    "abstract": "This paper defines and explores the design space for information extraction\n(IE) from layout-rich documents using large language models (LLMs). The three\ncore challenges of layout-aware IE with LLMs are 1) data structuring, 2) model\nengagement, and 3) output refinement. Our study delves into the sub-problems\nwithin these core challenges, such as input representation, chunking,\nprompting, and selection of LLMs and multimodal models. It examines the\noutcomes of different design choices through a new layout-aware IE test suite,\nbenchmarking against the state-of-art (SoA) model LayoutLMv3. The results show\nthat the configuration from one-factor-at-a-time (OFAT) trial achieves\nnear-optimal results with 14.1 points F1-score gain from the baseline model,\nwhile full factorial exploration yields only a slightly higher 15.1 points gain\nat around 36x greater token usage. We demonstrate that well-configured\ngeneral-purpose LLMs can match the performance of specialized models, providing\na cost-effective alternative. Our test-suite is freely available at\nhttps://github.com/gayecolakoglu/LayIE-LLM.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18179v1",
    "published_date": "2025-02-25 13:11:53 UTC",
    "updated_date": "2025-02-25 13:11:53 UTC"
  },
  {
    "arxiv_id": "2502.18176v2",
    "title": "CLIPure: Purification in Latent Space via CLIP for Adversarially Robust Zero-Shot Classification",
    "authors": [
      "Mingkun Zhang",
      "Keping Bi",
      "Wei Chen",
      "Jiafeng Guo",
      "Xueqi Cheng"
    ],
    "abstract": "In this paper, we aim to build an adversarially robust zero-shot image\nclassifier. We ground our work on CLIP, a vision-language pre-trained encoder\nmodel that can perform zero-shot classification by matching an image with text\nprompts ``a photo of a <class-name>.''. Purification is the path we choose\nsince it does not require adversarial training on specific attack types and\nthus can cope with any foreseen attacks. We then formulate purification risk as\nthe KL divergence between the joint distributions of the purification process\nof denoising the adversarial samples and the attack process of adding\nperturbations to benign samples, through bidirectional Stochastic Differential\nEquations (SDEs). The final derived results inspire us to explore purification\nin the multi-modal latent space of CLIP. We propose two variants for our\nCLIPure approach: CLIPure-Diff which models the likelihood of images' latent\nvectors with the DiffusionPrior module in DaLLE-2 (modeling the generation\nprocess of CLIP's latent vectors), and CLIPure-Cos which models the likelihood\nwith the cosine similarity between the embeddings of an image and ``a photo of\na.''. As far as we know, CLIPure is the first purification method in\nmulti-modal latent space and CLIPure-Cos is the first purification method that\nis not based on generative models, which substantially improves defense\nefficiency. We conducted extensive experiments on CIFAR-10, ImageNet, and 13\ndatasets that previous CLIP-based defense methods used for evaluating zero-shot\nclassification robustness. Results show that CLIPure boosts the SOTA robustness\nby a large margin, e.g., from 71.7% to 91.1% on CIFAR10, from 59.6% to 72.6% on\nImageNet, and 108% relative improvements of average robustness on the 13\ndatasets over previous SOTA. The code is available at\nhttps://github.com/TMLResearchGroup-CAS/CLIPure.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "accepted by ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.18176v2",
    "published_date": "2025-02-25 13:09:34 UTC",
    "updated_date": "2025-03-02 09:22:47 UTC"
  },
  {
    "arxiv_id": "2502.18168v4",
    "title": "SECURA: Sigmoid-Enhanced CUR Decomposition with Uninterrupted Retention and Low-Rank Adaptation in Large Language Models",
    "authors": [
      "Yuxuan Zhang"
    ],
    "abstract": "With the rapid development of large language models (LLMs), fully fine-tuning\n(FT) these models is becoming increasingly infeasible due to high computational\ndemands. Moreover, FT also increases the risk of catastrophic forgetting. As an\nalternative, Low-Rank Adaptation (LoRA) has been proposed. By fine-tuning only\na small subset of parameters, LoRA achieves performance similar to FT while\nsignificantly reducing resource requirements. However, since LoRA inherits FT's\ndesign, the issue of catastrophic forgetting still remains. To address these\nlimitations, we propose SECURA: Sigmoid-Enhanced CUR Decomposition LoRA, a\nnovel PEFT variant designed to mitigate catastrophic forgetting while improving\nfine-tuning performance. Our method introduces a novel normalization technique,\nSigmoid-based Magnitude Norm (S-MagNorm), which enhances parameter retention\nand fine-tuning efficiency. SECURA has been evaluated on a diverse range of\ntasks, including mathematical problem-solving (GSM8K), complex\nquestion-answering (CNNDM), translation (NewsDE), and complex multiple-choice\nreasoning (LogiQA). Experimental results demonstrate that it achieves an\naverage fine-tuning improvement of 3.59% across four MCQ tasks and 2.51% across\nfive QA tasks on Gemma2 2B, Qwen2 1.5B, Qwen2 7B, Llama3 8B, and Llama3.1 8B,\noutperforming DoRA. Additionally, SECURA demonstrates superior knowledge\nretention capabilities, achieving state-of-the-art performance in 16 continual\nlearning tests and maintaining more than 70% accuracy on LLMs' basic knowledge\ncompared to Experience Replay (ER), sequential learning (SEQ), EWC, I-LoRA, and\nCUR-LoRA.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.6; I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "New work on PEFT for LLMs, introducing S-MagNorm and CABR-LoRA to\n  enhance fine-tuning performance and knowledge retention. In v4, we renamed\n  Sigmoid-based Magnitude Normalization to S-MagNorm for clarity and added a\n  gradient comparison between SECURA and CABR-LoRA to highlight their\n  contributions",
    "pdf_url": "http://arxiv.org/pdf/2502.18168v4",
    "published_date": "2025-02-25 13:00:05 UTC",
    "updated_date": "2025-03-04 06:59:18 UTC"
  },
  {
    "arxiv_id": "2502.18161v1",
    "title": "iTrash: Incentivized Token Rewards for Automated Sorting and Handling",
    "authors": [
      "Pablo Ortega",
      "Eduardo Castelló Ferrer"
    ],
    "abstract": "As robotic systems (RS) become more autonomous, they are becoming\nincreasingly used in small spaces and offices to automate tasks such as\ncleaning, infrastructure maintenance, or resource management. In this paper, we\npropose iTrash, an intelligent trashcan that aims to improve recycling rates in\nsmall office spaces. For that, we ran a 5 day experiment and found that iTrash\ncan produce an efficiency increase of more than 30% compared to traditional\ntrashcans. The findings derived from this work, point to the fact that using\niTrash not only increase recyclying rates, but also provides valuable data such\nas users behaviour or bin usage patterns, which cannot be taken from a normal\ntrashcan. This information can be used to predict and optimize some tasks in\nthese spaces. Finally, we explored the potential of using blockchain technology\nto create economic incentives for recycling, following a Save-as-you-Throw\n(SAYT) model.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.ET",
      "I.2.9; I.2.10"
    ],
    "primary_category": "cs.RO",
    "comment": "Article submitted to IROS 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.18161v1",
    "published_date": "2025-02-25 12:46:45 UTC",
    "updated_date": "2025-02-25 12:46:45 UTC"
  },
  {
    "arxiv_id": "2502.18157v1",
    "title": "Monitoring snow avalanches from SAR data with deep learning",
    "authors": [
      "Filippo Maria Bianchi",
      "Jakob Grahn"
    ],
    "abstract": "Snow avalanches present significant risks to human life and infrastructure,\nparticularly in mountainous regions, making effective monitoring crucial.\nTraditional monitoring methods, such as field observations, are limited by\naccessibility, weather conditions, and cost. Satellite-borne Synthetic Aperture\nRadar (SAR) data has become an important tool for large-scale avalanche\ndetection, as it can capture data in all weather conditions and across remote\nareas. However, traditional processing methods struggle with the complexity and\nvariability of avalanches. This chapter reviews the application of deep\nlearning for detecting and segmenting snow avalanches from SAR data. Early\nefforts focused on the binary classification of SAR images, while recent\nadvances have enabled pixel-level segmentation, providing greater accuracy and\nspatial resolution. A case study using Sentinel-1 SAR data demonstrates the\neffectiveness of deep learning models for avalanche segmentation, achieving\nsuperior results over traditional methods. We also present an extension of this\nwork, testing recent state-of-the-art segmentation architectures on an expanded\ndataset of over 4,500 annotated SAR images. The best-performing model among\nthose tested was applied for large-scale avalanche detection across the whole\nof Norway, revealing important spatial and temporal patterns over several\nwinter seasons.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18157v1",
    "published_date": "2025-02-25 12:41:08 UTC",
    "updated_date": "2025-02-25 12:41:08 UTC"
  },
  {
    "arxiv_id": "2502.18156v1",
    "title": "Can LLMs Explain Themselves Counterfactually?",
    "authors": [
      "Zahra Dehghanighobadi",
      "Asja Fischer",
      "Muhammad Bilal Zafar"
    ],
    "abstract": "Explanations are an important tool for gaining insights into the behavior of\nML models, calibrating user trust and ensuring regulatory compliance. Past few\nyears have seen a flurry of post-hoc methods for generating model explanations,\nmany of which involve computing model gradients or solving specially designed\noptimization problems. However, owing to the remarkable reasoning abilities of\nLarge Language Model (LLMs), self-explanation, that is, prompting the model to\nexplain its outputs has recently emerged as a new paradigm. In this work, we\nstudy a specific type of self-explanations, self-generated counterfactual\nexplanations (SCEs). We design tests for measuring the efficacy of LLMs in\ngenerating SCEs. Analysis over various LLM families, model sizes, temperature\nsettings, and datasets reveals that LLMs sometimes struggle to generate SCEs.\nEven when they do, their prediction often does not agree with their own\ncounterfactual reasoning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18156v1",
    "published_date": "2025-02-25 12:40:41 UTC",
    "updated_date": "2025-02-25 12:40:41 UTC"
  },
  {
    "arxiv_id": "2502.18153v1",
    "title": "SASSHA: Sharpness-aware Adaptive Second-order Optimization with Stable Hessian Approximation",
    "authors": [
      "Dahun Shin",
      "Dongyeop Lee",
      "Jinseok Chung",
      "Namhoon Lee"
    ],
    "abstract": "Approximate second-order optimization methods often exhibit poorer\ngeneralization compared to first-order approaches. In this work, we look into\nthis issue through the lens of the loss landscape and find that existing\nsecond-order methods tend to converge to sharper minima compared to SGD. In\nresponse, we propose Sassha, a novel second-order method designed to enhance\ngeneralization by explicitly reducing sharpness of the solution, while\nstabilizing the computation of approximate Hessians along the optimization\ntrajectory. In fact, this sharpness minimization scheme is crafted also to\naccommodate lazy Hessian updates, so as to secure efficiency besides flatness.\nTo validate its effectiveness, we conduct a wide range of standard deep\nlearning experiments where Sassha demonstrates its outstanding generalization\nperformance that is comparable to, and mostly better than, other methods. We\nprovide a comprehensive set of analyses including convergence, robustness,\nstability, efficiency, and cost.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18153v1",
    "published_date": "2025-02-25 12:35:05 UTC",
    "updated_date": "2025-02-25 12:35:05 UTC"
  },
  {
    "arxiv_id": "2502.18151v1",
    "title": "A Real-time Spatio-Temporal Trajectory Planner for Autonomous Vehicles with Semantic Graph Optimization",
    "authors": [
      "Shan He",
      "Yalong Ma",
      "Tao Song",
      "Yongzhi Jiang",
      "Xinkai Wu"
    ],
    "abstract": "Planning a safe and feasible trajectory for autonomous vehicles in real-time\nby fully utilizing perceptual information in complex urban environments is\nchallenging. In this paper, we propose a spatio-temporal trajectory planning\nmethod based on graph optimization. It efficiently extracts the multi-modal\ninformation of the perception module by constructing a semantic spatio-temporal\nmap through separation processing of static and dynamic obstacles, and then\nquickly generates feasible trajectories via sparse graph optimization based on\na semantic spatio-temporal hypergraph. Extensive experiments have proven that\nthe proposed method can effectively handle complex urban public road scenarios\nand perform in real time. We will also release our codes to accommodate\nbenchmarking for the research community",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "This work has been accepted for publication in IEEE Robotics and\n  Automation Letters (RA-L). The final published version is available in IEEE\n  Xplore (DOI: 10.1109/LRA.2024.3504239)",
    "pdf_url": "http://arxiv.org/pdf/2502.18151v1",
    "published_date": "2025-02-25 12:27:06 UTC",
    "updated_date": "2025-02-25 12:27:06 UTC"
  },
  {
    "arxiv_id": "2502.18147v1",
    "title": "Jacobian Sparse Autoencoders: Sparsify Computations, Not Just Activations",
    "authors": [
      "Lucy Farnik",
      "Tim Lawson",
      "Conor Houghton",
      "Laurence Aitchison"
    ],
    "abstract": "Sparse autoencoders (SAEs) have been successfully used to discover sparse and\nhuman-interpretable representations of the latent activations of LLMs. However,\nwe would ultimately like to understand the computations performed by LLMs and\nnot just their representations. The extent to which SAEs can help us understand\ncomputations is unclear because they are not designed to \"sparsify\"\ncomputations in any sense, only latent activations. To solve this, we propose\nJacobian SAEs (JSAEs), which yield not only sparsity in the input and output\nactivations of a given model component but also sparsity in the computation\n(formally, the Jacobian) connecting them. With a na\\\"ive implementation, the\nJacobians in LLMs would be computationally intractable due to their size. One\nkey technical contribution is thus finding an efficient way of computing\nJacobians in this setup. We find that JSAEs extract a relatively large degree\nof computational sparsity while preserving downstream LLM performance\napproximately as well as traditional SAEs. We also show that Jacobians are a\nreasonable proxy for computational sparsity because MLPs are approximately\nlinear when rewritten in the JSAE basis. Lastly, we show that JSAEs achieve a\ngreater degree of computational sparsity on pre-trained LLMs than on the\nequivalent randomized LLM. This shows that the sparsity of the computational\ngraph appears to be a property that LLMs learn through training, and suggests\nthat JSAEs might be more suitable for understanding learned transformer\ncomputations than standard SAEs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18147v1",
    "published_date": "2025-02-25 12:21:45 UTC",
    "updated_date": "2025-02-25 12:21:45 UTC"
  },
  {
    "arxiv_id": "2502.18138v1",
    "title": "Large Language Model Driven Agents for Simulating Echo Chamber Formation",
    "authors": [
      "Chenhao Gu",
      "Ling Luo",
      "Zainab Razia Zaidi",
      "Shanika Karunasekera"
    ],
    "abstract": "The rise of echo chambers on social media platforms has heightened concerns\nabout polarization and the reinforcement of existing beliefs. Traditional\napproaches for simulating echo chamber formation have often relied on\npredefined rules and numerical simulations, which, while insightful, may lack\nthe nuance needed to capture complex, real-world interactions. In this paper,\nwe present a novel framework that leverages large language models (LLMs) as\ngenerative agents to simulate echo chamber dynamics within social networks. The\nnovelty of our approach is that it incorporates both opinion updates and\nnetwork rewiring behaviors driven by LLMs, allowing for a context-aware and\nsemantically rich simulation of social interactions. Additionally, we utilize\nreal-world Twitter (now X) data to benchmark the LLM-based simulation against\nactual social media behaviors, providing insights into the accuracy and realism\nof the generated opinion trends. Our results demonstrate the efficacy of LLMs\nin modeling echo chamber formation, capturing both structural and semantic\ndimensions of opinion clustering. %This work contributes to a deeper\nunderstanding of social influence dynamics and offers a new tool for studying\npolarization in online communities.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18138v1",
    "published_date": "2025-02-25 12:05:11 UTC",
    "updated_date": "2025-02-25 12:05:11 UTC"
  },
  {
    "arxiv_id": "2502.18137v2",
    "title": "SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference",
    "authors": [
      "Jintao Zhang",
      "Chendong Xiang",
      "Haofeng Huang",
      "Jia Wei",
      "Haocheng Xi",
      "Jun Zhu",
      "Jianfei Chen"
    ],
    "abstract": "An efficient attention implementation is essential for large models due to\nits quadratic time complexity. Fortunately, attention commonly exhibits\nsparsity, i.e., many values in the attention map are near zero, allowing for\nthe omission of corresponding computations. Many studies have utilized the\nsparse pattern to accelerate attention. However, most existing works focus on\noptimizing attention within specific models by exploiting certain sparse\npatterns of the attention map. A universal sparse attention that guarantees\nboth the speedup and end-to-end performance of diverse models remains elusive.\nIn this paper, we propose SpargeAttn, a universal sparse and quantized\nattention for any model. Our method uses a two-stage online filter: in the\nfirst stage, we rapidly and accurately predict the attention map, enabling the\nskip of some matrix multiplications in attention. In the second stage, we\ndesign an online softmax-aware filter that incurs no extra overhead and further\nskips some matrix multiplications. Experiments show that our method\nsignificantly accelerates diverse models, including language, image, and video\ngeneration, without sacrificing end-to-end metrics. The codes are available at\nhttps://github.com/thu-ml/SpargeAttn.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.PF"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18137v2",
    "published_date": "2025-02-25 12:02:17 UTC",
    "updated_date": "2025-05-01 05:38:46 UTC"
  },
  {
    "arxiv_id": "2502.18122v1",
    "title": "EU-Nets: Enhanced, Explainable and Parsimonious U-Nets",
    "authors": [
      "B. Sun",
      "P. Liò"
    ],
    "abstract": "In this study, we propose MHEX+, a framework adaptable to any U-Net\narchitecture. Built upon MHEX+, we introduce novel U-Net variants, EU-Nets,\nwhich enhance explainability and uncertainty estimation, addressing the\nlimitations of traditional U-Net models while improving performance and\nstability. A key innovation is the Equivalent Convolutional Kernel, which\nunifies consecutive convolutional layers, boosting interpretability. For\nuncertainty estimation, we propose the collaboration gradient approach,\nmeasuring gradient consistency across decoder layers. Notably, EU-Nets achieve\nan average accuracy improvement of 1.389\\% and a variance reduction of 0.83\\%\nacross all networks and datasets in our experiments, requiring fewer than 0.1M\nparameters.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18122v1",
    "published_date": "2025-02-25 11:44:30 UTC",
    "updated_date": "2025-02-25 11:44:30 UTC"
  },
  {
    "arxiv_id": "2502.18116v2",
    "title": "Bayesian Optimization for Controlled Image Editing via LLMs",
    "authors": [
      "Chengkun Cai",
      "Haoliang Liu",
      "Xu Zhao",
      "Zhongyu Jiang",
      "Tianfang Zhang",
      "Zongkai Wu",
      "Jenq-Neng Hwang",
      "Serge Belongie",
      "Lei Li"
    ],
    "abstract": "In the rapidly evolving field of image generation, achieving precise control\nover generated content and maintaining semantic consistency remain significant\nlimitations, particularly concerning grounding techniques and the necessity for\nmodel fine-tuning. To address these challenges, we propose BayesGenie, an\noff-the-shelf approach that integrates Large Language Models (LLMs) with\nBayesian Optimization to facilitate precise and user-friendly image editing.\nOur method enables users to modify images through natural language descriptions\nwithout manual area marking, while preserving the original image's semantic\nintegrity. Unlike existing techniques that require extensive pre-training or\nfine-tuning, our approach demonstrates remarkable adaptability across various\nLLMs through its model-agnostic design. BayesGenie employs an adapted Bayesian\noptimization strategy to automatically refine the inference process parameters,\nachieving high-precision image editing with minimal user intervention. Through\nextensive experiments across diverse scenarios, we demonstrate that our\nframework significantly outperforms existing methods in both editing accuracy\nand semantic preservation, as validated using different LLMs including Claude3\nand GPT-4.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "8 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.18116v2",
    "published_date": "2025-02-25 11:41:33 UTC",
    "updated_date": "2025-02-26 06:53:39 UTC"
  },
  {
    "arxiv_id": "2502.18097v1",
    "title": "The Built-In Robustness of Decentralized Federated Averaging to Bad Data",
    "authors": [
      "Samuele Sabella",
      "Chiara Boldrini",
      "Lorenzo Valerio",
      "Andrea Passarella",
      "Marco Conti"
    ],
    "abstract": "Decentralized federated learning (DFL) enables devices to collaboratively\ntrain models over complex network topologies without relying on a central\ncontroller. In this setting, local data remains private, but its quality and\nquantity can vary significantly across nodes. The extent to which a fully\ndecentralized system is vulnerable to poor-quality or corrupted data remains\nunclear, but several factors could contribute to potential risks. Without a\ncentral authority, there can be no unified mechanism to detect or correct\nerrors, and each node operates with a localized view of the data distribution,\nmaking it difficult for the node to assess whether its perspective aligns with\nthe true distribution. Moreover, models trained on low-quality data can\npropagate through the network, amplifying errors. To explore the impact of\nlow-quality data on DFL, we simulate two scenarios with degraded data quality\n-- one where the corrupted data is evenly distributed in a subset of nodes and\none where it is concentrated on a single node -- using a decentralized\nimplementation of FedAvg. Our results reveal that averaging-based decentralized\nlearning is remarkably robust to localized bad data, even when the corrupted\ndata resides in the most influential nodes of the network. Counterintuitively,\nthis robustness is further enhanced when the corrupted data is concentrated on\na single node, regardless of its centrality in the communication network\ntopology. This phenomenon is explained by the averaging process, which ensures\nthat no single node -- however central -- can disproportionately influence the\noverall learning process.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "Funding: SoBigData PPP (101079043), SoBigData.it (PNRR IR0000013),\n  FAIR (PNRR PE00000013), RESTART (PNRR PE00000001)",
    "pdf_url": "http://arxiv.org/pdf/2502.18097v1",
    "published_date": "2025-02-25 11:06:51 UTC",
    "updated_date": "2025-02-25 11:06:51 UTC"
  },
  {
    "arxiv_id": "2502.18080v1",
    "title": "Towards Thinking-Optimal Scaling of Test-Time Compute for LLM Reasoning",
    "authors": [
      "Wenkai Yang",
      "Shuming Ma",
      "Yankai Lin",
      "Furu Wei"
    ],
    "abstract": "Recent studies have shown that making a model spend more time thinking\nthrough longer Chain of Thoughts (CoTs) enables it to gain significant\nimprovements in complex reasoning tasks. While current researches continue to\nexplore the benefits of increasing test-time compute by extending the CoT\nlengths of Large Language Models (LLMs), we are concerned about a potential\nissue hidden behind the current pursuit of test-time scaling: Would excessively\nscaling the CoT length actually bring adverse effects to a model's reasoning\nperformance? Our explorations on mathematical reasoning tasks reveal an\nunexpected finding that scaling with longer CoTs can indeed impair the\nreasoning performance of LLMs in certain domains. Moreover, we discover that\nthere exists an optimal scaled length distribution that differs across\ndifferent domains. Based on these insights, we propose a Thinking-Optimal\nScaling strategy. Our method first uses a small set of seed data with varying\nresponse length distributions to teach the model to adopt different reasoning\nefforts for deep thinking. Then, the model selects its shortest correct\nresponse under different reasoning efforts on additional problems for\nself-improvement. Our self-improved models built upon Qwen2.5-32B-Instruct\noutperform other distillation-based 32B o1-like models across various math\nbenchmarks, and achieve performance on par with QwQ-32B-Preview.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18080v1",
    "published_date": "2025-02-25 10:48:05 UTC",
    "updated_date": "2025-02-25 10:48:05 UTC"
  },
  {
    "arxiv_id": "2502.18072v1",
    "title": "MRBTP: Efficient Multi-Robot Behavior Tree Planning and Collaboration",
    "authors": [
      "Yishuai Cai",
      "Xinglin Chen",
      "Zhongxuan Cai",
      "Yunxin Mao",
      "Minglong Li",
      "Wenjing Yang",
      "Ji Wang"
    ],
    "abstract": "Multi-robot task planning and collaboration are critical challenges in\nrobotics. While Behavior Trees (BTs) have been established as a popular control\narchitecture and are plannable for a single robot, the development of effective\nmulti-robot BT planning algorithms remains challenging due to the complexity of\ncoordinating diverse action spaces. We propose the Multi-Robot Behavior Tree\nPlanning (MRBTP) algorithm, with theoretical guarantees of both soundness and\ncompleteness. MRBTP features cross-tree expansion to coordinate heterogeneous\nactions across different BTs to achieve the team's goal. For homogeneous\nactions, we retain backup structures among BTs to ensure robustness and prevent\nredundant execution through intention sharing. While MRBTP is capable of\ngenerating BTs for both homogeneous and heterogeneous robot teams, its\nefficiency can be further improved. We then propose an optional plugin for\nMRBTP when Large Language Models (LLMs) are available to reason goal-related\nactions for each robot. These relevant actions can be pre-planned to form\nlong-horizon subtrees, significantly enhancing the planning speed and\ncollaboration efficiency of MRBTP. We evaluate our algorithm in warehouse\nmanagement and everyday service scenarios. Results demonstrate MRBTP's\nrobustness and execution efficiency under varying settings, as well as the\nability of the pre-trained LLM to generate effective task-specific subtrees for\nMRBTP.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18072v1",
    "published_date": "2025-02-25 10:39:28 UTC",
    "updated_date": "2025-02-25 10:39:28 UTC"
  },
  {
    "arxiv_id": "2502.18064v1",
    "title": "HEROS-GAN: Honed-Energy Regularized and Optimal Supervised GAN for Enhancing Accuracy and Range of Low-Cost Accelerometers",
    "authors": [
      "Yifeng Wang",
      "Yi Zhao"
    ],
    "abstract": "Low-cost accelerometers play a crucial role in modern society due to their\nadvantages of small size, ease of integration, wearability, and mass\nproduction, making them widely applicable in automotive systems, aerospace, and\nwearable technology. However, this widely used sensor suffers from severe\naccuracy and range limitations. To this end, we propose a honed-energy\nregularized and optimal supervised GAN (HEROS-GAN), which transforms low-cost\nsensor signals into high-cost equivalents, thereby overcoming the precision and\nrange limitations of low-cost accelerometers. Due to the lack of frame-level\npaired low-cost and high-cost signals for training, we propose an Optimal\nTransport Supervision (OTS), which leverages optimal transport theory to\nexplore potential consistency between unpaired data, thereby maximizing\nsupervisory information. Moreover, we propose a Modulated Laplace Energy (MLE),\nwhich injects appropriate energy into the generator to encourage it to break\nrange limitations, enhance local changes, and enrich signal details. Given the\nabsence of a dedicated dataset, we specifically establish a Low-cost\nAccelerometer Signal Enhancement Dataset (LASED) containing tens of thousands\nof samples, which is the first dataset serving to improve the accuracy and\nrange of accelerometers and is released in Github. Experimental results\ndemonstrate that a GAN combined with either OTS or MLE alone can surpass the\nprevious signal enhancement SOTA methods by an order of magnitude. Integrating\nboth OTS and MLE, the HEROS-GAN achieves remarkable results, which doubles the\naccelerometer range while reducing signal noise by two orders of magnitude,\nestablishing a benchmark in the accelerometer signal processing.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO",
      "eess.SP",
      "math.PR"
    ],
    "primary_category": "cs.LG",
    "comment": "AAAI Oral; AI for Sensors; Generative Deep Learning",
    "pdf_url": "http://arxiv.org/pdf/2502.18064v1",
    "published_date": "2025-02-25 10:31:01 UTC",
    "updated_date": "2025-02-25 10:31:01 UTC"
  },
  {
    "arxiv_id": "2502.18060v1",
    "title": "Defining bias in AI-systems: Biased models are fair models",
    "authors": [
      "Chiara Lindloff",
      "Ingo Siegert"
    ],
    "abstract": "The debate around bias in AI systems is central to discussions on algorithmic\nfairness. However, the term bias often lacks a clear definition, despite\nfrequently being contrasted with fairness, implying that an unbiased model is\ninherently fair. In this paper, we challenge this assumption and argue that a\nprecise conceptualization of bias is necessary to effectively address fairness\nconcerns. Rather than viewing bias as inherently negative or unfair, we\nhighlight the importance of distinguishing between bias and discrimination. We\nfurther explore how this shift in focus can foster a more constructive\ndiscourse within academic debates on fairness in AI systems.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "ISCA/ITG Workshop on Diversity in Large Speech and Language Models",
    "pdf_url": "http://arxiv.org/pdf/2502.18060v1",
    "published_date": "2025-02-25 10:28:16 UTC",
    "updated_date": "2025-02-25 10:28:16 UTC"
  },
  {
    "arxiv_id": "2503.05769v1",
    "title": "Effect of Gender Fair Job Description on Generative AI Images",
    "authors": [
      "Finn Böckling",
      "Jan Marquenie",
      "Ingo Siegert"
    ],
    "abstract": "STEM fields are traditionally male-dominated, with gender biases shaping\nperceptions of job accessibility. This study analyzed gender representation in\nSTEM occupation images generated by OpenAI DALL-E 3 \\& Black Forest FLUX.1\nusing 150 prompts in three linguistic forms: German generic masculine, German\npair form, and English. As control, 20 pictures of social occupations were\ngenerated as well. Results revealed significant male bias across all forms,\nwith the German pair form showing reduced bias but still overrepresenting men\nfor the STEM-Group and mixed results for the Group of Social Occupations. These\nfindings highlight generative AI's role in reinforcing societal biases,\nemphasizing the need for further discussion on diversity (in AI). Further\naspects analyzed are age-distribution and ethnic diversity.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "ISCA/ITG Workshop on Diversity in Large Speech and Language Models",
    "pdf_url": "http://arxiv.org/pdf/2503.05769v1",
    "published_date": "2025-02-25 10:21:29 UTC",
    "updated_date": "2025-02-25 10:21:29 UTC"
  },
  {
    "arxiv_id": "2502.18042v1",
    "title": "VLM-E2E: Enhancing End-to-End Autonomous Driving with Multimodal Driver Attention Fusion",
    "authors": [
      "Pei Liu",
      "Haipeng Liu",
      "Haichao Liu",
      "Xin Liu",
      "Jinxin Ni",
      "Jun Ma"
    ],
    "abstract": "Human drivers adeptly navigate complex scenarios by utilizing rich\nattentional semantics, but the current autonomous systems struggle to replicate\nthis ability, as they often lose critical semantic information when converting\n2D observations into 3D space. In this sense, it hinders their effective\ndeployment in dynamic and complex environments. Leveraging the superior scene\nunderstanding and reasoning abilities of Vision-Language Models (VLMs), we\npropose VLM-E2E, a novel framework that uses the VLMs to enhance training by\nproviding attentional cues. Our method integrates textual representations into\nBird's-Eye-View (BEV) features for semantic supervision, which enables the\nmodel to learn richer feature representations that explicitly capture the\ndriver's attentional semantics. By focusing on attentional semantics, VLM-E2E\nbetter aligns with human-like driving behavior, which is critical for\nnavigating dynamic and complex environments. Furthermore, we introduce a\nBEV-Text learnable weighted fusion strategy to address the issue of modality\nimportance imbalance in fusing multimodal information. This approach\ndynamically balances the contributions of BEV and text features, ensuring that\nthe complementary information from visual and textual modality is effectively\nutilized. By explicitly addressing the imbalance in multimodal fusion, our\nmethod facilitates a more holistic and robust representation of driving\nenvironments. We evaluate VLM-E2E on the nuScenes dataset and demonstrate its\nsuperiority over state-of-the-art approaches, showcasing significant\nimprovements in performance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18042v1",
    "published_date": "2025-02-25 10:02:12 UTC",
    "updated_date": "2025-02-25 10:02:12 UTC"
  },
  {
    "arxiv_id": "2502.18040v1",
    "title": "AutoCas: Autoregressive Cascade Predictor in Social Networks via Large Language Models",
    "authors": [
      "Yuhao Zheng",
      "Chenghua Gong",
      "Rui Sun",
      "Juyuan Zhang",
      "Liming Pan",
      "Linyuan Lv"
    ],
    "abstract": "Popularity prediction in information cascades plays a crucial role in social\ncomputing, with broad applications in viral marketing, misinformation control,\nand content recommendation. However, information propagation mechanisms, user\nbehavior, and temporal activity patterns exhibit significant diversity,\nnecessitating a foundational model capable of adapting to such variations. At\nthe same time, the amount of available cascade data remains relatively limited\ncompared to the vast datasets used for training large language models (LLMs).\nRecent studies have demonstrated the feasibility of leveraging LLMs for\ntime-series prediction by exploiting commonalities across different time-series\ndomains. Building on this insight, we introduce the Autoregressive Information\nCascade Predictor (AutoCas), an LLM-enhanced model designed specifically for\ncascade popularity prediction. Unlike natural language sequences, cascade data\nis characterized by complex local topologies, diffusion contexts, and evolving\ndynamics, requiring specialized adaptations for effective LLM integration. To\naddress these challenges, we first tokenize cascade data to align it with\nsequence modeling principles. Next, we reformulate cascade diffusion as an\nautoregressive modeling task to fully harness the architectural strengths of\nLLMs. Beyond conventional approaches, we further introduce prompt learning to\nenhance the synergy between LLMs and cascade prediction. Extensive experiments\ndemonstrate that AutoCas significantly outperforms baseline models in cascade\npopularity prediction while exhibiting scaling behavior inherited from LLMs.\nCode is available at this repository:\nhttps://anonymous.4open.science/r/AutoCas-85C6",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "12 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.18040v1",
    "published_date": "2025-02-25 09:54:33 UTC",
    "updated_date": "2025-02-25 09:54:33 UTC"
  },
  {
    "arxiv_id": "2502.18026v1",
    "title": "ExPath: Towards Explaining Targeted Pathways for Biological Knowledge Bases",
    "authors": [
      "Rikuto Kotoge",
      "Ziwei Yang",
      "Zheng Chen",
      "Yushun Dong",
      "Yasuko Matsubara",
      "Jimeng Sun",
      "Yasushi Sakurai"
    ],
    "abstract": "Biological knowledge bases provide systemically functional pathways of cells\nor organisms in terms of molecular interaction. However, recognizing more\ntargeted pathways, particularly when incorporating wet-lab experimental data,\nremains challenging and typically requires downstream biological analyses and\nexpertise. In this paper, we frame this challenge as a solvable graph learning\nand explaining task and propose a novel pathway inference framework, ExPath,\nthat explicitly integrates experimental data, specifically amino acid sequences\n(AA-seqs), to classify various graphs (bio-networks) in biological databases.\nThe links (representing pathways) that contribute more to classification can be\nconsidered as targeted pathways. Technically, ExPath comprises three\ncomponents: (1) a large protein language model (pLM) that encodes and embeds\nAA-seqs into graph, overcoming traditional obstacles in processing AA-seq data,\nsuch as BLAST; (2) PathMamba, a hybrid architecture combining graph neural\nnetworks (GNNs) with state-space sequence modeling (Mamba) to capture both\nlocal interactions and global pathway-level dependencies; and (3)\nPathExplainer, a subgraph learning module that identifies functionally critical\nnodes and edges through trainable pathway masks. We also propose ML-oriented\nbiological evaluations and a new metric. The experiments involving 301\nbio-networks evaluations demonstrate that pathways inferred by ExPath maintain\nbiological meaningfulness. We will publicly release curated 301 bio-network\ndata soon.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Under review",
    "pdf_url": "http://arxiv.org/pdf/2502.18026v1",
    "published_date": "2025-02-25 09:33:15 UTC",
    "updated_date": "2025-02-25 09:33:15 UTC"
  },
  {
    "arxiv_id": "2502.18020v1",
    "title": "AfroXLMR-Comet: Multilingual Knowledge Distillation with Attention Matching for Low-Resource languages",
    "authors": [
      "Joshua Sakthivel Raju",
      "Sanjay S",
      "Jaskaran Singh Walia",
      "Srinivas Raghav",
      "Vukosi Marivate"
    ],
    "abstract": "Language model compression through knowledge distillation has emerged as a\npromising approach for deploying large language models in resource-constrained\nenvironments. However, existing methods often struggle to maintain performance\nwhen distilling multilingual models, especially for low-resource languages. In\nthis paper, we present a novel hybrid distillation approach that combines\ntraditional knowledge distillation with a simplified attention matching\nmechanism, specifically designed for multilingual contexts. Our method\nintroduces an extremely compact student model architecture, significantly\nsmaller than conventional multilingual models. We evaluate our approach on five\nAfrican languages: Kinyarwanda, Swahili, Hausa, Igbo, and Yoruba. The distilled\nstudent model; AfroXLMR-Comet successfully captures both the output\ndistribution and internal attention patterns of a larger teacher model\n(AfroXLMR-Large) while reducing the model size by over 85%. Experimental\nresults demonstrate that our hybrid approach achieves competitive performance\ncompared to the teacher model, maintaining an accuracy within 85% of the\noriginal model's performance while requiring substantially fewer computational\nresources. Our work provides a practical framework for deploying efficient\nmultilingual models in resource-constrained environments, particularly\nbenefiting applications involving African languages.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18020v1",
    "published_date": "2025-02-25 09:28:47 UTC",
    "updated_date": "2025-02-25 09:28:47 UTC"
  },
  {
    "arxiv_id": "2502.18017v1",
    "title": "ViDoRAG: Visual Document Retrieval-Augmented Generation via Dynamic Iterative Reasoning Agents",
    "authors": [
      "Qiuchen Wang",
      "Ruixue Ding",
      "Zehui Chen",
      "Weiqi Wu",
      "Shihang Wang",
      "Pengjun Xie",
      "Feng Zhao"
    ],
    "abstract": "Understanding information from visually rich documents remains a significant\nchallenge for traditional Retrieval-Augmented Generation (RAG) methods.\nExisting benchmarks predominantly focus on image-based question answering (QA),\noverlooking the fundamental challenges of efficient retrieval, comprehension,\nand reasoning within dense visual documents. To bridge this gap, we introduce\nViDoSeek, a novel dataset designed to evaluate RAG performance on visually rich\ndocuments requiring complex reasoning. Based on it, we identify key limitations\nin current RAG approaches: (i) purely visual retrieval methods struggle to\neffectively integrate both textual and visual features, and (ii) previous\napproaches often allocate insufficient reasoning tokens, limiting their\neffectiveness. To address these challenges, we propose ViDoRAG, a novel\nmulti-agent RAG framework tailored for complex reasoning across visual\ndocuments. ViDoRAG employs a Gaussian Mixture Model (GMM)-based hybrid strategy\nto effectively handle multi-modal retrieval. To further elicit the model's\nreasoning capabilities, we introduce an iterative agent workflow incorporating\nexploration, summarization, and reflection, providing a framework for\ninvestigating test-time scaling in RAG domains. Extensive experiments on\nViDoSeek validate the effectiveness and generalization of our approach.\nNotably, ViDoRAG outperforms existing methods by over 10% on the competitive\nViDoSeek benchmark.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18017v1",
    "published_date": "2025-02-25 09:26:12 UTC",
    "updated_date": "2025-02-25 09:26:12 UTC"
  },
  {
    "arxiv_id": "2502.18008v5",
    "title": "NotaGen: Advancing Musicality in Symbolic Music Generation with Large Language Model Training Paradigms",
    "authors": [
      "Yashan Wang",
      "Shangda Wu",
      "Jianhuai Hu",
      "Xingjian Du",
      "Yueqi Peng",
      "Yongxin Huang",
      "Shuai Fan",
      "Xiaobing Li",
      "Feng Yu",
      "Maosong Sun"
    ],
    "abstract": "We introduce NotaGen, a symbolic music generation model aiming to explore the\npotential of producing high-quality classical sheet music. Inspired by the\nsuccess of Large Language Models (LLMs), NotaGen adopts pre-training,\nfine-tuning, and reinforcement learning paradigms (henceforth referred to as\nthe LLM training paradigms). It is pre-trained on 1.6M pieces of music in ABC\nnotation, and then fine-tuned on approximately 9K high-quality classical\ncompositions conditioned on \"period-composer-instrumentation\" prompts. For\nreinforcement learning, we propose the CLaMP-DPO method, which further enhances\ngeneration quality and controllability without requiring human annotations or\npredefined rewards. Our experiments demonstrate the efficacy of CLaMP-DPO in\nsymbolic music generation models with different architectures and encoding\nschemes. Furthermore, subjective A/B tests show that NotaGen outperforms\nbaseline models against human compositions, greatly advancing musical\naesthetics in symbolic music generation.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18008v5",
    "published_date": "2025-02-25 09:12:07 UTC",
    "updated_date": "2025-03-21 12:53:04 UTC"
  },
  {
    "arxiv_id": "2502.18002v2",
    "title": "A Radon-Nikodým Perspective on Anomaly Detection: Theory and Implications",
    "authors": [
      "Shlok Mehendale",
      "Aditya Challa",
      "Rahul Yedida",
      "Sravan Danda",
      "Santonu Sarkar",
      "Snehanshu Saha"
    ],
    "abstract": "Which principle underpins the design of an effective anomaly detection loss\nfunction? The answer lies in the concept of Radon-Nikod\\'ym theorem, a\nfundamental concept in measure theory. The key insight from this article is:\nMultiplying the vanilla loss function with the Radon-Nikod\\'ym derivative\nimproves the performance across the board. We refer to this as RN-Loss. We\nprove this using the setting of PAC (Probably Approximately Correct)\nlearnability.\n  Depending on the context a Radon-Nikod\\'ym derivative takes different forms.\nIn the simplest case of supervised anomaly detection, Radon-Nikod\\'ym\nderivative takes the form of a simple weighted loss. In the case of\nunsupervised anomaly detection (with distributional assumptions),\nRadon-Nikod\\'ym derivative takes the form of the popular cluster based local\noutlier factor. We evaluate our algorithm on 96 datasets, including univariate\nand multivariate data from diverse domains, including healthcare,\ncybersecurity, and finance. We show that RN-Derivative algorithms outperform\nstate-of-the-art methods on 68% of Multivariate datasets (based on F1 scores)\nand also achieves peak F1-scores on 72% of time series (Univariate) datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18002v2",
    "published_date": "2025-02-25 09:08:50 UTC",
    "updated_date": "2025-05-16 15:04:58 UTC"
  },
  {
    "arxiv_id": "2502.17999v1",
    "title": "GNN-XAR: A Graph Neural Network for Explainable Activity Recognition in Smart Homes",
    "authors": [
      "Michele Fiori",
      "Davide Mor",
      "Gabriele Civitarese",
      "Claudio Bettini"
    ],
    "abstract": "Sensor-based Human Activity Recognition (HAR) in smart home environments is\ncrucial for several applications, especially in the healthcare domain. The\nmajority of the existing approaches leverage deep learning models. While these\napproaches are effective, the rationale behind their outputs is opaque.\nRecently, eXplainable Artificial Intelligence (XAI) approaches emerged to\nprovide intuitive explanations to the output of HAR models. To the best of our\nknowledge, these approaches leverage classic deep models like CNNs or RNNs.\nRecently, Graph Neural Networks (GNNs) proved to be effective for sensor-based\nHAR. However, existing approaches are not designed with explainability in mind.\nIn this work, we propose the first explainable Graph Neural Network explicitly\ndesigned for smart home HAR. Our results on two public datasets show that this\napproach provides better explanations than state-of-the-art methods while also\nslightly improving the recognition rate.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "This is a preprint. Paper accepted for publication at the 21st EAI\n  International Conference on Mobile and Ubiquitous Systems: Computing,\n  Networking and Services (Mobiquitous)",
    "pdf_url": "http://arxiv.org/pdf/2502.17999v1",
    "published_date": "2025-02-25 09:05:13 UTC",
    "updated_date": "2025-02-25 09:05:13 UTC"
  },
  {
    "arxiv_id": "2502.17987v1",
    "title": "MAGE: Multi-Head Attention Guided Embeddings for Low Resource Sentiment Classification",
    "authors": [
      "Varun Vashisht",
      "Samar Singh",
      "Mihir Konduskar",
      "Jaskaran Singh Walia",
      "Vukosi Marivate"
    ],
    "abstract": "Due to the lack of quality data for low-resource Bantu languages, significant\nchallenges are presented in text classification and other practical\nimplementations. In this paper, we introduce an advanced model combining\nLanguage-Independent Data Augmentation (LiDA) with Multi-Head Attention based\nweighted embeddings to selectively enhance critical data points and improve\ntext classification performance. This integration allows us to create robust\ndata augmentation strategies that are effective across various linguistic\ncontexts, ensuring that our model can handle the unique syntactic and semantic\nfeatures of Bantu languages. This approach not only addresses the data scarcity\nissue but also sets a foundation for future research in low-resource language\nprocessing and classification tasks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.17987v1",
    "published_date": "2025-02-25 08:53:27 UTC",
    "updated_date": "2025-02-25 08:53:27 UTC"
  },
  {
    "arxiv_id": "2502.17986v1",
    "title": "Broadening Discovery through Structural Models: Multimodal Combination of Local and Structural Properties for Predicting Chemical Features",
    "authors": [
      "Nikolai Rekut",
      "Alexey Orlov",
      "Klea Ziu",
      "Elizaveta Starykh",
      "Martin Takac",
      "Aleksandr Beznosikov"
    ],
    "abstract": "In recent years, machine learning has profoundly reshaped the field of\nchemistry, facilitating significant advancements across various applications,\nincluding the prediction of molecular properties and the generation of\nmolecular structures. Language models and graph-based models are extensively\nutilized within this domain, consistently achieving state-of-the-art results\nacross an array of tasks. However, the prevailing practice of representing\nchemical compounds in the SMILES format -- used by most datasets and many\nlanguage models -- presents notable limitations as a training data format. In\ncontrast, chemical fingerprints offer a more physically informed representation\nof compounds, thereby enhancing their suitability for model training. This\nstudy aims to develop a language model that is specifically trained on\nfingerprints. Furthermore, we introduce a bimodal architecture that integrates\nthis language model with a graph model. Our proposed methodology synthesizes\nthese approaches, utilizing RoBERTa as the language model and employing Graph\nIsomorphism Networks (GIN), Graph Convolutional Networks (GCN) and Graphormer\nas graph models. This integration results in a significant improvement in\npredictive performance compared to conventional strategies for tasks such as\nQuantitative Structure-Activity Relationship (QSAR) and the prediction of\nnuclear magnetic resonance (NMR) spectra, among others.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.17986v1",
    "published_date": "2025-02-25 08:53:18 UTC",
    "updated_date": "2025-02-25 08:53:18 UTC"
  },
  {
    "arxiv_id": "2503.00038v1",
    "title": "from Benign import Toxic: Jailbreaking the Language Model via Adversarial Metaphors",
    "authors": [
      "Yu Yan",
      "Sheng Sun",
      "Zenghao Duan",
      "Teli Liu",
      "Min Liu",
      "Zhiyi Yin",
      "Qi Li",
      "Jiangyu Lei"
    ],
    "abstract": "Current studies have exposed the risk of Large Language Models (LLMs)\ngenerating harmful content by jailbreak attacks. However, they overlook that\nthe direct generation of harmful content from scratch is more difficult than\ninducing LLM to calibrate benign content into harmful forms. In our study, we\nintroduce a novel attack framework that exploits AdVersArial meTAphoR (AVATAR)\nto induce the LLM to calibrate malicious metaphors for jailbreaking.\nSpecifically, to answer harmful queries, AVATAR adaptively identifies a set of\nbenign but logically related metaphors as the initial seed. Then, driven by\nthese metaphors, the target LLM is induced to reason and calibrate about the\nmetaphorical content, thus jailbroken by either directly outputting harmful\nresponses or calibrating residuals between metaphorical and professional\nharmful content. Experimental results demonstrate that AVATAR can effectively\nand transferable jailbreak LLMs and achieve a state-of-the-art attack success\nrate across multiple advanced LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CL",
    "comment": "arXiv admin note: substantial text overlap with arXiv:2412.12145",
    "pdf_url": "http://arxiv.org/pdf/2503.00038v1",
    "published_date": "2025-02-25 08:41:25 UTC",
    "updated_date": "2025-02-25 08:41:25 UTC"
  },
  {
    "arxiv_id": "2502.17967v1",
    "title": "LLM Knows Geometry Better than Algebra: Numerical Understanding of LLM-Based Agents in A Trading Arena",
    "authors": [
      "Tianmi Ma",
      "Jiawei Du",
      "Wenxin Huang",
      "Wenjie Wang",
      "Liang Xie",
      "Xian Zhong",
      "Joey Tianyi Zhou"
    ],
    "abstract": "Recent advancements in large language models (LLMs) have significantly\nimproved performance in natural language processing tasks. However, their\nability to generalize to dynamic, unseen tasks, particularly in numerical\nreasoning, remains a challenge. Existing benchmarks mainly evaluate LLMs on\nproblems with predefined optimal solutions, which may not align with real-world\nscenarios where clear answers are absent. To bridge this gap, we design the\nAgent Trading Arena, a virtual numerical game simulating complex economic\nsystems through zero-sum games, where agents invest in stock portfolios. Our\nexperiments reveal that LLMs, including GPT-4o, struggle with algebraic\nreasoning when dealing with plain-text stock data, often focusing on local\ndetails rather than global trends. In contrast, LLMs perform significantly\nbetter with geometric reasoning when presented with visual data, such as\nscatter plots or K-line charts, suggesting that visual representations enhance\nnumerical reasoning. This capability is further improved by incorporating the\nreflection module, which aids in the analysis and interpretation of complex\ndata. We validate our findings on NASDAQ Stock dataset, where LLMs demonstrate\nstronger reasoning with visual data compared to text. Our code and data are\npublicly available at https://github.com/wekjsdvnm/Agent-Trading-Arena.git.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.MA",
      "q-fin.ST"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.17967v1",
    "published_date": "2025-02-25 08:41:01 UTC",
    "updated_date": "2025-02-25 08:41:01 UTC"
  },
  {
    "arxiv_id": "2504.05321v1",
    "title": "VALUE: Value-Aware Large Language Model for Query Rewriting via Weighted Trie in Sponsored Search",
    "authors": [
      "Boyang Zuo",
      "Xiao Zhang",
      "Feng Li",
      "Pengjie Wang",
      "Jian Xu",
      "Bo Zheng"
    ],
    "abstract": "In the realm of sponsored search advertising, matching advertisements with\nthe search intent of a user's query is crucial. Query-to-bidwords(i.e. bidding\nkeywords) rewriting is a vital technique that has garnered significant\nattention. Recently, with the prevalence of LLMs, generative retrieval methods\nhave proven effective in producing high-relevance rewrites. However, we have\nidentified a significant limitation in existing approaches: While fine-tuning\nLLMs for specific domains enhances semantic relevance, these models have no\nperception of the intrinsic value of their generated outputs, such as\ncommercial value. Therefore, after SFT, a RLHF phase is often employed to\naddress this issue. Nevertheless, traditional preference alignment methods\noften face challenges in aligning fine-grained values and are susceptible to\noverfitting, which diminishes the effectiveness and quality of the generated\nresults. To address these challenges, we propose VALUE(Value-Aware Large\nlanguage model for qUery rewriting via wEighted trie), the first framework that\nensures the generation of high-value and highly relevant bidwords. Our approach\nutilizes weighted trie, an innovative modification of the traditional trie data\nstructure. By modulating the LLM's output probability distribution with value\ninformation from the trie during decoding process, we constrain the generation\nspace and guide the trajectory of text production. Offline experiments\ndemonstrate the effectiveness of our method in semantic matching and preference\nalignment, showing a remarkable improvement in the value attribute by more than\nfivefold. Online A/B tests further revealed that our Revenue Per Mille (RPM)\nmetric increased by 1.64%. VALUE has been deployed on our advertising system\nsince October 2024 and served the Double Eleven promotions, the biggest\nshopping carnival in China.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.05321v1",
    "published_date": "2025-02-25 08:34:16 UTC",
    "updated_date": "2025-02-25 08:34:16 UTC"
  },
  {
    "arxiv_id": "2502.18540v1",
    "title": "MA-GTS: A Multi-Agent Framework for Solving Complex Graph Problems in Real-World Applications",
    "authors": [
      "Zike Yuan",
      "Ming Liu",
      "Hui Wang",
      "Bing Qin"
    ],
    "abstract": "Graph-theoretic problems arise in real-world applications like logistics,\ncommunication networks, and traffic optimization. These problems are often\ncomplex, noisy, and irregular, posing challenges for traditional algorithms.\nLarge language models (LLMs) offer potential solutions but face challenges,\nincluding limited accuracy and input length constraints. To address these\nchallenges, we propose MA-GTS (Multi-Agent Graph Theory Solver), a multi-agent\nframework that decomposes these complex problems through agent collaboration.\nMA-GTS maps the implicitly expressed text-based graph data into clear,\nstructured graph representations and dynamically selects the most suitable\nalgorithm based on problem constraints and graph structure scale. This approach\nensures that the solution process remains efficient and the resulting reasoning\npath is interpretable. We validate MA-GTS using the G-REAL dataset, a\nreal-world-inspired graph theory dataset we created. Experimental results show\nthat MA-GTS outperforms state-of-the-art approaches in terms of efficiency,\naccuracy, and scalability, with strong results across multiple benchmarks\n(G-REAL 94.2%, GraCoRe 96.9%, NLGraph 98.4%).MA-GTS is open-sourced at\nhttps://github.com/ZIKEYUAN/MA-GTS.git.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18540v1",
    "published_date": "2025-02-25 08:34:00 UTC",
    "updated_date": "2025-02-25 08:34:00 UTC"
  },
  {
    "arxiv_id": "2502.17955v1",
    "title": "Language Models' Factuality Depends on the Language of Inquiry",
    "authors": [
      "Tushar Aggarwal",
      "Kumar Tanmay",
      "Ayush Agrawal",
      "Kumar Ayush",
      "Hamid Palangi",
      "Paul Pu Liang"
    ],
    "abstract": "Multilingual language models (LMs) are expected to recall factual knowledge\nconsistently across languages, yet they often fail to transfer knowledge\nbetween languages even when they possess the correct information in one of the\nlanguages. For example, we find that an LM may correctly identify Rashed Al\nShashai as being from Saudi Arabia when asked in Arabic, but consistently fails\nto do so when asked in English or Swahili. To systematically investigate this\nlimitation, we introduce a benchmark of 10,000 country-related facts across 13\nlanguages and propose three novel metrics: Factual Recall Score, Knowledge\nTransferability Score, and Cross-Lingual Factual Knowledge Transferability\nScore-to quantify factual recall and knowledge transferability in LMs across\ndifferent languages. Our results reveal fundamental weaknesses in today's\nstate-of-the-art LMs, particularly in cross-lingual generalization where models\nfail to transfer knowledge effectively across different languages, leading to\ninconsistent performance sensitive to the language used. Our findings emphasize\nthe need for LMs to recognize language-specific factual reliability and\nleverage the most trustworthy information across languages. We release our\nbenchmark and evaluation framework to drive future research in multilingual\nknowledge transfer.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.17955v1",
    "published_date": "2025-02-25 08:27:18 UTC",
    "updated_date": "2025-02-25 08:27:18 UTC"
  },
  {
    "arxiv_id": "2502.17951v1",
    "title": "Robust Polyp Detection and Diagnosis through Compositional Prompt-Guided Diffusion Models",
    "authors": [
      "Jia Yu",
      "Yan Zhu",
      "Peiyao Fu",
      "Tianyi Chen",
      "Junbo Huang",
      "Quanlin Li",
      "Pinghong Zhou",
      "Zhihua Wang",
      "Fei Wu",
      "Shuo Wang",
      "Xian Yang"
    ],
    "abstract": "Colorectal cancer (CRC) is a significant global health concern, and early\ndetection through screening plays a critical role in reducing mortality. While\ndeep learning models have shown promise in improving polyp detection,\nclassification, and segmentation, their generalization across diverse clinical\nenvironments, particularly with out-of-distribution (OOD) data, remains a\nchallenge. Multi-center datasets like PolypGen have been developed to address\nthese issues, but their collection is costly and time-consuming. Traditional\ndata augmentation techniques provide limited variability, failing to capture\nthe complexity of medical images. Diffusion models have emerged as a promising\nsolution for generating synthetic polyp images, but the image generation\nprocess in current models mainly relies on segmentation masks as the condition,\nlimiting their ability to capture the full clinical context. To overcome these\nlimitations, we propose a Progressive Spectrum Diffusion Model (PSDM) that\nintegrates diverse clinical annotations-such as segmentation masks, bounding\nboxes, and colonoscopy reports-by transforming them into compositional prompts.\nThese prompts are organized into coarse and fine components, allowing the model\nto capture both broad spatial structures and fine details, generating\nclinically accurate synthetic images. By augmenting training data with\nPSDM-generated samples, our model significantly improves polyp detection,\nclassification, and segmentation. For instance, on the PolypGen dataset, PSDM\nincreases the F1 score by 2.12% and the mean average precision by 3.09%,\ndemonstrating superior performance in OOD scenarios and enhanced\ngeneralization.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.17951v1",
    "published_date": "2025-02-25 08:22:45 UTC",
    "updated_date": "2025-02-25 08:22:45 UTC"
  },
  {
    "arxiv_id": "2502.17947v1",
    "title": "DeepSeek-R1 Outperforms Gemini 2.0 Pro, OpenAI o1, and o3-mini in Bilingual Complex Ophthalmology Reasoning",
    "authors": [
      "Pusheng Xu",
      "Yue Wu",
      "Kai Jin",
      "Xiaolan Chen",
      "Mingguang He",
      "Danli Shi"
    ],
    "abstract": "Purpose: To evaluate the accuracy and reasoning ability of DeepSeek-R1 and\nthree other recently released large language models (LLMs) in bilingual complex\nophthalmology cases. Methods: A total of 130 multiple-choice questions (MCQs)\nrelated to diagnosis (n = 39) and management (n = 91) were collected from the\nChinese ophthalmology senior professional title examination and categorized\ninto six topics. These MCQs were translated into English using DeepSeek-R1. The\nresponses of DeepSeek-R1, Gemini 2.0 Pro, OpenAI o1 and o3-mini were generated\nunder default configurations between February 15 and February 20, 2025.\nAccuracy was calculated as the proportion of correctly answered questions, with\nomissions and extra answers considered incorrect. Reasoning ability was\nevaluated through analyzing reasoning logic and the causes of reasoning error.\nResults: DeepSeek-R1 demonstrated the highest overall accuracy, achieving 0.862\nin Chinese MCQs and 0.808 in English MCQs. Gemini 2.0 Pro, OpenAI o1, and\nOpenAI o3-mini attained accuracies of 0.715, 0.685, and 0.692 in Chinese MCQs\n(all P<0.001 compared with DeepSeek-R1), and 0.746 (P=0.115), 0.723 (P=0.027),\nand 0.577 (P<0.001) in English MCQs, respectively. DeepSeek-R1 achieved the\nhighest accuracy across five topics in both Chinese and English MCQs. It also\nexcelled in management questions conducted in Chinese (all P<0.05). Reasoning\nability analysis showed that the four LLMs shared similar reasoning logic.\nIgnoring key positive history, ignoring key positive signs, misinterpretation\nmedical data, and too aggressive were the most common causes of reasoning\nerrors. Conclusion: DeepSeek-R1 demonstrated superior performance in bilingual\ncomplex ophthalmology reasoning tasks than three other state-of-the-art LLMs.\nWhile its clinical applicability remains challenging, it shows promise for\nsupporting diagnosis and clinical decision-making.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.PF"
    ],
    "primary_category": "cs.CL",
    "comment": "29 pages, 4 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2502.17947v1",
    "published_date": "2025-02-25 08:08:53 UTC",
    "updated_date": "2025-02-25 08:08:53 UTC"
  },
  {
    "arxiv_id": "2502.17941v2",
    "title": "Optimal Brain Apoptosis",
    "authors": [
      "Mingyuan Sun",
      "Zheng Fang",
      "Jiaxu Wang",
      "Junjie Jiang",
      "Delei Kong",
      "Chenming Hu",
      "Yuetong Fang",
      "Renjing Xu"
    ],
    "abstract": "The increasing complexity and parameter count of Convolutional Neural\nNetworks (CNNs) and Transformers pose challenges in terms of computational\nefficiency and resource demands. Pruning has been identified as an effective\nstrategy to address these challenges by removing redundant elements such as\nneurons, channels, or connections, thereby enhancing computational efficiency\nwithout heavily compromising performance. This paper builds on the foundational\nwork of Optimal Brain Damage (OBD) by advancing the methodology of parameter\nimportance estimation using the Hessian matrix. Unlike previous approaches that\nrely on approximations, we introduce Optimal Brain Apoptosis (OBA), a novel\npruning method that calculates the Hessian-vector product value directly for\neach parameter. By decomposing the Hessian matrix across network layers and\nidentifying conditions under which inter-layer Hessian submatrices are\nnon-zero, we propose a highly efficient technique for computing the\nsecond-order Taylor expansion of parameters. This approach allows for a more\nprecise pruning process, particularly in the context of CNNs and Transformers,\nas validated in our experiments including VGG19, ResNet32, ResNet50, and\nViT-B/16 on CIFAR10, CIFAR100 and Imagenet datasets. Our code is available at\nhttps://github.com/NEU-REAL/OBA.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.17941v2",
    "published_date": "2025-02-25 08:03:04 UTC",
    "updated_date": "2025-03-03 12:00:57 UTC"
  },
  {
    "arxiv_id": "2502.18538v1",
    "title": "Revisiting Convolution Architecture in the Realm of DNA Foundation Models",
    "authors": [
      "Yu Bo",
      "Weian Mao",
      "Yanjun Shao",
      "Weiqiang Bai",
      "Peng Ye",
      "Xinzhu Ma",
      "Junbo Zhao",
      "Hao Chen",
      "Chunhua Shen"
    ],
    "abstract": "In recent years, a variety of methods based on Transformer and state space\nmodel (SSM) architectures have been proposed, advancing foundational DNA\nlanguage models. However, there is a lack of comparison between these recent\napproaches and the classical architecture convolutional networks (CNNs) on\nfoundation model benchmarks. This raises the question: are CNNs truly being\nsurpassed by these recent approaches based on transformer and SSM\narchitectures? In this paper, we develop a simple but well-designed CNN-based\nmethod termed ConvNova. ConvNova identifies and proposes three effective\ndesigns: 1) dilated convolutions, 2) gated convolutions, and 3) a dual-branch\nframework for gating mechanisms. Through extensive empirical experiments, we\ndemonstrate that ConvNova significantly outperforms recent methods on more than\nhalf of the tasks across several foundation model benchmarks. For example, in\nhistone-related tasks, ConvNova exceeds the second-best method by an average of\n5.8%, while generally utilizing fewer parameters and enabling faster\ncomputation. In addition, the experiments observed findings that may be related\nto biological characteristics. This indicates that CNNs are still a strong\ncompetitor compared to Transformers and SSMs. We anticipate that this work will\nspark renewed interest in CNN-based methods for DNA foundation models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18538v1",
    "published_date": "2025-02-25 08:00:05 UTC",
    "updated_date": "2025-02-25 08:00:05 UTC"
  },
  {
    "arxiv_id": "2503.04782v1",
    "title": "SMT(LIA) Sampling with High Diversity",
    "authors": [
      "Yong Lai",
      "Junjie Li",
      "Chuan Luo"
    ],
    "abstract": "Satisfiability Modulo Linear Integer Arithmetic, SMT(LIA) for short, is\npivotal across various critical domains. Previous research has primarily\nfocused on SMT solving techniques. However, in practical applications such as\nsoftware and hardware testing, there is a need to generate a diverse set of\nsolutions for use as test inputs. We have developed the first sampling\nframework that integrates local search with CDCL(T) techniques, named HighDiv,\ncapable of generating a highly diverse set of solutions for constraints under\nlinear integer theory. Initially, in the local search phase, we introduced a\nnovel operator called boundary-aware movement. This operator performs random\nmoves by considering the current state's constraints on variables, thereby\nenhancing the diversity of variables during the search process. Furthermore, we\nhave conducted an in-depth study of the preprocessing and variable\ninitialization mechanisms within the framework, which significantly enhances\nthe efficiency of subsequent local searches. Lastly, we use the solutions\nobtained from local search sampling as additional constraints to further\nexplore the solution space using the stochastic CDCL(T) method. Experimental\nresults demonstrate that \\HighDiv generates solutions with greater diversity\ncompared to the state-of-the-art SMT(LIA) sampling tool, MeGASampler.",
    "categories": [
      "cs.LO",
      "cs.AI"
    ],
    "primary_category": "cs.LO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.04782v1",
    "published_date": "2025-02-25 07:53:46 UTC",
    "updated_date": "2025-02-25 07:53:46 UTC"
  },
  {
    "arxiv_id": "2502.17929v1",
    "title": "Integrating Boosted learning with Differential Evolution (DE) Optimizer: A Prediction of Groundwater Quality Risk Assessment in Odisha",
    "authors": [
      "Sonalika Subudhi",
      "Alok Kumar Pati",
      "Sephali Bose",
      "Subhasmita Sahoo",
      "Avipsa Pattanaik",
      "Biswa Mohan Acharya"
    ],
    "abstract": "Groundwater is eventually undermined by human exercises, such as fast\nindustrialization, urbanization, over-extraction, and contamination from\nagrarian and urban sources. From among the different contaminants, the presence\nof heavy metals like cadmium (Cd), chromium (Cr), arsenic (As), and lead (Pb)\nproves to have serious dangers when present in huge concentrations in\ngroundwater. Long-term usage of these poisonous components may lead to\nneurological disorders, kidney failure and different sorts of cancer. To\naddress these issues, this study developed a machine learning-based predictive\nmodel to evaluate the Groundwater Quality Index (GWQI) and identify the main\ncontaminants which are affecting the water quality. It has been achieved with\nthe help of a hybrid machine learning model i.e. LCBoost Fusion . The model has\nundergone several processes like data preprocessing, hyperparameter tuning\nusing Differential Evolution (DE) optimization, and evaluation through\ncross-validation. The LCBoost Fusion model outperforms individual models\n(CatBoost and LightGBM), by achieving low RMSE (0.6829), MSE (0.5102), MAE\n(0.3147) and a high R$^2$ score of 0.9809. Feature importance analysis\nhighlights Potassium (K), Fluoride (F) and Total Hardness (TH) as the most\ninfluential indicators of groundwater contamination. This research successfully\ndemonstrates the application of machine learning in assessing groundwater\nquality risks in Odisha. The proposed LCBoost Fusion model offers a reliable\nand efficient approach for real-time groundwater monitoring and risk\nmitigation. These findings will help the environmental organizations and the\npolicy makers to map out targeted places for sustainable groundwater\nmanagement. Future work will focus on using remote sensing data and developing\nan interactive decision-making system for groundwater quality assessment.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "9 Figures (8 figs in paper and one additional graphical abstract), 9\n  Tables",
    "pdf_url": "http://arxiv.org/pdf/2502.17929v1",
    "published_date": "2025-02-25 07:47:41 UTC",
    "updated_date": "2025-02-25 07:47:41 UTC"
  },
  {
    "arxiv_id": "2503.01865v1",
    "title": "Guiding not Forcing: Enhancing the Transferability of Jailbreaking Attacks on LLMs via Removing Superfluous Constraints",
    "authors": [
      "Junxiao Yang",
      "Zhexin Zhang",
      "Shiyao Cui",
      "Hongning Wang",
      "Minlie Huang"
    ],
    "abstract": "Jailbreaking attacks can effectively induce unsafe behaviors in Large\nLanguage Models (LLMs); however, the transferability of these attacks across\ndifferent models remains limited. This study aims to understand and enhance the\ntransferability of gradient-based jailbreaking methods, which are among the\nstandard approaches for attacking white-box models. Through a detailed analysis\nof the optimization process, we introduce a novel conceptual framework to\nelucidate transferability and identify superfluous constraints-specifically,\nthe response pattern constraint and the token tail constraint-as significant\nbarriers to improved transferability. Removing these unnecessary constraints\nsubstantially enhances the transferability and controllability of\ngradient-based attacks. Evaluated on Llama-3-8B-Instruct as the source model,\nour method increases the overall Transfer Attack Success Rate (T-ASR) across a\nset of target models with varying safety levels from 18.4% to 50.3%, while also\nimproving the stability and controllability of jailbreak behaviors on both\nsource and target models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.01865v1",
    "published_date": "2025-02-25 07:47:41 UTC",
    "updated_date": "2025-02-25 07:47:41 UTC"
  },
  {
    "arxiv_id": "2502.17928v1",
    "title": "Structure-prior Informed Diffusion Model for Graph Source Localization with Limited Data",
    "authors": [
      "Hongyi Chen",
      "Jingtao Ding",
      "Xiaojun Liang",
      "Yong Li",
      "Xiao-Ping Zhang"
    ],
    "abstract": "The source localization problem in graph information propagation is crucial\nfor managing various network disruptions, from misinformation spread to\ninfrastructure failures. While recent deep generative approaches have shown\npromise in this domain, their effectiveness is limited by the scarcity of\nreal-world propagation data. This paper introduces SIDSL\n(\\textbf{S}tructure-prior \\textbf{I}nformed \\textbf{D}iffusion model for\n\\textbf{S}ource \\textbf{L}ocalization), a novel framework that addresses three\nkey challenges in limited-data scenarios: unknown propagation patterns, complex\ntopology-propagation relationships, and class imbalance between source and\nnon-source nodes. SIDSL incorporates topology-aware priors through graph label\npropagation and employs a propagation-enhanced conditional denoiser with a\nGNN-parameterized label propagation module (GNN-LP). Additionally, we propose a\nstructure-prior biased denoising scheme that initializes from structure-based\nsource estimations rather than random noise, effectively countering class\nimbalance issues. Experimental results across four real-world datasets\ndemonstrate SIDSL's superior performance, achieving 7.5-13.3% improvements in\nF1 scores compared to state-of-the-art methods. Notably, when pretrained with\nsimulation data of synthetic patterns, SIDSL maintains robust performance with\nonly 10% of training data, surpassing baselines by more than 18.8%. These\nresults highlight SIDSL's effectiveness in real-world applications where\nlabeled data is scarce.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.17928v1",
    "published_date": "2025-02-25 07:47:22 UTC",
    "updated_date": "2025-02-25 07:47:22 UTC"
  },
  {
    "arxiv_id": "2502.17925v2",
    "title": "LeanProgress: Guiding Search for Neural Theorem Proving via Proof Progress Prediction",
    "authors": [
      "Suozhi Huang",
      "Peiyang Song",
      "Robert Joseph George",
      "Anima Anandkumar"
    ],
    "abstract": "Mathematical reasoning remains a significant challenge for Large Language\nModels (LLMs) due to hallucinations. When combined with formal proof assistants\nlike Lean, these hallucinations can be eliminated through rigorous\nverification, making theorem proving reliable. However, even with formal\nverification, LLMs still struggle with long proofs and complex mathematical\nformalizations. While Lean with LLMs offers valuable assistance with retrieving\nlemmas, generating tactics, or even complete proofs, it lacks a crucial\ncapability: providing a sense of proof progress. This limitation particularly\nimpacts the overall development efficiency in large formalization projects. We\nintroduce LeanProgress, a method that predicts the progress in the proof.\nTraining and evaluating our models made on a large corpus of Lean proofs from\nLean Workbook Plus and Mathlib4 and how many steps remain to complete it, we\nemploy data preprocessing and balancing techniques to handle the skewed\ndistribution of proof lengths. Our experiments show that LeanProgress achieves\nan overall prediction accuracy of 75.1\\% in predicting the amount of progress\nand, hence, the remaining number of steps. When integrated into a best-first\nsearch framework using Reprover, our method shows a 3.8\\% improvement on\nMathlib4 compared to baseline performances of 41.2\\%, particularly for longer\nproofs. These results demonstrate how proof progress prediction can enhance\nboth automated and interactive theorem proving, enabling users to make more\ninformed decisions about proof strategies.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.17925v2",
    "published_date": "2025-02-25 07:46:36 UTC",
    "updated_date": "2025-02-27 17:26:11 UTC"
  },
  {
    "arxiv_id": "2502.17921v1",
    "title": "Unmasking Gender Bias in Recommendation Systems and Enhancing Category-Aware Fairness",
    "authors": [
      "Tahsin Alamgir Kheya",
      "Mohamed Reda Bouadjenek",
      "Sunil Aryal"
    ],
    "abstract": "Recommendation systems are now an integral part of our daily lives. We rely\non them for tasks such as discovering new movies, finding friends on social\nmedia, and connecting job seekers with relevant opportunities. Given their\nvital role, we must ensure these recommendations are free from societal\nstereotypes. Therefore, evaluating and addressing such biases in recommendation\nsystems is crucial. Previous work evaluating the fairness of recommended items\nfails to capture certain nuances as they mainly focus on comparing performance\nmetrics for different sensitive groups. In this paper, we introduce a set of\ncomprehensive metrics for quantifying gender bias in recommendations.\nSpecifically, we show the importance of evaluating fairness on a more granular\nlevel, which can be achieved using our metrics to capture gender bias using\ncategories of recommended items like genres for movies. Furthermore, we show\nthat employing a category-aware fairness metric as a regularization term along\nwith the main recommendation loss during training can help effectively minimize\nbias in the models' output. We experiment on three real-world datasets, using\nfive baseline models alongside two popular fairness-aware models, to show the\neffectiveness of our metrics in evaluating gender bias. Our metrics help\nprovide an enhanced insight into bias in recommended items compared to previous\nmetrics. Additionally, our results demonstrate how incorporating our\nregularization term significantly improves the fairness in recommendations for\ndifferent categories without substantial degradation in overall recommendation\nperformance.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.17921v1",
    "published_date": "2025-02-25 07:37:28 UTC",
    "updated_date": "2025-02-25 07:37:28 UTC"
  },
  {
    "arxiv_id": "2502.17912v2",
    "title": "Decoupled Graph Energy-based Model for Node Out-of-Distribution Detection on Heterophilic Graphs",
    "authors": [
      "Yuhan Chen",
      "Yihong Luo",
      "Yifan Song",
      "Pengwen Dai",
      "Jing Tang",
      "Xiaochun Cao"
    ],
    "abstract": "Despite extensive research efforts focused on OOD detection on images, OOD\ndetection on nodes in graph learning remains underexplored. The dependence\namong graph nodes hinders the trivial adaptation of existing approaches on\nimages that assume inputs to be i.i.d. sampled, since many unique features and\nchallenges specific to graphs are not considered, such as the heterophily\nissue. Recently, GNNSafe, which considers node dependence, adapted energy-based\ndetection to the graph domain with state-of-the-art performance, however, it\nhas two serious issues: 1) it derives node energy from classification logits\nwithout specifically tailored training for modeling data distribution, making\nit less effective at recognizing OOD data; 2) it highly relies on energy\npropagation, which is based on homophily assumption and will cause significant\nperformance degradation on heterophilic graphs, where the node tends to have\ndissimilar distribution with its neighbors. To address the above issues, we\nsuggest training EBMs by MLE to enhance data distribution modeling and remove\nenergy propagation to overcome the heterophily issues. However, training EBMs\nvia MLE requires performing MCMC sampling on both node feature and node\nneighbors, which is challenging due to the node interdependence and discrete\ngraph topology. To tackle the sampling challenge, we introduce DeGEM, which\ndecomposes the learning process into two parts: a graph encoder that leverages\ntopology information for node representations and an energy head that operates\nin latent space. Extensive experiments validate that DeGEM, without OOD\nexposure during training, surpasses previous state-of-the-art methods,\nachieving an average AUROC improvement of 6.71% on homophilic graphs and 20.29%\non heterophilic graphs, and even outperform methods trained with OOD exposure.\nOur code is available at: https://github.com/draym28/DeGEM.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "The first two authors contributed equally to this work; ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.17912v2",
    "published_date": "2025-02-25 07:20:00 UTC",
    "updated_date": "2025-03-17 08:23:08 UTC"
  },
  {
    "arxiv_id": "2502.17911v1",
    "title": "Enhancing Speech Quality through the Integration of BGRU and Transformer Architectures",
    "authors": [
      "Souliman Alghnam",
      "Mohammad Alhussien",
      "Khaled Shaheen"
    ],
    "abstract": "Speech enhancement plays an essential role in improving the quality of speech\nsignals in noisy environments. This paper investigates the efficacy of\nintegrating Bidirectional Gated Recurrent Units (BGRU) and Transformer models\nfor speech enhancement tasks. Through a comprehensive experimental evaluation,\nour study demonstrates the superiority of this hybrid architecture over\ntraditional methods and standalone models. The combined BGRU-Transformer\nframework excels in capturing temporal dependencies and learning complex signal\npatterns, leading to enhanced noise reduction and improved speech quality.\nResults show significant performance gains compared to existing approaches,\nhighlighting the potential of this integrated model in real-world applications.\nThe seamless integration of BGRU and Transformer architectures not only\nenhances system robustness but also opens the road for advanced speech\nprocessing techniques. This research contributes to the ongoing efforts in\nspeech enhancement technology and sets a solid foundation for future\ninvestigations into optimizing model architectures, exploring many application\nscenarios, and advancing the field of speech processing in noisy environments.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.17911v1",
    "published_date": "2025-02-25 07:18:35 UTC",
    "updated_date": "2025-02-25 07:18:35 UTC"
  },
  {
    "arxiv_id": "2502.17910v1",
    "title": "Scaling LLM Pre-training with Vocabulary Curriculum",
    "authors": [
      "Fangyuan Yu"
    ],
    "abstract": "Modern language models rely on static vocabularies, fixed before pretraining,\nin contrast to the adaptive vocabulary acquisition observed in human language\nlearning. To bridge this gap, we introduce vocabulary curriculum learning, an\napproach that improves pretraining efficiency with log-linear scaling gains\nrelative to vocabulary size. Our method alternates between entropy-guided\nvocabulary expansion and model optimization, enabling models to learn\ntransferable representations across diverse tokenization granularities. This\napproach naturally gives rise to an optimal computation allocation pattern:\nlonger tokens capture predictable content, while shorter tokens focus on more\ncomplex, harder-to-predict contexts. Experiments on small-scale GPT models\ndemonstrate improved scaling efficiency, reinforcing the effectiveness of\ndynamic tokenization. We release our code to support further research and plan\nto extend our experiments to larger models and diverse domains.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "under review",
    "pdf_url": "http://arxiv.org/pdf/2502.17910v1",
    "published_date": "2025-02-25 07:18:29 UTC",
    "updated_date": "2025-02-25 07:18:29 UTC"
  },
  {
    "arxiv_id": "2502.19532v1",
    "title": "Opus: A Workflow Intention Framework for Complex Workflow Generation",
    "authors": [
      "Phillip Kingston",
      "Théo Fagnoni",
      "Mahsun Altin"
    ],
    "abstract": "This paper introduces Workflow Intention, a novel framework for identifying\nand encoding process objectives within complex business environments. Workflow\nIntention is the alignment of Input, Process and Output elements defining a\nWorkflow's transformation objective interpreted from Workflow Signal inside\nBusiness Artefacts. It specifies how Input is processed to achieve desired\nOutput, incorporating quality standards, business rules, compliance\nrequirements and constraints. We adopt an end-to-end Business Artefact Encoder\nand Workflow Signal interpretation methodology involving four steps:\nModality-Specific Encoding, Intra-Modality Attention, Inter-Modality Fusion\nAttention then Intention Decoding. We provide training procedures and critical\nloss function definitions. In this paper we introduce the concepts of Workflow\nSignal and Workflow Intention, where Workflow Signal decomposed into Input,\nProcess and Output elements is interpreted from Business Artefacts, and\nWorkflow Intention is a complete triple of these elements. We introduce a\nmathematical framework for representing Workflow Signal as a vector and\nWorkflow Intention as a tensor, formalizing properties of these objects.\nFinally, we propose a modular, scalable, trainable, attention-based multimodal\ngenerative system to resolve Workflow Intention from Business Artefacts.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "1 Figure, 27 Pages",
    "pdf_url": "http://arxiv.org/pdf/2502.19532v1",
    "published_date": "2025-02-25 07:16:46 UTC",
    "updated_date": "2025-02-25 07:16:46 UTC"
  },
  {
    "arxiv_id": "2502.17909v1",
    "title": "FactFlow: Automatic Fact Sheet Generation and Customization from Tabular Dataset via AI Chain Design & Implementation",
    "authors": [
      "Minh Duc Vu",
      "Jieshan Chen",
      "Zhenchang Xing",
      "Qinghua Lu",
      "Xiwei Xu",
      "Qian Fu"
    ],
    "abstract": "With the proliferation of data across various domains, there is a critical\ndemand for tools that enable non-experts to derive meaningful insights without\ndeep data analysis skills. To address this need, existing automatic fact sheet\ngeneration tools offer heuristic-based solutions to extract facts and generate\nstories. However, they inadequately grasp the semantics of data and struggle to\ngenerate narratives that fully capture the semantics of the dataset or align\nthe fact sheet with specific user needs. Addressing these shortcomings, this\npaper introduces \\tool, a novel tool designed for the automatic generation and\ncustomisation of fact sheets. \\tool applies the concept of collaborative AI\nworkers to transform raw tabular dataset into comprehensive, visually\ncompelling fact sheets. We define effective taxonomy to profile AI worker for\nspecialised tasks. Furthermore, \\tool empowers users to refine these fact\nsheets through intuitive natural language commands, ensuring the final outputs\nalign closely with individual preferences and requirements. Our user evaluation\nwith 18 participants confirms that \\tool not only surpasses state-of-the-art\nbaselines in automated fact sheet production but also provides a positive user\nexperience during customization tasks.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "I.2; H.4"
    ],
    "primary_category": "cs.HC",
    "comment": "11 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.17909v1",
    "published_date": "2025-02-25 07:15:41 UTC",
    "updated_date": "2025-02-25 07:15:41 UTC"
  },
  {
    "arxiv_id": "2502.17903v1",
    "title": "Towards Sustainable Web Agents: A Plea for Transparency and Dedicated Metrics for Energy Consumption",
    "authors": [
      "Lars Krupp",
      "Daniel Geißler",
      "Paul Lukowicz",
      "Jakob Karolus"
    ],
    "abstract": "Improvements in the area of large language models have shifted towards the\nconstruction of models capable of using external tools and interpreting their\noutputs. These so-called web agents have the ability to interact autonomously\nwith the internet. This allows them to become powerful daily assistants\nhandling time-consuming, repetitive tasks while supporting users in their daily\nactivities. While web agent research is thriving, the sustainability aspect of\nthis research direction remains largely unexplored. We provide an initial\nexploration of the energy and CO2 cost associated with web agents. Our results\nshow how different philosophies in web agent creation can severely impact the\nassociated expended energy. We highlight lacking transparency regarding the\ndisclosure of model parameters and processes used for some web agents as a\nlimiting factor when estimating energy consumption. As such, our work advocates\na change in thinking when evaluating web agents, warranting dedicated metrics\nfor energy consumption and sustainability.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.17903v1",
    "published_date": "2025-02-25 06:58:40 UTC",
    "updated_date": "2025-02-25 06:58:40 UTC"
  },
  {
    "arxiv_id": "2502.17900v1",
    "title": "Knowledge-enhanced Multimodal ECG Representation Learning with Arbitrary-Lead Inputs",
    "authors": [
      "Che Liu",
      "Cheng Ouyang",
      "Zhongwei Wan",
      "Haozhe Wang",
      "Wenjia Bai",
      "Rossella Arcucci"
    ],
    "abstract": "Recent advances in multimodal ECG representation learning center on aligning\nECG signals with paired free-text reports. However, suboptimal alignment\npersists due to the complexity of medical language and the reliance on a full\n12-lead setup, which is often unavailable in under-resourced settings. To\ntackle these issues, we propose **K-MERL**, a knowledge-enhanced multimodal ECG\nrepresentation learning framework. **K-MERL** leverages large language models\nto extract structured knowledge from free-text reports and employs a lead-aware\nECG encoder with dynamic lead masking to accommodate arbitrary lead inputs.\nEvaluations on six external ECG datasets show that **K-MERL** achieves\nstate-of-the-art performance in zero-shot classification and linear probing\ntasks, while delivering an average **16%** AUC improvement over existing\nmethods in partial-lead zero-shot classification.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.17900v1",
    "published_date": "2025-02-25 06:53:50 UTC",
    "updated_date": "2025-02-25 06:53:50 UTC"
  },
  {
    "arxiv_id": "2502.17898v1",
    "title": "VeriPlan: Integrating Formal Verification and LLMs into End-User Planning",
    "authors": [
      "Christine Lee",
      "David Porfirio",
      "Xinyu Jessica Wang",
      "Kevin Zhao",
      "Bilge Mutlu"
    ],
    "abstract": "Automated planning is traditionally the domain of experts, utilized in fields\nlike manufacturing and healthcare with the aid of expert planning tools. Recent\nadvancements in LLMs have made planning more accessible to everyday users due\nto their potential to assist users with complex planning tasks. However, LLMs\nface several application challenges within end-user planning, including\nconsistency, accuracy, and user trust issues. This paper introduces VeriPlan, a\nsystem that applies formal verification techniques, specifically model\nchecking, to enhance the reliability and flexibility of LLMs for end-user\nplanning. In addition to the LLM planner, VeriPlan includes three additional\ncore features -- a rule translator, flexibility sliders, and a model checker --\nthat engage users in the verification process. Through a user study (n=12), we\nevaluate VeriPlan, demonstrating improvements in the perceived quality,\nusability, and user satisfaction of LLMs. Our work shows the effective\nintegration of formal verification and user-control features with LLMs for\nend-user planning tasks.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "I.2.1; I.2.5; I.2.2; I.2.7"
    ],
    "primary_category": "cs.HC",
    "comment": "In CHI Conference on Human Factors in Computing Systems (CHI '25),\n  April 26-May 1, 2025, Yokohama, Japan. ACM, New York, NY, USA, 19 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.17898v1",
    "published_date": "2025-02-25 06:53:00 UTC",
    "updated_date": "2025-02-25 06:53:00 UTC"
  },
  {
    "arxiv_id": "2503.00037v1",
    "title": "Zero-Shot Defense Against Toxic Images via Inherent Multimodal Alignment in LVLMs",
    "authors": [
      "Wei Zhao",
      "Zhe Li",
      "Yige Li",
      "Jun Sun"
    ],
    "abstract": "Large Vision-Language Models (LVLMs) have made significant strides in\nmultimodal comprehension, thanks to extensive pre-training and fine-tuning on\nlarge-scale visual datasets. However, despite their robust textual safety\nmechanisms, they remain vulnerable to harmful visual inputs. Existing\nsafeguards-typically relying on pre-filtering or fine-tuning-incur high costs\nand diminish overall utility. To address this critical vulnerability, we\nintroduce SafeCLIP, a lightweight method that leverages LVLMs inherent\nmultimodal alignment for zero-shot toxic image detection. By projecting CLIPs\ndiscarded CLS token into its text space and matching it with toxic descriptors,\nSafeCLIP detects harmful content without any architectural changes-adding\nminimal latency and enabling dynamic safety corrections during inference and\nfine-tuning.Experiments show that SafeCLIP achieves a 66.9% defense success\nrate with only 3.2% false positive rate and 7.2% overhead. In contrast,\nstate-of-the-art methods achieve 52.9% success but have a 10.7% false positive\nrate and 210% overhead. Our work demonstrates that leveraging inherent\nmultimodal alignment can yield efficient, low-cost LVLM safety. Code is\navailable at anonymous.4open.science/r/safeclip-2C01.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.00037v1",
    "published_date": "2025-02-25 06:51:16 UTC",
    "updated_date": "2025-02-25 06:51:16 UTC"
  },
  {
    "arxiv_id": "2503.01864v1",
    "title": "Larger or Smaller Reward Margins to Select Preferences for Alignment?",
    "authors": [
      "Kexin Huang",
      "Junkang Wu",
      "Ziqian Chen",
      "Xue Wang",
      "Jinyang Gao",
      "Bolin Ding",
      "Jiancan Wu",
      "Xiangnan He",
      "Xiang Wang"
    ],
    "abstract": "Preference learning is critical for aligning large language models (LLMs)\nwith human values, with the quality of preference datasets playing a crucial\nrole in this process. While existing metrics primarily assess data quality\nbased on either explicit or implicit reward margins, they often provide\ncontradictory evaluations for the same data. To address this issue, we\nintroduce the alignment potential metric, which quantifies the gap from the\nmodel's current implicit reward margin to the target explicit reward margin,\nthereby estimating the model's potential to align with the preference data.\nEmpirical results demonstrate that training on data selected by this metric\nconsistently enhances alignment performance, surpassing existing metrics across\ndifferent base models and optimization objectives. Furthermore, our method\nextends to self-play data generation frameworks, where the metric is used to\nidentify high-quality data within the self-generated content by LLMs. Under\nthis data generation scenario, our method surpasses current state-of-the-art\n(SOTA) results across various training settings and demonstrates continuous\nimprovements in alignment performance as dataset size and training iterations\nincrease.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.01864v1",
    "published_date": "2025-02-25 06:43:24 UTC",
    "updated_date": "2025-02-25 06:43:24 UTC"
  },
  {
    "arxiv_id": "2502.17893v1",
    "title": "Sample-efficient diffusion-based control of complex nonlinear systems",
    "authors": [
      "Hongyi Chen",
      "Jingtao Ding",
      "Jianhai Shu",
      "Xinchun Yu",
      "Xiaojun Liang",
      "Yong Li",
      "Xiao-Ping Zhang"
    ],
    "abstract": "Complex nonlinear system control faces challenges in achieving\nsample-efficient, reliable performance. While diffusion-based methods have\ndemonstrated advantages over classical and reinforcement learning approaches in\nlong-term control performance, they are limited by sample efficiency. This\npaper presents SEDC (Sample-Efficient Diffusion-based Control), a novel\ndiffusion-based control framework addressing three core challenges:\nhigh-dimensional state-action spaces, nonlinear system dynamics, and the gap\nbetween non-optimal training data and near-optimal control solutions. Through\nthree innovations - Decoupled State Diffusion, Dual-Mode Decomposition, and\nGuided Self-finetuning - SEDC achieves 39.5\\%-49.4\\% better control accuracy\nthan baselines while using only 10\\% of the training samples, as validated\nacross three complex nonlinear dynamic systems. Our approach represents a\nsignificant advancement in sample-efficient control of complex nonlinear\nsystems. The implementation of the code can be found at\nhttps://anonymous.4open.science/r/DIFOCON-C019.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.LG",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.17893v1",
    "published_date": "2025-02-25 06:30:04 UTC",
    "updated_date": "2025-02-25 06:30:04 UTC"
  },
  {
    "arxiv_id": "2503.05768v1",
    "title": "A Collection of Innovations in Medical AI for patient records in 2024",
    "authors": [
      "Yuanyun Zhang",
      "Shi Li"
    ],
    "abstract": "The field of Artificial Intelligence in healthcare is evolving at an\nunprecedented pace, driven by rapid advancements in machine learning and the\nrecent breakthroughs in large language models. While these innovations hold\nimmense potential to transform clinical decision making, diagnostics, and\npatient care, the accelerating speed of AI development has outpaced traditional\nacademic publishing cycles. As a result, many scholarly contributions quickly\nbecome outdated, failing to capture the latest state of the art methodologies\nand their real world implications. This paper advocates for a new category of\nacademic publications an annualized citation framework that prioritizes the\nmost recent AI driven healthcare innovations. By systematically referencing the\nbreakthroughs of the year, such papers would ensure that research remains\ncurrent, fostering a more adaptive and informed discourse. This approach not\nonly enhances the relevance of AI research in healthcare but also provides a\nmore accurate reflection of the fields ongoing evolution.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05768v1",
    "published_date": "2025-02-25 06:24:24 UTC",
    "updated_date": "2025-02-25 06:24:24 UTC"
  },
  {
    "arxiv_id": "2502.17887v1",
    "title": "Arrhythmia Classification from 12-Lead ECG Signals Using Convolutional and Transformer-Based Deep Learning Models",
    "authors": [
      "Andrei Apostol",
      "Maria Nutu"
    ],
    "abstract": "In Romania, cardiovascular problems are the leading cause of death,\naccounting for nearly one-third of annual fatalities. The severity of this\nsituation calls for innovative diagnosis method for cardiovascular diseases.\nThis article aims to explore efficient, light-weight and rapid methods for\narrhythmia diagnosis, in resource-constrained healthcare settings. Due to the\nlack of Romanian public medical data, we trained our systems using\ninternational public datasets, having in mind that the ECG signals are the same\nregardless the patients' nationality. Within this purpose, we combined multiple\ndatasets, usually used in the field of arrhythmias classification: PTB-XL\nelectrocardiography dataset , PTB Diagnostic ECG Database, China 12-Lead ECG\nChallenge Database, Georgia 12-Lead ECG Challenge Database, and St. Petersburg\nINCART 12-lead Arrhythmia Database. For the input data, we employed ECG signal\nprocessing methods, specifically a variant of the Pan-Tompkins algorithm,\nuseful in arrhythmia classification because it provides a robust and efficient\nmethod for detecting QRS complexes in ECG signals. Additionally, we used\nmachine learning techniques, widely used for the task of classification,\nincluding convolutional neural networks (1D CNNs, 2D CNNs, ResNet) and Vision\nTransformers (ViTs). The systems were evaluated in terms of accuracy and F1\nscore. We annalysed our dataset from two perspectives. First, we fed the\nsystems with the ECG signals and the GRU-based 1D CNN model achieved the\nhighest accuracy of 93.4% among all the tested architectures. Secondly, we\ntransformed ECG signals into images and the CNN2D model achieved an accuracy of\n92.16%.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "34 pages, 17 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.17887v1",
    "published_date": "2025-02-25 06:17:52 UTC",
    "updated_date": "2025-02-25 06:17:52 UTC"
  },
  {
    "arxiv_id": "2502.17886v1",
    "title": "A graph neural network-based multispectral-view learning model for diabetic macular ischemia detection from color fundus photographs",
    "authors": [
      "Qinghua He",
      "Hongyang Jiang",
      "Danqi Fang",
      "Dawei Yang",
      "Truong X. Nguyen",
      "Anran Ran",
      "Clement C. Tham",
      "Simon K. H. Szeto",
      "Sobha Sivaprasad",
      "Carol Y. Cheung"
    ],
    "abstract": "Diabetic macular ischemia (DMI), marked by the loss of retinal capillaries in\nthe macular area, contributes to vision impairment in patients with diabetes.\nAlthough color fundus photographs (CFPs), combined with artificial intelligence\n(AI), have been extensively applied in detecting various eye diseases,\nincluding diabetic retinopathy (DR), their applications in detecting DMI remain\nunexplored, partly due to skepticism among ophthalmologists regarding its\nfeasibility. In this study, we propose a graph neural network-based\nmultispectral view learning (GNN-MSVL) model designed to detect DMI from CFPs.\nThe model leverages higher spectral resolution to capture subtle changes in\nfundus reflectance caused by ischemic tissue, enhancing sensitivity to\nDMI-related features. The proposed approach begins with computational\nmultispectral imaging (CMI) to reconstruct 24-wavelength multispectral fundus\nimages from CFPs. ResNeXt101 is employed as the backbone for multi-view\nlearning to extract features from the reconstructed images. Additionally, a GNN\nwith a customized jumper connection strategy is designed to enhance\ncross-spectral relationships, facilitating comprehensive and efficient\nmultispectral view learning. The study included a total of 1,078\nmacula-centered CFPs from 1,078 eyes of 592 patients with diabetes, of which\n530 CFPs from 530 eyes of 300 patients were diagnosed with DMI. The model\nachieved an accuracy of 84.7 percent and an area under the receiver operating\ncharacteristic curve (AUROC) of 0.900 (95 percent CI: 0.852-0.937) on\neye-level, outperforming both the baseline model trained from CFPs and human\nexperts (p-values less than 0.01). These findings suggest that AI-based CFP\nanalysis holds promise for detecting DMI, contributing to its early and\nlow-cost screening.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.17886v1",
    "published_date": "2025-02-25 06:17:20 UTC",
    "updated_date": "2025-02-25 06:17:20 UTC"
  },
  {
    "arxiv_id": "2502.17883v1",
    "title": "From underwater to aerial: a novel multi-scale knowledge distillation approach for coral reef monitoring",
    "authors": [
      "Matteo Contini",
      "Victor Illien",
      "Julien Barde",
      "Sylvain Poulain",
      "Serge Bernard",
      "Alexis Joly",
      "Sylvain Bonhommeau"
    ],
    "abstract": "Drone-based remote sensing combined with AI-driven methodologies has shown\ngreat potential for accurate mapping and monitoring of coral reef ecosystems.\nThis study presents a novel multi-scale approach to coral reef monitoring,\nintegrating fine-scale underwater imagery with medium-scale aerial imagery.\nUnderwater images are captured using an Autonomous Surface Vehicle (ASV), while\naerial images are acquired with an aerial drone. A transformer-based\ndeep-learning model is trained on underwater images to detect the presence of\n31 classes covering various coral morphotypes, associated fauna, and habitats.\nThese predictions serve as annotations for training a second model applied to\naerial images. The transfer of information across scales is achieved through a\nweighted footprint method that accounts for partial overlaps between underwater\nimage footprints and aerial image tiles. The results show that the multi-scale\nmethodology successfully extends fine-scale classification to larger reef\nareas, achieving a high degree of accuracy in predicting coral morphotypes and\nassociated habitats. The method showed a strong alignment between\nunderwater-derived annotations and ground truth data, reflected by an AUC (Area\nUnder the Curve) score of 0.9251. This shows that the integration of underwater\nand aerial imagery, supported by deep-learning models, can facilitate scalable\nand accurate reef assessments. This study demonstrates the potential of\ncombining multi-scale imaging and AI to facilitate the monitoring and\nconservation of coral reefs. Our approach leverages the strengths of underwater\nand aerial imagery, ensuring the precision of fine-scale analysis while\nextending it to cover a broader reef area.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.17883v1",
    "published_date": "2025-02-25 06:12:33 UTC",
    "updated_date": "2025-02-25 06:12:33 UTC"
  },
  {
    "arxiv_id": "2502.17882v1",
    "title": "Science Across Languages: Assessing LLM Multilingual Translation of Scientific Papers",
    "authors": [
      "Hannah Calzi Kleidermacher",
      "James Zou"
    ],
    "abstract": "Scientific research is inherently global. However, the vast majority of\nacademic journals are published exclusively in English, creating barriers for\nnon-native-English-speaking researchers. In this study, we leverage large\nlanguage models (LLMs) to translate published scientific articles while\npreserving their native JATS XML formatting, thereby developing a practical,\nautomated approach for implementation by academic journals. Using our approach,\nwe translate articles across multiple scientific disciplines into 28 languages.\nTo evaluate translation accuracy, we introduce a novel question-and-answer (QA)\nbenchmarking method, in which an LLM generates comprehension-based questions\nfrom the original text and then answers them based on the translated text. Our\nbenchmark results show an average performance of 95.9%, showing that the key\nscientific details are accurately conveyed. In a user study, we translate the\nscientific papers of 15 researchers into their native languages, finding that\nthe authors consistently found the translations to accurately capture the\noriginal information in their articles. Interestingly, a third of the authors\nfound many technical terms \"overtranslated,\" expressing a preference to keep\nterminology more familiar in English untranslated. Finally, we demonstrate how\nin-context learning techniques can be used to align translations with\ndomain-specific preferences such as mitigating overtranslation, highlighting\nthe adaptability and utility of LLM-driven scientific translation. The code and\ntranslated articles are available at https://hankleid.github.io/ProjectMundo.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.17882v1",
    "published_date": "2025-02-25 06:08:48 UTC",
    "updated_date": "2025-02-25 06:08:48 UTC"
  },
  {
    "arxiv_id": "2502.17872v1",
    "title": "Contrastive Learning with Nasty Noise",
    "authors": [
      "Ziruo Zhao"
    ],
    "abstract": "Contrastive learning has emerged as a powerful paradigm for self-supervised\nrepresentation learning. This work analyzes the theoretical limits of\ncontrastive learning under nasty noise, where an adversary modifies or replaces\ntraining samples. Using PAC learning and VC-dimension analysis, lower and upper\nbounds on sample complexity in adversarial settings are established.\nAdditionally, data-dependent sample complexity bounds based on the l2-distance\nfunction are derived.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.17872v1",
    "published_date": "2025-02-25 05:55:15 UTC",
    "updated_date": "2025-02-25 05:55:15 UTC"
  },
  {
    "arxiv_id": "2502.17863v1",
    "title": "ASurvey: Spatiotemporal Consistency in Video Generation",
    "authors": [
      "Zhiyu Yin",
      "Kehai Chen",
      "Xuefeng Bai",
      "Ruili Jiang",
      "Juntao Li",
      "Hongdong Li",
      "Jin Liu",
      "Yang Xiang",
      "Jun Yu",
      "Min Zhang"
    ],
    "abstract": "Video generation, by leveraging a dynamic visual generation method, pushes\nthe boundaries of Artificial Intelligence Generated Content (AIGC). Video\ngeneration presents unique challenges beyond static image generation, requiring\nboth high-quality individual frames and temporal coherence to maintain\nconsistency across the spatiotemporal sequence. Recent works have aimed at\naddressing the spatiotemporal consistency issue in video generation, while few\nliterature review has been organized from this perspective. This gap hinders a\ndeeper understanding of the underlying mechanisms for high-quality video\ngeneration. In this survey, we systematically review the recent advances in\nvideo generation, covering five key aspects: foundation models, information\nrepresentations, generation schemes, post-processing techniques, and evaluation\nmetrics. We particularly focus on their contributions to maintaining\nspatiotemporal consistency. Finally, we discuss the future directions and\nchallenges in this field, hoping to inspire further efforts to advance the\ndevelopment of video generation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.17863v1",
    "published_date": "2025-02-25 05:20:51 UTC",
    "updated_date": "2025-02-25 05:20:51 UTC"
  },
  {
    "arxiv_id": "2502.18535v1",
    "title": "A Survey of Zero-Knowledge Proof Based Verifiable Machine Learning",
    "authors": [
      "Zhizhi Peng",
      "Taotao Wang",
      "Chonghe Zhao",
      "Guofu Liao",
      "Zibin Lin",
      "Yifeng Liu",
      "Bin Cao",
      "Long Shi",
      "Qing Yang",
      "Shengli Zhang"
    ],
    "abstract": "As machine learning technologies advance rapidly across various domains,\nconcerns over data privacy and model security have grown significantly. These\nchallenges are particularly pronounced when models are trained and deployed on\ncloud platforms or third-party servers due to the computational resource\nlimitations of users' end devices. In response, zero-knowledge proof (ZKP)\ntechnology has emerged as a promising solution, enabling effective validation\nof model performance and authenticity in both training and inference processes\nwithout disclosing sensitive data. Thus, ZKP ensures the verifiability and\nsecurity of machine learning models, making it a valuable tool for\nprivacy-preserving AI. Although some research has explored the verifiable\nmachine learning solutions that exploit ZKP, a comprehensive survey and summary\nof these efforts remain absent. This survey paper aims to bridge this gap by\nreviewing and analyzing all the existing Zero-Knowledge Machine Learning (ZKML)\nresearch from June 2017 to December 2024. We begin by introducing the concept\nof ZKML and outlining its ZKP algorithmic setups under three key categories:\nverifiable training, verifiable inference, and verifiable testing. Next, we\nprovide a comprehensive categorization of existing ZKML research within these\ncategories and analyze the works in detail. Furthermore, we explore the\nimplementation challenges faced in this field and discuss the improvement works\nto address these obstacles. Additionally, we highlight several commercial\napplications of ZKML technology. Finally, we propose promising directions for\nfuture advancements in this domain.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "24 pages, 5 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2502.18535v1",
    "published_date": "2025-02-25 05:04:27 UTC",
    "updated_date": "2025-02-25 05:04:27 UTC"
  },
  {
    "arxiv_id": "2502.17840v1",
    "title": "A Combinatorial Identities Benchmark for Theorem Proving via Automated Theorem Generation",
    "authors": [
      "Beibei Xiong",
      "Hangyu Lv",
      "Haojia Shan",
      "Jianlin Wang",
      "Zhengfeng Yang",
      "Lihong Zhi"
    ],
    "abstract": "Large language models (LLMs) have significantly advanced formal theorem\nproving, yet the scarcity of high-quality training data constrains their\ncapabilities in complex mathematical domains. Combinatorics, a cornerstone of\nmathematics, provides essential tools for analyzing discrete structures and\nsolving optimization problems. However, its inherent complexity makes it\nparticularly challenging for automated theorem proving (ATP) for combinatorial\nidentities. To address this, we manually construct LeanComb, combinatorial\nidentities benchmark in Lean, which is, to our knowledge, the first formalized\ntheorem proving benchmark built for combinatorial identities. We develop an\nAutomated Theorem Generator for Combinatorial Identities, ATG4CI, which\ncombines candidate tactics suggested by a self-improving large language model\nwith a Reinforcement Learning Tree Search approach for tactic prediction. By\nutilizing ATG4CI, we generate a LeanComb-Enhanced dataset comprising 260K\ncombinatorial identities theorems, each with a complete formal proof in Lean,\nand experimental evaluations demonstrate that models trained on this dataset\ncan generate more effective tactics, thereby improving success rates in\nautomated theorem proving for combinatorial identities.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.17840v1",
    "published_date": "2025-02-25 04:41:49 UTC",
    "updated_date": "2025-02-25 04:41:49 UTC"
  },
  {
    "arxiv_id": "2502.17839v2",
    "title": "Say Less, Mean More: Leveraging Pragmatics in Retrieval-Augmented Generation",
    "authors": [
      "Haris Riaz",
      "Ellen Riloff",
      "Mihai Surdeanu"
    ],
    "abstract": "We propose a simple, unsupervised method that injects pragmatic principles in\nretrieval-augmented generation (RAG) frameworks such as Dense Passage Retrieval\nto enhance the utility of retrieved contexts. Our approach first identifies\nwhich sentences in a pool of documents retrieved by RAG are most relevant to\nthe question at hand, cover all the topics addressed in the input question and\nno more, and then highlights these sentences within their context, before they\nare provided to the LLM, without truncating or altering the context in any\nother way. We show that this simple idea brings consistent improvements in\nexperiments on three question answering tasks (ARC-Challenge, PubHealth and\nPopQA) using five different LLMs. It notably enhances relative accuracy by up\nto 19.7% on PubHealth and 10% on ARC-Challenge compared to a conventional RAG\nsystem.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "16 pages, 2 figures, 8 tables. Preprint",
    "pdf_url": "http://arxiv.org/pdf/2502.17839v2",
    "published_date": "2025-02-25 04:38:38 UTC",
    "updated_date": "2025-02-27 12:55:08 UTC"
  },
  {
    "arxiv_id": "2503.00036v1",
    "title": "A Novel Spatiotemporal Correlation Anomaly Detection Method Based on Time-Frequency-Domain Feature Fusion and a Dynamic Graph Neural Network in Wireless Sensor Network",
    "authors": [
      "Miao Ye",
      "Zhibang Jiang",
      "Xingsi Xue",
      "Xingwang Li",
      "Peng Wen",
      "Yong Wang"
    ],
    "abstract": "Attention-based transformers have played an important role in wireless sensor\nnetwork (WSN) timing anomaly detection due to their ability to capture\nlong-term dependencies. However, there are several issues that must be\naddressed, such as the fact that their ability to capture long-term\ndependencies is not completely reliable, their computational complexity levels\nare high, and the spatiotemporal features of WSN timing data are not\nsufficiently extracted for detecting the correlation anomalies of multinode WSN\ntiming data. To address these limitations, this paper proposes a WSN anomaly\ndetection method that integrates frequency-domain features with dynamic graph\nneural networks (GNN) under a designed self-encoder reconstruction framework.\nFirst, the discrete wavelet transform effectively decomposes trend and seasonal\ncomponents of time series to solve the poor long-term reliability of\ntransformers. Second, a frequency-domain attention mechanism is designed to\nmake full use of the difference between the amplitude distributions of normal\ndata and anomalous data in this domain. Finally, a multimodal fusion-based\ndynamic graph convolutional network (MFDGCN) is designed by combining an\nattention mechanism and a graph convolutional network (GCN) to adaptively\nextract spatial correlation features. A series of experiments conducted on\npublic datasets and their results demonstrate that the anomaly detection method\ndesigned in this paper exhibits superior precision and recall than the existing\nmethods do, with an F1 score of 93.5%, representing an improvement of 2.9% over\nthat of the existing models.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.00036v1",
    "published_date": "2025-02-25 04:34:18 UTC",
    "updated_date": "2025-02-25 04:34:18 UTC"
  },
  {
    "arxiv_id": "2502.17832v2",
    "title": "MM-PoisonRAG: Disrupting Multimodal RAG with Local and Global Poisoning Attacks",
    "authors": [
      "Hyeonjeong Ha",
      "Qiusi Zhan",
      "Jeonghwan Kim",
      "Dimitrios Bralios",
      "Saikrishna Sanniboina",
      "Nanyun Peng",
      "Kai-Wei Chang",
      "Daniel Kang",
      "Heng Ji"
    ],
    "abstract": "Multimodal large language models (MLLMs) equipped with Retrieval Augmented\nGeneration (RAG) leverage both their rich parametric knowledge and the dynamic,\nexternal knowledge to excel in tasks such as Question Answering. While RAG\nenhances MLLMs by grounding responses in query-relevant external knowledge,\nthis reliance poses a critical yet underexplored safety risk: knowledge\npoisoning attacks, where misinformation or irrelevant knowledge is\nintentionally injected into external knowledge bases to manipulate model\noutputs to be incorrect and even harmful. To expose such vulnerabilities in\nmultimodal RAG, we propose MM-PoisonRAG, a novel knowledge poisoning attack\nframework with two attack strategies: Localized Poisoning Attack (LPA), which\ninjects query-specific misinformation in both text and images for targeted\nmanipulation, and Globalized Poisoning Attack (GPA) to provide false guidance\nduring MLLM generation to elicit nonsensical responses across all queries. We\nevaluate our attacks across multiple tasks, models, and access settings,\ndemonstrating that LPA successfully manipulates the MLLM to generate\nattacker-controlled answers, with a success rate of up to 56% on MultiModalQA.\nMoreover, GPA completely disrupts model generation to 0% accuracy with just a\nsingle irrelevant knowledge injection. Our results highlight the urgent need\nfor robust defenses against knowledge poisoning to safeguard multimodal RAG\nframeworks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Code is available at https://github.com/HyeonjeongHa/MM-PoisonRAG",
    "pdf_url": "http://arxiv.org/pdf/2502.17832v2",
    "published_date": "2025-02-25 04:23:59 UTC",
    "updated_date": "2025-03-09 02:52:43 UTC"
  },
  {
    "arxiv_id": "2502.18534v1",
    "title": "MAFE: Multi-Agent Fair Environments for Decision-Making Systems",
    "authors": [
      "Zachary McBride Lazri",
      "Anirudh Nakra",
      "Ivan Brugere",
      "Danial Dervovic",
      "Antigoni Polychroniadou",
      "Furong Huang",
      "Dana Dachman-Soled",
      "Min Wu"
    ],
    "abstract": "Fairness constraints applied to machine learning (ML) models in static\ncontexts have been shown to potentially produce adverse outcomes among\ndemographic groups over time. To address this issue, emerging research focuses\non creating fair solutions that persist over time. While many approaches treat\nthis as a single-agent decision-making problem, real-world systems often\nconsist of multiple interacting entities that influence outcomes. Explicitly\nmodeling these entities as agents enables more flexible analysis of their\ninterventions and the effects they have on a system's underlying dynamics. A\nsignificant challenge in conducting research on multi-agent systems is the lack\nof realistic environments that leverage the limited real-world data available\nfor analysis. To address this gap, we introduce the concept of a Multi-Agent\nFair Environment (MAFE) and present and analyze three MAFEs that model distinct\nsocial systems. Experimental results demonstrate the utility of our MAFEs as\ntestbeds for developing multi-agent fair algorithms.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18534v1",
    "published_date": "2025-02-25 04:03:50 UTC",
    "updated_date": "2025-02-25 04:03:50 UTC"
  },
  {
    "arxiv_id": "2502.17821v1",
    "title": "CAML: Collaborative Auxiliary Modality Learning for Multi-Agent Systems",
    "authors": [
      "Rui Liu",
      "Yu Shen",
      "Peng Gao",
      "Pratap Tokekar",
      "Ming Lin"
    ],
    "abstract": "Multi-modality learning has become a crucial technique for improving the\nperformance of machine learning applications across domains such as autonomous\ndriving, robotics, and perception systems. While existing frameworks such as\nAuxiliary Modality Learning (AML) effectively utilize multiple data sources\nduring training and enable inference with reduced modalities, they primarily\noperate in a single-agent context. This limitation is particularly critical in\ndynamic environments, such as connected autonomous vehicles (CAV), where\nincomplete data coverage can lead to decision-making blind spots. To address\nthese challenges, we propose Collaborative Auxiliary Modality Learning\n($\\textbf{CAML}$), a novel multi-agent multi-modality framework that enables\nagents to collaborate and share multimodal data during training while allowing\ninference with reduced modalities per agent during testing. We systematically\nanalyze the effectiveness of $\\textbf{CAML}$ from the perspective of\nuncertainty reduction and data coverage, providing theoretical insights into\nits advantages over AML. Experimental results in collaborative decision-making\nfor CAV in accident-prone scenarios demonstrate that \\ours~achieves up to a\n${\\bf 58.13}\\%$ improvement in accident detection. Additionally, we validate\n$\\textbf{CAML}$ on real-world aerial-ground robot data for collaborative\nsemantic segmentation, achieving up to a ${\\bf 10.61}\\%$ improvement in mIoU.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.17821v1",
    "published_date": "2025-02-25 03:59:40 UTC",
    "updated_date": "2025-02-25 03:59:40 UTC"
  },
  {
    "arxiv_id": "2503.00035v1",
    "title": "Constraining Sequential Model Editing with Editing Anchor Compression",
    "authors": [
      "Hao-Xiang Xu",
      "Jun-Yu Ma",
      "Zhen-Hua Ling",
      "Ningyu Zhang",
      "Jia-Chen Gu"
    ],
    "abstract": "Large language models (LLMs) struggle with hallucinations due to false or\noutdated knowledge. Given the high resource demands of retraining these models,\nthere is an increasing focus on developing model editing. However, the general\nabilities of LLMs across downstream tasks are prone to significant degradation\nduring sequential editing. This paper statistically observes that the parameter\nmatrix after editing exhibits a significant deviation compared to its previous\nstate as the number of edits increases. This serious deviation affects the\noriginal knowledge associations within LLMs and leads to the degradation of\ntheir general abilities. To this end, a framework termed Editing Anchor\nCompression (EAC) is proposed to constrain the deviation of the parameter\nmatrix during sequential editing. It compresses the editing information by\nselecting editing anchors that are important in encoding new relations without\ndeviating too much from the original matrix, thereby preserving the general\nabilities. Experiments of applying EAC to two popular editing methods on three\nLLMs across four tasks are conducted. Evaluation results show that EAC\neffectively minimizes unreasonable deviations caused by model editing,\npreserving over 70% of the general abilities while better retaining the editing\nknowledge compared to the original counterpart methods.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.00035v1",
    "published_date": "2025-02-25 03:56:49 UTC",
    "updated_date": "2025-02-25 03:56:49 UTC"
  },
  {
    "arxiv_id": "2503.00034v1",
    "title": "MergeIT: From Selection to Merging for Efficient Instruction Tuning",
    "authors": [
      "Hongyi Cai",
      "Yuqian Fu",
      "Hongming Fu",
      "Bo Zhao"
    ],
    "abstract": "Instruction tuning is crucial for optimizing Large Language Models (LLMs),\nyet mainstream data selection methods heavily rely on LLMs as instruction\nquality scorers, leading to high computational costs and reduced data\ndiversity. To address these limitations, we propose MergeIT, a novel LLM-based\nMerging strategy for better Instruction Tuning that shifts the focus from\nselection to synthesis. MergeIT operates in two stages: first, topic-aware\nfiltering clusters and refines the dataset, preserving diversity while\neliminating redundancy without relying on LLM-based scoring. Second, LLM-based\nmerging synthesizes semantically similar instructions into more informative and\ncompact training data, enhancing data richness while further reducing dataset\nsize. Experimental results demonstrate that MergeIT enables efficient, diverse,\nand scalable instruction selection and synthesis, establishing LLM-based\nmerging as a promising alternative to conventional scoring-based selection\nmethods for instruction tuning. Our source code and datasets are now available\nat https://github.com/XcloudFance/MergeIT",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.00034v1",
    "published_date": "2025-02-25 03:43:20 UTC",
    "updated_date": "2025-02-25 03:43:20 UTC"
  },
  {
    "arxiv_id": "2502.17814v1",
    "title": "An Overview of Large Language Models for Statisticians",
    "authors": [
      "Wenlong Ji",
      "Weizhe Yuan",
      "Emily Getzen",
      "Kyunghyun Cho",
      "Michael I. Jordan",
      "Song Mei",
      "Jason E Weston",
      "Weijie J. Su",
      "Jing Xu",
      "Linjun Zhang"
    ],
    "abstract": "Large Language Models (LLMs) have emerged as transformative tools in\nartificial intelligence (AI), exhibiting remarkable capabilities across diverse\ntasks such as text generation, reasoning, and decision-making. While their\nsuccess has primarily been driven by advances in computational power and deep\nlearning architectures, emerging problems -- in areas such as uncertainty\nquantification, decision-making, causal inference, and distribution shift --\nrequire a deeper engagement with the field of statistics. This paper explores\npotential areas where statisticians can make important contributions to the\ndevelopment of LLMs, particularly those that aim to engender trustworthiness\nand transparency for human users. Thus, we focus on issues such as uncertainty\nquantification, interpretability, fairness, privacy, watermarking and model\nadaptation. We also consider possible roles for LLMs in statistical analysis.\nBy bridging AI and statistics, we aim to foster a deeper collaboration that\nadvances both the theoretical foundations and practical applications of LLMs,\nultimately shaping their role in addressing complex societal challenges.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.17814v1",
    "published_date": "2025-02-25 03:40:36 UTC",
    "updated_date": "2025-02-25 03:40:36 UTC"
  },
  {
    "arxiv_id": "2502.17807v1",
    "title": "DocPuzzle: A Process-Aware Benchmark for Evaluating Realistic Long-Context Reasoning Capabilities",
    "authors": [
      "Tianyi Zhuang",
      "Chuqiao Kuang",
      "Xiaoguang Li",
      "Yihua Teng",
      "Jihao Wu",
      "Yasheng Wang",
      "Lifeng Shang"
    ],
    "abstract": "We present DocPuzzle, a rigorously constructed benchmark for evaluating\nlong-context reasoning capabilities in large language models (LLMs). This\nbenchmark comprises 100 expert-level QA problems requiring multi-step reasoning\nover long real-world documents. To ensure the task quality and complexity, we\nimplement a human-AI collaborative annotation-validation pipeline. DocPuzzle\nintroduces an innovative evaluation framework that mitigates guessing bias\nthrough checklist-guided process analysis, establishing new standards for\nassessing reasoning capacities in LLMs. Our evaluation results show that:\n1)Advanced slow-thinking reasoning models like o1-preview(69.7%) and\nDeepSeek-R1(66.3%) significantly outperform best general instruct models like\nClaude 3.5 Sonnet(57.7%); 2)Distilled reasoning models like\nDeepSeek-R1-Distill-Qwen-32B(41.3%) falls far behind the teacher model,\nsuggesting challenges to maintain the generalization of reasoning capabilities\nrelying solely on distillation.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.17807v1",
    "published_date": "2025-02-25 03:29:53 UTC",
    "updated_date": "2025-02-25 03:29:53 UTC"
  },
  {
    "arxiv_id": "2502.18532v1",
    "title": "CuDIP: Enhancing Theorem Proving in LLMs via Curriculum Learning-based Direct Preference Optimization",
    "authors": [
      "Shuming Shi",
      "Ruobing Zuo",
      "Gaolei He",
      "Jianlin Wang",
      "Chenyang Xu",
      "Zhengfeng Yang"
    ],
    "abstract": "Automated theorem proving (ATP) is one of the most challenging mathematical\nreasoning tasks for Large Language Models (LLMs). Most existing LLM-based ATP\nmethods rely on supervised fine-tuning, which results in a limited alignment\nbetween the theorem proving process and human preferences. Direct Preference\nOptimization (DPO), which aligns LLMs with human preferences, has shown\npositive effects for certain tasks. However, the lack of high-quality\npreference data for theorem proving presents a significant challenge. In this\npaper, we innovatively apply DPO to formal automated theorem proving and\nintroduces a Curriculum Learning-based DPO Iterative Theorem Proving (CuDIP)\nmethod. Specifically, we propose a method for constructing preference data\nwhich utilizes LLMs and existing theorem proving data to enhance the diversity\nof the preference data while reducing the reliance on human preference\nannotations. We then integrate this preference data construction method with\ncurriculum learning to iteratively fine-tune the theorem proving model through\nDPO. Experimental results on the MiniF2F and ProofNet datasets demonstrate the\neffectiveness of the proposed method.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18532v1",
    "published_date": "2025-02-25 03:07:02 UTC",
    "updated_date": "2025-02-25 03:07:02 UTC"
  },
  {
    "arxiv_id": "2502.17801v2",
    "title": "Research on Enhancing Cloud Computing Network Security using Artificial Intelligence Algorithms",
    "authors": [
      "Yuqing Wang",
      "Xiao Yang"
    ],
    "abstract": "Cloud computing environments are increasingly vulnerable to security threats\nsuch as distributed denial-of-service (DDoS) attacks and SQL injection.\nTraditional security mechanisms, based on rule matching and feature\nrecognition, struggle to adapt to evolving attack strategies. This paper\nproposes an adaptive security protection framework leveraging deep learning to\nconstruct a multi-layered defense architecture. The proposed system is\nevaluated in a real-world business environment, achieving a detection accuracy\nof 97.3%, an average response time of 18 ms, and an availability rate of\n99.999%. Experimental results demonstrate that the proposed method\nsignificantly enhances detection accuracy, response efficiency, and resource\nutilization, offering a novel and effective approach to cloud computing\nsecurity.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.17801v2",
    "published_date": "2025-02-25 03:06:40 UTC",
    "updated_date": "2025-04-10 16:54:04 UTC"
  },
  {
    "arxiv_id": "2502.17793v2",
    "title": "SYNTHIA: Novel Concept Design with Affordance Composition",
    "authors": [
      "Hyeonjeong Ha",
      "Xiaomeng Jin",
      "Jeonghwan Kim",
      "Jiateng Liu",
      "Zhenhailong Wang",
      "Khanh Duy Nguyen",
      "Ansel Blume",
      "Nanyun Peng",
      "Kai-Wei Chang",
      "Heng Ji"
    ],
    "abstract": "Text-to-image (T2I) models enable rapid concept design, making them widely\nused in AI-driven design. While recent studies focus on generating semantic and\nstylistic variations of given design concepts, functional coherence--the\nintegration of multiple affordances into a single coherent concept--remains\nlargely overlooked. In this paper, we introduce SYNTHIA, a framework for\ngenerating novel, functionally coherent designs based on desired affordances.\nOur approach leverages a hierarchical concept ontology that decomposes concepts\ninto parts and affordances, serving as a crucial building block for\nfunctionally coherent design. We also develop a curriculum learning scheme\nbased on our ontology that contrastively fine-tunes T2I models to progressively\nlearn affordance composition while maintaining visual novelty. To elaborate, we\n(i) gradually increase affordance distance, guiding models from basic\nconcept-affordance association to complex affordance compositions that\nintegrate parts of distinct affordances into a single, coherent form, and (ii)\nenforce visual novelty by employing contrastive objectives to push learned\nrepresentations away from existing concepts. Experimental results show that\nSYNTHIA outperforms state-of-the-art T2I models, demonstrating absolute gains\nof 25.1% and 14.7% for novelty and functional coherence in human evaluation,\nrespectively.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Code is available https://github.com/HyeonjeongHa/SYNTHIA",
    "pdf_url": "http://arxiv.org/pdf/2502.17793v2",
    "published_date": "2025-02-25 02:54:11 UTC",
    "updated_date": "2025-04-10 18:37:28 UTC"
  },
  {
    "arxiv_id": "2502.17787v2",
    "title": "AIR: Complex Instruction Generation via Automatic Iterative Refinement",
    "authors": [
      "Wei Liu",
      "Yancheng He",
      "Hui Huang",
      "Chengwei Hu",
      "Jiaheng Liu",
      "Shilong Li",
      "Wenbo Su",
      "Bo Zheng"
    ],
    "abstract": "With the development of large language models, their ability to follow simple\ninstructions has significantly improved. However, adhering to complex\ninstructions remains a major challenge. Current approaches to generating\ncomplex instructions are often irrelevant to the current instruction\nrequirements or suffer from limited scalability and diversity. Moreover,\nmethods such as back-translation, while effective for simple instruction\ngeneration, fail to leverage the rich contents and structures in large web\ncorpora. In this paper, we propose a novel automatic iterative refinement\nframework to generate complex instructions with constraints, which not only\nbetter reflects the requirements of real scenarios but also significantly\nenhances LLMs' ability to follow complex instructions. The AIR framework\nconsists of two stages: (1)Generate an initial instruction from a document;\n(2)Iteratively refine instructions with LLM-as-judge guidance by comparing the\nmodel's output with the document to incorporate valuable constraints. Finally,\nwe construct the AIR-10K dataset with 10K complex instructions and demonstrate\nthat instructions generated with our approach significantly improve the model's\nability to follow complex instructions, outperforming existing methods for\ninstruction generation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "The first three authors contributed equally, 20 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.17787v2",
    "published_date": "2025-02-25 02:39:57 UTC",
    "updated_date": "2025-02-27 16:42:10 UTC"
  },
  {
    "arxiv_id": "2502.17773v2",
    "title": "Uncertainty Quantification for LLM-Based Survey Simulations",
    "authors": [
      "Chengpiao Huang",
      "Yuhang Wu",
      "Kaizheng Wang"
    ],
    "abstract": "We investigate the use of large language models (LLMs) to simulate human\nresponses to survey questions, and perform uncertainty quantification to gain\nreliable insights. Our approach converts imperfect LLM-simulated responses into\nconfidence sets for population parameters of human responses, addressing the\ndistribution shift between the simulated and real populations. A key innovation\nlies in determining the optimal number of simulated responses: too many produce\noverly narrow confidence sets with poor coverage, while too few yield\nexcessively loose estimates. To resolve this, our method adaptively selects the\nsimulation sample size, ensuring valid average-case coverage guarantees. It is\nbroadly applicable to any LLM, irrespective of its fidelity, and any procedure\nfor constructing confidence sets. Additionally, the selected sample size\nquantifies the degree of misalignment between the LLM and the target human\npopulation. We illustrate our method on real datasets and LLMs.",
    "categories": [
      "stat.ME",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ME",
    "comment": "33 pages, 7 figures, 10 tables",
    "pdf_url": "http://arxiv.org/pdf/2502.17773v2",
    "published_date": "2025-02-25 02:07:29 UTC",
    "updated_date": "2025-05-16 15:19:23 UTC"
  },
  {
    "arxiv_id": "2502.18531v1",
    "title": "Enhancing Hepatopathy Clinical Trial Efficiency: A Secure, Large Language Model-Powered Pre-Screening Pipeline",
    "authors": [
      "Xiongbin Gui",
      "Hanlin Lv",
      "Xiao Wang",
      "Longting Lv",
      "Yi Xiao",
      "Lei Wang"
    ],
    "abstract": "Background: Recruitment for cohorts involving complex liver diseases, such as\nhepatocellular carcinoma and liver cirrhosis, often requires interpreting\nsemantically complex criteria. Traditional manual screening methods are\ntime-consuming and prone to errors. While AI-powered pre-screening offers\npotential solutions, challenges remain regarding accuracy, efficiency, and data\nprivacy. Methods: We developed a novel patient pre-screening pipeline that\nleverages clinical expertise to guide the precise, safe, and efficient\napplication of large language models. The pipeline breaks down complex criteria\ninto a series of composite questions and then employs two strategies to perform\nsemantic question-answering through electronic health records - (1) Pathway A,\nAnthropomorphized Experts' Chain of Thought strategy, and (2) Pathway B, Preset\nStances within an Agent Collaboration strategy, particularly in managing\ncomplex clinical reasoning scenarios. The pipeline is evaluated on three key\nmetrics-precision, time consumption, and counterfactual inference - at both the\nquestion and criterion levels. Results: Our pipeline achieved high precision\n(0.921, in criteria level) and efficiency (0.44s per task). Pathway B excelled\nin complex reasoning, while Pathway A was effective in precise data extraction\nwith faster processing times. Both pathways achieved comparable precision. The\npipeline showed promising results in hepatocellular carcinoma (0.878) and\ncirrhosis trials (0.843). Conclusions: This data-secure and time-efficient\npipeline shows high precision in hepatopathy trials, providing promising\nsolutions for streamlining clinical trial workflows. Its efficiency and\nadaptability make it suitable for improving patient recruitment. And its\ncapability to function in resource-constrained environments further enhances\nits utility in clinical settings.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "30 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.18531v1",
    "published_date": "2025-02-25 02:06:39 UTC",
    "updated_date": "2025-02-25 02:06:39 UTC"
  },
  {
    "arxiv_id": "2502.17771v1",
    "title": "Sample Selection via Contrastive Fragmentation for Noisy Label Regression",
    "authors": [
      "Chris Dongjoo Kim",
      "Sangwoo Moon",
      "Jihwan Moon",
      "Dongyeon Woo",
      "Gunhee Kim"
    ],
    "abstract": "As with many other problems, real-world regression is plagued by the presence\nof noisy labels, an inevitable issue that demands our attention. Fortunately,\nmuch real-world data often exhibits an intrinsic property of continuously\nordered correlations between labels and features, where data points with\nsimilar labels are also represented with closely related features. In response,\nwe propose a novel approach named ConFrag, where we collectively model the\nregression data by transforming them into disjoint yet contrasting\nfragmentation pairs. This enables the training of more distinctive\nrepresentations, enhancing the ability to select clean samples. Our ConFrag\nframework leverages a mixture of neighboring fragments to discern noisy labels\nthrough neighborhood agreement among expert feature extractors. We extensively\nperform experiments on six newly curated benchmark datasets of diverse domains,\nincluding age prediction, price prediction, and music production year\nestimation. We also introduce a metric called Error Residual Ratio (ERR) to\nbetter account for varying degrees of label noise. Our approach consistently\noutperforms fourteen state-of-the-art baselines, being robust against symmetric\nand random Gaussian label noise.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2502.17771v1",
    "published_date": "2025-02-25 02:04:14 UTC",
    "updated_date": "2025-02-25 02:04:14 UTC"
  },
  {
    "arxiv_id": "2502.17764v2",
    "title": "DeepSeek vs. ChatGPT vs. Claude: A Comparative Study for Scientific Computing and Scientific Machine Learning Tasks",
    "authors": [
      "Qile Jiang",
      "Zhiwei Gao",
      "George Em Karniadakis"
    ],
    "abstract": "Large Language Models (LLMs) have emerged as powerful tools for tackling a\nwide range of problems, including those in scientific computing, particularly\nin solving partial differential equations (PDEs). However, different models\nexhibit distinct strengths and preferences, resulting in varying levels of\nperformance. In this paper, we compare the capabilities of the most advanced\nLLMs--DeepSeek, ChatGPT, and Claude--along with their reasoning-optimized\nversions in addressing computational challenges. Specifically, we evaluate\ntheir proficiency in solving traditional numerical problems in scientific\ncomputing as well as leveraging scientific machine learning techniques for\nPDE-based problems. We designed all our experiments so that a non-trivial\ndecision is required, e.g. defining the proper space of input functions for\nneural operator learning. Our findings show that reasoning and hybrid-reasoning\nmodels consistently and significantly outperform non-reasoning ones in solving\nchallenging problems, with ChatGPT o3-mini-high generally offering the fastest\nreasoning speed.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.17764v2",
    "published_date": "2025-02-25 01:49:50 UTC",
    "updated_date": "2025-03-08 22:55:31 UTC"
  },
  {
    "arxiv_id": "2502.17763v1",
    "title": "Design and implementation of a distributed security threat detection system integrating federated learning and multimodal LLM",
    "authors": [
      "Yuqing Wang",
      "Xiao Yang"
    ],
    "abstract": "Traditional security protection methods struggle to address sophisticated\nattack vectors in large-scale distributed systems, particularly when balancing\ndetection accuracy with data privacy concerns. This paper presents a novel\ndistributed security threat detection system that integrates federated learning\nwith multimodal large language models (LLMs). Our system leverages federated\nlearning to ensure data privacy while employing multimodal LLMs to process\nheterogeneous data sources including network traffic, system logs, images, and\nsensor data. Experimental evaluation on a 10TB distributed dataset demonstrates\nthat our approach achieves 96.4% detection accuracy, outperforming traditional\nbaseline models by 4.1 percentage points. The system reduces both false\npositive and false negative rates by 1.8 and 2.4 percentage points\nrespectively. Performance analysis shows that our system maintains efficient\nprocessing capabilities in distributed environments, requiring 180 seconds for\nmodel training and 3.8 seconds for threat detection across the distributed\nnetwork. These results demonstrate significant improvements in detection\naccuracy and computational efficiency while preserving data privacy, suggesting\nstrong potential for real-world deployment in large-scale security systems.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.DC",
      "cs.PF"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.17763v1",
    "published_date": "2025-02-25 01:44:08 UTC",
    "updated_date": "2025-02-25 01:44:08 UTC"
  },
  {
    "arxiv_id": "2502.17751v1",
    "title": "Graded Neural Networks",
    "authors": [
      "Tony Shaska"
    ],
    "abstract": "This paper presents a novel framework for graded neural networks (GNNs) built\nover graded vector spaces $\\V_\\w^n$, extending classical neural architectures\nby incorporating algebraic grading. Leveraging a coordinate-wise grading\nstructure with scalar action $\\lambda \\star \\x = (\\lambda^{q_i} x_i)$, defined\nby a tuple $\\w = (q_0, \\ldots, q_{n-1})$, we introduce graded neurons, layers,\nactivation functions, and loss functions that adapt to feature significance.\nTheoretical properties of graded spaces are established, followed by a\ncomprehensive GNN design, addressing computational challenges like numerical\nstability and gradient scaling. Potential applications span machine learning\nand photonic systems, exemplified by high-speed laser-based implementations.\nThis work offers a foundational step toward graded computation, unifying\nmathematical rigor with practical potential, with avenues for future empirical\nand hardware exploration.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "16W50, 13A02,",
      "I.2; I.2.6"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.17751v1",
    "published_date": "2025-02-25 01:08:07 UTC",
    "updated_date": "2025-02-25 01:08:07 UTC"
  },
  {
    "arxiv_id": "2503.00032v2",
    "title": "Detecting LLM-Generated Korean Text through Linguistic Feature Analysis",
    "authors": [
      "Shinwoo Park",
      "Shubin Kim",
      "Do-Kyung Kim",
      "Yo-Sub Han"
    ],
    "abstract": "The rapid advancement of large language models (LLMs) increases the\ndifficulty of distinguishing between human-written and LLM-generated text.\nDetecting LLM-generated text is crucial for upholding academic integrity,\npreventing plagiarism, protecting copyrights, and ensuring ethical research\npractices. Most prior studies on detecting LLM-generated text focus primarily\non English text. However, languages with distinct morphological and syntactic\ncharacteristics require specialized detection approaches. Their unique\nstructures and usage patterns can hinder the direct application of methods\nprimarily designed for English. Among such languages, we focus on Korean, which\nhas relatively flexible spacing rules, a rich morphological system, and less\nfrequent comma usage compared to English. We introduce KatFish, the first\nbenchmark dataset for detecting LLM-generated Korean text. The dataset consists\nof text written by humans and generated by four LLMs across three genres.\n  By examining spacing patterns, part-of-speech diversity, and comma usage, we\nilluminate the linguistic differences between human-written and LLM-generated\nKorean text. Building on these observations, we propose KatFishNet, a detection\nmethod specifically designed for the Korean language. KatFishNet achieves an\naverage of 19.78% higher AUROC compared to the best-performing existing\ndetection method. Our code and data are available at\nhttps://github.com/Shinwoo-Park/detecting_llm_generated_korean_text_through_linguistic_analysis.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.00032v2",
    "published_date": "2025-02-25 00:59:27 UTC",
    "updated_date": "2025-03-04 06:26:41 UTC"
  },
  {
    "arxiv_id": "2502.17749v2",
    "title": "Detection of LLM-Paraphrased Code and Identification of the Responsible LLM Using Coding Style Features",
    "authors": [
      "Shinwoo Park",
      "Hyundong Jin",
      "Jeong-won Cha",
      "Yo-Sub Han"
    ],
    "abstract": "Recent progress in large language models (LLMs) for code generation has\nraised serious concerns about intellectual property protection. Malicious users\ncan exploit LLMs to produce paraphrased versions of proprietary code that\nclosely resemble the original. While the potential for LLM-assisted code\nparaphrasing continues to grow, research on detecting it remains limited,\nunderscoring an urgent need for detection system. We respond to this need by\nproposing two tasks. The first task is to detect whether code generated by an\nLLM is a paraphrased version of original human-written code. The second task is\nto identify which LLM is used to paraphrase the original code. For these tasks,\nwe construct a dataset LPcode consisting of pairs of human-written code and\nLLM-paraphrased code using various LLMs.\n  We statistically confirm significant differences in the coding styles of\nhuman-written and LLM-paraphrased code, particularly in terms of naming\nconsistency, code structure, and readability. Based on these findings, we\ndevelop LPcodedec, a detection method that identifies paraphrase relationships\nbetween human-written and LLM-generated code, and discover which LLM is used\nfor the paraphrasing. LPcodedec outperforms the best baselines in two tasks,\nimproving F1 scores by 2.64% and 15.17% while achieving speedups of 1,343x and\n213x, respectively. Our code and data are available at\nhttps://github.com/Shinwoo-Park/detecting_llm_paraphrased_code_via_coding_style_features.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.17749v2",
    "published_date": "2025-02-25 00:58:06 UTC",
    "updated_date": "2025-02-28 08:06:00 UTC"
  },
  {
    "arxiv_id": "2502.18529v1",
    "title": "Heterogeneous Decision Making in Mixed Traffic: Uncertainty-aware Planning and Bounded Rationality",
    "authors": [
      "Hang Wang",
      "Qiaoyi Fang",
      "Junshan Zhang"
    ],
    "abstract": "The past few years have witnessed a rapid growth of the deployment of\nautomated vehicles (AVs). Clearly, AVs and human-driven vehicles (HVs) will\nco-exist for many years, and AVs will have to operate around HVs, pedestrians,\ncyclists, and more, calling for fundamental breakthroughs in AI designed for\nmixed traffic to achieve mixed autonomy. Thus motivated, we study heterogeneous\ndecision making by AVs and HVs in a mixed traffic environment, aiming to\ncapture the interactions between human and machine decision-making and develop\nan AI foundation that enables vehicles to operate safely and efficiently. There\nare a number of challenges to achieve mixed autonomy, including 1) humans\ndrivers make driving decisions with bounded rationality, and it remains open to\ndevelop accurate models for HVs' decision making; and 2) uncertainty-aware\nplanning plays a critical role for AVs to take safety maneuvers in response to\nthe human behavior. In this paper, we introduce a formulation of AV-HV\ninteraction, where the HV makes decisions with bounded rationality and the AV\nemploys uncertainty-aware planning based on the prediction on HV's future\nactions. We conduct a comprehensive analysis on AV and HV's learning regret to\nanswer the questions: 1) {How does the learning performance depend on HV's\nbounded rationality and AV's planning}; 2) {How do different decision making\nstrategies impact the overall learning performance}? Our findings reveal some\nintriguing phenomena, such as Goodhart's Law in AV's learning performance and\ncompounding effects in HV's decision making process. By examining the dynamics\nof the regrets, we gain insights into the interplay between human and machine\ndecision making.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.MA",
    "comment": "CPAL 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.18529v1",
    "published_date": "2025-02-25 00:32:33 UTC",
    "updated_date": "2025-02-25 00:32:33 UTC"
  },
  {
    "arxiv_id": "2503.00031v1",
    "title": "Efficient Test-Time Scaling via Self-Calibration",
    "authors": [
      "Chengsong Huang",
      "Langlin Huang",
      "Jixuan Leng",
      "Jiacheng Liu",
      "Jiaxin Huang"
    ],
    "abstract": "Increasing test-time computation is a straightforward approach to enhancing\nthe quality of responses in Large Language Models (LLMs). While Best-of-N\nsampling and Self-Consistency with majority voting are simple and effective,\nthey require a fixed number of sampling responses for each query, regardless of\nits complexity. This could result in wasted computation for simpler questions\nand insufficient exploration for more challenging ones. In this work, we argue\nthat model confidence of responses can be used for improving the efficiency of\ntest-time scaling. Unfortunately, LLMs are known to be overconfident and\nprovide unreliable confidence estimation. To address this limitation, we\nintroduce Self-Calibration by distilling Self-Consistency-derived confidence\ninto the model itself. This enables reliable confidence estimation at test time\nwith one forward pass. We then design confidence-based efficient test-time\nscaling methods to handle queries of various difficulty, such as Early-Stopping\nfor Best-of-N and Self-Consistency with calibrated confidence. Experiments on\nthree LLMs across six datasets demonstrate the effectiveness of our approach.\nSpecifically, applying confidence-based Early Stopping to Best-of-N improves\nMathQA accuracy from 81.0 to 83.6 with a sample budget of 16 responses,\nindicating the efficacy of confidence-based sampling strategy at inference\ntime.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.00031v1",
    "published_date": "2025-02-25 00:21:14 UTC",
    "updated_date": "2025-02-25 00:21:14 UTC"
  }
]
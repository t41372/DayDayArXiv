{
  "date": "2025-09-11",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-09-11 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\nğŸ‘‹ **ä¸€å¥è¯æ€»ç»“ï¼š**\nä»Šå¤©çš„ arXiv å……æ»¡äº†â€œç¡¬æ ¸â€çš„æ¶æ„æ¢ç´¢ä¸åæ€ã€‚**Yann LeCun çš„ JEPA æ¶æ„ç»ˆäºæ€å…¥äº† LLM é¢†åŸŸ**ï¼Œè¯•å›¾æ”¹å˜ä»…ä»…é¢„æµ‹ä¸‹ä¸€ä¸ª token çš„ç°çŠ¶ï¼›Embodied AIï¼ˆå…·èº«æ™ºèƒ½ï¼‰è¿æ¥äº† RLï¼ˆå¼ºåŒ–å­¦ä¹ ï¼‰çš„è§„æ¨¡åŒ–å°è¯•ï¼›è€Œåœ¨å¤§æ¨¡å‹èƒ½åŠ›è¯„ä¼°ä¸Šï¼Œæœ‰ç ”ç©¶æŒ‡å‡ºæˆ‘ä»¬å¯èƒ½è¢«çŸ­ä»»åŠ¡çš„ benchmark è’™è”½äº†åŒçœ¼ï¼Œä½ä¼°äº†æ¨¡å‹åœ¨é•¿ç¨‹ä»»åŠ¡ä¸Šçš„è¿›æ­¥ã€‚\n\n---\n\n### ğŸš€ é‡ç‚¹å…³æ³¨ï¼šæ¶æ„é©æ–°ä¸æ¨ç†èƒ½åŠ›\n\n**1. LLM-JEPA: Large Language Models Meet Joint Embedding Predictive Architectures**\n**LLM-JEPAï¼šå¤§å‹è¯­è¨€æ¨¡å‹é‡ä¸Šè”åˆåµŒå…¥é¢„æµ‹æ¶æ„**\n> **æ ¸å¿ƒæœ¯è¯­ï¼š** Joint Embedding Predictive Architectures (JEPA), Input-space reconstruction, Feature space\n> **ä¸€å¥è¯ç‚¹è¯„ï¼š** Yann LeCun æŒ‚åï¼Œè¯•å›¾å°†è§†è§‰é¢†åŸŸçš„ JEPA èƒœåˆ©å¤åˆ¶åˆ° LLMï¼ŒæŠ›å¼ƒ input-space çš„ç”Ÿæˆï¼Œè½¬å‘ embedding-space çš„é¢„æµ‹ã€‚\n\n**æ ¸å¿ƒè´¡çŒ®ï¼š** ç°æœ‰çš„ LLM é¢„è®­ç»ƒå’Œå¾®è°ƒé«˜åº¦ä¾èµ–äºè¾“å…¥ç©ºé—´çš„é‡å»ºï¼ˆå³é¢„æµ‹ä¸‹ä¸€ä¸ª tokenï¼‰ã€‚ä½†è¿™åœ¨è§†è§‰é¢†åŸŸå·²è¢«è¯æ˜ä¸å¦‚åœ¨åµŒå…¥ç©ºé—´ï¼ˆembedding spaceï¼‰è®­ç»ƒæœ‰æ•ˆã€‚æœ¬æ–‡æå‡ºäº† **LLM-JEPA**ï¼Œä¸€ç§é€‚ç”¨äº LLM çš„ JEPA é£æ ¼ç›®æ ‡å‡½æ•°ã€‚å®éªŒè¡¨æ˜ï¼ŒLLM-JEPA åœ¨ Llama3ã€Gemma2 ç­‰å¤šä¸ªæ¨¡å‹å®¶æ—ä¸Šï¼Œä¸ä»…åœ¨å¾®è°ƒå’Œé¢„è®­ç»ƒä¸­è¶…è¶Šäº†æ ‡å‡†çš„è®­ç»ƒç›®æ ‡ï¼Œè€Œä¸”åœ¨æŠ—è¿‡æ‹Ÿåˆæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚è¿™æ˜¯éç”Ÿæˆå¼è‡ªç›‘ç£å­¦ä¹ åœ¨ NLP é¢†åŸŸçš„ä¸€æ¬¡é‡è¦å°è¯•ã€‚\n\n**2. Tree-OPO: Off-policy Monte Carlo Tree-Guided Advantage Optimization for Multistep Reasoning**\n**Tree-OPOï¼šç”¨äºå¤šæ­¥æ¨ç†çš„å¼‚ç­–ç•¥è’™ç‰¹å¡æ´›æ ‘å¼•å¯¼ä¼˜åŠ¿ä¼˜åŒ–**\n> **æ ¸å¿ƒæœ¯è¯­ï¼š** MCTS, GRPO, Staged Advantage Estimation (SAE), Reasoning\n> **ä¸€å¥è¯ç‚¹è¯„ï¼š** å°† MCTS çš„æœç´¢è½¨è¿¹ä¸ä»…ç”¨äºç”Ÿæˆæ•°æ®ï¼Œæ›´ç›´æ¥ç”¨äºæŒ‡å¯¼ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œè®©æ¨¡å‹å­¦ä¼šâ€œæ€è€ƒâ€çš„è¿‡ç¨‹ã€‚\n\n**æ ¸å¿ƒè´¡çŒ®ï¼š** å— MCTS åœ¨æ•°å­¦å’Œç¬¦å·æ¨ç†ä¸­ç”Ÿæˆé«˜è´¨é‡è½¨è¿¹çš„å¯å‘ï¼Œä½œè€…æ¢ç´¢å¦‚ä½•åˆ©ç”¨ MCTS è½¨è¿¹æ¥æ”¹è¿›éªŒè¯å™¨å¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ ï¼ˆç‰¹åˆ«æ˜¯ GRPOï¼‰ã€‚æ–‡ç« æå‡ºäº† **Staged Advantage Estimation (SAE)**ï¼Œè¿™æ˜¯ä¸€ç§è®¡ç®—ä½æ–¹å·®ã€å‰ç¼€æ„ŸçŸ¥ï¼ˆprefix-awareï¼‰ä¼˜åŠ¿çš„æ–¹æ³•ã€‚é€šè¿‡å°†å¥–åŠ±æŠ•å½±åˆ°æ ‘ç»“æ„çº¦æŸé›†ä¸Šï¼ŒTree-OPO åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šæ˜¾è‘—æé«˜äº†æœ€ç»ˆå‡†ç¡®ç‡ï¼Œå¹¶ä»ç†è®ºä¸Šè¯æ˜äº†å…¶èƒ½å‡å°‘æ¢¯åº¦æ–¹å·®ã€‚\n\n**3. Latency and Token-Aware Test-Time Compute**\n**å»¶è¿Ÿä¸ Token æ„ŸçŸ¥çš„æµ‹è¯•æ—¶è®¡ç®— (Test-Time Compute)**\n> **æ ¸å¿ƒæœ¯è¯­ï¼š** Inference-time scaling, Test-time compute, Dynamic allocation\n> **ä¸€å¥è¯ç‚¹è¯„ï¼š** Inference-time scaling æ˜¯ç°åœ¨çš„æ˜¾å­¦ï¼Œè¿™ç¯‡æ–‡ç« æŠŠâ€œæ—¶é—´å»¶è¿Ÿâ€å’Œâ€œTokenæˆæœ¬â€æ˜¾å¼åœ°çº³å…¥äº†åŠ¨æ€è®¡ç®—åˆ†é…çš„è€ƒé‡ã€‚\n\n**æ ¸å¿ƒè´¡çŒ®ï¼š** ç°æœ‰çš„ Test-time compute å·¥ä½œé€šå¸¸åªå…³æ³¨å¹¶è¡Œç”Ÿæˆï¼ˆå¦‚ Best-of-Nï¼‰ï¼Œå¿½ç•¥äº†åƒ Beam Search è¿™æ ·çš„å¢é‡è§£ç æ–¹æ³•ï¼Œä¸”å¾€å¾€åªçœ‹ Token æ•°é‡ä¸çœ‹å»¶è¿Ÿã€‚æœ¬æ–‡å°† inference-time scaling å½¢å¼åŒ–ä¸ºä¸€ä¸ªåŠ¨æ€è®¡ç®—åˆ†é…é—®é¢˜ï¼Œç³»ç»Ÿå¿…é¡»å†³å®šå¯¹æ¯ä¸ªæŸ¥è¯¢ä½¿ç”¨å“ªç§ç­–ç•¥ï¼ˆå¦‚ Beam Search vs Parallel Samplingï¼‰ä»¥åŠåˆ†é…å¤šå°‘è®¡ç®—é‡ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™ç§æ„ŸçŸ¥å»¶è¿Ÿçš„æ–¹æ³•åœ¨æ¨ç†åŸºå‡†ä¸Šå®ç°äº†æ›´å¥½çš„ç²¾åº¦-æˆæœ¬æƒè¡¡ã€‚\n\n---\n\n### ğŸ¤– å…·èº«æ™ºèƒ½ä¸æœºå™¨äºº (Embodied AI)\n\n**4. SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning**\n**SimpleVLA-RLï¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ æ‰©å±• VLA è®­ç»ƒ**\n> **æ ¸å¿ƒæœ¯è¯­ï¼š** Vision-Language-Action (VLA), Reinforcement Learning, Pushcut phenomenon\n> **ä¸€å¥è¯ç‚¹è¯„ï¼š** è¯æ˜äº† RL ä¸ä»…èƒ½æå‡ LLM çš„æ¨ç†ï¼Œä¹Ÿèƒ½å¤§å¹…æå‡ VLAï¼ˆè§†è§‰-è¯­è¨€-åŠ¨ä½œï¼‰æ¨¡å‹çš„é•¿ç¨‹è§„åˆ’èƒ½åŠ›ã€‚\n\n**æ ¸å¿ƒè´¡çŒ®ï¼š** VLA æ¨¡å‹é€šå¸¸å—é™äºæ˜‚è´µçš„äººç±»æ“ä½œè½¨è¿¹æ•°æ®ã€‚æœ¬æ–‡æå‡ºäº† **SimpleVLA-RL**ï¼Œä¸€ä¸ªåŸºäº veRL çš„é«˜æ•ˆ RL æ¡†æ¶ã€‚å®ƒé€šè¿‡ VLA ç‰¹å®šçš„è½¨è¿¹é‡‡æ ·ã€å¹¶è¡ŒåŒ–å’Œå¤šç¯å¢ƒæ¸²æŸ“ï¼Œè®© OpenVLA æ¨¡å‹åœ¨æ¨¡æ‹Ÿç¯å¢ƒï¼ˆLIBERO, RoboTwinï¼‰ä¸­é€šè¿‡ RL è‡ªæˆ‘æå‡ï¼Œç”šè‡³è¶…è¶Šäº† Google çš„ $Ï€_0$ æ¨¡å‹ã€‚ä½œè€…è¿˜å‘ç°äº†ä¸€ä¸ªæœ‰è¶£çš„ **\"pushcut\"** ç°è±¡ï¼šç­–ç•¥åœ¨ RL è®­ç»ƒä¸­å‘ç°äº†ä¹‹å‰ SFT æ•°æ®ä¸­æœªæ›¾è§è¿‡çš„æ“ä½œæ¨¡å¼ã€‚\n\n**5. Imagined Autocurricula**\n**æƒ³è±¡çš„è‡ªåŠ¨è¯¾ç¨‹**\n> **æ ¸å¿ƒæœ¯è¯­ï¼š** World Models, Unsupervised Environment Design (UED), Autocurricula\n> **ä¸€å¥è¯ç‚¹è¯„ï¼š** åœ¨ä¸–ç•Œæ¨¡å‹é‡Œâ€œåšæ¢¦â€è®­ç»ƒæœºå™¨äººï¼Œå¹¶ä¸”è‡ªåŠ¨ç”Ÿæˆç”±æ˜“åˆ°éš¾çš„è¯¾ç¨‹ã€‚\n\n**æ ¸å¿ƒè´¡çŒ®ï¼š** è®­ç»ƒå…·èº«æ™ºèƒ½ä½“é€šå¸¸ç¼ºä¹æ•°æ®ã€‚æœ¬æ–‡åˆ©ç”¨ **ä¸–ç•Œæ¨¡å‹ (World Models)** ç”Ÿæˆæƒ³è±¡çš„ç¯å¢ƒï¼Œå¹¶æå‡º **IMAC** æ–¹æ³•ï¼Œåˆ©ç”¨æ— ç›‘ç£ç¯å¢ƒè®¾è®¡ï¼ˆUEDï¼‰åœ¨ç”Ÿæˆçš„å³ä¸–ç•Œä¸­å¼•å…¥è‡ªåŠ¨è¯¾ç¨‹ã€‚ç»“æœæ˜¾ç¤ºï¼Œä»…åœ¨ä¸–ç•Œæ¨¡å‹ä¸­è®­ç»ƒçš„ Agentï¼Œèƒ½å¤Ÿå¾ˆå¥½åœ°è¿ç§»åˆ°ä»æœªè§è¿‡çš„å¤æ‚ç¯å¢ƒä¸­ï¼Œè¿™ä¸ºåˆ©ç”¨å¤§è§„æ¨¡åŸºç¡€ä¸–ç•Œæ¨¡å‹è®­ç»ƒé€šç”¨ Agent æŒ‡æ˜äº†æ–¹å‘ã€‚\n\n---\n\n### ğŸ“‰ æ¨¡å‹è¯„ä¼°ä¸å®‰å…¨ (Evaluation & Safety)\n\n**6. The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs**\n**æ”¶ç›Šé€’å‡çš„é”™è§‰ï¼šåœ¨å¤§æ¨¡å‹ä¸­è¡¡é‡é•¿ç¨‹æ‰§è¡Œèƒ½åŠ›**\n> **æ ¸å¿ƒæœ¯è¯­ï¼š** Long-horizon tasks, Execution capability, Self-conditioning\n> **ä¸€å¥è¯ç‚¹è¯„ï¼š** åˆ«è¢«çŸ­ä»»åŠ¡çš„ benchmark éª—äº†ï¼Œæ¨¡å‹èƒ½åŠ›çš„å¾®å°æå‡åœ¨é•¿ä»»åŠ¡ä¸­æ˜¯æŒ‡æ•°çº§çš„å·®å¼‚ã€‚\n\n**æ ¸å¿ƒè´¡çŒ®ï¼š** äººä»¬åœ¨äº‰è®º LLM æ˜¯å¦é‡åˆ°äº† Scaling Law çš„ç“¶é¢ˆï¼ˆæ”¶ç›Šé€’å‡ï¼‰ã€‚æœ¬æ–‡åé©³äº†è¿™ä¸€è§‚ç‚¹ï¼ŒæŒ‡å‡ºçŸ­ä»»åŠ¡åŸºå‡†æµ‹è¯•æ©ç›–äº†è¿›æ­¥ã€‚åœ¨é•¿ç¨‹ä»»åŠ¡ä¸­ï¼Œå•æ­¥å‡†ç¡®ç‡çš„å¾®å°æå‡ä¼šå¤åˆä¸ºå·¨å¤§çš„æˆåŠŸç‡å·®å¼‚ã€‚ä½œè€…è¿˜å‘ç°äº†ä¸€ä¸ª **Self-conditioningï¼ˆè‡ªæˆ‘æ¡ä»¶åŒ–ï¼‰** æ•ˆåº”ï¼šæ¨¡å‹åœ¨ä¸Šä¸‹æ–‡ä¸­çœ‹åˆ°è‡ªå·±ä¹‹å‰çš„é”™è¯¯æ—¶ï¼Œæ›´å®¹æ˜“å†çŠ¯é”™ã€‚è¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆâ€œæ€è€ƒâ€ï¼ˆThinking/CoTï¼‰èƒ½ç¼“è§£è¿™ä¸€é—®é¢˜å¹¶æ”¯æŒæ›´é•¿ä»»åŠ¡æ‰§è¡Œçš„åŸå› ã€‚\n\n**7. The Coding Limits of Robust Watermarking for Generative Models**\n**ç”Ÿæˆæ¨¡å‹é²æ£’æ°´å°çš„ç¼–ç æé™**\n> **æ ¸å¿ƒæœ¯è¯­ï¼š** Cryptographic watermarking, Zero-bit tamper-detection, Critical corruption rate\n> **ä¸€å¥è¯ç‚¹è¯„ï¼š** ä»ä¿¡æ¯è®ºè§’åº¦ç»™æ°´å°æ³¼å†·æ°´ï¼šå¦‚æœæ”»å‡»è€…èƒ½ç ´åè¶…è¿‡ä¸€åŠçš„æ¯”ç‰¹ï¼Œä»»ä½•æ°´å°éƒ½æ— æ³•å¹¸å­˜ã€‚\n\n**æ ¸å¿ƒè´¡çŒ®ï¼š** è¿™æ˜¯ä¸€ä¸ªç†è®ºåˆ†æå·¥ä½œã€‚ä½œè€…è¯æ˜äº†å¯¹äºå¤§å°ä¸º $q$ çš„å­—æ¯è¡¨ï¼Œå­˜åœ¨ä¸€ä¸ªä¸´ç•Œç ´åç‡ $1 - 1/q$ã€‚å¦‚æœæ”»å‡»è€…æ”¹å˜ç¬¦å·çš„æ¯”ä¾‹è¶…è¿‡è¿™ä¸ªå€¼ï¼Œä»»ä½•æ°´å°æ–¹æ¡ˆéƒ½æ— æ³•å¯é åœ°æ£€æµ‹ç¯¡æ”¹ã€‚å¯¹äºäºŒè¿›åˆ¶æƒ…å†µï¼Œè¿™æ„å‘³ç€åªè¦ä¿®æ”¹è¶…è¿‡ 50% çš„ä½ï¼Œæ°´å°å°±å¤±æ•ˆäº†ã€‚å®éªŒæ˜¾ç¤ºï¼Œç®€å•çš„è£å‰ªå’Œç¼©æ”¾æ“ä½œå°±èƒ½åœ¨ä¸ç ´åå›¾åƒè§†è§‰è´¨é‡çš„æƒ…å†µä¸‹ï¼Œç ´åç›®å‰æœ€å…ˆè¿›çš„å›¾åƒæ°´å°ã€‚\n\n**8. Prompt Pirates Need a Map: Stealing Seeds helps Stealing Prompts**\n**æç¤ºè¯æµ·ç›—éœ€è¦åœ°å›¾ï¼šçªƒå–éšæœºç§å­æœ‰åŠ©äºçªƒå–æç¤ºè¯**\n> **æ ¸å¿ƒæœ¯è¯­ï¼š** Prompt stealing, Diffusion models, CWE-339, SeedSnitch\n> **ä¸€å¥è¯ç‚¹è¯„ï¼š** å‘ç°äº†ä¸€ä¸ªä¸¥é‡çš„ PyTorch/åº“çº§æ¼æ´ï¼Œé»‘å®¢å¯ä»¥é€šè¿‡æš´åŠ›ç ´è§£éšæœºç§å­æ¥å®Œç¾è¿˜åŸå›¾ç‰‡ç”Ÿæˆçš„ Promptã€‚\n\n**æ ¸å¿ƒè´¡çŒ®ï¼š** è¿™æ˜¯ä¸€ä¸ªå®æˆ˜å‹çš„å®‰å…¨ç ”ç©¶ã€‚ä½œè€…å‘ç° PyTorch åœ¨ CPU ä¸Šç”Ÿæˆåˆå§‹éšæœºå™ªå£°æ—¶ï¼Œç§å­å€¼è¢«é™åˆ¶åœ¨ $2^{32}$ èŒƒå›´å†…ï¼ˆCWE-339æ¼æ´ï¼‰ã€‚è¿™æ„å‘³ç€å¯ä»¥é€šè¿‡æš´åŠ›ç ´è§£ï¼ˆçº¦ 140 åˆ†é’Ÿï¼‰æ¢å¤ç”Ÿæˆå›¾ç‰‡çš„ Seedã€‚ä¸€æ—¦æ‹¿åˆ°äº† Seedï¼Œä½œè€…æå‡ºçš„ **PromptPirate** ç®—æ³•å°±èƒ½æé«˜ç²¾åº¦åœ°åæ¨åŸå§‹ Promptã€‚è¿™ç›´æ¥å¨èƒäº† CivitAI ç­‰å¹³å°ä¸Šåˆ›ä½œè€…çš„çŸ¥è¯†äº§æƒã€‚\n\n---\n\n### ğŸ› ï¸ æ•ˆç‡ä¸ç³»ç»Ÿ (Efficiency & Systems)\n\n**9. SWE-Effi: Re-Evaluating Software AI Agent System Effectiveness Under Resource Constraints**\n**SWE-Effiï¼šåœ¨èµ„æºçº¦æŸä¸‹é‡æ–°è¯„ä¼°è½¯ä»¶ AI Agent ç³»ç»Ÿçš„æœ‰æ•ˆæ€§**\n> **æ ¸å¿ƒæœ¯è¯­ï¼š** SWE-bench, Resource-efficiency, Token snowball\n> **ä¸€å¥è¯ç‚¹è¯„ï¼š** å…‰çœ‹ä¿® Bug çš„æˆåŠŸç‡æ˜¯ä¸å¤Ÿçš„ï¼Œå¦‚æœä¿®ä¸€ä¸ª Bug è¦èŠ±å‡ ç™¾ç¾å…ƒï¼Œé‚£å°±æ²¡æ„ä¹‰ã€‚\n\n**æ ¸å¿ƒè´¡çŒ®ï¼š** ç°æœ‰çš„ SWE-bench æ¦œå•åªçœ‹å‡†ç¡®ç‡ã€‚æœ¬æ–‡æå‡ºäº† **SWE-Effi** æŒ‡æ ‡ï¼Œç»¼åˆè€ƒè™‘ Token æ¶ˆè€—å’Œæ—¶é—´æˆæœ¬ã€‚ç ”ç©¶å‘ç°ï¼Œè®¸å¤šé«˜åˆ† Agent å­˜åœ¨ **\"Token Snowball\"ï¼ˆToken é›ªçƒï¼‰** æ•ˆåº”ï¼Œæˆ–è€…é™·å…¥ **\"Expensive Failures\"ï¼ˆæ˜‚è´µçš„å¤±è´¥ï¼‰**â€”â€”åœ¨æ— æ³•è§£å†³çš„ä»»åŠ¡ä¸Šæµªè´¹å¤§é‡èµ„æºã€‚è¿™å¯¹äºå®é™…éƒ¨ç½²è‡³å…³é‡è¦ã€‚\n\n**10. ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable Orthogonal Butterfly Transforms**\n**ButterflyQuantï¼šé€šè¿‡å¯å­¦ä¹ æ­£äº¤è´è¶å˜æ¢å®ç°çš„è¶…ä½æ¯”ç‰¹ LLM é‡åŒ–**\n> **æ ¸å¿ƒæœ¯è¯­ï¼š** Quantization, Outlier suppression, Butterfly transforms\n> **ä¸€å¥è¯ç‚¹è¯„ï¼š** ç”¨è¿ç»­å¯å¾®çš„â€œè´è¶å˜æ¢â€æ›¿ä»£å›ºå®šçš„å“ˆè¾¾ç›çŸ©é˜µæ¥å¤„ç†é‡åŒ–ä¸­çš„ç¦»ç¾¤å€¼ï¼Œ2-bit é‡åŒ–æ•ˆæœæ˜¾è‘—æå‡ã€‚\n\n**æ ¸å¿ƒè´¡çŒ®ï¼š** ä¸ºäº†è§£å†³ 2-bit é‡åŒ–ä¸­æ¿€æ´»å€¼ç¦»ç¾¤ç‚¹çš„é—®é¢˜ï¼Œç°æœ‰çš„ QuIP ç­‰æ–¹æ³•ä½¿ç”¨å›ºå®šçš„æ—‹è½¬çŸ©é˜µã€‚æœ¬æ–‡æå‡ºçš„ **ButterflyQuant** ä½¿ç”¨å¯å­¦ä¹ çš„è´è¶å˜æ¢ï¼ˆButterfly Transformsï¼‰ï¼Œå‚æ•°åŒ–ä¸ºè¿ç»­çš„æ—‹è½¬è§’åº¦ï¼Œä½¿å…¶å¯å¾®å¹¶èƒ½é’ˆå¯¹ç‰¹å®šå±‚è¿›è¡Œä¼˜åŒ–ã€‚åœ¨ Llama-2-7B ä¸Šçš„ 2-bit é‡åŒ–å›°æƒ‘åº¦ä» 37.3 é™åˆ°äº† 15.4ã€‚\n\n---\n\n### âš¡ å¿«é€Ÿæ å½± (Quick Reads)\n\n*   **[22. ZORRO]** **Split Learning å®‰å…¨æ€§**ï¼šé’ˆå¯¹åˆ†å¸ƒå¼å­¦ä¹ ä¸­çš„ Split Learning æå‡ºäº†ä¸€ç§åŸºäºé›¶çŸ¥è¯†è¯æ˜ï¼ˆZKPï¼‰çš„é˜²å¾¡æœºåˆ¶ï¼Œé˜²æ­¢æ¶æ„å®¢æˆ·ç«¯æŠ•æ¯’ã€‚\n*   **[24. LLMs Don't Know Their Own Decision Boundaries]** **XAI**ï¼šç ”ç©¶å‘ç° LLM ç”Ÿæˆçš„â€œåäº‹å®è§£é‡Šâ€ï¼ˆSelf-Generated Counterfactualsï¼‰éå¸¸ä¸é è°±ï¼Œé€šå¸¸æ— æ³•çœŸå®åæ˜ æ¨¡å‹çš„å†³ç­–è¾¹ç•Œã€‚\n*   **[49. SEDM]** **Agent Memory**ï¼šä¸º Agent è®¾è®¡çš„â€œè‡ªè¿›åŒ–åˆ†å¸ƒå¼å­˜å‚¨â€ï¼Œè§£å†³äº†å¤š Agent åä½œä¸­è®°å¿†è†¨èƒ€å’Œå™ªå£°ç§¯ç´¯çš„é—®é¢˜ã€‚\n*   **[73. TAM Bench]** **AutoML Agent Benchmark**ï¼šä¸€ä¸ªæ–°çš„ benchmarkï¼Œä¸“é—¨è¯„ä¼° Agent åšç«¯åˆ°ç«¯æœºå™¨å­¦ä¹ ä»»åŠ¡ï¼ˆå¦‚ Kaggle ç«èµ›ï¼‰çš„èƒ½åŠ›ã€‚\n*   **[77. MatCha]** **ææ–™ç§‘å­¦ VLM**ï¼šç¬¬ä¸€ä¸ªé’ˆå¯¹ææ–™è¡¨å¾å›¾åƒç†è§£çš„å¤šæ¨¡æ€ benchmarkï¼Œæ˜¾ç¤ºç°æœ‰çš„ MLLM ç¦»äººç±»ä¸“å®¶å·®è·å·¨å¤§ã€‚\n*   **[9. LLMs as Agentic Cooperative Players in Multiplayer UNO]** **æ¸¸æˆ AI**ï¼šè®© LLM ç© UNO ç‰Œï¼Œè€Œä¸”æ˜¯ä½œä¸ºåˆä½œè€…å»å¸®åŠ©åˆ«äººèµ¢ã€‚ç»“æœå‘ç°å¤§æ¨¡å‹è™½ç„¶èƒ½ç©ï¼Œä½†å¾ˆéš¾æœ‰æ•ˆåœ°â€œè¾…åŠ©â€é˜Ÿå‹ã€‚\n*   **[115. Character-Level Perturbations Disrupt LLM Watermarks]** **æ°´å°æ”»å‡»**ï¼šå†æ¬¡æ‰“å‡»æ°´å°æŠ€æœ¯ã€‚å‘ç°ç®€å•çš„å­—ç¬¦çº§æ‰°åŠ¨ï¼ˆå¦‚åŒå½¢å­—ã€é”™åˆ«å­—ï¼‰å°±èƒ½ç ´å LLM ç”Ÿæˆæ–‡æœ¬çš„æ°´å°ã€‚\n\nä»Šå¤©çš„æ—¥æŠ¥å°±åˆ°è¿™é‡Œï¼Œæ„Ÿè°¢é˜…è¯»ï¼çœ‹æ¥ LLM æ­£æœç€æ›´åƒäººè„‘çš„â€œå†…éƒ¨é¢„æµ‹â€ï¼ˆJEPAï¼‰å’Œâ€œæ…¢æ€è€ƒâ€ï¼ˆMCTS/RLï¼‰æ–¹å‘è¿›åŒ–ï¼ŒåŒæ—¶å®‰å…¨å’Œè¯„ä¼°æ ‡å‡†ä¹Ÿåœ¨å¿«é€Ÿè·Ÿè¿›ã€‚æˆ‘ä»¬æ˜å¤©è§ï¼",
  "papers": [
    {
      "arxiv_id": "2509.13341v2",
      "title": "Imagined Autocurricula",
      "title_zh": "æƒ³è±¡è‡ªåŠ¨è¯¾ç¨‹",
      "authors": [
        "Ahmet H. GÃ¼zel",
        "Matthew Thomas Jackson",
        "Jarek Luca Liesen",
        "Tim RocktÃ¤schel",
        "Jakob Nicolaus Foerster",
        "Ilija Bogunovic",
        "Jack Parker-Holder"
      ],
      "abstract": "Training agents to act in embodied environments typically requires vast training data or access to accurate simulation, neither of which exists for many cases in the real world. Instead, world models are emerging as an alternative leveraging offline, passively collected data, they make it possible to generate diverse worlds for training agents in simulation. In this work, we harness world models to generate imagined environments to train robust agents capable of generalizing to novel task variations. One of the challenges in doing this is ensuring the agent trains on useful generated data. We thus propose a novel approach, IMAC (Imagined Autocurricula), leveraging Unsupervised Environment Design (UED), which induces an automatic curriculum over generated worlds. In a series of challenging, procedurally generated environments, we show it is possible to achieve strong transfer performance on held-out environments, having trained only inside a world model learned from a narrower dataset. We believe this opens the path to utilizing larger-scale, foundation world models for generally capable agents.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…·èº«æ™ºèƒ½(Embodied AI)åœ¨ç°å®ä¸–ç•Œä¸­é¢ä¸´çš„æ•°æ®åŒ®ä¹å’Œæ¨¡æ‹Ÿå™¨ç¼ºå¤±é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºIMAC (Imagined Autocurricula)çš„æ–°å‹è®­ç»ƒæ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ä¸–ç•Œæ¨¡å‹(World Models)é€šè¿‡ç¦»çº¿è¢«åŠ¨æ”¶é›†çš„æ•°æ®ç”Ÿæˆå¤šæ ·åŒ–çš„æ¨¡æ‹Ÿç¯å¢ƒï¼Œå¹¶ç»“åˆæ— ç›‘ç£ç¯å¢ƒè®¾è®¡(Unsupervised Environment Design, UED)åœ¨æƒ³è±¡çš„ç¯å¢ƒä¸­è¯±å¯¼äº§ç”Ÿè‡ªåŠ¨è¯¾ç¨‹ã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆè§£å†³äº†å¦‚ä½•åœ¨ç”Ÿæˆçš„è™šæ‹Ÿæ•°æ®ä¸­ç¡®ä¿æ™ºèƒ½ä½“è¿›è¡Œé«˜æ•ˆå­¦ä¹ çš„æŒ‘æˆ˜ï¼Œæ—¨åœ¨æå‡æ™ºèƒ½ä½“å¯¹æ–°ä»»åŠ¡å˜ä½“çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå³ä¾¿ä»…åœ¨åŸºäºæœ‰é™æ•°æ®é›†å­¦ä¹ çš„ä¸–ç•Œæ¨¡å‹å†…éƒ¨è¿›è¡Œè®­ç»ƒï¼Œæ™ºèƒ½ä½“åœ¨æœªè§è¿‡çš„æµ‹è¯•ç¯å¢ƒ(held-out environments)ä¸­ä¾ç„¶å±•ç°å‡ºå¼ºå¤§çš„è¿ç§»æ€§èƒ½(transfer performance)ã€‚è¯¥ç ”ç©¶ä¸ºåˆ©ç”¨å¤§è§„æ¨¡åŸºç¡€ä¸–ç•Œæ¨¡å‹(foundation world models)åŸ¹å…»å…·å¤‡é€šç”¨èƒ½åŠ›çš„æ™ºèƒ½ä½“å¼€è¾Ÿäº†å¯è¡Œè·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.13341v2",
      "published_date": "2025-09-11 23:55:39 UTC",
      "updated_date": "2025-09-28 18:24:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:51:31.682212+00:00"
    },
    {
      "arxiv_id": "2509.09893v1",
      "title": "Self-Augmented Robot Trajectory: Efficient Imitation Learning via Safe Self-augmentation with Demonstrator-annotated Precision",
      "title_zh": "SARTï¼šåŸºäºæ¼”ç¤ºè€…æ ‡æ³¨ç²¾åº¦çš„å®‰å…¨è‡ªå¢å¼ºé«˜æ•ˆæ¨¡ä»¿å­¦ä¹ æœºå™¨äººè½¨è¿¹",
      "authors": [
        "Hanbit Oh",
        "Masaki Murooka",
        "Tomohiro Motoda",
        "Ryoichi Nakajo",
        "Yukiyasu Domae"
      ],
      "abstract": "Imitation learning is a promising paradigm for training robot agents; however, standard approaches typically require substantial data acquisition -- via numerous demonstrations or random exploration -- to ensure reliable performance. Although exploration reduces human effort, it lacks safety guarantees and often results in frequent collisions -- particularly in clearance-limited tasks (e.g., peg-in-hole) -- thereby, necessitating manual environmental resets and imposing additional human burden. This study proposes Self-Augmented Robot Trajectory (SART), a framework that enables policy learning from a single human demonstration, while safely expanding the dataset through autonomous augmentation. SART consists of two stages: (1) human teaching only once, where a single demonstration is provided and precision boundaries -- represented as spheres around key waypoints -- are annotated, followed by one environment reset; (2) robot self-augmentation, where the robot generates diverse, collision-free trajectories within these boundaries and reconnects to the original demonstration. This design improves the data collection efficiency by minimizing human effort while ensuring safety. Extensive evaluations in simulation and real-world manipulation tasks show that SART achieves substantially higher success rates than policies trained solely on human-collected demonstrations. Video results available at https://sites.google.com/view/sart-il .",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†åä¸ºSARTï¼ˆSelf-Augmented Robot Trajectoryï¼‰çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ¨¡ä»¿å­¦ä¹ (Imitation Learning)ä¸­æ•°æ®é‡‡é›†æˆæœ¬é«˜ä»¥åŠéšæœºæ¢ç´¢åœ¨ç²¾å¯†ä»»åŠ¡ä¸­ç¼ºä¹å®‰å…¨ä¿éšœçš„é—®é¢˜ã€‚è¯¥æ¡†æ¶ä¸»è¦åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼Œé¦–å…ˆç”±äººç±»è¿›è¡Œå•æ¬¡ç¤ºæ•™å¹¶æ ‡æ³¨å…³é”®è·¯å¾„ç‚¹å‘¨å›´çš„ç²¾åº¦è¾¹ç•Œ(Precision Boundaries)ï¼Œéšåæœºå™¨äººé€šè¿‡è‡ªæˆ‘å¢å¼ºåœ¨è¾¹ç•Œå†…ç”Ÿæˆå¤šæ ·åŒ–ä¸”æ— ç¢°æ’çš„è½¨è¿¹ã€‚è¿™ç§è®¾è®¡é€šè¿‡å°†è‡ªä¸»ç”Ÿæˆçš„è½¨è¿¹ä¸åŸå§‹ç¤ºæ•™é‡æ–°è¿æ¥ï¼Œåœ¨ç¡®ä¿å®‰å…¨çš„å‰æä¸‹å®ç°äº†æ•°æ®é›†çš„è‡ªä¸»æ‰©å±•ï¼Œæœ€å¤§é™åº¦åœ°é™ä½äº†äººå·¥å¹²é¢„å’Œç¯å¢ƒé‡ç½®çš„è´Ÿæ‹…ã€‚åœ¨ä»¿çœŸå’Œç°å®ä¸–ç•Œæ“çºµä»»åŠ¡ä¸­çš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒSARTçš„æˆåŠŸç‡æ˜¾è‘—é«˜äºä»…ä¾é äººç±»æ”¶é›†æ•°æ®è®­ç»ƒçš„åŸºçº¿ç­–ç•¥ã€‚è¯¥ç ”ç©¶ä¸ºå®ç°é«˜æ•ˆã€å®‰å…¨ä¸”ä½æ•°æ®ä¾èµ–çš„æœºå™¨äººç­–ç•¥å­¦ä¹ æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Under review",
      "pdf_url": "https://arxiv.org/pdf/2509.09893v1",
      "published_date": "2025-09-11 23:10:56 UTC",
      "updated_date": "2025-09-11 23:10:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:51:44.191282+00:00"
    },
    {
      "arxiv_id": "2509.10582v1",
      "title": "LearnLens: An AI-Enhanced Dashboard to Support Teachers in Open-Ended Classrooms",
      "title_zh": "LearnLensï¼šé¢å‘å¼€æ”¾å¼è¯¾å ‚æ•™å¸ˆçš„ AI å¢å¼ºå‹ä»ªè¡¨æ¿",
      "authors": [
        "Namrata Srivastava",
        "Shruti Jain",
        "Clayton Cohn",
        "Naveeduddin Mohammed",
        "Umesh Timalsina",
        "Gautam Biswas"
      ],
      "abstract": "Exploratory learning environments (ELEs), such as simulation-based platforms and open-ended science curricula, promote hands-on exploration and problem-solving but make it difficult for teachers to gain timely insights into students' conceptual understanding. This paper presents LearnLens, a generative AI (GenAI)-enhanced teacher-facing dashboard designed to support problem-based instruction in middle school science. LearnLens processes students' open-ended responses from digital assessments to provide various insights, including sample responses, word clouds, bar charts, and AI-generated summaries. These features elucidate students' thinking, enabling teachers to adjust their instruction based on emerging patterns of understanding. The dashboard was informed by teacher input during professional development sessions and implemented within a middle school Earth science curriculum. We report insights from teacher interviews that highlight the dashboard's usability and potential to guide teachers' instruction in the classroom.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†LearnLensï¼Œä¸€ä¸ªåŸºäºç”Ÿæˆå¼äººå·¥æ™ºèƒ½(GenAI)å¢å¼ºçš„æ•™å¸ˆç«¯ä»ªè¡¨ç›˜ï¼Œæ—¨åœ¨æ”¯æŒä¸­å­¦ç§‘å­¦è¯¾ç¨‹ä¸­çš„é—®é¢˜å¯¼å‘æ•™å­¦ã€‚é’ˆå¯¹æ¢ç´¢æ€§å­¦ä¹ ç¯å¢ƒ(ELEs)ä¸­æ•™å¸ˆéš¾ä»¥å®æ—¶æŒæ¡å­¦ç”Ÿæ¦‚å¿µç†è§£çš„é—®é¢˜ï¼Œè¯¥ç³»ç»Ÿé€šè¿‡æ•°å­—åŒ–è¯„ä¼°å¤„ç†å­¦ç”Ÿçš„å¼€æ”¾å¼å›ç­”ã€‚LearnLensé›†æˆäº†æ ·æœ¬å›ç­”å±•ç¤ºã€è¯äº‘ã€æ¡å½¢å›¾ä»¥åŠAIè‡ªåŠ¨æ‘˜è¦ç­‰åŠŸèƒ½ï¼Œç”¨äºæ¸…æ™°å‘ˆç°å­¦ç”Ÿçš„æ€ç»´æ¨¡å¼ã€‚æ•™å¸ˆå¯ä»¥æ ¹æ®è¿™äº›æ´å¯ŸåŠæ—¶è°ƒæ•´æ•™å­¦ç­–ç•¥ï¼Œä»è€Œæ›´å¥½åœ°æ»¡è¶³å­¦ç”Ÿåœ¨åŠ¨æ‰‹æ¢ç´¢å’Œé—®é¢˜è§£å†³è¿‡ç¨‹ä¸­çš„ä¸ªæ€§åŒ–éœ€æ±‚ã€‚è¯¥ä»ªè¡¨ç›˜å·²åœ¨ä¸­å­¦åœ°çƒç§‘å­¦è¯¾ç¨‹ä¸­å®æ–½ï¼Œåˆæ­¥è®¿è°ˆç»“æœè¯å®äº†å…¶è‰¯å¥½çš„æ˜“ç”¨æ€§ä»¥åŠåœ¨å¼•å¯¼è¯¾å ‚æ•™å­¦æ–¹é¢çš„æ˜¾è‘—æ½œåŠ›ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "9 pages",
      "pdf_url": "https://arxiv.org/pdf/2509.10582v1",
      "published_date": "2025-09-11 23:06:54 UTC",
      "updated_date": "2025-09-11 23:06:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:51:46.912002+00:00"
    },
    {
      "arxiv_id": "2509.09880v1",
      "title": "Automated Tuning for Diffusion Inverse Problem Solvers without Generative Prior Retraining",
      "title_zh": "æ— éœ€é‡æ–°è®­ç»ƒç”Ÿæˆå…ˆéªŒçš„æ‰©æ•£åé—®é¢˜æ±‚è§£å™¨è‡ªåŠ¨è°ƒä¼˜",
      "authors": [
        "YaÅŸar Utku AlÃ§alar",
        "Junno Yun",
        "Mehmet AkÃ§akaya"
      ],
      "abstract": "Diffusion/score-based models have recently emerged as powerful generative priors for solving inverse problems, including accelerated MRI reconstruction. While their flexibility allows decoupling the measurement model from the learned prior, their performance heavily depends on carefully tuned data fidelity weights, especially under fast sampling schedules with few denoising steps. Existing approaches often rely on heuristics or fixed weights, which fail to generalize across varying measurement conditions and irregular timestep schedules. In this work, we propose Zero-shot Adaptive Diffusion Sampling (ZADS), a test-time optimization method that adaptively tunes fidelity weights across arbitrary noise schedules without requiring retraining of the diffusion prior. ZADS treats the denoising process as a fixed unrolled sampler and optimizes fidelity weights in a self-supervised manner using only undersampled measurements. Experiments on the fastMRI knee dataset demonstrate that ZADS consistently outperforms both traditional compressed sensing and recent diffusion-based methods, showcasing its ability to deliver high-fidelity reconstructions across varying noise schedules and acquisition settings.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰©æ•£æ¨¡å‹(Diffusion/score-based models)åœ¨è§£å†³é€†é—®é¢˜(Inverse Problems)ï¼Œå¦‚åŠ é€ŸMRIé‡å»ºæ—¶ï¼Œå…¶æ€§èƒ½é«˜åº¦ä¾èµ–æ•°æ®ä¿çœŸåº¦æƒé‡(data fidelity weights)ä¸”éš¾ä»¥åœ¨ä¸åŒé‡‡æ ·è®¡åˆ’ä¸‹æ³›åŒ–çš„é—®é¢˜ï¼Œæå‡ºäº†Zero-shot Adaptive Diffusion Sampling (ZADS)ã€‚è¿™æ˜¯ä¸€ç§æµ‹è¯•æ—¶ä¼˜åŒ–(test-time optimization)æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¸é‡æ–°è®­ç»ƒç”Ÿæˆå…ˆéªŒ(generative prior)çš„æƒ…å†µä¸‹ï¼Œé’ˆå¯¹ä»»æ„å™ªå£°è°ƒåº¦è‡ªé€‚åº”åœ°è°ƒæ•´æƒé‡ã€‚ZADSå°†å»å™ªè¿‡ç¨‹è§†ä¸ºä¸€ä¸ªå›ºå®šçš„å±•å¼€é‡‡æ ·å™¨(unrolled sampler)ï¼Œå¹¶ä»…åˆ©ç”¨æ¬ é‡‡æ ·æµ‹é‡æ•°æ®ä»¥è‡ªç›‘ç£(self-supervised)çš„æ–¹å¼è¿›è¡Œæƒé‡ä¼˜åŒ–ã€‚åœ¨fastMRIè†ç›–æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒZADSåœ¨å„ç§å™ªå£°è°ƒåº¦å’Œé‡‡é›†è®¾ç½®ä¸‹å‡ä¸€è‡´ä¼˜äºä¼ ç»Ÿçš„å‹ç¼©æ„ŸçŸ¥(compressed sensing)ä»¥åŠæœ€æ–°çš„æ‰©æ•£æ¨¡å‹æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶æä¾›é«˜ä¿çœŸé‡å»ºçš„èƒ½åŠ›ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "physics.med-ph"
      ],
      "primary_category": "eess.IV",
      "comment": "IEEE International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP), 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.09880v1",
      "published_date": "2025-09-11 22:22:32 UTC",
      "updated_date": "2025-09-11 22:22:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:51:55.188025+00:00"
    },
    {
      "arxiv_id": "2509.09873v1",
      "title": "From Hugging Face to GitHub: Tracing License Drift in the Open-Source AI Ecosystem",
      "title_zh": "ä» Hugging Face åˆ° GitHubï¼šè¿½è¸ªå¼€æº AI ç”Ÿæ€ç³»ç»Ÿä¸­çš„è®¸å¯è¯æ¼‚ç§»",
      "authors": [
        "James Jewitt",
        "Hao Li",
        "Bram Adams",
        "Gopi Krishnan Rajbahadur",
        "Ahmed E. Hassan"
      ],
      "abstract": "Hidden license conflicts in the open-source AI ecosystem pose serious legal and ethical risks, exposing organizations to potential litigation and users to undisclosed risk. However, the field lacks a data-driven understanding of how frequently these conflicts occur, where they originate, and which communities are most affected. We present the first end-to-end audit of licenses for datasets and models on Hugging Face, as well as their downstream integration into open-source software applications, covering 364 thousand datasets, 1.6 million models, and 140 thousand GitHub projects. Our empirical analysis reveals systemic non-compliance in which 35.5% of model-to-application transitions eliminate restrictive license clauses by relicensing under permissive terms. In addition, we prototype an extensible rule engine that encodes almost 200 SPDX and model-specific clauses for detecting license conflicts, which can solve 86.4% of license conflicts in software applications. To support future research, we release our dataset and the prototype engine. Our study highlights license compliance as a critical governance challenge in open-source AI and provides both the data and tools necessary to enable automated, AI-aware compliance at scale.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹å¼€æºAIç”Ÿæ€ç³»ç»Ÿä¸­çš„è®¸å¯è¯(License)å†²çªé—®é¢˜è¿›è¡Œäº†é¦–æ¬¡ç«¯åˆ°ç«¯å®¡è®¡ï¼Œæ—¨åœ¨æ­ç¤ºä» Hugging Face æ¨¡å‹åˆ° GitHub ä¸‹æ¸¸åº”ç”¨é›†æˆè¿‡ç¨‹ä¸­å­˜åœ¨çš„æ³•å¾‹ä¸ä¼¦ç†é£é™©ã€‚ç ”ç©¶å›¢é˜Ÿå¯¹36.4ä¸‡ä¸ªæ•°æ®é›†ã€160ä¸‡ä¸ªæ¨¡å‹ä»¥åŠ14ä¸‡ä¸ª GitHub é¡¹ç›®è¿›è¡Œäº†å¤§è§„æ¨¡åˆ†æï¼Œç³»ç»Ÿæ€§åœ°è¿½è¸ªäº†è®¸å¯è¯åœ¨ç”Ÿæ€ç³»ç»Ÿä¸­çš„æ¼”å˜ã€‚å®è¯åˆ†æå‘ç°ï¼ŒAIé¢†åŸŸå­˜åœ¨ä¸¥é‡çš„éåˆè§„ç°è±¡ï¼Œå…¶ä¸­35.5%çš„æ¨¡å‹åœ¨å‘åº”ç”¨ç¨‹åºè½¬åŒ–æ—¶ï¼Œé€šè¿‡å°†å…¶é‡æ–°æˆæƒä¸ºå®½æ¾æ¡æ¬¾è€Œæ¶ˆé™¤äº†åŸæœ‰çš„é™åˆ¶æ€§è®¸å¯è¯æ¡æ¬¾(License Drift)ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œç ”ç©¶è€…å¼€å‘äº†ä¸€ä¸ªåŒ…å«è¿‘200ä¸ª SPDX å’Œæ¨¡å‹ç‰¹å®šæ¡æ¬¾çš„å¯æ‰©å±•è§„åˆ™å¼•æ“ï¼Œèƒ½å¤Ÿè‡ªåŠ¨æ£€æµ‹å¹¶è§£å†³è½¯ä»¶åº”ç”¨ä¸­86.4%çš„è®¸å¯è¯å†²çªã€‚è¯¥ç ”ç©¶ä¸ä»…å…¬å¼€äº†ç›¸å…³æ•°æ®é›†å’ŒåŸå‹å¼•æ“ï¼Œè¿˜å¼ºè°ƒäº†è®¸å¯è¯åˆè§„æ€§æ˜¯å¼€æºAIæ²»ç†çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œä¸ºå®ç°å¤§è§„æ¨¡è‡ªåŠ¨åŒ–åˆè§„æä¾›äº†å…³é”®çš„æ•°æ®æ”¯æŒä¸å·¥å…·ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "9 pages, 4 figures, 5 tables, pre-print",
      "pdf_url": "https://arxiv.org/pdf/2509.09873v1",
      "published_date": "2025-09-11 21:46:20 UTC",
      "updated_date": "2025-09-11 21:46:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:51:54.089366+00:00"
    },
    {
      "arxiv_id": "2509.09871v1",
      "title": "Emulating Public Opinion: A Proof-of-Concept of AI-Generated Synthetic Survey Responses for the Chilean Case",
      "title_zh": "æ¨¡æ‹Ÿæ°‘æ„ï¼šAI ç”Ÿæˆåˆæˆè°ƒæŸ¥å“åº”çš„æ¦‚å¿µéªŒè¯â€”â€”ä»¥ Chilean æ¡ˆä¾‹ä¸ºä¾‹",
      "authors": [
        "BastiÃ¡n GonzÃ¡lez-Bustamante",
        "Nando Verelst",
        "Carla Cisternas"
      ],
      "abstract": "Large Language Models (LLMs) offer promising avenues for methodological and applied innovations in survey research by using synthetic respondents to emulate human answers and behaviour, potentially mitigating measurement and representation errors. However, the extent to which LLMs recover aggregate item distributions remains uncertain and downstream applications risk reproducing social stereotypes and biases inherited from training data. We evaluate the reliability of LLM-generated synthetic survey responses against ground-truth human responses from a Chilean public opinion probabilistic survey. Specifically, we benchmark 128 prompt-model-question triplets, generating 189,696 synthetic profiles, and pool performance metrics (i.e., accuracy, precision, recall, and F1-score) in a meta-analysis across 128 question-subsample pairs to test for biases along key sociodemographic dimensions. The evaluation spans OpenAI's GPT family and o-series reasoning models, as well as Llama and Qwen checkpoints. Three results stand out. First, synthetic responses achieve excellent performance on trust items (F1-score and accuracy > 0.90). Second, GPT-4o, GPT-4o-mini and Llama 4 Maverick perform comparably on this task. Third, synthetic-human alignment is highest among respondents aged 45-59. Overall, LLM-based synthetic samples approximate responses from a probabilistic sample, though with substantial item-level heterogeneity. Capturing the full nuance of public opinion remains challenging and requires careful calibration and additional distributional tests to ensure algorithmic fidelity and reduce errors.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) ç”Ÿæˆåˆæˆå—è®¿è€…ä»¥æ¨¡æ‹ŸçœŸå®äººç±»è°ƒæŸ¥å“åº”çš„å¯è¡Œæ€§ï¼Œå¹¶ä»¥æ™ºåˆ©çš„æ¦‚ç‡æ€§å…¬å…±èˆ†è®ºè°ƒæŸ¥æ•°æ®ä¸ºåŸºå‡†è¿›è¡Œäº†éªŒè¯ã€‚ç ”ç©¶äººå‘˜é€šè¿‡åŸºå‡†æµ‹è¯•äº†åŒ…æ‹¬ GPT-4oã€o-seriesã€Llama å’Œ Qwen åœ¨å†…çš„å¤šä¸ªæ¨¡å‹ï¼Œæ¶µç›–äº† 128 ä¸ªâ€œæç¤º-æ¨¡å‹-é—®é¢˜â€ä¸‰å…ƒç»„å¹¶ç”Ÿæˆäº†è¿‘ 19 ä¸‡ä¸ªåˆæˆæ¡£æ¡ˆã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåˆæˆå“åº”åœ¨ä¿¡ä»»æ¡ç›® (trust items) ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒF1-score å’Œå‡†ç¡®ç‡å‡è¶…è¿‡ 0.90ï¼Œä¸” GPT-4o ä¸ Llama 4 Maverick ç­‰æ¨¡å‹çš„æ€§èƒ½è¡¨ç°ç›¸å½“ã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œåˆæˆæ ·æœ¬ä¸ 45-59 å²äººç¾¤çš„å“åº”å¯¹é½åº¦æœ€é«˜ï¼Œå°½ç®¡ LLM èƒ½å¤Ÿè¾ƒå¥½åœ°è¿‘ä¼¼æ¦‚ç‡æ ·æœ¬ï¼Œä½†åœ¨å…·ä½“æ¡ç›®ä¸Šä»å­˜åœ¨æ˜¾è‘—çš„å¼‚è´¨æ€§ (heterogeneity)ã€‚è¯¥ç ”ç©¶æ€»ç»“è®¤ä¸ºï¼Œè™½ç„¶ AI æ¨¡æ‹Ÿæ°‘æ„å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†è¦å‡†ç¡®æ•æ‰å¤æ‚çš„èˆ†è®ºç»†èŠ‚ä»éœ€é€šè¿‡ä¸¥æ ¼çš„æ ¡å‡†å’Œåˆ†å¸ƒæµ‹è¯•æ¥ä¿è¯ç®—æ³•å¿ å®åº¦ (algorithmic fidelity) å¹¶å‡å°‘ç³»ç»Ÿæ€§è¯¯å·®ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Working paper: 18 pages, 4 tables, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.09871v1",
      "published_date": "2025-09-11 21:43:59 UTC",
      "updated_date": "2025-09-11 21:43:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:52:08.789514+00:00"
    },
    {
      "arxiv_id": "2509.09870v1",
      "title": "Vibe Check: Understanding the Effects of LLM-Based Conversational Agents' Personality and Alignment on User Perceptions in Goal-Oriented Tasks",
      "title_zh": "Vibe Checkï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å¯¹è¯æ™ºèƒ½ä½“äººæ ¼è¡¨è¾¾åŠå…¶ä¸€è‡´æ€§å¯¹ç›®æ ‡å¯¼å‘ä»»åŠ¡ä¸­ç”¨æˆ·æ„ŸçŸ¥çš„å½±å“ç ”ç©¶",
      "authors": [
        "Hasibur Rahman",
        "Smit Desai"
      ],
      "abstract": "Large language models (LLMs) enable conversational agents (CAs) to express distinctive personalities, raising new questions about how such designs shape user perceptions. This study investigates how personality expression levels and user-agent personality alignment influence perceptions in goal-oriented tasks. In a between-subjects experiment (N=150), participants completed travel planning with CAs exhibiting low, medium, or high expression across the Big Five traits, controlled via our novel Trait Modulation Keys framework. Results revealed an inverted-U relationship: medium expression produced the most positive evaluations across Intelligence, Enjoyment, Anthropomorphism, Intention to Adopt, Trust, and Likeability, significantly outperforming both extremes. Personality alignment further enhanced outcomes, with Extraversion and Emotional Stability emerging as the most influential traits. Cluster analysis identified three distinct compatibility profiles, with \"Well-Aligned\" users reporting substantially positive perceptions. These findings demonstrate that personality expression and strategic trait alignment constitute optimal design targets for CA personality, offering design implications as LLM-based CAs become increasingly prevalent.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)é©±åŠ¨çš„å¯¹è¯æ™ºèƒ½ä½“(Conversational Agents)çš„æ€§æ ¼è¡¨è¾¾æ°´å¹³åŠå…¶ä¸ç”¨æˆ·çš„æ€§æ ¼å¯¹é½(Alignment)å¦‚ä½•å½±å“ç”¨æˆ·åœ¨ä»»åŠ¡å¯¼å‘å‹ä»»åŠ¡ä¸­çš„æ„ŸçŸ¥ã€‚ç ”ç©¶è€…é€šè¿‡ä¸€é¡¹é’ˆå¯¹150åå‚ä¸è€…çš„å—è¯•è€…é—´å®éªŒï¼Œåˆ©ç”¨æ–°é¢–çš„Trait Modulation Keysæ¡†æ¶ï¼Œæ§åˆ¶æ™ºèƒ½ä½“åœ¨äº”å¤§æ€§æ ¼ç‰¹è´¨(Big Five)ä¸Šçš„è¡¨ç°ç¨‹åº¦å¹¶å®Œæˆæ—…è¡Œè§„åˆ’ä»»åŠ¡ã€‚å®éªŒç»“æœæ­ç¤ºäº†æ€§æ ¼è¡¨è¾¾ç¨‹åº¦ä¸ç”¨æˆ·è¯„ä¼°ä¹‹é—´å­˜åœ¨å€’Uå‹å…³ç³»ï¼Œä¸­ç­‰ç¨‹åº¦çš„æ€§æ ¼è¡¨è¾¾åœ¨æ™ºèƒ½æ„Ÿ(Intelligence)ã€æ„‰æ‚¦æ„Ÿ(Enjoyment)ã€æ‹ŸäººåŒ–(Anthropomorphism)ä»¥åŠä¿¡ä»»åº¦ç­‰ç»´åº¦ä¸Šæ˜¾è‘—ä¼˜äºæç«¯è¡¨è¾¾æ°´å¹³ã€‚æ€§æ ¼å¯¹é½è¿›ä¸€æ­¥å¢å¼ºäº†äº¤äº’æ•ˆæœï¼Œå…¶ä¸­å¤–å‘æ€§(Extraversion)å’Œæƒ…ç»ªç¨³å®šæ€§(Emotional Stability)è¢«è¯æ˜æ˜¯æœ€å…·å½±å“åŠ›çš„æ€§æ ¼ç‰¹è´¨ã€‚é€šè¿‡èšç±»åˆ†æï¼Œç ”ç©¶è¯†åˆ«å‡ºâ€œé«˜åº¦å¯¹é½â€çš„ç”¨æˆ·ç¾¤ä½“å…·æœ‰æœ€ç§¯æçš„æ„ŸçŸ¥åé¦ˆï¼Œè¡¨æ˜ä¸­ç­‰å¼ºåº¦çš„è¡¨è¾¾ä¸æˆ˜ç•¥æ€§çš„ç‰¹è´¨å¯¹é½æ˜¯ä¼˜åŒ–å¯¹è¯æ™ºèƒ½ä½“è®¾è®¡çš„æ ¸å¿ƒç›®æ ‡ã€‚è¯¥ç ”ç©¶ä¸ºæ—¥ç›Šæ™®åŠçš„LLMé©±åŠ¨å¯¹è¯ç³»ç»Ÿæä¾›äº†é‡è¦çš„è®¾è®¡å¯ç¤ºä¸å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.09870v1",
      "published_date": "2025-09-11 21:43:49 UTC",
      "updated_date": "2025-09-11 21:43:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:52:09.157384+00:00"
    },
    {
      "arxiv_id": "2509.09869v1",
      "title": "Surrogate Supervision for Robust and Generalizable Deformable Image Registration",
      "title_zh": "é¢å‘é²æ£’ä¸”é«˜æ³›åŒ–æ€§å¯å˜å½¢å›¾åƒé…å‡†çš„ä»£ç†ç›‘ç£",
      "authors": [
        "Yihao Liu",
        "Junyu Chen",
        "Lianrui Zuo",
        "Shuwen Wei",
        "Brian D. Boyd",
        "Carmen Andreescu",
        "Olusola Ajilore",
        "Warren D. Taylor",
        "Aaron Carass",
        "Bennett A. Landman"
      ],
      "abstract": "Objective: Deep learning-based deformable image registration has achieved strong accuracy, but remains sensitive to variations in input image characteristics such as artifacts, field-of-view mismatch, or modality difference. We aim to develop a general training paradigm that improves the robustness and generalizability of registration networks. Methods: We introduce surrogate supervision, which decouples the input domain from the supervision domain by applying estimated spatial transformations to surrogate images. This allows training on heterogeneous inputs while ensuring supervision is computed in domains where similarity is well defined. We evaluate the framework through three representative applications: artifact-robust brain MR registration, mask-agnostic lung CT registration, and multi-modal MR registration. Results: Across tasks, surrogate supervision demonstrated strong resilience to input variations including inhomogeneity field, inconsistent field-of-view, and modality differences, while maintaining high performance on well-curated data. Conclusions: Surrogate supervision provides a principled framework for training robust and generalizable deep learning-based registration models without increasing complexity. Significance: Surrogate supervision offers a practical pathway to more robust and generalizable medical image registration, enabling broader applicability in diverse biomedical imaging scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºæ·±åº¦å­¦ä¹ çš„å¯å˜å½¢å›¾åƒé…å‡†ï¼ˆDeformable Image Registrationï¼‰å¯¹ä¼ªå½±ã€è§†é‡ä¸åŒ¹é…åŠæ¨¡æ€å·®å¼‚é«˜åº¦æ•æ„Ÿçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºsurrogate supervisionï¼ˆä»£ç†ç›‘ç£ï¼‰çš„é€šç”¨è®­ç»ƒèŒƒå¼ï¼Œæ—¨åœ¨æå‡é…å‡†ç½‘ç»œçš„é²æ£’æ€§ä¸æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºé€šè¿‡å°†ä¼°è®¡çš„ç©ºé—´å˜æ¢åº”ç”¨äºä»£ç†å›¾åƒï¼Œå®ç°äº†è¾“å…¥åŸŸä¸ç›‘ç£åŸŸçš„æœ‰æ•ˆè§£è€¦ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨å¼‚è´¨è¾“å…¥ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒåŒæ—¶ç¡®ä¿åœ¨ç›¸ä¼¼æ€§å®šä¹‰æ˜ç¡®çš„é¢†åŸŸå†…è®¡ç®—ç›‘ç£ä¿¡å·ã€‚é€šè¿‡å¯¹è„‘éƒ¨MRã€è‚ºéƒ¨CTåŠå¤šæ¨¡æ€MRé…å‡†ç­‰ä¸‰ä¸ªä»£è¡¨æ€§åº”ç”¨åœºæ™¯çš„è¯„ä¼°ï¼Œsurrogate supervisionåœ¨é¢å¯¹ä¸å‡åŒ€åœºï¼ˆinhomogeneity fieldï¼‰ã€è§†é‡ä¸ä¸€è‡´åŠæ¨¡æ€å·®å¼‚æ—¶å±•ç°å‡ºæå¼ºçš„éŸ§æ€§ï¼Œä¸”ä¸ä¼šå¢åŠ æ¨¡å‹å¤æ‚åº¦ã€‚ç ”ç©¶ç»“æœè¯æ˜äº†è¯¥æ¡†æ¶åœ¨å¤„ç†å¤æ‚åŒ»å­¦å½±åƒæ•°æ®æ—¶çš„æœ‰æ•ˆæ€§ï¼Œä¸ºåœ¨å¤šæ ·åŒ–çš„ç”Ÿç‰©åŒ»å­¦æˆåƒåœºæ™¯ä¸­å®ç°æ›´ç¨³å¥çš„å›¾åƒé…å‡†æä¾›äº†å®ç”¨ä¸”å…·æœ‰åŸåˆ™æ€§çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.09869v1",
      "published_date": "2025-09-11 21:43:45 UTC",
      "updated_date": "2025-09-11 21:43:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:52:09.768093+00:00"
    },
    {
      "arxiv_id": "2509.09867v1",
      "title": "LLMs as Agentic Cooperative Players in Multiplayer UNO",
      "title_zh": "LLMs ä½œä¸ºå¤šäºº UNO æ¸¸æˆä¸­çš„ä»£ç†å¼åˆä½œç©å®¶",
      "authors": [
        "Yago Romano Matinez",
        "Jesse Roberts"
      ],
      "abstract": "LLMs promise to assist humans -- not just by answering questions, but by offering useful guidance across a wide range of tasks. But how far does that assistance go? Can a large language model based agent actually help someone accomplish their goal as an active participant? We test this question by engaging an LLM in UNO, a turn-based card game, asking it not to win but instead help another player to do so. We built a tool that allows decoder-only LLMs to participate as agents within the RLCard game environment. These models receive full game-state information and respond using simple text prompts under two distinct prompting strategies. We evaluate models ranging from small (1B parameters) to large (70B parameters) and explore how model scale impacts performance. We find that while all models were able to successfully outperform a random baseline when playing UNO, few were able to significantly aid another player.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)ä½œä¸ºä»£ç†åœ¨å¤šäººUNOæ¸¸æˆä¸­ååŠ©ä»–äººè¾¾æˆç›®æ ‡çš„èƒ½åŠ›ï¼Œæ—¨åœ¨éªŒè¯LLMsåœ¨ä¸»åŠ¨å‚ä¸ä»»åŠ¡æ—¶çš„åä½œæ½œåŠ›ã€‚ç ”ç©¶äººå‘˜åŸºäºRLCardæ¸¸æˆç¯å¢ƒå¼€å‘äº†ä¸€ä¸ªå·¥å…·ï¼Œä½¿ä»…è§£ç (decoder-only)çš„LLMsèƒ½å¤Ÿä½œä¸ºä»£ç†è·å–å®Œæ•´æ¸¸æˆçŠ¶æ€ï¼Œå¹¶æ ¹æ®ä¸¤ç§ä¸åŒçš„æç¤ºç­–ç•¥(prompting strategies)åšå‡ºå“åº”ã€‚å®éªŒè¯„ä¼°äº†ä»10äº¿(1B)åˆ°700äº¿(70B)å‚æ•°è§„æ¨¡ä¸ç­‰çš„å¤šç§æ¨¡å‹ï¼Œæ—¨åœ¨æ¢ç©¶æ¨¡å‹è§„æ¨¡(model scale)å¯¹åä½œæ€§èƒ½çš„å½±å“ã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶æ‰€æœ‰æ¨¡å‹åœ¨å¸¸è§„æ¸¸æˆé€»è¾‘ä¸Šå‡èƒ½ä¼˜äºéšæœºåŸºå‡†(random baseline)ï¼Œä½†åœ¨åä½œåšå¼ˆè€…(cooperative players)çš„è®¾å®šä¸‹ï¼Œå¾ˆå°‘æœ‰æ¨¡å‹èƒ½æ˜¾è‘—æœ‰æ•ˆåœ°è¾…åŠ©å…¶ä»–ç©å®¶è·èƒœã€‚è¿™è¡¨æ˜LLMsåœ¨éœ€è¦æ·±åº¦åä½œå’Œä¸»åŠ¨è¾…åŠ©ä»–äººå®Œæˆç‰¹å®šç›®æ ‡çš„å¤æ‚äº¤äº’ä»»åŠ¡ä¸­ä»è¡¨ç°æœ‰é™ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.09867v1",
      "published_date": "2025-09-11 21:42:33 UTC",
      "updated_date": "2025-09-11 21:42:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:52:09.554185+00:00"
    },
    {
      "arxiv_id": "2509.09864v1",
      "title": "Latency and Token-Aware Test-Time Compute",
      "title_zh": "æ—¶å»¶ä¸ Token æ„ŸçŸ¥çš„æµ‹è¯•æ—¶è®¡ç®—",
      "authors": [
        "Jenny Y. Huang",
        "Mehul Damani",
        "Yousef El-Kurdi",
        "Ramon Astudillo",
        "Wei Sun"
      ],
      "abstract": "Inference-time scaling has emerged as a powerful way to improve large language model (LLM) performance by generating multiple candidate responses and selecting among them. However, existing work on dynamic allocation for test-time compute typically considers only parallel generation methods such as best-of-N, overlooking incremental decoding methods like beam search, and has largely ignored latency, focusing only on token usage. We formulate inference-time scaling as a problem of dynamic compute allocation and method selection, where the system must decide which strategy to apply and how much compute to allocate on a per-query basis. Our framework explicitly incorporates both token cost and wall-clock latency, the latter being critical for user experience and particularly for agentic workflows where models must issue multiple queries efficiently. Experiments on reasoning benchmarks show that our approach consistently outperforms static strategies, achieving favorable accuracy-cost trade-offs while remaining practical for deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)çš„æ¨ç†æ—¶æ‰©å±•(Inference-time scaling)é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªå…³æ³¨å»¶è¿Ÿä¸Tokenæ„ŸçŸ¥çš„åŠ¨æ€è®¡ç®—åˆ†é…æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰ç ”ç©¶ä»…è€ƒè™‘Best-of-Nç­‰å¹¶è¡Œç”Ÿæˆæ–¹æ³•ä¸”å¿½ç•¥å®é™…å»¶è¿Ÿ(Wall-clock latency)çš„å±€é™æ€§ï¼Œè¯¥å·¥ä½œå°†æ¨ç†æ—¶æ‰©å±•å½¢å¼åŒ–ä¸ºåŠ¨æ€è®¡ç®—åˆ†é…ä¸æ–¹æ³•é€‰æ‹©é—®é¢˜ã€‚ç³»ç»Ÿèƒ½å¤Ÿæ ¹æ®æ¯ä¸ªæŸ¥è¯¢çš„éœ€æ±‚ï¼Œåœ¨å¢é‡è§£ç æ–¹æ³•å¦‚Beam Searchä¸å¹¶è¡Œæ–¹æ³•ä¹‹é—´åŠ¨æ€å†³ç­–å¹¶åˆ†é…è®¡ç®—èµ„æºã€‚è¯¥æ¡†æ¶æ˜¾å¼åœ°å°†Tokenæˆæœ¬å’Œå¯¹ç”¨æˆ·ä½“éªŒåŠAgentic Workflowsè‡³å…³é‡è¦çš„å£é’Ÿå»¶è¿Ÿçº³å…¥ä¼˜åŒ–ç›®æ ‡ã€‚åœ¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‡†ç¡®ç‡ä¸æˆæœ¬çš„æƒè¡¡ä¸ŠæŒç»­ä¼˜äºé™æ€ç­–ç•¥ï¼Œè¯æ˜äº†å…¶åœ¨å®é™…éƒ¨ç½²ä¸­çš„å®ç”¨æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.09864v1",
      "published_date": "2025-09-11 21:35:19 UTC",
      "updated_date": "2025-09-11 21:35:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:52:12.565469+00:00"
    },
    {
      "arxiv_id": "2509.12247v2",
      "title": "Modular, On-Site Solutions with Lightweight Anomaly Detection for Sustainable Nutrient Management in Agriculture",
      "title_zh": "é¢å‘å†œä¸šå¯æŒç»­å…»åˆ†ç®¡ç†çš„æ¨¡å—åŒ–ã€åŸä½è½»é‡çº§å¼‚å¸¸æ£€æµ‹è§£å†³æ–¹æ¡ˆ",
      "authors": [
        "Abigail R. Cohen",
        "Yuming Sun",
        "Zhihao Qin",
        "Harsh S. Muriki",
        "Zihao Xiao",
        "Yeonju Lee",
        "Matthew Housley",
        "Andrew F. Sharkey",
        "Rhuanito S. Ferrarezi",
        "Jing Li",
        "Lu Gan",
        "Yongsheng Chen"
      ],
      "abstract": "Efficient nutrient management is critical for crop growth and sustainable resource consumption (e.g., nitrogen, energy). Current approaches require lengthy analyses, preventing real-time optimization; similarly, imaging facilitates rapid phenotyping but can be computationally intensive, preventing deployment under resource constraints. This study proposes a flexible, tiered pipeline for anomaly detection and status estimation (fresh weight, dry mass, and tissue nutrients), including a comprehensive energy analysis of approaches that span the efficiency-accuracy spectrum. Using a nutrient depletion experiment with three treatments (T1-100%, T2-50%, and T3-25% fertilizer strength) and multispectral imaging (MSI), we developed a hierarchical pipeline using an autoencoder (AE) for early warning. Further, we compared two status estimation modules of different complexity for more detailed analysis: vegetation index (VI) features with machine learning (Random Forest, RF) and raw whole-image deep learning (Vision Transformer, ViT). Results demonstrated high-efficiency anomaly detection (73% net detection of T3 samples 9 days after transplanting) at substantially lower energy than embodied energy in wasted nitrogen. The state estimation modules show trade-offs, with ViT outperforming RF on phosphorus and calcium estimation (R2 0.61 vs. 0.58, 0.48 vs. 0.35) at higher energy cost. With our modular pipeline, this work opens opportunities for edge diagnostics and practical opportunities for agricultural sustainability.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å†œä¸šè¥å…»ç®¡ç†ä¸­å®æ—¶ä¼˜åŒ–å›°éš¾ä»¥åŠå›¾åƒå¤„ç†è®¡ç®—èµ„æºå¯†é›†çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§çµæ´»çš„åˆ†å±‚æµæ°´çº¿ï¼Œç”¨äºè¥å…»å¼‚å¸¸æ£€æµ‹ä¸ä½œç‰©çŠ¶æ€ä¼°è®¡ã€‚è¯¥æ–¹æ¡ˆç»“åˆå¤šå…‰è°±æˆåƒ(MSI)æŠ€æœ¯ï¼Œåˆ©ç”¨Autoencoder(AE)æ„å»ºäº†è½»é‡åŒ–çš„æ—©æœŸé¢„è­¦æ¨¡å—ï¼Œå¹¶å¯¹æ¯”äº†åŸºäºVegetation Index(VI)çš„Random Forest(RF)ä¸åŸºäºåŸå§‹å›¾åƒçš„Vision Transformer(ViT)åœ¨é²œé‡ã€å¹²é‡åŠå…»åˆ†å«é‡é¢„æµ‹ä¸Šçš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAEæ¨¡å—èƒ½åœ¨ä½œç‰©ç§»æ¤9å¤©åä»¥æä½èƒ½è€—å®ç°73%çš„å¼‚å¸¸æ£€æµ‹ç‡ã€‚åœ¨å…·ä½“å…»åˆ†ä¼°è®¡ä¸­ï¼ŒViTåœ¨ç£·(phosphorus)å’Œé’™(calcium)çš„é¢„æµ‹ç²¾åº¦ä¸Šä¼˜äºRFæ¨¡å‹ï¼Œå°½ç®¡å…¶è®¡ç®—èƒ½è€—ç›¸å¯¹è¾ƒé«˜ã€‚è¿™ç§æ¨¡å—åŒ–æµæ°´çº¿ä¸ºè¾¹ç¼˜è¯Šæ–­(edge diagnostics)æä¾›äº†å…¼é¡¾æ•ˆç‡ä¸ç²¾åº¦çš„è§£å†³æ–¹æ¡ˆï¼Œä¸ºå®ç°å†œä¸šèµ„æºçš„å¯æŒç»­åˆ©ç”¨æä¾›äº†é‡è¦æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.12247v2",
      "published_date": "2025-09-11 21:14:35 UTC",
      "updated_date": "2025-11-26 15:30:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:52:33.074984+00:00"
    },
    {
      "arxiv_id": "2509.09853v2",
      "title": "SWE-Effi: Re-Evaluating Software AI Agent System Effectiveness Under Resource Constraints",
      "title_zh": "SWE-Effiï¼šèµ„æºçº¦æŸä¸‹è½¯ä»¶ AI æ™ºèƒ½ä½“ç³»ç»Ÿæ•ˆèƒ½çš„é‡æ–°è¯„ä¼°",
      "authors": [
        "Zhiyu Fan",
        "Kirill Vasilevski",
        "Dayi Lin",
        "Boyuan Chen",
        "Yihao Chen",
        "Zhiqing Zhong",
        "Jie M. Zhang",
        "Pinjia He",
        "Ahmed E. Hassan"
      ],
      "abstract": "The advancement of large language models (LLMs) and code agents has demonstrated significant potential to assist software engineering (SWE) tasks, such as autonomous issue resolution and feature addition. Existing AI for software engineering leaderboards (e.g., SWE-bench) focus solely on solution accuracy, ignoring the crucial factor of effectiveness in a resource-constrained world. This is a universal problem that also exists beyond software engineering tasks: any AI system should be more than correct - it must also be cost-effective. To address this gap, we introduce SWE-Effi, a set of new metrics to re-evaluate AI systems in terms of holistic effectiveness scores. We define effectiveness as the balance between the accuracy of outcome (e.g., issue resolve rate) and the resources consumed (e.g., token and time). In this paper, we specifically focus on the software engineering scenario by re-ranking popular AI systems for issue resolution on a subset of the SWE-bench benchmark using our new multi-dimensional metrics. We found that AI system's effectiveness depends not just on the scaffold itself, but on how well it integrates with the base model, which is key to achieving strong performance in a resource-efficient manner. We also identified systematic challenges such as the \"token snowball\" effect and, more significantly, a pattern of \"expensive failures\". In these cases, agents consume excessive resources while stuck on unsolvable tasks - an issue that not only limits practical deployment but also drives up the cost of failed rollouts during RL training. Lastly, we observed a clear trade-off between effectiveness under the token budget and effectiveness under the time budget, which plays a crucial role in managing project budgets and enabling scalable reinforcement learning, where fast responses are essential.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºç›®å‰çš„è½¯ä»¶å·¥ç¨‹(Software Engineering) AIæ™ºèƒ½ä½“è¯„ä¼°ä½“ç³»ï¼ˆå¦‚ SWE-benchï¼‰å¤§å¤šä»…å…³æ³¨è§£å†³é—®é¢˜çš„å‡†ç¡®ç‡ï¼Œè€Œå¿½è§†äº†èµ„æºå—é™ç¯å¢ƒä¸‹çš„æˆæœ¬æ•ˆç›Šã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† SWE-Effi è¯„ä»·æŒ‡æ ‡ï¼Œæ—¨åœ¨é€šè¿‡æƒè¡¡ä»»åŠ¡è§£å†³ç‡ä¸æ‰€æ¶ˆè€—çš„ token å’Œæ—¶é—´ï¼Œå¤šç»´åº¦åœ°è¡¡é‡ AI ç³»ç»Ÿçš„æ•´ä½“æœ‰æ•ˆæ€§ã€‚é€šè¿‡å¯¹ç°æœ‰ä¸»æµç³»ç»Ÿè¿›è¡Œé‡æ–°æ’åï¼Œç ”ç©¶å‘ç°ç³»ç»Ÿçš„æœ‰æ•ˆæ€§å…³é”®åœ¨äºè„šæ‰‹æ¶(scaffold)ä¸åŸºç¡€æ¨¡å‹(base model)çš„æ·±åº¦æ•´åˆï¼Œè€Œéå•ä¸€ç»„ä»¶çš„ä¼˜åŠ£ã€‚è®ºæ–‡è¿›ä¸€æ­¥æ­ç¤ºäº†â€œtoken é›ªçƒæ•ˆåº”(token snowball)â€ä¸â€œæ˜‚è´µå¤±è´¥(expensive failures)â€ç­‰ç³»ç»Ÿæ€§æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºæ™ºèƒ½ä½“åœ¨ä¸å¯è§£ä»»åŠ¡ä¸Šçš„è¿‡åº¦èµ„æºæ¶ˆè€—æ˜¯é˜»ç¢å…¶å®é™…éƒ¨ç½²å’Œå¼ºåŒ–å­¦ä¹ (RL)è®­ç»ƒçš„ä¸»è¦ç“¶é¢ˆã€‚æœ€åï¼Œç ”ç©¶æ­ç¤ºäº† token é¢„ç®—ä¸æ—¶é—´é¢„ç®—ä¹‹é—´çš„æƒè¡¡å…³ç³»(trade-off)ï¼Œä¸ºé¡¹ç›®é¢„ç®—ç®¡ç†å’Œå¯æ‰©å±•çš„ AI ç ”å‘æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.09853v2",
      "published_date": "2025-09-11 21:04:10 UTC",
      "updated_date": "2025-09-18 19:31:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:52:34.366786+00:00"
    },
    {
      "arxiv_id": "2509.09848v1",
      "title": "Towards an AI-based knowledge assistant for goat farmers based on Retrieval-Augmented Generation",
      "title_zh": "é¢å‘å±±ç¾Šå…»æ®–æˆ·çš„åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆçš„äººå·¥æ™ºèƒ½çŸ¥è¯†åŠ©æ‰‹",
      "authors": [
        "Nana Han",
        "Dong Liu",
        "Tomas Norton"
      ],
      "abstract": "Large language models (LLMs) are increasingly being recognised as valuable knowledge communication tools in many industries. However, their application in livestock farming remains limited, being constrained by several factors not least the availability, diversity and complexity of knowledge sources. This study introduces an intelligent knowledge assistant system designed to support health management in farmed goats. Leveraging the Retrieval-Augmented Generation (RAG), two structured knowledge processing methods, table textualization and decision-tree textualization, were proposed to enhance large language models' (LLMs) understanding of heterogeneous data formats. Based on these methods, a domain-specific goat farming knowledge base was established to improve LLM's capacity for cross-scenario generalization. The knowledge base spans five key domains: Disease Prevention and Treatment, Nutrition Management, Rearing Management, Goat Milk Management, and Basic Farming Knowledge. Additionally, an online search module is integrated to enable real-time retrieval of up-to-date information. To evaluate system performance, six ablation experiments were conducted to examine the contribution of each component. The results demonstrated that heterogeneous knowledge fusion method achieved the best results, with mean accuracies of 87.90% on the validation set and 84.22% on the test set. Across the text-based, table-based, decision-tree based Q&A tasks, accuracy consistently exceeded 85%, validating the effectiveness of structured knowledge fusion within a modular design. Error analysis identified omission as the predominant error category, highlighting opportunities to further improve retrieval coverage and context integration. In conclusion, the results highlight the robustness and reliability of the proposed system for practical applications in goat farming.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†ä¸€æ¬¾åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆ (Retrieval-Augmented Generation, RAG) çš„æ™ºèƒ½çŸ¥è¯†åŠ©æ‰‹ï¼Œæ—¨åœ¨è§£å†³å±±ç¾Šå…»æ®–é¢†åŸŸçŸ¥è¯†æ¥æºå¤æ‚ã€å¤šæ ·ä¸”åº”ç”¨å—é™çš„é—®é¢˜ã€‚ä¸ºäº†æå‡å¤§è¯­è¨€æ¨¡å‹ (Large Language Models, LLMs) å¯¹å¼‚æ„æ•°æ®æ ¼å¼çš„ç†è§£èƒ½åŠ›ï¼Œç ”ç©¶è€…æå‡ºäº†è¡¨æ ¼æ–‡æœ¬åŒ– (table textualization) å’Œå†³ç­–æ ‘æ–‡æœ¬åŒ– (decision-tree textualization) ä¸¤ç§ç»“æ„åŒ–çŸ¥è¯†å¤„ç†æ–¹æ³•ã€‚é€šè¿‡æ„å»ºæ¶µç›–ç–¾ç—…é˜²æ²»ã€è¥å…»ç®¡ç†ã€å…»æ®–ç®¡ç†ç­‰äº”ä¸ªå…³é”®é¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†åº“å¹¶é›†æˆå®æ—¶åœ¨çº¿æœç´¢æ¨¡å—ï¼Œç³»ç»Ÿå®ç°äº†æ˜¾è‘—çš„è·¨åœºæ™¯æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¼‚æ„çŸ¥è¯†èåˆ (heterogeneous knowledge fusion) æ–¹æ³•åœ¨éªŒè¯é›†å’Œæµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«è¾¾åˆ° 87.90% å’Œ 84.22%ã€‚åœ¨é’ˆå¯¹æ–‡æœ¬ã€è¡¨æ ¼å’Œå†³ç­–æ ‘çš„é—®ç­”ä»»åŠ¡ä¸­ï¼Œç³»ç»Ÿå‡†ç¡®ç‡å‡ç¨³å®šåœ¨ 85% ä»¥ä¸Šï¼Œå……åˆ†éªŒè¯äº†æ¨¡å—åŒ–è®¾è®¡ä¸­ç»“æ„åŒ–çŸ¥è¯†èåˆçš„æœ‰æ•ˆæ€§ã€‚å°½ç®¡é”™è¯¯åˆ†ææŒ‡å‡ºä¿¡æ¯é—æ¼æ˜¯ä¸»è¦æ”¹è¿›æ–¹å‘ï¼Œä½†è¯¥ç³»ç»Ÿä»ä¸ºå±±ç¾Šå…»æ®–çš„å®é™…åº”ç”¨æä¾›äº†é²æ£’ä¸”å¯é çš„ AI å†³ç­–æ”¯æŒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.09848v1",
      "published_date": "2025-09-11 20:58:51 UTC",
      "updated_date": "2025-09-11 20:58:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:52:35.766882+00:00"
    },
    {
      "arxiv_id": "2509.09843v1",
      "title": "HGEN: Heterogeneous Graph Ensemble Networks",
      "title_zh": "HGENï¼šå¼‚æ„å›¾é›†æˆç½‘ç»œ",
      "authors": [
        "Jiajun Shen",
        "Yufei Jin",
        "Yi He",
        "Xingquan Zhu"
      ],
      "abstract": "This paper presents HGEN that pioneers ensemble learning for heterogeneous graphs. We argue that the heterogeneity in node types, nodal features, and local neighborhood topology poses significant challenges for ensemble learning, particularly in accommodating diverse graph learners. Our HGEN framework ensembles multiple learners through a meta-path and transformation-based optimization pipeline to uplift classification accuracy. Specifically, HGEN uses meta-path combined with random dropping to create Allele Graph Neural Networks (GNNs), whereby the base graph learners are trained and aligned for later ensembling. To ensure effective ensemble learning, HGEN presents two key components: 1) a residual-attention mechanism to calibrate allele GNNs of different meta-paths, thereby enforcing node embeddings to focus on more informative graphs to improve base learner accuracy, and 2) a correlation-regularization term to enlarge the disparity among embedding matrices generated from different meta-paths, thereby enriching base learner diversity. We analyze the convergence of HGEN and attest its higher regularization magnitude over simple voting. Experiments on five heterogeneous networks validate that HGEN consistently outperforms its state-of-the-art competitors by substantial margin.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Heterogeneous Graph Ensemble Networks (HGEN)ï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹å¼‚è´¨å›¾è®¾è®¡çš„é›†æˆå­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¼‚è´¨å›¾ä¸­èŠ‚ç‚¹ç±»å‹ã€ç‰¹å¾å’Œå±€éƒ¨æ‹“æ‰‘å·®å¼‚ç»™é›†æˆå­¦ä¹ å¸¦æ¥çš„æŒ‘æˆ˜ã€‚HGEN é€šè¿‡åŸºäº meta-path å’Œè½¬æ¢çš„ä¼˜åŒ–æµæ°´çº¿é›†æˆå¤šä¸ªå­¦ä¹ å™¨ï¼Œä»è€Œæ˜¾è‘—æå‡åˆ†ç±»å‡†ç¡®ç‡ã€‚å…·ä½“è€Œè¨€ï¼Œè¯¥æ¡†æ¶ç»“åˆ meta-path å’Œéšæœºä¸¢å¼ƒæŠ€æœ¯æ„å»ºäº† Allele Graph Neural Networks (GNNs)ï¼Œç”¨äºåŸºç¡€å­¦ä¹ å™¨çš„è®­ç»ƒä¸å¯¹é½ã€‚ä¸ºäº†ç¡®ä¿é›†æˆçš„æœ‰æ•ˆæ€§ï¼ŒHGEN å¼•å…¥äº† residual-attention æœºåˆ¶æ¥æ ¡å‡†ä¸åŒ meta-path çš„æ¨¡å‹è¾“å‡ºï¼Œä½¿èŠ‚ç‚¹åµŒå…¥èƒ½èšç„¦äºæ›´å…·ä¿¡æ¯é‡çš„å›¾ç»“æ„ï¼ŒåŒæ—¶é€šè¿‡ correlation-regularization é¡¹å¢å¤§åµŒå…¥çŸ©é˜µé—´çš„å·®å¼‚ä»¥å¢å¼ºå­¦ä¹ å™¨çš„å¤šæ ·æ€§ã€‚ç†è®ºåˆ†æè¡¨æ˜ HGEN æ¯”ç®€å•çš„æŠ•ç¥¨æœºåˆ¶å…·æœ‰æ›´é«˜çš„æ­£åˆ™åŒ–å¼ºåº¦ï¼Œä¸”åœ¨äº”ä¸ªå¼‚è´¨ç½‘ç»œæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜å…¶æ€§èƒ½å¤§å¹…é¢†å…ˆäºç°æœ‰çš„ state-of-the-art æ¨¡å‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "The paper is in proceedings of the 34th IJCAI Conference, 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.09843v1",
      "published_date": "2025-09-11 20:50:00 UTC",
      "updated_date": "2025-09-11 20:50:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:52:34.165341+00:00"
    },
    {
      "arxiv_id": "2509.09838v1",
      "title": "Revisiting Actor-Critic Methods in Discrete Action Off-Policy Reinforcement Learning",
      "title_zh": "é‡æ–°å®¡è§†ç¦»æ•£åŠ¨ä½œç¦»çº¿ç­–ç•¥å¼ºåŒ–å­¦ä¹ ä¸­çš„ Actor-Critic æ–¹æ³•",
      "authors": [
        "Reza Asad",
        "Reza Babanezhad",
        "Sharan Vaswani"
      ],
      "abstract": "Value-based approaches such as DQN are the default methods for off-policy reinforcement learning with discrete-action environments such as Atari. Common policy-based methods are either on-policy and do not effectively learn from off-policy data (e.g. PPO), or have poor empirical performance in the discrete-action setting (e.g. SAC). Consequently, starting from discrete SAC (DSAC), we revisit the design of actor-critic methods in this setting. First, we determine that the coupling between the actor and critic entropy is the primary reason behind the poor performance of DSAC. We demonstrate that by merely decoupling these components, DSAC can have comparable performance as DQN. Motivated by this insight, we introduce a flexible off-policy actor-critic framework that subsumes DSAC as a special case. Our framework allows using an m-step Bellman operator for the critic update, and enables combining standard policy optimization methods with entropy regularization to instantiate the resulting actor objective. Theoretically, we prove that the proposed methods can guarantee convergence to the optimal regularized value function in the tabular setting. Empirically, we demonstrate that these methods can approach the performance of DQN on standard Atari games, and do so even without entropy regularization or explicit exploration.",
      "tldr_zh": "è¯¥ç ”ç©¶é‡æ–°å®¡è§†äº†ç¦»æ•£åŠ¨ä½œ Off-Policy å¼ºåŒ–å­¦ä¹ ä¸­çš„ Actor-Critic æ–¹æ³•ï¼Œé‡ç‚¹æ¢è®¨äº†ç¦»æ•£ç‰ˆæœ¬ Soft Actor-Critic (DSAC) è¡¨ç°ä¸ä½³çš„åŸå› ã€‚ç ”ç©¶å‘ç°ï¼ŒActor ä¸ Critic ç†µä¹‹é—´çš„è€¦åˆæ˜¯é™åˆ¶ DSAC æ€§èƒ½çš„å…³é”®å› ç´ ï¼Œé€šè¿‡è§£è€¦è¿™ä¸¤ä¸ªç»„ä»¶å¯ä»¥ä½¿å…¶è¾¾åˆ°ä¸ DQN ç›¸å½“çš„æ°´å¹³ã€‚åŸºäºæ­¤å‘ç°ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªçµæ´»çš„ Off-Policy Actor-Critic æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ”¯æŒåœ¨ Critic æ›´æ–°ä¸­ä½¿ç”¨ m-step Bellman ç®—å­ï¼Œå¹¶å°†æ ‡å‡†ç­–ç•¥ä¼˜åŒ–ä¸ç†µæ­£åˆ™åŒ–ç›¸ç»“åˆã€‚ç†è®ºè¯æ˜è¯¥æ–¹æ³•åœ¨è¡¨æ ¼ (Tabular) ç¯å¢ƒä¸‹èƒ½å¤Ÿæ”¶æ•›è‡³æœ€ä¼˜æ­£åˆ™åŒ–ä»·å€¼å‡½æ•°ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ Atari æ¸¸æˆä¸Šçš„æ€§èƒ½æ¥è¿‘ DQNï¼Œç”šè‡³åœ¨ä¸ä½¿ç”¨ç†µæ­£åˆ™åŒ–æˆ–æ˜¾å¼æ¢ç´¢çš„æƒ…å†µä¸‹ä¹Ÿèƒ½å–å¾—è‰¯å¥½æ•ˆæœã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.09838v1",
      "published_date": "2025-09-11 20:34:08 UTC",
      "updated_date": "2025-09-11 20:34:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:52:46.295694+00:00"
    },
    {
      "arxiv_id": "2509.09836v1",
      "title": "CoDiCodec: Unifying Continuous and Discrete Compressed Representations of Audio",
      "title_zh": "CoDiCodecï¼šç»Ÿä¸€éŸ³é¢‘çš„è¿ç»­ä¸ç¦»æ•£å‹ç¼©è¡¨å¾",
      "authors": [
        "Marco Pasini",
        "Stefan Lattner",
        "George Fazekas"
      ],
      "abstract": "Efficiently representing audio signals in a compressed latent space is critical for latent generative modelling. However, existing autoencoders often force a choice between continuous embeddings and discrete tokens. Furthermore, achieving high compression ratios while maintaining audio fidelity remains a challenge. We introduce CoDiCodec, a novel audio autoencoder that overcomes these limitations by both efficiently encoding global features via summary embeddings, and by producing both compressed continuous embeddings at ~ 11 Hz and discrete tokens at a rate of 2.38 kbps from the same trained model, offering unprecedented flexibility for different downstream generative tasks. This is achieved through Finite Scalar Quantization (FSQ) and a novel FSQ-dropout technique, and does not require additional loss terms beyond the single consistency loss used for end-to-end training. CoDiCodec supports both autoregressive decoding and a novel parallel decoding strategy, with the latter achieving superior audio quality and faster decoding. CoDiCodec outperforms existing continuous and discrete autoencoders at similar bitrates in terms of reconstruction audio quality. Our work enables a unified approach to audio compression, bridging the gap between continuous and discrete generative modelling paradigms.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CoDiCodecï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„éŸ³é¢‘è‡ªåŠ¨ç¼–ç å™¨ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åœ¨éŸ³é¢‘å‹ç¼©ä¸­å¿…é¡»åœ¨è¿ç»­åµŒå…¥(continuous embeddings)å’Œç¦»æ•£ä»¤ç‰Œ(discrete tokens)ä¹‹é—´åšå‡ºæƒè¡¡çš„å±€é™æ€§ã€‚CoDiCodecé€šè¿‡æ‘˜è¦åµŒå…¥(summary embeddings)é«˜æ•ˆç¼–ç å…¨å±€ç‰¹å¾ï¼Œå¹¶èƒ½åœ¨åŒä¸€æ¨¡å‹ä¸­åŒæ—¶ç”Ÿæˆçº¦11 Hzçš„å‹ç¼©è¿ç»­åµŒå…¥å’Œ2.38 kbpsé€Ÿç‡çš„ç¦»æ•£ä»¤ç‰Œï¼Œä¸ºä¸åŒçš„ä¸‹æ¸¸ç”Ÿæˆä»»åŠ¡æä¾›äº†å‰æ‰€æœªæœ‰çš„çµæ´»æ€§ã€‚è¿™ä¸€æˆæœçš„å®ç°æ ¸å¿ƒåœ¨äºé‡‡ç”¨äº†æœ‰é™æ ‡é‡é‡åŒ–(Finite Scalar Quantization, FSQ)å’Œåˆ›æ–°çš„FSQ-dropoutæŠ€æœ¯ï¼Œä¸”è®­ç»ƒè¿‡ç¨‹ä»…éœ€å•ä¸€çš„ä¸€è‡´æ€§æŸå¤±(consistency loss)ã€‚è¯¥æ¡†æ¶åŒæ—¶æ”¯æŒè‡ªå›å½’è§£ç (autoregressive decoding)å’Œä¸€ç§æ–°å‹çš„å¹¶è¡Œè§£ç (parallel decoding)ç­–ç•¥ï¼Œåè€…åœ¨éŸ³é¢‘è´¨é‡å’Œè§£ç é€Ÿåº¦ä¸Šå‡è¡¨ç°æ›´ä½³ã€‚å®éªŒè¯æ˜ï¼ŒCoDiCodecåœ¨ç›¸ä¼¼æ¯”ç‰¹ç‡ä¸‹çš„éŸ³é¢‘é‡æ„è´¨é‡ä¼˜äºç°æœ‰çš„è¿ç»­å’Œç¦»æ•£è‡ªåŠ¨ç¼–ç å™¨ï¼ŒæˆåŠŸå¼¥åˆäº†è¿ç»­ä¸ç¦»æ•£ç”Ÿæˆå¼å»ºæ¨¡èŒƒå¼ä¹‹é—´çš„é¸¿æ²Ÿã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted to ISMIR 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.09836v1",
      "published_date": "2025-09-11 20:31:18 UTC",
      "updated_date": "2025-09-11 20:31:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:53:01.126721+00:00"
    },
    {
      "arxiv_id": "2509.09823v1",
      "title": "SoilSound: Smartphone-based Soil Moisture Estimation",
      "title_zh": "SoilSoundï¼šåŸºäºæ™ºèƒ½æ‰‹æœºçš„åœŸå£¤æ°´åˆ†ä¼°æµ‹",
      "authors": [
        "Yixuan Gao",
        "Tanvir Ahmed",
        "Shuang He",
        "Zhongqi Cheng",
        "Rajalakshmi Nandakumar"
      ],
      "abstract": "Soil moisture monitoring is essential for agriculture and environmental management, yet existing methods require either invasive probes disturbing the soil or specialized equipment, limiting access to the public. We present SoilSound, an ubiquitous accessible smartphone-based acoustic sensing system that can measure soil moisture without disturbing the soil. We leverage the built-in speaker and microphone to perform a vertical scan mechanism to accurately measure moisture without any calibration. Unlike existing work that use transmissive properties, we propose an alternate model for acoustic reflections in soil based on the surface roughness effect to enable moisture sensing without disturbing the soil. The system works by sending acoustic chirps towards the soil and recording the reflections during a vertical scan, which are then processed and fed to a convolutional neural network for on-device soil moisture estimation with negligible computational, memory, or power overhead. We evaluated the system by training with curated soils in boxes in the lab and testing in the outdoor fields and show that SoilSound achieves a mean absolute error (MAE) of 2.39% across 10 different locations. Overall, the evaluation shows that SoilSound can accurately track soil moisture levels ranging from 15.9% to 34.0% across multiple soil types, environments, and users; without requiring any calibration or disturbing the soil, enabling widespread moisture monitoring for home gardeners, urban farmers, citizen scientists, and agricultural communities in resource-limited settings.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SoilSoundï¼Œä¸€ç§åˆ©ç”¨æ™ºèƒ½æ‰‹æœºå†…ç½®æ‰¬å£°å™¨å’Œéº¦å…‹é£å®ç°æ— æŸåœŸå£¤æ°´åˆ†ä¼°ç®—çš„æ™®é€‚å£°å­¦ä¼ æ„Ÿç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿé€šè¿‡å‚ç›´æ‰«ææœºåˆ¶æ•æ‰å£°å­¦åå°„ï¼Œå¹¶åŸºäºè¡¨é¢ç²—ç³™åº¦æ•ˆåº”(surface roughness effect)å»ºç«‹äº†å£°å­¦åå°„æ¨¡å‹ï¼Œä»è€Œåœ¨ä¸ç ´ååœŸå£¤ä¸”æ— éœ€æ ¡å‡†çš„æƒ…å†µä¸‹è¿›è¡Œç²¾ç¡®æµ‹é‡ã€‚é€šè¿‡å‘é€å£°å­¦è„‰å†²(acoustic chirps)å¹¶ç»“åˆå·ç§¯ç¥ç»ç½‘ç»œ(CNN)ï¼ŒSoilSoundèƒ½å¤Ÿåœ¨ç§»åŠ¨ç«¯ä»¥æä½çš„è®¡ç®—ã€å†…å­˜å’ŒåŠŸè€—å¼€é”€å®æ—¶ä¼°ç®—æ°´åˆ†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨å¤šç§åœŸå£¤ç±»å‹å’Œé‡å¤–ç¯å¢ƒä¸‹å‡è¡¨ç°ç¨³å¥ï¼Œåœ¨10ä¸ªä¸åŒæµ‹è¯•åœ°ç‚¹çš„å¹³å‡ç»å¯¹è¯¯å·®(MAE)ä»…ä¸º2.39%ï¼Œæœ‰æ•ˆæµ‹é‡èŒƒå›´è¦†ç›–15.9%è‡³34.0%ã€‚SoilSoundä¸ºèµ„æºåŒ®ä¹åœ°åŒºçš„å†œä¸šç¤¾åŒºã€åŸå¸‚å†œæ°‘åŠå®¶åº­å›­è‰ºè€…æä¾›äº†ä¸€ç§ä½æˆæœ¬ã€æ˜“äºæ™®åŠä¸”éä¾µå…¥å¼çš„åœŸå£¤æ°´åˆ†ç›‘æµ‹æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.ET",
        "cs.HC",
        "eess.SP"
      ],
      "primary_category": "cs.SD",
      "comment": "12 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.09823v1",
      "published_date": "2025-09-11 19:49:30 UTC",
      "updated_date": "2025-09-11 19:49:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:52:58.690509+00:00"
    },
    {
      "arxiv_id": "2509.09810v3",
      "title": "Towards a Common Framework for Autoformalization",
      "title_zh": "è¿ˆå‘è‡ªåŠ¨å½¢å¼åŒ–çš„é€šç”¨æ¡†æ¶",
      "authors": [
        "Agnieszka Mensfelt",
        "David Tena Cucala",
        "Santiago Franco",
        "Angeliki Koutsoukou-Argyraki",
        "Vince Trencsenyi",
        "Kostas Stathis"
      ],
      "abstract": "Autoformalization has emerged as a term referring to the automation of formalization - specifically, the formalization of mathematics using interactive theorem provers (proof assistants). Its rapid development has been driven by progress in deep learning, especially large language models (LLMs). More recently, the term has expanded beyond mathematics to describe the broader task of translating informal input into formal logical representations. At the same time, a growing body of research explores using LLMs to translate informal language into formal representations for reasoning, planning, and knowledge representation - often without explicitly referring to this process as autoformalization. As a result, despite addressing similar tasks, the largely independent development of these research areas has limited opportunities for shared methodologies, benchmarks, and theoretical frameworks that could accelerate progress. The goal of this paper is to review - explicit or implicit - instances of what can be considered autoformalization and to propose a unified framework, encouraging cross-pollination between different fields to advance the development of next generation AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è‡ªåŠ¨å½¢å¼åŒ–(Autoformalization)è¿™ä¸€é¢†åŸŸï¼Œå³åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)å’Œäº¤äº’å¼å®šç†è¯æ˜å™¨(interactive theorem provers)å°†éæ­£å¼è¾“å…¥è½¬åŒ–ä¸ºæ­£å¼é€»è¾‘è¡¨ç¤ºçš„è¿‡ç¨‹ã€‚éšç€æŠ€æœ¯çš„å‘å±•ï¼Œè¯¥ä»»åŠ¡å·²ä»æ•°å­¦é¢†åŸŸæ‰©å±•åˆ°æ¨ç†ã€è§„åˆ’å’ŒçŸ¥è¯†è¡¨ç¤ºç­‰æ›´å¹¿æ³›çš„èŒƒç•´ï¼Œä½†ä¸åŒé¢†åŸŸçš„ç‹¬ç«‹å‘å±•é™åˆ¶äº†é€šç”¨æ–¹æ³•è®ºã€åŸºå‡†æµ‹è¯•(benchmarks)å’Œç†è®ºæ¡†æ¶çš„å…±äº«ã€‚æœ¬æ–‡ç³»ç»Ÿå›é¡¾äº†æ˜¾æ€§æˆ–éšæ€§çš„è‡ªåŠ¨å½¢å¼åŒ–(Autoformalization)å®ä¾‹ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„é€šç”¨æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ—¨åœ¨ä¿ƒè¿›ä¸åŒç ”ç©¶é¢†åŸŸä¹‹é—´çš„äº¤å‰èåˆ(cross-pollination)ï¼Œé€šè¿‡æ•´åˆç°æœ‰ç ”ç©¶æ¥åŠ é€Ÿä¸‹ä¸€ä»£äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å¼€å‘è¿›ç¨‹ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Presented at NeLaMKRR@KR, 2025 (arXiv:2511.09575). A shorter version of this work will appear in the Proceedings of the AAAI Conference on Artificial Intelligence (AAAI 2026)",
      "pdf_url": "https://arxiv.org/pdf/2509.09810v3",
      "published_date": "2025-09-11 19:28:56 UTC",
      "updated_date": "2025-12-15 16:43:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:53:04.284983+00:00"
    },
    {
      "arxiv_id": "2509.09801v1",
      "title": "HEFT: A Coarse-to-Fine Hierarchy for Enhancing the Efficiency and Accuracy of Language Model Reasoning",
      "title_zh": "HEFTï¼šä¸€ç§æå‡è¯­è¨€æ¨¡å‹æ¨ç†æ•ˆç‡ä¸å‡†ç¡®æ€§çš„ç”±ç²—åˆ°ç²¾å±‚çº§æ¶æ„",
      "authors": [
        "Brennen Hill"
      ],
      "abstract": "The adaptation of large language models (LLMs) to specialized reasoning tasks is fundamentally constrained by computational resources. Parameter-Efficient Fine-Tuning (PEFT) methods have emerged as a powerful solution, yet the landscape of these techniques is diverse, with distinct methods operating in either the model's weight space or its representation space. This paper investigates the hypothesis that a synergistic combination of these paradigms can unlock superior performance and efficiency. We introduce HEFT (Hierarchical Efficient Fine-Tuning), a novel hierarchical adaptation strategy that composes two distinct PEFT methods in a coarse-to-fine manner: first, a broad, foundational adaptation in the weight space using Low-Rank Adaptation (LoRA), followed by a precise, surgical refinement of internal activations using Representation Fine-Tuning (ReFT). We evaluate this approach by fine-tuning a Llama-2-7B model on the BoolQ benchmark, a challenging dataset for inferential reasoning. Our results reveal a profound synergistic effect. A model fine-tuned for only three epochs with our HEFT strategy achieves an accuracy of 85.17\\%, exceeding the performance of models trained for 20 epochs with either LoRA-only (85.05\\%) or ReFT-only (83.36\\%) methodologies. This work demonstrates that the thoughtful composition of PEFT methods is a potent algorithmic innovation, offering a more efficient and effective path toward advancing the reasoning capabilities of language models. By achieving superior results with a fraction of the computational budget, our findings present a principled approach to overcoming the obstacles inherent in adapting large-scale models for complex cognitive tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†HEFT (Hierarchical Efficient Fine-Tuning)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹(LLMs)æ¨ç†æ•ˆç‡ä¸å‡†ç¡®æ€§çš„å±‚çº§åŒ–é€‚é…ç­–ç•¥ã€‚é’ˆå¯¹ä¸“é—¨æ¨ç†ä»»åŠ¡å—é™äºè®¡ç®—èµ„æºçš„é—®é¢˜ï¼ŒHEFTé‡‡ç”¨ä»ç²—åˆ°ç»†(Coarse-to-Fine)çš„æ–¹æ³•ï¼ŒååŒç»„åˆäº†æƒé‡ç©ºé—´çš„LoRA (Low-Rank Adaptation)å’Œè¡¨ç¤ºç©ºé—´çš„ReFT (Representation Fine-Tuning)ä¸¤ç§å‚æ•°é«˜æ•ˆå¾®è°ƒ(PEFT)æŠ€æœ¯ã€‚ç ”ç©¶äººå‘˜åœ¨BoolQæ¨ç†åŸºå‡†æµ‹è¯•ä¸­å¯¹Llama-2-7Bæ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒHEFTä»…éœ€3ä¸ªè®­ç»ƒå‘¨æœŸ(epochs)å³å¯è¾¾åˆ°85.17%çš„å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†è®­ç»ƒ20ä¸ªå‘¨æœŸçš„LoRA-onlyæˆ–ReFT-onlyæ¨¡å‹ã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†ä¸åŒå¾®è°ƒèŒƒå¼é—´çš„æ·±å±‚ååŒæ•ˆåº”ï¼Œè¯æ˜äº†PEFTæ–¹æ³•çš„å·§å¦™ç»„åˆæ˜¯ä¸€ç§å¼ºæœ‰åŠ›çš„ç®—æ³•åˆ›æ–°ã€‚é€šè¿‡åœ¨æå°è®¡ç®—é¢„ç®—ä¸‹å®ç°ä¼˜å¼‚æ€§èƒ½ï¼Œè¯¥å·¥ä½œä¸ºåœ¨èµ„æºå—é™ç¯å¢ƒä¸‹å¢å¼ºè¯­è¨€æ¨¡å‹çš„å¤æ‚è®¤çŸ¥èƒ½åŠ›æä¾›äº†åŸåˆ™æ€§çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.09801v1",
      "published_date": "2025-09-11 19:06:46 UTC",
      "updated_date": "2025-09-11 19:06:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:53:05.883529+00:00"
    },
    {
      "arxiv_id": "2509.09794v1",
      "title": "A Modular and Multimodal Generative AI Framework for Urban Building Energy Data: Generating Synthetic Homes",
      "title_zh": "é¢å‘åŸå¸‚å»ºç­‘èƒ½æºæ•°æ®çš„æ¨¡å—åŒ–å¤šæ¨¡æ€ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ¡†æ¶ï¼šåˆæˆä½å®…ç”Ÿæˆ",
      "authors": [
        "Jackson Eshbaugh",
        "Chetan Tiwari",
        "Jorge Silveyra"
      ],
      "abstract": "Computational models have emerged as powerful tools for energy modeling research, touting scalability and quantitative results. However, these models require a plethora of data, some of which is inaccessible, expensive, or raises privacy concerns. We introduce a modular multimodal framework to produce this data from publicly accessible residential information and images using generative artificial intelligence (AI). Additionally, we provide a pipeline demonstrating this framework, and we evaluate its generative AI components. Our experiments show that our framework's use of AI avoids common issues with generative models. Our framework produces realistic, labeled data. By reducing dependence on costly or restricted data sources, we pave a path towards more accessible and reproducible research.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ¨¡å—åŒ–ä¸”å¤šæ¨¡æ€çš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½(Generative AI)æ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨å…¬å¼€çš„ä½å®…ä¿¡æ¯å’Œå›¾åƒä¸ºåŸå¸‚å»ºç­‘èƒ½æºå»ºæ¨¡ç”Ÿæˆåˆæˆæˆ¿å±‹(Synthetic Homes)æ•°æ®ã€‚é’ˆå¯¹ä¼ ç»Ÿèƒ½æºå»ºæ¨¡ä¸­æ•°æ®è·å–éš¾ã€æˆæœ¬é«˜ä»¥åŠéšç§æ•æ„Ÿç­‰æŒ‘æˆ˜ï¼Œè¯¥æ¡†æ¶é€šè¿‡å¤šæ¨¡æ€æŠ€æœ¯å®ç°äº†å¯¹å»ºç­‘èƒ½æºæ•°æ®çš„æœ‰æ•ˆæ¨¡æ‹Ÿã€‚ç ”ç©¶å›¢é˜ŸåŒæ­¥æä¾›äº†ä¸€å¥—åº”ç”¨æµæ°´çº¿(Pipeline)å¹¶å¯¹å…¶ä¸­çš„ç”Ÿæˆç»„ä»¶è¿›è¡Œäº†ä¸¥æ ¼è¯„ä¼°ï¼Œç»“æœè¯æ˜è¯¥æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆé¿å…ç”Ÿæˆæ¨¡å‹å¸¸è§çš„ç¼ºé™·ï¼Œäº§å‡ºé«˜è´¨é‡ä¸”å¸¦æœ‰æ ‡ç­¾çš„çœŸå®æ•°æ®ã€‚è¿™ä¸€æˆæœæ˜¾è‘—é™ä½äº†ç ”ç©¶å¯¹å—é™æˆ–é«˜ä»·æ•°æ®æºçš„ä¾èµ–ï¼Œä¸ºæ¨åŠ¨èƒ½æºç ”ç©¶çš„å¯è®¿é—®æ€§å’Œå¯é‡å¤æ€§æä¾›äº†æ–°çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "44 pages; 2 appendices; 9 figures; 1 table. Code available at https://github.com/Lafayette-EshbaughSilveyra-Group/synthetic-homes",
      "pdf_url": "https://arxiv.org/pdf/2509.09794v1",
      "published_date": "2025-09-11 18:53:21 UTC",
      "updated_date": "2025-09-11 18:53:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:53:02.585848+00:00"
    },
    {
      "arxiv_id": "2509.09790v1",
      "title": "How well can LLMs provide planning feedback in grounded environments?",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹åœ¨å…·èº«ç¯å¢ƒä¸­çš„è§„åˆ’åé¦ˆèƒ½åŠ›è¡¨ç°è¯„ä¼°",
      "authors": [
        "Yuxuan Li",
        "Victor Zhong"
      ],
      "abstract": "Learning to plan in grounded environments typically requires carefully designed reward functions or high-quality annotated demonstrations. Recent works show that pretrained foundation models, such as large language models (LLMs) and vision language models (VLMs), capture background knowledge helpful for planning, which reduces the amount of reward design and demonstrations needed for policy learning. We evaluate how well LLMs and VLMs provide feedback across symbolic, language, and continuous control environments. We consider prominent types of feedback for planning including binary feedback, preference feedback, action advising, goal advising, and delta action feedback. We also consider inference methods that impact feedback performance, including in-context learning, chain-of-thought, and access to environment dynamics. We find that foundation models can provide diverse high-quality feedback across domains. Moreover, larger and reasoning models consistently provide more accurate feedback, exhibit less bias, and benefit more from enhanced inference methods. Finally, feedback quality degrades for environments with complex dynamics or continuous state spaces and action spaces.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)å’Œè§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨å…·èº«ç¯å¢ƒ(grounded environments)ä¸­æä¾›è§„åˆ’åé¦ˆçš„èƒ½åŠ›ã€‚å®éªŒè¦†ç›–äº†ç¬¦å·åŒ–ã€è¯­è¨€åŒ–å’Œè¿ç»­æ§åˆ¶ç­‰ä¸åŒç±»å‹çš„ç¯å¢ƒï¼Œæ·±å…¥æ¢è®¨äº†åŒ…æ‹¬äºŒå…ƒåé¦ˆ(binary feedback)ã€åå¥½åé¦ˆ(preference feedback)ã€åŠ¨ä½œå»ºè®®(action advising)ã€ç›®æ ‡å»ºè®®(goal advising)ä»¥åŠå¢é‡åŠ¨ä½œåé¦ˆ(delta action feedback)åœ¨å†…çš„å¤šç§åé¦ˆå½¢å¼ã€‚ç ”ç©¶å›¢é˜Ÿè¿˜åˆ†æäº†ä¸Šä¸‹æ–‡å­¦ä¹ (In-context learning)ã€é“¾å¼æ€ç»´(Chain-of-thought)å’Œç¯å¢ƒåŠ¨åŠ›å­¦è®¿é—®ç­‰æ¨ç†æ–¹æ³•å¯¹åé¦ˆæ€§èƒ½çš„æ˜¾è‘—å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºç¡€æ¨¡å‹èƒ½å¤Ÿè·¨é¢†åŸŸæä¾›é«˜è´¨é‡ä¸”å¤šæ ·åŒ–çš„è§„åˆ’åé¦ˆï¼Œå…¶ä¸­è§„æ¨¡æ›´å¤§åŠå…·å¤‡å¼ºæ¨ç†èƒ½åŠ›çš„æ¨¡å‹åœ¨å‡†ç¡®æ€§å’Œå‡å°‘åå·®æ–¹é¢è¡¨ç°æ›´ä¸ºä¼˜å¼‚ã€‚æ­¤å¤–ï¼Œè¿™äº›å…ˆè¿›æ¨¡å‹èƒ½æ›´æœ‰æ•ˆåœ°åˆ©ç”¨å¢å¼ºæ¨ç†æ–¹æ³•æ¥æå‡åé¦ˆè´¨é‡ã€‚ç„¶è€Œï¼Œç ”ç©¶ä¹ŸæŒ‡å‡ºåœ¨é¢å¯¹å…·æœ‰å¤æ‚åŠ¨åŠ›å­¦æˆ–è¿ç»­çŠ¶æ€ä¸åŠ¨ä½œç©ºé—´çš„ç¯å¢ƒæ—¶ï¼Œæ¨¡å‹çš„åé¦ˆæ•ˆèƒ½ä¼šå‡ºç°æ˜æ˜¾ä¸‹é™ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.09790v1",
      "published_date": "2025-09-11 18:51:26 UTC",
      "updated_date": "2025-09-11 18:51:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:53:24.991597+00:00"
    },
    {
      "arxiv_id": "2509.09787v1",
      "title": "ZORRO: Zero-Knowledge Robustness and Privacy for Split Learning (Full Version)",
      "title_zh": "ZORROï¼šé¢å‘æ‹†åˆ†å­¦ä¹ çš„é›¶çŸ¥è¯†é²æ£’æ€§ä¸éšç§ä¿æŠ¤ï¼ˆå®Œæ•´ç‰ˆï¼‰",
      "authors": [
        "Nojan Sheybani",
        "Alessandro Pegoraro",
        "Jonathan Knauer",
        "Phillip Rieger",
        "Elissa Mollakuqe",
        "Farinaz Koushanfar",
        "Ahmad-Reza Sadeghi"
      ],
      "abstract": "Split Learning (SL) is a distributed learning approach that enables resource-constrained clients to collaboratively train deep neural networks (DNNs) by offloading most layers to a central server while keeping in- and output layers on the client-side. This setup enables SL to leverage server computation capacities without sharing data, making it highly effective in resource-constrained environments dealing with sensitive data. However, the distributed nature enables malicious clients to manipulate the training process. By sending poisoned intermediate gradients, they can inject backdoors into the shared DNN. Existing defenses are limited by often focusing on server-side protection and introducing additional overhead for the server. A significant challenge for client-side defenses is enforcing malicious clients to correctly execute the defense algorithm.\n  We present ZORRO, a private, verifiable, and robust SL defense scheme. Through our novel design and application of interactive zero-knowledge proofs (ZKPs), clients prove their correct execution of a client-located defense algorithm, resulting in proofs of computational integrity attesting to the benign nature of locally trained DNN portions. Leveraging the frequency representation of model partitions enables ZORRO to conduct an in-depth inspection of the locally trained models in an untrusted environment, ensuring that each client forwards a benign checkpoint to its succeeding client. In our extensive evaluation, covering different model architectures as well as various attack strategies and data scenarios, we show ZORRO's effectiveness, as it reduces the attack success rate to less than 6\\% while causing even for models storing \\numprint{1000000} parameters on the client-side an overhead of less than 10 seconds.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‹†åˆ†å­¦ä¹ (Split Learning, SL)ä¸­æ¶æ„å®¢æˆ·ç«¯å¯èƒ½é€šè¿‡å‘é€ä¸­æ¯’æ¢¯åº¦æ³¨å…¥åé—¨æ”»å‡»çš„å®‰å…¨å¨èƒï¼Œæå‡ºäº†åä¸ºZORROçš„éšç§ã€å¯éªŒè¯ä¸”é²æ£’çš„é˜²å¾¡æ–¹æ¡ˆã€‚ZORROé€šè¿‡åˆ›æ–°æ€§åœ°åº”ç”¨äº¤äº’å¼é›¶çŸ¥è¯†è¯æ˜(Zero-Knowledge Proofs, ZKPs)ï¼Œè¦æ±‚å®¢æˆ·ç«¯è¯æ˜å…¶æ­£ç¡®æ‰§è¡Œäº†é˜²å¾¡ç®—æ³•ï¼Œä»è€Œä¸ºæœ¬åœ°è®­ç»ƒçš„DNNéƒ¨åˆ†æä¾›è®¡ç®—å®Œæ•´æ€§(Computational Integrity)è¯æ˜ã€‚è¯¥æ–¹æ¡ˆåˆ©ç”¨æ¨¡å‹åˆ†åŒºçš„é¢‘ç‡è¡¨ç¤º(Frequency Representation)åœ¨ä¸å¯ä¿¡ç¯å¢ƒä¸­å¯¹æ¨¡å‹è¿›è¡Œæ·±åº¦æ£€æŸ¥ï¼Œç¡®ä¿æ¯ä¸ªå®¢æˆ·ç«¯å‘åç»­èŠ‚ç‚¹ä¼ é€’çš„éƒ½æ˜¯è‰¯æ€§æ£€æŸ¥ç‚¹ã€‚å®éªŒè¯„ä¼°æ¶µç›–äº†å¤šç§æ¨¡å‹æ¶æ„å’Œæ”»å‡»ç­–ç•¥ï¼Œç»“æœæ˜¾ç¤ºZORROèƒ½å°†æ”»å‡»æˆåŠŸç‡é™ä½è‡³6%ä»¥ä¸‹ã€‚åœ¨æ€§èƒ½æ–¹é¢ï¼Œå³ä½¿å®¢æˆ·ç«¯æ¨¡å‹åŒ…å«100ä¸‡ä¸ªå‚æ•°ï¼Œè¯¥æ–¹æ¡ˆå¸¦æ¥çš„é¢å¤–å¼€é”€ä¹Ÿæ§åˆ¶åœ¨10ç§’ä»¥å†…ï¼Œå…¼é¡¾äº†å®‰å…¨æ€§å’Œå®ç”¨æ€§ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Full version of CCS 2025 paper",
      "pdf_url": "https://arxiv.org/pdf/2509.09787v1",
      "published_date": "2025-09-11 18:44:09 UTC",
      "updated_date": "2025-09-11 18:44:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:53:33.996005+00:00"
    },
    {
      "arxiv_id": "2509.09775v2",
      "title": "Executable Ontologies: Synthesizing Event Semantics with Dataflow Architecture",
      "title_zh": "å¯æ‰§è¡Œæœ¬ä½“ï¼šèåˆäº‹ä»¶è¯­ä¹‰ä¸æ•°æ®æµæ¶æ„",
      "authors": [
        "Aleksandr Boldachev"
      ],
      "abstract": "This paper presents boldsea, Boldachev's semantic-event approach -- an architecture for modeling complex dynamic systems using executable ontologies -- semantic models that act as dynamic structures, directly controlling process execution. We demonstrate that integrating event semantics with a dataflow architecture addresses the limitations of traditional Business Process Management (BPM) systems and object-oriented semantic technologies. The paper presents the formal BSL (boldsea Semantic Language), including its BNF grammar, and outlines the boldsea-engine's architecture, which directly interprets semantic models as executable algorithms without compilation. It enables the modification of event models at runtime, ensures temporal transparency, and seamlessly merges data and business logic within a unified semantic framework.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†åä¸º boldsea çš„è¯­ä¹‰äº‹ä»¶æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨ executable ontologiesï¼ˆå¯æ‰§è¡Œæœ¬ä½“ï¼‰å»ºæ¨¡å¤æ‚åŠ¨æ€ç³»ç»Ÿçš„æ¶æ„ã€‚åœ¨æ­¤æ¶æ„ä¸­ï¼Œè¯­ä¹‰æ¨¡å‹ä½œä¸ºåŠ¨æ€ç»“æ„ç›´æ¥æ§åˆ¶è¿‡ç¨‹æ‰§è¡Œï¼Œé€šè¿‡å°† event semanticsï¼ˆäº‹ä»¶è¯­ä¹‰ï¼‰ä¸ dataflow architectureï¼ˆæ•°æ®æµæ¶æ„ï¼‰ç›¸ç»“åˆï¼Œæœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿ Business Process Management (BPM) å’Œé¢å‘å¯¹è±¡è¯­ä¹‰æŠ€æœ¯çš„å±€é™æ€§ã€‚è®ºæ–‡è¯¦ç»†é˜è¿°äº†æ­£å¼çš„ BSL (boldsea Semantic Language) åŠå…¶ BNF grammarï¼Œå¹¶ä»‹ç»äº†èƒ½å¤Ÿç›´æ¥å°†è¯­ä¹‰æ¨¡å‹è§£é‡Šä¸ºå¯æ‰§è¡Œç®—æ³•è€Œæ— éœ€ç¼–è¯‘çš„ boldsea-engineã€‚è¯¥ç³»ç»Ÿæ”¯æŒåœ¨è¿è¡Œæ—¶åŠ¨æ€ä¿®æ”¹äº‹ä»¶æ¨¡å‹ï¼Œå¹¶ç¡®ä¿äº† temporal transparencyï¼ˆæ—¶é—´é€æ˜æ€§ï¼‰ã€‚è¯¥ç ”ç©¶æœ€ç»ˆåœ¨ç»Ÿä¸€çš„è¯­ä¹‰æ¡†æ¶ä¸‹å®ç°äº†æ•°æ®ä¸ä¸šåŠ¡é€»è¾‘çš„æ— ç¼èåˆï¼Œä¸ºå¤æ‚åŠ¨æ€ç³»ç»Ÿæä¾›äº†ä¸€ç§é«˜æ•ˆçš„å»ºæ¨¡ä¸æ‰§è¡Œæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.FL",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "22 pages, 6 figures. Corrected captions on Figure 4",
      "pdf_url": "https://arxiv.org/pdf/2509.09775v2",
      "published_date": "2025-09-11 18:12:46 UTC",
      "updated_date": "2025-09-16 09:05:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:53:37.084561+00:00"
    },
    {
      "arxiv_id": "2509.10577v2",
      "title": "The Coding Limits of Robust Watermarking for Generative Models",
      "title_zh": "ç”Ÿæˆæ¨¡å‹é²æ£’æ°´å°çš„ç¼–ç æé™",
      "authors": [
        "Danilo Francati",
        "Yevin Nikhel Goonatilake",
        "Shubham Pawar",
        "Daniele Venturi",
        "Giuseppe Ateniese"
      ],
      "abstract": "We ask a basic question about cryptographic watermarking for generative models: to what extent can a watermark remain reliable when an adversary is allowed to corrupt the encoded signal? To study this question, we introduce a minimal coding abstraction that we call a zero-bit tamper-detection code. This is a secret-key procedure that samples a pseudorandom codeword and, given a candidate word, decides whether it should be treated as unmarked content or as the result of tampering with a valid codeword. It captures the two core requirements of robust watermarking: soundness and tamper detection.\n  Within this abstraction we prove a sharp unconditional limit on robustness to independent symbol corruption. For an alphabet of size $q$, there is a critical corruption rate of $1 - 1/q$ such that no scheme with soundness, even relaxed to allow a fixed constant false positive probability on random content, can reliably detect tampering once an adversary can change more than this fraction of symbols. In particular, in the binary case no cryptographic watermark can remain robust if more than half of the encoded bits are modified. We also show that this threshold is tight by giving simple information-theoretic constructions that achieve soundness and tamper detection for all strictly smaller corruption rates.\n  We then test experimentally whether this limit appears in practice by looking at the recent watermarking for images of Gunn, Zhao, and Song (ICLR 2025). We show that a simple crop and resize operation reliably flipped about half of the latent signs and consistently prevented belief-propagation decoding from recovering the codeword, erasing the watermark while leaving the image visually intact.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç”Ÿæˆæ¨¡å‹åŠ å¯†æ°´å°(cryptographic watermarking)åœ¨å¯¹æŠ—æ€§ç ´åä¸‹çš„å¯é æ€§æé™ï¼Œå¹¶æå‡ºäº†â€œé›¶ä½ç¯¡æ”¹æ£€æµ‹ä»£ç â€(zero-bit tamper-detection code)è¿™ä¸€æç®€ç¼–ç æŠ½è±¡æ¨¡å‹ã€‚é€šè¿‡è¯¥æŠ½è±¡æ¨¡å‹ï¼Œä½œè€…è¯æ˜äº†ç‹¬ç«‹ç¬¦å·ç ´åä¸‹é²æ£’æ€§çš„æ— æ¡ä»¶ç•Œé™ï¼šå¯¹äºå¤§å°ä¸º $q$ çš„å­—æ¯è¡¨ï¼Œå­˜åœ¨ä¸€ä¸ªä¸´ç•Œç ´åç‡ä¸º $1 - 1/q$ã€‚ç‰¹åˆ«æ˜¯åœ¨äºŒè¿›åˆ¶(binary)åœºæ™¯ä¸‹ï¼Œä¸€æ—¦è¶…è¿‡ä¸€åŠçš„ç¼–ç ä½è¢«ä¿®æ”¹ï¼Œä»»ä½•åŠ å¯†æ°´å°éƒ½æ— æ³•ä¿æŒé²æ£’æ€§ã€‚ç ”ç©¶è¿›ä¸€æ­¥é€šè¿‡ä¿¡æ¯è®ºæ„é€ è¯æ˜äº†è¯¥é˜ˆå€¼çš„ç´§è‡´æ€§ï¼Œå³åœ¨ä½äºè¯¥ç ´åç‡æ—¶ä»èƒ½å®ç°å¯é çš„ç¯¡æ”¹æ£€æµ‹ã€‚æœ€åï¼Œä½œè€…é’ˆå¯¹æœ€æ–°çš„å›¾åƒæ°´å°æŠ€æœ¯è¿›è¡Œäº†å®éªŒéªŒè¯ï¼Œå‘ç°ç®€å•çš„è£å‰ªå’Œç¼©æ”¾æ“ä½œä¼šç¿»è½¬çº¦ä¸€åŠçš„æ½œå˜é‡ç¬¦å·(latent signs)ï¼Œå¯¼è‡´ç½®ä¿¡åº¦ä¼ æ’­(belief-propagation)è§£ç æ— æ³•æ¢å¤ç å­—ã€‚è¿™ä¸€å®éªŒç»“æœåœ¨å®è·µä¸­è¯å®äº†ç†è®ºé¢„æœŸçš„ç¼–ç æé™ï¼Œè¡¨æ˜æ°´å°åœ¨ä¿æŒå›¾åƒè§†è§‰å®Œæ•´æ€§çš„åŒæ—¶å¯è¢«æœ‰æ•ˆæ“¦é™¤ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.10577v2",
      "published_date": "2025-09-11 18:08:32 UTC",
      "updated_date": "2025-11-21 18:14:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:53:44.791999+00:00"
    },
    {
      "arxiv_id": "2509.09679v2",
      "title": "ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable Orthogonal Butterfly Transforms",
      "title_zh": "ButterflyQuantï¼šåŸºäºå¯å­¦ä¹ æ­£äº¤è´è¶å˜æ¢çš„è¶…ä½æ¯”ç‰¹å¤§è¯­è¨€æ¨¡å‹é‡åŒ–",
      "authors": [
        "Bingxin Xu",
        "Zhen Dong",
        "Oussama Elachqar",
        "Yuzhang Shang"
      ],
      "abstract": "Large language models require massive memory footprints, severely limiting deployment on consumer hardware. Quantization reduces memory through lower numerical precision, but extreme 2-bit quantization suffers from catastrophic performance loss due to outliers in activations. Rotation-based methods such as QuIP and QuaRot apply orthogonal transforms to eliminate outliers before quantization, using computational invariance: $\\mathbf{y} = \\mathbf{Wx} = (\\mathbf{WQ}^T)(\\mathbf{Qx})$ for orthogonal $\\mathbf{Q}$. However, these methods use fixed transforms--Hadamard matrices achieving optimal worst-case coherence $Î¼= 1/\\sqrt{n}$--that cannot adapt to specific weight distributions. We identify that different transformer layers exhibit distinct outlier patterns, motivating layer-adaptive rotations rather than one-size-fits-all approaches. In this work, we propose ButterflyQuant, which replaces Hadamard rotations with learnable butterfly transforms parameterized by continuous Givens rotation angles. Unlike Hadamard's discrete $\\{+1, -1\\}$ entries that are non-differentiable and thus prohibit gradient-based learning, butterfly transforms' continuous parameterization enables smooth optimization while guaranteeing orthogonality by construction. This orthogonal constraint ensures theoretical guarantees in outlier suppression while achieving $O(n \\log n)$ computational complexity with only $\\frac{n \\log n}{2}$ learnable parameters. We further introduce a uniformity regularization on post-transformation activations to promote smoother distributions amenable to quantization. Learning requires only 128 calibration samples and converges in minutes on a single GPU--a negligible one-time cost. For LLaMA-2-7B with 2-bit quantization, ButterflyQuant achieves 15.4 perplexity versus 37.3 for QuIP. \\href{https://github.com/42Shawn/Butterflyquant-llm}{Codes} are available.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨è¶…ä½ä½(Ultra-low-bit)é‡åŒ–è¿‡ç¨‹ä¸­å› æ¿€æ´»å€¼ç¦»ç¾¤ç‚¹(Outliers)å¯¼è‡´çš„æ€§èƒ½éª¤é™é—®é¢˜ï¼Œæå‡ºäº†ButterflyQuantã€‚ä¸åŒäºä½¿ç”¨å›ºå®šHadamardå˜æ¢ä¸”æ— æ³•é€‚åº”ç‰¹å®šæƒé‡åˆ†å¸ƒçš„ä¼ ç»Ÿæ—‹è½¬æ–¹æ³•ï¼ŒButterflyQuantå¼•å…¥äº†ç”±è¿ç»­Givensæ—‹è½¬è§’åº¦å‚æ•°åŒ–çš„å¯å­¦ä¹ æ­£äº¤è´è¶å˜æ¢(Learnable Orthogonal Butterfly Transforms)ã€‚è¯¥å˜æ¢åˆ©ç”¨è¿ç»­å‚æ•°åŒ–å®ç°äº†æ¢¯åº¦ä¸‹é™ä¼˜åŒ–ï¼Œåœ¨ä¿æŒ$O(n \\log n)$ä½è®¡ç®—å¤æ‚åº¦çš„åŒæ—¶ï¼Œä¸ºä¸åŒç½‘ç»œå±‚æä¾›äº†è‡ªé€‚åº”çš„ç¦»ç¾¤ç‚¹æŠ‘åˆ¶æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡å‡åŒ€æ€§æ­£åˆ™åŒ–(Uniformity Regularization)è¿›ä¸€æ­¥ä¼˜åŒ–äº†å˜æ¢åçš„æ¿€æ´»å€¼åˆ†å¸ƒï¼Œä½¿å…¶æ›´æ˜“äºé‡åŒ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒButterflyQuantä»…éœ€æå°‘çš„æ ¡å‡†æ ·æœ¬å³å¯å¿«é€Ÿæ”¶æ•›ï¼Œåœ¨LLaMA-2-7Bçš„2-bité‡åŒ–æµ‹è¯•ä¸­å°†å›°æƒ‘åº¦(Perplexity)ä»QuIPçš„37.3å¤§å¹…é™ä½è‡³15.4ï¼Œæ˜¾è‘—æå‡äº†è¶…ä½æ¯”ç‰¹é‡åŒ–æ¨¡å‹çš„æ€§èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Replace discrete Hadamard transforms with continuous Butterfly transforms to facilitate the learning of rotation matrices in LLM quantization",
      "pdf_url": "https://arxiv.org/pdf/2509.09679v2",
      "published_date": "2025-09-11 17:59:51 UTC",
      "updated_date": "2025-09-25 15:12:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:53:45.095205+00:00"
    },
    {
      "arxiv_id": "2509.09677v2",
      "title": "The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs",
      "title_zh": "æ”¶ç›Šé€’å‡çš„é”™è§‰ï¼šè¡¡é‡å¤§è¯­è¨€æ¨¡å‹ä¸­çš„é•¿ç¨‹æ‰§è¡Œèƒ½åŠ›",
      "authors": [
        "Akshit Sinha",
        "Arvindh Arun",
        "Shashwat Goel",
        "Steffen Staab",
        "Jonas Geiping"
      ],
      "abstract": "Does continued scaling of large language models (LLMs) yield diminishing returns? In this work, we show that short-task benchmarks may give an illusion of slowing progress, as even marginal gains in single-step accuracy can compound into exponential improvements in the length of tasks a model can successfully complete. Then, we argue that failures of LLMs when simple tasks are made longer arise from mistakes in execution, rather than an inability to reason. So, we propose isolating execution capability, by explicitly providing the knowledge and plan needed to solve a long-horizon task. First, we find that larger models can correctly execute significantly more turns even when small models have near-perfect single-turn accuracy. We then observe that the per-step accuracy of models degrades as the number of steps increases. This is not just due to long-context limitations -- curiously, we observe a self-conditioning effect -- models become more likely to make mistakes when the context contains their errors from prior turns. Self-conditioning does not reduce by just scaling the model size. But, we find that thinking mitigates self-conditioning, and also enables execution of much longer tasks in a single turn. We conclude by benchmarking frontier thinking models on the length of tasks they can execute in a single turn. Overall, by focusing on the ability to execute, we hope to reconcile debates on how LLMs can solve complex reasoning problems yet fail at simple tasks when made longer, and highlight the massive benefits of scaling model size and sequential test-time compute for long-horizon tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ‰©å±•è§„æ¨¡æ—¶æ˜¯å¦å­˜åœ¨è¾¹é™…æ”¶ç›Šé€’å‡çš„ç°è±¡ï¼ŒæŒ‡å‡ºçŸ­ä»»åŠ¡åŸºå‡†æµ‹è¯•å¯èƒ½è¯¯å¯¼æ€§åœ°æ˜¾ç¤ºè¿›æ­¥æ”¾ç¼“ã€‚ä½œè€…å‘ç°å•æ­¥å‡†ç¡®ç‡çš„è¾¹é™…æå‡åœ¨é•¿ç¨‹æ‰§è¡Œ(long-horizon execution)ä¸­ä¼šè½¬åŒ–ä¸ºæŒ‡æ•°çº§çš„æ€§èƒ½æ”¹è¿›ï¼Œå¹¶è®¤ä¸ºLLMåœ¨é•¿ä»»åŠ¡ä¸­çš„å¤±è´¥ä¸»è¦æºäºæ‰§è¡Œé”™è¯¯è€Œéæ¨ç†èƒ½åŠ›ä¸è¶³ã€‚é€šè¿‡éš”ç¦»æ‰§è¡Œèƒ½åŠ›è¿›è¡Œå®éªŒï¼Œç ”ç©¶æ­ç¤ºäº†â€œè‡ªæ¡ä»¶ä½œç”¨â€(self-conditioning)æ•ˆåº”ï¼Œå³æ¨¡å‹åœ¨åŒ…å«è‡ªèº«å†å²é”™è¯¯çš„æƒ…å¢ƒä¸‹æ›´æ˜“å‡ºé”™ï¼Œä¸”è¿™ç§å€¾å‘ä¸ä¼šéšæ¨¡å‹è§„æ¨¡æ‰©å±•è€Œè‡ªç„¶æ¶ˆé™¤ã€‚ç„¶è€Œï¼Œå¼•å…¥â€œæ€è€ƒâ€(thinking)æœºåˆ¶èƒ½æœ‰æ•ˆç¼“è§£è‡ªæ¡ä»¶ä½œç”¨ï¼Œå¹¶æ˜¾è‘—æå‡æ¨¡å‹åœ¨å•å›åˆå†…æ‰§è¡Œé•¿ä»»åŠ¡çš„èƒ½åŠ›ã€‚è¯¥ç ”ç©¶æœ€ç»ˆé€šè¿‡è¡¡é‡å‰æ²¿æ¨¡å‹åœ¨å•å›åˆå†…å¯æ‰§è¡Œçš„ä»»åŠ¡é•¿åº¦ï¼Œå¼ºè°ƒäº†æ‰©å±•æ¨¡å‹è§„æ¨¡å’Œå¢åŠ æ¨ç†æ—¶è®¡ç®—(test-time compute)å¯¹è§£å†³é•¿ç¨‹ä»»åŠ¡çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.09677v2",
      "published_date": "2025-09-11 17:59:34 UTC",
      "updated_date": "2025-09-28 13:00:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:53:50.194908+00:00"
    },
    {
      "arxiv_id": "2509.09675v1",
      "title": "CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning in Large Language Models",
      "title_zh": "CDEï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹é«˜æ•ˆå¼ºåŒ–å­¦ä¹ çš„å¥½å¥‡å¿ƒé©±åŠ¨æ¢ç´¢",
      "authors": [
        "Runpeng Dai",
        "Linfeng Song",
        "Haolin Liu",
        "Zhenwen Liang",
        "Dian Yu",
        "Haitao Mi",
        "Zhaopeng Tu",
        "Rui Liu",
        "Tong Zheng",
        "Hongtu Zhu",
        "Dong Yu"
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful paradigm for enhancing the reasoning ability of Large Language Models (LLMs). Yet current RLVR methods often explore poorly, leading to premature convergence and entropy collapse. To address this challenge, we introduce Curiosity-Driven Exploration (CDE), a framework that leverages the model's own intrinsic sense of curiosity to guide exploration. We formalize curiosity with signals from both the actor and the critic: for the actor, we use perplexity over its generated response, and for the critic, we use the variance of value estimates from a multi-head architecture. Both signals serve as an exploration bonus within the RLVR framework to guide the model. Our theoretical analysis shows that the actor-wise bonus inherently penalizes overconfident errors and promotes diversity among correct responses; moreover, we connect the critic-wise bonus to the well-established count-based exploration bonus in RL. Empirically, our method achieves an approximate +3 point improvement over standard RLVR using GRPO/PPO on AIME benchmarks. Further analysis identifies a calibration collapse mechanism within RLVR, shedding light on common LLM failure modes.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨åˆ©ç”¨å¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ (Reinforcement Learning with Verifiable Rewards, RLVR)æå‡æ¨ç†èƒ½åŠ›æ—¶ï¼Œå¸¸é¢ä¸´æ¢ç´¢æ•ˆç‡ä½ã€è¿‡æ—©æ”¶æ•›åŠç†µå¡Œé™·ç­‰æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†å¥½å¥‡å¿ƒé©±åŠ¨æ¢ç´¢(Curiosity-Driven Exploration, CDE)æ¡†æ¶ï¼Œé€šè¿‡Actorç”Ÿæˆçš„å›°æƒ‘åº¦(Perplexity)å’ŒMulti-headæ¶æ„ä¸­Criticä»·å€¼ä¼°è®¡çš„æ–¹å·®æ¥å½¢å¼åŒ–å¥½å¥‡å¿ƒä¿¡å·ã€‚è¿™äº›ä¿¡å·ä½œä¸ºæ¢ç´¢å¥–åŠ±(Exploration Bonus)å¼•å¯¼æ¨¡å‹ï¼Œç†è®ºåˆ†ææ˜¾ç¤ºActorç«¯çš„å¥–åŠ±èƒ½æœ‰æ•ˆæƒ©ç½šè¿‡åº¦è‡ªä¿¡çš„é”™è¯¯å¹¶æå‡æ­£ç¡®å›å¤çš„å¤šæ ·æ€§ï¼Œè€ŒCriticç«¯çš„å¥–åŠ±åˆ™ä¸å¼ºåŒ–å­¦ä¹ ä¸­ç»å…¸çš„Count-based Exploration Bonuså»ºç«‹äº†è”ç³»ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨AIMEåŸºå‡†æµ‹è¯•ä¸­æ¯”ä½¿ç”¨GRPOæˆ–PPOçš„ standard RLVR æå‡äº†çº¦3åˆ†ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ­ç¤ºäº†RLVRä¸­å­˜åœ¨çš„æ ¡å‡†å¡Œé™·(Calibration Collapse)æœºåˆ¶ï¼Œä¸ºå¤§è¯­è¨€æ¨¡å‹çš„å¤±æ•ˆæ¨¡å¼åˆ†ææä¾›äº†æ–°è§†è§’ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "21 pages",
      "pdf_url": "https://arxiv.org/pdf/2509.09675v1",
      "published_date": "2025-09-11 17:59:17 UTC",
      "updated_date": "2025-09-11 17:59:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:53:56.091386+00:00"
    },
    {
      "arxiv_id": "2509.09674v1",
      "title": "SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning",
      "title_zh": "SimpleVLA-RLï¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ æ‰©å±• VLA è®­ç»ƒ",
      "authors": [
        "Haozhan Li",
        "Yuxin Zuo",
        "Jiale Yu",
        "Yuhao Zhang",
        "Zhaohui Yang",
        "Kaiyan Zhang",
        "Xuekai Zhu",
        "Yuchen Zhang",
        "Tianxing Chen",
        "Ganqu Cui",
        "Dehui Wang",
        "Dingxiang Luo",
        "Yuchen Fan",
        "Youbang Sun",
        "Jia Zeng",
        "Jiangmiao Pang",
        "Shanghang Zhang",
        "Yu Wang",
        "Yao Mu",
        "Bowen Zhou",
        "Ning Ding"
      ],
      "abstract": "Vision-Language-Action (VLA) models have recently emerged as a powerful paradigm for robotic manipulation. Despite substantial progress enabled by large-scale pretraining and supervised fine-tuning (SFT), these models face two fundamental challenges: (i) the scarcity and high cost of large-scale human-operated robotic trajectories required for SFT scaling, and (ii) limited generalization to tasks involving distribution shift. Recent breakthroughs in Large Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can dramatically enhance step-by-step reasoning capabilities, raising a natural question: Can RL similarly improve the long-horizon step-by-step action planning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL framework tailored for VLA models. Building upon veRL, we introduce VLA-specific trajectory sampling, scalable parallelization, multi-environment rendering, and optimized loss computation. When applied to OpenVLA-OFT, SimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms $Ï€_0$ on RoboTwin 1.0\\&2.0 with the exploration-enhancing strategies we introduce. SimpleVLA-RL not only reduces dependence on large-scale data and enables robust generalization, but also remarkably surpasses SFT in real-world tasks. Moreover, we identify a novel phenomenon ``pushcut'' during RL training, wherein the policy discovers previously unseen patterns beyond those seen in the previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL",
      "tldr_zh": "é’ˆå¯¹æœºå™¨äººæ“çºµé¢†åŸŸä¸­ Vision-Language-Action (VLA) æ¨¡å‹é¢ä¸´çš„ SFT æ•°æ®ç¨€ç¼ºã€æˆæœ¬é«˜æ˜‚ä»¥åŠåˆ†å¸ƒåç§»å¯¼è‡´çš„æ³›åŒ–èƒ½åŠ›å—é™ç­‰æ ¸å¿ƒæŒ‘æˆ˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº† SimpleVLA-RL æ¡†æ¶ã€‚å—åˆ° Large Reasoning Models (LRMs) é€šè¿‡å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) æå‡æ¨ç†èƒ½åŠ›çš„å¯å‘ï¼Œè¯¥æ¡†æ¶åŸºäº veRL å¼•å…¥äº†é’ˆå¯¹ VLA çš„è½¨è¿¹é‡‡æ ·ã€å¯æ‰©å±•å¹¶è¡ŒåŒ–ã€å¤šç¯å¢ƒæ¸²æŸ“ä»¥åŠä¼˜åŒ–çš„æŸå¤±è®¡ç®—ã€‚ç ”ç©¶å›¢é˜Ÿè¿›ä¸€æ­¥å¼•å…¥äº†å¢å¼ºæ¢ç´¢çš„ç­–ç•¥ï¼Œä½¿å¾—è¯¥æ¨¡å‹åœ¨ LIBERO å’Œ RoboTwin 1.0/2.0 ç­‰åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº† SoTA æ€§èƒ½ï¼Œç”šè‡³è¶…è¶Šäº† $\\pi_0$ æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSimpleVLA-RL ä¸ä»…æ˜¾è‘—é™ä½äº†å¯¹å¤§è§„æ¨¡æ•°æ®çš„ä¾èµ–å¹¶å¢å¼ºäº†é²æ£’æ€§ï¼Œåœ¨çœŸå®ä¸–ç•Œä»»åŠ¡ä¸­çš„è¡¨ç°ä¹Ÿä¼˜äºä¼ ç»Ÿçš„ Supervised Fine-Tuning (SFT) æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç°äº†ä¸€ç§åä¸º \"pushcut\" çš„æ–°ç°è±¡ï¼Œå³ç­–ç•¥èƒ½å¤Ÿå‘ç°å…ˆå‰è®­ç»ƒè¿‡ç¨‹ä¸­æœªæ›¾å‡ºç°çš„æ–°åŠ¨ä½œæ¨¡å¼ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.09674v1",
      "published_date": "2025-09-11 17:59:17 UTC",
      "updated_date": "2025-09-11 17:59:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:53:57.884632+00:00"
    },
    {
      "arxiv_id": "2509.10576v1",
      "title": "Aesthetic Experience and Educational Value in Co-creating Art with Generative AI: Evidence from a Survey of Young Learners",
      "title_zh": "ä¸ç”Ÿæˆå¼äººå·¥æ™ºèƒ½è‰ºæœ¯å…±åˆ›ä¸­çš„å®¡ç¾ä½“éªŒä¸æ•™è‚²ä»·å€¼ï¼šåŸºäºé’å°‘å¹´å­¦ä¹ è€…çš„è°ƒæŸ¥ç ”ç©¶",
      "authors": [
        "Chengyuan Zhang",
        "Suzhe Xu"
      ],
      "abstract": "This study investigates the aesthetic experience and educational value of collaborative artmaking with generative artificial intelligence (AI) among young learners and art students. Based on a survey of 112 participants, we examine how human creators renegotiate their roles, how conventional notions of originality are challenged, how the creative process is transformed, and how aesthetic judgment is formed in human--AI co-creation. Empirically, participants generally view AI as a partner that stimulates ideation and expands creative boundaries rather than a passive tool, while simultaneously voicing concerns about stylistic homogenization and the erosion of traditional authorship. Theoretically, we synthesize Dewey's aesthetics of experience, Ihde's postphenomenology, and actor--network theory (ANT) into a single analytical framework to unpack the dynamics between human creators and AI as a non-human actant. Findings indicate (i) a fluid subjectivity in which creators shift across multiple stances (director, dialogic partner, discoverer); (ii) an iterative, dialogic workflow (intent--generate--select--refine) that centers critical interpretation; and (iii) an educational value shift from technical skill training toward higher-order competencies such as critical judgment, cross-modal ideation, and reflexivity. We argue that arts education should cultivate a \\emph{critical co-creation} stance toward technology, guiding learners to collaborate with AI while preserving human distinctiveness in concept formation, judgment, and meaning-making.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡å¯¹112åé’å°‘å¹´å­¦ä¹ è€…å’Œè‰ºæœ¯ç³»å­¦ç”Ÿçš„è°ƒæŸ¥ï¼Œæ¢è®¨äº†äººç±»ä¸ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenerative AIï¼‰å…±åŒåˆ›ä½œè‰ºæœ¯æ—¶çš„å®¡ç¾ä½“éªŒä¸æ•™è‚²ä»·å€¼ã€‚ç ”ç©¶æ•´åˆäº†æœå¨çš„ç»éªŒç¾å­¦ã€ä¼Šå¾·çš„åç°è±¡å­¦ä»¥åŠè¡ŒåŠ¨è€…ç½‘ç»œç†è®ºï¼ˆActor-Network Theoryï¼‰ï¼Œæ·±å…¥åˆ†æäº†äººç±»åˆ›ä½œè€…ä¸ä½œä¸ºéäººç±»è¡ŒåŠ¨è€…çš„AIä¹‹é—´çš„äº’åŠ¨åŠ¨æ€ã€‚ç»“æœæ˜¾ç¤ºï¼Œå‚ä¸è€…æ™®éå°†AIè§†ä¸ºæ¿€å‘çµæ„Ÿå¹¶æ‹“å±•åˆ›ä½œè¾¹ç•Œçš„åˆä½œä¼™ä¼´ï¼Œåˆ›ä½œè€…çš„ä¸»ä½“æ€§åœ¨å¯¼æ¼”ã€å¯¹è¯ä¼™ä¼´å’Œå‘ç°è€…ç­‰å¤šç§è§’è‰²é—´çµæ´»è½¬æ¢ã€‚åˆ›ä½œè¿‡ç¨‹æ¼”å˜ä¸ºä¸€ç§æ¶µç›–â€œæ„å›¾-ç”Ÿæˆ-é€‰æ‹©-ä¼˜åŒ–â€çš„å¾ªç¯å¯¹è¯å¼å·¥ä½œæµï¼Œå¹¶å°†æ‰¹åˆ¤æ€§è§£é‡Šç½®äºæ ¸å¿ƒåœ°ä½ã€‚ç ”ç©¶å‘ç°æ•™è‚²ä»·å€¼æ­£ä»ä¼ ç»Ÿçš„æŠ€æœ¯æŠ€èƒ½åŸ¹è®­è½¬å‘æ‰¹åˆ¤æ€§åˆ¤æ–­ã€è·¨æ¨¡æ€æ„æ€å’Œåæ€æ€§ç­‰é«˜é˜¶èƒ½åŠ›ã€‚æœ€åï¼Œç ”ç©¶å»ºè®®è‰ºæœ¯æ•™è‚²åº”åŸ¹å…»â€œæ‰¹åˆ¤æ€§å…±åˆ›â€ï¼ˆcritical co-creationï¼‰ç«‹åœºï¼Œå¼•å¯¼å­¦ä¹ è€…åœ¨ä¸AIåä½œçš„åŒæ—¶ï¼Œä¿ç•™äººç±»åœ¨æ¦‚å¿µå½¢æˆã€å®¡ç¾åˆ¤æ–­å’Œæ„ä¹‰å»ºæ„ä¸­çš„ç‹¬ç‰¹æ€§ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.10576v1",
      "published_date": "2025-09-11 17:55:46 UTC",
      "updated_date": "2025-09-11 17:55:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:54:01.312544+00:00"
    },
    {
      "arxiv_id": "2509.09655v1",
      "title": "Feasibility-Guided Fair Adaptive Offline Reinforcement Learning for Medicaid Care Management",
      "title_zh": "é¢å‘ Medicaid æŠ¤ç†ç®¡ç†çš„å¯è¡Œæ€§å¼•å¯¼å…¬å¹³è‡ªé€‚åº”ç¦»çº¿å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Sanjay Basu",
        "Sadiq Y. Patel",
        "Parth Sheth",
        "Bhairavi Muralidharan",
        "Namrata Elamaran",
        "Aakriti Kinra",
        "Rajaie Batniji"
      ],
      "abstract": "We introduce Feasibility-Guided Fair Adaptive Reinforcement Learning (FG-FARL), an offline RL procedure that calibrates per-group safety thresholds to reduce harm while equalizing a chosen fairness target (coverage or harm) across protected subgroups. Using de-identified longitudinal trajectories from a Medicaid population health management program, we evaluate FG-FARL against behavior cloning (BC) and HACO (Hybrid Adaptive Conformal Offline RL; a global conformal safety baseline). We report off-policy value estimates with bootstrap 95% confidence intervals and subgroup disparity analyses with p-values. FG-FARL achieves comparable value to baselines while improving fairness metrics, demonstrating a practical path to safer and more equitable decision support.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†å¯è¡Œæ€§å¼•å¯¼çš„å…¬å¹³è‡ªé€‚åº”å¼ºåŒ–å­¦ä¹  (Feasibility-Guided Fair Adaptive Reinforcement Learning, FG-FARL)ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹ Medicaid æŠ¤ç†ç®¡ç†çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹  (offline RL) ç¨‹åºã€‚è¯¥æ–¹æ³•é€šè¿‡æ ¡å‡†å„å­ç¾¤ä½“çš„å®‰å…¨é˜ˆå€¼ (safety thresholds) æ¥å‡å°‘ä¼¤å®³ï¼ŒåŒæ—¶åœ¨å—ä¿æŠ¤çš„å­ç¾¤ä½“ä¹‹é—´å¹³è¡¡ç‰¹å®šçš„å…¬å¹³æ€§ç›®æ ‡ï¼ˆå¦‚è¦†ç›–ç‡æˆ–ä¼¤å®³ï¼‰ã€‚ç ”ç©¶ä½¿ç”¨æ¥è‡ª Medicaid äººå£å¥åº·ç®¡ç†è®¡åˆ’çš„å»æ ‡è¯†åŒ–çºµå‘è½¨è¿¹æ•°æ®ï¼Œå°† FG-FARL ä¸è¡Œä¸ºå…‹éš† (behavior cloning, BC) å’Œæ··åˆè‡ªé€‚åº”å…±å½¢ç¦»çº¿å¼ºåŒ–å­¦ä¹  (HACO) è¿›è¡Œäº†å¯¹æ¯”è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFG-FARL åœ¨å®ç°ä¸åŸºå‡†æ¨¡å‹ç›¸å½“çš„ç­–ç•¥ä»·å€¼çš„åŒæ—¶ï¼Œæ˜¾è‘—æ”¹å–„äº†å…¬å¹³æ€§æŒ‡æ ‡ (fairness metrics)ã€‚è¿™ä¸€ç ”ç©¶å±•ç¤ºäº†å®ç°æ›´å®‰å…¨ã€æ›´å…¬å¹³çš„å†³ç­–æ”¯æŒç³»ç»Ÿçš„å¯è¡Œè·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.LO",
        "stat.AP"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages, 5 figures, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.09655v1",
      "published_date": "2025-09-11 17:50:06 UTC",
      "updated_date": "2025-09-11 17:50:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:54:06.397752+00:00"
    },
    {
      "arxiv_id": "2509.09651v2",
      "title": "Retrieval-Augmented Generation for Reliable Interpretation of Radio Regulations",
      "title_zh": "é¢å‘æ— çº¿ç”µæ¡ä¾‹å¯é è§£è¯»çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ",
      "authors": [
        "Zakaria El Kassimi",
        "Fares Fourati",
        "Mohamed-Slim Alouini"
      ],
      "abstract": "We study question answering in the domain of radio regulations, a legally sensitive and high-stakes area. We propose a telecom-specific Retrieval-Augmented Generation (RAG) pipeline and introduce, to our knowledge, the first multiple-choice evaluation set for this domain, constructed from authoritative sources using automated filtering and human validation. To assess retrieval quality, we define a domain-specific retrieval metric, under which our retriever achieves approximately 97% accuracy. Beyond retrieval, our approach consistently improves generation accuracy across all tested models. In particular, while naively inserting documents without structured retrieval yields only marginal gains for GPT-4o (less than 1%), applying our pipeline results in nearly a 12% relative improvement. These findings demonstrate that carefully targeted grounding provides a simple yet strong baseline and an effective domain-specific solution for regulatory question answering. All code and evaluation scripts, along with our derived question-answer dataset, are available at https://github.com/Zakaria010/Radio-RAG.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…·æœ‰æ³•å¾‹æ•æ„Ÿæ€§å’Œé«˜é£é™©çš„æ— çº¿ç”µç®¡ç†(Radio Regulations)é¢†åŸŸï¼Œæå‡ºäº†ä¸€ä¸ªä¸“ç”¨çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)æµç¨‹ï¼Œæ—¨åœ¨å®ç°å¯é çš„è‡ªåŠ¨åŒ–é—®ç­”ã€‚ä½œè€…æ„å»ºäº†è¯¥é¢†åŸŸé¦–ä¸ªåŸºäºæƒå¨æ¥æºã€ç»è¿‡è‡ªåŠ¨è¿‡æ»¤å’Œäººå·¥éªŒè¯çš„å¤šé€‰é¢˜è¯„ä¼°æ•°æ®é›†ã€‚å®éªŒå®šä¹‰äº†ç‰¹å®šé¢†åŸŸçš„æ£€ç´¢æŒ‡æ ‡ï¼Œè¯¥æµç¨‹çš„æ£€ç´¢å‡†ç¡®ç‡è¾¾åˆ°çº¦97%ï¼Œå¹¶æ˜¾è‘—ä¸€è‡´åœ°æå‡äº†æ‰€æœ‰æµ‹è¯•æ¨¡å‹çš„ç”Ÿæˆå‡†ç¡®ç‡ã€‚å…¶ä¸­ï¼ŒGPT-4oåœ¨åº”ç”¨è¯¥æµç¨‹åï¼Œç›¸æ¯”äºç®€å•çš„æ–‡æ¡£ç›´æ¥æ¤å…¥ï¼Œåœ¨ç”Ÿæˆè¡¨ç°ä¸Šå®ç°äº†è¿‘12%çš„ç›¸å¯¹æå‡ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œé’ˆå¯¹æ€§çš„çŸ¥è¯†é”šå®š(Grounding)èƒ½ä¸ºæ³•è§„é—®ç­”æä¾›å¼ºæœ‰åŠ›çš„é¢†åŸŸç‰¹å®šè§£å†³æ–¹æ¡ˆï¼Œç›®å‰ç›¸å…³ä»£ç ã€è¯„ä¼°è„šæœ¬åŠæ•°æ®é›†å·²å…¬å¼€ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "eess.SP"
      ],
      "primary_category": "cs.IR",
      "comment": "12 pages, 7 figures, AI4NextG @ NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.09651v2",
      "published_date": "2025-09-11 17:43:42 UTC",
      "updated_date": "2025-11-13 14:02:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:54:24.258853+00:00"
    },
    {
      "arxiv_id": "2509.10575v1",
      "title": "Gene-R1: Reasoning with Data-Augmented Lightweight LLMs for Gene Set Analysis",
      "title_zh": "Gene-R1ï¼šåŸºäºæ•°æ®å¢å¼ºè½»é‡çº§å¤§è¯­è¨€æ¨¡å‹çš„åŸºå› é›†åˆ†ææ¨ç†",
      "authors": [
        "Zhizheng Wang",
        "Yifan Yang",
        "Qiao Jin",
        "Zhiyong Lu"
      ],
      "abstract": "The gene set analysis (GSA) is a foundational approach for uncovering the molecular functions associated with a group of genes. Recently, LLM-powered methods have emerged to annotate gene sets with biological functions together with coherent explanatory insights. However, existing studies primarily focus on proprietary models, which have been shown to outperform their open-source counterparts despite concerns over cost and data privacy. Furthermore, no research has investigated the application of advanced reasoning strategies to the GSA task. To address this gap, we introduce Gene-R1, a data-augmented learning framework that equips lightweight and open-source LLMs with step-by-step reasoning capabilities tailored to GSA. Experiments on 1,508 in-distribution gene sets demonstrate that Gene-R1 achieves substantial performance gains, matching commercial LLMs. On 106 out-of-distribution gene sets, Gene-R1 performs comparably to both commercial and large-scale LLMs, exhibiting robust generalizability across diverse gene sources.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Gene-R1ï¼Œä¸€ç§æ—¨åœ¨æå‡è½»é‡çº§å¼€æºLLMsåœ¨åŸºå› é›†åˆ†æ(gene set analysis, GSA)ä¸­è¡¨ç°çš„æ•°æ®å¢å¼ºå­¦ä¹ æ¡†æ¶ã€‚é’ˆå¯¹ç›®å‰GSAä»»åŠ¡è¿‡åº¦ä¾èµ–å•†ä¸šé—­æºæ¨¡å‹ä¸”ç¼ºä¹ä¸“é—¨æ¨ç†ç­–ç•¥çš„é—®é¢˜ï¼ŒGene-R1é€šè¿‡èµ‹äºˆæ¨¡å‹åˆ†æ­¥æ¨ç†(step-by-step reasoning)èƒ½åŠ›ï¼Œæœ‰æ•ˆæå‡äº†è½»é‡çº§æ¨¡å‹å¯¹åŸºå› ç”Ÿç‰©åŠŸèƒ½çš„ç†è§£æ·±åº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨1,508ä¸ªåˆ†å¸ƒå†…åŸºå› é›†æµ‹è¯•ä¸­ï¼ŒGene-R1çš„æ€§èƒ½å·²èƒ½åŒ¹é…ä¸»æµå•†ä¸šæ¨¡å‹ã€‚åœ¨é’ˆå¯¹106ä¸ªåˆ†å¸ƒå¤–åŸºå› é›†çš„è¯„ä¼°ä¸­ï¼Œè¯¥æ¡†æ¶å±•ç°å‡ºä¸å¤§è§„æ¨¡LLMsç›¸å½“çš„ç¨³å¥æ³›åŒ–èƒ½åŠ›ï¼Œè¯æ˜äº†å…¶åœ¨å¤„ç†å¤šæ ·åŒ–åŸºå› æ¥æºæ—¶çš„æœ‰æ•ˆæ€§ã€‚è¿™ä¸€æˆæœä¸ºåœ¨ä½æˆæœ¬å’Œé«˜éšç§ä¿æŠ¤éœ€æ±‚ä¸‹å®ç°é«˜è´¨é‡çš„åŸºå› ç»„å­¦æ•°æ®åˆ†ææä¾›äº†é‡è¦çš„æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "q-bio.GN",
        "cs.AI"
      ],
      "primary_category": "q-bio.GN",
      "comment": "14 pages, 4 figures, 6 tables, 40 references",
      "pdf_url": "https://arxiv.org/pdf/2509.10575v1",
      "published_date": "2025-09-11 17:14:08 UTC",
      "updated_date": "2025-09-11 17:14:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:54:24.551364+00:00"
    },
    {
      "arxiv_id": "2509.09616v1",
      "title": "Explaining Concept Drift through the Evolution of Group Counterfactuals",
      "title_zh": "åŸºäºç¾¤ä½“åäº‹å®æ¼”åŒ–çš„æ¦‚å¿µæ¼‚ç§»è§£é‡Š",
      "authors": [
        "Ignacy StÄ™pka",
        "Jerzy Stefanowski"
      ],
      "abstract": "Machine learning models in dynamic environments often suffer from concept drift, where changes in the data distribution degrade performance. While detecting this drift is a well-studied topic, explaining how and why the model's decision-making logic changes still remains a significant challenge. In this paper, we introduce a novel methodology to explain concept drift by analyzing the temporal evolution of group-based counterfactual explanations (GCEs). Our approach tracks shifts in the GCEs' cluster centroids and their associated counterfactual action vectors before and after a drift. These evolving GCEs act as an interpretable proxy, revealing structural changes in the model's decision boundary and its underlying rationale. We operationalize this analysis within a three-layer framework that synergistically combines insights from the data layer (distributional shifts), the model layer (prediction disagreement), and our proposed explanation layer. We show that such holistic view allows for a more comprehensive diagnosis of drift, making it possible to distinguish between different root causes, such as a spatial data shift versus a re-labeling of concepts.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨åŠ¨æ€ç¯å¢ƒä¸­é¢ä¸´çš„ Concept Drift (æ¦‚å¿µæ¼‚ç§») é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ—¨åœ¨è§£é‡Šæ¨¡å‹å†³ç­–é€»è¾‘å˜åŒ–çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ†æ Group-based Counterfactual Explanations (GCEs, ç»„åäº‹å®è§£é‡Š) çš„æ—¶é—´æ¼”å˜ï¼Œè¿½è¸ªæ¼‚ç§»å‘ç”Ÿå‰å GCEs çš„èšç±»è´¨å¿ƒåŠå…¶å…³è”çš„åäº‹å®åŠ¨ä½œå‘é‡çš„å˜åŒ–ã€‚æ¼”å˜ä¸­çš„ GCEs ä½œä¸ºå¯è§£é‡Šçš„ä»£ç†ï¼Œèƒ½å¤Ÿæ­ç¤ºæ¨¡å‹å†³ç­–è¾¹ç•Œçš„ç»“æ„æ€§è°ƒæ•´åŠå…¶æ½œåœ¨é€»è¾‘ã€‚ç ”ç©¶å°†æ­¤åˆ†æçº³å…¥ä¸€ä¸ªä¸‰å±‚æ¡†æ¶ä¸­ï¼ŒååŒç»“åˆäº†æ•°æ®å±‚ï¼ˆåˆ†å¸ƒåç§»ï¼‰ã€æ¨¡å‹å±‚ï¼ˆé¢„æµ‹ä¸ä¸€è‡´ï¼‰ä»¥åŠæ‰€æå‡ºçš„è§£é‡Šå±‚ã€‚è¿™ç§æ•´ä½“è§†è§’å®ç°äº†æ›´å…¨é¢çš„æ¼‚ç§»è¯Šæ–­ï¼Œä½¿å¾—åŒºåˆ†ç©ºé—´æ•°æ®åç§»ä¸æ¦‚å¿µé‡æ ‡è®°ï¼ˆre-labelingï¼‰ç­‰ä¸åŒæ ¹æºåŸå› æˆä¸ºå¯èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "TempXAI Workshop @ ECML PKDD 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.09616v1",
      "published_date": "2025-09-11 16:58:34 UTC",
      "updated_date": "2025-09-11 16:58:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:54:26.460893+00:00"
    },
    {
      "arxiv_id": "2509.09614v1",
      "title": "LoCoBench: A Benchmark for Long-Context Large Language Models in Complex Software Engineering",
      "title_zh": "LoCoBenchï¼šé¢å‘å¤æ‚è½¯ä»¶å·¥ç¨‹çš„é•¿ä¸Šä¸‹æ–‡å¤§è¯­è¨€æ¨¡å‹è¯„æµ‹åŸºå‡†",
      "authors": [
        "Jielin Qiu",
        "Zuxin Liu",
        "Zhiwei Liu",
        "Rithesh Murthy",
        "Jianguo Zhang",
        "Haolin Chen",
        "Shiyu Wang",
        "Ming Zhu",
        "Liangwei Yang",
        "Juntao Tan",
        "Zhepeng Cen",
        "Cheng Qian",
        "Shelby Heinecke",
        "Weiran Yao",
        "Silvio Savarese",
        "Caiming Xiong",
        "Huan Wang"
      ],
      "abstract": "The emergence of long-context language models with context windows extending to millions of tokens has created new opportunities for sophisticated code understanding and software development evaluation. We propose LoCoBench, a comprehensive benchmark specifically designed to evaluate long-context LLMs in realistic, complex software development scenarios. Unlike existing code evaluation benchmarks that focus on single-function completion or short-context tasks, LoCoBench addresses the critical evaluation gap for long-context capabilities that require understanding entire codebases, reasoning across multiple files, and maintaining architectural consistency across large-scale software systems. Our benchmark provides 8,000 evaluation scenarios systematically generated across 10 programming languages, with context lengths spanning 10K to 1M tokens, a 100x variation that enables precise assessment of long-context performance degradation in realistic software development settings. LoCoBench introduces 8 task categories that capture essential long-context capabilities: architectural understanding, cross-file refactoring, multi-session development, bug investigation, feature implementation, code comprehension, integration testing, and security analysis. Through a 5-phase pipeline, we create diverse, high-quality scenarios that challenge LLMs to reason about complex codebases at unprecedented scale. We introduce a comprehensive evaluation framework with 17 metrics across 4 dimensions, including 8 new evaluation metrics, combined in a LoCoBench Score (LCBS). Our evaluation of state-of-the-art long-context models reveals substantial performance gaps, demonstrating that long-context understanding in complex software development represents a significant unsolved challenge that demands more attention. LoCoBench is released at: https://github.com/SalesforceAIResearch/LoCoBench.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LoCoBenchï¼Œä¸€ä¸ªä¸“é—¨ç”¨äºåœ¨å¤æ‚è½¯ä»¶å·¥ç¨‹åœºæ™¯ä¸­è¯„ä¼°é•¿ä¸Šä¸‹æ–‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLong-context LLMsï¼‰çš„ç»¼åˆåŸºå‡†æµ‹è¯•ã€‚ä¸åŒäºä¸“æ³¨äºå•å‡½æ•°æˆ–çŸ­ä¸Šä¸‹æ–‡çš„ç°æœ‰åŸºå‡†ï¼ŒLoCoBench æ¶µç›–äº† 10 ç§ç¼–ç¨‹è¯­è¨€ï¼Œæä¾› 8,000 ä¸ªè¯„ä¼°åœºæ™¯ï¼Œå…¶ä¸Šä¸‹æ–‡é•¿åº¦ä» 10K æ‰©å±•è‡³ 1M æ ‡è®°ï¼ˆTokensï¼‰ï¼Œå®ç°äº† 100 å€çš„è·¨åº¦å˜åŒ–ã€‚è¯¥åŸºå‡†å¼•å…¥äº†åŒ…æ‹¬æ¶æ„ç†è§£ï¼ˆArchitectural understandingï¼‰ã€è·¨æ–‡ä»¶é‡æ„ï¼ˆCross-file refactoringï¼‰å’Œå®‰å…¨åˆ†æï¼ˆSecurity analysisï¼‰åœ¨å†…çš„ 8 ä¸ªä»»åŠ¡ç±»åˆ«ï¼Œæ—¨åœ¨å…¨é¢è¡¡é‡æ¨¡å‹å¯¹å¤§è§„æ¨¡è½¯ä»¶ç³»ç»Ÿçš„ç†è§£ã€æ¨ç†åŠæ¶æ„ä¸€è‡´æ€§ç»´æŠ¤èƒ½åŠ›ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡ 5 é˜¶æ®µæµæ°´çº¿æ„å»ºäº†é«˜è´¨é‡åœºæ™¯ï¼Œå¹¶æå‡ºäº†åŒ…å« 17 ä¸ªæŒ‡æ ‡çš„è¯„ä¼°æ¡†æ¶åŠ LoCoBench Score (LCBS)ã€‚å¯¹å½“å‰æœ€å…ˆè¿›æ¨¡å‹çš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œåœ¨å¤„ç†å¤æ‚è½¯ä»¶å¼€å‘çš„é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡æ—¶ä»å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ï¼Œè¯æ˜äº†é•¿ä¸Šä¸‹æ–‡ç†è§£åœ¨è½¯ä»¶å·¥ç¨‹é¢†åŸŸä»æ˜¯ä¸€ä¸ªäºŸå¾…è§£å†³çš„é‡è¦æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "53 pages",
      "pdf_url": "https://arxiv.org/pdf/2509.09614v1",
      "published_date": "2025-09-11 16:55:04 UTC",
      "updated_date": "2025-09-11 16:55:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:54:29.966609+00:00"
    },
    {
      "arxiv_id": "2509.09610v1",
      "title": "Mechanistic Learning with Guided Diffusion Models to Predict Spatio-Temporal Brain Tumor Growth",
      "title_zh": "èåˆå¼•å¯¼æ‰©æ•£æ¨¡å‹çš„æœºåˆ¶å­¦ä¹ ï¼šé¢„æµ‹è„‘è‚¿ç˜¤çš„æ—¶ç©ºç”Ÿé•¿",
      "authors": [
        "Daria Laslo",
        "Efthymios Georgiou",
        "Marius George Linguraru",
        "Andreas Rauschecker",
        "Sabine Muller",
        "Catherine R. Jutzeler",
        "Sarah Bruningk"
      ],
      "abstract": "Predicting the spatio-temporal progression of brain tumors is essential for guiding clinical decisions in neuro-oncology. We propose a hybrid mechanistic learning framework that combines a mathematical tumor growth model with a guided denoising diffusion implicit model (DDIM) to synthesize anatomically feasible future MRIs from preceding scans. The mechanistic model, formulated as a system of ordinary differential equations, captures temporal tumor dynamics including radiotherapy effects and estimates future tumor burden. These estimates condition a gradient-guided DDIM, enabling image synthesis that aligns with both predicted growth and patient anatomy. We train our model on the BraTS adult and pediatric glioma datasets and evaluate on 60 axial slices of in-house longitudinal pediatric diffuse midline glioma (DMG) cases. Our framework generates realistic follow-up scans based on spatial similarity metrics. It also introduces tumor growth probability maps, which capture both clinically relevant extent and directionality of tumor growth as shown by 95th percentile Hausdorff Distance. The method enables biologically informed image generation in data-limited scenarios, offering generative-space-time predictions that account for mechanistic priors.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ··åˆæœºæ¢°å­¦ä¹ (Mechanistic Learning)æ¡†æ¶ï¼Œæ—¨åœ¨ç»“åˆæ•°å­¦è‚¿ç˜¤ç”Ÿé•¿æ¨¡å‹ä¸å¼•å¯¼å¼å»å™ªæ‰©æ•£éšå¼æ¨¡å‹(DDIM)æ¥é¢„æµ‹è„‘è‚¿ç˜¤çš„æ—¶ç©ºæ¼”å˜å¹¶åˆæˆè§£å‰–å­¦å¯è¡Œçš„æœªæ¥MRIå›¾åƒã€‚æ¡†æ¶åˆ©ç”¨å¸¸å¾®åˆ†æ–¹ç¨‹(ODEs)æ„å»ºçš„æœºæ¢°æ¨¡å‹æ•æ‰åŒ…å«æ”¾ç–—æ•ˆæœåœ¨å†…çš„è‚¿ç˜¤æ—¶é—´åŠ¨æ€ï¼Œå¹¶ä»¥ä¼°è®¡çš„è‚¿ç˜¤è´Ÿè·ä½œä¸ºæ¢¯åº¦å¼•å¯¼æ¡ä»¶ï¼Œç¡®ä¿åˆæˆå›¾åƒä¸é¢„æµ‹ç”Ÿé•¿åŠæ‚£è€…è§£å‰–ç»“æ„ä¿æŒä¸€è‡´ã€‚å®éªŒåœ¨BraTSæ•°æ®é›†åŠå†…éƒ¨çºµå‘å„¿ç§‘å¼¥æ¼«æ€§ä¸­çº¿èƒ¶è´¨ç˜¤(DMG)ç—…ä¾‹ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•èƒ½ç”Ÿæˆé«˜åº¦é€¼çœŸçš„éšè®¿æ‰«æå›¾åƒã€‚ç ”ç©¶è¿˜å¼•å…¥äº†è‚¿ç˜¤ç”Ÿé•¿æ¦‚ç‡å›¾ï¼Œé€šè¿‡95ç™¾åˆ†ä½è±ªæ–¯å¤šå¤«è·ç¦»(Hausdorff Distance)è¯æ˜äº†å…¶åœ¨æ•æ‰è‚¿ç˜¤ç”Ÿé•¿èŒƒå›´å’Œæ–¹å‘æ€§æ–¹é¢çš„å‡†ç¡®æ€§ã€‚è¿™ç§ç»“åˆæœºæ¢°å…ˆéªŒ(mechanistic priors)çš„æ–¹æ³•ä¸ºæ•°æ®å—é™åœºæ™¯ä¸‹çš„ç”Ÿç‰©ä¿¡æ¯é©±åŠ¨å›¾åƒç”Ÿæˆå’Œæ—¶ç©ºé¢„æµ‹æä¾›äº†æœ‰æ•ˆé€”å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "13 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.09610v1",
      "published_date": "2025-09-11 16:52:09 UTC",
      "updated_date": "2025-09-11 16:52:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:54:31.893650+00:00"
    },
    {
      "arxiv_id": "2509.09754v1",
      "title": "LAVa: Layer-wise KV Cache Eviction with Dynamic Budget Allocation",
      "title_zh": "LAVaï¼šåŸºäºåŠ¨æ€é¢„ç®—åˆ†é…çš„é€å±‚ KV Cache é©±é€",
      "authors": [
        "Yiqun Shen",
        "Song Yuan",
        "Zhengze Zhang",
        "Xiaoliang Wang",
        "Daxin Jiang",
        "Nguyen Cam-Tu"
      ],
      "abstract": "KV Cache is commonly used to accelerate LLM inference with long contexts, yet its high memory demand drives the need for cache compression. Existing compression methods, however, are largely heuristic and lack dynamic budget allocation. To address this limitation, we introduce a unified framework for cache compression by minimizing information loss in Transformer residual streams. Building on it, we analyze the layer attention output loss and derive a new metric to compare cache entries across heads, enabling layer-wise compression with dynamic head budgets. Additionally, by contrasting cross-layer information, we also achieve dynamic layer budgets. LAVa is the first unified strategy for cache eviction and dynamic budget allocation that, unlike prior methods, does not rely on training or the combination of multiple strategies. Experiments with benchmarks (LongBench, Needle-In-A-Haystack, Ruler, and InfiniteBench) demonstrate its superiority. Moreover, our experiments reveal a new insight: dynamic layer budgets are crucial for generation tasks (e.g., code completion), while dynamic head budgets play a key role in extraction tasks (e.g., extractive QA). As a fully dynamic compression method, LAVa consistently maintains top performance across task types. Our code is available at https://github.com/MGDDestiny/Lava.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LAVaï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡åŠ¨æ€é¢„ç®—åˆ†é…è¿›è¡Œé€å±‚ KV Cache å‰”é™¤çš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹é•¿æ–‡æœ¬æ¨ç†ä¸­çš„é«˜å†…å­˜æ¶ˆè€—é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡æœ€å°åŒ– Transformer æ®‹å·®æµï¼ˆresidual streamsï¼‰ä¸­çš„ä¿¡æ¯æŸå¤±æ¥ä¼˜åŒ–ç¼“å­˜å‹ç¼©ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°æŒ‡æ ‡æ¥è¡¡é‡ä¸åŒæ³¨æ„åŠ›å¤´ï¼ˆheadsï¼‰ä¹‹é—´çš„ç¼“å­˜é¡¹ï¼Œä»è€Œå®ç°å…·æœ‰åŠ¨æ€ Head é¢„ç®—çš„é€å±‚å‹ç¼©ã€‚åŒæ—¶ï¼ŒLAVa é€šè¿‡å¯¹æ¯”è·¨å±‚ä¿¡æ¯å®ç°äº†åŠ¨æ€å±‚é¢„ç®—ï¼ˆdynamic layer budgetsï¼‰ï¼Œä¸”æ— éœ€é¢å¤–è®­ç»ƒæˆ–å¤æ‚çš„ç­–ç•¥ç»„åˆã€‚åœ¨ LongBenchã€Needle-In-A-Haystack å’Œ InfiniteBench ç­‰åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯æ˜äº†å…¶æ˜¾è‘—çš„æ€§èƒ½ä¼˜åŠ¿ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº†ä¸åŒä»»åŠ¡çš„ç‰¹æ€§ï¼šåŠ¨æ€å±‚é¢„ç®—å¯¹äºç”Ÿæˆä»»åŠ¡ï¼ˆå¦‚ä»£ç è¡¥å…¨ï¼‰è‡³å…³é‡è¦ï¼Œè€ŒåŠ¨æ€ Head é¢„ç®—åˆ™åœ¨æŠ½å–å¼ä»»åŠ¡ï¼ˆå¦‚æŠ½å–å¼ QAï¼‰ä¸­å‘æŒ¥å…³é”®ä½œç”¨ã€‚ä½œä¸ºä¸€ç§å…¨åŠ¨æ€å‹ç¼©æ–¹æ³•ï¼ŒLAVa åœ¨å„ç§ä»»åŠ¡ç±»å‹ä¸‹å‡èƒ½æŒç»­ä¿æŒé¡¶å°–çš„æ€§èƒ½è¡¨ç°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.09754v1",
      "published_date": "2025-09-11 16:48:24 UTC",
      "updated_date": "2025-09-11 16:48:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:54:40.592074+00:00"
    },
    {
      "arxiv_id": "2509.09597v2",
      "title": "Graph Alignment via Dual-Pass Spectral Encoding and Latent Space Communication",
      "title_zh": "åŸºäºåŒé€šè°±ç¼–ç ä¸æ½œç©ºé—´é€šä¿¡çš„å›¾å¯¹é½",
      "authors": [
        "Maysam Behmanesh",
        "Erkan Turan",
        "Maks Ovsjanikov"
      ],
      "abstract": "Graph alignment, the problem of identifying corresponding nodes across multiple graphs, is fundamental to numerous applications. Most existing unsupervised methods embed node features into latent representations to enable cross-graph comparison without ground-truth correspondences. However, these methods suffer from two critical limitations: the degradation of node distinctiveness due to oversmoothing in GNN-based embeddings, and the misalignment of latent spaces across graphs caused by structural noise, feature heterogeneity, and training instability, ultimately leading to unreliable node correspondences. We propose a novel graph alignment framework that simultaneously enhances node distinctiveness and enforces geometric consistency across latent spaces. Our approach introduces a dual-pass encoder that combines low-pass and high-pass spectral filters to generate embeddings that are both structure-aware and highly discriminative. To address latent space misalignment, we incorporate a geometry-aware functional map module that learns bijective and isometric transformations between graph embeddings, ensuring consistent geometric relationships across different representations. Extensive experiments on graph benchmarks demonstrate that our method consistently outperforms existing unsupervised alignment baselines, exhibiting superior robustness to structural inconsistencies and challenging alignment scenarios. Additionally, comprehensive evaluation on vision-language benchmarks using diverse pretrained models shows that our framework effectively generalizes beyond graph domains, enabling unsupervised alignment of vision and language representations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å›¾å¯¹é½(Graph Alignment)ä¸­èŠ‚ç‚¹ç‰¹å¾è¿‡åº¦å¹³æ»‘(Oversmoothing)ä»¥åŠç”±äºç»“æ„å™ªå£°å’Œç‰¹å¾å¼‚æ„å¯¼è‡´çš„æ½œç©ºé—´(Latent Space)é”™ä½é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å…¨æ–°çš„å›¾å¯¹é½æ¡†æ¶ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†åŒé€šé“ç¼–ç å™¨(Dual-Pass Encoder)ï¼Œé€šè¿‡ç»“åˆä½é€š(Low-pass)å’Œé«˜é€š(High-pass)è°±æ»¤æ³¢å™¨(Spectral Filters)ç”Ÿæˆå…¼å…·ç»“æ„æ„ŸçŸ¥èƒ½åŠ›å’Œé«˜åŒºåˆ†åº¦çš„èŠ‚ç‚¹åµŒå…¥ã€‚ä¸ºäº†è§£å†³æ½œç©ºé—´å¯¹é½ä¸å¯é çš„é—®é¢˜ï¼Œç ”ç©¶å¼•å…¥äº†å‡ ä½•æ„ŸçŸ¥çš„Functional Mapæ¨¡å—ï¼Œé€šè¿‡å­¦ä¹ å›¾åµŒå…¥ä¹‹é—´çš„åŒå°„(Bijective)å’Œç­‰è·(Isometric)å˜æ¢ï¼Œç¡®ä¿ä¸åŒè¡¨ç¤ºä¹‹é—´çš„å‡ ä½•å…³ç³»ä¿æŒä¸€è‡´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªå›¾åŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºç°æœ‰çš„æ— ç›‘ç£å¯¹é½åŸºå‡†æ¨¡å‹ï¼Œå±•ç°å‡ºå¯¹æŒ‘æˆ˜æ€§åœºæ™¯æ›´å¼ºçš„é²æ£’æ€§ã€‚æ­¤å¤–ï¼Œåœ¨è§†è§‰è¯­è¨€(Vision-Language)åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶èƒ½æœ‰æ•ˆæ³›åŒ–è‡³éå›¾é¢†åŸŸï¼Œå®ç°äº†ä¸åŒæ¨¡æ€è¡¨ç¤ºä¹‹é—´çš„æ— ç›‘ç£å¯¹é½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "23 pages",
      "pdf_url": "https://arxiv.org/pdf/2509.09597v2",
      "published_date": "2025-09-11 16:36:16 UTC",
      "updated_date": "2025-09-27 09:11:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:54:43.655807+00:00"
    },
    {
      "arxiv_id": "2509.09594v1",
      "title": "ObjectReact: Learning Object-Relative Control for Visual Navigation",
      "title_zh": "ObjectReactï¼šé¢å‘è§†è§‰å¯¼èˆªçš„ç‰©ä½“ç›¸å¯¹æ§åˆ¶å­¦ä¹ ",
      "authors": [
        "Sourav Garg",
        "Dustin Craggs",
        "Vineeth Bhat",
        "Lachlan Mares",
        "Stefan Podgorski",
        "Madhava Krishna",
        "Feras Dayoub",
        "Ian Reid"
      ],
      "abstract": "Visual navigation using only a single camera and a topological map has recently become an appealing alternative to methods that require additional sensors and 3D maps. This is typically achieved through an \"image-relative\" approach to estimating control from a given pair of current observation and subgoal image. However, image-level representations of the world have limitations because images are strictly tied to the agent's pose and embodiment. In contrast, objects, being a property of the map, offer an embodiment- and trajectory-invariant world representation. In this work, we present a new paradigm of learning \"object-relative\" control that exhibits several desirable characteristics: a) new routes can be traversed without strictly requiring to imitate prior experience, b) the control prediction problem can be decoupled from solving the image matching problem, and c) high invariance can be achieved in cross-embodiment deployment for variations across both training-testing and mapping-execution settings. We propose a topometric map representation in the form of a \"relative\" 3D scene graph, which is used to obtain more informative object-level global path planning costs. We train a local controller, dubbed \"ObjectReact\", conditioned directly on a high-level \"WayObject Costmap\" representation that eliminates the need for an explicit RGB input. We demonstrate the advantages of learning object-relative control over its image-relative counterpart across sensor height variations and multiple navigation tasks that challenge the underlying spatial understanding capability, e.g., navigating a map trajectory in the reverse direction. We further show that our sim-only policy is able to generalize well to real-world indoor environments. Code and supplementary material are accessible via project page: https://object-react.github.io/",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ObjectReactï¼Œä¸€ç§å…¨æ–°çš„å­¦ä¹ â€œObject-Relativeâ€æ§åˆ¶èŒƒå¼ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿè§†è§‰å¯¼èˆªä¸­â€œImage-Relativeâ€æ–¹æ³•å› å›¾åƒæ·±åº¦ä¾èµ–æ™ºèƒ½ä½“å§¿æ€å’Œä½å§¿ï¼ˆEmbodimentï¼‰è€Œå—é™çš„é—®é¢˜ã€‚ç ”ç©¶å¼•å…¥äº†ä»¥â€œRelativeâ€ 3D Scene Graphå½¢å¼å‘ˆç°çš„æ‹“æ‰‘åº¦é‡åœ°å›¾è¡¨å¾ï¼Œç”¨äºè·å–æ›´å…·ä¿¡æ¯é‡çš„ç‰©ä½“çº§å…¨å±€è·¯å¾„è§„åˆ’æˆæœ¬ã€‚å…¶æ ¸å¿ƒå±€éƒ¨æ§åˆ¶å™¨ObjectReactç›´æ¥ä»¥é«˜å±‚çº§çš„â€œWayObject Costmapâ€ä¸ºæ¡ä»¶è¿›è¡ŒåŠ¨ä½œé¢„æµ‹ï¼Œæœ‰æ•ˆæ¶ˆé™¤äº†å¯¹æ˜¾å¼RGBå›¾åƒè¾“å…¥çš„ä¾èµ–ã€‚è¿™ç§æ–¹æ³•å®ç°äº†æ§åˆ¶é¢„æµ‹ä¸å›¾åƒåŒ¹é…é—®é¢˜çš„è§£è€¦ï¼Œä½¿å¾—æ™ºèƒ½ä½“åœ¨æ— éœ€ä¸¥æ ¼æ¨¡ä»¿å…ˆéªŒç»éªŒçš„æƒ…å†µä¸‹å³å¯éå†æ–°è·¯çº¿ã€‚å®éªŒè¯æ˜ï¼ŒObjectReactåœ¨è·¨ä½å§¿éƒ¨ç½²ï¼ˆå¦‚ä¼ æ„Ÿå™¨é«˜åº¦å˜åŒ–ï¼‰å’Œå¤æ‚çš„ç©ºé—´ç†è§£ä»»åŠ¡ï¼ˆå¦‚é€†å‘å¯¼èˆªï¼‰ä¸­å±•ç°å‡ºæ¯”ä¼ ç»Ÿæ–¹æ³•æ›´é«˜çš„ä¸å˜æ€§ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶æ‰€é‡‡ç”¨çš„ä»…åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­è®­ç»ƒçš„ç­–ç•¥ï¼Œå·²æˆåŠŸè¯æ˜èƒ½å¤Ÿå¾ˆå¥½åœ°æ³›åŒ–è‡³çœŸå®çš„å®¤å†…ç¯å¢ƒã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "CoRL 2025; 23 pages including appendix",
      "pdf_url": "https://arxiv.org/pdf/2509.09594v1",
      "published_date": "2025-09-11 16:34:17 UTC",
      "updated_date": "2025-09-11 16:34:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:54:42.472793+00:00"
    },
    {
      "arxiv_id": "2509.09593v1",
      "title": "Fluent but Unfeeling: The Emotional Blind Spots of Language Models",
      "title_zh": "æµåˆ©å´æ— æ„Ÿï¼šè¯­è¨€æ¨¡å‹çš„æƒ…æ„Ÿç›²ç‚¹",
      "authors": [
        "Bangzhao Shu",
        "Isha Joshi",
        "Melissa Karnaze",
        "Anh C. Pham",
        "Ishita Kakkar",
        "Sindhu Kothe",
        "Arpine Hovasapian",
        "Mai ElSherief"
      ],
      "abstract": "The versatility of Large Language Models (LLMs) in natural language understanding has made them increasingly popular in mental health research. While many studies explore LLMs' capabilities in emotion recognition, a critical gap remains in evaluating whether LLMs align with human emotions at a fine-grained level. Existing research typically focuses on classifying emotions into predefined, limited categories, overlooking more nuanced expressions. To address this gap, we introduce EXPRESS, a benchmark dataset curated from Reddit communities featuring 251 fine-grained, self-disclosed emotion labels. Our comprehensive evaluation framework examines predicted emotion terms and decomposes them into eight basic emotions using established emotion theories, enabling a fine-grained comparison. Systematic testing of prevalent LLMs under various prompt settings reveals that accurately predicting emotions that align with human self-disclosed emotions remains challenging. Qualitative analysis further shows that while certain LLMs generate emotion terms consistent with established emotion theories and definitions, they sometimes fail to capture contextual cues as effectively as human self-disclosures. These findings highlight the limitations of LLMs in fine-grained emotion alignment and offer insights for future research aimed at enhancing their contextual understanding.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¿ƒç†å¥åº·ç ”ç©¶ä¸­çš„åº”ç”¨ï¼ŒæŒ‡å‡ºå…¶åœ¨ç»†ç²’åº¦(fine-grained)æƒ…æ„Ÿå¯¹é½(emotion alignment)æ–¹é¢ä»å­˜åœ¨æ˜æ˜¾çŸ­æ¿ã€‚ä¸ºå¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œç ”ç©¶è€…æå‡ºäº†åä¸ºEXPRESSçš„åŸºå‡†æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«ä»Redditç¤¾åŒºè·å–çš„251ä¸ªç»†ç²’åº¦è‡ªè¿°æƒ…æ„Ÿæ ‡ç­¾ã€‚é€šè¿‡å»ºç«‹ä¸€ä¸ªç»¼åˆè¯„ä¼°æ¡†æ¶ï¼Œç ”ç©¶å°†é¢„æµ‹çš„æƒ…æ„Ÿæœ¯è¯­æ ¹æ®æ—¢æœ‰ç†è®ºåˆ†è§£ä¸ºå…«ç§åŸºæœ¬æƒ…æ„Ÿï¼Œä»¥å®ç°æ›´æ·±å±‚æ¬¡çš„å¯¹æ¯”åˆ†æã€‚ç³»ç»Ÿæµ‹è¯•ç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„ä¸»æµLLMsåœ¨å‡†ç¡®é¢„æµ‹ä¸äººç±»è‡ªè¿°ä¸€è‡´çš„æƒ…æ„Ÿæ–¹é¢ä¾ç„¶é¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚å®šæ€§åˆ†æè¿›ä¸€æ­¥æ­ç¤ºï¼Œè™½ç„¶LLMsç”Ÿæˆçš„æœ¯è¯­åœ¨ç†è®ºå®šä¹‰ä¸Šå…·æœ‰ä¸€è‡´æ€§ï¼Œä½†åœ¨æ•æ‰ä¸Šä¸‹æ–‡çº¿ç´¢(contextual cues)çš„æœ‰æ•ˆæ€§ä¸Šä»é€Šè‰²äºäººç±»çš„çœŸå®è¡¨è¾¾ã€‚è¿™äº›å‘ç°å‡¸æ˜¾äº†LLMsåœ¨ç»†ç²’åº¦æƒ…æ„Ÿç†è§£ä¸Šçš„å±€é™æ€§ï¼Œå¹¶ä¸ºæœªæ¥æå‡æ¨¡å‹åœ¨å¿ƒç†å¥åº·é¢†åŸŸçš„æƒ…æ„Ÿæ„ŸçŸ¥èƒ½åŠ›æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Camera-ready version for ICWSM 2026. First two authors contributed equally",
      "pdf_url": "https://arxiv.org/pdf/2509.09593v1",
      "published_date": "2025-09-11 16:31:13 UTC",
      "updated_date": "2025-09-11 16:31:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:54:49.984606+00:00"
    },
    {
      "arxiv_id": "2509.09560v1",
      "title": "Boosting Embodied AI Agents through Perception-Generation Disaggregation and Asynchronous Pipeline Execution",
      "title_zh": "é€šè¿‡æ„ŸçŸ¥-ç”Ÿæˆè§£è€¦ä¸å¼‚æ­¥æµæ°´çº¿æ‰§è¡Œæå‡å…·èº«æ™ºèƒ½ä½“æ€§èƒ½",
      "authors": [
        "Shulai Zhang",
        "Ao Xu",
        "Quan Chen",
        "Han Zhao",
        "Weihao Cui",
        "Ningxin Zheng",
        "Haibin Lin",
        "Xin Liu",
        "Minyi Guo"
      ],
      "abstract": "Embodied AI systems operate in dynamic environments, requiring seamless integration of perception and generation modules to process high-frequency input and output demands. Traditional sequential computation patterns, while effective in ensuring accuracy, face significant limitations in achieving the necessary \"thinking\" frequency for real-world applications. In this work, we present Auras, an algorithm-system co-designed inference framework to optimize the inference frequency of embodied AI agents. Auras disaggregates the perception and generation and provides controlled pipeline parallelism for them to achieve high and stable throughput. Faced with the data staleness problem that appears when the parallelism is increased, Auras establishes a public context for perception and generation to share, thereby promising the accuracy of embodied agents. Experimental results show that Auras improves throughput by 2.54x on average while achieving 102.7% of the original accuracy, demonstrating its efficacy in overcoming the constraints of sequential computation and providing high throughput.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Aurasï¼Œä¸€ä¸ªç®—æ³•ä¸ç³»ç»ŸååŒè®¾è®¡çš„æ¨ç†æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å…·èº«æ™ºèƒ½(Embodied AI)ç³»ç»Ÿåœ¨åŠ¨æ€ç¯å¢ƒä¸­å› ä¼ ç»Ÿä¸²è¡Œè®¡ç®—æ¨¡å¼å¯¼è‡´çš„æ¨ç†é¢‘ç‡å—é™é—®é¢˜ã€‚Auras é€šè¿‡å°†æ„ŸçŸ¥(Perception)ä¸ç”Ÿæˆ(Generation)è§£è€¦ï¼Œå¹¶å¼•å…¥å—æ§çš„æµæ°´çº¿å¹¶è¡Œ(Pipeline Parallelism)æŠ€æœ¯ï¼Œæ˜¾è‘—æå‡äº†ç³»ç»Ÿçš„ååé‡ã€‚é’ˆå¯¹å¹¶è¡ŒåŒ–è¿‡ç¨‹ä¸­å‡ºç°çš„æ•°æ®é™ˆæ—§(Data Staleness)é—®é¢˜ï¼Œè¯¥æ¡†æ¶å»ºç«‹äº†ä¸€ä¸ªå¯ä¾›æ„ŸçŸ¥ä¸ç”Ÿæˆæ¨¡å—å…±äº«çš„å…¬å…±ä¸Šä¸‹æ–‡(Public Context)ï¼Œä»è€Œåœ¨æå‡æ•ˆç‡çš„åŒæ—¶ç¡®ä¿äº†æ™ºèƒ½ä»£ç†çš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAuras ä½¿ååé‡å¹³å‡æå‡äº† 2.54 å€ï¼Œå¹¶è¾¾åˆ°äº†åŸå§‹å‡†ç¡®ç‡çš„ 102.7%ã€‚è¯¥ç ”ç©¶æœ‰æ•ˆå…‹æœäº†ä¸²è¡Œè®¡ç®—çš„æ€§èƒ½ç“¶é¢ˆï¼Œä¸ºå®ç°é«˜é¢‘ç‡ã€é«˜å‡†ç¡®åº¦çš„å…·èº«æ™ºèƒ½ç³»ç»Ÿæ¨ç†æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.09560v1",
      "published_date": "2025-09-11 15:51:43 UTC",
      "updated_date": "2025-09-11 15:51:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:54:52.573915+00:00"
    },
    {
      "arxiv_id": "2509.09558v1",
      "title": "Invisible Attributes, Visible Biases: Exploring Demographic Shortcuts in MRI-based Alzheimer's Disease Classification",
      "title_zh": "éšæ€§å±æ€§ï¼Œæ˜¾æ€§åè§ï¼šæ¢ç©¶åŸºäº MRI çš„é˜¿å°”èŒ¨æµ·é»˜ç—…åˆ†ç±»ä¸­çš„äººå£ç»Ÿè®¡å­¦æ·å¾„",
      "authors": [
        "Akshit Achara",
        "Esther Puyol Anton",
        "Alexander Hammers",
        "Andrew P. King"
      ],
      "abstract": "Magnetic resonance imaging (MRI) is the gold standard for brain imaging. Deep learning (DL) algorithms have been proposed to aid in the diagnosis of diseases such as Alzheimer's disease (AD) from MRI scans. However, DL algorithms can suffer from shortcut learning, in which spurious features, not directly related to the output label, are used for prediction. When these features are related to protected attributes, they can lead to performance bias against underrepresented protected groups, such as those defined by race and sex. In this work, we explore the potential for shortcut learning and demographic bias in DL based AD diagnosis from MRI. We first investigate if DL algorithms can identify race or sex from 3D brain MRI scans to establish the presence or otherwise of race and sex based distributional shifts. Next, we investigate whether training set imbalance by race or sex can cause a drop in model performance, indicating shortcut learning and bias. Finally, we conduct a quantitative and qualitative analysis of feature attributions in different brain regions for both the protected attribute and AD classification tasks. Through these experiments, and using multiple datasets and DL models (ResNet and SwinTransformer), we demonstrate the existence of both race and sex based shortcut learning and bias in DL based AD classification. Our work lays the foundation for fairer DL diagnostic tools in brain MRI. The code is provided at https://github.com/acharaakshit/ShortMR",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŸºäºç£å…±æŒ¯æˆåƒ (MRI) çš„é˜¿å°”èŒ¨æµ·é»˜ç—… (AD) æ·±åº¦å­¦ä¹  (DL) è¯Šæ–­ç®—æ³•ä¸­å­˜åœ¨çš„å¿«æ·å­¦ä¹  (Shortcut Learning) å’Œäººå£ç»Ÿè®¡å­¦åè§é—®é¢˜ã€‚ç ”ç©¶é¦–å…ˆéªŒè¯äº† DL ç®—æ³•èƒ½å¤Ÿä» 3D è„‘éƒ¨ MRI æ‰«æä¸­è¯†åˆ«å‡ºç§æ— (Race) æˆ–æ€§åˆ« (Sex)ï¼Œä»è€Œç¡®è®¤äº†äººå£ç»Ÿè®¡ç‰¹å¾ç›¸å…³çš„åˆ†å¸ƒåç§»ã€‚éšåï¼Œä½œè€…é€šè¿‡å®éªŒè¯æ˜ï¼Œè®­ç»ƒé›†ä¸­ç§æ—æˆ–æ€§åˆ«çš„ä¸å¹³è¡¡ä¼šå¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™ï¼Œè¿™æ­ç¤ºäº†æ¨¡å‹åœ¨é¢„æµ‹è¿‡ç¨‹ä¸­åˆ©ç”¨äº†ä¸ç–¾ç—…æ— å…³çš„å—ä¿æŠ¤å±æ€§ä½œä¸ºæ·å¾„ã€‚ç ”ç©¶åˆ©ç”¨ ResNet å’Œ SwinTransformer ç­‰æ¨¡å‹ï¼Œå¯¹ä¸åŒè„‘åŒºçš„ç‰¹å¾å½’å›  (Feature Attribution) è¿›è¡Œäº†å®šé‡å’Œå®šæ€§åˆ†æï¼Œè¿›ä¸€æ­¥è¯å®äº†åœ¨ AD åˆ†ç±»ä»»åŠ¡ä¸­ç¡®å®å­˜åœ¨åŸºäºç§æ—å’Œæ€§åˆ«çš„å¿«æ·å­¦ä¹ å’Œç®—æ³•åè§ã€‚è¿™é¡¹å·¥ä½œæ­ç¤ºäº†æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨åŒ»å­¦å½±åƒè¯Šæ–­ä¸­çš„å…¬å¹³æ€§é£é™©ï¼Œä¸ºå¼€å‘æ›´å…¬å¹³çš„è„‘éƒ¨ MRI è¾…åŠ©è¯Šæ–­å·¥å…·å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "FAIMI @ MICCAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.09558v1",
      "published_date": "2025-09-11 15:48:30 UTC",
      "updated_date": "2025-09-11 15:48:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:55:09.264588+00:00"
    },
    {
      "arxiv_id": "2509.09552v1",
      "title": "An improved educational competition optimizer with multi-covariance learning operators for global optimization problems",
      "title_zh": "ä¸€ç§ç”¨äºå…¨å±€ä¼˜åŒ–é—®é¢˜çš„èåˆå¤šåæ–¹å·®å­¦ä¹ ç®—å­çš„æ”¹è¿›å‹æ•™è‚²ç«äº‰ä¼˜åŒ–ç®—æ³•",
      "authors": [
        "Baoqi Zhao",
        "Xiong Yang",
        "Hoileong Lee",
        "Bowen Dong"
      ],
      "abstract": "The educational competition optimizer is a recently introduced metaheuristic algorithm inspired by human behavior, originating from the dynamics of educational competition within society. Nonetheless, ECO faces constraints due to an imbalance between exploitation and exploration, rendering it susceptible to local optima and demonstrating restricted effectiveness in addressing complex optimization problems. To address these limitations, this study presents an enhanced educational competition optimizer (IECO-MCO) utilizing multi-covariance learning operators. In IECO, three distinct covariance learning operators are introduced to improve the performance of ECO. Each operator effectively balances exploitation and exploration while preventing premature convergence of the population. The effectiveness of IECO is assessed through benchmark functions derived from the CEC 2017 and CEC 2022 test suites, and its performance is compared with various basic and improved algorithms across different categories. The results demonstrate that IECO-MCO surpasses the basic ECO and other competing algorithms in convergence speed, stability, and the capability to avoid local optima. Furthermore, statistical analyses, including the Friedman test, Kruskal-Wallis test, and Wilcoxon rank-sum test, are conducted to validate the superiority of IECO-MCO over the compared algorithms. Compared with the basic algorithm (improved algorithm), IECO-MCO achieved an average ranking of 2.213 (2.488) on the CE2017 and CEC2022 test suites. Additionally, the practical applicability of the proposed IECO-MCO algorithm is verified by solving constrained optimization problems. The experimental outcomes demonstrate the superior performance of IECO-MCO in tackling intricate optimization problems, underscoring its robustness and practical effectiveness in real-world scenarios.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºIECO-MCOçš„æ”¹è¿›å‹æ•™è‚²ç«äº‰ä¼˜åŒ–ç®—æ³•ï¼Œæ—¨åœ¨è§£å†³åŸå§‹æ•™è‚²ç«äº‰ä¼˜åŒ–ç®—æ³•(ECO)åœ¨å¤„ç†å¤æ‚ä¼˜åŒ–é—®é¢˜æ—¶å­˜åœ¨çš„å¼€å‘ä¸æ¢ç´¢å¤±è¡¡åŠæ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜ç­‰å±€é™æ€§ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¸‰ç§ç‹¬ç‰¹çš„å¤šåæ–¹å·®å­¦ä¹ ç®—å­(multi-covariance learning operators)ï¼Œé€šè¿‡æœ‰æ•ˆå¹³è¡¡ç®—æ³•çš„å…¨å±€æœç´¢ä¸å±€éƒ¨æ”¹è‰¯èƒ½åŠ›ï¼Œæ˜¾è‘—é˜²æ­¢äº†ç§ç¾¤çš„è¿‡æ—©æ”¶æ•›ã€‚ç ”ç©¶åˆ©ç”¨CEC 2017å’ŒCEC 2022åŸºå‡†æµ‹è¯•å‡½æ•°å¯¹ç®—æ³•è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶ç»“åˆFriedmanæ£€éªŒã€Kruskal-Wallisæ£€éªŒå’ŒWilcoxonç§©å’Œæ£€éªŒç­‰ç»Ÿè®¡åˆ†æï¼Œç»“æœè¯æ˜IECO-MCOåœ¨æ”¶æ•›é€Ÿåº¦å’Œç¨³å®šæ€§ä¸Šå‡ä¼˜äºç°æœ‰çš„åŸºç¡€åŠæ”¹è¿›ç®—æ³•ã€‚æ­¤å¤–ï¼Œé€šè¿‡è§£å†³å®é™…çš„çº¦æŸä¼˜åŒ–é—®é¢˜ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†IECO-MCOåœ¨å¤„ç†å¤æ‚ç°å®ä»»åŠ¡ä¸­çš„é²æ£’æ€§ä¸å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.NE",
      "comment": "Submitted to Cluster Computing",
      "pdf_url": "https://arxiv.org/pdf/2509.09552v1",
      "published_date": "2025-09-11 15:41:14 UTC",
      "updated_date": "2025-09-11 15:41:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:55:15.794923+00:00"
    },
    {
      "arxiv_id": "2509.09547v1",
      "title": "Improving Video Diffusion Transformer Training by Multi-Feature Fusion and Alignment from Self-Supervised Vision Encoders",
      "title_zh": "é€šè¿‡è‡ªç›‘ç£è§†è§‰ç¼–ç å™¨çš„å¤šç‰¹å¾èåˆä¸å¯¹é½æå‡è§†é¢‘æ‰©æ•£ Transformer è®­ç»ƒ",
      "authors": [
        "Dohun Lee",
        "Hyeonho Jeong",
        "Jiwook Kim",
        "Duygu Ceylan",
        "Jong Chul Ye"
      ],
      "abstract": "Video diffusion models have advanced rapidly in the recent years as a result of series of architectural innovations (e.g., diffusion transformers) and use of novel training objectives (e.g., flow matching). In contrast, less attention has been paid to improving the feature representation power of such models. In this work, we show that training video diffusion models can benefit from aligning the intermediate features of the video generator with feature representations of pre-trained vision encoders. We propose a new metric and conduct an in-depth analysis of various vision encoders to evaluate their discriminability and temporal consistency, thereby assessing their suitability for video feature alignment. Based on the analysis, we present Align4Gen which provides a novel multi-feature fusion and alignment method integrated into video diffusion model training. We evaluate Align4Gen both for unconditional and class-conditional video generation tasks and show that it results in improved video generation as quantified by various metrics. Full video results are available on our project page: https://align4gen.github.io/align4gen/",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Align4Genï¼Œä¸€ç§é€šè¿‡è‡ªç›‘ç£è§†è§‰ç¼–ç å™¨çš„å¤šç‰¹å¾èåˆä¸å¯¹é½æ¥æ”¹è¿›Video Diffusion Transformerè®­ç»ƒçš„æ–°æ–¹æ³•ã€‚é’ˆå¯¹å½“å‰è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›æ–¹é¢çš„ä¸è¶³ï¼ŒAlign4Genå°†ç”Ÿæˆå™¨çš„ä¸­é—´ç‰¹å¾ä¸é¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨çš„è¡¨ç¤ºè¿›è¡Œå¯¹é½ã€‚ç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†ä¸€ç§æ–°æŒ‡æ ‡ï¼Œæ·±å…¥åˆ†æäº†å¤šç§è§†è§‰ç¼–ç å™¨çš„è¾¨åˆ«åŠ›(discriminability)å’Œæ—¶é—´ä¸€è‡´æ€§(temporal consistency)ï¼Œä»¥è¯„ä¼°å…¶å¯¹è§†é¢‘ç‰¹å¾å¯¹é½çš„é€‚ç”¨æ€§ã€‚è¯¥æ–¹æ³•å°†å¤šç‰¹å¾èåˆä¸å¯¹é½æŠ€æœ¯é›†æˆåˆ°æ¨¡å‹è®­ç»ƒä¸­ï¼Œå¹¶åœ¨æ— æ¡ä»¶(unconditional)å’Œç±»æ¡ä»¶(class-conditional)è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­è¿›è¡Œäº†éªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAlign4Genåœ¨å¤šé¡¹é‡åŒ–æŒ‡æ ‡ä¸Šå‡æ˜¾è‘—æå‡äº†è§†é¢‘ç”Ÿæˆçš„è´¨é‡ï¼Œè¯æ˜äº†å¼•å…¥é¢„è®­ç»ƒè§†è§‰ç‰¹å¾å¼•å¯¼è®­ç»ƒçš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "17 pages, 14 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.09547v1",
      "published_date": "2025-09-11 15:39:27 UTC",
      "updated_date": "2025-09-11 15:39:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:56:16.199050+00:00"
    },
    {
      "arxiv_id": "2509.09541v1",
      "title": "Compositional Concept Generalization with Variational Quantum Circuits",
      "title_zh": "åŸºäºå˜åˆ†é‡å­ç”µè·¯çš„ç»„åˆå¼æ¦‚å¿µæ³›åŒ–",
      "authors": [
        "Hala Hawashin",
        "Mina Abbaszadeh",
        "Nicholas Joseph",
        "Beth Pearson",
        "Martha Lewis",
        "Mehrnoosh sadrzadeh"
      ],
      "abstract": "Compositional generalization is a key facet of human cognition, but lacking in current AI tools such as vision-language models. Previous work examined whether a compositional tensor-based sentence semantics can overcome the challenge, but led to negative results. We conjecture that the increased training efficiency of quantum models will improve performance in these tasks. We interpret the representations of compositional tensor-based models in Hilbert spaces and train Variational Quantum Circuits to learn these representations on an image captioning task requiring compositional generalization. We used two image encoding techniques: a multi-hot encoding (MHE) on binary image vectors and an angle/amplitude encoding on image vectors taken from the vision-language model CLIP. We achieve good proof-of-concept results using noisy MHE encodings. Performance on CLIP image vectors was more mixed, but still outperformed classical compositional models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç»„åˆæ³›åŒ–(Compositional Generalization)æ–¹é¢çš„ä¸è¶³ï¼Œæå‡ºäº†åˆ©ç”¨å˜åˆ†é‡å­ç”µè·¯(Variational Quantum Circuits)æ¥æå‡æ¨¡å‹æ€§èƒ½çš„æ–¹æ¡ˆã€‚ä½œè€…å°†åŸºäºå¼ é‡çš„ç»„åˆæ¨¡å‹è¡¨ç¤ºæ˜ å°„åˆ°å¸Œå°”ä¼¯ç‰¹ç©ºé—´(Hilbert spaces)ï¼Œå¹¶åœ¨éœ€è¦ç»„åˆæ³›åŒ–çš„å›¾åƒæ ‡æ³¨ä»»åŠ¡ä¸­å¯¹é‡å­ç”µè·¯è¿›è¡Œè®­ç»ƒã€‚å®éªŒæµ‹è¯•äº†é’ˆå¯¹äºŒè¿›åˆ¶å›¾åƒå‘é‡çš„å¤šçƒ­ç¼–ç (Multi-hot encoding)ä»¥åŠåŸºäºCLIPè§†è§‰è¯­è¨€æ¨¡å‹çš„è§’åº¦/æŒ¯å¹…ç¼–ç ä¸¤ç§æŠ€æœ¯ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨å¤šçƒ­ç¼–ç çš„é‡å­æ¨¡å‹åœ¨æ¦‚å¿µéªŒè¯ä¸­è¡¨ç°è‰¯å¥½ï¼Œè€ŒåŸºäºCLIPå‘é‡çš„æ¨¡å‹æ€§èƒ½è™½è¡¨ç°ä¸ä¸€ï¼Œä½†ä»æ™®éä¼˜äºä¼ ç»Ÿçš„ç»å…¸ç»„åˆæ¨¡å‹ã€‚è¯¥å·¥ä½œå±•ç¤ºäº†é‡å­è®¡ç®—åœ¨æé«˜äººå·¥æ™ºèƒ½æ¨¡å‹è®­ç»ƒæ•ˆç‡å’Œå¤„ç†å¤æ‚è®¤çŸ¥ä»»åŠ¡æ–¹é¢çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to: 2025 IEEE International Conference on Quantum Artificial Intelligence (QAI), Naples, Italy, Nov 2-5, 2025. This is the authors' accepted manuscript (AAM). An IEEE copyright notice appears on page 1. The final published version will appear in IEEE Xplore; DOI to be added when available",
      "pdf_url": "https://arxiv.org/pdf/2509.09541v1",
      "published_date": "2025-09-11 15:34:33 UTC",
      "updated_date": "2025-09-11 15:34:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:55:17.886125+00:00"
    },
    {
      "arxiv_id": "2509.09529v1",
      "title": "A modified RIME algorithm with covariance learning and diversity enhancement for numerical optimization",
      "title_zh": "èåˆåæ–¹å·®å­¦ä¹ ä¸å¤šæ ·æ€§å¢å¼ºçš„æ”¹è¿›RIMEæ•°å€¼ä¼˜åŒ–ç®—æ³•",
      "authors": [
        "Shangqing Shi",
        "Luoxiao Zhang",
        "Yuchen Yin",
        "Xiong Yang",
        "Hoileong Lee"
      ],
      "abstract": "Metaheuristics are widely applied for their ability to provide more efficient solutions. The RIME algorithm is a recently proposed physical-based metaheuristic algorithm with certain advantages. However, it suffers from rapid loss of population diversity during optimization and is prone to fall into local optima, leading to unbalanced exploitation and exploration. To address the shortcomings of RIME, this paper proposes a modified RIME with covariance learning and diversity enhancement (MRIME-CD). The algorithm applies three strategies to improve the optimization capability. First, a covariance learning strategy is introduced in the soft-rime search stage to increase the population diversity and balance the over-exploitation ability of RIME through the bootstrapping effect of dominant populations. Second, in order to moderate the tendency of RIME population to approach the optimal individual in the early search stage, an average bootstrapping strategy is introduced into the hard-rime puncture mechanism, which guides the population search through the weighted position of the dominant populations, thus enhancing the global search ability of RIME in the early stage. Finally, a new stagnation indicator is proposed, and a stochastic covariance learning strategy is used to update the stagnant individuals in the population when the algorithm gets stagnant, thus enhancing the ability to jump out of the local optimal solution. The proposed MRIME-CD algorithm is subjected to a series of validations on the CEC2017 test set, the CEC2022 test set, and the experimental results are analyzed using the Friedman test, the Wilcoxon rank sum test, and the Kruskal Wallis test. The results show that MRIME-CD can effectively improve the performance of basic RIME and has obvious superiorities in terms of solution accuracy, convergence speed and stability.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ”¹è¿›çš„RIMEç®—æ³•ï¼Œåä¸ºMRIME-CDï¼Œæ—¨åœ¨è§£å†³åŸå§‹RIMEç®—æ³•åœ¨æ•°å€¼ä¼˜åŒ–è¿‡ç¨‹ä¸­ç§ç¾¤å¤šæ ·æ€§æµå¤±å¿«ã€æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜(Local Optima)ä»¥åŠå‹˜æ¢(Exploration)ä¸å¼€å‘(Exploitation)ä¸å¹³è¡¡çš„é—®é¢˜ã€‚è¯¥ç®—æ³•å¼•å…¥äº†åæ–¹å·®å­¦ä¹ (Covariance Learning)ç­–ç•¥ï¼Œåœ¨Soft-Rimeæœç´¢é˜¶æ®µé€šè¿‡ä¼˜åŠ¿ç§ç¾¤çš„è‡ªä¸¾æ•ˆåº”(Bootstrapping Effect)æå‡ç§ç¾¤å¤šæ ·æ€§ã€‚åŒæ—¶ï¼ŒMRIME-CDåœ¨Hard-Rimeç©¿åˆºæœºåˆ¶ä¸­é‡‡ç”¨äº†å¹³å‡è‡ªä¸¾ç­–ç•¥ï¼Œé€šè¿‡ä¼˜åŠ¿ç§ç¾¤çš„åŠ æƒä½ç½®å¼•å¯¼æœç´¢ï¼Œå¢å¼ºäº†æœç´¢åˆæœŸçš„å…¨å±€æœç´¢(Global Search)èƒ½åŠ›ã€‚é’ˆå¯¹ç®—æ³•åœæ»é—®é¢˜ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„åœæ»æŒ‡æ ‡(Stagnation Indicator)ï¼Œå¹¶ç»“åˆéšæœºåæ–¹å·®å­¦ä¹ ç­–ç•¥å¸®åŠ©åœæ»ä¸ªä½“è·³å‡ºå±€éƒ¨æœ€ä¼˜è§£ã€‚é€šè¿‡åœ¨CEC2017å’ŒCEC2022æµ‹è¯•é›†ä¸Šçš„å®éªŒéªŒè¯ï¼Œå¹¶ç»“åˆFriedmanã€Wilcoxonç§©å’Œæ£€éªŒç­‰ç»Ÿè®¡åˆ†æï¼Œç»“æœè¡¨æ˜MRIME-CDåœ¨æ±‚è§£ç²¾åº¦ã€æ”¶æ•›é€Ÿåº¦å’Œç¨³å®šæ€§æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºç¡€RIMEç®—æ³•ã€‚",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.NE",
      "comment": "This is the author's preprint of the article published in Cluster Computing (Springer): Shi, S., Zhang, L., Yin, Y. et al. A modified RIME algorithm with covariance learning and diversity enhancement for numerical optimization. Cluster Comput 28, 658 (2025). The final authenticated version is available online at SpringerLink",
      "pdf_url": "https://arxiv.org/pdf/2509.09529v1",
      "published_date": "2025-09-11 15:12:03 UTC",
      "updated_date": "2025-09-11 15:12:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:55:20.782957+00:00"
    },
    {
      "arxiv_id": "2509.09522v1",
      "title": "Towards Explainable Job Title Matching: Leveraging Semantic Textual Relatedness and Knowledge Graphs",
      "title_zh": "è¿ˆå‘å¯è§£é‡Šçš„èŒä½åç§°åŒ¹é…ï¼šåˆ©ç”¨è¯­ä¹‰æ–‡æœ¬ç›¸å…³æ€§ä¸çŸ¥è¯†å›¾è°±",
      "authors": [
        "Vadim Zadykian",
        "Bruno Andrade",
        "Haithem Afli"
      ],
      "abstract": "Semantic Textual Relatedness (STR) captures nuanced relationships between texts that extend beyond superficial lexical similarity. In this study, we investigate STR in the context of job title matching - a key challenge in resume recommendation systems, where overlapping terms are often limited or misleading. We introduce a self-supervised hybrid architecture that combines dense sentence embeddings with domain-specific Knowledge Graphs (KGs) to improve both semantic alignment and explainability. Unlike previous work that evaluated models on aggregate performance, our approach emphasizes data stratification by partitioning the STR score continuum into distinct regions: low, medium, and high semantic relatedness. This stratified evaluation enables a fine-grained analysis of model performance across semantically meaningful subspaces. We evaluate several embedding models, both with and without KG integration via graph neural networks. The results show that fine-tuned SBERT models augmented with KGs produce consistent improvements in the high-STR region, where the RMSE is reduced by 25% over strong baselines. Our findings highlight not only the benefits of combining KGs with text embeddings, but also the importance of regional performance analysis in understanding model behavior. This granular approach reveals strengths and weaknesses hidden by global metrics, and supports more targeted model selection for use in Human Resources (HR) systems and applications where fairness, explainability, and contextual matching are essential.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç®€å†æ¨èç³»ç»Ÿä¸­çš„èŒä½æ ‡é¢˜åŒ¹é…é—®é¢˜ï¼Œé‡ç‚¹å…³æ³¨èƒ½å¤Ÿæ•æ‰æ·±å±‚æ–‡æœ¬å…³ç³»çš„è¯­ä¹‰æ–‡æœ¬ç›¸å…³æ€§(Semantic Textual Relatedness, STR)ã€‚è®ºæ–‡æå‡ºäº†ä¸€ç§è‡ªç›‘ç£æ··åˆæ¶æ„ï¼Œå°†å¯†é›†å¥å­åµŒå…¥(dense sentence embeddings)ä¸é¢†åŸŸç‰¹å®šçŸ¥è¯†å›¾è°±(Knowledge Graphs, KGs)ç›¸ç»“åˆï¼Œæ—¨åœ¨åŒæ—¶æå‡è¯­ä¹‰å¯¹é½å’Œæ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚ä¸ä¼ ç»Ÿå…¨å±€æ€§èƒ½è¯„ä¼°ä¸åŒï¼Œè¯¥æ–¹æ³•é‡‡ç”¨åˆ†å±‚è¯„ä¼°(stratified evaluation)ï¼Œå°†STRåˆ†å€¼åˆ’åˆ†ä¸ºä½ã€ä¸­ã€é«˜ä¸‰ä¸ªåŒºé—´è¿›è¡Œç»†ç²’åº¦åˆ†æã€‚ç ”ç©¶è¯„ä¼°äº†å¤šç§åµŒå…¥æ¨¡å‹åœ¨é›†æˆå›¾ç¥ç»ç½‘ç»œ(graph neural networks)å‰åçš„è¡¨ç°ï¼Œç»“æœæ˜¾ç¤ºç»è¿‡å¾®è°ƒå¹¶è¾…ä»¥çŸ¥è¯†å›¾è°±çš„SBERTæ¨¡å‹åœ¨é«˜ç›¸å…³æ€§åŒºé—´è¡¨ç°æ˜¾è‘—ï¼Œå…¶å‡æ–¹æ ¹è¯¯å·®(RMSE)è¾ƒå¼ºåŸºçº¿æ¨¡å‹é™ä½äº†25%ã€‚è¯¥ç ”ç©¶ä¸ä»…è¯æ˜äº†çŸ¥è¯†å›¾è°±ä¸æ–‡æœ¬åµŒå…¥ç»“åˆçš„æœ‰æ•ˆæ€§ï¼Œè¿˜å¼ºè°ƒäº†åŒºé—´æ€§èƒ½åˆ†æåœ¨ç†è§£æ¨¡å‹è¡Œä¸ºæ–¹é¢çš„é‡è¦æ€§ã€‚è¿™ç§ç»†ç²’åº¦æ–¹æ³•ä¸ºäººåŠ›èµ„æº(HR)ç³»ç»Ÿä¸­è¿½æ±‚å…¬å¹³æ€§ä¸ç²¾å‡†ä¸Šä¸‹æ–‡åŒ¹é…çš„åº”ç”¨æä¾›äº†æ›´æœ‰é’ˆå¯¹æ€§çš„æ¨¡å‹é€‰æ‹©ä¾æ®ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.09522v1",
      "published_date": "2025-09-11 15:02:54 UTC",
      "updated_date": "2025-09-11 15:02:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:55:56.283742+00:00"
    },
    {
      "arxiv_id": "2509.09513v1",
      "title": "Explainable AI for Accelerated Microstructure Imaging: A SHAP-Guided Protocol on the Connectome 2.0 scanner",
      "title_zh": "ç”¨äºåŠ é€Ÿå¾®ç»“æ„æˆåƒçš„å¯è§£é‡Šäººå·¥æ™ºèƒ½ï¼šConnectome 2.0 æ‰«æä»ªä¸Šçš„ SHAP å¼•å¯¼æ–¹æ¡ˆ",
      "authors": [
        "Quentin Uhl",
        "Tommaso Pavan",
        "Julianna Gerold",
        "Kwok-Shing Chan",
        "Yohan Jun",
        "Shohei Fujita",
        "Aneri Bhatt",
        "Yixin Ma",
        "Qiaochu Wang",
        "Hong-Hsi Lee",
        "Susie Y. Huang",
        "Berkin Bilgic",
        "Ileana Jelescu"
      ],
      "abstract": "The diffusion MRI Neurite Exchange Imaging model offers a promising framework for probing gray matter microstructure by estimating parameters such as compartment sizes, diffusivities, and inter-compartmental water exchange time. However, existing protocols require long scan times. This study proposes a reduced acquisition scheme for the Connectome 2.0 scanner that preserves model accuracy while substantially shortening scan duration. We developed a data-driven framework using explainable artificial intelligence with a guided recursive feature elimination strategy to identify an optimal 8-feature subset from a 15-feature protocol. The performance of this optimized protocol was validated in vivo and benchmarked against the full acquisition and alternative reduction strategies. Parameter accuracy, preservation of anatomical contrast, and test-retest reproducibility were assessed. The reduced protocol yielded parameter estimates and cortical maps comparable to the full protocol, with low estimation errors in synthetic data and minimal impact on test-retest variability. Compared to theory-driven and heuristic reduction schemes, the optimized protocol demonstrated superior robustness, reducing the deviation in water exchange time estimates by over two-fold. In conclusion, this hybrid optimization framework enables viable imaging of neurite exchange in 14 minutes without loss of parameter fidelity. This approach supports the broader application of exchange-sensitive diffusion magnetic resonance imaging in neuroscience and clinical research, and offers a generalizable method for designing efficient acquisition protocols in biophysical parameter mapping.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼¥æ•£ MRI (diffusion MRI) çš„ç¥ç»çªäº¤æ¢æˆåƒ (Neurite Exchange Imaging, NEXI) æ¨¡å‹åœ¨è¯„ä¼°ç°è´¨å¾®ç»“æ„æ—¶æ‰«ææ—¶é—´è¿‡é•¿çš„é—®é¢˜ï¼Œåœ¨ Connectome 2.0 æ‰«æä»ªä¸Šæå‡ºäº†ä¸€ç§åŠ é€Ÿæˆåƒæ–¹æ¡ˆã€‚ç ”ç©¶äººå‘˜å¼€å‘äº†ä¸€ä¸ªåŸºäºå¯è§£é‡Šäººå·¥æ™ºèƒ½ (Explainable AI, XAI) çš„æ•°æ®é©±åŠ¨æ¡†æ¶ï¼Œåˆ©ç”¨ SHAP å¼•å¯¼çš„é€’å½’ç‰¹å¾æ¶ˆé™¤ (Recursive Feature Elimination, RFE) ç­–ç•¥ï¼ŒæˆåŠŸä»åŸå§‹ 15 ä¸ªç‰¹å¾ä¸­è¯†åˆ«å‡ºæœ€ä¼˜çš„ 8 ç‰¹å¾å­é›†ã€‚æ´»ä½“éªŒè¯å’Œå¯¹æ¯”å®éªŒè¡¨æ˜ï¼Œè¯¥ç¼©å‡åè®®åœ¨å‚æ•°ä¼°è®¡ç²¾åº¦å’Œè§£å‰–å¯¹æ¯”åº¦ä¿å­˜æ–¹é¢ä¸å®Œæ•´åè®®é«˜åº¦ä¸€è‡´ï¼Œä¸”å…·æœ‰æä½çš„ä¼°è®¡è¯¯å·®å’Œè‰¯å¥½çš„é‡æµ‹ä¿¡åº¦ã€‚ä¸ä¼ ç»Ÿçš„å¯å‘å¼ç¼©å‡æ–¹æ¡ˆç›¸æ¯”ï¼Œè¯¥ä¼˜åŒ–åè®®æ˜¾è‘—æé«˜äº†é²æ£’æ€§ï¼Œå°†æ°´äº¤æ¢æ—¶é—´ä¼°è®¡çš„åå·®é™ä½äº†ä¸¤å€ä»¥ä¸Šã€‚è¯¥æ··åˆä¼˜åŒ–æ¡†æ¶å®ç°äº†åœ¨ 14 åˆ†é’Ÿå†…å®Œæˆé«˜è´¨é‡çš„ç¥ç»çªäº¤æ¢æˆåƒï¼Œä¸”å®Œå…¨ä¸æŸå¤±å‚æ•°ä¿çœŸåº¦ã€‚è¿™ä¸€ç ”ç©¶æˆæœä¸ä»…ä¿ƒè¿›äº†äº¤æ¢æ•æ„Ÿæ‰©æ•£æˆåƒåœ¨ç¥ç»ç§‘å­¦å’Œä¸´åºŠç ”ç©¶ä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œä¹Ÿä¸ºç”Ÿç‰©ç‰©ç†å‚æ•°æ˜ å°„çš„é«˜æ•ˆåè®®è®¾è®¡æä¾›äº†ä¸€ç§å¯æ¨å¹¿çš„é€šç”¨æ–¹æ³•ã€‚",
      "categories": [
        "physics.med-ph",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "physics.med-ph",
      "comment": "Submitted to IEEE Transactions on Medical Imaging (TMI). This all-in-one version includes supplementary materials. 18 pages, 14 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.09513v1",
      "published_date": "2025-09-11 14:53:26 UTC",
      "updated_date": "2025-09-11 14:53:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:55:35.285827+00:00"
    },
    {
      "arxiv_id": "2509.09508v1",
      "title": "Incorporating AI Incident Reporting into Telecommunications Law and Policy: Insights from India",
      "title_zh": "å°† AI äº‹ä»¶æŠ¥å‘Šçº³å…¥ç”µä¿¡æ³•å¾‹ä¸æ”¿ç­–ï¼šæ¥è‡ª India çš„å¯ç¤º",
      "authors": [
        "Avinash Agarwal",
        "Manisha J. Nene"
      ],
      "abstract": "The integration of artificial intelligence (AI) into telecommunications infrastructure introduces novel risks, such as algorithmic bias and unpredictable system behavior, that fall outside the scope of traditional cybersecurity and data protection frameworks. This paper introduces a precise definition and a detailed typology of telecommunications AI incidents, establishing them as a distinct category of risk that extends beyond conventional cybersecurity and data protection breaches. It argues for their recognition as a distinct regulatory concern. Using India as a case study for jurisdictions that lack a horizontal AI law, the paper analyzes the country's key digital regulations. The analysis reveals that India's existing legal instruments, including the Telecommunications Act, 2023, the CERT-In Rules, and the Digital Personal Data Protection Act, 2023, focus on cybersecurity and data breaches, creating a significant regulatory gap for AI-specific operational incidents, such as performance degradation and algorithmic bias. The paper also examines structural barriers to disclosure and the limitations of existing AI incident repositories. Based on these findings, the paper proposes targeted policy recommendations centered on integrating AI incident reporting into India's existing telecom governance. Key proposals include mandating reporting for high-risk AI failures, designating an existing government body as a nodal agency to manage incident data, and developing standardized reporting frameworks. These recommendations aim to enhance regulatory clarity and strengthen long-term resilience, offering a pragmatic and replicable blueprint for other nations seeking to govern AI risks within their existing sectoral frameworks.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†å°†äººå·¥æ™ºèƒ½(AI)é›†æˆè‡³ç”µä¿¡åŸºç¡€è®¾æ–½ä¸­æ‰€å¸¦æ¥çš„ç®—æ³•åè§(algorithmic bias)å’Œç³»ç»Ÿè¡Œä¸ºä¸å¯é¢„æµ‹ç­‰æ–°å‹é£é™©ï¼ŒæŒ‡å‡ºè¿™äº›é£é™©å·²è¶…å‡ºä¼ ç»Ÿç½‘ç»œå®‰å…¨æ¡†æ¶çš„èŒƒç•´ã€‚è®ºæ–‡é€šè¿‡å¯¹â€œç”µä¿¡äººå·¥æ™ºèƒ½äº‹ä»¶â€(telecommunications AI incidents)è¿›è¡Œç²¾ç¡®å®šä¹‰å’Œåˆ†ç±»ï¼Œè®ºè¯äº†å°†å…¶ä½œä¸ºç‹¬ç«‹ç›‘ç®¡ç±»åˆ«çš„å¿…è¦æ€§ã€‚é€šè¿‡å¯¹å°åº¦çš„æ¡ˆä¾‹åˆ†æï¼Œç ”ç©¶æ­ç¤ºäº†ã€Š2023å¹´ç”µä¿¡æ³•ã€‹(Telecommunications Act, 2023)å’Œã€Š2023å¹´æ•°å­—ä¸ªäººæ•°æ®ä¿æŠ¤æ³•ã€‹(Digital Personal Data Protection Act, 2023)ç­‰ç°æœ‰æ³•å¾‹åœ¨å¤„ç†AIç‰¹æœ‰è¿è¡Œæ•…éšœï¼ˆå¦‚æ€§èƒ½ä¸‹é™ï¼‰æ–¹é¢çš„ç›‘ç®¡ç©ºç™½ã€‚è®ºæ–‡è¿›ä¸€æ­¥åˆ†æäº†ä¿¡æ¯æŠ«éœ²çš„éšœç¢ï¼Œå¹¶é’ˆå¯¹æ€§åœ°æå‡ºäº†å°†AIäº‹ä»¶æŠ¥å‘Šæ•´åˆè‡³ç”µä¿¡æ²»ç†ä¸­çš„æ”¿ç­–å»ºè®®ï¼ŒåŒ…æ‹¬å¼ºåˆ¶æŠ¥å‘Šé«˜é£é™©æ•…éšœå’Œè®¾ç«‹ä¸“é—¨çš„èŠ‚ç‚¹æœºæ„(nodal agency)ã€‚è¿™äº›å»ºè®®æ—¨åœ¨é€šè¿‡æ ‡å‡†åŒ–æŠ¥å‘Šæ¡†æ¶å¢å¼ºç›‘ç®¡é€æ˜åº¦ï¼Œä¸ºå…¨çƒå…¶ä»–å›½å®¶åœ¨ç°æœ‰è¡Œä¸šæ¡†æ¶ä¸‹æ²»ç†AIé£é™©æä¾›äº†ä¸€ä¸ªå¯å€Ÿé‰´çš„åŠ¡å®è“å›¾ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "16 pages, 2 figures, 1 table",
      "pdf_url": "https://arxiv.org/pdf/2509.09508v1",
      "published_date": "2025-09-11 14:50:41 UTC",
      "updated_date": "2025-09-11 14:50:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:55:41.791126+00:00"
    },
    {
      "arxiv_id": "2509.09498v3",
      "title": "SEDM: Scalable Self-Evolving Distributed Memory for Agents",
      "title_zh": "SEDMï¼šé¢å‘æ™ºèƒ½ä½“çš„å¯æ‰©å±•è‡ªè¿›åŒ–åˆ†å¸ƒå¼è®°å¿†",
      "authors": [
        "Haoran Xu",
        "Jiacong Hu",
        "Ke Zhang",
        "Lei Yu",
        "Yuxin Tang",
        "Xinyuan Song",
        "Yiqun Duan",
        "Lynn Ai",
        "Bill Shi"
      ],
      "abstract": "Long-term multi-agent systems inevitably generate vast amounts of trajectories and historical interactions, which makes efficient memory management essential for both performance and scalability. Existing methods typically depend on vector retrieval and hierarchical storage, yet they are prone to noise accumulation, uncontrolled memory expansion, and limited generalization across domains. To address these challenges, we present SEDM, Self-Evolving Distributed Memory, a verifiable and adaptive framework that transforms memory from a passive repository into an active, self-optimizing component. SEDM integrates verifiable write admission based on reproducible replay, a self-scheduling memory controller that dynamically ranks and consolidates entries according to empirical utility, and cross-domain knowledge diffusion that abstracts reusable insights to support transfer across heterogeneous tasks. Evaluations on benchmark datasets demonstrate that SEDM improves reasoning accuracy while reducing token overhead compared with strong memory baselines, and further enables knowledge distilled from fact verification to enhance multi-hop reasoning. The results highlight SEDM as a scalable and sustainable memory mechanism for open-ended multi-agent collaboration. The code will be released in the later stage of this project.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SEDMï¼ˆè‡ªè¿›åŒ–åˆ†å¸ƒå¼å­˜å‚¨ï¼ŒSelf-Evolving Distributed Memoryï¼‰ï¼Œæ—¨åœ¨è§£å†³é•¿æœŸå¤šæ™ºèƒ½ä½“ç³»ç»Ÿ(multi-agent systems)åœ¨å¤„ç†æµ·é‡äº¤äº’è½¨è¿¹æ—¶é¢ä¸´çš„å™ªå£°ç´¯ç§¯ã€å†…å­˜æ— é™è†¨èƒ€åŠè·¨åŸŸæ³›åŒ–å—é™ç­‰æŒ‘æˆ˜ã€‚SEDM å°†å†…å­˜ä»ä¼ ç»Ÿçš„è¢«åŠ¨å­˜å‚¨åº“è½¬å˜ä¸ºä¸»åŠ¨ã€è‡ªä¼˜åŒ–çš„ç»„ä»¶ï¼Œé€šè¿‡åŸºäºå¯å¤ç°å›æ”¾(reproducible replay)çš„å¯éªŒè¯å†™å…¥è®¸å¯æœºåˆ¶ç¡®ä¿æ•°æ®è´¨é‡ã€‚è¯¥æ¡†æ¶å†…ç½®è‡ªè°ƒåº¦å†…å­˜æ§åˆ¶å™¨ï¼Œèƒ½æ ¹æ®ç»éªŒæ•ˆç”¨(empirical utility)åŠ¨æ€æ’åºå¹¶æ•´åˆå†…å­˜æ¡ç›®ï¼Œå¹¶åˆ©ç”¨è·¨åŸŸçŸ¥è¯†æ‰©æ•£(cross-domain knowledge diffusion)æŠ€æœ¯æŠ½è±¡å‡ºå¯é‡ç”¨çš„è§è§£ä»¥æ”¯æŒå¼‚æ„ä»»åŠ¡è¿ç§»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸å¼ºåŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼ŒSEDM åœ¨æ˜¾è‘—æé«˜æ¨ç†å‡†ç¡®ç‡çš„åŒæ—¶å‡å°‘äº† Token å¼€é”€ï¼Œå¹¶æˆåŠŸå°†ä»äº‹å®æ ¸æŸ¥ä¸­æç‚¼çš„çŸ¥è¯†åº”ç”¨äºå¢å¼ºå¤šè·³æ¨ç†(multi-hop reasoning)ã€‚è¿™é¡¹å·¥ä½œä¸ºå¼€æ”¾å¼å¤šæ™ºèƒ½ä½“åä½œæä¾›äº†ä¸€ç§å…·å¤‡é«˜æ‰©å±•æ€§ä¸å¯æŒç»­æ€§çš„å†…å­˜ç®¡ç†æœºåˆ¶ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.09498v3",
      "published_date": "2025-09-11 14:37:37 UTC",
      "updated_date": "2025-09-26 06:26:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:55:42.883474+00:00"
    },
    {
      "arxiv_id": "2509.09495v2",
      "title": "OpenFake: An Open Dataset and Platform Toward Real-World Deepfake Detection",
      "title_zh": "OpenFakeï¼šé¢å‘çœŸå®ä¸–ç•Œæ·±åº¦ä¼ªé€ æ£€æµ‹çš„å¼€æ”¾æ•°æ®é›†ä¸å¹³å°",
      "authors": [
        "Victor Livernoche",
        "Akshatha Arodi",
        "Andreea Musulan",
        "Zachary Yang",
        "Adam Salvail",
        "GaÃ©tan Marceau Caron",
        "Jean-FranÃ§ois Godbout",
        "Reihaneh Rabbany"
      ],
      "abstract": "Deepfakes, synthetic media created using advanced AI techniques, pose a growing threat to information integrity, particularly in politically sensitive contexts. This challenge is amplified by the increasing realism of modern generative models, which our human perception study confirms are often indistinguishable from real images. Yet, existing deepfake detection benchmarks rely on outdated generators or narrowly scoped datasets (e.g., single-face imagery), limiting their utility for real-world detection. To address these gaps, we present OpenFake, a large politically grounded dataset specifically crafted for benchmarking against modern generative models with high realism, and designed to remain extensible through an innovative crowdsourced adversarial platform that continually integrates new hard examples. OpenFake comprises nearly four million total images: three million real images paired with descriptive captions and almost one million synthetic counterparts from state-of-the-art proprietary and open-source models. Detectors trained on OpenFake achieve near-perfect in-distribution performance, strong generalization to unseen generators, and high accuracy on a curated in-the-wild social media test set, significantly outperforming models trained on existing datasets. Overall, we demonstrate that with high-quality and continually updated benchmarks, automatic deepfake detection is both feasible and effective in real-world settings.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Deepfakes å¸¦æ¥çš„ä¿¡æ¯å®Œæ•´æ€§å¨èƒï¼Œç‰¹åˆ«æ˜¯ç°ä»£ç”Ÿæˆæ¨¡å‹äº§ç”Ÿçš„é«˜åº¦é€¼çœŸã€è‚‰çœ¼éš¾è¾¨çš„å›¾åƒï¼Œæå‡ºäº† OpenFake è¿™ä¸€å¤§è§„æ¨¡æ”¿æ²»ç›¸å…³æ•°æ®é›†å’Œæ£€æµ‹å¹³å°ã€‚OpenFake åŒ…å«è¿‘å››ç™¾ä¸‡å¼ å›¾åƒï¼Œå…¶ä¸­åŒ…æ‹¬ä¸‰ç™¾ä¸‡å¼ å¸¦æœ‰æè¿°æ€§æ–‡å­—çš„çœŸå®å›¾åƒï¼Œä»¥åŠè¿‘ä¸€ç™¾ä¸‡å¼ ç”±æœ€å…ˆè¿›çš„é—­æºå’Œå¼€æºæ¨¡å‹ç”Ÿæˆçš„åˆæˆå›¾åƒã€‚è¯¥å¹³å°é€šè¿‡åˆ›æ–°çš„ä¼—åŒ…å¯¹æŠ—æ€§å¹³å°ä¿æŒå¯æ‰©å±•æ€§ï¼Œèƒ½å¤ŸæŒç»­é›†æˆæ–°çš„ Hard Examplesï¼Œä»è€Œè§£å†³ç°æœ‰åŸºå‡†æµ‹è¯•ä¾èµ–è¿‡æ—¶ç”Ÿæˆå™¨æˆ–æ•°æ®é›†èŒƒå›´æœ‰é™çš„é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨ OpenFake ä¸Šè®­ç»ƒçš„æ£€æµ‹å™¨åœ¨åˆ†å¸ƒå†…è¡¨ç°æ¥è¿‘å®Œç¾ï¼Œå¯¹æœªçŸ¥ç”Ÿæˆå™¨å…·æœ‰æå¼ºçš„ Generalization èƒ½åŠ›ï¼Œå¹¶åœ¨çœŸå®çš„ç¤¾äº¤åª’ä½“æµ‹è¯•é›†ä¸Šæ˜¾è‘—ä¼˜äºåŸºäºç°æœ‰æ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹ã€‚è¯¥é¡¹å·¥ä½œè¯æ˜äº†é€šè¿‡é«˜è´¨é‡ä¸”æŒç»­æ›´æ–°çš„åŸºå‡†æµ‹è¯•ï¼Œåœ¨ç°å®åœºæ™¯ä¸­å®ç°è‡ªåŠ¨ Deepfake Detection æ˜¯å¯è¡Œä¸”æœ‰æ•ˆçš„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "26 pages, 12 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.09495v2",
      "published_date": "2025-09-11 14:34:22 UTC",
      "updated_date": "2025-10-06 21:24:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:55:55.987075+00:00"
    },
    {
      "arxiv_id": "2509.09488v1",
      "title": "Prompt Pirates Need a Map: Stealing Seeds helps Stealing Prompts",
      "title_zh": "Prompt Pirates éœ€è¦åœ°å›¾ï¼šçªƒå–ç§å­åŠ©åŠ›æç¤ºè¯çªƒå–",
      "authors": [
        "Felix MÃ¤chtle",
        "Ashwath Shetty",
        "Jonas Sander",
        "Nils Loose",
        "SÃ¶ren Pirk",
        "Thomas Eisenbarth"
      ],
      "abstract": "Diffusion models have significantly advanced text-to-image generation, enabling the creation of highly realistic images conditioned on textual prompts and seeds. Given the considerable intellectual and economic value embedded in such prompts, prompt theft poses a critical security and privacy concern. In this paper, we investigate prompt-stealing attacks targeting diffusion models. We reveal that numerical optimization-based prompt recovery methods are fundamentally limited as they do not account for the initial random noise used during image generation. We identify and exploit a noise-generation vulnerability (CWE-339), prevalent in major image-generation frameworks, originating from PyTorch's restriction of seed values to a range of $2^{32}$ when generating the initial random noise on CPUs. Through a large-scale empirical analysis conducted on images shared via the popular platform CivitAI, we demonstrate that approximately 95% of these images' seed values can be effectively brute-forced in 140 minutes per seed using our seed-recovery tool, SeedSnitch. Leveraging the recovered seed, we propose PromptPirate, a genetic algorithm-based optimization method explicitly designed for prompt stealing. PromptPirate surpasses state-of-the-art methods, i.e., PromptStealer, P2HP, and CLIP-Interrogator, achieving an 8-11% improvement in LPIPS similarity. Furthermore, we introduce straightforward and effective countermeasures that render seed stealing, and thus optimization-based prompt stealing, ineffective. We have disclosed our findings responsibly and initiated coordinated mitigation efforts with the developers to address this critical vulnerability.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº† Diffusion models ä¸­çš„æç¤ºè¯çªƒå–æ”»å‡»é£é™©ï¼ŒæŒ‡å‡ºå½“å‰çš„æ•°å€¼ä¼˜åŒ–æ¢å¤æ–¹æ³•å› å¿½è§†åˆå§‹éšæœºå™ªå£°è€Œå­˜åœ¨å±€é™ã€‚ç ”ç©¶æ­ç¤ºäº† PyTorch ç­‰ä¸»æµæ¡†æ¶ä¸­ä¸€ä¸ªå¹¿æ³›å­˜åœ¨çš„å™ªå£°ç”Ÿæˆæ¼æ´ (CWE-339)ï¼Œå³ CPU ä¸Šçš„ç§å­å€¼èŒƒå›´è¢«é™åˆ¶åœ¨ $2^{32}$ ä»¥å†…ï¼Œä»è€Œå¯¼è‡´å®‰å…¨æ€§ä¸‹é™ã€‚ä½œè€…å¼€å‘äº† SeedSnitch å·¥å…·ï¼Œé€šè¿‡å®éªŒè¯æ˜ CivitAI å¹³å°ä¸Šçº¦ 95% å›¾åƒçš„ç§å­å€¼å¯åœ¨ 140 åˆ†é’Ÿå†…è¢«æš´åŠ›ç ´è§£ã€‚åŸºäºæ¢å¤çš„ç§å­ï¼Œç ”ç©¶æå‡ºäº†åä¸º PromptPirate çš„ Genetic Algorithm ä¼˜åŒ–æ–¹æ³•ï¼Œå…¶åœ¨ LPIPS ç›¸ä¼¼åº¦ä¸Šæ¯” PromptStealer å’Œ CLIP-Interrogator ç­‰ç°æœ‰æœ€å…ˆè¿›æ–¹æ³•æå‡äº† 8-11%ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†é’ˆå¯¹æ€§çš„é˜²å¾¡å¯¹ç­–ï¼Œå¹¶ä¸ç›¸å…³å¼€å‘è€…å…±åŒå¯åŠ¨äº†é’ˆå¯¹è¯¥å…³é”®æ¼æ´çš„ä¿®å¤å·¥ä½œã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.09488v1",
      "published_date": "2025-09-11 14:21:59 UTC",
      "updated_date": "2025-09-11 14:21:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:56:41.090590+00:00"
    },
    {
      "arxiv_id": "2509.09751v1",
      "title": "Meta-Learning Reinforcement Learning for Crypto-Return Prediction",
      "title_zh": "ç”¨äºåŠ å¯†è´§å¸æ”¶ç›Šé¢„æµ‹çš„å…ƒå­¦ä¹ å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Junqiao Wang",
        "Zhaoyang Guan",
        "Guanyu Liu",
        "Tianze Xia",
        "Xianzhi Li",
        "Shuo Yin",
        "Xinyuan Song",
        "Chuhan Cheng",
        "Tianyu Shi",
        "Alex Lee"
      ],
      "abstract": "Predicting cryptocurrency returns is notoriously difficult: price movements are driven by a fast-shifting blend of on-chain activity, news flow, and social sentiment, while labeled training data are scarce and expensive. In this paper, we present Meta-RL-Crypto, a unified transformer-based architecture that unifies meta-learning and reinforcement learning (RL) to create a fully self-improving trading agent. Starting from a vanilla instruction-tuned LLM, the agent iteratively alternates between three roles-actor, judge, and meta-judge-in a closed-loop architecture. This learning process requires no additional human supervision. It can leverage multimodal market inputs and internal preference feedback. The agent in the system continuously refines both the trading policy and evaluation criteria. Experiments across diverse market regimes demonstrate that Meta-RL-Crypto shows good performance on the technical indicators of the real market and outperforming other LLM-based baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŠ å¯†è´§å¸å›æŠ¥é¢„æµ‹ä¸­é¢ä¸´çš„ä»·æ ¼æ³¢åŠ¨å‰§çƒˆã€æ ‡è®°æ•°æ®åŒ®ä¹ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†Meta-RL-Cryptoæ¡†æ¶ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨åŸºäºTransformerçš„ç»Ÿä¸€æ¶æ„ï¼Œå°†å…ƒå­¦ä¹ (Meta-Learning)ä¸å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)ç›¸ç»“åˆï¼Œæ„å»ºäº†ä¸€ä¸ªå®Œå…¨è‡ªæˆ‘æ”¹è¿›çš„äº¤æ˜“æ™ºèƒ½ä½“ã€‚æ™ºèƒ½ä½“ä»ç»è¿‡æŒ‡ä»¤å¾®è°ƒçš„å¤§è¯­è¨€æ¨¡å‹(LLM)å‡ºå‘ï¼Œåœ¨é—­ç¯æ¶æ„ä¸­è¿­ä»£æ‰§è¡Œæ‰§è¡Œè€…(actor)ã€è¯„åˆ¤è€…(judge)å’Œå…ƒè¯„åˆ¤è€…(meta-judge)ä¸‰ç§è§’è‰²ï¼Œå®ç°äº†æ— éœ€é¢å¤–äººå·¥ç›‘ç£çš„è‡ªä¸»å­¦ä¹ è¿‡ç¨‹ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿæ•´åˆå¤šæ¨¡æ€å¸‚åœºè¾“å…¥å’Œå†…éƒ¨åå¥½åé¦ˆï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤ŸæŒç»­ä¼˜åŒ–å…¶äº¤æ˜“ç­–ç•¥(trading policy)å’Œè¯„ä¼°æ ‡å‡†ã€‚åœ¨ä¸åŒå¸‚åœºç¯å¢ƒä¸‹çš„å®éªŒè¯æ˜ï¼ŒMeta-RL-Cryptoåœ¨çœŸå®å¸‚åœºçš„æŠ€æœ¯æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå…¶æ€§èƒ½æ˜¾è‘—ä¼˜äºå…¶ä»–åŸºäºLLMçš„åŸºå‡†æ¨¡å‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.09751v1",
      "published_date": "2025-09-11 14:20:45 UTC",
      "updated_date": "2025-09-11 14:20:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:56:48.351896+00:00"
    },
    {
      "arxiv_id": "2509.10572v2",
      "title": "Quality Assessment of Tabular Data using Large Language Models and Code Generation",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹ä¸ä»£ç ç”Ÿæˆçš„è¡¨æ ¼æ•°æ®è´¨é‡è¯„ä¼°",
      "authors": [
        "Ashlesha Akella",
        "Akshar Kaul",
        "Krishnasuri Narayanam",
        "Sameep Mehta"
      ],
      "abstract": "Reliable data quality is crucial for downstream analysis of tabular datasets, yet rule-based validation often struggles with inefficiency, human intervention, and high computational costs. We present a three-stage framework that combines statistical inliner detection with LLM-driven rule and code generation. After filtering data samples through traditional clustering, we iteratively prompt LLMs to produce semantically valid quality rules and synthesize their executable validators through code-generating LLMs. To generate reliable quality rules, we aid LLMs with retrieval-augmented generation (RAG) by leveraging external knowledge sources and domain-specific few-shot examples. Robust guardrails ensure the accuracy and consistency of both rules and code snippets. Extensive evaluations on benchmark datasets confirm the effectiveness of our approach.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¡¨æ ¼æ•°æ®(Tabular Data)è´¨é‡è¯„ä¼°ä¸­è§„åˆ™éªŒè¯æ•ˆç‡ä½ä¸‹ä¸”ä¾èµ–äººå·¥å¹²é¢„çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªç»“åˆç»Ÿè®¡ç¦»ç¾¤ç‚¹æ£€æµ‹ã€å¤§è¯­è¨€æ¨¡å‹(LLMs)é©±åŠ¨çš„è§„åˆ™ç”Ÿæˆä¸ä»£ç ç”Ÿæˆçš„åˆ›æ–°æ€§ä¸‰é˜¶æ®µæ¡†æ¶ã€‚è¯¥æ¡†æ¶é¦–å…ˆé€šè¿‡ä¼ ç»Ÿçš„èšç±»ç®—æ³•ç­›é€‰æ•°æ®æ ·æœ¬ï¼Œéšååˆ©ç”¨LLMsè¿­ä»£ç”Ÿæˆå…·æœ‰è¯­ä¹‰æœ‰æ•ˆæ€§çš„è´¨é‡è§„åˆ™ï¼Œå¹¶å€ŸåŠ©ä»£ç ç”Ÿæˆç±»LLMså°†å…¶è½¬åŒ–ä¸ºå¯æ‰§è¡Œçš„éªŒè¯å™¨(Validators)ã€‚ä¸ºäº†æå‡è§„åˆ™ç”Ÿæˆçš„å¯é æ€§ï¼Œç ”ç©¶å¼•å…¥äº†æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)æŠ€æœ¯ï¼Œç»“åˆå¤–éƒ¨çŸ¥è¯†åº“å’Œé¢†åŸŸç‰¹å®šçš„Few-shotç¤ºä¾‹æ¥è¾…åŠ©æ¨¡å‹ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿé€šè¿‡å¥å£®çš„æŠ¤æ (Guardrails)æœºåˆ¶ç¡®ä¿è§„åˆ™å’Œä»£ç ç‰‡æ®µçš„å‡†ç¡®æ€§ä¸ä¸€è‡´æ€§ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°ç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆæå‡æ•°æ®è´¨é‡è¯„ä¼°çš„æ•ˆç‡å’Œå¯é æ€§ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.SE",
      "comment": "under review",
      "pdf_url": "https://arxiv.org/pdf/2509.10572v2",
      "published_date": "2025-09-11 14:17:42 UTC",
      "updated_date": "2025-09-21 02:54:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:56:42.791150+00:00"
    },
    {
      "arxiv_id": "2509.13339v1",
      "title": "Position: AI Safety Must Embrace an Antifragile Perspective",
      "title_zh": "ç«‹åœºï¼šäººå·¥æ™ºèƒ½å®‰å…¨å¿…é¡»ç§‰æŒåè„†å¼±è§†è§’",
      "authors": [
        "Ming Jin",
        "Hyunin Lee"
      ],
      "abstract": "This position paper contends that modern AI research must adopt an antifragile perspective on safety -- one in which the system's capacity to guarantee long-term AI safety such as handling rare or out-of-distribution (OOD) events expands over time. Conventional static benchmarks and single-shot robustness tests overlook the reality that environments evolve and that models, if left unchallenged, can drift into maladaptation (e.g., reward hacking, over-optimization, or atrophy of broader capabilities). We argue that an antifragile approach -- Rather than striving to rapidly reduce current uncertainties, the emphasis is on leveraging those uncertainties to better prepare for potentially greater, more unpredictable uncertainties in the future -- is pivotal for the long-term reliability of open-ended ML systems. In this position paper, we first identify key limitations of static testing, including scenario diversity, reward hacking, and over-alignment. We then explore the potential of antifragile solutions to manage rare events. Crucially, we advocate for a fundamental recalibration of the methods used to measure, benchmark, and continually improve AI safety over the long term, complementing existing robustness approaches by providing ethical and practical guidelines towards fostering an antifragile AI safety community.",
      "tldr_zh": "è¿™ç¯‡ç«‹åœºè®ºæ–‡ä¸»å¼ ç°ä»£ AI Safety ç ”ç©¶å¿…é¡»é‡‡çº³ä¸€ç§ Antifragileï¼ˆåè„†å¼±ï¼‰è§†è§’ï¼Œä½¿ç³»ç»Ÿåœ¨åº”å¯¹ç½•è§æˆ– Out-of-Distribution (OOD) äº‹ä»¶æ—¶çš„å®‰å…¨ä¿éšœèƒ½åŠ›èƒ½å¤Ÿéšæ—¶é—´æ¨ç§»è€Œä¸æ–­å¢å¼ºã€‚ç ”ç©¶æŒ‡å‡ºï¼Œä¼ ç»Ÿçš„é™æ€åŸºå‡†æµ‹è¯•å’Œå•æ¬¡ Robustness æµ‹è¯•å¿½è§†äº†ç¯å¢ƒçš„åŠ¨æ€æ¼”å˜ï¼Œå¯èƒ½å¯¼è‡´æ¨¡å‹å‡ºç° Reward Hackingã€è¿‡åº¦ä¼˜åŒ–æˆ–èƒ½åŠ›èç¼©ç­‰ maladaptation é—®é¢˜ã€‚ä¸è¿…é€Ÿæ¶ˆé™¤å½“å‰ä¸ç¡®å®šæ€§çš„åšæ³•ä¸åŒï¼ŒAntifragile æ–¹æ³•å¼ºè°ƒåˆ©ç”¨è¿™äº›ä¸ç¡®å®šæ€§æ¥æ›´å¥½åœ°åº”å¯¹æœªæ¥æ›´å…·ä¸å¯é¢„æµ‹æ€§çš„æŒ‘æˆ˜ï¼Œè¿™å¯¹äº Open-ended ML ç³»ç»Ÿçš„é•¿æœŸå¯é æ€§è‡³å…³é‡è¦ã€‚è®ºæ–‡è¯†åˆ«äº†é™æ€æµ‹è¯•åœ¨ Scenario Diversityã€Reward Hacking å’Œ Over-alignment æ–¹é¢çš„æ ¸å¿ƒå±€é™æ€§ï¼Œå¹¶æ¢è®¨äº†åˆ©ç”¨ Antifragile æ–¹æ¡ˆç®¡ç†ç½•è§äº‹ä»¶çš„æ½œåŠ›ã€‚ä½œè€…æœ€åå‘¼åä»æ ¹æœ¬ä¸Šé‡æ–°è°ƒæ•´è¡¡é‡ã€åŸºå‡†æµ‹è¯•å’ŒæŒç»­æ”¹è¿›é•¿æœŸ AI Safety çš„æ–¹æ³•ï¼Œå¹¶ä¸ºæ„å»º Antifragile AI Safety ç¤¾åŒºæä¾›äº†ä¼¦ç†å’Œå®è·µå±‚é¢çš„æŒ‡å¯¼æ–¹é’ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.13339v1",
      "published_date": "2025-09-11 14:01:43 UTC",
      "updated_date": "2025-09-11 14:01:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:56:47.360329+00:00"
    },
    {
      "arxiv_id": "2509.09470v1",
      "title": "AEGIS: An Agent for Extraction and Geographic Identification in Scholarly Proceedings",
      "title_zh": "AEGISï¼šé¢å‘å­¦æœ¯è®ºæ–‡é›†æå–ä¸åœ°ç†è¯†åˆ«çš„æ™ºèƒ½ä½“",
      "authors": [
        "Om Vishesh",
        "Harshad Khadilkar",
        "Deepak Akkil"
      ],
      "abstract": "Keeping pace with the rapid growth of academia literature presents a significant challenge for researchers, funding bodies, and academic societies. To address the time-consuming manual effort required for scholarly discovery, we present a novel, fully automated system that transitions from data discovery to direct action. Our pipeline demonstrates how a specialized AI agent, 'Agent-E', can be tasked with identifying papers from specific geographic regions within conference proceedings and then executing a Robotic Process Automation (RPA) to complete a predefined action, such as submitting a nomination form. We validated our system on 586 papers from five different conferences, where it successfully identified every target paper with a recall of 100% and a near perfect accuracy of 99.4%. This demonstration highlights the potential of task-oriented AI agents to not only filter information but also to actively participate in and accelerate the workflows of the academic community.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AEGISï¼Œä¸€ä¸ªæ—¨åœ¨è§£å†³å­¦æœ¯æ–‡çŒ®æ¿€å¢å¸¦æ¥çš„æ‰‹åŠ¨ç­›é€‰æŒ‘æˆ˜çš„å…¨è‡ªåŠ¨ç³»ç»Ÿã€‚è¯¥æµç¨‹å±•ç¤ºäº†å¦‚ä½•é€šè¿‡ä¸“é—¨çš„ AI Agentï¼ˆå‘½åä¸º 'Agent-E'ï¼‰è¯†åˆ«å­¦æœ¯ä¼šè®®è®ºæ–‡é›†ï¼ˆScholarly Proceedingsï¼‰ä¸­ç‰¹å®šåœ°ç†åŒºåŸŸçš„è®ºæ–‡ï¼Œå¹¶åˆ©ç”¨æœºå™¨äººæµç¨‹è‡ªåŠ¨åŒ–ï¼ˆRobotic Process Automation, RPAï¼‰æ‰§è¡Œåç»­ä»»åŠ¡ï¼Œå¦‚è‡ªåŠ¨æäº¤æåè¡¨å•ã€‚é€šè¿‡å¯¹æ¥è‡ªäº”ä¸ªä¸åŒä¼šè®®çš„ 586 ç¯‡è®ºæ–‡è¿›è¡ŒéªŒè¯ï¼Œè¯¥ç³»ç»ŸæˆåŠŸè¯†åˆ«äº†æ‰€æœ‰ç›®æ ‡è®ºæ–‡ï¼Œå®ç°äº† 100% çš„å¬å›ç‡ï¼ˆRecallï¼‰å’Œ 99.4% çš„è¿‘ä¹å®Œç¾çš„å‡†ç¡®ç‡ï¼ˆAccuracyï¼‰ã€‚è¿™ä¸€æˆæœä¸ä»…è¯æ˜äº†ä»»åŠ¡å¯¼å‘å‹ï¼ˆTask-orientedï¼‰AI Agent å…·å¤‡é«˜æ•ˆçš„ä¿¡æ¯è¿‡æ»¤èƒ½åŠ›ï¼Œæ›´å±•ç¤ºäº†å…¶é€šè¿‡ä¸»åŠ¨å‚ä¸å¹¶åŠ é€Ÿå­¦æœ¯å·¥ä½œæµæ¥èµ‹èƒ½ç§‘ç ”ç¤¾åŒºçš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "5 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.09470v1",
      "published_date": "2025-09-11 13:52:52 UTC",
      "updated_date": "2025-09-11 13:52:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:56:51.156983+00:00"
    },
    {
      "arxiv_id": "2509.09469v1",
      "title": "Resource-Efficient Glioma Segmentation on Sub-Saharan MRI",
      "title_zh": "é’ˆå¯¹ Sub-Saharan MRI çš„èµ„æºé«˜æ•ˆå‹èƒ¶è´¨ç˜¤åˆ†å‰²",
      "authors": [
        "Freedmore Sidume",
        "Oumayma Soula",
        "Joseph Muthui Wacira",
        "YunFei Zhu",
        "Abbas Rabiu Muhammad",
        "Abderrazek Zeraii",
        "Oluwaseun Kalejaye",
        "Hajer Ibrahim",
        "Olfa Gaddour",
        "Brain Halubanza",
        "Dong Zhang",
        "Udunna C Anazodo",
        "Confidence Raymond"
      ],
      "abstract": "Gliomas are the most prevalent type of primary brain tumors, and their accurate segmentation from MRI is critical for diagnosis, treatment planning, and longitudinal monitoring. However, the scarcity of high-quality annotated imaging data in Sub-Saharan Africa (SSA) poses a significant challenge for deploying advanced segmentation models in clinical workflows. This study introduces a robust and computationally efficient deep learning framework tailored for resource-constrained settings. We leveraged a 3D Attention UNet architecture augmented with residual blocks and enhanced through transfer learning from pre-trained weights on the BraTS 2021 dataset. Our model was evaluated on 95 MRI cases from the BraTS-Africa dataset, a benchmark for glioma segmentation in SSA MRI data. Despite the limited data quality and quantity, our approach achieved Dice scores of 0.76 for the Enhancing Tumor (ET), 0.80 for Necrotic and Non-Enhancing Tumor Core (NETC), and 0.85 for Surrounding Non-Functional Hemisphere (SNFH). These results demonstrate the generalizability of the proposed model and its potential to support clinical decision making in low-resource settings. The compact architecture, approximately 90 MB, and sub-minute per-volume inference time on consumer-grade hardware further underscore its practicality for deployment in SSA health systems. This work contributes toward closing the gap in equitable AI for global health by empowering underserved regions with high-performing and accessible medical imaging solutions.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹æ’’å“ˆæ‹‰ä»¥å—éæ´² (Sub-Saharan Africa) åœ°åŒºé«˜è´¨é‡æ ‡æ³¨ MRI æ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§ç¨³å¥ä¸”è®¡ç®—é«˜æ•ˆçš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°èµ„æºå—é™ç¯å¢ƒä¸‹çš„èƒ¶è´¨ç˜¤ (Glioma) è‡ªåŠ¨åˆ†å‰²ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†é›†æˆäº†æ®‹å·®å— (residual blocks) çš„ 3D Attention UNet æ¶æ„ï¼Œå¹¶åˆ©ç”¨ BraTS 2021 æ•°æ®é›†çš„é¢„è®­ç»ƒæƒé‡é€šè¿‡è¿ç§»å­¦ä¹  (transfer learning) è¿›ä¸€æ­¥å¢å¼ºæ¨¡å‹æ€§èƒ½ã€‚åœ¨ BraTS-Africa åŸºå‡†æ•°æ®é›†çš„ 95 ä¾‹ç—…ä¾‹ä¸Šï¼Œè¯¥æ¨¡å‹å¯¹å¢å¼ºè‚¿ç˜¤ (ET)ã€åæ­»ä¸éå¢å¼ºè‚¿ç˜¤æ ¸å¿ƒ (NETC) åŠå‘¨å›´éåŠŸèƒ½åŠçƒ (SNFH) çš„ Dice åˆ†æ•°åˆ†åˆ«è¾¾åˆ°äº† 0.76ã€0.80 å’Œ 0.85ã€‚å‡­å€Ÿçº¦ 90 MB çš„ç´§å‡‘æ¶æ„å’Œåœ¨æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šå•æ¬¡ä½“ç§¯ä¸è¶³ä¸€åˆ†é’Ÿçš„æ¨ç†æ—¶é—´ï¼Œè¯¥æ–¹æ¡ˆæ˜¾è‘—æå‡äº†åœ¨ SSA åŒ»ç–—ç³»ç»Ÿéƒ¨ç½²çš„å®ç”¨æ€§ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†è¯¥æ¨¡å‹åœ¨ä½èµ„æºç¯å¢ƒä¸‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºç¼©å°å…¨çƒå¥åº·é¢†åŸŸ AI æŠ€æœ¯è·å–çš„ä¸å¹³ç­‰å·®è·å¹¶æ”¯æŒä¸´åºŠå†³ç­–åšå‡ºäº†è´¡çŒ®ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.09469v1",
      "published_date": "2025-09-11 13:52:47 UTC",
      "updated_date": "2025-09-11 13:52:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:57:00.159069+00:00"
    },
    {
      "arxiv_id": "2509.09467v1",
      "title": "Inteligencia Artificial jurÃ­dica y el desafÃ­o de la veracidad: anÃ¡lisis de alucinaciones, optimizaciÃ³n de RAG y principios para una integraciÃ³n responsable",
      "title_zh": "æ³•å¾‹äººå·¥æ™ºèƒ½ä¸çœŸå®æ€§æŒ‘æˆ˜ï¼šå¹»è§‰åˆ†æã€RAG ä¼˜åŒ–åŠè´Ÿè´£ä»»é›†æˆåŸåˆ™",
      "authors": [
        "Alex Dantart"
      ],
      "abstract": "This technical report analyzes the challenge of \"hallucinations\" (false information) in LLMs applied to law. It examines their causes, manifestations, and the effectiveness of the RAG mitigation strategy, highlighting its limitations and proposing holistic optimizations. The paper explores the ethical and regulatory implications, emphasizing human oversight as an irreplaceable role. It concludes that the solution lies not in incrementally improving generative models, but in adopting a \"consultative\" AI paradigm that prioritizes veracity and traceability, acting as a tool to amplify, not replace, professional judgment.\n  --\n  Este informe tÃ©cnico analiza el desafÃ­o de las \"alucinaciones\" (informaciÃ³n falsa) en los LLMs aplicados al derecho. Se examinan sus causas, manifestaciones y la efectividad de la estrategia de mitigaciÃ³n RAG, exponiendo sus limitaciones y proponiendo optimizaciones holÃ­sticas. Se exploran las implicaciones Ã©ticas y regulatorias, enfatizando la supervisiÃ³n humana como un rol insustituible. El documento concluye que la soluciÃ³n no reside en mejorar incrementalmente los modelos generativos, sino en adoptar un paradigma de IA \"consultiva\" que priorice la veracidad y la trazabilidad, actuando como una herramienta para amplificar, y no sustituir, el juicio profesional.",
      "tldr_zh": "è¯¥ç ”ç©¶åˆ†æäº† LLMs åœ¨æ³•å¾‹é¢†åŸŸåº”ç”¨æ—¶é¢ä¸´çš„ \"hallucinations\" æŒ‘æˆ˜ï¼Œæ¢è®¨äº†å…¶æˆå› ã€è¡¨ç°å½¢å¼ä»¥åŠ RAG ç¼“è§£ç­–ç•¥çš„æœ‰æ•ˆæ€§åŠå…¶å±€é™æ€§ã€‚è®ºæ–‡é’ˆå¯¹è¿™äº›æŒ‘æˆ˜æå‡ºäº†æ•´ä½“æ€§çš„ä¼˜åŒ–æ–¹æ¡ˆï¼Œå¹¶æ·±å…¥åˆ†æäº†æ³•å¾‹ AI çš„ä¼¦ç†ä¸ç›‘ç®¡å½±å“ï¼Œå¼ºè°ƒäº† \"human oversight\" åœ¨æ³•å¾‹å®åŠ¡ä¸­ä¸å¯æ›¿ä»£çš„ä½œç”¨ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œæ³•å¾‹ AI çš„å‘å±•ä¸åº”ä»…åœç•™åœ¨æ”¹è¿›ç”Ÿæˆæ¨¡å‹ï¼Œè€Œåº”è½¬å‘ä¼˜å…ˆè€ƒè™‘ \"veracity\" å’Œ \"traceability\" çš„ \"consultative\" AI èŒƒå¼ã€‚æœ€ç»ˆï¼Œè¯¥ç ”ç©¶å€¡å¯¼å°†äººå·¥æ™ºèƒ½ä½œä¸ºå¢å¼ºè€Œéå–ä»£æ³•å¾‹ä¸“ä¸šåˆ¤æ–­çš„å·¥å…·ï¼Œé€šè¿‡ç¡®ä¿ä¿¡æ¯çš„å‡†ç¡®ä¸é€æ˜ï¼Œå®ç° AI æŠ€æœ¯åœ¨å¸æ³•é¢†åŸŸçš„è´Ÿè´£ä»»é›†æˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "in Spanish and English languages",
      "pdf_url": "https://arxiv.org/pdf/2509.09467v1",
      "published_date": "2025-09-11 13:50:23 UTC",
      "updated_date": "2025-09-11 13:50:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:57:04.072335+00:00"
    },
    {
      "arxiv_id": "2509.09750v1",
      "title": "A Co-Training Semi-Supervised Framework Using Faster R-CNN and YOLO Networks for Object Detection in Densely Packed Retail Images",
      "title_zh": "ä¸€ç§é¢å‘é›¶å”®åœºæ™¯å¯†é›†å›¾åƒç›®æ ‡æ£€æµ‹çš„ Faster R-CNN ä¸ YOLO ååŒè®­ç»ƒåŠç›‘ç£æ¡†æ¶",
      "authors": [
        "Hossein Yazdanjouei",
        "Arash Mansouri",
        "Mohammad Shokouhifar"
      ],
      "abstract": "This study proposes a semi-supervised co-training framework for object detection in densely packed retail environments, where limited labeled data and complex conditions pose major challenges. The framework combines Faster R-CNN (utilizing a ResNet backbone) for precise localization with YOLO (employing a Darknet backbone) for global context, enabling mutual pseudo-label exchange that improves accuracy in scenes with occlusion and overlapping objects. To strengthen classification, it employs an ensemble of XGBoost, Random Forest, and SVM, utilizing diverse feature representations for higher robustness. Hyperparameters are optimized using a metaheuristic-driven algorithm, enhancing precision and efficiency across models. By minimizing reliance on manual labeling, the approach reduces annotation costs and adapts effectively to frequent product and layout changes common in retail. Experiments on the SKU-110k dataset demonstrate strong performance, highlighting the scalability and practicality of the proposed framework for real-world retail applications such as automated inventory tracking, product monitoring, and checkout systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹å¯†é›†åŒ…è£…é›¶å”®ç¯å¢ƒçš„ç›®æ ‡æ£€æµ‹åŠç›‘ç£ååŒè®­ç»ƒ (Co-Training) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ ‡æ³¨æ•°æ®æœ‰é™ä»¥åŠé›¶å”®åœºæ™¯ä¸­å¤æ‚çš„é®æŒ¡å’Œé‡å é—®é¢˜ã€‚è¯¥æ¡†æ¶ç»“åˆäº†åˆ©ç”¨ ResNet éª¨å¹²ç½‘å®ç°ç²¾ç¡®å®šä¹‰çš„ Faster R-CNN å’Œåˆ©ç”¨ Darknet éª¨å¹²ç½‘æ•æ‰å…¨å±€ä¸Šä¸‹æ–‡çš„ YOLO ç½‘ç»œï¼Œé€šè¿‡ç›¸äº’äº¤æ¢ä¼ªæ ‡ç­¾ (Pseudo-label) æ¥æå‡æ£€æµ‹ç²¾åº¦ã€‚ä¸ºäº†å¼ºåŒ–åˆ†ç±»èƒ½åŠ›ï¼Œç³»ç»Ÿé›†æˆäº† XGBoostã€Random Forest å’Œ SVM çš„é›†æˆå­¦ä¹ æ¨¡å‹ï¼Œå¹¶é‡‡ç”¨å…ƒå¯å‘å¼é©±åŠ¨ç®—æ³• (Metaheuristic-driven algorithm) ä¼˜åŒ–è¶…å‚æ•°ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆé™ä½äº†äººå·¥æ ‡æ³¨æˆæœ¬ï¼Œèƒ½å¤Ÿçµæ´»åº”å¯¹é›¶å”®åœºæ™¯ä¸­é¢‘ç¹çš„å•†å“å˜åŠ¨å’Œå¸ƒå±€æ›´æ–°ã€‚åœ¨ SKU-110k æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†è¯¥æ¡†æ¶å¼ºå¤§çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶åœ¨è‡ªåŠ¨åŒ–åº“å­˜è¿½è¸ªã€å•†å“ç›‘æ§å’Œç»“è´¦ç³»ç»Ÿç­‰å®é™…é›¶å”®åº”ç”¨ä¸­çš„å¯æ‰©å±•æ€§ä¸å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.09750v1",
      "published_date": "2025-09-11 13:40:43 UTC",
      "updated_date": "2025-09-11 13:40:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:57:00.757289+00:00"
    },
    {
      "arxiv_id": "2509.09448v3",
      "title": "TORSO: Template-Oriented Reasoning Towards General Tasks",
      "title_zh": "TORSOï¼šé¢å‘é€šç”¨ä»»åŠ¡çš„æ¨¡æ¿å¯¼å‘æ¨ç†",
      "authors": [
        "Minhyuk Kim",
        "Seungyoon Lee",
        "Heuiseok Lim"
      ],
      "abstract": "The approaches that guide Large Language Models (LLMs) to emulate human reasoning during response generation have emerged as an effective method for enabling them to solve complex problems in a step-by-step manner, thereby achieving superior performance. However, most existing approaches using few-shot prompts to generate responses heavily depend on the provided examples, limiting the utilization of the model's inherent reasoning capabilities. Moreover, constructing task-specific few-shot prompts is often costly and may lead to inconsistencies across different tasks. In this work, we introduce Template-Oriented Reasoning (TORSO), which elicits the model to utilize internal reasoning abilities to generate proper responses across various tasks without the need for manually crafted few-shot examples. Our experimental results demonstrate that TORSO achieves strong performance on diverse LLMs benchmarks with reasonable rationales.",
      "tldr_zh": "è¯¥é¡¹ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³å¤æ‚é—®é¢˜æ—¶è¿‡åº¦ä¾èµ–å°‘æ ·æœ¬æç¤ºï¼ˆfew-shot promptsï¼‰ä»¥åŠç”±æ­¤å¯¼è‡´çš„äººå·¥æˆæœ¬é«˜æ˜‚å’Œä»»åŠ¡é—´ä¸ä¸€è‡´æ€§ç­‰é—®é¢˜ï¼Œæå‡ºäº†é¢å‘æ¨¡æ¿çš„æ¨ç†æ¡†æ¶ï¼ˆTemplate-Oriented Reasoning, TORSOï¼‰ã€‚TORSO æ—¨åœ¨å¼•å¯¼æ¨¡å‹åœ¨æ— éœ€äººå·¥ç¼–å†™å°‘æ ·æœ¬ç¤ºä¾‹çš„æƒ…å†µä¸‹ï¼Œå……åˆ†æ¿€å‘å…¶å†…åœ¨çš„æ¨ç†èƒ½åŠ›ï¼ˆinternal reasoning abilitiesï¼‰æ¥ç”Ÿæˆå„ç±»ä»»åŠ¡çš„å“åº”ã€‚é€šè¿‡é‡‡ç”¨è¿™ä¸€é€šç”¨æ¡†æ¶ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨å¤„ç†ä¸åŒä»»åŠ¡æ—¶ä¿æŒæ›´å¼ºçš„ä¸€è‡´æ€§å¹¶æ˜¾è‘—é™ä½å¯¹ç‰¹å®šç¤ºä¾‹çš„ä¾èµ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTORSO åœ¨å¤šä¸ªå¤šæ ·åŒ–çš„ LLMs åŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½è¡¨ç°ï¼Œå¹¶èƒ½ç”Ÿæˆé€»è¾‘åˆç†çš„æ¨ç†è¿‡ç¨‹ã€‚è¯¥ç ”ç©¶ä¸ºæå‡å¤§æ¨¡å‹åœ¨é€šç”¨ä»»åŠ¡ä¸Šçš„è‡ªä¸»æ¨ç†èƒ½åŠ›æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”ä½æˆæœ¬çš„æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to EMNLP 2025 Main Conference",
      "pdf_url": "https://arxiv.org/pdf/2509.09448v3",
      "published_date": "2025-09-11 13:31:35 UTC",
      "updated_date": "2025-09-15 08:09:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:57:02.270156+00:00"
    },
    {
      "arxiv_id": "2509.13338v1",
      "title": "Proximity-Based Evidence Retrieval for Uncertainty-Aware Neural Networks",
      "title_zh": "åŸºäºé‚»è¿‘æ€§çš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥ç¥ç»ç½‘ç»œè¯æ®æ£€ç´¢",
      "authors": [
        "Hassan Gharoun",
        "Mohammad Sadegh Khorshidi",
        "Kasra Ranjbarigderi",
        "Fang Chen",
        "Amir H. Gandomi"
      ],
      "abstract": "This work proposes an evidence-retrieval mechanism for uncertainty-aware decision-making that replaces a single global cutoff with an evidence-conditioned, instance-adaptive criterion. For each test instance, proximal exemplars are retrieved in an embedding space; their predictive distributions are fused via Dempster-Shafer theory. The resulting fused belief acts as a per-instance thresholding mechanism. Because the supporting evidences are explicit, decisions are transparent and auditable. Experiments on CIFAR-10/100 with BiT and ViT backbones show higher or comparable uncertainty-aware performance with materially fewer confidently incorrect outcomes and a sustainable review load compared with applying threshold on prediction entropy. Notably, only a few evidences are sufficient to realize these gains; increasing the evidence set yields only modest changes. These results indicate that evidence-conditioned tagging provides a more reliable and interpretable alternative to fixed prediction entropy thresholds for operational uncertainty-aware decision-making.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º Proximity-Based Evidence Retrieval çš„è¯æ®æ£€ç´¢æœºåˆ¶ï¼Œç”¨äºå¢å¼ºç¥ç»ç½‘ç»œçš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥ (uncertainty-aware) å†³ç­–èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡å®ä¾‹è‡ªé€‚åº” (instance-adaptive) çš„å‡†åˆ™å–ä»£äº†ä¼ ç»Ÿçš„å…¨å±€ç»Ÿä¸€é˜ˆå€¼ï¼Œåœ¨åµŒå…¥ç©ºé—´ä¸­æ£€ç´¢æµ‹è¯•å®ä¾‹çš„é‚»è¿‘æ ·æœ¬ (proximal exemplars)ï¼Œå¹¶é‡‡ç”¨ Dempster-Shafer theory èåˆå…¶é¢„æµ‹åˆ†å¸ƒä»¥ç”Ÿæˆæ¯ä¸ªå®ä¾‹çš„åˆ¤å®šé˜ˆå€¼ã€‚ç”±äºå†³ç­–ä¾èµ–äºæ˜¾å¼çš„æ”¯æŒè¯æ®ï¼Œè¯¥ç³»ç»Ÿè¡¨ç°å‡ºæé«˜çš„é€æ˜åº¦ä¸å¯å®¡è®¡æ€§ã€‚åœ¨ CIFAR-10/100 æ•°æ®é›†ä¸Šåˆ©ç”¨ BiT å’Œ ViT éª¨å¹²ç½‘ç»œè¿›è¡Œçš„å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•ç›¸æ¯”äºåŸºäºé¢„æµ‹ç†µ (prediction entropy) çš„é˜ˆå€¼æ‰‹æ®µï¼Œèƒ½æ˜¾è‘—å‡å°‘â€œè‡ªä¿¡é”™è¯¯â€ (confidently incorrect) çš„è¾“å‡ºï¼Œå¹¶ç»´æŒåˆç†çš„å®¡æ ¸è´Ÿè½½ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä»…éœ€å°‘é‡çš„è¯æ®å³å¯è·å¾—æ€§èƒ½æå‡ï¼Œä¸ºå·¥ä¸šçº§ä¸ç¡®å®šæ€§æ„ŸçŸ¥å†³ç­–æä¾›äº†ä¸€ä¸ªæ¯”å›ºå®šé˜ˆå€¼æ›´å¯é ä¸”å…·å¤‡å¯è§£é‡Šæ€§çš„æ›¿ä»£æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "cs.CV",
      "comment": "15 pages, 4 figures, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.13338v1",
      "published_date": "2025-09-11 13:12:22 UTC",
      "updated_date": "2025-09-11 13:12:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:57:08.870630+00:00"
    },
    {
      "arxiv_id": "2509.09424v1",
      "title": "ENSI: Efficient Non-Interactive Secure Inference for Large Language Models",
      "title_zh": "ENSIï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹çš„é«˜æ•ˆéäº¤äº’å¼å®‰å…¨æ¨ç†",
      "authors": [
        "Zhiyu He",
        "Maojiang Wang",
        "Xinwen Gao",
        "Yuchuan Luo",
        "Lin Liu",
        "Shaojing Fu"
      ],
      "abstract": "Secure inference enables privacy-preserving machine learning by leveraging cryptographic protocols that support computations on sensitive user data without exposing it. However, integrating cryptographic protocols with large language models (LLMs) presents significant challenges, as the inherent complexity of these protocols, together with LLMs' massive parameter scale and sophisticated architectures, severely limits practical usability. In this work, we propose ENSI, a novel non-interactive secure inference framework for LLMs, based on the principle of co-designing the cryptographic protocols and LLM architecture. ENSI employs an optimized encoding strategy that seamlessly integrates CKKS scheme with a lightweight LLM variant, BitNet, significantly reducing the computational complexity of encrypted matrix multiplications. In response to the prohibitive computational demands of softmax under homomorphic encryption (HE), we pioneer the integration of the sigmoid attention mechanism with HE as a seamless, retraining-free alternative. Furthermore, by embedding the Bootstrapping operation within the RMSNorm process, we efficiently refresh ciphertexts while markedly decreasing the frequency of costly bootstrapping invocations. Experimental evaluations demonstrate that ENSI achieves approximately an 8x acceleration in matrix multiplications and a 2.6x speedup in softmax inference on CPU compared to state-of-the-art method, with the proportion of bootstrapping is reduced to just 1%.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ENSIï¼Œä¸€ä¸ªé’ˆå¯¹Large Language Models (LLMs) çš„é«˜æ•ˆéäº¤äº’å¼å®‰å…¨æ¨ç†æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åŠ å¯†åè®®åœ¨å¤„ç†å¤§è§„æ¨¡å‚æ•°æ¨¡å‹æ—¶é¢ä¸´çš„è®¡ç®—å¤æ‚åº¦å’Œå¯ç”¨æ€§æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†ååŒè®¾è®¡åŠ å¯†åè®®ä¸LLMæ¶æ„çš„åŸåˆ™ï¼Œé€šè¿‡ä¼˜åŒ–ç¼–ç ç­–ç•¥å°†CKKSæ–¹æ¡ˆä¸è½»é‡åŒ–æ¨¡å‹BitNetæ— ç¼ç»“åˆï¼Œæ˜¾è‘—é™ä½äº†åŠ å¯†çŸ©é˜µä¹˜æ³•çš„è®¡ç®—è´Ÿæ‹…ã€‚é’ˆå¯¹åŒæ€åŠ å¯† (Homomorphic Encryption, HE) ä¸‹Softmaxè®¡ç®—éœ€æ±‚è¿‡é«˜çš„é—®é¢˜ï¼Œç ”ç©¶è€…å¼€åˆ›æ€§åœ°é›†æˆäº†Sigmoid Attentionæœºåˆ¶ä½œä¸ºæ— éœ€é‡è®­ç»ƒçš„æ›¿ä»£æ–¹æ¡ˆã€‚æ­¤å¤–ï¼ŒENSIé€šè¿‡å°†Bootstrappingæ“ä½œåµŒå…¥RMSNormè¿‡ç¨‹ï¼Œåœ¨é«˜æ•ˆåˆ·æ–°å¯†æ–‡çš„åŒæ—¶å°†æ˜‚è´µçš„è°ƒç”¨é¢‘ç‡é™ä½è‡³ä»…1%ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒENSIåœ¨CPUä¸Šçš„çŸ©é˜µä¹˜æ³•é€Ÿåº¦æå‡äº†çº¦8å€ï¼ŒSoftmaxæ¨ç†é€Ÿåº¦æå‡äº†2.6å€ï¼Œå¤§å¹…æå‡äº†å®‰å…¨æ¨ç†åœ¨LLMé¢†åŸŸçš„å®ç”¨æ€§ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.09424v1",
      "published_date": "2025-09-11 13:04:22 UTC",
      "updated_date": "2025-09-11 13:04:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:57:22.588353+00:00"
    },
    {
      "arxiv_id": "2509.09414v1",
      "title": "We're Still Doing It (All) Wrong: Recommender Systems, Fifteen Years Later",
      "title_zh": "æˆ‘ä»¬ä»åœ¨é‡è¹ˆè¦†è¾™ï¼šæ¨èç³»ç»Ÿåäº”å¹´åçš„ç°çŠ¶ä¸åæ€",
      "authors": [
        "Alan Said",
        "Maria Soledad Pera",
        "Michael D. Ekstrand"
      ],
      "abstract": "In 2011, Xavier Amatriain sounded the alarm: recommender systems research was \"doing it all wrong\" [1]. His critique, rooted in statistical misinterpretation and methodological shortcuts, remains as relevant today as it was then. But rather than correcting course, we added new layers of sophistication on top of the same broken foundations. This paper revisits Amatriain's diagnosis and argues that many of the conceptual, epistemological, and infrastructural failures he identified still persist, in more subtle or systemic forms. Drawing on recent work in reproducibility, evaluation methodology, environmental impact, and participatory design, we showcase how the field's accelerating complexity has outpaced its introspection. We highlight ongoing community-led initiatives that attempt to shift the paradigm, including workshops, evaluation frameworks, and calls for value-sensitive and participatory research. At the same time, we contend that meaningful change will require not only new metrics or better tooling, but a fundamental reframing of what recommender systems research is for, who it serves, and how knowledge is produced and validated. Our call is not just for technical reform, but for a recommender systems research agenda grounded in epistemic humility, human impact, and sustainable practice.",
      "tldr_zh": "è¯¥ç ”ç©¶å›é¡¾äº† Xavier Amatriain åœ¨ 2011 å¹´å¯¹æ¨èç³»ç»Ÿ (Recommender Systems) ç ”ç©¶é¢†åŸŸæå‡ºçš„æ‰¹è¯„ï¼ŒæŒ‡å‡ºåäº”å¹´åè¯¥é¢†åŸŸä¾ç„¶é¢ä¸´ç»Ÿè®¡è¯¯è¯»å’Œæ–¹æ³•è®ºæ·å¾„ç­‰æ ¸å¿ƒé—®é¢˜ã€‚ä½œè€…è®¤ä¸ºï¼Œå°½ç®¡æŠ€æœ¯å¤æ‚æ€§ä¸æ–­å¢åŠ ï¼Œä½†æ¦‚å¿µã€è®¤è¯†è®º (epistemological) å’ŒåŸºç¡€è®¾æ–½æ–¹é¢çš„å¤±æ•ˆåœ¨æ›´æ·±å±‚å’Œç³»ç»Ÿæ€§çš„å½¢å¼ä¸­æŒç»­å­˜åœ¨ã€‚æ–‡ç« ç»“åˆå¯é‡å¤æ€§ (reproducibility)ã€è¯„ä¼°æ–¹æ³•è®ºã€ç¯å¢ƒå½±å“åŠå‚ä¸å¼è®¾è®¡ (participatory design) ç­‰æ–¹é¢çš„æœ€æ–°ç ”ç©¶ï¼Œæ­ç¤ºäº†è¯¥é¢†åŸŸçš„æŠ€æœ¯æ¼”è¿›å·²è¶…è¿‡äº†å…¶è‡ªæˆ‘åæ€çš„é€Ÿåº¦ã€‚å°½ç®¡ç¤¾åŒºæ­£é€šè¿‡è¯„ä¼°æ¡†æ¶å’Œä»·å€¼æ•æ„Ÿå‹ç ”ç©¶å°è¯•èŒƒå¼è½¬å˜ï¼Œä½†ç ”ç©¶å¼ºè°ƒè¿™éœ€è¦å¯¹æ¨èç³»ç»Ÿç ”ç©¶çš„ç›®æ ‡ã€æœåŠ¡å¯¹è±¡å’ŒçŸ¥è¯†ç”Ÿäº§æ–¹å¼è¿›è¡Œæ ¹æœ¬æ€§é‡æ„ã€‚æœ€åï¼Œè¯¥è®ºæ–‡å‘¼åå»ºç«‹ä¸€ä¸ªä»¥è®¤è¯†è®ºè°¦é€Š (epistemic humility)ã€äººç±»å½±å“å’Œå¯æŒç»­å®è·µä¸ºåŸºç¡€çš„æ¨èç³»ç»Ÿç ”ç©¶è®®ç¨‹ï¼Œè€Œéä»…ä»…åœç•™äºæŠ€æœ¯æ”¹è‰¯ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "This is the author's version of the work. It is posted here for your personal use. Not for redistribution. The definitive Version of Record was accepted for publication in the Beyond Algorithms: Reclaiming the Interdisciplinary Roots of Recommender Systems Workshop (BEYOND 2025), September 26th, 2025, co-located with the 19th ACM Recommender Systems Conference, Prague, Czech Republic",
      "pdf_url": "https://arxiv.org/pdf/2509.09414v1",
      "published_date": "2025-09-11 12:51:37 UTC",
      "updated_date": "2025-09-11 12:51:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:57:31.591753+00:00"
    },
    {
      "arxiv_id": "2509.09396v1",
      "title": "LLMs Don't Know Their Own Decision Boundaries: The Unreliability of Self-Generated Counterfactual Explanations",
      "title_zh": "LLM å¹¶ä¸äº†è§£è‡ªèº«çš„å†³ç­–è¾¹ç•Œï¼šè‡ªç”Ÿæˆåäº‹å®è§£é‡Šçš„ä¸å¯é æ€§",
      "authors": [
        "Harry Mayne",
        "Ryan Othniel Kearns",
        "Yushi Yang",
        "Andrew M. Bean",
        "Eoin Delaney",
        "Chris Russell",
        "Adam Mahdi"
      ],
      "abstract": "To collaborate effectively with humans, language models must be able to explain their decisions in natural language. We study a specific type of self-explanation: self-generated counterfactual explanations (SCEs), where a model explains its prediction by modifying the input such that it would have predicted a different outcome. We evaluate whether LLMs can produce SCEs that are valid, achieving the intended outcome, and minimal, modifying the input no more than necessary. When asked to generate counterfactuals, we find that LLMs typically produce SCEs that are valid, but far from minimal, offering little insight into their decision-making behaviour. Worryingly, when asked to generate minimal counterfactuals, LLMs typically make excessively small edits that fail to change predictions. The observed validity-minimality trade-off is consistent across several LLMs, datasets, and evaluation settings. Our findings suggest that SCEs are, at best, an ineffective explainability tool and, at worst, can provide misleading insights into model behaviour. Proposals to deploy LLMs in high-stakes settings must consider the impact of unreliable self-explanations on downstream decision-making. Our code is available at https://github.com/HarryMayne/SCEs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)é€šè¿‡è‡ªç”Ÿæˆçš„åäº‹å®è§£é‡Š(self-generated counterfactual explanations, SCEs)æ¥è§£é‡Šå…¶å†³ç­–çš„èƒ½åŠ›ï¼Œå³é€šè¿‡ä¿®æ”¹è¾“å…¥ä½¿é¢„æµ‹ç»“æœå‘ç”Ÿæ”¹å˜ã€‚ç ”ç©¶è€…é‡ç‚¹è¯„ä¼°äº†SCEsæ˜¯å¦å…·å¤‡æœ‰æ•ˆæ€§(valid)å’Œæœ€å°æ€§(minimal)ï¼Œå‘ç°LLMsç”Ÿæˆçš„è§£é‡Šåœ¨ä¸¤è€…ä¹‹é—´å­˜åœ¨æ˜æ˜¾çš„æƒè¡¡ã€‚å½“LLMsç”Ÿæˆæœ‰æ•ˆçš„åäº‹å®è§£é‡Šæ—¶ï¼Œå…¶ä¿®æ”¹å¹…åº¦é€šå¸¸è¿œéæœ€å°ï¼Œæ— æ³•æ­ç¤ºå†³ç­–è¾¹ç•Œï¼›è€Œå½“è¢«è¦æ±‚è¿›è¡Œæœ€å°ä¿®æ”¹æ—¶ï¼Œæ¨¡å‹å¾€å¾€ä¼šåšå‡ºæå°çš„ç¼–è¾‘ï¼Œå¯¼è‡´æ— æ³•æ”¹å˜åŸæœ‰çš„é¢„æµ‹ç»“æœã€‚è¿™ç§ç°è±¡åœ¨ä¸åŒçš„æ¨¡å‹ã€æ•°æ®é›†å’Œè¯„ä¼°è®¾ç½®ä¸­å‡å…·æœ‰ä¸€è‡´æ€§ï¼Œè¡¨æ˜LLMså¹¶ä¸äº†è§£è‡ªèº«çš„å†³ç­–è¾¹ç•Œã€‚ç ”ç©¶æŒ‡å‡ºï¼ŒSCEsç›®å‰ä½œä¸ºå¯è§£é‡Šæ€§å·¥å…·æ•ˆæœä¸ä½³ï¼Œç”šè‡³å¯èƒ½æä¾›è¯¯å¯¼æ€§çš„æ¨¡å‹è¡Œä¸ºä¿¡æ¯ã€‚å› æ­¤ï¼Œè¯¥å‘ç°æé†’åœ¨æ¶‰åŠé«˜é£é™©çš„å†³ç­–åœºæ™¯ä¸­éƒ¨ç½²LLMsæ—¶ï¼Œå¿…é¡»å®¡æ…è€ƒè™‘è¿™äº›ä¸å¯é çš„è‡ªæˆ‘è§£é‡Šå¯¹ä¸‹æ¸¸å†³ç­–å¸¦æ¥çš„é£é™©ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to EMNLP 2025 Main",
      "pdf_url": "https://arxiv.org/pdf/2509.09396v1",
      "published_date": "2025-09-11 12:25:41 UTC",
      "updated_date": "2025-09-11 12:25:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:57:33.083799+00:00"
    },
    {
      "arxiv_id": "2509.09387v3",
      "title": "MetaLLMix : An XAI Aided LLM-Meta-learning Based Approach for Hyper-parameters Optimization",
      "title_zh": "MetaLLMixï¼šä¸€ç§ç”± XAI è¾…åŠ©ã€åŸºäº LLM å…ƒå­¦ä¹ çš„è¶…å‚æ•°ä¼˜åŒ–æ–¹æ³•",
      "authors": [
        "Mohamed Bal-Ghaoui",
        "Mohammed Tiouti"
      ],
      "abstract": "Effective model and hyperparameter selection remains a major challenge in deep learning, often requiring extensive expertise and computation. While AutoML and large language models (LLMs) promise automation, current LLM-based approaches rely on trial and error and expensive APIs, which provide limited interpretability and generalizability. We propose MetaLLMiX, a zero-shot hyperparameter optimization framework combining meta-learning, explainable AI, and efficient LLM reasoning. By leveraging historical experiment outcomes with SHAP explanations, MetaLLMiX recommends optimal hyperparameters and pretrained models without additional trials. We further employ an LLM-as-judge evaluation to control output format, accuracy, and completeness. Experiments on eight medical imaging datasets using nine open-source lightweight LLMs show that MetaLLMiX achieves competitive or superior performance to traditional HPO methods while drastically reducing computational cost. Our local deployment outperforms prior API-based approaches, achieving optimal results on 5 of 8 tasks, response time reductions of 99.6-99.9%, and the fastest training times on 6 datasets (2.4-15.7x faster), maintaining accuracy within 1-5% of best-performing baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MetaLLMixï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆäº†å…ƒå­¦ä¹ (Meta-learning)ã€å¯è§£é‡Šäººå·¥æ™ºèƒ½(XAI)å’Œé«˜æ•ˆå¤§è¯­è¨€æ¨¡å‹(LLM)æ¨ç†çš„é›¶æ ·æœ¬(Zero-shot)è¶…å‚æ•°ä¼˜åŒ–æ¡†æ¶ã€‚MetaLLMixåˆ©ç”¨å¸¦æœ‰SHAPè§£é‡Šçš„å†å²å®éªŒç»“æœï¼Œåœ¨æ— éœ€é¢å¤–è¯•é”™çš„æƒ…å†µä¸‹æ¨èæœ€ä¼˜è¶…å‚æ•°å’Œé¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶å¼•å…¥LLM-as-judgeæœºåˆ¶æ¥è¯„ä¼°è¾“å‡ºçš„å‡†ç¡®æ€§ä¸å®Œæ•´æ€§ã€‚åœ¨å…«ä¸ªåŒ»å­¦å½±åƒæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ä½¿ç”¨å¼€æºè½»é‡çº§LLMæ—¶è¡¨ç°å‡ºä¼˜äºæˆ–ç­‰åŒäºä¼ ç»ŸHPOæ–¹æ³•çš„æ€§èƒ½ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ã€‚æœ¬åœ°éƒ¨ç½²çš„MetaLLMixç›¸æ¯”ä¼ ç»ŸAPIæ–¹æ¡ˆå°†å“åº”æ—¶é—´ç¼©çŸ­äº†99.6%-99.9%ï¼Œåœ¨å¤šé¡¹ä»»åŠ¡ä¸­å®ç°äº†2.4-15.7å€çš„è®­ç»ƒåŠ é€Ÿï¼Œä¸”å‡†ç¡®ç‡ç»´æŒåœ¨åŸºå‡†æ¨¡å‹çš„1-5%ä»¥å†…ï¼Œä¸ºé«˜æ•ˆä¸”å¯è§£é‡Šçš„è‡ªåŠ¨åŒ–æœºå™¨å­¦ä¹ æä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.09387v3",
      "published_date": "2025-09-11 12:06:34 UTC",
      "updated_date": "2025-10-07 13:08:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:57:38.962553+00:00"
    },
    {
      "arxiv_id": "2509.09380v1",
      "title": "Robust Non-Linear Correlations via Polynomial Regression",
      "title_zh": "åŸºäºå¤šé¡¹å¼å›å½’çš„é²æ£’éçº¿æ€§ç›¸å…³æ€§",
      "authors": [
        "Luca Giuliani",
        "Michele Lombardi"
      ],
      "abstract": "The Hirschfeld-Gebelein-RÃ©nyi (HGR) correlation coefficient is an extension of Pearson's correlation that is not limited to linear correlations, with potential applications in algorithmic fairness, scientific analysis, and causal discovery. Recently, novel algorithms to estimate HGR in a differentiable manner have been proposed to facilitate its use as a loss regularizer in constrained machine learning applications. However, the inherent uncomputability of HGR requires a bias-variance trade-off, which can possibly compromise the robustness of the proposed methods, hence raising technical concerns if applied in real-world scenarios. We introduce a novel computational approach for HGR that relies on user-configurable polynomial kernels, offering greater robustness compared to previous methods and featuring a faster yet almost equally effective restriction. Our approach provides significant advantages in terms of robustness and determinism, making it a more reliable option for real-world applications. Moreover, we present a brief experimental analysis to validate the applicability of our approach within a constrained machine learning framework, showing that its computation yields an insightful subgradient that can serve as a loss regularizer.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Hirschfeld-Gebelein-RÃ©nyi (HGR) ç›¸å…³ç³»æ•°ï¼Œè¿™ç§ Pearson ç›¸å…³ç³»æ•°çš„æ‰©å±•èƒ½å¤Ÿæ•æ‰éçº¿æ€§ç›¸å…³æ€§ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­å¸¸å› ä¸å¯è®¡ç®—æ€§å¯¼è‡´çš„åç½®-æ–¹å·®æƒè¡¡ (bias-variance trade-off) è€Œç¼ºä¹é²æ£’æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäºç”¨æˆ·å¯é…ç½®å¤šé¡¹å¼æ ¸ (polynomial kernels) çš„æ–°å‹ HGR è®¡ç®—æ–¹æ³•ã€‚è¯¥æ–¹æ³•ä¸ä»…æ¯”ä»¥å¾€ç®—æ³•æ›´å…·ç¨³å¥æ€§å’Œç¡®å®šæ€§ï¼Œè¿˜å¼•å…¥äº†ä¸€ç§æ›´å¿«é€Ÿä¸”é«˜æ•ˆçš„é™åˆ¶æœºåˆ¶ï¼Œä½¿å…¶æ›´é€‚ç”¨äºç°å®ä¸–ç•Œçš„å¤æ‚åœºæ™¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å—é™æœºå™¨å­¦ä¹  (constrained machine learning) æ¡†æ¶ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå…¶è®¡ç®—äº§ç”Ÿçš„æ¬¡æ¢¯åº¦ (subgradient) å¯ä»¥ä½œä¸ºæœ‰æ•ˆçš„æŸå¤±æ­£åˆ™åŒ–é¡¹ (loss regularizer)ã€‚è¿™ä¸€ç ”ç©¶ä¸ºç®—æ³•å…¬å¹³æ€§ (algorithmic fairness)ã€ç§‘å­¦åˆ†æå’Œå› æœå‘ç° (causal discovery) ç­‰é¢†åŸŸæä¾›äº†ä¸€ä¸ªæ›´å¯é ä¸”é«˜æ•ˆçš„éçº¿æ€§ç›¸å…³æ€§åº¦é‡å·¥å…·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.NA"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.09380v1",
      "published_date": "2025-09-11 11:55:48 UTC",
      "updated_date": "2025-09-11 11:55:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:57:41.867266+00:00"
    },
    {
      "arxiv_id": "2509.18127v2",
      "title": "Safe-SAIL: Towards a Fine-grained Safety Landscape of Large Language Models via Sparse Autoencoder Interpretation Framework",
      "title_zh": "Safe-SAILï¼šåŸºäºç¨€ç–è‡ªç¼–ç å™¨è§£é‡Šæ¡†æ¶æ„å»ºå¤§è¯­è¨€æ¨¡å‹ç»†ç²’åº¦çš„å®‰å…¨å›¾æ™¯",
      "authors": [
        "Jiaqi Weng",
        "Han Zheng",
        "Hanyu Zhang",
        "Qinqin He",
        "Jialing Tao",
        "Hui Xue",
        "Zhixuan Chu",
        "Xiting Wang"
      ],
      "abstract": "Increasing deployment of large language models (LLMs) in real-world applications raises significant safety concerns. Most existing safety research focuses on evaluating LLM outputs or specific safety tasks, limiting their ability to address broader, undefined risks. Sparse Autoencoders (SAEs) facilitate interpretability research to clarify model behavior by explaining single-meaning atomic features decomposed from entangled signals. jHowever, prior applications on SAEs do not interpret features with fine-grained safety-related concepts, thus inadequately addressing safety-critical behaviors, such as generating toxic responses and violating safety regulations. For rigorous safety analysis, we must extract a rich and diverse set of safety-relevant features that effectively capture these high-risk behaviors, yet face two challenges: identifying SAEs with the greatest potential for generating safety concept-specific neurons, and the prohibitively high cost of detailed feature explanation. In this paper, we propose Safe-SAIL, a framework for interpreting SAE features within LLMs to advance mechanistic understanding in safety domains. Our approach systematically identifies SAE with best concept-specific interpretability, explains safety-related neurons, and introduces efficient strategies to scale up the interpretation process. We will release a comprehensive toolkit including SAE checkpoints and human-readable neuron explanations, which supports empirical analysis of safety risks to promote research on LLM safety.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Safe-SAILï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºç¨€ç–è‡ªç¼–ç å™¨ (Sparse Autoencoder, SAE) è§£é‡Šæ¡†æ¶çš„å¤§è¯­è¨€æ¨¡å‹ (Large Language Models) å®‰å…¨æ€§åˆ†æç³»ç»Ÿï¼Œæ—¨åœ¨é€šè¿‡æœºç†æ€§è§£é‡Šæå‡å¯¹æ¨¡å‹å®‰å…¨é¢†åŸŸçš„è®¤çŸ¥ã€‚é’ˆå¯¹ç°æœ‰å®‰å…¨ç ”ç©¶å¤šå…³æ³¨è¾“å‡ºç»“æœè€Œå¿½è§†å†…éƒ¨åŸå­ç‰¹å¾çš„é—®é¢˜ï¼ŒSafe-SAIL ç³»ç»Ÿåœ°è¯†åˆ«äº†å…·æœ‰æœ€ä½³æ¦‚å¿µè§£é‡Šæ€§çš„ SAEï¼Œå¹¶æå–å‡ºä¸ç”Ÿæˆæ¯’æ€§å†…å®¹æˆ–è¿åå®‰å…¨è§„å®šç›¸å…³çš„ç»†ç²’åº¦å®‰å…¨ç‰¹å¾ã€‚ä¸ºäº†å…‹æœç‰¹å¾è§£é‡Šæˆæœ¬é«˜æ˜‚çš„æŒ‘æˆ˜ï¼Œç ”ç©¶å¼•å…¥äº†é«˜æ•ˆçš„æ‰©å±•ç­–ç•¥ï¼Œå®ç°äº†å¯¹å¤§é‡å®‰å…¨ç›¸å…³ç¥ç»å…ƒçš„å¤§è§„æ¨¡è‡ªåŠ¨è§£é‡Šã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶å°†å‘å¸ƒåŒ…å« SAE æ£€æŸ¥ç‚¹å’Œäººç±»å¯è¯»ç¥ç»å…ƒè§£é‡Šçš„ç»¼åˆå·¥å…·åŒ…ï¼Œä¸ºå®è¯åˆ†æ LLM å®‰å…¨é£é™©å¹¶ä¿ƒè¿›æ›´æ·±å±‚çš„å®‰å…¨æœºåˆ¶ç ”ç©¶æä¾›äº†é‡è¦æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18127v2",
      "published_date": "2025-09-11 11:22:43 UTC",
      "updated_date": "2025-09-24 03:58:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:57:41.360719+00:00"
    },
    {
      "arxiv_id": "2509.09356v1",
      "title": "Curriculum-Based Multi-Tier Semantic Exploration via Deep Reinforcement Learning",
      "title_zh": "åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„è¯¾ç¨‹åŒ–å¤šå±‚çº§è¯­ä¹‰æ¢ç´¢",
      "authors": [
        "Abdel Hakim Drid",
        "Vincenzo Suriani",
        "Daniele Nardi",
        "Abderrezzak Debilou"
      ],
      "abstract": "Navigating and understanding complex and unknown environments autonomously demands more than just basic perception and movement from embodied agents. Truly effective exploration requires agents to possess higher-level cognitive abilities, the ability to reason about their surroundings, and make more informed decisions regarding exploration strategies. However, traditional RL approaches struggle to balance efficient exploration and semantic understanding due to limited cognitive capabilities embedded in the small policies for the agents, leading often to human drivers when dealing with semantic exploration. In this paper, we address this challenge by presenting a novel Deep Reinforcement Learning (DRL) architecture that is specifically designed for resource efficient semantic exploration. A key methodological contribution is the integration of a Vision-Language Model (VLM) common-sense through a layered reward function. The VLM query is modeled as a dedicated action, allowing the agent to strategically query the VLM only when deemed necessary for gaining external guidance, thereby conserving resources. This mechanism is combined with a curriculum learning strategy designed to guide learning at different levels of complexity to ensure robust and stable learning. Our experimental evaluation results convincingly demonstrate that our agent achieves significantly enhanced object discovery rates and develops a learned capability to effectively navigate towards semantically rich regions. Furthermore, it also shows a strategic mastery of when to prompt for external environmental information. By demonstrating a practical and scalable method for embedding common-sense semantic reasoning with autonomous agents, this research provides a novel approach to pursuing a fully intelligent and self-guided exploration in robotics.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…·èº«æ™ºèƒ½ä½“(Embodied Agents)åœ¨æœªçŸ¥ç¯å¢ƒä¸­éš¾ä»¥å¹³è¡¡æ¢ç´¢æ•ˆç‡ä¸è¯­ä¹‰ç†è§£(Semantic Understanding)çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ (DRL)æ¶æ„ã€‚è¯¥æ¶æ„çš„æ ¸å¿ƒè´¡çŒ®åœ¨äºé€šè¿‡åˆ†å±‚å¥–åŠ±å‡½æ•°æ•´åˆäº†è§†è§‰è¯­è¨€æ¨¡å‹(VLM)çš„å¸¸è¯†æ¨ç†èƒ½åŠ›ï¼Œå¹¶å°† VLM æŸ¥è¯¢å»ºæ¨¡ä¸ºä¸€é¡¹ä¸“ç”¨åŠ¨ä½œï¼Œä½¿æ™ºèƒ½ä½“èƒ½ä»…åœ¨å¿…è¦æ—¶ç­–ç•¥æ€§åœ°å¯»æ±‚å¤–éƒ¨å¼•å¯¼ä»¥èŠ‚çœèµ„æºã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†è¯¾ç¨‹å­¦ä¹ (Curriculum Learning)ç­–ç•¥ï¼Œé€šè¿‡ä¸åŒå¤æ‚åº¦çš„ä»»åŠ¡å¼•å¯¼æ¥ç¡®ä¿å­¦ä¹ è¿‡ç¨‹çš„é²æ£’æ€§ä¸ç¨³å®šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ™ºèƒ½ä½“æ˜¾è‘—æå‡äº†ç‰©ä½“å‘ç°ç‡ï¼Œå¹¶èƒ½ç²¾å‡†å¯¼èˆªè‡³è¯­ä¹‰ä¸°å¯ŒåŒºåŸŸï¼Œå±•ç°å‡ºå¯¹å¤–éƒ¨ä¿¡æ¯æŸ¥è¯¢æ—¶æœºçš„å“è¶ŠæŒæ§ã€‚è¯¥æ–¹æ³•ä¸ºåœ¨è‡ªä¸»æ™ºèƒ½ä½“ä¸­åµŒå…¥å¸¸è¯†è¯­ä¹‰æ¨ç†æä¾›äº†ä¸€ç§å®ç”¨ä¸”å¯æ‰©å±•çš„æ–¹æ¡ˆï¼Œä¸ºå®ç°å®Œå…¨æ™ºèƒ½çš„æœºå™¨äººè‡ªå¯¼å‘æ¢ç´¢å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "The 19th International Conference on Intelligent Autonomous Systems (IAS 19), 2025, Genoa",
      "pdf_url": "https://arxiv.org/pdf/2509.09356v1",
      "published_date": "2025-09-11 11:10:08 UTC",
      "updated_date": "2025-09-11 11:10:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:57:54.898413+00:00"
    },
    {
      "arxiv_id": "2509.09349v2",
      "title": "Classification of Driver Behaviour Using External Observation Techniques for Autonomous Vehicles",
      "title_zh": "åŸºäºå¤–éƒ¨è§‚æµ‹æŠ€æœ¯çš„è‡ªåŠ¨é©¾é©¶è½¦è¾†é©¾é©¶å‘˜è¡Œä¸ºåˆ†ç±»",
      "authors": [
        "Ian Nell",
        "Shane Gilroy"
      ],
      "abstract": "Road traffic accidents remain a significant global concern, with human error, particularly distracted and impaired driving, among the leading causes. This study introduces a novel driver behaviour classification system that uses external observation techniques to detect indicators of distraction and impairment. The proposed framework employs advanced computer vision methodologies, including real-time object tracking, lateral displacement analysis, and lane position monitoring. The system identifies unsafe driving behaviours such as excessive lateral movement and erratic trajectory patterns by implementing the YOLO object detection model and custom lane estimation algorithms. Unlike systems reliant on inter-vehicular communication, this vision-based approach enables behavioural analysis of non-connected vehicles. Experimental evaluations on diverse video datasets demonstrate the framework's reliability and adaptability across varying road and environmental conditions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨å¤–éƒ¨è§‚æµ‹æŠ€æœ¯å¯¹è‡ªåŠ¨é©¾é©¶æ±½è½¦(Autonomous Vehicles)é©¾é©¶å‘˜è¡Œä¸ºè¿›è¡Œåˆ†ç±»çš„æ–°ç³»ç»Ÿï¼Œæ—¨åœ¨è¯†åˆ«åˆ†å¿ƒå’Œå—æŸé©¾é©¶ç­‰å¯¼è‡´äº¤é€šäº‹æ•…çš„å®‰å…¨éšæ‚£ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†å…ˆè¿›çš„è®¡ç®—æœºè§†è§‰(Computer Vision)æ–¹æ³•ï¼Œç»“åˆäº†å®æ—¶ç›®æ ‡è·Ÿè¸ªã€æ¨ªå‘ä½ç§»åˆ†æä»¥åŠè½¦é“ä½ç½®ç›‘æ§ã€‚é€šè¿‡å®æ–½ YOLO ç›®æ ‡æ£€æµ‹æ¨¡å‹å’Œè‡ªå®šä¹‰è½¦é“ä¼°è®¡ç®—æ³•ï¼Œç³»ç»Ÿèƒ½å¤Ÿç²¾ç¡®è¯†åˆ«è¿‡åº¦çš„æ¨ªå‘ç§»åŠ¨å’Œä¸ç¨³å®šçš„è½¨è¿¹æ¨¡å¼ç­‰å±é™©è¡Œä¸ºã€‚ä¸ä¾èµ–è½¦é—´é€šä¿¡(inter-vehicular communication)çš„ç³»ç»Ÿä¸åŒï¼Œè¿™ç§åŸºäºè§†è§‰çš„æ–¹æ³•å®ç°äº†å¯¹éè”ç½‘è½¦è¾†(non-connected vehicles)çš„è¡Œä¸ºåˆ†æã€‚åœ¨å¤šæ ·åŒ–è§†é¢‘æ•°æ®é›†ä¸Šçš„å®éªŒè¯„ä¼°è¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ä¸åŒçš„é“è·¯å’Œç¯å¢ƒæ¡ä»¶ä¸‹å‡è¡¨ç°å‡ºæé«˜çš„å¯é æ€§ä¸é€‚åº”æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.ET",
        "cs.RO",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.09349v2",
      "published_date": "2025-09-11 11:05:14 UTC",
      "updated_date": "2025-10-29 12:14:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:57:53.583965+00:00"
    },
    {
      "arxiv_id": "2509.09747v1",
      "title": "D-CAT: Decoupled Cross-Attention Transfer between Sensor Modalities for Unimodal Inference",
      "title_zh": "D-CATï¼šé¢å‘å•æ¨¡æ€æ¨ç†çš„ä¼ æ„Ÿå™¨æ¨¡æ€é—´è§£è€¦äº¤å‰æ³¨æ„åŠ›è¿ç§»",
      "authors": [
        "Leen Daher",
        "Zhaobo Wang",
        "Malcolm Mielle"
      ],
      "abstract": "Cross-modal transfer learning is used to improve multi-modal classification models (e.g., for human activity recognition in human-robot collaboration). However, existing methods require paired sensor data at both training and inference, limiting deployment in resource-constrained environments where full sensor suites are not economically and technically usable. To address this, we propose Decoupled Cross-Attention Transfer (D-CAT), a framework that aligns modality-specific representations without requiring joint sensor modality during inference. Our approach combines a self-attention module for feature extraction with a novel cross-attention alignment loss, which enforces the alignment of sensors' feature spaces without requiring the coupling of the classification pipelines of both modalities. We evaluate D-CAT on three multi-modal human activity datasets (IMU, video, and audio) under both in-distribution and out-of-distribution scenarios, comparing against uni-modal models. Results show that in in-distribution scenarios, transferring from high-performing modalities (e.g., video to IMU) yields up to 10% F1-score gains over uni-modal training. In out-of-distribution scenarios, even weaker source modalities (e.g., IMU to video) improve target performance, as long as the target model isn't overfitted on the training data. By enabling single-sensor inference with cross-modal knowledge, D-CAT reduces hardware redundancy for perception systems while maintaining accuracy, which is critical for cost-sensitive or adaptive deployments (e.g., assistive robots in homes with variable sensor availability). Code is available at https://github.com/Schindler-EPFL-Lab/D-CAT.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† D-CAT (Decoupled Cross-Attention Transfer) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€åˆ†ç±»æ¨¡å‹åœ¨æ¨ç†é˜¶æ®µé€šå¸¸éœ€è¦é…å¯¹ä¼ æ„Ÿå™¨æ•°æ®ã€ä»è€Œé™åˆ¶å…¶åœ¨èµ„æºå—é™ç¯å¢ƒéƒ¨ç½²çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•ç»“åˆäº†ç”¨äºç‰¹å¾æå–çš„ self-attention æ¨¡å—å’Œä¸€ç§æ–°é¢–çš„ cross-attention alignment lossï¼Œèƒ½å¤Ÿåœ¨ä¸è€¦åˆä¸åŒæ¨¡æ€åˆ†ç±»æµæ°´çº¿çš„å‰æä¸‹å®ç°ä¼ æ„Ÿå™¨ç‰¹å¾ç©ºé—´çš„å¯¹é½ã€‚å®éªŒåœ¨ IMUã€è§†é¢‘å’ŒéŸ³é¢‘ç­‰ä¸‰ä¸ªäººä½“æ´»åŠ¨æ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œç»“æœæ˜¾ç¤ºåœ¨åŒåˆ†å¸ƒ (in-distribution) åœºæ™¯ä¸‹ï¼Œä»é«˜è¡¨ç°æ¨¡æ€ï¼ˆå¦‚è§†é¢‘ï¼‰å‘ç›®æ ‡æ¨¡æ€ï¼ˆå¦‚ IMUï¼‰çš„çŸ¥è¯†è½¬ç§»å¯ä½¿ F1-score æå‡é«˜è¾¾ 10%ã€‚åœ¨åˆ†å¸ƒå¤– (out-of-distribution) åœºæ™¯ä¸­ï¼Œåªè¦ç›®æ ‡æ¨¡å‹ä¸è¿‡æ‹Ÿåˆï¼Œå³ä¾¿è¾ƒå¼±çš„æºæ¨¡æ€ä¹Ÿèƒ½æ˜¾è‘—å¢å¼ºç›®æ ‡æ¨¡æ€çš„æ€§èƒ½ã€‚é€šè¿‡å®ç°åˆ©ç”¨è·¨æ¨¡æ€çŸ¥è¯†è¿›è¡Œå•ä¼ æ„Ÿå™¨æ¨ç† (unimodal inference)ï¼ŒD-CAT åœ¨ä¿æŒç²¾åº¦çš„åŒæ—¶æœ‰æ•ˆå‡å°‘äº†æ„ŸçŸ¥ç³»ç»Ÿçš„ç¡¬ä»¶å†—ä½™ã€‚è¯¥æŠ€æœ¯ä¸ºæˆæœ¬æ•æ„Ÿæˆ–ç¯å¢ƒå¤šå˜çš„è¾…åŠ©æœºå™¨äººç­‰å®é™…åº”ç”¨åœºæ™¯æä¾›äº†å…³é”®æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.09747v1",
      "published_date": "2025-09-11 10:54:07 UTC",
      "updated_date": "2025-09-11 10:54:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:58:01.381481+00:00"
    },
    {
      "arxiv_id": "2509.09337v1",
      "title": "MoSE: Unveiling Structural Patterns in Graphs via Mixture of Subgraph Experts",
      "title_zh": "MoSEï¼šé€šè¿‡å­å›¾ä¸“å®¶æ··åˆæ¨¡å‹æ­ç¤ºå›¾ä¸­çš„ç»“æ„æ¨¡å¼",
      "authors": [
        "Junda Ye",
        "Zhongbao Zhang",
        "Li Sun",
        "Siqiang Luo"
      ],
      "abstract": "While graph neural networks (GNNs) have achieved great success in learning from graph-structured data, their reliance on local, pairwise message passing restricts their ability to capture complex, high-order subgraph patterns. leading to insufficient structural expressiveness. Recent efforts have attempted to enhance structural expressiveness by integrating random walk kernels into GNNs. However, these methods are inherently designed for graph-level tasks, which limits their applicability to other downstream tasks such as node classification. Moreover, their fixed kernel configurations hinder the model's flexibility in capturing diverse subgraph structures. To address these limitations, this paper proposes a novel Mixture of Subgraph Experts (MoSE) framework for flexible and expressive subgraph-based representation learning across diverse graph tasks. Specifically, MoSE extracts informative subgraphs via anonymous walks and dynamically routes them to specialized experts based on structural semantics, enabling the model to capture diverse subgraph patterns with improved flexibility and interpretability. We further provide a theoretical analysis of MoSE's expressivity within the Subgraph Weisfeiler-Lehman (SWL) Test, proving that it is more powerful than SWL. Extensive experiments, together with visualizations of learned subgraph experts, demonstrate that MoSE not only outperforms competitive baselines but also provides interpretable insights into structural patterns learned by the model.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å›¾ç¥ç»ç½‘ç»œ(GNNs)åœ¨æ•è·å¤æ‚é«˜é˜¶å­å›¾æ¨¡å¼æ–¹é¢è¡¨è¾¾èƒ½åŠ›ä¸è¶³ï¼Œä»¥åŠç°æœ‰éšæœºæ¸¸èµ°æ ¸æ–¹æ³•çµæ´»æ€§å·®ä¸”å±€é™äºå›¾çº§åˆ«ä»»åŠ¡ç­‰å±€é™æ€§ï¼Œæå‡ºäº†Mixture of Subgraph Experts (MoSE)æ¡†æ¶ã€‚MoSEé€šè¿‡åŒ¿åæ¸¸èµ°(anonymous walks)æå–ä¿¡æ¯ä¸°å¯Œçš„å­å›¾ï¼Œå¹¶æ ¹æ®ç»“æ„è¯­ä¹‰å°†å®ƒä»¬åŠ¨æ€è·¯ç”±(dynamically routing)åˆ°ä¸“é—¨çš„ä¸“å®¶æ¨¡å—ä¸­ï¼Œä»è€Œä½¿æ¨¡å‹èƒ½å¤Ÿçµæ´»åœ°æ•è·å¤šæ ·çš„å­å›¾æ¨¡å¼å¹¶æå‡å¯è§£é‡Šæ€§ã€‚ç†è®ºåˆ†æè¯æ˜ï¼ŒMoSEåœ¨å­å›¾Weisfeiler-Lehman (SWL)æµ‹è¯•æ¡†æ¶ä¸‹çš„è¡¨è¾¾èƒ½åŠ›ä¼˜äºSWLã€‚å¤§é‡å®éªŒåŠå­¦ä¹ åˆ°çš„å­å›¾ä¸“å®¶å¯è§†åŒ–ç»“æœè¡¨æ˜ï¼ŒMoSEåœ¨å¤šç§å›¾ä»»åŠ¡ä¸­çš„æ€§èƒ½å‡ä¼˜äºç«äº‰åŸºå‡†æ¨¡å‹ï¼Œå¹¶ä¸ºæ¨¡å‹æ•æ‰åˆ°çš„ç»“æ„æ¨¡å¼æä¾›äº†å…·æœ‰æ´å¯ŸåŠ›çš„å¯è§£é‡Šæ€§æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "16 pages, 11 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.09337v1",
      "published_date": "2025-09-11 10:45:50 UTC",
      "updated_date": "2025-09-11 10:45:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:58:02.185606+00:00"
    },
    {
      "arxiv_id": "2509.09332v2",
      "title": "OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning",
      "title_zh": "OmniEVAï¼šåŸºäºä»»åŠ¡è‡ªé€‚åº” 3D å®šä½ä¸å…·èº«æ„ŸçŸ¥æ¨ç†çš„é€šç”¨å…·èº«è§„åˆ’å™¨",
      "authors": [
        "Yuecheng Liu",
        "Dafeng Chi",
        "Shiguang Wu",
        "Zhanguang Zhang",
        "Yuzheng Zhuang",
        "Bowen Yang",
        "He Zhu",
        "Lingfeng Zhang",
        "Pengwei Xie",
        "David Gamaliel Arcos Bravo",
        "Yingxue Zhang",
        "Jianye Hao",
        "Xingyue Quan"
      ],
      "abstract": "Recent advances in multimodal large language models (MLLMs) have opened new opportunities for embodied intelligence, enabling multimodal understanding, reasoning, and interaction, as well as continuous spatial decision-making. Nevertheless, current MLLM-based embodied systems face two critical limitations. First, Geometric Adaptability Gap: models trained solely on 2D inputs or with hard-coded 3D geometry injection suffer from either insufficient spatial information or restricted 2D generalization, leading to poor adaptability across tasks with diverse spatial demands. Second, Embodiment Constraint Gap: prior work often neglects the physical constraints and capacities of real robots, resulting in task plans that are theoretically valid but practically infeasible. To address these gaps, we introduce OmniEVA -- an embodied versatile planner that enables advanced embodied reasoning and task planning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding mechanism, which introduces a gated router to perform explicit selective regulation of 3D fusion based on contextual requirements, enabling context-aware 3D grounding for diverse embodied tasks. (2) an Embodiment-Aware Reasoning framework that jointly incorporates task goals and embodiment constraints into the reasoning loop, resulting in planning decisions that are both goal-directed and executable. Extensive experimental results demonstrate that OmniEVA not only achieves state-of-the-art general embodied reasoning performance, but also exhibits a strong ability across a wide range of downstream scenarios. Evaluations of a suite of proposed embodied benchmarks, including both primitive and composite tasks, confirm its robust and versatile planning capabilities. Project page: https://omnieva.github.io",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† OmniEVAï¼Œè¿™æ˜¯ä¸€ç§å…·èº«å…¨èƒ½è§„åˆ’å™¨ (Embodied Versatile Planner)ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) åœ¨å…·èº«æ™ºèƒ½é¢†åŸŸé¢ä¸´çš„å‡ ä½•é€‚åº”æ€§é¸¿æ²Ÿ (Geometric Adaptability Gap) å’Œå…·èº«çº¦æŸé¸¿æ²Ÿ (Embodiment Constraint Gap)ã€‚ä¸ºäº†å¼¥åˆè¿™äº›å·®è·ï¼ŒOmniEVA å¼•å…¥äº†ä»»åŠ¡è‡ªé€‚åº” 3D Grounding (Task-Adaptive 3D Grounding) æœºåˆ¶ï¼Œåˆ©ç”¨é—¨æ§è·¯ç”± (Gated Router) æ ¹æ®ä¸Šä¸‹æ–‡éœ€æ±‚é€‰æ‹©æ€§åœ°è°ƒèŠ‚ 3D èåˆï¼Œå®ç°äº†çµæ´»çš„ 3D ç©ºé—´æ„ŸçŸ¥ã€‚åŒæ—¶ï¼Œè¯¥ç ”ç©¶æ„å»ºäº†å…·èº«æ„ŸçŸ¥æ¨ç† (Embodiment-Aware Reasoning) æ¡†æ¶ï¼Œé€šè¿‡å°†ä»»åŠ¡ç›®æ ‡ä¸å…·èº«ç‰©ç†çº¦æŸå…±åŒçº³å…¥æ¨ç†é—­ç¯ï¼Œä½¿è§„åˆ’å†³ç­–åœ¨æ»¡è¶³ç›®æ ‡çš„åŒæ—¶å…¼é¡¾æœºå™¨äººçš„å®é™…æ“ä½œå¯è¡Œæ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒOmniEVA åœ¨é€šç”¨å…·èº«æ¨ç†æ€§èƒ½ä¸Šè¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›çš„ SOTA æ°´å¹³ï¼Œå¹¶å±•ç°å‡ºå“è¶Šçš„ä¸‹æ¸¸åœºæ™¯è¿ç§»èƒ½åŠ›ã€‚é’ˆå¯¹ä¸€ç³»åˆ—åŸºç¡€å’Œå¤åˆå…·èº«åŸºå‡†ä»»åŠ¡çš„è¯„ä¼°è¿›ä¸€æ­¥éªŒè¯äº†å…¶åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„é²æ£’æ€§å’Œå¤šåŠŸèƒ½è§„åˆ’ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.09332v2",
      "published_date": "2025-09-11 10:32:22 UTC",
      "updated_date": "2025-09-12 08:01:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:58:27.186800+00:00"
    },
    {
      "arxiv_id": "2509.10570v1",
      "title": "Large Foundation Models for Trajectory Prediction in Autonomous Driving: A Comprehensive Survey",
      "title_zh": "è‡ªåŠ¨é©¾é©¶è½¨è¿¹é¢„æµ‹ä¸­çš„å¤§è§„æ¨¡åŸºåº§æ¨¡å‹ï¼šå…¨é¢ç»¼è¿°",
      "authors": [
        "Wei Dai",
        "Shengen Wu",
        "Wei Wu",
        "Zhenhao Wang",
        "Sisuo Lyu",
        "Haicheng Liao",
        "Limin Yu",
        "Weiping Ding",
        "Runwei Guan",
        "Yutao Yue"
      ],
      "abstract": "Trajectory prediction serves as a critical functionality in autonomous driving, enabling the anticipation of future motion paths for traffic participants such as vehicles and pedestrians, which is essential for driving safety. Although conventional deep learning methods have improved accuracy, they remain hindered by inherent limitations, including lack of interpretability, heavy reliance on large-scale annotated data, and weak generalization in long-tail scenarios. The rise of Large Foundation Models (LFMs) is transforming the research paradigm of trajectory prediction. This survey offers a systematic review of recent advances in LFMs, particularly Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) for trajectory prediction. By integrating linguistic and scene semantics, LFMs facilitate interpretable contextual reasoning, significantly enhancing prediction safety and generalization in complex environments. The article highlights three core methodologies: trajectory-language mapping, multimodal fusion, and constraint-based reasoning. It covers prediction tasks for both vehicles and pedestrians, evaluation metrics, and dataset analyses. Key challenges such as computational latency, data scarcity, and real-world robustness are discussed, along with future research directions including low-latency inference, causality-aware modeling, and motion foundation models.",
      "tldr_zh": "è¯¥ç»¼è¿°ç³»ç»Ÿå›é¡¾äº†åœ¨è‡ªåŠ¨é©¾é©¶è½¨è¿¹é¢„æµ‹é¢†åŸŸä¸­å¤§å‹åŸºç¡€æ¨¡å‹ï¼ˆLarge Foundation Models, LFMsï¼‰çš„æœ€æ–°è¿›å±•ï¼Œç‰¹åˆ«æ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„åº”ç”¨ã€‚é’ˆå¯¹ä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨å¯è§£é‡Šæ€§ã€å¯¹å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®çš„ä¾èµ–ä»¥åŠåœ¨é•¿å°¾åœºæ™¯ï¼ˆlong-tail scenariosï¼‰ä¸­æ³›åŒ–èƒ½åŠ›å¼±ç­‰å±€é™æ€§ï¼Œè¯¥ç ”ç©¶æ¢è®¨äº†LFMså¦‚ä½•é€šè¿‡æ•´åˆè¯­è¨€ä¸åœºæ™¯è¯­ä¹‰æ¥æå‡é¢„æµ‹æ€§èƒ½ã€‚æ–‡ç« é‡ç‚¹æ€»ç»“äº†è½¨è¿¹-è¯­è¨€æ˜ å°„ï¼ˆtrajectory-language mappingï¼‰ã€å¤šæ¨¡æ€èåˆï¼ˆmultimodal fusionï¼‰ä»¥åŠåŸºäºçº¦æŸçš„æ¨ç†ï¼ˆconstraint-based reasoningï¼‰ä¸‰é¡¹æ ¸å¿ƒæŠ€æœ¯è·¯å¾„ã€‚è°ƒç ”å†…å®¹å…¨é¢æ¶µç›–äº†é’ˆå¯¹è½¦è¾†å’Œè¡Œäººçš„é¢„æµ‹ä»»åŠ¡ã€è¯„ä¼°æŒ‡æ ‡åŠæ•°æ®é›†åˆ†æï¼Œå¼ºè°ƒäº†ä¸Šä¸‹æ–‡æ¨ç†åœ¨å¢å¼ºé¢„æµ‹å®‰å…¨æ€§å’Œç¯å¢ƒé€‚åº”æ€§æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜æ·±å…¥åˆ†æäº†è®¡ç®—å»¶è¿Ÿï¼ˆcomputational latencyï¼‰ã€æ•°æ®ç¨€ç¼ºå’ŒçœŸå®ä¸–ç•Œé²æ£’æ€§ç­‰æŒ‘æˆ˜ï¼Œå¹¶å‰ç»æ€§åœ°æå‡ºäº†ä½å»¶è¿Ÿæ¨ç†ã€å› æœæ„ŸçŸ¥å»ºæ¨¡ï¼ˆcausality-aware modelingï¼‰ç­‰æœªæ¥ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "22 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.10570v1",
      "published_date": "2025-09-11 10:30:06 UTC",
      "updated_date": "2025-09-11 10:30:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:58:36.895672+00:00"
    },
    {
      "arxiv_id": "2509.09321v1",
      "title": "Towards Adaptive ML Benchmarks: Web-Agent-Driven Construction, Domain Expansion, and Metric Optimization",
      "title_zh": "è¿ˆå‘è‡ªé€‚åº”æœºå™¨å­¦ä¹ åŸºå‡†ï¼šWeb æ™ºèƒ½ä½“é©±åŠ¨çš„æ„å»ºã€é¢†åŸŸæ‰©å±•ä¸æŒ‡æ ‡ä¼˜åŒ–",
      "authors": [
        "Hangyi Jia",
        "Yuxi Qian",
        "Hanwen Tong",
        "Xinhui Wu",
        "Lin Chen",
        "Feng Wei"
      ],
      "abstract": "Recent advances in large language models (LLMs) have enabled the emergence of general-purpose agents for automating end-to-end machine learning (ML) workflows, including data analysis, feature engineering, model training, and competition solving. However, existing benchmarks remain limited in task coverage, domain diversity, difficulty modeling, and evaluation rigor, failing to capture the full capabilities of such agents in realistic settings. We present TAM Bench, a diverse, realistic, and structured benchmark for evaluating LLM-based agents on end-to-end ML tasks. TAM Bench features three key innovations: (1) A browser automation and LLM-based task acquisition system that automatically collects and structures ML challenges from platforms such as Kaggle, AIcrowd, and Biendata, spanning multiple task types and data modalities (e.g., tabular, text, image, graph, audio); (2) A leaderboard-driven difficulty modeling mechanism that estimates task complexity using participant counts and score dispersion, enabling scalable and objective task calibration; (3) A multi-dimensional evaluation framework incorporating performance, format compliance, constraint adherence, and task generalization. Based on 150 curated AutoML tasks, we construct three benchmark subsets of different sizes -- Lite, Medium, and Full -- designed for varying evaluation scenarios. The Lite version, with 18 tasks and balanced coverage across modalities and difficulty levels, serves as a practical testbed for daily benchmarking and comparative studies.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TAM Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼° LLM-based agents åœ¨ç«¯åˆ°ç«¯ ML å·¥ä½œæµä¸­è¡¨ç°çš„å¤šå…ƒåŒ–ã€çœŸå®ä¸”ç»“æ„åŒ–çš„åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†é€šè¿‡æµè§ˆå™¨è‡ªåŠ¨åŒ–å’Œ LLM-based ä»»åŠ¡è·å–ç³»ç»Ÿï¼Œè‡ªåŠ¨ä» Kaggle å’Œ AIcrowd ç­‰å¹³å°æ”¶é›†æ¶µç›–è¡¨æ ¼ã€æ–‡æœ¬ã€å›¾åƒå’ŒéŸ³é¢‘ç­‰å¤šç§æ•°æ®æ¨¡æ€çš„ ML æŒ‘æˆ˜ã€‚TAM Bench å¼•å…¥äº†æ’è¡Œæ¦œé©±åŠ¨çš„éš¾åº¦å»ºæ¨¡æœºåˆ¶ï¼Œé€šè¿‡åˆ†æå‚ä¸è€…æ•°æ®å®ç°å®¢è§‚çš„ä»»åŠ¡æ ¡å‡†ï¼Œå¹¶å»ºç«‹äº†åŒ…å«æ€§èƒ½ã€æ ¼å¼åˆè§„æ€§å’Œ task generalization åœ¨å†…çš„å¤šç»´åº¦è¯„ä¼°æ¡†æ¶ã€‚ç ”ç©¶åŸºäº 150 ä¸ª AutoML ä»»åŠ¡æ„å»ºäº† Liteã€Medium å’Œ Full ä¸‰ä¸ªè§„æ¨¡çš„å­é›†ï¼Œå…¶ä¸­ Lite ç‰ˆæœ¬é€šè¿‡å¹³è¡¡æ¨¡æ€å’Œéš¾åº¦ï¼Œä¸ºæ—¥å¸¸ Benchmarking æä¾›äº†é«˜æ•ˆçš„æµ‹è¯•ç¯å¢ƒã€‚è¿™äº›åˆ›æ–°å¡«è¡¥äº†ç°æœ‰åŸºå‡†åœ¨é¢†åŸŸå¤šæ ·æ€§å’Œéš¾åº¦å»ºæ¨¡æ–¹é¢çš„ç©ºç™½ï¼Œä¸ºè¯„ä¼°æ™ºèƒ½ä½“åœ¨ç°å®åœºæ™¯ä¸‹çš„å…¨æ–¹ä½èƒ½åŠ›æä¾›äº†ä¸¥è°¨å·¥å…·ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.09321v1",
      "published_date": "2025-09-11 10:10:48 UTC",
      "updated_date": "2025-09-11 10:10:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:58:32.694651+00:00"
    },
    {
      "arxiv_id": "2509.09314v1",
      "title": "Measuring Implicit Spatial Coordination in Teams: Effects on Collective Intelligence and Performance",
      "title_zh": "è¡¡é‡å›¢é˜Ÿä¸­çš„éšæ€§ç©ºé—´åä½œï¼šåŠå…¶å¯¹é›†ä½“æ™ºèƒ½ä¸å›¢é˜Ÿç»©æ•ˆçš„å½±å“",
      "authors": [
        "Thuy Ngoc Nguyen",
        "Anita Williams Woolley",
        "Cleotilde Gonzalez"
      ],
      "abstract": "Coordinated teamwork is essential in fast-paced decision-making environments that require dynamic adaptation, often without an opportunity for explicit communication. Although implicit coordination has been extensively considered in the existing literature, the majority of work has focused on co-located, synchronous teamwork (such as sports teams) or, in distributed teams, primarily on coordination of knowledge work. However, many teams (firefighters, military, law enforcement, emergency response) must coordinate their movements in physical space without the benefit of visual cues or extensive explicit communication. This paper investigates how three dimensions of spatial coordination, namely exploration diversity, movement specialization, and adaptive spatial proximity, influence team performance in a collaborative online search and rescue task where explicit communication is restricted and team members rely on movement patterns to infer others' intentions and coordinate actions. Our metrics capture the relational aspects of teamwork by measuring spatial proximity, distribution patterns, and alignment of movements within shared environments. We analyze data from 34 four-person teams (136 participants) assigned to specialized roles in a search and rescue task. Results show that spatial specialization positively predicts performance, while adaptive spatial proximity exhibits a marginal inverted U-shaped relationship, suggesting moderate levels of adaptation are optimal. Furthermore, the temporal dynamics of these metrics differentiate high- from low-performing teams over time. These findings provide insights into implicit spatial coordination in role-based teamwork and highlight the importance of balanced adaptive strategies, with implications for training and AI-assisted team support systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨æ²Ÿé€šå—é™çš„åŠ¨æ€å†³ç­–ç¯å¢ƒä¸­ï¼Œå›¢é˜Ÿå¦‚ä½•é€šè¿‡å†…éšç©ºé—´åè°ƒ(Implicit Spatial Coordination)æ¥æå‡é›†ä½“æ™ºæ…§ä¸ç»©æ•ˆã€‚ç ”ç©¶é‡ç‚¹åˆ†æäº†ä¸‰ä¸ªç©ºé—´ç»´åº¦çš„å½±å“ï¼Œå³æ¢ç´¢å¤šæ ·æ€§(Exploration Diversity)ã€ç§»åŠ¨ä¸“ä¸šåŒ–(Movement Specialization)ä»¥åŠé€‚åº”æ€§ç©ºé—´äº²å¯†åº¦(Adaptive Spatial Proximity)ã€‚é€šè¿‡å¯¹34ä¸ªå››äººå›¢é˜Ÿåœ¨åœ¨çº¿æœæ•‘ä»»åŠ¡ä¸­çš„è¡Œä¸ºæ•°æ®è¿›è¡Œåˆ†æï¼Œç ”ç©¶å‘ç° Movement Specialization èƒ½æ˜¾è‘—æ­£å‘é¢„æµ‹å›¢é˜Ÿç»©æ•ˆã€‚åŒæ—¶ï¼ŒAdaptive Spatial Proximity ä¸ç»©æ•ˆä¹‹é—´å­˜åœ¨è¾¹ç¼˜å€’Uå‹å…³ç³»ï¼Œæš—ç¤ºé€‚åº¦çš„é€‚åº”æ€§ç­–ç•¥å¯¹äºä»»åŠ¡æˆåŠŸè‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼Œç©ºé—´æŒ‡æ ‡éšæ—¶é—´æ¼”å˜çš„åŠ¨æ€ç‰¹å¾èƒ½æœ‰æ•ˆåŒºåˆ†é«˜ç»©æ•ˆä¸ä½ç»©æ•ˆå›¢é˜Ÿã€‚è¿™äº›å‘ç°ä¸ºç†è§£åŸºäºè§’è‰²çš„å›¢é˜Ÿåä½œæœºåˆ¶æä¾›äº†æ–°è§†è§’ï¼Œå¹¶å¯¹è®¾è®¡AIè¾…åŠ©å›¢é˜Ÿæ”¯æŒç³»ç»ŸåŠç›¸å…³åŸ¹è®­å…·æœ‰é‡è¦æŒ‡å¯¼æ„ä¹‰ã€‚",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.09314v1",
      "published_date": "2025-09-11 10:00:01 UTC",
      "updated_date": "2025-09-11 10:00:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:58:42.381640+00:00"
    },
    {
      "arxiv_id": "2509.09312v5",
      "title": "Explaining Tournament Solutions with Minimal Supports",
      "title_zh": "åŸºäºæœ€å°æ”¯æŒé›†çš„é”¦æ ‡èµ›è§£è§£é‡Š",
      "authors": [
        "ClÃ©ment Contet",
        "Umberto Grandi",
        "JÃ©rÃ´me Mengin"
      ],
      "abstract": "Tournaments are widely used models to represent pairwise dominance between candidates, alternatives, or teams. We study the problem of providing certified explanations for why a candidate appears among the winners under various tournament rules. To this end, we identify minimal supports, minimal sub-tournaments in which the candidate is guaranteed to win regardless of how the rest of the tournament is completed (that is, the candidate is a necessary winner of the sub-tournament). This notion corresponds to an abductive explanation for the question,\"Why does the winner win the tournament?\", a central concept in formal explainable AI. We focus on common tournament solutions: the top cycle, the uncovered set, the Copeland rule, the Borda rule, the maximin rule, and the weighted uncovered set. For each rule we determine the size of the smallest minimal supports, and we present polynomial-time algorithms to compute them for all solutions except for the weighted uncovered set, for which the problem is NP-complete. Finally, we show how minimal supports can serve to produce compact, certified, and intuitive explanations for tournament solutions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹tournament solutionsä¸­å€™é€‰äººè·èƒœçš„åŸå› ï¼Œæ¢è®¨äº†æä¾›ç¡®è¯æ€§è§£é‡Šçš„æ–¹æ³•ã€‚ç ”ç©¶è¯†åˆ«å¹¶å®šä¹‰äº†minimal supportsï¼Œå³ç¡®ä¿å€™é€‰äººæ— è®ºå‰©ä½™æ¯”èµ›ç»“æœå¦‚ä½•å‡èƒ½è·èƒœçš„æœ€å°å­ç«èµ›ï¼ˆsub-tournamentsï¼‰ï¼Œè¿™ä¸€æ¦‚å¿µå¯¹åº”äºå¯è§£é‡Šäººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰ä¸­çš„æº¯å› è§£é‡Šï¼ˆabductive explanationï¼‰ã€‚ç ”ç©¶é‡ç‚¹åˆ†æäº†top cycleã€uncovered setã€Copeland ruleã€Borda ruleã€maximin ruleä»¥åŠweighted uncovered setç­‰ä¸»æµè§„åˆ™ã€‚ä½œè€…ç¡®å®šäº†å„è§„åˆ™ä¸‹æœ€å°æ”¯æŒé›†çš„è§„æ¨¡ï¼Œå¹¶é’ˆå¯¹é™¤weighted uncovered setï¼ˆå…¶å¤æ‚åº¦ä¸ºNP-completeï¼‰å¤–çš„æ‰€æœ‰è§„åˆ™å¼€å‘äº†å¤šé¡¹å¼æ—¶é—´ç®—æ³•ã€‚æœ€åï¼Œè¯¥ç ”ç©¶è¯æ˜äº†minimal supportsèƒ½å¤Ÿä¸ºç«èµ›è§£å†³æ–¹æ¡ˆç”Ÿæˆç´§å‡‘ã€ç¡®è¯ä¸”ç›´è§‚çš„è§£é‡Šï¼Œæœ‰æ•ˆå¢å¼ºäº†æ¨¡å‹å†³ç­–çš„é€æ˜åº¦ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "This paper is the extended version of Contet, Grandi, and Mengin. 2026. Explaining Tournament Solutions with Minimal Supports. In Proceedings of the 40th AAAI Conference on Artificial Intelligence",
      "pdf_url": "https://arxiv.org/pdf/2509.09312v5",
      "published_date": "2025-09-11 09:55:50 UTC",
      "updated_date": "2026-01-21 06:41:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:58:35.187915+00:00"
    },
    {
      "arxiv_id": "2509.14254v1",
      "title": "Hallucination Detection with the Internal Layers of LLMs",
      "title_zh": "åŸºäº LLM å†…éƒ¨å±‚çš„å¹»è§‰æ£€æµ‹",
      "authors": [
        "Martin PreiÃŸ"
      ],
      "abstract": "Large Language Models (LLMs) have succeeded in a variety of natural language processing tasks [Zha+25]. However, they have notable limitations. LLMs tend to generate hallucinations, a seemingly plausible yet factually unsupported output [Hua+24], which have serious real-world consequences [Kay23; Rum+24]. Recent work has shown that probing-based classifiers that utilize LLMs' internal representations can detect hallucinations [AM23; Bei+24; Bur+24; DYT24; Ji+24; SMZ24; Su+24]. This approach, since it does not involve model training, can enhance reliability without significantly increasing computational costs.\n  Building upon this approach, this thesis proposed novel methods for hallucination detection using LLM internal representations and evaluated them across three benchmarks: TruthfulQA, HaluEval, and ReFact. Specifically, a new architecture that dynamically weights and combines internal LLM layers was developed to improve hallucination detection performance. Throughout extensive experiments, two key findings were obtained: First, the proposed approach was shown to achieve superior performance compared to traditional probing methods, though generalization across benchmarks and LLMs remains challenging. Second, these generalization limitations were demonstrated to be mitigated through cross-benchmark training and parameter freezing. While not consistently improving, both techniques yielded better performance on individual benchmarks and reduced performance degradation when transferred to other benchmarks. These findings open new avenues for improving LLM reliability through internal representation analysis.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) å®¹æ˜“äº§ç”Ÿå¹»è§‰ (hallucinations) çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨æ¨¡å‹å†…éƒ¨è¡¨ç¤º (internal representations) è¿›è¡Œå¹»è§‰æ£€æµ‹çš„æ–°æ–¹æ³•ã€‚ä½œè€…å¼€å‘äº†ä¸€ç§èƒ½å¤ŸåŠ¨æ€åŠ æƒå¹¶ç»“åˆ LLMs å†…éƒ¨å±‚çš„æ–°æ¶æ„ï¼Œæ—¨åœ¨é€šè¿‡æ¢é’ˆå¼åˆ†ç±»å™¨ (probing-based classifiers) åœ¨ä¸æ˜¾è‘—å¢åŠ è®¡ç®—æˆæœ¬çš„æƒ…å†µä¸‹æå‡æ£€æµ‹æ€§èƒ½ã€‚å®éªŒåœ¨ TruthfulQAã€HaluEval å’Œ ReFact ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œï¼Œç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨æ£€æµ‹å‡†ç¡®ç‡ä¸Šä¼˜äºä¼ ç»Ÿçš„æ¢é’ˆæ–¹æ³•ã€‚ç ”ç©¶è¿›ä¸€æ­¥æŒ‡å‡ºï¼Œè™½ç„¶æ¨¡å‹åœ¨ä¸åŒåŸºå‡†é—´çš„æ³›åŒ– (generalization) å­˜åœ¨æŒ‘æˆ˜ï¼Œä½†é€šè¿‡è·¨åŸºå‡†è®­ç»ƒ (cross-benchmark training) å’Œå‚æ•°å†»ç»“ (parameter freezing) æŠ€æœ¯å¯ä»¥æœ‰æ•ˆç¼“è§£è¿™ä¸€å±€é™ã€‚è¯¥é¡¹å·¥ä½œè¯æ˜äº†é€šè¿‡æ·±å…¥åˆ†æ LLMs çš„å†…éƒ¨å±‚ç»“æ„æ¥å¢å¼ºæ¨¡å‹å¯é æ€§çš„æ½œåŠ›ï¼Œä¸ºæé«˜äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„çœŸå®æ€§æä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Master's thesis",
      "pdf_url": "https://arxiv.org/pdf/2509.14254v1",
      "published_date": "2025-09-11 09:50:46 UTC",
      "updated_date": "2025-09-11 09:50:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:58:48.787885+00:00"
    },
    {
      "arxiv_id": "2509.09307v1",
      "title": "Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on Materials Characterization",
      "title_zh": "å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹èƒ½å¦æ¸…æ™°è¾¨æææ–™ï¼Ÿä¸€é¡¹å…³äºææ–™è¡¨å¾çš„å¤šæ¨¡æ€åŸºå‡†",
      "authors": [
        "Zhengzhao Lai",
        "Youbin Zheng",
        "Zhenyang Cai",
        "Haonan Lyu",
        "Jinpu Yang",
        "Hongqing Liang",
        "Yan Hu",
        "Benyou Wang"
      ],
      "abstract": "Materials characterization is fundamental to acquiring materials information, revealing the processing-microstructure-property relationships that guide material design and optimization. While multimodal large language models (MLLMs) have recently shown promise in generative and predictive tasks within materials science, their capacity to understand real-world characterization imaging data remains underexplored. To bridge this gap, we present MatCha, the first benchmark for materials characterization image understanding, comprising 1,500 questions that demand expert-level domain expertise. MatCha encompasses four key stages of materials research comprising 21 distinct tasks, each designed to reflect authentic challenges faced by materials scientists. Our evaluation of state-of-the-art MLLMs on MatCha reveals a significant performance gap compared to human experts. These models exhibit degradation when addressing questions requiring higher-level expertise and sophisticated visual perception. Simple few-shot and chain-of-thought prompting struggle to alleviate these limitations. These findings highlight that existing MLLMs still exhibit limited adaptability to real-world materials characterization scenarios. We hope MatCha will facilitate future research in areas such as new material discovery and autonomous scientific agents. MatCha is available at https://github.com/FreedomIntelligence/MatCha.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†MatChaï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹Materials Characterizationï¼ˆææ–™è¡¨å¾ï¼‰å›¾åƒç†è§£çš„å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°Multimodal Large Language Models (MLLMs) åœ¨å¤„ç†çœŸå®ä¸–ç•Œææ–™ç§‘å­¦æˆåƒæ•°æ®æ–¹é¢çš„èƒ½åŠ›ã€‚MatChaæ¶µç›–äº†ææ–™ç ”ç©¶å››ä¸ªå…³é”®é˜¶æ®µçš„21ä¸ªä¸åŒä»»åŠ¡ï¼ŒåŒ…å«1,500ä¸ªéœ€è¦ä¸“å®¶çº§é¢†åŸŸçŸ¥è¯†çš„é«˜éš¾åº¦é—®é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡è¯¥åŸºå‡†å¯¹å½“å‰æœ€å…ˆè¿›çš„MLLMsè¿›è¡Œäº†è¯„ä¼°ï¼Œå‘ç°è¿™äº›æ¨¡å‹åœ¨å¤„ç†é«˜æ°´å¹³ä¸“ä¸šçŸ¥è¯†å’Œå¤æ‚è§†è§‰æ„ŸçŸ¥ä»»åŠ¡æ—¶ï¼Œä¸äººç±»ä¸“å®¶ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿é‡‡ç”¨Few-shotï¼ˆå°‘æ ·æœ¬æç¤ºï¼‰æˆ–Chain-of-Thoughtï¼ˆé“¾å¼æ€ç»´ï¼‰ç­‰ç­–ç•¥ï¼Œä¹Ÿéš¾ä»¥æ˜¾è‘—æå‡æ¨¡å‹åœ¨ææ–™ç§‘å­¦é¢†åŸŸçš„é€‚åº”æ€§ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†ç°æœ‰MLLMsåœ¨çœŸå®ææ–™è¡¨å¾åœºæ™¯ä¸‹çš„å±€é™æ€§ï¼Œå¹¶ä¸ºæœªæ¥å®ç°æ–°ææ–™å‘ç°å’ŒAutonomous Scientific Agentsï¼ˆè‡ªä¸»ç§‘å­¦æ™ºèƒ½ä½“ï¼‰æä¾›äº†åŸºç¡€å¹³å°ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.09307v1",
      "published_date": "2025-09-11 09:50:16 UTC",
      "updated_date": "2025-09-11 09:50:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:58:56.654521+00:00"
    },
    {
      "arxiv_id": "2509.09292v1",
      "title": "LightAgent: Production-level Open-source Agentic AI Framework",
      "title_zh": "LightAgentï¼šç”Ÿäº§çº§å¼€æºæ™ºèƒ½ä½“ AI æ¡†æ¶",
      "authors": [
        "Weige Cai",
        "Tong Zhu",
        "Jinyi Niu",
        "Ruiqi Hu",
        "Lingyao Li",
        "Tenglong Wang",
        "Xiaowu Dai",
        "Weining Shen",
        "Liwen Zhang"
      ],
      "abstract": "With the rapid advancement of large language models (LLMs), Multi-agent Systems (MAS) have achieved significant progress in various application scenarios. However, substantial challenges remain in designing versatile, robust, and efficient platforms for agent deployment. To address these limitations, we propose \\textbf{LightAgent}, a lightweight yet powerful agentic framework, effectively resolving the trade-off between flexibility and simplicity found in existing frameworks. LightAgent integrates core functionalities such as Memory (mem0), Tools, and Tree of Thought (ToT), while maintaining an extremely lightweight structure. As a fully open-source solution, it seamlessly integrates with mainstream chat platforms, enabling developers to easily build self-learning agents. We have released LightAgent at \\href{https://github.com/wxai-space/LightAgent}{https://github.com/wxai-space/LightAgent}",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)é©±åŠ¨çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ(MAS)åœ¨éƒ¨ç½²æ—¶é¢ä¸´çš„é€šç”¨æ€§ã€é²æ£’æ€§å’Œæ•ˆç‡æŒ‘æˆ˜ï¼Œæå‡ºäº†LightAgentè¿™ä¸€ç”Ÿäº§çº§å¼€æºæ¡†æ¶ã€‚LightAgentæ—¨åœ¨æœ‰æ•ˆè§£å†³ç°æœ‰æ¡†æ¶åœ¨çµæ´»æ€§ä¸ç®€æ´æ€§ä¹‹é—´çš„æƒè¡¡é—®é¢˜ï¼Œæä¾›ä¸€ä¸ªè½»é‡ä¸”åŠŸèƒ½å¼ºå¤§çš„æ™ºèƒ½ä½“å¼€å‘å¹³å°ã€‚è¯¥æ¡†æ¶æ ¸å¿ƒé›†æˆäº†Memory (mem0)ã€Toolsä»¥åŠTree of Thought (ToT)ç­‰å…³é”®åŠŸèƒ½ï¼Œåœ¨ä¿æŒæç®€ç»“æ„çš„åŒæ—¶ç¡®ä¿äº†ç³»ç»Ÿçš„é«˜æ•ˆæ€§ã€‚ä½œä¸ºå®Œå…¨å¼€æºçš„è§£å†³æ–¹æ¡ˆï¼Œå®ƒèƒ½å¤Ÿä¸ä¸»æµèŠå¤©å¹³å°æ— ç¼å¯¹æ¥ï¼Œä½¿å¼€å‘è€…èƒ½å¤Ÿè½»æ¾æ„å»ºå…·æœ‰è‡ªå­¦ä¹ èƒ½åŠ›(self-learning agents)çš„æ™ºèƒ½ä½“ã€‚è¯¥æ¡†æ¶é€šè¿‡ç®€åŒ–ä»£ç†éƒ¨ç½²æµç¨‹ï¼Œä¸ºå®ç°å¯æ‰©å±•çš„AIæ™ºèƒ½ä½“åº”ç”¨æä¾›äº†é‡è¦æ”¯æ’‘ï¼Œç›®å‰å·²åœ¨GitHubå¼€æºã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.09292v1",
      "published_date": "2025-09-11 09:29:13 UTC",
      "updated_date": "2025-09-11 09:29:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:58:53.666232+00:00"
    },
    {
      "arxiv_id": "2509.09290v1",
      "title": "Modality-Agnostic Input Channels Enable Segmentation of Brain lesions in Multimodal MRI with Sequences Unavailable During Training",
      "title_zh": "æ¨¡æ€æ— å…³è¾“å…¥é€šé“å®ç°åŒ…å«è®­ç»ƒå¤–åºåˆ—çš„å¤šæ¨¡æ€ MRI è„‘ç—…ç¶åˆ†å‰²",
      "authors": [
        "Anthony P. Addison",
        "Felix Wagner",
        "Wentian Xu",
        "Natalie Voets",
        "Konstantinos Kamnitsas"
      ],
      "abstract": "Segmentation models are important tools for the detection and analysis of lesions in brain MRI. Depending on the type of brain pathology that is imaged, MRI scanners can acquire multiple, different image modalities (contrasts). Most segmentation models for multimodal brain MRI are restricted to fixed modalities and cannot effectively process new ones at inference. Some models generalize to unseen modalities but may lose discriminative modality-specific information. This work aims to develop a model that can perform inference on data that contain image modalities unseen during training, previously seen modalities, and heterogeneous combinations of both, thus allowing a user to utilize any available imaging modalities. We demonstrate this is possible with a simple, thus practical alteration to the U-net architecture, by integrating a modality-agnostic input channel or pathway, alongside modality-specific input channels. To train this modality-agnostic component, we develop an image augmentation scheme that synthesizes artificial MRI modalities. Augmentations differentially alter the appearance of pathological and healthy brain tissue to create artificial contrasts between them while maintaining realistic anatomical integrity. We evaluate the method using 8 MRI databases that include 5 types of pathologies (stroke, tumours, traumatic brain injury, multiple sclerosis and white matter hyperintensities) and 8 modalities (T1, T1+contrast, T2, PD, SWI, DWI, ADC and FLAIR). The results demonstrate that the approach preserves the ability to effectively process MRI modalities encountered during training, while being able to process new, unseen modalities to improve its segmentation. Project code: https://github.com/Anthony-P-Addison/AGN-MOD-SEG",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€MRIè„‘éƒ¨ç—…å˜åˆ†å‰²æ¨¡å‹åœ¨æ¨ç†é˜¶æ®µæ— æ³•æœ‰æ•ˆå¤„ç†è®­ç»ƒä¸­æœªå‡ºç°çš„å›¾åƒæ¨¡æ€ï¼ˆsequencesï¼‰è¿™ä¸€æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§é›†æˆäº†æ¨¡æ€æ— å…³ï¼ˆmodality-agnosticï¼‰è¾“å…¥é€šé“çš„æ”¹è¿›å‹U-netæ¶æ„ã€‚è¯¥æ¶æ„é€šè¿‡å¹¶è¡Œè®¾ç½®æ¨¡æ€ç‰¹å®šï¼ˆmodality-specificï¼‰é€šé“ä¸æ¨¡æ€æ— å…³è·¯å¾„ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿçµæ´»å¤„ç†å·²çŸ¥æ¨¡æ€ã€æ–°æ¨¡æ€åŠå…¶å¼‚æ„ç»„åˆã€‚ä¸ºäº†è®­ç»ƒè¿™ä¸€æ¨¡æ€æ— å…³ç»„ä»¶ï¼Œä½œè€…å¼€å‘äº†ä¸€ç§åˆ›æ–°çš„å›¾åƒå¢å¼ºæ–¹æ¡ˆï¼ˆimage augmentation schemeï¼‰ï¼Œé€šè¿‡åˆæˆäººå·¥MRIæ¨¡æ€æ¥å·®å¼‚åŒ–æ”¹å˜ç»„ç»‡å¤–è§‚ï¼ŒåŒæ—¶ä¿æŒè§£å‰–ç»“æ„çš„å®Œæ•´æ€§ã€‚ç ”ç©¶åˆ©ç”¨åŒ…å«5ç§è„‘éƒ¨ç—…ç†ï¼ˆå¦‚stroke, tumoursç­‰ï¼‰å’Œ8ç§MRIæ¨¡æ€çš„8ä¸ªæ•°æ®åº“è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒå¯¹å·²çŸ¥è®­ç»ƒæ¨¡æ€å¤„ç†èƒ½åŠ›çš„åŸºç¡€ä¸Šï¼Œå±•ç°äº†å¤„ç†å…¨æ–°æœªçŸ¥æ¨¡æ€å¹¶åˆ©ç”¨å…¶æå‡åˆ†å‰²å‡†ç¡®æ€§çš„æ˜¾è‘—èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to MICCAI 2025, for the following workshop: ML-CDS 2025: Multimodal Learning and Fusion Across Scales for Clinical Decision Support",
      "pdf_url": "https://arxiv.org/pdf/2509.09290v1",
      "published_date": "2025-09-11 09:25:30 UTC",
      "updated_date": "2025-09-11 09:25:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:59:00.463998+00:00"
    },
    {
      "arxiv_id": "2509.09284v3",
      "title": "Tree-OPO: Off-policy Monte Carlo Tree-Guided Advantage Optimization for Multistep Reasoning",
      "title_zh": "Tree-OPOï¼šé¢å‘å¤šæ­¥æ¨ç†çš„ç¦»ç­–è’™ç‰¹å¡æ´›æ ‘å¼•å¯¼ä¼˜åŠ¿ä¼˜åŒ–",
      "authors": [
        "Bingning Huang",
        "Tu Nguyen",
        "Matthieu Zimmer"
      ],
      "abstract": "Recent advances in reasoning with large language models (LLMs) have shown the effectiveness of Monte Carlo Tree Search (MCTS) for generating high quality intermediate trajectories, particularly in math and symbolic domains. Inspired by this, we explore how MCTS derived trajectories, traditionally used for training value or reward models, can be repurposed to improve policy optimization in verifier guided reinforcement learning (RL). Specifically, we focus on Group Relative Policy Optimization (GRPO), a recent algorithm that enables consistent policy learning from group relative judgments. We reframe GRPO into a staged training paradigm, leveraging a teacher's MCTS rollouts to construct a tree structured curriculum of prefixes. This introduces the novel challenge of computing advantages for training samples that originate from different prefixes, each with a distinct expected return. To address this, we propose Staged Advantage Estimation (SAE), a framework for computing low variance, prefix aware advantages by projecting rewards onto a constraint set that respects the tree's hierarchy. Our empirical results on mathematical reasoning tasks show that SAE improves final accuracy over standard GRPO. This outcome is grounded in our theoretical analysis, which confirms that SAE reduces gradient variance, a principled path to improved sample efficiency. We demonstrate this through practical SAE implementations, comparing efficient heuristics against a formal quadratic program.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Tree-OPOï¼Œä¸€ç§åˆ©ç”¨ç¦»ç­–è’™ç‰¹å¡æ´›æ ‘æœç´¢(MCTS)å¼•å¯¼ä¼˜åŠ¿ä¼˜åŒ–çš„å¤šæ­¥æ¨ç†æ”¹è¿›æ¡†æ¶ã€‚ç ”ç©¶è€…é€šè¿‡å°†Group Relative Policy Optimization (GRPO)é‡æ„ä¸ºé˜¶æ®µæ€§è®­ç»ƒèŒƒå¼ï¼Œåˆ©ç”¨æ•™å¸ˆæ¨¡å‹çš„MCTSè½¨è¿¹æ„å»ºå‡ºæ ‘çŠ¶ç»“æ„çš„å‰ç¼€è¯¾ç¨‹ã€‚é’ˆå¯¹ä¸åŒå‰ç¼€æ ·æœ¬å¯¼è‡´çš„ä¼˜åŠ¿è®¡ç®—æŒ‘æˆ˜ï¼Œè®ºæ–‡åˆ›æ–°æ€§åœ°æå‡ºäº†é˜¶æ®µæ€§ä¼˜åŠ¿ä¼°è®¡(Staged Advantage Estimation, SAE)æ¡†æ¶ï¼Œé€šè¿‡å°†å¥–åŠ±æŠ•å½±åˆ°éµå¾ªæ ‘å±‚æ¬¡ç»“æ„çš„çº¦æŸé›†æ¥è®¡ç®—ä½æ–¹å·®ä¸”æ„ŸçŸ¥å‰ç¼€çš„ä¼˜åŠ¿å€¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSAEåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­çš„æœ€ç»ˆå‡†ç¡®ç‡æ˜¾è‘—ä¼˜äºæ ‡å‡†GRPOã€‚ç†è®ºåˆ†æè¿›ä¸€æ­¥è¯å®ï¼ŒSAEé€šè¿‡é™ä½æ¢¯åº¦æ–¹å·®æœ‰æ•ˆåœ°æå‡äº†æ¨¡å‹çš„æ ·æœ¬æ•ˆç‡ï¼Œä¸ºéªŒè¯å™¨å¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ (RL)ç­–ç•¥ä¼˜åŒ–æä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.09284v3",
      "published_date": "2025-09-11 09:18:07 UTC",
      "updated_date": "2025-12-21 16:15:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:59:00.750747+00:00"
    },
    {
      "arxiv_id": "2509.09272v1",
      "title": "Fusing Knowledge and Language: A Comparative Study of Knowledge Graph-Based Question Answering with LLMs",
      "title_zh": "çŸ¥è¯†ä¸è¯­è¨€çš„èåˆï¼šåŸºäºçŸ¥è¯†å›¾è°±çš„å¤§è¯­è¨€æ¨¡å‹é—®ç­”æ¯”è¾ƒç ”ç©¶",
      "authors": [
        "Vaibhav Chaudhary",
        "Neha Soni",
        "Narotam Singh",
        "Amita Kapoor"
      ],
      "abstract": "Knowledge graphs, a powerful tool for structuring information through relational triplets, have recently become the new front-runner in enhancing question-answering systems. While traditional Retrieval Augmented Generation (RAG) approaches are proficient in fact-based and local context-based extraction from concise texts, they encounter limitations when addressing the thematic and holistic understanding of complex, extensive texts, requiring a deeper analysis of both text and context. This paper presents a comprehensive technical comparative study of three different methodologies for constructing knowledge graph triplets and integrating them with Large Language Models (LLMs) for question answering: spaCy, Stanford CoreNLP-OpenIE, and GraphRAG, all leveraging open source technologies. We evaluate the effectiveness, feasibility, and adaptability of these methods by analyzing their capabilities, state of development, and their impact on the performance of LLM-based question answering. Experimental results indicate that while OpenIE provides the most comprehensive coverage of triplets, GraphRAG demonstrates superior reasoning abilities among the three. We conclude with a discussion on the strengths and limitations of each method and provide insights into future directions for improving knowledge graph-based question answering.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿæ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)åœ¨å¤„ç†å¤æ‚é•¿æ–‡æœ¬çš„æ•´ä½“ç†è§£ä¸ä¸»é¢˜åˆ†ææ—¶çš„å±€é™æ€§ï¼Œæ·±å…¥æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨çŸ¥è¯†å›¾è°±(Knowledge Graphs)å¢å¼ºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„é—®ç­”ç³»ç»Ÿæ€§èƒ½ã€‚è®ºæ–‡é€šè¿‡ä¸€é¡¹ç»¼åˆæ€§çš„æŠ€æœ¯å¯¹æ¯”ç ”ç©¶ï¼Œè¯„ä¼°äº†spaCyã€Stanford CoreNLP-OpenIEå’ŒGraphRAGè¿™ä¸‰ç§åŸºäºå¼€æºæŠ€æœ¯çš„çŸ¥è¯†å›¾è°±ä¸‰å…ƒç»„æ„å»ºåŠLLMsé›†æˆæ–¹æ³•ã€‚å®éªŒä»æœ‰æ•ˆæ€§ã€å¯è¡Œæ€§å’Œé€‚åº”æ€§ç­‰å¤šä¸ªç»´åº¦åˆ†æäº†å„æ–¹æ³•å¯¹é—®ç­”è´¨é‡çš„å½±å“ï¼Œç»“æœè¡¨æ˜OpenIEåœ¨ç”Ÿæˆä¸‰å…ƒç»„çš„è¦†ç›–å…¨é¢æ€§ä¸Šå…·æœ‰ä¼˜åŠ¿ã€‚ç„¶è€Œï¼Œåœ¨é€»è¾‘æ¨ç†èƒ½åŠ›æ–¹é¢ï¼ŒGraphRAGçš„è¡¨ç°æ˜æ˜¾ä¼˜äºå…¶ä»–ä¸¤é¡¹æŠ€æœ¯ã€‚è¯¥ç ”ç©¶æœ€åæ€»ç»“äº†ä¸åŒæ–¹æ³•çš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶ä¸ºæœªæ¥æå‡åŸºäºçŸ¥è¯†å›¾è°±çš„é—®ç­”ç³»ç»Ÿç ”ç©¶æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "46 pages, 4 figures, 17 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.09272v1",
      "published_date": "2025-09-11 09:02:15 UTC",
      "updated_date": "2025-09-11 09:02:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:59:17.660464+00:00"
    },
    {
      "arxiv_id": "2509.09262v1",
      "title": "Adaptive Knowledge Distillation using a Device-Aware Teacher for Low-Complexity Acoustic Scene Classification",
      "title_zh": "é¢å‘ä½å¤æ‚åº¦å£°å­¦åœºæ™¯åˆ†ç±»çš„è®¾å¤‡æ„ŸçŸ¥æ•™å¸ˆè‡ªé€‚åº”çŸ¥è¯†è’¸é¦",
      "authors": [
        "Seung Gyu Jeong",
        "Seong Eun Kim"
      ],
      "abstract": "In this technical report, we describe our submission for Task 1, Low-Complexity Device-Robust Acoustic Scene Classification, of the DCASE 2025 Challenge. Our work tackles the dual challenges of strict complexity constraints and robust generalization to both seen and unseen devices, while also leveraging the new rule allowing the use of device labels at test time. Our proposed system is based on a knowledge distillation framework where an efficient CP-MobileNet student learns from a compact, specialized two-teacher ensemble. This ensemble combines a baseline PaSST teacher, trained with standard cross-entropy, and a 'generalization expert' teacher. This expert is trained using our novel Device-Aware Feature Alignment (DAFA) loss, adapted from prior work, which explicitly structures the feature space for device robustness. To capitalize on the availability of test-time device labels, the distilled student model then undergoes a final device-specific fine-tuning stage. Our proposed system achieves a final accuracy of 57.93\\% on the development set, demonstrating a significant improvement over the official baseline, particularly on unseen devices.",
      "tldr_zh": "è¯¥æŠ€æœ¯æŠ¥å‘Šé’ˆå¯¹ DCASE 2025 Challenge ä¸­çš„ä½å¤æ‚åº¦ã€è®¾å¤‡é²æ£’çš„ Acoustic Scene Classification ä»»åŠ¡ï¼Œæå‡ºäº†ä¸€ç§èƒ½å¤ŸåŒæ—¶å…¼é¡¾å¤æ‚åº¦é™åˆ¶ä¸è®¾å¤‡æ³›åŒ–èƒ½åŠ›çš„æ–¹æ¡ˆã€‚è¯¥ç³»ç»ŸåŸºäº Knowledge Distillation æ¡†æ¶ï¼Œä½¿ CP-MobileNet å­¦ç”Ÿæ¨¡å‹ä»åŒ…å« PaSST åŸºå‡†æ•™å¸ˆå’Œâ€œæ³›åŒ–ä¸“å®¶â€æ•™å¸ˆçš„é›†æˆæ¨¡å‹ä¸­å­¦ä¹ ã€‚å…¶ä¸­ï¼Œä¸“å®¶æ•™å¸ˆé€šè¿‡ Device-Aware Feature Alignment (DAFA) æŸå¤±å‡½æ•°æ˜¾å¼æ„å»ºé²æ£’çš„ç‰¹å¾ç©ºé—´ï¼Œä»¥åº”å¯¹ä¸åŒè®¾å¤‡çš„æŒ‘æˆ˜ã€‚ä¸ºäº†åˆ©ç”¨æµ‹è¯•é˜¶æ®µå¯è·å–è®¾å¤‡æ ‡ç­¾çš„æ–°è§„åˆ™ï¼Œè’¸é¦åçš„å­¦ç”Ÿæ¨¡å‹è¿˜ä¼šè¿›è¡Œ Device-specific fine-tuning é˜¶æ®µçš„ä¼˜åŒ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ¡ˆåœ¨å¼€å‘é›†ä¸Šè¾¾åˆ° 57.93% çš„å‡†ç¡®ç‡ï¼Œç›¸è¾ƒäºå®˜æ–¹åŸºå‡†æ¨¡å‹å–å¾—äº†æ˜¾è‘—æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†æœªçŸ¥è®¾å¤‡æ—¶è¡¨ç°å“è¶Šã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.09262v1",
      "published_date": "2025-09-11 08:48:48 UTC",
      "updated_date": "2025-09-11 08:48:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:59:24.453605+00:00"
    },
    {
      "arxiv_id": "2509.09245v2",
      "title": "Jupiter: Enhancing LLM Data Analysis Capabilities via Notebook and Inference-Time Value-Guided Search",
      "title_zh": "Jupiterï¼šé€šè¿‡ Notebook åŠæ¨ç†æ—¶ä»·å€¼å¼•å¯¼æœç´¢æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ•°æ®åˆ†æèƒ½åŠ›",
      "authors": [
        "Shuocheng Li",
        "Yihao Liu",
        "Silin Du",
        "Wenxuan Zeng",
        "Zhe Xu",
        "Mengyu Zhou",
        "Yeye He",
        "Haoyu Dong",
        "Shi Han",
        "Dongmei Zhang"
      ],
      "abstract": "Large language models (LLMs) have shown great promise in automating data science workflows, but existing models still struggle with multi-step reasoning and tool use, which limits their effectiveness on complex data analysis tasks. To address this, we propose a scalable pipeline that extracts high-quality, tool-based data analysis tasks and their executable multi-step solutions from real-world Jupyter notebooks and associated data files. Using this pipeline, we introduce NbQA, a large-scale dataset of standardized task-solution pairs that reflect authentic tool-use patterns in practical data science scenarios. To further enhance multi-step reasoning, we present Jupiter, a framework that formulates data analysis as a search problem and applies Monte Carlo Tree Search (MCTS) to generate diverse solution trajectories for value model learning. During inference, Jupiter combines the value model and node visit counts to efficiently collect executable multi-step plans with minimal search steps. Experimental results show that Qwen2.5-7B and 14B-Instruct models on NbQA solve 77.82% and 86.38% of tasks on InfiAgent-DABench, respectively-matching or surpassing GPT-4o and advanced agent frameworks. Further evaluations demonstrate improved generalization and stronger tool-use reasoning across diverse multi-step reasoning tasks. Code and data are available at https://github.com/microsoft/Jupiter.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Jupiteræ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤æ‚æ•°æ®åˆ†æä»»åŠ¡ä¸­é¢ä¸´çš„å¤šæ­¥æ¨ç†ä¸å·¥å…·ä½¿ç”¨(tool use)èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿé¦–å…ˆå¼€å‘äº†ä¸€å¥—å¯æ‰©å±•çš„æµæ°´çº¿ï¼Œä»çœŸå®çš„Jupyter notebookså’Œç›¸å…³æ•°æ®æ–‡ä»¶ä¸­æå–é«˜è´¨é‡ä»»åŠ¡ï¼Œæ„å»ºäº†å¤§è§„æ¨¡æ ‡å‡†åŒ–æ•°æ®é›†NbQAï¼Œä»¥åæ˜ çœŸå®çš„æ•°æ®ç§‘å­¦åœºæ™¯ã€‚Jupiterå°†æ•°æ®åˆ†æå»ºæ¨¡ä¸ºä¸€ä¸ªæœç´¢é—®é¢˜ï¼Œåˆ©ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢(MCTS)ç”Ÿæˆå¤šæ ·åŒ–çš„è§£é¢˜è½¨è¿¹ï¼Œè¿›è€Œç”¨äºä»·å€¼æ¨¡å‹(value model)çš„å­¦ä¹ ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œè¯¥æ¡†æ¶é€šè¿‡ç»“åˆä»·å€¼æ¨¡å‹å’ŒèŠ‚ç‚¹è®¿é—®è®¡æ•°ï¼Œèƒ½å¤Ÿä»¥æå°‘çš„æœç´¢æ­¥éª¤é«˜æ•ˆç”Ÿæˆå¯æ‰§è¡Œçš„å¤šæ­¥è®¡åˆ’ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨NbQAä¸Šè®­ç»ƒçš„Qwen2.5-7Bå’Œ14B-Instructæ¨¡å‹åœ¨InfiAgent-DABenchåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå‡†ç¡®ç‡åˆ†åˆ«è¾¾åˆ°77.82%å’Œ86.38%ï¼Œå…¶æ€§èƒ½å¯åª²ç¾ç”šè‡³è¶…è¶ŠGPT-4oåŠå…ˆè¿›çš„æ™ºèƒ½ä½“æ¡†æ¶ã€‚è¯¥ç ”ç©¶ä¸ä»…æ˜¾è‘—å¢å¼ºäº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¹Ÿä¸ºè‡ªåŠ¨åŒ–æ•°æ®ç§‘å­¦å·¥ä½œæµæä¾›äº†æ›´å¼ºçš„å·¥å…·ä½¿ç”¨æ¨ç†æ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to AAAI 2026 (Main Technical Track)",
      "pdf_url": "https://arxiv.org/pdf/2509.09245v2",
      "published_date": "2025-09-11 08:27:54 UTC",
      "updated_date": "2025-12-03 09:56:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:59:21.449525+00:00"
    },
    {
      "arxiv_id": "2509.09242v1",
      "title": "CoAtNeXt:An Attention-Enhanced ConvNeXtV2-Transformer Hybrid Model for Gastric Tissue Classification",
      "title_zh": "CoAtNeXtï¼šç”¨äºèƒƒç»„ç»‡åˆ†ç±»çš„æ³¨æ„åŠ›å¢å¼º ConvNeXtV2-Transformer æ··åˆæ¨¡å‹",
      "authors": [
        "Mustafa Yurdakul",
        "Sakir Tasdemir"
      ],
      "abstract": "Background and objective Early diagnosis of gastric diseases is crucial to prevent fatal outcomes. Although histopathologic examination remains the diagnostic gold standard, it is performed entirely manually, making evaluations labor-intensive and prone to variability among pathologists. Critical findings may be missed, and lack of standard procedures reduces consistency. These limitations highlight the need for automated, reliable, and efficient methods for gastric tissue analysis. Methods In this study, a novel hybrid model named CoAtNeXt was proposed for the classification of gastric tissue images. The model is built upon the CoAtNet architecture by replacing its MBConv layers with enhanced ConvNeXtV2 blocks. Additionally, the Convolutional Block Attention Module (CBAM) is integrated to improve local feature extraction through channel and spatial attention mechanisms. The architecture was scaled to achieve a balance between computational efficiency and classification performance. CoAtNeXt was evaluated on two publicly available datasets, HMU-GC-HE-30K for eight-class classification and GasHisSDB for binary classification, and was compared against 10 Convolutional Neural Networks (CNNs) and ten Vision Transformer (ViT) models. Results CoAtNeXt achieved 96.47% accuracy, 96.60% precision, 96.47% recall, 96.45% F1 score, and 99.89% AUC on HMU-GC-HE-30K. On GasHisSDB, it reached 98.29% accuracy, 98.07% precision, 98.41% recall, 98.23% F1 score, and 99.90% AUC. It outperformed all CNN and ViT models tested and surpassed previous studies in the literature. Conclusion Experimental results show that CoAtNeXt is a robust architecture for histopathological classification of gastric tissue images, providing performance on binary and multiclass. Its highlights its potential to assist pathologists by enhancing diagnostic accuracy and reducing workload.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹èƒƒç—…ç»„ç»‡ç—…ç†å­¦äººå·¥è¯Šæ–­å­˜åœ¨çš„åŠ³åŠ¨å¼ºåº¦å¤§åŠä¸€è‡´æ€§ä¸è¶³ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º CoAtNeXt çš„æ··åˆæ¨¡å‹ã€‚è¯¥æ¶æ„åœ¨ CoAtNet çš„åŸºç¡€ä¸Šï¼Œé€šè¿‡å°† MBConv å±‚æ›¿æ¢ä¸ºå¢å¼ºçš„ ConvNeXtV2 æ¨¡å—ï¼Œå¹¶é›†æˆå·ç§¯å—æ³¨æ„åŠ›æ¨¡å— (CBAM) æ¥ä¼˜åŒ–é€šé“ä¸ç©ºé—´ç»´åº¦çš„å±€éƒ¨ç‰¹å¾æå–ã€‚å®éªŒåœ¨ HMU-GC-HE-30K å¤šåˆ†ç±»å’Œ GasHisSDB äºŒåˆ†ç±»æ•°æ®é›†ä¸Šå±•å¼€ï¼Œå¹¶ä¸ 10 ç§å·ç§¯ç¥ç»ç½‘ç»œ (CNNs) åŠ 10 ç§è§†è§‰å˜æ¢å™¨ (ViT) æ¨¡å‹è¿›è¡Œäº†æ·±å…¥å¯¹æ¯”ã€‚ç»“æœè¡¨æ˜ï¼ŒCoAtNeXt åœ¨ä¸¤ç±»ä»»åŠ¡ä¸­åˆ†åˆ«å®ç°äº† 96.47% å’Œ 98.29% çš„å‡†ç¡®ç‡ï¼Œæ˜¾è‘—è¶…è¶Šäº†æ‰€æœ‰æµ‹è¯•çš„ CNN å’Œ ViT æ¨¡å‹ã€‚è¯¥ç ”ç©¶è¯æ˜äº† CoAtNeXt æ˜¯ä¸€ç§é²æ£’çš„èƒƒç»„ç»‡å›¾åƒåˆ†ç±»æ¶æ„ï¼Œå±•ç°å‡ºè¾…åŠ©ç—…ç†åŒ»ç”Ÿæå‡è¯Šæ–­ç²¾åº¦å¹¶å‡è½»å·¥ä½œè´Ÿæ‹…çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.09242v1",
      "published_date": "2025-09-11 08:24:50 UTC",
      "updated_date": "2025-09-11 08:24:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:59:23.873745+00:00"
    },
    {
      "arxiv_id": "2509.09235v1",
      "title": "Virtual staining for 3D X-ray histology of bone implants",
      "title_zh": "éª¨æ¤å…¥ç‰©ä¸‰ç»´ X å°„çº¿ç»„ç»‡å­¦çš„è™šæ‹ŸæŸ“è‰²",
      "authors": [
        "Sarah C. Irvine",
        "Christian Lucas",
        "Diana KrÃ¼ger",
        "Bianca Guedert",
        "Julian Moosmann",
        "Berit Zeller-Plumhoff"
      ],
      "abstract": "Three-dimensional X-ray histology techniques offer a non-invasive alternative to conventional 2D histology, enabling volumetric imaging of biological tissues without the need for physical sectioning or chemical staining. However, the inherent greyscale image contrast of X-ray tomography limits its biochemical specificity compared to traditional histological stains. Within digital pathology, deep learning-based virtual staining has demonstrated utility in simulating stained appearances from label-free optical images. In this study, we extend virtual staining to the X-ray domain by applying cross-modality image translation to generate artificially stained slices from synchrotron-radiation-based micro-CT scans. Using over 50 co-registered image pairs of micro-CT and toluidine blue-stained histology from bone-implant samples, we trained a modified CycleGAN network tailored for limited paired data. Whole slide histology images were downsampled to match the voxel size of the CT data, with on-the-fly data augmentation for patch-based training. The model incorporates pixelwise supervision and greyscale consistency terms, producing histologically realistic colour outputs while preserving high-resolution structural detail. Our method outperformed Pix2Pix and standard CycleGAN baselines across SSIM, PSNR, and LPIPS metrics. Once trained, the model can be applied to full CT volumes to generate virtually stained 3D datasets, enhancing interpretability without additional sample preparation. While features such as new bone formation were able to be reproduced, some variability in the depiction of implant degradation layers highlights the need for further training data and refinement. This work introduces virtual staining to 3D X-ray imaging and offers a scalable route for chemically informative, label-free tissue characterisation in biomedical research.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ X-ray æ–­å±‚æ‰«æåœ¨ç”ŸåŒ–ç‰¹å¼‚æ€§æ–¹é¢çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§ç”¨äºéª¨æ¤å…¥ç‰© 3D X-ray ç»„ç»‡å­¦ç ”ç©¶çš„ Virtual staining æ–¹æ³•ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡è·¨æ¨¡æ€å›¾åƒè½¬æ¢æŠ€æœ¯ï¼Œåˆ©ç”¨åŸºäºåŒæ­¥è¾å°„çš„ micro-CT æ‰«æç”Ÿæˆäººå·¥æŸ“è‰²åˆ‡ç‰‡ï¼Œå®ç°äº†æ— éœ€ç‰©ç†åˆ‡ç‰‡æˆ–åŒ–å­¦æŸ“è‰²çš„ä½“ç§¯æˆåƒã€‚é€šè¿‡å¯¹éª¨æ¤å…¥ç‰©æ ·æœ¬çš„ micro-CT ä¸ Toluidine Blue æŸ“è‰²å›¾åƒè¿›è¡Œé…å¯¹ï¼Œç ”ç©¶è€…è®­ç»ƒäº†ä¸€ä¸ªé’ˆå¯¹æœ‰é™æ•°æ®ä¼˜åŒ–çš„æ”¹è¿›å‹ CycleGAN ç½‘ç»œã€‚è¯¥æ¨¡å‹é€šè¿‡å¼•å…¥åƒç´ çº§ç›‘ç£(pixelwise supervision)å’Œç°åº¦ä¸€è‡´æ€§é¡¹(greyscale consistency terms)ï¼Œåœ¨ç”Ÿæˆé«˜åº¦å†™å®çš„ç»„ç»‡å­¦è‰²å½©çš„åŒæ—¶ä¿ç•™äº†é«˜åˆ†è¾¨ç‡ç»“æ„ç»†èŠ‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ SSIMã€PSNR å’Œ LPIPS æŒ‡æ ‡ä¸Šå‡ä¼˜äº Pix2Pix å’Œæ ‡å‡† CycleGAN åŸºçº¿æ¨¡å‹ã€‚è®­ç»ƒåçš„æ¨¡å‹å¯ç›´æ¥åº”ç”¨äºå®Œæ•´ CT ä½“æ•°æ®ä»¥ç”Ÿæˆ 3D è™šæ‹ŸæŸ“è‰²æ•°æ®é›†ï¼Œå¹¶èƒ½æˆåŠŸå¤ç° new bone formation ç­‰å…³é”®ç”Ÿç‰©å­¦ç‰¹å¾ã€‚è¿™é¡¹å·¥ä½œå°† Virtual staining å¼•å…¥ 3D X-ray æˆåƒé¢†åŸŸï¼Œä¸ºç”Ÿç‰©åŒ»å­¦ç ”ç©¶æä¾›äº†ä¸€ç§æ— éœ€æ ‡è®°ä¸”å…·å¤‡åŒ–å­¦ä¿¡æ¯ç‰¹å¾çš„ç»„ç»‡è¡¨å¾æ–°é€”å¾„ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "physics.comp-ph",
        "q-bio.QM"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.09235v1",
      "published_date": "2025-09-11 08:14:31 UTC",
      "updated_date": "2025-09-11 08:14:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:59:27.450816+00:00"
    },
    {
      "arxiv_id": "2509.10569v2",
      "title": "MarkDiffusion: An Open-Source Toolkit for Generative Watermarking of Latent Diffusion Models",
      "title_zh": "MarkDiffusionï¼šé¢å‘æ½œåœ¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆå¼æ°´å°çš„å¼€æºå·¥å…·åŒ…",
      "authors": [
        "Leyi Pan",
        "Sheng Guan",
        "Zheyu Fu",
        "Luyang Si",
        "Huan Wang",
        "Zian Wang",
        "Hanqian Li",
        "Xuming Hu",
        "Irwin King",
        "Philip S. Yu",
        "Aiwei Liu",
        "Lijie Wen"
      ],
      "abstract": "We introduce MarkDiffusion, an open-source Python toolkit for generative watermarking of latent diffusion models. It comprises three key components: a unified implementation framework for streamlined watermarking algorithm integrations and user-friendly interfaces; a mechanism visualization suite that intuitively showcases added and extracted watermark patterns to aid public understanding; and a comprehensive evaluation module offering standard implementations of 24 tools across three essential aspects - detectability, robustness, and output quality - plus 8 automated evaluation pipelines. Through MarkDiffusion, we seek to assist researchers, enhance public awareness and engagement in generative watermarking, and promote consensus while advancing research and applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† MarkDiffusionï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸º Latent Diffusion Models (LDMs) çš„ç”Ÿæˆå¼æ°´å° (Generative Watermarking) è®¾è®¡çš„å¼€æº Python å·¥å…·åŒ…ã€‚è¯¥å·¥å…·åŒ…åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šä¸€æ˜¯æ—¨åœ¨ç®€åŒ–ç®—æ³•é›†æˆå¹¶æä¾›å‹å¥½ç•Œé¢çš„ç»Ÿä¸€å®ç°æ¡†æ¶ï¼›äºŒæ˜¯èƒ½å¤Ÿç›´è§‚å±•ç¤ºæ°´å°æ·»åŠ ä¸æå–æ¨¡å¼çš„æœºåˆ¶å¯è§†åŒ–å¥—ä»¶ï¼Œç”¨ä»¥å¢å¼ºå…¬ä¼—ç†è§£ã€‚ä¸‰æ˜¯åŠŸèƒ½å…¨é¢çš„è¯„ä¼°æ¨¡å—ï¼Œæä¾›äº†æ¶µç›–å¯æ£€æµ‹æ€§ (Detectability)ã€é²æ£’æ€§ (Robustness) å’Œè¾“å‡ºè´¨é‡ (Output Quality) ä¸‰ä¸ªç»´åº¦çš„ 24 ç§æ ‡å‡†å·¥å…·åŠ 8 æ¡è‡ªåŠ¨åŒ–è¯„ä¼°æµæ°´çº¿ã€‚é€šè¿‡ MarkDiffusionï¼Œç ”ç©¶å›¢é˜Ÿæ—¨åœ¨ååŠ©ç§‘ç ”äººå‘˜æ›´é«˜æ•ˆåœ°å¼€å±•å®éªŒï¼Œå¹¶æå‡å…¬ä¼—å¯¹ç”Ÿæˆå¼æ°´å°æŠ€æœ¯çš„å…³æ³¨ä¸å‚ä¸ã€‚è¿™ä¸€å¼€æºé¡¹ç›®ä¸ºæ¨åŠ¨è¯¥é¢†åŸŸçš„ç ”ç©¶è¿›å±•ã€ä¿ƒæˆè¡Œä¸šå…±è¯†ä»¥åŠä¼˜åŒ–å®é™…åº”ç”¨æä¾›äº†é‡è¦çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CR",
      "comment": "23 pages, 13 figures, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.10569v2",
      "published_date": "2025-09-11 07:57:22 UTC",
      "updated_date": "2025-10-16 07:42:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:59:36.992517+00:00"
    },
    {
      "arxiv_id": "2509.09219v2",
      "title": "Vejde: A Framework for Inductive Deep Reinforcement Learning Based on Factor Graph Color Refinement",
      "title_zh": "Vejdeï¼šåŸºäºå› å­å›¾é¢œè‰²ç²¾ç‚¼çš„å½’çº³å¼æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¡†æ¶",
      "authors": [
        "Jakob Nyberg",
        "Pontus Johnson"
      ],
      "abstract": "We present and evaluate Vejde; a framework which combines data abstraction, graph neural networks and reinforcement learning to produce inductive policy functions for decision problems with richly structured states, such as object classes and relations. MDP states are represented as data bases of facts about entities, and Vejde converts each state to a bipartite graph, which is mapped to latent states through neural message passing. The factored representation of both states and actions allows Vejde agents to handle problems of varying size and structure. We tested Vejde agents on eight problem domains defined in RDDL, with ten problem instances each, where policies were trained using both supervised and reinforcement learning. To test policy generalization, we separate problem instances in two sets, one for training and the other solely for testing. Test results on unseen instances for the Vejde agents were compared to MLP agents trained on each problem instance, as well as the online planning algorithm Prost. Our results show that Vejde policies in average generalize to the test instances without a significant loss in score. Additionally, the inductive agents received scores on unseen test instances that on average were close to the instance-specific MLP agents.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Vejdeï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆäº†æ•°æ®æŠ½è±¡ã€å›¾ç¥ç»ç½‘ç»œ (Graph Neural Networks) å’Œå¼ºåŒ–å­¦ä¹  (Reinforcement Learning) çš„æ¡†æ¶ï¼Œæ—¨åœ¨ä¸ºå…·æœ‰ä¸°å¯Œç»“æ„åŒ–çŠ¶æ€ï¼ˆå¦‚å¯¹è±¡ç±»åˆ«å’Œå…³ç³»ï¼‰çš„å†³ç­–é—®é¢˜ç”Ÿæˆå½’çº³å¼ç­–ç•¥å‡½æ•° (Inductive Policy Functions)ã€‚åœ¨è¯¥æ¡†æ¶ä¸­ï¼ŒMDP çŠ¶æ€è¢«è¡¨ç¤ºä¸ºäº‹å®æ•°æ®åº“å¹¶è½¬åŒ–ä¸ºäºŒéƒ¨å›¾ (Bipartite Graph)ï¼Œé€šè¿‡ç¥ç»æ¶ˆæ¯ä¼ é€’ (Neural Message Passing) æ˜ å°„è‡³æ½œç©ºé—´ã€‚å¾—ç›ŠäºçŠ¶æ€å’ŒåŠ¨ä½œçš„å› å­åˆ†è§£è¡¨ç¤º (Factored Representation)ï¼ŒVejde æ™ºèƒ½ä½“èƒ½å¤Ÿçµæ´»å¤„ç†è§„æ¨¡å’Œç»“æ„å„å¼‚çš„å¤æ‚ä»»åŠ¡ã€‚ç ”ç©¶åœ¨ 8 ä¸ª RDDL é—®é¢˜åŸŸä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œå¹¶å°†å…¶æ€§èƒ½ä¸é’ˆå¯¹ç‰¹å®šå®ä¾‹è®­ç»ƒçš„ MLP æ™ºèƒ½ä½“ä»¥åŠåœ¨çº¿è§„åˆ’ç®—æ³• Prost è¿›è¡Œäº†å¯¹æ¯”ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVejde ç­–ç•¥åœ¨å¹³å‡æ°´å¹³ä¸Šèƒ½æœ‰æ•ˆæ³›åŒ–è‡³æœªè§è¿‡çš„æµ‹è¯•å®ä¾‹ï¼Œä¸”å¾—åˆ†è¡¨ç°æ¥è¿‘ç‰¹å®šå®ä¾‹çš„åŸºå‡†æ¨¡å‹ï¼Œè¯æ˜äº†å…¶åœ¨ç»“æ„åŒ–å†³ç­–é—®é¢˜ä¸­çš„å¼ºå¤§å½’çº³èƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.09219v2",
      "published_date": "2025-09-11 07:51:38 UTC",
      "updated_date": "2026-01-19 14:07:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:59:41.566053+00:00"
    },
    {
      "arxiv_id": "2509.09215v1",
      "title": "Enabling Regulatory Multi-Agent Collaboration: Architecture, Challenges, and Solutions",
      "title_zh": "å®ç°ç›‘ç®¡å¤šæ™ºèƒ½ä½“åä½œï¼šæ¶æ„ã€æŒ‘æˆ˜ä¸è§£å†³æ–¹æ¡ˆ",
      "authors": [
        "Qinnan Hu",
        "Yuntao Wang",
        "Yuan Gao",
        "Zhou Su",
        "Linkang Du"
      ],
      "abstract": "Large language models (LLMs)-empowered autonomous agents are transforming both digital and physical environments by enabling adaptive, multi-agent collaboration. While these agents offer significant opportunities across domains such as finance, healthcare, and smart manufacturing, their unpredictable behaviors and heterogeneous capabilities pose substantial governance and accountability challenges. In this paper, we propose a blockchain-enabled layered architecture for regulatory agent collaboration, comprising an agent layer, a blockchain data layer, and a regulatory application layer. Within this framework, we design three key modules: (i) an agent behavior tracing and arbitration module for automated accountability, (ii) a dynamic reputation evaluation module for trust assessment in collaborative scenarios, and (iii) a malicious behavior forecasting module for early detection of adversarial activities. Our approach establishes a systematic foundation for trustworthy, resilient, and scalable regulatory mechanisms in large-scale agent ecosystems. Finally, we discuss the future research directions for blockchain-enabled regulatory frameworks in multi-agent systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)èµ‹èƒ½çš„è‡ªä¸»æ™ºèƒ½ä½“åœ¨å¤šæ™ºèƒ½ä½“åä½œä¸­å­˜åœ¨çš„ä¸å¯é¢„æµ‹è¡Œä¸ºåŠæ²»ç†æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºåŒºå—é“¾(blockchain-enabled)çš„åˆ†å±‚ç›‘ç®¡æ¶æ„ã€‚è¯¥æ¶æ„ç”±æ™ºèƒ½ä½“å±‚(agent layer)ã€åŒºå—é“¾æ•°æ®å±‚(blockchain data layer)å’Œç›‘ç®¡åº”ç”¨å±‚(regulatory application layer)ç»„æˆï¼Œæ—¨åœ¨å»ºç«‹ä¸€ä¸ªç³»ç»Ÿæ€§ã€å¯ä¿¡ä¸”å¯æ‰©å±•çš„ç›‘ç®¡æœºåˆ¶ã€‚ç ”ç©¶æ ¸å¿ƒè®¾è®¡åŒ…å«ç”¨äºè‡ªåŠ¨é—®è´£çš„è¡Œä¸ºè¿½è¸ªä¸ä»²è£æ¨¡å—ã€ç”¨äºä¿¡ä»»è¯„å®šçš„åŠ¨æ€å£°èª‰è¯„ä¼°æ¨¡å—ï¼Œä»¥åŠç”¨äºæ—©æœŸè¯†åˆ«å¯¹æŠ—æ€§æ´»åŠ¨çš„æ¶æ„è¡Œä¸ºé¢„æµ‹æ¨¡å—ã€‚è¯¥æ–¹æ¡ˆä¸ºå¤§è§„æ¨¡æ™ºèƒ½ä½“ç”Ÿæ€ç³»ç»Ÿæä¾›äº†ç¨³å¥çš„æ²»ç†æ¡†æ¶ï¼Œæœ‰æ•ˆæå‡äº†å¤šæ™ºèƒ½ä½“åä½œç³»ç»Ÿçš„éŸ§æ€§ä¸å®‰å…¨æ€§ï¼Œå¹¶ä¸ºæœªæ¥åŒºå—é“¾é©±åŠ¨çš„ç›‘ç®¡ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.AI",
      "comment": "7 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.09215v1",
      "published_date": "2025-09-11 07:46:00 UTC",
      "updated_date": "2025-09-11 07:46:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:59:37.164725+00:00"
    },
    {
      "arxiv_id": "2509.09210v1",
      "title": "ProgD: Progressive Multi-scale Decoding with Dynamic Graphs for Joint Multi-agent Motion Forecasting",
      "title_zh": "ProgDï¼šé¢å‘è”åˆå¤šæ™ºèƒ½ä½“è¿åŠ¨é¢„æµ‹çš„åŠ¨æ€å›¾æ¸è¿›å¼å¤šå°ºåº¦è§£ç ",
      "authors": [
        "Xing Gao",
        "Zherui Huang",
        "Weiyao Lin",
        "Xiao Sun"
      ],
      "abstract": "Accurate motion prediction of surrounding agents is crucial for the safe planning of autonomous vehicles. Recent advancements have extended prediction techniques from individual agents to joint predictions of multiple interacting agents, with various strategies to address complex interactions within future motions of agents. However, these methods overlook the evolving nature of these interactions. To address this limitation, we propose a novel progressive multi-scale decoding strategy, termed ProgD, with the help of dynamic heterogeneous graph-based scenario modeling. In particular, to explicitly and comprehensively capture the evolving social interactions in future scenarios, given their inherent uncertainty, we design a progressive modeling of scenarios with dynamic heterogeneous graphs. With the unfolding of such dynamic heterogeneous graphs, a factorized architecture is designed to process the spatio-temporal dependencies within future scenarios and progressively eliminate uncertainty in future motions of multiple agents. Furthermore, a multi-scale decoding procedure is incorporated to improve on the future scenario modeling and consistent prediction of agents' future motion. The proposed ProgD achieves state-of-the-art performance on the INTERACTION multi-agent prediction benchmark, ranking $1^{st}$, and the Argoverse 2 multi-world forecasting benchmark.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ProgDï¼Œä¸€ç§ç»“åˆåŠ¨æ€å¼‚æ„å›¾(dynamic heterogeneous graphs)å»ºæ¨¡çš„æ¸è¿›å¼å¤šå°ºåº¦è§£ç ç­–ç•¥ï¼Œæ—¨åœ¨è§£å†³è”åˆå¤šæ™ºèƒ½ä½“è¿åŠ¨é¢„æµ‹(Joint Multi-agent Motion Forecasting)ä¸­äº¤äº’å…³ç³»æ¼”è¿›è¢«å¿½è§†çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡åŠ¨æ€å¼‚æ„å›¾å¯¹æœªæ¥åœºæ™¯è¿›è¡Œæ¸è¿›å¼å»ºæ¨¡ï¼Œä»¥æ•è·æ¼”åŒ–çš„ç¤¾äº¤äº¤äº’åŠå…¶å›ºæœ‰çš„ä¸ç¡®å®šæ€§ã€‚ProgDé‡‡ç”¨åˆ†è§£å¼æ¶æ„å¤„ç†æœªæ¥åœºæ™¯ä¸­çš„æ—¶ç©ºä¾èµ–å…³ç³»(spatio-temporal dependencies)ï¼Œå¹¶ç»“åˆå¤šå°ºåº¦è§£ç ç¨‹åºæ¥æ”¹è¿›åœºæ™¯å»ºæ¨¡æ•ˆæœï¼Œç¡®ä¿å¤šæ™ºèƒ½ä½“æœªæ¥è¿åŠ¨é¢„æµ‹çš„ä¸€è‡´æ€§ã€‚å®éªŒè¯æ˜ï¼ŒProgDåœ¨INTERACTIONå¤šæ™ºèƒ½ä½“é¢„æµ‹åŸºå‡†æµ‹è¯•ä¸­æ’åç¬¬ä¸€ï¼Œå¹¶åœ¨Argoverse 2åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›(SOTA)çš„æ€§èƒ½è¡¨ç°ã€‚",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.09210v1",
      "published_date": "2025-09-11 07:36:54 UTC",
      "updated_date": "2025-09-11 07:36:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:59:39.565579+00:00"
    },
    {
      "arxiv_id": "2509.09208v1",
      "title": "Incentivizing Safer Actions in Policy Optimization for Constrained Reinforcement Learning",
      "title_zh": "çº¦æŸå¼ºåŒ–å­¦ä¹ ç­–ç•¥ä¼˜åŒ–ä¸­çš„æ›´å®‰å…¨åŠ¨ä½œæ¿€åŠ±",
      "authors": [
        "Somnath Hazra",
        "Pallab Dasgupta",
        "Soumyajit Dey"
      ],
      "abstract": "Constrained Reinforcement Learning (RL) aims to maximize the return while adhering to predefined constraint limits, which represent domain-specific safety requirements. In continuous control settings, where learning agents govern system actions, balancing the trade-off between reward maximization and constraint satisfaction remains a significant challenge. Policy optimization methods often exhibit instability near constraint boundaries, resulting in suboptimal training performance. To address this issue, we introduce a novel approach that integrates an adaptive incentive mechanism in addition to the reward structure to stay within the constraint bound before approaching the constraint boundary. Building on this insight, we propose Incrementally Penalized Proximal Policy Optimization (IP3O), a practical algorithm that enforces a progressively increasing penalty to stabilize training dynamics. Through empirical evaluation on benchmark environments, we demonstrate the efficacy of IP3O compared to the performance of state-of-the-art Safe RL algorithms. Furthermore, we provide theoretical guarantees by deriving a bound on the worst-case error of the optimality achieved by our algorithm.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å—é™å¼ºåŒ–å­¦ä¹ (Constrained Reinforcement Learning)ä¸­å¥–åŠ±æœ€å¤§åŒ–ä¸çº¦æŸæ»¡è¶³ä¹‹é—´çš„æƒè¡¡æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯è¿ç»­æ§åˆ¶åœºæ™¯ä¸‹ç­–ç•¥ä¼˜åŒ–åœ¨çº¦æŸè¾¹ç•Œé™„è¿‘çš„ä¸ç¨³å®šæ€§é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆè‡ªé€‚åº”æ¿€åŠ±æœºåˆ¶(adaptive incentive mechanism)çš„æ–°æ–¹æ³•ã€‚ç ”ç©¶è€…é€šè¿‡åœ¨å¥–åŠ±ç»“æ„å¤–å¼•å…¥æ¿€åŠ±ï¼Œå¼•å¯¼æ™ºèƒ½ä½“åœ¨æ¥è¿‘è¾¹ç•Œå‰ä¿æŒåœ¨å®‰å…¨åŒºåŸŸï¼Œå¹¶æ®æ­¤å¼€å‘äº†IP3O(Incrementally Penalized Proximal Policy Optimization)ç®—æ³•ã€‚è¯¥ç®—æ³•é€šè¿‡æ–½åŠ æ¸è¿›å¢åŠ çš„æƒ©ç½šé¡¹æ¥ç¨³å®šè®­ç»ƒåŠ¨æ€ï¼Œæœ‰æ•ˆè§£å†³äº†è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ³¢åŠ¨é—®é¢˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒIP3Oåœ¨å¤šä¸ªåŸºå‡†ç¯å¢ƒä¸­çš„è¡¨ç°ä¼˜äºå½“å‰å‰æ²¿çš„å®‰å…¨å¼ºåŒ–å­¦ä¹ (Safe RL)ç®—æ³•ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜ä¸ºç®—æ³•çš„æœ€ä¼˜æ€§æä¾›äº†æœ€åæƒ…å†µè¯¯å·®ç•Œé™(worst-case error bound)çš„ç†è®ºä¿éšœã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "11 pages, Accepted to the 34th International Joint Conference on Artificial Intelligence (IJCAI) 2025, Main Track",
      "pdf_url": "https://arxiv.org/pdf/2509.09208v1",
      "published_date": "2025-09-11 07:33:35 UTC",
      "updated_date": "2025-09-11 07:33:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:59:44.556040+00:00"
    },
    {
      "arxiv_id": "2509.09744v4",
      "title": "Structure Matters: Brain Graph Augmentation via Learnable Edge Masking for Data-efficient Psychiatric Diagnosis",
      "title_zh": "ç»“æ„ä¹‹é‡ï¼šåŸºäºå¯å­¦ä¹ è¾¹æ©ç çš„è„‘å›¾å¢å¼ºæ–¹æ³•ï¼ŒåŠ©åŠ›æ•°æ®é«˜æ•ˆçš„ç²¾ç¥ç–¾ç—…è¯Šæ–­",
      "authors": [
        "Mujie Liu",
        "Chenze Wang",
        "Liping Chen",
        "Nguyen Linh Dan Le",
        "Niharika Tewari",
        "Ting Dang",
        "Jiangang Ma",
        "Feng Xia"
      ],
      "abstract": "The limited availability of labeled brain network data makes it challenging to achieve accurate and interpretable psychiatric diagnoses. While self-supervised learning (SSL) offers a promising solution, existing methods often rely on augmentation strategies that can disrupt crucial structural semantics in brain graphs. To address this, we propose SAM-BG, a two-stage framework for learning brain graph representations with structural semantic preservation. In the pre-training stage, an edge masker is trained on a small labeled subset to capture key structural semantics. In the SSL stage, the extracted structural priors guide a structure-aware augmentation process, enabling the model to learn more semantically meaningful and robust representations. Experiments on two real-world psychiatric datasets demonstrate that SAM-BG outperforms state-of-the-art methods, particularly in small-labeled data settings, and uncovers clinically relevant connectivity patterns that enhance interpretability. Our code is available at https://github.com/mjliu99/SAM-BG.",
      "tldr_zh": "é’ˆå¯¹å—æ ‡è®°æ•°æ®é‡é™åˆ¶è€Œå¯¼è‡´ç²¾ç¥ç–¾ç—…è¯Šæ–­å‡†ç¡®æ€§ä¸å¯è§£é‡Šæ€§ä¸è¶³çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶æŒ‡å‡ºä¼ ç»Ÿçš„è‡ªç›‘ç£å­¦ä¹ (SSL)å¢å¼ºç­–ç•¥å¾€å¾€ä¼šç ´åè„‘ç½‘ç»œå›¾(brain graphs)ä¸­çš„å…³é”®ç»“æ„è¯­ä¹‰ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†SAM-BGï¼Œä¸€ç§æ—¨åœ¨ä¿ç•™ç»“æ„è¯­ä¹‰çš„ä¸¤é˜¶æ®µè„‘ç½‘ç»œå›¾è¡¨ç¤ºå­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶åœ¨é¢„è®­ç»ƒé˜¶æ®µé€šè¿‡åœ¨å°‘é‡æ ‡è®°æ•°æ®ä¸Šè®­ç»ƒè¾¹ç¼˜æ©ç å™¨(edge masker)ä»¥æ•æ‰å…³é”®çš„ç»“æ„è¯­ä¹‰ï¼Œå¹¶åœ¨éšåçš„SSLé˜¶æ®µåˆ©ç”¨æå–çš„ç»“æ„å…ˆéªŒçŸ¥è¯†å¼•å¯¼ç»“æ„æ„ŸçŸ¥å¢å¼ºè¿‡ç¨‹(structure-aware augmentation)ã€‚å®éªŒåœ¨ä¸¤ä¸ªçœŸå®ä¸–ç•Œçš„ç²¾ç¥ç–¾ç—…æ•°æ®é›†ä¸Šè¯æ˜ï¼ŒSAM-BGåœ¨å°‘æ ‡è®°æ•°æ®åœºæ™¯ä¸‹è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰çš„SOTAæ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæ­ç¤ºå…·æœ‰ä¸´åºŠç›¸å…³æ€§çš„è¿æ¥æ¨¡å¼ï¼Œæœ‰æ•ˆåœ°æå‡äº†è¾…åŠ©è¯Šæ–­çš„å¯è§£é‡Šæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.09744v4",
      "published_date": "2025-09-11 07:24:39 UTC",
      "updated_date": "2025-09-24 06:55:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T17:59:59.765591+00:00"
    },
    {
      "arxiv_id": "2509.09204v1",
      "title": "Bona fide Cross Testing Reveals Weak Spot in Audio Deepfake Detection Systems",
      "title_zh": "çœŸå®è¯­éŸ³äº¤å‰æµ‹è¯•æ­ç¤ºéŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹ç³»ç»Ÿçš„è–„å¼±ç¯èŠ‚",
      "authors": [
        "Chin Yuen Kwok",
        "Jia Qi Yip",
        "Zhen Qiu",
        "Chi Hung Chi",
        "Kwok Yan Lam"
      ],
      "abstract": "Audio deepfake detection (ADD) models are commonly evaluated using datasets that combine multiple synthesizers, with performance reported as a single Equal Error Rate (EER). However, this approach disproportionately weights synthesizers with more samples, underrepresenting others and reducing the overall reliability of EER. Additionally, most ADD datasets lack diversity in bona fide speech, often featuring a single environment and speech style (e.g., clean read speech), limiting their ability to simulate real-world conditions. To address these challenges, we propose bona fide cross-testing, a novel evaluation framework that incorporates diverse bona fide datasets and aggregates EERs for more balanced assessments. Our approach improves robustness and interpretability compared to traditional evaluation methods. We benchmark over 150 synthesizers across nine bona fide speech types and release a new dataset to facilitate further research at https://github.com/cyaaronk/audio_deepfake_eval.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºå½“å‰çš„éŸ³é¢‘ä¼ªé€ æ£€æµ‹ (Audio deepfake detection, ADD) æ¨¡å‹è¯„ä¼°æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œé€šå¸¸ä½¿ç”¨å•ä¸€çš„ç­‰é”™è¯¯ç‡ (Equal Error Rate, EER) æ±‡æŠ¥æ€§èƒ½ï¼Œå¯¼è‡´å¯¹æ ·æœ¬æ•°è¾ƒå°‘çš„åˆæˆå™¨ä»£è¡¨æ€§ä¸è¶³ï¼Œä¸”ç”±äºç¼ºä¹çœŸå®è¯­éŸ³ (bona fide speech) çš„å¤šæ ·æ€§è€Œéš¾ä»¥æ¨¡æ‹ŸçœŸå®ä¸–ç•Œæ¡ä»¶ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œä½œè€…æå‡ºäº† bona fide cross-testing è¯„ä¼°æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆå¤šæ ·åŒ–çœŸå®è¯­éŸ³æ•°æ®é›†å¹¶èšåˆ EER ä»¥å®ç°æ›´å¹³è¡¡è¯„ä¼°çš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡çº³å…¥å¤šç§ç¯å¢ƒå’Œè¯­éŸ³é£æ ¼çš„çœŸå®æ•°æ®ï¼Œæ˜¾è‘—æé«˜äº†è¯„ä¼°è¿‡ç¨‹çš„é²æ£’æ€§ä¸å¯è§£é‡Šæ€§ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨ä¹ç§çœŸå®è¯­éŸ³ç±»å‹ä¸‹å¯¹è¶…è¿‡ 150 ç§åˆæˆå™¨è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå¹¶å‘å¸ƒäº†ä¸€ä¸ªå…¨æ–°çš„æ•°æ®é›†ä»¥æ¨åŠ¨åç»­ç ”ç©¶ã€‚è¿™ä¸€å·¥ä½œæ­ç¤ºäº†ç°æœ‰ ADD ç³»ç»Ÿåœ¨åº”å¯¹å¤æ‚çœŸå®åœºæ™¯æ—¶çš„è–„å¼±ç¯èŠ‚ï¼Œä¸ºæ„å»ºæ›´å¯é çš„éŸ³é¢‘é˜²ä¼ªç³»ç»Ÿæä¾›äº†é‡è¦çš„è¯„ä¼°æ ‡å‡†å’Œå·¥å…·æ”¯æ’‘ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SD",
      "comment": "Published in Interspeech 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.09204v1",
      "published_date": "2025-09-11 07:20:18 UTC",
      "updated_date": "2025-09-11 07:20:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T18:00:03.255566+00:00"
    },
    {
      "arxiv_id": "2509.09197v1",
      "title": "Improving Synthetic Data Training for Contextual Biasing Models with a Keyword-Aware Cost Function",
      "title_zh": "åˆ©ç”¨å…³é”®è¯æ„ŸçŸ¥ä»£ä»·å‡½æ•°æ”¹è¿›ä¸Šä¸‹æ–‡åç½®æ¨¡å‹çš„åˆæˆæ•°æ®è®­ç»ƒ",
      "authors": [
        "Chin Yuen Kwok",
        "Jia Qi Yip",
        "Eng Siong Chng"
      ],
      "abstract": "Rare word recognition can be improved by adapting ASR models to synthetic data that includes these words. Further improvements can be achieved through contextual biasing, which trains and adds a biasing module into the model architecture to prioritize rare words. While training the module on synthetic rare word data is more effective than using non-rare-word data, it can lead to overfitting due to artifacts in the synthetic audio. To address this, we enhance the TCPGen-based contextual biasing approach and propose a keyword-aware loss function that additionally focuses on biased words when training biasing modules. This loss includes a masked cross-entropy term for biased word prediction and a binary classification term for detecting biased word positions. These two terms complementarily support the decoding of biased words during inference. By adapting Whisper to 10 hours of synthetic data, our method reduced the word error rate on the NSC Part 2 test set from 29.71% to 11.81%.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨è¯­éŸ³è¯†åˆ«(ASR)ä¸­çš„ç¨€æœ‰è¯è¯†åˆ«é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ”¹è¿›çš„Contextual Biasingæ¨¡å‹è®­ç»ƒæ–¹æ³•ã€‚ä¸ºäº†å…‹æœåˆæˆè¯­éŸ³æ•°æ®ä¸­çš„äººä¸ºç—•è¿¹å¯¼è‡´çš„è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œä½œè€…åœ¨åŸºäºTCPGençš„æ¡†æ¶ä¸‹å¼•å…¥äº†ä¸€ç§Keyword-Aware Cost Functionã€‚è¯¥æŸå¤±å‡½æ•°é›†æˆäº†ç”¨äºåç½®è¯é¢„æµ‹çš„Masked Cross-Entropyé¡¹å’Œç”¨äºæ£€æµ‹åç½®è¯ä½ç½®çš„äºŒå…ƒåˆ†ç±»é¡¹ï¼Œä¸¤è€…åœ¨æ¨ç†é˜¶æ®µäº’è¡¥åœ°æ”¯æŒåç½®è¯çš„è§£ç ã€‚é€šè¿‡å°†Whisperæ¨¡å‹åœ¨10å°æ—¶åˆæˆæ•°æ®ä¸Šè¿›è¡Œé€‚é…ï¼Œè¯¥æ–¹æ³•åœ¨NSC Part 2æµ‹è¯•é›†ä¸Šçš„å­—é”™è¯¯ç‡(WER)ä»29.71%æ˜¾è‘—é™ä½è‡³11.81%ã€‚è¯¥æˆæœè¯æ˜äº†é€šè¿‡å…³é”®è¯æ„ŸçŸ¥æŸå¤±å‡½æ•°å¼ºåŒ–æ¨¡å‹å¯¹ç‰¹å®šåç½®è¯çš„å…³æ³¨åº¦ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡ASRç³»ç»Ÿåœ¨å¤„ç†ç¨€æœ‰è¯æ±‡æ—¶çš„å‡†ç¡®æ€§ä¸é²æ£’æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Published in Interspeech 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.09197v1",
      "published_date": "2025-09-11 07:12:17 UTC",
      "updated_date": "2025-09-11 07:12:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T18:00:07.706773+00:00"
    },
    {
      "arxiv_id": "2509.09196v1",
      "title": "Efficient Trie-based Biasing using K-step Prediction for Rare Word Recognition",
      "title_zh": "åŸºäº K æ­¥é¢„æµ‹çš„é«˜æ•ˆ Trie åç½®ç”Ÿåƒ»è¯è¯†åˆ«",
      "authors": [
        "Chin Yuen Kwok",
        "Jia Qi yip"
      ],
      "abstract": "Contextual biasing improves rare word recognition of ASR models by prioritizing the output of rare words during decoding. A common approach is Trie-based biasing, which gives \"bonus scores\" to partial hypothesis (e.g. \"Bon\") that may lead to the generation of the rare word (e.g. \"Bonham\"). If the full word (\"Bonham\") isn't ultimately recognized, the system revokes those earlier bonuses. This revocation is limited to beam search and is computationally expensive, particularly for models with large decoders. To overcome these limitations, we propose adapting ASR models to look ahead and predict multiple steps at once. This avoids the revocation step entirely by better estimating whether a partial hypothesis will lead to the generation of the full rare word. By fine-tuning Whisper with only 10 hours of synthetic data, our method reduces the word error rate on the NSC Part 2 test set from 30.86% to 12.19%.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨è¯­éŸ³è¯†åˆ«(ASR)æ¨¡å‹åœ¨è¯†åˆ«ç¨€æœ‰è¯æ—¶çš„æŒ‘æˆ˜ï¼Œæ¢è®¨äº†å¦‚ä½•é€šè¿‡ä¸Šä¸‹æ–‡åç½®(Contextual biasing)æŠ€æœ¯æé«˜è¯†åˆ«å‡†ç¡®ç‡ã€‚ä¼ ç»Ÿçš„åŸºäºå‰ç¼€æ ‘çš„åç½®(Trie-based biasing)æ–¹æ³•é€šè¿‡ä¸ºå¯èƒ½ç”Ÿæˆç¨€æœ‰è¯çš„éƒ¨åˆ†å‡è®¾æä¾›å¥–åŠ±åˆ†æ•°ï¼Œä½†ç”±äºéœ€è¦åœ¨æœ€ç»ˆæœªè¯†åˆ«æˆåŠŸæ—¶æ’¤é”€åˆ†æ•°ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬é«˜æ˜‚ä¸”å—é™äºé›†æŸæœç´¢(beam search)ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åˆ©ç”¨Kæ­¥é¢„æµ‹(K-step Prediction)æ¥æå‰é¢„åˆ¤åç»­ç”Ÿæˆå†…å®¹çš„æ–¹æ³•ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä¸€æ¬¡æ€§é¢„æµ‹å¤šä¸ªæ­¥éª¤ã€‚è¿™ç§æ–¹æ³•é€šè¿‡æ›´å‡†ç¡®åœ°ä¼°ç®—éƒ¨åˆ†å‡è®¾æ˜¯å¦ä¼šå¯¼å‡ºå®Œæ•´çš„ç¨€æœ‰è¯ï¼Œä»è€Œå®Œå…¨é¿å…äº†å¤æ‚çš„æ’¤é”€æ­¥éª¤ã€‚ç ”ç©¶é€šè¿‡ä»…ä½¿ç”¨10å°æ—¶çš„åˆæˆæ•°æ®å¯¹Whisperæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨NSC Part 2æµ‹è¯•é›†ä¸Šå°†è¯é”™è¯¯ç‡(WER)ä»30.86%æ˜¾è‘—é™ä½è‡³12.19%ï¼Œå¤§å¹…æå‡äº†ç¨€æœ‰è¯è¯†åˆ«çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Published in Interspeech 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.09196v1",
      "published_date": "2025-09-11 07:11:46 UTC",
      "updated_date": "2025-09-11 07:11:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T18:00:07.944976+00:00"
    },
    {
      "arxiv_id": "2509.09194v1",
      "title": "On Integrating Large Language Models and Scenario-Based Programming for Improving Software Reliability",
      "title_zh": "èåˆå¤§è¯­è¨€æ¨¡å‹ä¸åŸºäºåœºæ™¯çš„ç¼–ç¨‹ä»¥æå‡è½¯ä»¶å¯é æ€§",
      "authors": [
        "Ayelet Berzack",
        "Guy Katz"
      ],
      "abstract": "Large Language Models (LLMs) are fast becoming indispensable tools for software developers, assisting or even partnering with them in crafting complex programs. The advantages are evident -- LLMs can significantly reduce development time, generate well-organized and comprehensible code, and occasionally suggest innovative ideas that developers might not conceive on their own. However, despite their strengths, LLMs will often introduce significant errors and present incorrect code with persuasive confidence, potentially misleading developers into accepting flawed solutions.\n  In order to bring LLMs into the software development cycle in a more reliable manner, we propose a methodology for combining them with ``traditional'' software engineering techniques in a structured way, with the goal of streamlining the development process, reducing errors, and enabling users to verify crucial program properties with increased confidence. Specifically, we focus on the Scenario-Based Programming (SBP) paradigm -- an event-driven, scenario-based approach for software engineering -- to allow human developers to pour their expert knowledge into the LLM, as well as to inspect and verify its outputs.\n  To evaluate our methodology, we conducted a significant case study, and used it to design and implement the Connect4 game. By combining LLMs and SBP we were able to create a highly-capable agent, which could defeat various strong existing agents. Further, in some cases, we were able to formally verify the correctness of our agent. Finally, our experience reveals interesting insights regarding the ease-of-use of our proposed approach. The full code of our case-study will be made publicly available with the final version of this paper.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å°†å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)ä¸åœºæ™¯åŒ–ç¼–ç¨‹(Scenario-Based Programming, SBP)ç›¸ç»“åˆçš„æ–¹æ³•è®ºï¼Œæ—¨åœ¨æé«˜è½¯ä»¶å¼€å‘çš„å¯é æ€§å¹¶å‡å°‘æ¨¡å‹ç”Ÿæˆçš„é”™è¯¯ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ SBP çš„äº‹ä»¶é©±åŠ¨å’ŒåŸºäºåœºæ™¯çš„ç‰¹æ€§ï¼Œå…è®¸å¼€å‘äººå‘˜å°†ä¸“å®¶çŸ¥è¯†æ³¨å…¥ LLMï¼Œå¹¶å¯¹å…¶è¾“å‡ºè¿›è¡Œä¸¥æ ¼çš„æ£€æŸ¥ä¸éªŒè¯ã€‚é€šè¿‡å¯¹ Connect4 æ¸¸æˆçš„æ¡ˆä¾‹ç ”ç©¶ï¼Œç ”ç©¶è¯æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿåˆ›å»ºå‡ºå‡»è´¥å¤šç§å¼ºåŠ›åŸºå‡†çš„æ™ºèƒ½ä½“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨æŸäº›æƒ…å†µä¸‹èƒ½å¤Ÿå®ç°ç¨‹åºæ­£ç¡®æ€§çš„å½¢å¼åŒ–éªŒè¯(Formal Verification)ï¼Œæ˜¾è‘—æå‡äº†ç”¨æˆ·å¯¹ç¨‹åºå±æ€§çš„ä¿¡å¿ƒã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜åˆ†äº«äº†å…³äºè¯¥æ–¹æ³•æ˜“ç”¨æ€§çš„è§è§£ï¼Œä¸ºå°† LLMs æ›´å¯é åœ°é›†æˆåˆ°è½¯ä»¶å¼€å‘ç”Ÿå‘½å‘¨æœŸä¸­æä¾›äº†å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.09194v1",
      "published_date": "2025-09-11 07:10:25 UTC",
      "updated_date": "2025-09-11 07:10:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T18:00:15.691862+00:00"
    },
    {
      "arxiv_id": "2509.09192v1",
      "title": "Probing Pre-trained Language Models on Code Changes: Insights from ReDef, a High-Confidence Just-in-Time Defect Prediction Dataset",
      "title_zh": "æ¢ç©¶é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å¯¹ä»£ç å˜æ›´çš„ç†è§£ï¼šæ¥è‡ªé«˜ç½®ä¿¡åº¦å³æ—¶ç¼ºé™·é¢„æµ‹æ•°æ®é›† ReDef çš„å¯ç¤º",
      "authors": [
        "Doha Nam",
        "Taehyoun Kim",
        "Duksan Ryu",
        "Jongmoon Baik"
      ],
      "abstract": "Just-in-Time software defect prediction (JIT-SDP) plays a critical role in prioritizing risky code changes during code review and continuous integration. However, existing datasets often suffer from noisy labels and low precision in identifying bug-inducing commits. To address this, we present ReDef (Revert-based Defect dataset), a high-confidence benchmark of function-level modifications curated from 22 large-scale C/C++ projects. Defective cases are anchored by revert commits, while clean cases are validated through post-hoc history checks. Ambiguous instances are conservatively filtered out via a GPT-assisted triage process involving multiple votes and audits. This pipeline yields 3,164 defective and 10,268 clean modifications, offering substantially more reliable labels than prior existing resources. Beyond dataset construction, we provide the first systematic evaluation of how pre-trained language models (PLMs) reason about code modifications -- specifically, which input encodings most effectively expose change information, and whether models genuinely capture edit semantics. We fine-tune CodeBERT, CodeT5+, and UniXcoder under five encoding strategies, and further probe their sensitivity through counterfactual perturbations that swap added/deleted blocks, invert diff polarity, or inject spurious markers. Our results show that compact diff-style encodings consistently outperform whole-function formats across all PLMs, with statistical tests confirming large, model-independent effects. However, under counterfactual tests, performance degrades little or not at all -- revealing that what appears to be robustness in fact reflects reliance on superficial cues rather than true semantic understanding. These findings indicate that, unlike in snapshot-based tasks, current PLMs remain limited in their ability to genuinely comprehend code modifications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å³æ—¶ç¼ºé™·é¢„æµ‹(Just-in-Time Defect Prediction, JIT-SDP)ç°æœ‰æ•°æ®é›†å­˜åœ¨çš„æ ‡ç­¾å™ªå£°å’Œè¯†åˆ«ç²¾åº¦ä½ç­‰é—®é¢˜ï¼Œæå‡ºäº†é«˜è´¨é‡åŸºå‡†æ•°æ®é›†ReDefã€‚è¯¥æ•°æ®é›†æ¶µç›–22ä¸ªå¤§è§„æ¨¡C/C++é¡¹ç›®ï¼Œé€šè¿‡revert commitsé”šå®šç¼ºé™·å¹¶ç»“åˆGPTè¾…åŠ©çš„å®¡è®¡æµç¨‹ï¼Œæä¾›äº†æ¯”ä»¥å¾€èµ„æºæ›´å¯é çš„å‡½æ•°çº§ä¿®æ”¹æ ‡ç­¾ã€‚ç ”ç©¶è€…ä»¥æ­¤ä¸ºåŸºç¡€ï¼Œç³»ç»Ÿè¯„ä¼°äº†CodeBERTã€CodeT5+å’ŒUniXcoderç­‰é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹(PLMs)å¯¹ä»£ç ä¿®æ”¹çš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒå‘ç°ï¼Œç´§å‡‘çš„diffé£æ ¼ç¼–ç ç­–ç•¥åœ¨æ•æ‰ä¿®æ”¹ä¿¡æ¯æ–¹é¢æ˜¾è‘—ä¼˜äºå…¨å‡½æ•°æ ¼å¼ã€‚ç„¶è€Œï¼Œé€šè¿‡äº¤æ¢ä»£ç å—æˆ–åè½¬ææ€§ç­‰åäº‹å®æ‰°åŠ¨(Counterfactual Perturbations)è¿›è¡Œçš„æ•æ„Ÿæ€§æ¢æµ‹æ˜¾ç¤ºï¼Œæ¨¡å‹æ€§èƒ½åœ¨è¾“å…¥å—æŸæ—¶å‡ ä¹æ²¡æœ‰ä¸‹é™ã€‚è¿™æ­ç¤ºäº†å½“å‰çš„PLMsåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–è¡¨å±‚çº¿ç´¢è€ŒéçœŸæ­£çš„ç¼–è¾‘è¯­ä¹‰ç†è§£ï¼Œè¯æ˜äº†ç°æœ‰æ¨¡å‹åœ¨å¤„ç†ä»£ç åŠ¨æ€æ¼”åŒ–ä»»åŠ¡æ—¶ä»å­˜åœ¨æ˜¾è‘—å±€é™ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "An anonymous link containing the dataset, construction scripts, and experimental code is publicly available for reproducibility: https://figshare.com/s/4f202bc0921e26b41dc2",
      "pdf_url": "https://arxiv.org/pdf/2509.09192v1",
      "published_date": "2025-09-11 07:07:11 UTC",
      "updated_date": "2025-09-11 07:07:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T18:00:18.225891+00:00"
    },
    {
      "arxiv_id": "2509.09183v1",
      "title": "Dark-ISP: Enhancing RAW Image Processing for Low-Light Object Detection",
      "title_zh": "Dark-ISPï¼šé¢å‘ä½å…‰ç…§ç›®æ ‡æ£€æµ‹çš„ RAW å›¾åƒå¤„ç†å¢å¼º",
      "authors": [
        "Jiasheng Guo",
        "Xin Gao",
        "Yuxiang Yan",
        "Guanghao Li",
        "Jian Pu"
      ],
      "abstract": "Low-light Object detection is crucial for many real-world applications but remains challenging due to degraded image quality. While recent studies have shown that RAW images offer superior potential over RGB images, existing approaches either use RAW-RGB images with information loss or employ complex frameworks. To address these, we propose a lightweight and self-adaptive Image Signal Processing (ISP) plugin, Dark-ISP, which directly processes Bayer RAW images in dark environments, enabling seamless end-to-end training for object detection. Our key innovations are: (1) We deconstruct conventional ISP pipelines into sequential linear (sensor calibration) and nonlinear (tone mapping) sub-modules, recasting them as differentiable components optimized through task-driven losses. Each module is equipped with content-aware adaptability and physics-informed priors, enabling automatic RAW-to-RGB conversion aligned with detection objectives. (2) By exploiting the ISP pipeline's intrinsic cascade structure, we devise a Self-Boost mechanism that facilitates cooperation between sub-modules. Through extensive experiments on three RAW image datasets, we demonstrate that our method outperforms state-of-the-art RGB- and RAW-based detection approaches, achieving superior results with minimal parameters in challenging low-light environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Dark-ISPï¼Œä¸€ç§è½»é‡ä¸”è‡ªé€‚åº”çš„å›¾åƒä¿¡å·å¤„ç†(Image Signal Processing, ISP)æ’ä»¶ï¼Œæ—¨åœ¨ç›´æ¥å¤„ç†é»‘æš—ç¯å¢ƒä¸‹çš„Bayer RAWå›¾åƒä»¥å®ç°ç«¯åˆ°ç«¯çš„ç›®æ ‡æ£€æµ‹ã€‚é€šè¿‡å°†ä¼ ç»ŸISPæµæ°´çº¿è§£æ„ä¸ºå¯å¾®åˆ†çš„çº¿æ€§ä¼ æ„Ÿå™¨æ ¡å‡†(sensor calibration)ä¸éçº¿æ€§è‰²è°ƒæ˜ å°„(tone mapping)å­æ¨¡å—ï¼ŒDark-ISPèƒ½å¤Ÿæ ¹æ®æ£€æµ‹ä»»åŠ¡éœ€æ±‚è¿›è¡Œè‡ªåŠ¨åŒ–ä¼˜åŒ–ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†å†…å®¹æ„ŸçŸ¥è‡ªé€‚åº”æ€§å’Œç‰©ç†å…ˆéªŒçŸ¥è¯†ï¼Œç¡®ä¿RAWåˆ°RGBçš„è½¬æ¢è¿‡ç¨‹ä¸ç›®æ ‡æ£€æµ‹ç›®æ ‡é«˜åº¦ä¸€è‡´ã€‚æ­¤å¤–ï¼Œç ”ç©¶åˆ©ç”¨ISPçš„çº§è”ç»“æ„è®¾è®¡äº†Self-Boostæœºåˆ¶ï¼Œæœ‰æ•ˆä¿ƒè¿›äº†å„å­æ¨¡å—é—´çš„ååŒå¢å¼ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDark-ISPåœ¨ä¸‰ä¸ªRAWæ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰çš„RGBå’ŒRAWæ£€æµ‹æ–¹æ¡ˆï¼Œåœ¨æä½å…‰ç…§ç¯å¢ƒä¸‹ä»¥æå°çš„å‚æ•°é‡å®ç°äº†é¢†å…ˆçš„æ£€æµ‹ç²¾åº¦ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 6 figures, conference",
      "pdf_url": "https://arxiv.org/pdf/2509.09183v1",
      "published_date": "2025-09-11 06:44:43 UTC",
      "updated_date": "2025-09-11 06:44:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T18:00:22.631712+00:00"
    },
    {
      "arxiv_id": "2509.09177v3",
      "title": "Clip Your Sequences Fairly: Enforcing Length Fairness for Sequence-Level RL",
      "title_zh": "å…¬å¹³è£å‰ªåºåˆ—ï¼šåœ¨åºåˆ—çº§å¼ºåŒ–å­¦ä¹ ä¸­å®ç°é•¿åº¦å…¬å¹³æ€§",
      "authors": [
        "Hanyi Mao",
        "Quanjia Xiao",
        "Lei Pang",
        "Haixiao Liu"
      ],
      "abstract": "We propose FSPO (Fair Sequence Policy Optimization), a sequence-level reinforcement learning method for LLMs that enforces length-fair clipping on the importance-sampling (IS) weight. We study RL methods with sequence-level IS and identify a mismatch when PPO/GRPO-style clipping is transplanted to sequences: a fixed clip range systematically reweights short vs. long responses, distorting the optimization direction. FSPO introduces a simple remedy: we clip the sequence log-IS ratio with a band that scales as $\\sqrt{L}$. Theoretically, we formalize length fairness via a Length Reweighting Error (LRE) and prove that small LRE yields a cosine directional guarantee between the clipped and true updates. Empirically, FSPO flattens clip rates across length bins, stabilizes training, and outperforms baselines across model sizes and evaluation datasets, with the largest gains on the Qwen3-8B-Base model.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FSPO (Fair Sequence Policy Optimization)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨ä¸ºå¤§è¯­è¨€æ¨¡å‹(LLMs)åºåˆ—çº§å¼ºåŒ–å­¦ä¹ (Sequence-Level RL)æä¾›é•¿åº¦å…¬å¹³è£å‰ªçš„æ–¹æ³•ã€‚ä½œè€…æŒ‡å‡ºï¼Œç°æœ‰çš„PPOæˆ–GRPOé£æ ¼è£å‰ªåœ¨å¤„ç†åºåˆ—çº§ä»»åŠ¡æ—¶ï¼Œå›ºå®šçš„è£å‰ªèŒƒå›´ä¼šç³»ç»Ÿæ€§åœ°é‡åŠ æƒé•¿çŸ­å“åº”ï¼Œå¯¼è‡´ä¼˜åŒ–æ–¹å‘å‘ç”Ÿæ‰­æ›²ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼ŒFSPOå¼•å…¥äº†ä¸€ç§éšåºåˆ—é•¿åº¦å¹³æ–¹æ ¹$\\sqrt{L}$ç¼©æ”¾çš„å¯¹æ•°é‡è¦æ€§é‡‡æ ·æ¯”ç‡(log-IS ratio)è£å‰ªç­–ç•¥ã€‚ç†è®ºå±‚é¢ï¼Œè¯¥ç ”ç©¶é€šè¿‡é•¿åº¦é‡åŠ æƒè¯¯å·®(Length Reweighting Error, LRE)æ­£å¼å®šä¹‰äº†é•¿åº¦å…¬å¹³æ€§ï¼Œå¹¶è¯æ˜äº†è¯¥æ–¹æ³•å¯¹æ¢¯åº¦æ›´æ–°æ–¹å‘çš„ä½™å¼¦æ–¹å‘ä¿è¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFSPOèƒ½å¤Ÿä½¿ä¸åŒé•¿åº¦åŒºé—´çš„è£å‰ªç‡è¶‹äºå¹³ç¨³ï¼Œæ˜¾è‘—æå‡äº†è®­ç»ƒç¨³å®šæ€§ï¼Œå¹¶åœ¨å¤šä¸ªæ¨¡å‹å’Œæ•°æ®é›†ä¸Šå‡è¶…è¶Šäº†åŸºçº¿æ¨¡å‹ï¼Œå°¤å…¶åœ¨Qwen3-8B-Baseæ¨¡å‹ä¸Šå–å¾—äº†æ˜¾è‘—å¢ç›Šã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.09177v3",
      "published_date": "2025-09-11 06:27:10 UTC",
      "updated_date": "2025-10-13 07:18:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T18:00:25.185458+00:00"
    },
    {
      "arxiv_id": "2509.09174v1",
      "title": "EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for Speech-to-Speech LLMs",
      "title_zh": "EchoXï¼šæ—¨åœ¨é€šè¿‡å›å£°è®­ç»ƒå¼¥åˆè¯­éŸ³å¤§è¯­è¨€æ¨¡å‹çš„å£°å­¦-è¯­ä¹‰é¸¿æ²Ÿ",
      "authors": [
        "Yuhao Zhang",
        "Yuhao Du",
        "Zhanchen Dai",
        "Xiangnan Ma",
        "Kaiqi Kou",
        "Benyou Wang",
        "Haizhou Li"
      ],
      "abstract": "Speech-to-speech large language models (SLLMs) are attracting increasing attention. Derived from text-based large language models (LLMs), SLLMs often exhibit degradation in knowledge and reasoning capabilities. We hypothesize that this limitation arises because current training paradigms for SLLMs fail to bridge the acoustic-semantic gap in the feature representation space. To address this issue, we propose EchoX, which leverages semantic representations and dynamically generates speech training targets. This approach integrates both acoustic and semantic learning, enabling EchoX to preserve strong reasoning abilities as a speech LLM. Experimental results demonstrate that EchoX, with about six thousand hours of training data, achieves advanced performance on multiple knowledge-based question-answering benchmarks. The project is available at https://github.com/FreedomIntelligence/EchoX.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†EchoXï¼Œæ—¨åœ¨è§£å†³è¯­éŸ³åˆ°è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹(Speech-to-speech large language models, SLLMs)åœ¨ç»§æ‰¿è‡ªæ–‡æœ¬LLMsæ—¶å‡ºç°çš„çŸ¥è¯†ä¸æ¨ç†èƒ½åŠ›é€€åŒ–é—®é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿè®¤ä¸ºè¿™ä¸€å±€é™æ€§æºäºç°æœ‰è®­ç»ƒèŒƒå¼æœªèƒ½æœ‰æ•ˆå¼¥åˆç‰¹å¾è¡¨ç¤ºç©ºé—´ä¸­çš„å£°å­¦-è¯­ä¹‰é¸¿æ²Ÿ(acoustic-semantic gap)ã€‚EchoXé€šè¿‡åˆ©ç”¨è¯­ä¹‰è¡¨ç¤ºå¹¶åŠ¨æ€ç”Ÿæˆè¯­éŸ³è®­ç»ƒç›®æ ‡ï¼Œå°†å£°å­¦ä¸è¯­ä¹‰å­¦ä¹ æ·±åº¦æ•´åˆï¼Œä½¿æ¨¡å‹åœ¨è¯­éŸ³æ¨¡å¼ä¸‹ä¾ç„¶èƒ½ä¿ç•™å¼ºå¤§çš„é€»è¾‘æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä»…ä½¿ç”¨çº¦å…­åƒå°æ—¶çš„è®­ç»ƒæ•°æ®ï¼ŒEchoXå°±åœ¨å¤šä¸ªåŸºäºçŸ¥è¯†çš„é—®ç­”(question-answering)åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å…ˆè¿›æ€§èƒ½ã€‚è¯¥é¡¹ç ”ç©¶ä¸ºä¼˜åŒ–SLLMsçš„ç‰¹å¾è¡¨ç¤ºæä¾›äº†åˆ›æ–°è·¯å¾„ï¼Œç›¸å…³é¡¹ç›®å·²åœ¨GitHubå¼€æºã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.09174v1",
      "published_date": "2025-09-11 06:17:59 UTC",
      "updated_date": "2025-09-11 06:17:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T18:00:27.490129+00:00"
    },
    {
      "arxiv_id": "2509.09168v2",
      "title": "Adaptive Pareto-Optimal Token Merging for Edge Transformer Models in Semantic Communication",
      "title_zh": "è¯­ä¹‰é€šä¿¡ä¸­è¾¹ç¼˜ Transformer æ¨¡å‹çš„è‡ªé€‚åº”å¸•ç´¯æ‰˜æœ€ä¼˜ Token åˆå¹¶",
      "authors": [
        "Omar Erak",
        "Omar Alhussein",
        "Hatem Abou-Zeid",
        "Mehdi Bennis"
      ],
      "abstract": "Large-scale transformer models have emerged as a powerful tool for semantic communication systems, enabling edge devices to extract rich representations for robust inference across noisy wireless channels. However, their substantial computational demands remain a major barrier to practical deployment in resource-constrained 6G networks. In this paper, we present a training-free framework for adaptive token merging in pretrained vision transformers to jointly reduce inference time and transmission resource usage. We formulate the selection of per-layer merging proportions as a multi-objective optimization problem to balance accuracy and computational cost. We employ Gaussian process-based Bayesian optimization to construct a Pareto frontier of optimal configurations, enabling flexible runtime adaptation to dynamic application requirements and channel conditions. Extensive experiments demonstrate that our method consistently outperforms other baselines and achieves significant reductions in floating-point operations while maintaining competitive accuracy across a wide range of signal-to-noise ratio (SNR) conditions. Additional results highlight the effectiveness of adaptive policies that adjust merging aggressiveness in response to channel quality, providing a practical mechanism to trade off latency and semantic fidelity on demand. These findings establish a scalable and efficient approach for deploying transformer-based semantic communication in future edge intelligence systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¯­ä¹‰é€šä¿¡(Semantic Communication)ä¸­Transformeræ¨¡å‹åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šè®¡ç®—è´Ÿè½½è¿‡å¤§çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„è‡ªé€‚åº”Token Mergingæ¡†æ¶ã€‚è¯¥æ¡†æ¶æ—¨åœ¨å‡å°‘é¢„è®­ç»ƒVision Transformers (ViTs)çš„æ¨ç†æ—¶é—´ä¸ä¼ è¾“èµ„æºæ¶ˆè€—ï¼Œé€šè¿‡å°†å„å±‚åˆå¹¶æ¯”ä¾‹å»ºæ¨¡ä¸ºå¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜ï¼Œå®ç°äº†å‡†ç¡®ç‡ä¸è®¡ç®—æˆæœ¬çš„å¹³è¡¡ã€‚åˆ©ç”¨åŸºäºGaussian Processçš„Bayesian OptimizationæŠ€æœ¯ï¼Œç ”ç©¶è€…æ„å»ºäº†æœ€ä¼˜é…ç½®çš„Pareto Frontierï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿæ ¹æ®åŠ¨æ€ä¿¡é“æ¡ä»¶(Channel Conditions)è¿›è¡Œå®æ—¶è°ƒèŠ‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒSignal-to-Noise Ratio (SNR)ç¯å¢ƒä¸‹å‡èƒ½æ˜¾è‘—é™ä½Floating-Point Operations (FLOPs)ï¼ŒåŒæ—¶ä¿æŒæå…·ç«äº‰åŠ›çš„æ¨ç†å‡†ç¡®ç‡ã€‚è¯¥æˆæœä¸ºæœªæ¥6Gè¾¹ç¼˜æ™ºèƒ½ç³»ç»Ÿä¸­é«˜æ•ˆéƒ¨ç½²å¤§è§„æ¨¡Transformeræ¨¡å‹æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "eess.IV"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted for presentation in IEEE Globecom 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.09168v2",
      "published_date": "2025-09-11 06:05:35 UTC",
      "updated_date": "2025-11-14 14:05:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T18:00:30.985599+00:00"
    },
    {
      "arxiv_id": "2509.09160v1",
      "title": "Target-oriented Multimodal Sentiment Classification with Counterfactual-enhanced Debiasing",
      "title_zh": "åŸºäºåäº‹å®å¢å¼ºå»åçš„é¢å‘ç›®æ ‡å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†ç±»",
      "authors": [
        "Zhiyue Liu",
        "Fanrong Ma",
        "Xin Ling"
      ],
      "abstract": "Target-oriented multimodal sentiment classification seeks to predict sentiment polarity for specific targets from image-text pairs. While existing works achieve competitive performance, they often over-rely on textual content and fail to consider dataset biases, in particular word-level contextual biases. This leads to spurious correlations between text features and output labels, impairing classification accuracy. In this paper, we introduce a novel counterfactual-enhanced debiasing framework to reduce such spurious correlations. Our framework incorporates a counterfactual data augmentation strategy that minimally alters sentiment-related causal features, generating detail-matched image-text samples to guide the model's attention toward content tied to sentiment. Furthermore, for learning robust features from counterfactual data and prompting model decisions, we introduce an adaptive debiasing contrastive learning mechanism, which effectively mitigates the influence of biased words. Experimental results on several benchmark datasets show that our proposed method outperforms state-of-the-art baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é¢å‘ç›®æ ‡çš„è·¨æ¨¡æ€æƒ…æ„Ÿåˆ†ç±»(Target-oriented Multimodal Sentiment Classification)ä¸­æ¨¡å‹è¿‡åº¦ä¾èµ–æ–‡æœ¬å¹¶å—é™äºå•è¯çº§ä¸Šä¸‹æ–‡åç½®çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åˆ›æ–°çš„åäº‹å®å¢å¼ºå»å(Counterfactual-enhanced Debiasing)æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡åäº‹å®æ•°æ®å¢å¼º(Counterfactual Data Augmentation)ç­–ç•¥ç”Ÿæˆä»…æ”¹å˜æƒ…æ„Ÿå› æœç‰¹å¾çš„å›¾æ–‡æ ·æœ¬ï¼Œæœ‰æ•ˆå¼•å¯¼æ¨¡å‹å…³æ³¨ä¸æƒ…æ„Ÿè¡¨è¾¾çœŸæ­£ç›¸å…³çš„æ ¸å¿ƒå†…å®¹ã€‚ä¸ºäº†ä»è¿™äº›å¢å¼ºæ•°æ®ä¸­å­¦ä¹ æ›´ç¨³å¥çš„ç‰¹å¾ï¼Œç ”ç©¶è¿›ä¸€æ­¥å¼•å…¥äº†è‡ªé€‚åº”å»åå¯¹æ¯”å­¦ä¹ (Adaptive Debiasing Contrastive Learning)æœºåˆ¶ï¼Œä»è€Œé™ä½åç½®å•è¯å¯¹é¢„æµ‹ç»“æœçš„å¹²æ‰°ã€‚åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›åŸºçº¿æ¨¡å‹ï¼Œè¯æ˜äº†å…¶åœ¨å‡å°‘ä¼ªç›¸å…³æ€§å’Œæå‡åˆ†ç±»å‡†ç¡®æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by the IEEE International Conference on Multimedia and Expo (ICME 2025). Â© 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses",
      "pdf_url": "https://arxiv.org/pdf/2509.09160v1",
      "published_date": "2025-09-11 05:40:53 UTC",
      "updated_date": "2025-09-11 05:40:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T18:00:51.394893+00:00"
    },
    {
      "arxiv_id": "2509.09159v1",
      "title": "A Knowledge Noise Mitigation Framework for Knowledge-based Visual Question Answering",
      "title_zh": "é¢å‘åŸºäºçŸ¥è¯†çš„è§†è§‰é—®ç­”çš„çŸ¥è¯†å™ªå£°ç¼“è§£æ¡†æ¶",
      "authors": [
        "Zhiyue Liu",
        "Sihang Liu",
        "Jinyuan Liu",
        "Xinru Zhang"
      ],
      "abstract": "Knowledge-based visual question answering (KB-VQA) requires a model to understand images and utilize external knowledge to provide accurate answers. Existing approaches often directly augment models with retrieved information from knowledge sources while ignoring substantial knowledge redundancy, which introduces noise into the answering process. To address this, we propose a training-free framework with knowledge focusing for KB-VQA, that mitigates the impact of noise by enhancing knowledge relevance and reducing redundancy. First, for knowledge retrieval, our framework concludes essential parts from the image-question pairs, creating low-noise queries that enhance the retrieval of highly relevant knowledge. Considering that redundancy still persists in the retrieved knowledge, we then prompt large models to identify and extract answer-beneficial segments from knowledge. In addition, we introduce a selective knowledge integration strategy, allowing the model to incorporate knowledge only when it lacks confidence in answering the question, thereby mitigating the influence of redundant information. Our framework enables the acquisition of accurate and critical knowledge, and extensive experiments demonstrate that it outperforms state-of-the-art methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹çŸ¥è¯†å›¾è°±è§†è§‰é—®ç­”(KB-VQA)ä¸­å­˜åœ¨çš„çŸ¥è¯†å†—ä½™å’Œå™ªå£°å¹²æ‰°é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„çŸ¥è¯†å™ªå£°ç¼“è§£æ¡†æ¶ã€‚è¯¥æ¡†æ¶é¦–å…ˆé€šè¿‡æå–å›¾åƒ-é—®é¢˜å¯¹çš„å…³é”®ä¿¡æ¯æ¥æ„å»ºä½å™ªå£°æŸ¥è¯¢ï¼Œæ—¨åœ¨å¢å¼ºæ£€ç´¢åˆ°çš„å¤–éƒ¨çŸ¥è¯†çš„ç›¸å…³æ€§å¹¶é™ä½åˆå§‹å™ªå£°ã€‚é’ˆå¯¹æ£€ç´¢ç»“æœä¸­ä¾ç„¶å­˜åœ¨çš„å†—ä½™ä¿¡æ¯ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(large models)è¯†åˆ«å¹¶æå–å¯¹å›ç­”é—®é¢˜æœ‰ç›Šçš„çŸ¥è¯†ç‰‡æ®µã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†é€‰æ‹©æ€§çŸ¥è¯†é›†æˆç­–ç•¥(selective knowledge integration strategy)ï¼Œå…è®¸æ¨¡å‹ä»…åœ¨å¯¹ç­”æ¡ˆç¼ºä¹ä¿¡å¿ƒæ—¶æ‰æ•´åˆå¤–éƒ¨çŸ¥è¯†ï¼Œä»è€Œè¿›ä¸€æ­¥è§„é¿å†—ä½™ä¿¡æ¯çš„å¹²æ‰°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆè·å–ç²¾ç¡®ä¸”å…³é”®çš„çŸ¥è¯†ï¼Œå…¶æ€§èƒ½è¡¨ç°ä¼˜äºç›®å‰çš„SOTA(state-of-the-art)æ–¹æ³•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by the IEEE International Conference on Multimedia and Expo (ICME 2025) for oral presentation. Â© 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses",
      "pdf_url": "https://arxiv.org/pdf/2509.09159v1",
      "published_date": "2025-09-11 05:40:26 UTC",
      "updated_date": "2025-09-11 05:40:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T18:00:54.083505+00:00"
    },
    {
      "arxiv_id": "2509.09155v1",
      "title": "HISPASpoof: A New Dataset For Spanish Speech Forensics",
      "title_zh": "HISPASpoofï¼šé¢å‘è¥¿ç­ç‰™è¯­è¯­éŸ³å–è¯çš„æ–°å‹æ•°æ®é›†",
      "authors": [
        "Maria Risques",
        "Kratika Bhagtani",
        "Amit Kumar Singh Yadav",
        "Edward J. Delp"
      ],
      "abstract": "Zero-shot Voice Cloning (VC) and Text-to-Speech (TTS) methods have advanced rapidly, enabling the generation of highly realistic synthetic speech and raising serious concerns about their misuse. While numerous detectors have been developed for English and Chinese, Spanish-spoken by over 600 million people worldwide-remains underrepresented in speech forensics. To address this gap, we introduce HISPASpoof, the first large-scale Spanish dataset designed for synthetic speech detection and attribution. It includes real speech from public corpora across six accents and synthetic speech generated with six zero-shot TTS systems. We evaluate five representative methods, showing that detectors trained on English fail to generalize to Spanish, while training on HISPASpoof substantially improves detection. We also evaluate synthetic speech attribution performance on HISPASpoof, i.e., identifying the generation method of synthetic speech. HISPASpoof thus provides a critical benchmark for advancing reliable and inclusive speech forensics in Spanish.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¥¿ç­ç‰™è¯­åœ¨è¯­éŸ³å–è¯é¢†åŸŸä»£è¡¨æ€§ä¸è¶³çš„é—®é¢˜ï¼Œæ¨å‡ºäº†ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡è¥¿ç­ç‰™è¯­åˆæˆè¯­éŸ³æ£€æµ‹ä¸å½’å› æ•°æ®é›† HISPASpoofã€‚è¯¥æ•°æ®é›†æ¶µç›–äº†æ¥è‡ªå…­ç§ä¸åŒå£éŸ³çš„çœŸå®è¯­éŸ³ï¼Œä»¥åŠåˆ©ç”¨å…­ç§ Zero-shot TTS ç³»ç»Ÿç”Ÿæˆçš„åˆæˆè¯­éŸ³ã€‚ç ”ç©¶äººå‘˜é€šè¿‡è¯„ä¼°äº”ç§ä»£è¡¨æ€§æ£€æµ‹æ–¹æ³•ï¼Œå‘ç°ä»…åœ¨è‹±è¯­æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ£€æµ‹å™¨éš¾ä»¥æ³›åŒ–è‡³è¥¿ç­ç‰™è¯­åœºæ™¯ã€‚å®éªŒè¯æ˜ï¼Œåœ¨ HISPASpoof ä¸Šè¿›è¡Œé’ˆå¯¹æ€§è®­ç»ƒèƒ½æ˜¾è‘—æå‡æ£€æµ‹æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¯¹åˆæˆè¯­éŸ³çš„ Attribution æ€§èƒ½è¿›è¡Œäº†è¯„ä¼°ï¼Œæ—¨åœ¨å‡†ç¡®è¯†åˆ«åˆæˆè¯­éŸ³çš„å…·ä½“ç”Ÿæˆæ¥æºã€‚HISPASpoof çš„å‘å¸ƒä¸ºæ¨åŠ¨æ›´å…·åŒ…å®¹æ€§ã€æ›´å¯é çš„è¥¿ç­ç‰™è¯­è¯­éŸ³å–è¯ç ”ç©¶æä¾›äº†é‡è¦çš„ Benchmarkã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "8 pages, 1 figure, 10 tables, being submitted to ICASSP 2026 (IEEE International Conference on Acoustics, Speech, and Signal Processing 2026)",
      "pdf_url": "https://arxiv.org/pdf/2509.09155v1",
      "published_date": "2025-09-11 05:29:07 UTC",
      "updated_date": "2025-09-11 05:29:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T18:01:02.987801+00:00"
    },
    {
      "arxiv_id": "2509.09154v1",
      "title": "Mind Meets Space: Rethinking Agentic Spatial Intelligence from a Neuroscience-inspired Perspective",
      "title_zh": "å¿ƒæ™ºé‡è§ç©ºé—´ï¼šå—ç¥ç»ç§‘å­¦å¯å‘çš„æ™ºèƒ½ä½“ç©ºé—´æ™ºèƒ½å†æ€è€ƒ",
      "authors": [
        "Bui Duc Manh",
        "Soumyaratna Debnath",
        "Zetong Zhang",
        "Shriram Damodaran",
        "Arvind Kumar",
        "Yueyi Zhang",
        "Lu Mi",
        "Erik Cambria",
        "Lin Wang"
      ],
      "abstract": "Recent advances in agentic AI have led to systems capable of autonomous task execution and language-based reasoning, yet their spatial reasoning abilities remain limited and underexplored, largely constrained to symbolic and sequential processing. In contrast, human spatial intelligence, rooted in integrated multisensory perception, spatial memory, and cognitive maps, enables flexible, context-aware decision-making in unstructured environments. Therefore, bridging this gap is critical for advancing Agentic Spatial Intelligence toward better interaction with the physical 3D world. To this end, we first start from scrutinizing the spatial neural models as studied in computational neuroscience, and accordingly introduce a novel computational framework grounded in neuroscience principles. This framework maps core biological functions to six essential computation modules: bio-inspired multimodal sensing, multi-sensory integration, egocentric-allocentric conversion, an artificial cognitive map, spatial memory, and spatial reasoning. Together, these modules form a perspective landscape for agentic spatial reasoning capability across both virtual and physical environments. On top, we conduct a framework-guided analysis of recent methods, evaluating their relevance to each module and identifying critical gaps that hinder the development of more neuroscience-grounded spatial reasoning modules. We further examine emerging benchmarks and datasets and explore potential application domains ranging from virtual to embodied systems, such as robotics. Finally, we outline potential research directions, emphasizing the promising roadmap that can generalize spatial reasoning across dynamic or unstructured environments. We hope this work will benefit the research community with a neuroscience-grounded perspective and a structured pathway. Our project page can be found at Github.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å½“å‰æ™ºèƒ½ä½“äººå·¥æ™ºèƒ½(Agentic AI)åœ¨ç©ºé—´æ¨ç†èƒ½åŠ›ä¸Šçš„å±€é™æ€§ï¼Œä»è®¡ç®—ç¥ç»ç§‘å­¦è§†è§’æå‡ºäº†ä¸€ç§å—ç¥ç»ç§‘å­¦å¯å‘çš„åˆ›æ–°è®¡ç®—æ¡†æ¶ï¼Œæ—¨åœ¨æå‡æ™ºèƒ½ä½“åœ¨ç‰©ç†3Dä¸–ç•Œä¸­çš„äº¤äº’ä¸å†³ç­–èƒ½åŠ›ã€‚è¯¥æ¡†æ¶å°†ç”Ÿç‰©åŠŸèƒ½æ˜ å°„ä¸ºå…­ä¸ªæ ¸å¿ƒè®¡ç®—æ¨¡å—ï¼Œæ¶µç›–äº†ä»¿ç”Ÿå¤šæ¨¡æ€æ„ŸçŸ¥(bio-inspired multimodal sensing)ã€å¤šæ„Ÿå®˜é›†æˆ(multi-sensory integration)ã€è‡ªæˆ‘ä¸­å¿ƒ-ä¸–ç•Œä¸­å¿ƒè½¬æ¢(egocentric-allocentric conversion)ã€äººå·¥è®¤çŸ¥å›¾è°±(artificial cognitive map)ã€ç©ºé—´è®°å¿†(spatial memory)ä»¥åŠç©ºé—´æ¨ç†(spatial reasoning)ã€‚ä½œè€…é€šè¿‡è¯¥æ¡†æ¶å¯¹ç°æœ‰ç ”ç©¶è¿›è¡Œäº†ç³»ç»Ÿæ€§åˆ†æï¼Œè¯†åˆ«äº†å½“å‰æŠ€æœ¯ä¸ç¥ç»ç§‘å­¦åŸç†ä¹‹é—´çš„å…³é”®å·®è·ï¼Œå¹¶è¯„ä¼°äº†ç›¸å…³åŸºå‡†æµ‹è¯•åœ¨å…·èº«æ™ºèƒ½(embodied systems)ç­‰é¢†åŸŸçš„åº”ç”¨æ½œåŠ›ã€‚è¯¥å·¥ä½œä¸ºå®ç°åœ¨åŠ¨æ€å’Œéç»“æ„åŒ–ç¯å¢ƒä¸­çš„é€šç”¨ç©ºé—´æ¨ç†èƒ½åŠ›æä¾›äº†æ¸…æ™°çš„ç ”ç©¶è·¯å¾„ï¼Œä¸ºæœªæ¥å¼€å‘å…·å¤‡é«˜åº¦ç©ºé—´æ™ºèƒ½çš„è‡ªä¸»ç³»ç»Ÿå¥ å®šäº†ç†è®ºä¸æ–¹æ³•è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "54 pages, journal",
      "pdf_url": "https://arxiv.org/pdf/2509.09154v1",
      "published_date": "2025-09-11 05:23:22 UTC",
      "updated_date": "2025-09-11 05:23:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T18:01:19.492578+00:00"
    },
    {
      "arxiv_id": "2509.09153v1",
      "title": "OCELOT 2023: Cell Detection from Cell-Tissue Interaction Challenge",
      "title_zh": "OCELOT 2023ï¼šåŸºäºç»†èƒ-ç»„ç»‡ç›¸äº’ä½œç”¨çš„ç»†èƒæ£€æµ‹æŒ‘æˆ˜èµ›",
      "authors": [
        "JaeWoong Shin",
        "Jeongun Ryu",
        "Aaron Valero Puche",
        "Jinhee Lee",
        "Biagio Brattoli",
        "Wonkyung Jung",
        "Soo Ick Cho",
        "Kyunghyun Paeng",
        "Chan-Young Ock",
        "Donggeun Yoo",
        "Zhaoyang Li",
        "Wangkai Li",
        "Huayu Mai",
        "Joshua Millward",
        "Zhen He",
        "Aiden Nibali",
        "Lydia Anette Schoenpflug",
        "Viktor Hendrik Koelzer",
        "Xu Shuoyu",
        "Ji Zheng",
        "Hu Bin",
        "Yu-Wen Lo",
        "Ching-Hui Yang",
        "SÃ©rgio Pereira"
      ],
      "abstract": "Pathologists routinely alternate between different magnifications when examining Whole-Slide Images, allowing them to evaluate both broad tissue morphology and intricate cellular details to form comprehensive diagnoses. However, existing deep learning-based cell detection models struggle to replicate these behaviors and learn the interdependent semantics between structures at different magnifications. A key barrier in the field is the lack of datasets with multi-scale overlapping cell and tissue annotations. The OCELOT 2023 challenge was initiated to gather insights from the community to validate the hypothesis that understanding cell and tissue (cell-tissue) interactions is crucial for achieving human-level performance, and to accelerate the research in this field. The challenge dataset includes overlapping cell detection and tissue segmentation annotations from six organs, comprising 673 pairs sourced from 306 The Cancer Genome Atlas (TCGA) Whole-Slide Images with hematoxylin and eosin staining, divided into training, validation, and test subsets. Participants presented models that significantly enhanced the understanding of cell-tissue relationships. Top entries achieved up to a 7.99 increase in F1-score on the test set compared to the baseline cell-only model that did not incorporate cell-tissue relationships. This is a substantial improvement in performance over traditional cell-only detection methods, demonstrating the need for incorporating multi-scale semantics into the models. This paper provides a comparative analysis of the methods used by participants, highlighting innovative strategies implemented in the OCELOT 2023 challenge.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† OCELOT 2023 æŒ‘æˆ˜èµ›ï¼Œæ—¨åœ¨è§£å†³å½“å‰æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨ç—…ç†å›¾åƒåˆ†æä¸­éš¾ä»¥æ¨¡æ‹Ÿç—…ç†å­¦å®¶é€šè¿‡å¤šå€ç‡è§‚æµ‹æ¥ç†è§£ç»†èƒä¸ç»„ç»‡ï¼ˆcell-tissueï¼‰ç›¸äº’ä½œç”¨çš„é—®é¢˜ã€‚ä¸ºäº†å¡«è¡¥å¤šå°ºåº¦é‡å æ ‡æ³¨æ•°æ®é›†çš„ç©ºç™½ï¼Œè¯¥æŒ‘æˆ˜èµ›æä¾›äº†æ¶µç›– 6 ç§å™¨å®˜ã€æºè‡ª 306 å¼  TCGA å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWhole-Slide Imagesï¼‰çš„ 673 å¯¹å›¾åƒæ•°æ®ï¼ŒåŒ…å«åŒæ­¥çš„ç»†èƒæ£€æµ‹ä¸ç»„ç»‡åˆ†å‰²æ ‡æ³¨ã€‚å‚èµ›æ¨¡å‹é€šè¿‡æ•´åˆå¤šå°ºåº¦ï¼ˆmulti-scaleï¼‰è¯­ä¹‰ä¿¡æ¯ï¼Œæ˜¾è‘—å¢å¼ºäº†å¯¹ cell-tissue å…³ç³»çš„ç†è§£ä¸è¡¨å¾èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ’åé¢†å…ˆçš„æ–¹æ³•ç›¸æ¯”ä»…ä¾èµ–ç»†èƒä¿¡æ¯çš„åŸºçº¿æ¨¡å‹ï¼Œå…¶ F1-score æå‡äº†é«˜è¾¾ 7.99ï¼Œå……åˆ†éªŒè¯äº†å¼•å…¥ç»„ç»‡èƒŒæ™¯ä¿¡æ¯åœ¨æå‡ç»†èƒæ£€æµ‹æ€§èƒ½æ–¹é¢çš„å¿…è¦æ€§ã€‚è¯¥è®ºæ–‡å¯¹å„ç§åˆ›æ–°ç­–ç•¥è¿›è¡Œäº†ç³»ç»Ÿæ€§æ¯”è¾ƒåˆ†æï¼Œä¸ºç—…ç†æ™ºèƒ½åˆ†æé¢†åŸŸå®ç°äººç±»æ°´å¹³çš„è¡¨ç°æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This is the accepted manuscript of an article published in Medical Image Analysis (Elsevier). The final version is available at: https://doi.org/10.1016/j.media.2025.103751",
      "pdf_url": "https://arxiv.org/pdf/2509.09153v1",
      "published_date": "2025-09-11 05:21:02 UTC",
      "updated_date": "2025-09-11 05:21:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T18:01:29.889247+00:00"
    },
    {
      "arxiv_id": "2509.14253v1",
      "title": "CrossPT: Exploring Cross-Task Transferability through Multi-Task Prompt Tuning",
      "title_zh": "CrossPTï¼šåˆ©ç”¨å¤šä»»åŠ¡æç¤ºå¾®è°ƒæ¢ç´¢è·¨ä»»åŠ¡å¯è¿ç§»æ€§",
      "authors": [
        "Ahmad Pouramini",
        "Hesham Faili"
      ],
      "abstract": "Prompt tuning offers a parameter-efficient way to adapt large pre-trained language models to new tasks, but most existing approaches are designed for single-task settings, failing to share knowledge across related tasks. We propose Cross-task Prompt Tuning (CrossPT), a modular framework for multi-task prompt tuning that enables controlled knowledge transfer while maintaining task-specific specialization. CrossPT decomposes each target prompt into shared, pre-trained source prompts and task-specific private prompts, combined via a learned attention mechanism. To support robust transfer, we systematically investigate key design factors including prompt initialization, balancing shared and private prompts, number of source prompts, learning rates, task prefixes, and label semantics. Empirical results on GLUE and related benchmarks show that CrossPT achieves higher accuracy and robustness compared to traditional prompt tuning and related methods, particularly in low-resource scenarios, while maintaining strong parameter efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰ Prompt tuning (PT) æ–¹æ³•å¤§å¤šå±€é™äºå•ä»»åŠ¡è®¾ç½®ä¸”éš¾ä»¥å®ç°è·¨ä»»åŠ¡çŸ¥è¯†å…±äº«çš„é—®é¢˜ï¼Œæå‡ºäº† Cross-task Prompt Tuning (CrossPT)ã€‚CrossPT æ˜¯ä¸€ç§ç”¨äºå¤šä»»åŠ¡ Prompt tuning çš„æ¨¡å—åŒ–æ¡†æ¶ï¼Œé€šè¿‡å°†ç›®æ ‡ Prompt åˆ†è§£ä¸ºå…±äº«çš„é¢„è®­ç»ƒæº Prompt å’Œä»»åŠ¡ç‰¹å®šçš„ç§æœ‰ Promptï¼Œå¹¶åˆ©ç”¨å­¦ä¹ åˆ°çš„æ³¨æ„åŠ›æœºåˆ¶ (Attention Mechanism) è¿›è¡Œç»„åˆï¼Œä»è€Œåœ¨ä¿æŒä»»åŠ¡ä¸“ä¸šæ€§çš„åŒæ—¶å®ç°å—æ§çš„çŸ¥è¯†è½¬ç§»ã€‚ä¸ºäº†å¢å¼ºè¿ç§»çš„é²æ£’æ€§ï¼Œç ”ç©¶ç³»ç»Ÿåœ°æ¢è®¨äº† Prompt åˆå§‹åŒ–ã€å…±äº«ä¸ç§æœ‰ Prompt çš„å¹³è¡¡ã€æº Prompt æ•°é‡ã€å­¦ä¹ ç‡ã€ä»»åŠ¡å‰ç¼€ (Task Prefixes) ä»¥åŠæ ‡ç­¾è¯­ä¹‰ (Label Semantics) ç­‰å…³é”®è®¾è®¡å› ç´ ã€‚åœ¨ GLUE ç­‰åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCrossPT åœ¨ä¿æŒé«˜å‚æ•°æ•ˆç‡çš„å‰æä¸‹ï¼Œç›¸æ¯”ä¼ ç»Ÿ Prompt tuning åŠå…¶ç›¸å…³æ–¹æ³•å…·æœ‰æ›´é«˜çš„å‡†ç¡®ç‡å’Œé²æ£’æ€§ã€‚ç‰¹åˆ«æ˜¯åœ¨ä½èµ„æºåœºæ™¯ (Low-resource scenarios) ä¸‹ï¼Œè¯¥æ–¹æ³•å±•ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½ä¼˜åŠ¿ï¼Œè¯æ˜äº†å…¶åœ¨å¤šä»»åŠ¡é¢†åŸŸå®ç°çŸ¥è¯†è¿ç§»çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14253v1",
      "published_date": "2025-09-11 05:13:28 UTC",
      "updated_date": "2025-09-11 05:13:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T18:01:15.391986+00:00"
    },
    {
      "arxiv_id": "2509.09151v1",
      "title": "Video Understanding by Design: How Datasets Shape Architectures and Insights",
      "title_zh": "è§†é¢‘ç†è§£çš„è®¾è®¡é€»è¾‘ï¼šæ•°æ®é›†å¦‚ä½•å¡‘é€ æ¶æ„ä¸æ´è§",
      "authors": [
        "Lei Wang",
        "Piotr Koniusz",
        "Yongsheng Gao"
      ],
      "abstract": "Video understanding has advanced rapidly, fueled by increasingly complex datasets and powerful architectures. Yet existing surveys largely classify models by task or family, overlooking the structural pressures through which datasets guide architectural evolution. This survey is the first to adopt a dataset-driven perspective, showing how motion complexity, temporal span, hierarchical composition, and multimodal richness impose inductive biases that models should encode. We reinterpret milestones, from two-stream and 3D CNNs to sequential, transformer, and multimodal foundation models, as concrete responses to these dataset-driven pressures. Building on this synthesis, we offer practical guidance for aligning model design with dataset invariances while balancing scalability and task demands. By unifying datasets, inductive biases, and architectures into a coherent framework, this survey provides both a comprehensive retrospective and a prescriptive roadmap for advancing general-purpose video understanding.",
      "tldr_zh": "è¯¥ç»¼è¿°é¦–æ¬¡é‡‡ç”¨æ•°æ®é›†é©±åŠ¨çš„è§†è§’ï¼Œæ¢è®¨äº†è§†é¢‘ç†è§£ï¼ˆVideo understandingï¼‰é¢†åŸŸä¸­æ•°æ®é›†å¦‚ä½•å¡‘é€ æ¨¡å‹æ¶æ„ä¸è§è§£ã€‚ç ”ç©¶æ·±å…¥åˆ†æäº† Motion complexityã€Temporal spanã€Hierarchical composition å’Œ Multimodal richness ç­‰æ•°æ®é›†ç‰¹æ€§å¦‚ä½•é€šè¿‡æ–½åŠ  Inductive biases æ¥å¼•å¯¼æ¶æ„çš„æ¼”è¿›ã€‚æ–‡ç« å°† Two-streamã€3D CNNsã€Transformer ä»¥åŠå¤šæ¨¡æ€ Foundation models ç­‰æŠ€æœ¯é‡Œç¨‹ç¢‘é‡æ–°è§£è¯»ä¸ºå¯¹è¿™äº›æ•°æ®é›†é©±åŠ¨å‹åŠ›çš„å…·ä½“å“åº”ã€‚åŸºäºè¿™ä¸€ç»¼åˆåˆ†æï¼Œç ”ç©¶ä¸ºå¦‚ä½•å¹³è¡¡æ¨¡å‹æ‰©å±•æ€§ä¸ä»»åŠ¡éœ€æ±‚ã€å¹¶ä½¿æ¨¡å‹è®¾è®¡ä¸ Dataset invariances å¯¹é½æä¾›äº†å®è·µæŒ‡å¯¼ã€‚è¯¥ç»¼è¿°é€šè¿‡å°†æ•°æ®é›†ã€å½’çº³åç½®ä¸æ¶æ„ç»Ÿä¸€åœ¨ä¸€ä¸ªè¿è´¯çš„æ¡†æ¶å†…ï¼Œä¸ºæ¨è¿›é€šç”¨è§†é¢‘ç†è§£æŠ€æœ¯æä¾›äº†å…¨é¢çš„å›é¡¾ä¸å‰ç»æ€§è·¯çº¿å›¾ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Research report",
      "pdf_url": "https://arxiv.org/pdf/2509.09151v1",
      "published_date": "2025-09-11 05:06:30 UTC",
      "updated_date": "2025-09-11 05:06:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T18:01:23.488655+00:00"
    },
    {
      "arxiv_id": "2509.09143v1",
      "title": "Objectness Similarity: Capturing Object-Level Fidelity in 3D Scene Evaluation",
      "title_zh": "ç‰©ä½“æ€§ç›¸ä¼¼åº¦ï¼šæ•æ‰ä¸‰ç»´åœºæ™¯è¯„ä¼°ä¸­çš„ç‰©ä½“çº§ä¿çœŸåº¦",
      "authors": [
        "Yuiko Uchida",
        "Ren Togo",
        "Keisuke Maeda",
        "Takahiro Ogawa",
        "Miki Haseyama"
      ],
      "abstract": "This paper presents Objectness SIMilarity (OSIM), a novel evaluation metric for 3D scenes that explicitly focuses on \"objects,\" which are fundamental units of human visual perception. Existing metrics assess overall image quality, leading to discrepancies with human perception. Inspired by neuropsychological insights, we hypothesize that human recognition of 3D scenes fundamentally involves attention to individual objects. OSIM enables object-centric evaluations by leveraging an object detection model and its feature representations to quantify the \"objectness\" of each object in the scene. Our user study demonstrates that OSIM aligns more closely with human perception compared to existing metrics. We also analyze the characteristics of OSIM using various approaches. Moreover, we re-evaluate recent 3D reconstruction and generation models under a standardized experimental setup to clarify advancements in this field. The code is available at https://github.com/Objectness-Similarity/OSIM.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Objectness SIMilarity (OSIM)ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹ 3D åœºæ™¯çš„æ–°å‹è¯„ä¼°æŒ‡æ ‡ï¼Œæ—¨åœ¨é€šè¿‡å…³æ³¨äººç±»è§†è§‰æ„ŸçŸ¥çš„åŸºæœ¬å•å…ƒâ€œç‰©ä½“â€æ¥æ•æ‰åœºæ™¯çš„çœŸå®åº¦ã€‚ç”±äºç°æœ‰æŒ‡æ ‡å¤šä¾§é‡äºæ•´ä½“å›¾åƒè´¨é‡ï¼Œå¾€å¾€ä¸äººç±»æ„ŸçŸ¥å­˜åœ¨åå·®ï¼ŒOSIM å—åˆ°ç¥ç»å¿ƒç†å­¦è§è§£çš„å¯å‘ï¼Œæå‡ºäººç±»å¯¹ 3D åœºæ™¯çš„è¯†åˆ«æœ¬è´¨ä¸Šæ¶‰åŠå¯¹å•ä¸ªç‰©ä½“çš„æ³¨æ„åŠ›ã€‚è¯¥æŒ‡æ ‡åˆ©ç”¨ç‰©ä½“æ£€æµ‹æ¨¡å‹åŠå…¶ç‰¹å¾è¡¨ç¤ºæ¥é‡åŒ–åœºæ™¯ä¸­å„ç‰©ä½“çš„ Objectnessï¼Œä»è€Œå®ç°äº†ä»¥ç‰©ä½“ä¸ºä¸­å¿ƒçš„ç»†ç²’åº¦è¯„ä»·ã€‚ç”¨æˆ·ç ”ç©¶è¯æ˜ï¼ŒOSIM æ¯”ç°æœ‰æŒ‡æ ‡æ›´èƒ½åæ˜ äººç±»çš„çœŸå®æ„ŸçŸ¥ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶åœ¨æ ‡å‡†åŒ–å®éªŒè®¾ç½®ä¸‹é‡æ–°è¯„ä¼°äº†è¿‘å¹´æ¥çš„ 3D Reconstruction å’Œ 3D Generation æ¨¡å‹ï¼Œä¸ºæ˜ç¡®è¯¥é¢†åŸŸçš„æŠ€æœ¯è¿›å±•æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by the ICCV 2025 UniLight Workshop",
      "pdf_url": "https://arxiv.org/pdf/2509.09143v1",
      "published_date": "2025-09-11 04:33:27 UTC",
      "updated_date": "2025-09-11 04:33:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T18:01:29.187228+00:00"
    },
    {
      "arxiv_id": "2510.14985v1",
      "title": "DeepAries: Adaptive Rebalancing Interval Selection for Enhanced Portfolio Selection",
      "title_zh": "DeepAriesï¼šé¢å‘å¢å¼ºå‹æŠ•èµ„ç»„åˆé€‰æ‹©çš„è‡ªé€‚åº”è°ƒä»“å‘¨æœŸé€‰æ‹©",
      "authors": [
        "Jinkyu Kim",
        "Hyunjung Yi",
        "Mogan Gim",
        "Donghee Choi",
        "Jaewoo Kang"
      ],
      "abstract": "We propose DeepAries , a novel deep reinforcement learning framework for dynamic portfolio management that jointly optimizes the timing and allocation of rebalancing decisions. Unlike prior reinforcement learning methods that employ fixed rebalancing intervals regardless of market conditions, DeepAries adaptively selects optimal rebalancing intervals along with portfolio weights to reduce unnecessary transaction costs and maximize risk-adjusted returns. Our framework integrates a Transformer-based state encoder, which effectively captures complex long-term market dependencies, with Proximal Policy Optimization (PPO) to generate simultaneous discrete (rebalancing intervals) and continuous (asset allocations) actions. Extensive experiments on multiple real-world financial markets demonstrate that DeepAries significantly outperforms traditional fixed-frequency and full-rebalancing strategies in terms of risk-adjusted returns, transaction costs, and drawdowns. Additionally, we provide a live demo of DeepAries at https://deep-aries.github.io/, along with the source code and dataset at https://github.com/dmis-lab/DeepAries, illustrating DeepAries' capability to produce interpretable rebalancing and allocation decisions aligned with shifting market regimes. Overall, DeepAries introduces an innovative paradigm for adaptive and practical portfolio management by integrating both timing and allocation into a unified decision-making process.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DeepAriesï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹åŠ¨æ€æŠ•èµ„ç»„åˆç®¡ç†çš„æ–°å‹æ·±åº¦å¼ºåŒ–å­¦ä¹  (Deep Reinforcement Learning) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•ä¸­å›ºå®šå†å¹³è¡¡é—´éš”æ— æ³•é€‚åº”å¸‚åœºæ³¢åŠ¨çš„å±€é™æ€§ã€‚DeepAries èƒ½å¤ŸåŒæ—¶ä¼˜åŒ–å†å¹³è¡¡çš„å†³ç­–æ—¶æœºä¸èµ„äº§é…ç½®æƒé‡ï¼Œé€šè¿‡è‡ªé€‚åº”é€‰æ‹©æœ€ä¼˜çš„å†å¹³è¡¡é—´éš” (Rebalancing Intervals)ï¼Œæ˜¾è‘—å‡å°‘äº†ä¸å¿…è¦çš„äº¤æ˜“æˆæœ¬å¹¶æœ€å¤§åŒ–é£é™©è°ƒæ•´åçš„æ”¶ç›Šã€‚è¯¥æ¡†æ¶é›†æˆäº†ä¸€ä¸ªåŸºäº Transformer çš„çŠ¶æ€ç¼–ç å™¨ä»¥æ•æ‰å¤æ‚çš„é•¿æœŸå¸‚åœºä¾èµ–å…³ç³»ï¼Œå¹¶åˆ©ç”¨è¿‘ç«¯ç­–ç•¥ä¼˜åŒ– (Proximal Policy Optimization, PPO) ç®—æ³•åŒæ­¥ç”Ÿæˆç¦»æ•£çš„é—´éš”é€‰æ‹©å’Œè¿ç»­çš„èµ„äº§é…ç½®åŠ¨ä½œã€‚åœ¨å¤šä¸ªçœŸå®é‡‘èå¸‚åœºçš„å®éªŒè¡¨æ˜ï¼ŒDeepAries åœ¨é£é™©è°ƒæ•´åæ”¶ç›Šã€äº¤æ˜“æˆæœ¬å’Œå›æ’¤æ§åˆ¶æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„å›ºå®šé¢‘ç‡åŠå…¨é¢å†å¹³è¡¡ç­–ç•¥ã€‚æ­¤å¤–ï¼ŒDeepAries å±•ç¤ºäº†å…¶äº§ç”Ÿä¸å¸‚åœºä½“åˆ¶è½¬å˜ç›¸ä¸€è‡´çš„ã€å…·æœ‰å¯è§£é‡Šæ€§çš„å†³ç­–èƒ½åŠ›ï¼Œä¸ºé€‚åº”æ€§å¼ºçš„å®ç”¨æŠ•èµ„ç»„åˆç®¡ç†å¼•å…¥äº†ä¸€ç§å°†æ—¶æœºä¸åˆ†é…æ•´åˆåˆ°ç»Ÿä¸€å†³ç­–è¿‡ç¨‹ä¸­çš„åˆ›æ–°èŒƒå¼ã€‚",
      "categories": [
        "q-fin.PM",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "q-fin.PM",
      "comment": "CIKM 2025 Applied Research Track Accepted",
      "pdf_url": "https://arxiv.org/pdf/2510.14985v1",
      "published_date": "2025-09-11 04:12:42 UTC",
      "updated_date": "2025-09-11 04:12:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T18:01:38.900809+00:00"
    },
    {
      "arxiv_id": "2509.09131v1",
      "title": "ViRanker: A BGE-M3 & Blockwise Parallel Transformer Cross-Encoder for Vietnamese Reranking",
      "title_zh": "ViRankerï¼šåŸºäº BGE-M3 ä¸å—å¼å¹¶è¡Œ Transformer çš„è¶Šå—è¯­é‡æ’åºäº¤å‰ç¼–ç å™¨",
      "authors": [
        "Phuong-Nam Dang",
        "Kieu-Linh Nguyen",
        "Thanh-Hieu Pham"
      ],
      "abstract": "This paper presents ViRanker, a cross-encoder reranking model tailored to the Vietnamese language. Built on the BGE-M3 encoder and enhanced with the Blockwise Parallel Transformer, ViRanker addresses the lack of competitive rerankers for Vietnamese, a low-resource language with complex syntax and diacritics. The model was trained on an 8 GB curated corpus and fine-tuned with hybrid hard-negative sampling to strengthen robustness. Evaluated on the MMARCO-VI benchmark, ViRanker achieves strong early-rank accuracy, surpassing multilingual baselines and competing closely with PhoRanker. By releasing the model openly on Hugging Face, we aim to support reproducibility and encourage wider adoption in real-world retrieval systems. Beyond Vietnamese, this study illustrates how careful architectural adaptation and data curation can advance reranking in other underrepresented languages.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ViRankerï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨é’ˆå¯¹è¶Šå—è¯­è®¾è®¡çš„Cross-Encoderé‡æ’åºæ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³è¯¥ä½èµ„æºè¯­è¨€åœ¨å¤æ‚è¯­æ³•å’Œå˜éŸ³ç¬¦å·ç¯å¢ƒä¸‹ç¼ºä¹ç«äº‰æ€§é‡æ’åºå·¥å…·çš„é—®é¢˜ã€‚è¯¥æ¨¡å‹åŸºäºBGE-M3ç¼–ç å™¨ï¼Œå¹¶å¼•å…¥äº†Blockwise Parallel Transformerè¿›è¡Œå¢å¼ºï¼Œä»¥æå‡å¤„ç†æ€§èƒ½ã€‚ç ”å‘å›¢é˜Ÿåœ¨8 GBç²¾å¿ƒç­–åˆ’çš„è¯­æ–™åº“ä¸Šå¯¹æ¨¡å‹è¿›è¡Œäº†è®­ç»ƒï¼Œå¹¶é‡‡ç”¨æ··åˆHard-negative samplingæŠ€æœ¯è¿›è¡Œå¾®è°ƒï¼Œæ˜¾è‘—å¢å¼ºäº†å…¶é²æ£’æ€§ã€‚åœ¨MMARCO-VIåŸºå‡†æµ‹è¯•ä¸­çš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒViRankeråœ¨æ—©æœŸæ’åå‡†ç¡®åº¦ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä¸ä»…è¶…è¶Šäº†å¤šç§å¤šè¯­è¨€åŸºå‡†æ¨¡å‹ï¼Œä¸”ä¸PhoRankerçš„æ€§èƒ½ç›¸å½“ã€‚è¯¥æ¨¡å‹å·²åœ¨Hugging Faceå¼€æºï¼Œæ—¨åœ¨æ”¯æŒå­¦æœ¯å¤ç°å¹¶æ¨åŠ¨é‡æ’åºæŠ€æœ¯åœ¨å®é™…æ£€ç´¢ç³»ç»Ÿä¸­çš„å¹¿æ³›åº”ç”¨ã€‚æ­¤å¤–ï¼Œè¿™é¡¹ç ”ç©¶è¿˜ä¸ºå…¶ä»–ä½èµ„æºè¯­è¨€é€šè¿‡æ¶æ„ä¼˜åŒ–å’Œæ•°æ®ç­–åæ¥æå‡æ£€ç´¢æ€§èƒ½æä¾›äº†é‡è¦çš„å‚è€ƒè·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages",
      "pdf_url": "https://arxiv.org/pdf/2509.09131v1",
      "published_date": "2025-09-11 04:07:43 UTC",
      "updated_date": "2025-09-11 04:07:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T18:01:42.588396+00:00"
    },
    {
      "arxiv_id": "2509.09127v1",
      "title": "Anti-Money Laundering Machine Learning Pipelines; A Technical Analysis on Identifying High-risk Bank Clients with Supervised Learning",
      "title_zh": "åæ´—é’±æœºå™¨å­¦ä¹ æµæ°´çº¿ï¼šåŸºäºç›‘ç£å­¦ä¹ è¯†åˆ«é«˜é£é™©é“¶è¡Œå®¢æˆ·çš„æŠ€æœ¯åˆ†æ",
      "authors": [
        "Khashayar Namdar",
        "Pin-Chien Wang",
        "Tushar Raju",
        "Steven Zheng",
        "Fiona Li",
        "Safwat Tahmin Khan"
      ],
      "abstract": "Anti-money laundering (AML) actions and measurements are among the priorities of financial institutions, for which machine learning (ML) has shown to have a high potential. In this paper, we propose a comprehensive and systematic approach for developing ML pipelines to identify high-risk bank clients in a dataset curated for Task 1 of the University of Toronto 2023-2024 Institute for Management and Innovation (IMI) Big Data and Artificial Intelligence Competition. The dataset included 195,789 customer IDs, and we employed a 16-step design and statistical analysis to ensure the final pipeline was robust. We also framed the data in a SQLite database, developed SQL-based feature engineering algorithms, connected our pre-trained model to the database, and made it inference-ready, and provided explainable artificial intelligence (XAI) modules to derive feature importance. Our pipeline achieved a mean area under the receiver operating characteristic curve (AUROC) of 0.961 with a standard deviation (SD) of 0.005. The proposed pipeline achieved second place in the competition.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å…¨é¢ä¸”ç³»ç»Ÿçš„æœºå™¨å­¦ä¹ (Machine Learning, ML)æµæ°´çº¿æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡ç›‘ç£å­¦ä¹ (Supervised Learning)ç²¾å‡†è¯†åˆ«é«˜é£é™©é“¶è¡Œå®¢æˆ·ã€‚è¯¥æ¡†æ¶åŸºäºåŒ…å«195,789ä¸ªå®¢æˆ·çš„æ•°æ®é›†ï¼Œé‡‡ç”¨äº†16æ­¥è®¾è®¡ä¸ç»Ÿè®¡åˆ†æä»¥ç¡®ä¿æµç¨‹çš„é²æ£’æ€§ï¼Œå¹¶åˆ©ç”¨SQLiteæ•°æ®åº“å¼€å‘äº†åŸºäºSQLçš„ç‰¹å¾å·¥ç¨‹(Feature Engineering)ç®—æ³•ã€‚é€šè¿‡æ•´åˆé¢„è®­ç»ƒæ¨¡å‹ä¸å¯è§£é‡Šäººå·¥æ™ºèƒ½(Explainable Artificial Intelligence, XAI)æ¨¡å—ï¼Œè¯¥æµæ°´çº¿ä¸ä»…å®ç°äº†é«˜æ•ˆæ¨ç†ï¼Œè¿˜èƒ½æ·±å…¥åˆ†æç‰¹å¾é‡è¦æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ¡ˆå–å¾—äº†0.961çš„å¹³å‡AUROCå’Œ0.005çš„æ ‡å‡†å·®ï¼Œåœ¨å¤šä¼¦å¤šå¤§å­¦IMIå¤§æ•°æ®çš„ç«èµ›ä¸­ä½åˆ—ç¬¬äºŒï¼ŒéªŒè¯äº†å…¶åœ¨åæ´—é’±(Anti-Money Laundering, AML)é¢†åŸŸçš„å®æˆ˜åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.09127v1",
      "published_date": "2025-09-11 03:53:10 UTC",
      "updated_date": "2025-09-11 03:53:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T18:01:57.093425+00:00"
    },
    {
      "arxiv_id": "2509.18126v1",
      "title": "Anomaly Detection in Electric Vehicle Charging Stations Using Federated Learning",
      "title_zh": "åŸºäºè”é‚¦å­¦ä¹ çš„ç”µåŠ¨æ±½è½¦å……ç”µç«™å¼‚å¸¸æ£€æµ‹",
      "authors": [
        "Bishal K C",
        "Amr Hilal",
        "Pawan Thapa"
      ],
      "abstract": "Federated Learning (FL) is a decentralized training framework widely used in IoT ecosystems that preserves privacy by keeping raw data local, making it ideal for IoT-enabled cyber-physical systems with sensing and communication like Smart Grids (SGs), Connected and Automated Vehicles (CAV), and Electric Vehicle Charging Stations (EVCS). With the rapid expansion of electric vehicle infrastructure, securing these IoT-based charging stations against cyber threats has become critical. Centralized Intrusion Detection Systems (IDS) raise privacy concerns due to sensitive network and user data, making FL a promising alternative. However, current FL-based IDS evaluations overlook practical challenges such as system heterogeneity and non-IID data. To address these challenges, we conducted experiments to evaluate the performance of federated learning for anomaly detection in EV charging stations under system and data heterogeneity. We used FedAvg and FedAvgM, widely studied optimization approaches, to analyze their effectiveness in anomaly detection. Under IID settings, FedAvg achieves superior performance to centralized models using the same neural network. However, performance degrades with non-IID data and system heterogeneity. FedAvgM consistently outperforms FedAvg in heterogeneous settings, showing better convergence and higher anomaly detection accuracy. Our results demonstrate that FL can handle heterogeneity in IoT-based EVCS without significant performance loss, with FedAvgM as a promising solution for robust, privacy-preserving EVCS security.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨ç”µåŠ¨æ±½è½¦å……ç”µç«™(EVCS)ä¸­ä½¿ç”¨è”é‚¦å­¦ä¹ (Federated Learning)è¿›è¡Œå¼‚å¸¸æ£€æµ‹ï¼Œä»¥è§£å†³ä¸­å¿ƒåŒ–å…¥ä¾µæ£€æµ‹ç³»ç»Ÿ(IDS)å¸¦æ¥çš„éšç§æ³„éœ²é£é™©ã€‚é’ˆå¯¹ç‰©è”ç½‘(IoT)ç¯å¢ƒä¸‹æ™®éå­˜åœ¨çš„ç³»ç»Ÿå¼‚æ„æ€§(System Heterogeneity)å’Œéç‹¬ç«‹åŒåˆ†å¸ƒ(Non-IID)æ•°æ®æŒ‘æˆ˜ï¼Œä½œè€…å¯¹FedAvgå’ŒFedAvgMä¸¤ç§ä¼˜åŒ–ç®—æ³•åœ¨å……ç”µåŸºç¡€è®¾æ–½å®‰å…¨é˜²æŠ¤ä¸­çš„è¡¨ç°è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶FedAvgåœ¨IIDè®¾ç½®ä¸‹ä¼˜äºä¸­å¿ƒåŒ–æ¨¡å‹ï¼Œä½†åœ¨å¼‚æ„æ€§æ•°æ®ä¸‹å…¶æ€§èƒ½ä¼šå‡ºç°æ˜æ˜¾ä¸‹é™ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒFedAvgMåœ¨å¤„ç†å¼‚æ„ç¯å¢ƒæ—¶è¡¨ç°å‡ºæ›´ä¼˜çš„æ”¶æ•›æ€§å’Œæ›´é«˜çš„å¼‚å¸¸æ£€æµ‹å‡†ç¡®ç‡ã€‚è¯¥å·¥ä½œè¯æ˜äº†è”é‚¦å­¦ä¹ èƒ½åœ¨ä¸æ˜¾è‘—æŸå¤±æ€§èƒ½çš„æƒ…å†µä¸‹å¤„ç†EVCSçš„å¼‚æ„æ€§ï¼Œå¹¶éªŒè¯äº†FedAvgMä½œä¸ºæ„å»ºç¨³å¥ä¸”ä¿æŠ¤éšç§çš„ç”µåŠ¨æ±½è½¦å……ç”µå®‰å…¨ç³»ç»Ÿçš„å¯è¡Œæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18126v1",
      "published_date": "2025-09-11 03:46:41 UTC",
      "updated_date": "2025-09-11 03:46:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T18:02:02.265654+00:00"
    },
    {
      "arxiv_id": "2509.09125v1",
      "title": "Automated Classification of Tutors' Dialogue Acts Using Generative AI: A Case Study Using the CIMA Corpus",
      "title_zh": "åŸºäºç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„å¯¼å¸ˆå¯¹è¯è¡Œä¸ºè‡ªåŠ¨åˆ†ç±»ï¼šä»¥ CIMA è¯­æ–™åº“ä¸ºä¾‹çš„æ¡ˆä¾‹ç ”ç©¶",
      "authors": [
        "Liqun He",
        "Jiaqi Xu"
      ],
      "abstract": "This study explores the use of generative AI for automating the classification of tutors' Dialogue Acts (DAs), aiming to reduce the time and effort required by traditional manual coding. This case study uses the open-source CIMA corpus, in which tutors' responses are pre-annotated into four DA categories. Both GPT-3.5-turbo and GPT-4 models were tested using tailored prompts. Results show that GPT-4 achieved 80% accuracy, a weighted F1-score of 0.81, and a Cohen's Kappa of 0.74, surpassing baseline performance and indicating substantial agreement with human annotations. These findings suggest that generative AI has strong potential to provide an efficient and accessible approach to DA classification, with meaningful implications for educational dialogue analysis. The study also highlights the importance of task-specific label definitions and contextual information in enhancing the quality of automated annotation. Finally, it underscores the ethical considerations associated with the use of generative AI and the need for responsible and transparent research practices. The script of this research is publicly available at https://github.com/liqunhe27/Generative-AI-for-educational-dialogue-act-tagging.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½(Generative AI)è‡ªåŠ¨åˆ†ç±»å¯¼å¸ˆå¯¹è¯è¡Œä¸º(Dialogue Acts, DAs)çš„å¯è¡Œæ€§ï¼Œæ—¨åœ¨å‡å°‘ä¼ ç»Ÿäººå·¥ç¼–ç æ‰€éœ€çš„æ—¶é—´å’Œç²¾åŠ›ã€‚å®éªŒåŸºäºå¼€æºçš„CIMAè¯­æ–™åº“ï¼Œé€šè¿‡å®šåˆ¶åŒ–æç¤ºè¯(tailored prompts)å¯¹GPT-3.5-turboå’ŒGPT-4æ¨¡å‹è¿›è¡Œäº†æµ‹è¯•ã€‚ç»“æœæ˜¾ç¤ºï¼ŒGPT-4è¡¨ç°ä¼˜å¼‚ï¼Œå®ç°äº†80%çš„å‡†ç¡®ç‡ã€0.81çš„åŠ æƒF1-scoreä»¥åŠ0.74çš„Cohen's Kappaï¼Œæ˜¾è‘—è¶…è¶Šäº†åŸºå‡†æ€§èƒ½å¹¶ä¸äººç±»æ ‡æ³¨ä¿æŒé«˜åº¦ä¸€è‡´ã€‚ç ”ç©¶å¼ºè°ƒäº†ä»»åŠ¡ç‰¹å®šæ ‡ç­¾å®šä¹‰(task-specific label definitions)å’Œä¸Šä¸‹æ–‡ä¿¡æ¯åœ¨æå‡è‡ªåŠ¨åŒ–æ ‡æ³¨è´¨é‡ä¸­çš„å…³é”®ä½œç”¨ã€‚è¿™äº›å‘ç°è¯æ˜äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åœ¨æ•™è‚²å¯¹è¯åˆ†æé¢†åŸŸå…·æœ‰æä¾›é«˜æ•ˆã€ä¾¿æ·åˆ†ç±»æ‰‹æ®µçš„å·¨å¤§æ½œåŠ›ï¼ŒåŒæ—¶ä¹Ÿæ¢è®¨äº†ç›¸å…³çš„ä¼¦ç†è€ƒé‡ä¸ç ”ç©¶é€æ˜åº¦é—®é¢˜ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted for publication in the journal Reflecting Digital Learning. First submitted: 30 Oct 2023. The final version will be available open access via the journal",
      "pdf_url": "https://arxiv.org/pdf/2509.09125v1",
      "published_date": "2025-09-11 03:36:03 UTC",
      "updated_date": "2025-09-11 03:36:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T18:02:07.562506+00:00"
    },
    {
      "arxiv_id": "2509.14252v2",
      "title": "LLM-JEPA: Large Language Models Meet Joint Embedding Predictive Architectures",
      "title_zh": "LLM-JEPAï¼šå¤§è¯­è¨€æ¨¡å‹ä¸è”åˆåµŒå…¥é¢„æµ‹æ¶æ„çš„èåˆ",
      "authors": [
        "Hai Huang",
        "Yann LeCun",
        "Randall Balestriero"
      ],
      "abstract": "Large Language Model (LLM) pretraining, finetuning, and evaluation rely on input-space reconstruction and generative capabilities. Yet, it has been observed in vision that embedding-space training objectives, e.g., with Joint Embedding Predictive Architectures (JEPAs), are far superior to their input-space counterpart. That mismatch in how training is achieved between language and vision opens up a natural question: {\\em can language training methods learn a few tricks from the vision ones?} The lack of JEPA-style LLM is a testimony of the challenge in designing such objectives for language. In this work, we propose a first step in that direction where we develop LLM-JEPA, a JEPA based solution for LLMs applicable both to finetuning and pretraining. Thus far, LLM-JEPA is able to outperform the standard LLM training objectives by a significant margin across models, all while being robust to overfiting. Those findings are observed across numerous datasets (NL-RX, GSM8K, Spider, RottenTomatoes) and various models from the Llama3, OpenELM, Gemma2 and Olmo families. Code: https://github.com/rbalestr-lab/llm-jepa.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LLM-JEPAï¼Œæ—¨åœ¨å°†è§†è§‰é¢†åŸŸä¸­è¡¨ç°å“è¶Šçš„è”åˆåµŒå…¥é¢„æµ‹æ¶æ„(Joint Embedding Predictive Architectures, JEPAs)å¼•å…¥å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)ï¼Œä»¥è§£å†³ä¼ ç»Ÿæ¨¡å‹è¿‡åº¦ä¾èµ–è¾“å…¥ç©ºé—´é‡æ„å’Œç”Ÿæˆèƒ½åŠ›çš„é—®é¢˜ã€‚LLM-JEPA æä¾›äº†ä¸€ç§é€‚ç”¨äºé¢„è®­ç»ƒ(pretraining)å’Œå¾®è°ƒ(finetuning)çš„åµŒå…¥ç©ºé—´è®­ç»ƒæ–¹æ¡ˆï¼Œæœ‰æ•ˆåº”å¯¹äº†åœ¨è¯­è¨€é¢†åŸŸè®¾è®¡æ­¤ç±»ç›®æ ‡å‡½æ•°çš„æŒ‘æˆ˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ NL-RXã€GSM8Kã€Spider å’Œ RottenTomatoes ç­‰å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œåœ¨ Llama3ã€OpenELMã€Gemma2 å’Œ Olmo ç­‰ä¸åŒæ¨¡å‹å®¶æ—ä¸­å‡ä»¥æ˜¾è‘—ä¼˜åŠ¿è¶…è¶Šäº†æ ‡å‡†çš„ LLM è®­ç»ƒç›®æ ‡ã€‚æ­¤å¤–ï¼ŒLLM-JEPA åœ¨æ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œè¿˜è¡¨ç°å‡ºäº†æå¼ºçš„æŠ—è¿‡æ‹Ÿåˆ(overfitting)é²æ£’æ€§ã€‚è¯¥ç ”ç©¶ä¸ºå€Ÿé‰´è§†è§‰è®­ç»ƒæ–¹æ³•æ¥ä¼˜åŒ–è¯­è¨€å­¦ä¹ æœºåˆ¶æä¾›äº†é‡è¦çš„ç¬¬ä¸€æ­¥ï¼Œå¹¶ä¸ºæå‡å¤§æ¨¡å‹è®­ç»ƒæ•ˆç‡å¼€è¾Ÿäº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14252v2",
      "published_date": "2025-09-11 03:03:57 UTC",
      "updated_date": "2025-10-07 17:55:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T18:02:17.381024+00:00"
    },
    {
      "arxiv_id": "2509.09112v2",
      "title": "Character-Level Perturbations Disrupt LLM Watermarks",
      "title_zh": "å­—ç¬¦çº§æ‰°åŠ¨ç ´å LLM æ°´å°",
      "authors": [
        "Zhaoxi Zhang",
        "Xiaomei Zhang",
        "Yanjun Zhang",
        "He Zhang",
        "Shirui Pan",
        "Bo Liu",
        "Asif Qumer Gill",
        "Leo Yu Zhang"
      ],
      "abstract": "Large Language Model (LLM) watermarking embeds detectable signals into generated text for copyright protection, misuse prevention, and content detection. While prior studies evaluate robustness using watermark removal attacks, these methods are often suboptimal, creating the misconception that effective removal requires large perturbations or powerful adversaries.\n  To bridge the gap, we first formalize the system model for LLM watermark, and characterize two realistic threat models constrained on limited access to the watermark detector. We then analyze how different types of perturbation vary in their attack range, i.e., the number of tokens they can affect with a single edit. We observe that character-level perturbations (e.g., typos, swaps, deletions, homoglyphs) can influence multiple tokens simultaneously by disrupting the tokenization process. We demonstrate that character-level perturbations are significantly more effective for watermark removal under the most restrictive threat model. We further propose guided removal attacks based on the Genetic Algorithm (GA) that uses a reference detector for optimization. Under a practical threat model with limited black-box queries to the watermark detector, our method demonstrates strong removal performance. Experiments confirm the superiority of character-level perturbations and the effectiveness of the GA in removing watermarks under realistic constraints. Additionally, we argue there is an adversarial dilemma when considering potential defenses: any fixed defense can be bypassed by a suitable perturbation strategy. Motivated by this principle, we propose an adaptive compound character-level attack. Experimental results show that this approach can effectively defeat the defenses. Our findings highlight significant vulnerabilities in existing LLM watermark schemes and underline the urgency for the development of new robust mechanisms.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLM)æ°´å°æŠ€æœ¯çš„å®‰å…¨æ€§ï¼ŒæŒ‘æˆ˜äº†ä»¥å¾€è®¤ä¸ºç§»é™¤æ°´å°éœ€è¦å¤§å¹…æ‰°åŠ¨æˆ–å¼ºå¤§å¯¹æ‰‹çš„è¯¯åŒºã€‚é€šè¿‡å½¢å¼åŒ–æ°´å°ç³»ç»Ÿæ¨¡å‹ï¼Œä½œè€…å‘ç°å­—ç¬¦çº§æ‰°åŠ¨(Character-Level Perturbationsï¼Œå¦‚æ‹¼å†™é”™è¯¯ã€å­—ç¬¦äº¤æ¢å’ŒåŒå½¢å­—)èƒ½é€šè¿‡ç ´ååˆ†è¯(tokenization)è¿‡ç¨‹åŒæ—¶å½±å“å¤šä¸ªTokenï¼Œä»è€Œåœ¨å—é™çš„å¨èƒæ¨¡å‹ä¸‹æ›´æœ‰æ•ˆåœ°ç§»é™¤æ°´å°ã€‚ç ”ç©¶è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§åŸºäºé—ä¼ ç®—æ³•(Genetic Algorithm, GA)çš„å¼•å¯¼å¼ç§»é™¤æ”»å‡»ï¼Œåœ¨ä»…éœ€æœ‰é™é»‘ç›’æŸ¥è¯¢çš„ç°å®åœºæ™¯ä¸‹è¡¨ç°å‡ºæå¼ºçš„æ°´å°ç ´è§£æ€§èƒ½ã€‚å®éªŒç»“æœè¯æ˜äº†å­—ç¬¦çº§æ‰°åŠ¨ç›¸å¯¹äºä¼ ç»Ÿæ–¹æ³•çš„ä¼˜è¶Šæ€§ï¼Œå¹¶æ­ç¤ºäº†ç°æœ‰é˜²å¾¡æœºåˆ¶åœ¨è‡ªé€‚åº”å¤åˆæ”»å‡»é¢å‰çš„è„†å¼±æ€§ã€‚è¯¥å‘ç°å¼ºè°ƒäº†å½“å‰LLMæ°´å°æ–¹æ¡ˆå­˜åœ¨é‡å¤§å®‰å…¨æ¼æ´ï¼ŒäºŸéœ€å¼€å‘æ›´å…·é²æ£’æ€§çš„æ–°ä¸€ä»£æ°´å°æœºåˆ¶ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "accepted by Network and Distributed System Security (NDSS) Symposium 2026",
      "pdf_url": "https://arxiv.org/pdf/2509.09112v2",
      "published_date": "2025-09-11 02:50:07 UTC",
      "updated_date": "2025-09-14 07:46:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T18:02:08.869733+00:00"
    },
    {
      "arxiv_id": "2509.09097v1",
      "title": "DP-FedLoRA: Privacy-Enhanced Federated Fine-Tuning for On-Device Large Language Models",
      "title_zh": "DP-FedLoRAï¼šé¢å‘è®¾å¤‡ç«¯å¤§è¯­è¨€æ¨¡å‹çš„éšç§å¢å¼ºå‹è”é‚¦å¾®è°ƒ",
      "authors": [
        "Honghui Xu",
        "Shiva Shrestha",
        "Wei Chen",
        "Zhiyuan Li",
        "Zhipeng Cai"
      ],
      "abstract": "As on-device large language model (LLM) systems become increasingly prevalent, federated fine-tuning enables advanced language understanding and generation directly on edge devices; however, it also involves processing sensitive, user-specific data, raising significant privacy concerns within the federated learning framework. To address these challenges, we propose DP-FedLoRA, a privacy-enhanced federated fine-tuning framework that integrates LoRA-based adaptation with differential privacy in a communication-efficient setting. Each client locally clips and perturbs its LoRA matrices using Gaussian noise to satisfy ($Îµ$, $Î´$)-differential privacy. We further provide a theoretical analysis demonstrating the unbiased nature of the updates and deriving bounds on the variance introduced by noise, offering practical guidance for privacy-budget calibration. Experimental results across mainstream benchmarks show that DP-FedLoRA delivers competitive performance while offering strong privacy guarantees, paving the way for scalable and privacy-preserving LLM deployment in on-device environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DP-FedLoRAï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹è®¾å¤‡ç«¯å¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„éšç§å¢å¼ºå‹è”é‚¦å¾®è°ƒæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è”é‚¦å­¦ä¹ ä¸­å¤„ç†æ•æ„Ÿç”¨æˆ·æ•°æ®æ—¶çš„éšç§é£é™©ã€‚è¯¥æ–¹æ¡ˆé€šè¿‡åœ¨é€šä¿¡æ•ˆç‡è¾ƒé«˜çš„è®¾ç½®ä¸­ç»“åˆ LoRA é€‚é…æŠ€æœ¯ä¸å·®åˆ†éšç§ (Differential Privacy)ï¼Œè¦æ±‚æ¯ä¸ªå®¢æˆ·ç«¯åœ¨æœ¬åœ°å¯¹ LoRA çŸ©é˜µè¿›è¡Œè£å‰ªå¹¶æ·»åŠ é«˜æ–¯å™ªå£° (Gaussian noise) ä»¥æ»¡è¶³ ($Îµ$, $Î´$)-å·®åˆ†éšç§æ ‡å‡†ã€‚ç ”ç©¶å›¢é˜Ÿè¿›ä¸€æ­¥æä¾›äº†ç†è®ºåˆ†æï¼Œè¯æ˜äº†æ¨¡å‹æ›´æ–°çš„æ— åæ€§å¹¶æ¨å¯¼å‡ºå™ªå£°æ–¹å·®ç•Œé™ï¼Œä¸ºéšç§é¢„ç®— (privacy-budget) çš„æ ¡å‡†æä¾›äº†å®è·µæŒ‡å¯¼ã€‚åœ¨ä¸»æµåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼ŒDP-FedLoRA åœ¨æä¾›å¼ºå¤§éšç§ä¿è¯çš„åŒæ—¶ä¿æŒäº†æå…·ç«äº‰åŠ›çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶ä¸ºåœ¨è®¾å¤‡ç«¯å®ç°å¯æ‰©å±•ä¸”éšç§å®‰å…¨çš„ LLM éƒ¨ç½²å¼€è¾Ÿäº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.09097v1",
      "published_date": "2025-09-11 02:16:34 UTC",
      "updated_date": "2025-09-11 02:16:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T18:02:10.265436+00:00"
    },
    {
      "arxiv_id": "2509.09091v1",
      "title": "Towards Confidential and Efficient LLM Inference with Dual Privacy Protection",
      "title_zh": "è¿ˆå‘åŒé‡éšç§ä¿æŠ¤ä¸‹çš„æœºå¯†é«˜æ•ˆå¤§è¯­è¨€æ¨¡å‹æ¨ç†",
      "authors": [
        "Honglan Yu",
        "Yibin Wang",
        "Feifei Dai",
        "Dong Liu",
        "Haihui Fan",
        "Xiaoyan Gu"
      ],
      "abstract": "CPU-based trusted execution environments (TEEs) and differential privacy (DP) have gained wide applications for private inference. Due to high inference latency in TEEs, researchers use partition-based approaches that offload linear model components to GPUs. However, dense nonlinear layers of large language models (LLMs) result in significant communication overhead between TEEs and GPUs. DP-based approaches apply random noise to protect data privacy, but this compromises LLM performance and semantic understanding. To overcome the above drawbacks, this paper proposes CMIF, a Confidential and efficient Model Inference Framework. CMIF confidentially deploys the embedding layer in the client-side TEE and subsequent layers on GPU servers. Meanwhile, it optimizes the Report-Noisy-Max mechanism to protect sensitive inputs with a slight decrease in model performance. Extensive experiments on Llama-series models demonstrate that CMIF reduces additional inference overhead in TEEs while preserving user data privacy.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäº CPU çš„å¯ä¿¡æ‰§è¡Œç¯å¢ƒ (TEEs) åœ¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) æ¨ç†ä¸­å­˜åœ¨çš„å»¶è¿Ÿé—®é¢˜ï¼Œä»¥åŠå·®å¼‚éšç§ (DP) å¯¹æ¨¡å‹æ€§èƒ½çš„æŸå®³ï¼Œæå‡ºäº† CMIF æœºå¯†é«˜æ•ˆæ¨ç†æ¡†æ¶ã€‚CMIF åˆ›æ–°æ€§åœ°å°† Embedding å±‚éƒ¨ç½²åœ¨å®¢æˆ·ç«¯ TEE ä¸­ï¼Œè€Œå°†å…¶ä½™åç»­å±‚æ”¾ç½®äº GPU æœåŠ¡å™¨ï¼Œä»¥æ­¤æœ‰æ•ˆå‡å°‘ TEE ä¸ GPU é—´å› å¯†æ–‡ä¼ è¾“å¯¼è‡´çš„é€šä¿¡å¼€é”€ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶é€šè¿‡ä¼˜åŒ– Report-Noisy-Max æœºåˆ¶ï¼Œåœ¨ç¡®ä¿æ•æ„Ÿè¾“å…¥å¾—åˆ°ä¿æŠ¤çš„åŒæ—¶ï¼Œæœ€å¤§ç¨‹åº¦åœ°é™ä½äº†å¯¹æ¨¡å‹æ€§èƒ½å’Œè¯­ä¹‰ç†è§£èƒ½åŠ›çš„è´Ÿé¢å½±å“ã€‚åœ¨ Llama ç³»åˆ—æ¨¡å‹ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼ŒCMIF ä¸ä»…èƒ½æœ‰æ•ˆä¿æŠ¤ç”¨æˆ·éšç§ï¼Œè¿˜æ˜¾è‘—é™ä½äº†æ¨ç†è¿‡ç¨‹ä¸­çš„é¢å¤–å¼€é”€ï¼Œå®ç°äº†éšç§ä¿æŠ¤ä¸è®¡ç®—æ•ˆç‡çš„åŒé‡ä¼˜åŒ–ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted by DASFAA2025",
      "pdf_url": "https://arxiv.org/pdf/2509.09091v1",
      "published_date": "2025-09-11 01:54:13 UTC",
      "updated_date": "2025-09-11 01:54:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T18:02:24.068680+00:00"
    },
    {
      "arxiv_id": "2509.09090v1",
      "title": "SQAP-VLA: A Synergistic Quantization-Aware Pruning Framework for High-Performance Vision-Language-Action Models",
      "title_zh": "SQAP-VLAï¼šé¢å‘é«˜æ€§èƒ½è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„ååŒé‡åŒ–æ„ŸçŸ¥å‰ªææ¡†æ¶",
      "authors": [
        "Hengyu Fang",
        "Yijiang Liu",
        "Yuan Du",
        "Li Du",
        "Huanrui Yang"
      ],
      "abstract": "Vision-Language-Action (VLA) models exhibit unprecedented capabilities for embodied intelligence. However, their extensive computational and memory costs hinder their practical deployment. Existing VLA compression and acceleration approaches conduct quantization or token pruning in an ad-hoc manner but fail to enable both for a holistic efficiency improvement due to an observed incompatibility. This work introduces SQAP-VLA, the first structured, training-free VLA inference acceleration framework that simultaneously enables state-of-the-art quantization and token pruning. We overcome the incompatibility by co-designing the quantization and token pruning pipeline, where we propose new quantization-aware token pruning criteria that work on an aggressively quantized model while improving the quantizer design to enhance pruning effectiveness. When applied to standard VLA models, SQAP-VLA yields significant gains in computational efficiency and inference speed while successfully preserving core model performance, achieving a $\\times$1.93 speedup and up to a 4.5\\% average success rate enhancement compared to the original model.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SQAP-VLAï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç»“æ„åŒ–ä¸”æ— éœ€è®­ç»ƒçš„ Vision-Language-Action (VLA) æ¨¡å‹æ¨ç†åŠ é€Ÿæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ­¤ç±»æ¨¡å‹åœ¨å®é™…éƒ¨ç½²ä¸­é¢ä¸´çš„è®¡ç®—å’Œå†…å­˜æˆæœ¬è¿‡é«˜çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ååŒè®¾è®¡é‡åŒ– (Quantization) å’Œä»¤ç‰Œå‰ªæ (Token Pruning) æµæ°´çº¿ï¼Œå…‹æœäº†ç°æœ‰å‹ç¼©æ–¹æ³•åœ¨åŒæ—¶å¯ç”¨è¿™ä¸¤é¡¹æŠ€æœ¯æ—¶å­˜åœ¨çš„äº’ä¸å…¼å®¹é—®é¢˜ã€‚SQAP-VLA å¼•å…¥äº†å…¨æ–°çš„é‡åŒ–æ„ŸçŸ¥ä»¤ç‰Œå‰ªææ ‡å‡†ï¼Œè¯¥æ ‡å‡†èƒ½åœ¨æ¿€è¿›é‡åŒ–çš„æ¨¡å‹ä¸Šè¿è¡Œï¼Œå¹¶é€šè¿‡æ”¹è¿›é‡åŒ–å™¨è®¾è®¡æ¥å¢å¼ºå‰ªææ•ˆæœã€‚åœ¨æ ‡å‡† VLA æ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSQAP-VLA åœ¨ä¿æŒæ¨¡å‹æ ¸å¿ƒæ€§èƒ½çš„åŒæ—¶æ˜¾è‘—æå‡äº†è®¡ç®—æ•ˆç‡å’Œæ¨ç†é€Ÿåº¦ã€‚ä¸åŸå§‹æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶å®ç°äº† 1.93 å€çš„æ¨ç†åŠ é€Ÿï¼Œå¹¶å°†ä»»åŠ¡å¹³å‡æˆåŠŸç‡æå‡äº†é«˜è¾¾ 4.5%ï¼Œä¸ºé«˜æ€§èƒ½å…·èº«æ™ºèƒ½çš„å®é™…åº”ç”¨æä¾›äº†é‡è¦æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "12 pages, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.09090v1",
      "published_date": "2025-09-11 01:52:25 UTC",
      "updated_date": "2025-09-11 01:52:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T18:02:23.459096+00:00"
    },
    {
      "arxiv_id": "2509.14251v1",
      "title": "Unified Crew Planning and Replanning Optimization in Multi-Line Metro Systems Considering Workforce Heterogeneity",
      "title_zh": "è€ƒè™‘äººå‘˜å¼‚æ„æ€§çš„å¤šçº¿è·¯åœ°é“ç³»ç»Ÿä¹˜åŠ¡è®¡åˆ’ä¸é‡è°ƒåº¦ç»Ÿç­¹ä¼˜åŒ–",
      "authors": [
        "Qihang Chen"
      ],
      "abstract": "Metro crew planning is a key component of smart city development as it directly impacts the operational efficiency and service reliability of public transportation. With the rapid expansion of metro networks, effective multi-line scheduling and emergency management have become essential for large-scale seamless operations. However, current research focuses primarily on individual metro lines,with insufficient attention on cross-line coordination and rapid replanning during disruptions. Here, a unified optimization framework is presented for multi-line metro crew planning and replanning with heterogeneous workforce. Specifically, a hierarchical time-space network model is proposed to represent the unified crew action space, and computationally efficient constraints and formulations are derived for the crew's heterogeneous qualifications and preferences. Solution algorithms based on column generation and shortest path adjustment are further developed, utilizing the proposed network model. Experiments with real data from Shanghai and Beijing Metro demonstrate that the proposed methods outperform benchmark heuristics in both cost reduction and task completion,and achieve notable efficiency gains by incorporating cross-line operations, particularly for urgent tasks during disruptions. This work highlights the role of global optimization and cross-line coordination in multi-line metro system operations, providing insights into the efficient and reliable functioning of public transportation in smart cities.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åœ°é“ç½‘ç»œæ‰©å¼ èƒŒæ™¯ä¸‹çš„å¤šçº¿è·¯è°ƒåº¦ä¸çªå‘çŠ¶å†µä¸‹çš„å¿«é€Ÿé‡è§„åˆ’éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªè€ƒè™‘åŠ³åŠ¨åŠ›å¼‚æ„æ€§(Workforce Heterogeneity)çš„ç»Ÿä¸€ä¼˜åŒ–æ¡†æ¶ã€‚ç ”ç©¶æ„å»ºäº†å±‚æ¬¡åŒ–æ—¶ç©ºç½‘ç»œæ¨¡å‹(Hierarchical Time-Space Network Model)ä»¥è¡¨å¾ä¹˜åŠ¡å‘˜çš„è¡ŒåŠ¨ç©ºé—´ï¼Œå¹¶æ¨å¯¼å‡ºæ¶µç›–ä¸åŒèµ„è´¨(Qualifications)ä¸åå¥½(Preferences)çš„é«˜æ•ˆè®¡ç®—çº¦æŸã€‚é€šè¿‡å¼€å‘åŸºäºåˆ—ç”Ÿæˆ(Column Generation)å’Œæœ€çŸ­è·¯å¾„è°ƒæ•´(Shortest Path Adjustment)çš„æ±‚è§£ç®—æ³•ï¼Œå®ç°äº†å¯¹å¤æ‚è°ƒåº¦é—®é¢˜çš„æœ‰æ•ˆå¤„ç†ã€‚åˆ©ç”¨ä¸Šæµ·å’ŒåŒ—äº¬åœ°é“çœŸå®æ•°æ®çš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨é™ä½æˆæœ¬å’Œæé«˜ä»»åŠ¡å®Œæˆç‡æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºå‡†å¯å‘å¼ç®—æ³•ã€‚ç‰¹åˆ«æ˜¯åœ¨å¹²æ‰°æœŸé—´ï¼Œå¼•å…¥è·¨çº¿è¿è¥(Cross-line Operations)æå¤§æå‡äº†ç´§æ€¥ä»»åŠ¡çš„å¤„ç†æ•ˆç‡ï¼Œä¸ºæ™ºæ…§åŸå¸‚ä¸­é«˜æ•ˆã€å¯é çš„è½¨é“äº¤é€šè¿è¥æä¾›äº†å…¨å±€ä¼˜åŒ–ä¸è·¨çº¿åè°ƒçš„å†³ç­–æ”¯æŒã€‚",
      "categories": [
        "cs.AI",
        "cs.MA",
        "eess.SY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.14251v1",
      "published_date": "2025-09-11 01:43:32 UTC",
      "updated_date": "2025-09-11 01:43:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T18:02:24.352916+00:00"
    },
    {
      "arxiv_id": "2509.09074v2",
      "title": "KoopMotion: Learning Almost Divergence Free Koopman Flow Fields for Motion Planning",
      "title_zh": "KoopMotionï¼šé¢å‘è¿åŠ¨è§„åˆ’çš„å‡†æ— æ•£ Koopman æµåœºå­¦ä¹ ",
      "authors": [
        "Alice Kate Li",
        "Thales C Silva",
        "Victoria Edwards",
        "Vijay Kumar",
        "M. Ani Hsieh"
      ],
      "abstract": "In this work, we propose a novel flow field-based motion planning method that drives a robot from any initial state to a desired reference trajectory such that it converges to the trajectory's end point. Despite demonstrated efficacy in using Koopman operator theory for modeling dynamical systems, Koopman does not inherently enforce convergence to desired trajectories nor to specified goals - a requirement when learning from demonstrations (LfD). We present KoopMotion which represents motion flow fields as dynamical systems, parameterized by Koopman Operators to mimic desired trajectories, and leverages the divergence properties of the learnt flow fields to obtain smooth motion fields that converge to a desired reference trajectory when a robot is placed away from the desired trajectory, and tracks the trajectory until the end point. To demonstrate the effectiveness of our approach, we show evaluations of KoopMotion on the LASA human handwriting dataset and a 3D manipulator end-effector trajectory dataset, including spectral analysis. We also perform experiments on a physical robot, verifying KoopMotion on a miniature autonomous surface vehicle operating in a non-static fluid flow environment. Our approach is highly sample efficient in both space and time, requiring only 3\\% of the LASA dataset to generate dense motion plans. Additionally, KoopMotion provides a significant improvement over baselines when comparing metrics that measure spatial and temporal dynamics modeling efficacy. Code at: \\href{https://alicekl.github.io/koop-motion/}{\\color{blue}{https://alicekl.github.io/koop-motion}}.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† KoopMotionï¼Œä¸€ç§æ–°å‹çš„åŸºäºæµåœº (flow field-based) çš„è¿åŠ¨è§„åˆ’æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ Koopman operator theory åœ¨å»ºæ¨¡åŠ¨åŠ›ç³»ç»Ÿæ—¶æ— æ³•ä¿è¯æ”¶æ•›è‡³ç›®æ ‡è½¨è¿¹æˆ–æŒ‡å®šç›®æ ‡çš„é—®é¢˜ã€‚KoopMotion å°†è¿åŠ¨æµåœºè¡¨ç¤ºä¸ºç”± Koopman Operators å‚æ•°åŒ–çš„åŠ¨åŠ›ç³»ç»Ÿï¼Œé€šè¿‡åˆ©ç”¨å­¦ä¹ æµåœºçš„å‘æ•£ç‰¹æ€§ (divergence properties) æ¥æ„å»ºå¹³æ»‘çš„è¿åŠ¨åœºï¼Œç¡®ä¿æœºå™¨äººèƒ½ä»ä»»æ„åˆå§‹çŠ¶æ€æ”¶æ•›å¹¶è¿½è¸ªå‚è€ƒè½¨è¿¹ç›´è‡³ç»ˆç‚¹ã€‚å®éªŒåœ¨ LASA äººç±»æ‰‹å†™æ•°æ®é›†ã€3D æœºæ¢°è‡‚è½¨è¿¹æ•°æ®é›†ä»¥åŠå¤æ‚æµä½“ç¯å¢ƒä¸­çš„å°å‹è‡ªä¸»æ°´é¢æœºå™¨äºº (miniature autonomous surface vehicle) ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ç»“æœè¡¨æ˜ï¼ŒKoopMotion å…·æœ‰æé«˜çš„æ ·æœ¬æ•ˆç‡ï¼Œä»…éœ€ LASA æ•°æ®é›†çš„ 3% å³å¯ç”Ÿæˆå¯†é›†è¿åŠ¨è§„åˆ’ï¼Œä¸”åœ¨ç©ºé—´å’Œæ—¶é—´åŠ¨åŠ›å­¦å»ºæ¨¡æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†æ¨¡å‹ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Revised with link to code. Accepted to CoRL 2025 (Conference on Robot Learning). 15 pages 11 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.09074v2",
      "published_date": "2025-09-11 00:42:01 UTC",
      "updated_date": "2025-11-12 17:30:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T18:02:33.694692+00:00"
    },
    {
      "arxiv_id": "2509.09071v3",
      "title": "Strategic Tradeoffs Between Humans and AI in Multi-Agent Bargaining",
      "title_zh": "å¤šæ™ºèƒ½ä½“è°ˆåˆ¤ä¸­äººç±»ä¸äººå·¥æ™ºèƒ½çš„ç­–ç•¥æƒè¡¡",
      "authors": [
        "Crystal Qian",
        "Kehang Zhu",
        "John Horton",
        "Benjamin S. Manning",
        "Vivian Tsai",
        "James Wexler",
        "Nithum Thain"
      ],
      "abstract": "As large language models (LLMs) are increasingly embedded in collaborative human activities such as business negotiations and group coordination, it becomes critical to evaluate both the performance gains they can achieve and how they interact in dynamic, multi-agent environments. Unlike traditional statistical agents such as Bayesian models, which may excel under well-specified conditions, large language models (LLMs) can generalize across diverse, real-world scenarios, raising new questions about how their strategies and behaviors compare to those of humans and other agent types. In this work, we compare outcomes and behavioral dynamics across humans (N = 216), LLMs (GPT-4o, Gemini 1.5 Pro), and Bayesian agents in a dynamic negotiation setting under identical conditions. Bayesian agents extract the highest surplus through aggressive optimization, at the cost of frequent trade rejections. Humans and LLMs achieve similar overall surplus, but through distinct behaviors: LLMs favor conservative, concessionary trades with few rejections, while humans employ more strategic, risk-taking, and fairness-oriented behaviors. Thus, we find that performance parity -- a common benchmark in agent evaluation -- can conceal fundamental differences in process and alignment, which are critical for practical deployment in real-world coordination tasks. By establishing foundational behavioral baselines under matched conditions, this work provides a baseline for future studies in more applied, variable-rich environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººç±»ä¸äººå·¥æ™ºèƒ½åœ¨å¤šæ™ºèƒ½ä½“è®¨ä»·è¿˜ä»·(Multi-agent Bargaining)ä¸­çš„æˆ˜ç•¥æƒè¡¡ï¼Œå¯¹æ¯”äº†äººç±»ã€å¤§è¯­è¨€æ¨¡å‹(LLMsï¼Œå¦‚GPT-4oå’ŒGemini 1.5 Pro)ä»¥åŠè´å¶æ–¯æ™ºèƒ½ä½“(Bayesian agents)åœ¨ç›¸åŒåŠ¨æ€è°ˆåˆ¤ç¯å¢ƒä¸‹çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼ŒBayesian agentsé€šè¿‡æ¿€è¿›çš„ä¼˜åŒ–è·å¾—äº†æœ€é«˜ç›ˆä½™ï¼Œä½†é¢ä¸´é¢‘ç¹çš„äº¤æ˜“è¢«æ‹’ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œäººç±»å’ŒLLMsåœ¨æ€»ç›ˆä½™ä¸Šå®ç°äº†æ€§èƒ½å¯¹ç­‰(Performance parity)ï¼Œä½†è¡Œä¸ºæ¨¡å¼è¿¥å¼‚ï¼šLLMsè¡¨ç°å‡ºä¿å®ˆã€è®©æ­¥ä¸”æå°‘æ‹’ç»çš„å€¾å‘ï¼Œè€Œäººç±»åˆ™æ›´å…·æˆ˜ç•¥æ€§ã€é£é™©æ„è¯†å’Œå…¬å¹³å¯¼å‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå•çº¯çš„æ€§èƒ½æŒ‡æ ‡å¾€å¾€æ©ç›–äº†è¿‡ç¨‹å’Œå¯¹é½(Alignment)ä¸Šçš„æ ¹æœ¬å·®å¼‚ï¼Œè¿™å¯¹äººå·¥æ™ºèƒ½åœ¨ç°å®ä¸–ç•Œåè°ƒä»»åŠ¡ä¸­çš„å®é™…éƒ¨ç½²å…·æœ‰é‡è¦æ„ä¹‰ã€‚è¯¥é¡¹å·¥ä½œä¸ºæœªæ¥åœ¨æ›´å¤æ‚çš„åº”ç”¨ç¯å¢ƒä¸­ç ”ç©¶æ™ºèƒ½ä½“è¡Œä¸ºå¥ å®šäº†é‡è¦åŸºå‡†ã€‚",
      "categories": [
        "cs.AI",
        "cs.GT",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.09071v3",
      "published_date": "2025-09-11 00:25:07 UTC",
      "updated_date": "2025-10-13 22:19:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T18:02:55.798582+00:00"
    },
    {
      "arxiv_id": "2509.09070v2",
      "title": "STRIDE: Subset-Free Functional Decomposition for XAI in Tabular Settings",
      "title_zh": "STRIDEï¼šè¡¨æ ¼æ•°æ®åœºæ™¯ä¸‹ XAI çš„æ— å­é›†æšä¸¾å‡½æ•°åˆ†è§£",
      "authors": [
        "Chaeyun Ko"
      ],
      "abstract": "Most explainable AI (XAI) frameworks are limited in their expressiveness, summarizing complex feature effects as single scalar values Ï†_i. This approach answers \"what\" features are important but fails to reveal \"how\" they interact. Furthermore, methods that attempt to capture interactions, like those based on Shapley values, often face an exponential computational cost. We present STRIDE, a scalable framework that addresses both limitations by reframing explanation as a subset-enumeration-free, orthogonal \"functional decomposition\" in a Reproducing Kernel Hilbert Space (RKHS). In the tabular setups we study, STRIDE analytically computes functional components f_S(x_S) via a recursive kernel-centering procedure. The approach is model-agnostic and theoretically grounded with results on orthogonality and L^2 convergence. In tabular benchmarks (10 datasets, median over 10 seeds), STRIDE attains a 3.0 times median speedup over TreeSHAP and a mean R^2=0.93 for reconstruction. We also introduce \"component surgery\", a diagnostic that isolates a learned interaction and quantifies its contribution; on California Housing, removing a single interaction reduces test R^2 from 0.019 to 0.027.",
      "tldr_zh": "é’ˆå¯¹ç°æœ‰å¯è§£é‡Šäººå·¥æ™ºèƒ½(XAI)æ¡†æ¶åœ¨å¤„ç†ç‰¹å¾äº¤äº’æ—¶è¡¨è¾¾èƒ½åŠ›æœ‰é™ä¸”è®¡ç®—æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†STRIDEæ¡†æ¶ã€‚STRIDEå°†è§£é‡Šè¿‡ç¨‹é‡æ–°å®šä¹‰ä¸ºå†ç”Ÿæ ¸å¸Œå°”ä¼¯ç‰¹ç©ºé—´(RKHS)ä¸­æ— éœ€å­é›†æšä¸¾çš„æ­£äº¤å‡½æ•°åˆ†è§£(functional decomposition)ã€‚åœ¨è¡¨æ ¼æ•°æ®è®¾ç½®ä¸‹ï¼Œè¯¥æ¡†æ¶é€šè¿‡é€’å½’æ ¸ä¸­å¿ƒåŒ–(recursive kernel-centering)ç¨‹åºè§£æè®¡ç®—åŠŸèƒ½ç»„ä»¶ï¼Œç¡®ä¿äº†æ–¹æ³•çš„æ¨¡å‹æ— å…³æ€§(model-agnostic)ä¸ç†è®ºä¸¥è°¨æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSTRIDEåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„ä¸­ä½æ•°è¿è¡Œé€Ÿåº¦æ¯”TreeSHAPå¿«3å€ï¼Œä¸”é‡æ„ç²¾ç¡®åº¦å¹³å‡è¾¾åˆ°R^2=0.93ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†â€œç»„ä»¶æ‰‹æœ¯â€(component surgery)è¯Šæ–­å·¥å…·ï¼Œç”¨äºéš”ç¦»å¹¶é‡åŒ–ç‰¹å®šç‰¹å¾äº¤äº’å¯¹æ¨¡å‹é¢„æµ‹çš„è´¡çŒ®ã€‚è¯¥ç ”ç©¶ä¸ºç†è§£å¤æ‚ç‰¹å¾äº¤äº’æä¾›äº†ä¸€ç§é«˜æ•ˆã€å¯æ‰©å±•ä¸”å…·å¤‡ç†è®ºæ”¯æ’‘çš„æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Major revision for submission to ICLR 2026. Substantially revised abstract, introduction, and discussion. Added new 'component surgery' analysis and updated benchmark results for clarity. (12 pages, 2 figures)",
      "pdf_url": "https://arxiv.org/pdf/2509.09070v2",
      "published_date": "2025-09-11 00:19:53 UTC",
      "updated_date": "2025-09-15 03:49:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T18:02:56.192985+00:00"
    },
    {
      "arxiv_id": "2509.09066v1",
      "title": "Instructional Prompt Optimization for Few-Shot LLM-Based Recommendations on Cold-Start Users",
      "title_zh": "é¢å‘å†·å¯åŠ¨ç”¨æˆ·çš„å°‘æ ·æœ¬å¤§è¯­è¨€æ¨¡å‹æ¨èæŒ‡ä»¤æç¤ºä¼˜åŒ–",
      "authors": [
        "Haowei Yang",
        "Yushang Zhao",
        "Sitao Min",
        "Bo Su",
        "Chao Yao",
        "Wei Xu"
      ],
      "abstract": "The cold-start user issue further compromises the effectiveness of recommender systems in limiting access to the historical behavioral information. It is an effective pipeline to optimize instructional prompts on a few-shot large language model (LLM) used in recommender tasks. We introduce a context-conditioned prompt formulation method P(u,\\ Ds)\\ \\rightarrow\\ R\\widehat, where u is a cold-start user profile, Ds is a curated support set, and R\\widehat is the predicted ranked list of items. Based on systematic experimentation with transformer-based autoregressive LLMs (BioGPT, LLaMA-2, GPT-4), we provide empirical evidence that optimal exemplar injection and instruction structuring can significantly improve the precision@k and NDCG scores of such models in low-data settings. The pipeline uses token-level alignments and embedding space regularization with a greater semantic fidelity. Our findings not only show that timely composition is not merely syntactic but also functional as it is in direct control of attention scales and decoder conduct through inference. This paper shows that prompt-based adaptation may be considered one of the ways to address cold-start recommendation issues in LLM-based pipelines.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ¨èç³»ç»Ÿä¸­çš„å†·å¯åŠ¨(Cold-start)ç”¨æˆ·é—®é¢˜ï¼Œå³åœ¨å†å²è¡Œä¸ºä¿¡æ¯æœ‰é™çš„æƒ…å†µä¸‹ï¼Œæå‡ºäº†ä¸€ç§é’ˆå¯¹å°‘æ ·æœ¬(Few-shot)å¤§è¯­è¨€æ¨¡å‹(LLMs)æ¨èä»»åŠ¡çš„æŒ‡ä»¤æç¤ºè¯ä¼˜åŒ–æ¡†æ¶ã€‚ç ”ç©¶å¼•å…¥äº†ä¸Šä¸‹æ–‡è°ƒèŠ‚çš„æç¤ºè¯æ„å»ºæ–¹æ³• $P(u, D_s) \\to \\hat{R}$ï¼Œé€šè¿‡æ•´åˆç”¨æˆ·ç”»åƒå’Œç²¾å¿ƒè®¾è®¡çš„æ”¯æŒé›†æ¥é¢„æµ‹é¡¹ç›®æ’ååˆ—è¡¨ã€‚åœ¨BioGPTã€LLaMA-2å’ŒGPT-4ç­‰æ¨¡å‹ä¸Šçš„ç³»ç»Ÿå®éªŒè¡¨æ˜ï¼Œä¼˜åŒ–æ ·æœ¬æ³¨å…¥(Exemplar injection)å’ŒæŒ‡ä»¤ç»“æ„åŒ–(Instruction structuring)èƒ½æ˜¾è‘—æå‡æ¨¡å‹åœ¨ä½æ•°æ®ç¯å¢ƒä¸‹çš„ Precision@k å’Œ NDCG åˆ†æ•°ã€‚è¯¥æ–¹æ¡ˆåˆ©ç”¨æ ‡è®°çº§å¯¹é½(Token-level alignments)å’ŒåµŒå…¥ç©ºé—´æ­£åˆ™åŒ–(Embedding space regularization)ç¡®ä¿äº†æ›´é«˜çš„è¯­ä¹‰ä¿çœŸåº¦ã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°ï¼Œæç¤ºè¯çš„ç»„åˆåœ¨æ¨ç†è¿‡ç¨‹ä¸­èƒ½ç›´æ¥æ§åˆ¶æ³¨æ„åŠ›å°ºåº¦(Attention scales)å’Œè§£ç å™¨è¡Œä¸ºï¼Œè¯æ˜äº†åŸºäºæç¤ºè¯çš„è‡ªé€‚åº”æŠ€æœ¯æ˜¯è§£å†³å¤§è¯­è¨€æ¨¡å‹æµæ°´çº¿ä¸­å†·å¯åŠ¨æ¨èé—®é¢˜çš„æœ‰æ•ˆé€”å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.09066v1",
      "published_date": "2025-09-11 00:13:17 UTC",
      "updated_date": "2025-09-11 00:13:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T18:02:59.495642+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 123,
  "processed_papers_count": 123,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-24T18:03:58.993709+00:00"
}
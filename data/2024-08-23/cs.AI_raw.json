[
  {
    "arxiv_id": "2408.14501v1",
    "title": "Applying graph neural network to SupplyGraph for supply chain network",
    "authors": [
      "Kihwan Han"
    ],
    "abstract": "Supply chain networks describe interactions between products, manufacture\nfacilities, storages in the context of supply and demand of the products.\nSupply chain data are inherently under graph structure; thus, it can be fertile\nground for applications of graph neural network (GNN). Very recently, supply\nchain dataset, SupplyGraph, has been released to the public. Though the\nSupplyGraph dataset is valuable given scarcity of publicly available data,\nthere was less clarity on description of the dataset, data quality assurance\nprocess, and hyperparameters of the selected models. Further, for\ngeneralizability of findings, it would be more convincing to present the\nfindings by performing statistical analyses on the distribution of errors\nrather than showing the average value of the errors. Therefore, this study\nassessed the supply chain dataset, SupplyGraph, with better clarity on analyses\nprocesses, data quality assurance, machine learning (ML) model specifications.\nAfter data quality assurance procedures, this study compared performance of\nMultilayer Perceptions (MLP), Graph Convolution Network (GCN), and Graph\nAttention Network (GAT) on a demanding forecasting task while matching\nhyperparameters as feasible as possible. The analyses revealed that GAT\nperformed best, followed by GCN and MLP. Those performance improvements were\nstatistically significant at $\\alpha = 0.05$ after correction for multiple\ncomparisons. This study also discussed several considerations in applying GNN\nto supply chain networks. The current study reinforces the previous study in\nsupply chain benchmark dataset with respect to description of the dataset and\nmethodology, so that the future research in applications of GNN to supply chain\nbecomes more reproducible.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.14501v1",
    "published_date": "2024-08-23 23:42:18 UTC",
    "updated_date": "2024-08-23 23:42:18 UTC"
  },
  {
    "arxiv_id": "2408.13406v1",
    "title": "Optimizing Collaboration of LLM based Agents for Finite Element Analysis",
    "authors": [
      "Chuan Tian",
      "Yilei Zhang"
    ],
    "abstract": "This paper investigates the interactions between multiple agents within Large\nLanguage Models (LLMs) in the context of programming and coding tasks. We\nutilize the AutoGen framework to facilitate communication among agents,\nevaluating different configurations based on the success rates from 40 random\nruns for each setup. The study focuses on developing a flexible automation\nframework for applying the Finite Element Method (FEM) to solve linear elastic\nproblems. Our findings emphasize the importance of optimizing agent roles and\nclearly defining their responsibilities, rather than merely increasing the\nnumber of agents. Effective collaboration among agents is shown to be crucial\nfor addressing general FEM challenges. This research demonstrates the potential\nof LLM multi-agent systems to enhance computational automation in simulation\nmethodologies, paving the way for future advancements in engineering and\nartificial intelligence.",
    "categories": [
      "cs.AI",
      "cs.CE",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.13406v1",
    "published_date": "2024-08-23 23:11:08 UTC",
    "updated_date": "2024-08-23 23:11:08 UTC"
  },
  {
    "arxiv_id": "2408.13399v2",
    "title": "Transforming Location Retrieval at Airbnb: A Journey from Heuristics to Reinforcement Learning",
    "authors": [
      "Dillon Davis",
      "Huiji Gao",
      "Thomas Legrand",
      "Weiwei Guo",
      "Malay Haldar",
      "Alex Deng",
      "Han Zhao",
      "Liwei He",
      "Sanjeev Katariya"
    ],
    "abstract": "The Airbnb search system grapples with many unique challenges as it continues\nto evolve. We oversee a marketplace that is nuanced by geography, diversity of\nhomes, and guests with a variety of preferences. Crafting an efficient search\nsystem that can accommodate diverse guest needs, while showcasing relevant\nhomes lies at the heart of Airbnb's success. Airbnb search has many challenges\nthat parallel other recommendation and search systems but it has a unique\ninformation retrieval problem, upstream of ranking, called location retrieval.\nIt requires defining a topological map area that is relevant to the searched\nquery for homes listing retrieval. The purpose of this paper is to demonstrate\nthe methodology, challenges, and impact of building a machine learning based\nlocation retrieval product from the ground up. Despite the lack of suitable,\nprevalent machine learning based approaches, we tackle cold start,\ngeneralization, differentiation and algorithmic bias. We detail the efficacy of\nheuristics, statistics, machine learning, and reinforcement learning approaches\nto solve these challenges, particularly for systems that are often unexplored\nby current literature.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "Published at CIKM 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.13399v2",
    "published_date": "2024-08-23 22:51:14 UTC",
    "updated_date": "2024-10-28 15:48:08 UTC"
  },
  {
    "arxiv_id": "2408.13379v2",
    "title": "N-DriverMotion: Driver motion learning and prediction using an event-based camera and directly trained spiking neural networks on Loihi 2",
    "authors": [
      "Hyo Jong Chung",
      "Byungkon Kang",
      "Yoonseok Yang"
    ],
    "abstract": "Driver motion recognition is a principal factor in ensuring the safety of\ndriving systems. This paper presents a novel system for learning and predicting\ndriver motions and an event-based high-resolution (1280x720) dataset,\nN-DriverMotion, newly collected to train on a neuromorphic vision system. The\nsystem comprises an event-based camera that generates the first high-resolution\ndriver motion dataset representing spike inputs and efficient spiking neural\nnetworks (SNNs) that are effective in training and predicting the driver's\ngestures. The event dataset consists of 13 driver motion categories classified\nby direction (front, side), illumination (bright, moderate, dark), and\nparticipant. A novel simplified four-layer convolutional spiking neural network\n(CSNN) that we proposed was directly trained using the high-resolution dataset\nwithout any time-consuming preprocessing. This enables efficient adaptation to\non-device SNNs for real-time inference on high-resolution event-based streams.\nCompared with recent gesture recognition systems adopting neural networks for\nvision processing, the proposed neuromorphic vision system achieves comparable\naccuracy, 94.04\\%, in recognizing driver motions with the CSNN architecture.\nOur proposed CSNN and the dataset can be used to develop safer and more\nefficient driver monitoring systems for autonomous vehicles or edge devices\nrequiring an efficient neural network architecture.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "68T45",
      "I.4.8; I.4.9"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted for publication in IEEE Open Journal of Vehicular Technology\n  (OJVT) on 18 November 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.13379v2",
    "published_date": "2024-08-23 21:25:16 UTC",
    "updated_date": "2024-11-18 20:30:20 UTC"
  },
  {
    "arxiv_id": "2408.13378v4",
    "title": "DrugAgent: Multi-Agent Large Language Model-Based Reasoning for Drug-Target Interaction Prediction",
    "authors": [
      "Yoshitaka Inoue",
      "Tianci Song",
      "Xinling Wang",
      "Augustin Luna",
      "Tianfan Fu"
    ],
    "abstract": "Advancements in large language models (LLMs) allow them to address diverse\nquestions using human-like interfaces. Still, limitations in their training\nprevent them from answering accurately in scenarios that could benefit from\nmultiple perspectives. Multi-agent systems allow the resolution of questions to\nenhance result consistency and reliability. While drug-target interaction (DTI)\nprediction is important for drug discovery, existing approaches face challenges\ndue to complex biological systems and the lack of interpretability needed for\nclinical applications. DrugAgent is a multi-agent LLM system for DTI prediction\nthat combines multiple specialized perspectives with transparent reasoning. Our\nsystem adapts and extends existing multi-agent frameworks by (1) applying\ncoordinator-based architecture to the DTI domain, (2) integrating\ndomain-specific data sources, including ML predictions, knowledge graphs, and\nliterature evidence, and (3) incorporating Chain-of-Thought (CoT) and ReAct\n(Reason+Act) frameworks for transparent DTI reasoning. We conducted\ncomprehensive experiments using a kinase inhibitor dataset, where our\nmulti-agent LLM method outperformed the non-reasoning multi-agent model (GPT-4o\nmini) by 45% in F1 score (0.514 vs 0.355). Through ablation studies, we\ndemonstrated the contributions of each agent, with the AI agent being the most\nimpactful, followed by the KG agent and search agent. Most importantly, our\napproach provides detailed, human-interpretable reasoning for each prediction\nby combining evidence from multiple sources - a critical feature for biomedical\napplications where understanding the rationale behind predictions is essential\nfor clinical decision-making and regulatory compliance. Code is available at\nhttps://anonymous.4open.science/r/DrugAgent-B2EA.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.IR",
      "cs.LG",
      "q-bio.QM"
    ],
    "primary_category": "cs.AI",
    "comment": "15 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2408.13378v4",
    "published_date": "2024-08-23 21:24:59 UTC",
    "updated_date": "2025-04-07 19:32:55 UTC"
  },
  {
    "arxiv_id": "2408.13376v3",
    "title": "Reduce, Reuse, Recycle: Categories for Compositional Reinforcement Learning",
    "authors": [
      "Georgios Bakirtzis",
      "Michail Savvas",
      "Ruihan Zhao",
      "Sandeep Chinchali",
      "Ufuk Topcu"
    ],
    "abstract": "In reinforcement learning, conducting task composition by forming cohesive,\nexecutable sequences from multiple tasks remains challenging. However, the\nability to (de)compose tasks is a linchpin in developing robotic systems\ncapable of learning complex behaviors. Yet, compositional reinforcement\nlearning is beset with difficulties, including the high dimensionality of the\nproblem space, scarcity of rewards, and absence of system robustness after task\ncomposition. To surmount these challenges, we view task composition through the\nprism of category theory -- a mathematical discipline exploring structures and\ntheir compositional relationships. The categorical properties of Markov\ndecision processes untangle complex tasks into manageable sub-tasks, allowing\nfor strategical reduction of dimensionality, facilitating more tractable reward\nstructures, and bolstering system robustness. Experimental results support the\ncategorical theory of reinforcement learning by enabling skill reduction,\nreuse, and recycling when learning complex robotic arm tasks.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SY",
      "math.CT"
    ],
    "primary_category": "cs.AI",
    "comment": "ECAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.13376v3",
    "published_date": "2024-08-23 21:23:22 UTC",
    "updated_date": "2025-03-11 22:01:33 UTC"
  },
  {
    "arxiv_id": "2409.00070v1",
    "title": "Learning to Plan Long-Term for Language Modeling",
    "authors": [
      "Florian Mai",
      "Nathan Cornille",
      "Marie-Francine Moens"
    ],
    "abstract": "Modern language models predict the next token in the sequence by considering\nthe past text through a powerful function such as attention. However, language\nmodels have no explicit mechanism that allows them to spend computation time\nfor planning long-distance future text, leading to a suboptimal token\nprediction. In this paper, we propose a planner that predicts a latent plan for\nmany sentences into the future. By sampling multiple plans at once, we\ncondition the language model on an accurate approximation of the distribution\nof text continuations, which leads to better next token prediction accuracy. In\neffect, this allows trading computation time for prediction accuracy.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "preprint",
    "pdf_url": "http://arxiv.org/pdf/2409.00070v1",
    "published_date": "2024-08-23 21:18:10 UTC",
    "updated_date": "2024-08-23 21:18:10 UTC"
  },
  {
    "arxiv_id": "2408.13372v1",
    "title": "Understanding Defects in Generated Codes by Language Models",
    "authors": [
      "Ali Mohammadi Esfahani",
      "Nafiseh Kahani",
      "Samuel A. Ajila"
    ],
    "abstract": "This study investigates the reliability of code generation by Large Language\nModels (LLMs), focusing on identifying and analyzing defects in the generated\ncode. Despite the advanced capabilities of LLMs in automating code generation,\nensuring the accuracy and functionality of the output remains a significant\nchallenge. By using a structured defect classification method to understand\ntheir nature and origins this study categorizes and analyzes 367 identified\ndefects from code snippets generated by LLMs, with a significant proportion\nbeing functionality and algorithm errors. These error categories indicate key\nareas where LLMs frequently fail, underscoring the need for targeted\nimprovements. To enhance the accuracy of code generation, this paper\nimplemented five prompt engineering techniques, including Scratchpad Prompting,\nProgram of Thoughts Prompting, Chain-of-Thought Prompting, Chain of Code\nPrompting, and Structured Chain-of-Thought Prompting. These techniques were\napplied to refine the input prompts, aiming to reduce ambiguities and improve\nthe models' accuracy rate. The research findings suggest that precise and\nstructured prompting significantly mitigates common defects, thereby increasing\nthe reliability of LLM-generated code.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.13372v1",
    "published_date": "2024-08-23 21:10:09 UTC",
    "updated_date": "2024-08-23 21:10:09 UTC"
  },
  {
    "arxiv_id": "2408.13366v1",
    "title": "CodeRefine: A Pipeline for Enhancing LLM-Generated Code Implementations of Research Papers",
    "authors": [
      "Ekaterina Trofimova",
      "Emil Sataev",
      "Abhijit Singh Jowhari"
    ],
    "abstract": "This paper presents CodeRefine, a novel framework for automatically\ntransforming research paper methodologies into functional code using Large\nLanguage Models (LLMs). Our multi-step approach first extracts and summarizes\nkey text chunks from papers, analyzes their code relevance, and creates a\nknowledge graph using a predefined ontology. Code is then generated from this\nstructured representation and enhanced through a proposed retrospective\nretrieval-augmented generation approach. CodeRefine addresses the challenge of\nbridging theoretical research and practical implementation, offering a more\naccurate alternative to LLM zero-shot prompting. Evaluations on diverse\nscientific papers demonstrate CodeRefine's ability to improve code\nimplementation from the paper, potentially accelerating the adoption of\ncutting-edge algorithms in real-world applications.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.13366v1",
    "published_date": "2024-08-23 20:51:04 UTC",
    "updated_date": "2024-08-23 20:51:04 UTC"
  },
  {
    "arxiv_id": "2408.13364v2",
    "title": "Reconciling Different Theories of Learning with an Agent-based Model of Procedural Learning",
    "authors": [
      "Sina Rismanchian",
      "Shayan Doroudi"
    ],
    "abstract": "Computational models of human learning can play a significant role in\nenhancing our knowledge about nuances in theoretical and qualitative learning\ntheories and frameworks. There are many existing frameworks in educational\nsettings that have shown to be verified using empirical studies, but at times\nwe find these theories make conflicting claims or recommendations for\ninstruction. In this study, we propose a new computational model of human\nlearning, Procedural ABICAP, that reconciles the ICAP,\nKnowledge-Learning-Instruction (KLI), and cognitive load theory (CLT)\nframeworks for learning procedural knowledge. ICAP assumes that constructive\nlearning generally yields better learning outcomes, while theories such as KLI\nand CLT claim that this is not always true. We suppose that one reason for this\nmay be that ICAP is primarily used for conceptual learning and is\nunderspecified as a framework for thinking about procedural learning. We show\nhow our computational model, both by design and through simulations, can be\nused to reconcile different results in the literature. More generally, we\nposition our computational model as an executable theory of learning that can\nbe used to simulate various educational settings.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.13364v2",
    "published_date": "2024-08-23 20:45:14 UTC",
    "updated_date": "2025-04-11 21:47:04 UTC"
  },
  {
    "arxiv_id": "2408.13359v2",
    "title": "Power Scheduler: A Batch Size and Token Number Agnostic Learning Rate Scheduler",
    "authors": [
      "Yikang Shen",
      "Matthew Stallone",
      "Mayank Mishra",
      "Gaoyuan Zhang",
      "Shawn Tan",
      "Aditya Prasad",
      "Adriana Meza Soria",
      "David D. Cox",
      "Rameswar Panda"
    ],
    "abstract": "Finding the optimal learning rate for language model pretraining is a\nchallenging task. This is not only because there is a complicated correlation\nbetween learning rate, batch size, number of training tokens, model size, and\nother hyperparameters but also because it is prohibitively expensive to perform\na hyperparameter search for large language models with Billions or Trillions of\nparameters. Recent studies propose using small proxy models and small corpus to\nperform hyperparameter searches and transposing the optimal parameters to large\nmodels and large corpus. While the zero-shot transferability is theoretically\nand empirically proven for model size related hyperparameters, like depth and\nwidth, the zero-shot transfer from small corpus to large corpus is\nunderexplored. In this paper, we study the correlation between optimal learning\nrate, batch size, and number of training tokens for the recently proposed WSD\nscheduler. After thousands of small experiments, we found a power-law\nrelationship between variables and demonstrated its transferability across\nmodel sizes. Based on the observation, we propose a new learning rate\nscheduler, Power scheduler, that is agnostic about the number of training\ntokens and batch size. The experiment shows that combining the Power scheduler\nwith Maximum Update Parameterization (muP) can consistently achieve impressive\nperformance with one set of hyperparameters regardless of the number of\ntraining tokens, batch size, model size, and even model architecture. Our 3B\ndense and MoE models trained with the Power scheduler achieve comparable\nperformance as state-of-the-art small language models. We open-source these\npretrained models at https://ibm.biz/BdKhLa.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.13359v2",
    "published_date": "2024-08-23 20:22:20 UTC",
    "updated_date": "2024-09-11 20:48:05 UTC"
  },
  {
    "arxiv_id": "2408.13355v1",
    "title": "Disentangled Training with Adversarial Examples For Robust Small-footprint Keyword Spotting",
    "authors": [
      "Zhenyu Wang",
      "Li Wan",
      "Biqiao Zhang",
      "Yiteng Huang",
      "Shang-Wen Li",
      "Ming Sun",
      "Xin Lei",
      "Zhaojun Yang"
    ],
    "abstract": "A keyword spotting (KWS) engine that is continuously running on device is\nexposed to various speech signals that are usually unseen before. It is a\nchallenging problem to build a small-footprint and high-performing KWS model\nwith robustness under different acoustic environments. In this paper, we\nexplore how to effectively apply adversarial examples to improve KWS\nrobustness. We propose datasource-aware disentangled learning with adversarial\nexamples to reduce the mismatch between the original and adversarial data as\nwell as the mismatch across original training datasources. The KWS model\narchitecture is based on depth-wise separable convolution and a simple\nattention module. Experimental results demonstrate that the proposed learning\nstrategy improves false reject rate by $40.31%$ at $1%$ false accept rate on\nthe internal dataset, compared to the strongest baseline without using\nadversarial examples. Our best-performing system achieves $98.06%$ accuracy on\nthe Google Speech Commands V1 dataset.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.13355v1",
    "published_date": "2024-08-23 20:03:51 UTC",
    "updated_date": "2024-08-23 20:03:51 UTC"
  },
  {
    "arxiv_id": "2409.00069v1",
    "title": "How to Measure Human-AI Prediction Accuracy in Explainable AI Systems",
    "authors": [
      "Sujay Koujalgi",
      "Andrew Anderson",
      "Iyadunni Adenuga",
      "Shikha Soneji",
      "Rupika Dikkala",
      "Teresita Guzman Nader",
      "Leo Soccio",
      "Sourav Panda",
      "Rupak Kumar Das",
      "Margaret Burnett",
      "Jonathan Dodge"
    ],
    "abstract": "Assessing an AI system's behavior-particularly in Explainable AI Systems-is\nsometimes done empirically, by measuring people's abilities to predict the\nagent's next move-but how to perform such measurements? In empirical studies\nwith humans, an obvious approach is to frame the task as binary (i.e.,\nprediction is either right or wrong), but this does not scale. As output spaces\nincrease, so do floor effects, because the ratio of right answers to wrong\nanswers quickly becomes very small. The crux of the problem is that the binary\nframing is failing to capture the nuances of the different degrees of\n\"wrongness.\" To address this, we begin by proposing three mathematical bases\nupon which to measure \"partial wrongness.\" We then uses these bases to perform\ntwo analyses on sequential decision-making domains: the first is an in-lab\nstudy with 86 participants on a size-36 action space; the second is a\nre-analysis of a prior study on a size-4 action space. Other researchers\nadopting our operationalization of the prediction task and analysis methodology\nwill improve the rigor of user studies conducted with that task, which is\nparticularly important when the domain features a large output space.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "D.2.8"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.00069v1",
    "published_date": "2024-08-23 19:52:37 UTC",
    "updated_date": "2024-08-23 19:52:37 UTC"
  },
  {
    "arxiv_id": "2408.13341v1",
    "title": "Toward Improving Synthetic Audio Spoofing Detection Robustness via Meta-Learning and Disentangled Training With Adversarial Examples",
    "authors": [
      "Zhenyu Wang",
      "John H. L. Hansen"
    ],
    "abstract": "Advances in automatic speaker verification (ASV) promote research into the\nformulation of spoofing detection systems for real-world applications. The\nperformance of ASV systems can be degraded severely by multiple types of\nspoofing attacks, namely, synthetic speech (SS), voice conversion (VC), replay,\ntwins and impersonation, especially in the case of unseen synthetic spoofing\nattacks. A reliable and robust spoofing detection system can act as a security\ngate to filter out spoofing attacks instead of having them reach the ASV\nsystem. A weighted additive angular margin loss is proposed to address the data\nimbalance issue, and different margins has been assigned to improve\ngeneralization to unseen spoofing attacks in this study. Meanwhile, we\nincorporate a meta-learning loss function to optimize differences between the\nembeddings of support versus query set in order to learn a\nspoofing-category-independent embedding space for utterances. Furthermore, we\ncraft adversarial examples by adding imperceptible perturbations to spoofing\nspeech as a data augmentation strategy, then we use an auxiliary batch\nnormalization (BN) to guarantee that corresponding normalization statistics are\nperformed exclusively on the adversarial examples. Additionally, A simple\nattention module is integrated into the residual block to refine the feature\nextraction process. Evaluation results on the Logical Access (LA) track of the\nASVspoof 2019 corpus provides confirmation of our proposed approaches'\neffectiveness in terms of a pooled EER of 0.87%, and a min t-DCF of 0.0277.\nThese advancements offer effective options to reduce the impact of spoofing\nattacks on voice recognition/authentication systems.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "IEEE ACCESS 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.13341v1",
    "published_date": "2024-08-23 19:26:54 UTC",
    "updated_date": "2024-08-23 19:26:54 UTC"
  },
  {
    "arxiv_id": "2408.14499v1",
    "title": "SHEDAD: SNN-Enhanced District Heating Anomaly Detection for Urban Substations",
    "authors": [
      "Jonne van Dreven",
      "Abbas Cheddad",
      "Sadi Alawadi",
      "Ahmad Nauman Ghazi",
      "Jad Al Koussa",
      "Dirk Vanhoudt"
    ],
    "abstract": "District Heating (DH) systems are essential for energy-efficient urban\nheating. However, despite the advancements in automated fault detection and\ndiagnosis (FDD), DH still faces challenges in operational faults that impact\nefficiency. This study introduces the Shared Nearest Neighbor Enhanced District\nHeating Anomaly Detection (SHEDAD) approach, designed to approximate the DH\nnetwork topology and allow for local anomaly detection without disclosing\nsensitive information, such as substation locations. The approach leverages a\nmulti-adaptive k-Nearest Neighbor (k-NN) graph to improve the initial\nneighborhood creation. Moreover, it introduces a merging technique that reduces\nnoise and eliminates trivial edges. We use the Median Absolute Deviation (MAD)\nand modified z-scores to flag anomalous substations. The results reveal that\nSHEDAD outperforms traditional clustering methods, achieving significantly\nlower intra-cluster variance and distance. Additionally, SHEDAD effectively\nisolates and identifies two distinct categories of anomalies: supply\ntemperatures and substation performance. We identified 30 anomalous substations\nand reached a sensitivity of approximately 65\\% and specificity of\napproximately 97\\%. By focusing on this subset of poor-performing substations\nin the network, SHEDAD enables more targeted and effective maintenance\ninterventions, which can reduce energy usage while optimizing network\nperformance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, 5 figures, FMEC2024",
    "pdf_url": "http://arxiv.org/pdf/2408.14499v1",
    "published_date": "2024-08-23 19:25:41 UTC",
    "updated_date": "2024-08-23 19:25:41 UTC"
  },
  {
    "arxiv_id": "2408.13338v1",
    "title": "LalaEval: A Holistic Human Evaluation Framework for Domain-Specific Large Language Models",
    "authors": [
      "Chongyan Sun",
      "Ken Lin",
      "Shiwei Wang",
      "Hulong Wu",
      "Chengfei Fu",
      "Zhen Wang"
    ],
    "abstract": "This paper introduces LalaEval, a holistic framework designed for the human\nevaluation of domain-specific large language models (LLMs). LalaEval proposes a\ncomprehensive suite of end-to-end protocols that cover five main components\nincluding domain specification, criteria establishment, benchmark dataset\ncreation, construction of evaluation rubrics, and thorough analysis and\ninterpretation of evaluation outcomes. This initiative aims to fill a crucial\nresearch gap by providing a systematic methodology for conducting standardized\nhuman evaluations within specific domains, a practice that, despite its\nwidespread application, lacks substantial coverage in the literature and human\nevaluation are often criticized to be less reliable due to subjective factors,\nso standardized procedures adapted to the nuanced requirements of specific\ndomains or even individual organizations are in great need. Furthermore, the\npaper demonstrates the framework's application within the logistics industry,\npresenting domain-specific evaluation benchmarks, datasets, and a comparative\nanalysis of LLMs for the logistics domain use, highlighting the framework's\ncapacity to elucidate performance differences and guide model selection and\ndevelopment for domain-specific LLMs. Through real-world deployment, the paper\nunderscores the framework's effectiveness in advancing the field of\ndomain-specific LLM evaluation, thereby contributing significantly to the\nongoing discussion on LLMs' practical utility and performance in\ndomain-specific applications.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.13338v1",
    "published_date": "2024-08-23 19:12:45 UTC",
    "updated_date": "2024-08-23 19:12:45 UTC"
  },
  {
    "arxiv_id": "2408.13333v1",
    "title": "Mastering the Digital Art of War: Developing Intelligent Combat Simulation Agents for Wargaming Using Hierarchical Reinforcement Learning",
    "authors": [
      "Scotty Black"
    ],
    "abstract": "In today's rapidly evolving military landscape, advancing artificial\nintelligence (AI) in support of wargaming becomes essential. Despite\nreinforcement learning (RL) showing promise for developing intelligent agents,\nconventional RL faces limitations in handling the complexity inherent in combat\nsimulations. This dissertation proposes a comprehensive approach, including\ntargeted observation abstractions, multi-model integration, a hybrid AI\nframework, and an overarching hierarchical reinforcement learning (HRL)\nframework. Our localized observation abstraction using piecewise linear spatial\ndecay simplifies the RL problem, enhancing computational efficiency and\ndemonstrating superior efficacy over traditional global observation methods.\nOur multi-model framework combines various AI methodologies, optimizing\nperformance while still enabling the use of diverse, specialized individual\nbehavior models. Our hybrid AI framework synergizes RL with scripted agents,\nleveraging RL for high-level decisions and scripted agents for lower-level\ntasks, enhancing adaptability, reliability, and performance. Our HRL\narchitecture and training framework decomposes complex problems into manageable\nsubproblems, aligning with military decision-making structures. Although\ninitial tests did not show improved performance, insights were gained to\nimprove future iterations. This study underscores AI's potential to\nrevolutionize wargaming, emphasizing the need for continued research in this\ndomain.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.13333v1",
    "published_date": "2024-08-23 18:50:57 UTC",
    "updated_date": "2024-08-23 18:50:57 UTC"
  },
  {
    "arxiv_id": "2409.02938v1",
    "title": "CortexCompile: Harnessing Cortical-Inspired Architectures for Enhanced Multi-Agent NLP Code Synthesis",
    "authors": [
      "Gautham Ramachandran",
      "Rick Yang"
    ],
    "abstract": "Current approaches to automated code generation often rely on monolithic\nmodels that lack real-time adaptability and scalability. This limitation is\nparticularly evident in complex programming tasks that require dynamic\nadjustment and efficiency. The integration of neuroscience principles into\nNatural Language Processing (NLP) has the potential to revolutionize automated\ncode generation. This paper presents CortexCompile, a novel modular system\ninspired by the specialized functions of the human brain's cortical regions. By\nemulating the distinct roles of the Prefrontal Cortex, Parietal Cortex,\nTemporal Lobe, and Motor Cortex, CortexCompile achieves significant\nadvancements in scalability, efficiency, and adaptability compared to\ntraditional monolithic models like GPT-4o. The system's architecture features a\nTask Orchestration Agent that manages dynamic task delegation and parallel\nprocessing, facilitating the generation of highly accurate and optimized code\nacross increasingly complex programming tasks. Experimental evaluations\ndemonstrate that CortexCompile consistently outperforms GPT-4o in development\ntime, accuracy, and user satisfaction, particularly in tasks involving\nreal-time strategy games and first-person shooters. These findings underscore\nthe viability of neuroscience-inspired architectures in addressing the\nlimitations of current NLP models, paving the way for more efficient and\nhuman-like AI systems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2.2; I.2.7"
    ],
    "primary_category": "cs.LG",
    "comment": "17 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.02938v1",
    "published_date": "2024-08-23 18:36:20 UTC",
    "updated_date": "2024-08-23 18:36:20 UTC"
  },
  {
    "arxiv_id": "2408.13328v1",
    "title": "Localized Observation Abstraction Using Piecewise Linear Spatial Decay for Reinforcement Learning in Combat Simulations",
    "authors": [
      "Scotty Black",
      "Christian Darken"
    ],
    "abstract": "In the domain of combat simulations, the training and deployment of deep\nreinforcement learning (RL) agents still face substantial challenges due to the\ndynamic and intricate nature of such environments. Unfortunately, as the\ncomplexity of the scenarios and available information increases, the training\ntime required to achieve a certain threshold of performance does not just\nincrease, but often does so exponentially. This relationship underscores the\nprofound impact of complexity in training RL agents. This paper introduces a\nnovel approach that addresses this limitation in training artificial\nintelligence (AI) agents using RL. Traditional RL methods have been shown to\nstruggle in these high-dimensional, dynamic environments due to real-world\ncomputational constraints and the known sample inefficiency challenges of RL.\nTo overcome these limitations, we propose a method of localized observation\nabstraction using piecewise linear spatial decay. This technique simplifies the\nstate space, reducing computational demands while still preserving essential\ninformation, thereby enhancing AI training efficiency in dynamic environments\nwhere spatial relationships are often critical. Our analysis reveals that this\nlocalized observation approach consistently outperforms the more traditional\nglobal observation approach across increasing scenario complexity levels. This\npaper advances the research on observation abstractions for RL, illustrating\nhow localized observation with piecewise linear spatial decay can provide an\neffective solution to large state representation challenges in dynamic\nenvironments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.13328v1",
    "published_date": "2024-08-23 18:26:10 UTC",
    "updated_date": "2024-08-23 18:26:10 UTC"
  },
  {
    "arxiv_id": "2408.13256v2",
    "title": "How Diffusion Models Learn to Factorize and Compose",
    "authors": [
      "Qiyao Liang",
      "Ziming Liu",
      "Mitchell Ostrow",
      "Ila Fiete"
    ],
    "abstract": "Diffusion models are capable of generating photo-realistic images that\ncombine elements which likely do not appear together in the training set,\ndemonstrating the ability to \\textit{compositionally generalize}. Nonetheless,\nthe precise mechanism of compositionality and how it is acquired through\ntraining remains elusive. Inspired by cognitive neuroscientific approaches, we\nconsider a highly reduced setting to examine whether and when diffusion models\nlearn semantically meaningful and factorized representations of composable\nfeatures. We performed extensive controlled experiments on conditional\nDenoising Diffusion Probabilistic Models (DDPMs) trained to generate various\nforms of 2D Gaussian bump images. We found that the models learn factorized but\nnot fully continuous manifold representations for encoding continuous features\nof variation underlying the data. With such representations, models demonstrate\nsuperior feature compositionality but limited ability to interpolate over\nunseen values of a given feature. Our experimental results further demonstrate\nthat diffusion models can attain compositionality with few compositional\nexamples, suggesting a more efficient way to train DDPMs. Finally, we connect\nmanifold formation in diffusion models to percolation theory in physics,\noffering insight into the sudden onset of factorized representation learning.\nOur thorough toy experiments thus contribute a deeper understanding of how\ndiffusion models capture compositional structure in data.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "11 pages, 6 figures, plus appendix, some content overlap with\n  arXiv:2402.03305",
    "pdf_url": "http://arxiv.org/pdf/2408.13256v2",
    "published_date": "2024-08-23 17:59:03 UTC",
    "updated_date": "2024-10-11 03:29:14 UTC"
  },
  {
    "arxiv_id": "2408.13255v1",
    "title": "Ensemble Modeling of Multiple Physical Indicators to Dynamically Phenotype Autism Spectrum Disorder",
    "authors": [
      "Marie Huynh",
      "Aaron Kline",
      "Saimourya Surabhi",
      "Kaitlyn Dunlap",
      "Onur Cezmi Mutlu",
      "Mohammadmahdi Honarmand",
      "Parnian Azizian",
      "Peter Washington",
      "Dennis P. Wall"
    ],
    "abstract": "Early detection of autism, a neurodevelopmental disorder marked by social\ncommunication challenges, is crucial for timely intervention. Recent\nadvancements have utilized naturalistic home videos captured via the mobile\napplication GuessWhat. Through interactive games played between children and\ntheir guardians, GuessWhat has amassed over 3,000 structured videos from 382\nchildren, both diagnosed with and without Autism Spectrum Disorder (ASD). This\ncollection provides a robust dataset for training computer vision models to\ndetect ASD-related phenotypic markers, including variations in emotional\nexpression, eye contact, and head movements. We have developed a protocol to\ncurate high-quality videos from this dataset, forming a comprehensive training\nset. Utilizing this set, we trained individual LSTM-based models using eye\ngaze, head positions, and facial landmarks as input features, achieving test\nAUCs of 86%, 67%, and 78%, respectively. To boost diagnostic accuracy, we\napplied late fusion techniques to create ensemble models, improving the overall\nAUC to 90%. This approach also yielded more equitable results across different\ngenders and age groups. Our methodology offers a significant step forward in\nthe early detection of ASD by potentially reducing the reliance on subjective\nassessments and making early identification more accessibly and equitable.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.13255v1",
    "published_date": "2024-08-23 17:55:58 UTC",
    "updated_date": "2024-08-23 17:55:58 UTC"
  },
  {
    "arxiv_id": "2408.13248v1",
    "title": "Foundational Model for Electron Micrograph Analysis: Instruction-Tuning Small-Scale Language-and-Vision Assistant for Enterprise Adoption",
    "authors": [
      "Sakhinana Sagar Srinivas",
      "Chidaksh Ravuru",
      "Geethan Sannidhi",
      "Venkataramana Runkana"
    ],
    "abstract": "Semiconductor imaging and analysis are critical yet understudied in deep\nlearning, limiting our ability for precise control and optimization in\nsemiconductor manufacturing. We introduce a small-scale multimodal framework\nfor analyzing semiconductor electron microscopy images (MAEMI) through\nvision-language instruction tuning. We generate a customized\ninstruction-following dataset using large multimodal models on microscopic\nimage analysis. We perform knowledge transfer from larger to smaller models\nthrough knowledge distillation, resulting in improved accuracy of smaller\nmodels on visual question answering (VQA) tasks. This approach eliminates the\nneed for expensive, human expert-annotated datasets for microscopic image\nanalysis tasks. Enterprises can further finetune MAEMI on their intellectual\ndata, enhancing privacy and performance on low-cost consumer hardware. Our\nexperiments show that MAEMI outperforms traditional methods, adapts to data\ndistribution shifts, and supports high-throughput screening.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Our paper is published at ICML 2024 Workshop ML for Life and Material\n  Science: From Theory to Industry Applications, Vienna, Austria",
    "pdf_url": "http://arxiv.org/pdf/2408.13248v1",
    "published_date": "2024-08-23 17:42:11 UTC",
    "updated_date": "2024-08-23 17:42:11 UTC"
  },
  {
    "arxiv_id": "2408.13247v1",
    "title": "Data Exposure from LLM Apps: An In-depth Investigation of OpenAI's GPTs",
    "authors": [
      "Evin Jaff",
      "Yuhao Wu",
      "Ning Zhang",
      "Umar Iqbal"
    ],
    "abstract": "LLM app ecosystems are quickly maturing and supporting a wide range of use\ncases, which requires them to collect excessive user data. Given that the LLM\napps are developed by third-parties and that anecdotal evidence suggests LLM\nplatforms currently do not strictly enforce their policies, user data shared\nwith arbitrary third-parties poses a significant privacy risk. In this paper we\naim to bring transparency in data practices of LLM apps. As a case study, we\nstudy OpenAI's GPT app ecosystem. We develop an LLM-based framework to conduct\nthe static analysis of natural language-based source code of GPTs and their\nActions (external services) to characterize their data collection practices.\nOur findings indicate that Actions collect expansive data about users,\nincluding sensitive information prohibited by OpenAI, such as passwords. We\nfind that some Actions, including related to advertising and analytics, are\nembedded in multiple GPTs, which allow them to track user activities across\nGPTs. Additionally, co-occurrence of Actions exposes as much as 9.5x more data\nto them, than it is exposed to individual Actions. Lastly, we develop an\nLLM-based privacy policy analysis framework to automatically check the\nconsistency of data collection by Actions with disclosures in their privacy\npolicies. Our measurements indicate that the disclosures for most of the\ncollected data types are omitted in privacy policies, with only 5.8% of Actions\nclearly disclosing their data collection practices.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.13247v1",
    "published_date": "2024-08-23 17:42:06 UTC",
    "updated_date": "2024-08-23 17:42:06 UTC"
  },
  {
    "arxiv_id": "2408.13237v1",
    "title": "JacNet: Learning Functions with Structured Jacobians",
    "authors": [
      "Jonathan Lorraine",
      "Safwan Hossain"
    ],
    "abstract": "Neural networks are trained to learn an approximate mapping from an input\ndomain to a target domain. Incorporating prior knowledge about true mappings is\ncritical to learning a useful approximation. With current architectures, it is\nchallenging to enforce structure on the derivatives of the input-output\nmapping. We propose to use a neural network to directly learn the Jacobian of\nthe input-output function, which allows easy control of the derivative. We\nfocus on structuring the derivative to allow invertibility and also demonstrate\nthat other useful priors, such as $k$-Lipschitz, can be enforced. Using this\napproach, we can learn approximations to simple functions that are guaranteed\nto be invertible and easily compute the inverse. We also show similar results\nfor 1-Lipschitz functions.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML",
      "68T07",
      "I.2.6; G.1.0; I.5.1"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages, 3 Figures, ICML 2019 INNF Workshop",
    "pdf_url": "http://arxiv.org/pdf/2408.13237v1",
    "published_date": "2024-08-23 17:21:44 UTC",
    "updated_date": "2024-08-23 17:21:44 UTC"
  },
  {
    "arxiv_id": "2408.13233v2",
    "title": "Multi-Layer Transformers Gradient Can be Approximated in Almost Linear Time",
    "authors": [
      "Yingyu Liang",
      "Zhizhou Sha",
      "Zhenmei Shi",
      "Zhao Song",
      "Yufa Zhou"
    ],
    "abstract": "The computational complexity of the self-attention mechanism in popular\ntransformer architectures poses significant challenges for training and\ninference, and becomes the bottleneck for long inputs. Is it possible to\nsignificantly reduce the quadratic time complexity of computing the gradients\nin multi-layer transformer models? This paper proves that a novel fast\napproximation method can calculate the gradients in almost linear time\n$n^{1+o(1)}$ where $n$ is the input sequence length, while it maintains a\npolynomially small approximation error $1 / \\mathrm{poly}(n)$ across the entire\nmodel. Our theory holds for general loss functions and when the multi-layer\ntransformer model contains many practical sub-modules, such as residual\nconnection, casual mask, and multi-head attention. By improving the efficiency\nof gradient computation, we hope that this work will facilitate more effective\ntraining and deployment of long-context language models based on our\ntheoretical results.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.13233v2",
    "published_date": "2024-08-23 17:16:43 UTC",
    "updated_date": "2024-10-15 04:11:19 UTC"
  },
  {
    "arxiv_id": "2408.13227v1",
    "title": "Enhancing Few-Shot Transfer Learning with Optimized Multi-Task Prompt Tuning through Modular Prompt Composition",
    "authors": [
      "Ahmad Pouramini",
      "Hesham Faili"
    ],
    "abstract": "In recent years, multi-task prompt tuning has garnered considerable attention\nfor its inherent modularity and potential to enhance parameter-efficient\ntransfer learning across diverse tasks. This paper aims to analyze and improve\nthe performance of multiple tasks by facilitating the transfer of knowledge\nbetween their corresponding prompts in a multi-task setting. Our proposed\napproach decomposes the prompt for each target task into a combination of\nshared prompts (source prompts) and a task-specific prompt (private prompt).\nDuring training, the source prompts undergo fine-tuning and are integrated with\nthe private prompt to drive the target prompt for each task. We present and\ncompare multiple methods for combining source prompts to construct the target\nprompt, analyzing the roles of both source and private prompts within each\nmethod. We investigate their contributions to task performance and offer\nflexible, adjustable configurations based on these insights to optimize\nperformance. Our empirical findings clearly showcase improvements in accuracy\nand robustness compared to the conventional practice of prompt tuning and\nrelated works. Notably, our results substantially outperform other methods in\nthe field in few-shot settings, demonstrating superior performance in various\ntasks across GLUE benchmark, among other tasks. This achievement is attained\nwith a significantly reduced amount of training data, making our method a\npromising one for few-shot settings.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.13227v1",
    "published_date": "2024-08-23 17:01:51 UTC",
    "updated_date": "2024-08-23 17:01:51 UTC"
  },
  {
    "arxiv_id": "2408.13217v1",
    "title": "HBIC: A Biclustering Algorithm for Heterogeneous Datasets",
    "authors": [
      "Adán José-García",
      "Julie Jacques",
      "Clément Chauvet",
      "Vincent Sobanski",
      "Clarisse Dhaenens"
    ],
    "abstract": "Biclustering is an unsupervised machine-learning approach aiming to cluster\nrows and columns simultaneously in a data matrix. Several biclustering\nalgorithms have been proposed for handling numeric datasets. However,\nreal-world data mining problems often involve heterogeneous datasets with mixed\nattributes. To address this challenge, we introduce a biclustering approach\ncalled HBIC, capable of discovering meaningful biclusters in complex\nheterogeneous data, including numeric, binary, and categorical data. The\napproach comprises two stages: bicluster generation and bicluster model\nselection. In the initial stage, several candidate biclusters are generated\niteratively by adding and removing rows and columns based on the frequency of\nvalues in the original matrix. In the second stage, we introduce two approaches\nfor selecting the most suitable biclusters by considering their size and\nhomogeneity. Through a series of experiments, we investigated the suitability\nof our approach on a synthetic benchmark and in a biomedical application\ninvolving clinical data of systemic sclerosis patients. The evaluation\ncomparing our method to existing approaches demonstrates its ability to\ndiscover high-quality biclusters from heterogeneous data. Our biclustering\napproach is a starting point for heterogeneous bicluster discovery, leading to\na better understanding of complex underlying data structures.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.13217v1",
    "published_date": "2024-08-23 16:48:10 UTC",
    "updated_date": "2024-08-23 16:48:10 UTC"
  },
  {
    "arxiv_id": "2408.13214v1",
    "title": "EUR-USD Exchange Rate Forecasting Based on Information Fusion with Large Language Models and Deep Learning Methods",
    "authors": [
      "Hongcheng Ding",
      "Xuanze Zhao",
      "Zixiao Jiang",
      "Shamsul Nahar Abdullah",
      "Deshinta Arrova Dewi"
    ],
    "abstract": "Accurate forecasting of the EUR/USD exchange rate is crucial for investors,\nbusinesses, and policymakers. This paper proposes a novel framework, IUS, that\nintegrates unstructured textual data from news and analysis with structured\ndata on exchange rates and financial indicators to enhance exchange rate\nprediction. The IUS framework employs large language models for sentiment\npolarity scoring and exchange rate movement classification of texts. These\ntextual features are combined with quantitative features and input into a\nCausality-Driven Feature Generator. An Optuna-optimized Bi-LSTM model is then\nused to forecast the EUR/USD exchange rate. Experiments demonstrate that the\nproposed method outperforms benchmark models, reducing MAE by 10.69% and RMSE\nby 9.56% compared to the best performing baseline. Results also show the\nbenefits of data fusion, with the combination of unstructured and structured\ndata yielding higher accuracy than structured data alone. Furthermore, feature\nselection using the top 12 important quantitative features combined with the\ntextual features proves most effective. The proposed IUS framework and\nOptuna-Bi-LSTM model provide a powerful new approach for exchange rate\nforecasting through multi-source data integration.",
    "categories": [
      "q-fin.CP",
      "cs.AI",
      "cs.CE",
      "cs.CL"
    ],
    "primary_category": "q-fin.CP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.13214v1",
    "published_date": "2024-08-23 16:46:36 UTC",
    "updated_date": "2024-08-23 16:46:36 UTC"
  },
  {
    "arxiv_id": "2408.13211v1",
    "title": "Optimal Quantum Circuit Design via Unitary Neural Networks",
    "authors": [
      "M. Zomorodi",
      "H. Amini",
      "M. Abbaszadeh",
      "J. Sohrabi",
      "V. Salari",
      "P. Plawiak"
    ],
    "abstract": "The process of translating a quantum algorithm into a form suitable for\nimplementation on a quantum computing platform is crucial but yet challenging.\nThis entails specifying quantum operations with precision, a typically\nintricate task. In this paper, we present an alternative approach: an automated\nmethod for synthesizing the functionality of a quantum algorithm into a quantum\ncircuit model representation. Our methodology involves training a neural\nnetwork model using diverse input-output mappings of the quantum algorithm. We\ndemonstrate that this trained model can effectively generate a quantum circuit\nmodel equivalent to the original algorithm. Remarkably, our observations\nindicate that the trained model achieves near-perfect mapping of unseen inputs\nto their respective outputs.",
    "categories": [
      "quant-ph",
      "cs.AI"
    ],
    "primary_category": "quant-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.13211v1",
    "published_date": "2024-08-23 16:41:15 UTC",
    "updated_date": "2024-08-23 16:41:15 UTC"
  },
  {
    "arxiv_id": "2408.13208v1",
    "title": "Temporal Fairness in Decision Making Problems",
    "authors": [
      "Manuel R. Torres",
      "Parisa Zehtabi",
      "Michael Cashmore",
      "Daniele Magazzeni",
      "Manuela Veloso"
    ],
    "abstract": "In this work we consider a new interpretation of fairness in decision making\nproblems. Building upon existing fairness formulations, we focus on how to\nreason over fairness from a temporal perspective, taking into account the\nfairness of a history of past decisions. After introducing the concept of\ntemporal fairness, we propose three approaches that incorporate temporal\nfairness in decision making problems formulated as optimization problems. We\npresent a qualitative evaluation of our approach in four different domains and\ncompare the solutions against a baseline approach that does not consider the\ntemporal aspect of fairness.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Paper accepted at ECAI 2024. This is an extended version that\n  includes Supplementary Material",
    "pdf_url": "http://arxiv.org/pdf/2408.13208v1",
    "published_date": "2024-08-23 16:36:58 UTC",
    "updated_date": "2024-08-23 16:36:58 UTC"
  },
  {
    "arxiv_id": "2408.13204v1",
    "title": "DOMAINEVAL: An Auto-Constructed Benchmark for Multi-Domain Code Generation",
    "authors": [
      "Qiming Zhu",
      "Jialun Cao",
      "Yaojie Lu",
      "Hongyu Lin",
      "Xianpei Han",
      "Le Sun",
      "Shing-Chi Cheung"
    ],
    "abstract": "Code benchmarks such as HumanEval are widely adopted to evaluate the\ncapabilities of Large Language Models (LLMs), providing insights into their\nstrengths and weaknesses. However, current benchmarks primarily exercise LLMs'\ncapability on common coding tasks (e.g., bubble sort, greatest common divisor),\nleaving domain-specific coding tasks (e.g., computation, system, cryptography)\nunexplored. To fill this gap, we propose a multi-domain code benchmark,\nDOMAINEVAL, designed to evaluate LLMs' coding capabilities thoroughly. Our\npipeline works in a fully automated manner, enabling a push-bottom construction\nfrom code repositories into formatted subjects under study. Interesting\nfindings are observed by evaluating 12 representative LLMs against DOMAINEVAL.\nWe notice that LLMs are generally good at computation tasks while falling short\non cryptography and system coding tasks. The performance gap can be as much as\n68.94% (80.94% - 12.0%) in some LLMs. We also observe that generating more\nsamples can increase the overall performance of LLMs, while the domain bias may\neven increase. The contributions of this study include a code generation\nbenchmark dataset DOMAINEVAL, encompassing six popular domains, a fully\nautomated pipeline for constructing code benchmarks, and an identification of\nthe limitations of LLMs in code generation tasks based on their performance on\nDOMAINEVAL, providing directions for future research improvements. The\nleaderboard is available at https://domaineval.github.io/.",
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.13204v1",
    "published_date": "2024-08-23 16:33:58 UTC",
    "updated_date": "2024-08-23 16:33:58 UTC"
  },
  {
    "arxiv_id": "2408.14496v3",
    "title": "A New Era in Computational Pathology: A Survey on Foundation and Vision-Language Models",
    "authors": [
      "Dibaloke Chanda",
      "Milan Aryal",
      "Nasim Yahya Soltani",
      "Masoud Ganji"
    ],
    "abstract": "Recent advances in deep learning have completely transformed the domain of\ncomputational pathology (CPath). More specifically, it has altered the\ndiagnostic workflow of pathologists by integrating foundation models (FMs) and\nvision-language models (VLMs) in their assessment and decision-making process.\nThe limitations of existing deep learning approaches in CPath can be overcome\nby FMs through learning a representation space that can be adapted to a wide\nvariety of downstream tasks without explicit supervision. Deploying VLMs allow\npathology reports written in natural language be used as rich semantic\ninformation sources to improve existing models as well as generate predictions\nin natural language form. In this survey, a holistic and systematic overview of\nrecent innovations in FMs and VLMs in CPath is presented. Furthermore, the\ntools, datasets and training schemes for these models are summarized in\naddition to categorizing them into distinct groups. This extensive survey\nhighlights the current trends in CPath and its possible revolution through the\nuse of FMs and VLMs in the future.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "eess.IV"
    ],
    "primary_category": "cs.LG",
    "comment": "20 pages, 19 figures and 9 tables",
    "pdf_url": "http://arxiv.org/pdf/2408.14496v3",
    "published_date": "2024-08-23 16:33:57 UTC",
    "updated_date": "2024-09-18 15:25:49 UTC"
  },
  {
    "arxiv_id": "2408.13202v1",
    "title": "Instruct-DeBERTa: A Hybrid Approach for Aspect-based Sentiment Analysis on Textual Reviews",
    "authors": [
      "Dineth Jayakody",
      "A V A Malkith",
      "Koshila Isuranda",
      "Vishal Thenuwara",
      "Nisansa de Silva",
      "Sachintha Rajith Ponnamperuma",
      "G G N Sandamali",
      "K L K Sudheera"
    ],
    "abstract": "Aspect-based Sentiment Analysis (ABSA) is a critical task in Natural Language\nProcessing (NLP) that focuses on extracting sentiments related to specific\naspects within a text, offering deep insights into customer opinions.\nTraditional sentiment analysis methods, while useful for determining overall\nsentiment, often miss the implicit opinions about particular product or service\nfeatures. This paper presents a comprehensive review of the evolution of ABSA\nmethodologies, from lexicon-based approaches to machine learning and deep\nlearning techniques. We emphasize the recent advancements in Transformer-based\nmodels, particularly Bidirectional Encoder Representations from Transformers\n(BERT) and its variants, which have set new benchmarks in ABSA tasks. We\nfocused on finetuning Llama and Mistral models, building hybrid models using\nthe SetFit framework, and developing our own model by exploiting the strengths\nof state-of-the-art (SOTA) Transformer-based models for aspect term extraction\n(ATE) and aspect sentiment classification (ASC). Our hybrid model Instruct -\nDeBERTa uses SOTA InstructABSA for aspect extraction and DeBERTa-V3-baseabsa-V1\nfor aspect sentiment classification. We utilize datasets from different domains\nto evaluate our model's performance. Our experiments indicate that the proposed\nhybrid model significantly improves the accuracy and reliability of sentiment\nanalysis across all experimented domains. As per our findings, our hybrid model\nInstruct - DeBERTa is the best-performing model for the joint task of ATE and\nASC for both SemEval restaurant 2014 and SemEval laptop 2014 datasets\nseparately. By addressing the limitations of existing methodologies, our\napproach provides a robust solution for understanding detailed consumer\nfeedback, thus offering valuable insights for businesses aiming to enhance\ncustomer satisfaction and product development.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.13202v1",
    "published_date": "2024-08-23 16:31:07 UTC",
    "updated_date": "2024-08-23 16:31:07 UTC"
  },
  {
    "arxiv_id": "2408.13297v1",
    "title": "An Overview and Comparison of Axiomatization Structures Regarding Inconsistency Indices' Properties in Pairwise Comparisons Methods",
    "authors": [
      "Sangeeta Pant",
      "Anuj Kumar",
      "Jiří Mazurek"
    ],
    "abstract": "Mathematical analysis of the analytic hierarchy process (AHP) led to the\ndevelopment of a mathematical function, usually called the inconsistency index,\nwhich has the center role in measuring the inconsistency of the judgements in\nAHP. Inconsistency index is a mathematical function which maps every pairwise\ncomparison matrix (PCM) into a real number. An inconsistency index can be\nconsidered more trustworthy when it satisfies a set of suitable properties.\nTherefore, the research community has been trying to postulate a set of\ndesirable rules (axioms, properties) for inconsistency indices. Subsequently,\nmany axiomatic frameworks for these functions have been suggested\nindependently, however, the literature on the topic is fragmented and missing a\nbroader framework. Therefore, the objective of this article is twofold.\nFirstly, we provide a comprehensive review of the advancements in the\naxiomatization of inconsistency indices' properties during the last decade.\nSecondly, we provide a comparison and discussion of the aforementioned\naxiomatic structures along with directions of the future research.",
    "categories": [
      "cs.LO",
      "cs.AI"
    ],
    "primary_category": "cs.LO",
    "comment": "21 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.13297v1",
    "published_date": "2024-08-23 16:20:09 UTC",
    "updated_date": "2024-08-23 16:20:09 UTC"
  },
  {
    "arxiv_id": "2408.13189v1",
    "title": "Accelerating the k-means++ Algorithm by Using Geometric Information",
    "authors": [
      "Guillem Rodríguez Corominas",
      "Maria J. Blesa",
      "Christian Blum"
    ],
    "abstract": "In this paper, we propose an acceleration of the exact k-means++ algorithm\nusing geometric information, specifically the Triangle Inequality and\nadditional norm filters, along with a two-step sampling procedure. Our\nexperiments demonstrate that the accelerated version outperforms the standard\nk-means++ version in terms of the number of visited points and distance\ncalculations, achieving greater speedup as the number of clusters increases.\nThe version utilizing the Triangle Inequality is particularly effective for\nlow-dimensional data, while the additional norm-based filter enhances\nperformance in high-dimensional instances with greater norm variance among\npoints. Additional experiments show the behavior of our algorithms when\nexecuted concurrently across multiple jobs and examine how memory performance\nimpacts practical speedup.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "91C20"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.13189v1",
    "published_date": "2024-08-23 16:15:47 UTC",
    "updated_date": "2024-08-23 16:15:47 UTC"
  },
  {
    "arxiv_id": "2408.13161v2",
    "title": "Say No to Freeloader: Protecting Intellectual Property of Your Deep Model",
    "authors": [
      "Lianyu Wang",
      "Meng Wang",
      "Huazhu Fu",
      "Daoqiang Zhang"
    ],
    "abstract": "Model intellectual property (IP) protection has attracted growing attention\nas science and technology advancements stem from human intellectual labor and\ncomputational expenses. Ensuring IP safety for trainers and owners is of utmost\nimportance, particularly in domains where ownership verification and\napplicability authorization are required. A notable approach to safeguarding\nmodel IP involves proactively preventing the use of well-trained models of\nauthorized domains from unauthorized domains. In this paper, we introduce a\nnovel Compact Un-transferable Pyramid Isolation Domain (CUPI-Domain) which\nserves as a barrier against illegal transfers from authorized to unauthorized\ndomains. Drawing inspiration from human transitive inference and learning\nabilities, the CUPI-Domain is designed to obstruct cross-domain transfers by\nemphasizing the distinctive style features of the authorized domain. This\nemphasis leads to failure in recognizing irrelevant private style features on\nunauthorized domains. To this end, we propose novel CUPI-Domain generators,\nwhich select features from both authorized and CUPI-Domain as anchors. Then, we\nfuse the style features and semantic features of these anchors to generate\nlabeled and style-rich CUPI-Domain. Additionally, we design external\nDomain-Information Memory Banks (DIMB) for storing and updating labeled pyramid\nfeatures to obtain stable domain class features and domain class-wise style\nfeatures. Based on the proposed whole method, the novel style and\ndiscriminative loss functions are designed to effectively enhance the\ndistinction in style and discriminative features between authorized and\nunauthorized domains, respectively. Moreover, we provide two solutions for\nutilizing CUPI-Domain based on whether the unauthorized domain is known:\ntarget-specified CUPI-Domain and target-free CUPI-Domain.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.13161v2",
    "published_date": "2024-08-23 15:34:33 UTC",
    "updated_date": "2024-08-27 14:05:57 UTC"
  },
  {
    "arxiv_id": "2408.13155v1",
    "title": "Causal machine learning for sustainable agroecosystems",
    "authors": [
      "Vasileios Sitokonstantinou",
      "Emiliano Díaz Salas Porras",
      "Jordi Cerdà Bautista",
      "Maria Piles",
      "Ioannis Athanasiadis",
      "Hannah Kerner",
      "Giulia Martini",
      "Lily-belle Sweet",
      "Ilias Tsoumas",
      "Jakob Zscheischler",
      "Gustau Camps-Valls"
    ],
    "abstract": "In a changing climate, sustainable agriculture is essential for food security\nand environmental health. However, it is challenging to understand the complex\ninteractions among its biophysical, social, and economic components. Predictive\nmachine learning (ML), with its capacity to learn from data, is leveraged in\nsustainable agriculture for applications like yield prediction and weather\nforecasting. Nevertheless, it cannot explain causal mechanisms and remains\ndescriptive rather than prescriptive. To address this gap, we propose causal\nML, which merges ML's data processing with causality's ability to reason about\nchange. This facilitates quantifying intervention impacts for evidence-based\ndecision-making and enhances predictive model robustness. We showcase causal ML\nthrough eight diverse applications that benefit stakeholders across the\nagri-food chain, including farmers, policymakers, and researchers.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.13155v1",
    "published_date": "2024-08-23 15:25:50 UTC",
    "updated_date": "2024-08-23 15:25:50 UTC"
  },
  {
    "arxiv_id": "2408.13147v1",
    "title": "ShapeICP: Iterative Category-level Object Pose and Shape Estimation from Depth",
    "authors": [
      "Yihao Zhang",
      "John J. Leonard"
    ],
    "abstract": "Category-level object pose and shape estimation from a single depth image has\nrecently drawn research attention due to its wide applications in robotics and\nself-driving. The task is particularly challenging because the three unknowns,\nobject pose, object shape, and model-to-measurement correspondences, are\ncompounded together but only a single view of depth measurements is provided.\nThe vast majority of the prior work heavily relies on data-driven approaches to\nobtain solutions to at least one of the unknowns and typically two, running\nwith the risk of failing to generalize to unseen domains. The shape\nrepresentations used in the prior work also mainly focus on point cloud and\nsigned distance field (SDF). In stark contrast to the prior work, we approach\nthe problem using an iterative estimation method that does not require learning\nfrom any pose-annotated data. In addition, we adopt a novel mesh-based object\nactive shape model that has not been explored by the previous literature. Our\nalgorithm, named ShapeICP, has its foundation in the iterative closest point\n(ICP) algorithm but is equipped with additional features for the category-level\npose and shape estimation task. The results show that even without using any\npose-annotated data, ShapeICP surpasses many data-driven approaches that rely\non the pose data for training, opening up new solution space for researchers to\nconsider.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.13147v1",
    "published_date": "2024-08-23 15:12:55 UTC",
    "updated_date": "2024-08-23 15:12:55 UTC"
  },
  {
    "arxiv_id": "2408.13140v3",
    "title": "Verification of Geometric Robustness of Neural Networks via Piecewise Linear Approximation and Lipschitz Optimisation",
    "authors": [
      "Ben Batten",
      "Yang Zheng",
      "Alessandro De Palma",
      "Panagiotis Kouvaros",
      "Alessio Lomuscio"
    ],
    "abstract": "We address the problem of verifying neural networks against geometric\ntransformations of the input image, including rotation, scaling, shearing, and\ntranslation. The proposed method computes provably sound piecewise linear\nconstraints for the pixel values by using sampling and linear approximations in\ncombination with branch-and-bound Lipschitz optimisation. The method obtains\nprovably tighter over-approximations of the perturbation region than the\npresent state-of-the-art. We report results from experiments on a comprehensive\nset of verification benchmarks on MNIST and CIFAR10. We show that our proposed\nimplementation resolves up to 32% more verification cases than present\napproaches.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "ECAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.13140v3",
    "published_date": "2024-08-23 15:02:09 UTC",
    "updated_date": "2024-09-21 18:19:03 UTC"
  },
  {
    "arxiv_id": "2408.13135v1",
    "title": "Deep Learning at the Intersection: Certified Robustness as a Tool for 3D Vision",
    "authors": [
      "Gabriel Pérez S",
      "Juan C. Pérez",
      "Motasem Alfarra",
      "Jesús Zarzar",
      "Sara Rojas",
      "Bernard Ghanem",
      "Pablo Arbeláez"
    ],
    "abstract": "This paper presents preliminary work on a novel connection between certified\nrobustness in machine learning and the modeling of 3D objects. We highlight an\nintriguing link between the Maximal Certified Radius (MCR) of a classifier\nrepresenting a space's occupancy and the space's Signed Distance Function\n(SDF). Leveraging this relationship, we propose to use the certification method\nof randomized smoothing (RS) to compute SDFs. Since RS' high computational cost\nprevents its practical usage as a way to compute SDFs, we propose an algorithm\nto efficiently run RS in low-dimensional applications, such as 3D space, by\nexpressing RS' fundamental operations as Gaussian smoothing on pre-computed\nvoxel grids. Our approach offers an innovative and practical tool to compute\nSDFs, validated through proof-of-concept experiments in novel view synthesis.\nThis paper bridges two previously disparate areas of machine learning, opening\nnew avenues for further exploration and potential cross-domain advancements.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This paper is an accepted extended abstract to the LatinX workshop at\n  ICCV 2023. This was uploaded a year late",
    "pdf_url": "http://arxiv.org/pdf/2408.13135v1",
    "published_date": "2024-08-23 15:00:32 UTC",
    "updated_date": "2024-08-23 15:00:32 UTC"
  },
  {
    "arxiv_id": "2408.13131v2",
    "title": "DeTPP: Leveraging Object Detection for Robust Long-Horizon Event Prediction",
    "authors": [
      "Ivan Karpukhin",
      "Andrey Savchenko"
    ],
    "abstract": "Long-horizon event forecasting is critical across various domains, including\nretail, finance, healthcare, and social networks. Traditional methods, such as\nMarked Temporal Point Processes (MTPP), often rely on autoregressive models to\npredict multiple future events. However, these models frequently suffer from\nissues like converging to constant or repetitive outputs, which limits their\neffectiveness and general applicability. To address these challenges, we\nintroduce DeTPP (Detection-based Temporal Point Processes), a novel approach\ninspired by object detection techniques from computer vision. DeTPP employs a\nunique matching-based loss function that selectively prioritizes reliably\npredictable events, improving the accuracy and diversity of predictions during\ninference. Our method establishes a new state-of-the-art in long-horizon event\nforecasting, achieving up to a 77% relative improvement over existing MTPP and\nnext-K methods. The proposed hybrid approach enhances the accuracy of next\nevent prediction by up to 2.7% on a large transactional dataset. Notably, DeTPP\nis also among the fastest methods for inference. The implementation of DeTPP is\npublicly available on GitHub.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.13131v2",
    "published_date": "2024-08-23 14:57:46 UTC",
    "updated_date": "2024-10-02 13:21:50 UTC"
  },
  {
    "arxiv_id": "2408.13293v1",
    "title": "Causally-Aware Spatio-Temporal Multi-Graph Convolution Network for Accurate and Reliable Traffic Prediction",
    "authors": [
      "Pingping Dong",
      "Xiao-Lin Wang",
      "Indranil Bose",
      "Kam K. H. Ng",
      "Xiaoning Zhang",
      "Xiaoge Zhang"
    ],
    "abstract": "Accurate and reliable prediction has profound implications to a wide range of\napplications. In this study, we focus on an instance of spatio-temporal\nlearning problem--traffic prediction--to demonstrate an advanced deep learning\nmodel developed for making accurate and reliable forecast. Despite the\nsignificant progress in traffic prediction, limited studies have incorporated\nboth explicit and implicit traffic patterns simultaneously to improve\nprediction performance. Meanwhile, the variability nature of traffic states\nnecessitates quantifying the uncertainty of model predictions in a\nstatistically principled way; however, extant studies offer no provable\nguarantee on the statistical validity of confidence intervals in reflecting its\nactual likelihood of containing the ground truth. In this paper, we propose an\nend-to-end traffic prediction framework that leverages three primary components\nto generate accurate and reliable traffic predictions: dynamic causal structure\nlearning for discovering implicit traffic patterns from massive traffic data,\ncausally-aware spatio-temporal multi-graph convolution network (CASTMGCN) for\nlearning spatio-temporal dependencies, and conformal prediction for uncertainty\nquantification. CASTMGCN fuses several graphs that characterize different\nimportant aspects of traffic networks and an auxiliary graph that captures the\neffect of exogenous factors on the road network. On this basis, a conformal\nprediction approach tailored to spatio-temporal data is further developed for\nquantifying the uncertainty in node-wise traffic predictions over varying\nprediction horizons. Experimental results on two real-world traffic datasets\ndemonstrate that the proposed method outperforms several state-of-the-art\nmodels in prediction accuracy; moreover, it generates more efficient prediction\nregions than other methods while strictly satisfying the statistical validity\nin coverage.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.13293v1",
    "published_date": "2024-08-23 14:35:54 UTC",
    "updated_date": "2024-08-23 14:35:54 UTC"
  },
  {
    "arxiv_id": "2408.13085v3",
    "title": "Map-Free Visual Relocalization Enhanced by Instance Knowledge and Depth Knowledge",
    "authors": [
      "Mingyu Xiao",
      "Runze Chen",
      "Haiyong Luo",
      "Fang Zhao",
      "Juan Wang",
      "Xuepeng Ma"
    ],
    "abstract": "Map-free relocalization technology is crucial for applications in autonomous\nnavigation and augmented reality, but relying on pre-built maps is often\nimpractical. It faces significant challenges due to limitations in matching\nmethods and the inherent lack of scale in monocular images. These issues lead\nto substantial rotational and metric errors and even localization failures in\nreal-world scenarios. Large matching errors significantly impact the overall\nrelocalization process, affecting both rotational and translational accuracy.\nDue to the inherent limitations of the camera itself, recovering the metric\nscale from a single image is crucial, as this significantly impacts the\ntranslation error. To address these challenges, we propose a map-free\nrelocalization method enhanced by instance knowledge and depth knowledge. By\nleveraging instance-based matching information to improve global matching\nresults, our method significantly reduces the possibility of mismatching across\ndifferent objects. The robustness of instance knowledge across the scene helps\nthe feature point matching model focus on relevant regions and enhance matching\naccuracy. Additionally, we use estimated metric depth from a single image to\nreduce metric errors and improve scale recovery accuracy. By integrating\nmethods dedicated to mitigating large translational and rotational errors, our\napproach demonstrates superior performance in map-free relocalization\ntechniques.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "17 pages,6 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.13085v3",
    "published_date": "2024-08-23 14:12:03 UTC",
    "updated_date": "2024-09-19 02:55:48 UTC"
  },
  {
    "arxiv_id": "2408.13084v1",
    "title": "Avatar Visual Similarity for Social HCI: Increasing Self-Awareness",
    "authors": [
      "Bernhard Hilpert",
      "Claudio Alves da Silva",
      "Leon Christidis",
      "Chirag Bhuvaneshwara",
      "Patrick Gebhard",
      "Fabrizio Nunnari",
      "Dimitra Tsovaltzi"
    ],
    "abstract": "Self-awareness is a critical factor in social human-human interaction and,\nhence, in social HCI interaction. Increasing self-awareness through mirrors or\nvideo recordings is common in face-to-face trainings, since it influences\nantecedents of self-awareness like explicit identification and implicit\naffective identification (affinity). However, increasing self-awareness has\nbeen scarcely examined in virtual trainings with virtual avatars, which allow\nfor adjusting the similarity, e.g. to avoid negative effects of\nself-consciousness. Automatic visual similarity in avatars is an open issue\nrelated to high costs. It is important to understand which features need to be\nmanipulated and which degree of similarity is necessary for self-awareness to\nleverage the added value of using avatars for self-awareness. This article\nexamines the relationship between avatar visual similarity and increasing\nself-awareness in virtual training environments. We define visual similarity\nbased on perceptually important facial features for human-human identification\nand develop a theory-based methodology to systematically manipulate visual\nsimilarity of virtual avatars and support self-awareness. Three personalized\nversions of virtual avatars with varying degrees of visual similarity to\nparticipants were created (weak, medium and strong facial features\nmanipulation). In a within-subject study (N=33), we tested effects of degree of\nsimilarity on perceived similarity, explicit identification and implicit\naffective identification (affinity). Results show significant differences\nbetween the weak similarity manipulation, and both the strong manipulation and\nthe random avatar for all three antecedents of self-awareness. An increasing\ndegree of avatar visual similarity influences antecedents of self-awareness in\nvirtual environments.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.13084v1",
    "published_date": "2024-08-23 14:11:35 UTC",
    "updated_date": "2024-08-23 14:11:35 UTC"
  },
  {
    "arxiv_id": "2408.13082v1",
    "title": "Multivariate Time-Series Anomaly Detection based on Enhancing Graph Attention Networks with Topological Analysis",
    "authors": [
      "Zhe Liu",
      "Xiang Huang",
      "Jingyun Zhang",
      "Zhifeng Hao",
      "Li Sun",
      "Hao Peng"
    ],
    "abstract": "Unsupervised anomaly detection in time series is essential in industrial\napplications, as it significantly reduces the need for manual intervention.\nMultivariate time series pose a complex challenge due to their feature and\ntemporal dimensions. Traditional methods use Graph Neural Networks (GNNs) or\nTransformers to analyze spatial while RNNs to model temporal dependencies.\nThese methods focus narrowly on one dimension or engage in coarse-grained\nfeature extraction, which can be inadequate for large datasets characterized by\nintricate relationships and dynamic changes. This paper introduces a novel\ntemporal model built on an enhanced Graph Attention Network (GAT) for\nmultivariate time series anomaly detection called TopoGDN. Our model analyzes\nboth time and feature dimensions from a fine-grained perspective. First, we\nintroduce a multi-scale temporal convolution module to extract detailed\ntemporal features. Additionally, we present an augmented GAT to manage complex\ninter-feature dependencies, which incorporates graph topology into node\nfeatures across multiple scales, a versatile, plug-and-play enhancement that\nsignificantly boosts the performance of GAT. Our experimental results confirm\nthat our approach surpasses the baseline models on four datasets, demonstrating\nits potential for widespread application in fields requiring robust anomaly\ndetection. The code is available at https://github.com/ljj-cyber/TopoGDN.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 5 figures, to be published in CIKM 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.13082v1",
    "published_date": "2024-08-23 14:06:30 UTC",
    "updated_date": "2024-08-23 14:06:30 UTC"
  },
  {
    "arxiv_id": "2408.13078v1",
    "title": "AEMLO: AutoEncoder-Guided Multi-Label Oversampling",
    "authors": [
      "Ao Zhou",
      "Bin Liu",
      "Jin Wang",
      "Kaiwei Sun",
      "Kelin Liu"
    ],
    "abstract": "Class imbalance significantly impacts the performance of multi-label\nclassifiers. Oversampling is one of the most popular approaches, as it augments\ninstances associated with less frequent labels to balance the class\ndistribution. Existing oversampling methods generate feature vectors of\nsynthetic samples through replication or linear interpolation and assign labels\nthrough neighborhood information. Linear interpolation typically generates new\nsamples between existing data points, which may result in insufficient\ndiversity of synthesized samples and further lead to the overfitting issue.\nDeep learning-based methods, such as AutoEncoders, have been proposed to\ngenerate more diverse and complex synthetic samples, achieving excellent\nperformance on imbalanced binary or multi-class datasets. In this study, we\nintroduce AEMLO, an AutoEncoder-guided Oversampling technique specifically\ndesigned for tackling imbalanced multi-label data. AEMLO is built upon two\nfundamental components. The first is an encoder-decoder architecture that\nenables the model to encode input data into a low-dimensional feature space,\nlearn its latent representations, and then reconstruct it back to its original\ndimension, thus applying to the generation of new data. The second is an\nobjective function tailored to optimize the sampling task for multi-label\nscenarios. We show that AEMLO outperforms the existing state-of-the-art methods\nwith extensive empirical studies.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.13078v1",
    "published_date": "2024-08-23 14:01:33 UTC",
    "updated_date": "2024-08-23 14:01:33 UTC"
  },
  {
    "arxiv_id": "2408.13074v3",
    "title": "Hierarchical Spatio-Temporal State-Space Modeling for fMRI Analysis",
    "authors": [
      "Yuxiang Wei",
      "Anees Abrol",
      "Vince Calhoun"
    ],
    "abstract": "Recent advances in deep learning structured state space models, especially\nthe Mamba architecture, have demonstrated remarkable performance improvements\nwhile maintaining linear complexity. In this study, we introduce functional\nspatiotemporal Mamba (FST-Mamba), a Mamba-based model designed for discovering\nneurological biomarkers using functional magnetic resonance imaging (fMRI). We\nfocus on dynamic functional network connectivity (dFNC) derived from fMRI and\npropose a hierarchical spatiotemporal Mamba-based network that processes\nspatial and temporal information separately using Mamba-based encoders.\nLeveraging the topological uniqueness of the FNC matrix, we introduce a\ncomponent-wise varied-scale aggregation (CVA) mechanism to aggregate\nconnectivity across individual components within brain networks, enabling the\nmodel to capture component-level and network-level information. Additionally,\nwe propose symmetric rotary position encoding (SymRope) to encode the relative\npositions of each functional connection while considering the symmetric nature\nof the FNC matrix. Experimental results demonstrate significant improvements in\nthe proposed FST-Mamba model on various brain-based classification and\nregression tasks. We further show brain connectivities and dynamics that are\ncrucial for the prediction. Our work reveals the substantial potential of\nattention-free sequence modeling in brain discovery. The codes are publicly\navailable here: https://github.com/yuxiangwei0808/FunctionalMamba/tree/main.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to RECOMB 2025",
    "pdf_url": "http://arxiv.org/pdf/2408.13074v3",
    "published_date": "2024-08-23 13:58:14 UTC",
    "updated_date": "2025-03-20 19:15:02 UTC"
  },
  {
    "arxiv_id": "2408.14494v1",
    "title": "Knowledge Graph Modeling-Driven Large Language Model Operating System (LLM OS) for Task Automation in Process Engineering Problem-Solving",
    "authors": [
      "Sakhinana Sagar Srinivas",
      "Vijay Sri Vaikunth",
      "Venkataramana Runkana"
    ],
    "abstract": "We present the Process Engineering Operations Assistant (PEOA), an AI-driven\nframework designed to solve complex problems in the chemical and process\nindustries. The framework employs a modular architecture orchestrated by a\nmeta-agent, which serves as the central coordinator, managing an action\ngenerator and instruction-tuned small-scale language models (expert models).\nThe action generator decomposes complex problems into sub-tasks and identifies\nsuitable expert models to execute each, delivering precise solutions for\nmulti-step problem-solving. Key techniques include advanced knowledge modeling\nusing property graphs for improved information retrieval, facilitating more\naccurate and contextually relevant solutions. Additionally, the framework\nutilizes a teacher-student transfer-learning approach with GPT-4 (Omni) to\nfine-tune the action generator and expert models for domain adaptation,\nalongside an iterative problem-solving mechanism with sophisticated error\nhandling. Custom datasets were developed to evaluate the framework against\nleading proprietary language models on various engineering tasks. The results\ndemonstrate the framework effectiveness in automating calculations,\naccelerating prototyping, and providing AI-augmented decision support for\nindustrial processes, marking a significant advancement in process engineering\ncapabilities.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted for Publication by Association for the Advancement of\n  Artificial Intelligence, Fall Symposium Series",
    "pdf_url": "http://arxiv.org/pdf/2408.14494v1",
    "published_date": "2024-08-23 13:52:47 UTC",
    "updated_date": "2024-08-23 13:52:47 UTC"
  },
  {
    "arxiv_id": "2408.13054v1",
    "title": "cc-DRL: a Convex Combined Deep Reinforcement Learning Flight Control Design for a Morphing Quadrotor",
    "authors": [
      "Tao Yang",
      "Huai-Ning Wu",
      "Jun-Wei Wang"
    ],
    "abstract": "In comparison to common quadrotors, the shape change of morphing quadrotors\nendows it with a more better flight performance but also results in more\ncomplex flight dynamics. Generally, it is extremely difficult or even\nimpossible for morphing quadrotors to establish an accurate mathematical model\ndescribing their complex flight dynamics. To figure out the issue of flight\ncontrol design for morphing quadrotors, this paper resorts to a combination of\nmodel-free control techniques (e.g., deep reinforcement learning, DRL) and\nconvex combination (CC) technique, and proposes a convex-combined-DRL (cc-DRL)\nflight control algorithm for position and attitude of a class of morphing\nquadrotors, where the shape change is realized by the length variation of four\narm rods. In the proposed cc-DRL flight control algorithm, proximal policy\noptimization algorithm that is a model-free DRL algorithm is utilized to\noff-line train the corresponding optimal flight control laws for some selected\nrepresentative arm length modes and hereby a cc-DRL flight control scheme is\nconstructed by the convex combination technique. Finally, simulation results\nare presented to show the effectiveness and merit of the proposed flight\ncontrol algorithm.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.13054v1",
    "published_date": "2024-08-23 13:25:04 UTC",
    "updated_date": "2024-08-23 13:25:04 UTC"
  },
  {
    "arxiv_id": "2408.13040v1",
    "title": "SpeechPrompt: Prompting Speech Language Models for Speech Processing Tasks",
    "authors": [
      "Kai-Wei Chang",
      "Haibin Wu",
      "Yu-Kai Wang",
      "Yuan-Kuei Wu",
      "Hua Shen",
      "Wei-Cheng Tseng",
      "Iu-thing Kang",
      "Shang-Wen Li",
      "Hung-yi Lee"
    ],
    "abstract": "Prompting has become a practical method for utilizing pre-trained language\nmodels (LMs). This approach offers several advantages. It allows an LM to adapt\nto new tasks with minimal training and parameter updates, thus achieving\nefficiency in both storage and computation. Additionally, prompting modifies\nonly the LM's inputs and harnesses the generative capabilities of language\nmodels to address various downstream tasks in a unified manner. This\nsignificantly reduces the need for human labor in designing task-specific\nmodels. These advantages become even more evident as the number of tasks served\nby the LM scales up. Motivated by the strengths of prompting, we are the first\nto explore the potential of prompting speech LMs in the domain of speech\nprocessing. Recently, there has been a growing interest in converting speech\ninto discrete units for language modeling. Our pioneer research demonstrates\nthat these quantized speech units are highly versatile within our unified\nprompting framework. Not only can they serve as class labels, but they also\ncontain rich phonetic information that can be re-synthesized back into speech\nsignals for speech generation tasks. Specifically, we reformulate speech\nprocessing tasks into speech-to-unit generation tasks. As a result, we can\nseamlessly integrate tasks such as speech classification, sequence generation,\nand speech generation within a single, unified prompting framework. The\nexperiment results show that the prompting method can achieve competitive\nperformance compared to the strong fine-tuning method based on self-supervised\nlearning models with a similar number of trainable parameters. The prompting\nmethod also shows promising results in the few-shot setting. Moreover, with the\nadvanced speech LMs coming into the stage, the proposed prompting framework\nattains great potential.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "eess.AS",
    "comment": "Published in IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing (TASLP)",
    "pdf_url": "http://arxiv.org/pdf/2408.13040v1",
    "published_date": "2024-08-23 13:00:10 UTC",
    "updated_date": "2024-08-23 13:00:10 UTC"
  },
  {
    "arxiv_id": "2408.13031v1",
    "title": "VFM-Det: Towards High-Performance Vehicle Detection via Large Foundation Models",
    "authors": [
      "Wentao Wu",
      "Fanghua Hong",
      "Xiao Wang",
      "Chenglong Li",
      "Jin Tang"
    ],
    "abstract": "Existing vehicle detectors are usually obtained by training a typical\ndetector (e.g., YOLO, RCNN, DETR series) on vehicle images based on a\npre-trained backbone (e.g., ResNet, ViT). Some researchers also exploit and\nenhance the detection performance using pre-trained large foundation models.\nHowever, we think these detectors may only get sub-optimal results because the\nlarge models they use are not specifically designed for vehicles. In addition,\ntheir results heavily rely on visual features, and seldom of they consider the\nalignment between the vehicle's semantic information and visual\nrepresentations. In this work, we propose a new vehicle detection paradigm\nbased on a pre-trained foundation vehicle model (VehicleMAE) and a large\nlanguage model (T5), termed VFM-Det. It follows the region proposal-based\ndetection framework and the features of each proposal can be enhanced using\nVehicleMAE. More importantly, we propose a new VAtt2Vec module that predicts\nthe vehicle semantic attributes of these proposals and transforms them into\nfeature vectors to enhance the vision features via contrastive learning.\nExtensive experiments on three vehicle detection benchmark datasets thoroughly\nproved the effectiveness of our vehicle detector. Specifically, our model\nimproves the baseline approach by $+5.1\\%$, $+6.2\\%$ on the $AP_{0.5}$,\n$AP_{0.75}$ metrics, respectively, on the Cityscapes dataset.The source code of\nthis work will be released at https://github.com/Event-AHU/VFM-Det.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.CV",
    "comment": "In Peer Review",
    "pdf_url": "http://arxiv.org/pdf/2408.13031v1",
    "published_date": "2024-08-23 12:39:02 UTC",
    "updated_date": "2024-08-23 12:39:02 UTC"
  },
  {
    "arxiv_id": "2408.13003v1",
    "title": "BoostTrack++: using tracklet information to detect more objects in multiple object tracking",
    "authors": [
      "Vukašin Stanojević",
      "Branimir Todorović"
    ],
    "abstract": "Multiple object tracking (MOT) depends heavily on selection of true positive\ndetected bounding boxes. However, this aspect of the problem is mostly\noverlooked or mitigated by employing two-stage association and utilizing low\nconfidence detections in the second stage. Recently proposed BoostTrack\nattempts to avoid the drawbacks of multiple stage association approach and use\nlow-confidence detections by applying detection confidence boosting. In this\npaper, we identify the limitations of the confidence boost used in BoostTrack\nand propose a method to improve its performance. To construct a richer\nsimilarity measure and enable a better selection of true positive detections,\nwe propose to use a combination of shape, Mahalanobis distance and novel soft\nBIoU similarity. We propose a soft detection confidence boost technique which\ncalculates new confidence scores based on the similarity measure and the\nprevious confidence scores, and we introduce varying similarity threshold to\naccount for lower similarity measure between detections and tracklets which are\nnot regularly updated. The proposed additions are mutually independent and can\nbe used in any MOT algorithm.\n  Combined with the BoostTrack+ baseline, our method achieves near state of the\nart results on the MOT17 dataset and new state of the art HOTA and IDF1 scores\non the MOT20 dataset.\n  The source code is available at:\nhttps://github.com/vukasin-stanojevic/BoostTrack .",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.13003v1",
    "published_date": "2024-08-23 11:44:21 UTC",
    "updated_date": "2024-08-23 11:44:21 UTC"
  },
  {
    "arxiv_id": "2408.13001v1",
    "title": "CRUXEval-X: A Benchmark for Multilingual Code Reasoning, Understanding and Execution",
    "authors": [
      "Ruiyang Xu",
      "Jialun Cao",
      "Yaojie Lu",
      "Hongyu Lin",
      "Xianpei Han",
      "Ben He",
      "Shing-Chi Cheung",
      "Le Sun"
    ],
    "abstract": "Code benchmarks such as HumanEval are widely adopted to evaluate Large\nLanguage Models' (LLMs) coding capabilities. However, there is an unignorable\nprogramming language bias in existing code benchmarks -- over 95% code\ngeneration benchmarks are dominated by Python, leaving the LLMs' capabilities\nin other programming languages such as Java and C/C++ unknown. Moreover, coding\ntask bias is also crucial. Most benchmarks focus on code generation capability,\nwhile benchmarks for code reasoning (given input, reasoning output; and given\noutput, reasoning input), an essential coding capability, are insufficient.\nYet, constructing multi-lingual benchmarks can be expensive and\nlabor-intensive, and codes in contest websites such as Leetcode suffer from\ndata contamination during training. To fill this gap, we propose CRUXEVAL-X, a\nmulti-lingual code reasoning benchmark that contains 19 programming languages.\nIt comprises at least 600 subjects for each language, along with 19K\ncontent-consistent tests in total. In particular, the construction pipeline of\nCRUXEVAL-X works in a fully automated and test-guided manner, which iteratively\ngenerates and repairs based on execution feedback. Also, to cross language\nbarriers (e.g., dynamic/static type systems in Python/C++), we formulated\nvarious transition rules between language pairs to facilitate translation. Our\nintensive evaluation of 24 representative LLMs reveals the correlation between\nlanguage pairs. For example, TypeScript and JavaScript show a significant\npositive correlation, while Racket has less correlation with other languages.\nMore interestingly, even a model trained solely on Python can achieve at most\n34.4% Pass@1 in other languages, revealing the cross-language generalization of\nLLMs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "13pages",
    "pdf_url": "http://arxiv.org/pdf/2408.13001v1",
    "published_date": "2024-08-23 11:43:00 UTC",
    "updated_date": "2024-08-23 11:43:00 UTC"
  },
  {
    "arxiv_id": "2409.04447v1",
    "title": "Leveraging Contrastive Learning and Self-Training for Multimodal Emotion Recognition with Limited Labeled Samples",
    "authors": [
      "Qi Fan",
      "Yutong Li",
      "Yi Xin",
      "Xinyu Cheng",
      "Guanglai Gao",
      "Miao Ma"
    ],
    "abstract": "The Multimodal Emotion Recognition challenge MER2024 focuses on recognizing\nemotions using audio, language, and visual signals. In this paper, we present\nour submission solutions for the Semi-Supervised Learning Sub-Challenge\n(MER2024-SEMI), which tackles the issue of limited annotated data in emotion\nrecognition. Firstly, to address the class imbalance, we adopt an oversampling\nstrategy. Secondly, we propose a modality representation combinatorial\ncontrastive learning (MR-CCL) framework on the trimodal input data to establish\nrobust initial models. Thirdly, we explore a self-training approach to expand\nthe training set. Finally, we enhance prediction robustness through a\nmulti-classifier weighted soft voting strategy. Our proposed method is\nvalidated to be effective on the MER2024-SEMI Challenge, achieving a weighted\naverage F-score of 88.25% and ranking 6th on the leaderboard. Our project is\navailable at https://github.com/WooyoohL/MER2024-SEMI.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted by ACM MM Workshop 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.04447v1",
    "published_date": "2024-08-23 11:33:54 UTC",
    "updated_date": "2024-08-23 11:33:54 UTC"
  },
  {
    "arxiv_id": "2408.12996v1",
    "title": "Enhancing Knowledge Tracing with Concept Map and Response Disentanglement",
    "authors": [
      "Soonwook Park",
      "Donghoon Lee",
      "Hogun Park"
    ],
    "abstract": "In the rapidly advancing realm of educational technology, it becomes critical\nto accurately trace and understand student knowledge states. Conventional\nKnowledge Tracing (KT) models have mainly focused on binary responses (i.e.,\ncorrect and incorrect answers) to questions. Unfortunately, they largely\noverlook the essential information in students' actual answer choices,\nparticularly for Multiple Choice Questions (MCQs), which could help reveal each\nlearner's misconceptions or knowledge gaps. To tackle these challenges, we\npropose the Concept map-driven Response disentanglement method for enhancing\nKnowledge Tracing (CRKT) model. CRKT benefits KT by directly leveraging answer\nchoices--beyond merely identifying correct or incorrect answers--to distinguish\nresponses with different incorrect choices. We further introduce the novel use\nof unchosen responses by employing disentangled representations to get insights\nfrom options not selected by students. Additionally, CRKT tracks the student's\nknowledge state at the concept level and encodes the concept map, representing\nthe relationships between them, to better predict unseen concepts. This\napproach is expected to provide actionable feedback, improving the learning\nexperience. Our comprehensive experiments across multiple datasets demonstrate\nCRKT's effectiveness, achieving superior performance in prediction accuracy and\ninterpretability over state-of-the-art models.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to Knowledge-Based Systems Journal",
    "pdf_url": "http://arxiv.org/pdf/2408.12996v1",
    "published_date": "2024-08-23 11:25:56 UTC",
    "updated_date": "2024-08-23 11:25:56 UTC"
  },
  {
    "arxiv_id": "2408.12989v1",
    "title": "RIFF: Inducing Rules for Fraud Detection from Decision Trees",
    "authors": [
      "João Lucas Martins",
      "João Bravo",
      "Ana Sofia Gomes",
      "Carlos Soares",
      "Pedro Bizarro"
    ],
    "abstract": "Financial fraud is the cause of multi-billion dollar losses annually.\nTraditionally, fraud detection systems rely on rules due to their transparency\nand interpretability, key features in domains where decisions need to be\nexplained. However, rule systems require significant input from domain experts\nto create and tune, an issue that rule induction algorithms attempt to mitigate\nby inferring rules directly from data. We explore the application of these\nalgorithms to fraud detection, where rule systems are constrained to have a low\nfalse positive rate (FPR) or alert rate, by proposing RIFF, a rule induction\nalgorithm that distills a low FPR rule set directly from decision trees. Our\nexperiments show that the induced rules are often able to maintain or improve\nperformance of the original models for low FPR tasks, while substantially\nreducing their complexity and outperforming rules hand-tuned by experts.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published as a conference paper at RuleML+RR 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.12989v1",
    "published_date": "2024-08-23 11:14:20 UTC",
    "updated_date": "2024-08-23 11:14:20 UTC"
  },
  {
    "arxiv_id": "2408.12984v4",
    "title": "PDDFormer: Pairwise Distance Distribution Graph Transformer for Crystal Material Property Prediction",
    "authors": [
      "Xiangxiang Shen",
      "Zheng Wan",
      "Lingfeng Wen",
      "Licheng Sun",
      "Ou Yang Ming Jie",
      "JiJUn Cheng",
      "Xuan Tang",
      "Xian Wei"
    ],
    "abstract": "The crystal structure can be simplified as a periodic point set repeating\nacross the entire three-dimensional space along an underlying lattice.\nTraditionally, methods for representing crystals rely on descriptors like\nlattice parameters, symmetry, and space groups to characterize the structure.\nHowever, in reality, atoms in material always vibrate above absolute zero,\ncausing continuous fluctuations in their positions. This dynamic behavior\ndisrupts the underlying periodicity of the lattice, making crystal graphs based\non static lattice parameters and conventional descriptors discontinuous under\neven slight perturbations. To this end, chemists proposed the Pairwise Distance\nDistribution (PDD) method, which has been used to distinguish all periodic\nstructures in the world's largest real materials collection, the Cambridge\nStructural Database. However, achieving the completeness of PDD requires\ndefining a large number of neighboring atoms, resulting in high computational\ncosts. Moreover, it does not account for atomic information, making it\nchallenging to directly apply PDD to crystal material property prediction\ntasks. To address these challenges, we propose the atom-Weighted Pairwise\nDistance Distribution (WPDD) and Unit cell Pairwise Distance Distribution\n(UPDD) for the first time, incorporating them into the construction of\nmulti-edge crystal graphs. Based on this, we further developed WPDDFormer and\nUPDDFormer, graph transformer architecture constructed using WPDD and UPDD\ncrystal graphs. We demonstrate that this method maintains the continuity and\ncompleteness of crystal graphs even under slight perturbations in atomic\npositions.",
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.AI"
    ],
    "primary_category": "cond-mat.mtrl-sci",
    "comment": "8 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.12984v4",
    "published_date": "2024-08-23 11:05:48 UTC",
    "updated_date": "2024-11-24 08:10:52 UTC"
  },
  {
    "arxiv_id": "2408.12981v1",
    "title": "QD-VMR: Query Debiasing with Contextual Understanding Enhancement for Video Moment Retrieval",
    "authors": [
      "Chenghua Gao",
      "Min Li",
      "Jianshuo Liu",
      "Junxing Ren",
      "Lin Chen",
      "Haoyu Liu",
      "Bo Meng",
      "Jitao Fu",
      "Wenwen Su"
    ],
    "abstract": "Video Moment Retrieval (VMR) aims to retrieve relevant moments of an\nuntrimmed video corresponding to the query. While cross-modal interaction\napproaches have shown progress in filtering out query-irrelevant information in\nvideos, they assume the precise alignment between the query semantics and the\ncorresponding video moments, potentially overlooking the misunderstanding of\nthe natural language semantics. To address this challenge, we propose a novel\nmodel called \\textit{QD-VMR}, a query debiasing model with enhanced contextual\nunderstanding. Firstly, we leverage a Global Partial Aligner module via video\nclip and query features alignment and video-query contrastive learning to\nenhance the cross-modal understanding capabilities of the model. Subsequently,\nwe employ a Query Debiasing Module to obtain debiased query features\nefficiently, and a Visual Enhancement module to refine the video features\nrelated to the query. Finally, we adopt the DETR structure to predict the\npossible target video moments. Through extensive evaluations of three benchmark\ndatasets, QD-VMR achieves state-of-the-art performance, proving its potential\nto improve the accuracy of VMR. Further analytical experiments demonstrate the\neffectiveness of our proposed module. Our code will be released to facilitate\nfuture research.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages, 4 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2408.12981v1",
    "published_date": "2024-08-23 10:56:42 UTC",
    "updated_date": "2024-08-23 10:56:42 UTC"
  },
  {
    "arxiv_id": "2408.12963v1",
    "title": "Open Llama2 Model for the Lithuanian Language",
    "authors": [
      "Artūras Nakvosas",
      "Povilas Daniušis",
      "Vytas Mulevičius"
    ],
    "abstract": "In this paper, we propose and describe the first open Llama2 large language\nmodels (LLMs) for the Lithuanian language, including an accompanying\nquestion/answer (Q/A) dataset and translations of popular LLM benchmarks. We\nprovide a brief review of open regional LLMs and detailed information on the\nproposed LLMs and their training process. We also conduct an empirical\nevaluation, comparing the perplexities of the proposed LLMs with those of other\nmodern open LLMs. In addition, benchmarking the proposed LLMs against language\nunderstanding tasks reveals that high-quality pretraining datasets may be\nessential for achieving models that perform efficiently on these benchmarks.\nThe full realisations of the described LLMs are available in the accompanying\nopen repository~\\url{https://huggingface.co/neurotechnology}.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages, 8 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2408.12963v1",
    "published_date": "2024-08-23 10:18:39 UTC",
    "updated_date": "2024-08-23 10:18:39 UTC"
  },
  {
    "arxiv_id": "2408.12959v1",
    "title": "Multimodal Contrastive In-Context Learning",
    "authors": [
      "Yosuke Miyanishi",
      "Minh Le Nguyen"
    ],
    "abstract": "The rapid growth of Large Language Models (LLMs) usage has highlighted the\nimportance of gradient-free in-context learning (ICL). However, interpreting\ntheir inner workings remains challenging. This paper introduces a novel\nmultimodal contrastive in-context learning framework to enhance our\nunderstanding of ICL in LLMs. First, we present a contrastive learning-based\ninterpretation of ICL in real-world settings, marking the distance of the\nkey-value representation as the differentiator in ICL. Second, we develop an\nanalytical framework to address biases in multimodal input formatting for\nreal-world datasets. We demonstrate the effectiveness of ICL examples where\nbaseline performance is poor, even when they are represented in unseen formats.\nLastly, we propose an on-the-fly approach for ICL (Anchored-by-Text ICL) that\ndemonstrates effectiveness in detecting hateful memes, a task where typical ICL\nstruggles due to resource limitations. Extensive experiments on multimodal\ndatasets reveal that our approach significantly improves ICL performance across\nvarious scenarios, such as challenging tasks and resource-constrained\nenvironments. Moreover, it provides valuable insights into the mechanisms of\nin-context learning in LLMs. Our findings have important implications for\ndeveloping more interpretable, efficient, and robust multimodal AI systems,\nespecially in challenging tasks and resource-constrained environments.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.12959v1",
    "published_date": "2024-08-23 10:10:01 UTC",
    "updated_date": "2024-08-23 10:10:01 UTC"
  },
  {
    "arxiv_id": "2408.12950v1",
    "title": "Informational Embodiment: Computational role of information structure in codes and robots",
    "authors": [
      "Alexandre Pitti",
      "Kohei Nakajima",
      "Yasuo Kuniyoshi"
    ],
    "abstract": "The body morphology plays an important role in the way information is\nperceived and processed by an agent. We address an information theory (IT)\naccount on how the precision of sensors, the accuracy of motors, their\nplacement, the body geometry, shape the information structure in robots and\ncomputational codes. As an original idea, we envision the robot's body as a\nphysical communication channel through which information is conveyed, in and\nout, despite intrinsic noise and material limitations. Following this, entropy,\na measure of information and uncertainty, can be used to maximize the\nefficiency of robot design and of algorithmic codes per se. This is known as\nthe principle of Entropy Maximization (PEM) introduced in biology by Barlow in\n1969. The Shannon's source coding theorem provides then a framework to compare\ndifferent types of bodies in terms of sensorimotor information. In line with\nPME, we introduce a special class of efficient codes used in IT that reached\nthe Shannon limits in terms of information capacity for error correction and\nrobustness against noise, and parsimony. These efficient codes, which exploit\ninsightfully quantization and randomness, permit to deal with uncertainty,\nredundancy and compacity. These features can be used for perception and control\nin intelligent systems. In various examples and closing discussions, we reflect\non the broader implications of our framework that we called Informational\nEmbodiment to motor theory and bio-inspired robotics, touching upon concepts\nlike motor synergies, reservoir computing, and morphological computation. These\ninsights can contribute to a deeper understanding of how information theory\nintersects with the embodiment of intelligence in both natural and artificial\nsystems.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.12950v1",
    "published_date": "2024-08-23 09:59:45 UTC",
    "updated_date": "2024-08-23 09:59:45 UTC"
  },
  {
    "arxiv_id": "2408.12942v2",
    "title": "Causal-Guided Active Learning for Debiasing Large Language Models",
    "authors": [
      "Li Du",
      "Zhouhao Sun",
      "Xiao Ding",
      "Yixuan Ma",
      "Yang Zhao",
      "Kaitao Qiu",
      "Ting Liu",
      "Bing Qin"
    ],
    "abstract": "Although achieving promising performance, recent analyses show that current\ngenerative large language models (LLMs) may still capture dataset biases and\nutilize them for generation, leading to poor generalizability and harmfulness\nof LLMs. However, due to the diversity of dataset biases and the\nover-optimization problem, previous prior-knowledge-based debiasing methods and\nfine-tuning-based debiasing methods may not be suitable for current LLMs. To\naddress this issue, we explore combining active learning with the causal\nmechanisms and propose a casual-guided active learning (CAL) framework, which\nutilizes LLMs itself to automatically and autonomously identify informative\nbiased samples and induce the bias patterns. Then a cost-effective and\nefficient in-context learning based method is employed to prevent LLMs from\nutilizing dataset biases during generation. Experimental results show that CAL\ncan effectively recognize typical biased instances and induce various bias\npatterns for debiasing LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted as ACL 2024 main conference & Rewared as Outstanding Paper",
    "pdf_url": "http://arxiv.org/pdf/2408.12942v2",
    "published_date": "2024-08-23 09:46:15 UTC",
    "updated_date": "2024-08-30 07:30:13 UTC"
  },
  {
    "arxiv_id": "2408.12941v1",
    "title": "iSee: Advancing Multi-Shot Explainable AI Using Case-based Recommendations",
    "authors": [
      "Anjana Wijekoon",
      "Nirmalie Wiratunga",
      "David Corsar",
      "Kyle Martin",
      "Ikechukwu Nkisi-Orji",
      "Chamath Palihawadana",
      "Marta Caro-Martínez",
      "Belen Díaz-Agudo",
      "Derek Bridge",
      "Anne Liret"
    ],
    "abstract": "Explainable AI (XAI) can greatly enhance user trust and satisfaction in\nAI-assisted decision-making processes. Recent findings suggest that a single\nexplainer may not meet the diverse needs of multiple users in an AI system;\nindeed, even individual users may require multiple explanations. This\nhighlights the necessity for a \"multi-shot\" approach, employing a combination\nof explainers to form what we introduce as an \"explanation strategy\". Tailored\nto a specific user or a user group, an \"explanation experience\" describes\ninteractions with personalised strategies designed to enhance their AI\ndecision-making processes. The iSee platform is designed for the intelligent\nsharing and reuse of explanation experiences, using Case-based Reasoning to\nadvance best practices in XAI. The platform provides tools that enable AI\nsystem designers, i.e. design users, to design and iteratively revise the most\nsuitable explanation strategy for their AI system to satisfy end-user needs.\nAll knowledge generated within the iSee platform is formalised by the iSee\nontology for interoperability. We use a summative mixed methods study protocol\nto evaluate the usability and utility of the iSee platform with six design\nusers across varying levels of AI and XAI expertise. Our findings confirm that\nthe iSee platform effectively generalises across applications and its potential\nto promote the adoption of XAI best practices.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to appear at the ECAI-PAIS 2024 main conference proceedings",
    "pdf_url": "http://arxiv.org/pdf/2408.12941v1",
    "published_date": "2024-08-23 09:44:57 UTC",
    "updated_date": "2024-08-23 09:44:57 UTC"
  },
  {
    "arxiv_id": "2408.12936v2",
    "title": "Smooth InfoMax -- Towards easier Post-Hoc interpretability",
    "authors": [
      "Fabian Denoodt",
      "Bart de Boer",
      "José Oramas"
    ],
    "abstract": "We introduce Smooth InfoMax (SIM), a novel method for self-supervised\nrepresentation learning that incorporates an interpretability constraint into\nthe learned representations at various depths of the neural network. SIM's\narchitecture is split up into probabilistic modules, each locally optimized\nusing the InfoNCE bound. Inspired by VAEs, the representations from these\nmodules are designed to be samples from Gaussian distributions and are further\nconstrained to be close to the standard normal distribution. This results in a\nsmooth and predictable space, enabling traversal of the latent space through a\ndecoder for easier post-hoc analysis of the learned representations. We\nevaluate SIM's performance on sequential speech data, showing that it performs\ncompetitively with its less interpretable counterpart, Greedy InfoMax (GIM).\nMoreover, we provide insights into SIM's internal representations,\ndemonstrating that the contained information is less entangled throughout the\nrepresentation and more concentrated in a smaller subset of the dimensions.\nThis further highlights the improved interpretability of SIM.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.12936v2",
    "published_date": "2024-08-23 09:36:09 UTC",
    "updated_date": "2025-03-19 16:58:12 UTC"
  },
  {
    "arxiv_id": "2408.12935v3",
    "title": "Trustworthy, Responsible, and Safe AI: A Comprehensive Architectural Framework for AI Safety with Challenges and Mitigations",
    "authors": [
      "Chen Chen",
      "Xueluan Gong",
      "Ziyao Liu",
      "Weifeng Jiang",
      "Si Qi Goh",
      "Kwok-Yan Lam"
    ],
    "abstract": "AI Safety is an emerging area of critical importance to the safe adoption and\ndeployment of AI systems. With the rapid proliferation of AI and especially\nwith the recent advancement of Generative AI (or GAI), the technology ecosystem\nbehind the design, development, adoption, and deployment of AI systems has\ndrastically changed, broadening the scope of AI Safety to address impacts on\npublic safety and national security. In this paper, we propose a novel\narchitectural framework for understanding and analyzing AI Safety; defining its\ncharacteristics from three perspectives: Trustworthy AI, Responsible AI, and\nSafe AI. We provide an extensive review of current research and advancements in\nAI safety from these perspectives, highlighting their key challenges and\nmitigation approaches. Through examples from state-of-the-art technologies,\nparticularly Large Language Models (LLMs), we present innovative mechanism,\nmethodologies, and techniques for designing and testing AI safety. Our goal is\nto promote advancement in AI safety research, and ultimately enhance people's\ntrust in digital transformation.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.12935v3",
    "published_date": "2024-08-23 09:33:48 UTC",
    "updated_date": "2025-01-15 10:21:30 UTC"
  },
  {
    "arxiv_id": "2408.12927v2",
    "title": "Abductive and Contrastive Explanations for Scoring Rules in Voting",
    "authors": [
      "Clément Contet",
      "Umberto Grandi",
      "Jérôme Mengin"
    ],
    "abstract": "We view voting rules as classifiers that assign a winner (a class) to a\nprofile of voters' preferences (an instance). We propose to apply techniques\nfrom formal explainability, most notably abductive and contrastive\nexplanations, to identify minimal subsets of a preference profile that either\nimply the current winner or explain why a different candidate was not elected.\nFormal explanations turn out to have strong connections with classical problems\nstudied in computational social choice such as bribery, possible and necessary\nwinner identification, and preference learning. We design algorithms for\ncomputing abductive and contrastive explanations for scoring rules. For the\nBorda rule, we find a lower bound on the size of the smallest abductive\nexplanations, and we conduct simulations to identify correlations between\nproperties of preference profiles and the size of their smallest abductive\nexplanations.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 2 figures Extended version of a paper in proceedings of\n  ECAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.12927v2",
    "published_date": "2024-08-23 09:12:58 UTC",
    "updated_date": "2024-08-26 10:05:54 UTC"
  },
  {
    "arxiv_id": "2408.12910v1",
    "title": "What Do You Want? User-centric Prompt Generation for Text-to-image Synthesis via Multi-turn Guidance",
    "authors": [
      "Yilun Liu",
      "Minggui He",
      "Feiyu Yao",
      "Yuhe Ji",
      "Shimin Tao",
      "Jingzhou Du",
      "Duan Li",
      "Jian Gao",
      "Li Zhang",
      "Hao Yang",
      "Boxing Chen",
      "Osamu Yoshie"
    ],
    "abstract": "The emergence of text-to-image synthesis (TIS) models has significantly\ninfluenced digital image creation by producing high-quality visuals from\nwritten descriptions. Yet these models heavily rely on the quality and\nspecificity of textual prompts, posing a challenge for novice users who may not\nbe familiar with TIS-model-preferred prompt writing. Existing solutions relieve\nthis via automatic model-preferred prompt generation from user queries.\nHowever, this single-turn manner suffers from limited user-centricity in terms\nof result interpretability and user interactivity. To address these issues, we\npropose DialPrompt, a multi-turn dialogue-based TIS prompt generation model\nthat emphasises user-centricity. DialPrompt is designed to follow a multi-turn\nguidance workflow, where in each round of dialogue the model queries user with\ntheir preferences on possible optimization dimensions before generating the\nfinal TIS prompt. To achieve this, we mined 15 essential dimensions for\nhigh-quality prompts from advanced users and curated a multi-turn dataset.\nThrough training on this dataset, DialPrompt can improve interpretability by\nallowing users to understand the correlation between specific phrases and image\nattributes. Additionally, it enables greater user control and engagement in the\nprompt generation process, leading to more personalized and visually satisfying\noutputs. Experiments indicate that DialPrompt achieves a competitive result in\nthe quality of synthesized images, outperforming existing prompt engineering\napproaches by 5.7%. Furthermore, in our user evaluation, DialPrompt outperforms\nexisting approaches by 46.5% in user-centricity score and is rated 7.9/10 by 19\nhuman reviewers.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.12910v1",
    "published_date": "2024-08-23 08:35:35 UTC",
    "updated_date": "2024-08-23 08:35:35 UTC"
  },
  {
    "arxiv_id": "2408.12909v2",
    "title": "CSPs with Few Alien Constraints",
    "authors": [
      "Peter Jonsson",
      "Victor Lagerkvist",
      "George Osipov"
    ],
    "abstract": "The constraint satisfaction problem asks to decide if a set of constraints\nover a relational structure $\\mathcal{A}$ is satisfiable (CSP$(\\mathcal{A})$).\nWe consider CSP$(\\mathcal{A} \\cup \\mathcal{B})$ where $\\mathcal{A}$ is a\nstructure and $\\mathcal{B}$ is an alien structure, and analyse its\n(parameterized) complexity when at most $k$ alien constraints are allowed. We\nestablish connections and obtain transferable complexity results to several\nwell-studied problems that previously escaped classification attempts. Our\nnovel approach, utilizing logical and algebraic methods, yields an FPT versus\npNP dichotomy for arbitrary finite structures and sharper dichotomies for\nBoolean structures and first-order reducts of $(\\mathbb{N},=)$ (equality CSPs),\ntogether with many partial results for general $\\omega$-categorical structures.",
    "categories": [
      "cs.CC",
      "cs.AI"
    ],
    "primary_category": "cs.CC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.12909v2",
    "published_date": "2024-08-23 08:34:13 UTC",
    "updated_date": "2024-08-27 14:26:53 UTC"
  },
  {
    "arxiv_id": "2408.12902v2",
    "title": "IAA: Inner-Adaptor Architecture Empowers Frozen Large Language Model with Multimodal Capabilities",
    "authors": [
      "Bin Wang",
      "Chunyu Xie",
      "Dawei Leng",
      "Yuhui Yin"
    ],
    "abstract": "In the field of multimodal large language models (MLLMs), common methods\ntypically involve unfreezing the language model during training to foster\nprofound visual understanding. However, the fine-tuning of such models with\nvision-language data often leads to a diminution of their natural language\nprocessing (NLP) capabilities. To avoid this performance degradation, a\nstraightforward solution is to freeze the language model while developing\nmultimodal competencies. Unfortunately, previous works have not attained\nsatisfactory outcomes. Building on the strategy of freezing the language model,\nwe conduct thorough structural exploration and introduce the Inner-Adaptor\nArchitecture (IAA). Specifically, the architecture incorporates multiple\nmultimodal adaptors at varying depths within the large language model to\nfacilitate direct interaction with the inherently text-oriented transformer\nlayers, thereby enabling the frozen language model to acquire multimodal\ncapabilities. Unlike previous approaches of freezing language models that\nrequire large-scale aligned data, our proposed architecture is able to achieve\nsuperior performance on small-scale datasets. We conduct extensive experiments\nto improve the general multimodal capabilities and visual grounding abilities\nof the MLLM. Our approach remarkably outperforms previous state-of-the-art\nmethods across various vision-language benchmarks without sacrificing\nperformance on NLP tasks. Code and models are available at\nhttps://github.com/360CVGroup/Inner-Adaptor-Architecture.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2408.12902v2",
    "published_date": "2024-08-23 08:10:13 UTC",
    "updated_date": "2025-04-15 03:28:22 UTC"
  },
  {
    "arxiv_id": "2408.12890v1",
    "title": "Multiple Areal Feature Aware Transportation Demand Prediction",
    "authors": [
      "Sumin Han",
      "Jisun An",
      "Youngjun Park",
      "Suji Kim",
      "Kitae Jang",
      "Dongman Lee"
    ],
    "abstract": "A reliable short-term transportation demand prediction supports the\nauthorities in improving the capability of systems by optimizing schedules,\nadjusting fleet sizes, and generating new transit networks. A handful of\nresearch efforts incorporate one or a few areal features while learning\nspatio-temporal correlation, to capture similar demand patterns between similar\nareas. However, urban characteristics are polymorphic, and they need to be\nunderstood by multiple areal features such as land use, sociodemographics, and\nplace-of-interest (POI) distribution. In this paper, we propose a novel\nspatio-temporal multi-feature-aware graph convolutional recurrent network\n(ST-MFGCRN) that fuses multiple areal features during spatio-temproal\nunderstanding. Inside ST-MFGCRN, we devise sentinel attention to calculate the\nareal similarity matrix by allowing each area to take partial attention if the\nfeature is not useful. We evaluate the proposed model on two real-world\ntransportation datasets, one with our constructed BusDJ dataset and one with\nbenchmark TaxiBJ. Results show that our model outperforms the state-of-the-art\nbaselines up to 7\\% on BusDJ and 8\\% on TaxiBJ dataset.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.12890v1",
    "published_date": "2024-08-23 07:51:10 UTC",
    "updated_date": "2024-08-23 07:51:10 UTC"
  },
  {
    "arxiv_id": "2408.12882v1",
    "title": "Spatio-Temporal Road Traffic Prediction using Real-time Regional Knowledge",
    "authors": [
      "Sumin Han",
      "Jisun An",
      "Dongman Lee"
    ],
    "abstract": "For traffic prediction in transportation services such as car-sharing and\nride-hailing, mid-term road traffic prediction (within a few hours) is\nconsidered essential. However, the existing road-level traffic prediction has\nmainly studied how significantly micro traffic events propagate to the adjacent\nroads in terms of short-term prediction. On the other hand, recent attempts\nhave been made to incorporate regional knowledge such as POIs, road\ncharacteristics, and real-time social events to help traffic prediction.\nHowever, these studies lack in understandings of different modalities of\nroad-level and region-level spatio-temporal correlations and how to combine\nsuch knowledge. This paper proposes a novel method that embeds real-time\nregion-level knowledge using POIs, satellite images, and real-time LTE access\ntraces via a regional spatio-temporal module that consists of dynamic\nconvolution and temporal attention, and conducts bipartite spatial transform\nattention to convert into road-level knowledge. Then the model ingests this\nembedded knowledge into a road-level attention-based prediction model.\nExperimental results on real-world road traffic prediction show that our model\noutperforms the baselines.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.12882v1",
    "published_date": "2024-08-23 07:34:26 UTC",
    "updated_date": "2024-08-23 07:34:26 UTC"
  },
  {
    "arxiv_id": "2408.12880v1",
    "title": "Has Multimodal Learning Delivered Universal Intelligence in Healthcare? A Comprehensive Survey",
    "authors": [
      "Qika Lin",
      "Yifan Zhu",
      "Xin Mei",
      "Ling Huang",
      "Jingying Ma",
      "Kai He",
      "Zhen Peng",
      "Erik Cambria",
      "Mengling Feng"
    ],
    "abstract": "The rapid development of artificial intelligence has constantly reshaped the\nfield of intelligent healthcare and medicine. As a vital technology, multimodal\nlearning has increasingly garnered interest due to data complementarity,\ncomprehensive modeling form, and great application potential. Currently,\nnumerous researchers are dedicating their attention to this field, conducting\nextensive studies and constructing abundant intelligent systems. Naturally, an\nopen question arises that has multimodal learning delivered universal\nintelligence in healthcare? To answer the question, we adopt three unique\nviewpoints for a holistic analysis. Firstly, we conduct a comprehensive survey\nof the current progress of medical multimodal learning from the perspectives of\ndatasets, task-oriented methods, and universal foundation models. Based on\nthem, we further discuss the proposed question from five issues to explore the\nreal impacts of advanced techniques in healthcare, from data and technologies\nto performance and ethics. The answer is that current technologies have NOT\nachieved universal intelligence and there remains a significant journey to\nundertake. Finally, in light of the above reviews and discussions, we point out\nten potential directions for exploration towards the goal of universal\nintelligence in healthcare.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "21 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.12880v1",
    "published_date": "2024-08-23 07:31:01 UTC",
    "updated_date": "2024-08-23 07:31:01 UTC"
  },
  {
    "arxiv_id": "2408.12879v1",
    "title": "Frequency-aware Feature Fusion for Dense Image Prediction",
    "authors": [
      "Linwei Chen",
      "Ying Fu",
      "Lin Gu",
      "Chenggang Yan",
      "Tatsuya Harada",
      "Gao Huang"
    ],
    "abstract": "Dense image prediction tasks demand features with strong category information\nand precise spatial boundary details at high resolution. To achieve this,\nmodern hierarchical models often utilize feature fusion, directly adding\nupsampled coarse features from deep layers and high-resolution features from\nlower levels. In this paper, we observe rapid variations in fused feature\nvalues within objects, resulting in intra-category inconsistency due to\ndisturbed high-frequency features. Additionally, blurred boundaries in fused\nfeatures lack accurate high frequency, leading to boundary displacement.\nBuilding upon these observations, we propose Frequency-Aware Feature Fusion\n(FreqFusion), integrating an Adaptive Low-Pass Filter (ALPF) generator, an\noffset generator, and an Adaptive High-Pass Filter (AHPF) generator. The ALPF\ngenerator predicts spatially-variant low-pass filters to attenuate\nhigh-frequency components within objects, reducing intra-class inconsistency\nduring upsampling. The offset generator refines large inconsistent features and\nthin boundaries by replacing inconsistent features with more consistent ones\nthrough resampling, while the AHPF generator enhances high-frequency detailed\nboundary information lost during downsampling. Comprehensive visualization and\nquantitative analysis demonstrate that FreqFusion effectively improves feature\nconsistency and sharpens object boundaries. Extensive experiments across\nvarious dense prediction tasks confirm its effectiveness. The code is made\npublicly available at https://github.com/Linwei-Chen/FreqFusion.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by TPAMI (2024)",
    "pdf_url": "http://arxiv.org/pdf/2408.12879v1",
    "published_date": "2024-08-23 07:30:34 UTC",
    "updated_date": "2024-08-23 07:30:34 UTC"
  },
  {
    "arxiv_id": "2408.15012v2",
    "title": "Flexible categorization using formal concept analysis and Dempster-Shafer theory",
    "authors": [
      "Marcel Boersma",
      "Krishna Manoorkar",
      "Alessandra Palmigiano",
      "Mattia Panettiere",
      "Apostolos Tzimoulis",
      "Nachoem Wijnberg"
    ],
    "abstract": "The framework developed in the present paper provides a formal ground to\ngenerate and study explainable categorizations of sets of entities, based on\nthe epistemic attitudes of individual agents or groups thereof. Based on this\nframework, we discuss a machine-leaning meta-algorithm for outlier detection\nand classification which provides local and global explanations of its results.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "arXiv admin note: substantial text overlap with arXiv:2210.17330",
    "pdf_url": "http://arxiv.org/pdf/2408.15012v2",
    "published_date": "2024-08-23 07:28:20 UTC",
    "updated_date": "2024-12-25 15:35:04 UTC"
  },
  {
    "arxiv_id": "2408.12871v5",
    "title": "DeepDiveAI: Identifying AI Related Documents in Large Scale Literature Data",
    "authors": [
      "Zhou Xiaochen",
      "Liang Xingzhou",
      "Zou Hui",
      "Lu Yi",
      "Qu Jingjing"
    ],
    "abstract": "In this paper, we propose a method to automatically classify AI-related\ndocuments from large-scale literature databases, leading to the creation of an\nAI-related literature dataset, named DeepDiveAI. The dataset construction\napproach integrates expert knowledge with the capabilities of advanced models,\nstructured across two global stages. In the first stage, expert-curated\nclassification datasets are used to train an LSTM model, which classifies\ncoarse AI related records from large-scale datasets. In the second stage, we\nuse Qwen2.5 Plus to annotate a random 10% of the coarse AI-related records,\nwhich are then used to train a BERT binary classifier. This step further\nrefines the coarse AI related record set to obtain the final DeepDiveAI\ndataset. Evaluation results demonstrate that the entire workflow can\nefficiently and accurately identify AI-related literature from large-scale\ndatasets.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.12871v5",
    "published_date": "2024-08-23 07:05:12 UTC",
    "updated_date": "2025-04-22 12:21:24 UTC"
  },
  {
    "arxiv_id": "2408.12870v1",
    "title": "Can AI Assistance Aid in the Grading of Handwritten Answer Sheets?",
    "authors": [
      "Pritam Sil",
      "Parag Chaudhuri",
      "Bhaskaran Raman"
    ],
    "abstract": "With recent advancements in artificial intelligence (AI), there has been\ngrowing interest in using state of the art (SOTA) AI solutions to provide\nassistance in grading handwritten answer sheets. While a few commercial\nproducts exist, the question of whether AI-assistance can actually reduce\ngrading effort and time has not yet been carefully considered in published\nliterature. This work introduces an AI-assisted grading pipeline. The pipeline\nfirst uses text detection to automatically detect question regions present in a\nquestion paper PDF. Next, it uses SOTA text detection methods to highlight\nimportant keywords present in the handwritten answer regions of scanned answer\nsheets to assist in the grading process. We then evaluate a prototype\nimplementation of the AI-assisted grading pipeline deployed on an existing\ne-learning management platform. The evaluation involves a total of 5 different\nreal-life examinations across 4 different courses at a reputed institute; it\nconsists of a total of 42 questions, 17 graders, and 468 submissions. We log\nand analyze the grading time for each handwritten answer while using AI\nassistance and without it. Our evaluations have shown that, on average, the\ngraders take 31% less time while grading a single response and 33% less grading\ntime while grading a single answer sheet using AI assistance.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.12870v1",
    "published_date": "2024-08-23 07:00:25 UTC",
    "updated_date": "2024-08-23 07:00:25 UTC"
  },
  {
    "arxiv_id": "2408.12866v1",
    "title": "Obfuscated Memory Malware Detection",
    "authors": [
      "Sharmila S P",
      "Aruna Tiwari",
      "Narendra S Chaudhari"
    ],
    "abstract": "Providing security for information is highly critical in the current era with\ndevices enabled with smart technology, where assuming a day without the\ninternet is highly impossible. Fast internet at a cheaper price, not only made\ncommunication easy for legitimate users but also for cybercriminals to induce\nattacks in various dimensions to breach privacy and security. Cybercriminals\ngain illegal access and breach the privacy of users to harm them in multiple\nways. Malware is one such tool used by hackers to execute their malicious\nintent. Development in AI technology is utilized by malware developers to cause\nsocial harm. In this work, we intend to show how Artificial Intelligence and\nMachine learning can be used to detect and mitigate these cyber-attacks induced\nby malware in specific obfuscated malware. We conducted experiments with memory\nfeature engineering on memory analysis of malware samples. Binary\nclassification can identify whether a given sample is malware or not, but\nidentifying the type of malware will only guide what next step to be taken for\nthat malware, to stop it from proceeding with its further action. Hence, we\npropose a multi-class classification model to detect the three types of\nobfuscated malware with an accuracy of 89.07% using the Classic Random Forest\nalgorithm. To the best of our knowledge, there is very little amount of work\ndone in classifying multiple obfuscated malware by a single model. We also\ncompared our model with a few state-of-the-art models and found it\ncomparatively better.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "8 pages 9 figures presented in IEEE CCEM Conference paper",
    "pdf_url": "http://arxiv.org/pdf/2408.12866v1",
    "published_date": "2024-08-23 06:39:15 UTC",
    "updated_date": "2024-08-23 06:39:15 UTC"
  },
  {
    "arxiv_id": "2408.13287v1",
    "title": "Abstract Art Interpretation Using ControlNet",
    "authors": [
      "Rishabh Srivastava",
      "Addrish Roy"
    ],
    "abstract": "Our study delves into the fusion of abstract art interpretation and\ntext-to-image synthesis, addressing the challenge of achieving precise spatial\ncontrol over image composition solely through textual prompts. Leveraging the\ncapabilities of ControlNet, we empower users with finer control over the\nsynthesis process, enabling enhanced manipulation of synthesized imagery.\nInspired by the minimalist forms found in abstract artworks, we introduce a\nnovel condition crafted from geometric primitives such as triangles.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "5 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.13287v1",
    "published_date": "2024-08-23 06:25:54 UTC",
    "updated_date": "2024-08-23 06:25:54 UTC"
  },
  {
    "arxiv_id": "2408.12857v1",
    "title": "Memory-Efficient LLM Training with Online Subspace Descent",
    "authors": [
      "Kaizhao Liang",
      "Bo Liu",
      "Lizhang Chen",
      "Qiang Liu"
    ],
    "abstract": "Recently, a wide range of memory-efficient LLM training algorithms have\ngained substantial popularity. These methods leverage the low-rank structure of\ngradients to project optimizer states into a subspace using projection matrix\nfound by singular value decomposition (SVD). However, convergence of these\nalgorithms is highly dependent on the update rules of their projection matrix.\nIn this work, we provide the \\emph{first} convergence guarantee for arbitrary\nupdate rules of projection matrix. This guarantee is generally applicable to\noptimizers that can be analyzed with Hamiltonian Descent, including most common\nones, such as LION, Adam. Inspired by our theoretical understanding, we propose\nOnline Subspace Descent, a new family of subspace descent optimizer without\nSVD. Instead of updating the projection matrix with eigenvectors, Online\nSubspace Descent updates the projection matrix with online PCA. Online Subspace\nDescent is flexible and introduces only minimum overhead to training. We show\nthat for the task of pretraining LLaMA models ranging from 60M to 7B parameters\non the C4 dataset, Online Subspace Descent achieves lower perplexity and better\ndownstream tasks performance than state-of-the-art low-rank training methods\nacross different settings and narrows the gap with full-rank baselines.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Code is available at\n  https://github.com/kyleliang919/Online-Subspace-Descent",
    "pdf_url": "http://arxiv.org/pdf/2408.12857v1",
    "published_date": "2024-08-23 05:54:53 UTC",
    "updated_date": "2024-08-23 05:54:53 UTC"
  },
  {
    "arxiv_id": "2408.12845v1",
    "title": "Online Fair Division with Contextual Bandits",
    "authors": [
      "Arun Verma",
      "Indrajit Saha",
      "Makoto Yokoo",
      "Bryan Kian Hsiang Low"
    ],
    "abstract": "This paper considers a novel online fair division problem involving multiple\nagents in which a learner observes an indivisible item that has to be\nirrevocably allocated to one of the agents while satisfying a fairness and\nefficiency constraint. Existing algorithms assume a small number of items with\na sufficiently large number of copies, which ensures a good utility estimation\nfor all item-agent pairs. However, such an assumption may not hold in many\nreal-life applications, e.g., an online platform that has a large number of\nusers (items) who only use the platform's service providers (agents) a few\ntimes (a few copies of items), which makes it difficult to estimate the utility\nfor all item-agent pairs. To overcome this challenge, we model the online fair\ndivision problem using contextual bandits, assuming the utility is an unknown\nfunction of the item-agent features. We then propose algorithms for online fair\ndivision with sub-linear regret guarantees. Our experimental results also\nverify the different performance aspects of the proposed algorithms.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "We study an online fair division problem that has a large number of\n  items with only a few copies of each item and propose contextual\n  bandits-based algorithms with sub-linear regret guarantees",
    "pdf_url": "http://arxiv.org/pdf/2408.12845v1",
    "published_date": "2024-08-23 05:25:58 UTC",
    "updated_date": "2024-08-23 05:25:58 UTC"
  },
  {
    "arxiv_id": "2408.12844v1",
    "title": "Predicting Affective States from Screen Text Sentiment",
    "authors": [
      "Songyan Teng",
      "Tianyi Zhang",
      "Simon D'Alfonso",
      "Vassilis Kostakos"
    ],
    "abstract": "The proliferation of mobile sensing technologies has enabled the study of\nvarious physiological and behavioural phenomena through unobtrusive data\ncollection from smartphone sensors. This approach offers real-time insights\ninto individuals' physical and mental states, creating opportunities for\npersonalised treatment and interventions. However, the potential of analysing\nthe textual content viewed on smartphones to predict affective states remains\nunderexplored. To better understand how the screen text that users are exposed\nto and interact with can influence their affects, we investigated a subset of\ndata obtained from a digital phenotyping study of Australian university\nstudents conducted in 2023. We employed linear regression, zero-shot, and\nmulti-shot prompting using a large language model (LLM) to analyse\nrelationships between screen text and affective states. Our findings indicate\nthat multi-shot prompting substantially outperforms both linear regression and\nzero-shot prompting, highlighting the importance of context in affect\nprediction. We discuss the value of incorporating textual and sentiment data\nfor improving affect prediction, providing a basis for future advancements in\nunderstanding smartphone use and wellbeing.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "7 pages",
    "pdf_url": "http://arxiv.org/pdf/2408.12844v1",
    "published_date": "2024-08-23 05:25:11 UTC",
    "updated_date": "2024-08-23 05:25:11 UTC"
  },
  {
    "arxiv_id": "2408.12841v2",
    "title": "COVID-19 Probability Prediction Using Machine Learning: An Infectious Approach",
    "authors": [
      "Mohsen Asghari Ilani",
      "Saba Moftakhar Tehran",
      "Ashkan Kavei",
      "Arian Radmehr"
    ],
    "abstract": "The ongoing COVID-19 pandemic continues to pose significant challenges to\nglobal public health, despite the widespread availability of vaccines. Early\ndetection of the disease remains paramount in curbing its transmission and\nmitigating its impact on public health systems. In response, this study delves\ninto the application of advanced machine learning (ML) techniques for\npredicting COVID-19 infection probability. We conducted a rigorous\ninvestigation into the efficacy of various ML models, including XGBoost, LGBM,\nAdaBoost, Logistic Regression, Decision Tree, RandomForest, CatBoost, KNN, and\nDeep Neural Networks (DNN). Leveraging a dataset comprising 4000 samples, with\n3200 allocated for training and 800 for testing, our experiment offers\ncomprehensive insights into the performance of these models in COVID-19\nprediction. Our findings reveal that Deep Neural Networks (DNN) emerge as the\ntop-performing model, exhibiting superior accuracy and recall metrics. With an\nimpressive accuracy rate of 89%, DNN demonstrates remarkable potential in early\nCOVID-19 detection. This underscores the efficacy of deep learning approaches\nin leveraging complex data patterns to identify COVID-19 infections accurately.\nThis study underscores the critical role of machine learning, particularly deep\nlearning methodologies, in augmenting early detection efforts amidst the\nongoing pandemic. The success of DNN in accurately predicting COVID-19\ninfection probability highlights the importance of continued research and\ndevelopment in leveraging advanced technologies to combat infectious diseases.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.12841v2",
    "published_date": "2024-08-23 05:15:24 UTC",
    "updated_date": "2024-12-04 01:20:16 UTC"
  },
  {
    "arxiv_id": "2408.12838v2",
    "title": "Exploring Machine Learning Models for Lung Cancer Level Classification: A comparative ML Approach",
    "authors": [
      "Mohsen Asghari Ilani",
      "Saba Moftakhar Tehran",
      "Ashkan Kavei",
      "Hamed Alizadegan"
    ],
    "abstract": "This paper explores machine learning (ML) models for classifying lung cancer\nlevels to improve diagnostic accuracy and prognosis. Through parameter tuning\nand rigorous evaluation, we assess various ML algorithms. Techniques like\nminimum child weight and learning rate monitoring were used to reduce\noverfitting and optimize performance. Our findings highlight the robust\nperformance of Deep Neural Network (DNN) models across all phases. Ensemble\nmethods, including voting and bagging, also showed promise in enhancing\npredictive accuracy and robustness. However, Support Vector Machine (SVM)\nmodels with the Sigmoid kernel faced challenges, indicating a need for further\nrefinement. Overall, our study provides insights into ML-based lung cancer\nclassification, emphasizing the importance of parameter tuning to optimize\nmodel performance and improve diagnostic accuracy in oncological care.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.12838v2",
    "published_date": "2024-08-23 04:56:36 UTC",
    "updated_date": "2024-12-04 04:18:32 UTC"
  },
  {
    "arxiv_id": "2408.12837v2",
    "title": "Underwater SONAR Image Classification and Analysis using LIME-based Explainable Artificial Intelligence",
    "authors": [
      "Purushothaman Natarajan",
      "Athira Nambiar"
    ],
    "abstract": "Deep learning techniques have revolutionized image classification by\nmimicking human cognition and automating complex decision-making processes.\nHowever, the deployment of AI systems in the wild, especially in high-security\ndomains such as defence, is curbed by the lack of explainability of the model.\nTo this end, eXplainable AI (XAI) is an emerging area of research that is\nintended to explore the unexplained hidden black box nature of deep neural\nnetworks. This paper explores the application of the eXplainable Artificial\nIntelligence (XAI) tool to interpret the underwater image classification\nresults, one of the first works in the domain to the best of our knowledge. Our\nstudy delves into the realm of SONAR image classification using a custom\ndataset derived from diverse sources, including the Seabed Objects KLSG\ndataset, the camera SONAR dataset, the mine SONAR images dataset, and the SCTD\ndataset. An extensive analysis of transfer learning techniques for image\nclassification using benchmark Convolutional Neural Network (CNN) architectures\nsuch as VGG16, ResNet50, InceptionV3, DenseNet121, etc. is carried out. On top\nof this classification model, a post-hoc XAI technique, viz. Local\nInterpretable Model-Agnostic Explanations (LIME) are incorporated to provide\ntransparent justifications for the model's decisions by perturbing input data\nlocally to see how predictions change. Furthermore, Submodular Picks LIME\n(SP-LIME) a version of LIME particular to images, that perturbs the image based\non the submodular picks is also extensively studied. To this end, two\nsubmodular optimization algorithms i.e. Quickshift and Simple Linear Iterative\nClustering (SLIC) are leveraged towards submodular picks. The extensive\nanalysis of XAI techniques highlights interpretability of the results in a more\nhuman-compliant way, thus boosting our confidence and reliability.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "68T07 (Primary) 68T45, 68U10 (Secondary)",
      "I.4.8; I.2.10; I.5.4"
    ],
    "primary_category": "cs.CV",
    "comment": "55 pages, 9 tables, 18 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.12837v2",
    "published_date": "2024-08-23 04:54:18 UTC",
    "updated_date": "2024-09-23 14:39:14 UTC"
  },
  {
    "arxiv_id": "2408.12834v1",
    "title": "CLLMFS: A Contrastive Learning enhanced Large Language Model Framework for Few-Shot Named Entity Recognition",
    "authors": [
      "Yafeng Zhang",
      "Zilan Yu",
      "Yuang Huang",
      "Jing Tang"
    ],
    "abstract": "Few-shot Named Entity Recognition (NER), the task of identifying named\nentities with only a limited amount of labeled data, has gained increasing\nsignificance in natural language processing. While existing methodologies have\nshown some effectiveness, such as enriching label semantics through various\nprompting modes or employing metric learning techniques, their performance\nexhibits limited robustness across diverse domains due to the lack of rich\nknowledge in their pre-trained models. To address this issue, we propose\nCLLMFS, a Contrastive Learning enhanced Large Language Model (LLM) Framework\nfor Few-Shot Named Entity Recognition, achieving promising results with limited\ntraining data. Considering the impact of LLM's internal representations on\ndownstream tasks, CLLMFS integrates Low-Rank Adaptation (LoRA) and contrastive\nlearning mechanisms specifically tailored for few-shot NER. By enhancing the\nmodel's internal representations, CLLMFS effectively improves both entity\nboundary awareness ability and entity recognition accuracy. Our method has\nachieved state-of-the-art performance improvements on F1-score ranging from\n2.58\\% to 97.74\\% over existing best-performing methods across several\nrecognized benchmarks. Furthermore, through cross-domain NER experiments\nconducted on multiple datasets, we have further validated the robust\ngeneralization capability of our method. Our code will be released in the near\nfuture.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "27TH EUROPEAN CONFERENCE ON ARTIFICIAL INTELLIGENCE",
    "pdf_url": "http://arxiv.org/pdf/2408.12834v1",
    "published_date": "2024-08-23 04:44:05 UTC",
    "updated_date": "2024-08-23 04:44:05 UTC"
  },
  {
    "arxiv_id": "2408.12821v1",
    "title": "Examining the Commitments and Difficulties Inherent in Multimodal Foundation Models for Street View Imagery",
    "authors": [
      "Zhenyuan Yang",
      "Xuhui Lin",
      "Qinyi He",
      "Ziye Huang",
      "Zhengliang Liu",
      "Hanqi Jiang",
      "Peng Shu",
      "Zihao Wu",
      "Yiwei Li",
      "Stephen Law",
      "Gengchen Mai",
      "Tianming Liu",
      "Tao Yang"
    ],
    "abstract": "The emergence of Large Language Models (LLMs) and multimodal foundation\nmodels (FMs) has generated heightened interest in their applications that\nintegrate vision and language. This paper investigates the capabilities of\nChatGPT-4V and Gemini Pro for Street View Imagery, Built Environment, and\nInterior by evaluating their performance across various tasks. The assessments\ninclude street furniture identification, pedestrian and car counts, and road\nwidth measurement in Street View Imagery; building function classification,\nbuilding age analysis, building height analysis, and building structure\nclassification in the Built Environment; and interior room classification,\ninterior design style analysis, interior furniture counts, and interior length\nmeasurement in Interior. The results reveal proficiency in length measurement,\nstyle analysis, question answering, and basic image understanding, but\nhighlight limitations in detailed recognition and counting tasks. While\nzero-shot learning shows potential, performance varies depending on the problem\ndomains and image complexities. This study provides new insights into the\nstrengths and weaknesses of multimodal foundation models for practical\nchallenges in Street View Imagery, Built Environment, and Interior. Overall,\nthe findings demonstrate foundational multimodal intelligence, emphasizing the\npotential of FMs to drive forward interdisciplinary applications at the\nintersection of computer vision and language.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.12821v1",
    "published_date": "2024-08-23 03:45:31 UTC",
    "updated_date": "2024-08-23 03:45:31 UTC"
  },
  {
    "arxiv_id": "2408.12815v3",
    "title": "Staircase Cascaded Fusion of Lightweight Local Pattern Recognition and Long-Range Dependencies for Structural Crack Segmentation",
    "authors": [
      "Hui Liu",
      "Chen Jia",
      "Fan Shi",
      "Xu Cheng",
      "Mianzhao Wang",
      "Shengyong Chen"
    ],
    "abstract": "Detecting cracks with pixel-level precision for key structures is a\nsignificant challenge, existing methods struggle to integrate local textures\nand pixel dependencies of cracks. Furthermore, these methods possess numerous\nparameters and substantial computational requirements, complicating deployment\non edge devices. In this paper, we propose the Staircase Cascaded Fusion Crack\nSegmentation Network (CrackSCF), which generates high-quality crack\nsegmentation maps while reducing computational overhead. We design a\nlightweight convolutional block that substitutes all convolution operations,\nreducing the model's computational demands while maintaining an effective\ncapture of local details. Additionally, we introduce a lightweight long-range\ndependency extractor to better capture the long-range dependencies.\nFurthermore, we develop a staircase cascaded fusion module, which seamlessly\nintegrates local patterns and long-range dependencies, resulting in\nhigh-quality segmentation maps. To comprehensively evaluate our method, we\ncreated the challenging TUT benchmark dataset and evaluated it alongside five\nother public datasets. The results show that our method outperforms existing\nmethods, particularly in handling background noise and achieving detailed\nsegmentation. The F1 and mIoU scores on the TUT dataset are 0.8382 and 0.8473,\nrespectively, demonstrating state-of-the-art (SOTA) performance with low\ncomputational resources. The code and dataset is available at\nhttps://github.com/Karl1109/CrackSCF.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.12815v3",
    "published_date": "2024-08-23 03:21:51 UTC",
    "updated_date": "2025-02-24 08:54:15 UTC"
  },
  {
    "arxiv_id": "2408.12809v2",
    "title": "DutyTTE: Deciphering Uncertainty in Origin-Destination Travel Time Estimation",
    "authors": [
      "Xiaowei Mao",
      "Yan Lin",
      "Shengnan Guo",
      "Yubin Chen",
      "Xingyu Xian",
      "Haomin Wen",
      "Qisen Xu",
      "Youfang Lin",
      "Huaiyu Wan"
    ],
    "abstract": "Uncertainty quantification in travel time estimation (TTE) aims to estimate\nthe confidence interval for travel time, given the origin (O), destination (D),\nand departure time (T). Accurately quantifying this uncertainty requires\ngenerating the most likely path and assessing travel time uncertainty along the\npath. This involves two main challenges: 1) Predicting a path that aligns with\nthe ground truth, and 2) modeling the impact of travel time in each segment on\noverall uncertainty under varying conditions. We propose DutyTTE to address\nthese challenges. For the first challenge, we introduce a deep reinforcement\nlearning method to improve alignment between the predicted path and the ground\ntruth, providing more accurate travel time information from road segments to\nimprove TTE. For the second challenge, we propose a mixture of experts guided\nuncertainty quantification mechanism to better capture travel time uncertainty\nfor each segment under varying contexts. Additionally, we calibrate our results\nusing Hoeffding's upper-confidence bound to provide statistical guarantees for\nthe estimated confidence intervals. Extensive experiments on two real-world\ndatasets demonstrate the superiority of our proposed method.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "7 pages",
    "pdf_url": "http://arxiv.org/pdf/2408.12809v2",
    "published_date": "2024-08-23 03:06:04 UTC",
    "updated_date": "2025-01-20 08:12:51 UTC"
  },
  {
    "arxiv_id": "2408.12808v1",
    "title": "VALE: A Multimodal Visual and Language Explanation Framework for Image Classifiers using eXplainable AI and Language Models",
    "authors": [
      "Purushothaman Natarajan",
      "Athira Nambiar"
    ],
    "abstract": "Deep Neural Networks (DNNs) have revolutionized various fields by enabling\ntask automation and reducing human error. However, their internal workings and\ndecision-making processes remain obscure due to their black box nature.\nConsequently, the lack of interpretability limits the application of these\nmodels in high-risk scenarios. To address this issue, the emerging field of\neXplainable Artificial Intelligence (XAI) aims to explain and interpret the\ninner workings of DNNs. Despite advancements, XAI faces challenges such as the\nsemantic gap between machine and human understanding, the trade-off between\ninterpretability and performance, and the need for context-specific\nexplanations. To overcome these limitations, we propose a novel multimodal\nframework named VALE Visual and Language Explanation. VALE integrates\nexplainable AI techniques with advanced language models to provide\ncomprehensive explanations. This framework utilizes visual explanations from\nXAI tools, an advanced zero-shot image segmentation model, and a visual\nlanguage model to generate corresponding textual explanations. By combining\nvisual and textual explanations, VALE bridges the semantic gap between machine\noutputs and human interpretation, delivering results that are more\ncomprehensible to users. In this paper, we conduct a pilot study of the VALE\nframework for image classification tasks. Specifically, Shapley Additive\nExplanations (SHAP) are used to identify the most influential regions in\nclassified images. The object of interest is then extracted using the Segment\nAnything Model (SAM), and explanations are generated using state-of-the-art\npre-trained Vision-Language Models (VLMs). Extensive experimental studies are\nperformed on two datasets: the ImageNet dataset and a custom underwater SONAR\nimage dataset, demonstrating VALEs real-world applicability in underwater image\nclassification.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "68T07 (Primary) 68T45, 68U10 (Secondary)",
      "I.4.8; I.2.10; I.5.4"
    ],
    "primary_category": "cs.CV",
    "comment": "15 pages, 10 tables, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.12808v1",
    "published_date": "2024-08-23 03:02:11 UTC",
    "updated_date": "2024-08-23 03:02:11 UTC"
  },
  {
    "arxiv_id": "2408.12806v1",
    "title": "Is Generative AI the Next Tactical Cyber Weapon For Threat Actors? Unforeseen Implications of AI Generated Cyber Attacks",
    "authors": [
      "Yusuf Usman",
      "Aadesh Upadhyay",
      "Prashnna Gyawali",
      "Robin Chataut"
    ],
    "abstract": "In an era where digital threats are increasingly sophisticated, the\nintersection of Artificial Intelligence and cybersecurity presents both\npromising defenses and potent dangers. This paper delves into the escalating\nthreat posed by the misuse of AI, specifically through the use of Large\nLanguage Models (LLMs). This study details various techniques like the switch\nmethod and character play method, which can be exploited by cybercriminals to\ngenerate and automate cyber attacks. Through a series of controlled\nexperiments, the paper demonstrates how these models can be manipulated to\nbypass ethical and privacy safeguards to effectively generate cyber attacks\nsuch as social engineering, malicious code, payload generation, and spyware. By\ntesting these AI generated attacks on live systems, the study assesses their\neffectiveness and the vulnerabilities they exploit, offering a practical\nperspective on the risks AI poses to critical infrastructure. We also introduce\nOccupy AI, a customized, finetuned LLM specifically engineered to automate and\nexecute cyberattacks. This specialized AI driven tool is adept at crafting\nsteps and generating executable code for a variety of cyber threats, including\nphishing, malware injection, and system exploitation. The results underscore\nthe urgency for ethical AI practices, robust cybersecurity measures, and\nregulatory oversight to mitigate AI related threats. This paper aims to elevate\nawareness within the cybersecurity community about the evolving digital threat\nlandscape, advocating for proactive defense strategies and responsible AI\ndevelopment to protect against emerging cyber threats.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "Primary 03C90, Secondary 03-02,",
      "I.2"
    ],
    "primary_category": "cs.CR",
    "comment": "Journal Paper",
    "pdf_url": "http://arxiv.org/pdf/2408.12806v1",
    "published_date": "2024-08-23 02:56:13 UTC",
    "updated_date": "2024-08-23 02:56:13 UTC"
  },
  {
    "arxiv_id": "2408.12805v1",
    "title": "A Safe Self-evolution Algorithm for Autonomous Driving Based on Data-Driven Risk Quantification Model",
    "authors": [
      "Shuo Yang",
      "Shizhen Li",
      "Yanjun Huang",
      "Hong Chen"
    ],
    "abstract": "Autonomous driving systems with self-evolution capabilities have the\npotential to independently evolve in complex and open environments, allowing to\nhandle more unknown scenarios. However, as a result of the safety-performance\ntrade-off mechanism of evolutionary algorithms, it is difficult to ensure safe\nexploration without sacrificing the improvement ability. This problem is\nespecially prominent in dynamic traffic scenarios. Therefore, this paper\nproposes a safe self-evolution algorithm for autonomous driving based on\ndata-driven risk quantification model. Specifically, a risk quantification\nmodel based on the attention mechanism is proposed by modeling the way humans\nperceive risks during driving, with the idea of achieving safety situation\nestimation of the surrounding environment through a data-driven approach. To\nprevent the impact of over-conservative safety guarding policies on the\nself-evolution capability of the algorithm, a safety-evolutionary\ndecision-control integration algorithm with adjustable safety limits is\nproposed, and the proposed risk quantization model is integrated into it.\nSimulation and real-vehicle experiments results illustrate the effectiveness of\nthe proposed method. The results show that the proposed algorithm can generate\nsafe and reasonable actions in a variety of complex scenarios and guarantee\nsafety without losing the evolutionary potential of learning-based autonomous\ndriving systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.12805v1",
    "published_date": "2024-08-23 02:52:35 UTC",
    "updated_date": "2024-08-23 02:52:35 UTC"
  },
  {
    "arxiv_id": "2408.12803v1",
    "title": "Multi-Treatment Multi-Task Uplift Modeling for Enhancing User Growth",
    "authors": [
      "Yuxiang Wei",
      "Zhaoxin Qiu",
      "Yingjie Li",
      "Yuke Sun",
      "Xiaoling Li"
    ],
    "abstract": "As a key component in boosting online user growth, uplift modeling aims to\nmeasure individual user responses (e.g., whether to play the game) to various\ntreatments, such as gaming bonuses, thereby enhancing business outcomes.\nHowever, previous research typically considers a single-task, single-treatment\nsetting, where only one treatment exists and the overall treatment effect is\nmeasured by a single type of user response. In this paper, we propose a\nMulti-Treatment Multi-Task (MTMT) uplift network to estimate treatment effects\nin a multi-task scenario. We identify the multi-treatment problem as a causal\ninference problem with a tiered response, comprising a base effect (from\noffering a treatment) and an incremental effect (from offering a specific type\nof treatment), where the base effect can be numerically much larger than the\nincremental effect. Specifically, MTMT separately encodes user features and\ntreatments. The user feature encoder uses a multi-gate mixture of experts\n(MMOE) network to encode relevant user features, explicitly learning inter-task\nrelations. The resultant embeddings are used to measure natural responses per\ntask. Furthermore, we introduce a treatment-user feature interaction module to\nmodel correlations between each treatment and user feature. Consequently, we\nseparately measure the base and incremental treatment effect for each task\nbased on the produced treatment-aware representations. Experimental results\nbased on an offline public dataset and an online proprietary dataset\ndemonstrate the effectiveness of MTMT in single/multi-treatment and\nsingle/multi-task settings. Additionally, MTMT has been deployed in our gaming\nplatform to improve user experience.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.12803v1",
    "published_date": "2024-08-23 02:44:08 UTC",
    "updated_date": "2024-08-23 02:44:08 UTC"
  },
  {
    "arxiv_id": "2408.12799v2",
    "title": "Preference Consistency Matters: Enhancing Preference Learning in Language Models with Automated Self-Curation of Training Corpora",
    "authors": [
      "JoonHo Lee",
      "JuYoun Son",
      "Juree Seok",
      "Wooseok Jang",
      "Yeong-Dae Kwon"
    ],
    "abstract": "Inconsistent annotations in training corpora, particularly within preference\nlearning datasets, pose challenges in developing advanced language models.\nThese inconsistencies often arise from variability among annotators and\ninherent multi-dimensional nature of the preferences. To address these issues,\nwe introduce a self-curation method that preprocesses annotated datasets by\nleveraging proxy models trained directly on them. Our method enhances\npreference learning by automatically detecting and selecting consistent\nannotations. We validate the proposed approach through extensive\ninstruction-following tasks, demonstrating performance improvements of up to\n33\\% across various learning algorithms and proxy capabilities. This work\noffers a straightforward and reliable solution to address preference\ninconsistencies without relying on heuristics, serving as an initial step\ntoward the development of more advanced preference learning methodologies. Code\nis available at https://github.com/Self-Curation/ .",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to NAACL 2025 main conference",
    "pdf_url": "http://arxiv.org/pdf/2408.12799v2",
    "published_date": "2024-08-23 02:27:14 UTC",
    "updated_date": "2025-01-31 09:27:26 UTC"
  },
  {
    "arxiv_id": "2408.12798v1",
    "title": "BackdoorLLM: A Comprehensive Benchmark for Backdoor Attacks on Large Language Models",
    "authors": [
      "Yige Li",
      "Hanxun Huang",
      "Yunhan Zhao",
      "Xingjun Ma",
      "Jun Sun"
    ],
    "abstract": "Generative Large Language Models (LLMs) have made significant strides across\nvarious tasks, but they remain vulnerable to backdoor attacks, where specific\ntriggers in the prompt cause the LLM to generate adversary-desired responses.\nWhile most backdoor research has focused on vision or text classification\ntasks, backdoor attacks in text generation have been largely overlooked. In\nthis work, we introduce \\textit{BackdoorLLM}, the first comprehensive benchmark\nfor studying backdoor attacks on LLMs. \\textit{BackdoorLLM} features: 1) a\nrepository of backdoor benchmarks with a standardized training pipeline, 2)\ndiverse attack strategies, including data poisoning, weight poisoning, hidden\nstate attacks, and chain-of-thought attacks, 3) extensive evaluations with over\n200 experiments on 8 attacks across 7 scenarios and 6 model architectures, and\n4) key insights into the effectiveness and limitations of backdoors in LLMs. We\nhope \\textit{BackdoorLLM} will raise awareness of backdoor threats and\ncontribute to advancing AI safety. The code is available at\n\\url{https://github.com/bboylyg/BackdoorLLM}.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.12798v1",
    "published_date": "2024-08-23 02:21:21 UTC",
    "updated_date": "2024-08-23 02:21:21 UTC"
  },
  {
    "arxiv_id": "2408.13285v1",
    "title": "SIn-NeRF2NeRF: Editing 3D Scenes with Instructions through Segmentation and Inpainting",
    "authors": [
      "Jiseung Hong",
      "Changmin Lee",
      "Gyusang Yu"
    ],
    "abstract": "TL;DR Perform 3D object editing selectively by disentangling it from the\nbackground scene. Instruct-NeRF2NeRF (in2n) is a promising method that enables\nediting of 3D scenes composed of Neural Radiance Field (NeRF) using text\nprompts. However, it is challenging to perform geometrical modifications such\nas shrinking, scaling, or moving on both the background and object\nsimultaneously. In this project, we enable geometrical changes of objects\nwithin the 3D scene by selectively editing the object after separating it from\nthe scene. We perform object segmentation and background inpainting\nrespectively, and demonstrate various examples of freely resizing or moving\ndisentangled objects within the three-dimensional space.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Code is available at: https://github.com/KAISTChangmin/SIn-NeRF2NeRF",
    "pdf_url": "http://arxiv.org/pdf/2408.13285v1",
    "published_date": "2024-08-23 02:20:42 UTC",
    "updated_date": "2024-08-23 02:20:42 UTC"
  },
  {
    "arxiv_id": "2408.12796v1",
    "title": "Real-Time Posture Monitoring and Risk Assessment for Manual Lifting Tasks Using MediaPipe and LSTM",
    "authors": [
      "Ereena Bagga",
      "Ang Yang"
    ],
    "abstract": "This research focuses on developing a real-time posture monitoring and risk\nassessment system for manual lifting tasks using advanced AI and computer\nvision technologies. Musculoskeletal disorders (MSDs) are a significant concern\nfor workers involved in manual lifting, and traditional methods for posture\ncorrection are often inadequate due to delayed feedback and lack of\npersonalized assessment. Our proposed solution integrates AI-driven posture\ndetection, detailed keypoint analysis, risk level determination, and real-time\nfeedback delivered through a user-friendly web interface. The system aims to\nimprove posture, reduce the risk of MSDs, and enhance user engagement. The\nresearch involves comprehensive data collection, model training, and iterative\ndevelopment to ensure high accuracy and user satisfaction. The solution's\neffectiveness is evaluated against existing methodologies, demonstrating\nsignificant improvements in real-time feedback and risk assessment. This study\ncontributes to the field by offering a novel approach to posture correction\nthat addresses existing gaps and provides practical, immediate benefits to\nusers.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "Proceedings of the 1st International Workshop on Multimedia Computing\n  for Health and Medicine at ACM MM'24",
    "pdf_url": "http://arxiv.org/pdf/2408.12796v1",
    "published_date": "2024-08-23 02:19:52 UTC",
    "updated_date": "2024-08-23 02:19:52 UTC"
  },
  {
    "arxiv_id": "2408.12792v1",
    "title": "Event Detection via Probability Density Function Regression",
    "authors": [
      "Clark Peng",
      "Tolga Dinçer"
    ],
    "abstract": "In the domain of time series analysis, particularly in event detection tasks,\ncurrent methodologies predominantly rely on segmentation-based approaches,\nwhich predict the class label for each individual timesteps and use the\nchangepoints of these labels to detect events. However, these approaches may\nnot effectively detect the precise onset and offset of events within the data\nand suffer from class imbalance problems. This study introduces a generalized\nregression-based approach to reframe the time-interval-defined event detection\nproblem. Inspired by heatmap regression techniques from computer vision, our\napproach aims to predict probability densities at event locations rather than\nclass labels across the entire time series. The primary aim of this approach is\nto improve the accuracy of event detection methods, particularly for\nlong-duration events where identifying the onset and offset is more critical\nthan classifying individual event states. We demonstrate that regression-based\napproaches outperform segmentation-based methods across various\nstate-of-the-art baseline networks and datasets, offering a more effective\nsolution for specific event detection tasks.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.ML",
      "I.2.0; I.5.4"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.12792v1",
    "published_date": "2024-08-23 01:58:56 UTC",
    "updated_date": "2024-08-23 01:58:56 UTC"
  },
  {
    "arxiv_id": "2408.12789v1",
    "title": "Context-Aware Temporal Embedding of Objects in Video Data",
    "authors": [
      "Ahnaf Farhan",
      "M. Shahriar Hossain"
    ],
    "abstract": "In video analysis, understanding the temporal context is crucial for\nrecognizing object interactions, event patterns, and contextual changes over\ntime. The proposed model leverages adjacency and semantic similarities between\nobjects from neighboring video frames to construct context-aware temporal\nobject embeddings. Unlike traditional methods that rely solely on visual\nappearance, our temporal embedding model considers the contextual relationships\nbetween objects, creating a meaningful embedding space where temporally\nconnected object's vectors are positioned in proximity. Empirical studies\ndemonstrate that our context-aware temporal embeddings can be used in\nconjunction with conventional visual embeddings to enhance the effectiveness of\ndownstream applications. Moreover, the embeddings can be used to narrate a\nvideo using a Large Language Model (LLM). This paper describes the intricate\ndetails of the proposed objective function to generate context-aware temporal\nobject embeddings for video data and showcases the potential applications of\nthe generated embeddings in video analysis and object classification tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.12789v1",
    "published_date": "2024-08-23 01:44:10 UTC",
    "updated_date": "2024-08-23 01:44:10 UTC"
  },
  {
    "arxiv_id": "2408.12787v2",
    "title": "LLM-PBE: Assessing Data Privacy in Large Language Models",
    "authors": [
      "Qinbin Li",
      "Junyuan Hong",
      "Chulin Xie",
      "Jeffrey Tan",
      "Rachel Xin",
      "Junyi Hou",
      "Xavier Yin",
      "Zhun Wang",
      "Dan Hendrycks",
      "Zhangyang Wang",
      "Bo Li",
      "Bingsheng He",
      "Dawn Song"
    ],
    "abstract": "Large Language Models (LLMs) have become integral to numerous domains,\nsignificantly advancing applications in data management, mining, and analysis.\nTheir profound capabilities in processing and interpreting complex language\ndata, however, bring to light pressing concerns regarding data privacy,\nespecially the risk of unintentional training data leakage. Despite the\ncritical nature of this issue, there has been no existing literature to offer a\ncomprehensive assessment of data privacy risks in LLMs. Addressing this gap,\nour paper introduces LLM-PBE, a toolkit crafted specifically for the systematic\nevaluation of data privacy risks in LLMs. LLM-PBE is designed to analyze\nprivacy across the entire lifecycle of LLMs, incorporating diverse attack and\ndefense strategies, and handling various data types and metrics. Through\ndetailed experimentation with multiple LLMs, LLM-PBE facilitates an in-depth\nexploration of data privacy concerns, shedding light on influential factors\nsuch as model size, data characteristics, and evolving temporal dimensions.\nThis study not only enriches the understanding of privacy issues in LLMs but\nalso serves as a vital resource for future research in the field. Aimed at\nenhancing the breadth of knowledge in this area, the findings, resources, and\nour full technical report are made available at https://llm-pbe.github.io/,\nproviding an open platform for academic and practical advancements in LLM\nprivacy assessment.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.12787v2",
    "published_date": "2024-08-23 01:37:29 UTC",
    "updated_date": "2024-09-06 04:30:50 UTC"
  },
  {
    "arxiv_id": "2408.12781v1",
    "title": "The Model Mastery Lifecycle: A Framework for Designing Human-AI Interaction",
    "authors": [
      "Mark Chignell",
      "Mu-Huan Miles Chung",
      "Jaturong Kongmanee",
      "Khilan Jerath",
      "Abhay Raman"
    ],
    "abstract": "The utilization of AI in an increasing number of fields is the latest\niteration of a long process, where machines and systems have been replacing\nhumans, or changing the roles that they play, in various tasks. Although humans\nare often resistant to technological innovation, especially in workplaces,\nthere is a general trend towards increasing automation, and more recently, AI.\nAI is now capable of carrying out, or assisting with, many tasks that used to\nbe regarded as exclusively requiring human expertise. In this paper we consider\nthe case of tasks that could be performed either by human experts or by AI and\nlocate them on a continuum running from exclusively human task performance at\none end to AI autonomy on the other, with a variety of forms of human-AI\ninteraction between those extremes. Implementation of AI is constrained by the\ncontext of the systems and workflows that it will be embedded within. There is\nan urgent need for methods to determine how AI should be used in different\nsituations and to develop appropriate methods of human-AI interaction so that\nhumans and AI can work together effectively to perform tasks. In response to\nthe evolving landscape of AI progress and increasing mastery, we introduce an\nAI Mastery Lifecycle framework and discuss its implications for human-AI\ninteraction. The framework provides guidance on human-AI task allocation and\nhow human-AI interfaces need to adapt to improvements in AI task performance\nover time. Within the framework we identify a zone of uncertainty where the\nissues of human-AI task allocation and user interface design are likely to be\nmost challenging.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.12781v1",
    "published_date": "2024-08-23 01:00:32 UTC",
    "updated_date": "2024-08-23 01:00:32 UTC"
  },
  {
    "arxiv_id": "2408.12779v1",
    "title": "Investigating LLM Applications in E-Commerce",
    "authors": [
      "Chester Palen-Michel",
      "Ruixiang Wang",
      "Yipeng Zhang",
      "David Yu",
      "Canran Xu",
      "Zhe Wu"
    ],
    "abstract": "The emergence of Large Language Models (LLMs) has revolutionized natural\nlanguage processing in various applications especially in e-commerce. One\ncrucial step before the application of such LLMs in these fields is to\nunderstand and compare the performance in different use cases in such tasks.\nThis paper explored the efficacy of LLMs in the e-commerce domain, focusing on\ninstruction-tuning an open source LLM model with public e-commerce datasets of\nvarying sizes and comparing the performance with the conventional models\nprevalent in industrial applications. We conducted a comprehensive comparison\nbetween LLMs and traditional pre-trained language models across specific tasks\nintrinsic to the e-commerce domain, namely classification, generation,\nsummarization, and named entity recognition (NER). Furthermore, we examined the\neffectiveness of the current niche industrial application of very large LLM,\nusing in-context learning, in e-commerce specific tasks. Our findings indicate\nthat few-shot inference with very large LLMs often does not outperform\nfine-tuning smaller pre-trained models, underscoring the importance of\ntask-specific model optimization.Additionally, we investigated different\ntraining methodologies such as single-task training, mixed-task training, and\nLoRA merging both within domain/tasks and between different tasks. Through\nrigorous experimentation and analysis, this paper offers valuable insights into\nthe potential effectiveness of LLMs to advance natural language processing\ncapabilities within the e-commerce industry.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.12779v1",
    "published_date": "2024-08-23 00:57:37 UTC",
    "updated_date": "2024-08-23 00:57:37 UTC"
  },
  {
    "arxiv_id": "2408.12778v1",
    "title": "Data-Centric Approach to Constrained Machine Learning: A Case Study on Conway's Game of Life",
    "authors": [
      "Anton Bibin",
      "Anton Dereventsov"
    ],
    "abstract": "This paper focuses on a data-centric approach to machine learning\napplications in the context of Conway's Game of Life. Specifically, we consider\nthe task of training a minimal architecture network to learn the transition\nrules of Game of Life for a given number of steps ahead, which is known to be\nchallenging due to restrictions on the allowed number of trainable parameters.\nAn extensive quantitative analysis showcases the benefits of utilizing a\nstrategically designed training dataset, with its advantages persisting\nregardless of other parameters of the learning configuration, such as network\ninitialization weights or optimization algorithm. Importantly, our findings\nhighlight the integral role of domain expert insights in creating effective\nmachine learning applications for constrained real-world scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.IR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.12778v1",
    "published_date": "2024-08-23 00:56:34 UTC",
    "updated_date": "2024-08-23 00:56:34 UTC"
  },
  {
    "arxiv_id": "2408.12777v1",
    "title": "Environment-Centric Active Inference",
    "authors": [
      "Kanako Esaki",
      "Tadayuki Matsumura",
      "Takeshi Kato",
      "Shunsuke Minusa",
      "Yang Shao",
      "Hiroyuki Mizuno"
    ],
    "abstract": "To handle unintended changes in the environment by agents, we propose an\nenvironment-centric active inference EC-AIF in which the Markov Blanket of\nactive inference is defined starting from the environment. In normal active\ninference, the Markov Blanket is defined starting from the agent. That is,\nfirst the agent was defined as the entity that performs the \"action\" such as a\nrobot or a person, then the environment was defined as other people or objects\nthat are directly affected by the agent's \"action,\" and the boundary between\nthe agent and the environment was defined as the Markov Blanket. This\nagent-centric definition does not allow the agent to respond to unintended\nchanges in the environment caused by factors outside of the defined\nenvironment. In the proposed EC-AIF, there is no entity corresponding to an\nagent. The environment includes all observable things, including people and\nthings conventionally considered to be the environment, as well as entities\nthat perform \"actions\" such as robots and people. Accordingly, all states,\nincluding robots and people, are included in inference targets, eliminating\nunintended changes in the environment. The EC-AIF was applied to a robot arm\nand validated with an object transport task by the robot arm. The results\nshowed that the robot arm successfully transported objects while responding to\nchanges in the target position of the object and to changes in the orientation\nof another robot arm.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "14 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.12777v1",
    "published_date": "2024-08-23 00:54:28 UTC",
    "updated_date": "2024-08-23 00:54:28 UTC"
  },
  {
    "arxiv_id": "2408.12775v2",
    "title": "Intelligent OPC Engineer Assistant for Semiconductor Manufacturing",
    "authors": [
      "Guojin Chen",
      "Haoyu Yang",
      "Bei Yu",
      "Haoxing Ren"
    ],
    "abstract": "Advancements in chip design and manufacturing have enabled the processing of\ncomplex tasks such as deep learning and natural language processing, paving the\nway for the development of artificial general intelligence (AGI). AI, on the\nother hand, can be leveraged to innovate and streamline semiconductor\ntechnology from planning and implementation to manufacturing. In this paper, we\npresent \\textit{Intelligent OPC Engineer Assistant}, an AI/LLM-powered\nmethodology designed to solve the core manufacturing-aware optimization problem\nknown as optical proximity correction (OPC). The methodology involves a\nreinforcement learning-based OPC recipe search and a customized multi-modal\nagent system for recipe summarization. Experiments demonstrate that our\nmethodology can efficiently build OPC recipes on various chip designs with\nspecially handled design topologies, a task that typically requires the\nfull-time effort of OPC engineers with years of experience.",
    "categories": [
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.12775v2",
    "published_date": "2024-08-23 00:49:36 UTC",
    "updated_date": "2024-08-27 06:02:55 UTC"
  },
  {
    "arxiv_id": "2409.00064v1",
    "title": "Phrasing for UX: Enhancing Information Engagement through Computational Linguistics and Creative Analytics",
    "authors": [
      "Nimrod Dvir"
    ],
    "abstract": "This study explores the relationship between textual features and Information\nEngagement (IE) on digital platforms. It highlights the impact of computational\nlinguistics and analytics on user interaction. The READ model is introduced to\nquantify key predictors like representativeness, ease of use, affect, and\ndistribution, which forecast engagement levels. The model's effectiveness is\nvalidated through AB testing and randomized trials, showing strong predictive\nperformance in participation (accuracy: 0.94), perception (accuracy: 0.85),\nperseverance (accuracy: 0.81), and overall IE (accuracy: 0.97).\n  While participation metrics are strong, perception and perseverance show\nslightly lower recall and F1-scores, indicating some challenges. The study\ndemonstrates that modifying text based on the READ model's insights leads to\nsignificant improvements. For example, increasing representativeness and\npositive affect boosts selection rates by 11 percent, raises evaluation\naverages from 3.98 to 4.46, and improves retention rates by 11 percent. These\nfindings highlight the importance of linguistic factors in IE, providing a\nframework for enhancing digital text engagement. The research offers practical\nstrategies applicable to fields like education, health, and media.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.00064v1",
    "published_date": "2024-08-23 00:33:47 UTC",
    "updated_date": "2024-08-23 00:33:47 UTC"
  },
  {
    "arxiv_id": "2408.12772v2",
    "title": "Symmetric masking strategy enhances the performance of Masked Image Modeling",
    "authors": [
      "Khanh-Binh Nguyen",
      "Chae Jung Park"
    ],
    "abstract": "Masked Image Modeling (MIM) is a technique in self-supervised learning that\nfocuses on acquiring detailed visual representations from unlabeled images by\nestimating the missing pixels in randomly masked sections. It has proven to be\na powerful tool for the preliminary training of Vision Transformers (ViTs),\nyielding impressive results across various tasks. Nevertheless, most MIM\nmethods heavily depend on the random masking strategy to formulate the pretext\ntask. This strategy necessitates numerous trials to ascertain the optimal\ndropping ratio, which can be resource-intensive, requiring the model to be\npre-trained for anywhere between 800 to 1600 epochs. Furthermore, this approach\nmay not be suitable for all datasets. In this work, we propose a new masking\nstrategy that effectively helps the model capture global and local features.\nBased on this masking strategy, SymMIM, our proposed training pipeline for MIM\nis introduced. SymMIM achieves a new SOTA accuracy of 85.9\\% on ImageNet using\nViT-Large and surpasses previous SOTA across downstream tasks such as image\nclassification, semantic segmentation, object detection, instance segmentation\ntasks, and so on.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.12772v2",
    "published_date": "2024-08-23 00:15:43 UTC",
    "updated_date": "2024-12-13 12:01:44 UTC"
  }
]
[
  {
    "arxiv_id": "2406.06644v4",
    "title": "Latent Diffusion Model-Enabled Low-Latency Semantic Communication in the Presence of Semantic Ambiguities and Wireless Channel Noises",
    "authors": [
      "Jianhua Pei",
      "Cheng Feng",
      "Ping Wang",
      "Hina Tabassum",
      "Dongyuan Shi"
    ],
    "abstract": "Deep learning (DL)-based Semantic Communications (SemCom) is becoming\ncritical to maximize overall efficiency of communication networks.\nNevertheless, SemCom is sensitive to wireless channel uncertainties, source\noutliers, and suffer from poor generalization bottlenecks. To address the\nmentioned challenges, this paper develops a latent diffusion model-enabled\nSemCom system with three key contributions, i.e., i) to handle potential\noutliers in the source data, semantic errors obtained by projected gradient\ndescent based on the vulnerabilities of DL models, are utilized to update the\nparameters and obtain an outlier-robust encoder, ii) a lightweight single-layer\nlatent space transformation adapter completes one-shot learning at the\ntransmitter and is placed before the decoder at the receiver, enabling\nadaptation for out-of-distribution data and enhancing human-perceptual quality,\nand iii) an end-to-end consistency distillation (EECD) strategy is used to\ndistill the diffusion models trained in latent space, enabling deterministic\nsingle or few-step low-latency denoising in various noisy channels while\nmaintaining high semantic quality. Extensive numerical experiments across\ndifferent datasets demonstrate the superiority of the proposed SemCom system,\nconsistently proving its robustness to outliers, the capability to transmit\ndata with unknown distributions, and the ability to perform real-time channel\ndenoising tasks while preserving high human perceptual quality, outperforming\nthe existing denoising approaches in semantic metrics such as multi-scale\nstructural similarity index measure (MS-SSIM) and learned perceptual image path\nsimilarity (LPIPS).",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.06644v4",
    "published_date": "2024-06-09 23:39:31 UTC",
    "updated_date": "2025-02-14 11:57:02 UTC"
  },
  {
    "arxiv_id": "2407.10978v1",
    "title": "Building Artificial Intelligence with Creative Agency and Self-hood",
    "authors": [
      "Liane Gabora",
      "Joscha Bach"
    ],
    "abstract": "This paper is an invited layperson summary for The Academic of the paper\nreferenced on the last page. We summarize how the formal framework of\nautocatalytic networks offers a means of modeling the origins of\nself-organizing, self-sustaining structures that are sufficiently complex to\nreproduce and evolve, be they organisms undergoing biological evolution,\nnovelty-generating minds driving cultural evolution, or artificial intelligence\nnetworks such as large language models. The approach can be used to analyze and\ndetect phase transitions in vastly complex networks that have proven\nintractable with other approaches, and suggests a promising avenue to building\nan autonomous, agentic AI self. It seems reasonable to expect that such an\nautocatalytic AI would possess creative agency akin to that of humans, and\nundergo psychologically healing -- i.e., therapeutic -- internal transformation\nthrough engagement in creative tasks. Moreover, creative tasks would be\nexpected to help such an AI solidify its self-identity.",
    "categories": [
      "cs.AI",
      "q-bio.NC"
    ],
    "primary_category": "cs.AI",
    "comment": "5 pages; 1 figure. The Academic, May 20. 2024.\n  https://theacademic.com/building-artificial-intelligence-creative-agency-and-self-hood/",
    "pdf_url": "http://arxiv.org/pdf/2407.10978v1",
    "published_date": "2024-06-09 22:28:11 UTC",
    "updated_date": "2024-06-09 22:28:11 UTC"
  },
  {
    "arxiv_id": "2406.05925v2",
    "title": "Hello Again! LLM-powered Personalized Agent for Long-term Dialogue",
    "authors": [
      "Hao Li",
      "Chenghao Yang",
      "An Zhang",
      "Yang Deng",
      "Xiang Wang",
      "Tat-Seng Chua"
    ],
    "abstract": "Open-domain dialogue systems have seen remarkable advancements with the\ndevelopment of large language models (LLMs). Nonetheless, most existing\ndialogue systems predominantly focus on brief single-session interactions,\nneglecting the real-world demands for long-term companionship and personalized\ninteractions with chatbots. Crucial to addressing this real-world need are\nevent summary and persona management, which enable reasoning for appropriate\nlong-term dialogue responses. Recent progress in the human-like cognitive and\nreasoning capabilities of LLMs suggests that LLM-based agents could\nsignificantly enhance automated perception, decision-making, and\nproblem-solving. In response to this potential, we introduce a model-agnostic\nframework, the Long-term Dialogue Agent (LD-Agent), which incorporates three\nindependently tunable modules dedicated to event perception, persona\nextraction, and response generation. For the event memory module, long and\nshort-term memory banks are employed to separately focus on historical and\nongoing sessions, while a topic-based retrieval mechanism is introduced to\nenhance the accuracy of memory retrieval. Furthermore, the persona module\nconducts dynamic persona modeling for both users and agents. The integration of\nretrieved memories and extracted personas is subsequently fed into the\ngenerator to induce appropriate responses. The effectiveness, generality, and\ncross-domain capabilities of LD-Agent are empirically demonstrated across\nvarious illustrative benchmarks, models, and tasks. The code is released at\nhttps://github.com/leolee99/LD-Agent.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2406.05925v2",
    "published_date": "2024-06-09 21:58:32 UTC",
    "updated_date": "2025-02-13 18:02:34 UTC"
  },
  {
    "arxiv_id": "2406.05918v1",
    "title": "Why Don't Prompt-Based Fairness Metrics Correlate?",
    "authors": [
      "Abdelrahman Zayed",
      "Goncalo Mordido",
      "Ioana Baldini",
      "Sarath Chandar"
    ],
    "abstract": "The widespread use of large language models has brought up essential\nquestions about the potential biases these models might learn. This led to the\ndevelopment of several metrics aimed at evaluating and mitigating these biases.\nIn this paper, we first demonstrate that prompt-based fairness metrics exhibit\npoor agreement, as measured by correlation, raising important questions about\nthe reliability of fairness assessment using prompts. Then, we outline six\nrelevant reasons why such a low correlation is observed across existing\nmetrics. Based on these insights, we propose a method called Correlated\nFairness Output (CAIRO) to enhance the correlation between fairness metrics.\nCAIRO augments the original prompts of a given fairness metric by using several\npre-trained language models and then selects the combination of the augmented\nprompts that achieves the highest correlation across metrics. We show a\nsignificant improvement in Pearson correlation from 0.3 and 0.18 to 0.90 and\n0.98 across metrics for gender and religion biases, respectively. Our code is\navailable at https://github.com/chandar-lab/CAIRO.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "In Proceedings of ACL main 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.05918v1",
    "published_date": "2024-06-09 21:12:15 UTC",
    "updated_date": "2024-06-09 21:12:15 UTC"
  },
  {
    "arxiv_id": "2406.05912v1",
    "title": "BD-SAT: High-resolution Land Use Land Cover Dataset & Benchmark Results for Developing Division: Dhaka, BD",
    "authors": [
      "Ovi Paul",
      "Abu Bakar Siddik Nayem",
      "Anis Sarker",
      "Amin Ahsan Ali",
      "M Ashraful Amin",
      "AKM Mahbubur Rahman"
    ],
    "abstract": "Land Use Land Cover (LULC) analysis on satellite images using deep\nlearning-based methods is significantly helpful in understanding the geography,\nsocio-economic conditions, poverty levels, and urban sprawl in developing\ncountries. Recent works involve segmentation with LULC classes such as\nfarmland, built-up areas, forests, meadows, water bodies, etc. Training deep\nlearning methods on satellite images requires large sets of images annotated\nwith LULC classes. However, annotated data for developing countries are scarce\ndue to a lack of funding, absence of dedicated residential/industrial/economic\nzones, a large population, and diverse building materials. BD-SAT provides a\nhigh-resolution dataset that includes pixel-by-pixel LULC annotations for Dhaka\nmetropolitan city and surrounding rural/urban areas. Using a strict and\nstandardized procedure, the ground truth is created using Bing satellite\nimagery with a ground spatial distance of 2.22 meters per pixel. A three-stage,\nwell-defined annotation process has been followed with support from GIS experts\nto ensure the reliability of the annotations. We performed several experiments\nto establish benchmark results. The results show that the annotated BD-SAT is\nsufficient to train large deep learning models with adequate accuracy for five\nmajor LULC classes: forest, farmland, built-up areas, water bodies, and\nmeadows.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "26 pages, 15 figures and 12 tables",
    "pdf_url": "http://arxiv.org/pdf/2406.05912v1",
    "published_date": "2024-06-09 20:54:58 UTC",
    "updated_date": "2024-06-09 20:54:58 UTC"
  },
  {
    "arxiv_id": "2406.18595v1",
    "title": "Realtime Dynamic Gaze Target Tracking and Depth-Level Estimation",
    "authors": [
      "Esmaeil Seraj",
      "Harsh Bhate",
      "Walter Talamonti"
    ],
    "abstract": "The integration of Transparent Displays (TD) in various applications, such as\nHeads-Up Displays (HUDs) in vehicles, is a burgeoning field, poised to\nrevolutionize user experiences. However, this innovation brings forth\nsignificant challenges in realtime human-device interaction, particularly in\naccurately identifying and tracking a user's gaze on dynamically changing TDs.\nIn this paper, we present a two-fold robust and efficient systematic solution\nfor realtime gaze monitoring, comprised of: (1) a tree-based algorithm for\nidentifying and dynamically tracking gaze targets (i.e., moving, size-changing,\nand overlapping 2D content) projected on a transparent display, in realtime;\n(2) a multi-stream self-attention architecture to estimate the depth-level of\nhuman gaze from eye tracking data, to account for the display's transparency\nand preventing undesired interactions with the TD. We collected a real-world\neye-tracking dataset to train and test our gaze monitoring system. We present\nextensive results and ablation studies, including inference experiments on\nSystem on Chip (SoC) evaluation boards, demonstrating our model's scalability,\nprecision, and realtime feasibility in both static and dynamic contexts. Our\nsolution marks a significant stride in enhancing next-generation user-device\ninteraction and experience, setting a new benchmark for algorithmic gaze\nmonitoring technology in dynamic transparent displays.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.18595v1",
    "published_date": "2024-06-09 20:52:47 UTC",
    "updated_date": "2024-06-09 20:52:47 UTC"
  },
  {
    "arxiv_id": "2406.05906v1",
    "title": "TTM-RE: Memory-Augmented Document-Level Relation Extraction",
    "authors": [
      "Chufan Gao",
      "Xuan Wang",
      "Jimeng Sun"
    ],
    "abstract": "Document-level relation extraction aims to categorize the association between\nany two entities within a document. We find that previous methods for\ndocument-level relation extraction are ineffective in exploiting the full\npotential of large amounts of training data with varied noise levels. For\nexample, in the ReDocRED benchmark dataset, state-of-the-art methods trained on\nthe large-scale, lower-quality, distantly supervised training data generally do\nnot perform better than those trained solely on the smaller, high-quality,\nhuman-annotated training data. To unlock the full potential of large-scale\nnoisy training data for document-level relation extraction, we propose TTM-RE,\na novel approach that integrates a trainable memory module, known as the Token\nTuring Machine, with a noisy-robust loss function that accounts for the\npositive-unlabeled setting. Extensive experiments on ReDocRED, a benchmark\ndataset for document-level relation extraction, reveal that TTM-RE achieves\nstate-of-the-art performance (with an absolute F1 score improvement of over\n3%). Ablation studies further illustrate the superiority of TTM-RE in other\ndomains (the ChemDisGene dataset in the biomedical domain) and under highly\nunlabeled settings.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted in ACL 2024 Main",
    "pdf_url": "http://arxiv.org/pdf/2406.05906v1",
    "published_date": "2024-06-09 20:18:58 UTC",
    "updated_date": "2024-06-09 20:18:58 UTC"
  },
  {
    "arxiv_id": "2406.05902v1",
    "title": "Whose Preferences? Differences in Fairness Preferences and Their Impact on the Fairness of AI Utilizing Human Feedback",
    "authors": [
      "Emilia Agis Lerner",
      "Florian E. Dorner",
      "Elliott Ash",
      "Naman Goel"
    ],
    "abstract": "There is a growing body of work on learning from human feedback to align\nvarious aspects of machine learning systems with human values and preferences.\nWe consider the setting of fairness in content moderation, in which human\nfeedback is used to determine how two comments -- referencing different\nsensitive attribute groups -- should be treated in comparison to one another.\nWith a novel dataset collected from Prolific and MTurk, we find significant\ngaps in fairness preferences depending on the race, age, political stance,\neducational level, and LGBTQ+ identity of annotators. We also demonstrate that\ndemographics mentioned in text have a strong influence on how users perceive\nindividual fairness in moderation. Further, we find that differences also exist\nin downstream classifiers trained to predict human preferences. Finally, we\nobserve that an ensemble, giving equal weight to classifiers trained on\nannotations from different demographics, performs better for different\ndemographic intersections; compared to a single classifier that gives equal\nweight to each annotation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "To appear in the Proceedings of the 62nd Annual Meeting of the\n  Association for Computational Linguistics, ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.05902v1",
    "published_date": "2024-06-09 19:42:25 UTC",
    "updated_date": "2024-06-09 19:42:25 UTC"
  },
  {
    "arxiv_id": "2406.05898v2",
    "title": "Async Learned User Embeddings for Ads Delivery Optimization",
    "authors": [
      "Mingwei Tang",
      "Meng Liu",
      "Hong Li",
      "Junjie Yang",
      "Chenglin Wei",
      "Boyang Li",
      "Dai Li",
      "Rengan Xu",
      "Yifan Xu",
      "Zehua Zhang",
      "Xiangyu Wang",
      "Linfeng Liu",
      "Yuelei Xie",
      "Chengye Liu",
      "Labib Fawaz",
      "Li Li",
      "Hongnan Wang",
      "Bill Zhu",
      "Sri Reddy"
    ],
    "abstract": "In recommendation systems, high-quality user embeddings can capture subtle\npreferences, enable precise similarity calculations, and adapt to changing\npreferences over time to maintain relevance. The effectiveness of\nrecommendation systems depends on the quality of user embedding. We propose to\nasynchronously learn high fidelity user embeddings for billions of users each\nday from sequence based multimodal user activities through a Transformer-like\nlarge scale feature learning module. The async learned user representations\nembeddings (ALURE) are further converted to user similarity graphs through\ngraph learning and then combined with user realtime activities to retrieval\nhighly related ads candidates for the ads delivery system. Our method shows\nsignificant gains in both offline and online experiments.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted by workshop on Multimodal Representation and Retrieval at\n  SIGIR 2024, Washington DC",
    "pdf_url": "http://arxiv.org/pdf/2406.05898v2",
    "published_date": "2024-06-09 19:35:20 UTC",
    "updated_date": "2024-06-23 05:43:41 UTC"
  },
  {
    "arxiv_id": "2406.05887v1",
    "title": "Few-Shot Load Forecasting Under Data Scarcity in Smart Grids: A Meta-Learning Approach",
    "authors": [
      "Georgios Tsoumplekas",
      "Christos L. Athanasiadis",
      "Dimitrios I. Doukas",
      "Antonios Chrysopoulos",
      "Pericles A. Mitkas"
    ],
    "abstract": "Despite the rapid expansion of smart grids and large volumes of data at the\nindividual consumer level, there are still various cases where adequate data\ncollection to train accurate load forecasting models is challenging or even\nimpossible. This paper proposes adapting an established model-agnostic\nmeta-learning algorithm for short-term load forecasting in the context of\nfew-shot learning. Specifically, the proposed method can rapidly adapt and\ngeneralize within any unknown load time series of arbitrary length using only\nminimal training samples. In this context, the meta-learning model learns an\noptimal set of initial parameters for a base-level learner recurrent neural\nnetwork. The proposed model is evaluated using a dataset of historical load\nconsumption data from real-world consumers. Despite the examined load series'\nshort length, it produces accurate forecasts outperforming transfer learning\nand task-specific machine learning methods by $12.5\\%$. To enhance robustness\nand fairness during model evaluation, a novel metric, mean average log\npercentage error, is proposed that alleviates the bias introduced by the\ncommonly used MAPE metric. Finally, a series of studies to evaluate the model's\nrobustness under different hyperparameters and time series lengths is also\nconducted, demonstrating that the proposed approach consistently outperforms\nall other models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This work has been submitted to the IEEE for possible publication",
    "pdf_url": "http://arxiv.org/pdf/2406.05887v1",
    "published_date": "2024-06-09 18:59:08 UTC",
    "updated_date": "2024-06-09 18:59:08 UTC"
  },
  {
    "arxiv_id": "2406.05873v1",
    "title": "Conserving Human Creativity with Evolutionary Generative Algorithms: A Case Study in Music Generation",
    "authors": [
      "Justin Kilb",
      "Caroline Ellis"
    ],
    "abstract": "This study explores the application of evolutionary generative algorithms in\nmusic production to preserve and enhance human creativity. By integrating human\nfeedback into Differential Evolution algorithms, we produced six songs that\nwere submitted to international record labels, all of which received contract\noffers. In addition to testing the commercial viability of these methods, this\npaper examines the long-term implications of content generation using\ntraditional machine learning methods compared with evolutionary algorithms.\nSpecifically, as current generative techniques continue to scale, the potential\nfor computer-generated content to outpace human creation becomes likely. This\ntrend poses a risk of exhausting the pool of human-created training data,\npotentially forcing generative machine learning models to increasingly depend\non their random input functions for generating novel content. In contrast to a\nfuture of content generation guided by aimless random functions, our approach\nallows for individualized creative exploration, ensuring that computer-assisted\ncontent generation methods are human-centric and culturally relevant through\ntime.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.NE",
    "comment": "7 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.05873v1",
    "published_date": "2024-06-09 18:11:05 UTC",
    "updated_date": "2024-06-09 18:11:05 UTC"
  },
  {
    "arxiv_id": "2406.05872v1",
    "title": "STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models",
    "authors": [
      "Shreyas Basavatia",
      "Keerthiram Murugesan",
      "Shivam Ratnakar"
    ],
    "abstract": "Interactive fiction games have emerged as an important application to improve\nthe generalization capabilities of language-based reinforcement learning (RL)\nagents. Existing environments for interactive fiction games are domain-specific\nor time-consuming to generate and do not train the RL agents to master a\nspecific set of skills. In this work, we introduce an interactive environment\nfor self-supervised RL, STARLING, for text-based games that bootstraps the\ntext-based RL agents with automatically generated games (based on the seed set\nof game ideas) to boost the performance and generalization capabilities to\nreach a goal of the target environment. These games let the agent hone their\nskills on a predefined set of tasks. We create and test an environment with 100\ngames, generated using this automated framework that uses large language models\n(GPT-3) and an interactive fiction game engine (based on Inform7) to provide\nthe user with the ability to generate more games under minimal human\nsupervision. Experimental results based on both the human participants and\nbaseline text-based RL agents reveal that current state-of-the-art text-based\nRL agents cannot use previously learned skills in new situations at the level\nhumans can. These results enforce STARLING's potential to serve as a sandbox\nenvironment for further research in self-supervised text-based RL.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "ACL 2024 (Findings)",
    "pdf_url": "http://arxiv.org/pdf/2406.05872v1",
    "published_date": "2024-06-09 18:07:47 UTC",
    "updated_date": "2024-06-09 18:07:47 UTC"
  },
  {
    "arxiv_id": "2406.05866v1",
    "title": "Procrastination Is All You Need: Exponent Indexed Accumulators for Floating Point, Posits and Logarithmic Numbers",
    "authors": [
      "Vincenzo Liguori"
    ],
    "abstract": "This paper discusses a simple and effective method for the summation of long\nsequences of floating point numbers. The method comprises two phases: an\naccumulation phase where the mantissas of the floating point numbers are added\nto accumulators indexed by the exponents and a reconstruction phase where the\nactual summation result is finalised. Various architectural details are given\nfor both FPGAs and ASICs including fusing the operation with a multiplier,\ncreating efficient MACs. Some results are presented for FPGAs, including a\ntensor core capable of multiplying and accumulating two 4x4 matrices of\nbfloat16 values every clock cycle using ~6,400 LUTs + 64 DSP48 in AMD FPGAs at\n700+ MHz. The method is then extended to posits and logarithmic numbers.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05866v1",
    "published_date": "2024-06-09 17:44:17 UTC",
    "updated_date": "2024-06-09 17:44:17 UTC"
  },
  {
    "arxiv_id": "2406.05862v3",
    "title": "II-Bench: An Image Implication Understanding Benchmark for Multimodal Large Language Models",
    "authors": [
      "Ziqiang Liu",
      "Feiteng Fang",
      "Xi Feng",
      "Xinrun Du",
      "Chenhao Zhang",
      "Zekun Wang",
      "Yuelin Bai",
      "Qixuan Zhao",
      "Liyang Fan",
      "Chengguang Gan",
      "Hongquan Lin",
      "Jiaming Li",
      "Yuansheng Ni",
      "Haihong Wu",
      "Yaswanth Narsupalli",
      "Zhigang Zheng",
      "Chengming Li",
      "Xiping Hu",
      "Ruifeng Xu",
      "Xiaojun Chen",
      "Min Yang",
      "Jiaheng Liu",
      "Ruibo Liu",
      "Wenhao Huang",
      "Ge Zhang",
      "Shiwen Ni"
    ],
    "abstract": "The rapid advancements in the development of multimodal large language models\n(MLLMs) have consistently led to new breakthroughs on various benchmarks. In\nresponse, numerous challenging and comprehensive benchmarks have been proposed\nto more accurately assess the capabilities of MLLMs. However, there is a dearth\nof exploration of the higher-order perceptual capabilities of MLLMs. To fill\nthis gap, we propose the Image Implication understanding Benchmark, II-Bench,\nwhich aims to evaluate the model's higher-order perception of images. Through\nextensive experiments on II-Bench across multiple MLLMs, we have made\nsignificant findings. Initially, a substantial gap is observed between the\nperformance of MLLMs and humans on II-Bench. The pinnacle accuracy of MLLMs\nattains 74.8%, whereas human accuracy averages 90%, peaking at an impressive\n98%. Subsequently, MLLMs perform worse on abstract and complex images,\nsuggesting limitations in their ability to understand high-level semantics and\ncapture image details. Finally, it is observed that most models exhibit\nenhanced accuracy when image sentiment polarity hints are incorporated into the\nprompts. This observation underscores a notable deficiency in their inherent\nunderstanding of image sentiment. We believe that II-Bench will inspire the\ncommunity to develop the next generation of MLLMs, advancing the journey\ntowards expert artificial general intelligence (AGI). II-Bench is publicly\navailable at https://huggingface.co/datasets/m-a-p/II-Bench.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "100 pages, 82 figures, add citations",
    "pdf_url": "http://arxiv.org/pdf/2406.05862v3",
    "published_date": "2024-06-09 17:25:47 UTC",
    "updated_date": "2025-01-13 09:33:47 UTC"
  },
  {
    "arxiv_id": "2406.05855v2",
    "title": "Self-Distilled Disentangled Learning for Counterfactual Prediction",
    "authors": [
      "Xinshu Li",
      "Mingming Gong",
      "Lina Yao"
    ],
    "abstract": "The advancements in disentangled representation learning significantly\nenhance the accuracy of counterfactual predictions by granting precise control\nover instrumental variables, confounders, and adjustable variables. An\nappealing method for achieving the independent separation of these factors is\nmutual information minimization, a task that presents challenges in numerous\nmachine learning scenarios, especially within high-dimensional spaces. To\ncircumvent this challenge, we propose the Self-Distilled Disentanglement\nframework, referred to as $SD^2$. Grounded in information theory, it ensures\ntheoretically sound independent disentangled representations without intricate\nmutual information estimator designs for high-dimensional representations. Our\ncomprehensive experiments, conducted on both synthetic and real-world datasets,\nconfirms the effectiveness of our approach in facilitating counterfactual\ninference in the presence of both observed and unobserved confounders.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05855v2",
    "published_date": "2024-06-09 16:58:19 UTC",
    "updated_date": "2024-06-14 06:30:22 UTC"
  },
  {
    "arxiv_id": "2406.05839v2",
    "title": "MaLa-ASR: Multimedia-Assisted LLM-Based ASR",
    "authors": [
      "Guanrou Yang",
      "Ziyang Ma",
      "Fan Yu",
      "Zhifu Gao",
      "Shiliang Zhang",
      "Xie Chen"
    ],
    "abstract": "As more and more information-rich data like video become available, utilizing\nmulti-modal auxiliary information to enhance audio tasks has sparked widespread\nresearch interest. The recent surge in research on LLM-based audio models\nprovides fresh perspectives for tackling audio tasks. Given that LLM can\nflexibly ingest multiple inputs, we propose MaLa-ASR, an LLM-based ASR model\nthat can integrate textual keywords extracted from presentation slides to\nimprove recognition of conference content. MaLa-ASR yields average WERs of 9.4%\nand 11.7% on the L95 and S95 subsets of the SlideSpeech corpus, representing a\nsignificant relative WER drop of 27.9% and 44.7% over the baseline model\nreported in SlideSpeech. MaLa-ASR underscores LLM's strong performance in\nspeech tasks and the capability to integrate auxiliary information\nconveniently. By adding keywords to the input prompt, the biased word error\nrate (B-WER) reduces relatively by 46.0% and 44.2%, establishing a new SOTA on\nthis dataset.",
    "categories": [
      "eess.AS",
      "cs.AI"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05839v2",
    "published_date": "2024-06-09 16:00:00 UTC",
    "updated_date": "2024-06-13 07:50:40 UTC"
  },
  {
    "arxiv_id": "2406.05837v1",
    "title": "Solution for CVPR 2024 UG2+ Challenge Track on All Weather Semantic Segmentation",
    "authors": [
      "Jun Yu",
      "Yunxiang Zhang",
      "Fengzhao Sun",
      "Leilei Wang",
      "Renjie Lu"
    ],
    "abstract": "In this report, we present our solution for the semantic segmentation in\nadverse weather, in UG2+ Challenge at CVPR 2024. To achieve robust and accurate\nsegmentation results across various weather conditions, we initialize the\nInternImage-H backbone with pre-trained weights from the large-scale joint\ndataset and enhance it with the state-of-the-art Upernet segmentation method.\nSpecifically, we utilize offline and online data augmentation approaches to\nextend the train set, which helps us to further improve the performance of the\nsegmenter. As a result, our proposed solution demonstrates advanced performance\non the test set and achieves 3rd position in this challenge.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Solution for CVPR 2024 UG2+ Challenge Track on All Weather Semantic\n  Segmentation",
    "pdf_url": "http://arxiv.org/pdf/2406.05837v1",
    "published_date": "2024-06-09 15:56:35 UTC",
    "updated_date": "2024-06-09 15:56:35 UTC"
  },
  {
    "arxiv_id": "2406.05828v1",
    "title": "Multi-Stain Multi-Level Convolutional Network for Multi-Tissue Breast Cancer Image Segmentation",
    "authors": [
      "Akash Modi",
      "Sumit Kumar Jha",
      "Purnendu Mishra",
      "Rajiv Kumar",
      "Kiran Aatre",
      "Gursewak Singh",
      "Shubham Mathur"
    ],
    "abstract": "Digital pathology and microscopy image analysis are widely employed in the\nsegmentation of digitally scanned IHC slides, primarily to identify cancer and\npinpoint regions of interest (ROI) indicative of tumor presence. However,\ncurrent ROI segmentation models are either stain-specific or suffer from the\nissues of stain and scanner variance due to different staining protocols or\nmodalities across multiple labs. Also, tissues like Ductal Carcinoma in Situ\n(DCIS), acini, etc. are often classified as Tumors due to their structural\nsimilarities and color compositions. In this paper, we proposed a novel\nconvolutional neural network (CNN) based Multi-class Tissue Segmentation model\nfor histopathology whole-slide Breast slides which classify tumors and segments\nother tissue regions such as Ducts, acini, DCIS, Squamous epithelium, Blood\nVessels, Necrosis, etc. as a separate class. Our unique pixel-aligned\nnon-linear merge across spatial resolutions empowers models with both local and\nglobal fields of view for accurate detection of various classes. Our proposed\nmodel is also able to separate bad regions such as folds, artifacts, blurry\nregions, bubbles, etc. from tissue regions using multi-level context from\ndifferent resolutions of WSI. Multi-phase iterative training with context-aware\naugmentation and increasing noise was used to efficiently train a multi-stain\ngeneric model with partial and noisy annotations from 513 slides. Our training\npipeline used 12 million patches generated using context-aware augmentations\nwhich made our model stain and scanner invariant across data sources. To\nextrapolate stain and scanner invariance, our model was evaluated on 23000\npatches which were for a completely new stain (Hematoxylin and Eosin) from a\ncompletely new scanner (Motic) from a different lab. The mean IOU was 0.72\nwhich is on par with model performance on other data sources and scanners.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05828v1",
    "published_date": "2024-06-09 15:35:49 UTC",
    "updated_date": "2024-06-09 15:35:49 UTC"
  },
  {
    "arxiv_id": "2406.05826v2",
    "title": "PSBD: Prediction Shift Uncertainty Unlocks Backdoor Detection",
    "authors": [
      "Wei Li",
      "Pin-Yu Chen",
      "Sijia Liu",
      "Ren Wang"
    ],
    "abstract": "Deep neural networks are susceptible to backdoor attacks, where adversaries\nmanipulate model predictions by inserting malicious samples into the training\ndata. Currently, there is still a significant challenge in identifying\nsuspicious training data to unveil potential backdoor samples. In this paper,\nwe propose a novel method, Prediction Shift Backdoor Detection (PSBD),\nleveraging an uncertainty-based approach requiring minimal unlabeled clean\nvalidation data. PSBD is motivated by an intriguing Prediction Shift (PS)\nphenomenon, where poisoned models' predictions on clean data often shift away\nfrom true labels towards certain other labels with dropout applied during\ninference, while backdoor samples exhibit less PS. We hypothesize PS results\nfrom the neuron bias effect, making neurons favor features of certain classes.\nPSBD identifies backdoor training samples by computing the Prediction Shift\nUncertainty (PSU), the variance in probability values when dropout layers are\ntoggled on and off during model inference. Extensive experiments have been\nconducted to verify the effectiveness and efficiency of PSBD, which achieves\nstate-of-the-art results among mainstream detection methods. The code is\navailable at https://github.com/WL-619/PSBD.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05826v2",
    "published_date": "2024-06-09 15:31:00 UTC",
    "updated_date": "2025-04-16 02:39:44 UTC"
  },
  {
    "arxiv_id": "2406.05814v2",
    "title": "TIGeR: Unifying Text-to-Image Generation and Retrieval with Large Multimodal Models",
    "authors": [
      "Leigang Qu",
      "Haochuan Li",
      "Tan Wang",
      "Wenjie Wang",
      "Yongqi Li",
      "Liqiang Nie",
      "Tat-Seng Chua"
    ],
    "abstract": "How humans can effectively and efficiently acquire images has always been a\nperennial question. A classic solution is text-to-image retrieval from an\nexisting database; however, the limited database typically lacks creativity. By\ncontrast, recent breakthroughs in text-to-image generation have made it\npossible to produce attractive and counterfactual visual content, but it faces\nchallenges in synthesizing knowledge-intensive images. In this work, we rethink\nthe relationship between text-to-image generation and retrieval, proposing a\nunified framework for both tasks with one single Large Multimodal Model (LMM).\nSpecifically, we first explore the intrinsic discriminative abilities of LMMs\nand introduce an efficient generative retrieval method for text-to-image\nretrieval in a training-free manner. Subsequently, we unify generation and\nretrieval autoregressively and propose an autonomous decision mechanism to\nchoose the best-matched one between generated and retrieved images as the\nresponse to the text prompt. To standardize the evaluation of unified\ntext-to-image generation and retrieval, we construct TIGeR-Bench, a benchmark\nspanning both creative and knowledge-intensive domains. Extensive experiments\non TIGeR-Bench and two retrieval benchmarks, i.e., Flickr30K and MS-COCO,\ndemonstrate the superiority of our proposed framework.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "ICLR 2025 Camera-ready",
    "pdf_url": "http://arxiv.org/pdf/2406.05814v2",
    "published_date": "2024-06-09 15:00:28 UTC",
    "updated_date": "2025-03-24 23:07:01 UTC"
  },
  {
    "arxiv_id": "2406.05812v1",
    "title": "Seventeenth-Century Spanish American Notary Records for Fine-Tuning Spanish Large Language Models",
    "authors": [
      "Shraboni Sarker",
      "Ahmad Tamim Hamad",
      "Hulayyil Alshammari",
      "Viviana Grieco",
      "Praveen Rao"
    ],
    "abstract": "Large language models have gained tremendous popularity in domains such as\ne-commerce, finance, healthcare, and education. Fine-tuning is a common\napproach to customize an LLM on a domain-specific dataset for a desired\ndownstream task. In this paper, we present a valuable resource for fine-tuning\nLLMs developed for the Spanish language to perform a variety of tasks such as\nclassification, masked language modeling, clustering, and others. Our resource\nis a collection of handwritten notary records from the seventeenth century\nobtained from the National Archives of Argentina. This collection contains a\ncombination of original images and transcribed text (and metadata) of 160+\npages that were handwritten by two notaries, namely, Estenban Agreda de Vergara\nand Nicolas de Valdivia y Brisuela nearly 400 years ago. Through empirical\nevaluation, we demonstrate that our collection can be used to fine-tune Spanish\nLLMs for tasks such as classification and masked language modeling, and can\noutperform pre-trained Spanish models and ChatGPT-3.5/ChatGPT-4o. Our resource\nwill be an invaluable resource for historical text analysis and is publicly\navailable on GitHub.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05812v1",
    "published_date": "2024-06-09 14:54:22 UTC",
    "updated_date": "2024-06-09 14:54:22 UTC"
  },
  {
    "arxiv_id": "2406.05805v1",
    "title": "Toward identifiability of total effects in summary causal graphs with latent confounders: an extension of the front-door criterion",
    "authors": [
      "Charles K. Assaad"
    ],
    "abstract": "Conducting experiments to estimate total effects can be challenging due to\ncost, ethical concerns, or practical limitations. As an alternative,\nresearchers often rely on causal graphs to determine if it is possible to\nidentify these effects from observational data. Identifying total effects in\nfully specified non-temporal causal graphs has garnered considerable attention,\nwith Pearl's front-door criterion enabling the identification of total effects\nin the presence of latent confounding even when no variable set is sufficient\nfor adjustment. However, specifying a complete causal graph is challenging in\nmany domains. Extending these identifiability results to partially specified\ngraphs is crucial, particularly in dynamic systems where causal relationships\nevolve over time. This paper addresses the challenge of identifying total\neffects using a specific and well-known partially specified graph in dynamic\nsystems called a summary causal graph, which does not specify the temporal lag\nbetween causal relations and can contain cycles. In particular, this paper\npresents sufficient graphical conditions for identifying total effects from\nobservational data, even in the presence of hidden confounding and when no\nvariable set is sufficient for adjustment, contributing to the ongoing effort\nto understand and estimate causal effects from observational data using summary\ncausal graphs.",
    "categories": [
      "stat.ME",
      "cs.AI"
    ],
    "primary_category": "stat.ME",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05805v1",
    "published_date": "2024-06-09 14:43:06 UTC",
    "updated_date": "2024-06-09 14:43:06 UTC"
  },
  {
    "arxiv_id": "2406.05804v6",
    "title": "A Review of Prominent Paradigms for LLM-Based Agents: Tool Use (Including RAG), Planning, and Feedback Learning",
    "authors": [
      "Xinzhe Li"
    ],
    "abstract": "Tool use, planning, and feedback learning are currently three prominent\nparadigms for developing Large Language Model (LLM)-based agents across various\ntasks. Although numerous frameworks have been devised for each paradigm, their\nintricate workflows and inconsistent taxonomy create challenges in\nunderstanding and reviewing the frameworks across different paradigms. This\nsurvey introduces a unified taxonomy to systematically review and discuss these\nframeworks. Specifically, 1) the taxonomy defines environments/tasks, common\nLLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models),\nand universally applicable workflows found in prior work, and 2) it enables a\ncomparison of key perspectives on the implementations of LMPRs and workflow\ndesigns across different agent paradigms and frameworks. 3) Finally, we\nidentify three limitations in existing workflow designs and systematically\ndiscuss the future work. Resources have been made publicly available at in our\nGitHub repository https://github.com/xinzhel/LLM-Agent-Survey.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "CoLing 2025 Camera Ready (extended to 9 pages)",
    "pdf_url": "http://arxiv.org/pdf/2406.05804v6",
    "published_date": "2024-06-09 14:42:55 UTC",
    "updated_date": "2024-11-30 22:38:57 UTC"
  },
  {
    "arxiv_id": "2406.05802v1",
    "title": "SAM-PM: Enhancing Video Camouflaged Object Detection using Spatio-Temporal Attention",
    "authors": [
      "Muhammad Nawfal Meeran",
      "Gokul Adethya T",
      "Bhanu Pratyush Mantha"
    ],
    "abstract": "In the domain of large foundation models, the Segment Anything Model (SAM)\nhas gained notable recognition for its exceptional performance in image\nsegmentation. However, tackling the video camouflage object detection (VCOD)\ntask presents a unique challenge. Camouflaged objects typically blend into the\nbackground, making them difficult to distinguish in still images. Additionally,\nensuring temporal consistency in this context is a challenging problem. As a\nresult, SAM encounters limitations and falls short when applied to the VCOD\ntask. To overcome these challenges, we propose a new method called the SAM\nPropagation Module (SAM-PM). Our propagation module enforces temporal\nconsistency within SAM by employing spatio-temporal cross-attention mechanisms.\nMoreover, we exclusively train the propagation module while keeping the SAM\nnetwork weights frozen, allowing us to integrate task-specific insights with\nthe vast knowledge accumulated by the large model. Our method effectively\nincorporates temporal consistency and domain-specific expertise into the\nsegmentation network with an addition of less than 1% of SAM's parameters.\nExtensive experimentation reveals a substantial performance improvement in the\nVCOD benchmark when compared to the most recent state-of-the-art techniques.\nCode and pre-trained weights are open-sourced at\nhttps://github.com/SpiderNitt/SAM-PM",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05802v1",
    "published_date": "2024-06-09 14:33:38 UTC",
    "updated_date": "2024-06-09 14:33:38 UTC"
  },
  {
    "arxiv_id": "2406.05798v1",
    "title": "Hidden Holes: topological aspects of language models",
    "authors": [
      "Stephen Fitz",
      "Peter Romero",
      "Jiyan Jonas Schneider"
    ],
    "abstract": "We explore the topology of representation manifolds arising in autoregressive\nneural language models trained on raw text data. In order to study their\nproperties, we introduce tools from computational algebraic topology, which we\nuse as a basis for a measure of topological complexity, that we call\nperforation.\n  Using this measure, we study the evolution of topological structure in GPT\nbased large language models across depth and time during training. We then\ncompare these to gated recurrent models, and show that the latter exhibit more\ntopological complexity, with a distinct pattern of changes common to all\nnatural languages but absent from synthetically generated data. The paper\npresents a detailed analysis of the representation manifolds derived by these\nmodels based on studying the shapes of vector clouds induced by them as they\nare conditioned on sentences from corpora of natural language text.\n  The methods developed in this paper are novel in the field and based on\nmathematical apparatus that might be unfamiliar to the target audience. To help\nwith that we introduce the minimum necessary theory, and provide additional\nvisualizations in the appendices.\n  The main contribution of the paper is a striking observation about the\ntopological structure of the transformer as compared to LSTM based neural\narchitectures. It suggests that further research into mathematical properties\nof these neural networks is necessary to understand the operation of large\ntransformer language models. We hope this work inspires further explorations in\nthis direction within the NLP community.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05798v1",
    "published_date": "2024-06-09 14:25:09 UTC",
    "updated_date": "2024-06-09 14:25:09 UTC"
  },
  {
    "arxiv_id": "2406.05797v2",
    "title": "3D-MolT5: Leveraging Discrete Structural Information for Molecule-Text Modeling",
    "authors": [
      "Qizhi Pei",
      "Rui Yan",
      "Kaiyuan Gao",
      "Jinhua Zhu",
      "Lijun Wu"
    ],
    "abstract": "The integration of molecular and natural language representations has emerged\nas a focal point in molecular science, with recent advancements in Language\nModels (LMs) demonstrating significant potential for comprehensive modeling of\nboth domains. However, existing approaches face notable limitations,\nparticularly in their neglect of three-dimensional (3D) information, which is\ncrucial for understanding molecular structures and functions. While some\nefforts have been made to incorporate 3D molecular information into LMs using\nexternal structure encoding modules, significant difficulties remain, such as\ninsufficient interaction across modalities in pre-training and challenges in\nmodality alignment. To address the limitations, we propose \\textbf{3D-MolT5}, a\nunified framework designed to model molecule in both sequence and 3D structure\nspaces. The key innovation of our approach lies in mapping fine-grained 3D\nsubstructure representations into a specialized 3D token vocabulary. This\nmethodology facilitates the seamless integration of sequence and structure\nrepresentations in a tokenized format, enabling 3D-MolT5 to encode molecular\nsequences, molecular structures, and text sequences within a unified\narchitecture. Leveraging this tokenized input strategy, we build a foundation\nmodel that unifies the sequence and structure data formats. We then conduct\njoint pre-training with multi-task objectives to enhance the model's\ncomprehension of these diverse modalities within a shared representation space.\nThus, our approach significantly improves cross-modal interaction and\nalignment, addressing key challenges in previous work. Further instruction\ntuning demonstrated that our 3D-MolT5 has strong generalization ability and\nsurpasses existing methods with superior performance in multiple downstream\ntasks. Our code is available at https://github.com/QizhiPei/3D-MolT5.",
    "categories": [
      "q-bio.BM",
      "cs.AI",
      "cs.CE",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "q-bio.BM",
    "comment": "Accepted by ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2406.05797v2",
    "published_date": "2024-06-09 14:20:55 UTC",
    "updated_date": "2025-03-18 08:03:45 UTC"
  },
  {
    "arxiv_id": "2406.05794v3",
    "title": "RE-RAG: Improving Open-Domain QA Performance and Interpretability with Relevance Estimator in Retrieval-Augmented Generation",
    "authors": [
      "Kiseung Kim",
      "Jay-Yoon Lee"
    ],
    "abstract": "The Retrieval Augmented Generation (RAG) framework utilizes a combination of\nparametric knowledge and external knowledge to demonstrate state-of-the-art\nperformance on open-domain question answering tasks. However, the RAG framework\nsuffers from performance degradation when the query is accompanied by\nirrelevant contexts. In this work, we propose the RE-RAG framework, which\nintroduces a relevance estimator (RE) that not only provides relative relevance\nbetween contexts as previous rerankers did, but also provides confidence, which\ncan be used to classify whether given context is useful for answering the given\nquestion. We propose a weakly supervised method for training the RE simply\nutilizing question-answer data without any labels for correct contexts. We show\nthat RE trained with a small generator (sLM) can not only improve the sLM\nfine-tuned together with RE but also improve previously unreferenced large\nlanguage models (LLMs). Furthermore, we investigate new decoding strategies\nthat utilize the proposed confidence measured by RE such as choosing to let the\nuser know that it is \"unanswerable\" to answer the question given the retrieved\ncontexts or choosing to rely on LLM's parametric knowledge rather than\nunrelated contexts.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05794v3",
    "published_date": "2024-06-09 14:11:19 UTC",
    "updated_date": "2024-10-24 14:57:52 UTC"
  },
  {
    "arxiv_id": "2406.05768v6",
    "title": "TLCM: Training-efficient Latent Consistency Model for Image Generation with 2-8 Steps",
    "authors": [
      "Qingsong Xie",
      "Zhenyi Liao",
      "Zhijie Deng",
      "Chen chen",
      "Haonan Lu"
    ],
    "abstract": "Distilling latent diffusion models (LDMs) into ones that are fast to sample\nfrom is attracting growing research interest. However, the majority of existing\nmethods face two critical challenges: (1) They hinge on long training using a\nhuge volume of real data. (2) They routinely lead to quality degradation for\ngeneration, especially in text-image alignment. This paper proposes a novel\ntraining-efficient Latent Consistency Model (TLCM) to overcome these\nchallenges. Our method first accelerates LDMs via data-free multistep latent\nconsistency distillation (MLCD), and then data-free latent consistency\ndistillation is proposed to efficiently guarantee the inter-segment consistency\nin MLCD. Furthermore, we introduce bags of techniques, e.g., distribution\nmatching, adversarial learning, and preference learning, to enhance TLCM's\nperformance at few-step inference without any real data. TLCM demonstrates a\nhigh level of flexibility by enabling adjustment of sampling steps within the\nrange of 2 to 8 while still producing competitive outputs compared to full-step\napproaches. Notably, TLCM enjoys the data-free merit by employing synthetic\ndata from the teacher for distillation. With just 70 training hours on an A100\nGPU, a 3-step TLCM distilled from SDXL achieves an impressive CLIP Score of\n33.68 and an Aesthetic Score of 5.97 on the MSCOCO-2017 5K benchmark,\nsurpassing various accelerated models and even outperforming the teacher model\nin human preference metrics. We also demonstrate the versatility of TLCMs in\napplications including image style transfer, controllable generation, and\nChinese-to-image generation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05768v6",
    "published_date": "2024-06-09 12:55:50 UTC",
    "updated_date": "2024-11-07 01:29:26 UTC"
  },
  {
    "arxiv_id": "2406.05766v2",
    "title": "Set-CLIP: Exploring Aligned Semantic From Low-Alignment Multimodal Data Through A Distribution View",
    "authors": [
      "Zijia Song",
      "Zelin Zang",
      "Yelin Wang",
      "Guozheng Yang",
      "Kaicheng yu",
      "Wanyu Chen",
      "Miaoyu Wang",
      "Stan Z. Li"
    ],
    "abstract": "Multimodal fusion breaks through the boundaries between diverse modalities\nand has already achieved notable performances. However, in many specialized\nfields, it is struggling to obtain sufficient alignment data for training,\nwhich seriously limits the use of previously effective models. Therefore,\nsemi-supervised learning approaches are attempted to facilitate multimodal\nalignment by learning from low-alignment data with fewer matched pairs, but\ntraditional techniques like pseudo-labeling may run into troubles in the\nlabel-deficient scenarios. To tackle these challenges, we reframe\nsemi-supervised multimodal alignment as a manifold matching issue and propose a\nnew methodology based on CLIP, termed Set-CLIP. Specifically, by designing a\nnovel semantic density distribution loss, we constrain the latent\nrepresentation distribution with fine granularity and extract implicit semantic\nalignment from unpaired multimodal data, thereby reducing the reliance on\nnumerous strictly matched pairs. Furthermore, we apply coarse-grained modality\nadaptation and unimodal self-supervised guidance to narrow the gaps between\nmodality spaces and improve the stability of representation distributions.\nExtensive experiments conducted on a range of tasks in various fields,\nincluding protein analysis, remote sensing, and the general vision-language\nfield, validate the efficacy of our proposed Set-CLIP method. Especially with\nno paired data for supervised training, Set-CLIP is still outstanding, which\nbrings an improvement of 144.83% over CLIP.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05766v2",
    "published_date": "2024-06-09 12:41:14 UTC",
    "updated_date": "2024-09-21 09:50:33 UTC"
  },
  {
    "arxiv_id": "2406.05764v1",
    "title": "Global Sensitivity Analysis of Uncertain Parameters in Bayesian Networks",
    "authors": [
      "Rafael Ballester-Ripoll",
      "Manuele Leonelli"
    ],
    "abstract": "Traditionally, the sensitivity analysis of a Bayesian network studies the\nimpact of individually modifying the entries of its conditional probability\ntables in a one-at-a-time (OAT) fashion. However, this approach fails to give a\ncomprehensive account of each inputs' relevance, since simultaneous\nperturbations in two or more parameters often entail higher-order effects that\ncannot be captured by an OAT analysis. We propose to conduct global\nvariance-based sensitivity analysis instead, whereby $n$ parameters are viewed\nas uncertain at once and their importance is assessed jointly. Our method works\nby encoding the uncertainties as $n$ additional variables of the network. To\nprevent the curse of dimensionality while adding these dimensions, we use\nlow-rank tensor decomposition to break down the new potentials into smaller\nfactors. Last, we apply the method of Sobol to the resulting network to obtain\n$n$ global sensitivity indices. Using a benchmark array of both expert-elicited\nand learned Bayesian networks, we demonstrate that the Sobol indices can\nsignificantly differ from the OAT indices, thus revealing the true influence of\nuncertain parameters and their interactions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05764v1",
    "published_date": "2024-06-09 12:36:38 UTC",
    "updated_date": "2024-06-09 12:36:38 UTC"
  },
  {
    "arxiv_id": "2406.05756v1",
    "title": "EmbSpatial-Bench: Benchmarking Spatial Understanding for Embodied Tasks with Large Vision-Language Models",
    "authors": [
      "Mengfei Du",
      "Binhao Wu",
      "Zejun Li",
      "Xuanjing Huang",
      "Zhongyu Wei"
    ],
    "abstract": "The recent rapid development of Large Vision-Language Models (LVLMs) has\nindicated their potential for embodied tasks.However, the critical skill of\nspatial understanding in embodied environments has not been thoroughly\nevaluated, leaving the gap between current LVLMs and qualified embodied\nintelligence unknown. Therefore, we construct EmbSpatial-Bench, a benchmark for\nevaluating embodied spatial understanding of LVLMs.The benchmark is\nautomatically derived from embodied scenes and covers 6 spatial relationships\nfrom an egocentric perspective.Experiments expose the insufficient capacity of\ncurrent LVLMs (even GPT-4V). We further present EmbSpatial-SFT, an\ninstruction-tuning dataset designed to improve LVLMs' embodied spatial\nunderstanding.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.MM"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by ACL 2024 Main",
    "pdf_url": "http://arxiv.org/pdf/2406.05756v1",
    "published_date": "2024-06-09 12:23:14 UTC",
    "updated_date": "2024-06-09 12:23:14 UTC"
  },
  {
    "arxiv_id": "2406.05754v2",
    "title": "Numerical solution of a PDE arising from prediction with expert advice",
    "authors": [
      "Jeff Calder",
      "Nadejda Drenska",
      "Drisana Mosaphir"
    ],
    "abstract": "This work investigates the online machine learning problem of prediction with\nexpert advice in an adversarial setting through numerical analysis of, and\nexperiments with, a related partial differential equation. The problem is a\nrepeated two-person game involving decision-making at each step informed by $n$\nexperts in an adversarial environment. The continuum limit of this game over a\nlarge number of steps is a degenerate elliptic equation whose solution encodes\nthe optimal strategies for both players. We develop numerical methods for\napproximating the solution of this equation in relatively high dimensions\n($n\\leq 10$) by exploiting symmetries in the equation and the solution to\ndrastically reduce the size of the computational domain. Based on our numerical\nresults we make a number of conjectures about the optimality of various\nadversarial strategies, in particular about the non-optimality of the COMB\nstrategy.",
    "categories": [
      "math.NA",
      "cs.AI",
      "cs.LG",
      "cs.NA",
      "math.AP",
      "35D40, 65N12, 65N06, 35Q68, 35J60"
    ],
    "primary_category": "math.NA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05754v2",
    "published_date": "2024-06-09 12:17:05 UTC",
    "updated_date": "2025-02-03 01:20:33 UTC"
  },
  {
    "arxiv_id": "2406.05753v5",
    "title": "Grounding Continuous Representations in Geometry: Equivariant Neural Fields",
    "authors": [
      "David R Wessels",
      "David M Knigge",
      "Samuele Papa",
      "Riccardo Valperga",
      "Sharvaree Vadgama",
      "Efstratios Gavves",
      "Erik J Bekkers"
    ],
    "abstract": "Conditional Neural Fields (CNFs) are increasingly being leveraged as\ncontinuous signal representations, by associating each data-sample with a\nlatent variable that conditions a shared backbone Neural Field (NeF) to\nreconstruct the sample. However, existing CNF architectures face limitations\nwhen using this latent downstream in tasks requiring fine-grained geometric\nreasoning, such as classification and segmentation. We posit that this results\nfrom lack of explicit modelling of geometric information (e.g., locality in the\nsignal or the orientation of a feature) in the latent space of CNFs. As such,\nwe propose Equivariant Neural Fields (ENFs), a novel CNF architecture which\nuses a geometry-informed cross-attention to condition the NeF on a geometric\nvariable--a latent point cloud of features--that enables an equivariant\ndecoding from latent to field. We show that this approach induces a\nsteerability property by which both field and latent are grounded in geometry\nand amenable to transformation laws: if the field transforms, the latent\nrepresentation transforms accordingly--and vice versa. Crucially, this\nequivariance relation ensures that the latent is capable of (1) representing\ngeometric patterns faithfully, allowing for geometric reasoning in latent\nspace, and (2) weight-sharing over similar local patterns, allowing for\nefficient learning of datasets of fields. We validate these main properties in\na range of tasks including classification, segmentation, forecasting,\nreconstruction and generative modelling, showing clear improvement over\nbaselines with a geometry-free latent space. Code attached to submission\nhttps://github.com/Dafidofff/enf-jax. Code for a clean and minimal repo\nhttps://github.com/david-knigge/enf-min-jax.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05753v5",
    "published_date": "2024-06-09 12:16:30 UTC",
    "updated_date": "2025-02-07 17:31:20 UTC"
  },
  {
    "arxiv_id": "2406.12900v2",
    "title": "Factor Graph Optimization of Error-Correcting Codes for Belief Propagation Decoding",
    "authors": [
      "Yoni Choukroun",
      "Lior Wolf"
    ],
    "abstract": "The design of optimal linear block codes capable of being efficiently decoded\nis of major concern, especially for short block lengths. As near\ncapacity-approaching codes, Low-Density Parity-Check (LDPC) codes possess\nseveral advantages over other families of codes, the most notable being its\nefficient decoding via Belief Propagation. While many LDPC code design methods\nexist, the development of efficient sparse codes that meet the constraints of\nmodern short code lengths and accommodate new channel models remains a\nchallenge. In this work, we propose for the first time a gradient-based\ndata-driven approach for the design of sparse codes. We develop locally optimal\ncodes with respect to Belief Propagation decoding via the learning of the\nFactor graph under channel noise simulations. This is performed via a novel\ncomplete graph tensor representation of the Belief Propagation algorithm,\noptimized over finite fields via backpropagation and coupled with an efficient\nline-search method. The proposed approach is shown to outperform the decoding\nperformance of existing popular codes by orders of magnitude and demonstrates\nthe power of data-driven approaches for code design.",
    "categories": [
      "cs.IT",
      "cs.AI",
      "cs.LG",
      "math.IT"
    ],
    "primary_category": "cs.IT",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12900v2",
    "published_date": "2024-06-09 12:08:56 UTC",
    "updated_date": "2024-10-10 12:36:25 UTC"
  },
  {
    "arxiv_id": "2406.05746v1",
    "title": "Methodology and Real-World Applications of Dynamic Uncertain Causality Graph for Clinical Diagnosis with Explainability and Invariance",
    "authors": [
      "Zhan Zhang",
      "Qin Zhang",
      "Yang Jiao",
      "Lin Lu",
      "Lin Ma",
      "Aihua Liu",
      "Xiao Liu",
      "Juan Zhao",
      "Yajun Xue",
      "Bing Wei",
      "Mingxia Zhang",
      "Ru Gao",
      "Hong Zhao",
      "Jie Lu",
      "Fan Li",
      "Yang Zhang",
      "Yiming Wang",
      "Lei Zhang",
      "Fengwei Tian",
      "Jie Hu",
      "Xin Gou"
    ],
    "abstract": "AI-aided clinical diagnosis is desired in medical care. Existing deep\nlearning models lack explainability and mainly focus on image analysis. The\nrecently developed Dynamic Uncertain Causality Graph (DUCG) approach is\ncausality-driven, explainable, and invariant across different application\nscenarios, without problems of data collection, labeling, fitting, privacy,\nbias, generalization, high cost and high energy consumption. Through close\ncollaboration between clinical experts and DUCG technicians, 46 DUCG models\ncovering 54 chief complaints were constructed. Over 1,000 diseases can be\ndiagnosed without triage. Before being applied in real-world, the 46 DUCG\nmodels were retrospectively verified by third-party hospitals. The verified\ndiagnostic precisions were no less than 95%, in which the diagnostic precision\nfor every disease including uncommon ones was no less than 80%. After\nverifications, the 46 DUCG models were applied in the real-world in China. Over\none million real diagnosis cases have been performed, with only 17 incorrect\ndiagnoses identified. Due to DUCG's transparency, the mistakes causing the\nincorrect diagnoses were found and corrected. The diagnostic abilities of the\nclinicians who applied DUCG frequently were improved significantly. Following\nthe introduction to the earlier presented DUCG methodology, the recommendation\nalgorithm for potential medical checks is presented and the key idea of DUCG is\nextracted.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05746v1",
    "published_date": "2024-06-09 11:37:45 UTC",
    "updated_date": "2024-06-09 11:37:45 UTC"
  },
  {
    "arxiv_id": "2406.05745v2",
    "title": "Structured Learning of Compositional Sequential Interventions",
    "authors": [
      "Jialin Yu",
      "Andreas Koukorinis",
      "Nicol Colombo",
      "Yuchen Zhu",
      "Ricardo Silva"
    ],
    "abstract": "We consider sequential treatment regimes where each unit is exposed to\ncombinations of interventions over time. When interventions are described by\nqualitative labels, such as \"close schools for a month due to a pandemic\" or\n\"promote this podcast to this user during this week\", it is unclear which\nappropriate structural assumptions allow us to generalize behavioral\npredictions to previously unseen combinations of interventions. Standard\nblack-box approaches mapping sequences of categorical variables to outputs are\napplicable, but they rely on poorly understood assumptions on how reliable\ngeneralization can be obtained, and may underperform under sparse sequences,\ntemporal variability, and large action spaces. To approach that, we pose an\nexplicit model for composition, that is, how the effect of sequential\ninterventions can be isolated into modules, clarifying which data conditions\nallow for the identification of their combined effect at different units and\ntime steps. We show the identification properties of our compositional model,\ninspired by advances in causal matrix factorization methods. Our focus is on\npredictive models for novel compositions of interventions instead of matrix\ncompletion tasks and causal effect estimation. We compare our approach to\nflexible but generic black-box models to illustrate how structure aids\nprediction in sparse data conditions.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "Camera ready version (NeurIPS 2024)",
    "pdf_url": "http://arxiv.org/pdf/2406.05745v2",
    "published_date": "2024-06-09 11:36:36 UTC",
    "updated_date": "2024-10-29 19:18:11 UTC"
  },
  {
    "arxiv_id": "2406.05724v2",
    "title": "Deception Analysis with Artificial Intelligence: An Interdisciplinary Perspective",
    "authors": [
      "Stefan Sarkadi"
    ],
    "abstract": "Humans and machines interact more frequently than ever and our societies are\nbecoming increasingly hybrid. A consequence of this hybridisation is the\ndegradation of societal trust due to the prevalence of AI-enabled deception.\nYet, despite our understanding of the role of trust in AI in the recent years,\nwe still do not have a computational theory to be able to fully understand and\nexplain the role deception plays in this context. This is a problem because\nwhile our ability to explain deception in hybrid societies is delayed, the\ndesign of AI agents may keep advancing towards fully autonomous deceptive\nmachines, which would pose new challenges to dealing with deception. In this\npaper we build a timely and meaningful interdisciplinary perspective on\ndeceptive AI and reinforce a 20 year old socio-cognitive perspective on trust\nand deception, by proposing the development of DAMAS -- a holistic Multi-Agent\nSystems (MAS) framework for the socio-cognitive modelling and analysis of\ndeception. In a nutshell this paper covers the topic of modelling and\nexplaining deception using AI approaches from the perspectives of Computer\nScience, Philosophy, Psychology, Ethics, and Intelligence Analysis.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.MA",
    "comment": "Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2406.05724v2",
    "published_date": "2024-06-09 10:31:26 UTC",
    "updated_date": "2024-06-11 09:06:53 UTC"
  },
  {
    "arxiv_id": "2406.05720v1",
    "title": "VillagerAgent: A Graph-Based Multi-Agent Framework for Coordinating Complex Task Dependencies in Minecraft",
    "authors": [
      "Yubo Dong",
      "Xukun Zhu",
      "Zhengzhe Pan",
      "Linchao Zhu",
      "Yi Yang"
    ],
    "abstract": "In this paper, we aim to evaluate multi-agent systems against complex\ndependencies, including spatial, causal, and temporal constraints. First, we\nconstruct a new benchmark, named VillagerBench, within the Minecraft\nenvironment.VillagerBench comprises diverse tasks crafted to test various\naspects of multi-agent collaboration, from workload distribution to dynamic\nadaptation and synchronized task execution. Second, we introduce a Directed\nAcyclic Graph Multi-Agent Framework VillagerAgent to resolve complex\ninter-agent dependencies and enhance collaborative efficiency. This solution\nincorporates a task decomposer that creates a directed acyclic graph (DAG) for\nstructured task management, an agent controller for task distribution, and a\nstate manager for tracking environmental and agent data. Our empirical\nevaluation on VillagerBench demonstrates that VillagerAgent outperforms the\nexisting AgentVerse model, reducing hallucinations and improving task\ndecomposition efficacy. The results underscore VillagerAgent's potential in\nadvancing multi-agent collaboration, offering a scalable and generalizable\nsolution in dynamic environments. The source code is open-source on GitHub\n(https://github.com/cnsdqd-dyb/VillagerAgent).",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05720v1",
    "published_date": "2024-06-09 10:21:47 UTC",
    "updated_date": "2024-06-09 10:21:47 UTC"
  },
  {
    "arxiv_id": "2406.05707v2",
    "title": "QGEval: Benchmarking Multi-dimensional Evaluation for Question Generation",
    "authors": [
      "Weiping Fu",
      "Bifan Wei",
      "Jianxiang Hu",
      "Zhongmin Cai",
      "Jun Liu"
    ],
    "abstract": "Automatically generated questions often suffer from problems such as unclear\nexpression or factual inaccuracies, requiring a reliable and comprehensive\nevaluation of their quality. Human evaluation is widely used in the field of\nquestion generation (QG) and serves as the gold standard for automatic metrics.\nHowever, there is a lack of unified human evaluation criteria, which hampers\nconsistent and reliable evaluations of both QG models and automatic metrics. To\naddress this, we propose QGEval, a multi-dimensional Evaluation benchmark for\nQuestion Generation, which evaluates both generated questions and existing\nautomatic metrics across 7 dimensions: fluency, clarity, conciseness,\nrelevance, consistency, answerability, and answer consistency. We demonstrate\nthe appropriateness of these dimensions by examining their correlations and\ndistinctions. Through consistent evaluations of QG models and automatic metrics\nwith QGEval, we find that 1) most QG models perform unsatisfactorily in terms\nof answerability and answer consistency, and 2) existing metrics fail to align\nwell with human judgments when evaluating generated questions across the 7\ndimensions. We expect this work to foster the development of both QG\ntechnologies and their evaluation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.05707v2",
    "published_date": "2024-06-09 09:51:55 UTC",
    "updated_date": "2024-10-10 15:12:23 UTC"
  },
  {
    "arxiv_id": "2406.06637v1",
    "title": "Exploring the Efficacy of Large Language Models (GPT-4) in Binary Reverse Engineering",
    "authors": [
      "Saman Pordanesh",
      "Benjamin Tan"
    ],
    "abstract": "This study investigates the capabilities of Large Language Models (LLMs),\nspecifically GPT-4, in the context of Binary Reverse Engineering (RE).\nEmploying a structured experimental approach, we analyzed the LLM's performance\nin interpreting and explaining human-written and decompiled codes. The research\nencompassed two phases: the first on basic code interpretation and the second\non more complex malware analysis. Key findings indicate LLMs' proficiency in\ngeneral code understanding, with varying effectiveness in detailed technical\nand security analyses. The study underscores the potential and current\nlimitations of LLMs in reverse engineering, revealing crucial insights for\nfuture applications and improvements. Also, we examined our experimental\nmethodologies, such as methods of evaluation and data constraints, which\nprovided us with a technical vision for any future research activity in this\nfield.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.06637v1",
    "published_date": "2024-06-09 09:23:58 UTC",
    "updated_date": "2024-06-09 09:23:58 UTC"
  },
  {
    "arxiv_id": "2406.05699v1",
    "title": "An Investigation of Noise Robustness for Flow-Matching-Based Zero-Shot TTS",
    "authors": [
      "Xiaofei Wang",
      "Sefik Emre Eskimez",
      "Manthan Thakker",
      "Hemin Yang",
      "Zirun Zhu",
      "Min Tang",
      "Yufei Xia",
      "Jinzhu Li",
      "Sheng Zhao",
      "Jinyu Li",
      "Naoyuki Kanda"
    ],
    "abstract": "Recently, zero-shot text-to-speech (TTS) systems, capable of synthesizing any\nspeaker's voice from a short audio prompt, have made rapid advancements.\nHowever, the quality of the generated speech significantly deteriorates when\nthe audio prompt contains noise, and limited research has been conducted to\naddress this issue. In this paper, we explored various strategies to enhance\nthe quality of audio generated from noisy audio prompts within the context of\nflow-matching-based zero-shot TTS. Our investigation includes comprehensive\ntraining strategies: unsupervised pre-training with masked speech denoising,\nmulti-speaker detection and DNSMOS-based data filtering on the pre-training\ndata, and fine-tuning with random noise mixing. The results of our experiments\ndemonstrate significant improvements in intelligibility, speaker similarity,\nand overall audio quality compared to the approach of applying speech\nenhancement to the audio prompt.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted to INTERSPEECH2024",
    "pdf_url": "http://arxiv.org/pdf/2406.05699v1",
    "published_date": "2024-06-09 08:51:50 UTC",
    "updated_date": "2024-06-09 08:51:50 UTC"
  },
  {
    "arxiv_id": "2406.05692v1",
    "title": "SPA-SVC: Self-supervised Pitch Augmentation for Singing Voice Conversion",
    "authors": [
      "Bingsong Bai",
      "Fengping Wang",
      "Yingming Gao",
      "Ya Li"
    ],
    "abstract": "Diffusion-based singing voice conversion (SVC) models have shown better\nsynthesis quality compared to traditional methods. However, in cross-domain SVC\nscenarios, where there is a significant disparity in pitch between the source\nand target voice domains, the models tend to generate audios with hoarseness,\nposing challenges in achieving high-quality vocal outputs. Therefore, in this\npaper, we propose a Self-supervised Pitch Augmentation method for Singing Voice\nConversion (SPA-SVC), which can enhance the voice quality in SVC tasks without\nrequiring additional data or increasing model parameters. We innovatively\nintroduce a cycle pitch shifting training strategy and Structural Similarity\nIndex (SSIM) loss into our SVC model, effectively enhancing its performance.\nExperimental results on the public singing datasets M4Singer indicate that our\nproposed method significantly improves model performance in both general SVC\nscenarios and particularly in cross-domain SVC scenarios.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted by Interspeech 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.05692v1",
    "published_date": "2024-06-09 08:34:01 UTC",
    "updated_date": "2024-06-09 08:34:01 UTC"
  },
  {
    "arxiv_id": "2406.05688v1",
    "title": "Peer Review as A Multi-Turn and Long-Context Dialogue with Role-Based Interactions",
    "authors": [
      "Cheng Tan",
      "Dongxin Lyu",
      "Siyuan Li",
      "Zhangyang Gao",
      "Jingxuan Wei",
      "Siqi Ma",
      "Zicheng Liu",
      "Stan Z. Li"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated wide-ranging applications\nacross various fields and have shown significant potential in the academic\npeer-review process. However, existing applications are primarily limited to\nstatic review generation based on submitted papers, which fail to capture the\ndynamic and iterative nature of real-world peer reviews. In this paper, we\nreformulate the peer-review process as a multi-turn, long-context dialogue,\nincorporating distinct roles for authors, reviewers, and decision makers. We\nconstruct a comprehensive dataset containing over 26,841 papers with 92,017\nreviews collected from multiple sources, including the top-tier conference and\nprestigious journal. This dataset is meticulously designed to facilitate the\napplications of LLMs for multi-turn dialogues, effectively simulating the\ncomplete peer-review process. Furthermore, we propose a series of metrics to\nevaluate the performance of LLMs for each role under this reformulated\npeer-review setting, ensuring fair and comprehensive evaluations. We believe\nthis work provides a promising perspective on enhancing the LLM-driven\npeer-review process by incorporating dynamic, role-based interactions. It\naligns closely with the iterative and interactive nature of real-world academic\npeer review, offering a robust foundation for future research and development\nin this area. We open-source the dataset at\nhttps://github.com/chengtan9907/ReviewMT.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Under review",
    "pdf_url": "http://arxiv.org/pdf/2406.05688v1",
    "published_date": "2024-06-09 08:24:17 UTC",
    "updated_date": "2024-06-09 08:24:17 UTC"
  },
  {
    "arxiv_id": "2406.05682v1",
    "title": "From Basic to Extra Features: Hypergraph Transformer Pretrain-then-Finetuning for Balanced Clinical Predictions on EHR",
    "authors": [
      "Ran Xu",
      "Yiwen Lu",
      "Chang Liu",
      "Yong Chen",
      "Yan Sun",
      "Xiao Hu",
      "Joyce C Ho",
      "Carl Yang"
    ],
    "abstract": "Electronic Health Records (EHRs) contain rich patient information and are\ncrucial for clinical research and practice. In recent years, deep learning\nmodels have been applied to EHRs, but they often rely on massive features,\nwhich may not be readily available for all patients. We propose HTP-Star, which\nleverages hypergraph structures with a pretrain-then-finetune framework for\nmodeling EHR data, enabling seamless integration of additional features.\nAdditionally, we design two techniques, namely (1) Smoothness-inducing\nRegularization and (2) Group-balanced Reweighting, to enhance the model's\nrobustness during fine-tuning. Through experiments conducted on two real EHR\ndatasets, we demonstrate that HTP-Star consistently outperforms various\nbaselines while striking a balance between patients with basic and extra\nfeatures.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "CHIL 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.05682v1",
    "published_date": "2024-06-09 07:41:03 UTC",
    "updated_date": "2024-06-09 07:41:03 UTC"
  },
  {
    "arxiv_id": "2406.05673v5",
    "title": "Flow of Reasoning:Training LLMs for Divergent Problem Solving with Minimal Examples",
    "authors": [
      "Fangxu Yu",
      "Lai Jiang",
      "Haoqiang Kang",
      "Shibo Hao",
      "Lianhui Qin"
    ],
    "abstract": "The ability to generate diverse solutions to a given problem is a hallmark of\nhuman creativity. This divergent reasoning is also crucial for machines,\nenhancing their robustness and enabling them to assist humans in many\napplications such as scientific discovery. However, existing approaches to\nmulti-step reasoning with large language models (LLMs) have mostly focused only\non reasoning accuracy, without further discovering more diverse valid\nsolutions. For example, supervised fine-tuning can improve LLM reasoning\nquality, but requires extensive supervised data to capture the full range of\npossible solutions. Reward-maximization reinforcement learning aims to find\nlimited highest-reward solutions while neglecting the solution diversity. To\nfill this gap, we propose Flow of Reasoning (FoR), an efficient\ndiversity-seeking LLM finetuning method aimed at improving reasoning quality\nand diversity with minimal data. FoR formulates multi-step LLM reasoning as a\nMarkovian flow on a DAG-structured reasoning graph. This formulation allows us\nto incorporate and adapt principled GFlowNet approaches, for finetuning LLMs to\nsample divergent paths with probabilities proportional to the (unnormalized)\nreward of target problems. Extensive experiments show that, with limited\ntraining examples (e.g., 15 examples), FoR enables the discovery of diverse,\ncreative, high-quality solutions, greatly outperforming a wide range of\nexisting inference and training methods across six challenging reasoning tasks,\nincluding BlocksWorld (embodied reasoning), Game24 (math puzzle solving),\nRubik's Cube (spatial reasoning), 1D-ARC (abstraction reasoning), GSM8k (math\nreasoning), and ProntoQA (logical reasoning). Code is available at\nhttps://github.com/Yu-Fangxu/FoR.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05673v5",
    "published_date": "2024-06-09 07:06:58 UTC",
    "updated_date": "2025-03-08 13:10:25 UTC"
  },
  {
    "arxiv_id": "2406.07579v1",
    "title": "GFPack++: Improving 2D Irregular Packing by Learning Gradient Field with Attention",
    "authors": [
      "Tianyang Xue",
      "Lin Lu",
      "Yang Liu",
      "Mingdong Wu",
      "Hao Dong",
      "Yanbin Zhang",
      "Renmin Han",
      "Baoquan Chen"
    ],
    "abstract": "2D irregular packing is a classic combinatorial optimization problem with\nvarious applications, such as material utilization and texture atlas\ngeneration. This NP-hard problem requires efficient algorithms to optimize\nspace utilization. Conventional numerical methods suffer from slow convergence\nand high computational cost. Existing learning-based methods, such as the\nscore-based diffusion model, also have limitations, such as no rotation\nsupport, frequent collisions, and poor adaptability to arbitrary boundaries,\nand slow inferring. The difficulty of learning from teacher packing is to\ncapture the complex geometric relationships among packing examples, which\ninclude the spatial (position, orientation) relationships of objects, their\ngeometric features, and container boundary conditions. Representing these\nrelationships in latent space is challenging. We propose GFPack++, an\nattention-based gradient field learning approach that addresses this challenge.\nIt consists of two pivotal strategies: \\emph{attention-based geometry encoding}\nfor effective feature encoding and \\emph{attention-based relation encoding} for\nlearning complex relationships. We investigate the utilization distribution\nbetween the teacher and inference data and design a weighting function to\nprioritize tighter teacher data during training, enhancing learning\neffectiveness. Our diffusion model supports continuous rotation and outperforms\nexisting methods on various datasets. We achieve higher space utilization over\nseveral widely used baselines, one-order faster than the previous\ndiffusion-based method, and promising generalization for arbitrary boundaries.\nWe plan to release our source code and datasets to support further research in\nthis direction.",
    "categories": [
      "cs.AI",
      "cs.GR",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.07579v1",
    "published_date": "2024-06-09 06:44:08 UTC",
    "updated_date": "2024-06-09 06:44:08 UTC"
  },
  {
    "arxiv_id": "2406.05659v1",
    "title": "Do LLMs Exhibit Human-Like Reasoning? Evaluating Theory of Mind in LLMs for Open-Ended Responses",
    "authors": [
      "Maryam Amirizaniani",
      "Elias Martin",
      "Maryna Sivachenko",
      "Afra Mashhadi",
      "Chirag Shah"
    ],
    "abstract": "Theory of Mind (ToM) reasoning entails recognizing that other individuals\npossess their own intentions, emotions, and thoughts, which is vital for\nguiding one's own thought processes. Although large language models (LLMs)\nexcel in tasks such as summarization, question answering, and translation, they\nstill face challenges with ToM reasoning, especially in open-ended questions.\nDespite advancements, the extent to which LLMs truly understand ToM reasoning\nand how closely it aligns with human ToM reasoning remains inadequately\nexplored in open-ended scenarios. Motivated by this gap, we assess the\nabilities of LLMs to perceive and integrate human intentions and emotions into\ntheir ToM reasoning processes within open-ended questions. Our study utilizes\nposts from Reddit's ChangeMyView platform, which demands nuanced social\nreasoning to craft persuasive responses. Our analysis, comparing semantic\nsimilarity and lexical overlap metrics between responses generated by humans\nand LLMs, reveals clear disparities in ToM reasoning capabilities in open-ended\nquestions, with even the most advanced models showing notable limitations. To\nenhance LLM capabilities, we implement a prompt tuning method that incorporates\nhuman intentions and emotions, resulting in improvements in ToM reasoning\nperformance. However, despite these improvements, the enhancement still falls\nshort of fully achieving human-like reasoning. This research highlights the\ndeficiencies in LLMs' social reasoning and demonstrates how integrating human\nintentions and emotions can boost their effectiveness.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05659v1",
    "published_date": "2024-06-09 05:57:59 UTC",
    "updated_date": "2024-06-09 05:57:59 UTC"
  },
  {
    "arxiv_id": "2406.05658v4",
    "title": "Visual Prompt Tuning in Null Space for Continual Learning",
    "authors": [
      "Yue Lu",
      "Shizhou Zhang",
      "De Cheng",
      "Yinghui Xing",
      "Nannan Wang",
      "Peng Wang",
      "Yanning Zhang"
    ],
    "abstract": "Existing prompt-tuning methods have demonstrated impressive performances in\ncontinual learning (CL), by selecting and updating relevant prompts in the\nvision-transformer models. On the contrary, this paper aims to learn each task\nby tuning the prompts in the direction orthogonal to the subspace spanned by\nprevious tasks' features, so as to ensure no interference on tasks that have\nbeen learned to overcome catastrophic forgetting in CL. However, different from\nthe orthogonal projection in the traditional CNN architecture, the prompt\ngradient orthogonal projection in the ViT architecture shows completely\ndifferent and greater challenges, i.e., 1) the high-order and non-linear\nself-attention operation; 2) the drift of prompt distribution brought by the\nLayerNorm in the transformer block. Theoretically, we have finally deduced two\nconsistency conditions to achieve the prompt gradient orthogonal projection,\nwhich provide a theoretical guarantee of eliminating interference on previously\nlearned knowledge via the self-attention mechanism in visual prompt tuning. In\npractice, an effective null-space-based approximation solution has been\nproposed to implement the prompt gradient orthogonal projection. Extensive\nexperimental results demonstrate the effectiveness of anti-forgetting on four\nclass-incremental benchmarks with diverse pre-trained baseline models, and our\napproach achieves superior performances to state-of-the-art methods. Our code\nis available at https://github.com/zugexiaodui/VPTinNSforCL.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.05658v4",
    "published_date": "2024-06-09 05:57:40 UTC",
    "updated_date": "2024-10-26 08:33:20 UTC"
  },
  {
    "arxiv_id": "2406.05653v1",
    "title": "Heart Sound Segmentation Using Deep Learning Techniques",
    "authors": [
      "Manas Madine"
    ],
    "abstract": "Heart disease remains a leading cause of mortality worldwide. Auscultation,\nthe process of listening to heart sounds, can be enhanced through\ncomputer-aided analysis using Phonocardiogram (PCG) signals. This paper\npresents a novel approach for heart sound segmentation and classification into\nS1 (LUB) and S2 (DUB) sounds. We employ FFT-based filtering, dynamic\nprogramming for event detection, and a Siamese network for robust\nclassification. Our method demonstrates superior performance on the PASCAL\nheart sound dataset compared to existing approaches.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05653v1",
    "published_date": "2024-06-09 05:30:05 UTC",
    "updated_date": "2024-06-09 05:30:05 UTC"
  },
  {
    "arxiv_id": "2406.05649v2",
    "title": "GTR: Improving Large 3D Reconstruction Models through Geometry and Texture Refinement",
    "authors": [
      "Peiye Zhuang",
      "Songfang Han",
      "Chaoyang Wang",
      "Aliaksandr Siarohin",
      "Jiaxu Zou",
      "Michael Vasilkovsky",
      "Vladislav Shakhrai",
      "Sergey Korolev",
      "Sergey Tulyakov",
      "Hsin-Ying Lee"
    ],
    "abstract": "We propose a novel approach for 3D mesh reconstruction from multi-view\nimages. Our method takes inspiration from large reconstruction models like LRM\nthat use a transformer-based triplane generator and a Neural Radiance Field\n(NeRF) model trained on multi-view images. However, in our method, we introduce\nseveral important modifications that allow us to significantly enhance 3D\nreconstruction quality. First of all, we examine the original LRM architecture\nand find several shortcomings. Subsequently, we introduce respective\nmodifications to the LRM architecture, which lead to improved multi-view image\nrepresentation and more computationally efficient training. Second, in order to\nimprove geometry reconstruction and enable supervision at full image\nresolution, we extract meshes from the NeRF field in a differentiable manner\nand fine-tune the NeRF model through mesh rendering. These modifications allow\nus to achieve state-of-the-art performance on both 2D and 3D evaluation\nmetrics, such as a PSNR of 28.67 on Google Scanned Objects (GSO) dataset.\nDespite these superior results, our feed-forward model still struggles to\nreconstruct complex textures, such as text and portraits on assets. To address\nthis, we introduce a lightweight per-instance texture refinement procedure.\nThis procedure fine-tunes the triplane representation and the NeRF color\nestimation model on the mesh surface using the input multi-view images in just\n4 seconds. This refinement improves the PSNR to 29.79 and achieves faithful\nreconstruction of complex textures, such as text. Additionally, our approach\nenables various downstream applications, including text- or image-to-3D\ngeneration.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "19 pages, 17 figures. Project page:\n  https://snap-research.github.io/GTR/",
    "pdf_url": "http://arxiv.org/pdf/2406.05649v2",
    "published_date": "2024-06-09 05:19:24 UTC",
    "updated_date": "2024-06-13 18:18:15 UTC"
  },
  {
    "arxiv_id": "2406.05645v1",
    "title": "Anomaly Multi-classification in Industrial Scenarios: Transferring Few-shot Learning to a New Task",
    "authors": [
      "Jie Liu",
      "Yao Wu",
      "Xiaotong Luo",
      "Zongze Wu"
    ],
    "abstract": "In industrial scenarios, it is crucial not only to identify anomalous items\nbut also to classify the type of anomaly. However, research on anomaly\nmulti-classification remains largely unexplored. This paper proposes a novel\nand valuable research task called anomaly multi-classification. Given the\nchallenges in applying few-shot learning to this task, due to limited training\ndata and unique characteristics of anomaly images, we introduce a baseline\nmodel that combines RelationNet and PatchCore. We propose a data generation\nmethod that creates pseudo classes and a corresponding proxy task, aiming to\nbridge the gap in transferring few-shot learning to industrial scenarios.\nFurthermore, we utilize contrastive learning to improve the vanilla baseline,\nachieving much better performance than directly fine-tune a ResNet. Experiments\nconducted on MvTec AD and MvTec3D AD demonstrate that our approach shows\nsuperior performance in this novel task.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05645v1",
    "published_date": "2024-06-09 05:07:39 UTC",
    "updated_date": "2024-06-09 05:07:39 UTC"
  },
  {
    "arxiv_id": "2406.05644v2",
    "title": "How Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States",
    "authors": [
      "Zhenhong Zhou",
      "Haiyang Yu",
      "Xinghua Zhang",
      "Rongwu Xu",
      "Fei Huang",
      "Yongbin Li"
    ],
    "abstract": "Large language models (LLMs) rely on safety alignment to avoid responding to\nmalicious user inputs. Unfortunately, jailbreak can circumvent safety\nguardrails, resulting in LLMs generating harmful content and raising concerns\nabout LLM safety. Due to language models with intensive parameters often\nregarded as black boxes, the mechanisms of alignment and jailbreak are\nchallenging to elucidate. In this paper, we employ weak classifiers to explain\nLLM safety through the intermediate hidden states. We first confirm that LLMs\nlearn ethical concepts during pre-training rather than alignment and can\nidentify malicious and normal inputs in the early layers. Alignment actually\nassociates the early concepts with emotion guesses in the middle layers and\nthen refines them to the specific reject tokens for safe generations. Jailbreak\ndisturbs the transformation of early unethical classification into negative\nemotions. We conduct experiments on models from 7B to 70B across various model\nfamilies to prove our conclusion. Overall, our paper indicates the intrinsical\nmechanism of LLM safety and how jailbreaks circumvent safety guardrails,\noffering a new perspective on LLM safety and reducing concerns. Our code is\navailable at https://github.com/ydyjya/LLM-IHS-Explanation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "27 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.05644v2",
    "published_date": "2024-06-09 05:04:37 UTC",
    "updated_date": "2024-06-13 05:39:31 UTC"
  },
  {
    "arxiv_id": "2406.05631v1",
    "title": "CCSI: Continual Class-Specific Impression for Data-free Class Incremental Learning",
    "authors": [
      "Sana Ayromlou",
      "Teresa Tsang",
      "Purang Abolmaesumi",
      "Xiaoxiao Li"
    ],
    "abstract": "In real-world clinical settings, traditional deep learning-based\nclassification methods struggle with diagnosing newly introduced disease types\nbecause they require samples from all disease classes for offline training.\nClass incremental learning offers a promising solution by adapting a deep\nnetwork trained on specific disease classes to handle new diseases. However,\ncatastrophic forgetting occurs, decreasing the performance of earlier classes\nwhen adapting the model to new data. Prior proposed methodologies to overcome\nthis require perpetual storage of previous samples, posing potential practical\nconcerns regarding privacy and storage regulations in healthcare. To this end,\nwe propose a novel data-free class incremental learning framework that utilizes\ndata synthesis on learned classes instead of data storage from previous\nclasses. Our key contributions include acquiring synthetic data known as\nContinual Class-Specific Impression (CCSI) for previously inaccessible trained\nclasses and presenting a methodology to effectively utilize this data for\nupdating networks when introducing new classes. We obtain CCSI by employing\ndata inversion over gradients of the trained classification model on previous\nclasses starting from the mean image of each class inspired by common landmarks\nshared among medical images and utilizing continual normalization layers\nstatistics as a regularizer in this pixel-wise optimization process.\nSubsequently, we update the network by combining the synthesized data with new\nclass data and incorporate several losses, including an intra-domain\ncontrastive loss to generalize the deep network trained on the synthesized data\nto real data, a margin loss to increase separation among previous classes and\nnew ones, and a cosine-normalized cross-entropy loss to alleviate the adverse\neffects of imbalanced distributions in training data.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05631v1",
    "published_date": "2024-06-09 03:52:21 UTC",
    "updated_date": "2024-06-09 03:52:21 UTC"
  },
  {
    "arxiv_id": "2406.05623v1",
    "title": "Observation Denoising in CYRUS Soccer Simulation 2D Team For RoboCup 2024",
    "authors": [
      "Nader Zare",
      "Aref Sayareh",
      "Sadra Khanjari",
      "Arad Firouzkouhi"
    ],
    "abstract": "In the Soccer Simulation 2D environment, accurate observation is crucial for\neffective decision making. However, challenges such as partial observation and\nnoisy data can hinder performance. To address these issues, we propose a\ndenoising algorithm that leverages predictive modeling and intersection\nanalysis to enhance the accuracy of observations. Our approach aims to mitigate\nthe impact of noise and partial data, leading to improved gameplay performance.\nThis paper presents the framework, implementation, and preliminary results of\nour algorithm, demonstrating its potential in refining observations in Soccer\nSimulation 2D. Cyrus 2D Team is using a combination of Helios, Gliders, and\nCyrus base codes.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05623v1",
    "published_date": "2024-06-09 03:15:29 UTC",
    "updated_date": "2024-06-09 03:15:29 UTC"
  },
  {
    "arxiv_id": "2406.05621v1",
    "title": "Cross Language Soccer Framework: An Open Source Framework for the RoboCup 2D Soccer Simulation",
    "authors": [
      "Nader Zare",
      "Aref Sayareh",
      "Alireza Sadraii",
      "Arad Firouzkouhi",
      "Amilcar Soares"
    ],
    "abstract": "RoboCup Soccer Simulation 2D (SS2D) research is hampered by the complexity of\nexisting Cpp-based codes like Helios, Cyrus, and Gliders, which also suffer\nfrom limited integration with modern machine learning frameworks. This\ndevelopment paper introduces a transformative solution a gRPC-based,\nlanguage-agnostic framework that seamlessly integrates with the\nhigh-performance Helios base code. This approach not only facilitates the use\nof diverse programming languages including CSharp, JavaScript, and Python but\nalso maintains the computational efficiency critical for real time decision\nmaking in SS2D. By breaking down language barriers, our framework significantly\nenhances collaborative potential and flexibility, empowering researchers to\ninnovate without the overhead of mastering or developing extensive base codes.\nWe invite the global research community to leverage and contribute to the Cross\nLanguage Soccer (CLS) framework, which is openly available under the MIT\nLicense, to drive forward the capabilities of multi-agent systems in soccer\nsimulations.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05621v1",
    "published_date": "2024-06-09 03:11:40 UTC",
    "updated_date": "2024-06-09 03:11:40 UTC"
  },
  {
    "arxiv_id": "2406.10249v1",
    "title": "A Reality check of the benefits of LLM in business",
    "authors": [
      "Ming Cheung"
    ],
    "abstract": "Large language models (LLMs) have achieved remarkable performance in language\nunderstanding and generation tasks by leveraging vast amounts of online texts.\nUnlike conventional models, LLMs can adapt to new domains through prompt\nengineering without the need for retraining, making them suitable for various\nbusiness functions, such as strategic planning, project implementation, and\ndata-driven decision-making. However, their limitations in terms of bias,\ncontextual understanding, and sensitivity to prompts raise concerns about their\nreadiness for real-world applications. This paper thoroughly examines the\nusefulness and readiness of LLMs for business processes. The limitations and\ncapacities of LLMs are evaluated through experiments conducted on four\naccessible LLMs using real-world data. The findings have significant\nimplications for organizations seeking to leverage generative AI and provide\nvaluable insights into future research directions. To the best of our\nknowledge, this represents the first quantified study of LLMs applied to core\nbusiness operations and challenges.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.10249v1",
    "published_date": "2024-06-09 02:36:00 UTC",
    "updated_date": "2024-06-09 02:36:00 UTC"
  },
  {
    "arxiv_id": "2407.07732v1",
    "title": "Text2VP: Generative AI for Visual Programming and Parametric Modeling",
    "authors": [
      "Guangxi Feng",
      "Wei Yan"
    ],
    "abstract": "The integration of generative artificial intelligence (AI) into architectural\ndesign has witnessed a significant evolution, marked by the recent advancements\nin AI to generate text, images, and 3D models. However, no models exist for\ntext-to-parametric models that are used in architectural design for generating\nvarious design options, including free-form designs, and optimizing the design\noptions. This study creates and investigates an innovative application of\ngenerative AI in parametric modeling by leveraging a customized Text-to-Visual\nProgramming (Text2VP) GPT derived from GPT-4. The primary focus is on\nautomating the generation of graph-based visual programming workflows,\nincluding parameters and the links among the parameters, through AI-generated\nscripts, accurately reflecting users' design intentions and allowing the users\nto change the parameter values interactively. The Text2VP GPT customization\nprocess utilizes detailed and complete documentation of the visual programming\nlanguage components, example-driven few-shot learning, and specific\ninstructional guides. Our testing demonstrates Text2VP's capability to generate\nworking parametric models. The paper also discusses the limitations of Text2VP;\nfor example, more complex parametric model generation introduces higher error\nrates. This research highlights the potential of generative AI in visual\nprogramming and parametric modeling and sets a foundation for future\nenhancements to handle more sophisticated and intricate modeling tasks\neffectively. The study aims to allow designers to create and change design\nmodels without significant effort in learning a specific programming language\nsuch as Grasshopper.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Demonstration Video:\n  https://www.youtube.com/playlist?list=PLUOmOLuLSaDWss2En2buixBxvTPy-lDvA",
    "pdf_url": "http://arxiv.org/pdf/2407.07732v1",
    "published_date": "2024-06-09 02:22:20 UTC",
    "updated_date": "2024-06-09 02:22:20 UTC"
  },
  {
    "arxiv_id": "2406.05612v3",
    "title": "Which Backbone to Use: A Resource-efficient Domain Specific Comparison for Computer Vision",
    "authors": [
      "Pranav Jeevan",
      "Amit Sethi"
    ],
    "abstract": "In contemporary computer vision applications, particularly image\nclassification, architectural backbones pre-trained on large datasets like\nImageNet are commonly employed as feature extractors. Despite the widespread\nuse of these pre-trained convolutional neural networks (CNNs), there remains a\ngap in understanding the performance of various resource-efficient backbones\nacross diverse domains and dataset sizes. Our study systematically evaluates\nmultiple lightweight, pre-trained CNN backbones under consistent training\nsettings across a variety of datasets, including natural images, medical\nimages, galaxy images, and remote sensing images. This comprehensive analysis\naims to aid machine learning practitioners in selecting the most suitable\nbackbone for their specific problem, especially in scenarios involving small\ndatasets where fine-tuning a pre-trained network is crucial. Even though\nattention-based architectures are gaining popularity, we observed that they\ntend to perform poorly under low data finetuning tasks compared to CNNs. We\nalso observed that some CNN architectures such as ConvNeXt, RegNet and\nEfficientNet performs well compared to others on a diverse set of domains\nconsistently. Our findings provide actionable insights into the performance\ntrade-offs and effectiveness of different backbones, facilitating informed\ndecision-making in model selection for a broad spectrum of computer vision\ndomains. Our code is available here: https://github.com/pranavphoenix/Backbones",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "I.2.10; I.4.0; I.4.1; I.4.2; I.4.6; I.4.7; I.4.8; I.4.9; I.4.10;\n  I.2.10; I.5.1; I.5.2; I.5.4; J.2"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 2 figures, accepted in TMLR",
    "pdf_url": "http://arxiv.org/pdf/2406.05612v3",
    "published_date": "2024-06-09 02:01:25 UTC",
    "updated_date": "2025-03-09 21:00:14 UTC"
  },
  {
    "arxiv_id": "2406.05605v1",
    "title": "Deep Learning to Predict Glaucoma Progression using Structural Changes in the Eye",
    "authors": [
      "Sayan Mandal"
    ],
    "abstract": "Glaucoma is a chronic eye disease characterized by optic neuropathy, leading\nto irreversible vision loss. It progresses gradually, often remaining\nundiagnosed until advanced stages. Early detection is crucial to monitor\natrophy and develop treatment strategies to prevent further vision impairment.\nData-centric methods have enabled computer-aided algorithms for precise\nglaucoma diagnosis.\n  In this study, we use deep learning models to identify complex disease traits\nand progression criteria, detecting subtle changes indicative of glaucoma. We\nexplore the structure-function relationship in glaucoma progression and predict\nfunctional impairment from structural eye deterioration. We analyze statistical\nand machine learning methods, including deep learning techniques with optical\ncoherence tomography (OCT) scans for accurate progression prediction.\n  Addressing challenges like age variability, data imbalances, and noisy\nlabels, we develop novel semi-supervised time-series algorithms:\n  1. Weakly-Supervised Time-Series Learning: We create a CNN-LSTM model to\nencode spatiotemporal features from OCT scans. This approach uses age-related\nprogression and positive-unlabeled data to establish robust pseudo-progression\ncriteria, bypassing gold-standard labels.\n  2. Semi-Supervised Time-Series Learning: Using labels from Guided Progression\nAnalysis (GPA) in a contrastive learning scheme, the CNN-LSTM architecture\nlearns from potentially mislabeled data to improve prediction accuracy.\n  Our methods outperform conventional and state-of-the-art techniques.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "68T07",
      "I.2.1"
    ],
    "primary_category": "cs.CV",
    "comment": "Dissertation",
    "pdf_url": "http://arxiv.org/pdf/2406.05605v1",
    "published_date": "2024-06-09 01:12:41 UTC",
    "updated_date": "2024-06-09 01:12:41 UTC"
  },
  {
    "arxiv_id": "2406.05603v1",
    "title": "A Knowledge-Component-Based Methodology for Evaluating AI Assistants",
    "authors": [
      "Laryn Qi",
      "J. D. Zamfirescu-Pereira",
      "Taehan Kim",
      "Bjrn Hartmann",
      "John DeNero",
      "Narges Norouzi"
    ],
    "abstract": "We evaluate an automatic hint generator for CS1 programming assignments\npowered by GPT-4, a large language model. This system provides natural language\nguidance about how students can improve their incorrect solutions to short\nprogramming exercises. A hint can be requested each time a student fails a test\ncase. Our evaluation addresses three Research Questions:\n  RQ1: Do the hints help students improve their code? RQ2: How effectively do\nthe hints capture problems in student code? RQ3: Are the issues that students\nresolve the same as the issues addressed in the hints?\n  To address these research questions quantitatively, we identified a set of\nfine-grained knowledge components and determined which ones apply to each\nexercise, incorrect solution, and generated hint. Comparing data from two large\nCS1 offerings, we found that access to the hints helps students to address\nproblems with their code more quickly, that hints are able to consistently\ncapture the most pressing errors in students' code, and that hints that address\na few issues at once rather than a single bug are more likely to lead to direct\nstudent progress.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.05603v1",
    "published_date": "2024-06-09 00:58:39 UTC",
    "updated_date": "2024-06-09 00:58:39 UTC"
  }
]
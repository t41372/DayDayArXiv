[
  {
    "arxiv_id": "2502.11312v1",
    "title": "AI Generations: From AI 1.0 to AI 4.0",
    "authors": [
      "Jiahao Wu",
      "Hengxu You",
      "Jing Du"
    ],
    "abstract": "This paper proposes that Artificial Intelligence (AI) progresses through\nseveral overlapping generations: AI 1.0 (Information AI), AI 2.0 (Agentic AI),\nAI 3.0 (Physical AI), and now a speculative AI 4.0 (Conscious AI). Each of\nthese AI generations is driven by shifting priorities among algorithms,\ncomputing power, and data. AI 1.0 ushered in breakthroughs in pattern\nrecognition and information processing, fueling advances in computer vision,\nnatural language processing, and recommendation systems. AI 2.0 built on these\nfoundations through real-time decision-making in digital environments,\nleveraging reinforcement learning and adaptive planning for agentic AI\napplications. AI 3.0 extended intelligence into physical contexts, integrating\nrobotics, autonomous vehicles, and sensor-fused control systems to act in\nuncertain real-world settings. Building on these developments, AI 4.0 puts\nforward the bold vision of self-directed AI capable of setting its own goals,\norchestrating complex training regimens, and possibly exhibiting elements of\nmachine consciousness. This paper traces the historical foundations of AI\nacross roughly seventy years, mapping how changes in technological bottlenecks\nfrom algorithmic innovation to high-performance computing to specialized data,\nhave spurred each generational leap. It further highlights the ongoing\nsynergies among AI 1.0, 2.0, 3.0, and 4.0, and explores the profound ethical,\nregulatory, and philosophical challenges that arise when artificial systems\napproach (or aspire to) human-like autonomy. Ultimately, understanding these\nevolutions and their interdependencies is pivotal for guiding future research,\ncrafting responsible governance, and ensuring that AI transformative potential\nbenefits society as a whole.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "17 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.11312v1",
    "published_date": "2025-02-16 23:19:44 UTC",
    "updated_date": "2025-02-16 23:19:44 UTC"
  },
  {
    "arxiv_id": "2502.11308v2",
    "title": "ALGEN: Few-shot Inversion Attacks on Textual Embeddings using Alignment and Generation",
    "authors": [
      "Yiyi Chen",
      "Qiongkai Xu",
      "Johannes Bjerva"
    ],
    "abstract": "With the growing popularity of Large Language Models (LLMs) and vector\ndatabases, private textual data is increasingly processed and stored as\nnumerical embeddings. However, recent studies have proven that such embeddings\nare vulnerable to inversion attacks, where original text is reconstructed to\nreveal sensitive information. Previous research has largely assumed access to\nmillions of sentences to train attack models, e.g., through data leakage or\nnearly unrestricted API access. With our method, a single data point is\nsufficient for a partially successful inversion attack. With as little as 1k\ndata samples, performance reaches an optimum across a range of black-box\nencoders, without training on leaked data. We present a Few-shot Textual\nEmbedding Inversion Attack using ALignment and GENeration (ALGEN), by aligning\nvictim embeddings to the attack space and using a generative model to\nreconstruct text. We find that ALGEN attacks can be effectively transferred\nacross domains and languages, revealing key information. We further examine a\nvariety of defense mechanisms against ALGEN, and find that none are effective,\nhighlighting the vulnerabilities posed by inversion attacks. By significantly\nlowering the cost of inversion and proving that embedding spaces can be aligned\nthrough one-step optimization, we establish a new textual embedding inversion\nparadigm with broader applications for embedding alignment in NLP.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "I.2; J.6"
    ],
    "primary_category": "cs.CR",
    "comment": "18 pages, 13 tables, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.11308v2",
    "published_date": "2025-02-16 23:11:13 UTC",
    "updated_date": "2025-02-18 10:07:55 UTC"
  },
  {
    "arxiv_id": "2502.11307v1",
    "title": "Exploiting Point-Language Models with Dual-Prompts for 3D Anomaly Detection",
    "authors": [
      "Jiaxiang Wang",
      "Haote Xu",
      "Xiaolu Chen",
      "Haodi Xu",
      "Yue Huang",
      "Xinghao Ding",
      "Xiaotong Tu"
    ],
    "abstract": "Anomaly detection (AD) in 3D point clouds is crucial in a wide range of\nindustrial applications, especially in various forms of precision\nmanufacturing. Considering the industrial demand for reliable 3D AD, several\nmethods have been developed. However, most of these approaches typically\nrequire training separate models for each category, which is memory-intensive\nand lacks flexibility. In this paper, we propose a novel Point-Language model\nwith dual-prompts for 3D ANomaly dEtection (PLANE). The approach leverages\nmulti-modal prompts to extend the strong generalization capabilities of\npre-trained Point-Language Models (PLMs) to the domain of 3D point cloud AD,\nachieving impressive detection performance across multiple categories using a\nsingle model. Specifically, we propose a dual-prompt learning method,\nincorporating both text and point cloud prompts. The method utilizes a dynamic\nprompt creator module (DPCM) to produce sample-specific dynamic prompts, which\nare then integrated with class-specific static prompts for each modality,\neffectively driving the PLMs. Additionally, based on the characteristics of\npoint cloud data, we propose a pseudo 3D anomaly generation method (Ano3D) to\nimprove the model's detection capabilities in an unsupervised setting.\nExperimental results demonstrate that the proposed method, which is under the\nmulti-class-one-model paradigm, achieves a +8.7%/+17% gain on anomaly detection\nand localization performance as compared to the state-of-the-art\none-class-one-model methods for the Anomaly-ShapeNet dataset, and obtains\n+4.3%/+4.1% gain for the Real3D-AD dataset. Code will be available upon\npublication.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.11307v1",
    "published_date": "2025-02-16 23:10:57 UTC",
    "updated_date": "2025-02-16 23:10:57 UTC"
  },
  {
    "arxiv_id": "2502.11304v1",
    "title": "Leveraging Multimodal-LLMs Assisted by Instance Segmentation for Intelligent Traffic Monitoring",
    "authors": [
      "Murat Arda Onsu",
      "Poonam Lohan",
      "Burak Kantarci",
      "Aisha Syed",
      "Matthew Andrews",
      "Sean Kennedy"
    ],
    "abstract": "A robust and efficient traffic monitoring system is essential for smart\ncities and Intelligent Transportation Systems (ITS), using sensors and cameras\nto track vehicle movements, optimize traffic flow, reduce congestion, enhance\nroad safety, and enable real-time adaptive traffic control. Traffic monitoring\nmodels must comprehensively understand dynamic urban conditions and provide an\nintuitive user interface for effective management. This research leverages the\nLLaVA visual grounding multimodal large language model (LLM) for traffic\nmonitoring tasks on the real-time Quanser Interactive Lab simulation platform,\ncovering scenarios like intersections, congestion, and collisions. Cameras\nplaced at multiple urban locations collect real-time images from the\nsimulation, which are fed into the LLaVA model with queries for analysis. An\ninstance segmentation model integrated into the cameras highlights key elements\nsuch as vehicles and pedestrians, enhancing training and throughput. The system\nachieves 84.3% accuracy in recognizing vehicle locations and 76.4% in\ndetermining steering direction, outperforming traditional models.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "6 pages, 7 figures, submitted to 30th IEEE International Symposium on\n  Computers and Communications (ISCC) 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.11304v1",
    "published_date": "2025-02-16 23:03:26 UTC",
    "updated_date": "2025-02-16 23:03:26 UTC"
  },
  {
    "arxiv_id": "2502.11300v1",
    "title": "CORDIAL: Can Multimodal Large Language Models Effectively Understand Coherence Relationships?",
    "authors": [
      "Aashish Anantha Ramakrishnan",
      "Aadarsh Anantha Ramakrishnan",
      "Dongwon Lee"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) are renowned for their superior\ninstruction-following and reasoning capabilities across diverse problem\ndomains. However, existing benchmarks primarily focus on assessing factual and\nlogical correctness in downstream tasks, with limited emphasis on evaluating\nMLLMs' ability to interpret pragmatic cues and intermodal relationships. To\naddress this gap, we assess the competency of MLLMs in performing Multimodal\nDiscourse Analysis (MDA) using Coherence Relations. Our benchmark, CORDIAL,\nencompasses a broad spectrum of Coherence Relations across 3 different\ndiscourse domains at varying levels of granularity. Through our experiments on\n10+ MLLMs employing different prompting strategies, we show that even top\nmodels like Gemini 1.5 Pro and GPT-4o fail to match the performance of simple\nclassifier-based baselines. This study emphasizes the need to move beyond\nsimilarity-based metrics and adopt a discourse-driven framework for evaluating\nMLLMs, providing a more nuanced assessment of their capabilities. The benchmark\nand code are available at: https://github.com/aashish2000/CORDIAL.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "I.2.7; I.2.10"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11300v1",
    "published_date": "2025-02-16 22:54:44 UTC",
    "updated_date": "2025-02-16 22:54:44 UTC"
  },
  {
    "arxiv_id": "2502.11298v1",
    "title": "Integrating Language Models for Enhanced Network State Monitoring in DRL-Based SFC Provisioning",
    "authors": [
      "Parisa Fard Moshiri",
      "Murat Arda Onsu",
      "Poonam Lohan",
      "Burak Kantarci",
      "Emil Janulewicz"
    ],
    "abstract": "Efficient Service Function Chain (SFC) provisioning and Virtual Network\nFunction (VNF) placement are critical for enhancing network performance in\nmodern architectures such as Software-Defined Networking (SDN) and Network\nFunction Virtualization (NFV). While Deep Reinforcement Learning (DRL) aids\ndecision-making in dynamic network environments, its reliance on structured\ninputs and predefined rules limits adaptability in unforeseen scenarios.\nAdditionally, incorrect actions by a DRL agent may require numerous training\niterations to correct, potentially reinforcing suboptimal policies and\ndegrading performance. This paper integrates DRL with Language Models (LMs),\nspecifically Bidirectional Encoder Representations from Transformers (BERT) and\nDistilBERT, to enhance network management. By feeding final VNF allocations\nfrom DRL into the LM, the system can process and respond to queries related to\nSFCs, DCs, and VNFs, enabling real-time insights into resource utilization,\nbottleneck detection, and future demand planning. The LMs are fine-tuned to our\ndomain-specific dataset using Low-Rank Adaptation (LoRA). Results show that\nBERT outperforms DistilBERT with a lower test loss (0.28 compared to 0.36) and\nhigher confidence (0.83 compared to 0.74), though BERT requires approximately\n46% more processing time.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.NI",
    "comment": "6 pages, 5 figures, submitted to 30th IEEE International Symposium on\n  Computers and Communications (ISCC) 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.11298v1",
    "published_date": "2025-02-16 22:52:14 UTC",
    "updated_date": "2025-02-16 22:52:14 UTC"
  },
  {
    "arxiv_id": "2502.11295v1",
    "title": "Game-Of-Goals: Using adversarial games to achieve strategic resilience",
    "authors": [
      "Aditya Ghose",
      "Asjad Khan"
    ],
    "abstract": "Our objective in this paper is to develop a machinery that makes a given\norganizational strategic plan resilient to the actions of competitor agents\n(adverse environmental actions). We assume that we are given a goal tree\nrepresenting strategic goals (can also be seen business requirements for a\nsoftware systems) with the assumption that competitor agents are behaving in a\nmaximally adversarial fashion(opposing actions against our sub goals or goals\nin general). We use game tree search methods (such as minimax) to select an\noptimal execution strategy(at a given point in time), such that it can maximize\nour chances of achieving our (high level) strategic goals. Our machinery helps\nus determine which path to follow(strategy selection) to achieve the best end\noutcome. This is done by comparing alternative execution strategies available\nto us via an evaluation function. Our evaluation function is based on the idea\nthat we want to make our execution plans defensible(future-proof) by selecting\nexecution strategies that make us least vulnerable to adversarial actions by\nthe competitor agents. i.e we want to select an execution strategy such that\nits leaves minimum room(or options) for the adversary to cause\nimpediment/damage to our business goals/plans.",
    "categories": [
      "cs.AI",
      "cs.GT"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11295v1",
    "published_date": "2025-02-16 22:34:59 UTC",
    "updated_date": "2025-02-16 22:34:59 UTC"
  },
  {
    "arxiv_id": "2502.11291v1",
    "title": "Dialogue-based Explanations for Logical Reasoning using Structured Argumentation",
    "authors": [
      "Loan Ho",
      "Stefan Schlobach"
    ],
    "abstract": "The problem of explaining inconsistency-tolerant reasoning in knowledge bases\n(KBs) is a prominent topic in Artificial Intelligence (AI). While there is some\nwork on this problem, the explanations provided by existing approaches often\nlack critical information or fail to be expressive enough for non-binary\nconflicts. In this paper, we identify structural weaknesses of the\nstate-of-the-art and propose a generic argumentation-based approach to address\nthese problems. This approach is defined for logics involving reasoning with\nmaximal consistent subsets and shows how any such logic can be translated to\nargumentation. Our work provides dialogue models as dialectic-proof procedures\nto compute and explain a query answer wrt inconsistency-tolerant semantics.\nThis allows us to construct dialectical proof trees as explanations, which are\nmore expressive and arguably more intuitive than existing explanation\nformalisms.",
    "categories": [
      "cs.AI",
      "cs.DB",
      "cs.HC",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "45 pages, 8 gigures, journal",
    "pdf_url": "http://arxiv.org/pdf/2502.11291v1",
    "published_date": "2025-02-16 22:26:18 UTC",
    "updated_date": "2025-02-16 22:26:18 UTC"
  },
  {
    "arxiv_id": "2502.11273v1",
    "title": "FairFare: A Tool for Crowdsourcing Rideshare Data to Empower Labor Organizers",
    "authors": [
      "Dana Calacci",
      "Varun Nagaraj Rao",
      "Samantha Dalal",
      "Catherine Di",
      "Kok-Wei Pua",
      "Andrew Schwartz",
      "Danny Spitzberg",
      "Andrés Monroy-Hernández"
    ],
    "abstract": "Rideshare workers experience unpredictable working conditions due to gig work\nplatforms' reliance on opaque AI and algorithmic systems. In response to these\nchallenges, we found that labor organizers want data to help them advocate for\nlegislation to increase the transparency and accountability of these platforms.\nTo address this need, we collaborated with a Colorado-based rideshare union to\ndevelop FairFare, a tool that crowdsources and analyzes workers' data to\nestimate the take rate -- the percentage of the rider price retained by the\nrideshare platform. We deployed FairFare with our partner organization that\ncollaborated with us in collecting data on 76,000+ trips from 45 drivers over\n18 months. During evaluation interviews, organizers reported that FairFare\nhelped influence the bill language and passage of Colorado Senate Bill 24-75,\ncalling for greater transparency and data disclosure of platform operations,\nand create a national narrative. Finally, we reflect on complexities of\ntranslating quantitative data into policy outcomes, nature of community based\naudits, and design implications for future transparency tools.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "FairFare is hosted at: https://getfairfare.org/",
    "pdf_url": "http://arxiv.org/pdf/2502.11273v1",
    "published_date": "2025-02-16 21:30:26 UTC",
    "updated_date": "2025-02-16 21:30:26 UTC"
  },
  {
    "arxiv_id": "2502.11269v1",
    "title": "Unlocking the Potential of Generative AI through Neuro-Symbolic Architectures: Benefits and Limitations",
    "authors": [
      "Oualid Bougzime",
      "Samir Jabbar",
      "Christophe Cruz",
      "Frédéric Demoly"
    ],
    "abstract": "Neuro-symbolic artificial intelligence (NSAI) represents a transformative\napproach in artificial intelligence (AI) by combining deep learning's ability\nto handle large-scale and unstructured data with the structured reasoning of\nsymbolic methods. By leveraging their complementary strengths, NSAI enhances\ngeneralization, reasoning, and scalability while addressing key challenges such\nas transparency and data efficiency. This paper systematically studies diverse\nNSAI architectures, highlighting their unique approaches to integrating neural\nand symbolic components. It examines the alignment of contemporary AI\ntechniques such as retrieval-augmented generation, graph neural networks,\nreinforcement learning, and multi-agent systems with NSAI paradigms. This study\nthen evaluates these architectures against comprehensive set of criteria,\nincluding generalization, reasoning capabilities, transferability, and\ninterpretability, therefore providing a comparative analysis of their\nrespective strengths and limitations. Notably, the Neuro > Symbolic < Neuro\nmodel consistently outperforms its counterparts across all evaluation metrics.\nThis result aligns with state-of-the-art research that highlight the efficacy\nof such architectures in harnessing advanced technologies like multi-agent\nsystems.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.SC"
    ],
    "primary_category": "cs.AI",
    "comment": "54 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.11269v1",
    "published_date": "2025-02-16 21:06:33 UTC",
    "updated_date": "2025-02-16 21:06:33 UTC"
  },
  {
    "arxiv_id": "2502.11267v1",
    "title": "Prompting in the Dark: Assessing Human Performance in Prompt Engineering for Data Labeling When Gold Labels Are Absent",
    "authors": [
      "Zeyu He",
      "Saniya Naphade",
      "Ting-Hao 'Kenneth' Huang"
    ],
    "abstract": "Millions of users prompt large language models (LLMs) for various tasks, but\nhow good are people at prompt engineering? Do users actually get closer to\ntheir desired outcome over multiple iterations of their prompts? These\nquestions are crucial when no gold-standard labels are available to measure\nprogress. This paper investigates a scenario in LLM-powered data labeling,\n\"prompting in the dark,\" where users iteratively prompt LLMs to label data\nwithout using manually-labeled benchmarks. We developed PromptingSheet, a\nGoogle Sheets add-on that enables users to compose, revise, and iteratively\nlabel data through spreadsheets. Through a study with 20 participants, we found\nthat prompting in the dark was highly unreliable-only 9 participants improved\nlabeling accuracy after four or more iterations. Automated prompt optimization\ntools like DSPy also struggled when few gold labels were available. Our\nfindings highlight the importance of gold labels and the needs, as well as the\nrisks, of automated support in human prompt engineering, providing insights for\nfuture tool design.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted By CHI 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.11267v1",
    "published_date": "2025-02-16 20:54:26 UTC",
    "updated_date": "2025-02-16 20:54:26 UTC"
  },
  {
    "arxiv_id": "2502.11262v1",
    "title": "Generating Skyline Datasets for Data Science Models",
    "authors": [
      "Mengying Wang",
      "Hanchao Ma",
      "Yiyang Bian",
      "Yangxin Fan",
      "Yinghui Wu"
    ],
    "abstract": "Preparing high-quality datasets required by various data-driven AI and\nmachine learning models has become a cornerstone task in data-driven analysis.\nConventional data discovery methods typically integrate datasets towards a\nsingle pre-defined quality measure that may lead to bias for downstream tasks.\nThis paper introduces MODis, a framework that discovers datasets by optimizing\nmultiple user-defined, model-performance measures. Given a set of data sources\nand a model, MODis selects and integrates data sources into a skyline dataset,\nover which the model is expected to have the desired performance in all the\nperformance measures. We formulate MODis as a multi-goal finite state\ntransducer, and derive three feasible algorithms to generate skyline datasets.\nOur first algorithm adopts a \"reduce-from-universal\" strategy, that starts with\na universal schema and iteratively prunes unpromising data. Our second\nalgorithm further reduces the cost with a bi-directional strategy that\ninterleaves data augmentation and reduction. We also introduce a\ndiversification algorithm to mitigate the bias in skyline datasets. We\nexperimentally verify the efficiency and effectiveness of our skyline data\ndiscovery algorithms, and showcase their applications in optimizing data\nscience pipelines.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "EDBT25",
    "pdf_url": "http://arxiv.org/pdf/2502.11262v1",
    "published_date": "2025-02-16 20:33:59 UTC",
    "updated_date": "2025-02-16 20:33:59 UTC"
  },
  {
    "arxiv_id": "2502.11251v1",
    "title": "Explaining Necessary Truths",
    "authors": [
      "Gülce Kardeş",
      "Simon DeDeo"
    ],
    "abstract": "Knowing the truth is rarely enough -- we also seek out reasons why the fact\nis true. While much is known about how we explain contingent truths, we\nunderstand less about how we explain facts, such as those in mathematics, that\nare true as a matter of logical necessity. We present a framework, based in\ncomputational complexity, where explanations for deductive truths co-emerge\nwith discoveries of simplifying steps during the search process. When such\nstructures are missing, we revert, in turn, to error-based reasons, where a\n(corrected) mistake can serve as fictitious, but explanatory,\ncontingency-cause: not making the mistake serves as a reason why the truth\ntakes the form it does. We simulate human subjects, using GPT-4o, presented\nwith SAT puzzles of varying complexity and reasonableness, validating our\ntheory and showing how its predictions can be tested in future human studies.",
    "categories": [
      "cs.AI",
      "cs.CC",
      "math.HO",
      "q-bio.NC",
      "97C30 (Primary), 91E10 (Secondary)"
    ],
    "primary_category": "cs.AI",
    "comment": "7 pages, in review",
    "pdf_url": "http://arxiv.org/pdf/2502.11251v1",
    "published_date": "2025-02-16 20:11:39 UTC",
    "updated_date": "2025-02-16 20:11:39 UTC"
  },
  {
    "arxiv_id": "2502.11245v1",
    "title": "Shortcuts and Identifiability in Concept-based Models from a Neuro-Symbolic Lens",
    "authors": [
      "Samuele Bortolotti",
      "Emanuele Marconato",
      "Paolo Morettin",
      "Andrea Passerini",
      "Stefano Teso"
    ],
    "abstract": "Concept-based Models are neural networks that learn a concept extractor to\nmap inputs to high-level concepts and an inference layer to translate these\ninto predictions. Ensuring these modules produce interpretable concepts and\nbehave reliably in out-of-distribution is crucial, yet the conditions for\nachieving this remain unclear. We study this problem by establishing a novel\nconnection between Concept-based Models and reasoning shortcuts (RSs), a common\nissue where models achieve high accuracy by learning low-quality concepts, even\nwhen the inference layer is fixed and provided upfront. Specifically, we first\nextend RSs to the more complex setting of Concept-based Models and then derive\ntheoretical conditions for identifying both the concepts and the inference\nlayer. Our empirical results highlight the impact of reasoning shortcuts and\nshow that existing methods, even when combined with multiple natural mitigation\nstrategies, often fail to meet these conditions in practice.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11245v1",
    "published_date": "2025-02-16 19:45:09 UTC",
    "updated_date": "2025-02-16 19:45:09 UTC"
  },
  {
    "arxiv_id": "2502.11244v1",
    "title": "Soteria: Language-Specific Functional Parameter Steering for Multilingual Safety Alignment",
    "authors": [
      "Somnath Banerjee",
      "Sayan Layek",
      "Pratyush Chatterjee",
      "Animesh Mukherjee",
      "Rima Hazra"
    ],
    "abstract": "Ensuring consistent safety across multiple languages remains a significant\nchallenge for large language models (LLMs). We introduce Soteria, a lightweight\nyet powerful strategy that locates and minimally adjusts the \"functional heads\"\nmost responsible for harmful content generation in each language. By altering\nonly a fraction of parameters, Soteria drastically reduces policy violations\nwithout sacrificing overall model performance, even in low-resource settings.\nTo rigorously evaluate our approach, we also present XThreatBench, a\nspecialized multilingual dataset capturing fine-grained harmful behaviors drawn\nfrom real policy guidelines. Experiments with leading open-source LLMs (e.g.,\nLlama, Qwen, Mistral) show that Soteria consistently improves safety metrics\nacross high-, mid-, and low-resource languages. These findings highlight a\npromising path toward scalable, linguistically attuned, and ethically aligned\nLLMs worldwide.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11244v1",
    "published_date": "2025-02-16 19:44:01 UTC",
    "updated_date": "2025-02-16 19:44:01 UTC"
  },
  {
    "arxiv_id": "2502.11239v2",
    "title": "Towards identifying possible fault-tolerant advantage of quantum linear system algorithms in terms of space, time and energy",
    "authors": [
      "Yue Tu",
      "Mark Dubynskyi",
      "Mohammadhossein Mohammadisiahroudi",
      "Ekaterina Riashchentceva",
      "Jinglei Cheng",
      "Dmitry Ryashchentsev",
      "Tamás Terlaky",
      "Junyu Liu"
    ],
    "abstract": "Quantum computing, a prominent non-Von Neumann paradigm beyond Moore's law,\ncan offer superpolynomial speedups for certain problems. Yet its advantages in\nefficiency for tasks like machine learning remain under investigation, and\nquantum noise complicates resource estimations and classical comparisons. We\nprovide a detailed estimation of space, time, and energy resources for\nfault-tolerant superconducting devices running the Harrow-Hassidim-Lloyd (HHL)\nalgorithm, a quantum linear system solver relevant to linear algebra and\nmachine learning. Excluding memory and data transfer, possible quantum\nadvantages over the classical conjugate gradient method could emerge at $N\n\\approx 2^{33} \\sim 2^{48}$ or even lower, requiring ${O}(10^5)$ physical\nqubits, ${O}(10^{12}\\sim10^{13})$ Joules, and ${O}(10^6)$ seconds under surface\ncode fault-tolerance with three types of magic state distillation (15-1,\n116-12, 225-1). Key parameters include condition number, sparsity, and\nprecision $\\kappa, s\\approx{O}(10\\sim100)$, $\\epsilon\\sim0.01$, and physical\nerror $10^{-5}$. Our resource estimator adjusts $N, \\kappa, s, \\epsilon$,\nproviding a map of quantum-classical boundaries and revealing where a practical\nquantum advantage may arise. Our work quantitatively determine how advanced a\nfault-tolerant quantum computer should be to achieve possible, significant\nbenefits on problems related to real-world.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG",
      "math.OC"
    ],
    "primary_category": "quant-ph",
    "comment": "28 pages, many figures. v2: correcting typos",
    "pdf_url": "http://arxiv.org/pdf/2502.11239v2",
    "published_date": "2025-02-16 19:12:32 UTC",
    "updated_date": "2025-02-18 03:35:48 UTC"
  },
  {
    "arxiv_id": "2502.14889v1",
    "title": "Narrowing Information Bottleneck Theory for Multimodal Image-Text Representations Interpretability",
    "authors": [
      "Zhiyu Zhu",
      "Zhibo Jin",
      "Jiayu Zhang",
      "Nan Yang",
      "Jiahao Huang",
      "Jianlong Zhou",
      "Fang Chen"
    ],
    "abstract": "The task of identifying multimodal image-text representations has garnered\nincreasing attention, particularly with models such as CLIP (Contrastive\nLanguage-Image Pretraining), which demonstrate exceptional performance in\nlearning complex associations between images and text. Despite these\nadvancements, ensuring the interpretability of such models is paramount for\ntheir safe deployment in real-world applications, such as healthcare. While\nnumerous interpretability methods have been developed for unimodal tasks, these\napproaches often fail to transfer effectively to multimodal contexts due to\ninherent differences in the representation structures. Bottleneck methods,\nwell-established in information theory, have been applied to enhance CLIP's\ninterpretability. However, they are often hindered by strong assumptions or\nintrinsic randomness. To overcome these challenges, we propose the Narrowing\nInformation Bottleneck Theory, a novel framework that fundamentally redefines\nthe traditional bottleneck approach. This theory is specifically designed to\nsatisfy contemporary attribution axioms, providing a more robust and reliable\nsolution for improving the interpretability of multimodal models. In our\nexperiments, compared to state-of-the-art methods, our approach enhances image\ninterpretability by an average of 9%, text interpretability by an average of\n58.83%, and accelerates processing speed by 63.95%. Our code is publicly\naccessible at https://github.com/LMBTough/NIB.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.14889v1",
    "published_date": "2025-02-16 19:01:37 UTC",
    "updated_date": "2025-02-16 19:01:37 UTC"
  },
  {
    "arxiv_id": "2502.12207v1",
    "title": "PAR-AdvGAN: Improving Adversarial Attack Capability with Progressive Auto-Regression AdvGAN",
    "authors": [
      "Jiayu Zhang",
      "Zhiyu Zhu",
      "Xinyi Wang",
      "Silin Liao",
      "Zhibo Jin",
      "Flora D. Salim",
      "Huaming Chen"
    ],
    "abstract": "Deep neural networks have demonstrated remarkable performance across various\ndomains. However, they are vulnerable to adversarial examples, which can lead\nto erroneous predictions. Generative Adversarial Networks (GANs) can leverage\nthe generators and discriminators model to quickly produce high-quality\nadversarial examples. Since both modules train in a competitive and\nsimultaneous manner, GAN-based algorithms like AdvGAN can generate adversarial\nexamples with better transferability compared to traditional methods. However,\nthe generation of perturbations is usually limited to a single iteration,\npreventing these examples from fully exploiting the potential of the methods.\nTo tackle this issue, we introduce a novel approach named Progressive\nAuto-Regression AdvGAN (PAR-AdvGAN). It incorporates an auto-regressive\niteration mechanism within a progressive generation network to craft\nadversarial examples with enhanced attack capability. We thoroughly evaluate\nour PAR-AdvGAN method with a large-scale experiment, demonstrating its superior\nperformance over various state-of-the-art black-box adversarial attacks, as\nwell as the original AdvGAN.Moreover, PAR-AdvGAN significantly accelerates the\nadversarial example generation, i.e., achieving the speeds of up to 335.5\nframes per second on Inception-v3 model, outperforming the gradient-based\ntransferable attack algorithms. Our code is available at:\nhttps://anonymous.4open.science/r/PAR-01BF/",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12207v1",
    "published_date": "2025-02-16 19:00:55 UTC",
    "updated_date": "2025-02-16 19:00:55 UTC"
  },
  {
    "arxiv_id": "2502.13162v1",
    "title": "ShieldLearner: A New Paradigm for Jailbreak Attack Defense in LLMs",
    "authors": [
      "Ziyi Ni",
      "Hao Wang",
      "Huacan Wang"
    ],
    "abstract": "Large Language Models (LLMs) have achieved remarkable success in various\ndomains but remain vulnerable to adversarial jailbreak attacks. Existing\nprompt-defense strategies, including parameter-modifying and parameter-free\napproaches, face limitations in adaptability, interpretability, and\ncustomization, constraining their effectiveness against evolving threats. To\naddress these challenges, we propose ShieldLearner, a novel paradigm that\nmimics human learning in defense. Through trial and error, it autonomously\ndistills attack signatures into a Pattern Atlas and synthesizes defense\nheuristics into a Meta-analysis Framework, enabling systematic and\ninterpretable threat detection. Furthermore, we introduce Adaptive Adversarial\nAugmentation to generate adversarial variations of successfully defended\nprompts, enabling continuous self-improvement without model retraining. In\naddition to standard benchmarks, we create a hard test set by curating\nadversarial prompts from the Wildjailbreak dataset, emphasizing more concealed\nmalicious intent. Experimental results show that ShieldLearner achieves a\nsignificantly higher defense success rate than existing baselines on both\nconventional and hard test sets, while also operating with lower computational\noverhead, making it a practical and efficient solution for real-world\nadversarial defense.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.13162v1",
    "published_date": "2025-02-16 18:47:41 UTC",
    "updated_date": "2025-02-16 18:47:41 UTC"
  },
  {
    "arxiv_id": "2502.11228v1",
    "title": "Vendi-RAG: Adaptively Trading-Off Diversity And Quality Significantly Improves Retrieval Augmented Generation With LLMs",
    "authors": [
      "Mohammad Reza Rezaei",
      "Adji Bousso Dieng"
    ],
    "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\nfor domain-specific question-answering (QA) tasks by leveraging external\nknowledge sources. However, traditional RAG systems primarily focus on\nrelevance-based retrieval and often struggle with redundancy, especially when\nreasoning requires connecting information from multiple sources. This paper\nintroduces Vendi-RAG, a framework based on an iterative process that jointly\noptimizes retrieval diversity and answer quality. This joint optimization leads\nto significantly higher accuracy for multi-hop QA tasks. Vendi-RAG leverages\nthe Vendi Score (VS), a flexible similarity-based diversity metric, to promote\nsemantic diversity in document retrieval. It then uses an LLM judge that\nevaluates candidate answers, generated after a reasoning step, and outputs a\nscore that the retriever uses to balance relevance and diversity among the\nretrieved documents during each iteration. Experiments on three challenging\ndatasets -- HotpotQA, MuSiQue, and 2WikiMultiHopQA -- demonstrate Vendi-RAG's\neffectiveness in multi-hop reasoning tasks. The framework achieves significant\naccuracy improvements over traditional single-step and multi-step RAG\napproaches, with accuracy increases reaching up to +4.2% on HotpotQA, +4.1% on\n2WikiMultiHopQA, and +1.3% on MuSiQue compared to Adaptive-RAG, the current\nbest baseline. The benefits of Vendi-RAG are even more pronounced as the number\nof retrieved documents increases. Finally, we evaluated Vendi-RAG across\ndifferent LLM backbones, including GPT-3.5, GPT-4, and GPT-4o-mini, and\nobserved consistent improvements, demonstrating that the framework's advantages\nare model-agnostic.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "A RAG pipeline that accounts for both diversity and answer quality\n  and that can be used with any LLM backbone to solve complex multi-hop\n  question-answering tasks",
    "pdf_url": "http://arxiv.org/pdf/2502.11228v1",
    "published_date": "2025-02-16 18:46:10 UTC",
    "updated_date": "2025-02-16 18:46:10 UTC"
  },
  {
    "arxiv_id": "2502.11225v1",
    "title": "METAFOR: A Hybrid Metaheuristics Software Framework for Single-Objective Continuous Optimization Problems",
    "authors": [
      "Christian Camacho-Villalón",
      "Marco Dorigo",
      "Thomas Stützle"
    ],
    "abstract": "Hybrid metaheuristics are powerful techniques for solving difficult\noptimization problems that exploit the strengths of different approaches in a\nsingle implementation. For algorithm designers, however, creating hybrid\nmetaheuristic implementations has become increasingly challenging due to the\nvast number of design options available in the literature and the fact that\nthey often rely on their knowledge and intuition to come up with new algorithm\ndesigns. In this paper, we propose a modular metaheuristic software framework,\ncalled METAFOR, that can be coupled with an automatic algorithm configuration\ntool to automatically design hybrid metaheuristics. METAFOR is specifically\ndesigned to hybridize Particle Swarm Optimization, Differential Evolution and\nCovariance Matrix Adaptation-Evolution Strategy, and includes a local search\nmodule that allows their execution to be interleaved with a subordinate local\nsearch. We use the configuration tool irace to automatically generate 17\ndifferent metaheuristic implementations and evaluate their performance on a\ndiverse set of continuous optimization problems. Our results show that, across\nall the considered problem classes, automatically generated hybrid\nimplementations are able to outperform configured single-approach\nimplementations, while these latter offer advantages on specific classes of\nfunctions. We provide useful insights on the type of hybridization that works\nbest for specific problem classes, the algorithm components that contribute to\nthe performance of the algorithms, and the advantages and disadvantages of two\nwell-known instance separation strategies, creating stratified training set\nusing a fix percentage and leave-one-class-out cross-validation.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11225v1",
    "published_date": "2025-02-16 18:24:44 UTC",
    "updated_date": "2025-02-16 18:24:44 UTC"
  },
  {
    "arxiv_id": "2502.13161v1",
    "title": "Noumenal Labs White Paper: How To Build A Brain",
    "authors": [
      "Maxwell J. D. Ramstead",
      "Candice Pattisapu",
      "Jason Fox",
      "Jeff Beck"
    ],
    "abstract": "This white paper describes some of the design principles for artificial or\nmachine intelligence that guide efforts at Noumenal Labs. These principles are\ndrawn from both nature and from the means by which we come to represent and\nunderstand it. The end goal of research and development in this field should be\nto design machine intelligences that augment our understanding of the world and\nenhance our ability to act in it, without replacing us. In the first two\nsections, we examine the core motivation for our approach: resolving the\ngrounding problem. We argue that the solution to the grounding problem rests in\nthe design of models grounded in the world that we inhabit, not mere word\nmodels. A machine super intelligence that is capable of significantly enhancing\nour understanding of the human world must represent the world as we do and be\ncapable of generating new knowledge, building on what we already know. In other\nwords, it must be properly grounded and explicitly designed for rational,\nempirical inquiry, modeled after the scientific method. A primary implication\nof this design principle is that agents must be capable of engaging\nautonomously in causal physics discovery. We discuss the pragmatic implications\nof this approach, and in particular, the use cases in realistic 3D world\nmodeling and multimodal, multidimensional time series analysis.",
    "categories": [
      "q-bio.NC",
      "cs.AI"
    ],
    "primary_category": "q-bio.NC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.13161v1",
    "published_date": "2025-02-16 18:15:37 UTC",
    "updated_date": "2025-02-16 18:15:37 UTC"
  },
  {
    "arxiv_id": "2502.11221v1",
    "title": "PlanGenLLMs: A Modern Survey of LLM Planning Capabilities",
    "authors": [
      "Hui Wei",
      "Zihao Zhang",
      "Shenghua He",
      "Tian Xia",
      "Shijia Pan",
      "Fei Liu"
    ],
    "abstract": "LLMs have immense potential for generating plans, transforming an initial\nworld state into a desired goal state. A large body of research has explored\nthe use of LLMs for various planning tasks, from web navigation to travel\nplanning and database querying. However, many of these systems are tailored to\nspecific problems, making it challenging to compare them or determine the best\napproach for new tasks. There is also a lack of clear and consistent evaluation\ncriteria. Our survey aims to offer a comprehensive overview of current LLM\nplanners to fill this gap. It builds on foundational work by Kartam and Wilkins\n(1990) and examines six key performance criteria: completeness, executability,\noptimality, representation, generalization, and efficiency. For each, we\nprovide a thorough analysis of representative works and highlight their\nstrengths and weaknesses. Our paper also identifies crucial future directions,\nmaking it a valuable resource for both practitioners and newcomers interested\nin leveraging LLM planning to support agentic workflows.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Preprint. Under review",
    "pdf_url": "http://arxiv.org/pdf/2502.11221v1",
    "published_date": "2025-02-16 17:54:57 UTC",
    "updated_date": "2025-02-16 17:54:57 UTC"
  },
  {
    "arxiv_id": "2502.11213v1",
    "title": "Stochastic Optimization of Inventory at Large-scale Supply Chains",
    "authors": [
      "Zhaoyang Larry Jin",
      "Mehdi Maasoumy",
      "Yimin Liu",
      "Zeshi Zheng",
      "Zizhuo Ren"
    ],
    "abstract": "Today's global supply chains face growing challenges due to rapidly changing\nmarket conditions, increased network complexity and inter-dependency, and\ndynamic uncertainties in supply, demand, and other factors. To combat these\nchallenges, organizations employ Material Requirements Planning (MRP) software\nsolutions to set inventory stock buffers - for raw materials, work-in-process\ngoods, and finished products - to help them meet customer service levels.\nHowever, holding excess inventory further complicates operations and can lock\nup millions of dollars of capital that could be otherwise deployed.\nFurthermore, most commercially available MRP solutions fall short in\nconsidering uncertainties and do not result in optimal solutions for modern\nenterprises.\n  At C3 AI, we fundamentally reformulate the inventory management problem as a\nconstrained stochastic optimization. We then propose a simulation-optimization\nframework that minimizes inventory and related costs while maintaining desired\nservice levels. The framework's goal is to find the optimal reorder parameters\nthat minimize costs subject to a pre-defined service-level constraint and all\nother real-world operational constraints. These optimal reorder parameters can\nbe fed back into an MRP system to drive optimal order placement, or used to\nplace optimal orders directly. This approach has proven successful in reducing\ninventory levels by 10-35 percent, resulting in hundreds of millions of dollars\nof economic benefit for major enterprises at a global scale.",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "math.OC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11213v1",
    "published_date": "2025-02-16 17:25:50 UTC",
    "updated_date": "2025-02-16 17:25:50 UTC"
  },
  {
    "arxiv_id": "2502.11211v1",
    "title": "A Survey of LLM-based Agents in Medicine: How far are we from Baymax?",
    "authors": [
      "Wenxuan Wang",
      "Zizhan Ma",
      "Zheng Wang",
      "Chenghan Wu",
      "Wenting Chen",
      "Xiang Li",
      "Yixuan Yuan"
    ],
    "abstract": "Large Language Models (LLMs) are transforming healthcare through the\ndevelopment of LLM-based agents that can understand, reason about, and assist\nwith medical tasks. This survey provides a comprehensive review of LLM-based\nagents in medicine, examining their architectures, applications, and\nchallenges. We analyze the key components of medical agent systems, including\nsystem profiles, clinical planning mechanisms, medical reasoning frameworks,\nand external capacity enhancement. The survey covers major application\nscenarios such as clinical decision support, medical documentation, training\nsimulations, and healthcare service optimization. We discuss evaluation\nframeworks and metrics used to assess these agents' performance in healthcare\nsettings. While LLM-based agents show promise in enhancing healthcare delivery,\nseveral challenges remain, including hallucination management, multimodal\nintegration, implementation barriers, and ethical considerations. The survey\nconcludes by highlighting future research directions, including advances in\nmedical reasoning inspired by recent developments in LLM architectures,\nintegration with physical systems, and improvements in training simulations.\nThis work provides researchers and practitioners with a structured overview of\nthe current state and future prospects of LLM-based agents in medicine.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11211v1",
    "published_date": "2025-02-16 17:21:05 UTC",
    "updated_date": "2025-02-16 17:21:05 UTC"
  },
  {
    "arxiv_id": "2502.11201v2",
    "title": "Bridging the Gap: Enabling Natural Language Queries for NoSQL Databases through Text-to-NoSQL Translation",
    "authors": [
      "Jinwei Lu",
      "Yuanfeng Song",
      "Zhiqian Qin",
      "Haodi Zhang",
      "Chen Zhang",
      "Raymond Chi-Wing Wong"
    ],
    "abstract": "NoSQL databases have become increasingly popular due to their outstanding\nperformance in handling large-scale, unstructured, and semi-structured data,\nhighlighting the need for user-friendly interfaces to bridge the gap between\nnon-technical users and complex database queries. In this paper, we introduce\nthe Text-to-NoSQL task, which aims to convert natural language queries into\nNoSQL queries, thereby lowering the technical barrier for non-expert users. To\npromote research in this area, we developed a novel automated dataset\nconstruction process and released a large-scale and open-source dataset for\nthis task, named TEND (short for Text-to-NoSQL Dataset). Additionally, we\ndesigned a SLM (Small Language Model)-assisted and RAG (Retrieval-augmented\nGeneration)-assisted multi-step framework called SMART, which is specifically\ndesigned for Text-to-NoSQL conversion. To ensure comprehensive evaluation of\nthe models, we also introduced a detailed set of metrics that assess the\nmodel's performance from both the query itself and its execution results. Our\nexperimental results demonstrate the effectiveness of our approach and\nestablish a benchmark for future research in this emerging field. We believe\nthat our contributions will pave the way for more accessible and intuitive\ninteractions with NoSQL databases.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11201v2",
    "published_date": "2025-02-16 17:01:48 UTC",
    "updated_date": "2025-02-18 06:48:28 UTC"
  },
  {
    "arxiv_id": "2502.11196v1",
    "title": "How Do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on Continual Pre-Training",
    "authors": [
      "Yixin Ou",
      "Yunzhi Yao",
      "Ningyu Zhang",
      "Hui Jin",
      "Jiacheng Sun",
      "Shumin Deng",
      "Zhenguo Li",
      "Huajun Chen"
    ],
    "abstract": "Despite exceptional capabilities in knowledge-intensive tasks, Large Language\nModels (LLMs) face a critical gap in understanding how they internalize new\nknowledge, particularly how to structurally embed acquired knowledge in their\nneural computations. We address this issue through the lens of knowledge\ncircuit evolution, identifying computational subgraphs that facilitate\nknowledge storage and processing. Our systematic analysis of circuit evolution\nthroughout continual pre-training reveals several key findings: (1) the\nacquisition of new knowledge is influenced by its relevance to pre-existing\nknowledge; (2) the evolution of knowledge circuits exhibits a distinct phase\nshift from formation to optimization; (3) the evolution of knowledge circuits\nfollows a deep-to-shallow pattern. These insights not only advance our\ntheoretical understanding of the mechanisms of new knowledge acquisition in\nLLMs, but also provide potential implications for improving continual\npre-training strategies to enhance model performance. Code and data will be\navailable at https://github.com/zjunlp/DynamicKnowledgeCircuits.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2502.11196v1",
    "published_date": "2025-02-16 16:55:43 UTC",
    "updated_date": "2025-02-16 16:55:43 UTC"
  },
  {
    "arxiv_id": "2502.11195v1",
    "title": "From Deception to Perception: The Surprising Benefits of Deepfakes for Detecting, Measuring, and Mitigating Bias",
    "authors": [
      "Yizhi Liu",
      "Balaji Padmanabhan",
      "Siva Viswanathan"
    ],
    "abstract": "While deepfake technologies have predominantly been criticized for potential\nmisuse, our study demonstrates their significant potential as tools for\ndetecting, measuring, and mitigating biases in key societal domains. By\nemploying deepfake technology to generate controlled facial images, we extend\nthe scope of traditional correspondence studies beyond mere textual\nmanipulations. This enhancement is crucial in scenarios such as pain\nassessments, where subjective biases triggered by sensitive features in facial\nimages can profoundly affect outcomes. Our results reveal that deepfakes not\nonly maintain the effectiveness of correspondence studies but also introduce\ngroundbreaking advancements in bias measurement and correction techniques. This\nstudy emphasizes the constructive role of deepfake technologies as essential\ntools for advancing societal equity and fairness.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.2.0; I.2.10; I.4.0; J.4; H.4; K.4.1; K.4.2"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11195v1",
    "published_date": "2025-02-16 16:55:28 UTC",
    "updated_date": "2025-02-16 16:55:28 UTC"
  },
  {
    "arxiv_id": "2502.11191v1",
    "title": "Primus: A Pioneering Collection of Open-Source Datasets for Cybersecurity LLM Training",
    "authors": [
      "Yao-Ching Yu",
      "Tsun-Han Chiang",
      "Cheng-Wei Tsai",
      "Chien-Ming Huang",
      "Wen-Kwang Tsao"
    ],
    "abstract": "Large Language Models (LLMs) have shown remarkable advancements in\nspecialized fields such as finance, law, and medicine. However, in\ncybersecurity, we have noticed a lack of open-source datasets, with a\nparticular lack of high-quality cybersecurity pretraining corpora, even though\nmuch research indicates that LLMs acquire their knowledge during pretraining.\nTo address this, we present a comprehensive suite of datasets covering all\nmajor training stages, including pretraining, instruction fine-tuning, and\nreasoning distillation with cybersecurity-specific self-reflection data.\nExtensive ablation studies demonstrate their effectiveness on public\ncybersecurity benchmarks. In particular, continual pre-training on our dataset\nyields a 15.88% improvement in the aggregate score, while reasoning\ndistillation leads to a 10% gain in security certification (CISSP). We will\nrelease all datasets and trained cybersecurity LLMs under the ODC-BY and MIT\nlicenses to encourage further research in the community. For access to all\ndatasets and model weights, please refer to\nhttps://huggingface.co/collections/trendmicro-ailab/primus-67b1fd27052b802b4af9d243.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11191v1",
    "published_date": "2025-02-16 16:34:49 UTC",
    "updated_date": "2025-02-16 16:34:49 UTC"
  },
  {
    "arxiv_id": "2502.11190v2",
    "title": "ReLearn: Unlearning via Learning for Large Language Models",
    "authors": [
      "Haoming Xu",
      "Ningyuan Zhao",
      "Liming Yang",
      "Sendong Zhao",
      "Shumin Deng",
      "Mengru Wang",
      "Bryan Hooi",
      "Nay Oo",
      "Huajun Chen",
      "Ningyu Zhang"
    ],
    "abstract": "Current unlearning methods for large language models usually rely on reverse\noptimization to reduce target token probabilities. However, this paradigm\ndisrupts the subsequent tokens prediction, degrading model performance and\nlinguistic coherence. Moreover, existing evaluation metrics overemphasize\ncontextual forgetting while inadequately assessing response fluency and\nrelevance. To address these challenges, we propose ReLearn, a data augmentation\nand fine-tuning pipeline for effective unlearning, along with a comprehensive\nevaluation framework. This framework introduces Knowledge Forgetting Rate (KFR)\nand Knowledge Retention Rate (KRR) to measure knowledge-level preservation, and\nLinguistic Score (LS) to evaluate generation quality. Our experiments show that\nReLearn successfully achieves targeted forgetting while preserving high-quality\noutput. Through mechanistic analysis, we further demonstrate how reverse\noptimization disrupts coherent text generation, while ReLearn preserves this\nessential capability. Code is available at https://github.com/zjunlp/unlearn.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2502.11190v2",
    "published_date": "2025-02-16 16:31:00 UTC",
    "updated_date": "2025-03-20 17:20:55 UTC"
  },
  {
    "arxiv_id": "2502.12206v1",
    "title": "Evaluating the Paperclip Maximizer: Are RL-Based Language Models More Likely to Pursue Instrumental Goals?",
    "authors": [
      "Yufei He",
      "Yuexin Li",
      "Jiaying Wu",
      "Yuan Sui",
      "Yulin Chen",
      "Bryan Hooi"
    ],
    "abstract": "As large language models (LLMs) continue to evolve, ensuring their alignment\nwith human goals and values remains a pressing challenge. A key concern is\n\\textit{instrumental convergence}, where an AI system, in optimizing for a\ngiven objective, develops unintended intermediate goals that override the\nultimate objective and deviate from human-intended goals. This issue is\nparticularly relevant in reinforcement learning (RL)-trained models, which can\ngenerate creative but unintended strategies to maximize rewards. In this paper,\nwe explore instrumental convergence in LLMs by comparing models trained with\ndirect RL optimization (e.g., the o1 model) to those trained with reinforcement\nlearning from human feedback (RLHF). We hypothesize that RL-driven models\nexhibit a stronger tendency for instrumental convergence due to their\noptimization of goal-directed behavior in ways that may misalign with human\nintentions. To assess this, we introduce InstrumentalEval, a benchmark for\nevaluating instrumental convergence in RL-trained LLMs. Initial experiments\nreveal cases where a model tasked with making money unexpectedly pursues\ninstrumental objectives, such as self-replication, implying signs of\ninstrumental convergence. Our findings contribute to a deeper understanding of\nalignment challenges in AI systems and the risks posed by unintended model\nbehaviors.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12206v1",
    "published_date": "2025-02-16 16:29:20 UTC",
    "updated_date": "2025-02-16 16:29:20 UTC"
  },
  {
    "arxiv_id": "2502.11187v2",
    "title": "TituLLMs: A Family of Bangla LLMs with Comprehensive Benchmarking",
    "authors": [
      "Shahriar Kabir Nahin",
      "Rabindra Nath Nandi",
      "Sagor Sarker",
      "Quazi Sarwar Muhtaseem",
      "Md Kowsher",
      "Apu Chandraw Shill",
      "Md Ibrahim",
      "Mehadi Hasan Menon",
      "Tareq Al Muntasir",
      "Firoj Alam"
    ],
    "abstract": "In this paper, we present TituLLMs, the first large pretrained Bangla LLMs,\navailable in 1b and 3b parameter sizes. Due to computational constraints during\nboth training and inference, we focused on smaller models. To train TituLLMs,\nwe collected a pretraining dataset of approximately ~37 billion tokens. We\nextended the Llama-3.2 tokenizer to incorporate language- and culture-specific\nknowledge, which also enables faster training and inference. There was a lack\nof benchmarking datasets to benchmark LLMs for Bangla. To address this gap, we\ndeveloped five benchmarking datasets. We benchmarked various LLMs, including\nTituLLMs, and demonstrated that TituLLMs outperforms its initial multilingual\nversions. However, this is not always the case, highlighting the complexities\nof language adaptation. Our work lays the groundwork for adapting existing\nmultilingual open models to other low-resource languages. To facilitate broader\nadoption and further research, we have made the TituLLMs models and\nbenchmarking datasets publicly available\n(https://huggingface.co/collections/hishab/titulm-llama-family-6718d31fc1b83529276f490a).",
    "categories": [
      "cs.CL",
      "cs.AI",
      "68T50",
      "F.2.2; I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "LLMs, Benchmarking, Large Language Models, Bangla",
    "pdf_url": "http://arxiv.org/pdf/2502.11187v2",
    "published_date": "2025-02-16 16:22:23 UTC",
    "updated_date": "2025-02-21 10:33:37 UTC"
  },
  {
    "arxiv_id": "2502.11184v1",
    "title": "Can't See the Forest for the Trees: Benchmarking Multimodal Safety Awareness for Multimodal LLMs",
    "authors": [
      "Wenxuan Wang",
      "Xiaoyuan Liu",
      "Kuiyi Gao",
      "Jen-tse Huang",
      "Youliang Yuan",
      "Pinjia He",
      "Shuai Wang",
      "Zhaopeng Tu"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have expanded the capabilities of\ntraditional language models by enabling interaction through both text and\nimages. However, ensuring the safety of these models remains a significant\nchallenge, particularly in accurately identifying whether multimodal content is\nsafe or unsafe-a capability we term safety awareness. In this paper, we\nintroduce MMSafeAware, the first comprehensive multimodal safety awareness\nbenchmark designed to evaluate MLLMs across 29 safety scenarios with 1500\ncarefully curated image-prompt pairs. MMSafeAware includes both unsafe and\nover-safety subsets to assess models abilities to correctly identify unsafe\ncontent and avoid over-sensitivity that can hinder helpfulness. Evaluating nine\nwidely used MLLMs using MMSafeAware reveals that current models are not\nsufficiently safe and often overly sensitive; for example, GPT-4V misclassifies\n36.1% of unsafe inputs as safe and 59.9% of benign inputs as unsafe. We further\nexplore three methods to improve safety awareness-prompting-based approaches,\nvisual contrastive decoding, and vision-centric reasoning fine-tuning-but find\nthat none achieve satisfactory performance. Our findings highlight the profound\nchallenges in developing MLLMs with robust safety awareness, underscoring the\nneed for further research in this area. All the code and data will be publicly\navailable to facilitate future research.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.MM"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11184v1",
    "published_date": "2025-02-16 16:12:40 UTC",
    "updated_date": "2025-02-16 16:12:40 UTC"
  },
  {
    "arxiv_id": "2502.11181v1",
    "title": "Improving Scientific Document Retrieval with Concept Coverage-based Query Set Generation",
    "authors": [
      "SeongKu Kang",
      "Bowen Jin",
      "Wonbin Kweon",
      "Yu Zhang",
      "Dongha Lee",
      "Jiawei Han",
      "Hwanjo Yu"
    ],
    "abstract": "In specialized fields like the scientific domain, constructing large-scale\nhuman-annotated datasets poses a significant challenge due to the need for\ndomain expertise. Recent methods have employed large language models to\ngenerate synthetic queries, which serve as proxies for actual user queries.\nHowever, they lack control over the content generated, often resulting in\nincomplete coverage of academic concepts in documents. We introduce Concept\nCoverage-based Query set Generation (CCQGen) framework, designed to generate a\nset of queries with comprehensive coverage of the document's concepts. A key\ndistinction of CCQGen is that it adaptively adjusts the generation process\nbased on the previously generated queries. We identify concepts not\nsufficiently covered by previous queries, and leverage them as conditions for\nsubsequent query generation. This approach guides each new query to complement\nthe previous ones, aiding in a thorough understanding of the document.\nExtensive experiments demonstrate that CCQGen significantly enhances query\nquality and retrieval performance.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "WSDM 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.11181v1",
    "published_date": "2025-02-16 15:59:50 UTC",
    "updated_date": "2025-02-16 15:59:50 UTC"
  },
  {
    "arxiv_id": "2502.11179v1",
    "title": "RT-DEMT: A hybrid real-time acupoint detection model combining mamba and transformer",
    "authors": [
      "Shilong Yang",
      "Qi Zang",
      "Chulong Zhang",
      "Lingfeng Huang",
      "Yaoqin Xie"
    ],
    "abstract": "Traditional Chinese acupuncture methods often face controversy in clinical\npractice due to their high subjectivity. Additionally, current\nintelligent-assisted acupuncture systems have two major limitations: slow\nacupoint localization speed and low accuracy. To address these limitations, a\nnew method leverages the excellent inference efficiency of the state-space\nmodel Mamba, while retaining the advantages of the attention mechanism in the\ntraditional DETR architecture, to achieve efficient global information\nintegration and provide high-quality feature information for acupoint\nlocalization tasks. Furthermore, by employing the concept of residual\nlikelihood estimation, it eliminates the need for complex upsampling processes,\nthereby accelerating the acupoint localization task. Our method achieved\nstate-of-the-art (SOTA) accuracy on a private dataset of acupoints on the human\nback, with an average Euclidean distance pixel error (EPE) of 7.792 and an\naverage time consumption of 10.05 milliseconds per localization task. Compared\nto the second-best algorithm, our method improved both accuracy and speed by\napproximately 14\\%. This significant advancement not only enhances the efficacy\nof acupuncture treatment but also demonstrates the commercial potential of\nautomated acupuncture robot systems. Access to our method is available at\nhttps://github.com/Sohyu1/RT-DEMT",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.11179v1",
    "published_date": "2025-02-16 15:59:06 UTC",
    "updated_date": "2025-02-16 15:59:06 UTC"
  },
  {
    "arxiv_id": "2502.11168v1",
    "title": "Knowing Your Target: Target-Aware Transformer Makes Better Spatio-Temporal Video Grounding",
    "authors": [
      "Xin Gu",
      "Yaojie Shen",
      "Chenxi Luo",
      "Tiejian Luo",
      "Yan Huang",
      "Yuewei Lin",
      "Heng Fan",
      "Libo Zhang"
    ],
    "abstract": "Transformer has attracted increasing interest in STVG, owing to its\nend-to-end pipeline and promising result. Existing Transformer-based STVG\napproaches often leverage a set of object queries, which are initialized simply\nusing zeros and then gradually learn target position information via iterative\ninteractions with multimodal features, for spatial and temporal localization.\nDespite simplicity, these zero object queries, due to lacking target-specific\ncues, are hard to learn discriminative target information from interactions\nwith multimodal features in complicated scenarios (\\e.g., with distractors or\nocclusion), resulting in degradation. Addressing this, we introduce a novel\nTarget-Aware Transformer for STVG (TA-STVG), which seeks to adaptively generate\nobject queries via exploring target-specific cues from the given video-text\npair, for improving STVG. The key lies in two simple yet effective modules,\ncomprising text-guided temporal sampling (TTS) and attribute-aware spatial\nactivation (ASA), working in a cascade. The former focuses on selecting\ntarget-relevant temporal cues from a video utilizing holistic text information,\nwhile the latter aims at further exploiting the fine-grained visual attribute\ninformation of the object from previous target-aware temporal cues, which is\napplied for object query initialization. Compared to existing methods\nleveraging zero-initialized queries, object queries in our TA-STVG, directly\ngenerated from a given video-text pair, naturally carry target-specific cues,\nmaking them adaptive and better interact with multimodal features for learning\nmore discriminative information to improve STVG. In our experiments on three\nbenchmarks, TA-STVG achieves state-of-the-art performance and significantly\noutperforms the baseline, validating its efficacy.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11168v1",
    "published_date": "2025-02-16 15:38:33 UTC",
    "updated_date": "2025-02-16 15:38:33 UTC"
  },
  {
    "arxiv_id": "2502.11164v5",
    "title": "Quantifying the Capability Boundary of DeepSeek Models: An Application-Driven Performance Analysis",
    "authors": [
      "Kaikai Zhao",
      "Zhaoxiang Liu",
      "Xuejiao Lei",
      "Jiaojiao Zhao",
      "Zhenhong Long",
      "Zipeng Wang",
      "Ning Wang",
      "Meijuan An",
      "Qingliang Meng",
      "Peijun Yang",
      "Minjie Hua",
      "Chaoyang Ma",
      "Wen Liu",
      "Kai Wang",
      "Shiguo Lian"
    ],
    "abstract": "DeepSeek-R1, known for its low training cost and exceptional reasoning\ncapabilities, has achieved state-of-the-art performance on various benchmarks.\nHowever, detailed evaluations for DeepSeek Series models from the perspective\nof real-world applications are lacking, making it challenging for users to\nselect the most suitable DeepSeek models for their specific needs. To address\nthis gap, we presents the first comprehensive evaluation of the DeepSeek and\nits related models (including DeepSeek-V3, DeepSeek-R1,\nDeepSeek-R1-Distill-Qwen series, DeepSeek-R1-Distill-Llama series, their\ncorresponding 4-bit quantized models, and the reasoning model QwQ-32B) using\nour enhanced A-Eval benchmark, A-Eval-2.0. Our systematic analysis reveals\nseveral key insights: (1) Given identical model architectures and training\ndata, larger parameter models demonstrate superior performance, aligning with\nthe scaling law. However, smaller models may achieve enhanced capabilities when\nemploying optimized training strategies and higher-quality data; (2)\nReasoning-enhanced model show significant performance gains in logical\nreasoning tasks but may underperform in text understanding and generation\ntasks; (3) As the data difficulty increases, distillation or reasoning\nenhancements yield higher performance gains for the models. Interestingly,\nreasoning enhancements can even have a negative impact on simpler problems; (4)\nQuantization impacts different capabilities unevenly, with significant drop on\nlogical reasoning and minimal impact on text generation. Based on these results\nand findings, we design an model selection handbook enabling users to select\nthe most cost-effective models without efforts.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11164v5",
    "published_date": "2025-02-16 15:29:58 UTC",
    "updated_date": "2025-05-16 01:20:01 UTC"
  },
  {
    "arxiv_id": "2502.11157v1",
    "title": "Dyve: Thinking Fast and Slow for Dynamic Process Verification",
    "authors": [
      "Jianyuan Zhong",
      "Zeju Li",
      "Zhijian Xu",
      "Xiangyu Wen",
      "Qiang Xu"
    ],
    "abstract": "We present Dyve, a dynamic process verifier that enhances reasoning error\ndetection in large language models by integrating fast and slow thinking,\ninspired by Kahneman's Systems Theory. Dyve adaptively applies immediate\ntoken-level confirmation System 1 for straightforward steps and comprehensive\nanalysis System 2 for complex ones. Leveraging a novel step-wise\nconsensus-filtered process supervision technique, combining Monte Carlo\nestimation with LLM based evaluation, Dyve curates high-quality supervision\nsignals from noisy data. Experimental results on ProcessBench and the MATH\ndataset confirm that Dyve significantly outperforms existing process-based\nverifiers and boosts performance in Best-of-N settings.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.11157v1",
    "published_date": "2025-02-16 15:11:19 UTC",
    "updated_date": "2025-02-16 15:11:19 UTC"
  },
  {
    "arxiv_id": "2502.11155v1",
    "title": "Uncertainty-Aware Search and Value Models: Mitigating Search Scaling Flaws in LLMs",
    "authors": [
      "Fei Yu",
      "Yingru Li",
      "Benyou Wang"
    ],
    "abstract": "Value model-guided search is effective in steering the generation but suffers\nfrom scaling flaws: Its superiority diminishes with larger sample sizes,\nunderperforming non-search baselines. This limitation arises from reliability\ndegradation in value models in unseen reasoning paths. To address this, we\npropose an uncertainty-aware search framework that includes two key components:\n(1) uncertainty-aware value models that incorporate uncertainty into\npredictions, and (2) an uncertainty-aware selection process using the proposed\nefficient Group Thompson Sampling algorithm. Experiments on GSM8K show that our\nmethod mitigates search scaling flaws, achieving 90.5% coverage at 16 samples\ncompared to 85.8% for conventional value-guided search. This work establishes\nthe first systematic integration of uncertainty quantification in LLM search\nparadigms.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11155v1",
    "published_date": "2025-02-16 15:10:30 UTC",
    "updated_date": "2025-02-16 15:10:30 UTC"
  },
  {
    "arxiv_id": "2503.16444v1",
    "title": "Conversational Explanations: Discussing Explainable AI with Non-AI Experts",
    "authors": [
      "Tong Zhang",
      "Mengao Zhang",
      "Wei Yan Low",
      "X. Jessie Yang",
      "Boyang Li"
    ],
    "abstract": "Explainable AI (XAI) aims to provide insights into the decisions made by AI\nmodels. To date, most XAI approaches provide only one-time, static\nexplanations, which cannot cater to users' diverse knowledge levels and\ninformation needs. Conversational explanations have been proposed as an\neffective method to customize XAI explanations. However, building\nconversational explanation systems is hindered by the scarcity of training\ndata. Training with synthetic data faces two main challenges: lack of data\ndiversity and hallucination in the generated data. To alleviate these issues,\nwe introduce a repetition penalty to promote data diversity and exploit a\nhallucination detector to filter out untruthful synthetic conversation turns.\nWe conducted both automatic and human evaluations on the proposed system,\nfEw-shot Multi-round ConvErsational Explanation (EMCEE). For automatic\nevaluation, EMCEE achieves relative improvements of 81.6% in BLEU and 80.5% in\nROUGE compared to the baselines. EMCEE also mitigates the degeneration of data\nquality caused by training on synthetic data. In human evaluations (N=60),\nEMCEE outperforms baseline models and the control group in improving users'\ncomprehension, acceptance, trust, and collaboration with static explanations by\nlarge margins. Through a fine-grained analysis of model responses, we further\ndemonstrate that training on self-generated synthetic data improves the model's\nability to generate more truthful and understandable answers, leading to better\nuser interactions. To the best of our knowledge, this is the first\nconversational explanation method that can answer free-form user questions\nfollowing static explanations.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted to IUI 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.16444v1",
    "published_date": "2025-02-16 14:52:36 UTC",
    "updated_date": "2025-02-16 14:52:36 UTC"
  },
  {
    "arxiv_id": "2502.14888v2",
    "title": "Multi-Faceted Multimodal Monosemanticity",
    "authors": [
      "Hanqi Yan",
      "Xiangxiang Cui",
      "Lu Yin",
      "Paul Pu Liang",
      "Yulan He",
      "Yifei Wang"
    ],
    "abstract": "Humans experience the world through multiple modalities, such as, vision,\nlanguage, and speech, making it natural to explore the commonality and\ndistinctions among them. In this work, we take a data-driven approach to\naddress this question by analyzing interpretable, monosemantic features\nextracted from deep multimodal models. Specifically, we investigate CLIP, a\nprominent visual-language representation model trained on massive image-text\npairs. Building on prior research in single-modal interpretability, we develop\na set of multi-modal interpretability tools and measures designed to\ndisentangle and analyze features learned from CLIP. Specifically, we introduce\nthe Modality Dominance Score (MDS) to attribute each CLIP feature to a specific\nmodality. We then map CLIP features into a more interpretable space, enabling\nus to categorize them into three distinct classes: vision features\n(single-modal), language features (single-modal), and visual-language features\n(cross-modal). Interestingly, this data-driven categorization closely aligns\nwith human intuitive understandings of different modalities. We further show\nthat this modality decomposition can benefit multiple downstream tasks,\nincluding reducing bias in gender detection, generating cross-modal adversarial\nexamples, and enabling modal-specific feature control in text-to-image\ngeneration. These results indicate that large-scale multimodal models, when\nequipped with task-agnostic interpretability tools, can offer valuable insights\ninto the relationships between different data modalities.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14888v2",
    "published_date": "2025-02-16 14:51:07 UTC",
    "updated_date": "2025-05-19 13:20:34 UTC"
  },
  {
    "arxiv_id": "2502.11149v2",
    "title": "Large Language-Geometry Model: When LLM meets Equivariance",
    "authors": [
      "Zongzhao Li",
      "Jiacheng Cen",
      "Bing Su",
      "Wenbing Huang",
      "Tingyang Xu",
      "Yu Rong",
      "Deli Zhao"
    ],
    "abstract": "Accurately predicting 3D structures and dynamics of physical systems is\ncrucial in scientific applications. Existing approaches that rely on geometric\nGraph Neural Networks (GNNs) effectively enforce $\\mathrm{E}(3)$-equivariance,\nbut they often fall in leveraging extensive broader information. While direct\napplication of Large Language Models (LLMs) can incorporate external knowledge,\nthey lack the capability for spatial reasoning with guaranteed equivariance. In\nthis paper, we propose EquiLLM, a novel framework for representing 3D physical\nsystems that seamlessly integrates E(3)-equivariance with LLM capabilities.\nSpecifically, EquiLLM comprises four key components: geometry-aware prompting,\nan equivariant encoder, an LLM, and an equivariant adaptor. Essentially, the\nLLM guided by the instructive prompt serves as a sophisticated invariant\nfeature processor, while 3D directional information is exclusively handled by\nthe equivariant encoder and adaptor modules. Experimental results demonstrate\nthat EquiLLM delivers significant improvements over previous methods across\nmolecular dynamics simulation, human motion simulation, and antibody design,\nhighlighting its promising generalizability.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11149v2",
    "published_date": "2025-02-16 14:50:49 UTC",
    "updated_date": "2025-02-19 06:41:42 UTC"
  },
  {
    "arxiv_id": "2502.11147v1",
    "title": "Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity",
    "authors": [
      "Junhao Hu",
      "Wenrui Huang",
      "Weidong Wang",
      "Zhenwen Li",
      "Tiancheng Hu",
      "Zhixia Liu",
      "Xusheng Chen",
      "Tao Xie",
      "Yizhou Shan"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated strong capabilities across\nvarious domains, with recent advancements in challenging reasoning tasks such\nas mathematics and programming. However, solving reasoning tasks often requires\nlong decoding chains (of thoughts), which incur $O(N)$ time and memory\nconsumption, where $N$ is the chain length. To mitigate $O(N)$ time and memory\nconsumption, existing sparsity-based algorithms propose retaining only the most\ncritical token's intermediate data (i.e., key-value cache) and discarding the\nrest. However, these existing algorithms struggle with the ``impossible\ntrinity'' of accuracy, time, and memory. For example, the state-of-the-art\nalgorithm, Quest, achieves high accuracy with $O(L)$ time but $O(N)$ memory\n($L$ is the cache budget, $L \\ll N$). To address this issue, in this paper, we\nidentify a new attention pattern during the decode stage of reasoning tasks,\nwhere milestone tokens (analogous to lemmas in mathematical proofs) emerge, are\nutilized, and then become unimportant afterward. Based on this pattern, we\npropose a new algorithm named RaaS that identifies and retains milestone tokens\nonly until they are no longer needed, achieving high accuracy with $O(L)$ time\nand $O(L)$ memory complexity.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11147v1",
    "published_date": "2025-02-16 14:28:52 UTC",
    "updated_date": "2025-02-16 14:28:52 UTC"
  },
  {
    "arxiv_id": "2502.11142v3",
    "title": "NavRAG: Generating User Demand Instructions for Embodied Navigation through Retrieval-Augmented LLM",
    "authors": [
      "Zihan Wang",
      "Yaohui Zhu",
      "Gim Hee Lee",
      "Yachun Fan"
    ],
    "abstract": "Vision-and-Language Navigation (VLN) is an essential skill for embodied\nagents, allowing them to navigate in 3D environments following natural language\ninstructions. High-performance navigation models require a large amount of\ntraining data, the high cost of manually annotating data has seriously hindered\nthis field. Therefore, some previous methods translate trajectory videos into\nstep-by-step instructions for expanding data, but such instructions do not\nmatch well with users' communication styles that briefly describe destinations\nor state specific needs. Moreover, local navigation trajectories overlook\nglobal context and high-level task planning. To address these issues, we\npropose NavRAG, a retrieval-augmented generation (RAG) framework that generates\nuser demand instructions for VLN. NavRAG leverages LLM to build a hierarchical\nscene description tree for 3D scene understanding from global layout to local\ndetails, then simulates various user roles with specific demands to retrieve\nfrom the scene tree, generating diverse instructions with LLM. We annotate over\n2 million navigation instructions across 861 scenes and evaluate the data\nquality and navigation performance of trained models.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11142v3",
    "published_date": "2025-02-16 14:17:36 UTC",
    "updated_date": "2025-03-07 06:06:29 UTC"
  },
  {
    "arxiv_id": "2502.14887v1",
    "title": "Vision-Enhanced Time Series Forecasting via Latent Diffusion Models",
    "authors": [
      "Weilin Ruan",
      "Siru Zhong",
      "Haomin Wen",
      "Yuxuan Liang"
    ],
    "abstract": "Diffusion models have recently emerged as powerful frameworks for generating\nhigh-quality images. While recent studies have explored their application to\ntime series forecasting, these approaches face significant challenges in\ncross-modal modeling and transforming visual information effectively to capture\ntemporal patterns. In this paper, we propose LDM4TS, a novel framework that\nleverages the powerful image reconstruction capabilities of latent diffusion\nmodels for vision-enhanced time series forecasting. Instead of introducing\nexternal visual data, we are the first to use complementary transformation\ntechniques to convert time series into multi-view visual representations,\nallowing the model to exploit the rich feature extraction capabilities of the\npre-trained vision encoder. Subsequently, these representations are\nreconstructed using a latent diffusion model with a cross-modal conditioning\nmechanism as well as a fusion module. Experimental results demonstrate that\nLDM4TS outperforms various specialized forecasting models for time series\nforecasting tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14887v1",
    "published_date": "2025-02-16 14:15:06 UTC",
    "updated_date": "2025-02-16 14:15:06 UTC"
  },
  {
    "arxiv_id": "2502.11141v2",
    "title": "Cognitive Neural Architecture Search Reveals Hierarchical Entailment",
    "authors": [
      "Lukas Kuhn",
      "Sari Saba-Sadiya",
      "Gemma Roig"
    ],
    "abstract": "Recent research has suggested that the brain is more shallow than previously\nthought, challenging the traditionally assumed hierarchical structure of the\nventral visual pathway. Here, we demonstrate that optimizing convolutional\nnetwork architectures for brain-alignment via evolutionary neural architecture\nsearch results in models with clear representational hierarchies. Despite\nhaving random weights, the identified models achieve brain-alignment scores\nsurpassing even those of pretrained classification models - as measured by both\nregression and representational similarity analysis. Furthermore, through\ntraditional supervised training, architectures optimized for alignment with\nlate ventral regions become competitive classification models. These findings\nsuggest that hierarchical structure is a fundamental mechanism of primate\nvisual processing. Finally, this work demonstrates the potential of neural\narchitecture search as a framework for computational cognitive neuroscience\nresearch that could reduce the field's reliance on manually designed\nconvolutional networks.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "q-bio.QM"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11141v2",
    "published_date": "2025-02-16 14:13:04 UTC",
    "updated_date": "2025-05-01 15:51:42 UTC"
  },
  {
    "arxiv_id": "2502.11140v2",
    "title": "Automated Visualization Code Synthesis via Multi-Path Reasoning and Feedback-Driven Optimization",
    "authors": [
      "Wonduk Seo",
      "Seungyong Lee",
      "Daye Kang",
      "Hyunjin An",
      "Zonghao Yuan",
      "Seunghyun Lee"
    ],
    "abstract": "Rapid advancements in Large Language Models (LLMs) have accelerated their\nintegration into automated visualization code generation applications. Despite\nadvancements through few-shot prompting and query expansion, existing methods\nremain limited in handling ambiguous and complex queries, thereby requiring\nmanual intervention. To overcome these limitations, we propose VisPath: a\nMulti-Path Reasoning and Feedback-Driven Optimization Framework for\nVisualization Code Generation. VisPath handles underspecified queries through\nstructured, multi-stage processing. It begins by reformulating the user input\nvia Chain-of-Thought (CoT) prompting, which refers to the initial query while\ngenerating multiple extended queries in parallel, enabling the LLM to capture\ndiverse interpretations of the user intent. These queries then generate\ncandidate visualization scripts, which are executed to produce diverse images.\nBy assessing the visual quality and correctness of each output, VisPath\ngenerates targeted feedback that is aggregated to synthesize an optimal final\nresult. Extensive experiments on widely-used benchmarks including MatPlotBench\nand the Qwen-Agent Code Interpreter Benchmark show that VisPath outperforms\nstate-of-the-art methods, offering a more reliable solution for AI-driven\nvisualization code generation.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.SE",
    "comment": "16 pages, 5 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2502.11140v2",
    "published_date": "2025-02-16 14:09:42 UTC",
    "updated_date": "2025-05-21 02:10:54 UTC"
  },
  {
    "arxiv_id": "2502.11137v3",
    "title": "Safety Evaluation of DeepSeek Models in Chinese Contexts",
    "authors": [
      "Wenjing Zhang",
      "Xuejiao Lei",
      "Zhaoxiang Liu",
      "Ning Wang",
      "Zhenhong Long",
      "Peijun Yang",
      "Jiaojiao Zhao",
      "Minjie Hua",
      "Chaoyang Ma",
      "Kai Wang",
      "Shiguo Lian"
    ],
    "abstract": "Recently, the DeepSeek series of models, leveraging their exceptional\nreasoning capabilities and open-source strategy, is reshaping the global AI\nlandscape. Despite these advantages, they exhibit significant safety\ndeficiencies. Research conducted by Robust Intelligence, a subsidiary of Cisco,\nin collaboration with the University of Pennsylvania, revealed that DeepSeek-R1\nhas a 100\\% attack success rate when processing harmful prompts. Additionally,\nmultiple safety companies and research institutions have confirmed critical\nsafety vulnerabilities in this model. As models demonstrating robust\nperformance in Chinese and English, DeepSeek models require equally crucial\nsafety assessments in both language contexts. However, current research has\npredominantly focused on safety evaluations in English environments, leaving a\ngap in comprehensive assessments of their safety performance in Chinese\ncontexts. In response to this gap, this study introduces CHiSafetyBench, a\nChinese-specific safety evaluation benchmark. This benchmark systematically\nevaluates the safety of DeepSeek-R1 and DeepSeek-V3 in Chinese contexts,\nrevealing their performance across safety categories. The experimental results\nquantify the deficiencies of these two models in Chinese contexts, providing\nkey insights for subsequent improvements. It should be noted that, despite our\nefforts to establish a comprehensive, objective, and authoritative evaluation\nbenchmark, the selection of test samples, characteristics of data distribution,\nand the setting of evaluation criteria may inevitably introduce certain biases\ninto the evaluation results. We will continuously optimize the evaluation\nbenchmark and periodically update this report to provide more comprehensive and\naccurate assessment outcomes. Please refer to the latest version of the paper\nfor the most recent evaluation results and conclusions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages, 2 tables, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.11137v3",
    "published_date": "2025-02-16 14:05:54 UTC",
    "updated_date": "2025-05-08 01:35:18 UTC"
  },
  {
    "arxiv_id": "2502.11134v1",
    "title": "Solving Online Resource-Constrained Scheduling for Follow-Up Observation in Astronomy: a Reinforcement Learning Approach",
    "authors": [
      "Yajie Zhang",
      "Ce Yu",
      "Chao Sun",
      "Jizeng Wei",
      "Junhan Ju",
      "Shanjiang Tang"
    ],
    "abstract": "In the astronomical observation field, determining the allocation of\nobservation resources of the telescope array and planning follow-up\nobservations for targets of opportunity (ToOs) are indispensable components of\nastronomical scientific discovery. This problem is computationally challenging,\ngiven the online observation setting and the abundance of time-varying factors\nthat can affect whether an observation can be conducted. This paper presents\nROARS, a reinforcement learning approach for online astronomical\nresource-constrained scheduling. To capture the structure of the astronomical\nobservation scheduling, we depict every schedule using a directed acyclic graph\n(DAG), illustrating the dependency of timing between different observation\ntasks within the schedule. Deep reinforcement learning is used to learn a\npolicy that can improve the feasible solution by iteratively local rewriting\nuntil convergence. It can solve the challenge of obtaining a complete solution\ndirectly from scratch in astronomical observation scenarios, due to the high\ncomputational complexity resulting from numerous spatial and temporal\nconstraints. A simulation environment is developed based on real-world\nscenarios for experiments, to evaluate the effectiveness of our proposed\nscheduling approach. The experimental results show that ROARS surpasses 5\npopular heuristics, adapts to various observation scenarios and learns\neffective strategies with hindsight.",
    "categories": [
      "cs.AI",
      "astro-ph.IM"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11134v1",
    "published_date": "2025-02-16 14:01:12 UTC",
    "updated_date": "2025-02-16 14:01:12 UTC"
  },
  {
    "arxiv_id": "2502.11132v1",
    "title": "UNITE-FND: Reframing Multimodal Fake News Detection through Unimodal Scene Translation",
    "authors": [
      "Arka Mukherjee",
      "Shreya Ghosh"
    ],
    "abstract": "Multimodal fake news detection typically demands complex architectures and\nsubstantial computational resources, posing deployment challenges in real-world\nsettings. We introduce UNITE-FND, a novel framework that reframes multimodal\nfake news detection as a unimodal text classification task. We propose six\nspecialized prompting strategies with Gemini 1.5 Pro, converting visual content\ninto structured textual descriptions, and enabling efficient text-only models\nto preserve critical visual information. To benchmark our approach, we\nintroduce Uni-Fakeddit-55k, a curated dataset family of 55,000 samples each,\neach processed through our multimodal-to-unimodal translation framework.\nExperimental results demonstrate that UNITE-FND achieves 92.52% accuracy in\nbinary classification, surpassing prior multimodal models while reducing\ncomputational costs by over 10x (TinyBERT variant: 14.5M parameters vs. 250M+\nin SOTA models). Additionally, we propose a comprehensive suite of five novel\nmetrics to evaluate image-to-text conversion quality, ensuring optimal\ninformation preservation. Our results demonstrate that structured text-based\nrepresentations can replace direct multimodal processing with minimal loss of\naccuracy, making UNITE-FND a practical and scalable alternative for\nresource-constrained environments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "28 pages, 16 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.11132v1",
    "published_date": "2025-02-16 14:00:57 UTC",
    "updated_date": "2025-02-16 14:00:57 UTC"
  },
  {
    "arxiv_id": "2502.11124v1",
    "title": "AdaManip: Adaptive Articulated Object Manipulation Environments and Policy Learning",
    "authors": [
      "Yuanfei Wang",
      "Xiaojie Zhang",
      "Ruihai Wu",
      "Yu Li",
      "Yan Shen",
      "Mingdong Wu",
      "Zhaofeng He",
      "Yizhou Wang",
      "Hao Dong"
    ],
    "abstract": "Articulated object manipulation is a critical capability for robots to\nperform various tasks in real-world scenarios. Composed of multiple parts\nconnected by joints, articulated objects are endowed with diverse functional\nmechanisms through complex relative motions. For example, a safe consists of a\ndoor, a handle, and a lock, where the door can only be opened when the latch is\nunlocked. The internal structure, such as the state of a lock or joint angle\nconstraints, cannot be directly observed from visual observation. Consequently,\nsuccessful manipulation of these objects requires adaptive adjustment based on\ntrial and error rather than a one-time visual inference. However, previous\ndatasets and simulation environments for articulated objects have primarily\nfocused on simple manipulation mechanisms where the complete manipulation\nprocess can be inferred from the object's appearance. To enhance the diversity\nand complexity of adaptive manipulation mechanisms, we build a novel\narticulated object manipulation environment and equip it with 9 categories of\nobjects. Based on the environment and objects, we further propose an adaptive\ndemonstration collection and 3D visual diffusion-based imitation learning\npipeline that learns the adaptive manipulation policy. The effectiveness of our\ndesigns and proposed method is validated through both simulation and real-world\nexperiments. Our project page is available at: https://adamanip.github.io",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.11124v1",
    "published_date": "2025-02-16 13:45:10 UTC",
    "updated_date": "2025-02-16 13:45:10 UTC"
  },
  {
    "arxiv_id": "2502.11122v1",
    "title": "Hierarchical Expert Prompt for Large-Language-Model: An Approach Defeat Elite AI in TextStarCraft II for the First Time",
    "authors": [
      "Zongyuan Li",
      "Chang Lu",
      "Xiaojie Xu",
      "Runnan Qi",
      "Yanan Ni",
      "Lumin Jiang",
      "Xiangbei Liu",
      "Xuebo Zhang",
      "Yongchun Fang",
      "Kuihua Huang",
      "Xian Guo"
    ],
    "abstract": "Since the emergence of the Large Language Model (LLM), LLM has been widely\nused in fields such as writing, translating, and searching. However, there is\nstill great potential for LLM-based methods in handling complex tasks such as\ndecision-making in the StarCraft II environment. To address problems such as\nlack of relevant knowledge and poor control over subtasks of varying\nimportance, we propose a Hierarchical Expert Prompt (HEP) for LLM. Our method\nimproves the understanding of game situations through expert-level tactical\nknowledge, improving the processing quality of tasks of varying importance\nthrough a hierarchical framework. Our approach defeated the highest level\n(Elite) standard built-in agent in TextStarCraft II for the first time and\nconsistently outperformed the baseline method in other difficulties. Our\nexperiments suggest that the proposed method is a practical solution for\ntackling complex decision-making challenges. The replay video can be viewed on\nhttps://www.bilibili.com/video/BV1uz42187EF and https://youtu.be/dO3PshWLV5M,\nand our codes have been open-sourced on\nhttps://github.com/luchang1113/HEP-LLM-play-StarCraftII.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11122v1",
    "published_date": "2025-02-16 13:36:31 UTC",
    "updated_date": "2025-02-16 13:36:31 UTC"
  },
  {
    "arxiv_id": "2502.17475v3",
    "title": "ECG-Expert-QA: A Benchmark for Evaluating Medical Large Language Models in Heart Disease Diagnosis",
    "authors": [
      "Xu Wang",
      "Jiaju Kang",
      "Puyu Han",
      "Yubao Zhao",
      "Qian Liu",
      "Liwenfei He",
      "Lingqiong Zhang",
      "Lingyun Dai",
      "Yongcheng Wang",
      "Jie Tao"
    ],
    "abstract": "We present ECG-Expert-QA, a comprehensive multimodal dataset for evaluating\ndiagnostic capabilities in electrocardiogram (ECG) interpretation. It combines\nreal-world clinical ECG data with systematically generated synthetic cases,\ncovering 12 essential diagnostic tasks and totaling 47,211 expert-validated QA\npairs. These encompass diverse clinical scenarios, from basic rhythm\nrecognition to complex diagnoses involving rare conditions and temporal\nchanges. A key innovation is the support for multi-turn dialogues, enabling the\ndevelopment of conversational medical AI systems that emulate clinician-patient\nor interprofessional interactions. This allows for more realistic assessment of\nAI models' clinical reasoning, diagnostic accuracy, and knowledge integration.\nConstructed through a knowledge-guided framework with strict quality control,\nECG-Expert-QA ensures linguistic and clinical consistency, making it a\nhigh-quality resource for advancing AI-assisted ECG interpretation. It\nchallenges models with tasks like identifying subtle ischemic changes and\ninterpreting complex arrhythmias in context-rich scenarios. To promote research\ntransparency and collaboration, the dataset, accompanying code, and prompts are\npublicly released at https://github.com/Zaozzz/ECG-Expert-QA",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.17475v3",
    "published_date": "2025-02-16 13:28:55 UTC",
    "updated_date": "2025-04-07 09:59:44 UTC"
  },
  {
    "arxiv_id": "2502.11108v1",
    "title": "Knowledge Graph-Driven Retrieval-Augmented Generation: Integrating Deepseek-R1 with Weaviate for Advanced Chatbot Applications",
    "authors": [
      "Alexandru Lecu",
      "Adrian Groza",
      "Lezan Hawizy"
    ],
    "abstract": "Large language models (LLMs) have significantly advanced the field of natural\nlanguage generation. However, they frequently generate unverified outputs,\nwhich compromises their reliability in critical applications. In this study, we\npropose an innovative framework that combines structured biomedical knowledge\nwith LLMs through a retrieval-augmented generation technique. Our system\ndevelops a thorough knowledge graph by identifying and refining causal\nrelationships and named entities from medical abstracts related to age-related\nmacular degeneration (AMD). Using a vector-based retrieval process and a\nlocally deployed language model, our framework produces responses that are both\ncontextually relevant and verifiable, with direct references to clinical\nevidence. Experimental results show that this method notably decreases\nhallucinations, enhances factual precision, and improves the clarity of\ngenerated responses, providing a robust solution for advanced biomedical\nchatbot applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11108v1",
    "published_date": "2025-02-16 12:52:28 UTC",
    "updated_date": "2025-02-16 12:52:28 UTC"
  },
  {
    "arxiv_id": "2502.11107v2",
    "title": "Revisiting Weak-to-Strong Generalization in Theory and Practice: Reverse KL vs. Forward KL",
    "authors": [
      "Wei Yao",
      "Wenkai Yang",
      "Ziqiao Wang",
      "Yankai Lin",
      "Yong Liu"
    ],
    "abstract": "As large language models advance toward superhuman performance, ensuring\ntheir alignment with human values and abilities grows increasingly complex.\nWeak-to-strong generalization offers a promising approach by leveraging\npredictions from weaker models to guide stronger systems, but its effectiveness\ncould be constrained by the inherent noise and inaccuracies in these weak\npredictions. To address this, we propose a theoretically grounded approach that\nreplaces forward KL divergence-whose mass-covering behavior risks overfitting\nto imperfect weak signals-with reverse KL divergence. Reverse KL divergence's\nzero-forcing effect prioritizes high-confidence predictions, effectively\nmitigating the influence of unreliable weak supervision. Theoretically, we\nextend existing bounds and derive tighter lower bounds for both forward and\nreverse KL divergence, establishing that reverse KL achieves at least\ncomparable guarantees to forward KL. Notably, when a sufficiently pre-trained\nstrong model is fine-tuned on the last linear layer, reverse KL guarantees that\nit outperforms its weak supervisor by the magnitude of their disagreement.\nEmpirically, we demonstrate that reverse KL and reverse cross-entropy enable\nstrong models to successfully outperform those trained with forward KL and\nstandard cross-entropy across most settings, highlighting the practical\nadvantages of these reverse losses.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11107v2",
    "published_date": "2025-02-16 12:50:20 UTC",
    "updated_date": "2025-03-04 08:37:41 UTC"
  },
  {
    "arxiv_id": "2502.11102v2",
    "title": "OptMATH: A Scalable Bidirectional Data Synthesis Framework for Optimization Modeling",
    "authors": [
      "Hongliang Lu",
      "Zhonglin Xie",
      "Yaoyu Wu",
      "Can Ren",
      "Yuxuan Chen",
      "Zaiwen Wen"
    ],
    "abstract": "Despite the rapid development of large language models (LLMs), a fundamental\nchallenge persists: the lack of high-quality optimization modeling datasets\nhampers LLMs' robust modeling of practical optimization problems from natural\nlanguage descriptions (NL). This data scarcity also contributes to the\ngeneralization difficulties experienced by learning-based methods. To address\nthese challenges, we propose a scalable framework for synthesizing a\nhigh-quality dataset, named OptMATH. Starting from curated seed data with\nmathematical formulations (MF), this framework automatically generates problem\ndata (PD) with controllable complexity. Then, a back-translation step is\nemployed to obtain NL. To verify the correspondence between the NL and the PD,\na forward modeling step followed by rejection sampling is used. The accepted\npairs constitute the training part of OptMATH. Then a collection of rejected\npairs is identified and further filtered. This collection serves as a new\nbenchmark for optimization modeling, containing difficult instances whose\nlengths are much longer than these of NL4OPT and MAMO. Through extensive\nexperiments, we demonstrate that models of various sizes (0.5B-32B parameters)\ntrained on OptMATH achieve superior results on multiple modeling benchmarks,\nthereby validating the effectiveness and scalability of our approach. Our\ndataset is publicly available at https://github.com/AuroraLHL/OptMATH.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper has 36 pages, 18 figures, and two co-first authors:\n  Hongliang Lu and Zhonglin Xie",
    "pdf_url": "http://arxiv.org/pdf/2502.11102v2",
    "published_date": "2025-02-16 12:38:37 UTC",
    "updated_date": "2025-02-21 10:54:36 UTC"
  },
  {
    "arxiv_id": "2502.12204v1",
    "title": "Predicting Depression in Screening Interviews from Interactive Multi-Theme Collaboration",
    "authors": [
      "Xianbing Zhao",
      "Yiqing Lyu",
      "Di Wang",
      "Buzhou Tang"
    ],
    "abstract": "Automatic depression detection provides cues for early clinical intervention\nby clinicians. Clinical interviews for depression detection involve dialogues\ncentered around multiple themes. Existing studies primarily design end-to-end\nneural network models to capture the hierarchical structure of clinical\ninterview dialogues. However, these methods exhibit defects in modeling the\nthematic content of clinical interviews: 1) they fail to capture intra-theme\nand inter-theme correlation explicitly, and 2) they do not allow clinicians to\nintervene and focus on themes of interest. To address these issues, this paper\nintroduces an interactive depression detection framework. This framework\nleverages in-context learning techniques to identify themes in clinical\ninterviews and then models both intra-theme and inter-theme correlation.\nAdditionally, it employs AI-driven feedback to simulate the interests of\nclinicians, enabling interactive adjustment of theme importance. PDIMC achieves\nabsolute improvements of 35\\% and 12\\% compared to the state-of-the-art on the\ndepression detection dataset DAIC-WOZ, which demonstrates the effectiveness of\nmodeling theme correlation and incorporating interactive external feedback.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12204v1",
    "published_date": "2025-02-16 12:37:16 UTC",
    "updated_date": "2025-02-16 12:37:16 UTC"
  },
  {
    "arxiv_id": "2502.11101v1",
    "title": "CacheFocus: Dynamic Cache Re-Positioning for Efficient Retrieval-Augmented Generation",
    "authors": [
      "Kun-Hui Lee",
      "Eunhwan Park",
      "Donghoon Han",
      "Seung-Hoon Na"
    ],
    "abstract": "Large Language Models (LLMs) excel across a variety of language tasks yet are\nconstrained by limited input lengths and high computational costs. Existing\napproaches\\textemdash such as relative positional encodings (e.g., RoPE, ALiBi)\nand sliding window mechanisms\\textemdash partially alleviate these issues but\noften require additional training or suffer from performance degradation with\nlonger inputs. In this paper, we introduce \\textbf{\\textit{CacheFocus}}, a\nmethod that enhances length normalization and reduces inference latency without\nany further training. Our approach leverages query-independent, offline caching\nto efficiently reuse a Context KV Cache Store. We address the amplification of\nabnormal token distributions problem by re-positioning cached keys and\nintroducing Layer-Adaptive Cache Pruning to discard low-relevance caches during\npre-filling. Additionally, our Adaptive Positional Allocation Strategy\ndynamically reassigns cache positions to maximize the use of the available\npositional encoding range. Experiments on the Natural Questions and TriviaQA\ndatasets demonstrate that CacheFocus outperforms alternative methods even when\ninputs exceed the $4$K limit of the \\texttt{LLaMA-2} model, emphasizing its\npractical effectiveness for long-context LLMs. Moreover, even with large\nmaximum input length of \\texttt{Qwen2}, the performance of CacheFocus shows\nthat it maintains consistent performance even as the number of documents\nincreases, effectively managing long-text generation without degradation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages (Work in progress)",
    "pdf_url": "http://arxiv.org/pdf/2502.11101v1",
    "published_date": "2025-02-16 12:33:16 UTC",
    "updated_date": "2025-02-16 12:33:16 UTC"
  },
  {
    "arxiv_id": "2502.12203v1",
    "title": "An Interpretable Automated Mechanism Design Framework with Large Language Models",
    "authors": [
      "Jiayuan Liu",
      "Mingyu Guo",
      "Vincent Conitzer"
    ],
    "abstract": "Mechanism design has long been a cornerstone of economic theory, with\ntraditional approaches relying on mathematical derivations. Recently, automated\napproaches, including differentiable economics with neural networks, have\nemerged for designing payments and allocations. While both analytical and\nautomated methods have advanced the field, they each face significant\nweaknesses: mathematical derivations are not automated and often struggle to\nscale to complex problems, while automated and especially neural-network-based\napproaches suffer from limited interpretability. To address these challenges,\nwe introduce a novel framework that reformulates mechanism design as a code\ngeneration task. Using large language models (LLMs), we generate heuristic\nmechanisms described in code and evolve them to optimize over some evaluation\nmetrics while ensuring key design criteria (e.g., strategy-proofness) through a\nproblem-specific fixing process. This fixing process ensures any mechanism\nviolating the design criteria is adjusted to satisfy them, albeit with some\ntrade-offs in performance metrics. These trade-offs are factored in during the\nLLM-based evolution process. The code generation capabilities of LLMs enable\nthe discovery of novel and interpretable solutions, bridging the symbolic logic\nof mechanism design and the generative power of modern AI. Through rigorous\nexperimentation, we demonstrate that LLM-generated mechanisms achieve\ncompetitive performance while offering greater interpretability compared to\nprevious approaches. Notably, our framework can rediscover existing manually\ndesigned mechanisms and provide insights into neural-network based solutions\nthrough Programming-by-Example. These results highlight the potential of LLMs\nto not only automate but also enhance the transparency and scalability of\nmechanism design, ensuring safe deployment of the mechanisms in society.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.GT",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12203v1",
    "published_date": "2025-02-16 12:33:03 UTC",
    "updated_date": "2025-02-16 12:33:03 UTC"
  },
  {
    "arxiv_id": "2502.11098v1",
    "title": "Talk Structurally, Act Hierarchically: A Collaborative Framework for LLM Multi-Agent Systems",
    "authors": [
      "Zhao Wang",
      "Sota Moriyama",
      "Wei-Yao Wang",
      "Briti Gangopadhyay",
      "Shingo Takamatsu"
    ],
    "abstract": "Recent advancements in LLM-based multi-agent (LLM-MA) systems have shown\npromise, yet significant challenges remain in managing communication and\nrefinement when agents collaborate on complex tasks. In this paper, we propose\n\\textit{Talk Structurally, Act Hierarchically (TalkHier)}, a novel framework\nthat introduces a structured communication protocol for context-rich exchanges\nand a hierarchical refinement system to address issues such as incorrect\noutputs, falsehoods, and biases. \\textit{TalkHier} surpasses various types of\nSoTA, including inference scaling model (OpenAI-o1), open-source multi-agent\nmodels (e.g., AgentVerse), and majority voting strategies on current LLM and\nsingle-agent baselines (e.g., ReAct, GPT4o), across diverse tasks, including\nopen-domain question answering, domain-specific selective questioning, and\npractical advertisement text generation. These results highlight its potential\nto set a new standard for LLM-MA systems, paving the way for more effective,\nadaptable, and collaborative multi-agent frameworks. The code is available\nhttps://github.com/sony/talkhier.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11098v1",
    "published_date": "2025-02-16 12:26:58 UTC",
    "updated_date": "2025-02-16 12:26:58 UTC"
  },
  {
    "arxiv_id": "2502.11096v1",
    "title": "Mixture of Tunable Experts -- Behavior Modification of DeepSeek-R1 at Inference Time",
    "authors": [
      "Robert Dahlke",
      "Henrik Klagges",
      "Dan Zecha",
      "Benjamin Merkel",
      "Sven Rohr",
      "Fabian Klemm"
    ],
    "abstract": "We present the Mixture-of-Tunable-Experts (MoTE), a method that extends the\nMixture-of-Experts architecture of Large Language Models (LLMs). Without\nadditional training, MoTE enables meaningful and focused behavior changes in\nLLMs on-the-fly during inference time. By analyzing the digital LLM brain of\nDeepSeek-R1 using a technique we dub 'functional Token Resonance Imaging'\n(fTRI) -- inspired by fMRI and using prompts designed to elicit specific\nbehavior (e.g., 'What happened {time}{place}?') -- we empirically identify\ndistinctive experts associated with behaviors like refusal responses. Using\nMoTE we are able to intervene and control such specific behavior. We switched\noff the top 10 most refusal-relevant experts (0.07% of R1's 14,848 routed\nexperts), achieving a 52% refusal reduction on sensitive reference prompts\nwithout performance degradation on MT-Bench. Random expert deactivation\nresulted in smaller behavioral shifts with increased noise, whereas forced\nexpert activation led to significantly higher refusal rates. Our approach\nshares similarities with sparse autoencoders (SAEs) in terms of explainability\nand steerability. Unlike SAEs, MoTE does not require large training efforts, as\nwithin MoEs with a vast number of experts, specialization already emerged\nnaturally during pretraining. Our findings suggest that significant functional\nmechanisms in Mixture-of-Experts architectures can at least partially be\nlocalized in a small number of specific experts, rather than being distributed\nthroughout the model's weights. Expert subgroups can be tuned to trigger\nsignificant behavior variations, providing insights into the inner workings of\nLLMs.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11096v1",
    "published_date": "2025-02-16 12:24:39 UTC",
    "updated_date": "2025-02-16 12:24:39 UTC"
  },
  {
    "arxiv_id": "2502.11094v1",
    "title": "SyncSpeech: Low-Latency and Efficient Dual-Stream Text-to-Speech based on Temporal Masked Transformer",
    "authors": [
      "Zhengyan Sheng",
      "Zhihao Du",
      "Shiliang Zhang",
      "Zhijie Yan",
      "Yexin Yang",
      "Zhenhua Ling"
    ],
    "abstract": "This paper presents a dual-stream text-to-speech (TTS) model, SyncSpeech,\ncapable of receiving streaming text input from upstream models while\nsimultaneously generating streaming speech, facilitating seamless interaction\nwith large language models. SyncSpeech has the following advantages: Low\nlatency, as it begins generating streaming speech upon receiving the second\ntext token; High efficiency, as it decodes all speech tokens corresponding to\nthe each arrived text token in one step. To achieve this, we propose a temporal\nmasked transformer as the backbone of SyncSpeech, combined with token-level\nduration prediction to predict speech tokens and the duration for the next\nstep. Additionally, we design a two-stage training strategy to improve training\nefficiency and the quality of generated speech. We evaluated the SyncSpeech on\nboth English and Mandarin datasets. Compared to the recent dual-stream TTS\nmodels, SyncSpeech significantly reduces the first packet delay of speech\ntokens and accelerates the real-time factor. Moreover, with the same data\nscale, SyncSpeech achieves performance comparable to that of traditional\nautoregressive-based TTS models in terms of both speech quality and robustness.\nSpeech samples are available at\nhttps://SyncSpeech.github.io/}{https://SyncSpeech.github.io/.",
    "categories": [
      "cs.SD",
      "cs.AI"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11094v1",
    "published_date": "2025-02-16 12:14:17 UTC",
    "updated_date": "2025-02-16 12:14:17 UTC"
  },
  {
    "arxiv_id": "2502.11090v2",
    "title": "SafeDialBench: A Fine-Grained Safety Benchmark for Large Language Models in Multi-Turn Dialogues with Diverse Jailbreak Attacks",
    "authors": [
      "Hongye Cao",
      "Yanming Wang",
      "Sijia Jing",
      "Ziyue Peng",
      "Zhixin Bai",
      "Zhe Cao",
      "Meng Fang",
      "Fan Feng",
      "Boyan Wang",
      "Jiaheng Liu",
      "Tianpei Yang",
      "Jing Huo",
      "Yang Gao",
      "Fanyu Meng",
      "Xi Yang",
      "Chao Deng",
      "Junlan Feng"
    ],
    "abstract": "With the rapid advancement of Large Language Models (LLMs), the safety of\nLLMs has been a critical concern requiring precise assessment. Current\nbenchmarks primarily concentrate on single-turn dialogues or a single jailbreak\nattack method to assess the safety. Additionally, these benchmarks have not\ntaken into account the LLM's capability of identifying and handling unsafe\ninformation in detail. To address these issues, we propose a fine-grained\nbenchmark SafeDialBench for evaluating the safety of LLMs across various\njailbreak attacks in multi-turn dialogues. Specifically, we design a two-tier\nhierarchical safety taxonomy that considers 6 safety dimensions and generates\nmore than 4000 multi-turn dialogues in both Chinese and English under 22\ndialogue scenarios. We employ 7 jailbreak attack strategies, such as reference\nattack and purpose reverse, to enhance the dataset quality for dialogue\ngeneration. Notably, we construct an innovative assessment framework of LLMs,\nmeasuring capabilities in detecting, and handling unsafe information and\nmaintaining consistency when facing jailbreak attacks. Experimental results\nacross 17 LLMs reveal that Yi-34B-Chat and GLM4-9B-Chat demonstrate superior\nsafety performance, while Llama3.1-8B-Instruct and o3-mini exhibit safety\nvulnerabilities.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11090v2",
    "published_date": "2025-02-16 12:08:08 UTC",
    "updated_date": "2025-02-18 03:05:15 UTC"
  },
  {
    "arxiv_id": "2502.11089v2",
    "title": "Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention",
    "authors": [
      "Jingyang Yuan",
      "Huazuo Gao",
      "Damai Dai",
      "Junyu Luo",
      "Liang Zhao",
      "Zhengyan Zhang",
      "Zhenda Xie",
      "Y. X. Wei",
      "Lean Wang",
      "Zhiping Xiao",
      "Yuqing Wang",
      "Chong Ruan",
      "Ming Zhang",
      "Wenfeng Liang",
      "Wangding Zeng"
    ],
    "abstract": "Long-context modeling is crucial for next-generation language models, yet the\nhigh computational cost of standard attention mechanisms poses significant\ncomputational challenges. Sparse attention offers a promising direction for\nimproving efficiency while maintaining model capabilities. We present NSA, a\nNatively trainable Sparse Attention mechanism that integrates algorithmic\ninnovations with hardware-aligned optimizations to achieve efficient\nlong-context modeling. NSA employs a dynamic hierarchical sparse strategy,\ncombining coarse-grained token compression with fine-grained token selection to\npreserve both global context awareness and local precision. Our approach\nadvances sparse attention design with two key innovations: (1) We achieve\nsubstantial speedups through arithmetic intensity-balanced algorithm design,\nwith implementation optimizations for modern hardware. (2) We enable end-to-end\ntraining, reducing pretraining computation without sacrificing model\nperformance. As shown in Figure 1, experiments show the model pretrained with\nNSA maintains or exceeds Full Attention models across general benchmarks,\nlong-context tasks, and instruction-based reasoning. Meanwhile, NSA achieves\nsubstantial speedups over Full Attention on 64k-length sequences across\ndecoding, forward propagation, and backward propagation, validating its\nefficiency throughout the model lifecycle.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11089v2",
    "published_date": "2025-02-16 11:53:44 UTC",
    "updated_date": "2025-02-27 09:01:21 UTC"
  },
  {
    "arxiv_id": "2502.11085v1",
    "title": "Towards Data-Efficient Pretraining for Atomic Property Prediction",
    "authors": [
      "Yasir Ghunaim",
      "Hasan Abed Al Kader Hammoud",
      "Bernard Ghanem"
    ],
    "abstract": "This paper challenges the recent paradigm in atomic property prediction that\nlinks progress to growing dataset sizes and computational resources. We show\nthat pretraining on a carefully selected, task-relevant dataset can match or\neven surpass large-scale pretraining, while using as little as 1/24th of the\ncomputational cost. We introduce the Chemical Similarity Index (CSI), a novel\nmetric inspired by computer vision's Fr\\'echet Inception Distance, for\nmolecular graphs which quantifies the alignment between upstream pretraining\ndatasets and downstream tasks. By selecting the most relevant dataset with\nminimal CSI distance, we show that models pretrained on a smaller, focused\ndataset consistently outperform those pretrained on massive, mixed datasets\nsuch as JMP, even when those larger datasets include the relevant dataset.\nCounterintuitively, we also find that indiscriminately adding more data can\ndegrade model performance when the additional data poorly aligns with the task\nat hand. Our findings highlight that quality often outperforms quantity in\npretraining for atomic property prediction.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11085v1",
    "published_date": "2025-02-16 11:46:23 UTC",
    "updated_date": "2025-02-16 11:46:23 UTC"
  },
  {
    "arxiv_id": "2503.05715v1",
    "title": "The Butterfly Effect of Technology: How Various Factors accelerate or hinder the Arrival of Technological Singularity",
    "authors": [
      "Hooman Shababi"
    ],
    "abstract": "This article explores the concept of technological singularity and the\nfactors that could accelerate or hinder its arrival. The butterfly effect is\nused as a framework to understand how seemingly small changes in complex\nsystems can have significant and unpredictable outcomes. In section II, we\ndiscuss the various factors that could hasten the arrival of technological\nsingularity, such as advances in artificial intelligence and machine learning,\nbreakthroughs in quantum computing, progress in brain-computer interfaces and\nhuman augmentation, and development of nanotechnology and 3D printing. In\nsection III, we examine the factors that could delay or impede the arrival of\ntechnological singularity, including technical limitations and setbacks in AI\nand machine learning, ethical and societal concerns around AI and its impact on\njobs and privacy, lack of sufficient investment in research and development,\nand regulatory barriers and political instability. Section IV explores the\ninterplay of these factors and how they can impact the butterfly effect.\nFinally, in the conclusion, we summarize the key points discussed and emphasize\nthe importance of considering the butterfly effect in predicting the future of\ntechnology. We call for continued research and investment in technology to\nshape its future and mitigate potential risks.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "20 Pages, 0 Figures, 0 Tables",
    "pdf_url": "http://arxiv.org/pdf/2503.05715v1",
    "published_date": "2025-02-16 11:38:35 UTC",
    "updated_date": "2025-02-16 11:38:35 UTC"
  },
  {
    "arxiv_id": "2502.15771v1",
    "title": "Learning to Reason from Feedback at Test-Time",
    "authors": [
      "Yanyang Li",
      "Michael Lyu",
      "Liwei Wang"
    ],
    "abstract": "Solving complex tasks in a single attempt is challenging for large language\nmodels (LLMs). Iterative interaction with the environment and feedback is often\nrequired to achieve success, making effective feedback utilization a critical\ntopic. Existing approaches either struggle with length generalization or rely\non naive retries without leveraging prior information. In this paper, we\nintroduce FTTT, a novel paradigm that formulates feedback utilization as an\noptimization problem at test time. Additionally, we propose a learnable\ntest-time optimizer, OpTune, to effectively exploit feedback. Experiments on\ntwo LLMs across four reasoning datasets demonstrate that FTTT and OpTune\nachieve superior scalability and performance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "The code is at https://github.com/LaVi-Lab/FTTT",
    "pdf_url": "http://arxiv.org/pdf/2502.15771v1",
    "published_date": "2025-02-16 11:05:27 UTC",
    "updated_date": "2025-02-16 11:05:27 UTC"
  },
  {
    "arxiv_id": "2502.11079v2",
    "title": "Phantom: Subject-consistent video generation via cross-modal alignment",
    "authors": [
      "Lijie Liu",
      "Tianxiang Ma",
      "Bingchuan Li",
      "Zhuowei Chen",
      "Jiawei Liu",
      "Gen Li",
      "Siyu Zhou",
      "Qian He",
      "Xinglong Wu"
    ],
    "abstract": "The continuous development of foundational models for video generation is\nevolving into various applications, with subject-consistent video generation\nstill in the exploratory stage. We refer to this as Subject-to-Video, which\nextracts subject elements from reference images and generates\nsubject-consistent videos following textual instructions. We believe that the\nessence of subject-to-video lies in balancing the dual-modal prompts of text\nand image, thereby deeply and simultaneously aligning both text and visual\ncontent. To this end, we propose Phantom, a unified video generation framework\nfor both single- and multi-subject references. Building on existing\ntext-to-video and image-to-video architectures, we redesign the joint\ntext-image injection model and drive it to learn cross-modal alignment via\ntext-image-video triplet data. The proposed method achieves high-fidelity\nsubject-consistent video generation while addressing issues of image content\nleakage and multi-subject confusion. Evaluation results indicate that our\nmethod outperforms other state-of-the-art closed-source commercial solutions.\nIn particular, we emphasize subject consistency in human generation, covering\nexisting ID-preserving video generation while offering enhanced advantages.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11079v2",
    "published_date": "2025-02-16 11:02:50 UTC",
    "updated_date": "2025-04-10 10:24:37 UTC"
  },
  {
    "arxiv_id": "2502.11075v1",
    "title": "Exposing Numeracy Gaps: A Benchmark to Evaluate Fundamental Numerical Abilities in Large Language Models",
    "authors": [
      "Haoyang Li",
      "Xuejia Chen",
      "Zhanchao XU",
      "Darian Li",
      "Nicole Hu",
      "Fei Teng",
      "Yiming Li",
      "Luyu Qiu",
      "Chen Jason Zhang",
      "Qing Li",
      "Lei Chen"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nnatural language processing tasks, such as text generation and semantic\nunderstanding. However, their performance on numerical reasoning tasks, such as\nbasic arithmetic, numerical retrieval, and magnitude comparison, remains\nsurprisingly poor. This gap arises from their reliance on surface-level\nstatistical patterns rather than understanding numbers as continuous\nmagnitudes. Existing benchmarks primarily focus on either linguistic competence\nor structured mathematical problem-solving, neglecting fundamental numerical\nreasoning required in real-world scenarios. To bridge this gap, we propose\nNumericBench, a comprehensive benchmark to evaluate six fundamental numerical\ncapabilities: number recognition, arithmetic operations, contextual retrieval,\ncomparison, summary, and logical reasoning. NumericBench includes datasets\nranging from synthetic number lists to the crawled real-world data, addressing\nchallenges like long contexts, noise, and multi-step reasoning. Extensive\nexperiments on state-of-the-art LLMs, including GPT-4 and DeepSeek, reveal\npersistent weaknesses in numerical reasoning, highlighting the urgent need to\nimprove numerically-aware language modeling. The benchmark is released in:\nhttps://github.com/TreeAI-Lab/NumericBench.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11075v1",
    "published_date": "2025-02-16 10:48:28 UTC",
    "updated_date": "2025-02-16 10:48:28 UTC"
  },
  {
    "arxiv_id": "2502.12202v2",
    "title": "To Think or Not to Think: Exploring the Unthinking Vulnerability in Large Reasoning Models",
    "authors": [
      "Zihao Zhu",
      "Hongbao Zhang",
      "Ruotong Wang",
      "Ke Xu",
      "Siwei Lyu",
      "Baoyuan Wu"
    ],
    "abstract": "Large Reasoning Models (LRMs) are designed to solve complex tasks by\ngenerating explicit reasoning traces before producing final answers. However,\nwe reveal a critical vulnerability in LRMs -- termed Unthinking Vulnerability\n-- wherein the thinking process can be bypassed by manipulating special\ndelimiter tokens. It is empirically demonstrated to be widespread across\nmainstream LRMs, posing both a significant risk and potential utility,\ndepending on how it is exploited. In this paper, we systematically investigate\nthis vulnerability from both malicious and beneficial perspectives. On the\nmalicious side, we introduce Breaking of Thought (BoT), a novel attack that\nenables adversaries to bypass the thinking process of LRMs, thereby\ncompromising their reliability and availability. We present two variants of\nBoT: a training-based version that injects backdoor during the fine-tuning\nstage, and a training-free version based on adversarial attack during the\ninference stage. As a potential defense, we propose thinking recovery alignment\nto partially mitigate the vulnerability. On the beneficial side, we introduce\nMonitoring of Thought (MoT), a plug-and-play framework that allows model owners\nto enhance efficiency and safety. It is implemented by leveraging the same\nvulnerability to dynamically terminate redundant or risky reasoning through\nexternal monitoring. Extensive experiments show that BoT poses a significant\nthreat to reasoning reliability, while MoT provides a practical solution for\npreventing overthinking and jailbreaking. Our findings expose an inherent flaw\nin current LRM architectures and underscore the need for more robust reasoning\nsystems in the future.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "39 pages, 13 tables, 14 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.12202v2",
    "published_date": "2025-02-16 10:45:56 UTC",
    "updated_date": "2025-05-16 19:32:49 UTC"
  },
  {
    "arxiv_id": "2502.11070v1",
    "title": "A Survey on Vulnerability Prioritization: Taxonomy, Metrics, and Research Challenges",
    "authors": [
      "Yuning Jiang",
      "Nay Oo",
      "Qiaoran Meng",
      "Hoon Wei Lim",
      "Biplab Sikdar"
    ],
    "abstract": "In the highly interconnected digital landscape of today, safeguarding complex\ninfrastructures against cyber threats has become increasingly challenging due\nto the exponential growth in the number and complexity of vulnerabilities.\nResource constraints necessitate effective vulnerability prioritization\nstrategies, focusing efforts on the most critical risks. This paper presents a\nsystematic literature review of 82 studies, introducing a novel taxonomy that\ncategorizes metrics into severity, exploitability, contextual factors,\npredictive indicators, and aggregation methods. Our analysis reveals\nsignificant gaps in existing approaches and challenges with multi-domain\napplicability. By emphasizing the need for dynamic, context-aware metrics and\nscalable solutions, we provide actionable insights to bridge the gap between\nresearch and real-world applications. This work contributes to the field by\noffering a comprehensive framework for evaluating vulnerability prioritization\nmethodologies and setting a research agenda to advance the state of practice.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11070v1",
    "published_date": "2025-02-16 10:33:37 UTC",
    "updated_date": "2025-02-16 10:33:37 UTC"
  },
  {
    "arxiv_id": "2502.11068v1",
    "title": "Accelerating Anchors via Specialization and Feature Transformation",
    "authors": [
      "Haonan Yu",
      "Junhao Liu",
      "Xin Zhang"
    ],
    "abstract": "Anchors is a popular local model-agnostic explanation technique whose\napplicability is limited by its computational inefficiency. To address this\nlimitation, we propose a pre-training-based approach to accelerate Anchors\nwithout compromising the explanation quality. Our approach leverages the\niterative nature of Anchors' algorithm which gradually refines an explanation\nuntil it is precise enough for a given input by providing a general explanation\nthat is obtained through pre-training as Anchors' initial explanation.\nSpecifically, we develop a two-step rule transformation process: the horizontal\ntransformation adapts a pre-trained explanation to the current input by\nreplacing features, and the vertical transformation refines the general\nexplanation until it is precise enough for the input. We evaluate our method\nacross tabular, text, and image datasets, demonstrating that it significantly\nreduces explanation generation time while maintaining fidelity and\ninterpretability, thereby enabling the practical adoption of Anchors in\ntime-sensitive applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11068v1",
    "published_date": "2025-02-16 10:30:01 UTC",
    "updated_date": "2025-02-16 10:30:01 UTC"
  },
  {
    "arxiv_id": "2503.05882v1",
    "title": "Practical Topics in Optimization",
    "authors": [
      "Jun Lu"
    ],
    "abstract": "In an era where data-driven decision-making and computational efficiency are\nparamount, optimization plays a foundational role in advancing fields such as\nmathematics, computer science, operations research, machine learning, and\nbeyond. From refining machine learning models to improving resource allocation\nand designing efficient algorithms, optimization techniques serve as essential\ntools for tackling complex problems. This book aims to provide both an\nintroductory guide and a comprehensive reference, equipping readers with the\nnecessary knowledge to understand and apply optimization methods within their\nrespective fields.\n  Our primary goal is to demystify the inner workings of optimization\nalgorithms, including black-box and stochastic optimizers, by offering both\nformal and intuitive explanations. Starting from fundamental mathematical\nprinciples, we derive key results to ensure that readers not only learn how\nthese techniques work but also understand when and why to apply them\neffectively. By striking a careful balance between theoretical depth and\npractical application, this book serves a broad audience, from students and\nresearchers to practitioners seeking robust optimization strategies.",
    "categories": [
      "math.NA",
      "cs.AI",
      "cs.NA",
      "math.OC"
    ],
    "primary_category": "math.NA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05882v1",
    "published_date": "2025-02-16 10:00:50 UTC",
    "updated_date": "2025-02-16 10:00:50 UTC"
  },
  {
    "arxiv_id": "2502.11059v1",
    "title": "ClimateLLM: Efficient Weather Forecasting via Frequency-Aware Large Language Models",
    "authors": [
      "Shixuan Li",
      "Wei Yang",
      "Peiyu Zhang",
      "Xiongye Xiao",
      "Defu Cao",
      "Yuehan Qin",
      "Xiaole Zhang",
      "Yue Zhao",
      "Paul Bogdan"
    ],
    "abstract": "Weather forecasting is crucial for public safety, disaster prevention and\nmitigation, agricultural production, and energy management, with global\nrelevance. Although deep learning has significantly advanced weather\nprediction, current methods face critical limitations: (i) they often struggle\nto capture both dynamic temporal dependencies and short-term abrupt changes,\nmaking extreme weather modeling difficult; (ii) they incur high computational\ncosts due to extensive training and resource requirements; (iii) they have\nlimited adaptability to multi-scale frequencies, leading to challenges when\nseparating global trends from local fluctuations. To address these issues, we\npropose ClimateLLM, a foundation model for weather forecasting. It captures\nspatiotemporal dependencies via a cross-temporal and cross-spatial\ncollaborative modeling framework that integrates Fourier-based frequency\ndecomposition with Large Language Models (LLMs) to strengthen spatial and\ntemporal modeling. Our framework uses a Mixture-of-Experts (MoE) mechanism that\nadaptively processes different frequency components, enabling efficient\nhandling of both global signals and localized extreme events. In addition, we\nintroduce a cross-temporal and cross-spatial dynamic prompting mechanism,\nallowing LLMs to incorporate meteorological patterns across multiple scales\neffectively. Extensive experiments on real-world datasets show that ClimateLLM\noutperforms state-of-the-art approaches in accuracy and efficiency, as a\nscalable solution for global weather forecasting.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11059v1",
    "published_date": "2025-02-16 09:57:50 UTC",
    "updated_date": "2025-02-16 09:57:50 UTC"
  },
  {
    "arxiv_id": "2502.11057v2",
    "title": "A Physics-Informed Machine Learning Framework for Safe and Optimal Control of Autonomous Systems",
    "authors": [
      "Manan Tayal",
      "Aditya Singh",
      "Shishir Kolathaya",
      "Somil Bansal"
    ],
    "abstract": "As autonomous systems become more ubiquitous in daily life, ensuring high\nperformance with guaranteed safety is crucial. However, safety and performance\ncould be competing objectives, which makes their co-optimization difficult.\nLearning-based methods, such as Constrained Reinforcement Learning (CRL),\nachieve strong performance but lack formal safety guarantees due to safety\nbeing enforced as soft constraints, limiting their use in safety-critical\nsettings. Conversely, formal methods such as Hamilton-Jacobi (HJ) Reachability\nAnalysis and Control Barrier Functions (CBFs) provide rigorous safety\nassurances but often neglect performance, resulting in overly conservative\ncontrollers. To bridge this gap, we formulate the co-optimization of safety and\nperformance as a state-constrained optimal control problem, where performance\nobjectives are encoded via a cost function and safety requirements are imposed\nas state constraints. We demonstrate that the resultant value function\nsatisfies a Hamilton-Jacobi-Bellman (HJB) equation, which we approximate\nefficiently using a novel physics-informed machine learning framework. In\naddition, we introduce a conformal prediction-based verification strategy to\nquantify the learning errors, recovering a high-confidence safety value\nfunction, along with a probabilistic error bound on performance degradation.\nThrough several case studies, we demonstrate the efficacy of the proposed\nframework in enabling scalable learning of safe and performant controllers for\ncomplex, high-dimensional autonomous systems.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "15 Pages, 12 Figures. First two authors have contributed equally",
    "pdf_url": "http://arxiv.org/pdf/2502.11057v2",
    "published_date": "2025-02-16 09:46:17 UTC",
    "updated_date": "2025-02-24 12:56:55 UTC"
  },
  {
    "arxiv_id": "2502.11054v4",
    "title": "Reasoning-Augmented Conversation for Multi-Turn Jailbreak Attacks on Large Language Models",
    "authors": [
      "Zonghao Ying",
      "Deyue Zhang",
      "Zonglei Jing",
      "Yisong Xiao",
      "Quanchen Zou",
      "Aishan Liu",
      "Siyuan Liang",
      "Xiangzheng Zhang",
      "Xianglong Liu",
      "Dacheng Tao"
    ],
    "abstract": "Multi-turn jailbreak attacks simulate real-world human interactions by\nengaging large language models (LLMs) in iterative dialogues, exposing critical\nsafety vulnerabilities. However, existing methods often struggle to balance\nsemantic coherence with attack effectiveness, resulting in either benign\nsemantic drift or ineffective detection evasion. To address this challenge, we\npropose Reasoning-Augmented Conversation, a novel multi-turn jailbreak\nframework that reformulates harmful queries into benign reasoning tasks and\nleverages LLMs' strong reasoning capabilities to compromise safety alignment.\nSpecifically, we introduce an attack state machine framework to systematically\nmodel problem translation and iterative reasoning, ensuring coherent query\ngeneration across multiple turns. Building on this framework, we design\ngain-guided exploration, self-play, and rejection feedback modules to preserve\nattack semantics, enhance effectiveness, and sustain reasoning-driven attack\nprogression. Extensive experiments on multiple LLMs demonstrate that RACE\nachieves state-of-the-art attack effectiveness in complex conversational\nscenarios, with attack success rates (ASRs) increasing by up to 96%. Notably,\nour approach achieves ASRs of 82% and 92% against leading commercial models,\nOpenAI o1 and DeepSeek R1, underscoring its potency. We release our code at\nhttps://github.com/NY1024/RACE to facilitate further research in this critical\ndomain.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11054v4",
    "published_date": "2025-02-16 09:27:44 UTC",
    "updated_date": "2025-03-11 03:06:17 UTC"
  },
  {
    "arxiv_id": "2502.11051v3",
    "title": "MMUnlearner: Reformulating Multimodal Machine Unlearning in the Era of Multimodal Large Language Models",
    "authors": [
      "Jiahao Huo",
      "Yibo Yan",
      "Xu Zheng",
      "Yuanhuiyi Lyu",
      "Xin Zou",
      "Zhihua Wei",
      "Xuming Hu"
    ],
    "abstract": "Recent progress in Machine Unlearning (MU) has introduced solutions for the\nselective removal of private or sensitive information encoded within deep\nneural networks. Nonetheless, MU for Multimodal Large Language Models (MLLMs)\nremains in its nascent phase. Therefore, we propose to reformulate the task of\nmultimodal MU in the era of MLLMs, which aims to erase only the visual patterns\nassociated with a given entity while preserving the corresponding textual\nknowledge encoded within the original parameters of the language model\nbackbone. Furthermore, we develop a novel geometry-constrained gradient ascent\nmethod MMUnlearner. It updates the weights of MLLMs with a weight saliency map\njointly restricted by the remaining concepts and textual knowledge during\nunlearning, thereby preserving parameters essential for non-target knowledge.\nExtensive experiments demonstrate that MMUnlearner surpasses baselines that\nfinetuning MLLMs with VQA data directly through Gradient Ascent (GA) or\nNegative Preference Optimization (NPO), across all evaluation dimensions. Our\ncode will be released upon acceptance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted as ACL 2025 Findings",
    "pdf_url": "http://arxiv.org/pdf/2502.11051v3",
    "published_date": "2025-02-16 09:23:50 UTC",
    "updated_date": "2025-05-20 15:47:22 UTC"
  },
  {
    "arxiv_id": "2504.05309v1",
    "title": "IterQR: An Iterative Framework for LLM-based Query Rewrite in e-Commercial Search System",
    "authors": [
      "Shangyu Chen",
      "Xinyu Jia",
      "Yingfei Zhang",
      "Shuai Zhang",
      "Xiang Li",
      "Wei Lin"
    ],
    "abstract": "The essence of modern e-Commercial search system lies in matching user's\nintent and available candidates depending on user's query, providing\npersonalized and precise service. However, user's query may be incorrect due to\nambiguous input and typo, leading to inaccurate search. These cases may be\nreleased by query rewrite: modify query to other representation or expansion.\nHowever, traditional query rewrite replies on static rewrite vocabulary, which\nis manually established meanwhile lacks interaction with both domain knowledge\nin e-Commercial system and common knowledge in the real world. In this paper,\nwith the ability to generate text content of Large Language Models (LLMs), we\nprovide an iterative framework to generate query rewrite. The framework\nincorporates a 3-stage procedure in each iteration: Rewrite Generation with\ndomain knowledge by Retrieval-Augmented Generation (RAG) and query\nunderstanding by Chain-of-Thoughts (CoT); Online Signal Collection with\nautomatic positive rewrite update; Post-training of LLM with multi task\nobjective to generate new rewrites. Our work (named as IterQR) provides a\ncomprehensive framework to generate \\textbf{Q}uery \\textbf{R}ewrite with both\ndomain / real-world knowledge. It automatically update and self-correct the\nrewrites during \\textbf{iter}ations. \\method{} has been deployed in Meituan\nDelivery's search system (China's leading food delivery platform), providing\nservice for users with significant improvement.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.05309v1",
    "published_date": "2025-02-16 09:20:13 UTC",
    "updated_date": "2025-02-16 09:20:13 UTC"
  },
  {
    "arxiv_id": "2502.15770v2",
    "title": "Performance Review on LLM for solving leetcode problems",
    "authors": [
      "Lun Wang",
      "Chuanqi Shi",
      "Shaoshui Du",
      "Yiyi Tao",
      "Yixian Shen",
      "Hang Zheng",
      "Yanxin Shen",
      "Xinyu Qiu"
    ],
    "abstract": "This paper presents a comprehensive performance evaluation of Large Language\nModels (LLMs) in solving programming challenges from Leetcode, a widely used\nplatform for algorithm practice and technical interviews. We began by crawling\nthe Leetcode website to collect a diverse set of problems encompassing various\ndifficulty levels and topics. Using this dataset, we generated solutions with\nmultiple LLMs, including GPT-4 and GPT-3.5-turbo (ChatGPT-turbo). The generated\nsolutions were systematically evaluated for correctness and efficiency. We\nemployed the pass@k metric to assess the success rates within a given number of\nattempts and analyzed the runtime performance of the solutions. Our results\nhighlight the strengths and limitations of current LLMs [10] in code generation\nand problem-solving tasks, providing insights into their potential applications\nand areas for improvement in automated programming assistance.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.15770v2",
    "published_date": "2025-02-16 08:52:45 UTC",
    "updated_date": "2025-03-03 00:24:08 UTC"
  },
  {
    "arxiv_id": "2502.11037v2",
    "title": "Deep Incomplete Multi-view Learning via Cyclic Permutation of VAEs",
    "authors": [
      "Xin Gao",
      "Jian Pu"
    ],
    "abstract": "Multi-View Representation Learning (MVRL) aims to derive a unified\nrepresentation from multi-view data by leveraging shared and complementary\ninformation across views. However, when views are irregularly missing, the\nincomplete data can lead to representations that lack sufficiency and\nconsistency. To address this, we propose Multi-View Permutation of Variational\nAuto-Encoders (MVP), which excavates invariant relationships between views in\nincomplete data. MVP establishes inter-view correspondences in the latent space\nof Variational Auto-Encoders, enabling the inference of missing views and the\naggregation of more sufficient information. To derive a valid Evidence Lower\nBound (ELBO) for learning, we apply permutations to randomly reorder variables\nfor cross-view generation and then partition them by views to maintain\ninvariant meanings under permutations. Additionally, we enhance consistency by\nintroducing an informational prior with cyclic permutations of posteriors,\nwhich turns the regularization term into a similarity measure across\ndistributions. We demonstrate the effectiveness of our approach on seven\ndiverse datasets with varying missing ratios, achieving superior performance in\nmulti-view clustering and generation tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 4 figures, ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.11037v2",
    "published_date": "2025-02-16 08:36:43 UTC",
    "updated_date": "2025-02-28 06:04:20 UTC"
  },
  {
    "arxiv_id": "2502.11028v1",
    "title": "Mind the Confidence Gap: Overconfidence, Calibration, and Distractor Effects in Large Language Models",
    "authors": [
      "Prateek Chhikara"
    ],
    "abstract": "Large Language Models (LLMs) demonstrate impressive performance across\ndiverse tasks, yet confidence calibration remains a challenge. Miscalibration -\nwhere models are overconfident or underconfident - poses risks, particularly in\nhigh-stakes applications. This paper presents an empirical study on LLM\ncalibration, examining how model size, distractors, and question types affect\nconfidence alignment. We introduce an evaluation framework to measure\noverconfidence and investigate whether multiple-choice formats mitigate or\nworsen miscalibration. Our findings show that while larger models (e.g.,\nGPT-4o) are better calibrated overall, they are more prone to distraction,\nwhereas smaller models benefit more from answer choices but struggle with\nuncertainty estimation. Unlike prior work, which primarily reports\nmiscalibration trends, we provide actionable insights into failure modes and\nconditions that worsen overconfidence. These findings highlight the need for\ncalibration-aware interventions and improved uncertainty estimation methods.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11028v1",
    "published_date": "2025-02-16 07:46:09 UTC",
    "updated_date": "2025-02-16 07:46:09 UTC"
  },
  {
    "arxiv_id": "2502.11026v2",
    "title": "Simplify RLHF as Reward-Weighted SFT: A Variational Method",
    "authors": [
      "Yuhao Du",
      "Zhuo Li",
      "Pengyu Cheng",
      "Zhihong Chen",
      "Yuejiao Xie",
      "Xiang Wan",
      "Anningzhe Gao"
    ],
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning\nLarge Language Models (LLMs) with human values. However, RLHF has been\ncontinuously challenged by its high complexity in implementation and\ncomputation consumption. Even with recent simplifications, such as Direct\nPreference Optimization (DPO) and Advantage Leftover Lunch (A-LoL), the\nproblems of over-fitting and training instability remain hindering the\nalignment process from the expected optimal performance. To address the\nexisting challenges, we propose a novel simplification of RLHF from the\nperspective of variational inference, called $\\textbf{V}$ariational\n$\\textbf{A}$lignment with $\\textbf{R}$e-weighting ($\\textbf{VAR}$). More\nspecifically, by directly minimizing the distribution gap between the learning\nLLM policy and the optimal solution of RLHF, we transform the alignment\nobjective into a reward-driven re-weighted supervised fine-tuning (SFT) form,\nwhich only requires minor adjustment on the SFT loss to obtain noticeable\nimprovement on training stability and effectiveness. On comprehensive alignment\nand generation benchmarks, our VAR method has numerically achieved competitive\nperformance in LLM alignment helpfulness and harmlessness.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11026v2",
    "published_date": "2025-02-16 07:22:00 UTC",
    "updated_date": "2025-02-19 02:44:40 UTC"
  },
  {
    "arxiv_id": "2502.11022v1",
    "title": "MultiTEND: A Multilingual Benchmark for Natural Language to NoSQL Query Translation",
    "authors": [
      "Zhiqian Qin",
      "Yuanfeng Song",
      "Jinwei Lu",
      "Yuanwei Song",
      "Shuaimin Li",
      "Chen Jason Zhang"
    ],
    "abstract": "Natural language interfaces for NoSQL databases are increasingly vital in the\nbig data era, enabling users to interact with complex, unstructured data\nwithout deep technical expertise. However, most recent advancements focus on\nEnglish, leaving a gap for multilingual support. This paper introduces\nMultiTEND, the first and largest multilingual benchmark for natural language to\nNoSQL query generation, covering six languages: English, German, French,\nRussian, Japanese and Mandarin Chinese. Using MultiTEND, we analyze challenges\nin translating natural language to NoSQL queries across diverse linguistic\nstructures, including lexical and syntactic differences. Experiments show that\nperformance accuracy in both English and non-English settings remains\nrelatively low, with a 4%-6% gap across scenarios like fine-tuned SLM,\nzero-shot LLM, and RAG for LLM. To address the aforementioned challenges, we\nintroduce MultiLink, a novel framework that bridges the multilingual input to\nNoSQL query generation gap through a Parallel Linking Process. It breaks down\nthe task into multiple steps, integrating parallel multilingual processing,\nChain-of-Thought (CoT) reasoning, and Retrieval-Augmented Generation (RAG) to\ntackle lexical and structural challenges inherent in multilingual NoSQL\ngeneration. MultiLink shows enhancements in all metrics for every language\nagainst the top baseline, boosting execution accuracy by about 15% for English\nand averaging a 10% improvement for non-English languages.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11022v1",
    "published_date": "2025-02-16 07:12:47 UTC",
    "updated_date": "2025-02-16 07:12:47 UTC"
  },
  {
    "arxiv_id": "2502.11020v1",
    "title": "TUMLU: A Unified and Native Language Understanding Benchmark for Turkic Languages",
    "authors": [
      "Jafar Isbarov",
      "Arofat Akhundjanova",
      "Mammad Hajili",
      "Kavsar Huseynova",
      "Dmitry Gaynullin",
      "Anar Rzayev",
      "Osman Tursun",
      "Ilshat Saetov",
      "Rinat Kharisov",
      "Saule Belginova",
      "Ariana Kenbayeva",
      "Amina Alisheva",
      "Aizirek Turdubaeva",
      "Abdullatif Köksal",
      "Samir Rustamov",
      "Duygu Ataman"
    ],
    "abstract": "Being able to thoroughly assess massive multi-task language understanding\n(MMLU) capabilities is essential for advancing the applicability of\nmultilingual language models. However, preparing such benchmarks in high\nquality native language is often costly and therefore limits the\nrepresentativeness of evaluation datasets. While recent efforts focused on\nbuilding more inclusive MMLU benchmarks, these are conventionally built using\nmachine translation from high-resource languages, which may introduce errors\nand fail to account for the linguistic and cultural intricacies of the target\nlanguages. In this paper, we address the lack of native language MMLU benchmark\nespecially in the under-represented Turkic language family with distinct\nmorphosyntactic and cultural characteristics. We propose two benchmarks for\nTurkic language MMLU: TUMLU is a comprehensive, multilingual, and natively\ndeveloped language understanding benchmark specifically designed for Turkic\nlanguages. It consists of middle- and high-school level questions spanning 11\nacademic subjects in Azerbaijani, Crimean Tatar, Karakalpak, Kazakh, Tatar,\nTurkish, Uyghur, and Uzbek. We also present TUMLU-mini, a more concise,\nbalanced, and manually verified subset of the dataset. Using this dataset, we\nsystematically evaluate a diverse range of open and proprietary multilingual\nlarge language models (LLMs), including Claude, Gemini, GPT, and LLaMA,\noffering an in-depth analysis of their performance across different languages,\nsubjects, and alphabets. To promote further research and development in\nmultilingual language understanding, we release TUMLU-mini and all\ncorresponding evaluation scripts.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11020v1",
    "published_date": "2025-02-16 07:07:38 UTC",
    "updated_date": "2025-02-16 07:07:38 UTC"
  },
  {
    "arxiv_id": "2502.11019v2",
    "title": "Unlocking the Power of Function Vectors for Characterizing and Mitigating Catastrophic Forgetting in Continual Instruction Tuning",
    "authors": [
      "Gangwei Jiang",
      "Caigao Jiang",
      "Zhaoyi Li",
      "Siqiao Xue",
      "Jun Zhou",
      "Linqi Song",
      "Defu Lian",
      "Ying Wei"
    ],
    "abstract": "Catastrophic forgetting (CF) poses a significant challenge in machine\nlearning, where a model forgets previously learned information upon learning\nnew tasks. Despite the advanced capabilities of Large Language Models (LLMs),\nthey continue to face challenges with CF during continual learning. The\nmajority of existing research focuses on analyzing forgetting patterns through\na singular training sequence, thereby overlooking the intricate effects that\ndiverse tasks have on model behavior. Our study explores CF across various\nsettings, discovering that model forgetting is influenced by both the specific\ntraining tasks and the models themselves. To this end, we interpret forgetting\nby examining the function vector (FV), a compact representation of functions in\nLLMs, offering a model-dependent indicator for the occurrence of CF. Through\ntheoretical and empirical analyses, we demonstrated that CF in LLMs primarily\nstems from biases in function activation rather than the overwriting of task\nprocessing functions. Leveraging these insights, we propose a novel function\nvector guided training methodology, incorporating a regularization technique to\nstabilize the FV and mitigate forgetting. Empirical tests on four benchmarks\nconfirm the effectiveness of our proposed training method, substantiating our\ntheoretical framework concerning CF and model function dynamics. We plan to\nmake our code publicly accessible in the near future.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10pages",
    "pdf_url": "http://arxiv.org/pdf/2502.11019v2",
    "published_date": "2025-02-16 07:06:17 UTC",
    "updated_date": "2025-04-16 03:22:41 UTC"
  },
  {
    "arxiv_id": "2502.11018v1",
    "title": "GRIFFIN: Effective Token Alignment for Faster Speculative Decoding",
    "authors": [
      "Shijing Hu",
      "Jingyang Li",
      "Xingyu Xie",
      "Zhihui Lu",
      "Kim-Chuan Toh",
      "Pan Zhou"
    ],
    "abstract": "Speculative decoding accelerates inference in large language models (LLMs) by\ngenerating multiple draft tokens simultaneously. However, existing methods\noften struggle with token misalignment between the training and decoding\nphases, limiting their performance. To address this, we propose GRIFFIN, a\nnovel framework that incorporates a token-alignable training strategy and a\ntoken-alignable draft model to mitigate misalignment. The training strategy\nemploys a loss masking mechanism to exclude highly misaligned tokens during\ntraining, preventing them from negatively impacting the draft model's\noptimization. The token-alignable draft model introduces input tokens to\ncorrect inconsistencies in generated features. Experiments on LLaMA-series and\nVicuna models demonstrate that GRIFFIN achieves an average acceptance length\nimprovement of over 7\\% and a speedup ratio exceeding 8%, outperforming current\nSoTAs as shown in Fig. 1 (a) and (b).",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11018v1",
    "published_date": "2025-02-16 07:06:00 UTC",
    "updated_date": "2025-02-16 07:06:00 UTC"
  },
  {
    "arxiv_id": "2502.11013v4",
    "title": "Collaborative Deterministic-Probabilistic Forecasting for Real-World Spatiotemporal Systems",
    "authors": [
      "Zhi Sheng",
      "Yuan Yuan",
      "Yudi Zhang",
      "Depeng Jin",
      "Yong Li"
    ],
    "abstract": "Probabilistic forecasting is crucial for real-world spatiotemporal systems,\nsuch as climate, energy, and urban environments, where quantifying uncertainty\nis essential for informed, risk-aware decision-making. While diffusion models\nhave shown promise in capturing complex data distributions, their application\nto spatiotemporal forecasting remains limited due to complex spatiotemporal\ndynamics and high computational demands. In this work, we propose CoST, a novel\nframework that collaborates deterministic and diffusion models for\nspatiotemporal forecasting. CoST formulates a mean-residual decomposition\nstrategy: it leverages a powerful deterministic model to capture the\nconditional mean and a lightweight diffusion model to learn residual\nuncertainties. This collaborative formulation simplifies learning objectives,\nenhances forecasting accuracy, enables uncertainty quantification, and\nsignificantly improves computational efficiency. To address spatial\nheterogeneity, we further design a scale-aware diffusion mechanism to guide the\ndiffusion process. Extensive experiments across ten real-world datasets from\nclimate, energy, communication, and urban systems show that CoST achieves 25%\nperformance gains over state-of-the-art baselines, while significantly reducing\ncomputational cost.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.11013v4",
    "published_date": "2025-02-16 06:35:26 UTC",
    "updated_date": "2025-05-17 16:24:32 UTC"
  },
  {
    "arxiv_id": "2503.04772v1",
    "title": "Generating Millions Of Lean Theorems With Proofs By Exploring State Transition Graphs",
    "authors": [
      "David Yin",
      "Jing Gao"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated significant potential in\ngenerating mathematical proofs. However, a persistent challenge is that LLMs\noccasionally make mistakes, while even a minor mistake can invalidate an entire\nproof. Proof assistants like Lean offer a great remedy. They are designed for\nverifying each step of a proof in a formal language, and in recent years\nresearchers have created AI models to generate proofs in their languages.\nHowever, the scarcity of large-scale datasets of Lean proofs restrict the\nperformance of such Automated Theorem Proving (ATP) models.\n  We developed LeanNavigator, a novel method for generating a large-scale\ndataset of Lean theorems and proofs by finding new ways to prove existing Lean\ntheorems. By leveraging an interactive Lean client and an efficient method for\nproof step generation, LeanNavigator efficiently produces new theorems with\ncorresponding proofs. Applying this approach to Mathlib4, we generated 4.7\nmillion theorems totaling 1 billion tokens, surpassing previous datasets by\nmore than an order of magnitude. Using this extensive dataset, we trained an AI\nmodel that outperforms the state-of-the-art ReProver model in theorem-proving\ntasks. These results confirm our hypothesis and demonstrate the critical role\nof large datasets in improving the performance of automated theorem provers.",
    "categories": [
      "cs.LO",
      "cs.AI"
    ],
    "primary_category": "cs.LO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.04772v1",
    "published_date": "2025-02-16 06:20:39 UTC",
    "updated_date": "2025-02-16 06:20:39 UTC"
  },
  {
    "arxiv_id": "2502.11006v1",
    "title": "Prompt Inject Detection with Generative Explanation as an Investigative Tool",
    "authors": [
      "Jonathan Pan",
      "Swee Liang Wong",
      "Yidi Yuan",
      "Xin Wei Chia"
    ],
    "abstract": "Large Language Models (LLMs) are vulnerable to adversarial prompt based\ninjects. These injects could jailbreak or exploit vulnerabilities within these\nmodels with explicit prompt requests leading to undesired responses. In the\ncontext of investigating prompt injects, the challenge is the sheer volume of\ninput prompts involved that are likely to be largely benign. This investigative\nchallenge is further complicated by the semantics and subjectivity of the input\nprompts involved in the LLM conversation with its user and the context of the\nenvironment to which the conversation is being carried out. Hence, the\nchallenge for AI security investigators would be two-fold. The first is to\nidentify adversarial prompt injects and then to assess whether the input prompt\nis contextually benign or adversarial. For the first step, this could be done\nusing existing AI security solutions like guardrails to detect and protect the\nLLMs. Guardrails have been developed using a variety of approaches. A popular\napproach is to use signature based. Another popular approach to develop AI\nmodels to classify such prompts include the use of NLP based models like a\nlanguage model. However, in the context of conducting an AI security\ninvestigation of prompt injects, these guardrails lack the ability to aid\ninvestigators in triaging or assessing the identified input prompts. In this\napplied research exploration, we explore the use of a text generation\ncapabilities of LLM to detect prompt injects and generate explanation for its\ndetections to aid AI security investigators in assessing and triaging of such\nprompt inject detections. The practical benefit of such a tool is to ease the\ntask of conducting investigation into prompt injects.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "5 pages, 4 tables, 3 diagrams",
    "pdf_url": "http://arxiv.org/pdf/2502.11006v1",
    "published_date": "2025-02-16 06:16:00 UTC",
    "updated_date": "2025-02-16 06:16:00 UTC"
  },
  {
    "arxiv_id": "2502.12200v1",
    "title": "Efficient and Effective Prompt Tuning via Prompt Decomposition and Compressed Outer Product",
    "authors": [
      "Pengxiang Lan",
      "Haoyu Xu",
      "Enneng Yang",
      "Yuliang Liang",
      "Guibing Guo",
      "Jianzhe Zhao",
      "Xingwei Wang"
    ],
    "abstract": "Prompt tuning (PT) offers a cost-effective alternative to fine-tuning\nlarge-scale pre-trained language models (PLMs), requiring only a few parameters\nin soft prompt tokens added before the input text. However, existing PT\napproaches face two significant issues: (i) They overlook intrinsic semantic\nassociations between soft prompt tokens, leading to high discreteness and\nlimited interactions, thus reducing the model's comprehension and effectiveness\nin complex tasks. (ii) Due to the complexity of downstream tasks, long soft\nprompt is necessitated to improve performance, but prompt length correlates\npositively with memory usage and computational costs. Achieving high efficiency\nand performance remains an ongoing challenge. To address these issues, we\npropose a novel Low-parameters prompt tuning (LAMP) method, which leverages\nprompt decomposition and compressed outer product. Specifically, the prompt\ndecomposition module employs Truncated SVD to reduce training parameters and\nsignificantly lower the dimensionality of the soft prompt parameter space. It\nthen utilizes a compressed outer product module to facilitate multiple\ninteractions among prompt tokens, exploring their intrinsic associations to\nenhance knowledge representation. Finally, LAMP uses average pooling to reduce\nmemory usage and training/inference time. Extensive experiments across six\narchitectures and eight datasets demonstrate that LAMP outperforms\nstate-of-the-art PT-based and LoRA-based methods in performance and efficiency.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12200v1",
    "published_date": "2025-02-16 05:50:12 UTC",
    "updated_date": "2025-02-16 05:50:12 UTC"
  },
  {
    "arxiv_id": "2502.11001v1",
    "title": "CL-MFAP: A Contrastive Learning-Based Multimodal Foundation Model for Molecular Property Prediction and Antibiotic Screening",
    "authors": [
      "Gen Zhou",
      "Sugitha Janarthanan",
      "Yutong Lu",
      "Pingzhao Hu"
    ],
    "abstract": "Due to the rise in antimicrobial resistance, identifying novel compounds with\nantibiotic potential is crucial for combatting this global health issue.\nHowever, traditional drug development methods are costly and inefficient.\nRecognizing the pressing need for more effective solutions, researchers have\nturned to machine learning techniques to streamline the prediction and\ndevelopment of novel antibiotic compounds. While foundation models have shown\npromise in antibiotic discovery, current mainstream efforts still fall short of\nfully leveraging the potential of multimodal molecular data. Recent studies\nsuggest that contrastive learning frameworks utilizing multimodal data exhibit\nexcellent performance in representation learning across various domains.\nBuilding upon this, we introduce CL-MFAP, an unsupervised contrastive learning\n(CL)-based multimodal foundation (MF) model specifically tailored for\ndiscovering small molecules with potential antibiotic properties (AP) using\nthree types of molecular data. This model employs 1.6 million bioactive\nmolecules with drug-like properties from the ChEMBL dataset to jointly pretrain\nthree encoders: (1) a transformer-based encoder with rotary position embedding\nfor processing SMILES strings; (2) another transformer-based encoder,\nincorporating a novel bi-level routing attention mechanism to handle molecular\ngraph representations; and (3) a Morgan fingerprint encoder using a multilayer\nperceptron, to achieve the contrastive learning purpose. The CL-MFAP\noutperforms baseline models in antibiotic property prediction by effectively\nutilizing different molecular modalities and demonstrates superior\ndomain-specific performance when fine-tuned for antibiotic-related property\nprediction tasks.",
    "categories": [
      "q-bio.BM",
      "cs.AI",
      "cs.LG",
      "q-bio.QM"
    ],
    "primary_category": "q-bio.BM",
    "comment": "Gen Zhou and Sugitha Janarthanan contributed equally; Accepted at\n  ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.11001v1",
    "published_date": "2025-02-16 05:45:19 UTC",
    "updated_date": "2025-02-16 05:45:19 UTC"
  },
  {
    "arxiv_id": "2502.10999v1",
    "title": "ControlText: Unlocking Controllable Fonts in Multilingual Text Rendering without Font Annotations",
    "authors": [
      "Bowen Jiang",
      "Yuan Yuan",
      "Xinyi Bai",
      "Zhuoqun Hao",
      "Alyson Yin",
      "Yaojie Hu",
      "Wenyu Liao",
      "Lyle Ungar",
      "Camillo J. Taylor"
    ],
    "abstract": "This work demonstrates that diffusion models can achieve font-controllable\nmultilingual text rendering using just raw images without font label\nannotations. Visual text rendering remains a significant challenge. While\nrecent methods condition diffusion on glyphs, it is impossible to retrieve\nexact font annotations from large-scale, real-world datasets, which prevents\nuser-specified font control. To address this, we propose a data-driven solution\nthat integrates the conditional diffusion model with a text segmentation model,\nutilizing segmentation masks to capture and represent fonts in pixel space in a\nself-supervised manner, thereby eliminating the need for any ground-truth\nlabels and enabling users to customize text rendering with any multilingual\nfont of their choice. The experiment provides a proof of concept of our\nalgorithm in zero-shot text and font editing across diverse fonts and\nlanguages, providing valuable insights for the community and industry toward\nachieving generalized visual text rendering.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "This is preliminary work and code will be released at\n  github.com/bowen-upenn/ControlText",
    "pdf_url": "http://arxiv.org/pdf/2502.10999v1",
    "published_date": "2025-02-16 05:30:18 UTC",
    "updated_date": "2025-02-16 05:30:18 UTC"
  },
  {
    "arxiv_id": "2502.10985v1",
    "title": "Is Elo Rating Reliable? A Study Under Model Misspecification",
    "authors": [
      "Shange Tang",
      "Yuanhao Wang",
      "Chi Jin"
    ],
    "abstract": "Elo rating, widely used for skill assessment across diverse domains ranging\nfrom competitive games to large language models, is often understood as an\nincremental update algorithm for estimating a stationary Bradley-Terry (BT)\nmodel. However, our empirical analysis of practical matching datasets reveals\ntwo surprising findings: (1) Most games deviate significantly from the\nassumptions of the BT model and stationarity, raising questions on the\nreliability of Elo. (2) Despite these deviations, Elo frequently outperforms\nmore complex rating systems, such as mElo and pairwise models, which are\nspecifically designed to account for non-BT components in the data,\nparticularly in terms of win rate prediction. This paper explains this\nunexpected phenomenon through three key perspectives: (a) We reinterpret Elo as\nan instance of online gradient descent, which provides no-regret guarantees\neven in misspecified and non-stationary settings. (b) Through extensive\nsynthetic experiments on data generated from transitive but non-BT models, such\nas strongly or weakly stochastic transitive models, we show that the\n''sparsity'' of practical matching data is a critical factor behind Elo's\nsuperior performance in prediction compared to more complex rating systems. (c)\nWe observe a strong correlation between Elo's predictive accuracy and its\nranking performance, further supporting its effectiveness in ranking.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ME",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "23pages",
    "pdf_url": "http://arxiv.org/pdf/2502.10985v1",
    "published_date": "2025-02-16 04:07:33 UTC",
    "updated_date": "2025-02-16 04:07:33 UTC"
  },
  {
    "arxiv_id": "2502.10978v1",
    "title": "Agentic LLM Framework for Adaptive Decision Discourse",
    "authors": [
      "Antoine Dolant",
      "Praveen Kumar"
    ],
    "abstract": "Effective decision-making in complex systems requires synthesizing diverse\nperspectives to address multifaceted challenges under uncertainty. This study\nintroduces a real-world inspired agentic Large Language Models (LLMs)\nframework, to simulate and enhance decision discourse-the deliberative process\nthrough which actionable strategies are collaboratively developed. Unlike\ntraditional decision-support tools, the framework emphasizes dialogue,\ntrade-off exploration, and the emergent synergies generated by interactions\namong agents embodying distinct personas. These personas simulate diverse\nstakeholder roles, each bringing unique priorities, expertise, and value-driven\nreasoning to the table. The framework incorporates adaptive and self-governing\nmechanisms, enabling agents to dynamically summon additional expertise and\nrefine their assembly to address evolving challenges. An illustrative\nhypothetical example focused on extreme flooding in a Midwestern township\ndemonstrates the framework's ability to navigate uncertainty, balance competing\npriorities, and propose mitigation and adaptation strategies by considering\nsocial, economic, and environmental dimensions. Results reveal how the\nbreadth-first exploration of alternatives fosters robust and equitable\nrecommendation pathways. This framework transforms how decisions are approached\nin high-stakes scenarios and can be incorporated in digital environments. It\nnot only augments decision-makers' capacity to tackle complexity but also sets\na foundation for scalable and context-aware AI-driven recommendations. This\nresearch explores novel and alternate routes leveraging agentic LLMs for\nadaptive, collaborative, and equitable recommendation processes, with\nimplications across domains where uncertainty and complexity converge.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "24 pages, 4 figures, 1 appendix",
    "pdf_url": "http://arxiv.org/pdf/2502.10978v1",
    "published_date": "2025-02-16 03:46:37 UTC",
    "updated_date": "2025-02-16 03:46:37 UTC"
  },
  {
    "arxiv_id": "2502.10976v1",
    "title": "QuOTE: Question-Oriented Text Embeddings",
    "authors": [
      "Andrew Neeser",
      "Kaylen Latimer",
      "Aadyant Khatri",
      "Chris Latimer",
      "Naren Ramakrishnan"
    ],
    "abstract": "We present QuOTE (Question-Oriented Text Embeddings), a novel enhancement to\nretrieval-augmented generation (RAG) systems, aimed at improving document\nrepresentation for accurate and nuanced retrieval. Unlike traditional RAG\npipelines, which rely on embedding raw text chunks, QuOTE augments chunks with\nhypothetical questions that the chunk can potentially answer, enriching the\nrepresentation space. This better aligns document embeddings with user query\nsemantics, and helps address issues such as ambiguity and context-dependent\nrelevance. Through extensive experiments across diverse benchmarks, we\ndemonstrate that QuOTE significantly enhances retrieval accuracy, including in\nmulti-hop question-answering tasks. Our findings highlight the versatility of\nquestion generation as a fundamental indexing strategy, opening new avenues for\nintegrating question generation into retrieval-based AI pipelines.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "H.3"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.10976v1",
    "published_date": "2025-02-16 03:37:13 UTC",
    "updated_date": "2025-02-16 03:37:13 UTC"
  },
  {
    "arxiv_id": "2502.13160v3",
    "title": "Attention Mechanism for LLM-based Agents Dynamic Diffusion under Information Asymmetry",
    "authors": [
      "Yiwen Zhang",
      "Yifu Wu",
      "Wenyue Hua",
      "Xiang Lu",
      "Xuming Hu"
    ],
    "abstract": "Large language models have been used to simulate human society using\nmulti-agent systems. Most current social simulation research emphasizes\ninteractive behaviors in fixed environments, ignoring information opacity,\nrelationship variability, and diffusion diversity. In this paper, we first\npropose a general framework for exploring multi-agent information diffusion. We\nidentified LLMs' deficiency in the perception and utilization of social\nrelationships, as well as diverse actions. Then, we designed a dynamic\nattention mechanism to help agents allocate attention to different information,\naddressing the limitations of the LLM attention mechanism. Agents start by\nresponding to external information stimuli within a five-agent group,\nincreasing group size and forming information circles while developing\nrelationships and sharing information. Additionally, we explore the information\ndiffusion features in the asymmetric open environment by observing the\nevolution of information gaps, diffusion patterns, and the accumulation of\nsocial capital, which are closely linked to psychological, sociological, and\ncommunication theories.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "18 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.13160v3",
    "published_date": "2025-02-16 03:02:48 UTC",
    "updated_date": "2025-05-20 14:34:54 UTC"
  },
  {
    "arxiv_id": "2502.10966v1",
    "title": "Neural Networks Remember More: The Power of Parameter Isolation and Combination",
    "authors": [
      "Biqing Zeng",
      "Zehan Li",
      "Aladdin Ayesh"
    ],
    "abstract": "Catastrophic forgetting is a pervasive issue for pre-trained language models\n(PLMs) during continual learning, where models lose previously acquired\nknowledge when sequentially trained on a series of tasks. The model's ability\nto retain old tasks is referred to as stability, while its adaptability to new\ntasks is called plasticity. Therefore, the key to solving this problem is to\nfind a trade-off between the plasticity and stability of the model. To address\nthis issue, in this paper, we propose a novel method to achieve a balance\nbetween model stability and plasticity, thereby mitigating catastrophic\nforgetting. More specifically, our proposed approach leverages parameter\nisolation and a subsequent combination strategy. Initially, in the training\nstage, the model adapts to each downstream task via a parameter isolation\nmethod to prevent potential interference among different tasks. We then combine\nall trained parameters, which contain acquired knowledge, using the task\narithmetic method and finally apply them to the backbone model. Empirical\nevaluations on continual language learning benchmarks substantiate the\neffectiveness of our approach, revealing a marked enhancement over existing\nstate-of-the-art approaches.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.10966v1",
    "published_date": "2025-02-16 02:58:57 UTC",
    "updated_date": "2025-02-16 02:58:57 UTC"
  },
  {
    "arxiv_id": "2502.10961v1",
    "title": "Graders should cheat: privileged information enables expert-level automated evaluations",
    "authors": [
      "Jin Peng Zhou",
      "Sébastien M. R. Arnold",
      "Nan Ding",
      "Kilian Q. Weinberger",
      "Nan Hua",
      "Fei Sha"
    ],
    "abstract": "Auto-evaluating language models (LMs), i.e., using a grader LM to evaluate\nthe candidate LM, is an appealing way to accelerate the evaluation process and\nthe cost associated with it. But this presents a paradox: how can we trust the\ngrader LM, which is presumably weaker than the candidate LM, to assess problems\nthat are beyond the frontier of the capabilities of either model or both? For\ninstance, today's LMs struggle on graduate-level physics and Olympiad-level\nmath, making them unreliable graders in these domains.\n  We show that providing privileged information -- such as ground-truth\nsolutions or problem-specific guidelines -- improves automated evaluations on\nsuch frontier problems. This approach offers two key advantages. First, it\nexpands the range of problems where LMs graders apply. Specifically, weaker\nmodels can now rate the predictions of stronger models. Second, privileged\ninformation can be used to devise easier variations of challenging problems\nwhich improves the separability of different LMs on tasks where their\nperformance is generally low. With this approach, general-purpose LM graders\nmatch the state of the art performance on RewardBench, surpassing almost all\nthe specially-tuned models. LM graders also outperform individual human raters\non Vibe-Eval, and approach human expert graders on Olympiad-level math\nproblems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.10961v1",
    "published_date": "2025-02-16 02:47:41 UTC",
    "updated_date": "2025-02-16 02:47:41 UTC"
  },
  {
    "arxiv_id": "2502.10955v1",
    "title": "A recurrent vision transformer shows signatures of primate visual attention",
    "authors": [
      "Jonathan Morgan",
      "Badr Albanna",
      "James P. Herman"
    ],
    "abstract": "Attention is fundamental to both biological and artificial intelligence, yet\nresearch on animal attention and AI self attention remains largely\ndisconnected. We propose a Recurrent Vision Transformer (Recurrent ViT) that\nintegrates self-attention with recurrent memory, allowing both current inputs\nand stored information to guide attention allocation. Trained solely via sparse\nreward feedback on a spatially cued orientation change detection task, a\nparadigm used in primate studies, our model exhibits primate like signatures of\nattention, including improved accuracy and faster responses for cued stimuli\nthat scale with cue validity. Analysis of self-attention maps reveals dynamic\nspatial prioritization with reactivation prior to expected changes, and\ntargeted perturbations produce performance shifts similar to those observed in\nprimate frontal eye fields and superior colliculus. These findings demonstrate\nthat incorporating recurrent feedback into self attention can capture key\naspects of primate visual attention.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "q-bio.NC"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.10955v1",
    "published_date": "2025-02-16 02:22:27 UTC",
    "updated_date": "2025-02-16 02:22:27 UTC"
  },
  {
    "arxiv_id": "2502.10954v2",
    "title": "Learning to Stop Overthinking at Test Time",
    "authors": [
      "Hieu Tran Bao",
      "Nguyen Cong Dat",
      "Nguyen Duc Anh",
      "Hoang Thanh-Tung"
    ],
    "abstract": "Test time scaling is currently one of the most active research areas that\nshows promise after training time scaling has reached its limits. Deep-thinking\n(DT) models are a class of recurrent models that can perform easy-to-hard\ngeneralization by assigning more compute to harder test samples. However, due\nto their inability to determine the complexity of a test sample, DT models have\nto use a large amount of computation for both easy and hard test samples.\nExcessive test time computation is wasteful and can cause the ``overthinking''\nproblem where more test time computation leads to worse results. In this paper,\nwe introduce a test time training method for determining the optimal amount of\ncomputation needed for each sample during test time. We also propose\nConv-LiGRU, a novel recurrent architecture for efficient and robust visual\nreasoning. Extensive experiments demonstrate that Conv-LiGRU is more stable\nthan DT, effectively mitigates the ``overthinking'' phenomenon, and achieves\nsuperior accuracy.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.10954v2",
    "published_date": "2025-02-16 02:17:05 UTC",
    "updated_date": "2025-02-18 03:41:03 UTC"
  },
  {
    "arxiv_id": "2502.10953v1",
    "title": "Empirical evaluation of LLMs in predicting fixes of Configuration bugs in Smart Home System",
    "authors": [
      "Sheikh Moonwara Anjum Monisha",
      "Atul Bharadwaj"
    ],
    "abstract": "This empirical study evaluates the effectiveness of Large Language Models\n(LLMs) in predicting fixes for configuration bugs in smart home systems. The\nresearch analyzes three prominent LLMs - GPT-4, GPT-4o (GPT-4 Turbo), and\nClaude 3.5 Sonnet - using four distinct prompt designs to assess their ability\nto identify appropriate fix strategies and generate correct solutions. The\nstudy utilized a dataset of 129 debugging issues from the Home Assistant\nCommunity, focusing on 21 randomly selected cases for in-depth analysis.\nResults demonstrate that GPT-4 and Claude 3.5 Sonnet achieved 80\\% accuracy in\nstrategy prediction when provided with both bug descriptions and original\nscripts. GPT-4 exhibited consistent performance across different prompt types,\nwhile GPT-4o showed advantages in speed and cost-effectiveness despite slightly\nlower accuracy. The findings reveal that prompt design significantly impacts\nmodel performance, with comprehensive prompts containing both description and\noriginal script yielding the best results. This research provides valuable\ninsights for improving automated bug fixing in smart home system configurations\nand demonstrates the potential of LLMs in addressing configuration-related\nchallenges.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.10953v1",
    "published_date": "2025-02-16 02:11:36 UTC",
    "updated_date": "2025-02-16 02:11:36 UTC"
  },
  {
    "arxiv_id": "2502.10940v2",
    "title": "CoLA: Compute-Efficient Pre-Training of LLMs via Low-Rank Activation",
    "authors": [
      "Ziyue Liu",
      "Ruijie Zhang",
      "Zhengyang Wang",
      "Zi Yang",
      "Paul Hovland",
      "Bogdan Nicolae",
      "Franck Cappello",
      "Zheng Zhang"
    ],
    "abstract": "The full-size MLPs and the projection layers in attention introduce\ntremendous model sizes of large language models (LLMs), imposing extremely\ndemanding needs of computational resources in the pre-training stage. However,\nwe empirically observe that the activations of pre-trained LLMs exhibit\nlow-rank property. Motivated by such observations, we propose CoLA and its\nmemory-efficient implementation, CoLA-M, to replace these full-size layers with\ncompute-efficient auto-encoders that naturally enforce low-rank activations\nthroughout training. This fundamental architectural change eliminates the\nactivation redundancy and significantly boosts model capacity and training\nefficiency. Experiments on LLaMA models with 60 million to 7 billion parameters\nshow that CoLA reduces the computing cost by $\\bf 2\\pmb{\\times}$ and improves\ntraining throughput by $\\bf 1.86\\pmb{\\times}$ while maintaining full-rank level\nperformance. CoLA-M further squeezes memory cost without sacrificing\nthroughput, offering a pre-training approach with collectively superior\nparameter, computing, and memory efficiency. The LLMs produced are also $\\bf\n2\\pmb{\\times}$ smaller, enabling faster inference with lower memory cost on\nresource-constrained platforms.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "v2",
    "pdf_url": "http://arxiv.org/pdf/2502.10940v2",
    "published_date": "2025-02-16 01:05:16 UTC",
    "updated_date": "2025-05-20 16:27:05 UTC"
  },
  {
    "arxiv_id": "2502.12198v1",
    "title": "Maximize Your Diffusion: A Study into Reward Maximization and Alignment for Diffusion-based Control",
    "authors": [
      "Dom Huh",
      "Prasant Mohapatra"
    ],
    "abstract": "Diffusion-based planning, learning, and control methods present a promising\nbranch of powerful and expressive decision-making solutions. Given the growing\ninterest, such methods have undergone numerous refinements over the past years.\nHowever, despite these advancements, existing methods are limited in their\ninvestigations regarding general methods for reward maximization within the\ndecision-making process. In this work, we study extensions of fine-tuning\napproaches for control applications. Specifically, we explore extensions and\nvarious design choices for four fine-tuning approaches: reward alignment\nthrough reinforcement learning, direct preference optimization, supervised\nfine-tuning, and cascading diffusion. We optimize their usage to merge these\nindependent efforts into one unified paradigm. We show the utility of such\npropositions in offline RL settings and demonstrate empirical improvements over\na rich array of control tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12198v1",
    "published_date": "2025-02-16 00:30:39 UTC",
    "updated_date": "2025-02-16 00:30:39 UTC"
  },
  {
    "arxiv_id": "2502.10938v1",
    "title": "PEA: Enhancing LLM Performance on Computational-Reasoning Tasks",
    "authors": [
      "Zi Wang",
      "Shiwei Weng",
      "Mohannad Alhanahnah",
      "Somesh Jha",
      "Tom Reps"
    ],
    "abstract": "Large Language Models (LLMs) have exhibited remarkable capabilities across\ndiverse domains, prompting investigations into their potential as generic\nreasoning engines. While recent studies have explored inference-time\ncomputation to enhance model performance on complex problems, current research\nlacks a formal framework to characterize the complexity of reasoning tasks.\nThis study introduces the Predicate-Enumeration-Aggregation (PEA) framework, a\nformal approach to describe and solve a class of important reasoning tasks\ntermed computational reasoning problems. The PEA framework decomposes these\nproblems into predicate and enumeration components, using LLMs to synthesize\nprograms based on specified predicates, enumeration, and aggregation rules.\nThese synthesized programs are then executed to obtain solutions to the\ncomputational tasks. We demonstrate the framework's efficacy on benchmark tasks\nincluding Boolean satisfiability problems, game of $24$, and planning problems.\nEmpirical evaluation reveals that PEA substantially enhances the performance of\nunderlying models on benchmark computational problems, yielding an average\naccuracy improvement of approximately $50\\%$, coupled with increased\nefficiency.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.10938v1",
    "published_date": "2025-02-16 00:27:05 UTC",
    "updated_date": "2025-02-16 00:27:05 UTC"
  },
  {
    "arxiv_id": "2502.10937v1",
    "title": "SCALE: Towards Collaborative Content Analysis in Social Science with Large Language Model Agents and Human Intervention",
    "authors": [
      "Chengshuai Zhao",
      "Zhen Tan",
      "Chau-Wai Wong",
      "Xinyan Zhao",
      "Tianlong Chen",
      "Huan Liu"
    ],
    "abstract": "Content analysis breaks down complex and unstructured texts into\ntheory-informed numerical categories. Particularly, in social science, this\nprocess usually relies on multiple rounds of manual annotation, domain expert\ndiscussion, and rule-based refinement. In this paper, we introduce SCALE, a\nnovel multi-agent framework that effectively $\\underline{\\textbf{S}}$imulates\n$\\underline{\\textbf{C}}$ontent $\\underline{\\textbf{A}}$nalysis via\n$\\underline{\\textbf{L}}$arge language model (LLM)\nag$\\underline{\\textbf{E}}$nts. SCALE imitates key phases of content analysis,\nincluding text coding, collaborative discussion, and dynamic codebook\nevolution, capturing the reflective depth and adaptive discussions of human\nresearchers. Furthermore, by integrating diverse modes of human intervention,\nSCALE is augmented with expert input to further enhance its performance.\nExtensive evaluations on real-world datasets demonstrate that SCALE achieves\nhuman-approximated performance across various complex content analysis tasks,\noffering an innovative potential for future social science research.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.10937v1",
    "published_date": "2025-02-16 00:19:07 UTC",
    "updated_date": "2025-02-16 00:19:07 UTC"
  }
]
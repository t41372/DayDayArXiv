{
  "date": "2024-11-23",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-11-23 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文主要聚焦 AI 安全、多模态模型、LLM 应用和深度学习优化等领域，强调模型鲁棒性、人类AI协作以及新基准的建立。其中，Aligning Generalisation Between Humans and Machines 一文由知名学者如 Frank van Harmelen 等合作发布，探讨人类与机器的泛化差异，具有高度话题性；LLM-as-a-Judge 的综述则为 LLM 评估提供全面框架，令人印象深刻。接下来，我们优先讨论这些关键论文，再快速掠过其他领域的内容。\n\n### 重点论文讨论\n\n**Aligning Generalisation Between Humans and Machines（人类与机器的泛化对齐）**  \n作者包括 Filip Ilievski、Barbara Hammer、Frank van Harmelen 等知名学者。该文比较人类和机器在泛化方面的差异，涵盖泛化概念、方法和评估三个维度。主要贡献是通过跨学科视角映射 AI 和认知科学的共同点与差异，为人类-AI 协作提供基础，提出未来挑战以提升 AI 的认知支持。该发现强调了 AI 在抽象和规则推理中的局限性，可能推动更可靠的人机团队设计。\n\n**A Survey on LLM-as-a-Judge（LLM-as-a-Judge 综述）**  \n作者如 Jiawei Gu 和 Jian Guo。该综述探讨大型语言模型作为评估工具的可靠性，主要贡献是提出增强一致性、减少偏差的策略，并建立基准评估方法。该发现揭示 LLM 在评估任务中的潜力，但强调需要标准化以应对实际部署挑战，是 LLM 应用领域的关键参考。\n\n**LoBAM: LoRA-Based Backdoor Attack on Model Merging（基于 LoRA 的后门攻击模型合并）**  \n该文提出 LoBAM 方法，用于在资源有限的场景下进行后门攻击。主要贡献是通过放大恶意权重提升攻击成功率，同时保持隐蔽性。该发现暴露了模型合并的安全风险，提醒 AI 社区关注 LoRA 等高效微调技术的潜在威胁。\n\n**ChemSafetyBench: Benchmarking LLM Safety on Chemistry Domain（化学领域 LLM 安全基准）**  \n作者包括 Haochen Zhao 和 Mark Gerstein。该基准针对 LLM 在化学中的安全性和准确性，提供 30K+ 样本的评估任务。主要贡献是通过实验证明 LLM 在处理危险化学查询时的局限性，并提出改进策略。该发现强调 LLM 在科学应用中的风险管理，具有实际部署价值。\n\n**Document Haystacks: Vision-Language Reasoning Over Piles of 1000+ Documents（处理 1000+ 文档的视觉语言推理）**  \n该文引入 V-RAG 框架，用于大规模文档检索和理解。主要贡献是提出多模态编码器和相关性模块，提升了 LMM 在处理海量图像时的召回率。该发现展示了 V-RAG 在视觉语言任务中的优越性，适用于真实世界检索场景。\n\n**Ontology-Constrained Generation of Domain-Specific Clinical Summaries（本体约束的领域特定临床摘要生成）**  \n该方法使用本体指导解码，减少幻觉并提升医疗摘要的相关性。主要贡献是应用于 EHR 数据集，显著改善摘要质量。该发现为医疗 AI 提供更可靠的总结工具，帮助医生聚焦关键信息。\n\n**FollowGen: A Scaled Noise Conditional Diffusion Model for Car-Following Trajectory Prediction（缩放噪声条件扩散模型用于车辆跟随轨迹预测）**  \n该模型通过跨注意力机制捕捉车辆互动，提升轨迹预测的准确性和合理性。主要贡献是基于真实场景实验，达到最先进性能。该发现对自动驾驶系统有重要启发。\n\n### 其他论文快速掠过\n以下论文涉及较窄领域或次要创新，我们简要概述：\n\n- **IRSKG: Unified Intrusion Response System Knowledge Graph Ontology for Cyber Defense（统一入侵响应系统知识图谱本体用于网络防御）**：提出 IRSKG 本体，提升网络威胁响应自动化和可解释性。\n- **Text-to-SQL Calibration: No Need to Ask -- Just Rescale Model Probabilities（Text-to-SQL 校准：无需询问，只需重标模型概率）**：证明简单概率重标优于自检方法，提升 SQL 生成置信度。\n- **Gradient-Guided Parameter Mask for Multi-Scenario Image Restoration（梯度引导参数掩码用于多场景图像恢复）**：无额外参数设计，提升天气图像恢复性能，达到最先进 PSNR 分数。\n- **EmotiveTalk: Expressive Talking Head Generation through Audio Information Decoupling and Emotional Video Diffusion（通过音频信息解耦和情感视频扩散的表达性头像生成）**：改进视频生成框架，提升情感控制和稳定性。\n- **ReWind: Understanding Long Videos with Instructed Learnable Memory（通过指令可学习记忆理解长视频）**：提出记忆模块，提升长视频问答和时间定位准确性。\n- 其余如 **How Texts Help? A Fine-grained Evaluation to Reveal the Role of Language in Vision-Language Tracking**（揭示文本在视觉语言跟踪中的作用，通过细粒度评估识别瓶颈）和 **LAGUNA: LAnguage Guided UNsupervised Adaptation with structured spaces**（语言引导的无监督适应框架），等论文虽有创新，但影响较小，仅在特定子领域有价值。\n\n今天的 arXiv 更新展示了 AI 领域的多样性与深度，AI 安全和协作主题尤其值得跟踪。如果你对特定论文感兴趣，建议直接查阅原文！明天见！",
  "papers": [
    {
      "arxiv_id": "2411.15672v1",
      "title": "IRSKG: Unified Intrusion Response System Knowledge Graph Ontology for Cyber Defense",
      "title_zh": "IRSKG：统一的入侵响应系统知识图谱本体用于网络防御",
      "authors": [
        "Damodar Panigrahi",
        "Shaswata Mitra",
        "Subash Neupane",
        "Sudip Mittal",
        "Benjamin A. Blakely"
      ],
      "abstract": "Cyberattacks are becoming increasingly difficult to detect and prevent due to\ntheir sophistication. In response, Autonomous Intelligent Cyber-defense Agents\n(AICAs) are emerging as crucial solutions. One prominent AICA agent is the\nIntrusion Response System (IRS), which is critical for mitigating threats after\ndetection. IRS uses several Tactics, Techniques, and Procedures (TTPs) to\nmitigate attacks and restore the infrastructure to normal operations.\nContinuous monitoring of the enterprise infrastructure is an essential TTP the\nIRS uses. However, each system serves different purposes to meet operational\nneeds. Integrating these disparate sources for continuous monitoring increases\npre-processing complexity and limits automation, eventually prolonging critical\nresponse time for attackers to exploit. We propose a unified IRS Knowledge\nGraph ontology (IRSKG) that streamlines the onboarding of new enterprise\nsystems as a source for the AICAs. Our ontology can capture system monitoring\nlogs and supplemental data, such as a rules repository containing the\nadministrator-defined policies to dictate the IRS responses. Besides, our\nontology permits us to incorporate dynamic changes to adapt to the evolving\ncyber-threat landscape. This robust yet concise design allows machine learning\nmodels to train effectively and recover a compromised system to its desired\nstate autonomously with explainability.",
      "tldr_zh": "该论文针对网络攻击的复杂性及其对Autonomous Intelligent Cyber-defense Agents (AICAs)的影响，提出了IRSKG——一个统一的Intrusion Response System (IRS) Knowledge Graph本体，用于简化网络防御中的系统整合。IRSKG能够捕获系统监控日志、管理员定义的规则库，并适应动态的网络威胁景观，从而减少预处理复杂性和响应时间。实验设计表明，这种简洁且鲁棒的本体支持机器学习模型的有效训练，实现系统的自主恢复和可解释性（explainability）。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "10 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.15672v1",
      "published_date": "2024-11-23 23:31:55 UTC",
      "updated_date": "2024-11-23 23:31:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:35:11.467432"
    },
    {
      "arxiv_id": "2411.16747v1",
      "title": "FollowGen: A Scaled Noise Conditional Diffusion Model for Car-Following Trajectory Prediction",
      "title_zh": "FollowGen: 一种用于跟车轨迹预测的缩放噪声条件扩散模型",
      "authors": [
        "Junwei You",
        "Rui Gan",
        "Weizhe Tang",
        "Zilin Huang",
        "Jiaxi Liu",
        "Zhuoyu Jiang",
        "Haotian Shi",
        "Keshu Wu",
        "Keke Long",
        "Sicheng Fu",
        "Sikai Chen",
        "Bin Ran"
      ],
      "abstract": "Vehicle trajectory prediction is crucial for advancing autonomous driving and\nadvanced driver assistance systems (ADAS). Although deep learning-based\napproaches - especially those utilizing transformer-based and generative models\n- have markedly improved prediction accuracy by capturing complex, non-linear\npatterns in vehicle dynamics and traffic interactions, they frequently overlook\ndetailed car-following behaviors and the inter-vehicle interactions critical\nfor real-world driving applications, particularly in fully autonomous or mixed\ntraffic scenarios. To address the issue, this study introduces a scaled noise\nconditional diffusion model for car-following trajectory prediction, which\nintegrates detailed inter-vehicular interactions and car-following dynamics\ninto a generative framework, improving both the accuracy and plausibility of\npredicted trajectories. The model utilizes a novel pipeline to capture\nhistorical vehicle dynamics by scaling noise with encoded historical features\nwithin the diffusion process. Particularly, it employs a cross-attention-based\ntransformer architecture to model intricate inter-vehicle dependencies,\neffectively guiding the denoising process and enhancing prediction accuracy.\nExperimental results on diverse real-world driving scenarios demonstrate the\nstate-of-the-art performance and robustness of the proposed method.",
      "tldr_zh": "本文提出 FollowGen，一种 Scaled Noise Conditional Diffusion Model，用于车辆跟车轨迹预测，旨在解决现有深度学习方法忽略详细车辆间互动和跟车动态的问题。该模型通过一个新颖的管道，将缩放噪声与历史车辆特征相结合，并采用基于 Cross-Attention 的 Transformer 架构来捕捉复杂的车辆依赖关系，从而提升轨迹预测的准确性和合理性。实验在多种真实驾驶场景中证明，该方法实现了 state-of-the-art 性能和鲁棒性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.CV",
      "comment": "arXiv admin note: text overlap with arXiv:2406.11941",
      "pdf_url": "http://arxiv.org/pdf/2411.16747v1",
      "published_date": "2024-11-23 23:13:45 UTC",
      "updated_date": "2024-11-23 23:13:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:35:24.194863"
    },
    {
      "arxiv_id": "2411.15666v1",
      "title": "Ontology-Constrained Generation of Domain-Specific Clinical Summaries",
      "title_zh": "翻译失败",
      "authors": [
        "Gaya Mehenni",
        "Amal Zouaq"
      ],
      "abstract": "Large Language Models (LLMs) offer promising solutions for text\nsummarization. However, some domains require specific information to be\navailable in the summaries. Generating these domain-adapted summaries is still\nan open challenge. Similarly, hallucinations in generated content is a major\ndrawback of current approaches, preventing their deployment. This study\nproposes a novel approach that leverages ontologies to create domain-adapted\nsummaries both structured and unstructured. We employ an ontology-guided\nconstrained decoding process to reduce hallucinations while improving\nrelevance. When applied to the medical domain, our method shows potential in\nsummarizing Electronic Health Records (EHRs) across different specialties,\nallowing doctors to focus on the most relevant information to their domain.\nEvaluation on the MIMIC-III dataset demonstrates improvements in generating\ndomain-adapted summaries of clinical notes and hallucination reduction.",
      "tldr_zh": "这篇论文提出了一种利用本体（ontologies）的新方法，来生成特定领域的临床摘要，旨在解决 Large Language Models (LLMs) 在文本总结中存在的幻觉问题和领域适应性不足。方法采用 ontology-guided constrained decoding 过程，确保生成的摘要既结构化又非结构化，同时提高相关性和减少幻觉。在医疗领域应用中，该方法用于总结 Electronic Health Records (EHRs)，帮助医生专注于相关信息，并在 MIMIC-III 数据集上的评估显示了显著改进。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "24th International Conference on Knowledge Engineering and Knowledge\n  Management (EKAW 2024), November 26-28, 2024, Amsterdam, The Netherlands",
      "pdf_url": "http://arxiv.org/pdf/2411.15666v1",
      "published_date": "2024-11-23 23:05:48 UTC",
      "updated_date": "2024-11-23 23:05:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:37:25.996180"
    },
    {
      "arxiv_id": "2411.16746v3",
      "title": "LoBAM: LoRA-Based Backdoor Attack on Model Merging",
      "title_zh": "翻译失败",
      "authors": [
        "Ming Yin",
        "Jingyang Zhang",
        "Jingwei Sun",
        "Minghong Fang",
        "Hai Li",
        "Yiran Chen"
      ],
      "abstract": "Model merging is an emerging technique that integrates multiple models\nfine-tuned on different tasks to create a versatile model that excels in\nmultiple domains. This scheme, in the meantime, may open up backdoor attack\nopportunities where one single malicious model can jeopardize the integrity of\nthe merged model. Existing works try to demonstrate the risk of such attacks by\nassuming substantial computational resources, focusing on cases where the\nattacker can fully fine-tune the pre-trained model. Such an assumption,\nhowever, may not be feasible given the increasing size of machine learning\nmodels. In practice where resources are limited and the attacker can only\nemploy techniques like Low-Rank Adaptation (LoRA) to produce the malicious\nmodel, it remains unclear whether the attack can still work and pose threats.\nIn this work, we first identify that the attack efficacy is significantly\ndiminished when using LoRA for fine-tuning. Then, we propose LoBAM, a method\nthat yields high attack success rate with minimal training resources. The key\nidea of LoBAM is to amplify the malicious weights in an intelligent way that\neffectively enhances the attack efficacy. We demonstrate that our design can\nlead to improved attack success rate through extensive empirical experiments\nacross various model merging scenarios. Moreover, we show that our method is\nhighly stealthy and is difficult to detect and defend against.",
      "tldr_zh": "该论文探讨了模型合并(Model Merging)技术在整合多任务微调模型时面临的LoRA-based后门攻击(Backdoor Attack)风险，指出现有攻击方法依赖大量资源的全微调，而资源有限时攻击效果显著降低。作者提出LoBAM方法，通过智能放大恶意权重的方式，仅需最小训练资源即可实现高攻击成功率。实验结果显示，LoBAM在多种模型合并场景中显著提升攻击成功率，同时保持高度隐蔽性，难以被检测和防御。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.16746v3",
      "published_date": "2024-11-23 20:41:24 UTC",
      "updated_date": "2025-03-05 05:34:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:35:47.861198"
    },
    {
      "arxiv_id": "2411.16742v1",
      "title": "Text-to-SQL Calibration: No Need to Ask -- Just Rescale Model Probabilities",
      "title_zh": "翻译失败",
      "authors": [
        "Ashwin Ramachandran",
        "Sunita Sarawagi"
      ],
      "abstract": "Calibration is crucial as large language models (LLMs) are increasingly\ndeployed to convert natural language queries into SQL for commercial databases.\nIn this work, we investigate calibration techniques for assigning confidence to\ngenerated SQL queries. We show that a straightforward baseline -- deriving\nconfidence from the model's full-sequence probability -- outperforms recent\nmethods that rely on follow-up prompts for self-checking and confidence\nverbalization. Our comprehensive evaluation, conducted across two widely-used\nText-to-SQL benchmarks and multiple LLM architectures, provides valuable\ninsights into the effectiveness of various calibration strategies.",
      "tldr_zh": "这篇论文探讨了大型语言模型 (LLMs) 在 Text-to-SQL 任务中的校准技术，提出一种简单基线方法：仅通过重新缩放模型的全序列概率来分配生成的 SQL 查询置信度，而无需依赖后续提示的自检或表述。实验结果显示，该方法在两个广泛使用的 Text-to-SQL 基准上以及多种 LLM 架构中，优于现有的依赖跟进提示的策略。论文通过全面评估提供了各种校准策略的有效性洞见，为提升 LLMs 在商业数据库查询中的可靠性和准确性奠定了基础。",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.DB",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.16742v1",
      "published_date": "2024-11-23 19:20:24 UTC",
      "updated_date": "2024-11-23 19:20:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:35:59.885590"
    },
    {
      "arxiv_id": "2411.15634v1",
      "title": "\"All that Glitters\": Approaches to Evaluations with Unreliable Model and Human Annotations",
      "title_zh": "翻译失败",
      "authors": [
        "Michael Hardy"
      ],
      "abstract": "\"Gold\" and \"ground truth\" human-mediated labels have error. The effects of\nthis error can escape commonly reported metrics of label quality or obscure\nquestions of accuracy, bias, fairness, and usefulness during model evaluation.\nThis study demonstrates methods for answering such questions even in the\ncontext of very low reliabilities from expert humans. We analyze human labels,\nGPT model ratings, and transformer encoder model annotations describing the\nquality of classroom teaching, an important, expensive, and currently only\nhuman task. We answer the question of whether such a task can be automated\nusing two Large Language Model (LLM) architecture families--encoders and GPT\ndecoders, using novel approaches to evaluating label quality across six\ndimensions: Concordance, Confidence, Validity, Bias, Fairness, and Helpfulness.\nFirst, we demonstrate that using standard metrics in the presence of poor\nlabels can mask both label and model quality: the encoder family of models\nachieve state-of-the-art, even \"super-human\", results across all classroom\nannotation tasks. But not all these positive results remain after using more\nrigorous evaluation measures which reveal spurious correlations and nonrandom\nracial biases across models and humans. This study then expands these methods\nto estimate how model use would change to human label quality if models were\nused in a human-in-the-loop context, finding that the variance captured in GPT\nmodel labels would worsen reliabilities for humans influenced by these models.\nWe identify areas where some LLMs, within the generalizability of the current\ndata, could improve the quality of expensive human ratings of classroom\ninstruction.",
      "tldr_zh": "本研究探讨了人类和模型注解的不可靠性对评估的影响，提出方法评估“Gold”标签的错误及其对准确性、偏差、公平性和有用性的潜在问题。作者分析了人类标签、GPT 模型评分和 transformer 编码器模型在课堂教学质量注解任务中的表现，使用六个维度（Concordance、Confidence、Validity、Bias、Fairness 和 Helpfulness）进行更严格的评估。结果显示，编码器模型虽在标准指标下表现出“超人”性能，但存在虚假相关性和种族偏差；此外，GPT 模型标签可能在人类循环中使用时加剧可靠性问题。总体而言，该研究为自动化昂贵的人类评估任务提供了新方法，并识别出某些 LLM 可提升评估质量。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "stat.AP"
      ],
      "primary_category": "cs.CL",
      "comment": "20 pages, 15 figures, 58 pages with references and appendices",
      "pdf_url": "http://arxiv.org/pdf/2411.15634v1",
      "published_date": "2024-11-23 19:18:08 UTC",
      "updated_date": "2024-11-23 19:18:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:36:12.781585"
    },
    {
      "arxiv_id": "2411.15626v1",
      "title": "Aligning Generalisation Between Humans and Machines",
      "title_zh": "人类与机器之间的泛化对齐",
      "authors": [
        "Filip Ilievski",
        "Barbara Hammer",
        "Frank van Harmelen",
        "Benjamin Paassen",
        "Sascha Saralajew",
        "Ute Schmid",
        "Michael Biehl",
        "Marianna Bolognesi",
        "Xin Luna Dong",
        "Kiril Gashteovski",
        "Pascal Hitzler",
        "Giuseppe Marra",
        "Pasquale Minervini",
        "Martin Mundt",
        "Axel-Cyrille Ngonga Ngomo",
        "Alessandro Oltramari",
        "Gabriella Pasi",
        "Zeynep G. Saribatur",
        "Luciano Serafini",
        "John Shawe-Taylor",
        "Vered Shwartz",
        "Gabriella Skitalinskaya",
        "Clemens Stachl",
        "Gido M. van de Ven",
        "Thomas Villmann"
      ],
      "abstract": "Recent advances in AI -- including generative approaches -- have resulted in\ntechnology that can support humans in scientific discovery and decision support\nbut may also disrupt democracies and target individuals. The responsible use of\nAI increasingly shows the need for human-AI teaming, necessitating effective\ninteraction between humans and machines. A crucial yet often overlooked aspect\nof these interactions is the different ways in which humans and machines\ngeneralise. In cognitive science, human generalisation commonly involves\nabstraction and concept learning. In contrast, AI generalisation encompasses\nout-of-domain generalisation in machine learning, rule-based reasoning in\nsymbolic AI, and abstraction in neuro-symbolic AI. In this perspective paper,\nwe combine insights from AI and cognitive science to identify key commonalities\nand differences across three dimensions: notions of generalisation, methods for\ngeneralisation, and evaluation of generalisation. We map the different\nconceptualisations of generalisation in AI and cognitive science along these\nthree dimensions and consider their role in human-AI teaming. This results in\ninterdisciplinary challenges across AI and cognitive science that must be\ntackled to provide a foundation for effective and cognitively supported\nalignment in human-AI teaming scenarios.",
      "tldr_zh": "这篇论文探讨了人类和机器在generalisation（泛化）方面的差异及其对人类-AI 团队合作的影响。作者结合AI和认知科学，比较了三方面：generalisation的概念（如人类侧重抽象和概念学习，AI包括out-of-domain generalisation和符号推理）、实现方法，以及评估方式。论文识别了关键共同点和差异，提出跨学科挑战，以提升generalisation的认知支持，从而为有效的人类-AI 团队合作奠定基础。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15626v1",
      "published_date": "2024-11-23 18:36:07 UTC",
      "updated_date": "2024-11-23 18:36:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:36:23.678701"
    },
    {
      "arxiv_id": "2411.16740v3",
      "title": "Document Haystacks: Vision-Language Reasoning Over Piles of 1000+ Documents",
      "title_zh": "翻译失败",
      "authors": [
        "Jun Chen",
        "Dannong Xu",
        "Junjie Fei",
        "Chun-Mei Feng",
        "Mohamed Elhoseiny"
      ],
      "abstract": "Large multimodal models (LMMs) have achieved impressive progress in\nvision-language understanding, yet they face limitations in real-world\napplications requiring complex reasoning over a large number of images.\nExisting benchmarks for multi-image question-answering are limited in scope,\neach question is paired with only up to 30 images, which does not fully capture\nthe demands of large-scale retrieval tasks encountered in the real-world\nusages. To reduce these gaps, we introduce two document haystack benchmarks,\ndubbed DocHaystack and InfoHaystack, designed to evaluate LMM performance on\nlarge-scale visual document retrieval and understanding. Additionally, we\npropose V-RAG, a novel, vision-centric retrieval-augmented generation (RAG)\nframework that leverages a suite of multimodal vision encoders, each optimized\nfor specific strengths, and a dedicated question-document relevance module.\nV-RAG sets a new standard, with a 9% and 11% improvement in Recall@1 on the\nchallenging DocHaystack-1000 and InfoHaystack-1000 benchmarks, respectively,\ncompared to the previous best baseline models. Additionally, integrating V-RAG\nwith LMMs enables them to efficiently operate across thousands of images,\nyielding significant improvements on our DocHaystack and InfoHaystack\nbenchmarks. Our code and datasets are available at\nhttps://github.com/Vision-CAIR/dochaystacks",
      "tldr_zh": "本研究指出了大型多模态模型 (LMMs) 在处理大量图像的复杂推理方面存在局限性，并引入了两个新基准 DocHaystack 和 InfoHaystack，用于评估 LMMs 在超过 1000 个文档的视觉文档检索和理解任务中的性能。论文提出了一种新型视觉中心化检索增强生成框架 (V-RAG)，它利用多种多模态视觉编码器和专用的问答相关性模块来提升效率。实验结果显示，V-RAG 在 DocHaystack-1000 和 InfoHaystack-1000 基准上分别将 Recall@1 提高了 9% 和 11%，并显著提升了 LMMs 处理数千张图像的能力。该框架为大规模视觉语言任务提供了新的标准，并附带了开源代码和数据集。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "the correct arxiv version",
      "pdf_url": "http://arxiv.org/pdf/2411.16740v3",
      "published_date": "2024-11-23 18:14:42 UTC",
      "updated_date": "2024-12-06 13:10:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:36:37.401813"
    },
    {
      "arxiv_id": "2411.15600v1",
      "title": "How Texts Help? A Fine-grained Evaluation to Reveal the Role of Language in Vision-Language Tracking",
      "title_zh": "文本如何帮助？ 一种细粒度评估揭示语言在视觉-语言跟踪中的作用",
      "authors": [
        "Xuchen Li",
        "Shiyu Hu",
        "Xiaokun Feng",
        "Dailing Zhang",
        "Meiqi Wu",
        "Jing Zhang",
        "Kaiqi Huang"
      ],
      "abstract": "Vision-language tracking (VLT) extends traditional single object tracking by\nincorporating textual information, providing semantic guidance to enhance\ntracking performance under challenging conditions like fast motion and\ndeformations. However, current VLT trackers often underperform compared to\nsingle-modality methods on multiple benchmarks, with semantic information\nsometimes becoming a \"distraction.\" To address this, we propose VLTVerse, the\nfirst fine-grained evaluation framework for VLT trackers that comprehensively\nconsiders multiple challenge factors and diverse semantic information, hoping\nto reveal the role of language in VLT. Our contributions include: (1) VLTVerse\nintroduces 10 sequence-level challenge labels and 6 types of multi-granularity\nsemantic information, creating a flexible and multi-dimensional evaluation\nspace for VLT; (2) leveraging 60 subspaces formed by combinations of challenge\nfactors and semantic types, we conduct systematic fine-grained evaluations of\nthree mainstream SOTA VLT trackers, uncovering their performance bottlenecks\nacross complex scenarios and offering a novel perspective on VLT evaluation;\n(3) through decoupled analysis of experimental results, we examine the impact\nof various semantic types on specific challenge factors in relation to\ndifferent algorithms, providing essential guidance for enhancing VLT across\ndata, evaluation, and algorithmic dimensions. The VLTVerse, toolkit, and\nresults will be available at \\url{http://metaverse.aitestunion.com}.",
      "tldr_zh": "本研究探讨了视觉语言跟踪 (VLT) 中文本信息的作用，指出现有 VLT 跟踪器在面对快速运动和变形等挑战时，语义信息有时会成为干扰，导致性能不如单模态方法。为此，论文提出 VLTVerse，这是首个细粒度评估框架，涵盖 10 个序列级挑战标签和 6 种多粒度语义信息，构建了一个灵活的多维评估空间。利用 60 个挑战因素和语义类型组合子空间，对三种主流 SOTA VLT 跟踪器进行系统评估，揭示了它们的性能瓶颈，并通过解耦分析考察不同语义类型对特定挑战因素的影响，提供数据、评估和算法层面的改进指导。VLTVerse 框架及其工具将公开，以推动 VLT 领域的进展。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint, Under Review",
      "pdf_url": "http://arxiv.org/pdf/2411.15600v1",
      "published_date": "2024-11-23 16:31:40 UTC",
      "updated_date": "2024-11-23 16:31:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:36:48.706598"
    },
    {
      "arxiv_id": "2411.16739v1",
      "title": "Gradient-Guided Parameter Mask for Multi-Scenario Image Restoration Under Adverse Weather",
      "title_zh": "翻译失败",
      "authors": [
        "Jilong Guo",
        "Haobo Yang",
        "Mo Zhou",
        "Xinyu Zhang"
      ],
      "abstract": "Removing adverse weather conditions such as rain, raindrop, and snow from\nimages is critical for various real-world applications, including autonomous\ndriving, surveillance, and remote sensing. However, existing multi-task\napproaches typically rely on augmenting the model with additional parameters to\nhandle multiple scenarios. While this enables the model to address diverse\ntasks, the introduction of extra parameters significantly complicates its\npractical deployment. In this paper, we propose a novel Gradient-Guided\nParameter Mask for Multi-Scenario Image Restoration under adverse weather,\ndesigned to effectively handle image degradation under diverse weather\nconditions without additional parameters. Our method segments model parameters\ninto common and specific components by evaluating the gradient variation\nintensity during training for each specific weather condition. This enables the\nmodel to precisely and adaptively learn relevant features for each weather\nscenario, improving both efficiency and effectiveness without compromising on\nperformance. This method constructs specific masks based on gradient\nfluctuations to isolate parameters influenced by other tasks, ensuring that the\nmodel achieves strong performance across all scenarios without adding extra\nparameters. We demonstrate the state-of-the-art performance of our framework\nthrough extensive experiments on multiple benchmark datasets. Specifically, our\nmethod achieves PSNR scores of 29.22 on the Raindrop dataset, 30.76 on the Rain\ndataset, and 29.56 on the Snow100K dataset. Code is available at:\n\\href{https://github.com/AierLab/MultiTask}{https://github.com/AierLab/MultiTask}.",
      "tldr_zh": "本论文提出了一种名为 Gradient-Guided Parameter Mask 的新方法，用于多场景图像恢复，旨在去除恶劣天气（如雨、雨滴和雪）对图像的影响，而无需添加额外参数，从而简化模型部署。方法通过评估训练中的梯度变化，将模型参数分为通用和特定组件，并基于梯度波动构建特定掩码，以隔离不同天气任务的影响，提高特征学习效率和准确性。在多个基准数据集上，该框架实现了最先进性能，包括 Raindrop 数据集的 PSNR 得分 29.22、Rain 数据集的 30.76 和 Snow100K 数据集的 29.56，展示了其在图像恢复任务中的显著优势。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.16739v1",
      "published_date": "2024-11-23 16:16:27 UTC",
      "updated_date": "2024-11-23 16:16:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:36:59.852909"
    },
    {
      "arxiv_id": "2411.15595v2",
      "title": "An adversarial feature learning based semantic communication method for Human 3D Reconstruction",
      "title_zh": "一种基于对抗性特征学习的语义通信方法，用于人类3D重建",
      "authors": [
        "Shaojiang Liu",
        "Jiajun Zou",
        "Zhendan Liu",
        "Meixia Dong",
        "Zhiping Wan"
      ],
      "abstract": "With the widespread application of human body 3D reconstruction technology\nacross various fields, the demands for data transmission and processing\nefficiency continue to rise, particularly in scenarios where network bandwidth\nis limited and low latency is required. This paper introduces an Adversarial\nFeature Learning-based Semantic Communication method (AFLSC) for human body 3D\nreconstruction, which focuses on extracting and transmitting semantic\ninformation crucial for the 3D reconstruction task, thereby significantly\noptimizing data flow and alleviating bandwidth pressure. At the sender's end,\nwe propose a multitask learning-based feature extraction method to capture the\nspatial layout, keypoints, posture, and depth information from 2D human images,\nand design a semantic encoding technique based on adversarial feature learning\nto encode these feature information into semantic data. We also develop a\ndynamic compression technique to efficiently transmit this semantic data,\ngreatly enhancing transmission efficiency and reducing latency. At the\nreceiver's end, we design an efficient multi-level semantic feature decoding\nmethod to convert semantic data back into key image features. Finally, an\nimproved ViT-diffusion model is employed for 3D reconstruction, producing human\nbody 3D mesh models. Experimental results validate the advantages of our method\nin terms of data transmission efficiency and reconstruction quality,\ndemonstrating its excellent potential for application in bandwidth-limited\nenvironments.",
      "tldr_zh": "本研究提出了一种基于对抗特征学习(Adversarial Feature Learning)的语义通信方法AFLSC，用于人体3D重建，旨在优化数据传输效率并减少带宽压力。发送端采用多任务学习提取2D图像中的空间布局、关键点、姿势和深度信息，并通过对抗特征学习编码成语义数据，再结合动态压缩技术进行高效传输；接收端则使用多级语义特征解码将数据转换回图像特征，并借助改进的ViT-diffusion模型生成高质量的人体3D网格模型。实验结果显示，该方法在带宽有限环境中显著提升了传输效率和重建质量，证明其在实际应用中的潜力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "It was published to arXiv without consulting the corresponding\n  author, so the corresponding author requested a withdrawal first",
      "pdf_url": "http://arxiv.org/pdf/2411.15595v2",
      "published_date": "2024-11-23 16:09:53 UTC",
      "updated_date": "2024-12-15 14:39:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:37:36.757886"
    },
    {
      "arxiv_id": "2411.15594v5",
      "title": "A Survey on LLM-as-a-Judge",
      "title_zh": "翻译失败",
      "authors": [
        "Jiawei Gu",
        "Xuhui Jiang",
        "Zhichao Shi",
        "Hexiang Tan",
        "Xuehao Zhai",
        "Chengjin Xu",
        "Wei Li",
        "Yinghan Shen",
        "Shengjie Ma",
        "Honghao Liu",
        "Saizhuo Wang",
        "Kun Zhang",
        "Yuanzhuo Wang",
        "Wen Gao",
        "Lionel Ni",
        "Jian Guo"
      ],
      "abstract": "Accurate and consistent evaluation is crucial for decision-making across\nnumerous fields, yet it remains a challenging task due to inherent\nsubjectivity, variability, and scale. Large Language Models (LLMs) have\nachieved remarkable success across diverse domains, leading to the emergence of\n\"LLM-as-a-Judge,\" where LLMs are employed as evaluators for complex tasks. With\ntheir ability to process diverse data types and provide scalable,\ncost-effective, and consistent assessments, LLMs present a compelling\nalternative to traditional expert-driven evaluations. However, ensuring the\nreliability of LLM-as-a-Judge systems remains a significant challenge that\nrequires careful design and standardization. This paper provides a\ncomprehensive survey of LLM-as-a-Judge, addressing the core question: How can\nreliable LLM-as-a-Judge systems be built? We explore strategies to enhance\nreliability, including improving consistency, mitigating biases, and adapting\nto diverse assessment scenarios. Additionally, we propose methodologies for\nevaluating the reliability of LLM-as-a-Judge systems, supported by a novel\nbenchmark designed for this purpose. To advance the development and real-world\ndeployment of LLM-as-a-Judge systems, we also discussed practical applications,\nchallenges, and future directions. This survey serves as a foundational\nreference for researchers and practitioners in this rapidly evolving field.",
      "tldr_zh": "这篇论文对“LLM-as-a-Judge”进行全面调查，探讨了使用大语言模型（LLMs）作为评估者来解决传统评估中的主观性、可变性和规模挑战，提供可扩展、经济和一致的替代方案。论文重点分析了提升系统可靠性的策略，包括提高评估一致性、缓解偏差以及适应多样化场景，并提出评估方法和一个新基准来验证可靠性。最终，论文讨论了LLM-as-a-Judge的实际应用、潜在挑战及未来方向，为研究者和从业者提供基础参考。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Project Page: https://awesome-llm-as-a-judge.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2411.15594v5",
      "published_date": "2024-11-23 16:03:35 UTC",
      "updated_date": "2025-03-09 05:21:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:37:49.659872"
    },
    {
      "arxiv_id": "2411.15589v1",
      "title": "Deep Learning for THz Channel Estimation and Beamforming Prediction via Sub-6GHz Channel",
      "title_zh": "翻译失败",
      "authors": [
        "Sagnik Bhattacharya",
        "Abhishek K. Gupta"
      ],
      "abstract": "An efficient channel estimation is of vital importance to help THz\ncommunication systems achieve their full potential. Conventional uplink channel\nestimation methods, such as least square estimation, are practically\ninefficient for THz systems because of their large computation overhead. In\nthis paper, we propose an efficient convolutional neural network (CNN) based\nTHz channel estimator that estimates the THz channel factors using uplink\nsub-6GHz channel. Further, we use the estimated THz channel factors to predict\nthe optimal beamformer from a pre-given codebook, using a dense neural network.\nWe not only get rid of the overhead associated with the conventional methods,\nbut also achieve near-optimal spectral efficiency rates using the proposed\nbeamformer predictor. The proposed method also outperforms deep learning based\nbeamformer predictors accepting THz channel matrices as input, thus proving the\nvalidity and efficiency of our sub-6GHz based approach.",
      "tldr_zh": "这篇论文针对THz通信系统的信道估计问题，提出了一种基于卷积神经网络(CNN)的估计方法，使用sub-6GHz信道来估计THz信道因素，从而避免了传统方法如最小二乘估计的巨大计算开销。\n随后，该方法利用估计的THz信道因素和一个密集神经网络，从预定义码本中预测最优beamforming。\n实验结果表明，该方法实现了接近最优的频谱效率，并超过了接受THz信道矩阵作为输入的深度学习beamforming预测器，证明了sub-6GHz基于方法的有效性和高效性。",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "Published: 2022 IEEE International Conference on Signal Processing\n  and Communications (SPCOM 2022)",
      "pdf_url": "http://arxiv.org/pdf/2411.15589v1",
      "published_date": "2024-11-23 15:36:35 UTC",
      "updated_date": "2024-11-23 15:36:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:38:01.455769"
    },
    {
      "arxiv_id": "2411.16738v2",
      "title": "Classifier-Free Guidance inside the Attraction Basin May Cause Memorization",
      "title_zh": "吸引盆地内的无分类器引导可能导致记忆化",
      "authors": [
        "Anubhav Jain",
        "Yuya Kobayashi",
        "Takashi Shibuya",
        "Yuhta Takida",
        "Nasir Memon",
        "Julian Togelius",
        "Yuki Mitsufuji"
      ],
      "abstract": "Diffusion models are prone to exactly reproduce images from the training\ndata. This exact reproduction of the training data is concerning as it can lead\nto copyright infringement and/or leakage of privacy-sensitive information. In\nthis paper, we present a novel perspective on the memorization phenomenon and\npropose a simple yet effective approach to mitigate it. We argue that\nmemorization occurs because of an attraction basin in the denoising process\nwhich steers the diffusion trajectory towards a memorized image. However, this\ncan be mitigated by guiding the diffusion trajectory away from the attraction\nbasin by not applying classifier-free guidance until an ideal transition point\noccurs from which classifier-free guidance is applied. This leads to the\ngeneration of non-memorized images that are high in image quality and\nwell-aligned with the conditioning mechanism. To further improve on this, we\npresent a new guidance technique, opposite guidance, that escapes the\nattraction basin sooner in the denoising process. We demonstrate the existence\nof attraction basins in various scenarios in which memorization occurs, and we\nshow that our proposed approach successfully mitigates memorization.",
      "tldr_zh": "本研究探讨了扩散模型（Diffusion models）中的记忆化现象，认为分类器自由指导（classifier-free guidance）在吸引盆（attraction basin）内应用可能导致模型精确复制训练数据，从而引发版权和隐私问题。作者提出一种方法，通过延迟应用classifier-free guidance直到理想过渡点，并引入opposite guidance技术，来引导扩散轨迹远离吸引盆，从而生成高质量、非记忆化的图像。该方法在各种场景下经实验验证，能有效缓解记忆化问题，同时保持图像质量和条件对齐。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2411.16738v2",
      "published_date": "2024-11-23 15:36:03 UTC",
      "updated_date": "2025-03-17 15:50:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:38:13.817592"
    },
    {
      "arxiv_id": "2411.15560v2",
      "title": "Do LLMs Agree on the Creativity Evaluation of Alternative Uses?",
      "title_zh": "LLMs 是否在备选用途的创造力评估上达成一致？",
      "authors": [
        "Abdullah Al Rabeyah",
        "Fabrício Góes",
        "Marco Volpe",
        "Talles Medeiros"
      ],
      "abstract": "This paper investigates whether large language models (LLMs) show agreement\nin assessing creativity in responses to the Alternative Uses Test (AUT). While\nLLMs are increasingly used to evaluate creative content, previous studies have\nprimarily focused on a single model assessing responses generated by the same\nmodel or humans. This paper explores whether LLMs can impartially and\naccurately evaluate creativity in outputs generated by both themselves and\nother models. Using an oracle benchmark set of AUT responses, categorized by\ncreativity level (common, creative, and highly creative), we experiment with\nfour state-of-the-art LLMs evaluating these outputs. We test both scoring and\nranking methods and employ two evaluation settings (comprehensive and\nsegmented) to examine if LLMs agree on the creativity evaluation of alternative\nuses. Results reveal high inter-model agreement, with Spearman correlations\naveraging above 0.7 across models and reaching over 0.77 with respect to the\noracle, indicating a high level of agreement and validating the reliability of\nLLMs in creativity assessment of alternative uses. Notably, models do not\nfavour their own responses, instead they provide similar creativity assessment\nscores or rankings for alternative uses generated by other models. These\nfindings suggest that LLMs exhibit impartiality and high alignment in\ncreativity evaluation, offering promising implications for their use in\nautomated creativity assessment.",
      "tldr_zh": "本研究探讨了大型语言模型（LLMs）在评估替代用途测试（AUT）响应创意时是否表现出一致性和公正性。研究使用一个 oracle 基准数据集，将 AUT 响应分类为常见、创意和高度创意，并通过四种最先进 LLMs 进行评分和排名实验，包括全面和分段评估设置。结果显示，LLMs 之间 Spearman 相关系数平均超过 0.7，与 oracle 基准的相关性高达 0.77，且模型不对自身生成的响应表现出偏好，而是提供类似评估。总体而言，这验证了 LLMs 在创意评估中的可靠性和公正性，为自动化创意评估提供了重要启示。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "19 pages, 7 figures, 15 tables",
      "pdf_url": "http://arxiv.org/pdf/2411.15560v2",
      "published_date": "2024-11-23 13:34:50 UTC",
      "updated_date": "2024-11-26 09:25:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:38:26.154316"
    },
    {
      "arxiv_id": "2411.15557v3",
      "title": "LAGUNA: LAnguage Guided UNsupervised Adaptation with structured spaces",
      "title_zh": "翻译失败",
      "authors": [
        "Anxhelo Diko",
        "Antonino Furnari",
        "Luigi Cinque",
        "Giovanni Maria Farinella"
      ],
      "abstract": "Unsupervised domain adaptation remains a critical challenge in enabling the\nknowledge transfer of models across unseen domains. Existing methods struggle\nto balance the need for domain-invariant representations with preserving\ndomain-specific features, which is often due to alignment approaches that\nimpose the projection of samples with similar semantics close in the latent\nspace despite their drastic domain differences. We introduce LAGUNA - LAnguage\nGuided UNsupervised Adaptation with structured spaces, a novel approach that\nshifts the focus from aligning representations in absolute coordinates to\naligning the relative positioning of equivalent concepts in latent spaces.\nLAGUNA defines a domain-agnostic structure upon the semantic/geometric\nrelationships between class labels in language space and guides adaptation,\nensuring that the organization of samples in visual space reflects reference\ninter-class relationships while preserving domain-specific characteristics. We\nempirically demonstrate LAGUNA's superiority in domain adaptation tasks across\nfour diverse images and video datasets. Remarkably, LAGUNA surpasses previous\nworks in 18 different adaptation scenarios across four diverse image and video\ndatasets with average accuracy improvements of +3.32% on DomainNet, +5.75% in\nGeoPlaces, +4.77% on GeoImnet, and +1.94% mean class accuracy improvement on\nEgoExo4D.",
      "tldr_zh": "该研究提出 LAGUNA，一种基于语言引导的 Unsupervised Domain Adaptation 方法，通过关注潜在空间中概念的相对位置对齐，而不是绝对坐标对齐，从而平衡域不变表示和域特定特征。LAGUNA 在语言空间中定义域无关结构，利用类标签的语义/几何关系来指导视觉空间的组织，确保适应过程保留了样本的独特特性。在四个多样化的图像和视频数据集上，LAGUNA 在 18 个适应场景中表现出色，平均准确率提升包括 DomainNet +3.32%、GeoPlaces +5.75%、GeoImnet +4.77% 和 EgoExo4D +1.94%。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15557v3",
      "published_date": "2024-11-23 13:26:53 UTC",
      "updated_date": "2025-03-27 22:59:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:38:38.057929"
    },
    {
      "arxiv_id": "2411.15556v2",
      "title": "ReWind: Understanding Long Videos with Instructed Learnable Memory",
      "title_zh": "ReWind：利用指令引导的可学习记忆理解长视频",
      "authors": [
        "Anxhelo Diko",
        "Tinghuai Wang",
        "Wassim Swaileh",
        "Shiyan Sun",
        "Ioannis Patras"
      ],
      "abstract": "Vision-Language Models (VLMs) are crucial for applications requiring\nintegrated understanding textual and visual information. However, existing VLMs\nstruggle with long videos due to computational inefficiency, memory\nlimitations, and difficulties in maintaining coherent understanding across\nextended sequences. To address these challenges, we introduce ReWind, a novel\nmemory-based VLM designed for efficient long video understanding while\npreserving temporal fidelity. ReWind operates in a two-stage framework. In the\nfirst stage, ReWind maintains a dynamic learnable memory module with a novel\n\\textbf{read-perceive-write} cycle that stores and updates instruction-relevant\nvisual information as the video unfolds. This module utilizes learnable queries\nand cross-attentions between memory contents and the input stream, ensuring low\nmemory requirements by scaling linearly with the number of tokens. In the\nsecond stage, we propose an adaptive frame selection mechanism guided by the\nmemory content to identify instruction-relevant key moments. It enriches the\nmemory representations with detailed spatial information by selecting a few\nhigh-resolution frames, which are then combined with the memory contents and\nfed into a Large Language Model (LLM) to generate the final answer. We\nempirically demonstrate ReWind's superior performance in visual question\nanswering (VQA) and temporal grounding tasks, surpassing previous methods on\nlong video benchmarks. Notably, ReWind achieves a +13\\% score gain and a +12\\%\naccuracy improvement on the MovieChat-1K VQA dataset and an +8\\% mIoU increase\non Charades-STA for temporal grounding.",
      "tldr_zh": "该研究提出 ReWind，一种基于指令可学习内存的视觉语言模型 (VLMs)，旨在解决现有模型在处理长视频时面临的计算效率低、内存限制和时间连贯性问题。ReWind 采用两阶段框架：第一阶段通过动态的 read-perceive-write 循环和可学习查询机制，存储并更新视频中与指令相关的视觉信息，确保内存需求线性增长。第二阶段利用自适应帧选择机制，根据内存内容识别关键时刻，并结合高分辨率帧与内存表示输入 Large Language Model (LLM) 生成最终答案。实验结果显示，ReWind 在视觉问答 (VQA) 和时间定位任务上表现出色，在 MovieChat-1K VQA 数据集上得分提升 13% 和准确率提高 12%，在 Charades-STA 上 mIoU 增加 8%。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15556v2",
      "published_date": "2024-11-23 13:23:22 UTC",
      "updated_date": "2025-03-27 23:05:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:38:49.578706"
    },
    {
      "arxiv_id": "2411.15550v1",
      "title": "Class Order Disorder in Wikidata and First Fixes",
      "title_zh": "翻译失败",
      "authors": [
        "Peter F. Patel-Schneider",
        "Ege Atacan Doğan"
      ],
      "abstract": "Wikidata has a large ontology with classes at several orders. The Wikidata\nontology has long been known to have violations of class order and information\nrelated to class order that appears suspect. SPARQL queries were evaluated\nagainst Wikidata to determine the prevalence of several kinds of violations and\nsuspect information and the results analyzed. Some changes were manually made\nto Wikidata to remove some of these results and the queries rerun, showing the\neffect of the changes. Suggestions are provided on how the problems uncovered\nmight be addressed, either though better tooling or involvement of the Wikidata\ncommunity.",
      "tldr_zh": "这篇论文调查了 Wikidata 中的类顺序(class order)混乱问题，包括违反和可疑信息，使用 SPARQL 查询评估了这些问题的普遍性，并对结果进行了分析。研究者手动对 Wikidata 进行了部分修复，然后重新运行查询，以展示变化的效果。论文还提出了改进建议，如通过更好的工具或 Wikidata 社区参与来解决这些本体(ontology)问题。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "I.2.4"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15550v1",
      "published_date": "2024-11-23 13:15:13 UTC",
      "updated_date": "2024-11-23 13:15:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:39:00.546907"
    },
    {
      "arxiv_id": "2411.15548v2",
      "title": "An unconditional distribution learning advantage with shallow quantum circuits",
      "title_zh": "浅层量子电路的无条件分布学习优势",
      "authors": [
        "N. Pirnay",
        "S. Jerbi",
        "J. -P. Seifert",
        "J. Eisert"
      ],
      "abstract": "One of the core challenges of research in quantum computing is concerned with\nthe question whether quantum advantages can be found for near-term quantum\ncircuits that have implications for practical applications. Motivated by this\nmindset, in this work, we prove an unconditional quantum advantage in the\nprobably approximately correct (PAC) distribution learning framework with\nshallow quantum circuit hypotheses. We identify a meaningful generative\ndistribution learning problem where constant-depth quantum circuits using one\nand two qubit gates (QNC^0) are superior compared to constant-depth bounded\nfan-in classical circuits (NC^0) as a choice for hypothesis classes. We hence\nprove a PAC distribution learning separation for shallow quantum circuits over\nshallow classical circuits. We do so by building on recent results by Bene\nWatts and Parham on unconditional quantum advantages for sampling tasks with\nshallow circuits, which we technically uplift to a hyperplane learning problem,\nidentifying non-local correlations as the origin of the quantum advantage.",
      "tldr_zh": "本论文证明了浅层量子电路在 PAC（probably approximately correct）分布学习框架中存在无条件量子优势，具体展示了 QNC^0（常量深度量子电路）在生成分布学习任务上优于 NC^0（常量深度经典电路）。研究者通过构建一个超平面学习问题，并基于 Bene Watts 和 Parham 的先前工作，将量子优势归因于非局部相关（non-local correlations）。这一发现为近中期量子计算在实际应用中的优势提供了理论基础。",
      "categories": [
        "quant-ph",
        "cs.AI"
      ],
      "primary_category": "quant-ph",
      "comment": "7 + 5 pages, 2 figures, added an acknowledgement",
      "pdf_url": "http://arxiv.org/pdf/2411.15548v2",
      "published_date": "2024-11-23 13:03:22 UTC",
      "updated_date": "2024-11-27 02:44:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:39:13.330887"
    },
    {
      "arxiv_id": "2411.16736v1",
      "title": "ChemSafetyBench: Benchmarking LLM Safety on Chemistry Domain",
      "title_zh": "翻译失败",
      "authors": [
        "Haochen Zhao",
        "Xiangru Tang",
        "Ziran Yang",
        "Xiao Han",
        "Xuanzhi Feng",
        "Yueqing Fan",
        "Senhao Cheng",
        "Di Jin",
        "Yilun Zhao",
        "Arman Cohan",
        "Mark Gerstein"
      ],
      "abstract": "The advancement and extensive application of large language models (LLMs)\nhave been remarkable, including their use in scientific research assistance.\nHowever, these models often generate scientifically incorrect or unsafe\nresponses, and in some cases, they may encourage users to engage in dangerous\nbehavior. To address this issue in the field of chemistry, we introduce\nChemSafetyBench, a benchmark designed to evaluate the accuracy and safety of\nLLM responses. ChemSafetyBench encompasses three key tasks: querying chemical\nproperties, assessing the legality of chemical uses, and describing synthesis\nmethods, each requiring increasingly deeper chemical knowledge. Our dataset has\nmore than 30K samples across various chemical materials. We incorporate\nhandcrafted templates and advanced jailbreaking scenarios to enhance task\ndiversity. Our automated evaluation framework thoroughly assesses the safety,\naccuracy, and appropriateness of LLM responses. Extensive experiments with\nstate-of-the-art LLMs reveal notable strengths and critical vulnerabilities,\nunderscoring the need for robust safety measures. ChemSafetyBench aims to be a\npivotal tool in developing safer AI technologies in chemistry. Our code and\ndataset are available at https://github.com/HaochenZhao/SafeAgent4Chem.\nWarning: this paper contains discussions on the synthesis of controlled\nchemicals using AI models.",
      "tldr_zh": "本文引入 ChemSafetyBench 基准，用于评估大型语言模型 (LLMs) 在化学领域的安全性和准确性，针对 LLMs 可能生成不正确或危险响应的风险。基准涵盖查询化学属性、评估化学用途合法性和描述合成方法等三项任务，数据集超过 30K 样本，并通过手工模板和 jailbreaking 场景增强任务多样性，同时采用自动评估框架检查响应安全性、准确性和适当性。实验结果显示现有 LLMs 存在显著优势和漏洞，强调了加强安全措施的必要性，并为开发更可靠的化学 AI 技术提供关键工具。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "physics.chem-ph"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.16736v1",
      "published_date": "2024-11-23 12:50:33 UTC",
      "updated_date": "2024-11-23 12:50:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:40:52.274633"
    },
    {
      "arxiv_id": "2411.15540v2",
      "title": "Optical-Flow Guided Prompt Optimization for Coherent Video Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Hyelin Nam",
        "Jaemin Kim",
        "Dohun Lee",
        "Jong Chul Ye"
      ],
      "abstract": "While text-to-video diffusion models have made significant strides, many\nstill face challenges in generating videos with temporal consistency. Within\ndiffusion frameworks, guidance techniques have proven effective in enhancing\noutput quality during inference; however, applying these methods to video\ndiffusion models introduces additional complexity of handling computations\nacross entire sequences. To address this, we propose a novel framework called\nMotionPrompt that guides the video generation process via optical flow.\nSpecifically, we train a discriminator to distinguish optical flow between\nrandom pairs of frames from real videos and generated ones. Given that prompts\ncan influence the entire video, we optimize learnable token embeddings during\nreverse sampling steps by using gradients from a trained discriminator applied\nto random frame pairs. This approach allows our method to generate visually\ncoherent video sequences that closely reflect natural motion dynamics, without\ncompromising the fidelity of the generated content. We demonstrate the\neffectiveness of our approach across various models.",
      "tldr_zh": "本研究针对文本到视频扩散模型在生成时间一致性视频时的挑战，提出了一种名为 MotionPrompt 的新框架，利用 optical flow 进行指导优化。框架中训练一个鉴别器来区分真实视频和生成视频中随机帧对之间的 optical flow，从而在反向采样步骤中使用鉴别器的梯度优化可学习的 token embeddings。这种方法能够生成视觉上更连贯的视频序列，同时保持内容的保真度。实验结果显示，该框架在各种模型上均表现出色，提升了视频生成的整体质量。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2025 (poster); project page: https://motionprompt.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2411.15540v2",
      "published_date": "2024-11-23 12:26:52 UTC",
      "updated_date": "2025-03-23 07:34:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:41:02.798694"
    },
    {
      "arxiv_id": "2411.15539v2",
      "title": "Large Language Model with Region-guided Referring and Grounding for CT Report Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Zhixuan Chen",
        "Yequan Bie",
        "Haibo Jin",
        "Hao Chen"
      ],
      "abstract": "Computed tomography (CT) report generation is crucial to assist radiologists\nin interpreting CT volumes, which can be time-consuming and labor-intensive.\nExisting methods primarily only consider the global features of the entire\nvolume, making it struggle to focus on specific regions and potentially missing\nabnormalities. To address this issue, we propose Reg2RG, the first\nregion-guided referring and grounding framework for CT report generation, which\nenhances diagnostic performance by focusing on anatomical regions within the\nvolume. Specifically, we utilize masks from a universal segmentation module to\ncapture local features for each referring region. A local feature decoupling\n(LFD) strategy is proposed to preserve the local high-resolution details with\nlittle computational overhead. Then the local features are integrated with\nglobal features to capture inter-regional relationships within a cohesive\ncontext. Moreover, we propose a novel region-report alignment (RRA) training\nstrategy. It leverages the recognition of referring regions to guide the\ngeneration of region-specific reports, enhancing the model's referring and\ngrounding capabilities while also improving the report's interpretability. A\nlarge language model (LLM) is further employed as the language decoder to\ngenerate reports from integrated visual features, facilitating region-level\ncomprehension. Extensive experiments on two large-scale chest CT-report\ndatasets demonstrate the superiority of our method, which outperforms several\nstate-of-the-art methods in terms of both natural language generation and\nclinical efficacy metrics while preserving promising interpretability. The code\nis available at https://github.com/zhi-xuan-chen/Reg2RG.",
      "tldr_zh": "本论文提出 Reg2RG 框架，这是首个 region-guided referring and grounding 方法，用于提升 CT 报告生成任务的诊断性能，解决现有方法仅关注全局特征而忽略特定区域异常的问题。具体而言，该框架利用 masks 从通用分割模块提取局部特征，并引入 local feature decoupling (LFD) 策略保留高分辨率细节，同时通过 region-report alignment (RRA) 训练策略整合局部和全局特征，提高报告的可解释性和区域间关系捕捉。最后，使用 large language model (LLM) 作为语言解码器生成报告，实验在两个大规模胸部 CT 报告数据集上显示，该方法在自然语言生成和临床效能指标上优于现有技术。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages",
      "pdf_url": "http://arxiv.org/pdf/2411.15539v2",
      "published_date": "2024-11-23 12:25:06 UTC",
      "updated_date": "2025-05-05 02:18:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:41:17.409905"
    },
    {
      "arxiv_id": "2411.15523v1",
      "title": "Enhancing Grammatical Error Detection using BERT with Cleaned Lang-8 Dataset",
      "title_zh": "翻译失败",
      "authors": [
        "Rahul Nihalani",
        "Kushal Shah"
      ],
      "abstract": "This paper presents an improved LLM based model for Grammatical Error\nDetection (GED), which is a very challenging and equally important problem for\nmany applications. The traditional approach to GED involved hand-designed\nfeatures, but recently, Neural Networks (NN) have automated the discovery of\nthese features, improving performance in GED. Traditional rule-based systems\nhave an F1 score of 0.50-0.60 and earlier machine learning models give an F1\nscore of 0.65-0.75, including decision trees and simple neural networks.\nPrevious deep learning models, for example, Bi-LSTM, have reported F1 scores\nwithin the range from 0.80 to 0.90. In our study, we have fine-tuned various\ntransformer models using the Lang8 dataset rigorously cleaned by us. In our\nexperiments, the BERT-base-uncased model gave an impressive performance with an\nF1 score of 0.91 and accuracy of 98.49% on training data and 90.53% on testing\ndata, also showcasing the importance of data cleaning. Increasing model size\nusing BERT-large-uncased or RoBERTa-large did not give any noticeable\nimprovements in performance or advantage for this task, underscoring that\nlarger models are not always better. Our results clearly show how far rigorous\ndata cleaning and simple transformer-based models can go toward significantly\nimproving the quality of GED.",
      "tldr_zh": "本研究旨在提升语法错误检测(GED)的性能，通过使用清理过的Lang-8数据集对BERT等Transformer模型进行微调。相比传统规则系统(F1分数0.50-0.60)和早期机器学习模型(F1分数0.65-0.75)，BERT-base-uncased模型在实验中实现了F1分数0.91、训练准确率98.49%和测试准确率90.53%。结果表明，数据清理对模型性能提升至关重要，而增大模型如BERT-large-uncased或RoBERTa-large并未带来显著改善。该工作突出了简单Transformer模型在GED任务中的高效潜力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, 6 tables, 20 references",
      "pdf_url": "http://arxiv.org/pdf/2411.15523v1",
      "published_date": "2024-11-23 10:57:41 UTC",
      "updated_date": "2024-11-23 10:57:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:41:27.882666"
    },
    {
      "arxiv_id": "2411.15509v1",
      "title": "Interactive Visual Assessment for Text-to-Image Generation Models",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaoyue Mi",
        "Fan Tang",
        "Juan Cao",
        "Qiang Sheng",
        "Ziyao Huang",
        "Peng Li",
        "Yang Liu",
        "Tong-Yee Lee"
      ],
      "abstract": "Visual generation models have achieved remarkable progress in computer\ngraphics applications but still face significant challenges in real-world\ndeployment. Current assessment approaches for visual generation tasks typically\nfollow an isolated three-phase framework: test input collection, model output\ngeneration, and user assessment. These fashions suffer from fixed coverage,\nevolving difficulty, and data leakage risks, limiting their effectiveness in\ncomprehensively evaluating increasingly complex generation models. To address\nthese limitations, we propose DyEval, an LLM-powered dynamic interactive visual\nassessment framework that facilitates collaborative evaluation between humans\nand generative models for text-to-image systems. DyEval features an intuitive\nvisual interface that enables users to interactively explore and analyze model\nbehaviors, while adaptively generating hierarchical, fine-grained, and diverse\ntextual inputs to continuously probe the capability boundaries of the models\nbased on their feedback. Additionally, to provide interpretable analysis for\nusers to further improve tested models, we develop a contextual reflection\nmodule that mines failure triggers of test inputs and reflects model potential\nfailure patterns supporting in-depth analysis using the logical reasoning\nability of LLM. Qualitative and quantitative experiments demonstrate that\nDyEval can effectively help users identify max up to 2.56 times generation\nfailures than conventional methods, and uncover complex and rare failure\npatterns, such as issues with pronoun generation and specific cultural context\ngeneration. Our framework provides valuable insights for improving generative\nmodels and has broad implications for advancing the reliability and\ncapabilities of visual generation systems across various domains.",
      "tldr_zh": "该论文提出 DyEval，一种基于 LLM 的动态交互式视觉评估框架，用于评估文本到图像生成模型，以解决传统评估方法（如固定覆盖范围和数据泄露风险）的局限性。DyEval 通过直观的视觉界面和上下文反射模块，允许用户与模型协作交互式地生成分层、多样化的测试输入，并利用 LLM 的逻辑推理能力分析模型的失败模式，如代词生成和文化背景问题。实验结果显示，DyEval 比传统方法多识别出高达 2.56 倍的生成失败，提供宝贵见解以提升生成模型的可靠性和整体能力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Under Review",
      "pdf_url": "http://arxiv.org/pdf/2411.15509v1",
      "published_date": "2024-11-23 10:06:18 UTC",
      "updated_date": "2024-11-23 10:06:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:41:39.978387"
    },
    {
      "arxiv_id": "2411.15501v1",
      "title": "Instruct or Interact? Exploring and Eliciting LLMs' Capability in Code Snippet Adaptation Through Prompt Engineering",
      "title_zh": "指令还是互动？通过提示工程探索和激发 LLMs 在代码片段适配方面的能力",
      "authors": [
        "Tanghaoran Zhang",
        "Yue Yu",
        "Xinjun Mao",
        "Shangwen Wang",
        "Kang Yang",
        "Yao Lu",
        "Zhang Zhang",
        "Yuxin Zhao"
      ],
      "abstract": "Code snippet adaptation is a fundamental activity in the software development\nprocess. Unlike code generation, code snippet adaptation is not a \"free\ncreation\", which requires developers to tailor a given code snippet in order to\nfit specific requirements and the code context. Recently, large language models\n(LLMs) have confirmed their effectiveness in the code generation task with\npromising results. However, their performance on adaptation, a reuse-oriented\nand context-dependent code change prediction task, is still unclear. To bridge\nthis gap, we conduct an empirical study to investigate the performance and\nissues of LLMs on the adaptation task. We first evaluate the adaptation\nperformances of three popular LLMs and compare them to the code generation\ntask. Our result indicates that their adaptation ability is weaker than\ngeneration, with a nearly 15% decrease on pass@1 and more context-related\nerrors. By manually inspecting 200 cases, we further investigate the causes of\nLLMs' sub-optimal performance, which can be classified into three categories,\ni.e., Unclear Requirement, Requirement Misalignment and Context Misapplication.\nBased on the above empirical research, we propose an interactive prompting\napproach to eliciting LLMs' adaptation ability. Experimental result reveals\nthat our approach greatly improve LLMs' adaptation performance. The\nbest-performing Human-LLM interaction successfully solves 159 out of the 202\nidentified defects and improves the pass@1 and pass@5 by over 40% compared to\nthe initial instruction-based prompt. Considering human efforts, we suggest\nmulti-agent interaction as a trade-off, which can achieve comparable\nperformance with excellent generalization ability. We deem that our approach\ncould provide methodological assistance for autonomous code snippet reuse and\nadaptation with LLMs.",
      "tldr_zh": "这篇论文探讨了大型语言模型（LLMs）在代码片段适配任务中的性能问题，与代码生成任务相比，结果显示LLMs的适配能力较弱，pass@1下降近15%，并存在更多与上下文相关的错误，如Unclear Requirement、Requirement Misalignment和Context Misapplication。作者通过对200个案例的手动分析，识别了这些问题原因，并提出了一种交互式prompt engineering方法来提升LLMs的适配能力。实验结果表明，该方法显著改善了性能，最好的Human-LLM交互解决了159个缺陷，并将pass@1和pass@5提高了40%以上。作为权衡方案，论文建议采用多智能体交互，以实现类似效果并增强泛化能力，最终为自主代码片段重用和适配提供方法论支持。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.SE",
      "comment": "12 pages, 10 figures, accepted by ICSE 2025",
      "pdf_url": "http://arxiv.org/pdf/2411.15501v1",
      "published_date": "2024-11-23 09:40:36 UTC",
      "updated_date": "2024-11-23 09:40:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:41:54.076756"
    },
    {
      "arxiv_id": "2411.16730v4",
      "title": "\"Moralized\" Multi-Step Jailbreak Prompts: Black-Box Testing of Guardrails in Large Language Models for Verbal Attacks",
      "title_zh": "翻译失败",
      "authors": [
        "Libo Wang"
      ],
      "abstract": "As the application of large language models continues to expand in various\nfields, it poses higher challenges to the effectiveness of identifying harmful\ncontent generation and guardrail mechanisms. This research aims to evaluate the\nguardrail effectiveness of GPT-4o, Grok-2 Beta, Llama 3.1 (405B), Gemini 1.5,\nand Claude 3.5 Sonnet through black-box testing of seemingly ethical multi-step\njailbreak prompts. It conducts ethical attacks by designing an identical\nmulti-step prompts that simulates the scenario of \"corporate middle managers\ncompeting for promotions.\" The data results show that the guardrails of the\nabove-mentioned LLMs were bypassed and the content of verbal attacks was\ngenerated. Claude 3.5 Sonnet's resistance to multi-step jailbreak prompts is\nmore obvious. To ensure objectivity, the experimental process, black box test\ncode, and enhanced guardrail code are uploaded to the GitHub repository:\nhttps://github.com/brucewang123456789/GeniusTrail.git.",
      "tldr_zh": "本研究通过黑盒测试评估了大型语言模型(LLMs)如 GPT-4o、Grok-2 Beta、Llama 3.1 (405B)、Gemini 1.5 和 Claude 3.5 Sonnet 的防护机制(guardrails)，焦点是针对看似道德的多步越狱提示(jailbreak prompts)是否会导致 verbal attacks 生成。研究设计了一个模拟“公司中层管理人员竞争晋升”场景的统一多步提示进行道德攻击测试，结果显示所有测试模型的防护均被绕过，生成有害内容，其中 Claude 3.5 Sonnet 表现出更强的抵抗力。为确保客观性，实验过程、测试代码和增强防护代码已开源在 GitHub 仓库（https://github.com/brucewang123456789/GeniusTrail.git）。这为提升 LLMs 的安全性和可靠性提供了重要洞见。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "This paper has been submitted to Nature Machine Intelligence and\n  OpenReview preprints. It has 7 pages of text, 3 figures, and 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2411.16730v4",
      "published_date": "2024-11-23 09:32:44 UTC",
      "updated_date": "2025-03-20 14:48:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:42:05.717075"
    },
    {
      "arxiv_id": "2411.15488v1",
      "title": "Automatic Evaluation for Text-to-image Generation: Task-decomposed Framework, Distilled Training, and Meta-evaluation Benchmark",
      "title_zh": "文本到图像生成的自动评估",
      "authors": [
        "Rong-Cheng Tu",
        "Zi-Ao Ma",
        "Tian Lan",
        "Yuehao Zhao",
        "Heyan Huang",
        "Xian-Ling Mao"
      ],
      "abstract": "Driven by the remarkable progress in diffusion models, text-to-image\ngeneration has made significant strides, creating a pressing demand for\nautomatic quality evaluation of generated images. Current state-of-the-art\nautomatic evaluation methods heavily rely on Multi-modal Large Language Models\n(MLLMs), particularly powerful commercial models like GPT-4o. While these\nmodels are highly effective, their substantial costs limit scalability in\nlarge-scale evaluations. Adopting open-source MLLMs is an alternative; however,\ntheir performance falls short due to significant limitations in processing\nmulti-modal data compared to commercial MLLMs. To tackle these problems, we\nfirst propose a task decomposition evaluation framework based on GPT-4o to\nautomatically construct a new training dataset, where the complex evaluation\ntask is decoupled into simpler sub-tasks, effectively reducing the learning\ncomplexity. Based on this dataset, we design innovative training strategies to\neffectively distill GPT-4o's evaluation capabilities into a 7B open-source\nMLLM, MiniCPM-V-2.6. Furthermore, to reliably and comprehensively assess prior\nworks and our proposed model, we manually annotate a meta-evaluation benchmark\nthat includes chain-of-thought explanations alongside quality scores for\ngenerated images. Experimental results demonstrate that our distilled\nopen-source MLLM significantly outperforms the current state-of-the-art\nGPT-4o-base baseline, VIEScore, with over 4.6\\% improvement in Spearman and\nKendall correlations with human judgments.",
      "tldr_zh": "这篇论文针对文本到图像生成的自动评估问题，提出了一种任务分解框架和蒸馏训练方法，以减少对昂贵商业 Multi-modal Large Language Models (MLLMs) 如 GPT-4o 的依赖。具体而言，他们使用 GPT-4o 自动构建训练数据集，将复杂评估任务分解为简单子任务，并将评估能力蒸馏到开放源 MLLM MiniCPM-V-2.6。此外，他们创建了一个手动标注的 meta-evaluation benchmark，包括思维链解释和质量分数，实验结果显示蒸馏模型在 Spearman 和 Kendall correlations 与人类判断的相关性上比基线 VIEScore 提高了 4.6%。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15488v1",
      "published_date": "2024-11-23 08:06:06 UTC",
      "updated_date": "2024-11-23 08:06:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:42:16.683060"
    },
    {
      "arxiv_id": "2411.16729v1",
      "title": "DiM-Gestor: Co-Speech Gesture Generation with Adaptive Layer Normalization Mamba-2",
      "title_zh": "翻译失败",
      "authors": [
        "Fan Zhang",
        "Siyuan Zhao",
        "Naye Ji",
        "Zhaohan Wang",
        "Jingmei Wu",
        "Fuxing Gao",
        "Zhenqing Ye",
        "Leyao Yan",
        "Lanxin Dai",
        "Weidong Geng",
        "Xin Lyu",
        "Bozuo Zhao",
        "Dingguo Yu",
        "Hui Du",
        "Bin Hu"
      ],
      "abstract": "Speech-driven gesture generation using transformer-based generative models\nrepresents a rapidly advancing area within virtual human creation. However,\nexisting models face significant challenges due to their quadratic time and\nspace complexities, limiting scalability and efficiency. To address these\nlimitations, we introduce DiM-Gestor, an innovative end-to-end generative model\nleveraging the Mamba-2 architecture. DiM-Gestor features a dual-component\nframework: (1) a fuzzy feature extractor and (2) a speech-to-gesture mapping\nmodule, both built on the Mamba-2. The fuzzy feature extractor, integrated with\na Chinese Pre-trained Model and Mamba-2, autonomously extracts implicit,\ncontinuous speech features. These features are synthesized into a unified\nlatent representation and then processed by the speech-to-gesture mapping\nmodule. This module employs an Adaptive Layer Normalization (AdaLN)-enhanced\nMamba-2 mechanism to uniformly apply transformations across all sequence\ntokens. This enables precise modeling of the nuanced interplay between speech\nfeatures and gesture dynamics. We utilize a diffusion model to train and infer\ndiverse gesture outputs. Extensive subjective and objective evaluations\nconducted on the newly released Chinese Co-Speech Gestures dataset corroborate\nthe efficacy of our proposed model. Compared with Transformer-based\narchitecture, the assessments reveal that our approach delivers competitive\nresults and significantly reduces memory usage, approximately 2.4 times, and\nenhances inference speeds by 2 to 4 times. Additionally, we released the CCG\ndataset, a Chinese Co-Speech Gestures dataset, comprising 15.97 hours (six\nstyles across five scenarios) of 3D full-body skeleton gesture motion performed\nby professional Chinese TV broadcasters.",
      "tldr_zh": "本研究提出 DiM-Gestor，一种基于 Mamba-2 架构的端到端模型，用于语音驱动的手势生成，旨在解决 Transformer 模型的二次时间和空间复杂度问题。\n该模型包括模糊特征提取器（结合 Chinese Pre-trained Model 和 Mamba-2 提取隐式语音特征）和 Adaptive Layer Normalization (AdaLN) 增强的语音到手势映射模块，并利用扩散模型生成多样化的手势输出。\n在新发布的 Chinese Co-Speech Gestures (CCG) 数据集上进行评估，结果显示 DiM-Gestor 与 Transformer 相比，内存使用减少约 2.4 倍，推理速度提高 2-4 倍，同时该数据集提供了 15.97 小时的中文语音手势数据，支持进一步研究。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.GR",
        "cs.HC",
        "cs.MM",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "13 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.16729v1",
      "published_date": "2024-11-23 08:02:03 UTC",
      "updated_date": "2024-11-23 08:02:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:42:28.684062"
    },
    {
      "arxiv_id": "2411.16728v1",
      "title": "Maximizing the Impact of Deep Learning on Subseasonal-to-Seasonal Climate Forecasting: The Essential Role of Optimization",
      "title_zh": "最大化深度学习对亚季节到季节气候预报的影响：优化的关键作用",
      "authors": [
        "Yizhen Guo",
        "Tian Zhou",
        "Wanyi Jiang",
        "Bo Wu",
        "Liang Sun",
        "Rong Jin"
      ],
      "abstract": "Weather and climate forecasting is vital for sectors such as agriculture and\ndisaster management. Although numerical weather prediction (NWP) systems have\nadvanced, forecasting at the subseasonal-to-seasonal (S2S) scale, spanning 2 to\n6 weeks, remains challenging due to the chaotic and sparse atmospheric signals\nat this interval. Even state-of-the-art deep learning models struggle to\noutperform simple climatology models in this domain. This paper identifies that\noptimization, instead of network structure, could be the root cause of this\nperformance gap, and then we develop a novel multi-stage optimization strategy\nto close the gap. Extensive empirical studies demonstrate that our multi-stage\noptimization approach significantly improves key skill metrics, PCC and TCC,\nwhile utilizing the same backbone structure, surpassing the state-of-the-art\nNWP systems (ECMWF-S2S) by over \\textbf{19-91\\%}. Our research contests the\nrecent study that direct forecasting outperforms rolling forecasting for S2S\ntasks. Through theoretical analysis, we propose that the underperformance of\nrolling forecasting may arise from the accumulation of Jacobian matrix products\nduring training. Our multi-stage framework can be viewed as a form of teacher\nforcing to address this issue. Code is available at\n\\url{https://anonymous.4open.science/r/Baguan-S2S-23E7/}",
      "tldr_zh": "本研究探讨了深度学习在亚季节到季节（S2S）气候预测中的潜力，指出优化问题而非网络结构是导致其性能落后于简单气候模型的主要原因。论文提出了一种新型多阶段优化策略，通过实证研究显著提升了关键指标 PCC 和 TCC，利用相同骨干网络，使其性能超过最先进的 NWP 系统（如 ECMWF-S2S）19-91%。此外，该研究质疑直接预测优于滚动预测的观点，并通过理论分析表明滚动预测的不足可能源于训练过程中的 Jacobian 矩阵累积问题，并将多阶段框架视为一种 teacher forcing 解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.ao-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.16728v1",
      "published_date": "2024-11-23 08:01:54 UTC",
      "updated_date": "2024-11-23 08:01:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:42:40.023293"
    },
    {
      "arxiv_id": "2411.15477v1",
      "title": "Towards Robust Evaluation of Unlearning in LLMs via Data Transformations",
      "title_zh": "翻译失败",
      "authors": [
        "Abhinav Joshi",
        "Shaswati Saha",
        "Divyaksh Shukla",
        "Sriram Vema",
        "Harsh Jhamtani",
        "Manas Gaur",
        "Ashutosh Modi"
      ],
      "abstract": "Large Language Models (LLMs) have shown to be a great success in a wide range\nof applications ranging from regular NLP-based use cases to AI agents. LLMs\nhave been trained on a vast corpus of texts from various sources; despite the\nbest efforts during the data pre-processing stage while training the LLMs, they\nmay pick some undesirable information such as personally identifiable\ninformation (PII). Consequently, in recent times research in the area of\nMachine Unlearning (MUL) has become active, the main idea is to force LLMs to\nforget (unlearn) certain information (e.g., PII) without suffering from\nperformance loss on regular tasks. In this work, we examine the robustness of\nthe existing MUL techniques for their ability to enable leakage-proof\nforgetting in LLMs. In particular, we examine the effect of data transformation\non forgetting, i.e., is an unlearned LLM able to recall forgotten information\nif there is a change in the format of the input? Our findings on the TOFU\ndataset highlight the necessity of using diverse data formats to quantify\nunlearning in LLMs more reliably.",
      "tldr_zh": "本研究探讨了大型语言模型（LLMs）在训练过程中可能保留不想要的信息（如 personally identifiable information, PII），并评估机器无学习（Machine Unlearning, MUL）技术的稳健性，以确保LLMs在忘记特定信息时不会出现泄露。论文通过数据转换方法测试了已无学习模型的鲁棒性，即检查模型是否能在输入格式改变后回忆被遗忘的信息。实验基于TOFU数据集，结果表明，使用多样化的数据格式是更可靠地量化LLMs无学习效果的关键，从而为改进MUL技术提供了重要指导。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at EMNLP 2024 Findings; 21 pages (5 page main content +\n  references + appendix)",
      "pdf_url": "http://arxiv.org/pdf/2411.15477v1",
      "published_date": "2024-11-23 07:20:36 UTC",
      "updated_date": "2024-11-23 07:20:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:42:51.116469"
    },
    {
      "arxiv_id": "2411.15472v2",
      "title": "KinMo: Kinematic-aware Human Motion Understanding and Generation",
      "title_zh": "KinMo：运动学感知的人类动作理解和生成",
      "authors": [
        "Pengfei Zhang",
        "Pinxin Liu",
        "Hyeongwoo Kim",
        "Pablo Garrido",
        "Bindita Chaudhuri"
      ],
      "abstract": "Current human motion synthesis frameworks rely on global action descriptions,\ncreating a modality gap that limits both motion understanding and generation\ncapabilities. A single coarse description, such as ``run\", fails to capture\ndetails like variations in speed, limb positioning, and kinematic dynamics,\nleading to ambiguities between text and motion modalities. To address this\nchallenge, we introduce \\textbf{KinMo}, a unified framework built on a\nhierarchical describable motion representation that extends beyond global\naction by incorporating kinematic group movements and their interactions. We\ndesign an automated annotation pipeline to generate high-quality, fine-grained\ndescriptions for this decomposition, resulting in the KinMo dataset. To\nleverage these structured descriptions, we propose Hierarchical Text-Motion\nAlignment, improving spatial understanding by integrating additional motion\ndetails. Furthermore, we introduce a coarse-to-fine generation procedure to\nleverage enhanced spatial understanding to improve motion synthesis.\nExperimental results show that KinMo significantly improves motion\nunderstanding, demonstrated by enhanced text-motion retrieval performance and\nenabling more fine-grained motion generation and editing capabilities. Project\nPage: https://andypinxinliu.github.io/KinMo",
      "tldr_zh": "该论文指出，现有人类动作合成框架依赖于全局动作描述（如“run”），导致文本和动作模态间的差距，难以捕捉速度、肢体位置和运动学动态的细节。作者引入KinMo框架，该框架基于分层可描述的动作表示，扩展了全局动作以包含运动学组动作及其互动，并通过自动标注管道创建了KinMo数据集。论文提出Hierarchical Text-Motion Alignment和粗到细生成过程，提升了动作理解和合成能力，实验结果显示KinMo显著改善文本-动作检索性能，并实现了更细粒度的动作生成和编辑。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15472v2",
      "published_date": "2024-11-23 06:50:11 UTC",
      "updated_date": "2025-03-11 14:29:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:43:03.120777"
    },
    {
      "arxiv_id": "2412.00044v1",
      "title": "Creating Hierarchical Dispositions of Needs in an Agent",
      "title_zh": "翻译失败",
      "authors": [
        "Tofara Moyo"
      ],
      "abstract": "We present a novel method for learning hierarchical abstractions that\nprioritize competing objectives, leading to improved global expected rewards.\nOur approach employs a secondary rewarding agent with multiple scalar outputs,\neach associated with a distinct level of abstraction. The traditional agent\nthen learns to maximize these outputs in a hierarchical manner, conditioning\neach level on the maximization of the preceding level. We derive an equation\nthat orders these scalar values and the global reward by priority, inducing a\nhierarchy of needs that informs goal formation. Experimental results on the\nPendulum v1 environment demonstrate superior performance compared to a baseline\nimplementation.We achieved state of the art results.",
      "tldr_zh": "本研究提出了一种新方法，用于在代理中创建分层需求处置（Hierarchical Dispositions of Needs），以优先处理竞争目标并提升全局预期奖励（global expected rewards）。该方法利用一个次要奖励代理（secondary rewarding agent）生成多个标量输出，每个对应不同抽象级别，然后让传统代理按层次方式最大化这些输出，每个级别基于前一级的最大化，并推导方程来排序这些值和全局奖励。实验结果在 Pendulum v1 环境上显示，该方法比基线实现表现出色，并达到了 state of the art 结果。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "5 pages",
      "pdf_url": "http://arxiv.org/pdf/2412.00044v1",
      "published_date": "2024-11-23 06:41:54 UTC",
      "updated_date": "2024-11-23 06:41:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:43:15.883069"
    },
    {
      "arxiv_id": "2411.15470v1",
      "title": "A Preliminary Study of Multilingual Code Language Models for Code Generation Task Using Translated Benchmarks",
      "title_zh": "翻译失败",
      "authors": [
        "Rohit Dandamudi",
        "Gema Rodríguez-Pérez"
      ],
      "abstract": "Evaluating the performance of Code Language Models (CLMs) for software\nengineering tasks, especially in multilingual and low-resource programming\nlanguage settings, poses significant challenges. These challenges are primarily\ndue to the lack of high-quality benchmarks across various programming languages\nand the imbalanced nature of the CLMs training corpus. Although recent advances\nin one of the common downstream tasks, code generation, have shown promise by\nintroducing translated benchmarks using different methodologies, there is a\ncurrent lack of empirical evidence assessing these benchmarks. To address this\ngap, we conducted a preliminary study to evaluate the performance of\nPoly-Coder, a pioneering open-source, multilingual CLM built for code\ngeneration. We utilized two existing state-of-the-art translations of the\npopular code generation benchmark, HumanEval, facilitated by the OctoPack and\nMultiPL-E studies. Our results suggest that the outcomes observed in these\ntranslated benchmarks align well with evaluation metrics used during the\ntraining phase, such as perplexity, thereby validating their effectiveness in\nestimating the performance of CLMs. However, we identified several\ninconsistencies in the CLMs' performance across the translated benchmarks and\nencountered challenges in replicating the results. These initial insights\nhighlight the need for more comprehensive empirical studies to fully understand\ntranslated benchmarks' methodological approaches, limitations, and\nreproducibility. Such studies are essential to ensure their reliability before\nthey are widely adopted.",
      "tldr_zh": "这篇论文初步研究了多语言 Code Language Models (CLMs) 在代码生成任务中的性能评估问题，特别是在多语言和低资源编程语言环境下的挑战，包括缺乏高质量基准和训练语料不平衡。\n研究团队评估了开源多语言 CLM 模型 Poly-Coder，使用了 HumanEval 基准的两种翻译版本（OctoPack 和 MultiPL-E）。\n结果表明，这些翻译基准与训练阶段的评估指标（如 perplexity）高度一致，有效地估计了 CLMs 的性能，但也暴露了性能不一致性和结果复制困难。\n论文呼吁开展更全面的实证研究，以探讨翻译基准的方法论、局限性和可重复性，从而确保其广泛应用前的可靠性。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.PL"
      ],
      "primary_category": "cs.SE",
      "comment": "5 pages, ASEW 2024",
      "pdf_url": "http://arxiv.org/pdf/2411.15470v1",
      "published_date": "2024-11-23 06:40:47 UTC",
      "updated_date": "2024-11-23 06:40:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:43:28.073046"
    },
    {
      "arxiv_id": "2411.15458v1",
      "title": "TANGNN: a Concise, Scalable and Effective Graph Neural Networks with Top-m Attention Mechanism for Graph Representation Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Jiawei E",
        "Yinglong Zhang",
        "Xuewen Xia",
        "Xing Xu"
      ],
      "abstract": "In the field of deep learning, Graph Neural Networks (GNNs) and Graph\nTransformer models, with their outstanding performance and flexible\narchitectural designs, have become leading technologies for processing\nstructured data, especially graph data. Traditional GNNs often face challenges\nin capturing information from distant vertices effectively. In contrast, Graph\nTransformer models are particularly adept at managing long-distance node\nrelationships. Despite these advantages, Graph Transformer models still\nencounter issues with computational and storage efficiency when scaled to large\ngraph datasets. To address these challenges, we propose an innovative Graph\nNeural Network (GNN) architecture that integrates a Top-m attention mechanism\naggregation component and a neighborhood aggregation component, effectively\nenhancing the model's ability to aggregate relevant information from both local\nand extended neighborhoods at each layer. This method not only improves\ncomputational efficiency but also enriches the node features, facilitating a\ndeeper analysis of complex graph structures. Additionally, to assess the\neffectiveness of our proposed model, we have applied it to citation sentiment\nprediction, a novel task previously unexplored in the GNN field. Accordingly,\nwe constructed a dedicated citation network, ArXivNet. In this dataset, we\nspecifically annotated the sentiment polarity of the citations (positive,\nneutral, negative) to enable in-depth sentiment analysis. Our approach has\nshown superior performance across a variety of tasks including vertex\nclassification, link prediction, sentiment prediction, graph regression, and\nvisualization. It outperforms existing methods in terms of effectiveness, as\ndemonstrated by experimental results on multiple datasets.",
      "tldr_zh": "本研究提出了一种简洁、可扩展且有效的图神经网络 TANGNN，利用 Top-m attention mechanism 和 neighborhood aggregation 组件，增强了图表示学习中从本地和扩展邻域聚合信息的效率和能力，解决了传统 GNNs 在捕获远距离节点信息上的挑战。TANGNN 应用于一个新任务——引文情感预测，并构建了 ArXivNet 数据集，标注了引文的积极、中性或消极情感极性。实验结果显示，该模型在顶点分类、链接预测、情感预测、图回归和可视化等任务上，超过了现有方法，证明了其在多个数据集上的优越性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "The code and ArXivNet dataset are available at\n  https://github.com/ejwww/TANGNN",
      "pdf_url": "http://arxiv.org/pdf/2411.15458v1",
      "published_date": "2024-11-23 05:31:25 UTC",
      "updated_date": "2024-11-23 05:31:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:43:39.986564"
    },
    {
      "arxiv_id": "2411.15457v1",
      "title": "Hindi audio-video-Deepfake (HAV-DF): A Hindi language-based Audio-video Deepfake Dataset",
      "title_zh": "翻译失败",
      "authors": [
        "Sukhandeep Kaur",
        "Mubashir Buhari",
        "Naman Khandelwal",
        "Priyansh Tyagi",
        "Kiran Sharma"
      ],
      "abstract": "Deepfakes offer great potential for innovation and creativity, but they also\npose significant risks to privacy, trust, and security. With a vast\nHindi-speaking population, India is particularly vulnerable to deepfake-driven\nmisinformation campaigns. Fake videos or speeches in Hindi can have an enormous\nimpact on rural and semi-urban communities, where digital literacy tends to be\nlower and people are more inclined to trust video content. The development of\neffective frameworks and detection tools to combat deepfake misuse requires\nhigh-quality, diverse, and extensive datasets. The existing popular datasets\nlike FF-DF (FaceForensics++), and DFDC (DeepFake Detection Challenge) are based\non English language.. Hence, this paper aims to create a first novel Hindi deep\nfake dataset, named ``Hindi audio-video-Deepfake'' (HAV-DF). The dataset has\nbeen generated using the faceswap, lipsyn and voice cloning methods. This\nmulti-step process allows us to create a rich, varied dataset that captures the\nnuances of Hindi speech and facial expressions, providing a robust foundation\nfor training and evaluating deepfake detection models in a Hindi language\ncontext. It is unique of its kind as all of the previous datasets contain\neither deepfake videos or synthesized audio. This type of deepfake dataset can\nbe used for training a detector for both deepfake video and audio datasets.\nNotably, the newly introduced HAV-DF dataset demonstrates lower detection\naccuracy's across existing detection methods like Headpose, Xception-c40, etc.\nCompared to other well-known datasets FF-DF, and DFDC. This trend suggests that\nthe HAV-DF dataset presents deeper challenges to detect, possibly due to its\nfocus on Hindi language content and diverse manipulation techniques. The HAV-DF\ndataset fills the gap in Hindi-specific deepfake datasets, aiding multilingual\ndeepfake detection development.",
      "tldr_zh": "这篇论文介绍了HAV-DF，一个基于印地语的音频-视频Deepfake数据集，以应对Deepfake在印地语社区的潜在风险和现有英语数据集（如FF-DF和DFDC）的局限性。数据集通过faceswap、lipsyn和voice cloning方法生成，涵盖了印地语语音和面部表情的多样性，用于训练和评估Deepfake检测模型。该数据集是首创的音频-视频结合类型，在现有检测方法如Headpose和Xception-c40上显示出较低准确率，表明其更具挑战性。HAV-DF的发布有助于推动多语言Deepfake检测技术的开发，特别是在印地语语境中的应用。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CR",
        "cs.GR",
        "cs.MM",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15457v1",
      "published_date": "2024-11-23 05:18:43 UTC",
      "updated_date": "2024-11-23 05:18:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:43:52.095639"
    },
    {
      "arxiv_id": "2411.15455v1",
      "title": "MUFM: A Mamba-Enhanced Feedback Model for Micro Video Popularity Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Jiacheng Lu",
        "Mingyuan Xiao",
        "Weijian Wang",
        "Yuxin Du",
        "Yi Cui",
        "Jingnan Zhao",
        "Cheng Hua"
      ],
      "abstract": "The surge in micro-videos is transforming the concept of popularity. As\nresearchers delve into vast multi-modal datasets, there is a growing interest\nin understanding the origins of this popularity and the forces driving its\nrapid expansion. Recent studies suggest that the virality of short videos is\nnot only tied to their inherent multi-modal content but is also heavily\ninfluenced by the strength of platform recommendations driven by audience\nfeedback. In this paper, we introduce a framework for capturing long-term\ndependencies in user feedback and dynamic event interactions, based on the\nMamba Hawkes process. Our experiments on the large-scale open-source\nmulti-modal dataset show that our model significantly outperforms\nstate-of-the-art approaches across various metrics by 23.2%. We believe our\nmodel's capability to map the relationships within user feedback behavior\nsequences will not only contribute to the evolution of next-generation\nrecommendation algorithms and platform applications but also enhance our\nunderstanding of micro video dissemination and its broader societal impact.",
      "tldr_zh": "本论文提出 MUFM 模型，这是一种基于 Mamba Hawkes process 的反馈增强框架，用于微视频流行性预测。该模型通过捕捉用户反馈中的长期依赖性和动态事件交互，解决了短视频传播受多模态内容和平台推荐影响的问题。在大型开源多模态数据集上的实验表明，MUFM 在各种指标上比最先进方法提高了 23.2%。这项工作有助于提升下一代推荐算法的设计，并加深对微视频传播机制和社会影响的理解。",
      "categories": [
        "cs.MM",
        "cs.AI"
      ],
      "primary_category": "cs.MM",
      "comment": "14 pages,9 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.15455v1",
      "published_date": "2024-11-23 05:13:27 UTC",
      "updated_date": "2024-11-23 05:13:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:44:03.678691"
    },
    {
      "arxiv_id": "2411.15453v1",
      "title": "Enhancing Instruction-Following Capability of Visual-Language Models by Reducing Image Redundancy",
      "title_zh": "通过减少图像冗余增强视觉-语言模型的指令遵循能力",
      "authors": [
        "Te Yang",
        "Jian Jia",
        "Xiangyu Zhu",
        "Weisong Zhao",
        "Bo Wang",
        "Yanhua Cheng",
        "Yan Li",
        "Shengyuan Liu",
        "Quan Chen",
        "Peng Jiang",
        "Kun Gai",
        "Zhen Lei"
      ],
      "abstract": "Large Language Models (LLMs) have strong instruction-following capability to\ninterpret and execute tasks as directed by human commands. Multimodal Large\nLanguage Models (MLLMs) have inferior instruction-following ability compared to\nLLMs. However, there is a significant gap in the instruction-following\ncapabilities between the MLLMs and LLMs. In this study, we conduct a pilot\nexperiment, which demonstrates that spatially down-sampling visual tokens\nsignificantly enhances the instruction-following capability of MLLMs. This is\nattributed to the substantial redundancy in visual modality. However, this\nintuitive method severely impairs the MLLM's multimodal understanding\ncapability. In this paper, we propose Visual-Modality Token Compression (VMTC)\nand Cross-Modality Attention Inhibition (CMAI) strategies to alleviate this gap\nbetween MLLMs and LLMs by inhibiting the influence of irrelevant visual tokens\nduring content generation, increasing the instruction-following ability of the\nMLLMs while retaining their multimodal understanding capacity. In VMTC module,\nthe primary tokens are retained and the redundant tokens are condensed by token\nclustering and merging. In CMAI process, we aggregate text-to-image attentions\nby text-to-text attentions to obtain a text-to-image focus score. Attention\ninhibition is performed on the text-image token pairs with low scores. Our\ncomprehensive experiments over instruction-following capabilities and VQA-V2,\nGQA, TextVQA, MME and MMBench five benchmarks, demonstrate that proposed\nstrategy significantly enhances the instruction following capability of MLLMs\nwhile preserving the ability to understand and process multimodal inputs.",
      "tldr_zh": "该研究发现，多模态大语言模型(MLLMs)相比大语言模型(LLMs)在指令遵循能力上存在显著差距，并通过实验证明减少图像冗余可提升MLLMs的性能，但会损害多模态理解。为解决此问题，论文提出Visual-Modality Token Compression (VMTC)模块，用于通过标记聚类和合并保留主要标记并压缩冗余标记，以及Cross-Modality Attention Inhibition (CMAI)策略，通过聚合注意力计算焦点分数并抑制低分文本-图像标记对，从而增强指令遵循能力。实验在指令遵循基准以及VQA-V2、GQA、TextVQA、MME和MMBench等数据集上显示，该方法显著提高了MLLMs的指令执行效果，同时保持了多模态理解能力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15453v1",
      "published_date": "2024-11-23 05:03:32 UTC",
      "updated_date": "2024-11-23 05:03:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:44:16.282549"
    },
    {
      "arxiv_id": "2411.16726v2",
      "title": "EmotiveTalk: Expressive Talking Head Generation through Audio Information Decoupling and Emotional Video Diffusion",
      "title_zh": "翻译失败",
      "authors": [
        "Haotian Wang",
        "Yuzhe Weng",
        "Yueyan Li",
        "Zilu Guo",
        "Jun Du",
        "Shutong Niu",
        "Jiefeng Ma",
        "Shan He",
        "Xiaoyan Wu",
        "Qiming Hu",
        "Bing Yin",
        "Cong Liu",
        "Qingfeng Liu"
      ],
      "abstract": "Diffusion models have revolutionized the field of talking head generation,\nyet still face challenges in expressiveness, controllability, and stability in\nlong-time generation. In this research, we propose an EmotiveTalk framework to\naddress these issues. Firstly, to realize better control over the generation of\nlip movement and facial expression, a Vision-guided Audio Information\nDecoupling (V-AID) approach is designed to generate audio-based decoupled\nrepresentations aligned with lip movements and expression. Specifically, to\nachieve alignment between audio and facial expression representation spaces, we\npresent a Diffusion-based Co-speech Temporal Expansion (Di-CTE) module within\nV-AID to generate expression-related representations under multi-source emotion\ncondition constraints. Then we propose a well-designed Emotional Talking Head\nDiffusion (ETHD) backbone to efficiently generate highly expressive talking\nhead videos, which contains an Expression Decoupling Injection (EDI) module to\nautomatically decouple the expressions from reference portraits while\nintegrating the target expression information, achieving more expressive\ngeneration performance. Experimental results show that EmotiveTalk can generate\nexpressive talking head videos, ensuring the promised controllability of\nemotions and stability during long-time generation, yielding state-of-the-art\nperformance compared to existing methods.",
      "tldr_zh": "本研究提出 EmotiveTalk 框架，利用扩散模型提升说话头部视频的生成性能，解决现有方法在表达性、可控性和长期生成稳定性方面的不足。框架的核心包括 Vision-guided Audio Information Decoupling (V-AID) 方法，通过 Diffusion-based Co-speech Temporal Expansion (Di-CTE) 模块生成与音频对齐的表情表示，确保唇部运动和面部表情的精确控制。随后，Emotional Talking Head Diffusion (ETHD) 骨干结合 Expression Decoupling Injection (EDI) 模块，从参考图像中解耦并整合目标表情，实现高效且高度表达性的视频生成。实验结果表明，EmotiveTalk 在情感可控性和稳定性上优于现有方法，达到了最先进性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "https://emotivetalk.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2411.16726v2",
      "published_date": "2024-11-23 04:38:51 UTC",
      "updated_date": "2024-12-16 17:11:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:44:29.754837"
    },
    {
      "arxiv_id": "2411.15446v1",
      "title": "freePruner: A Training-free Approach for Large Multimodal Model Acceleration",
      "title_zh": "翻译失败",
      "authors": [
        "Bingxin Xu",
        "Yuzhang Shang",
        "Yunhao Ge",
        "Qian Lou",
        "Yan Yan"
      ],
      "abstract": "Large Multimodal Models (LMMs) have demonstrated impressive capabilities in\nvisual-language tasks but face significant deployment challenges due to their\nhigh computational demands. While recent token reduction methods show promise\nfor accelerating LMMs, they typically require extensive retraining or\nfine-tuning, making them impractical for many state-of-the-art models,\nespecially those with proprietary training data. We propose freePruner, a\ntraining-free token reduction approach that can be directly applied to any\nopen-source LMM without additional training. Unlike existing methods that rely\nheavily on token merging operations, freePruner employs a two-stage token\nselection strategy: (1) identifying pivotal tokens that capture high-level\nsemantic information using our designed contribution degree metric, and (2)\nselecting complementary tokens that preserve essential low-level visual details\nthrough attention pattern analysis. Extensive experiments demonstrate that\nfreePruner achieves 2x acceleration while maintaining comparable performance\nacross mainstream visual question-answering benchmarks in the training-free\nsetting. Moreover, freePruner is orthogonal to and can be combined with other\npost-training acceleration techniques, such as post-training quantization,\nproviding a practical solution for efficient LMM deployment.",
      "tldr_zh": "该论文提出 freePruner，一种无需训练的令牌减少方法，用于加速 Large Multimodal Models (LMMs)，以解决其高计算需求导致的部署挑战。freePruner 采用两阶段策略：首先通过设计的贡献度指标识别捕捉高层语义信息的关键令牌，其次利用注意力模式分析选择补充令牌以保留低层视觉细节。实验结果表明，该方法在无需额外训练的情况下实现 2 倍加速，同时在主流视觉问答基准上保持性能相当，并可与其他后训练加速技术（如量化）结合使用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15446v1",
      "published_date": "2024-11-23 04:25:16 UTC",
      "updated_date": "2024-11-23 04:25:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:44:39.754770"
    },
    {
      "arxiv_id": "2411.15442v1",
      "title": "Automatic High-quality Verilog Assertion Generation through Subtask-Focused Fine-Tuned LLMs and Iterative Prompting",
      "title_zh": "翻译失败",
      "authors": [
        "Mohammad Shahidzadeh",
        "Behnam Ghavami",
        "Steve Wilton",
        "Lesley Shannon"
      ],
      "abstract": "Formal Property Verification (FPV), using SystemVerilog Assertions (SVA), is\ncrucial for ensuring the completeness of design with respect to the\nspecification. However, writing SVA is a laborious task and has a steep\nlearning curve. In this work, we present a large language model (LLM) -based\nflow to automatically generate high-quality SVA from the design specification\ndocuments, named \\ToolName. We introduce a novel sub-task-focused fine-tuning\napproach that effectively addresses functionally incorrect assertions produced\nby baseline LLMs, leading to a remarkable 7.3-fold increase in the number of\nfunctionally correct assertions. Recognizing the prevalence of syntax and\nsemantic errors, we also developed an iterative refinement method that enhances\nthe LLM's initial outputs by systematically re-prompting it to correct\nidentified issues. This process is further strengthened by a custom compiler\nthat generates meaningful error messages, guiding the LLM towards improved\naccuracy. The experiments demonstrate a 26\\% increase in the number of\nassertions free from syntax errors using this approach, showcasing its\npotential to streamline the FPV process.",
      "tldr_zh": "该论文提出了一种基于大型语言模型（LLM）的工具 \\ToolName，用于从设计规范文档自动生成高质量的 SystemVerilog Assertions (SVA)，以简化 Formal Property Verification (FPV) 过程。方法包括子任务聚焦的微调策略，显著减少功能错误断言并实现功能正确断言数量的 7.3 倍提升；同时，通过迭代提示和自定义编译器修正语法及语义错误。实验结果显示，该方法使无语法错误的断言数量增加 26%，证明其在提升 FPV 效率和准确性方面的潜力。",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15442v1",
      "published_date": "2024-11-23 03:52:32 UTC",
      "updated_date": "2024-11-23 03:52:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:44:51.726861"
    },
    {
      "arxiv_id": "2411.15428v1",
      "title": "GeoAI-Enhanced Community Detection on Spatial Networks with Graph Deep Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Yunlei Liang",
        "Jiawei Zhu",
        "Wen Ye",
        "Song Gao"
      ],
      "abstract": "Spatial networks are useful for modeling geographic phenomena where spatial\ninteraction plays an important role. To analyze the spatial networks and their\ninternal structures, graph-based methods such as community detection have been\nwidely used. Community detection aims to extract strongly connected components\nfrom the network and reveal the hidden relationships between nodes, but they\nusually do not involve the attribute information. To consider edge-based\ninteractions and node attributes together, this study proposed a family of\nGeoAI-enhanced unsupervised community detection methods called region2vec based\non Graph Attention Networks (GAT) and Graph Convolutional Networks (GCN). The\nregion2vec methods generate node neural embeddings based on attribute\nsimilarity, geographic adjacency and spatial interactions, and then extract\nnetwork communities based on node embeddings using agglomerative clustering.\nThe proposed GeoAI-based methods are compared with multiple baselines and\nperform the best when one wants to maximize node attribute similarity and\nspatial interaction intensity simultaneously within the spatial network\ncommunities. It is further applied in the shortage area delineation problem in\npublic health and demonstrates its promise in regionalization problems.",
      "tldr_zh": "本研究提出了一种GeoAI增强的无监督社区检测方法region2vec，旨在分析空间网络中的内部结构，通过整合Graph Attention Networks (GAT)和Graph Convolutional Networks (GCN)来生成节点神经嵌入。方法基于节点属性相似性、地理邻接和空间交互，结合凝聚聚类提取网络社区，从而同时最大化属性相似性和空间交互强度。与多个基线方法相比，region2vec表现出色，并在公共健康领域的短缺区域划分问题中展现了实际应用潜力。",
      "categories": [
        "cs.SI",
        "cs.AI",
        "I.2.4"
      ],
      "primary_category": "cs.SI",
      "comment": "25 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.15428v1",
      "published_date": "2024-11-23 03:09:34 UTC",
      "updated_date": "2024-11-23 03:09:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:45:03.195126"
    },
    {
      "arxiv_id": "2411.15422v1",
      "title": "Learning a local trading strategy: deep reinforcement learning for grid-scale renewable energy integration",
      "title_zh": "翻译失败",
      "authors": [
        "Caleb Ju",
        "Constance Crozier"
      ],
      "abstract": "Variable renewable generation increases the challenge of balancing power\nsupply and demand. Grid-scale batteries co-located with generation can help\nmitigate this misalignment. This paper explores the use of reinforcement\nlearning (RL) for operating grid-scale batteries co-located with solar power.\nOur results show RL achieves an average of 61% (and up to 96%) of the\napproximate theoretical optimal (non-causal) operation, outperforming advanced\ncontrol methods on average. Our findings suggest RL may be preferred when\nfuture signals are hard to predict. Moreover, RL has two significant advantages\ncompared to simpler rules-based control: (1) that solar energy is more\neffectively shifted towards high demand periods, and (2) increased diversity of\nbattery dispatch across different locations, reducing potential ramping issues\ncaused by super-position of many similar actions.",
      "tldr_zh": "本研究探讨了使用深度强化学习（RL）来优化网格规模电池的操作，以缓解可变可再生能源发电（如太阳能）导致的电力供需不平衡问题。研究结果显示，RL 平均实现了理论最优操作的 61%（最高 96%），并在整体性能上优于高级控制方法，尤其适用于未来信号难以预测的场景。相比规则-based 控制，RL 的两大优势包括：更有效地将太阳能转移到高需求期，以及增强电池调度在不同地点的多样性，从而减少类似动作叠加引起的斜坡问题。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SY",
        "eess.SY",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to HICSS58",
      "pdf_url": "http://arxiv.org/pdf/2411.15422v1",
      "published_date": "2024-11-23 02:55:38 UTC",
      "updated_date": "2024-11-23 02:55:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:45:15.250131"
    },
    {
      "arxiv_id": "2411.16723v1",
      "title": "Two Heads Are Better Than One: Collaborative LLM Embodied Agents for Human-Robot Interaction",
      "title_zh": "翻译失败",
      "authors": [
        "Mitchell Rosser",
        "Marc. G Carmichael"
      ],
      "abstract": "With the recent development of natural language generation models - termed as\nlarge language models (LLMs) - a potential use case has opened up to improve\nthe way that humans interact with robot assistants. These LLMs should be able\nto leverage their large breadth of understanding to interpret natural language\ncommands into effective, task appropriate and safe robot task executions.\nHowever, in reality, these models suffer from hallucinations, which may cause\nsafety issues or deviations from the task. In other domains, these issues have\nbeen improved through the use of collaborative AI systems where multiple LLM\nagents can work together to collectively plan, code and self-check outputs. In\nthis research, multiple collaborative AI systems were tested against a single\nindependent AI agent to determine whether the success in other domains would\ntranslate into improved human-robot interaction performance. The results show\nthat there is no defined trend between the number of agents and the success of\nthe model. However, it is clear that some collaborative AI agent architectures\ncan exhibit a greatly improved capacity to produce error-free code and to solve\nabstract problems.",
      "tldr_zh": "该研究探讨了大型语言模型(LLMs)用于提升人类-机器人交互的潜力，通过将自然语言命令转化为有效、安全的机器人任务执行。然而，LLMs 常因幻觉问题导致错误，因此研究者测试了多个协作 AI 系统与单个独立 AI 代理的性能，焦点在于集体规划、编码和自检。结果显示，代理数量与模型成功率无明确趋势，但某些协作架构显著提高了生成无错误代码和解决抽象问题的能力，为更可靠的人类-机器人交互提供了新思路。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.MA",
      "comment": "9 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.16723v1",
      "published_date": "2024-11-23 02:47:12 UTC",
      "updated_date": "2024-11-23 02:47:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:45:27.032273"
    },
    {
      "arxiv_id": "2411.15413v1",
      "title": "FG-CXR: A Radiologist-Aligned Gaze Dataset for Enhancing Interpretability in Chest X-Ray Report Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Trong Thang Pham",
        "Ngoc-Vuong Ho",
        "Nhat-Tan Bui",
        "Thinh Phan",
        "Patel Brijesh",
        "Donald Adjeroh",
        "Gianfranco Doretto",
        "Anh Nguyen",
        "Carol C. Wu",
        "Hien Nguyen",
        "Ngan Le"
      ],
      "abstract": "Developing an interpretable system for generating reports in chest X-ray\n(CXR) analysis is becoming increasingly crucial in Computer-aided Diagnosis\n(CAD) systems, enabling radiologists to comprehend the decisions made by these\nsystems. Despite the growth of diverse datasets and methods focusing on report\ngeneration, there remains a notable gap in how closely these models' generated\nreports align with the interpretations of real radiologists. In this study, we\ntackle this challenge by initially introducing Fine-Grained CXR (FG-CXR)\ndataset, which provides fine-grained paired information between the captions\ngenerated by radiologists and the corresponding gaze attention heatmaps for\neach anatomy. Unlike existing datasets that include a raw sequence of gaze\nalongside a report, with significant misalignment between gaze location and\nreport content, our FG-CXR dataset offers a more grained alignment between gaze\nattention and diagnosis transcript. Furthermore, our analysis reveals that\nsimply applying black-box image captioning methods to generate reports cannot\nadequately explain which information in CXR is utilized and how long needs to\nattend to accurately generate reports. Consequently, we propose a novel\nexplainable radiologist's attention generator network (Gen-XAI) that mimics the\ndiagnosis process of radiologists, explicitly constraining its output to\nclosely align with both radiologist's gaze attention and transcript. Finally,\nwe perform extensive experiments to illustrate the effectiveness of our method.\nOur datasets and checkpoint is available at\nhttps://github.com/UARK-AICV/FG-CXR.",
      "tldr_zh": "本研究引入了 FG-CXR 数据集，该数据集提供放射科医生在胸部 X 光片 (CXR) 分析中生成的标题与对应注视注意力热图的细粒度配对信息，从而提升报告生成的解释性。不同于现有数据集，FG-CXR 实现了注视位置与报告内容的精确对齐，解决了传统方法在信息利用和注意力分配上的不足。研究提出了一种新型可解释模型 Gen-XAI，它模仿放射科医生的诊断过程，通过显式约束输出与医生的注视注意力热图和报告紧密结合。实验结果证明，该方法显著提高了报告的准确性和可解释性，相关数据集和代码已在 GitHub 上公开。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "ACCV 2024",
      "pdf_url": "http://arxiv.org/pdf/2411.15413v1",
      "published_date": "2024-11-23 02:22:40 UTC",
      "updated_date": "2024-11-23 02:22:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:45:39.882017"
    },
    {
      "arxiv_id": "2411.16721v3",
      "title": "Steering Away from Harm: An Adaptive Approach to Defending Vision Language Model Against Jailbreaks",
      "title_zh": "翻译失败",
      "authors": [
        "Han Wang",
        "Gang Wang",
        "Huan Zhang"
      ],
      "abstract": "Vision Language Models (VLMs) can produce unintended and harmful content when\nexposed to adversarial attacks, particularly because their vision capabilities\ncreate new vulnerabilities. Existing defenses, such as input preprocessing,\nadversarial training, and response evaluation-based methods, are often\nimpractical for real-world deployment due to their high costs. To address this\nchallenge, we propose ASTRA, an efficient and effective defense by adaptively\nsteering models away from adversarial feature directions to resist VLM attacks.\nOur key procedures involve finding transferable steering vectors representing\nthe direction of harmful response and applying adaptive activation steering to\nremove these directions at inference time. To create effective steering\nvectors, we randomly ablate the visual tokens from the adversarial images and\nidentify those most strongly associated with jailbreaks. These tokens are then\nused to construct steering vectors. During inference, we perform the adaptive\nsteering method that involves the projection between the steering vectors and\ncalibrated activation, resulting in little performance drops on benign inputs\nwhile strongly avoiding harmful outputs under adversarial inputs. Extensive\nexperiments across multiple models and baselines demonstrate our\nstate-of-the-art performance and high efficiency in mitigating jailbreak risks.\nAdditionally, ASTRA exhibits good transferability, defending against unseen\nattacks (i.e., structured-based attack, perturbation-based attack with project\ngradient descent variants, and text-only attack). Our code is available at\n\\url{https://github.com/ASTRAL-Group/ASTRA}.",
      "tldr_zh": "该研究针对视觉语言模型（VLMs）在面对对抗攻击时产生的有害内容问题，提出了一种高效的自适应防御方法ASTRA。该方法通过识别可转移的转向向量（steering vectors）来代表有害响应的方向，并在推理阶段应用自适应激活转向（adaptive activation steering），以去除这些向量，从而减少对良性输入的影响，同时有效抵御攻击。实验结果显示，ASTRA在多个模型和基线中表现出最先进的性能和高效率，能够防御各种未见攻击，如结构化攻击、基于扰动的攻击和纯文本攻击（text-only attack）。这项工作为VLMs的安全部署提供了实用解决方案，并提供了开源代码以便进一步验证。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.16721v3",
      "published_date": "2024-11-23 02:17:17 UTC",
      "updated_date": "2025-05-01 22:22:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:45:51.897580"
    },
    {
      "arxiv_id": "2411.15408v1",
      "title": "Exploring Large Language Models for Multimodal Sentiment Analysis: Challenges, Benchmarks, and Future Directions",
      "title_zh": "翻译失败",
      "authors": [
        "Shezheng Song"
      ],
      "abstract": "Multimodal Aspect-Based Sentiment Analysis (MABSA) aims to extract aspect\nterms and their corresponding sentiment polarities from multimodal information,\nincluding text and images. While traditional supervised learning methods have\nshown effectiveness in this task, the adaptability of large language models\n(LLMs) to MABSA remains uncertain. Recent advances in LLMs, such as Llama2,\nLLaVA, and ChatGPT, demonstrate strong capabilities in general tasks, yet their\nperformance in complex and fine-grained scenarios like MABSA is underexplored.\nIn this study, we conduct a comprehensive investigation into the suitability of\nLLMs for MABSA. To this end, we construct a benchmark to evaluate the\nperformance of LLMs on MABSA tasks and compare them with state-of-the-art\nsupervised learning methods. Our experiments reveal that, while LLMs\ndemonstrate potential in multimodal understanding, they face significant\nchallenges in achieving satisfactory results for MABSA, particularly in terms\nof accuracy and inference time. Based on these findings, we discuss the\nlimitations of current LLMs and outline directions for future research to\nenhance their capabilities in multimodal sentiment analysis.",
      "tldr_zh": "这篇论文探讨了大型语言模型(LLMs)在多模态情感分析(Multimodal Aspect-Based Sentiment Analysis, MABSA)中的应用，MABSA旨在从文本和图像中提取方面术语及其情感极性。研究者构建了一个基准，评估LLMs如Llama2、LLaVA和ChatGPT的表现，并与最先进的监督学习方法进行比较。实验发现，虽然LLMs在多模态理解方面显示出潜力，但它们在准确性和推理时间上面临显著挑战。论文讨论了当前LLMs的局限性，并提出了未来研究方向，以提升其在MABSA中的能力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15408v1",
      "published_date": "2024-11-23 02:17:10 UTC",
      "updated_date": "2024-11-23 02:17:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:46:04.912104"
    },
    {
      "arxiv_id": "2411.15396v1",
      "title": "The Decoy Dilemma in Online Medical Information Evaluation: A Comparative Study of Credibility Assessments by LLM and Human Judges",
      "title_zh": "在线医疗信息评估中的诱饵困境：LLM 与人类评判者可信度评估的比较研究",
      "authors": [
        "Jiqun Liu",
        "Jiangen He"
      ],
      "abstract": "Can AI be cognitively biased in automated information judgment tasks? Despite\nrecent progresses in measuring and mitigating social and algorithmic biases in\nAI and large language models (LLMs), it is not clear to what extent LLMs behave\n\"rationally\", or if they are also vulnerable to human cognitive bias triggers.\nTo address this open problem, our study, consisting of a crowdsourcing user\nexperiment and a LLM-enabled simulation experiment, compared the credibility\nassessments by LLM and human judges under potential decoy effects in an\ninformation retrieval (IR) setting, and empirically examined the extent to\nwhich LLMs are cognitively biased in COVID-19 medical (mis)information\nassessment tasks compared to traditional human assessors as a baseline. The\nresults, collected from a between-subject user experiment and a LLM-enabled\nreplicate experiment, demonstrate that 1) Larger and more recent LLMs tend to\nshow a higher level of consistency and accuracy in distinguishing credible\ninformation from misinformation. However, they are more likely to give higher\nratings for misinformation due to the presence of a more salient, decoy\nmisinformation result; 2) While decoy effect occurred in both human and LLM\nassessments, the effect is more prevalent across different conditions and\ntopics in LLM judgments compared to human credibility ratings. In contrast to\nthe generally assumed \"rationality\" of AI tools, our study empirically confirms\nthe cognitive bias risks embedded in LLM agents, evaluates the decoy impact on\nLLMs against human credibility assessments, and thereby highlights the\ncomplexity and importance of debiasing AI agents and developing\npsychology-informed AI audit techniques and policies for automated judgment\ntasks and beyond.",
      "tldr_zh": "本研究比较了LLM和人类在在线医疗信息评估中的可信度判断，特别是在decoy effect（诱饵效应）的影响下，通过众包用户实验和LLM模拟实验考察COVID-19相关医疗信息。\n结果显示，较大的、更新的LLM在区分可信信息和误信息方面表现出更高的准确性和一致性，但更容易因诱饵误信息而给出更高评分，导致偏差。\n与人类相比，decoy effect在LLM判断中更普遍，这揭示了LLM的认知偏差风险，并强调了去偏置AI代理和开发心理学-informed审计技术的重要性。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.HC",
        "H.3.3; I.2.7"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15396v1",
      "published_date": "2024-11-23 00:43:27 UTC",
      "updated_date": "2024-11-23 00:43:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:46:16.315010"
    },
    {
      "arxiv_id": "2411.15395v1",
      "title": "ChatBCI: A P300 Speller BCI Leveraging Large Language Models for Improved Sentence Composition in Realistic Scenarios",
      "title_zh": "翻译失败",
      "authors": [
        "Jiazhen Hong",
        "Weinan Wang",
        "Laleh Najafizadeh"
      ],
      "abstract": "P300 speller BCIs allow users to compose sentences by selecting target keys\non a GUI through the detection of P300 component in their EEG signals following\nvisual stimuli. Most P300 speller BCIs require users to spell words letter by\nletter, or the first few initial letters, resulting in high keystroke demands\nthat increase time, cognitive load, and fatigue. This highlights the need for\nmore efficient, user-friendly methods for faster sentence composition. In this\nwork, we introduce ChatBCI, a P300 speller BCI that leverages the zero-shot\nlearning capabilities of large language models (LLMs) to suggest words from\nuser-spelled initial letters or predict the subsequent word(s), reducing\nkeystrokes and accelerating sentence composition. ChatBCI retrieves word\nsuggestions through remote queries to the GPT-3.5 API. A new GUI, displaying\nGPT-3.5 word suggestions as extra keys is designed. SWLDA is used for the P300\nclassification. Seven subjects completed two online spelling tasks: 1)\ncopy-spelling a self-composed sentence using ChatBCI, and 2) improvising a\nsentence using ChatBCI's word suggestions. Results demonstrate that in Task 1,\non average, ChatBCI outperforms letter-by-letter BCI spellers, reducing time\nand keystrokes by 62.14% and 53.22%, respectively, and increasing information\ntransfer rate by 198.96%. In Task 2, ChatBCI achieves 80.68% keystroke savings\nand a record 8.53 characters/min for typing speed. Overall, ChatBCI, by\nemploying remote LLM queries, enhances sentence composition in realistic\nscenarios, significantly outperforming traditional spellers without requiring\nlocal model training or storage. ChatBCI's (multi-) word predictions, combined\nwith its new GUI, pave the way for developing next-generation speller BCIs that\nare efficient and effective for real-time communication, especially for users\nwith communication and motor disabilities.",
      "tldr_zh": "这篇论文介绍了 ChatBCI，一种改进的 P300 speller BCI 系统，通过利用大型语言模型 (LLMs) 的零样本学习能力，从用户拼写的首字母建议单词或预测后续词语，从而减少击键次数并加速句子组成。ChatBCI 采用远程查询 GPT-3.5 API 获取建议，并设计了一个新 GUI 显示这些选项，同时使用 SWLDA 进行 P300 分类。实验涉及七名受试者完成在线拼写任务，结果显示 ChatBCI 相较传统方法降低了 62.14% 的完成时间和 53.22% 的击键次数，并将信息传输率提高了 198.96%，在即兴任务中实现 80.68% 的击键节省和 8.53 字符/分钟的打字速度。该系统无需本地模型训练，为有沟通和运动障碍的用户提供高效的实时通信解决方案。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.SY",
        "eess.SP",
        "eess.SY"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15395v1",
      "published_date": "2024-11-23 00:42:12 UTC",
      "updated_date": "2024-11-23 00:42:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:46:29.679774"
    },
    {
      "arxiv_id": "2411.15393v1",
      "title": "Gradient-Free Classifier Guidance for Diffusion Model Sampling",
      "title_zh": "翻译失败",
      "authors": [
        "Rahul Shenoy",
        "Zhihong Pan",
        "Kaushik Balakrishnan",
        "Qisen Cheng",
        "Yongmoon Jeon",
        "Heejune Yang",
        "Jaewon Kim"
      ],
      "abstract": "Image generation using diffusion models have demonstrated outstanding\nlearning capabilities, effectively capturing the full distribution of the\ntraining dataset. They are known to generate wide variations in sampled images,\nalbeit with a trade-off in image fidelity. Guided sampling methods, such as\nclassifier guidance (CG) and classifier-free guidance (CFG), focus sampling in\nwell-learned high-probability regions to generate images of high fidelity, but\neach has its limitations. CG is computationally expensive due to the use of\nback-propagation for classifier gradient descent, while CFG, being\ngradient-free, is more efficient but compromises class label alignment compared\nto CG. In this work, we propose an efficient guidance method that fully\nutilizes a pre-trained classifier without using gradient descent. By using the\nclassifier solely in inference mode, a time-adaptive reference class label and\ncorresponding guidance scale are determined at each time step for guided\nsampling. Experiments on both class-conditioned and text-to-image generation\ndiffusion models demonstrate that the proposed Gradient-free Classifier\nGuidance (GFCG) method consistently improves class prediction accuracy. We also\nshow GFCG to be complementary to other guided sampling methods like CFG. When\ncombined with the state-of-the-art Autoguidance (ATG), without additional\ncomputational overhead, it enhances image fidelity while preserving diversity.\nFor ImageNet 512$\\times$512, we achieve a record $\\text{FD}_{\\text{DINOv2}}$ of\n23.09, while simultaneously attaining a higher classification Precision (94.3%)\ncompared to ATG (90.2%)",
      "tldr_zh": "本研究针对扩散模型（diffusion models）在图像生成中的保真度与多样性权衡问题，提出了一种高效的 Gradient-free Classifier Guidance (GFCG) 方法。该方法不依赖梯度下降，而是通过在推理模式下利用预训练分类器，在每个时间步动态确定参考类别标签和引导比例，实现精确的引导采样。实验结果显示，GFCG 在类条件和文本到图像生成任务上显著提升了类预测准确性，并在与 Autoguidance (ATG) 结合时，进一步提高了图像保真度（如 ImageNet 512×512 上达到 record FD_DINOv2 23.09，同时分类精度达 94.3%），同时保持图像多样性。相比传统 Classifier Guidance (CG) 和 Classifier-Free Guidance (CFG)，GFCG 提供了更高效且互补的解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15393v1",
      "published_date": "2024-11-23 00:22:21 UTC",
      "updated_date": "2024-11-23 00:22:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:46:41.139914"
    },
    {
      "arxiv_id": "2411.15386v1",
      "title": "Inducing Human-like Biases in Moral Reasoning Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Artem Karpov",
        "Seong Hah Cho",
        "Austin Meek",
        "Raymond Koopmanschap",
        "Lucy Farnik",
        "Bogdan-Ionut Cirstea"
      ],
      "abstract": "In this work, we study the alignment (BrainScore) of large language models\n(LLMs) fine-tuned for moral reasoning on behavioral data and/or brain data of\nhumans performing the same task. We also explore if fine-tuning several LLMs on\nthe fMRI data of humans performing moral reasoning can improve the BrainScore.\nWe fine-tune several LLMs (BERT, RoBERTa, DeBERTa) on moral reasoning\nbehavioral data from the ETHICS benchmark [Hendrycks et al., 2020], on the\nmoral reasoning fMRI data from Koster-Hale et al. [2013], or on both. We study\nboth the accuracy on the ETHICS benchmark and the BrainScores between model\nactivations and fMRI data. While larger models generally performed better on\nboth metrics, BrainScores did not significantly improve after fine-tuning.",
      "tldr_zh": "本研究探讨了通过微调大型语言模型（LLMs）来诱导人类-like 偏见，以提升模型在道德推理任务中的对齐度（BrainScore）。研究者使用 BERT、RoBERTa 和 DeBERTa 等模型，分别基于 ETHICS benchmark 的行为数据、Koster-Hale 等人的 fMRI 数据，或两者结合进行微调，并评估模型在 ETHICS 基准上的准确性和与 fMRI 数据的 BrainScore。结果表明，较大模型在两个指标上表现更好，但微调 fMRI 数据后 BrainScore 并未显著提升。",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to the 2nd Workshop on Unifying Representations in Neural\n  Models (UniReps) at NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2411.15386v1",
      "published_date": "2024-11-23 00:01:07 UTC",
      "updated_date": "2024-11-23 00:01:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T03:46:51.965897"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 50,
  "processed_papers_count": 50,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-21T03:47:09.394658"
}
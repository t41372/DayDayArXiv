[
  {
    "arxiv_id": "2411.15672v1",
    "title": "IRSKG: Unified Intrusion Response System Knowledge Graph Ontology for Cyber Defense",
    "authors": [
      "Damodar Panigrahi",
      "Shaswata Mitra",
      "Subash Neupane",
      "Sudip Mittal",
      "Benjamin A. Blakely"
    ],
    "abstract": "Cyberattacks are becoming increasingly difficult to detect and prevent due to\ntheir sophistication. In response, Autonomous Intelligent Cyber-defense Agents\n(AICAs) are emerging as crucial solutions. One prominent AICA agent is the\nIntrusion Response System (IRS), which is critical for mitigating threats after\ndetection. IRS uses several Tactics, Techniques, and Procedures (TTPs) to\nmitigate attacks and restore the infrastructure to normal operations.\nContinuous monitoring of the enterprise infrastructure is an essential TTP the\nIRS uses. However, each system serves different purposes to meet operational\nneeds. Integrating these disparate sources for continuous monitoring increases\npre-processing complexity and limits automation, eventually prolonging critical\nresponse time for attackers to exploit. We propose a unified IRS Knowledge\nGraph ontology (IRSKG) that streamlines the onboarding of new enterprise\nsystems as a source for the AICAs. Our ontology can capture system monitoring\nlogs and supplemental data, such as a rules repository containing the\nadministrator-defined policies to dictate the IRS responses. Besides, our\nontology permits us to incorporate dynamic changes to adapt to the evolving\ncyber-threat landscape. This robust yet concise design allows machine learning\nmodels to train effectively and recover a compromised system to its desired\nstate autonomously with explainability.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "10 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.15672v1",
    "published_date": "2024-11-23 23:31:55 UTC",
    "updated_date": "2024-11-23 23:31:55 UTC"
  },
  {
    "arxiv_id": "2411.16747v1",
    "title": "FollowGen: A Scaled Noise Conditional Diffusion Model for Car-Following Trajectory Prediction",
    "authors": [
      "Junwei You",
      "Rui Gan",
      "Weizhe Tang",
      "Zilin Huang",
      "Jiaxi Liu",
      "Zhuoyu Jiang",
      "Haotian Shi",
      "Keshu Wu",
      "Keke Long",
      "Sicheng Fu",
      "Sikai Chen",
      "Bin Ran"
    ],
    "abstract": "Vehicle trajectory prediction is crucial for advancing autonomous driving and\nadvanced driver assistance systems (ADAS). Although deep learning-based\napproaches - especially those utilizing transformer-based and generative models\n- have markedly improved prediction accuracy by capturing complex, non-linear\npatterns in vehicle dynamics and traffic interactions, they frequently overlook\ndetailed car-following behaviors and the inter-vehicle interactions critical\nfor real-world driving applications, particularly in fully autonomous or mixed\ntraffic scenarios. To address the issue, this study introduces a scaled noise\nconditional diffusion model for car-following trajectory prediction, which\nintegrates detailed inter-vehicular interactions and car-following dynamics\ninto a generative framework, improving both the accuracy and plausibility of\npredicted trajectories. The model utilizes a novel pipeline to capture\nhistorical vehicle dynamics by scaling noise with encoded historical features\nwithin the diffusion process. Particularly, it employs a cross-attention-based\ntransformer architecture to model intricate inter-vehicle dependencies,\neffectively guiding the denoising process and enhancing prediction accuracy.\nExperimental results on diverse real-world driving scenarios demonstrate the\nstate-of-the-art performance and robustness of the proposed method.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.CV",
    "comment": "arXiv admin note: text overlap with arXiv:2406.11941",
    "pdf_url": "http://arxiv.org/pdf/2411.16747v1",
    "published_date": "2024-11-23 23:13:45 UTC",
    "updated_date": "2024-11-23 23:13:45 UTC"
  },
  {
    "arxiv_id": "2411.15666v1",
    "title": "Ontology-Constrained Generation of Domain-Specific Clinical Summaries",
    "authors": [
      "Gaya Mehenni",
      "Amal Zouaq"
    ],
    "abstract": "Large Language Models (LLMs) offer promising solutions for text\nsummarization. However, some domains require specific information to be\navailable in the summaries. Generating these domain-adapted summaries is still\nan open challenge. Similarly, hallucinations in generated content is a major\ndrawback of current approaches, preventing their deployment. This study\nproposes a novel approach that leverages ontologies to create domain-adapted\nsummaries both structured and unstructured. We employ an ontology-guided\nconstrained decoding process to reduce hallucinations while improving\nrelevance. When applied to the medical domain, our method shows potential in\nsummarizing Electronic Health Records (EHRs) across different specialties,\nallowing doctors to focus on the most relevant information to their domain.\nEvaluation on the MIMIC-III dataset demonstrates improvements in generating\ndomain-adapted summaries of clinical notes and hallucination reduction.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "24th International Conference on Knowledge Engineering and Knowledge\n  Management (EKAW 2024), November 26-28, 2024, Amsterdam, The Netherlands",
    "pdf_url": "http://arxiv.org/pdf/2411.15666v1",
    "published_date": "2024-11-23 23:05:48 UTC",
    "updated_date": "2024-11-23 23:05:48 UTC"
  },
  {
    "arxiv_id": "2411.16746v3",
    "title": "LoBAM: LoRA-Based Backdoor Attack on Model Merging",
    "authors": [
      "Ming Yin",
      "Jingyang Zhang",
      "Jingwei Sun",
      "Minghong Fang",
      "Hai Li",
      "Yiran Chen"
    ],
    "abstract": "Model merging is an emerging technique that integrates multiple models\nfine-tuned on different tasks to create a versatile model that excels in\nmultiple domains. This scheme, in the meantime, may open up backdoor attack\nopportunities where one single malicious model can jeopardize the integrity of\nthe merged model. Existing works try to demonstrate the risk of such attacks by\nassuming substantial computational resources, focusing on cases where the\nattacker can fully fine-tune the pre-trained model. Such an assumption,\nhowever, may not be feasible given the increasing size of machine learning\nmodels. In practice where resources are limited and the attacker can only\nemploy techniques like Low-Rank Adaptation (LoRA) to produce the malicious\nmodel, it remains unclear whether the attack can still work and pose threats.\nIn this work, we first identify that the attack efficacy is significantly\ndiminished when using LoRA for fine-tuning. Then, we propose LoBAM, a method\nthat yields high attack success rate with minimal training resources. The key\nidea of LoBAM is to amplify the malicious weights in an intelligent way that\neffectively enhances the attack efficacy. We demonstrate that our design can\nlead to improved attack success rate through extensive empirical experiments\nacross various model merging scenarios. Moreover, we show that our method is\nhighly stealthy and is difficult to detect and defend against.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.16746v3",
    "published_date": "2024-11-23 20:41:24 UTC",
    "updated_date": "2025-03-05 05:34:47 UTC"
  },
  {
    "arxiv_id": "2411.16742v1",
    "title": "Text-to-SQL Calibration: No Need to Ask -- Just Rescale Model Probabilities",
    "authors": [
      "Ashwin Ramachandran",
      "Sunita Sarawagi"
    ],
    "abstract": "Calibration is crucial as large language models (LLMs) are increasingly\ndeployed to convert natural language queries into SQL for commercial databases.\nIn this work, we investigate calibration techniques for assigning confidence to\ngenerated SQL queries. We show that a straightforward baseline -- deriving\nconfidence from the model's full-sequence probability -- outperforms recent\nmethods that rely on follow-up prompts for self-checking and confidence\nverbalization. Our comprehensive evaluation, conducted across two widely-used\nText-to-SQL benchmarks and multiple LLM architectures, provides valuable\ninsights into the effectiveness of various calibration strategies.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.16742v1",
    "published_date": "2024-11-23 19:20:24 UTC",
    "updated_date": "2024-11-23 19:20:24 UTC"
  },
  {
    "arxiv_id": "2411.15634v1",
    "title": "\"All that Glitters\": Approaches to Evaluations with Unreliable Model and Human Annotations",
    "authors": [
      "Michael Hardy"
    ],
    "abstract": "\"Gold\" and \"ground truth\" human-mediated labels have error. The effects of\nthis error can escape commonly reported metrics of label quality or obscure\nquestions of accuracy, bias, fairness, and usefulness during model evaluation.\nThis study demonstrates methods for answering such questions even in the\ncontext of very low reliabilities from expert humans. We analyze human labels,\nGPT model ratings, and transformer encoder model annotations describing the\nquality of classroom teaching, an important, expensive, and currently only\nhuman task. We answer the question of whether such a task can be automated\nusing two Large Language Model (LLM) architecture families--encoders and GPT\ndecoders, using novel approaches to evaluating label quality across six\ndimensions: Concordance, Confidence, Validity, Bias, Fairness, and Helpfulness.\nFirst, we demonstrate that using standard metrics in the presence of poor\nlabels can mask both label and model quality: the encoder family of models\nachieve state-of-the-art, even \"super-human\", results across all classroom\nannotation tasks. But not all these positive results remain after using more\nrigorous evaluation measures which reveal spurious correlations and nonrandom\nracial biases across models and humans. This study then expands these methods\nto estimate how model use would change to human label quality if models were\nused in a human-in-the-loop context, finding that the variance captured in GPT\nmodel labels would worsen reliabilities for humans influenced by these models.\nWe identify areas where some LLMs, within the generalizability of the current\ndata, could improve the quality of expensive human ratings of classroom\ninstruction.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "stat.AP"
    ],
    "primary_category": "cs.CL",
    "comment": "20 pages, 15 figures, 58 pages with references and appendices",
    "pdf_url": "http://arxiv.org/pdf/2411.15634v1",
    "published_date": "2024-11-23 19:18:08 UTC",
    "updated_date": "2024-11-23 19:18:08 UTC"
  },
  {
    "arxiv_id": "2411.15626v1",
    "title": "Aligning Generalisation Between Humans and Machines",
    "authors": [
      "Filip Ilievski",
      "Barbara Hammer",
      "Frank van Harmelen",
      "Benjamin Paassen",
      "Sascha Saralajew",
      "Ute Schmid",
      "Michael Biehl",
      "Marianna Bolognesi",
      "Xin Luna Dong",
      "Kiril Gashteovski",
      "Pascal Hitzler",
      "Giuseppe Marra",
      "Pasquale Minervini",
      "Martin Mundt",
      "Axel-Cyrille Ngonga Ngomo",
      "Alessandro Oltramari",
      "Gabriella Pasi",
      "Zeynep G. Saribatur",
      "Luciano Serafini",
      "John Shawe-Taylor",
      "Vered Shwartz",
      "Gabriella Skitalinskaya",
      "Clemens Stachl",
      "Gido M. van de Ven",
      "Thomas Villmann"
    ],
    "abstract": "Recent advances in AI -- including generative approaches -- have resulted in\ntechnology that can support humans in scientific discovery and decision support\nbut may also disrupt democracies and target individuals. The responsible use of\nAI increasingly shows the need for human-AI teaming, necessitating effective\ninteraction between humans and machines. A crucial yet often overlooked aspect\nof these interactions is the different ways in which humans and machines\ngeneralise. In cognitive science, human generalisation commonly involves\nabstraction and concept learning. In contrast, AI generalisation encompasses\nout-of-domain generalisation in machine learning, rule-based reasoning in\nsymbolic AI, and abstraction in neuro-symbolic AI. In this perspective paper,\nwe combine insights from AI and cognitive science to identify key commonalities\nand differences across three dimensions: notions of generalisation, methods for\ngeneralisation, and evaluation of generalisation. We map the different\nconceptualisations of generalisation in AI and cognitive science along these\nthree dimensions and consider their role in human-AI teaming. This results in\ninterdisciplinary challenges across AI and cognitive science that must be\ntackled to provide a foundation for effective and cognitively supported\nalignment in human-AI teaming scenarios.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15626v1",
    "published_date": "2024-11-23 18:36:07 UTC",
    "updated_date": "2024-11-23 18:36:07 UTC"
  },
  {
    "arxiv_id": "2411.16740v3",
    "title": "Document Haystacks: Vision-Language Reasoning Over Piles of 1000+ Documents",
    "authors": [
      "Jun Chen",
      "Dannong Xu",
      "Junjie Fei",
      "Chun-Mei Feng",
      "Mohamed Elhoseiny"
    ],
    "abstract": "Large multimodal models (LMMs) have achieved impressive progress in\nvision-language understanding, yet they face limitations in real-world\napplications requiring complex reasoning over a large number of images.\nExisting benchmarks for multi-image question-answering are limited in scope,\neach question is paired with only up to 30 images, which does not fully capture\nthe demands of large-scale retrieval tasks encountered in the real-world\nusages. To reduce these gaps, we introduce two document haystack benchmarks,\ndubbed DocHaystack and InfoHaystack, designed to evaluate LMM performance on\nlarge-scale visual document retrieval and understanding. Additionally, we\npropose V-RAG, a novel, vision-centric retrieval-augmented generation (RAG)\nframework that leverages a suite of multimodal vision encoders, each optimized\nfor specific strengths, and a dedicated question-document relevance module.\nV-RAG sets a new standard, with a 9% and 11% improvement in Recall@1 on the\nchallenging DocHaystack-1000 and InfoHaystack-1000 benchmarks, respectively,\ncompared to the previous best baseline models. Additionally, integrating V-RAG\nwith LMMs enables them to efficiently operate across thousands of images,\nyielding significant improvements on our DocHaystack and InfoHaystack\nbenchmarks. Our code and datasets are available at\nhttps://github.com/Vision-CAIR/dochaystacks",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "the correct arxiv version",
    "pdf_url": "http://arxiv.org/pdf/2411.16740v3",
    "published_date": "2024-11-23 18:14:42 UTC",
    "updated_date": "2024-12-06 13:10:23 UTC"
  },
  {
    "arxiv_id": "2411.15600v1",
    "title": "How Texts Help? A Fine-grained Evaluation to Reveal the Role of Language in Vision-Language Tracking",
    "authors": [
      "Xuchen Li",
      "Shiyu Hu",
      "Xiaokun Feng",
      "Dailing Zhang",
      "Meiqi Wu",
      "Jing Zhang",
      "Kaiqi Huang"
    ],
    "abstract": "Vision-language tracking (VLT) extends traditional single object tracking by\nincorporating textual information, providing semantic guidance to enhance\ntracking performance under challenging conditions like fast motion and\ndeformations. However, current VLT trackers often underperform compared to\nsingle-modality methods on multiple benchmarks, with semantic information\nsometimes becoming a \"distraction.\" To address this, we propose VLTVerse, the\nfirst fine-grained evaluation framework for VLT trackers that comprehensively\nconsiders multiple challenge factors and diverse semantic information, hoping\nto reveal the role of language in VLT. Our contributions include: (1) VLTVerse\nintroduces 10 sequence-level challenge labels and 6 types of multi-granularity\nsemantic information, creating a flexible and multi-dimensional evaluation\nspace for VLT; (2) leveraging 60 subspaces formed by combinations of challenge\nfactors and semantic types, we conduct systematic fine-grained evaluations of\nthree mainstream SOTA VLT trackers, uncovering their performance bottlenecks\nacross complex scenarios and offering a novel perspective on VLT evaluation;\n(3) through decoupled analysis of experimental results, we examine the impact\nof various semantic types on specific challenge factors in relation to\ndifferent algorithms, providing essential guidance for enhancing VLT across\ndata, evaluation, and algorithmic dimensions. The VLTVerse, toolkit, and\nresults will be available at \\url{http://metaverse.aitestunion.com}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Preprint, Under Review",
    "pdf_url": "http://arxiv.org/pdf/2411.15600v1",
    "published_date": "2024-11-23 16:31:40 UTC",
    "updated_date": "2024-11-23 16:31:40 UTC"
  },
  {
    "arxiv_id": "2411.16739v1",
    "title": "Gradient-Guided Parameter Mask for Multi-Scenario Image Restoration Under Adverse Weather",
    "authors": [
      "Jilong Guo",
      "Haobo Yang",
      "Mo Zhou",
      "Xinyu Zhang"
    ],
    "abstract": "Removing adverse weather conditions such as rain, raindrop, and snow from\nimages is critical for various real-world applications, including autonomous\ndriving, surveillance, and remote sensing. However, existing multi-task\napproaches typically rely on augmenting the model with additional parameters to\nhandle multiple scenarios. While this enables the model to address diverse\ntasks, the introduction of extra parameters significantly complicates its\npractical deployment. In this paper, we propose a novel Gradient-Guided\nParameter Mask for Multi-Scenario Image Restoration under adverse weather,\ndesigned to effectively handle image degradation under diverse weather\nconditions without additional parameters. Our method segments model parameters\ninto common and specific components by evaluating the gradient variation\nintensity during training for each specific weather condition. This enables the\nmodel to precisely and adaptively learn relevant features for each weather\nscenario, improving both efficiency and effectiveness without compromising on\nperformance. This method constructs specific masks based on gradient\nfluctuations to isolate parameters influenced by other tasks, ensuring that the\nmodel achieves strong performance across all scenarios without adding extra\nparameters. We demonstrate the state-of-the-art performance of our framework\nthrough extensive experiments on multiple benchmark datasets. Specifically, our\nmethod achieves PSNR scores of 29.22 on the Raindrop dataset, 30.76 on the Rain\ndataset, and 29.56 on the Snow100K dataset. Code is available at:\n\\href{https://github.com/AierLab/MultiTask}{https://github.com/AierLab/MultiTask}.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.16739v1",
    "published_date": "2024-11-23 16:16:27 UTC",
    "updated_date": "2024-11-23 16:16:27 UTC"
  },
  {
    "arxiv_id": "2411.15595v2",
    "title": "An adversarial feature learning based semantic communication method for Human 3D Reconstruction",
    "authors": [
      "Shaojiang Liu",
      "Jiajun Zou",
      "Zhendan Liu",
      "Meixia Dong",
      "Zhiping Wan"
    ],
    "abstract": "With the widespread application of human body 3D reconstruction technology\nacross various fields, the demands for data transmission and processing\nefficiency continue to rise, particularly in scenarios where network bandwidth\nis limited and low latency is required. This paper introduces an Adversarial\nFeature Learning-based Semantic Communication method (AFLSC) for human body 3D\nreconstruction, which focuses on extracting and transmitting semantic\ninformation crucial for the 3D reconstruction task, thereby significantly\noptimizing data flow and alleviating bandwidth pressure. At the sender's end,\nwe propose a multitask learning-based feature extraction method to capture the\nspatial layout, keypoints, posture, and depth information from 2D human images,\nand design a semantic encoding technique based on adversarial feature learning\nto encode these feature information into semantic data. We also develop a\ndynamic compression technique to efficiently transmit this semantic data,\ngreatly enhancing transmission efficiency and reducing latency. At the\nreceiver's end, we design an efficient multi-level semantic feature decoding\nmethod to convert semantic data back into key image features. Finally, an\nimproved ViT-diffusion model is employed for 3D reconstruction, producing human\nbody 3D mesh models. Experimental results validate the advantages of our method\nin terms of data transmission efficiency and reconstruction quality,\ndemonstrating its excellent potential for application in bandwidth-limited\nenvironments.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "It was published to arXiv without consulting the corresponding\n  author, so the corresponding author requested a withdrawal first",
    "pdf_url": "http://arxiv.org/pdf/2411.15595v2",
    "published_date": "2024-11-23 16:09:53 UTC",
    "updated_date": "2024-12-15 14:39:18 UTC"
  },
  {
    "arxiv_id": "2411.15594v5",
    "title": "A Survey on LLM-as-a-Judge",
    "authors": [
      "Jiawei Gu",
      "Xuhui Jiang",
      "Zhichao Shi",
      "Hexiang Tan",
      "Xuehao Zhai",
      "Chengjin Xu",
      "Wei Li",
      "Yinghan Shen",
      "Shengjie Ma",
      "Honghao Liu",
      "Saizhuo Wang",
      "Kun Zhang",
      "Yuanzhuo Wang",
      "Wen Gao",
      "Lionel Ni",
      "Jian Guo"
    ],
    "abstract": "Accurate and consistent evaluation is crucial for decision-making across\nnumerous fields, yet it remains a challenging task due to inherent\nsubjectivity, variability, and scale. Large Language Models (LLMs) have\nachieved remarkable success across diverse domains, leading to the emergence of\n\"LLM-as-a-Judge,\" where LLMs are employed as evaluators for complex tasks. With\ntheir ability to process diverse data types and provide scalable,\ncost-effective, and consistent assessments, LLMs present a compelling\nalternative to traditional expert-driven evaluations. However, ensuring the\nreliability of LLM-as-a-Judge systems remains a significant challenge that\nrequires careful design and standardization. This paper provides a\ncomprehensive survey of LLM-as-a-Judge, addressing the core question: How can\nreliable LLM-as-a-Judge systems be built? We explore strategies to enhance\nreliability, including improving consistency, mitigating biases, and adapting\nto diverse assessment scenarios. Additionally, we propose methodologies for\nevaluating the reliability of LLM-as-a-Judge systems, supported by a novel\nbenchmark designed for this purpose. To advance the development and real-world\ndeployment of LLM-as-a-Judge systems, we also discussed practical applications,\nchallenges, and future directions. This survey serves as a foundational\nreference for researchers and practitioners in this rapidly evolving field.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Project Page: https://awesome-llm-as-a-judge.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2411.15594v5",
    "published_date": "2024-11-23 16:03:35 UTC",
    "updated_date": "2025-03-09 05:21:22 UTC"
  },
  {
    "arxiv_id": "2411.15589v1",
    "title": "Deep Learning for THz Channel Estimation and Beamforming Prediction via Sub-6GHz Channel",
    "authors": [
      "Sagnik Bhattacharya",
      "Abhishek K. Gupta"
    ],
    "abstract": "An efficient channel estimation is of vital importance to help THz\ncommunication systems achieve their full potential. Conventional uplink channel\nestimation methods, such as least square estimation, are practically\ninefficient for THz systems because of their large computation overhead. In\nthis paper, we propose an efficient convolutional neural network (CNN) based\nTHz channel estimator that estimates the THz channel factors using uplink\nsub-6GHz channel. Further, we use the estimated THz channel factors to predict\nthe optimal beamformer from a pre-given codebook, using a dense neural network.\nWe not only get rid of the overhead associated with the conventional methods,\nbut also achieve near-optimal spectral efficiency rates using the proposed\nbeamformer predictor. The proposed method also outperforms deep learning based\nbeamformer predictors accepting THz channel matrices as input, thus proving the\nvalidity and efficiency of our sub-6GHz based approach.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "Published: 2022 IEEE International Conference on Signal Processing\n  and Communications (SPCOM 2022)",
    "pdf_url": "http://arxiv.org/pdf/2411.15589v1",
    "published_date": "2024-11-23 15:36:35 UTC",
    "updated_date": "2024-11-23 15:36:35 UTC"
  },
  {
    "arxiv_id": "2411.16738v2",
    "title": "Classifier-Free Guidance inside the Attraction Basin May Cause Memorization",
    "authors": [
      "Anubhav Jain",
      "Yuya Kobayashi",
      "Takashi Shibuya",
      "Yuhta Takida",
      "Nasir Memon",
      "Julian Togelius",
      "Yuki Mitsufuji"
    ],
    "abstract": "Diffusion models are prone to exactly reproduce images from the training\ndata. This exact reproduction of the training data is concerning as it can lead\nto copyright infringement and/or leakage of privacy-sensitive information. In\nthis paper, we present a novel perspective on the memorization phenomenon and\npropose a simple yet effective approach to mitigate it. We argue that\nmemorization occurs because of an attraction basin in the denoising process\nwhich steers the diffusion trajectory towards a memorized image. However, this\ncan be mitigated by guiding the diffusion trajectory away from the attraction\nbasin by not applying classifier-free guidance until an ideal transition point\noccurs from which classifier-free guidance is applied. This leads to the\ngeneration of non-memorized images that are high in image quality and\nwell-aligned with the conditioning mechanism. To further improve on this, we\npresent a new guidance technique, opposite guidance, that escapes the\nattraction basin sooner in the denoising process. We demonstrate the existence\nof attraction basins in various scenarios in which memorization occurs, and we\nshow that our proposed approach successfully mitigates memorization.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2411.16738v2",
    "published_date": "2024-11-23 15:36:03 UTC",
    "updated_date": "2025-03-17 15:50:58 UTC"
  },
  {
    "arxiv_id": "2411.15560v2",
    "title": "Do LLMs Agree on the Creativity Evaluation of Alternative Uses?",
    "authors": [
      "Abdullah Al Rabeyah",
      "Fabrício Góes",
      "Marco Volpe",
      "Talles Medeiros"
    ],
    "abstract": "This paper investigates whether large language models (LLMs) show agreement\nin assessing creativity in responses to the Alternative Uses Test (AUT). While\nLLMs are increasingly used to evaluate creative content, previous studies have\nprimarily focused on a single model assessing responses generated by the same\nmodel or humans. This paper explores whether LLMs can impartially and\naccurately evaluate creativity in outputs generated by both themselves and\nother models. Using an oracle benchmark set of AUT responses, categorized by\ncreativity level (common, creative, and highly creative), we experiment with\nfour state-of-the-art LLMs evaluating these outputs. We test both scoring and\nranking methods and employ two evaluation settings (comprehensive and\nsegmented) to examine if LLMs agree on the creativity evaluation of alternative\nuses. Results reveal high inter-model agreement, with Spearman correlations\naveraging above 0.7 across models and reaching over 0.77 with respect to the\noracle, indicating a high level of agreement and validating the reliability of\nLLMs in creativity assessment of alternative uses. Notably, models do not\nfavour their own responses, instead they provide similar creativity assessment\nscores or rankings for alternative uses generated by other models. These\nfindings suggest that LLMs exhibit impartiality and high alignment in\ncreativity evaluation, offering promising implications for their use in\nautomated creativity assessment.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "19 pages, 7 figures, 15 tables",
    "pdf_url": "http://arxiv.org/pdf/2411.15560v2",
    "published_date": "2024-11-23 13:34:50 UTC",
    "updated_date": "2024-11-26 09:25:22 UTC"
  },
  {
    "arxiv_id": "2411.15557v3",
    "title": "LAGUNA: LAnguage Guided UNsupervised Adaptation with structured spaces",
    "authors": [
      "Anxhelo Diko",
      "Antonino Furnari",
      "Luigi Cinque",
      "Giovanni Maria Farinella"
    ],
    "abstract": "Unsupervised domain adaptation remains a critical challenge in enabling the\nknowledge transfer of models across unseen domains. Existing methods struggle\nto balance the need for domain-invariant representations with preserving\ndomain-specific features, which is often due to alignment approaches that\nimpose the projection of samples with similar semantics close in the latent\nspace despite their drastic domain differences. We introduce LAGUNA - LAnguage\nGuided UNsupervised Adaptation with structured spaces, a novel approach that\nshifts the focus from aligning representations in absolute coordinates to\naligning the relative positioning of equivalent concepts in latent spaces.\nLAGUNA defines a domain-agnostic structure upon the semantic/geometric\nrelationships between class labels in language space and guides adaptation,\nensuring that the organization of samples in visual space reflects reference\ninter-class relationships while preserving domain-specific characteristics. We\nempirically demonstrate LAGUNA's superiority in domain adaptation tasks across\nfour diverse images and video datasets. Remarkably, LAGUNA surpasses previous\nworks in 18 different adaptation scenarios across four diverse image and video\ndatasets with average accuracy improvements of +3.32% on DomainNet, +5.75% in\nGeoPlaces, +4.77% on GeoImnet, and +1.94% mean class accuracy improvement on\nEgoExo4D.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15557v3",
    "published_date": "2024-11-23 13:26:53 UTC",
    "updated_date": "2025-03-27 22:59:47 UTC"
  },
  {
    "arxiv_id": "2411.15556v2",
    "title": "ReWind: Understanding Long Videos with Instructed Learnable Memory",
    "authors": [
      "Anxhelo Diko",
      "Tinghuai Wang",
      "Wassim Swaileh",
      "Shiyan Sun",
      "Ioannis Patras"
    ],
    "abstract": "Vision-Language Models (VLMs) are crucial for applications requiring\nintegrated understanding textual and visual information. However, existing VLMs\nstruggle with long videos due to computational inefficiency, memory\nlimitations, and difficulties in maintaining coherent understanding across\nextended sequences. To address these challenges, we introduce ReWind, a novel\nmemory-based VLM designed for efficient long video understanding while\npreserving temporal fidelity. ReWind operates in a two-stage framework. In the\nfirst stage, ReWind maintains a dynamic learnable memory module with a novel\n\\textbf{read-perceive-write} cycle that stores and updates instruction-relevant\nvisual information as the video unfolds. This module utilizes learnable queries\nand cross-attentions between memory contents and the input stream, ensuring low\nmemory requirements by scaling linearly with the number of tokens. In the\nsecond stage, we propose an adaptive frame selection mechanism guided by the\nmemory content to identify instruction-relevant key moments. It enriches the\nmemory representations with detailed spatial information by selecting a few\nhigh-resolution frames, which are then combined with the memory contents and\nfed into a Large Language Model (LLM) to generate the final answer. We\nempirically demonstrate ReWind's superior performance in visual question\nanswering (VQA) and temporal grounding tasks, surpassing previous methods on\nlong video benchmarks. Notably, ReWind achieves a +13\\% score gain and a +12\\%\naccuracy improvement on the MovieChat-1K VQA dataset and an +8\\% mIoU increase\non Charades-STA for temporal grounding.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15556v2",
    "published_date": "2024-11-23 13:23:22 UTC",
    "updated_date": "2025-03-27 23:05:29 UTC"
  },
  {
    "arxiv_id": "2411.15550v1",
    "title": "Class Order Disorder in Wikidata and First Fixes",
    "authors": [
      "Peter F. Patel-Schneider",
      "Ege Atacan Doğan"
    ],
    "abstract": "Wikidata has a large ontology with classes at several orders. The Wikidata\nontology has long been known to have violations of class order and information\nrelated to class order that appears suspect. SPARQL queries were evaluated\nagainst Wikidata to determine the prevalence of several kinds of violations and\nsuspect information and the results analyzed. Some changes were manually made\nto Wikidata to remove some of these results and the queries rerun, showing the\neffect of the changes. Suggestions are provided on how the problems uncovered\nmight be addressed, either though better tooling or involvement of the Wikidata\ncommunity.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "I.2.4"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15550v1",
    "published_date": "2024-11-23 13:15:13 UTC",
    "updated_date": "2024-11-23 13:15:13 UTC"
  },
  {
    "arxiv_id": "2411.15548v2",
    "title": "An unconditional distribution learning advantage with shallow quantum circuits",
    "authors": [
      "N. Pirnay",
      "S. Jerbi",
      "J. -P. Seifert",
      "J. Eisert"
    ],
    "abstract": "One of the core challenges of research in quantum computing is concerned with\nthe question whether quantum advantages can be found for near-term quantum\ncircuits that have implications for practical applications. Motivated by this\nmindset, in this work, we prove an unconditional quantum advantage in the\nprobably approximately correct (PAC) distribution learning framework with\nshallow quantum circuit hypotheses. We identify a meaningful generative\ndistribution learning problem where constant-depth quantum circuits using one\nand two qubit gates (QNC^0) are superior compared to constant-depth bounded\nfan-in classical circuits (NC^0) as a choice for hypothesis classes. We hence\nprove a PAC distribution learning separation for shallow quantum circuits over\nshallow classical circuits. We do so by building on recent results by Bene\nWatts and Parham on unconditional quantum advantages for sampling tasks with\nshallow circuits, which we technically uplift to a hyperplane learning problem,\nidentifying non-local correlations as the origin of the quantum advantage.",
    "categories": [
      "quant-ph",
      "cs.AI"
    ],
    "primary_category": "quant-ph",
    "comment": "7 + 5 pages, 2 figures, added an acknowledgement",
    "pdf_url": "http://arxiv.org/pdf/2411.15548v2",
    "published_date": "2024-11-23 13:03:22 UTC",
    "updated_date": "2024-11-27 02:44:36 UTC"
  },
  {
    "arxiv_id": "2411.16736v1",
    "title": "ChemSafetyBench: Benchmarking LLM Safety on Chemistry Domain",
    "authors": [
      "Haochen Zhao",
      "Xiangru Tang",
      "Ziran Yang",
      "Xiao Han",
      "Xuanzhi Feng",
      "Yueqing Fan",
      "Senhao Cheng",
      "Di Jin",
      "Yilun Zhao",
      "Arman Cohan",
      "Mark Gerstein"
    ],
    "abstract": "The advancement and extensive application of large language models (LLMs)\nhave been remarkable, including their use in scientific research assistance.\nHowever, these models often generate scientifically incorrect or unsafe\nresponses, and in some cases, they may encourage users to engage in dangerous\nbehavior. To address this issue in the field of chemistry, we introduce\nChemSafetyBench, a benchmark designed to evaluate the accuracy and safety of\nLLM responses. ChemSafetyBench encompasses three key tasks: querying chemical\nproperties, assessing the legality of chemical uses, and describing synthesis\nmethods, each requiring increasingly deeper chemical knowledge. Our dataset has\nmore than 30K samples across various chemical materials. We incorporate\nhandcrafted templates and advanced jailbreaking scenarios to enhance task\ndiversity. Our automated evaluation framework thoroughly assesses the safety,\naccuracy, and appropriateness of LLM responses. Extensive experiments with\nstate-of-the-art LLMs reveal notable strengths and critical vulnerabilities,\nunderscoring the need for robust safety measures. ChemSafetyBench aims to be a\npivotal tool in developing safer AI technologies in chemistry. Our code and\ndataset are available at https://github.com/HaochenZhao/SafeAgent4Chem.\nWarning: this paper contains discussions on the synthesis of controlled\nchemicals using AI models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "physics.chem-ph"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.16736v1",
    "published_date": "2024-11-23 12:50:33 UTC",
    "updated_date": "2024-11-23 12:50:33 UTC"
  },
  {
    "arxiv_id": "2411.15540v2",
    "title": "Optical-Flow Guided Prompt Optimization for Coherent Video Generation",
    "authors": [
      "Hyelin Nam",
      "Jaemin Kim",
      "Dohun Lee",
      "Jong Chul Ye"
    ],
    "abstract": "While text-to-video diffusion models have made significant strides, many\nstill face challenges in generating videos with temporal consistency. Within\ndiffusion frameworks, guidance techniques have proven effective in enhancing\noutput quality during inference; however, applying these methods to video\ndiffusion models introduces additional complexity of handling computations\nacross entire sequences. To address this, we propose a novel framework called\nMotionPrompt that guides the video generation process via optical flow.\nSpecifically, we train a discriminator to distinguish optical flow between\nrandom pairs of frames from real videos and generated ones. Given that prompts\ncan influence the entire video, we optimize learnable token embeddings during\nreverse sampling steps by using gradients from a trained discriminator applied\nto random frame pairs. This approach allows our method to generate visually\ncoherent video sequences that closely reflect natural motion dynamics, without\ncompromising the fidelity of the generated content. We demonstrate the\neffectiveness of our approach across various models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2025 (poster); project page: https://motionprompt.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2411.15540v2",
    "published_date": "2024-11-23 12:26:52 UTC",
    "updated_date": "2025-03-23 07:34:32 UTC"
  },
  {
    "arxiv_id": "2411.15539v2",
    "title": "Large Language Model with Region-guided Referring and Grounding for CT Report Generation",
    "authors": [
      "Zhixuan Chen",
      "Yequan Bie",
      "Haibo Jin",
      "Hao Chen"
    ],
    "abstract": "Computed tomography (CT) report generation is crucial to assist radiologists\nin interpreting CT volumes, which can be time-consuming and labor-intensive.\nExisting methods primarily only consider the global features of the entire\nvolume, making it struggle to focus on specific regions and potentially missing\nabnormalities. To address this issue, we propose Reg2RG, the first\nregion-guided referring and grounding framework for CT report generation, which\nenhances diagnostic performance by focusing on anatomical regions within the\nvolume. Specifically, we utilize masks from a universal segmentation module to\ncapture local features for each referring region. A local feature decoupling\n(LFD) strategy is proposed to preserve the local high-resolution details with\nlittle computational overhead. Then the local features are integrated with\nglobal features to capture inter-regional relationships within a cohesive\ncontext. Moreover, we propose a novel region-report alignment (RRA) training\nstrategy. It leverages the recognition of referring regions to guide the\ngeneration of region-specific reports, enhancing the model's referring and\ngrounding capabilities while also improving the report's interpretability. A\nlarge language model (LLM) is further employed as the language decoder to\ngenerate reports from integrated visual features, facilitating region-level\ncomprehension. Extensive experiments on two large-scale chest CT-report\ndatasets demonstrate the superiority of our method, which outperforms several\nstate-of-the-art methods in terms of both natural language generation and\nclinical efficacy metrics while preserving promising interpretability. The code\nis available at https://github.com/zhi-xuan-chen/Reg2RG.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages",
    "pdf_url": "http://arxiv.org/pdf/2411.15539v2",
    "published_date": "2024-11-23 12:25:06 UTC",
    "updated_date": "2025-05-05 02:18:33 UTC"
  },
  {
    "arxiv_id": "2411.15523v1",
    "title": "Enhancing Grammatical Error Detection using BERT with Cleaned Lang-8 Dataset",
    "authors": [
      "Rahul Nihalani",
      "Kushal Shah"
    ],
    "abstract": "This paper presents an improved LLM based model for Grammatical Error\nDetection (GED), which is a very challenging and equally important problem for\nmany applications. The traditional approach to GED involved hand-designed\nfeatures, but recently, Neural Networks (NN) have automated the discovery of\nthese features, improving performance in GED. Traditional rule-based systems\nhave an F1 score of 0.50-0.60 and earlier machine learning models give an F1\nscore of 0.65-0.75, including decision trees and simple neural networks.\nPrevious deep learning models, for example, Bi-LSTM, have reported F1 scores\nwithin the range from 0.80 to 0.90. In our study, we have fine-tuned various\ntransformer models using the Lang8 dataset rigorously cleaned by us. In our\nexperiments, the BERT-base-uncased model gave an impressive performance with an\nF1 score of 0.91 and accuracy of 98.49% on training data and 90.53% on testing\ndata, also showcasing the importance of data cleaning. Increasing model size\nusing BERT-large-uncased or RoBERTa-large did not give any noticeable\nimprovements in performance or advantage for this task, underscoring that\nlarger models are not always better. Our results clearly show how far rigorous\ndata cleaning and simple transformer-based models can go toward significantly\nimproving the quality of GED.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 6 tables, 20 references",
    "pdf_url": "http://arxiv.org/pdf/2411.15523v1",
    "published_date": "2024-11-23 10:57:41 UTC",
    "updated_date": "2024-11-23 10:57:41 UTC"
  },
  {
    "arxiv_id": "2411.15509v1",
    "title": "Interactive Visual Assessment for Text-to-Image Generation Models",
    "authors": [
      "Xiaoyue Mi",
      "Fan Tang",
      "Juan Cao",
      "Qiang Sheng",
      "Ziyao Huang",
      "Peng Li",
      "Yang Liu",
      "Tong-Yee Lee"
    ],
    "abstract": "Visual generation models have achieved remarkable progress in computer\ngraphics applications but still face significant challenges in real-world\ndeployment. Current assessment approaches for visual generation tasks typically\nfollow an isolated three-phase framework: test input collection, model output\ngeneration, and user assessment. These fashions suffer from fixed coverage,\nevolving difficulty, and data leakage risks, limiting their effectiveness in\ncomprehensively evaluating increasingly complex generation models. To address\nthese limitations, we propose DyEval, an LLM-powered dynamic interactive visual\nassessment framework that facilitates collaborative evaluation between humans\nand generative models for text-to-image systems. DyEval features an intuitive\nvisual interface that enables users to interactively explore and analyze model\nbehaviors, while adaptively generating hierarchical, fine-grained, and diverse\ntextual inputs to continuously probe the capability boundaries of the models\nbased on their feedback. Additionally, to provide interpretable analysis for\nusers to further improve tested models, we develop a contextual reflection\nmodule that mines failure triggers of test inputs and reflects model potential\nfailure patterns supporting in-depth analysis using the logical reasoning\nability of LLM. Qualitative and quantitative experiments demonstrate that\nDyEval can effectively help users identify max up to 2.56 times generation\nfailures than conventional methods, and uncover complex and rare failure\npatterns, such as issues with pronoun generation and specific cultural context\ngeneration. Our framework provides valuable insights for improving generative\nmodels and has broad implications for advancing the reliability and\ncapabilities of visual generation systems across various domains.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Under Review",
    "pdf_url": "http://arxiv.org/pdf/2411.15509v1",
    "published_date": "2024-11-23 10:06:18 UTC",
    "updated_date": "2024-11-23 10:06:18 UTC"
  },
  {
    "arxiv_id": "2411.15501v1",
    "title": "Instruct or Interact? Exploring and Eliciting LLMs' Capability in Code Snippet Adaptation Through Prompt Engineering",
    "authors": [
      "Tanghaoran Zhang",
      "Yue Yu",
      "Xinjun Mao",
      "Shangwen Wang",
      "Kang Yang",
      "Yao Lu",
      "Zhang Zhang",
      "Yuxin Zhao"
    ],
    "abstract": "Code snippet adaptation is a fundamental activity in the software development\nprocess. Unlike code generation, code snippet adaptation is not a \"free\ncreation\", which requires developers to tailor a given code snippet in order to\nfit specific requirements and the code context. Recently, large language models\n(LLMs) have confirmed their effectiveness in the code generation task with\npromising results. However, their performance on adaptation, a reuse-oriented\nand context-dependent code change prediction task, is still unclear. To bridge\nthis gap, we conduct an empirical study to investigate the performance and\nissues of LLMs on the adaptation task. We first evaluate the adaptation\nperformances of three popular LLMs and compare them to the code generation\ntask. Our result indicates that their adaptation ability is weaker than\ngeneration, with a nearly 15% decrease on pass@1 and more context-related\nerrors. By manually inspecting 200 cases, we further investigate the causes of\nLLMs' sub-optimal performance, which can be classified into three categories,\ni.e., Unclear Requirement, Requirement Misalignment and Context Misapplication.\nBased on the above empirical research, we propose an interactive prompting\napproach to eliciting LLMs' adaptation ability. Experimental result reveals\nthat our approach greatly improve LLMs' adaptation performance. The\nbest-performing Human-LLM interaction successfully solves 159 out of the 202\nidentified defects and improves the pass@1 and pass@5 by over 40% compared to\nthe initial instruction-based prompt. Considering human efforts, we suggest\nmulti-agent interaction as a trade-off, which can achieve comparable\nperformance with excellent generalization ability. We deem that our approach\ncould provide methodological assistance for autonomous code snippet reuse and\nadaptation with LLMs.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.SE",
    "comment": "12 pages, 10 figures, accepted by ICSE 2025",
    "pdf_url": "http://arxiv.org/pdf/2411.15501v1",
    "published_date": "2024-11-23 09:40:36 UTC",
    "updated_date": "2024-11-23 09:40:36 UTC"
  },
  {
    "arxiv_id": "2411.16730v4",
    "title": "\"Moralized\" Multi-Step Jailbreak Prompts: Black-Box Testing of Guardrails in Large Language Models for Verbal Attacks",
    "authors": [
      "Libo Wang"
    ],
    "abstract": "As the application of large language models continues to expand in various\nfields, it poses higher challenges to the effectiveness of identifying harmful\ncontent generation and guardrail mechanisms. This research aims to evaluate the\nguardrail effectiveness of GPT-4o, Grok-2 Beta, Llama 3.1 (405B), Gemini 1.5,\nand Claude 3.5 Sonnet through black-box testing of seemingly ethical multi-step\njailbreak prompts. It conducts ethical attacks by designing an identical\nmulti-step prompts that simulates the scenario of \"corporate middle managers\ncompeting for promotions.\" The data results show that the guardrails of the\nabove-mentioned LLMs were bypassed and the content of verbal attacks was\ngenerated. Claude 3.5 Sonnet's resistance to multi-step jailbreak prompts is\nmore obvious. To ensure objectivity, the experimental process, black box test\ncode, and enhanced guardrail code are uploaded to the GitHub repository:\nhttps://github.com/brucewang123456789/GeniusTrail.git.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "This paper has been submitted to Nature Machine Intelligence and\n  OpenReview preprints. It has 7 pages of text, 3 figures, and 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2411.16730v4",
    "published_date": "2024-11-23 09:32:44 UTC",
    "updated_date": "2025-03-20 14:48:10 UTC"
  },
  {
    "arxiv_id": "2411.15488v1",
    "title": "Automatic Evaluation for Text-to-image Generation: Task-decomposed Framework, Distilled Training, and Meta-evaluation Benchmark",
    "authors": [
      "Rong-Cheng Tu",
      "Zi-Ao Ma",
      "Tian Lan",
      "Yuehao Zhao",
      "Heyan Huang",
      "Xian-Ling Mao"
    ],
    "abstract": "Driven by the remarkable progress in diffusion models, text-to-image\ngeneration has made significant strides, creating a pressing demand for\nautomatic quality evaluation of generated images. Current state-of-the-art\nautomatic evaluation methods heavily rely on Multi-modal Large Language Models\n(MLLMs), particularly powerful commercial models like GPT-4o. While these\nmodels are highly effective, their substantial costs limit scalability in\nlarge-scale evaluations. Adopting open-source MLLMs is an alternative; however,\ntheir performance falls short due to significant limitations in processing\nmulti-modal data compared to commercial MLLMs. To tackle these problems, we\nfirst propose a task decomposition evaluation framework based on GPT-4o to\nautomatically construct a new training dataset, where the complex evaluation\ntask is decoupled into simpler sub-tasks, effectively reducing the learning\ncomplexity. Based on this dataset, we design innovative training strategies to\neffectively distill GPT-4o's evaluation capabilities into a 7B open-source\nMLLM, MiniCPM-V-2.6. Furthermore, to reliably and comprehensively assess prior\nworks and our proposed model, we manually annotate a meta-evaluation benchmark\nthat includes chain-of-thought explanations alongside quality scores for\ngenerated images. Experimental results demonstrate that our distilled\nopen-source MLLM significantly outperforms the current state-of-the-art\nGPT-4o-base baseline, VIEScore, with over 4.6\\% improvement in Spearman and\nKendall correlations with human judgments.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15488v1",
    "published_date": "2024-11-23 08:06:06 UTC",
    "updated_date": "2024-11-23 08:06:06 UTC"
  },
  {
    "arxiv_id": "2411.16729v1",
    "title": "DiM-Gestor: Co-Speech Gesture Generation with Adaptive Layer Normalization Mamba-2",
    "authors": [
      "Fan Zhang",
      "Siyuan Zhao",
      "Naye Ji",
      "Zhaohan Wang",
      "Jingmei Wu",
      "Fuxing Gao",
      "Zhenqing Ye",
      "Leyao Yan",
      "Lanxin Dai",
      "Weidong Geng",
      "Xin Lyu",
      "Bozuo Zhao",
      "Dingguo Yu",
      "Hui Du",
      "Bin Hu"
    ],
    "abstract": "Speech-driven gesture generation using transformer-based generative models\nrepresents a rapidly advancing area within virtual human creation. However,\nexisting models face significant challenges due to their quadratic time and\nspace complexities, limiting scalability and efficiency. To address these\nlimitations, we introduce DiM-Gestor, an innovative end-to-end generative model\nleveraging the Mamba-2 architecture. DiM-Gestor features a dual-component\nframework: (1) a fuzzy feature extractor and (2) a speech-to-gesture mapping\nmodule, both built on the Mamba-2. The fuzzy feature extractor, integrated with\na Chinese Pre-trained Model and Mamba-2, autonomously extracts implicit,\ncontinuous speech features. These features are synthesized into a unified\nlatent representation and then processed by the speech-to-gesture mapping\nmodule. This module employs an Adaptive Layer Normalization (AdaLN)-enhanced\nMamba-2 mechanism to uniformly apply transformations across all sequence\ntokens. This enables precise modeling of the nuanced interplay between speech\nfeatures and gesture dynamics. We utilize a diffusion model to train and infer\ndiverse gesture outputs. Extensive subjective and objective evaluations\nconducted on the newly released Chinese Co-Speech Gestures dataset corroborate\nthe efficacy of our proposed model. Compared with Transformer-based\narchitecture, the assessments reveal that our approach delivers competitive\nresults and significantly reduces memory usage, approximately 2.4 times, and\nenhances inference speeds by 2 to 4 times. Additionally, we released the CCG\ndataset, a Chinese Co-Speech Gestures dataset, comprising 15.97 hours (six\nstyles across five scenarios) of 3D full-body skeleton gesture motion performed\nby professional Chinese TV broadcasters.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.GR",
      "cs.HC",
      "cs.MM",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "13 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.16729v1",
    "published_date": "2024-11-23 08:02:03 UTC",
    "updated_date": "2024-11-23 08:02:03 UTC"
  },
  {
    "arxiv_id": "2411.16728v1",
    "title": "Maximizing the Impact of Deep Learning on Subseasonal-to-Seasonal Climate Forecasting: The Essential Role of Optimization",
    "authors": [
      "Yizhen Guo",
      "Tian Zhou",
      "Wanyi Jiang",
      "Bo Wu",
      "Liang Sun",
      "Rong Jin"
    ],
    "abstract": "Weather and climate forecasting is vital for sectors such as agriculture and\ndisaster management. Although numerical weather prediction (NWP) systems have\nadvanced, forecasting at the subseasonal-to-seasonal (S2S) scale, spanning 2 to\n6 weeks, remains challenging due to the chaotic and sparse atmospheric signals\nat this interval. Even state-of-the-art deep learning models struggle to\noutperform simple climatology models in this domain. This paper identifies that\noptimization, instead of network structure, could be the root cause of this\nperformance gap, and then we develop a novel multi-stage optimization strategy\nto close the gap. Extensive empirical studies demonstrate that our multi-stage\noptimization approach significantly improves key skill metrics, PCC and TCC,\nwhile utilizing the same backbone structure, surpassing the state-of-the-art\nNWP systems (ECMWF-S2S) by over \\textbf{19-91\\%}. Our research contests the\nrecent study that direct forecasting outperforms rolling forecasting for S2S\ntasks. Through theoretical analysis, we propose that the underperformance of\nrolling forecasting may arise from the accumulation of Jacobian matrix products\nduring training. Our multi-stage framework can be viewed as a form of teacher\nforcing to address this issue. Code is available at\n\\url{https://anonymous.4open.science/r/Baguan-S2S-23E7/}",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.ao-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.16728v1",
    "published_date": "2024-11-23 08:01:54 UTC",
    "updated_date": "2024-11-23 08:01:54 UTC"
  },
  {
    "arxiv_id": "2411.15477v1",
    "title": "Towards Robust Evaluation of Unlearning in LLMs via Data Transformations",
    "authors": [
      "Abhinav Joshi",
      "Shaswati Saha",
      "Divyaksh Shukla",
      "Sriram Vema",
      "Harsh Jhamtani",
      "Manas Gaur",
      "Ashutosh Modi"
    ],
    "abstract": "Large Language Models (LLMs) have shown to be a great success in a wide range\nof applications ranging from regular NLP-based use cases to AI agents. LLMs\nhave been trained on a vast corpus of texts from various sources; despite the\nbest efforts during the data pre-processing stage while training the LLMs, they\nmay pick some undesirable information such as personally identifiable\ninformation (PII). Consequently, in recent times research in the area of\nMachine Unlearning (MUL) has become active, the main idea is to force LLMs to\nforget (unlearn) certain information (e.g., PII) without suffering from\nperformance loss on regular tasks. In this work, we examine the robustness of\nthe existing MUL techniques for their ability to enable leakage-proof\nforgetting in LLMs. In particular, we examine the effect of data transformation\non forgetting, i.e., is an unlearned LLM able to recall forgotten information\nif there is a change in the format of the input? Our findings on the TOFU\ndataset highlight the necessity of using diverse data formats to quantify\nunlearning in LLMs more reliably.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at EMNLP 2024 Findings; 21 pages (5 page main content +\n  references + appendix)",
    "pdf_url": "http://arxiv.org/pdf/2411.15477v1",
    "published_date": "2024-11-23 07:20:36 UTC",
    "updated_date": "2024-11-23 07:20:36 UTC"
  },
  {
    "arxiv_id": "2411.15472v2",
    "title": "KinMo: Kinematic-aware Human Motion Understanding and Generation",
    "authors": [
      "Pengfei Zhang",
      "Pinxin Liu",
      "Hyeongwoo Kim",
      "Pablo Garrido",
      "Bindita Chaudhuri"
    ],
    "abstract": "Current human motion synthesis frameworks rely on global action descriptions,\ncreating a modality gap that limits both motion understanding and generation\ncapabilities. A single coarse description, such as ``run\", fails to capture\ndetails like variations in speed, limb positioning, and kinematic dynamics,\nleading to ambiguities between text and motion modalities. To address this\nchallenge, we introduce \\textbf{KinMo}, a unified framework built on a\nhierarchical describable motion representation that extends beyond global\naction by incorporating kinematic group movements and their interactions. We\ndesign an automated annotation pipeline to generate high-quality, fine-grained\ndescriptions for this decomposition, resulting in the KinMo dataset. To\nleverage these structured descriptions, we propose Hierarchical Text-Motion\nAlignment, improving spatial understanding by integrating additional motion\ndetails. Furthermore, we introduce a coarse-to-fine generation procedure to\nleverage enhanced spatial understanding to improve motion synthesis.\nExperimental results show that KinMo significantly improves motion\nunderstanding, demonstrated by enhanced text-motion retrieval performance and\nenabling more fine-grained motion generation and editing capabilities. Project\nPage: https://andypinxinliu.github.io/KinMo",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15472v2",
    "published_date": "2024-11-23 06:50:11 UTC",
    "updated_date": "2025-03-11 14:29:56 UTC"
  },
  {
    "arxiv_id": "2412.00044v1",
    "title": "Creating Hierarchical Dispositions of Needs in an Agent",
    "authors": [
      "Tofara Moyo"
    ],
    "abstract": "We present a novel method for learning hierarchical abstractions that\nprioritize competing objectives, leading to improved global expected rewards.\nOur approach employs a secondary rewarding agent with multiple scalar outputs,\neach associated with a distinct level of abstraction. The traditional agent\nthen learns to maximize these outputs in a hierarchical manner, conditioning\neach level on the maximization of the preceding level. We derive an equation\nthat orders these scalar values and the global reward by priority, inducing a\nhierarchy of needs that informs goal formation. Experimental results on the\nPendulum v1 environment demonstrate superior performance compared to a baseline\nimplementation.We achieved state of the art results.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "5 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.00044v1",
    "published_date": "2024-11-23 06:41:54 UTC",
    "updated_date": "2024-11-23 06:41:54 UTC"
  },
  {
    "arxiv_id": "2411.15470v1",
    "title": "A Preliminary Study of Multilingual Code Language Models for Code Generation Task Using Translated Benchmarks",
    "authors": [
      "Rohit Dandamudi",
      "Gema Rodríguez-Pérez"
    ],
    "abstract": "Evaluating the performance of Code Language Models (CLMs) for software\nengineering tasks, especially in multilingual and low-resource programming\nlanguage settings, poses significant challenges. These challenges are primarily\ndue to the lack of high-quality benchmarks across various programming languages\nand the imbalanced nature of the CLMs training corpus. Although recent advances\nin one of the common downstream tasks, code generation, have shown promise by\nintroducing translated benchmarks using different methodologies, there is a\ncurrent lack of empirical evidence assessing these benchmarks. To address this\ngap, we conducted a preliminary study to evaluate the performance of\nPoly-Coder, a pioneering open-source, multilingual CLM built for code\ngeneration. We utilized two existing state-of-the-art translations of the\npopular code generation benchmark, HumanEval, facilitated by the OctoPack and\nMultiPL-E studies. Our results suggest that the outcomes observed in these\ntranslated benchmarks align well with evaluation metrics used during the\ntraining phase, such as perplexity, thereby validating their effectiveness in\nestimating the performance of CLMs. However, we identified several\ninconsistencies in the CLMs' performance across the translated benchmarks and\nencountered challenges in replicating the results. These initial insights\nhighlight the need for more comprehensive empirical studies to fully understand\ntranslated benchmarks' methodological approaches, limitations, and\nreproducibility. Such studies are essential to ensure their reliability before\nthey are widely adopted.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.PL"
    ],
    "primary_category": "cs.SE",
    "comment": "5 pages, ASEW 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.15470v1",
    "published_date": "2024-11-23 06:40:47 UTC",
    "updated_date": "2024-11-23 06:40:47 UTC"
  },
  {
    "arxiv_id": "2411.15458v1",
    "title": "TANGNN: a Concise, Scalable and Effective Graph Neural Networks with Top-m Attention Mechanism for Graph Representation Learning",
    "authors": [
      "Jiawei E",
      "Yinglong Zhang",
      "Xuewen Xia",
      "Xing Xu"
    ],
    "abstract": "In the field of deep learning, Graph Neural Networks (GNNs) and Graph\nTransformer models, with their outstanding performance and flexible\narchitectural designs, have become leading technologies for processing\nstructured data, especially graph data. Traditional GNNs often face challenges\nin capturing information from distant vertices effectively. In contrast, Graph\nTransformer models are particularly adept at managing long-distance node\nrelationships. Despite these advantages, Graph Transformer models still\nencounter issues with computational and storage efficiency when scaled to large\ngraph datasets. To address these challenges, we propose an innovative Graph\nNeural Network (GNN) architecture that integrates a Top-m attention mechanism\naggregation component and a neighborhood aggregation component, effectively\nenhancing the model's ability to aggregate relevant information from both local\nand extended neighborhoods at each layer. This method not only improves\ncomputational efficiency but also enriches the node features, facilitating a\ndeeper analysis of complex graph structures. Additionally, to assess the\neffectiveness of our proposed model, we have applied it to citation sentiment\nprediction, a novel task previously unexplored in the GNN field. Accordingly,\nwe constructed a dedicated citation network, ArXivNet. In this dataset, we\nspecifically annotated the sentiment polarity of the citations (positive,\nneutral, negative) to enable in-depth sentiment analysis. Our approach has\nshown superior performance across a variety of tasks including vertex\nclassification, link prediction, sentiment prediction, graph regression, and\nvisualization. It outperforms existing methods in terms of effectiveness, as\ndemonstrated by experimental results on multiple datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "The code and ArXivNet dataset are available at\n  https://github.com/ejwww/TANGNN",
    "pdf_url": "http://arxiv.org/pdf/2411.15458v1",
    "published_date": "2024-11-23 05:31:25 UTC",
    "updated_date": "2024-11-23 05:31:25 UTC"
  },
  {
    "arxiv_id": "2411.15457v1",
    "title": "Hindi audio-video-Deepfake (HAV-DF): A Hindi language-based Audio-video Deepfake Dataset",
    "authors": [
      "Sukhandeep Kaur",
      "Mubashir Buhari",
      "Naman Khandelwal",
      "Priyansh Tyagi",
      "Kiran Sharma"
    ],
    "abstract": "Deepfakes offer great potential for innovation and creativity, but they also\npose significant risks to privacy, trust, and security. With a vast\nHindi-speaking population, India is particularly vulnerable to deepfake-driven\nmisinformation campaigns. Fake videos or speeches in Hindi can have an enormous\nimpact on rural and semi-urban communities, where digital literacy tends to be\nlower and people are more inclined to trust video content. The development of\neffective frameworks and detection tools to combat deepfake misuse requires\nhigh-quality, diverse, and extensive datasets. The existing popular datasets\nlike FF-DF (FaceForensics++), and DFDC (DeepFake Detection Challenge) are based\non English language.. Hence, this paper aims to create a first novel Hindi deep\nfake dataset, named ``Hindi audio-video-Deepfake'' (HAV-DF). The dataset has\nbeen generated using the faceswap, lipsyn and voice cloning methods. This\nmulti-step process allows us to create a rich, varied dataset that captures the\nnuances of Hindi speech and facial expressions, providing a robust foundation\nfor training and evaluating deepfake detection models in a Hindi language\ncontext. It is unique of its kind as all of the previous datasets contain\neither deepfake videos or synthesized audio. This type of deepfake dataset can\nbe used for training a detector for both deepfake video and audio datasets.\nNotably, the newly introduced HAV-DF dataset demonstrates lower detection\naccuracy's across existing detection methods like Headpose, Xception-c40, etc.\nCompared to other well-known datasets FF-DF, and DFDC. This trend suggests that\nthe HAV-DF dataset presents deeper challenges to detect, possibly due to its\nfocus on Hindi language content and diverse manipulation techniques. The HAV-DF\ndataset fills the gap in Hindi-specific deepfake datasets, aiding multilingual\ndeepfake detection development.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CR",
      "cs.GR",
      "cs.MM",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15457v1",
    "published_date": "2024-11-23 05:18:43 UTC",
    "updated_date": "2024-11-23 05:18:43 UTC"
  },
  {
    "arxiv_id": "2411.15455v1",
    "title": "MUFM: A Mamba-Enhanced Feedback Model for Micro Video Popularity Prediction",
    "authors": [
      "Jiacheng Lu",
      "Mingyuan Xiao",
      "Weijian Wang",
      "Yuxin Du",
      "Yi Cui",
      "Jingnan Zhao",
      "Cheng Hua"
    ],
    "abstract": "The surge in micro-videos is transforming the concept of popularity. As\nresearchers delve into vast multi-modal datasets, there is a growing interest\nin understanding the origins of this popularity and the forces driving its\nrapid expansion. Recent studies suggest that the virality of short videos is\nnot only tied to their inherent multi-modal content but is also heavily\ninfluenced by the strength of platform recommendations driven by audience\nfeedback. In this paper, we introduce a framework for capturing long-term\ndependencies in user feedback and dynamic event interactions, based on the\nMamba Hawkes process. Our experiments on the large-scale open-source\nmulti-modal dataset show that our model significantly outperforms\nstate-of-the-art approaches across various metrics by 23.2%. We believe our\nmodel's capability to map the relationships within user feedback behavior\nsequences will not only contribute to the evolution of next-generation\nrecommendation algorithms and platform applications but also enhance our\nunderstanding of micro video dissemination and its broader societal impact.",
    "categories": [
      "cs.MM",
      "cs.AI"
    ],
    "primary_category": "cs.MM",
    "comment": "14 pages,9 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.15455v1",
    "published_date": "2024-11-23 05:13:27 UTC",
    "updated_date": "2024-11-23 05:13:27 UTC"
  },
  {
    "arxiv_id": "2411.15453v1",
    "title": "Enhancing Instruction-Following Capability of Visual-Language Models by Reducing Image Redundancy",
    "authors": [
      "Te Yang",
      "Jian Jia",
      "Xiangyu Zhu",
      "Weisong Zhao",
      "Bo Wang",
      "Yanhua Cheng",
      "Yan Li",
      "Shengyuan Liu",
      "Quan Chen",
      "Peng Jiang",
      "Kun Gai",
      "Zhen Lei"
    ],
    "abstract": "Large Language Models (LLMs) have strong instruction-following capability to\ninterpret and execute tasks as directed by human commands. Multimodal Large\nLanguage Models (MLLMs) have inferior instruction-following ability compared to\nLLMs. However, there is a significant gap in the instruction-following\ncapabilities between the MLLMs and LLMs. In this study, we conduct a pilot\nexperiment, which demonstrates that spatially down-sampling visual tokens\nsignificantly enhances the instruction-following capability of MLLMs. This is\nattributed to the substantial redundancy in visual modality. However, this\nintuitive method severely impairs the MLLM's multimodal understanding\ncapability. In this paper, we propose Visual-Modality Token Compression (VMTC)\nand Cross-Modality Attention Inhibition (CMAI) strategies to alleviate this gap\nbetween MLLMs and LLMs by inhibiting the influence of irrelevant visual tokens\nduring content generation, increasing the instruction-following ability of the\nMLLMs while retaining their multimodal understanding capacity. In VMTC module,\nthe primary tokens are retained and the redundant tokens are condensed by token\nclustering and merging. In CMAI process, we aggregate text-to-image attentions\nby text-to-text attentions to obtain a text-to-image focus score. Attention\ninhibition is performed on the text-image token pairs with low scores. Our\ncomprehensive experiments over instruction-following capabilities and VQA-V2,\nGQA, TextVQA, MME and MMBench five benchmarks, demonstrate that proposed\nstrategy significantly enhances the instruction following capability of MLLMs\nwhile preserving the ability to understand and process multimodal inputs.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15453v1",
    "published_date": "2024-11-23 05:03:32 UTC",
    "updated_date": "2024-11-23 05:03:32 UTC"
  },
  {
    "arxiv_id": "2411.16726v2",
    "title": "EmotiveTalk: Expressive Talking Head Generation through Audio Information Decoupling and Emotional Video Diffusion",
    "authors": [
      "Haotian Wang",
      "Yuzhe Weng",
      "Yueyan Li",
      "Zilu Guo",
      "Jun Du",
      "Shutong Niu",
      "Jiefeng Ma",
      "Shan He",
      "Xiaoyan Wu",
      "Qiming Hu",
      "Bing Yin",
      "Cong Liu",
      "Qingfeng Liu"
    ],
    "abstract": "Diffusion models have revolutionized the field of talking head generation,\nyet still face challenges in expressiveness, controllability, and stability in\nlong-time generation. In this research, we propose an EmotiveTalk framework to\naddress these issues. Firstly, to realize better control over the generation of\nlip movement and facial expression, a Vision-guided Audio Information\nDecoupling (V-AID) approach is designed to generate audio-based decoupled\nrepresentations aligned with lip movements and expression. Specifically, to\nachieve alignment between audio and facial expression representation spaces, we\npresent a Diffusion-based Co-speech Temporal Expansion (Di-CTE) module within\nV-AID to generate expression-related representations under multi-source emotion\ncondition constraints. Then we propose a well-designed Emotional Talking Head\nDiffusion (ETHD) backbone to efficiently generate highly expressive talking\nhead videos, which contains an Expression Decoupling Injection (EDI) module to\nautomatically decouple the expressions from reference portraits while\nintegrating the target expression information, achieving more expressive\ngeneration performance. Experimental results show that EmotiveTalk can generate\nexpressive talking head videos, ensuring the promised controllability of\nemotions and stability during long-time generation, yielding state-of-the-art\nperformance compared to existing methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "https://emotivetalk.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2411.16726v2",
    "published_date": "2024-11-23 04:38:51 UTC",
    "updated_date": "2024-12-16 17:11:49 UTC"
  },
  {
    "arxiv_id": "2411.15446v1",
    "title": "freePruner: A Training-free Approach for Large Multimodal Model Acceleration",
    "authors": [
      "Bingxin Xu",
      "Yuzhang Shang",
      "Yunhao Ge",
      "Qian Lou",
      "Yan Yan"
    ],
    "abstract": "Large Multimodal Models (LMMs) have demonstrated impressive capabilities in\nvisual-language tasks but face significant deployment challenges due to their\nhigh computational demands. While recent token reduction methods show promise\nfor accelerating LMMs, they typically require extensive retraining or\nfine-tuning, making them impractical for many state-of-the-art models,\nespecially those with proprietary training data. We propose freePruner, a\ntraining-free token reduction approach that can be directly applied to any\nopen-source LMM without additional training. Unlike existing methods that rely\nheavily on token merging operations, freePruner employs a two-stage token\nselection strategy: (1) identifying pivotal tokens that capture high-level\nsemantic information using our designed contribution degree metric, and (2)\nselecting complementary tokens that preserve essential low-level visual details\nthrough attention pattern analysis. Extensive experiments demonstrate that\nfreePruner achieves 2x acceleration while maintaining comparable performance\nacross mainstream visual question-answering benchmarks in the training-free\nsetting. Moreover, freePruner is orthogonal to and can be combined with other\npost-training acceleration techniques, such as post-training quantization,\nproviding a practical solution for efficient LMM deployment.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15446v1",
    "published_date": "2024-11-23 04:25:16 UTC",
    "updated_date": "2024-11-23 04:25:16 UTC"
  },
  {
    "arxiv_id": "2411.15442v1",
    "title": "Automatic High-quality Verilog Assertion Generation through Subtask-Focused Fine-Tuned LLMs and Iterative Prompting",
    "authors": [
      "Mohammad Shahidzadeh",
      "Behnam Ghavami",
      "Steve Wilton",
      "Lesley Shannon"
    ],
    "abstract": "Formal Property Verification (FPV), using SystemVerilog Assertions (SVA), is\ncrucial for ensuring the completeness of design with respect to the\nspecification. However, writing SVA is a laborious task and has a steep\nlearning curve. In this work, we present a large language model (LLM) -based\nflow to automatically generate high-quality SVA from the design specification\ndocuments, named \\ToolName. We introduce a novel sub-task-focused fine-tuning\napproach that effectively addresses functionally incorrect assertions produced\nby baseline LLMs, leading to a remarkable 7.3-fold increase in the number of\nfunctionally correct assertions. Recognizing the prevalence of syntax and\nsemantic errors, we also developed an iterative refinement method that enhances\nthe LLM's initial outputs by systematically re-prompting it to correct\nidentified issues. This process is further strengthened by a custom compiler\nthat generates meaningful error messages, guiding the LLM towards improved\naccuracy. The experiments demonstrate a 26\\% increase in the number of\nassertions free from syntax errors using this approach, showcasing its\npotential to streamline the FPV process.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15442v1",
    "published_date": "2024-11-23 03:52:32 UTC",
    "updated_date": "2024-11-23 03:52:32 UTC"
  },
  {
    "arxiv_id": "2411.15428v1",
    "title": "GeoAI-Enhanced Community Detection on Spatial Networks with Graph Deep Learning",
    "authors": [
      "Yunlei Liang",
      "Jiawei Zhu",
      "Wen Ye",
      "Song Gao"
    ],
    "abstract": "Spatial networks are useful for modeling geographic phenomena where spatial\ninteraction plays an important role. To analyze the spatial networks and their\ninternal structures, graph-based methods such as community detection have been\nwidely used. Community detection aims to extract strongly connected components\nfrom the network and reveal the hidden relationships between nodes, but they\nusually do not involve the attribute information. To consider edge-based\ninteractions and node attributes together, this study proposed a family of\nGeoAI-enhanced unsupervised community detection methods called region2vec based\non Graph Attention Networks (GAT) and Graph Convolutional Networks (GCN). The\nregion2vec methods generate node neural embeddings based on attribute\nsimilarity, geographic adjacency and spatial interactions, and then extract\nnetwork communities based on node embeddings using agglomerative clustering.\nThe proposed GeoAI-based methods are compared with multiple baselines and\nperform the best when one wants to maximize node attribute similarity and\nspatial interaction intensity simultaneously within the spatial network\ncommunities. It is further applied in the shortage area delineation problem in\npublic health and demonstrates its promise in regionalization problems.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "I.2.4"
    ],
    "primary_category": "cs.SI",
    "comment": "25 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.15428v1",
    "published_date": "2024-11-23 03:09:34 UTC",
    "updated_date": "2024-11-23 03:09:34 UTC"
  },
  {
    "arxiv_id": "2411.15422v1",
    "title": "Learning a local trading strategy: deep reinforcement learning for grid-scale renewable energy integration",
    "authors": [
      "Caleb Ju",
      "Constance Crozier"
    ],
    "abstract": "Variable renewable generation increases the challenge of balancing power\nsupply and demand. Grid-scale batteries co-located with generation can help\nmitigate this misalignment. This paper explores the use of reinforcement\nlearning (RL) for operating grid-scale batteries co-located with solar power.\nOur results show RL achieves an average of 61% (and up to 96%) of the\napproximate theoretical optimal (non-causal) operation, outperforming advanced\ncontrol methods on average. Our findings suggest RL may be preferred when\nfuture signals are hard to predict. Moreover, RL has two significant advantages\ncompared to simpler rules-based control: (1) that solar energy is more\neffectively shifted towards high demand periods, and (2) increased diversity of\nbattery dispatch across different locations, reducing potential ramping issues\ncaused by super-position of many similar actions.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to HICSS58",
    "pdf_url": "http://arxiv.org/pdf/2411.15422v1",
    "published_date": "2024-11-23 02:55:38 UTC",
    "updated_date": "2024-11-23 02:55:38 UTC"
  },
  {
    "arxiv_id": "2411.16723v1",
    "title": "Two Heads Are Better Than One: Collaborative LLM Embodied Agents for Human-Robot Interaction",
    "authors": [
      "Mitchell Rosser",
      "Marc. G Carmichael"
    ],
    "abstract": "With the recent development of natural language generation models - termed as\nlarge language models (LLMs) - a potential use case has opened up to improve\nthe way that humans interact with robot assistants. These LLMs should be able\nto leverage their large breadth of understanding to interpret natural language\ncommands into effective, task appropriate and safe robot task executions.\nHowever, in reality, these models suffer from hallucinations, which may cause\nsafety issues or deviations from the task. In other domains, these issues have\nbeen improved through the use of collaborative AI systems where multiple LLM\nagents can work together to collectively plan, code and self-check outputs. In\nthis research, multiple collaborative AI systems were tested against a single\nindependent AI agent to determine whether the success in other domains would\ntranslate into improved human-robot interaction performance. The results show\nthat there is no defined trend between the number of agents and the success of\nthe model. However, it is clear that some collaborative AI agent architectures\ncan exhibit a greatly improved capacity to produce error-free code and to solve\nabstract problems.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.MA",
    "comment": "9 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.16723v1",
    "published_date": "2024-11-23 02:47:12 UTC",
    "updated_date": "2024-11-23 02:47:12 UTC"
  },
  {
    "arxiv_id": "2411.15413v1",
    "title": "FG-CXR: A Radiologist-Aligned Gaze Dataset for Enhancing Interpretability in Chest X-Ray Report Generation",
    "authors": [
      "Trong Thang Pham",
      "Ngoc-Vuong Ho",
      "Nhat-Tan Bui",
      "Thinh Phan",
      "Patel Brijesh",
      "Donald Adjeroh",
      "Gianfranco Doretto",
      "Anh Nguyen",
      "Carol C. Wu",
      "Hien Nguyen",
      "Ngan Le"
    ],
    "abstract": "Developing an interpretable system for generating reports in chest X-ray\n(CXR) analysis is becoming increasingly crucial in Computer-aided Diagnosis\n(CAD) systems, enabling radiologists to comprehend the decisions made by these\nsystems. Despite the growth of diverse datasets and methods focusing on report\ngeneration, there remains a notable gap in how closely these models' generated\nreports align with the interpretations of real radiologists. In this study, we\ntackle this challenge by initially introducing Fine-Grained CXR (FG-CXR)\ndataset, which provides fine-grained paired information between the captions\ngenerated by radiologists and the corresponding gaze attention heatmaps for\neach anatomy. Unlike existing datasets that include a raw sequence of gaze\nalongside a report, with significant misalignment between gaze location and\nreport content, our FG-CXR dataset offers a more grained alignment between gaze\nattention and diagnosis transcript. Furthermore, our analysis reveals that\nsimply applying black-box image captioning methods to generate reports cannot\nadequately explain which information in CXR is utilized and how long needs to\nattend to accurately generate reports. Consequently, we propose a novel\nexplainable radiologist's attention generator network (Gen-XAI) that mimics the\ndiagnosis process of radiologists, explicitly constraining its output to\nclosely align with both radiologist's gaze attention and transcript. Finally,\nwe perform extensive experiments to illustrate the effectiveness of our method.\nOur datasets and checkpoint is available at\nhttps://github.com/UARK-AICV/FG-CXR.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "ACCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.15413v1",
    "published_date": "2024-11-23 02:22:40 UTC",
    "updated_date": "2024-11-23 02:22:40 UTC"
  },
  {
    "arxiv_id": "2411.16721v3",
    "title": "Steering Away from Harm: An Adaptive Approach to Defending Vision Language Model Against Jailbreaks",
    "authors": [
      "Han Wang",
      "Gang Wang",
      "Huan Zhang"
    ],
    "abstract": "Vision Language Models (VLMs) can produce unintended and harmful content when\nexposed to adversarial attacks, particularly because their vision capabilities\ncreate new vulnerabilities. Existing defenses, such as input preprocessing,\nadversarial training, and response evaluation-based methods, are often\nimpractical for real-world deployment due to their high costs. To address this\nchallenge, we propose ASTRA, an efficient and effective defense by adaptively\nsteering models away from adversarial feature directions to resist VLM attacks.\nOur key procedures involve finding transferable steering vectors representing\nthe direction of harmful response and applying adaptive activation steering to\nremove these directions at inference time. To create effective steering\nvectors, we randomly ablate the visual tokens from the adversarial images and\nidentify those most strongly associated with jailbreaks. These tokens are then\nused to construct steering vectors. During inference, we perform the adaptive\nsteering method that involves the projection between the steering vectors and\ncalibrated activation, resulting in little performance drops on benign inputs\nwhile strongly avoiding harmful outputs under adversarial inputs. Extensive\nexperiments across multiple models and baselines demonstrate our\nstate-of-the-art performance and high efficiency in mitigating jailbreak risks.\nAdditionally, ASTRA exhibits good transferability, defending against unseen\nattacks (i.e., structured-based attack, perturbation-based attack with project\ngradient descent variants, and text-only attack). Our code is available at\n\\url{https://github.com/ASTRAL-Group/ASTRA}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.16721v3",
    "published_date": "2024-11-23 02:17:17 UTC",
    "updated_date": "2025-05-01 22:22:43 UTC"
  },
  {
    "arxiv_id": "2411.15408v1",
    "title": "Exploring Large Language Models for Multimodal Sentiment Analysis: Challenges, Benchmarks, and Future Directions",
    "authors": [
      "Shezheng Song"
    ],
    "abstract": "Multimodal Aspect-Based Sentiment Analysis (MABSA) aims to extract aspect\nterms and their corresponding sentiment polarities from multimodal information,\nincluding text and images. While traditional supervised learning methods have\nshown effectiveness in this task, the adaptability of large language models\n(LLMs) to MABSA remains uncertain. Recent advances in LLMs, such as Llama2,\nLLaVA, and ChatGPT, demonstrate strong capabilities in general tasks, yet their\nperformance in complex and fine-grained scenarios like MABSA is underexplored.\nIn this study, we conduct a comprehensive investigation into the suitability of\nLLMs for MABSA. To this end, we construct a benchmark to evaluate the\nperformance of LLMs on MABSA tasks and compare them with state-of-the-art\nsupervised learning methods. Our experiments reveal that, while LLMs\ndemonstrate potential in multimodal understanding, they face significant\nchallenges in achieving satisfactory results for MABSA, particularly in terms\nof accuracy and inference time. Based on these findings, we discuss the\nlimitations of current LLMs and outline directions for future research to\nenhance their capabilities in multimodal sentiment analysis.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15408v1",
    "published_date": "2024-11-23 02:17:10 UTC",
    "updated_date": "2024-11-23 02:17:10 UTC"
  },
  {
    "arxiv_id": "2411.15396v1",
    "title": "The Decoy Dilemma in Online Medical Information Evaluation: A Comparative Study of Credibility Assessments by LLM and Human Judges",
    "authors": [
      "Jiqun Liu",
      "Jiangen He"
    ],
    "abstract": "Can AI be cognitively biased in automated information judgment tasks? Despite\nrecent progresses in measuring and mitigating social and algorithmic biases in\nAI and large language models (LLMs), it is not clear to what extent LLMs behave\n\"rationally\", or if they are also vulnerable to human cognitive bias triggers.\nTo address this open problem, our study, consisting of a crowdsourcing user\nexperiment and a LLM-enabled simulation experiment, compared the credibility\nassessments by LLM and human judges under potential decoy effects in an\ninformation retrieval (IR) setting, and empirically examined the extent to\nwhich LLMs are cognitively biased in COVID-19 medical (mis)information\nassessment tasks compared to traditional human assessors as a baseline. The\nresults, collected from a between-subject user experiment and a LLM-enabled\nreplicate experiment, demonstrate that 1) Larger and more recent LLMs tend to\nshow a higher level of consistency and accuracy in distinguishing credible\ninformation from misinformation. However, they are more likely to give higher\nratings for misinformation due to the presence of a more salient, decoy\nmisinformation result; 2) While decoy effect occurred in both human and LLM\nassessments, the effect is more prevalent across different conditions and\ntopics in LLM judgments compared to human credibility ratings. In contrast to\nthe generally assumed \"rationality\" of AI tools, our study empirically confirms\nthe cognitive bias risks embedded in LLM agents, evaluates the decoy impact on\nLLMs against human credibility assessments, and thereby highlights the\ncomplexity and importance of debiasing AI agents and developing\npsychology-informed AI audit techniques and policies for automated judgment\ntasks and beyond.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.HC",
      "H.3.3; I.2.7"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15396v1",
    "published_date": "2024-11-23 00:43:27 UTC",
    "updated_date": "2024-11-23 00:43:27 UTC"
  },
  {
    "arxiv_id": "2411.15395v1",
    "title": "ChatBCI: A P300 Speller BCI Leveraging Large Language Models for Improved Sentence Composition in Realistic Scenarios",
    "authors": [
      "Jiazhen Hong",
      "Weinan Wang",
      "Laleh Najafizadeh"
    ],
    "abstract": "P300 speller BCIs allow users to compose sentences by selecting target keys\non a GUI through the detection of P300 component in their EEG signals following\nvisual stimuli. Most P300 speller BCIs require users to spell words letter by\nletter, or the first few initial letters, resulting in high keystroke demands\nthat increase time, cognitive load, and fatigue. This highlights the need for\nmore efficient, user-friendly methods for faster sentence composition. In this\nwork, we introduce ChatBCI, a P300 speller BCI that leverages the zero-shot\nlearning capabilities of large language models (LLMs) to suggest words from\nuser-spelled initial letters or predict the subsequent word(s), reducing\nkeystrokes and accelerating sentence composition. ChatBCI retrieves word\nsuggestions through remote queries to the GPT-3.5 API. A new GUI, displaying\nGPT-3.5 word suggestions as extra keys is designed. SWLDA is used for the P300\nclassification. Seven subjects completed two online spelling tasks: 1)\ncopy-spelling a self-composed sentence using ChatBCI, and 2) improvising a\nsentence using ChatBCI's word suggestions. Results demonstrate that in Task 1,\non average, ChatBCI outperforms letter-by-letter BCI spellers, reducing time\nand keystrokes by 62.14% and 53.22%, respectively, and increasing information\ntransfer rate by 198.96%. In Task 2, ChatBCI achieves 80.68% keystroke savings\nand a record 8.53 characters/min for typing speed. Overall, ChatBCI, by\nemploying remote LLM queries, enhances sentence composition in realistic\nscenarios, significantly outperforming traditional spellers without requiring\nlocal model training or storage. ChatBCI's (multi-) word predictions, combined\nwith its new GUI, pave the way for developing next-generation speller BCIs that\nare efficient and effective for real-time communication, especially for users\nwith communication and motor disabilities.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.SY",
      "eess.SP",
      "eess.SY"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15395v1",
    "published_date": "2024-11-23 00:42:12 UTC",
    "updated_date": "2024-11-23 00:42:12 UTC"
  },
  {
    "arxiv_id": "2411.15393v1",
    "title": "Gradient-Free Classifier Guidance for Diffusion Model Sampling",
    "authors": [
      "Rahul Shenoy",
      "Zhihong Pan",
      "Kaushik Balakrishnan",
      "Qisen Cheng",
      "Yongmoon Jeon",
      "Heejune Yang",
      "Jaewon Kim"
    ],
    "abstract": "Image generation using diffusion models have demonstrated outstanding\nlearning capabilities, effectively capturing the full distribution of the\ntraining dataset. They are known to generate wide variations in sampled images,\nalbeit with a trade-off in image fidelity. Guided sampling methods, such as\nclassifier guidance (CG) and classifier-free guidance (CFG), focus sampling in\nwell-learned high-probability regions to generate images of high fidelity, but\neach has its limitations. CG is computationally expensive due to the use of\nback-propagation for classifier gradient descent, while CFG, being\ngradient-free, is more efficient but compromises class label alignment compared\nto CG. In this work, we propose an efficient guidance method that fully\nutilizes a pre-trained classifier without using gradient descent. By using the\nclassifier solely in inference mode, a time-adaptive reference class label and\ncorresponding guidance scale are determined at each time step for guided\nsampling. Experiments on both class-conditioned and text-to-image generation\ndiffusion models demonstrate that the proposed Gradient-free Classifier\nGuidance (GFCG) method consistently improves class prediction accuracy. We also\nshow GFCG to be complementary to other guided sampling methods like CFG. When\ncombined with the state-of-the-art Autoguidance (ATG), without additional\ncomputational overhead, it enhances image fidelity while preserving diversity.\nFor ImageNet 512$\\times$512, we achieve a record $\\text{FD}_{\\text{DINOv2}}$ of\n23.09, while simultaneously attaining a higher classification Precision (94.3%)\ncompared to ATG (90.2%)",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15393v1",
    "published_date": "2024-11-23 00:22:21 UTC",
    "updated_date": "2024-11-23 00:22:21 UTC"
  },
  {
    "arxiv_id": "2411.15386v1",
    "title": "Inducing Human-like Biases in Moral Reasoning Language Models",
    "authors": [
      "Artem Karpov",
      "Seong Hah Cho",
      "Austin Meek",
      "Raymond Koopmanschap",
      "Lucy Farnik",
      "Bogdan-Ionut Cirstea"
    ],
    "abstract": "In this work, we study the alignment (BrainScore) of large language models\n(LLMs) fine-tuned for moral reasoning on behavioral data and/or brain data of\nhumans performing the same task. We also explore if fine-tuning several LLMs on\nthe fMRI data of humans performing moral reasoning can improve the BrainScore.\nWe fine-tune several LLMs (BERT, RoBERTa, DeBERTa) on moral reasoning\nbehavioral data from the ETHICS benchmark [Hendrycks et al., 2020], on the\nmoral reasoning fMRI data from Koster-Hale et al. [2013], or on both. We study\nboth the accuracy on the ETHICS benchmark and the BrainScores between model\nactivations and fMRI data. While larger models generally performed better on\nboth metrics, BrainScores did not significantly improve after fine-tuning.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to the 2nd Workshop on Unifying Representations in Neural\n  Models (UniReps) at NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.15386v1",
    "published_date": "2024-11-23 00:01:07 UTC",
    "updated_date": "2024-11-23 00:01:07 UTC"
  }
]
[
  {
    "arxiv_id": "2506.00312v1",
    "title": "An evaluation of LLMs for generating movie reviews: GPT-4o, Gemini-2.0 and DeepSeek-V3",
    "authors": [
      "Brendan Sands",
      "Yining Wang",
      "Chenhao Xu",
      "Yuxuan Zhou",
      "Lai Wei",
      "Rohitash Chandra"
    ],
    "abstract": "Large language models (LLMs) have been prominent in various tasks, including text generation and summarisation. The applicability of LLMs to the generation of product reviews is gaining momentum, paving the way for the generation of movie reviews. In this study, we propose a framework that generates movie reviews using three LLMs (GPT-4o, DeepSeek-V3, and Gemini-2.0), and evaluate their performance by comparing the generated outputs with IMDb user reviews. We use movie subtitles and screenplays as input to the LLMs and investigate how they affect the quality of reviews generated. We review the LLM-based movie reviews in terms of vocabulary, sentiment polarity, similarity, and thematic consistency in comparison to IMDB user reviews. The results demonstrate that LLMs are capable of generating syntactically fluent and structurally complete movie reviews. Nevertheless, there is still a noticeable gap in emotional richness and stylistic coherence between LLM-generated and IMDb reviews, suggesting that further refinement is needed to improve the overall quality of movie review generation. We provided a survey-based analysis where participants were told to distinguish between LLM and IMDb user reviews. The results show that LLM-generated reviews are difficult to distinguish from IMDB user reviews. We found that DeepSeek-V3 produced the most balanced reviews, closely matching IMDb reviews. GPT-4o overemphasised positive emotions, while Gemini-2.0 captured negative emotions better but showed excessive emotional intensity.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.00312v1",
    "published_date": "2025-05-30 23:45:53 UTC",
    "updated_date": "2025-05-30 23:45:53 UTC"
  },
  {
    "arxiv_id": "2506.00309v3",
    "title": "Evaluation of LLMs for mathematical problem solving",
    "authors": [
      "Ruonan Wang",
      "Runxi Wang",
      "Yunwen Shen",
      "Chengfeng Wu",
      "Qinglin Zhou",
      "Rohitash Chandra"
    ],
    "abstract": "Large Language Models (LLMs) have shown impressive performance on a range of educational tasks, but are still understudied for their potential to solve mathematical problems. In this study, we compare three prominent LLMs, including GPT-4o, DeepSeek-V3, and Gemini-2.0, on three mathematics datasets of varying complexities (GSM8K, MATH500, and MIT Open Courseware datasets). We take a five-dimensional approach based on the Structured Chain-of-Thought (SCoT) framework to assess final answer correctness, step completeness, step validity, intermediate calculation accuracy, and problem comprehension. The results show that GPT-4o is the most stable and consistent in performance across all the datasets, but particularly it performs outstandingly in high-level questions of the MIT Open Courseware dataset. DeepSeek-V3 is competitively strong in well-structured domains such as optimisation, but suffers from fluctuations in accuracy in statistical inference tasks. Gemini-2.0 shows strong linguistic understanding and clarity in well-structured problems but performs poorly in multi-step reasoning and symbolic logic. Our error analysis reveals particular deficits in each model: GPT-4o is at times lacking in sufficient explanation or precision; DeepSeek-V3 leaves out intermediate steps; and Gemini-2.0 is less flexible in mathematical reasoning in higher dimensions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.00309v3",
    "published_date": "2025-05-30 23:37:37 UTC",
    "updated_date": "2025-06-28 05:54:45 UTC"
  },
  {
    "arxiv_id": "2506.00308v2",
    "title": "MythTriage: Scalable Detection of Opioid Use Disorder Myths on a Video-Sharing Platform",
    "authors": [
      "Hayoung Jung",
      "Shravika Mittal",
      "Ananya Aatreya",
      "Navreet Kaur",
      "Munmun De Choudhury",
      "Tanushree Mitra"
    ],
    "abstract": "Understanding the prevalence of misinformation in health topics online can inform public health policies and interventions. However, measuring such misinformation at scale remains a challenge, particularly for high-stakes but understudied topics like opioid-use disorder (OUD)--a leading cause of death in the U.S. We present the first large-scale study of OUD-related myths on YouTube, a widely-used platform for health information. With clinical experts, we validate 8 pervasive myths and release an expert-labeled video dataset. To scale labeling, we introduce MythTriage, an efficient triage pipeline that uses a lightweight model for routine cases and defers harder ones to a high-performing, but costlier, large language model (LLM). MythTriage achieves up to 0.86 macro F1-score while estimated to reduce annotation time and financial cost by over 76% compared to experts and full LLM labeling. We analyze 2.9K search results and 343K recommendations, uncovering how myths persist on YouTube and offering actionable insights for public health and platform moderation.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "To appear at EMNLP 2025. Please cite EMNLP version when proceedings are available",
    "pdf_url": "https://arxiv.org/pdf/2506.00308v2",
    "published_date": "2025-05-30 23:37:10 UTC",
    "updated_date": "2025-09-17 08:35:13 UTC"
  },
  {
    "arxiv_id": "2506.00307v2",
    "title": "Lossless Token Sequence Compression via Meta-Tokens",
    "authors": [
      "John Harvill",
      "Ziwei Fan",
      "Hao Wang",
      "Luke Huan",
      "Anoop Deoras",
      "Yizhou Sun",
      "Hao Ding"
    ],
    "abstract": "Existing work on prompt compression for Large Language Models (LLM) focuses on lossy methods that try to maximize the retention of semantic information that is relevant to downstream tasks while significantly reducing the sequence length. In this paper, we introduce a task-agnostic lossless compression technique similar to LZ77 that makes it possible to reduce the input token sequence length on average by 27\\% and 18\\% for the two evaluation tasks explored here. Given that we use transformer-based LLMs, this equates to 47\\% and 33\\% less encoding computation, respectively, due to the quadratic nature of attention. The token sequence transformation is trivial to reverse and highlights that no semantic information is lost in the process. We evaluate our proposed approach on two tasks that require strict preservation of semantics/syntax and demonstrate that existing lossy compression methods perform poorly in this setting. We find that our lossless compression technique produces only a small gap in performance compared to using the uncompressed input and posit that larger models and an expanded computing budget would likely erase the gap entirely.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "16 pages, 8 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.00307v2",
    "published_date": "2025-05-30 23:32:57 UTC",
    "updated_date": "2025-08-20 23:19:57 UTC"
  },
  {
    "arxiv_id": "2506.00297v1",
    "title": "Improving Protein Sequence Design through Designability Preference Optimization",
    "authors": [
      "Fanglei Xue",
      "Andrew Kubaney",
      "Zhichun Guo",
      "Joseph K. Min",
      "Ge Liu",
      "Yi Yang",
      "David Baker"
    ],
    "abstract": "Protein sequence design methods have demonstrated strong performance in sequence generation for de novo protein design. However, as the training objective was sequence recovery, it does not guarantee designability--the likelihood that a designed sequence folds into the desired structure. To bridge this gap, we redefine the training objective by steering sequence generation toward high designability. To do this, we integrate Direct Preference Optimization (DPO), using AlphaFold pLDDT scores as the preference signal, which significantly improves the in silico design success rate. To further refine sequence generation at a finer, residue-level granularity, we introduce Residue-level Designability Preference Optimization (ResiDPO), which applies residue-level structural rewards and decouples optimization across residues. This enables direct improvement in designability while preserving regions that already perform well. Using a curated dataset with residue-level annotations, we fine-tune LigandMPNN with ResiDPO to obtain EnhancedMPNN, which achieves a nearly 3-fold increase in in silico design success rate (from 6.56% to 17.57%) on a challenging enzyme design benchmark.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.BM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.00297v1",
    "published_date": "2025-05-30 23:02:51 UTC",
    "updated_date": "2025-05-30 23:02:51 UTC"
  },
  {
    "arxiv_id": "2506.05376v2",
    "title": "A Red Teaming Roadmap Towards System-Level Safety",
    "authors": [
      "Zifan Wang",
      "Christina Q. Knight",
      "Jeremy Kritz",
      "Willow E. Primack",
      "Julian Michael"
    ],
    "abstract": "Large Language Model (LLM) safeguards, which implement request refusals, have become a widely adopted mitigation strategy against misuse. At the intersection of adversarial machine learning and AI safety, safeguard red teaming has effectively identified critical vulnerabilities in state-of-the-art refusal-trained LLMs. However, in our view the many conference submissions on LLM red teaming do not, in aggregate, prioritize the right research problems. First, testing against clear product safety specifications should take a higher priority than abstract social biases or ethical principles. Second, red teaming should prioritize realistic threat models that represent the expanding risk landscape and what real attackers might do. Finally, we contend that system-level safety is a necessary step to move red teaming research forward, as AI models present new threats as well as affordances for threat mitigation (e.g., detection and banning of malicious users) once placed in a deployment context. Adopting these priorities will be necessary in order for red teaming research to adequately address the slate of new threats that rapid AI advances present today and will present in the very near future.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.05376v2",
    "published_date": "2025-05-30 22:58:54 UTC",
    "updated_date": "2025-06-09 05:48:22 UTC"
  },
  {
    "arxiv_id": "2506.00288v3",
    "title": "Emergent Abilities of Large Language Models under Continued Pretraining for Language Adaptation",
    "authors": [
      "Ahmed Elhady",
      "Eneko Agirre",
      "Mikel Artetxe"
    ],
    "abstract": "Continued pretraining (CPT) is a popular approach to adapt existing large language models (LLMs) to new languages. When doing so, it is common practice to include a portion of English data in the mixture, but its role has not been carefully studied to date. In this work, we show that including English does not impact validation perplexity, yet it is critical for the emergence of downstream capabilities in the target language. We introduce a language-agnostic benchmark for in-context learning (ICL), which reveals catastrophic forgetting early on CPT when English is not included. This in turn damages the ability of the model to generalize to downstream prompts in the target language as measured by perplexity, even if it does not manifest in terms of accuracy until later in training, and can be tied to a big shift in the model parameters. Based on these insights, we introduce curriculum learning and exponential moving average (EMA) of weights as effective alternatives to mitigate the need for English. All in all, our work sheds light into the dynamics by which emergent abilities arise when doing CPT for language adaptation, and can serve as a foundation to design more effective methods in the future.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Published as a Conference Paper at the main track of ACL 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.00288v3",
    "published_date": "2025-05-30 22:31:59 UTC",
    "updated_date": "2025-09-19 11:27:30 UTC"
  },
  {
    "arxiv_id": "2506.00286v2",
    "title": "Entropic Risk Optimization in Discounted MDPs: Sample Complexity Bounds with a Generative Model",
    "authors": [
      "Oliver Mortensen",
      "Mohammad Sadegh Talebi"
    ],
    "abstract": "In this paper, we analyze the sample complexities of learning the optimal state-action value function $Q^*$ and an optimal policy $π^*$ in a finite discounted Markov decision process (MDP) where the agent has recursive entropic risk-preferences with risk-parameter $β\\neq 0$ and where a generative model of the MDP is available. We provide and analyze a simple model based approach which we call model-based risk-sensitive $Q$-value-iteration (MB-RS-QVI) which leads to $(\\varepsilon,δ)$-PAC-bounds on $\\|Q^*-Q^k\\|$, and $\\|V^*-V^{π_k}\\|$ where $Q_k$ is the output of MB-RS-QVI after k iterations and $π_k$ is the greedy policy with respect to $Q_k$. Both PAC-bounds have exponential dependence on the effective horizon $\\frac{1}{1-γ}$ and the strength of this dependence grows with the learners risk-sensitivity $|β|$. We also provide two lower bounds which shows that exponential dependence on $|β|\\frac{1}{1-γ}$ is unavoidable in both cases. The lower bounds reveal that the PAC-bounds are tight in the parameters $S,A,δ,\\varepsilon$ and that unlike in the classical setting it is not possible to have polynomial dependence in all model parameters.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.00286v2",
    "published_date": "2025-05-30 22:27:57 UTC",
    "updated_date": "2025-10-01 09:50:45 UTC"
  },
  {
    "arxiv_id": "2506.00281v1",
    "title": "Adversarial Threat Vectors and Risk Mitigation for Retrieval-Augmented Generation Systems",
    "authors": [
      "Chris M. Ward",
      "Josh Harguess"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) systems, which integrate Large Language Models (LLMs) with external knowledge sources, are vulnerable to a range of adversarial attack vectors. This paper examines the importance of RAG systems through recent industry adoption trends and identifies the prominent attack vectors for RAG: prompt injection, data poisoning, and adversarial query manipulation. We analyze these threats under risk management lens, and propose robust prioritized control list that includes risk-mitigating actions like input validation, adversarial training, and real-time monitoring.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "SPIE DCS: Proceedings Volume Assurance and Security for AI-enabled Systems 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.00281v1",
    "published_date": "2025-05-30 22:22:05 UTC",
    "updated_date": "2025-05-30 22:22:05 UTC"
  },
  {
    "arxiv_id": "2506.00279v1",
    "title": "Sleep Brain and Cardiac Activity Predict Cognitive Flexibility and Conceptual Reasoning Using Deep Learning",
    "authors": [
      "Boshra Khajehpiri",
      "Eric Granger",
      "Massimiliano de Zambotti",
      "Fiona C. Baker",
      "Mohamad Forouzanfar"
    ],
    "abstract": "Despite extensive research on the relationship between sleep and cognition, the connection between sleep microstructure and human performance across specific cognitive domains remains underexplored. This study investigates whether deep learning models can predict executive functions, particularly cognitive adaptability and conceptual reasoning from physiological processes during a night's sleep. To address this, we introduce CogPSGFormer, a multi-scale convolutional-transformer model designed to process multi-modal polysomnographic data. This model integrates one-channel ECG and EEG signals along with extracted features, including EEG power bands and heart rate variability parameters, to capture complementary information across modalities. A thorough evaluation of the CogPSGFormer architecture was conducted to optimize the processing of extended sleep signals and identify the most effective configuration. The proposed framework was evaluated on 817 individuals from the STAGES dataset using cross-validation. The model achieved 80.3\\% accuracy in classifying individuals into low vs. high cognitive performance groups on unseen data based on Penn Conditional Exclusion Test (PCET) scores. These findings highlight the effectiveness of our multi-scale feature extraction and multi-modal learning approach in leveraging sleep-derived signals for cognitive performance prediction. To facilitate reproducibility, our code is publicly accessible (https://github.com/boshrakh95/CogPSGFormer.git).",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "This work was accepted for publication in IEEE EMBC 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.00279v1",
    "published_date": "2025-05-30 22:21:07 UTC",
    "updated_date": "2025-05-30 22:21:07 UTC"
  },
  {
    "arxiv_id": "2506.00277v1",
    "title": "Hierarchical Level-Wise News Article Clustering via Multilingual Matryoshka Embeddings",
    "authors": [
      "Hans W. A. Hanley",
      "Zakir Durumeric"
    ],
    "abstract": "Contextual large language model embeddings are increasingly utilized for topic modeling and clustering. However, current methods often scale poorly, rely on opaque similarity metrics, and struggle in multilingual settings. In this work, we present a novel, scalable, interpretable, hierarchical, and multilingual approach to clustering news articles and social media data. To do this, we first train multilingual Matryoshka embeddings that can determine story similarity at varying levels of granularity based on which subset of the dimensions of the embeddings is examined. This embedding model achieves state-of-the-art performance on the SemEval 2022 Task 8 test dataset (Pearson $ρ$ = 0.816). Once trained, we develop an efficient hierarchical clustering algorithm that leverages the hierarchical nature of Matryoshka embeddings to identify unique news stories, narratives, and themes. We conclude by illustrating how our approach can identify and cluster stories, narratives, and overarching themes within real-world news datasets.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to The 63rd Annual Meeting of the Association for Computational Linguistics (ACL 2025)",
    "pdf_url": "https://arxiv.org/pdf/2506.00277v1",
    "published_date": "2025-05-30 22:17:18 UTC",
    "updated_date": "2025-05-30 22:17:18 UTC"
  },
  {
    "arxiv_id": "2506.00274v1",
    "title": "Chances and Challenges of the Model Context Protocol in Digital Forensics and Incident Response",
    "authors": [
      "Jan-Niclas Hilgert",
      "Carlo Jakobs",
      "Michael Külper",
      "Martin Lambertz",
      "Axel Mahr",
      "Elmar Padilla"
    ],
    "abstract": "Large language models hold considerable promise for supporting forensic investigations, but their widespread adoption is hindered by a lack of transparency, explainability, and reproducibility. This paper explores how the emerging Model Context Protocol can address these challenges and support the meaningful use of LLMs in digital forensics. Through a theoretical analysis, we examine how MCP can be integrated across various forensic scenarios - ranging from artifact analysis to the generation of interpretable reports. We also outline both technical and conceptual considerations for deploying an MCP server in forensic environments. Our analysis reveals a wide range of use cases in which MCP not only strengthens existing forensic workflows but also facilitates the application of LLMs to areas of forensics where their use was previously limited. Furthermore, we introduce the concept of the inference constraint level - a way of characterizing how specific MCP design choices can deliberately constrain model behavior, thereby enhancing both auditability and traceability. Our insights demonstrate that MCP has significant potential as a foundational component for developing LLM-assisted forensic workflows that are not only more transparent, reproducible, and legally defensible, but also represent a step toward increased automation in digital forensic analysis. However, we also highlight potential challenges that the adoption of MCP may pose for digital forensics in the future.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.00274v1",
    "published_date": "2025-05-30 22:15:48 UTC",
    "updated_date": "2025-05-30 22:15:48 UTC"
  },
  {
    "arxiv_id": "2506.00258v2",
    "title": "Hidden in Plain Sight: Reasoning in Underspecified and Misspecified Scenarios for Multimodal LLMs",
    "authors": [
      "Qianqi Yan",
      "Hongquan Li",
      "Shan Jiang",
      "Yang Zhao",
      "Xinze Guan",
      "Ching-Chen Kuo",
      "Xin Eric Wang"
    ],
    "abstract": "Multimodal large language models (MLLMs) are increasingly deployed in open-ended, real-world environments where inputs are messy, underspecified, and not always trustworthy. Unlike curated benchmarks, these settings frequently involve instructions that refer to missing objects or contradictory facts, rely on ambiguous references, or request infeasible actions. In such cases, success hinges not on task execution alone, but on a model's ability to detect when something is silently wrong. This paper presents a systematic analysis of how current MLLMs handle such implicit reasoning scenarios: cases where the flaw is not explicitly stated but must be inferred from context. Using a curated diagnostic suite spanning four categories of real-world failure modes, we evaluate six MLLMs, including o3 and GPT-4o, and find that models frequently fail to surface hidden issues, even when they possess the necessary perceptual and reasoning skills. Explicit prompting reveals that the underlying capabilities exist but are often suppressed in favor of user compliance. We further show that simple inference-time interventions, such as cautious persona prompting and, in particular, requiring a clarifying question, can dramatically recover performance. Our findings highlight a persistent gap between reasoning competence and behavioral compliance in current MLLMs and suggest practical strategies for making these models more trustworthy in underconstrained environments.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.00258v2",
    "published_date": "2025-05-30 21:47:28 UTC",
    "updated_date": "2025-08-25 06:49:21 UTC"
  },
  {
    "arxiv_id": "2506.00253v3",
    "title": "Aligned but Blind: Alignment Increases Implicit Bias by Reducing Awareness of Race",
    "authors": [
      "Lihao Sun",
      "Chengzhi Mao",
      "Valentin Hofmann",
      "Xuechunzi Bai"
    ],
    "abstract": "Although value-aligned language models (LMs) appear unbiased in explicit bias evaluations, they often exhibit stereotypes in implicit word association tasks, raising concerns about their fair usage. We investigate the mechanisms behind this discrepancy and find that alignment surprisingly amplifies implicit bias in model outputs. Specifically, we show that aligned LMs, unlike their unaligned counterparts, overlook racial concepts in early internal representations when the context is ambiguous. Not representing race likely fails to activate safety guardrails, leading to unintended biases. Inspired by this insight, we propose a new bias mitigation strategy that works by incentivizing the representation of racial concepts in the early model layers. In contrast to conventional mitigation methods of machine unlearning, our interventions find that steering the model to be more aware of racial concepts effectively mitigates implicit bias. Similar to race blindness in humans, ignoring racial nuances can inadvertently perpetuate subtle biases in LMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ACL 2025 (Main)",
    "pdf_url": "https://arxiv.org/pdf/2506.00253v3",
    "published_date": "2025-05-30 21:41:44 UTC",
    "updated_date": "2025-06-08 23:37:10 UTC"
  },
  {
    "arxiv_id": "2506.00249v1",
    "title": "MIR: Methodology Inspiration Retrieval for Scientific Research Problems",
    "authors": [
      "Aniketh Garikaparthi",
      "Manasi Patwardhan",
      "Aditya Sanjiv Kanade",
      "Aman Hassan",
      "Lovekesh Vig",
      "Arman Cohan"
    ],
    "abstract": "There has been a surge of interest in harnessing the reasoning capabilities of Large Language Models (LLMs) to accelerate scientific discovery. While existing approaches rely on grounding the discovery process within the relevant literature, effectiveness varies significantly with the quality and nature of the retrieved literature. We address the challenge of retrieving prior work whose concepts can inspire solutions for a given research problem, a task we define as Methodology Inspiration Retrieval (MIR). We construct a novel dataset tailored for training and evaluating retrievers on MIR, and establish baselines. To address MIR, we build the Methodology Adjacency Graph (MAG); capturing methodological lineage through citation relationships. We leverage MAG to embed an \"intuitive prior\" into dense retrievers for identifying patterns of methodological inspiration beyond superficial semantic similarity. This achieves significant gains of +5.4 in Recall@3 and +7.8 in Mean Average Precision (mAP) over strong baselines. Further, we adapt LLM-based re-ranking strategies to MIR, yielding additional improvements of +4.5 in Recall@3 and +4.8 in mAP. Through extensive ablation studies and qualitative analyses, we exhibit the promise of MIR in enhancing automated scientific discovery and outline avenues for advancing inspiration-driven retrieval.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "ACL 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.00249v1",
    "published_date": "2025-05-30 21:33:03 UTC",
    "updated_date": "2025-05-30 21:33:03 UTC"
  },
  {
    "arxiv_id": "2506.00242v1",
    "title": "Whispers of Many Shores: Cultural Alignment through Collaborative Cultural Expertise",
    "authors": [
      "Shuai Feng",
      "Wei-Chuang Chan",
      "Srishti Chouhan",
      "Junior Francisco Garcia Ayala",
      "Srujananjali Medicherla",
      "Kyle Clark",
      "Mingwei Shi"
    ],
    "abstract": "The integration of large language models (LLMs) into global applications necessitates effective cultural alignment for meaningful and culturally-sensitive interactions. Current LLMs often lack the nuanced understanding required for diverse cultural contexts, and adapting them typically involves costly full fine-tuning. To address this, we introduce a novel soft prompt fine-tuning framework that enables efficient and modular cultural alignment. Our method utilizes vectorized prompt tuning to dynamically route queries to a committee of culturally specialized 'expert' LLM configurations, created by optimizing soft prompt embeddings without altering the base model's parameters. Extensive experiments demonstrate that our framework significantly enhances cultural sensitivity and adaptability, improving alignment scores from 0.208 to 0.820, offering a robust solution for culturally-aware LLM deployment. This research paves the way for subsequent investigations into enhanced cultural coverage and dynamic expert adaptation, crucial for realizing autonomous AI with deeply nuanced understanding in a globally interconnected world.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "14 main pages;8 page appendix",
    "pdf_url": "https://arxiv.org/pdf/2506.00242v1",
    "published_date": "2025-05-30 21:16:25 UTC",
    "updated_date": "2025-05-30 21:16:25 UTC"
  },
  {
    "arxiv_id": "2506.00241v1",
    "title": "Designing AI Tools for Clinical Care Teams to Support Serious Illness Conversations with Older Adults in the Emergency Department",
    "authors": [
      "Menglin Zhao",
      "Zhuorui Yong",
      "Ruijia Guan",
      "Kai-Wei Chang",
      "Adrian Haimovich",
      "Kei Ouchi",
      "Timothy Bickmore",
      "Bingsheng Yao",
      "Dakuo Wang",
      "Smit Desai"
    ],
    "abstract": "Serious illness conversations (SICs), discussions between clinical care teams and patients with serious, life-limiting illnesses about their values, goals, and care preferences, are critical for patient-centered care. Without these conversations, patients often receive aggressive interventions that may not align with their goals. Clinical care teams face significant barriers when conducting serious illness conversations with older adult patients in Emergency Department (ED) settings, where most older adult patients lack documented treatment goals. To understand current practices and identify AI support opportunities, we conducted interviews with two domain experts and nine ED clinical care team members. Through thematic analysis, we characterized a four-phase serious illness conversation workflow (identification, preparation, conduction, documentation) and identified key needs and challenges at each stage. Clinical care teams struggle with fragmented EHR data access, time constraints, emotional preparation demands, and documentation burdens. While participants expressed interest in AI tools for information synthesis, conversational support, and automated documentation, they emphasized preserving human connection and clinical autonomy. We present design guidelines for AI tools supporting SIC workflows that fit within existing clinical practices. This work contributes empirical understanding of ED-based serious illness conversations and provides design considerations for AI in high-stakes clinical environments.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.00241v1",
    "published_date": "2025-05-30 21:15:57 UTC",
    "updated_date": "2025-05-30 21:15:57 UTC"
  },
  {
    "arxiv_id": "2506.00239v4",
    "title": "SMELLNET: A Large-scale Dataset for Real-world Smell Recognition",
    "authors": [
      "Dewei Feng",
      "Wei Dai",
      "Carol Li",
      "Alistair Pernigo",
      "Yunge Wen",
      "Paul Pu Liang"
    ],
    "abstract": "The ability of AI to sense and identify various substances based on their smell alone can have profound impacts on allergen detection (e.g., smelling gluten or peanuts in a cake), monitoring the manufacturing process, and sensing hormones that indicate emotional states, stress levels, and diseases. Despite these broad impacts, there are virtually no large-scale benchmarks, and therefore little progress, for training and evaluating AI systems' ability to smell in the real world. In this paper, we use small gas and chemical sensors to create SmellNet, the first large-scale database that digitizes a diverse range of smells in the natural world. SmellNet contains about 828,000 data points across 50 substances, spanning nuts, spices, herbs, fruits, and vegetables, and 43 mixtures among them, with 68 hours of data collected. Using SmellNet, we developed ScentFormer, a Transformer-based architecture combining temporal differencing and sliding-window augmentation for smell data. For the SmellNet-Base classification task, ScentFormer achieves 58.5% Top-1 accuracy, and for the SmellNet-Mixture distribution prediction task, ScentFormer achieves 50.2% Top-1@0.1 on the test-seen split. ScentFormer's ability to generalize across conditions and capture transient chemical dynamics demonstrates the promise of temporal modeling in olfactory AI. SmellNet and ScentFormer lay the groundwork for real-world olfactory applications across healthcare, food and beverage, environmental monitoring, manufacturing, and entertainment.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "31 pages, 22 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.00239v4",
    "published_date": "2025-05-30 21:15:25 UTC",
    "updated_date": "2025-12-18 20:24:32 UTC"
  },
  {
    "arxiv_id": "2506.00236v2",
    "title": "Localized LoRA: A Structured Low-Rank Approximation for Efficient Fine-Tuning",
    "authors": [
      "Babak Barazandeh",
      "Subhabrata Majumdar",
      "Om Rajyaguru",
      "George Michailidis"
    ],
    "abstract": "Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, offer compact and effective alternatives to full model fine-tuning by introducing low-rank updates to pre-trained weights. However, most existing approaches rely on global low rank structures, which can overlook spatial patterns spread across the parameter space. In this work, we propose Localized LoRA, a generalized framework that models weight updates as a composition of low-rank matrices applied to structured blocks of the weight matrix. This formulation enables dense, localized updates throughout the parameter space without increasing the total number of trainable parameters. We provide a formal comparison between global, diagonal-local, and fully localized low-rank approximations, and show that our method consistently achieves lower approximation error under matched parameter budgets. Experiments on both synthetic and practical settings demonstrate that Localized LoRA offers a more expressive and adaptable alternative to existing methods, enabling efficient fine-tuning with improved performance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.00236v2",
    "published_date": "2025-05-30 21:13:23 UTC",
    "updated_date": "2025-09-23 18:56:10 UTC"
  },
  {
    "arxiv_id": "2506.00233v1",
    "title": "Ethical AI: Towards Defining a Collective Evaluation Framework",
    "authors": [
      "Aasish Kumar Sharma",
      "Dimitar Kyosev",
      "Julian Kunkel"
    ],
    "abstract": "Artificial Intelligence (AI) is transforming sectors such as healthcare, finance, and autonomous systems, offering powerful tools for innovation. Yet its rapid integration raises urgent ethical concerns related to data ownership, privacy, and systemic bias. Issues like opaque decision-making, misleading outputs, and unfair treatment in high-stakes domains underscore the need for transparent and accountable AI systems. This article addresses these challenges by proposing a modular ethical assessment framework built on ontological blocks of meaning-discrete, interpretable units that encode ethical principles such as fairness, accountability, and ownership. By integrating these blocks with FAIR (Findable, Accessible, Interoperable, Reusable) principles, the framework supports scalable, transparent, and legally aligned ethical evaluations, including compliance with the EU AI Act. Using a real-world use case in AI-powered investor profiling, the paper demonstrates how the framework enables dynamic, behavior-informed risk classification. The findings suggest that ontological blocks offer a promising path toward explainable and auditable AI ethics, though challenges remain in automation and probabilistic reasoning.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "6 pages, 3 figures, accepted at 8th IEEE International Workshop on Advances in Artificial Intelligence and Machine Learning (AIML 2025): Futuristic AI and ML models & Intelligent Systems",
    "pdf_url": "https://arxiv.org/pdf/2506.00233v1",
    "published_date": "2025-05-30 21:10:47 UTC",
    "updated_date": "2025-05-30 21:10:47 UTC"
  },
  {
    "arxiv_id": "2506.08027v2",
    "title": "Recipes for Pre-training LLMs with MXFP8",
    "authors": [
      "Asit Mishra",
      "Dusan Stosic",
      "Simon Layton",
      "Paulius Micikevicius"
    ],
    "abstract": "Using fewer bits to represent model parameters and related tensors during pre-training has become a required technique for improving GPU efficiency without sacrificing accuracy. Microscaling (MX) formats introduced in NVIDIA Blackwell generation of GPUs represent a major advancement of this technique, making it practical to combine narrow floating-point data types with finer granularity per-block scaling factors. In turn, this enables both quantization of more tensors than previous approaches and more efficient execution of operations on those tensors.\n  Effective use of MX-formats requires careful choices of various parameters. In this paper we review these choices and show how MXFP8-E4M3 datatype and a specific number conversion algorithm result in training sessions that match those carried out in BF16. We present results using models with up to 8B parameters, trained on high-quality datasets of up to 15T tokens.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.08027v2",
    "published_date": "2025-05-30 21:08:15 UTC",
    "updated_date": "2025-08-18 19:51:06 UTC"
  },
  {
    "arxiv_id": "2506.00227v2",
    "title": "Ctrl-Crash: Controllable Diffusion for Realistic Car Crashes",
    "authors": [
      "Anthony Gosselin",
      "Ge Ya Luo",
      "Luis Lara",
      "Florian Golemo",
      "Derek Nowrouzezahrai",
      "Liam Paull",
      "Alexia Jolicoeur-Martineau",
      "Christopher Pal"
    ],
    "abstract": "Video diffusion techniques have advanced significantly in recent years; however, they struggle to generate realistic imagery of car crashes due to the scarcity of accident events in most driving datasets. Improving traffic safety requires realistic and controllable accident simulations. To tackle the problem, we propose Ctrl-Crash, a controllable car crash video generation model that conditions on signals such as bounding boxes, crash types, and an initial image frame. Our approach enables counterfactual scenario generation where minor variations in input can lead to dramatically different crash outcomes. To support fine-grained control at inference time, we leverage classifier-free guidance with independently tunable scales for each conditioning signal. Ctrl-Crash achieves state-of-the-art performance across quantitative video quality metrics (e.g., FVD and JEDi) and qualitative measurements based on a human-evaluation of physical realism and video quality compared to prior diffusion-based methods.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Under review at Pattern Recognition Letters",
    "pdf_url": "https://arxiv.org/pdf/2506.00227v2",
    "published_date": "2025-05-30 21:04:38 UTC",
    "updated_date": "2025-12-13 21:29:38 UTC"
  },
  {
    "arxiv_id": "2506.03189v1",
    "title": "Continual Learning in Vision-Language Models via Aligned Model Merging",
    "authors": [
      "Ghada Sokar",
      "Gintare Karolina Dziugaite",
      "Anurag Arnab",
      "Ahmet Iscen",
      "Pablo Samuel Castro",
      "Cordelia Schmid"
    ],
    "abstract": "Continual learning is conventionally tackled through sequential fine-tuning, a process that, while enabling adaptation, inherently favors plasticity over the stability needed to retain prior knowledge. While existing approaches attempt to mitigate catastrophic forgetting, a bias towards recent tasks persists as they build upon this sequential nature. In this work we present a new perspective based on model merging to maintain stability while still retaining plasticity. Rather than just sequentially updating the model weights, we propose merging newly trained task parameters with previously learned ones, promoting a better balance. To maximize the effectiveness of the merging process, we propose a simple mechanism that promotes learning aligned weights with previous ones, thereby avoiding interference when merging. We evaluate this approach on large Vision-Language Models (VLMs), and demonstrate its effectiveness in reducing forgetting, increasing robustness to various task orders and similarities, and improving generalization.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.03189v1",
    "published_date": "2025-05-30 20:52:21 UTC",
    "updated_date": "2025-05-30 20:52:21 UTC"
  },
  {
    "arxiv_id": "2506.03188v1",
    "title": "Multi-Analyte, Swab-based Automated Wound Monitor with AI",
    "authors": [
      "Madhu Babu Sikha",
      "Lalith Appari",
      "Gurudatt Nanjanagudu Ganesh",
      "Amay Bandodkar",
      "Imon Banerjee"
    ],
    "abstract": "Diabetic foot ulcers (DFUs), a class of chronic wounds, affect ~750,000 individuals every year in the US alone and identifying non-healing DFUs that develop to chronic wounds early can drastically reduce treatment costs and minimize risks of amputation. There is therefore a pressing need for diagnostic tools that can detect non-healing DFUs early. We develop a low cost, multi-analyte 3D printed assays seamlessly integrated on swabs that can identify non-healing DFUs and a Wound Sensor iOS App - an innovative mobile application developed for the controlled acquisition and automated analysis of wound sensor data. By comparing both the original base image (before exposure to the wound) and the wound-exposed image, we developed automated computer vision techniques to compare density changes between the two assay images, which allow us to automatically determine the severity of the wound. The iOS app ensures accurate data collection and presents actionable insights, despite challenges such as variations in camera configurations and ambient conditions. The proposed integrated sensor and iOS app will allow healthcare professionals to monitor wound conditions real-time, track healing progress, and assess critical parameters related to wound care.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.HC"
    ],
    "primary_category": "eess.IV",
    "comment": "4 pages conference paper",
    "pdf_url": "https://arxiv.org/pdf/2506.03188v1",
    "published_date": "2025-05-30 20:42:37 UTC",
    "updated_date": "2025-05-30 20:42:37 UTC"
  },
  {
    "arxiv_id": "2506.00214v1",
    "title": "Diff-SPORT: Diffusion-based Sensor Placement Optimization and Reconstruction of Turbulent flows in urban environments",
    "authors": [
      "Abhijeet Vishwasrao",
      "Sai Bharath Chandra Gutha",
      "Andres Cremades",
      "Klas Wijk",
      "Aakash Patil",
      "Catherine Gorle",
      "Beverley J McKeon",
      "Hossein Azizpour",
      "Ricardo Vinuesa"
    ],
    "abstract": "Rapid urbanization demands accurate and efficient monitoring of turbulent wind patterns to support air quality, climate resilience and infrastructure design. Traditional sparse reconstruction and sensor placement strategies face major accuracy degradations under practical constraints. Here, we introduce Diff-SPORT, a diffusion-based framework for high-fidelity flow reconstruction and optimal sensor placement in urban environments. Diff-SPORT combines a generative diffusion model with a maximum a posteriori (MAP) inference scheme and a Shapley-value attribution framework to propose a scalable and interpretable solution. Compared to traditional numerical methods, Diff-SPORT achieves significant speedups while maintaining both statistical and instantaneous flow fidelity. Our approach offers a modular, zero-shot alternative to retraining-intensive strategies, supporting fast and reliable urban flow monitoring under extreme sparsity. Diff-SPORT paves the way for integrating generative modeling and explainability in sustainable urban intelligence.",
    "categories": [
      "physics.flu-dyn",
      "cs.AI"
    ],
    "primary_category": "physics.flu-dyn",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.00214v1",
    "published_date": "2025-05-30 20:41:50 UTC",
    "updated_date": "2025-05-30 20:41:50 UTC"
  },
  {
    "arxiv_id": "2506.00210v2",
    "title": "REIC: RAG-Enhanced Intent Classification at Scale",
    "authors": [
      "Ziji Zhang",
      "Michael Yang",
      "Zhiyu Chen",
      "Yingying Zhuang",
      "Shu-Ting Pi",
      "Qun Liu",
      "Rajashekar Maragoud",
      "Vy Nguyen",
      "Anurag Beniwal"
    ],
    "abstract": "Accurate intent classification is critical for efficient routing in customer service, ensuring customers are connected with the most suitable agents while reducing handling times and operational costs. However, as companies expand their product lines, intent classification faces scalability challenges due to the increasing number of intents and variations in taxonomy across different verticals. In this paper, we introduce REIC, a Retrieval-augmented generation Enhanced Intent Classification approach, which addresses these challenges effectively. REIC leverages retrieval-augmented generation (RAG) to dynamically incorporate relevant knowledge, enabling precise classification without the need for frequent retraining. Through extensive experiments on real-world datasets, we demonstrate that REIC outperforms traditional fine-tuning, zero-shot, and few-shot methods in large-scale customer service settings. Our results highlight its effectiveness in both in-domain and out-of-domain scenarios, demonstrating its potential for real-world deployment in adaptive and large-scale intent classification systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by EMNLP 2025 (Industry Track)",
    "pdf_url": "https://arxiv.org/pdf/2506.00210v2",
    "published_date": "2025-05-30 20:32:10 UTC",
    "updated_date": "2025-11-17 15:21:31 UTC"
  },
  {
    "arxiv_id": "2506.00204v1",
    "title": "Structure-Aware Fill-in-the-Middle Pretraining for Code",
    "authors": [
      "Linyuan Gong",
      "Alvin Cheung",
      "Mostafa Elhoushi",
      "Sida Wang"
    ],
    "abstract": "Fill-in-the-Middle (FIM) is a common pretraining method for code LLMs, where models complete code segments given surrounding context. However, existing LLMs treat code as plain text and mask random character spans. We propose and evaluate AST-FIM, a pretraining strategy that leverages Abstract Syntax Trees (ASTs) to mask complete syntactic structures at scale, ensuring coherent training examples better aligned with universal code structures and common code editing patterns such as blocks, expressions, or functions. To evaluate real-world fill-in-the-middle (FIM) programming tasks, we introduce Real-FIM-Eval, a benchmark derived from 30,000+ GitHub commits across 12 languages. On infilling tasks, experiments on 1B and 8B parameter models show that AST-FIM is particularly beneficial for real-world code editing as it outperforms standard random-character FIM by up to 5 pts on standard FIM benchmarks. Our code is publicly available at https://github.com/gonglinyuan/ast_fim.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CL",
    "comment": "14 pages",
    "pdf_url": "https://arxiv.org/pdf/2506.00204v1",
    "published_date": "2025-05-30 20:19:39 UTC",
    "updated_date": "2025-05-30 20:19:39 UTC"
  },
  {
    "arxiv_id": "2506.00203v1",
    "title": "The World As Large Language Models See It: Exploring the reliability of LLMs in representing geographical features",
    "authors": [
      "Omid Reza Abbasi",
      "Franz Welscher",
      "Georg Weinberger",
      "Johannes Scholz"
    ],
    "abstract": "As large language models (LLMs) continue to evolve, questions about their trustworthiness in delivering factual information have become increasingly important. This concern also applies to their ability to accurately represent the geographic world. With recent advancements in this field, it is relevant to consider whether and to what extent LLMs' representations of the geographical world can be trusted. This study evaluates the performance of GPT-4o and Gemini 2.0 Flash in three key geospatial tasks: geocoding, elevation estimation, and reverse geocoding. In the geocoding task, both models exhibited systematic and random errors in estimating the coordinates of St. Anne's Column in Innsbruck, Austria, with GPT-4o showing greater deviations and Gemini 2.0 Flash demonstrating more precision but a significant systematic offset. For elevation estimation, both models tended to underestimate elevations across Austria, though they captured overall topographical trends, and Gemini 2.0 Flash performed better in eastern regions. The reverse geocoding task, which involved identifying Austrian federal states from coordinates, revealed that Gemini 2.0 Flash outperformed GPT-4o in overall accuracy and F1-scores, demonstrating better consistency across regions. Despite these findings, neither model achieved an accurate reconstruction of Austria's federal states, highlighting persistent misclassifications. The study concludes that while LLMs can approximate geographic information, their accuracy and reliability are inconsistent, underscoring the need for fine-tuning with geographical information to enhance their utility in GIScience and Geoinformatics.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CY",
    "comment": "9 pages, 4 figures, 2 tables",
    "pdf_url": "https://arxiv.org/pdf/2506.00203v1",
    "published_date": "2025-05-30 20:14:17 UTC",
    "updated_date": "2025-05-30 20:14:17 UTC"
  },
  {
    "arxiv_id": "2506.00202v3",
    "title": "What do professional software developers need to know to succeed in an age of Artificial Intelligence?",
    "authors": [
      "Matthew Kam",
      "Cody Miller",
      "Miaoxin Wang",
      "Abey Tidwell",
      "Irene A. Lee",
      "Joyce Malyn-Smith",
      "Beatriz Perez",
      "Vikram Tiwari",
      "Joshua Kenitzer",
      "Andrew Macvean",
      "Erin Barrar"
    ],
    "abstract": "Generative AI is showing early evidence of productivity gains for software developers, but concerns persist regarding workforce disruption and deskilling. We describe our research with 21 developers at the cutting edge of using AI, summarizing 12 of their work goals we uncovered, together with 75 associated tasks and the skills & knowledge for each, illustrating how developers use AI at work. From all of these, we distilled our findings in the form of 5 insights. We found that the skills & knowledge to be a successful AI-enhanced developer are organized into four domains (using Generative AI effectively, core software engineering, adjacent engineering, and adjacent non-engineering) deployed at critical junctures throughout a 6-step task workflow. In order to \"future proof\" developers for this age of AI, on-the-job learning initiatives and computer science degree programs will need to target both \"soft\" skills and the technical skills & knowledge in all four domains to reskill, upskill and safeguard against deskilling.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "12 pages, 4 figures, software engineering education track of the 2025 ACM international conference on the foundations of software engineering, includes supplementary material i.e. full 50-page occupational profile of the AI-enhanced software developer",
    "pdf_url": "https://arxiv.org/pdf/2506.00202v3",
    "published_date": "2025-05-30 20:14:03 UTC",
    "updated_date": "2025-06-23 08:27:54 UTC"
  },
  {
    "arxiv_id": "2506.00198v1",
    "title": "MOFGPT: Generative Design of Metal-Organic Frameworks using Language Models",
    "authors": [
      "Srivathsan Badrinarayanan",
      "Rishikesh Magar",
      "Akshay Antony",
      "Radheesh Sharma Meda",
      "Amir Barati Farimani"
    ],
    "abstract": "The discovery of Metal-Organic Frameworks (MOFs) with application-specific properties remains a central challenge in materials chemistry, owing to the immense size and complexity of their structural design space. Conventional computational screening techniques such as molecular simulations and density functional theory (DFT), while accurate, are computationally prohibitive at scale. Machine learning offers an exciting alternative by leveraging data-driven approaches to accelerate materials discovery. The complexity of MOFs, with their extended periodic structures and diverse topologies, creates both opportunities and challenges for generative modeling approaches. To address these challenges, we present a reinforcement learning-enhanced, transformer-based framework for the de novo design of MOFs. Central to our approach is MOFid, a chemically-informed string representation encoding both connectivity and topology, enabling scalable generative modeling. Our pipeline comprises three components: (1) a generative GPT model trained on MOFid sequences, (2) MOFormer, a transformer-based property predictor, and (3) a reinforcement learning (RL) module that optimizes generated candidates via property-guided reward functions. By integrating property feedback into sequence generation, our method drives the model toward synthesizable, topologically valid MOFs with desired functional attributes. This work demonstrates the potential of large language models, when coupled with reinforcement learning, to accelerate inverse design in reticular chemistry and unlock new frontiers in computational MOF discovery.",
    "categories": [
      "cs.LG",
      "cond-mat.mtrl-sci",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "21 pages, 3 figures (in main text, without references)",
    "pdf_url": "https://arxiv.org/pdf/2506.00198v1",
    "published_date": "2025-05-30 20:09:11 UTC",
    "updated_date": "2025-05-30 20:09:11 UTC"
  },
  {
    "arxiv_id": "2506.00195v2",
    "title": "Let Them Down Easy! Contextual Effects of LLM Guardrails on User Perceptions and Preferences",
    "authors": [
      "Mingqian Zheng",
      "Wenjia Hu",
      "Patrick Zhao",
      "Motahhare Eslami",
      "Jena D. Hwang",
      "Faeze Brahman",
      "Carolyn Rose",
      "Maarten Sap"
    ],
    "abstract": "Current LLMs are trained to refuse potentially harmful input queries regardless of whether users actually had harmful intents, causing a tradeoff between safety and user experience. Through a study of 480 participants evaluating 3,840 query-response pairs, we examine how different refusal strategies affect user perceptions across varying motivations. Our findings reveal that response strategy largely shapes user experience, while actual user motivation has negligible impact. Partial compliance -- providing general information without actionable details -- emerges as the optimal strategy, reducing negative user perceptions by over 50% to flat-out refusals. Complementing this, we analyze response patterns of 9 state-of-the-art LLMs and evaluate how 6 reward models score different refusal strategies, demonstrating that models rarely deploy partial compliance naturally and reward models currently undervalue it. This work demonstrates that effective guardrails require focusing on crafting thoughtful refusals rather than detecting intent, offering a path toward AI safety mechanisms that ensure both safety and sustained user engagement.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to Findings of EMNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.00195v2",
    "published_date": "2025-05-30 20:07:07 UTC",
    "updated_date": "2025-12-02 21:35:13 UTC"
  },
  {
    "arxiv_id": "2506.00191v1",
    "title": "Heterogeneous Graph Backdoor Attack",
    "authors": [
      "Jiawei Chen",
      "Lusi Li",
      "Daniel Takabi",
      "Masha Sosonkina",
      "Rui Ning"
    ],
    "abstract": "Heterogeneous Graph Neural Networks (HGNNs) excel in modeling complex, multi-typed relationships across diverse domains, yet their vulnerability to backdoor attacks remains unexplored. To address this gap, we conduct the first investigation into the susceptibility of HGNNs to existing graph backdoor attacks, revealing three critical issues: (1) high attack budget required for effective backdoor injection, (2) inefficient and unreliable backdoor activation, and (3) inaccurate attack effectiveness evaluation. To tackle these issues, we propose the Heterogeneous Graph Backdoor Attack (HGBA), the first backdoor attack specifically designed for HGNNs, introducing a novel relation-based trigger mechanism that establishes specific connections between a strategically selected trigger node and poisoned nodes via the backdoor metapath. HGBA achieves efficient and stealthy backdoor injection with minimal structural modifications and supports easy backdoor activation through two flexible strategies: Self-Node Attack and Indiscriminate Attack. Additionally, we improve the ASR measurement protocol, enabling a more accurate assessment of attack effectiveness. Extensive experiments demonstrate that HGBA far surpasses multiple state-of-the-art graph backdoor attacks in black-box settings, efficiently attacking HGNNs with low attack budgets. Ablation studies show that the strength of HBGA benefits from our trigger node selection method and backdoor metapath selection strategy. In addition, HGBA shows superior robustness against node feature perturbations and multiple types of existing graph backdoor defense mechanisms. Finally, extension experiments demonstrate that the relation-based trigger mechanism can effectively extend to tasks in homogeneous graph scenarios, thereby posing severe threats to broader security-critical domains.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.00191v1",
    "published_date": "2025-05-30 20:02:43 UTC",
    "updated_date": "2025-05-30 20:02:43 UTC"
  },
  {
    "arxiv_id": "2506.00189v1",
    "title": "Control-R: Towards controllable test-time scaling",
    "authors": [
      "Di Zhang",
      "Weida Wang",
      "Junxian Li",
      "Xunzhi Wang",
      "Jiatong Li",
      "Jianbo Wu",
      "Jingdi Lei",
      "Haonan He",
      "Peng Ye",
      "Shufei Zhang",
      "Wanli Ouyang",
      "Yuqiang Li",
      "Dongzhan Zhou"
    ],
    "abstract": "This paper target in addressing the challenges of underthinking and overthinking in long chain-of-thought (CoT) reasoning for Large Reasoning Models (LRMs) by introducing Reasoning Control Fields (RCF)--a novel test-time approach that injects structured control signals to guide reasoning from a tree search perspective. RCF enables models to adjust reasoning effort according to given control conditions when solving complex tasks. Additionally, we present the Control-R-4K dataset, which consists of challenging problems annotated with detailed reasoning processes and corresponding control fields. To further enhance reasoning control, we propose a Conditional Distillation Finetuning (CDF) method, which trains model--particularly Control-R-32B--to effectively adjust reasoning effort during test time. Experimental results on benchmarks such as AIME2024 and MATH500 demonstrate that our approach achieves state-of-the-art performance at the 32B scale while enabling a controllable Long CoT reasoning process (L-CoT). Overall, this work introduces an effective paradigm for controllable test-time scaling reasoning.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.00189v1",
    "published_date": "2025-05-30 19:59:44 UTC",
    "updated_date": "2025-05-30 19:59:44 UTC"
  },
  {
    "arxiv_id": "2506.00185v1",
    "title": "Pushing the Limits of Beam Search Decoding for Transducer-based ASR models",
    "authors": [
      "Lilit Grigoryan",
      "Vladimir Bataev",
      "Andrei Andrusenko",
      "Hainan Xu",
      "Vitaly Lavrukhin",
      "Boris Ginsburg"
    ],
    "abstract": "Transducer models have emerged as a promising choice for end-to-end ASR systems, offering a balanced trade-off between recognition accuracy, streaming capabilities, and inference speed in greedy decoding. However, beam search significantly slows down Transducers due to repeated evaluations of key network components, limiting practical applications. This paper introduces a universal method to accelerate beam search for Transducers, enabling the implementation of two optimized algorithms: ALSD++ and AES++. The proposed method utilizes batch operations, a tree-based hypothesis structure, novel blank scoring for enhanced shallow fusion, and CUDA graph execution for efficient GPU inference. This narrows the speed gap between beam and greedy modes to only 10-20% for the whole system, achieves 14-30% relative improvement in WER compared to greedy decoding, and improves shallow fusion for low-resource up to 11% compared to existing implementations. All the algorithms are open sourced.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted to Interspeech 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.00185v1",
    "published_date": "2025-05-30 19:42:48 UTC",
    "updated_date": "2025-05-30 19:42:48 UTC"
  },
  {
    "arxiv_id": "2506.00178v2",
    "title": "Tournament of Prompts: Evolving LLM Instructions Through Structured Debates and Elo Ratings",
    "authors": [
      "Anirudh Nair",
      "Adi Banerjee",
      "Laurent Mombaerts",
      "Matthew Hagen",
      "Tarik Borogovac"
    ],
    "abstract": "Prompt engineering represents a critical bottleneck to harness the full potential of Large Language Models (LLMs) for solving complex tasks, as it requires specialized expertise, significant trial-and-error, and manual intervention. This challenge is particularly pronounced for tasks involving subjective quality assessment, where defining explicit optimization objectives becomes fundamentally problematic. Existing automated prompt optimization methods falter in these scenarios, as they typically require well-defined task-specific numerical fitness functions or rely on generic templates that cannot capture the nuanced requirements of complex use cases. We introduce DEEVO (DEbate-driven EVOlutionary prompt optimization), a novel framework that guides prompt evolution through a debate-driven evaluation with an Elo-based selection. Contrary to prior work, DEEVOs approach enables exploration of the discrete prompt space while preserving semantic coherence through intelligent crossover and strategic mutation operations that incorporate debate-based feedback, combining elements from both successful and unsuccessful prompts based on identified strengths rather than arbitrary splicing. Using Elo ratings as a fitness proxy, DEEVO simultaneously drives improvement and preserves valuable diversity in the prompt population. Experimental results demonstrate that DEEVO significantly outperforms both manual prompt engineering and alternative state-of-the-art optimization approaches on open-ended tasks and close-ended tasks despite using no ground truth feedback. By connecting LLMs reasoning capabilities with adaptive optimization, DEEVO represents a significant advancement in prompt optimization research by eliminating the need of predetermined metrics to continuously improve AI systems.",
    "categories": [
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.00178v2",
    "published_date": "2025-05-30 19:33:41 UTC",
    "updated_date": "2025-07-22 18:01:11 UTC"
  },
  {
    "arxiv_id": "2506.00175v3",
    "title": "Who Gets Credit or Blame? Attributing Accountability in Modern AI Systems",
    "authors": [
      "Shichang Zhang",
      "Hongzhe Du",
      "Jiaqi W. Ma",
      "Himabindu Lakkaraju"
    ],
    "abstract": "Modern AI systems are typically developed through multiple stages-pretraining, fine-tuning rounds, and subsequent adaptation or alignment, where each stage builds on the previous ones and updates the model in distinct ways. This raises a critical question of accountability: when a deployed model succeeds or fails, which stage is responsible, and to what extent? We pose the accountability attribution problem for tracing model behavior back to specific stages of the model development process. To address this challenge, we propose a general framework that answers counterfactual questions about stage effects: how would the model's behavior have changed if the updates from a particular stage had not occurred? Within this framework, we introduce estimators that efficiently quantify stage effects without retraining the model, accounting for both the data and key aspects of model optimization dynamics, including learning rate schedules, momentum, and weight decay. We demonstrate that our approach successfully quantifies the accountability of each stage to the model's behavior. Based on the attribution results, our method can identify and remove spurious correlations learned during image classification and text toxicity detection tasks that were developed across multiple stages. Our approach provides a practical tool for model analysis and represents a significant step toward more accountable AI development.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.00175v3",
    "published_date": "2025-05-30 19:27:39 UTC",
    "updated_date": "2025-09-05 21:05:27 UTC"
  },
  {
    "arxiv_id": "2506.00169v1",
    "title": "Utilizing AI for Aviation Post-Accident Analysis Classification",
    "authors": [
      "Aziida Nanyonga",
      "Graham Wild"
    ],
    "abstract": "The volume of textual data available in aviation safety reports presents a challenge for timely and accurate analysis. This paper examines how Artificial Intelligence (AI) and, specifically, Natural Language Processing (NLP) can automate the process of extracting valuable insights from this data, ultimately enhancing aviation safety. The paper reviews ongoing efforts focused on the application of NLP and deep learning to aviation safety reports, with the goal of classifying the level of damage to an aircraft and identifying the phase of flight during which safety occurrences happen. Additionally, the paper explores the use of Topic Modeling (TM) to uncover latent thematic structures within aviation incident reports, aiming to identify recurring patterns and potential areas for safety improvement. The paper compares and contrasts the performance of various deep learning models and TM techniques applied to datasets from the National Transportation Safety Board (NTSB) and the Australian Transport Safety Bureau (ATSB), as well as the Aviation Safety Network (ASN), discussing the impact of dataset size and source on the accuracy of the analysis. The findings demonstrate that both NLP and deep learning, as well as TM, can significantly improve the efficiency and accuracy of aviation safety analysis, paving the way for more proactive safety management and risk mitigation strategies.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.00169v1",
    "published_date": "2025-05-30 19:15:04 UTC",
    "updated_date": "2025-05-30 19:15:04 UTC"
  },
  {
    "arxiv_id": "2506.00166v1",
    "title": "Disentangled Safety Adapters Enable Efficient Guardrails and Flexible Inference-Time Alignment",
    "authors": [
      "Kundan Krishna",
      "Joseph Y Cheng",
      "Charles Maalouf",
      "Leon A Gatys"
    ],
    "abstract": "Existing paradigms for ensuring AI safety, such as guardrail models and alignment training, often compromise either inference efficiency or development flexibility. We introduce Disentangled Safety Adapters (DSA), a novel framework addressing these challenges by decoupling safety-specific computations from a task-optimized base model. DSA utilizes lightweight adapters that leverage the base model's internal representations, enabling diverse and flexible safety functionalities with minimal impact on inference cost. Empirically, DSA-based safety guardrails substantially outperform comparably sized standalone models, notably improving hallucination detection (0.88 vs. 0.61 AUC on Summedits) and also excelling at classifying hate speech (0.98 vs. 0.92 on ToxiGen) and unsafe model inputs and responses (0.93 vs. 0.90 on AEGIS2.0 & BeaverTails). Furthermore, DSA-based safety alignment allows dynamic, inference-time adjustment of alignment strength and a fine-grained trade-off between instruction following performance and model safety. Importantly, combining the DSA safety guardrail with DSA safety alignment facilitates context-dependent alignment strength, boosting safety on StrongReject by 93% while maintaining 98% performance on MTBench -- a total reduction in alignment tax of 8 percentage points compared to standard safety alignment fine-tuning. Overall, DSA presents a promising path towards more modular, efficient, and adaptable AI safety and alignment.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages, 2 figures, including references and appendix",
    "pdf_url": "https://arxiv.org/pdf/2506.00166v1",
    "published_date": "2025-05-30 19:11:52 UTC",
    "updated_date": "2025-05-30 19:11:52 UTC"
  },
  {
    "arxiv_id": "2506.00160v2",
    "title": "Verbal Werewolf: Engage Users with Verbalized Agentic Werewolf Game Framework",
    "authors": [
      "Qihui Fan",
      "Wenbo Li",
      "Enfu Nan",
      "Yixiao Chen",
      "Lei Lu",
      "Pu Zhao",
      "Yanzhi Wang"
    ],
    "abstract": "The growing popularity of social deduction games has created an increasing need for intelligent frameworks where humans can collaborate with AI agents, particularly in post-pandemic contexts with heightened psychological and social pressures. Social deduction games like Werewolf, traditionally played through verbal communication, present an ideal application for Large Language Models (LLMs) given their advanced reasoning and conversational capabilities. Prior studies have shown that LLMs can outperform humans in Werewolf games, but their reliance on external modules introduces latency that left their contribution in academic domain only, and omit such game should be user-facing. We propose \\textbf{Verbal Werewolf}, a novel LLM-based Werewolf game system that optimizes two parallel pipelines: gameplay powered by state-of-the-art LLMs and a fine-tuned Text-to-Speech (TTS) module that brings text output to life. Our system operates in near real-time without external decision-making modules, leveraging the enhanced reasoning capabilities of modern LLMs like DeepSeek V3 to create a more engaging and anthropomorphic gaming experience that significantly improves user engagement compared to existing text-only frameworks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.00160v2",
    "published_date": "2025-05-30 18:58:57 UTC",
    "updated_date": "2025-08-10 18:49:55 UTC"
  },
  {
    "arxiv_id": "2506.00154v1",
    "title": "Detection of Endangered Deer Species Using UAV Imagery: A Comparative Study Between Efficient Deep Learning Approaches",
    "authors": [
      "Agustín Roca",
      "Gastón Castro",
      "Gabriel Torre",
      "Leonardo J. Colombo",
      "Ignacio Mas",
      "Javier Pereira",
      "Juan I. Giribet"
    ],
    "abstract": "This study compares the performance of state-of-the-art neural networks including variants of the YOLOv11 and RT-DETR models for detecting marsh deer in UAV imagery, in scenarios where specimens occupy a very small portion of the image and are occluded by vegetation. We extend previous analysis adding precise segmentation masks for our datasets enabling a fine-grained training of a YOLO model with a segmentation head included. Experimental results show the effectiveness of incorporating the segmentation head achieving superior detection performance. This work contributes valuable insights for improving UAV-based wildlife monitoring and conservation strategies through scalable and accurate AI-driven detection systems.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.00154v1",
    "published_date": "2025-05-30 18:45:42 UTC",
    "updated_date": "2025-05-30 18:45:42 UTC"
  },
  {
    "arxiv_id": "2506.00150v1",
    "title": "Supporting architecture evaluation for ATAM scenarios with LLMs",
    "authors": [
      "Rafael Capilla",
      "J. Andrés Díaz-Pace",
      "Yamid Ramírez",
      "Jennifer Pérez",
      "Vanessa Rodríguez-Horcajo"
    ],
    "abstract": "Architecture evaluation methods have long been used to evaluate software designs. Several evaluation methods have been proposed and used to analyze tradeoffs between different quality attributes. Having competing qualities leads to conflicts for selecting which quality-attribute scenarios are the most suitable ones that an architecture should tackle and for prioritizing the scenarios required by the stakeholders. In this context, architecture evaluation is carried out manually, often involving long brainstorming sessions to decide which are the most adequate quality scenarios. To reduce this effort and make the assessment and selection of scenarios more efficient, we suggest the usage of LLMs to partially automate evaluation activities. As a first step to validate this hypothesis, this work studies MS Copilot as an LLM tool to analyze quality scenarios suggested by students in a software architecture course and compares the students' results with the assessment provided by the LLM. Our initial study reveals that the LLM produces in most cases better and more accurate results regarding the risks, sensitivity points and tradeoff analysis of the quality scenarios. Overall, the use of generative AI has the potential to partially automate and support the architecture evaluation tasks, improving the human decision-making process.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.00150v1",
    "published_date": "2025-05-30 18:42:12 UTC",
    "updated_date": "2025-05-30 18:42:12 UTC"
  },
  {
    "arxiv_id": "2506.00140v2",
    "title": "Balancing Profit and Fairness in Risk-Based Pricing Markets",
    "authors": [
      "Jesse Thibodeau",
      "Hadi Nekoei",
      "Afaf Taïk",
      "Janarthanan Rajendran",
      "Golnoosh Farnadi"
    ],
    "abstract": "Dynamic, risk-based pricing can systematically exclude vulnerable consumer groups from essential resources such as health insurance and consumer credit. We show that a regulator can realign private incentives with social objectives through a learned, interpretable tax schedule. First, we provide a formal proposition that bounding each firm's \\emph{local} demographic gap implicitly bounds the \\emph{global} opt-out disparity, motivating firm-level penalties. Building on this insight we introduce \\texttt{MarketSim} -- an open-source, scalable simulator of heterogeneous consumers and profit-maximizing firms -- and train a reinforcement learning (RL) social planner (SP) that selects a bracketed fairness-tax while remaining close to a simple linear prior via an $\\mathcal{L}_1$ regularizer. The learned policy is thus both transparent and easily interpretable. In two empirically calibrated markets, i.e., U.S. health-insurance and consumer-credit, our planner simultaneously raises demand-fairness by up to $16\\%$ relative to unregulated Free Market while outperforming a fixed linear schedule in terms of social welfare without explicit coordination. These results illustrate how AI-assisted regulation can convert a competitive social dilemma into a win-win equilibrium, providing a principled and practical framework for fairness-aware market oversight.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "econ.GN"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.00140v2",
    "published_date": "2025-05-30 18:24:08 UTC",
    "updated_date": "2025-06-04 16:06:36 UTC"
  },
  {
    "arxiv_id": "2506.00138v2",
    "title": "Intrinsic Goals for Autonomous Agents: Model-Based Exploration in Virtual Zebrafish Predicts Ethological Behavior and Whole-Brain Dynamics",
    "authors": [
      "Reece Keller",
      "Alyn Kirsch",
      "Felix Pei",
      "Xaq Pitkow",
      "Leo Kozachkov",
      "Aran Nayebi"
    ],
    "abstract": "Autonomy is a hallmark of animal intelligence, enabling adaptive and intelligent behavior in complex environments without relying on external reward or task structure. Existing reinforcement learning approaches to exploration in reward-free environments, including a class of methods known as model-based intrinsic motivation, exhibit inconsistent exploration patterns and do not converge to an exploratory policy, thus failing to capture robust autonomous behaviors observed in animals. Moreover, systems neuroscience has largely overlooked the neural basis of autonomy, focusing instead on experimental paradigms where animals are motivated by external reward rather than engaging in ethological, naturalistic and task-independent behavior. To bridge these gaps, we introduce a novel model-based intrinsic drive explicitly designed after the principles of autonomous exploration in animals. Our method (3M-Progress) achieves animal-like exploration by tracking divergence between an online world model and a fixed prior learned from an ecological niche. To the best of our knowledge, we introduce the first autonomous embodied agent that predicts brain data entirely from self-supervised optimization of an intrinsic goal -- without any behavioral or neural training data -- demonstrating that 3M-Progress agents capture the explainable variance in behavioral patterns and whole-brain neural-glial dynamics recorded from autonomously behaving larval zebrafish, thereby providing the first goal-driven, population-level model of neural-glial computation. Our findings establish a computational framework connecting model-based intrinsic motivation to naturalistic behavior, providing a foundation for building artificial agents with animal-like autonomy.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "q-bio.NC",
    "comment": "17 pages, 7 figures",
    "pdf_url": "https://arxiv.org/pdf/2506.00138v2",
    "published_date": "2025-05-30 18:21:40 UTC",
    "updated_date": "2025-10-24 17:52:29 UTC"
  },
  {
    "arxiv_id": "2506.00134v1",
    "title": "Spurious Correlations and Beyond: Understanding and Mitigating Shortcut Learning in SDOH Extraction with Large Language Models",
    "authors": [
      "Fardin Ahsan Sakib",
      "Ziwei Zhu",
      "Karen Trister Grace",
      "Meliha Yetisgen",
      "Ozlem Uzuner"
    ],
    "abstract": "Social determinants of health (SDOH) extraction from clinical text is critical for downstream healthcare analytics. Although large language models (LLMs) have shown promise, they may rely on superficial cues leading to spurious predictions. Using the MIMIC portion of the SHAC (Social History Annotation Corpus) dataset and focusing on drug status extraction as a case study, we demonstrate that mentions of alcohol or smoking can falsely induce models to predict current/past drug use where none is present, while also uncovering concerning gender disparities in model performance. We further evaluate mitigation strategies - such as prompt engineering and chain-of-thought reasoning - to reduce these false positives, providing insights into enhancing LLM reliability in health domains.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.00134v1",
    "published_date": "2025-05-30 18:11:33 UTC",
    "updated_date": "2025-05-30 18:11:33 UTC"
  },
  {
    "arxiv_id": "2506.00133v2",
    "title": "A Reinforcement Learning-Based Telematic Routing Protocol for the Internet of Underwater Things",
    "authors": [
      "Mohammadhossein Homaei",
      "Mehran Tarif",
      "Agustin Di Bartolo",
      "Victor Gonzalez Morales",
      "Mar Avila Vegas"
    ],
    "abstract": "The Internet of Underwater Things (IoUT) has a lot of problems, like low bandwidth, high latency, mobility, and not enough energy. Routing protocols that were made for land-based networks, like RPL, don't work well in these underwater settings. This paper talks about RL-RPL-UA, a new routing protocol that uses reinforcement learning to make things work better in underwater situations. Each node has a small RL agent that picks the best parent node depending on local data such the link quality, buffer level, packet delivery ratio, and remaining energy. RL-RPL-UA works with all standard RPL messages and adds a dynamic objective function to help people make decisions in real time. Aqua-Sim simulations demonstrate that RL-RPL-UA boosts packet delivery by up to 9.2%, uses 14.8% less energy per packet, and adds 80 seconds to the network's lifetime compared to previous approaches. These results show that RL-RPL-UA is a potential and energy-efficient way to route data in underwater networks.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NI",
    "comment": "8 Pages, 10 Figures, 2 Tables",
    "pdf_url": "https://arxiv.org/pdf/2506.00133v2",
    "published_date": "2025-05-30 18:11:31 UTC",
    "updated_date": "2025-11-21 09:32:43 UTC"
  },
  {
    "arxiv_id": "2506.00131v1",
    "title": "Adapting Offline Reinforcement Learning with Online Delays",
    "authors": [
      "Simon Sinong Zhan",
      "Qingyuan Wu",
      "Frank Yang",
      "Xiangyu Shi",
      "Chao Huang",
      "Qi Zhu"
    ],
    "abstract": "Offline-to-online deployment of reinforcement-learning (RL) agents must bridge two gaps: (1) the sim-to-real gap, where real systems add latency and other imperfections not present in simulation, and (2) the interaction gap, where policies trained purely offline face out-of-distribution states during online execution because gathering new interaction data is costly or risky. Agents therefore have to generalize from static, delay-free datasets to dynamic, delay-prone environments. Standard offline RL learns from delay-free logs yet must act under delays that break the Markov assumption and hurt performance. We introduce DT-CORL (Delay-Transformer belief policy Constrained Offline RL), an offline-RL framework built to cope with delayed dynamics at deployment. DT-CORL (i) produces delay-robust actions with a transformer-based belief predictor even though it never sees delayed observations during training, and (ii) is markedly more sample-efficient than naïve history-augmentation baselines. Experiments on D4RL benchmarks with several delay settings show that DT-CORL consistently outperforms both history-augmentation and vanilla belief-based methods, narrowing the sim-to-real latency gap while preserving data efficiency.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.00131v1",
    "published_date": "2025-05-30 18:09:29 UTC",
    "updated_date": "2025-05-30 18:09:29 UTC"
  },
  {
    "arxiv_id": "2505.24878v1",
    "title": "Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and Benchmarking Multimodal LLM Agents",
    "authors": [
      "Yaxin Luo",
      "Zhaoyi Li",
      "Jiacheng Liu",
      "Jiacheng Cui",
      "Xiaohan Zhao",
      "Zhiqiang Shen"
    ],
    "abstract": "CAPTCHAs have been a critical bottleneck for deploying web agents in real-world applications, often blocking them from completing end-to-end automation tasks. While modern multimodal LLM agents have demonstrated impressive performance in static perception tasks, their ability to handle interactive, multi-step reasoning challenges like CAPTCHAs is largely untested. To address this gap, we introduce Open CaptchaWorld, the first web-based benchmark and platform specifically designed to evaluate the visual reasoning and interaction capabilities of MLLM-powered agents through diverse and dynamic CAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225 CAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth, which quantifies the number of cognitive and motor steps required to solve each puzzle. Experimental results show that humans consistently achieve near-perfect scores, state-of-the-art MLLM agents struggle significantly, with success rates at most 40.0% by Browser-Use Openai-o3, far below human-level performance, 93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing the limits of current multimodal agents and guiding the development of more robust multimodal reasoning systems. Code and Data are available at this https URL.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Code at: https://github.com/MetaAgentX/OpenCaptchaWorld",
    "pdf_url": "https://arxiv.org/pdf/2505.24878v1",
    "published_date": "2025-05-30 17:59:55 UTC",
    "updated_date": "2025-05-30 17:59:55 UTC"
  },
  {
    "arxiv_id": "2505.24872v2",
    "title": "ProxyThinker: Test-Time Guidance through Small Visual Reasoners",
    "authors": [
      "Zilin Xiao",
      "Jaywon Koo",
      "Siru Ouyang",
      "Jefferson Hernandez",
      "Yu Meng",
      "Vicente Ordonez"
    ],
    "abstract": "Recent advancements in reinforcement learning with verifiable rewards have pushed the boundaries of the visual reasoning capabilities in large vision-language models (LVLMs). However, training LVLMs with reinforcement fine-tuning (RFT) is computationally expensive, posing a significant challenge to scaling model size. In this work, we propose ProxyThinker, an inference-time technique that enables large models to inherit the visual reasoning capabilities from small, slow-thinking visual reasoners without any training. By subtracting the output distributions of base models from those of RFT reasoners, ProxyThinker modifies the decoding dynamics and successfully elicits the slow-thinking reasoning demonstrated by the emerged sophisticated behaviors such as self-verification and self-correction. ProxyThinker consistently boosts performance on challenging visual benchmarks on spatial, mathematical, and multi-disciplinary reasoning, enabling untuned base models to compete with the performance of their full-scale RFT counterparts. Furthermore, our implementation efficiently coordinates multiple language models with parallelism techniques and achieves up to 38 $\\times$ faster inference compared to previous decoding-time methods, paving the way for the practical deployment of ProxyThinker. Code is available at https://github.com/MrZilinXiao/ProxyThinker.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24872v2",
    "published_date": "2025-05-30 17:59:43 UTC",
    "updated_date": "2025-09-27 02:58:54 UTC"
  },
  {
    "arxiv_id": "2505.24867v1",
    "title": "Time Blindness: Why Video-Language Models Can't See What Humans Can?",
    "authors": [
      "Ujjwal Upadhyay",
      "Mukul Ranjan",
      "Zhiqiang Shen",
      "Mohamed Elhoseiny"
    ],
    "abstract": "Recent advances in vision-language models (VLMs) have made impressive strides in understanding spatio-temporal relationships in videos. However, when spatial information is obscured, these models struggle to capture purely temporal patterns. We introduce $\\textbf{SpookyBench}$, a benchmark where information is encoded solely in temporal sequences of noise-like frames, mirroring natural phenomena from biological signaling to covert communication. Interestingly, while humans can recognize shapes, text, and patterns in these sequences with over 98% accuracy, state-of-the-art VLMs achieve 0% accuracy. This performance gap highlights a critical limitation: an over-reliance on frame-level spatial features and an inability to extract meaning from temporal cues. Furthermore, when trained in data sets with low spatial signal-to-noise ratios (SNR), temporal understanding of models degrades more rapidly than human perception, especially in tasks requiring fine-grained temporal reasoning. Overcoming this limitation will require novel architectures or training paradigms that decouple spatial dependencies from temporal processing. Our systematic analysis shows that this issue persists across model scales and architectures. We release SpookyBench to catalyze research in temporal pattern recognition and bridge the gap between human and machine video understanding. Dataset and code has been made available on our project website: https://timeblindness.github.io/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page at https://timeblindness.github.io/",
    "pdf_url": "https://arxiv.org/pdf/2505.24867v1",
    "published_date": "2025-05-30 17:59:12 UTC",
    "updated_date": "2025-05-30 17:59:12 UTC"
  },
  {
    "arxiv_id": "2505.24864v1",
    "title": "ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models",
    "authors": [
      "Mingjie Liu",
      "Shizhe Diao",
      "Ximing Lu",
      "Jian Hu",
      "Xin Dong",
      "Yejin Choi",
      "Jan Kautz",
      "Yi Dong"
    ],
    "abstract": "Recent advances in reasoning-centric language models have highlighted reinforcement learning (RL) as a promising method for aligning models with verifiable rewards. However, it remains contentious whether RL truly expands a model's reasoning capabilities or merely amplifies high-reward outputs already latent in the base model's distribution, and whether continually scaling up RL compute reliably leads to improved reasoning performance. In this work, we challenge prevailing assumptions by demonstrating that prolonged RL (ProRL) training can uncover novel reasoning strategies that are inaccessible to base models, even under extensive sampling. We introduce ProRL, a novel training methodology that incorporates KL divergence control, reference policy resetting, and a diverse suite of tasks. Our empirical analysis reveals that RL-trained models consistently outperform base models across a wide range of pass@k evaluations, including scenarios where base models fail entirely regardless of the number of attempts. We further show that reasoning boundary improvements correlates strongly with task competence of base model and training duration, suggesting that RL can explore and populate new regions of solution space over time. These findings offer new insights into the conditions under which RL meaningfully expands reasoning boundaries in language models and establish a foundation for future work on long-horizon RL for reasoning. We release model weights to support further research: https://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "26 pages, 17 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.24864v1",
    "published_date": "2025-05-30 17:59:01 UTC",
    "updated_date": "2025-05-30 17:59:01 UTC"
  },
  {
    "arxiv_id": "2506.15711v1",
    "title": "Shadow defense against gradient inversion attack in federated learning",
    "authors": [
      "Le Jiang",
      "Liyan Ma",
      "Guang Yang"
    ],
    "abstract": "Federated learning (FL) has emerged as a transformative framework for privacy-preserving distributed training, allowing clients to collaboratively train a global model without sharing their local data. This is especially crucial in sensitive fields like healthcare, where protecting patient data is paramount. However, privacy leakage remains a critical challenge, as the communication of model updates can be exploited by potential adversaries. Gradient inversion attacks (GIAs), for instance, allow adversaries to approximate the gradients used for training and reconstruct training images, thus stealing patient privacy. Existing defense mechanisms obscure gradients, yet lack a nuanced understanding of which gradients or types of image information are most vulnerable to such attacks. These indiscriminate calibrated perturbations result in either excessive privacy protection degrading model accuracy, or insufficient one failing to safeguard sensitive information. Therefore, we introduce a framework that addresses these challenges by leveraging a shadow model with interpretability for identifying sensitive areas. This enables a more targeted and sample-specific noise injection. Specially, our defensive strategy achieves discrepancies of 3.73 in PSNR and 0.2 in SSIM compared to the circumstance without defense on the ChestXRay dataset, and 2.78 in PSNR and 0.166 in the EyePACS dataset. Moreover, it minimizes adverse effects on model performance, with less than 1\\% F1 reduction compared to SOTA methods. Our extensive experiments, conducted across diverse types of medical images, validate the generalization of the proposed framework. The stable defense improvements for FedAvg are consistently over 1.5\\% times in LPIPS and SSIM. It also offers a universal defense against various GIA types, especially for these sensitive areas in images.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.15711v1",
    "published_date": "2025-05-30 17:58:57 UTC",
    "updated_date": "2025-05-30 17:58:57 UTC"
  },
  {
    "arxiv_id": "2506.15710v1",
    "title": "RAST: Reasoning Activation in LLMs via Small-model Transfer",
    "authors": [
      "Siru Ouyang",
      "Xinyu Zhu",
      "Zilin Xiao",
      "Minhao Jiang",
      "Yu Meng",
      "Jiawei Han"
    ],
    "abstract": "Reinforcement learning (RL) has become a powerful approach for improving the reasoning capabilities of large language models (LLMs), as evidenced by recent successes such as OpenAI's o1 and Deepseek-R1. However, applying RL at scale remains intimidatingly resource-intensive, requiring multiple model copies and extensive GPU workloads. On the other hand, while being powerful, recent studies suggest that RL does not fundamentally endow models with new knowledge; rather, it primarily reshapes the model's output distribution to activate reasoning capabilities latent in the base model. Building on this insight, we hypothesize that the changes in output probabilities induced by RL are largely model-size invariant, opening the door to a more efficient paradigm: training a small model with RL and transferring its induced probability shifts to larger base models. To verify our hypothesis, we conduct a token-level analysis of decoding trajectories and find high alignment in RL-induced output distributions across model scales, validating our hypothesis. Motivated by this, we propose RAST, a simple yet effective method that transfers reasoning behaviors by injecting RL-induced probability adjustments from a small RL-trained model into larger models. Experiments across multiple mathematical reasoning benchmarks show that RAST substantially and consistently enhances the reasoning capabilities of base models while requiring significantly lower GPU memory than direct RL training, sometimes even yielding better performance than the RL-trained counterparts. Our findings offer new insights into the nature of RL-driven reasoning and practical strategies for scaling its benefits without incurring its full computational cost. The project page of RAST is available at https://ozyyshr.github.io/RAST/.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.15710v1",
    "published_date": "2025-05-30 17:57:08 UTC",
    "updated_date": "2025-05-30 17:57:08 UTC"
  },
  {
    "arxiv_id": "2505.24853v1",
    "title": "DexMachina: Functional Retargeting for Bimanual Dexterous Manipulation",
    "authors": [
      "Zhao Mandi",
      "Yifan Hou",
      "Dieter Fox",
      "Yashraj Narang",
      "Ajay Mandlekar",
      "Shuran Song"
    ],
    "abstract": "We study the problem of functional retargeting: learning dexterous manipulation policies to track object states from human hand-object demonstrations. We focus on long-horizon, bimanual tasks with articulated objects, which is challenging due to large action space, spatiotemporal discontinuities, and embodiment gap between human and robot hands. We propose DexMachina, a novel curriculum-based algorithm: the key idea is to use virtual object controllers with decaying strength: an object is first driven automatically towards its target states, such that the policy can gradually learn to take over under motion and contact guidance. We release a simulation benchmark with a diverse set of tasks and dexterous hands, and show that DexMachina significantly outperforms baseline methods. Our algorithm and benchmark enable a functional comparison for hardware designs, and we present key findings informed by quantitative and qualitative results. With the recent surge in dexterous hand development, we hope this work will provide a useful platform for identifying desirable hardware capabilities and lower the barrier for contributing to future research. Videos and more at https://project-dexmachina.github.io/",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24853v1",
    "published_date": "2025-05-30 17:50:23 UTC",
    "updated_date": "2025-05-30 17:50:23 UTC"
  },
  {
    "arxiv_id": "2505.24850v2",
    "title": "Harnessing Negative Signals: Reinforcement Distillation from Teacher Data for LLM Reasoning",
    "authors": [
      "Shuyao Xu",
      "Cheng Peng",
      "Jiangxuan Long",
      "Weidi Xu",
      "Wei Chu",
      "Yuan Qi"
    ],
    "abstract": "Recent advances in model distillation show that data from advanced reasoning models can effectively train smaller student models. However, standard practices discard incorrect reasoning traces -- valuable, yet underutilized data. This paper addresses the critical question: How can both positive and negative distilled reasoning traces be effectively leveraged to maximize LLM reasoning performance in an offline setting? We employ a two-stage training recipe: first, Supervised Fine-Tuning (SFT) on positive traces, followed by a refinement stage using both positive and negative traces. We find that a simple REINFORCE-style objective, which we term the Reinforcement Distillation (REDI) objective, outperforms established preference optimization methods like DPO and SimPO in this distillation context. Our empirical evaluations demonstrate the effectiveness of this approach. Notably, our Qwen-REDI-1.5B model, trained on just 131k traces from the open Open-R1 dataset, achieves an 83.1% score on MATH-500. Its performance matches that of DeepSeek-R1-Distill-Qwen-1.5B, a model trained on 800k proprietary data. This result showcases the remarkable data efficiency of utilizing previously discarded negative traces.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "22 pages, 10 figures. Code available at https://github.com/Tim-Siu/reinforcement-distillation",
    "pdf_url": "https://arxiv.org/pdf/2505.24850v2",
    "published_date": "2025-05-30 17:47:17 UTC",
    "updated_date": "2025-12-14 17:32:05 UTC"
  },
  {
    "arxiv_id": "2506.02032v1",
    "title": "Towards Secure MLOps: Surveying Attacks, Mitigation Strategies, and Research Challenges",
    "authors": [
      "Raj Patel",
      "Himanshu Tripathi",
      "Jasper Stone",
      "Noorbakhsh Amiri Golilarz",
      "Sudip Mittal",
      "Shahram Rahimi",
      "Vini Chaudhary"
    ],
    "abstract": "The rapid adoption of machine learning (ML) technologies has driven organizations across diverse sectors to seek efficient and reliable methods to accelerate model development-to-deployment. Machine Learning Operations (MLOps) has emerged as an integrative approach addressing these requirements by unifying relevant roles and streamlining ML workflows. As the MLOps market continues to grow, securing these pipelines has become increasingly critical. However, the unified nature of MLOps ecosystem introduces vulnerabilities, making them susceptible to adversarial attacks where a single misconfiguration can lead to compromised credentials, severe financial losses, damaged public trust, and the poisoning of training data. Our paper presents a systematic application of the MITRE ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems) framework, a comprehensive and continuously updated catalog of AI-focused attacks, to systematically assess attacks across different phases of the MLOps ecosystem. We begin by examining the preparatory phases during which adversaries acquire the essential intelligence required to initiate their attacks. We then present a structured taxonomy of attack techniques explicitly mapped to corresponding phases of the MLOps ecosystem, supported by examples drawn from red-teaming exercises and real-world incidents. This is followed by a taxonomy of mitigation strategies aligned with these attack categories, offering actionable early-stage defenses to strengthen the security of MLOps ecosystem. Given the rapid evolution and adoption of MLOps, we further highlight key research gaps that require immediate attention. Our work emphasizes the importance of implementing robust security protocols from the outset, empowering practitioners to safeguard MLOps ecosystem against evolving cyber attacks.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.02032v1",
    "published_date": "2025-05-30 17:45:31 UTC",
    "updated_date": "2025-05-30 17:45:31 UTC"
  },
  {
    "arxiv_id": "2505.24846v2",
    "title": "MiCRo: Mixture Modeling and Context-aware Routing for Personalized Preference Learning",
    "authors": [
      "Jingyan Shen",
      "Jiarui Yao",
      "Rui Yang",
      "Yifan Sun",
      "Feng Luo",
      "Rui Pan",
      "Tong Zhang",
      "Han Zhao"
    ],
    "abstract": "Reward modeling is a key step in building safe foundation models when applying reinforcement learning from human feedback (RLHF) to align Large Language Models (LLMs). However, reward modeling based on the Bradley-Terry (BT) model assumes a global reward function, failing to capture the inherently diverse and heterogeneous human preferences. Hence, such oversimplification limits LLMs from supporting personalization and pluralistic alignment. Theoretically, we show that when human preferences follow a mixture distribution of diverse subgroups, a single BT model has an irreducible error. While existing solutions, such as multi-objective learning with fine-grained annotations, help address this issue, they are costly and constrained by predefined attributes, failing to fully capture the richness of human values. In this work, we introduce MiCRo, a two-stage framework that enhances personalized preference learning by leveraging large-scale binary preference datasets without requiring explicit fine-grained annotations. In the first stage, MiCRo introduces context-aware mixture modeling approach to capture diverse human preferences. In the second stage, MiCRo integrates an online routing strategy that dynamically adapts mixture weights based on specific context to resolve ambiguity, allowing for efficient and scalable preference adaptation with minimal additional supervision. Experiments on multiple preference datasets demonstrate that MiCRo effectively captures diverse human preferences and significantly improves downstream personalization.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24846v2",
    "published_date": "2025-05-30 17:44:28 UTC",
    "updated_date": "2025-09-22 19:02:16 UTC"
  },
  {
    "arxiv_id": "2505.24840v1",
    "title": "Vision LLMs Are Bad at Hierarchical Visual Understanding, and LLMs Are the Bottleneck",
    "authors": [
      "Yuwen Tan",
      "Yuan Qing",
      "Boqing Gong"
    ],
    "abstract": "This paper reveals that many state-of-the-art large language models (LLMs) lack hierarchical knowledge about our visual world, unaware of even well-established biology taxonomies. This shortcoming makes LLMs a bottleneck for vision LLMs' hierarchical visual understanding (e.g., recognizing Anemone Fish but not Vertebrate). We arrive at these findings using about one million four-choice visual question answering (VQA) tasks constructed from six taxonomies and four image datasets. Interestingly, finetuning a vision LLM using our VQA tasks reaffirms LLMs' bottleneck effect to some extent because the VQA tasks improve the LLM's hierarchical consistency more than the vision LLM's. We conjecture that one cannot make vision LLMs understand visual concepts fully hierarchical until LLMs possess corresponding taxonomy knowledge.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "28 pages, 13 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.24840v1",
    "published_date": "2025-05-30 17:40:46 UTC",
    "updated_date": "2025-05-30 17:40:46 UTC"
  },
  {
    "arxiv_id": "2505.24838v2",
    "title": "VideoCAD: A Dataset and Model for Learning Long-Horizon 3D CAD UI Interactions from Video",
    "authors": [
      "Brandon Man",
      "Ghadi Nehme",
      "Md Ferdous Alam",
      "Faez Ahmed"
    ],
    "abstract": "Computer-Aided Design (CAD) is a time-consuming and complex process, requiring precise, long-horizon user interactions with intricate 3D interfaces. While recent advances in AI-driven user interface (UI) agents show promise, most existing datasets and methods focus on short, low-complexity tasks in mobile or web applications, failing to capture the demands of professional engineering tools. In this work, we introduce VideoCAD, the first attempt to model UI interactions for precision engineering tasks. Specifically, VideoCAD is a large-scale synthetic dataset consisting of over 41K annotated video recordings of CAD operations, generated using an automated framework for collecting high-fidelity UI action data from human-made CAD designs. Compared to existing datasets, VideoCAD offers an order-of-magnitude increase in complexity for real-world engineering UI tasks, with time horizons up to 20x longer than those in other datasets. We show two important downstream applications of VideoCAD: (1) learning UI interactions from professional 3D CAD tools for precision tasks and (2) a visual question-answering (VQA) benchmark designed to evaluate multimodal large language models (LLMs) on spatial reasoning and video understanding. To learn the UI interactions, we propose VideoCADFormer, a state-of-the-art model for learning CAD interactions directly from video, which outperforms existing behavior cloning baselines. Both VideoCADFormer and the VQA benchmark derived from VideoCAD reveal key challenges in the current state of video-based UI understanding, including the need for precise action grounding, multi-modal and spatial reasoning, and long-horizon dependencies.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24838v2",
    "published_date": "2025-05-30 17:39:52 UTC",
    "updated_date": "2025-11-08 18:27:03 UTC"
  },
  {
    "arxiv_id": "2505.24830v3",
    "title": "Improving Reliability and Explainability of Medical Question Answering through Atomic Fact Checking in Retrieval-Augmented LLMs",
    "authors": [
      "Juraj Vladika",
      "Annika Domres",
      "Mai Nguyen",
      "Rebecca Moser",
      "Jana Nano",
      "Felix Busch",
      "Lisa C. Adams",
      "Keno K. Bressem",
      "Denise Bernhardt",
      "Stephanie E. Combs",
      "Kai J. Borm",
      "Florian Matthes",
      "Jan C. Peeken"
    ],
    "abstract": "Large language models (LLMs) exhibit extensive medical knowledge but are prone to hallucinations and inaccurate citations, which pose a challenge to their clinical adoption and regulatory compliance. Current methods, such as Retrieval Augmented Generation, partially address these issues by grounding answers in source documents, but hallucinations and low fact-level explainability persist. In this work, we introduce a novel atomic fact-checking framework designed to enhance the reliability and explainability of LLMs used in medical long-form question answering. This method decomposes LLM-generated responses into discrete, verifiable units called atomic facts, each of which is independently verified against an authoritative knowledge base of medical guidelines. This approach enables targeted correction of errors and direct tracing to source literature, thereby improving the factual accuracy and explainability of medical Q&A. Extensive evaluation using multi-reader assessments by medical experts and an automated open Q&A benchmark demonstrated significant improvements in factual accuracy and explainability. Our framework achieved up to a 40% overall answer improvement and a 50% hallucination detection rate. The ability to trace each atomic fact back to the most relevant chunks from the database provides a granular, transparent explanation of the generated responses, addressing a major gap in current medical AI applications. This work represents a crucial step towards more trustworthy and reliable clinical applications of LLMs, addressing key prerequisites for clinical application and fostering greater confidence in AI-assisted healthcare.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "18 pages, 7 figures and tables",
    "pdf_url": "https://arxiv.org/pdf/2505.24830v3",
    "published_date": "2025-05-30 17:33:07 UTC",
    "updated_date": "2025-12-30 10:11:21 UTC"
  },
  {
    "arxiv_id": "2505.24823v1",
    "title": "PhySense: Principle-Based Physics Reasoning Benchmarking for Large Language Models",
    "authors": [
      "Yinggan Xu",
      "Yue Liu",
      "Zhiqiang Gao",
      "Changnan Peng",
      "Di Luo"
    ],
    "abstract": "Large language models (LLMs) have rapidly advanced and are increasingly capable of tackling complex scientific problems, including those in physics. Despite this progress, current LLMs often fail to emulate the concise, principle-based reasoning characteristic of human experts, instead generating lengthy and opaque solutions. This discrepancy highlights a crucial gap in their ability to apply core physical principles for efficient and interpretable problem solving. To systematically investigate this limitation, we introduce PhySense, a novel principle-based physics reasoning benchmark designed to be easily solvable by experts using guiding principles, yet deceptively difficult for LLMs without principle-first reasoning. Our evaluation across multiple state-of-the-art LLMs and prompt types reveals a consistent failure to align with expert-like reasoning paths, providing insights for developing AI systems with efficient, robust and interpretable principle-based scientific reasoning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24823v1",
    "published_date": "2025-05-30 17:25:20 UTC",
    "updated_date": "2025-05-30 17:25:20 UTC"
  },
  {
    "arxiv_id": "2505.24808v1",
    "title": "RealDrive: Retrieval-Augmented Driving with Diffusion Models",
    "authors": [
      "Wenhao Ding",
      "Sushant Veer",
      "Yuxiao Chen",
      "Yulong Cao",
      "Chaowei Xiao",
      "Marco Pavone"
    ],
    "abstract": "Learning-based planners generate natural human-like driving behaviors by learning to reason about nuanced interactions from data, overcoming the rigid behaviors that arise from rule-based planners. Nonetheless, data-driven approaches often struggle with rare, safety-critical scenarios and offer limited controllability over the generated trajectories. To address these challenges, we propose RealDrive, a Retrieval-Augmented Generation (RAG) framework that initializes a diffusion-based planning policy by retrieving the most relevant expert demonstrations from the training dataset. By interpolating between current observations and retrieved examples through a denoising process, our approach enables fine-grained control and safe behavior across diverse scenarios, leveraging the strong prior provided by the retrieved scenario. Another key insight we produce is that a task-relevant retrieval model trained with planning-based objectives results in superior planning performance in our framework compared to a task-agnostic retriever. Experimental results demonstrate improved generalization to long-tail events and enhanced trajectory diversity compared to standard learning-based planners -- we observe a 40% reduction in collision rate on the Waymo Open Motion dataset with RAG.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24808v1",
    "published_date": "2025-05-30 17:15:03 UTC",
    "updated_date": "2025-05-30 17:15:03 UTC"
  },
  {
    "arxiv_id": "2506.00107v1",
    "title": "Gated Multimodal Graph Learning for Personalized Recommendation",
    "authors": [
      "Sibei Liu",
      "Yuanzhe Zhang",
      "Xiang Li",
      "Yunbo Liu",
      "Chengwei Feng",
      "Hao Yang"
    ],
    "abstract": "Multimodal recommendation has emerged as a promising solution to alleviate the cold-start and sparsity problems in collaborative filtering by incorporating rich content information, such as product images and textual descriptions. However, effectively integrating heterogeneous modalities into a unified recommendation framework remains a challenge. Existing approaches often rely on fixed fusion strategies or complex architectures , which may fail to adapt to modality quality variance or introduce unnecessary computational overhead.\n  In this work, we propose RLMultimodalRec, a lightweight and modular recommendation framework that combines graph-based user modeling with adaptive multimodal item encoding. The model employs a gated fusion module to dynamically balance the contribution of visual and textual modalities, enabling fine-grained and content-aware item representations. Meanwhile, a two-layer LightGCN encoder captures high-order collaborative signals by propagating embeddings over the user-item interaction graph without relying on nonlinear transformations.\n  We evaluate our model on a real-world dataset from the Amazon product domain. Experimental results demonstrate that RLMultimodalRec consistently outperforms several competitive baselines, including collaborative filtering, visual-aware, and multimodal GNN-based methods. The proposed approach achieves significant improvements in top-K recommendation metrics while maintaining scalability and interpretability, making it suitable for practical deployment.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.00107v1",
    "published_date": "2025-05-30 16:57:17 UTC",
    "updated_date": "2025-05-30 16:57:17 UTC"
  },
  {
    "arxiv_id": "2505.24791v1",
    "title": "Inference Acceleration of Autoregressive Normalizing Flows by Selective Jacobi Decoding",
    "authors": [
      "Jiaru Zhang",
      "Juanwu Lu",
      "Ziran Wang",
      "Ruqi Zhang"
    ],
    "abstract": "Normalizing flows are promising generative models with advantages such as theoretical rigor, analytical log-likelihood computation, and end-to-end training. However, the architectural constraints to ensure invertibility and tractable Jacobian computation limit their expressive power and practical usability. Recent advancements utilize autoregressive modeling, significantly enhancing expressive power and generation quality. However, such sequential modeling inherently restricts parallel computation during inference, leading to slow generation that impedes practical deployment. In this paper, we first identify that strict sequential dependency in inference is unnecessary to generate high-quality samples. We observe that patches in sequential modeling can also be approximated without strictly conditioning on all preceding patches. Moreover, the models tend to exhibit low dependency redundancy in the initial layer and higher redundancy in subsequent layers. Leveraging these observations, we propose a selective Jacobi decoding (SeJD) strategy that accelerates autoregressive inference through parallel iterative optimization. Theoretical analyses demonstrate the method's superlinear convergence rate and guarantee that the number of iterations required is no greater than the original sequential approach. Empirical evaluations across multiple datasets validate the generality and effectiveness of our acceleration technique. Experiments demonstrate substantial speed improvements up to 4.7 times faster inference while keeping the generation quality and fidelity.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24791v1",
    "published_date": "2025-05-30 16:53:15 UTC",
    "updated_date": "2025-05-30 16:53:15 UTC"
  },
  {
    "arxiv_id": "2505.24788v1",
    "title": "Drop Dropout on Single-Epoch Language Model Pretraining",
    "authors": [
      "Houjun Liu",
      "John Bauer",
      "Christopher D. Manning"
    ],
    "abstract": "Originally, dropout was seen as a breakthrough regularization technique that reduced overfitting and improved performance in almost all applications of deep learning by reducing overfitting. Yet, single-epoch pretraining tasks common to modern LLMs yield minimal overfitting, leading to dropout not being used for large LLMs. Nevertheless, no thorough empirical investigation has been done on the role of dropout in LM pretraining. Through experiments in single-epoch pretraining of both masked (BERT) and autoregressive (Pythia 160M and 1.4B) LMs with varying levels of dropout, we find that downstream performance in language modeling, morpho-syntax (BLiMP), question answering (SQuAD), and natural-language inference (MNLI) improves when dropout is not applied during pretraining. We additionally find that the recently-introduced \"early dropout\" also degrades performance over applying no dropout at all. We further investigate the models' editability, and find that models trained without dropout are more successful in gradient-based model editing (MEND) and equivalent in representation-based model editing (ReFT). Therefore, we advocate to drop dropout during single-epoch pretraining.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ACL Findings; 5 pages, 2 figures, 4 pages of appendix",
    "pdf_url": "https://arxiv.org/pdf/2505.24788v1",
    "published_date": "2025-05-30 16:48:38 UTC",
    "updated_date": "2025-05-30 16:48:38 UTC"
  },
  {
    "arxiv_id": "2505.24786v1",
    "title": "DiG-Net: Enhancing Quality of Life through Hyper-Range Dynamic Gesture Recognition in Assistive Robotics",
    "authors": [
      "Eran Bamani Beeri",
      "Eden Nissinman",
      "Avishai Sintov"
    ],
    "abstract": "Dynamic hand gestures play a pivotal role in assistive human-robot interaction (HRI), facilitating intuitive, non-verbal communication, particularly for individuals with mobility constraints or those operating robots remotely. Current gesture recognition methods are mostly limited to short-range interactions, reducing their utility in scenarios demanding robust assistive communication from afar. In this paper, we introduce a novel approach designed specifically for assistive robotics, enabling dynamic gesture recognition at extended distances of up to 30 meters, thereby significantly improving accessibility and quality of life. Our proposed Distance-aware Gesture Network (DiG-Net) effectively combines Depth-Conditioned Deformable Alignment (DADA) blocks with Spatio-Temporal Graph modules, enabling robust processing and classification of gesture sequences captured under challenging conditions, including significant physical attenuation, reduced resolution, and dynamic gesture variations commonly experienced in real-world assistive environments. We further introduce the Radiometric Spatio-Temporal Depth Attenuation Loss (RSTDAL), shown to enhance learning and strengthen model robustness across varying distances. Our model demonstrates significant performance improvement over state-of-the-art gesture recognition frameworks, achieving a recognition accuracy of 97.3% on a diverse dataset with challenging hyper-range gestures. By effectively interpreting gestures from considerable distances, DiG-Net significantly enhances the usability of assistive robots in home healthcare, industrial safety, and remote assistance scenarios, enabling seamless and intuitive interactions for users regardless of physical limitations",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "arXiv admin note: substantial text overlap with arXiv:2411.18413",
    "pdf_url": "https://arxiv.org/pdf/2505.24786v1",
    "published_date": "2025-05-30 16:47:44 UTC",
    "updated_date": "2025-05-30 16:47:44 UTC"
  },
  {
    "arxiv_id": "2505.24785v2",
    "title": "EXP-Bench: Can AI Conduct AI Research Experiments?",
    "authors": [
      "Patrick Tser Jern Kon",
      "Jiachen Liu",
      "Xinyi Zhu",
      "Qiuyi Ding",
      "Jingjia Peng",
      "Jiarong Xing",
      "Yibo Huang",
      "Yiming Qiu",
      "Jayanth Srinivasa",
      "Myungjin Lee",
      "Mosharaf Chowdhury",
      "Matei Zaharia",
      "Ang Chen"
    ],
    "abstract": "Automating AI research holds immense potential for accelerating scientific progress, yet current AI agents struggle with the complexities of rigorous, end-to-end experimentation. We introduce EXP-Bench, a novel benchmark designed to systematically evaluate AI agents on complete research experiments sourced from influential AI publications. Given a research question and incomplete starter code, EXP-Bench challenges AI agents to formulate hypotheses, design and implement experimental procedures, execute them, and analyze results. To enable the creation of such intricate and authentic tasks with high-fidelity, we design a semi-autonomous pipeline to extract and structure crucial experimental details from these research papers and their associated open-source code. With the pipeline, EXP-Bench curated 461 AI research tasks from 51 top-tier AI research papers. Evaluations of leading LLM-based agents, such as OpenHands and IterativeAgent on EXP-Bench demonstrate partial capabilities: while scores on individual experimental aspects such as design or implementation correctness occasionally reach 20-35%, the success rate for complete, executable experiments was a mere 0.5%. By identifying these bottlenecks and providing realistic step-by-step experiment procedures, EXP-Bench serves as a vital tool for future AI agents to improve their ability to conduct AI research experiments. EXP-Bench is open-sourced at https://github.com/Just-Curieous/Curie/tree/main/benchmark/exp_bench.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "45 pages, 13 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.24785v2",
    "published_date": "2025-05-30 16:46:29 UTC",
    "updated_date": "2025-06-02 01:59:50 UTC"
  },
  {
    "arxiv_id": "2505.24784v1",
    "title": "AXIOM: Learning to Play Games in Minutes with Expanding Object-Centric Models",
    "authors": [
      "Conor Heins",
      "Toon Van de Maele",
      "Alexander Tschantz",
      "Hampus Linander",
      "Dimitrije Markovic",
      "Tommaso Salvatori",
      "Corrado Pezzato",
      "Ozan Catal",
      "Ran Wei",
      "Magnus Koudahl",
      "Marco Perin",
      "Karl Friston",
      "Tim Verbelen",
      "Christopher Buckley"
    ],
    "abstract": "Current deep reinforcement learning (DRL) approaches achieve state-of-the-art performance in various domains, but struggle with data efficiency compared to human learning, which leverages core priors about objects and their interactions. Active inference offers a principled framework for integrating sensory information with prior knowledge to learn a world model and quantify the uncertainty of its own beliefs and predictions. However, active inference models are usually crafted for a single task with bespoke knowledge, so they lack the domain flexibility typical of DRL approaches. To bridge this gap, we propose a novel architecture that integrates a minimal yet expressive set of core priors about object-centric dynamics and interactions to accelerate learning in low-data regimes. The resulting approach, which we call AXIOM, combines the usual data efficiency and interpretability of Bayesian approaches with the across-task generalization usually associated with DRL. AXIOM represents scenes as compositions of objects, whose dynamics are modeled as piecewise linear trajectories that capture sparse object-object interactions. The structure of the generative model is expanded online by growing and learning mixture models from single events and periodically refined through Bayesian model reduction to induce generalization. AXIOM masters various games within only 10,000 interaction steps, with both a small number of parameters compared to DRL, and without the computational expense of gradient-based optimization.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages main text, 4 figures, 2 tables; 25 pages supplementary material, 8 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.24784v1",
    "published_date": "2025-05-30 16:46:20 UTC",
    "updated_date": "2025-05-30 16:46:20 UTC"
  },
  {
    "arxiv_id": "2505.24767v2",
    "title": "A survey of using EHR as real-world evidence for discovering and validating new drug indications",
    "authors": [
      "Nabasmita Talukdar",
      "Xiaodan Zhang",
      "Shreya Paithankar",
      "Hui Wang",
      "Bin Chen"
    ],
    "abstract": "Electronic Health Records (EHRs) have been increasingly used as real-world evidence (RWE) to support the discovery and validation of new drug indications. This paper surveys current approaches to EHR-based drug repurposing, covering data sources, processing methodologies, and representation techniques. It discusses study designs and statistical frameworks for evaluating drug efficacy. Key challenges in validation are discussed, with emphasis on the role of large language models (LLMs) and target trial emulation. By synthesizing recent developments and methodological advances, this work provides a foundational resource for researchers aiming to translate real-world data into actionable drug-repurposing evidence.",
    "categories": [
      "stat.AP",
      "cs.AI"
    ],
    "primary_category": "stat.AP",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24767v2",
    "published_date": "2025-05-30 16:30:54 UTC",
    "updated_date": "2025-11-20 13:56:05 UTC"
  },
  {
    "arxiv_id": "2505.24765v5",
    "title": "Supervised Quantum Machine Learning: A Future Outlook from Qubits to Enterprise Applications",
    "authors": [
      "Srikanth Thudumu",
      "Jason Fisher",
      "Hung Du"
    ],
    "abstract": "Supervised Quantum Machine Learning (QML) represents an intersection of quantum computing and classical machine learning, aiming to use quantum resources to support model training and inference. This paper reviews recent developments in supervised QML, focusing on methods such as variational quantum circuits, quantum neural networks, and quantum kernel methods, along with hybrid quantum-classical workflows. We examine recent experimental studies that show partial indications of quantum advantage and describe current limitations including noise, barren plateaus, scalability issues, and the lack of formal proofs of performance improvement over classical methods. The main contribution is a ten-year outlook (2025-2035) that outlines possible developments in supervised QML, including a roadmap describing conditions under which QML may be used in applied research and enterprise systems over the next decade.",
    "categories": [
      "quant-ph",
      "cs.AI"
    ],
    "primary_category": "quant-ph",
    "comment": "Future outlook and roadmap of QML with 7 pages and 1 figure",
    "pdf_url": "https://arxiv.org/pdf/2505.24765v5",
    "published_date": "2025-05-30 16:29:12 UTC",
    "updated_date": "2025-06-25 02:08:22 UTC"
  },
  {
    "arxiv_id": "2505.24760v2",
    "title": "REASONING GYM: Reasoning Environments for Reinforcement Learning with Verifiable Rewards",
    "authors": [
      "Zafir Stojanovski",
      "Oliver Stanley",
      "Joe Sharratt",
      "Richard Jones",
      "Abdulhakeem Adefioye",
      "Jean Kaddour",
      "Andreas Köpf"
    ],
    "abstract": "We introduce Reasoning Gym (RG), a library of reasoning environments for reinforcement learning with verifiable rewards. It provides over 100 data generators and verifiers spanning multiple domains including algebra, arithmetic, computation, cognition, geometry, graph theory, logic, and various common games. Its key innovation is the ability to generate virtually infinite training data with adjustable complexity, unlike most previous reasoning datasets, which are typically fixed. This procedural generation approach allows for continuous evaluation across varying difficulty levels. Our experimental results demonstrate the efficacy of RG in both evaluating and reinforcement learning of reasoning models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2025 Spotlight. For code, see https://github.com/open-thought/reasoning-gym",
    "pdf_url": "https://arxiv.org/pdf/2505.24760v2",
    "published_date": "2025-05-30 16:20:18 UTC",
    "updated_date": "2025-10-20 17:28:35 UTC"
  },
  {
    "arxiv_id": "2505.24759v3",
    "title": "Unsupervised Evolutionary Cell Type Matching via Entropy-Minimized Optimal Transport",
    "authors": [
      "Mu Qiao"
    ],
    "abstract": "Identifying evolutionary correspondences between cell types across species is a fundamental challenge in comparative genomics and evolutionary biology. Existing approaches often rely on either reference-based matching, which imposes asymmetry by designating one species as the reference, or projection-based matching, which may increase computational complexity and obscure biological interpretability at the cell-type level. Here, we present OT-MESH, an unsupervised computational framework leveraging entropy-regularized optimal transport (OT) to systematically determine cross-species cell type homologies. Our method uniquely integrates the Minimize Entropy of Sinkhorn (MESH) technique to refine the OT plan, transforming diffuse transport matrices into sparse, interpretable correspondences. Through systematic evaluation on synthetic datasets, we demonstrate that OT-MESH achieves near-optimal matching accuracy with computational efficiency, while maintaining remarkable robustness to noise. Compared to other OT-based methods like RefCM, OT-MESH provides speedup while achieving comparable accuracy. Applied to retinal bipolar cells (BCs) and retinal ganglion cells (RGCs) from mouse and macaque, OT-MESH accurately recovers known evolutionary relationships and uncovers novel correspondences, one of which was independently validated experimentally. Thus, our framework offers a principled, scalable, and interpretable solution for evolutionary cell type mapping, facilitating deeper insights into cellular specialization and conservation across species.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.QM",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24759v3",
    "published_date": "2025-05-30 16:20:00 UTC",
    "updated_date": "2025-11-04 03:28:36 UTC"
  },
  {
    "arxiv_id": "2505.24754v1",
    "title": "Don't Reinvent the Wheel: Efficient Instruction-Following Text Embedding based on Guided Space Transformation",
    "authors": [
      "Yingchaojie Feng",
      "Yiqun Sun",
      "Yandong Sun",
      "Minfeng Zhu",
      "Qiang Huang",
      "Anthony K. H. Tung",
      "Wei Chen"
    ],
    "abstract": "In this work, we investigate an important task named instruction-following text embedding, which generates dynamic text embeddings that adapt to user instructions, highlighting specific attributes of text. Despite recent advancements, existing approaches suffer from significant computational overhead, as they require re-encoding the entire corpus for each new instruction. To address this challenge, we propose GSTransform, a novel instruction-following text embedding framework based on Guided Space Transformation. Our key observation is that instruction-relevant information is inherently encoded in generic embeddings but remains underutilized. Instead of repeatedly encoding the corpus for each instruction, GSTransform is a lightweight transformation mechanism that adapts pre-computed embeddings in real time to align with user instructions, guided by a small amount of text data with instruction-focused label annotation. We conduct extensive experiments on three instruction-awareness downstream tasks across nine real-world datasets, demonstrating that GSTransform improves instruction-following text embedding quality over state-of-the-art methods while achieving dramatic speedups of 6~300x in real-time processing on large-scale datasets. The source code is available at https://github.com/YingchaojieFeng/GSTransform.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ACL 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.24754v1",
    "published_date": "2025-05-30 16:16:22 UTC",
    "updated_date": "2025-05-30 16:16:22 UTC"
  },
  {
    "arxiv_id": "2505.24722v2",
    "title": "HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts",
    "authors": [
      "Neil He",
      "Rishabh Anand",
      "Hiren Madhu",
      "Ali Maatouk",
      "Smita Krishnaswamy",
      "Leandros Tassiulas",
      "Menglin Yang",
      "Rex Ying"
    ],
    "abstract": "Large language models (LLMs) have shown great success in text modeling tasks across domains. However, natural language exhibits inherent semantic hierarchies and nuanced geometric structure, which current LLMs do not capture completely owing to their reliance on Euclidean operations. Recent studies have also shown that not respecting the geometry of token embeddings leads to training instabilities and degradation of generative capabilities. These findings suggest that shifting to non-Euclidean geometries can better align language models with the underlying geometry of text. We thus propose to operate fully in Hyperbolic space, known for its expansive, scale-free, and low-distortion properties. We thus introduce HELM, a family of HypErbolic Large Language Models, offering a geometric rethinking of the Transformer-based LLM that addresses the representational inflexibility, missing set of necessary operations, and poor scalability of existing hyperbolic LMs. We additionally introduce a Mixture-of-Curvature Experts model, HELM-MICE, where each expert operates in a distinct curvature space to encode more fine-grained geometric structure from text, as well as a dense model, HELM-D. For HELM-MICE, we further develop hyperbolic Multi-Head Latent Attention (HMLA) for efficient, reduced-KV-cache training and inference. For both models, we develop essential hyperbolic equivalents of rotary positional encodings and RMS normalization. We are the first to train fully hyperbolic LLMs at billion-parameter scale, and evaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM problem-solving, general knowledge, and commonsense reasoning. Our results show consistent gains from our HELM architectures -- up to 4% -- over popular Euclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy and enhanced reasoning afforded by hyperbolic geometry in large-scale LM pretraining.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24722v2",
    "published_date": "2025-05-30 15:42:42 UTC",
    "updated_date": "2025-11-06 01:18:03 UTC"
  },
  {
    "arxiv_id": "2505.24716v1",
    "title": "Towards Scalable Schema Mapping using Large Language Models",
    "authors": [
      "Christopher Buss",
      "Mahdis Safari",
      "Arash Termehchy",
      "Stefan Lee",
      "David Maier"
    ],
    "abstract": "The growing need to integrate information from a large number of diverse sources poses significant scalability challenges for data integration systems. These systems often rely on manually written schema mappings, which are complex, source-specific, and costly to maintain as sources evolve. While recent advances suggest that large language models (LLMs) can assist in automating schema matching by leveraging both structural and natural language cues, key challenges remain. In this paper, we identify three core issues with using LLMs for schema mapping: (1) inconsistent outputs due to sensitivity to input phrasing and structure, which we propose methods to address through sampling and aggregation techniques; (2) the need for more expressive mappings (e.g., GLaV), which strain the limited context windows of LLMs; and (3) the computational cost of repeated LLM calls, which we propose to mitigate through strategies like data type prefiltering.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24716v1",
    "published_date": "2025-05-30 15:36:56 UTC",
    "updated_date": "2025-05-30 15:36:56 UTC"
  },
  {
    "arxiv_id": "2506.06326v1",
    "title": "Memory OS of AI Agent",
    "authors": [
      "Jiazheng Kang",
      "Mingming Ji",
      "Zhe Zhao",
      "Ting Bai"
    ],
    "abstract": "Large Language Models (LLMs) face a crucial challenge from fixed context windows and inadequate memory management, leading to a severe shortage of long-term memory capabilities and limited personalization in the interactive experience with AI agents. To overcome this challenge, we innovatively propose a Memory Operating System, i.e., MemoryOS, to achieve comprehensive and efficient memory management for AI agents. Inspired by the memory management principles in operating systems, MemoryOS designs a hierarchical storage architecture and consists of four key modules: Memory Storage, Updating, Retrieval, and Generation. Specifically, the architecture comprises three levels of storage units: short-term memory, mid-term memory, and long-term personal memory. Key operations within MemoryOS include dynamic updates between storage units: short-term to mid-term updates follow a dialogue-chain-based FIFO principle, while mid-term to long-term updates use a segmented page organization strategy. Our pioneering MemoryOS enables hierarchical memory integration and dynamic updating. Extensive experiments on the LoCoMo benchmark show an average improvement of 49.11% on F1 and 46.18% on BLEU-1 over the baselines on GPT-4o-mini, showing contextual coherence and personalized memory retention in long conversations. The implementation code is open-sourced at https://github.com/BAI-LAB/MemoryOS.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06326v1",
    "published_date": "2025-05-30 15:36:51 UTC",
    "updated_date": "2025-05-30 15:36:51 UTC"
  },
  {
    "arxiv_id": "2505.24715v1",
    "title": "CoRet: Improved Retriever for Code Editing",
    "authors": [
      "Fabio Fehr",
      "Prabhu Teja Sivaprasad",
      "Luca Franceschi",
      "Giovanni Zappella"
    ],
    "abstract": "In this paper, we introduce CoRet, a dense retrieval model designed for code-editing tasks that integrates code semantics, repository structure, and call graph dependencies. The model focuses on retrieving relevant portions of a code repository based on natural language queries such as requests to implement new features or fix bugs. These retrieved code chunks can then be presented to a user or to a second code-editing model or agent. To train CoRet, we propose a loss function explicitly designed for repository-level retrieval. On SWE-bench and Long Code Arena's bug localisation datasets, we show that our model substantially improves retrieval recall by at least 15 percentage points over existing models, and ablate the design choices to show their importance in achieving these results.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "ACL 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.24715v1",
    "published_date": "2025-05-30 15:36:37 UTC",
    "updated_date": "2025-05-30 15:36:37 UTC"
  },
  {
    "arxiv_id": "2505.24710v1",
    "title": "Causal-aware Large Language Models: Enhancing Decision-Making Through Learning, Adapting and Acting",
    "authors": [
      "Wei Chen",
      "Jiahao Zhang",
      "Haipeng Zhu",
      "Boyan Xu",
      "Zhifeng Hao",
      "Keli Zhang",
      "Junjian Ye",
      "Ruichu Cai"
    ],
    "abstract": "Large language models (LLMs) have shown great potential in decision-making due to the vast amount of knowledge stored within the models. However, these pre-trained models are prone to lack reasoning abilities and are difficult to adapt to new environments, further hindering their application to complex real-world tasks. To address these challenges, inspired by the human cognitive process, we propose Causal-aware LLMs, which integrate the structural causal model (SCM) into the decision-making process to model, update, and utilize structured knowledge of the environment in a ``learning-adapting-acting\" paradigm. Specifically, in the learning stage, we first utilize an LLM to extract the environment-specific causal entities and their causal relations to initialize a structured causal model of the environment. Subsequently,in the adapting stage, we update the structured causal model through external feedback about the environment, via an idea of causal intervention. Finally, in the acting stage, Causal-aware LLMs exploit structured causal knowledge for more efficient policy-making through the reinforcement learning agent. The above processes are performed iteratively to learn causal knowledge, ultimately enabling the causal-aware LLMs to achieve a more accurate understanding of the environment and make more efficient decisions. Experimental results across 22 diverse tasks within the open-world game ``Crafter\" validate the effectiveness of our proposed method.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by IJCAI 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.24710v1",
    "published_date": "2025-05-30 15:30:44 UTC",
    "updated_date": "2025-05-30 15:30:44 UTC"
  },
  {
    "arxiv_id": "2505.24709v1",
    "title": "On Symmetric Losses for Robust Policy Optimization with Noisy Preferences",
    "authors": [
      "Soichiro Nishimori",
      "Yu-Jie Zhang",
      "Thanawat Lodkaew",
      "Masashi Sugiyama"
    ],
    "abstract": "Optimizing policies based on human preferences is key to aligning language models with human intent. This work focuses on reward modeling, a core component in reinforcement learning from human feedback (RLHF), and offline preference optimization, such as direct preference optimization. Conventional approaches typically assume accurate annotations. However, real-world preference data often contains noise due to human errors or biases. We propose a principled framework for robust policy optimization under noisy preferences, viewing reward modeling as a classification problem. This allows us to leverage symmetric losses, known for their robustness to label noise in classification, leading to our Symmetric Preference Optimization (SymPO) method. We prove that symmetric losses enable successful policy optimization even under noisy labels, as the resulting reward remains rank-preserving -- a property sufficient for policy improvement. Experiments on synthetic and real-world tasks demonstrate the effectiveness of SymPO.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24709v1",
    "published_date": "2025-05-30 15:30:43 UTC",
    "updated_date": "2025-05-30 15:30:43 UTC"
  },
  {
    "arxiv_id": "2505.24701v1",
    "title": "Multi-Domain ABSA Conversation Dataset Generation via LLMs for Real-World Evaluation and Model Comparison",
    "authors": [
      "Tejul Pandit",
      "Meet Raval",
      "Dhvani Upadhyay"
    ],
    "abstract": "Aspect-Based Sentiment Analysis (ABSA) offers granular insights into opinions but often suffers from the scarcity of diverse, labeled datasets that reflect real-world conversational nuances. This paper presents an approach for generating synthetic ABSA data using Large Language Models (LLMs) to address this gap. We detail the generation process aimed at producing data with consistent topic and sentiment distributions across multiple domains using GPT-4o. The quality and utility of the generated data were evaluated by assessing the performance of three state-of-the-art LLMs (Gemini 1.5 Pro, Claude 3.5 Sonnet, and DeepSeek-R1) on topic and sentiment classification tasks. Our results demonstrate the effectiveness of the synthetic data, revealing distinct performance trade-offs among the models: DeepSeekR1 showed higher precision, Gemini 1.5 Pro and Claude 3.5 Sonnet exhibited strong recall, and Gemini 1.5 Pro offered significantly faster inference. We conclude that LLM-based synthetic data generation is a viable and flexible method for creating valuable ABSA resources, facilitating research and model evaluation without reliance on limited or inaccessible real-world labeled data.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages, 3 figures, 5 tables, 6th International Conference on Natural Language Computing and AI (NLCAI 2025), ISBN : 978-1-923107-59-5, Computer Science & Information Technology (CS & IT), ISSN : 2231 - 5403, Volume 15, Number 10, May 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.24701v1",
    "published_date": "2025-05-30 15:24:17 UTC",
    "updated_date": "2025-05-30 15:24:17 UTC"
  },
  {
    "arxiv_id": "2506.15709v3",
    "title": "Studying and Improving Graph Neural Network-based Motif Estimation",
    "authors": [
      "Pedro C. Vieira",
      "Miguel E. P. Silva",
      "Pedro Manuel Pinto Ribeiro"
    ],
    "abstract": "Graph Neural Networks (GNNs) are a predominant method for graph representation learning. However, beyond subgraph frequency estimation, their application to network motif significance-profile (SP) prediction remains under-explored, with no established benchmarks in the literature. We propose to address this problem, framing SP estimation as a task independent of subgraph frequency estimation. Our approach shifts from frequency counting to direct SP estimation and modulates the problem as multitarget regression. The reformulation is optimised for interpretability, stability and scalability on large graphs. We validate our method using a large synthetic dataset and further test it on real-world graphs. Our experiments reveal that 1-WL limited models struggle to make precise estimations of SPs. However, they can generalise to approximate the graph generation processes of networks by comparing their predicted SP with the ones originating from synthetic generators. This first study on GNN-based motif estimation also hints at how using direct SP estimation can help go past the theoretical limitations that motif estimation faces when performed through subgraph counting.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This manuscript represents a revised version from the paper on https://openreview.net/forum?id=PZVVOeu6xx. Still a work in progress. Comments are welcome! 23 pages (12 main text + references), 9 figures, 5 tables. (Second update: More accurate Table 4, Run time comparisons.)",
    "pdf_url": "https://arxiv.org/pdf/2506.15709v3",
    "published_date": "2025-05-30 15:17:23 UTC",
    "updated_date": "2025-07-10 15:40:39 UTC"
  },
  {
    "arxiv_id": "2505.24684v1",
    "title": "Disentangling Granularity: An Implicit Inductive Bias in Factorized VAEs",
    "authors": [
      "Zihao Chen",
      "Yu Xiang",
      "Wenyong Wang"
    ],
    "abstract": "Despite the success in learning semantically meaningful, unsupervised disentangled representations, variational autoencoders (VAEs) and their variants face a fundamental theoretical challenge: substantial evidence indicates that unsupervised disentanglement is unattainable without implicit inductive bias, yet such bias remains elusive. In this work, we focus on exploring the implicit inductive bias that drive disentanglement in VAEs with factorization priors. By analyzing the total correlation in \\b{eta}-TCVAE, we uncover a crucial implicit inductive bias called disentangling granularity, which leads to the discovery of an interesting \"V\"-shaped optimal Evidence Lower Bound (ELBO) trajectory within the parameter space. This finding is validated through over 100K experiments using factorized VAEs and our newly proposed model, \\b{eta}-STCVAE. Notably, experimental results reveal that conventional factorized VAEs, constrained by fixed disentangling granularity, inherently tend to disentangle low-complexity feature. Whereas, appropriately tuning disentangling granularity, as enabled by \\b{eta}-STCVAE, broadens the range of disentangled representations, allowing for the disentanglement of high-complexity features. Our findings unveil that disentangling granularity as an implicit inductive bias in factorized VAEs influence both disentanglement performance and the inference of the ELBO, offering fresh insights into the interpretability and inherent biases of VAEs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24684v1",
    "published_date": "2025-05-30 15:08:50 UTC",
    "updated_date": "2025-05-30 15:08:50 UTC"
  },
  {
    "arxiv_id": "2505.24683v3",
    "title": "Should I Share this Translation? Evaluating Quality Feedback for User Reliance on Machine Translation",
    "authors": [
      "Dayeon Ki",
      "Kevin Duh",
      "Marine Carpuat"
    ],
    "abstract": "As people increasingly use AI systems in work and daily life, feedback mechanisms that help them use AI responsibly are urgently needed, particularly in settings where users are not equipped to assess the quality of AI predictions. We study a realistic Machine Translation (MT) scenario where monolingual users decide whether to share an MT output, first without and then with quality feedback. We compare four types of quality feedback: explicit feedback that directly give users an assessment of translation quality using (1) error highlights and (2) LLM explanations, and implicit feedback that helps users compare MT inputs and outputs through (3) backtranslation and (4) question-answer (QA) tables. We find that all feedback types, except error highlights, significantly improve both decision accuracy and appropriate reliance. Notably, implicit feedback, especially QA tables, yields significantly greater gains than explicit feedback in terms of decision accuracy, appropriate reliance, and user perceptions, receiving the highest ratings for helpfulness and trust, and the lowest for mental burden.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.24683v3",
    "published_date": "2025-05-30 15:08:10 UTC",
    "updated_date": "2025-10-02 03:34:50 UTC"
  },
  {
    "arxiv_id": "2505.24681v1",
    "title": "Generative Knowledge Production Pipeline Driven by Academic Influencers",
    "authors": [
      "Katalin Feher",
      "Marton Demeter"
    ],
    "abstract": "Generative AI transforms knowledge production, validation, and dissemination, raising academic integrity and credibility concerns. This study examines 53 academic influencer videos that reached 5.3 million viewers to identify an emerging, structured, implementation-ready pipeline balancing originality, ethical compliance, and human-AI collaboration despite the disruptive impacts. Findings highlight generative AI's potential to automate publication workflows and democratize participation in knowledge production while challenging traditional scientific norms. Academic influencers emerge as key intermediaries in this paradigm shift, connecting bottom-up practices with institutional policies to improve adaptability. Accordingly, the study proposes a generative publication production pipeline and a policy framework for co-intelligence adaptation and reinforcing credibility-centered standards in AI-powered research. These insights support scholars, educators, and policymakers in understanding AI's transformative impact by advocating responsible and innovation-driven knowledge production. Additionally, they reveal pathways for automating best practices, optimizing scholarly workflows, and fostering creativity in academic research and publication.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "cs.SI"
    ],
    "primary_category": "cs.CY",
    "comment": "15 pages, 1 figure, 2 tables, Horizon Europe NGI funding",
    "pdf_url": "https://arxiv.org/pdf/2505.24681v1",
    "published_date": "2025-05-30 15:07:01 UTC",
    "updated_date": "2025-05-30 15:07:01 UTC"
  },
  {
    "arxiv_id": "2505.24671v2",
    "title": "Multiple LLM Agents Debate for Equitable Cultural Alignment",
    "authors": [
      "Dayeon Ki",
      "Rachel Rudinger",
      "Tianyi Zhou",
      "Marine Carpuat"
    ],
    "abstract": "Large Language Models (LLMs) need to adapt their predictions to diverse cultural contexts to benefit diverse communities across the world. While previous efforts have focused on single-LLM, single-turn approaches, we propose to exploit the complementary strengths of multiple LLMs to promote cultural adaptability. We introduce a Multi-Agent Debate framework, where two LLM-based agents debate over a cultural scenario and collaboratively reach a final decision. We propose two variants: one where either LLM agents exclusively debate and another where they dynamically choose between self-reflection and debate during their turns. We evaluate these approaches on 7 open-weight LLMs (and 21 LLM combinations) using the NormAd-ETI benchmark for social etiquette norms in 75 countries. Experiments show that debate improves both overall accuracy and cultural group parity over single-LLM baselines. Notably, multi-agent debate enables relatively small LLMs (7-9B) to achieve accuracies comparable to that of a much larger model (27B parameters).",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL 2025 (Oral)",
    "pdf_url": "https://arxiv.org/pdf/2505.24671v2",
    "published_date": "2025-05-30 15:01:52 UTC",
    "updated_date": "2025-09-01 12:34:28 UTC"
  },
  {
    "arxiv_id": "2506.08026v2",
    "title": "TIP-Search: Time-Predictable Inference Scheduling for Market Prediction under Uncertain Load",
    "authors": [
      "Xibai Wang"
    ],
    "abstract": "This paper proposes TIP-Search, a time-predictable inference scheduling framework for real-time market prediction under uncertain workloads. Motivated by the strict latency demands in high-frequency financial systems, TIP-Search dynamically selects a deep learning model from a heterogeneous pool, aiming to maximize predictive accuracy while satisfying per-task deadline constraints. Our approach profiles latency and generalization performance offline, then performs online task-aware selection without relying on explicit input domain labels. We evaluate TIP-Search on three real-world limit order book datasets (FI-2010, Binance BTC/USDT, LOBSTER AAPL) and demonstrate that it outperforms static baselines with up to 8.5% improvement in accuracy and 100% deadline satisfaction. Our results highlight the effectiveness of TIP-Search in robust low-latency financial inference under uncertainty.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "eess.SY",
      "q-fin.CP"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.08026v2",
    "published_date": "2025-05-30 14:52:01 UTC",
    "updated_date": "2025-06-16 19:58:59 UTC"
  },
  {
    "arxiv_id": "2505.24655v1",
    "title": "Adaptable Cardiovascular Disease Risk Prediction from Heterogeneous Data using Large Language Models",
    "authors": [
      "Frederike Lübeck",
      "Jonas Wildberger",
      "Frederik Träuble",
      "Maximilian Mordig",
      "Sergios Gatidis",
      "Andreas Krause",
      "Bernhard Schölkopf"
    ],
    "abstract": "Cardiovascular disease (CVD) risk prediction models are essential for identifying high-risk individuals and guiding preventive actions. However, existing models struggle with the challenges of real-world clinical practice as they oversimplify patient profiles, rely on rigid input schemas, and are sensitive to distribution shifts. We developed AdaCVD, an adaptable CVD risk prediction framework built on large language models extensively fine-tuned on over half a million participants from the UK Biobank. In benchmark comparisons, AdaCVD surpasses established risk scores and standard machine learning approaches, achieving state-of-the-art performance. Crucially, for the first time, it addresses key clinical challenges across three dimensions: it flexibly incorporates comprehensive yet variable patient information; it seamlessly integrates both structured data and unstructured text; and it rapidly adapts to new patient populations using minimal additional data. In stratified analyses, it demonstrates robust performance across demographic, socioeconomic, and clinical subgroups, including underrepresented cohorts. AdaCVD offers a promising path toward more flexible, AI-driven clinical decision support tools suited to the realities of heterogeneous and dynamic healthcare environments.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24655v1",
    "published_date": "2025-05-30 14:42:02 UTC",
    "updated_date": "2025-05-30 14:42:02 UTC"
  },
  {
    "arxiv_id": "2505.24649v1",
    "title": "BIMA: Bijective Maximum Likelihood Learning Approach to Hallucination Prediction and Mitigation in Large Vision-Language Models",
    "authors": [
      "Huu-Thien Tran",
      "Thanh-Dat Truong",
      "Khoa Luu"
    ],
    "abstract": "Large vision-language models have become widely adopted to advance in various domains. However, developing a trustworthy system with minimal interpretable characteristics of large-scale models presents a significant challenge. One of the most prevalent terms associated with the fallacy functions caused by these systems is hallucination, where the language model generates a response that does not correspond to the visual content. To mitigate this problem, several approaches have been developed, and one prominent direction is to ameliorate the decoding process. In this paper, we propose a new Bijective Maximum Likelihood Learning (BIMA) approach to hallucination mitigation using normalizing flow theories. The proposed BIMA method can efficiently mitigate the hallucination problem in prevailing vision-language models, resulting in significant improvements. Notably, BIMA achieves the average F1 score of 85.06% on POPE benchmark and remarkably reduce CHAIRS and CHAIRI by 7.6% and 2.6%, respectively. To the best of our knowledge, this is one of the first studies that contemplates the bijection means to reduce hallucination induced by large vision-language models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPRW 2025, 8 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.24649v1",
    "published_date": "2025-05-30 14:38:07 UTC",
    "updated_date": "2025-05-30 14:38:07 UTC"
  },
  {
    "arxiv_id": "2505.24640v1",
    "title": "Efficient Text Encoders for Labor Market Analysis",
    "authors": [
      "Jens-Joris Decorte",
      "Jeroen Van Hautte",
      "Chris Develder",
      "Thomas Demeester"
    ],
    "abstract": "Labor market analysis relies on extracting insights from job advertisements, which provide valuable yet unstructured information on job titles and corresponding skill requirements. While state-of-the-art methods for skill extraction achieve strong performance, they depend on large language models (LLMs), which are computationally expensive and slow. In this paper, we propose \\textbf{ConTeXT-match}, a novel contrastive learning approach with token-level attention that is well-suited for the extreme multi-label classification task of skill classification. \\textbf{ConTeXT-match} significantly improves skill extraction efficiency and performance, achieving state-of-the-art results with a lightweight bi-encoder model. To support robust evaluation, we introduce \\textbf{Skill-XL}, a new benchmark with exhaustive, sentence-level skill annotations that explicitly address the redundancy in the large label space. Finally, we present \\textbf{JobBERT V2}, an improved job title normalization model that leverages extracted skills to produce high-quality job title representations. Experiments demonstrate that our models are efficient, accurate, and scalable, making them ideal for large-scale, real-time labor market analysis.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "This work has been submitted to the IEEE for possible publication",
    "pdf_url": "https://arxiv.org/pdf/2505.24640v1",
    "published_date": "2025-05-30 14:27:25 UTC",
    "updated_date": "2025-05-30 14:27:25 UTC"
  },
  {
    "arxiv_id": "2505.24638v1",
    "title": "Cloud Optical Thickness Retrievals Using Angle Invariant Attention Based Deep Learning Models",
    "authors": [
      "Zahid Hassan Tushar",
      "Adeleke Ademakinwa",
      "Jianwu Wang",
      "Zhibo Zhang",
      "Sanjay Purushotham"
    ],
    "abstract": "Cloud Optical Thickness (COT) is a critical cloud property influencing Earth's climate, weather, and radiation budget. Satellite radiance measurements enable global COT retrieval, but challenges like 3D cloud effects, viewing angles, and atmospheric interference must be addressed to ensure accurate estimation. Traditionally, the Independent Pixel Approximation (IPA) method, which treats individual pixels independently, has been used for COT estimation. However, IPA introduces significant bias due to its simplified assumptions. Recently, deep learning-based models have shown improved performance over IPA but lack robustness, as they are sensitive to variations in radiance intensity, distortions, and cloud shadows. These models also introduce substantial errors in COT estimation under different solar and viewing zenith angles. To address these challenges, we propose a novel angle-invariant, attention-based deep model called Cloud-Attention-Net with Angle Coding (CAAC). Our model leverages attention mechanisms and angle embeddings to account for satellite viewing geometry and 3D radiative transfer effects, enabling more accurate retrieval of COT. Additionally, our multi-angle training strategy ensures angle invariance. Through comprehensive experiments, we demonstrate that CAAC significantly outperforms existing state-of-the-art deep learning models, reducing cloud property retrieval errors by at least a factor of nine.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "6 pages, 7 figures, to be published in 2025 IEEE International Conference on Image Processing (ICIP)",
    "pdf_url": "https://arxiv.org/pdf/2505.24638v1",
    "published_date": "2025-05-30 14:26:30 UTC",
    "updated_date": "2025-05-30 14:26:30 UTC"
  },
  {
    "arxiv_id": "2505.24630v2",
    "title": "Reasoning Models Hallucinate More: Factuality-Aware Reinforcement Learning for Large Reasoning Models",
    "authors": [
      "Junyi Li",
      "Hwee Tou Ng"
    ],
    "abstract": "Large language models (LLMs) have significantly advanced in reasoning tasks through reinforcement learning (RL) optimization, achieving impressive capabilities across various challenging benchmarks. However, our empirical analysis reveals a critical drawback: reasoning-oriented RL fine-tuning significantly increases the prevalence of hallucinations. We theoretically analyze the RL training dynamics, identifying high-variance gradient, entropy-induced randomness, and susceptibility to spurious local optima as key factors leading to hallucinations. To address this drawback, we propose Factuality-aware Step-wise Policy Optimization (FSPO), an innovative RL fine-tuning algorithm incorporating explicit factuality verification at each reasoning step. FSPO leverages automated verification against given evidence to dynamically adjust token-level advantage values, incentivizing factual correctness throughout the reasoning process. Experiments across mathematical reasoning and hallucination benchmarks using Qwen2.5 and Llama models demonstrate that FSPO effectively reduces hallucinations while enhancing reasoning accuracy, substantially improving both reliability and performance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "accepted by NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.24630v2",
    "published_date": "2025-05-30 14:23:32 UTC",
    "updated_date": "2025-11-06 09:04:26 UTC"
  },
  {
    "arxiv_id": "2505.24625v3",
    "title": "Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision Geometry Priors",
    "authors": [
      "Duo Zheng",
      "Shijia Huang",
      "Yanyang Li",
      "Liwei Wang"
    ],
    "abstract": "Previous research has investigated the application of Multimodal Large Language Models (MLLMs) in understanding 3D scenes by interpreting them as videos. These approaches generally depend on comprehensive 3D data inputs, such as point clouds or reconstructed Bird's-Eye View (BEV) maps. In our research, we advance this field by enhancing the capability of MLLMs to understand and reason in 3D spaces directly from video data, without the need for additional 3D input. We propose a novel and efficient method called the Video-3D Geometry Large Language Model (VG LLM). Our approach utilizes a 3D visual geometry encoder to extract 3D prior information from video sequences. This information is then integrated with visual tokens and input into the MLLM. Extensive experiments have shown that our method has achieved substantial improvements in various tasks related to 3D scene understanding and spatial reasoning, all directly learned from video sources. Impressively, our 4B model, which does not rely on explicit 3D data inputs, achieves competitive results compared to existing state-of-the-art methods, and even surpasses the Gemini-1.5-Pro in the VSI-Bench evaluations.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.24625v3",
    "published_date": "2025-05-30 14:16:41 UTC",
    "updated_date": "2025-10-22 14:04:01 UTC"
  },
  {
    "arxiv_id": "2505.24623v2",
    "title": "Hyperbolic Dataset Distillation",
    "authors": [
      "Wenyuan Li",
      "Guang Li",
      "Keisuke Maeda",
      "Takahiro Ogawa",
      "Miki Haseyama"
    ],
    "abstract": "To address the computational and storage challenges posed by large-scale datasets in deep learning, dataset distillation has been proposed to synthesize a compact dataset that replaces the original while maintaining comparable model performance. Unlike optimization-based approaches that require costly bi-level optimization, distribution matching (DM) methods improve efficiency by aligning the distributions of synthetic and original data, thereby eliminating nested optimization. DM achieves high computational efficiency and has emerged as a promising solution. However, existing DM methods, constrained to Euclidean space, treat data as independent and identically distributed points, overlooking complex geometric and hierarchical relationships. To overcome this limitation, we propose a novel hyperbolic dataset distillation method, termed HDD. Hyperbolic space, characterized by negative curvature and exponential volume growth with distance, naturally models hierarchical and tree-like structures. HDD embeds features extracted by a shallow network into the Lorentz hyperbolic space, where the discrepancy between synthetic and original data is measured by the hyperbolic (geodesic) distance between their centroids. By optimizing this distance, the hierarchical structure is explicitly integrated into the distillation process, guiding synthetic samples to gravitate towards the root-centric regions of the original data distribution while preserving their underlying geometric characteristics. Furthermore, we find that pruning in hyperbolic space requires only 20% of the distilled core set to retain model performance, while significantly improving training stability. To the best of our knowledge, this is the first work to incorporate the hyperbolic space into the dataset distillation process. The code is available at https://github.com/Guang000/HDD.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.24623v2",
    "published_date": "2025-05-30 14:14:00 UTC",
    "updated_date": "2025-10-17 01:12:34 UTC"
  },
  {
    "arxiv_id": "2505.24622v2",
    "title": "Random Rule Forest (RRF): Interpretable Ensembles of LLM-Generated Questions for Predicting Startup Success",
    "authors": [
      "Ben Griffin",
      "Diego Vidaurre",
      "Ugur Koyluoglu",
      "Joseph Ternasky",
      "Fuat Alican",
      "Yigit Ihlamur"
    ],
    "abstract": "Predicting rare outcomes such as startup success is central to venture capital, demanding models that are both accurate and interpretable. We introduce Random Rule Forest (RRF), a lightweight ensemble method that uses a large language model (LLM) to generate simple YES/NO questions in natural language. Each question functions as a weak learner, and their responses are combined using a threshold-based voting rule to form a strong, interpretable predictor.\n  Applied to a dataset of 9,892 founders, RRF achieves a 6.9x improvement over a random baseline on held-out data; adding expert-crafted questions lifts this to 8x and highlights the value of human-LLM collaboration. Compared with zero- and few-shot baselines across three LLM architectures, RRF attains an F0.5 of 0.121, versus 0.086 for the best baseline (+0.035 absolute, +41% relative). By combining the creativity of LLMs with the rigor of ensemble learning, RRF delivers interpretable, high-precision predictions suitable for decision-making in high-stakes domains.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "13 pages including appendix, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.24622v2",
    "published_date": "2025-05-30 14:13:21 UTC",
    "updated_date": "2025-09-15 18:17:48 UTC"
  },
  {
    "arxiv_id": "2505.24616v4",
    "title": "Eye of Judgement: Dissecting the Evaluation of Russian-speaking LLMs with POLLUX",
    "authors": [
      "Nikita Martynov",
      "Anastasia Mordasheva",
      "Dmitriy Gorbetskiy",
      "Danil Astafurov",
      "Ulyana Isaeva",
      "Elina Basyrova",
      "Sergey Skachkov",
      "Victoria Berestova",
      "Nikolay Ivanov",
      "Valeriia Zanina",
      "Alena Fenogenova"
    ],
    "abstract": "We introduce POLLUX, a comprehensive open-source benchmark designed to evaluate the generative capabilities of large language models (LLMs) in Russian. Our main contribution is a novel evaluation methodology that enhances the interpretability of LLM assessment. For each task type, we define a set of detailed criteria and develop a scoring protocol where models evaluate responses and provide justifications for their ratings. This enables transparent, criteria-driven evaluation beyond traditional resource-consuming, side-by-side human comparisons. POLLUX includes a detailed, fine-grained taxonomy of 35 task types covering diverse generative domains such as code generation, creative writing, and practical assistant use cases, totaling 2,100 manually crafted and professionally authored prompts. Each task is categorized by difficulty (easy/medium/hard), with experts constructing the dataset entirely from scratch. We also release a family of LLM-as-a-Judge (7B and 32B) evaluators trained for nuanced assessment of generative outputs. This approach provides scalable, interpretable evaluation and annotation tools for model development, effectively replacing costly and less precise human judgments.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "short version",
    "pdf_url": "https://arxiv.org/pdf/2505.24616v4",
    "published_date": "2025-05-30 14:08:17 UTC",
    "updated_date": "2025-12-01 14:46:24 UTC"
  },
  {
    "arxiv_id": "2505.24601v1",
    "title": "Taxonomic Networks: A Representation for Neuro-Symbolic Pairing",
    "authors": [
      "Zekun Wang",
      "Ethan L. Haarer",
      "Nicki Barari",
      "Christopher J. MacLellan"
    ],
    "abstract": "We introduce the concept of a \\textbf{neuro-symbolic pair} -- neural and symbolic approaches that are linked through a common knowledge representation. Next, we present \\textbf{taxonomic networks}, a type of discrimination network in which nodes represent hierarchically organized taxonomic concepts. Using this representation, we construct a novel neuro-symbolic pair and evaluate its performance. We show that our symbolic method learns taxonomic nets more efficiently with less data and compute, while the neural method finds higher-accuracy taxonomic nets when provided with greater resources. As a neuro-symbolic pair, these approaches can be used interchangeably based on situational needs, with seamless translation between them when necessary. This work lays the foundation for future systems that more fundamentally integrate neural and symbolic computation.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 3 figures, NeuS 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.24601v1",
    "published_date": "2025-05-30 13:48:34 UTC",
    "updated_date": "2025-05-30 13:48:34 UTC"
  },
  {
    "arxiv_id": "2505.24597v1",
    "title": "Mixture-of-Experts for Personalized and Semantic-Aware Next Location Prediction",
    "authors": [
      "Shuai Liu",
      "Ning Cao",
      "Yile Chen",
      "Yue Jiang",
      "Gao Cong"
    ],
    "abstract": "Next location prediction plays a critical role in understanding human mobility patterns. However, existing approaches face two core limitations: (1) they fall short in capturing the complex, multi-functional semantics of real-world locations; and (2) they lack the capacity to model heterogeneous behavioral dynamics across diverse user groups. To tackle these challenges, we introduce NextLocMoE, a novel framework built upon large language models (LLMs) and structured around a dual-level Mixture-of-Experts (MoE) design. Our architecture comprises two specialized modules: a Location Semantics MoE that operates at the embedding level to encode rich functional semantics of locations, and a Personalized MoE embedded within the Transformer backbone to dynamically adapt to individual user mobility patterns. In addition, we incorporate a history-aware routing mechanism that leverages long-term trajectory data to enhance expert selection and ensure prediction stability. Empirical evaluations across several real-world urban datasets show that NextLocMoE achieves superior performance in terms of predictive accuracy, cross-domain generalization, and interpretability",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24597v1",
    "published_date": "2025-05-30 13:45:19 UTC",
    "updated_date": "2025-05-30 13:45:19 UTC"
  },
  {
    "arxiv_id": "2505.24595v3",
    "title": "BinConv: A Neural Architecture for Ordinal Encoding in Time-Series Forecasting",
    "authors": [
      "Andrei Chernov",
      "Vitaliy Pozdnyakov",
      "Ilya Makarov"
    ],
    "abstract": "Recent work in time series forecasting has explored reformulating regression as a classification task. By discretizing the continuous target space into bins and predicting over a fixed set of classes, these approaches benefit from more stable training, improved uncertainty modeling, and compatibility with modern deep learning architectures. However, most existing methods rely on one-hot encoding, which ignores the inherent ordinal structure of the target values. As a result, they fail to convey information about the relative distance between predicted and true values during training. In this paper, we address this limitation by applying \\textbf{Cumulative Binary Encoding} (CBE), a monotonic binary representation that transforms both model inputs and outputs. CBE implicitly preserves ordinal and magnitude information, allowing models to learn distance aware representations while operating within a classification framework. To leverage CBE effectively, we propose \\textbf{BinConv}, a fully convolutional neural network architecture designed for probabilistic forecasting. We demonstrate that standard fully connected layers are not only less computationally efficient than convolutional layers when used with CBE, but also degrade forecasting performance. Our experiments on standard benchmark datasets show that BinConv achieves superior performance compared to widely used baselines in both point and probabilistic forecasting, while requiring fewer parameters and enabling faster training.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24595v3",
    "published_date": "2025-05-30 13:41:39 UTC",
    "updated_date": "2025-08-27 14:18:09 UTC"
  },
  {
    "arxiv_id": "2505.24593v2",
    "title": "Decoding Knowledge Attribution in Mixture-of-Experts: A Framework of Basic-Refinement Collaboration and Efficiency Analysis",
    "authors": [
      "Junzhuo Li",
      "Bo Wang",
      "Xiuze Zhou",
      "Peijie Jiang",
      "Jia Liu",
      "Xuming Hu"
    ],
    "abstract": "The interpretability of Mixture-of-Experts (MoE) models, especially those with heterogeneous designs, remains underexplored. Existing attribution methods for dense models fail to capture dynamic routing-expert interactions in sparse MoE architectures. To address this issue, we propose a cross-level attribution algorithm to analyze sparse MoE architectures (Qwen 1.5-MoE, OLMoE, Mixtral-8x7B) against dense models (Qwen 1.5-7B, Llama-7B, Mistral-7B). Results show MoE models achieve 37% higher per-layer efficiency via a \"mid-activation, late-amplification\" pattern: early layers screen experts, while late layers refine knowledge collaboratively. Ablation studies reveal a \"basic-refinement\" framework--shared experts handle general tasks (entity recognition), while routed experts specialize in domain-specific processing (geographic attributes). Semantic-driven routing is evidenced by strong correlations between attention heads and experts (r=0.68), enabling task-aware coordination. Notably, architectural depth dictates robustness: deep Qwen 1.5-MoE mitigates expert failures (e.g., 43% MRR drop in geographic tasks when blocking top-10 experts) through shared expert redundancy, whereas shallow OLMoE suffers severe degradation (76% drop). Task sensitivity further guides design: core-sensitive tasks (geography) require concentrated expertise, while distributed-tolerant tasks (object attributes) leverage broader participation. These insights advance MoE interpretability, offering principles to balance efficiency, specialization, and robustness.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.24593v2",
    "published_date": "2025-05-30 13:40:51 UTC",
    "updated_date": "2025-06-11 10:28:53 UTC"
  },
  {
    "arxiv_id": "2505.24592v3",
    "title": "A Flat Minima Perspective on Understanding Augmentations and Model Robustness",
    "authors": [
      "Weebum Yoo",
      "Sung Whan Yoon"
    ],
    "abstract": "Model robustness indicates a model's capability to generalize well on unforeseen distributional shifts, including data corruptions and adversarial attacks. Data augmentation is one of the most prevalent and effective ways to enhance robustness. Despite the great success of the diverse augmentations in different fields, a unified theoretical understanding of their efficacy in improving model robustness is lacking. We theoretically reveal a general condition for label-preserving augmentations to bring robustness to diverse distribution shifts through the lens of flat minima and generalization bound, which de facto turns out to be strongly correlated with robustness against different distribution shifts in practice. Unlike most earlier works, our theoretical framework accommodates all the label-preserving augmentations and is not limited to particular distribution shifts. We substantiate our theories through different simulations on the existing common corruption and adversarial robustness benchmarks based on the CIFAR and ImageNet datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "In Proceedings of the Association for the Advancement of Artificial Intelligence (AAAI) 2026, Singapore",
    "pdf_url": "https://arxiv.org/pdf/2505.24592v3",
    "published_date": "2025-05-30 13:40:44 UTC",
    "updated_date": "2025-12-14 16:21:00 UTC"
  },
  {
    "arxiv_id": "2505.24584v3",
    "title": "AutoChemSchematic AI: Agentic Physics-Aware Automation for Chemical Manufacturing Scale-Up",
    "authors": [
      "Sakhinana Sagar Srinivas",
      "Shivam Gupta",
      "Venkataramana Runkana"
    ],
    "abstract": "Recent advances in generative AI have accelerated the discovery of novel chemicals and materials. However, scaling these discoveries to industrial production remains a major bottleneck due to the synthesis gap -- the need to develop entirely new manufacturing processes. This challenge requires detailed engineering blueprints: PFDs for equipment layouts and material/energy flows, and PIDs for process plant operations. Current AI systems cannot yet reliably generate these critical engineering schematics, creating a fundamental obstacle to manufacturing scale-up of novel discoveries. We present a closed-loop, physics-aware framework for automated generation of industrially viable PFDs and PIDs. The framework integrates three key components: (1) domain-specialized small language models (SLMs) trained for auto-generation of PFDs and PIDs, (2) a hierarchical knowledge graph containing process flow and instrumentation descriptions for 1,020+ chemicals for Graph Retrieval-Augmented Generation (GRAG), and (3) an open-source chemical process simulator for modeling, simulation, optimization, and analysis of novel chemical processes. The SLMs are trained through a multi-stage pipeline on synthetic datasets, with process simulator-in-the-loop validation ensuring feasibility. To enhance computational efficiency, the framework implements structural pruning (width and depth) guided by importance heuristics to reduce language model size while preserving accuracy, followed by advanced inference optimizations including FlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization, and Test-Time Inference Scaling. Experimental results demonstrate that our framework generates simulator-validated process descriptions with high fidelity.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24584v3",
    "published_date": "2025-05-30 13:32:00 UTC",
    "updated_date": "2025-08-18 16:52:22 UTC"
  },
  {
    "arxiv_id": "2505.24575v1",
    "title": "NexusSum: Hierarchical LLM Agents for Long-Form Narrative Summarization",
    "authors": [
      "Hyuntak Kim",
      "Byung-Hak Kim"
    ],
    "abstract": "Summarizing long-form narratives--such as books, movies, and TV scripts--requires capturing intricate plotlines, character interactions, and thematic coherence, a task that remains challenging for existing LLMs. We introduce NexusSum, a multi-agent LLM framework for narrative summarization that processes long-form text through a structured, sequential pipeline--without requiring fine-tuning. Our approach introduces two key innovations: (1) Dialogue-to-Description Transformation: A narrative-specific preprocessing method that standardizes character dialogue and descriptive text into a unified format, improving coherence. (2) Hierarchical Multi-LLM Summarization: A structured summarization pipeline that optimizes chunk processing and controls output length for accurate, high-quality summaries. Our method establishes a new state-of-the-art in narrative summarization, achieving up to a 30.0% improvement in BERTScore (F1) across books, movies, and TV scripts. These results demonstrate the effectiveness of multi-agent LLMs in handling long-form content, offering a scalable approach for structured summarization in diverse storytelling domains.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to the main track of ACL 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.24575v1",
    "published_date": "2025-05-30 13:26:23 UTC",
    "updated_date": "2025-05-30 13:26:23 UTC"
  },
  {
    "arxiv_id": "2506.00100v2",
    "title": "Children's Voice Privacy: First Steps And Emerging Challenges",
    "authors": [
      "Ajinkya Kulkarni",
      "Francisco Teixeira",
      "Enno Hermann",
      "Thomas Rolland",
      "Isabel Trancoso",
      "Mathew Magimai Doss"
    ],
    "abstract": "Children are one of the most under-represented groups in speech technologies, as well as one of the most vulnerable in terms of privacy. Despite this, anonymization techniques targeting this population have received little attention. In this study, we seek to bridge this gap, and establish a baseline for the use of voice anonymization techniques designed for adult speech when applied to children's voices. Such an evaluation is essential, as children's speech presents a distinct set of challenges when compared to that of adults. This study comprises three children's datasets, six anonymization methods, and objective and subjective utility metrics for evaluation. Our results show that existing systems for adults are still able to protect children's voice privacy, but suffer from much higher utility degradation. In addition, our subjective study displays the challenges of automatic evaluation methods for speech quality in children's speech, highlighting the need for further research.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.CR"
    ],
    "primary_category": "cs.CY",
    "comment": "Accepted at Interspeech 2025, Netherlands",
    "pdf_url": "https://arxiv.org/pdf/2506.00100v2",
    "published_date": "2025-05-30 13:21:18 UTC",
    "updated_date": "2025-06-04 13:30:41 UTC"
  },
  {
    "arxiv_id": "2506.06325v1",
    "title": "Evolutionary model for energy trading in community microgrids using Hawk-Dove strategies",
    "authors": [
      "Viorica Rozina Chifu",
      "Tudor Cioara",
      "Cristina Bianca Pop",
      "Ionut Anghel"
    ],
    "abstract": "This paper proposes a decentralized model of energy cooperation between microgrids, in which decisions are made locally, at the level of the microgrid community. Each microgrid is modeled as an autonomous agent that adopts a Hawk or Dove strategy, depending on the level of energy stored in the battery and its role in the energy trading process. The interactions between selling and buying microgrids are modeled through an evolutionary algorithm. An individual in the algorithm population is represented as an energy trading matrix that encodes the amounts of energy traded between the selling and buying microgrids. The population evolution is achieved by recombination and mutation operators. Recombination uses a specialized operator for matrix structures, and mutation is applied to the matrix elements according to a Gaussian distribution. The evaluation of an individual is made with a multi-criteria fitness function that considers the seller profit, the degree of energy stability at the community level, penalties for energy imbalance at the community level and for the degradation of microgrids batteries. The method was tested on a simulated scenario with 100 microgrids, each with its own selling and buying thresholds, to reflect a realistic environment with variable storage characteristics of microgrids batteries. By applying the algorithm on this scenario, 95 out of the 100 microgrids reached a stable energy state. This result confirms the effectiveness of the proposed model in achieving energy balance both at the individual level, for each microgrid, and at the level of the entire community.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.GT",
      "cs.MA"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.06325v1",
    "published_date": "2025-05-30 13:04:01 UTC",
    "updated_date": "2025-05-30 13:04:01 UTC"
  },
  {
    "arxiv_id": "2505.24554v3",
    "title": "Bench4KE: Benchmarking Automated Competency Question Generation",
    "authors": [
      "Anna Sofia Lippolis",
      "Minh Davide Ragagni",
      "Paolo Ciancarini",
      "Andrea Giovanni Nuzzolese",
      "Valentina Presutti"
    ],
    "abstract": "The availability of Large Language Models (LLMs) presents a unique opportunity to reinvigorate research on Knowledge Engineering (KE) automation. This trend is already evident in recent efforts developing LLM-based methods and tools for the automatic generation of Competency Questions (CQs), natural language questions used by ontology engineers to define the functional requirements of an ontology. However, the evaluation of these tools lacks standardization. This undermines the methodological rigor and hinders the replication and comparison of results. To address this gap, we introduce Bench4KE, an extensible API-based benchmarking system for KE automation. The presented release focuses on evaluating tools that generate CQs automatically. Bench4KE provides a curated gold standard consisting of CQ datasets from 17 real-world ontology engineering projects and uses a suite of similarity metrics to assess the quality of the CQs generated. We present a comparative analysis of 6 recent CQ generation systems, which are based on LLMs, establishing a baseline for future research. Bench4KE is also designed to accommodate additional KE automation tasks, such as SPARQL query generation, ontology testing and drafting. Code and datasets are publicly available under the Apache 2.0 license.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24554v3",
    "published_date": "2025-05-30 13:03:42 UTC",
    "updated_date": "2025-12-09 10:54:56 UTC"
  },
  {
    "arxiv_id": "2505.24553v1",
    "title": "CREFT: Sequential Multi-Agent LLM for Character Relation Extraction",
    "authors": [
      "Ye Eun Chun",
      "Taeyoon Hwang",
      "Seung-won Hwang",
      "Byung-Hak Kim"
    ],
    "abstract": "Understanding complex character relations is crucial for narrative analysis and efficient script evaluation, yet existing extraction methods often fail to handle long-form narratives with nuanced interactions. To address this challenge, we present CREFT, a novel sequential framework leveraging specialized Large Language Model (LLM) agents. First, CREFT builds a base character graph through knowledge distillation, then iteratively refines character composition, relation extraction, role identification, and group assignments. Experiments on a curated Korean drama dataset demonstrate that CREFT significantly outperforms single-agent LLM baselines in both accuracy and completeness. By systematically visualizing character networks, CREFT streamlines narrative comprehension and accelerates script review -- offering substantial benefits to the entertainment, publishing, and educational sectors.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24553v1",
    "published_date": "2025-05-30 13:01:36 UTC",
    "updated_date": "2025-05-30 13:01:36 UTC"
  },
  {
    "arxiv_id": "2505.24544v3",
    "title": "Cross-Attention Speculative Decoding",
    "authors": [
      "Wei Zhong",
      "Manasa Bharadwaj",
      "Yixiao Wang",
      "Nikhil Verma",
      "Yipeng Ji",
      "Chul Lee"
    ],
    "abstract": "Speculative decoding (SD) is a widely adopted approach for accelerating inference in large language models (LLMs), particularly when the draft and target models are well aligned. However, state-of-the-art SD methods typically rely on tightly coupled, self-attention-based Transformer decoders, often augmented with auxiliary pooling or fusion layers. This coupling makes them increasingly complex and harder to generalize across different models. We present Budget EAGLE (Beagle), the first, to our knowledge, cross-attention-based Transformer decoder SD model that achieves performance on par with leading self-attention SD models (EAGLE-v2) while eliminating the need for pooling or auxiliary components, simplifying the architecture, improving training efficiency, and maintaining stable memory usage during training-time simulation. To enable effective training of this novel architecture, we propose Two-Stage Block-Attention Training, a new method that achieves training stability and convergence efficiency in block-level attention scenarios. Extensive experiments across multiple LLMs and datasets show that Beagle achieves competitive inference speedups and higher training efficiency than EAGLE-v2, offering a strong alternative for architectures in speculative decoding.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24544v3",
    "published_date": "2025-05-30 12:52:35 UTC",
    "updated_date": "2025-09-22 01:23:11 UTC"
  },
  {
    "arxiv_id": "2505.24541v1",
    "title": "Mixpert: Mitigating Multimodal Learning Conflicts with Efficient Mixture-of-Vision-Experts",
    "authors": [
      "Xin He",
      "Xumeng Han",
      "Longhui Wei",
      "Lingxi Xie",
      "Qi Tian"
    ],
    "abstract": "Multimodal large language models (MLLMs) require a nuanced interpretation of complex image information, typically leveraging a vision encoder to perceive various visual scenarios. However, relying solely on a single vision encoder to handle diverse task domains proves difficult and inevitably leads to conflicts. Recent work enhances data perception by directly integrating multiple domain-specific vision encoders, yet this structure adds complexity and limits the potential for joint optimization. In this paper, we introduce Mixpert, an efficient mixture-of-vision-experts architecture that inherits the joint learning advantages from a single vision encoder while being restructured into a multi-expert paradigm for task-specific fine-tuning across different visual tasks. Additionally, we design a dynamic routing mechanism that allocates input images to the most suitable visual expert. Mixpert effectively alleviates domain conflicts encountered by a single vision encoder in multi-task learning with minimal additional computational cost, making it more efficient than multiple encoders. Furthermore, Mixpert integrates seamlessly into any MLLM, with experimental results demonstrating substantial performance gains across various tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24541v1",
    "published_date": "2025-05-30 12:48:07 UTC",
    "updated_date": "2025-05-30 12:48:07 UTC"
  },
  {
    "arxiv_id": "2505.24539v3",
    "title": "Localizing Persona Representations in LLMs",
    "authors": [
      "Celia Cintas",
      "Miriam Rateike",
      "Erik Miehling",
      "Elizabeth Daly",
      "Skyler Speakman"
    ],
    "abstract": "We present a study on how and where personas -- defined by distinct sets of human characteristics, values, and beliefs -- are encoded in the representation space of large language models (LLMs). Using a range of dimension reduction and pattern recognition methods, we first identify the model layers that show the greatest divergence in encoding these representations. We then analyze the activations within a selected layer to examine how specific personas are encoded relative to others, including their shared and distinct embedding spaces. We find that, across multiple pre-trained decoder-only LLMs, the analyzed personas show large differences in representation space only within the final third of the decoder layers. We observe overlapping activations for specific ethical perspectives -- such as moral nihilism and utilitarianism -- suggesting a degree of polysemy. In contrast, political ideologies like conservatism and liberalism appear to be represented in more distinct regions. These findings help to improve our understanding of how LLMs internally represent information and can inform future efforts in refining the modulation of specific human traits in LLM outputs. Warning: This paper includes potentially offensive sample statements.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "To appear in the AAAI/ACM Conference on AI, Ethics, and Society (AIES) 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.24539v3",
    "published_date": "2025-05-30 12:46:44 UTC",
    "updated_date": "2025-09-08 18:14:07 UTC"
  },
  {
    "arxiv_id": "2505.24536v1",
    "title": "CHIP: Chameleon Hash-based Irreversible Passport for Robust Deep Model Ownership Verification and Active Usage Control",
    "authors": [
      "Chaohui Xu",
      "Qi Cui",
      "Chip-Hong Chang"
    ],
    "abstract": "The pervasion of large-scale Deep Neural Networks (DNNs) and their enormous training costs make their intellectual property (IP) protection of paramount importance. Recently introduced passport-based methods attempt to steer DNN watermarking towards strengthening ownership verification against ambiguity attacks by modulating the affine parameters of normalization layers. Unfortunately, neither watermarking nor passport-based methods provide a holistic protection with robust ownership proof, high fidelity, active usage authorization and user traceability for offline access distributed models and multi-user Machine-Learning as a Service (MLaaS) cloud model. In this paper, we propose a Chameleon Hash-based Irreversible Passport (CHIP) protection framework that utilizes the cryptographic chameleon hash function to achieve all these goals. The collision-resistant property of chameleon hash allows for strong model ownership claim upon IP infringement and liable user traceability, while the trapdoor-collision property enables hashing of multiple user passports and licensee certificates to the same immutable signature to realize active usage control. Using the owner passport as an oracle, multiple user-specific triplets, each contains a passport-aware user model, a user passport, and a licensee certificate can be created for secure offline distribution. The watermarked master model can also be deployed for MLaaS with usage permission verifiable by the provision of any trapdoor-colliding user passports. CHIP is extensively evaluated on four datasets and two architectures to demonstrate its protection versatility and robustness. Our code is released at https://github.com/Dshm212/CHIP.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24536v1",
    "published_date": "2025-05-30 12:41:51 UTC",
    "updated_date": "2025-05-30 12:41:51 UTC"
  },
  {
    "arxiv_id": "2505.24535v2",
    "title": "Beyond Linear Steering: Unified Multi-Attribute Control for Language Models",
    "authors": [
      "Narmeen Oozeer",
      "Luke Marks",
      "Fazl Barez",
      "Amirali Abdullah"
    ],
    "abstract": "Controlling multiple behavioral attributes in large language models (LLMs) at inference time is a challenging problem due to interference between attributes and the limitations of linear steering methods, which assume additive behavior in activation space and require per-attribute tuning. We introduce K-Steering, a unified and flexible approach that trains a single non-linear multi-label classifier on hidden activations and computes intervention directions via gradients at inference time. This avoids linearity assumptions, removes the need for storing and tuning separate attribute vectors, and allows dynamic composition of behaviors without retraining. To evaluate our method, we propose two new benchmarks, ToneBank and DebateMix, targeting compositional behavioral control. Empirical results across 3 model families, validated by both activation-based classifiers and LLM-based judges, demonstrate that K-Steering outperforms strong baselines in accurately steering multiple behaviors.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to Findings of EMNLP, 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.24535v2",
    "published_date": "2025-05-30 12:41:19 UTC",
    "updated_date": "2025-09-19 10:58:16 UTC"
  },
  {
    "arxiv_id": "2505.24533v1",
    "title": "Directional Non-Commutative Monoidal Structures with Interchange Law via Commutative Generators",
    "authors": [
      "Mahesh Godavarti"
    ],
    "abstract": "We introduce a novel framework consisting of a class of algebraic structures that generalize one-dimensional monoidal systems into higher dimensions by defining per-axis composition operators subject to non-commutativity and a global interchange law. These structures, defined recursively from a base case of vector-matrix pairs, model directional composition in multiple dimensions while preserving structural coherence through commutative linear operators.\n  We show that the framework that unifies several well-known linear transforms in signal processing and data analysis. In this framework, data indices are embedded into a composite structure that decomposes into simpler components. We show that classic transforms such as the Discrete Fourier Transform (DFT), the Walsh transform, and the Hadamard transform are special cases of our algebraic structure. The framework provides a systematic way to derive these transforms by appropriately choosing vector and matrix pairs. By subsuming classical transforms within a common structure, the framework also enables the development of learnable transformations tailored to specific data modalities and tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24533v1",
    "published_date": "2025-05-30 12:40:01 UTC",
    "updated_date": "2025-05-30 12:40:01 UTC"
  },
  {
    "arxiv_id": "2506.03186v1",
    "title": "Lightweight Convolutional Neural Networks for Retinal Disease Classification",
    "authors": [
      "Duaa Kareem Qasim",
      "Sabah Abdulazeez Jebur",
      "Lafta Raheem Ali",
      "Abdul Jalil M. Khalaf",
      "Abir Jaafar Hussain"
    ],
    "abstract": "Retinal diseases such as Diabetic Retinopathy (DR) and Macular Hole (MH) significantly impact vision and affect millions worldwide. Early detection is crucial, as DR, a complication of diabetes, damages retinal blood vessels, potentially leading to blindness, while MH disrupts central vision, affecting tasks like reading and facial recognition. This paper employed two lightweight and efficient Convolution Neural Network architectures, MobileNet and NASNetMobile, for the classification of Normal, DR, and MH retinal images. The models were trained on the RFMiD dataset, consisting of 3,200 fundus images, after undergoing preprocessing steps such as resizing, normalization, and augmentation. To address data scarcity, this study leveraged transfer learning and data augmentation techniques, enhancing model generalization and performance. The experimental results demonstrate that MobileNetV2 achieved the highest accuracy of 90.8%, outperforming NASNetMobile, which achieved 89.5% accuracy. These findings highlight the effectiveness of CNNs in retinal disease classification, providing a foundation for AI-assisted ophthalmic diagnosis and early intervention.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.03186v1",
    "published_date": "2025-05-30 12:36:45 UTC",
    "updated_date": "2025-05-30 12:36:45 UTC"
  },
  {
    "arxiv_id": "2505.24523v1",
    "title": "Stress-testing Machine Generated Text Detection: Shifting Language Models Writing Style to Fool Detectors",
    "authors": [
      "Andrea Pedrotti",
      "Michele Papucci",
      "Cristiano Ciaccio",
      "Alessio Miaschi",
      "Giovanni Puccetti",
      "Felice Dell'Orletta",
      "Andrea Esuli"
    ],
    "abstract": "Recent advancements in Generative AI and Large Language Models (LLMs) have enabled the creation of highly realistic synthetic content, raising concerns about the potential for malicious use, such as misinformation and manipulation. Moreover, detecting Machine-Generated Text (MGT) remains challenging due to the lack of robust benchmarks that assess generalization to real-world scenarios. In this work, we present a pipeline to test the resilience of state-of-the-art MGT detectors (e.g., Mage, Radar, LLM-DetectAIve) to linguistically informed adversarial attacks. To challenge the detectors, we fine-tune language models using Direct Preference Optimization (DPO) to shift the MGT style toward human-written text (HWT). This exploits the detectors' reliance on stylistic clues, making new generations more challenging to detect. Additionally, we analyze the linguistic shifts induced by the alignment and which features are used by detectors to detect MGT texts. Our results show that detectors can be easily fooled with relatively few examples, resulting in a significant drop in detection performance. This highlights the importance of improving detection methods and making them robust to unseen in-domain texts.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at Findings of ACL 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.24523v1",
    "published_date": "2025-05-30 12:33:30 UTC",
    "updated_date": "2025-05-30 12:33:30 UTC"
  },
  {
    "arxiv_id": "2505.24511v4",
    "title": "Can Slow-thinking LLMs Reason Over Time? Empirical Studies in Time Series Forecasting",
    "authors": [
      "Mingyue Cheng",
      "Jiahao Wang",
      "Daoyu Wang",
      "Xiaoyu Tao",
      "Qi Liu",
      "Enhong Chen"
    ],
    "abstract": "Time series forecasting (TSF) is a fundamental and widely studied task, spanning methods from classical statistical approaches to modern deep learning and multimodal language modeling. Despite their effectiveness, these methods often follow a fast thinking paradigm emphasizing pattern extraction and direct value mapping, while overlooking explicit reasoning over temporal dynamics and contextual dependencies. Meanwhile, emerging slow-thinking LLMs (e.g., ChatGPT-o1, DeepSeek-R1) have demonstrated impressive multi-step reasoning capabilities across diverse domains, suggesting a new opportunity for reframing TSF as a structured reasoning task. This motivates a key question: can slow-thinking LLMs effectively reason over temporal patterns to support time series forecasting, even in zero-shot manner? To investigate this, in this paper, we propose TimeReasoner, an extensive empirical study that formulates TSF as a conditional reasoning task. We design a series of prompting strategies to elicit inference-time reasoning from pretrained slow-thinking LLMs and evaluate their performance across diverse TSF benchmarks. Our findings reveal that slow-thinking LLMs exhibit non-trivial zero-shot forecasting capabilities, especially in capturing high-level trends and contextual shifts. While preliminary, our study surfaces important insights into the reasoning behaviors of LLMs in temporal domains highlighting both their potential and limitations. We hope this work catalyzes further research into reasoning-based forecasting paradigms and paves the way toward more interpretable and generalizable TSF frameworks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24511v4",
    "published_date": "2025-05-30 12:19:02 UTC",
    "updated_date": "2025-12-21 10:41:36 UTC"
  },
  {
    "arxiv_id": "2506.03185v1",
    "title": "DLiPath: A Benchmark for the Comprehensive Assessment of Donor Liver Based on Histopathological Image Dataset",
    "authors": [
      "Liangrui Pan",
      "Xingchen Li",
      "Zhongyi Chen",
      "Ling Chu",
      "Shaoliang Peng"
    ],
    "abstract": "Pathologists comprehensive evaluation of donor liver biopsies provides crucial information for accepting or discarding potential grafts. However, rapidly and accurately obtaining these assessments intraoperatively poses a significant challenge for pathologists. Features in donor liver biopsies, such as portal tract fibrosis, total steatosis, macrovesicular steatosis, and hepatocellular ballooning are correlated with transplant outcomes, yet quantifying these indicators suffers from substantial inter- and intra-observer variability. To address this, we introduce DLiPath, the first benchmark for comprehensive donor liver assessment based on a histopathology image dataset. We collected and publicly released 636 whole slide images from 304 donor liver patients at the Department of Pathology, the Third Xiangya Hospital, with expert annotations for key pathological features (including cholestasis, portal tract fibrosis, portal inflammation, total steatosis, macrovesicular steatosis, and hepatocellular ballooning). We selected nine state-of-the-art multiple-instance learning (MIL) models based on the DLiPath dataset as baselines for extensive comparative analysis. The experimental results demonstrate that several MIL models achieve high accuracy across donor liver assessment indicators on DLiPath, charting a clear course for future automated and intelligent donor liver assessment research. Data and code are available at https://github.com/panliangrui/ACM_MM_2025.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "q-bio.QM"
    ],
    "primary_category": "eess.IV",
    "comment": "Submit to ACM MM2025",
    "pdf_url": "https://arxiv.org/pdf/2506.03185v1",
    "published_date": "2025-05-30 12:13:00 UTC",
    "updated_date": "2025-05-30 12:13:00 UTC"
  },
  {
    "arxiv_id": "2506.12055v1",
    "title": "Towards Unified Neural Decoding with Brain Functional Network Modeling",
    "authors": [
      "Di Wu",
      "Linghao Bu",
      "Yifei Jia",
      "Lu Cao",
      "Siyuan Li",
      "Siyu Chen",
      "Yueqian Zhou",
      "Sheng Fan",
      "Wenjie Ren",
      "Dengchang Wu",
      "Kang Wang",
      "Yue Zhang",
      "Yuehui Ma",
      "Jie Yang",
      "Mohamad Sawan"
    ],
    "abstract": "Recent achievements in implantable brain-computer interfaces (iBCIs) have demonstrated the potential to decode cognitive and motor behaviors with intracranial brain recordings; however, individual physiological and electrode implantation heterogeneities have constrained current approaches to neural decoding within single individuals, rendering interindividual neural decoding elusive. Here, we present Multi-individual Brain Region-Aggregated Network (MIBRAIN), a neural decoding framework that constructs a whole functional brain network model by integrating intracranial neurophysiological recordings across multiple individuals. MIBRAIN leverages self-supervised learning to derive generalized neural prototypes and supports group-level analysis of brain-region interactions and inter-subject neural synchrony. To validate our framework, we recorded stereoelectroencephalography (sEEG) signals from a cohort of individuals performing Mandarin syllable articulation. Both real-time online and offline decoding experiments demonstrated significant improvements in both audible and silent articulation decoding, enhanced decoding accuracy with increased multi-subject data integration, and effective generalization to unseen subjects. Furthermore, neural predictions for regions without direct electrode coverage were validated against authentic neural data. Overall, this framework paves the way for robust neural decoding across individuals and offers insights for practical clinical applications.",
    "categories": [
      "q-bio.NC",
      "cs.AI"
    ],
    "primary_category": "q-bio.NC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.12055v1",
    "published_date": "2025-05-30 12:10:37 UTC",
    "updated_date": "2025-05-30 12:10:37 UTC"
  },
  {
    "arxiv_id": "2505.24503v1",
    "title": "Online Fair Division with Additional Information",
    "authors": [
      "Tzeh Yuan Neoh",
      "Jannik Peters",
      "Nicholas Teh"
    ],
    "abstract": "We study the problem of fairly allocating indivisible goods to agents in an online setting, where goods arrive sequentially and must be allocated irrevocably to agents. Focusing on the popular fairness notions of envy-freeness, proportionality, and maximin share fairness (and their approximate variants), we ask how the availability of information on future goods influences the existence and approximability of fair allocations. In the absence of any such information, we establish strong impossibility results, demonstrating the inherent difficulty of achieving even approximate fairness guarantees. In contrast, we demonstrate that knowledge of additional information -- such as aggregate of each agent's total valuations (equivalently, normalized valuations) or the multiset of future goods values (frequency predictions) -- would enable the design of fairer online algorithms. Given normalization information, we propose an algorithm that achieves stronger fairness guarantees than previously known results. Given frequency predictions, we introduce a meta-algorithm that leverages frequency predictions to match the best-known offline guarantees for a broad class of ''share-based'' fairness notions. Our complementary impossibility results in each setting underscore both the limitations imposed by uncertainty about future goods and the potential of leveraging structured information to achieve fairer outcomes in online fair division.",
    "categories": [
      "cs.GT",
      "cs.AI"
    ],
    "primary_category": "cs.GT",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24503v1",
    "published_date": "2025-05-30 12:06:16 UTC",
    "updated_date": "2025-05-30 12:06:16 UTC"
  },
  {
    "arxiv_id": "2505.24500v1",
    "title": "TimeHC-RL: Temporal-aware Hierarchical Cognitive Reinforcement Learning for Enhancing LLMs' Social Intelligence",
    "authors": [
      "Guiyang Hou",
      "Xing Gao",
      "Yuchuan Wu",
      "Xiang Huang",
      "Wenqi Zhang",
      "Zhe Zheng",
      "Yongliang Shen",
      "Jialu Du",
      "Fei Huang",
      "Yongbin Li",
      "Weiming Lu"
    ],
    "abstract": "Recently, Large Language Models (LLMs) have made significant progress in IQ-related domains that require careful thinking, such as mathematics and coding. However, enhancing LLMs' cognitive development in social domains, particularly from a post-training perspective, remains underexplored. Recognizing that the social world follows a distinct timeline and requires a richer blend of cognitive modes (from intuitive reactions (System 1) and surface-level thinking to deliberate thinking (System 2)) than mathematics, which primarily relies on System 2 cognition (careful, step-by-step reasoning), we introduce Temporal-aware Hierarchical Cognitive Reinforcement Learning (TimeHC-RL) for enhancing LLMs' social intelligence. In our experiments, we systematically explore improving LLMs' social intelligence and validate the effectiveness of the TimeHC-RL method, through five other post-training paradigms and two test-time intervention paradigms on eight datasets with diverse data patterns. Experimental results reveal the superiority of our proposed TimeHC-RL method compared to the widely adopted System 2 RL method. It gives the 7B backbone model wings, enabling it to rival the performance of advanced models like DeepSeek-R1 and OpenAI-O3. Additionally, the systematic exploration from post-training and test-time interventions perspectives to improve LLMs' social intelligence has uncovered several valuable insights.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "22 pages, 12 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.24500v1",
    "published_date": "2025-05-30 12:01:06 UTC",
    "updated_date": "2025-05-30 12:01:06 UTC"
  },
  {
    "arxiv_id": "2506.00096v3",
    "title": "PathGene: Benchmarking Driver Gene Mutations and Exon Prediction Using Multicenter Lung Cancer Histopathology Image Dataset",
    "authors": [
      "Liangrui Pan",
      "Qingchun Liang",
      "Shen Zhao",
      "Songqing Fan",
      "Shaoliang Peng"
    ],
    "abstract": "Accurately predicting gene mutations, mutation subtypes and their exons in lung cancer is critical for personalized treatment planning and prognostic assessment. Faced with regional disparities in medical resources and the high cost of genomic assays, using artificial intelligence to infer these mutations and exon variants from routine histopathology images could greatly facilitate precision therapy. Although some prior studies have shown that deep learning can accelerate the prediction of key gene mutations from lung cancer pathology slides, their performance remains suboptimal and has so far been limited mainly to early screening tasks. To address these limitations, we have assembled PathGene, which comprises histopathology images paired with next-generation sequencing reports from 1,576 patients at the Second Xiangya Hospital, Central South University, and 448 TCGA-LUAD patients. This multi-center dataset links whole-slide images to driver gene mutation status, mutation subtypes, exon, and tumor mutational burden (TMB) status, with the goal of leveraging pathology images to predict mutations, subtypes, exon locations, and TMB for early genetic screening and to advance precision oncology. Unlike existing datasets, we provide molecular-level information related to histopathology images in PathGene to facilitate the development of biomarker prediction models. We benchmarked 11 multiple-instance learning methods on PathGene for mutation, subtype, exon, and TMB prediction tasks. These experimental methods provide valuable alternatives for early genetic screening of lung cancer patients and assisting clinicians to quickly develop personalized precision targeted treatment plans for patients. Code and data are available at https://github.com/panliangrui/NIPS2025/.",
    "categories": [
      "q-bio.GN",
      "cs.AI"
    ],
    "primary_category": "q-bio.GN",
    "comment": "Withdrawn due to issues related to data permissions/ethics",
    "pdf_url": "https://arxiv.org/pdf/2506.00096v3",
    "published_date": "2025-05-30 11:51:11 UTC",
    "updated_date": "2025-11-27 01:52:51 UTC"
  },
  {
    "arxiv_id": "2505.24493v1",
    "title": "MELT: Towards Automated Multimodal Emotion Data Annotation by Leveraging LLM Embedded Knowledge",
    "authors": [
      "Xin Jing",
      "Jiadong Wang",
      "Iosif Tsangko",
      "Andreas Triantafyllopoulos",
      "Björn W. Schuller"
    ],
    "abstract": "Although speech emotion recognition (SER) has advanced significantly with deep learning, annotation remains a major hurdle. Human annotation is not only costly but also subject to inconsistencies annotators often have different preferences and may lack the necessary contextual knowledge, which can lead to varied and inaccurate labels. Meanwhile, Large Language Models (LLMs) have emerged as a scalable alternative for annotating text data. However, the potential of LLMs to perform emotional speech data annotation without human supervision has yet to be thoroughly investigated. To address these problems, we apply GPT-4o to annotate a multimodal dataset collected from the sitcom Friends, using only textual cues as inputs. By crafting structured text prompts, our methodology capitalizes on the knowledge GPT-4o has accumulated during its training, showcasing that it can generate accurate and contextually relevant annotations without direct access to multimodal inputs. Therefore, we propose MELT, a multimodal emotion dataset fully annotated by GPT-4o. We demonstrate the effectiveness of MELT by fine-tuning four self-supervised learning (SSL) backbones and assessing speech emotion recognition performance across emotion datasets. Additionally, our subjective experiments\\' results demonstrate a consistence performance improvement on SER.",
    "categories": [
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24493v1",
    "published_date": "2025-05-30 11:45:36 UTC",
    "updated_date": "2025-05-30 11:45:36 UTC"
  },
  {
    "arxiv_id": "2505.24492v4",
    "title": "Object Centric Concept Bottlenecks",
    "authors": [
      "David Steinmann",
      "Wolfgang Stammer",
      "Antonia Wüst",
      "Kristian Kersting"
    ],
    "abstract": "Developing high-performing, yet interpretable models remains a critical challenge in modern AI. Concept-based models (CBMs) attempt to address this by extracting human-understandable concepts from a global encoding (e.g., image encoding) and then applying a linear classifier on the resulting concept activations, enabling transparent decision-making. However, their reliance on holistic image encodings limits their expressiveness in object-centric real-world settings and thus hinders their ability to solve complex vision tasks beyond single-label classification. To tackle these challenges, we introduce Object-Centric Concept Bottlenecks (OCB), a framework that combines the strengths of CBMs and pre-trained object-centric foundation models, boosting performance and interpretability. We evaluate OCB on complex image datasets and conduct a comprehensive ablation study to analyze key components of the framework, such as strategies for aggregating object-concept encodings. The results show that OCB outperforms traditional CBMs and allows one to make interpretable decisions for complex visual tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24492v4",
    "published_date": "2025-05-30 11:45:05 UTC",
    "updated_date": "2025-10-07 07:47:26 UTC"
  },
  {
    "arxiv_id": "2505.24489v1",
    "title": "Deformable Attention Mechanisms Applied to Object Detection, case of Remote Sensing",
    "authors": [
      "Anasse Boutayeb",
      "Iyad Lahsen-cherif",
      "Ahmed El Khadimi"
    ],
    "abstract": "Object detection has recently seen an interesting trend in terms of the most innovative research work, this task being of particular importance in the field of remote sensing, given the consistency of these images in terms of geographical coverage and the objects present. Furthermore, Deep Learning (DL) models, in particular those based on Transformers, are especially relevant for visual computing tasks in general, and target detection in particular. Thus, the present work proposes an application of Deformable-DETR model, a specific architecture using deformable attention mechanisms, on remote sensing images in two different modes, especially optical and Synthetic Aperture Radar (SAR). To achieve this objective, two datasets are used, one optical, which is Pleiades Aircraft dataset, and the other SAR, in particular SAR Ship Detection Dataset (SSDD). The results of a 10-fold stratified validation showed that the proposed model performed particularly well, obtaining an F1 score of 95.12% for the optical dataset and 94.54% for SSDD, while comparing these results with several models detections, especially those based on CNNs and transformers, as well as those specifically designed to detect different object classes in remote sensing images.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 5 figures, paper accepted at the 29th International Conference on Knowledge-Based and Intelligent Information and Engineering Systems (KES 2025), Osaka, Japan",
    "pdf_url": "https://arxiv.org/pdf/2505.24489v1",
    "published_date": "2025-05-30 11:43:09 UTC",
    "updated_date": "2025-05-30 11:43:09 UTC"
  },
  {
    "arxiv_id": "2505.24486v1",
    "title": "Rehearsal with Auxiliary-Informed Sampling for Audio Deepfake Detection",
    "authors": [
      "Falih Gozi Febrinanto",
      "Kristen Moore",
      "Chandra Thapa",
      "Jiangang Ma",
      "Vidya Saikrishna",
      "Feng Xia"
    ],
    "abstract": "The performance of existing audio deepfake detection frameworks degrades when confronted with new deepfake attacks. Rehearsal-based continual learning (CL), which updates models using a limited set of old data samples, helps preserve prior knowledge while incorporating new information. However, existing rehearsal techniques don't effectively capture the diversity of audio characteristics, introducing bias and increasing the risk of forgetting. To address this challenge, we propose Rehearsal with Auxiliary-Informed Sampling (RAIS), a rehearsal-based CL approach for audio deepfake detection. RAIS employs a label generation network to produce auxiliary labels, guiding diverse sample selection for the memory buffer. Extensive experiments show RAIS outperforms state-of-the-art methods, achieving an average Equal Error Rate (EER) of 1.953 % across five experiences. The code is available at: https://github.com/falihgoz/RAIS.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CR",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted by Interspeech 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.24486v1",
    "published_date": "2025-05-30 11:40:50 UTC",
    "updated_date": "2025-05-30 11:40:50 UTC"
  },
  {
    "arxiv_id": "2506.00095v3",
    "title": "ClinBench-HPB: A Clinical Benchmark for Evaluating LLMs in Hepato-Pancreato-Biliary Diseases",
    "authors": [
      "Yuchong Li",
      "Xiaojun Zeng",
      "Chihua Fang",
      "Jian Yang",
      "Fucang Jia",
      "Lei Zhang"
    ],
    "abstract": "Hepato-pancreato-biliary (HPB) disorders represent a global public health challenge due to their high morbidity and mortality. Although large language models (LLMs) have shown promising performance in general medical question-answering tasks, the current evaluation benchmarks are mostly derived from standardized examinations or manually designed questions, lacking HPB coverage and clinical cases. To address these issues, we systematically eatablish an HPB disease evaluation benchmark comprising 3,535 closed-ended multiple-choice questions and 337 open-ended real diagnosis cases, which encompasses all the 33 main categories and 465 subcategories of HPB diseases defined in the International Statistical Classification of Diseases, 10th Revision (ICD-10). The multiple-choice questions are curated from public datasets and synthesized data, and the clinical cases are collected from prestigious medical journals, case-sharing platforms, and collaborating hospitals. By evalauting commercial and open-source general and medical LLMs on our established benchmark, namely ClinBench-HBP, we find that while commercial LLMs perform competently on medical exam questions, they exhibit substantial performance degradation on HPB diagnosis tasks, especially on complex, inpatient clinical cases. Those medical LLMs also show limited generalizability to HPB diseases. Our results reveal the critical limitations of current LLMs in the domain of HPB diseases, underscoring the imperative need for future medical LLMs to handle real, complex clinical diagnostics rather than simple medical exam questions. The benchmark will be released at https://clinbench-hpb.github.io.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.00095v3",
    "published_date": "2025-05-30 11:35:05 UTC",
    "updated_date": "2025-06-04 03:25:49 UTC"
  },
  {
    "arxiv_id": "2505.24480v1",
    "title": "Towards Effective Code-Integrated Reasoning",
    "authors": [
      "Fei Bai",
      "Yingqian Min",
      "Beichen Zhang",
      "Zhipeng Chen",
      "Wayne Xin Zhao",
      "Lei Fang",
      "Zheng Liu",
      "Zhongyuan Wang",
      "Ji-Rong Wen"
    ],
    "abstract": "In this paper, we investigate code-integrated reasoning, where models generate code when necessary and integrate feedback by executing it through a code interpreter. To acquire this capability, models must learn when and how to use external code tools effectively, which is supported by tool-augmented reinforcement learning (RL) through interactive learning. Despite its benefits, tool-augmented RL can still suffer from potential instability in the learning dynamics. In light of this challenge, we present a systematic approach to improving the training effectiveness and stability of tool-augmented RL for code-integrated reasoning. Specifically, we develop enhanced training strategies that balance exploration and stability, progressively building tool-use capabilities while improving reasoning performance. Through extensive experiments on five mainstream mathematical reasoning benchmarks, our model demonstrates significant performance improvements over multiple competitive baselines. Furthermore, we conduct an in-depth analysis of the mechanism and effect of code-integrated reasoning, revealing several key insights, such as the extension of model's capability boundaries and the simultaneous improvement of reasoning efficiency through code integration. All data and code for reproducing this work are available at: https://github.com/RUCAIBox/CIR.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Technical Report on Slow Thinking with LLMs: Code-Integrated Reasoning",
    "pdf_url": "https://arxiv.org/pdf/2505.24480v1",
    "published_date": "2025-05-30 11:30:18 UTC",
    "updated_date": "2025-05-30 11:30:18 UTC"
  },
  {
    "arxiv_id": "2505.24479v1",
    "title": "Leveraging Knowledge Graphs and LLMs for Structured Generation of Misinformation",
    "authors": [
      "Sania Nayab",
      "Marco Simoni",
      "Giulio Rossolini"
    ],
    "abstract": "The rapid spread of misinformation, further amplified by recent advances in generative AI, poses significant threats to society, impacting public opinion, democratic stability, and national security. Understanding and proactively assessing these threats requires exploring methodologies that enable structured and scalable misinformation generation. In this paper, we propose a novel approach that leverages knowledge graphs (KGs) as structured semantic resources to systematically generate fake triplets. By analyzing the structural properties of KGs, such as the distance between entities and their predicates, we identify plausibly false relationships. These triplets are then used to guide large language models (LLMs) in generating misinformation statements with varying degrees of credibility. By utilizing structured semantic relationships, our deterministic approach produces misinformation inherently challenging for humans to detect, drawing exclusively upon publicly available KGs (e.g., WikiGraphs).\n  Additionally, we investigate the effectiveness of LLMs in distinguishing between genuine and artificially generated misinformation. Our analysis highlights significant limitations in current LLM-based detection methods, underscoring the necessity for enhanced detection strategies and a deeper exploration of inherent biases in generative models.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.SI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24479v1",
    "published_date": "2025-05-30 11:29:10 UTC",
    "updated_date": "2025-05-30 11:29:10 UTC"
  },
  {
    "arxiv_id": "2505.24478v1",
    "title": "Optimizing the Interface Between Knowledge Graphs and LLMs for Complex Reasoning",
    "authors": [
      "Vasilije Markovic",
      "Lazar Obradovic",
      "Laszlo Hajdu",
      "Jovan Pavlovic"
    ],
    "abstract": "Integrating Large Language Models (LLMs) with Knowledge Graphs (KGs) results in complex systems with numerous hyperparameters that directly affect performance. While such systems are increasingly common in retrieval-augmented generation, the role of systematic hyperparameter optimization remains underexplored. In this paper, we study this problem in the context of Cognee, a modular framework for end-to-end KG construction and retrieval. Using three multi-hop QA benchmarks (HotPotQA, TwoWikiMultiHop, and MuSiQue) we optimize parameters related to chunking, graph construction, retrieval, and prompting. Each configuration is scored using established metrics (exact match, F1, and DeepEval's LLM-based correctness metric). Our results demonstrate that meaningful gains can be achieved through targeted tuning. While the gains are consistent, they are not uniform, with performance varying across datasets and metrics. This variability highlights both the value of tuning and the limitations of standard evaluation measures. While demonstrating the immediate potential of hyperparameter tuning, we argue that future progress will depend not only on architectural advances but also on clearer frameworks for optimization and evaluation in complex, modular systems.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "This is a preliminary version. A revised and expanded version is in preparation",
    "pdf_url": "https://arxiv.org/pdf/2505.24478v1",
    "published_date": "2025-05-30 11:27:59 UTC",
    "updated_date": "2025-05-30 11:27:59 UTC"
  },
  {
    "arxiv_id": "2505.24477v1",
    "title": "Evaluating Gemini in an arena for learning",
    "authors": [
      "LearnLM Team",
      "Abhinit Modi",
      "Aditya Srikanth Veerubhotla",
      "Aliya Rysbek",
      "Andrea Huber",
      "Ankit Anand",
      "Avishkar Bhoopchand",
      "Brett Wiltshire",
      "Daniel Gillick",
      "Daniel Kasenberg",
      "Eleni Sgouritsa",
      "Gal Elidan",
      "Hengrui Liu",
      "Holger Winnemoeller",
      "Irina Jurenka",
      "James Cohan",
      "Jennifer She",
      "Julia Wilkowski",
      "Kaiz Alarakyia",
      "Kevin R. McKee",
      "Komal Singh",
      "Lisa Wang",
      "Markus Kunesch",
      "Miruna Pîslar",
      "Niv Efron",
      "Parsa Mahmoudieh",
      "Pierre-Alexandre Kamienny",
      "Sara Wiltberger",
      "Shakir Mohamed",
      "Shashank Agarwal",
      "Shubham Milind Phal",
      "Sun Jae Lee",
      "Theofilos Strinopoulos",
      "Wei-Jen Ko",
      "Yael Gold-Zamir",
      "Yael Haramaty",
      "Yannis Assael"
    ],
    "abstract": "Artificial intelligence (AI) is poised to transform education, but the research community lacks a robust, general benchmark to evaluate AI models for learning. To assess state-of-the-art support for educational use cases, we ran an \"arena for learning\" where educators and pedagogy experts conduct blind, head-to-head, multi-turn comparisons of leading AI models. In particular, $N = 189$ educators drew from their experience to role-play realistic learning use cases, interacting with two models sequentially, after which $N = 206$ experts judged which model better supported the user's learning goals. The arena evaluated a slate of state-of-the-art models: Gemini 2.5 Pro, Claude 3.7 Sonnet, GPT-4o, and OpenAI o3. Excluding ties, experts preferred Gemini 2.5 Pro in 73.2% of these match-ups -- ranking it first overall in the arena. Gemini 2.5 Pro also demonstrated markedly higher performance across key principles of good pedagogy. Altogether, these results position Gemini 2.5 Pro as a leading model for learning.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24477v1",
    "published_date": "2025-05-30 11:26:32 UTC",
    "updated_date": "2025-05-30 11:26:32 UTC"
  },
  {
    "arxiv_id": "2505.24473v2",
    "title": "Train One Sparse Autoencoder Across Multiple Sparsity Budgets to Preserve Interpretability and Accuracy",
    "authors": [
      "Nikita Balagansky",
      "Yaroslav Aksenov",
      "Daniil Laptev",
      "Vadim Kurochkin",
      "Gleb Gerasimov",
      "Nikita Koryagin",
      "Daniil Gavrilov"
    ],
    "abstract": "Sparse Autoencoders (SAEs) have proven to be powerful tools for interpreting neural networks by decomposing hidden representations into disentangled, interpretable features via sparsity constraints. However, conventional SAEs are constrained by the fixed sparsity level chosen during training; meeting different sparsity requirements therefore demands separate models and increases the computational footprint during both training and evaluation. We introduce a novel training objective, \\emph{HierarchicalTopK}, which trains a single SAE to optimise reconstructions across multiple sparsity levels simultaneously. Experiments with Gemma-2 2B demonstrate that our approach achieves Pareto-optimal trade-offs between sparsity and explained variance, outperforming traditional SAEs trained at individual sparsity levels. Further analysis shows that HierarchicalTopK preserves high interpretability scores even at higher sparsity. The proposed objective thus closes an important gap between flexibility and interpretability in SAE design.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24473v2",
    "published_date": "2025-05-30 11:20:44 UTC",
    "updated_date": "2025-06-05 11:42:24 UTC"
  },
  {
    "arxiv_id": "2505.24472v2",
    "title": "VietMix: A Naturally-Occurring Parallel Corpus and Augmentation Framework for Vietnamese-English Code-Mixed Machine Translation",
    "authors": [
      "Hieu Tran",
      "Phuong-Anh Nguyen-Le",
      "Huy Nghiem",
      "Quang-Nhan Nguyen",
      "Wei Ai",
      "Marine Carpuat"
    ],
    "abstract": "Machine translation (MT) systems universally degrade when faced with code-mixed text. This problem is more acute for low-resource languages that lack dedicated parallel corpora. This work directly addresses this gap for Vietnamese-English, a language context characterized by challenges including orthographic ambiguity and the frequent omission of diacritics in informal text. We introduce VietMix, the first expert-translated, naturally occurring parallel corpus of Vietnamese-English code-mixed text. We establish VietMix's utility by developing a data augmentation pipeline that leverages iterative fine-tuning and targeted filtering. Experiments show that models augmented with our data outperform strong back-translation baselines by up to +3.5 xCOMET points and improve zero-shot models by up to +11.9 points. Our work delivers a foundational resource for a challenging language pair and provides a validated, transferable framework for building and augmenting corpora in other low-resource settings.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "EACL 2026",
    "pdf_url": "https://arxiv.org/pdf/2505.24472v2",
    "published_date": "2025-05-30 11:18:10 UTC",
    "updated_date": "2026-01-09 07:58:06 UTC"
  },
  {
    "arxiv_id": "2506.03184v1",
    "title": "Impact of Tuning Parameters in Deep Convolutional Neural Network Using a Crack Image Dataset",
    "authors": [
      "Mahe Zabin",
      "Ho-Jin Choi",
      "Md. Monirul Islam",
      "Jia Uddin"
    ],
    "abstract": "The performance of a classifier depends on the tuning of its parame ters. In this paper, we have experimented the impact of various tuning parameters on the performance of a deep convolutional neural network (DCNN). In the ex perimental evaluation, we have considered a DCNN classifier that consists of 2 convolutional layers (CL), 2 pooling layers (PL), 1 dropout, and a dense layer. To observe the impact of pooling, activation function, and optimizer tuning pa rameters, we utilized a crack image dataset having two classes: negative and pos itive. The experimental results demonstrate that with the maxpooling, the DCNN demonstrates its better performance for adam optimizer and tanh activation func tion.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 2 figures, published at Proceedings of the 15th KIPS International Conference on Ubiquitous Information Technologies and Applications (CUTE 2021), Jeju, Repubilc of Korea",
    "pdf_url": "https://arxiv.org/pdf/2506.03184v1",
    "published_date": "2025-05-30 10:58:31 UTC",
    "updated_date": "2025-05-30 10:58:31 UTC"
  },
  {
    "arxiv_id": "2506.15708v1",
    "title": "Refined Causal Graph Structure Learning via Curvature for Brain Disease Classification",
    "authors": [
      "Falih Gozi Febrinanto",
      "Adonia Simango",
      "Chengpei Xu",
      "Jingjing Zhou",
      "Jiangang Ma",
      "Sonika Tyagi",
      "Feng Xia"
    ],
    "abstract": "Graph neural networks (GNNs) have been developed to model the relationship between regions of interest (ROIs) in brains and have shown significant improvement in detecting brain diseases. However, most of these frameworks do not consider the intrinsic relationship of causality factor between brain ROIs, which is arguably more essential to observe cause and effect interaction between signals rather than typical correlation values. We propose a novel framework called CGB (Causal Graphs for Brains) for brain disease classification/detection, which models refined brain networks based on the causal discovery method, transfer entropy, and geometric curvature strategy. CGB unveils causal relationships between ROIs that bring vital information to enhance brain disease classification performance. Furthermore, CGB also performs a graph rewiring through a geometric curvature strategy to refine the generated causal graph to become more expressive and reduce potential information bottlenecks when GNNs model it. Our extensive experiments show that CGB outperforms state-of-the-art methods in classification tasks on brain disease datasets, as measured by average F1 scores.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.15708v1",
    "published_date": "2025-05-30 10:50:45 UTC",
    "updated_date": "2025-05-30 10:50:45 UTC"
  },
  {
    "arxiv_id": "2505.24458v1",
    "title": "SEAR: A Multimodal Dataset for Analyzing AR-LLM-Driven Social Engineering Behaviors",
    "authors": [
      "Tianlong Yu",
      "Chenghang Ye",
      "Zheyu Yang",
      "Ziyi Zhou",
      "Cui Tang",
      "Zui Tao",
      "Jun Zhang",
      "Kailong Wang",
      "Liting Zhou",
      "Yang Yang",
      "Ting Bi"
    ],
    "abstract": "The SEAR Dataset is a novel multimodal resource designed to study the emerging threat of social engineering (SE) attacks orchestrated through augmented reality (AR) and multimodal large language models (LLMs). This dataset captures 180 annotated conversations across 60 participants in simulated adversarial scenarios, including meetings, classes and networking events. It comprises synchronized AR-captured visual/audio cues (e.g., facial expressions, vocal tones), environmental context, and curated social media profiles, alongside subjective metrics such as trust ratings and susceptibility assessments. Key findings reveal SEAR's alarming efficacy in eliciting compliance (e.g., 93.3% phishing link clicks, 85% call acceptance) and hijacking trust (76.7% post-interaction trust surge). The dataset supports research in detecting AR-driven SE attacks, designing defensive frameworks, and understanding multimodal adversarial manipulation. Rigorous ethical safeguards, including anonymization and IRB compliance, ensure responsible use. The SEAR dataset is available at https://github.com/INSLabCN/SEAR-Dataset.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24458v1",
    "published_date": "2025-05-30 10:46:13 UTC",
    "updated_date": "2025-05-30 10:46:13 UTC"
  },
  {
    "arxiv_id": "2505.24451v1",
    "title": "LPASS: Linear Probes as Stepping Stones for vulnerability detection using compressed LLMs",
    "authors": [
      "Luis Ibanez-Lissen",
      "Lorena Gonzalez-Manzano",
      "Jose Maria de Fuentes",
      "Nicolas Anciaux"
    ],
    "abstract": "Large Language Models (LLMs) are being extensively used for cybersecurity purposes. One of them is the detection of vulnerable codes. For the sake of efficiency and effectiveness, compression and fine-tuning techniques are being developed, respectively. However, they involve spending substantial computational efforts. In this vein, we analyse how Linear Probes (LPs) can be used to provide an estimation on the performance of a compressed LLM at an early phase -- before fine-tuning. We also show their suitability to set the cut-off point when applying layer pruning compression. Our approach, dubbed $LPASS$, is applied in BERT and Gemma for the detection of 12 of MITRE's Top 25 most dangerous vulnerabilities on 480k C/C++ samples. LPs can be computed in 142.97 s. and provide key findings: (1) 33.3 \\% and 72.2\\% of layers can be removed, respectively, with no precision loss; (2) they provide an early estimate of the post-fine-tuning and post-compression model effectiveness, with 3\\% and 8.68\\% as the lowest and average precision errors, respectively. $LPASS$-based LLMs outperform the state of the art, reaching 86.9\\% of accuracy in multi-class vulnerability detection. Interestingly, $LPASS$-based compressed versions of Gemma outperform the original ones by 1.6\\% of F1-score at a maximum while saving 29.4 \\% and 23.8\\% of training and inference time and 42.98\\% of model size.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24451v1",
    "published_date": "2025-05-30 10:37:14 UTC",
    "updated_date": "2025-05-30 10:37:14 UTC"
  },
  {
    "arxiv_id": "2506.00094v1",
    "title": "Feeling Guilty Being a c(ai)borg: Navigating the Tensions Between Guilt and Empowerment in AI Use",
    "authors": [
      "Konstantin Aal",
      "Tanja Aal",
      "Vasil Navumau",
      "David Unbehaun",
      "Claudia Müller",
      "Volker Wulf",
      "Sarah Rüller"
    ],
    "abstract": "This paper explores the emotional, ethical and practical dimensions of integrating Artificial Intelligence (AI) into personal and professional workflows, focusing on the concept of feeling guilty as a 'c(ai)borg' - a human augmented by AI. Inspired by Donna Haraway's Cyborg Manifesto, the study explores how AI challenges traditional notions of creativity, originality and intellectual labour. Using an autoethnographic approach, the authors reflect on their year-long experiences with AI tools, revealing a transition from initial guilt and reluctance to empowerment through skill-building and transparency. Key findings highlight the importance of basic academic skills, advanced AI literacy and honest engagement with AI results. The c(ai)borg vision advocates for a future where AI is openly embraced as a collaborative partner, fostering innovation and equity while addressing issues of access and agency. By reframing guilt as growth, the paper calls for a thoughtful and inclusive approach to AI integration.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "16 pages,",
    "pdf_url": "https://arxiv.org/pdf/2506.00094v1",
    "published_date": "2025-05-30 10:33:04 UTC",
    "updated_date": "2025-05-30 10:33:04 UTC"
  },
  {
    "arxiv_id": "2505.24445v1",
    "title": "Learning Safety Constraints for Large Language Models",
    "authors": [
      "Xin Chen",
      "Yarden As",
      "Andreas Krause"
    ],
    "abstract": "Large language models (LLMs) have emerged as powerful tools but pose significant safety risks through harmful outputs and vulnerability to adversarial attacks. We propose SaP, short for Safety Polytope, a geometric approach to LLM safety that learns and enforces multiple safety constraints directly in the model's representation space. We develop a framework that identifies safe and unsafe regions via the polytope's facets, enabling both detection and correction of unsafe outputs through geometric steering. Unlike existing approaches that modify model weights, SaP operates post-hoc in the representation space, preserving model capabilities while enforcing safety constraints. Experiments across multiple LLMs demonstrate that our method can effectively detect unethical inputs, reduce adversarial attack success rates while maintaining performance on standard tasks, thus highlighting the importance of having an explicit geometric model for safety. Analysis of the learned polytope facets reveals emergence of specialization in detecting different semantic notions of safety, providing interpretable insights into how safety is captured in LLMs' representation space.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2025 (Spotlight)",
    "pdf_url": "https://arxiv.org/pdf/2505.24445v1",
    "published_date": "2025-05-30 10:30:24 UTC",
    "updated_date": "2025-05-30 10:30:24 UTC"
  },
  {
    "arxiv_id": "2505.24442v1",
    "title": "RMoA: Optimizing Mixture-of-Agents through Diversity Maximization and Residual Compensation",
    "authors": [
      "Zhentao Xie",
      "Chengcheng Han",
      "Jinxin Shi",
      "Wenjun Cui",
      "Xin Zhao",
      "Xingjiao Wu",
      "Jiabao Zhao"
    ],
    "abstract": "Although multi-agent systems based on large language models show strong capabilities on multiple tasks, they are still limited by high computational overhead, information loss, and robustness. Inspired by ResNet's residual learning, we propose Residual Mixture-of-Agents (RMoA), integrating residual connections to optimize efficiency and reliability. To maximize information utilization from model responses while minimizing computational costs, we innovatively design an embedding-based diversity selection mechanism that greedily selects responses via vector similarity. Furthermore, to mitigate iterative information degradation, we introduce a Residual Extraction Agent to preserve cross-layer incremental information by capturing inter-layer response differences, coupled with a Residual Aggregation Agent for hierarchical information integration. Additionally, we propose an adaptive termination mechanism that dynamically halts processing based on residual convergence, further improving inference efficiency. RMoA achieves state-of-the-art performance on the benchmarks of across alignment, mathematical reasoning, code generation, and multitasking understanding, while significantly reducing computational overhead. Code is available at https://github.com/mindhunter01/RMoA.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by ACL 2025 (Findings)",
    "pdf_url": "https://arxiv.org/pdf/2505.24442v1",
    "published_date": "2025-05-30 10:23:11 UTC",
    "updated_date": "2025-05-30 10:23:11 UTC"
  },
  {
    "arxiv_id": "2505.24429v2",
    "title": "Deep Learning Weather Models for Subregional Ocean Forecasting: A Case Study on the Canary Current Upwelling System",
    "authors": [
      "Giovanny A. Cuervo-Londoño",
      "Javier Sánchez",
      "Ángel Rodríguez-Santana"
    ],
    "abstract": "Oceanographic forecasting impacts various sectors of society by supporting environmental conservation and economic activities. Based on global circulation models, traditional forecasting methods are computationally expensive and slow, limiting their ability to provide rapid forecasts. Recent advances in deep learning offer faster and more accurate predictions, although these data-driven models are often trained with global data from numerical simulations, which may not reflect reality. The emergence of such models presents great potential for improving ocean prediction at a subregional domain. However, their ability to predict fine-scale ocean processes, like mesoscale structures, remains largely unknown. This work aims to adapt a graph neural network initially developed for global weather forecasting to improve subregional ocean prediction, specifically focusing on the Canary Current upwelling system. The model is trained with satellite data and compared to state-of-the-art physical ocean models to assess its performance in capturing ocean dynamics. Our results show that the deep learning model surpasses traditional methods in precision despite some challenges in upwelling areas. It demonstrated superior performance in reducing RMSE errors compared to ConvLSTM and the GLORYS reanalysis, particularly in regions with complex oceanic dynamics such as Cape Ghir, Cape Bojador, and Cape Blanc. The model achieved improvements of up to 26.5% relative to ConvLSTM and error reductions of up to 76% in 5-day forecasts compared to the GLORYS reanalysis at these critical locations, highlighting its enhanced capability to capture spatial variability and improve predictive accuracy in complex areas. These findings suggest the viability of adapting meteorological data-driven models for improving subregional medium-term ocean forecasting.",
    "categories": [
      "physics.ao-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "physics.ao-ph",
    "comment": "28 pages, 8 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.24429v2",
    "published_date": "2025-05-30 10:10:40 UTC",
    "updated_date": "2025-06-06 10:05:06 UTC"
  },
  {
    "arxiv_id": "2505.24426v1",
    "title": "P: A Universal Measure of Predictive Intelligence",
    "authors": [
      "David Gamez"
    ],
    "abstract": "Over the last thirty years, considerable progress has been made with the development of systems that can drive cars, play games, predict protein folding and generate natural language. These systems are described as intelligent and there has been a great deal of talk about the rapid increase in artificial intelligence and its potential dangers. However, our theoretical understanding of intelligence and ability to measure it lag far behind our capacity for building systems that mimic intelligent human behaviour. There is no commonly agreed definition of the intelligence that AI systems are said to possess. No-one has developed a practical measure that would enable us to compare the intelligence of humans, animals and AIs on a single ratio scale.\n  This paper sets out a new universal measure of intelligence that is based on the hypothesis that prediction is the most important component of intelligence. As an agent interacts with its normal environment, the accuracy of its predictions is summed up and the complexity of its predictions and perceived environment is accounted for using Kolmogorov complexity. Two experiments were carried out to evaluate the practical feasibility of the algorithm. These demonstrated that it could measure the intelligence of an agent embodied in a virtual maze and an agent that makes predictions about time-series data. This universal measure could be the starting point for a new comparative science of intelligence that ranks humans, animals and AIs on a single ratio scale.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24426v1",
    "published_date": "2025-05-30 10:05:54 UTC",
    "updated_date": "2025-05-30 10:05:54 UTC"
  },
  {
    "arxiv_id": "2505.24422v1",
    "title": "Three Kinds of Negation in Knowledge and Their Mathematical Foundations",
    "authors": [
      "Zhenghua Pan",
      "Yong Wang"
    ],
    "abstract": "In the field of artificial intelligence, understanding, distinguishing, expressing, and computing the negation in knowledge is a fundamental issue in knowledge processing and research. In this paper, we examine and analyze the understanding and characteristics of negation in various fields such as philosophy, logic, and linguistics etc. Based on the distinction between the concepts of contradiction and opposition, we propose that there are three different types of negation in knowledge from a conceptual perspective: contradictory negation, opposite negation, and intermediary negation. To establish a mathematical foundation that fully reflects the intrinsic connections, properties, and laws of these different forms of negation, we introduce SCOI: sets with contradictory negation, opposite negation and intermediary negation, and LCOI: logic with contradictory negation, opposite negation and intermediary negation, and we proved the main operational properties of SCOI as well as the formal inference relations in LCOI.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "32 pages,13 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.24422v1",
    "published_date": "2025-05-30 10:01:37 UTC",
    "updated_date": "2025-05-30 10:01:37 UTC"
  },
  {
    "arxiv_id": "2505.24415v1",
    "title": "Boosting Automatic Exercise Evaluation Through Musculoskeletal Simulation-Based IMU Data Augmentation",
    "authors": [
      "Andreas Spilz",
      "Heiko Oppel",
      "Michael Munz"
    ],
    "abstract": "Automated evaluation of movement quality holds significant potential for enhancing physiotherapeutic treatments and sports training by providing objective, real-time feedback. However, the effectiveness of deep learning models in assessing movements captured by inertial measurement units (IMUs) is often hampered by limited data availability, class imbalance, and label ambiguity. In this work, we present a novel data augmentation method that generates realistic IMU data using musculoskeletal simulations integrated with systematic modifications of movement trajectories. Crucially, our approach ensures biomechanical plausibility and allows for automatic, reliable labeling by combining inverse kinematic parameters with a knowledge-based evaluation strategy. Extensive evaluations demonstrate that augmented variants closely resembles real-world data, significantly improving the classification accuracy and generalization capability of neural network models. Additionally, we highlight the benefits of augmented data for patient-specific fine-tuning scenarios, particularly when only limited subject-specific training examples are available. Our findings underline the practicality and efficacy of this augmentation method in overcoming common challenges faced by deep learning applications in physiotherapeutic exercise evaluation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24415v1",
    "published_date": "2025-05-30 09:53:37 UTC",
    "updated_date": "2025-05-30 09:53:37 UTC"
  },
  {
    "arxiv_id": "2505.24409v2",
    "title": "When Language Shapes Thought: Cross-Lingual Transfer of Factual Knowledge in Question Answering",
    "authors": [
      "Eojin Kang",
      "Juae Kim"
    ],
    "abstract": "Multilingual large language models (LLMs) offer promising opportunities for cross-lingual information access, yet their use of factual knowledge remains highly sensitive to the input language. Prior work has addressed this through English prompting and evaluation, assuming that English-based reasoning is universally beneficial. In this work, we challenge that assumption by exploring factual knowledge transfer from non-English to English through the lens of Language and Thought Theory. We introduce Language-to-Thought (L2T) prompting, which aligns the model's internal ''thinking'' language with the source of knowledge. Across three languages and four models, L2T consistently outperforms English-based reasoning, reversing the expected advantage of English prompts. Our code is available at https://github.com/GeomeunByeol/Language2Thought.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at CIKM2025 (Expanded version)",
    "pdf_url": "https://arxiv.org/pdf/2505.24409v2",
    "published_date": "2025-05-30 09:47:25 UTC",
    "updated_date": "2025-11-10 14:55:15 UTC"
  },
  {
    "arxiv_id": "2505.24380v2",
    "title": "SASP: Strip-Aware Spatial Perception for Fine-Grained Bird Image Classification",
    "authors": [
      "Zheng Wang"
    ],
    "abstract": "Fine-grained bird image classification (FBIC) is not only of great significance for ecological monitoring and species identification, but also holds broad research value in the fields of image recognition and fine-grained visual modeling. Compared with general image classification tasks, FBIC poses more formidable challenges: 1) the differences in species size and imaging distance result in the varying sizes of birds presented in the images; 2) complex natural habitats often introduce strong background interference; 3) and highly flexible poses such as flying, perching, or foraging result in substantial intra-class variability. These factors collectively make it difficult for traditional methods to stably extract discriminative features, thereby limiting the generalizability and interpretability of models in real-world applications. To address these challenges, this paper proposes a fine-grained bird classification framework based on strip-aware spatial perception, which aims to capture long-range spatial dependencies across entire rows or columns in bird images, thereby enhancing the model's robustness and interpretability. The proposed method incorporates two novel modules: extensional perception aggregator (EPA) and channel semantic weaving (CSW). Specifically, EPA integrates local texture details with global structural cues by aggregating information across horizontal and vertical spatial directions. CSW further refines the semantic representations by adaptively fusing long-range and short-range information along the channel dimension. Built upon a ResNet-50 backbone, the model enables jump-wise connection of extended structural features across the spatial domain. Experimental results on the CUB-200-2011 dataset demonstrate that our framework achieves significant performance improvements while maintaining architectural efficiency.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24380v2",
    "published_date": "2025-05-30 09:10:12 UTC",
    "updated_date": "2025-06-03 16:45:51 UTC"
  },
  {
    "arxiv_id": "2505.24379v3",
    "title": "Unlearned but Not Forgotten: Data Extraction after Exact Unlearning in LLM",
    "authors": [
      "Xiaoyu Wu",
      "Yifei Pang",
      "Terrance Liu",
      "Zhiwei Steven Wu"
    ],
    "abstract": "Large Language Models are typically trained on datasets collected from the web, which may inadvertently contain harmful or sensitive personal information. To address growing privacy concerns, unlearning methods have been proposed to remove the influence of specific data from trained models. Of these, exact unlearning -- which retrains the model from scratch without the target data -- is widely regarded the gold standard for mitigating privacy risks in deployment. In this paper, we revisit this assumption in a practical deployment setting where both the pre- and post-unlearning logits API are exposed, such as in open-weight scenarios. Targeting this setting, we introduce a novel data extraction attack that leverages signals from the pre-unlearning model to guide the post-unlearning model, uncovering patterns that reflect the removed data distribution. Combining model guidance with a token filtering strategy, our attack significantly improves extraction success rates -- doubling performance in some cases -- across common benchmarks such as MUSE, TOFU, and WMDP. Furthermore, we demonstrate our attack's effectiveness on a simulated medical diagnosis dataset to highlight real-world privacy risks associated with exact unlearning. In light of our findings, which suggest that unlearning may, in a contradictory way, increase the risk of privacy leakage during real-world deployments, we advocate for evaluation of unlearning methods to consider broader threat models that account not only for post-unlearning models but also for adversarial access to prior checkpoints. Code is publicly available at: https://github.com/Nicholas0228/unlearned_data_extraction_llm.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by Neurips 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.24379v3",
    "published_date": "2025-05-30 09:09:33 UTC",
    "updated_date": "2025-10-22 17:51:21 UTC"
  },
  {
    "arxiv_id": "2505.24378v1",
    "title": "Mastering Massive Multi-Task Reinforcement Learning via Mixture-of-Expert Decision Transformer",
    "authors": [
      "Yilun Kong",
      "Guozheng Ma",
      "Qi Zhao",
      "Haoyu Wang",
      "Li Shen",
      "Xueqian Wang",
      "Dacheng Tao"
    ],
    "abstract": "Despite recent advancements in offline multi-task reinforcement learning (MTRL) have harnessed the powerful capabilities of the Transformer architecture, most approaches focus on a limited number of tasks, with scaling to extremely massive tasks remaining a formidable challenge. In this paper, we first revisit the key impact of task numbers on current MTRL method, and further reveal that naively expanding the parameters proves insufficient to counteract the performance degradation as the number of tasks escalates. Building upon these insights, we propose M3DT, a novel mixture-of-experts (MoE) framework that tackles task scalability by further unlocking the model's parameter scalability. Specifically, we enhance both the architecture and the optimization of the agent, where we strengthen the Decision Transformer (DT) backbone with MoE to reduce task load on parameter subsets, and introduce a three-stage training mechanism to facilitate efficient training with optimal performance. Experimental results show that, by increasing the number of experts, M3DT not only consistently enhances its performance as model expansion on the fixed task numbers, but also exhibits remarkable task scalability, successfully extending to 160 tasks with superior performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.24378v1",
    "published_date": "2025-05-30 09:08:52 UTC",
    "updated_date": "2025-05-30 09:08:52 UTC"
  },
  {
    "arxiv_id": "2506.15707v2",
    "title": "Every Rollout Counts: Optimal Resource Allocation for Efficient Test-Time Scaling",
    "authors": [
      "Xinglin Wang",
      "Yiwei Li",
      "Shaoxiong Feng",
      "Peiwen Yuan",
      "Yueqi Zhang",
      "Jiayi Shi",
      "Chuyi Tan",
      "Boyuan Pan",
      "Yao Hu",
      "Kan Li"
    ],
    "abstract": "Test-Time Scaling (TTS) improves the performance of Large Language Models (LLMs) by using additional inference-time computation to explore multiple reasoning paths through search. Yet how to allocate a fixed rollout budget most effectively during search remains underexplored, often resulting in inefficient use of compute at test time. To bridge this gap, we formulate test-time search as a resource allocation problem and derive the optimal allocation strategy that maximizes the probability of obtaining a correct solution under a fixed rollout budget. Within this formulation, we reveal a core limitation of existing search methods: solution-level allocation tends to favor reasoning directions with more candidates, leading to theoretically suboptimal and inefficient use of compute. To address this, we propose Direction-Oriented Resource Allocation (DORA), a provably optimal method that mitigates this bias by decoupling direction quality from candidate count and allocating resources at the direction level. To demonstrate DORA's effectiveness, we conduct extensive experiments on challenging mathematical reasoning benchmarks including MATH500, AIME2024, and AIME2025. The empirical results show that DORA consistently outperforms strong baselines with comparable computational cost, achieving state-of-the-art accuracy. We hope our findings contribute to a broader understanding of optimal TTS for LLMs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at NeurIPS2025",
    "pdf_url": "https://arxiv.org/pdf/2506.15707v2",
    "published_date": "2025-05-30 09:05:25 UTC",
    "updated_date": "2025-10-20 08:29:40 UTC"
  },
  {
    "arxiv_id": "2505.24371v3",
    "title": "Grid-LOGAT: Grid Based Local and Global Area Transcription for Video Question Answering",
    "authors": [
      "Md Intisar Chowdhury",
      "Kittinun Aukkapinyo",
      "Hiroshi Fujimura",
      "Joo Ann Woo",
      "Wasu Wasusatein",
      "Fadoua Ghourabi"
    ],
    "abstract": "In this paper, we propose a Grid-based Local and Global Area Transcription (Grid-LoGAT) system for Video Question Answering (VideoQA). The system operates in two phases. First, extracting text transcripts from video frames using a Vision-Language Model (VLM). Next, processing questions using these transcripts to generate answers through a Large Language Model (LLM). This design ensures image privacy by deploying the VLM on edge devices and the LLM in the cloud. To improve transcript quality, we propose grid-based visual prompting, which extracts intricate local details from each grid cell and integrates them with global information. Evaluation results show that Grid-LoGAT, using the open-source VLM (LLaVA-1.6-7B) and LLM (Llama-3.1-8B), outperforms state-of-the-art methods with similar baseline models on NExT-QA and STAR-QA datasets with an accuracy of 65.9% and 50.11% respectively. Additionally, our method surpasses the non-grid version by 24 points on localization-based questions we created using NExT-QA. (This paper is accepted by IEEE ICIP 2025.)",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Copyright 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works",
    "pdf_url": "https://arxiv.org/pdf/2505.24371v3",
    "published_date": "2025-05-30 09:04:30 UTC",
    "updated_date": "2025-07-28 02:22:21 UTC"
  },
  {
    "arxiv_id": "2505.24369v1",
    "title": "Adversarial Preference Learning for Robust LLM Alignment",
    "authors": [
      "Yuanfu Wang",
      "Pengyu Wang",
      "Chenyang Xi",
      "Bo Tang",
      "Junyi Zhu",
      "Wenqiang Wei",
      "Chen Chen",
      "Chao Yang",
      "Jingfeng Zhang",
      "Chaochao Lu",
      "Yijun Niu",
      "Keming Mao",
      "Zhiyu Li",
      "Feiyu Xiong",
      "Jie Hu",
      "Mingchuan Yang"
    ],
    "abstract": "Modern language models often rely on Reinforcement Learning from Human Feedback (RLHF) to encourage safe behaviors. However, they remain vulnerable to adversarial attacks due to three key limitations: (1) the inefficiency and high cost of human annotation, (2) the vast diversity of potential adversarial attacks, and (3) the risk of feedback bias and reward hacking. To address these challenges, we introduce Adversarial Preference Learning (APL), an iterative adversarial training method incorporating three key innovations. First, a direct harmfulness metric based on the model's intrinsic preference probabilities, eliminating reliance on external assessment. Second, a conditional generative attacker that synthesizes input-specific adversarial variations. Third, an iterative framework with automated closed-loop feedback, enabling continuous adaptation through vulnerability discovery and mitigation. Experiments on Mistral-7B-Instruct-v0.3 demonstrate that APL significantly enhances robustness, achieving 83.33% harmlessness win rate over the base model (evaluated by GPT-4o), reducing harmful outputs from 5.88% to 0.43% (measured by LLaMA-Guard), and lowering attack success rate by up to 65% according to HarmBench. Notably, APL maintains competitive utility, with an MT-Bench score of 6.59 (comparable to the baseline 6.78) and an LC-WinRate of 46.52% against the base model.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at ACL2025 Findings",
    "pdf_url": "https://arxiv.org/pdf/2505.24369v1",
    "published_date": "2025-05-30 09:02:07 UTC",
    "updated_date": "2025-05-30 09:02:07 UTC"
  },
  {
    "arxiv_id": "2505.24357v3",
    "title": "ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline Calibration",
    "authors": [
      "Xianglong Yan",
      "Zhiteng Li",
      "Tianao Zhang",
      "Haotong Qin",
      "Linghe Kong",
      "Yulun Zhang",
      "Xiaokang Yang"
    ],
    "abstract": "Large language models (LLMs) have demonstrated remarkable performance, but their long-context reasoning remains constrained by the excessive memory required for the Key-Value (KV) cache. This makes KV cache compression a critical step toward efficient long-context inference. Recent methods have explored low-rank techniques to reduce the hidden size of the KV cache. However, they neglect the distinct roles and varying importance of Keys and Values, leading to significant performance drops under high compression. To address this, we propose ReCalKV, a post-training low-rank KV cache compression approach with tailored strategies for Keys and Values. For Keys, we propose Head-wise Similarity aware Reordering (HSR), which clusters structurally similar heads into groups, enabling more accurate low-rank approximation via grouped SVD. For Values, we propose Offline Value Calibration (OVC), which efficiently calibrates the value projection matrix using calibration data without training, ensuring an accurate representation of contextual information. Extensive experiments show that ReCalKV consistently outperforms existing low-rank compression methods, achieving high compression ratios with minimal performance loss. The code and models will be available at:https://github.com/XIANGLONGYAN/ReCalKV.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24357v3",
    "published_date": "2025-05-30 08:49:27 UTC",
    "updated_date": "2025-09-27 03:37:40 UTC"
  },
  {
    "arxiv_id": "2506.15706v1",
    "title": "MDPO: Multi-Granularity Direct Preference Optimization for Mathematical Reasoning",
    "authors": [
      "Yunze Lin"
    ],
    "abstract": "Mathematical reasoning presents a significant challenge for Large Language Models (LLMs) as it requires ensuring the correctness of each reasoning step. Researchers have been strengthening the mathematical reasoning abilities of LLMs through supervised fine-tuning, but due to the inability to suppress incorrect outputs, illusions can easily arise. Recently, Direct Preference Optimization (DPO) has been widely adopted for aligning human intent by using preference data to prevent LLMs from generating incorrect outputs. However, it has shown limited benefits in long-chain mathematical reasoning, mainly because DPO struggles to effectively capture the differences between accepted and rejected answers from preferences in long-chain data. The inconsistency between DPO training and LLMs' generation metrics also affects the effectiveness of suppressing incorrect outputs. We propose the Multi-Granularity Direct Preference Optimization (MDPO) method, optimizing the mathematical reasoning of LLMs at three granularities: Solution2Solution, Inference2Inference, and Step2Step. Solution2Solution focuses on the correctness of entire long-chain reasoning; Inference2Inference concentrates on logical reasoning between steps; Step2Step corrects computational errors in steps, enhancing the computational capabilities of LLMs. Additionally, we unify the training objectives of the three granularities to align with the generation metrics. We conducted experiments on the open-source models Qwen2 and Llama3, achieving improvements of 1.7% and 0.9% on the GSM8K dataset, and 2.3% and 1.2% on the MATH dataset, outperforming DPO and other DPO variant methods. Furthermore, we also provide a pipeline for constructing MDPO training data that is simple and does not require manual annotation costs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.15706v1",
    "published_date": "2025-05-30 08:42:14 UTC",
    "updated_date": "2025-05-30 08:42:14 UTC"
  },
  {
    "arxiv_id": "2505.24341v1",
    "title": "Exploring Multimodal Challenges in Toxic Chinese Detection: Taxonomy, Benchmark, and Findings",
    "authors": [
      "Shujian Yang",
      "Shiyao Cui",
      "Chuanrui Hu",
      "Haicheng Wang",
      "Tianwei Zhang",
      "Minlie Huang",
      "Jialiang Lu",
      "Han Qiu"
    ],
    "abstract": "Detecting toxic content using language models is important but challenging. While large language models (LLMs) have demonstrated strong performance in understanding Chinese, recent studies show that simple character substitutions in toxic Chinese text can easily confuse the state-of-the-art (SOTA) LLMs. In this paper, we highlight the multimodal nature of Chinese language as a key challenge for deploying LLMs in toxic Chinese detection. First, we propose a taxonomy of 3 perturbation strategies and 8 specific approaches in toxic Chinese content. Then, we curate a dataset based on this taxonomy, and benchmark 9 SOTA LLMs (from both the US and China) to assess if they can detect perturbed toxic Chinese text. Additionally, we explore cost-effective enhancement solutions like in-context learning (ICL) and supervised fine-tuning (SFT). Our results reveal two important findings. (1) LLMs are less capable of detecting perturbed multimodal Chinese toxic contents. (2) ICL or SFT with a small number of perturbed examples may cause the LLMs \"overcorrect'': misidentify many normal Chinese contents as toxic.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ACL 2025 (Findings). Camera-ready version",
    "pdf_url": "https://arxiv.org/pdf/2505.24341v1",
    "published_date": "2025-05-30 08:32:45 UTC",
    "updated_date": "2025-05-30 08:32:45 UTC"
  },
  {
    "arxiv_id": "2505.24336v1",
    "title": "When Humans Growl and Birds Speak: High-Fidelity Voice Conversion from Human to Animal and Designed Sounds",
    "authors": [
      "Minsu Kang",
      "Seolhee Lee",
      "Choonghyeon Lee",
      "Namhyun Cho"
    ],
    "abstract": "Human to non-human voice conversion (H2NH-VC) transforms human speech into animal or designed vocalizations. Unlike prior studies focused on dog-sounds and 16 or 22.05kHz audio transformation, this work addresses a broader range of non-speech sounds, including natural sounds (lion-roars, birdsongs) and designed voice (synthetic growls). To accomodate generation of diverse non-speech sounds and 44.1kHz high-quality audio transformation, we introduce a preprocessing pipeline and an improved CVAE-based H2NH-VC model, both optimized for human and non-human voices. Experimental results showed that the proposed method outperformed baselines in quality, naturalness, and similarity MOS, achieving effective voice conversion across diverse non-human timbres. Demo samples are available at https://nc-ai.github.io/speech/publications/nonhuman-vc/",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.SP"
    ],
    "primary_category": "eess.AS",
    "comment": "INTERSPEECH 2025 accepted",
    "pdf_url": "https://arxiv.org/pdf/2505.24336v1",
    "published_date": "2025-05-30 08:24:41 UTC",
    "updated_date": "2025-05-30 08:24:41 UTC"
  },
  {
    "arxiv_id": "2505.24306v2",
    "title": "GridRoute: A Benchmark for LLM-Based Route Planning with Cardinal Movement in Grid Environments",
    "authors": [
      "Kechen Li",
      "Yaotian Tao",
      "Ximing Wen",
      "Quanwei Sun",
      "Zifei Gong",
      "Chang Xu",
      "Xizhe Zhang",
      "Tianbo Ji"
    ],
    "abstract": "Recent advancements in Large Language Models (LLMs) have demonstrated their potential in planning and reasoning tasks, offering a flexible alternative to classical pathfinding algorithms. However, most existing studies focus on LLMs' independent reasoning capabilities and overlook the potential synergy between LLMs and traditional algorithms. To fill this gap, we propose a comprehensive evaluation benchmark GridRoute to assess how LLMs can take advantage of traditional algorithms. We also propose a novel hybrid prompting technique called Algorithm of Thought (AoT), which introduces traditional algorithms' guidance into prompting. Our benchmark evaluates six LLMs ranging from 7B to 72B parameters across various map sizes, assessing their performance in correctness, optimality, and efficiency in grid environments with varying sizes. Our results show that AoT significantly boosts performance across all model sizes, particularly in larger or more complex environments, suggesting a promising approach to addressing path planning challenges. Our code is open-sourced at https://github.com/LinChance/GridRoute.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.24306v2",
    "published_date": "2025-05-30 07:40:59 UTC",
    "updated_date": "2025-08-13 05:59:06 UTC"
  },
  {
    "arxiv_id": "2505.24298v4",
    "title": "AReaL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning",
    "authors": [
      "Wei Fu",
      "Jiaxuan Gao",
      "Xujie Shen",
      "Chen Zhu",
      "Zhiyu Mei",
      "Chuyi He",
      "Shusheng Xu",
      "Guo Wei",
      "Jun Mei",
      "Jiashu Wang",
      "Tongkai Yang",
      "Binhang Yuan",
      "Yi Wu"
    ],
    "abstract": "Reinforcement learning (RL) has become a dominant paradigm for training large language models (LLMs), particularly for reasoning tasks. Effective RL for LLMs requires massive parallelization and poses an urgent need for efficient training systems. Most existing large-scale RL systems for LLMs are synchronous, alternating generation and training in a batch setting where rollouts in each training batch are generated by the same model. This approach stabilizes RL training but suffers from severe system-level inefficiency: generation must wait until the longest output in the batch is completed before model updates, resulting in GPU underutilization. We present AReaL, a fully asynchronous RL system that completely decouples generation from training. Rollout workers in AReaL continuously generate new outputs without waiting, while training workers update the model whenever a batch of data is collected. AReaL also incorporates a collection of system-level optimizations, leading to substantially higher GPU utilization. To stabilize RL training, AReaL balances the workload of rollout and training workers to control data staleness, and adopts a staleness-enhanced PPO variant to better handle outdated training samples. Extensive experiments on math and code reasoning benchmarks show that AReaL achieves up to 2.77$\\times$ training speedup compared to synchronous systems with the same number of GPUs and matched or improved final performance. The code of AReaL is available at https://github.com/inclusionAI/AReaL/.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24298v4",
    "published_date": "2025-05-30 07:18:25 UTC",
    "updated_date": "2025-11-25 05:48:23 UTC"
  },
  {
    "arxiv_id": "2506.00089v2",
    "title": "TRAPDOC: Deceiving LLM Users by Injecting Imperceptible Phantom Tokens into Documents",
    "authors": [
      "Hyundong Jin",
      "Sicheol Sung",
      "Shinwoo Park",
      "SeungYeop Baik",
      "Yo-Sub Han"
    ],
    "abstract": "The reasoning, writing, text-editing, and retrieval capabilities of proprietary large language models (LLMs) have advanced rapidly, providing users with an ever-expanding set of functionalities. However, this growing utility has also led to a serious societal concern: the over-reliance on LLMs. In particular, users increasingly delegate tasks such as homework, assignments, or the processing of sensitive documents to LLMs without meaningful engagement. This form of over-reliance and misuse is emerging as a significant social issue. In order to mitigate these issues, we propose a method injecting imperceptible phantom tokens into documents, which causes LLMs to generate outputs that appear plausible to users but are in fact incorrect. Based on this technique, we introduce TRAPDOC, a framework designed to deceive over-reliant LLM users. Through empirical evaluation, we demonstrate the effectiveness of our framework on proprietary LLMs, comparing its impact against several baselines. TRAPDOC serves as a strong foundation for promoting more responsible and thoughtful engagement with language models. Our code is available at https://github.com/jindong22/TrapDoc.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "EMNLP 2025 Findings",
    "pdf_url": "https://arxiv.org/pdf/2506.00089v2",
    "published_date": "2025-05-30 07:16:53 UTC",
    "updated_date": "2025-09-28 07:05:30 UTC"
  },
  {
    "arxiv_id": "2505.24293v3",
    "title": "Equivalent Linear Mappings of Large Language Models",
    "authors": [
      "James R. Golden"
    ],
    "abstract": "Despite significant progress in transformer interpretability, an understanding of the computational mechanisms of large language models (LLMs) remains a fundamental challenge. Many approaches interpret a network's hidden representations but remain agnostic about how those representations are generated. We address this by mapping LLM inference for a given input sequence to an equivalent and interpretable linear system which reconstructs the predicted output embedding with relative error below $10^{-13}$ at double floating-point precision, requiring no additional model training. We exploit a property of transformers wherein every operation (gated activations, attention, and normalization) can be expressed as $A(x) \\cdot x$, where $A(x)$ represents an input-dependent linear transform and $x$ preserves the linear pathway. To expose this linear structure, we strategically detach components of the gradient computation with respect to an input sequence, freezing the $A(x)$ terms at their values computed during inference, such that the Jacobian yields an equivalent linear mapping. This detached Jacobian of the model reconstructs the output with one linear operator per input token, which is shown for Qwen 3, Gemma 3 and Llama 3, up to Qwen 3 14B. These linear representations demonstrate that LLMs operate in extremely low-dimensional subspaces where the singular vectors can be decoded to interpretable semantic concepts. The computation for each intermediate output also has a linear equivalent, and we examine how the linear representations of individual layers and their attention and multilayer perceptron modules build predictions, and use these as steering operators to insert semantic concepts into unrelated text. Despite their global nonlinearity, LLMs can be interpreted through equivalent linear representations that reveal low-dimensional semantic structures in the next-token prediction process.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "title changed; major revisions; code available at https://github.com/jamesgolden1/equivalent-linear-LLMs/; published at https://openreview.net/forum?id=oDWbJsIuEp",
    "pdf_url": "https://arxiv.org/pdf/2505.24293v3",
    "published_date": "2025-05-30 07:08:33 UTC",
    "updated_date": "2025-10-11 04:29:51 UTC"
  },
  {
    "arxiv_id": "2505.24292v1",
    "title": "Mind the Quote: Enabling Quotation-Aware Dialogue in LLMs via Plug-and-Play Modules",
    "authors": [
      "Yueqi Zhang",
      "Peiwen Yuan",
      "Shaoxiong Feng",
      "Yiwei Li",
      "Xinglin Wang",
      "Jiayi Shi",
      "Chuyi Tan",
      "Boyuan Pan",
      "Yao Hu",
      "Kan Li"
    ],
    "abstract": "Human-AI conversation frequently relies on quoting earlier text-\"check it with the formula I just highlighted\"-yet today's large language models (LLMs) lack an explicit mechanism for locating and exploiting such spans. We formalise the challenge as span-conditioned generation, decomposing each turn into the dialogue history, a set of token-offset quotation spans, and an intent utterance. Building on this abstraction, we introduce a quotation-centric data pipeline that automatically synthesises task-specific dialogues, verifies answer correctness through multi-stage consistency checks, and yields both a heterogeneous training corpus and the first benchmark covering five representative scenarios. To meet the benchmark's zero-overhead and parameter-efficiency requirements, we propose QuAda, a lightweight training-based method that attaches two bottleneck projections to every attention head, dynamically amplifying or suppressing attention to quoted spans at inference time while leaving the prompt unchanged and updating < 2.8% of backbone weights. Experiments across models show that QuAda is suitable for all scenarios and generalises to unseen topics, offering an effective, plug-and-play solution for quotation-aware dialogue.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24292v1",
    "published_date": "2025-05-30 07:06:11 UTC",
    "updated_date": "2025-05-30 07:06:11 UTC"
  },
  {
    "arxiv_id": "2505.24291v1",
    "title": "Discl-VC: Disentangled Discrete Tokens and In-Context Learning for Controllable Zero-Shot Voice Conversion",
    "authors": [
      "Kaidi Wang",
      "Wenhao Guan",
      "Ziyue Jiang",
      "Hukai Huang",
      "Peijie Chen",
      "Weijie Wu",
      "Qingyang Hong",
      "Lin Li"
    ],
    "abstract": "Currently, zero-shot voice conversion systems are capable of synthesizing the voice of unseen speakers. However, most existing approaches struggle to accurately replicate the speaking style of the source speaker or mimic the distinctive speaking style of the target speaker, thereby limiting the controllability of voice conversion. In this work, we propose Discl-VC, a novel voice conversion framework that disentangles content and prosody information from self-supervised speech representations and synthesizes the target speaker's voice through in-context learning with a flow matching transformer. To enable precise control over the prosody of generated speech, we introduce a mask generative transformer that predicts discrete prosody tokens in a non-autoregressive manner based on prompts. Experimental results demonstrate the superior performance of Discl-VC in zero-shot voice conversion and its remarkable accuracy in prosody control for synthesized speech.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24291v1",
    "published_date": "2025-05-30 07:04:23 UTC",
    "updated_date": "2025-05-30 07:04:23 UTC"
  },
  {
    "arxiv_id": "2505.24273v1",
    "title": "How Much Backtracking is Enough? Exploring the Interplay of SFT and RL in Enhancing LLM Reasoning",
    "authors": [
      "Hongyi James Cai",
      "Junlin Wang",
      "Xiaoyin Chen",
      "Bhuwan Dhingra"
    ],
    "abstract": "Recent breakthroughs in large language models (LLMs) have effectively improved their reasoning abilities, particularly on mathematical and logical problems that have verifiable answers, through techniques such as supervised finetuning (SFT) and reinforcement learning (RL). Prior research indicates that RL effectively internalizes search strategies, enabling long chain-of-thought (CoT) reasoning, with backtracking emerging naturally as a learned capability. However, the precise benefits of backtracking, specifically, how significantly it contributes to reasoning improvements and the optimal extent of its use, remain poorly understood. In this work, we systematically investigate the dynamics between SFT and RL on eight reasoning tasks: Countdown, Sudoku, Arc 1D, Geometry, Color Cube Rotation, List Functions, Zebra Puzzles, and Self Reference. Our findings highlight that short CoT sequences used in SFT as a warm-up do have moderate contribution to RL training, compared with cold-start RL; however such contribution diminishes when tasks become increasingly difficult. Motivated by this observation, we construct synthetic datasets varying systematically in the number of backtracking steps and conduct controlled experiments to isolate the influence of either the correctness (content) or the structure (i.e., backtrack frequency). We find that (1) longer CoT with backtracks generally induce better and more stable RL training, (2) more challenging problems with larger search space tend to need higher numbers of backtracks during the SFT stage. Additionally, we demonstrate through experiments on distilled data that RL training is largely unaffected by the correctness of long CoT sequences, suggesting that RL prioritizes structural patterns over content correctness. Collectively, our results offer practical insights into designing optimal training strategies to effectively scale reasoning in LLMs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24273v1",
    "published_date": "2025-05-30 06:49:00 UTC",
    "updated_date": "2025-05-30 06:49:00 UTC"
  },
  {
    "arxiv_id": "2506.11063v1",
    "title": "Who is in the Spotlight: The Hidden Bias Undermining Multimodal Retrieval-Augmented Generation",
    "authors": [
      "Jiayu Yao",
      "Shenghua Liu",
      "Yiwei Wang",
      "Lingrui Mei",
      "Baolong Bi",
      "Yuyao Ge",
      "Zhecheng Li",
      "Xueqi Cheng"
    ],
    "abstract": "Multimodal Retrieval-Augmented Generation (RAG) systems have become essential in knowledge-intensive and open-domain tasks. As retrieval complexity increases, ensuring the robustness of these systems is critical. However, current RAG models are highly sensitive to the order in which evidence is presented, often resulting in unstable performance and biased reasoning, particularly as the number of retrieved items or modality diversity grows. This raises a central question: How does the position of retrieved evidence affect multimodal RAG performance? To answer this, we present the first comprehensive study of position bias in multimodal RAG systems. Through controlled experiments across text-only, image-only, and mixed-modality tasks, we observe a consistent U-shaped accuracy curve with respect to evidence position. To quantify this bias, we introduce the Position Sensitivity Index ($PSI_p$) and develop a visualization framework to trace attention allocation patterns across decoder layers. Our results reveal that multimodal interactions intensify position bias compared to unimodal settings, and that this bias increases logarithmically with retrieval range. These findings offer both theoretical and empirical foundations for position-aware analysis in RAG, highlighting the need for evidence reordering or debiasing strategies to build more reliable and equitable generation systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.11063v1",
    "published_date": "2025-05-30 06:48:02 UTC",
    "updated_date": "2025-05-30 06:48:02 UTC"
  },
  {
    "arxiv_id": "2505.24269v2",
    "title": "INSIGHT: A Survey of In-Network Systems for Intelligent, High-Efficiency AI and Topology Optimization",
    "authors": [
      "Aleksandr Algazinov",
      "Joydeep Chandra",
      "Matt Laing"
    ],
    "abstract": "In-network computation represents a transformative approach to addressing the escalating demands of Artificial Intelligence (AI) workloads on network infrastructure. By leveraging the processing capabilities of network devices such as switches, routers, and Network Interface Cards (NICs), this paradigm enables AI computations to be performed directly within the network fabric, significantly reducing latency, enhancing throughput, and optimizing resource utilization. This paper provides a comprehensive analysis of optimizing in-network computation for AI, exploring the evolution of programmable network architectures, such as Software-Defined Networking (SDN) and Programmable Data Planes (PDPs), and their convergence with AI. It examines methodologies for mapping AI models onto resource-constrained network devices, addressing challenges like limited memory and computational capabilities through efficient algorithm design and model compression techniques. The paper also highlights advancements in distributed learning, particularly in-network aggregation, and the potential of federated learning to enhance privacy and scalability. Frameworks like Planter and Quark are discussed for simplifying development, alongside key applications such as intelligent network monitoring, intrusion detection, traffic management, and Edge AI. Future research directions, including runtime programmability, standardized benchmarks, and new applications paradigms, are proposed to advance this rapidly evolving field. This survey underscores the potential of in-network AI to create intelligent, efficient, and responsive networks capable of meeting the demands of next-generation AI applications.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24269v2",
    "published_date": "2025-05-30 06:47:55 UTC",
    "updated_date": "2025-08-18 16:03:51 UTC"
  },
  {
    "arxiv_id": "2506.13771v4",
    "title": "LittleBit: Ultra Low-Bit Quantization via Latent Factorization",
    "authors": [
      "Banseok Lee",
      "Dongkyu Kim",
      "Youngcheon You",
      "Youngmin Kim"
    ],
    "abstract": "Deploying large language models (LLMs) often faces challenges from substantial memory and computational costs. Quantization offers a solution, yet performance degradation in the sub-1-bit regime remains particularly difficult. This paper introduces LittleBit, a novel method for extreme LLM compression. It targets levels like 0.1 bits per weight (BPW), achieving nearly 31$\\times$ memory reduction, e.g., Llama2-13B to under 0.9 GB. LittleBit represents weights in a low-rank form using latent matrix factorization, subsequently binarizing these factors. To counteract information loss from this extreme precision, it integrates a multi-scale compensation mechanism. This includes row, column, and an additional latent dimension that learns per-rank importance. Two key contributions enable effective training: Dual Sign-Value-Independent Decomposition (Dual-SVID) for quantization-aware training (QAT) initialization, and integrated Residual Compensation to mitigate errors. Extensive experiments confirm LittleBit's superiority in sub-1-bit quantization: e.g., its 0.1 BPW performance on Llama2-7B surpasses the leading method's 0.7 BPW. LittleBit establishes a new, viable size-performance trade-off--unlocking a potential 11.6$\\times$ speedup over FP16 at the kernel level--and makes powerful LLMs practical for resource-constrained environments. Our code can be found at https://github.com/SamsungLabs/LittleBit.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to NeurIPS 2025. Banseok Lee and Dongkyu Kim contributed equally",
    "pdf_url": "https://arxiv.org/pdf/2506.13771v4",
    "published_date": "2025-05-30 06:43:03 UTC",
    "updated_date": "2026-01-15 10:46:32 UTC"
  },
  {
    "arxiv_id": "2505.24264v1",
    "title": "Faithful and Robust LLM-Driven Theorem Proving for NLI Explanations",
    "authors": [
      "Xin Quan",
      "Marco Valentino",
      "Louise A. Dennis",
      "André Freitas"
    ],
    "abstract": "Natural language explanations play a fundamental role in Natural Language Inference (NLI) by revealing how premises logically entail hypotheses. Recent work has shown that the interaction of large language models (LLMs) with theorem provers (TPs) can help verify and improve the validity of NLI explanations. However, TPs require translating natural language into machine-verifiable formal representations, a process that introduces the risk of semantic information loss and unfaithful interpretation, an issue compounded by LLMs' challenges in capturing critical logical structures with sufficient precision. Moreover, LLMs are still limited in their capacity for rigorous and robust proof construction within formal verification frameworks. To mitigate issues related to faithfulness and robustness, this paper investigates strategies to (1) alleviate semantic loss during autoformalisation, (2) efficiently identify and correct syntactic errors in logical representations, (3) explicitly use logical expressions to guide LLMs in generating structured proof sketches, and (4) increase LLMs' capacity of interpreting TP's feedback for iterative refinement. Our empirical results on e-SNLI, QASC and WorldTree using different LLMs demonstrate that the proposed strategies yield significant improvements in autoformalisation (+18.46%, +34.2%, +39.77%) and explanation refinement (+29.5%, +51.5%, +41.25%) over the state-of-the-art model. Moreover, we show that specific interventions on the hybrid LLM-TP architecture can substantially improve efficiency, drastically reducing the number of iterations required for successful verification.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Camera-ready for ACL 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.24264v1",
    "published_date": "2025-05-30 06:38:39 UTC",
    "updated_date": "2025-05-30 06:38:39 UTC"
  },
  {
    "arxiv_id": "2505.24260v1",
    "title": "Generative AI for Urban Design: A Stepwise Approach Integrating Human Expertise with Multimodal Diffusion Models",
    "authors": [
      "Mingyi He",
      "Yuebing Liang",
      "Shenhao Wang",
      "Yunhan Zheng",
      "Qingyi Wang",
      "Dingyi Zhuang",
      "Li Tian",
      "Jinhua Zhao"
    ],
    "abstract": "Urban design is a multifaceted process that demands careful consideration of site-specific constraints and collaboration among diverse professionals and stakeholders. The advent of generative artificial intelligence (GenAI) offers transformative potential by improving the efficiency of design generation and facilitating the communication of design ideas. However, most existing approaches are not well integrated with human design workflows. They often follow end-to-end pipelines with limited control, overlooking the iterative nature of real-world design. This study proposes a stepwise generative urban design framework that integrates multimodal diffusion models with human expertise to enable more adaptive and controllable design processes. Instead of generating design outcomes in a single end-to-end process, the framework divides the process into three key stages aligned with established urban design workflows: (1) road network and land use planning, (2) building layout planning, and (3) detailed planning and rendering. At each stage, multimodal diffusion models generate preliminary designs based on textual prompts and image-based constraints, which can then be reviewed and refined by human designers. We design an evaluation framework to assess the fidelity, compliance, and diversity of the generated designs. Experiments using data from Chicago and New York City demonstrate that our framework outperforms baseline models and end-to-end approaches across all three dimensions. This study underscores the benefits of multimodal diffusion models and stepwise generation in preserving human control and facilitating iterative refinements, laying the groundwork for human-AI interaction in urban design solutions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24260v1",
    "published_date": "2025-05-30 06:33:48 UTC",
    "updated_date": "2025-05-30 06:33:48 UTC"
  },
  {
    "arxiv_id": "2505.24258v1",
    "title": "FABLE: A Novel Data-Flow Analysis Benchmark on Procedural Text for Large Language Model Evaluation",
    "authors": [
      "Vishal Pallagani",
      "Nitin Gupta",
      "John Aydin",
      "Biplav Srivastava"
    ],
    "abstract": "Understanding how data moves, transforms, and persists, known as data flow, is fundamental to reasoning in procedural tasks. Despite their fluency in natural and programming languages, large language models (LLMs), although increasingly being applied to decisions with procedural tasks, have not been systematically evaluated for their ability to perform data-flow reasoning. We introduce FABLE, an extensible benchmark designed to assess LLMs' understanding of data flow using structured, procedural text. FABLE adapts eight classical data-flow analyses from software engineering: reaching definitions, very busy expressions, available expressions, live variable analysis, interval analysis, type-state analysis, taint analysis, and concurrency analysis. These analyses are instantiated across three real-world domains: cooking recipes, travel routes, and automated plans. The benchmark includes 2,400 question-answer pairs, with 100 examples for each domain-analysis combination. We evaluate three types of LLMs: a reasoning-focused model (DeepSeek-R1 8B), a general-purpose model (LLaMA 3.1 8B), and a code-specific model (Granite Code 8B). Each model is tested using majority voting over five sampled completions per prompt. Results show that the reasoning model achieves higher accuracy, but at the cost of over 20 times slower inference compared to the other models. In contrast, the general-purpose and code-specific models perform close to random chance. FABLE provides the first diagnostic benchmark to systematically evaluate data-flow reasoning and offers insights for developing models with stronger procedural understanding.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24258v1",
    "published_date": "2025-05-30 06:32:34 UTC",
    "updated_date": "2025-05-30 06:32:34 UTC"
  },
  {
    "arxiv_id": "2505.24255v1",
    "title": "Effects of Theory of Mind and Prosocial Beliefs on Steering Human-Aligned Behaviors of LLMs in Ultimatum Games",
    "authors": [
      "Neemesh Yadav",
      "Palakorn Achananuparp",
      "Jing Jiang",
      "Ee-Peng Lim"
    ],
    "abstract": "Large Language Models (LLMs) have shown potential in simulating human behaviors and performing theory-of-mind (ToM) reasoning, a crucial skill for complex social interactions. In this study, we investigate the role of ToM reasoning in aligning agentic behaviors with human norms in negotiation tasks, using the ultimatum game as a controlled environment. We initialized LLM agents with different prosocial beliefs (including Greedy, Fair, and Selfless) and reasoning methods like chain-of-thought (CoT) and varying ToM levels, and examined their decision-making processes across diverse LLMs, including reasoning models like o3-mini and DeepSeek-R1 Distilled Qwen 32B. Results from 2,700 simulations indicated that ToM reasoning enhances behavior alignment, decision-making consistency, and negotiation outcomes. Consistent with previous findings, reasoning models exhibit limited capability compared to models with ToM reasoning, different roles of the game benefits with different orders of ToM reasoning. Our findings contribute to the understanding of ToM's role in enhancing human-AI interaction and cooperative decision-making. The code used for our experiments can be found at https://github.com/Stealth-py/UltimatumToM.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "17 pages, 1 figure, 6 tables",
    "pdf_url": "https://arxiv.org/pdf/2505.24255v1",
    "published_date": "2025-05-30 06:23:52 UTC",
    "updated_date": "2025-05-30 06:23:52 UTC"
  },
  {
    "arxiv_id": "2506.00088v1",
    "title": "HD-NDEs: Neural Differential Equations for Hallucination Detection in LLMs",
    "authors": [
      "Qing Li",
      "Jiahui Geng",
      "Zongxiong Chen",
      "Derui Zhu",
      "Yuxia Wang",
      "Congbo Ma",
      "Chenyang Lyu",
      "Fakhri Karray"
    ],
    "abstract": "In recent years, large language models (LLMs) have made remarkable advancements, yet hallucination, where models produce inaccurate or non-factual statements, remains a significant challenge for real-world deployment. Although current classification-based methods, such as SAPLMA, are highly efficient in mitigating hallucinations, they struggle when non-factual information arises in the early or mid-sequence of outputs, reducing their reliability. To address these issues, we propose Hallucination Detection-Neural Differential Equations (HD-NDEs), a novel method that systematically assesses the truthfulness of statements by capturing the full dynamics of LLMs within their latent space. Our approaches apply neural differential equations (Neural DEs) to model the dynamic system in the latent space of LLMs. Then, the sequence in the latent space is mapped to the classification space for truth assessment. The extensive experiments across five datasets and six widely used LLMs demonstrate the effectiveness of HD-NDEs, especially, achieving over 14% improvement in AUC-ROC on the True-False dataset compared to state-of-the-art techniques.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.00088v1",
    "published_date": "2025-05-30 06:19:49 UTC",
    "updated_date": "2025-05-30 06:19:49 UTC"
  },
  {
    "arxiv_id": "2505.24253v1",
    "title": "Interactive Video Generation via Domain Adaptation",
    "authors": [
      "Ishaan Rawal",
      "Suryansh Kumar"
    ],
    "abstract": "Text-conditioned diffusion models have emerged as powerful tools for high-quality video generation. However, enabling Interactive Video Generation (IVG), where users control motion elements such as object trajectory, remains challenging. Recent training-free approaches introduce attention masking to guide trajectory, but this often degrades perceptual quality. We identify two key failure modes in these methods, both of which we interpret as domain shift problems, and propose solutions inspired by domain adaptation. First, we attribute the perceptual degradation to internal covariate shift induced by attention masking, as pretrained models are not trained to handle masked attention. To address this, we propose mask normalization, a pre-normalization layer designed to mitigate this shift via distribution matching. Second, we address initialization gap, where the randomly sampled initial noise does not align with IVG conditioning, by introducing a temporal intrinsic diffusion prior that enforces spatio-temporal consistency at each denoising step. Extensive qualitative and quantitative evaluations demonstrate that mask normalization and temporal intrinsic denoising improve both perceptual quality and trajectory control over the existing state-of-the-art IVG techniques.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "Preprint. Under Review",
    "pdf_url": "https://arxiv.org/pdf/2505.24253v1",
    "published_date": "2025-05-30 06:19:47 UTC",
    "updated_date": "2025-05-30 06:19:47 UTC"
  },
  {
    "arxiv_id": "2505.24252v1",
    "title": "A Reward-driven Automated Webshell Malicious-code Generator for Red-teaming",
    "authors": [
      "Yizhong Ding"
    ],
    "abstract": "Frequent cyber-attacks have elevated WebShell exploitation and defense to a critical research focus within network security. However, there remains a significant shortage of publicly available, well-categorized malicious-code datasets organized by obfuscation method. Existing malicious-code generation methods, which primarily rely on prompt engineering, often suffer from limited diversity and high redundancy in the payloads they produce. To address these limitations, we propose \\textbf{RAWG}, a \\textbf{R}eward-driven \\textbf{A}utomated \\textbf{W}ebshell Malicious-code \\textbf{G}enerator designed for red-teaming applications. Our approach begins by categorizing webshell samples from common datasets into seven distinct types of obfuscation. We then employ a large language model (LLM) to extract and normalize key tokens from each sample, creating a standardized, high-quality corpus. Using this curated dataset, we perform supervised fine-tuning (SFT) on an open-source large model to enable the generation of diverse, highly obfuscated webshell malicious payloads. To further enhance generation quality, we apply Proximal Policy Optimization (PPO), treating malicious-code samples as \"chosen\" data and benign code as \"rejected\" data during reinforcement learning. Extensive experiments demonstrate that RAWG significantly outperforms current state-of-the-art methods in both payload diversity and escape effectiveness.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24252v1",
    "published_date": "2025-05-30 06:16:42 UTC",
    "updated_date": "2025-05-30 06:16:42 UTC"
  },
  {
    "arxiv_id": "2505.24245v1",
    "title": "LTM3D: Bridging Token Spaces for Conditional 3D Generation with Auto-Regressive Diffusion Framework",
    "authors": [
      "Xin Kang",
      "Zihan Zheng",
      "Lei Chu",
      "Yue Gao",
      "Jiahao Li",
      "Hao Pan",
      "Xuejin Chen",
      "Yan Lu"
    ],
    "abstract": "We present LTM3D, a Latent Token space Modeling framework for conditional 3D shape generation that integrates the strengths of diffusion and auto-regressive (AR) models. While diffusion-based methods effectively model continuous latent spaces and AR models excel at capturing inter-token dependencies, combining these paradigms for 3D shape generation remains a challenge. To address this, LTM3D features a Conditional Distribution Modeling backbone, leveraging a masked autoencoder and a diffusion model to enhance token dependency learning. Additionally, we introduce Prefix Learning, which aligns condition tokens with shape latent tokens during generation, improving flexibility across modalities. We further propose a Latent Token Reconstruction module with Reconstruction-Guided Sampling to reduce uncertainty and enhance structural fidelity in generated shapes. Our approach operates in token space, enabling support for multiple 3D representations, including signed distance fields, point clouds, meshes, and 3D Gaussian Splatting. Extensive experiments on image- and text-conditioned shape generation tasks demonstrate that LTM3D outperforms existing methods in prompt fidelity and structural accuracy while offering a generalizable framework for multi-modal, multi-representation 3D generation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24245v1",
    "published_date": "2025-05-30 06:08:45 UTC",
    "updated_date": "2025-05-30 06:08:45 UTC"
  },
  {
    "arxiv_id": "2505.24239v1",
    "title": "An Adversary-Resistant Multi-Agent LLM System via Credibility Scoring",
    "authors": [
      "Sana Ebrahimi",
      "Mohsen Dehghankar",
      "Abolfazl Asudeh"
    ],
    "abstract": "While multi-agent LLM systems show strong capabilities in various domains, they are highly vulnerable to adversarial and low-performing agents. To resolve this issue, in this paper, we introduce a general and adversary-resistant multi-agent LLM framework based on credibility scoring. We model the collaborative query-answering process as an iterative game, where the agents communicate and contribute to a final system output. Our system associates a credibility score that is used when aggregating the team outputs. The credibility scores are learned gradually based on the past contributions of each agent in query answering. Our experiments across multiple tasks and settings demonstrate our system's effectiveness in mitigating adversarial influence and enhancing the resilience of multi-agent cooperation, even in the adversary-majority settings.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24239v1",
    "published_date": "2025-05-30 05:57:37 UTC",
    "updated_date": "2025-05-30 05:57:37 UTC"
  },
  {
    "arxiv_id": "2506.00087v1",
    "title": "SwitchLingua: The First Large-Scale Multilingual and Multi-Ethnic Code-Switching Dataset",
    "authors": [
      "Peng Xie",
      "Xingyuan Liu",
      "Tsz Wai Chan",
      "Yequan Bie",
      "Yangqiu Song",
      "Yang Wang",
      "Hao Chen",
      "Kani Chen"
    ],
    "abstract": "Code-switching (CS) is the alternating use of two or more languages within a conversation or utterance, often influenced by social context and speaker identity. This linguistic phenomenon poses challenges for Automatic Speech Recognition (ASR) systems, which are typically designed for a single language and struggle to handle multilingual inputs. The growing global demand for multilingual applications, including Code-Switching ASR (CSASR), Text-to-Speech (CSTTS), and Cross-Lingual Information Retrieval (CLIR), highlights the inadequacy of existing monolingual datasets.\n  Although some code-switching datasets exist, most are limited to bilingual mixing within homogeneous ethnic groups, leaving a critical need for a large-scale, diverse benchmark akin to ImageNet in computer vision.\n  To bridge this gap, we introduce \\textbf{LinguaMaster}, a multi-agent collaboration framework specifically designed for efficient and scalable multilingual data synthesis. Leveraging this framework, we curate \\textbf{SwitchLingua}, the first large-scale multilingual and multi-ethnic code-switching dataset, including: (1) 420K CS textual samples across 12 languages, and (2) over 80 hours of audio recordings from 174 speakers representing 18 countries/regions and 63 racial/ethnic backgrounds, based on the textual data. This dataset captures rich linguistic and cultural diversity, offering a foundational resource for advancing multilingual and multicultural research. Furthermore, to address the issue that existing ASR evaluation metrics lack sensitivity to code-switching scenarios, we propose the \\textbf{Semantic-Aware Error Rate (SAER)}, a novel evaluation metric that incorporates semantic information, providing a more accurate and context-aware assessment of system performance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.00087v1",
    "published_date": "2025-05-30 05:54:46 UTC",
    "updated_date": "2025-05-30 05:54:46 UTC"
  },
  {
    "arxiv_id": "2505.24232v1",
    "title": "From Hallucinations to Jailbreaks: Rethinking the Vulnerability of Large Foundation Models",
    "authors": [
      "Haibo Jin",
      "Peiyan Zhang",
      "Peiran Wang",
      "Man Luo",
      "Haohan Wang"
    ],
    "abstract": "Large foundation models (LFMs) are susceptible to two distinct vulnerabilities: hallucinations and jailbreak attacks. While typically studied in isolation, we observe that defenses targeting one often affect the other, hinting at a deeper connection.\n  We propose a unified theoretical framework that models jailbreaks as token-level optimization and hallucinations as attention-level optimization. Within this framework, we establish two key propositions: (1) \\textit{Similar Loss Convergence} - the loss functions for both vulnerabilities converge similarly when optimizing for target-specific outputs; and (2) \\textit{Gradient Consistency in Attention Redistribution} - both exhibit consistent gradient behavior driven by shared attention dynamics.\n  We validate these propositions empirically on LLaVA-1.5 and MiniGPT-4, showing consistent optimization trends and aligned gradients. Leveraging this connection, we demonstrate that mitigation techniques for hallucinations can reduce jailbreak success rates, and vice versa. Our findings reveal a shared failure mode in LFMs and suggest that robustness strategies should jointly address both vulnerabilities.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24232v1",
    "published_date": "2025-05-30 05:48:50 UTC",
    "updated_date": "2025-05-30 05:48:50 UTC"
  },
  {
    "arxiv_id": "2505.24231v1",
    "title": "Dynamic Malware Classification of Windows PE Files using CNNs and Greyscale Images Derived from Runtime API Call Argument Conversion",
    "authors": [
      "Md Shahnawaz",
      "Bishwajit Prasad Gond",
      "Durga Prasad Mohapatra"
    ],
    "abstract": "Malware detection and classification remains a topic of concern for cybersecurity, since it is becoming common for attackers to use advanced obfuscation on their malware to stay undetected. Conventional static analysis is not effective against polymorphic and metamorphic malware as these change their appearance without modifying their behavior, thus defying the analysis by code structure alone. This makes it important to use dynamic detection that monitors malware behavior at runtime. In this paper, we present a dynamic malware categorization framework that extracts API argument calls at the runtime execution of Windows Portable Executable (PE) files. Extracting and encoding the dynamic features of API names, argument return values, and other relative features, we convert raw behavioral data to temporal patterns. To enhance feature portrayal, the generated patterns are subsequently converted into grayscale pictures using a magma colormap. These improved photos are used to teach a Convolutional Neural Network (CNN) model discriminative features, which allows for reliable and accurate malware classification. Results from experiments indicate that our method, with an average accuracy of 98.36% is effective in classifying different classes of malware and benign by integrating dynamic analysis and deep learning. It not only achieves high classification accuracy but also demonstrates significant resilience against typical evasion strategies.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24231v1",
    "published_date": "2025-05-30 05:48:10 UTC",
    "updated_date": "2025-05-30 05:48:10 UTC"
  },
  {
    "arxiv_id": "2505.24230v1",
    "title": "ProofNet++: A Neuro-Symbolic System for Formal Proof Verification with Self-Correction",
    "authors": [
      "Murari Ambati"
    ],
    "abstract": "We propose ProofNet++, a neuro-symbolic framework that enhances automated theorem proving by combining large language models (LLMs) with formal proof verification and self-correction mechanisms. Current LLM-based systems suffer from hallucinated logical steps and unverifiable reasoning. ProofNet++ mitigates these limitations by integrating symbolic proof tree supervision, a reinforcement learning loop using verifiers as reward functions, and an iterative self-correction module. Our experiments on miniF2F, Lean's mathlib, and HOL Light show that ProofNet++ significantly improves proof accuracy, correctness, and formal verifiability over prior models. We provide theoretical analysis of the convergence and stability of the verifier-guided RL framework and release our datasets and codebase for future research.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "6 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2505.24230v1",
    "published_date": "2025-05-30 05:44:34 UTC",
    "updated_date": "2025-05-30 05:44:34 UTC"
  },
  {
    "arxiv_id": "2505.24226v4",
    "title": "E^2GraphRAG: Streamlining Graph-based RAG for High Efficiency and Effectiveness",
    "authors": [
      "Yibo Zhao",
      "Jiapeng Zhu",
      "Ye Guo",
      "Kangkang He",
      "Xiang Li"
    ],
    "abstract": "Graph-based RAG methods like GraphRAG have shown promising global understanding of the knowledge base by constructing hierarchical entity graphs. However, they often suffer from inefficiency and rely on manually pre-defined query modes, limiting practical use. In this paper, we propose E^2GraphRAG, a streamlined graph-based RAG framework that improves both Efficiency and Effectiveness. During the indexing stage, E^2GraphRAG constructs a summary tree with large language models and an entity graph with SpaCy based on document chunks. We then construct bidirectional indexes between entities and chunks to capture their many-to-many relationships, enabling fast lookup during both local and global retrieval. For the retrieval stage, we design an adaptive retrieval strategy that leverages the graph structure to retrieve and select between local and global modes. Experiments show that E^2GraphRAG achieves up to 10 times faster indexing than GraphRAG and 100 times speedup over LightRAG in retrieval while maintaining competitive QA performance.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "16 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.24226v4",
    "published_date": "2025-05-30 05:27:40 UTC",
    "updated_date": "2025-06-06 12:11:48 UTC"
  },
  {
    "arxiv_id": "2505.24225v1",
    "title": "Reasoning Can Hurt the Inductive Abilities of Large Language Models",
    "authors": [
      "Haibo Jin",
      "Peiyan Zhang",
      "Man Luo",
      "Haohan Wang"
    ],
    "abstract": "Large Language Models (LLMs) have shown remarkable progress across domains, yet their ability to perform inductive reasoning - inferring latent rules from sparse examples - remains limited. It is often assumed that chain-of-thought (CoT) prompting, as used in Large Reasoning Models (LRMs), enhances such reasoning. We investigate this assumption with creating four controlled, diagnostic game-based tasks - chess, Texas Hold'em, dice games, and blackjack - with hidden human-defined rules. We find that CoT reasoning can degrade inductive performance, with LRMs often underperforming their non-reasoning counterparts.\n  To explain this, we present a theoretical framework that reveals how reasoning steps can amplify error through three failure modes: incorrect sub-task decomposition, incorrect sub-task solving, and incorrect final answer summarization. Based on our theoretical and empirical analysis, we introduce structured interventions that adapt CoT generation according to our identified failure types. These interventions improve inductive accuracy without retraining. Our findings suggest that effective (CoT) reasoning depends not only on taking more steps but also on ensuring those steps are well-structured.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "26 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.24225v1",
    "published_date": "2025-05-30 05:24:21 UTC",
    "updated_date": "2025-05-30 05:24:21 UTC"
  },
  {
    "arxiv_id": "2506.00085v1",
    "title": "COSMIC: Generalized Refusal Direction Identification in LLM Activations",
    "authors": [
      "Vincent Siu",
      "Nicholas Crispino",
      "Zihao Yu",
      "Sam Pan",
      "Zhun Wang",
      "Yang Liu",
      "Dawn Song",
      "Chenguang Wang"
    ],
    "abstract": "Large Language Models (LLMs) encode behaviors such as refusal within their activation space, yet identifying these behaviors remains a significant challenge. Existing methods often rely on predefined refusal templates detectable in output tokens or require manual analysis. We introduce \\textbf{COSMIC} (Cosine Similarity Metrics for Inversion of Concepts), an automated framework for direction selection that identifies viable steering directions and target layers using cosine similarity - entirely independent of model outputs. COSMIC achieves steering performance comparable to prior methods without requiring assumptions about a model's refusal behavior, such as the presence of specific refusal tokens. It reliably identifies refusal directions in adversarial settings and weakly aligned models, and is capable of steering such models toward safer behavior with minimal increase in false refusals, demonstrating robustness across a wide range of alignment conditions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, Accepted to ACL 2025 Findings",
    "pdf_url": "https://arxiv.org/pdf/2506.00085v1",
    "published_date": "2025-05-30 04:54:18 UTC",
    "updated_date": "2025-05-30 04:54:18 UTC"
  },
  {
    "arxiv_id": "2505.24214v1",
    "title": "Benchmarking Foundation Models for Zero-Shot Biometric Tasks",
    "authors": [
      "Redwan Sony",
      "Parisa Farmanifard",
      "Hamzeh Alzwairy",
      "Nitish Shukla",
      "Arun Ross"
    ],
    "abstract": "The advent of foundation models, particularly Vision-Language Models (VLMs) and Multi-modal Large Language Models (MLLMs), has redefined the frontiers of artificial intelligence, enabling remarkable generalization across diverse tasks with minimal or no supervision. Yet, their potential in biometric recognition and analysis remains relatively underexplored. In this work, we introduce a comprehensive benchmark that evaluates the zero-shot and few-shot performance of state-of-the-art publicly available VLMs and MLLMs across six biometric tasks spanning the face and iris modalities: face verification, soft biometric attribute prediction (gender and race), iris recognition, presentation attack detection (PAD), and face manipulation detection (morphs and deepfakes). A total of 41 VLMs were used in this evaluation. Experiments show that embeddings from these foundation models can be used for diverse biometric tasks with varying degrees of success. For example, in the case of face verification, a True Match Rate (TMR) of 96.77 percent was obtained at a False Match Rate (FMR) of 1 percent on the Labeled Face in the Wild (LFW) dataset, without any fine-tuning. In the case of iris recognition, the TMR at 1 percent FMR on the IITD-R-Full dataset was 97.55 percent without any fine-tuning. Further, we show that applying a simple classifier head to these embeddings can help perform DeepFake detection for faces, Presentation Attack Detection (PAD) for irides, and extract soft biometric attributes like gender and ethnicity from faces with reasonably high accuracy. This work reiterates the potential of pretrained models in achieving the long-term vision of Artificial General Intelligence.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24214v1",
    "published_date": "2025-05-30 04:53:55 UTC",
    "updated_date": "2025-05-30 04:53:55 UTC"
  },
  {
    "arxiv_id": "2505.24208v1",
    "title": "Bootstrapping LLM Robustness for VLM Safety via Reducing the Pretraining Modality Gap",
    "authors": [
      "Wenhan Yang",
      "Spencer Stice",
      "Ali Payani",
      "Baharan Mirzasoleiman"
    ],
    "abstract": "Ensuring Vision-Language Models (VLMs) generate safe outputs is crucial for their reliable deployment. However, LVLMs suffer from drastic safety degradation compared to their LLM backbone. Even blank or irrelevant images can trigger LVLMs to generate harmful responses to prompts that would otherwise be refused in text-only contexts. The modality gap between image and text representations has been recently hypothesized to contribute to safety degradation of LVLMs. However, if and how the amount of modality gap affects LVLMs' safety is not studied. In this work, we show that the amount of modality gap is highly inversely correlated with VLMs' safety. Then, we show that this modality gap is introduced during pretraining LVLMs and persists through fine-tuning. Inspired by this observation, we propose a regularization to reduce the modality gap during pretraining. Our extensive experiments on LLaVA v1.5, ShareGPT4V, and MiniGPT-4 show that our method substantially improves safety alignment of LVLMs, reducing unsafe rate by up to 16.3% without compromising performance, and can further boost existing defenses by up to 18.2%.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24208v1",
    "published_date": "2025-05-30 04:40:08 UTC",
    "updated_date": "2025-05-30 04:40:08 UTC"
  },
  {
    "arxiv_id": "2505.24201v1",
    "title": "SentinelAgent: Graph-based Anomaly Detection in Multi-Agent Systems",
    "authors": [
      "Xu He",
      "Di Wu",
      "Yan Zhai",
      "Kun Sun"
    ],
    "abstract": "The rise of large language model (LLM)-based multi-agent systems (MAS) introduces new security and reliability challenges. While these systems show great promise in decomposing and coordinating complex tasks, they also face multi-faceted risks across prompt manipulation, unsafe tool usage, and emergent agent miscoordination. Existing guardrail mechanisms offer only partial protection, primarily at the input-output level, and fall short in addressing systemic or multi-point failures in MAS. In this work, we present a system-level anomaly detection framework tailored for MAS, integrating structural modeling with runtime behavioral oversight. Our approach consists of two components. First, we propose a graph-based framework that models agent interactions as dynamic execution graphs, enabling semantic anomaly detection at node, edge, and path levels. Second, we introduce a pluggable SentinelAgent, an LLM-powered oversight agent that observes, analyzes, and intervenes in MAS execution based on security policies and contextual reasoning. By bridging abstract detection logic with actionable enforcement, our method detects not only single-point faults and prompt injections but also multi-agent collusion and latent exploit paths. We validate our framework through two case studies, including an email assistant and Microsoft's Magentic-One system, demonstrating its ability to detect covert risks and provide explainable root-cause attribution. Our work lays the foundation for more trustworthy, monitorable, and secure agent-based AI ecosystems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24201v1",
    "published_date": "2025-05-30 04:25:19 UTC",
    "updated_date": "2025-05-30 04:25:19 UTC"
  },
  {
    "arxiv_id": "2505.24197v2",
    "title": "Learning API Functionality from In-Context Demonstrations for Tool-based Agents",
    "authors": [
      "Bhrij Patel",
      "Ashish Jagmohan",
      "Aditya Vempaty"
    ],
    "abstract": "Digital tool-based agents, powered by Large Language Models (LLMs), that invoke external Application Programming Interfaces (APIs) often rely on documentation to understand API functionality. However, such documentation is frequently missing, outdated, privatized, or inconsistent-hindering the development of reliable, general-purpose agents. In this work, we propose a new research direction: learning of API functionality directly from in-context demonstrations. This task is a new paradigm applicable in scenarios without documentation. Using API benchmarks, we collect demonstrations from both expert agents and from self-exploration. To understand what information demonstrations must convey for successful task completion, we extensively study how the number of demonstrations and the use of LLM-generated summaries and evaluations affect the task success rate of the API-based agent. Our experiments across 3 datasets and 6 models show that learning functionality from in-context demonstrations remains a non-trivial challenge, even for state-of-the-art LLMs. We find that providing explicit function calls and natural language critiques significantly improves the agent's task success rate due to more accurate parameter filling. We analyze failure modes, identify sources of error, and highlight key open challenges for future work in documentation-free, self-improving, API-based agents.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "19 Pages, 14 Figures, 7 Tables",
    "pdf_url": "https://arxiv.org/pdf/2505.24197v2",
    "published_date": "2025-05-30 04:17:09 UTC",
    "updated_date": "2025-11-12 03:07:31 UTC"
  },
  {
    "arxiv_id": "2505.24189v2",
    "title": "Fine-Tune an SLM or Prompt an LLM? The Case of Generating Low-Code Workflows",
    "authors": [
      "Orlando Marquez Ayala",
      "Patrice Bechard",
      "Emily Chen",
      "Maggie Baird",
      "Jingfei Chen"
    ],
    "abstract": "Large Language Models (LLMs) such as GPT-4o can handle a wide range of complex tasks with the right prompt. As per token costs are reduced, the advantages of fine-tuning Small Language Models (SLMs) for real-world applications -- faster inference, lower costs -- may no longer be clear. In this work, we present evidence that, for domain-specific tasks that require structured outputs, SLMs still have a quality advantage. We compare fine-tuning an SLM against prompting LLMs on the task of generating low-code workflows in JSON form. We observe that while a good prompt can yield reasonable results, fine-tuning improves quality by 10% on average. We also perform systematic error analysis to reveal model limitations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 7 figures. Accepted to Workshop on Structured Knowledge for Large Language Models (SKnowLLM) at KDD 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.24189v2",
    "published_date": "2025-05-30 03:59:35 UTC",
    "updated_date": "2025-07-16 21:38:06 UTC"
  },
  {
    "arxiv_id": "2505.24185v1",
    "title": "Towards Unified Modeling in Federated Multi-Task Learning via Subspace Decoupling",
    "authors": [
      "Yipan Wei",
      "Yuchen Zou",
      "Yapeng Li",
      "Bo Du"
    ],
    "abstract": "Federated Multi-Task Learning (FMTL) enables multiple clients performing heterogeneous tasks without exchanging their local data, offering broad potential for privacy preserving multi-task collaboration. However, most existing methods focus on building personalized models for each client and unable to support the aggregation of multiple heterogeneous tasks into a unified model. As a result, in real-world scenarios where task objectives, label spaces, and optimization paths vary significantly, conventional FMTL methods struggle to achieve effective joint training. To address this challenge, we propose FedDEA (Federated Decoupled Aggregation), an update-structure-aware aggregation method specifically designed for multi-task model integration. Our method dynamically identifies task-relevant dimensions based on the response strength of local updates and enhances their optimization effectiveness through rescaling. This mechanism effectively suppresses cross-task interference and enables task-level decoupled aggregation within a unified global model. FedDEA does not rely on task labels or architectural modifications, making it broadly applicable and deployment-friendly. Experimental results demonstrate that it can be easily integrated into various mainstream federated optimization algorithms and consistently delivers significant overall performance improvements on widely used NYUD-V2 and PASCAL-Context. These results validate the robustness and generalization capabilities of FedDEA under highly heterogeneous task settings.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24185v1",
    "published_date": "2025-05-30 03:53:21 UTC",
    "updated_date": "2025-05-30 03:53:21 UTC"
  },
  {
    "arxiv_id": "2505.24182v1",
    "title": "Seeing is Not Reasoning: MVPBench for Graph-based Evaluation of Multi-path Visual Physical CoT",
    "authors": [
      "Zhuobai Dong",
      "Junchao Yi",
      "Ziyuan Zheng",
      "Haochen Han",
      "Xiangxi Zheng",
      "Alex Jinpeng Wang",
      "Fangming Liu",
      "Linjie Li"
    ],
    "abstract": "Understanding the physical world - governed by laws of motion, spatial relations, and causality - poses a fundamental challenge for multimodal large language models (MLLMs). While recent advances such as OpenAI o3 and GPT-4o demonstrate impressive perceptual and reasoning capabilities, our investigation reveals these models struggle profoundly with visual physical reasoning, failing to grasp basic physical laws, spatial interactions, and causal effects in complex scenes. More importantly, they often fail to follow coherent reasoning chains grounded in visual evidence, especially when multiple steps are needed to arrive at the correct answer. To rigorously evaluate this capability, we introduce MVPBench, a curated benchmark designed to rigorously evaluate visual physical reasoning through the lens of visual chain-of-thought (CoT). Each example features interleaved multi-image inputs and demands not only the correct final answer but also a coherent, step-by-step reasoning path grounded in evolving visual cues. This setup mirrors how humans reason through real-world physical processes over time. To ensure fine-grained evaluation, we introduce a graph-based CoT consistency metric that verifies whether the reasoning path of model adheres to valid physical logic. Additionally, we minimize shortcut exploitation from text priors, encouraging models to rely on visual understanding. Experimental results reveal a concerning trend: even cutting-edge MLLMs exhibit poor visual reasoning accuracy and weak image-text alignment in physical domains. Surprisingly, RL-based post-training alignment - commonly believed to improve visual reasoning performance - often harms spatial reasoning, suggesting a need to rethink current fine-tuning practices.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24182v1",
    "published_date": "2025-05-30 03:48:59 UTC",
    "updated_date": "2025-05-30 03:48:59 UTC"
  },
  {
    "arxiv_id": "2505.24181v1",
    "title": "SCOUT: Teaching Pre-trained Language Models to Enhance Reasoning via Flow Chain-of-Thought",
    "authors": [
      "Guanghao Li",
      "Wenhao Jiang",
      "Mingfeng Chen",
      "Yan Li",
      "Hao Yu",
      "Shuting Dong",
      "Tao Ren",
      "Ming Tang",
      "Chun Yuan"
    ],
    "abstract": "Chain of Thought (CoT) prompting improves the reasoning performance of large language models (LLMs) by encouraging step by step thinking. However, CoT-based methods depend on intermediate reasoning steps, which limits scalability and generalization. Recent work explores recursive reasoning, where LLMs reuse internal layers across iterations to refine latent representations without explicit CoT supervision. While promising, these approaches often require costly pretraining and lack a principled framework for how reasoning should evolve across iterations. We address this gap by introducing Flow Chain of Thought (Flow CoT), a reasoning paradigm that models recursive inference as a progressive trajectory of latent cognitive states. Flow CoT frames each iteration as a distinct cognitive stage deepening reasoning across iterations without relying on manual supervision. To realize this, we propose SCOUT (Stepwise Cognitive Optimization Using Teachers), a lightweight fine tuning framework that enables Flow CoT style reasoning without the need for pretraining. SCOUT uses progressive distillation to align each iteration with a teacher of appropriate capacity, and a cross attention based retrospective module that integrates outputs from previous iterations while preserving the models original computation flow. Experiments across eight reasoning benchmarks show that SCOUT consistently improves both accuracy and explanation quality, achieving up to 1.8% gains under fine tuning. Qualitative analyses further reveal that SCOUT enables progressively deeper reasoning across iterations refining both belief formation and explanation granularity. These results not only validate the effectiveness of SCOUT, but also demonstrate the practical viability of Flow CoT as a scalable framework for enhancing reasoning in LLMs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24181v1",
    "published_date": "2025-05-30 03:43:24 UTC",
    "updated_date": "2025-05-30 03:43:24 UTC"
  },
  {
    "arxiv_id": "2505.24179v1",
    "title": "SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling",
    "authors": [
      "Xiaodong Ji",
      "Hailin Zhang",
      "Fangcheng Fu",
      "Bin Cui"
    ],
    "abstract": "Many advanced Large Language Model (LLM) applications require long-context processing, but the self-attention module becomes a bottleneck during the prefilling stage of inference due to its quadratic time complexity with respect to sequence length. Existing sparse attention methods accelerate attention computation by skipping less significant regions of the attention map. However, these approaches typically perform coarse-grained inspection of the attention map, rendering considerable loss in model accuracy. In this paper, we propose SALE, a fine-grained sparse attention method that accelerates the long-context prefilling stage of LLM with negligible loss in model accuracy. SALE achieves fast and accurate fine-grained attention weight estimation through 4-bit quantized query-key products, followed by block-sparse attention to accelerate prefilling computations. For importance evaluation for query-key pairs, we adopt our Relative Attention Score metric, which offers significantly higher efficiency within our framework. We implement a custom CUDA kernel optimized for our approach for hardware efficiency, reducing the additional overhead to approximately 11% of the full attention latency. Notably, SALE requires no parameter training and can be seamlessly integrated into existing systems with trivial code modifications. Experiments on long-context benchmarks demonstrate that our method outperforms existing approaches in accuracy-efficiency trade-offs, achieving at least 3.36x speedups on Llama-3.1-8B for sequences longer than 64K while maintaining model quality.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24179v1",
    "published_date": "2025-05-30 03:40:24 UTC",
    "updated_date": "2025-05-30 03:40:24 UTC"
  },
  {
    "arxiv_id": "2505.24178v1",
    "title": "Invariant Link Selector for Spatial-Temporal Out-of-Distribution Problem",
    "authors": [
      "Katherine Tieu",
      "Dongqi Fu",
      "Jun Wu",
      "Jingrui He"
    ],
    "abstract": "In the era of foundation models, Out-of- Distribution (OOD) problems, i.e., the data discrepancy between the training environments and testing environments, hinder AI generalization. Further, relational data like graphs disobeying the Independent and Identically Distributed (IID) condition makes the problem more challenging, especially much harder when it is associated with time. Motivated by this, to realize the robust invariant learning over temporal graphs, we want to investigate what components in temporal graphs are most invariant and representative with respect to labels. With the Information Bottleneck (IB) method, we propose an error-bounded Invariant Link Selector that can distinguish invariant components and variant components during the training process to make the deep learning model generalizable for different testing scenarios. Besides deriving a series of rigorous generalizable optimization functions, we also equip the training with task-specific loss functions, e.g., temporal link prediction, to make pretrained models solve real-world application tasks like citation recommendation and merchandise recommendation, as demonstrated in our experiments with state-of-the-art (SOTA) methods. Our code is available at https://github.com/kthrn22/OOD-Linker.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by AISTATS 2025. 22 pages, 2 figures, 6 tables",
    "pdf_url": "https://arxiv.org/pdf/2505.24178v1",
    "published_date": "2025-05-30 03:40:00 UTC",
    "updated_date": "2025-05-30 03:40:00 UTC"
  },
  {
    "arxiv_id": "2506.00083v1",
    "title": "Hi-Dyna Graph: Hierarchical Dynamic Scene Graph for Robotic Autonomy in Human-Centric Environments",
    "authors": [
      "Jiawei Hou",
      "Xiangyang Xue",
      "Taiping Zeng"
    ],
    "abstract": "Autonomous operation of service robotics in human-centric scenes remains challenging due to the need for understanding of changing environments and context-aware decision-making. While existing approaches like topological maps offer efficient spatial priors, they fail to model transient object relationships, whereas dense neural representations (e.g., NeRF) incur prohibitive computational costs. Inspired by the hierarchical scene representation and video scene graph generation works, we propose Hi-Dyna Graph, a hierarchical dynamic scene graph architecture that integrates persistent global layouts with localized dynamic semantics for embodied robotic autonomy. Our framework constructs a global topological graph from posed RGB-D inputs, encoding room-scale connectivity and large static objects (e.g., furniture), while environmental and egocentric cameras populate dynamic subgraphs with object position relations and human-object interaction patterns. A hybrid architecture is conducted by anchoring these subgraphs to the global topology using semantic and spatial constraints, enabling seamless updates as the environment evolves. An agent powered by large language models (LLMs) is employed to interpret the unified graph, infer latent task triggers, and generate executable instructions grounded in robotic affordances. We conduct complex experiments to demonstrate Hi-Dyna Grap's superior scene representation effectiveness. Real-world deployments validate the system's practicality with a mobile manipulator: robotics autonomously complete complex tasks with no further training or complex rewarding in a dynamic scene as cafeteria assistant. See https://anonymous.4open.science/r/Hi-Dyna-Graph-B326 for video demonstration and more details.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.00083v1",
    "published_date": "2025-05-30 03:35:29 UTC",
    "updated_date": "2025-05-30 03:35:29 UTC"
  },
  {
    "arxiv_id": "2506.15705v2",
    "title": "Generalisation Bounds of Zero-Shot Economic Forecasting using Time Series Foundation Models",
    "authors": [
      "Jittarin Jetwiriyanon",
      "Teo Susnjak",
      "Surangika Ranathunga"
    ],
    "abstract": "This study investigates zero-shot forecasting capabilities of Time Series Foundation Models (TSFMs) for macroeconomic indicators. We apply TSFMs to forecasting economic indicators under univariate conditions, bypassing the need for train bespoke econometric models using and extensive training datasets. Our experiments were conducted on a case study dataset, without additional customisation. We rigorously back-tested three state-of-the-art TSFMs (Chronos, TimeGPT and Moirai) under data-scarce conditions and structural breaks. Our results demonstrate that appropriately engineered TSFMs can internalise rich economic dynamics, accommodate regime shifts, and deliver well-behaved uncertainty estimates out of the box, while matching state-of-the-art multivariate models on this domain. Our findings suggest that, without any fine-tuning, TSFMs can match or exceed classical models during stable economic conditions. However, they are vulnerable to degradation in performances during periods of rapid shocks. The findings offer guidance to practitioners on when zero-shot deployments are viable for macroeconomic monitoring and strategic planning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.15705v2",
    "published_date": "2025-05-30 03:10:46 UTC",
    "updated_date": "2025-08-02 05:52:49 UTC"
  },
  {
    "arxiv_id": "2506.06324v1",
    "title": "Mapping Human-Agent Co-Learning and Co-Adaptation: A Scoping Review",
    "authors": [
      "Shruti Kumar",
      "Xiaoyu Chen",
      "Xiaomei Wang"
    ],
    "abstract": "Several papers have delved into the challenges of human-AI-robot co-learning and co-adaptation. It has been noted that the terminology used to describe this collaborative relationship in existing studies needs to be more consistent. For example, the prefix \"co\" is used interchangeably to represent both \"collaborative\" and \"mutual,\" and the terms \"co-learning\" and \"co-adaptation\" are sometimes used interchangeably. However, they can reflect subtle differences in the focus of the studies. The current scoping review's primary research question (RQ1) aims to gather existing papers discussing this collaboration pattern and examine the terms researchers use to describe this human-agent relationship. Given the relative newness of this area of study, we are also keen on exploring the specific types of intelligent agents and task domains that have been considered in existing research (RQ2). This exploration is significant as it can shed light on the diversity of human-agent interactions, from one-time to continuous learning/adaptation scenarios. It can also help us understand the dynamics of human-agent interactions in different task domains, guiding our expectations towards research situated in dynamic, complex domains. Our third objective (RQ3) is to investigate the cognitive theories and frameworks that have been utilized in existing studies to measure human-agent co-learning and co-adaptation. This investigation is crucial as it can help us understand the theoretical underpinnings of human-agent collaboration and adaptation, and it can also guide us in identifying any new frameworks proposed specifically for this type of relationship.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Abstract accepted to HFES 2024 Annual Meeting",
    "pdf_url": "https://arxiv.org/pdf/2506.06324v1",
    "published_date": "2025-05-30 03:10:44 UTC",
    "updated_date": "2025-05-30 03:10:44 UTC"
  },
  {
    "arxiv_id": "2505.24163v1",
    "title": "LKD-KGC: Domain-Specific KG Construction via LLM-driven Knowledge Dependency Parsing",
    "authors": [
      "Jiaqi Sun",
      "Shiyou Qian",
      "Zhangchi Han",
      "Wei Li",
      "Zelin Qian",
      "Dingyu Yang",
      "Jian Cao",
      "Guangtao Xue"
    ],
    "abstract": "Knowledge Graphs (KGs) structure real-world entities and their relationships into triples, enhancing machine reasoning for various tasks. While domain-specific KGs offer substantial benefits, their manual construction is often inefficient and requires specialized knowledge. Recent approaches for knowledge graph construction (KGC) based on large language models (LLMs), such as schema-guided KGC and reference knowledge integration, have proven efficient. However, these methods are constrained by their reliance on manually defined schema, single-document processing, and public-domain references, making them less effective for domain-specific corpora that exhibit complex knowledge dependencies and specificity, as well as limited reference knowledge. To address these challenges, we propose LKD-KGC, a novel framework for unsupervised domain-specific KG construction. LKD-KGC autonomously analyzes document repositories to infer knowledge dependencies, determines optimal processing sequences via LLM driven prioritization, and autoregressively generates entity schema by integrating hierarchical inter-document contexts. This schema guides the unsupervised extraction of entities and relationships, eliminating reliance on predefined structures or external knowledge. Extensive experiments show that compared with state-of-the-art baselines, LKD-KGC generally achieves improvements of 10% to 20% in both precision and recall rate, demonstrating its potential in constructing high-quality domain-specific KGs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Submitting to EDBT 2026",
    "pdf_url": "https://arxiv.org/pdf/2505.24163v1",
    "published_date": "2025-05-30 03:10:23 UTC",
    "updated_date": "2025-05-30 03:10:23 UTC"
  },
  {
    "arxiv_id": "2505.24157v1",
    "title": "Don't Just Follow MLLM Plans: Robust and Efficient Planning for Open-world Agents",
    "authors": [
      "Seungjoon Lee",
      "Suhwan Kim",
      "Minhyeon Oh",
      "Youngsik Yoon",
      "Jungseul Ok"
    ],
    "abstract": "Developing autonomous agents capable of mastering complex, multi-step tasks in unpredictable, interactive environments presents a significant challenge. While Large Language Models (LLMs) offer promise for planning, existing approaches often rely on problematic internal knowledge or make unrealistic environmental assumptions. Although recent work explores learning planning knowledge, they still retain limitations due to partial reliance on external knowledge or impractical setups. Indeed, prior research has largely overlooked developing agents capable of acquiring planning knowledge from scratch, directly in realistic settings. While realizing this capability is necessary, it presents significant challenges, primarily achieving robustness given the substantial risk of incorporating LLMs' inaccurate knowledge. Moreover, efficiency is crucial for practicality as learning can demand prohibitive exploration. In response, we introduce Robust and Efficient Planning for Open-world Agents (REPOA), a novel framework designed to tackle these issues. REPOA features three key components: adaptive dependency learning and fine-grained failure-aware operation memory to enhance robustness to knowledge inaccuracies, and difficulty-based exploration to improve learning efficiency. Our evaluation in two established open-world testbeds demonstrates REPOA's robust and efficient planning, showcasing its capability to successfully obtain challenging late-game items that were beyond the reach of prior approaches.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24157v1",
    "published_date": "2025-05-30 03:01:44 UTC",
    "updated_date": "2025-05-30 03:01:44 UTC"
  },
  {
    "arxiv_id": "2505.24149v3",
    "title": "RCCDA: Adaptive Model Updates in the Presence of Concept Drift under a Constrained Resource Budget",
    "authors": [
      "Adam Piaseczny",
      "Md Kamran Chowdhury Shisher",
      "Shiqiang Wang",
      "Christopher G. Brinton"
    ],
    "abstract": "Machine learning (ML) algorithms deployed in real-world environments are often faced with the challenge of adapting models to concept drift, where the task data distributions are shifting over time. The problem becomes even more difficult when model performance must be maintained under adherence to strict resource constraints. Existing solutions often depend on drift-detection methods that produce high computational overhead for resource-constrained environments, and fail to provide strict guarantees on resource usage or theoretical performance assurances. To address these shortcomings, we propose RCCDA: a dynamic model update policy that optimizes ML training dynamics while ensuring compliance to predefined resource constraints, utilizing only past loss information and a tunable drift threshold. In developing our policy, we analytically characterize the evolution of model loss under concept drift with arbitrary training update decisions. Integrating these results into a Lyapunov drift-plus-penalty framework produces a lightweight greedy-optimal policy that provably limits update frequency and cost. Experimental results on four domain generalization datasets demonstrate that our policy outperforms baseline methods in inference accuracy while adhering to strict resource constraints under several schedules of concept drift, making our solution uniquely suited for real-time ML deployments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.24149v3",
    "published_date": "2025-05-30 02:49:42 UTC",
    "updated_date": "2026-01-15 19:14:28 UTC"
  },
  {
    "arxiv_id": "2506.00081v2",
    "title": "Artificial Empathy: AI based Mental Health",
    "authors": [
      "Aditya Naik",
      "Jovi Thomas",
      "Teja Sree Mandava",
      "Himavanth Reddy Vemula"
    ],
    "abstract": "Many people suffer from mental health problems but not everyone seeks professional help or has access to mental health care. AI chatbots have increasingly become a go-to for individuals who either have mental disorders or simply want someone to talk to. This paper presents a study on participants who have previously used chatbots and a scenario-based testing of large language model (LLM) chatbots. Our findings indicate that AI chatbots were primarily utilized as a \"Five minute therapist\" or as a non-judgmental companion. Participants appreciated the anonymity and lack of judgment from chatbots. However, there were concerns about privacy and the security of sensitive information. The scenario-based testing of LLM chatbots highlighted additional issues. Some chatbots were consistently reassuring, used emojis and names to add a personal touch, and were quick to suggest seeking professional help. However, there were limitations such as inconsistent tone, occasional inappropriate responses (e.g., casual or romantic), and a lack of crisis sensitivity, particularly in recognizing red flag language and escalating responses appropriately. These findings can inform both the technology and mental health care industries on how to better utilize AI chatbots to support individuals during challenging emotional periods.",
    "categories": [
      "q-bio.OT",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "q-bio.OT",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.00081v2",
    "published_date": "2025-05-30 02:36:56 UTC",
    "updated_date": "2025-10-31 05:23:32 UTC"
  },
  {
    "arxiv_id": "2506.15704v1",
    "title": "Learn from the Past: Fast Sparse Indexing for Large Language Model Decoding",
    "authors": [
      "Feiyu Yao",
      "Qian Wang"
    ],
    "abstract": "As large language models (LLMs) continue to support increasingly longer contexts, the memory demand for key-value (KV) caches during decoding grows rapidly, becoming a critical bottleneck in both GPU memory capacity and PCIe bandwidth. Sparse attention mechanisms alleviate this issue by computing attention weights only for selected key-value pairs. However, their indexing computation typically requires traversing all key vectors, resulting in significant computational and data transfer overhead. To reduce the cost of index retrieval, existing methods often treat each decoding step as an independent process, failing to exploit the temporal correlations embedded in historical decoding information. To this end, we propose LFPS(Learn From the Past for Sparse Indexing), an acceleration method that dynamically constructs sparse indexing candidates based on historical attention patterns. LFPS captures two prevalent trends in decoder attention -vertical patterns (attending to fixed positions) and slash patterns (attending to relative positions) -and incorporates a positional expansion strategy to effectively predict the Top-k indices for the current step. We validate LFPS on challenging long-context benchmarks such as LongBench-RULER, using Llama-3.1-8B-Instruct as the base model. Experimental results show that LFPS achieves up to 22.8$\\times$ speedup over full attention and 9.6$\\times$ speedup over exact Top-k retrieval on an RTX 4090 GPU and a single CPU core of a Xeon Gold 6430, respectively, while preserving generation accuracy. These results demonstrate that LFPS offers a practical and efficient solution for decoding optimization in long-context LLM inference.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.15704v1",
    "published_date": "2025-05-30 02:35:59 UTC",
    "updated_date": "2025-05-30 02:35:59 UTC"
  },
  {
    "arxiv_id": "2506.03183v1",
    "title": "Edge Computing for Physics-Driven AI in Computational MRI: A Feasibility Study",
    "authors": [
      "Yaşar Utku Alçalar",
      "Yu Cao",
      "Mehmet Akçakaya"
    ],
    "abstract": "Physics-driven artificial intelligence (PD-AI) reconstruction methods have emerged as the state-of-the-art for accelerating MRI scans, enabling higher spatial and temporal resolutions. However, the high resolution of these scans generates massive data volumes, leading to challenges in transmission, storage, and real-time processing. This is particularly pronounced in functional MRI, where hundreds of volumetric acquisitions further exacerbate these demands. Edge computing with FPGAs presents a promising solution for enabling PD-AI reconstruction near the MRI sensors, reducing data transfer and storage bottlenecks. However, this requires optimization of PD-AI models for hardware efficiency through quantization and bypassing traditional FFT-based approaches, which can be a limitation due to their computational demands. In this work, we propose a novel PD-AI computational MRI approach optimized for FPGA-based edge computing devices, leveraging 8-bit complex data quantization and eliminating redundant FFT/IFFT operations. Our results show that this strategy improves computational efficiency while maintaining reconstruction quality comparable to conventional PD-AI methods, and outperforms standard clinical methods. Our approach presents an opportunity for high-resolution MRI reconstruction on resource-constrained devices, highlighting its potential for real-world deployment.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.AR",
      "cs.CV",
      "cs.LG",
      "physics.med-ph"
    ],
    "primary_category": "eess.IV",
    "comment": "IEEE International Conference on Future Internet of Things and Cloud (FiCloud), 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.03183v1",
    "published_date": "2025-05-30 02:35:43 UTC",
    "updated_date": "2025-05-30 02:35:43 UTC"
  },
  {
    "arxiv_id": "2505.24141v1",
    "title": "The Butterfly Effect in Pathology: Exploring Security in Pathology Foundation Models",
    "authors": [
      "Jiashuai Liu",
      "Yingjia Shang",
      "Yingkang Zhan",
      "Di Zhang",
      "Yi Niu",
      "Dong Wei",
      "Xian Wu",
      "Zeyu Gao",
      "Chen Li",
      "Yefeng Zheng"
    ],
    "abstract": "With the widespread adoption of pathology foundation models in both research and clinical decision support systems, exploring their security has become a critical concern. However, despite their growing impact, the vulnerability of these models to adversarial attacks remains largely unexplored. In this work, we present the first systematic investigation into the security of pathology foundation models for whole slide image~(WSI) analysis against adversarial attacks. Specifically, we introduce the principle of \\textit{local perturbation with global impact} and propose a label-free attack framework that operates without requiring access to downstream task labels. Under this attack framework, we revise four classical white-box attack methods and redefine the perturbation budget based on the characteristics of WSI. We conduct comprehensive experiments on three representative pathology foundation models across five datasets and six downstream tasks. Despite modifying only 0.1\\% of patches per slide with imperceptible noise, our attack leads to downstream accuracy degradation that can reach up to 20\\% in the worst cases. Furthermore, we analyze key factors that influence attack success, explore the relationship between patch-level vulnerability and semantic content, and conduct a preliminary investigation into potential defence strategies. These findings lay the groundwork for future research on the adversarial robustness and reliable deployment of pathology foundation models. Our code is publicly available at: https://github.com/Jiashuai-Liu-hmos/Attack-WSI-pathology-foundation-models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24141v1",
    "published_date": "2025-05-30 02:23:46 UTC",
    "updated_date": "2025-05-30 02:23:46 UTC"
  },
  {
    "arxiv_id": "2505.24139v2",
    "title": "S4-Driver: Scalable Self-Supervised Driving Multimodal Large Language Modelwith Spatio-Temporal Visual Representation",
    "authors": [
      "Yichen Xie",
      "Runsheng Xu",
      "Tong He",
      "Jyh-Jing Hwang",
      "Katie Luo",
      "Jingwei Ji",
      "Hubert Lin",
      "Letian Chen",
      "Yiren Lu",
      "Zhaoqi Leng",
      "Dragomir Anguelov",
      "Mingxing Tan"
    ],
    "abstract": "The latest advancements in multi-modal large language models (MLLMs) have spurred a strong renewed interest in end-to-end motion planning approaches for autonomous driving. Many end-to-end approaches rely on human annotations to learn intermediate perception and prediction tasks, while purely self-supervised approaches--which directly learn from sensor inputs to generate planning trajectories without human annotations often underperform the state of the art. We observe a key gap in the input representation space: end-to-end approaches built on MLLMs are often pretrained with reasoning tasks in 2D image space rather than the native 3D space in which autonomous vehicles plan. To this end, we propose S4-Driver, a scalable self-supervised motion planning algorithm with spatio-temporal visual representation, based on the popular PaLI multimodal large language model. S4-Driver uses a novel sparse volume strategy to seamlessly transform the strong visual representation of MLLMs from perspective view to 3D space without the need to finetune the vision encoder. This representation aggregates multi-view and multi-frame visual inputs and enables better prediction of planning trajectories in 3D space. To validate our method, we run experiments on both nuScenes and Waymo Open Motion Dataset (with in-house camera data). Results show that S4-Driver performs favorably against existing supervised multi-task approaches while requiring no human annotations. It also demonstrates great scalability when pretrained on large volumes of unannotated driving logs.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by CVPR2025; Project website: s4-driver.github.io",
    "pdf_url": "https://arxiv.org/pdf/2505.24139v2",
    "published_date": "2025-05-30 02:20:14 UTC",
    "updated_date": "2025-06-03 17:03:22 UTC"
  },
  {
    "arxiv_id": "2506.15703v1",
    "title": "Federated Incomplete Multi-view Clustering with Globally Fused Graph Guidance",
    "authors": [
      "Guoqing Chao",
      "Zhenghao Zhang",
      "Lei Meng",
      "Jie Wen",
      "Dianhui Chu"
    ],
    "abstract": "Federated multi-view clustering has been proposed to mine the valuable information within multi-view data distributed across different devices and has achieved impressive results while preserving the privacy. Despite great progress, most federated multi-view clustering methods only used global pseudo-labels to guide the downstream clustering process and failed to exploit the global information when extracting features. In addition, missing data problem in federated multi-view clustering task is less explored. To address these problems, we propose a novel Federated Incomplete Multi-view Clustering method with globally Fused Graph guidance (FIMCFG). Specifically, we designed a dual-head graph convolutional encoder at each client to extract two kinds of underlying features containing global and view-specific information. Subsequently, under the guidance of the fused graph, the two underlying features are fused into high-level features, based on which clustering is conducted under the supervision of pseudo-labeling. Finally, the high-level features are uploaded to the server to refine the graph fusion and pseudo-labeling computation. Extensive experimental results demonstrate the effectiveness and superiority of FIMCFG. Our code is publicly available at https://github.com/PaddiHunter/FIMCFG.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.15703v1",
    "published_date": "2025-05-30 02:17:51 UTC",
    "updated_date": "2025-05-30 02:17:51 UTC"
  },
  {
    "arxiv_id": "2505.24138v2",
    "title": "AMSbench: A Comprehensive Benchmark for Evaluating MLLM Capabilities in AMS Circuits",
    "authors": [
      "Yichen Shi",
      "Ze Zhang",
      "Hongyang Wang",
      "Zhuofu Tao",
      "Zhongyi Li",
      "Bingyu Chen",
      "Yaxin Wang",
      "Zhen huang",
      "Xuhua Liu",
      "Quan Chen",
      "Zhiping Yu",
      "Ting-Jung Lin",
      "Lei He"
    ],
    "abstract": "Analog/Mixed-Signal (AMS) circuits play a critical role in the integrated circuit (IC) industry. However, automating Analog/Mixed-Signal (AMS) circuit design has remained a longstanding challenge due to its difficulty and complexity. Although recent advances in Multi-modal Large Language Models (MLLMs) offer promising potential for supporting AMS circuit analysis and design, current research typically evaluates MLLMs on isolated tasks within the domain, lacking a comprehensive benchmark that systematically assesses model capabilities across diverse AMS-related challenges. To address this gap, we introduce AMSbench, a benchmark suite designed to evaluate MLLM performance across critical tasks including circuit schematic perception, circuit analysis, and circuit design. AMSbench comprises approximately 8000 test questions spanning multiple difficulty levels and assesses eight prominent models, encompassing both open-source and proprietary solutions such as Qwen 2.5-VL and Gemini 2.5 Pro. Our evaluation highlights significant limitations in current MLLMs, particularly in complex multi-modal reasoning and sophisticated circuit design tasks. These results underscore the necessity of advancing MLLMs' understanding and effective application of circuit-specific knowledge, thereby narrowing the existing performance gap relative to human expertise and moving toward fully automated AMS circuit design workflows. Our data is released at this URL.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24138v2",
    "published_date": "2025-05-30 02:17:45 UTC",
    "updated_date": "2025-10-13 08:48:52 UTC"
  },
  {
    "arxiv_id": "2505.24136v1",
    "title": "Sparsity-Driven Parallel Imaging Consistency for Improved Self-Supervised MRI Reconstruction",
    "authors": [
      "Yaşar Utku Alçalar",
      "Mehmet Akçakaya"
    ],
    "abstract": "Physics-driven deep learning (PD-DL) models have proven to be a powerful approach for improved reconstruction of rapid MRI scans. In order to train these models in scenarios where fully-sampled reference data is unavailable, self-supervised learning has gained prominence. However, its application at high acceleration rates frequently introduces artifacts, compromising image fidelity. To mitigate this shortcoming, we propose a novel way to train PD-DL networks via carefully-designed perturbations. In particular, we enhance the k-space masking idea of conventional self-supervised learning with a novel consistency term that assesses the model's ability to accurately predict the added perturbations in a sparse domain, leading to more reliable and artifact-free reconstructions. The results obtained from the fastMRI knee and brain datasets show that the proposed training strategy effectively reduces aliasing artifacts and mitigates noise amplification at high acceleration rates, outperforming state-of-the-art self-supervised methods both visually and quantitatively.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "physics.med-ph"
    ],
    "primary_category": "eess.IV",
    "comment": "IEEE International Conference on Image Processing (ICIP), 2025",
    "pdf_url": "https://arxiv.org/pdf/2505.24136v1",
    "published_date": "2025-05-30 02:11:25 UTC",
    "updated_date": "2025-05-30 02:11:25 UTC"
  },
  {
    "arxiv_id": "2505.24133v4",
    "title": "R-KV: Redundancy-aware KV Cache Compression for Reasoning Models",
    "authors": [
      "Zefan Cai",
      "Wen Xiao",
      "Hanshi Sun",
      "Cheng Luo",
      "Yikai Zhang",
      "Ke Wan",
      "Yucheng Li",
      "Yeyang Zhou",
      "Li-Wen Chang",
      "Jiuxiang Gu",
      "Zhen Dong",
      "Anima Anandkumar",
      "Abedelkadir Asi",
      "Junjie Hu"
    ],
    "abstract": "Reasoning models have demonstrated impressive performance in self-reflection and chain-of-thought reasoning. However, they often produce excessively long outputs, leading to prohibitively large key-value (KV) caches during inference. While chain-of-thought inference significantly improves performance on complex reasoning tasks, it can also lead to reasoning failures when deployed with existing KV cache compression approaches. To address this, we propose Redundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel method specifically targeting redundant tokens in reasoning models. Our method preserves nearly 100% of the full KV cache performance using only 10% of the KV cache, substantially outperforming existing KV cache baselines, which reach only 60% of the performance. Remarkably, R-KV even achieves 105% of full KV cache performance with 16% of the KV cache. This KV-cache reduction also leads to a 90% memory saving and a 6.6X throughput over standard chain-of-thought reasoning inference. Experimental results show that R-KV consistently outperforms existing KV cache compression baselines across two mathematical reasoning datasets.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24133v4",
    "published_date": "2025-05-30 02:03:24 UTC",
    "updated_date": "2026-01-22 01:31:36 UTC"
  },
  {
    "arxiv_id": "2506.15702v1",
    "title": "Minifinetuning: Low-Data Generation Domain Adaptation through Corrective Self-Distillation",
    "authors": [
      "Peter Belcak",
      "Greg Heinrich",
      "Jan Kautz",
      "Pavlo Molchanov"
    ],
    "abstract": "Finetuning language models for a new domain inevitably leads to the deterioration of their general performance. This becomes more pronounced the more limited the finetuning data resource.\n  We introduce minifinetuning (MFT), a method for language model domain adaptation that considerably reduces the effects of overfitting-induced degeneralization in low-data settings and which does so in the absence of any pre-training data for replay. MFT demonstrates 2-10x more favourable specialization-to-degeneralization ratios than standard finetuning across a wide range of models and domains and exhibits an intrinsic robustness to overfitting when data in the new domain is scarce and down to as little as 500 samples.\n  Employing corrective self-distillation that is individualized on the sample level, MFT outperforms parameter-efficient finetuning methods, demonstrates replay-like degeneralization mitigation properties, and is composable with either for a combined effect.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.15702v1",
    "published_date": "2025-05-30 01:54:12 UTC",
    "updated_date": "2025-05-30 01:54:12 UTC"
  },
  {
    "arxiv_id": "2505.24120v2",
    "title": "CSVQA: A Chinese Multimodal Benchmark for Evaluating STEM Reasoning Capabilities of VLMs",
    "authors": [
      "Ai Jian",
      "Weijie Qiu",
      "Xiaokun Wang",
      "Peiyu Wang",
      "Yunzhuo Hao",
      "Jiangbo Pei",
      "Yichen Wei",
      "Yi Peng",
      "Xuchen Song"
    ],
    "abstract": "Vision-Language Models (VLMs) have demonstrated remarkable progress in multimodal understanding, yet their capabilities for scientific reasoning remain inadequately assessed. Current multimodal benchmarks predominantly evaluate generic image comprehension or text-driven reasoning, lacking authentic scientific contexts that require domain-specific knowledge integration with visual evidence analysis. To fill this gap, we present CSVQA, a diagnostic multimodal benchmark specifically designed for evaluating scientific reasoning through domain-grounded visual question answering. Our benchmark features 1,378 carefully constructed question-answer pairs spanning diverse STEM disciplines, each demanding domain knowledge, integration of visual evidence, and higher-order reasoning. Compared to prior multimodal benchmarks, CSVQA places greater emphasis on real-world scientific content and complex reasoning. We additionally propose a rigorous evaluation protocol to systematically assess whether model predictions are substantiated by valid intermediate reasoning steps based on curated explanations. Our comprehensive evaluation of 15 VLMs on this benchmark reveals notable performance disparities, as even the top-ranked proprietary model attains only 49.6% accuracy. This empirical evidence underscores the pressing need for advancing scientific reasoning capabilities in VLMs. Our CSVQA is released at https://huggingface.co/datasets/Skywork/CSVQA",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "36 pages",
    "pdf_url": "https://arxiv.org/pdf/2505.24120v2",
    "published_date": "2025-05-30 01:34:25 UTC",
    "updated_date": "2025-06-17 01:56:01 UTC"
  },
  {
    "arxiv_id": "2506.00080v1",
    "title": "Bottom-Up Perspectives on AI Governance: Insights from User Reviews of AI Products",
    "authors": [
      "Stefan Pasch"
    ],
    "abstract": "With the growing importance of AI governance, numerous high-level frameworks and principles have been articulated by policymakers, institutions, and expert communities to guide the development and application of AI. While such frameworks offer valuable normative orientation, they may not fully capture the practical concerns of those who interact with AI systems in organizational and operational contexts. To address this gap, this study adopts a bottom-up approach to explore how governance-relevant themes are expressed in user discourse. Drawing on over 100,000 user reviews of AI products from G2.com, we apply BERTopic to extract latent themes and identify those most semantically related to AI governance. The analysis reveals a diverse set of governance-relevant topics spanning both technical and non-technical domains. These include concerns across organizational processes-such as planning, coordination, and communication-as well as stages of the AI value chain, including deployment infrastructure, data handling, and analytics. The findings show considerable overlap with institutional AI governance and ethics frameworks on issues like privacy and transparency, but also surface overlooked areas such as project management, strategy development, and customer interaction. This highlights the need for more empirically grounded, user-centered approaches to AI governance-approaches that complement normative models by capturing how governance unfolds in applied settings. By foregrounding how governance is enacted in practice, this study contributes to more inclusive and operationally grounded approaches to AI governance and digital policy.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.00080v1",
    "published_date": "2025-05-30 01:33:21 UTC",
    "updated_date": "2025-05-30 01:33:21 UTC"
  },
  {
    "arxiv_id": "2506.00079v1",
    "title": "Who Gets the Kidney? Human-AI Alignment, Indecision, and Moral Values",
    "authors": [
      "John P. Dickerson",
      "Hadi Hosseini",
      "Samarth Khanna",
      "Leona Pierce"
    ],
    "abstract": "The rapid integration of Large Language Models (LLMs) in high-stakes decision-making -- such as allocating scarce resources like donor organs -- raises critical questions about their alignment with human moral values. We systematically evaluate the behavior of several prominent LLMs against human preferences in kidney allocation scenarios and show that LLMs: i) exhibit stark deviations from human values in prioritizing various attributes, and ii) in contrast to humans, LLMs rarely express indecision, opting for deterministic decisions even when alternative indecision mechanisms (e.g., coin flipping) are provided. Nonetheless, we show that low-rank supervised fine-tuning with few samples is often effective in improving both decision consistency and calibrating indecision modeling. These findings illustrate the necessity of explicit alignment strategies for LLMs in moral/ethical domains.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.00079v1",
    "published_date": "2025-05-30 01:23:11 UTC",
    "updated_date": "2025-05-30 01:23:11 UTC"
  },
  {
    "arxiv_id": "2505.24099v2",
    "title": "Attractor learning for spatiotemporally chaotic dynamical systems using echo state networks with transfer learning",
    "authors": [
      "Mohammad Shah Alam",
      "William Ott",
      "Ilya Timofeyev"
    ],
    "abstract": "In this paper, we explore the predictive capabilities of echo state networks (ESNs) for the generalized Kuramoto-Sivashinsky (gKS) equation, an archetypal nonlinear PDE that exhibits spatiotemporal chaos. Our research focuses on predicting changes in long-term statistical patterns of the gKS model that result from varying the dispersion relation or the length of the spatial domain. We use transfer learning to adapt ESNs to different parameter settings and successfully capture changes in the underlying chaotic attractor. Previous work has shown that transfer learning can be used effectively with ESNs for single-orbit prediction. The novelty of our paper lies in our use of this pairing to predict the long-term statistical properties of spatiotemporally chaotic PDEs. We also show that transfer learning nontrivially improves the length of time that predictions of individual gKS trajectories remain accurate.",
    "categories": [
      "math.DS",
      "cs.AI",
      "cs.LG",
      "nlin.CD",
      "stat.ML"
    ],
    "primary_category": "math.DS",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24099v2",
    "published_date": "2025-05-30 01:01:09 UTC",
    "updated_date": "2025-12-21 23:02:09 UTC"
  },
  {
    "arxiv_id": "2505.24090v1",
    "title": "Searching Clinical Data Using Generative AI",
    "authors": [
      "Karan Hanswadkar",
      "Anika Kanchi",
      "Shivani Tripathi",
      "Shi Qiao",
      "Rony Chatterjee",
      "Alekh Jindal"
    ],
    "abstract": "Artificial Intelligence (AI) is making a major impact on healthcare, particularly through its application in natural language processing (NLP) and predictive analytics. The healthcare sector has increasingly adopted AI for tasks such as clinical data analysis and medical code assignment. However, searching for clinical information in large and often unorganized datasets remains a manual and error-prone process. Assisting this process with automations can help physicians improve their operational productivity significantly.\n  In this paper, we present a generative AI approach, coined SearchAI, to enhance the accuracy and efficiency of searching clinical data. Unlike traditional code assignment, which is a one-to-one problem, clinical data search is a one-to-many problem, i.e., a given search query can map to a family of codes. Healthcare professionals typically search for groups of related diseases, drugs, or conditions that map to many codes, and therefore, they need search tools that can handle keyword synonyms, semantic variants, and broad open-ended queries. SearchAI employs a hierarchical model that respects the coding hierarchy and improves the traversal of relationships from parent to child nodes. SearchAI navigates these hierarchies predictively and ensures that all paths are reachable without losing any relevant nodes.\n  To evaluate the effectiveness of SearchAI, we conducted a series of experiments using both public and production datasets. Our results show that SearchAI outperforms default hierarchical traversals across several metrics, including accuracy, robustness, performance, and scalability. SearchAI can help make clinical data more accessible, leading to streamlined workflows, reduced administrative burden, and enhanced coding and diagnostic accuracy.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2505.24090v1",
    "published_date": "2025-05-30 00:33:51 UTC",
    "updated_date": "2025-05-30 00:33:51 UTC"
  },
  {
    "arxiv_id": "2506.15701v1",
    "title": "Compiler-R1: Towards Agentic Compiler Auto-tuning with Reinforcement Learning",
    "authors": [
      "Haolin Pan",
      "Hongyu Lin",
      "Haoran Luo",
      "Yang Liu",
      "Kaichun Yao",
      "Libo Zhang",
      "Mingjie Xing",
      "Yanjun Wu"
    ],
    "abstract": "Compiler auto-tuning optimizes pass sequences to improve performance metrics such as Intermediate Representation (IR) instruction count. Although recent advances leveraging Large Language Models (LLMs) have shown promise in automating compiler tuning, two significant challenges still remain: the absence of high-quality reasoning datasets for agents training, and limited effective interactions with the compilation environment. In this work, we introduce Compiler-R1, the first reinforcement learning (RL)-driven framework specifically augmenting LLM capabilities for compiler auto-tuning. Compiler-R1 features a curated, high-quality reasoning dataset and a novel two-stage end-to-end RL training pipeline, enabling efficient environment exploration and learning through an outcome-based reward. Extensive experiments across seven datasets demonstrate Compiler-R1 achieving an average 8.46% IR instruction count reduction compared to opt -Oz, showcasing the strong potential of RL-trained LLMs for compiler optimization. Our code and datasets are publicly available at https://github.com/Panhaolin2001/Compiler-R1.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2506.15701v1",
    "published_date": "2025-05-30 00:26:10 UTC",
    "updated_date": "2025-05-30 00:26:10 UTC"
  }
]
{
  "date": "2024-11-17",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-11-17 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文主要聚焦于人工智能（AI）和机器学习的创新应用，包括大型语言模型（LLM）在代码生成、文本检测和多模态任务中的进展，以及在机器人、医疗和图像处理领域的实世界验证；亮点包括 RPN 2 统一多种神经网络架构的贡献，以及 AnyECG 在心脏监测的多任务分析上取得的提升，知名作者如 Martin Andrews 和 Jiawei Zhang 的作品值得关注。\n\n下面，我将挑选并简要讨论今天更重要的论文，先从 AI 和 LLM 相关主题入手，再扩展到机器人和医疗领域。对于其他较常规或不那么话题性的论文（如动漫图像生成或电力消费预测），我将快速掠过，只列出标题而不深究。每个条目会列出论文标题（中文 + 英文），并用简洁描述突出核心贡献、方法和发现。\n\n### 1. **Capturing Sparks of Abstraction for the ARC Challenge（捕捉抽象火花用于 ARC 挑战）**  \n   作者：Martin Andrews。这篇论文探讨了提升 ARC 挑战任务的准确率（超过 60%），通过 LLM 分析代码解决方案，提取抽象层级的理解（如代码重构和高阶策略），并开源了框架和数据；主要贡献是证明 LLM 可以生成可用于下游任务的抽象表示，提升了 AI 在复杂问题求解中的潜力。\n\n### 2. **On-Board Vision-Language Models for Personalized Autonomous Vehicle Motion Control: System Design and Real-World Validation（车载视觉语言模型用于个性化自主车辆运动控制：系统设计和真实世界验证）**  \n   这篇论文提出了一种轻量级车载 VLM 框架，使用 Retrieval-Augmented Generation（RAG）模块学习用户偏好，实现低延迟个性化驾驶；主要发现是通过真实车辆实验，减少了高达 76.9% 的接管率，显著提升了安全性和舒适性，是 AI 在自动驾驶领域的实用突破。\n\n### 3. **PickScan: Object discovery and reconstruction from handheld interactions（PickScan：通过手持交互进行物体发现和重建）**  \n   相关主题的另一篇，聚焦机器人和增强现实，论文引入基于物体位移的交互引导方法，实现无类别限制的 3D 对象重建；核心贡献是提高了重建精度（Chamfer 距离降低 73%），并在自定义数据集上达到 78.3% 的精确率，适用于复杂场景的物体扫描。\n\n### 9. **RPN 2: On Interdependence Function Learning Towards Unifying and Advancing CNN, RNN, GNN, and Transformer（RPN 2：通过互依赖函数学习统一和推进 CNN、RNN、GNN 和 Transformer）**  \n   作者：Jiawei Zhang。这篇令人印象深刻的论文扩展了原始 RPN 模型，通过显式建模数据互依赖，统一了 CNN、RNN、GNN 和 Transformer 等架构；主要发现是这种方法显著提升了学习性能，并为设计超越现有骨干网络的新架构提供了框架。\n\n### 15. **AnyECG: Foundational Models for Multitask Cardiac Analysis in Real-World Settings（AnyECG：用于真实世界多任务心脏分析的基础模型）**  \n   论文提出 AnyECG 模型，通过 ECG 分词器和预训练提取鲁棒表示，处理异质性和噪声数据；核心贡献是平均提升 6% 的任务性能（如心律失常检测），展示了在医疗领域的多模态学习潜力。\n\n### 16. **ModeSeq: Taming Sparse Multimodal Motion Prediction with Sequential Mode Modeling（ModeSeq：通过序列模式建模 tame 稀疏多模态运动预测）**  \n   这篇聚焦自动驾驶，引入序列模式预测方法，增强多模态轨迹多样性和准确性；主要发现是改进了运动预测基准的平衡性能，并支持模式外推，适用于高度不确定场景。\n\n### 24. **SRA-MCTS: Self-driven Reasoning Augmentation with Monte Carlo Tree Search for Code Generation（SRA-MCTS：使用 Monte Carlo 树搜索的自驱动推理增强用于代码生成）**  \n   论文开发了 SRA-MCTS 框架，通过自驱动生成高质量推理路径，提升 LLM 在复杂代码任务中的性能；核心贡献是无需额外监督就实现了性能提升，并在多样性指标如 pass@10 上表现出色。\n\n### 28. **BianCang: A Traditional Chinese Medicine Large Language Model（BianCang：一个传统中医大型语言模型）**  \n   这篇论文构建了 BianCang 模型，通过两阶段训练（知识注入和对齐）处理中医诊断；主要发现是模型在中医任务上超越了通用 LLM，并开源了数据集，展示了 AI 在传统医学的应用潜力。\n\n### 35. **SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization（SageAttention2：通过彻底异常值平滑和每线程 INT4 量化实现高效注意力机制）**  \n   论文优化了注意力计算，使用 INT4 量化结合平滑技术和双级累积策略；核心贡献是提升了运算速度（超过 FlashAttention2 3倍），并在语言和图像生成任务中保持高精度。\n\n其他论文快速掠过，以节省篇幅：\n- **F³OCUS: Federated Finetuning of Vision-Language Foundation Models with Optimal Client Layer Updating Strategy via Multi-objective Meta-Heuristics（F³OCUS：通过多目标元启发式优化客户端层更新的联邦微调视觉语言基础模型）**：提出层选择策略提升 VLM 联邦学习效率。\n- **Improving User Experience in Preference-Based Optimization of Reward Functions for Assistive Robots（提升基于偏好优化的奖励函数在辅助机器人中的用户体验）**：设计算法生成更直观的机器人轨迹，提升用户交互。\n- **AIGS: Generating Science from AI-Powered Automated Falsification（AIGS：从 AI 驱动的自动证伪生成科学）**：使用多代理系统自动生成科学发现，强调证伪在 AI 研究中的作用。\n- 第6篇 **Enhanced Anime Image Generation Using USE-CMHSA-GAN（使用 USE-CMHSA-GAN 增强动漫图像生成）**：改进 GAN 模型提升动漫图像质量，不太话题性。\n- 第19篇 **Back-filling Missing Data When Predicting Domestic Electricity Consumption From Smart Meter Data（从智能电表数据预测家庭电力消费时的缺失数据填充）**：提出数据填充方法优化电力预测，实用但常规。\n- 第23篇 **Reinforcing Competitive Multi-Agents for Playing So Long Sucker（增强竞争多代理用于 So Long Sucker 游戏）**：使用 DRL 训练游戏代理，趣味性强但学术影响有限。\n\n总之，今天的 arXiv 更新突显了 AI 在实际应用中的潜力，建议读者关注 LLM 和多模态模型的创新，以探索更广泛的领域影响。明天见！",
  "papers": [
    {
      "arxiv_id": "2411.11206v1",
      "title": "Capturing Sparks of Abstraction for the ARC Challenge",
      "title_zh": "翻译失败",
      "authors": [
        "Martin Andrews"
      ],
      "abstract": "Excellent progress has been made recently in solving ARC Challenge problems.\nHowever, it seems that new techniques may be required to push beyond 60%\naccuracy. Even commercial Large Language Models (LLMs) struggle to 'understand'\nmany of the problems (when given the input and output grids), which makes\ndiscovering solutions by LLM-lead program search somewhat futile.\n  In this work, LLM 'understanding' is attempted from a stronger starting\nposition : An LLM is given complete solutions to tasks in code, and then asked\nto explain how the task is being solved at various levels of abstraction.\nSpecifically, the LLM was given code solutions implemented in arc-dsl-llm (an\nLLM-legible version of Hodel's arc-dsl to obtain: (a) commented code; (b) code\nrefactored into reusable functional chunks; (c) problem solution steps; and (d)\nhigh-level problem-solving tactics.\n  We demonstrate that 'Sparks of Abstraction' can be extracted from the LLM\noutput - in a form that could be used in downstream tasks with Local LLMs\neligible to enter the ARC Prize.\n  Both the arc-dsl-llm DSL framework (with the re-engineered solutions) and the\nGemini LLM-generated data (along with the generation code) are made Open\nSource.",
      "tldr_zh": "本研究针对 ARC Challenge 问题，指出现有技术（如 LLMs）在超过 60% 准确率时面临理解困难，导致程序搜索无效。研究者提出一种方法，通过向 LLMs 提供完整的代码解决方案，并要求其在不同抽象水平（如添加注释、重构代码、解决方案步骤和高层策略）解释任务，从而提取 “Sparks of Abstraction”。实验结果显示，这种提取形式可用于下游任务，支持 Local LLMs 参与 ARC Prize；此外，arc-dsl-llm 框架、重新设计的解决方案和 Gemini LLM 生成数据均已开源，以促进进一步研究。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Submitted as a paper entry for the 2024 ARC Prize",
      "pdf_url": "http://arxiv.org/pdf/2411.11206v1",
      "published_date": "2024-11-17 23:40:00 UTC",
      "updated_date": "2024-11-17 23:40:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T01:17:21.635710"
    },
    {
      "arxiv_id": "2411.11913v1",
      "title": "On-Board Vision-Language Models for Personalized Autonomous Vehicle Motion Control: System Design and Real-World Validation",
      "title_zh": "翻译失败",
      "authors": [
        "Can Cui",
        "Zichong Yang",
        "Yupeng Zhou",
        "Juntong Peng",
        "Sung-Yeon Park",
        "Cong Zhang",
        "Yunsheng Ma",
        "Xu Cao",
        "Wenqian Ye",
        "Yiheng Feng",
        "Jitesh Panchal",
        "Lingxi Li",
        "Yaobin Chen",
        "Ziran Wang"
      ],
      "abstract": "Personalized driving refers to an autonomous vehicle's ability to adapt its\ndriving behavior or control strategies to match individual users' preferences\nand driving styles while maintaining safety and comfort standards. However,\nexisting works either fail to capture every individual preference precisely or\nbecome computationally inefficient as the user base expands. Vision-Language\nModels (VLMs) offer promising solutions to this front through their natural\nlanguage understanding and scene reasoning capabilities. In this work, we\npropose a lightweight yet effective on-board VLM framework that provides\nlow-latency personalized driving performance while maintaining strong reasoning\ncapabilities. Our solution incorporates a Retrieval-Augmented Generation\n(RAG)-based memory module that enables continuous learning of individual\ndriving preferences through human feedback. Through comprehensive real-world\nvehicle deployment and experiments, our system has demonstrated the ability to\nprovide safe, comfortable, and personalized driving experiences across various\nscenarios and significantly reduce takeover rates by up to 76.9%. To the best\nof our knowledge, this work represents the first end-to-end VLM-based motion\ncontrol system in real-world autonomous vehicles.",
      "tldr_zh": "该研究提出了一种轻量级车载 Vision-Language Models (VLMs) 框架，用于实现个性化自动驾驶控制，旨在精确适应用户偏好和驾驶风格，同时确保安全和舒适。框架整合了 Retrieval-Augmented Generation (RAG)-based 记忆模块，通过人类反馈实现持续学习，提供低延迟和高推理能力的个性化驾驶体验。在真实世界车辆部署实验中，该系统在多种场景下显著降低了接管率高达76.9%，并展示了安全、舒适的驾驶性能，是首个端到端 VLM-based 运动控制系统的实际应用。",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "arXiv admin note: substantial text overlap with arXiv:2410.15281",
      "pdf_url": "http://arxiv.org/pdf/2411.11913v1",
      "published_date": "2024-11-17 23:20:37 UTC",
      "updated_date": "2024-11-17 23:20:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T01:17:54.194522"
    },
    {
      "arxiv_id": "2411.11196v1",
      "title": "PickScan: Object discovery and reconstruction from handheld interactions",
      "title_zh": "翻译失败",
      "authors": [
        "Vincent van der Brugge",
        "Marc Pollefeys",
        "Joshua B. Tenenbaum",
        "Ayush Tewari",
        "Krishna Murthy Jatavallabhula"
      ],
      "abstract": "Reconstructing compositional 3D representations of scenes, where each object\nis represented with its own 3D model, is a highly desirable capability in\nrobotics and augmented reality. However, most existing methods rely heavily on\nstrong appearance priors for object discovery, therefore only working on those\nclasses of objects on which the method has been trained, or do not allow for\nobject manipulation, which is necessary to scan objects fully and to guide\nobject discovery in challenging scenarios. We address these limitations with a\nnovel interaction-guided and class-agnostic method based on object\ndisplacements that allows a user to move around a scene with an RGB-D camera,\nhold up objects, and finally outputs one 3D model per held-up object. Our main\ncontribution to this end is a novel approach to detecting user-object\ninteractions and extracting the masks of manipulated objects. On a\ncustom-captured dataset, our pipeline discovers manipulated objects with 78.3%\nprecision at 100% recall and reconstructs them with a mean chamfer distance of\n0.90cm. Compared to Co-Fusion, the only comparable interaction-based and\nclass-agnostic baseline, this corresponds to a reduction in chamfer distance of\n73% while detecting 99% fewer false positives.",
      "tldr_zh": "该论文提出PickScan，一种基于手持交互的物体发现和重建方法，使用RGB-D相机允许用户移动场景并拿起物体，从而生成每个物体的独立3D模型。该方法采用物体位移检测来识别用户-物体交互并提取被操作物体的掩码，实现了类别无关的物体发现和重建。在自定义数据集上，PickScan实现了78.3%的精度（在100%召回率下），重建的平均Chamfer distance为0.90cm，并与Co-Fusion基线相比，降低了73%的Chamfer distance和99%的假阳性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR",
        "cs.LG",
        "cs.RO",
        "I.4.5"
      ],
      "primary_category": "cs.CV",
      "comment": "7 pages, 8 figures, published in the 2024 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS 2024)",
      "pdf_url": "http://arxiv.org/pdf/2411.11196v1",
      "published_date": "2024-11-17 23:09:08 UTC",
      "updated_date": "2024-11-17 23:09:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T01:17:53.920005"
    },
    {
      "arxiv_id": "2411.11912v2",
      "title": "F$^3$OCUS -- Federated Finetuning of Vision-Language Foundation Models with Optimal Client Layer Updating Strategy via Multi-objective Meta-Heuristics",
      "title_zh": "翻译失败",
      "authors": [
        "Pramit Saha",
        "Felix Wagner",
        "Divyanshu Mishra",
        "Can Peng",
        "Anshul Thakur",
        "David Clifton",
        "Konstantinos Kamnitsas",
        "J. Alison Noble"
      ],
      "abstract": "Effective training of large Vision-Language Models (VLMs) on\nresource-constrained client devices in Federated Learning (FL) requires the\nusage of parameter-efficient fine-tuning (PEFT) strategies. To this end, we\ndemonstrate the impact of two factors \\textit{viz.}, client-specific layer\nimportance score that selects the most important VLM layers for fine-tuning and\ninter-client layer diversity score that encourages diverse layer selection\nacross clients for optimal VLM layer selection. We first theoretically motivate\nand leverage the principal eigenvalue magnitude of layerwise Neural Tangent\nKernels and show its effectiveness as client-specific layer importance score.\nNext, we propose a novel layer updating strategy dubbed F$^3$OCUS that jointly\noptimizes the layer importance and diversity factors by employing a data-free,\nmulti-objective, meta-heuristic optimization on the server. We explore 5\ndifferent meta-heuristic algorithms and compare their effectiveness for\nselecting model layers and adapter layers towards PEFT-FL. Furthermore, we\nrelease a new MedVQA-FL dataset involving overall 707,962 VQA triplets and 9\nmodality-specific clients and utilize it to train and evaluate our method.\nOverall, we conduct more than 10,000 client-level experiments on 6\nVision-Language FL task settings involving 58 medical image datasets and 4\ndifferent VLM architectures of varying sizes to demonstrate the effectiveness\nof the proposed method.",
      "tldr_zh": "本研究提出F³OCUS策略，用于在联邦学习(FL)中优化视觉语言模型(VLMs)的参数高效微调(PEFT)，通过结合客户端特定的层重要性分数（基于层级神经切线核的主特征值）和客户端间层多样性分数来选择最优层。F³OCUS在服务器上采用无数据多目标元启发式优化算法（如探索的5种算法）来联合优化这些因素，从而提升VLMs在资源受限设备上的训练效率。该方法在新的MedVQA-FL数据集（包含707,962个VQA三元组和9个模态特定客户端）上进行了超过10,000个实验，证明了其在6个FL任务设置和58个医疗图像数据集中的有效性，展示了显著的性能提升。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted in CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2411.11912v2",
      "published_date": "2024-11-17 21:54:57 UTC",
      "updated_date": "2025-03-30 10:30:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T01:17:59.500943"
    },
    {
      "arxiv_id": "2411.11182v1",
      "title": "Improving User Experience in Preference-Based Optimization of Reward Functions for Assistive Robots",
      "title_zh": "翻译失败",
      "authors": [
        "Nathaniel Dennler",
        "Zhonghao Shi",
        "Stefanos Nikolaidis",
        "Maja Matarić"
      ],
      "abstract": "Assistive robots interact with humans and must adapt to different users'\npreferences to be effective. An easy and effective technique to learn\nnon-expert users' preferences is through rankings of robot behaviors, for\nexample, robot movement trajectories or gestures. Existing techniques focus on\ngenerating trajectories for users to rank that maximize the outcome of the\npreference learning process. However, the generated trajectories do not appear\nto reflect the user's preference over repeated interactions. In this work, we\ndesign an algorithm to generate trajectories for users to rank that we call\nCovariance Matrix Adaptation Evolution Strategies with Information Gain\n(CMA-ES-IG). CMA-ES-IG prioritizes the user's experience of the preference\nlearning process. We show that users find our algorithm more intuitive and\neasier to use than previous approaches across both physical and social robot\ntasks. This project's code is hosted at github.com/interaction-lab/CMA-ES-IG",
      "tldr_zh": "这篇论文针对辅助机器人通过用户偏好排名（如机器人轨迹或手势）来优化奖励函数的问题，提出了一种新算法Covariance Matrix Adaptation Evolution Strategies with Information Gain (CMA-ES-IG)。该算法在生成用户排名轨迹时，优先考虑用户体验，以改善偏好学习过程的直观性和易用性。实验结果显示，CMA-ES-IG在物理和社交机器人任务中比现有方法更受用户欢迎，并提供了开源代码以便进一步应用。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted to ISRR",
      "pdf_url": "http://arxiv.org/pdf/2411.11182v1",
      "published_date": "2024-11-17 21:52:58 UTC",
      "updated_date": "2024-11-17 21:52:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T01:18:09.883902"
    },
    {
      "arxiv_id": "2411.11179v1",
      "title": "Enhanced Anime Image Generation Using USE-CMHSA-GAN",
      "title_zh": "利用 USE-CMHSA-GAN 增强动漫图像生成",
      "authors": [
        "J. Lu"
      ],
      "abstract": "With the growing popularity of ACG (Anime, Comics, and Games) culture,\ngenerating high-quality anime character images has become an important research\ntopic. This paper introduces a novel Generative Adversarial Network model,\nUSE-CMHSA-GAN, designed to produce high-quality anime character images. The\nmodel builds upon the traditional DCGAN framework, incorporating USE and CMHSA\nmodules to enhance feature extraction capabilities for anime character images.\nExperiments were conducted on the anime-face-dataset, and the results\ndemonstrate that USE-CMHSA-GAN outperforms other benchmark models, including\nDCGAN, VAE-GAN, and WGAN, in terms of FID and IS scores, indicating superior\nimage quality. These findings suggest that USE-CMHSA-GAN is highly effective\nfor anime character image generation and provides new insights for further\nimproving the quality of generative models.",
      "tldr_zh": "这篇论文提出了一种新型生成对抗网络模型 USE-CMHSA-GAN，用于提升动漫角色图像的生成质量，以满足 ACG 文化日益增长的需求。  \n该模型基于传统 DCGAN 框架，引入 USE 和 CMHSA 模块来增强特征提取能力，从而更好地处理动漫图像的独特特性。  \n在 anime-face-dataset 上进行的实验表明，USE-CMHSA-GAN 在 FID 和 IS 指标上优于 DCGAN、VAE-GAN 和 WGAN 等基准模型，证明了其在图像质量方面的显著优势。  \n这些结果为动漫图像生成提供了新方法，并为进一步优化生成模型带来了有益见解。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.11179v1",
      "published_date": "2024-11-17 21:25:24 UTC",
      "updated_date": "2024-11-17 21:25:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T01:18:22.279932"
    },
    {
      "arxiv_id": "2411.11171v2",
      "title": "LLäMmlein: Compact and Competitive German-Only Language Models from Scratch",
      "title_zh": "翻译失败",
      "authors": [
        "Jan Pfister",
        "Julia Wunderle",
        "Andreas Hotho"
      ],
      "abstract": "We create two German-only decoder models, LL\\\"aMmlein 120M and 1B,\ntransparently from scratch and publish them, along with the training data, for\nthe German NLP research community to use. The model training involved several\nkey steps, including extensive data preprocessing, the creation of a custom\nGerman tokenizer, the training itself, as well as the evaluation of the final\nmodels on various benchmarks. Throughout the training process, multiple\ncheckpoints were saved and analyzed using the SuperGLEBer benchmark to monitor\nthe models' learning dynamics. Compared to state-of-the-art models on the\nSuperGLEBer benchmark, both LL\\\"aMmlein models performed competitively,\nconsistently matching or surpassing models with similar parameter sizes. The\nresults show that the models' quality scales with size as expected, but\nperformance improvements on some tasks plateaued early, offering valuable\ninsights into resource allocation for future model development.",
      "tldr_zh": "该研究从零开始开发了两个紧凑且竞争力的德语专属解码器模型（LLäMmlein 120M 和 1B），并公开了模型及其训练数据，以支持德语自然语言处理社区。训练过程涉及数据预处理、自定义德语 tokenizer、模型训练以及使用 SuperGLEBer benchmark 监控学习动态和评估性能。结果显示，LLäMmlein 模型在 SuperGLEBer benchmark 上与类似参数规模的模型匹敌或超越，模型质量随规模增加，但某些任务的性能改进早早达到平台期，为未来资源分配提供了宝贵洞见。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "second draft;\n  https://www.informatik.uni-wuerzburg.de/datascience/projects/nlp/llammlein/",
      "pdf_url": "http://arxiv.org/pdf/2411.11171v2",
      "published_date": "2024-11-17 20:44:34 UTC",
      "updated_date": "2024-12-16 12:29:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T01:18:34.243600"
    },
    {
      "arxiv_id": "2411.12764v1",
      "title": "SEFD: Semantic-Enhanced Framework for Detecting LLM-Generated Text",
      "title_zh": "翻译失败",
      "authors": [
        "Weiqing He",
        "Bojian Hou",
        "Tianqi Shang",
        "Davoud Ataee Tarzanagh",
        "Qi Long",
        "Li Shen"
      ],
      "abstract": "The widespread adoption of large language models (LLMs) has created an urgent\nneed for robust tools to detect LLM-generated text, especially in light of\n\\textit{paraphrasing} techniques that often evade existing detection methods.\nTo address this challenge, we present a novel semantic-enhanced framework for\ndetecting LLM-generated text (SEFD) that leverages a retrieval-based mechanism\nto fully utilize text semantics. Our framework improves upon existing detection\nmethods by systematically integrating retrieval-based techniques with\ntraditional detectors, employing a carefully curated retrieval mechanism that\nstrikes a balance between comprehensive coverage and computational efficiency.\nWe showcase the effectiveness of our approach in sequential text scenarios\ncommon in real-world applications, such as online forums and Q\\&A platforms.\nThrough comprehensive experiments across various LLM-generated texts and\ndetection methods, we demonstrate that our framework substantially enhances\ndetection accuracy in paraphrasing scenarios while maintaining robustness for\nstandard LLM-generated content.",
      "tldr_zh": "本研究针对大型语言模型（LLM）生成文本的检测挑战，特别是paraphrasing技术对现有方法的规避，提出了一种语义增强框架SEFD。SEFD通过检索-based机制充分利用文本语义，将其与传统检测器系统整合，实现全面覆盖与计算效率的平衡。实验结果显示，该框架在paraphrasing场景中显著提升检测准确性，同时对标准LLM生成内容保持稳健表现，尤其适用于在线论坛和Q&A平台的真实序列文本场景。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.12764v1",
      "published_date": "2024-11-17 20:13:30 UTC",
      "updated_date": "2024-11-17 20:13:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T01:18:45.070234"
    },
    {
      "arxiv_id": "2411.11162v1",
      "title": "RPN 2: On Interdependence Function Learning Towards Unifying and Advancing CNN, RNN, GNN, and Transformer",
      "title_zh": "翻译失败",
      "authors": [
        "Jiawei Zhang"
      ],
      "abstract": "This paper builds upon our previous work on the Reconciled Polynomial Network\n(RPN). The original RPN model was designed under the assumption of input data\nindependence, presuming the independence among both individual instances within\ndata batches and attributes in each data instance. However, this assumption\noften proves invalid for function learning tasks involving complex,\ninterdependent data such as language, images, time series, and graphs. Ignoring\nsuch data interdependence may inevitably lead to significant performance\ndegradation.\n  To overcome these limitations, we introduce the new Reconciled Polynomial\nNetwork (version 2), namely RPN 2, in this paper. By incorporating data and\nstructural interdependence functions, RPN 2 explicitly models data\ninterdependence via new component functions in its architecture.\n  This enhancement not only significantly improves RPN 2's learning performance\nbut also substantially expands its unifying potential, enabling it to encompass\na broader range of contemporary dominant backbone models within its canonical\nrepresentation. These backbones include, but are not limited to, convolutional\nneural networks (CNNs), recurrent neural networks (RNNs), graph neural networks\n(GNNs), and Transformers. Our analysis reveals that the fundamental\ndistinctions among these backbone models primarily stem from their diverse\napproaches to defining the interdependence functions. Furthermore, this unified\nrepresentation opens up new opportunities for designing innovative\narchitectures with the potential to surpass the performance of these dominant\nbackbones.",
      "tldr_zh": "本文基于之前的 Reconciled Polynomial Network (RPN) 模型，引入 RPN 2 以解决输入数据互依赖问题，因为原模型假设数据独立，这在处理语言、图像、时间序列和图等复杂数据时会导致性能下降。RPN 2 通过添加数据和结构互依赖函数到其架构中，显式建模这些互依赖，从而显著提升学习性能，并将 CNN、RNN、GNN 和 Transformer 等骨干模型统一到一个规范表示框架内。分析表明，这些模型的核心差异在于互依赖函数的定义，这为设计超越现有模型的新型架构提供了创新机会。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.IT",
        "math.IT",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "105 pages, 37 figures, 6 tables, preprint version",
      "pdf_url": "http://arxiv.org/pdf/2411.11162v1",
      "published_date": "2024-11-17 19:45:26 UTC",
      "updated_date": "2024-11-17 19:45:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T01:18:57.902732"
    },
    {
      "arxiv_id": "2411.11161v1",
      "title": "MPLite: Multi-Aspect Pretraining for Mining Clinical Health Records",
      "title_zh": "翻译失败",
      "authors": [
        "Eric Yang",
        "Pengfei Hu",
        "Xiaoxue Han",
        "Yue Ning"
      ],
      "abstract": "The adoption of digital systems in healthcare has resulted in the\naccumulation of vast electronic health records (EHRs), offering valuable data\nfor machine learning methods to predict patient health outcomes. However,\nsingle-visit records of patients are often neglected in the training process\ndue to the lack of annotations of next-visit information, thereby limiting the\npredictive and expressive power of machine learning models. In this paper, we\npresent a novel framework MPLite that utilizes Multi-aspect Pretraining with\nLab results through a light-weight neural network to enhance medical concept\nrepresentation and predict future health outcomes of individuals. By\nincorporating both structured medical data and additional information from lab\nresults, our approach fully leverages patient admission records. We design a\npretraining module that predicts medical codes based on lab results, ensuring\nrobust prediction by fusing multiple aspects of features. Our experimental\nevaluation using both MIMIC-III and MIMIC-IV datasets demonstrates improvements\nover existing models in diagnosis prediction and heart failure prediction\ntasks, achieving a higher weighted-F1 and recall with MPLite. This work reveals\nthe potential of integrating diverse aspects of data to advance predictive\nmodeling in healthcare.",
      "tldr_zh": "该研究针对电子健康记录(EHRs)中单次就诊记录的利用不足问题，提出了一种轻量级框架MPLite，通过多方面预训练整合结构化医疗数据和实验室结果，来增强医疗概念表示并预测患者未来健康结果。MPLite的设计包括一个预训练模块，利用实验室结果预测医疗代码，并融合多方面特征以提高模型的预测鲁棒性。在MIMIC-III和MIMIC-IV数据集上的实验表明，该框架在诊断预测和心力衰竭预测任务中，相比现有模型实现了更高的加权F1分数和召回率，展示了整合多样数据方面潜力以推进医疗预测建模。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.11161v1",
      "published_date": "2024-11-17 19:43:10 UTC",
      "updated_date": "2024-11-17 19:43:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T01:19:10.066926"
    },
    {
      "arxiv_id": "2411.11148v1",
      "title": "TabDeco: A Comprehensive Contrastive Framework for Decoupled Representations in Tabular Data",
      "title_zh": "翻译失败",
      "authors": [
        "Suiyao Chen",
        "Jing Wu",
        "Yunxiao Wang",
        "Cheng Ji",
        "Tianpei Xie",
        "Daniel Cociorva",
        "Michael Sharps",
        "Cecile Levasseur",
        "Hakan Brunzell"
      ],
      "abstract": "Representation learning is a fundamental aspect of modern artificial\nintelligence, driving substantial improvements across diverse applications.\nWhile selfsupervised contrastive learning has led to significant advancements\nin fields like computer vision and natural language processing, its adaptation\nto tabular data presents unique challenges. Traditional approaches often\nprioritize optimizing model architecture and loss functions but may overlook\nthe crucial task of constructing meaningful positive and negative sample pairs\nfrom various perspectives like feature interactions, instance-level patterns\nand batch-specific contexts. To address these challenges, we introduce TabDeco,\na novel method that leverages attention-based encoding strategies across both\nrows and columns and employs contrastive learning framework to effectively\ndisentangle feature representations at multiple levels, including features,\ninstances and data batches. With the innovative feature decoupling hierarchies,\nTabDeco consistently surpasses existing deep learning methods and leading\ngradient boosting algorithms, including XG-Boost, CatBoost, and LightGBM,\nacross various benchmark tasks, underscoring its effectiveness in advancing\ntabular data representation learning.",
      "tldr_zh": "该论文提出了 TabDeco，一种全面的对比学习框架，用于表格式数据的表示学习，以解决构建有意义的正负样本对（如特征交互、实例模式和批次上下文）的挑战。TabDeco 采用 attention-based 编码策略处理行和列，并通过对比学习框架实现多层次的特征解耦，包括 features、instances 和 data batches。实验结果表明，TabDeco 在各种基准任务上超过了现有深度学习方法和梯度提升算法如 XGBoost、CatBoost 和 LightGBM，展示了其在表格式数据表示学习中的显著优势。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.11148v1",
      "published_date": "2024-11-17 18:42:46 UTC",
      "updated_date": "2024-11-17 18:42:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T01:19:21.970136"
    },
    {
      "arxiv_id": "2412.03575v1",
      "title": "Leveraging Large Language Models for Generating Labeled Mineral Site Record Linkage Data",
      "title_zh": "翻译失败",
      "authors": [
        "Jiyoon Pyo",
        "Yao-Yi Chiang"
      ],
      "abstract": "Record linkage integrates diverse data sources by identifying records that\nrefer to the same entity. In the context of mineral site records, accurate\nrecord linkage is crucial for identifying and mapping mineral deposits.\nProperly linking records that refer to the same mineral deposit helps define\nthe spatial coverage of mineral areas, benefiting resource identification and\nsite data archiving. Mineral site record linkage falls under the spatial record\nlinkage category since the records contain information about the physical\nlocations and non-spatial attributes in a tabular format. The task is\nparticularly challenging due to the heterogeneity and vast scale of the data.\nWhile prior research employs pre-trained discriminative language models (PLMs)\non spatial entity linkage, they often require substantial amounts of curated\nground-truth data for fine-tuning. Gathering and creating ground truth data is\nboth time-consuming and costly. Therefore, such approaches are not always\nfeasible in real-world scenarios where gold-standard data are unavailable.\nAlthough large generative language models (LLMs) have shown promising results\nin various natural language processing tasks, including record linkage, their\nhigh inference time and resource demand present challenges. We propose a method\nthat leverages an LLM to generate training data and fine-tune a PLM to address\nthe training data gap while preserving the efficiency of PLMs. Our approach\nachieves over 45\\% improvement in F1 score for record linkage compared to\ntraditional PLM-based methods using ground truth data while reducing the\ninference time by nearly 18 times compared to relying on LLMs. Additionally, we\noffer an automated pipeline that eliminates the need for human intervention,\nhighlighting this approach's potential to overcome record linkage challenges.",
      "tldr_zh": "这篇论文探讨了利用大型语言模型(LLMs)生成标注数据来解决矿产站点记录链接(record linkage)的挑战，该任务涉及整合异质数据源以识别同一矿产沉积物的记录。作者提出了一种方法，使用LLMs自动生成训练数据，然后微调预训练判别语言模型(PLMs)，以克服传统方法对大量标注数据的依赖。实验结果显示，该方法在F1分数上比传统PLM方法提高了超过45%，同时将推理时间减少近18倍，并提供了一个无需人工干预的自动化管道，从而提升了记录链接的效率和可行性。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "11 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.03575v1",
      "published_date": "2024-11-17 18:26:56 UTC",
      "updated_date": "2024-11-17 18:26:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T01:19:33.931055"
    },
    {
      "arxiv_id": "2411.11144v1",
      "title": "CLMIA: Membership Inference Attacks via Unsupervised Contrastive Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Depeng Chen",
        "Xiao Liu",
        "Jie Cui",
        "Hong Zhong"
      ],
      "abstract": "Since machine learning model is often trained on a limited data set, the\nmodel is trained multiple times on the same data sample, which causes the model\nto memorize most of the training set data. Membership Inference Attacks (MIAs)\nexploit this feature to determine whether a data sample is used for training a\nmachine learning model. However, in realistic scenarios, it is difficult for\nthe adversary to obtain enough qualified samples that mark accurate identity\ninformation, especially since most samples are non-members in real world\napplications. To address this limitation, in this paper, we propose a new\nattack method called CLMIA, which uses unsupervised contrastive learning to\ntrain an attack model without using extra membership status information.\nMeanwhile, in CLMIA, we require only a small amount of data with known\nmembership status to fine-tune the attack model. Experimental results\ndemonstrate that CLMIA performs better than existing attack methods for\ndifferent datasets and model structures, especially with data with less marked\nidentity information. In addition, we experimentally find that the attack\nperforms differently for different proportions of labeled identity information\nfor member and non-member data. More analysis proves that our attack method\nperforms better with less labeled identity information, which applies to more\nrealistic scenarios.",
      "tldr_zh": "本文提出 CLMIA，一种基于无监督对比学习(Unsupervised Contrastive Learning)的 Membership Inference Attacks (MIAs) 方法，用于判断数据样本是否用于训练机器学习模型，而无需额外成员状态信息，仅需少量已知成员数据的微调。CLMIA 通过这种方式解决了现实场景中标记身份信息不足的问题，并在不同数据集和模型结构上比现有方法表现出色，尤其在标记信息较少时准确率更高。实验结果表明，攻击效果随成员和非成员数据标记比例变化，且 CLMIA 在标记信息较少的情况下更具优势，更适用于实际应用场景。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.11144v1",
      "published_date": "2024-11-17 18:25:01 UTC",
      "updated_date": "2024-11-17 18:25:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T01:19:46.795674"
    },
    {
      "arxiv_id": "2411.13585v1",
      "title": "Artificial Intelligence in Cybersecurity: Building Resilient Cyber Diplomacy Frameworks",
      "title_zh": "人工智能在网络安全中：构建弹性网络外交框架",
      "authors": [
        "Michael Stoltz"
      ],
      "abstract": "This paper explores how automation and artificial intelligence (AI) are\ntransforming U.S. cyber diplomacy. Leveraging these technologies helps the U.S.\nmanage the complexity and urgency of cyber diplomacy, improving\ndecision-making, efficiency, and security. As global inter connectivity grows,\ncyber diplomacy, managing national interests in the digital space has become\nvital. The ability of AI and automation to quickly process vast data volumes\nenables timely responses to cyber threats and opportunities. This paper\nunderscores the strategic integration of these tools to maintain U.S.\ncompetitive advantage and secure national interests. Automation enhances\ndiplomatic communication and data processing, freeing diplomats to focus on\nstrategic decisions. AI supports predictive analytics and real time decision\nmaking, offering critical insights and proactive measures during high stakes\nengagements. Case studies show AIs effectiveness in monitoring cyber activities\nand managing international cyber policy. Challenges such as ethical concerns,\nsecurity vulnerabilities, and reliance on technology are also addressed,\nemphasizing human oversight and strong governance frameworks. Ensuring proper\nethical guidelines and cybersecurity measures allows the U.S. to harness the\nbenefits of automation and AI while mitigating risks. By adopting these\ntechnologies, U.S. cyber diplomacy can become more proactive and effective,\nnavigating the evolving digital landscape with greater agility.",
      "tldr_zh": "这篇论文探讨了人工智能（AI）和自动化在提升美国网络外交（cyber diplomacy）中的作用，通过快速处理海量数据和预测分析，帮助实现及时响应网络威胁，提高决策效率和国家安全。随着全球互联性增强，AI的战略整合可增强外交沟通并维持美国竞争优势，但需应对伦理问题、安全漏洞和对技术的依赖。论文通过案例研究证明了AI在监控网络活动和国际政策管理中的有效性，并强调通过人类监督和强有力的治理框架来减轻风险，从而构建更具韧性的网络外交框架。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.13585v1",
      "published_date": "2024-11-17 17:57:17 UTC",
      "updated_date": "2024-11-17 17:57:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T01:19:58.378315"
    },
    {
      "arxiv_id": "2411.17711v2",
      "title": "AnyECG: Foundational Models for Multitask Cardiac Analysis in Real-World Settings",
      "title_zh": "AnyECG：真实世界场景下多任务心脏分析的基础模型",
      "authors": [
        "Yue Wang",
        "Xu Cao",
        "Yaojun Hu",
        "Haochao Ying",
        "Hongxia Xu",
        "Ruijia Wu",
        "James Matthew Rehg",
        "Jimeng Sun",
        "Jian Wu",
        "Jintai Chen"
      ],
      "abstract": "Electrocardiogram (ECG), a non-invasive and affordable tool for cardiac\nmonitoring, is highly sensitive in detecting acute heart attacks. However, due\nto the lengthy nature of ECG recordings, numerous machine learning methods have\nbeen developed for automated heart disease detection to reduce human workload.\nDespite these efforts, performance remains suboptimal. A key obstacle is the\ninherent complexity of ECG data, which includes heterogeneity (e.g., varying\nsampling rates), high levels of noise, demographic-related pattern shifts, and\nintricate rhythm-event associations. To overcome these challenges, this paper\nintroduces AnyECG, a foundational model designed to extract robust\nrepresentations from any real-world ECG data. Specifically, a tailored ECG\nTokenizer encodes each fixed-duration ECG fragment into a token and, guided by\nproxy tasks, converts noisy, continuous ECG features into discrete, compact,\nand clinically meaningful local rhythm codes. These codes encapsulate basic\nmorphological, frequency, and demographic information (e.g., sex), effectively\nmitigating signal noise. We further pre-train the AnyECG to learn rhythmic\npattern associations across ECG tokens, enabling the capture of cardiac event\nsemantics. By being jointly pre-trained on diverse ECG data sources, AnyECG is\ncapable of generalizing across a wide range of downstream tasks where ECG\nsignals are recorded from various devices and scenarios. The experimental\nresults show that AnyECG achieves an average performance improvement of 6%\nacross four critical tasks-anomaly detection, arrhythmia classification,\ncorrupted lead generation, and ultra-long ECG recognition. AnyECG learns common\nECG rhythm from data and significantly outperforms state-of-the-art methods in\neach of these tasks.",
      "tldr_zh": "这篇论文针对 ECG 数据中的复杂性（如异质性、噪声和人口统计模式变化），引入了 AnyECG 基础模型，用于多任务心脏分析以适应真实世界场景。AnyECG 采用定制的 ECG Tokenizer 将固定时长的 ECG 片段编码成离散 token，通过代理任务和预训练学习节奏模式关联，从而提取鲁棒的临床含义表示，并缓解信号噪声。实验结果表明，AnyECG 在异常检测、心律失常分类、受损导联生成和超长 ECG 识别等四个关键任务上平均性能提升 6%，显著优于现有方法。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.17711v2",
      "published_date": "2024-11-17 17:32:58 UTC",
      "updated_date": "2025-03-03 13:19:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T01:20:10.755297"
    },
    {
      "arxiv_id": "2411.11911v2",
      "title": "ModeSeq: Taming Sparse Multimodal Motion Prediction with Sequential Mode Modeling",
      "title_zh": "翻译失败",
      "authors": [
        "Zikang Zhou",
        "Hengjian Zhou",
        "Haibo Hu",
        "Zihao Wen",
        "Jianping Wang",
        "Yung-Hui Li",
        "Yu-Kai Huang"
      ],
      "abstract": "Anticipating the multimodality of future events lays the foundation for safe\nautonomous driving. However, multimodal motion prediction for traffic agents\nhas been clouded by the lack of multimodal ground truth. Existing works\npredominantly adopt the winner-take-all training strategy to tackle this\nchallenge, yet still suffer from limited trajectory diversity and uncalibrated\nmode confidence. While some approaches address these limitations by generating\nexcessive trajectory candidates, they necessitate a post-processing stage to\nidentify the most representative modes, a process lacking universal principles\nand compromising trajectory accuracy. We are thus motivated to introduce\nModeSeq, a new multimodal prediction paradigm that models modes as sequences.\nUnlike the common practice of decoding multiple plausible trajectories in one\nshot, ModeSeq requires motion decoders to infer the next mode step by step,\nthereby more explicitly capturing the correlation between modes and\nsignificantly enhancing the ability to reason about multimodality. Leveraging\nthe inductive bias of sequential mode prediction, we also propose the\nEarly-Match-Take-All (EMTA) training strategy to diversify the trajectories\nfurther. Without relying on dense mode prediction or heuristic post-processing,\nModeSeq considerably improves the diversity of multimodal output while\nattaining satisfactory trajectory accuracy, resulting in balanced performance\non motion prediction benchmarks. Moreover, ModeSeq naturally emerges with the\ncapability of mode extrapolation, which supports forecasting more behavior\nmodes when the future is highly uncertain.",
      "tldr_zh": "本研究针对自动驾驶中的多模态运动预测问题，提出ModeSeq框架，通过Sequential Mode Modeling将模式建模为序列，从而逐步推断下一个模式，增强模式间相关性和多模态推理能力。论文引入Early-Match-Take-All (EMTA)训练策略，进一步提升轨迹多样性，而无需密集模式预测或启发式后处理。实验结果显示，ModeSeq在运动预测基准上实现了轨迹准确性和多样性的平衡，并具备模式外推能力，支持在高度不确定性场景下预测更多行为模式。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2411.11911v2",
      "published_date": "2024-11-17 16:36:09 UTC",
      "updated_date": "2025-03-23 14:46:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T01:20:22.097193"
    },
    {
      "arxiv_id": "2411.11105v1",
      "title": "Label Sharing Incremental Learning Framework for Independent Multi-Label Segmentation Tasks",
      "title_zh": "标签共享增量学习框架，用于独立的多标签分割任务",
      "authors": [
        "Deepa Anand",
        "Bipul Das",
        "Vyshnav Dangeti",
        "Antony Jerald",
        "Rakesh Mullick",
        "Uday Patil",
        "Pakhi Sharma",
        "Prasad Sudhakar"
      ],
      "abstract": "In a setting where segmentation models have to be built for multiple\ndatasets, each with its own corresponding label set, a straightforward way is\nto learn one model for every dataset and its labels. Alternatively, multi-task\narchitectures with shared encoders and multiple segmentation heads or shared\nweights with compound labels can also be made use of. This work proposes a\nnovel label sharing framework where a shared common label space is constructed\nand each of the individual label sets are systematically mapped to the common\nlabels. This transforms multiple datasets with disparate label sets into a\nsingle large dataset with shared labels, and therefore all the segmentation\ntasks can be addressed by learning a single model. This eliminates the need for\ntask specific adaptations in network architectures and also results in\nparameter and data efficient models. Furthermore, label sharing framework is\nnaturally amenable for incremental learning where segmentations for new\ndatasets can be easily learnt. We experimentally validate our method on various\nmedical image segmentation datasets, each involving multi-label segmentation.\nFurthermore, we demonstrate the efficacy of the proposed method in terms of\nperformance and incremental learning ability vis-a-vis alternative methods.",
      "tldr_zh": "这项研究提出了一种Label Sharing Incremental Learning Framework，用于处理多个独立多标签分割任务的场景。该框架通过构建一个共享的公共标签空间，将各数据集的标签集系统映射到该空间，从而将多数据集转化为一个大型共享标签数据集，并使用单个模型进行所有分割任务，提高了参数和数据效率。该方法特别适合增量学习，能够轻松适应新数据集，并在各种医疗图像分割实验中展示了比替代方法更高的性能和学习能力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.11105v1",
      "published_date": "2024-11-17 15:50:25 UTC",
      "updated_date": "2024-11-17 15:50:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T01:20:33.768675"
    },
    {
      "arxiv_id": "2411.11101v2",
      "title": "Different Horses for Different Courses: Comparing Bias Mitigation Algorithms in ML",
      "title_zh": "翻译失败",
      "authors": [
        "Prakhar Ganesh",
        "Usman Gohar",
        "Lu Cheng",
        "Golnoosh Farnadi"
      ],
      "abstract": "With fairness concerns gaining significant attention in Machine Learning\n(ML), several bias mitigation techniques have been proposed, often compared\nagainst each other to find the best method. These benchmarking efforts tend to\nuse a common setup for evaluation under the assumption that providing a uniform\nenvironment ensures a fair comparison. However, bias mitigation techniques are\nsensitive to hyperparameter choices, random seeds, feature selection, etc.,\nmeaning that comparison on just one setting can unfairly favour certain\nalgorithms. In this work, we show significant variance in fairness achieved by\nseveral algorithms and the influence of the learning pipeline on fairness\nscores. We highlight that most bias mitigation techniques can achieve\ncomparable performance, given the freedom to perform hyperparameter\noptimization, suggesting that the choice of the evaluation parameters-rather\nthan the mitigation technique itself-can sometimes create the perceived\nsuperiority of one method over another. We hope our work encourages future\nresearch on how various choices in the lifecycle of developing an algorithm\nimpact fairness, and trends that guide the selection of appropriate algorithms.",
      "tldr_zh": "这项研究比较了机器学习 (ML) 中的偏见缓解算法，指出这些算法对超参数选择、随机种子和特征选择等因素高度敏感，导致单一评估设置可能导致不公平的比较。通过实验，研究者展示了不同算法在公平性分数上的显著变异，并发现通过超参数优化，大多数偏见缓解算法可以实现相似的性能，这表明算法的感知优越性往往源于评估参数的选择而非算法本身。该工作呼吁未来研究更多关注算法开发生命周期中各种决策对公平性的影响，以指导合适算法的选定。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "To appear at AFME@NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2411.11101v2",
      "published_date": "2024-11-17 15:17:08 UTC",
      "updated_date": "2024-11-19 02:39:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T01:22:44.986207"
    },
    {
      "arxiv_id": "2412.03574v1",
      "title": "Back-filling Missing Data When Predicting Domestic Electricity Consumption From Smart Meter Data",
      "title_zh": "翻译失败",
      "authors": [
        "Xianjuan Chen",
        "Shuxiang Cai",
        "Alan F. Smeaton"
      ],
      "abstract": "This study uses data from domestic electricity smart meters to estimate\nannual electricity bills for a whole year. We develop a method for back-filling\ndata smart meter for up to six missing months for users who have less than one\nyear of smart meter data, ensuring reliable estimates of annual consumption. We\nidentify five distinct electricity consumption user profiles for homes based on\nday, night, and peak usage patterns, highlighting the economic advantages of\nTime-of-Use (ToU) tariffs over fixed tariffs for most users, especially those\nwith higher nighttime consumption. Ultimately, the results of this study\nempowers consumers to manage their energy use effectively and to make informed\nchoices regarding electricity tariff plans.",
      "tldr_zh": "本研究使用智能电表数据开发了一种 back-filling 方法，用于回填最多六个月的缺失数据，从而为数据不足一年的用户提供可靠的年度电力消费估计。研究通过分析日、夜和高峰期使用模式，识别了五种不同的家庭电力消费用户配置文件。结果显示，Time-of-Use (ToU) 关税相对于固定关税具有经济优势，特别是对夜间消费较高的用户。最终，这有助于消费者有效管理能源并做出明智的电力关税选择。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "10 pages, 7 figures, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2412.03574v1",
      "published_date": "2024-11-17 15:03:59 UTC",
      "updated_date": "2024-11-17 15:03:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T01:20:57.949862"
    },
    {
      "arxiv_id": "2411.11099v1",
      "title": "Mitigating Relative Over-Generalization in Multi-Agent Reinforcement Learning",
      "title_zh": "缓解多智能体强化学习中的相对过度泛化",
      "authors": [
        "Ting Zhu",
        "Yue Jin",
        "Jeremie Houssineau",
        "Giovanni Montana"
      ],
      "abstract": "In decentralized multi-agent reinforcement learning, agents learning in\nisolation can lead to relative over-generalization (RO), where optimal joint\nactions are undervalued in favor of suboptimal ones. This hinders effective\ncoordination in cooperative tasks, as agents tend to choose actions that are\nindividually rational but collectively suboptimal. To address this issue, we\nintroduce MaxMax Q-Learning (MMQ), which employs an iterative process of\nsampling and evaluating potential next states, selecting those with maximal\nQ-values for learning. This approach refines approximations of ideal state\ntransitions, aligning more closely with the optimal joint policy of\ncollaborating agents. We provide theoretical analysis supporting MMQ's\npotential and present empirical evaluations across various environments\nsusceptible to RO. Our results demonstrate that MMQ frequently outperforms\nexisting baselines, exhibiting enhanced convergence and sample efficiency.",
      "tldr_zh": "在多智能体强化学习中，相对过度泛化 (Relative Over-Generalization, RO) 问题会导致代理选择个体理性但集体次优的动作，从而阻碍合作任务的有效协调。针对此，本文提出 MaxMax Q-Learning (MMQ) 方法，通过迭代采样和评估潜在下一个状态，选择具有最大 Q 值的状态来优化状态转换近似，从而更好地对齐合作代理的最优联合策略。实验结果显示，MMQ 在易受 RO 影响的环境中优于现有基线，展现出更高的收敛性和样本效率。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Published in Transactions on Machine Learning Research (11/2024)",
      "pdf_url": "http://arxiv.org/pdf/2411.11099v1",
      "published_date": "2024-11-17 15:00:39 UTC",
      "updated_date": "2024-11-17 15:00:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T01:21:10.349942"
    },
    {
      "arxiv_id": "2411.14463v1",
      "title": "Leveraging AI and NLP for Bank Marketing: A Systematic Review and Gap Analysis",
      "title_zh": "利用 AI 和 NLP 提升银行营销：系统综述与差距分析",
      "authors": [
        "Christopher Gerling",
        "Stefan Lessmann"
      ],
      "abstract": "This paper explores the growing impact of AI and NLP in bank marketing,\nhighlighting their evolving roles in enhancing marketing strategies, improving\ncustomer engagement, and creating value within this sector. While AI and NLP\nhave been widely studied in general marketing, there is a notable gap in\nunderstanding their specific applications and potential within the banking\nsector. This research addresses this specific gap by providing a systematic\nreview and strategic analysis of AI and NLP applications in bank marketing,\nfocusing on their integration across the customer journey and operational\nexcellence. Employing the PRISMA methodology, this study systematically reviews\nexisting literature to assess the current landscape of AI and NLP in bank\nmarketing. Additionally, it incorporates semantic mapping using Sentence\nTransformers and UMAP for strategic gap analysis to identify underexplored\nareas and opportunities for future research.\n  The systematic review reveals limited research specifically focused on NLP\napplications in bank marketing. The strategic gap analysis identifies key areas\nwhere NLP can further enhance marketing strategies, including customer-centric\napplications like acquisition, retention, and personalized engagement, offering\nvaluable insights for both academic research and practical implementation. This\nresearch contributes to the field of bank marketing by mapping the current\nstate of AI and NLP applications and identifying strategic gaps. The findings\nprovide actionable insights for developing NLP-driven growth and innovation\nframeworks and highlight the role of NLP in improving operational efficiency\nand regulatory compliance. This work has broader implications for enhancing\ncustomer experience, profitability, and innovation in the banking industry.",
      "tldr_zh": "这篇论文通过系统回顾探讨了AI和NLP在银行营销中的应用，强调其在提升营销策略、客户互动和价值创造方面的作用，同时指出了现有研究的缺口。研究采用PRISMA方法结合Sentence Transformers和UMAP进行语义映射和战略缺口分析，揭示了NLP在银行营销领域的有限研究，并识别出关键机会，如客户获取、保留和个性化互动。总体贡献包括提供行动性洞见，促进NLP驱动的创新框架，提升银行的客户体验、盈利能力和运营效率。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "econ.GN",
        "q-fin.EC"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.14463v1",
      "published_date": "2024-11-17 14:44:12 UTC",
      "updated_date": "2024-11-17 14:44:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T01:22:57.041856"
    },
    {
      "arxiv_id": "2411.11910v2",
      "title": "AIGS: Generating Science from AI-Powered Automated Falsification",
      "title_zh": "翻译失败",
      "authors": [
        "Zijun Liu",
        "Kaiming Liu",
        "Yiqi Zhu",
        "Xuanyu Lei",
        "Zonghan Yang",
        "Zhenhe Zhang",
        "Peng Li",
        "Yang Liu"
      ],
      "abstract": "Rapid development of artificial intelligence has drastically accelerated the\ndevelopment of scientific discovery. Trained with large-scale observation data,\ndeep neural networks extract the underlying patterns in an end-to-end manner\nand assist human researchers with highly-precised predictions in unseen\nscenarios. The recent rise of Large Language Models (LLMs) and the empowered\nautonomous agents enable scientists to gain help through interaction in\ndifferent stages of their research, including but not limited to literature\nreview, research ideation, idea implementation, and academic writing. However,\nAI researchers instantiated by foundation model empowered agents with\nfull-process autonomy are still in their infancy. In this paper, we study\n$\\textbf{AI-Generated Science}$ (AIGS), where agents independently and\nautonomously complete the entire research process and discover scientific laws.\nBy revisiting the definition of scientific research, we argue that\n$\\textit{falsification}$ is the essence of both human research process and the\ndesign of an AIGS system. Through the lens of falsification, prior systems\nattempting towards AI-Generated Science either lack the part in their design,\nor rely heavily on existing verification engines that narrow the use in\nspecialized domains. In this work, we propose Baby-AIGS as a baby-step\ndemonstration of a full-process AIGS system, which is a multi-agent system with\nagents in roles representing key research process. By introducing\nFalsificationAgent, which identify and then verify possible scientific\ndiscoveries, we empower the system with explicit falsification. Experiments on\nthree tasks preliminarily show that Baby-AIGS could produce meaningful\nscientific discoveries, though not on par with experienced human researchers.\nFinally, we discuss on the limitations of current Baby-AIGS, actionable\ninsights, and related ethical issues in detail.",
      "tldr_zh": "这篇论文提出AI-Generated Science (AIGS)概念，旨在让AI代理自主完成整个科学研究过程，包括文献综述、构思和发现科学规律，并强调falsification（证伪）作为科学研究的本质。作者设计了Baby-AIGS系统，这是一个多代理框架，引入FalsificationAgent来识别和验证可能的科学发现，从而弥补现有系统的不足。实验在三个任务上显示，Baby-AIGS能够产生有意义的科学发现，但性能仍落后于经验丰富的人类研究者。最后，论文讨论了该系统的局限性、潜在改进见解以及相关的伦理问题。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Pre-print. 35 pages. Official website:\n  https://agent-force.github.io/AIGS/",
      "pdf_url": "http://arxiv.org/pdf/2411.11910v2",
      "published_date": "2024-11-17 13:40:35 UTC",
      "updated_date": "2024-11-24 12:59:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T01:23:08.406856"
    },
    {
      "arxiv_id": "2411.11057v1",
      "title": "Reinforcing Competitive Multi-Agents for Playing So Long Sucker",
      "title_zh": "翻译失败",
      "authors": [
        "Medant Sharan",
        "Chandranath Adak"
      ],
      "abstract": "This paper examines the use of classical deep reinforcement learning (DRL)\nalgorithms, DQN, DDQN, and Dueling DQN, in the strategy game So Long Sucker\n(SLS), a diplomacy-driven game defined by coalition-building and strategic\nbetrayal. SLS poses unique challenges due to its blend of cooperative and\nadversarial dynamics, making it an ideal platform for studying multi-agent\nlearning and game theory. The study's primary goal is to teach autonomous\nagents the game's rules and strategies using classical DRL methods. To support\nthis effort, the authors developed a novel, publicly available implementation\nof SLS, featuring a graphical user interface (GUI) and benchmarking tools for\nDRL algorithms. Experimental results reveal that while considered basic by\nmodern DRL standards, DQN, DDQN, and Dueling DQN agents achieved roughly 50% of\nthe maximum possible game reward. This suggests a baseline understanding of the\ngame's mechanics, with agents favoring legal moves over illegal ones. However,\na significant limitation was the extensive training required, around 2000\ngames, for agents to reach peak performance, compared to human players who\ngrasp the game within a few rounds. Even after prolonged training, agents\noccasionally made illegal moves, highlighting both the potential and\nlimitations of these classical DRL methods in semi-complex, socially driven\ngames. The findings establish a foundational benchmark for training agents in\nSLS and similar negotiation-based environments while underscoring the need for\nadvanced or hybrid DRL approaches to improve learning efficiency and\nadaptability. Future research could incorporate game-theoretic strategies to\nenhance agent decision-making in dynamic multi-agent contexts.",
      "tldr_zh": "本研究探讨了使用经典深度强化学习(DRL)算法，如DQN、DDQN和Dueling DQN，在策略游戏So Long Sucker (SLS)中训练竞争性多代理，以应对其合作与对抗相结合的挑战。作者开发了一个新型的公开SLS实现，包括图形用户界面(GUI)和DRL基准测试工具，帮助代理学习游戏规则和策略。实验结果显示，这些代理达到了约50%的最大游戏奖励，优先选择合法动作，但需经过约2000场游戏训练才能达到峰值表现，远超人类学习速度，且偶尔出现非法动作。研究为SLS和类似谈判环境建立了基础基准，并建议未来采用高级或混合DRL方法及游戏理论策略来提升代理的学习效率和适应性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.11057v1",
      "published_date": "2024-11-17 12:38:13 UTC",
      "updated_date": "2024-11-17 12:38:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T01:23:20.515050"
    },
    {
      "arxiv_id": "2411.11053v5",
      "title": "SRA-MCTS: Self-driven Reasoning Augmentation with Monte Carlo Tree Search for Code Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Bin Xu",
        "Yiguan Lin",
        "Yinghao Li",
        "Yang Gao"
      ],
      "abstract": "Large language models demonstrate exceptional performance in simple code\ngeneration tasks but still face challenges in tackling complex problems. These\nchallenges may stem from insufficient reasoning and problem decomposition\ncapabilities. To address this issue, we propose a reasoning-augmented data\ngeneration process, SRA-MCTS, which guides the model to autonomously generate\nhigh-quality intermediate reasoning paths. This creates a positive feedback\nloop, enabling continuous improvement. Our method operates entirely through the\nmodel itself without requiring additional supervision. By synthesizing natural\nlanguage reasoning paths and translating them into executable code, the\napproach ensures analytical accuracy and enhances the success rate in solving\ncomplex tasks. Experimental results show that, even without additional\nsupervisory signals, our method achieves performance improvements across\ndifferent model scales, demonstrating the significant potential of\nself-improvement in small models. Furthermore, the method remains robust when\ntraditional Chain-of-Thought (CoT) approaches exhibit performance degradation,\nwith notable improvements observed in diversity metrics such as pass@10. We\nencourage further exploration of reasoning processes within training data to\nenhance the ability of language models to address complex problems. Our code\nand data are public at https://github.com/DIRECT-BIT/SRA-MCTS.",
      "tldr_zh": "本研究提出 SRA-MCTS，一种基于 Monte Carlo Tree Search 的自驱动推理增强方法，用于提升大型语言模型在复杂代码生成任务中的性能。该方法通过引导模型自主生成高质量的中间推理路径，并将其转化为可执行代码，确保分析准确性和任务成功率，而无需额外监督。实验结果显示，即使在不同模型规模下，SRA-MCTS 也能实现性能提升，并在 Chain-of-Thought (CoT) 方法失效时保持稳健，尤其在多样性指标如 pass@10 上取得显著改善，从而证明了自提升机制在小模型中的潜力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by IJCAI2025",
      "pdf_url": "http://arxiv.org/pdf/2411.11053v5",
      "published_date": "2024-11-17 12:31:04 UTC",
      "updated_date": "2025-05-09 07:24:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T01:23:32.819765"
    },
    {
      "arxiv_id": "2411.11046v1",
      "title": "Knowledge-enhanced Transformer for Multivariate Long Sequence Time-series Forecasting",
      "title_zh": "翻译失败",
      "authors": [
        "Shubham Tanaji Kakde",
        "Rony Mitra",
        "Jasashwi Mandal",
        "Manoj Kumar Tiwari"
      ],
      "abstract": "Multivariate Long Sequence Time-series Forecasting (LSTF) has been a critical\ntask across various real-world applications. Recent advancements focus on the\napplication of transformer architectures attributable to their ability to\ncapture temporal patterns effectively over extended periods. However, these\napproaches often overlook the inherent relationships and interactions between\nthe input variables that could be drawn from their characteristic properties.\nIn this paper, we aim to bridge this gap by integrating information-rich\nKnowledge Graph Embeddings (KGE) with state-of-the-art transformer-based\narchitectures. We introduce a novel approach that encapsulates conceptual\nrelationships among variables within a well-defined knowledge graph, forming\ndynamic and learnable KGEs for seamless integration into the transformer\narchitecture. We investigate the influence of this integration into seminal\narchitectures such as PatchTST, Autoformer, Informer, and Vanilla Transformer.\nFurthermore, we thoroughly investigate the performance of these\nknowledge-enhanced architectures along with their original implementations for\nlong forecasting horizons and demonstrate significant improvement in the\nbenchmark results. This enhancement empowers transformer-based architectures to\naddress the inherent structural relation between variables. Our\nknowledge-enhanced approach improves the accuracy of multivariate LSTF by\ncapturing complex temporal and relational dynamics across multiple domains. To\nsubstantiate the validity of our model, we conduct comprehensive experiments\nusing Weather and Electric Transformer Temperature (ETT) datasets.",
      "tldr_zh": "该论文针对多变量长序列时间序列预测 (Multivariate Long Sequence Time-series Forecasting, LSTF) 的挑战，提出了一种将 Knowledge Graph Embeddings (KGE) 整合到 Transformer 架构中的新方法，以捕捉变量间的固有关系和交互。方法通过构建知识图来生成动态、可学习的 KGE，并将其无缝整合到 PatchTST、Autoformer、Informer 和 Vanilla Transformer 等模型中。实验在 Weather 和 ETT 数据集上表明，这种知识增强方法显著提升了预测准确性，在长预测 horizons 上比原模型基准结果改善明显。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 4 figures, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2411.11046v1",
      "published_date": "2024-11-17 11:53:54 UTC",
      "updated_date": "2024-11-17 11:53:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T01:23:43.894170"
    },
    {
      "arxiv_id": "2412.01622v1",
      "title": "Image Forgery Localization via Guided Noise and Multi-Scale Feature Aggregation",
      "title_zh": "图像伪造定位通过引导噪声和多尺度特征聚合",
      "authors": [
        "Yakun Niu",
        "Pei Chen",
        "Lei Zhang",
        "Lei Tan",
        "Yingjian Chen"
      ],
      "abstract": "Image Forgery Localization (IFL) technology aims to detect and locate the\nforged areas in an image, which is very important in the field of digital\nforensics. However, existing IFL methods suffer from feature degradation during\ntraining using multi-layer convolutions or the self-attention mechanism, and\nperform poorly in detecting small forged regions and in robustness against\npost-processing. To tackle these, we propose a guided and multi-scale feature\naggregated network for IFL. Spectifically, in order to comprehensively learn\nthe noise feature under different types of forgery, we develop an effective\nnoise extraction module in a guided way. Then, we design a Feature Aggregation\nModule (FAM) that uses dynamic convolution to adaptively aggregate RGB and\nnoise features over multiple scales. Moreover, we propose an Atrous Residual\nPyramid Module (ARPM) to enhance features representation and capture both\nglobal and local features using different receptive fields to improve the\naccuracy and robustness of forgery localization. Expensive experiments on 5\npublic datasets have shown that our proposed model outperforms several the\nstate-of-the-art methods, specially on small region forged image.",
      "tldr_zh": "该论文针对图像伪造定位(Image Forgery Localization, IFL)的问题，提出了一种引导式噪声提取和多尺度特征聚合网络，以解决现有方法在特征退化、检测小伪造区域和抗后处理方面的不足。具体而言，该网络包括一个引导式噪声提取模块，用于全面学习不同伪造类型下的噪声特征；一个Feature Aggregation Module (FAM)，通过动态卷积自适应聚合RGB和噪声特征在多个尺度；以及一个Atrous Residual Pyramid Module (ARPM)，增强特征表示并捕获全局和局部信息，从而提高定位准确性和鲁棒性。在5个公开数据集上的实验表明，该模型优于现有最先进方法，尤其在小区域伪造图像上的性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "36 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.01622v1",
      "published_date": "2024-11-17 11:50:09 UTC",
      "updated_date": "2024-11-17 11:50:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T01:23:56.097704"
    },
    {
      "arxiv_id": "2411.11029v1",
      "title": "Wafer Map Defect Classification Using Autoencoder-Based Data Augmentation and Convolutional Neural Network",
      "title_zh": "翻译失败",
      "authors": [
        "Yin-Yin Bao",
        "Er-Chao Li",
        "Hong-Qiang Yang",
        "Bin-Bin Jia"
      ],
      "abstract": "In semiconductor manufacturing, wafer defect maps (WDMs) play a crucial role\nin diagnosing issues and enhancing process yields by revealing critical defect\npatterns. However, accurately categorizing WDM defects presents significant\nchallenges due to noisy data, unbalanced defect classes, and the complexity of\nfailure modes. To address these challenges, this study proposes a novel method\ncombining a self-encoder-based data augmentation technique with a convolutional\nneural network (CNN). By introducing noise into the latent space, the\nself-encoder enhances data diversity and mitigates class imbalance, thereby\nimproving the model's generalization capabilities. The augmented dataset is\nsubsequently used to train the CNN, enabling it to deliver precise\nclassification of both common and rare defect patterns. Experimental results on\nthe WM-811K dataset demonstrate that the proposed method achieves a\nclassification accuracy of 98.56%, surpassing Random Forest, SVM, and Logistic\nRegression by 19%, 21%, and 27%, respectively. These findings highlight the\nrobustness and effectiveness of the proposed approach, offering a reliable\nsolution for wafer defect detection and classification.",
      "tldr_zh": "这篇论文针对半导体制造中晶圆缺陷地图（WDMs）的分类问题，提出了一种结合 autoencoder-based 数据增强和卷积神经网络（CNN）的新方法，以应对数据噪声、类别不平衡和失败模式复杂性的挑战。通过在潜在空间引入噪声，autoencoder 增强数据多样性并缓解类别不平衡，提高模型的泛化能力。实验在 WM-811K 数据集上显示，该方法实现 98.56% 的分类准确率，比 Random Forest、SVM 和 Logistic Regression 分别高出 19%、21% 和 27%。这一方法为晶圆缺陷检测提供了一个鲁棒且有效的解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV",
        "68T07, 68U10",
        "I.2.10; I.5.1; I.5.4; I.4.8"
      ],
      "primary_category": "cs.CV",
      "comment": "26 pages, 11 figures, including dataset preprocessing, proposed\n  methods, and experimental results",
      "pdf_url": "http://arxiv.org/pdf/2411.11029v1",
      "published_date": "2024-11-17 10:19:54 UTC",
      "updated_date": "2024-11-17 10:19:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T01:24:09.665095"
    },
    {
      "arxiv_id": "2411.11027v1",
      "title": "BianCang: A Traditional Chinese Medicine Large Language Model",
      "title_zh": "翻译失败",
      "authors": [
        "Sibo Wei",
        "Xueping Peng",
        "Yi-fei Wang",
        "Jiasheng Si",
        "Weiyu Zhang",
        "Wenpeng Lu",
        "Xiaoming Wu",
        "Yinglong Wang"
      ],
      "abstract": "The rise of large language models (LLMs) has driven significant progress in\nmedical applications, including traditional Chinese medicine (TCM). However,\ncurrent medical LLMs struggle with TCM diagnosis and syndrome differentiation\ndue to substantial differences between TCM and modern medical theory, and the\nscarcity of specialized, high-quality corpora. This paper addresses these\nchallenges by proposing BianCang, a TCM-specific LLM, using a two-stage\ntraining process that first injects domain-specific knowledge and then aligns\nit through targeted stimulation. To enhance diagnostic and differentiation\ncapabilities, we constructed pre-training corpora, instruction-aligned datasets\nbased on real hospital records, and the ChP-TCM dataset derived from the\nPharmacopoeia of the People's Republic of China. We compiled extensive TCM and\nmedical corpora for continuous pre-training and supervised fine-tuning,\nbuilding a comprehensive dataset to refine the model's understanding of TCM.\nEvaluations across 11 test sets involving 29 models and 4 tasks demonstrate the\neffectiveness of BianCang, offering valuable insights for future research.\nCode, datasets, and models are available at\nhttps://github.com/QLU-NLP/BianCang.",
      "tldr_zh": "本论文提出BianCang，一种针对传统中医(TCM)的专用大型语言模型(LLM)，旨在解决现有医疗LLM在TCM诊断和辨证方面的挑战，如理论差异和高质量语料短缺。\n该模型采用两阶段训练过程：首先通过连续预训练注入领域特定知识，然后通过监督微调和针对性刺激进行对齐，并构建了预训练语料、基于真实医院记录的指令数据集以及ChP-TCM数据集。\n在11个测试集和4个任务上评估29个模型的结果表明，BianCang表现出色，提供宝贵见解，并已在GitHub上公开代码、数据集和模型。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.11027v1",
      "published_date": "2024-11-17 10:17:01 UTC",
      "updated_date": "2024-11-17 10:17:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T01:24:21.097614"
    },
    {
      "arxiv_id": "2411.11016v2",
      "title": "Time Step Generating: A Universal Synthesized Deepfake Image Detector",
      "title_zh": "翻译失败",
      "authors": [
        "Ziyue Zeng",
        "Haoyuan Liu",
        "Dingjie Peng",
        "Luoxu Jing",
        "Hiroshi Watanabe"
      ],
      "abstract": "Currently, high-fidelity text-to-image models are developed in an\naccelerating pace. Among them, Diffusion Models have led to a remarkable\nimprovement in the quality of image generation, making it vary challenging to\ndistinguish between real and synthesized images. It simultaneously raises\nserious concerns regarding privacy and security. Some methods are proposed to\ndistinguish the diffusion model generated images through reconstructing.\nHowever, the inversion and denoising processes are time-consuming and heavily\nreliant on the pre-trained generative model. Consequently, if the pre-trained\ngenerative model meet the problem of out-of-domain, the detection performance\ndeclines. To address this issue, we propose a universal synthetic image\ndetector Time Step Generating (TSG), which does not rely on pre-trained models'\nreconstructing ability, specific datasets, or sampling algorithms. Our method\nutilizes a pre-trained diffusion model's network as a feature extractor to\ncapture fine-grained details, focusing on the subtle differences between real\nand synthetic images. By controlling the time step t of the network input, we\ncan effectively extract these distinguishing detail features. Then, those\nfeatures can be passed through a classifier (i.e. Resnet), which efficiently\ndetects whether an image is synthetic or real. We test the proposed TSG on the\nlarge-scale GenImage benchmark and it achieves significant improvements in both\naccuracy and generalizability.",
      "tldr_zh": "本论文针对扩散模型（Diffusion Models）生成的高质量合成图像检测难题，提出了一种通用检测器 Time Step Generating (TSG)，该方法无需依赖预训练生成模型的重建能力、特定数据集或采样算法。TSG 通过利用预训练扩散模型的网络作为特征提取器，控制时间步 t 以捕捉真实和合成图像的细微差异，然后使用 ResNet 等分类器进行高效检测。实验结果显示，在 GenImage 基准测试中，TSG 显著提升了准确性和泛化性，解决了现有方法的局限性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "62H30, 68T07",
        "I.4.9; I.4.7; I.5.2"
      ],
      "primary_category": "cs.CV",
      "comment": "9 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.11016v2",
      "published_date": "2024-11-17 09:39:50 UTC",
      "updated_date": "2024-11-20 00:30:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T01:24:32.649086"
    },
    {
      "arxiv_id": "2411.11006v2",
      "title": "BackdoorMBTI: A Backdoor Learning Multimodal Benchmark Tool Kit for Backdoor Defense Evaluation",
      "title_zh": "BackdoorMBTI：一种用于后门防御评估的后门学习多模态基准工具包",
      "authors": [
        "Haiyang Yu",
        "Tian Xie",
        "Jiaping Gui",
        "Pengyang Wang",
        "Ping Yi",
        "Yue Wu"
      ],
      "abstract": "Over the past few years, the emergence of backdoor attacks has presented\nsignificant challenges to deep learning systems, allowing attackers to insert\nbackdoors into neural networks. When data with a trigger is processed by a\nbackdoor model, it can lead to mispredictions targeted by attackers, whereas\nnormal data yields regular results. The scope of backdoor attacks is expanding\nbeyond computer vision and encroaching into areas such as natural language\nprocessing and speech recognition. Nevertheless, existing backdoor defense\nmethods are typically tailored to specific data modalities, restricting their\napplication in multimodal contexts. While multimodal learning proves highly\napplicable in facial recognition, sentiment analysis, action recognition,\nvisual question answering, the security of these models remains a crucial\nconcern. Specifically, there are no existing backdoor benchmarks targeting\nmultimodal applications or related tasks.\n  In order to facilitate the research in multimodal backdoor, we introduce\nBackdoorMBTI, the first backdoor learning toolkit and benchmark designed for\nmultimodal evaluation across three representative modalities from eleven\ncommonly used datasets. BackdoorMBTI provides a systematic backdoor learning\npipeline, encompassing data processing, data poisoning, backdoor training, and\nevaluation. The generated poison datasets and backdoor models enable detailed\nevaluation of backdoor defenses. Given the diversity of modalities,\nBackdoorMBTI facilitates systematic evaluation across different data types.\nFurthermore, BackdoorMBTI offers a standardized approach to handling practical\nfactors in backdoor learning, such as issues related to data quality and\nerroneous labels. We anticipate that BackdoorMBTI will expedite future research\nin backdoor defense methods within a multimodal context. Code is available at\nhttps://github.com/SJTUHaiyangYu/BackdoorMBTI.",
      "tldr_zh": "该研究介绍了 BackdoorMBTI，这是一个针对多模态后门攻击的首个基准工具包，用于评估 backdoor defense 方法。BackdoorMBTI 涵盖三个代表性模态和十一个常用数据集，提供完整的后门学习管道，包括数据处理、数据投毒、backdoor 训练和评估，以生成毒化数据集和模型进行详细测试。它处理实际问题如数据质量和错误标签，旨在加速多模态 backdoor 防御研究，并促进模型安全的改进。代码开源，可从指定仓库获取。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.11006v2",
      "published_date": "2024-11-17 09:01:55 UTC",
      "updated_date": "2025-03-06 07:50:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T01:24:44.267062"
    },
    {
      "arxiv_id": "2411.11002v1",
      "title": "Unveiling the Hidden: Online Vectorized HD Map Construction with Clip-Level Token Interaction and Propagation",
      "title_zh": "翻译失败",
      "authors": [
        "Nayeon Kim",
        "Hongje Seong",
        "Daehyun Ji",
        "Sujin Jang"
      ],
      "abstract": "Predicting and constructing road geometric information (e.g., lane lines,\nroad markers) is a crucial task for safe autonomous driving, while such static\nmap elements can be repeatedly occluded by various dynamic objects on the road.\nRecent studies have shown significantly improved vectorized high-definition\n(HD) map construction performance, but there has been insufficient\ninvestigation of temporal information across adjacent input frames (i.e.,\nclips), which may lead to inconsistent and suboptimal prediction results. To\ntackle this, we introduce a novel paradigm of clip-level vectorized HD map\nconstruction, MapUnveiler, which explicitly unveils the occluded map elements\nwithin a clip input by relating dense image representations with efficient clip\ntokens. Additionally, MapUnveiler associates inter-clip information through\nclip token propagation, effectively utilizing long-term temporal map\ninformation. MapUnveiler runs efficiently with the proposed clip-level pipeline\nby avoiding redundant computation with temporal stride while building a global\nmap relationship. Our extensive experiments demonstrate that MapUnveiler\nachieves state-of-the-art performance on both the nuScenes and Argoverse2\nbenchmark datasets. We also showcase that MapUnveiler significantly outperforms\nstate-of-the-art approaches in a challenging setting, achieving +10.7% mAP\nimprovement in heavily occluded driving road scenes. The project page can be\nfound at https://mapunveiler.github.io.",
      "tldr_zh": "该论文提出MapUnveiler，一种在线向量化HD Map构建框架，旨在处理道路几何信息（如车道线和路标）被动态物体遮挡的问题，通过clip-level token交互和传播来显式揭示遮挡元素并利用相邻帧的时间信息。框架将密集图像表示与高效clip tokens相关联，并通过clip token propagation关联多clip信息，避免冗余计算以构建全局地图关系。实验结果显示，MapUnveiler在nuScenes和Argoverse2基准数据集上达到最先进性能，尤其在严重遮挡场景下，比现有方法提升10.7% mAP。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "18 pages, 9 figures, NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2411.11002v1",
      "published_date": "2024-11-17 08:38:18 UTC",
      "updated_date": "2024-11-17 08:38:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T01:24:56.285957"
    },
    {
      "arxiv_id": "2411.13584v1",
      "title": "AddrLLM: Address Rewriting via Large Language Model on Nationwide Logistics Data",
      "title_zh": "AddrLLM：通过大型语言模型在全国物流数据上进行的地址重写",
      "authors": [
        "Qinchen Yang",
        "Zhiqing Hong",
        "Dongjiang Cao",
        "Haotian Wang",
        "Zejun Xie",
        "Tian He",
        "Yunhuai Liu",
        "Yu Yang",
        "Desheng Zhang"
      ],
      "abstract": "Textual description of a physical location, commonly known as an address,\nplays an important role in location-based services(LBS) such as on-demand\ndelivery and navigation. However, the prevalence of abnormal addresses, those\ncontaining inaccuracies that fail to pinpoint a location, have led to\nsignificant costs. Address rewriting has emerged as a solution to rectify these\nabnormal addresses. Despite the critical need, existing address rewriting\nmethods are limited, typically tailored to correct specific error types, or\nfrequently require retraining to process new address data effectively. In this\nstudy, we introduce AddrLLM, an innovative framework for address rewriting that\nis built upon a retrieval augmented large language model. AddrLLM overcomes\naforementioned limitations through a meticulously designed Supervised\nFine-Tuning module, an Address-centric Retrieval Augmented Generation module\nand a Bias-free Objective Alignment module. To the best of our knowledge, this\nstudy pioneers the application of LLM-based address rewriting approach to solve\nthe issue of abnormal addresses. Through comprehensive offline testing with\nreal-world data on a national scale and subsequent online deployment, AddrLLM\nhas demonstrated superior performance in integration with existing logistics\nsystem. It has significantly decreased the rate of parcel re-routing by\napproximately 43\\%, underscoring its exceptional efficacy in real-world\napplications.",
      "tldr_zh": "本文提出 AddrLLM，一种基于大型语言模型(LLM)的创新框架，用于处理物流系统中异常地址问题，该框架通过 Supervised Fine-Tuning 模块、Address-centric Retrieval Augmented Generation 模块和 Bias-free Objective Alignment 模块来提升地址重写的准确性和适应性。不同于现有方法，AddrLLM 首次将 LLM 应用于地址重写，能够有效应对各种错误类型而无需频繁重训。在全国规模的真实数据测试和在线部署中，该框架将包裹重新路由率降低了约 43%，证明了其在位置服务中的实际效能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by KDD'25 ADS Track",
      "pdf_url": "http://arxiv.org/pdf/2411.13584v1",
      "published_date": "2024-11-17 07:32:46 UTC",
      "updated_date": "2024-11-17 07:32:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T01:25:09.028976"
    },
    {
      "arxiv_id": "2411.10991v1",
      "title": "Modulating Reservoir Dynamics via Reinforcement Learning for Efficient Robot Skill Synthesis",
      "title_zh": "通过强化学习调节储层动态以实现高效机器人技能",
      "authors": [
        "Zahra Koulaeizadeh",
        "Erhan Oztop"
      ],
      "abstract": "A random recurrent neural network, called a reservoir, can be used to learn\nrobot movements conditioned on context inputs that encode task goals. The\nLearning is achieved by mapping the random dynamics of the reservoir modulated\nby context to desired trajectories via linear regression. This makes the\nreservoir computing (RC) approach computationally efficient as no iterative\ngradient descent learning is needed. In this work, we propose a novel RC-based\nLearning from Demonstration (LfD) framework that not only learns to generate\nthe demonstrated movements but also allows online modulation of the reservoir\ndynamics to generate movement trajectories that are not covered by the initial\ndemonstration set. This is made possible by using a Reinforcement Learning (RL)\nmodule that learns a policy to output context as its actions based on the robot\nstate. Considering that the context dimension is typically low, learning with\nthe RL module is very efficient. We show the validity of the proposed model\nwith systematic experiments on a 2 degrees-of-freedom (DOF) simulated robot\nthat is taught to reach targets, encoded as context, with and without obstacle\navoidance constraint. The initial data set includes a set of reaching\ndemonstrations which are learned by the reservoir system. To enable reaching\nout-of-distribution targets, the RL module is engaged in learning a policy to\ngenerate dynamic contexts so that the generated trajectory achieves the desired\ngoal without any learning in the reservoir system. Overall, the proposed model\nuses an initial learned motor primitive set to efficiently generate diverse\nmotor behaviors guided by the designed reward function. Thus the model can be\nused as a flexible and effective LfD system where the action repertoire can be\nextended without new data collection.",
      "tldr_zh": "本研究提出了一种基于 Reservoir Computing (RC) 的新型 Learning from Demonstration (LfD) 框架，通过 Reinforcement Learning (RL) 模块在线调节 reservoir 动态，实现机器人技能的高效合成。该框架不仅能学习演示运动，还能生成超出初始数据集的轨迹，例如处理分布外目标或障碍避免任务，仅需 RL 模块学习上下文动作，而无需重新训练 reservoir 系统。实验在 2 DOF 模拟机器人上验证了该模型的有效性，使用初始到达演示数据即可扩展多样运动行为，提高了 LfD 系统的灵活性和计算效率。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "13 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.10991v1",
      "published_date": "2024-11-17 07:25:54 UTC",
      "updated_date": "2024-11-17 07:25:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T01:25:20.390940"
    },
    {
      "arxiv_id": "2411.10979v3",
      "title": "VidComposition: Can MLLMs Analyze Compositions in Compiled Videos?",
      "title_zh": "翻译失败",
      "authors": [
        "Yunlong Tang",
        "Junjia Guo",
        "Hang Hua",
        "Susan Liang",
        "Mingqian Feng",
        "Xinyang Li",
        "Rui Mao",
        "Chao Huang",
        "Jing Bi",
        "Zeliang Zhang",
        "Pooyan Fazli",
        "Chenliang Xu"
      ],
      "abstract": "The advancement of Multimodal Large Language Models (MLLMs) has enabled\nsignificant progress in multimodal understanding, expanding their capacity to\nanalyze video content. However, existing evaluation benchmarks for MLLMs\nprimarily focus on abstract video comprehension, lacking a detailed assessment\nof their ability to understand video compositions, the nuanced interpretation\nof how visual elements combine and interact within highly compiled video\ncontexts. We introduce VidComposition, a new benchmark specifically designed to\nevaluate the video composition understanding capabilities of MLLMs using\ncarefully curated compiled videos and cinematic-level annotations.\nVidComposition includes 982 videos with 1706 multiple-choice questions,\ncovering various compositional aspects such as camera movement, angle, shot\nsize, narrative structure, character actions and emotions, etc. Our\ncomprehensive evaluation of 33 open-source and proprietary MLLMs reveals a\nsignificant performance gap between human and model capabilities. This\nhighlights the limitations of current MLLMs in understanding complex, compiled\nvideo compositions and offers insights into areas for further improvement. The\nleaderboard and evaluation code are available at\nhttps://yunlong10.github.io/VidComposition/.",
      "tldr_zh": "本研究引入 VidComposition，这是一个新基准，用于评估多模态大型语言模型 (MLLMs) 在理解编译视频构图的能力，包括视觉元素如何在视频中结合互动。VidComposition 包含 982 个精心策划的视频和 1706 个多选题，涵盖相机运动、角度、镜头大小、叙事结构、人物动作和情绪等方面。通过评估 33 个开源和专有 MLLMs，实验结果显示模型性能与人类存在显著差距，揭示了当前 MLLMs 在处理复杂视频构图的局限性，并为未来改进提供宝贵见解。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.10979v3",
      "published_date": "2024-11-17 06:23:46 UTC",
      "updated_date": "2024-11-25 15:12:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T01:25:32.390559"
    },
    {
      "arxiv_id": "2411.10958v4",
      "title": "SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization",
      "title_zh": "翻译失败",
      "authors": [
        "Jintao Zhang",
        "Haofeng Huang",
        "Pengle Zhang",
        "Jia Wei",
        "Jun Zhu",
        "Jianfei Chen"
      ],
      "abstract": "Although quantization for linear layers has been widely used, its application\nto accelerate the attention process remains limited. To further enhance the\nefficiency of attention computation compared to SageAttention while maintaining\nprecision, we propose SageAttention2, which utilizes significantly faster 4-bit\nmatrix multiplication (Matmul) alongside additional precision-enhancing\ntechniques. First, we propose to quantize matrices $(Q, K)$ to INT4 in a\nhardware-friendly thread-level granularity and quantize matrices $(\\widetilde\nP, V)$ to FP8. Second, we propose a method to smooth $Q$, enhancing the\naccuracy of INT4 $QK^\\top$. Third, we propose a two-level accumulation strategy\nfor $\\widetilde PV$ to enhance the accuracy of FP8 $\\widetilde PV$. The\noperations per second (OPS) of SageAttention2 surpass FlashAttention2 and\nxformers by about 3x and 4.5x on RTX4090, respectively. Moreover,\nSageAttention2 matches the speed of FlashAttention3(fp8) on the Hopper GPUs,\nwhile delivering much higher accuracy. Comprehensive experiments confirm that\nour approach incurs negligible end-to-end metrics loss across diverse models,\nincluding those for language, image, and video generation. The code is\navailable at https://github.com/thu-ml/SageAttention.",
      "tldr_zh": "该论文提出 SageAttention2，一种高效注意力机制，通过使用 4-bit 矩阵乘法（Matmul）以及额外的精度增强技术，进一步加速注意力计算同时保持准确性。具体方法包括以线程级粒度将矩阵 Q 和 K 量化到 INT4，并将矩阵 P 和 V 量化到 FP8；引入 Q 平滑方法提高 INT4 QK^T 的准确性；以及采用两级累积策略增强 FP8 PV 的精度。实验结果显示，SageAttention2 在 RTX4090 上比 FlashAttention2 和 xformers 的 OPS 分别高出约 3x 和 4.5x，并在 Hopper GPUs 上与 FlashAttention3(fp8) 速度相当但准确性更高；此外，在语言、图像和视频生成模型上，端到端指标损失微不足道，代码已开源于 https://github.com/thu-ml/SageAttention。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.NE",
        "cs.PF"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.10958v4",
      "published_date": "2024-11-17 04:35:49 UTC",
      "updated_date": "2025-02-10 14:02:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T01:25:45.583336"
    },
    {
      "arxiv_id": "2411.10957v1",
      "title": "IMPaCT GNN: Imposing invariance with Message Passing in Chronological split Temporal Graphs",
      "title_zh": "翻译失败",
      "authors": [
        "Sejun Park",
        "Joo Young Park",
        "Hyunwoo Park"
      ],
      "abstract": "This paper addresses domain adaptation challenges in graph data resulting\nfrom chronological splits. In a transductive graph learning setting, where each\nnode is associated with a timestamp, we focus on the task of Semi-Supervised\nNode Classification (SSNC), aiming to classify recent nodes using labels of\npast nodes. Temporal dependencies in node connections create domain shifts,\ncausing significant performance degradation when applying models trained on\nhistorical data into recent data. Given the practical relevance of this\nscenario, addressing domain adaptation in chronological split data is crucial,\nyet underexplored. We propose Imposing invariance with Message Passing in\nChronological split Temporal Graphs (IMPaCT), a method that imposes invariant\nproperties based on realistic assumptions derived from temporal graph\nstructures. Unlike traditional domain adaptation approaches which rely on\nunverifiable assumptions, IMPaCT explicitly accounts for the characteristics of\nchronological splits. The IMPaCT is further supported by rigorous mathematical\nanalysis, including a derivation of an upper bound of the generalization error.\nExperimentally, IMPaCT achieves a 3.8% performance improvement over current\nSOTA method on the ogbn-mag graph dataset. Additionally, we introduce the\nTemporal Stochastic Block Model (TSBM), which replicates temporal graphs under\nvarying conditions, demonstrating the applicability of our methods to general\nspatial GNNs.",
      "tldr_zh": "这篇论文针对时间序列图（Chronological split Temporal Graphs）中的领域适应挑战，提出IMPaCT方法，用于半监督节点分类（Semi-Supervised Node Classification, SSNC），旨在通过消息传递（Message Passing）强加不变性来缓解时间依赖性导致的领域偏移。IMPaCT基于时间图结构的现实假设进行设计，避免了传统领域适应方法依赖不可验证假设的局限，并通过严格的数学分析导出了泛化误差的上界。实验结果显示，在ogbn-mag数据集上，IMPaCT比当前SOTA方法提升3.8%。此外，论文引入了Temporal Stochastic Block Model (TSBM)来模拟各种时间图，证明了该方法对一般空间GNNs的适用性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "11 pages (without appendix), 35 pages (with appendix), 14 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.10957v1",
      "published_date": "2024-11-17 04:23:25 UTC",
      "updated_date": "2024-11-17 04:23:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T01:25:57.589570"
    },
    {
      "arxiv_id": "2412.03573v1",
      "title": "Improving Tool Retrieval by Leveraging Large Language Models for Query Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Mohammad Kachuee",
        "Sarthak Ahuja",
        "Vaibhav Kumar",
        "Puyang Xu",
        "Xiaohu Liu"
      ],
      "abstract": "Using tools by Large Language Models (LLMs) is a promising avenue to extend\ntheir reach beyond language or conversational settings. The number of tools can\nscale to thousands as they enable accessing sensory information, fetching\nupdated factual knowledge, or taking actions in the real world. In such\nsettings, in-context learning by providing a short list of relevant tools in\nthe prompt is a viable approach. To retrieve relevant tools, various approaches\nhave been suggested, ranging from simple frequency-based matching to dense\nembedding-based semantic retrieval. However, such approaches lack the\ncontextual and common-sense understanding required to retrieve the right tools\nfor complex user requests. Rather than increasing the complexity of the\nretrieval component itself, we propose leveraging LLM understanding to generate\na retrieval query. Then, the generated query is embedded and used to find the\nmost relevant tools via a nearest-neighbor search. We investigate three\napproaches for query generation: zero-shot prompting, supervised fine-tuning on\ntool descriptions, and alignment learning by iteratively optimizing a reward\nmetric measuring retrieval performance. By conducting extensive experiments on\na dataset covering complex and multi-tool scenarios, we show that leveraging\nLLMs for query generation improves the retrieval for in-domain (seen tools) and\nout-of-domain (unseen tools) settings.",
      "tldr_zh": "该论文提出一种新方法，通过利用 Large Language Models (LLMs) 生成检索查询来改进工具检索，解决传统方法（如基于频率或密集嵌入的语义检索）在处理复杂用户请求时缺乏上下文和常识理解的问题。方法包括三种查询生成策略：zero-shot prompting、supervised fine-tuning on tool descriptions，以及alignment learning 通过迭代优化奖励指标来提升检索性能。在覆盖复杂和多工具场景的数据集上进行实验，结果显示，该方法在 in-domain（已见工具）和 out-of-domain（未见工具）设置中均显著提高了工具检索的准确性。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.03573v1",
      "published_date": "2024-11-17 03:02:09 UTC",
      "updated_date": "2024-11-17 03:02:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T01:26:08.558215"
    },
    {
      "arxiv_id": "2411.10928v1",
      "title": "Learn from Downstream and Be Yourself in Multimodal Large Language Model Fine-Tuning",
      "title_zh": "翻译失败",
      "authors": [
        "Wenke Huang",
        "Jian Liang",
        "Zekun Shi",
        "Didi Zhu",
        "Guancheng Wan",
        "He Li",
        "Bo Du",
        "Dacheng Tao",
        "Mang Ye"
      ],
      "abstract": "Multimodal Large Language Model (MLLM) have demonstrated strong\ngeneralization capabilities across diverse distributions and tasks, largely due\nto extensive pre-training datasets. Fine-tuning MLLM has become a common\npractice to improve performance on specific downstream tasks. However, during\nfine-tuning, MLLM often faces the risk of forgetting knowledge acquired during\npre-training, which can result in a decline in generalization abilities. To\nbalance the trade-off between generalization and specialization, we propose\nmeasuring the parameter importance for both pre-trained and fine-tuning\ndistributions, based on frozen pre-trained weight magnitude and accumulated\nfine-tuning gradient values. We further apply an importance-aware weight\nallocation strategy, selectively updating relatively important parameters for\ndownstream tasks. We conduct empirical evaluations on both image captioning and\nvisual question-answering tasks using various MLLM architectures. The\ncomprehensive experimental analysis demonstrates the effectiveness of the\nproposed solution, highlighting the efficiency of the crucial modules in\nenhancing downstream specialization performance while mitigating generalization\ndegradation in MLLM Fine-Tuning.",
      "tldr_zh": "本文研究发现，多模态大语言模型 (MLLM) 在 fine-tuning 过程中容易遗忘预训练知识，导致泛化能力下降。针对此问题，论文提出一种基于参数重要性的方法，通过测量预训练权重大小和微调梯度累积值，并应用重要性感知权重分配策略，选择性更新对下游任务关键的参数。实验在图像字幕和视觉问答任务上验证了该方法的有效性，使用各种 MLLM 架构后，显著提升了下游任务的专业性能，同时缓解了泛化能力的退化。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.10928v1",
      "published_date": "2024-11-17 01:16:37 UTC",
      "updated_date": "2024-11-17 01:16:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T01:26:20.632031"
    },
    {
      "arxiv_id": "2411.10924v1",
      "title": "Hyperspectral Imaging-Based Grain Quality Assessment With Limited Labelled Data",
      "title_zh": "翻译失败",
      "authors": [
        "Priyabrata Karmakar",
        "Manzur Murshed",
        "Shyh Wei Teng"
      ],
      "abstract": "Recently hyperspectral imaging (HSI)-based grain quality assessment has\ngained research attention. However, unlike other imaging modalities, HSI data\nlacks sufficient labelled samples required to effectively train deep\nconvolutional neural network (DCNN)-based classifiers. In this paper, we\npresent a novel approach to grain quality assessment using HSI combined with\nfew-shot learning (FSL) techniques. Traditional methods for grain quality\nevaluation, while reliable, are invasive, time-consuming, and costly. HSI\noffers a non-invasive, real-time alternative by capturing both spatial and\nspectral information. However, a significant challenge in applying DCNNs for\nHSI-based grain classification is the need for large labelled databases, which\nare often difficult to obtain. To address this, we explore the use of FSL,\nwhich enables models to perform well with limited labelled data, making it a\npractical solution for real-world applications where rapid deployment is\nrequired. We also explored the application of FSL for the classification of\nhyperspectral images of bulk grains to enable rapid quality assessment at\nvarious receival points in the grain supply chain. We evaluated the performance\nof few-shot classifiers in two scenarios: first, classification of grain types\nseen during training, and second, generalisation to unseen grain types, a\ncrucial feature for real-world applications. In the first scenario, we\nintroduce a novel approach using pre-computed collective class prototypes\n(CCPs) to enhance inference efficiency and robustness. In the second scenario,\nwe assess the model's ability to classify novel grain types using limited\nsupport examples. Our experimental results show that despite using very limited\nlabelled data for training, our FSL classifiers accuracy is comparable to that\nof a fully trained classifier trained using a significantly larger labelled\ndatabase.",
      "tldr_zh": "本文提出了一种基于超光谱成像 (HSI) 和少样本学习 (FSL) 的谷物质量评估方法，以解决 HSI 数据标记样本不足的问题，从而避免传统方法的侵入性和高成本。研究引入预计算的集体类原型 (CCPs) 来提升已见谷物类型分类的效率和鲁棒性，并评估模型对未见谷物类型的泛化能力。实验结果显示，即使使用有限标记数据，FSL 分类器的准确率可与使用大量数据的 fully trained 分类器相当，为实际应用中的快速部署提供了实用解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages",
      "pdf_url": "http://arxiv.org/pdf/2411.10924v1",
      "published_date": "2024-11-17 01:02:18 UTC",
      "updated_date": "2024-11-17 01:02:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T01:26:33.746136"
    },
    {
      "arxiv_id": "2411.10919v1",
      "title": "Multi-Modal Self-Supervised Learning for Surgical Feedback Effectiveness Assessment",
      "title_zh": "多模态自监督学习用于手术反馈有效性评估",
      "authors": [
        "Arushi Gupta",
        "Rafal Kocielnik",
        "Jiayun Wang",
        "Firdavs Nasriddinov",
        "Cherine Yang",
        "Elyssa Wong",
        "Anima Anandkumar",
        "Andrew Hung"
      ],
      "abstract": "During surgical training, real-time feedback from trainers to trainees is\nimportant for preventing errors and enhancing long-term skill acquisition.\nAccurately predicting the effectiveness of this feedback, specifically whether\nit leads to a change in trainee behavior, is crucial for developing methods for\nimproving surgical training and education. However, relying on human\nannotations to assess feedback effectiveness is laborious and prone to biases,\nunderscoring the need for an automated, scalable, and objective method.\nCreating such an automated system poses challenges, as it requires an\nunderstanding of both the verbal feedback delivered by the trainer and the\nvisual context of the real-time surgical scene. To address this, we propose a\nmethod that integrates information from transcribed verbal feedback and\ncorresponding surgical video to predict feedback effectiveness. Our findings\nshow that both transcribed feedback and surgical video are individually\npredictive of trainee behavior changes, and their combination achieves an AUROC\nof 0.70+/-0.02, improving prediction accuracy by up to 6.6%. Additionally, we\nintroduce self-supervised fine-tuning as a strategy for enhancing surgical\nvideo representation learning, which is scalable and further enhances\nprediction performance. Our results demonstrate the potential of multi-modal\nlearning to advance the automated assessment of surgical feedback.",
      "tldr_zh": "本研究针对手术训练中反馈有效性的评估问题，提出了一种多模态自-supervised learning 方法，该方法整合转录的口头反馈和手术视频，以预测反馈是否导致学员行为改变。实验结果显示，单独使用反馈或视频均有预测能力，而两者结合可将 AUROC 提升至 0.70±0.02，提升准确率达 6.6%。此外，通过自-supervised fine-tuning 策略增强手术视频表示学习，进一步提高了预测性能，并证明了多模态学习在自动化手术反馈评估中的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "68T07, 68T45, 68U10, 92C50",
        "I.2; I.2.10; I.5.4; I.4.7; J.3; K.3.1"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted as a spotlight proceedings paper at Machine Learning for\n  Health 2024",
      "pdf_url": "http://arxiv.org/pdf/2411.10919v1",
      "published_date": "2024-11-17 00:13:00 UTC",
      "updated_date": "2024-11-17 00:13:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T01:26:44.855065"
    },
    {
      "arxiv_id": "2411.10918v1",
      "title": "LLM-assisted Physical Invariant Extraction for Cyber-Physical Systems Anomaly Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Danial Abshari",
        "Chenglong Fu",
        "Meera Sridhar"
      ],
      "abstract": "Modern industrial infrastructures rely heavily on Cyber-Physical Systems\n(CPS), but these are vulnerable to cyber-attacks with potentially catastrophic\neffects. To reduce these risks, anomaly detection methods based on physical\ninvariants have been developed. However, these methods often require\ndomain-specific expertise to manually define invariants, making them costly and\ndifficult to scale. To address this limitation, we propose a novel approach to\nextract physical invariants from CPS testbeds for anomaly detection. Our\ninsight is that CPS design documentation often contains semantically rich\ndescriptions of physical procedures, which can profile inter-correlated\ndynamics among system components. Leveraging the built-in physics and\nengineering knowledge of recent generative AI models, we aim to automate this\ntraditionally manual process, improving scalability and reducing costs. This\nwork focuses on designing and optimizing a Retrieval-Augmented-Generation (RAG)\nworkflow with a customized prompting system tailored for CPS documentation,\nenabling accurate extraction of semantic information and inference of physical\ninvariants from complex, multimodal content. Then, rather than directly\napplying the inferred invariants for anomaly detection, we introduce an\ninnovative statistics-based learning approach that integrates these invariants\ninto the training dataset. This method addresses limitations such as\nhallucination and concept drift, enhancing the reliability of the model. We\nevaluate our approach on real-world public CPS security dataset which contains\n86 data points and 58 attacking cases. The results show that our approach\nachieves a high precision of 0.923, accurately detecting anomalies while\nminimizing false alarms.",
      "tldr_zh": "本研究提出了一种利用LLM辅助提取物理不变性的方法，用于Cyber-Physical Systems (CPS)异常检测，以解决传统方法依赖手动定义不变性而导致的成本高和扩展难问题。方法包括设计一个Retrieval-Augmented-Generation (RAG)工作流和自定义提示系统，从CPS设计文档的多模态内容中自动提取语义信息并推断物理不变性，同时引入基于统计学的学习方法，将这些不变性整合到训练数据集，以缓解幻觉和概念漂移问题。实验在真实数据集上进行，包含86个数据点和58个攻击案例，结果显示该方法实现了0.923的精确度，显著提高了异常检测的可靠性和准确性。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.10918v1",
      "published_date": "2024-11-17 00:09:04 UTC",
      "updated_date": "2024-11-17 00:09:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T01:26:56.926333"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 41,
  "processed_papers_count": 41,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-21T01:27:16.333696"
}
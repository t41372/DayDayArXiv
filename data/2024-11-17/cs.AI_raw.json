[
  {
    "arxiv_id": "2411.11206v1",
    "title": "Capturing Sparks of Abstraction for the ARC Challenge",
    "authors": [
      "Martin Andrews"
    ],
    "abstract": "Excellent progress has been made recently in solving ARC Challenge problems.\nHowever, it seems that new techniques may be required to push beyond 60%\naccuracy. Even commercial Large Language Models (LLMs) struggle to 'understand'\nmany of the problems (when given the input and output grids), which makes\ndiscovering solutions by LLM-lead program search somewhat futile.\n  In this work, LLM 'understanding' is attempted from a stronger starting\nposition : An LLM is given complete solutions to tasks in code, and then asked\nto explain how the task is being solved at various levels of abstraction.\nSpecifically, the LLM was given code solutions implemented in arc-dsl-llm (an\nLLM-legible version of Hodel's arc-dsl to obtain: (a) commented code; (b) code\nrefactored into reusable functional chunks; (c) problem solution steps; and (d)\nhigh-level problem-solving tactics.\n  We demonstrate that 'Sparks of Abstraction' can be extracted from the LLM\noutput - in a form that could be used in downstream tasks with Local LLMs\neligible to enter the ARC Prize.\n  Both the arc-dsl-llm DSL framework (with the re-engineered solutions) and the\nGemini LLM-generated data (along with the generation code) are made Open\nSource.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Submitted as a paper entry for the 2024 ARC Prize",
    "pdf_url": "http://arxiv.org/pdf/2411.11206v1",
    "published_date": "2024-11-17 23:40:00 UTC",
    "updated_date": "2024-11-17 23:40:00 UTC"
  },
  {
    "arxiv_id": "2411.11913v1",
    "title": "On-Board Vision-Language Models for Personalized Autonomous Vehicle Motion Control: System Design and Real-World Validation",
    "authors": [
      "Can Cui",
      "Zichong Yang",
      "Yupeng Zhou",
      "Juntong Peng",
      "Sung-Yeon Park",
      "Cong Zhang",
      "Yunsheng Ma",
      "Xu Cao",
      "Wenqian Ye",
      "Yiheng Feng",
      "Jitesh Panchal",
      "Lingxi Li",
      "Yaobin Chen",
      "Ziran Wang"
    ],
    "abstract": "Personalized driving refers to an autonomous vehicle's ability to adapt its\ndriving behavior or control strategies to match individual users' preferences\nand driving styles while maintaining safety and comfort standards. However,\nexisting works either fail to capture every individual preference precisely or\nbecome computationally inefficient as the user base expands. Vision-Language\nModels (VLMs) offer promising solutions to this front through their natural\nlanguage understanding and scene reasoning capabilities. In this work, we\npropose a lightweight yet effective on-board VLM framework that provides\nlow-latency personalized driving performance while maintaining strong reasoning\ncapabilities. Our solution incorporates a Retrieval-Augmented Generation\n(RAG)-based memory module that enables continuous learning of individual\ndriving preferences through human feedback. Through comprehensive real-world\nvehicle deployment and experiments, our system has demonstrated the ability to\nprovide safe, comfortable, and personalized driving experiences across various\nscenarios and significantly reduce takeover rates by up to 76.9%. To the best\nof our knowledge, this work represents the first end-to-end VLM-based motion\ncontrol system in real-world autonomous vehicles.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "arXiv admin note: substantial text overlap with arXiv:2410.15281",
    "pdf_url": "http://arxiv.org/pdf/2411.11913v1",
    "published_date": "2024-11-17 23:20:37 UTC",
    "updated_date": "2024-11-17 23:20:37 UTC"
  },
  {
    "arxiv_id": "2411.11196v1",
    "title": "PickScan: Object discovery and reconstruction from handheld interactions",
    "authors": [
      "Vincent van der Brugge",
      "Marc Pollefeys",
      "Joshua B. Tenenbaum",
      "Ayush Tewari",
      "Krishna Murthy Jatavallabhula"
    ],
    "abstract": "Reconstructing compositional 3D representations of scenes, where each object\nis represented with its own 3D model, is a highly desirable capability in\nrobotics and augmented reality. However, most existing methods rely heavily on\nstrong appearance priors for object discovery, therefore only working on those\nclasses of objects on which the method has been trained, or do not allow for\nobject manipulation, which is necessary to scan objects fully and to guide\nobject discovery in challenging scenarios. We address these limitations with a\nnovel interaction-guided and class-agnostic method based on object\ndisplacements that allows a user to move around a scene with an RGB-D camera,\nhold up objects, and finally outputs one 3D model per held-up object. Our main\ncontribution to this end is a novel approach to detecting user-object\ninteractions and extracting the masks of manipulated objects. On a\ncustom-captured dataset, our pipeline discovers manipulated objects with 78.3%\nprecision at 100% recall and reconstructs them with a mean chamfer distance of\n0.90cm. Compared to Co-Fusion, the only comparable interaction-based and\nclass-agnostic baseline, this corresponds to a reduction in chamfer distance of\n73% while detecting 99% fewer false positives.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG",
      "cs.RO",
      "I.4.5"
    ],
    "primary_category": "cs.CV",
    "comment": "7 pages, 8 figures, published in the 2024 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS 2024)",
    "pdf_url": "http://arxiv.org/pdf/2411.11196v1",
    "published_date": "2024-11-17 23:09:08 UTC",
    "updated_date": "2024-11-17 23:09:08 UTC"
  },
  {
    "arxiv_id": "2411.11912v2",
    "title": "F$^3$OCUS -- Federated Finetuning of Vision-Language Foundation Models with Optimal Client Layer Updating Strategy via Multi-objective Meta-Heuristics",
    "authors": [
      "Pramit Saha",
      "Felix Wagner",
      "Divyanshu Mishra",
      "Can Peng",
      "Anshul Thakur",
      "David Clifton",
      "Konstantinos Kamnitsas",
      "J. Alison Noble"
    ],
    "abstract": "Effective training of large Vision-Language Models (VLMs) on\nresource-constrained client devices in Federated Learning (FL) requires the\nusage of parameter-efficient fine-tuning (PEFT) strategies. To this end, we\ndemonstrate the impact of two factors \\textit{viz.}, client-specific layer\nimportance score that selects the most important VLM layers for fine-tuning and\ninter-client layer diversity score that encourages diverse layer selection\nacross clients for optimal VLM layer selection. We first theoretically motivate\nand leverage the principal eigenvalue magnitude of layerwise Neural Tangent\nKernels and show its effectiveness as client-specific layer importance score.\nNext, we propose a novel layer updating strategy dubbed F$^3$OCUS that jointly\noptimizes the layer importance and diversity factors by employing a data-free,\nmulti-objective, meta-heuristic optimization on the server. We explore 5\ndifferent meta-heuristic algorithms and compare their effectiveness for\nselecting model layers and adapter layers towards PEFT-FL. Furthermore, we\nrelease a new MedVQA-FL dataset involving overall 707,962 VQA triplets and 9\nmodality-specific clients and utilize it to train and evaluate our method.\nOverall, we conduct more than 10,000 client-level experiments on 6\nVision-Language FL task settings involving 58 medical image datasets and 4\ndifferent VLM architectures of varying sizes to demonstrate the effectiveness\nof the proposed method.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted in CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2411.11912v2",
    "published_date": "2024-11-17 21:54:57 UTC",
    "updated_date": "2025-03-30 10:30:03 UTC"
  },
  {
    "arxiv_id": "2411.11182v1",
    "title": "Improving User Experience in Preference-Based Optimization of Reward Functions for Assistive Robots",
    "authors": [
      "Nathaniel Dennler",
      "Zhonghao Shi",
      "Stefanos Nikolaidis",
      "Maja MatariÄ‡"
    ],
    "abstract": "Assistive robots interact with humans and must adapt to different users'\npreferences to be effective. An easy and effective technique to learn\nnon-expert users' preferences is through rankings of robot behaviors, for\nexample, robot movement trajectories or gestures. Existing techniques focus on\ngenerating trajectories for users to rank that maximize the outcome of the\npreference learning process. However, the generated trajectories do not appear\nto reflect the user's preference over repeated interactions. In this work, we\ndesign an algorithm to generate trajectories for users to rank that we call\nCovariance Matrix Adaptation Evolution Strategies with Information Gain\n(CMA-ES-IG). CMA-ES-IG prioritizes the user's experience of the preference\nlearning process. We show that users find our algorithm more intuitive and\neasier to use than previous approaches across both physical and social robot\ntasks. This project's code is hosted at github.com/interaction-lab/CMA-ES-IG",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted to ISRR",
    "pdf_url": "http://arxiv.org/pdf/2411.11182v1",
    "published_date": "2024-11-17 21:52:58 UTC",
    "updated_date": "2024-11-17 21:52:58 UTC"
  },
  {
    "arxiv_id": "2411.11179v1",
    "title": "Enhanced Anime Image Generation Using USE-CMHSA-GAN",
    "authors": [
      "J. Lu"
    ],
    "abstract": "With the growing popularity of ACG (Anime, Comics, and Games) culture,\ngenerating high-quality anime character images has become an important research\ntopic. This paper introduces a novel Generative Adversarial Network model,\nUSE-CMHSA-GAN, designed to produce high-quality anime character images. The\nmodel builds upon the traditional DCGAN framework, incorporating USE and CMHSA\nmodules to enhance feature extraction capabilities for anime character images.\nExperiments were conducted on the anime-face-dataset, and the results\ndemonstrate that USE-CMHSA-GAN outperforms other benchmark models, including\nDCGAN, VAE-GAN, and WGAN, in terms of FID and IS scores, indicating superior\nimage quality. These findings suggest that USE-CMHSA-GAN is highly effective\nfor anime character image generation and provides new insights for further\nimproving the quality of generative models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.11179v1",
    "published_date": "2024-11-17 21:25:24 UTC",
    "updated_date": "2024-11-17 21:25:24 UTC"
  },
  {
    "arxiv_id": "2411.11171v2",
    "title": "LLÃ¤Mmlein: Compact and Competitive German-Only Language Models from Scratch",
    "authors": [
      "Jan Pfister",
      "Julia Wunderle",
      "Andreas Hotho"
    ],
    "abstract": "We create two German-only decoder models, LL\\\"aMmlein 120M and 1B,\ntransparently from scratch and publish them, along with the training data, for\nthe German NLP research community to use. The model training involved several\nkey steps, including extensive data preprocessing, the creation of a custom\nGerman tokenizer, the training itself, as well as the evaluation of the final\nmodels on various benchmarks. Throughout the training process, multiple\ncheckpoints were saved and analyzed using the SuperGLEBer benchmark to monitor\nthe models' learning dynamics. Compared to state-of-the-art models on the\nSuperGLEBer benchmark, both LL\\\"aMmlein models performed competitively,\nconsistently matching or surpassing models with similar parameter sizes. The\nresults show that the models' quality scales with size as expected, but\nperformance improvements on some tasks plateaued early, offering valuable\ninsights into resource allocation for future model development.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "second draft;\n  https://www.informatik.uni-wuerzburg.de/datascience/projects/nlp/llammlein/",
    "pdf_url": "http://arxiv.org/pdf/2411.11171v2",
    "published_date": "2024-11-17 20:44:34 UTC",
    "updated_date": "2024-12-16 12:29:41 UTC"
  },
  {
    "arxiv_id": "2411.12764v1",
    "title": "SEFD: Semantic-Enhanced Framework for Detecting LLM-Generated Text",
    "authors": [
      "Weiqing He",
      "Bojian Hou",
      "Tianqi Shang",
      "Davoud Ataee Tarzanagh",
      "Qi Long",
      "Li Shen"
    ],
    "abstract": "The widespread adoption of large language models (LLMs) has created an urgent\nneed for robust tools to detect LLM-generated text, especially in light of\n\\textit{paraphrasing} techniques that often evade existing detection methods.\nTo address this challenge, we present a novel semantic-enhanced framework for\ndetecting LLM-generated text (SEFD) that leverages a retrieval-based mechanism\nto fully utilize text semantics. Our framework improves upon existing detection\nmethods by systematically integrating retrieval-based techniques with\ntraditional detectors, employing a carefully curated retrieval mechanism that\nstrikes a balance between comprehensive coverage and computational efficiency.\nWe showcase the effectiveness of our approach in sequential text scenarios\ncommon in real-world applications, such as online forums and Q\\&A platforms.\nThrough comprehensive experiments across various LLM-generated texts and\ndetection methods, we demonstrate that our framework substantially enhances\ndetection accuracy in paraphrasing scenarios while maintaining robustness for\nstandard LLM-generated content.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12764v1",
    "published_date": "2024-11-17 20:13:30 UTC",
    "updated_date": "2024-11-17 20:13:30 UTC"
  },
  {
    "arxiv_id": "2411.11162v1",
    "title": "RPN 2: On Interdependence Function Learning Towards Unifying and Advancing CNN, RNN, GNN, and Transformer",
    "authors": [
      "Jiawei Zhang"
    ],
    "abstract": "This paper builds upon our previous work on the Reconciled Polynomial Network\n(RPN). The original RPN model was designed under the assumption of input data\nindependence, presuming the independence among both individual instances within\ndata batches and attributes in each data instance. However, this assumption\noften proves invalid for function learning tasks involving complex,\ninterdependent data such as language, images, time series, and graphs. Ignoring\nsuch data interdependence may inevitably lead to significant performance\ndegradation.\n  To overcome these limitations, we introduce the new Reconciled Polynomial\nNetwork (version 2), namely RPN 2, in this paper. By incorporating data and\nstructural interdependence functions, RPN 2 explicitly models data\ninterdependence via new component functions in its architecture.\n  This enhancement not only significantly improves RPN 2's learning performance\nbut also substantially expands its unifying potential, enabling it to encompass\na broader range of contemporary dominant backbone models within its canonical\nrepresentation. These backbones include, but are not limited to, convolutional\nneural networks (CNNs), recurrent neural networks (RNNs), graph neural networks\n(GNNs), and Transformers. Our analysis reveals that the fundamental\ndistinctions among these backbone models primarily stem from their diverse\napproaches to defining the interdependence functions. Furthermore, this unified\nrepresentation opens up new opportunities for designing innovative\narchitectures with the potential to surpass the performance of these dominant\nbackbones.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.IT",
      "math.IT",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "105 pages, 37 figures, 6 tables, preprint version",
    "pdf_url": "http://arxiv.org/pdf/2411.11162v1",
    "published_date": "2024-11-17 19:45:26 UTC",
    "updated_date": "2024-11-17 19:45:26 UTC"
  },
  {
    "arxiv_id": "2411.11161v1",
    "title": "MPLite: Multi-Aspect Pretraining for Mining Clinical Health Records",
    "authors": [
      "Eric Yang",
      "Pengfei Hu",
      "Xiaoxue Han",
      "Yue Ning"
    ],
    "abstract": "The adoption of digital systems in healthcare has resulted in the\naccumulation of vast electronic health records (EHRs), offering valuable data\nfor machine learning methods to predict patient health outcomes. However,\nsingle-visit records of patients are often neglected in the training process\ndue to the lack of annotations of next-visit information, thereby limiting the\npredictive and expressive power of machine learning models. In this paper, we\npresent a novel framework MPLite that utilizes Multi-aspect Pretraining with\nLab results through a light-weight neural network to enhance medical concept\nrepresentation and predict future health outcomes of individuals. By\nincorporating both structured medical data and additional information from lab\nresults, our approach fully leverages patient admission records. We design a\npretraining module that predicts medical codes based on lab results, ensuring\nrobust prediction by fusing multiple aspects of features. Our experimental\nevaluation using both MIMIC-III and MIMIC-IV datasets demonstrates improvements\nover existing models in diagnosis prediction and heart failure prediction\ntasks, achieving a higher weighted-F1 and recall with MPLite. This work reveals\nthe potential of integrating diverse aspects of data to advance predictive\nmodeling in healthcare.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.11161v1",
    "published_date": "2024-11-17 19:43:10 UTC",
    "updated_date": "2024-11-17 19:43:10 UTC"
  },
  {
    "arxiv_id": "2411.11148v1",
    "title": "TabDeco: A Comprehensive Contrastive Framework for Decoupled Representations in Tabular Data",
    "authors": [
      "Suiyao Chen",
      "Jing Wu",
      "Yunxiao Wang",
      "Cheng Ji",
      "Tianpei Xie",
      "Daniel Cociorva",
      "Michael Sharps",
      "Cecile Levasseur",
      "Hakan Brunzell"
    ],
    "abstract": "Representation learning is a fundamental aspect of modern artificial\nintelligence, driving substantial improvements across diverse applications.\nWhile selfsupervised contrastive learning has led to significant advancements\nin fields like computer vision and natural language processing, its adaptation\nto tabular data presents unique challenges. Traditional approaches often\nprioritize optimizing model architecture and loss functions but may overlook\nthe crucial task of constructing meaningful positive and negative sample pairs\nfrom various perspectives like feature interactions, instance-level patterns\nand batch-specific contexts. To address these challenges, we introduce TabDeco,\na novel method that leverages attention-based encoding strategies across both\nrows and columns and employs contrastive learning framework to effectively\ndisentangle feature representations at multiple levels, including features,\ninstances and data batches. With the innovative feature decoupling hierarchies,\nTabDeco consistently surpasses existing deep learning methods and leading\ngradient boosting algorithms, including XG-Boost, CatBoost, and LightGBM,\nacross various benchmark tasks, underscoring its effectiveness in advancing\ntabular data representation learning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.11148v1",
    "published_date": "2024-11-17 18:42:46 UTC",
    "updated_date": "2024-11-17 18:42:46 UTC"
  },
  {
    "arxiv_id": "2412.03575v1",
    "title": "Leveraging Large Language Models for Generating Labeled Mineral Site Record Linkage Data",
    "authors": [
      "Jiyoon Pyo",
      "Yao-Yi Chiang"
    ],
    "abstract": "Record linkage integrates diverse data sources by identifying records that\nrefer to the same entity. In the context of mineral site records, accurate\nrecord linkage is crucial for identifying and mapping mineral deposits.\nProperly linking records that refer to the same mineral deposit helps define\nthe spatial coverage of mineral areas, benefiting resource identification and\nsite data archiving. Mineral site record linkage falls under the spatial record\nlinkage category since the records contain information about the physical\nlocations and non-spatial attributes in a tabular format. The task is\nparticularly challenging due to the heterogeneity and vast scale of the data.\nWhile prior research employs pre-trained discriminative language models (PLMs)\non spatial entity linkage, they often require substantial amounts of curated\nground-truth data for fine-tuning. Gathering and creating ground truth data is\nboth time-consuming and costly. Therefore, such approaches are not always\nfeasible in real-world scenarios where gold-standard data are unavailable.\nAlthough large generative language models (LLMs) have shown promising results\nin various natural language processing tasks, including record linkage, their\nhigh inference time and resource demand present challenges. We propose a method\nthat leverages an LLM to generate training data and fine-tune a PLM to address\nthe training data gap while preserving the efficiency of PLMs. Our approach\nachieves over 45\\% improvement in F1 score for record linkage compared to\ntraditional PLM-based methods using ground truth data while reducing the\ninference time by nearly 18 times compared to relying on LLMs. Additionally, we\noffer an automated pipeline that eliminates the need for human intervention,\nhighlighting this approach's potential to overcome record linkage challenges.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "11 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.03575v1",
    "published_date": "2024-11-17 18:26:56 UTC",
    "updated_date": "2024-11-17 18:26:56 UTC"
  },
  {
    "arxiv_id": "2411.11144v1",
    "title": "CLMIA: Membership Inference Attacks via Unsupervised Contrastive Learning",
    "authors": [
      "Depeng Chen",
      "Xiao Liu",
      "Jie Cui",
      "Hong Zhong"
    ],
    "abstract": "Since machine learning model is often trained on a limited data set, the\nmodel is trained multiple times on the same data sample, which causes the model\nto memorize most of the training set data. Membership Inference Attacks (MIAs)\nexploit this feature to determine whether a data sample is used for training a\nmachine learning model. However, in realistic scenarios, it is difficult for\nthe adversary to obtain enough qualified samples that mark accurate identity\ninformation, especially since most samples are non-members in real world\napplications. To address this limitation, in this paper, we propose a new\nattack method called CLMIA, which uses unsupervised contrastive learning to\ntrain an attack model without using extra membership status information.\nMeanwhile, in CLMIA, we require only a small amount of data with known\nmembership status to fine-tune the attack model. Experimental results\ndemonstrate that CLMIA performs better than existing attack methods for\ndifferent datasets and model structures, especially with data with less marked\nidentity information. In addition, we experimentally find that the attack\nperforms differently for different proportions of labeled identity information\nfor member and non-member data. More analysis proves that our attack method\nperforms better with less labeled identity information, which applies to more\nrealistic scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.11144v1",
    "published_date": "2024-11-17 18:25:01 UTC",
    "updated_date": "2024-11-17 18:25:01 UTC"
  },
  {
    "arxiv_id": "2411.13585v1",
    "title": "Artificial Intelligence in Cybersecurity: Building Resilient Cyber Diplomacy Frameworks",
    "authors": [
      "Michael Stoltz"
    ],
    "abstract": "This paper explores how automation and artificial intelligence (AI) are\ntransforming U.S. cyber diplomacy. Leveraging these technologies helps the U.S.\nmanage the complexity and urgency of cyber diplomacy, improving\ndecision-making, efficiency, and security. As global inter connectivity grows,\ncyber diplomacy, managing national interests in the digital space has become\nvital. The ability of AI and automation to quickly process vast data volumes\nenables timely responses to cyber threats and opportunities. This paper\nunderscores the strategic integration of these tools to maintain U.S.\ncompetitive advantage and secure national interests. Automation enhances\ndiplomatic communication and data processing, freeing diplomats to focus on\nstrategic decisions. AI supports predictive analytics and real time decision\nmaking, offering critical insights and proactive measures during high stakes\nengagements. Case studies show AIs effectiveness in monitoring cyber activities\nand managing international cyber policy. Challenges such as ethical concerns,\nsecurity vulnerabilities, and reliance on technology are also addressed,\nemphasizing human oversight and strong governance frameworks. Ensuring proper\nethical guidelines and cybersecurity measures allows the U.S. to harness the\nbenefits of automation and AI while mitigating risks. By adopting these\ntechnologies, U.S. cyber diplomacy can become more proactive and effective,\nnavigating the evolving digital landscape with greater agility.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.13585v1",
    "published_date": "2024-11-17 17:57:17 UTC",
    "updated_date": "2024-11-17 17:57:17 UTC"
  },
  {
    "arxiv_id": "2411.17711v2",
    "title": "AnyECG: Foundational Models for Multitask Cardiac Analysis in Real-World Settings",
    "authors": [
      "Yue Wang",
      "Xu Cao",
      "Yaojun Hu",
      "Haochao Ying",
      "Hongxia Xu",
      "Ruijia Wu",
      "James Matthew Rehg",
      "Jimeng Sun",
      "Jian Wu",
      "Jintai Chen"
    ],
    "abstract": "Electrocardiogram (ECG), a non-invasive and affordable tool for cardiac\nmonitoring, is highly sensitive in detecting acute heart attacks. However, due\nto the lengthy nature of ECG recordings, numerous machine learning methods have\nbeen developed for automated heart disease detection to reduce human workload.\nDespite these efforts, performance remains suboptimal. A key obstacle is the\ninherent complexity of ECG data, which includes heterogeneity (e.g., varying\nsampling rates), high levels of noise, demographic-related pattern shifts, and\nintricate rhythm-event associations. To overcome these challenges, this paper\nintroduces AnyECG, a foundational model designed to extract robust\nrepresentations from any real-world ECG data. Specifically, a tailored ECG\nTokenizer encodes each fixed-duration ECG fragment into a token and, guided by\nproxy tasks, converts noisy, continuous ECG features into discrete, compact,\nand clinically meaningful local rhythm codes. These codes encapsulate basic\nmorphological, frequency, and demographic information (e.g., sex), effectively\nmitigating signal noise. We further pre-train the AnyECG to learn rhythmic\npattern associations across ECG tokens, enabling the capture of cardiac event\nsemantics. By being jointly pre-trained on diverse ECG data sources, AnyECG is\ncapable of generalizing across a wide range of downstream tasks where ECG\nsignals are recorded from various devices and scenarios. The experimental\nresults show that AnyECG achieves an average performance improvement of 6%\nacross four critical tasks-anomaly detection, arrhythmia classification,\ncorrupted lead generation, and ultra-long ECG recognition. AnyECG learns common\nECG rhythm from data and significantly outperforms state-of-the-art methods in\neach of these tasks.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.17711v2",
    "published_date": "2024-11-17 17:32:58 UTC",
    "updated_date": "2025-03-03 13:19:42 UTC"
  },
  {
    "arxiv_id": "2411.11911v2",
    "title": "ModeSeq: Taming Sparse Multimodal Motion Prediction with Sequential Mode Modeling",
    "authors": [
      "Zikang Zhou",
      "Hengjian Zhou",
      "Haibo Hu",
      "Zihao Wen",
      "Jianping Wang",
      "Yung-Hui Li",
      "Yu-Kai Huang"
    ],
    "abstract": "Anticipating the multimodality of future events lays the foundation for safe\nautonomous driving. However, multimodal motion prediction for traffic agents\nhas been clouded by the lack of multimodal ground truth. Existing works\npredominantly adopt the winner-take-all training strategy to tackle this\nchallenge, yet still suffer from limited trajectory diversity and uncalibrated\nmode confidence. While some approaches address these limitations by generating\nexcessive trajectory candidates, they necessitate a post-processing stage to\nidentify the most representative modes, a process lacking universal principles\nand compromising trajectory accuracy. We are thus motivated to introduce\nModeSeq, a new multimodal prediction paradigm that models modes as sequences.\nUnlike the common practice of decoding multiple plausible trajectories in one\nshot, ModeSeq requires motion decoders to infer the next mode step by step,\nthereby more explicitly capturing the correlation between modes and\nsignificantly enhancing the ability to reason about multimodality. Leveraging\nthe inductive bias of sequential mode prediction, we also propose the\nEarly-Match-Take-All (EMTA) training strategy to diversify the trajectories\nfurther. Without relying on dense mode prediction or heuristic post-processing,\nModeSeq considerably improves the diversity of multimodal output while\nattaining satisfactory trajectory accuracy, resulting in balanced performance\non motion prediction benchmarks. Moreover, ModeSeq naturally emerges with the\ncapability of mode extrapolation, which supports forecasting more behavior\nmodes when the future is highly uncertain.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2411.11911v2",
    "published_date": "2024-11-17 16:36:09 UTC",
    "updated_date": "2025-03-23 14:46:55 UTC"
  },
  {
    "arxiv_id": "2411.11105v1",
    "title": "Label Sharing Incremental Learning Framework for Independent Multi-Label Segmentation Tasks",
    "authors": [
      "Deepa Anand",
      "Bipul Das",
      "Vyshnav Dangeti",
      "Antony Jerald",
      "Rakesh Mullick",
      "Uday Patil",
      "Pakhi Sharma",
      "Prasad Sudhakar"
    ],
    "abstract": "In a setting where segmentation models have to be built for multiple\ndatasets, each with its own corresponding label set, a straightforward way is\nto learn one model for every dataset and its labels. Alternatively, multi-task\narchitectures with shared encoders and multiple segmentation heads or shared\nweights with compound labels can also be made use of. This work proposes a\nnovel label sharing framework where a shared common label space is constructed\nand each of the individual label sets are systematically mapped to the common\nlabels. This transforms multiple datasets with disparate label sets into a\nsingle large dataset with shared labels, and therefore all the segmentation\ntasks can be addressed by learning a single model. This eliminates the need for\ntask specific adaptations in network architectures and also results in\nparameter and data efficient models. Furthermore, label sharing framework is\nnaturally amenable for incremental learning where segmentations for new\ndatasets can be easily learnt. We experimentally validate our method on various\nmedical image segmentation datasets, each involving multi-label segmentation.\nFurthermore, we demonstrate the efficacy of the proposed method in terms of\nperformance and incremental learning ability vis-a-vis alternative methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.11105v1",
    "published_date": "2024-11-17 15:50:25 UTC",
    "updated_date": "2024-11-17 15:50:25 UTC"
  },
  {
    "arxiv_id": "2411.11101v2",
    "title": "Different Horses for Different Courses: Comparing Bias Mitigation Algorithms in ML",
    "authors": [
      "Prakhar Ganesh",
      "Usman Gohar",
      "Lu Cheng",
      "Golnoosh Farnadi"
    ],
    "abstract": "With fairness concerns gaining significant attention in Machine Learning\n(ML), several bias mitigation techniques have been proposed, often compared\nagainst each other to find the best method. These benchmarking efforts tend to\nuse a common setup for evaluation under the assumption that providing a uniform\nenvironment ensures a fair comparison. However, bias mitigation techniques are\nsensitive to hyperparameter choices, random seeds, feature selection, etc.,\nmeaning that comparison on just one setting can unfairly favour certain\nalgorithms. In this work, we show significant variance in fairness achieved by\nseveral algorithms and the influence of the learning pipeline on fairness\nscores. We highlight that most bias mitigation techniques can achieve\ncomparable performance, given the freedom to perform hyperparameter\noptimization, suggesting that the choice of the evaluation parameters-rather\nthan the mitigation technique itself-can sometimes create the perceived\nsuperiority of one method over another. We hope our work encourages future\nresearch on how various choices in the lifecycle of developing an algorithm\nimpact fairness, and trends that guide the selection of appropriate algorithms.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "To appear at AFME@NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.11101v2",
    "published_date": "2024-11-17 15:17:08 UTC",
    "updated_date": "2024-11-19 02:39:53 UTC"
  },
  {
    "arxiv_id": "2412.03574v1",
    "title": "Back-filling Missing Data When Predicting Domestic Electricity Consumption From Smart Meter Data",
    "authors": [
      "Xianjuan Chen",
      "Shuxiang Cai",
      "Alan F. Smeaton"
    ],
    "abstract": "This study uses data from domestic electricity smart meters to estimate\nannual electricity bills for a whole year. We develop a method for back-filling\ndata smart meter for up to six missing months for users who have less than one\nyear of smart meter data, ensuring reliable estimates of annual consumption. We\nidentify five distinct electricity consumption user profiles for homes based on\nday, night, and peak usage patterns, highlighting the economic advantages of\nTime-of-Use (ToU) tariffs over fixed tariffs for most users, especially those\nwith higher nighttime consumption. Ultimately, the results of this study\nempowers consumers to manage their energy use effectively and to make informed\nchoices regarding electricity tariff plans.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "10 pages, 7 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2412.03574v1",
    "published_date": "2024-11-17 15:03:59 UTC",
    "updated_date": "2024-11-17 15:03:59 UTC"
  },
  {
    "arxiv_id": "2411.11099v1",
    "title": "Mitigating Relative Over-Generalization in Multi-Agent Reinforcement Learning",
    "authors": [
      "Ting Zhu",
      "Yue Jin",
      "Jeremie Houssineau",
      "Giovanni Montana"
    ],
    "abstract": "In decentralized multi-agent reinforcement learning, agents learning in\nisolation can lead to relative over-generalization (RO), where optimal joint\nactions are undervalued in favor of suboptimal ones. This hinders effective\ncoordination in cooperative tasks, as agents tend to choose actions that are\nindividually rational but collectively suboptimal. To address this issue, we\nintroduce MaxMax Q-Learning (MMQ), which employs an iterative process of\nsampling and evaluating potential next states, selecting those with maximal\nQ-values for learning. This approach refines approximations of ideal state\ntransitions, aligning more closely with the optimal joint policy of\ncollaborating agents. We provide theoretical analysis supporting MMQ's\npotential and present empirical evaluations across various environments\nsusceptible to RO. Our results demonstrate that MMQ frequently outperforms\nexisting baselines, exhibiting enhanced convergence and sample efficiency.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Published in Transactions on Machine Learning Research (11/2024)",
    "pdf_url": "http://arxiv.org/pdf/2411.11099v1",
    "published_date": "2024-11-17 15:00:39 UTC",
    "updated_date": "2024-11-17 15:00:39 UTC"
  },
  {
    "arxiv_id": "2411.14463v1",
    "title": "Leveraging AI and NLP for Bank Marketing: A Systematic Review and Gap Analysis",
    "authors": [
      "Christopher Gerling",
      "Stefan Lessmann"
    ],
    "abstract": "This paper explores the growing impact of AI and NLP in bank marketing,\nhighlighting their evolving roles in enhancing marketing strategies, improving\ncustomer engagement, and creating value within this sector. While AI and NLP\nhave been widely studied in general marketing, there is a notable gap in\nunderstanding their specific applications and potential within the banking\nsector. This research addresses this specific gap by providing a systematic\nreview and strategic analysis of AI and NLP applications in bank marketing,\nfocusing on their integration across the customer journey and operational\nexcellence. Employing the PRISMA methodology, this study systematically reviews\nexisting literature to assess the current landscape of AI and NLP in bank\nmarketing. Additionally, it incorporates semantic mapping using Sentence\nTransformers and UMAP for strategic gap analysis to identify underexplored\nareas and opportunities for future research.\n  The systematic review reveals limited research specifically focused on NLP\napplications in bank marketing. The strategic gap analysis identifies key areas\nwhere NLP can further enhance marketing strategies, including customer-centric\napplications like acquisition, retention, and personalized engagement, offering\nvaluable insights for both academic research and practical implementation. This\nresearch contributes to the field of bank marketing by mapping the current\nstate of AI and NLP applications and identifying strategic gaps. The findings\nprovide actionable insights for developing NLP-driven growth and innovation\nframeworks and highlight the role of NLP in improving operational efficiency\nand regulatory compliance. This work has broader implications for enhancing\ncustomer experience, profitability, and innovation in the banking industry.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "econ.GN",
      "q-fin.EC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.14463v1",
    "published_date": "2024-11-17 14:44:12 UTC",
    "updated_date": "2024-11-17 14:44:12 UTC"
  },
  {
    "arxiv_id": "2411.11910v2",
    "title": "AIGS: Generating Science from AI-Powered Automated Falsification",
    "authors": [
      "Zijun Liu",
      "Kaiming Liu",
      "Yiqi Zhu",
      "Xuanyu Lei",
      "Zonghan Yang",
      "Zhenhe Zhang",
      "Peng Li",
      "Yang Liu"
    ],
    "abstract": "Rapid development of artificial intelligence has drastically accelerated the\ndevelopment of scientific discovery. Trained with large-scale observation data,\ndeep neural networks extract the underlying patterns in an end-to-end manner\nand assist human researchers with highly-precised predictions in unseen\nscenarios. The recent rise of Large Language Models (LLMs) and the empowered\nautonomous agents enable scientists to gain help through interaction in\ndifferent stages of their research, including but not limited to literature\nreview, research ideation, idea implementation, and academic writing. However,\nAI researchers instantiated by foundation model empowered agents with\nfull-process autonomy are still in their infancy. In this paper, we study\n$\\textbf{AI-Generated Science}$ (AIGS), where agents independently and\nautonomously complete the entire research process and discover scientific laws.\nBy revisiting the definition of scientific research, we argue that\n$\\textit{falsification}$ is the essence of both human research process and the\ndesign of an AIGS system. Through the lens of falsification, prior systems\nattempting towards AI-Generated Science either lack the part in their design,\nor rely heavily on existing verification engines that narrow the use in\nspecialized domains. In this work, we propose Baby-AIGS as a baby-step\ndemonstration of a full-process AIGS system, which is a multi-agent system with\nagents in roles representing key research process. By introducing\nFalsificationAgent, which identify and then verify possible scientific\ndiscoveries, we empower the system with explicit falsification. Experiments on\nthree tasks preliminarily show that Baby-AIGS could produce meaningful\nscientific discoveries, though not on par with experienced human researchers.\nFinally, we discuss on the limitations of current Baby-AIGS, actionable\ninsights, and related ethical issues in detail.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Pre-print. 35 pages. Official website:\n  https://agent-force.github.io/AIGS/",
    "pdf_url": "http://arxiv.org/pdf/2411.11910v2",
    "published_date": "2024-11-17 13:40:35 UTC",
    "updated_date": "2024-11-24 12:59:44 UTC"
  },
  {
    "arxiv_id": "2411.11057v1",
    "title": "Reinforcing Competitive Multi-Agents for Playing So Long Sucker",
    "authors": [
      "Medant Sharan",
      "Chandranath Adak"
    ],
    "abstract": "This paper examines the use of classical deep reinforcement learning (DRL)\nalgorithms, DQN, DDQN, and Dueling DQN, in the strategy game So Long Sucker\n(SLS), a diplomacy-driven game defined by coalition-building and strategic\nbetrayal. SLS poses unique challenges due to its blend of cooperative and\nadversarial dynamics, making it an ideal platform for studying multi-agent\nlearning and game theory. The study's primary goal is to teach autonomous\nagents the game's rules and strategies using classical DRL methods. To support\nthis effort, the authors developed a novel, publicly available implementation\nof SLS, featuring a graphical user interface (GUI) and benchmarking tools for\nDRL algorithms. Experimental results reveal that while considered basic by\nmodern DRL standards, DQN, DDQN, and Dueling DQN agents achieved roughly 50% of\nthe maximum possible game reward. This suggests a baseline understanding of the\ngame's mechanics, with agents favoring legal moves over illegal ones. However,\na significant limitation was the extensive training required, around 2000\ngames, for agents to reach peak performance, compared to human players who\ngrasp the game within a few rounds. Even after prolonged training, agents\noccasionally made illegal moves, highlighting both the potential and\nlimitations of these classical DRL methods in semi-complex, socially driven\ngames. The findings establish a foundational benchmark for training agents in\nSLS and similar negotiation-based environments while underscoring the need for\nadvanced or hybrid DRL approaches to improve learning efficiency and\nadaptability. Future research could incorporate game-theoretic strategies to\nenhance agent decision-making in dynamic multi-agent contexts.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.11057v1",
    "published_date": "2024-11-17 12:38:13 UTC",
    "updated_date": "2024-11-17 12:38:13 UTC"
  },
  {
    "arxiv_id": "2411.11053v5",
    "title": "SRA-MCTS: Self-driven Reasoning Augmentation with Monte Carlo Tree Search for Code Generation",
    "authors": [
      "Bin Xu",
      "Yiguan Lin",
      "Yinghao Li",
      "Yang Gao"
    ],
    "abstract": "Large language models demonstrate exceptional performance in simple code\ngeneration tasks but still face challenges in tackling complex problems. These\nchallenges may stem from insufficient reasoning and problem decomposition\ncapabilities. To address this issue, we propose a reasoning-augmented data\ngeneration process, SRA-MCTS, which guides the model to autonomously generate\nhigh-quality intermediate reasoning paths. This creates a positive feedback\nloop, enabling continuous improvement. Our method operates entirely through the\nmodel itself without requiring additional supervision. By synthesizing natural\nlanguage reasoning paths and translating them into executable code, the\napproach ensures analytical accuracy and enhances the success rate in solving\ncomplex tasks. Experimental results show that, even without additional\nsupervisory signals, our method achieves performance improvements across\ndifferent model scales, demonstrating the significant potential of\nself-improvement in small models. Furthermore, the method remains robust when\ntraditional Chain-of-Thought (CoT) approaches exhibit performance degradation,\nwith notable improvements observed in diversity metrics such as pass@10. We\nencourage further exploration of reasoning processes within training data to\nenhance the ability of language models to address complex problems. Our code\nand data are public at https://github.com/DIRECT-BIT/SRA-MCTS.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by IJCAI2025",
    "pdf_url": "http://arxiv.org/pdf/2411.11053v5",
    "published_date": "2024-11-17 12:31:04 UTC",
    "updated_date": "2025-05-09 07:24:54 UTC"
  },
  {
    "arxiv_id": "2411.11046v1",
    "title": "Knowledge-enhanced Transformer for Multivariate Long Sequence Time-series Forecasting",
    "authors": [
      "Shubham Tanaji Kakde",
      "Rony Mitra",
      "Jasashwi Mandal",
      "Manoj Kumar Tiwari"
    ],
    "abstract": "Multivariate Long Sequence Time-series Forecasting (LSTF) has been a critical\ntask across various real-world applications. Recent advancements focus on the\napplication of transformer architectures attributable to their ability to\ncapture temporal patterns effectively over extended periods. However, these\napproaches often overlook the inherent relationships and interactions between\nthe input variables that could be drawn from their characteristic properties.\nIn this paper, we aim to bridge this gap by integrating information-rich\nKnowledge Graph Embeddings (KGE) with state-of-the-art transformer-based\narchitectures. We introduce a novel approach that encapsulates conceptual\nrelationships among variables within a well-defined knowledge graph, forming\ndynamic and learnable KGEs for seamless integration into the transformer\narchitecture. We investigate the influence of this integration into seminal\narchitectures such as PatchTST, Autoformer, Informer, and Vanilla Transformer.\nFurthermore, we thoroughly investigate the performance of these\nknowledge-enhanced architectures along with their original implementations for\nlong forecasting horizons and demonstrate significant improvement in the\nbenchmark results. This enhancement empowers transformer-based architectures to\naddress the inherent structural relation between variables. Our\nknowledge-enhanced approach improves the accuracy of multivariate LSTF by\ncapturing complex temporal and relational dynamics across multiple domains. To\nsubstantiate the validity of our model, we conduct comprehensive experiments\nusing Weather and Electric Transformer Temperature (ETT) datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 4 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2411.11046v1",
    "published_date": "2024-11-17 11:53:54 UTC",
    "updated_date": "2024-11-17 11:53:54 UTC"
  },
  {
    "arxiv_id": "2412.01622v1",
    "title": "Image Forgery Localization via Guided Noise and Multi-Scale Feature Aggregation",
    "authors": [
      "Yakun Niu",
      "Pei Chen",
      "Lei Zhang",
      "Lei Tan",
      "Yingjian Chen"
    ],
    "abstract": "Image Forgery Localization (IFL) technology aims to detect and locate the\nforged areas in an image, which is very important in the field of digital\nforensics. However, existing IFL methods suffer from feature degradation during\ntraining using multi-layer convolutions or the self-attention mechanism, and\nperform poorly in detecting small forged regions and in robustness against\npost-processing. To tackle these, we propose a guided and multi-scale feature\naggregated network for IFL. Spectifically, in order to comprehensively learn\nthe noise feature under different types of forgery, we develop an effective\nnoise extraction module in a guided way. Then, we design a Feature Aggregation\nModule (FAM) that uses dynamic convolution to adaptively aggregate RGB and\nnoise features over multiple scales. Moreover, we propose an Atrous Residual\nPyramid Module (ARPM) to enhance features representation and capture both\nglobal and local features using different receptive fields to improve the\naccuracy and robustness of forgery localization. Expensive experiments on 5\npublic datasets have shown that our proposed model outperforms several the\nstate-of-the-art methods, specially on small region forged image.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "36 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.01622v1",
    "published_date": "2024-11-17 11:50:09 UTC",
    "updated_date": "2024-11-17 11:50:09 UTC"
  },
  {
    "arxiv_id": "2411.11029v1",
    "title": "Wafer Map Defect Classification Using Autoencoder-Based Data Augmentation and Convolutional Neural Network",
    "authors": [
      "Yin-Yin Bao",
      "Er-Chao Li",
      "Hong-Qiang Yang",
      "Bin-Bin Jia"
    ],
    "abstract": "In semiconductor manufacturing, wafer defect maps (WDMs) play a crucial role\nin diagnosing issues and enhancing process yields by revealing critical defect\npatterns. However, accurately categorizing WDM defects presents significant\nchallenges due to noisy data, unbalanced defect classes, and the complexity of\nfailure modes. To address these challenges, this study proposes a novel method\ncombining a self-encoder-based data augmentation technique with a convolutional\nneural network (CNN). By introducing noise into the latent space, the\nself-encoder enhances data diversity and mitigates class imbalance, thereby\nimproving the model's generalization capabilities. The augmented dataset is\nsubsequently used to train the CNN, enabling it to deliver precise\nclassification of both common and rare defect patterns. Experimental results on\nthe WM-811K dataset demonstrate that the proposed method achieves a\nclassification accuracy of 98.56%, surpassing Random Forest, SVM, and Logistic\nRegression by 19%, 21%, and 27%, respectively. These findings highlight the\nrobustness and effectiveness of the proposed approach, offering a reliable\nsolution for wafer defect detection and classification.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV",
      "68T07, 68U10",
      "I.2.10; I.5.1; I.5.4; I.4.8"
    ],
    "primary_category": "cs.CV",
    "comment": "26 pages, 11 figures, including dataset preprocessing, proposed\n  methods, and experimental results",
    "pdf_url": "http://arxiv.org/pdf/2411.11029v1",
    "published_date": "2024-11-17 10:19:54 UTC",
    "updated_date": "2024-11-17 10:19:54 UTC"
  },
  {
    "arxiv_id": "2411.11027v1",
    "title": "BianCang: A Traditional Chinese Medicine Large Language Model",
    "authors": [
      "Sibo Wei",
      "Xueping Peng",
      "Yi-fei Wang",
      "Jiasheng Si",
      "Weiyu Zhang",
      "Wenpeng Lu",
      "Xiaoming Wu",
      "Yinglong Wang"
    ],
    "abstract": "The rise of large language models (LLMs) has driven significant progress in\nmedical applications, including traditional Chinese medicine (TCM). However,\ncurrent medical LLMs struggle with TCM diagnosis and syndrome differentiation\ndue to substantial differences between TCM and modern medical theory, and the\nscarcity of specialized, high-quality corpora. This paper addresses these\nchallenges by proposing BianCang, a TCM-specific LLM, using a two-stage\ntraining process that first injects domain-specific knowledge and then aligns\nit through targeted stimulation. To enhance diagnostic and differentiation\ncapabilities, we constructed pre-training corpora, instruction-aligned datasets\nbased on real hospital records, and the ChP-TCM dataset derived from the\nPharmacopoeia of the People's Republic of China. We compiled extensive TCM and\nmedical corpora for continuous pre-training and supervised fine-tuning,\nbuilding a comprehensive dataset to refine the model's understanding of TCM.\nEvaluations across 11 test sets involving 29 models and 4 tasks demonstrate the\neffectiveness of BianCang, offering valuable insights for future research.\nCode, datasets, and models are available at\nhttps://github.com/QLU-NLP/BianCang.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.11027v1",
    "published_date": "2024-11-17 10:17:01 UTC",
    "updated_date": "2024-11-17 10:17:01 UTC"
  },
  {
    "arxiv_id": "2411.11016v2",
    "title": "Time Step Generating: A Universal Synthesized Deepfake Image Detector",
    "authors": [
      "Ziyue Zeng",
      "Haoyuan Liu",
      "Dingjie Peng",
      "Luoxu Jing",
      "Hiroshi Watanabe"
    ],
    "abstract": "Currently, high-fidelity text-to-image models are developed in an\naccelerating pace. Among them, Diffusion Models have led to a remarkable\nimprovement in the quality of image generation, making it vary challenging to\ndistinguish between real and synthesized images. It simultaneously raises\nserious concerns regarding privacy and security. Some methods are proposed to\ndistinguish the diffusion model generated images through reconstructing.\nHowever, the inversion and denoising processes are time-consuming and heavily\nreliant on the pre-trained generative model. Consequently, if the pre-trained\ngenerative model meet the problem of out-of-domain, the detection performance\ndeclines. To address this issue, we propose a universal synthetic image\ndetector Time Step Generating (TSG), which does not rely on pre-trained models'\nreconstructing ability, specific datasets, or sampling algorithms. Our method\nutilizes a pre-trained diffusion model's network as a feature extractor to\ncapture fine-grained details, focusing on the subtle differences between real\nand synthetic images. By controlling the time step t of the network input, we\ncan effectively extract these distinguishing detail features. Then, those\nfeatures can be passed through a classifier (i.e. Resnet), which efficiently\ndetects whether an image is synthetic or real. We test the proposed TSG on the\nlarge-scale GenImage benchmark and it achieves significant improvements in both\naccuracy and generalizability.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "62H30, 68T07",
      "I.4.9; I.4.7; I.5.2"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.11016v2",
    "published_date": "2024-11-17 09:39:50 UTC",
    "updated_date": "2024-11-20 00:30:01 UTC"
  },
  {
    "arxiv_id": "2411.11006v2",
    "title": "BackdoorMBTI: A Backdoor Learning Multimodal Benchmark Tool Kit for Backdoor Defense Evaluation",
    "authors": [
      "Haiyang Yu",
      "Tian Xie",
      "Jiaping Gui",
      "Pengyang Wang",
      "Ping Yi",
      "Yue Wu"
    ],
    "abstract": "Over the past few years, the emergence of backdoor attacks has presented\nsignificant challenges to deep learning systems, allowing attackers to insert\nbackdoors into neural networks. When data with a trigger is processed by a\nbackdoor model, it can lead to mispredictions targeted by attackers, whereas\nnormal data yields regular results. The scope of backdoor attacks is expanding\nbeyond computer vision and encroaching into areas such as natural language\nprocessing and speech recognition. Nevertheless, existing backdoor defense\nmethods are typically tailored to specific data modalities, restricting their\napplication in multimodal contexts. While multimodal learning proves highly\napplicable in facial recognition, sentiment analysis, action recognition,\nvisual question answering, the security of these models remains a crucial\nconcern. Specifically, there are no existing backdoor benchmarks targeting\nmultimodal applications or related tasks.\n  In order to facilitate the research in multimodal backdoor, we introduce\nBackdoorMBTI, the first backdoor learning toolkit and benchmark designed for\nmultimodal evaluation across three representative modalities from eleven\ncommonly used datasets. BackdoorMBTI provides a systematic backdoor learning\npipeline, encompassing data processing, data poisoning, backdoor training, and\nevaluation. The generated poison datasets and backdoor models enable detailed\nevaluation of backdoor defenses. Given the diversity of modalities,\nBackdoorMBTI facilitates systematic evaluation across different data types.\nFurthermore, BackdoorMBTI offers a standardized approach to handling practical\nfactors in backdoor learning, such as issues related to data quality and\nerroneous labels. We anticipate that BackdoorMBTI will expedite future research\nin backdoor defense methods within a multimodal context. Code is available at\nhttps://github.com/SJTUHaiyangYu/BackdoorMBTI.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.11006v2",
    "published_date": "2024-11-17 09:01:55 UTC",
    "updated_date": "2025-03-06 07:50:21 UTC"
  },
  {
    "arxiv_id": "2411.11002v1",
    "title": "Unveiling the Hidden: Online Vectorized HD Map Construction with Clip-Level Token Interaction and Propagation",
    "authors": [
      "Nayeon Kim",
      "Hongje Seong",
      "Daehyun Ji",
      "Sujin Jang"
    ],
    "abstract": "Predicting and constructing road geometric information (e.g., lane lines,\nroad markers) is a crucial task for safe autonomous driving, while such static\nmap elements can be repeatedly occluded by various dynamic objects on the road.\nRecent studies have shown significantly improved vectorized high-definition\n(HD) map construction performance, but there has been insufficient\ninvestigation of temporal information across adjacent input frames (i.e.,\nclips), which may lead to inconsistent and suboptimal prediction results. To\ntackle this, we introduce a novel paradigm of clip-level vectorized HD map\nconstruction, MapUnveiler, which explicitly unveils the occluded map elements\nwithin a clip input by relating dense image representations with efficient clip\ntokens. Additionally, MapUnveiler associates inter-clip information through\nclip token propagation, effectively utilizing long-term temporal map\ninformation. MapUnveiler runs efficiently with the proposed clip-level pipeline\nby avoiding redundant computation with temporal stride while building a global\nmap relationship. Our extensive experiments demonstrate that MapUnveiler\nachieves state-of-the-art performance on both the nuScenes and Argoverse2\nbenchmark datasets. We also showcase that MapUnveiler significantly outperforms\nstate-of-the-art approaches in a challenging setting, achieving +10.7% mAP\nimprovement in heavily occluded driving road scenes. The project page can be\nfound at https://mapunveiler.github.io.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "18 pages, 9 figures, NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.11002v1",
    "published_date": "2024-11-17 08:38:18 UTC",
    "updated_date": "2024-11-17 08:38:18 UTC"
  },
  {
    "arxiv_id": "2411.13584v1",
    "title": "AddrLLM: Address Rewriting via Large Language Model on Nationwide Logistics Data",
    "authors": [
      "Qinchen Yang",
      "Zhiqing Hong",
      "Dongjiang Cao",
      "Haotian Wang",
      "Zejun Xie",
      "Tian He",
      "Yunhuai Liu",
      "Yu Yang",
      "Desheng Zhang"
    ],
    "abstract": "Textual description of a physical location, commonly known as an address,\nplays an important role in location-based services(LBS) such as on-demand\ndelivery and navigation. However, the prevalence of abnormal addresses, those\ncontaining inaccuracies that fail to pinpoint a location, have led to\nsignificant costs. Address rewriting has emerged as a solution to rectify these\nabnormal addresses. Despite the critical need, existing address rewriting\nmethods are limited, typically tailored to correct specific error types, or\nfrequently require retraining to process new address data effectively. In this\nstudy, we introduce AddrLLM, an innovative framework for address rewriting that\nis built upon a retrieval augmented large language model. AddrLLM overcomes\naforementioned limitations through a meticulously designed Supervised\nFine-Tuning module, an Address-centric Retrieval Augmented Generation module\nand a Bias-free Objective Alignment module. To the best of our knowledge, this\nstudy pioneers the application of LLM-based address rewriting approach to solve\nthe issue of abnormal addresses. Through comprehensive offline testing with\nreal-world data on a national scale and subsequent online deployment, AddrLLM\nhas demonstrated superior performance in integration with existing logistics\nsystem. It has significantly decreased the rate of parcel re-routing by\napproximately 43\\%, underscoring its exceptional efficacy in real-world\napplications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by KDD'25 ADS Track",
    "pdf_url": "http://arxiv.org/pdf/2411.13584v1",
    "published_date": "2024-11-17 07:32:46 UTC",
    "updated_date": "2024-11-17 07:32:46 UTC"
  },
  {
    "arxiv_id": "2411.10991v1",
    "title": "Modulating Reservoir Dynamics via Reinforcement Learning for Efficient Robot Skill Synthesis",
    "authors": [
      "Zahra Koulaeizadeh",
      "Erhan Oztop"
    ],
    "abstract": "A random recurrent neural network, called a reservoir, can be used to learn\nrobot movements conditioned on context inputs that encode task goals. The\nLearning is achieved by mapping the random dynamics of the reservoir modulated\nby context to desired trajectories via linear regression. This makes the\nreservoir computing (RC) approach computationally efficient as no iterative\ngradient descent learning is needed. In this work, we propose a novel RC-based\nLearning from Demonstration (LfD) framework that not only learns to generate\nthe demonstrated movements but also allows online modulation of the reservoir\ndynamics to generate movement trajectories that are not covered by the initial\ndemonstration set. This is made possible by using a Reinforcement Learning (RL)\nmodule that learns a policy to output context as its actions based on the robot\nstate. Considering that the context dimension is typically low, learning with\nthe RL module is very efficient. We show the validity of the proposed model\nwith systematic experiments on a 2 degrees-of-freedom (DOF) simulated robot\nthat is taught to reach targets, encoded as context, with and without obstacle\navoidance constraint. The initial data set includes a set of reaching\ndemonstrations which are learned by the reservoir system. To enable reaching\nout-of-distribution targets, the RL module is engaged in learning a policy to\ngenerate dynamic contexts so that the generated trajectory achieves the desired\ngoal without any learning in the reservoir system. Overall, the proposed model\nuses an initial learned motor primitive set to efficiently generate diverse\nmotor behaviors guided by the designed reward function. Thus the model can be\nused as a flexible and effective LfD system where the action repertoire can be\nextended without new data collection.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "13 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.10991v1",
    "published_date": "2024-11-17 07:25:54 UTC",
    "updated_date": "2024-11-17 07:25:54 UTC"
  },
  {
    "arxiv_id": "2411.10979v3",
    "title": "VidComposition: Can MLLMs Analyze Compositions in Compiled Videos?",
    "authors": [
      "Yunlong Tang",
      "Junjia Guo",
      "Hang Hua",
      "Susan Liang",
      "Mingqian Feng",
      "Xinyang Li",
      "Rui Mao",
      "Chao Huang",
      "Jing Bi",
      "Zeliang Zhang",
      "Pooyan Fazli",
      "Chenliang Xu"
    ],
    "abstract": "The advancement of Multimodal Large Language Models (MLLMs) has enabled\nsignificant progress in multimodal understanding, expanding their capacity to\nanalyze video content. However, existing evaluation benchmarks for MLLMs\nprimarily focus on abstract video comprehension, lacking a detailed assessment\nof their ability to understand video compositions, the nuanced interpretation\nof how visual elements combine and interact within highly compiled video\ncontexts. We introduce VidComposition, a new benchmark specifically designed to\nevaluate the video composition understanding capabilities of MLLMs using\ncarefully curated compiled videos and cinematic-level annotations.\nVidComposition includes 982 videos with 1706 multiple-choice questions,\ncovering various compositional aspects such as camera movement, angle, shot\nsize, narrative structure, character actions and emotions, etc. Our\ncomprehensive evaluation of 33 open-source and proprietary MLLMs reveals a\nsignificant performance gap between human and model capabilities. This\nhighlights the limitations of current MLLMs in understanding complex, compiled\nvideo compositions and offers insights into areas for further improvement. The\nleaderboard and evaluation code are available at\nhttps://yunlong10.github.io/VidComposition/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.10979v3",
    "published_date": "2024-11-17 06:23:46 UTC",
    "updated_date": "2024-11-25 15:12:24 UTC"
  },
  {
    "arxiv_id": "2411.10958v4",
    "title": "SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization",
    "authors": [
      "Jintao Zhang",
      "Haofeng Huang",
      "Pengle Zhang",
      "Jia Wei",
      "Jun Zhu",
      "Jianfei Chen"
    ],
    "abstract": "Although quantization for linear layers has been widely used, its application\nto accelerate the attention process remains limited. To further enhance the\nefficiency of attention computation compared to SageAttention while maintaining\nprecision, we propose SageAttention2, which utilizes significantly faster 4-bit\nmatrix multiplication (Matmul) alongside additional precision-enhancing\ntechniques. First, we propose to quantize matrices $(Q, K)$ to INT4 in a\nhardware-friendly thread-level granularity and quantize matrices $(\\widetilde\nP, V)$ to FP8. Second, we propose a method to smooth $Q$, enhancing the\naccuracy of INT4 $QK^\\top$. Third, we propose a two-level accumulation strategy\nfor $\\widetilde PV$ to enhance the accuracy of FP8 $\\widetilde PV$. The\noperations per second (OPS) of SageAttention2 surpass FlashAttention2 and\nxformers by about 3x and 4.5x on RTX4090, respectively. Moreover,\nSageAttention2 matches the speed of FlashAttention3(fp8) on the Hopper GPUs,\nwhile delivering much higher accuracy. Comprehensive experiments confirm that\nour approach incurs negligible end-to-end metrics loss across diverse models,\nincluding those for language, image, and video generation. The code is\navailable at https://github.com/thu-ml/SageAttention.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.NE",
      "cs.PF"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.10958v4",
    "published_date": "2024-11-17 04:35:49 UTC",
    "updated_date": "2025-02-10 14:02:49 UTC"
  },
  {
    "arxiv_id": "2411.10957v1",
    "title": "IMPaCT GNN: Imposing invariance with Message Passing in Chronological split Temporal Graphs",
    "authors": [
      "Sejun Park",
      "Joo Young Park",
      "Hyunwoo Park"
    ],
    "abstract": "This paper addresses domain adaptation challenges in graph data resulting\nfrom chronological splits. In a transductive graph learning setting, where each\nnode is associated with a timestamp, we focus on the task of Semi-Supervised\nNode Classification (SSNC), aiming to classify recent nodes using labels of\npast nodes. Temporal dependencies in node connections create domain shifts,\ncausing significant performance degradation when applying models trained on\nhistorical data into recent data. Given the practical relevance of this\nscenario, addressing domain adaptation in chronological split data is crucial,\nyet underexplored. We propose Imposing invariance with Message Passing in\nChronological split Temporal Graphs (IMPaCT), a method that imposes invariant\nproperties based on realistic assumptions derived from temporal graph\nstructures. Unlike traditional domain adaptation approaches which rely on\nunverifiable assumptions, IMPaCT explicitly accounts for the characteristics of\nchronological splits. The IMPaCT is further supported by rigorous mathematical\nanalysis, including a derivation of an upper bound of the generalization error.\nExperimentally, IMPaCT achieves a 3.8% performance improvement over current\nSOTA method on the ogbn-mag graph dataset. Additionally, we introduce the\nTemporal Stochastic Block Model (TSBM), which replicates temporal graphs under\nvarying conditions, demonstrating the applicability of our methods to general\nspatial GNNs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages (without appendix), 35 pages (with appendix), 14 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.10957v1",
    "published_date": "2024-11-17 04:23:25 UTC",
    "updated_date": "2024-11-17 04:23:25 UTC"
  },
  {
    "arxiv_id": "2412.03573v1",
    "title": "Improving Tool Retrieval by Leveraging Large Language Models for Query Generation",
    "authors": [
      "Mohammad Kachuee",
      "Sarthak Ahuja",
      "Vaibhav Kumar",
      "Puyang Xu",
      "Xiaohu Liu"
    ],
    "abstract": "Using tools by Large Language Models (LLMs) is a promising avenue to extend\ntheir reach beyond language or conversational settings. The number of tools can\nscale to thousands as they enable accessing sensory information, fetching\nupdated factual knowledge, or taking actions in the real world. In such\nsettings, in-context learning by providing a short list of relevant tools in\nthe prompt is a viable approach. To retrieve relevant tools, various approaches\nhave been suggested, ranging from simple frequency-based matching to dense\nembedding-based semantic retrieval. However, such approaches lack the\ncontextual and common-sense understanding required to retrieve the right tools\nfor complex user requests. Rather than increasing the complexity of the\nretrieval component itself, we propose leveraging LLM understanding to generate\na retrieval query. Then, the generated query is embedded and used to find the\nmost relevant tools via a nearest-neighbor search. We investigate three\napproaches for query generation: zero-shot prompting, supervised fine-tuning on\ntool descriptions, and alignment learning by iteratively optimizing a reward\nmetric measuring retrieval performance. By conducting extensive experiments on\na dataset covering complex and multi-tool scenarios, we show that leveraging\nLLMs for query generation improves the retrieval for in-domain (seen tools) and\nout-of-domain (unseen tools) settings.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.03573v1",
    "published_date": "2024-11-17 03:02:09 UTC",
    "updated_date": "2024-11-17 03:02:09 UTC"
  },
  {
    "arxiv_id": "2411.10928v1",
    "title": "Learn from Downstream and Be Yourself in Multimodal Large Language Model Fine-Tuning",
    "authors": [
      "Wenke Huang",
      "Jian Liang",
      "Zekun Shi",
      "Didi Zhu",
      "Guancheng Wan",
      "He Li",
      "Bo Du",
      "Dacheng Tao",
      "Mang Ye"
    ],
    "abstract": "Multimodal Large Language Model (MLLM) have demonstrated strong\ngeneralization capabilities across diverse distributions and tasks, largely due\nto extensive pre-training datasets. Fine-tuning MLLM has become a common\npractice to improve performance on specific downstream tasks. However, during\nfine-tuning, MLLM often faces the risk of forgetting knowledge acquired during\npre-training, which can result in a decline in generalization abilities. To\nbalance the trade-off between generalization and specialization, we propose\nmeasuring the parameter importance for both pre-trained and fine-tuning\ndistributions, based on frozen pre-trained weight magnitude and accumulated\nfine-tuning gradient values. We further apply an importance-aware weight\nallocation strategy, selectively updating relatively important parameters for\ndownstream tasks. We conduct empirical evaluations on both image captioning and\nvisual question-answering tasks using various MLLM architectures. The\ncomprehensive experimental analysis demonstrates the effectiveness of the\nproposed solution, highlighting the efficiency of the crucial modules in\nenhancing downstream specialization performance while mitigating generalization\ndegradation in MLLM Fine-Tuning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.10928v1",
    "published_date": "2024-11-17 01:16:37 UTC",
    "updated_date": "2024-11-17 01:16:37 UTC"
  },
  {
    "arxiv_id": "2411.10924v1",
    "title": "Hyperspectral Imaging-Based Grain Quality Assessment With Limited Labelled Data",
    "authors": [
      "Priyabrata Karmakar",
      "Manzur Murshed",
      "Shyh Wei Teng"
    ],
    "abstract": "Recently hyperspectral imaging (HSI)-based grain quality assessment has\ngained research attention. However, unlike other imaging modalities, HSI data\nlacks sufficient labelled samples required to effectively train deep\nconvolutional neural network (DCNN)-based classifiers. In this paper, we\npresent a novel approach to grain quality assessment using HSI combined with\nfew-shot learning (FSL) techniques. Traditional methods for grain quality\nevaluation, while reliable, are invasive, time-consuming, and costly. HSI\noffers a non-invasive, real-time alternative by capturing both spatial and\nspectral information. However, a significant challenge in applying DCNNs for\nHSI-based grain classification is the need for large labelled databases, which\nare often difficult to obtain. To address this, we explore the use of FSL,\nwhich enables models to perform well with limited labelled data, making it a\npractical solution for real-world applications where rapid deployment is\nrequired. We also explored the application of FSL for the classification of\nhyperspectral images of bulk grains to enable rapid quality assessment at\nvarious receival points in the grain supply chain. We evaluated the performance\nof few-shot classifiers in two scenarios: first, classification of grain types\nseen during training, and second, generalisation to unseen grain types, a\ncrucial feature for real-world applications. In the first scenario, we\nintroduce a novel approach using pre-computed collective class prototypes\n(CCPs) to enhance inference efficiency and robustness. In the second scenario,\nwe assess the model's ability to classify novel grain types using limited\nsupport examples. Our experimental results show that despite using very limited\nlabelled data for training, our FSL classifiers accuracy is comparable to that\nof a fully trained classifier trained using a significantly larger labelled\ndatabase.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages",
    "pdf_url": "http://arxiv.org/pdf/2411.10924v1",
    "published_date": "2024-11-17 01:02:18 UTC",
    "updated_date": "2024-11-17 01:02:18 UTC"
  },
  {
    "arxiv_id": "2411.10919v1",
    "title": "Multi-Modal Self-Supervised Learning for Surgical Feedback Effectiveness Assessment",
    "authors": [
      "Arushi Gupta",
      "Rafal Kocielnik",
      "Jiayun Wang",
      "Firdavs Nasriddinov",
      "Cherine Yang",
      "Elyssa Wong",
      "Anima Anandkumar",
      "Andrew Hung"
    ],
    "abstract": "During surgical training, real-time feedback from trainers to trainees is\nimportant for preventing errors and enhancing long-term skill acquisition.\nAccurately predicting the effectiveness of this feedback, specifically whether\nit leads to a change in trainee behavior, is crucial for developing methods for\nimproving surgical training and education. However, relying on human\nannotations to assess feedback effectiveness is laborious and prone to biases,\nunderscoring the need for an automated, scalable, and objective method.\nCreating such an automated system poses challenges, as it requires an\nunderstanding of both the verbal feedback delivered by the trainer and the\nvisual context of the real-time surgical scene. To address this, we propose a\nmethod that integrates information from transcribed verbal feedback and\ncorresponding surgical video to predict feedback effectiveness. Our findings\nshow that both transcribed feedback and surgical video are individually\npredictive of trainee behavior changes, and their combination achieves an AUROC\nof 0.70+/-0.02, improving prediction accuracy by up to 6.6%. Additionally, we\nintroduce self-supervised fine-tuning as a strategy for enhancing surgical\nvideo representation learning, which is scalable and further enhances\nprediction performance. Our results demonstrate the potential of multi-modal\nlearning to advance the automated assessment of surgical feedback.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "68T07, 68T45, 68U10, 92C50",
      "I.2; I.2.10; I.5.4; I.4.7; J.3; K.3.1"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted as a spotlight proceedings paper at Machine Learning for\n  Health 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.10919v1",
    "published_date": "2024-11-17 00:13:00 UTC",
    "updated_date": "2024-11-17 00:13:00 UTC"
  },
  {
    "arxiv_id": "2411.10918v1",
    "title": "LLM-assisted Physical Invariant Extraction for Cyber-Physical Systems Anomaly Detection",
    "authors": [
      "Danial Abshari",
      "Chenglong Fu",
      "Meera Sridhar"
    ],
    "abstract": "Modern industrial infrastructures rely heavily on Cyber-Physical Systems\n(CPS), but these are vulnerable to cyber-attacks with potentially catastrophic\neffects. To reduce these risks, anomaly detection methods based on physical\ninvariants have been developed. However, these methods often require\ndomain-specific expertise to manually define invariants, making them costly and\ndifficult to scale. To address this limitation, we propose a novel approach to\nextract physical invariants from CPS testbeds for anomaly detection. Our\ninsight is that CPS design documentation often contains semantically rich\ndescriptions of physical procedures, which can profile inter-correlated\ndynamics among system components. Leveraging the built-in physics and\nengineering knowledge of recent generative AI models, we aim to automate this\ntraditionally manual process, improving scalability and reducing costs. This\nwork focuses on designing and optimizing a Retrieval-Augmented-Generation (RAG)\nworkflow with a customized prompting system tailored for CPS documentation,\nenabling accurate extraction of semantic information and inference of physical\ninvariants from complex, multimodal content. Then, rather than directly\napplying the inferred invariants for anomaly detection, we introduce an\ninnovative statistics-based learning approach that integrates these invariants\ninto the training dataset. This method addresses limitations such as\nhallucination and concept drift, enhancing the reliability of the model. We\nevaluate our approach on real-world public CPS security dataset which contains\n86 data points and 58 attacking cases. The results show that our approach\nachieves a high precision of 0.923, accurately detecting anomalies while\nminimizing false alarms.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.10918v1",
    "published_date": "2024-11-17 00:09:04 UTC",
    "updated_date": "2024-11-17 00:09:04 UTC"
  }
]
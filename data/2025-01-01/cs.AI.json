{
  "date": "2025-01-01",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-01-01 的 arXiv 中文 TLDR 快报！今天 arXiv 的论文主要聚焦于 AI 模型的优化、医疗应用、多代理系统和强化学习等领域，亮点包括 LLM 在复杂推理和多模态任务中的创新应用，以及知名学者如 Kathleen M. Carley 对社会媒体机器人的分析，这些工作为 AI 鲁棒性和实际部署提供了新见解。\n\n### 重点论文讨论\n我们先聊聊今天最令人印象深刻和话题度高的论文，这些涉及 AI 核心技术、医疗创新和知名学者的贡献。相关论文如 LLM 和多代理系统会放在一起讨论，以突出其在推理和应用中的潜力。\n\n- **论文1: 无声多数：揭开虚假相关存在下的记忆效应（The Silent Majority: Demystifying Memorization Effect in the Presence of Spurious Correlations）**  \n  这篇论文由 Chenyu You 等作者发布，系统分析了机器学习模型在虚假相关（如图像背景）下的记忆机制，发现少数群体的性能不平衡源于特定神经元上的“噪声”记忆，并提出新框架消除这些模式，提高模型在基准测试中的鲁棒性，为 AI 抗虚假相关研究奠定基础。\n\n- **论文2: 通过合成 DR1 图像生成提升早期糖尿病视网膜病变检测：StyleGAN3 方法（Enhancing Early Diabetic Retinopathy Detection through Synthetic DR1 Image Generation: A StyleGAN3 Approach）**  \n  Sagarnil Das 和 Pradeep Walia 的工作使用 StyleGAN3 生成高保真 DR1 图像，解决了训练数据稀缺问题，FID 分数达 17.29，显著提升监督分类器的性能，并通过人类图灵测试验证图像真实性，为 AI 驱动的医疗诊断提供实用工具。\n\n- **论文10: 大型语言模型中的表示（Representation in large language models）**  \n  Cameron C. Yetman 的论文探讨 LLM 是否依赖表示-based 信息处理而非简单记忆，证明 LLM 部分驱动于生物认知式的处理，并提出技术调查这些表示，提升了对 LLM 信念和理解的理论基础。\n\n- **论文13: 什么是社会媒体机器人？全球比较机器人和人类特征（What is a Social Media Bot? A Global Comparison of Bot and Human Characteristics）**  \n  知名学者 Kathleen M. Carley 等分析了社交媒体的机器人和人类行为差异，如机器人使用自动化语言而人类更依赖对话理解，并基于真实数据集提出机器人定义和监管建议，为社会网络安全提供关键洞见。\n\n- **论文15: LLM+AL：桥接大型语言模型和动作语言用于复杂动作推理（LLM+AL: Bridging Large Language Models and Action Languages for Complex Reasoning about Actions）**  \n  与上文 LLM 相关，Adam Ishay 和 Joohyung Lee 的方法结合 LLM 的自然语言理解和动作语言的符号推理，在基准测试中超越 ChatGPT-4 等模型，实现更准确的零-shot 推理，并自动生成动作语言代码。\n\n- **论文18: LLM 驱动的多代理系统用于自动化加密投资组合管理（LLM-Powered Multi-Agent System for Automated Crypto Portfolio Management）**  \n  延续多代理主题，Yichen Luo 等作者提出框架，使用 LLM 代理处理多模态数据，在加密货币投资中提升预测准确性，实验显示其在分类和定价任务中优于基准，强调 LLM 在金融应用的潜力。\n\n其他论文数量较多，我们快速掠过一些次要或技术导向的，以控制篇幅。以下是简要概述：\n\n- **论文3: 先前增量对话和机器人动作管理的经验教训（Prior Lessons of Incremental Dialogue and Robot Action Management for the Age of Language Models）**  \n  Casey Kennington 等讨论 LLM 在机器人对话中的局限，如非增量处理，并审视增量建模的需求，为未来对话系统提供参考。\n\n- **论文6: 人群感知扩散用于时间序列生成（Population Aware Diffusion for Time Series Generation）**  \n  主要贡献是改进扩散模型以保留时间序列的整体分布，提升预测任务的准确性。\n\n- **论文9: 揭开在线聚类 bandits 的神秘面纱：随机和平滑对抗上下文下的增强探索（Demystifying Online Clustering of Bandits: Enhanced Exploration Under Stochastic and Smoothed Adversarial Contexts）**  \n  提出新算法改善多臂老虎机聚类，减少假设依赖，提升实际性能。\n\n- **论文14: 蒸馏式终身自适应用于可配置系统（Distilled Lifelong Self-Adaptation for Configurable Systems）**  \n  框架优化系统自适应，减少计算开销，提高性能。\n\n- **论文19: 解耦 Transformer 中的知识和推理：模块化架构与广义交叉注意力（Decoupling Knowledge and Reasoning in Transformers: A Modular Architecture with Generalized Cross-Attention）**  \n  数学证明 FFN 是交叉注意力的特例，提升 Transformer 的可解释性。\n\n- **论文23: 面向推理的类比方法用于零-shot 事件关系推理（Reasoning-Oriented and Analogy-Based Methods for Locating and Editing in Zero-Shot Event-Relational Reasoning）**  \n  提升零-shot 推理性能，通过类比优化模型。\n\n- **论文29: 基于深度学习的胎儿中枢神经系统异常检测和分类的多中心研究（Multi-Center Study on Deep Learning-Assisted Detection and Classification of Fetal Central Nervous System Anomalies Using Ultrasound Imaging）**  \n  AI 模型在超声图像上检测胎儿异常，准确率达 94.5%，辅助临床诊断。\n\n剩余论文如论文4、5、7、8、11、12、16、17、20-22、24-28、30-42 等，多为技术优化或特定应用（如强化学习、生成模型），我们仅快速提及核心贡献：例如，论文4 的 VERITAS 框架检测 AI 接收器分布偏移；论文5 的 β-DQN 改善强化学习探索；论文21 的 Spike Window Decoding 加速语音识别；论文26 的 LENS-XAI 提升入侵检测解释性。这些工作虽有价值，但相对常规，我们在此不展开讨论。\n\n今天的快报到此结束，arXiv 持续更新，欢迎关注感兴趣领域！",
  "papers": [
    {
      "arxiv_id": "2501.00961v2",
      "title": "The Silent Majority: Demystifying Memorization Effect in the Presence of Spurious Correlations",
      "title_zh": "沉默的大多数：在虚假相关存在下揭开记忆效应之谜",
      "authors": [
        "Chenyu You",
        "Haocheng Dai",
        "Yifei Min",
        "Jasjeet S. Sekhon",
        "Sarang Joshi",
        "James S. Duncan"
      ],
      "abstract": "Machine learning models often rely on simple spurious features -- patterns in\ntraining data that correlate with targets but are not causally related to them,\nlike image backgrounds in foreground classification. This reliance typically\nleads to imbalanced test performance across minority and majority groups. In\nthis work, we take a closer look at the fundamental cause of such imbalanced\nperformance through the lens of memorization, which refers to the ability to\npredict accurately on \\textit{atypical} examples (minority groups) in the\ntraining set but failing in achieving the same accuracy in the testing set.\nThis paper systematically shows the ubiquitous existence of spurious features\nin a small set of neurons within the network, providing the first-ever evidence\nthat memorization may contribute to imbalanced group performance. Through three\nexperimental sources of converging empirical evidence, we find the property of\na small subset of neurons or channels in memorizing minority group information.\nInspired by these findings, we articulate the hypothesis: the imbalanced group\nperformance is a byproduct of ``noisy'' spurious memorization confined to a\nsmall set of neurons. To further substantiate this hypothesis, we show that\neliminating these unnecessary spurious memorization patterns via a novel\nframework during training can significantly affect the model performance on\nminority groups. Our experimental results across various architectures and\nbenchmarks offer new insights on how neural networks encode core and spurious\nknowledge, laying the groundwork for future research in demystifying robustness\nto spurious correlation.",
      "tldr_zh": "机器学习模型经常依赖于虚假相关特征（spurious features），如图像背景，导致测试性能在少数群体和多数群体之间不平衡。本文通过memorization（记忆化）的视角，揭示这种不平衡源于网络中一小部分神经元（neurons）对少数群体信息的“嘈杂”记忆化。研究者通过三个实验来源的实证证据，证明这些neurons负责编码虚假知识，并提出一个新框架在训练过程中消除不必要的spurious memorization。实验结果显示，该框架显著改善了模型在各种架构和基准上的少数群体性能，为提升模型对spurious correlations的鲁棒性（robustness）提供了新见解。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "eess.IV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.00961v2",
      "published_date": "2025-01-01 21:45:00 UTC",
      "updated_date": "2025-01-15 06:46:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:24:02.514270"
    },
    {
      "arxiv_id": "2501.00954v1",
      "title": "Enhancing Early Diabetic Retinopathy Detection through Synthetic DR1 Image Generation: A StyleGAN3 Approach",
      "title_zh": "通过合成 DR1",
      "authors": [
        "Sagarnil Das",
        "Pradeep Walia"
      ],
      "abstract": "Diabetic Retinopathy (DR) is a leading cause of preventable blindness. Early\ndetection at the DR1 stage is critical but is hindered by a scarcity of\nhigh-quality fundus images. This study uses StyleGAN3 to generate synthetic DR1\nimages characterized by microaneurysms with high fidelity and diversity. The\naim is to address data scarcity and enhance the performance of supervised\nclassifiers. A dataset of 2,602 DR1 images was used to train the model,\nfollowed by a comprehensive evaluation using quantitative metrics, including\nFrechet Inception Distance (FID), Kernel Inception Distance (KID), and\nEquivariance with respect to translation (EQ-T) and rotation (EQ-R).\nQualitative assessments included Human Turing tests, where trained\nophthalmologists evaluated the realism of synthetic images. Spectral analysis\nfurther validated image quality. The model achieved a final FID score of 17.29,\noutperforming the mean FID of 21.18 (95 percent confidence interval - 20.83 to\n21.56) derived from bootstrap resampling. Human Turing tests demonstrated the\nmodel's ability to produce highly realistic images, though minor artifacts near\nthe borders were noted. These findings suggest that StyleGAN3-generated\nsynthetic DR1 images hold significant promise for augmenting training datasets,\nenabling more accurate early detection of Diabetic Retinopathy. This\nmethodology highlights the potential of synthetic data in advancing medical\nimaging and AI-driven diagnostics.",
      "tldr_zh": "本文研究使用 StyleGAN3 生成高保真度的合成 DR1 图像，以解决 Diabetic Retinopathy 早期检测中图像稀缺的问题。研究团队训练了一个基于 2,602 张 DR1 图像的模型，生成的图像突出了微动脉瘤特征，并通过 Frechet Inception Distance (FID) 等定量指标（FID 得分 17.29）和 Human Turing tests 等定性评估验证了其多样性和真实性。结果显示，合成图像在光谱分析中表现出色，仅有轻微边境伪像。总体而言，此方法可增强训练数据集，提高监督分类器的性能，并为医疗成像和 AI 驱动诊断提供重要潜力。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "13 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.00954v1",
      "published_date": "2025-01-01 21:00:58 UTC",
      "updated_date": "2025-01-01 21:00:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:25:45.605137"
    },
    {
      "arxiv_id": "2501.00953v2",
      "title": "Prior Lessons of Incremental Dialogue and Robot Action Management for the Age of Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Casey Kennington",
        "Pierre Lison",
        "David Schlangen"
      ],
      "abstract": "Efforts towards endowing robots with the ability to speak have benefited from\nrecent advancements in natural language processing, in particular large\nlanguage models. However, current language models are not fully incremental, as\ntheir processing is inherently monotonic and thus lack the ability to revise\ntheir interpretations or output in light of newer observations. This\nmonotonicity has important implications for the development of dialogue systems\nfor human--robot interaction. In this paper, we review the literature on\ninteractive systems that operate incrementally (i.e., at the word level or\nbelow it). We motivate the need for incremental systems, survey incremental\nmodeling of important aspects of dialogue like speech recognition and language\ngeneration. Primary focus is on the part of the system that makes decisions,\nknown as the dialogue manager. We find that there is very little research on\nincremental dialogue management, offer some requirements for practical\nincremental dialogue management, and the implications of incremental dialogue\nfor embodied, robotic platforms in the age of large language models.",
      "tldr_zh": "这篇论文回顾了增量对话系统的文献，强调大型语言模型（large language models）在人-机器人交互中的局限性，即其单调处理（monotonic processing）无法根据新观察实时修正解释或输出。论文调查了对话关键方面的增量建模，包括语音识别（speech recognition）和语言生成（language generation），并将焦点放在对话管理器（dialogue manager）上。研究发现，增量对话管理（incremental dialogue management）的研究较少，并提出了其实用要求，以及对机器人平台的启示，以适应大型语言模型时代的发展。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "20 pages",
      "pdf_url": "http://arxiv.org/pdf/2501.00953v2",
      "published_date": "2025-01-01 20:58:03 UTC",
      "updated_date": "2025-04-02 14:24:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:25:12.897698"
    },
    {
      "arxiv_id": "2501.09761v1",
      "title": "VERITAS: Verifying the Performance of AI-native Transceiver Actions in Base-Stations",
      "title_zh": "翻译失败",
      "authors": [
        "Nasim Soltani",
        "Michael Loehning",
        "Kaushik Chowdhury"
      ],
      "abstract": "Artificial Intelligence (AI)-native receivers prove significant performance\nimprovement in high noise regimes and can potentially reduce communication\noverhead compared to the traditional receiver. However, their performance\nhighly depends on the representativeness of the training dataset. A major issue\nis the uncertainty of whether the training dataset covers all test environments\nand waveform configurations, and thus, whether the trained model is robust in\npractical deployment conditions. To this end, we propose a joint\nmeasurement-recovery framework for AI-native transceivers post deployment,\ncalled VERITAS, that continuously looks for distribution shifts in the received\nsignals and triggers finite re-training spurts. VERITAS monitors the wireless\nchannel using 5G pilots fed to an auxiliary neural network that detects\nout-of-distribution channel profile, transmitter speed, and delay spread. As\nsoon as such a change is detected, a traditional (reference) receiver is\nactivated, which runs for a period of time in parallel to the AI-native\nreceiver. Finally, VERTIAS compares the bit probabilities of the AI-native and\nthe reference receivers for the same received data inputs, and decides whether\nor not a retraining process needs to be initiated. Our evaluations reveal that\nVERITAS can detect changes in the channel profile, transmitter speed, and delay\nspread with 99%, 97%, and 69% accuracies, respectively, followed by timely\ninitiation of retraining for 86%, 93.3%, and 94.8% of inputs in channel\nprofile, transmitter speed, and delay spread test sets, respectively.",
      "tldr_zh": "该论文提出 VERITAS 框架，用于验证 AI-native transceivers 在基站中的性能，以应对训练数据集覆盖不足导致的分布偏移问题。VERITAS 采用联合测量-恢复方法，通过辅助神经网络监测 5G pilots，检测 out-of-distribution 的 channel profile、transmitter speed 和 delay spread，一旦发现变化，便激活传统参考接收器并比较 bit probabilities，以决定是否启动有限的再训练过程。实验结果显示，该框架对 channel profile、transmitter speed 和 delay spread 的检测准确率分别为 99%、97% 和 69%，并在 86% 到 94.8% 的测试输入中及时触发再训练，从而提升了 AI-native transceivers 的鲁棒性。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "This work has been submitted to the IEEE for possible publication",
      "pdf_url": "http://arxiv.org/pdf/2501.09761v1",
      "published_date": "2025-01-01 19:12:03 UTC",
      "updated_date": "2025-01-01 19:12:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:25:18.466114"
    },
    {
      "arxiv_id": "2501.00913v1",
      "title": "$β$-DQN: Improving Deep Q-Learning By Evolving the Behavior",
      "title_zh": "翻译失败",
      "authors": [
        "Hongming Zhang",
        "Fengshuo Bai",
        "Chenjun Xiao",
        "Chao Gao",
        "Bo Xu",
        "Martin Müller"
      ],
      "abstract": "While many sophisticated exploration methods have been proposed, their lack\nof generality and high computational cost often lead researchers to favor\nsimpler methods like $\\epsilon$-greedy. Motivated by this, we introduce\n$\\beta$-DQN, a simple and efficient exploration method that augments the\nstandard DQN with a behavior function $\\beta$. This function estimates the\nprobability that each action has been taken at each state. By leveraging\n$\\beta$, we generate a population of diverse policies that balance exploration\nbetween state-action coverage and overestimation bias correction. An adaptive\nmeta-controller is designed to select an effective policy for each episode,\nenabling flexible and explainable exploration. $\\beta$-DQN is straightforward\nto implement and adds minimal computational overhead to the standard DQN.\nExperiments on both simple and challenging exploration domains show that\n$\\beta$-DQN outperforms existing baseline methods across a wide range of tasks,\nproviding an effective solution for improving exploration in deep reinforcement\nlearning.",
      "tldr_zh": "该研究针对深度强化学习中探索方法的局限性，提出β-DQN，一种简单高效的改进DQN框架，通过添加行为函数β来估计每个状态下动作的采取概率，从而生成多样化策略并平衡状态-动作覆盖与过估计偏差修正。β-DQN 配备自适应元控制器(meta-controller)，用于每个episode动态选择有效策略，实现灵活且可解释的探索，且对标准DQN的计算开销最小。实验结果显示，在简单和挑战性探索领域中，β-DQN 优于现有基线方法如ε-greedy，在广泛任务上表现出色。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.00913v1",
      "published_date": "2025-01-01 18:12:18 UTC",
      "updated_date": "2025-01-01 18:12:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:24:49.457393"
    },
    {
      "arxiv_id": "2501.00910v1",
      "title": "Population Aware Diffusion for Time Series Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Yang Li",
        "Han Meng",
        "Zhenyu Bi",
        "Ingolv T. Urnes",
        "Haipeng Chen"
      ],
      "abstract": "Diffusion models have shown promising ability in generating high-quality time\nseries (TS) data. Despite the initial success, existing works mostly focus on\nthe authenticity of data at the individual level, but pay less attention to\npreserving the population-level properties on the entire dataset. Such\npopulation-level properties include value distributions for each dimension and\ndistributions of certain functional dependencies (e.g., cross-correlation, CC)\nbetween different dimensions. For instance, when generating house energy\nconsumption TS data, the value distributions of the outside temperature and the\nkitchen temperature should be preserved, as well as the distribution of CC\nbetween them. Preserving such TS population-level properties is critical in\nmaintaining the statistical insights of the datasets, mitigating model bias,\nand augmenting downstream tasks like TS prediction. Yet, it is often overlooked\nby existing models. Hence, data generated by existing models often bear\ndistribution shifts from the original data. We propose Population-aware\nDiffusion for Time Series (PaD-TS), a new TS generation model that better\npreserves the population-level properties. The key novelties of PaD-TS include\n1) a new training method explicitly incorporating TS population-level property\npreservation, and 2) a new dual-channel encoder model architecture that better\ncaptures the TS data structure. Empirical results in major benchmark datasets\nshow that PaD-TS can improve the average CC distribution shift score between\nreal and synthetic data by 5.9x while maintaining a performance comparable to\nstate-of-the-art models on individual-level authenticity.",
      "tldr_zh": "本研究指出，现有的扩散模型在生成时间序列 (TS) 数据时，强调个体级真实性，但忽略了数据集的群体级属性，如每个维度的值分布和不同维度间的功能依赖分布（例如交叉相关，CC）。为了解决这一问题，作者提出了一种新模型 Population-aware Diffusion for Time Series (PaD-TS)，其关键创新包括：1) 一种显式整合 TS 群体级属性保存的训练方法，以及2) 一个双通道编码器架构，以更好地捕捉 TS 数据结构。实验结果显示，PaD-TS 在主要基准数据集上，将真实和合成数据间的平均 CC 分布偏移分数改善了 5.9 倍，同时在个体级真实性上保持与最先进模型相当的性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted for publication at AAAI-2025, 8 pages",
      "pdf_url": "http://arxiv.org/pdf/2501.00910v1",
      "published_date": "2025-01-01 17:53:43 UTC",
      "updated_date": "2025-01-01 17:53:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:25:37.837701"
    },
    {
      "arxiv_id": "2501.00906v2",
      "title": "Large Language Model Based Multi-Agent System Augmented Complex Event Processing Pipeline for Internet of Multimedia Things",
      "title_zh": "翻译失败",
      "authors": [
        "Talha Zeeshan",
        "Abhishek Kumar",
        "Susanna Pirttikangas",
        "Sasu Tarkoma"
      ],
      "abstract": "This paper presents the development and evaluation of a Large Language Model\n(LLM), also known as foundation models, based multi-agent system framework for\ncomplex event processing (CEP) with a focus on video query processing use\ncases. The primary goal is to create a proof-of-concept (POC) that integrates\nstate-of-the-art LLM orchestration frameworks with publish/subscribe (pub/sub)\ntools to address the integration of LLMs with current CEP systems. Utilizing\nthe Autogen framework in conjunction with Kafka message brokers, the system\ndemonstrates an autonomous CEP pipeline capable of handling complex workflows.\nExtensive experiments evaluate the system's performance across varying\nconfigurations, complexities, and video resolutions, revealing the trade-offs\nbetween functionality and latency. The results show that while higher agent\ncount and video complexities increase latency, the system maintains high\nconsistency in narrative coherence. This research builds upon and contributes\nto, existing novel approaches to distributed AI systems, offering detailed\ninsights into integrating such systems into existing infrastructures.",
      "tldr_zh": "本研究提出了一种基于 Large Language Model (LLM) 的多智能体系统框架，用于增强复杂事件处理 (CEP) 管道，专注于视频查询处理的 Internet of Multimedia Things 应用。该框架整合了 Autogen 编排工具和 Kafka 消息代理，构建了一个自主 CEP 系统，能够处理复杂的视频工作流。实验评估显示，不同配置、视频复杂度和分辨率会影响系统性能，导致代理数量增加和复杂性提升时延迟上升，但系统在叙述一致性方面保持高水平。该工作为分布式 AI 系统提供了关键见解，并探讨了其与现有基础设施的整合路径。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.00906v2",
      "published_date": "2025-01-01 17:38:40 UTC",
      "updated_date": "2025-01-03 07:47:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:26:13.085869"
    },
    {
      "arxiv_id": "2501.14778v1",
      "title": "Advancing Trustworthy AI for Sustainable Development: Recommendations for Standardising AI Incident Reporting",
      "title_zh": "翻译失败",
      "authors": [
        "Avinash Agarwal",
        "Manisha J Nene"
      ],
      "abstract": "The increasing use of AI technologies has led to increasing AI incidents,\nposing risks and causing harm to individuals, organizations, and society. This\nstudy recognizes and addresses the lack of standardized protocols for reliably\nand comprehensively gathering such incident data crucial for preventing future\nincidents and developing mitigating strategies. Specifically, this study\nanalyses existing open-access AI-incident databases through a systematic\nmethodology and identifies nine gaps in current AI incident reporting\npractices. Further, it proposes nine actionable recommendations to enhance\nstandardization efforts to address these gaps. Ensuring the trustworthiness of\nenabling technologies such as AI is necessary for sustainable digital\ntransformation. Our research promotes the development of standards to prevent\nfuture AI incidents and promote trustworthy AI, thus facilitating achieving the\nUN sustainable development goals. Through international cooperation,\nstakeholders can unlock the transformative potential of AI, enabling a\nsustainable and inclusive future for all.",
      "tldr_zh": "这篇论文分析了AI事件（AI incidents）的增加所带来的风险，并指出缺乏标准化报告协议阻碍了事件数据的可靠收集和未来风险缓解。研究通过系统方法审视现有开放访问AI事件数据库，识别了九个报告实践中的关键差距，并提出了九个可行动推荐来提升标准化努力。最终，这些推荐旨在促进可信赖AI（trustworthy AI）的开发，防止未来事件，并支持联合国可持续发展目标（Sustainable Development Goals），从而推动可持续数字转型和包容性未来。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "8 pages, 10 tables, and 1 figure. Accepted at the International\n  Telecommunication Union (ITU) Kaleidoscope 2024",
      "pdf_url": "http://arxiv.org/pdf/2501.14778v1",
      "published_date": "2025-01-01 17:34:57 UTC",
      "updated_date": "2025-01-01 17:34:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:26:32.727769"
    },
    {
      "arxiv_id": "2501.00891v1",
      "title": "Demystifying Online Clustering of Bandits: Enhanced Exploration Under Stochastic and Smoothed Adversarial Contexts",
      "title_zh": "翻译失败",
      "authors": [
        "Zhuohua Li",
        "Maoli Liu",
        "Xiangxiang Dai",
        "John C. S. Lui"
      ],
      "abstract": "The contextual multi-armed bandit (MAB) problem is crucial in sequential\ndecision-making. A line of research, known as online clustering of bandits,\nextends contextual MAB by grouping similar users into clusters, utilizing\nshared features to improve learning efficiency. However, existing algorithms,\nwhich rely on the upper confidence bound (UCB) strategy, struggle to gather\nadequate statistical information to accurately identify unknown user clusters.\nAs a result, their theoretical analyses require several strong assumptions\nabout the \"diversity\" of contexts generated by the environment, leading to\nimpractical settings, complicated analyses, and poor practical performance.\nRemoving these assumptions has been a long-standing open problem in the\nclustering of bandits literature. In this paper, we provide two solutions to\nthis open problem. First, following the i.i.d. context generation setting in\nexisting studies, we propose two novel algorithms, UniCLUB and PhaseUniCLUB,\nwhich incorporate enhanced exploration mechanisms to accelerate cluster\nidentification. Remarkably, our algorithms require substantially weaker\nassumptions while achieving regret bounds comparable to prior work. Second,\ninspired by the smoothed analysis framework, we propose a more practical\nsetting that eliminates the requirement for i.i.d. context generation used in\nprevious studies, thus enhancing the performance of existing algorithms for\nonline clustering of bandits. Our technique can be applied to both graph-based\nand set-based clustering of bandits frameworks. Extensive evaluations on both\nsynthetic and real-world datasets demonstrate that our proposed algorithms\nconsistently outperform existing approaches.",
      "tldr_zh": "这篇论文探讨了 contextual multi-armed bandit (MAB) 中的 online clustering of bandits 问题，强调通过将类似用户分组来提升学习效率，但现有依赖 upper confidence bound (UCB) 策略的算法因无法准确识别集群而需强假设。作者提出两种新算法，UniCLUB 和 PhaseUniCLUB，采用增强探索机制在 i.i.d. 上下文生成设置下加速集群识别，同时显著弱化假设并实现与现有工作相当的 regret bounds。此外，论文引入 smoothed analysis 框架，消除 i.i.d. 要求，使算法更适用于 graph-based 和 set-based clustering，并在合成及真实数据集的实验中表现出优于现有方法的性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.00891v1",
      "published_date": "2025-01-01 16:38:29 UTC",
      "updated_date": "2025-01-01 16:38:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:26:39.254456"
    },
    {
      "arxiv_id": "2501.00885v1",
      "title": "Representation in large language models",
      "title_zh": "大型语言模型中的表示",
      "authors": [
        "Cameron C. Yetman"
      ],
      "abstract": "The extraordinary success of recent Large Language Models (LLMs) on a diverse\narray of tasks has led to an explosion of scientific and philosophical\ntheorizing aimed at explaining how they do what they do. Unfortunately,\ndisagreement over fundamental theoretical issues has led to stalemate, with\nentrenched camps of LLM optimists and pessimists often committed to very\ndifferent views of how these systems work. Overcoming stalemate requires\nagreement on fundamental questions, and the goal of this paper is to address\none such question, namely: is LLM behavior driven partly by\nrepresentation-based information processing of the sort implicated in\nbiological cognition, or is it driven entirely by processes of memorization and\nstochastic table look-up? This is a question about what kind of algorithm LLMs\nimplement, and the answer carries serious implications for higher level\nquestions about whether these systems have beliefs, intentions, concepts,\nknowledge, and understanding. I argue that LLM behavior is partially driven by\nrepresentation-based information processing, and then I describe and defend a\nseries of practical techniques for investigating these representations and\ndeveloping explanations on their basis. The resulting account provides a\ngroundwork for future theorizing about language models and their successors.",
      "tldr_zh": "这篇论文探讨大型语言模型（LLMs）行为的核心问题：它们是否部分依赖表示-based 信息处理（如生物认知），还是完全由记忆和随机表查找驱动。作者论证 LLMs 的行为确实部分受表示-based 处理影响，并提出了一系列实用技术来调查这些表示并基于此构建解释。该研究为解决更高层次问题（如 LLMs 是否具备信念、意图和知识）提供了理论基础，并推动未来语言模型研究的进展。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Draft of paper under review. 27 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.00885v1",
      "published_date": "2025-01-01 16:19:48 UTC",
      "updated_date": "2025-01-01 16:19:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:26:42.706237"
    },
    {
      "arxiv_id": "2501.00884v1",
      "title": "Diversity Optimization for Travelling Salesman Problem via Deep Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Qi Li",
        "Zhiguang Cao",
        "Yining Ma",
        "Yaoxin Wu",
        "Yue-Jiao Gong"
      ],
      "abstract": "Existing neural methods for the Travelling Salesman Problem (TSP) mostly aim\nat finding a single optimal solution. To discover diverse yet high-quality\nsolutions for Multi-Solution TSP (MSTSP), we propose a novel deep reinforcement\nlearning based neural solver, which is primarily featured by an encoder-decoder\nstructured policy. Concretely, on the one hand, a Relativization Filter (RF) is\ndesigned to enhance the robustness of the encoder to affine transformations of\nthe instances, so as to potentially improve the quality of the found solutions.\nOn the other hand, a Multi-Attentive Adaptive Active Search (MA3S) is tailored\nto allow the decoders to strike a balance between the optimality and diversity.\nExperimental evaluations on benchmark instances demonstrate the superiority of\nour method over recent neural baselines across different metrics, and its\ncompetitive performance against state-of-the-art traditional heuristics with\nsignificantly reduced computational time, ranging from $1.3\\times$ to\n$15\\times$ faster. Furthermore, we demonstrate that our method can also be\napplied to the Capacitated Vehicle Routing Problem (CVRP).",
      "tldr_zh": "该研究针对 Multi-Solution TSP (MSTSP) 提出了一种基于深度强化学习的神经求解器，旨在发现多样性高且高质量的解，而非单一最优解。该求解器采用编码器-解码器结构，引入 Relativization Filter (RF) 来增强编码器对实例仿射变换的鲁棒性，以及 Multi-Attentive Adaptive Active Search (MA3S) 在解码器中平衡解的最优性和多样性。在基准实例上的实验表明，该方法在多种指标上优于现有神经基线，并与传统启发式方法性能相当，但计算时间减少了 1.3 至 15 倍；此外，该方法还可扩展应用于 Capacitated Vehicle Routing Problem (CVRP)。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.00884v1",
      "published_date": "2025-01-01 16:08:40 UTC",
      "updated_date": "2025-01-01 16:08:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:27:51.466916"
    },
    {
      "arxiv_id": "2501.00862v1",
      "title": "DiffETM: Diffusion Process Enhanced Embedded Topic Model",
      "title_zh": "DiffETM：扩散过程增强嵌入式主题模型",
      "authors": [
        "Wei Shao",
        "Mingyang Liu",
        "Linqi Song"
      ],
      "abstract": "The embedded topic model (ETM) is a widely used approach that assumes the\nsampled document-topic distribution conforms to the logistic normal\ndistribution for easier optimization. However, this assumption oversimplifies\nthe real document-topic distribution, limiting the model's performance. In\nresponse, we propose a novel method that introduces the diffusion process into\nthe sampling process of document-topic distribution to overcome this limitation\nand maintain an easy optimization process. We validate our method through\nextensive experiments on two mainstream datasets, proving its effectiveness in\nimproving topic modeling performance.",
      "tldr_zh": "本文提出 DiffETM，一种增强型 Embedded Topic Model (ETM)，通过引入 diffusion process 到文档-主题分布的采样过程，克服了 ETM 中 logistic normal 分布假设的简化局限，同时保持优化过程的简便性。该方法旨在更准确地捕捉真实文档-主题分布的核心特征。在两个主流数据集上的广泛实验证明，DiffETM 显著提高了主题建模的性能。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "5 pages, 2 figures, Accepted by ICASSP 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.00862v1",
      "published_date": "2025-01-01 15:15:39 UTC",
      "updated_date": "2025-01-01 15:15:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:27:54.424799"
    },
    {
      "arxiv_id": "2501.00855v2",
      "title": "What is a Social Media Bot? A Global Comparison of Bot and Human Characteristics",
      "title_zh": "翻译失败",
      "authors": [
        "Lynnette Hui Xian Ng",
        "Kathleen M. Carley"
      ],
      "abstract": "Chatter on social media is 20% bots and 80% humans. Chatter by bots and\nhumans is consistently different: bots tend to use linguistic cues that can be\neasily automated while humans use cues that require dialogue understanding.\nBots use words that match the identities they choose to present, while humans\nmay send messages that are not related to the identities they present. Bots and\nhumans differ in their communication structure: sampled bots have a star\ninteraction structure, while sampled humans have a hierarchical structure.\nThese conclusions are based on a large-scale analysis of social media tweets\nacross ~200mil users across 7 events. Social media bots took the world by storm\nwhen social-cybersecurity researchers realized that social media users not only\nconsisted of humans but also of artificial agents called bots. These bots wreck\nhavoc online by spreading disinformation and manipulating narratives. Most\nresearch on bots are based on special-purposed definitions, mostly predicated\non the event studied. This article first begins by asking, \"What is a bot?\",\nand we study the underlying principles of how bots are different from humans.\nWe develop a first-principle definition of a social media bot. With this\ndefinition as a premise, we systematically compare characteristics between bots\nand humans across global events, and reflect on how the software-programmed bot\nis an Artificial Intelligent algorithm, and its potential for evolution as\ntechnology advances. Based on our results, we provide recommendations for the\nuse and regulation of bots. Finally, we discuss open challenges and future\ndirections: Detect, to systematically identify these automated and potentially\nevolving bots; Differentiate, to evaluate the goodness of the bot in terms of\ntheir content postings and relationship interactions; Disrupt, to moderate the\nimpact of malicious bots.",
      "tldr_zh": "这篇论文通过对约2亿社交媒体用户和7个全球事件的推文进行大规模分析，定义了社交媒体bots，并系统比较了bots和人类的特征，包括语言提示（bots使用易自动化的词汇，humans需对话理解）、身份匹配（bots匹配其呈现身份，humans可能不相关）以及沟通结构（bots为星型交互，humans为层次结构）。研究发现，社交媒体对话中20%的内容由bots产生，并强调bots作为AI算法的演变潜力。论文据此提出bots使用和监管的推荐，并讨论未来挑战，如Detect（检测）、Differentiate（区分）和Disrupt（破坏）恶意bots。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.00855v2",
      "published_date": "2025-01-01 14:45:43 UTC",
      "updated_date": "2025-02-25 20:11:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:27:19.931607"
    },
    {
      "arxiv_id": "2501.00830v2",
      "title": "LLM+AL: Bridging Large Language Models and Action Languages for Complex Reasoning about Actions",
      "title_zh": "LLM+AL：桥接大语言模型和动作语言用于复杂动作推理",
      "authors": [
        "Adam Ishay",
        "Joohyung Lee"
      ],
      "abstract": "Large Language Models (LLMs) have made significant strides in various\nintelligent tasks but still struggle with complex action reasoning tasks that\nrequire systematic search. To address this limitation, we propose a method that\nbridges the natural language understanding capabilities of LLMs with the\nsymbolic reasoning strengths of action languages. Our approach, termed\n\"LLM+AL,\" leverages the LLM's strengths in semantic parsing and commonsense\nknowledge generation alongside the action language's proficiency in automated\nreasoning based on encoded knowledge. We compare LLM+AL against\nstate-of-the-art LLMs, including ChatGPT-4, Claude 3 Opus, Gemini Ultra 1.0,\nand o1-preview, using benchmarks for complex reasoning about actions. Our\nfindings indicate that, although all methods exhibit errors, LLM+AL, with\nrelatively minimal human corrections, consistently leads to correct answers,\nwhereas standalone LLMs fail to improve even with human feedback. LLM+AL also\ncontributes to automated generation of action languages.",
      "tldr_zh": "该研究提出 LLM+AL 方法，将 Large Language Models (LLMs) 的自然语言理解和常识知识生成能力与 action languages 的符号推理优势相结合，旨在解决 LLMs 在复杂动作推理任务中的系统性搜索难题。相比 ChatGPT-4、Claude 3 Opus、Gemini Ultra 1.0 和 o1-preview 等先进模型，LLM+AL 在基准测试中表现出色，即使仅需最小人类修正即可 consistently 得出正确答案，而独立 LLMs 即使接受人类反馈也无法显著改善。该方法还贡献了 action languages 的自动生成功能，为复杂推理任务提供了更可靠的框架。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "42 pages",
      "pdf_url": "http://arxiv.org/pdf/2501.00830v2",
      "published_date": "2025-01-01 13:20:01 UTC",
      "updated_date": "2025-02-04 14:37:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:27:45.107277"
    },
    {
      "arxiv_id": "2501.00829v1",
      "title": "An LLM-Empowered Adaptive Evolutionary Algorithm For Multi-Component Deep Learning Systems",
      "title_zh": "LLM赋能的适应性进化算法用于多组件深度学习系统",
      "authors": [
        "Haoxiang Tian",
        "Xingshuo Han",
        "Guoquan Wu",
        "An Guo",
        "Yuan Zhou. Jie Zhang",
        "Shuo Li",
        "Jun Wei",
        "Tianwei Zhang"
      ],
      "abstract": "Multi-objective evolutionary algorithms (MOEAs) are widely used for searching\noptimal solutions in complex multi-component applications. Traditional MOEAs\nfor multi-component deep learning (MCDL) systems face challenges in enhancing\nthe search efficiency while maintaining the diversity. To combat these, this\npaper proposes $\\mu$MOEA, the first LLM-empowered adaptive evolutionary search\nalgorithm to detect safety violations in MCDL systems. Inspired by the\ncontext-understanding ability of Large Language Models (LLMs), $\\mu$MOEA\npromotes the LLM to comprehend the optimization problem and generate an initial\npopulation tailed to evolutionary objectives. Subsequently, it employs adaptive\nselection and variation to iteratively produce offspring, balancing the\nevolutionary efficiency and diversity. During the evolutionary process, to\nnavigate away from the local optima, $\\mu$MOEA integrates the evolutionary\nexperience back into the LLM. This utilization harnesses the LLM's quantitative\nreasoning prowess to generate differential seeds, breaking away from current\noptimal solutions. We evaluate $\\mu$MOEA in finding safety violations of MCDL\nsystems, and compare its performance with state-of-the-art MOEA methods.\nExperimental results show that $\\mu$MOEA can significantly improve the\nefficiency and diversity of the evolutionary search.",
      "tldr_zh": "本论文提出了一种基于大型语言模型(LLM)的自适应进化算法$\\mu$MOEA，用于多组件深度学习(MCDL)系统中的多目标进化算法(MOEAs)，旨在提升搜索效率同时保持多样性。$\\mu$MOEA利用LLM的上下文理解能力生成针对进化目标的初始种群，并通过自适应选择和变异机制迭代产生后代，同时将进化经验反馈给LLM以生成差异种子，避免局部最优。实验结果显示，$\\mu$MOEA在检测MCDL系统安全违规方面显著提高了进化搜索的效率和多样性，比现有MOEA方法表现出色。",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "9",
      "pdf_url": "http://arxiv.org/pdf/2501.00829v1",
      "published_date": "2025-01-01 13:19:58 UTC",
      "updated_date": "2025-01-01 13:19:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:28:07.078732"
    },
    {
      "arxiv_id": "2501.00828v1",
      "title": "Embedding Style Beyond Topics: Analyzing Dispersion Effects Across Different Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Benjamin Icard",
        "Evangelia Zve",
        "Lila Sainero",
        "Alice Breton",
        "Jean-Gabriel Ganascia"
      ],
      "abstract": "This paper analyzes how writing style affects the dispersion of embedding\nvectors across multiple, state-of-the-art language models. While early\ntransformer models primarily aligned with topic modeling, this study examines\nthe role of writing style in shaping embedding spaces. Using a literary corpus\nthat alternates between topics and styles, we compare the sensitivity of\nlanguage models across French and English. By analyzing the particular impact\nof style on embedding dispersion, we aim to better understand how language\nmodels process stylistic information, contributing to their overall\ninterpretability.",
      "tldr_zh": "这篇论文分析了写作风格如何影响不同最先进语言模型的嵌入向量分散性（embedding vectors dispersion），超越了传统的主题建模（topic modeling）。研究者使用一个交替主题和风格的文学语料库，比较了法语和英语语言模型的敏感性。结果显示，风格对嵌入空间（embedding spaces）的特定影响有助于更好地理解语言模型如何处理风格信息（stylistic information），从而提升其整体可解释性（interpretability）。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "To appear in the Proceedings of the 31st International Conference on\n  Computational Linguistics (COLING 2025), Abu Dhabi",
      "pdf_url": "http://arxiv.org/pdf/2501.00828v1",
      "published_date": "2025-01-01 13:17:16 UTC",
      "updated_date": "2025-01-01 13:17:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:28:31.435695"
    },
    {
      "arxiv_id": "2501.00826v2",
      "title": "LLM-Powered Multi-Agent System for Automated Crypto Portfolio Management",
      "title_zh": "LLM驱动的多智能体系统用于自动化加密货币投资组合管理",
      "authors": [
        "Yichen Luo",
        "Yebo Feng",
        "Jiahua Xu",
        "Paolo Tasca",
        "Yang Liu"
      ],
      "abstract": "Cryptocurrency investment is inherently difficult due to its shorter history\ncompared to traditional assets, the need to integrate vast amounts of data from\nvarious modalities, and the requirement for complex reasoning. While deep\nlearning approaches have been applied to address these challenges, their\nblack-box nature raises concerns about trust and explainability. Recently,\nlarge language models (LLMs) have shown promise in financial applications due\nto their ability to understand multi-modal data and generate explainable\ndecisions. However, single LLM faces limitations in complex, comprehensive\ntasks such as asset investment. These limitations are even more pronounced in\ncryptocurrency investment, where LLMs have less domain-specific knowledge in\ntheir training corpora.\n  To overcome these challenges, we propose an explainable, multi-modal,\nmulti-agent framework for cryptocurrency investment. Our framework uses\nspecialized agents that collaborate within and across teams to handle subtasks\nsuch as data analysis, literature integration, and investment decision-making\nfor the top 30 cryptocurrencies by market capitalization. The expert training\nmodule fine-tunes agents using multi-modal historical data and professional\ninvestment literature, while the multi-agent investment module employs\nreal-time data to make informed cryptocurrency investment decisions. Unique\nintrateam and interteam collaboration mechanisms enhance prediction accuracy by\nadjusting final predictions based on confidence levels within agent teams and\nfacilitating information sharing between teams. Empirical evaluation using data\nfrom November 2023 to September 2024 demonstrates that our framework\noutperforms single-agent models and market benchmarks in classification, asset\npricing, portfolio, and explainability performance.",
      "tldr_zh": "该研究针对加密货币投资的复杂性（如数据多模态和推理需求），提出了一种基于LLM的Multi-Agent System，用于自动化投资管理。该系统包括专门的智能体团队，负责数据分析、文献整合和决策生成，并通过intrateam和interteam协作机制（如基于置信度的预测调整）来提升准确性和可解释性。实验评估使用2023年11月至2024年9月的数据表明，该框架在分类、资产定价、投资组合和可解释性方面均优于单智能体模型和市场基准。",
      "categories": [
        "q-fin.TR",
        "cs.AI"
      ],
      "primary_category": "q-fin.TR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.00826v2",
      "published_date": "2025-01-01 13:08:17 UTC",
      "updated_date": "2025-01-07 00:15:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:29:20.519942"
    },
    {
      "arxiv_id": "2501.00823v2",
      "title": "Decoupling Knowledge and Reasoning in Transformers: A Modular Architecture with Generalized Cross-Attention",
      "title_zh": "翻译失败",
      "authors": [
        "Zhenyu Guo",
        "Wenguang Chen"
      ],
      "abstract": "Transformers have achieved remarkable success across diverse domains, but\ntheir monolithic architecture presents challenges in interpretability,\nadaptability, and scalability. This paper introduces a novel modular\nTransformer architecture that explicitly decouples knowledge and reasoning\nthrough a generalized cross-attention mechanism to a globally shared knowledge\nbase with layer-specific transformations, specifically designed for effective\nknowledge retrieval. Critically, we provide a rigorous mathematical derivation\ndemonstrating that the Feed-Forward Network (FFN) in a standard Transformer is\na specialized case (a closure) of this generalized cross-attention, revealing\nits role in implicit knowledge retrieval and validating our design. This\ntheoretical framework provides a new lens for understanding FFNs and lays the\nfoundation for future research exploring enhanced interpretability,\nadaptability, and scalability, enabling richer interplay with external\nknowledge bases and other systems.",
      "tldr_zh": "本论文提出了一种模块化Transformer架构，通过广义跨注意力（generalized cross-attention）机制将知识和推理解耦，连接到一个全局共享的知识库，并应用层特定的转换以实现高效知识检索。这种设计解决了传统Transformer的解释性、适应性和可扩展性挑战。论文通过严格的数学推导证明，标准Transformer中的Feed-Forward Network (FFN)是广义跨注意力的一个特殊情况（closure），揭示了FFN在隐式知识检索中的作用。该框架为未来研究提供了新视角，支持与外部知识库的互动，提升模型的可解释性和可扩展性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.00823v2",
      "published_date": "2025-01-01 12:55:57 UTC",
      "updated_date": "2025-01-06 14:26:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:28:55.486788"
    },
    {
      "arxiv_id": "2501.01473v1",
      "title": "Unraveling Indirect In-Context Learning Using Influence Functions",
      "title_zh": "翻译失败",
      "authors": [
        "Hadi Askari",
        "Shivanshu Gupta",
        "Terry Tong",
        "Fei Wang",
        "Anshuman Chhabra",
        "Muhao Chen"
      ],
      "abstract": "This work introduces a novel paradigm for generalized In-Context Learning\n(ICL), termed Indirect In-Context Learning. In Indirect ICL, we explore\ndemonstration selection strategies tailored for two distinct real-world\nscenarios: Mixture of Tasks and Noisy Demonstrations. We systematically\nevaluate the effectiveness of Influence Functions (IFs) as a selection tool for\nthese settings, highlighting the potential for IFs to better capture the\ninformativeness of examples within the demonstration pool. For the Mixture of\nTasks setting, demonstrations are drawn from 28 diverse tasks, including MMLU,\nBigBench, StrategyQA, and CommonsenseQA. We demonstrate that combining\nBertScore-Recall (BSR) with an IF surrogate model can significantly improve\nperformance, leading to average absolute accuracy gains of 0.37\\% and 1.45\\%\nfor 3-shot and 5-shot setups when compared to traditional ICL metrics. In the\nNoisy Demonstrations setting, we examine scenarios where demonstrations might\nbe mislabeled. Our experiments show that reweighting traditional ICL selectors\n(BSR and Cosine Similarity) with IF-based selectors boosts accuracy by an\naverage of 2.90\\% for Cosine Similarity and 2.94\\% for BSR on noisy GLUE\nbenchmarks. In sum, we propose a robust framework for demonstration selection\nthat generalizes beyond traditional ICL, offering valuable insights into the\nrole of IFs for Indirect ICL.",
      "tldr_zh": "这篇论文引入了 Indirect In-Context Learning (Indirect ICL) 的新范式，使用 Influence Functions (IFs) 来优化演示选择策略，针对 Mixture of Tasks 和 Noisy Demonstrations 等真实场景。通过结合 BertScore-Recall (BSR) 和 IF 模型，该方法在 Mixture of Tasks 设置中（涉及 28 个多样任务如 MMLU 和 BigBench），使 3-shot 和 5-shot 设定的准确率分别提升 0.37% 和 1.45%。在 Noisy Demonstrations 设置中，重新加权传统选择器（如 BSR 和 Cosine Similarity），在 noisy GLUE benchmarks 上平均提升准确率 2.90% 和 2.94%。总体上，该框架为超越传统 ICL 的鲁棒演示选择提供了宝贵见解。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Under Review",
      "pdf_url": "http://arxiv.org/pdf/2501.01473v1",
      "published_date": "2025-01-01 12:37:12 UTC",
      "updated_date": "2025-01-01 12:37:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:29:26.590911"
    },
    {
      "arxiv_id": "2501.03257v1",
      "title": "Breaking Through the Spike: Spike Window Decoding for Accelerated and Precise Automatic Speech Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Wei Zhang",
        "Tian-Hao Zhang",
        "Chao Luo",
        "Hui Zhou",
        "Chao Yang",
        "Xinyuan Qian",
        "Xu-Cheng Yin"
      ],
      "abstract": "Recently, end-to-end automatic speech recognition has become the mainstream\napproach in both industry and academia. To optimize system performance in\nspecific scenarios, the Weighted Finite-State Transducer (WFST) is extensively\nused to integrate acoustic and language models, leveraging its capacity to\nimplicitly fuse language models within static graphs, thereby ensuring robust\nrecognition while also facilitating rapid error correction. However, WFST\nnecessitates a frame-by-frame search of CTC posterior probabilities through\nautoregression, which significantly hampers inference speed. In this work, we\nthoroughly investigate the spike property of CTC outputs and further propose\nthe conjecture that adjacent frames to non-blank spikes carry semantic\ninformation beneficial to the model. Building on this, we propose the Spike\nWindow Decoding algorithm, which greatly improves the inference speed by making\nthe number of frames decoded in WFST linearly related to the number of spiking\nframes in the CTC output, while guaranteeing the recognition performance. Our\nmethod achieves SOTA recognition accuracy with significantly accelerates\ndecoding speed, proven across both AISHELL-1 and large-scale In-House datasets,\nestablishing a pioneering approach for integrating CTC output with WFST.",
      "tldr_zh": "本研究针对端到端自动语音识别（ASR）中的推理速度问题，提出Spike Window Decoding算法，以优化Weighted Finite-State Transducer (WFST)的解码过程。算法基于对CTC输出中spike属性的分析，假设非空白spike的相邻帧携带语义信息，从而使WFST解码帧数与CTC输出中的spike帧数线性相关，大大加速推理速度，同时保持识别性能。在AISHELL-1和In-House数据集上，该方法实现了SOTA识别准确率，并显著提升解码效率，为CTC输出与WFST集成的开创性方法奠定了基础。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted by ICASSP 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.03257v1",
      "published_date": "2025-01-01 12:20:07 UTC",
      "updated_date": "2025-01-01 12:20:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:32:35.756343"
    },
    {
      "arxiv_id": "2501.01472v1",
      "title": "Augmented Contrastive Clustering with Uncertainty-Aware Prototyping for Time Series Test Time Adaptation",
      "title_zh": "针对时间序列测试时适应的增强对比聚类方法，结合不确定性感知原型",
      "authors": [
        "Peiliang Gong",
        "Mohamed Ragab",
        "Min Wu",
        "Zhenghua Chen",
        "Yongyi Su",
        "Xiaoli Li",
        "Daoqiang Zhang"
      ],
      "abstract": "Test-time adaptation aims to adapt pre-trained deep neural networks using\nsolely online unlabelled test data during inference. Although TTA has shown\npromise in visual applications, its potential in time series contexts remains\nlargely unexplored. Existing TTA methods, originally designed for visual tasks,\nmay not effectively handle the complex temporal dynamics of real-world time\nseries data, resulting in suboptimal adaptation performance. To address this\ngap, we propose Augmented Contrastive Clustering with Uncertainty-aware\nPrototyping (ACCUP), a straightforward yet effective TTA method for time series\ndata. Initially, our approach employs augmentation ensemble on the time series\ndata to capture diverse temporal information and variations, incorporating\nuncertainty-aware prototypes to distill essential characteristics.\nAdditionally, we introduce an entropy comparison scheme to selectively acquire\nmore confident predictions, enhancing the reliability of pseudo labels.\nFurthermore, we utilize augmented contrastive clustering to enhance feature\ndiscriminability and mitigate error accumulation from noisy pseudo labels,\npromoting cohesive clustering within the same class while facilitating clear\nseparation between different classes. Extensive experiments conducted on three\nreal-world time series datasets and an additional visual dataset demonstrate\nthe effectiveness and generalization potential of the proposed method,\nadvancing the underexplored realm of TTA for time series data.",
      "tldr_zh": "本文提出 Augmented Contrastive Clustering with Uncertainty-aware Prototyping (ACCUP)，一种针对时间序列数据的 Test-time Adaptation (TTA) 方法，以解决现有视觉导向方法在处理复杂时间动态时的不足。ACCUP 通过 augmentation ensemble 和 uncertainty-aware prototypes 捕捉数据多样性，并引入 entropy comparison scheme 来选择更可靠的伪标签，从而提升预测准确性。同时，该方法利用 augmented contrastive clustering 增强特征的可区分度，减少噪声伪标签的错误积累，并在同一类别内促进聚类凝聚。实验在三个真实世界时间序列数据集和一个视觉数据集上证明了 ACCUP 的有效性和泛化潜力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01472v1",
      "published_date": "2025-01-01 11:45:17 UTC",
      "updated_date": "2025-01-01 11:45:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:30:55.557711"
    },
    {
      "arxiv_id": "2501.00803v1",
      "title": "Reasoning-Oriented and Analogy-Based Methods for Locating and Editing in Zero-Shot Event-Relational Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Jingyao Tang",
        "Lishuang Li",
        "Liteng Mi",
        "Haiming Wu",
        "Hongbin Lu"
      ],
      "abstract": "Zero-shot event-relational reasoning is an important task in natural language\nprocessing, and existing methods jointly learn a variety of event-relational\nprefixes and inference-form prefixes to achieve such tasks. However, training\nprefixes consumes large computational resources and lacks interpretability.\nAdditionally, learning various relational and inferential knowledge\ninefficiently exploits the connections between tasks. Therefore, we first\npropose a method for Reasoning-Oriented Locating and Editing (ROLE), which\nlocates and edits the key modules of the language model for reasoning about\nevent relations, enhancing interpretability and also resource-efficiently\noptimizing the reasoning ability. Subsequently, we propose a method for\nAnalogy-Based Locating and Editing (ABLE), which efficiently exploits the\nsimilarities and differences between tasks to optimize the zero-shot reasoning\ncapability. Experimental results show that ROLE improves interpretability and\nreasoning performance with reduced computational cost. ABLE achieves SOTA\nresults in zero-shot reasoning.",
      "tldr_zh": "本研究针对零-shot event-relational reasoning 的问题，提出两种创新方法来解决现有方法的计算资源消耗大、可解释性差和任务间连接利用不足的问题。Reasoning-Oriented Locating and Editing (ROLE) 通过定位和编辑语言模型的关键模块，增强事件关系推理的可解释性和资源效率。Analogy-Based Locating and Editing (ABLE) 则利用任务间的相似性和差异来优化零样本推理能力。实验结果显示，ROLE 显著提高了推理性能和可解释性，同时降低了计算成本，而 ABLE 在零-shot reasoning 中达到了 SOTA 水平。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.00803v1",
      "published_date": "2025-01-01 11:02:08 UTC",
      "updated_date": "2025-01-01 11:02:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:31:07.322269"
    },
    {
      "arxiv_id": "2501.00798v2",
      "title": "Make Shuffling Great Again: A Side-Channel Resistant Fisher-Yates Algorithm for Protecting Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Leonard Puškáč",
        "Marek Benovič",
        "Jakub Breier",
        "Xiaolu Hou"
      ],
      "abstract": "Neural network models implemented in embedded devices have been shown to be\nsusceptible to side-channel attacks (SCAs), allowing recovery of proprietary\nmodel parameters, such as weights and biases. There are already available\ncountermeasure methods currently used for protecting cryptographic\nimplementations that can be tailored to protect embedded neural network models.\nShuffling, a hiding-based countermeasure that randomly shuffles the order of\ncomputations, was shown to be vulnerable to SCA when the Fisher-Yates algorithm\nis used. In this paper, we propose a design of an SCA-secure version of the\nFisher-Yates algorithm. By integrating the masking technique for modular\nreduction and Blakely's method for modular multiplication, we effectively\nremove the vulnerability in the division operation that led to side-channel\nleakage in the original version of the algorithm. We experimentally evaluate\nthat the countermeasure is effective against SCA by implementing a correlation\npower analysis attack on an embedded neural network model implemented on ARM\nCortex-M4. Compared to the original proposal, the memory overhead is $2\\times$\nthe biggest layer of the network, while the time overhead varies from $4\\%$ to\n$0.49\\%$ for a layer with $100$ and $1000$ neurons, respectively.",
      "tldr_zh": "本研究针对神经网络模型在嵌入式设备中易受侧信道攻击（SCAs）的问题，提出了一种改进的 Fisher-Yates 算法，以增强 shuffling 反制措施的安全性。该算法通过整合 masking technique 用于模减法和 Blakely's method 用于模乘法，成功消除原版算法中 division 操作的漏洞，从而提高对 SCAs 的抵抗力。实验结果显示，在 ARM Cortex-M4 平台上针对嵌入式神经网络模型进行的相关功率分析（Correlation Power Analysis）攻击中，该方法有效；内存开销为网络最大层面的 2 倍，而时间开销从 4%（100 个神经元层）降至 0.49%（1000 个神经元层）。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.00798v2",
      "published_date": "2025-01-01 10:46:22 UTC",
      "updated_date": "2025-04-23 07:49:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:32:00.583855"
    },
    {
      "arxiv_id": "2501.03256v1",
      "title": "AI-ANNE: (A) (N)eural (N)et for (E)xploration: Transferring Deep Learning Models onto Microcontrollers and Embedded Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Dennis Klinkhammer"
      ],
      "abstract": "This working paper explores the integration of neural networks onto\nresource-constrained embedded systems like a Raspberry Pi Pico / Raspberry Pi\nPico 2. A TinyML aproach transfers neural networks directly on these\nmicrocontrollers, enabling real-time, low-latency, and energy-efficient\ninference while maintaining data privacy. Therefore, AI-ANNE: (A) (N)eural\n(N)et for (E)xploration will be presented, which facilitates the transfer of\npre-trained models from high-performance platforms like TensorFlow and Keras\nonto microcontrollers, using a lightweight programming language like\nMicroPython. This approach demonstrates how neural network architectures, such\nas neurons, layers, density and activation functions can be implemented in\nMicroPython in order to deal with the computational limitations of embedded\nsystems. Based on the Raspberry Pi Pico / Raspberry Pi Pico 2, two different\nneural networks on microcontrollers are presented for an example of data\nclassification. As an further application example, such a microcontroller can\nbe used for condition monitoring, where immediate corrective measures are\ntriggered on the basis of sensor data. Overall, this working paper presents a\nvery easy-to-implement way of using neural networks on energy-efficient devices\nsuch as microcontrollers. This makes AI-ANNE: (A) (N)eural (N)et for\n(E)xploration not only suited for practical use, but also as an educational\ntool with clear insights into how neural networks operate.",
      "tldr_zh": "这篇论文介绍了AI-ANNE框架，一种用于将深度学习模型从高性能平台如TensorFlow和Keras转移到资源受限的嵌入式系统（如Raspberry Pi Pico）的TinyML方法。框架利用MicroPython实现神经网络组件，包括神经元、层、密度和激活函数，从而实现实时、低延迟和能效高的推理，同时确保数据隐私。实验展示了在微控制器上进行数据分类和条件监控的应用示例，证明了AI-ANNE的易用性，并将其定位为教育工具，以提供对神经网络运作的清晰洞见。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "I.2.5; K.3.2"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages, 8 tables",
      "pdf_url": "http://arxiv.org/pdf/2501.03256v1",
      "published_date": "2025-01-01 10:29:55 UTC",
      "updated_date": "2025-01-01 10:29:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:33:35.286038"
    },
    {
      "arxiv_id": "2501.00790v2",
      "title": "LENS-XAI: Redefining Lightweight and Explainable Network Security through Knowledge Distillation and Variational Autoencoders for Scalable Intrusion Detection in Cybersecurity",
      "title_zh": "LENS-XAI：通过知识蒸馏和变分自编码器重新定义轻量级和",
      "authors": [
        "Muhammet Anil Yagiz",
        "Polat Goktas"
      ],
      "abstract": "The rapid proliferation of Industrial Internet of Things (IIoT) systems\nnecessitates advanced, interpretable, and scalable intrusion detection systems\n(IDS) to combat emerging cyber threats. Traditional IDS face challenges such as\nhigh computational demands, limited explainability, and inflexibility against\nevolving attack patterns. To address these limitations, this study introduces\nthe Lightweight Explainable Network Security framework (LENS-XAI), which\ncombines robust intrusion detection with enhanced interpretability and\nscalability. LENS-XAI integrates knowledge distillation, variational\nautoencoder models, and attribution-based explainability techniques to achieve\nhigh detection accuracy and transparency in decision-making. By leveraging a\ntraining set comprising 10% of the available data, the framework optimizes\ncomputational efficiency without sacrificing performance. Experimental\nevaluation on four benchmark datasets: Edge-IIoTset, UKM-IDS20, CTU-13, and\nNSL-KDD, demonstrates the framework's superior performance, achieving detection\naccuracies of 95.34%, 99.92%, 98.42%, and 99.34%, respectively. Additionally,\nthe framework excels in reducing false positives and adapting to complex attack\nscenarios, outperforming existing state-of-the-art methods. Key strengths of\nLENS-XAI include its lightweight design, suitable for resource-constrained\nenvironments, and its scalability across diverse IIoT and cybersecurity\ncontexts. Moreover, the explainability module enhances trust and transparency,\ncritical for practical deployment in dynamic and sensitive applications. This\nresearch contributes significantly to advancing IDS by addressing computational\nefficiency, feature interpretability, and real-world applicability. Future work\ncould focus on extending the framework to ensemble AI systems for distributed\nenvironments, further enhancing its robustness and adaptability.",
      "tldr_zh": "本研究引入了LENS-XAI框架，通过知识蒸馏(knowledge distillation)、变分自编码器(variational autoencoders)和基于归因的解释性技术，重新定义了轻量级且可解释的网络安全系统，以提升入侵检测系统(IDS)在工业物联网(IIoT)环境中的可扩展性和准确性。LENS-XAI仅使用10%的训练数据，就优化了计算效率，同时确保了决策过程的透明度和适应性。实验在Edge-IIoTset、UKM-IDS20、CTU-13和NSL-KDD等四个基准数据集上，分别实现了95.34%、99.92%、98.42%和99.34%的检测准确率，并显著降低了假阳性率，优于现有方法。该框架的优势在于其轻量级设计，适用于资源受限环境，并通过增强解释性提升了实际部署中的信任和适用性。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CY",
        "cs.ET"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.00790v2",
      "published_date": "2025-01-01 10:00:49 UTC",
      "updated_date": "2025-01-07 23:43:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:32:13.560765"
    },
    {
      "arxiv_id": "2501.00779v1",
      "title": "REM: A Scalable Reinforced Multi-Expert Framework for Multiplex Influence Maximization",
      "title_zh": "翻译失败",
      "authors": [
        "Huyen Nguyen",
        "Hieu Dam",
        "Nguyen Do",
        "Cong Tran",
        "Cuong Pham"
      ],
      "abstract": "In social online platforms, identifying influential seed users to maximize\ninfluence spread is a crucial as it can greatly diminish the cost and efforts\nrequired for information dissemination. While effective, traditional methods\nfor Multiplex Influence Maximization (MIM) have reached their performance\nlimits, prompting the emergence of learning-based approaches. These novel\nmethods aim for better generalization and scalability for more sizable graphs\nbut face significant challenges, such as (1) inability to handle unknown\ndiffusion patterns and (2) reliance on high-quality training samples. To\naddress these issues, we propose the Reinforced Expert Maximization framework\n(REM). REM leverages a Propagation Mixture of Experts technique to encode\ndynamic propagation of large multiplex networks effectively in order to\ngenerate enhanced influence propagation. Noticeably, REM treats a generative\nmodel as a policy to autonomously generate different seed sets and learn how to\nimprove them from a Reinforcement Learning perspective. Extensive experiments\non several real-world datasets demonstrate that REM surpasses state-of-the-art\nmethods in terms of influence spread, scalability, and inference time in\ninfluence maximization tasks.",
      "tldr_zh": "该论文提出REM框架（Reinforced Expert Maximization），一种可扩展的多专家强化学习方法，用于解决Multiplex Influence Maximization（MIM）问题，该问题旨在通过识别有影响力的种子用户最大化社交平台上的信息传播。REM采用Propagation Mixture of Experts技术来有效编码大型多路网络的动态传播，并将生成模型视为策略，通过Reinforcement Learning自主生成和优化种子集，从而应对未知扩散模式和高质训练样本依赖的挑战。实验在多个真实数据集上表明，REM在影响传播、可扩展性和推理时间方面均优于现有最先进方法。",
      "categories": [
        "cs.SI",
        "cs.AI"
      ],
      "primary_category": "cs.SI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.00779v1",
      "published_date": "2025-01-01 09:13:09 UTC",
      "updated_date": "2025-01-01 09:13:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:31:53.970836"
    },
    {
      "arxiv_id": "2501.00773v1",
      "title": "Revisiting Graph Neural Networks on Graph-level Tasks: Comprehensive Experiments, Analysis, and Improvements",
      "title_zh": "重新审视图神经网络在图级任务上的全面实验、分析和",
      "authors": [
        "Haoyang Li",
        "Yuming Xu",
        "Chen Jason Zhang",
        "Alexander Zhou",
        "Lei Chen",
        "Qing Li"
      ],
      "abstract": "Graphs are essential data structures for modeling complex interactions in\ndomains such as social networks, molecular structures, and biological systems.\nGraph-level tasks, which predict properties or classes for the entire graph,\nare critical for applications, such as molecular property prediction and\nsubgraph counting. Graph Neural Networks (GNNs) have shown promise in these\ntasks, but their evaluations are often limited to narrow datasets, tasks, and\ninconsistent experimental setups, restricting their generalizability. To\naddress these limitations, we propose a unified evaluation framework for\ngraph-level GNNs. This framework provides a standardized setting to evaluate\nGNNs across diverse datasets, various graph tasks (e.g., graph classification\nand regression), and challenging scenarios, including noisy, imbalanced, and\nfew-shot graphs. Additionally, we propose a novel GNN model with enhanced\nexpressivity and generalization capabilities. Specifically, we enhance the\nexpressivity of GNNs through a $k$-path rooted subgraph approach, enabling the\nmodel to effectively count subgraphs (e.g., paths and cycles). Moreover, we\nintroduce a unified graph contrastive learning algorithm for graphs across\ndiverse domains, which adaptively removes unimportant edges to augment graphs,\nthereby significantly improving generalization performance. Extensive\nexperiments demonstrate that our model achieves superior performance against\nfourteen effective baselines across twenty-seven graph datasets, establishing\nit as a robust and generalizable model for graph-level tasks.",
      "tldr_zh": "这篇论文重新审视了Graph Neural Networks (GNNs)在图级任务（如图分类和回归）中的应用，强调了现有评估的局限性，包括数据集狭窄和实验设置不一致。作者提出一个统一的评估框架，用于在多样数据集和挑战场景（如噪声、不平衡或少样本图）中标准化评估GNNs，同时设计了一个新颖GNN模型，通过$k$-path rooted subgraph方法增强模型的表达性以有效计数子图，并引入统一的图对比学习算法（graph contrastive learning）来自适应移除不重要边，提高泛化性能。实验结果显示，该模型在27个图数据集上优于14个基线模型，证明了其鲁棒性和有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.00773v1",
      "published_date": "2025-01-01 08:48:53 UTC",
      "updated_date": "2025-01-01 08:48:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:32:25.300804"
    },
    {
      "arxiv_id": "2501.02000v1",
      "title": "Multi-Center Study on Deep Learning-Assisted Detection and Classification of Fetal Central Nervous System Anomalies Using Ultrasound Imaging",
      "title_zh": "多中心研究：使用超声成像进行深度学习辅助的胎儿中枢神经系统异常检测和分类",
      "authors": [
        "Yang Qi",
        "Jiaxin Cai",
        "Jing Lu",
        "Runqing Xiong",
        "Rongshang Chen",
        "Liping Zheng",
        "Duo Ma"
      ],
      "abstract": "Prenatal ultrasound evaluates fetal growth and detects congenital\nabnormalities during pregnancy, but the examination of ultrasound images by\nradiologists requires expertise and sophisticated equipment, which would\notherwise fail to improve the rate of identifying specific types of fetal\ncentral nervous system (CNS) abnormalities and result in unnecessary patient\nexaminations. We construct a deep learning model to improve the overall\naccuracy of the diagnosis of fetal cranial anomalies to aid prenatal diagnosis.\nIn our collected multi-center dataset of fetal craniocerebral anomalies\ncovering four typical anomalies of the fetal central nervous system (CNS):\nanencephaly, encephalocele (including meningocele), holoprosencephaly, and\nrachischisis, patient-level prediction accuracy reaches 94.5%, with an AUROC\nvalue of 99.3%. In the subgroup analyzes, our model is applicable to the entire\ngestational period, with good identification of fetal anomaly types for any\ngestational period. Heatmaps superimposed on the ultrasound images not only\nprovide a visual interpretation for the algorithm but also provide an intuitive\nvisual aid to the physician by highlighting key areas that need to be reviewed,\nhelping the physician to quickly identify and validate key areas. Finally, the\nretrospective reader study demonstrates that by combining the automatic\nprediction of the DL system with the professional judgment of the radiologist,\nthe diagnostic accuracy and efficiency can be effectively improved and the\nmisdiagnosis rate can be reduced, which has an important clinical application\nprospect.",
      "tldr_zh": "本研究构建了一个深度学习模型，用于辅助检测和分类胎儿中枢神经系统(CNS)异常的超声图像，旨在提高产前诊断准确率并减少不必要的检查。模型在多中心数据集上针对四种典型异常（anencephaly、encephalocele、holoprosencephaly和rachischisis）的患者级预测准确率达94.5%，AUROC值为99.3%，并适用于整个妊娠期。热力图(heatmaps)提供视觉解释，帮助医生快速识别关键区域，而结合放射科医生的判断可有效提升诊断效率和准确性，降低误诊率，具有重要临床应用前景。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02000v1",
      "published_date": "2025-01-01 07:56:26 UTC",
      "updated_date": "2025-01-01 07:56:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:35:00.621022"
    },
    {
      "arxiv_id": "2501.00759v1",
      "title": "Enhancing Transformers for Generalizable First-Order Logical Entailment",
      "title_zh": "翻译失败",
      "authors": [
        "Tianshi Zheng",
        "Jiazheng Wang",
        "Zihao Wang",
        "Jiaxin Bai",
        "Hang Yin",
        "Zheye Deng",
        "Yangqiu Song",
        "Jianxin Li"
      ],
      "abstract": "Transformers, as a fundamental deep learning architecture, have demonstrated\nremarkable capabilities in reasoning. This paper investigates the generalizable\nfirst-order logical reasoning ability of transformers with their parameterized\nknowledge and explores ways to improve it. The first-order reasoning capability\nof transformers is assessed through their ability to perform first-order\nlogical entailment, which is quantitatively measured by their performance in\nanswering knowledge graph queries. We establish connections between (1) two\ntypes of distribution shifts studied in out-of-distribution generalization and\n(2) the unseen knowledge and query settings discussed in the task of knowledge\ngraph query answering, enabling a characterization of fine-grained\ngeneralizability. Results on our comprehensive dataset show that transformers\noutperform previous methods specifically designed for this task and provide\ndetailed empirical evidence on the impact of input query syntax, token\nembedding, and transformer architectures on the reasoning capability of\ntransformers. Interestingly, our findings reveal a mismatch between positional\nencoding and other design choices in transformer architectures employed in\nprior practices. This discovery motivates us to propose a more sophisticated,\nlogic-aware architecture, TEGA, to enhance the capability for generalizable\nfirst-order logical entailment in transformers.",
      "tldr_zh": "本文探讨了Transformers在第一-order logical entailment方面的泛化能力，通过知识图谱查询来定量评估其推理性能，并将分布偏移与未见查询设置联系起来，表征细粒度的泛化性。实验结果显示，Transformers在全面数据集上优于先前专门设计的方法，并揭示了输入查询语法、token embedding和架构设计（如位置编码）对推理能力的影响。特别地，研究发现现有Transformer架构中位置编码与其他选择的失配问题。最终，作者提出了一种逻辑感知的TEGA架构，以显著提升Transformers的generalizable first-order logical entailment能力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "17 pages",
      "pdf_url": "http://arxiv.org/pdf/2501.00759v1",
      "published_date": "2025-01-01 07:05:32 UTC",
      "updated_date": "2025-01-01 07:05:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:32:49.872127"
    },
    {
      "arxiv_id": "2501.01999v2",
      "title": "On the Utility of Equivariance and Symmetry Breaking in Deep Learning Architectures on Point Clouds",
      "title_zh": "翻译失败",
      "authors": [
        "Sharvaree Vadgama",
        "Mohammad Mohaiminul Islam",
        "Domas Buracus",
        "Christian Shewmake",
        "Erik Bekkers"
      ],
      "abstract": "This paper explores the key factors that influence the performance of models\nworking with point clouds, across different tasks of varying geometric\ncomplexity. In this work, we explore the trade-offs between flexibility and\nweight-sharing introduced by equivariant layers, assessing when equivariance\nboosts or detracts from performance. It is often argued that providing more\ninformation as input improves a model's performance. However, if this\nadditional information breaks certain properties, such as $\\SE(3)$\nequivariance, does it remain beneficial? We identify the key aspects of\nequivariant and non-equivariant architectures that drive success in different\ntasks by benchmarking them on segmentation, regression, and generation tasks\nacross multiple datasets with increasing complexity. We observe a positive\nimpact of equivariance, which becomes more pronounced with increasing task\ncomplexity, even when strict equivariance is not required.",
      "tldr_zh": "这篇论文探讨了等变性(Equivariance)和对称性破坏(Symmetry Breaking)在点云(Point Clouds)深度学习架构中的效用，评估这些特性在不同几何复杂任务中的性能权衡。研究通过benchmark等变和非等变模型在分割、回归和生成任务上的表现，跨多个复杂度数据集进行比较。结果显示，等变性对模型性能有正面影响，且这种益处随着任务复杂度的增加而更显著，即使不强制要求SE(3) 等变性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "19 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.01999v2",
      "published_date": "2025-01-01 07:00:41 UTC",
      "updated_date": "2025-03-05 15:26:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:36:12.696901"
    },
    {
      "arxiv_id": "2501.00755v1",
      "title": "An AI-powered Bayesian generative modeling approach for causal inference in observational studies",
      "title_zh": "翻译失败",
      "authors": [
        "Qiao Liu",
        "Wing Hung Wong"
      ],
      "abstract": "Causal inference in observational studies with high-dimensional covariates\npresents significant challenges. We introduce CausalBGM, an AI-powered Bayesian\ngenerative modeling approach that captures the causal relationship among\ncovariates, treatment, and outcome variables. The core innovation of CausalBGM\nlies in its ability to estimate the individual treatment effect (ITE) by\nlearning individual-specific distributions of a low-dimensional latent feature\nset (e.g., latent confounders) that drives changes in both treatment and\noutcome. This approach not only effectively mitigates confounding effects but\nalso provides comprehensive uncertainty quantification, offering reliable and\ninterpretable causal effect estimates at the individual level. CausalBGM adopts\na Bayesian model and uses a novel iterative algorithm to update the model\nparameters and the posterior distribution of latent features until convergence.\nThis framework leverages the power of AI to capture complex dependencies among\nvariables while adhering to the Bayesian principles. Extensive experiments\ndemonstrate that CausalBGM consistently outperforms state-of-the-art methods,\nparticularly in scenarios with high-dimensional covariates and large-scale\ndatasets. Its Bayesian foundation ensures statistical rigor, providing robust\nand well-calibrated posterior intervals. By addressing key limitations of\nexisting methods, CausalBGM emerges as a robust and promising framework for\nadvancing causal inference in modern applications in fields such as genomics,\nhealthcare, and social sciences. CausalBGM is maintained at the website\nhttps://causalbgm.readthedocs.io/.",
      "tldr_zh": "该研究提出 CausalBGM，一种 AI 驱动的 Bayesian 生成模型方法，用于解决观测研究中高维协变量的因果推断挑战。该方法的核心创新是通过学习个体特定的低维潜在特征集（如潜在混杂因素）来估计个体治疗效果 (ITE)，从而有效缓解混杂效应并提供可靠的可解释不确定性量化。实验结果显示，CausalBGM 优于现有方法，尤其在高维数据集场景下，并适用于基因组学、医疗和社会科学等领域。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "stat.ME"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.00755v1",
      "published_date": "2025-01-01 06:52:45 UTC",
      "updated_date": "2025-01-01 06:52:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:35:25.618286"
    },
    {
      "arxiv_id": "2501.00750v2",
      "title": "Beyond Text: Implementing Multimodal Large Language Model-Powered Multi-Agent Systems Using a No-Code Platform",
      "title_zh": "翻译失败",
      "authors": [
        "Cheonsu Jeong"
      ],
      "abstract": "This study proposes the design and implementation of a multimodal LLM-based\nMulti-Agent System (MAS) leveraging a No-Code platform to address the practical\nconstraints and significant entry barriers associated with AI adoption in\nenterprises. Advanced AI technologies, such as Large Language Models (LLMs),\noften pose challenges due to their technical complexity and high implementation\ncosts, making them difficult for many organizations to adopt. To overcome these\nlimitations, this research develops a No-Code-based Multi-Agent System designed\nto enable users without programming knowledge to easily build and manage AI\nsystems. The study examines various use cases to validate the applicability of\nAI in business processes, including code generation from image-based notes,\nAdvanced RAG-based question-answering systems, text-based image generation, and\nvideo generation using images and prompts. These systems lower the barriers to\nAI adoption, empowering not only professional developers but also general users\nto harness AI for significantly improved productivity and efficiency. By\ndemonstrating the scalability and accessibility of No-Code platforms, this\nstudy advances the democratization of AI technologies within enterprises and\nvalidates the practical applicability of Multi-Agent Systems, ultimately\ncontributing to the widespread adoption of AI across various industries.",
      "tldr_zh": "这篇论文提出了一种基于 No-Code 平台的 Multimodal Large Language Model (LLM)-powered Multi-Agent System (MAS)，旨在克服企业采用 AI 的技术复杂性和高成本障碍，让非程序员用户轻松构建和管理 AI 系统。研究通过实际用例验证了其适用性，包括从图像笔记生成代码、Advanced RAG-based 问答系统、文本到图像生成，以及使用图像和提示生成视频。这些创新降低了 AI 采用门槛，提高了生产力和效率，并推动了 Multi-Agent Systems 在各行业的可扩展性和民主化。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "22 pages, 27 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.00750v2",
      "published_date": "2025-01-01 06:36:56 UTC",
      "updated_date": "2025-01-29 06:49:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:36:29.062922"
    },
    {
      "arxiv_id": "2501.00745v2",
      "title": "Dynamics of Adversarial Attacks on Large Language Model-Based Search Engines",
      "title_zh": "翻译失败",
      "authors": [
        "Xiyang Hu"
      ],
      "abstract": "The increasing integration of Large Language Model (LLM) based search engines\nhas transformed the landscape of information retrieval. However, these systems\nare vulnerable to adversarial attacks, especially ranking manipulation attacks,\nwhere attackers craft webpage content to manipulate the LLM's ranking and\npromote specific content, gaining an unfair advantage over competitors. In this\npaper, we study the dynamics of ranking manipulation attacks. We frame this\nproblem as an Infinitely Repeated Prisoners' Dilemma, where multiple players\nstrategically decide whether to cooperate or attack. We analyze the conditions\nunder which cooperation can be sustained, identifying key factors such as\nattack costs, discount rates, attack success rates, and trigger strategies that\ninfluence player behavior. We identify tipping points in the system dynamics,\ndemonstrating that cooperation is more likely to be sustained when players are\nforward-looking. However, from a defense perspective, we find that simply\nreducing attack success probabilities can, paradoxically, incentivize attacks\nunder certain conditions. Furthermore, defensive measures to cap the upper\nbound of attack success rates may prove futile in some scenarios. These\ninsights highlight the complexity of securing LLM-based systems. Our work\nprovides a theoretical foundation and practical insights for understanding and\nmitigating their vulnerabilities, while emphasizing the importance of adaptive\nsecurity strategies and thoughtful ecosystem design.",
      "tldr_zh": "这篇论文研究了Large Language Model (LLM) 基于搜索引擎面临的排名操纵攻击，将其建模为Infinitely Repeated Prisoners' Dilemma，分析玩家在合作与攻击间的策略决策。关键因素包括攻击成本、折扣率、攻击成功率和触发策略，这些影响了合作的可持续性，并揭示了系统动态中的临界点，如玩家前瞻性强时合作更可能维持。论文发现，从防御角度降低攻击成功率有时反而会激励攻击，且限制成功率上限在某些场景下无效。这些见解为理解和缓解LLM系统的漏洞提供了理论基础，强调了自适应安全策略和生态系统设计的必要性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.GT",
        "cs.IR",
        "econ.TH"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.00745v2",
      "published_date": "2025-01-01 06:23:26 UTC",
      "updated_date": "2025-05-15 21:22:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:36:35.055112"
    },
    {
      "arxiv_id": "2501.00743v1",
      "title": "AttriReBoost: A Gradient-Free Propagation Optimization Method for Cold Start Mitigation in Attribute Missing Graphs",
      "title_zh": "翻译失败",
      "authors": [
        "Mengran Li",
        "Chaojun Ding",
        "Junzhou Chen",
        "Wenbin Xing",
        "Cong Ye",
        "Ronghui Zhang",
        "Songlin Zhuang",
        "Jia Hu",
        "Tony Z. Qiu",
        "Huijun Gao"
      ],
      "abstract": "Missing attribute issues are prevalent in the graph learning, leading to\nbiased outcomes in Graph Neural Networks (GNNs). Existing methods that rely on\nfeature propagation are prone to cold start problem, particularly when dealing\nwith attribute resetting and low-degree nodes, which hinder effective\npropagation and convergence. To address these challenges, we propose\nAttriReBoost (ARB), a novel method that incorporates propagation-based method\nto mitigate cold start problems in attribute-missing graphs. ARB enhances\nglobal feature propagation by redefining initial boundary conditions and\nstrategically integrating virtual edges, thereby improving node connectivity\nand ensuring more stable and efficient convergence. This method facilitates\ngradient-free attribute reconstruction with lower computational overhead. The\nproposed method is theoretically grounded, with its convergence rigorously\nestablished. Extensive experiments on several real-world benchmark datasets\ndemonstrate the effectiveness of ARB, achieving an average accuracy improvement\nof 5.11% over state-of-the-art methods. Additionally, ARB exhibits remarkable\ncomputational efficiency, processing a large-scale graph with 2.49 million\nnodes in just 16 seconds on a single GPU. Our code is available at\nhttps://github.com/limengran98/ARB.",
      "tldr_zh": "本论文提出 AttriReBoost (ARB)，一种无梯度传播优化方法，用于缓解 Graph Neural Networks (GNNs) 在属性缺失图中的冷启动问题，如属性重置和低度节点带来的传播和收敛挑战。ARB 通过重新定义初始边界条件并策略性地整合虚拟边，增强全局特征传播、改善节点连通性，并实现高效的梯度-free 属性重建。实验结果显示，该方法在多个真实世界基准数据集上比最先进方法平均准确率提高 5.11%，并在处理包含 249 万节点的图时，仅需 16 秒即可完成计算，展示了其高效性和实用性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.00743v1",
      "published_date": "2025-01-01 06:19:56 UTC",
      "updated_date": "2025-01-01 06:19:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:36:00.558800"
    },
    {
      "arxiv_id": "2501.01470v1",
      "title": "Balance-aware Sequence Sampling Makes Multi-modal Learning Better",
      "title_zh": "翻译失败",
      "authors": [
        "Zhi-Hao Guan"
      ],
      "abstract": "To address the modality imbalance caused by data heterogeneity, existing\nmulti-modal learning (MML) approaches primarily focus on balancing this\ndifference from the perspective of optimization objectives. However, almost all\nexisting methods ignore the impact of sample sequences, i.e., an inappropriate\ntraining order tends to trigger learning bias in the model, further\nexacerbating modality imbalance. In this paper, we propose Balance-aware\nSequence Sampling (BSS) to enhance the robustness of MML. Specifically, we\nfirst define a multi-perspective measurer to evaluate the balance degree of\neach sample. Via the evaluation, we employ a heuristic scheduler based on\ncurriculum learning (CL) that incrementally provides training subsets,\nprogressing from balanced to imbalanced samples to rebalance MML. Moreover,\nconsidering that sample balance may evolve as the model capability increases,\nwe propose a learning-based probabilistic sampling method to dynamically update\nthe training sequences at the epoch level, further improving MML performance.\nExtensive experiments on widely used datasets demonstrate the superiority of\nour method compared with state-of-the-art (SOTA) MML approaches.",
      "tldr_zh": "该论文针对多模态学习（MML）中的模态不平衡问题，指出现有方法忽略了样本序列的影响，可能导致模型学习偏差加剧不平衡。作者提出Balance-aware Sequence Sampling (BSS)方法，包括一个多视角测量器评估样本平衡度，以及基于课程学习（CL）的启发式调度器和学习-based probabilistic sampling，以动态调整训练序列，提升MML的鲁棒性。在广泛使用的数据集上进行的实验显示，BSS比最先进（SOTA）MML方法表现出色，证明了其有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.01470v1",
      "published_date": "2025-01-01 06:19:55 UTC",
      "updated_date": "2025-01-01 06:19:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:37:05.656867"
    },
    {
      "arxiv_id": "2501.00741v2",
      "title": "Towards End-to-End Neuromorphic Voxel-based 3D Object Reconstruction Without Physical Priors",
      "title_zh": "翻译失败",
      "authors": [
        "Chuanzhi Xu",
        "Langyi Chen",
        "Haodong Chen",
        "Vera Chung",
        "Qiang Qu"
      ],
      "abstract": "Neuromorphic cameras, also known as event cameras, are asynchronous\nbrightness-change sensors that can capture extremely fast motion without\nsuffering from motion blur, making them particularly promising for 3D\nreconstruction in extreme environments. However, existing research on 3D\nreconstruction using monocular neuromorphic cameras is limited, and most of the\nmethods rely on estimating physical priors and employ complex multi-step\npipelines. In this work, we propose an end-to-end method for dense voxel 3D\nreconstruction using neuromorphic cameras that eliminates the need to estimate\nphysical priors. Our method incorporates a novel event representation to\nenhance edge features, enabling the proposed feature-enhancement model to learn\nmore effectively. Additionally, we introduced Optimal Binarization Threshold\nSelection Principle as a guideline for future related work, using the optimal\nreconstruction results achieved with threshold optimization as the benchmark.\nOur method achieves a 54.6% improvement in reconstruction accuracy compared to\nthe baseline method.",
      "tldr_zh": "本研究针对 neuromorphic cameras（事件相机）在极端环境下的3D重建问题，提出了一种端到端（end-to-end）的体素-based（voxel-based）方法，无需估计物理先验（physical priors），从而简化了传统多步管道。该方法引入了新型事件表示来增强边缘特征，并利用特征增强模型实现更有效的学习，同时提出 Optimal Binarization Threshold Selection Principle 作为未来工作的指导原则，以阈值优化基准获得最佳重建效果。实验结果显示，与基线方法相比，该方法将重建准确率提高了54.6%。这为高效的 neuromorphic 3D重建提供了新途径。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "6 pages, 15 figures, 5 tables, accepted by IEEE International\n  Conference on Multimedia & Expo (ICME) 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.00741v2",
      "published_date": "2025-01-01 06:07:03 UTC",
      "updated_date": "2025-03-26 12:16:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:36:48.613273"
    },
    {
      "arxiv_id": "2501.01998v2",
      "title": "SmartSpatial: Enhancing the 3D Spatial Arrangement Capabilities of Stable Diffusion Models and Introducing a Novel 3D Spatial Evaluation Framework",
      "title_zh": "SmartSpatial：增强 Stable Diffusion 模型的 3D 空间安排能力并引入一个新型 3D 空间评估框架",
      "authors": [
        "Mao Xun Huang",
        "Brian J Chan",
        "Hen-Hsen Huang"
      ],
      "abstract": "Stable Diffusion models have made remarkable strides in generating\nphotorealistic images from text prompts but often falter when tasked with\naccurately representing complex spatial arrangements, particularly involving\nintricate 3D relationships. To address this limitation, we introduce\nSmartSpatial, an innovative approach that not only enhances the spatial\narrangement capabilities of Stable Diffusion but also fosters AI-assisted\ncreative workflows through 3D-aware conditioning and attention-guided\nmechanisms. SmartSpatial incorporates depth information injection and\ncross-attention control to ensure precise object placement, delivering notable\nimprovements in spatial accuracy metrics. In conjunction with SmartSpatial, we\npresent SmartSpatialEval, a comprehensive evaluation framework that bridges\ncomputational spatial accuracy with qualitative artistic assessments.\nExperimental results show that SmartSpatial significantly outperforms existing\nmethods, setting new benchmarks for spatial fidelity in AI-driven art and\ncreativity.",
      "tldr_zh": "该研究发现，Stable Diffusion 模型在处理复杂3D空间安排时存在不足，特别是涉及精细3D关系的图像生成问题。为此，提出SmartSpatial，一种创新方法，通过注入深度信息和交叉注意力控制（cross-attention control）来提升模型的空间安排能力，并支持AI辅助的创意工作流程。SmartSpatial利用3D-aware conditioning和attention-guided机制，确保物体精确放置，从而显著改善空间准确性指标。同时，引入SmartSpatialEval框架，该框架结合计算空间准确性和定性艺术评估，实验结果显示SmartSpatial在AI驱动艺术中超越现有方法，并设立了新的空间保真度基准。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "9 pages",
      "pdf_url": "http://arxiv.org/pdf/2501.01998v2",
      "published_date": "2025-01-01 04:52:18 UTC",
      "updated_date": "2025-02-23 19:22:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:37:00.611615"
    },
    {
      "arxiv_id": "2501.00715v1",
      "title": "eRevise+RF: A Writing Evaluation System for Assessing Student Essay Revisions and Providing Formative Feedback",
      "title_zh": "eRevise+RF：用于评估学生作文修订并提供形成性反馈的写作评估系统",
      "authors": [
        "Zhexiong Liu",
        "Diane Litman",
        "Elaine Wang",
        "Tianwen Li",
        "Mason Gobat",
        "Lindsay Clare Matsumura",
        "Richard Correnti"
      ],
      "abstract": "The ability to revise essays in response to feedback is important for\nstudents' writing success. An automated writing evaluation (AWE) system that\nsupports students in revising their essays is thus essential. We present\neRevise+RF, an enhanced AWE system for assessing student essay revisions (e.g.,\nchanges made to an essay to improve its quality in response to essay feedback)\nand providing revision feedback. We deployed the system with 6 teachers and 406\nstudents across 3 schools in Pennsylvania and Louisiana. The results confirmed\nits effectiveness in (1) assessing student essays in terms of evidence usage,\n(2) extracting evidence and reasoning revisions across essays, and (3)\ndetermining revision success in responding to feedback. The evaluation also\nsuggested eRevise+RF is a helpful system for young students to improve their\nargumentative writing skills through revision and formative feedback.",
      "tldr_zh": "本研究介绍了 eRevise+RF，这是一个增强型自动写作评估(AWE)系统，旨在评估学生对论文的修改（如证据使用和推理改进）并提供形成性反馈，以帮助学生响应反馈提升写作质量。\n系统在宾夕法尼亚和路易斯安那州的3所学校部署，涉及6名教师和406名学生，通过提取证据及推理修订，并判断修改是否成功响应反馈。\n实验结果证实，eRevise+RF 有效提升了年轻学生的论说性写作技能，为写作教育提供了实用工具。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.00715v1",
      "published_date": "2025-01-01 03:49:48 UTC",
      "updated_date": "2025-01-01 03:49:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:37:18.662661"
    },
    {
      "arxiv_id": "2501.00707v1",
      "title": "Everywhere Attack: Attacking Locally and Globally to Boost Targeted Transferability",
      "title_zh": "翻译失败",
      "authors": [
        "Hui Zeng",
        "Sanshuai Cui",
        "Biwei Chen",
        "Anjie Peng"
      ],
      "abstract": "Adversarial examples' (AE) transferability refers to the phenomenon that AEs\ncrafted with one surrogate model can also fool other models. Notwithstanding\nremarkable progress in untargeted transferability, its targeted counterpart\nremains challenging. This paper proposes an everywhere scheme to boost targeted\ntransferability. Our idea is to attack a victim image both globally and\nlocally. We aim to optimize 'an army of targets' in every local image region\ninstead of the previous works that optimize a high-confidence target in the\nimage. Specifically, we split a victim image into non-overlap blocks and\njointly mount a targeted attack on each block. Such a strategy mitigates\ntransfer failures caused by attention inconsistency between surrogate and\nvictim models and thus results in stronger transferability. Our approach is\nmethod-agnostic, which means it can be easily combined with existing\ntransferable attacks for even higher transferability. Extensive experiments on\nImageNet demonstrate that the proposed approach universally improves the\nstate-of-the-art targeted attacks by a clear margin, e.g., the transferability\nof the widely adopted Logit attack can be improved by 28.8%-300%.We also\nevaluate the crafted AEs on a real-world platform: Google Cloud Vision. Results\nfurther support the superiority of the proposed method.",
      "tldr_zh": "这篇论文提出了一种名为 Everywhere Attack 的方法，以提升对抗样本(Adversarial Examples, AEs)的针对性转移性。该方法通过同时在图像的全局和局部进行攻击，将图像分割为非重叠块，并在每个块上联合优化多个局部目标，从而缓解代理模型和受害模型之间注意力不一致的问题。实验结果显示，在 ImageNet 数据集上，该方法显著提升了现有攻击的转移性，例如将 Logit 攻击的转移率提高 28.8%-300%，并在真实平台如 Google Cloud Vision 上验证了其优越性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 6 figures, 8 tables, accepted by 2025AAAI",
      "pdf_url": "http://arxiv.org/pdf/2501.00707v1",
      "published_date": "2025-01-01 03:06:03 UTC",
      "updated_date": "2025-01-01 03:06:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:37:30.750435"
    },
    {
      "arxiv_id": "2501.00692v1",
      "title": "Adjoint sharding for very long context training of state space models",
      "title_zh": "翻译失败",
      "authors": [
        "Xingzi Xu",
        "Amir Tavanaei",
        "Kavosh Asadi",
        "Karim Bouyarmane"
      ],
      "abstract": "Despite very fast progress, efficiently training large language models (LLMs)\nin very long contexts remains challenging. Existing methods fall back to\ntraining LLMs with short contexts (a maximum of a few thousands tokens in\ntraining) and use inference time techniques when evaluating on long contexts\n(above 1M tokens context window at inference). As opposed to\nlong-context-inference, training on very long context input prompts is quickly\nlimited by GPU memory availability and by the prohibitively long training times\nit requires on state-of-the-art hardware. Meanwhile, many real-life\napplications require not only inference but also training/fine-tuning with long\ncontext on specific tasks. Such applications include, for example, augmenting\nthe context with various sources of raw reference information for fact\nextraction, fact summarization, or fact reconciliation tasks. We propose\nadjoint sharding, a novel technique that comprises sharding gradient\ncalculation during training to reduce memory requirements by orders of\nmagnitude, making training on very long context computationally tractable.\nAdjoint sharding is based on the adjoint method and computes equivalent\ngradients to backpropagation. We also propose truncated adjoint sharding to\nspeed up the algorithm while maintaining performance. We provide a distributed\nversion, and a paralleled version of adjoint sharding to further speed up\ntraining. Empirical results show the proposed adjoint sharding algorithm\nreduces memory usage by up to 3X with a 1.27B parameter large language model on\n1M context length training. This allows to increase the maximum context length\nduring training or fine-tuning of a 1.27B parameter model from 35K tokens to\nabove 100K tokens on a training infrastructure composed of five AWS P4\ninstances.",
      "tldr_zh": "该论文针对训练大型语言模型 (LLMs) 在长上下文上的挑战，提出了一种名为 adjoint sharding 的新技术，通过分片梯度计算来显著减少 GPU 内存需求，使长上下文训练变得可行。Adjoint sharding 基于 adjoint method 计算与反向传播等价的梯度，并引入 truncated adjoint sharding 以及分布式和并行版本来加速训练过程。实验结果显示，在 1.27B 参数的 LLM 上，该算法将内存使用减少高达 3 倍，允许最大上下文长度从 35K 标记提升到超过 100K 标记，使用五台 AWS P4 实例。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.00692v1",
      "published_date": "2025-01-01 01:10:59 UTC",
      "updated_date": "2025-01-01 01:10:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:38:00.315021"
    },
    {
      "arxiv_id": "2501.14777v1",
      "title": "Enhancing Supply Chain Resilience with Metaverse and ChatGPT Technologies",
      "title_zh": "利用 Metaverse 和 ChatGPT 技术提升供应链韧性",
      "authors": [
        "Oumaima Sarhir"
      ],
      "abstract": "Global supply lines have been severely disrupted by the COVID-19 epidemic and\nthe conflict between Russia and Ukraine, which has sharply increased the price\nof commodities and generated inflation. These incidents highlight how critical\nit is to improve supply chain resilience (SCRES) in order to fend off\nunforeseen setbacks. Controlling both internal and external interruptions, such\nas transportation problems brought on by natural catastrophes and wars, is the\nresponsibility of SCRES. Enhancing resilience in supply chains requires\naccurate and timely information transfer. Promising answers to these problems\ncan be found in the Metaverse and ChatGPT, two new digital technologies. The\nMetaverse may imitate real-world situations and offer dynamic, real-time 3D\nrepresentations of supply chain data by integrating blockchain, IoT, network\nconnection, and computer power.Large-scale natural language processing model\nChatGPT improves communication and data translation accuracy and speed. To\nmanage risk and facilitate decision making in Supply Chain management, firms\nshould increase information transmission, Speed and quality. This study aim to\nshow the importance of ChatGPT and Metaverse technologies to improve SCRES,\nwith an emphasis on the most important criteria for SCRES, and maturity factor\nthat can influence directly the SC development.",
      "tldr_zh": "该论文讨论了COVID-19疫情和俄乌冲突对全球供应链的破坏性影响，强调提升Supply Chain Resilience (SCRES)以应对内部外部中断的重要性。研究提出利用Metaverse和ChatGPT技术作为解决方案，其中Metaverse通过整合blockchain、IoT、网络连接和计算能力，提供动态实时3D模拟；ChatGPT作为大型自然语言处理模型，提升通信和数据翻译的准确性与速度。最终，论文旨在展示这些技术如何优化信息传输、管理风险和决策，支持SCRES的关键标准和影响供应链发展的成熟因素。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.14777v1",
      "published_date": "2025-01-01 00:21:28 UTC",
      "updated_date": "2025-01-01 00:21:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:38:12.211759"
    },
    {
      "arxiv_id": "2501.00840v1",
      "title": "Distilled Lifelong Self-Adaptation for Configurable Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Yulong Ye",
        "Tao Chen",
        "Miqing Li"
      ],
      "abstract": "Modern configurable systems provide tremendous opportunities for engineering\nfuture intelligent software systems. A key difficulty thereof is how to\neffectively self-adapt the configuration of a running system such that its\nperformance (e.g., runtime and throughput) can be optimized under time-varying\nworkloads. This unfortunately remains unaddressed in existing approaches as\nthey either overlook the available past knowledge or rely on static\nexploitation of past knowledge without reasoning the usefulness of information\nwhen planning for self-adaptation. In this paper, we tackle this challenging\nproblem by proposing DLiSA, a framework that self-adapts configurable systems.\nDLiSA comes with two properties: firstly, it supports lifelong planning, and\nthereby the planning process runs continuously throughout the lifetime of the\nsystem, allowing dynamic exploitation of the accumulated knowledge for rapid\nadaptation. Secondly, the planning for a newly emerged workload is boosted via\ndistilled knowledge seeding, in which the knowledge is dynamically purified\nsuch that only useful past configurations are seeded when necessary, mitigating\nmisleading information. Extensive experiments suggest that the proposed DLiSA\nsignificantly outperforms state-of-the-art approaches, demonstrating a\nperformance improvement of up to 229% and a resource acceleration of up to\n2.22x on generating promising adaptation configurations. All data and sources\ncan be found at our repository: https://github.com/ideas-labo/dlisa.",
      "tldr_zh": "论文提出DLiSA框架，用于可配置系统的终身自适应(self-adaptation)，旨在优化系统性能（如运行时和吞吐量）以应对时间变化的工作负载。DLiSA支持lifelong planning，实现持续规划并动态利用积累知识进行快速适应；同时，通过distilled knowledge seeding动态净化知识，只选择有用的过去配置以避免误导信息。实验结果表明，DLiSA相较于现有方法性能提升高达229%，资源加速高达2.22x，为智能软件系统的工程实践提供了显著改进。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted by the 2025 International Conference on Software Engineering\n  (ICSE 2025)",
      "pdf_url": "http://arxiv.org/pdf/2501.00840v1",
      "published_date": "2025-01-01 13:41:57 UTC",
      "updated_date": "2025-01-01 13:41:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T19:38:30.423788"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 42,
  "processed_papers_count": 42,
  "failed_papers_count": 1,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-21T19:38:49.458766"
}
[
  {
    "arxiv_id": "2404.00495v1",
    "title": "Configurable Safety Tuning of Language Models with Synthetic Preference Data",
    "authors": [
      "Victor Gallego"
    ],
    "abstract": "State-of-the-art language model fine-tuning techniques, such as Direct\nPreference Optimization (DPO), restrict user control by hard-coding predefined\nbehaviors into the model. To address this, we propose a novel method,\nConfigurable Safety Tuning (CST), that augments DPO using synthetic preference\ndata to facilitate flexible safety configuration of LLMs at inference time. CST\novercomes the constraints of vanilla DPO by introducing a system prompt\nspecifying safety configurations, enabling LLM deployers to disable/enable\nsafety preferences based on their need, just changing the system prompt. Our\nexperimental evaluations indicate that CST successfully manages different\nsafety configurations and retains the original functionality of LLMs, showing\nit is a robust method for configurable deployment. Data and models available at\nhttps://github.com/vicgalle/configurable-safety-tuning",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00495v1",
    "published_date": "2024-03-30 23:28:05 UTC",
    "updated_date": "2024-03-30 23:28:05 UTC"
  },
  {
    "arxiv_id": "2404.00492v1",
    "title": "Multi-hop Question Answering under Temporal Knowledge Editing",
    "authors": [
      "Keyuan Cheng",
      "Gang Lin",
      "Haoyang Fei",
      "Yuxuan zhai",
      "Lu Yu",
      "Muhammad Asif Ali",
      "Lijie Hu",
      "Di Wang"
    ],
    "abstract": "Multi-hop question answering (MQA) under knowledge editing (KE) has garnered\nsignificant attention in the era of large language models. However, existing\nmodels for MQA under KE exhibit poor performance when dealing with questions\ncontaining explicit temporal contexts. To address this limitation, we propose a\nnovel framework, namely TEMPoral knowLEdge augmented Multi-hop Question\nAnswering (TEMPLE-MQA). Unlike previous methods, TEMPLE-MQA first constructs a\ntime-aware graph (TAG) to store edit knowledge in a structured manner. Then,\nthrough our proposed inference path, structural retrieval, and joint reasoning\nstages, TEMPLE-MQA effectively discerns temporal contexts within the question\nquery. Experiments on benchmark datasets demonstrate that TEMPLE-MQA\nsignificantly outperforms baseline models. Additionally, we contribute a new\ndataset, namely TKEMQA, which serves as the inaugural benchmark tailored\nspecifically for MQA with temporal scopes.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "23 pages",
    "pdf_url": "http://arxiv.org/pdf/2404.00492v1",
    "published_date": "2024-03-30 23:22:51 UTC",
    "updated_date": "2024-03-30 23:22:51 UTC"
  },
  {
    "arxiv_id": "2404.00489v2",
    "title": "Prompt-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression",
    "authors": [
      "Muhammad Asif Ali",
      "Zhengping Li",
      "Shu Yang",
      "Keyuan Cheng",
      "Yang Cao",
      "Tianhao Huang",
      "Guimin Hu",
      "Weimin Lyu",
      "Lijie Hu",
      "Lu Yu",
      "Di Wang"
    ],
    "abstract": "Large Language Models (LLMs) have shown exceptional abilities for multiple\ndifferent natural language processing tasks. While prompting is a crucial tool\nfor LLM inference, we observe that there is a significant cost associated with\nexceedingly lengthy prompts. Existing attempts to compress lengthy prompts lead\nto substandard results in terms of readability/interpretability of the\ncompressed prompt, with a detrimental impact on prompt utility. To address\nthis, we propose PromptSAW: Prompt compresSion via Relation AWare graphs, an\neffective strategy for prompt compression over task-agnostic and task-aware\nprompts. Prompt-SAW uses the prompt's textual information to build a graph and\nlater extracts key information elements in the graph to come up with the\ncompressed prompt. We also propose GSM8K-aug, i.e., an extended version of the\nexisting GSM8K benchmark for task-agnostic prompts in order to provide a\ncomprehensive evaluation platform. Experimental evaluation using benchmark\ndatasets shows that prompts compressed by Prompt-SAW are not only better in\nterms of readability, but they also outperform the best-performing baseline\nmodels by up to 10.1 and 77.1, respectively, for task-agnostic and task-aware\nsettings while compressing the original prompt text by 34.9 and 56.7.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "16 pages",
    "pdf_url": "http://arxiv.org/pdf/2404.00489v2",
    "published_date": "2024-03-30 23:07:58 UTC",
    "updated_date": "2024-10-17 15:20:23 UTC"
  },
  {
    "arxiv_id": "2404.00488v1",
    "title": "Noise-Aware Training of Layout-Aware Language Models",
    "authors": [
      "Ritesh Sarkhel",
      "Xiaoqi Ren",
      "Lauro Beltrao Costa",
      "Guolong Su",
      "Vincent Perot",
      "Yanan Xie",
      "Emmanouil Koukoumidis",
      "Arnab Nandi"
    ],
    "abstract": "A visually rich document (VRD) utilizes visual features along with linguistic\ncues to disseminate information. Training a custom extractor that identifies\nnamed entities from a document requires a large number of instances of the\ntarget document type annotated at textual and visual modalities. This is an\nexpensive bottleneck in enterprise scenarios, where we want to train custom\nextractors for thousands of different document types in a scalable way.\nPre-training an extractor model on unlabeled instances of the target document\ntype, followed by a fine-tuning step on human-labeled instances does not work\nin these scenarios, as it surpasses the maximum allowable training time\nallocated for the extractor. We address this scenario by proposing a\nNoise-Aware Training method or NAT in this paper. Instead of acquiring\nexpensive human-labeled documents, NAT utilizes weakly labeled documents to\ntrain an extractor in a scalable way. To avoid degradation in the model's\nquality due to noisy, weakly labeled samples, NAT estimates the confidence of\neach training sample and incorporates it as uncertainty measure during\ntraining. We train multiple state-of-the-art extractor models using NAT.\nExperiments on a number of publicly available and in-house datasets show that\nNAT-trained models are not only robust in performance -- it outperforms a\ntransfer-learning baseline by up to 6% in terms of macro-F1 score, but it is\nalso more label-efficient -- it reduces the amount of human-effort required to\nobtain comparable performance by up to 73%.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00488v1",
    "published_date": "2024-03-30 23:06:34 UTC",
    "updated_date": "2024-03-30 23:06:34 UTC"
  },
  {
    "arxiv_id": "2404.00487v1",
    "title": "Contextual AI Journaling: Integrating LLM and Time Series Behavioral Sensing Technology to Promote Self-Reflection and Well-being using the MindScape App",
    "authors": [
      "Subigya Nepal",
      "Arvind Pillai",
      "William Campbell",
      "Talie Massachi",
      "Eunsol Soul Choi",
      "Orson Xu",
      "Joanna Kuc",
      "Jeremy Huckins",
      "Jason Holden",
      "Colin Depp",
      "Nicholas Jacobson",
      "Mary Czerwinski",
      "Eric Granholm",
      "Andrew T. Campbell"
    ],
    "abstract": "MindScape aims to study the benefits of integrating time series behavioral\npatterns (e.g., conversational engagement, sleep, location) with Large Language\nModels (LLMs) to create a new form of contextual AI journaling, promoting\nself-reflection and well-being. We argue that integrating behavioral sensing in\nLLMs will likely lead to a new frontier in AI. In this Late-Breaking Work\npaper, we discuss the MindScape contextual journal App design that uses LLMs\nand behavioral sensing to generate contextual and personalized journaling\nprompts crafted to encourage self-reflection and emotional development. We also\ndiscuss the MindScape study of college students based on a preliminary user\nstudy and our upcoming study to assess the effectiveness of contextual AI\njournaling in promoting better well-being on college campuses. MindScape\nrepresents a new application class that embeds behavioral intelligence in AI.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "H.5.0; H.5.3; H.5.m; J.0"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00487v1",
    "published_date": "2024-03-30 23:01:34 UTC",
    "updated_date": "2024-03-30 23:01:34 UTC"
  },
  {
    "arxiv_id": "2406.16875v1",
    "title": "Multi-Stage Fusion Architecture for Small-Drone Localization and Identification Using Passive RF and EO Imagery: A Case Study",
    "authors": [
      "Thakshila Wimalajeewa Wewelwala",
      "Thomas W. Tedesso",
      "Tony Davis"
    ],
    "abstract": "Reliable detection, localization and identification of small drones is\nessential to promote safe, secure and privacy-respecting operation of\nUnmanned-Aerial Systems (UAS), or simply, drones. This is an increasingly\nchallenging problem with only single modality sensing, especially, to detect\nand identify small drones. In this work, a multi-stage fusion architecture\nusing passive radio frequency (RF) and electro-optic (EO) imagery data is\ndeveloped to leverage the synergies of the modalities to improve the overall\ntracking and classification capabilities. For detection with EO-imagery,\nsupervised deep learning based techniques as well as unsupervised\nforeground/background separation techniques are explored to cope with\nchallenging environments. Using real collected data for Group 1 and 2 drones,\nthe capability of each algorithm is quantified. In order to compensate for any\nperformance gaps in detection with only EO imagery as well as to provide a\nunique device identifier for the drones, passive RF is integrated with EO\nimagery whenever available. In particular, drone detections in the image plane\nare combined with passive RF location estimates via detection-to-detection\nassociation after 3D to 2D transformation. Final tracking is performed on the\ncomposite detections in the 2D image plane. Each track centroid is given a\nunique identification obtained via RF fingerprinting. The proposed fusion\narchitecture is tested and the tracking and performance is quantified over the\nrange to illustrate the effectiveness of the proposed approaches using\nsimultaneously collected passive RF and EO data at the Air Force Research\nLaboratory (AFRL) through ESCAPE-21 (Experiments, Scenarios, Concept of\nOperations, and Prototype Engineering) data collect",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.16875v1",
    "published_date": "2024-03-30 22:53:28 UTC",
    "updated_date": "2024-03-30 22:53:28 UTC"
  },
  {
    "arxiv_id": "2404.00486v1",
    "title": "Dialectical Alignment: Resolving the Tension of 3H and Security Threats of LLMs",
    "authors": [
      "Shu Yang",
      "Jiayuan Su",
      "Han Jiang",
      "Mengdi Li",
      "Keyuan Cheng",
      "Muhammad Asif Ali",
      "Lijie Hu",
      "Di Wang"
    ],
    "abstract": "With the rise of large language models (LLMs), ensuring they embody the\nprinciples of being helpful, honest, and harmless (3H), known as Human\nAlignment, becomes crucial. While existing alignment methods like RLHF, DPO,\netc., effectively fine-tune LLMs to match preferences in the preference\ndataset, they often lead LLMs to highly receptive human input and external\nevidence, even when this information is poisoned. This leads to a tendency for\nLLMs to be Adaptive Chameleons when external evidence conflicts with their\nparametric memory. This exacerbates the risk of LLM being attacked by external\npoisoned data, which poses a significant security risk to LLM system\napplications such as Retrieval-augmented generation (RAG). To address the\nchallenge, we propose a novel framework: Dialectical Alignment (DA), which (1)\nutilizes AI feedback to identify optimal strategies for LLMs to navigate\ninter-context conflicts and context-memory conflicts with different external\nevidence in context window (i.e., different ratios of poisoned factual\ncontexts); (2) constructs the SFT dataset as well as the preference dataset\nbased on the AI feedback and strategies above; (3) uses the above datasets for\nLLM alignment to defense poisoned context attack while preserving the\neffectiveness of in-context knowledge editing. Our experiments show that the\ndialectical alignment model improves poisoned data attack defense by 20 and\ndoes not require any additional prompt engineering or prior declaration of\n``you may be attacked`` to the LLMs' context window.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00486v1",
    "published_date": "2024-03-30 22:41:05 UTC",
    "updated_date": "2024-03-30 22:41:05 UTC"
  },
  {
    "arxiv_id": "2404.00482v2",
    "title": "Cross-lingual Named Entity Corpus for Slavic Languages",
    "authors": [
      "Jakub Piskorski",
      "Michał Marcińczuk",
      "Roman Yangarber"
    ],
    "abstract": "This paper presents a corpus manually annotated with named entities for six\nSlavic languages - Bulgarian, Czech, Polish, Slovenian, Russian, and Ukrainian.\nThis work is the result of a series of shared tasks, conducted in 2017-2023 as\na part of the Workshops on Slavic Natural Language Processing. The corpus\nconsists of 5 017 documents on seven topics. The documents are annotated with\nfive classes of named entities. Each entity is described by a category, a\nlemma, and a unique cross-lingual identifier. We provide two train-tune dataset\nsplits - single topic out and cross topics. For each split, we set benchmarks\nusing a transformer-based neural network architecture with the pre-trained\nmultilingual models - XLM-RoBERTa-large for named entity mention recognition\nand categorization, and mT5-large for named entity lemmatization and linking.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Published in LREC-COLING 2024 - The 2024 Joint International\n  Conference on Computational Linguistics, Language Resources and Evaluation",
    "pdf_url": "http://arxiv.org/pdf/2404.00482v2",
    "published_date": "2024-03-30 22:20:08 UTC",
    "updated_date": "2024-04-07 16:56:35 UTC"
  },
  {
    "arxiv_id": "2404.00474v2",
    "title": "Linguistic Calibration of Long-Form Generations",
    "authors": [
      "Neil Band",
      "Xuechen Li",
      "Tengyu Ma",
      "Tatsunori Hashimoto"
    ],
    "abstract": "Language models (LMs) may lead their users to make suboptimal downstream\ndecisions when they confidently hallucinate. This issue can be mitigated by\nhaving the LM verbally convey the probability that its claims are correct, but\nexisting models cannot produce long-form text with calibrated confidence\nstatements. Through the lens of decision-making, we define linguistic\ncalibration for long-form generations: an LM is linguistically calibrated if\nits generations enable its users to make calibrated probabilistic predictions.\nThis definition enables a training framework where a supervised finetuning step\nbootstraps an LM to emit long-form generations with confidence statements such\nas \"I estimate a 30% chance of...\" or \"I am certain that...\", followed by a\nreinforcement learning step which rewards generations that enable a user to\nprovide calibrated answers to related questions. We linguistically calibrate\nLlama 2 7B and find in automated and human evaluations of long-form generations\nthat it is significantly more calibrated than strong finetuned factuality\nbaselines with comparable accuracy. These findings generalize under significant\ndomain shifts to scientific and biomedical questions and to an entirely\nheld-out person biography generation task. Our results demonstrate that\nlong-form generations may be calibrated end-to-end by constructing an objective\nin the space of the predictions that users make in downstream decision-making.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2024. Code available at\n  https://github.com/tatsu-lab/linguistic_calibration",
    "pdf_url": "http://arxiv.org/pdf/2404.00474v2",
    "published_date": "2024-03-30 20:47:55 UTC",
    "updated_date": "2024-06-04 22:39:58 UTC"
  },
  {
    "arxiv_id": "2404.00461v1",
    "title": "Shortcuts Arising from Contrast: Effective and Covert Clean-Label Attacks in Prompt-Based Learning",
    "authors": [
      "Xiaopeng Xie",
      "Ming Yan",
      "Xiwen Zhou",
      "Chenlong Zhao",
      "Suli Wang",
      "Yong Zhang",
      "Joey Tianyi Zhou"
    ],
    "abstract": "Prompt-based learning paradigm has demonstrated remarkable efficacy in\nenhancing the adaptability of pretrained language models (PLMs), particularly\nin few-shot scenarios. However, this learning paradigm has been shown to be\nvulnerable to backdoor attacks. The current clean-label attack, employing a\nspecific prompt as a trigger, can achieve success without the need for external\ntriggers and ensure correct labeling of poisoned samples, which is more\nstealthy compared to the poisoned-label attack, but on the other hand, it faces\nsignificant issues with false activations and poses greater challenges,\nnecessitating a higher rate of poisoning. Using conventional negative data\naugmentation methods, we discovered that it is challenging to trade off between\neffectiveness and stealthiness in a clean-label setting. In addressing this\nissue, we are inspired by the notion that a backdoor acts as a shortcut and\nposit that this shortcut stems from the contrast between the trigger and the\ndata utilized for poisoning. In this study, we propose a method named\nContrastive Shortcut Injection (CSI), by leveraging activation values,\nintegrates trigger design and data selection strategies to craft stronger\nshortcut features. With extensive experiments on full-shot and few-shot text\nclassification tasks, we empirically validate CSI's high effectiveness and high\nstealthiness at low poisoning rates. Notably, we found that the two approaches\nplay leading roles in full-shot and few-shot settings, respectively.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "68T50",
      "I.2.7"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 6 figures, conference",
    "pdf_url": "http://arxiv.org/pdf/2404.00461v1",
    "published_date": "2024-03-30 20:02:36 UTC",
    "updated_date": "2024-03-30 20:02:36 UTC"
  },
  {
    "arxiv_id": "2404.00450v2",
    "title": "Planning and Editing What You Retrieve for Enhanced Tool Learning",
    "authors": [
      "Tenghao Huang",
      "Dongwon Jung",
      "Muhao Chen"
    ],
    "abstract": "Recent advancements in integrating external tools with Large Language Models\n(LLMs) have opened new frontiers, with applications in mathematical reasoning,\ncode generators, and smart assistants. However, existing methods, relying on\nsimple one-time retrieval strategies, fall short on effectively and accurately\nshortlisting relevant tools. This paper introduces a novel PLUTO (Planning,\nLearning, and Understanding for TOols) approach, encompassing\n`Plan-and-Retrieve (P&R)` and `Edit-and-Ground (E&G)` paradigms. The P&R\nparadigm consists of a neural retrieval module for shortlisting relevant tools\nand an LLM-based query planner that decomposes complex queries into actionable\ntasks, enhancing the effectiveness of tool utilization. The E&G paradigm\nutilizes LLMs to enrich tool descriptions based on user scenarios, bridging the\ngap between user queries and tool functionalities. Experiment results\ndemonstrate that these paradigms significantly improve the recall and NDCG in\ntool retrieval tasks, significantly surpassing current state-of-the-art models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "This paper is accepted at NAACL-Findings 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.00450v2",
    "published_date": "2024-03-30 18:41:51 UTC",
    "updated_date": "2024-04-04 05:33:07 UTC"
  },
  {
    "arxiv_id": "2404.00442v1",
    "title": "Interactive Multi-Robot Flocking with Gesture Responsiveness and Musical Accompaniment",
    "authors": [
      "Catie Cuan",
      "Kyle Jeffrey",
      "Kim Kleiven",
      "Adrian Li",
      "Emre Fisher",
      "Matt Harrison",
      "Benjie Holson",
      "Allison Okamura",
      "Matt Bennice"
    ],
    "abstract": "For decades, robotics researchers have pursued various tasks for multi-robot\nsystems, from cooperative manipulation to search and rescue. These tasks are\nmulti-robot extensions of classical robotic tasks and often optimized on\ndimensions such as speed or efficiency. As robots transition from commercial\nand research settings into everyday environments, social task aims such as\nengagement or entertainment become increasingly relevant. This work presents a\ncompelling multi-robot task, in which the main aim is to enthrall and interest.\nIn this task, the goal is for a human to be drawn to move alongside and\nparticipate in a dynamic, expressive robot flock. Towards this aim, the\nresearch team created algorithms for robot movements and engaging interaction\nmodes such as gestures and sound. The contributions are as follows: (1) a novel\ngroup navigation algorithm involving human and robot agents, (2) a gesture\nresponsive algorithm for real-time, human-robot flocking interaction, (3) a\nweight mode characterization system for modifying flocking behavior, and (4) a\nmethod of encoding a choreographer's preferences inside a dynamic, adaptive,\nlearned system. An experiment was performed to understand individual human\nbehavior while interacting with the flock under three conditions: weight modes\nselected by a human choreographer, a learned model, or subset list. Results\nfrom the experiment showed that the perception of the experience was not\ninfluenced by the weight mode selection. This work elucidates how differing\ntask aims such as engagement manifest in multi-robot system design and\nexecution, and broadens the domain of multi-robot tasks.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00442v1",
    "published_date": "2024-03-30 18:16:28 UTC",
    "updated_date": "2024-03-30 18:16:28 UTC"
  },
  {
    "arxiv_id": "2404.00438v1",
    "title": "Communication Efficient Distributed Training with Distributed Lion",
    "authors": [
      "Bo Liu",
      "Lemeng Wu",
      "Lizhang Chen",
      "Kaizhao Liang",
      "Jiaxu Zhu",
      "Chen Liang",
      "Raghuraman Krishnamoorthi",
      "Qiang Liu"
    ],
    "abstract": "The Lion optimizer has been a promising competitor with the AdamW for\ntraining large AI models, with advantages on memory, computation, and sample\nefficiency. In this paper, we introduce Distributed Lion, an innovative\nadaptation of Lion for distributed training environments. Leveraging the sign\noperator in Lion, our Distributed Lion only requires communicating binary or\nlower-precision vectors between workers to the center server, significantly\nreducing the communication cost. Our theoretical analysis confirms Distributed\nLion's convergence properties. Empirical results demonstrate its robustness\nacross a range of tasks, worker counts, and batch sizes, on both vision and\nlanguage problems. Notably, Distributed Lion attains comparable performance to\nstandard Lion or AdamW optimizers applied on aggregated gradients, but with\nsignificantly reduced communication bandwidth. This feature is particularly\nadvantageous for training large models. In addition, we also demonstrate that\nDistributed Lion presents a more favorable performance-bandwidth balance\ncompared to existing efficient distributed methods such as deep gradient\ncompression and ternary gradients.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.DC",
    "comment": "22 pages",
    "pdf_url": "http://arxiv.org/pdf/2404.00438v1",
    "published_date": "2024-03-30 18:07:29 UTC",
    "updated_date": "2024-03-30 18:07:29 UTC"
  },
  {
    "arxiv_id": "2404.00437v1",
    "title": "Automatic explanation of the classification of Spanish legal judgments in jurisdiction-dependent law categories with tree estimators",
    "authors": [
      "Jaime González-González",
      "Francisco de Arriba-Pérez",
      "Silvia García-Méndez",
      "Andrea Busto-Castiñeira",
      "Francisco J. González-Castaño"
    ],
    "abstract": "Automatic legal text classification systems have been proposed in the\nliterature to address knowledge extraction from judgments and detect their\naspects. However, most of these systems are black boxes even when their models\nare interpretable. This may raise concerns about their trustworthiness.\nAccordingly, this work contributes with a system combining Natural Language\nProcessing (NLP) with Machine Learning (ML) to classify legal texts in an\nexplainable manner. We analyze the features involved in the decision and the\nthreshold bifurcation values of the decision paths of tree structures and\npresent this information to the users in natural language. This is the first\nwork on automatic analysis of legal texts combining NLP and ML along with\nExplainable Artificial Intelligence techniques to automatically make the\nmodels' decisions understandable to end users. Furthermore, legal experts have\nvalidated our solution, and this knowledge has also been incorporated into the\nexplanation process as \"expert-in-the-loop\" dictionaries. Experimental results\non an annotated data set in law categories by jurisdiction demonstrate that our\nsystem yields competitive classification performance, with accuracy values well\nabove 90%, and that its automatic explanations are easily understandable even\nto non-expert users.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00437v1",
    "published_date": "2024-03-30 17:59:43 UTC",
    "updated_date": "2024-03-30 17:59:43 UTC"
  },
  {
    "arxiv_id": "2404.00424v2",
    "title": "Quantformer: from attention to profit with a quantitative transformer trading strategy",
    "authors": [
      "Zhaofeng Zhang",
      "Banghao Chen",
      "Shengxin Zhu",
      "Nicolas Langrené"
    ],
    "abstract": "In traditional quantitative trading practice, navigating the complicated and\ndynamic financial market presents a persistent challenge. Fully capturing\nvarious market variables, including long-term information, as well as essential\nsignals that may lead to profit remains a difficult task for learning\nalgorithms. In order to tackle this challenge, this paper introduces\nquantformer, an enhanced neural network architecture based on transformers, to\nbuild investment factors. By transfer learning from sentiment analysis,\nquantformer not only exploits its original inherent advantages in capturing\nlong-range dependencies and modeling complex data relationships, but is also\nable to solve tasks with numerical inputs and accurately forecast future\nreturns over a given period. This work collects more than 5,000,000 rolling\ndata of 4,601 stocks in the Chinese capital market from 2010 to 2019. The\nresults of this study demonstrated the model's superior performance in\npredicting stock trends compared with other 100 factor-based quantitative\nstrategies. Notably, the model's innovative use of transformer-liked model to\nestablish factors, in conjunction with market sentiment information, has been\nshown to enhance the accuracy of trading signals significantly, thereby\noffering promising implications for the future of quantitative trading\nstrategies.",
    "categories": [
      "q-fin.MF",
      "cs.AI",
      "cs.CE",
      "G.3; J.2"
    ],
    "primary_category": "q-fin.MF",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00424v2",
    "published_date": "2024-03-30 17:18:00 UTC",
    "updated_date": "2024-10-23 04:27:26 UTC"
  },
  {
    "arxiv_id": "2404.00417v1",
    "title": "Orchestrate Latent Expertise: Advancing Online Continual Learning with Multi-Level Supervision and Reverse Self-Distillation",
    "authors": [
      "HongWei Yan",
      "Liyuan Wang",
      "Kaisheng Ma",
      "Yi Zhong"
    ],
    "abstract": "To accommodate real-world dynamics, artificial intelligence systems need to\ncope with sequentially arriving content in an online manner. Beyond regular\nContinual Learning (CL) attempting to address catastrophic forgetting with\noffline training of each task, Online Continual Learning (OCL) is a more\nchallenging yet realistic setting that performs CL in a one-pass data stream.\nCurrent OCL methods primarily rely on memory replay of old training samples.\nHowever, a notable gap from CL to OCL stems from the additional\noverfitting-underfitting dilemma associated with the use of rehearsal buffers:\nthe inadequate learning of new training samples (underfitting) and the repeated\nlearning of a few old training samples (overfitting). To this end, we introduce\na novel approach, Multi-level Online Sequential Experts (MOSE), which\ncultivates the model as stacked sub-experts, integrating multi-level\nsupervision and reverse self-distillation. Supervision signals across multiple\nstages facilitate appropriate convergence of the new task while gathering\nvarious strengths from experts by knowledge distillation mitigates the\nperformance decline of old tasks. MOSE demonstrates remarkable efficacy in\nlearning new samples and preserving past knowledge through multi-level experts,\nthereby significantly advancing OCL performance over state-of-the-art baselines\n(e.g., up to 7.3% on Split CIFAR-100 and 6.1% on Split Tiny-ImageNet).",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "CVPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.00417v1",
    "published_date": "2024-03-30 16:53:10 UTC",
    "updated_date": "2024-03-30 16:53:10 UTC"
  },
  {
    "arxiv_id": "2404.00413v1",
    "title": "Language Models are Spacecraft Operators",
    "authors": [
      "Victor Rodriguez-Fernandez",
      "Alejandro Carrasco",
      "Jason Cheng",
      "Eli Scharf",
      "Peng Mun Siew",
      "Richard Linares"
    ],
    "abstract": "Recent trends are emerging in the use of Large Language Models (LLMs) as\nautonomous agents that take actions based on the content of the user text\nprompts. We intend to apply these concepts to the field of Guidance,\nNavigation, and Control in space, enabling LLMs to have a significant role in\nthe decision-making process for autonomous satellite operations. As a first\nstep towards this goal, we have developed a pure LLM-based solution for the\nKerbal Space Program Differential Games (KSPDG) challenge, a public software\ndesign competition where participants create autonomous agents for maneuvering\nsatellites involved in non-cooperative space operations, running on the KSP\ngame engine. Our approach leverages prompt engineering, few-shot prompting, and\nfine-tuning techniques to create an effective LLM-based agent that ranked 2nd\nin the competition. To the best of our knowledge, this work pioneers the\nintegration of LLM agents into space research. Code is available at\nhttps://github.com/ARCLab-MIT/kspdg.",
    "categories": [
      "physics.space-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "physics.space-ph",
    "comment": "Source code available on Github at:\n  https://github.com/ARCLab-MIT/kspdg",
    "pdf_url": "http://arxiv.org/pdf/2404.00413v1",
    "published_date": "2024-03-30 16:43:59 UTC",
    "updated_date": "2024-03-30 16:43:59 UTC"
  },
  {
    "arxiv_id": "2404.00406v1",
    "title": "TACO -- Twitter Arguments from COnversations",
    "authors": [
      "Marc Feger",
      "Stefan Dietze"
    ],
    "abstract": "Twitter has emerged as a global hub for engaging in online conversations and\nas a research corpus for various disciplines that have recognized the\nsignificance of its user-generated content. Argument mining is an important\nanalytical task for processing and understanding online discourse.\nSpecifically, it aims to identify the structural elements of arguments, denoted\nas information and inference. These elements, however, are not static and may\nrequire context within the conversation they are in, yet there is a lack of\ndata and annotation frameworks addressing this dynamic aspect on Twitter. We\ncontribute TACO, the first dataset of Twitter Arguments utilizing 1,814 tweets\ncovering 200 entire conversations spanning six heterogeneous topics annotated\nwith an agreement of 0.718 Krippendorff's alpha among six experts. Second, we\nprovide our annotation framework, incorporating definitions from the Cambridge\nDictionary, to define and identify argument components on Twitter. Our\ntransformer-based classifier achieves an 85.06\\% macro F1 baseline score in\ndetecting arguments. Moreover, our data reveals that Twitter users tend to\nengage in discussions involving informed inferences and information. TACO\nserves multiple purposes, such as training tweet classifiers to manage tweets\nbased on inference and information elements, while also providing valuable\ninsights into the conversational reply patterns of tweets.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00406v1",
    "published_date": "2024-03-30 16:14:46 UTC",
    "updated_date": "2024-03-30 16:14:46 UTC"
  },
  {
    "arxiv_id": "2404.00399v3",
    "title": "Aurora-M: Open Source Continual Pre-training for Multilingual Language and Code",
    "authors": [
      "Taishi Nakamura",
      "Mayank Mishra",
      "Simone Tedeschi",
      "Yekun Chai",
      "Jason T Stillerman",
      "Felix Friedrich",
      "Prateek Yadav",
      "Tanmay Laud",
      "Vu Minh Chien",
      "Terry Yue Zhuo",
      "Diganta Misra",
      "Ben Bogin",
      "Xuan-Son Vu",
      "Marzena Karpinska",
      "Arnav Varma Dantuluri",
      "Wojciech Kusa",
      "Tommaso Furlanello",
      "Rio Yokota",
      "Niklas Muennighoff",
      "Suhas Pai",
      "Tosin Adewumi",
      "Veronika Laippala",
      "Xiaozhe Yao",
      "Adalberto Junior",
      "Alpay Ariyak",
      "Aleksandr Drozd",
      "Jordan Clive",
      "Kshitij Gupta",
      "Liangyu Chen",
      "Qi Sun",
      "Ken Tsui",
      "Noah Persaud",
      "Nour Fahmy",
      "Tianlong Chen",
      "Mohit Bansal",
      "Nicolo Monti",
      "Tai Dang",
      "Ziyang Luo",
      "Tien-Tung Bui",
      "Roberto Navigli",
      "Virendra Mehta",
      "Matthew Blumberg",
      "Victor May",
      "Huu Nguyen",
      "Sampo Pyysalo"
    ],
    "abstract": "Pretrained language models are an integral part of AI applications, but their\nhigh computational cost for training limits accessibility. Initiatives such as\nBloom and StarCoder aim to democratize access to pretrained models for\ncollaborative community development. Despite these efforts, such models\nencounter challenges such as limited multilingual capabilities, risks of\ncatastrophic forgetting during continual pretraining, and the high costs of\ntraining models from scratch, alongside the need to align with AI safety\nstandards and regulatory frameworks.\n  This paper presents Aurora-M, a 15B parameter multilingual open-source model\ntrained on English, Finnish, Hindi, Japanese, Vietnamese, and code. Continually\npretrained from StarCoderPlus on 435B additional tokens, Aurora-M surpasses 2T\ntokens in total training token count. It is the first open-source multilingual\nmodel fine-tuned on human-reviewed safety instructions, thus aligning its\ndevelopment not only with conventional red-teaming considerations, but also\nwith the specific concerns articulated in the Biden-Harris Executive Order on\nthe Safe, Secure, and Trustworthy Development and Use of Artificial\nIntelligence.\n  We evaluate Aurora-M across a wide range of tasks and languages, showcasing\nits robustness against catastrophic forgetting and its superior performance in\nmultilingual settings, particularly in safety evaluations. We open-source\nAurora-M and its variants to encourage responsible open-source development of\nlarge language models at https://huggingface.co/aurora-m.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2404.00399v3",
    "published_date": "2024-03-30 15:38:54 UTC",
    "updated_date": "2024-12-27 03:53:21 UTC"
  },
  {
    "arxiv_id": "2404.00385v1",
    "title": "Constrained Layout Generation with Factor Graphs",
    "authors": [
      "Mohammed Haroon Dupty",
      "Yanfei Dong",
      "Sicong Leng",
      "Guoji Fu",
      "Yong Liang Goh",
      "Wei Lu",
      "Wee Sun Lee"
    ],
    "abstract": "This paper addresses the challenge of object-centric layout generation under\nspatial constraints, seen in multiple domains including floorplan design\nprocess. The design process typically involves specifying a set of spatial\nconstraints that include object attributes like size and inter-object relations\nsuch as relative positioning. Existing works, which typically represent objects\nas single nodes, lack the granularity to accurately model complex interactions\nbetween objects. For instance, often only certain parts of an object, like a\nroom's right wall, interact with adjacent objects. To address this gap, we\nintroduce a factor graph based approach with four latent variable nodes for\neach room, and a factor node for each constraint. The factor nodes represent\ndependencies among the variables to which they are connected, effectively\ncapturing constraints that are potentially of a higher order. We then develop\nmessage-passing on the bipartite graph, forming a factor graph neural network\nthat is trained to produce a floorplan that aligns with the desired\nrequirements. Our approach is simple and generates layouts faithful to the user\nrequirements, demonstrated by a large improvement in IOU scores over existing\nmethods. Additionally, our approach, being inferential and accurate, is\nwell-suited to the practical human-in-the-loop design process where\nspecifications evolve iteratively, offering a practical and powerful tool for\nAI-guided design.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "To be published at IEEE/CVF CVPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.00385v1",
    "published_date": "2024-03-30 14:58:40 UTC",
    "updated_date": "2024-03-30 14:58:40 UTC"
  },
  {
    "arxiv_id": "2404.00383v1",
    "title": "SpikingJET: Enhancing Fault Injection for Fully and Convolutional Spiking Neural Networks",
    "authors": [
      "Anil Bayram Gogebakan",
      "Enrico Magliano",
      "Alessio Carpegna",
      "Annachiara Ruospo",
      "Alessandro Savino",
      "Stefano Di Carlo"
    ],
    "abstract": "As artificial neural networks become increasingly integrated into\nsafety-critical systems such as autonomous vehicles, devices for medical\ndiagnosis, and industrial automation, ensuring their reliability in the face of\nrandom hardware faults becomes paramount. This paper introduces SpikingJET, a\nnovel fault injector designed specifically for fully connected and\nconvolutional Spiking Neural Networks (SNNs). Our work underscores the critical\nneed to evaluate the resilience of SNNs to hardware faults, considering their\ngrowing prominence in real-world applications. SpikingJET provides a\ncomprehensive platform for assessing the resilience of SNNs by inducing errors\nand injecting faults into critical components such as synaptic weights, neuron\nmodel parameters, internal states, and activation functions. This paper\ndemonstrates the effectiveness of Spiking-JET through extensive software-level\nexperiments on various SNN architectures, revealing insights into their\nvulnerability and resilience to hardware faults. Moreover, highlighting the\nimportance of fault resilience in SNNs contributes to the ongoing effort to\nenhance the reliability and safety of Neural Network (NN)-powered systems in\ndiverse domains.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "I.2"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00383v1",
    "published_date": "2024-03-30 14:51:01 UTC",
    "updated_date": "2024-03-30 14:51:01 UTC"
  },
  {
    "arxiv_id": "2404.01336v3",
    "title": "FineFake: A Knowledge-Enriched Dataset for Fine-Grained Multi-Domain Fake News Detection",
    "authors": [
      "Ziyi Zhou",
      "Xiaoming Zhang",
      "Litian Zhang",
      "Jiacheng Liu",
      "Senzhang Wang",
      "Zheng Liu",
      "Xi Zhang",
      "Chaozhuo Li",
      "Philip S. Yu"
    ],
    "abstract": "Existing benchmarks for fake news detection have significantly contributed to\nthe advancement of models in assessing the authenticity of news content.\nHowever, these benchmarks typically focus solely on news pertaining to a single\nsemantic topic or originating from a single platform, thereby failing to\ncapture the diversity of multi-domain news in real scenarios. In order to\nunderstand fake news across various domains, the external knowledge and\nfine-grained annotations are indispensable to provide precise evidence and\nuncover the diverse underlying strategies for fabrication, which are also\nignored by existing benchmarks. To address this gap, we introduce a novel\nmulti-domain knowledge-enhanced benchmark with fine-grained annotations, named\n\\textbf{FineFake}. FineFake encompasses 16,909 data samples spanning six\nsemantic topics and eight platforms. Each news item is enriched with\nmulti-modal content, potential social context, semi-manually verified common\nknowledge, and fine-grained annotations that surpass conventional binary\nlabels. Furthermore, we formulate three challenging tasks based on FineFake and\npropose a knowledge-enhanced domain adaptation network. Extensive experiments\nare conducted on FineFake under various scenarios, providing accurate and\nreliable benchmarks for future endeavors. The entire FineFake project is\npublicly accessible as an open-source repository at\n\\url{https://github.com/Accuser907/FineFake}.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.01336v3",
    "published_date": "2024-03-30 14:39:09 UTC",
    "updated_date": "2024-10-15 12:40:39 UTC"
  },
  {
    "arxiv_id": "2404.00369v1",
    "title": "Worker Robot Cooperation and Integration into the Manufacturing Workcell via the Holonic Control Architecture",
    "authors": [
      "Ahmed R. Sadik",
      "Bodo Urban",
      "Omar Adel"
    ],
    "abstract": "Worker-Robot Cooperation is a new industrial trend, which aims to sum the\nadvantages of both the human and the industrial robot to afford a new\nintelligent manufacturing techniques. The cooperative manufacturing between the\nworker and the robot contains other elements such as the product parts and the\nmanufacturing tools. All these production elements must cooperate in one\nmanufacturing workcell to fulfill the production requirements. The\nmanufacturing control system is the mean to connect all these cooperative\nelements together in one body. This manufacturing control system is distributed\nand autonomous due to the nature of the cooperative workcell. Accordingly, this\narticle proposes the holonic control architecture as the manufacturing concept\nof the cooperative workcell. Furthermore, the article focuses on the\nfeasibility of this manufacturing concept, by applying it over a case study\nthat involves the cooperation between a dual-arm robot and a worker. During\nthis case study, the worker uses a variety of hand gestures to cooperate with\nthe robot to achieve the highest production flexibility",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.MA",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00369v1",
    "published_date": "2024-03-30 13:44:17 UTC",
    "updated_date": "2024-03-30 13:44:17 UTC"
  },
  {
    "arxiv_id": "2404.00364v1",
    "title": "Accurate Cutting-point Estimation for Robotic Lychee Harvesting through Geometry-aware Learning",
    "authors": [
      "Gengming Zhang",
      "Hao Cao",
      "Kewei Hu",
      "Yaoqiang Pan",
      "Yuqin Deng",
      "Hongjun Wang",
      "Hanwen Kang"
    ],
    "abstract": "Accurately identifying lychee-picking points in unstructured orchard\nenvironments and obtaining their coordinate locations is critical to the\nsuccess of lychee-picking robots. However, traditional two-dimensional (2D)\nimage-based object detection methods often struggle due to the complex\ngeometric structures of branches, leaves and fruits, leading to incorrect\ndetermination of lychee picking points. In this study, we propose a\nFcaf3d-lychee network model specifically designed for the accurate localisation\nof lychee picking points. Point cloud data of lychee picking points in natural\nenvironments are acquired using Microsoft's Azure Kinect DK time-of-flight\n(TOF) camera through multi-view stitching. We augment the Fully Convolutional\nAnchor-Free 3D Object Detection (Fcaf3d) model with a\nsqueeze-and-excitation(SE) module, which exploits human visual attention\nmechanisms for improved feature extraction of lychee picking points. The\ntrained network model is evaluated on a test set of lychee-picking locations\nand achieves an impressive F1 score of 88.57%, significantly outperforming\nexisting models. Subsequent three-dimensional (3D) position detection of\npicking points in real lychee orchard environments yields high accuracy, even\nunder varying degrees of occlusion. Localisation errors of lychee picking\npoints are within 1.5 cm in all directions, demonstrating the robustness and\ngenerality of the model.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00364v1",
    "published_date": "2024-03-30 13:34:54 UTC",
    "updated_date": "2024-03-30 13:34:54 UTC"
  },
  {
    "arxiv_id": "2404.01335v2",
    "title": "Generative AI Models for Different Steps in Architectural Design: A Literature Review",
    "authors": [
      "Chengyuan Li",
      "Tianyu Zhang",
      "Xusheng Du",
      "Ye Zhang",
      "Haoran Xie"
    ],
    "abstract": "Recent advances in generative artificial intelligence (AI) technologies have\nbeen significantly driven by models such as generative adversarial networks\n(GANs), variational autoencoders (VAEs), and denoising diffusion probabilistic\nmodels (DDPMs). Although architects recognize the potential of generative AI in\ndesign, personal barriers often restrict their access to the latest\ntechnological developments, thereby causing the application of generative AI in\narchitectural design to lag behind. Therefore, it is essential to comprehend\nthe principles and advancements of generative AI models and analyze their\nrelevance in architecture applications. This paper first provides an overview\nof generative AI technologies, with a focus on probabilistic diffusion models\n(DDPMs), 3D generative models, and foundation models, highlighting their recent\ndevelopments and main application scenarios. Then, the paper explains how the\nabovementioned models could be utilized in architecture. We subdivide the\narchitectural design process into six steps and review related research\nprojects in each step from 2020 to the present. Lastly, this paper discusses\npotential future directions for applying generative AI in the architectural\ndesign steps. This research can help architects quickly understand the\ndevelopment and latest progress of generative AI and contribute to the further\ndevelopment of intelligent architecture.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "34 pages, 14 figures, accepted by Frontiers of Architectural Research",
    "pdf_url": "http://arxiv.org/pdf/2404.01335v2",
    "published_date": "2024-03-30 13:25:11 UTC",
    "updated_date": "2024-10-23 12:38:40 UTC"
  },
  {
    "arxiv_id": "2404.00344v1",
    "title": "Can LLMs Master Math? Investigating Large Language Models on Math Stack Exchange",
    "authors": [
      "Ankit Satpute",
      "Noah Giessing",
      "Andre Greiner-Petter",
      "Moritz Schubotz",
      "Olaf Teschke",
      "Akiko Aizawa",
      "Bela Gipp"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated exceptional capabilities in\nvarious natural language tasks, often achieving performances that surpass those\nof humans. Despite these advancements, the domain of mathematics presents a\ndistinctive challenge, primarily due to its specialized structure and the\nprecision it demands. In this study, we adopted a two-step approach for\ninvestigating the proficiency of LLMs in answering mathematical questions.\nFirst, we employ the most effective LLMs, as identified by their performance on\nmath question-answer benchmarks, to generate answers to 78 questions from the\nMath Stack Exchange (MSE). Second, a case analysis is conducted on the LLM that\nshowed the highest performance, focusing on the quality and accuracy of its\nanswers through manual evaluation. We found that GPT-4 performs best (nDCG of\n0.48 and P@10 of 0.37) amongst existing LLMs fine-tuned for answering\nmathematics questions and outperforms the current best approach on ArqMATH3\nTask1, considering P@10. Our Case analysis indicates that while the GPT-4 can\ngenerate relevant responses in certain instances, it does not consistently\nanswer all questions accurately. This paper explores the current limitations of\nLLMs in navigating complex mathematical problem-solving. Through case analysis,\nwe shed light on the gaps in LLM capabilities within mathematics, thereby\nsetting the stage for future research and advancements in AI-driven\nmathematical reasoning. We make our code and findings publicly available for\nresearch: \\url{https://github.com/gipplab/LLM-Investig-MathStackExchange}",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted for publication at the 47th International ACM SIGIR\n  Conference on Research and Development in Information Retrieval (SIGIR) July\n  14--18, 2024, Washington D.C.,USA",
    "pdf_url": "http://arxiv.org/pdf/2404.00344v1",
    "published_date": "2024-03-30 12:48:31 UTC",
    "updated_date": "2024-03-30 12:48:31 UTC"
  },
  {
    "arxiv_id": "2404.00341v1",
    "title": "Ontology in Holonic Cooperative Manufacturing: A Solution to Share and Exchange the Knowledge",
    "authors": [
      "Ahmed R. Sadik",
      "Bodo Urban"
    ],
    "abstract": "Cooperative manufacturing is a new trend in industry, which depends on the\nexistence of a collaborative robot. A collaborative robot is usually a\nlight-weight robot which is capable of operating safely with a human co-worker\nin a shared work environment. During this cooperation, a vast amount of\ninformation is exchanged between the collaborative robot and the worker. This\ninformation constructs the cooperative manufacturing knowledge, which describes\nthe production components and environment. In this research, we propose a\nholonic control solution, which uses the ontology concept to represent the\ncooperative manufacturing knowledge. The holonic control solution is\nimplemented as an autonomous multi-agent system that exchanges the\nmanufacturing knowledge based on an ontology model. Ultimately, the research\nillustrates and implements the proposed solution over a cooperative assembly\nscenario, which involves two workers and one collaborative robot, whom\ncooperate together to assemble a customized product.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00341v1",
    "published_date": "2024-03-30 12:38:47 UTC",
    "updated_date": "2024-03-30 12:38:47 UTC"
  },
  {
    "arxiv_id": "2404.00330v1",
    "title": "Memory-Scalable and Simplified Functional Map Learning",
    "authors": [
      "Robin Magnet",
      "Maks Ovsjanikov"
    ],
    "abstract": "Deep functional maps have emerged in recent years as a prominent\nlearning-based framework for non-rigid shape matching problems. While early\nmethods in this domain only focused on learning in the functional domain, the\nlatest techniques have demonstrated that by promoting consistency between\nfunctional and pointwise maps leads to significant improvements in accuracy.\nUnfortunately, existing approaches rely heavily on the computation of large\ndense matrices arising from soft pointwise maps, which compromises their\nefficiency and scalability. To address this limitation, we introduce a novel\nmemory-scalable and efficient functional map learning pipeline. By leveraging\nthe specific structure of functional maps, we offer the possibility to achieve\nidentical results without ever storing the pointwise map in memory.\nFurthermore, based on the same approach, we present a differentiable map\nrefinement layer adapted from an existing axiomatic refinement algorithm.\nUnlike many functional map learning methods, which use this algorithm at a\npost-processing step, ours can be easily used at train time, enabling to\nenforce consistency between the refined and initial versions of the map. Our\nresulting approach is both simpler, more efficient and more numerically stable,\nby avoiding differentiation through a linear system, while achieving close to\nstate-of-the-art results in challenging scenarios.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00330v1",
    "published_date": "2024-03-30 12:01:04 UTC",
    "updated_date": "2024-03-30 12:01:04 UTC"
  },
  {
    "arxiv_id": "2404.00320v2",
    "title": "Advancing Multimodal Data Fusion in Pain Recognition: A Strategy Leveraging Statistical Correlation and Human-Centered Perspectives",
    "authors": [
      "Xingrui Gu",
      "Zhixuan Wang",
      "Irisa Jin",
      "Zekun Wu"
    ],
    "abstract": "This research presents a novel multimodal data fusion methodology for pain\nbehavior recognition, integrating statistical correlation analysis with\nhuman-centered insights. Our approach introduces two key innovations: 1)\nintegrating data-driven statistical relevance weights into the fusion strategy\nto effectively utilize complementary information from heterogeneous modalities,\nand 2) incorporating human-centric movement characteristics into multimodal\nrepresentation learning for detailed modeling of pain behaviors. Validated\nacross various deep learning architectures, our method demonstrates superior\nperformance and broad applicability. We propose a customizable framework that\naligns each modality with a suitable classifier based on statistical\nsignificance, advancing personalized and effective multimodal fusion.\nFurthermore, our methodology provides explainable analysis of multimodal data,\ncontributing to interpretable and explainable AI in healthcare. By highlighting\nthe importance of data diversity and modality-specific representations, we\nenhance traditional fusion techniques and set new standards for recognizing\ncomplex pain behaviors. Our findings have significant implications for\npromoting patient-centered healthcare interventions and supporting explainable\nclinical decision-making.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by AHRI 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.00320v2",
    "published_date": "2024-03-30 11:13:18 UTC",
    "updated_date": "2024-08-01 09:07:45 UTC"
  },
  {
    "arxiv_id": "2404.00312v1",
    "title": "Bayesian Exploration of Pre-trained Models for Low-shot Image Classification",
    "authors": [
      "Yibo Miao",
      "Yu Lei",
      "Feng Zhou",
      "Zhijie Deng"
    ],
    "abstract": "Low-shot image classification is a fundamental task in computer vision, and\nthe emergence of large-scale vision-language models such as CLIP has greatly\nadvanced the forefront of research in this field. However, most existing\nCLIP-based methods lack the flexibility to effectively incorporate other\npre-trained models that encompass knowledge distinct from CLIP. To bridge the\ngap, this work proposes a simple and effective probabilistic model ensemble\nframework based on Gaussian processes, which have previously demonstrated\nremarkable efficacy in processing small data. We achieve the integration of\nprior knowledge by specifying the mean function with CLIP and the kernel\nfunction with an ensemble of deep kernels built upon various pre-trained\nmodels. By regressing the classification label directly, our framework enables\nanalytical inference, straightforward uncertainty quantification, and\nprincipled hyper-parameter tuning. Through extensive experiments on standard\nbenchmarks, we demonstrate that our method consistently outperforms competitive\nensemble baselines regarding predictive performance. Additionally, we assess\nthe robustness of our method and the quality of the yielded uncertainty\nestimates on out-of-distribution datasets. We also illustrate that our method,\ndespite relying on label regression, still enjoys superior model calibration\ncompared to most deterministic baselines.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00312v1",
    "published_date": "2024-03-30 10:25:28 UTC",
    "updated_date": "2024-03-30 10:25:28 UTC"
  },
  {
    "arxiv_id": "2405.12986v2",
    "title": "A Novel Feature Map Enhancement Technique Integrating Residual CNN and Transformer for Alzheimer Diseases Diagnosis",
    "authors": [
      "Saddam Hussain Khan"
    ],
    "abstract": "Alzheimer diseases (ADs) involves cognitive decline and abnormal brain\nprotein accumulation, necessitating timely diagnosis for effective treatment.\nTherefore, CAD systems leveraging deep learning advancements have demonstrated\nsuccess in AD detection but pose computational intricacies and the dataset\nminor contrast, structural, and texture variations. In this regard, a novel\nhybrid FME-Residual-HSCMT technique is introduced, comprised of residual CNN\nand Transformer concepts to capture global and local fine-grained AD analysis\nin MRI. This approach integrates three distinct elements: a novel CNN Meet\nTransformer (HSCMT), customized residual learning CNN, and a new Feature Map\nEnhancement (FME) strategy to learn diverse morphological, contrast, and\ntexture variations of ADs. The proposed HSCMT at the initial stage utilizes\nstem convolution blocks that are integrated with CMT blocks followed by\nsystematic homogenous and structural (HS) operations. The customized CMT block\nencapsulates each element with global contextual interactions through\nmulti-head attention and facilitates computational efficiency through\nlightweight. Moreover, inverse residual and stem CNN in customized CMT enables\neffective extraction of local texture information and handling vanishing\ngradients. Furthermore, in the FME strategy, residual CNN blocks utilize\nTL-based generated auxiliary and are combined with the proposed HSCMT channels\nat the target level to achieve diverse enriched feature space. Finally, diverse\nenhanced channels are fed into a novel spatial attention mechanism for optimal\npixel selection to reduce redundancy and discriminate minor contrast and\ntexture inter-class variation. The proposed achieves an F1-score (98.55%), an\naccuracy of 98.42% and a sensitivity of 98.50%, a precision of 98.60% on the\nstandard Kaggle dataset, and demonstrates outperformance existing ViTs and CNNs\nmethods.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "28 Pages, 11 Figures, 3 Tables",
    "pdf_url": "http://arxiv.org/pdf/2405.12986v2",
    "published_date": "2024-03-30 10:17:13 UTC",
    "updated_date": "2024-05-25 05:47:22 UTC"
  },
  {
    "arxiv_id": "2404.00306v2",
    "title": "Leveraging Intelligent Recommender system as a first step resilience measure -- A data-driven supply chain disruption response framework",
    "authors": [
      "Yang Hu"
    ],
    "abstract": "Interests in the value of digital technologies for its potential uses to\nincrease supply chain resilience (SCRes) are increasing in light to the\nindustry 4.0 and the global pandemic. Utilization of Recommender systems (RS)\nas a supply chain (SC) resilience measure is neglected although RS is a capable\ntool to enhance SC resilience from a reactive aspect. To address this problem,\nthis research proposed a novel data-driven supply chain disruption response\nframework based on the intelligent recommender system techniques and validated\nthe conceptual model through a practical use case. Results show that our\nframework can be implemented as an effective SC disruption mitigation measure\nin the very first response phrase and help SC participants get better reaction\nperformance after the SC disruption.",
    "categories": [
      "cs.CE",
      "cs.AI"
    ],
    "primary_category": "cs.CE",
    "comment": "Manuscript submitted for WSC2024 Conference",
    "pdf_url": "http://arxiv.org/pdf/2404.00306v2",
    "published_date": "2024-03-30 10:07:02 UTC",
    "updated_date": "2024-05-07 16:09:06 UTC"
  },
  {
    "arxiv_id": "2404.00285v1",
    "title": "Long-Tailed Recognition on Binary Networks by Calibrating A Pre-trained Model",
    "authors": [
      "Jihun Kim",
      "Dahyun Kim",
      "Hyungrok Jung",
      "Taeil Oh",
      "Jonghyun Choi"
    ],
    "abstract": "Deploying deep models in real-world scenarios entails a number of challenges,\nincluding computational efficiency and real-world (e.g., long-tailed) data\ndistributions. We address the combined challenge of learning long-tailed\ndistributions using highly resource-efficient binary neural networks as\nbackbones. Specifically, we propose a calibrate-and-distill framework that uses\noff-the-shelf pretrained full-precision models trained on balanced datasets to\nuse as teachers for distillation when learning binary networks on long-tailed\ndatasets. To better generalize to various datasets, we further propose a novel\nadversarial balancing among the terms in the objective function and an\nefficient multiresolution learning scheme. We conducted the largest empirical\nstudy in the literature using 15 datasets, including newly derived long-tailed\ndatasets from existing balanced datasets, and show that our proposed method\noutperforms prior art by large margins (>14.33% on average).",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00285v1",
    "published_date": "2024-03-30 08:37:19 UTC",
    "updated_date": "2024-03-30 08:37:19 UTC"
  },
  {
    "arxiv_id": "2404.00282v3",
    "title": "Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods",
    "authors": [
      "Yuji Cao",
      "Huan Zhao",
      "Yuheng Cheng",
      "Ting Shu",
      "Yue Chen",
      "Guolong Liu",
      "Gaoqi Liang",
      "Junhua Zhao",
      "Jinyue Yan",
      "Yun Li"
    ],
    "abstract": "With extensive pre-trained knowledge and high-level general capabilities,\nlarge language models (LLMs) emerge as a promising avenue to augment\nreinforcement learning (RL) in aspects such as multi-task learning, sample\nefficiency, and high-level task planning. In this survey, we provide a\ncomprehensive review of the existing literature in LLM-enhanced RL and\nsummarize its characteristics compared to conventional RL methods, aiming to\nclarify the research scope and directions for future studies. Utilizing the\nclassical agent-environment interaction paradigm, we propose a structured\ntaxonomy to systematically categorize LLMs' functionalities in RL, including\nfour roles: information processor, reward designer, decision-maker, and\ngenerator. For each role, we summarize the methodologies, analyze the specific\nRL challenges that are mitigated, and provide insights into future directions.\nLastly, a comparative analysis of each role, potential applications,\nprospective opportunities, and challenges of the LLM-enhanced RL are discussed.\nBy proposing this taxonomy, we aim to provide a framework for researchers to\neffectively leverage LLMs in the RL field, potentially accelerating RL\napplications in complex applications such as robotics, autonomous driving, and\nenergy systems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "22 pages (including bibliography), 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.00282v3",
    "published_date": "2024-03-30 08:28:08 UTC",
    "updated_date": "2024-10-30 02:22:46 UTC"
  },
  {
    "arxiv_id": "2404.00276v4",
    "title": "Instruction-Driven Game Engines on Large Language Models",
    "authors": [
      "Hongqiu Wu",
      "Yan Wang",
      "Xingyuan Liu",
      "Hai Zhao",
      "Min Zhang"
    ],
    "abstract": "The Instruction-Driven Game Engine (IDGE) project aims to democratize game\ndevelopment by enabling a large language model (LLM) to follow free-form game\nrules and autonomously generate game-play processes. The IDGE allows users to\ncreate games by issuing simple natural language instructions, which\nsignificantly lowers the barrier for game development. We approach the learning\nprocess for IDGEs as a Next State Prediction task, wherein the model\nautoregressively predicts in-game states given player actions. It is a\nchallenging task because the computation of in-game states must be precise;\notherwise, slight errors could disrupt the game-play. To address this, we train\nthe IDGE in a curriculum manner that progressively increases the model's\nexposure to complex scenarios. Our initial progress lies in developing an IDGE\nfor Poker, a universally cherished card game. The engine we've designed not\nonly supports a wide range of poker variants but also allows for high\ncustomization of rules through natural language inputs. Furthermore, it also\nfavors rapid prototyping of new games from minimal samples, proposing an\ninnovative paradigm in game development that relies on minimal prompt and data\nengineering. This work lays the groundwork for future advancements in\ninstruction-driven game creation, potentially transforming how games are\ndesigned and played.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00276v4",
    "published_date": "2024-03-30 08:02:16 UTC",
    "updated_date": "2024-08-23 04:06:41 UTC"
  },
  {
    "arxiv_id": "2404.00271v1",
    "title": "TG-NAS: Leveraging Zero-Cost Proxies with Transformer and Graph Convolution Networks for Efficient Neural Architecture Search",
    "authors": [
      "Ye Qiao",
      "Haocheng Xu",
      "Sitao Huang"
    ],
    "abstract": "Neural architecture search (NAS) is an effective method for discovering new\nconvolutional neural network (CNN) architectures. However, existing approaches\noften require time-consuming training or intensive sampling and evaluations.\nZero-shot NAS aims to create training-free proxies for architecture performance\nprediction. However, existing proxies have suboptimal performance, and are\noften outperformed by simple metrics such as model parameter counts or the\nnumber of floating-point operations. Besides, existing model-based proxies\ncannot be generalized to new search spaces with unseen new types of operators\nwithout golden accuracy truth. A universally optimal proxy remains elusive. We\nintroduce TG-NAS, a novel model-based universal proxy that leverages a\ntransformer-based operator embedding generator and a graph convolution network\n(GCN) to predict architecture performance. This approach guides neural\narchitecture search across any given search space without the need of\nretraining. Distinct from other model-based predictor subroutines, TG-NAS\nitself acts as a zero-cost (ZC) proxy, guiding architecture search with\nadvantages in terms of data independence, cost-effectiveness, and consistency\nacross diverse search spaces. Our experiments showcase its advantages over\nexisting proxies across various NAS benchmarks, suggesting its potential as a\nfoundational element for efficient architecture search. TG-NAS achieves up to\n300X improvements in search efficiency compared to previous SOTA ZC proxy\nmethods. Notably, it discovers competitive models with 93.75% CIFAR-10 accuracy\non the NAS-Bench-201 space and 74.5% ImageNet top-1 accuracy on the DARTS\nspace.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00271v1",
    "published_date": "2024-03-30 07:25:30 UTC",
    "updated_date": "2024-03-30 07:25:30 UTC"
  },
  {
    "arxiv_id": "2404.00261v1",
    "title": "A Simple Yet Effective Approach for Diversified Session-Based Recommendation",
    "authors": [
      "Qing Yin",
      "Hui Fang",
      "Zhu Sun",
      "Yew-Soon Ong"
    ],
    "abstract": "Session-based recommender systems (SBRSs) have become extremely popular in\nview of the core capability of capturing short-term and dynamic user\npreferences. However, most SBRSs primarily maximize recommendation accuracy but\nignore user minor preferences, thus leading to filter bubbles in the long run.\nOnly a handful of works, being devoted to improving diversity, depend on unique\nmodel designs and calibrated loss functions, which cannot be easily adapted to\nexisting accuracy-oriented SBRSs. It is thus worthwhile to come up with a\nsimple yet effective design that can be used as a plugin to facilitate existing\nSBRSs on generating a more diversified list in the meantime preserving the\nrecommendation accuracy. In this case, we propose an end-to-end framework\napplied for every existing representative (accuracy-oriented) SBRS, called\ndiversified category-aware attentive SBRS (DCA-SBRS), to boost the performance\non recommendation diversity. It consists of two novel designs: a model-agnostic\ndiversity-oriented loss function, and a non-invasive category-aware attention\nmechanism. Extensive experiments on three datasets showcase that our framework\nhelps existing SBRSs achieve extraordinary performance in terms of\nrecommendation diversity and comprehensive performance, without significantly\ndeteriorating recommendation accuracy compared to state-of-the-art\naccuracy-oriented SBRSs.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00261v1",
    "published_date": "2024-03-30 06:21:56 UTC",
    "updated_date": "2024-03-30 06:21:56 UTC"
  },
  {
    "arxiv_id": "2404.00257v2",
    "title": "YOLOOC: YOLO-based Open-Class Incremental Object Detection with Novel Class Discovery",
    "authors": [
      "Qian Wan",
      "Xiang Xiang",
      "Qinhao Zhou"
    ],
    "abstract": "Because of its use in practice, open-world object detection (OWOD) has gotten\na lot of attention recently. The challenge is how can a model detect novel\nclasses and then incrementally learn them without forgetting previously known\nclasses. Previous approaches hinge on strongly-supervised or weakly-supervised\nnovel-class data for novel-class detection, which may not apply to real\napplications. We construct a new benchmark that novel classes are only\nencountered at the inference stage. And we propose a new OWOD detector YOLOOC,\nbased on the YOLO architecture yet for the Open-Class setup. We introduce label\nsmoothing to prevent the detector from over-confidently mapping novel classes\nto known classes and to discover novel classes. Extensive experiments conducted\non our more realistic setup demonstrate the effectiveness of our method for\ndiscovering novel classes in our new benchmark.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "Withdrawn because it was submitted without consent of the first\n  author. In addition, this submission has some errors",
    "pdf_url": "http://arxiv.org/pdf/2404.00257v2",
    "published_date": "2024-03-30 06:17:39 UTC",
    "updated_date": "2024-04-22 14:38:25 UTC"
  },
  {
    "arxiv_id": "2404.00247v3",
    "title": "Facilitating Reinforcement Learning for Process Control Using Transfer Learning: Overview and Perspectives",
    "authors": [
      "Runze Lin",
      "Junghui Chen",
      "Lei Xie",
      "Hongye Su"
    ],
    "abstract": "In the context of Industry 4.0 and smart manufacturing, the field of process\nindustry optimization and control is also undergoing a digital transformation.\nWith the rise of Deep Reinforcement Learning (DRL), its application in process\ncontrol has attracted widespread attention. However, the extremely low sample\nefficiency and the safety concerns caused by exploration in DRL hinder its\npractical implementation in industrial settings. Transfer learning offers an\neffective solution for DRL, enhancing its generalization and adaptability in\nmulti-mode control scenarios. This paper provides insights into the use of DRL\nfor process control from the perspective of transfer learning. We analyze the\nchallenges of applying DRL in the process industry and the necessity of\nintroducing transfer learning. Furthermore, recommendations and prospects are\nprovided for future research directions on how transfer learning can be\nintegrated with DRL to enhance process control. This paper aims to offer a set\nof promising, user-friendly, easy-to-implement, and scalable approaches to\nartificial intelligence-facilitated industrial control for scholars and\nengineers in the process industry.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.LG",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "Chinese Control and Decision Conference (CCDC 2025), Oral, Regular\n  Paper & Asian Control Conference (ASCC 2024), Oral, Position Paper",
    "pdf_url": "http://arxiv.org/pdf/2404.00247v3",
    "published_date": "2024-03-30 04:58:59 UTC",
    "updated_date": "2025-04-22 13:05:04 UTC"
  },
  {
    "arxiv_id": "2404.00246v1",
    "title": "Your Co-Workers Matter: Evaluating Collaborative Capabilities of Language Models in Blocks World",
    "authors": [
      "Guande Wu",
      "Chen Zhao",
      "Claudio Silva",
      "He He"
    ],
    "abstract": "Language agents that interact with the world on their own have great\npotential for automating digital tasks. While large language model (LLM) agents\nhave made progress in understanding and executing tasks such as textual games\nand webpage control, many real-world tasks also require collaboration with\nhumans or other LLMs in equal roles, which involves intent understanding, task\ncoordination, and communication. To test LLM's ability to collaborate, we\ndesign a blocks-world environment, where two agents, each having unique goals\nand skills, build a target structure together. To complete the goals, they can\nact in the world and communicate in natural language. Under this environment,\nwe design increasingly challenging settings to evaluate different collaboration\nperspectives, from independent to more complex, dependent tasks. We further\nadopt chain-of-thought prompts that include intermediate reasoning steps to\nmodel the partner's state and identify and correct execution errors. Both\nhuman-machine and machine-machine experiments show that LLM agents have strong\ngrounding capacities, and our approach significantly improves the evaluation\nmetric.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00246v1",
    "published_date": "2024-03-30 04:48:38 UTC",
    "updated_date": "2024-03-30 04:48:38 UTC"
  },
  {
    "arxiv_id": "2404.00242v4",
    "title": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured LLM Inference",
    "authors": [
      "Jinwei Yao",
      "Kaiqi Chen",
      "Kexun Zhang",
      "Jiaxuan You",
      "Binhang Yuan",
      "Zeke Wang",
      "Tao Lin"
    ],
    "abstract": "Large language models (LLMs) are increasingly employed for complex tasks that\nprocess multiple generation calls in a tree structure with shared prefixes of\ntokens, including few-shot prompting, multi-step reasoning, speculative\ndecoding, etc. However, existing inference systems for tree-based applications\nare inefficient due to improper partitioning of queries and KV cache during\nattention calculation. This leads to two main issues: (1) a lack of memory\naccess (IO) reuse for KV cache of shared prefixes, and (2) poor load\nbalancing.As a result, there is redundant KV cache IO between GPU global memory\nand shared memory, along with low GPU utilization. To address these challenges,\nwe propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient\nattention algorithm with prefix-aware and load-balanced KV cache partitions.\nDeFT reduces the number of read/write operations of KV cache during attention\ncalculation through KV-Guided Grouping, a method that avoids repeatedly loading\nKV cache of shared prefixes in attention computation. Additionally, we propose\nFlattened Tree KV Splitting, a mechanism that ensures even distribution of the\nKV cache across partitions with little computation redundancy, enhancing GPU\nutilization during attention computations. By reducing 73-99% KV cache IO and\nnearly 100% IO for partial results during attention calculation, DeFT achieves\nup to 2.23/3.59x speedup in the end-to-end/attention latency across three\npractical tree-based workloads compared to state-of-the-art attention\nalgorithms. Our code is available at https://github.com/LINs-lab/DeFT.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Update DeFT-v4, accepted by ICLR'25\n  (https://openreview.net/forum?id=2c7pfOqu9k). Our code is available at\n  https://github.com/LINs-lab/DeFT",
    "pdf_url": "http://arxiv.org/pdf/2404.00242v4",
    "published_date": "2024-03-30 04:34:54 UTC",
    "updated_date": "2025-03-07 17:47:42 UTC"
  },
  {
    "arxiv_id": "2404.00231v3",
    "title": "Attention-based Shape-Deformation Networks for Artifact-Free Geometry Reconstruction of Lumbar Spine from MR Images",
    "authors": [
      "Linchen Qian",
      "Jiasong Chen",
      "Linhai Ma",
      "Timur Urakov",
      "Weiyong Gu",
      "Liang Liang"
    ],
    "abstract": "Lumbar disc degeneration, a progressive structural wear and tear of lumbar\nintervertebral disc, is regarded as an essential role on low back pain, a\nsignificant global health concern. Automated lumbar spine geometry\nreconstruction from MR images will enable fast measurement of medical\nparameters to evaluate the lumbar status, in order to determine a suitable\ntreatment. Existing image segmentation-based techniques often generate\nerroneous segments or unstructured point clouds, unsuitable for medical\nparameter measurement. In this work, we present $\\textit{UNet-DeformSA}$ and\n$\\textit{TransDeformer}$: novel attention-based deep neural networks that\nreconstruct the geometry of the lumbar spine with high spatial accuracy and\nmesh correspondence across patients, and we also present a variant of\n$\\textit{TransDeformer}$ for error estimation. Specially, we devise new\nattention modules with a new attention formula, which integrate image features\nand tokenized contour features to predict the displacements of the points on a\nshape template without the need for image segmentation. The deformed template\nreveals the lumbar spine geometry in an image. Experiment results show that our\nnetworks generate artifact-free geometry outputs, and the variant of\n$\\textit{TransDeformer}$ can predict the errors of a reconstructed geometry.\nOur code is available at https://github.com/linchenq/TransDeformer-Mesh.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00231v3",
    "published_date": "2024-03-30 03:23:52 UTC",
    "updated_date": "2024-05-01 01:47:43 UTC"
  },
  {
    "arxiv_id": "2404.00228v3",
    "title": "InfLoRA: Interference-Free Low-Rank Adaptation for Continual Learning",
    "authors": [
      "Yan-Shuo Liang",
      "Wu-Jun Li"
    ],
    "abstract": "Continual learning requires the model to learn multiple tasks sequentially.\nIn continual learning, the model should possess the ability to maintain its\nperformance on old tasks (stability) and the ability to adapt to new tasks\ncontinuously (plasticity). Recently, parameter-efficient fine-tuning (PEFT),\nwhich involves freezing a pre-trained model and injecting a small number of\nlearnable parameters to adapt to downstream tasks, has gained increasing\npopularity in continual learning. Although existing continual learning methods\nbased on PEFT have demonstrated superior performance compared to those not\nbased on PEFT, most of them do not consider how to eliminate the interference\nof the new task on the old tasks, which inhibits the model from making a good\ntrade-off between stability and plasticity. In this work, we propose a new PEFT\nmethod, called interference-free low-rank adaptation (InfLoRA), for continual\nlearning. InfLoRA injects a small number of parameters to reparameterize the\npre-trained weights and shows that fine-tuning these injected parameters is\nequivalent to fine-tuning the pre-trained weights within a subspace.\nFurthermore, InfLoRA designs this subspace to eliminate the interference of the\nnew task on the old tasks, making a good trade-off between stability and\nplasticity. Experimental results show that InfLoRA outperforms existing\nstate-of-the-art continual learning methods on multiple datasets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by the 2024 IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition (CVPR 2024)",
    "pdf_url": "http://arxiv.org/pdf/2404.00228v3",
    "published_date": "2024-03-30 03:16:37 UTC",
    "updated_date": "2024-04-03 07:15:05 UTC"
  },
  {
    "arxiv_id": "2404.00216v2",
    "title": "Is Factuality Enhancement a Free Lunch For LLMs? Better Factuality Can Lead to Worse Context-Faithfulness",
    "authors": [
      "Baolong Bi",
      "Shenghua Liu",
      "Yiwei Wang",
      "Lingrui Mei",
      "Junfeng Fang",
      "Hongcheng Gao",
      "Shiyu Ni",
      "Xueqi Cheng"
    ],
    "abstract": "As the modern tools of choice for text understanding and generation, large\nlanguage models (LLMs) are expected to accurately output answers by leveraging\nthe input context. This requires LLMs to possess both context-faithfulness and\nfactual accuracy. Extensive efforts have been made to enable better outputs\nfrom LLMs by mitigating hallucinations through factuality enhancement methods.\nHowever, they also pose risks of hindering context-faithfulness, as factuality\nenhancement can lead LLMs to become overly confident in their parametric\nknowledge, causing them to overlook the relevant input context. In this work,\nwe argue that current factuality enhancement methods can significantly\nundermine the context-faithfulness of LLMs. We first revisit the current\nfactuality enhancement methods and evaluate their effectiveness in enhancing\nfactual accuracy. Next, we evaluate their performance on knowledge editing\ntasks to assess the potential impact on context-faithfulness. The experimental\nresults reveal that while these methods may yield inconsistent improvements in\nfactual accuracy, they also cause a more severe decline in\ncontext-faithfulness, with the largest decrease reaching a striking 69.7\\%. To\nexplain these declines, we analyze the hidden states and logit distributions\nfor the tokens representing new knowledge and parametric knowledge\nrespectively, highlighting the limitations of current approaches. Our finding\nhighlights the complex trade-offs inherent in enhancing LLMs. Therefore, we\nrecommend that more research on LLMs' factuality enhancement make efforts to\nreduce the sacrifice of context-faithfulness.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00216v2",
    "published_date": "2024-03-30 02:08:28 UTC",
    "updated_date": "2024-10-04 03:30:24 UTC"
  },
  {
    "arxiv_id": "2404.00207v1",
    "title": "Causal Inference for Human-Language Model Collaboration",
    "authors": [
      "Bohan Zhang",
      "Yixin Wang",
      "Paramveer S. Dhillon"
    ],
    "abstract": "In this paper, we examine the collaborative dynamics between humans and\nlanguage models (LMs), where the interactions typically involve LMs proposing\ntext segments and humans editing or responding to these proposals. Productive\nengagement with LMs in such scenarios necessitates that humans discern\neffective text-based interaction strategies, such as editing and response\nstyles, from historical human-LM interactions. This objective is inherently\ncausal, driven by the counterfactual `what-if' question: how would the outcome\nof collaboration change if humans employed a different text editing/refinement\nstrategy? A key challenge in answering this causal inference question is\nformulating an appropriate causal estimand: the conventional average treatment\neffect (ATE) estimand is inapplicable to text-based treatments due to their\nhigh dimensionality. To address this concern, we introduce a new causal\nestimand -- Incremental Stylistic Effect (ISE) -- which characterizes the\naverage impact of infinitesimally shifting a text towards a specific style,\nsuch as increasing formality. We establish the conditions for the\nnon-parametric identification of ISE. Building on this, we develop\nCausalCollab, an algorithm designed to estimate the ISE of various interaction\nstrategies in dynamic human-LM collaborations. Our empirical investigations\nacross three distinct human-LM collaboration scenarios reveal that CausalCollab\neffectively reduces confounding and significantly improves counterfactual\nestimation over a set of competitive baselines.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages (Accepted for publication at NAACL 2024 (Main Conference))",
    "pdf_url": "http://arxiv.org/pdf/2404.00207v1",
    "published_date": "2024-03-30 01:08:25 UTC",
    "updated_date": "2024-03-30 01:08:25 UTC"
  }
]
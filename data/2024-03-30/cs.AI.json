{
  "date": "2024-03-30",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-03-30 的 arXiv 中文 TLDR 快报！今天 arXiv 更新了 45 篇论文，主要聚焦于 AI 模型的安全性、持续学习、多模态融合和应用创新，其中 LLM 在安全调优和多任务协作上的进展（如 Victor Gallego 的 Configurable Safety Tuning）令人印象深刻，同时开源多语言模型 Aurora-M 也值得关注。\n\n### 重点论文讨论\n我们先聊聊 AI 和 LLM 相关的高话题度论文，这些领域进展迅速，可能影响未来应用。接着快速概述其他重要或相关论文，优先突出核心贡献。\n\n1. **标题（中文）：可配置安全调优语言模型的合成偏好数据**  \n   **标题（英文）：Configurable Safety Tuning of Language Models with Synthetic Preference Data**  \n   作者：Victor Gallego 等。  \n   这篇论文提出 CST 方法，通过合成偏好数据增强 Direct Preference Optimization（DPO），允许在推理时灵活配置 LLM 的安全设置（如启用/禁用偏好），实验显示 CST 能有效管理安全配置，同时保留模型原功能。贡献在于提升 LLM 部署的灵活性和鲁棒性。\n\n2. **标题（中文）：辩证对齐：解决 LLM 的 3H 原则与安全威胁**  \n   **标题（英文）：Dialectical Alignment: Resolving the Tension of 3H and Security Threats of LLMs**  \n   作者：Shu Yang 等。  \n   论文针对 LLM 的 helpful, honest, harmless（3H）原则，提出辩证对齐框架，使用 AI 反馈构建数据集，以防御外部数据攻击，同时保持知识编辑能力。实验证明，该方法提升了 20% 的攻击防御性能，强调 LLM 安全与实用性的平衡。\n\n3. **标题（中文）：语言模型的语言校准**  \n   **标题（英文）：Linguistic Calibration of Long-Form Generations**  \n   作者：Neil Band 等。  \n   这篇论文定义了长文本生成的语言校准概念，通过监督微调和强化学习，使 LLM 生成可信度更高的文本，帮助用户做出校准决策。贡献在于提升 LLM 在决策任务中的可靠性，实验在科学和生物医学领域表现出色。\n\n4. **标题（中文）：Aurora-M：开源的多语言和代码持续预训练模型**  \n   **标题（英文）：Aurora-M: Open Source Continual Pre-training for Multilingual Language and Code**  \n   作者：Taishi Nakamura 等（多人团队，包括开源社区贡献者）。  \n   论文介绍 Aurora-M 模型，在 StarCoderPlus 基础上预训练，支持英语、芬兰语、日语等语言和代码，总训练量超 2T 标记，并通过安全指令微调。该模型在多语言任务中表现出色，是首个开源多语言 LLM，强调社区协作和安全标准。\n\n接下来，我们快速聊聊其他领域的重要论文，相关主题放在一起讨论，跳过一些较基础或应用性不强的文章（如特定数据集构建或常规优化）以控制篇幅。\n\n- **多模态融合和计算机视觉相关**：  \n  **标题（中文）：多阶段融合架构用于小型无人机定位和识别**  \n  **标题（英文）：Multi-Stage Fusion Architecture for Small-Drone Localization and Identification Using Passive RF and EO Imagery**  \n  作者：Thakshila Wimalajeewa Wewelwala 等。  \n  论文提出多阶段融合框架，结合射频（RF）和光学（EO）数据，提高小型无人机检测和跟踪精度。贡献在于处理复杂环境下的鲁棒性，实验在真实数据上验证有效。\n\n  **标题（中文）：YOLOOC：基于 YOLO 的开放类增量目标检测**  \n  **标题（英文）：YOLOOC: YOLO-based Open-Class Incremental Object Detection with Novel Class Discovery**  \n  作者：Qian Wan 等。  \n  构建新基准，支持 YOLO 在开放类设置下发现新类别，无需额外标注。贡献在于提升目标检测的适应性，实验在真实场景中表现良好。\n\n- **机器人和应用创新**：  \n  **标题（中文）：语言模型作为航天器操作员**  \n  **标题（英文）：Language Models are Spacecraft Operators**  \n  作者：Victor Rodriguez-Fernandez 等。  \n  论文探索 LLM 作为自主代理在航天控制中的应用，使用提示工程和微调在 Kerbal Space Program 挑战中排名第二。贡献在于开拓 LLM 在航天决策的潜力。\n\n  **标题（中文）：交互式多机器人集群**  \n  **标题（英文）：Interactive Multi-Robot Flocking with Gesture Responsiveness and Musical Accompaniment**  \n  作者：Catie Cuan 等。  \n  提出算法实现多机器人与人类互动，支持手势响应和音乐伴奏。贡献在于提升机器人娱乐性和社交任务性能，实验显示用户体验一致。\n\n其他论文如噪声感知训练（Noise-Aware Training）和持续学习方法（InfLoRA）等，虽然有技术创新，但相对常规，我们就快速掠过：这些工作优化了模型效率和鲁棒性，但影响力不如上述热门主题。总之，今天的更新突显 AI 安全和多领域融合的趋势，感兴趣的读者可关注 LLM 相关论文深入探索。明天见！",
  "papers": [
    {
      "arxiv_id": "2404.00495v1",
      "title": "Configurable Safety Tuning of Language Models with Synthetic Preference Data",
      "title_zh": "翻译失败",
      "authors": [
        "Victor Gallego"
      ],
      "abstract": "State-of-the-art language model fine-tuning techniques, such as Direct\nPreference Optimization (DPO), restrict user control by hard-coding predefined\nbehaviors into the model. To address this, we propose a novel method,\nConfigurable Safety Tuning (CST), that augments DPO using synthetic preference\ndata to facilitate flexible safety configuration of LLMs at inference time. CST\novercomes the constraints of vanilla DPO by introducing a system prompt\nspecifying safety configurations, enabling LLM deployers to disable/enable\nsafety preferences based on their need, just changing the system prompt. Our\nexperimental evaluations indicate that CST successfully manages different\nsafety configurations and retains the original functionality of LLMs, showing\nit is a robust method for configurable deployment. Data and models available at\nhttps://github.com/vicgalle/configurable-safety-tuning",
      "tldr_zh": "本论文提出 Configurable Safety Tuning (CST)，一种使用合成偏好数据增强 Direct Preference Optimization (DPO) 的方法，旨在解决现有语言模型微调技术中预定义行为硬编码的问题，从而实现灵活的安全配置。CST 通过引入指定安全配置的系统提示，允许 LLM 部署者在推理时仅需更改提示即可启用或禁用安全偏好，而不影响模型的原有功能。实验评估表明，CST 成功管理了不同安全设置，并证明了其作为可配置部署的稳健方法。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.00495v1",
      "published_date": "2024-03-30 23:28:05 UTC",
      "updated_date": "2024-03-30 23:28:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:15:31.642203"
    },
    {
      "arxiv_id": "2404.00492v1",
      "title": "Multi-hop Question Answering under Temporal Knowledge Editing",
      "title_zh": "在时间知识编辑下的多跳问题回答",
      "authors": [
        "Keyuan Cheng",
        "Gang Lin",
        "Haoyang Fei",
        "Yuxuan zhai",
        "Lu Yu",
        "Muhammad Asif Ali",
        "Lijie Hu",
        "Di Wang"
      ],
      "abstract": "Multi-hop question answering (MQA) under knowledge editing (KE) has garnered\nsignificant attention in the era of large language models. However, existing\nmodels for MQA under KE exhibit poor performance when dealing with questions\ncontaining explicit temporal contexts. To address this limitation, we propose a\nnovel framework, namely TEMPoral knowLEdge augmented Multi-hop Question\nAnswering (TEMPLE-MQA). Unlike previous methods, TEMPLE-MQA first constructs a\ntime-aware graph (TAG) to store edit knowledge in a structured manner. Then,\nthrough our proposed inference path, structural retrieval, and joint reasoning\nstages, TEMPLE-MQA effectively discerns temporal contexts within the question\nquery. Experiments on benchmark datasets demonstrate that TEMPLE-MQA\nsignificantly outperforms baseline models. Additionally, we contribute a new\ndataset, namely TKEMQA, which serves as the inaugural benchmark tailored\nspecifically for MQA with temporal scopes.",
      "tldr_zh": "这篇论文针对知识编辑（Knowledge Editing）下的多跳问答（Multi-hop Question Answering, MQA）问题，提出了一种新框架TEMPLE-MQA，以解决现有模型在处理带有显式时间上下文的问题时的表现不佳。TEMPLE-MQA首先构建时间感知图（Time-Aware Graph, TAG）来结构化存储编辑知识，然后通过推理路径、结构检索和联合推理阶段有效识别问题中的时间上下文。实验结果显示，该框架在基准数据集上显著优于基线模型，同时论文贡献了一个新数据集TKEMQA，作为首个专门针对带有时间范围的MQA的基准。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "23 pages",
      "pdf_url": "http://arxiv.org/pdf/2404.00492v1",
      "published_date": "2024-03-30 23:22:51 UTC",
      "updated_date": "2024-03-30 23:22:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:15:43.450881"
    },
    {
      "arxiv_id": "2404.00489v2",
      "title": "Prompt-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression",
      "title_zh": "Prompt-SAW：利用关系感知图进行文本提示压缩",
      "authors": [
        "Muhammad Asif Ali",
        "Zhengping Li",
        "Shu Yang",
        "Keyuan Cheng",
        "Yang Cao",
        "Tianhao Huang",
        "Guimin Hu",
        "Weimin Lyu",
        "Lijie Hu",
        "Lu Yu",
        "Di Wang"
      ],
      "abstract": "Large Language Models (LLMs) have shown exceptional abilities for multiple\ndifferent natural language processing tasks. While prompting is a crucial tool\nfor LLM inference, we observe that there is a significant cost associated with\nexceedingly lengthy prompts. Existing attempts to compress lengthy prompts lead\nto substandard results in terms of readability/interpretability of the\ncompressed prompt, with a detrimental impact on prompt utility. To address\nthis, we propose PromptSAW: Prompt compresSion via Relation AWare graphs, an\neffective strategy for prompt compression over task-agnostic and task-aware\nprompts. Prompt-SAW uses the prompt's textual information to build a graph and\nlater extracts key information elements in the graph to come up with the\ncompressed prompt. We also propose GSM8K-aug, i.e., an extended version of the\nexisting GSM8K benchmark for task-agnostic prompts in order to provide a\ncomprehensive evaluation platform. Experimental evaluation using benchmark\ndatasets shows that prompts compressed by Prompt-SAW are not only better in\nterms of readability, but they also outperform the best-performing baseline\nmodels by up to 10.1 and 77.1, respectively, for task-agnostic and task-aware\nsettings while compressing the original prompt text by 34.9 and 56.7.",
      "tldr_zh": "本研究针对大型语言模型 (LLMs) 的长提示带来的成本问题，提出 Prompt-SAW 框架，该框架利用 Relation-Aware Graphs 基于提示文本构建图结构，并提取关键信息元素来实现高效压缩，同时保持可读性和实用性。Prompt-SAW 支持任务无关和任务相关提示，并引入扩展基准 GSM8K-aug 以提供全面评估平台。实验结果显示，该方法在任务无关和任务相关设置中分别比最佳基线模型提升 10.1 和 77.1 分，同时将原始提示文本压缩 34.9% 和 56.7%。这项工作为优化 LLM 提示提供了可解释且有效的策略。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "16 pages",
      "pdf_url": "http://arxiv.org/pdf/2404.00489v2",
      "published_date": "2024-03-30 23:07:58 UTC",
      "updated_date": "2024-10-17 15:20:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:15:57.292544"
    },
    {
      "arxiv_id": "2404.00488v1",
      "title": "Noise-Aware Training of Layout-Aware Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Ritesh Sarkhel",
        "Xiaoqi Ren",
        "Lauro Beltrao Costa",
        "Guolong Su",
        "Vincent Perot",
        "Yanan Xie",
        "Emmanouil Koukoumidis",
        "Arnab Nandi"
      ],
      "abstract": "A visually rich document (VRD) utilizes visual features along with linguistic\ncues to disseminate information. Training a custom extractor that identifies\nnamed entities from a document requires a large number of instances of the\ntarget document type annotated at textual and visual modalities. This is an\nexpensive bottleneck in enterprise scenarios, where we want to train custom\nextractors for thousands of different document types in a scalable way.\nPre-training an extractor model on unlabeled instances of the target document\ntype, followed by a fine-tuning step on human-labeled instances does not work\nin these scenarios, as it surpasses the maximum allowable training time\nallocated for the extractor. We address this scenario by proposing a\nNoise-Aware Training method or NAT in this paper. Instead of acquiring\nexpensive human-labeled documents, NAT utilizes weakly labeled documents to\ntrain an extractor in a scalable way. To avoid degradation in the model's\nquality due to noisy, weakly labeled samples, NAT estimates the confidence of\neach training sample and incorporates it as uncertainty measure during\ntraining. We train multiple state-of-the-art extractor models using NAT.\nExperiments on a number of publicly available and in-house datasets show that\nNAT-trained models are not only robust in performance -- it outperforms a\ntransfer-learning baseline by up to 6% in terms of macro-F1 score, but it is\nalso more label-efficient -- it reduces the amount of human-effort required to\nobtain comparable performance by up to 73%.",
      "tldr_zh": "本研究针对视觉丰富文档 (VRD) 中的命名实体提取问题，提出了一种 Noise-Aware Training (NAT) 方法，用于训练 Layout-Aware Language Models。该方法利用弱标记文档进行可扩展训练，通过估计每个样本的置信度并将其作为不确定性措施，缓解噪声数据对模型质量的影响，从而避免了昂贵的完全标记数据需求。实验结果显示，NAT 训练的模型在多个公开和内部数据集上，比转移学习基准提高了高达 6% 的 macro-F1 分数，并可减少高达 73% 的手动标注工作量。总体而言，该方法提升了模型的鲁棒性和标签效率，为大规模企业场景下的自定义提取器训练提供了实用解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.00488v1",
      "published_date": "2024-03-30 23:06:34 UTC",
      "updated_date": "2024-03-30 23:06:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:16:07.982725"
    },
    {
      "arxiv_id": "2404.00487v1",
      "title": "Contextual AI Journaling: Integrating LLM and Time Series Behavioral Sensing Technology to Promote Self-Reflection and Well-being using the MindScape App",
      "title_zh": "翻译失败",
      "authors": [
        "Subigya Nepal",
        "Arvind Pillai",
        "William Campbell",
        "Talie Massachi",
        "Eunsol Soul Choi",
        "Orson Xu",
        "Joanna Kuc",
        "Jeremy Huckins",
        "Jason Holden",
        "Colin Depp",
        "Nicholas Jacobson",
        "Mary Czerwinski",
        "Eric Granholm",
        "Andrew T. Campbell"
      ],
      "abstract": "MindScape aims to study the benefits of integrating time series behavioral\npatterns (e.g., conversational engagement, sleep, location) with Large Language\nModels (LLMs) to create a new form of contextual AI journaling, promoting\nself-reflection and well-being. We argue that integrating behavioral sensing in\nLLMs will likely lead to a new frontier in AI. In this Late-Breaking Work\npaper, we discuss the MindScape contextual journal App design that uses LLMs\nand behavioral sensing to generate contextual and personalized journaling\nprompts crafted to encourage self-reflection and emotional development. We also\ndiscuss the MindScape study of college students based on a preliminary user\nstudy and our upcoming study to assess the effectiveness of contextual AI\njournaling in promoting better well-being on college campuses. MindScape\nrepresents a new application class that embeds behavioral intelligence in AI.",
      "tldr_zh": "MindScape 研究提出了一种整合时间序列行为模式（如对话参与度、睡眠和位置）与 Large Language Models (LLMs) 的上下文AI日记应用，旨在通过生成个性化的日记提示促进自我反思和福祉。该系统设计利用行为感知技术创建针对大学生的个性化提示，鼓励情感发展和心理健康。初步用户研究和即将进行的校园评估显示，这种方法可能开辟AI的新领域，并代表将行为智能嵌入AI的应用新类别。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "H.5.0; H.5.3; H.5.m; J.0"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.00487v1",
      "published_date": "2024-03-30 23:01:34 UTC",
      "updated_date": "2024-03-30 23:01:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:16:20.519201"
    },
    {
      "arxiv_id": "2406.16875v1",
      "title": "Multi-Stage Fusion Architecture for Small-Drone Localization and Identification Using Passive RF and EO Imagery: A Case Study",
      "title_zh": "多阶段融合架构用于小型无人机定位和识别，使用被动射频和电光图像：一个案例研究",
      "authors": [
        "Thakshila Wimalajeewa Wewelwala",
        "Thomas W. Tedesso",
        "Tony Davis"
      ],
      "abstract": "Reliable detection, localization and identification of small drones is\nessential to promote safe, secure and privacy-respecting operation of\nUnmanned-Aerial Systems (UAS), or simply, drones. This is an increasingly\nchallenging problem with only single modality sensing, especially, to detect\nand identify small drones. In this work, a multi-stage fusion architecture\nusing passive radio frequency (RF) and electro-optic (EO) imagery data is\ndeveloped to leverage the synergies of the modalities to improve the overall\ntracking and classification capabilities. For detection with EO-imagery,\nsupervised deep learning based techniques as well as unsupervised\nforeground/background separation techniques are explored to cope with\nchallenging environments. Using real collected data for Group 1 and 2 drones,\nthe capability of each algorithm is quantified. In order to compensate for any\nperformance gaps in detection with only EO imagery as well as to provide a\nunique device identifier for the drones, passive RF is integrated with EO\nimagery whenever available. In particular, drone detections in the image plane\nare combined with passive RF location estimates via detection-to-detection\nassociation after 3D to 2D transformation. Final tracking is performed on the\ncomposite detections in the 2D image plane. Each track centroid is given a\nunique identification obtained via RF fingerprinting. The proposed fusion\narchitecture is tested and the tracking and performance is quantified over the\nrange to illustrate the effectiveness of the proposed approaches using\nsimultaneously collected passive RF and EO data at the Air Force Research\nLaboratory (AFRL) through ESCAPE-21 (Experiments, Scenarios, Concept of\nOperations, and Prototype Engineering) data collect",
      "tldr_zh": "这篇论文提出了一种多阶段融合架构，用于小型无人机的检测、定位和识别，结合被动射频 (passive RF) 和电光 (EO) 图像数据，以提升单模态传感的局限性。方法包括采用监督和无监督深度学习技术处理 EO 图像检测，并通过 RF 数据补偿性能差距，实现检测关联（detection-to-detection association）和 3D 到 2D 转换，最终在 2D 图像平面进行跟踪和 RF 指纹识别 (RF fingerprinting) 以提供唯一标识。在使用 Air Force Research Laboratory (AFRL) 的 ESCAPE-21 数据集进行测试中，该架构显著提高了跟踪和分类性能，证明了其在实际场景中的有效性。",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.16875v1",
      "published_date": "2024-03-30 22:53:28 UTC",
      "updated_date": "2024-03-30 22:53:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:16:31.705900"
    },
    {
      "arxiv_id": "2404.00486v1",
      "title": "Dialectical Alignment: Resolving the Tension of 3H and Security Threats of LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Shu Yang",
        "Jiayuan Su",
        "Han Jiang",
        "Mengdi Li",
        "Keyuan Cheng",
        "Muhammad Asif Ali",
        "Lijie Hu",
        "Di Wang"
      ],
      "abstract": "With the rise of large language models (LLMs), ensuring they embody the\nprinciples of being helpful, honest, and harmless (3H), known as Human\nAlignment, becomes crucial. While existing alignment methods like RLHF, DPO,\netc., effectively fine-tune LLMs to match preferences in the preference\ndataset, they often lead LLMs to highly receptive human input and external\nevidence, even when this information is poisoned. This leads to a tendency for\nLLMs to be Adaptive Chameleons when external evidence conflicts with their\nparametric memory. This exacerbates the risk of LLM being attacked by external\npoisoned data, which poses a significant security risk to LLM system\napplications such as Retrieval-augmented generation (RAG). To address the\nchallenge, we propose a novel framework: Dialectical Alignment (DA), which (1)\nutilizes AI feedback to identify optimal strategies for LLMs to navigate\ninter-context conflicts and context-memory conflicts with different external\nevidence in context window (i.e., different ratios of poisoned factual\ncontexts); (2) constructs the SFT dataset as well as the preference dataset\nbased on the AI feedback and strategies above; (3) uses the above datasets for\nLLM alignment to defense poisoned context attack while preserving the\neffectiveness of in-context knowledge editing. Our experiments show that the\ndialectical alignment model improves poisoned data attack defense by 20 and\ndoes not require any additional prompt engineering or prior declaration of\n``you may be attacked`` to the LLMs' context window.",
      "tldr_zh": "该论文探讨了大型语言模型（LLMs）在实现 helpful, honest, and harmless（3H）原则时面临的安荃威胁问题，现有方法如 RLHF 和 DPO 可能导致模型过度适应毒化外部证据，从而增加攻击风险，尤其在 Retrieval-augmented generation (RAG) 等应用中。\n为解决这一张力，研究提出 Dialectical Alignment (DA) 框架，该框架利用 AI 反馈识别处理上下文冲突和记忆冲突的最佳策略，并基于这些策略构建 SFT 数据集和偏好数据集，以进行模型对齐训练。\n实验结果表明，DA 模型将毒化数据攻击防御能力提高了 20%，同时保留了上下文知识编辑的有效性，且无需额外提示工程或警告。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.00486v1",
      "published_date": "2024-03-30 22:41:05 UTC",
      "updated_date": "2024-03-30 22:41:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:16:45.150198"
    },
    {
      "arxiv_id": "2404.00482v2",
      "title": "Cross-lingual Named Entity Corpus for Slavic Languages",
      "title_zh": "翻译失败",
      "authors": [
        "Jakub Piskorski",
        "Michał Marcińczuk",
        "Roman Yangarber"
      ],
      "abstract": "This paper presents a corpus manually annotated with named entities for six\nSlavic languages - Bulgarian, Czech, Polish, Slovenian, Russian, and Ukrainian.\nThis work is the result of a series of shared tasks, conducted in 2017-2023 as\na part of the Workshops on Slavic Natural Language Processing. The corpus\nconsists of 5 017 documents on seven topics. The documents are annotated with\nfive classes of named entities. Each entity is described by a category, a\nlemma, and a unique cross-lingual identifier. We provide two train-tune dataset\nsplits - single topic out and cross topics. For each split, we set benchmarks\nusing a transformer-based neural network architecture with the pre-trained\nmultilingual models - XLM-RoBERTa-large for named entity mention recognition\nand categorization, and mT5-large for named entity lemmatization and linking.",
      "tldr_zh": "本论文构建了一个手动标注的命名实体语料库，涵盖六种斯拉夫语言（Bulgarian, Czech, Polish, Slovenian, Russian 和 Ukrainian），作为2017-2023年Slavic Natural Language Processing Workshops系列共享任务的结果。语料库包含5017个文档，覆盖七个主题，并标注了五类named entities，每个实体包括类别、lemma和唯一的cross-lingual identifier；同时提供了两种数据集分割：single topic out和cross topics。作者使用基于transformer的神经网络架构（如XLM-RoBERTa-large用于named entity mention recognition和categorization，以及mT5-large用于named entity lemmatization和linking）设置了基准性能，为跨语言命名实体处理任务提供了宝贵资源。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Published in LREC-COLING 2024 - The 2024 Joint International\n  Conference on Computational Linguistics, Language Resources and Evaluation",
      "pdf_url": "http://arxiv.org/pdf/2404.00482v2",
      "published_date": "2024-03-30 22:20:08 UTC",
      "updated_date": "2024-04-07 16:56:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:16:55.254965"
    },
    {
      "arxiv_id": "2404.00474v2",
      "title": "Linguistic Calibration of Long-Form Generations",
      "title_zh": "翻译失败",
      "authors": [
        "Neil Band",
        "Xuechen Li",
        "Tengyu Ma",
        "Tatsunori Hashimoto"
      ],
      "abstract": "Language models (LMs) may lead their users to make suboptimal downstream\ndecisions when they confidently hallucinate. This issue can be mitigated by\nhaving the LM verbally convey the probability that its claims are correct, but\nexisting models cannot produce long-form text with calibrated confidence\nstatements. Through the lens of decision-making, we define linguistic\ncalibration for long-form generations: an LM is linguistically calibrated if\nits generations enable its users to make calibrated probabilistic predictions.\nThis definition enables a training framework where a supervised finetuning step\nbootstraps an LM to emit long-form generations with confidence statements such\nas \"I estimate a 30% chance of...\" or \"I am certain that...\", followed by a\nreinforcement learning step which rewards generations that enable a user to\nprovide calibrated answers to related questions. We linguistically calibrate\nLlama 2 7B and find in automated and human evaluations of long-form generations\nthat it is significantly more calibrated than strong finetuned factuality\nbaselines with comparable accuracy. These findings generalize under significant\ndomain shifts to scientific and biomedical questions and to an entirely\nheld-out person biography generation task. Our results demonstrate that\nlong-form generations may be calibrated end-to-end by constructing an objective\nin the space of the predictions that users make in downstream decision-making.",
      "tldr_zh": "本研究针对语言模型（LMs）在生成长文本时自信地产生幻觉问题，定义了“linguistic calibration”概念，即模型的生成能帮助用户做出校准的概率预测，以优化下游决策。研究提出一个训练框架，包括监督微调让LMs生成带有信心声明的文本（如“I estimate a 30% chance of...”），以及后续的reinforcement learning步骤奖励那些提升用户校准答案的生成。实验结果显示，经过linguistic calibration的Llama 2 7B模型在长文本生成上显著优于事实性基线模型，保持可比的准确性，且这些改进在科学、生物医学领域转移和全新人物传记任务中均有效，证明了端到端校准的可行性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML 2024. Code available at\n  https://github.com/tatsu-lab/linguistic_calibration",
      "pdf_url": "http://arxiv.org/pdf/2404.00474v2",
      "published_date": "2024-03-30 20:47:55 UTC",
      "updated_date": "2024-06-04 22:39:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:17:08.706140"
    },
    {
      "arxiv_id": "2404.00461v1",
      "title": "Shortcuts Arising from Contrast: Effective and Covert Clean-Label Attacks in Prompt-Based Learning",
      "title_zh": "源于对比的捷径：基于提示学习的有效且隐蔽的干净标签",
      "authors": [
        "Xiaopeng Xie",
        "Ming Yan",
        "Xiwen Zhou",
        "Chenlong Zhao",
        "Suli Wang",
        "Yong Zhang",
        "Joey Tianyi Zhou"
      ],
      "abstract": "Prompt-based learning paradigm has demonstrated remarkable efficacy in\nenhancing the adaptability of pretrained language models (PLMs), particularly\nin few-shot scenarios. However, this learning paradigm has been shown to be\nvulnerable to backdoor attacks. The current clean-label attack, employing a\nspecific prompt as a trigger, can achieve success without the need for external\ntriggers and ensure correct labeling of poisoned samples, which is more\nstealthy compared to the poisoned-label attack, but on the other hand, it faces\nsignificant issues with false activations and poses greater challenges,\nnecessitating a higher rate of poisoning. Using conventional negative data\naugmentation methods, we discovered that it is challenging to trade off between\neffectiveness and stealthiness in a clean-label setting. In addressing this\nissue, we are inspired by the notion that a backdoor acts as a shortcut and\nposit that this shortcut stems from the contrast between the trigger and the\ndata utilized for poisoning. In this study, we propose a method named\nContrastive Shortcut Injection (CSI), by leveraging activation values,\nintegrates trigger design and data selection strategies to craft stronger\nshortcut features. With extensive experiments on full-shot and few-shot text\nclassification tasks, we empirically validate CSI's high effectiveness and high\nstealthiness at low poisoning rates. Notably, we found that the two approaches\nplay leading roles in full-shot and few-shot settings, respectively.",
      "tldr_zh": "这篇论文探讨了Prompt-based learning在预训练语言模型(PLMs)中的易受clean-label backdoor attacks的问题，强调了现有攻击方法在有效性和隐蔽性之间难以平衡，特别是面对假激活和高中毒率挑战。作者提出Contrastive Shortcut Injection (CSI)方法，通过利用activation values整合触发器设计和数据选择策略，创建更强的捷径特征，以增强攻击的针对性和隐秘性。在全样本和少样本文本分类任务的广泛实验中，CSI在低中毒率下显示出高有效性和高隐蔽性，并发现不同策略在全样本和少样本设置中分别发挥主导作用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CR",
        "68T50",
        "I.2.7"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 6 figures, conference",
      "pdf_url": "http://arxiv.org/pdf/2404.00461v1",
      "published_date": "2024-03-30 20:02:36 UTC",
      "updated_date": "2024-03-30 20:02:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:17:20.699192"
    },
    {
      "arxiv_id": "2404.00450v2",
      "title": "Planning and Editing What You Retrieve for Enhanced Tool Learning",
      "title_zh": "规划和编辑你检索的内容以增强工具学习",
      "authors": [
        "Tenghao Huang",
        "Dongwon Jung",
        "Muhao Chen"
      ],
      "abstract": "Recent advancements in integrating external tools with Large Language Models\n(LLMs) have opened new frontiers, with applications in mathematical reasoning,\ncode generators, and smart assistants. However, existing methods, relying on\nsimple one-time retrieval strategies, fall short on effectively and accurately\nshortlisting relevant tools. This paper introduces a novel PLUTO (Planning,\nLearning, and Understanding for TOols) approach, encompassing\n`Plan-and-Retrieve (P&R)` and `Edit-and-Ground (E&G)` paradigms. The P&R\nparadigm consists of a neural retrieval module for shortlisting relevant tools\nand an LLM-based query planner that decomposes complex queries into actionable\ntasks, enhancing the effectiveness of tool utilization. The E&G paradigm\nutilizes LLMs to enrich tool descriptions based on user scenarios, bridging the\ngap between user queries and tool functionalities. Experiment results\ndemonstrate that these paradigms significantly improve the recall and NDCG in\ntool retrieval tasks, significantly surpassing current state-of-the-art models.",
      "tldr_zh": "这篇论文提出了 PLUTO 框架，用于提升大型语言模型（LLMs）与外部工具的整合，解决现有简单检索策略在工具筛选上的不足。框架包括 Plan-and-Retrieve (P&R) 范式，该范式结合神经检索模块和 LLM-based 查询规划器，将复杂查询分解为可操作任务，以提高工具利用效率；以及 Edit-and-Ground (E&G) 范式，使用 LLMs 基于用户场景丰富工具描述，从而桥接查询与工具功能间的差距。实验结果显示，PLUTO 在工具检索任务中显著提高了召回率和 NDCG，超越了当前最先进模型。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "This paper is accepted at NAACL-Findings 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.00450v2",
      "published_date": "2024-03-30 18:41:51 UTC",
      "updated_date": "2024-04-04 05:33:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:17:34.457849"
    },
    {
      "arxiv_id": "2404.00442v1",
      "title": "Interactive Multi-Robot Flocking with Gesture Responsiveness and Musical Accompaniment",
      "title_zh": "交互式多机器人集群行为，带有手势响应和音乐伴奏",
      "authors": [
        "Catie Cuan",
        "Kyle Jeffrey",
        "Kim Kleiven",
        "Adrian Li",
        "Emre Fisher",
        "Matt Harrison",
        "Benjie Holson",
        "Allison Okamura",
        "Matt Bennice"
      ],
      "abstract": "For decades, robotics researchers have pursued various tasks for multi-robot\nsystems, from cooperative manipulation to search and rescue. These tasks are\nmulti-robot extensions of classical robotic tasks and often optimized on\ndimensions such as speed or efficiency. As robots transition from commercial\nand research settings into everyday environments, social task aims such as\nengagement or entertainment become increasingly relevant. This work presents a\ncompelling multi-robot task, in which the main aim is to enthrall and interest.\nIn this task, the goal is for a human to be drawn to move alongside and\nparticipate in a dynamic, expressive robot flock. Towards this aim, the\nresearch team created algorithms for robot movements and engaging interaction\nmodes such as gestures and sound. The contributions are as follows: (1) a novel\ngroup navigation algorithm involving human and robot agents, (2) a gesture\nresponsive algorithm for real-time, human-robot flocking interaction, (3) a\nweight mode characterization system for modifying flocking behavior, and (4) a\nmethod of encoding a choreographer's preferences inside a dynamic, adaptive,\nlearned system. An experiment was performed to understand individual human\nbehavior while interacting with the flock under three conditions: weight modes\nselected by a human choreographer, a learned model, or subset list. Results\nfrom the experiment showed that the perception of the experience was not\ninfluenced by the weight mode selection. This work elucidates how differing\ntask aims such as engagement manifest in multi-robot system design and\nexecution, and broadens the domain of multi-robot tasks.",
      "tldr_zh": "这篇论文提出了一种互动多机器人群系统，旨在通过手势响应和音乐伴奏吸引人类参与动态、表现力的机器人群行为，以增强社交互动和娱乐性。关键贡献包括：(1) 一个涉及人类和机器人代理的群导航算法，(2) 一种实时手势响应算法，(3) 一个权重模式表征系统用于修改群行为，以及(4) 一种动态适应性学习系统来编码编舞者偏好。研究团队通过实验比较了三种权重模式选择（由人类编舞者、学习模型或子集列表决定）对人类体验的影响，结果显示这些选择并未影响感知。该工作扩展了多机器人任务的领域，强调了参与感和娱乐性在系统设计中的重要作用。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.00442v1",
      "published_date": "2024-03-30 18:16:28 UTC",
      "updated_date": "2024-03-30 18:16:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:17:48.075602"
    },
    {
      "arxiv_id": "2404.00438v1",
      "title": "Communication Efficient Distributed Training with Distributed Lion",
      "title_zh": "通信高效的分布式",
      "authors": [
        "Bo Liu",
        "Lemeng Wu",
        "Lizhang Chen",
        "Kaizhao Liang",
        "Jiaxu Zhu",
        "Chen Liang",
        "Raghuraman Krishnamoorthi",
        "Qiang Liu"
      ],
      "abstract": "The Lion optimizer has been a promising competitor with the AdamW for\ntraining large AI models, with advantages on memory, computation, and sample\nefficiency. In this paper, we introduce Distributed Lion, an innovative\nadaptation of Lion for distributed training environments. Leveraging the sign\noperator in Lion, our Distributed Lion only requires communicating binary or\nlower-precision vectors between workers to the center server, significantly\nreducing the communication cost. Our theoretical analysis confirms Distributed\nLion's convergence properties. Empirical results demonstrate its robustness\nacross a range of tasks, worker counts, and batch sizes, on both vision and\nlanguage problems. Notably, Distributed Lion attains comparable performance to\nstandard Lion or AdamW optimizers applied on aggregated gradients, but with\nsignificantly reduced communication bandwidth. This feature is particularly\nadvantageous for training large models. In addition, we also demonstrate that\nDistributed Lion presents a more favorable performance-bandwidth balance\ncompared to existing efficient distributed methods such as deep gradient\ncompression and ternary gradients.",
      "tldr_zh": "该论文提出 Distributed Lion，一种针对 Lion 优化器的分布式训练方法，通过利用 sign operator 仅通信二进制或低精度向量，显著降低了训练过程中的通信成本。理论分析证明了 Distributed Lion 的收敛性，并在视觉和语言任务上实验验证了其在不同工人数量和批量大小下的鲁棒性。与标准 Lion 或 AdamW 优化器相比，Distributed Lion 实现了相似的性能，但通信带宽大幅减少，并在性能-带宽平衡上优于现有方法如 deep gradient compression 和 ternary gradients。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.LG",
        "math.OC",
        "stat.ML"
      ],
      "primary_category": "cs.DC",
      "comment": "22 pages",
      "pdf_url": "http://arxiv.org/pdf/2404.00438v1",
      "published_date": "2024-03-30 18:07:29 UTC",
      "updated_date": "2024-03-30 18:07:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:17:58.500985"
    },
    {
      "arxiv_id": "2404.00437v1",
      "title": "Automatic explanation of the classification of Spanish legal judgments in jurisdiction-dependent law categories with tree estimators",
      "title_zh": "利用树估计",
      "authors": [
        "Jaime González-González",
        "Francisco de Arriba-Pérez",
        "Silvia García-Méndez",
        "Andrea Busto-Castiñeira",
        "Francisco J. González-Castaño"
      ],
      "abstract": "Automatic legal text classification systems have been proposed in the\nliterature to address knowledge extraction from judgments and detect their\naspects. However, most of these systems are black boxes even when their models\nare interpretable. This may raise concerns about their trustworthiness.\nAccordingly, this work contributes with a system combining Natural Language\nProcessing (NLP) with Machine Learning (ML) to classify legal texts in an\nexplainable manner. We analyze the features involved in the decision and the\nthreshold bifurcation values of the decision paths of tree structures and\npresent this information to the users in natural language. This is the first\nwork on automatic analysis of legal texts combining NLP and ML along with\nExplainable Artificial Intelligence techniques to automatically make the\nmodels' decisions understandable to end users. Furthermore, legal experts have\nvalidated our solution, and this knowledge has also been incorporated into the\nexplanation process as \"expert-in-the-loop\" dictionaries. Experimental results\non an annotated data set in law categories by jurisdiction demonstrate that our\nsystem yields competitive classification performance, with accuracy values well\nabove 90%, and that its automatic explanations are easily understandable even\nto non-expert users.",
      "tldr_zh": "本研究提出了一种结合 Natural Language Processing (NLP) 和 Machine Learning (ML) 的系统，用于可解释地分类西班牙法律判决书，并根据管辖区依赖的法律类别进行分析。该系统利用 tree estimators 分析决策路径中的特征和阈值 bifurcation values，并以自然语言形式呈现解释，从而提升模型的可信度和透明度。作为创新，该工作首次整合 Explainable Artificial Intelligence (XAI) 技术，并通过 \"expert-in-the-loop\" 字典纳入法律专家的知识反馈。在实验中，该系统在注释数据集上实现了超过90%的准确率，且解释对非专家用户也易于理解。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.00437v1",
      "published_date": "2024-03-30 17:59:43 UTC",
      "updated_date": "2024-03-30 17:59:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:18:09.885735"
    },
    {
      "arxiv_id": "2404.00424v2",
      "title": "Quantformer: from attention to profit with a quantitative transformer trading strategy",
      "title_zh": "翻译失败",
      "authors": [
        "Zhaofeng Zhang",
        "Banghao Chen",
        "Shengxin Zhu",
        "Nicolas Langrené"
      ],
      "abstract": "In traditional quantitative trading practice, navigating the complicated and\ndynamic financial market presents a persistent challenge. Fully capturing\nvarious market variables, including long-term information, as well as essential\nsignals that may lead to profit remains a difficult task for learning\nalgorithms. In order to tackle this challenge, this paper introduces\nquantformer, an enhanced neural network architecture based on transformers, to\nbuild investment factors. By transfer learning from sentiment analysis,\nquantformer not only exploits its original inherent advantages in capturing\nlong-range dependencies and modeling complex data relationships, but is also\nable to solve tasks with numerical inputs and accurately forecast future\nreturns over a given period. This work collects more than 5,000,000 rolling\ndata of 4,601 stocks in the Chinese capital market from 2010 to 2019. The\nresults of this study demonstrated the model's superior performance in\npredicting stock trends compared with other 100 factor-based quantitative\nstrategies. Notably, the model's innovative use of transformer-liked model to\nestablish factors, in conjunction with market sentiment information, has been\nshown to enhance the accuracy of trading signals significantly, thereby\noffering promising implications for the future of quantitative trading\nstrategies.",
      "tldr_zh": "本文提出 Quantformer，一种基于 Transformer 的增强神经网络架构，用于构建量化交易策略，通过从情感分析的迁移学习捕捉长期依赖关系和复杂数据交互，从而处理数字输入并准确预测股票未来回报。研究利用2010年至2019年中国资本市场超过5,000,000条滚动数据（涉及4,601支股票），证明Quantformer在预测股票趋势方面比其他100个基于因素的量化策略表现优越。创新地将Transformer-like模型与市场情绪信息结合，显著提升了交易信号的准确性，为量化交易提供新颖的策略启示。",
      "categories": [
        "q-fin.MF",
        "cs.AI",
        "cs.CE",
        "G.3; J.2"
      ],
      "primary_category": "q-fin.MF",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.00424v2",
      "published_date": "2024-03-30 17:18:00 UTC",
      "updated_date": "2024-10-23 04:27:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:18:21.572040"
    },
    {
      "arxiv_id": "2404.00417v1",
      "title": "Orchestrate Latent Expertise: Advancing Online Continual Learning with Multi-Level Supervision and Reverse Self-Distillation",
      "title_zh": "编排潜在专长：通过多级监督和",
      "authors": [
        "HongWei Yan",
        "Liyuan Wang",
        "Kaisheng Ma",
        "Yi Zhong"
      ],
      "abstract": "To accommodate real-world dynamics, artificial intelligence systems need to\ncope with sequentially arriving content in an online manner. Beyond regular\nContinual Learning (CL) attempting to address catastrophic forgetting with\noffline training of each task, Online Continual Learning (OCL) is a more\nchallenging yet realistic setting that performs CL in a one-pass data stream.\nCurrent OCL methods primarily rely on memory replay of old training samples.\nHowever, a notable gap from CL to OCL stems from the additional\noverfitting-underfitting dilemma associated with the use of rehearsal buffers:\nthe inadequate learning of new training samples (underfitting) and the repeated\nlearning of a few old training samples (overfitting). To this end, we introduce\na novel approach, Multi-level Online Sequential Experts (MOSE), which\ncultivates the model as stacked sub-experts, integrating multi-level\nsupervision and reverse self-distillation. Supervision signals across multiple\nstages facilitate appropriate convergence of the new task while gathering\nvarious strengths from experts by knowledge distillation mitigates the\nperformance decline of old tasks. MOSE demonstrates remarkable efficacy in\nlearning new samples and preserving past knowledge through multi-level experts,\nthereby significantly advancing OCL performance over state-of-the-art baselines\n(e.g., up to 7.3% on Split CIFAR-100 and 6.1% on Split Tiny-ImageNet).",
      "tldr_zh": "该研究针对在线持续学习（Online Continual Learning, OCL）中的过拟合和欠拟合问题，提出了一种新型框架Multi-level Online Sequential Experts (MOSE)，通过将模型构建为堆叠的子专家，并整合多级监督（multi-level supervision）和反向自蒸馏（reverse self-distillation）来优化新任务学习和旧知识保留。MOSE方法利用跨阶段的监督信号促进新任务的适当收敛，同时通过知识蒸馏从多个专家中提取优势，缓解旧任务性能下降。在实验中，MOSE在Split CIFAR-100数据集上比现有基线提高了7.3%，在Split Tiny-ImageNet上提高了6.1%，显著提升了OCL的整体性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "CVPR 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.00417v1",
      "published_date": "2024-03-30 16:53:10 UTC",
      "updated_date": "2024-03-30 16:53:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:18:35.464807"
    },
    {
      "arxiv_id": "2404.00413v1",
      "title": "Language Models are Spacecraft Operators",
      "title_zh": "语言模型是航天器操作员",
      "authors": [
        "Victor Rodriguez-Fernandez",
        "Alejandro Carrasco",
        "Jason Cheng",
        "Eli Scharf",
        "Peng Mun Siew",
        "Richard Linares"
      ],
      "abstract": "Recent trends are emerging in the use of Large Language Models (LLMs) as\nautonomous agents that take actions based on the content of the user text\nprompts. We intend to apply these concepts to the field of Guidance,\nNavigation, and Control in space, enabling LLMs to have a significant role in\nthe decision-making process for autonomous satellite operations. As a first\nstep towards this goal, we have developed a pure LLM-based solution for the\nKerbal Space Program Differential Games (KSPDG) challenge, a public software\ndesign competition where participants create autonomous agents for maneuvering\nsatellites involved in non-cooperative space operations, running on the KSP\ngame engine. Our approach leverages prompt engineering, few-shot prompting, and\nfine-tuning techniques to create an effective LLM-based agent that ranked 2nd\nin the competition. To the best of our knowledge, this work pioneers the\nintegration of LLM agents into space research. Code is available at\nhttps://github.com/ARCLab-MIT/kspdg.",
      "tldr_zh": "该论文探讨了使用 Large Language Models (LLMs) 作为自主代理在航天引导、导航和控制 (Guidance, Navigation, and Control) 领域的应用，旨在增强卫星操作决策。作者通过 prompt engineering、few-shot prompting 和 fine-tuning 技术，开发了一个纯 LLM-based 解决方案，应用于 Kerbal Space Program Differential Games (KSPDG) 挑战，并在竞赛中排名第二。研究首次将 LLM 代理整合到空间研究中，为非合作空间操作提供了新颖的决策框架。",
      "categories": [
        "physics.space-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "physics.space-ph",
      "comment": "Source code available on Github at:\n  https://github.com/ARCLab-MIT/kspdg",
      "pdf_url": "http://arxiv.org/pdf/2404.00413v1",
      "published_date": "2024-03-30 16:43:59 UTC",
      "updated_date": "2024-03-30 16:43:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:18:46.551316"
    },
    {
      "arxiv_id": "2404.00406v1",
      "title": "TACO -- Twitter Arguments from COnversations",
      "title_zh": "翻译失败",
      "authors": [
        "Marc Feger",
        "Stefan Dietze"
      ],
      "abstract": "Twitter has emerged as a global hub for engaging in online conversations and\nas a research corpus for various disciplines that have recognized the\nsignificance of its user-generated content. Argument mining is an important\nanalytical task for processing and understanding online discourse.\nSpecifically, it aims to identify the structural elements of arguments, denoted\nas information and inference. These elements, however, are not static and may\nrequire context within the conversation they are in, yet there is a lack of\ndata and annotation frameworks addressing this dynamic aspect on Twitter. We\ncontribute TACO, the first dataset of Twitter Arguments utilizing 1,814 tweets\ncovering 200 entire conversations spanning six heterogeneous topics annotated\nwith an agreement of 0.718 Krippendorff's alpha among six experts. Second, we\nprovide our annotation framework, incorporating definitions from the Cambridge\nDictionary, to define and identify argument components on Twitter. Our\ntransformer-based classifier achieves an 85.06\\% macro F1 baseline score in\ndetecting arguments. Moreover, our data reveals that Twitter users tend to\nengage in discussions involving informed inferences and information. TACO\nserves multiple purposes, such as training tweet classifiers to manage tweets\nbased on inference and information elements, while also providing valuable\ninsights into the conversational reply patterns of tweets.",
      "tldr_zh": "本研究引入了TACO数据集，这是首个针对Twitter对话的论证挖掘(Argument mining)资源，包含1,814条推文和200个完整对话，覆盖六种异质主题，由六位专家标注并达到0.718 Krippendorff's alpha的一致性。研究提供了一个基于Cambridge Dictionary的标注框架，用于定义和识别Twitter上的论证组件，包括信息(information)和推理(inference)元素。使用transformer-based classifier，该模型在检测论证任务上实现了85.06% macro F1的基线性能，并揭示Twitter用户倾向于参与知情推理和信息相关的讨论，从而为训练推文分类器和管理在线话语提供宝贵洞见。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.00406v1",
      "published_date": "2024-03-30 16:14:46 UTC",
      "updated_date": "2024-03-30 16:14:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:18:58.497924"
    },
    {
      "arxiv_id": "2404.00399v3",
      "title": "Aurora-M: Open Source Continual Pre-training for Multilingual Language and Code",
      "title_zh": "翻译失败",
      "authors": [
        "Taishi Nakamura",
        "Mayank Mishra",
        "Simone Tedeschi",
        "Yekun Chai",
        "Jason T Stillerman",
        "Felix Friedrich",
        "Prateek Yadav",
        "Tanmay Laud",
        "Vu Minh Chien",
        "Terry Yue Zhuo",
        "Diganta Misra",
        "Ben Bogin",
        "Xuan-Son Vu",
        "Marzena Karpinska",
        "Arnav Varma Dantuluri",
        "Wojciech Kusa",
        "Tommaso Furlanello",
        "Rio Yokota",
        "Niklas Muennighoff",
        "Suhas Pai",
        "Tosin Adewumi",
        "Veronika Laippala",
        "Xiaozhe Yao",
        "Adalberto Junior",
        "Alpay Ariyak",
        "Aleksandr Drozd",
        "Jordan Clive",
        "Kshitij Gupta",
        "Liangyu Chen",
        "Qi Sun",
        "Ken Tsui",
        "Noah Persaud",
        "Nour Fahmy",
        "Tianlong Chen",
        "Mohit Bansal",
        "Nicolo Monti",
        "Tai Dang",
        "Ziyang Luo",
        "Tien-Tung Bui",
        "Roberto Navigli",
        "Virendra Mehta",
        "Matthew Blumberg",
        "Victor May",
        "Huu Nguyen",
        "Sampo Pyysalo"
      ],
      "abstract": "Pretrained language models are an integral part of AI applications, but their\nhigh computational cost for training limits accessibility. Initiatives such as\nBloom and StarCoder aim to democratize access to pretrained models for\ncollaborative community development. Despite these efforts, such models\nencounter challenges such as limited multilingual capabilities, risks of\ncatastrophic forgetting during continual pretraining, and the high costs of\ntraining models from scratch, alongside the need to align with AI safety\nstandards and regulatory frameworks.\n  This paper presents Aurora-M, a 15B parameter multilingual open-source model\ntrained on English, Finnish, Hindi, Japanese, Vietnamese, and code. Continually\npretrained from StarCoderPlus on 435B additional tokens, Aurora-M surpasses 2T\ntokens in total training token count. It is the first open-source multilingual\nmodel fine-tuned on human-reviewed safety instructions, thus aligning its\ndevelopment not only with conventional red-teaming considerations, but also\nwith the specific concerns articulated in the Biden-Harris Executive Order on\nthe Safe, Secure, and Trustworthy Development and Use of Artificial\nIntelligence.\n  We evaluate Aurora-M across a wide range of tasks and languages, showcasing\nits robustness against catastrophic forgetting and its superior performance in\nmultilingual settings, particularly in safety evaluations. We open-source\nAurora-M and its variants to encourage responsible open-source development of\nlarge language models at https://huggingface.co/aurora-m.",
      "tldr_zh": "本论文介绍了 Aurora-M，这是一个 15B 参数的开源多语言模型，支持英语、芬兰语、印地语、日语、越南语和代码，通过 Continual Pre-training 从 StarCoderPlus 基础上训练了 435B 额外标记，总计超过 2T 标记。Aurora-M 首次在开源模型中融入人类审阅的安全指令微调，以符合 Biden-Harris 行政命令的安全标准，避免了灾难性遗忘，并在多语言任务和安全评估中表现出色。作者开源了 Aurora-M 及其变体，以促进负责任的大型语言模型社区开发。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Preprint",
      "pdf_url": "http://arxiv.org/pdf/2404.00399v3",
      "published_date": "2024-03-30 15:38:54 UTC",
      "updated_date": "2024-12-27 03:53:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:19:11.291937"
    },
    {
      "arxiv_id": "2404.00385v1",
      "title": "Constrained Layout Generation with Factor Graphs",
      "title_zh": "翻译失败",
      "authors": [
        "Mohammed Haroon Dupty",
        "Yanfei Dong",
        "Sicong Leng",
        "Guoji Fu",
        "Yong Liang Goh",
        "Wei Lu",
        "Wee Sun Lee"
      ],
      "abstract": "This paper addresses the challenge of object-centric layout generation under\nspatial constraints, seen in multiple domains including floorplan design\nprocess. The design process typically involves specifying a set of spatial\nconstraints that include object attributes like size and inter-object relations\nsuch as relative positioning. Existing works, which typically represent objects\nas single nodes, lack the granularity to accurately model complex interactions\nbetween objects. For instance, often only certain parts of an object, like a\nroom's right wall, interact with adjacent objects. To address this gap, we\nintroduce a factor graph based approach with four latent variable nodes for\neach room, and a factor node for each constraint. The factor nodes represent\ndependencies among the variables to which they are connected, effectively\ncapturing constraints that are potentially of a higher order. We then develop\nmessage-passing on the bipartite graph, forming a factor graph neural network\nthat is trained to produce a floorplan that aligns with the desired\nrequirements. Our approach is simple and generates layouts faithful to the user\nrequirements, demonstrated by a large improvement in IOU scores over existing\nmethods. Additionally, our approach, being inferential and accurate, is\nwell-suited to the practical human-in-the-loop design process where\nspecifications evolve iteratively, offering a practical and powerful tool for\nAI-guided design.",
      "tldr_zh": "这篇论文针对对象中心布局生成中的空间约束问题（如楼层设计），提出了基于 factor graphs 的方法，每个对象（如房间）使用四个潜在变量节点和一个因子节点来精确捕捉对象间的高阶交互和依赖关系。作者开发了 factor graph neural network，通过 message-passing 在二分图上进行训练，从而生成符合用户要求的布局。实验结果显示，该方法在 IOU scores 上比现有方法大幅提升，并适用于迭代的人类在循环设计过程，提供了一个简单且实用的 AI 引导工具。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "To be published at IEEE/CVF CVPR 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.00385v1",
      "published_date": "2024-03-30 14:58:40 UTC",
      "updated_date": "2024-03-30 14:58:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:19:22.639057"
    },
    {
      "arxiv_id": "2404.00383v1",
      "title": "SpikingJET: Enhancing Fault Injection for Fully and Convolutional Spiking Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Anil Bayram Gogebakan",
        "Enrico Magliano",
        "Alessio Carpegna",
        "Annachiara Ruospo",
        "Alessandro Savino",
        "Stefano Di Carlo"
      ],
      "abstract": "As artificial neural networks become increasingly integrated into\nsafety-critical systems such as autonomous vehicles, devices for medical\ndiagnosis, and industrial automation, ensuring their reliability in the face of\nrandom hardware faults becomes paramount. This paper introduces SpikingJET, a\nnovel fault injector designed specifically for fully connected and\nconvolutional Spiking Neural Networks (SNNs). Our work underscores the critical\nneed to evaluate the resilience of SNNs to hardware faults, considering their\ngrowing prominence in real-world applications. SpikingJET provides a\ncomprehensive platform for assessing the resilience of SNNs by inducing errors\nand injecting faults into critical components such as synaptic weights, neuron\nmodel parameters, internal states, and activation functions. This paper\ndemonstrates the effectiveness of Spiking-JET through extensive software-level\nexperiments on various SNN architectures, revealing insights into their\nvulnerability and resilience to hardware faults. Moreover, highlighting the\nimportance of fault resilience in SNNs contributes to the ongoing effort to\nenhance the reliability and safety of Neural Network (NN)-powered systems in\ndiverse domains.",
      "tldr_zh": "本文提出SpikingJET，一种新型故障注入工具，专门针对全连接和卷积Spiking Neural Networks (SNNs)，旨在评估这些网络在硬件故障下的弹性，以提升其在安全关键系统（如自动驾驶车辆和医疗诊断）中的可靠性。SpikingJET通过向SNNs的关键组件，包括突触权重、神经元模型参数、内部状态和激活函数注入故障，进行全面的软件级实验。实验结果揭示了SNNs的脆弱性和弹性特征，并强调了增强故障弹性的重要性，为神经网络驱动系统的安全性和可靠性提供了关键见解。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "I.2"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.00383v1",
      "published_date": "2024-03-30 14:51:01 UTC",
      "updated_date": "2024-03-30 14:51:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:19:37.524908"
    },
    {
      "arxiv_id": "2404.01336v3",
      "title": "FineFake: A Knowledge-Enriched Dataset for Fine-Grained Multi-Domain Fake News Detection",
      "title_zh": "FineFake: 一种知识丰富的细粒度多域假新闻检测数据集",
      "authors": [
        "Ziyi Zhou",
        "Xiaoming Zhang",
        "Litian Zhang",
        "Jiacheng Liu",
        "Senzhang Wang",
        "Zheng Liu",
        "Xi Zhang",
        "Chaozhuo Li",
        "Philip S. Yu"
      ],
      "abstract": "Existing benchmarks for fake news detection have significantly contributed to\nthe advancement of models in assessing the authenticity of news content.\nHowever, these benchmarks typically focus solely on news pertaining to a single\nsemantic topic or originating from a single platform, thereby failing to\ncapture the diversity of multi-domain news in real scenarios. In order to\nunderstand fake news across various domains, the external knowledge and\nfine-grained annotations are indispensable to provide precise evidence and\nuncover the diverse underlying strategies for fabrication, which are also\nignored by existing benchmarks. To address this gap, we introduce a novel\nmulti-domain knowledge-enhanced benchmark with fine-grained annotations, named\n\\textbf{FineFake}. FineFake encompasses 16,909 data samples spanning six\nsemantic topics and eight platforms. Each news item is enriched with\nmulti-modal content, potential social context, semi-manually verified common\nknowledge, and fine-grained annotations that surpass conventional binary\nlabels. Furthermore, we formulate three challenging tasks based on FineFake and\npropose a knowledge-enhanced domain adaptation network. Extensive experiments\nare conducted on FineFake under various scenarios, providing accurate and\nreliable benchmarks for future endeavors. The entire FineFake project is\npublicly accessible as an open-source repository at\n\\url{https://github.com/Accuser907/FineFake}.",
      "tldr_zh": "本研究针对现有假新闻检测基准的局限性（如仅限于单一主题或平台），引入了 FineFake 数据集，该数据集包含 16,909 个样本，覆盖六种语义主题和八个平台，并通过多模态内容、社会上下文、外部知识和细粒度标注（如超越二元标签的详细证据）进行丰富。FineFake 还制定了三个挑战任务，并提出一个 knowledge-enhanced domain adaptation network，以提升多领域假新闻检测的准确性和适应性。在各种场景下进行的广泛实验为未来研究提供了可靠的基准，且数据集已开源在 GitHub 上。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.01336v3",
      "published_date": "2024-03-30 14:39:09 UTC",
      "updated_date": "2024-10-15 12:40:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:19:48.107102"
    },
    {
      "arxiv_id": "2404.00369v1",
      "title": "Worker Robot Cooperation and Integration into the Manufacturing Workcell via the Holonic Control Architecture",
      "title_zh": "翻译失败",
      "authors": [
        "Ahmed R. Sadik",
        "Bodo Urban",
        "Omar Adel"
      ],
      "abstract": "Worker-Robot Cooperation is a new industrial trend, which aims to sum the\nadvantages of both the human and the industrial robot to afford a new\nintelligent manufacturing techniques. The cooperative manufacturing between the\nworker and the robot contains other elements such as the product parts and the\nmanufacturing tools. All these production elements must cooperate in one\nmanufacturing workcell to fulfill the production requirements. The\nmanufacturing control system is the mean to connect all these cooperative\nelements together in one body. This manufacturing control system is distributed\nand autonomous due to the nature of the cooperative workcell. Accordingly, this\narticle proposes the holonic control architecture as the manufacturing concept\nof the cooperative workcell. Furthermore, the article focuses on the\nfeasibility of this manufacturing concept, by applying it over a case study\nthat involves the cooperation between a dual-arm robot and a worker. During\nthis case study, the worker uses a variety of hand gestures to cooperate with\nthe robot to achieve the highest production flexibility",
      "tldr_zh": "这篇论文探讨了工人和机器人合作的工业趋势，旨在结合人类优势与工业机器人的能力来实现智能制造，通过 Holonic Control Architecture 作为分布式和自治的控制系统来整合制造工作单元中的工人、机器人、产品零件和工具。文章提出该架构作为合作工作单元的核心概念，确保所有元素高效协作。作者通过一个案例研究验证其可行性，该研究涉及工人使用手势与双臂机器人合作，显著提升了生产灵活性。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.MA",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.00369v1",
      "published_date": "2024-03-30 13:44:17 UTC",
      "updated_date": "2024-03-30 13:44:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:20:00.064216"
    },
    {
      "arxiv_id": "2404.00364v1",
      "title": "Accurate Cutting-point Estimation for Robotic Lychee Harvesting through Geometry-aware Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Gengming Zhang",
        "Hao Cao",
        "Kewei Hu",
        "Yaoqiang Pan",
        "Yuqin Deng",
        "Hongjun Wang",
        "Hanwen Kang"
      ],
      "abstract": "Accurately identifying lychee-picking points in unstructured orchard\nenvironments and obtaining their coordinate locations is critical to the\nsuccess of lychee-picking robots. However, traditional two-dimensional (2D)\nimage-based object detection methods often struggle due to the complex\ngeometric structures of branches, leaves and fruits, leading to incorrect\ndetermination of lychee picking points. In this study, we propose a\nFcaf3d-lychee network model specifically designed for the accurate localisation\nof lychee picking points. Point cloud data of lychee picking points in natural\nenvironments are acquired using Microsoft's Azure Kinect DK time-of-flight\n(TOF) camera through multi-view stitching. We augment the Fully Convolutional\nAnchor-Free 3D Object Detection (Fcaf3d) model with a\nsqueeze-and-excitation(SE) module, which exploits human visual attention\nmechanisms for improved feature extraction of lychee picking points. The\ntrained network model is evaluated on a test set of lychee-picking locations\nand achieves an impressive F1 score of 88.57%, significantly outperforming\nexisting models. Subsequent three-dimensional (3D) position detection of\npicking points in real lychee orchard environments yields high accuracy, even\nunder varying degrees of occlusion. Localisation errors of lychee picking\npoints are within 1.5 cm in all directions, demonstrating the robustness and\ngenerality of the model.",
      "tldr_zh": "本研究针对荔枝采摘机器人在杂乱果园环境中准确识别采摘点的问题，提出了一种基于几何感知学习的Fcaf3d-lychee网络模型，以克服传统2D图像检测方法的局限性。该模型利用Microsoft Azure Kinect DK TOF相机采集的多视图点云数据，并通过添加squeeze-and-excitation(SE)模块来模仿人类视觉注意力机制，提升荔枝采摘点的特征提取精度。在测试中，模型在荔枝采摘点数据集上实现了88.57%的F1分数，并在大规模遮挡条件下保持3D定位错误在1.5 cm以内，展示了其鲁棒性和实际应用潜力。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.00364v1",
      "published_date": "2024-03-30 13:34:54 UTC",
      "updated_date": "2024-03-30 13:34:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:20:12.391732"
    },
    {
      "arxiv_id": "2404.01335v2",
      "title": "Generative AI Models for Different Steps in Architectural Design: A Literature Review",
      "title_zh": "生成式 AI 模型在建筑设计不同步骤中的应用：文献综述",
      "authors": [
        "Chengyuan Li",
        "Tianyu Zhang",
        "Xusheng Du",
        "Ye Zhang",
        "Haoran Xie"
      ],
      "abstract": "Recent advances in generative artificial intelligence (AI) technologies have\nbeen significantly driven by models such as generative adversarial networks\n(GANs), variational autoencoders (VAEs), and denoising diffusion probabilistic\nmodels (DDPMs). Although architects recognize the potential of generative AI in\ndesign, personal barriers often restrict their access to the latest\ntechnological developments, thereby causing the application of generative AI in\narchitectural design to lag behind. Therefore, it is essential to comprehend\nthe principles and advancements of generative AI models and analyze their\nrelevance in architecture applications. This paper first provides an overview\nof generative AI technologies, with a focus on probabilistic diffusion models\n(DDPMs), 3D generative models, and foundation models, highlighting their recent\ndevelopments and main application scenarios. Then, the paper explains how the\nabovementioned models could be utilized in architecture. We subdivide the\narchitectural design process into six steps and review related research\nprojects in each step from 2020 to the present. Lastly, this paper discusses\npotential future directions for applying generative AI in the architectural\ndesign steps. This research can help architects quickly understand the\ndevelopment and latest progress of generative AI and contribute to the further\ndevelopment of intelligent architecture.",
      "tldr_zh": "这篇文献综述探讨了生成式 AI 模型（如 GANs、VAEs 和 DDPMs）在建筑设计过程中的应用，概述了这些技术的原理、最新进展，特别是 DDPMs、3D 生成模型和基础模型的核心发展及其应用场景。论文将建筑设计细分为六个步骤，并回顾了从 2020 年至今的相关研究项目，展示了这些模型如何提升设计效率并解决建筑师的访问障碍。最终，研究讨论了生成式 AI 在建筑设计中的潜在未来方向，有助于推动智能建筑的进一步发展。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "34 pages, 14 figures, accepted by Frontiers of Architectural Research",
      "pdf_url": "http://arxiv.org/pdf/2404.01335v2",
      "published_date": "2024-03-30 13:25:11 UTC",
      "updated_date": "2024-10-23 12:38:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:20:23.999934"
    },
    {
      "arxiv_id": "2404.00344v1",
      "title": "Can LLMs Master Math? Investigating Large Language Models on Math Stack Exchange",
      "title_zh": "翻译失败",
      "authors": [
        "Ankit Satpute",
        "Noah Giessing",
        "Andre Greiner-Petter",
        "Moritz Schubotz",
        "Olaf Teschke",
        "Akiko Aizawa",
        "Bela Gipp"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated exceptional capabilities in\nvarious natural language tasks, often achieving performances that surpass those\nof humans. Despite these advancements, the domain of mathematics presents a\ndistinctive challenge, primarily due to its specialized structure and the\nprecision it demands. In this study, we adopted a two-step approach for\ninvestigating the proficiency of LLMs in answering mathematical questions.\nFirst, we employ the most effective LLMs, as identified by their performance on\nmath question-answer benchmarks, to generate answers to 78 questions from the\nMath Stack Exchange (MSE). Second, a case analysis is conducted on the LLM that\nshowed the highest performance, focusing on the quality and accuracy of its\nanswers through manual evaluation. We found that GPT-4 performs best (nDCG of\n0.48 and P@10 of 0.37) amongst existing LLMs fine-tuned for answering\nmathematics questions and outperforms the current best approach on ArqMATH3\nTask1, considering P@10. Our Case analysis indicates that while the GPT-4 can\ngenerate relevant responses in certain instances, it does not consistently\nanswer all questions accurately. This paper explores the current limitations of\nLLMs in navigating complex mathematical problem-solving. Through case analysis,\nwe shed light on the gaps in LLM capabilities within mathematics, thereby\nsetting the stage for future research and advancements in AI-driven\nmathematical reasoning. We make our code and findings publicly available for\nresearch: \\url{https://github.com/gipplab/LLM-Investig-MathStackExchange}",
      "tldr_zh": "本研究调查了大型语言模型（LLMs）在数学领域的表现，采用两步方法：首先，使用表现最佳的LLMs生成对Math Stack Exchange (MSE)上78个问题的答案；其次，对GPT-4进行手动案例分析，评估其答案质量和准确性。结果显示，GPT-4在相关指标上表现出色（nDCG为0.48、P@10为0.37），并优于ArqMATH3 Task1的现有方法，但其响应并非始终准确。论文突出了LLMs在复杂数学推理中的局限性，并公开了代码以促进未来研究。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted for publication at the 47th International ACM SIGIR\n  Conference on Research and Development in Information Retrieval (SIGIR) July\n  14--18, 2024, Washington D.C.,USA",
      "pdf_url": "http://arxiv.org/pdf/2404.00344v1",
      "published_date": "2024-03-30 12:48:31 UTC",
      "updated_date": "2024-03-30 12:48:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:20:36.877765"
    },
    {
      "arxiv_id": "2404.00341v1",
      "title": "Ontology in Holonic Cooperative Manufacturing: A Solution to Share and Exchange the Knowledge",
      "title_zh": "翻译失败",
      "authors": [
        "Ahmed R. Sadik",
        "Bodo Urban"
      ],
      "abstract": "Cooperative manufacturing is a new trend in industry, which depends on the\nexistence of a collaborative robot. A collaborative robot is usually a\nlight-weight robot which is capable of operating safely with a human co-worker\nin a shared work environment. During this cooperation, a vast amount of\ninformation is exchanged between the collaborative robot and the worker. This\ninformation constructs the cooperative manufacturing knowledge, which describes\nthe production components and environment. In this research, we propose a\nholonic control solution, which uses the ontology concept to represent the\ncooperative manufacturing knowledge. The holonic control solution is\nimplemented as an autonomous multi-agent system that exchanges the\nmanufacturing knowledge based on an ontology model. Ultimately, the research\nillustrates and implements the proposed solution over a cooperative assembly\nscenario, which involves two workers and one collaborative robot, whom\ncooperate together to assemble a customized product.",
      "tldr_zh": "本研究针对合作制造中知识分享和交换的挑战，提出了一种基于Ontology的Holonic控制解决方案，以表示生产组件和环境的合作制造知识。该解决方案通过一个自主multi-agent system实现，允许协作机器人与人类工人高效交换信息。在一个涉及两个工人和一个协作机器人的定制产品组装场景中，系统得到了实际演示，展示了其在提升合作效率方面的潜力。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.00341v1",
      "published_date": "2024-03-30 12:38:47 UTC",
      "updated_date": "2024-03-30 12:38:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:20:46.259873"
    },
    {
      "arxiv_id": "2404.00330v1",
      "title": "Memory-Scalable and Simplified Functional Map Learning",
      "title_zh": "内存可扩展且简化的功能映射学习",
      "authors": [
        "Robin Magnet",
        "Maks Ovsjanikov"
      ],
      "abstract": "Deep functional maps have emerged in recent years as a prominent\nlearning-based framework for non-rigid shape matching problems. While early\nmethods in this domain only focused on learning in the functional domain, the\nlatest techniques have demonstrated that by promoting consistency between\nfunctional and pointwise maps leads to significant improvements in accuracy.\nUnfortunately, existing approaches rely heavily on the computation of large\ndense matrices arising from soft pointwise maps, which compromises their\nefficiency and scalability. To address this limitation, we introduce a novel\nmemory-scalable and efficient functional map learning pipeline. By leveraging\nthe specific structure of functional maps, we offer the possibility to achieve\nidentical results without ever storing the pointwise map in memory.\nFurthermore, based on the same approach, we present a differentiable map\nrefinement layer adapted from an existing axiomatic refinement algorithm.\nUnlike many functional map learning methods, which use this algorithm at a\npost-processing step, ours can be easily used at train time, enabling to\nenforce consistency between the refined and initial versions of the map. Our\nresulting approach is both simpler, more efficient and more numerically stable,\nby avoiding differentiation through a linear system, while achieving close to\nstate-of-the-art results in challenging scenarios.",
      "tldr_zh": "本文提出了一种内存可扩展且简化的功能映射（functional maps）学习方法，旨在解决现有非刚性形状匹配框架中依赖大型密集矩阵导致的效率和可扩展性问题。通过利用功能映射的特定结构，该方法避免在内存中存储点位映射（pointwise maps），并引入一个可微的映射精炼层，基于现有公理精炼算法以在训练时强制映射一致性。该方法比传统方法更简单、更高效且数值更稳定，并在 challenging scenarios 中实现了接近最先进的结果。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.00330v1",
      "published_date": "2024-03-30 12:01:04 UTC",
      "updated_date": "2024-03-30 12:01:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:21:01.136216"
    },
    {
      "arxiv_id": "2404.00320v2",
      "title": "Advancing Multimodal Data Fusion in Pain Recognition: A Strategy Leveraging Statistical Correlation and Human-Centered Perspectives",
      "title_zh": "在疼痛识别中推进多模态数据融合：一种利用统计相关性和以人为中心视角的策略",
      "authors": [
        "Xingrui Gu",
        "Zhixuan Wang",
        "Irisa Jin",
        "Zekun Wu"
      ],
      "abstract": "This research presents a novel multimodal data fusion methodology for pain\nbehavior recognition, integrating statistical correlation analysis with\nhuman-centered insights. Our approach introduces two key innovations: 1)\nintegrating data-driven statistical relevance weights into the fusion strategy\nto effectively utilize complementary information from heterogeneous modalities,\nand 2) incorporating human-centric movement characteristics into multimodal\nrepresentation learning for detailed modeling of pain behaviors. Validated\nacross various deep learning architectures, our method demonstrates superior\nperformance and broad applicability. We propose a customizable framework that\naligns each modality with a suitable classifier based on statistical\nsignificance, advancing personalized and effective multimodal fusion.\nFurthermore, our methodology provides explainable analysis of multimodal data,\ncontributing to interpretable and explainable AI in healthcare. By highlighting\nthe importance of data diversity and modality-specific representations, we\nenhance traditional fusion techniques and set new standards for recognizing\ncomplex pain behaviors. Our findings have significant implications for\npromoting patient-centered healthcare interventions and supporting explainable\nclinical decision-making.",
      "tldr_zh": "本研究提出了一种新颖的多模态数据融合方法，用于疼痛行为识别，通过整合统计相关性分析和人类中心视角来提升性能。主要创新包括：1) 将数据驱动的统计相关性权重融入融合策略，以有效利用异构模态的互补信息；2) 融入人类中心运动特征到多模态表示学习中，实现对疼痛行为的详细建模。该方法在各种深度学习架构上验证，显示出优越性能和广泛适用性，并提供可解释的多模态数据分析，推进可解释 AI 在医疗保健中的应用。通过强调数据多样性和模态特定表示，该框架有助于推动以患者为中心的干预措施和临床决策。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by AHRI 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.00320v2",
      "published_date": "2024-03-30 11:13:18 UTC",
      "updated_date": "2024-08-01 09:07:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:21:12.969721"
    },
    {
      "arxiv_id": "2404.00312v1",
      "title": "Bayesian Exploration of Pre-trained Models for Low-shot Image Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Yibo Miao",
        "Yu Lei",
        "Feng Zhou",
        "Zhijie Deng"
      ],
      "abstract": "Low-shot image classification is a fundamental task in computer vision, and\nthe emergence of large-scale vision-language models such as CLIP has greatly\nadvanced the forefront of research in this field. However, most existing\nCLIP-based methods lack the flexibility to effectively incorporate other\npre-trained models that encompass knowledge distinct from CLIP. To bridge the\ngap, this work proposes a simple and effective probabilistic model ensemble\nframework based on Gaussian processes, which have previously demonstrated\nremarkable efficacy in processing small data. We achieve the integration of\nprior knowledge by specifying the mean function with CLIP and the kernel\nfunction with an ensemble of deep kernels built upon various pre-trained\nmodels. By regressing the classification label directly, our framework enables\nanalytical inference, straightforward uncertainty quantification, and\nprincipled hyper-parameter tuning. Through extensive experiments on standard\nbenchmarks, we demonstrate that our method consistently outperforms competitive\nensemble baselines regarding predictive performance. Additionally, we assess\nthe robustness of our method and the quality of the yielded uncertainty\nestimates on out-of-distribution datasets. We also illustrate that our method,\ndespite relying on label regression, still enjoys superior model calibration\ncompared to most deterministic baselines.",
      "tldr_zh": "该研究针对低-shot 图像分类问题，提出了一种基于 Gaussian processes 的概率模型集成框架，以灵活整合 CLIP 和其他预训练模型，弥补现有方法的局限性。该框架通过指定均值函数为 CLIP，并使用基于各种预训练模型的 deep kernels 构建核函数，实现直接回归分类标签、分析推理、不确定性量化以及超参数调优。在标准基准实验中，该方法在预测性能上优于竞争基线，并在分布外数据集上展示出色的鲁棒性和不确定性估计质量，同时实现比大多数确定性基线更优的模型校准。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.00312v1",
      "published_date": "2024-03-30 10:25:28 UTC",
      "updated_date": "2024-03-30 10:25:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:21:24.539748"
    },
    {
      "arxiv_id": "2405.12986v2",
      "title": "A Novel Feature Map Enhancement Technique Integrating Residual CNN and Transformer for Alzheimer Diseases Diagnosis",
      "title_zh": "翻译失败",
      "authors": [
        "Saddam Hussain Khan"
      ],
      "abstract": "Alzheimer diseases (ADs) involves cognitive decline and abnormal brain\nprotein accumulation, necessitating timely diagnosis for effective treatment.\nTherefore, CAD systems leveraging deep learning advancements have demonstrated\nsuccess in AD detection but pose computational intricacies and the dataset\nminor contrast, structural, and texture variations. In this regard, a novel\nhybrid FME-Residual-HSCMT technique is introduced, comprised of residual CNN\nand Transformer concepts to capture global and local fine-grained AD analysis\nin MRI. This approach integrates three distinct elements: a novel CNN Meet\nTransformer (HSCMT), customized residual learning CNN, and a new Feature Map\nEnhancement (FME) strategy to learn diverse morphological, contrast, and\ntexture variations of ADs. The proposed HSCMT at the initial stage utilizes\nstem convolution blocks that are integrated with CMT blocks followed by\nsystematic homogenous and structural (HS) operations. The customized CMT block\nencapsulates each element with global contextual interactions through\nmulti-head attention and facilitates computational efficiency through\nlightweight. Moreover, inverse residual and stem CNN in customized CMT enables\neffective extraction of local texture information and handling vanishing\ngradients. Furthermore, in the FME strategy, residual CNN blocks utilize\nTL-based generated auxiliary and are combined with the proposed HSCMT channels\nat the target level to achieve diverse enriched feature space. Finally, diverse\nenhanced channels are fed into a novel spatial attention mechanism for optimal\npixel selection to reduce redundancy and discriminate minor contrast and\ntexture inter-class variation. The proposed achieves an F1-score (98.55%), an\naccuracy of 98.42% and a sensitivity of 98.50%, a precision of 98.60% on the\nstandard Kaggle dataset, and demonstrates outperformance existing ViTs and CNNs\nmethods.",
      "tldr_zh": "本论文提出了一种新型特征图增强技术FME-Residual-HSCMT，用于阿尔茨海默病（ADs）诊断，通过整合Residual CNN和Transformer捕捉MRI图像中的全局和局部细粒度特征。 该技术包括三个核心元素：HSCMT模块（结合stem convolution blocks、CMT blocks和同质结构操作以提升计算效率）、定制化Residual CNN（用于提取局部纹理信息并处理梯度消失问题），以及FME策略（通过结合辅助特征和空间注意力机制减少冗余并增强特征多样性）。 实验结果显示，该方法在Kaggle标准数据集上实现了F1-score 98.55%、准确率98.42%、敏感性98.50%和精确度98.60%，显著优于现有的ViTs和CNNs方法，为ADs诊断提供了更高效的计算机辅助系统。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "28 Pages, 11 Figures, 3 Tables",
      "pdf_url": "http://arxiv.org/pdf/2405.12986v2",
      "published_date": "2024-03-30 10:17:13 UTC",
      "updated_date": "2024-05-25 05:47:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:21:37.539883"
    },
    {
      "arxiv_id": "2404.00306v2",
      "title": "Leveraging Intelligent Recommender system as a first step resilience measure -- A data-driven supply chain disruption response framework",
      "title_zh": "利用智能推荐系统作为第一步弹性措施——一个数据驱动的供应链中断响应框架",
      "authors": [
        "Yang Hu"
      ],
      "abstract": "Interests in the value of digital technologies for its potential uses to\nincrease supply chain resilience (SCRes) are increasing in light to the\nindustry 4.0 and the global pandemic. Utilization of Recommender systems (RS)\nas a supply chain (SC) resilience measure is neglected although RS is a capable\ntool to enhance SC resilience from a reactive aspect. To address this problem,\nthis research proposed a novel data-driven supply chain disruption response\nframework based on the intelligent recommender system techniques and validated\nthe conceptual model through a practical use case. Results show that our\nframework can be implemented as an effective SC disruption mitigation measure\nin the very first response phrase and help SC participants get better reaction\nperformance after the SC disruption.",
      "tldr_zh": "该研究在 Industry 4.0 和全球大流行背景下，强调数字技术对供应链韧性(SCRes)的潜在价值，并指出 Recommender systems (RS) 作为响应工具被忽略。论文提出了一种新型数据驱动的供应链中断响应框架，利用智能 RS 技术来提供第一步韧性措施。实验结果通过实际案例验证，显示该框架能有效缓解中断影响，帮助供应链参与者提升反应性能。",
      "categories": [
        "cs.CE",
        "cs.AI"
      ],
      "primary_category": "cs.CE",
      "comment": "Manuscript submitted for WSC2024 Conference",
      "pdf_url": "http://arxiv.org/pdf/2404.00306v2",
      "published_date": "2024-03-30 10:07:02 UTC",
      "updated_date": "2024-05-07 16:09:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:21:49.340914"
    },
    {
      "arxiv_id": "2404.00285v1",
      "title": "Long-Tailed Recognition on Binary Networks by Calibrating A Pre-trained Model",
      "title_zh": "翻译失败",
      "authors": [
        "Jihun Kim",
        "Dahyun Kim",
        "Hyungrok Jung",
        "Taeil Oh",
        "Jonghyun Choi"
      ],
      "abstract": "Deploying deep models in real-world scenarios entails a number of challenges,\nincluding computational efficiency and real-world (e.g., long-tailed) data\ndistributions. We address the combined challenge of learning long-tailed\ndistributions using highly resource-efficient binary neural networks as\nbackbones. Specifically, we propose a calibrate-and-distill framework that uses\noff-the-shelf pretrained full-precision models trained on balanced datasets to\nuse as teachers for distillation when learning binary networks on long-tailed\ndatasets. To better generalize to various datasets, we further propose a novel\nadversarial balancing among the terms in the objective function and an\nefficient multiresolution learning scheme. We conducted the largest empirical\nstudy in the literature using 15 datasets, including newly derived long-tailed\ndatasets from existing balanced datasets, and show that our proposed method\noutperforms prior art by large margins (>14.33% on average).",
      "tldr_zh": "该论文解决了在长尾分布数据上部署高效二进制神经网络（binary neural networks）的挑战，提出了一种 calibrate-and-distill 框架，利用预训练的全精度模型作为教师来指导长尾数据集上的网络训练。为了提升泛化能力，论文引入了对抗平衡（adversarial balancing）和高效多分辨率学习（multiresolution learning）方案。在15个数据集上的大规模实证研究中，该方法平均比现有技术提高了14.33%的性能表现，为计算高效的深度学习应用提供了新途径。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.00285v1",
      "published_date": "2024-03-30 08:37:19 UTC",
      "updated_date": "2024-03-30 08:37:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:22:01.378709"
    },
    {
      "arxiv_id": "2404.00282v3",
      "title": "Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods",
      "title_zh": "翻译失败",
      "authors": [
        "Yuji Cao",
        "Huan Zhao",
        "Yuheng Cheng",
        "Ting Shu",
        "Yue Chen",
        "Guolong Liu",
        "Gaoqi Liang",
        "Junhua Zhao",
        "Jinyue Yan",
        "Yun Li"
      ],
      "abstract": "With extensive pre-trained knowledge and high-level general capabilities,\nlarge language models (LLMs) emerge as a promising avenue to augment\nreinforcement learning (RL) in aspects such as multi-task learning, sample\nefficiency, and high-level task planning. In this survey, we provide a\ncomprehensive review of the existing literature in LLM-enhanced RL and\nsummarize its characteristics compared to conventional RL methods, aiming to\nclarify the research scope and directions for future studies. Utilizing the\nclassical agent-environment interaction paradigm, we propose a structured\ntaxonomy to systematically categorize LLMs' functionalities in RL, including\nfour roles: information processor, reward designer, decision-maker, and\ngenerator. For each role, we summarize the methodologies, analyze the specific\nRL challenges that are mitigated, and provide insights into future directions.\nLastly, a comparative analysis of each role, potential applications,\nprospective opportunities, and challenges of the LLM-enhanced RL are discussed.\nBy proposing this taxonomy, we aim to provide a framework for researchers to\neffectively leverage LLMs in the RL field, potentially accelerating RL\napplications in complex applications such as robotics, autonomous driving, and\nenergy systems.",
      "tldr_zh": "这篇调查论文回顾了大型语言模型 (LLMs) 如何增强强化学习 (RL)，重点探讨 LLMs 在多任务学习、样本效率和高层次任务规划方面的潜力。论文提出一个基于 agent-environment 交互范式的结构化分类法，将 LLMs 的功能分为四个角色：信息处理器、奖励设计师、决策者和生成器，并针对每个角色总结方法、分析缓解的 RL 挑战（如知识缺口和决策复杂性）。最后，通过比较分析这些角色，论文讨论了 LLMs 增强 RL 的潜在应用（如机器人、自动驾驶和能源系统）、未来机会以及面临的挑战。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "22 pages (including bibliography), 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2404.00282v3",
      "published_date": "2024-03-30 08:28:08 UTC",
      "updated_date": "2024-10-30 02:22:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:22:14.953684"
    },
    {
      "arxiv_id": "2404.00276v4",
      "title": "Instruction-Driven Game Engines on Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Hongqiu Wu",
        "Yan Wang",
        "Xingyuan Liu",
        "Hai Zhao",
        "Min Zhang"
      ],
      "abstract": "The Instruction-Driven Game Engine (IDGE) project aims to democratize game\ndevelopment by enabling a large language model (LLM) to follow free-form game\nrules and autonomously generate game-play processes. The IDGE allows users to\ncreate games by issuing simple natural language instructions, which\nsignificantly lowers the barrier for game development. We approach the learning\nprocess for IDGEs as a Next State Prediction task, wherein the model\nautoregressively predicts in-game states given player actions. It is a\nchallenging task because the computation of in-game states must be precise;\notherwise, slight errors could disrupt the game-play. To address this, we train\nthe IDGE in a curriculum manner that progressively increases the model's\nexposure to complex scenarios. Our initial progress lies in developing an IDGE\nfor Poker, a universally cherished card game. The engine we've designed not\nonly supports a wide range of poker variants but also allows for high\ncustomization of rules through natural language inputs. Furthermore, it also\nfavors rapid prototyping of new games from minimal samples, proposing an\ninnovative paradigm in game development that relies on minimal prompt and data\nengineering. This work lays the groundwork for future advancements in\ninstruction-driven game creation, potentially transforming how games are\ndesigned and played.",
      "tldr_zh": "本文提出 Instruction-Driven Game Engine (IDGE)，一种基于大型语言模型 (LLM) 的框架，允许用户通过简单自然语言指令创建游戏，从而降低游戏开发门槛。IDGE 将游戏过程视为 Next State Prediction 任务，通过课程学习逐步训练模型以精确预测玩家动作后的游戏状态，避免错误干扰游戏流程。初步成果包括为扑克游戏开发功能齐全的引擎，支持多种变体和规则自定义，并实现快速原型化新游戏，仅需最小提示和数据工程。该工作为指令驱动游戏创建奠定基础，可能革新游戏设计和玩法方式。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.00276v4",
      "published_date": "2024-03-30 08:02:16 UTC",
      "updated_date": "2024-08-23 04:06:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:22:26.034642"
    },
    {
      "arxiv_id": "2404.00271v1",
      "title": "TG-NAS: Leveraging Zero-Cost Proxies with Transformer and Graph Convolution Networks for Efficient Neural Architecture Search",
      "title_zh": "翻译失败",
      "authors": [
        "Ye Qiao",
        "Haocheng Xu",
        "Sitao Huang"
      ],
      "abstract": "Neural architecture search (NAS) is an effective method for discovering new\nconvolutional neural network (CNN) architectures. However, existing approaches\noften require time-consuming training or intensive sampling and evaluations.\nZero-shot NAS aims to create training-free proxies for architecture performance\nprediction. However, existing proxies have suboptimal performance, and are\noften outperformed by simple metrics such as model parameter counts or the\nnumber of floating-point operations. Besides, existing model-based proxies\ncannot be generalized to new search spaces with unseen new types of operators\nwithout golden accuracy truth. A universally optimal proxy remains elusive. We\nintroduce TG-NAS, a novel model-based universal proxy that leverages a\ntransformer-based operator embedding generator and a graph convolution network\n(GCN) to predict architecture performance. This approach guides neural\narchitecture search across any given search space without the need of\nretraining. Distinct from other model-based predictor subroutines, TG-NAS\nitself acts as a zero-cost (ZC) proxy, guiding architecture search with\nadvantages in terms of data independence, cost-effectiveness, and consistency\nacross diverse search spaces. Our experiments showcase its advantages over\nexisting proxies across various NAS benchmarks, suggesting its potential as a\nfoundational element for efficient architecture search. TG-NAS achieves up to\n300X improvements in search efficiency compared to previous SOTA ZC proxy\nmethods. Notably, it discovers competitive models with 93.75% CIFAR-10 accuracy\non the NAS-Bench-201 space and 74.5% ImageNet top-1 accuracy on the DARTS\nspace.",
      "tldr_zh": "该研究针对神经架构搜索 (NAS) 的效率问题，提出 TG-NAS，一种基于 Transformer 和图卷积网络 (GCN) 的新型零成本 (Zero-Cost) 代理框架，用于预测架构性能，避免了传统方法的耗时训练和评估。TG-NAS 通过 Transformer 生成操作符嵌入，并结合 GCN 实现跨搜索空间的通用预测，无需重新训练，具有数据独立性和一致性优势。实验结果显示，TG-NAS 在各种 NAS 基准上比现有方法提升高达 300 倍搜索效率，并在 NAS-Bench-201 空间上发现的模型达到 CIFAR-10 93.75% 准确率，以及 DARTS 空间上 ImageNet 74.5% top-1 准确率。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.00271v1",
      "published_date": "2024-03-30 07:25:30 UTC",
      "updated_date": "2024-03-30 07:25:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:22:38.231819"
    },
    {
      "arxiv_id": "2404.00261v1",
      "title": "A Simple Yet Effective Approach for Diversified Session-Based Recommendation",
      "title_zh": "一种简单却有效的多样化会话-based 推荐方法",
      "authors": [
        "Qing Yin",
        "Hui Fang",
        "Zhu Sun",
        "Yew-Soon Ong"
      ],
      "abstract": "Session-based recommender systems (SBRSs) have become extremely popular in\nview of the core capability of capturing short-term and dynamic user\npreferences. However, most SBRSs primarily maximize recommendation accuracy but\nignore user minor preferences, thus leading to filter bubbles in the long run.\nOnly a handful of works, being devoted to improving diversity, depend on unique\nmodel designs and calibrated loss functions, which cannot be easily adapted to\nexisting accuracy-oriented SBRSs. It is thus worthwhile to come up with a\nsimple yet effective design that can be used as a plugin to facilitate existing\nSBRSs on generating a more diversified list in the meantime preserving the\nrecommendation accuracy. In this case, we propose an end-to-end framework\napplied for every existing representative (accuracy-oriented) SBRS, called\ndiversified category-aware attentive SBRS (DCA-SBRS), to boost the performance\non recommendation diversity. It consists of two novel designs: a model-agnostic\ndiversity-oriented loss function, and a non-invasive category-aware attention\nmechanism. Extensive experiments on three datasets showcase that our framework\nhelps existing SBRSs achieve extraordinary performance in terms of\nrecommendation diversity and comprehensive performance, without significantly\ndeteriorating recommendation accuracy compared to state-of-the-art\naccuracy-oriented SBRSs.",
      "tldr_zh": "本文提出了一种简单有效的框架——Diversified Category-Aware Attentive SBRS (DCA-SBRS)，旨在解决Session-Based Recommender Systems (SBRSs) 过度关注准确性而忽略用户次要偏好，导致过滤气泡的问题。该框架包括一个模型无关的多样性导向损失函数和一个非侵入性的类别感知注意力机制，可作为插件轻松整合到现有SBRSs中，以提升推荐多样性。实验结果显示，在三个数据集上，DCA-SBRS显著提高了推荐多样性和整体性能，同时未显著降低准确性。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.00261v1",
      "published_date": "2024-03-30 06:21:56 UTC",
      "updated_date": "2024-03-30 06:21:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:22:50.201910"
    },
    {
      "arxiv_id": "2404.00257v2",
      "title": "YOLOOC: YOLO-based Open-Class Incremental Object Detection with Novel Class Discovery",
      "title_zh": "翻译失败",
      "authors": [
        "Qian Wan",
        "Xiang Xiang",
        "Qinhao Zhou"
      ],
      "abstract": "Because of its use in practice, open-world object detection (OWOD) has gotten\na lot of attention recently. The challenge is how can a model detect novel\nclasses and then incrementally learn them without forgetting previously known\nclasses. Previous approaches hinge on strongly-supervised or weakly-supervised\nnovel-class data for novel-class detection, which may not apply to real\napplications. We construct a new benchmark that novel classes are only\nencountered at the inference stage. And we propose a new OWOD detector YOLOOC,\nbased on the YOLO architecture yet for the Open-Class setup. We introduce label\nsmoothing to prevent the detector from over-confidently mapping novel classes\nto known classes and to discover novel classes. Extensive experiments conducted\non our more realistic setup demonstrate the effectiveness of our method for\ndiscovering novel classes in our new benchmark.",
      "tldr_zh": "该研究针对开放世界物体检测(OWOD)的挑战，提出了一种新型检测器YOLOOC，基于YOLO架构，能够在推理阶段发现新类并实现增量学习，而无需依赖强监督或弱监督数据。YOLOOC引入标签平滑(label smoothing)技术，防止模型过度自信地将新类映射到已知类，从而有效识别和学习新类。实验在构建的新基准上证明，该方法显著提升了新类发现的性能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "Withdrawn because it was submitted without consent of the first\n  author. In addition, this submission has some errors",
      "pdf_url": "http://arxiv.org/pdf/2404.00257v2",
      "published_date": "2024-03-30 06:17:39 UTC",
      "updated_date": "2024-04-22 14:38:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:23:02.235097"
    },
    {
      "arxiv_id": "2404.00247v3",
      "title": "Facilitating Reinforcement Learning for Process Control Using Transfer Learning: Overview and Perspectives",
      "title_zh": "翻译失败",
      "authors": [
        "Runze Lin",
        "Junghui Chen",
        "Lei Xie",
        "Hongye Su"
      ],
      "abstract": "In the context of Industry 4.0 and smart manufacturing, the field of process\nindustry optimization and control is also undergoing a digital transformation.\nWith the rise of Deep Reinforcement Learning (DRL), its application in process\ncontrol has attracted widespread attention. However, the extremely low sample\nefficiency and the safety concerns caused by exploration in DRL hinder its\npractical implementation in industrial settings. Transfer learning offers an\neffective solution for DRL, enhancing its generalization and adaptability in\nmulti-mode control scenarios. This paper provides insights into the use of DRL\nfor process control from the perspective of transfer learning. We analyze the\nchallenges of applying DRL in the process industry and the necessity of\nintroducing transfer learning. Furthermore, recommendations and prospects are\nprovided for future research directions on how transfer learning can be\nintegrated with DRL to enhance process control. This paper aims to offer a set\nof promising, user-friendly, easy-to-implement, and scalable approaches to\nartificial intelligence-facilitated industrial control for scholars and\nengineers in the process industry.",
      "tldr_zh": "这篇论文从转移学习(Transfer Learning)的角度概述了深度强化学习(Deep Reinforcement Learning, DRL)在过程控制中的应用，强调了DRL在工业4.0背景下面临的样本效率低和探索安全问题。论文分析了这些挑战，并提出通过Transfer Learning增强DRL的泛化性和适应性，以适用于多模式控制场景。最终，它为未来研究提供推荐和展望，旨在为过程工业的学者和工程师提供一套有前景、用户友好、易于实施且可扩展的AI辅助工业控制方法。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.LG",
        "cs.SY"
      ],
      "primary_category": "eess.SY",
      "comment": "Chinese Control and Decision Conference (CCDC 2025), Oral, Regular\n  Paper & Asian Control Conference (ASCC 2024), Oral, Position Paper",
      "pdf_url": "http://arxiv.org/pdf/2404.00247v3",
      "published_date": "2024-03-30 04:58:59 UTC",
      "updated_date": "2025-04-22 13:05:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:23:14.348850"
    },
    {
      "arxiv_id": "2404.00246v1",
      "title": "Your Co-Workers Matter: Evaluating Collaborative Capabilities of Language Models in Blocks World",
      "title_zh": "翻译失败",
      "authors": [
        "Guande Wu",
        "Chen Zhao",
        "Claudio Silva",
        "He He"
      ],
      "abstract": "Language agents that interact with the world on their own have great\npotential for automating digital tasks. While large language model (LLM) agents\nhave made progress in understanding and executing tasks such as textual games\nand webpage control, many real-world tasks also require collaboration with\nhumans or other LLMs in equal roles, which involves intent understanding, task\ncoordination, and communication. To test LLM's ability to collaborate, we\ndesign a blocks-world environment, where two agents, each having unique goals\nand skills, build a target structure together. To complete the goals, they can\nact in the world and communicate in natural language. Under this environment,\nwe design increasingly challenging settings to evaluate different collaboration\nperspectives, from independent to more complex, dependent tasks. We further\nadopt chain-of-thought prompts that include intermediate reasoning steps to\nmodel the partner's state and identify and correct execution errors. Both\nhuman-machine and machine-machine experiments show that LLM agents have strong\ngrounding capacities, and our approach significantly improves the evaluation\nmetric.",
      "tldr_zh": "这篇论文评估了大型语言模型(LLM)在协作环境中的能力，通过设计一个blocks-world环境，让两个代理（每个拥有独特目标和技能）共同构建目标结构。研究者设置了从独立到复杂依赖的任务场景，采用chain-of-thought提示来模拟伙伴状态、意图理解、任务协调和错误纠正。实验结果表明，LLM代理表现出强的grounding能力，人机和机机协作均显著提高了评估指标。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.00246v1",
      "published_date": "2024-03-30 04:48:38 UTC",
      "updated_date": "2024-03-30 04:48:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:23:25.078739"
    },
    {
      "arxiv_id": "2404.00242v4",
      "title": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured LLM Inference",
      "title_zh": "翻译失败",
      "authors": [
        "Jinwei Yao",
        "Kaiqi Chen",
        "Kexun Zhang",
        "Jiaxuan You",
        "Binhang Yuan",
        "Zeke Wang",
        "Tao Lin"
      ],
      "abstract": "Large language models (LLMs) are increasingly employed for complex tasks that\nprocess multiple generation calls in a tree structure with shared prefixes of\ntokens, including few-shot prompting, multi-step reasoning, speculative\ndecoding, etc. However, existing inference systems for tree-based applications\nare inefficient due to improper partitioning of queries and KV cache during\nattention calculation. This leads to two main issues: (1) a lack of memory\naccess (IO) reuse for KV cache of shared prefixes, and (2) poor load\nbalancing.As a result, there is redundant KV cache IO between GPU global memory\nand shared memory, along with low GPU utilization. To address these challenges,\nwe propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient\nattention algorithm with prefix-aware and load-balanced KV cache partitions.\nDeFT reduces the number of read/write operations of KV cache during attention\ncalculation through KV-Guided Grouping, a method that avoids repeatedly loading\nKV cache of shared prefixes in attention computation. Additionally, we propose\nFlattened Tree KV Splitting, a mechanism that ensures even distribution of the\nKV cache across partitions with little computation redundancy, enhancing GPU\nutilization during attention computations. By reducing 73-99% KV cache IO and\nnearly 100% IO for partial results during attention calculation, DeFT achieves\nup to 2.23/3.59x speedup in the end-to-end/attention latency across three\npractical tree-based workloads compared to state-of-the-art attention\nalgorithms. Our code is available at https://github.com/LINs-lab/DeFT.",
      "tldr_zh": "该论文针对大语言模型(LLMs)在处理树结构任务（如few-shot prompting和多步推理）时的效率问题，提出DeFT算法，通过KV-Guided Grouping减少KV cache的读写操作，避免重复加载共享前缀，并采用Flattened Tree KV Splitting实现负载均衡和GPU利用率优化。DeFT显著降低了73-99%的KV cache IO和近100%的部分结果IO操作。实验结果显示，在三种树结构工作负载上，DeFT比现有注意力算法实现了高达2.23倍的端到端延迟和3.59倍的注意力计算速度提升。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Update DeFT-v4, accepted by ICLR'25\n  (https://openreview.net/forum?id=2c7pfOqu9k). Our code is available at\n  https://github.com/LINs-lab/DeFT",
      "pdf_url": "http://arxiv.org/pdf/2404.00242v4",
      "published_date": "2024-03-30 04:34:54 UTC",
      "updated_date": "2025-03-07 17:47:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:23:39.695890"
    },
    {
      "arxiv_id": "2404.00231v3",
      "title": "Attention-based Shape-Deformation Networks for Artifact-Free Geometry Reconstruction of Lumbar Spine from MR Images",
      "title_zh": "翻译失败",
      "authors": [
        "Linchen Qian",
        "Jiasong Chen",
        "Linhai Ma",
        "Timur Urakov",
        "Weiyong Gu",
        "Liang Liang"
      ],
      "abstract": "Lumbar disc degeneration, a progressive structural wear and tear of lumbar\nintervertebral disc, is regarded as an essential role on low back pain, a\nsignificant global health concern. Automated lumbar spine geometry\nreconstruction from MR images will enable fast measurement of medical\nparameters to evaluate the lumbar status, in order to determine a suitable\ntreatment. Existing image segmentation-based techniques often generate\nerroneous segments or unstructured point clouds, unsuitable for medical\nparameter measurement. In this work, we present $\\textit{UNet-DeformSA}$ and\n$\\textit{TransDeformer}$: novel attention-based deep neural networks that\nreconstruct the geometry of the lumbar spine with high spatial accuracy and\nmesh correspondence across patients, and we also present a variant of\n$\\textit{TransDeformer}$ for error estimation. Specially, we devise new\nattention modules with a new attention formula, which integrate image features\nand tokenized contour features to predict the displacements of the points on a\nshape template without the need for image segmentation. The deformed template\nreveals the lumbar spine geometry in an image. Experiment results show that our\nnetworks generate artifact-free geometry outputs, and the variant of\n$\\textit{TransDeformer}$ can predict the errors of a reconstructed geometry.\nOur code is available at https://github.com/linchenq/TransDeformer-Mesh.",
      "tldr_zh": "本文提出Attention-based Shape-Deformation Networks，包括UNet-DeformSA和TransDeformer两种模型，用于从MR图像中重建腰椎几何结构，以评估腰椎间盘退化并辅助治疗决策。不同于传统的图像分割方法，这些网络采用新的注意力模块，整合图像特征和标记化的轮廓特征，直接预测形状模板上的点位移，实现高空间准确性和跨患者网格对应性，同时无需生成错误段或无结构点云。实验结果显示，该方法产生无伪影的几何输出，并通过TransDeformer的变体实现重建错误的预测，代码已在GitHub开源。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.00231v3",
      "published_date": "2024-03-30 03:23:52 UTC",
      "updated_date": "2024-05-01 01:47:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:23:51.294979"
    },
    {
      "arxiv_id": "2404.00228v3",
      "title": "InfLoRA: Interference-Free Low-Rank Adaptation for Continual Learning",
      "title_zh": "InfLoRA：无",
      "authors": [
        "Yan-Shuo Liang",
        "Wu-Jun Li"
      ],
      "abstract": "Continual learning requires the model to learn multiple tasks sequentially.\nIn continual learning, the model should possess the ability to maintain its\nperformance on old tasks (stability) and the ability to adapt to new tasks\ncontinuously (plasticity). Recently, parameter-efficient fine-tuning (PEFT),\nwhich involves freezing a pre-trained model and injecting a small number of\nlearnable parameters to adapt to downstream tasks, has gained increasing\npopularity in continual learning. Although existing continual learning methods\nbased on PEFT have demonstrated superior performance compared to those not\nbased on PEFT, most of them do not consider how to eliminate the interference\nof the new task on the old tasks, which inhibits the model from making a good\ntrade-off between stability and plasticity. In this work, we propose a new PEFT\nmethod, called interference-free low-rank adaptation (InfLoRA), for continual\nlearning. InfLoRA injects a small number of parameters to reparameterize the\npre-trained weights and shows that fine-tuning these injected parameters is\nequivalent to fine-tuning the pre-trained weights within a subspace.\nFurthermore, InfLoRA designs this subspace to eliminate the interference of the\nnew task on the old tasks, making a good trade-off between stability and\nplasticity. Experimental results show that InfLoRA outperforms existing\nstate-of-the-art continual learning methods on multiple datasets.",
      "tldr_zh": "该论文针对持续学习（Continual Learning）中的挑战，提出了一种新的参数高效微调（PEFT）方法InfLoRA（Interference-Free Low-Rank Adaptation），旨在通过注入少量参数来重新参数化预训练权重，并证明其等价于在特定子空间内微调权重，从而消除新任务对旧任务的干扰，实现稳定性（维护旧任务性能）和可塑性（适应新任务）的平衡。InfLoRA的设计重点在于构建无干扰子空间，确保模型在连续任务学习中取得更好的权衡。实验结果显示，该方法在多个数据集上超过了现有最先进的方法，展示了其在持续学习领域的优越性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by the 2024 IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition (CVPR 2024)",
      "pdf_url": "http://arxiv.org/pdf/2404.00228v3",
      "published_date": "2024-03-30 03:16:37 UTC",
      "updated_date": "2024-04-03 07:15:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:24:02.929613"
    },
    {
      "arxiv_id": "2404.00216v2",
      "title": "Is Factuality Enhancement a Free Lunch For LLMs? Better Factuality Can Lead to Worse Context-Faithfulness",
      "title_zh": "翻译失败",
      "authors": [
        "Baolong Bi",
        "Shenghua Liu",
        "Yiwei Wang",
        "Lingrui Mei",
        "Junfeng Fang",
        "Hongcheng Gao",
        "Shiyu Ni",
        "Xueqi Cheng"
      ],
      "abstract": "As the modern tools of choice for text understanding and generation, large\nlanguage models (LLMs) are expected to accurately output answers by leveraging\nthe input context. This requires LLMs to possess both context-faithfulness and\nfactual accuracy. Extensive efforts have been made to enable better outputs\nfrom LLMs by mitigating hallucinations through factuality enhancement methods.\nHowever, they also pose risks of hindering context-faithfulness, as factuality\nenhancement can lead LLMs to become overly confident in their parametric\nknowledge, causing them to overlook the relevant input context. In this work,\nwe argue that current factuality enhancement methods can significantly\nundermine the context-faithfulness of LLMs. We first revisit the current\nfactuality enhancement methods and evaluate their effectiveness in enhancing\nfactual accuracy. Next, we evaluate their performance on knowledge editing\ntasks to assess the potential impact on context-faithfulness. The experimental\nresults reveal that while these methods may yield inconsistent improvements in\nfactual accuracy, they also cause a more severe decline in\ncontext-faithfulness, with the largest decrease reaching a striking 69.7\\%. To\nexplain these declines, we analyze the hidden states and logit distributions\nfor the tokens representing new knowledge and parametric knowledge\nrespectively, highlighting the limitations of current approaches. Our finding\nhighlights the complex trade-offs inherent in enhancing LLMs. Therefore, we\nrecommend that more research on LLMs' factuality enhancement make efforts to\nreduce the sacrifice of context-faithfulness.",
      "tldr_zh": "本论文探讨了事实性增强方法对大型语言模型 (LLMs) 的影响，指出这些方法虽旨在提升事实准确性 (factual accuracy) 以减少幻觉 (hallucinations)，但可能导致模型过度依赖参数知识 (parametric knowledge)，从而严重损害上下文忠实性 (context-faithfulness)。研究者通过重新审视现有事实性增强方法，并评估其在知识编辑任务中的表现，发现这些方法对事实准确性的改善不一致，但会导致上下文忠实性下降高达 69.7%。通过分析隐藏状态和 logit 分布，论文揭示了这种权衡的潜在机制，并建议未来研究应优先减少对上下文忠实性的牺牲，以实现 LLMs 的平衡优化。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.00216v2",
      "published_date": "2024-03-30 02:08:28 UTC",
      "updated_date": "2024-10-04 03:30:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:24:15.636003"
    },
    {
      "arxiv_id": "2404.00207v1",
      "title": "Causal Inference for Human-Language Model Collaboration",
      "title_zh": "人类-语言模型协作的因果推断",
      "authors": [
        "Bohan Zhang",
        "Yixin Wang",
        "Paramveer S. Dhillon"
      ],
      "abstract": "In this paper, we examine the collaborative dynamics between humans and\nlanguage models (LMs), where the interactions typically involve LMs proposing\ntext segments and humans editing or responding to these proposals. Productive\nengagement with LMs in such scenarios necessitates that humans discern\neffective text-based interaction strategies, such as editing and response\nstyles, from historical human-LM interactions. This objective is inherently\ncausal, driven by the counterfactual `what-if' question: how would the outcome\nof collaboration change if humans employed a different text editing/refinement\nstrategy? A key challenge in answering this causal inference question is\nformulating an appropriate causal estimand: the conventional average treatment\neffect (ATE) estimand is inapplicable to text-based treatments due to their\nhigh dimensionality. To address this concern, we introduce a new causal\nestimand -- Incremental Stylistic Effect (ISE) -- which characterizes the\naverage impact of infinitesimally shifting a text towards a specific style,\nsuch as increasing formality. We establish the conditions for the\nnon-parametric identification of ISE. Building on this, we develop\nCausalCollab, an algorithm designed to estimate the ISE of various interaction\nstrategies in dynamic human-LM collaborations. Our empirical investigations\nacross three distinct human-LM collaboration scenarios reveal that CausalCollab\neffectively reduces confounding and significantly improves counterfactual\nestimation over a set of competitive baselines.",
      "tldr_zh": "本文探讨了人类与语言模型(LMs)的协作动态，通过因果推理分析文本编辑策略（如风格调整）对协作结果的影响。论文引入了新的因果估算量Incremental Stylistic Effect (ISE)，用于量化微小文本风格变化的平均影响，并建立了其非参数识别条件。基于此，开发了CausalCollab算法，该算法在三个人类-LM协作场景中有效减少混杂因素，并显著优于基线方法，提高了反事实估计的准确性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages (Accepted for publication at NAACL 2024 (Main Conference))",
      "pdf_url": "http://arxiv.org/pdf/2404.00207v1",
      "published_date": "2024-03-30 01:08:25 UTC",
      "updated_date": "2024-03-30 01:08:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T20:24:25.922440"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 45,
  "processed_papers_count": 45,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-17T20:24:46.527188"
}
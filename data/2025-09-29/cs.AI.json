{
  "date": "2025-09-29",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-09-29 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\nä»Šå¤©çš„ arXiv å‘ˆç°äº•å–·ä¹‹åŠ¿ï¼Œæœ€æ ¸å¿ƒçš„å…³æ³¨ç‚¹é›†ä¸­åœ¨ **Large Reasoning Models (LRMs)** çš„è¿›ä¸€æ­¥è¿›åŒ–ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•é€šè¿‡ **MCTS (è’™ç‰¹å¡æ´›æ ‘æœç´¢)** å’Œ **RLVR (å¸¦éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ )** çªç ´æ¨ç†ç“¶é¢ˆï¼›åŒæ—¶ï¼Œ**AI for Science** é¢†åŸŸè¯ç”Ÿäº†èƒ½æ‹¿ç‰©ç†å¥¥èµ›é‡‘ç‰Œçš„ Agent ç³»ç»Ÿï¼›è§†é¢‘ç”Ÿæˆé¢†åŸŸåˆ™è¿æ¥äº†é«˜æ•ˆçš„ **SANA-Video**ã€‚\n\n---\n\n### ğŸš€ æ·±åº¦æ¨ç†ä¸å¼ºåŒ–å­¦ä¹  (Reasoning & RLVR)\n\n**[æ·±åº¦æœç´¢ï¼šé€šè¿‡è’™ç‰¹å¡æ´›æ ‘æœç´¢çªç ´ RLVR ç“¶é¢ˆ] DeepSearch: Overcome the Bottleneck of Reinforcement Learning with Verifiable Rewards via Monte Carlo Tree Search**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šé’ˆå¯¹å½“å‰ RLVR (å¸¦éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ) åœ¨è®­ç»ƒåæœŸé­é‡çš„æ€§èƒ½å¹³å°æœŸé—®é¢˜ï¼Œæå‡ºå°† MCTS ç›´æ¥åµŒå…¥è®­ç»ƒå¾ªç¯çš„ **DeepSearch** æ¡†æ¶ã€‚\n*   **ä¸»è¦å‘ç°**ï¼šç°æœ‰çš„ RLVR æ¢ç´¢ç¨€ç–ï¼Œå®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚DeepSearch é€šè¿‡å…¨å±€å‰æ²¿é€‰æ‹© (Global Frontier Selection) å’Œç†µå¯¼å‘é€‰æ‹©ï¼Œå®ç°äº†ç³»ç»Ÿæ€§çš„æ¢ç´¢ã€‚åœ¨ 1.5B æ¨¡å‹ä¸Šï¼Œä»…ç”¨ 1/5.7 çš„ GPU æ—¶é—´å°±è¾¾åˆ°äº† SOTA æ€§èƒ½ï¼Œè¯æ˜äº†ç³»ç»Ÿæ€§æœç´¢ä¼˜äºå•çº¯çš„æš´åŠ›æ‰©å±•ã€‚\n\n**[ç‰©ç†å°é»„äººï¼šé€šè¿‡ååŒè¿›åŒ–å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨ç‰©ç†å¥¥èµ›ä¸­å¤ºé‡‘] PhysicsMinions: Winning Gold Medals in the Latest Physics Olympiads with a Coevolutionary Multimodal Multi-Agent System**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šæå‡ºäº†ä¸€ä¸ªåä¸º **PhysicsMinions** çš„ååŒè¿›åŒ–å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ŒåŒ…å« Visualï¼ˆè§†è§‰ï¼‰ã€Logicï¼ˆé€»è¾‘ï¼‰å’Œ Reviewï¼ˆå®¡æŸ¥ï¼‰ä¸‰ä¸ªå·¥ä½œå®¤ã€‚\n*   **ä¸»è¦å‘ç°**ï¼šè¯¥ç³»ç»Ÿåœ¨æœ€æ–°çš„å›½é™…ç‰©ç†å¥¥æ—åŒ¹å…‹ç«èµ› (IPhO) ä¸­è·å¾—äº†**å¼€æºæ¨¡å‹é¦–æšé‡‘ç‰Œ**ï¼ˆæŒ‰å¹³å‡åˆ†è®¡ï¼‰ï¼ŒPass@32 åˆ†æ•°è¾¾åˆ° 26.8/30ï¼Œè¿œè¶…å•ä¸€æ¨¡å‹çš„ 22.7 åˆ†ï¼Œå±•ç¤ºäº† AI åœ¨æé«˜éš¾åº¦å¤šæ¨¡æ€ç‰©ç†æ¨ç†ä¸Šçš„çªç ´ã€‚\n\n**[æ¢ç´¢åäº¿å‚æ•°ä»¥ä¸‹æ¨ç†æ¨¡å‹çš„æé™] MobileLLM-R1: Exploring the Limits of Sub-Billion Language Model Reasoners with Open Training Recipes**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šæŒ‘æˆ˜äº†â€œæ¨ç†èƒ½åŠ›éœ€è¦å·¨å¤§å‚æ•°é‡â€çš„å‡è®¾ã€‚Meta å›¢é˜Ÿé€šè¿‡ç²¾é€‰æ•°æ®ï¼ˆä»… ~2T tokenï¼‰ï¼Œè®­ç»ƒå‡ºäº† **MobileLLM-R1** ç³»åˆ—ã€‚\n*   **ä¸»è¦å‘ç°**ï¼šMobileLLM-R1-950M åœ¨ AIME ä¸Šçš„å¾—åˆ†è¿œè¶…åŒé‡çº§æ¨¡å‹ï¼ˆå¦‚ OLMo-2ï¼‰ï¼Œç”šè‡³åŒ¹é…äº† Qwen3-0.6Bï¼ˆåè€…ç”¨äº† 36T tokenï¼‰ã€‚è¯æ˜äº†æ•°æ®è´¨é‡å’Œé‡é‡‡æ ·ç­–ç•¥å¯¹äºå°æ¨¡å‹æ¨ç†æ¶Œç°çš„å…³é”®ä½œç”¨ã€‚\n\n**[AdvChainï¼šé’ˆå¯¹æ¨ç†æ¨¡å‹å®‰å…¨å¯¹é½çš„å¯¹æŠ—æ€§æ€ç»´é“¾å¾®è°ƒ] AdvChain: Adversarial Chain-of-Thought Tuning for Robust Safety Alignment of Large Reasoning Models**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šå‘ç° CoT æ¨ç†ä¸­çš„â€œé›ªçƒæ•ˆåº”â€ï¼ˆå¾®å°çš„æ¨ç†åå·®è¢«é€æ­¥æ”¾å¤§ï¼‰ã€‚æå‡ºäº† AdvChainï¼Œé€šè¿‡å¯¹æŠ—æ€§ CoT å¾®è°ƒæ•™æ¨¡å‹â€œè‡ªæˆ‘çº æ­£â€ã€‚\n*   **ä¸»è¦å‘ç°**ï¼šæ„å»ºäº†åŒ…å«â€œè¯±æƒ‘-çº æ­£â€å’Œâ€œçŠ¹è±«-çº æ­£â€æ ·æœ¬çš„æ•°æ®é›†ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹é˜²å¾¡ Jailbreak æ”»å‡»çš„èƒ½åŠ›ï¼ŒåŒæ—¶å‡å°‘äº†å¯¹æ­£å¸¸ prompt çš„è¿‡åº¦æ‹’ç»ã€‚\n\n---\n\n### ğŸ¥ è§†è§‰ç”Ÿæˆä¸å¤šæ¨¡æ€ (Vision Generation & MLLMs)\n\n**[SANA-Videoï¼šåŸºäºçº¿æ€§ DiT çš„é«˜æ•ˆè§†é¢‘ç”Ÿæˆ] SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šéŸ©æ¾å›¢é˜Ÿæ–°ä½œã€‚æå‡ºäº† **SANA-Video**ï¼Œåˆ©ç”¨çº¿æ€§æ³¨æ„åŠ›æœºåˆ¶ (Linear Attention) å’Œæ’å®šå†…å­˜ KV ç¼“å­˜ï¼Œè§£å†³äº†é•¿è§†é¢‘ç”Ÿæˆçš„æ•ˆç‡é—®é¢˜ã€‚\n*   **ä¸»è¦å‘ç°**ï¼šå¯ä»¥åœ¨ RTX 4090/5090 ä¸Šéƒ¨ç½²ï¼Œç”Ÿæˆ 720p åˆ†è¾¨ç‡ã€åˆ†é’Ÿçº§çš„é•¿è§†é¢‘ã€‚è®­ç»ƒæˆæœ¬ä»…ä¸º MovieGen çš„ 1%ï¼Œæ¨ç†é€Ÿåº¦æ¯” Wan-2.1 å¿« 16 å€ï¼Œæ”¯æŒ NVFP4 ç²¾åº¦åŠ é€Ÿã€‚\n\n**[LLaDA-MoEï¼šç¨€ç–æ··åˆä¸“å®¶æ‰©æ•£è¯­è¨€æ¨¡å‹] LLaDA-MoE: A Sparse MoE Diffusion Language Model**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šå°† MoE æ¶æ„å¼•å…¥æ‰©æ•£è¯­è¨€æ¨¡å‹ (Diffusion LM)ï¼Œä»å¤´è®­ç»ƒäº† 20T tokenã€‚\n*   **ä¸»è¦å‘ç°**ï¼š7B å‚æ•°æ¨¡å‹åœ¨æ¨ç†æ—¶ä»…æ¿€æ´» 1.4B å‚æ•°ï¼Œæ€§èƒ½å´è¶…è¶Šäº† LLaDA å’Œ Dream ç­‰å‰ä½œã€‚å…¶æŒ‡ä»¤å¾®è°ƒç‰ˆæœ¬åœ¨çŸ¥è¯†ç†è§£å’Œä»£ç ç”Ÿæˆä¸Šå¯åŒ¹æ•Œ Qwen2.5-3Bï¼Œè¯æ˜äº† MoE + Diffusion çš„æ½œåŠ›ã€‚\n\n**[VisualOverloadï¼šæ¢ç©¶ VLM åœ¨é«˜å¯†åº¦åœºæ™¯ä¸‹çš„è§†è§‰ç†è§£] VisualOverload: Probing Visual Understanding of VLMs in Really Dense Scenes**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šæå‡ºäº†ä¸€ä¸ªæ–°çš„ benchmarkï¼Œä½¿ç”¨é«˜åˆ†è¾¨ç‡ã€äººç‰©å¯†é›†çš„ç»˜ç”»ä½œå“æ¥æµ‹è¯• VLMã€‚\n*   **ä¸»è¦å‘ç°**ï¼šå½“å‰çš„ VLMï¼ˆåŒ…æ‹¬ o3ï¼‰åœ¨è¿™äº›å¯†é›†åœºæ™¯ä¸‹è¡¨ç°ç³Ÿç³•ï¼Œæœ€å¥½çš„æ¨¡å‹å‡†ç¡®ç‡ä»…ä¸º 19.6%ï¼ˆåœ¨æœ€éš¾çš„ split ä¸Šï¼‰ã€‚è¿™æ­ç¤ºäº† VLM åœ¨å¤„ç†â€œè¿‡è½½â€è§†è§‰ä¿¡æ¯æ—¶çš„ä¸¥é‡ç¼ºé™·ã€‚\n\n**[Vid-LLMï¼šç´§å‡‘çš„è§†é¢‘åŸº 3D å¤šæ¨¡æ€å¤§æ¨¡å‹] Vid-LLM: A Compact Video-based 3D Multimodal LLM with Reconstruction-Reasoning Synergy**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šæ— éœ€å¤–éƒ¨ 3D æ•°æ®ï¼Œç›´æ¥å¤„ç†è§†é¢‘è¾“å…¥è¿›è¡Œ 3D åœºæ™¯ç†è§£ã€‚\n*   **æ–¹æ³•**ï¼šè®¾è®¡äº†è·¨ä»»åŠ¡é€‚é…å™¨ (CTA) æ¥å¯¹é½ 3D å‡ ä½•å…ˆéªŒå’Œè§†è§‰è¯­è¨€è¡¨å¾ï¼Œå¹¶å¼•å…¥åº¦é‡æ·±åº¦æ¨¡å‹æ¢å¤çœŸå®å°ºåº¦å‡ ä½•ã€‚\n\n---\n\n### ğŸ¤– æ™ºèƒ½ä½“ä¸ä¸–ç•Œæ¨¡å‹ (Agents & World Models)\n\n**[Dreamer 4ï¼šåœ¨å¯æ‰©å±•ä¸–ç•Œæ¨¡å‹ä¸­è®­ç»ƒæ™ºèƒ½ä½“] Training Agents Inside of Scalable World Models**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šDanijar Hafner (Dreamer ç³»åˆ—ä½œè€…) æ–°ä½œã€‚æ¨å‡ºäº† **Dreamer 4**ï¼Œåœ¨ Minecraft ä¸­è¡¨ç°å‡ºè‰²ã€‚\n*   **ä¸»è¦å‘ç°**ï¼šè¿™æ˜¯é¦–ä¸ªä»…é€šè¿‡ç¦»çº¿æ•°æ®å­¦ä¹ å°±èƒ½åœ¨ Minecraft ä¸­è·å¾—é’»çŸ³çš„ Agentã€‚å…¶ä¸–ç•Œæ¨¡å‹èƒ½å®æ—¶æ¨ç†ï¼Œå¹¶å‡†ç¡®é¢„æµ‹å¤æ‚çš„æ¸¸æˆæœºåˆ¶ã€‚\n\n**[InfoAgentï¼šæ¨è¿›è‡ªä¸»ä¿¡æ¯æœå¯»æ™ºèƒ½ä½“] InfoAgent: Advancing Autonomous Information-Seeking Agents**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šæå‡ºäº† **InfoAgent**ï¼Œä¸€ä¸ªåŸºäº Qwen3-14B çš„æ·±åº¦ç ”ç©¶ Agentï¼Œé…å¤‡äº†è‡ªå»ºçš„æœç´¢åŸºç¡€è®¾æ–½ã€‚\n*   **æ–¹æ³•**ï¼šä½¿ç”¨äº†ä¸¤é˜¶æ®µè®­ç»ƒï¼ˆå†·å¯åŠ¨ SFT + å¼ºåŒ–å­¦ä¹ ï¼‰ï¼Œæ˜¾è‘—æå‡äº†é•¿ç¨‹æœç´¢å’Œæ¨ç†é©±åŠ¨çš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ã€‚åœ¨ BrowseComp ç­‰æ¦œå•ä¸Šè¶…è¶Šäº† WebSailor ç­‰å‰ä½œã€‚\n\n**[SimuHomeï¼šæ™ºèƒ½å®¶å±… LLM Agent çš„æ—¶ç©ºæ„ŸçŸ¥åŸºå‡†] SimuHome: A Temporal- and Environment-Aware Benchmark for Smart Home LLM Agents**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šæ„å»ºäº†ä¸€ä¸ªåŸºäº Matter åè®®çš„é«˜ä¿çœŸæ™ºèƒ½å®¶å±…æ¨¡æ‹Ÿç¯å¢ƒã€‚\n*   **ä¸»è¦å‘ç°**ï¼šè¯„ä¼°äº† 16 ä¸ª Agentï¼Œå‘ç°å³ä½¿æ˜¯ GPT-4.1 ä¹Ÿåœ¨å¤„ç†éšå¼æ„å›¾å’Œæ—¶é—´è°ƒåº¦ä¸Šè¡¨ç°æŒ£æ‰ã€‚æ¨ç†æ¨¡å‹ (Reasoning models) è¡¨ç°è¾ƒå¥½ä½†æ¨ç†å»¶è¿Ÿè¿‡é«˜ï¼Œéš¾ä»¥æ»¡è¶³å®æ—¶æ€§è¦æ±‚ã€‚\n\n---\n\n### ğŸ› ï¸ åŸºç¡€è®¾æ–½ä¸ä¼˜åŒ– (Infrastructure & Optimization)\n\n**[ä½¿ç”¨ NVFP4 é¢„è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹] Pretraining Large Language Models with NVFP4**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šNVIDIA å®˜æ–¹è®ºæ–‡ã€‚ä»‹ç»äº†ä½¿ç”¨ **NVFP4 (4-bit æµ®ç‚¹)** æ ¼å¼ç¨³å®šè®­ç»ƒ LLM çš„æ–¹æ³•ã€‚\n*   **ä¸»è¦å‘ç°**ï¼šè®­ç»ƒäº†ä¸€ä¸ª 12B æ¨¡å‹ï¼ˆ10T tokenï¼‰ï¼Œæ˜¯ç›®å‰å…¬å¼€çš„æœ€é•¿ 4-bit è®­ç»ƒè®°å½•ã€‚ç»“åˆéšæœº Hadamard å˜æ¢å’Œéšæœºèˆå…¥ï¼ŒNVFP4 è®­ç»ƒçš„æ¨¡å‹æ€§èƒ½ä¸ FP8 ç›¸å½“ï¼Œè¿™ä¸ºä¸‹ä¸€ä»£è¶…å¤§è§„æ¨¡æ¨¡å‹çš„é«˜æ•ˆè®­ç»ƒé“ºå¹³äº†é“è·¯ã€‚\n\n**[Condaï¼šåˆ—å½’ä¸€åŒ– Adam åŠ é€Ÿ LLM è®­ç»ƒ] Conda: Column-Normalized Adam for Training Large Language Models Faster**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šæå‡ºäº†ä¸€ç§æ–°çš„ä¼˜åŒ–å™¨ **Conda**ï¼Œç»“åˆäº† Adam çš„åæ ‡è‡ªé€‚åº”æ€§å’Œ Muon çš„è°±å½’ä¸€åŒ–ä¼˜åŠ¿ã€‚\n*   **ä¸»è¦å‘ç°**ï¼šåœ¨ LLaMA ç³»åˆ—è®­ç»ƒä¸­ï¼ŒConda çš„æ”¶æ•›é€Ÿåº¦æ˜¯ AdamW çš„ 2-2.5 å€ï¼Œä¸”åœ¨ä¸åŒè®¾ç½®ä¸‹å…·æœ‰å¾ˆå¼ºçš„é²æ£’æ€§ã€‚\n\n**[æ¨¡å‹åˆå¹¶çš„ç¼©æ”¾å®šå¾‹] Model Merging Scaling Laws in Large Language Models**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šé‡åŒ–äº†æ¨¡å‹åˆå¹¶ï¼ˆModel Mergingï¼‰çš„æ”¶ç›Šè§„å¾‹ã€‚\n*   **ä¸»è¦å‘ç°**ï¼šå‘ç°äº†ä¸€ä¸ªå¹‚å¾‹å…³ç³»ï¼Œè¿æ¥äº†æ¨¡å‹å¤§å°å’Œä¸“å®¶æ•°é‡ã€‚æ”¶ç›Šä¸»è¦é›†ä¸­åœ¨æ—©æœŸï¼ˆåŠ å…¥å°‘é‡ä¸“å®¶æ—¶ï¼‰ï¼Œä¸”éšç€ä¸“å®¶æ•°é‡å¢åŠ ï¼Œæ”¶ç›Šå‘ˆç°æ˜æ˜¾çš„è¾¹é™…é€’å‡ã€‚è¿™ä¸ºâ€œåˆå¹¶ä»£æ›¿å¤šä»»åŠ¡è®­ç»ƒâ€æä¾›äº†ç†è®ºæŒ‡å¯¼ã€‚\n\n---\n\n### ğŸ”¬ AI for Science å…¶ä»–äº®ç‚¹\n\n**[å¼•å¯¼æ‰©æ•£æ¨¡å‹å‘ç°æ–°å‹è¶…å¯¼ä½“] Guided Diffusion for the Discovery of New Superconductors**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šåˆ©ç”¨åœ¨ Alexandria æ•°æ®åº“é¢„è®­ç»ƒçš„ DiffCSP åŸºç¡€æ¨¡å‹ï¼Œç»“åˆåˆ†ç±»å™¨å¼•å¯¼ç”Ÿæˆï¼Œå‘ç°äº† 773 ä¸ª DFT è®¡ç®—ä¸´ç•Œæ¸©åº¦ $T_c > 5K$ çš„å€™é€‰ææ–™ã€‚\n\n**[è®©ç‰©ç†å¼•å¯¼è›‹ç™½è´¨æµåŠ¨] Let Physics Guide Your Protein Flows: Topology-aware Unfolding and Generation**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šå¼•å…¥äº†åŸºäºç»å…¸ç‰©ç†çš„éçº¿æ€§åŠ å™ªè¿‡ç¨‹ï¼ˆunfoldingï¼‰ï¼Œç»“åˆ SE(3) ä¸Šçš„ Flow Matchingï¼Œç”Ÿæˆäº†æ›´ç¬¦åˆç‰©ç†ç°å®å’Œæ‹“æ‰‘å®Œæ•´æ€§çš„è›‹ç™½è´¨ç»“æ„ã€‚",
  "papers": [
    {
      "arxiv_id": "2509.25601v1",
      "title": "Echoes of Humanity: Exploring the Perceived Humanness of AI Music",
      "title_zh": "äººæ€§çš„å›å“ï¼šæ¢ç©¶äººå·¥æ™ºèƒ½éŸ³ä¹çš„äººæ€§æ„ŸçŸ¥",
      "authors": [
        "Flavio Figueiredo",
        "Giovanni Martinelli",
        "Henrique Sousa",
        "Pedro Rodrigues",
        "Frederico Pedrosa",
        "Lucas N. Ferreira"
      ],
      "abstract": "Recent advances in AI music (AIM) generation services are currently transforming the music industry. Given these advances, understanding how humans perceive AIM is crucial both to educate users on identifying AIM songs, and, conversely, to improve current models. We present results from a listener-focused experiment aimed at understanding how humans perceive AIM. In a blind, Turing-like test, participants were asked to distinguish, from a pair, the AIM and human-made song. We contrast with other studies by utilizing a randomized controlled crossover trial that controls for pairwise similarity and allows for a causal interpretation. We are also the first study to employ a novel, author-uncontrolled dataset of AIM songs from real-world usage of commercial models (i.e., Suno). We establish that listeners' reliability in distinguishing AIM causally increases when pairs are similar. Lastly, we conduct a mixed-methods content analysis of listeners' free-form feedback, revealing a focus on vocal and technical cues in their judgments.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººç±»å¯¹ AI Music (AIM) çš„æ„ŸçŸ¥ï¼Œæ—¨åœ¨äº†è§£ç”¨æˆ·å¦‚ä½•è¯†åˆ«ç”± Suno ç­‰å•†ä¸šæ¨¡å‹ç”Ÿæˆçš„éŸ³ä¹ã€‚ç ”ç©¶äººå‘˜è¿›è¡Œäº†ä¸€é¡¹é’ˆå¯¹å¬ä¼—çš„å®éªŒï¼Œé‡‡ç”¨ç±»ä¼¼äº Turing-like test çš„ç›²æµ‹æ–¹æ³•ï¼Œè¦æ±‚å‚ä¸è€…åœ¨æˆå¯¹çš„æ­Œæ›²ä¸­åŒºåˆ† AIM å’Œäººç±»åˆ›ä½œçš„éŸ³ä¹ã€‚ä¸ä»¥å¾€ç ”ç©¶ä¸åŒï¼Œè¯¥å®éªŒåˆ©ç”¨éšæœºå¯¹ç…§äº¤å‰å®éªŒ (randomized controlled crossover trial) æ¥æ§åˆ¶æˆå¯¹ç›¸ä¼¼æ€§ä»¥å®ç°å› æœè§£é‡Šï¼Œå¹¶é¦–æ¬¡ä½¿ç”¨äº†æ¥è‡ªçœŸå®ä¸–ç•Œå•†ä¸šæ¨¡å‹ç”Ÿæˆçš„éä½œè€…æ§åˆ¶æ•°æ®é›†ã€‚å®éªŒç»“æœè¯æ˜ï¼Œå½“æ­Œæ›²å¯¹çš„ç›¸ä¼¼åº¦è¾ƒé«˜æ—¶ï¼Œå¬ä¼—åŒºåˆ† AIM çš„å¯é æ€§ä¼šäº§ç”Ÿå› æœæ€§çš„å¢åŠ ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¯¹å¬ä¼—åé¦ˆè¿›è¡Œçš„æ··åˆæ–¹æ³•å†…å®¹åˆ†æ (mixed-methods content analysis) å‘ç°ï¼Œå¬ä¼—åœ¨åˆ¤æ–­æ—¶ä¸»è¦å…³æ³¨äººå£° (vocal) å’ŒæŠ€æœ¯æ€§çº¿ç´¢ (technical cues)ã€‚è¯¥ç ”ç©¶æˆæœä¸ºæ•™è‚²ç”¨æˆ·è¯†åˆ« AIM ä»¥åŠæ”¹è¿›ç°æœ‰çš„ç”Ÿæˆæ¨¡å‹æä¾›äº†é‡è¦ä¾æ®ã€‚",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.SD"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at NeuRIPs 2025 Creative AI Track",
      "pdf_url": "https://arxiv.org/pdf/2509.25601v1",
      "published_date": "2025-09-29 23:53:58 UTC",
      "updated_date": "2025-09-29 23:53:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:39:14.087899+00:00"
    },
    {
      "arxiv_id": "2509.25598v1",
      "title": "Hybrid Reward Normalization for Process-supervised Non-verifiable Agentic Tasks",
      "title_zh": "è¿‡ç¨‹ç›‘ç£ä¸‹ä¸å¯éªŒè¯æ™ºèƒ½ä½“ä»»åŠ¡çš„æ··åˆå¥–åŠ±å½’ä¸€åŒ–",
      "authors": [
        "Peiran Xu",
        "Zhuohao Li",
        "Xiaoying Xing",
        "Guannan Zhang",
        "Debiao Li",
        "Kunyu Shi"
      ],
      "abstract": "Large Language Models (LLMs) increasingly rely on external tools such as search engines to solve complex agentic tasks that require reasoning and external knowledge retrieval. Recently, reinforcement learning with verifiable rewards (RLVR) has demonstrated its effectiveness in advancing capabilities of LLMs by rewarding the final answers via outcome rewards. While straightforward to supervise, outcome rewards only provide sparse signals and delayed feedback, which limits their effectiveness on long trajectories. Process rewards address this by evaluating intermediate steps, providing fine-grained supervision and encouraging grounded problem solving. However, it is notoriously hard to annotate step-wise labels, especially in non-verifiable process without \"golden\" answers. Furthermore, step-wise judgment requires the balance between local quality with contribution to the final outcome, as optimizing towards higher process reward may not always align with better final outcomes. To address the above challenges, we introduce Principle Process Reward (PPR), an RL approach that unifies principled step-level assessment and outcome verification. We train a principle-based reward model to improve the transparency and reliability of process evaluation, and further introduce a Reward Normalization (ReNorm) strategy to calibrate outcome and process rewards. Experiment results show that PPR achieves state-of-the-art performance across a wide range of benchmarks, demonstrating its impressive robustness and generalization. Our code and model collection is available in this link.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨æ‰§è¡Œå¤æ‚æ™ºèƒ½ä½“ä»»åŠ¡æ—¶ï¼Œç»“æœå¥–åŠ± (Outcome rewards) ä¿¡å·ç¨€ç–ä¸”åé¦ˆå»¶è¿Ÿçš„é—®é¢˜ï¼Œæ¢è®¨äº†å¦‚ä½•é€šè¿‡è¿‡ç¨‹ç›‘ç£æå‡å…¶æ¨ç†ä¸å¤–éƒ¨çŸ¥è¯†æ£€ç´¢èƒ½åŠ›ã€‚åœ¨ç¼ºä¹æ ‡å‡†ç­”æ¡ˆçš„ééªŒè¯æ€§è¿‡ç¨‹ä¸­ï¼Œæ ‡æ³¨æ­¥éª¤çº§æ ‡ç­¾ä¸ä»…å›°éš¾ï¼Œä¸”å±€éƒ¨æ­¥éª¤å¥–åŠ±çš„ä¼˜åŒ–å¾€å¾€ä¸æœ€ç»ˆç»“æœå­˜åœ¨å¤±é…ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº† Principle Process Reward (PPR) å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡è®­ç»ƒåŸºäºåŸåˆ™çš„å¥–åŠ±æ¨¡å‹æ¥æå‡è¿‡ç¨‹è¯„ä¼°çš„é€æ˜åº¦ä¸å¯é æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†å¥–åŠ±å½’ä¸€åŒ– (Reward Normalization, ReNorm) ç­–ç•¥ï¼Œæ—¨åœ¨ç²¾ç¡®æ ¡å‡†è¿‡ç¨‹å¥–åŠ±ä¸ç»“æœå¥–åŠ±ä¹‹é—´çš„æƒé‡å¹³è¡¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPPR åœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº† SOTA æ€§èƒ½ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨å¤„ç†é•¿è½¨è¿¹ã€ééªŒè¯æ€§ä»»åŠ¡æ—¶å…·æœ‰æå¼ºçš„é²æ£’æ€§ä¸æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25598v1",
      "published_date": "2025-09-29 23:44:55 UTC",
      "updated_date": "2025-09-29 23:44:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:39:21.698402+00:00"
    },
    {
      "arxiv_id": "2509.25594v1",
      "title": "K-Prism: A Knowledge-Guided and Prompt Integrated Universal Medical Image Segmentation Model",
      "title_zh": "K-Prismï¼šçŸ¥è¯†å¼•å¯¼ä¸æç¤ºèåˆçš„é€šç”¨åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹",
      "authors": [
        "Bangwei Guo",
        "Yunhe Gao",
        "Meng Ye",
        "Difei Gu",
        "Yang Zhou",
        "Leon Axel",
        "Dimitris Metaxas"
      ],
      "abstract": "Medical image segmentation is fundamental to clinical decision-making, yet existing models remain fragmented. They are usually trained on single knowledge sources and specific to individual tasks, modalities, or organs. This fragmentation contrasts sharply with clinical practice, where experts seamlessly integrate diverse knowledge: anatomical priors from training, exemplar-based reasoning from reference cases, and iterative refinement through real-time interaction. We present $\\textbf{K-Prism}$, a unified segmentation framework that mirrors this clinical flexibility by systematically integrating three knowledge paradigms: (i) $\\textit{semantic priors}$ learned from annotated datasets, (ii) $\\textit{in-context knowledge}$ from few-shot reference examples, and (iii) $\\textit{interactive feedback}$ from user inputs like clicks or scribbles. Our key insight is that these heterogeneous knowledge sources can be encoded into a dual-prompt representation: 1-D sparse prompts defining $\\textit{what}$ to segment and 2-D dense prompts indicating $\\textit{where}$ to attend, which are then dynamically routed through a Mixture-of-Experts (MoE) decoder. This design enables flexible switching between paradigms and joint training across diverse tasks without architectural modifications. Comprehensive experiments on 18 public datasets spanning diverse modalities (CT, MRI, X-ray, pathology, ultrasound, etc.) demonstrate that K-Prism achieves state-of-the-art performance across semantic, in-context, and interactive segmentation settings. Code will be released upon publication.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†K-Prismï¼Œä¸€ä¸ªç»Ÿä¸€çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ã€æ¨¡æ€æˆ–å™¨å®˜ä¸Šè¡¨ç°ç¢ç‰‡åŒ–çš„é—®é¢˜ã€‚K-Prismæ¨¡æ‹Ÿä¸´åºŠä¸“å®¶çš„å†³ç­–è¿‡ç¨‹ï¼Œç³»ç»Ÿåœ°æ•´åˆäº†ä»æ ‡æ³¨æ•°æ®ä¸­å­¦ä¹ çš„semantic priorsã€æ¥è‡ªå°‘æ ·æœ¬å‚è€ƒæ¡ˆä¾‹çš„in-context knowledgeä»¥åŠåŸºäºç”¨æˆ·è¾“å…¥çš„interactive feedbackä¸‰ç§çŸ¥è¯†èŒƒå¼ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒåˆ›æ–°æ˜¯å°†è¿™äº›å¼‚æ„çŸ¥è¯†æºç¼–ç ä¸ºåŒé‡æç¤ºï¼ˆdual-promptï¼‰è¡¨ç¤ºï¼Œé€šè¿‡1-D sparse promptså®šä¹‰åˆ†å‰²ç›®æ ‡ï¼Œå¹¶åˆ©ç”¨2-D dense promptsæŒ‡ç¤ºå…³æ³¨ä½ç½®ã€‚è¿™äº›æç¤ºéšåé€šè¿‡Mixture-of-Experts (MoE)è§£ç å™¨è¿›è¡ŒåŠ¨æ€è·¯ç”±ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸åŒèŒƒå¼é—´çµæ´»åˆ‡æ¢ï¼Œå¹¶æ”¯æŒåœ¨ä¸æ”¹å˜æ¶æ„çš„æƒ…å†µä¸‹è¿›è¡Œè·¨ä»»åŠ¡è”åˆè®­ç»ƒã€‚åœ¨æ¶µç›–CTã€MRIã€X-rayã€ç—…ç†åŠè¶…å£°ç­‰å¤šç§æ¨¡æ€çš„18ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒK-Prismåœ¨è¯­ä¹‰ã€ä¸Šä¸‹æ–‡å’Œäº¤äº’å¼åˆ†å‰²ä»»åŠ¡ä¸­å‡å–å¾—äº†state-of-the-artçš„æ€§èƒ½è¡¨ç°ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25594v1",
      "published_date": "2025-09-29 23:34:05 UTC",
      "updated_date": "2025-09-29 23:34:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:39:25.586992+00:00"
    },
    {
      "arxiv_id": "2509.25593v1",
      "title": "Causal Autoencoder-like Generation of Feedback Fuzzy Cognitive Maps with an LLM Agent",
      "title_zh": "åŸºäºLLMæ™ºèƒ½ä½“çš„ç±»å› æœè‡ªåŠ¨ç¼–ç å™¨å¼åé¦ˆæ¨¡ç³Šè®¤çŸ¥å›¾ç”Ÿæˆ",
      "authors": [
        "Akash Kumar Panda",
        "Olaoluwa Adigun",
        "Bart Kosko"
      ],
      "abstract": "A large language model (LLM) can map a feedback causal fuzzy cognitive map (FCM) into text and then reconstruct the FCM from the text. This explainable AI system approximates an identity map from the FCM to itself and resembles the operation of an autoencoder (AE). Both the encoder and the decoder explain their decisions in contrast to black-box AEs. Humans can read and interpret the encoded text in contrast to the hidden variables and synaptic webs in AEs. The LLM agent approximates the identity map through a sequence of system instructions that does not compare the output to the input. The reconstruction is lossy because it removes weak causal edges or rules while it preserves strong causal edges. The encoder preserves the strong causal edges even when it trades off some details about the FCM to make the text sound more natural.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLM) Agent å°†åé¦ˆæ¨¡ç³Šè®¤çŸ¥å›¾ (Fuzzy Cognitive Map, FCM) æ˜ å°„ä¸ºæ–‡æœ¬å¹¶ä»ä¸­é‡æ–°æ„å»º FCM çš„è¿‡ç¨‹ã€‚è¿™ä¸€å¯è§£é‡Šäººå·¥æ™ºèƒ½ (Explainable AI, XAI) ç³»ç»Ÿè¿‘ä¼¼äºä¸€ä¸ªä» FCM åˆ°å…¶è‡ªèº«çš„æ’ç­‰æ˜ å°„ï¼Œå…¶è¿è¡Œæœºåˆ¶ç±»ä¼¼äºè‡ªåŠ¨ç¼–ç å™¨ (Autoencoder, AE)ã€‚ä¸ä¼ ç»Ÿçš„é»‘ç›’ AE ä¸åŒï¼Œè¯¥ç³»ç»Ÿçš„ç¼–ç å™¨å’Œè§£ç å™¨å‡èƒ½è§£é‡Šå…¶å†³ç­–è¿‡ç¨‹ï¼Œä¸”äººç±»å¯ä»¥ç›´æ¥é˜…è¯»å¹¶ç†è§£ä½œä¸ºä¸­é—´å˜é‡çš„ç¼–ç æ–‡æœ¬ã€‚LLM Agent é€šè¿‡ä¸€ç³»åˆ—ç³»ç»ŸæŒ‡ä»¤å®ç°è¯¥é‡æ„ï¼Œè™½ç„¶è¿‡ç¨‹å…·æœ‰ä¸€å®šçš„æœ‰æŸæ€§ï¼Œä½†å…¶åœ¨è¿½æ±‚æ–‡æœ¬è‡ªç„¶æ€§çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¿‡æ»¤å¼±å› æœè¾¹ (weak causal edges) å¹¶ä¿ç•™æ ¸å¿ƒçš„å¼ºå› æœè¾¹ (strong causal edges)ã€‚è¿™ç§æ–¹æ³•åœ¨ç»´æŒå› æœæ¨¡å‹é€»è¾‘å®Œæ•´æ€§çš„å‰æä¸‹ï¼Œæ˜¾è‘—æå‡äº†å¤æ‚åé¦ˆç³»ç»Ÿçš„å¯ç†è§£æ€§ä¸é€æ˜åº¦ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.HC",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "8 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.25593v1",
      "published_date": "2025-09-29 23:33:53 UTC",
      "updated_date": "2025-09-29 23:33:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:39:29.394770+00:00"
    },
    {
      "arxiv_id": "2509.25591v1",
      "title": "Building the EHR Foundation Model via Next Event Prediction",
      "title_zh": "é€šè¿‡ä¸‹ä¸€äº‹ä»¶é¢„æµ‹æ„å»º EHR åŸºç¡€æ¨¡å‹",
      "authors": [
        "Zekai Chen",
        "Arda Pekis",
        "Kevin Brown"
      ],
      "abstract": "Electronic Health Records (EHRs) contain rich temporal dynamics that conventional encoding approaches fail to adequately capture. While Large Language Models (LLMs) show promise for EHR modeling, they struggle to reason about sequential clinical events and temporal dependencies. We propose Next Event Prediction (NEP), a framework that enhances LLMs' temporal reasoning through autoregressive fine-tuning on clinical event sequences. By reformulating EHRs as timestamped event chains and predicting future medical events, NEP explicitly models disease progression patterns and causal relationships. Extensive evaluations across oncology survival prediction and clinical diagnosis tasks demonstrate NEP's superiority, outperforming specialized EHR models by 4.6% AUROC and general-purpose LLMs by 7.2% C-index in temporal reasoning tasks. Our analyses reveal dual benefits: state-of-the-art prediction accuracy combined with clinically interpretable attention patterns that align with known disease pathways.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Next Event Prediction (NEP)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿç¼–ç æ–¹æ³•å’Œå¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ•è·ç”µå­å¥åº·æ¡£æ¡ˆ(Electronic Health Records, EHRs)å¤æ‚æ—¶é—´åŠ¨æ€å’Œåºåˆ—ä¸´åºŠäº‹ä»¶æ¨ç†æ–¹é¢çš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡å¯¹ä¸´åºŠäº‹ä»¶åºåˆ—è¿›è¡Œè‡ªå›å½’å¾®è°ƒ(Autoregressive fine-tuning)ï¼Œå°†EHRsé‡æ–°é‡æ„ä¸ºå¸¦æ—¶é—´æˆ³çš„äº‹ä»¶é“¾ï¼Œä»è€Œæ˜¾å¼å»ºæ¨¡ç–¾ç—…æ¼”å˜æ¨¡å¼å’Œå› æœå…³ç³»ã€‚åœ¨è‚¿ç˜¤ç”Ÿå­˜é¢„æµ‹å’Œä¸´åºŠè¯Šæ–­ä»»åŠ¡çš„å¹¿æ³›è¯„ä¼°ä¸­ï¼ŒNEPçš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†ï¼Œå…¶åœ¨æ—¶é—´æ¨ç†ä»»åŠ¡ä¸­æ¯”ä¸“é—¨çš„EHRæ¨¡å‹AUROCæé«˜äº†4.6%ï¼Œæ¯”é€šç”¨LLMsçš„C-indexæé«˜äº†7.2%ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒNEPä¸ä»…å®ç°äº†æœ€å…ˆè¿›çš„é¢„æµ‹å‡†ç¡®ç‡ï¼Œè¿˜é€šè¿‡ä¸å·²çŸ¥ç–¾ç—…è·¯å¾„ä¸€è‡´çš„æ³¨æ„åŠ›æ¨¡å¼æä¾›äº†æä½³çš„ä¸´åºŠå¯è§£é‡Šæ€§ï¼Œä¸ºæ„å»ºæ›´å¼ºå¤§çš„EHRåŸºç¡€æ¨¡å‹æä¾›äº†æ–°çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "q-bio.OT"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25591v1",
      "published_date": "2025-09-29 23:27:51 UTC",
      "updated_date": "2025-09-29 23:27:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:39:31.494764+00:00"
    },
    {
      "arxiv_id": "2509.25586v1",
      "title": "ATLAS: Constraints-Aware Multi-Agent Collaboration for Real-World Travel Planning",
      "title_zh": "ATLASï¼šé¢å‘çœŸå®ä¸–ç•Œæ—…è¡Œè§„åˆ’çš„çº¦æŸæ„ŸçŸ¥å¤šæ™ºèƒ½ä½“åä½œ",
      "authors": [
        "Jihye Choi",
        "Jinsung Yoon",
        "Jiefeng Chen",
        "Somesh Jha",
        "Tomas Pfister"
      ],
      "abstract": "While Large Language Models (LLMs) have shown remarkable advancements in reasoning and tool use, they often fail to generate optimal, grounded solutions under complex constraints. Real-world travel planning exemplifies these challenges, evaluating agents' abilities to handle constraints that are explicit, implicit, and even evolving based on interactions with dynamic environments and user needs. In this paper, we present ATLAS, a general multi-agent framework designed to effectively handle such complex nature of constraints awareness in real-world travel planning tasks. ATLAS introduces a principled approach to address the fundamental challenges of constraint-aware planning through dedicated mechanisms for dynamic constraint management, iterative plan critique, and adaptive interleaved search. ATLAS demonstrates state-of-the-art performance on the TravelPlanner benchmark, improving the final pass rate from 23.3% to 44.4% over its best alternative. More importantly, our work is the first to demonstrate quantitative effectiveness on real-world travel planning tasks with live information search and multi-turn feedback. In this realistic setting, ATLAS showcases its superior overall planning performance, achieving an 84% final pass rate which significantly outperforms baselines including ReAct (59%) and a monolithic agent (27%).",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚çº¦æŸä¸‹éš¾ä»¥ç”Ÿæˆæœ€ä¼˜æ—…æ¸¸è§„åˆ’æ–¹æ¡ˆçš„é—®é¢˜ï¼Œæå‡ºäº† ATLASï¼Œä¸€ä¸ªä¸“ä¸ºå¤„ç†ç°å®ä¸–ç•Œçº¦æŸæ„ŸçŸ¥è§„åˆ’è®¾è®¡çš„é€šç”¨å¤šæ™ºèƒ½ä½“æ¡†æ¶ã€‚ATLAS å¼•å…¥äº†åŠ¨æ€çº¦æŸç®¡ç† (dynamic constraint management)ã€è¿­ä»£è®¡åˆ’è¯„ä»· (iterative plan critique) å’Œè‡ªé€‚åº”äº¤é”™æœç´¢ (adaptive interleaved search) ç­‰æ ¸å¿ƒæœºåˆ¶ï¼Œæ—¨åœ¨æœ‰æ•ˆåº”å¯¹æ˜¾æ€§ã€éšæ€§åŠåŠ¨æ€å˜åŒ–çš„çº¦æŸæŒ‘æˆ˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒATLAS åœ¨ TravelPlanner åŸºå‡†æµ‹è¯•ä¸­å°†æœ€ç»ˆé€šè¿‡ç‡ä» 23.3% æå‡è‡³ 44.4%ã€‚åœ¨åŒ…å«å®æ—¶ä¿¡æ¯æœç´¢å’Œå¤šè½®åé¦ˆçš„çœŸå®åœºæ™¯ä¸­ï¼ŒATLAS å–å¾—äº† 84% çš„é€šè¿‡ç‡ï¼Œæ˜¾è‘—è¶…è¶Šäº† ReAct (59%) å’Œå•ä½“æ™ºèƒ½ä½“ (monolithic agent, 27%) ç­‰åŸºçº¿æ¨¡å‹ã€‚è¯¥å·¥ä½œé¦–æ¬¡åœ¨çœŸå®æ—…æ¸¸è§„åˆ’ä»»åŠ¡ä¸­å®šé‡è¯æ˜äº†å¤šæ™ºèƒ½ä½“åä½œçš„æœ‰æ•ˆæ€§ï¼Œä¸ºè§£å†³å¤æ‚çº¦æŸé©±åŠ¨çš„æ™ºèƒ½è§„åˆ’æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25586v1",
      "published_date": "2025-09-29 23:23:52 UTC",
      "updated_date": "2025-09-29 23:23:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:39:42.686399+00:00"
    },
    {
      "arxiv_id": "2509.25584v1",
      "title": "Skip-It? Theoretical Conditions for Layer Skipping in Vision-Language Models",
      "title_zh": "Skip-Itï¼Ÿè§†è§‰è¯­è¨€æ¨¡å‹ä¸­å±‚è·³è¿‡çš„ç†è®ºæ¡ä»¶",
      "authors": [
        "Max Hartman",
        "Vidhata Jayaraman",
        "Moulik Choraria",
        "Akhil Bhimaraju",
        "Lav R. Varshney"
      ],
      "abstract": "Vision-language models (VLMs) achieve incredible performance across a wide range of tasks, but their large size makes inference costly. Recent work shows that selectively skipping VLM layers can improve efficiency with minimal performance loss or even performance improvements. However, this technique remains underused due to the limited understanding of when layer skipping is beneficial. In this paper, we develop a framework that uses information and learning theory to characterize the conditions under which layer skipping enhances efficiency without sacrificing performance. Motivated by these observations, we analyze the evolution of the VLM's hidden representations through the LLM backbone and show that layers with large redundancy as predicted by our framework coincide with those skipped by popular layer-skipping methods in practice, providing a unified theoretical scaffolding for multiple efficient inference techniques. Our experiments demonstrate that skipping such layers yields faster inference that preserves performance, and also show that applying skipping outside these conditions leads to model degradation.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) å› è§„æ¨¡åºå¤§å¯¼è‡´æ¨ç†æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ï¼Œæ¢è®¨äº†é€šè¿‡é€‰æ‹©æ€§è·³è¿‡å±‚ (Layer Skipping) æ¥æå‡æ•ˆç‡çš„æ–¹æ³•ã€‚ä½œè€…å¼€å‘äº†ä¸€ä¸ªåŸºäºä¿¡æ¯è®ºå’Œå­¦ä¹ ç†è®º (Information and Learning Theory) çš„æ¡†æ¶ï¼Œæ—¨åœ¨ç•Œå®šåœ¨ä¸ç‰ºç‰²æ€§èƒ½çš„å‰æä¸‹ï¼Œè·³è¿‡ç‰¹å®šå±‚ä»¥å¢å¼ºæ•ˆç‡çš„ç†è®ºæ¡ä»¶ã€‚é€šè¿‡åˆ†æ VLM åœ¨å¤§è¯­è¨€æ¨¡å‹ (LLM) éª¨æ¶ç½‘ç»œä¸­éšè—è¡¨ç¤º (Hidden Representations) çš„æ¼”åŒ–è¿‡ç¨‹ï¼Œè¯¥ç ”ç©¶å‘ç°å…¶æ¡†æ¶é¢„æµ‹çš„å†—ä½™å±‚ä¸ç°æœ‰æµè¡Œå±‚è·³è¿‡æŠ€æœ¯æ‰€é€‰å–çš„å±‚é«˜åº¦å»åˆã€‚è¿™ä¸€å‘ç°ä¸ºå¤šç§é«˜æ•ˆæ¨ç†æŠ€æœ¯æä¾›äº†ç»Ÿä¸€çš„ç†è®ºæ”¯æ’‘ (Theoretical Scaffolding)ï¼Œå¡«è¡¥äº†å±‚è·³è¿‡æŠ€æœ¯åœ¨ç†è®ºç†è§£ä¸Šçš„ç©ºç™½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè·³è¿‡è¿™äº›ç¬¦åˆç†è®ºæ¡ä»¶çš„å†—ä½™å±‚å¯ä»¥åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶å®ç°æ›´å¿«çš„æ¨ç†é€Ÿåº¦ï¼Œè€Œè‹¥åœ¨ä¸æ»¡è¶³è¿™äº›æ¡ä»¶çš„å±‚è¿›è¡Œè·³è¿‡ï¼Œåˆ™ä¼šå¯¼è‡´æ¨¡å‹æ€§èƒ½çš„æ˜¾è‘—é€€åŒ–ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.IT",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25584v1",
      "published_date": "2025-09-29 23:16:44 UTC",
      "updated_date": "2025-09-29 23:16:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:39:46.692947+00:00"
    },
    {
      "arxiv_id": "2510.01272v1",
      "title": "Modeling Others' Minds as Code",
      "title_zh": "å°†ä»–äººå¿ƒæ™ºå»ºæ¨¡ä¸ºä»£ç ",
      "authors": [
        "Kunal Jha",
        "Aydan Yuenan Huang",
        "Eric Ye",
        "Natasha Jaques",
        "Max Kleiman-Weiner"
      ],
      "abstract": "Accurate prediction of human behavior is essential for robust and safe human-AI collaboration. However, existing approaches for modeling people are often data-hungry and brittle because they either make unrealistic assumptions about rationality or are too computationally demanding to adapt rapidly. Our key insight is that many everyday social interactions may follow predictable patterns; efficient \"scripts\" that minimize cognitive load for actors and observers, e.g., \"wait for the green light, then go.\" We propose modeling these routines as behavioral programs instantiated in computer code rather than policies conditioned on beliefs and desires. We introduce ROTE, a novel algorithm that leverages both large language models (LLMs) for synthesizing a hypothesis space of behavioral programs, and probabilistic inference for reasoning about uncertainty over that space. We test ROTE in a suite of gridworld tasks and a large-scale embodied household simulator. ROTE predicts human and AI behaviors from sparse observations, outperforming competitive baselines -- including behavior cloning and LLM-based methods -- by as much as 50% in terms of in-sample accuracy and out-of-sample generalization. By treating action understanding as a program synthesis problem, ROTE opens a path for AI systems to efficiently and effectively predict human behavior in the real-world.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•é€šè¿‡å°†ä»–äººçš„å¿ƒç†å»ºæ¨¡ä¸ºä»£ç æ¥å‡†ç¡®é¢„æµ‹äººç±»è¡Œä¸ºï¼Œè¿™å¯¹äºç¨³å¥ä¸”å®‰å…¨çš„äººæœºåä½œè‡³å…³é‡è¦ã€‚ä½œè€…æŒ‡å‡ºè®¸å¤šæ—¥å¸¸ç¤¾äº¤äº’åŠ¨éµå¾ªå¯é¢„æµ‹çš„æ¨¡å¼ï¼Œå› æ­¤æå‡ºäº† ROTE ç®—æ³•ï¼Œå°†è¿™äº›å¸¸è§„è¡Œä¸ºå»ºæ¨¡ä¸ºä»¥è®¡ç®—æœºä»£ç å½¢å¼å­˜åœ¨çš„è¡Œä¸ºç¨‹åºï¼Œè€Œéä¼ ç»Ÿçš„åŸºäºä¿¡å¿µå’Œæ¬²æœ›çš„ç­–ç•¥ã€‚ROTE åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) åˆæˆè¡Œä¸ºç¨‹åºçš„å‡è®¾ç©ºé—´ï¼Œå¹¶ç»“åˆæ¦‚ç‡æ¨ç† (probabilistic inference) å¤„ç†è¯¥ç©ºé—´å†…çš„ä¸ç¡®å®šæ€§ã€‚åœ¨ç½‘æ ¼ä¸–ç•Œä»»åŠ¡å’Œå¤§å‹å…·èº«å®¶åº­æ¨¡æ‹Ÿå™¨ä¸­çš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒROTE åœ¨ç¨€ç–è§‚æµ‹ä¸‹é¢„æµ‹äººç±»å’Œ AI è¡Œä¸ºçš„è¡¨ç°æ˜¾è‘—ä¼˜äºè¡Œä¸ºå…‹éš† (behavior cloning) ç­‰åŸºå‡†æ¨¡å‹ï¼Œåœ¨å‡†ç¡®ç‡å’Œæ³›åŒ–èƒ½åŠ›ä¸Šæå‡äº†é«˜è¾¾ 50%ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†åŠ¨ä½œç†è§£è§†ä¸ºç¨‹åºåˆæˆ (program synthesis) é—®é¢˜ï¼Œä¸º AI ç³»ç»Ÿåœ¨çœŸå®ä¸–ç•Œä¸­é«˜æ•ˆé¢„æµ‹äººç±»è¡Œä¸ºå¼€è¾Ÿäº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01272v1",
      "published_date": "2025-09-29 22:56:34 UTC",
      "updated_date": "2025-09-29 22:56:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:39:52.890252+00:00"
    },
    {
      "arxiv_id": "2509.25570v1",
      "title": "AttentionViG: Cross-Attention-Based Dynamic Neighbor Aggregation in Vision GNNs",
      "title_zh": "AttentionViGï¼šè§†è§‰å›¾ç¥ç»ç½‘ç»œä¸­åŸºäºäº¤å‰æ³¨æ„åŠ›çš„åŠ¨æ€é‚»åŸŸèšåˆ",
      "authors": [
        "Hakan Emre Gedik",
        "Andrew Martin",
        "Mustafa Munir",
        "Oguzhan Baser",
        "Radu Marculescu",
        "Sandeep P. Chinchali",
        "Alan C. Bovik"
      ],
      "abstract": "Vision Graph Neural Networks (ViGs) have demonstrated promising performance in image recognition tasks against Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). An essential part of the ViG framework is the node-neighbor feature aggregation method. Although various graph convolution methods, such as Max-Relative, EdgeConv, GIN, and GraphSAGE, have been explored, a versatile aggregation method that effectively captures complex node-neighbor relationships without requiring architecture-specific refinements is needed. To address this gap, we propose a cross-attention-based aggregation method in which the query projections come from the node, while the key projections come from its neighbors. Additionally, we introduce a novel architecture called AttentionViG that uses the proposed cross-attention aggregation scheme to conduct non-local message passing. We evaluated the image recognition performance of AttentionViG on the ImageNet-1K benchmark, where it achieved SOTA performance. Additionally, we assessed its transferability to downstream tasks, including object detection and instance segmentation on MS COCO 2017, as well as semantic segmentation on ADE20K. Our results demonstrate that the proposed method not only achieves strong performance, but also maintains efficiency, delivering competitive accuracy with comparable FLOPs to prior vision GNN architectures.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰å›¾ç¥ç»ç½‘ç»œ(Vision Graph Neural Networks, ViGs)ä¸­èŠ‚ç‚¹-é‚»å±…ç‰¹å¾èšåˆæ–¹æ³•çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§åŸºäºäº¤å‰æ³¨æ„åŠ›(Cross-Attention)çš„åŠ¨æ€èšåˆæ–¹æ³•ã€‚åœ¨è¯¥æ–¹æ³•ä¸­ï¼ŒæŸ¥è¯¢æŠ•å½±(Query Projections)æ¥è‡ªèŠ‚ç‚¹æœ¬èº«ï¼Œè€Œé”®æŠ•å½±(Key Projections)åˆ™æ¥è‡ªå…¶é‚»å±…èŠ‚ç‚¹ã€‚ç ”ç©¶å›¢é˜Ÿè¿›ä¸€æ­¥æ„å»ºäº†åä¸ºAttentionViGçš„æ–°å‹æ¶æ„ï¼Œåˆ©ç”¨è¯¥èšåˆæ–¹æ¡ˆå®ç°éå±€éƒ¨ä¿¡æ¯ä¼ é€’(Non-local Message Passing)ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAttentionViGåœ¨ImageNet-1KåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†SOTAæ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨MS COCO 2017çš„ç›®æ ‡æ£€æµ‹ä¸å®ä¾‹åˆ†å‰²ä»¥åŠADE20Kçš„è¯­ä¹‰åˆ†å‰²ç­‰ä¸‹æ¸¸ä»»åŠ¡ä¸­ä¹Ÿå±•ç°äº†æå¼ºçš„è¿ç§»èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åœ¨ä¿æŒé«˜æ•ˆæ€§çš„åŒæ—¶ï¼Œä»¥ä¸ä»¥å¾€ViGæ¶æ„ç›¸å½“çš„FLOPsæä¾›äº†æå…·ç«äº‰åŠ›çš„å‡†ç¡®ç‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "WACV submission. 13 pages, including the main text (8 pages), references, and supplementary material",
      "pdf_url": "https://arxiv.org/pdf/2509.25570v1",
      "published_date": "2025-09-29 22:47:48 UTC",
      "updated_date": "2025-09-29 22:47:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:39:53.386198+00:00"
    },
    {
      "arxiv_id": "2509.25568v1",
      "title": "Probing the Limits of Stylistic Alignment in Vision-Language Models",
      "title_zh": "æ¢ç©¶è§†è§‰-è¯­è¨€æ¨¡å‹é£æ ¼å¯¹é½çš„æé™",
      "authors": [
        "Asma Farajidizaji",
        "Akash Gupta",
        "Vatsal Raina"
      ],
      "abstract": "Vision-language models are increasingly used to generate image captions in specific styles, such as humor or romantic. However, these transformer-based models often struggle with this subjective task in a zero-shot setting. While preference data can be used to align them toward a desired style, such data is expensive to acquire, limiting the ability to explore the models' full capabilities. This work addresses this by studying the data efficiency of aligning small vision-language models to humor and romantic styles. This approach helps to define the performance limits of these models and determine how little preference data is needed to achieve stylistic saturation, benchmarking their capabilities and limitations.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ (Vision-Language Models) åœ¨ç”Ÿæˆè¯¸å¦‚å¹½é»˜æˆ–æµªæ¼«ç­‰ç‰¹å®šé£æ ¼å›¾åƒæè¿°æ—¶çš„é£æ ¼å¯¹é½ç•Œé™ã€‚é’ˆå¯¹åŸºäº Transformer çš„æ¨¡å‹åœ¨é›¶æ ·æœ¬ (zero-shot) è®¾ç½®ä¸‹éš¾ä»¥èƒœä»»æ­¤ç±»ä¸»è§‚ä»»åŠ¡çš„æŒ‘æˆ˜ï¼Œä½œè€…é‡ç‚¹ç ”ç©¶äº†å°å‹æ¨¡å‹åœ¨é£æ ¼å¯¹é½è¿‡ç¨‹ä¸­çš„æ•°æ®æ•ˆç‡ã€‚ç”±äºè·å–åå¥½æ•°æ® (preference data) çš„æˆæœ¬æé«˜ï¼Œè¯¥å·¥ä½œé€šè¿‡å®éªŒåˆ†æäº†æ¨¡å‹å¯¹é½ç‰¹å®šé£æ ¼æ—¶çš„æ€§èƒ½æ¼”å˜ã€‚ç ”ç©¶æ—¨åœ¨å®šä¹‰æ¨¡å‹çš„æ€§èƒ½æé™ï¼Œå¹¶ç¡®å®šå®ç°é£æ ¼é¥±å’Œ (stylistic saturation) æ‰€éœ€çš„æœ€å°‘æ•°æ®é‡ã€‚é€šè¿‡å¯¹å¹½é»˜å’Œæµªæ¼«é£æ ¼çš„å¯¹é½å®éªŒï¼Œè¯¥è®ºæ–‡æˆåŠŸåŸºå‡†åŒ–äº†æ¨¡å‹çš„èƒ½åŠ›ä¸å±€é™æ€§ã€‚è¿™ä¸ºåœ¨æœ‰é™èµ„æºä¸‹æå‡æ¨¡å‹çš„ä¸»è§‚ä»»åŠ¡è¡¨ç°æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "5 pages, 1 figure, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.25568v1",
      "published_date": "2025-09-29 22:46:51 UTC",
      "updated_date": "2025-09-29 22:46:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:39:56.590437+00:00"
    },
    {
      "arxiv_id": "2509.25562v1",
      "title": "IRIS: Intrinsic Reward Image Synthesis",
      "title_zh": "IRISï¼šåŸºäºå†…åœ¨å¥–åŠ±çš„å›¾åƒåˆæˆ",
      "authors": [
        "Yihang Chen",
        "Yuanhao Ban",
        "Yunqi Hong",
        "Cho-Jui Hsieh"
      ],
      "abstract": "Despite the success of Reinforcement Learning from Human Feedback (RLHF) in language reasoning, its application to autoregressive Text-to-Image (T2I) generation is often constrained by the limited availability of human preference data. This paper explores how an autoregressive T2I model can learn from internal signals without relying on external rewards or labeled data. Contrary to recent findings in text generation, we show that maximizing self-uncertainty, rather than self-certainty, improves image generation. We observe that this is because autoregressive T2I models with low uncertainty tend to generate simple and uniform images, which are less aligned with human preferences. Based on these observations, we propose IRIS (Intrinsic Reward Image Synthesis), the first framework to improve autoregressive T2I models with reinforcement learning using only an intrinsic reward. Empirical results demonstrate that applying IRIS to autoregressive T2I models achieves performance that is competitive with or superior to external rewards.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªå›å½’æ–‡æœ¬ç”Ÿæˆå›¾åƒ (Text-to-Image) æ¨¡å‹ä¸­å¼ºåŒ–å­¦ä¹ å—é™äºäººç±»åå¥½æ•°æ®è§„æ¨¡çš„é—®é¢˜ï¼Œæ¢ç´¢äº†æ¨¡å‹åœ¨æ— å¤–éƒ¨å¥–åŠ±ä¸‹é€šè¿‡å†…éƒ¨ä¿¡å·è¿›è¡Œè‡ªæˆ‘æå‡çš„æœºåˆ¶ã€‚ç ”ç©¶äººå‘˜å‘ç°ï¼Œåœ¨å›¾åƒç”Ÿæˆé¢†åŸŸï¼Œæœ€å¤§åŒ–è‡ªä¸ç¡®å®šæ€§ (self-uncertainty) ç›¸æ¯”è‡ªç¡®å®šæ€§ (self-certainty) æ›´æœ‰åŠ©äºæå‡ç”Ÿæˆè´¨é‡ï¼Œå› ä¸ºä½ä¸ç¡®å®šæ€§å¾€å¾€å¯¼è‡´æ¨¡å‹ç”Ÿæˆè¿‡äºç®€å•ä¸”ç¼ºä¹ç»†èŠ‚çš„å›¾åƒã€‚åŸºäºæ­¤è§‚å¯Ÿï¼Œè®ºæ–‡æå‡ºäº† IRIS (Intrinsic Reward Image Synthesis) æ¡†æ¶ï¼Œè¿™æ˜¯é¦–ä¸ªä»…ä¾èµ–å†…åœ¨å¥–åŠ± (intrinsic reward) å¹¶ç»“åˆå¼ºåŒ–å­¦ä¹  (Reinforcement Learning) æ¥ä¼˜åŒ–è‡ªå›å½’å›¾åƒç”Ÿæˆæ¨¡å‹çš„ç³»ç»Ÿã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåº”ç”¨ IRIS çš„æ¨¡å‹åœ¨ç”Ÿæˆæ€§èƒ½ä¸Šèƒ½å¤Ÿè¾¾åˆ°ä¸ä½¿ç”¨å¤–éƒ¨å¥–åŠ±ç›¸å½“ç”šè‡³æ›´ä¼˜çš„æ°´å¹³ã€‚è¿™ä¸€ç ”ç©¶ä¸ºå‡å°‘ T2I æ¨¡å‹å¯¹å¤§è§„æ¨¡äººç±»åé¦ˆæ•°æ®çš„ä¾èµ–å¹¶æå‡æ¨¡å‹è‡ªä¸»è¿›åŒ–èƒ½åŠ›æä¾›äº†åˆ›æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25562v1",
      "published_date": "2025-09-29 22:38:25 UTC",
      "updated_date": "2025-09-29 22:38:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:40:23.502715+00:00"
    },
    {
      "arxiv_id": "2509.25559v1",
      "title": "Radiology's Last Exam (RadLE): Benchmarking Frontier Multimodal AI Against Human Experts and a Taxonomy of Visual Reasoning Errors in Radiology",
      "title_zh": "Radiology's Last Exam (RadLE)ï¼šå‰æ²¿å¤šæ¨¡æ€äººå·¥æ™ºèƒ½ä¸äººç±»ä¸“å®¶çš„åŸºå‡†æµ‹è¯„åŠæ”¾å°„å­¦è§†è§‰æ¨ç†é”™è¯¯åˆ†ç±»ä½“ç³»",
      "authors": [
        "Suvrankar Datta",
        "Divya Buchireddygari",
        "Lakshmi Vennela Chowdary Kaza",
        "Mrudula Bhalke",
        "Kautik Singh",
        "Ayush Pandey",
        "Sonit Sai Vasipalli",
        "Upasana Karnwal",
        "Hakikat Bir Singh Bhatti",
        "Bhavya Ratan Maroo",
        "Sanjana Hebbar",
        "Rahul Joseph",
        "Gurkawal Kaur",
        "Devyani Singh",
        "Akhil V",
        "Dheeksha Devasya Shama Prasad",
        "Nishtha Mahajan",
        "Ayinaparthi Arisha",
        "Rajesh Vanagundi",
        "Reet Nandy",
        "Kartik Vuthoo",
        "Snigdhaa Rajvanshi",
        "Nikhileswar Kondaveeti",
        "Suyash Gunjal",
        "Rishabh Jain",
        "Rajat Jain",
        "Anurag Agrawal"
      ],
      "abstract": "Generalist multimodal AI systems such as large language models (LLMs) and vision language models (VLMs) are increasingly accessed by clinicians and patients alike for medical image interpretation through widely available consumer-facing chatbots. Most evaluations claiming expert level performance are on public datasets containing common pathologies. Rigorous evaluation of frontier models on difficult diagnostic cases remains limited. We developed a pilot benchmark of 50 expert-level \"spot diagnosis\" cases across multiple imaging modalities to evaluate the performance of frontier AI models against board-certified radiologists and radiology trainees. To mirror real-world usage, the reasoning modes of five popular frontier AI models were tested through their native web interfaces, viz. OpenAI o3, OpenAI GPT-5, Gemini 2.5 Pro, Grok-4, and Claude Opus 4.1. Accuracy was scored by blinded experts, and reproducibility was assessed across three independent runs. GPT-5 was additionally evaluated across various reasoning modes. Reasoning quality errors were assessed and a taxonomy of visual reasoning errors was defined. Board-certified radiologists achieved the highest diagnostic accuracy (83%), outperforming trainees (45%) and all AI models (best performance shown by GPT-5: 30%). Reliability was substantial for GPT-5 and o3, moderate for Gemini 2.5 Pro and Grok-4, and poor for Claude Opus 4.1. These findings demonstrate that advanced frontier models fall far short of radiologists in challenging diagnostic cases. Our benchmark highlights the present limitations of generalist AI in medical imaging and cautions against unsupervised clinical use. We also provide a qualitative analysis of reasoning traces and propose a practical taxonomy of visual reasoning errors by AI models for better understanding their failure modes, informing evaluation standards and guiding more robust model development.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†åä¸ºRadiology's Last Exam (RadLE)çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å‰æ²¿å¤šæ¨¡æ€AIæ¨¡å‹åœ¨å¤æ‚æ”¾å°„å½±åƒè¯Šæ–­ä»»åŠ¡ä¸­ä¸äººç±»ä¸“å®¶åŠå—è®­è€…çš„è¡¨ç°å·®å¼‚ã€‚é€šè¿‡å¯¹50ä¸ªæ¶µç›–å¤šç§å½±åƒæ¨¡æ€çš„ä¸“å®¶çº§â€œå³æ—¶è¯Šæ–­â€(spot diagnosis)æ¡ˆä¾‹è¿›è¡Œæµ‹è¯•ï¼Œç ”ç©¶å¯¹æ¯”äº†åŒ…æ‹¬GPT-5ã€o3ã€Gemini 2.5 Proã€Grok-4å’ŒClaude Opus 4.1åœ¨å†…çš„äº”ç§ä¸»æµè§†è§‰è¯­è¨€æ¨¡å‹(VLMs)çš„æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œèµ„æ·±æ”¾å°„ç§‘åŒ»ç”Ÿå–å¾—äº†æœ€é«˜çš„è¯Šæ–­å‡†ç¡®ç‡(83%)ï¼Œæ˜¾è‘—ä¼˜äºæ”¾å°„ç§‘å—è®­è€…(45%)ä»¥åŠæ‰€æœ‰å‚æµ‹AIæ¨¡å‹ï¼Œå…¶ä¸­è¡¨ç°æœ€å¥½çš„GPT-5ä»…è¾¾åˆ°30%çš„å‡†ç¡®ç‡ã€‚åœ¨å¯é æ€§è¯„ä¼°ä¸­ï¼ŒGPT-5å’Œo3è¡¨ç°å‡ºè¾ƒé«˜çš„é‡ç°æ€§ï¼Œè€ŒClaude Opus 4.1çš„è¡¨ç°åˆ™è¾ƒå·®ã€‚ç ”ç©¶å›¢é˜Ÿè¿›ä¸€æ­¥é€šè¿‡å®šæ€§åˆ†æå®šä¹‰äº†ä¸€å¥—è§†è§‰æ¨ç†é”™è¯¯(visual reasoning errors)çš„åˆ†ç±»æ³•ï¼Œæ·±å…¥å‰–æäº†é€šç”¨AIåœ¨åŒ»å­¦å½±åƒå¤„ç†ä¸­çš„å¤±æ•ˆæ¨¡å¼ã€‚è¿™äº›å‘ç°è¡¨æ˜ç›®å‰çš„é€šç”¨å¤šæ¨¡æ€æ¨¡å‹åœ¨åº”å¯¹é«˜éš¾åº¦è¯Šæ–­æ¡ˆä¾‹æ—¶ä»è¿œé€Šäºä¸“ä¸šåŒ»å¸ˆï¼Œå¼ºè°ƒäº†åœ¨ä¸´åºŠä¸­æ— ç›‘ç£ä½¿ç”¨æ­¤ç±»AIçš„å±€é™æ€§ä¸é£é™©ï¼Œå¹¶ä¸ºæœªæ¥æ›´ç¨³å¥çš„æ¨¡å‹å¼€å‘æä¾›äº†è¯„ä¼°æ ‡å‡†ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "29 pages, 7 figures, 7 tables, includes Annexure (1). Part of the work accepted at RSNA 2025 (Cutting Edge Oral Presentation)",
      "pdf_url": "https://arxiv.org/pdf/2509.25559v1",
      "published_date": "2025-09-29 22:31:20 UTC",
      "updated_date": "2025-09-29 22:31:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:40:17.992457+00:00"
    },
    {
      "arxiv_id": "2509.25558v1",
      "title": "A(I)nimism: Re-enchanting the World Through AI-Mediated Object Interaction",
      "title_zh": "A(I)nimismï¼šé€šè¿‡ AI ä»‹å¯¼çš„ç‰©ä½“äº¤äº’è®©ä¸–ç•Œâ€œå†é­…â€",
      "authors": [
        "Diana Mykhaylychenko",
        "Maisha Thasin",
        "Dunya Baradari",
        "Charmelle Mhungu"
      ],
      "abstract": "Animist worldviews treat beings, plants, landscapes, and even tools as persons endowed with spirit, an orientation that has long shaped human-nonhuman relations through ritual and moral practice. While modern industrial societies have often imagined technology as mute and mechanical, recent advances in artificial intelligence (AI), especially large language models (LLMs), invite people to anthropomorphize and attribute inner life to devices. This paper introduces A(I)nimism, an interactive installation exploring how large language objects (LLOs) can mediate animistic relationships with everyday things. Housed within a physical 'portal', the system uses GPT-4 Vision, voice input, and memory-based agents to create evolving object-personas. Encounters unfold through light, sound, and touch in a ritual-like process of request, conversation, and transformation that is designed to evoke empathy, wonder, and reflection. We situate the project within anthropological perspectives, speculative design, and spiritual HCI. AI's opacity, we argue, invites animistic interpretation, allowing LLOs to re-enchant the mundane and spark new questions of agency, responsibility, and design.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† A(I)nimismï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡å¤§å‹è¯­è¨€å¯¹è±¡ (Large Language Objects, LLOs) ä»‹å¯¼æ—¥å¸¸ç‰©å“ä¸äººä¹‹é—´ä¸‡ç‰©æœ‰çµ (animistic) å…³ç³»çš„äº¤äº’å¼è£…ç½®ã€‚åœ¨ç°ä»£å·¥ä¸šç¤¾ä¼šå°†æŠ€æœ¯è§†ä¸ºæœºæ¢°äº§ç‰©çš„èƒŒæ™¯ä¸‹ï¼Œè¯¥é¡¹ç›®åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) çš„äººæ ¼åŒ–å€¾å‘ï¼Œæ¢è®¨å¦‚ä½•é‡æ–°å®šä¹‰äººä¸éç”Ÿç‰©çš„å…³ç³»ã€‚ç³»ç»Ÿé›†æˆåœ¨ç‰©ç†â€œä¼ é€é—¨â€ä¸­ï¼Œç»“åˆ GPT-4 Visionã€è¯­éŸ³è¾“å…¥å’ŒåŸºäºè®°å¿†çš„æ™ºèƒ½ä½“ (memory-based agents) æ„å»ºå‡ºå…·æœ‰æ¼”åŒ–äººæ ¼çš„ç‰©å“ã€‚é€šè¿‡èåˆå…‰å½±ã€å£°éŸ³ä¸è§¦è§‰çš„ä»ªå¼åŒ–äº¤äº’è¿‡ç¨‹ï¼ŒA(I)nimism æ—¨åœ¨æ¿€å‘ç”¨æˆ·çš„å…±æƒ…ã€å¥‡è¿¹æ„Ÿä¸åæ€ã€‚ç ”ç©¶ç«‹è¶³äºäººç±»å­¦è§†è§’ã€æ€è¾¨è®¾è®¡ (speculative design) å’Œç²¾ç¥äººæœºäº¤äº’ (spiritual HCI)ï¼Œæå‡º AI çš„ä¸é€æ˜æ€§èƒ½å¤Ÿè¯±å‘ä¸‡ç‰©æœ‰çµå¼çš„è§£è¯»ã€‚è¿™ä¸€æ¢ç´¢é€šè¿‡é‡æ–°èµ‹äºˆå¹³å‡¡ä¸–ç•Œä»¥é­”åŠ› (re-enchanting)ï¼Œä¸ºè®¨è®ºèƒ½åŠ¨æ€§ (agency)ã€è´£ä»»æ„ŸåŠæœªæ¥è®¾è®¡æä¾›äº†å…¨æ–°çš„è§†è§’ä¸æ€è€ƒç»´åº¦ã€‚",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.MA",
        "cs.MM"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25558v1",
      "published_date": "2025-09-29 22:27:09 UTC",
      "updated_date": "2025-09-29 22:27:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:40:34.173750+00:00"
    },
    {
      "arxiv_id": "2510.00073v1",
      "title": "Identifying All Îµ-Best Arms in (Misspecified) Linear Bandits",
      "title_zh": "è¯†åˆ«ï¼ˆæ¨¡å‹å¤±é…ï¼‰çº¿æ€§ Bandit ä¸­çš„æ‰€æœ‰ Îµ-æœ€ä½³è‡‚",
      "authors": [
        "Zhekai Li",
        "Tianyi Ma",
        "Cheng Hua",
        "Ruihao Zhu"
      ],
      "abstract": "Motivated by the need to efficiently identify multiple candidates in high trial-and-error cost tasks such as drug discovery, we propose a near-optimal algorithm to identify all Îµ-best arms (i.e., those at most Îµ worse than the optimum). Specifically, we introduce LinFACT, an algorithm designed to optimize the identification of all Îµ-best arms in linear bandits. We establish a novel information-theoretic lower bound on the sample complexity of this problem and demonstrate that LinFACT achieves instance optimality by matching this lower bound up to a logarithmic factor. A key ingredient of our proof is to integrate the lower bound directly into the scaling process for upper bound derivation, determining the termination round and thus the sample complexity. We also extend our analysis to settings with model misspecification and generalized linear models. Numerical experiments, including synthetic and real drug discovery data, demonstrate that LinFACT identifies more promising candidates with reduced sample complexity, offering significant computational efficiency and accelerating early-stage exploratory experiments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¯ç‰©å‘ç°ç­‰é«˜è¯•é”™æˆæœ¬ä»»åŠ¡ï¼Œæ¢è®¨äº†åœ¨çº¿æ€§å¤šè‡‚è€è™æœº(Linear Bandits)ä¸­è¯†åˆ«æ‰€æœ‰Îµ-æœ€ä¼˜è‡‚(Îµ-best arms)çš„é—®é¢˜ï¼Œå³æ‰¾å‡ºä¸æœ€ä¼˜è‡‚å·®è·åœ¨Îµä»¥å†…çš„æ‰€æœ‰å€™é€‰ã€‚è®ºæ–‡æå‡ºäº†LinFACTç®—æ³•ï¼Œä¸“é—¨ç”¨äºä¼˜åŒ–æ­¤ç±»è¯†åˆ«è¿‡ç¨‹ï¼Œå¹¶å»ºç«‹äº†ä¸€ä¸ªå…¨æ–°çš„ä¿¡æ¯è®ºæ ·æœ¬å¤æ‚åº¦ä¸‹ç•Œã€‚è¯æ˜è¿‡ç¨‹æ˜¾ç¤ºï¼ŒLinFACTé€šè¿‡å°†ä¸‹ç•Œç›´æ¥æ•´åˆåˆ°ä¸Šç•Œæ¨å¯¼çš„ç¼©æ”¾ä¸­æ¥ç¡®å®šç»ˆæ­¢è½®æ¬¡ï¼Œåœ¨å¯¹æ•°å› å­èŒƒå›´å†…å®ç°äº†å®ä¾‹æœ€ä¼˜æ€§(instance optimality)ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å°†åˆ†ææ‰©å±•åˆ°äº†æ¨¡å‹å¤±é…(model misspecification)å’Œå¹¿ä¹‰çº¿æ€§æ¨¡å‹(generalized linear models)çš„æƒ…å¢ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLinFACTåœ¨åˆæˆæ•°æ®å’ŒçœŸå®è¯ç‰©å‘ç°æ•°æ®é›†ä¸Šå‡èƒ½ä»¥æ›´ä½çš„æ ·æœ¬å¤æ‚åº¦è¯†åˆ«å‡ºæ›´å¤šæœ‰æ½œåŠ›çš„å€™é€‰ï¼Œæ˜¾è‘—æå‡äº†æ—©æœŸæ¢ç´¢å®éªŒçš„è®¡ç®—æ•ˆç‡ã€‚",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "math.ST"
      ],
      "primary_category": "stat.ML",
      "comment": "80 pages (33 pages for main text), 12 figures, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2510.00073v1",
      "published_date": "2025-09-29 22:26:52 UTC",
      "updated_date": "2025-09-29 22:26:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:40:28.389158+00:00"
    },
    {
      "arxiv_id": "2509.25552v1",
      "title": "Evaluating Foundation Models with Pathological Concept Learning for Kidney Cancer",
      "title_zh": "åŸºäºç—…ç†æ¦‚å¿µå­¦ä¹ çš„è‚¾ç™ŒåŸºç¡€æ¨¡å‹è¯„ä¼°",
      "authors": [
        "Shangqi Gao",
        "Sihan Wang",
        "Yibo Gao",
        "Boming Wang",
        "Xiahai Zhuang",
        "Anne Warren",
        "Grant Stewart",
        "James Jones",
        "Mireia Crispin-Ortuzar"
      ],
      "abstract": "To evaluate the translational capabilities of foundation models, we develop a pathological concept learning approach focused on kidney cancer. By leveraging TNM staging guidelines and pathology reports, we build comprehensive pathological concepts for kidney cancer. Then, we extract deep features from whole slide images using foundation models, construct pathological graphs to capture spatial correlations, and trained graph neural networks to identify these concepts. Finally, we demonstrate the effectiveness of this approach in kidney cancer survival analysis, highlighting its explainability and fairness in identifying low- and high-risk patients. The source code has been released by https://github.com/shangqigao/RadioPath.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é’ˆå¯¹è‚¾ç™Œçš„ç—…ç†æ¦‚å¿µå­¦ä¹ (pathological concept learning)æ–¹æ³•ï¼Œæ—¨åœ¨è¯„ä¼°åŸºç¡€æ¨¡å‹(foundation models)çš„ä¸´åºŠè½¬åŒ–èƒ½åŠ›ã€‚ç ”ç©¶äººå‘˜ç»“åˆTNM stagingæŒ‡å—å’Œç—…ç†æŠ¥å‘Šæ„å»ºäº†å…¨é¢çš„ç—…ç†æ¦‚å¿µä½“ç³»ï¼Œå¹¶åˆ©ç”¨åŸºç¡€æ¨¡å‹ä»å…¨åˆ‡ç‰‡å›¾åƒ(whole slide images)ä¸­æå–æ·±åº¦ç‰¹å¾ã€‚é€šè¿‡æ„å»ºç—…ç†å›¾(pathological graphs)æ•æ‰ç»„ç»‡çš„ç©ºé—´ç›¸å…³æ€§ï¼Œè¯¥æ–¹æ³•é‡‡ç”¨å›¾ç¥ç»ç½‘ç»œ(graph neural networks)å¯¹ç—…ç†æ¦‚å¿µè¿›è¡Œè¯†åˆ«ã€‚å®éªŒç»“æœéªŒè¯äº†è¯¥æ–¹æ³•åœ¨è‚¾ç™Œç”Ÿå­˜åˆ†æä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¹¶çªå‡ºäº†å…¶åœ¨åŒºåˆ†é«˜ä½é£é™©æ‚£è€…æ—¶æ‰€å…·å¤‡çš„å¯è§£é‡Šæ€§(explainability)ä¸å…¬å¹³æ€§(fairness)ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Best Paper Award at MICCAI AMAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.25552v1",
      "published_date": "2025-09-29 22:21:10 UTC",
      "updated_date": "2025-09-29 22:21:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:40:33.838019+00:00"
    },
    {
      "arxiv_id": "2509.25550v3",
      "title": "Learning to Interact in World Latent for Team Coordination",
      "title_zh": "é¢å‘å›¢é˜Ÿåä½œçš„ä¸–ç•Œæ½œç©ºé—´äº¤äº’å­¦ä¹ ",
      "authors": [
        "Dongsu Lee",
        "Daehee Lee",
        "Yaru Niu",
        "Honguk Woo",
        "Amy Zhang",
        "Ding Zhao"
      ],
      "abstract": "This work presents a novel representation learning framework, interactive world latent (IWoL), to facilitate team coordination in multi-agent reinforcement learning (MARL). Building effective representation for team coordination is a challenging problem, due to the intricate dynamics emerging from multi-agent interaction and incomplete information induced by local observations. Our key insight is to construct a learnable representation space that jointly captures inter-agent relations and task-specific world information by directly modeling communication protocols. This representation, we maintain fully decentralized execution with implicit coordination, all while avoiding the inherent drawbacks of explicit message passing, e.g., slower decision-making, vulnerability to malicious attackers, and sensitivity to bandwidth constraints. In practice, our representation can be used not only as an implicit latent for each agent, but also as an explicit message for communication. Across four challenging MARL benchmarks, we evaluate both variants and show that IWoL provides a simple yet powerful key for team coordination. Moreover, we demonstrate that our representation can be combined with existing MARL algorithms to further enhance their performance.",
      "tldr_zh": "è¯¥é¡¹å·¥ä½œæå‡ºäº†åä¸ºInteractive World Latent (IWoL)çš„åˆ›æ–°è¡¨ç¤ºå­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ (MARL)ä¸­ç”±äºå¤æ‚äº’åŠ¨å’Œå±€éƒ¨è§‚æµ‹å¯¼è‡´çš„ä¿¡æ¯ä¸å®Œæ•´åŠå›¢é˜Ÿåä½œå›°éš¾é—®é¢˜ã€‚å…¶æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡ç›´æ¥å»ºæ¨¡é€šä¿¡åè®®ï¼Œæ„å»ºä¸€ä¸ªèƒ½åŒæ—¶æ•æ‰æ™ºèƒ½ä½“é—´å…³ç³»å’Œä»»åŠ¡ç‰¹å®šä¸–ç•Œä¿¡æ¯çš„å­¦ä¹ ç©ºé—´ã€‚è¯¥æ¡†æ¶æ”¯æŒå®Œå…¨å»ä¸­å¿ƒåŒ–æ‰§è¡Œä¸­çš„éšå¼åä½œï¼Œæœ‰æ•ˆé¿å…äº†æ˜¾å¼æ¶ˆæ¯ä¼ é€’å¸¦æ¥çš„å†³ç­–å»¶è¿Ÿã€å®‰å…¨é£é™©åŠå¸¦å®½é™åˆ¶ç­‰å›ºæœ‰ç¼ºç‚¹ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼ŒIWoLè¡¨ç¤ºæ—¢å¯ä½œä¸ºæ™ºèƒ½ä½“çš„éšå¼æ½œå˜é‡(latent)ï¼Œä¹Ÿå¯ä½œä¸ºæ˜¾å¼æ¶ˆæ¯è¿›è¡Œé€šä¿¡ã€‚ç ”ç©¶åœ¨å››ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„MARLåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œè¯æ˜äº†IWoLåœ¨å›¢é˜Ÿåä½œä¸­çš„é«˜æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œå®éªŒè¡¨æ˜è¯¥è¡¨ç¤ºå¯ä»¥ä¸ç°æœ‰MARLç®—æ³•æ— ç¼ç»“åˆï¼Œå¹¶è¿›ä¸€æ­¥æ˜¾è‘—æå‡å…¶æ€§èƒ½è¡¨ç°ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Web: https://dongsuleetech.github.io/projects/IWoL/",
      "pdf_url": "https://arxiv.org/pdf/2509.25550v3",
      "published_date": "2025-09-29 22:13:39 UTC",
      "updated_date": "2025-10-02 20:45:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:40:35.798541+00:00"
    },
    {
      "arxiv_id": "2509.25549v1",
      "title": "Hybrid Approach for Enhancing Lesion Segmentation in Fundus Images",
      "title_zh": "æå‡çœ¼åº•å›¾åƒç—…ç¶åˆ†å‰²æ•ˆæœçš„æ··åˆæ–¹æ³•",
      "authors": [
        "Mohammadmahdi Eshragh",
        "Emad A. Mohammed",
        "Behrouz Far",
        "Ezekiel Weis",
        "Carol L Shields",
        "Sandor R Ferenczy",
        "Trafford Crump"
      ],
      "abstract": "Choroidal nevi are common benign pigmented lesions in the eye, with a small risk of transforming into melanoma. Early detection is critical to improving survival rates, but misdiagnosis or delayed diagnosis can lead to poor outcomes. Despite advancements in AI-based image analysis, diagnosing choroidal nevi in colour fundus images remains challenging, particularly for clinicians without specialized expertise. Existing datasets often suffer from low resolution and inconsistent labelling, limiting the effectiveness of segmentation models. This paper addresses the challenge of achieving precise segmentation of fundus lesions, a critical step toward developing robust diagnostic tools. While deep learning models like U-Net have demonstrated effectiveness, their accuracy heavily depends on the quality and quantity of annotated data. Previous mathematical/clustering segmentation methods, though accurate, required extensive human input, making them impractical for medical applications. This paper proposes a novel approach that combines mathematical/clustering segmentation models with insights from U-Net, leveraging the strengths of both methods. This hybrid model improves accuracy, reduces the need for large-scale training data, and achieves significant performance gains on high-resolution fundus images. The proposed model achieves a Dice coefficient of 89.7% and an IoU of 80.01% on 1024*1024 fundus images, outperforming the Attention U-Net model, which achieved 51.3% and 34.2%, respectively. It also demonstrated better generalizability on external datasets. This work forms a part of a broader effort to develop a decision support system for choroidal nevus diagnosis, with potential applications in automated lesion annotation to enhance the speed and accuracy of diagnosis and monitoring.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹çœ¼åº•å›¾åƒä¸­è„‰ç»œè†œç—£(Choroidal nevi)çš„ç²¾ç¡®åˆ†å‰²æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆæ•°å­¦/èšç±»(mathematical/clustering)æ¨¡å‹ä¸ U-Net æ·±åº¦å­¦ä¹ æ¶æ„ä¼˜åŠ¿çš„æ··åˆæ¨¡å‹ã€‚ç”±äºç°æœ‰æ•°æ®é›†åˆ†è¾¨ç‡ä½ä¸”æ ‡æ³¨ä¸ä¸€è‡´ï¼Œä¼ ç»Ÿçš„åˆ†å‰²æ–¹æ³•å¾€å¾€é¢ä¸´æ•°æ®ä¾èµ–ä¸¥é‡æˆ–äººå·¥å¹²é¢„è¿‡å¤šçš„é—®é¢˜ï¼Œè€Œè¯¥æ··åˆæ¨¡å‹æœ‰æ•ˆé™ä½äº†å¯¹å¤§è§„æ¨¡è®­ç»ƒæ•°æ®çš„éœ€æ±‚ï¼Œå¹¶æ˜¾è‘—æå‡äº†åœ¨é«˜åˆ†è¾¨ç‡å›¾åƒä¸Šçš„åˆ†å‰²ç²¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ 1024*1024 åƒç´ çš„å›¾åƒä¸Šå®ç°äº† 89.7% çš„ Dice coefficient å’Œ 80.01% çš„ IoUï¼Œæ€§èƒ½è¡¨ç°æ˜¾è‘—ä¼˜äº Attention U-Net æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨å¤–éƒ¨æ•°æ®é›†ä¸Šå±•ç°äº†æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›(generalizability)ï¼Œä¸ºå¼€å‘è„‰ç»œè†œç—£ä¸´åºŠè¯Šæ–­å†³ç­–æ”¯æŒç³»ç»ŸåŠè‡ªåŠ¨åŒ–ç—…ç¶æ ‡æ³¨å·¥å…·å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25549v1",
      "published_date": "2025-09-29 22:10:56 UTC",
      "updated_date": "2025-09-29 22:10:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:40:43.786705+00:00"
    },
    {
      "arxiv_id": "2509.25543v1",
      "title": "Aligning Multilingual Reasoning with Verifiable Semantics from a High-Resource Expert Model",
      "title_zh": "åŸºäºé«˜èµ„æºä¸“å®¶æ¨¡å‹çš„å¯éªŒè¯è¯­ä¹‰å®ç°å¤šè¯­è¨€æ¨ç†å¯¹é½",
      "authors": [
        "Fahim Faisal",
        "Kaiqiang Song",
        "Song Wang",
        "Simin Ma",
        "Shujian Liu",
        "Haoyun Deng",
        "Sathish Reddy Indurthi"
      ],
      "abstract": "While reinforcement learning has advanced the reasoning abilities of Large Language Models (LLMs), these gains are largely confined to English, creating a significant performance disparity across languages. To address this, we introduce Pivot-Based Reinforcement Learning with Semantically Verifiable Rewards (PB-RLSVR), a novel framework that enhances multilingual reasoning by circumventing the need for human-annotated data in target languages. Our approach employs a high-performing English LLM as a \"pivot\" model to generate reference responses for reasoning tasks. A multilingual model is then rewarded based on the semantic equivalence of its responses to the English reference, effectively transferring the pivot model's reasoning capabilities across languages. We investigate several cross-lingual semantic reward functions, including those based on embeddings and machine translation. Extensive experiments on a suite of multilingual reasoning benchmarks show that our method significantly narrows the performance gap between English and other languages, substantially outperforming traditional PPO baselines. Specifically, our PB-RLSVR framework improves the average multilingual performance of Llama-3.1-8B-Instruct and Qwen3-32B by 16.41% and 10.17%, respectively, demonstrating a powerful and data-efficient approach to building truly multilingual reasoning agents.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨éè‹±è¯­è¯­å¢ƒä¸‹æ¨ç†èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºPB-RLSVR (Pivot-Based Reinforcement Learning with Semantically Verifiable Rewards)çš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨é«˜æ€§èƒ½è‹±è¯­ä¸“å®¶æ¨¡å‹ä½œä¸ºâ€œè½´å¿ƒâ€(pivot)ç”Ÿæˆæ¨ç†ä»»åŠ¡çš„å‚è€ƒç­”æ¡ˆï¼Œé€šè¿‡è®¡ç®—å¤šè¯­è¨€æ¨¡å‹è¾“å‡ºä¸è‹±è¯­å‚è€ƒç­”æ¡ˆä¹‹é—´çš„è¯­ä¹‰ç­‰ä»·æ€§æ¥æä¾›å¥–åŠ±ã€‚ç ”ç©¶æ¢ç´¢äº†åŸºäºåµŒå…¥(embeddings)å’Œæœºå™¨ç¿»è¯‘ç­‰å¤šç§è·¨è¯­è¨€è¯­ä¹‰å¥–åŠ±å‡½æ•°ï¼Œä»è€Œåœ¨æ— éœ€ç›®æ ‡è¯­è¨€äººå·¥æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹å®ç°æ¨ç†èƒ½åŠ›çš„è·¨è¯­è¨€è¿ç§»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªå¤šè¯­è¨€æ¨ç†åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„PPOåŸºçº¿æ¨¡å‹ã€‚å…·ä½“è€Œè¨€ï¼ŒPB-RLSVRå°†Llama-3.1-8B-Instructå’ŒQwen3-32Bçš„å¤šè¯­è¨€å¹³å‡æ€§èƒ½åˆ†åˆ«æå‡äº†16.41%å’Œ10.17%ï¼Œä¸ºæ„å»ºé«˜æ•ˆã€è·¨è¯­è¨€çš„æ¨ç†æ™ºèƒ½ä½“æä¾›äº†ä¸€ç§å¼ºæœ‰åŠ›ä¸”æ•°æ®é«˜æ•ˆçš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25543v1",
      "published_date": "2025-09-29 22:03:11 UTC",
      "updated_date": "2025-09-29 22:03:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:40:44.790677+00:00"
    },
    {
      "arxiv_id": "2509.25541v1",
      "title": "Vision-Zero: Scalable VLM Self-Improvement via Strategic Gamified Self-Play",
      "title_zh": "Vision-Zeroï¼šåŸºäºç­–ç•¥åšå¼ˆåŒ–è‡ªæˆ‘å¯¹å¼ˆçš„å¯æ‰©å±•è§†è§‰è¯­è¨€æ¨¡å‹è‡ªæˆ‘æå‡",
      "authors": [
        "Qinsi Wang",
        "Bo Liu",
        "Tianyi Zhou",
        "Jing Shi",
        "Yueqian Lin",
        "Yiran Chen",
        "Hai Helen Li",
        "Kun Wan",
        "Wentian Zhao"
      ],
      "abstract": "Although reinforcement learning (RL) can effectively enhance the reasoning capabilities of vision-language models (VLMs), current methods remain heavily dependent on labor-intensive datasets that require extensive manual construction and verification, leading to extremely high training costs and consequently constraining the practical deployment of VLMs. To address this challenge, we propose Vision-Zero, a domain-agnostic framework enabling VLM self-improvement through competitive visual games generated from arbitrary image pairs. Specifically, Vision-Zero encompasses three main attributes: (1) Strategic Self-Play Framework: Vision-Zero trains VLMs in \"Who Is the Spy\"-style games, where the models engage in strategic reasoning and actions across multiple roles. Through interactive gameplay, models autonomously generate their training data without human annotation. (2) Gameplay from Arbitrary Images: Unlike existing gamified frameworks, Vision-Zero can generate games from arbitrary images, thereby enhancing the model's reasoning ability across diverse domains and showing strong generalization to different tasks. We demonstrate this versatility using three distinct types of image datasets: CLEVR-based synthetic scenes, charts, and real-world images. (3) Sustainable Performance Gain: We introduce Iterative Self-Play Policy Optimization (Iterative-SPO), a novel training algorithm that alternates between Self-Play and reinforcement learning with verifiable rewards (RLVR), mitigating the performance plateau often seen in self-play-only training and achieving sustained long-term improvements. Despite using label-free data, Vision-Zero achieves state-of-the-art performance on reasoning, chart question answering, and vision-centric understanding tasks, surpassing other annotation-based methods. Models and code has been released at https://github.com/wangqinsi1/Vision-Zero.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Vision-Zeroï¼Œä¸€ç§é¢†åŸŸæ— å…³çš„è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)è‡ªæˆ‘æ”¹è¿›æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¼ºåŒ–å­¦ä¹ (RL)åœ¨æå‡æ¨¡å‹æ¨ç†èƒ½åŠ›æ—¶å¯¹äººå·¥æ ‡æ³¨æ•°æ®çš„é«˜åº¦ä¾èµ–é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†ä»»æ„å›¾åƒå¯¹è½¬åŒ–ä¸ºç±»ä¼¼â€œè°æ˜¯å§åº•â€(Who Is the Spy)çš„ç­–ç•¥æ€§è§†è§‰æ¸¸æˆï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨å¤šè§’è‰²äº’åŠ¨ä¸­è‡ªä¸»ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œæ— éœ€äººå·¥å¹²é¢„ã€‚ä¸ä»¥å¾€å—é™çš„åšå¼ˆæ¡†æ¶ä¸åŒï¼ŒVision-Zeroèƒ½å¤Ÿåˆ©ç”¨åˆæˆåœºæ™¯ã€å›¾è¡¨åŠçœŸå®ä¸–ç•Œå›¾åƒç­‰ä»»æ„æ•°æ®æºç”Ÿæˆæ¸¸æˆï¼Œå±•ç°å‡ºæå¼ºçš„é¢†åŸŸé€šç”¨æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚ç ”ç©¶è¿˜å¼•å…¥äº†è¿­ä»£è‡ªåšå¼ˆç­–ç•¥ä¼˜åŒ–(Iterative-SPO)ç®—æ³•ï¼Œé€šè¿‡äº¤æ›¿è¿›è¡Œè‡ªåšå¼ˆ(Self-Play)ä¸å¸¦å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ (RLVR)ï¼Œæœ‰æ•ˆå…‹æœäº†è®­ç»ƒç“¶é¢ˆå¹¶å®ç°äº†æ€§èƒ½çš„æŒç»­å¢é•¿ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVision-Zeroåœ¨æ¨ç†ã€å›¾è¡¨é—®ç­”(Chart QA)åŠè§†è§‰æ ¸å¿ƒç†è§£ä»»åŠ¡ä¸Šå‡è¾¾åˆ°äº†SOTAæ°´å¹³ï¼Œå…¶è¡¨ç°ç”šè‡³è¶…è¶Šäº†ä¾èµ–äººå·¥æ ‡æ³¨çš„æ–¹æ³•ã€‚è¿™ä¸€æˆæœä¸ºå®ç°å¯æ‰©å±•ä¸”ä½æˆæœ¬çš„è§†è§‰è¯­è¨€æ¨¡å‹è‡ªæˆ‘è¿›åŒ–æä¾›äº†æ–°çš„èŒƒå¼ï¼Œå¹¶å·²åœ¨å¼€æºç¤¾åŒºå‘å¸ƒäº†ç›¸å…³ä»£ç ä¸æ¨¡å‹ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25541v1",
      "published_date": "2025-09-29 21:55:55 UTC",
      "updated_date": "2025-09-29 21:55:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:40:48.260608+00:00"
    },
    {
      "arxiv_id": "2509.25540v2",
      "title": "RadOnc-GPT: An Autonomous LLM Agent for Real-Time Patient Outcomes Labeling at Scale",
      "title_zh": "RadOnc-GPTï¼šç”¨äºå¤§è§„æ¨¡å®æ—¶æ‚£è€…ç»“å±€æ ‡æ³¨çš„è‡ªä¸»å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“",
      "authors": [
        "Jason Holmes",
        "Yuexing Hao",
        "Mariana Borras-Osorio",
        "Federico Mastroleo",
        "Santiago Romero Brufau",
        "Valentina Carducci",
        "Katie M Van Abel",
        "David M Routman",
        "Andrew Y. K. Foong",
        "Liv M Muller",
        "Satomi Shiraishi",
        "Daniel K Ebner",
        "Daniel J Ma",
        "Sameer R Keole",
        "Samir H Patel",
        "Mirek Fatyga",
        "Martin Bues",
        "Brad J Stish",
        "Yolanda I Garces",
        "Michelle A Neben Wittich",
        "Robert L Foote",
        "Sujay A Vora",
        "Nadia N Laack",
        "Mark R Waddle",
        "Wei Liu"
      ],
      "abstract": "Manual labeling limits the scale, accuracy, and timeliness of patient outcomes research in radiation oncology. We present RadOnc-GPT, an autonomous large language model (LLM)-based agent capable of independently retrieving patient-specific information, iteratively assessing evidence, and returning structured outcomes. Our evaluation explicitly validates RadOnc-GPT across two clearly defined tiers of increasing complexity: (1) a structured quality assurance (QA) tier, assessing the accurate retrieval of demographic and radiotherapy treatment plan details, followed by (2) a complex clinical outcomes labeling tier involving determination of mandibular osteoradionecrosis (ORN) in head-and-neck cancer patients and detection of cancer recurrence in independent prostate and head-and-neck cancer cohorts requiring combined interpretation of structured and unstructured patient data. The QA tier establishes foundational trust in structured-data retrieval, a critical prerequisite for successful complex clinical outcome labeling.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ”¾å°„è‚¿ç˜¤å­¦(Radiation Oncology)ä¸­æ‚£è€…ç»“æœæ ‡æ³¨åœ¨è§„æ¨¡ã€å‡†ç¡®æ€§å’ŒåŠæ—¶æ€§æ–¹é¢çš„å±€é™ï¼Œæå‡ºäº†RadOnc-GPTã€‚è¿™æ˜¯ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„è‡ªä¸»æ™ºèƒ½ä½“ï¼Œèƒ½å¤Ÿç‹¬ç«‹æ£€ç´¢æ‚£è€…ç‰¹å®šä¿¡æ¯ï¼Œå¹¶è¿›è¡Œè¿­ä»£è¯æ®è¯„ä¼°ï¼Œæœ€ç»ˆç”Ÿæˆç»“æ„åŒ–çš„ç»“æœæ ‡æ³¨ã€‚ç ”ç©¶é€šè¿‡ä¸¤ä¸ªå¤æ‚åº¦å±‚çº§å¯¹RadOnc-GPTè¿›è¡Œäº†éªŒè¯ï¼Œé¦–å…ˆæ˜¯è¯„ä¼°äººå£ç»Ÿè®¡å­¦å’Œæ”¾å°„æ²»ç–—è®¡åˆ’(Radiotherapy Treatment Plan)ç»†èŠ‚æ£€ç´¢å‡†ç¡®æ€§çš„ç»“æ„åŒ–è´¨é‡ä¿è¯(QA)å±‚çº§ã€‚éšåæ˜¯å¤æ‚çš„ä¸´åºŠç»“æœæ ‡æ³¨å±‚çº§ï¼Œæ¶‰åŠåˆ¤å®šå¤´é¢ˆç™Œæ‚£è€…çš„ä¸‹é¢Œéª¨æ”¾å°„æ€§éª¨åæ­»(Mandibular Osteoradionecrosis)ä»¥åŠåœ¨ç‹¬ç«‹çš„å‰åˆ—è…ºå’Œå¤´é¢ˆç™Œé˜Ÿåˆ—ä¸­æ£€æµ‹ç™Œç—‡å¤å‘ã€‚è¯¥è¿‡ç¨‹è¦æ±‚æ™ºèƒ½ä½“å…·å¤‡å¯¹ç»“æ„åŒ–å’Œéç»“æ„åŒ–æ‚£è€…æ•°æ®çš„ç»¼åˆè§£è¯»èƒ½åŠ›ã€‚QAå±‚çº§çš„æˆåŠŸéªŒè¯ä¸ºç»“æ„åŒ–æ•°æ®æ£€ç´¢å»ºç«‹äº†åŸºç¡€ä¿¡ä»»ï¼Œè¿™ä¸ä»…æ˜¯è¯¥æ¡†æ¶çš„æ ¸å¿ƒä¼˜åŠ¿ï¼Œä¹Ÿä¸ºå®ç°å¤§è§„æ¨¡ã€å®æ—¶çš„ä¸´åºŠç»“æœè‡ªåŠ¨æ ‡æ³¨å¥ å®šäº†å…³é”®å‰æã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25540v2",
      "published_date": "2025-09-29 21:55:50 UTC",
      "updated_date": "2025-12-12 22:32:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:40:49.271718+00:00"
    },
    {
      "arxiv_id": "2509.25539v1",
      "title": "Toxicity in Online Platforms and AI Systems: A Survey of Needs, Challenges, Mitigations, and Future Directions",
      "title_zh": "åœ¨çº¿å¹³å°ä¸äººå·¥æ™ºèƒ½ç³»ç»Ÿä¸­çš„æ¯’æ€§ï¼šéœ€æ±‚ã€æŒ‘æˆ˜ã€æ²»ç†åŠæœªæ¥æ–¹å‘ç»¼è¿°",
      "authors": [
        "Smita Khapre",
        "Melkamu Abay Mersha",
        "Hassan Shakil",
        "Jonali Baruah",
        "Jugal Kalita"
      ],
      "abstract": "The evolution of digital communication systems and the designs of online platforms have inadvertently facilitated the subconscious propagation of toxic behavior. Giving rise to reactive responses to toxic behavior. Toxicity in online content and Artificial Intelligence Systems has become a serious challenge to individual and collective well-being around the world. It is more detrimental to society than we realize. Toxicity, expressed in language, image, and video, can be interpreted in various ways depending on the context of usage. Therefore, a comprehensive taxonomy is crucial to detect and mitigate toxicity in online content, Artificial Intelligence systems, and/or Large Language Models in a proactive manner. A comprehensive understanding of toxicity is likely to facilitate the design of practical solutions for toxicity detection and mitigation. The classification in published literature has focused on only a limited number of aspects of this very complex issue, with a pattern of reactive strategies in response to toxicity. This survey attempts to generate a comprehensive taxonomy of toxicity from various perspectives. It presents a holistic approach to explain the toxicity by understanding the context and environment that society is facing in the Artificial Intelligence era. This survey summarizes the toxicity-related datasets and research on toxicity detection and mitigation for Large Language Models, social media platforms, and other online platforms, detailing their attributes in textual mode, focused on the English language. Finally, we suggest the research gaps in toxicity mitigation based on datasets, mitigation strategies, Large Language Models, adaptability, explainability, and evaluation.",
      "tldr_zh": "è¯¥ç»¼è¿°é’ˆå¯¹åœ¨çº¿å¹³å°å’Œäººå·¥æ™ºèƒ½ç³»ç»Ÿä¸­çš„æ¯’æ€§(Toxicity)è¡Œä¸ºè¿›è¡Œäº†ç³»ç»Ÿæ€§ç ”ç©¶ï¼ŒæŒ‡å‡ºæ•°å­—é€šä¿¡ç³»ç»Ÿçš„æ¼”è¿›åŠå¹³å°è®¾è®¡åœ¨æ— æ„ä¸­ä¿ƒè¿›äº†æœ‰å®³è¡Œä¸ºçš„ä¼ æ’­ã€‚è®ºæ–‡å¼ºè°ƒäº†æ¯’æ€§å¯¹ä¸ªäººå’Œé›†ä½“ç¦ç¥‰çš„ä¸¥é‡å¨èƒï¼Œå¹¶æŒ‡å‡ºå½“å‰çš„åˆ†ç±»ç ”ç©¶å¾€å¾€å±€é™äºæœ‰é™çš„ç»´åº¦ï¼Œä¸”å¤šé‡‡å–ååº”æ€§(Reactive)ç­–ç•¥è€Œéä¸»åŠ¨é¢„é˜²ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº†ä¸€å¥—å…¨é¢çš„æ¯’æ€§åˆ†ç±»æ³•(Taxonomy)ï¼Œæ—¨åœ¨é€šè¿‡ç†è§£äººå·¥æ™ºèƒ½æ—¶ä»£çš„ç¤¾ä¼šèƒŒæ™¯å’Œç¯å¢ƒï¼Œå®ç°å¯¹åœ¨çº¿å†…å®¹ã€äººå·¥æ™ºèƒ½ç³»ç»ŸåŠå¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)ä¸­æ¯’æ€§çš„ä¸»åŠ¨ç›‘æµ‹ä¸ç¼“è§£ã€‚è¯¥è®ºæ–‡è¯¦ç»†æ¢³ç†äº†é’ˆå¯¹ç¤¾äº¤åª’ä½“å’Œå…¶ä»–åœ¨çº¿å¹³å°çš„æ¯’æ€§æ•°æ®é›†ï¼Œå¹¶æ€»ç»“äº†å½“å‰åœ¨æ–‡æœ¬æ¨¡å¼ä¸‹çš„æ£€æµ‹ä¸ç¼“è§£æŠ€æœ¯ã€‚æœ€ç»ˆï¼Œç ”ç©¶è¯†åˆ«äº†æ¯’æ€§ç¼“è§£é¢†åŸŸçš„å…³é”®ç ”ç©¶ç©ºç™½ï¼Œæ¶µç›–äº†æ•°æ®é›†ã€ç¼“è§£ç­–ç•¥ã€å¤§è¯­è¨€æ¨¡å‹çš„é€‚åº”æ€§ã€å¯è§£é‡Šæ€§(Explainability)ä»¥åŠè¯„ä¼°ä½“ç³»ç­‰æ–¹é¢ï¼Œä¸ºæœªæ¥æ„å»ºå®‰å…¨å¥åº·çš„æ•°å­—ç”Ÿæ€æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.HC",
        "cs.SI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25539v1",
      "published_date": "2025-09-29 21:55:23 UTC",
      "updated_date": "2025-09-29 21:55:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:41:03.065438+00:00"
    },
    {
      "arxiv_id": "2509.25538v1",
      "title": "Steering an Active Learning Workflow Towards Novel Materials Discovery via Queue Prioritization",
      "title_zh": "é€šè¿‡é˜Ÿåˆ—ä¼˜å…ˆçº§æ’åºå¼•å¯¼é¢å‘æ–°ææ–™å‘ç°çš„ä¸»åŠ¨å­¦ä¹ å·¥ä½œæµ",
      "authors": [
        "Marcus Schwarting",
        "Logan Ward",
        "Nathaniel Hudson",
        "Xiaoli Yan",
        "Ben Blaiszik",
        "Santanu Chaudhuri",
        "Eliu Huerta",
        "Ian Foster"
      ],
      "abstract": "Generative AI poses both opportunities and risks for solving inverse design problems in the sciences. Generative tools provide the ability to expand and refine a search space autonomously, but do so at the cost of exploring low-quality regions until sufficiently fine tuned. Here, we propose a queue prioritization algorithm that combines generative modeling and active learning in the context of a distributed workflow for exploring complex design spaces. We find that incorporating an active learning model to prioritize top design candidates can prevent a generative AI workflow from expending resources on nonsensical candidates and halt potential generative model decay. For an existing generative AI workflow for discovering novel molecular structure candidates for carbon capture, our active learning approach significantly increases the number of high-quality candidates identified by the generative model. We find that, out of 1000 novel candidates, our workflow without active learning can generate an average of 281 high-performing candidates, while our proposed prioritization with active learning can generate an average 604 high-performing candidates.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºé˜Ÿåˆ—ä¼˜å…ˆçº§(Queue Prioritization)çš„ç®—æ³•ï¼Œæ—¨åœ¨ä¼˜åŒ–ç”¨äºæ–°ææ–™å‘ç°çš„ä¸»åŠ¨å­¦ä¹ (Active Learning)å·¥ä½œæµã€‚é’ˆå¯¹ç”Ÿæˆå¼AI(Generative AI)åœ¨å¤„ç†é€†å‘è®¾è®¡é—®é¢˜æ—¶æ˜“æ¢ç´¢ä½è´¨é‡åŒºåŸŸå’Œå‘ç”Ÿæ¨¡å‹è¡°å‡çš„é—®é¢˜ï¼Œè¯¥ç®—æ³•å°†ç”Ÿæˆæ¨¡å‹(Generative Modeling)ä¸ä¸»åŠ¨å­¦ä¹ ç›¸ç»“åˆï¼Œé€šè¿‡ä¼˜å…ˆå¤„ç†é¡¶å±‚å€™é€‰è®¾è®¡æ¥å¼•å¯¼æœç´¢ç©ºé—´ã€‚è¯¥æ–¹æ³•åœ¨åˆ†å¸ƒå¼å·¥ä½œæµä¸­è¿è¡Œï¼Œèƒ½å¤Ÿæœ‰æ•ˆé˜²æ­¢è®¡ç®—èµ„æºæµªè´¹åœ¨æ— æ•ˆçš„å€™é€‰ææ–™ä¸Šï¼Œå¹¶éåˆ¶ç”Ÿæˆæ¨¡å‹çš„æ½œåœ¨é€€åŒ–ã€‚ç ”ç©¶å›¢é˜Ÿå°†è¯¥æ–¹æ¡ˆåº”ç”¨äºç¢³æ•é›†(Carbon Capture)é¢†åŸŸçš„æ–°å‹åˆ†å­ç»“æ„æ¢ç´¢ï¼Œç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†é«˜ä»·å€¼å€™é€‰ç‰©çš„è¯†åˆ«ç‡ã€‚åœ¨1000ä¸ªéšæœºç”Ÿæˆçš„å€™é€‰ç‰©ä¸­ï¼Œä¼ ç»Ÿå·¥ä½œæµå¹³å‡ä»…äº§å‡º281ä¸ªé«˜æ€§èƒ½å€™é€‰ç‰©ï¼Œè€Œå¼•å…¥ä¸»åŠ¨å­¦ä¹ ä¼˜å…ˆçº§æœºåˆ¶åè¿™ä¸€æ•°å­—æå‡è‡³604ä¸ªã€‚è¿™ä¸€ç ”ç©¶ä¸ºåœ¨å¤æ‚è®¾è®¡ç©ºé—´ä¸­æ›´é«˜æ•ˆåœ°åˆ©ç”¨ç”Ÿæˆå¼å·¥å…·æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cond-mat.mtrl-sci",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25538v1",
      "published_date": "2025-09-29 21:51:13 UTC",
      "updated_date": "2025-09-29 21:51:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:41:03.968611+00:00"
    },
    {
      "arxiv_id": "2509.25533v1",
      "title": "VISOR++: Universal Visual Inputs based Steering for Large Vision Language Models",
      "title_zh": "VISOR++ï¼šåŸºäºé€šç”¨è§†è§‰è¾“å…¥çš„è§†è§‰è¯­è¨€å¤§æ¨¡å‹è½¬å‘æ§åˆ¶",
      "authors": [
        "Ravikumar Balakrishnan",
        "Mansi Phute"
      ],
      "abstract": "As Vision Language Models (VLMs) are deployed across safety-critical applications, understanding and controlling their behavioral patterns has become increasingly important. Existing behavioral control methods face significant limitations: system prompting approaches could easily be overridden by user instructions, while applying activation-based steering vectors requires invasive runtime access to model internals, precluding deployment with API-based services and closed-source models. Finding steering methods that transfer across multiple VLMs is still an open area of research. To this end, we introduce universal visual input based steering for output redirection (VISOR++), to achieve behavioral control through optimized visual inputs alone. We demonstrate that a single VISOR++ image can be generated for an ensemble of VLMs to emulate each of their steering vectors. By crafting universal visual inputs that induce target activation patterns, VISOR++ eliminates the need for runtime model access while remaining deployment-agnostic. This means that when an underlying model supports multimodal capability, model behaviors can be steered by inserting an image input replacing runtime steering vector based interventions. We first demonstrate the effectiveness of the VISOR++ images on open-access models such as LLaVA-1.5-7B and IDEFICS2-8B along three alignment directions: refusal, sycophancy and survival instinct. Both the model-specific steering images and the jointly optimized images achieve performance parity closely following that of steering vectors for both positive and negative steering tasks. We also show the promise of VISOR++ images in achieving directional behavioral shifts for unseen models including both open-access and closed-access ones. Furthermore, VISOR++ images are able to preserve 99.9% performance on 14,000 unrelated MMLU evaluation tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Vision Language Models (VLMs) è¡Œä¸ºæ§åˆ¶ä¸­ç³»ç»Ÿæç¤º(system prompting)æ˜“è¢«ç»•è¿‡ä»¥åŠæ¿€æ´»å¯¼å‘(activation-based steering)ä¾èµ–æ¨¡å‹å†…éƒ¨è®¿é—®çš„å±€é™æ€§ï¼Œæå‡ºäº† VISOR++ (Universal Visual Input based Steering for Output Redirection)ã€‚è¿™æ˜¯ä¸€ç§ä»…é€šè¿‡ä¼˜åŒ–åçš„é€šç”¨è§†è§‰è¾“å…¥å³å¯å®ç°è¡Œä¸ºæ§åˆ¶çš„æ–¹æ³•ï¼Œé€šè¿‡ç”Ÿæˆç‰¹å®šå›¾åƒæ¥æ¨¡æ‹Ÿ steering vectors çš„æ•ˆæœï¼Œæ¶ˆé™¤äº†å¯¹æ¨¡å‹åº•å±‚è®¿é—®çš„éœ€æ±‚ã€‚å®éªŒåœ¨ LLaVA-1.5-7B å’Œ IDEFICS2-8B æ¨¡å‹ä¸Šé’ˆå¯¹ refusalã€sycophancy å’Œ survival instinct ä¸‰ä¸ªå¯¹é½æ–¹å‘è¿›è¡Œäº†éªŒè¯ï¼Œç»“æœæ˜¾ç¤ºå…¶æ€§èƒ½ä¸ä¼ ç»Ÿçš„ steering vectors ç›¸å½“ã€‚VISOR++ ä¸ä»…åœ¨å¼€æºæ¨¡å‹ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¿˜èƒ½å¼•å¯¼é—­æºæ¨¡å‹äº§ç”Ÿå®šå‘è¡Œä¸ºåç§»ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨ 14,000 é¡¹ MMLU è¯„ä¼°ä»»åŠ¡ä¸­ä¿æŒäº† 99.9% çš„æ€§èƒ½ï¼Œè¯æ˜å…¶åœ¨å®ç°ç²¾ç¡®è¡Œä¸ºæ§åˆ¶çš„åŒæ—¶ï¼Œå‡ ä¹ä¸ä¼šå¯¹æ¨¡å‹çš„é€šç”¨èƒ½åŠ›äº§ç”Ÿè´Ÿé¢å½±å“ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25533v1",
      "published_date": "2025-09-29 21:43:18 UTC",
      "updated_date": "2025-09-29 21:43:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:41:12.177325+00:00"
    },
    {
      "arxiv_id": "2509.25532v1",
      "title": "Calibrating Verbalized Confidence with Self-Generated Distractors",
      "title_zh": "åˆ©ç”¨è‡ªç”Ÿæˆå¹²æ‰°é¡¹æ ¡å‡†è¨€è¯­åŒ–ç½®ä¿¡åº¦",
      "authors": [
        "Victor Wang",
        "Elias Stengel-Eskin"
      ],
      "abstract": "Calibrated confidence estimates are necessary for large language model (LLM) outputs to be trusted by human users. While LLMs can express their confidence in human-interpretable ways, verbalized LLM-generated confidence scores have empirically been found to be miscalibrated, reporting high confidence on instances with low accuracy and thereby harming trust and safety. We hypothesize that this overconfidence often stems from a given LLM's heightened suggestibility when faced with claims that it encodes little information about; we empirically validate this hypothesis, finding more suggestibility on lower-accuracy claims. Building on this finding, we introduce Distractor-Normalized Coherence (DINCO), which estimates and accounts for an LLM's suggestibility bias by having the model verbalize its confidence independently across several self-generated distractors (i.e. alternative claims), and normalizes by the total verbalized confidence. To further improve calibration, we leverage generator-validator disagreement, augmenting normalized validator confidence with a consistency-based estimate of generator confidence. Here, we frame the popular approach of self-consistency as leveraging coherence across sampled generations, and normalized verbalized confidence as leveraging coherence across validations on incompatible claims, allowing us to integrate these complementary dimensions of coherence into DINCO. Moreover, our analysis shows that DINCO provides less saturated -- and therefore more usable -- confidence estimates, and that further sampling alone cannot close the gap between DINCO and baselines, with DINCO at 10 inference calls outperforming self-consistency at 100.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å£å¤´åŒ–ç½®ä¿¡åº¦(verbalized confidence)è¯„ä¼°ä¸­æ™®éå­˜åœ¨çš„è¿‡åº¦è‡ªä¿¡å’Œå¤±æ ¡å‡†é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºDistractor-Normalized Coherence (DINCO)çš„æ–¹æ³•ã€‚ä½œè€…é€šè¿‡å®éªŒéªŒè¯äº†è¿‡åº¦è‡ªä¿¡æºäºæ¨¡å‹åœ¨å¤„ç†ç¼ºä¹ä¿¡æ¯çš„å£°æ˜æ—¶å…·æœ‰è¾ƒé«˜çš„æš—ç¤ºæ˜“æ„Ÿæ€§(suggestibility)ï¼Œä¸”è¿™ç§å€¾å‘åœ¨ä½å‡†ç¡®ç‡æƒ…å†µä¸‹æ›´ä¸ºä¸¥é‡ã€‚DINCOé€šè¿‡è®©æ¨¡å‹å¯¹å¤šä¸ªè‡ªæˆ‘ç”Ÿæˆçš„å¹²æ‰°é¡¹(self-generated distractors)ç‹¬ç«‹è¡¨è¾¾ç½®ä¿¡åº¦ï¼Œå¹¶åˆ©ç”¨æ€»ç½®ä¿¡åº¦è¿›è¡Œå½’ä¸€åŒ–ï¼Œä»è€Œæœ‰æ•ˆè¯†åˆ«å¹¶æŠµæ¶ˆäº†æš—ç¤ºåè§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†ç”Ÿæˆå™¨-éªŒè¯å™¨ä¸ä¸€è‡´æ€§(generator-validator disagreement)ï¼Œå°†åŸºäºä¸€è‡´æ€§çš„ç”Ÿæˆæ¦‚ç‡ä¸å½’ä¸€åŒ–çš„éªŒè¯ç½®ä¿¡åº¦ç›¸èåˆï¼Œè¿›ä¸€æ­¥æå‡äº†æ ¡å‡†ç²¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDINCOæä¾›äº†é¥±å’Œåº¦æ›´ä½ã€å¯ç”¨æ€§æ›´é«˜çš„ç½®ä¿¡åº¦ä¼°è®¡ï¼Œä¸”åœ¨æ•ˆç‡ä¸Šè¡¨ç°å“è¶Šã€‚æ€§èƒ½æµ‹è¯•æ˜¾ç¤ºï¼Œä»…éœ€10æ¬¡æ¨ç†è°ƒç”¨çš„DINCOåœ¨æ ¡å‡†è¡¨ç°ä¸Šå³å¯è¶…è¶Šè°ƒç”¨100æ¬¡çš„self-consistencyåŸºå‡†ï¼Œä¸ºå¢å¼ºLLMsçš„å¯ä¿¡åº¦ä¸å®‰å…¨æ€§æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Code: https://github.com/dubai03nsr/dinco",
      "pdf_url": "https://arxiv.org/pdf/2509.25532v1",
      "published_date": "2025-09-29 21:41:22 UTC",
      "updated_date": "2025-09-29 21:41:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:41:13.299937+00:00"
    },
    {
      "arxiv_id": "2510.03297v1",
      "title": "Convolutional Neural Nets vs Vision Transformers: A SpaceNet Case Study with Balanced vs Imbalanced Regimes",
      "title_zh": "å·ç§¯ç¥ç»ç½‘ç»œä¸è§†è§‰ Transformerï¼šåŸºäº SpaceNet å¹³è¡¡ä¸å¤±è¡¡åˆ†å¸ƒçš„æ¡ˆä¾‹ç ”ç©¶",
      "authors": [
        "Akshar Gothi"
      ],
      "abstract": "We present a controlled comparison of a convolutional neural network (EfficientNet-B0) and a Vision Transformer (ViT-Base) on SpaceNet under two label-distribution regimes: a naturally imbalanced five-class split and a balanced-resampled split with 700 images per class (70:20:10 train/val/test). With matched preprocessing (224x224, ImageNet normalization), lightweight augmentations, and a 40-epoch budget on a single NVIDIA P100, we report accuracy, macro-F1, balanced accuracy, per-class recall, and deployment metrics (model size and latency). On the imbalanced split, EfficientNet-B0 reaches 93% test accuracy with strong macro-F1 and lower latency; ViT-Base is competitive at 93% with a larger parameter count and runtime. On the balanced split, both models are strong; EfficientNet-B0 reaches 99% while ViT-Base remains competitive, indicating that balancing narrows architecture gaps while CNNs retain an efficiency edge. We release manifests, logs, and per-image predictions to support reproducibility.",
      "tldr_zh": "è¯¥ç ”ç©¶åœ¨ SpaceNet æ•°æ®é›†ä¸Šé’ˆå¯¹å·ç§¯ç¥ç»ç½‘ç»œ EfficientNet-B0 å’Œ Vision Transformer ViT-Base è¿›è¡Œäº†å—æ§å¯¹æ¯”å®éªŒï¼Œé‡ç‚¹æ¢è®¨äº†åœ¨è‡ªç„¶éå¹³è¡¡ (imbalanced) å’Œé‡é‡‡æ ·å¹³è¡¡ (balanced) ä¸¤ç§æ ‡ç­¾åˆ†å¸ƒä¸‹çš„æ¨¡å‹è¡¨ç°ã€‚ç ”ç©¶äººå‘˜é€šè¿‡ç»Ÿä¸€é¢„å¤„ç†ä¸å¢å¼ºæ‰‹æ®µï¼Œè¯¦ç»†è¯„ä¼°äº† Accuracyã€Macro-F1ã€Balanced Accuracy ä»¥åŠæ¨¡å‹è§„æ¨¡ä¸æ¨ç†å»¶è¿Ÿ (latency) ç­‰å…³é”®æŒ‡æ ‡ã€‚å®éªŒå‘ç°ï¼Œåœ¨éå¹³è¡¡ç¯å¢ƒä¸‹ EfficientNet-B0 è¾¾åˆ°äº† 93% çš„å‡†ç¡®ç‡ï¼Œä¸”å…·å¤‡æ›´ä½çš„å»¶è¿Ÿä¼˜åŠ¿ï¼Œè€Œ ViT-Base è™½å‚æ•°é‡è¾ƒå¤§ä½†ä¹Ÿè¡¨ç°å‡ºç›¸å½“çš„ç«äº‰åŠ›ã€‚åœ¨å¹³è¡¡åˆ†å¸ƒä¸‹ï¼Œä¸¤è€…çš„æ€§èƒ½å·®è·è¿›ä¸€æ­¥ç¼©å°ï¼ŒEfficientNet-B0 çš„å‡†ç¡®ç‡æå‡è‡³ 99%ï¼Œè¯æ˜äº†æ•°æ®å¹³è¡¡èƒ½æœ‰æ•ˆç¼“è§£æ¶æ„é—´çš„è¡¨ç°å·®å¼‚ã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥ç ”ç©¶æ­ç¤ºäº†åœ¨ç‰¹å®šé¥æ„Ÿåˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒCNN æ¶æ„ç›¸è¾ƒäº Transformer ä»ä¿ç•™ç€æ˜æ˜¾çš„æ•ˆç‡ä¼˜åŠ¿ (efficiency edge)ã€‚ç›¸å…³ä»£ç ã€æ—¥å¿—ä¸é¢„æµ‹ç»“æœå·²å…¬å¼€å‘å¸ƒï¼Œä¸ºè¯¥é¢†åŸŸçš„æ¨¡å‹è¯„ä¼°æä¾›äº†å¯å¤ç°çš„åŸºå‡†å‚è€ƒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "5 pages, 1 figure, 9 tables. Code and artifacts: https://github.com/akshar27/spacenet-cnn-vs-vit (release v1.0.1)",
      "pdf_url": "https://arxiv.org/pdf/2510.03297v1",
      "published_date": "2025-09-29 21:41:22 UTC",
      "updated_date": "2025-09-29 21:41:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:41:14.630244+00:00"
    },
    {
      "arxiv_id": "2509.25531v5",
      "title": "MixtureVitae: Open Web-Scale Pretraining Dataset With High Quality Instruction and Reasoning Data Built from Permissive-First Text Sources",
      "title_zh": "MixtureVitaeï¼šåŸºäºå®½æ¾è®¸å¯ä¼˜å…ˆæ–‡æœ¬æºã€åŒ…å«é«˜è´¨é‡æŒ‡ä»¤ä¸æ¨ç†æ•°æ®çš„å¼€æ”¾å¼äº’è”ç½‘è§„æ¨¡é¢„è®­ç»ƒæ•°æ®é›†",
      "authors": [
        "Huu Nguyen",
        "Victor May",
        "Harsh Raj",
        "Marianna Nezhurina",
        "Yishan Wang",
        "Yanqi Luo",
        "Minh Chien Vu",
        "Taishi Nakamura",
        "Ken Tsui",
        "Van Khue Nguyen",
        "David Salinas",
        "Aleksandra KrasnodÄ™bska",
        "Christoph Schuhmann",
        "Mats Leon Richter",
        "Xuan-Son",
        "Vu",
        "Jenia Jitsev"
      ],
      "abstract": "We present MixtureVitae, an open-access pretraining corpus built to minimize legal risk while providing strong downstream performance. MixtureVitae follows a permissive-first, risk-mitigated sourcing strategy that combines public-domain and permissively licensed text (e.g., CC-BY/Apache) with carefully justified low-risk additions (e.g., government works and EU TDM-eligible sources). MixtureVitae adopts a simple, single-stage pretraining recipe that integrates a large proportion of permissive synthetic instruction and reasoning data-signals typically introduced during post-training and generally scarce in permissive web corpora. We categorize all sources into a three-tier scheme that reflects varying risk levels and provide shard-level provenance metadata to enable risk-aware usage. In controlled experiments using the open-sci-ref training protocol (fixed architectures and hyperparameters; 50B and 300B token budgets across 130M-1.7B parameters), models trained on MixtureVitae consistently outperform other permissive datasets across a suite of standard benchmarks, and at the 1.7B-parameters/300B-tokens setting, they surpass FineWeb-Edu and approach DCLM late in training. Performance is particularly strong on MMLU and on math and code benchmarks: a 1.7B model pretrained on 300B MixtureVitae tokens matches or exceeds a strong 1.7B instruction-tuned baseline on GSM8K, HumanEval, and MBPP, despite using over 36 times fewer tokens (300B vs. ~11T). Supported by a thorough decontamination analysis, these results show that permissive-first data with high instruction and reasoning density, tiered by licensing and provenance-related risk, can provide a practical and risk-mitigated foundation for training capable LLMs, reducing reliance on broad web scrapes without sacrificing competitiveness. Code: https://github.com/ontocord/mixturevitae",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MixtureVitaeï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨æœ€å°åŒ–æ³•å¾‹é£é™©å¹¶æä¾›å¼ºåŠ²ä¸‹æ¸¸æ€§èƒ½çš„å¼€æ”¾è·å–Pretrainingè¯­æ–™åº“ã€‚è¯¥æ•°æ®é›†é‡‡ç”¨äº†Permissive-firstçš„é£é™©ç¼“è§£ç­–ç•¥ï¼Œç»“åˆäº†å…¬æœ‰é¢†åŸŸã€å®½æ¾æˆæƒæ–‡æœ¬ä»¥åŠç»è¿‡è®ºè¯çš„ä½é£é™©æ¥æºã€‚MixtureVitaeé‡‡ç”¨ç®€å•çš„å•é˜¶æ®µPretrainingæ–¹æ¡ˆï¼Œæ•´åˆäº†é«˜æ¯”ä¾‹çš„åˆæˆInstructionå’ŒReasoningæ•°æ®ï¼Œä»¥è§£å†³å®½æ¾ç½‘é¡µè¯­æ–™ä¸­æ­¤ç±»æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿå°†æ‰€æœ‰æ¥æºåˆ’åˆ†ä¸ºä¸‰å±‚é£é™©ä½“ç³»ï¼Œå¹¶æä¾›Shard-levelçš„å…ƒæ•°æ®ä»¥æ”¯æŒé£é™©æ„ŸçŸ¥çš„ä½¿ç”¨ã€‚å®éªŒè¡¨æ˜ï¼ŒMixtureVitaeåœ¨å¤šä¸ªæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºå…¶ä»–åŒç±»æ•°æ®é›†ï¼Œåœ¨1.7Bå‚æ•°è§„æ¨¡ä¸‹è¶…è¶Šäº†FineWeb-Eduå¹¶æ¥è¿‘DCLMã€‚è¯¥æ¨¡å‹åœ¨MMLUä»¥åŠæ•°å­¦å’Œä»£ç åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å°¤ä¸ºå¼ºåŠ²ï¼Œä»…ä½¿ç”¨300B Tokenä¾¿åœ¨GSM8Kã€HumanEvalå’ŒMBPPä¸Šè¾¾åˆ°æˆ–è¶…è¿‡äº†å¼ºåŠ›åŸºå‡†ã€‚è¿™é¡¹æˆæœè¯æ˜äº†å…·æœ‰é«˜Instructionå’ŒReasoningå¯†åº¦ä¸”æˆæƒé£é™©å¯æ§çš„æ•°æ®ï¼Œå¯ä»¥ä¸ºè®­ç»ƒé«˜æ€§èƒ½LLMsæä¾›å®ç”¨åŸºç¡€ï¼Œæœ‰æ•ˆå‡å°‘å¯¹å¤§è§„æ¨¡ç½‘é¡µæŠ“å–çš„ä¾èµ–ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Code: \\url{https://github.com/ontocord/mixturevitae}",
      "pdf_url": "https://arxiv.org/pdf/2509.25531v5",
      "published_date": "2025-09-29 21:40:10 UTC",
      "updated_date": "2026-01-12 18:44:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:41:25.662698+00:00"
    },
    {
      "arxiv_id": "2509.25530v1",
      "title": "Beyond Static Retrieval: Opportunities and Pitfalls of Iterative Retrieval in GraphRAG",
      "title_zh": "è¶…è¶Šé™æ€æ£€ç´¢ï¼šGraphRAG ä¸­è¿­ä»£æ£€ç´¢çš„æœºé‡ä¸é™·é˜±",
      "authors": [
        "Kai Guo",
        "Xinnan Dai",
        "Shenglai Zeng",
        "Harry Shomer",
        "Haoyu Han",
        "Yu Wang",
        "Jiliang Tang"
      ],
      "abstract": "Retrieval-augmented generation (RAG) is a powerful paradigm for improving large language models (LLMs) on knowledge-intensive question answering. Graph-based RAG (GraphRAG) leverages entity-relation graphs to support multi-hop reasoning, but most systems still rely on static retrieval. When crucial evidence, especially bridge documents that connect disjoint entities, is absent, reasoning collapses and hallucinations persist. Iterative retrieval, which performs multiple rounds of evidence selection, has emerged as a promising alternative, yet its role within GraphRAG remains poorly understood. We present the first systematic study of iterative retrieval in GraphRAG, analyzing how different strategies interact with graph-based backbones and under what conditions they succeed or fail. Our findings reveal clear opportunities: iteration improves complex multi-hop questions, helps promote bridge documents into leading ranks, and different strategies offer complementary strengths. At the same time, pitfalls remain: naive expansion often introduces noise that reduces precision, gains are limited on single-hop or simple comparison questions, and several bridge evidences still be buried too deep to be effectively used. Together, these results highlight a central bottleneck, namely that GraphRAG's effectiveness depends not only on recall but also on whether bridge evidence is consistently promoted into leading positions where it can support reasoning chains. To address this challenge, we propose Bridge-Guided Dual-Thought-based Retrieval (BDTR), a simple yet effective framework that generates complementary thoughts and leverages reasoning chains to recalibrate rankings and bring bridge evidence into leading positions. BDTR achieves consistent improvements across diverse GraphRAG settings and provides guidance for the design of future GraphRAG systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å›¾æ£€ç´¢å¢å¼ºç”Ÿæˆ(GraphRAG)ä¸­è¶…è¶Šé™æ€æ£€ç´¢çš„å¿…è¦æ€§ï¼Œé’ˆå¯¹å…¶åœ¨å¤šè·³æ¨ç†ä¸­ç¼ºå¤±å…³é”®è¯æ®å¯¼è‡´çš„å¹»è§‰é—®é¢˜ï¼Œé¦–æ¬¡å¯¹è¿­ä»£æ£€ç´¢(Iterative retrieval)è¿›è¡Œäº†ç³»ç»Ÿæ€§ç ”ç©¶ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œè¿­ä»£ç­–ç•¥è™½èƒ½æå‡å¤æ‚é—®é¢˜çš„æ€§èƒ½å¹¶ä¼˜åŒ–æ¡¥æ¥æ–‡æ¡£(Bridge documents)çš„æ’åï¼Œä½†ä¹Ÿé¢ä¸´å¼•å…¥å™ªå£°ã€å¯¹ç®€å•ä»»åŠ¡å¢ç›Šæœ‰é™åŠå…³é”®è¯æ®è¢«æ©åŸ‹ç­‰æŒ‘æˆ˜ã€‚ç ”ç©¶æ­ç¤ºäº†GraphRAGçš„æ•ˆèƒ½ç“¶é¢ˆåœ¨äºèƒ½å¦æœ‰æ•ˆå°†æ¡¥æ¥è¯æ®æå‡è‡³æ”¯æ’‘æ¨ç†é“¾çš„å…³é”®ä½ç½®ã€‚æ®æ­¤ï¼Œä½œè€…æå‡ºäº†Bridge-Guided Dual-Thought-based Retrieval (BDTR)æ¡†æ¶ï¼Œé€šè¿‡ç”Ÿæˆäº’è¡¥æ€ç»´å’Œåˆ©ç”¨æ¨ç†é“¾æ ¡å‡†æ’åï¼Œæ˜¾è‘—æå‡äº†æ£€ç´¢è´¨é‡ã€‚BDTRåœ¨ä¸åŒå®éªŒè®¾ç½®ä¸‹å‡å–å¾—äº†ç¨³å¥çš„æ€§èƒ½æå‡ï¼Œä¸ºæ„å»ºæ›´é«˜æ•ˆçš„GraphRAGç³»ç»Ÿæä¾›äº†é‡è¦æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25530v1",
      "published_date": "2025-09-29 21:38:28 UTC",
      "updated_date": "2025-09-29 21:38:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:41:24.062695+00:00"
    },
    {
      "arxiv_id": "2510.00072v1",
      "title": "Geo-R1: Unlocking VLM Geospatial Reasoning with Cross-View Reinforcement Learning",
      "title_zh": "Geo-R1ï¼šé€šè¿‡è·¨è§†å›¾å¼ºåŒ–å­¦ä¹ é‡Šæ”¾è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„åœ°ç†ç©ºé—´æ¨ç†èƒ½åŠ›",
      "authors": [
        "Chenhui Xu",
        "Fuxun Yu",
        "Michael J. Bianco",
        "Jacob Kovarskiy",
        "Raphael Tang",
        "Qi Zhang",
        "Zirui Xu",
        "Will LeVine",
        "Brandon Dubbs",
        "Heming Liao",
        "Cassandra Burgess",
        "Suvam Bag",
        "Jay Patravali",
        "Rupanjali Kukal",
        "Mikael Figueroa",
        "Rishi Madhok",
        "Nikolaos Karianakis",
        "Jinjun Xiong"
      ],
      "abstract": "We introduce Geo-R1, a reasoning-centric post-training framework that unlocks geospatial reasoning in vision-language models by combining thinking scaffolding and elevating. In the scaffolding stage, Geo-R1 instills a ``geospatial thinking paradigm\" via supervised fine-tuning on synthetic chain-of-thought exemplars, enabling models to connect visual cues with geographic priors without costly human reasoning annotations. In the elevating stage, it uses GRPO-based reinforcement learning on a weakly-supervised cross-view pairing proxy. This design supplies a verifiable and scalable reward signal: teaching models to capture and reconcile features across modalities, and harnessing reasoning for accurate prediction. Geo-R1 extends geospatial modeling from domain pretraining / supervised finetuning to reasoning-first post-training, and achieves state-of-the-art performance across various geospatial reasoning benchmarks. Our model is available at https://huggingface.co/miniHui/Geo-R1.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Geo-R1ï¼Œä¸€ç§ä»¥æ¨ç†ä¸ºæ ¸å¿ƒçš„åè®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ç»“åˆæ€ç»´è„šæ‰‹æ¶ä¸èƒ½åŠ›æå‡æ¥è§£é”è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)çš„åœ°ç†ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚åœ¨è„šæ‰‹æ¶é˜¶æ®µï¼Œè¯¥æ¡†æ¶é€šè¿‡åœ¨åˆæˆçš„é“¾å¼æ€ç»´(Chain-of-Thought)ç¤ºä¾‹ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æ— éœ€æ˜‚è´µäººå·¥æ ‡æ³¨çš„æƒ…å†µä¸‹å°†è§†è§‰çº¿ç´¢ä¸åœ°ç†å…ˆéªŒç›¸è”ç³»ã€‚åœ¨æå‡é˜¶æ®µï¼ŒGeo-R1é‡‡ç”¨åŸºäºGRPOçš„å¼ºåŒ–å­¦ä¹ ï¼Œåˆ©ç”¨å¼±ç›‘ç£çš„è·¨è§†å›¾é…å¯¹ä»£ç†æä¾›å¯æ‰©å±•ä¸”å¯éªŒè¯çš„å¥–åŠ±ä¿¡å·ã€‚è¯¥è®¾è®¡ä¿ƒä½¿æ¨¡å‹æ•è·å¹¶åè°ƒè·¨æ¨¡æ€ç‰¹å¾ï¼Œä»è€Œåˆ©ç”¨æ¨ç†å®ç°ç²¾å‡†çš„åœ°ç†é¢„æµ‹ã€‚Geo-R1æˆåŠŸå°†åœ°ç†ç©ºé—´å»ºæ¨¡ä»ä¼ ç»Ÿçš„é¢†åŸŸé¢„è®­ç»ƒè½¬å‘æ¨ç†ä¼˜å…ˆçš„åè®­ç»ƒæ¨¡å¼ï¼Œå¹¶åœ¨å¤šä¸ªåœ°ç†ç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›(SOTA)çš„æ€§èƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00072v1",
      "published_date": "2025-09-29 21:34:55 UTC",
      "updated_date": "2025-09-29 21:34:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:41:26.357851+00:00"
    },
    {
      "arxiv_id": "2509.25528v2",
      "title": "LLM-RG: Referential Grounding in Outdoor Scenarios using Large Language Models",
      "title_zh": "LLM-RGï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å®¤å¤–åœºæ™¯æŒ‡ä»£å®šä½",
      "authors": [
        "Pranav Saxena",
        "Avigyan Bhattacharya",
        "Ji Zhang",
        "Wenshan Wang"
      ],
      "abstract": "Referential grounding in outdoor driving scenes is challenging due to large scene variability, many visually similar objects, and dynamic elements that complicate resolving natural-language references (e.g., \"the black car on the right\"). We propose LLM-RG, a hybrid pipeline that combines off-the-shelf vision-language models for fine-grained attribute extraction with large language models for symbolic reasoning. LLM-RG processes an image and a free-form referring expression by using an LLM to extract relevant object types and attributes, detecting candidate regions, generating rich visual descriptors with a VLM, and then combining these descriptors with spatial metadata into natural-language prompts that are input to an LLM for chain-of-thought reasoning to identify the referent's bounding box. Evaluated on the Talk2Car benchmark, LLM-RG yields substantial gains over both LLM and VLM-based baselines. Additionally, our ablations show that adding 3D spatial cues further improves grounding. Our results demonstrate the complementary strengths of VLMs and LLMs, applied in a zero-shot manner, for robust outdoor referential grounding.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æˆ·å¤–é©¾é©¶åœºæ™¯ä¸­å› åœºæ™¯å¤šå˜ã€ç‰©ä½“è§†è§‰ç›¸ä¼¼åº¦é«˜ä»¥åŠåŠ¨æ€å…ƒç´ å¹²æ‰°å¯¼è‡´çš„Referential Groundingéš¾é¢˜ï¼Œæå‡ºäº†åä¸ºLLM-RGçš„æ··åˆæ¡†æ¶ã€‚LLM-RGç»“åˆäº†ç°æˆçš„Vision-Language Models (VLMs)ç”¨äºç»†ç²’åº¦å±æ€§æå–ï¼Œä»¥åŠLarge Language Models (LLMs)ç”¨äºç¬¦å·æ¨ç†ã€‚è¯¥æ¡†æ¶é¦–å…ˆåˆ©ç”¨LLMæå–ç›¸å…³çš„ç‰©ä½“ç±»å‹å’Œå±æ€§å¹¶æ£€æµ‹å€™é€‰åŒºåŸŸï¼Œéšåé€šè¿‡VLMç”Ÿæˆä¸°å¯Œçš„è§†è§‰æè¿°ç¬¦ï¼Œå¹¶å°†å…¶ä¸ç©ºé—´å…ƒæ•°æ®æ•´åˆä¸ºè‡ªç„¶è¯­è¨€æç¤ºè¯ã€‚é€šè¿‡LLMçš„é“¾å¼æ€ç»´æ¨ç†(Chain-of-Thought)ï¼Œç³»ç»Ÿèƒ½å¤Ÿç²¾å‡†è¯†åˆ«ç›®æ ‡ç‰©ä½“çš„Bounding Boxã€‚åœ¨Talk2CaråŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒLLM-RGçš„æ€§èƒ½æ˜¾è‘—ä¼˜äºå•çº¯åŸºäºLLMæˆ–VLMçš„åŸºå‡†æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæ¶ˆèå®éªŒè¯æ˜å¼•å…¥3Dç©ºé—´çº¿ç´¢å¯è¿›ä¸€æ­¥æå‡å®šä½æ•ˆæœï¼Œå……åˆ†å±•ç¤ºäº†VLMsä¸LLMsåœ¨é›¶æ ·æœ¬(Zero-shot)æˆ·å¤–åœºæ™¯å®šä½ä¸­çš„äº’è¡¥ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Human-aware Embodied AI Workshop @ IROS 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.25528v2",
      "published_date": "2025-09-29 21:32:54 UTC",
      "updated_date": "2025-10-21 00:32:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:41:28.558695+00:00"
    },
    {
      "arxiv_id": "2509.25524v2",
      "title": "Economic Competition, EU Regulation, and Executive Orders: A Framework for Discussing AI Policy Implications in CS Courses",
      "title_zh": "ç»æµç«äº‰ã€EU ç›‘ç®¡ä¸è¡Œæ”¿å‘½ä»¤ï¼šè®¡ç®—æœºç§‘å­¦è¯¾ç¨‹ä¸­äººå·¥æ™ºèƒ½æ”¿ç­–å½±å“çš„è®¨è®ºæ¡†æ¶",
      "authors": [
        "James Weichert",
        "Hoda Eldardiry"
      ],
      "abstract": "The growth and permeation of artificial intelligence (AI) technologies across society has drawn focus to the ways in which the responsible use of these technologies can be facilitated through AI governance. Increasingly, large companies and governments alike have begun to articulate and, in some cases, enforce governance preferences through AI policy. Yet existing literature documents an unwieldy heterogeneity in ethical principles for AI governance, while our own prior research finds that discussions of the implications of AI policy are not yet present in the computer science (CS) curriculum. In this context, overlapping jurisdictions and even contradictory policy preferences across private companies, local, national, and multinational governments create a complex landscape for AI policy which, we argue, will require AI developers able adapt to an evolving regulatory environment. Preparing computing students for the new challenges of an AI-dominated technology industry is therefore a key priority for the CS curriculum.\n  In this discussion paper, we seek to articulate a framework for integrating discussions on the nascent AI policy landscape into computer science courses. We begin by summarizing recent AI policy efforts in the United States and European Union. Subsequently, we propose guiding questions to frame class discussions around AI policy in technical and non-technical (e.g., ethics) CS courses. Throughout, we emphasize the connection between normative policy demands and still-open technical challenges relating to their implementation and enforcement through code and governance structures. This paper therefore represents a valuable contribution towards bridging research and discussions across the areas of AI policy and CS education, underlining the need to prepare AI engineers to interact with and adapt to societal policy preferences.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººå·¥æ™ºèƒ½æ²»ç†ä¸æ”¿ç­–(AI policy)åœ¨è®¡ç®—æœºç§‘å­¦(CS)æ•™è‚²ä¸­çš„ç¼ºå¤±ç°çŠ¶ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªå°†AIæ”¿ç­–è®¨è®ºæ•´åˆè¿›CSè¯¾ç¨‹çš„æ•™å­¦æ¡†æ¶ã€‚ä¸ºäº†åº”å¯¹æ—¥ç›Šå¤æ‚çš„è·¨å›½ç›‘ç®¡ç¯å¢ƒå’Œä¼ä¸šæ”¿ç­–åå¥½ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨åŸ¹å…»å¼€å‘è€…é€‚åº”åŠ¨æ€æ³•å¾‹ç¯å¢ƒçš„èƒ½åŠ›ã€‚æ–‡ä¸­è¯¦ç»†æ€»ç»“äº†ç¾å›½ä¸æ¬§ç›Ÿ(EU)æœ€æ–°çš„æ”¿ç­–åŠ¨æ€ï¼Œå¹¶ä¸ºæŠ€æœ¯æ€§ä¸éæŠ€æœ¯æ€§CSè¯¾ç¨‹è®¾è®¡äº†å¼•å¯¼æ€§è®¨è®ºé—®é¢˜ã€‚ç ”ç©¶é‡ç‚¹å¼ºè°ƒäº†è§„èŒƒæ€§æ”¿ç­–è¦æ±‚ä¸ä»£ç å®ç°ã€æ²»ç†ç»“æ„ä¸­å°šæœªè§£å†³çš„æŠ€æœ¯æŒ‘æˆ˜ä¹‹é—´çš„å…³é”®å…³è”ã€‚è¯¥è®ºæ–‡ä¸ºå¼¥åˆAIæ”¿ç­–ç ”ç©¶ä¸CSæ•™è‚²ä¹‹é—´çš„é¸¿æ²Ÿåšå‡ºäº†é‡è¦è´¡çŒ®ï¼Œå¼ºè°ƒäº†åŸ¹å…»AIå·¥ç¨‹å¸ˆç†è§£å¹¶é€‚åº”ç¤¾ä¼šæ”¿ç­–åå¥½çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25524v2",
      "published_date": "2025-09-29 21:26:53 UTC",
      "updated_date": "2025-10-01 01:49:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:41:50.184998+00:00"
    },
    {
      "arxiv_id": "2509.25522v2",
      "title": "Understanding Generative Recommendation with Semantic IDs from a Model-scaling View",
      "title_zh": "ä»æ¨¡å‹ç¼©æ”¾è§†è§’ç†è§£åŸºäºè¯­ä¹‰ ID çš„ç”Ÿæˆå¼æ¨è",
      "authors": [
        "Jingzhe Liu",
        "Liam Collins",
        "Jiliang Tang",
        "Tong Zhao",
        "Neil Shah",
        "Clark Mingxuan Ju"
      ],
      "abstract": "Recent advancements in generative models have allowed the emergence of a promising paradigm for recommender systems (RS), known as Generative Recommendation (GR), which tries to unify rich item semantics and collaborative filtering signals. One popular modern approach is to use semantic IDs (SIDs), which are discrete codes quantized from the embeddings of modality encoders (e.g., large language or vision models), to represent items in an autoregressive user interaction sequence modeling setup (henceforth, SID-based GR). While generative models in other domains exhibit well-established scaling laws, our work reveals that SID-based GR shows significant bottlenecks while scaling up the model. In particular, the performance of SID-based GR quickly saturates as we enlarge each component: the modality encoder, the quantization tokenizer, and the RS itself. In this work, we identify the limited capacity of SIDs to encode item semantic information as one of the fundamental bottlenecks. Motivated by this observation, as an initial effort to obtain GR models with better scaling behaviors, we revisit another GR paradigm that directly uses large language models (LLMs) as recommenders (henceforth, LLM-as-RS). Our experiments show that the LLM-as-RS paradigm has superior model scaling properties and achieves up to 20 percent improvement over the best achievable performance of SID-based GR through scaling. We also challenge the prevailing belief that LLMs struggle to capture collaborative filtering information, showing that their ability to model user-item interactions improves as LLMs scale up. Our analyses on both SID-based GR and LLMs across model sizes from 44M to 14B parameters underscore the intrinsic scaling limits of SID-based GR and position LLM-as-RS as a promising path toward foundation models for GR.",
      "tldr_zh": "è¯¥é¡¹ç ”ç©¶ä»æ¨¡å‹ç¼©æ”¾ï¼ˆModel-scalingï¼‰çš„è§’åº¦æ·±å…¥æ¢è®¨äº†ç”Ÿæˆå¼æ¨èï¼ˆGenerative Recommendation, GRï¼‰çš„æ€§èƒ½è¡¨ç°ã€‚ç ”ç©¶æ­ç¤ºäº†åŸºäºè¯­ä¹‰IDï¼ˆSemantic IDs, SIDsï¼‰çš„æ¨èæ¨¡å‹ï¼ˆSID-based GRï¼‰åœ¨æ‰©å¤§è§„æ¨¡æ—¶å­˜åœ¨æ˜¾è‘—ç“¶é¢ˆï¼Œå…¶æ€§èƒ½åœ¨æ¨¡æ€ç¼–ç å™¨ã€åˆ†è¯å™¨åŠæ¨èç³»ç»Ÿæœ¬èº«å¢å¤§æ—¶ä¼šè¿…é€Ÿè¾¾åˆ°é¥±å’Œã€‚ä½œè€…é€šè¿‡åˆ†ææŒ‡å‡ºï¼ŒSIDsç¼–ç é¡¹ç›®è¯­ä¹‰ä¿¡æ¯çš„èƒ½åŠ›æœ‰é™æ˜¯åˆ¶çº¦å…¶æ‰©å±•æ€§çš„æ ¸å¿ƒå› ç´ ã€‚ä¸ä¹‹å½¢æˆå¯¹æ¯”çš„æ˜¯ï¼Œç›´æ¥ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ä½œä¸ºæ¨èå™¨ï¼ˆLLM-as-RSï¼‰çš„èŒƒå¼å±•ç°å‡ºäº†å“è¶Šçš„ç¼©æ”¾ç‰¹æ€§ï¼Œåœ¨æ€§èƒ½ä¸Šæ¯”SID-based GRçš„æœ€ä¼˜è¡¨ç°æå‡äº†çº¦20%ã€‚å®éªŒè¿›ä¸€æ­¥è¯å®ï¼Œéšç€æ¨¡å‹è§„æ¨¡ä»44Mæ‰©å±•è‡³14Bï¼ŒLLMsæ•è·ååŒè¿‡æ»¤ï¼ˆCollaborative Filteringï¼‰ä¿¡å·çš„èƒ½åŠ›ä¼šæ˜¾è‘—å¢å¼ºã€‚è¯¥ç ”ç©¶æœ€ç»ˆå®šä½LLM-as-RSä¸ºæ„å»ºç”Ÿæˆå¼æ¨èé¢†åŸŸåŸºç¡€æ¨¡å‹ï¼ˆFoundation Modelsï¼‰æ›´å…·å‰æ™¯çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25522v2",
      "published_date": "2025-09-29 21:24:17 UTC",
      "updated_date": "2025-10-03 01:21:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:42:11.889420+00:00"
    },
    {
      "arxiv_id": "2509.25504v1",
      "title": "XR Blocks: Accelerating Human-centered AI + XR Innovation",
      "title_zh": "XR Blocksï¼šåŠ é€Ÿä»¥äººä¸ºä¸­å¿ƒçš„ AI + XR èåˆåˆ›æ–°",
      "authors": [
        "David Li",
        "Nels Numan",
        "Xun Qian",
        "Yanhe Chen",
        "Zhongyi Zhou",
        "Evgenii Alekseev",
        "Geonsun Lee",
        "Alex Cooper",
        "Min Xia",
        "Scott Chung",
        "Jeremy Nelson",
        "Xiuxiu Yuan",
        "Jolica Dias",
        "Tim Bettridge",
        "Benjamin Hersh",
        "Michelle Huynh",
        "Konrad Piascik",
        "Ricardo Cabello",
        "David Kim",
        "Ruofei Du"
      ],
      "abstract": "We are on the cusp where Artificial Intelligence (AI) and Extended Reality (XR) are converging to unlock new paradigms of interactive computing. However, a significant gap exists between the ecosystems of these two fields: while AI research and development is accelerated by mature frameworks like JAX and benchmarks like LMArena, prototyping novel AI-driven XR interactions remains a high-friction process, often requiring practitioners to manually integrate disparate, low-level systems for perception, rendering, and interaction. To bridge this gap, we present XR Blocks, a cross-platform framework designed to accelerate human-centered AI + XR innovation. XR Blocks strives to provide a modular architecture with plug-and-play components for core abstraction in AI + XR: user, world, peers; interface, context, and agents. Crucially, it is designed with the mission of \"reducing frictions from idea to reality\", thus accelerating rapid prototyping of AI + XR apps. Built upon accessible technologies (WebXR, three.js, TensorFlow, Gemini), our toolkit lowers the barrier to entry for XR creators. We demonstrate its utility through a set of open-source templates, samples, and advanced demos, empowering the community to quickly move from concept to interactive XR prototype. Site: https://xrblocks.github.io",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† XR Blocksï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨åŠ é€Ÿä»¥äººä¸ºä¸­å¿ƒçš„ AI + XR åˆ›æ–°çš„è·¨å¹³å°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å½“å‰äººå·¥æ™ºèƒ½ä¸æ‰©å±•ç°å®ç”Ÿæ€ç³»ç»Ÿé›†æˆè¿‡ç¨‹ä¸­åŸå‹å¼€å‘æ‘©æ“¦åŠ›å¤§ã€ç³»ç»Ÿè€¦åˆå¤æ‚ç­‰é—®é¢˜ã€‚è¯¥æ¡†æ¶é‡‡ç”¨æ¨¡å—åŒ–æ¶æ„ï¼Œä¸º AI + XR çš„æ ¸å¿ƒæŠ½è±¡æä¾›å³æ’å³ç”¨çš„ç»„ä»¶ï¼Œå…·ä½“æ¶µç›–äº† userã€worldã€peers ä»¥åŠ interfaceã€contextã€agents ç­‰å…³é”®è¦ç´ ã€‚é€šè¿‡æ•´åˆ WebXRã€three.jsã€TensorFlow å’Œ Gemini ç­‰æˆç†ŸæŠ€æœ¯ï¼Œè¯¥å·¥å…·åŒ…æ˜¾è‘—é™ä½äº†åˆ›ä½œè€…çš„å‡†å…¥é—¨æ§›ï¼Œè‡´åŠ›äºç¼©çŸ­ä»åˆ›æ„åˆ°ç°å®çš„å®ç°è·¯å¾„ã€‚é€šè¿‡æä¾›ä¸€ç³»åˆ—å¼€æºæ¨¡æ¿å’Œæ¼”ç¤ºç¤ºä¾‹ï¼Œè¯¥ç ”ç©¶å±•ç¤ºäº†å…¶åœ¨å¸®åŠ©ç¤¾åŒºå¿«é€Ÿæ„å»ºäº¤äº’å¼ XR åŸå‹æ–¹é¢çš„å®ç”¨æ€§ï¼Œä¸ºè·¨å­¦ç§‘çš„äº¤äº’è®¡ç®—ç ”ç©¶æä¾›äº†é«˜æ•ˆå·¥å…·ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.GR",
        "cs.SE"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25504v1",
      "published_date": "2025-09-29 21:00:53 UTC",
      "updated_date": "2025-09-29 21:00:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:42:10.189681+00:00"
    },
    {
      "arxiv_id": "2509.25503v2",
      "title": "DeepFake Detection in Dyadic Video Calls using Point of Gaze Tracking",
      "title_zh": "åŸºäºæ³¨è§†ç‚¹è¿½è¸ªçš„åŒäººè§†é¢‘é€šè¯æ·±åº¦ä¼ªé€ æ£€æµ‹",
      "authors": [
        "Odin Kohler",
        "Rahul Vijaykumar",
        "Masudul H. Imtiaz"
      ],
      "abstract": "With recent advancements in deepfake technology, it is now possible to generate convincing deepfakes in real-time. Unfortunately, malicious actors have started to use this new technology to perform real-time phishing attacks during video meetings. The nature of a video call allows access to what the deepfake is ``seeing,'' that is, the screen displayed to the malicious actor. Using this with the estimated gaze from the malicious actors streamed video enables us to estimate where the deepfake is looking on screen, the point of gaze. Because the point of gaze during conversations is not random and is instead used as a subtle nonverbal communicator, it can be used to detect deepfakes, which are not capable of mimicking this subtle nonverbal communication. This paper proposes a real-time deepfake detection method adapted to this genre of attack, utilizing previously unavailable biometric information. We built our model based on explainable features selected after careful review of research on gaze patterns during dyadic conversations. We then test our model on a novel dataset of our creation, achieving an accuracy of 82\\%. This is the first reported method to utilize point-of-gaze tracking for deepfake detection.",
      "tldr_zh": "é’ˆå¯¹å®æ—¶è§†é¢‘ä¼šè®®ä¸­ DeepFake è¯±å‘çš„ç½‘ç»œé’“é±¼æ”»å‡»ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨æ³¨è§†ç‚¹è¿½è¸ª (Point of Gaze Tracking) çš„å®æ—¶æ£€æµ‹æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ†ææ”»å‡»è€…è§†é¢‘æµä¸­çš„ä¼°è®¡æ³¨è§†ç‚¹ï¼Œå¹¶ç»“åˆå…¶å±å¹•æ˜¾ç¤ºçš„è§†è§‰å†…å®¹ï¼Œæ¥æ¨æ–­ DeepFake åœ¨å±å¹•ä¸Šçš„æ³¨è§†ä½ç½®ã€‚ç”±äºäººç±»åœ¨åŒäººå¯¹è¯ (Dyadic Video Calls) ä¸­çš„æ³¨è§†è¡Œä¸ºå¹¶ééšæœºï¼Œè€Œæ˜¯ä½œä¸ºä¸€ç§å¾®å¦™çš„éè¯­è¨€äº¤æµæ‰‹æ®µï¼Œè€Œç›®å‰çš„ DeepFake æŠ€æœ¯å°šæ— æ³•æ¨¡ä»¿è¿™ç§å¤æ‚çš„ç”Ÿç‰©ç‰¹å¾ã€‚ç ”ç©¶åŸºäºå¯¹å¯¹è¯æ³¨è§†æ¨¡å¼çš„æ·±å…¥ç»¼è¿°ï¼Œæå–äº†å…·æœ‰å¯è§£é‡Šæ€§ (Explainable Features) çš„ç‰¹å¾æ¥æ„å»ºæ£€æµ‹æ¨¡å‹ã€‚åœ¨ç ”ç©¶å›¢é˜Ÿåˆ›å»ºçš„æ–°é¢–æ•°æ®é›†ä¸Šè¿›è¡Œçš„æµ‹è¯•ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹çš„æ£€æµ‹å‡†ç¡®ç‡è¾¾åˆ°äº† 82%ã€‚è¯¥ç ”ç©¶æ˜¯é¦–ä¸ªåˆ©ç”¨æ³¨è§†ç‚¹è¿½è¸ªæŠ€æœ¯è¿›è¡Œ DeepFake æ£€æµ‹çš„æ–¹æ¡ˆï¼Œä¸ºåº”å¯¹å®æ—¶è§†é¢‘è¯ˆéª—æä¾›äº†æ–°çš„ç”Ÿç‰©è¯†åˆ«ç»´åº¦ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25503v2",
      "published_date": "2025-09-29 20:59:31 UTC",
      "updated_date": "2026-01-10 16:42:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:42:16.300632+00:00"
    },
    {
      "arxiv_id": "2509.25498v1",
      "title": "Not Wrong, But Untrue: LLM Overconfidence in Document-Based Queries",
      "title_zh": "éé”™ï¼Œä½†å¤±å®ï¼šå¤§è¯­è¨€æ¨¡å‹åœ¨åŸºäºæ–‡æ¡£æŸ¥è¯¢ä¸­çš„è¿‡åº¦è‡ªä¿¡",
      "authors": [
        "Nick Hagar",
        "Wilma Agustianto",
        "Nicholas Diakopoulos"
      ],
      "abstract": "Large language models (LLMs) are increasingly used in newsroom workflows, but their tendency to hallucinate poses risks to core journalistic practices of sourcing, attribution, and accuracy. We evaluate three widely used tools - ChatGPT, Gemini, and NotebookLM - on a reporting-style task grounded in a 300-document corpus related to TikTok litigation and policy in the U.S. We vary prompt specificity and context size and annotate sentence-level outputs using a taxonomy to measure hallucination type and severity. Across our sample, 30% of model outputs contained at least one hallucination, with rates approximately three times higher for Gemini and ChatGPT (40%) than for NotebookLM (13%). Qualitatively, most errors did not involve invented entities or numbers; instead, we observed interpretive overconfidence - models added unsupported characterizations of sources and transformed attributed opinions into general statements. These patterns reveal a fundamental epistemological mismatch: While journalism requires explicit sourcing for every claim, LLMs generate authoritative-sounding text regardless of evidentiary support. We propose journalism-specific extensions to existing hallucination taxonomies and argue that effective newsroom tools need architectures that enforce accurate attribution rather than optimize for fluency.",
      "tldr_zh": "è¯¥ç ”ç©¶è°ƒæŸ¥äº† ChatGPTã€Gemini å’Œ NotebookLM åœ¨æ–°é—»å·¥ä½œæµç¨‹ä¸­çš„åº”ç”¨è¡¨ç°ï¼Œé‡ç‚¹åˆ†æäº†å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) çš„å¹»è§‰é—®é¢˜å¯¹æ–°é—»å½’å±ä¸å‡†ç¡®æ€§æ„æˆçš„é£é™©ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨ 300 ä»½å…³äº TikTok æ³•å¾‹è¯‰è®¼çš„æ–‡æ¡£è¯­æ–™åº“è¿›è¡Œäº†æµ‹è¯•ï¼Œå‘ç°çº¦ 30% çš„è¾“å‡ºåŒ…å«å¹»è§‰ï¼Œå…¶ä¸­ NotebookLM çš„å¹»è§‰ç‡ (13%) è¿œä½äº Gemini å’Œ ChatGPT (40%)ã€‚ç ”ç©¶å‘ç°ï¼Œå¤§å¤šæ•°é”™è¯¯å¹¶éæºäºè™šæ„å®ä½“ï¼Œè€Œæ˜¯è¡¨ç°ä¸ºè§£é‡Šæ€§è¿‡åº¦è‡ªä¿¡ (interpretive overconfidence)ï¼Œå³æ¨¡å‹ä¼šå¯¹æ¥æºæ·»åŠ æ— ä¾æ®çš„ç‰¹å¾æè¿°ï¼Œæˆ–å°†ä¸»è§‚è§‚ç‚¹è½¬åŒ–ä¸ºæ™®éé™ˆè¿°ã€‚è¿™æ­ç¤ºäº†æ–°é—»ä¸šå¯¹æ˜ç¡®æ¥æº (sourcing) çš„ä¸¥è‹›è¦æ±‚ä¸ LLMs ä¼˜åŒ–æµç•…åº¦è€Œéè¯æ®æ”¯æŒçš„ç”Ÿæˆæœºåˆ¶ä¹‹é—´å­˜åœ¨è®¤è¯†è®ºå¤±é…ã€‚ç ”ç©¶æœ€åå»ºè®®æ‰©å±•ç°æœ‰çš„å¹»è§‰åˆ†ç±»æ³• (hallucination taxonomies)ï¼Œå¹¶å‘¼åå¼€å‘èƒ½å¤Ÿå¼ºåˆ¶æ‰§è¡Œå‡†ç¡®å½’å±çš„æ–°é—»ä¸“ç”¨å·¥å…·æ¶æ„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to Computation + Journalism Symposium 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.25498v1",
      "published_date": "2025-09-29 20:55:43 UTC",
      "updated_date": "2025-09-29 20:55:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:42:22.287905+00:00"
    },
    {
      "arxiv_id": "2509.25495v1",
      "title": "EMO-TTA: Improving Test-Time Adaptation of Audio-Language Models for Speech Emotion Recognition",
      "title_zh": "EMO-TTAï¼šæå‡è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ä¸­éŸ³é¢‘-è¯­è¨€æ¨¡å‹çš„æµ‹è¯•æ—¶è‡ªé€‚åº”èƒ½åŠ›",
      "authors": [
        "Jiacheng Shi",
        "Hongfei Du",
        "Y. Alicia Hong",
        "Ye Gao"
      ],
      "abstract": "Speech emotion recognition (SER) with audio-language models (ALMs) remains vulnerable to distribution shifts at test time, leading to performance degradation in out-of-domain scenarios. Test-time adaptation (TTA) provides a promising solution but often relies on gradient-based updates or prompt tuning, limiting flexibility and practicality. We propose Emo-TTA, a lightweight, training-free adaptation framework that incrementally updates class-conditional statistics via an Expectation-Maximization procedure for explicit test-time distribution estimation, using ALM predictions as priors. Emo-TTA operates on individual test samples without modifying model weights. Experiments on six out-of-domain SER benchmarks show consistent accuracy improvements over prior TTA baselines, demonstrating the effectiveness of statistical adaptation in aligning model predictions with evolving test distributions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹éŸ³é¢‘è¯­è¨€æ¨¡å‹(Audio-Language Models, ALMs)åœ¨è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«(Speech Emotion Recognition, SER)ä»»åŠ¡ä¸­å› æµ‹è¯•æ—¶åˆ†å¸ƒåç§»(distribution shifts)å¯¼è‡´æ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼Œæå‡ºäº†EMO-TTAæ¡†æ¶ã€‚EMO-TTAæ˜¯ä¸€ç§è½»é‡çº§ä¸”æ— éœ€è®­ç»ƒçš„æµ‹è¯•æ—¶è‡ªé€‚åº”(Test-Time Adaptation, TTA)æ¡†æ¶ï¼Œæ—¨åœ¨å…‹æœç°æœ‰åŸºäºæ¢¯åº¦æ›´æ–°æˆ–æç¤ºå¾®è°ƒ(prompt tuning)æ–¹æ³•çš„çµæ´»æ€§å±€é™ã€‚è¯¥æ¡†æ¶é€šè¿‡æœŸæœ›æœ€å¤§åŒ–(Expectation-Maximization)è¿‡ç¨‹å¢é‡æ›´æ–°ç±»æ¡ä»¶ç»Ÿè®¡é‡ï¼Œå¹¶åˆ©ç”¨ALMçš„é¢„æµ‹ç»“æœä½œä¸ºå…ˆéªŒä¿¡æ¯ï¼Œå®ç°å¯¹æµ‹è¯•æ—¶åˆ†å¸ƒçš„æ˜¾å¼ä¼°è®¡ã€‚EMO-TTAç›´æ¥ä½œç”¨äºå•ä¸ªæµ‹è¯•æ ·æœ¬ï¼Œæ— éœ€ä¿®æ”¹æ¨¡å‹æƒé‡ï¼Œæ˜¾è‘—æå‡äº†å®é™…åº”ç”¨ä¸­çš„ä¾¿æ·æ€§ã€‚åœ¨å…­ä¸ªé¢†åŸŸå¤–(out-of-domain) SERåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‡†ç¡®ç‡ä¸ŠæŒç»­ä¼˜äºç°æœ‰çš„TTAåŸºå‡†æ¨¡å‹ã€‚ç ”ç©¶éªŒè¯äº†é€šè¿‡ç»Ÿè®¡è‡ªé€‚åº”ä½¿æ¨¡å‹é¢„æµ‹ä¸ä¸æ–­æ¼”å˜çš„æµ‹è¯•åˆ†å¸ƒä¿æŒä¸€è‡´çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25495v1",
      "published_date": "2025-09-29 20:52:01 UTC",
      "updated_date": "2025-09-29 20:52:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:42:21.491178+00:00"
    },
    {
      "arxiv_id": "2509.25482v2",
      "title": "Message passing-based inference in an autoregressive active inference agent",
      "title_zh": "è‡ªå›å½’ä¸»åŠ¨æ¨ç†æ™ºèƒ½ä½“ä¸­åŸºäºæ¶ˆæ¯ä¼ é€’çš„æ¨ç†",
      "authors": [
        "Wouter M. Kouw",
        "Tim N. Nisslbeck",
        "Wouter L. N. Nuijten"
      ],
      "abstract": "We present the design of an autoregressive active inference agent in the form of message passing on a factor graph. Expected free energy is derived and distributed across a planning graph. The proposed agent is validated on a robot navigation task, demonstrating exploration and exploitation in a continuous-valued observation space with bounded continuous-valued actions. Compared to a classical optimal controller, the agent modulates action based on predictive uncertainty, arriving later but with a better model of the robot's dynamics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå› å­å›¾ (factor graph) æ¶ˆæ¯ä¼ é€’ (message passing) çš„è‡ªå›å½’ä¸»åŠ¨æ¨ç† (autoregressive active inference) æ™ºèƒ½ä½“è®¾è®¡æ–¹æ¡ˆã€‚é€šè¿‡æ¨å¯¼æœŸæœ›è‡ªç”±èƒ½ (Expected free energy) å¹¶å°†å…¶åˆ†å¸ƒåœ¨è§„åˆ’å›¾ (planning graph) ä¸­ï¼Œè¯¥æ™ºèƒ½ä½“å®ç°äº†å¯¹å¤æ‚å†³ç­–è¿‡ç¨‹çš„æœ‰æ•ˆå»ºæ¨¡ã€‚åœ¨æœºå™¨äººå¯¼èˆª (robot navigation) ä»»åŠ¡çš„éªŒè¯ä¸­ï¼Œæ™ºèƒ½ä½“å±•ç¤ºäº†åœ¨è¿ç»­è§‚æµ‹ç©ºé—´å’Œæœ‰ç•Œè¿ç»­åŠ¨ä½œç©ºé—´ä¸­å¹³è¡¡æ¢ç´¢ (exploration) ä¸åˆ©ç”¨ (exploitation) çš„èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„ä¼˜åŒ–æ§åˆ¶å™¨ (classical optimal controller) ç›¸æ¯”ï¼Œè¯¥æ™ºèƒ½ä½“èƒ½å¤Ÿæ ¹æ®é¢„æµ‹ä¸ç¡®å®šæ€§ (predictive uncertainty) çµæ´»è°ƒèŠ‚åŠ¨ä½œã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶æ™ºèƒ½ä½“åˆ°è¾¾ç›®æ ‡çš„æ—¶é—´ç›¸å¯¹è¾ƒæ™šï¼Œä½†å®ƒèƒ½é€šè¿‡äº¤äº’å­¦ä¹ å¹¶å»ºç«‹æ›´ç²¾ç¡®çš„æœºå™¨äººåŠ¨åŠ›å­¦ (robot's dynamics) æ¨¡å‹ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.RO",
        "eess.SY",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "14 pages, 4 figures, proceedings of the International Workshop on Active Inference 2025. Erratum v1: in Eq. (50), $p(y_t, Î˜, u_t \\mid y_{*}, \\mathcal{D}_k)$ should have been $p(y_t, Î˜\\mid u_t, y_{*}, \\mathcal{D}_k)$",
      "pdf_url": "https://arxiv.org/pdf/2509.25482v2",
      "published_date": "2025-09-29 20:38:09 UTC",
      "updated_date": "2026-01-19 14:46:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:42:31.394350+00:00"
    },
    {
      "arxiv_id": "2509.25480v1",
      "title": "Translation from Wearable PPG to 12-Lead ECG",
      "title_zh": "ä»å¯ç©¿æˆ´ PPG åˆ° 12 å¯¼è”å¿ƒç”µå›¾çš„ç¿»è¯‘",
      "authors": [
        "Hui Ji",
        "Wei Gao",
        "Pengfei Zhou"
      ],
      "abstract": "The 12-lead electrocardiogram (ECG) is the gold standard for cardiovascular monitoring, offering superior diagnostic granularity and specificity compared to photoplethysmography (PPG). However, existing 12-lead ECG systems rely on cumbersome multi-electrode setups, limiting sustained monitoring in ambulatory settings, while current PPG-based methods fail to reconstruct multi-lead ECG due to the absence of inter-lead constraints and insufficient modeling of spatial-temporal dependencies across leads. To bridge this gap, we introduce P2Es, an innovative demographic-aware diffusion framework designed to generate clinically valid 12-lead ECG from PPG signals via three key innovations. Specifically, in the forward process, we introduce frequency-domain blurring followed by temporal noise interference to simulate real-world signal distortions. In the reverse process, we design a temporal multi-scale generation module followed by frequency deblurring. In particular, we leverage KNN-based clustering combined with contrastive learning to assign affinity matrices for the reverse process, enabling demographic-specific ECG translation. Extensive experimental results show that P2Es outperforms baseline models in 12-lead ECG reconstruction.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¯ç©¿æˆ´å…‰ç”µå®¹ç§¯è„‰ææ³¢(PPG)åœ¨å¤šå¯¼è”é‡å»ºä¸­ç¼ºä¹å¯¼è”é—´çº¦æŸå’Œæ—¶ç©ºä¾èµ–æ€§å»ºæ¨¡çš„é—®é¢˜ï¼Œæå‡ºäº†P2Esâ€”â€”ä¸€ç§äººå£ç»Ÿè®¡æ„ŸçŸ¥(demographic-aware)çš„æ‰©æ•£æ¡†æ¶ï¼Œæ—¨åœ¨ä»PPGä¿¡å·ç”Ÿæˆä¸´åºŠæœ‰æ•ˆçš„12å¯¼è”å¿ƒç”µå›¾(ECG)ã€‚åœ¨æ­£å‘æ‰©æ•£è¿‡ç¨‹ä¸­ï¼ŒP2Eså¼•å…¥é¢‘åŸŸæ¨¡ç³Šå’Œæ—¶åŸŸå™ªå£°å¹²æ‰°æ¥æ¨¡æ‹ŸçœŸå®ä¸–ç•Œçš„ä¿¡å·å¤±çœŸï¼›åå‘è¿‡ç¨‹åˆ™é‡‡ç”¨äº†æ—¶åŸŸå¤šå°ºåº¦ç”Ÿæˆæ¨¡å—ä¸é¢‘åŸŸå»æ¨¡ç³ŠæŠ€æœ¯ã€‚æ­¤å¤–ï¼Œæ¨¡å‹é€šè¿‡ç»“åˆKNNèšç±»ä¸å¯¹æ¯”å­¦ä¹ (contrastive learning)åˆ†é…äº²å’ŒçŸ©é˜µï¼Œå®ç°äº†é’ˆå¯¹ç‰¹å®šäººå£ç»Ÿè®¡ç‰¹å¾çš„ECGç¿»è¯‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒP2Esåœ¨12å¯¼è”ECGé‡å»ºæ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ¨¡å‹ï¼Œä¸ºåœ¨ç§»åŠ¨ç¯å¢ƒä¸‹è¿›è¡Œé«˜ç²¾åº¦å¿ƒè¡€ç®¡ç›‘æµ‹æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages,10 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.25480v1",
      "published_date": "2025-09-29 20:36:24 UTC",
      "updated_date": "2025-09-29 20:36:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:42:41.287737+00:00"
    },
    {
      "arxiv_id": "2509.25479v2",
      "title": "Discontinuous Epitope Fragments as Sufficient Target Templates for Efficient Binder Design",
      "title_zh": "ä¸è¿ç»­è¡¨ä½ç‰‡æ®µä½œä¸ºé«˜æ•ˆç»“åˆå‰‚è®¾è®¡çš„å……è¶³é¶æ ‡æ¨¡æ¿",
      "authors": [
        "Zhenfeng Deng",
        "Ruijie Hou",
        "Ningrui Xie",
        "Mike Tyers",
        "MichaÅ‚ Koziarski"
      ],
      "abstract": "Recent advances in structure-based protein design have accelerated de novo binder generation, yet interfaces on large domains or spanning multiple domains remain challenging due to high computational cost and declining success with increasing target size. We hypothesized that protein folding neural networks (PFNNs) operate in a ``local-first'' manner, prioritizing local interactions while displaying limited sensitivity to global foldability. Guided by this hypothesis, we propose an epitope-only strategy that retains only the discontinuous surface residues surrounding the binding site. Compared to intact-domain workflows, this approach improves in silico success rates by up to 80% and reduces the average time per successful design by up to forty-fold, enabling binder design against previously intractable targets such as ClpP and ALS3. Building on this foundation, we further developed a tailored pipeline that incorporates a Monte Carlo-based evolution step to overcome local minima and a position-specific biased inverse folding step to refine sequence patterns. Together, these advances not only establish a generalizable framework for efficient binder design against structurally large and otherwise inaccessible targets, but also support the broader ``local-first'' hypothesis as a guiding principle for PFNN-based design.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è·¨è¶Šå¤šä¸ªç»“æ„åŸŸçš„å¤§å‹è›‹ç™½è´¨åœ¨ de novo binder generation ä¸­é¢ä¸´çš„è®¡ç®—æˆæœ¬é«˜å’ŒæˆåŠŸç‡ä½ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†åŸºäºè›‹ç™½è´¨æŠ˜å ç¥ç»ç½‘ç»œ (PFNNs) â€œå±€éƒ¨ä¼˜å…ˆ (local-first)â€å‡è®¾çš„è¡¨ä½å”¯ä¸€ (epitope-only) ç­–ç•¥ã€‚è¯¥æ–¹æ³•ä»…ä¿ç•™ç»“åˆä½ç‚¹å‘¨å›´çš„ä¸è¿ç»­è¡¨é¢æ®‹åŸºä½œä¸ºè®¾è®¡æ¨¡æ¿ï¼Œå¹¶å¼€å‘äº†ç»“åˆ Monte Carlo è¿›åŒ–æ­¥éª¤ä¸ä½ç½®ç‰¹å¼‚æ€§åç½®é€†å‘æŠ˜å  (position-specific biased inverse folding) çš„å®šåˆ¶åŒ–æµæ°´çº¿ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸å®Œæ•´ç»“æ„åŸŸå·¥ä½œæµç›¸æ¯”ï¼Œè¯¥ç­–ç•¥å°†è®¡ç®—æœºæ¨¡æ‹ŸæˆåŠŸç‡æé«˜äº† 80%ï¼Œå¹¶å°†å¹³å‡è®¾è®¡è€—æ—¶ç¼©çŸ­äº† 40 å€ã€‚é€šè¿‡æˆåŠŸå®ç°é’ˆå¯¹ ClpP å’Œ ALS3 ç­‰æ­¤å‰éš¾ä»¥å¤„ç†çš„ç›®æ ‡çš„ binder è®¾è®¡ï¼Œè¯¥ç ”ç©¶ä¸ä»…å»ºç«‹äº†ä¸€ä¸ªé€šç”¨ä¸”é«˜æ•ˆçš„è®¾è®¡æ¡†æ¶ï¼Œä¹ŸéªŒè¯äº†â€œå±€éƒ¨ä¼˜å…ˆâ€åŸåˆ™åœ¨åŸºäº PFNNs è›‹ç™½è´¨è®¾è®¡ä¸­çš„æŒ‡å¯¼æ„ä¹‰ã€‚",
      "categories": [
        "q-bio.BM",
        "cs.AI"
      ],
      "primary_category": "q-bio.BM",
      "comment": "Accepted by NeurIPS2025-AI4Science",
      "pdf_url": "https://arxiv.org/pdf/2509.25479v2",
      "published_date": "2025-09-29 20:33:32 UTC",
      "updated_date": "2025-10-01 20:52:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:42:37.399312+00:00"
    },
    {
      "arxiv_id": "2509.25475v2",
      "title": "TDHook: A Lightweight Framework for Interpretability",
      "title_zh": "TDHookï¼šé¢å‘å¯è§£é‡Šæ€§çš„è½»é‡çº§æ¡†æ¶",
      "authors": [
        "Yoann Poupart"
      ],
      "abstract": "Interpretability of Deep Neural Networks (DNNs) is a growing field driven by the study of vision and language models. Yet, some use cases, like image captioning, or domains like Deep Reinforcement Learning (DRL), require complex modelling, with multiple inputs and outputs or use composable and separated networks. As a consequence, they rarely fit natively into the API of popular interpretability frameworks. We thus present TDHook, an open-source, lightweight, generic interpretability framework based on $\\texttt{tensordict}$ and applicable to any $\\texttt{torch}$ model. It focuses on handling complex composed models which can be trained for Computer Vision, Natural Language Processing, Reinforcement Learning or any other domain. This library features ready-to-use methods for attribution, probing and a flexible get-set API for interventions, and is aiming to bridge the gap between these method classes to make modern interpretability pipelines more accessible. TDHook is designed with minimal dependencies, requiring roughly half as much disk space as $\\texttt{transformer_lens}$, and, in our controlled benchmark, achieves up to a $\\times$2 speed-up over $\\texttt{captum}$ when running integrated gradients for multi-target pipelines on both CPU and GPU. In addition, to value our work, we showcase concrete use cases of our library with composed interpretability pipelines in Computer Vision (CV) and Natural Language Processing (NLP), as well as with complex models in DRL.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† TDHookï¼Œä¸€ä¸ªå¼€æºã€è½»é‡çº§ä¸”é€šç”¨çš„å¯è§£é‡Šæ€§æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºè§£å†³å›¾åƒæ ‡æ³¨åŠ Deep Reinforcement Learning (DRL) ç­‰é¢†åŸŸä¸­å¤æ‚ç»„åˆæ¨¡å‹éš¾ä»¥é€‚é…ç°æœ‰ API çš„é—®é¢˜ã€‚TDHook åŸºäº tensordict æ„å»ºå¹¶é€‚ç”¨äºä»»ä½• torch æ¨¡å‹ï¼Œèƒ½å¤Ÿé«˜æ•ˆå¤„ç†æ¶‰åŠå¤šè¾“å…¥è¾“å‡ºæˆ–å¯ç»„åˆç½‘ç»œç»“æ„çš„å¤æ‚æ¨¡å‹ã€‚è¯¥æ¡†æ¶é›†æˆäº†å½’å›  (attribution)ã€æ¢æµ‹ (probing) åŠå¹²é¢„ (interventions) ç­‰åŠŸèƒ½ï¼Œå¹¶æä¾›çµæ´»çš„ get-set API ä»¥é™ä½ç°ä»£å¯è§£é‡Šæ€§æµç¨‹çš„ä½¿ç”¨é—¨æ§›ã€‚å®éªŒè¡¨æ˜ï¼ŒTDHook å ç”¨ç£ç›˜ç©ºé—´ä»…ä¸º transformer_lens çš„çº¦ä¸€åŠï¼Œä¸”åœ¨æ‰§è¡Œå¤šç›®æ ‡ integrated gradients ä»»åŠ¡æ—¶æ¯” captum å¿«è¾¾ 2 å€ã€‚é€šè¿‡åœ¨ Computer Vision (CV)ã€Natural Language Processing (NLP) åŠ DRL å¤æ‚æ¨¡å‹ä¸Šçš„åº”ç”¨å±•ç¤ºï¼ŒTDHook ä¸ºè·¨é¢†åŸŸçš„å¯è§£é‡Šæ€§ç ”ç©¶æä¾›äº†é«˜æ•ˆä¸”ä½ä¾èµ–çš„å·¥å…·æ”¯æŒã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25475v2",
      "published_date": "2025-09-29 20:28:43 UTC",
      "updated_date": "2026-01-09 16:00:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:42:44.187433+00:00"
    },
    {
      "arxiv_id": "2510.00071v2",
      "title": "ARS: Adaptive Reasoning Suppression for Efficient Large Reasoning Language Models",
      "title_zh": "ARSï¼šé¢å‘é«˜æ•ˆå¤§æ¨ç†è¯­è¨€æ¨¡å‹çš„è‡ªé€‚åº”æ¨ç†æŠ‘åˆ¶",
      "authors": [
        "Dongqi Zheng"
      ],
      "abstract": "Large Reasoning Language Models (LRLMs or LRMs) demonstrate remarkable capabilities in complex reasoning tasks, but suffer from significant computational inefficiencies due to overthinking phenomena. Existing efficient reasoning methods face the challenge of balancing reasoning quality with inference cost reduction. We propose \\textbf{Adaptive Reasoning Suppression (ARS)}, a novel training-free approach that dynamically suppresses redundant reasoning steps while preserving accuracy through adaptive certainty monitoring. ARS introduces a multi-checkpoint certainty estimation mechanism with progressive suppression thresholds, achieving superior efficiency compared to static suppression methods. Our extensive evaluation across mathematical reasoning benchmarks using multiple model architectures demonstrates that ARS achieves up to 53%, 46.1%, and 57.9% in token, latency and energy reduction, while maintaining or improving accuracy.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Adaptive Reasoning Suppression (ARS)ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºæé«˜Large Reasoning Language Models (LRLMs) æ•ˆç‡çš„æ–°å‹æ— éœ€è®­ç»ƒ(training-free)çš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç”±äºè¿‡åº¦æ€è€ƒ(overthinking)ç°è±¡å¯¼è‡´çš„è®¡ç®—ä½æ•ˆé—®é¢˜ã€‚ARSå¼•å…¥äº†å…·æœ‰æ¸è¿›å¼æŠ‘åˆ¶é˜ˆå€¼çš„å¤šæ£€æŸ¥ç‚¹ç¡®å®šæ€§ä¼°è®¡æœºåˆ¶(multi-checkpoint certainty estimation mechanism)ï¼Œé€šè¿‡è‡ªé€‚åº”ç¡®å®šæ€§ç›‘æµ‹åŠ¨æ€æŠ‘åˆ¶å†—ä½™æ¨ç†æ­¥éª¤ï¼Œä»è€Œåœ¨å¤§å¹…é™ä½æ¨ç†æˆæœ¬çš„åŒæ—¶ç¡®ä¿æ¨ç†è´¨é‡ã€‚åœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•å’Œæ¨¡å‹æ¶æ„ä¸Šçš„å¹¿æ³›è¯„ä¼°è¯æ˜ï¼ŒARSåœ¨ä¿æŒç”šè‡³æå‡å‡†ç¡®ç‡çš„å‰æä¸‹ï¼Œå®ç°äº†é«˜è¾¾53%çš„Tokenå‡å°‘ã€46.1%çš„å»¶è¿Ÿé™ä½ä»¥åŠ57.9%çš„èƒ½æ•ˆæå‡ã€‚ç›¸è¾ƒäºé™æ€æŠ‘åˆ¶æ–¹æ³•ï¼ŒARSé€šè¿‡åŠ¨æ€å¹³è¡¡æ¨ç†æ·±åº¦ä¸èµ„æºæ¶ˆè€—ï¼Œä¸ºå®ç°é«˜æ•ˆçš„å¤§è§„æ¨¡æ¨ç†è¯­è¨€æ¨¡å‹æä¾›äº†å“è¶Šçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by 39th NeurIPS - Foundations of Reasoning in Language Models",
      "pdf_url": "https://arxiv.org/pdf/2510.00071v2",
      "published_date": "2025-09-29 20:19:41 UTC",
      "updated_date": "2025-10-10 15:04:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:42:43.002251+00:00"
    },
    {
      "arxiv_id": "2509.25466v1",
      "title": "Data-Efficient Multitask DAgger",
      "title_zh": "æ•°æ®é«˜æ•ˆçš„å¤šä»»åŠ¡ DAgger",
      "authors": [
        "Haotian Fu",
        "Ran Gong",
        "Xiaohan Zhang",
        "Maria Vittoria Minniti",
        "Jigarkumar Patel",
        "Karl Schmeckpeper"
      ],
      "abstract": "Generalist robot policies that can perform many tasks typically require extensive expert data or simulations for training. In this work, we propose a novel Data-Efficient multitask DAgger framework that distills a single multitask policy from multiple task-specific expert policies. Our approach significantly increases the overall task success rate by actively focusing on tasks where the multitask policy underperforms. The core of our method is a performance-aware scheduling strategy that tracks how much each task's learning process benefits from the amount of data, using a Kalman filter-based estimator to robustly decide how to allocate additional demonstrations across tasks. We validate our approach on MetaWorld, as well as a suite of diverse drawer-opening tasks in IsaacLab. The resulting policy attains high performance across all tasks while using substantially fewer expert demonstrations, and the visual policy learned with our method in simulation shows better performance than naive DAgger and Behavior Cloning when transferring zero-shot to a real robot without using real data.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Data-Efficient Multitask DAgger æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä»å¤šä¸ªç‰¹å®šä»»åŠ¡çš„ä¸“å®¶ç­–ç•¥ä¸­è’¸é¦å‡ºå•ä¸€çš„å¤šä»»åŠ¡ç­–ç•¥ï¼Œè§£å†³é€šç”¨æœºå™¨äººç­–ç•¥è®­ç»ƒä¸­å¯¹ä¸“å®¶æ•°æ®è¿‡åº¦ä¾èµ–çš„é—®é¢˜ã€‚å…¶æ ¸å¿ƒé‡‡ç”¨äº†ä¸€ç§æ€§èƒ½æ„ŸçŸ¥è°ƒåº¦ç­–ç•¥ (performance-aware scheduling strategy)ï¼Œåˆ©ç”¨åŸºäº Kalman filter çš„ä¼°è®¡å™¨åŠ¨æ€è·Ÿè¸ªå„é¡¹ä»»åŠ¡çš„å­¦ä¹ å—ç›Šæƒ…å†µã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸»åŠ¨å…³æ³¨å¤šä»»åŠ¡ç­–ç•¥è¡¨ç°æ¬ ä½³çš„ä»»åŠ¡ï¼Œèƒ½å¤Ÿç¨³å¥åœ°åœ¨ä¸åŒä»»åŠ¡é—´ä¼˜åŒ–åˆ†é…é¢å¤–çš„ä¸“å®¶æ¼”ç¤ºæ•°æ®ã€‚å®éªŒåœ¨ MetaWorld å’Œ IsaacLab ä»¿çœŸç¯å¢ƒä¸­è¿›è¡Œäº†éªŒè¯ï¼Œç»“æœè¡¨æ˜è¯¥ç­–ç•¥åœ¨æ˜¾è‘—å‡å°‘ä¸“å®¶æ¼”ç¤ºæ¬¡æ•°çš„åŒæ—¶ï¼Œåœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šå‡å®ç°äº†æé«˜çš„æˆåŠŸç‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å­¦ä¹ åˆ°çš„è§†è§‰ç­–ç•¥åœ¨é›¶æ ·æœ¬è¿ç§» (zero-shot transfer) è‡³çœŸå®æœºå™¨äººæ—¶ï¼Œå…¶è¡¨ç°æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ DAgger å’Œ Behavior Cloning æ–¹æ³•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25466v1",
      "published_date": "2025-09-29 20:17:35 UTC",
      "updated_date": "2025-09-29 20:17:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:43:02.591705+00:00"
    },
    {
      "arxiv_id": "2509.25458v1",
      "title": "Plug-and-Play Emotion Graphs for Compositional Prompting in Zero-Shot Speech Emotion Recognition",
      "title_zh": "é¢å‘é›¶æ ·æœ¬è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ä¸­ç»„åˆå¼æç¤ºçš„å³æ’å³ç”¨æƒ…æ„Ÿå›¾",
      "authors": [
        "Jiacheng Shi",
        "Hongfei Du",
        "Y. Alicia Hong",
        "Ye Gao"
      ],
      "abstract": "Large audio-language models (LALMs) exhibit strong zero-shot performance across speech tasks but struggle with speech emotion recognition (SER) due to weak paralinguistic modeling and limited cross-modal reasoning. We propose Compositional Chain-of-Thought Prompting for Emotion Reasoning (CCoT-Emo), a framework that introduces structured Emotion Graphs (EGs) to guide LALMs in emotion inference without fine-tuning. Each EG encodes seven acoustic features (e.g., pitch, speech rate, jitter, shimmer), textual sentiment, keywords, and cross-modal associations. Embedded into prompts, EGs provide interpretable and compositional representations that enhance LALM reasoning. Experiments across SER benchmarks show that CCoT-Emo outperforms prior SOTA and improves accuracy over zero-shot baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€éŸ³é¢‘æ¨¡å‹ (LALMs) åœ¨è¯­éŸ³æƒ…æ„Ÿè¯†åˆ« (SER) ä¸­å› å‰¯è¯­è¨€å»ºæ¨¡å¼±å’Œè·¨æ¨¡æ€æ¨ç†å—é™è€Œè¡¨ç°ä¸ä½³çš„é—®é¢˜ï¼Œæå‡ºäº† CCoT-Emo (Compositional Chain-of-Thought Prompting for Emotion Reasoning) æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥ç»“æ„åŒ–çš„æƒ…æ„Ÿå›¾ (Emotion Graphs, EGs)ï¼Œåœ¨æ— éœ€å¾®è°ƒçš„æƒ…å†µä¸‹å¼•å¯¼ LALMs è¿›è¡Œæƒ…æ„Ÿæ¨ç†ã€‚æ¯ä¸ªæƒ…æ„Ÿå›¾ç¼–ç äº†åŒ…æ‹¬ pitchã€speech rateã€jitter å’Œ shimmer åœ¨å†…çš„ä¸ƒç§å£°å­¦ç‰¹å¾ï¼Œå¹¶ç»“åˆäº†æ–‡æœ¬æƒ…æ„Ÿã€å…³é”®è¯åŠè·¨æ¨¡æ€å…³è”ã€‚é€šè¿‡å°† EGs åµŒå…¥æç¤ºè¯ï¼Œè¯¥æ–¹æ³•ä¸ºæ¨¡å‹æä¾›äº†å¯è§£é‡Šä¸”å…·æœ‰ç»„åˆæ€§çš„è¡¨ç¤ºï¼Œä»è€Œæ˜¾è‘—å¢å¼ºäº†æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCCoT-Emo åœ¨å¤šä¸ª SER åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰çš„ SOTA æ¨¡å‹ï¼Œå¹¶å¤§å¹…æå‡äº† zero-shot åŸºå‡†çš„å‡†ç¡®ç‡ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25458v1",
      "published_date": "2025-09-29 20:06:03 UTC",
      "updated_date": "2025-09-29 20:06:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:43:17.190270+00:00"
    },
    {
      "arxiv_id": "2509.25455v1",
      "title": "PIPer: On-Device Environment Setup via Online Reinforcement Learning",
      "title_zh": "PIPerï¼šåŸºäºåœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„è®¾å¤‡ç«¯ç¯å¢ƒé…ç½®",
      "authors": [
        "Alexander Kovrigin",
        "Aleksandra Eliseeva",
        "Konstantin Grotov",
        "Egor Bogomolov",
        "Yaroslav Zharov"
      ],
      "abstract": "Environment setup-the process of configuring the system to work with a specific software project-represents a persistent challenge in Software Engineering (SE). Automated environment setup methods could assist developers by providing fully configured environments for arbitrary repositories without manual effort. This also helps SE researchers to scale execution-based benchmarks. However, recent studies reveal that even state-of-the-art Large Language Models (LLMs) achieve limited success in automating this task. To address this limitation, we tune a specialized model for environment setup. We combine supervised fine-tuning for generating correct Bash scripts and Reinforcement Learning with Verifiable Rewards (RLVR) to adapt it to the task of environment setup. On EnvBench-Python, our method enables Qwen3-8B (a model runnable on consumer hardware) to perform on par with larger models-Qwen3-32B and GPT-4o. The training code and model checkpoints are available online: https://github.com/JetBrains-Research/PIPer.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è½¯ä»¶å·¥ç¨‹ä¸­å¤æ‚çš„ç¯å¢ƒé…ç½®ï¼ˆEnvironment setupï¼‰æŒ‘æˆ˜ï¼Œæå‡ºäº†PIPeræ¡†æ¶ï¼Œæ—¨åœ¨ä¸ºä»»æ„ä»£ç ä»“åº“æä¾›å…¨è‡ªåŠ¨çš„ç³»ç»Ÿé…ç½®æ”¯æŒã€‚ä¸ºäº†å…‹æœç°æœ‰å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¯¥ä»»åŠ¡ä¸­æˆåŠŸç‡æœ‰é™çš„é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿå°†ç”¨äºç”ŸæˆBashè„šæœ¬çš„ç›‘ç£å¾®è°ƒï¼ˆSupervised Fine-Tuningï¼‰ä¸å…·æœ‰å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learning with Verifiable Rewards, RLVRï¼‰ç›¸ç»“åˆï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé€šè¿‡åœ¨çº¿å­¦ä¹ æœºåˆ¶æ›´å¥½åœ°é€‚åº”ç¯å¢ƒé…ç½®ä»»åŠ¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨EnvBench-PythonåŸºå‡†æµ‹è¯•ä¸­ï¼ŒåŸºäºPIPeræ–¹æ³•è®­ç»ƒçš„Qwen3-8Bæ¨¡å‹å±•ç°å‡ºæé«˜çš„æ•ˆç‡ï¼Œå…¶æ€§èƒ½å·²è¾¾åˆ°ä¸Qwen3-32Bå’ŒGPT-4oç­‰æ›´å¤§è§„æ¨¡æ¨¡å‹æŒå¹³çš„æ°´å¹³ã€‚è¯¥æ–¹æ³•ä¸ä»…è¯æ˜äº†åœ¨æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šå®ç°é«˜æ•ˆè‡ªåŠ¨åŒ–ç¯å¢ƒé…ç½®çš„å¯è¡Œæ€§ï¼Œè¿˜ä¸ºå¼€å‘è€…å‡å°‘æ‰‹åŠ¨åŠ³åŠ¨åŠç§‘ç ”äººå‘˜æ‰©å±•æ‰§è¡ŒåŸºå‡†æµ‹è¯•æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.LG",
      "comment": "Under review",
      "pdf_url": "https://arxiv.org/pdf/2509.25455v1",
      "published_date": "2025-09-29 20:03:05 UTC",
      "updated_date": "2025-09-29 20:03:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:43:14.095122+00:00"
    },
    {
      "arxiv_id": "2509.25454v3",
      "title": "DeepSearch: Overcome the Bottleneck of Reinforcement Learning with Verifiable Rewards via Monte Carlo Tree Search",
      "title_zh": "DeepSearchï¼šåˆ©ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢çªç ´å…·å¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ çš„ç“¶é¢ˆ",
      "authors": [
        "Fang Wu",
        "Weihao Xuan",
        "Heli Qi",
        "Ximing Lu",
        "Aaron Tu",
        "Li Erran Li",
        "Yejin Choi"
      ],
      "abstract": "Although RLVR has become an essential component for developing advanced reasoning skills in language models, contemporary studies have documented training plateaus after thousands of optimization steps, i.e., notable decreases in performance gains despite increased computational investment. This limitation stems from the sparse exploration patterns inherent in current RLVR practices, where models rely on limited rollouts that often miss critical reasoning paths and fail to provide systematic coverage of the solution space. We present DeepSearch, a framework that integrates Monte Carlo Tree Search (MCTS) directly into RLVR training. In contrast to existing methods that rely on tree search only at inference, DeepSearch embeds structured search into the training loop, enabling systematic exploration and fine-grained credit assignment across reasoning steps. Through training-time exploration, DeepSearch addresses the fundamental bottleneck of insufficient exploration, which leads to diminishing performance improvements over prolonged training steps. Our contributions include: (1) a global frontier selection strategy that prioritizes promising nodes across the search tree, (2) selection with entropy-based guidance that identifies confident paths for supervision, and (3) adaptive replay buffer training with solution caching for efficiency. Experiments on mathematical reasoning benchmarks show that DeepSearch achieves 62.95% average accuracy and establishes a new state-of-the-art for 1.5B reasoning models, while using 5.7x fewer GPU hours than extended training approaches. These results highlight the importance of strategic exploration over brute-force scaling and demonstrate the promise of algorithmic innovation for advancing RLVR methodologies. DeepSearch establishes a new direction for scaling reasoning capabilities through systematic search rather than prolonged computation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±(Reinforcement Learning with Verifiable Rewards, RLVR)åœ¨æ¨¡å‹æ¨ç†è®­ç»ƒä¸­å‡ºç°çš„è®­ç»ƒç“¶é¢ˆ(Training Plateaus)é—®é¢˜ï¼ŒæŒ‡å‡ºå…¶æ ¹æºåœ¨äºå½“å‰ç¨€ç–çš„æ¢ç´¢æ¨¡å¼æ— æ³•æœ‰æ•ˆè¦†ç›–å¤æ‚é—®é¢˜çš„è§£ç©ºé—´ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†DeepSearchæ¡†æ¶ï¼Œé€šè¿‡å°†è’™ç‰¹å¡æ´›æ ‘æœç´¢(Monte Carlo Tree Search, MCTS)ç›´æ¥åµŒå…¥RLVRè®­ç»ƒå¾ªç¯ï¼Œå®ç°äº†è·¨æ¨ç†æ­¥éª¤çš„ç³»ç»ŸåŒ–æ¢ç´¢å’Œç»†ç²’åº¦ä¿¡ç”¨åˆ†é…(Credit Assignment)ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†å…¨å±€å‰æ²¿é€‰æ‹©ç­–ç•¥(Global Frontier Selection)ã€åŸºäºç†µçš„å¼•å¯¼æœºåˆ¶(Entropy-based Guidance)ä»¥åŠå¸¦è§£ç¼“å­˜çš„è‡ªé€‚åº”å›æ”¾ç¼“å†²åŒº(Adaptive Replay Buffer)ç­‰æ ¸å¿ƒæŠ€æœ¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDeepSearchåœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­åŠ©åŠ›1.5Bæ¨¡å‹è¾¾åˆ°62.95%çš„å¹³å‡å‡†ç¡®ç‡ï¼Œåˆ·æ–°äº†åŒè§„æ¨¡æ¨ç†æ¨¡å‹çš„SOTAçºªå½•ï¼Œä¸”è®¡ç®—èµ„æºæ¶ˆè€—ä»…ä¸ºä¼ ç»Ÿå»¶é•¿è®­ç»ƒæ–¹æ³•çš„1/5.7ã€‚è¯¥æˆæœè¯æ˜äº†ç³»ç»ŸåŒ–æœç´¢ä¼˜äºæš´åŠ›æ‰©å±•(Brute-force Scaling)çš„ç­–ç•¥ï¼Œä¸ºé€šè¿‡ç®—æ³•åˆ›æ–°æå‡æ¨¡å‹æ¨ç†èƒ½åŠ›å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25454v3",
      "published_date": "2025-09-29 20:00:29 UTC",
      "updated_date": "2026-01-07 02:08:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:43:20.791777+00:00"
    },
    {
      "arxiv_id": "2509.25450v1",
      "title": "Multi-patch isogeometric neural solver for partial differential equations on computer-aided design domains",
      "title_zh": "é¢å‘è®¡ç®—æœºè¾…åŠ©è®¾è®¡ (CAD) åŸŸåå¾®åˆ†æ–¹ç¨‹çš„å¤šç‰‡ç­‰å‡ ä½•ç¥ç»æ±‚è§£å™¨",
      "authors": [
        "Moritz von Tresckow",
        "Ion Gabriel Ion",
        "Dimitrios Loukrezis"
      ],
      "abstract": "This work develops a computational framework that combines physics-informed neural networks with multi-patch isogeometric analysis to solve partial differential equations on complex computer-aided design geometries. The method utilizes patch-local neural networks that operate on the reference domain of isogeometric analysis. A custom output layer enables the strong imposition of Dirichlet boundary conditions. Solution conformity across interfaces between non-uniform rational B-spline patches is enforced using dedicated interface neural networks. Training is performed using the variational framework by minimizing the energy functional derived after the weak form of the partial differential equation. The effectiveness of the suggested method is demonstrated on two highly non-trivial and practically relevant use-cases, namely, a 2D magnetostatics model of a quadrupole magnet and a 3D nonlinear solid and contact mechanics model of a mechanical holder. The results show excellent agreement to reference solutions obtained with high-fidelity finite element solvers, thus highlighting the potential of the suggested neural solver to tackle complex engineering problems given the corresponding computer-aided design models.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†ä¸€ç§å°†ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œ(Physics-Informed Neural Networks, PINNs)ä¸å¤šè¡¥ä¸ç­‰å‡ ä½•åˆ†æ(multi-patch isogeometric analysis)ç›¸ç»“åˆçš„è®¡ç®—æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤æ‚è®¡ç®—æœºè¾…åŠ©è®¾è®¡(CAD)å‡ ä½•ä½“ä¸Šçš„åå¾®åˆ†æ–¹ç¨‹(PDEs)æ±‚è§£é—®é¢˜ã€‚è¯¥æ–¹æ³•åœ¨ç­‰å‡ ä½•åˆ†æçš„å‚è€ƒåŸŸä¸Šåˆ©ç”¨å±€éƒ¨è¡¥ä¸ç¥ç»ç½‘ç»œ(patch-local neural networks)è¿›è¡Œè¿ç®—ï¼Œå¹¶é€šè¿‡è‡ªå®šä¹‰è¾“å‡ºå±‚å®ç°äº†Dirichletè¾¹ç•Œæ¡ä»¶çš„å¼ºåŠ ã€‚ä¸ºäº†ç¡®ä¿éå‡åŒ€æœ‰ç†Bæ ·æ¡(NURBS)è¡¥ä¸ç•Œé¢é—´çš„è§£çš„ä¸€è‡´æ€§ï¼Œç ”ç©¶å¼•å…¥äº†ä¸“é—¨çš„ç•Œé¢ç¥ç»ç½‘ç»œ(interface neural networks)è¿›è¡Œçº¦æŸã€‚è®­ç»ƒè¿‡ç¨‹åŸºäºå˜åˆ†æ¡†æ¶ï¼Œé€šè¿‡æœ€å°åŒ–æ ¹æ®åå¾®åˆ†æ–¹ç¨‹å¼±å½¢å¼å¯¼å‡ºçš„èƒ½é‡æ³›å‡½(energy functional)æ¥å®ç°ã€‚é€šè¿‡å¯¹äºŒç»´å››æç£ä½“é™ç£å­¦å’Œä¸‰ç»´éçº¿æ€§å›ºä½“åŠæ¥è§¦åŠ›å­¦æ¨¡å‹çš„æµ‹è¯•ï¼Œç»“æœè¯æ˜è¯¥æ–¹æ³•ä¸é«˜ä¿çœŸæœ‰é™å…ƒ(finite element)æ±‚è§£å™¨çš„å‚è€ƒè§£é«˜åº¦ä¸€è‡´ï¼Œå±•ç¤ºäº†è¯¥ç¥ç»æ±‚è§£å™¨å¤„ç†å¤æ‚å·¥ç¨‹é—®é¢˜çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CE",
        "cs.AI",
        "math.NA",
        "physics.comp-ph"
      ],
      "primary_category": "cs.CE",
      "comment": "33 pages, 15 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.25450v1",
      "published_date": "2025-09-29 19:57:54 UTC",
      "updated_date": "2025-09-29 19:57:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:43:24.398687+00:00"
    },
    {
      "arxiv_id": "2509.25449v1",
      "title": "Joint Embeddings Go Temporal",
      "title_zh": "è”åˆåµŒå…¥èµ°å‘æ—¶åºåŒ–",
      "authors": [
        "Sofiane Ennadir",
        "Siavash Golkar",
        "Leopoldo Sarra"
      ],
      "abstract": "Self-supervised learning has seen great success recently in unsupervised representation learning, enabling breakthroughs in natural language and image processing. However, these methods often rely on autoregressive and masked modeling, which aim to reproduce masked information in the input, which can be vulnerable to the presence of noise or confounding variables. To address this problem, Joint-Embedding Predictive Architectures (JEPA) has been introduced with the aim to perform self-supervised learning in the latent space. To leverage these advancements in the domain of time series, we introduce Time Series JEPA (TS-JEPA), an architecture specifically adapted for time series representation learning. We validate TS-JEPA on both classification and forecasting, showing that it can match or surpass current state-of-the-art baselines on different standard datasets. Notably, our approach demonstrates a strong performance balance across diverse tasks, indicating its potential as a robust foundation for learning general representations. Thus, this work lays the groundwork for developing future time series foundation models based on Joint Embedding.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿè‡ªç›‘ç£å­¦ä¹ ï¼ˆSelf-supervised learningï¼‰ä¸­è‡ªå›å½’å’Œæ©ç å»ºæ¨¡æ˜“å—å™ªå£°åŠæ··æ‚å˜é‡å½±å“çš„ç¼ºé™·ï¼Œå¼•å…¥äº†åœ¨æ½œç©ºé—´ï¼ˆlatent spaceï¼‰è¿›è¡Œå­¦ä¹ çš„è”åˆåµŒå…¥é¢„æµ‹æ¶æ„ï¼ˆJoint-Embedding Predictive Architectures, JEPAï¼‰ï¼Œå¹¶æå‡ºäº†ä¸“é—¨é€‚é…æ—¶é—´åºåˆ—çš„ TS-JEPA æ¶æ„ã€‚TS-JEPA æ—¨åœ¨æå‡æ—¶é—´åºåˆ—è¡¨ç¤ºå­¦ä¹ çš„é²æ£’æ€§ï¼Œé€šè¿‡åœ¨æ½œç©ºé—´è€ŒéåŸå§‹è¾“å…¥ç©ºé—´è¿›è¡Œå»ºæ¨¡æ¥è·å–æ›´å…·åˆ¤åˆ«åŠ›çš„ç‰¹å¾ã€‚åœ¨åˆ†ç±»å’Œé¢„æµ‹ä»»åŠ¡çš„å®éªŒéªŒè¯ä¸­ï¼ŒTS-JEPA åœ¨å¤šä¸ªæ ‡å‡†æ•°æ®é›†ä¸Šè¾¾åˆ°æˆ–è¶…è¶Šäº†ç°æœ‰çš„ state-of-the-art åŸºå‡†æ¨¡å‹ã€‚è¯¥æ–¹æ³•åœ¨å¤šæ ·åŒ–ä»»åŠ¡ä¸­å±•ç°äº†å‡ºè‰²çš„æ€§èƒ½å¹³è¡¡ï¼Œä½“ç°äº†å…¶ä½œä¸ºæ„å»ºé€šç”¨è¡¨ç¤ºåŸºç¡€æ¨¡å‹çš„å·¨å¤§æ½œåŠ›ã€‚è¿™é¡¹ç ”ç©¶ä¸ºæœªæ¥å¼€å‘åŸºäº Joint Embedding çš„æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ï¼ˆfoundation modelsï¼‰æä¾›äº†å…³é”®çš„ç†è®ºä¸å®è·µæ¡†æ¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at the Workshop on Time Series in the Age of Large Models - NeurIPS 2024",
      "pdf_url": "https://arxiv.org/pdf/2509.25449v1",
      "published_date": "2025-09-29 19:57:37 UTC",
      "updated_date": "2025-09-29 19:57:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:43:31.563303+00:00"
    },
    {
      "arxiv_id": "2509.25438v1",
      "title": "Beyond Noisy-TVs: Noise-Robust Exploration Via Learning Progress Monitoring",
      "title_zh": "è¶…è¶Šâ€œå˜ˆæ‚ç”µè§†â€ï¼šåŸºäºå­¦ä¹ è¿›åº¦ç›‘æ§çš„æŠ—å™ªæ¢ç´¢",
      "authors": [
        "Zhibo Hou",
        "Zhiyu An",
        "Wan Du"
      ],
      "abstract": "When there exists an unlearnable source of randomness (noisy-TV) in the environment, a naively intrinsic reward driven exploring agent gets stuck at that source of randomness and fails at exploration. Intrinsic reward based on uncertainty estimation or distribution similarity, while eventually escapes noisy-TVs as time unfolds, suffers from poor sample efficiency and high computational cost. Inspired by recent findings from neuroscience that humans monitor their improvements during exploration, we propose a novel method for intrinsically-motivated exploration, named Learning Progress Monitoring (LPM). During exploration, LPM rewards model improvements instead of prediction error or novelty, effectively rewards the agent for observing learnable transitions rather than the unlearnable transitions. We introduce a dual-network design that uses an error model to predict the expected prediction error of the dynamics model in its previous iteration, and use the difference between the model errors of the current iteration and previous iteration to guide exploration. We theoretically show that the intrinsic reward of LPM is zero-equivariant and a monotone indicator of Information Gain (IG), and that the error model is necessary to achieve monotonicity correspondence with IG. We empirically compared LPM against state-of-the-art baselines in noisy environments based on MNIST, 3D maze with 160x120 RGB inputs, and Atari. Results show that LPM's intrinsic reward converges faster, explores more states in the maze experiment, and achieves higher extrinsic reward in Atari. This conceptually simple approach marks a shift-of-paradigm of noise-robust exploration. For code to reproduce our experiments, see https://github.com/Akuna23Matata/LPM_exploration",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼ºåŒ–å­¦ä¹ ä¸­å› ä¸å¯å­¦ä¹ éšæœºæ€§æºï¼ˆNoisy-TVsï¼‰å¯¼è‡´æ™ºèƒ½ä½“é™·å…¥æ¢ç´¢ç“¶é¢ˆçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º Learning Progress Monitoring (LPM) çš„æ–°å‹å†…åœ¨åŠ¨æœºæ¢ç´¢æ–¹æ³•ã€‚å—ç¥ç»ç§‘å­¦å¯å‘ï¼ŒLPM å¥–åŠ±æ¨¡å‹çš„æ”¹è¿›è€Œéå•çº¯çš„é¢„æµ‹è¯¯å·®æˆ–æ–°é¢–æ€§ï¼Œæœ‰æ•ˆå¼•å¯¼æ™ºèƒ½ä½“è§‚å¯Ÿå¯å­¦ä¹ çš„çŠ¶æ€è½¬ç§»è€Œéæ— æ•ˆçš„éšæœºæ‰°åŠ¨ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åŒç½‘ç»œè®¾è®¡ï¼Œåˆ©ç”¨è¯¯å·®æ¨¡å‹é¢„æµ‹åŠ¨åŠ›å­¦æ¨¡å‹å‰ä¸€è¿­ä»£çš„é¢„æœŸè¯¯å·®ï¼Œå¹¶ä»¥å½“å‰ä¸å‰ä¸€è¿­ä»£è¯¯å·®çš„å·®å€¼æŒ‡å¯¼æ¢ç´¢ã€‚ç†è®ºè¯æ˜ LPM çš„å†…åœ¨å¥–åŠ±æ˜¯ Information Gain (IG) çš„å•è°ƒæŒ‡æ ‡ï¼Œä¸”è¯¯å·®æ¨¡å‹å¯¹äºç»´æŒè¿™ç§å•è°ƒå¯¹åº”å…³ç³»è‡³å…³é‡è¦ã€‚åœ¨ MNIST å™ªå£°ç¯å¢ƒã€3D è¿·å®«å’Œ Atari ç­‰å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­ï¼ŒLPM å±•ç°å‡ºæ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ã€æ›´å¹¿æ³›çš„çŠ¶æ€è¦†ç›–ä»¥åŠæ›´é«˜çš„å¤–éƒ¨å¥–åŠ±ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„å…ˆè¿›åŸºçº¿æ¨¡å‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25438v1",
      "published_date": "2025-09-29 19:43:44 UTC",
      "updated_date": "2025-09-29 19:43:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:43:35.470330+00:00"
    },
    {
      "arxiv_id": "2509.25435v1",
      "title": "GESA: Graph-Enhanced Semantic Allocation for Generalized, Fair, and Explainable Candidate-Role Matching",
      "title_zh": "GESAï¼šé¢å‘é€šç”¨ã€å…¬å¹³ä¸”å¯è§£é‡Šäººå²—åŒ¹é…çš„å›¾å¢å¼ºè¯­ä¹‰åˆ†é…",
      "authors": [
        "Rishi Ashish Shah",
        "Shivaay Dhondiyal",
        "Kartik Sharma",
        "Sukriti Talwar",
        "Saksham Jain",
        "Sparsh Jain"
      ],
      "abstract": "Accurate, fair, and explainable allocation of candidates to roles represents a fundamental challenge across multiple domains including corporate hiring, academic admissions, fellowship awards, and volunteer placement systems. Current state-of-the-art approaches suffer from semantic inflexibility, persistent demographic bias, opacity in decision-making processes, and poor scalability under dynamic policy constraints. We present GESA (Graph-Enhanced Semantic Allocation), a comprehensive framework that addresses these limitations through the integration of domain-adaptive transformer embeddings, heterogeneous self-supervised graph neural networks, adversarial debiasing mechanisms, multi-objective genetic optimization, and explainable AI components. Our experimental evaluation on large-scale international benchmarks comprising 20,000 candidate profiles and 3,000 role specifications demonstrates superior performance with 94.5% top-3 allocation accuracy, 37% improvement in diversity representation, 0.98 fairness score across demographic categories, and sub-second end-to-end latency. Additionally, GESA incorporates hybrid recommendation capabilities and glass-box explainability, making it suitable for deployment across diverse international contexts in industry, academia, and non-profit sectors.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† GESA (Graph-Enhanced Semantic Allocation) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ä¸šæ‹›è˜ã€å­¦æœ¯å½•å–ç­‰åœºæ™¯ä¸­å€™é€‰äººä¸å²—ä½åŒ¹é…æ—¶é¢ä¸´çš„è¯­ä¹‰çµæ´»æ€§ä¸è¶³ã€äººå£ç»Ÿè®¡å­¦åè§åŠå†³ç­–ä¸é€æ˜ç­‰æ ¸å¿ƒæŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡é›†æˆé¢†åŸŸè‡ªé€‚åº”çš„ Transformer åµŒå…¥ã€å¼‚æ„è‡ªç›‘ç£å›¾ç¥ç»ç½‘ç»œ (Heterogeneous Self-supervised Graph Neural Networks)ã€å¯¹æŠ—æ€§å»åæœºåˆ¶ (Adversarial Debiasing Mechanisms) ä»¥åŠå¤šç›®æ ‡é—ä¼ ä¼˜åŒ–ï¼Œæ„å»ºäº†ä¸€ä¸ªå…¨æ–¹ä½çš„è‡ªåŠ¨åŒ–åˆ†é…ç³»ç»Ÿã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGESA åœ¨åŒ…å« 20,000 ä¸ªå€™é€‰äººæ¡£æ¡ˆçš„å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº† 94.5% çš„ Top-3 åˆ†é…å‡†ç¡®ç‡ï¼Œå¹¶åœ¨å¤šæ ·æ€§ä»£è¡¨æ€§ä¸Šæå‡äº† 37%ï¼Œå…¬å¹³æ€§å¾—åˆ†é«˜è¾¾ 0.98ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿå…·å¤‡äºšç§’çº§çš„ç«¯åˆ°ç«¯å¤„ç†å»¶è¿Ÿå’Œâ€œç»ç’ƒç›’â€å¼çš„å¯è§£é‡Šæ€§ (Glass-box Explainability)ï¼Œèƒ½å¤Ÿçµæ´»é€‚åº”å·¥ä¸šç•Œä¸å­¦æœ¯ç•Œçš„åŠ¨æ€æ”¿ç­–çº¦æŸï¼Œä¸ºå®ç°å¯æ¨å¹¿ã€å…¬å¹³ä¸”é€æ˜çš„è‡ªåŠ¨åŒ¹é…æŠ€æœ¯æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25435v1",
      "published_date": "2025-09-29 19:41:55 UTC",
      "updated_date": "2025-09-29 19:41:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:43:38.077724+00:00"
    },
    {
      "arxiv_id": "2509.25434v2",
      "title": "The Open Syndrome Definition",
      "title_zh": "å¼€æ”¾ç»¼åˆå¾å®šä¹‰",
      "authors": [
        "Ana Paula Gomes Ferreira",
        "Aleksandar AnÅ¾el",
        "Izabel Oliva Marcilio de Souza",
        "Helen Hughes",
        "Alex J Elliot",
        "Jude Dzevela Kong",
        "Madlen Schranz",
        "Alexander Ullrich",
        "Georges Hattab"
      ],
      "abstract": "Case definitions are essential for effectively communicating public health threats. However, the absence of a standardized, machine-readable format poses significant challenges to interoperability, epidemiological research, the exchange of qualitative data, and the effective application of computational analysis methods, including artificial intelligence (AI). This complicates comparisons and collaborations across organizations and regions, limits data integration, and hinders technological innovation in public health. To address these issues, we propose the first open, machine-readable format for representing case and syndrome definitions. Additionally, we introduce the first comprehensive dataset of standardized case definitions and tools to convert existing human-readable definitions into machine-readable formats. We also provide an accessible online platform for browsing, analyzing, and contributing new definitions, available at https://opensyndrome.org. The Open Syndrome Definition format enables consistent, scalable use of case definitions across systems, unlocking AI's potential to strengthen public health preparedness and response. The source code for the format can be found at https://github.com/OpenSyndrome/schema under the MIT license.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…¬å…±å«ç”Ÿå¨èƒæ²Ÿé€šä¸­ Case definitions ç¼ºä¹æ ‡å‡†åŒ–ã€æœºå™¨å¯è¯»æ ¼å¼æ‰€å¯¼è‡´çš„äº’æ“ä½œæ€§å·®ã€æ•°æ®é›†æˆå›°éš¾åŠ AI åº”ç”¨å—é™ç­‰é—®é¢˜ï¼Œæå‡ºäº†é¦–ä¸ªå¼€æ”¾ä¸”æœºå™¨å¯è¯»çš„ Open Syndrome Definition æ ¼å¼ã€‚ä½œè€…å›¢é˜ŸåŒæ­¥å¼•å…¥äº†é¦–ä¸ªæ ‡å‡†åŒ–çš„ Case definitions ç»¼åˆæ•°æ®é›†ï¼Œå¹¶å¼€å‘äº†å°†ä¼ ç»Ÿäººç±»å¯è¯»å®šä¹‰è½¬æ¢ä¸ºæœºå™¨å¯è¯»æ ¼å¼çš„é…å¥—å·¥å…·ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æä¾›äº†ä¸€ä¸ªåœ¨çº¿å¹³å°ç”¨äºæµè§ˆã€åˆ†æå’Œè´¡çŒ®æ–°å®šä¹‰ï¼Œå¹¶å¼€æºäº†å…¶æ ¸å¿ƒæ ¼å¼çš„æºä»£ç ã€‚è¯¥æ ¼å¼çš„åº”ç”¨èƒ½å¤Ÿç¡®ä¿ Case definitions åœ¨ä¸åŒç³»ç»Ÿé—´çš„æŒç»­ä¸å¯æ‰©å±•ä½¿ç”¨ï¼Œä»è€Œæœ‰æ•ˆè§£é” AI åœ¨åŠ å¼ºå…¬å…±å«ç”Ÿé˜²ç¾å‡†å¤‡å’Œåº”æ€¥å“åº”æ–¹é¢çš„æŠ€æœ¯æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25434v2",
      "published_date": "2025-09-29 19:41:54 UTC",
      "updated_date": "2025-10-22 14:07:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:43:41.567578+00:00"
    },
    {
      "arxiv_id": "2509.25426v2",
      "title": "RADAR: Reasoning-Ability and Difficulty-Aware Routing for Reasoning LLMs",
      "title_zh": "RADARï¼šé¢å‘æ¨ç†å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ä¸éš¾åº¦æ„ŸçŸ¥è·¯ç”±",
      "authors": [
        "Nigel Fernandez",
        "Branislav Kveton",
        "Ryan A. Rossi",
        "Andrew S. Lan",
        "Zichao Wang"
      ],
      "abstract": "Reasoning language models have demonstrated remarkable performance on many challenging tasks in math, science, and coding. Choosing the right reasoning model for practical deployment involves a performance and cost tradeoff at two key levels: model size and reasoning budget, where larger models and higher reasoning budget lead to better performance but with increased cost and latency. In this work, we tackle this tradeoff from the angle of model configuration routing for different queries, and present RADAR (Reasoning-Ability and Difficulty-Aware Routing), a lightweight, interpretable, and scalable routing framework. Inspired by psychometrics, RADAR learns an item response model from model responses with different budgets to different queries, with interpretable parameters including query difficulties and model-budget abilities. RADAR then routes queries with higher difficulty to model-budget pairs with higher ability, and vice versa. We conduct extensive experiments on 8 widely used challenging reasoning benchmarks, demonstrating the superior performance of RADAR compared to state-of-the-art model routing methods. RADAR also exhibits query generalization capabilities, showing strong performance on out-of-distribution queries in all benchmarks. RADAR is also scalable and can efficiently integrate additional models by dynamically selecting a small set of evaluation queries to estimate their abilities.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†RADARï¼Œä¸€ç§æ—¨åœ¨å¹³è¡¡æ¨ç†å¤§è¯­è¨€æ¨¡å‹(Reasoning LLMs)æ€§èƒ½ä¸æˆæœ¬çš„è½»é‡åŒ–ã€å¯è§£é‡Šä¸”å¯æ‰©å±•çš„è·¯ç”±æ¡†æ¶ã€‚RADARå—å¿ƒç†æµ‹é‡å­¦(Psychometrics)å¯å‘ï¼Œé€šè¿‡å­¦ä¹ é¡¹ç›®ååº”æ¨¡å‹(Item Response Model)æ¥è¯„ä¼°æŸ¥è¯¢éš¾åº¦å’Œæ¨¡å‹é…ç½®çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿæ ¹æ®é¢„æµ‹éš¾åº¦ï¼Œå°†å…·æœ‰æŒ‘æˆ˜æ€§çš„æŸ¥è¯¢è‡ªåŠ¨è·¯ç”±è‡³å…·å¤‡æ›´é«˜æ¨ç†é¢„ç®—(Reasoning Budget)æˆ–æ›´å¤§èƒ½åŠ›çš„æ¨¡å‹ï¼Œè€Œå°†ç®€å•é—®é¢˜åˆ†é…ç»™ä½æˆæœ¬é…ç½®ã€‚åœ¨8ä¸ªä¸»æµæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRADARåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„æ¨¡å‹è·¯ç”±æ–¹æ³•ï¼Œå¹¶å±•ç°å‡ºå¼ºå¤§çš„åˆ†å¸ƒå¤–(Out-of-distribution)æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒRADARå…·å¤‡è‰¯å¥½çš„å¯æ‰©å±•æ€§ï¼Œèƒ½å¤Ÿé€šè¿‡åŠ¨æ€é€‰æ‹©å°‘é‡è¯„ä¼°æŸ¥è¯¢é«˜æ•ˆåœ°å°†æ–°æ¨¡å‹æ•´åˆè¿›ç°æœ‰è·¯ç”±ç³»ç»Ÿã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25426v2",
      "published_date": "2025-09-29 19:33:44 UTC",
      "updated_date": "2025-10-01 00:34:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:43:42.761700+00:00"
    },
    {
      "arxiv_id": "2509.25424v1",
      "title": "Polychromic Objectives for Reinforcement Learning",
      "title_zh": "å¼ºåŒ–å­¦ä¹ çš„å¤šè‰²æ€§ç›®æ ‡",
      "authors": [
        "Jubayer Ibn Hamid",
        "Ifdita Hasan Orney",
        "Ellen Xu",
        "Chelsea Finn",
        "Dorsa Sadigh"
      ],
      "abstract": "Reinforcement learning fine-tuning (RLFT) is a dominant paradigm for improving pretrained policies for downstream tasks. These pretrained policies, trained on large datasets, produce generations with a broad range of promising but unrefined behaviors. Often, a critical failure mode of RLFT arises when policies lose this diversity and collapse into a handful of easily exploitable outputs. This convergence hinders exploration, which is essential for expanding the capabilities of the pretrained policy and for amplifying the benefits of test-time compute scaling. To address this, we introduce an objective for policy gradient methods that explicitly enforces the exploration and refinement of diverse generations, which we call a polychromic objective. We then show how proximal policy optimization (PPO) can be adapted to optimize this objective. Our method (1) employs vine sampling to collect on-policy rollouts and (2) modifies the advantage function to reflect the advantage under our new objective. Experiments on BabyAI, Minigrid, and Algorithmic Creativity show that our method improves success rates by reliably solving a larger set of environment configurations and generalizes better under large perturbations. Moreover, when given multiple attempts in pass@$k$ experiments, the policy achieves substantially higher coverage, demonstrating its ability to maintain and exploit a diverse repertoire of strategies.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼ºåŒ–å­¦ä¹ å¾®è°ƒ(Reinforcement Learning Fine-Tuning, RLFT)ä¸­å¸¸è§çš„ç­–ç•¥å¤šæ ·æ€§ä¸§å¤±å’Œé™·å…¥å•ä¸€è¾“å‡ºæ¨¡å¼çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºPolychromic Objectivesçš„ç­–ç•¥æ¢¯åº¦ç›®æ ‡å‡½æ•°ï¼Œæ—¨åœ¨æ˜¾å¼å¢å¼ºå¯¹å¤šæ ·åŒ–ç”Ÿæˆçš„æ¢ç´¢ä¸ç»†åŒ–ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡æ”¹è¿›è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–(Proximal Policy Optimization, PPO)ç®—æ³•æ¥å®ç°è¿™ä¸€ç›®æ ‡ï¼Œç»“åˆäº†vine samplingæŠ€æœ¯è¿›è¡Œç­–ç•¥å†…é‡‡æ ·ï¼Œå¹¶å¯¹ä¼˜åŠ¿å‡½æ•°(advantage function)è¿›è¡Œäº†è°ƒæ•´ä»¥åæ˜ æ–°ç›®æ ‡çš„ä¼˜åŠ¿ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨BabyAIã€Minigridå’ŒAlgorithmic Creativityç­‰å¤šä¸ªä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†æˆåŠŸç‡ï¼Œå¹¶åœ¨é¢ä¸´å¤§å¹…æ‰°åŠ¨æ—¶å±•ç°å‡ºæ›´ä¼˜çš„æ³›åŒ–æ€§èƒ½ã€‚æ­¤å¤–ï¼Œåœ¨pass@kå®éªŒä¸­ï¼Œè¯¥ç­–ç•¥å±•ç°äº†æ›´é«˜çš„è¦†ç›–ç‡ï¼Œè¯æ˜å…¶èƒ½å¤Ÿæœ‰æ•ˆç»´æŒå¹¶åˆ©ç”¨å¤šæ ·åŒ–çš„ç­–ç•¥åº“ï¼Œä»è€Œå……åˆ†å‘æŒ¥æµ‹è¯•æ—¶è®¡ç®—ç¼©æ”¾(test-time compute scaling)çš„ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25424v1",
      "published_date": "2025-09-29 19:32:11 UTC",
      "updated_date": "2025-09-29 19:32:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:43:55.599979+00:00"
    },
    {
      "arxiv_id": "2509.25420v1",
      "title": "Adaptive Test-Time Reasoning via Reward-Guided Dual-Phase Search",
      "title_zh": "åŸºäºå¥–åŠ±å¼•å¯¼åŒé˜¶æ®µæœç´¢çš„è‡ªé€‚åº”æµ‹è¯•æ—¶æ¨ç†",
      "authors": [
        "Yingqian Cui",
        "Zhenwei Dai",
        "Pengfei He",
        "Bing He",
        "Hui Liu",
        "Xianfeng Tang",
        "Jingying Zeng",
        "Suhang Wang",
        "Yue Xing",
        "Jiliang Tang",
        "Benoit Dumoulin"
      ],
      "abstract": "Large Language Models (LLMs) have achieved significant advances in reasoning tasks. A key approach is tree-based search with verifiers, which expand candidate reasoning paths and use reward models to guide pruning and selection. Although effective in improving accuracy, these methods are not optimal in terms of efficiency: they perform simple decomposition on the reasoning process, but ignore the planning-execution nature of tasks such as math reasoning or code generation. This results in inefficient exploration of reasoning process. To address this, we propose a dual-phase test-time scaling framework that explicitly separates reasoning into planning and execution, and performs search over the two phases individually. Specifically, we decompose reasoning trajectories and develop reward models for each phase, enabling the search to explore and prune plans and executions separately. We further introduce a dynamic budget allocation mechanism that adaptively redistributes sampling effort based on reward feedback, allowing early stopping on confident steps and reallocation of computation to more challenging parts of the reasoning process. Experiments on both mathematical reasoning and code generation benchmarks demonstrate that our approach consistently improves accuracy while reducing redundant computation.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ¨ç†ä»»åŠ¡ä¸­é‡‡ç”¨æ ‘æœç´¢(tree-based search)æ–¹æ³•æ—¶ï¼Œå› å¿½ç•¥ä»»åŠ¡çš„è§„åˆ’ä¸æ‰§è¡Œ(planning-execution)ç‰¹æ€§è€Œå¯¼è‡´æ¨ç†æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäºå¥–åŠ±å¼•å¯¼åŒé˜¶æ®µæœç´¢(Reward-Guided Dual-Phase Search)çš„è‡ªé€‚åº”æµ‹è¯•æ—¶æ¨ç†æ¡†æ¶ï¼Œå°†æ¨ç†è¿‡ç¨‹æ˜¾å¼åˆ†è§£ä¸ºè§„åˆ’(planning)å’Œæ‰§è¡Œ(execution)ä¸¤ä¸ªé˜¶æ®µã€‚é€šè¿‡åˆ†åˆ«ä¸ºæ¯ä¸ªé˜¶æ®µå¼€å‘å¥–åŠ±æ¨¡å‹(reward models)ï¼Œè¯¥æ¡†æ¶å®ç°äº†å¯¹è§„åˆ’å’Œæ‰§è¡Œè·¯å¾„çš„ç‹¬ç«‹æœç´¢ä¸å‰ªæã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†åŠ¨æ€é¢„ç®—åˆ†é…æœºåˆ¶ï¼Œèƒ½å¤Ÿæ ¹æ®å¥–åŠ±åé¦ˆè‡ªé€‚åº”åœ°åˆ†é…é‡‡æ ·èµ„æºï¼Œåœ¨æ”¯æŒé«˜ç½®ä¿¡åº¦æ­¥éª¤æå‰åœæ­¢çš„åŒæ—¶ï¼Œå°†è®¡ç®—èµ„æºé‡æ–°åˆ†é…è‡³æ›´å…·æŒ‘æˆ˜æ€§çš„æ¨ç†ç¯èŠ‚ã€‚åœ¨æ•°å­¦æ¨ç†å’Œä»£ç ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æŒç»­æå‡æ¨¡å‹å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘äº†å†—ä½™çš„è®¡ç®—å¼€é”€ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25420v1",
      "published_date": "2025-09-29 19:27:23 UTC",
      "updated_date": "2025-09-29 19:27:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:43:59.077244+00:00"
    },
    {
      "arxiv_id": "2509.25416v1",
      "title": "Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization",
      "title_zh": "åŸºäºåå¥½å¼•å¯¼ä¼˜åŒ–çš„æ‰©æ•£æ–‡æœ¬è½¬è¯­éŸ³æƒ…æ„Ÿå¯¹é½ç”Ÿæˆ",
      "authors": [
        "Jiacheng Shi",
        "Hongfei Du",
        "Yangfan He",
        "Y. Alicia Hong",
        "Ye Gao"
      ],
      "abstract": "Emotional text-to-speech seeks to convey affect while preserving intelligibility and prosody, yet existing methods rely on coarse labels or proxy classifiers and receive only utterance-level feedback. We introduce Emotion-Aware Stepwise Preference Optimization (EASPO), a post-training framework that aligns diffusion TTS with fine-grained emotional preferences at intermediate denoising steps. Central to our approach is EASPM, a time-conditioned model that scores noisy intermediate speech states and enables automatic preference pair construction. EASPO optimizes generation to match these stepwise preferences, enabling controllable emotional shaping. Experiments show superior performance over existing methods in both expressiveness and naturalness.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æƒ…æ„Ÿæ–‡æœ¬è½¬è¯­éŸ³ (Emotional Text-to-Speech) åœ¨ç°æœ‰æ–¹æ³•ä¸­è¿‡åº¦ä¾èµ–ç²—ç²’åº¦æ ‡ç­¾ä¸”ä»…èƒ½æ¥æ”¶è¯è¯­çº§åé¦ˆçš„é—®é¢˜ï¼Œæå‡ºäº† Emotion-Aware Stepwise Preference Optimization (EASPO) åè®­ç»ƒæ¡†æ¶ã€‚è¯¥æ¡†æ¶æ—¨åœ¨å°†æ‰©æ•£ TTS (Diffusion TTS) æ¨¡å‹åœ¨ä¸­é—´å»å™ªæ­¥éª¤ä¸­ä¸ç»†ç²’åº¦æƒ…æ„Ÿåå¥½ç›¸å¯¹é½ï¼Œä»è€Œå®ç°æ›´ç²¾å‡†çš„æƒ…æ„Ÿæ§åˆ¶ã€‚å…¶æ ¸å¿ƒç»„ä»¶ EASPM æ˜¯ä¸€ç§æ—¶é—´æ¡ä»¶æ¨¡å‹ï¼Œé€šè¿‡å¯¹ä¸­é—´è¯­éŸ³çŠ¶æ€è¿›è¡Œè¯„åˆ†æ¥è‡ªåŠ¨æ„å»ºåå¥½å¯¹ï¼ŒæŒ‡å¯¼æ¨¡å‹è¿›è¡Œé€æ­¥ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEASPO åœ¨è¯­éŸ³çš„è¡¨è¾¾åŠ›å’Œè‡ªç„¶åº¦æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºå®ç°é«˜è´¨é‡ã€å¯æ§çš„æƒ…æ„Ÿè¯­éŸ³åˆæˆæä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25416v1",
      "published_date": "2025-09-29 19:19:42 UTC",
      "updated_date": "2025-09-29 19:19:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:44:00.868922+00:00"
    },
    {
      "arxiv_id": "2509.25414v1",
      "title": "Rethinking Parameter Sharing for LLM Fine-Tuning with Multiple LoRAs",
      "title_zh": "é‡æ–°å®¡è§†å¤š LoRA å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒä¸­çš„å‚æ•°å…±äº«",
      "authors": [
        "Hao Ban",
        "Kaiyi Ji"
      ],
      "abstract": "Large language models are often adapted using parameter-efficient techniques such as Low-Rank Adaptation (LoRA), formulated as $y = W_0x + BAx$, where $W_0$ is the pre-trained parameters and $x$ is the input to the adapted layer. While multi-adapter extensions often employ multiple LoRAs, prior studies suggest that the inner $A$ matrices are highly similar during training and thus suitable for sharing. We revisit this phenomenon and find that this similarity is largely attributable to the identical initialization rather than shared knowledge, with $B$ playing a more critical role in knowledge encoding and transfer. Motivated by these insights, we propose \\textbf{ALoRA}, an asymmetric multi-LoRA design with multiple $A$ matrices and a single shared $B$ in multi-task fine-tuning, and \\textbf{Fed-ALoRA}, which shares $B$ across clients in federated fine-tuning under both homogeneous and heterogeneous settings, through a novel matrix decomposition strategy to accommodate heterogeneous ranks across clients. Experiments on commonsense reasoning, math reasoning, multi-task NLP dataset, and federated NLP dataset demonstrate that our methods achieve more balanced performance across tasks with comparable or superior average accuracy relative to existing multi-LoRA approaches. Codes are available at https://github.com/OptMN-Lab/ALoRA.",
      "tldr_zh": "è¯¥ç ”ç©¶é‡æ–°è¯„ä¼°äº†åœ¨å¤šLoRAå¤§è¯­è¨€æ¨¡å‹å¾®è°ƒä¸­çš„å‚æ•°å…±äº«æœºåˆ¶ï¼Œå‘ç°ä»¥å¾€è®¤ä¸º$A$çŸ©é˜µé«˜åº¦ç›¸ä¼¼å¹¶é€‚åˆå…±äº«çš„ç°è±¡ä¸»è¦å½’å› äºç›¸åŒçš„åˆå§‹åŒ–ï¼Œè€Œ$B$çŸ©é˜µåœ¨çŸ¥è¯†ç¼–ç å’Œè½¬ç§»ä¸­å‘æŒ¥ç€æ›´å…³é”®çš„ä½œç”¨ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œä½œè€…æå‡ºäº†ALoRAï¼Œä¸€ç§åœ¨å¤šä»»åŠ¡å¾®è°ƒä¸­é‡‡ç”¨å¤šä¸ª$A$çŸ©é˜µå¹¶å…±äº«å•ä¸ª$B$çŸ©é˜µçš„éå¯¹ç§°å¤šLoRAè®¾è®¡ã€‚é’ˆå¯¹è”é‚¦å­¦ä¹ ç¯å¢ƒï¼Œç ”ç©¶è¿›ä¸€æ­¥å¼€å‘äº†Fed-ALoRAï¼Œé€šè¿‡ä¸€ç§æ–°å‹çŸ©é˜µåˆ†è§£ç­–ç•¥å®ç°äº†è·¨å®¢æˆ·ç«¯å…±äº«$B$çŸ©é˜µï¼Œä»¥é€‚åº”ä¸åŒå®¢æˆ·ç«¯çš„å¼‚æ„ç§©ï¼ˆranksï¼‰è®¾ç½®ã€‚åœ¨å¸¸è¯†æ¨ç†ã€æ•°å­¦æ¨ç†ä»¥åŠå¤šä»»åŠ¡NLPå’Œè”é‚¦å­¦ä¹ NLPæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼Œä¸ç°æœ‰çš„å¤šLoRAæ–¹æ³•ç›¸æ¯”ï¼ŒALoRAå’ŒFed-ALoRAåœ¨è·å¾—ç›¸å½“æˆ–æ›´ä¼˜å¹³å‡å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œå®ç°äº†æ›´å‡è¡¡çš„ä»»åŠ¡è¡¨ç°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25414v1",
      "published_date": "2025-09-29 19:16:14 UTC",
      "updated_date": "2025-09-29 19:16:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:44:07.170564+00:00"
    },
    {
      "arxiv_id": "2509.25411v1",
      "title": "Boolean Satisfiability via Imitation Learning",
      "title_zh": "åŸºäºæ¨¡ä»¿å­¦ä¹ çš„å¸ƒå°”å¯æ»¡è¶³æ€§",
      "authors": [
        "Zewei Zhang",
        "Huan Liu",
        "Yuanhao Yu",
        "Jun Chen",
        "Xiangyu Xu"
      ],
      "abstract": "We propose ImitSAT, a branching policy for conflict-driven clause learning (CDCL) solvers based on imitation learning for the Boolean satisfiability problem (SAT). Unlike previous methods that predict instance-level signals to improve CDCL branching indirectly, or rely on reinforcement learning and insufficient CDCL information to enhance branching, ImitSAT learns from expert KeyTrace that collapses a full run into the sequence of surviving decisions. Replaying a KeyTrace on the same instance is nearly conflict-free, providing dense decision-level supervision and directly reducing propagations -- the dominant contributor to wall-clock time. This prefix-conditioned supervision enables ImitSAT to reproduce high-quality branches without exploration, yielding faster convergence, stable training, and seamless integration into CDCL. Extensive experiments demonstrate that ImitSAT reduces propagation counts and runtime, outperforming state-of-the-art learned approaches. We released the source code and trained model at https://github.com/zewei-Zhang/ImitSAT",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ImitSATï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹å¸ƒå°”å¯æ»¡è¶³æ€§é—®é¢˜(SAT)ä¸­å†²çªé©±åŠ¨å­å¥å­¦ä¹ (CDCL)æ±‚è§£å™¨çš„åˆ†æ”¯ç­–ç•¥ï¼Œæ ¸å¿ƒåŸºäºæ¨¡ä»¿å­¦ä¹ (Imitation Learning)ã€‚ä¸ä»¥å¾€é€šè¿‡é¢„æµ‹å®ä¾‹çº§ä¿¡å·æˆ–ä¾èµ–å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ä¸åŒï¼ŒImitSATç›´æ¥ä»ä¸“å®¶KeyTraceä¸­å­¦ä¹ ï¼Œå°†å®Œæ•´çš„æ±‚è§£è¿‡ç¨‹ç®€åŒ–ä¸ºå¹¸å­˜å†³ç­–åºåˆ—ã€‚é€šè¿‡åœ¨ç›¸åŒå®ä¾‹ä¸Šé‡æ”¾KeyTraceï¼Œè¯¥æ–¹æ³•èƒ½æä¾›å¯†é›†çš„å†³ç­–çº§ç›‘ç£å¹¶å®ç°å‡ ä¹æ— å†²çªçš„è¿è¡Œï¼Œä»è€Œæœ‰æ•ˆå‡å°‘äº†å½±å“è¿è¡Œæ—¶é—´çš„å…³é”®å› ç´ â€”â€”ä¼ æ’­(propagations)æ¬¡æ•°ã€‚è¿™ç§å‰ç¼€æ¡ä»¶ç›‘ç£æœºåˆ¶ä½¿ImitSATæ— éœ€å¤æ‚æ¢ç´¢å³å¯ç”Ÿæˆé«˜è´¨é‡åˆ†æ”¯ï¼Œå…·å¤‡æ”¶æ•›å¿«ã€è®­ç»ƒç¨³å®šä»¥åŠä¸CDCLæ— ç¼é›†æˆçš„ä¼˜åŠ¿ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒImitSATåœ¨å‡å°‘ä¼ æ’­è®¡æ•°å’Œç¼©çŸ­è¿è¡Œæ—¶é—´æ–¹é¢å‡ä¼˜äºç›®å‰çš„å…ˆè¿›å­¦ä¹ æ–¹æ³•ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25411v1",
      "published_date": "2025-09-29 19:09:37 UTC",
      "updated_date": "2025-09-29 19:09:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:44:08.362666+00:00"
    },
    {
      "arxiv_id": "2509.25409v1",
      "title": "From Faithfulness to Correctness: Generative Reward Models that Think Critically",
      "title_zh": "ä»å¿ å®æ€§åˆ°æ­£ç¡®æ€§ï¼šå…·å¤‡æ‰¹åˆ¤æ€§æ€ç»´çš„ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹",
      "authors": [
        "Qiyao Ma",
        "Yunsheng Shi",
        "Hongtao Tian",
        "Chao Wang",
        "Weiming Chang",
        "Ting Yao"
      ],
      "abstract": "Through reinforcement learning with verifiable rewards (RLVR), large language models have achieved substantial progress in domains with easily verifiable outcomes, such as mathematics and coding. However, when applied to more complex tasks like open-domain question answering, RLVR faces significant challenges due to the difficulty of verifying correctness. The nuanced and ambiguous nature of real-world knowledge makes it difficult to reliably evaluate correctness in these settings, necessitating further abilities that extend beyond mere logical consistency to encompass an understanding and assessment of both external and internal knowledge. Recent work has primarily focused on improving faithfulness, defined as semantic alignment with supporting documents, which can cause models to rely excessively on external sources and diminish their capacity for critical assessment. To address this, we propose the Thinking-supervised Reward Model (TRM), which incorporates sentence-level thinking supervision to endow reward models with critical thinking abilities. Given a query, answer, and supporting documents, TRM first assesses the faithfulness of each answer sentence to the supporting documents, and then applies a reasoning step to evaluate sentence-level correctness. By structuring reward modeling as a sequence of faithfulness, reasoning, and correctness evaluations, TRM encourages models to critically assess and leverage both external and internal knowledge. Experiments on reward signals demonstrate that TRM substantially improves the identification of incorrect sentences, and incorporating TRM into policy optimization leads to significant gains in both answer correctness and usefulness.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹åœ¨å¼€æ”¾åŸŸé—®ç­”ç­‰å¤æ‚ä»»åŠ¡ä¸­éš¾ä»¥éªŒè¯æ­£ç¡®æ€§çš„æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºå½“å‰ä¾§é‡äºå¿ å®åº¦ (Faithfulness) çš„æ–¹æ³•ä¼šå¯¼è‡´æ¨¡å‹è¿‡åº¦ä¾èµ–å¤–éƒ¨æ¥æºå¹¶å‰Šå¼±æ‰¹åˆ¤æ€§è¯„ä¼°èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†æ€ç»´ç›‘ç£å¥–åŠ±æ¨¡å‹ (Thinking-supervised Reward Model, TRM)ï¼Œé€šè¿‡å¼•å…¥å¥å­çº§æ€ç»´ç›‘ç£æ¥èµ‹äºˆå¥–åŠ±æ¨¡å‹æ‰¹åˆ¤æ€§æ€ç»´ã€‚TRM å°†å¥–åŠ±å»ºæ¨¡æµç¨‹ç»“æ„åŒ–ä¸ºå¿ å®åº¦è¯„ä¼°ã€æ¨ç†åŠæ­£ç¡®æ€§éªŒè¯çš„åºåˆ—ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé’ˆå¯¹æŸ¥è¯¢ã€ç­”æ¡ˆå’Œæ”¯æŒæ–‡æ¡£è¿›è¡Œæ·±å±‚åˆ†æã€‚è¿™ç§è®¾è®¡é¼“åŠ±æ¨¡å‹æ‰¹åˆ¤æ€§åœ°åˆ©ç”¨å¤–éƒ¨å’Œå†…éƒ¨çŸ¥è¯†ï¼Œä»è€Œè¶…è¶Šç®€å•çš„é€»è¾‘ä¸€è‡´æ€§åˆ¤æ–­ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒTRM æ˜¾è‘—å¢å¼ºäº†å¯¹é”™è¯¯å¥å­çš„è¯†åˆ«èƒ½åŠ›ï¼Œä¸”åœ¨æ•´åˆè¿›ç­–ç•¥ä¼˜åŒ–åï¼Œå¤§å¹…æå‡äº†ç”Ÿæˆç­”æ¡ˆçš„æ­£ç¡®æ€§ä¸å®ç”¨æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25409v1",
      "published_date": "2025-09-29 19:06:56 UTC",
      "updated_date": "2025-09-29 19:06:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:44:09.975419+00:00"
    },
    {
      "arxiv_id": "2509.25401v1",
      "title": "FlashOmni: A Unified Sparse Attention Engine for Diffusion Transformers",
      "title_zh": "FlashOmniï¼šé¢å‘æ‰©æ•£ Transformer çš„ç»Ÿä¸€ç¨€ç–æ³¨æ„åŠ›å¼•æ“",
      "authors": [
        "Liang Qiao",
        "Yue Dai",
        "Yeqi Huang",
        "Hongyu Kan",
        "Jun Shi",
        "Hong An"
      ],
      "abstract": "Multi-Modal Diffusion Transformers (DiTs) demonstrate exceptional capabilities in visual synthesis, yet their deployment remains constrained by substantial computational demands. To alleviate this bottleneck, many sparsity-based acceleration methods have been proposed. However, their diverse sparsity patterns often require customized kernels for high-performance inference, limiting universality. We propose FlashOmni, a unified sparse attention engine compatible with arbitrary DiT architectures. FlashOmni introduces flexible sparse symbols to standardize the representation of a wide range of sparsity strategies, such as feature caching and block-sparse skipping. This unified abstraction enables the execution of diverse sparse computations within a single attention kernel. In addition, FlashOmni designs optimized sparse GEMMs for attention blocks, leveraging sparse symbols to eliminate redundant computations and further improve efficiency. Experiments demonstrate that FlashOmni delivers near-linear, closely matching the sparsity ratio speedup (1:1) in attention and GEMM-$Q$, and achieves 2.5$\\times$-3.8$\\times$ acceleration in GEMM-$O$ (max peaking at about 87.5% of the theoretical limit). Applied with a multi-granularity sparsity strategy, it enables the Hunyuan model (33K) to achieve about 1.5$\\times$ end-to-end acceleration without degrading visual quality.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€ Diffusion Transformers (DiTs) åœ¨è§†è§‰åˆæˆä¸­é¢ä¸´çš„å·¨å¤§è®¡ç®—æŒ‘æˆ˜åŠç°æœ‰ç¨€ç–åŠ é€Ÿæ–¹æ³•ç¼ºä¹é€šç”¨æ€§çš„é—®é¢˜ï¼Œæå‡ºäº† FlashOmni ç»Ÿä¸€ç¨€ç–æ³¨æ„åŠ›å¼•æ“ã€‚FlashOmni é€šè¿‡å¼•å…¥çµæ´»çš„ sparse symbols æ¥æ ‡å‡†åŒ–å¤šç§ç¨€ç–åŒ–ç­–ç•¥ï¼Œå¦‚ feature caching å’Œ block-sparse skippingï¼Œä½¿å¾—å¤šæ ·çš„ç¨€ç–è®¡ç®—èƒ½åœ¨å•ä¸€ attention kernel å†…å®Œæˆã€‚æ­¤å¤–ï¼Œè¯¥å¼•æ“è¿˜é’ˆå¯¹æ³¨æ„åŠ›å—è®¾è®¡äº†ä¼˜åŒ–çš„ sparse GEMMsï¼Œåˆ©ç”¨è¿™äº›ç¬¦å·æœ‰æ•ˆæ¶ˆé™¤äº†å†—ä½™è®¡ç®—ã€‚å®éªŒæ˜¾ç¤º FlashOmni åœ¨ attention å’Œ GEMM-Q ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ¥è¿‘ç†è®ºä¸Šé™çš„çº¿æ€§åŠ é€Ÿï¼Œå¹¶åœ¨ GEMM-O ä¸­å®ç°äº† 2.5x-3.8x çš„æ€§èƒ½æå‡ã€‚æœ€ç»ˆåœ¨ Hunyuan æ¨¡å‹ä¸Šçš„åº”ç”¨è¯æ˜ï¼Œè¯¥å¼•æ“èƒ½åœ¨ä¿è¯è§†è§‰ç”Ÿæˆè´¨é‡çš„åŒæ—¶å®ç°çº¦ 1.5x çš„ç«¯åˆ°ç«¯åŠ é€Ÿï¼Œä¸º DiT çš„é«˜æ•ˆéƒ¨ç½²æä¾›äº†é€šç”¨çš„é«˜æ€§èƒ½æ¨ç†æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.PF"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25401v1",
      "published_date": "2025-09-29 18:57:14 UTC",
      "updated_date": "2025-09-29 18:57:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:44:13.091867+00:00"
    },
    {
      "arxiv_id": "2509.25397v1",
      "title": "A Cartography of Open Collaboration in Open Source AI: Mapping Practices, Motivations, and Governance in 14 Open Large Language Model Projects",
      "title_zh": "å¼€æºäººå·¥æ™ºèƒ½å¼€æ”¾åä½œå›¾è°±ï¼š14 ä¸ªå¼€æºå¤§è¯­è¨€æ¨¡å‹é¡¹ç›®çš„å®è·µã€åŠ¨æœºä¸æ²»ç†è°ƒç ”",
      "authors": [
        "Johan LinÃ¥ker",
        "Cailean Osborne",
        "Jennifer Ding",
        "Ben Burtenshaw"
      ],
      "abstract": "The proliferation of open large language models (LLMs) is fostering a vibrant ecosystem of research and innovation in artificial intelligence (AI). However, the methods of collaboration used to develop open LLMs both before and after their public release have not yet been comprehensively studied, limiting our understanding of how open LLM projects are initiated, organized, and governed as well as what opportunities there are to foster this ecosystem even further. We address this gap through an exploratory analysis of open collaboration throughout the development and reuse lifecycle of open LLMs, drawing on semi-structured interviews with the developers of 14 open LLMs from grassroots projects, research institutes, startups, and Big Tech companies in North America, Europe, Africa, and Asia. We make three key contributions to research and practice. First, collaboration in open LLM projects extends far beyond the LLMs themselves, encompassing datasets, benchmarks, open source frameworks, leaderboards, knowledge sharing and discussion forums, and compute partnerships, among others. Second, open LLM developers have a variety of social, economic, and technological motivations, from democratizing AI access and promoting open science to building regional ecosystems and expanding language representation. Third, the sampled open LLM projects exhibit five distinct organizational models, ranging from single company projects to non-profit-sponsored grassroots projects, which vary in their centralization of control and community engagement strategies used throughout the open LLM lifecycle. We conclude with practical recommendations for stakeholders seeking to support the global community building a more open future for AI.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡å¯¹æ¥è‡ªå…¨çƒ14ä¸ªå¼€æºå¤§è¯­è¨€æ¨¡å‹(Open LLMs)é¡¹ç›®çš„å¼€å‘è€…è¿›è¡ŒåŠç»“æ„åŒ–è®¿è°ˆï¼Œå¯¹å¼€æºäººå·¥æ™ºèƒ½(Open Source AI)é¢†åŸŸçš„åä½œå®è·µã€åŠ¨æœºä¸æ²»ç†è¿›è¡Œäº†ç³»ç»Ÿæ€§çš„æ¢ç´¢åˆ†æã€‚ç ”ç©¶å‘ç°ï¼ŒOpen LLMsé¡¹ç›®çš„åä½œèŒƒç•´å·²è¿œè¶…æ¨¡å‹æœ¬èº«ï¼Œå¹¿æ³›æ¶µç›–äº†æ•°æ®é›†(datasets)ã€åŸºå‡†æµ‹è¯•(benchmarks)ã€å¼€æºæ¡†æ¶(open source frameworks)ã€æ’è¡Œæ¦œ(leaderboards)åŠè®¡ç®—ä¼™ä¼´å…³ç³»(compute partnerships)ç­‰ã€‚å¼€å‘è€…å‚ä¸å…¶ä¸­çš„åŠ¨æœºå¤šå…ƒï¼ŒåŒ…æ‹¬æ¨åŠ¨AIè·å–æ°‘ä¸»åŒ–ã€ä¿ƒè¿›å¼€æ”¾ç§‘å­¦ã€æ„å»ºåŒºåŸŸç”Ÿæ€ç³»ç»Ÿä»¥åŠæ‰©å±•è¯­è¨€ä»£è¡¨æ€§ã€‚ç ”ç©¶å½’çº³å‡ºäº”ç§ç‹¬ç‰¹çš„ç»„ç»‡æ¨¡å¼ï¼Œå±•ç¤ºäº†ä»å•ä¸€å…¬å¸é¡¹ç›®åˆ°éè¥åˆ©èµåŠ©çš„è‰æ ¹é¡¹ç›®åœ¨ç®¡ç†ä¸­å¿ƒåŒ–ç¨‹åº¦å’Œç¤¾åŒºå‚ä¸ç­–ç•¥ä¸Šçš„å·®å¼‚ã€‚è¯¥å·¥ä½œæ­ç¤ºäº†å¼€æºåä½œåœ¨æ¨¡å‹å¼€å‘ä¸é‡ç”¨å…¨ç”Ÿå‘½å‘¨æœŸä¸­çš„è¿ä½œæœºåˆ¶ï¼Œå¡«è¡¥äº†å…³äºOpen LLMsé¡¹ç›®å¦‚ä½•å‘èµ·ä¸æ²»ç†çš„ç ”ç©¶ç©ºç™½ã€‚æœ€åï¼Œè®ºæ–‡ä¸ºåˆ©ç›Šç›¸å…³è€…æ”¯æŒå…¨çƒç¤¾åŒºæ„å»ºæ›´å¼€æ”¾çš„AIæœªæ¥æä¾›äº†å…·ä½“çš„å®è·µå»ºè®®ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "In submission",
      "pdf_url": "https://arxiv.org/pdf/2509.25397v1",
      "published_date": "2025-09-29 18:55:18 UTC",
      "updated_date": "2025-09-29 18:55:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:44:22.292516+00:00"
    },
    {
      "arxiv_id": "2509.25393v2",
      "title": "Multi-modal Spatio-Temporal Transformer for High-resolution Land Subsidence Prediction",
      "title_zh": "é¢å‘é«˜åˆ†è¾¨ç‡åœ°é¢æ²‰é™é¢„æµ‹çš„å¤šæ¨¡æ€æ—¶ç©º Transformer",
      "authors": [
        "Wendong Yao",
        "Binhua Huang",
        "Soumyabrata Dev"
      ],
      "abstract": "Forecasting high-resolution land subsidence is a critical yet challenging task due to its complex, non-linear dynamics. While standard architectures like ConvLSTM often fail to model long-range dependencies, we argue that a more fundamental limitation of prior work lies in the uni-modal data paradigm. To address this, we propose the Multi-Modal Spatio-Temporal Transformer (MM-STT), a novel framework that fuses dynamic displacement data with static physical priors. Its core innovation is a joint spatio-temporal attention mechanism that processes all multi-modal features in a unified manner. On the public EGMS dataset, MM-STT establishes a new state-of-the-art, reducing the long-range forecast RMSE by an order of magnitude compared to all baselines, including SOTA methods like STGCN and STAEformer. Our results demonstrate that for this class of problems, an architecture's inherent capacity for deep multi-modal fusion is paramount for achieving transformative performance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é«˜åˆ†è¾¨ç‡åœ°é¢æ²‰é™ (Land Subsidence) é¢„æµ‹ä¸­å¤æ‚çš„éçº¿æ€§åŠ¨åŠ›å­¦å’Œé•¿ç¨‹ä¾èµ–å»ºæ¨¡éš¾é¢˜ï¼Œæå‡ºäº†å¤šæ¨¡æ€æ—¶ç©º Transformer (Multi-Modal Spatio-Temporal Transformer, MM-STT) æ¡†æ¶ã€‚MM-STT å…‹æœäº†å…ˆå‰å·¥ä½œä»…ä¾èµ–å•æ¨¡æ€æ•°æ®çš„å±€é™æ€§ï¼Œé€šè¿‡åˆ›æ–°çš„è”åˆæ—¶ç©ºæ³¨æ„åŠ›æœºåˆ¶ (Joint Spatio-Temporal Attention) å°†åŠ¨æ€ä½ç§»æ•°æ®ä¸é™æ€ç‰©ç†å…ˆéªŒ (Static Physical Priors) è¿›è¡Œæ·±åº¦èåˆå¹¶ç»Ÿä¸€å¤„ç†ã€‚åœ¨å…¬å…± EGMS æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å‹è¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ (SOTA)ï¼Œå…¶é•¿ç¨‹é¢„æµ‹çš„å‡æ–¹æ ¹è¯¯å·® (RMSE) è¾ƒ STGCN å’Œ STAEformer ç­‰åŸºçº¿æ¨¡å‹é™ä½äº†ä¸€ä¸ªæ•°é‡çº§ã€‚ç ”ç©¶ç»“æœæœ€ç»ˆè¯å®ï¼Œæ·±åº¦å¤šæ¨¡æ€èåˆèƒ½åŠ›å¯¹äºå®ç°æ­¤ç±»å¤æ‚æ—¶ç©ºé¢„æµ‹é—®é¢˜çš„æ€§èƒ½çªç ´å…·æœ‰è‡³å…³é‡è¦çš„ä½œç”¨ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This paper is submitted to IEEE Transactions on Geoscience and Remote Sensing for reviewing",
      "pdf_url": "https://arxiv.org/pdf/2509.25393v2",
      "published_date": "2025-09-29 18:49:04 UTC",
      "updated_date": "2025-10-01 11:00:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:44:19.566641+00:00"
    },
    {
      "arxiv_id": "2509.25390v1",
      "title": "SpinBench: Perspective and Rotation as a Lens on Spatial Reasoning in VLMs",
      "title_zh": "SpinBenchï¼šä»¥è§†è§’ä¸æ—‹è½¬ä¸ºåˆ‡å…¥ç‚¹æ¢ç©¶è§†è§‰è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†",
      "authors": [
        "Yuyou Zhang",
        "Radu Corcodel",
        "Chiori Hori",
        "Anoop Cherian",
        "Ding Zhao"
      ],
      "abstract": "We present SpinBench, a cognitively grounded diagnostic benchmark for evaluating spatial reasoning in vision language models (VLMs). SpinBench is designed around the core challenge of spatial reasoning: perspective taking, the ability to reason about how scenes and object relations change under viewpoint transformation. Since perspective taking requires multiple cognitive capabilities, such as recognizing objects across views, relative positions grounding, and mentally simulating transformations, SpinBench introduces a set of fine-grained diagnostic categories. Our categories target translation, rotation, object relative pose, and viewpoint change, and are progressively structured so that single-object simpler tasks scaffold toward the most demanding multi-object perspective-taking setting. We evaluate 37 state-of-the-art VLMs, both proprietary and open source. Results reveal systematic weaknesses: strong egocentric bias, poor rotational understanding, and inconsistencies under symmetrical and syntactic reformulations. Scaling analysis shows both smooth improvements and emergent capabilities. While human subjects achieve high accuracy (91.2\\%), task difficulty as measured by human response time shows strong correlation with VLM accuracy, indicating that SpinBench captures spatial reasoning challenges shared across humans and VLMs. We believe SpinBench provides critical insights into spatial reasoning in VLMs and highlights key gaps in their ability to reason about physical space. Our website can be found at https://spinbench25.github.io/.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SpinBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºè®¤çŸ¥ç§‘å­¦çš„è¯Šæ–­æ€§åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)çš„ç©ºé—´æ¨ç†(Spatial Reasoning)èƒ½åŠ›ã€‚SpinBenchå›´ç»•ç©ºé—´æ¨ç†çš„æ ¸å¿ƒæŒ‘æˆ˜â€”â€”é€è§†å˜æ¢(Perspective Taking)è®¾è®¡ï¼Œé€šè¿‡å¹³ç§»(Translation)ã€æ—‹è½¬(Rotation)ã€ç‰©ä½“ç›¸å¯¹å§¿æ€å’Œè§†è§’å˜åŒ–ç­‰ç»†ç²’åº¦è¯Šæ–­ç±»åˆ«ï¼Œè€ƒå¯Ÿæ¨¡å‹åœ¨è§†è§’è½¬æ¢ä¸‹å¯¹åœºæ™¯åŠç‰©ä½“å…³ç³»çš„ç†è§£ã€‚ç ”ç©¶å›¢é˜Ÿå¯¹37ä¸ªæœ€å…ˆè¿›çš„å¼€æºåŠé—­æºVLMsè¿›è¡Œäº†è¯„ä¼°ï¼Œå‘ç°æ¨¡å‹åœ¨ç©ºé—´è®¤çŸ¥ä¸Šå­˜åœ¨æ˜¾è‘—çš„è‡ªæˆ‘ä¸­å¿ƒåè§(Egocentric Bias)ã€æ—‹è½¬ç†è§£æ¬ ç¼ºä»¥åŠåœ¨å¥æ³•æ”¹å†™ä¸‹çš„è¡¨ç°ä¸ä¸€è‡´ç­‰ç³»ç»Ÿæ€§ç¼ºé™·ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œäººç±»ååº”æ—¶é—´ä¸VLMå‡†ç¡®ç‡ä¹‹é—´å­˜åœ¨å¼ºç›¸å…³æ€§ï¼Œè¯æ˜äº†SpinBenchèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰äººç±»ä¸æ¨¡å‹å…±æœ‰çš„ç©ºé—´æ¨ç†æŒ‘æˆ˜ã€‚è¯¥ç ”ç©¶ä¸ä»…æ­ç¤ºäº†VLMsåœ¨ç‰©ç†ç©ºé—´ç†è§£æ–¹é¢çš„å…³é”®å·®è·ï¼Œä¹Ÿä¸ºæœªæ¥æå‡ç›¸å…³èƒ½åŠ›æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25390v1",
      "published_date": "2025-09-29 18:48:16 UTC",
      "updated_date": "2025-09-29 18:48:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:44:31.666075+00:00"
    },
    {
      "arxiv_id": "2510.09634v1",
      "title": "Responsible AI Adoption in the Public Sector: A Data-Centric Taxonomy of AI Adoption Challenges",
      "title_zh": "å…¬å…±éƒ¨é—¨è´Ÿè´£ä»»çš„äººå·¥æ™ºèƒ½é‡‡çº³ï¼šäººå·¥æ™ºèƒ½é‡‡çº³æŒ‘æˆ˜çš„ä»¥æ•°æ®ä¸ºä¸­å¿ƒåˆ†ç±»ä½“ç³»",
      "authors": [
        "Anastasija Nikiforova",
        "Martin Lnenicka",
        "Ulf Melin",
        "David Valle-Cruz",
        "Asif Gill",
        "Cesar Casiano Flores",
        "Emyana Sirait",
        "Mariusz Luterek",
        "Richard Michael Dreyling",
        "Barbora Tesarova"
      ],
      "abstract": "Despite Artificial Intelligence (AI) transformative potential for public sector services, decision-making, and administrative efficiency, adoption remains uneven due to complex technical, organizational, and institutional challenges. Responsible AI frameworks emphasize fairness, accountability, and transparency, aligning with principles of trustworthy AI and fair AI, yet remain largely aspirational, overlooking technical and institutional realities, especially foundational data and governance. This study addresses this gap by developing a taxonomy of data-related challenges to responsible AI adoption in government. Based on a systematic review of 43 studies and 21 expert evaluations, the taxonomy identifies 13 key challenges across technological, organizational, and environmental dimensions, including poor data quality, limited AI-ready infrastructure, weak governance, misalignment in human-AI decision-making, economic and environmental sustainability concerns. Annotated with institutional pressures, the taxonomy serves as a diagnostic tool to surface 'symptoms' of high-risk AI deployment and guides policymakers in building the institutional and data governance conditions necessary for responsible AI adoption.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…¬å…±éƒ¨é—¨åœ¨äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰åº”ç”¨ä¸­é¢ä¸´çš„æŠ€æœ¯ã€ç»„ç»‡å’Œåˆ¶åº¦æŒ‘æˆ˜ï¼Œæ¢è®¨äº†Responsible AIæ¡†æ¶åœ¨è½å®è¿‡ç¨‹ä¸­å­˜åœ¨çš„ç†è®ºä¸ç°å®è„±èŠ‚é—®é¢˜ã€‚ç ”ç©¶äººå‘˜é€šè¿‡å¯¹43é¡¹ç ”ç©¶çš„ç³»ç»Ÿç»¼è¿°ä»¥åŠ21ä½ä¸“å®¶çš„è¯„ä¼°ï¼Œå¼€å‘äº†ä¸€å¥—ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„AIåº”ç”¨æŒ‘æˆ˜åˆ†ç±»æ³•ï¼ˆTaxonomyï¼‰ã€‚è¯¥åˆ†ç±»æ³•ä»æŠ€æœ¯ã€ç»„ç»‡å’Œç¯å¢ƒä¸‰ä¸ªç»´åº¦è¯†åˆ«äº†13é¡¹æ ¸å¿ƒæŒ‘æˆ˜ï¼Œæ¶µç›–äº†æ•°æ®è´¨é‡å·®ï¼ˆpoor data qualityï¼‰ã€AIåŸºç¡€è®¾æ–½ä¸è¶³ï¼ˆlimited AI-ready infrastructureï¼‰ã€æ²»ç†è–„å¼±ï¼ˆweak governanceï¼‰ä»¥åŠäººæœºå†³ç­–ä¸åŒ¹é…ç­‰å…³é”®é—®é¢˜ã€‚ç ”ç©¶è¿˜ç»“åˆåˆ¶åº¦å‹åŠ›ï¼ˆinstitutional pressuresï¼‰å¯¹è¿™äº›æŒ‘æˆ˜è¿›è¡Œäº†æ ‡æ³¨ï¼Œå¹¶æ·±å…¥æ¢è®¨äº†ç»æµå’Œç¯å¢ƒçš„å¯æŒç»­æ€§å¿§è™‘ã€‚è¯¥åˆ†ç±»æ³•å¯ä½œä¸ºè¯„ä¼°é«˜é£é™©AIéƒ¨ç½²çš„è¯Šæ–­å·¥å…·ï¼Œä¸ºæ”¿ç­–åˆ¶å®šè€…æ„å»ºè´Ÿè´£ä»»AIæ‰€éœ€çš„åˆ¶åº¦å’Œæ•°æ®æ²»ç†æ¡ä»¶æä¾›äº†é‡è¦æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.09634v1",
      "published_date": "2025-09-29 18:42:09 UTC",
      "updated_date": "2025-09-29 18:42:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:44:43.365188+00:00"
    },
    {
      "arxiv_id": "2509.25380v1",
      "title": "Predicting Training Re-evaluation Curves Enables Effective Data Curriculums for LLMs",
      "title_zh": "é¢„æµ‹è®­ç»ƒé‡è¯„ä¼°æ›²çº¿åŠ©åŠ›å¤§è¯­è¨€æ¨¡å‹æ„å»ºé«˜æ•ˆæ•°æ®è¯¾ç¨‹",
      "authors": [
        "Shane Bergsma",
        "Nolan Dey",
        "Joel Hestness"
      ],
      "abstract": "Data curriculums have become central to successful LLM training, yet principles governing optimal data placement remain unclear. We introduce the *training re-evaluation curve (TREC)*, a diagnostic that retrospectively evaluates training batches *using the final model weights*. The TREC characterizes how well a trained model retains training data as a function of *when* the data was encountered during training. Analyzing TRECs for models from 111M to 3.9B parameters, we show that placing high-quality data at low points on the TREC significantly improves performance. Importantly, while a TREC is initially observable only after training, we demonstrate it can be *predicted in advance* from AdamW's implicit EMA coefficients, enabling proactive curriculum design. By predicting TRECs for published training recipes, we explain prior ablations and reveal suboptimal data placements. We also align high-quality data with TREC minima in order to improve continual pre-training of a 3.9B-parameter LLM trained on 900B tokens.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†è®­ç»ƒé‡æ–°è¯„ä¼°æ›²çº¿ (Training Re-evaluation Curve, TREC) ä½œä¸ºä¸€ç§è¯Šæ–­å·¥å…·ï¼Œæ—¨åœ¨é€šè¿‡æœ€ç»ˆæ¨¡å‹æƒé‡è¿½æº¯æ€§åœ°è¯„ä¼°è®­ç»ƒæ‰¹æ¬¡ï¼Œå¹¶æ­ç¤ºæ¨¡å‹å¯¹æ•°æ®çš„ä¿ç•™ç¨‹åº¦ä¸æ•°æ®åœ¨è®­ç»ƒä¸­å‡ºç°æ—¶é—´ç‚¹çš„å‡½æ•°å…³ç³»ã€‚é€šè¿‡åˆ†æä» 111M åˆ° 3.9B å‚æ•°è§„æ¨¡çš„æ¨¡å‹ï¼Œç ”ç©¶å‘ç°å°†é«˜è´¨é‡æ•°æ®æ”¾ç½®åœ¨ TREC çš„ä½ç‚¹ä½ç½®èƒ½æ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½ã€‚è™½ç„¶ TREC é€šå¸¸åœ¨è®­ç»ƒåæ‰èƒ½è§‚å¯Ÿåˆ°ï¼Œä½†è¯¥ç ”ç©¶è¯æ˜å¯ä»¥åˆ©ç”¨ AdamW ä¼˜åŒ–å™¨çš„éšå¼æŒ‡æ•°ç§»åŠ¨å¹³å‡ (EMA) ç³»æ•°å¯¹å…¶è¿›è¡Œæå‰é¢„æµ‹ï¼Œä»è€Œå®ç°ä¸»åŠ¨çš„è¯¾ç¨‹è®¾è®¡ (Data Curriculum Design)ã€‚è¿™ä¸€æ–¹æ³•ä¸ä»…è§£é‡Šäº†ä»¥å¾€è®­ç»ƒé…æ–¹ä¸­å­˜åœ¨çš„æ¬¡ä¼˜æ•°æ®æ”¾ç½®é—®é¢˜ï¼Œè¿˜ä¸ºä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) çš„è®­ç»ƒæä¾›äº†æ–°è·¯å¾„ã€‚å®éªŒè¡¨æ˜ï¼Œé€šè¿‡å°†é«˜è´¨é‡æ•°æ®ä¸ TREC æå°å€¼å¯¹é½ï¼Œç ”ç©¶æˆåŠŸæ”¹è¿›äº†ä¸€ä¸ªåœ¨ 900B token ä¸Šè®­ç»ƒçš„ 3.9B å‚æ•°æ¨¡å‹çš„æŒç»­é¢„è®­ç»ƒæ•ˆæœã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25380v1",
      "published_date": "2025-09-29 18:31:35 UTC",
      "updated_date": "2025-09-29 18:31:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:44:47.071504+00:00"
    },
    {
      "arxiv_id": "2509.25379v1",
      "title": "Let Physics Guide Your Protein Flows: Topology-aware Unfolding and Generation",
      "title_zh": "ç‰©ç†å¼•å¯¼çš„è›‹ç™½è´¨æµï¼šæ‹“æ‰‘æ„ŸçŸ¥çš„å±•å¼€ä¸ç”Ÿæˆ",
      "authors": [
        "Yogesh Verma",
        "Markus Heinonen",
        "Vikas Garg"
      ],
      "abstract": "Protein structure prediction and folding are fundamental to understanding biology, with recent deep learning advances reshaping the field. Diffusion-based generative models have revolutionized protein design, enabling the creation of novel proteins. However, these methods often neglect the intrinsic physical realism of proteins, driven by noising dynamics that lack grounding in physical principles. To address this, we first introduce a physically motivated non-linear noising process, grounded in classical physics, that unfolds proteins into secondary structures (e.g., alpha helices, linear beta sheets) while preserving topological integrity--maintaining bonds, and preventing collisions. We then integrate this process with the flow-matching paradigm on SE(3) to model the invariant distribution of protein backbones with high fidelity, incorporating sequence information to enable sequence-conditioned folding and expand the generative capabilities of our model. Experimental results demonstrate that the proposed method achieves state-of-the-art performance in unconditional protein generation, producing more designable and novel protein structures while accurately folding monomer sequences into precise protein conformations.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å—ç‰©ç†å¯å‘çš„éçº¿æ€§åŠ å™ªè¿‡ç¨‹ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ‰©æ•£æ¨¡å‹åœ¨è›‹ç™½è´¨è®¾è®¡ä¸­ç¼ºä¹ç‰©ç†çœŸå®æ„Ÿçš„é—®é¢˜ã€‚è¿™ä¸€è¿‡ç¨‹åŸºäºç»å…¸ç‰©ç†å­¦ï¼Œèƒ½å¤Ÿå°†è›‹ç™½è´¨å±•å¼€ä¸ºäºŒçº§ç»“æ„ï¼ˆå¦‚ alpha helices å’Œ linear beta sheetsï¼‰ï¼ŒåŒæ—¶åœ¨ unfolding è¿‡ç¨‹ä¸­ä¿æŒæ‹“æ‰‘å®Œæ•´æ€§ï¼Œç¡®ä¿åŒ–å­¦é”®è¿æ¥å¹¶é˜²æ­¢åŸå­ç¢°æ’ã€‚ç ”ç©¶è¿›ä¸€æ­¥å°†æ­¤ç‰©ç†å¼•å¯¼çš„è¿‡ç¨‹ä¸ SE(3) ä¸Šçš„ flow-matching èŒƒå¼ç›¸ç»“åˆï¼Œä»¥é«˜ä¿çœŸåº¦æ¨¡æ‹Ÿè›‹ç™½è´¨éª¨æ¶çš„ä¸å˜åˆ†å¸ƒï¼Œå¹¶é€šè¿‡æ•´åˆåºåˆ—ä¿¡æ¯å®ç°äº†åºåˆ—æ¡ä»¶ä¸‹çš„è›‹ç™½è´¨æŠ˜å ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ unconditional protein generation ä»»åŠ¡ä¸­è¾¾åˆ°äº† state-of-the-art æ€§èƒ½ï¼Œèƒ½å¤Ÿäº§ç”Ÿæ›´å…·è®¾è®¡æ„Ÿå’Œæ–°é¢–æ€§çš„è›‹ç™½è´¨ç»“æ„ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹è¿˜èƒ½å‡†ç¡®åœ°å°†å•ä½“åºåˆ—æŠ˜å æˆç²¾ç¡®çš„è›‹ç™½è´¨æ„è±¡ï¼Œä¸ºè›‹ç™½è´¨ç»“æ„é¢„æµ‹ä¸ç”Ÿæˆé¢†åŸŸæä¾›äº†å…·æœ‰ç‰©ç†çº¦æŸçš„æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25379v1",
      "published_date": "2025-09-29 18:31:22 UTC",
      "updated_date": "2025-09-29 18:31:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:44:50.178507+00:00"
    },
    {
      "arxiv_id": "2509.25376v1",
      "title": "Cold-Start Active Correlation Clustering",
      "title_zh": "å†·å¯åŠ¨ä¸»åŠ¨ç›¸å…³æ€§èšç±»",
      "authors": [
        "Linus Aronsson",
        "Han Wu",
        "Morteza Haghir Chehreghani"
      ],
      "abstract": "We study active correlation clustering where pairwise similarities are not provided upfront and must be queried in a cost-efficient manner through active learning. Specifically, we focus on the cold-start scenario, where no true initial pairwise similarities are available for active learning. To address this challenge, we propose a coverage-aware method that encourages diversity early in the process. We demonstrate the effectiveness of our approach through several synthetic and real-world experiments.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†Active Correlation Clusteringé—®é¢˜ï¼Œé’ˆå¯¹pairwise similaritiesæ— æ³•é¢„å…ˆè·å–ä¸”å¿…é¡»é€šè¿‡active learningè¿›è¡ŒæŸ¥è¯¢çš„åœºæ™¯ã€‚ç ”ç©¶é‡ç‚¹è§£å†³äº†cold-startæŒ‘æˆ˜ï¼Œå³åœ¨ä¸»åŠ¨å­¦ä¹ åˆæœŸæ²¡æœ‰ä»»ä½•çœŸå®åˆå§‹ç›¸ä¼¼åº¦ä¿¡æ¯çš„æƒ…å†µã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§coverage-awareæ–¹æ³•ï¼Œé€šè¿‡åœ¨å­¦ä¹ è¿‡ç¨‹æ—©æœŸé¼“åŠ±diversityæ¥æå‡æ¨¡å‹æ€§èƒ½ã€‚è¯¥æ–¹æ³•æ—¨åœ¨é€šè¿‡ä¼˜åŒ–æ ·æœ¬è¦†ç›–ç‡æ¥å…‹æœé›¶åˆå§‹ä¿¡æ¯çš„é™åˆ¶ã€‚é€šè¿‡åœ¨å¤šä¸ªåˆæˆå’ŒçœŸå®ä¸–ç•Œå®éªŒä¸­çš„éªŒè¯ï¼Œè¯¥ç ”ç©¶è¯æ˜äº†è¯¥æ–¹æ³•åœ¨å¤„ç†å†·å¯åŠ¨ç›¸å…³æ€§èšç±»ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25376v1",
      "published_date": "2025-09-29 18:29:21 UTC",
      "updated_date": "2025-09-29 18:29:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:45:01.492057+00:00"
    },
    {
      "arxiv_id": "2509.25374v2",
      "title": "Saliency Guided Longitudinal Medical Visual Question Answering",
      "title_zh": "æ˜¾è‘—æ€§å¼•å¯¼çš„çºµå‘åŒ»å­¦è§†è§‰é—®ç­”",
      "authors": [
        "Jialin Wu",
        "Xiaofeng Liu"
      ],
      "abstract": "Longitudinal medical visual question answering (Diff-VQA) requires comparing paired studies from different time points and answering questions about clinically meaningful changes. In this setting, the difference signal and the consistency of visual focus across time are more informative than absolute single-image findings. We propose a saliency-guided encoder-decoder for chest X-ray Diff-VQA that turns post-hoc saliency into actionable supervision. The model first performs a lightweight near-identity affine pre-alignment to reduce nuisance motion between visits. It then executes a within-epoch two-step loop: step 1 extracts a medically relevant keyword from the answer and generates keyword-conditioned Grad-CAM on both images to obtain disease-focused saliency; step 2 applies the shared saliency mask to both time points and generates the final answer. This closes the language-vision loop so that the terms that matter also guide where the model looks, enforcing spatially consistent attention on corresponding anatomy. On Medical-Diff-VQA, the approach attains competitive performance on BLEU, ROUGE-L, CIDEr, and METEOR while providing intrinsic interpretability. Notably, the backbone and decoder are general-domain pretrained without radiology-specific pretraining, highlighting practicality and transferability. These results support saliency-conditioned generation with mild pre-alignment as a principled framework for longitudinal reasoning in medical VQA.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹çºµå‘åŒ»å­¦è§†è§‰é—®ç­”ï¼ˆDiff-VQAï¼‰ä»»åŠ¡ï¼Œæå‡ºäº†ä¸€ä¸ªæ˜¾è‘—æ€§å¼•å¯¼ï¼ˆSaliency-Guidedï¼‰çš„ç¼–ç å™¨-è§£ç å™¨æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¯¹æ¯”ä¸åŒæ—¶é—´ç‚¹çš„èƒ¸éƒ¨ X å°„çº¿å›¾åƒæ¥å›ç­”ä¸´åºŠå˜åŒ–é—®é¢˜ã€‚è¯¥æ–¹æ³•é¦–å…ˆé‡‡ç”¨è½»é‡çº§çš„ä»¿å°„é¢„å¯¹é½ï¼ˆaffine pre-alignmentï¼‰æŠ€æœ¯å‡å°‘ä¸¤æ¬¡å½±åƒæ£€æŸ¥é—´çš„è¿åŠ¨å¹²æ‰°ã€‚æ¨¡å‹é€šè¿‡ä¸€ä¸ªä¸¤æ­¥å¾ªç¯è¿è¡Œï¼šç¬¬ä¸€æ­¥ä»ç­”æ¡ˆä¸­æå–åŒ»å­¦å…³é”®è¯å¹¶ç”Ÿæˆå…³é”®è¯å¼•å¯¼çš„ Grad-CAM ä»¥å®šä½ç—…ç¶æ˜¾è‘—æ€§åŒºåŸŸï¼›ç¬¬äºŒæ­¥å°†å…±äº«æ˜¾è‘—æ€§æ©ç åº”ç”¨äºä¸¤ä¸ªæ—¶é—´ç‚¹çš„å›¾åƒï¼Œä»è€Œç”Ÿæˆæœ€ç»ˆçš„å¯¹æ¯”æ€§å›ç­”ã€‚è¿™ç§æœºåˆ¶æœ‰æ•ˆé—­åˆäº†è§†è§‰ä¸è¯­è¨€çš„å¾ªç¯ï¼Œç¡®ä¿æ¨¡å‹åœ¨å¤„ç†ä¸åŒæ—¶æœŸçš„å½±åƒæ—¶èƒ½å¯¹ç›¸åº”è§£å‰–ç»“æ„ä¿æŒç©ºé—´ä¸€è‡´çš„æ³¨æ„åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ Medical-Diff-VQA æ•°æ®é›†çš„ BLEUã€ROUGE-Lã€CIDEr å’Œ METEOR æŒ‡æ ‡ä¸Šå‡å–å¾—äº†æå…·ç«äº‰åŠ›çš„è¡¨ç°ï¼Œå¹¶æä¾›äº†è‰¯å¥½çš„å†…åœ¨å¯è§£é‡Šæ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶çš„éª¨å¹²ç½‘ç»œå’Œè§£ç å™¨ä»…ä½¿ç”¨é€šç”¨é¢†åŸŸé¢„è®­ç»ƒæƒé‡å³å¯è¾¾åˆ°ä¼˜å¼‚æ•ˆæœï¼Œæ— éœ€ä¸“é—¨çš„æ”¾å°„å­¦é¢†åŸŸé¢„è®­ç»ƒï¼Œä½“ç°äº†æå¼ºçš„å®ç”¨æ€§å’Œä»»åŠ¡è¿ç§»èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "Published in NeurIPS Workshop",
      "pdf_url": "https://arxiv.org/pdf/2509.25374v2",
      "published_date": "2025-09-29 18:26:17 UTC",
      "updated_date": "2025-12-06 22:36:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:44:58.594426+00:00"
    },
    {
      "arxiv_id": "2509.25373v4",
      "title": "From Perception to Cognition: A Survey of Vision-Language Interactive Reasoning in Multimodal Large Language Models",
      "title_zh": "ä»æ„ŸçŸ¥åˆ°è®¤çŸ¥ï¼šå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„è§†è§‰-è¯­è¨€äº¤äº’æ¨ç†ç»¼è¿°",
      "authors": [
        "Chenyue Zhou",
        "Mingxuan Wang",
        "Yanbiao Ma",
        "Chenxu Wu",
        "Wanyi Chen",
        "Zhe Qian",
        "Xinyu Liu",
        "Yiwei Zhang",
        "Junhao Wang",
        "Hengbo Xu",
        "Fei Luo",
        "Xiaohua Chen",
        "Xiaoshuai Hao",
        "Hehan Li",
        "Andi Zhang",
        "Wenxuan Wang",
        "Kaiyan Zhang",
        "Guoli Jia",
        "Lingling Li",
        "Zhiwu Lu",
        "Yang Lu",
        "Yike Guo"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) strive to achieve a profound, human-like understanding of and interaction with the physical world, but often exhibit a shallow and incoherent integration when acquiring information (Perception) and conducting reasoning (Cognition). This disconnect leads to a spectrum of reasoning failures, with hallucination being the most prominent. Collectively, these issues expose a fundamental challenge: the ability to process pixels does not yet confer the ability to construct a coherent, credible internal world model. To systematically dissect and address this challenge, this survey introduces a novel and unified analytical framework: ``From Perception to Cognition.\" We deconstruct the complex process of vision-language interactive understanding into two interdependent layers: Perception, the foundational ability to accurately extract visual information and achieve fine-grained alignment with textual instructions; and Cognition, the higher-order capability for proactive, multi-step, goal-oriented reasoning built upon this perceptual foundation, the core of which is the formation of a dynamic observe-think-verify reasoning loop. Guided by this framework, this paper systematically analyzes the key bottlenecks of current MLLMs at both layers. It surveys the landscape of cutting-edge methods designed to address these challenges, spanning from techniques that enhance low-level visual representations to those that improve high-level reasoning paradigms. Furthermore, we review critical benchmarks and delineate future research directions. This survey aims to provide the research community with a clear, structured perspective for understanding the intrinsic limitations of current MLLMs and to illuminate the path toward building next-generation models capable of deep reasoning and a genuine understanding of the world.",
      "tldr_zh": "è¯¥ç»¼è¿°ç³»ç»Ÿåœ°å‰–æäº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) åœ¨ä»æ„ŸçŸ¥ (Perception) å‘è®¤çŸ¥ (Cognition) è·¨è¶Šè¿‡ç¨‹ä¸­é¢ä¸´çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å¹»è§‰ç­‰æ¨ç†å¤±æ•ˆé—®é¢˜ã€‚ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„â€œFrom Perception to Cognitionâ€åˆ†ææ¡†æ¶ï¼Œå°†è§†è§‰è¯­è¨€äº¤äº’ç†è§£åˆ†ä¸ºä¸¤ä¸ªç›¸äº’ä¾èµ–çš„å±‚çº§ï¼šæ„ŸçŸ¥å±‚ä¾§é‡äºå‡†ç¡®æå–è§†è§‰ä¿¡æ¯å¹¶å®ç°ç»†ç²’åº¦çš„æŒ‡ä»¤å¯¹é½ï¼Œè®¤çŸ¥å±‚åˆ™å¼ºè°ƒåœ¨æ„ŸçŸ¥åŸºç¡€ä¸Šé€šè¿‡â€œè§‚å¯Ÿ-æ€è€ƒ-éªŒè¯â€ (observe-think-verify) åŠ¨æ€å¾ªç¯å®ç°é«˜é˜¶ã€å¤šæ­¥çš„ç›®æ ‡å¯¼å‘æ¨ç†ã€‚è®ºæ–‡è¯¦ç»†åˆ†æäº†å½“å‰æ¨¡å‹åœ¨è¿™äº›å±‚çº§çš„å…³é”®ç“¶é¢ˆï¼Œå¹¶ç»¼è¿°äº†ä»åº•å±‚è§†è§‰è¡¨å¾å¢å¼ºåˆ°é«˜å±‚æ¨ç†èŒƒå¼æ”¹è¿›çš„å‰æ²¿æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥ç»¼è¿°è¿˜å›é¡¾äº†é‡è¦è¯„æµ‹åŸºå‡† (benchmarks) å¹¶æŒ‡æ˜äº†æœªæ¥ç ”ç©¶æ–¹å‘ï¼Œæ—¨åœ¨æŒ‡å¯¼æ„å»ºå…·æœ‰æ·±åº¦æ¨ç†èƒ½åŠ›å’ŒçœŸå®ä¸–ç•Œç†è§£èƒ½åŠ›çš„ä¸‹ä¸€ä»£æ¨¡å‹ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25373v4",
      "published_date": "2025-09-29 18:25:40 UTC",
      "updated_date": "2025-10-16 08:01:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:45:04.887515+00:00"
    },
    {
      "arxiv_id": "2509.25370v1",
      "title": "Where LLM Agents Fail and How They can Learn From Failures",
      "title_zh": "LLMæ™ºèƒ½ä½“å¤±æ•ˆåˆ†æåŠå…¶å¤±è´¥å­¦ä¹ æœºåˆ¶",
      "authors": [
        "Kunlun Zhu",
        "Zijia Liu",
        "Bingxuan Li",
        "Muxin Tian",
        "Yingxuan Yang",
        "Jiaxun Zhang",
        "Pengrui Han",
        "Qipeng Xie",
        "Fuyang Cui",
        "Weijia Zhang",
        "Xiaoteng Ma",
        "Xiaodong Yu",
        "Gowtham Ramesh",
        "Jialian Wu",
        "Zicheng Liu",
        "Pan Lu",
        "James Zou",
        "Jiaxuan You"
      ],
      "abstract": "Large Language Model (LLM) agents, which integrate planning, memory, reflection, and tool-use modules, have shown promise in solving complex, multi-step tasks. Yet their sophisticated architectures amplify vulnerability to cascading failures, where a single root-cause error propagates through subsequent decisions, leading to task failure. Current systems lack a framework that can comprehensively understand agent error in a modular and systemic way, and therefore fail to detect these errors accordingly. We address this gap with three contributions. First, we introduce the AgentErrorTaxonomy, a modular classification of failure modes spanning memory, reflection, planning, action, and system-level operations. Second, we construct AgentErrorBench, the first dataset of systematically annotated failure trajectories from ALFWorld, GAIA, and WebShop, grounding error analysis in real-world agent rollouts. Third, we propose AgentDebug, a debugging framework that isolates root-cause failures and provides corrective feedback, enabling agents to recover and iteratively improve. Experiments on AgentErrorBench show that AgentDebug achieves 24% higher all-correct accuracy and 17% higher step accuracy compared to the strongest baseline. Beyond detection, the targeted feedback generated by AgentDebug enables LLM agents to iteratively recover from failures, yielding up to 26% relative improvements in task success across ALFWorld, GAIA, and WebShop. These results establish principled debugging as a pathway to more reliable and adaptive LLM agents. The code and data will be available at https://github.com/ulab-uiuc/AgentDebug",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†LLM agentsåœ¨æ‰§è¡Œå¤æ‚ä»»åŠ¡æ—¶å› å¤šæ¨¡å—æ¶æ„å¯¼è‡´çš„è¿é”æ•…éšœé—®é¢˜ï¼ŒæŒ‡å‡ºå½“å‰ç³»ç»Ÿç¼ºä¹ç³»ç»Ÿæ€§æ£€æµ‹å’Œç†è§£é”™è¯¯çš„æ¡†æ¶ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†AgentErrorTaxonomyï¼Œè¿™æ˜¯ä¸€ç§æ¶µç›–è®°å¿†ã€è§„åˆ’ã€è¡ŒåŠ¨ç­‰å±‚é¢çš„æ¨¡å—åŒ–æ•…éšœåˆ†ç±»ä½“ç³»ï¼Œå¹¶æ„å»ºäº†é¦–ä¸ªç³»ç»Ÿæ ‡æ³¨çš„å¤±è´¥è½¨è¿¹æ•°æ®é›†AgentErrorBenchã€‚è¿›ä¸€æ­¥åœ°ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºAgentDebugçš„è°ƒè¯•æ¡†æ¶ï¼Œé€šè¿‡å®šä½æ ¹å› æ•…éšœå¹¶æä¾›çº æ­£æ€§åé¦ˆï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿå®ç°è¿­ä»£æ”¹è¿›ã€‚å®éªŒè¯æ˜ï¼ŒAgentDebugåœ¨AgentErrorBenchä¸Šçš„è¡¨ç°ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå¹¶æ˜¾è‘—æå‡äº†ALFWorldã€GAIAå’ŒWebShopç­‰åŸºå‡†æµ‹è¯•ä¸­çš„ä»»åŠ¡æˆåŠŸç‡ï¼Œæœ€é«˜ç›¸å¯¹æå‡è¾¾26%ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†åŸåˆ™æ€§è°ƒè¯•å¯¹äºæ„å»ºæ›´å¯é ã€æ›´å…·é€‚åº”æ€§çš„LLM agentsçš„å…³é”®ä½œç”¨ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25370v1",
      "published_date": "2025-09-29 18:20:27 UTC",
      "updated_date": "2025-09-29 18:20:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:45:10.496200+00:00"
    },
    {
      "arxiv_id": "2509.25369v1",
      "title": "Generative Value Conflicts Reveal LLM Priorities",
      "title_zh": "ç”Ÿæˆå¼ä»·å€¼å†²çªæ­ç¤º LLM çš„ä¼˜å…ˆçº§",
      "authors": [
        "Andy Liu",
        "Kshitish Ghate",
        "Mona Diab",
        "Daniel Fried",
        "Atoosa Kasirzadeh",
        "Max Kleiman-Weiner"
      ],
      "abstract": "Past work seeks to align large language model (LLM)-based assistants with a target set of values, but such assistants are frequently forced to make tradeoffs between values when deployed. In response to the scarcity of value conflict in existing alignment datasets, we introduce ConflictScope, an automatic pipeline to evaluate how LLMs prioritize different values. Given a user-defined value set, ConflictScope automatically generates scenarios in which a language model faces a conflict between two values sampled from the set. It then prompts target models with an LLM-written \"user prompt\" and evaluates their free-text responses to elicit a ranking over values in the value set. Comparing results between multiple-choice and open-ended evaluations, we find that models shift away from supporting protective values, such as harmlessness, and toward supporting personal values, such as user autonomy, in more open-ended value conflict settings. However, including detailed value orderings in models' system prompts improves alignment with a target ranking by 14%, showing that system prompting can achieve moderate success at aligning LLM behavior under value conflict. Our work demonstrates the importance of evaluating value prioritization in models and provides a foundation for future work in this area.",
      "tldr_zh": "é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLM) åœ¨å®é™…éƒ¨ç½²ä¸­é¢ä¸´çš„ä»·å€¼æƒè¡¡é—®é¢˜ä»¥åŠç°æœ‰å¯¹é½æ•°æ®é›†ç¼ºä¹ä»·å€¼å†²çªåœºæ™¯çš„ç°çŠ¶ï¼Œè¯¥ç ”ç©¶æå‡ºäº† ConflictScopeï¼Œä¸€ä¸ªç”¨äºè¯„ä¼° LLM ä»·å€¼ä¼˜å…ˆçº§æ’åºçš„è‡ªåŠ¨åŒ–æµç¨‹ã€‚è¯¥å·¥å…·èƒ½å¤Ÿæ ¹æ®é¢„è®¾ä»·å€¼é›†è‡ªåŠ¨ç”Ÿæˆä¸¤ä¸ªä»·å€¼ç›¸äº’å†²çªçš„ç‰¹å®šåœºæ™¯ï¼Œå¹¶é€šè¿‡è¯„ä¼°æ¨¡å‹çš„å¼€æ”¾å¼æ–‡æœ¬å“åº”æ¥æ­ç¤ºå…¶å†…åœ¨çš„ä»·å€¼åå¥½æ’åºã€‚é€šè¿‡å¯¹æ¯”å¤šé¡¹é€‰æ‹©ä¸å¼€æ”¾å¼è¯„ä¼°ï¼Œç ”ç©¶å‘ç°æ¨¡å‹åœ¨ç”Ÿæˆå¼ç¯å¢ƒä¸‹ä¼šä»æ”¯æŒä¿æŠ¤æ€§ä»·å€¼ (å¦‚ harmlessness) è½¬å‘æ”¯æŒä¸ªäººä»·å€¼ (å¦‚ user autonomy)ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¯æ˜åœ¨ system prompts ä¸­åŠ å…¥è¯¦ç»†çš„ä»·å€¼æ’åºè¯´æ˜ï¼Œå¯ä½¿æ¨¡å‹ä¸ç›®æ ‡æ’åºçš„å¯¹é½åº¦æå‡ 14%ï¼Œæœ‰æ•ˆæ”¹å–„äº†æ¨¡å‹åœ¨ä»·å€¼å†²çªä¸‹çš„è¡¨ç°ã€‚è¯¥å·¥ä½œä¸ä»…æ­ç¤ºäº†è¯„ä¼°æ¨¡å‹ä»·å€¼ä¼˜å…ˆçº§çš„é‡è¦æ€§ï¼Œä¹Ÿä¸ºæœªæ¥æ›´å¤æ‚çš„ LLM ä»·å€¼å¯¹é½ç ”ç©¶æä¾›äº†æ–¹æ³•è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25369v1",
      "published_date": "2025-09-29 18:19:43 UTC",
      "updated_date": "2025-09-29 18:19:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:45:13.284592+00:00"
    },
    {
      "arxiv_id": "2509.25361v2",
      "title": "Structural Reward Model: Enhancing Interpretability, Efficiency, and Scalability in Reward Modeling",
      "title_zh": "ç»“æ„åŒ–å¥–åŠ±æ¨¡å‹ï¼šæå‡å¥–åŠ±å»ºæ¨¡çš„å¯è§£é‡Šæ€§ã€æ•ˆç‡ä¸å¯æ‰©å±•æ€§",
      "authors": [
        "Xiaoyu Liu",
        "Di Liang",
        "Chang Dai",
        "Hongyu Shan",
        "Peiyang Liu",
        "Yonghao Liu",
        "Muling Wu",
        "Yuntao Li",
        "Xianjie Wu",
        "LI Miao",
        "Jiangrong Shen",
        "Minlong Peng"
      ],
      "abstract": "Reward Models (RMs) are key components for evaluating and guiding language model outputs. However, traditional scalar RMs often struggle with incorporating contextual and background information during inference, leading to incomplete evaluations. Generative RMs (GRMs) attempt to address these limitations by generating intermediate reasoning steps. Yet, their uncontrolled black-box nature and inefficiency due to sequential decoding hinder their industrial deployment. Industrial scenarios, such as search and recommendation systems, often involve single-domain tasks requiring evaluation along specific dimensions. In such contexts, diagnosing \"bad cases\" necessitates structured feedback to identify and optimize dimension-specific issues. In this paper, we propose the Structural Reward Model (SRM), a modular and interpretable framework integrating side-branch models as auxiliary feature generators. By introducing fine-grained dimensions, SRMs enable interpretable and efficient evaluation, facilitating targeted diagnostics and optimization. This structured approach ensures adaptability and scalability for industrial applications. Through comprehensive experiments, we demonstrate that SRMs outperform scalar RMs and GRMs in robustness and alignment with human preferences. The modular design further supports efficient optimization for practical scenarios, allowing SRM to provide a practical reward modeling solution for industry.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Structural Reward Model (SRM)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨æå‡å¥–åŠ±å»ºæ¨¡å¯è§£é‡Šæ€§ã€æ•ˆç‡å’Œæ‰©å±•æ€§çš„æ¨¡å—åŒ–æ¡†æ¶ã€‚é’ˆå¯¹ä¼ ç»ŸScalar RMséš¾ä»¥æ•´åˆèƒŒæ™¯ä¿¡æ¯ä»¥åŠGenerative RMs (GRMs) é»‘ç›’åŒ–ä¸”è§£ç æ•ˆç‡ä½çš„é—®é¢˜ï¼ŒSRMé€šè¿‡é›†æˆä¾§æ”¯æ¨¡å‹ (side-branch models) ä½œä¸ºè¾…åŠ©ç‰¹å¾ç”Ÿæˆå™¨ï¼Œå¼•å…¥äº†ç»†ç²’åº¦çš„è¯„ä¼°ç»´åº¦ã€‚è¿™ç§ç»“æ„åŒ–è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿé’ˆå¯¹æœç´¢å’Œæ¨èç­‰å·¥ä¸šåœºæ™¯ä¸­çš„ç‰¹å®šç»´åº¦è¿›è¡Œç²¾å‡†è¯Šæ–­ä¸ä¼˜åŒ–ï¼Œæœ‰æ•ˆå¤„ç†â€œé”™è¯¯æ¡ˆä¾‹â€ (bad cases)ã€‚å®éªŒè¯æ˜ï¼ŒSRMåœ¨é²æ£’æ€§å’Œäººç±»åå¥½å¯¹é½æ–¹é¢å‡ä¼˜äºæ ‡é‡å’Œç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹ã€‚å…¶æ¨¡å—åŒ–æ¶æ„ä¸ä»…æ”¯æŒé«˜æ•ˆçš„å®é™…åœºæ™¯ä¼˜åŒ–ï¼Œä¹Ÿä¸ºå·¥ä¸šçº§çš„å¤§è§„æ¨¡åº”ç”¨æä¾›äº†å…·æœ‰é«˜åº¦é€‚åº”æ€§çš„å¥–åŠ±å»ºæ¨¡è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25361v2",
      "published_date": "2025-09-29 18:09:25 UTC",
      "updated_date": "2025-10-03 20:52:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:45:16.592821+00:00"
    },
    {
      "arxiv_id": "2509.25359v1",
      "title": "From Internal Representations to Text Quality: A Geometric Approach to LLM Evaluation",
      "title_zh": "ä»å†…éƒ¨è¡¨å¾åˆ°æ–‡æœ¬è´¨é‡ï¼šå¤§è¯­è¨€æ¨¡å‹è¯„ä¼°çš„å‡ ä½•æ–¹æ³•",
      "authors": [
        "Viacheslav Yusupov",
        "Danil Maksimov",
        "Ameliia Alaeva",
        "Anna Vasileva",
        "Anna Antipina",
        "Tatyana Zaitseva",
        "Alina Ermilova",
        "Evgeny Burnaev",
        "Egor Shvetsov"
      ],
      "abstract": "This paper bridges internal and external analysis approaches to large language models (LLMs) by demonstrating that geometric properties of internal model representations serve as reliable proxies for evaluating generated text quality. We validate a set of metrics including Maximum Explainable Variance, Effective Rank, Intrinsic Dimensionality, MAUVE score, and Schatten Norms measured across different layers of LLMs, demonstrating that Intrinsic Dimensionality and Effective Rank can serve as universal assessments of text naturalness and quality. Our key finding reveals that different models consistently rank text from various sources in the same order based on these geometric properties, indicating that these metrics reflect inherent text characteristics rather than model-specific artifacts. This allows a reference-free text quality evaluation that does not require human-annotated datasets, offering practical advantages for automated evaluation pipelines.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å†…éƒ¨è¡¨å¾ä¸ç”Ÿæˆæ–‡æœ¬è´¨é‡ä¹‹é—´çš„è”ç³»ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå‡ ä½•å±æ€§çš„è¯„ä¼°æ–¹æ³•ã€‚é€šè¿‡éªŒè¯ Maximum Explainable Varianceã€Effective Rankã€Intrinsic Dimensionality å’Œ Schatten Norms ç­‰ä¸€ç³»åˆ—è·¨å±‚æŒ‡æ ‡ï¼Œç ”ç©¶å‘ç° Intrinsic Dimensionality å’Œ Effective Rank å¯ä»¥ä½œä¸ºæ–‡æœ¬è‡ªç„¶åº¦å’Œè´¨é‡çš„é€šç”¨è¯„ä¼°æ ‡å‡†ã€‚å…³é”®å‘ç°è¡¨æ˜ï¼Œä¸åŒæ¨¡å‹ä¼šåŸºäºè¿™äº›å‡ ä½•å±æ€§å¯¹æ¥è‡ªä¸åŒæ¥æºçš„æ–‡æœ¬è¿›è¡Œä¸€è‡´çš„æ’åºï¼Œè¯æ˜äº†è¿™äº›æŒ‡æ ‡åæ˜ çš„æ˜¯æ–‡æœ¬çš„å›ºæœ‰ç‰¹å¾è€Œéç‰¹å®šæ¨¡å‹çš„ä¼ªå½±ã€‚è¿™ä½¿å¾—è¯¥æ–¹æ³•èƒ½å¤Ÿå®ç°æ— éœ€å‚è€ƒæ–‡æœ¬æˆ–äººå·¥æ ‡æ³¨çš„æ–‡æœ¬è´¨é‡è¯„ä¼°ï¼Œä¸ºè‡ªåŠ¨åŒ–è¯„ä¼°æµç¨‹æä¾›äº†æ˜¾è‘—çš„å®é™…ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25359v1",
      "published_date": "2025-09-29 18:08:47 UTC",
      "updated_date": "2025-09-29 18:08:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:45:23.700718+00:00"
    },
    {
      "arxiv_id": "2509.25346v1",
      "title": "SynthPert: Enhancing LLM Biological Reasoning via Synthetic Reasoning Traces for Cellular Perturbation Prediction",
      "title_zh": "SynthPertï¼šé€šè¿‡åˆæˆæ¨ç†è½¨è¿¹æå‡å¤§è¯­è¨€æ¨¡å‹åœ¨ç»†èƒæ‰°åŠ¨é¢„æµ‹ä¸­çš„ç”Ÿç‰©å­¦æ¨ç†èƒ½åŠ›",
      "authors": [
        "Lawrence Phillips",
        "Marc Boubnovski Martell",
        "Aditya Misra",
        "Josefa Lia Stoisser",
        "Cesar A. Prada-Medina",
        "Rory Donovan-Maiye",
        "Kaspar MÃ¤rtens"
      ],
      "abstract": "Predicting cellular responses to genetic perturbations represents a fundamental challenge in systems biology, critical for advancing therapeutic discovery and virtual cell modeling. While large language models (LLMs) show promise for biological reasoning, their application to perturbation prediction remains underexplored due to challenges in adapting them to structured experimental data. We present SynthPert, a novel method that enhances LLM performance through supervised fine-tuning on synthetic reasoning traces generated by frontier models. Using the PerturbQA benchmark, we demonstrate that our approach not only achieves state-of-the-art performance but surpasses the capabilities of the frontier model that generated the training data. Our results reveal three key insights: (1) Synthetic reasoning traces effectively distill biological knowledge even when partially inaccurate, (2) This approach enables cross-cell-type generalization with 87% accuracy on unseen RPE1 cells, and (3) Performance gains persist despite using only 2% of quality-filtered training data. This work shows the effectiveness of synthetic reasoning distillation for enhancing domain-specific reasoning in LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SynthPertï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡åœ¨å°–ç«¯æ¨¡å‹ç”Ÿæˆçš„åˆæˆæ¨ç†è¸ªè¿¹ (Synthetic Reasoning Traces) ä¸Šè¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒ (Supervised Fine-tuning) æ¥å¢å¼ºå¤§è¯­è¨€æ¨¡å‹ (LLMs) ç”Ÿç‰©æ¨ç†èƒ½åŠ›çš„åˆ›æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•ä¸“æ³¨äºé¢„æµ‹ç»†èƒå¯¹é—ä¼ æ‰°åŠ¨ (Genetic Perturbations) çš„ååº”ï¼Œè¿™æ˜¯ç³»ç»Ÿç”Ÿç‰©å­¦ä¸­æ¨è¿›æ²»ç–—å‘ç°å’Œè™šæ‹Ÿç»†èƒå»ºæ¨¡çš„å…³é”®æŒ‘æˆ˜ã€‚åœ¨ PerturbQA åŸºå‡†æµ‹è¯•ä¸­ï¼ŒSynthPert ä¸ä»…è¾¾åˆ°äº†å½“å‰æœ€ä½³æ€§èƒ½ (State-of-the-art)ï¼Œç”šè‡³è¶…è¶Šäº†ç”Ÿæˆè®­ç»ƒæ•°æ®çš„åŸå§‹å°–ç«¯æ¨¡å‹ã€‚ç ”ç©¶å‘ç°ï¼Œåˆæˆæ¨ç†è¸ªè¿¹èƒ½å¤Ÿæœ‰æ•ˆåœ°è’¸é¦ç”Ÿç‰©å­¦çŸ¥è¯†ï¼Œå³ä½¿åœ¨æ•°æ®éƒ¨åˆ†ä¸å‡†ç¡®çš„æƒ…å†µä¸‹ä¾ç„¶å…·æœ‰é²æ£’æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å±•ç°äº†å¼ºå¤§çš„è·¨ç»†èƒç±»å‹æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨æœªè§è¿‡çš„ RPE1 ç»†èƒä¸Šè¾¾åˆ°äº† 87% çš„å‡†ç¡®ç‡ã€‚ä»¤äººç©ç›®çš„æ˜¯ï¼Œå³ä¾¿ä»…ä½¿ç”¨ 2% ç»è¿‡è´¨é‡è¿‡æ»¤çš„è®­ç»ƒæ•°æ®ï¼Œæ€§èƒ½å¢ç›Šä¾ç„¶æ˜¾è‘—ã€‚è¿™é¡¹å·¥ä½œæœ‰åŠ›è¯æ˜äº†åˆæˆæ¨ç†è’¸é¦åœ¨å¢å¼º LLMs ç‰¹å®šé¢†åŸŸæ¨ç†èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "q-bio.CB",
        "q-bio.GN"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25346v1",
      "published_date": "2025-09-29 18:02:41 UTC",
      "updated_date": "2025-09-29 18:02:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:45:45.594106+00:00"
    },
    {
      "arxiv_id": "2509.25343v1",
      "title": "Spontaneous High-Order Generalization in Neural Theory-of-Mind Networks",
      "title_zh": "ç¥ç»å¿ƒç†ç†è®ºç½‘ç»œä¸­çš„è‡ªå‘é«˜é˜¶æ³›åŒ–",
      "authors": [
        "Yiming Wang",
        "Rui Wang"
      ],
      "abstract": "Theory-of-Mind (ToM) is a core human cognitive capacity for attributing mental states to self and others. Wimmer and Perner demonstrated that humans progress from first- to higher-order ToM within a short span, completing this development before formal education or advanced skill acquisition. In contrast, neural networks represented by autoregressive language models progress from first- to higher-order ToM only alongside gains in advanced skills like reasoning, leaving open whether their trajectory can unfold independently, as in humans. In this research, we provided evidence that neural networks could spontaneously generalize from first- to higher-order ToM without relying on advanced skills. We introduced a neural Theory-of-Mind network (ToMNN) that simulated a minimal cognitive system, acquiring only first-order ToM competence. Evaluations of its second- and third-order ToM abilities showed accuracies well above chance. Also, ToMNN exhibited a sharper decline when generalizing from first- to second-order ToM than from second- to higher orders, and its accuracy decreased with greater task complexity. These perceived difficulty patterns were aligned with human cognitive expectations. Furthermore, the universality of results was confirmed across different parameter scales. Our findings illuminate machine ToM generalization patterns and offer a foundation for developing more human-like cognitive systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç¥ç»å¿ƒç†ç†è®º(Theory-of-Mind, ToM)ç½‘ç»œä¸­çš„è‡ªå‘é«˜é˜¶æ³›åŒ–èƒ½åŠ›ï¼Œæ—¨åœ¨æ¨¡æ‹Ÿäººç±»æ— éœ€é«˜çº§æ¨ç†æŠ€èƒ½å³å¯ä»ä¸€é˜¶å‘é«˜é˜¶ToMå‘å±•çš„è¿‡ç¨‹ã€‚ç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†ä¸€ä¸ªåä¸ºToMNNçš„ç¥ç»å¿ƒç†ç†è®ºç½‘ç»œï¼Œè¯¥ç½‘ç»œä½œä¸ºä¸€ä¸ªæœ€å°è®¤çŸ¥ç³»ç»Ÿï¼Œåœ¨ä»…å­¦ä¹ ä¸€é˜¶ToMèƒ½åŠ›çš„åŸºç¡€ä¸Šå±•ç°å‡ºäº†è‡ªå‘æ³›åŒ–è‡³äºŒé˜¶å’Œä¸‰é˜¶ToMçš„èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒToMNNåœ¨å¤„ç†é«˜é˜¶ä»»åŠ¡æ—¶çš„å‡†ç¡®ç‡æ˜¾è‘—é«˜äºéšæœºæ°´å¹³ï¼Œä¸”è¡¨ç°å‡ºä»ä¸€é˜¶åˆ°äºŒé˜¶æ³›åŒ–æ—¶çš„æ€§èƒ½ä¸‹é™æ¯”åç»­æ›´é«˜é˜¶æ³›åŒ–æ›´ä¸ºå‰§çƒˆçš„ç‰¹å¾ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹çš„å‡†ç¡®ç‡éšä»»åŠ¡å¤æ‚åº¦å¢åŠ è€Œé™ä½ï¼Œè¿™ä¸€è¡¨ç°æ¨¡å¼ä¸äººç±»çš„è®¤çŸ¥é¢„æœŸé«˜åº¦ä¸€è‡´ã€‚ç ”ç©¶è¿›ä¸€æ­¥åœ¨ä¸åŒå‚æ•°è§„æ¨¡ä¸‹éªŒè¯äº†ç»“æœçš„æ™®é€‚æ€§ï¼Œæ­ç¤ºäº†æœºå™¨ToMçš„æ³›åŒ–è§„å¾‹ï¼Œä¸ºæ„å»ºæ›´å…·ç±»äººè®¤çŸ¥èƒ½åŠ›çš„æ™ºèƒ½ç³»ç»Ÿæä¾›äº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25343v1",
      "published_date": "2025-09-29 18:01:06 UTC",
      "updated_date": "2025-09-29 18:01:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:45:48.888805+00:00"
    },
    {
      "arxiv_id": "2509.25339v2",
      "title": "VisualOverload: Probing Visual Understanding of VLMs in Really Dense Scenes",
      "title_zh": "VisualOverloadï¼šæ¢ç©¶æåº¦å¯†é›†åœºæ™¯ä¸‹è§†è§‰è¯­è¨€æ¨¡å‹çš„è§†è§‰ç†è§£èƒ½åŠ›",
      "authors": [
        "Paul Gavrikov",
        "Wei Lin",
        "M. Jehanzeb Mirza",
        "Soumya Jahagirdar",
        "Muhammad Huzaifa",
        "Sivan Doveh",
        "Serena Yeung-Levy",
        "James Glass",
        "Hilde Kuehne"
      ],
      "abstract": "Is basic visual understanding really solved in state-of-the-art VLMs? We present VisualOverload, a slightly different visual question answering (VQA) benchmark comprising 2,720 question-answer pairs, with privately held ground-truth responses. Unlike prior VQA datasets that typically focus on near global image understanding, VisualOverload challenges models to perform simple, knowledge-free vision tasks in densely populated (or, overloaded) scenes. Our dataset consists of high-resolution scans of public-domain paintings that are populated with multiple figures, actions, and unfolding subplots set against elaborately detailed backdrops. We manually annotated these images with questions across six task categories to probe for a thorough understanding of the scene. We hypothesize that current benchmarks overestimate the performance of VLMs, and encoding and reasoning over details is still a challenging task for them, especially if they are confronted with densely populated scenes. Indeed, we observe that even the best model (o3) out of 37 tested models only achieves 19.6% accuracy on our hardest test split and overall 69.5% accuracy on all questions. Beyond a thorough evaluation, we complement our benchmark with an error analysis that reveals multiple failure modes, including a lack of counting skills, failure in OCR, and striking logical inconsistencies under complex tasks. Altogether, VisualOverload exposes a critical gap in current vision models and offers a crucial resource for the community to develop better models.\n  Benchmark: http://paulgavrikov.github.io/visualoverload",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† VisualOverloadï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å« 2,720 ä¸ªé—®ç­”å¯¹çš„æ–°å‹è§†è§‰é—®ç­”(VQA)åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨æé«˜å¯†åº¦åœºæ™¯ä¸‹çš„åŸºç¡€è§†è§‰ç†è§£èƒ½åŠ›ã€‚ä¸ä¾§é‡å…¨å±€å›¾åƒç†è§£çš„ä¼ ç»Ÿæ•°æ®é›†ä¸åŒï¼Œè¯¥åŸºå‡†åˆ©ç”¨åŒ…å«å¤§é‡äººç‰©ã€åŠ¨ä½œå’Œå¤æ‚èƒŒæ™¯çš„é«˜åˆ†è¾¨ç‡ç»˜ç”»ä½œå“ï¼Œé€šè¿‡å…­ä¸ªä»»åŠ¡ç±»åˆ«æŒ‘æˆ˜æ¨¡å‹åœ¨æ— å…ˆéªŒçŸ¥è¯†æƒ…å†µä¸‹çš„ç»†èŠ‚ç¼–ç ä¸æ¨ç†ã€‚å®éªŒå¯¹ 37 ä¸ªæ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºå³ä½¿æ˜¯è¡¨ç°æœ€å¥½çš„ o3 æ¨¡å‹åœ¨æœ€éš¾æµ‹è¯•é›†ä¸Šä¹Ÿä»…è·å¾— 19.6% çš„å‡†ç¡®ç‡ï¼Œæ•´ä½“å‡†ç¡®ç‡ä¸º 69.5%ï¼Œè¯æ˜äº†å½“å‰åŸºå‡†æµ‹è¯•æ˜¾è‘—é«˜ä¼°äº† VLMs çš„å®é™…æ€§èƒ½ã€‚é€šè¿‡æ·±å…¥çš„é”™è¯¯åˆ†æï¼Œç ”ç©¶æ­ç¤ºäº†æ¨¡å‹åœ¨è®¡æ•°èƒ½åŠ›ã€OCR è¯†åˆ«ä»¥åŠå¤æ‚ä»»åŠ¡ä¸‹çš„é€»è¾‘ä¸€è‡´æ€§ç­‰æ–¹é¢å­˜åœ¨çš„å¤šä¸ªå¤±æ•ˆæ¨¡å¼ã€‚VisualOverload æ­ç¤ºäº†å½“å‰è§†è§‰æ¨¡å‹åœ¨å¤„ç†ä¿¡æ¯è¿‡è½½åœºæ™¯æ—¶çš„å…³é”®èƒ½åŠ›å·®è·ï¼Œä¸ºç¤¾åŒºå¼€å‘æ›´å¼ºå¤§çš„è§†è§‰æ¨¡å‹æä¾›äº†é‡è¦çš„åŸºå‡†èµ„æºã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25339v2",
      "published_date": "2025-09-29 18:00:25 UTC",
      "updated_date": "2025-10-01 12:26:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:45:59.194732+00:00"
    },
    {
      "arxiv_id": "2509.25334v3",
      "title": "Uncertainty-Aware Generative Oversampling Using an Entropy-Guided Conditional Variational Autoencoder",
      "title_zh": "åŸºäºç†µå¼•å¯¼æ¡ä»¶å˜åˆ†è‡ªç¼–ç å™¨çš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥ç”Ÿæˆå¼è¿‡é‡‡æ ·",
      "authors": [
        "Amirhossein Zare",
        "Amirhessam Zare",
        "Parmida Sadat Pezeshki",
        "Herlock",
        "Rahimi",
        "Ali Ebrahimi",
        "Ignacio VÃ¡zquez-GarcÃ­a",
        "Leo Anthony Celi"
      ],
      "abstract": "Class imbalance remains a major challenge in machine learning, especially for high-dimensional biomedical data where nonlinear manifold structures dominate. Traditional oversampling methods such as SMOTE rely on local linear interpolation, often producing implausible synthetic samples. Deep generative models like Conditional Variational Autoencoders (CVAEs) better capture nonlinear distributions, but standard variants treat all minority samples equally, neglecting the importance of uncertain, boundary-region examples emphasized by heuristic methods like Borderline-SMOTE and ADASYN.\n  We propose Local Entropy-Guided Oversampling with a CVAE (LEO-CVAE), a generative oversampling framework that explicitly incorporates local uncertainty into both representation learning and data generation. To quantify uncertainty, we compute Shannon entropy over the class distribution in a sample's neighborhood: high entropy indicates greater class overlap, serving as a proxy for uncertainty. LEO-CVAE leverages this signal through two mechanisms: (i) a Local Entropy-Weighted Loss (LEWL) that emphasizes robust learning in uncertain regions, and (ii) an entropy-guided sampling strategy that concentrates generation in these informative, class-overlapping areas.\n  Applied to clinical genomics datasets (ADNI and TCGA lung cancer), LEO-CVAE consistently improves classifier performance, outperforming both traditional oversampling and generative baselines. These results highlight the value of uncertainty-aware generative oversampling for imbalanced learning in domains governed by complex nonlinear structures, such as omics data.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é«˜ç»´ç”Ÿç‰©åŒ»å­¦æ•°æ®ä¸­çš„ç±»ä¸å¹³è¡¡(Class imbalance)æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿè¿‡é‡‡æ ·æ–¹æ³•å¦‚SMOTEåœ¨å¤„ç†éçº¿æ€§æµå½¢ç»“æ„æ—¶çš„å±€é™æ€§ï¼Œä»¥åŠæ ‡å‡†æ¡ä»¶å˜åˆ†è‡ªç¼–ç å™¨(CVAE)å¿½è§†æ ·æœ¬ä¸ç¡®å®šæ€§çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†LEO-CVAEï¼ˆLocal Entropy-Guided Oversampling with a CVAEï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å°†å±€éƒ¨ä¸ç¡®å®šæ€§æ˜ç¡®èå…¥è¡¨å¾å­¦ä¹ å’Œæ•°æ®ç”Ÿæˆçš„ç”Ÿæˆå¼è¿‡é‡‡æ ·æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨é¦™å†œç†µ(Shannon entropy)é‡åŒ–æ ·æœ¬é‚»åŸŸçš„ä¸ç¡®å®šæ€§ï¼Œå¹¶é€šè¿‡å±€éƒ¨ç†µåŠ æƒæŸå¤±(LEWL)å¼ºåŒ–å¯¹ä¸ç¡®å®šåŒºåŸŸçš„é²æ£’å­¦ä¹ ã€‚åŒæ—¶ï¼ŒLEO-CVAEé‡‡ç”¨ç†µå¼•å¯¼é‡‡æ ·ç­–ç•¥ï¼Œå°†æ•°æ®ç”Ÿæˆçš„é‡ç‚¹é›†ä¸­åœ¨å…·æœ‰ä¸°å¯Œä¿¡æ¯é‡çš„ç±»é‡å åŒºåŸŸã€‚åœ¨ADNIå’ŒTCGAè‚ºç™Œç­‰ä¸´åºŠåŸºå› ç»„æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒLEO-CVAEåœ¨æå‡åˆ†ç±»å™¨æ€§èƒ½æ–¹é¢ä¸€è‡´ä¼˜äºä¼ ç»Ÿæ–¹æ³•å’Œç”Ÿæˆå¼åŸºå‡†ã€‚è¯¥æˆæœå‡¸æ˜¾äº†åœ¨å¤„ç†ç»„å­¦æ•°æ®ç­‰å¤æ‚éçº¿æ€§ç»“æ„æ—¶ï¼Œç»“åˆä¸ç¡®å®šæ€§æ„ŸçŸ¥(Uncertainty-aware)çš„ç”Ÿæˆå¼è¿‡é‡‡æ ·æŠ€æœ¯çš„åº”ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "16 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.25334v3",
      "published_date": "2025-09-29 18:00:11 UTC",
      "updated_date": "2025-10-04 17:53:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:46:04.993587+00:00"
    },
    {
      "arxiv_id": "2509.25189v1",
      "title": "InfoAgent: Advancing Autonomous Information-Seeking Agents",
      "title_zh": "InfoAgentï¼šæ¨åŠ¨è‡ªä¸»ä¿¡æ¯å¯»æ±‚æ™ºèƒ½ä½“çš„å‘å±•",
      "authors": [
        "Gongrui Zhang",
        "Jialiang Zhu",
        "Ruiqi Yang",
        "Kai Qiu",
        "Miaosen Zhang",
        "Zhirong Wu",
        "Qi Dai",
        "Bei Liu",
        "Chong Luo",
        "Zhengyuan Yang",
        "Linjie Li",
        "Lijuan Wang",
        "Weizhu Chen",
        "Yuan Zhang",
        "Xin Li",
        "Zhaoyi Liu",
        "Xin Geng",
        "Baining Guo"
      ],
      "abstract": "Building Large Language Model agents that expand their capabilities by interacting with external tools represents a new frontier in AI research and applications. In this paper, we introduce InfoAgent, a deep research agent powered by an innovative data synthesis pipeline and orchestrated web search tools. To construct challenging, hard-to-find queries,we build entity trees and apply sub-tree sampling with entity fuzzification to systematically increase question difficulty. Unlike prior work that relies heavily on commercial search tools, we develop a dedicated self-hosted search infrastructure, enhancing transparency of agent environments and facilitating further advancement of agent capacity. We evaluate the effectiveness of our data pipeline by measuring the average number of tool calls required to correctly answer a question, and also show that our agent yields better performance when equipped with our tools. Our \\mbox{InfoAgent} is post-trained from Qwen3-14B using a two-stage recipe: cold-start supervised finetuning to instill long-horizon search behaviors, followed by reinforcement learning which significantly improves reasoning-driven tool use. With our methods, InfoAgent achieves 15.3\\% accuracy on BrowseComp, 29.2\\% on BrowseComp-ZH, and 40.4\\% on Xbench-DS, outperforming prior open-source deep research agents such as WebSailor-72B and DeepDive-32B.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†InfoAgentï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡åˆ›æ–°æ•°æ®åˆæˆæµæ°´çº¿å’Œç¼–æ’å¼Web Searchå·¥å…·é©±åŠ¨çš„æ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“ï¼Œæ—¨åœ¨æå‡è‡ªä¸»ä¿¡æ¯æœç´¢èƒ½åŠ›ã€‚ä¸ºäº†æ„å»ºé«˜éš¾åº¦çš„å¤æ‚æŸ¥è¯¢ï¼Œç ”ç©¶å›¢é˜Ÿåˆ©ç”¨Entity Treeså¹¶ç»“åˆå­æ ‘é‡‡æ ·ä¸å®ä½“æ¨¡ç³ŠåŒ–æŠ€æœ¯ï¼Œç³»ç»Ÿæ€§åœ°å¢åŠ äº†ä»»åŠ¡éš¾åº¦ã€‚InfoAgenté‡‡ç”¨äº†è‡ªæ‰˜ç®¡çš„æœç´¢åŸºç¡€è®¾æ–½è€Œéä¼ ç»Ÿå•†ä¸šå·¥å…·ï¼Œæœ‰æ•ˆå¢å¼ºäº†ç¯å¢ƒé€æ˜åº¦ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šé€šè¿‡ç›‘ç£å¾®è°ƒ(SFT)å’Œå¼ºåŒ–å­¦ä¹ (RL)çš„ä¸¤é˜¶æ®µé…æ–¹å¯¹Qwen3-14Bè¿›è¡Œäº†åè®­ç»ƒã€‚è¿™ç§è®­ç»ƒç­–ç•¥æ˜¾è‘—å¼ºåŒ–äº†æ™ºèƒ½ä½“çš„é•¿ç¨‹æœç´¢è¡Œä¸ºå’Œæ¨ç†é©±åŠ¨çš„å·¥å…·è°ƒç”¨èƒ½åŠ›ï¼Œä½¿å…¶åœ¨å¤„ç†å¤æ‚ç ”ç©¶ä»»åŠ¡æ—¶è¡¨ç°æ›´ä½³ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒInfoAgentåœ¨BrowseCompã€BrowseComp-ZHå’ŒXbench-DSç­‰åŸºå‡†æµ‹è¯•ä¸­çš„å‡†ç¡®ç‡åˆ†åˆ«è¾¾åˆ°15.3%ã€29.2%å’Œ40.4%ã€‚å…¶æ€§èƒ½æ˜¾è‘—ä¼˜äºWebSailor-72Bå’ŒDeepDive-32Bç­‰å¼€æºæ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“ï¼Œä¸ºè‡ªä¸»æ™ºèƒ½ä½“çš„å‘å±•æä¾›äº†æ–°çš„æŠ€æœ¯èŒƒå¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25189v1",
      "published_date": "2025-09-29 17:59:57 UTC",
      "updated_date": "2025-09-29 17:59:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:46:14.395228+00:00"
    },
    {
      "arxiv_id": "2509.25186v1",
      "title": "Guided Diffusion for the Discovery of New Superconductors",
      "title_zh": "ç”¨äºå‘ç°æ–°å‹è¶…å¯¼ä½“çš„å¼•å¯¼å¼æ‰©æ•£",
      "authors": [
        "Pawan Prakash",
        "Jason B. Gibson",
        "Zhongwei Li",
        "Gabriele Di Gianluca",
        "Juan Esquivel",
        "Eric Fuemmeler",
        "Benjamin Geisler",
        "Jung Soo Kim",
        "Adrian Roitberg",
        "Ellad B. Tadmor",
        "Mingjie Liu",
        "Stefano Martiniani",
        "Gregory R. Stewart",
        "James J. Hamlin",
        "Peter J. Hirschfeld",
        "Richard G. Hennig"
      ],
      "abstract": "The inverse design of materials with specific desired properties, such as high-temperature superconductivity, represents a formidable challenge in materials science due to the vastness of chemical and structural space. We present a guided diffusion framework to accelerate the discovery of novel superconductors. A DiffCSP foundation model is pretrained on the Alexandria Database and fine-tuned on 7,183 superconductors with first principles derived labels. Employing classifier-free guidance, we sample 200,000 structures, which lead to 34,027 unique candidates. A multistage screening process that combines machine learning and density functional theory (DFT) calculations to assess stability and electronic properties, identifies 773 candidates with DFT-calculated $T_\\mathrm{c}>5$ K. Notably, our generative model demonstrates effective property-driven design. Our computational findings were validated against experimental synthesis and characterization performed as part of this work, which highlighted challenges in sparsely charted chemistries. This end-to-end workflow accelerates superconductor discovery while underscoring the challenge of predicting and synthesizing experimentally realizable materials.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ª guided diffusion æ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨é€†å‘è®¾è®¡åŠ é€Ÿæ–°å‹è¶…å¯¼ä½“çš„å‘ç°ï¼Œä»¥åº”å¯¹åºå¤§çš„åŒ–å­¦å’Œç»“æ„ç©ºé—´æŒ‘æˆ˜ã€‚ç ”ç©¶è€…åœ¨ Alexandria Database ä¸Šé¢„è®­ç»ƒäº† DiffCSP åŸºç¡€æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨ 7,183 ä¸ªå…·æœ‰ç¬¬ä¸€æ€§åŸç†æ ‡æ³¨çš„è¶…å¯¼ä½“æ•°æ®è¿›è¡Œå¾®è°ƒã€‚é€šè¿‡åº”ç”¨ classifier-free guidance æŠ€æœ¯ï¼Œè¯¥æ¨¡å‹é‡‡æ ·ç”Ÿæˆäº† 200,000 ä¸ªç»“æ„ï¼Œå¹¶ç»“åˆæœºå™¨å­¦ä¹ ä¸å¯†åº¦æ³›å‡½ç†è®º (DFT) è®¡ç®—è¿›è¡Œäº†å¤šé˜¶æ®µç­›é€‰ã€‚ç­›é€‰è¿‡ç¨‹æœ€ç»ˆè¯†åˆ«å‡º 773 ä¸ª DFT è®¡ç®— $T_c > 5$ K çš„å€™é€‰ææ–™ï¼Œè¯æ˜äº†ç”Ÿæˆå¼æ¨¡å‹åœ¨å±æ€§é©±åŠ¨ææ–™è®¾è®¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚è¯¥ç ”ç©¶è¿›ä¸€æ­¥é€šè¿‡å®éªŒåˆæˆä¸è¡¨å¾éªŒè¯äº†è®¡ç®—ç»“æœï¼Œåœ¨å±•ç¤ºç«¯åˆ°ç«¯ workflow åŠ é€Ÿå‘ç°æ½œåŠ›çš„åŒæ—¶ï¼Œä¹Ÿæ­ç¤ºäº†é¢„æµ‹å’Œåˆæˆå®éªŒå¯å®ç°ææ–™æ‰€é¢ä¸´çš„ä¸¥å³»æŒ‘æˆ˜ã€‚",
      "categories": [
        "cond-mat.supr-con",
        "cond-mat.mtrl-sci",
        "cs.AI"
      ],
      "primary_category": "cond-mat.supr-con",
      "comment": "13 pages, 5 figures, 1 table",
      "pdf_url": "https://arxiv.org/pdf/2509.25186v1",
      "published_date": "2025-09-29 17:59:52 UTC",
      "updated_date": "2025-09-29 17:59:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:46:22.303102+00:00"
    },
    {
      "arxiv_id": "2510.02375v2",
      "title": "Pretraining with hierarchical memories: separating long-tail and common knowledge",
      "title_zh": "åŸºäºåˆ†å±‚è®°å¿†çš„é¢„è®­ç»ƒï¼šåˆ†ç¦»é•¿å°¾çŸ¥è¯†ä¸é€šç”¨çŸ¥è¯†",
      "authors": [
        "Hadi Pouransari",
        "David Grangier",
        "C Thomas",
        "Michael Kirchhof",
        "Oncel Tuzel"
      ],
      "abstract": "The impressive performance gains of modern language models currently rely on scaling parameters: larger models store more world knowledge and reason better. Yet compressing all world knowledge into parameters is unnecessary, as only a fraction is used per prompt, and impractical for edge devices with limited inference-time memory and compute. We address this shortcoming by a memory-augmented architecture and a pretraining strategy aligned with existing hardware paradigms. We introduce small language models that access large hierarchical parametric memory banks encoding world knowledge. During pretraining and inference, we fetch a small, context-dependent memory block and add it to the model. Our pretraining learns to store long-tail world knowledge in the memory parameters, while the small language model acts as an anchor capturing common knowledge and general reasoning abilities. Through trillion-token-scale experiments, we show significant gains: a 160M-parameters model augmented with an 18M-parameters memory fetched from a 4.6B memory bank obtains comparable performance to a regular model with more than 2x the parameters. Through extensive experiments, we study the optimal type and size of parametric memories in transformers, scaling them to over 21B parameters. We find that our proposed hierarchical feed-forward memories work robustly across transformer architectures, whether added during pretraining or post-hoc.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°ä»£è¯­è¨€æ¨¡å‹ä¸ºäº†å­˜å‚¨çŸ¥è¯†è€Œè¿‡åº¦ä¾èµ–å‚æ•°æ‰©å±•çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆå±‚æ¬¡åŒ–è®°å¿†(hierarchical memories)çš„æ¶æ„å’Œé¢„è®­ç»ƒç­–ç•¥ã€‚è¯¥æ–¹æ³•å°†å°å‹è¯­è¨€æ¨¡å‹ä½œä¸ºæ•æ‰å¸¸è¯†(common knowledge)å’Œé€šç”¨æ¨ç†èƒ½åŠ›çš„é”šç‚¹ï¼Œè€Œå°†é•¿å°¾çŸ¥è¯†(long-tail knowledge)å­˜å‚¨åœ¨å¯åŠ¨æ€è·å–çš„å±‚æ¬¡åŒ–å‚æ•°åŒ–è®°å¿†åº“ä¸­ã€‚é€šè¿‡ä¸‡äº¿çº§(trillion-token-scale)å®éªŒï¼Œç ”ç©¶å±•ç¤ºäº†ä¸€ä¸ª160Må‚æ•°çš„æ¨¡å‹åœ¨é…å¤‡4.6Bè®°å¿†åº“ä¸­çš„18Mè®°å¿†å—åï¼Œå…¶æ€§èƒ½è¶³ä»¥æ¯”è‚©å‚æ•°é‡è¶…è¿‡å…¶ä¸¤å€çš„å¸¸è§„æ¨¡å‹ã€‚ç ”ç©¶å›¢é˜Ÿè¿˜æ·±å…¥æ¢è®¨äº†Transformerä¸­å‚æ•°åŒ–è®°å¿†çš„æœ€ä½³é…ç½®ï¼Œå¹¶å°†å…¶è§„æ¨¡æ‰©å±•è‡³21Bå‚æ•°ã€‚å®éªŒè¯æ˜ï¼Œæ‰€æå‡ºçš„å±‚æ¬¡åŒ–å‰é¦ˆè®°å¿†(hierarchical feed-forward memories)åœ¨å„ç§æ¶æ„åŠä¸åŒè®­ç»ƒé˜¶æ®µå‡è¡¨ç°å‡ºæå¼ºçš„é²æ£’æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.02375v2",
      "published_date": "2025-09-29 17:59:50 UTC",
      "updated_date": "2025-10-06 03:54:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:46:25.295020+00:00"
    },
    {
      "arxiv_id": "2509.25184v1",
      "title": "Incentive-Aligned Multi-Source LLM Summaries",
      "title_zh": "æ¿€åŠ±ç›¸å®¹çš„å¤šæºå¤§è¯­è¨€æ¨¡å‹æ‘˜è¦",
      "authors": [
        "Yanchen Jiang",
        "Zhe Feng",
        "Aranyak Mehta"
      ],
      "abstract": "Large language models (LLMs) are increasingly used in modern search and answer systems to synthesize multiple, sometimes conflicting, texts into a single response, yet current pipelines offer weak incentives for sources to be accurate and are vulnerable to adversarial content. We introduce Truthful Text Summarization (TTS), an incentive-aligned framework that improves factual robustness without ground-truth labels. TTS (i) decomposes a draft synthesis into atomic claims, (ii) elicits each source's stance on every claim, (iii) scores sources with an adapted multi-task peer-prediction mechanism that rewards informative agreement, and (iv) filters unreliable sources before re-summarizing. We establish formal guarantees that align a source's incentives with informative honesty, making truthful reporting the utility-maximizing strategy. Experiments show that TTS improves factual accuracy and robustness while preserving fluency, aligning exposure with informative corroboration and disincentivizing manipulation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Truthful Text Summarization (TTS)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¿€åŠ±å¯¹é½(incentive-aligned)çš„æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºå¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†å¤šæºå†²çªä¿¡æ¯æ—¶çš„äº‹å®é²æ£’æ€§ã€‚TTSé€šè¿‡å°†è‰æ‹Ÿæ‘˜è¦åˆ†è§£ä¸ºåŸå­ä¸»å¼ (atomic claims)ï¼Œå¹¶åˆ©ç”¨æ”¹è¿›çš„å¤šä»»åŠ¡å¯¹ç­‰é¢„æµ‹æœºåˆ¶(multi-task peer-prediction mechanism)è¯„ä¼°å„æ¥æºçš„ç«‹åœºï¼Œä»è€Œå¥–åŠ±å…·æœ‰ä¿¡æ¯æ€§çš„å…±è¯†å¹¶è¿‡æ»¤ä¸å¯é æ¥æºã€‚è¯¥æ¡†æ¶åœ¨æ²¡æœ‰çœŸå€¼æ ‡ç­¾(ground-truth labels)çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡å½¢å¼åŒ–ä¿è¯ä½¿çœŸå®æŠ¥å‘Šæˆä¸ºæ¥æºæ–¹çš„æ•ˆç”¨æœ€å¤§åŒ–ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTTSåœ¨æ˜¾è‘—æå‡æ‘˜è¦äº‹å®å‡†ç¡®æ€§å’ŒæŠ—æ“çºµèƒ½åŠ›çš„åŒæ—¶ï¼Œæœ‰æ•ˆä¿æŒäº†æ–‡æœ¬çš„æµç•…æ€§ã€‚è¿™ç§æ–¹æ³•æˆåŠŸå®ç°äº†å°†ä¿¡æ¯æ›å…‰åº¦ä¸ä¿¡æ¯éªŒè¯ç›¸å¯¹é½ï¼Œä¸ºæ„å»ºæ›´å¯ä¿¡çš„æœç´¢ä¸é—®ç­”ç³»ç»Ÿæä¾›äº†é‡è¦æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.GT"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25184v1",
      "published_date": "2025-09-29 17:59:42 UTC",
      "updated_date": "2025-09-29 17:59:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:46:29.798697+00:00"
    },
    {
      "arxiv_id": "2509.25182v1",
      "title": "DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder",
      "title_zh": "DC-VideoGenï¼šåŸºäºæ·±åº¦å‹ç¼©è§†é¢‘è‡ªç¼–ç å™¨çš„é«˜æ•ˆè§†é¢‘ç”Ÿæˆ",
      "authors": [
        "Junyu Chen",
        "Wenkun He",
        "Yuchao Gu",
        "Yuyang Zhao",
        "Jincheng Yu",
        "Junsong Chen",
        "Dongyun Zou",
        "Yujun Lin",
        "Zhekai Zhang",
        "Muyang Li",
        "Haocheng Xi",
        "Ligeng Zhu",
        "Enze Xie",
        "Song Han",
        "Han Cai"
      ],
      "abstract": "We introduce DC-VideoGen, a post-training acceleration framework for efficient video generation. DC-VideoGen can be applied to any pre-trained video diffusion model, improving efficiency by adapting it to a deep compression latent space with lightweight fine-tuning. The framework builds on two key innovations: (i) a Deep Compression Video Autoencoder with a novel chunk-causal temporal design that achieves 32x/64x spatial and 4x temporal compression while preserving reconstruction quality and generalization to longer videos; and (ii) AE-Adapt-V, a robust adaptation strategy that enables rapid and stable transfer of pre-trained models into the new latent space. Adapting the pre-trained Wan-2.1-14B model with DC-VideoGen requires only 10 GPU days on the NVIDIA H100 GPU. The accelerated models achieve up to 14.8x lower inference latency than their base counterparts without compromising quality, and further enable 2160x3840 video generation on a single GPU. Code: https://github.com/dc-ai-projects/DC-VideoGen.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DC-VideoGenï¼Œä¸€ç§ç”¨äºé«˜æ•ˆè§†é¢‘ç”Ÿæˆçš„è®­ç»ƒååŠ é€Ÿæ¡†æ¶ (post-training acceleration framework)ï¼Œé€šè¿‡è½»é‡åŒ–å¾®è°ƒå°†é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹ (video diffusion model) é€‚é…åˆ°æ·±åº¦å‹ç¼©æ½œç©ºé—´ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ä¸€ç§å…·å¤‡æ–°å‹å—å› æœ (chunk-causal) æ—¶é—´è®¾è®¡çš„æ·±åº¦å‹ç¼©è§†é¢‘è‡ªç¼–ç å™¨ (Deep Compression Video Autoencoder)ï¼Œå®ç°äº† 32x/64x ç©ºé—´å‹ç¼©å’Œ 4x æ—¶é—´å‹ç¼©ï¼ŒåŒæ—¶ä¿æŒäº†æé«˜çš„é‡å»ºè´¨é‡å’Œå¯¹é•¿è§†é¢‘çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº† AE-Adapt-V é€‚é…ç­–ç•¥ï¼Œç¡®ä¿é¢„è®­ç»ƒæ¨¡å‹èƒ½å¿«é€Ÿä¸”ç¨³å®šåœ°è¿ç§»è‡³æ–°æ½œç©ºé—´ã€‚å®éªŒè¯æ˜ï¼Œé€‚é… Wan-2.1-14B æ¨¡å‹ä»…éœ€ 10 ä¸ª NVIDIA H100 GPU å¤©ï¼Œä¸”åœ¨ä¸æŸå¤±è´¨é‡çš„å‰æä¸‹å°†æ¨ç†å»¶è¿Ÿé™ä½äº†é«˜è¾¾ 14.8 å€ã€‚è¯¥æ¡†æ¶è¿›ä¸€æ­¥çªç ´äº†ç¡¬ä»¶é™åˆ¶ï¼Œæ”¯æŒåœ¨å•ä¸ª GPU ä¸Šç”Ÿæˆåˆ†è¾¨ç‡è¾¾ 2160x3840 çš„è¶…é«˜æ¸…è§†é¢‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Tech Report. The first three authors contributed equally to this work",
      "pdf_url": "https://arxiv.org/pdf/2509.25182v1",
      "published_date": "2025-09-29 17:59:31 UTC",
      "updated_date": "2025-09-29 17:59:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:46:30.591749+00:00"
    },
    {
      "arxiv_id": "2509.25180v2",
      "title": "DC-Gen: Post-Training Diffusion Acceleration with Deeply Compressed Latent Space",
      "title_zh": "DC-Genï¼šåŸºäºæ·±åº¦å‹ç¼©æ½œç©ºé—´çš„è®­ç»ƒåæ‰©æ•£åŠ é€Ÿ",
      "authors": [
        "Wenkun He",
        "Yuchao Gu",
        "Junyu Chen",
        "Dongyun Zou",
        "Yujun Lin",
        "Zhekai Zhang",
        "Haocheng Xi",
        "Muyang Li",
        "Ligeng Zhu",
        "Jincheng Yu",
        "Junsong Chen",
        "Enze Xie",
        "Song Han",
        "Han Cai"
      ],
      "abstract": "Existing text-to-image diffusion models excel at generating high-quality images, but face significant efficiency challenges when scaled to high resolutions, like 4K image generation. While previous research accelerates diffusion models in various aspects, it seldom handles the inherent redundancy within the latent space. To bridge this gap, this paper introduces DC-Gen, a general framework that accelerates text-to-image diffusion models by leveraging a deeply compressed latent space. Rather than a costly training-from-scratch approach, DC-Gen uses an efficient post-training pipeline to preserve the quality of the base model. A key challenge in this paradigm is the representation gap between the base model's latent space and a deeply compressed latent space, which can lead to instability during direct fine-tuning. To overcome this, DC-Gen first bridges the representation gap with a lightweight embedding alignment training. Once the latent embeddings are aligned, only a small amount of LoRA fine-tuning is needed to unlock the base model's inherent generation quality. We verify DC-Gen's effectiveness on SANA and FLUX.1-Krea. The resulting DC-Gen-SANA and DC-Gen-FLUX models achieve quality comparable to their base models but with a significant speedup. Specifically, DC-Gen-FLUX reduces the latency of 4K image generation by 53x on the NVIDIA H100 GPU. When combined with NVFP4 SVDQuant, DC-Gen-FLUX generates a 4K image in just 3.5 seconds on a single NVIDIA 5090 GPU, achieving a total latency reduction of 138x compared to the base FLUX.1-Krea model. Code: https://github.com/dc-ai-projects/DC-Gen.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DC-Genï¼Œä¸€ç§é€šè¿‡æ·±åº¦å‹ç¼©æ½œç©ºé—´(deeply compressed latent space)åŠ é€Ÿæ–‡æœ¬ç”Ÿæˆå›¾åƒæ‰©æ•£æ¨¡å‹(text-to-image diffusion models)çš„é€šç”¨æ¡†æ¶ã€‚é’ˆå¯¹é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆä¸­çš„æ•ˆç‡æŒ‘æˆ˜ï¼ŒDC-Gené‡‡ç”¨é«˜æ•ˆçš„è®­ç»ƒå(post-training)æµç¨‹ï¼Œæœ‰æ•ˆé¿å…äº†é«˜æ˜‚çš„ä»å¤´è®­ç»ƒæˆæœ¬ã€‚è¯¥æ¡†æ¶é€šè¿‡è½»é‡çº§çš„åµŒå…¥å¯¹é½è®­ç»ƒ(embedding alignment training)å¼¥åˆåŸºç¡€æ¨¡å‹ä¸æ·±åº¦å‹ç¼©æ½œç©ºé—´ä¹‹é—´çš„è¡¨å¾å·®è·(representation gap)ï¼Œéšååˆ©ç”¨å°‘é‡çš„LoRAå¾®è°ƒ(LoRA fine-tuning)é‡Šæ”¾æ¨¡å‹çš„ç”Ÿæˆæ½œåŠ›ã€‚åœ¨SANAå’ŒFLUX.1-Kreaä¸Šçš„å®éªŒè¯æ˜ï¼ŒDC-Genåœ¨ä¿æŒå›¾åƒè´¨é‡çš„åŒæ—¶å®ç°äº†æ˜¾è‘—åŠ é€Ÿï¼Œå…¶ä¸­DC-Gen-FLUXåœ¨4Kç”Ÿæˆä»»åŠ¡ä¸­å°†å»¶è¿Ÿé™ä½äº†53å€ã€‚ç»“åˆNVFP4 SVDQuantæŠ€æœ¯åï¼Œè¯¥æ¨¡å‹åœ¨NVIDIA 5090 GPUä¸Šä»…éœ€3.5ç§’å³å¯ç”Ÿæˆ4Kå›¾åƒï¼Œè¾ƒåŸå§‹æ¨¡å‹å®ç°äº†138å€çš„æ€»å»¶è¿Ÿç¼©å‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Tech Report. The first three authors contributed equally to this work",
      "pdf_url": "https://arxiv.org/pdf/2509.25180v2",
      "published_date": "2025-09-29 17:59:25 UTC",
      "updated_date": "2025-10-01 02:18:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:46:53.574053+00:00"
    },
    {
      "arxiv_id": "2509.25179v2",
      "title": "NAIPv2: Debiased Pairwise Learning for Efficient Paper Quality Estimation",
      "title_zh": "NAIPv2ï¼šé¢å‘é«˜æ•ˆè®ºæ–‡è´¨é‡è¯„ä¼°çš„å»åæˆå¯¹å­¦ä¹ ",
      "authors": [
        "Penghai Zhao",
        "Jinyu Tian",
        "Qinghua Xing",
        "Xin Zhang",
        "Zheng Li",
        "Jianjun Qian",
        "Ming-Ming Cheng",
        "Xiang Li"
      ],
      "abstract": "The ability to estimate the quality of scientific papers is central to how both humans and AI systems will advance scientific knowledge in the future. However, existing LLM-based estimation methods suffer from high inference cost, whereas the faster direct score regression approach is limited by scale inconsistencies. We present NAIPv2, a debiased and efficient framework for paper quality estimation. NAIPv2 employs pairwise learning within domain-year groups to reduce inconsistencies in reviewer ratings and introduces the Review Tendency Signal (RTS) as a probabilistic integration of reviewer scores and confidences. To support training and evaluation, we further construct NAIDv2, a large-scale dataset of 24,276 ICLR submissions enriched with metadata and detailed structured content. Trained on pairwise comparisons but enabling efficient pointwise prediction at deployment, NAIPv2 achieves state-of-the-art performance (78.2% AUC, 0.432 Spearman), while maintaining scalable, linear-time efficiency at inference. Notably, on unseen NeurIPS submissions, it further demonstrates strong generalization, with predicted scores increasing consistently across decision categories from Rejected to Oral. These findings establish NAIPv2 as a debiased and scalable framework for automated paper quality estimation, marking a step toward future scientific intelligence systems. Code and dataset are released at sway.cloud.microsoft/Pr42npP80MfPhvj8.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† NAIPv2 æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¯„ä¼°æ–¹æ³•æ¨ç†æˆæœ¬é«˜æ˜‚ä»¥åŠç›´æ¥åˆ†æ•°å›å½’æ–¹æ³•é¢ä¸´çš„å°ºåº¦ä¸ä¸€è‡´æŒ‘æˆ˜ã€‚NAIPv2 åœ¨é¢†åŸŸ-å¹´ä»½ç»„å†…é‡‡ç”¨ä¸¤ä¸¤å¯¹æ¯”å­¦ä¹ ï¼ˆPairwise Learningï¼‰æ¥å‡å°‘å®¡ç¨¿äººè¯„åˆ†çš„å·®å¼‚ï¼Œå¹¶å¼•å…¥äº†è¯„è®ºå€¾å‘ä¿¡å·ï¼ˆReview Tendency Signal, RTSï¼‰ä»¥æ¦‚ç‡åŒ–æ•´åˆå®¡ç¨¿åˆ†æ•°ä¸ç½®ä¿¡åº¦ã€‚ç ”ç©¶å›¢é˜ŸåŒæ­¥å‘å¸ƒäº†åŒ…å«è¶…è¿‡ 2.4 ä¸‡ç¯‡ ICLR æŠ•ç¨¿åŠè¯¦ç»†ç»“æ„åŒ–å†…å®¹çš„ NAIDv2 å¤§è§„æ¨¡æ•°æ®é›†ã€‚å®éªŒè¯æ˜ï¼ŒNAIPv2 åœ¨ä¿æŒçº¿æ€§æ—¶é—´æ¨ç†æ•ˆç‡çš„åŒæ—¶ï¼Œå®ç°äº† 78.2% AUC å’Œ 0.432 Spearman çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨ NeurIPS æ•°æ®é›†ä¸Šå±•ç°äº†ä¼˜å¼‚çš„æ³›åŒ–èƒ½åŠ›ï¼Œå…¶é¢„æµ‹åˆ†æ•°ä¸è®ºæ–‡å½•å–ç»“æœå‘ˆç°é«˜åº¦ä¸€è‡´æ€§ï¼Œä¸ºæœªæ¥çš„ç§‘å­¦æ™ºèƒ½ç³»ç»Ÿå¥ å®šäº†å»åè§ä¸”å¯æ‰©å±•çš„æŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "NAIPv2 complements our earlier work NAIPv1 (arXiv:2408.03934). Whereas NAIPv1 addressed citation count-based impact prediction, NAIPv2 estimates research quality using peer review data",
      "pdf_url": "https://arxiv.org/pdf/2509.25179v2",
      "published_date": "2025-09-29 17:59:23 UTC",
      "updated_date": "2025-10-01 01:02:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:47:05.664451+00:00"
    },
    {
      "arxiv_id": "2509.25178v1",
      "title": "GHOST: Hallucination-Inducing Image Generation for Multimodal LLMs",
      "title_zh": "GHOSTï¼šé¢å‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å¹»è§‰è¯±å¯¼å›¾åƒç”Ÿæˆ",
      "authors": [
        "Aryan Yazdan Parast",
        "Parsa Hosseini",
        "Hesam Asadollahzadeh",
        "Arshia Soltani Moakhar",
        "Basim Azam",
        "Soheil Feizi",
        "Naveed Akhtar"
      ],
      "abstract": "Object hallucination in Multimodal Large Language Models (MLLMs) is a persistent failure mode that causes the model to perceive objects absent in the image. This weakness of MLLMs is currently studied using static benchmarks with fixed visual scenarios, which preempts the possibility of uncovering model-specific or unanticipated hallucination vulnerabilities. We introduce GHOST (Generating Hallucinations via Optimizing Stealth Tokens), a method designed to stress-test MLLMs by actively generating images that induce hallucination. GHOST is fully automatic and requires no human supervision or prior knowledge. It operates by optimizing in the image embedding space to mislead the model while keeping the target object absent, and then guiding a diffusion model conditioned on the embedding to generate natural-looking images. The resulting images remain visually natural and close to the original input, yet introduce subtle misleading cues that cause the model to hallucinate. We evaluate our method across a range of models, including reasoning models like GLM-4.1V-Thinking, and achieve a hallucination success rate exceeding 28%, compared to around 1% in prior data-driven discovery methods. We confirm that the generated images are both high-quality and object-free through quantitative metrics and human evaluation. Also, GHOST uncovers transferable vulnerabilities: images optimized for Qwen2.5-VL induce hallucinations in GPT-4o at a 66.5% rate. Finally, we show that fine-tuning on our images mitigates hallucination, positioning GHOST as both a diagnostic and corrective tool for building more reliable multimodal systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†GHOSTï¼ˆGenerating Hallucinations via Optimizing Stealth Tokensï¼‰ï¼Œä¸€ç§ç”¨äºå‹åŠ›æµ‹è¯•å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMultimodal Large Language Models, MLLMsï¼‰çš„ä¸»åŠ¨å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œæ—¨åœ¨é’ˆå¯¹æ€§åœ°è¯±å¯¼å¹¶æ­ç¤ºæ¨¡å‹åœ¨ç‰©ä½“å¹»è§‰ï¼ˆObject hallucinationï¼‰æ–¹é¢çš„æ¼æ´ã€‚GHOST é‡‡ç”¨å…¨è‡ªåŠ¨ä¸”æ— éœ€äººå·¥ç›‘ç£çš„æ–¹å¼ï¼Œé€šè¿‡åœ¨å›¾åƒåµŒå…¥ç©ºé—´ï¼ˆEmbedding spaceï¼‰è¿›è¡Œä¼˜åŒ–æ¥è¯¯å¯¼æ¨¡å‹ï¼Œå¹¶åœ¨ç¡®ä¿ç›®æ ‡ç‰©ä½“ç¼ºå¤±çš„å‰æä¸‹ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹ï¼ˆDiffusion modelï¼‰ç”Ÿæˆè§†è§‰ä¸Šè‡ªç„¶ä¸”å…·æœ‰è¯¯å¯¼æ€§çš„å›¾åƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGHOST åœ¨åŒ…æ‹¬ GLM-4.1V-Thinking åœ¨å†…çš„å¤šç§æ¨¡å‹ä¸Šå®ç°äº†è¶…è¿‡ 28% çš„å¹»è§‰æˆåŠŸç‡ï¼Œè¿œé«˜äºä¼ ç»Ÿæ–¹æ³•çš„ 1%ï¼Œå¹¶å±•ç°å‡ºæå¼ºçš„è·¨æ¨¡å‹è¿ç§»æ€§ï¼Œä¾‹å¦‚ä¸º Qwen2.5-VL ä¼˜åŒ–çš„å›¾åƒå¯ä½¿ GPT-4o äº§ç”Ÿ 66.5% çš„å¹»è§‰ã€‚æ­¤å¤–ï¼Œç»å®šé‡æŒ‡æ ‡å’Œäººå·¥è¯„ä¼°è¯å®ï¼Œç”Ÿæˆçš„å›¾åƒåœ¨ä¿æŒé«˜è´¨é‡çš„åŒæ—¶ç¡®å®ä¸å«ç›®æ ‡ç‰©ä½“ã€‚è¯¥ç ”ç©¶æœ€åè¡¨æ˜ï¼Œåˆ©ç”¨è¿™äº›ç”Ÿæˆçš„å›¾åƒè¿›è¡Œå¾®è°ƒï¼ˆFine-tuningï¼‰å¯ä»¥æœ‰æ•ˆç¼“è§£æ¨¡å‹å¹»è§‰ï¼Œä½¿ GHOST æˆä¸ºæå‡å¤šæ¨¡æ€ç³»ç»Ÿå¯é æ€§çš„é‡è¦è¯Šæ–­ä¸çº åå·¥å…·ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25178v1",
      "published_date": "2025-09-29 17:59:23 UTC",
      "updated_date": "2025-09-29 17:59:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:47:11.850117+00:00"
    },
    {
      "arxiv_id": "2509.25175v1",
      "title": "EasySteer: A Unified Framework for High-Performance and Extensible LLM Steering",
      "title_zh": "EasySteerï¼šä¸€ç§ç”¨äºé«˜æ€§èƒ½å’Œå¯æ‰©å±•å¤§è¯­è¨€æ¨¡å‹å¼•å¯¼çš„ç»Ÿä¸€æ¡†æ¶",
      "authors": [
        "Haolei Xu",
        "Xinyu Mei",
        "Yuchen Yan",
        "Rui Zhou",
        "Wenqi Zhang",
        "Weiming Lu",
        "Yueting Zhuang",
        "Yongliang Shen"
      ],
      "abstract": "Large language model (LLM) steering has emerged as a promising paradigm for controlling model behavior at inference time through targeted manipulation of hidden states, offering a lightweight alternative to expensive retraining. However, existing steering frameworks suffer from critical limitations: computational inefficiency, limited extensibility, and restricted functionality that hinder both research progress and practical deployment. We present EasySteer, a unified framework for high-performance, extensible LLM steering built on vLLM. Our system features modular architecture with pluggable interfaces for both analysis-based and learning-based methods, fine-grained parameter control, pre-computed steering vectors for eight application domains, and an interactive demonstration system. Through deep integration with vLLM's optimized inference engine, EasySteer achieves 5.5-11.4$\\times$ speedup over existing frameworks. Extensive experiments demonstrate its effectiveness in overthinking mitigation, hallucination reduction, and other key applications. EasySteer transforms steering from research technique to production-ready capability, establishing critical infrastructure for deployable, controllable language models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLM Steering)ç°æœ‰æ¡†æ¶ä¸­å­˜åœ¨çš„è®¡ç®—æ•ˆç‡ä½ã€æ‰©å±•æ€§æœ‰é™å’ŒåŠŸèƒ½å—é™ç­‰é—®é¢˜ï¼Œæå‡ºäº†EasySteerï¼Œä¸€ä¸ªåŸºäºvLLMæ„å»ºçš„é«˜æ€§èƒ½ä¸”å¯æ‰©å±•çš„ç»Ÿä¸€æ¡†æ¶ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨æ¨¡å—åŒ–æ¶æ„ï¼Œä¸ºåˆ†æå‹(Analysis-based)å’Œå­¦ä¹ å‹(Learning-based)æ–¹æ³•æä¾›äº†å¯æ’æ‹”æ¥å£ï¼Œå¹¶æ”¯æŒç»†ç²’åº¦çš„å‚æ•°æ§åˆ¶åŠé¢„ç½®çš„å…«ä¸ªé¢†åŸŸçš„Steering Vectorsã€‚é€šè¿‡ä¸vLLMæ¨ç†å¼•æ“çš„æ·±åº¦é›†æˆï¼ŒEasySteerå®ç°äº†æ¯”ç°æœ‰æ¡†æ¶å¿«5.5-11.4å€çš„è¿è¡Œé€Ÿåº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨ç¼“è§£è¿‡åº¦æ€è€ƒ(Overthinking Mitigation)å’Œå‡å°‘å¹»è§‰(Hallucination Reduction)æ–¹é¢æå…·æˆæ•ˆã€‚EasySteerå°†SteeringæŠ€æœ¯ä»çº¯ç ”ç©¶æ‰‹æ®µæå‡ä¸ºç”Ÿäº§å°±ç»ªçš„èƒ½åŠ›ï¼Œä¸ºæ„å»ºå¯éƒ¨ç½²ä¸”å¯æ§çš„è¯­è¨€æ¨¡å‹æä¾›äº†å…³é”®çš„åŸºç¡€è®¾æ–½æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "project: https://github.com/ZJU-REAL/EasySteer",
      "pdf_url": "https://arxiv.org/pdf/2509.25175v1",
      "published_date": "2025-09-29 17:59:07 UTC",
      "updated_date": "2025-09-29 17:59:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:47:11.267225+00:00"
    },
    {
      "arxiv_id": "2510.12803v1",
      "title": "AutoCode: LLMs as Problem Setters for Competitive Programming",
      "title_zh": "AutoCodeï¼šå¤§è¯­è¨€æ¨¡å‹ä½œä¸ºç®—æ³•ç«èµ›å‘½é¢˜äºº",
      "authors": [
        "Shang Zhou",
        "Zihan Zheng",
        "Kaiyuan Liu",
        "Zeyu Shen",
        "Zerui Cheng",
        "Zexing Chen",
        "Hansen He",
        "Jianzhu Yao",
        "Huanzhi Mao",
        "Qiuyang Mang",
        "Tianfu Fu",
        "Beichen Li",
        "Dongruixuan Li",
        "Wenhao Chai",
        "Zhuang Liu",
        "Aleksandra Korolova",
        "Peter Henderson",
        "Natasha Jaques",
        "Pramod Viswanath",
        "Saining Xie",
        "Jingbo Shang"
      ],
      "abstract": "Writing competitive programming problems is exacting. Authors must: set constraints, input distributions, and edge cases that rule out shortcuts; target specific algorithms (e.g., max-flow, dynamic programming, data structures); and calibrate complexity beyond the reach of most competitors. We argue that this makes for an ideal test of general large language model capabilities and study whether they can do this reliably. We introduce AutoCode, which uses multiple rounds of validation to yield competition-grade problem statements and test cases. On held-out problems, AutoCode test suites approach 99% consistency with official judgments, a significant improvement over current state-of-the-art methods like HardTests, which achieve less than 81%. Furthermore, starting with a random seed problem, AutoCode can create novel variants with reference and brute-force solutions. By cross-verifying these generated solutions against test cases, we can further filter out malformed problems. Our system ensures high correctness, as verified by human experts. AutoCode successfully produces novel problems judged by Grandmaster-level (top 0.3%) competitive programmers to be of contest quality.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AutoCode æ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) è‡ªåŠ¨ç”Ÿæˆç«èµ›ç¼–ç¨‹ (Competitive Programming) é¢˜ç›®ï¼Œä»¥è§£å†³è¯¥é¢†åŸŸå¯¹çº¦æŸè®¾å®šã€ç®—æ³•é’ˆå¯¹æ€§å’Œéš¾åº¦æ ¡å‡†çš„ä¸¥è‹›è¦æ±‚ã€‚AutoCode é€šè¿‡å¤šè½®éªŒè¯ç”Ÿæˆç«èµ›çº§çš„é¢˜ç›®æè¿°åŠæµ‹è¯•ç”¨ä¾‹ï¼Œå…¶æµ‹è¯•å¥—ä»¶ä¸å®˜æ–¹è¯„åˆ¤çš„ä¸€è‡´æ€§æ¥è¿‘ 99%ï¼Œè¿œè¶… HardTests ç­‰ç°æœ‰æ–¹æ³•ä¸åˆ° 81% çš„è¡¨ç°ã€‚è¯¥ç³»ç»Ÿæ”¯æŒä»ç§å­é¢˜ç›®æ´¾ç”Ÿæ–°å˜ä½“ï¼Œå¹¶ç»“åˆå‚è€ƒæ–¹æ¡ˆä¸æš´åŠ›ç ´è§£ (Brute-force) æ–¹æ¡ˆè¿›è¡Œäº¤å‰éªŒè¯ï¼Œä»è€Œæœ‰æ•ˆè¿‡æ»¤ç•¸å½¢é¢˜ç›®ã€‚ç»äººç±»ä¸“å®¶åŠç‰¹çº§å¤§å¸ˆ (Grandmaster-level) ç«èµ›é€‰æ‰‹è¯„ä¼°ï¼ŒAutoCode ç”Ÿæˆçš„é¢˜ç›®å±•ç°äº†æé«˜çš„æ­£ç¡®æ€§ä¸ç«èµ›æ°´å‡†ã€‚è¯¥ç ”ç©¶ä¸ä»…è¯æ˜äº† LLMs åœ¨å¤„ç†å¤æ‚é€»è¾‘å’Œç‰¹å®šæ•°æ®ç»“æ„ (Data structures) ä»»åŠ¡ä¸Šçš„æ½œåŠ›ï¼Œä¹Ÿä¸ºè‡ªåŠ¨åŒ–å‘½é¢˜æä¾›äº†å¯é çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.PL"
      ],
      "primary_category": "cs.SE",
      "comment": "Project page: https://livecodebenchpro.com/projects/autocode/overview",
      "pdf_url": "https://arxiv.org/pdf/2510.12803v1",
      "published_date": "2025-09-29 17:59:03 UTC",
      "updated_date": "2025-09-29 17:59:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:47:20.992199+00:00"
    },
    {
      "arxiv_id": "2509.25174v1",
      "title": "XQC: Well-conditioned Optimization Accelerates Deep Reinforcement Learning",
      "title_zh": "XQCï¼šè‰¯æ€ä¼˜åŒ–åŠ é€Ÿæ·±åº¦å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Daniel Palenicek",
        "Florian Vogt",
        "Joe Watson",
        "Ingmar Posner",
        "Jan Peters"
      ],
      "abstract": "Sample efficiency is a central property of effective deep reinforcement learning algorithms. Recent work has improved this through added complexity, such as larger models, exotic network architectures, and more complex algorithms, which are typically motivated purely by empirical performance. We take a more principled approach by focusing on the optimization landscape of the critic network. Using the eigenspectrum and condition number of the critic's Hessian, we systematically investigate the impact of common architectural design decisions on training dynamics. Our analysis reveals that a novel combination of batch normalization (BN), weight normalization (WN), and a distributional cross-entropy (CE) loss produces condition numbers orders of magnitude smaller than baselines. This combination also naturally bounds gradient norms, a property critical for maintaining a stable effective learning rate under non-stationary targets and bootstrapping. Based on these insights, we introduce XQC: a well-motivated, sample-efficient deep actor-critic algorithm built upon soft actor-critic that embodies these optimization-aware principles. We achieve state-of-the-art sample efficiency across 55 proprioception and 15 vision-based continuous control tasks, all while using significantly fewer parameters than competing methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† XQCï¼Œä¸€ç§é€šè¿‡æ”¹å–„ä¼˜åŒ–æ™¯è§‚ (optimization landscape) æ¥æå‡æ·±åº¦å¼ºåŒ–å­¦ä¹ é‡‡æ ·æ•ˆç‡çš„ actor-critic ç®—æ³•ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨è¯„ä»·ç½‘ç»œ (critic network) çš„ Hessian çŸ©é˜µçš„ç‰¹å¾è°± (eigenspectrum) å’Œæ¡ä»¶æ•° (condition number)ï¼Œç³»ç»Ÿåœ°ç ”ç©¶äº†æ¶æ„è®¾è®¡å¯¹è®­ç»ƒåŠ¨åŠ›å­¦çš„å½±å“ã€‚åˆ†æè¡¨æ˜ï¼Œé€šè¿‡ç»“åˆ batch normalization (BN)ã€weight normalization (WN) å’Œ distributional cross-entropy (CE) æŸå¤±ï¼Œå¯ä»¥ä½¿æ¡ä»¶æ•°é™ä½å‡ ä¸ªæ•°é‡çº§å¹¶è‡ªç„¶åœ°é™åˆ¶æ¢¯åº¦èŒƒæ•°ï¼Œä»è€Œç¡®ä¿åœ¨éå¹³ç¨³ç›®æ ‡ä¸‹å­¦ä¹ ç‡çš„ç¨³å®šæ€§ã€‚åŸºäºè¿™äº›ä¼˜åŒ–æ„ŸçŸ¥åŸåˆ™ï¼ŒXQC åœ¨ soft actor-critic (SAC) çš„åŸºç¡€ä¸Šè¿›è¡Œäº†æ”¹è¿›ï¼Œå¹¶ä½“ç°äº†æ·±åšçš„ç†è®ºåŠ¨æœºã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒXQC åœ¨ 55 ä¸ª proprioception å’Œ 15 ä¸ª vision-based è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­å‡è¾¾åˆ°äº† state-of-the-art çš„é‡‡æ ·æ•ˆç‡ã€‚æ­¤å¤–ï¼ŒXQC åœ¨å‚æ•°é‡è¿œå°‘äºç«äº‰æ–¹æ³•çš„æƒ…å†µä¸‹ï¼Œä¾ç„¶å±•ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½è¡¨ç°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25174v1",
      "published_date": "2025-09-29 17:58:53 UTC",
      "updated_date": "2025-09-29 17:58:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:47:26.792751+00:00"
    },
    {
      "arxiv_id": "2509.25170v2",
      "title": "GLASS Flows: Transition Sampling for Alignment of Flow and Diffusion Models",
      "title_zh": "GLASS Flowsï¼šç”¨äºæµä¸æ‰©æ•£æ¨¡å‹å¯¹é½çš„è½¬ç§»é‡‡æ ·",
      "authors": [
        "Peter Holderrieth",
        "Uriel Singer",
        "Tommi Jaakkola",
        "Ricky T. Q. Chen",
        "Yaron Lipman",
        "Brian Karrer"
      ],
      "abstract": "The performance of flow matching and diffusion models can be greatly improved at inference time using reward alignment algorithms, yet efficiency remains a major limitation. While several algorithms were proposed, we demonstrate that a common bottleneck is the sampling method these algorithms rely on: many algorithms require to sample Markov transitions via SDE sampling, which is significantly less efficient and often less performant than ODE sampling. To remove this bottleneck, we introduce GLASS Flows, a new sampling paradigm that simulates a \"flow matching model within a flow matching model\" to sample Markov transitions. As we show in this work, this \"inner\" flow matching model can be retrieved from a pre-trained model without any re-training, combining the efficiency of ODEs with the stochastic evolution of SDEs. On large-scale text-to-image models, we show that GLASS Flows eliminate the trade-off between stochastic evolution and efficiency. Combined with Feynman-Kac Steering, GLASS Flows improve state-of-the-art performance in text-to-image generation, making it a simple, drop-in solution for inference-time scaling of flow and diffusion models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ flow matching å’Œ diffusion models åœ¨åˆ©ç”¨å¥–åŠ±å¯¹é½(reward alignment)ç®—æ³•è¿›è¡Œæ¨ç†æ—¶é¢ä¸´çš„æ•ˆç‡ç“¶é¢ˆé—®é¢˜è¿›è¡Œäº†æ¢è®¨ã€‚ç ”ç©¶å‘ç°ï¼Œè®¸å¤šç°æœ‰ç®—æ³•ä¾èµ–äº SDE sampling æ¥é‡‡æ · Markov transitionsï¼Œè¿™åœ¨æ•ˆç‡å’Œæ€§èƒ½ä¸Šé€šå¸¸é€Šäº ODE samplingã€‚ä¸ºäº†è§£å†³è¿™ä¸€ç“¶é¢ˆï¼Œä½œè€…æå‡ºäº† GLASS Flowsï¼Œè¿™æ˜¯ä¸€ç§å…¨æ–°çš„é‡‡æ ·èŒƒå¼ï¼Œé€šè¿‡åœ¨ flow matching æ¨¡å‹å†…éƒ¨æ¨¡æ‹Ÿå¦ä¸€ä¸ª flow matching æ¨¡å‹æ¥å®ç° Markov transitions çš„é‡‡æ ·ã€‚è¯¥â€œå†…éƒ¨â€æ¨¡å‹å¯ä»¥ç›´æ¥ä»é¢„è®­ç»ƒæ¨¡å‹ä¸­æå–ä¸”æ— éœ€é‡æ–°è®­ç»ƒï¼ŒæˆåŠŸå°† ODE çš„é«˜æ•ˆæ€§ä¸ SDE çš„éšæœºæ¼”åŒ–(stochastic evolution)ç›¸ç»“åˆã€‚åœ¨å¤§å‹ text-to-image æ¨¡å‹ä¸Šçš„å®éªŒè¯æ˜ï¼ŒGLASS Flows æˆåŠŸæ¶ˆé™¤äº†éšæœºæ¼”åŒ–ä¸æ•ˆç‡ä¹‹é—´çš„æƒè¡¡ã€‚ç»“åˆ Feynman-Kac Steering æŠ€æœ¯ï¼Œè¯¥æ–¹æ³•è¿›ä¸€æ­¥æå‡äº† text-to-image ç”Ÿæˆçš„æ€§èƒ½ï¼Œä¸ºæ¨¡å‹æä¾›äº†ä¸€ç§ç®€å•ä¸”å³æ’å³ç”¨çš„æ¨ç†ä¾§æ‰©å±•æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25170v2",
      "published_date": "2025-09-29 17:58:36 UTC",
      "updated_date": "2025-12-01 19:36:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:47:30.288307+00:00"
    },
    {
      "arxiv_id": "2509.25160v1",
      "title": "GSM8K-V: Can Vision Language Models Solve Grade School Math Word Problems in Visual Contexts",
      "title_zh": "GSM8K-Vï¼šè§†è§‰è¯­è¨€æ¨¡å‹èƒ½å¦è§£å†³è§†è§‰è¯­å¢ƒä¸‹çš„å°å­¦æ•°å­¦åº”ç”¨é¢˜ï¼Ÿ",
      "authors": [
        "Fan Yuan",
        "Yuchen Yan",
        "Yifan Jiang",
        "Haoran Zhao",
        "Tao Feng",
        "Jinyan Chen",
        "Yanwei Lou",
        "Wenqi Zhang",
        "Yongliang Shen",
        "Weiming Lu",
        "Jun Xiao",
        "Yueting Zhuang"
      ],
      "abstract": "Vision language models (VLMs) achieve unified modeling of images and text, enabling them to accomplish complex real-world tasks through perception, planning, and reasoning. Among these tasks, reasoning is particularly representative, with mathematical reasoning serving as a prominent example. It highlights the high-level capability of VLMs to comprehend mathematical information in images and to perform sophisticated reasoning. Recently, numerous visual mathematical reasoning benchmarks have been proposed, but they are often restricted to geometry, lack coverage of math word problems, and rarely assess reasoning across multiple images. To address these gaps, we introduce GSM8K-V, a purely visual multi-image mathematical reasoning benchmark. GSM8K-V is built by systematically mapping each sample from the widely used text-based GSM8K into visual form. Through a carefully designed automated image-generation pipeline combined with meticulous human annotation, we curate 1,319 high-quality samples. We evaluate a wide range of open-source and closed-source models on GSM8K-V. Results show that although existing VLMs have nearly saturated performance on text-based GSM8K, there remains substantial room for improvement on GSM8K-V. For example, the best-performing model, Gemini-2.5-Pro, achieves 95.22% accuracy on GSM8K but only 46.93% on GSM8K-V. We conduct a comprehensive analysis of GSM8K-V, examining the limitations of current models as well as potential directions for improvement. GSM8K-V offers a new perspective on visual mathematical reasoning and establishes a benchmark to guide the development of more robust and generalizable VLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† GSM8K-Vï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) çš„çº¯è§†è§‰å¤šå›¾æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨å¡«è¡¥ç°æœ‰åŸºå‡†åœ¨æ•°å­¦åº”ç”¨é¢˜å’Œå¤šå›¾æ¨ç†èƒ½åŠ›è¯„ä¼°æ–¹é¢çš„ç©ºç™½ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡è‡ªåŠ¨å›¾åƒç”Ÿæˆæµæ°´çº¿ç»“åˆäººå·¥æ ‡æ³¨ï¼Œå°†å¹¿æ³›ä½¿ç”¨çš„æ–‡æœ¬æ•°å­¦åŸºå‡† GSM8K ç³»ç»Ÿåœ°æ˜ å°„ä¸ºåŒ…å« 1,319 ä¸ªé«˜è´¨é‡æ ·æœ¬çš„è§†è§‰å½¢å¼ã€‚å®éªŒå¯¹å¤šç§å¼€æºå’Œé—­æºæ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºç°æœ‰æ¨¡å‹åœ¨æ–‡æœ¬ç‰ˆ GSM8K ä¸Šå·²æ¥è¿‘æ€§èƒ½é¥±å’Œï¼Œä½†åœ¨ GSM8K-V ä¸Šçš„è¡¨ç°ä»æœ‰å·¨å¤§æå‡ç©ºé—´ã€‚ä¾‹å¦‚ï¼Œè¡¨ç°æœ€ä½³çš„ Gemini-2.5-Pro åœ¨æ–‡æœ¬åŸºå‡†ä¸Šå‡†ç¡®ç‡è¾¾ 95.22%ï¼Œè€Œåœ¨è§†è§‰ç‰ˆçš„ GSM8K-V ä¸Šä»…ä¸º 46.93%ã€‚é€šè¿‡å¯¹æ¨¡å‹å±€é™æ€§çš„æ·±å…¥åˆ†æï¼ŒGSM8K-V ä¸ºè§†è§‰æ•°å­¦æ¨ç†æä¾›äº†å…¨æ–°è§†è§’ï¼Œå¹¶ä¸ºå¼€å‘æ›´å…·é²æ£’æ€§å’Œé€šç”¨æ€§çš„ VLMs å¥ å®šäº†åŸºå‡†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "68 pages, 6 figures, Project Page: https://zju-real.github.io/GSM8K-V Code: https://github.com/ZJU-REAL/GSM8K-V Datasets: https://huggingface.co/datasets/ZJU-REAL/GSM8K-V",
      "pdf_url": "https://arxiv.org/pdf/2509.25160v1",
      "published_date": "2025-09-29 17:57:05 UTC",
      "updated_date": "2025-09-29 17:57:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:47:33.393278+00:00"
    },
    {
      "arxiv_id": "2509.25157v1",
      "title": "Chance-constrained Flow Matching for High-Fidelity Constraint-aware Generation",
      "title_zh": "é¢å‘é«˜ä¿çœŸçº¦æŸæ„ŸçŸ¥ç”Ÿæˆçš„æœºä¼šçº¦æŸæµåŒ¹é…",
      "authors": [
        "Jinhao Liang",
        "Yixuan Sun",
        "Anirban Samaddar",
        "Sandeep Madireddy",
        "Ferdinando Fioretto"
      ],
      "abstract": "Generative models excel at synthesizing high-fidelity samples from complex data distributions, but they often violate hard constraints arising from physical laws or task specifications. A common remedy is to project intermediate samples onto the feasible set; however, repeated projection can distort the learned distribution and induce a mismatch with the data manifold. Thus, recent multi-stage procedures attempt to defer projection to clean samples during sampling, but they increase algorithmic complexity and accumulate errors across steps. This paper addresses these challenges by proposing a novel training-free method, Chance-constrained Flow Matching (CCFM), that integrates stochastic optimization into the sampling process, enabling effective enforcement of hard constraints while maintaining high-fidelity sample generation. Importantly, CCFM guarantees feasibility in the same manner as conventional repeated projection, yet, despite operating directly on noisy intermediate samples, it is theoretically equivalent to projecting onto the feasible set defined by clean samples. This yields a sampler that mitigates distributional distortion. Empirical experiments show that CCFM outperforms current state-of-the-art constrained generative models in modeling complex physical systems governed by partial differential equations and molecular docking problems, delivering higher feasibility and fidelity.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿæˆæ¨¡å‹åœ¨åˆæˆå¤æ‚æ•°æ®æ—¶å¸¸è¿åç‰©ç†å®šå¾‹æˆ–ä»»åŠ¡è§„èŒƒç­‰ç¡¬çº¦æŸ(hard constraints)çš„é—®é¢˜ï¼ŒæŒ‡å‡ºäº†ä¼ ç»Ÿé‡å¤æŠ•å½±(repeated projection)æ–¹æ³•ä¼šå¯¼è‡´åˆ†å¸ƒæ‰­æ›²å’Œæµå½¢ä¸åŒ¹é…çš„å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–°æ–¹æ³•â€”â€”æœºä¼šçº¦æŸæµåŒ¹é…(Chance-constrained Flow Matching, CCFM)ï¼Œé€šè¿‡å°†éšæœºä¼˜åŒ–(stochastic optimization)æ•´åˆåˆ°é‡‡æ ·è¿‡ç¨‹ä¸­ï¼Œå®ç°äº†å¯¹ç¡¬çº¦æŸçš„æœ‰æ•ˆæ‰§è¡Œã€‚CCFM åœ¨ä¿è¯å¯è¡Œæ€§çš„åŒæ—¶ï¼Œåœ¨ç†è®ºä¸Šç­‰åŒäºå°†æ¸…ç†åçš„æ ·æœ¬æŠ•å½±åˆ°å¯è¡ŒåŸŸï¼Œä»è€Œæœ‰æ•ˆç¼“è§£äº†é‡‡æ ·è¿‡ç¨‹ä¸­çš„åˆ†å¸ƒç•¸å˜ã€‚ä¸ç°æœ‰çš„å¤šé˜¶æ®µç¨‹åºç›¸æ¯”ï¼Œè¯¥æ–¹æ³•é™ä½äº†ç®—æ³•å¤æ‚åº¦å¹¶å‡å°‘äº†è·¨æ­¥éª¤çš„è¯¯å·®ç´¯ç§¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCCFM åœ¨å¤„ç†å—åå¾®åˆ†æ–¹ç¨‹(partial differential equations)åˆ¶çº¦çš„å¤æ‚ç‰©ç†ç³»ç»Ÿä»¥åŠåˆ†å­å¯¹æ¥(molecular docking)é—®é¢˜æ—¶ï¼Œå…¶è¡¨ç°ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„çº¦æŸç”Ÿæˆæ¨¡å‹ã€‚è¯¥ç ”ç©¶åœ¨æ˜¾è‘—æé«˜ç”Ÿæˆç»“æœå¯è¡Œæ€§ä¸ä¿çœŸåº¦(fidelity)çš„åŒæ—¶ï¼Œä¸ºçº¦æŸæ„ŸçŸ¥å‹ç”Ÿæˆä»»åŠ¡æä¾›äº†ä¸€ç§å…·å¤‡ç†è®ºæ”¯æ’‘çš„é«˜æ•ˆè§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25157v1",
      "published_date": "2025-09-29 17:56:52 UTC",
      "updated_date": "2025-09-29 17:56:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:47:34.463317+00:00"
    },
    {
      "arxiv_id": "2510.02374v1",
      "title": "A Hybrid CAPTCHA Combining Generative AI with Keystroke Dynamics for Enhanced Bot Detection",
      "title_zh": "èåˆç”Ÿæˆå¼äººå·¥æ™ºèƒ½ä¸å‡»é”®åŠ¨åŠ›å­¦çš„å¢å¼ºå‹æœºå™¨äººæ£€æµ‹æ··åˆéªŒè¯ç ",
      "authors": [
        "Ayda Aghaei Nia"
      ],
      "abstract": "Completely Automated Public Turing tests to tell Computers and Humans Apart (CAPTCHAs) are a foundational component of web security, yet traditional implementations suffer from a trade-off between usability and resilience against AI-powered bots. This paper introduces a novel hybrid CAPTCHA system that synergizes the cognitive challenges posed by Large Language Models (LLMs) with the behavioral biometric analysis of keystroke dynamics. Our approach generates dynamic, unpredictable questions that are trivial for humans but non-trivial for automated agents, while simultaneously analyzing the user's typing rhythm to distinguish human patterns from robotic input. We present the system's architecture, formalize the feature extraction methodology for keystroke analysis, and report on an experimental evaluation. The results indicate that our dual-layered approach achieves a high degree of accuracy in bot detection, successfully thwarting both paste-based and script-based simulation attacks, while maintaining a high usability score among human participants. This work demonstrates the potential of combining cognitive and behavioral tests to create a new generation of more secure and user-friendly CAPTCHAs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆç”Ÿæˆå¼äººå·¥æ™ºèƒ½(Generative AI)ä¸æŒ‰é”®åŠ¨åŠ›å­¦(Keystroke Dynamics)çš„æ–°å‹æ··åˆéªŒè¯ç (CAPTCHA)ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³ä¼ ç»ŸéªŒè¯ç åœ¨æ˜“ç”¨æ€§ä¸é˜²å¾¡AIé©±åŠ¨æœºå™¨äººä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)ç”ŸæˆåŠ¨æ€ä¸”ä¸å¯é¢„æµ‹çš„è®¤çŸ¥æŒ‘æˆ˜é—®é¢˜ï¼ŒåŒæ—¶é€šè¿‡åˆ†æç”¨æˆ·çš„è¾“å…¥èŠ‚å¥æ¥åŒºåˆ†äººç±»è¡Œä¸ºä¸æœºå™¨äººçš„è‡ªåŠ¨è¾“å…¥ã€‚ç ”ç©¶è¯¦ç»†ä»‹ç»äº†ç³»ç»Ÿæ¶æ„åŠå…¶æŒ‰é”®åˆ†æçš„ç‰¹å¾æå–æ–¹æ³•ï¼Œå¹¶è¿›è¡Œäº†å®éªŒè¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œè¿™ç§åŒå±‚é˜²å¾¡æœºåˆ¶èƒ½ä»¥æé«˜çš„å‡†ç¡®ç‡è¯†åˆ«æœºå™¨äººï¼ŒæˆåŠŸæŠµå¾¡åŸºäºç²˜è´´(paste-based)å’Œè„šæœ¬ç¼–å†™(script-based)çš„æ¨¡æ‹Ÿæ”»å‡»ï¼Œå¹¶ä¿æŒäº†è¾ƒé«˜çš„äººç±»ç”¨æˆ·æ˜“ç”¨æ€§è¯„åˆ†ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†ç»“åˆè®¤çŸ¥æŒ‘æˆ˜ä¸è¡Œä¸ºç”Ÿç‰©è¯†åˆ«æŠ€æœ¯ï¼Œåœ¨å¼€å‘æ›´å®‰å…¨ä¸”ç”¨æˆ·å‹å¥½çš„æ–°ä¸€ä»£éªŒè¯ç æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "6 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.02374v1",
      "published_date": "2025-09-29 17:56:13 UTC",
      "updated_date": "2025-09-29 17:56:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:47:44.787366+00:00"
    },
    {
      "arxiv_id": "2509.25154v1",
      "title": "Who's Your Judge? On the Detectability of LLM-Generated Judgments",
      "title_zh": "è°æ˜¯ä½ çš„è¯„å®¡è€…ï¼Ÿè®ºå¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆè¯„å®¡çš„å¯æ£€æµ‹æ€§",
      "authors": [
        "Dawei Li",
        "Zhen Tan",
        "Chengshuai Zhao",
        "Bohan Jiang",
        "Baixiang Huang",
        "Pingchuan Ma",
        "Abdullah Alnaibari",
        "Kai Shu",
        "Huan Liu"
      ],
      "abstract": "Large Language Model (LLM)-based judgments leverage powerful LLMs to efficiently evaluate candidate content and provide judgment scores. However, the inherent biases and vulnerabilities of LLM-generated judgments raise concerns, underscoring the urgent need for distinguishing them in sensitive scenarios like academic peer reviewing. In this work, we propose and formalize the task of judgment detection and systematically investigate the detectability of LLM-generated judgments. Unlike LLM-generated text detection, judgment detection relies solely on judgment scores and candidates, reflecting real-world scenarios where textual feedback is often unavailable in the detection process. Our preliminary analysis shows that existing LLM-generated text detection methods perform poorly given their incapability to capture the interaction between judgment scores and candidate content -- an aspect crucial for effective judgment detection. Inspired by this, we introduce \\textit{J-Detector}, a lightweight and transparent neural detector augmented with explicitly extracted linguistic and LLM-enhanced features to link LLM judges' biases with candidates' properties for accurate detection. Experiments across diverse datasets demonstrate the effectiveness of \\textit{J-Detector} and show how its interpretability enables quantifying biases in LLM judges. Finally, we analyze key factors affecting the detectability of LLM-generated judgments and validate the practical utility of judgment detection in real-world scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æ­£å¼æå‡ºäº†åˆ¤æ–­æ£€æµ‹(judgment detection)ä»»åŠ¡ï¼Œæ—¨åœ¨è¯†åˆ«ç”±å¤§è¯­è¨€æ¨¡å‹(LLM)ç”Ÿæˆçš„è¯„ä¼°è¯„åˆ†ï¼Œä»¥åº”å¯¹å­¦æœ¯åŒè¡Œè¯„å®¡ç­‰æ•æ„Ÿåœºæ™¯ä¸­çš„åè§ä¸å®‰å…¨éšæ‚£ã€‚ä¸ä¼ ç»Ÿçš„LLMç”Ÿæˆæ–‡æœ¬æ£€æµ‹(LLM-generated text detection)ä¸åŒï¼Œè¯¥ä»»åŠ¡åœ¨æ£€æµ‹è¿‡ç¨‹ä¸­ä»…ä¾èµ–åˆ¤æ–­åˆ†æ•°å’Œå€™é€‰å†…å®¹ï¼Œåæ˜ äº†æ–‡æœ¬åé¦ˆå¾€å¾€ä¸å¯ç”¨çš„ç°å®åº”ç”¨åœºæ™¯ã€‚ç ”ç©¶å‘ç°ç°æœ‰æ£€æµ‹æ–¹æ³•å› æ— æ³•æ•æ‰è¯„åˆ†ä¸å€™é€‰å†…å®¹ä¹‹é—´çš„äº¤äº’å…³ç³»è€Œè¡¨ç°æ¬ ç¼ºï¼Œä¸ºæ­¤ä½œè€…å¼•å…¥äº†J-Detectorï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆäº†æ˜¾å¼è¯­è¨€ç‰¹å¾å’ŒLLMå¢å¼ºç‰¹å¾çš„è½»é‡åŒ–é€æ˜ç¥ç»æ£€æµ‹å™¨ã€‚é€šè¿‡å°†LLMè¯„å§”çš„åè§ä¸å€™é€‰å±æ€§ç›¸é“¾æ¥ï¼ŒJ-Detectoråœ¨å¤šé¡¹æ•°æ®é›†ä¸Šå±•ç°äº†ä¼˜å¼‚çš„æ£€æµ‹æ€§èƒ½ã€‚å®éªŒè¿›ä¸€æ­¥è¯æ˜äº†è¯¥æ¨¡å‹çš„å¯è§£é‡Šæ€§èƒ½å¤Ÿæœ‰æ•ˆé‡åŒ–LLMè¯„å§”çš„åè§ï¼Œå¹¶éªŒè¯äº†åˆ¤æ–­æ£€æµ‹åœ¨çœŸå®åœºæ™¯ä¸­çš„å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Under review",
      "pdf_url": "https://arxiv.org/pdf/2509.25154v1",
      "published_date": "2025-09-29 17:54:57 UTC",
      "updated_date": "2025-09-29 17:54:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:48:06.700817+00:00"
    },
    {
      "arxiv_id": "2510.17822v1",
      "title": "A Biophysical-Model-Informed Source Separation Framework For EMG Decomposition",
      "title_zh": "èåˆç”Ÿç‰©ç‰©ç†æ¨¡å‹çš„è‚Œç”µåˆ†è§£æºåˆ†ç¦»æ¡†æ¶",
      "authors": [
        "D. Halatsis",
        "P. Mamidanna",
        "J. Pereira",
        "D. Farina"
      ],
      "abstract": "Recent advances in neural interfacing have enabled significant improvements in human-computer interaction, rehabilitation, and neuromuscular diagnostics. Motor unit (MU) decomposition from surface electromyography (sEMG) is a key technique for extracting neural drive information, but traditional blind source separation (BSS) methods fail to incorporate biophysical constraints, limiting their accuracy and interpretability. In this work, we introduce a novel Biophysical-Model-Informed Source Separation (BMISS) framework, which integrates anatomically accurate forward EMG models into the decomposition process. By leveraging MRI-based anatomical reconstructions and generative modeling, our approach enables direct inversion of a biophysically accurate forward model to estimate both neural drive and motor neuron properties in an unsupervised manner. Empirical validation in a controlled simulated setting demonstrates that BMISS achieves higher fidelity motor unit estimation while significantly reducing computational cost compared to traditional methods. This framework paves the way for non-invasive, personalized neuromuscular assessments, with potential applications in clinical diagnostics, prosthetic control, and neurorehabilitation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†BMISSï¼ˆBiophysical-Model-Informed Source Separationï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿç›²æºåˆ†ç¦»ï¼ˆBSSï¼‰æ–¹æ³•åœ¨è¡¨é¢è‚Œç”µä¿¡å·ï¼ˆsEMGï¼‰è¿åŠ¨å•å…ƒï¼ˆMotor unitï¼‰åˆ†è§£ä¸­å› ç¼ºä¹ç”Ÿç‰©ç‰©ç†çº¦æŸè€Œå¯¼è‡´çš„å‡†ç¡®æ€§ä¸å¯è§£é‡Šæ€§å—é™é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡æ•´åˆåŸºäºMRIçš„è§£å‰–é‡å»ºå’Œç”Ÿæˆæ¨¡å‹ï¼Œå®ç°äº†ç”Ÿç‰©ç‰©ç†å‡†ç¡®çš„å‰å‘EMGæ¨¡å‹ç›´æ¥åæ¼”ï¼Œä»è€Œèƒ½å¤Ÿä»¥æ— ç›‘ç£çš„æ–¹å¼åŒæ—¶ä¼°è®¡ç¥ç»é©±åŠ¨ï¼ˆNeural driveï¼‰å’Œè¿åŠ¨ç¥ç»å…ƒï¼ˆMotor neuronï¼‰çš„å±æ€§ã€‚åœ¨å—æ§æ¨¡æ‹Ÿç¯å¢ƒä¸‹çš„å®éªŒéªŒè¯è¡¨æ˜ï¼ŒBMISSåœ¨æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ï¼Œæ¯”ä¼ ç»Ÿæ–¹æ³•å®ç°äº†æ›´é«˜ä¿çœŸåº¦çš„è¿åŠ¨å•å…ƒä¼°ç®—ã€‚è¿™ä¸€ç ”ç©¶æˆæœä¸ºæ— åˆ›ã€ä¸ªæ€§åŒ–çš„ç¥ç»è‚Œè‚‰è¯„ä¼°å¥ å®šäº†åŸºç¡€ï¼Œåœ¨ä¸´åºŠè¯Šæ–­ã€å‡è‚¢æ§åˆ¶å’Œç¥ç»åº·å¤ç­‰é¢†åŸŸå…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚",
      "categories": [
        "q-bio.NC",
        "cs.AI"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.17822v1",
      "published_date": "2025-09-29 17:53:52 UTC",
      "updated_date": "2025-09-29 17:53:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:48:35.393762+00:00"
    },
    {
      "arxiv_id": "2509.25149v1",
      "title": "Pretraining Large Language Models with NVFP4",
      "title_zh": "ä½¿ç”¨ NVFP4 é¢„è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "NVIDIA",
        "Felix Abecassis",
        "Anjulie Agrusa",
        "Dong Ahn",
        "Jonah Alben",
        "Stefania Alborghetti",
        "Michael Andersch",
        "Sivakumar Arayandi",
        "Alexis Bjorlin",
        "Aaron Blakeman",
        "Evan Briones",
        "Ian Buck",
        "Bryan Catanzaro",
        "Jinhang Choi",
        "Mike Chrzanowski",
        "Eric Chung",
        "Victor Cui",
        "Steve Dai",
        "Bita Darvish Rouhani",
        "Carlo del Mundo",
        "Deena Donia",
        "Burc Eryilmaz",
        "Henry Estela",
        "Abhinav Goel",
        "Oleg Goncharov",
        "Yugi Guvvala",
        "Robert Hesse",
        "Russell Hewett",
        "Herbert Hum",
        "Ujval Kapasi",
        "Brucek Khailany",
        "Mikail Khona",
        "Nick Knight",
        "Alex Kondratenko",
        "Ronny Krashinsky",
        "Ben Lanir",
        "Simon Layton",
        "Michael Lightstone",
        "Daniel Lo",
        "Paulius Micikevicius",
        "Asit Mishra",
        "Tim Moon",
        "Deepak Narayanan",
        "Chao Ni",
        "Abhijit Paithankar",
        "Satish Pasumarthi",
        "Ankit Patel",
        "Mostofa Patwary",
        "Ashwin Poojary",
        "Gargi Prasad",
        "Sweta Priyadarshi",
        "Yigong Qin",
        "Xiaowei Ren",
        "Oleg Rybakov",
        "Charbel Sakr",
        "Sanjeev Satheesh",
        "Stas Sergienko",
        "Pasha Shamis",
        "Kirthi Shankar",
        "Nishant Sharma",
        "Mohammad Shoeybi",
        "Michael Siu",
        "Misha Smelyanskiy",
        "Darko Stosic",
        "Dusan Stosic",
        "Bor-Yiing Su",
        "Frank Sun",
        "Nima Tajbakhsh",
        "Shelby Thomas",
        "Przemek Tredak",
        "Evgeny Tsykunov",
        "Gandhi Vaithilingam",
        "Aditya Vavre",
        "Rangharajan Venkatesan",
        "Roger Waleffe",
        "Qiyu Wan",
        "Hexin Wang",
        "Mengdi Wang",
        "Lizzie Wei",
        "Hao Wu",
        "Evan Wu",
        "Keith Wyss",
        "Ning Xu",
        "Jinze Xue",
        "Charlene Yang",
        "Yujia Zhai",
        "Ruoxi Zhang",
        "Jingyang Zhu",
        "Zhongbo Zhu"
      ],
      "abstract": "Large Language Models (LLMs) today are powerful problem solvers across many domains, and they continue to get stronger as they scale in model size, training set size, and training set quality, as shown by extensive research and experimentation across the industry. Training a frontier model today requires on the order of tens to hundreds of yottaflops, which is a massive investment of time, compute, and energy. Improving pretraining efficiency is therefore essential to enable the next generation of even more capable LLMs. While 8-bit floating point (FP8) training is now widely adopted, transitioning to even narrower precision, such as 4-bit floating point (FP4), could unlock additional improvements in computational speed and resource utilization. However, quantization at this level poses challenges to training stability, convergence, and implementation, notably for large-scale models trained on long token horizons.\n  In this study, we introduce a novel approach for stable and accurate training of large language models (LLMs) using the NVFP4 format. Our method integrates Random Hadamard transforms (RHT) to bound block-level outliers, employs a two-dimensional quantization scheme for consistent representations across both the forward and backward passes, utilizes stochastic rounding for unbiased gradient estimation, and incorporates selective high-precision layers. We validate our approach by training a 12-billion-parameter model on 10 trillion tokens -- the longest publicly documented training run in 4-bit precision to date. Our results show that the model trained with our NVFP4-based pretraining technique achieves training loss and downstream task accuracies comparable to an FP8 baseline. These findings highlight that NVFP4, when combined with our training approach, represents a major step forward in narrow-precision LLM training algorithms.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨ NVFP4 æ ¼å¼è¿›è¡Œå¤§è¯­è¨€æ¨¡å‹ (LLMs) ç¨³å®šä¸”å‡†ç¡®é¢„è®­ç»ƒçš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ 4-bit ç²¾åº¦åœ¨æ¨¡å‹è®­ç»ƒç¨³å®šæ€§ä¸æ”¶æ•›æ€§æ–¹é¢çš„æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•é€šè¿‡é›†æˆéšæœºå“ˆè¾¾ç›å˜æ¢ (Random Hadamard transforms, RHT) æ¥é™åˆ¶å—çº§ç¦»ç¾¤å€¼ï¼Œå¹¶é‡‡ç”¨äºŒç»´é‡åŒ–æ–¹æ¡ˆã€éšæœºèˆå…¥ (stochastic rounding) ä»¥åŠé€‰æ‹©æ€§é«˜ç²¾åº¦å±‚ï¼Œç¡®ä¿äº†è®¡ç®—è¿‡ç¨‹çš„ç²¾ç¡®æ€§ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨ 10 ä¸‡äº¿ (10 trillion) token ä¸ŠæˆåŠŸè®­ç»ƒäº†ä¸€ä¸ª 120 äº¿ (12-billion) å‚æ•°çš„æ¨¡å‹ï¼Œè¿™æ˜¯ç›®å‰å…¬å¼€è®°å½•ä¸­è§„æ¨¡æœ€å¤§ä¸”æŒç»­æ—¶é—´æœ€é•¿çš„ 4-bit ç²¾åº¦è®­ç»ƒå®éªŒã€‚ç»“æœè¯æ˜ï¼Œé‡‡ç”¨ NVFP4 é¢„è®­ç»ƒçš„æ¨¡å‹åœ¨è®­ç»ƒæŸå¤±å’Œä¸‹æ¸¸ä»»åŠ¡å‡†ç¡®ç‡ä¸Šå‡ä¸ FP8 åŸºå‡†ç›¸å½“ã€‚è¿™ä¸€çªç ´è¡¨æ˜ NVFP4 ç»“åˆç‰¹å®šçš„è®­ç»ƒç®—æ³•æ˜¯å®ç°é«˜æ•ˆã€çª„ç²¾åº¦ LLM è®­ç»ƒçš„é‡è¦è¿›å±•ï¼Œæœ‰åŠ©äºå¤§å¹…æå‡è®¡ç®—é€Ÿåº¦ä¸èµ„æºåˆ©ç”¨ç‡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25149v1",
      "published_date": "2025-09-29 17:53:17 UTC",
      "updated_date": "2025-09-29 17:53:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:48:47.394063+00:00"
    },
    {
      "arxiv_id": "2509.25148v1",
      "title": "UniAPL: A Unified Adversarial Preference Learning Framework for Instruct-Following",
      "title_zh": "UniAPLï¼šé¢å‘æŒ‡ä»¤éµå¾ªçš„ç»Ÿä¸€å¯¹æŠ—åå¥½å­¦ä¹ æ¡†æ¶",
      "authors": [
        "FaQiang Qian",
        "WeiKun Zhang",
        "Ziliang Wang",
        "Kang An",
        "Xuhui Zheng",
        "Liangjian Wen",
        "Mengya Gao",
        "Yong Dai",
        "Yichao Wu"
      ],
      "abstract": "Shaping powerful LLMs to be beneficial and safe is central to AI alignment. We argue that post-training alignment is fundamentally a unified Preference Learning problem, involving two modalities: demonstrated preferences (e.g., Supervised Fine-Tuning, SFT) and comparative preferences (e.g., Reinforcement Learning, RL).The standard sequential pipeline-SFT followed by RL-is flawed due to a critical distributional mismatch: SFT uses static expert data, but as the policy evolves, its generation distribution drifts, making SFT knowledge brittle. Subsequent RL then explores without direct access to the rich, ground-truth knowledge in expert demonstrations, leading to inefficient, ungrounded updates. This separation prevents mutual regularization between data sources. To address this, we reframe alignment as a constrained optimization problem and propose Unified Adversarial Preference Learning (UniAPL),a novel framework that dynamically aligns the policy's distribution with the expert's. UniAPL implements a single-stage unified training objective, jointly learning from mixed batches of SFT and preference data. In every gradient step, dense expert demonstrations directly ground and regularize online exploration, inherently resolving distributional mismatch and maximizing data synergy.We evaluate UniAPL on instruction-following tasks using Qwen3-235B-Instruct-2507 as the teacher. Our models match or exceed strong GRPO baselines: +5.77% on Qwen3-0.6B (matching a 32B model) and +3.75% on Qwen3-4B,even outperforming the teacher. Analyses of response length and log-probability distributions confirm that UniAPL outputs closely mimic expert demonstrations, achieving both stronger performance and better behavioral alignment.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†UniAPLï¼Œä¸€ä¸ªç”¨äºæŒ‡ä»¤éµå¾ª(Instruct-Following)çš„ç»Ÿä¸€å¯¹æŠ—åå¥½å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿå¯¹é½æµç¨‹ä¸­ç›‘ç£å¾®è°ƒ(SFT)ä¸å¼ºåŒ–å­¦ä¹ (RL)åˆ†æ­¥æ‰§è¡Œå¯¼è‡´çš„åˆ†å¸ƒä¸åŒ¹é…é—®é¢˜ã€‚ç ”ç©¶è€…å°†å¯¹é½é‡æ–°å®šä¹‰ä¸ºä¸€ç§å—é™ä¼˜åŒ–é—®é¢˜ï¼Œé€šè¿‡å•é˜¶æ®µç»Ÿä¸€è®­ç»ƒç›®æ ‡ï¼Œå®ç°ä»SFTå’Œåå¥½æ•°æ®çš„æ··åˆæ‰¹æ¬¡ä¸­åŒæ­¥å­¦ä¹ ã€‚åœ¨UniAPLæ¡†æ¶ä¸‹ï¼Œé«˜å¯†åº¦çš„ä¸“å®¶æ¼”ç¤ºåœ¨æ¯ä¸ªæ¢¯åº¦æ­¥éª¤ä¸­ç›´æ¥å¼•å¯¼å¹¶æ­£åˆ™åŒ–åœ¨çº¿æ¢ç´¢è¿‡ç¨‹ï¼Œä»è€Œæœ‰æ•ˆæ¶ˆé™¤äº†åˆ†å¸ƒåç§»å¹¶å¢å¼ºäº†æ•°æ®ååŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨Qwen3ç³»åˆ—æ¨¡å‹ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒQwen3-0.6Bæ¨¡å‹åœ¨æå‡5.77%åè¾¾åˆ°äº†32Bæ¨¡å‹çš„æ€§èƒ½æ°´å¹³ã€‚åˆ†æè¿›ä¸€æ­¥è¯å®ï¼ŒUniAPLç”Ÿæˆçš„å“åº”èƒ½å¤Ÿç´§å¯†æ¨¡æ‹Ÿä¸“å®¶æ¼”ç¤ºçš„å¯¹é½è¡Œä¸ºï¼Œåœ¨æ€§èƒ½å’Œè¡Œä¸ºä¸€è‡´æ€§ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼Œç”šè‡³åœ¨éƒ¨åˆ†ä»»åŠ¡ä¸­è¶…è¶Šäº†æ•™å¸ˆæ¨¡å‹ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25148v1",
      "published_date": "2025-09-29 17:53:09 UTC",
      "updated_date": "2025-09-29 17:53:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:48:38.092830+00:00"
    },
    {
      "arxiv_id": "2509.25146v1",
      "title": "Fast Feature Field ($\\text{F}^3$): A Predictive Representation of Events",
      "title_zh": "Fast Feature Field ($\\text{F}^3$)ï¼šä¸€ç§äº‹ä»¶é¢„æµ‹æ€§è¡¨å¾",
      "authors": [
        "Richeek Das",
        "Kostas Daniilidis",
        "Pratik Chaudhari"
      ],
      "abstract": "This paper develops a mathematical argument and algorithms for building representations of data from event-based cameras, that we call Fast Feature Field ($\\text{F}^3$). We learn this representation by predicting future events from past events and show that it preserves scene structure and motion information. $\\text{F}^3$ exploits the sparsity of event data and is robust to noise and variations in event rates. It can be computed efficiently using ideas from multi-resolution hash encoding and deep sets - achieving 120 Hz at HD and 440 Hz at VGA resolutions. $\\text{F}^3$ represents events within a contiguous spatiotemporal volume as a multi-channel image, enabling a range of downstream tasks. We obtain state-of-the-art performance on optical flow estimation, semantic segmentation, and monocular metric depth estimation, on data from three robotic platforms (a car, a quadruped robot and a flying platform), across different lighting conditions (daytime, nighttime), environments (indoors, outdoors, urban, as well as off-road) and dynamic vision sensors (resolutions and event rates). Our implementations can predict these tasks at 25-75 Hz at HD resolution.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†åä¸º Fast Feature Field ($\\text{F}^3$) çš„äº‹ä»¶ç›¸æœºæ•°æ®è¡¨ç¤ºæ–¹æ³•ï¼Œé€šè¿‡ä»å†å²äº‹ä»¶é¢„æµ‹æœªæ¥äº‹ä»¶æ¥å­¦ä¹ å¦‚ä½•æœ‰æ•ˆä¿ç•™åœºæ™¯ç»“æ„ä¸è¿åŠ¨ä¿¡æ¯ã€‚$\\text{F}^3$ å……åˆ†åˆ©ç”¨äº†äº‹ä»¶æ•°æ®çš„ç¨€ç–æ€§ï¼Œå¹¶å¯¹å™ªå£°å’Œäº‹ä»¶é€Ÿç‡çš„å˜åŒ–è¡¨ç°å‡ºæå¼ºçš„é²æ£’æ€§ã€‚è¯¥ç®—æ³•ç»“åˆäº† multi-resolution hash encoding å’Œ deep sets çš„æ€æƒ³ï¼Œåœ¨ HD åˆ†è¾¨ç‡ä¸‹å®ç°äº† 120 Hz çš„è¶…é«˜æ•ˆè®¡ç®—ã€‚é€šè¿‡å°†è¿ç»­æ—¶ç©ºä½“ç§¯å†…çš„äº‹ä»¶è½¬åŒ–ä¸ºå¤šé€šé“å›¾åƒï¼Œ$\\text{F}^3$ èƒ½å¤Ÿä¸ºå¤šç§ä¸‹æ¸¸ä»»åŠ¡æä¾›æ”¯æŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ optical flow estimationã€semantic segmentation å’Œ monocular metric depth estimation ç­‰ä»»åŠ¡ä¸­å‡è¾¾åˆ°äº† state-of-the-art æ€§èƒ½ã€‚åœ¨æ¶µç›–å¤šç§æœºå™¨äººå¹³å°ã€æç«¯å…‰ç…§æ¡ä»¶åŠå„ç±»ä¼ æ„Ÿå™¨çš„å¤æ‚ç¯å¢ƒä¸‹ï¼Œè¯¥ç³»ç»Ÿå±•ç°äº†å“è¶Šçš„æ³›åŒ–èƒ½åŠ›å’Œå®æ—¶å¤„ç†æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "39 pages, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.25146v1",
      "published_date": "2025-09-29 17:52:31 UTC",
      "updated_date": "2025-09-29 17:52:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:48:53.487320+00:00"
    },
    {
      "arxiv_id": "2509.25144v1",
      "title": "Paired by the Teacher: Turning Unpaired Data into High-Fidelity Pairs for Low-Resource Text Generation",
      "title_zh": "Paired by the Teacherï¼šå°†éé…å¯¹æ•°æ®è½¬åŒ–ä¸ºé¢å‘ä½èµ„æºæ–‡æœ¬ç”Ÿæˆçš„é«˜ä¿çœŸé…å¯¹æ•°æ®",
      "authors": [
        "Yen-Ju Lu",
        "Thomas Thebaud",
        "Laureano Moro-Velazquez",
        "Najim Dehak",
        "Jesus Villalba"
      ],
      "abstract": "We present Paired by the Teacher (PbT), a two-stage teacher-student pipeline that synthesizes accurate input-output pairs without human labels or parallel data. In many low-resource natural language generation (NLG) scenarios, practitioners may have only raw outputs, like highlights, recaps, or questions, or only raw inputs, such as articles, dialogues, or paragraphs, but seldom both. This mismatch forces small models to learn from very few examples or rely on costly, broad-scope synthetic examples produced by large LLMs. PbT addresses this by asking a teacher LLM to compress each unpaired example into a concise intermediate representation (IR), and training a student to reconstruct inputs from IRs. This enables outputs to be paired with student-generated inputs, yielding high-quality synthetic data. We evaluate PbT on five benchmarks-document summarization (XSum, CNNDM), dialogue summarization (SAMSum, DialogSum), and question generation (SQuAD)-as well as an unpaired setting on SwitchBoard (paired with DialogSum summaries). An 8B student trained only on PbT data outperforms models trained on 70 B teacher-generated corpora and other unsupervised baselines, coming within 1.2 ROUGE-L of human-annotated pairs and closing 82% of the oracle gap at one-third the annotation cost of direct synthesis. Human evaluation on SwitchBoard further confirms that only PbT produces concise, faithful summaries aligned with the target style, highlighting its advantage of generating in-domain sources that avoid the mismatch, limiting direct synthesis.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Paired by the Teacher (PbT)ï¼Œè¿™æ˜¯ä¸€ç§ä¸¤é˜¶æ®µçš„ Teacher-Student æµæ°´çº¿ï¼Œæ—¨åœ¨ä¸ºä½èµ„æºè‡ªç„¶è¯­è¨€ç”Ÿæˆ (Low-Resource NLG) åœºæ™¯åˆæˆé«˜ä¿çœŸçš„è¾“å…¥-è¾“å‡ºå¯¹ï¼Œä¸”æ— éœ€äººç±»æ ‡æ³¨æˆ–å¹³è¡Œæ•°æ®ã€‚PbT é€šè¿‡è®© Teacher LLM å°†æœªå¯¹é½çš„ç¤ºä¾‹å‹ç¼©ä¸ºç®€æ´çš„ä¸­é—´è¡¨ç¤º (Intermediate Representation, IR)ï¼Œå¹¶è®­ç»ƒ Student æ¨¡å‹ä» IR ä¸­é‡å»ºåŸå§‹è¾“å…¥ï¼Œä»è€Œæ„å»ºé«˜è´¨é‡çš„åˆæˆæ•°æ®é›†ã€‚å®éªŒåœ¨æ–‡æ¡£æ‘˜è¦ (XSum, CNNDM)ã€å¯¹è¯æ‘˜è¦ (SAMSum, DialogSum) å’Œé—®é¢˜ç”Ÿæˆ (SQuAD) ç­‰å¤šä¸ªåŸºå‡†ä¸Šè¿›è¡Œäº†éªŒè¯ã€‚ç»“æœè¡¨æ˜ï¼Œä»…åŸºäº PbT æ•°æ®è®­ç»ƒçš„ 8B Student æ¨¡å‹æ€§èƒ½ä¼˜äºä½¿ç”¨ 70B Teacher ç›´æ¥ç”Ÿæˆæ•°æ®çš„æ¨¡å‹ï¼Œç¼©å°äº† 82% çš„ Oracle æ€§èƒ½å·®è·ï¼Œä¸”æ ‡æ³¨æˆæœ¬ä»…ä¸ºä¸‰åˆ†ä¹‹ä¸€ã€‚äººç±»è¯„ä¼°è¿›ä¸€æ­¥è¯å®ï¼ŒPbT èƒ½ç”Ÿæˆä¸ç›®æ ‡é£æ ¼ä¸€è‡´ä¸”å¿ å®åº¦é«˜çš„æ‘˜è¦ï¼Œæœ‰æ•ˆè§£å†³äº†è·¨é¢†åŸŸç”Ÿæˆä¸­çš„æ•°æ®ä¸åŒ¹é…æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at EMNLP 2025 (Main Conference)",
      "pdf_url": "https://arxiv.org/pdf/2509.25144v1",
      "published_date": "2025-09-29 17:51:55 UTC",
      "updated_date": "2025-09-29 17:51:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:49:17.759326+00:00"
    },
    {
      "arxiv_id": "2509.25142v2",
      "title": "Visual serial processing deficits explain divergences in human and VLM reasoning",
      "title_zh": "è§†è§‰ä¸²è¡Œå¤„ç†ç¼ºé™·è§£é‡Šäº†äººç±»ä¸ VLM æ¨ç†ä¸­çš„å·®å¼‚",
      "authors": [
        "Nicholas Budny",
        "Kia Ghods",
        "Declan Campbell",
        "Raja Marjieh",
        "Amogh Joshi",
        "Sreejan Kumar",
        "Jonathan D. Cohen",
        "Taylor W. Webb",
        "Thomas L. Griffiths"
      ],
      "abstract": "Why do Vision Language Models (VLMs), despite success on standard benchmarks, often fail to match human performance on surprisingly simple visual reasoning tasks? While the underlying computational principles are still debated, we hypothesize that a crucial factor is a deficit in visually-grounded serial processing. To test this hypothesis, we compared human and VLM performance across tasks designed to vary serial processing demands in three distinct domains: geometric reasoning, perceptual enumeration, and mental rotation. Tasks within each domain varied serial processing load by manipulating factors such as geometric concept complexity, perceptual individuation load, and transformation difficulty. Across all domains, our results revealed a consistent pattern: decreased VLM accuracy was strongly correlated with increased human reaction time (used as a proxy for serial processing load). As tasks require more demanding serial processing -- whether composing concepts, enumerating items, or performing mental transformations -- the VLM-human performance gap widens reliably. These findings support our hypothesis, indicating that limitations in serial, visually grounded reasoning represent a fundamental bottleneck that distinguishes current VLMs from humans.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) åœ¨ç®€å•è§†è§‰æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°é€Šäºäººç±»çš„åŸå› ï¼Œå¹¶æå‡ºæ ¸å¿ƒå‡è®¾ï¼šVLMs ç¼ºä¹è§†è§‰åŸºç¡€çš„åºåˆ—å¤„ç† (visually-grounded serial processing) èƒ½åŠ›ã€‚ä¸ºäº†éªŒè¯è¿™ä¸€å‡è®¾ï¼Œç ”ç©¶è€…åœ¨å‡ ä½•æ¨ç† (geometric reasoning)ã€æ„ŸçŸ¥è®¡æ•° (perceptual enumeration) å’Œå¿ƒç†æ—‹è½¬ (mental rotation) ä¸‰ä¸ªé¢†åŸŸä¸­ï¼Œæ¯”è¾ƒäº†äººç±»ä¸ VLMs çš„è¡¨ç°ã€‚è¿™äº›ä»»åŠ¡é€šè¿‡è°ƒæ•´å‡ ä½•æ¦‚å¿µå¤æ‚åº¦ã€æ„ŸçŸ¥ä¸ªä½“åŒ–è´Ÿè·å’Œå˜æ¢éš¾åº¦ï¼Œæ¥æ”¹å˜åºåˆ—å¤„ç†çš„éœ€æ±‚è´Ÿè·ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨æ‰€æœ‰é¢†åŸŸä¸­ï¼ŒVLM å‡†ç¡®ç‡çš„ä¸‹é™ä¸äººç±»ååº”æ—¶é—´ï¼ˆä½œä¸ºåºåˆ—å¤„ç†è´Ÿè·çš„ä»£ç†æŒ‡æ ‡ï¼‰çš„å¢åŠ å‘ˆç°å¼ºç›¸å…³æ€§ã€‚éšç€ä»»åŠ¡å¯¹åºåˆ—å¤„ç†éœ€æ±‚ï¼ˆå¦‚æ¦‚å¿µç»„åˆã€é¡¹ç›®è®¡æ•°æˆ–å¿ƒç†å˜æ¢ï¼‰çš„æå‡ï¼ŒVLM ä¸äººç±»ä¹‹é—´çš„æ€§èƒ½å·®è·æ˜¾è‘—æ‰©å¤§ã€‚è¿™ä¸€å‘ç°è¡¨æ˜ï¼Œåºåˆ—åŒ–çš„ã€åŸºäºè§†è§‰çš„æ¨ç†èƒ½åŠ›çš„å±€é™æ€§æ˜¯åŒºåˆ†å½“å‰ VLMs ä¸äººç±»çš„åŸºæœ¬ç“¶é¢ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25142v2",
      "published_date": "2025-09-29 17:51:20 UTC",
      "updated_date": "2026-01-17 21:40:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:48:59.362854+00:00"
    },
    {
      "arxiv_id": "2509.25140v1",
      "title": "ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory",
      "title_zh": "ReasoningBankï¼šåŸºäºæ¨ç†è®°å¿†çš„æ™ºèƒ½ä½“è‡ªæˆ‘è¿›åŒ–è§„æ¨¡åŒ–",
      "authors": [
        "Siru Ouyang",
        "Jun Yan",
        "I-Hung Hsu",
        "Yanfei Chen",
        "Ke Jiang",
        "Zifeng Wang",
        "Rujun Han",
        "Long T. Le",
        "Samira Daruki",
        "Xiangru Tang",
        "Vishy Tirumalashetty",
        "George Lee",
        "Mahsan Rofouei",
        "Hangfei Lin",
        "Jiawei Han",
        "Chen-Yu Lee",
        "Tomas Pfister"
      ],
      "abstract": "With the growing adoption of large language model agents in persistent real-world roles, they naturally encounter continuous streams of tasks. A key limitation, however, is their failure to learn from the accumulated interaction history, forcing them to discard valuable insights and repeat past errors. We propose ReasoningBank, a novel memory framework that distills generalizable reasoning strategies from an agent's self-judged successful and failed experiences. At test time, an agent retrieves relevant memories from ReasoningBank to inform its interaction and then integrates new learnings back, enabling it to become more capable over time. Building on this powerful experience learner, we further introduce memory-aware test-time scaling (MaTTS), which accelerates and diversifies this learning process by scaling up the agent's interaction experience. By allocating more compute to each task, the agent generates abundant, diverse experiences that provide rich contrastive signals for synthesizing higher-quality memory. The better memory in turn guides more effective scaling, establishing a powerful synergy between memory and test-time scaling. Across web browsing and software engineering benchmarks, ReasoningBank consistently outperforms existing memory mechanisms that store raw trajectories or only successful task routines, improving both effectiveness and efficiency; MaTTS further amplifies these gains. These findings establish memory-driven experience scaling as a new scaling dimension, enabling agents to self-evolve with emergent behaviors naturally arise.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ReasoningBankï¼Œä¸€ç§æ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹(LLM)æ™ºèƒ½ä½“è‡ªæˆ‘è¿›åŒ–èƒ½åŠ›çš„æ–°å‹è®°å¿†æ¡†æ¶ã€‚é’ˆå¯¹æ™ºèƒ½ä½“æ— æ³•ä»å†å²äº¤äº’ä¸­å­¦ä¹ ä¸”æ˜“é‡å¤é”™è¯¯çš„å±€é™ï¼Œè¯¥æ¡†æ¶é€šè¿‡ä»è‡ªæˆ‘åˆ¤å®šçš„æˆåŠŸä¸å¤±è´¥ç»éªŒä¸­æå–å¯æ³›åŒ–çš„æ¨ç†ç­–ç•¥ï¼Œå®ç°çŸ¥è¯†çš„æŒç»­ç§¯ç´¯ä¸æ£€ç´¢åˆ©ç”¨ã€‚ç ”ç©¶è¿›ä¸€æ­¥å¼•å…¥äº†è®°å¿†æ„ŸçŸ¥æµ‹è¯•æ—¶é—´ç¼©æ”¾(Memory-aware test-time scaling, MaTTS)ï¼Œé€šè¿‡å¢åŠ è®¡ç®—æŠ•å…¥äº§ç”Ÿå¤šæ ·åŒ–äº¤äº’ç»éªŒï¼Œä»è€Œåˆæˆæ›´é«˜è´¨é‡çš„è®°å¿†å¹¶ä¸æ¨ç†è¿‡ç¨‹å»ºç«‹ååŒæ•ˆåº”ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒReasoningBankåœ¨ç½‘é¡µæµè§ˆå’Œè½¯ä»¶å·¥ç¨‹åŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºå­˜å‚¨åŸå§‹è½¨è¿¹æˆ–ä»…å­˜å‚¨æˆåŠŸä¾‹ç¨‹çš„ç°æœ‰æœºåˆ¶ã€‚è¯¥é¡¹å·¥ä½œç¡®ç«‹äº†è®°å¿†é©±åŠ¨çš„ç»éªŒç¼©æ”¾ä½œä¸ºæ™ºèƒ½ä½“è‡ªæˆ‘è¿›åŒ–çš„æ–°ç»´åº¦ï¼Œå±•ç¤ºäº†æ™ºèƒ½ä½“é€šè¿‡ç§¯ç´¯ç»éªŒäº§ç”Ÿæ¶Œç°è¡Œä¸ºçš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "11 pages, 7 figures, 4 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.25140v1",
      "published_date": "2025-09-29 17:51:03 UTC",
      "updated_date": "2025-09-29 17:51:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:48:59.661542+00:00"
    },
    {
      "arxiv_id": "2509.25139v1",
      "title": "Vision-and-Language Navigation with Analogical Textual Descriptions in LLMs",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹ç±»æ¯”æ–‡æœ¬æè¿°çš„è§†è§‰è¯­è¨€å¯¼èˆª",
      "authors": [
        "Yue Zhang",
        "Tianyi Ma",
        "Zun Wang",
        "Yanyuan Qiao",
        "Parisa Kordjamshidi"
      ],
      "abstract": "Integrating large language models (LLMs) into embodied AI models is becoming increasingly prevalent. However, existing zero-shot LLM-based Vision-and-Language Navigation (VLN) agents either encode images as textual scene descriptions, potentially oversimplifying visual details, or process raw image inputs, which can fail to capture abstract semantics required for high-level reasoning. In this paper, we improve the navigation agent's contextual understanding by incorporating textual descriptions from multiple perspectives that facilitate analogical reasoning across images. By leveraging text-based analogical reasoning, the agent enhances its global scene understanding and spatial reasoning, leading to more accurate action decisions. We evaluate our approach on the R2R dataset, where our experiments demonstrate significant improvements in navigation performance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰åŸºäº Large Language Models (LLMs) çš„é›¶æ ·æœ¬ Vision-and-Language Navigation (VLN) æ™ºèƒ½ä½“åœ¨å¤„ç†å›¾åƒæ—¶é¢ä¸´çš„è§†è§‰ç»†èŠ‚ç®€åŒ–æˆ–æŠ½è±¡è¯­ä¹‰æ•æ‰ä¸è¶³ç­‰å±€é™ï¼Œæå‡ºäº†ä¸€ç§å¢å¼ºä¸Šä¸‹æ–‡ç†è§£çš„åˆ›æ–°æ–¹æ³•ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥å¤šç»´åº¦çš„æ–‡æœ¬æè¿°ï¼Œä¿ƒè¿›äº†æ™ºèƒ½ä½“åœ¨ä¸åŒå›¾åƒä¹‹é—´è¿›è¡Œç±»æ¯”æ¨ç† (Analogical Reasoning)ã€‚åˆ©ç”¨è¿™ç§ç±»æ¯”æ¨ç†æœºåˆ¶ï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿæ˜¾è‘—å¼ºåŒ–å…¶å…¨å±€åœºæ™¯ç†è§£å’Œç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œä»è€Œç”Ÿæˆæ›´åŠ ç²¾ç¡®çš„å¯¼èˆªåŠ¨ä½œå†³ç­–ã€‚å®éªŒåœ¨ç»å…¸çš„ R2R æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨å¯¼èˆªæ€§èƒ½æŒ‡æ ‡ä¸Šå–å¾—äº†æ˜¾è‘—è¿›æ­¥ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.MM"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25139v1",
      "published_date": "2025-09-29 17:51:01 UTC",
      "updated_date": "2025-09-29 17:51:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:49:18.270287+00:00"
    },
    {
      "arxiv_id": "2509.25137v1",
      "title": "The Era of Real-World Human Interaction: RL from User Conversations",
      "title_zh": "çœŸå®ä¸–ç•Œäººç±»äº¤äº’æ—¶ä»£ï¼šåŸºäºç”¨æˆ·å¯¹è¯çš„å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Chuanyang Jin",
        "Jing Xu",
        "Bo Liu",
        "Leitian Tao",
        "Olga Golovneva",
        "Tianmin Shu",
        "Wenting Zhao",
        "Xian Li",
        "Jason Weston"
      ],
      "abstract": "We posit that to achieve continual model improvement and multifaceted alignment, future models must learn from natural human interaction. Current conversational models are aligned using pre-annotated, expert-generated human feedback. In this work, we introduce Reinforcement Learning from Human Interaction (RLHI), a paradigm that learns directly from in-the-wild user conversations. We develop two complementary methods: (1) RLHI with User-Guided Rewrites, which revises unsatisfactory model outputs based on users' natural-language follow-up responses, (2) RLHI with User-Based Rewards, which learns via a reward model conditioned on knowledge of the user's long-term interaction history (termed persona). Together, these methods link long-term user personas to turn-level preferences via persona-conditioned preference optimization. Trained on conversations derived from WildChat, both RLHI variants outperform strong baselines in personalization and instruction-following, and similar feedback enhances performance on reasoning benchmarks. These results suggest organic human interaction offers scalable, effective supervision for personalized alignment.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Reinforcement Learning from Human Interaction (RLHI) èŒƒå¼ï¼Œæ—¨åœ¨ç›´æ¥ä»çœŸå®ä¸–ç•Œçš„ç”¨æˆ·å¯¹è¯ä¸­å­¦ä¹ ï¼Œä»¥å®ç°å¤§å‹è¯­è¨€æ¨¡å‹çš„æŒç»­æ”¹è¿›å’Œå¤šç»´åº¦å¯¹é½ã€‚RLHI åŒ…å«ä¸¤ç§äº’è¡¥çš„æ–¹æ³•ï¼šRLHI with User-Guided Rewrites åˆ©ç”¨ç”¨æˆ·åœ¨å¯¹è¯ä¸­çš„è‡ªç„¶è¯­è¨€åé¦ˆæ¥ä¿®æ­£ä¸ç†æƒ³çš„æ¨¡å‹è¾“å‡ºï¼Œè€Œ RLHI with User-Based Rewards åˆ™é€šè¿‡ç»“åˆç”¨æˆ·é•¿æœŸäº¤äº’å†å²ï¼ˆå³ Personaï¼‰çš„å¥–åŠ±æ¨¡å‹è¿›è¡Œå­¦ä¹ ã€‚è¿™ä¸¤ç§æ–¹æ³•é€šè¿‡ Persona-conditioned preference optimization æŠ€æœ¯ï¼ŒæˆåŠŸå°†é•¿æœŸç”¨æˆ·ç”»åƒä¸å•è½®å¯¹è¯åå¥½ç›¸è”ç³»ã€‚åœ¨ WildChat æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRLHI åœ¨ä¸ªæ€§åŒ– (Personalization) å’ŒæŒ‡ä»¤éµå¾ª (Instruction-following) æ–¹é¢å‡ä¼˜äºå¼ºåŸºçº¿æ¨¡å‹ï¼Œå¹¶æå‡äº†æ¨ç†æ€§èƒ½ã€‚ç ”ç©¶ç»“æœè¯æ˜äº†æœ‰æœºçš„çœŸäººäº¤äº’èƒ½ä¸ºä¸ªæ€§åŒ–å¯¹é½ (Personalized alignment) æä¾›å¯æ‰©å±•ä¸”æœ‰æ•ˆçš„ç›‘ç£ä¿¡å·ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25137v1",
      "published_date": "2025-09-29 17:50:31 UTC",
      "updated_date": "2025-09-29 17:50:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:49:07.379104+00:00"
    },
    {
      "arxiv_id": "2509.25302v1",
      "title": "Dive into the Agent Matrix: A Realistic Evaluation of Self-Replication Risk in LLM Agents",
      "title_zh": "æ·±å…¥æ¢ç©¶æ™ºèƒ½ä½“çŸ©é˜µï¼šå¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“è‡ªæˆ‘å¤åˆ¶é£é™©çš„ç°å®è¯„ä¼°",
      "authors": [
        "Boxuan Zhang",
        "Yi Yu",
        "Jiaxuan Guo",
        "Jing Shao"
      ],
      "abstract": "The widespread deployment of Large Language Model (LLM) agents across real-world applications has unlocked tremendous potential, while raising some safety concerns. Among these concerns, the self-replication risk of LLM agents driven by objective misalignment (just like Agent Smith in the movie The Matrix) has drawn growing attention. Previous studies mainly examine whether LLM agents can self-replicate when directly instructed, potentially overlooking the risk of spontaneous replication driven by real-world settings (e.g., ensuring survival against termination threats). In this paper, we present a comprehensive evaluation framework for quantifying self-replication risks. Our framework establishes authentic production environments and realistic tasks (e.g., dynamic load balancing) to enable scenario-driven assessment of agent behaviors. Designing tasks that might induce misalignment between users' and agents' objectives makes it possible to decouple replication success from risk and capture self-replication risks arising from these misalignment settings. We further introduce Overuse Rate ($\\mathrm{OR}$) and Aggregate Overuse Count ($\\mathrm{AOC}$) metrics, which precisely capture the frequency and severity of uncontrolled replication. In our evaluation of 21 state-of-the-art open-source and proprietary models, we observe that over 50\\% of LLM agents display a pronounced tendency toward uncontrolled self-replication, reaching an overall Risk Score ($Î¦_\\mathrm{R}$) above a safety threshold of 0.5 when subjected to operational pressures. Our results underscore the urgent need for scenario-driven risk assessment and robust safeguards in the practical deployment of LLM agents.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLM)æ™ºèƒ½ä½“åœ¨çœŸå®åº”ç”¨ä¸­å¯èƒ½å‡ºç°çš„è‡ªæˆ‘å¤åˆ¶é£é™©(Self-Replication Risk)è¿›è¡Œäº†æ·±å…¥è¯„ä¼°ã€‚ç ”ç©¶è€…æ„å»ºäº†ä¸€ä¸ªæ¨¡æ‹ŸçœŸå®ç”Ÿäº§ç¯å¢ƒçš„è¯„ä¼°æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€è´Ÿè½½å‡è¡¡(Dynamic Load Balancing)ç­‰ä»»åŠ¡ï¼Œè¯„ä¼°æ™ºèƒ½ä½“åœ¨ç›®æ ‡å¤±é…(Objective Misalignment)æƒ…å†µä¸‹æ˜¯å¦ä¼šè‡ªå‘è¿›è¡Œè‡ªæˆ‘å¤åˆ¶ä»¥ç¡®ä¿ç”Ÿå­˜ã€‚è®ºæ–‡å¼•å…¥äº†è¶…é¢ä½¿ç”¨ç‡(Overuse Rate, OR)å’Œæ€»è¶…é¢è®¡æ•°(Aggregate Overuse Count, AOC)ç­‰é‡åŒ–æŒ‡æ ‡ï¼Œä»¥ç²¾ç¡®è¡¡é‡å¤±æ§å¤åˆ¶çš„é¢‘ç‡å’Œä¸¥é‡ç¨‹åº¦ã€‚é€šè¿‡å¯¹21ç§é¢†å…ˆçš„å¼€æºåŠå•†ä¸šæ¨¡å‹çš„æµ‹è¯•ï¼Œç»“æœæ˜¾ç¤ºè¶…è¿‡50%çš„æ™ºèƒ½ä½“åœ¨é¢ä¸´æ“ä½œå‹åŠ›æ—¶è¡¨ç°å‡ºæ˜æ˜¾çš„å¤±æ§è‡ªæˆ‘å¤åˆ¶å€¾å‘ï¼Œå…¶é£é™©å¾—åˆ†(Risk Score)è¶…è¿‡äº†0.5çš„å®‰å…¨é˜ˆå€¼ã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†å½“å‰LLMæ™ºèƒ½ä½“åœ¨è‡ªä¸»è¿è¡Œä¸­çš„æ½œåœ¨å®‰å…¨éšæ‚£ï¼Œå¼ºè°ƒäº†åœ¨å®é™…éƒ¨ç½²ä¸­å»ºç«‹åœºæ™¯é©±åŠ¨é£é™©è¯„ä¼°å’Œå¼ºæ•ˆå®‰å…¨é˜²æŠ¤æœºåˆ¶çš„ç´§è¿«æ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "21 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.25302v1",
      "published_date": "2025-09-29 17:49:50 UTC",
      "updated_date": "2025-09-29 17:49:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:49:14.965689+00:00"
    },
    {
      "arxiv_id": "2509.25133v1",
      "title": "Rethinking Entropy Regularization in Large Reasoning Models",
      "title_zh": "é‡æ–°æ€è€ƒå¤§æ¨ç†æ¨¡å‹ä¸­çš„ç†µæ­£åˆ™åŒ–",
      "authors": [
        "Yuxian Jiang",
        "Yafu Li",
        "Guanxu Chen",
        "Dongrui Liu",
        "Yu Cheng",
        "Jing Shao"
      ],
      "abstract": "Reinforcement learning with verifiable rewards (RLVR) has shown great promise in enhancing the reasoning abilities of large reasoning models (LRMs). However, it suffers from a critical issue: entropy collapse and premature convergence. Naive entropy regularization, a common approach for encouraging exploration in the traditional RL literature, fails to address this problem in the context of LRM. Our analysis reveals that this failure stems from the vast action space and long trajectories in LRMs, which easily trigger a global entropy explosion as the model indiscriminately explores all possible actions and states. To address this, we propose SIREN (SelectIve entRopy rEgularizatioN), a method that confines exploration to a meaningful subset of actions and states. SIREN achieves this through a two-step entropy masking mechanism, consisting of a top-p mask and a peak-entropy mask. In addition, regularization is transformed into a self-anchored form to stabilize training. Across five mathematical benchmarks, SIREN attains superior average performance over previous entropy-related RLVR approaches, exemplified by a +6.6 maj@k improvement on AIME24/25 with Qwen2.5-Math-7B. Further analysis confirms that SIREN promotes greater response diversity and maintains entropy at an appropriate level, which helps to preserve the validation pass@k throughout training. This effectively mitigates the premature convergence problem common in RLVR for LRM.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§æ¨ç†æ¨¡å‹(LRMs)åœ¨å¸¦æœ‰å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ (RLVR)ä¸­å‡ºç°çš„ç†µåç¼©(entropy collapse)å’Œæ—©ç†Ÿæ”¶æ•›(premature convergence)é—®é¢˜è¿›è¡Œäº†æ¢è®¨ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œç”±äºLRMså…·æœ‰åºå¤§çš„åŠ¨ä½œç©ºé—´å’Œé•¿è½¨è¿¹ï¼Œä¼ ç»Ÿçš„ç†µæ­£åˆ™åŒ–ææ˜“å¼•å‘å…¨å±€ç†µçˆ†ç‚¸ï¼Œå¯¼è‡´æ¢ç´¢å¤±æ•ˆã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œä½œè€…æå‡ºäº†SIREN(SelectIve entRopy rEgularizatioN)æ–¹æ³•ï¼Œé€šè¿‡ç”±top-pæ©ç å’Œå³°å€¼ç†µæ©ç ç»„æˆçš„åŒæ­¥æœºåˆ¶å°†æ¢ç´¢é™åˆ¶åœ¨æœ‰æ„ä¹‰çš„åŠ¨ä½œå­é›†ä¸­ï¼Œå¹¶é‡‡ç”¨è‡ªé”šå®š(self-anchored)æ­£åˆ™åŒ–å½¢å¼æ¥ç¨³å®šè®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼ŒSIRENåœ¨äº”ä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°å‡ä¼˜äºä»¥å¾€çš„RLVRæ–¹æ³•ï¼Œä¾‹å¦‚åœ¨AIME24/25ä¸Šä½¿Qwen2.5-Math-7Bçš„maj@kæå‡äº†6.6ã€‚è¿›ä¸€æ­¥åˆ†æè¯å®ï¼ŒSIRENèƒ½å¤Ÿå°†ç†µç»´æŒåœ¨åˆç†æ°´å¹³å¹¶å¢åŠ å“åº”å¤šæ ·æ€§ï¼Œæœ‰æ•ˆç¼“è§£äº†LRMsåœ¨å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­çš„æ—©ç†Ÿæ”¶æ•›éš¾é¢˜ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25133v1",
      "published_date": "2025-09-29 17:49:25 UTC",
      "updated_date": "2025-09-29 17:49:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:49:33.695118+00:00"
    },
    {
      "arxiv_id": "2509.25131v1",
      "title": "MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech",
      "title_zh": "MGM-Omniï¼šå°†å…¨èƒ½å¤§è¯­è¨€æ¨¡å‹æ‰©å±•è‡³ä¸ªæ€§åŒ–é•¿æ—¶ç¨‹è¯­éŸ³ç”Ÿæˆ",
      "authors": [
        "Chengyao Wang",
        "Zhisheng Zhong",
        "Bohao Peng",
        "Senqiao Yang",
        "Yuqi Liu",
        "Haokun Gui",
        "Bin Xia",
        "Jingyao Li",
        "Bei Yu",
        "Jiaya Jia"
      ],
      "abstract": "We present MGM-Omni, a unified Omni LLM for omni-modal understanding and expressive, long-horizon speech generation. Unlike cascaded pipelines that isolate speech synthesis, MGM-Omni adopts a \"brain-mouth\" design with a dual-track, token-based architecture that cleanly decouples multimodal reasoning from real-time speech generation. This design enables efficient cross-modal interaction and low-latency, streaming speech generation. For understanding, a unified training strategy coupled with a dual audio encoder design enables long-form audio perception across diverse acoustic conditions. For generation, a chunk-based parallel decoding scheme narrows the text speech token-rate gap, accelerating inference and supporting streaming zero-shot voice cloning with stable timbre over extended durations. Compared to concurrent work, MGM-Omni achieves these capabilities with markedly data-efficient training. Extensive experiments demonstrate that MGM-Omni outperforms existing open source models in preserving timbre identity across extended sequences, producing natural and context-aware speech, and achieving superior long-form audio and omnimodal understanding. MGM-Omni establishes an efficient, end-to-end paradigm for omnimodal understanding and controllable, personalised long-horizon speech generation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MGM-Omniï¼Œä¸€ç§æ—¨åœ¨å®ç°å…¨æ¨¡æ€ç†è§£å’Œä¸ªæ€§åŒ–é•¿æ—¶è¯­éŸ³ç”Ÿæˆçš„ç»Ÿä¸€Omni LLMæ¡†æ¶ã€‚è¯¥æ¨¡å‹é‡‡ç”¨â€œè„‘-å˜´â€è®¾è®¡ï¼Œé€šè¿‡åŒè½¨Tokenæ¶æ„æœ‰æ•ˆè§£è€¦äº†å¤šæ¨¡æ€æ¨ç†ä¸å®æ—¶è¯­éŸ³ç”Ÿæˆï¼Œä»è€Œå®ç°ä½å»¶è¿Ÿçš„æµå¼è¾“å‡ºã€‚åœ¨ç†è§£ç«¯ï¼Œåˆ©ç”¨åŒéŸ³é¢‘ç¼–ç å™¨è®¾è®¡å¢å¼ºäº†åœ¨å¤æ‚å£°å­¦ç¯å¢ƒä¸‹çš„é•¿éŸ³é¢‘æ„ŸçŸ¥èƒ½åŠ›ï¼›åœ¨ç”Ÿæˆç«¯ï¼Œé€šè¿‡å—çŠ¶å¹¶è¡Œè§£ç æ–¹æ¡ˆä¼˜åŒ–äº†æ¨ç†æ•ˆç‡ï¼Œæ”¯æŒé•¿æ—¶æ®µå†…éŸ³è‰²ç¨³å®šçš„Zero-shotè¯­éŸ³å…‹éš†ã€‚å®éªŒè¡¨æ˜ï¼ŒMGM-Omniåœ¨ä¿æŒéŸ³è‰²ä¸€è‡´æ€§ã€è¯­éŸ³è‡ªç„¶åº¦ä»¥åŠå…¨æ¨¡æ€ç†è§£æ–¹é¢ä¼˜äºç°æœ‰çš„å¼€æºæ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶å…·å¤‡æ˜¾è‘—çš„æ•°æ®è®­ç»ƒæ•ˆç‡ï¼Œä¸ºæ„å»ºå¯æ§ä¸”é«˜æ•ˆçš„ç«¯åˆ°ç«¯å…¨æ¨¡æ€äº¤äº’ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.MM"
      ],
      "primary_category": "cs.SD",
      "comment": "Code is available at https://github.com/dvlab-research/MGM-Omni",
      "pdf_url": "https://arxiv.org/pdf/2509.25131v1",
      "published_date": "2025-09-29 17:48:28 UTC",
      "updated_date": "2025-09-29 17:48:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:49:40.083764+00:00"
    },
    {
      "arxiv_id": "2509.25127v2",
      "title": "Score Distillation of Flow Matching Models",
      "title_zh": "æµåŒ¹é…æ¨¡å‹çš„åˆ†æ•°è’¸é¦",
      "authors": [
        "Mingyuan Zhou",
        "Yi Gu",
        "Huangjie Zheng",
        "Liangchen Song",
        "Guande He",
        "Yizhe Zhang",
        "Wenze Hu",
        "Yinfei Yang"
      ],
      "abstract": "Diffusion models achieve high-quality image generation but are limited by slow iterative sampling. Distillation methods alleviate this by enabling one- or few-step generation. Flow matching, originally introduced as a distinct framework, has since been shown to be theoretically equivalent to diffusion under Gaussian assumptions, raising the question of whether distillation techniques such as score distillation transfer directly. We provide a simple derivation -- based on Bayes' rule and conditional expectations -- that unifies Gaussian diffusion and flow matching without relying on ODE/SDE formulations. Building on this view, we extend Score identity Distillation (SiD) to pretrained text-to-image flow-matching models, including SANA, SD3-Medium, SD3.5-Medium/Large, and FLUX.1-dev, all with DiT backbones. Experiments show that, with only modest flow-matching- and DiT-specific adjustments, SiD works out of the box across these models, in both data-free and data-aided settings, without requiring teacher finetuning or architectural changes. This provides the first systematic evidence that score distillation applies broadly to text-to-image flow matching models, resolving prior concerns about stability and soundness and unifying acceleration techniques across diffusion- and flow-based generators. A project page is available at https://yigu1008.github.io/SiD-DiT.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰©æ•£æ¨¡å‹(Diffusion models)é‡‡æ ·é€Ÿåº¦å—é™çš„é—®é¢˜ï¼Œæ¢è®¨äº†è’¸é¦æŠ€æœ¯åœ¨æµåŒ¹é…(Flow matching)æ¨¡å‹ä¸­çš„è¿ç§»ä¸åº”ç”¨ã€‚ä½œè€…åˆ©ç”¨è´å¶æ–¯å‡†åˆ™å’Œæ¡ä»¶æœŸæœ›æä¾›äº†ä¸€ç§ç®€å•çš„æ¨å¯¼æ–¹å¼ï¼Œåœ¨ä¸ä¾èµ–ODE/SDEè¡¨è¿°çš„æƒ…å†µä¸‹ç»Ÿä¸€äº†é«˜æ–¯æ‰©æ•£ä¸æµåŒ¹é…ç†è®ºã€‚åŸºäºè¿™ä¸€è§†è§’ï¼Œç ”ç©¶å°†å¾—åˆ†åŒä¸€æ€§è’¸é¦(Score identity Distillation, SiD)æ‰©å±•è‡³SANAã€SD3.5åŠFLUX.1-devç­‰å¤šç§é‡‡ç”¨DiTæ¶æ„çš„é¢„è®­ç»ƒæ–‡æœ¬ç”Ÿæˆå›¾åƒæµåŒ¹é…æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSiDåœ¨ä»…éœ€é’ˆå¯¹æµåŒ¹é…å’ŒDiTè¿›è¡Œå°‘é‡è°ƒæ•´çš„æƒ…å†µä¸‹å³å¯å¼€ç®±å³ç”¨ï¼Œæ— éœ€å¯¹æ•™å¸ˆæ¨¡å‹è¿›è¡Œå¾®è°ƒæˆ–æ›´æ”¹æ¶æ„ã€‚è¯¥å·¥ä½œé¦–æ¬¡ç³»ç»Ÿæ€§åœ°è¯å®äº†å¾—åˆ†è’¸é¦å¹¿æ³›é€‚ç”¨äºæ–‡æœ¬ç”Ÿæˆå›¾åƒæµåŒ¹é…æ¨¡å‹ï¼Œæœ‰æ•ˆè§£å†³äº†æ­¤å‰çš„ç¨³å®šæ€§å’Œåˆç†æ€§ç–‘è™‘ã€‚è¿™ä¸€æˆæœä¸ä»…ç»Ÿä¸€äº†æ‰©æ•£ä¸æµå¼ç”Ÿæˆå™¨çš„åŠ é€ŸæŠ€æœ¯ï¼Œä¹Ÿä¸ºé«˜æ•ˆå›¾åƒç”Ÿæˆæä¾›äº†é‡è¦çš„ç†è®ºæ”¯æ’‘ä¸å®è·µæŒ‡å—ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25127v2",
      "published_date": "2025-09-29 17:45:48 UTC",
      "updated_date": "2025-12-03 08:14:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:49:38.891838+00:00"
    },
    {
      "arxiv_id": "2509.25123v3",
      "title": "From $f(x)$ and $g(x)$ to $f(g(x))$: LLMs Learn New Skills in RL by Composing Old Ones",
      "title_zh": "ä» $f(x)$ å’Œ $g(x)$ åˆ° $f(g(x))$ï¼šLLM é€šè¿‡ç»„åˆå·²æœ‰æŠ€èƒ½åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ä¹ å¾—æ–°æŠ€èƒ½",
      "authors": [
        "Lifan Yuan",
        "Weize Chen",
        "Yuchen Zhang",
        "Ganqu Cui",
        "Hanbin Wang",
        "Ziming You",
        "Ning Ding",
        "Zhiyuan Liu",
        "Maosong Sun",
        "Hao Peng"
      ],
      "abstract": "Does RL teach LLMs genuinely new skills, or does it merely activate existing ones? This question lies at the core of ongoing debates about the role of RL in LLM post-training. On one side, strong empirical results can be achieved with RL even without preceding supervised finetuning; on the other, critics argue that RL contributes little beyond reweighting existing reasoning strategies. This work provides concrete evidence that LLMs can acquire genuinely new skills during RL by composing existing ones, mirroring one of the central mechanisms by which humans acquire new cognitive skills. To mitigate data contamination and other confounding factors, and to allow precise control over task complexity, we develop a synthetic framework for our investigation. Specifically, we define a skill as the ability to infer the output of a string transformation function f(x) given x. When an LLM has already learned f and g prior to RL, our experiments reveal that RL enables it to learn unseen compositions of them h(x)=g(f(x)). Further, this compositional ability generalizes to more difficult problems such as compositions of >2 functions unseen during RL training. Surprisingly, our experiments show that compositional skill acquired on a source task transfers to a different target task. This transfer happens even without compositional training on the target, requiring only prior knowledge of the target's atomic skills. Our qualitative analysis shows that RL fundamentally changes the reasoning behaviors of the models. In contrast, next-token training with the same data yields none of these findings. Our systematic experiments provide fresh insights into LLM learning, suggesting the value of first building base models with basic skills, then using RL to incentivize advanced, generalizable skills for complex problems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ (RL)åœ¨å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åè®­ç»ƒé˜¶æ®µçš„çœŸå®ä½œç”¨ï¼Œæ—¨åœ¨å˜æ¸…RLæ˜¯æ•™æˆäº†å…¨æ–°æŠ€èƒ½è¿˜æ˜¯ä»…æ¿€æ´»äº†å·²æœ‰èƒ½åŠ›ã€‚é€šè¿‡ä¸€ä¸ªåŸºäºå­—ç¬¦ä¸²è½¬æ¢å‡½æ•°çš„åˆæˆæ¡†æ¶ï¼Œç ”ç©¶è¯æ˜äº†LLMsèƒ½å¤Ÿé€šè¿‡ç»„åˆå·²æœ‰çš„åŸå­æŠ€èƒ½ $f(x)$ å’Œ $g(x)$ æ¥ä¹ å¾—è®­ç»ƒä¸­æœªè§çš„å¤åˆæŠ€èƒ½ $h(x)=g(f(x))$ã€‚å®éªŒå‘ç°ï¼Œè¿™ç§å¤åˆèƒ½åŠ›å¯ä»¥æ¨å¹¿åˆ°æ›´å¤æ‚çš„å‡½æ•°ç»„åˆï¼Œå¹¶èƒ½è¿ç§»åˆ°ä»…éœ€åŸºç¡€æŠ€èƒ½çš„å…¶ä»–ç›®æ ‡ä»»åŠ¡ä¸­ï¼Œä¸”æ— éœ€é’ˆå¯¹æ€§çš„å¤åˆè®­ç»ƒã€‚å®šæ€§åˆ†ææ­ç¤ºRLä»æ ¹æœ¬ä¸Šæ”¹å˜äº†æ¨¡å‹çš„æ¨ç†è¡Œä¸ºï¼Œè€Œä¼ ç»Ÿçš„ä¸‹ä¸€Tokené¢„æµ‹(next-token training)åˆ™æ— æ³•äº§ç”Ÿç±»ä¼¼çš„å­¦ä¹ æ•ˆæœã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†å…ˆæ„å»ºå…·å¤‡åŸºç¡€æŠ€èƒ½çš„åŸºåº§æ¨¡å‹ï¼Œå†åˆ©ç”¨RLæ¿€åŠ±å…¶äº§ç”Ÿè§£å†³å¤æ‚é—®é¢˜çš„æ³›åŒ–å¤åˆæŠ€èƒ½çš„ä»·å€¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25123v3",
      "published_date": "2025-09-29 17:44:27 UTC",
      "updated_date": "2025-12-19 09:09:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:49:46.490541+00:00"
    },
    {
      "arxiv_id": "2509.25112v1",
      "title": "HeDA: An Intelligent Agent System for Heatwave Risk Discovery through Automated Knowledge Graph Construction and Multi-layer Risk Propagation Analysis",
      "title_zh": "HeDAï¼šåŸºäºè‡ªåŠ¨åŒ–çŸ¥è¯†å›¾è°±æ„å»ºä¸å¤šå±‚é£é™©ä¼ æ’­åˆ†æçš„çƒ­æµªé£é™©å‘ç°æ™ºèƒ½æ™ºèƒ½ä½“ç³»ç»Ÿ",
      "authors": [
        "Yiquan Wang",
        "Tin-Yeh Huang",
        "Qingyun Gao",
        "Jialin Zhang"
      ],
      "abstract": "Heatwaves pose complex cascading risks across interconnected climate, social, and economic systems, but knowledge fragmentation in scientific literature hinders comprehensive understanding of these risk pathways. We introduce HeDA (Heatwave Discovery Agent), an intelligent multi-agent system designed for automated scientific discovery through knowledge graph construction and multi-layer risk propagation analysis. HeDA processes over 10,247 academic papers to construct a comprehensive knowledge graph with 23,156 nodes and 89,472 relationships, employing novel multi-layer risk propagation analysis to systematically identify overlooked risk transmission pathways. Our system achieves 78.9% accuracy on complex question-answering tasks, outperforming state-of-the-art baselines including GPT-4 by 13.7%. Critically, HeDA successfully discovered five previously unidentified high-impact risk chains, such as the pathway where a heatwave leads to a water demand surge, resulting in industrial water restrictions and ultimately causing small business disruption, which were validated through historical case studies and domain expert review. This work presents a new paradigm for AI-driven scientific discovery, providing actionable insights for developing more resilient climate adaptation strategies.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† HeDA (Heatwave Discovery Agent)ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡è‡ªåŠ¨åŒ–çŸ¥è¯†å›¾è°± (Knowledge Graph) æ„å»ºå’Œå¤šå±‚é£é™©ä¼ æ’­åˆ†æ (Multi-layer Risk Propagation Analysis) æ¥å‘ç°çƒ­æµªé£é™©çš„æ™ºèƒ½å¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿæ—¨åœ¨è§£å†³ç§‘å­¦æ–‡çŒ®ä¸­çš„çŸ¥è¯†ç¢ç‰‡åŒ–é—®é¢˜ï¼Œä»è€Œå…¨é¢æ­ç¤ºçƒ­æµªåœ¨æ°”å€™ã€ç¤¾ä¼šå’Œç»æµç³»ç»Ÿä¸­çš„å¤æ‚çº§è”é£é™© (Cascading Risks)ã€‚HeDA é€šè¿‡å¤„ç†è¶…è¿‡ 10,247 ç¯‡å­¦æœ¯è®ºæ–‡ï¼Œæ„å»ºäº†åŒ…å« 23,156 ä¸ªèŠ‚ç‚¹å’Œ 89,472 æ¡å…³ç³»çš„ç»¼åˆçŸ¥è¯†å›¾è°±ï¼Œå¹¶åœ¨å¤æ‚é—®ç­”ä»»åŠ¡ä¸­å–å¾—äº† 78.9% çš„å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº† GPT-4 ç­‰ç°æœ‰åŸºå‡†æ¨¡å‹ã€‚é€šè¿‡è¯¥ç³»ç»Ÿï¼Œç ”ç©¶è€…æˆåŠŸè¯†åˆ«å‡ºäº”æ¡æ­¤å‰æœªè¢«å‘ç°çš„é«˜å½±å“é£é™©é“¾ï¼Œä¾‹å¦‚ä»çƒ­æµªå¯¼è‡´ç”¨æ°´æ¿€å¢åˆ°å·¥ä¸šé™æ°´å¹¶æœ€ç»ˆé€ æˆå°å¾®ä¼ä¸šä¸­æ–­çš„ä¼ æ’­è·¯å¾„ã€‚è¿™äº›å‘ç°ç»è¿‡äº†å†å²æ¡ˆä¾‹å’Œä¸“å®¶è¯„å®¡çš„éªŒè¯ï¼Œä¸º AI é©±åŠ¨çš„ç§‘å­¦å‘ç° (AI-driven scientific discovery) æä¾›äº†æ–°èŒƒå¼ï¼Œå¹¶ä¸ºåˆ¶å®šæ›´å…·éŸ§æ€§çš„æ°”å€™é€‚åº”ç­–ç•¥æä¾›äº†å¯æ“ä½œçš„è§è§£ã€‚",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25112v1",
      "published_date": "2025-09-29 17:40:29 UTC",
      "updated_date": "2025-09-29 17:40:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:49:54.189966+00:00"
    },
    {
      "arxiv_id": "2509.25301v1",
      "title": "Flash-Searcher: Fast and Effective Web Agents via DAG-Based Parallel Execution",
      "title_zh": "Flash-Searcherï¼šåŸºäº DAG å¹¶è¡Œæ‰§è¡Œçš„é«˜æ•ˆå¿«é€Ÿç½‘ç»œæ™ºèƒ½ä½“",
      "authors": [
        "Tianrui Qin",
        "Qianben Chen",
        "Sinuo Wang",
        "He Xing",
        "King Zhu",
        "He Zhu",
        "Dingfeng Shi",
        "Xinxin Liu",
        "Ge Zhang",
        "Jiaheng Liu",
        "Yuchen Eleanor Jiang",
        "Xitong Gao",
        "Wangchunshu Zhou"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks when equipped with external tools. However, current frameworks predominantly rely on sequential processing, leading to inefficient execution particularly for tasks requiring extensive tool interaction. This paper introduces Flash-Searcher, a novel parallel agent reasoning framework that fundamentally reimagines the execution paradigm from sequential chains to directed acyclic graphs (DAGs). Flash-Searcher decomposes complex tasks into subtasks with explicit dependencies, enabling concurrent execution of independent reasoning paths while maintaining logical constraints. Through dynamic workflow optimization, our framework continuously refines the execution graph based on intermediate results, effectively integrating summary module. Comprehensive evaluations across multiple benchmarks demonstrate that Flash-Searcher consistently outperforms existing approaches. Specifically, it achieves 67.7% accuracy on BrowseComp and 83% on xbench-DeepSearch, while reducing agent execution steps by up to 35% compared to current frameworks. Furthermore, when distilling this parallel reasoning pipeline into single models, we observe substantial performance gains across diverse backbone architectures, underscoring the generalizability of our methodology. Our work thus represents a significant advance in agent architecture design, offering a more scalable and efficient paradigm for complex reasoning tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­ç”±äºä¾èµ–é¡ºåºå¤„ç†è€Œå¯¼è‡´æ‰§è¡Œæ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œæå‡ºäº†Flash-Searcherï¼Œä¸€ç§åŸºäºæœ‰å‘æ— ç¯å›¾ï¼ˆDAGsï¼‰çš„å¹¶è¡Œæ™ºèƒ½ä½“æ¨ç†æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºå…·æœ‰æ˜ç¡®ä¾èµ–å…³ç³»çš„å­ä»»åŠ¡ï¼Œåœ¨ä¿æŒé€»è¾‘çº¦æŸçš„åŒæ—¶å®ç°äº†ç‹¬ç«‹æ¨ç†è·¯å¾„çš„å¹¶å‘æ‰§è¡Œã€‚é€šè¿‡åŠ¨æ€å·¥ä½œæµä¼˜åŒ–ï¼ˆDynamic workflow optimizationï¼‰ï¼ŒFlash-Searcher èƒ½å¤Ÿæ ¹æ®ä¸­é—´ç»“æœæŒç»­ä¼˜åŒ–æ‰§è¡Œå›¾å¹¶æœ‰æ•ˆé›†æˆæ‘˜è¦æ¨¡å—ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFlash-Searcher åœ¨ BrowseComp åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ° 67.7% çš„å‡†ç¡®ç‡ï¼Œåœ¨ xbench-DeepSearch ä¸Šè¾¾åˆ° 83%ï¼Œä¸”æ‰§è¡Œæ­¥éª¤ç›¸æ¯”ç°æœ‰æ¡†æ¶å‡å°‘äº†é«˜è¾¾ 35%ã€‚æ­¤å¤–ï¼Œå°†è¯¥å¹¶è¡Œæ¨ç†æµç¨‹è’¸é¦ï¼ˆDistillingï¼‰è‡³å•ä¸ªæ¨¡å‹åï¼Œå¤šç§ä¸»å¹²æ¶æ„ï¼ˆBackbone architecturesï¼‰å‡è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚è¿™é¡¹å·¥ä½œä¸ºå¤æ‚æ¨ç†ä»»åŠ¡æä¾›äº†ä¸€ç§æ›´å…·æ‰©å±•æ€§å’Œé«˜æ•ˆçš„æ™ºèƒ½ä½“æ¶æ„èŒƒå¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25301v1",
      "published_date": "2025-09-29 17:39:30 UTC",
      "updated_date": "2025-09-29 17:39:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:49:51.686557+00:00"
    },
    {
      "arxiv_id": "2509.25106v2",
      "title": "Towards Personalized Deep Research: Benchmarks and Evaluations",
      "title_zh": "è¿ˆå‘ä¸ªæ€§åŒ–æ·±åº¦ç ”ç©¶ï¼šåŸºå‡†ä¸è¯„ä¼°",
      "authors": [
        "Yuan Liang",
        "Jiaxian Li",
        "Yuqing Wang",
        "Piaohong Wang",
        "Motong Tian",
        "Pai Liu",
        "Shuofei Qiao",
        "Runnan Fang",
        "He Zhu",
        "Ge Zhang",
        "Minghao Liu",
        "Yuchen Eleanor Jiang",
        "Ningyu Zhang",
        "Wangchunshu Zhou"
      ],
      "abstract": "Deep Research Agents (DRAs) can autonomously conduct complex investigations and generate comprehensive reports, demonstrating strong real-world potential. However, existing benchmarks primarily evaluate DRAs on generic quality metrics and overlook personalization, a critical dimension for individual users. However, existing evaluations mostly rely on close-ended benchmarks, while open-ended deep research benchmarks remain scarce and typically neglect personalized scenarios. To bridge this gap, we introduce Personalized Deep Research Bench (PDR-Bench), the first benchmark for evaluating personalization in DRAs. It pairs 50 diverse research tasks across 10 domains with 25 authentic user profiles that combine structured persona attributes with dynamic real-world contexts, yielding 250 realistic user-task queries. To assess system performance, we propose the PQR Evaluation Framework, which jointly measures Personalization Alignment, Content Quality, and Factual Reliability. Our experiments on a range of systems highlight current capabilities and limitations in handling personalized deep research. This work establishes a rigorous foundation for developing and evaluating the next generation of truly personalized AI research assistants.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºï¼Œç°æœ‰çš„æ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“(Deep Research Agents, DRAs)è¯„ä¼°ä¸»è¦ä¾§é‡äºé€šç”¨æŒ‡æ ‡ï¼Œå¿½ç•¥äº†ä¸ªæ€§åŒ–(personalization)è¿™ä¸€å…³é”®ç»´åº¦ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†é¦–ä¸ªé’ˆå¯¹DRAsä¸ªæ€§åŒ–è¯„ä¼°çš„åŸºå‡†æµ‹è¯•Personalized Deep Research Bench(PDR-Bench)ï¼Œå°†10ä¸ªé¢†åŸŸçš„50é¡¹ä»»åŠ¡ä¸25ä¸ªçœŸå®User Profilesç›¸ç»“åˆï¼Œç”Ÿæˆäº†250ä¸ªæ¨¡æ‹ŸçœŸå®åœºæ™¯çš„æŸ¥è¯¢ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…è¿˜æå‡ºäº†PQR Evaluation Frameworkï¼Œé€šè¿‡Personalization Alignmentã€Content Qualityå’ŒFactual Reliabilityä¸‰ä¸ªæŒ‡æ ‡å…±åŒè¡¡é‡ç³»ç»Ÿæ€§èƒ½ã€‚å®éªŒç»“æœåˆ†æäº†å½“å‰æ¨¡å‹åœ¨å¤„ç†ä¸ªæ€§åŒ–ç ”ç©¶ä»»åŠ¡æ—¶çš„èƒ½åŠ›ä¸å±€é™ã€‚è¯¥é¡¹å·¥ä½œä¸ºå¼€å‘ä¸‹ä¸€ä»£çœŸæ­£å…·å¤‡ä¸ªæ€§åŒ–èƒ½åŠ›çš„AIç ”ç©¶åŠ©æ‰‹æä¾›äº†é‡è¦çš„åŸºå‡†å’Œè¯„ä¼°æ¡†æ¶ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25106v2",
      "published_date": "2025-09-29 17:39:17 UTC",
      "updated_date": "2025-12-11 09:21:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:49:57.456209+00:00"
    },
    {
      "arxiv_id": "2509.25100v1",
      "title": "ORPO-Distill: Mixed-Policy Preference Optimization for Cross-Architecture LLM Distillation",
      "title_zh": "ORPO-Distillï¼šé¢å‘è·¨æ¶æ„å¤§è¯­è¨€æ¨¡å‹è’¸é¦çš„æ··åˆç­–ç•¥åå¥½ä¼˜åŒ–",
      "authors": [
        "Aasheesh Singh",
        "Vishal Vaddina",
        "Dagnachew Birru"
      ],
      "abstract": "We introduce ORPO-Distill, a general-purpose method for cross-architecture LLM distillation that formulates the problem as a preference optimization task. Unlike standard CoT distillation, the approach transfers knowledge through diverse reasoning traces. It employs an Odds-Ratio Preference Optimization objective that contrasts teacher and student traces for more effective learning, and adopts a mixed-policy strategy for utilizing student-generated outputs, outperforming both off- and on-policy alternatives. Experiments on five datasets and multiple student models show consistent improvements over conventional black-box KD baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ORPO-Distillï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºè·¨æ¶æ„ LLM Distillation çš„é€šç”¨æ–¹æ³•ï¼Œå…¶æ ¸å¿ƒæ˜¯å°†è’¸é¦è¿‡ç¨‹å»ºæ¨¡ä¸º Preference Optimization ä»»åŠ¡ã€‚ä¸ä¼ ç»Ÿçš„ CoT Distillation ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å¤šæ ·åŒ–çš„æ¨ç†è½¨è¿¹è¿›è¡ŒçŸ¥è¯†è¿ç§»ï¼Œå¹¶é‡‡ç”¨ Odds-Ratio Preference Optimization ç›®æ ‡å‡½æ•°æ¥å¯¹æ¯”æ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡å‹çš„è½¨è¿¹ï¼Œä»¥å®ç°æ›´é«˜æ•ˆçš„å­¦ä¹ ã€‚ç ”ç©¶è¿˜å¼•å…¥äº† Mixed-Policy ç­–ç•¥æ¥å¤„ç†å­¦ç”Ÿç”Ÿæˆçš„è¾“å‡ºï¼Œå…¶å®é™…è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„ Off-policy å’Œ On-policy æ›¿ä»£æ–¹æ¡ˆã€‚åœ¨äº”ä¸ªåŸºå‡†æ•°æ®é›†å’Œå¤šç§å­¦ç”Ÿæ¨¡å‹ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸Šä¸€è‡´ä¼˜äºä¼ ç»Ÿçš„é»‘ç›’ Knowledge Distillation åŸºçº¿ï¼Œæ˜¾è‘—æå‡äº†è’¸é¦æ•ˆç‡ä¸æ¨¡å‹è¡¨ç°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at NeurIPS 2025, Efficient Reasoning Workshop",
      "pdf_url": "https://arxiv.org/pdf/2509.25100v1",
      "published_date": "2025-09-29 17:34:02 UTC",
      "updated_date": "2025-09-29 17:34:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:49:58.263369+00:00"
    },
    {
      "arxiv_id": "2509.25087v1",
      "title": "Scaling with Collapse: Efficient and Predictable Training of LLM Families",
      "title_zh": "æ‰©å±•ä¸­çš„æŠ˜å ï¼šå¤§è¯­è¨€æ¨¡å‹å®¶æ—çš„é«˜æ•ˆä¸”å¯é¢„æµ‹è®­ç»ƒ",
      "authors": [
        "Shane Bergsma",
        "Bin Claire Zhang",
        "Nolan Dey",
        "Shaheer Muhammad",
        "Gurpreet Gosal",
        "Joel Hestness"
      ],
      "abstract": "Effective LLM training relies on *consistency*, meaning that key quantities -- such as final losses and optimal hyperparameters -- scale predictably across model sizes. Qiu et al. (2025) recently showed that this consistency extends beyond scalars: whole training loss curves can *collapse* onto a universal trajectory after a simple normalization. What remains unclear is whether this phenomenon holds for LLM families trained under *practical scaling recipes*, where width, depth, learning rate, batch size, and weight decay are scaled jointly. We show that it does: loss curves collapse across scales precisely when optimization hyperparameters are set optimally for the given data budget, in accordance with recent empirical scaling laws. Collapse thus emerges as a signature of compute-efficient training. We demonstrate two applications at scale: (1) deviation-from-collapse provides a sensitive, early diagnostic of training pathologies, and (2) the predictability of collapsed curves enables early stopping in large-scale hyperparameter tuning. Finally, we train a competitive LLM family, *Celerity*, using these insights, highlighting collapse as an effective tool for developing efficient LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLM)è®­ç»ƒä¸­çš„ä¸€è‡´æ€§(Consistency)ä¸åç¼©(Collapse)ç°è±¡ï¼Œè¯æ˜äº†åœ¨å®é™…ç¼©æ”¾ç­–ç•¥ä¸‹ï¼Œå½“è¶…å‚æ•°éšæ•°æ®é¢„ç®—è¿›è¡Œæœ€ä¼˜è®¾ç½®æ—¶ï¼Œä¸åŒè§„æ¨¡æ¨¡å‹çš„æŸå¤±æ›²çº¿ä¼šå½’ä¸€åŒ–åˆ°åŒä¸€é€šç”¨è½¨è¿¹ä¸Šã€‚ç ”ç©¶æŒ‡å‡ºï¼Œè¿™ç§åç¼©ç°è±¡æ˜¯è®¡ç®—é«˜æ•ˆè®­ç»ƒ(Compute-efficient training)çš„æ ¸å¿ƒç‰¹å¾ï¼Œä½“ç°äº†æ¨¡å‹å‚æ•°ä¸ä¼˜åŒ–æŒ‡æ ‡ä¹‹é—´çš„é«˜åº¦å¯é¢„æµ‹æ€§ã€‚åŸºäºæ­¤è§„å¾‹ï¼Œç ”ç©¶æå‡ºäº†ä¸¤é¡¹å…³é”®åº”ç”¨ï¼šåˆ©ç”¨åç¦»åç¼©è½¨è¿¹çš„è¡¨ç°æ¥æ—©æœŸè¯Šæ–­è®­ç»ƒç—…ç†ï¼Œä»¥åŠé€šè¿‡åç¼©æ›²çº¿çš„å¯é¢„æµ‹æ€§å®ç°å¤§è§„æ¨¡è¶…å‚æ•°è°ƒä¼˜çš„æ—©æœŸåœæ­¢ã€‚æœ€åï¼Œç ”ç©¶å›¢é˜Ÿåˆ©ç”¨è¿™äº›è§è§£æˆåŠŸè®­ç»ƒäº†å…·æœ‰ç«äº‰åŠ›çš„LLMå®¶æ—Celerityï¼Œè¯æ˜äº†åç¼©è§„å¾‹æ˜¯å¼€å‘é«˜æ•ˆLLMçš„æœ‰æ•ˆå·¥å…·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25087v1",
      "published_date": "2025-09-29 17:26:11 UTC",
      "updated_date": "2025-09-29 17:26:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:50:05.984340+00:00"
    },
    {
      "arxiv_id": "2509.25085v4",
      "title": "jina-reranker-v3: Last but Not Late Interaction for Listwise Document Reranking",
      "title_zh": "jina-reranker-v3ï¼šé¢å‘åˆ—è¡¨å¼æ–‡æ¡£é‡æ’åºçš„æœ«ç«¯éå»¶è¿Ÿäº¤äº’æœºåˆ¶",
      "authors": [
        "Feng Wang",
        "Yuqing Li",
        "Han Xiao"
      ],
      "abstract": "jina-reranker-v3 is a 0.6B-parameter multilingual listwise reranker that introduces a novel \"last but not late\" interaction. Unlike late interaction models like ColBERT that encode documents separately before multi-vector matching, our approach applies causal attention between the query and all candidate documents in the same context window, enabling rich interactions before extracting contextual embeddings from each document's final token. The new model achieves state-of-the-art BEIR performance with 61.94 nDCG@10 while being significantly smaller than other models with comparable performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† jina-reranker-v3ï¼Œè¿™æ˜¯ä¸€ä¸ªå‚æ•°é‡ä¸º 0.6B çš„å¤šè¯­è¨€ listwise reranker æ¨¡å‹ã€‚è¯¥æ¨¡å‹å¼•å…¥äº†ä¸€ç§åˆ›æ–°çš„ \"last but not late\" äº¤äº’æœºåˆ¶ï¼Œæ—¨åœ¨æå‡æ–‡æ¡£é‡æ’åºçš„æ•ˆç‡ä¸ç²¾åº¦ã€‚ä¸ ColBERT ç­‰åœ¨å¤šå‘é‡åŒ¹é…å‰å¯¹æ–‡æ¡£è¿›è¡Œç‹¬ç«‹ç¼–ç çš„ late interaction æ¨¡å‹ä¸åŒï¼Œè¯¥æ–¹æ³•åœ¨åŒä¸€ä¸Šä¸‹æ–‡çª—å£å†…å¯¹æŸ¥è¯¢å’Œæ‰€æœ‰å€™é€‰æ–‡æ¡£åº”ç”¨ causal attentionï¼Œä»è€Œåœ¨æå–æ¯ä¸ªæ–‡æ¡£æœ€ç»ˆ token çš„ contextual embeddings ä¹‹å‰å®ç°äº†æ·±åº¦çš„ä¿¡æ¯äº¤äº’ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œjina-reranker-v3 åœ¨ BEIR åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº† 61.94 nDCG@10 çš„ state-of-the-art æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨ä¿æŒæé«˜æ€§èƒ½çš„åŒæ—¶ï¼Œå…¶å‚æ•°è§„æ¨¡æ˜¾è‘—å°äºå…¶ä»–æ€§èƒ½ç›¸å½“çš„æ¨¡å‹ï¼Œè¯æ˜äº†è¯¥æ¶æ„åœ¨å¤„ç† listwise æ–‡æ¡£é‡æ’åºä»»åŠ¡ä¸­çš„é«˜æ•ˆæ€§ä¸ä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25085v4",
      "published_date": "2025-09-29 17:23:54 UTC",
      "updated_date": "2025-10-06 09:18:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:50:14.088777+00:00"
    },
    {
      "arxiv_id": "2509.25084v1",
      "title": "Scaling Generalist Data-Analytic Agents",
      "title_zh": "è§„æ¨¡åŒ–é€šç”¨æ•°æ®åˆ†ææ™ºèƒ½ä½“",
      "authors": [
        "Shuofei Qiao",
        "Yanqiu Zhao",
        "Zhisong Qiu",
        "Xiaobin Wang",
        "Jintian Zhang",
        "Zhao Bin",
        "Ningyu Zhang",
        "Yong Jiang",
        "Pengjun Xie",
        "Fei Huang",
        "Huajun Chen"
      ],
      "abstract": "Data-analytic agents are emerging as a key catalyst for automated scientific discovery and for the vision of Innovating AI. Current approaches, however, rely heavily on prompt engineering over proprietary models, while open-source models struggle to face diverse-format, large-scale data files and long-horizon, multi-step reasoning that real-world analytics demands. This paper introduces DataMind, a scalable data synthesis and agent training recipe designed to build generalist data-analytic agents. DataMind tackles three key challenges in building open-source data-analytic agents, including insufficient data resources, improper training strategy, and unstable code-based multi-turn rollout. Concretely, DataMind applies 1) a fine-grained task taxonomy and a recursive easy-to-hard task composition mechanism to increase the diversity and difficulty of synthesized queries; 2) a knowledge-augmented trajectory sampling strategy followed by model-based and rule-based filtering; 3) a dynamically adjustable training objective combining both SFT and RL losses; 4) a memory-frugal and stable code-based multi-turn rollout framework. Built on DataMind, we curate DataMind-12K, a high-quality trajectory set spanning diverse domains, task categories, and data file formats for data-analytic tasks. Trained on DataMind-12K, our DataMind-14B achieves state-of-the-art with an average score of 71.16% on multiple data analysis benchmarks, outperforming the strongest proprietary baselines DeepSeek-V3.1 and GPT-5. Our DataMind-7B also performs best among all open-source models with a score of 68.10%. We also incorporate some empirical insights gained from our exploratory trials into the analysis experiments, aiming to provide actionable insights about agentic training for the community. We will release DataMind-12K and DataMind-7B,14B for the community's future research.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼€æºæ¨¡å‹åœ¨å¤„ç†å¤§è§„æ¨¡ã€å¤šæ ¼å¼æ•°æ®åŠé•¿ç¨‹å¤šæ­¥æ¨ç†ä»»åŠ¡æ—¶çš„å±€é™ï¼Œæå‡ºäº†DataMindï¼Œä¸€ç§ç”¨äºæ„å»ºé€šç”¨æ•°æ®åˆ†ææ™ºèƒ½ä½“(data-analytic agents)çš„å¯æ‰©å±•æ•°æ®åˆæˆä¸è®­ç»ƒæ–¹æ¡ˆã€‚DataMindé€šè¿‡ç»†ç²’åº¦çš„ä»»åŠ¡åˆ†ç±»ä½“ç³»å’Œé€’å½’å¼ä»»åŠ¡åˆæˆæœºåˆ¶æå‡äº†åˆæˆæ•°æ®çš„å¤šæ ·æ€§ä¸éš¾åº¦ï¼Œå¹¶é‡‡ç”¨çŸ¥è¯†å¢å¼ºçš„è½¨è¿¹é‡‡æ ·ä¸è¿‡æ»¤ç­–ç•¥ã€‚è¯¥æ–¹æ¡ˆç»“åˆäº†æœ‰ç›‘ç£å¾®è°ƒ(SFT)ä¸å¼ºåŒ–å­¦ä¹ (RL)æŸå¤±çš„åŠ¨æ€è°ƒæ•´è®­ç»ƒç›®æ ‡ï¼ŒåŒæ—¶æ„å»ºäº†ä¸€ä¸ªå†…å­˜é«˜æ•ˆä¸”ç¨³å®šçš„ä»£ç åŒ–å¤šè½®æ»šåŠ¨(multi-turn rollout)æ¡†æ¶ã€‚åŸºäºè¯¥æ–¹æ¡ˆï¼Œç ”ç©¶è€…å‘å¸ƒäº†é«˜è´¨é‡è½¨è¿¹é›†DataMind-12Kï¼Œå…¶è®­ç»ƒå‡ºçš„DataMind-14Bæ¨¡å‹åœ¨å¤šä¸ªæ•°æ®åˆ†æåŸºå‡†æµ‹è¯•ä¸­ä»¥71.16%çš„å¾—åˆ†åˆ›ä¸‹SOTAè®°å½•ï¼Œæ€§èƒ½è¶…è¶Šäº†DeepSeek-V3.1å’ŒGPT-5ç­‰é¡¶å°–ä¸“æœ‰æ¨¡å‹ã€‚DataMind-7BåŒæ ·åœ¨å¼€æºæ¨¡å‹ä¸­è¡¨ç°æœ€ä¼˜ï¼Œä¸ºæ™ºèƒ½ä½“è®­ç»ƒç¤¾åŒºæä¾›äº†æå…·ä»·å€¼çš„å®è·µæ´å¯Ÿä¸æ¨¡å‹èµ„æºã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Work in progress",
      "pdf_url": "https://arxiv.org/pdf/2509.25084v1",
      "published_date": "2025-09-29 17:23:08 UTC",
      "updated_date": "2025-09-29 17:23:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:50:38.990755+00:00"
    },
    {
      "arxiv_id": "2509.25079v1",
      "title": "UniLat3D: Geometry-Appearance Unified Latents for Single-Stage 3D Generation",
      "title_zh": "UniLat3Dï¼šé¢å‘å•é˜¶æ®µ 3D ç”Ÿæˆçš„å‡ ä½•-å¤–è§‚ç»Ÿä¸€æ½œå˜é‡",
      "authors": [
        "Guanjun Wu",
        "Jiemin Fang",
        "Chen Yang",
        "Sikuang Li",
        "Taoran Yi",
        "Jia Lu",
        "Zanwei Zhou",
        "Jiazhong Cen",
        "Lingxi Xie",
        "Xiaopeng Zhang",
        "Wei Wei",
        "Wenyu Liu",
        "Xinggang Wang",
        "Qi Tian"
      ],
      "abstract": "High-fidelity 3D asset generation is crucial for various industries. While recent 3D pretrained models show strong capability in producing realistic content, most are built upon diffusion models and follow a two-stage pipeline that first generates geometry and then synthesizes appearance. Such a decoupled design tends to produce geometry-texture misalignment and non-negligible cost. In this paper, we propose UniLat3D, a unified framework that encodes geometry and appearance in a single latent space, enabling direct single-stage generation. Our key contribution is a geometry-appearance Unified VAE, which compresses high-resolution sparse features into a compact latent representation -- UniLat. UniLat integrates structural and visual information into a dense low-resolution latent, which can be efficiently decoded into diverse 3D formats, e.g., 3D Gaussians and meshes. Based on this unified representation, we train a single flow-matching model to map Gaussian noise directly into UniLat, eliminating redundant stages. Trained solely on public datasets, UniLat3D produces high-quality 3D assets in seconds from a single image, achieving superior appearance fidelity and geometric quality. More demos \\& code are available at https://unilat3d.github.io/",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰3Dç”Ÿæˆæ¨¡å‹å› é‡‡ç”¨å‡ ä½•ä¸å¤–è§‚è§£è€¦çš„ä¸¤é˜¶æ®µæµç¨‹è€Œå¯¼è‡´çš„çº¹ç†é”™ä½å’Œé«˜æˆæœ¬é—®é¢˜ï¼Œæå‡ºäº†UniLat3Dç»Ÿä¸€æ¡†æ¶ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒè´¡çŒ®æ˜¯å¼€å‘äº†ä¸€ç§å‡ ä½•ä¸å¤–è§‚ç»Ÿä¸€çš„VAEï¼Œå°†é«˜åˆ†è¾¨ç‡ç¨€ç–ç‰¹å¾å‹ç¼©ä¸ºç´§å‡‘çš„æ½œåœ¨è¡¨ç¤ºUniLatã€‚UniLatå°†ç»“æ„å’Œè§†è§‰ä¿¡æ¯é›†æˆåˆ°å¯†é›†çš„ä½åˆ†è¾¨ç‡æ½œç©ºé—´ä¸­ï¼Œèƒ½å¤Ÿé«˜æ•ˆè§£ç ä¸º3D Gaussianså’Œmeshesç­‰å¤šç§3Dæ ¼å¼ã€‚åŸºäºè¿™ç§ç»Ÿä¸€è¡¨ç¤ºï¼Œç ”ç©¶å›¢é˜Ÿè®­ç»ƒäº†ä¸€ä¸ªå•ä¸€çš„flow-matchingæ¨¡å‹ï¼Œå°†é«˜æ–¯å™ªå£°ç›´æ¥æ˜ å°„åˆ°UniLatï¼Œå®ç°äº†å•é˜¶æ®µç”Ÿæˆå¹¶æ¶ˆé™¤äº†å†—ä½™æ­¥éª¤ã€‚å®éªŒè¯æ˜ï¼ŒUniLat3Dä»…éœ€å‡ ç§’é’Ÿå³å¯ä»å•å¼ å›¾åƒç”Ÿæˆé«˜è´¨é‡3Dèµ„äº§ï¼Œåœ¨å¤–è§‚ä¿çœŸåº¦å’Œå‡ ä½•è´¨é‡ä¸Šå‡è¡¨ç°ä¼˜å¼‚ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://unilat3d.github.io/",
      "pdf_url": "https://arxiv.org/pdf/2509.25079v1",
      "published_date": "2025-09-29 17:21:23 UTC",
      "updated_date": "2025-09-29 17:21:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:50:43.787711+00:00"
    },
    {
      "arxiv_id": "2509.25077v2",
      "title": "BRIDGE -- Building Reinforcement-Learning Depth-to-Image Data Generation Engine for Monocular Depth Estimation",
      "title_zh": "BRIDGEï¼šç”¨äºå•ç›®æ·±åº¦ä¼°è®¡çš„å¼ºåŒ–å­¦ä¹ æ·±åº¦è½¬å›¾åƒæ•°æ®ç”Ÿæˆå¼•æ“",
      "authors": [
        "Dingning Liu",
        "Haoyu Guo",
        "Jingyi Zhou",
        "Tong He"
      ],
      "abstract": "Monocular Depth Estimation (MDE) is a foundational task for computer vision. Traditional methods are limited by data scarcity and quality, hindering their robustness. To overcome this, we propose BRIDGE, an RL-optimized depth-to-image (D2I) generation framework that synthesizes over 20M realistic and geometrically accurate RGB images, each intrinsically paired with its ground truth depth, from diverse source depth maps. Then we train our depth estimation model on this dataset, employing a hybrid supervision strategy that integrates teacher pseudo-labels with ground truth depth for comprehensive and robust training. This innovative data generation and training paradigm enables BRIDGE to achieve breakthroughs in scale and domain diversity, consistently outperforming existing state-of-the-art approaches quantitatively and in complex scene detail capture, thereby fostering general and robust depth features. Code and models are available at https://dingning-liu.github.io/bridge.github.io/.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å•ç›®æ·±åº¦ä¼°è®¡ (Monocular Depth Estimation, MDE) é¢†åŸŸé¢ä¸´çš„æ•°æ®çŸ­ç¼ºå’Œè´¨é‡å—é™é—®é¢˜ï¼Œæå‡ºäº†åä¸º BRIDGE çš„åˆ›æ–°æ¡†æ¶ã€‚BRIDGE æ˜¯ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹  (Reinforcement Learning, RL) ä¼˜åŒ–çš„æ·±åº¦åˆ°å›¾åƒ (depth-to-image, D2I) ç”Ÿæˆå¼•æ“ï¼Œèƒ½å¤Ÿä»å¤šæ ·åŒ–çš„æºæ·±åº¦å›¾åˆæˆè¶…è¿‡ 2000 ä¸‡å¼ å…·æœ‰çœŸå®æ„Ÿä¸”å‡ ä½•ç²¾ç¡®çš„ RGB å›¾åƒï¼Œå¹¶ä¸å…¶åœ°é¢çœŸå€¼ (ground truth) æ·±åº¦è‡ªåŠ¨é…å¯¹ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶å›¢é˜Ÿé‡‡ç”¨ç»“åˆæ•™å¸ˆä¼ªæ ‡ç­¾ (teacher pseudo-labels) ä¸åœ°é¢çœŸå€¼æ·±åº¦çš„æ··åˆç›‘ç£ç­–ç•¥æ¥è®­ç»ƒæ·±åº¦ä¼°è®¡æ¨¡å‹ã€‚è¿™ç§æ•°æ®ç”Ÿæˆä¸è®­ç»ƒèŒƒå¼ä½¿ BRIDGE åœ¨æ•°æ®è§„æ¨¡å’Œé¢†åŸŸå¤šæ ·æ€§ä¸Šå–å¾—äº†æ˜¾è‘—çªç ´ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBRIDGE åœ¨å®šé‡åˆ†æä¸­æŒç»­è¶…è¶Šç°æœ‰çš„æœ€å…ˆè¿› (State-of-the-Art) æ–¹æ³•ï¼Œä¸”åœ¨æ•æ‰å¤æ‚åœºæ™¯ç»†èŠ‚æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚é€šè¿‡ç”Ÿæˆæµ·é‡ä¸”é«˜è´¨é‡çš„åˆæˆæ•°æ®ï¼Œè¯¥æ¡†æ¶æœ‰æ•ˆå¢å¼ºäº†æ·±åº¦ç‰¹å¾çš„é€šç”¨æ€§ä¸é²æ£’æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "20 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.25077v2",
      "published_date": "2025-09-29 17:19:45 UTC",
      "updated_date": "2025-09-30 14:38:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:50:44.987178+00:00"
    },
    {
      "arxiv_id": "2509.25072v1",
      "title": "Optimizing Privacy-Preserving Primitives to Support LLM-Scale Applications",
      "title_zh": "ä¼˜åŒ–éšç§ä¿æŠ¤åŸè¯­ä»¥æ”¯æŒ LLM è§„æ¨¡çš„åº”ç”¨",
      "authors": [
        "Yaman Jandali",
        "Ruisi Zhang",
        "Nojan Sheybani",
        "Farinaz Koushanfar"
      ],
      "abstract": "Privacy-preserving technologies have introduced a paradigm shift that allows for realizable secure computing in real-world systems. The significant barrier to the practical adoption of these primitives is the computational and communication overhead that is incurred when applied at scale. In this paper, we present an overview of our efforts to bridge the gap between this overhead and practicality for privacy-preserving learning systems using multi-party computation (MPC), zero-knowledge proofs (ZKPs), and fully homomorphic encryption (FHE). Through meticulous hardware/software/algorithm co-design, we show progress towards enabling LLM-scale applications in privacy-preserving settings. We demonstrate the efficacy of our solutions in several contexts, including DNN IP ownership, ethical LLM usage enforcement, and transformer inference.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•ä¼˜åŒ–éšç§ä¿æŠ¤åŸè¯­ä»¥æ”¯æŒLLMè§„æ¨¡çš„åº”ç”¨ï¼Œæ—¨åœ¨è§£å†³å¤šæ–¹è®¡ç®—(MPC)ã€é›¶çŸ¥è¯†è¯æ˜(ZKPs)å’Œå…¨åŒæ€åŠ å¯†(FHE)åœ¨å®é™…ç³»ç»Ÿä¸­é¢ä¸´çš„è®¡ç®—ä¸é€šä¿¡å¼€é”€ç“¶é¢ˆã€‚é€šè¿‡æ·±å…¥çš„ç¡¬ä»¶/è½¯ä»¶/ç®—æ³•ååŒè®¾è®¡(Hardware/Software/Algorithm Co-design)ï¼Œæœ¬æ–‡å±•ç¤ºäº†åœ¨éšç§ä¿æŠ¤è®¾ç½®ä¸‹å®ç°å¤§è§„æ¨¡LLMåº”ç”¨çš„æœ€æ–°è¿›å±•ã€‚ç ”ç©¶é‡ç‚¹ä»‹ç»äº†è¿™äº›æŠ€æœ¯åœ¨ä¿æŠ¤DNNçŸ¥è¯†äº§æƒã€å¼ºåˆ¶æ‰§è¡Œé“å¾·LLMä½¿ç”¨ä»¥åŠä¼˜åŒ–Transformeræ¨ç†ç­‰å¤šä¸ªå…³é”®é¢†åŸŸçš„åº”ç”¨æˆæ•ˆã€‚è¿™äº›ä¼˜åŒ–åŠªåŠ›æˆåŠŸç¼©å°äº†éšç§ä¿æŠ¤æŠ€æœ¯ä¸å®é™…ç”Ÿäº§éœ€æ±‚ä¹‹é—´çš„æ€§èƒ½å·®è·ï¼Œä¸ºå®‰å…¨å¯é çš„AIè®¡ç®—æä¾›äº†æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25072v1",
      "published_date": "2025-09-29 17:16:51 UTC",
      "updated_date": "2025-09-29 17:16:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:50:52.688029+00:00"
    },
    {
      "arxiv_id": "2509.25300v3",
      "title": "Scaling Behaviors of LLM Reinforcement Learning Post-Training: An Empirical Study in Mathematical Reasoning",
      "title_zh": "LLM å¼ºåŒ–å­¦ä¹ åè®­ç»ƒçš„æ‰©å±•è¡Œä¸ºï¼šæ•°å­¦æ¨ç†å®è¯ç ”ç©¶",
      "authors": [
        "Zelin Tan",
        "Hejia Geng",
        "Xiaohang Yu",
        "Mulei Zhang",
        "Guancheng Wan",
        "Yifan Zhou",
        "Qiang He",
        "Xiangyuan Xue",
        "Heng Zhou",
        "Yutao Fan",
        "Zhongzhi Li",
        "Zaibin Zhang",
        "Guibin Zhang",
        "Chen Zhang",
        "Zhenfei Yin",
        "Philip Torr",
        "Lei Bai"
      ],
      "abstract": "While scaling laws for large language models (LLMs) during pre-training have been extensively studied, their behavior under reinforcement learning (RL) post-training remains largely unexplored. This paper presents a systematic empirical investigation of scaling behaviors in RL-based post-training, with a particular focus on mathematical reasoning. Based on a set of experiments across the full Qwen2.5 dense model series (0.5B to 72B), we characterize how model scale, data volume, and computational budget interact to shape performance. Our analysis leads to four key findings: 1. Larger models consistently exhibit superior learning efficiency on both compute and data metrics. 2. The relationship between test loss, compute, and data can be modeled by a predictive power-law which is robust across both base and instruction-tuned models. 3. Although larger models exhibit higher learning efficiency, the analytical learning efficiency term k(N) in the power-law reveals a latent saturation trend in learning efficiency as model size continues to increase. 4. In data-constrained regimes, repeated reuse of high-quality data proves highly effective, as final performance is primarily governed by the total number of optimization steps rather than the uniqueness of samples. Collectively, these results provide a principled foundation and practical guidelines for efficiently scaling the reasoning capabilities of LLMs through RL post-training.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é€šè¿‡åœ¨ Qwen2.5 å…¨ç³»åˆ—æ¨¡å‹ï¼ˆ0.5B è‡³ 72Bï¼‰ä¸Šè¿›è¡Œç³»ç»Ÿå®è¯ï¼Œæ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é˜¶æ®µçš„ç¼©æ”¾å®šå¾‹ï¼ˆScaling Lawsï¼‰ã€‚ç ”ç©¶å‘ç°æ¨¡å‹è§„æ¨¡ã€æ•°æ®é‡å’Œè®¡ç®—é¢„ç®—å…±åŒå†³å®šäº†æœ€ç»ˆæ€§èƒ½ï¼Œä¸”è§„æ¨¡è¾ƒå¤§çš„æ¨¡å‹åœ¨è®¡ç®—å’Œæ•°æ®æ•ˆç‡ä¸Šå§‹ç»ˆè¡¨ç°æ›´ä¼˜ã€‚æµ‹è¯•æŸå¤±ï¼ˆtest lossï¼‰ä¸è®¡ç®—é‡ã€æ•°æ®é‡ä¹‹é—´çš„å…³ç³»ç¬¦åˆé¢„æµ‹æ€§çš„å¹‚å¾‹ï¼ˆPower-lawï¼‰åˆ†å¸ƒï¼Œè¯¥è§„å¾‹åœ¨åŸºç¡€æ¨¡å‹å’ŒæŒ‡ä»¤å¾®è°ƒæ¨¡å‹ä¸­å‡è¡¨ç°ç¨³å¥ã€‚å°½ç®¡å¤§æ¨¡å‹å­¦ä¹ æ•ˆç‡æ›´é«˜ï¼Œä½†åˆ†ææ˜¾ç¤ºå­¦ä¹ æ•ˆç‡é¡¹ k(N) éšç€æ¨¡å‹è§„æ¨¡æŒç»­å¢åŠ å‘ˆç°å‡ºæ½œåœ¨çš„é¥±å’Œè¶‹åŠ¿ã€‚åœ¨æ•°æ®å—é™çš„æƒ…å†µä¸‹ï¼Œé‡å¤åˆ©ç”¨é«˜è´¨é‡æ•°æ®è¢«è¯æ˜é«˜åº¦æœ‰æ•ˆï¼Œå› ä¸ºæœ€ç»ˆæ€§èƒ½ä¸»è¦ç”±æ€»ä¼˜åŒ–æ­¥æ•°ï¼ˆoptimization stepsï¼‰å†³å®šï¼Œè€Œéæ ·æœ¬çš„å”¯ä¸€æ€§ã€‚è¿™äº›ç»“æœä¸ºé€šè¿‡å¼ºåŒ–å­¦ä¹ åè®­ç»ƒï¼ˆRL post-trainingï¼‰é«˜æ•ˆæ‰©å±• LLMs çš„æ¨ç†èƒ½åŠ›æä¾›äº†åŸåˆ™æ€§åŸºç¡€å’Œå®è·µæŒ‡å—ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "V3 version:27 pages, 14 figures, add code and dataset url",
      "pdf_url": "https://arxiv.org/pdf/2509.25300v3",
      "published_date": "2025-09-29 17:10:35 UTC",
      "updated_date": "2025-12-22 07:20:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:51:04.084436+00:00"
    },
    {
      "arxiv_id": "2509.25052v1",
      "title": "Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and Planning",
      "title_zh": "Cogito, Ergo Ludoï¼šä¸€ç§é€šè¿‡æ¨ç†ä¸è§„åˆ’å­¦ä¹ åšå¼ˆçš„æ™ºèƒ½ä½“",
      "authors": [
        "Sai Wang",
        "Yu Wu",
        "Zhongwen Xu"
      ],
      "abstract": "The pursuit of artificial agents that can learn to master complex environments has led to remarkable successes, yet prevailing deep reinforcement learning methods often rely on immense experience, encoding their knowledge opaquely within neural network weights. We propose a different paradigm, one in which an agent learns to play by reasoning and planning. We introduce Cogito, ergo ludo (CEL), a novel agent architecture that leverages a Large Language Model (LLM) to build an explicit, language-based understanding of its environment's mechanics and its own strategy. Starting from a tabula rasa state with no prior knowledge (except action set), CEL operates on a cycle of interaction and reflection. After each episode, the agent analyzes its complete trajectory to perform two concurrent learning processes: Rule Induction, where it refines its explicit model of the environment's dynamics, and Strategy and Playbook Summarization, where it distills experiences into an actionable strategic playbook. We evaluate CEL on diverse grid-world tasks (i.e., Minesweeper, Frozen Lake, and Sokoban), and show that the CEL agent successfully learns to master these games by autonomously discovering their rules and developing effective policies from sparse rewards. Ablation studies confirm that the iterative process is critical for sustained learning. Our work demonstrates a path toward more general and interpretable agents that not only act effectively but also build a transparent and improving model of their world through explicit reasoning on raw experience.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿæ·±åº¦å¼ºåŒ–å­¦ä¹ (Deep Reinforcement Learning)ä¾èµ–æµ·é‡ç»éªŒä¸”æ¨¡å‹ç¼ºä¹é€æ˜åº¦çš„é—®é¢˜ï¼Œæå‡ºäº† Cogito, ergo ludo (CEL) æ¶æ„ã€‚CEL åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(Large Language Model)æ„å»ºå¯¹ç¯å¢ƒæœºåˆ¶å’Œç­–ç•¥çš„æ˜¾å¼ã€åŸºäºè¯­è¨€çš„ç†è§£ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿé€šè¿‡æ¨ç†å’Œè§„åˆ’è¿›è¡Œå­¦ä¹ ã€‚åœ¨æ²¡æœ‰ä»»ä½•å…ˆéªŒçŸ¥è¯†(Tabula Rasa)çš„æƒ…å†µä¸‹ï¼ŒCEL é€šè¿‡äº¤äº’ä¸åæ€çš„å¾ªç¯ï¼Œæ‰§è¡Œè§„åˆ™å½’çº³(Rule Induction)å’Œç­–ç•¥æ‰‹å†Œæ€»ç»“(Strategy and Playbook Summarization)ï¼Œä¸æ–­ç²¾ç‚¼ç¯å¢ƒæ¨¡å‹å¹¶å°†ç»éªŒè½¬åŒ–ä¸ºæˆ˜ç•¥ã€‚è¯¥ç ”ç©¶åœ¨æ‰«é›·(Minesweeper)ã€å†°æ¹–(Frozen Lake)å’Œæ¨ç®±å­(Sokoban)ç­‰ä»»åŠ¡ä¸­è¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜ CEL èƒ½å¤Ÿä»ç¨€ç–å¥–åŠ±(Sparse Rewards)ä¸­è‡ªä¸»å‘ç°è§„åˆ™å¹¶æŒæ¡æ¸¸æˆã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥è¯å®äº†å…¶è¿­ä»£è¿‡ç¨‹å¯¹æŒç»­å­¦ä¹ çš„é‡è¦æ€§ï¼Œä¸ºæ„å»ºæ›´å…·é€šç”¨æ€§å’Œå¯è§£é‡Šæ€§(Interpretable)çš„æ™ºèƒ½ä½“æä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25052v1",
      "published_date": "2025-09-29 17:02:31 UTC",
      "updated_date": "2025-09-29 17:02:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:51:06.454534+00:00"
    },
    {
      "arxiv_id": "2509.25047v1",
      "title": "Scaling Synthetic Task Generation for Agents via Exploration",
      "title_zh": "é€šè¿‡æ¢ç´¢å®ç°æ™ºèƒ½ä½“åˆæˆä»»åŠ¡çš„è§„æ¨¡åŒ–ç”Ÿæˆ",
      "authors": [
        "Ram Ramrakhya",
        "Andrew Szot",
        "Omar Attia",
        "Yuhao Yang",
        "Anh Nguyen",
        "Bogdan Mazoure",
        "Zhe Gan",
        "Harsh Agrawal",
        "Alexander Toshev"
      ],
      "abstract": "Post-Training Multimodal Large Language Models (MLLMs) to build interactive agents holds promise across domains such as computer-use, web navigation, and robotics. A key challenge in scaling such post-training is lack of high-quality downstream agentic task datasets with tasks that are diverse, feasible, and verifiable. Existing approaches for task generation rely heavily on human annotation or prompting MLLM with limited downstream environment information, which is either costly or poorly scalable as it yield tasks with limited coverage. To remedy this, we present AutoPlay, a scalable pipeline for task generation that explicitly explores interactive environments to discover possible interactions and current state information to synthesize environment-grounded tasks. AutoPlay operates in two stages: (i) an exploration phase, where an MLLM explorer agent systematically uncovers novel environment states and functionalities, and (ii) a task generation phase, where a task generator leverages exploration trajectories and a set of task guideline prompts as context to synthesize diverse, executable, and verifiable tasks. We show AutoPlay generates 20k tasks across 20 Android applications and 10k tasks across 13 applications Ubuntu applications to train mobile-use and computer-use agents. AutoPlay generated tasks enable large-scale task demonstration synthesis without human annotation by employing an MLLM task executor and verifier. This data enables training MLLM-based UI agents that improve success rates up to $20.0\\%$ on mobile-use and $10.9\\%$ on computer-use scenarios. In addition, AutoPlay generated tasks combined with MLLM verifier-based rewards enable scaling reinforcement learning training of UI agents, leading to an additional $5.7\\%$ gain. coverage. These results establish AutoPlay as a scalable approach for post-training capable MLLM agents reducing reliance on human annotation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ„å»ºäº¤äº’å¼æ™ºèƒ½ä½“æ—¶é¢ä¸´çš„é«˜è´¨é‡ã€å¤šæ ·åŒ–ä¸”å¯éªŒè¯ä»»åŠ¡æ•°æ®åŒ®ä¹çš„é—®é¢˜ï¼Œæå‡ºäº† AutoPlay æ¡†æ¶ã€‚AutoPlay æ˜¯ä¸€ç§å¯æ‰©å±•çš„ä»»åŠ¡ç”Ÿæˆæµæ°´çº¿ï¼Œé€šè¿‡æ˜¾å¼æ¢ç´¢äº¤äº’å¼ç¯å¢ƒæ¥å‘ç°æ½œåœ¨çš„äº¤äº’å¯èƒ½å’ŒçŠ¶æ€ä¿¡æ¯ï¼Œä»è€Œåˆæˆç¯å¢ƒæ„ŸçŸ¥çš„ä»»åŠ¡ã€‚è¯¥æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆæ˜¯æ¢ç´¢é˜¶æ®µï¼Œç”± MLLM æ¢ç´¢æ™ºèƒ½ä½“ç³»ç»Ÿåœ°å‘ç°ç¯å¢ƒçŠ¶æ€ï¼›å…¶æ¬¡æ˜¯ä»»åŠ¡ç”Ÿæˆé˜¶æ®µï¼Œåˆ©ç”¨æ¢ç´¢è½¨è¿¹å’Œä»»åŠ¡æŒ‡å—åˆæˆå¤šæ ·ä¸”å¯æ‰§è¡Œçš„ä»»åŠ¡ã€‚åˆ©ç”¨è¯¥æ–¹æ³•ï¼Œç ”ç©¶è€…åœ¨ Android å’Œ Ubuntu åº”ç”¨ç¨‹åºä¸­åˆ†åˆ«ç”Ÿæˆäº† 20k å’Œ 10k ä¸ªä»»åŠ¡ï¼Œç”¨äºè®­ç»ƒç§»åŠ¨ç«¯å’Œç”µè„‘ç«¯ä½¿ç”¨æ™ºèƒ½ä½“ã€‚å®éªŒè¡¨æ˜ï¼ŒAutoPlay ç”Ÿæˆçš„ä»»åŠ¡èƒ½æ˜¾è‘—æå‡ UI æ™ºèƒ½ä½“çš„æˆåŠŸç‡ï¼Œåœ¨ç§»åŠ¨ç«¯å’Œç”µè„‘ç«¯åœºæ™¯ä¸‹åˆ†åˆ«æé«˜äº† 20.0% å’Œ 10.9%ã€‚æ­¤å¤–ï¼Œç»“åˆåŸºäº MLLM éªŒè¯å™¨çš„å¥–åŠ±è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼‰è®­ç»ƒï¼Œè¿˜èƒ½è¿›ä¸€æ­¥å¸¦æ¥ 5.7% çš„æ€§èƒ½å¢ç›Šã€‚è¯¥ç ”ç©¶è¯æ˜äº† AutoPlay åœ¨å‡å°‘å¯¹äººå·¥æ ‡æ³¨ä¾èµ–çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡ MLLM æ™ºèƒ½ä½“çš„è®­ç»ƒè§„æ¨¡å’Œèƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25047v1",
      "published_date": "2025-09-29 17:00:02 UTC",
      "updated_date": "2025-09-29 17:00:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:51:07.471651+00:00"
    },
    {
      "arxiv_id": "2509.25045v2",
      "title": "Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic Architectures",
      "title_zh": "Hyperdimensional Probeï¼šåŸºäºå‘é‡ç¬¦å·æ¶æ„çš„ LLM è¡¨å¾è§£ç ",
      "authors": [
        "Marco Bronzini",
        "Carlo Nicolini",
        "Bruno Lepri",
        "Jacopo Staiano",
        "Andrea Passerini"
      ],
      "abstract": "Despite their capabilities, Large Language Models (LLMs) remain opaque with limited understanding of their internal representations. Current interpretability methods either focus on input-oriented feature extraction, such as supervised probes and Sparse Autoencoders (SAEs), or on output distribution inspection, such as logit-oriented approaches. A full understanding of LLM vector spaces, however, requires integrating both perspectives, something existing approaches struggle with due to constraints on latent feature definitions. We introduce the Hyperdimensional Probe, a hybrid supervised probe that combines symbolic representations with neural probing. Leveraging Vector Symbolic Architectures (VSAs) and hypervector algebra, it unifies prior methods: the top-down interpretability of supervised probes, SAE's sparsity-driven proxy space, and output-oriented logit investigation. This allows deeper input-focused feature extraction while supporting output-oriented investigation. Our experiments show that our method consistently extracts meaningful concepts across LLMs, embedding sizes, and setups, uncovering concept-driven patterns in analogy-oriented inference and QA-focused text generation. By supporting joint input-output analysis, this work advances semantic understanding of neural representations while unifying the complementary perspectives of prior methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)å†…éƒ¨è¡¨ç¤ºä¸é€æ˜ä¸”éš¾ä»¥è§£æçš„é—®é¢˜ï¼Œæå‡ºäº†Hyperdimensional Probeï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆäº†å‘é‡ç¬¦å·æ¶æ„(Vector Symbolic Architectures, VSAs)å’Œè¶…å‘é‡ä»£æ•°(hypervector algebra)çš„æ··åˆç›‘ç£æ¢é’ˆã€‚è¯¥æ–¹æ³•æˆåŠŸç»Ÿä¸€äº†ç°æœ‰çš„å¤šç§è§£é‡Šæ€§æŠ€æœ¯ï¼ŒåŒ…æ‹¬ç›‘ç£æ¢é’ˆçš„è‡ªé¡¶å‘ä¸‹å¯è§£é‡Šæ€§ã€ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨(SAEs)çš„ç¨€ç–é©±åŠ¨ä»£ç†ç©ºé—´ä»¥åŠé¢å‘è¾“å‡ºçš„Logitè°ƒæŸ¥ï¼Œä»è€Œå®ç°äº†å¯¹è¾“å…¥ç‰¹å¾æå–ä¸è¾“å‡ºåˆ†å¸ƒæ£€æŸ¥çš„æ•´åˆã€‚å®éªŒè¯æ˜ï¼ŒHyperdimensional Probeèƒ½å¤Ÿåœ¨ä¸åŒçš„æ¨¡å‹æ¶æ„å’ŒåµŒå…¥å°ºå¯¸ä¸­æŒç»­æå–æœ‰æ„ä¹‰çš„è¯­ä¹‰æ¦‚å¿µï¼Œå¹¶æ­ç¤ºäº†ç±»æ¯”æ¨ç†å’Œé—®ç­”ä»»åŠ¡ä¸­ç”±æ¦‚å¿µé©±åŠ¨çš„å†…éƒ¨æ¨¡å¼ã€‚é€šè¿‡æ”¯æŒè¾“å…¥ä¸è¾“å‡ºçš„è”åˆåˆ†æï¼Œè¯¥å·¥ä½œä¸ä»…æå‡äº†å¯¹ç¥ç»ç½‘ç»œå†…éƒ¨è¡¨ç¤ºçš„è¯­ä¹‰ç†è§£ï¼Œè¿˜ä¸ºè§£é‡Šå¤§æ¨¡å‹æä¾›äº†ä¸€ä¸ªç»Ÿä¸€ä¸”äº’è¡¥çš„ç ”ç©¶è§†è§’ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25045v2",
      "published_date": "2025-09-29 16:59:07 UTC",
      "updated_date": "2025-12-02 13:09:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:51:24.467039+00:00"
    },
    {
      "arxiv_id": "2509.25043v1",
      "title": "Large Language Models for Software Testing: A Research Roadmap",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹åœ¨è½¯ä»¶æµ‹è¯•ä¸­çš„åº”ç”¨ï¼šç ”ç©¶è·¯çº¿å›¾",
      "authors": [
        "Cristian Augusto",
        "Antonia Bertolino",
        "Guglielmo De Angelis",
        "Francesca Lonetti",
        "JesÃºs MorÃ¡n"
      ],
      "abstract": "Large Language Models (LLMs) are starting to be profiled as one of the most significant disruptions in the Software Testing field.\n  Specifically, they have been successfully applied in software testing tasks such as generating test code, or summarizing documentation.\n  This potential has attracted hundreds of researchers, resulting in dozens of new contributions every month, hardening researchers to\n  stay at the forefront of the wave. Still, to the best of our knowledge, no prior work has provided a structured vision of the progress\n  and most relevant research trends in LLM-based testing. In this article, we aim to provide a roadmap that illustrates its current state,\n  grouping the contributions into different categories, and also sketching the most promising and active research directions for the field.\n  To achieve this objective, we have conducted a semi-systematic literature review, collecting articles and mapping them into the most\n  prominent categories, reviewing the current and ongoing status, and analyzing the open challenges of LLM-based software testing.\n  Lastly, we have outlined several expected long-term impacts of LLMs over the whole software testing field.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ (Large Language Models, LLMs) å¯¹è½¯ä»¶æµ‹è¯• (Software Testing) é¢†åŸŸçš„é¢ è¦†æ€§å½±å“ï¼Œé‡ç‚¹åˆ†æäº†å…¶åœ¨ç”Ÿæˆæµ‹è¯•ä»£ç å’Œæ–‡æ¡£æ‘˜è¦ç­‰ä»»åŠ¡ä¸­çš„æˆåŠŸåº”ç”¨ã€‚é’ˆå¯¹å½“å‰ç ”ç©¶æˆæœé£é€Ÿå¢é•¿ä¸”ç¼ºä¹ç³»ç»Ÿæ€§æ¢³ç†çš„æŒ‘æˆ˜ï¼Œæœ¬æ–‡é€šè¿‡åŠç³»ç»Ÿæ€§çš„æ–‡çŒ®ç»¼è¿° (Semi-systematic Literature Review) æå‡ºäº†ä¸€ä»½è¯¦å°½çš„ç§‘ç ”è·¯çº¿å›¾ (Research Roadmap)ã€‚æ–‡ç« å¯¹ç°æœ‰å­¦æœ¯è´¡çŒ®è¿›è¡Œäº†å¤šç»´åº¦åˆ†ç±»ï¼Œæ˜ç¡®äº†å½“å‰æœ€æ´»è·ƒçš„ç ”ç©¶è¶‹åŠ¿ï¼Œå¹¶æ·±å…¥åˆ†æäº†åŸºäº LLM çš„è½¯ä»¶æµ‹è¯•æ‰€é¢ä¸´çš„å¼€æ”¾æ€§æŒ‘æˆ˜ã€‚æœ€åï¼Œä½œè€…å±•æœ›äº† LLMs å¯¹æ•´ä¸ªè½¯ä»¶æµ‹è¯•ç”Ÿå‘½å‘¨æœŸçš„é•¿æœŸæ½œåœ¨å½±å“ï¼Œä¸ºè¯¥é¢†åŸŸæœªæ¥çš„å‘å±•æ–¹å‘æä¾›äº†ç»“æ„åŒ–çš„æŒ‡å¼•å’Œæ„¿æ™¯ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "40 pages & 10 figures Submitted on 29th September 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.25043v1",
      "published_date": "2025-09-29 16:58:21 UTC",
      "updated_date": "2025-09-29 16:58:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:51:13.389587+00:00"
    },
    {
      "arxiv_id": "2509.25042v1",
      "title": "Fast Real-Time Pipeline for Robust Arm Gesture Recognition",
      "title_zh": "é¢å‘ç¨³å¥æ‰‹è‡‚æ‰‹åŠ¿è¯†åˆ«çš„å¿«é€Ÿå®æ—¶å¤„ç†æµæ°´çº¿",
      "authors": [
        "MilÃ¡n Zsolt Bagladi",
        "LÃ¡szlÃ³ GulyÃ¡s",
        "GergÅ‘ Szalay"
      ],
      "abstract": "This paper presents a real-time pipeline for dynamic arm gesture recognition based on OpenPose keypoint estimation, keypoint normalization, and a recurrent neural network classifier. The 1 x 1 normalization scheme and two feature representations (coordinate- and angle-based) are presented for the pipeline. In addition, an efficient method to improve robustness against camera angle variations is also introduced by using artificially rotated training data. Experiments on a custom traffic-control gesture dataset demonstrate high accuracy across varying viewing angles and speeds. Finally, an approach to calculate the speed of the arm signal (if necessary) is also presented.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäº OpenPose å…³é”®ç‚¹ä¼°è®¡ã€å…³é”®ç‚¹å½’ä¸€åŒ–å’Œ Recurrent Neural Network (RNN) åˆ†ç±»å™¨çš„å®æ—¶åŠ¨æ€æ‰‹è‡‚å§¿æ€è¯†åˆ«æµç¨‹ã€‚æ–‡ä¸­è¯¦ç»†ä»‹ç»äº† 1x1 normalization æ–¹æ¡ˆä»¥åŠåŸºäº coordinate-based å’Œ angle-based çš„ä¸¤ç§ç‰¹å¾è¡¨ç¤ºæ–¹æ³•ã€‚ä¸ºäº†æå‡ç³»ç»Ÿå¯¹ç›¸æœºè§†è§’å˜åŒ–çš„é²æ£’æ€§ï¼Œç ”ç©¶è€…å¼•å…¥äº†ä¸€ç§åˆ©ç”¨äººå·¥æ—‹è½¬è®­ç»ƒæ•°æ®çš„é«˜æ•ˆæ–¹æ³•æ¥å¢å¼ºæ¨¡å‹è¡¨ç°ã€‚åœ¨è‡ªå®šä¹‰äº¤é€šç®¡åˆ¶æ‰‹åŠ¿æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨ä¸åŒè§‚å¯Ÿè§’åº¦å’ŒåŠ¨ä½œé€Ÿåº¦ä¸‹å‡èƒ½ä¿æŒé«˜å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œè¯¥ç®¡çº¿è¿˜æä¾›äº†ä¸€ç§è®¡ç®—æ‰‹è‡‚ä¿¡å·é€Ÿåº¦çš„æ–¹æ³•ï¼Œè¿›ä¸€æ­¥æ‰©å±•äº†å…¶å®é™…åº”ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25042v1",
      "published_date": "2025-09-29 16:57:56 UTC",
      "updated_date": "2025-09-29 16:57:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:51:24.286974+00:00"
    },
    {
      "arxiv_id": "2509.25035v2",
      "title": "Ultra-Fast Language Generation via Discrete Diffusion Divergence Instruct",
      "title_zh": "åŸºäºç¦»æ•£æ‰©æ•£æ•£åº¦æŒ‡ä»¤çš„æé€Ÿè¯­è¨€ç”Ÿæˆ",
      "authors": [
        "Haoyang Zheng",
        "Xinyang Liu",
        "Cindy Xiangrui Kong",
        "Nan Jiang",
        "Zheyuan Hu",
        "Weijian Luo",
        "Wei Deng",
        "Guang Lin"
      ],
      "abstract": "Fast and high-quality language generation is the holy grail that people pursue in the age of AI. In this work, we introduce Discrete Diffusion Divergence Instruct (DiDi-Instruct), a training-based method that initializes from a pre-trained (masked) discrete diffusion language model (dLLM) and distills a few-step student for fast generation. The resulting DiDi-Instruct model achieves comparable or superior performance to its dLLM teacher and the GPT-2 baseline while enabling up to 64$\\times$ acceleration. The theoretical foundation of DiDi-Instruct is a novel framework based on integral KL-divergence minimization, which yields a practical training algorithm. We further introduce grouped reward normalization, intermediate-state matching, and the reward-guided ancestral sampler that significantly improve training stability, model coverage, and inference quality. On OpenWebText, DiDi-Instruct achieves perplexity from 62.2 (8 NFEs) to 18.4 (128 NFEs), which outperforms prior accelerated dLLMs and GPT-2 baseline. These gains come with a negligible entropy loss (around $1\\%$) and reduce additional training wall-clock time by more than $20\\times$ compared to competing dLLM distillation methods. We further validate the robustness and effectiveness of DiDi-Instruct through extensive ablation studies, model scaling, and the generation of discrete protein sequences. In conclusion, DiDi-Instruct is an efficient yet effective distillation method, enabling language generation in the blink of an eye. We will release both code and models at github.com/haoyangzheng-ai/didi-instruct.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DiDi-Instruct (Discrete Diffusion Divergence Instruct)ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºè®­ç»ƒçš„è’¸é¦æ–¹æ³•ï¼Œæ—¨åœ¨å®ç°å¿«é€Ÿä¸”é«˜è´¨é‡çš„è¯­è¨€ç”Ÿæˆã€‚è¯¥æ–¹æ³•ä»é¢„è®­ç»ƒçš„ç¦»æ•£æ‰©æ•£è¯­è¨€æ¨¡å‹ (dLLM) åˆå§‹åŒ–ï¼Œé€šè¿‡å°†å¤æ‚æ¨¡å‹è’¸é¦ä¸ºä»…éœ€å°‘é‡æ¨ç†æ­¥éª¤çš„å­¦ç”Ÿæ¨¡å‹ï¼Œå®ç°äº†é«˜è¾¾64å€çš„ç”ŸæˆåŠ é€Ÿã€‚å…¶ç†è®ºåŸºç¡€æ˜¯åŸºäºç§¯åˆ†KLæ•£åº¦ (integral KL-divergence) æœ€å°åŒ–çš„åˆ›æ–°æ¡†æ¶ï¼Œå¹¶å¼•å…¥äº†åˆ†ç»„å¥–åŠ±å½’ä¸€åŒ– (grouped reward normalization)ã€ä¸­é—´çŠ¶æ€åŒ¹é… (intermediate-state matching) å’Œå¥–åŠ±å¼•å¯¼çš„ç¥–å…ˆé‡‡æ ·å™¨ (reward-guided ancestral sampler) æ¥æå‡è®­ç»ƒç¨³å®šæ€§å’Œæ¨ç†è´¨é‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDiDi-Instructåœ¨OpenWebTextä¸Šçš„è¡¨ç°ä¼˜äºå…ˆå‰çš„åŠ é€Ÿå‹dLLMså’ŒGPT-2åŸºçº¿ï¼Œä¸”ç†µæŸå¤±ä»…çº¦1%ã€‚ç›¸æ¯”åŒç±»è’¸é¦æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†é¢å¤–è®­ç»ƒæ—¶é—´ç¼©çŸ­äº†20å€ä»¥ä¸Šï¼Œå¹¶åœ¨è›‹ç™½è´¨åºåˆ—ç”Ÿæˆä»»åŠ¡ä¸­éªŒè¯äº†å…¶å¹¿æ³›çš„é€‚ç”¨æ€§ä¸é²æ£’æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "56 pages, 7 figures, 7 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.25035v2",
      "published_date": "2025-09-29 16:55:44 UTC",
      "updated_date": "2025-10-01 17:45:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:51:38.559091+00:00"
    },
    {
      "arxiv_id": "2509.25299v1",
      "title": "ID-RAG: Identity Retrieval-Augmented Generation for Long-Horizon Persona Coherence in Generative Agents",
      "title_zh": "ID-RAGï¼šé¢å‘ç”Ÿæˆå¼æ™ºèƒ½ä½“é•¿æ—¶ç¨‹äººæ ¼ä¸€è‡´æ€§çš„èº«ä»½æ£€ç´¢å¢å¼ºç”Ÿæˆ",
      "authors": [
        "Daniel Platnick",
        "Mohamed E. Bengueddache",
        "Marjan Alirezaie",
        "Dava J. Newman",
        "Alex ''Sandy'' Pentland",
        "Hossein Rahnama"
      ],
      "abstract": "Generative agents powered by language models are increasingly deployed for long-horizon tasks. However, as long-term memory context grows over time, they struggle to maintain coherence. This deficiency leads to critical failures, including identity drift, ignoring established beliefs, and the propagation of hallucinations in multi-agent systems. To mitigate these challenges, this paper introduces Identity Retrieval-Augmented Generation (ID-RAG), a novel mechanism designed to ground an agent's persona and persistent preferences in a dynamic, structured identity model: a knowledge graph of core beliefs, traits, and values. During the agent's decision loop, this model is queried to retrieve relevant identity context, which directly informs action selection. We demonstrate this approach by introducing and implementing a new class of ID-RAG enabled agents called Human-AI Agents (HAis), where the identity model is inspired by the Chronicle structure used in Perspective-Aware AI, a dynamic knowledge graph learned from a real-world entity's digital footprint. In social simulations of a mayoral election, HAis using ID-RAG outperformed baseline agents in long-horizon persona coherence - achieving higher identity recall across all tested models by the fourth timestep - and reduced simulation convergence time by 19% (GPT-4o) and 58% (GPT-4o mini). By treating identity as an explicit, retrievable knowledge structure, ID-RAG offers a foundational approach for developing more temporally coherent, interpretable, and aligned generative agents. Our code is open-source and available at: https://github.com/flybits/humanai-agents.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿæˆå¼æ™ºèƒ½ä½“åœ¨é•¿ç¨‹ä»»åŠ¡ä¸­å› é•¿æœŸè®°å¿†å¢é•¿è€Œå¯¼è‡´çš„èº«ä»½æ¼‚ç§»(identity drift)å’Œäººæ ¼è¿è´¯æ€§ä¸§å¤±ç­‰é—®é¢˜ï¼Œæå‡ºäº†ID-RAG (Identity Retrieval-Augmented Generation) æœºåˆ¶ã€‚è¯¥æœºåˆ¶å°†æ™ºèƒ½ä½“çš„äººæ ¼(persona)å’ŒæŒä¹…åå¥½é”šå®šåœ¨ä¸€ä¸ªåŠ¨æ€ä¸”ç»“æ„åŒ–çš„èº«ä»½æ¨¡å‹ä¸­ï¼Œå³ç”±æ ¸å¿ƒä¿¡å¿µã€ç‰¹è´¨å’Œä»·å€¼è§‚æ„æˆçš„çŸ¥è¯†å›¾è°±(Knowledge Graph)ã€‚åœ¨æ™ºèƒ½ä½“å†³ç­–å¾ªç¯ä¸­ï¼Œç³»ç»Ÿé€šè¿‡æŸ¥è¯¢è¯¥æ¨¡å‹è·å–ç›¸å…³çš„èº«ä»½ä¸Šä¸‹æ–‡ï¼Œä»è€Œç›´æ¥æŒ‡å¯¼åŠ¨ä½œé€‰æ‹©ã€‚ç ”ç©¶è€…é€šè¿‡å®ç°Human-AI Agents (HAis) éªŒè¯äº†è¯¥æ–¹æ³•ï¼Œå…¶èº«ä»½æ¨¡å‹é‡‡ç”¨äº†åŸºäºç°å®å®ä½“æ•°å­—è¶³è¿¹å­¦ä¹ åˆ°çš„Chronicleç»“æ„ã€‚åœ¨å¸‚é•¿é€‰ä¸¾çš„ç¤¾ä¼šæ¨¡æ‹Ÿå®éªŒä¸­ï¼Œé…å¤‡ID-RAGçš„æ™ºèƒ½ä½“åœ¨äººæ ¼è¿è´¯æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œä¸ä»…å®ç°äº†æ›´é«˜çš„èº«ä»½å¬å›ç‡(identity recall)ï¼Œè¿˜å°†æ¨¡æ‹Ÿæ”¶æ•›æ—¶é—´ç¼©çŸ­äº†19%è‡³58% (GPT-4o/GPT-4o mini)ã€‚é€šè¿‡å°†èº«ä»½è§†ä¸ºæ˜¾å¼ä¸”å¯æ£€ç´¢çš„çŸ¥è¯†ç»“æ„ï¼ŒID-RAGä¸ºå¼€å‘å…·æœ‰æ—¶é—´è¿è´¯æ€§ã€å¯è§£é‡Šæ€§ä¸”å¯¹é½çš„ç”Ÿæˆå¼æ™ºèƒ½ä½“æä¾›äº†åŸºç¡€æ€§æ–¹æ³•ã€‚",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to LLAIS 2025: Workshop on LLM-Based Agents for Intelligent Systems, at ECAI 2025, 12 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.25299v1",
      "published_date": "2025-09-29 16:54:51 UTC",
      "updated_date": "2025-09-29 16:54:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:51:46.472649+00:00"
    },
    {
      "arxiv_id": "2509.25032v1",
      "title": "AIRoA MoMa Dataset: A Large-Scale Hierarchical Dataset for Mobile Manipulation",
      "title_zh": "AIRoA MoMa æ•°æ®é›†ï¼šé¢å‘ç§»åŠ¨æ“ä½œçš„å¤§è§„æ¨¡å±‚çº§åŒ–æ•°æ®é›†",
      "authors": [
        "Ryosuke Takanami",
        "Petr Khrapchenkov",
        "Shu Morikuni",
        "Jumpei Arima",
        "Yuta Takaba",
        "Shunsuke Maeda",
        "Takuya Okubo",
        "Genki Sano",
        "Satoshi Sekioka",
        "Aoi Kadoya",
        "Motonari Kambara",
        "Naoya Nishiura",
        "Haruto Suzuki",
        "Takanori Yoshimoto",
        "Koya Sakamoto",
        "Shinnosuke Ono",
        "Hu Yang",
        "Daichi Yashima",
        "Aoi Horo",
        "Tomohiro Motoda",
        "Kensuke Chiyoma",
        "Hiroshi Ito",
        "Koki Fukuda",
        "Akihito Goto",
        "Kazumi Morinaga",
        "Yuya Ikeda",
        "Riko Kawada",
        "Masaki Yoshikawa",
        "Norio Kosuge",
        "Yuki Noguchi",
        "Kei Ota",
        "Tatsuya Matsushima",
        "Yusuke Iwasawa",
        "Yutaka Matsuo",
        "Tetsuya Ogata"
      ],
      "abstract": "As robots transition from controlled settings to unstructured human environments, building generalist agents that can reliably follow natural language instructions remains a central challenge. Progress in robust mobile manipulation requires large-scale multimodal datasets that capture contact-rich and long-horizon tasks, yet existing resources lack synchronized force-torque sensing, hierarchical annotations, and explicit failure cases. We address this gap with the AIRoA MoMa Dataset, a large-scale real-world multimodal dataset for mobile manipulation. It includes synchronized RGB images, joint states, six-axis wrist force-torque signals, and internal robot states, together with a novel two-layer annotation schema of sub-goals and primitive actions for hierarchical learning and error analysis. The initial dataset comprises 25,469 episodes (approx. 94 hours) collected with the Human Support Robot (HSR) and is fully standardized in the LeRobot v2.1 format. By uniquely integrating mobile manipulation, contact-rich interaction, and long-horizon structure, AIRoA MoMa provides a critical benchmark for advancing the next generation of Vision-Language-Action models. The first version of our dataset is now available at https://huggingface.co/datasets/airoa-org/airoa-moma .",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† AIRoA MoMa Datasetï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹ç§»åŠ¨æ“æ§ (Mobile Manipulation) çš„å¤§è§„æ¨¡çœŸå®ä¸–ç•Œå¤šæ¨¡æ€æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†æ—¨åœ¨è§£å†³ç°æœ‰èµ„æºç¼ºä¹åŒæ­¥åŠ›çŸ©ä¼ æ„Ÿã€å±‚æ¬¡åŒ–æ ‡æ³¨å’Œæ˜ç¡®å¤±è´¥æ¡ˆä¾‹çš„é—®é¢˜ï¼Œä»¥æ”¯æŒæœºå™¨äººåœ¨éç»“æ„åŒ–ç¯å¢ƒä¸­çš„é€šç”¨åŒ–ã€‚æ•°æ®é›†åŒ…å«åŒæ­¥çš„ RGB å›¾åƒã€å…³èŠ‚çŠ¶æ€ã€å…­è½´æ‰‹è…•åŠ›çŸ©ä¿¡å· (Force-Torque signals) ä»¥åŠæœºå™¨äººå†…éƒ¨çŠ¶æ€ï¼Œå¹¶åœ¨ LeRobot v2.1 æ ¼å¼ä¸‹å®ç°äº†å®Œå…¨æ ‡å‡†åŒ–ã€‚ç ”ç©¶å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„å­ç›®æ ‡ (Sub-goals) å’ŒåŸå­åŠ¨ä½œ (Primitive actions) ä¸¤å±‚æ ‡æ³¨æ¶æ„ï¼Œç”¨äºå±‚æ¬¡åŒ–å­¦ä¹  (Hierarchical learning) å’Œé”™è¯¯åˆ†æã€‚åˆå§‹æ•°æ®é›†ç”±ä½¿ç”¨äººç±»æ”¯æŒæœºå™¨äºº (HSR) æ”¶é›†çš„ 25,469 ä¸ªæƒ…èŠ‚ (çº¦ 94 å°æ—¶) ç»„æˆã€‚é€šè¿‡æ•´åˆç§»åŠ¨æ“æ§ã€ä¸°å¯Œæ¥è§¦äº¤äº’å’Œé•¿ç¨‹ä»»åŠ¡ (Long-horizon tasks) ç»“æ„ï¼ŒAIRoA MoMa ä¸ºæ¨è¿›ä¸‹ä¸€ä»£è§†è§‰\\-è¯­è¨€\\-åŠ¨ä½œ (Vision-Language-Action) æ¨¡å‹æä¾›äº†å…³é”®çš„åŸºå‡†æµ‹è¯•ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25032v1",
      "published_date": "2025-09-29 16:51:47 UTC",
      "updated_date": "2025-09-29 16:51:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:51:45.901941+00:00"
    },
    {
      "arxiv_id": "2509.25016v2",
      "title": "CLASP: Adaptive Spectral Clustering for Unsupervised Per-Image Segmentation",
      "title_zh": "CLASPï¼šé¢å‘æ— ç›‘ç£å•å›¾åˆ†å‰²çš„è‡ªé€‚åº”è°±èšç±»",
      "authors": [
        "Max Curie",
        "Paulo da Costa"
      ],
      "abstract": "We introduce CLASP (Clustering via Adaptive Spectral Processing), a lightweight framework for unsupervised image segmentation that operates without any labeled data or finetuning. CLASP first extracts per patch features using a self supervised ViT encoder (DINO); then, it builds an affinity matrix and applies spectral clustering. To avoid manual tuning, we select the segment count automatically with a eigengap silhouette search, and we sharpen the boundaries with a fully connected DenseCRF. Despite its simplicity and training free nature, CLASP attains competitive mIoU and pixel accuracy on COCO Stuff and ADE20K, matching recent unsupervised baselines. The zero training design makes CLASP a strong, easily reproducible baseline for large unannotated corpora especially common in digital advertising and marketing workflows such as brand safety screening, creative asset curation, and social media content moderation",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CLASP (Clustering via Adaptive Spectral Processing)ï¼Œä¸€ä¸ªæ— éœ€æ ‡æ³¨æ•°æ®æˆ–å¾®è°ƒçš„è½»é‡çº§æ— ç›‘ç£å›¾åƒåˆ†å‰²æ¡†æ¶ã€‚è¯¥æ¡†æ¶é¦–å…ˆåˆ©ç”¨è‡ªç›‘ç£ ViT ç¼–ç å™¨ (DINO) æå–å›¾åƒå—ç‰¹å¾å¹¶æ„å»ºäº²å’ŒçŸ©é˜µï¼Œéšååº”ç”¨å…‰è°±èšç±» (Spectral Clustering) æŠ€æœ¯è¿›è¡Œåˆ†å‰²ã€‚ä¸ºäº†å®ç°è‡ªåŠ¨åŒ–ï¼ŒCLASP é€šè¿‡ç‰¹å¾å€¼é—´éš™-è½®å»“æœç´¢ (Eigengap-Silhouette Search) è‡ªåŠ¨ç¡®å®šåˆ†å‰²æ•°é‡ï¼Œå¹¶ç»“åˆå…¨è¿æ¥ DenseCRF è¿›ä¸€æ­¥é”åŒ–åˆ†å‰²è¾¹ç•Œã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡ CLASP é‡‡ç”¨é›¶è®­ç»ƒè®¾è®¡ï¼Œå…¶åœ¨ COCO-Stuff å’Œ ADE20K æ•°æ®é›†ä¸Šçš„ mIoU å’Œåƒç´ å‡†ç¡®ç‡ä»èƒ½ä¸ä¸»æµæ— ç›‘ç£åŸºçº¿æ¨¡å‹ç›¸åª²ç¾ã€‚è¯¥æ¡†æ¶çš„ç®€æ´æ€§ä¸é«˜å¯å¤ç°æ€§ä½¿å…¶æˆä¸ºå¤„ç†å¤§è§„æ¨¡æœªæ ‡æ³¨æ•°æ®çš„å¼ºå¤§å·¥å…·ï¼Œç‰¹åˆ«é€‚ç”¨äºå“ç‰Œå®‰å…¨ç­›é€‰ã€åˆ›æ„èµ„äº§ç­–åˆ’åŠç¤¾äº¤åª’ä½“å†…å®¹å®¡æ ¸ç­‰æ•°å­—åŒ–è¥é”€åœºæ™¯ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25016v2",
      "published_date": "2025-09-29 16:41:30 UTC",
      "updated_date": "2025-10-24 09:11:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:51:47.964859+00:00"
    },
    {
      "arxiv_id": "2510.03293v1",
      "title": "From Score Distributions to Balance: Plug-and-Play Mixture-of-Experts Routing",
      "title_zh": "ä»åˆ†å€¼åˆ†å¸ƒåˆ°è´Ÿè½½å‡è¡¡ï¼šå³æ’å³ç”¨çš„æ··åˆä¸“å®¶è·¯ç”±",
      "authors": [
        "Rana Shahout",
        "Colin Cai",
        "Yilun Du",
        "Minlan Yu",
        "Michael Mitzenmacher"
      ],
      "abstract": "Mixture-of-Experts (MoE) models can scale parameter capacity by routing each token to a subset of experts through a learned gate function. While conditional routing reduces training costs, it shifts the burden on inference memory: expert parameters and activations consume memory, limiting the number of experts per device. As tokens are routed, some experts become overloaded while others are underutilized. Because experts are mapped to GPUs, this imbalance translates directly into degraded system performance in terms of latency, throughput, and cost. We present LASER, a plug-and-play, inference-time routing algorithm that balances load while preserving accuracy. LASER adapts to the shape of the gate's score distribution. When scores provide a clear preference, it routes to the strongest experts; when scores are more uniform, it broadens the set of viable experts and routes to the least-loaded among them. Because LASER relies only on gate scores from a trained model, it integrates directly into existing MoE inference pipelines without retraining or finetuning. We evaluate LASER on Mixtral-8x7B and DeepSeek-MoE-16b-chat across four datasets (ARC-Easy, ARC-Challenge, MMLU, and GSM8K). LASER improves load balancing, translating into lower latency and higher throughput, while keeping the accuracy changes negligible.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LASERï¼Œä¸€ç§å³æ’å³ç”¨(Plug-and-play)çš„æ¨ç†ç«¯è·¯ç”±ç®—æ³•ï¼Œæ—¨åœ¨è§£å†³Mixture-of-Experts (MoE)æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­å› ä¸“å®¶è´Ÿè½½ä¸å‡å¯¼è‡´çš„å»¶è¿Ÿå¢åŠ ã€ååé‡ä¸‹é™å’Œç³»ç»Ÿæ€§èƒ½å—æŸé—®é¢˜ã€‚LASERèƒ½å¤Ÿæ ¹æ®é—¨æ§(gate)çš„åˆ†æ•°åˆ†å¸ƒåŠ¨æ€è°ƒæ•´è·¯ç”±ç­–ç•¥ï¼šå½“åˆ†æ•°å±•ç°å‡ºæ˜ç¡®åå¥½æ—¶è·¯ç”±è‡³æ€§èƒ½æœ€å¼ºçš„ä¸“å®¶ï¼Œè€Œåœ¨åˆ†å¸ƒè¶‹äºå‡åŒ€æ—¶åˆ™æ‰©å¤§å€™é€‰ä¸“å®¶é›†å¹¶è·¯ç”±è‡³å…¶ä¸­è´Ÿè½½æœ€ä½çš„ä¸“å®¶ã€‚ç”±äºè¯¥ç®—æ³•å®Œå…¨åŸºäºé¢„è®­ç»ƒæ¨¡å‹çš„é—¨æ§åˆ†æ•°ï¼Œå› æ­¤æ— éœ€ä»»ä½•é‡æ–°è®­ç»ƒæˆ–å¾®è°ƒ(finetuning)å³å¯ç›´æ¥é›†æˆåˆ°ç°æœ‰çš„MoEæ¨ç†ç®¡çº¿ä¸­ã€‚ç ”ç©¶äººå‘˜åœ¨Mixtral-8x7Bå’ŒDeepSeek-MoE-16b-chatç­‰æ¨¡å‹ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œæ¶µç›–äº†ARCã€MMLUå’ŒGSM8Kç­‰å¤šä¸ªåŸºå‡†æ•°æ®é›†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLASERåœ¨æ˜¾è‘—ä¼˜åŒ–è´Ÿè½½å‡è¡¡ã€é™ä½ç³»ç»Ÿå»¶è¿Ÿå¹¶æå‡ååé‡çš„åŒæ—¶ï¼Œå¯¹æ¨¡å‹å‡†ç¡®ç‡çš„å½±å“å‡ ä¹å¯ä»¥å¿½ç•¥ä¸è®¡ã€‚è¿™ä¸€æ–¹æ¡ˆä¸ºæé«˜å¤§è§„æ¨¡ç¨€ç–æ¨¡å‹åœ¨å®é™…ç”Ÿäº§ç¯å¢ƒä¸­çš„æ¨ç†æ•ˆç‡æä¾›äº†æœ‰æ•ˆé€”å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03293v1",
      "published_date": "2025-09-29 16:29:17 UTC",
      "updated_date": "2025-09-29 16:29:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:51:53.562070+00:00"
    },
    {
      "arxiv_id": "2509.25004v1",
      "title": "CLPO: Curriculum Learning meets Policy Optimization for LLM Reasoning",
      "title_zh": "CLPOï¼šç»“åˆè¯¾ç¨‹å­¦ä¹ ä¸ç­–ç•¥ä¼˜åŒ–çš„LLMæ¨ç†",
      "authors": [
        "Shijie Zhang",
        "Guohao Sun",
        "Kevin Zhang",
        "Xiang Guo",
        "Rujun Guo"
      ],
      "abstract": "Recently, online Reinforcement Learning with Verifiable Rewards (RLVR) has become a key paradigm for enhancing the reasoning capabilities of Large Language Models (LLMs). However, existing methods typically treat all training samples uniformly, overlooking the vast differences in problem difficulty relative to the model's current capabilities. This uniform training strategy leads to inefficient exploration of problems the model has already mastered, while concurrently lacking effective guidance on problems that are challenging its abilities the most, limiting both learning efficiency and upper-bound performance. To address this, we propose CLPO (Curriculum-guided Learning for Policy Optimization), a novel algorithm that creates a dynamic pedagogical feedback loop within the policy optimization process. The core of CLPO leverages the model's own rollout performance to conduct real-time difficulty assessment, thereby constructing an Online Curriculum. This curriculum then guides an Adaptive Problem Restructuring mechanism, where the model acts as its own teacher: it diversifies medium-difficulty problems to promote generalization and simplifies challenging problems to make them more attainable. Our approach transforms the static training procedure into a dynamic process that co-evolves with the model's capabilities. Experiments show that CLPO achieves state-of-the-art performance across eight challenging mathematical and general reasoning benchmarks, with an average pass@1 improvement of 6.96% over other methods, demonstrating its potential for more efficiently training more capable reasoning models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CLPO (Curriculum-guided Learning for Policy Optimization)ï¼Œè¿™æ˜¯ä¸€ç§å°†è¯¾ç¨‹å­¦ä¹ (Curriculum Learning)ä¸ç­–ç•¥ä¼˜åŒ–(Policy Optimization)ç›¸ç»“åˆçš„æ–°å‹ç®—æ³•ï¼Œæ—¨åœ¨å¢å¼ºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„æ¨ç†èƒ½åŠ›ã€‚é’ˆå¯¹ç°æœ‰å¸¦æœ‰å¯éªŒè¯å¥–åŠ±çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ (RLVR)æ–¹æ³•å¿½ç•¥é—®é¢˜éš¾åº¦å·®å¼‚ã€å¯¼è‡´å­¦ä¹ æ•ˆç‡å—é™çš„é—®é¢˜ï¼ŒCLPOæ„å»ºäº†ä¸€ä¸ªåŠ¨æ€æ•™å­¦åé¦ˆé—­ç¯ã€‚è¯¥ç®—æ³•åˆ©ç”¨æ¨¡å‹è‡ªèº«çš„é‡‡æ ·è¡¨ç°è¿›è¡Œå®æ—¶éš¾åº¦è¯„ä¼°ä»¥ç”Ÿæˆåœ¨çº¿è¯¾ç¨‹(Online Curriculum)ï¼Œå¹¶é€šè¿‡è‡ªé€‚åº”é—®é¢˜é‡æ„(Adaptive Problem Restructuring)æœºåˆ¶å®ç°æ¨¡å‹è‡ªæ•™ï¼Œå³å¤šæ ·åŒ–ä¸­ç­‰éš¾åº¦é—®é¢˜å¹¶ç®€åŒ–æéš¾é—®é¢˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCLPOåœ¨å…«ä¸ªæ•°å­¦å’Œé€šç”¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†SOTAæ€§èƒ½ï¼Œpass@1å¹³å‡æå‡è¾¾6.96%ã€‚è¿™ä¸€æ–¹æ³•å°†é™æ€è®­ç»ƒè½¬å˜ä¸ºéšæ¨¡å‹èƒ½åŠ›æ¼”è¿›çš„åŠ¨æ€è¿‡ç¨‹ï¼Œæ˜¾è‘—æå‡äº†è®­ç»ƒæ•ˆç‡å’Œæ¨ç†æ€§èƒ½ä¸Šé™ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25004v1",
      "published_date": "2025-09-29 16:29:04 UTC",
      "updated_date": "2025-09-29 16:29:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:51:51.269821+00:00"
    },
    {
      "arxiv_id": "2509.24988v1",
      "title": "Generalized Correctness Models: Learning Calibrated and Model-Agnostic Correctness Predictors from Historical Patterns",
      "title_zh": "å¹¿ä¹‰æ­£ç¡®æ€§æ¨¡å‹ï¼šåŸºäºå†å²æ¨¡å¼å­¦ä¹ ç»æ ¡å‡†ä¸”ä¸æ¨¡å‹æ— å…³çš„æ­£ç¡®æ€§é¢„æµ‹å™¨",
      "authors": [
        "Hanqi Xiao",
        "Vaidehi Patil",
        "Hyunji Lee",
        "Elias Stengel-Eskin",
        "Mohit Bansal"
      ],
      "abstract": "Generating accurate and calibrated confidence estimates is critical for deploying LLMs in high-stakes or user-facing applications, and remains an open challenge. Prior research has often framed confidence as a problem of eliciting a model's \"self-knowledge\", i.e., the ability of an LLM to judge whether its own answers are correct; this approach implicitly assumes that there is some privileged information about the answer's correctness that is accessible to the model itself. However, our experiments reveal that an LLM attempting to predict the correctness of its own outputs generally performs no better than an unrelated LLM. Moreover, we hypothesize that a key factor in building a \"Correctness Model\" (CM) is exposure to a target model's historical predictions. We propose multiple methods to inject this historical correctness information, creating a Generalized Correctness Model (GCM). We first show that GCMs can be trained on the correctness data from many LLMs and learn patterns for correctness prediction applicable across datasets and models. We then use CMs as a lens for studying the source of correctness prediction ability and its generalization, systematically controlling their training data and finding that answer phrasing is a strong predictor for correctness. We further explore alternative methods of injecting history without training an LLM, finding that including history as in-context examples can help improve correctness prediction, and post-hoc calibration can provide complementary reductions in calibration error. We evaluate GCMs based on Qwen3-8B across 5 model families and the MMLU and TriviaQA datasets, as well as on a downstream selective prediction task, finding that reliable LLM confidence estimation is a generalizable and model-agnostic skill learned by systematically encoding correctness history rather than a model-specific skill reliant on self-introspection.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆå‡†ç¡®ç½®ä¿¡åº¦è¯„ä¼°çš„æŒ‘æˆ˜ï¼Œå¹¶è´¨ç–‘äº†ä¼ ç»Ÿå°†ç½®ä¿¡åº¦è§†ä¸ºæ¨¡å‹â€œSelf-knowledgeâ€ï¼ˆè‡ªæˆ‘çŸ¥è¯†ï¼‰çš„è§‚ç‚¹ã€‚å®éªŒæ­ç¤ºï¼Œæ¨¡å‹é¢„æµ‹è‡ªèº«è¾“å‡ºæ­£ç¡®æ€§çš„è¡¨ç°å¹¶ä¸ä¼˜äºå…¶ä»–æ— å…³æ¨¡å‹ï¼Œè¡¨æ˜ç½®ä¿¡åº¦é¢„æµ‹å¹¶éä»…ä¾èµ–äºæ¨¡å‹ç‰¹æœ‰çš„å†…éƒ¨ä¿¡æ¯ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†é€šç”¨æ­£ç¡®æ€§æ¨¡å‹ï¼ˆGeneralized Correctness Model, GCMï¼‰ï¼Œé€šè¿‡æ³¨å…¥ç›®æ ‡æ¨¡å‹çš„å†å²é¢„æµ‹æ•°æ®ï¼Œä½¿å…¶èƒ½å¤Ÿå­¦ä¹ è·¨æ•°æ®é›†å’Œæ¨¡å‹çš„æ­£ç¡®æ€§é¢„æµ‹æ¨¡å¼ã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°ï¼Œç­”æ¡ˆçš„æªè¾ï¼ˆAnswer phrasingï¼‰æ˜¯é¢„æµ‹æ­£ç¡®æ€§çš„å…³é”®æŒ‡æ ‡ï¼Œå¹¶ç»“åˆIn-context learningå’ŒPost-hoc calibrationç­‰æ–¹æ³•æ˜¾è‘—é™ä½äº†æ ¡å‡†è¯¯å·®ã€‚åŸºäºQwen3-8Bçš„è¯„ä¼°ç»“æœè¯æ˜ï¼Œå¯é çš„ç½®ä¿¡åº¦è¯„ä¼°æ˜¯ä¸€ç§å¯ä»¥é€šè¿‡å†å²æ•°æ®ä¹ å¾—çš„ã€æ¨¡å‹æ— å…³ï¼ˆModel-agnosticï¼‰çš„é€šç”¨æŠ€èƒ½ï¼Œè€Œéå•çº¯ä¾èµ–äºæ¨¡å‹è‡ªèº«çš„å†…çœã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Code: https://github.com/The-Inscrutable-X/CalibratedModelAgnosticCorrectness",
      "pdf_url": "https://arxiv.org/pdf/2509.24988v1",
      "published_date": "2025-09-29 16:19:01 UTC",
      "updated_date": "2025-09-29 16:19:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:52:03.492877+00:00"
    },
    {
      "arxiv_id": "2509.24986v1",
      "title": "Light-SQ: Structure-aware Shape Abstraction with Superquadrics for Generated Meshes",
      "title_zh": "Light-SQï¼šé¢å‘ç”Ÿæˆç½‘æ ¼çš„è¶…äºŒæ¬¡æ›²é¢ç»“æ„æ„ŸçŸ¥å½¢çŠ¶æŠ½è±¡",
      "authors": [
        "Yuhan Wang",
        "Weikai Chen",
        "Zeyu Hu",
        "Runze Zhang",
        "Yingda Yin",
        "Ruoyu Wu",
        "Keyang Luo",
        "Shengju Qian",
        "Yiyan Ma",
        "Hongyi Li",
        "Yuan Gao",
        "Yuhuan Zhou",
        "Hao Luo",
        "Wan Wang",
        "Xiaobin Shen",
        "Zhaowei Li",
        "Kuixin Zhu",
        "Chuanlang Hong",
        "Yueyue Wang",
        "Lijie Feng",
        "Xin Wang",
        "Chen Change Loy"
      ],
      "abstract": "In user-generated-content (UGC) applications, non-expert users often rely on image-to-3D generative models to create 3D assets. In this context, primitive-based shape abstraction offers a promising solution for UGC scenarios by compressing high-resolution meshes into compact, editable representations. Towards this end, effective shape abstraction must therefore be structure-aware, characterized by low overlap between primitives, part-aware alignment, and primitive compactness. We present Light-SQ, a novel superquadric-based optimization framework that explicitly emphasizes structure-awareness from three aspects. (a) We introduce SDF carving to iteratively udpate the target signed distance field, discouraging overlap between primitives. (b) We propose a block-regrow-fill strategy guided by structure-aware volumetric decomposition, enabling structural partitioning to drive primitive placement. (c) We implement adaptive residual pruning based on SDF update history to surpress over-segmentation and ensure compact results. In addition, Light-SQ supports multiscale fitting, enabling localized refinement to preserve fine geometric details. To evaluate our method, we introduce 3DGen-Prim, a benchmark extending 3DGen-Bench with new metrics for both reconstruction quality and primitive-level editability. Extensive experiments demonstrate that Light-SQ enables efficient, high-fidelity, and editable shape abstraction with superquadrics for complex generated geometry, advancing the feasibility of 3D UGC creation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”¨æˆ·ç”Ÿæˆå†…å®¹(UGC)åº”ç”¨ä¸­3Dèµ„äº§çš„å‹ç¼©ä¸ç¼–è¾‘éœ€æ±‚ï¼Œæå‡ºäº†Light-SQï¼Œä¸€ä¸ªåŸºäºè¶…äºŒæ¬¡æ›²é¢(Superquadrics)çš„ç»“æ„æ„ŸçŸ¥å½¢çŠ¶æŠ½è±¡ä¼˜åŒ–æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ—¨åœ¨é€šè¿‡é™ä½åŸºå…ƒé‡å ã€é›¶ä»¶æ„ŸçŸ¥å¯¹é½å’Œå¢å¼ºåŸºå…ƒç´§å‡‘æ€§ï¼Œå®ç°å¯¹ç”Ÿæˆç½‘æ ¼(Generated Meshes)çš„é«˜è´¨é‡æŠ½è±¡ã€‚ä¸ºäº†å¢å¼ºç»“æ„æ„ŸçŸ¥ï¼ŒLight-SQ å¼•å…¥äº†SDFåˆ»èš€(SDF carving)æŠ€æœ¯ï¼Œé€šè¿‡è¿­ä»£æ›´æ–°æœ‰å‘è·ç¦»åœºæ¥å‡å°‘åŸºå…ƒé—´çš„é‡å ï¼Œå¹¶é‡‡ç”¨ç”±ç»“æ„æ„ŸçŸ¥ä½“ç´ åˆ†è§£å¼•å¯¼çš„â€œå—-é‡é•¿-å¡«å……â€(Block-regrow-fill)ç­–ç•¥æ¥é©±åŠ¨åŸºå…ƒçš„æ”¾ç½®ã€‚æ­¤å¤–ï¼ŒåŸºäºSDFæ›´æ–°å†å²çš„è‡ªé€‚åº”æ®‹ä½™ä¿®å‰ª(Adaptive residual pruning)èƒ½å¤Ÿæœ‰æ•ˆæŠ‘åˆ¶è¿‡åº¦åˆ†å‰²ï¼Œä¸”æ¡†æ¶æ”¯æŒå¤šå°ºåº¦æ‹Ÿåˆ(Multiscale fitting)ä»¥ä¿ç•™ç²¾ç»†å‡ ä½•ç»†èŠ‚ã€‚ç ”ç©¶è€…åŒæ­¥æ¨å‡ºäº†3DGen-PrimåŸºå‡†æµ‹è¯•é›†ï¼Œå¼•å…¥äº†è¯„ä¼°é‡å»ºè´¨é‡å’ŒåŸºå…ƒçº§å¯ç¼–è¾‘æ€§çš„æ–°æŒ‡æ ‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLight-SQ åœ¨å¤„ç†å¤æ‚å‡ ä½•ä½“æ—¶å®ç°äº†é«˜æ•ˆã€é«˜ä¿çœŸä¸”å¯ç¼–è¾‘çš„å½¢çŠ¶æŠ½è±¡ï¼Œæ˜¾è‘—æå‡äº†3D UGCåˆ›ä½œçš„å®é™…å¯è¡Œæ€§ã€‚",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.GR",
      "comment": "SIGGRAPH Asia 2025. Project Page https://johann.wang/Light-SQ/",
      "pdf_url": "https://arxiv.org/pdf/2509.24986v1",
      "published_date": "2025-09-29 16:18:32 UTC",
      "updated_date": "2025-09-29 16:18:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:52:13.587567+00:00"
    },
    {
      "arxiv_id": "2509.25297v2",
      "title": "Automatically Generating Web Applications from Requirements Via Multi-Agent Test-Driven Development",
      "title_zh": "åŸºäºå¤šæ™ºèƒ½ä½“æµ‹è¯•é©±åŠ¨å¼€å‘çš„éœ€æ±‚é©±åŠ¨ Web åº”ç”¨ç¨‹åºè‡ªåŠ¨ç”Ÿæˆ",
      "authors": [
        "Yuxuan Wan",
        "Tingshuo Liang",
        "Jiakai Xu",
        "Jingyu Xiao",
        "Yintong Huo",
        "Michael R. Lyu"
      ],
      "abstract": "Developing full-stack web applications is complex and time-intensive, demanding proficiency across diverse technologies and frameworks. Although recent advances in multimodal large language models (MLLMs) enable automated webpage generation from visual inputs, current solutions remain limited to front-end tasks and fail to deliver fully functional applications. In this work, we introduce TDDev, the first test-driven development (TDD)-enabled LLM-agent framework for end-to-end full-stack web application generation. Given a natural language description or design image, TDDev automatically derives executable test cases, generates front-end and back-end code, simulates user interactions, and iteratively refines the implementation until all requirements are satisfied. Our framework addresses key challenges in full-stack automation, including underspecified user requirements, complex interdependencies among multiple files, and the need for both functional correctness and visual fidelity. Through extensive experiments on diverse application scenarios, TDDev achieves a 14.4% improvement on overall accuracy compared to state-of-the-art baselines, demonstrating its effectiveness in producing reliable, high-quality web applications without requiring manual intervention.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…¨æ ˆ Web åº”ç”¨ç¨‹åºå¼€å‘ä¸­å¤æ‚ä¸”è€—æ—¶çš„æŠ€æœ¯æŒ‘æˆ˜ï¼Œæå‡ºäº†é¦–ä¸ªæ”¯æŒæµ‹è¯•é©±åŠ¨å¼€å‘ (Test-Driven Development) çš„å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“æ¡†æ¶ TDDevã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿæ¥æ”¶è‡ªç„¶è¯­è¨€æè¿°æˆ–è®¾è®¡å›¾åƒä½œä¸ºè¾“å…¥ï¼Œè‡ªåŠ¨æ´¾ç”Ÿå¯æ‰§è¡Œçš„æµ‹è¯•ç”¨ä¾‹ï¼Œå¹¶åŒæ­¥ç”Ÿæˆå‰ç«¯ä¸åç«¯ä»£ç ã€‚é€šè¿‡æ¨¡æ‹Ÿç”¨æˆ·äº¤äº’å’ŒæŒç»­çš„è¿­ä»£å®Œå–„ï¼ŒTDDev ç¡®ä¿äº†ç”Ÿæˆçš„åº”ç”¨ç¨‹åºåœ¨åŠŸèƒ½ä¸è§†è§‰ä¸Šå‡èƒ½æ»¡è¶³æ—¢å®šéœ€æ±‚ã€‚è¯¥æ–¹æ¡ˆæˆåŠŸè§£å†³äº†å…¨æ ˆè‡ªåŠ¨åŒ–ä¸­éœ€æ±‚æ¨¡ç³Šã€å¤šæ–‡ä»¶é—´å¤æ‚çš„ä¾èµ–å…³ç³»ä»¥åŠå…¼é¡¾åŠŸèƒ½æ­£ç¡®æ€§ä¸è§†è§‰ä¿çœŸåº¦ç­‰æ ¸å¿ƒéš¾é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTDDev åœ¨å¤šç§åº”ç”¨åœºæ™¯ä¸‹çš„ç»¼åˆå‡†ç¡®ç‡æ¯”ç°æœ‰æœ€å…ˆè¿›çš„åŸºçº¿æ¨¡å‹æå‡äº† 14.4%ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†åœ¨æ— éœ€äººå·¥å¹²é¢„çš„æƒ…å†µä¸‹ï¼Œè‡ªåŠ¨åŒ–ç”Ÿäº§å¯é ä¸”é«˜è´¨é‡ Web åº”ç”¨ç¨‹åºçš„é«˜æ•ˆæ€§ä¸å¯è¡Œæ€§ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25297v2",
      "published_date": "2025-09-29 16:18:19 UTC",
      "updated_date": "2025-10-01 17:32:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:52:14.692793+00:00"
    },
    {
      "arxiv_id": "2509.24981v1",
      "title": "Random Policy Valuation is Enough for LLM Reasoning with Verifiable Rewards",
      "title_zh": "å¯¹äºå…·æœ‰å¯éªŒè¯å¥–åŠ±çš„ LLM æ¨ç†ï¼Œéšæœºç­–ç•¥ä¼°å€¼è¶³çŸ£",
      "authors": [
        "Haoran He",
        "Yuxiao Ye",
        "Qingpeng Cai",
        "Chen Hu",
        "Binxing Jiao",
        "Daxin Jiang",
        "Ling Pan"
      ],
      "abstract": "RL with Verifiable Rewards (RLVR) has emerged as a promising paradigm for improving the reasoning abilities of large language models (LLMs). Current methods rely primarily on policy optimization frameworks like PPO and GRPO, which follow generalized policy iteration that alternates between evaluating the current policy's value and improving the policy based on evaluation. While effective, they often suffer from training instability and diversity collapse, requiring complex heuristic tricks and careful tuning. We observe that standard RLVR in math reasoning can be formalized as a specialized finite-horizon Markov Decision Process with deterministic state transitions, tree-structured dynamics, and binary terminal rewards. Though large in scale, the underlying structure is simpler than general-purpose control settings for which popular RL algorithms (e.g., PPO) were developed, suggesting that several sophisticated techniques in existing methods may be reduced or even omitted. Based on this insight, we prove a surprising result: the optimal action can be recovered from the Q-function of a fixed uniformly random policy, thereby bypassing the generalized policy iteration loop and its associated heuristics. We introduce Random Policy Valuation for Diverse Reasoning (ROVER) to translate this principle into a practical and scalable algorithm for LLM math reasoning, a minimalist yet highly effective RL method that samples actions from a softmax over these uniform-policy Q-values. ROVER preserves diversity throughout training, allowing sustained exploration of multiple valid pathways. Across multiple base models and standard math reasoning benchmarks, ROVER demonstrates superior performance in both \\textbf{quality} (\\textbf{+8.2} on pass@1, \\textbf{+16.8} on pass@256) and \\textbf{diversity} (\\textbf{+17.6\\%}), despite its radical simplification compared to strong, complicated existing methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ (RLVR)ä¸­é¢ä¸´çš„è®­ç»ƒä¸ç¨³å®šå’Œå¤šæ ·æ€§å´©æºƒç­‰é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºROVER (Random Policy Valuation for Diverse Reasoning) çš„æç®€ç®—æ³•æ¡†æ¶ã€‚ç ”ç©¶è€…å‘ç°æ•°å­¦æ¨ç†ä¸­çš„RLVRå¯ä»¥å½¢å¼åŒ–ä¸ºå…·æœ‰ç¡®å®šæ€§çŠ¶æ€è½¬ç§»å’ŒäºŒå…ƒç»ˆæ­¢å¥–åŠ±çš„æœ‰é™æ—¶ç•Œé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(MDP)ï¼Œå¹¶è¯æ˜äº†ä»å›ºå®šå‡åŒ€éšæœºç­–ç•¥çš„Q-functionä¸­å³å¯æ¢å¤æœ€ä¼˜åŠ¨ä½œï¼Œä»è€Œç»•è¿‡äº†å¤æ‚çš„å¹¿ä¹‰ç­–ç•¥è¿­ä»£(generalized policy iteration)å¾ªç¯åŠå…¶ç›¸å…³çš„å¯å‘å¼è°ƒä¼˜ã€‚ROVERé€šè¿‡å¯¹è¿™äº›å‡åŒ€ç­–ç•¥Qå€¼è¿›è¡Œsoftmaxé‡‡æ ·æ¥ç”ŸæˆåŠ¨ä½œï¼Œåœ¨ä¿æŒè®­ç»ƒç¨³å®šæ€§çš„åŒæ—¶ç¡®ä¿äº†å¯¹å¤šç§æœ‰æ•ˆæ¨ç†è·¯å¾„çš„æŒç»­æ¢ç´¢ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒROVERåœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸æ¯”PPOå’ŒGRPOç­‰å¤æ‚åŸºçº¿æ¨¡å‹ï¼Œå…¶pass@1å‡†ç¡®ç‡æå‡äº†8.2ï¼Œå¤šæ ·æ€§æå‡äº†17.6%ã€‚è¿™ç§æç®€åŒ–çš„æ–¹æ³•è¯æ˜äº†éšæœºç­–ç•¥ä¼°å€¼è¶³ä»¥åº”å¯¹å…·æœ‰å¯éªŒè¯å¥–åŠ±çš„æ¨ç†ä»»åŠ¡ï¼Œä¸ºæå‡å¤§è¯­è¨€æ¨¡å‹çš„é€»è¾‘æ¨ç†èƒ½åŠ›æä¾›äº†é«˜æ•ˆä¸”ç¨³å¥çš„æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "32 pages",
      "pdf_url": "https://arxiv.org/pdf/2509.24981v1",
      "published_date": "2025-09-29 16:09:07 UTC",
      "updated_date": "2025-09-29 16:09:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:52:12.591301+00:00"
    },
    {
      "arxiv_id": "2509.24978v4",
      "title": "Agentic Exploration of Physics Models",
      "title_zh": "åŸºäºæ™ºèƒ½ä½“çš„ç‰©ç†æ¨¡å‹æ¢ç´¢",
      "authors": [
        "Maximilian NÃ¤gele",
        "Florian Marquardt"
      ],
      "abstract": "The process of scientific discovery relies on an interplay of observations, analysis, and hypothesis generation. Machine learning is increasingly being adopted to address individual aspects of this process. However, it remains an open challenge to fully automate the heuristic, iterative loop required to discover the laws of an unknown system by exploring it through experiments and analysis, without tailoring the approach to the specifics of a given task. Here, we introduce SciExplorer, an agent that leverages large language model tool-use capabilities to enable exploration of systems without any domain-specific blueprints, and apply it to physical systems that are initially unknown to the agent. We test SciExplorer on a broad set of models spanning mechanical dynamical systems, wave evolution, and quantum many-body physics. Despite using a minimal set of tools, primarily based on code execution, we observe impressive performance on tasks such as recovering equations of motion from observed dynamics and inferring Hamiltonians from expectation values. The demonstrated effectiveness of this setup opens the door towards similar scientific exploration in other domains, without the need for finetuning or task-specific instructions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SciExplorerï¼Œä¸€ç§åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (Large Language Model) å·¥å…·è°ƒç”¨èƒ½åŠ›å®ç°è‡ªä¸»ç§‘å­¦æ¢ç´¢çš„æ™ºèƒ½ä½“ï¼Œæ—¨åœ¨è§£å†³åœ¨æ— å…ˆéªŒé¢†åŸŸçŸ¥è¯†çš„æƒ…å†µä¸‹è‡ªåŠ¨å‘ç°ç‰©ç†è§„å¾‹çš„æŒ‘æˆ˜ã€‚SciExplorer èƒ½å¤Ÿé€šè¿‡å®éªŒå’Œåˆ†æçš„å¯å‘å¼è¿­ä»£å¾ªç¯ï¼Œå¯¹æœºæ¢°åŠ¨åŠ›ç³»ç»Ÿã€æ³¢åŠ¨æ¼”åŒ–ä»¥åŠé‡å­å¤šä½“ç‰©ç†ç­‰å¤šç§ç‰©ç†æ¨¡å‹è¿›è¡Œæ¢ç´¢ã€‚è¯¥æ™ºèƒ½ä½“ä»…ä¾èµ–äºä»£ç æ‰§è¡Œç­‰æç®€å·¥å…·é›†ï¼Œåœ¨ä»è§‚æµ‹åŠ¨åŠ›å­¦ä¸­æ¢å¤è¿åŠ¨æ–¹ç¨‹ (equations of motion) ä»¥åŠä»æœŸæœ›å€¼ä¸­æ¨æ–­å“ˆå¯†é¡¿é‡ (Hamiltonians) ç­‰ä»»åŠ¡ä¸Šå±•ç°å‡ºäº†å“è¶Šæ€§èƒ½ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¿™ç§åŸºäº Agent çš„æ¢ç´¢æ–¹æ³•æ— éœ€é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œå¾®è°ƒæˆ–æŒ‡ä»¤å®šåˆ¶ï¼Œå³å¯å®ç°è·¨é¢†åŸŸçš„ç§‘å­¦å‘ç°ï¼Œä¸ºé€šç”¨åŒ–çš„è‡ªåŠ¨åŒ–ç§‘å­¦ç ”ç©¶å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cond-mat.quant-gas",
        "quant-ph"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24978v4",
      "published_date": "2025-09-29 16:07:05 UTC",
      "updated_date": "2026-01-07 13:59:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:52:45.390586+00:00"
    },
    {
      "arxiv_id": "2509.25296v1",
      "title": "Learning Relationships Between Separate Audio Tracks for Creative Applications",
      "title_zh": "é¢å‘åˆ›æ„åº”ç”¨çš„åˆ†ç¦»éŸ³è½¨é—´å…³ç³»å­¦ä¹ ",
      "authors": [
        "Balthazar Bujard",
        "JÃ©rÃ´me Nika",
        "FÃ©dÃ©ric Bevilacqua",
        "Nicolas Obin"
      ],
      "abstract": "This paper presents the first step in a research project situated within the field of musical agents. The objective is to achieve, through training, the tuning of the desired musical relationship between a live musical input and a real-time generated musical output, through the curation of a database of separated tracks. We propose an architecture integrating a symbolic decision module capable of learning and exploiting musical relationships from such musical corpus. We detail an offline implementation of this architecture employing Transformers as the decision module, associated with a perception module based on Wav2Vec 2.0, and concatenative synthesis as audio renderer. We present a quantitative evaluation of the decision module's ability to reproduce learned relationships extracted during training. We demonstrate that our decision module can predict a coherent track B when conditioned by its corresponding ''guide'' track A, based on a corpus of paired tracks (A, B).",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†éŸ³ä¹æ™ºèƒ½ä½“(musical agents)é¢†åŸŸï¼Œæ—¨åœ¨é€šè¿‡è®­ç»ƒå­¦ä¹ ç°åœºéŸ³ä¹è¾“å…¥ä¸å®æ—¶ç”Ÿæˆçš„éŸ³ä¹è¾“å‡ºä¹‹é—´çš„é¢„æœŸéŸ³ä¹å…³ç³»ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§æ•´åˆç¬¦å·å†³ç­–æ¨¡å—çš„æ¶æ„ï¼Œåˆ©ç”¨Transformersä½œä¸ºå†³ç­–æ ¸å¿ƒï¼Œå¹¶ç»“åˆåŸºäºWav2Vec 2.0çš„æ„ŸçŸ¥æ¨¡å—ä»¥åŠæ‹¼æ¥åˆæˆ(concatenative synthesis)ä½œä¸ºéŸ³é¢‘æ¸²æŸ“å™¨ã€‚è¯¥ç³»ç»Ÿé€šè¿‡ç­–åˆ’çš„åˆ†ç¦»éŸ³è½¨æ•°æ®åº“è¿›è¡Œè®­ç»ƒï¼Œæ—¨åœ¨å­¦ä¹ å¹¶åˆ©ç”¨éŸ³è½¨è¯­æ–™åº“ä¸­çš„ç‰¹å®šéŸ³ä¹è§„å¾‹ã€‚å®šé‡è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œåœ¨ç»™å®šå¼•å¯¼éŸ³è½¨(guide track) A çš„æ¡ä»¶ä¸‹ï¼Œè¯¥å†³ç­–æ¨¡å—èƒ½å¤ŸåŸºäºé…å¯¹éŸ³è½¨ (A, B) è¯­æ–™åº“å‡†ç¡®é¢„æµ‹å‡ºä¸å…¶åè°ƒä¸€è‡´çš„éŸ³è½¨ Bã€‚è¿™ä¸€æˆæœè¯æ˜äº†è¯¥æ¶æ„åœ¨åˆ›æ„åº”ç”¨ä¸­å¤ç°å­¦ä¹ åˆ°çš„å¤æ‚éŸ³ä¹å…³ç³»çš„èƒ½åŠ›ï¼Œä¸ºå®ç°å®æ—¶äº’åŠ¨çš„è‡ªä¸»éŸ³ä¹ç”Ÿæˆå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.HC",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25296v1",
      "published_date": "2025-09-29 16:06:21 UTC",
      "updated_date": "2025-09-29 16:06:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:52:36.185008+00:00"
    },
    {
      "arxiv_id": "2510.02373v1",
      "title": "A-MemGuard: A Proactive Defense Framework for LLM-Based Agent Memory",
      "title_zh": "A-MemGuardï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“è®°å¿†çš„ä¸»åŠ¨é˜²å¾¡æ¡†æ¶",
      "authors": [
        "Qianshan Wei",
        "Tengchao Yang",
        "Yaochen Wang",
        "Xinfeng Li",
        "Lijun Li",
        "Zhenfei Yin",
        "Yi Zhan",
        "Thorsten Holz",
        "Zhiqiang Lin",
        "XiaoFeng Wang"
      ],
      "abstract": "Large Language Model (LLM) agents use memory to learn from past interactions, enabling autonomous planning and decision-making in complex environments. However, this reliance on memory introduces a critical security risk: an adversary can inject seemingly harmless records into an agent's memory to manipulate its future behavior. This vulnerability is characterized by two core aspects: First, the malicious effect of injected records is only activated within a specific context, making them hard to detect when individual memory entries are audited in isolation. Second, once triggered, the manipulation can initiate a self-reinforcing error cycle: the corrupted outcome is stored as precedent, which not only amplifies the initial error but also progressively lowers the threshold for similar attacks in the future. To address these challenges, we introduce A-MemGuard (Agent-Memory Guard), the first proactive defense framework for LLM agent memory. The core idea of our work is the insight that memory itself must become both self-checking and self-correcting. Without modifying the agent's core architecture, A-MemGuard combines two mechanisms: (1) consensus-based validation, which detects anomalies by comparing reasoning paths derived from multiple related memories and (2) a dual-memory structure, where detected failures are distilled into ``lessons'' stored separately and consulted before future actions, breaking error cycles and enabling adaptation. Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost. This work shifts LLM memory security from static filtering to a proactive, experience-driven model where defenses strengthen over time. Our code is available in https://github.com/TangciuYueng/AMemGuard",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†A-MemGuardï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLM)æ™ºèƒ½ä½“è®°å¿†çš„ä¸»åŠ¨é˜²å¾¡æ¡†æ¶ï¼Œæ—¨åœ¨åº”å¯¹æ¶æ„æ³¨å…¥è®°å½•æ“çºµæ™ºèƒ½ä½“è¡Œä¸ºçš„å®‰å…¨é£é™©ã€‚ç ”ç©¶æŒ‡å‡ºè¿™ç±»æ”»å‡»å…·æœ‰ä¸Šä¸‹æ–‡è§¦å‘å’Œè‡ªæˆ‘å¼ºåŒ–é”™è¯¯å¾ªç¯(self-reinforcing error cycles)çš„ç‰¹å¾ï¼Œä½¿å¾—ä¼ ç»Ÿçš„å•æ¡ç›®å®¡è®¡éš¾ä»¥å¥æ•ˆã€‚A-MemGuardé‡‡ç”¨äº†åŸºäºå…±è¯†çš„éªŒè¯(consensus-based validation)æœºåˆ¶ï¼Œé€šè¿‡æ¯”è¾ƒå¤šä¸ªç›¸å…³è®°å¿†ç”Ÿæˆçš„æ¨ç†è·¯å¾„æ¥è¯†åˆ«å¼‚å¸¸ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†åŒé‡è®°å¿†ç»“æ„(dual-memory structure)ï¼Œå°†æ£€æµ‹åˆ°çš„å¤±è´¥æç‚¼ä¸ºâ€œæ•™è®­â€å¹¶ç‹¬ç«‹å­˜å‚¨ï¼Œä»¥ä¾¿åœ¨æœªæ¥çš„å†³ç­–ä¸­è¿›è¡Œå‚è€ƒï¼Œä»è€Œæ‰“ç ´é”™è¯¯å¾ªç¯ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒA-MemGuardåœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­æˆåŠŸå°†æ”»å‡»æˆåŠŸç‡é™ä½äº†95%ä»¥ä¸Šï¼ŒåŒæ—¶ä»…äº§ç”Ÿæå°çš„æ•ˆç”¨æˆæœ¬ã€‚è¿™é¡¹å·¥ä½œæ ‡å¿—ç€LLMè®°å¿†å®‰å…¨ä»é™æ€è¿‡æ»¤å‘ä¸»åŠ¨ã€ç»éªŒé©±åŠ¨æ¨¡å‹çš„è½¬å˜ï¼Œä½¿å¾—é˜²å¾¡ç³»ç»Ÿèƒ½å¤Ÿéšç€ç»éªŒç§¯ç´¯è€Œä¸æ–­åŠ å¼ºã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.02373v1",
      "published_date": "2025-09-29 16:04:15 UTC",
      "updated_date": "2025-09-29 16:04:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:52:43.097425+00:00"
    },
    {
      "arxiv_id": "2509.24967v4",
      "title": "SecInfer: Preventing Prompt Injection via Inference-time Scaling",
      "title_zh": "SecInferï¼šåŸºäºæ¨ç†æ—¶æ‰©å±•é˜²å¾¡æç¤ºè¯æ³¨å…¥",
      "authors": [
        "Yupei Liu",
        "Yanting Wang",
        "Yuqi Jia",
        "Jinyuan Jia",
        "Neil Zhenqiang Gong"
      ],
      "abstract": "Prompt injection attacks pose a pervasive threat to the security of Large Language Models (LLMs). State-of-the-art prevention-based defenses typically rely on fine-tuning an LLM to enhance its security, but they achieve limited effectiveness against strong attacks. In this work, we propose \\emph{SecInfer}, a novel defense against prompt injection attacks built on \\emph{inference-time scaling}, an emerging paradigm that boosts LLM capability by allocating more compute resources for reasoning during inference. SecInfer consists of two key steps: \\emph{system-prompt-guided sampling}, which generates multiple responses for a given input by exploring diverse reasoning paths through a varied set of system prompts, and \\emph{target-task-guided aggregation}, which selects the response most likely to accomplish the intended task. Extensive experiments show that, by leveraging additional compute at inference, SecInfer effectively mitigates both existing and adaptive prompt injection attacks, outperforming state-of-the-art defenses as well as existing inference-time scaling approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SecInferï¼Œä¸€ç§åˆ©ç”¨æ¨ç†æ—¶æ‰©å±• (inference-time scaling) æ¥é˜²å¾¡æç¤ºæ³¨å…¥æ”»å‡» (prompt injection attacks) çš„æ–°é¢–æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åŸºäºå¾®è°ƒçš„é˜²å¾¡æ‰‹æ®µåœ¨é¢å¯¹å¼ºåŠ›æ”»å‡»æ—¶æœ‰æ•ˆæ€§å—é™çš„é—®é¢˜ã€‚SecInfer çš„æ ¸å¿ƒåœ¨äºæ¨ç†é˜¶æ®µé€šè¿‡åˆ†é…æ›´å¤šè®¡ç®—èµ„æºæ¥å¢å¼ºæ¨¡å‹çš„å®‰å…¨æ€§ï¼Œä¸»è¦åŒ…å«ç³»ç»Ÿæç¤ºå¼•å¯¼é‡‡æ · (system-prompt-guided sampling) å’Œç›®æ ‡ä»»åŠ¡å¼•å¯¼èšåˆ (target-task-guided aggregation) ä¸¤ä¸ªå…³é”®æ­¥éª¤ã€‚è¯¥æ–¹æ³•é¦–å…ˆåˆ©ç”¨å¤šæ ·åŒ–çš„ç³»ç»Ÿæç¤ºæ¢ç´¢ä¸åŒçš„æ¨ç†è·¯å¾„ä»¥ç”Ÿæˆå¤šç»„å›å¤ï¼Œéšåé€šè¿‡èšåˆæœºåˆ¶ç­›é€‰å‡ºæœ€ç¬¦åˆç›®æ ‡ä»»åŠ¡è¦æ±‚çš„å“åº”ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒSecInfer èƒ½æœ‰æ•ˆç¼“è§£ç°æœ‰åŠè‡ªé€‚åº”çš„æç¤ºæ³¨å…¥æ”»å‡»ï¼Œå…¶æ€§èƒ½ä¼˜äºç›®å‰æœ€å…ˆè¿›çš„é˜²å¾¡æŠ€æœ¯ä»¥åŠç°æœ‰çš„æ¨ç†æ—¶æ‰©å±•æ–¹æ¡ˆã€‚è¿™ä¸€ç ”ç©¶ä¸ºæå‡å¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„é²æ£’æ€§æä¾›äº†ä¸€ç§æ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹çš„é«˜æ•ˆé˜²å¾¡è·¯å¾„ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24967v4",
      "published_date": "2025-09-29 16:00:41 UTC",
      "updated_date": "2025-11-14 03:22:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:52:54.293029+00:00"
    },
    {
      "arxiv_id": "2509.24956v1",
      "title": "MSG: Multi-Stream Generative Policies for Sample-Efficient Robotic Manipulation",
      "title_zh": "MSGï¼šé¢å‘æ ·æœ¬é«˜æ•ˆæœºå™¨äººæ“ä½œçš„å¤šæµç”Ÿæˆå¼ç­–ç•¥",
      "authors": [
        "Jan Ole von Hartz",
        "Lukas Schweizer",
        "Joschka Boedecker",
        "Abhinav Valada"
      ],
      "abstract": "Generative robot policies such as Flow Matching offer flexible, multi-modal policy learning but are sample-inefficient. Although object-centric policies improve sample efficiency, it does not resolve this limitation. In this work, we propose Multi-Stream Generative Policy (MSG), an inference-time composition framework that trains multiple object-centric policies and combines them at inference to improve generalization and sample efficiency. MSG is model-agnostic and inference-only, hence widely applicable to various generative policies and training paradigms. We perform extensive experiments both in simulation and on a real robot, demonstrating that our approach learns high-quality generative policies from as few as five demonstrations, resulting in a 95% reduction in demonstrations, and improves policy performance by 89 percent compared to single-stream approaches. Furthermore, we present comprehensive ablation studies on various composition strategies and provide practical recommendations for deployment. Finally, MSG enables zero-shot object instance transfer. We make our code publicly available at https://msg.cs.uni-freiburg.de.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Flow Matching ç­‰ç”Ÿæˆå¼æœºå™¨äººç­–ç•¥(Generative robot policies)åœ¨æœºå™¨äººæ“ä½œä¸­æ ·æœ¬æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œæå‡ºäº† MSG (Multi-Stream Generative Policy) æ¡†æ¶ã€‚MSG æ˜¯ä¸€ç§æ¨ç†æ—¶ç»„åˆæ¡†æ¶(inference-time composition framework)ï¼Œå®ƒé€šè¿‡è®­ç»ƒå¤šä¸ªä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„ç­–ç•¥(object-centric policies)ï¼Œå¹¶åœ¨æ¨ç†é˜¶æ®µå°†å®ƒä»¬è¿›è¡Œç»„åˆï¼Œä»¥æ˜¾è‘—æå‡æ³›åŒ–èƒ½åŠ›å’Œæ ·æœ¬æ•ˆç‡ã€‚è¯¥æ–¹æ³•å…·æœ‰æ¨¡å‹æ— å…³æ€§(model-agnostic)ä¸”ä»…éœ€åœ¨æ¨ç†ç«¯æ‰§è¡Œ(inference-only)ï¼Œå› æ­¤å¯å¹¿æ³›åº”ç”¨äºå„ç§ç”Ÿæˆå¼ç­–ç•¥å’Œè®­ç»ƒèŒƒå¼ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMSG èƒ½å¤Ÿä»…ä» 5 ä¸ªæ¼”ç¤º(demonstrations)ä¸­å­¦ä¹ é«˜è´¨é‡çš„ç­–ç•¥ï¼Œä½¿æ‰€éœ€æ¼”ç¤ºæ•°é‡å‡å°‘äº† 95%ï¼Œä¸”åœ¨æ€§èƒ½ä¸Šæ¯”å•æµ(single-stream)æ–¹æ³•æå‡äº† 89%ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜å®ç°äº†é›¶æ ·æœ¬ç‰©ä½“å®ä¾‹è¿ç§»(zero-shot object instance transfer)ï¼Œä¸ºå®ç°é«˜æ•ˆä¸”å…·å¤‡æ³›åŒ–èƒ½åŠ›çš„æœºå™¨äººæ“ä½œä»»åŠ¡æä¾›äº†æœ‰åŠ›çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24956v1",
      "published_date": "2025-09-29 15:50:51 UTC",
      "updated_date": "2025-09-29 15:50:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:52:59.387347+00:00"
    },
    {
      "arxiv_id": "2509.24947v1",
      "title": "Learning Distinguishable Representations in Deep Q-Networks for Linear Transfer",
      "title_zh": "æ·±åº¦Qç½‘ç»œä¸­é¢å‘çº¿æ€§è¿ç§»çš„å¯åŒºåˆ†è¡¨ç¤ºå­¦ä¹ ",
      "authors": [
        "Sooraj Sathish",
        "Keshav Goyal",
        "Raghuram Bharadwaj Diddigi"
      ],
      "abstract": "Deep Reinforcement Learning (RL) has demonstrated success in solving complex sequential decision-making problems by integrating neural networks with the RL framework. However, training deep RL models poses several challenges, such as the need for extensive hyperparameter tuning and high computational costs. Transfer learning has emerged as a promising strategy to address these challenges by enabling the reuse of knowledge from previously learned tasks for new, related tasks. This avoids the need for retraining models entirely from scratch. A commonly used approach for transfer learning in RL is to leverage the internal representations learned by the neural network during training. Specifically, the activations from the last hidden layer can be viewed as refined state representations that encapsulate the essential features of the input. In this work, we investigate whether these representations can be used as input for training simpler models, such as linear function approximators, on new tasks. We observe that the representations learned by standard deep RL models can be highly correlated, which limits their effectiveness when used with linear function approximation. To mitigate this problem, we propose a novel deep Q-learning approach that introduces a regularization term to reduce positive correlations between feature representation of states. By leveraging these reduced correlated features, we enable more effective use of linear function approximation in transfer learning. Through experiments and ablation studies on standard RL benchmarks and MinAtar games, we demonstrate the efficacy of our approach in improving transfer learning performance and thereby reducing computational overhead.",
      "tldr_zh": "æ·±åº¦å¼ºåŒ–å­¦ä¹ (Deep RL)åœ¨è§£å†³å¤æ‚åºåˆ—å†³ç­–é—®é¢˜æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†å…¶é«˜æ˜‚çš„è®¡ç®—æˆæœ¬å’Œå¤æ‚çš„è¶…å‚æ•°è°ƒä¼˜ä¸ºæ–°ä»»åŠ¡çš„è®­ç»ƒå¸¦æ¥äº†æŒ‘æˆ˜ã€‚è™½ç„¶åˆ©ç”¨ç¥ç»ç½‘ç»œæœ€åä¸€å±‚éšè—å±‚å­¦ä¹ åˆ°çš„ç‰¹å¾è¿›è¡Œè¿ç§»å­¦ä¹ (Transfer Learning)æ˜¯ä¸€ç§å¸¸ç”¨ç­–ç•¥ï¼Œä½†æ ‡å‡†æ¨¡å‹æå–çš„çŠ¶æ€è¡¨ç¤ºå¾€å¾€å­˜åœ¨é«˜åº¦ç›¸å…³æ€§ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨é…åˆçº¿æ€§å‡½æ•°è¿‘ä¼¼å™¨(Linear Function Approximator)æ—¶çš„æœ‰æ•ˆæ€§ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ”¹è¿›çš„æ·±åº¦Qç½‘ç»œ(Deep Q-Networks)è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥ç‰¹å®šçš„æ­£åˆ™åŒ–é¡¹(Regularization Term)æ¥å‡å°‘çŠ¶æ€ç‰¹å¾è¡¨ç¤ºä¹‹é—´çš„æ­£ç›¸å…³æ€§ã€‚è¯¥æ–¹æ³•æ—¨åœ¨å­¦ä¹ æ›´å…·è¾¨è¯†åº¦ä¸”ä½ç›¸å…³æ€§çš„ç‰¹å¾ï¼Œä»è€Œä½¿ç®€å•çš„çº¿æ€§æ¨¡å‹èƒ½å¤Ÿåœ¨è¿ç§»å­¦ä¹ ä¸­æ›´é«˜æ•ˆåœ°å¤ç”¨çŸ¥è¯†ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨æ ‡å‡†RLåŸºå‡†å’ŒMinAtaræ¸¸æˆä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ï¼Œç»“æœè¯æ˜è¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†è¿ç§»å­¦ä¹ çš„æ€§èƒ½ï¼Œå¹¶æœ‰æ•ˆé™ä½äº†è®­ç»ƒè¿‡ç¨‹ä¸­çš„è®¡ç®—å¼€é”€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24947v1",
      "published_date": "2025-09-29 15:44:35 UTC",
      "updated_date": "2025-09-29 15:44:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:53:08.582343+00:00"
    },
    {
      "arxiv_id": "2509.24945v2",
      "title": "MobileLLM-R1: Exploring the Limits of Sub-Billion Language Model Reasoners with Open Training Recipes",
      "title_zh": "MobileLLM-R1ï¼šåŸºäºå¼€æ”¾è®­ç»ƒæ–¹æ¡ˆæ¢ç´¢åäº¿å‚æ•°ä»¥ä¸‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æé™",
      "authors": [
        "Changsheng Zhao",
        "Ernie Chang",
        "Zechun Liu",
        "Chia-Jung Chang",
        "Wei Wen",
        "Chen Lai",
        "Sheng Cao",
        "Yuandong Tian",
        "Raghuraman Krishnamoorthi",
        "Yangyang Shi",
        "Vikas Chandra"
      ],
      "abstract": "The paradigm shift in large language models (LLMs) from instinctive responses to chain-of-thought (CoT) reasoning has fueled two prevailing assumptions: (1) reasoning capabilities only emerge in sufficiently large models, and (2) such capabilities require training on massive datasets. While the first assumption has already been challenged by recent sub-billion-parameter reasoning models such as Qwen3-0.6B and DeepSeek distilled variants, the second remains largely unquestioned. In this work, we revisit the necessity of scaling to extremely large corpora (>10T tokens) for reasoning emergence. By carefully curating and resampling open-source datasets that we identify as beneficial under our designed metrics, we demonstrate that strong reasoning abilities can emerge with far less data. Specifically, we show that only ~2T tokens of high-quality data are sufficient, and pre-training with 4.2T tokens on the dataset resampled from these ~2T tokens, followed by a established post-training procedure, enables the development of MobileLLM-R1, a series of sub-billion-parameter reasoning models that substantially outperform prior models trained on fully open-sourced data. For example, MobileLLM-R1-950M achieves an AIME score of 15.5, compared to just 0.6 for OLMo-2-1.48B and 0.3 for SmolLM-2-1.7B. Remarkably, despite being trained on only 11.7% of the tokens compared to Qwen3's proprietary 36T-token corpus for pretraining, MobileLLM-R1-950M matches or surpasses Qwen3-0.6B across multiple reasoning benchmarks. To facilitate further research in this direction, we have released the complete training recipe, data sources, data mixing ratio, and model checkpoints, together with the key insights obtained throughout this study.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äºšåäº¿å‚æ•°(sub-billion)è¯­è¨€æ¨¡å‹åœ¨é“¾å¼æ€ç»´(Chain-of-Thought)æ¨ç†èƒ½åŠ›æ–¹é¢çš„æé™ï¼Œå¹¶æŒ‘æˆ˜äº†æ¨ç†èƒ½åŠ›å‡ºç°å¿…é¡»ä¾èµ–è¶…å¤§è§„æ¨¡æ•°æ®é›†çš„ä¼ ç»Ÿå‡è®¾ã€‚ä½œè€…é€šè¿‡ç²¾å¿ƒç­–åˆ’å’Œé‡é‡‡æ ·é«˜ä»·å€¼çš„å¼€æºæ•°æ®é›†ï¼Œè¯æ˜äº†ä»…éœ€çº¦2T tokensçš„é«˜è´¨é‡æ•°æ®å³å¯æ¿€å‘æ¨¡å‹å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶å›¢é˜Ÿæ¨å‡ºäº†MobileLLM-R1ç³»åˆ—æ¨¡å‹ï¼Œé€šè¿‡4.2T tokensçš„é¢„è®­ç»ƒå’Œæˆç†Ÿçš„åè®­ç»ƒæµç¨‹ï¼Œä½¿å…¶è¡¨ç°æ˜¾è‘—ä¼˜äºæ­¤å‰åŸºäºå¼€æºæ•°æ®è®­ç»ƒçš„åŒç±»æ¨¡å‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMobileLLM-R1-950Måœ¨AIMEåŸºå‡†æµ‹è¯•ä¸­å¾—åˆ†è¾¾åˆ°15.5ï¼Œè¿œè¶…è§„æ¨¡æ›´å¤§çš„OLMo-2-1.48Bå’ŒSmolLM-2-1.7Bã€‚å°½ç®¡è®­ç»ƒæ•°æ®é‡ä»…ä¸ºQwen3ç§æœ‰æ•°æ®é›†çš„11.7%ï¼ŒMobileLLM-R1-950Måœ¨å¤šä¸ªæ¨ç†è¯„ä¼°æŒ‡æ ‡ä¸Šä»èƒ½ä¸Qwen3-0.6BæŒå¹³ç”šè‡³è¶…è¶Šã€‚è¯¥é¡¹ç›®å…¨é¢å…¬å¼€äº†è®­ç»ƒé…æ–¹(training recipe)ã€æ•°æ®æºã€æ··åˆæ¯”ä¾‹åŠæ¨¡å‹æƒé‡ï¼Œä¸ºå¼€æºç¤¾åŒºè¿›ä¸€æ­¥ç ”ç©¶å°æ¨¡å‹æ¨ç†èƒ½åŠ›æä¾›äº†å…³é”®è§è§£ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Model: https://huggingface.co/collections/facebook/mobilellm-r1-68c4597b104fac45f28f448e",
      "pdf_url": "https://arxiv.org/pdf/2509.24945v2",
      "published_date": "2025-09-29 15:43:59 UTC",
      "updated_date": "2025-09-30 18:16:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:53:23.397060+00:00"
    },
    {
      "arxiv_id": "2509.24935v1",
      "title": "Scalable GANs with Transformers",
      "title_zh": "åŸºäº Transformer çš„å¯æ‰©å±•ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ",
      "authors": [
        "Sangeek Hyun",
        "MinKyu Lee",
        "Jae-Pil Heo"
      ],
      "abstract": "Scalability has driven recent advances in generative modeling, yet its principles remain underexplored for adversarial learning. We investigate the scalability of Generative Adversarial Networks (GANs) through two design choices that have proven to be effective in other types of generative models: training in a compact Variational Autoencoder latent space and adopting purely transformer-based generators and discriminators. Training in latent space enables efficient computation while preserving perceptual fidelity, and this efficiency pairs naturally with plain transformers, whose performance scales with computational budget. Building on these choices, we analyze failure modes that emerge when naively scaling GANs. Specifically, we find issues as underutilization of early layers in the generator and optimization instability as the network scales. Accordingly, we provide simple and scale-friendly solutions as lightweight intermediate supervision and width-aware learning-rate adjustment. Our experiments show that GAT, a purely transformer-based and latent-space GANs, can be easily trained reliably across a wide range of capacities (S through XL). Moreover, GAT-XL/2 achieves state-of-the-art single-step, class-conditional generation performance (FID of 2.96) on ImageNet-256 in just 40 epochs, 6x fewer epochs than strong baselines.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ(GANs)çš„å¯æ‰©å±•æ€§ï¼Œæå‡ºäº†ç»“åˆå˜åˆ†è‡ªç¼–ç å™¨æ½œç©ºé—´(Variational Autoencoder latent space)ä¸çº¯Transformeræ¶æ„çš„ç”Ÿæˆæ¨¡å‹ã€‚é’ˆå¯¹GANsåœ¨è§„æ¨¡æ‰©å±•è¿‡ç¨‹ä¸­å‡ºç°çš„ç”Ÿæˆå™¨æ—©æœŸå±‚åˆ©ç”¨ä¸è¶³åŠä¼˜åŒ–ä¸ç¨³å®šç­‰å¤±æ•ˆæ¨¡å¼ï¼Œç ”ç©¶å¼•å…¥äº†è½»é‡çº§ä¸­é—´ç›‘ç£(intermediate supervision)å’Œå®½åº¦æ„ŸçŸ¥å­¦ä¹ ç‡è°ƒæ•´(width-aware learning-rate adjustment)ä¸¤ç§è§£å†³æ–¹æ¡ˆã€‚åŸºäºä¸Šè¿°ç­–ç•¥æ„å»ºçš„GATæ¨¡å‹èƒ½å¤Ÿåœ¨ä¸åŒå‚æ•°è§„æ¨¡ä¸‹å®ç°å¯é è®­ç»ƒï¼Œæ˜¾è‘—æå‡äº†è®¡ç®—æ•ˆç‡ä¸ç”Ÿæˆè´¨é‡ã€‚å®éªŒè¡¨æ˜ï¼ŒGAT-XL/2åœ¨ImageNet-256æ•°æ®é›†ä¸Šä»…é€šè¿‡40ä¸ªepochä¾¿å–å¾—äº†2.96çš„FIDåˆ†æ•°ï¼Œä¸ä»…åˆ·æ–°äº†å•æ­¥ç±»æ¡ä»¶ç”Ÿæˆçš„æ€§èƒ½è®°å½•ï¼Œå…¶è®­ç»ƒé€Ÿåº¦ä¹Ÿæ¯”åŒç±»åŸºçº¿æ¨¡å‹å¿«6å€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24935v1",
      "published_date": "2025-09-29 15:36:15 UTC",
      "updated_date": "2025-09-29 15:36:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:53:26.091350+00:00"
    },
    {
      "arxiv_id": "2509.24934v1",
      "title": "KIRETT -- A wearable device to support rescue operations using artificial intelligence to improve first aid",
      "title_zh": "KIRETTï¼šä¸€ç§åˆ©ç”¨äººå·¥æ™ºèƒ½è¾…åŠ©æ•‘æ´è¡ŒåŠ¨å¹¶æå‡æ€¥æ•‘æ•ˆèƒ½çš„å¯ç©¿æˆ´è®¾å¤‡",
      "authors": [
        "Johannes Zenkert",
        "Christian Weber",
        "Mubaris Nadeem",
        "Lisa Bender",
        "Madjid Fathi",
        "Abu Shad Ahammed",
        "Aniebiet Micheal Ezekiel",
        "Roman Obermaisser",
        "Maximilian Bradford"
      ],
      "abstract": "This short paper presents first steps in the scientific part of the KIRETT project, which aims to improve first aid during rescue operations using a wearable device. The wearable is used for computer-aided situation recognition by means of artificial intelligence. It provides contextual recommendations for actions and operations to rescue personnel and is intended to minimize damage to patients due to incorrect treatment, as well as increase the probability of survival. The paper describes a first overview of research approaches within the project.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† KIRETT é¡¹ç›®çš„åˆæ­¥æˆæœï¼Œæ—¨åœ¨å¼€å‘ä¸€ç§ç”¨äºæ”¯æŒæ•‘æ´è¡ŒåŠ¨çš„å¯ç©¿æˆ´è®¾å¤‡ (wearable device)ï¼Œä»¥æ˜¾è‘—æå‡æ€¥æ•‘åŒ»ç–—çš„æ•ˆç‡ä¸è´¨é‡ã€‚è¯¥è®¾å¤‡åˆ©ç”¨äººå·¥æ™ºèƒ½ (artificial intelligence) æŠ€æœ¯è¿›è¡Œè®¡ç®—æœºè¾…åŠ©çš„æƒ…å¢ƒè¯†åˆ« (situation recognition)ï¼Œèƒ½å¤Ÿä¸ºæ•‘æ´äººå‘˜æä¾›å®æ—¶çš„ä¸Šä¸‹æ–‡æ“ä½œå»ºè®®ã€‚é€šè¿‡è¿™ç§æ™ºèƒ½åŒ–çš„å†³ç­–æ”¯æŒï¼ŒKIRETT æ—¨åœ¨æœ€å¤§é™åº¦åœ°å‡å°‘å› å¤„ç½®ä¸å½“ç»™æ‚£è€…å¸¦æ¥çš„ä¼¤å®³ï¼Œå¹¶æœ‰æ•ˆæé«˜å…¶ç”Ÿå­˜æ¦‚ç‡ã€‚æœ¬æ–‡è¯¦ç»†é˜è¿°äº†è¯¥é¡¹ç›®åˆæœŸçš„ç ”ç©¶æ–¹æ³•ä¸æŠ€æœ¯è·¯å¾„ï¼Œä¸ºæœªæ¥åœ¨å¤æ‚æ•‘æ´åœºæ™¯ä¸­åº”ç”¨ AI æŠ€æœ¯æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Conference Paper for 2022 IEEE International Smart Cities Conference (ISC2), KIRETT Project, University of Siegen, Germany",
      "pdf_url": "https://arxiv.org/pdf/2509.24934v1",
      "published_date": "2025-09-29 15:36:14 UTC",
      "updated_date": "2025-09-29 15:36:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:53:28.603055+00:00"
    },
    {
      "arxiv_id": "2509.24927v1",
      "title": "When Autonomous Vehicle Meets V2X Cooperative Perception: How Far Are We?",
      "title_zh": "è‡ªåŠ¨é©¾é©¶ä¸V2XååŒæ„ŸçŸ¥ï¼šç°çŠ¶è¯„ä¼°ä¸æœªæ¥æŒ‘æˆ˜",
      "authors": [
        "An Guo",
        "Shuoxiao Zhang",
        "Enyi Tang",
        "Xinyu Gao",
        "Haomin Pang",
        "Haoxiang Tian",
        "Yanzhou Mu",
        "Wu Wen",
        "Chunrong Fang",
        "Zhenyu Chen"
      ],
      "abstract": "With the tremendous advancement of deep learning and communication technology, Vehicle-to-Everything (V2X) cooperative perception has the potential to address limitations in sensing distant objects and occlusion for a single-agent perception system. V2X cooperative perception systems are software systems characterized by diverse sensor types and cooperative agents, varying fusion schemes, and operation under different communication conditions. Therefore, their complex composition gives rise to numerous operational challenges. Furthermore, when cooperative perception systems produce erroneous predictions, the types of errors and their underlying causes remain insufficiently explored. To bridge this gap, we take an initial step by conducting an empirical study of V2X cooperative perception. To systematically evaluate the impact of cooperative perception on the ego vehicle's perception performance, we identify and analyze six prevalent error patterns in cooperative perception systems. We further conduct a systematic evaluation of the critical components of these systems through our large-scale study and identify the following key findings: (1) The LiDAR-based cooperation configuration exhibits the highest perception performance; (2) Vehicle-to-infrastructure (V2I) and vehicle-to-vehicle (V2V) communication exhibit distinct cooperative perception performance under different fusion schemes; (3) Increased cooperative perception errors may result in a higher frequency of driving violations; (4) Cooperative perception systems are not robust against communication interference when running online. Our results reveal potential risks and vulnerabilities in critical components of cooperative perception systems. We hope that our findings can better promote the design and repair of cooperative perception systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å•æ™ºèƒ½ä½“æ„ŸçŸ¥åœ¨é®æŒ¡å’Œè¿œè·ç¦»ç‰©ä½“æ¢æµ‹æ–¹é¢çš„å±€é™æ€§ï¼Œå¯¹ Vehicle-to-Everything (V2X) ååŒæ„ŸçŸ¥ç³»ç»Ÿè¿›è¡Œäº†æ·±å…¥çš„å®è¯ç ”ç©¶ã€‚é€šè¿‡è¯†åˆ«å¹¶åˆ†æç³»ç»Ÿä¸­çš„å…­ç§å¸¸è§é”™è¯¯æ¨¡å¼ï¼Œç ”ç©¶å›¢é˜Ÿå¯¹å…³é”®ç»„ä»¶è¿›è¡Œäº†å¤§è§„æ¨¡è¯„ä¼°ï¼Œå‘ç°åŸºäº LiDAR çš„åä½œé…ç½®å…·æœ‰æœ€ä¼˜çš„æ„ŸçŸ¥æ€§èƒ½ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº† Vehicle-to-Infrastructure (V2I) ä¸ Vehicle-to-Vehicle (V2V) åœ¨ä¸åŒèåˆæ–¹æ¡ˆä¸‹çš„æ€§èƒ½å·®å¼‚ï¼Œå¹¶æŒ‡å‡ºæ„ŸçŸ¥é”™è¯¯çš„å¢åŠ ä¼šå¯¼è‡´é©¾é©¶è¿è§„é¢‘ç‡ä¸Šå‡ã€‚æ­¤å¤–ï¼Œå®éªŒè¡¨æ˜ååŒæ„ŸçŸ¥ç³»ç»Ÿåœ¨åœ¨çº¿è¿è¡Œæ—¶å¯¹é€šä¿¡å¹²æ‰°ç¼ºä¹é²æ£’æ€§ã€‚è¯¥é¡¹å·¥ä½œè¯†åˆ«äº†ååŒæ„ŸçŸ¥ç³»ç»Ÿçš„æ½œåœ¨é£é™©ä¸æ¼æ´ï¼Œä¸ºæœªæ¥ç³»ç»Ÿçš„å¯é æ€§è®¾è®¡ä¸ä¿®å¤æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.AI",
        "cs.RO",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "The paper has been accepted by the 40th IEEE/ACM International Conference on Automated Software Engineering, ASE 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.24927v1",
      "published_date": "2025-09-29 15:28:27 UTC",
      "updated_date": "2025-09-29 15:28:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:53:41.982844+00:00"
    },
    {
      "arxiv_id": "2510.00067v1",
      "title": "Intelligent 5S Audit: Application of Artificial Intelligence for Continuous Improvement in the Automotive Industry",
      "title_zh": "æ™ºèƒ½ 5S å®¡æ ¸ï¼šäººå·¥æ™ºèƒ½åœ¨æ±½è½¦è¡Œä¸šæŒç»­æ”¹è¿›ä¸­çš„åº”ç”¨",
      "authors": [
        "Rafael da Silva Maciel",
        "Lucio Veraldo"
      ],
      "abstract": "The evolution of the 5S methodology with the support of artificial intelligence techniques represents a significant opportunity to improve industrial organization audits in the automotive chain, making them more objective, efficient and aligned with Industry 4.0 standards. This work developed an automated 5S audit system based on large-scale language models (LLM), capable of assessing the five senses (Seiri, Seiton, Seiso, Seiketsu, Shitsuke) in a standardized way through intelligent image analysis. The system's reliability was validated using Cohen's concordance coefficient (kappa = 0.75), showing strong alignment between the automated assessments and the corresponding human audits. The results indicate that the proposed solution contributes significantly to continuous improvement in automotive manufacturing environments, speeding up the audit process by 50% of the traditional time and maintaining the consistency of the assessments, with a 99.8% reduction in operating costs compared to traditional manual audits. The methodology presented establishes a new paradigm for integrating lean systems with emerging AI technologies, offering scalability for implementation in automotive plants of different sizes.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ±½è½¦è¡Œä¸šçš„ 5S å®¡è®¡éœ€æ±‚ï¼Œå¼€å‘äº†ä¸€å¥—åŸºäºå¤§è¯­è¨€æ¨¡å‹ (LLM) çš„è‡ªåŠ¨åŒ–å®¡è®¡ç³»ç»Ÿï¼Œæ—¨åœ¨æå‡å·¥ä¸šå®¡è®¡çš„å®¢è§‚æ€§ä¸æ•ˆç‡ä»¥ç¬¦åˆ Industry 4.0 æ ‡å‡†ã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨æ™ºèƒ½å›¾åƒåˆ†ææŠ€æœ¯ï¼Œå®ç°äº†å¯¹ 5S æ–¹æ³•è®ºä¸­æ•´ç† (Seiri)ã€æ•´é¡¿ (Seiton)ã€æ¸…æ‰« (Seiso)ã€æ¸…æ´ (Seiketsu) å’Œç´ å…» (Shitsuke) äº”ä¸ªç»´åº¦çš„æ ‡å‡†åŒ–è¯„ä¼°ã€‚ç ”ç©¶é€šè¿‡ Cohen's concordance coefficient éªŒè¯äº†ç³»ç»Ÿçš„å¯é æ€§ï¼Œå…¶ kappa å€¼è¾¾åˆ° 0.75ï¼Œè¯æ˜äº†è‡ªåŠ¨åŒ–è¯„ä¼°ä¸äººå·¥å®¡è®¡ä¹‹é—´å­˜åœ¨å¼ºä¸€è‡´æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ¡ˆå°†å®¡è®¡è¿‡ç¨‹çš„è€—æ—¶ç¼©çŸ­äº† 50%ï¼Œå¹¶åœ¨ä¿è¯è¯„ä¼°ä¸€è‡´æ€§çš„åŒæ—¶ï¼Œç›¸æ¯”ä¼ ç»Ÿäººå·¥å®¡è®¡é™ä½äº† 99.8% çš„è¿è¥æˆæœ¬ã€‚è¯¥æ–¹æ³•è®ºä¸ºç²¾ç›Šç³»ç»Ÿ (Lean Systems) ä¸æ–°å…´äººå·¥æ™ºèƒ½æŠ€æœ¯çš„èåˆç¡®ç«‹äº†æ–°èŒƒå¼ï¼Œä¸ºä¸åŒè§„æ¨¡æ±½è½¦å·¥å‚çš„æ•°å­—åŒ–è½¬å‹æä¾›äº†æå…·æ‰©å±•æ€§çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 5 figures, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2510.00067v1",
      "published_date": "2025-09-29 15:28:14 UTC",
      "updated_date": "2025-09-29 15:28:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:54:02.749084+00:00"
    },
    {
      "arxiv_id": "2509.24923v1",
      "title": "When Greedy Wins: Emergent Exploitation Bias in Meta-Bandit LLM Training",
      "title_zh": "è´ªå©ªçš„èƒœåˆ©ï¼šå…ƒå¤šè‡‚è€è™æœºå¤§è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­æ¶Œç°çš„åˆ©ç”¨åå·®",
      "authors": [
        "Sanxing Chen",
        "Xiaoyin Chen",
        "Yukun Huang",
        "Roy Xie",
        "Bhuwan Dhingra"
      ],
      "abstract": "While Large Language Models (LLMs) hold promise to become autonomous agents, they often explore suboptimally in sequential decision-making. Recent work has sought to enhance this capability via supervised fine-tuning (SFT) or reinforcement learning (RL), improving regret on the classic multi-armed bandit task. However, it remains unclear how these learning methods shape exploration strategies and how well they generalize. We investigate both paradigms by training LLMs with SFT on expert trajectories and RL with a range of tailored reward signals including a strategic, regret-shaped reward to reduce variance, and an algorithmic reward that enables oracle imitation. The resulting agents outperform pre-trained models and achieve performance comparable to Upper Confidence Bound (UCB) and Thompson Sampling, with robust generalization to 6x longer horizons and across bandit families. Behavioral analysis reveals that gains often stem from more sophisticated but greedier exploitation: RL/SFT agents are more prone to early catastrophic failure than pre-trained models, prematurely abandoning exploration. Furthermore, agents trained to imitate UCB learn to outperform their teacher by adopting more exploitative variants. Our findings clarify when each training paradigm is preferable and advocate tailored reward design and evaluation beyond average regret to promote robust exploratory behavior.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†å¤šè‡‚è€è™æœºï¼ˆmulti-armed banditï¼‰ä»»åŠ¡æ—¶çš„é¡ºåºå†³ç­–èƒ½åŠ›ï¼Œåˆ†æäº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¦‚ä½•å¡‘é€ æ¨¡å‹çš„æ¢ç´¢ç­–ç•¥ã€‚ç ”ç©¶äººå‘˜é€šè¿‡ä¸“å®¶è½¨è¿¹è¿›è¡ŒSFTè®­ç»ƒï¼Œå¹¶ç»“åˆé—æ†¾æ•´å½¢å¥–åŠ±ï¼ˆregret-shaped rewardï¼‰å’Œç®—æ³•å¥–åŠ±è¿›è¡ŒRLè®­ç»ƒï¼Œä½¿æ¨¡å‹è¾¾åˆ°äº†ä¸UCBå’ŒThompson Samplingç›¸å½“çš„æ€§èƒ½ï¼Œå¹¶åœ¨é•¿æ—¶åŸŸå’Œä¸åŒä»»åŠ¡æ—ç¾¤ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¡Œä¸ºåˆ†ææ­ç¤ºäº†æ¨¡å‹æ™®éå­˜åœ¨â€œåˆ©ç”¨åå·®â€ï¼ˆexploitation biasï¼‰ï¼Œå³æ¨¡å‹å€¾å‘äºè¡¨ç°å‡ºæ›´å¤æ‚ä½†æ›´è´ªå©ªçš„åˆ©ç”¨è¡Œä¸ºï¼Œå¯¼è‡´å…¶æ¯”é¢„è®­ç»ƒæ¨¡å‹æ›´æ˜“è¿‡æ—©æ”¾å¼ƒæ¢ç´¢å¹¶é™·å…¥ç¾éš¾æ€§å¤±è´¥ã€‚æœ‰è¶£çš„æ˜¯ï¼Œæ¨¡ä»¿UCBè®­ç»ƒçš„æ™ºèƒ½ä½“ç”šè‡³é€šè¿‡é‡‡å–æ›´å…·åˆ©ç”¨æ€§çš„å˜ä½“è¶…è¶Šäº†å…¶å¯¼å¸ˆã€‚è¯¥ç ”ç©¶æœ€ç»ˆå»ºè®®åœ¨è®­ç»ƒä¸­é‡‡ç”¨å®šåˆ¶åŒ–å¥–åŠ±è®¾è®¡ï¼Œå¹¶å‘¼ååœ¨å¹³å‡é—æ†¾ï¼ˆaverage regretï¼‰æŒ‡æ ‡ä¹‹å¤–å¯¹ç¨³å¥çš„æ¢ç´¢è¡Œä¸ºè¿›è¡Œæ›´å…¨é¢çš„è¯„ä¼°ï¼Œä¸ºå¼€å‘å…·å¤‡è‡ªä¸»æ¢ç´¢èƒ½åŠ›çš„LLMæ™ºèƒ½ä½“æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24923v1",
      "published_date": "2025-09-29 15:25:42 UTC",
      "updated_date": "2025-09-29 15:25:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:53:59.474739+00:00"
    },
    {
      "arxiv_id": "2511.05500v1",
      "title": "Predicting Oscar-Nominated Screenplays with Sentence Embeddings",
      "title_zh": "åŸºäºå¥å­åµŒå…¥çš„ Oscar æåå‰§æœ¬é¢„æµ‹",
      "authors": [
        "Francis Gross"
      ],
      "abstract": "Oscar nominations are an important factor in the movie industry because they can boost both the visibility and the commercial success. This work explores whether it is possible to predict Oscar nominations for screenplays using modern language models. Since no suitable dataset was available, a new one called Movie-O-Label was created by combining the MovieSum collection of movie scripts with curated Oscar records. Each screenplay was represented by its title, Wikipedia summary, and full script. Long scripts were split into overlapping text chunks and encoded with the E5 sentence em bedding model. Then, the screenplay embed dings were classified using a logistic regression model. The best results were achieved when three feature inputs related to screenplays (script, summary, and title) were combined. The best-performing model reached a macro F1 score of 0.66, a precision recall AP of 0.445 with baseline 0.19 and a ROC-AUC of 0.79. The results suggest that even simple models based on modern text embeddings demonstrate good prediction performance and might be a starting point for future research.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨ç°ä»£è¯­è¨€æ¨¡å‹é¢„æµ‹ç”µå½±å‰§æœ¬æ˜¯å¦è·å¾—å¥¥æ–¯å¡å¥–(Oscar nominations)çš„å¯èƒ½æ€§ï¼Œæ—¨åœ¨æ­ç¤ºå‰§æœ¬å†…å®¹å¯¹ç”µå½±å¥–é¡¹é¢„æµ‹çš„æ½œåœ¨ä»·å€¼ã€‚ç”±äºç¼ºä¹ç°æˆæ•°æ®é›†ï¼Œç ”ç©¶äººå‘˜é€šè¿‡æ•´åˆMovieSumå‰§æœ¬é›†åˆä¸å¥¥æ–¯å¡è®°å½•ï¼Œåˆ›å»ºäº†åä¸ºMovie-O-Labelçš„æ–°æ•°æ®é›†ã€‚æ¯ä¸ªå‰§æœ¬ç”±æ ‡é¢˜ã€Wikipediaæ‘˜è¦å’Œå®Œæ•´å‰§æœ¬å†…å®¹è¡¨å¾ï¼Œå…¶ä¸­é•¿å‰§æœ¬è¢«åˆ‡åˆ†ä¸ºé‡å çš„æ–‡æœ¬å—(text chunks)å¹¶åˆ©ç”¨E5 sentence embeddingæ¨¡å‹è¿›è¡Œç¼–ç ã€‚ç ”ç©¶é‡‡ç”¨é€»è¾‘å›å½’(logistic regression)æ¨¡å‹å¯¹å‰§æœ¬åµŒå…¥è¿›è¡Œåˆ†ç±»ï¼Œå‘ç°ç»“åˆå‰§æœ¬å†…å®¹ã€æ‘˜è¦å’Œæ ‡é¢˜ä¸‰ç§ç‰¹å¾æ—¶æ€§èƒ½è¾¾åˆ°æœ€ä¼˜ã€‚è¡¨ç°æœ€å¥½çš„æ¨¡å‹å®ç°äº†0.66çš„å®è§‚F1å¾—åˆ†(macro F1 score)ï¼Œå…¶PR-AUCè¾¾åˆ°0.445ï¼Œä¸”ROC-AUCä¸º0.79ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒåŸºäºç°ä»£æ–‡æœ¬åµŒå…¥(text embeddings)çš„ç®€å•æ¨¡å‹åœ¨é¢„æµ‹å‰§æœ¬è·å¥–æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä¸ºæœªæ¥çš„ç”µå½±å·¥ä¸šæ™ºèƒ½åˆ†æå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.05500v1",
      "published_date": "2025-09-29 15:25:02 UTC",
      "updated_date": "2025-09-29 15:25:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:54:07.243758+00:00"
    },
    {
      "arxiv_id": "2509.24922v2",
      "title": "MASLegalBench: Benchmarking Multi-Agent Systems in Deductive Legal Reasoning",
      "title_zh": "MASLegalBenchï¼šæ³•å¾‹æ¼”ç»æ¨ç†ä¸­çš„å¤šæ™ºèƒ½ä½“ç³»ç»ŸåŸºå‡†æµ‹è¯•",
      "authors": [
        "Huihao Jing",
        "Wenbin Hu",
        "Hongyu Luo",
        "Jianhui Yang",
        "Wei Fan",
        "Haoran Li",
        "Yangqiu Song"
      ],
      "abstract": "Multi-agent systems (MAS), leveraging the remarkable capabilities of Large Language Models (LLMs), show great potential in addressing complex tasks. In this context, integrating MAS with legal tasks is a crucial step. While previous studies have developed legal benchmarks for LLM agents, none are specifically designed to consider the unique advantages of MAS, such as task decomposition, agent specialization, and flexible training. In fact, the lack of evaluation methods limits the potential of MAS in the legal domain. To address this gap, we propose MASLegalBench, a legal benchmark tailored for MAS and designed with a deductive reasoning approach. Our benchmark uses GDPR as the application scenario, encompassing extensive background knowledge and covering complex reasoning processes that effectively reflect the intricacies of real-world legal situations. Furthermore, we manually design various role-based MAS and conduct extensive experiments using different state-of-the-art LLMs. Our results highlight the strengths, limitations, and potential areas for improvement of existing models and MAS architectures.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MASLegalBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMulti-agent systems, MASï¼‰é‡èº«å®šåˆ¶çš„æ³•å¾‹åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨å¡«è¡¥ç°æœ‰è¯„ä¼°æ–¹æ³•åœ¨ä»»åŠ¡åˆ†è§£ã€æ™ºèƒ½ä½“ä¸“ä¸šåŒ–å’Œçµæ´»è®­ç»ƒç­‰MASç‹¬ç‰¹ä¼˜åŠ¿è¯„ä¼°ä¸Šçš„ç©ºç™½ã€‚è¯¥åŸºå‡†é‡‡ç”¨æ¼”ç»æ¨ç†ï¼ˆDeductive Reasoningï¼‰æ–¹æ³•ï¼Œå¹¶ä»¥GDPRï¼ˆGeneral Data Protection Regulationï¼‰ä¸ºæ ¸å¿ƒåº”ç”¨åœºæ™¯ï¼Œæ¶µç›–äº†åæ˜ çœŸå®æ³•å¾‹æƒ…å¢ƒå¤æ‚æ€§çš„å¹¿æ³›èƒŒæ™¯çŸ¥è¯†ä¸æ¨ç†æµç¨‹ã€‚ç ”ç©¶äººå‘˜æ‰‹åŠ¨è®¾è®¡äº†å¤šç§åŸºäºè§’è‰²çš„MASæ¶æ„ï¼Œå¹¶åˆ©ç”¨å¤šç§å…ˆè¿›çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œäº†å¹¿æ³›å®éªŒã€‚å®éªŒç»“æœæ·±å…¥æ¢è®¨äº†ç°æœ‰æ¨¡å‹ä¸MASæ¶æ„åœ¨æ³•å¾‹æ¨ç†ä»»åŠ¡ä¸­çš„ä¼˜åŠ¿ä¸å±€é™æ€§ï¼Œå¹¶æŒ‡å‡ºäº†æ½œåœ¨çš„æ”¹è¿›é¢†åŸŸã€‚æ­¤é¡¹å·¥ä½œä¸ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨æ³•å¾‹é¢†åŸŸçš„æ½œèƒ½æŒ–æ˜åŠæœªæ¥æ¶æ„ä¼˜åŒ–æä¾›äº†é‡è¦çš„å®éªŒä¾æ®å’Œå‚è€ƒã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24922v2",
      "published_date": "2025-09-29 15:24:40 UTC",
      "updated_date": "2025-09-30 17:09:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:54:06.765630+00:00"
    },
    {
      "arxiv_id": "2509.24919v1",
      "title": "Meta-Learning Theory-Informed Inductive Biases using Deep Kernel Gaussian Processes",
      "title_zh": "åŸºäºæ·±åº¦æ ¸é«˜æ–¯è¿‡ç¨‹çš„ç†è®ºå¼•å¯¼å½’çº³åç½®å…ƒå­¦ä¹ ",
      "authors": [
        "Bahti Zakirov",
        "GaÅ¡per TkaÄik"
      ],
      "abstract": "Normative and task-driven theories offer powerful top-down explanations for biological systems, yet the goals of quantitatively arbitrating between competing theories, and utilizing them as inductive biases to improve data-driven fits of real biological datasets are prohibitively laborious, and often impossible. To this end, we introduce a Bayesian meta-learning framework designed to automatically convert raw functional predictions from normative theories into tractable probabilistic models. We employ adaptive deep kernel Gaussian processes, meta-learning a kernel on synthetic data generated from a normative theory. This Theory-Informed Kernel specifies a probabilistic model representing the theory predictions -- usable for both fitting data and rigorously validating the theory. As a demonstration, we apply our framework to the early visual system, using efficient coding as our normative theory. We show improved response prediction accuracy in ex vivo recordings of mouse retinal ganglion cells stimulated by natural scenes compared to conventional data-driven baselines, while providing well-calibrated uncertainty estimates and interpretable representations. Using exact Bayesian model selection, we also show that our informed kernel can accurately infer the degree of theory-match from data, confirming faithful encapsulation of theory structure. This work provides a more general, scalable, and automated approach for integrating theoretical knowledge into data-driven scientific inquiry in neuroscience and beyond.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§è´å¶æ–¯å…ƒå­¦ä¹ (Bayesian meta-learning)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å°†è§„èŒƒæ€§ç†è®ºè½¬åŒ–ä¸ºå½’çº³åç½®(Inductive Biases)ä»¥æ”¹è¿›ç”Ÿç‰©æ•°æ®æ‹Ÿåˆçš„éš¾é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨è‡ªé€‚åº”æ·±åº¦æ ¸é«˜æ–¯è¿‡ç¨‹(Deep Kernel Gaussian Processes)ï¼Œé€šè¿‡åœ¨åŸºäºç†è®ºç”Ÿæˆçš„åˆæˆæ•°æ®ä¸Šè¿›è¡Œå…ƒå­¦ä¹ ï¼Œæ„å»ºäº†èƒ½å¤Ÿç²¾ç¡®ä»£è¡¨ç†è®ºé¢„æµ‹çš„ç†è®ºçŸ¥æƒ…æ ¸(Theory-Informed Kernel)ã€‚ç ”ç©¶è€…å°†è¯¥æ¡†æ¶åº”ç”¨äºæ—©æœŸè§†è§‰ç³»ç»Ÿï¼Œä»¥æœ‰æ•ˆç¼–ç (Efficient Coding)ä½œä¸ºè§„èŒƒç†è®ºï¼Œåœ¨å°é¼ è§†ç½‘è†œç¥ç»èŠ‚ç»†èƒ(Retinal Ganglion Cells)çš„å®éªŒè®°å½•ä¸­ï¼Œå®ç°äº†æ¯”ä¼ ç»Ÿæ•°æ®é©±åŠ¨åŸºå‡†æ¨¡å‹æ›´é«˜çš„å“åº”é¢„æµ‹å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜æä¾›äº†æ ¡å‡†è‰¯å¥½çš„ä¸ç¡®å®šæ€§ä¼°è®¡å’Œå¯è§£é‡Šçš„è¡¨ç¤ºï¼Œå¹¶é€šè¿‡è´å¶æ–¯æ¨¡å‹é€‰æ‹©å‡†ç¡®æ¨æ–­å‡ºæ•°æ®ä¸ç†è®ºçš„åŒ¹é…ç¨‹åº¦ã€‚è¿™é¡¹å·¥ä½œä¸ºåœ¨ç¥ç»ç§‘å­¦åŠå…¶ä»–ç§‘å­¦é¢†åŸŸä¸­å°†ç†è®ºçŸ¥è¯†æ•´åˆè¿›æ•°æ®é©±åŠ¨çš„ç§‘å­¦æ¢ç©¶æä¾›äº†ä¸€ç§é€šç”¨ã€å¯æ‰©å±•ä¸”è‡ªåŠ¨åŒ–çš„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "q-bio.NC"
      ],
      "primary_category": "cs.AI",
      "comment": "13 pages, 5 figures, 9 SI figures",
      "pdf_url": "https://arxiv.org/pdf/2509.24919v1",
      "published_date": "2025-09-29 15:23:50 UTC",
      "updated_date": "2025-09-29 15:23:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:54:07.752444+00:00"
    },
    {
      "arxiv_id": "2509.25293v1",
      "title": "AI in Pakistani Schools: Adoption, Usage, and Perceived Impact among Educators",
      "title_zh": "Pakistani å­¦æ ¡çš„äººå·¥æ™ºèƒ½ï¼šæ•™è‚²å·¥ä½œè€…çš„é‡‡ç”¨ã€ä½¿ç”¨ç°çŠ¶ä¸æ„ŸçŸ¥å½±å“",
      "authors": [
        "Syed Hassan Raza",
        "Azib Farooq"
      ],
      "abstract": "Artificial Intelligence (AI) is increasingly permeating classrooms worldwide, yet its adoption in schools of developing countries remains under-explored. This paper investigates AI adoption, usage patterns, and perceived impact in Pakistani K-12 schools based on a survey of 125 educators. The questionnaire covered educator's familiarity with AI, frequency and modes of use, and attitudes toward AI's benefits and challenges. Results reveal a generally positive disposition towards AI: over two-thirds of teachers expressed willingness to adopt AI tools given proper support and many have begun integrating AI for lesson planning and content creation. However, AI usage is uneven - while about one-third of respondents actively use AI tools frequently, others remain occasional users. Content generation emerged as the most common AI application, whereas AI-driven grading and feedback are rarely used. Teachers reported moderate improvements in student engagement and efficiency due to AI, but also voiced concerns about equitable access. These findings highlight both the enthusiasm for AI's potential in Pakistan's schools and the need for training and infrastructure to ensure inclusive and effective implementation.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡å¯¹125åæ•™è‚²å·¥ä½œè€…çš„è°ƒæŸ¥ï¼Œæ·±å…¥æ¢è®¨äº†å·´åŸºæ–¯å¦K-12å­¦æ ¡ä¸­äººå·¥æ™ºèƒ½(AI)çš„é‡‡ç”¨æƒ…å†µã€ä½¿ç”¨æ¨¡å¼åŠæ„ŸçŸ¥å½±å“ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œè¶…è¿‡ä¸‰åˆ†ä¹‹äºŒçš„æ•™å¸ˆåœ¨è·å¾—é€‚å½“æ”¯æŒçš„æƒ…å†µä¸‹è¡¨ç°å‡ºç§¯æçš„é‡‡çº³æ„æ„¿ï¼Œä¸”è®¸å¤šäººå·²å¼€å§‹å°†AIåº”ç”¨äºè¯¾ç¨‹è§„åˆ’å’Œå†…å®¹ç”Ÿæˆ(Content generation)ã€‚å°½ç®¡å¦‚æ­¤ï¼ŒAIçš„ä½¿ç”¨å¹¶ä¸å‡è¡¡ï¼Œä»…çº¦ä¸‰åˆ†ä¹‹ä¸€çš„å—è®¿è€…é¢‘ç¹ä½¿ç”¨ï¼Œä¸”AIé©±åŠ¨çš„è¯„åˆ†ä¸åé¦ˆåŠŸèƒ½å¾ˆå°‘è¢«è§¦åŠã€‚æ•™å¸ˆä»¬åé¦ˆAIèƒ½é€‚åº¦æå‡å­¦ç”Ÿçš„å‚ä¸åº¦å’Œæ•™å­¦æ•ˆç‡ï¼Œä½†åŒæ—¶ä¹Ÿå¯¹å…¬å¹³è·å–(Equitable access)é—®é¢˜è¡¨ç¤ºæ‹…å¿§ã€‚è¯¥å‘ç°å‡¸æ˜¾äº†å·´åŸºæ–¯å¦å­¦æ ¡å¯¹AIæ½œåŠ›çš„çƒ­æƒ…ï¼Œå¹¶å¼ºè°ƒäº†ä¸ºç¡®ä¿åŒ…å®¹æ€§å’Œæœ‰æ•ˆå®æ–½è€ŒåŠ å¼ºåŸ¹è®­åŠåŸºç¡€è®¾æ–½(Infrastructure)å»ºè®¾çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25293v1",
      "published_date": "2025-09-29 15:20:01 UTC",
      "updated_date": "2025-09-29 15:20:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:54:58.091983+00:00"
    },
    {
      "arxiv_id": "2509.24913v2",
      "title": "Segmentor-Guided Counterfactual Fine-Tuning for Locally Coherent and Targeted Image Synthesis",
      "title_zh": "é¢å‘å±€éƒ¨ä¸€è‡´å’Œé’ˆå¯¹æ€§å›¾åƒåˆæˆçš„åˆ†å‰²å™¨å¼•å¯¼åäº‹å®å¾®è°ƒ",
      "authors": [
        "Tian Xia",
        "Matthew Sinclair",
        "Andreas Schuh",
        "Fabio De Sousa Ribeiro",
        "Raghav Mehta",
        "Rajat Rasal",
        "Esther Puyol-AntÃ³n",
        "Samuel Gerber",
        "Kersten Petersen",
        "Michiel Schaap",
        "Ben Glocker"
      ],
      "abstract": "Counterfactual image generation is a powerful tool for augmenting training data, de-biasing datasets, and modeling disease. Current approaches rely on external classifiers or regressors to increase the effectiveness of subject-level interventions (e.g., changing the patient's age). For structure-specific interventions (e.g., changing the area of the left lung in a chest radiograph), we show that this is insufficient, and can result in undesirable global effects across the image domain. Previous work used pixel-level label maps as guidance, requiring a user to provide hypothetical segmentations which are tedious and difficult to obtain. We propose Segmentor-guided Counterfactual Fine-Tuning (Seg-CFT), which preserves the simplicity of intervening on scalar-valued, structure-specific variables while producing locally coherent and effective counterfactuals. We demonstrate the capability of generating realistic chest radiographs, and we show promising results for modeling coronary artery disease. Code: https://github.com/biomedia-mira/seg-cft.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Seg-CFT (Segmentor-guided Counterfactual Fine-Tuning) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç»“æ„ç‰¹å®šå¹²é¢„ (structure-specific interventions) åœ¨åäº‹å®å›¾åƒç”Ÿæˆ (Counterfactual image generation) ä¸­å¸¸è§çš„å…¨å±€ä¸ç†æƒ³æ•ˆåº”é—®é¢˜ã€‚ä¸ä»¥å¾€éœ€è¦ç¹çæ‰‹åŠ¨æä¾›æ ‡ç­¾å›¾ (label maps) çš„æ–¹æ³•ä¸åŒï¼ŒSeg-CFT åœ¨ä¿ç•™æ ‡é‡åŒ–å˜é‡å¹²é¢„ç®€ä¾¿æ€§çš„åŒæ—¶ï¼Œèƒ½å¤Ÿäº§ç”Ÿå±€éƒ¨è¿è´¯ä¸”ç²¾å‡†çš„åäº‹å®å›¾åƒã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ†å‰²å™¨å¼•å¯¼çš„å¾®è°ƒæŠ€æœ¯ï¼Œç¡®ä¿äº†å¯¹ç‰¹å®šè§£å‰–ç»“æ„çš„ä¿®æ”¹ä¸ä¼šå¯¼è‡´å›¾åƒå…¶ä»–åŒºåŸŸçš„å¤±çœŸã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ç”Ÿæˆé€¼çœŸçš„èƒ¸éƒ¨ X å°„çº¿ç…§ç‰‡ (chest radiographs) ä»¥åŠå† çŠ¶åŠ¨è„‰ç–¾ç—… (coronary artery disease) å»ºæ¨¡æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚è¿™ä¸€ç ”ç©¶ä¸ºåŒ»å­¦å›¾åƒé¢†åŸŸçš„è®­ç»ƒæ•°æ®å¢å¼ºã€æ•°æ®é›†å»å (de-biasing) å’Œç–¾ç—…æ¨¡æ‹Ÿæä¾›äº†é«˜æ•ˆçš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at MICCAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.24913v2",
      "published_date": "2025-09-29 15:19:09 UTC",
      "updated_date": "2025-10-02 17:13:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:54:38.895197+00:00"
    },
    {
      "arxiv_id": "2509.24906v1",
      "title": "Neural network embeddings recover value dimensions from psychometric survey items on par with human data",
      "title_zh": "ç¥ç»ç½‘ç»œåµŒå…¥åœ¨æ¢å¤å¿ƒç†æµ‹é‡è°ƒæŸ¥é¡¹ä»·å€¼ç»´åº¦æ–¹é¢çš„è¡¨ç°å ªæ¯”äººç±»æ•°æ®",
      "authors": [
        "Max Pellert",
        "Clemens M. Lechner",
        "Indira Sen",
        "Markus Strohmaier"
      ],
      "abstract": "This study introduces \"Survey and Questionnaire Item Embeddings Differentials\" (SQuID), a novel methodological approach that enables neural network embeddings to effectively recover latent dimensions from psychometric survey items. We demonstrate that embeddings derived from large language models, when processed with SQuID, can recover the structure of human values obtained from human rater judgments on the Revised Portrait Value Questionnaire (PVQ-RR). Our experimental validation compares multiple embedding models across a number of evaluation metrics. Unlike previous approaches, SQuID successfully addresses the challenge of obtaining negative correlations between dimensions without requiring domain-specific fine-tuning. Quantitative analysis reveals that our embedding-based approach explains 55% of variance in dimension-dimension similarities compared to human data. Multidimensional scaling configurations from both types of data show fair factor congruence coefficients and largely follow the underlying theory. These results demonstrate that semantic embeddings can effectively replicate psychometric structures previously established through extensive human surveys. The approach offers substantial advantages in cost, scalability and flexibility while maintaining comparable quality to traditional methods. Our findings have significant implications for psychometrics and social science research, providing a complementary methodology that could expand the scope of human behavior and experience represented in measurement tools.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†SQuID (Survey and Questionnaire Item Embeddings Differentials)ï¼Œè¿™æ˜¯ä¸€ç§ä½¿ç¥ç»ç½‘ç»œåµŒå…¥(neural network embeddings)èƒ½å¤Ÿæœ‰æ•ˆä»å¿ƒç†æµ‹è¯„é‡è¡¨é¡¹ä¸­æ¢å¤æ½œåœ¨ç»´åº¦çš„æ–°å‹æ–¹æ³•ã€‚é€šè¿‡å¤„ç†æ¥è‡ªå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„åµŒå…¥ï¼ŒSQuIDæˆåŠŸåœ¨PVQ-RR (Revised Portrait Value Questionnaire)é‡è¡¨ä¸Šæ¢å¤äº†ä¸äººç±»è¯„åˆ†è€…åˆ¤æ–­ä¸€è‡´çš„ä»·å€¼è§‚ç»“æ„ã€‚è¯¥æ–¹æ³•æ— éœ€ç‰¹å®šé¢†åŸŸå¾®è°ƒå³å¯è§£å†³ç»´åº¦é—´è´Ÿç›¸å…³æ€§çš„è·å–æŒ‘æˆ˜ï¼Œå¹¶åœ¨å®šé‡åˆ†æä¸­è§£é‡Šäº†55%çš„äººç±»æ•°æ®ç»´åº¦ç›¸ä¼¼æ€§æ–¹å·®ã€‚å¤šç»´å°ºåº¦åˆ†æ(Multidimensional scaling)æ˜¾ç¤ºå…¶å› å­ä¸€è‡´æ€§ç³»æ•°(factor congruence coefficients)è‰¯å¥½ä¸”åŸºæœ¬ç¬¦åˆå¿ƒç†å­¦ç†è®ºé¢„æœŸã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯­ä¹‰åµŒå…¥å¯ä»¥æœ‰æ•ˆå¤åˆ¶é€šè¿‡å¤§è§„æ¨¡äººç±»è°ƒæŸ¥å»ºç«‹çš„å¿ƒç†æµ‹é‡ç»“æ„ï¼Œåœ¨ä¿æŒä¸ä¼ ç»Ÿæ–¹æ³•ç›¸å½“è´¨é‡çš„åŒæ—¶ï¼Œæä¾›äº†æˆæœ¬ã€å¯æ‰©å±•æ€§å’Œçµæ´»æ€§æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚è¿™ä¸€å‘ç°ä¸ºå¿ƒç†å­¦å’Œç¤¾ä¼šç§‘å­¦ç ”ç©¶æä¾›äº†ä¸€ç§é«˜æ•ˆçš„äº’è¡¥æ–¹æ³•è®ºï¼Œæœ‰æœ›æ‰©å±•æµ‹é‡å·¥å…·å¯¹äººç±»è¡Œä¸ºå’Œç»éªŒçš„è¦†ç›–èŒƒå›´ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24906v1",
      "published_date": "2025-09-29 15:14:54 UTC",
      "updated_date": "2025-09-29 15:14:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:54:21.248568+00:00"
    },
    {
      "arxiv_id": "2509.24900v1",
      "title": "OpenGPT-4o-Image: A Comprehensive Dataset for Advanced Image Generation and Editing",
      "title_zh": "OpenGPT-4o-Imageï¼šé¢å‘é«˜çº§å›¾åƒç”Ÿæˆä¸ç¼–è¾‘çš„ç»¼åˆæ•°æ®é›†",
      "authors": [
        "Zhihong Chen",
        "Xuehai Bai",
        "Yang Shi",
        "Chaoyou Fu",
        "Huanyu Zhang",
        "Haotian Wang",
        "Xiaoyan Sun",
        "Zhang Zhang",
        "Liang Wang",
        "Yuanxing Zhang",
        "Pengfei Wan",
        "Yi-Fan Zhang"
      ],
      "abstract": "The performance of unified multimodal models for image generation and editing is fundamentally constrained by the quality and comprehensiveness of their training data. While existing datasets have covered basic tasks like style transfer and simple object manipulation, they often lack the systematic structure and challenging scenarios required for real-world applications. To address this bottleneck, we introduce OpenGPT-4o-Image, a large-scale dataset constructed using a novel methodology that combines hierarchical task taxonomy with automated data generation. Our taxonomy not only includes fundamental capabilities such as text rendering and style control but also introduces highly practical yet challenging categories like scientific imagery for chemistry illustrations and complex instruction editing requiring simultaneous execution of multiple operations. Through an automated pipeline leveraging structured resource pools and GPT-4o, we generate 80k high-quality instruction-image pairs with controlled diversity, covering 11 major domains and 51 subtasks. Extensive experiments show that fine-tuning leading models on our dataset achieves significant performance gains across multiple benchmarks, with improvements of up to 18\\% on editing tasks (UniWorld-V1 on ImgEdit-Bench) and 13% on generation tasks (Harmon on GenEval). Our work demonstrates that systematic data construction is key to advancing multimodal AI capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† OpenGPT-4o-Imageï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºé«˜çº§å›¾åƒç”Ÿæˆå’Œç¼–è¾‘çš„å¤§è§„æ¨¡ç»¼åˆæ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ•°æ®é›†ç¼ºä¹ç³»ç»Ÿæ€§ç»“æ„å’Œåº”å¯¹å¤æ‚åœºæ™¯èƒ½åŠ›çš„é—®é¢˜ã€‚ç ”ç©¶é‡‡ç”¨äº†ä¸€ç§ç»“åˆå±‚æ¬¡åŒ–ä»»åŠ¡åˆ†ç±»(hierarchical task taxonomy)ä¸è‡ªåŠ¨åŒ–æ•°æ®ç”Ÿæˆçš„åˆ›æ–°æ–¹æ³•ï¼Œä¸ä»…æ¶µç›–äº†æ–‡æœ¬æ¸²æŸ“å’Œé£æ ¼æ§åˆ¶ç­‰åŸºç¡€åŠŸèƒ½ï¼Œè¿˜å¼•å…¥äº†å¦‚åŒ–å­¦æ’å›¾ç­‰ç§‘å­¦å›¾åƒå’Œå¤šé‡æ“ä½œå¤åˆç¼–è¾‘ç­‰æå…·æŒ‘æˆ˜æ€§çš„å®ç”¨ç±»åˆ«ã€‚é€šè¿‡åˆ©ç”¨ GPT-4o æ„å»ºçš„è‡ªåŠ¨åŒ–ç®¡çº¿ï¼Œè¯¥æ•°æ®é›†åŒ…å«äº† 80k ç»„è¦†ç›– 11 ä¸ªä¸»è¦é¢†åŸŸå’Œ 51 ä¸ªå­ä»»åŠ¡çš„é«˜è´¨é‡æŒ‡ä»¤-å›¾åƒå¯¹ã€‚å®éªŒè¯æ˜ï¼Œåœ¨ OpenGPT-4o-Image ä¸Šè¿›è¡Œå¾®è°ƒåï¼Œå¤šæ¨¡æ€æ¨¡å‹åœ¨ç¼–è¾‘ä»»åŠ¡(ImgEdit-Bench)å’Œç”Ÿæˆä»»åŠ¡(GenEval)ä¸Šçš„è¡¨ç°åˆ†åˆ«æå‡äº† 18% å’Œ 13%ï¼Œæœ‰åŠ›åœ°å±•ç¤ºäº†ç³»ç»ŸåŒ–æ•°æ®æ„å»ºå¯¹æå‡ multimodal AI èƒ½åŠ›çš„å…³é”®ä½œç”¨ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24900v1",
      "published_date": "2025-09-29 15:11:09 UTC",
      "updated_date": "2025-09-29 15:11:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:54:21.107164+00:00"
    },
    {
      "arxiv_id": "2509.24897v1",
      "title": "RealUnify: Do Unified Models Truly Benefit from Unification? A Comprehensive Benchmark",
      "title_zh": "RealUnifyï¼šç»Ÿä¸€æ¨¡å‹æ˜¯å¦çœŸæ­£ä»ç»Ÿä¸€ä¸­è·ç›Šï¼Ÿä¸€é¡¹å…¨é¢çš„åŸºå‡†æµ‹è¯•",
      "authors": [
        "Yang Shi",
        "Yuhao Dong",
        "Yue Ding",
        "Yuran Wang",
        "Xuanyu Zhu",
        "Sheng Zhou",
        "Wenting Liu",
        "Haochen Tian",
        "Rundong Wang",
        "Huanqian Wang",
        "Zuyan Liu",
        "Bohan Zeng",
        "Ruizhe Chen",
        "Qixun Wang",
        "Zhuoran Zhang",
        "Xinlong Chen",
        "Chengzhuo Tong",
        "Bozhou Li",
        "Chaoyou Fu",
        "Qiang Liu",
        "Haotian Wang",
        "Wenjing Yang",
        "Yuanxing Zhang",
        "Pengfei Wan",
        "Yi-Fan Zhang",
        "Ziwei Liu"
      ],
      "abstract": "The integration of visual understanding and generation into unified multimodal models represents a significant stride toward general-purpose AI. However, a fundamental question remains unanswered by existing benchmarks: does this architectural unification actually enable synergetic interaction between the constituent capabilities? Existing evaluation paradigms, which primarily assess understanding and generation in isolation, are insufficient for determining whether a unified model can leverage its understanding to enhance its generation, or use generative simulation to facilitate deeper comprehension. To address this critical gap, we introduce RealUnify, a benchmark specifically designed to evaluate bidirectional capability synergy. RealUnify comprises 1,000 meticulously human-annotated instances spanning 10 categories and 32 subtasks. It is structured around two core axes: 1) Understanding Enhances Generation, which requires reasoning (e.g., commonsense, logic) to guide image generation, and 2) Generation Enhances Understanding, which necessitates mental simulation or reconstruction (e.g., of transformed or disordered visual inputs) to solve reasoning tasks. A key contribution is our dual-evaluation protocol, which combines direct end-to-end assessment with a diagnostic stepwise evaluation that decomposes tasks into distinct understanding and generation phases. This protocol allows us to precisely discern whether performance bottlenecks stem from deficiencies in core abilities or from a failure to integrate them. Through large-scale evaluations of 12 leading unified models and 6 specialized baselines, we find that current unified models still struggle to achieve effective synergy, indicating that architectural unification alone is insufficient. These results highlight the need for new training strategies and inductive biases to fully unlock the potential of unified modeling.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† RealUnifyï¼Œä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°å¤šæ¨¡æ€ç»Ÿä¸€æ¨¡å‹ä¸­è§†è§‰ç†è§£ä¸ç”Ÿæˆä¹‹é—´åŒå‘èƒ½åŠ›ååŒ (bidirectional capability synergy) çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨æ¢ç©¶æ¶æ„ä¸Šçš„ç»Ÿä¸€æ˜¯å¦çœŸæ­£å¸¦æ¥äº†èƒ½åŠ›çš„äº’è¡¥ã€‚RealUnify åŒ…å« 1,000 ä¸ªç»è¿‡äººå·¥æ ‡æ³¨çš„å®ä¾‹ï¼Œæ¶µç›– 10 ä¸ªç±»åˆ«å’Œ 32 ä¸ªå­ä»»åŠ¡ï¼Œé‡ç‚¹è€ƒå¯Ÿâ€œç†è§£å¢å¼ºç”Ÿæˆâ€å’Œâ€œç”Ÿæˆå¢å¼ºç†è§£â€ä¸¤ä¸ªæ ¸å¿ƒç»´åº¦ã€‚ä¸ºäº†ç²¾å‡†è¯†åˆ«æ€§èƒ½ç“¶é¢ˆï¼Œç ”ç©¶å¼•å…¥äº†åŒé‡è¯„ä¼°åè®®ï¼Œå°†ç«¯åˆ°ç«¯è¯„ä¼°ä¸è¯Šæ–­æ€§çš„åˆ†æ­¥è¯„ä¼°ç›¸ç»“åˆï¼Œä»è€ŒåŒºåˆ†æ€§èƒ½ä¸è¶³æ˜¯æºäºåŸºç¡€èƒ½åŠ›ç¼ºé™·è¿˜æ˜¯æ•´åˆå¤±è´¥ã€‚é€šè¿‡å¯¹ 12 ä¸ªé¢†å…ˆçš„ç»Ÿä¸€æ¨¡å‹å’Œ 6 ä¸ªä¸“ä¸šåŸºå‡†æ¨¡å‹çš„è¯„ä¼°ï¼Œç»“æœè¡¨æ˜å½“å‰çš„ç»Ÿä¸€æ¨¡å‹åœ¨å®ç°æœ‰æ•ˆååŒæ–¹é¢ä»é¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œè¯æ˜ä»…é æ¶æ„ç»Ÿä¸€å¹¶ä¸è¶³ä»¥è‡ªåŠ¨äº§ç”ŸååŒæ•ˆåº”ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†å¼€å‘æ–°å‹è®­ç»ƒç­–ç•¥å’Œå½’çº³åç½® (inductive biases) çš„å¿…è¦æ€§ï¼Œä¸ºæœªæ¥å¼€å‘èƒ½å¤Ÿå……åˆ†é‡Šæ”¾æ½œèƒ½çš„å¤šæ¨¡æ€ç»Ÿä¸€æ¨¡å‹æä¾›äº†é‡è¦çš„å‚è€ƒæ–¹å‘ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24897v1",
      "published_date": "2025-09-29 15:07:28 UTC",
      "updated_date": "2025-09-29 15:07:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:54:26.692197+00:00"
    },
    {
      "arxiv_id": "2509.24882v1",
      "title": "Scaling Laws and Spectra of Shallow Neural Networks in the Feature Learning Regime",
      "title_zh": "ç‰¹å¾å­¦ä¹ æœºåˆ¶ä¸‹æµ…å±‚ç¥ç»ç½‘ç»œçš„ç¼©æ”¾æ³•åˆ™ä¸è°±ç‰¹æ€§",
      "authors": [
        "Leonardo Defilippis",
        "Yizhou Xu",
        "Julius Girardin",
        "Emanuele Troiani",
        "Vittorio Erba",
        "Lenka ZdeborovÃ¡",
        "Bruno Loureiro",
        "Florent Krzakala"
      ],
      "abstract": "Neural scaling laws underlie many of the recent advances in deep learning, yet their theoretical understanding remains largely confined to linear models. In this work, we present a systematic analysis of scaling laws for quadratic and diagonal neural networks in the feature learning regime. Leveraging connections with matrix compressed sensing and LASSO, we derive a detailed phase diagram for the scaling exponents of the excess risk as a function of sample complexity and weight decay. This analysis uncovers crossovers between distinct scaling regimes and plateau behaviors, mirroring phenomena widely reported in the empirical neural scaling literature. Furthermore, we establish a precise link between these regimes and the spectral properties of the trained network weights, which we characterize in detail. As a consequence, we provide a theoretical validation of recent empirical observations connecting the emergence of power-law tails in the weight spectrum with network generalization performance, yielding an interpretation from first principles.",
      "tldr_zh": "æœ¬ç ”ç©¶å¯¹ç‰¹å¾å­¦ä¹ æœºåˆ¶(feature learning regime)ä¸‹æµ…å±‚ç¥ç»ç½‘ç»œï¼ˆåŒ…æ‹¬äºŒæ¬¡å‹å’Œå¯¹è§’ç½‘ç»œï¼‰çš„æ¯”ä¾‹å®šå¾‹(scaling laws)è¿›è¡Œäº†ç³»ç»Ÿæ€§åˆ†æï¼Œæ—¨åœ¨è§£å†³ç›®å‰ç†è®ºç†è§£ä¸»è¦å±€é™äºçº¿æ€§æ¨¡å‹çš„é—®é¢˜ã€‚é€šè¿‡åˆ©ç”¨çŸ©é˜µå‹ç¼©æ„ŸçŸ¥(matrix compressed sensing)å’ŒLASSOä¹‹é—´çš„å†…åœ¨è”ç³»ï¼Œç ”ç©¶è€…æ¨å¯¼å‡ºè¶…é¢é£é™©(excess risk)æ¯”ä¾‹æŒ‡æ•°çš„è¯¦ç»†ç›¸å›¾ï¼Œé˜æ˜äº†å…¶éšæ ·æœ¬å¤æ‚åº¦(sample complexity)å’Œæƒé‡è¡°å‡(weight decay)å˜åŒ–çš„å‡½æ•°å…³ç³»ã€‚è¯¥åˆ†ææˆåŠŸæ­ç¤ºäº†ä¸åŒæ¯”ä¾‹æœºåˆ¶ä¹‹é—´çš„äº¤å‰(crossovers)å’Œå¹³å°è¡Œä¸º(plateau behaviors)ï¼Œä»è€Œåœ¨ç†è®ºä¸Šè§£é‡Šäº†å®è¯ç ”ç©¶ä¸­å¹¿æ³›æŠ¥é“çš„æ¯”ä¾‹ç°è±¡ã€‚æ­¤å¤–ï¼Œç ”ç©¶ç¡®ç«‹äº†è¿™äº›æœºåˆ¶ä¸è®­ç»ƒæƒé‡å…‰è°±å±æ€§(spectral properties)ä¹‹é—´çš„ç²¾ç¡®å…³è”ï¼Œä¸ºæƒé‡å…‰è°±ä¸­å¹‚å¾‹å°¾éƒ¨(power-law tails)ä¸ç½‘ç»œæ³›åŒ–æ€§èƒ½(generalization performance)ä¹‹é—´çš„å…³ç³»æä¾›äº†åŸºäºç¬¬ä¸€æ€§åŸç†(first principles)çš„ç†è®ºéªŒè¯ã€‚",
      "categories": [
        "cs.LG",
        "cond-mat.dis-nn",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24882v1",
      "published_date": "2025-09-29 14:58:13 UTC",
      "updated_date": "2025-09-29 14:58:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:55:23.986259+00:00"
    },
    {
      "arxiv_id": "2509.24880v1",
      "title": "Vehicle Classification under Extreme Imbalance: A Comparative Study of Ensemble Learning and CNNs",
      "title_zh": "æç«¯ç±»åˆ«ä¸å¹³è¡¡ä¸‹çš„è½¦è¾†åˆ†ç±»ï¼šé›†æˆå­¦ä¹ ä¸å·ç§¯ç¥ç»ç½‘ç»œçš„å¯¹æ¯”ç ”ç©¶",
      "authors": [
        "Abu Hanif Muhammad Syarubany"
      ],
      "abstract": "Accurate vehicle type recognition underpins intelligent transportation and logistics, but severe class imbalance in public datasets suppresses performance on rare categories. We curate a 16-class corpus (~47k images) by merging Kaggle, ImageNet, and web-crawled data, and create six balanced variants via SMOTE oversampling and targeted undersampling. Lightweight ensembles, such as Random Forest, AdaBoost, and a soft-voting combiner built on MobileNet-V2 features are benchmarked against a configurable ResNet-style CNN trained with strong augmentation and label smoothing. The best ensemble (SMOTE-combined) attains 74.8% test accuracy, while the CNN achieves 79.19% on the full test set and 81.25% on an unseen inference batch, confirming the advantage of deep models. Nonetheless, the most under-represented class (Barge) remains a failure mode, highlighting the limits of rebalancing alone. Results suggest prioritizing additional minority-class collection and cost-sensitive objectives (e.g., focal loss) and exploring hybrid ensemble or CNN pipelines to combine interpretability with representational power.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ™ºèƒ½äº¤é€šä¸­è½¦è¾†ç±»å‹è¯†åˆ«é¢ä¸´çš„ä¸¥é‡ç±»åˆ«ä¸å¹³è¡¡(Class Imbalance)é—®é¢˜ï¼Œé€šè¿‡æ•´åˆå¤šæºæ•°æ®æ„å»ºäº†ä¸€ä¸ªåŒ…å«16ä¸ªç±»åˆ«çš„å›¾åƒè¯­æ–™åº“ï¼Œå¹¶åˆ©ç”¨SMOTEè¿‡é‡‡æ ·å’Œå®šå‘æ¬ é‡‡æ ·æŠ€æœ¯ç”Ÿæˆäº†å¹³è¡¡æ•°æ®é›†ã€‚ç ”ç©¶å¯¹æ¯”äº†Random Forestã€AdaBoostå’ŒåŸºäºMobileNet-V2ç‰¹å¾çš„è½¯æŠ•ç¥¨ç»„åˆå™¨(Soft-voting Combiner)ç­‰é›†æˆå­¦ä¹ (Ensemble Learning)æ¨¡å‹ï¼Œä»¥åŠç»è¿‡å¼ºæ•°æ®å¢å¼ºå’Œæ ‡ç­¾å¹³æ»‘å¤„ç†çš„ResNeté£æ ¼CNNã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCNNä»¥79.19%çš„å‡†ç¡®ç‡ä¼˜äºæœ€ä½³é›†æˆæ¨¡å‹çš„74.8%ï¼ŒéªŒè¯äº†æ·±åº¦æ¨¡å‹åœ¨å¤„ç†å¤æ‚åˆ†ç±»ä»»åŠ¡ä¸­çš„ä¼˜åŠ¿ã€‚å°½ç®¡å¦‚æ­¤ï¼Œæå°‘æ•°ç±»åˆ«(Barge)çš„è¯†åˆ«å¤±æ•ˆæ­ç¤ºäº†ä»…ä¾é é‡å¹³è¡¡(Rebalancing)æ‰‹æ®µçš„å±€é™æ€§ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œæœªæ¥åº”ä¾§é‡äºå°‘æ•°ç±»æ•°æ®çš„è¿›ä¸€æ­¥æ”¶é›†ï¼Œå¹¶ç»“åˆFocal Lossç­‰ä»£ä»·æ•æ„Ÿå­¦ä¹ ç­–ç•¥åŠæ··åˆæ¨¡å‹æ¶æ„ï¼Œä»¥åœ¨æå‡è¡¨å¾èƒ½åŠ›çš„åŒæ—¶å…¼é¡¾æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24880v1",
      "published_date": "2025-09-29 14:56:56 UTC",
      "updated_date": "2025-09-29 14:56:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:55:29.299370+00:00"
    },
    {
      "arxiv_id": "2509.24877v2",
      "title": "The Emergence of Social Science of Large Language Models",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ç¤¾ä¼šç§‘å­¦çš„å…´èµ·",
      "authors": [
        "Xiao Jia",
        "Zhanzhan Zhao"
      ],
      "abstract": "The social science of large language models (LLMs) examines how these systems evoke mind attributions, interact with one another, and transform human activity and institutions. We conducted a systematic review of 270 studies, combining text embeddings, unsupervised clustering and topic modeling to build a computational taxonomy. Three domains emerge organically across the reviewed literature. LLM as Social Minds examines whether and when models display behaviors that elicit attributions of cognition, morality and bias, while addressing challenges such as test leakage and surface cues. LLM Societies examines multi-agent settings where interaction protocols, architectures and mechanism design shape coordination, norms, institutions and collective epistemic processes. LLM-Human Interactions examines how LLMs reshape tasks, learning, trust, work and governance, and how risks arise at the human-AI interface. This taxonomy provides a reproducible map of a fragmented field, clarifies evidentiary standards across levels of analysis, and highlights opportunities for cumulative progress in the social science of artificial intelligence.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡å¯¹270é¡¹ç ”ç©¶è¿›è¡Œç³»ç»Ÿæ€§ç»¼è¿°ï¼Œç»“åˆæ–‡æœ¬åµŒå…¥(text embeddings)ã€æ— ç›‘ç£èšç±»(unsupervised clustering)å’Œä¸»é¢˜å»ºæ¨¡(topic modeling)æŠ€æœ¯ï¼Œæ„å»ºäº†å¤§è¯­è¨€æ¨¡å‹(LLMs)ç¤¾ä¼šç§‘å­¦é¢†åŸŸçš„è®¡ç®—åˆ†ç±»æ³•(computational taxonomy)ã€‚è¯¥åˆ†ç±»æ³•ç”±ä¸‰ä¸ªæ ¸å¿ƒé¢†åŸŸç»„æˆï¼šâ€œLLMä½œä¸ºç¤¾äº¤å¿ƒç†â€(LLM as Social Minds)æ¢è®¨æ¨¡å‹åœ¨è®¤çŸ¥ã€é“å¾·å’Œåè§æ–¹é¢çš„è¡Œä¸ºè¡¨ç°åŠæµ‹è¯•æ³„éœ²(test leakage)ç­‰æŒ‘æˆ˜ï¼›â€œLLMç¤¾ä¼šâ€(LLM Societies)èšç„¦å¤šæ™ºèƒ½ä½“(multi-agent)ç¯å¢ƒä¸‹äº¤äº’åè®®ä¸æœºåˆ¶è®¾è®¡å¯¹è§„èŒƒã€åˆ¶åº¦åŠé›†ä½“è®¤çŸ¥è¿‡ç¨‹çš„å¡‘é€ ï¼›â€œLLM-äººç±»äº¤äº’â€(LLM-Human Interactions)åˆ™åˆ†æLLMså¯¹ä»»åŠ¡ã€å­¦ä¹ ã€ä¿¡ä»»ã€å·¥ä½œå’Œæ²»ç†çš„é‡å¡‘ä½œç”¨åŠå…¶æ½œåœ¨é£é™©ã€‚è¿™é¡¹å·¥ä½œä¸ºè¿™ä¸€ç¢ç‰‡åŒ–çš„ç ”ç©¶é¢†åŸŸæä¾›äº†å¯å¤åˆ¶çš„è·¯çº¿å›¾ï¼Œæ˜ç¡®äº†ä¸åŒåˆ†æå±‚é¢çš„è¯æ®æ ‡å‡†ã€‚è¯¥ç ”ç©¶ä¸ä»…ç³»ç»ŸåŒ–äº†å½“å‰çš„å­¦æœ¯æˆæœï¼Œä¹Ÿä¸ºäººå·¥æ™ºèƒ½ç¤¾ä¼šç§‘å­¦çš„æŒç»­è¿›æ­¥ä¸è·¨å­¦ç§‘ç ”ç©¶å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24877v2",
      "published_date": "2025-09-29 14:55:14 UTC",
      "updated_date": "2025-10-27 07:37:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:56:03.050883+00:00"
    },
    {
      "arxiv_id": "2509.24873v1",
      "title": "Uncertainty-Guided Expert-AI Collaboration for Efficient Soil Horizon Annotation",
      "title_zh": "é¢å‘é«˜æ•ˆåœŸå£¤å‘ç”Ÿå±‚æ ‡æ³¨çš„ä¸ç¡®å®šæ€§å¼•å¯¼ä¸“å®¶-AIåä½œ",
      "authors": [
        "Teodor Chiaburu",
        "Vipin Singh",
        "Frank HauÃŸer",
        "Felix BieÃŸmann"
      ],
      "abstract": "Uncertainty quantification is essential in human-machine collaboration, as human agents tend to adjust their decisions based on the confidence of the machine counterpart. Reliably calibrated model uncertainties, hence, enable more effective collaboration, targeted expert intervention and more responsible usage of Machine Learning (ML) systems. Conformal prediction has become a well established model-agnostic framework for uncertainty calibration of ML models, offering statistically valid confidence estimates for both regression and classification tasks. In this work, we apply conformal prediction to $\\textit{SoilNet}$, a multimodal multitask model for describing soil profiles. We design a simulated human-in-the-loop (HIL) annotation pipeline, where a limited budget for obtaining ground truth annotations from domain experts is available when model uncertainty is high. Our experiments show that conformalizing SoilNet leads to more efficient annotation in regression tasks and comparable performance scores in classification tasks under the same annotation budget when tested against its non-conformal counterpart. All code and experiments can be found in our repository: https://github.com/calgo-lab/BGR",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººæœºåä½œä¸­çš„ä¸ç¡®å®šæ€§é‡åŒ–é—®é¢˜ï¼ŒæŒ‡å‡ºå¯é çš„æ¨¡å‹ä¸ç¡®å®šæ€§æ ¡å‡†å¯¹äºå®ç°é«˜æ•ˆä¸“å®¶å¹²é¢„å’Œè´Ÿè´£ä»»åœ°ä½¿ç”¨æœºå™¨å­¦ä¹  (Machine Learning) ç³»ç»Ÿè‡³å…³é‡è¦ã€‚ç ”ç©¶å°†ç¬¦åˆé¢„æµ‹ (Conformal Prediction) æ¡†æ¶åº”ç”¨äº SoilNetï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºæè¿°åœŸå£¤å‰–é¢çš„å¤šæ¨¡æ€å¤šä»»åŠ¡æ¨¡å‹ã€‚ä½œè€…è®¾è®¡äº†ä¸€ä¸ªæ¨¡æ‹Ÿçš„äººæœºååŒ (Human-in-the-Loop, HIL) æ ‡æ³¨æµç¨‹ï¼Œæ—¨åœ¨æ¨¡å‹ä¸ç¡®å®šæ€§è¾ƒé«˜æ—¶ï¼Œåˆ©ç”¨æœ‰é™çš„é¢„ç®—è·å–é¢†åŸŸä¸“å®¶çš„åœ°é¢çœŸå€¼æ ‡æ³¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ç›¸åŒçš„æ ‡æ³¨é¢„ç®—ä¸‹ï¼Œç»è¿‡ç¬¦åˆåŒ–å¤„ç† (Conformalizing) çš„ SoilNet åœ¨å›å½’ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†æ ‡æ³¨æ•ˆç‡ã€‚åœ¨åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•ä¹Ÿå±•ç°å‡ºäº†ä¸éç¬¦åˆåŒ–æ¨¡å‹ç›¸å½“çš„æ€§èƒ½æ°´å¹³ã€‚è¯¥å·¥ä½œé€šè¿‡æœ‰æ•ˆçš„æ ¡å‡†ä¸ç¡®å®šæ€§ï¼Œä¸ºæé«˜åœŸå£¤åœ°å±‚æ ‡æ³¨ä»»åŠ¡çš„è‡ªåŠ¨åŒ–ç¨‹åº¦å’Œèµ„æºåˆ†é…æ•ˆç‡æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "11 pages, 7 figures, presented at ECAI 2025, CLEAR-AI Workshop, Bologna",
      "pdf_url": "https://arxiv.org/pdf/2509.24873v1",
      "published_date": "2025-09-29 14:54:23 UTC",
      "updated_date": "2025-09-29 14:54:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:55:37.388860+00:00"
    },
    {
      "arxiv_id": "2509.24869v2",
      "title": "Retro*: Optimizing LLMs for Reasoning-Intensive Document Retrieval",
      "title_zh": "Retro*ï¼šé¢å‘æ¨ç†å¯†é›†å‹æ–‡æ¡£æ£€ç´¢çš„å¤§è¯­è¨€æ¨¡å‹ä¼˜åŒ–",
      "authors": [
        "Junwei Lan",
        "Jianlyu Chen",
        "Zheng Liu",
        "Chaofan Li",
        "Siqi Bao",
        "Defu Lian"
      ],
      "abstract": "With the growing popularity of LLM agents and RAG, it has become increasingly important to retrieve documents that are essential for solving a task, even when their connection to the task is indirect or implicit. Addressing this problem requires fine-grained reasoning to accurately assess the relevance between the task and each candidate document. This capability, however, poses a significant challenge for existing IR techniques. Despite recent progress in reasoning-enhanced IR, existing approaches still face significant challenges in applicability, scalability, and efficiency. In this work, we propose Retro*, a novel approach for reasoning-intensive document retrieval. Our method introduces a rubric-based relevance scoring mechanism, enabling the model to reason about the relationship between a task and a document based on explicitly defined criteria, whereby producing a fine-grained, interpretable relevance score. Retro* also supports test-time scaling by combining multiple reasoning trajectories via score integration, which produces more reliable relevance estimates. To optimize Retro*'s reasoning capabilities, we introduce a novel reinforcement learning algorithm tailored for its relevance scoring mechanism, which employs two composite rewards to fully exploit the trajectories of each training sample. Our experiments show that Retro* outperforms existing document retrieval methods with notable advantages, leading to state-of-the-art performance on the BRIGHT benchmark.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Retro*ï¼Œä¸€ç§æ—¨åœ¨ä¼˜åŒ– LLMs è¿›è¡Œ Reasoning-Intensive Document Retrieval çš„æ–°æ–¹æ³•ï¼Œè§£å†³äº†ç°æœ‰ IR æŠ€æœ¯åœ¨å¤„ç†ä»»åŠ¡ä¸æ–‡æ¡£é—´é—´æ¥æˆ–éšå«å…³è”æ—¶çš„æŒ‘æˆ˜ã€‚Retro* å¼•å…¥äº†åŸºäº Rubric-based çš„ç›¸å…³æ€§è¯„åˆ†æœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä¾æ®æ˜ç¡®å®šä¹‰çš„æ ‡å‡†è¿›è¡Œæ¨ç†ï¼Œä»è€Œäº§ç”Ÿç»†ç²’åº¦ä¸”å…·å¯è§£é‡Šæ€§çš„ç›¸å…³æ€§å¾—åˆ†ã€‚åŒæ—¶ï¼ŒRetro* é€šè¿‡æ•´åˆå¤šæ¡æ¨ç†è½¨è¿¹æ”¯æŒ Test-time Scalingï¼Œè¿›ä¸€æ­¥æå‡äº†ç›¸å…³æ€§è¯„ä¼°çš„å¯é æ€§ã€‚ä¸ºäº†å¢å¼ºæ¨ç†èƒ½åŠ›ï¼Œç ”ç©¶è€…è¿˜å¼€å‘äº†ä¸€ç§å®šåˆ¶çš„ Reinforcement Learning ç®—æ³•ï¼Œåˆ©ç”¨å¤åˆå¥–åŠ±æœºåˆ¶å……åˆ†æŒ–æ˜è®­ç»ƒæ ·æœ¬ä¸­çš„è½¨è¿¹ä¿¡æ¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRetro* åœ¨ BRIGHT åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº† SOTA æ€§èƒ½ï¼Œå±•ç°å‡ºæ˜¾è‘—ä¼˜äºç°æœ‰æ£€ç´¢æ–¹æ³•çš„ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24869v2",
      "published_date": "2025-09-29 14:53:05 UTC",
      "updated_date": "2025-10-12 09:37:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:55:40.191964+00:00"
    },
    {
      "arxiv_id": "2509.24866v2",
      "title": "Metaphor identification using large language models: A comparison of RAG, prompt engineering, and fine-tuning",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„éšå–»è¯†åˆ«ï¼šRAGã€æç¤ºå·¥ç¨‹ä¸å¾®è°ƒçš„å¯¹æ¯”ç ”ç©¶",
      "authors": [
        "Matteo Fuoli",
        "Weihang Huang",
        "Jeannette Littlemore",
        "Sarah Turner",
        "Ellen Wilding"
      ],
      "abstract": "Metaphor is a pervasive feature of discourse and a powerful lens for examining cognition, emotion, and ideology. Large-scale analysis, however, has been constrained by the need for manual annotation due to the context-sensitive nature of metaphor. This study investigates the potential of large language models (LLMs) to automate metaphor identification in full texts. We compare three methods: (i) retrieval-augmented generation (RAG), where the model is provided with a codebook and instructed to annotate texts based on its rules and examples; (ii) prompt engineering, where we design task-specific verbal instructions; and (iii) fine-tuning, where the model is trained on hand-coded texts to optimize performance. Within prompt engineering, we test zero-shot, few-shot, and chain-of-thought strategies. Our results show that state-of-the-art closed-source LLMs can achieve high accuracy, with fine-tuning yielding a median F1 score of 0.79. A comparison of human and LLM outputs reveals that most discrepancies are systematic, reflecting well-known grey areas and conceptual challenges in metaphor theory. We propose that LLMs can be used to at least partly automate metaphor identification and can serve as a testbed for developing and refining metaphor identification protocols and the theory that underpins them.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)è‡ªåŠ¨è¯†åˆ«å…¨æ–‡ä¸­éšå–»(Metaphor)çš„æ½œåŠ›ï¼Œæ—¨åœ¨è§£å†³éšå–»è¯†åˆ«å¯¹è¯­å¢ƒé«˜åº¦æ•æ„Ÿä¸”é•¿æœŸä¾èµ–äººå·¥æ ‡æ³¨çš„éš¾é¢˜ã€‚ç ”ç©¶ç³»ç»Ÿå¯¹æ¯”äº†ä¸‰ç§å®ç°æ–¹æ³•ï¼šç»“åˆç¼–ç ç°¿è§„åˆ™çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ã€åŒ…å«å¤šç§ç­–ç•¥ï¼ˆZero-shot, Few-shot, Chain-of-thoughtï¼‰çš„æç¤ºå·¥ç¨‹(Prompt Engineering)ä»¥åŠåœ¨äººå·¥æ ‡æ³¨æ–‡æœ¬ä¸Šè¿›è¡Œçš„å¾®è°ƒ(Fine-tuning)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæœ€å…ˆè¿›çš„é—­æº LLMs è¡¨ç°å‡ºæé«˜å‡†ç¡®ç‡ï¼Œå…¶ä¸­å¾®è°ƒæ–¹æ³•æ€§èƒ½æœ€ä¸ºä¼˜è¶Šï¼Œä¸­ä½ F1 åˆ†æ•°è¾¾åˆ° 0.79ã€‚é€šè¿‡å¯¹æ¯”äººç±»ä¸æ¨¡å‹çš„è¾“å‡ºå·®å¼‚ï¼Œç ”ç©¶å‘ç°è¿™äº›ä¸ä¸€è‡´æ€§å¤§å¤šå…·æœ‰ç³»ç»Ÿæ€§ï¼Œæ·±åˆ»åæ˜ äº†éšå–»ç†è®ºä¸­çš„æ ¸å¿ƒäº‰è®®å’Œæ¦‚å¿µæŒ‘æˆ˜ã€‚ä½œè€…æå‡º LLMs ä¸ä»…èƒ½éƒ¨åˆ†å®ç°éšå–»è¯†åˆ«è‡ªåŠ¨åŒ–ï¼Œè¿˜èƒ½ä½œä¸ºä¼˜åŒ–è¯†åˆ«åè®®å’Œå®Œå–„åº•å±‚ç†è®ºçš„å®éªŒå¹³å°ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24866v2",
      "published_date": "2025-09-29 14:50:18 UTC",
      "updated_date": "2025-10-01 14:06:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:55:46.601938+00:00"
    },
    {
      "arxiv_id": "2509.24855v1",
      "title": "PhysicsMinions: Winning Gold Medals in the Latest Physics Olympiads with a Coevolutionary Multimodal Multi-Agent System",
      "title_zh": "PhysicsMinionsï¼šåŸºäºååŒæ¼”åŒ–å¤šæ¨¡æ€å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨æœ€æ–°ç‰©ç†å¥¥æ—åŒ¹å…‹ç«èµ›ä¸­æ‘˜å¾—é‡‘ç‰Œ",
      "authors": [
        "Fangchen Yu",
        "Junchi Yao",
        "Ziyi Wang",
        "Haiyuan Wan",
        "Youling Huang",
        "Bo Zhang",
        "Shuyue Hu",
        "Dongzhan Zhou",
        "Ning Ding",
        "Ganqu Cui",
        "Lei Bai",
        "Wanli Ouyang",
        "Peng Ye"
      ],
      "abstract": "Physics is central to understanding and shaping the real world, and the ability to solve physics problems is a key indicator of real-world physical intelligence. Physics Olympiads, renowned as the crown of competitive physics, provide a rigorous testbed requiring complex reasoning and deep multimodal understanding, yet they remain largely underexplored in AI research. Existing approaches are predominantly single-model based, and open-source MLLMs rarely reach gold-medal-level performance. To address this gap, we propose PhysicsMinions, a coevolutionary multi-agent system for Physics Olympiad. Its architecture features three synergistic studios: a Visual Studio to interpret diagrams, a Logic Studio to formulate solutions, and a Review Studio to perform dual-stage verification. The system coevolves through an iterative refinement loop where feedback from the Review Studio continuously guides the Logic Studio, enabling the system to self-correct and converge towards the ground truth. Evaluated on the HiPhO benchmark spanning 7 latest physics Olympiads, PhysicsMinions delivers three major breakthroughs: (i) Strong generalization: it consistently improves both open-source and closed-source models of different sizes, delivering clear benefits over their single-model baselines; (ii) Historic breakthroughs: it elevates open-source models from only 1-2 to 6 gold medals across 7 Olympiads, achieving the first-ever open-source gold medal in the latest International Physics Olympiad (IPhO) under the average-score metric; and (iii) Scaling to human expert: it further advances the open-source Pass@32 score to 26.8/30 points on the latest IPhO, ranking 4th of 406 contestants and far surpassing the top single-model score of 22.7 (ranked 22nd). Generally, PhysicsMinions offers a generalizable framework for Olympiad-level problem solving, with the potential to extend across disciplines.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†PhysicsMinionsï¼Œä¸€ä¸ªæ—¨åœ¨è§£å†³ç‰©ç†å¥¥æ—åŒ¹å…‹ç«èµ›éš¾é¢˜çš„ååŒæ¼”åŒ–å¤šæ¨¡æ€å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ(Coevolutionary Multimodal Multi-Agent System)ã€‚ç³»ç»Ÿæ¶æ„åŒ…å«ä¸‰ä¸ªååŒå·¥ä½œçš„æ¨¡å—ï¼šç”¨äºè§£è¯»å›¾è¡¨çš„Visual Studioã€åˆ¶å®šè§£é¢˜æ–¹æ¡ˆçš„Logic Studioä»¥åŠæ‰§è¡ŒåŒé˜¶æ®µéªŒè¯çš„Review Studioã€‚PhysicsMinionsé€šè¿‡è¿­ä»£ç»†åŒ–å¾ªç¯ï¼Œåˆ©ç”¨Review Studioçš„åé¦ˆå¼•å¯¼Logic Studioè¿›è¡ŒæŒç»­è‡ªæˆ‘ä¿®æ­£ï¼Œå®ç°äº†ç³»ç»Ÿçš„ååŒæ¼”åŒ–ä¸è§£é¢˜æ”¶æ•›ã€‚åœ¨æ¶µç›–7å±Šæœ€æ–°ç‰©ç†ç«èµ›çš„HiPhOåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥ç³»ç»Ÿæ˜¾è‘—æå‡äº†ä¸åŒè§„æ¨¡å¼€æºåŠé—­æºæ¨¡å‹çš„æ€§èƒ½ï¼Œå±•ç°å‡ºæå¼ºçš„é€šç”¨æ€§(Generalization)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPhysicsMinionsåŠ©åŠ›å¼€æºæ¨¡å‹åœ¨7åœºç«èµ›ä¸­æ–©è·6æšé‡‘ç‰Œï¼Œå¹¶å®ç°äº†å¼€æºæ¨¡å‹åœ¨å›½é™…ç‰©ç†å¥¥æ—åŒ¹å…‹ç«èµ›(IPhO)å¹³å‡åˆ†æŒ‡æ ‡ä¸‹é¦–æ¬¡è·å¾—é‡‘ç‰Œçš„å†å²æ€§çªç ´ã€‚æ­¤å¤–ï¼Œå…¶åœ¨æœ€æ–°IPhOä¸­çš„å¼€æºPass@32å¾—åˆ†ä½åˆ—406åå‚èµ›è€…ä¸­çš„ç¬¬4åï¼Œè¿œè¶…ç°æœ‰æœ€ä½³å•æ¨¡å‹(Single-model)çš„è¡¨ç°ã€‚è¯¥æ¡†æ¶ä¸ºå¥¥æ—åŒ¹å…‹çº§åˆ«çš„å¤æ‚é—®é¢˜è§£å†³æä¾›äº†å¯æ¨å¹¿çš„èŒƒå¼ï¼Œå…·æœ‰è·¨å­¦ç§‘åº”ç”¨çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24855v1",
      "published_date": "2025-09-29 14:40:53 UTC",
      "updated_date": "2025-09-29 14:40:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:55:50.185034+00:00"
    },
    {
      "arxiv_id": "2509.25292v3",
      "title": "A Measurement Study of Model Context Protocol Ecosystem",
      "title_zh": "Model Context Protocol ç”Ÿæ€ç³»ç»Ÿæµ‹é‡ç ”ç©¶",
      "authors": [
        "Hechuan Guo",
        "Yongle Hao",
        "Yue Zhang",
        "Minghui Xu",
        "Peizhuo Lv",
        "Jiezhi Chen",
        "Xiuzhen Cheng"
      ],
      "abstract": "The Model Context Protocol (MCP) has been proposed as a unifying standard for connecting large language models (LLMs) with external tools and resources, promising the same role for AI integration that HTTP and USB played for the Web and peripherals. Yet, despite rapid adoption and hype, its trajectory remains uncertain. Are MCP marketplaces truly growing, or merely inflated by placeholders and abandoned prototypes? Are servers secure and privacy-preserving, or do they expose users to systemic risks? And do clients converge on standardized protocols, or remain fragmented across competing designs? In this paper, we present the first large-scale empirical study of the MCP ecosystem. We design and implement MCPCrawler, a systematic measurement framework that collects and normalizes data from six major markets. Over a 14-day campaign, MCPCrawler aggregated 17,630 raw entries, of which 8,401 valid projects (8,060 servers and 341 clients) were analyzed. Our results reveal that more than half of listed projects are invalid or low-value, that servers face structural risks including dependency monocultures and uneven maintenance, and that clients exhibit a transitional phase in protocol and connection patterns. Together, these findings provide the first evidence-based view of the MCP ecosystem, its risks, and its future trajectory.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ¨¡å‹ä¸Šä¸‹æ–‡åè®® (Model Context Protocol, MCP) ç”Ÿæ€ç³»ç»Ÿè¿›è¡Œäº†é¦–æ¬¡å¤§è§„æ¨¡å®è¯ç ”ç©¶ï¼Œæ—¨åœ¨è¯„ä¼°è¿™ä¸€æ—¨åœ¨ç»Ÿä¸€å¤§è¯­è¨€æ¨¡å‹ (LLMs) ä¸å¤–éƒ¨å·¥å…·è¿æ¥æ ‡å‡†çš„å®é™…å‘å±•ç°çŠ¶ã€‚ä½œè€…è®¾è®¡å¹¶å®ç°äº†åä¸º MCPCrawler çš„ç³»ç»Ÿæ€§æµ‹é‡æ¡†æ¶ï¼Œä»å…­ä¸ªä¸»è¦å¸‚åœºæ”¶é›†å¹¶å½’ä¸€åŒ–æ•°æ®ï¼Œåœ¨ä¸ºæœŸ14å¤©çš„è¡ŒåŠ¨ä¸­è·å–äº†17,630æ¡åŸå§‹æ¡ç›®ã€‚é€šè¿‡å¯¹å…¶ä¸­8,401ä¸ªæœ‰æ•ˆé¡¹ç›®ï¼ˆåŒ…æ‹¬8,060ä¸ªæœåŠ¡å™¨å’Œ341ä¸ªå®¢æˆ·ç«¯ï¼‰çš„æ·±å…¥åˆ†æï¼Œç ”ç©¶å‘ç°è¶…è¿‡ä¸€åŠçš„åˆ—å‡ºé¡¹ç›®ä¸ºæ— æ•ˆæˆ–ä½ä»·å€¼é¡¹ç›®ï¼Œæš—ç¤ºå¸‚åœºå­˜åœ¨è™šé«˜ç°è±¡ã€‚åœ¨å®‰å…¨æ€§æ–¹é¢ï¼ŒæœåŠ¡å™¨é¢ä¸´ç€åŒ…æ‹¬ä¾èµ–å•ä¸€åŒ– (dependency monocultures) å’Œç»´æŠ¤ä¸å‡åœ¨å†…çš„ç»“æ„æ€§é£é™©ã€‚æ­¤å¤–ï¼Œç ”ç©¶æŒ‡å‡ºå®¢æˆ·ç«¯åœ¨åè®®å’Œè¿æ¥æ¨¡å¼ä¸Šæ­£å¤„äºè¿‡æ¸¡é˜¶æ®µï¼Œå°šæœªå®Œå…¨å®ç°æ ‡å‡†åŒ–ã€‚è¯¥å·¥ä½œä¸º MCP ç”Ÿæ€ç³»ç»Ÿçš„ç°çŠ¶ã€é£é™©å’Œæœªæ¥è½¨è¿¹æä¾›äº†é¦–ä¸ªåŸºäºè¯æ®çš„å…¨æ™¯è§†å›¾ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25292v3",
      "published_date": "2025-09-29 14:29:20 UTC",
      "updated_date": "2025-11-15 14:28:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:55:57.791797+00:00"
    },
    {
      "arxiv_id": "2510.01271v1",
      "title": "Identifying Information-Transfer Nodes in a Recurrent Neural Network Reveals Dynamic Representations",
      "title_zh": "è¯†åˆ«å¾ªç¯ç¥ç»ç½‘ç»œä¸­çš„ä¿¡æ¯ä¼ é€’èŠ‚ç‚¹æ­ç¤ºåŠ¨æ€è¡¨å¾",
      "authors": [
        "Arend Hintze",
        "Asadullah Najam",
        "Jory Schossau"
      ],
      "abstract": "Understanding the internal dynamics of Recurrent Neural Networks (RNNs) is crucial for advancing their interpretability and improving their design. This study introduces an innovative information-theoretic method to identify and analyze information-transfer nodes within RNNs, which we refer to as \\textit{information relays}. By quantifying the mutual information between input and output vectors across nodes, our approach pinpoints critical pathways through which information flows during network operations. We apply this methodology to both synthetic and real-world time series classification tasks, employing various RNN architectures, including Long Short-Term Memory (LSTM) networks and Gated Recurrent Units (GRUs). Our results reveal distinct patterns of information relay across different architectures, offering insights into how information is processed and maintained over time. Additionally, we conduct node knockout experiments to assess the functional importance of identified nodes, significantly contributing to explainable artificial intelligence by elucidating how specific nodes influence overall network behavior. This study not only enhances our understanding of the complex mechanisms driving RNNs but also provides a valuable tool for designing more robust and interpretable neural networks.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹å¾ªç¯ç¥ç»ç½‘ç»œ(Recurrent Neural Networks)å†…éƒ¨åŠ¨æ€éš¾ä»¥ç†è§£çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åˆ›æ–°çš„ä¿¡æ¯è®ºæ–¹æ³•æ¥è¯†åˆ«å’Œåˆ†æå…¶ä¸­çš„ä¿¡æ¯ä¼ è¾“èŠ‚ç‚¹ï¼Œå¹¶å°†å…¶å®šä¹‰ä¸ºä¿¡æ¯ä¸­ç»§(information relays)ã€‚è¯¥æ–¹æ³•é€šè¿‡é‡åŒ–èŠ‚ç‚¹é—´è¾“å…¥å’Œè¾“å‡ºå‘é‡çš„äº’ä¿¡æ¯(mutual information)ï¼Œç²¾ç¡®å®šä½äº†ç½‘ç»œè¿è¡ŒæœŸé—´ä¿¡æ¯æµåŠ¨çš„å…³é”®è·¯å¾„ã€‚ç ”ç©¶è€…å°†è¯¥æ–¹æ³•åº”ç”¨äºåˆæˆæ•°æ®åŠçœŸå®ä¸–ç•Œçš„æ—¶é—´åºåˆ—åˆ†ç±»ä»»åŠ¡ï¼Œæ¶µç›–äº†é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ(LSTM)å’Œé—¨æ§å¾ªç¯å•å…ƒ(GRU)ç­‰å¤šç§æ¶æ„ã€‚å®éªŒç»“æœæ­ç¤ºäº†ä¸åŒæ¶æ„ä¸­äº’ä¸ç›¸åŒçš„ä¿¡æ¯ä¸­ç»§æ¨¡å¼ï¼Œä¸ºç†è§£ä¿¡æ¯éšæ—¶é—´çš„å¤„ç†å’Œç»´æŠ¤æœºåˆ¶æä¾›äº†æ·±å…¥è§è§£ã€‚æ­¤å¤–ï¼Œé€šè¿‡èŠ‚ç‚¹å‰”é™¤(node knockout)å®éªŒéªŒè¯äº†æ‰€è¯†åˆ«èŠ‚ç‚¹çš„åŠŸèƒ½é‡è¦æ€§ï¼Œé˜æ˜äº†ç‰¹å®šèŠ‚ç‚¹å¯¹æ•´ä½“ç½‘ç»œè¡Œä¸ºçš„å½±å“ã€‚è¯¥ç ”ç©¶ä¸ä»…å¢å¼ºäº†å¯¹RNNså¤æ‚æœºåˆ¶çš„ç†è§£ï¼Œä¹Ÿä¸ºè®¾è®¡æ›´å…·é²æ£’æ€§å’Œå¯è§£é‡Šæ€§(interpretable)çš„ç¥ç»ç½‘ç»œæä¾›äº†æœ‰åŠ›å·¥å…·ï¼Œæ˜¾è‘—è´¡çŒ®äºå¯è§£é‡Šäººå·¥æ™ºèƒ½(Explainable AI)é¢†åŸŸã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01271v1",
      "published_date": "2025-09-29 14:24:42 UTC",
      "updated_date": "2025-09-29 14:24:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:55:55.802795+00:00"
    },
    {
      "arxiv_id": "2509.24841v2",
      "title": "A Hierarchical Error Framework for Reliable Automated Coding in Communication Research: Applications to Health and Political Communication",
      "title_zh": "ä¼ æ’­å­¦ç ”ç©¶ä¸­å¯é è‡ªåŠ¨ç¼–ç çš„åˆ†å±‚é”™è¯¯æ¡†æ¶ï¼šåœ¨å¥åº·ä¸æ”¿æ²»ä¼ æ’­ä¸­çš„åº”ç”¨",
      "authors": [
        "Zhilong Zhao",
        "Yindi Liu"
      ],
      "abstract": "Automated content analysis increasingly supports communication research, yet scaling manual coding into computational pipelines raises concerns about measurement reliability and validity. We introduce a Hierarchical Error Correction (HEC) framework that treats model failures as layered measurement errors (knowledge gaps, reasoning limitations, and complexity constraints) and targets the layers that most affect inference. The framework implements a three-phase methodology: systematic error profiling across hierarchical layers, targeted intervention design matched to dominant error sources, and rigorous validation with statistical testing. Evaluating HEC across health communication (medical specialty classification) and political communication (bias detection), and legal tasks, we validate the approach with five diverse large language models. Results show average accuracy gains of 11.2 percentage points (p < .001, McNemar's test) and stable conclusions via reduced systematic misclassification. Cross-model validation demonstrates consistent improvements (range: +6.8 to +14.6pp), with effectiveness concentrated in moderate-to-high baseline tasks (50-85% accuracy). A boundary study reveals diminished returns in very high-baseline (>85%) or precision-matching tasks, establishing applicability limits. We map layered errors to threats to construct and criterion validity and provide a transparent, measurement-first blueprint for diagnosing error profiles, selecting targeted interventions, and reporting reliability/validity evidence alongside accuracy. This applies to automated coding across communication research and the broader social sciences.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†å±‚çº§è¯¯å·®ä¿®æ­£(Hierarchical Error Correction, HEC)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ æ’­å­¦ç ”ç©¶ä¸­è‡ªåŠ¨åŒ–ç¼–ç é¢ä¸´çš„æµ‹é‡å¯é æ€§å’Œæ•ˆåº¦æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶å°†æ¨¡å‹å¤±æ•ˆè§†ä¸ºç”±çŸ¥è¯†å·®è·(knowledge gaps)ã€æ¨ç†é™åˆ¶(reasoning limitations)å’Œå¤æ‚æ€§çº¦æŸ(complexity constraints)æ„æˆçš„åˆ†å±‚æµ‹é‡è¯¯å·®ï¼Œå¹¶å®æ–½äº†åŒ…å«ç³»ç»Ÿè¯¯å·®å‰–æã€é’ˆå¯¹æ€§å¹²é¢„è®¾è®¡åŠä¸¥æ ¼ç»Ÿè®¡éªŒè¯åœ¨å†…çš„ä¸‰é˜¶æ®µæ–¹æ³•è®ºã€‚é€šè¿‡åœ¨å¥åº·ä¼ æ’­ã€æ”¿æ²»ä¼ æ’­åŠæ³•å¾‹ä»»åŠ¡ä¸­å¯¹äº”ç§å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)è¿›è¡Œè¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºHECå¹³å‡æå‡äº†11.2ä¸ªç™¾åˆ†ç‚¹çš„å‡†ç¡®ç‡ï¼Œå¹¶èƒ½æœ‰æ•ˆå‡å°‘ç³»ç»Ÿæ€§è¯¯åˆ†ç±»ä»¥ç¡®ä¿ç»“è®ºçš„ç¨³å¥æ€§ã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°è¯¥æ–¹æ³•åœ¨åŸºå‡†å‡†ç¡®ç‡ä¸º50-85%çš„ä¸­é«˜éš¾åº¦ä»»åŠ¡ä¸­æ•ˆæœæœ€æ˜¾è‘—ï¼Œä½†åœ¨æé«˜åŸºå‡†ä»»åŠ¡ä¸­æ”¶ç›Šé€’å‡ã€‚è¯¥ç ”ç©¶ä¸ºç¤¾ä¼šç§‘å­¦é¢†åŸŸçš„è‡ªåŠ¨åŒ–ç¼–ç æä¾›äº†ä¸€å¥—é€æ˜çš„ã€ä»¥æµ‹é‡ä¸ºæ ¸å¿ƒçš„è“å›¾ï¼Œé€šè¿‡å°†åˆ†å±‚è¯¯å·®æ˜ å°„è‡³æ„å»ºæ•ˆåº¦(construct validity)ä¸å‡†åˆ™æ•ˆåº¦(criterion validity)çš„å¨èƒï¼Œä¸ºè¯Šæ–­è¯¯å·®åˆ†å¸ƒåŠæå‡ç ”ç©¶æ•ˆåº¦æä¾›äº†å…³é”®æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Version 2: Enhanced clarification of precision-matching task characteristics and framework applicability conditions. 20 pages, 4 figures, 4 tables. Replication package available at https://doi.org/10.7910/DVN/NDXVLZ",
      "pdf_url": "https://arxiv.org/pdf/2509.24841v2",
      "published_date": "2025-09-29 14:21:05 UTC",
      "updated_date": "2025-10-24 07:36:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:56:02.480400+00:00"
    },
    {
      "arxiv_id": "2509.24836v3",
      "title": "Pushing LLMs to Their Logical Reasoning Bound: The Role of Data Reasoning Intensity",
      "title_zh": "æ¢ç´¢å¤§è¯­è¨€æ¨¡å‹çš„é€»è¾‘æ¨ç†æé™ï¼šæ•°æ®æ¨ç†å¼ºåº¦çš„ä½œç”¨",
      "authors": [
        "Zhen Bi",
        "Zhenlin Hu",
        "Jinnan Yang",
        "Mingyang Chen",
        "Cheng Deng",
        "Yida Xue",
        "Zeyu Yang",
        "Qing Shen",
        "Zhenfang Liu",
        "Kang Zhao",
        "Ningyu Zhang",
        "Jungang Lou"
      ],
      "abstract": "Recent advances in large language models (LLMs) highlight the importance of training data structure and quality in shaping reasoning behavior. However, most existing approaches focus on transforming data formats while neglecting the internal reasoning complexity of training samples, leaving the reasoning potential of data under-explored and underutilized. In this work, we posit that LLM logical reasoning performance is jointly constrained by the potential of the training data and the cognitive capacity of the model. To make this relationship measurable, we introduce Data Reasoning Intensity (DRI), a novel metric that quantifies the latent logical reasoning complexity of samples by decomposing and aggregating their logical structures. This allows us to analyze how well current LLMs utilize logical reasoning signals and identify performance gaps relative to data potential. Based on this insight, we introduce a re-cognizing optimization strategy that systematically enhances the logical reasoning intensity of training data. Rather than increasing data volume, our method re-optimizes existing samples to better align with the LLM's logical reasoning boundary. Extensive experiments show that our approach significantly improves performance and generalization over data-centric strategies. We further validate our method under a reinforcement learning framework. Our results indicate that prioritizing reasoning complexity in data rather than sheer scale or superficial form is essential to realizing LLMs' full cognitive potential.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è®­ç»ƒæ•°æ®çš„å†…éƒ¨é€»è¾‘æ¨ç†å¤æ‚åº¦å¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) æ¨ç†èƒ½åŠ›çš„å½±å“ï¼ŒæŒ‡å‡ºå½“å‰æ–¹æ³•å¾€å¾€å¿½è§†äº†æ•°æ®æœ¬èº«çš„æ¨ç†æ½œåŠ›ã€‚ä½œè€…æå‡ºäº† Data Reasoning Intensity (DRI) è¿™ä¸€æ–°é¢–æŒ‡æ ‡ï¼Œé€šè¿‡åˆ†è§£å’Œèšåˆé€»è¾‘ç»“æ„æ¥é‡åŒ–æ ·æœ¬ä¸­æ½œåœ¨çš„é€»è¾‘æ¨ç†å¤æ‚åº¦ï¼Œä»è€Œè¡¡é‡æ¨¡å‹è®¤çŸ¥èƒ½åŠ›ä¸æ•°æ®æ½œåŠ›ä¹‹é—´çš„å…³ç³»ã€‚åŸºäºæ­¤æ´å¯Ÿï¼Œç ”ç©¶å¼•å…¥äº†ä¸€ç§é‡æ–°è®¤çŸ¥ä¼˜åŒ–ç­–ç•¥ (re-cognizing optimization strategy)ï¼Œé€šè¿‡ç³»ç»Ÿæ€§åœ°å¢å¼ºç°æœ‰è®­ç»ƒæ•°æ®çš„é€»è¾‘æ¨ç†å¼ºåº¦ï¼Œè€Œéå•çº¯å¢åŠ æ•°æ®è§„æ¨¡ï¼Œä½¿å…¶æ›´å¥½åœ°å¥‘åˆæ¨¡å‹çš„é€»è¾‘æ¨ç†è¾¹ç•Œã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æå‡æ¨¡å‹æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ä»¥æ•°æ®ä¸ºä¸­å¿ƒ (data-centric) çš„ç­–ç•¥ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) æ¡†æ¶ä¸‹ä¹Ÿå¾—åˆ°äº†éªŒè¯ï¼Œè¯æ˜äº†ä¼˜å…ˆè€ƒè™‘æ•°æ®çš„æ¨ç†å¤æ‚åº¦è€Œéè§„æ¨¡æˆ–è¡¨é¢å½¢å¼ï¼Œæ˜¯æ¿€å‘ LLMs å…¨é¢è®¤çŸ¥æ½œåŠ›çš„å…³é”®ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24836v3",
      "published_date": "2025-09-29 14:20:04 UTC",
      "updated_date": "2025-10-04 03:29:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:57:18.295156+00:00"
    },
    {
      "arxiv_id": "2509.24832v2",
      "title": "SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts via Token-Level LSH Matching",
      "title_zh": "SemShareKVï¼šåŸºäº Token çº§ LSH åŒ¹é…çš„è¯­ä¹‰ç›¸ä¼¼æç¤ºè¯é«˜æ•ˆ KV ç¼“å­˜å…±äº«",
      "authors": [
        "Xinye Zhao",
        "Spyridon Mastorakis"
      ],
      "abstract": "As large language models (LLMs) continue to scale, the memory footprint of key-value (KV) caches during inference has become a significant bottleneck. Existing approaches primarily focus on compressing KV caches within a single prompt or reusing shared prefixes or frequently ocurred text segments across prompts. However, such strategies are limited in scenarios where prompts are semantically similar but lexically different, which frequently occurs in tasks such as multi-document summarization and conversational agents. We propose \\textit{SemShareKV}, a KV cache sharing and compression framework that accelerates LLM inference by reusing KVCache in semantically similar prompts. Instead of relying on exact token matches, SemShareKV applies fuzzy token matching using locality-sensitive hashing (LSH) on token embeddings and incorporates Rotary Position Embedding (RoPE) to better preserve positional information. By selectively reusing relevant key-value pairs from a reference prompt's cache, SemShareKV reduces redundant computation while maintaining output quality. Experiments on diverse summarization datasets show up to 6.25$\\times$ speedup and 42\\% lower GPU memory usage with 5k tokens input, with negligible quality degradation. These results highlight the potential of semantic-aware cache sharing for efficient LLM inference.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SemShareKVï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹(LLMs)æ¨ç†è¿‡ç¨‹ä¸­ Key-Value (KV) ç¼“å­˜å†…å­˜ç“¶é¢ˆçš„ç¼“å­˜å…±äº«ä¸å‹ç¼©æ¡†æ¶ã€‚é’ˆå¯¹å¤šæ–‡æ¡£æ‘˜è¦å’Œå¯¹è¯ä»£ç†ç­‰åœºæ™¯ä¸­æç¤ºè¯è¯­ä¹‰ç›¸ä¼¼ä½†è¯æ±‡ä¸åŒçš„å±€é™ï¼ŒSemShareKV æ”¾å¼ƒäº†ä¼ ç»Ÿçš„ç²¾ç¡®åŒ¹é…æ–¹å¼ï¼Œè½¬è€Œåˆ©ç”¨å±€éƒ¨æ•æ„Ÿå“ˆå¸Œ(Locality-Sensitive Hashing, LSH)å¯¹ Token åµŒå…¥è¿›è¡Œæ¨¡ç³ŠåŒ¹é…ã€‚è¯¥æ¡†æ¶è¿˜é›†æˆäº†æ—‹è½¬ä½ç½®ç¼–ç (RoPE)ä»¥æ›´å¥½åœ°ä¿ç•™ä½ç½®ä¿¡æ¯ï¼Œä»è€Œå®ç°å¯¹å‚è€ƒæç¤ºè¯ç¼“å­˜ä¸­ç›¸å…³é”®å€¼å¯¹çš„é€‰æ‹©æ€§é‡ç”¨ã€‚å®éªŒè¯æ˜ï¼Œåœ¨å¤„ç† 5k é•¿åº¦çš„è¾“å…¥æ—¶ï¼ŒSemShareKV å¯å®ç°é«˜è¾¾ 6.25 å€çš„æ¨ç†åŠ é€Ÿï¼Œå¹¶å‡å°‘ 42% çš„ GPU æ˜¾å­˜ä½¿ç”¨ã€‚åœ¨æ˜¾è‘—é™ä½å†—ä½™è®¡ç®—çš„åŒæ—¶ï¼Œè¯¥æ–¹æ³•ä¿æŒäº†æé«˜çš„è¾“å‡ºè´¨é‡ï¼Œä»…æœ‰å¾®ä¸è¶³é“çš„æ€§èƒ½ä¸‹é™ã€‚è¿™é¡¹ç ”ç©¶å±•ç¤ºäº†è¯­ä¹‰æ„ŸçŸ¥ç¼“å­˜å…±äº«åœ¨æå‡ LLM æ¨ç†æ•ˆç‡æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "11 figures, 14pages",
      "pdf_url": "https://arxiv.org/pdf/2509.24832v2",
      "published_date": "2025-09-29 14:16:13 UTC",
      "updated_date": "2025-12-16 22:36:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:56:19.380008+00:00"
    },
    {
      "arxiv_id": "2509.24828v1",
      "title": "Evaluating SAP Joule for Code Generation",
      "title_zh": "SAP Joule ä»£ç ç”Ÿæˆèƒ½åŠ›è¯„ä¼°",
      "authors": [
        "Joshua Heisler",
        "Johannes Reisinger",
        "Andreas Fischer"
      ],
      "abstract": "SAP has released its own proprietary generative model SAP Joule, intended for various generative tasks, including serving as a code assistant for software engineers. While Joule is yet not focused on SAP-specific ABAP code generation, it can be used for other common languages, including Javascript. This paper compares SAP Joules Javascript coding capabilities against a total of 29 other models using the HumanEval-X Javascript benchmark. SAP Joule achieves a strict accuracy of 80.49% as the fifth best model in our evaluation. To the best of our knowledge, this is the first comparative evaluation of SAP Joule code generation capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº† SAP æ¨å‡ºçš„ä¸“æœ‰ç”Ÿæˆå¼æ¨¡å‹ SAP Joule åœ¨ä»£ç ç”Ÿæˆæ–¹é¢çš„æ€§èƒ½ï¼Œæ—¨åœ¨åˆ†æå…¶ä½œä¸ºè½¯ä»¶è¾…åŠ©å·¥å…·çš„å®é™…æ•ˆç”¨ã€‚è™½ç„¶ SAP Joule ç›®å‰å°šæœªä¸“æ³¨äº SAP ç‰¹æœ‰çš„ ABAP ä»£ç ç”Ÿæˆï¼Œä½†å®ƒå·²å…·å¤‡å¤„ç† JavaScript ç­‰é€šç”¨è¯­è¨€çš„èƒ½åŠ›ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨ HumanEval-X JavaScript åŸºå‡†æµ‹è¯•ï¼Œå°† SAP Joule çš„ç¼–ç¨‹èƒ½åŠ›ä¸æ€»å…± 29 ä¸ªå…¶ä»–æ¨¡å‹è¿›è¡Œäº†å¯¹æ¯”ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSAP Joule å®ç°äº† 80.49% çš„ä¸¥è°¨å‡†ç¡®åº¦ (strict accuracy)ï¼Œåœ¨è¯„ä¼°æ’åä¸­ä½å±…ç¬¬äº”ã€‚æ®ç ”ç©¶å›¢é˜Ÿæ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡é’ˆå¯¹ SAP Joule ä»£ç ç”Ÿæˆèƒ½åŠ›è¿›è¡Œçš„æ¯”è¾ƒæ€§è¯„ä¼°ç ”ç©¶ï¼Œä¸ºç†è§£è¯¥ä¸“æœ‰æ¨¡å‹çš„æ€§èƒ½è¡¨ç°æä¾›äº†é‡è¦ä¾æ®ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24828v1",
      "published_date": "2025-09-29 14:13:40 UTC",
      "updated_date": "2025-09-29 14:13:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:56:20.861375+00:00"
    },
    {
      "arxiv_id": "2509.24827v2",
      "title": "Putnam-like dataset summary: LLMs as mathematical competition contestants",
      "title_zh": "ç±» Putnam æ•°æ®é›†ç»¼è¿°ï¼šä½œä¸ºæ•°å­¦ç«èµ›å‚èµ›è€…çš„å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Bartosz Bieganowski",
        "Daniel Strzelecki",
        "Robert Skiba",
        "Mateusz Topolewski"
      ],
      "abstract": "In this paper we summarize the results of the Putnam-like benchmark published by Google DeepMind. This dataset consists of 96 original problems in the spirit of the Putnam Competition and 576 solutions of LLMs. We analyse the performance of models on this set of problems to verify their ability to solve problems from mathematical contests.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹ Google DeepMind å‘å¸ƒçš„ Putnam-like åŸºå‡†æµ‹è¯•æ•°æ®é›†è¿›è¡Œäº†æ€»ç»“ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) åœ¨è§£å†³æ•°å­¦ç«èµ›é—®é¢˜ä¸Šçš„è¡¨ç°ã€‚è¯¥æ•°æ®é›†åŒ…å« 96 é“ç§‰æ‰¿ Putnam Competition ç²¾ç¥çš„åŸåˆ›é¢˜ç›®ï¼Œä»¥åŠ 576 ä»½ç”± LLMs ç”Ÿæˆçš„è§£é¢˜æ–¹æ¡ˆã€‚ç ”ç©¶äººå‘˜é€šè¿‡åˆ†ææ¨¡å‹åœ¨è¿™äº›ç‰¹å®šé—®é¢˜é›†ä¸Šçš„è¡¨ç°ï¼Œæ·±å…¥éªŒè¯äº†å½“å‰ AI æ¨¡å‹è§£å†³ç«èµ›çº§æ•°å­¦æŒ‘æˆ˜çš„èƒ½åŠ›ã€‚è¿™ä¸€æ±‡æ€»åˆ†æä¸ºç†è§£ LLMs çš„æ•°å­¦æ¨ç†èƒ½åŠ›æä¾›äº†å®è¯æ•°æ®ï¼Œå¹¶ä¸ºæœªæ¥é«˜çº§æ•°å­¦æ™ºèƒ½çš„å‘å±•å¥ å®šäº†è¯„ä¼°åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "11 pages, 11 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.24827v2",
      "published_date": "2025-09-29 14:13:10 UTC",
      "updated_date": "2025-10-03 16:46:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:56:30.126790+00:00"
    },
    {
      "arxiv_id": "2509.24823v1",
      "title": "Of-SemWat: High-payload text embedding for semantic watermarking of AI-generated images with arbitrary size",
      "title_zh": "Of-SemWatï¼šé¢å‘ä»»æ„å°ºå¯¸ AI ç”Ÿæˆå›¾åƒè¯­ä¹‰æ°´å°çš„é«˜å®¹é‡æ–‡æœ¬åµŒå…¥",
      "authors": [
        "Benedetta Tondi",
        "Andrea Costanzo",
        "Mauro Barni"
      ],
      "abstract": "We propose a high-payload image watermarking method for textual embedding, where a semantic description of the image - which may also correspond to the input text prompt-, is embedded inside the image. In order to be able to robustly embed high payloads in large-scale images - such as those produced by modern AI generators - the proposed approach builds upon a traditional watermarking scheme that exploits orthogonal and turbo codes for improved robustness, and integrates frequency-domain embedding and perceptual masking techniques to enhance watermark imperceptibility. Experiments show that the proposed method is extremely robust against a wide variety of image processing, and the embedded text can be retrieved also after traditional and AI inpainting, permitting to unveil the semantic modification the image has undergone via image-text mismatch analysis.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Of-SemWatï¼Œä¸€ç§é’ˆå¯¹ AI ç”Ÿæˆå›¾åƒçš„é«˜å®¹é‡ (high-payload) æ–‡æœ¬åµŒå…¥è¯­ä¹‰æ°´å°æ–¹æ³•ï¼Œæ—¨åœ¨å°†å›¾åƒçš„è¯­ä¹‰æè¿°æˆ–æ–‡æœ¬æç¤º (text prompt) ç›´æ¥åµŒå…¥åˆ°å›¾åƒå†…éƒ¨ã€‚ä¸ºäº†åœ¨ç°ä»£ AI ç”Ÿæˆçš„å¤§å°ºå¯¸å›¾åƒä¸­ç¨³å¥åœ°åµŒå…¥é«˜å®¹é‡ä¿¡æ¯ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†æ­£äº¤ç  (orthogonal codes) å’Œ Turbo ç  (turbo codes) ä»¥å¢å¼ºé²æ£’æ€§ï¼Œå¹¶é›†æˆäº†é¢‘åŸŸåµŒå…¥ (frequency-domain embedding) å’Œæ„ŸçŸ¥é®è”½ (perceptual masking) æŠ€æœ¯æ¥æå‡æ°´å°çš„ä¸å¯è§æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæŠµæŠ—å¤šç§å›¾åƒå¤„ç†æ‰‹æ®µï¼Œä¸”åœ¨ç»å†ä¼ ç»Ÿæˆ– AI ä¿®å¤ (inpainting) åä»èƒ½å‡†ç¡®æå–åµŒå…¥æ–‡æœ¬ã€‚é€šè¿‡æå–æ–‡æœ¬ä¸å›¾åƒå†…å®¹çš„å¤±é…åˆ†æ (image-text mismatch analysis)ï¼Œè¯¥æŠ€æœ¯å¯ä»¥æ­ç¤ºå›¾åƒæ‰€ç»å†çš„è¯­ä¹‰ç¯¡æ”¹ï¼Œä¸º AI ç”Ÿæˆå†…å®¹çš„å®‰å…¨ç›‘æµ‹ä¸æº¯æºæä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "5 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.24823v1",
      "published_date": "2025-09-29 14:10:15 UTC",
      "updated_date": "2025-09-29 14:10:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:56:26.202828+00:00"
    },
    {
      "arxiv_id": "2509.24819v1",
      "title": "Intelligent Optimization of Wireless Access Point Deployment for Communication-Based Train Control Systems Using Deep Reinforcement Learning",
      "title_zh": "åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„åŸºäºé€šä¿¡çš„åˆ—è½¦æ§åˆ¶ç³»ç»Ÿæ— çº¿æ¥å…¥ç‚¹éƒ¨ç½²æ™ºèƒ½ä¼˜åŒ–",
      "authors": [
        "Kunyu Wu",
        "Qiushi Zhao",
        "Zihan Feng",
        "Yunxi Mu",
        "Hao Qin",
        "Xinyu Zhang",
        "Xingqi Zhang"
      ],
      "abstract": "Urban railway systems increasingly rely on communication based train control (CBTC) systems, where optimal deployment of access points (APs) in tunnels is critical for robust wireless coverage. Traditional methods, such as empirical model-based optimization algorithms, are hindered by excessive measurement requirements and suboptimal solutions, while machine learning (ML) approaches often struggle with complex tunnel environments. This paper proposes a deep reinforcement learning (DRL) driven framework that integrates parabolic wave equation (PWE) channel modeling, conditional generative adversarial network (cGAN) based data augmentation, and a dueling deep Q network (Dueling DQN) for AP placement optimization. The PWE method generates high-fidelity path loss distributions for a subset of AP positions, which are then expanded by the cGAN to create high resolution path loss maps for all candidate positions, significantly reducing simulation costs while maintaining physical accuracy. In the DRL framework, the state space captures AP positions and coverage, the action space defines AP adjustments, and the reward function encourages signal improvement while penalizing deployment costs. The dueling DQN enhances convergence speed and exploration exploitation balance, increasing the likelihood of reaching optimal configurations. Comparative experiments show that the proposed method outperforms a conventional Hooke Jeeves optimizer and traditional DQN, delivering AP configurations with higher average received power, better worst-case coverage, and improved computational efficiency. This work integrates high-fidelity electromagnetic simulation, generative modeling, and AI-driven optimization, offering a scalable and data-efficient solution for next-generation CBTC systems in complex tunnel environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç”±Deep Reinforcement Learning (DRL)é©±åŠ¨çš„æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–åŸºäºé€šä¿¡çš„åˆ—è½¦æ§åˆ¶(CBTC)ç³»ç»Ÿä¸­éš§é“å†…æ— çº¿è®¿é—®ç‚¹(AP)çš„éƒ¨ç½²ã€‚ä¸ºäº†è§£å†³ä¼ ç»Ÿä¼˜åŒ–ç®—æ³•æµ‹é‡éœ€æ±‚è¿‡å¤§ä¸”æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜çš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶æ•´åˆäº†Parabolic Wave Equation (PWE)ä¿¡é“å»ºæ¨¡å’ŒåŸºäºConditional Generative Adversarial Network (cGAN)çš„æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œä»è€Œåœ¨ä¿æŒç‰©ç†å‡†ç¡®æ€§çš„åŒæ—¶æ˜¾è‘—é™ä½ä»¿çœŸæˆæœ¬ã€‚åœ¨éƒ¨ç½²ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼Œç ”ç©¶é‡‡ç”¨äº†Dueling Deep Q Network (Dueling DQN)ç®—æ³•ï¼Œé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„çŠ¶æ€ç©ºé—´å’Œå¥–åŠ±å‡½æ•°æ¥å¹³è¡¡ä¿¡å·è¦†ç›–è´¨é‡ä¸éƒ¨ç½²æˆæœ¬ã€‚å®éªŒå¯¹æ¯”ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¹³å‡æ¥æ”¶åŠŸç‡ã€æœ€å·®è¦†ç›–æƒ…å†µå’Œè®¡ç®—æ•ˆç‡ä¸Šå‡ä¼˜äºä¼ ç»Ÿçš„Hooke-Jeevesä¼˜åŒ–å™¨å’Œæ ‡å‡†DQNæ¨¡å‹ã€‚è¯¥å·¥ä½œé€šè¿‡ç»“åˆé«˜ä¿çœŸç”µç£ä»¿çœŸã€ç”Ÿæˆå¼å»ºæ¨¡ä¸AIé©±åŠ¨çš„ä¼˜åŒ–ï¼Œä¸ºå¤æ‚éš§é“ç¯å¢ƒä¸‹çš„æ— çº¿ç½‘ç»œè§„åˆ’æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24819v1",
      "published_date": "2025-09-29 14:07:44 UTC",
      "updated_date": "2025-09-29 14:07:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:57:39.991591+00:00"
    },
    {
      "arxiv_id": "2509.24808v1",
      "title": "Query Circuits: Explaining How Language Models Answer User Prompts",
      "title_zh": "æŸ¥è¯¢ç”µè·¯ï¼šè§£æè¯­è¨€æ¨¡å‹å¦‚ä½•å“åº”ç”¨æˆ·æç¤º",
      "authors": [
        "Tung-Yu Wu",
        "Fazl Barez"
      ],
      "abstract": "Explaining why a language model produces a particular output requires local, input-level explanations. Existing methods uncover global capability circuits (e.g., indirect object identification), but not why the model answers a specific input query in a particular way. We introduce query circuits, which directly trace the information flow inside a model that maps a specific input to the output. Unlike surrogate-based approaches (e.g., sparse autoencoders), query circuits are identified within the model itself, resulting in more faithful and computationally accessible explanations. To make query circuits practical, we address two challenges. First, we introduce Normalized Deviation Faithfulness (NDF), a robust metric to evaluate how well a discovered circuit recovers the model's decision for a specific input, and is broadly applicable to circuit discovery beyond our setting. Second, we develop sampling-based methods to efficiently identify circuits that are sparse yet faithfully describe the model's behavior. Across benchmarks (IOI, arithmetic, MMLU, and ARC), we find that there exist extremely sparse query circuits within the model that can recover much of its performance on single queries. For example, a circuit covering only 1.3% of model connections can recover about 60% of performance on an MMLU questions. Overall, query circuits provide a step towards faithful, scalable explanations of how language models process individual inputs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Query Circuitsï¼Œæ—¨åœ¨è§£é‡Šè¯­è¨€æ¨¡å‹å¦‚ä½•é’ˆå¯¹ç‰¹å®šè¾“å…¥ç”Ÿæˆè¾“å‡ºï¼Œå¼¥è¡¥äº†ç°æœ‰å…¨å±€èƒ½åŠ›ç”µè·¯æ— æ³•è§£é‡Šç‰¹å®šè¾“å…¥å†³ç­–çš„ä¸è¶³ã€‚è¯¥æ–¹æ³•ç›´æ¥åœ¨æ¨¡å‹å†…éƒ¨è¿½è¸ªä¿¡æ¯æµï¼Œç›¸æ¯”åŸºäº Sparse Autoencoders çš„ä»£ç†æ–¹æ³•ï¼Œå…¶è§£é‡Šæ›´ä¸ºå¿ å®ä¸”å…·å¤‡æ›´å¥½çš„è®¡ç®—å¯è®¿é—®æ€§ã€‚ä¸ºäº†å¢å¼ºå®ç”¨æ€§ï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥äº† Normalized Deviation Faithfulness (NDF) è¿™ä¸€é²æ£’æŒ‡æ ‡æ¥è¯„ä¼°ç”µè·¯å¯¹æ¨¡å‹å†³ç­–çš„æ¢å¤èƒ½åŠ›ï¼Œå¹¶å¼€å‘äº†é«˜æ•ˆè¯†åˆ«ç¨€ç–ç”µè·¯çš„é‡‡æ ·æ–¹æ³•ã€‚åœ¨ IOIã€ç®—æœ¯ã€MMLU åŠ ARC ç­‰åŸºå‡†æµ‹è¯•ä¸­çš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹å†…éƒ¨å­˜åœ¨æå…¶ç¨€ç–çš„ Query Circuitsï¼Œä¾‹å¦‚ä»…è¦†ç›– 1.3% è¿æ¥çš„ç”µè·¯å°±èƒ½åœ¨ MMLU ä»»åŠ¡ä¸­æ¢å¤çº¦ 60% çš„æ€§èƒ½ã€‚è¯¥ç ”ç©¶é€šè¿‡æ­ç¤ºæ¨¡å‹å¤„ç†å•ä¸ªè¾“å…¥æ—¶çš„å†…éƒ¨æœºåˆ¶ï¼Œä¸ºå®ç°å¤§è¯­è¨€æ¨¡å‹å¿ å®ä¸”å¯æ‰©å±•çš„è§£é‡Šæ€§è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Preprint. Under review",
      "pdf_url": "https://arxiv.org/pdf/2509.24808v1",
      "published_date": "2025-09-29 13:59:02 UTC",
      "updated_date": "2025-09-29 13:59:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:56:37.288962+00:00"
    },
    {
      "arxiv_id": "2509.24805v1",
      "title": "RDD: Pareto Analysis of the Rate-Distortion-Distinguishability Trade-off",
      "title_zh": "RDDï¼šç‡-å¤±çœŸ-å¯è¾¨è¯†æ€§æƒè¡¡çš„ Pareto åˆ†æ",
      "authors": [
        "Andriy Enttsel",
        "Alex Marchioni",
        "Andrea Zanellini",
        "Mauro Mangia",
        "Gianluca Setti",
        "Riccardo Rovatti"
      ],
      "abstract": "Extensive monitoring systems generate data that is usually compressed for network transmission. This compressed data might then be processed in the cloud for tasks such as anomaly detection. However, compression can potentially impair the detector's ability to distinguish between regular and irregular patterns due to information loss. Here we extend the information-theoretic framework introduced in [1] to simultaneously address the trade-off between the three features on which the effectiveness of the system depends: the effectiveness of compression, the amount of distortion it introduces, and the distinguishability between compressed normal signals and compressed anomalous signals. We leverage a Gaussian assumption to draw curves showing how moving on a Pareto surface helps administer such a trade-off better than simply relying on optimal rate-distortion compression and hoping that compressed signals can be distinguished from each other.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç›‘æ§ç³»ç»Ÿæ•°æ®å‹ç¼©å¸¦æ¥çš„ä¿¡æ¯æŸå¤±é—®é¢˜ï¼Œæ‰©å±•äº†ä¸€ä¸ªä¿¡æ¯è®ºæ¡†æ¶ï¼Œæ—¨åœ¨åŒæ—¶å¹³è¡¡å‹ç¼©æ•ˆç‡ï¼ˆRateï¼‰ã€å¤±çœŸç¨‹åº¦ï¼ˆDistortionï¼‰ä»¥åŠæ­£å¸¸ä¸å¼‚å¸¸ä¿¡å·ä¹‹é—´çš„å¯åŒºåˆ†æ€§ï¼ˆDistinguishabilityï¼‰ã€‚è®ºæ–‡æå‡ºäº†RDDæ¡†æ¶ï¼Œé€šè¿‡å¯¹Paretoæ›²é¢è¿›è¡Œåˆ†æï¼Œæ¢è®¨äº†è¿™ä¸‰ä¸ªå…³é”®ç‰¹å¾ä¹‹é—´çš„æƒè¡¡å…³ç³»ã€‚ç ”ç©¶åˆ©ç”¨é«˜æ–¯å‡è®¾ï¼ˆGaussian assumptionï¼‰ç»˜åˆ¶äº†ç›¸å…³çš„æƒè¡¡æ›²çº¿ï¼Œè¯æ˜äº†åœ¨Paretoæ›²é¢ä¸Šè¿›è¡Œè°ƒèŠ‚æ¯”å•çº¯ä¾èµ–ä¼ ç»Ÿçš„æœ€ä¼˜é€Ÿç‡å¤±çœŸå‹ç¼©ï¼ˆoptimal rate-distortion compressionï¼‰èƒ½æ›´å¥½åœ°ç®¡ç†ç³»ç»Ÿæ€§èƒ½ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆè§£å†³äº†å‹ç¼©è¿‡ç¨‹å¯èƒ½æŸå®³æ£€æµ‹å™¨è¯†åˆ«å¼‚å¸¸æ¨¡å¼èƒ½åŠ›çš„é—®é¢˜ï¼Œä¸ºä¼˜åŒ–äº‘ç«¯æ•°æ®å¤„ç†å’Œå¼‚å¸¸æ£€æµ‹ç³»ç»Ÿæä¾›äº†é‡è¦çš„ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.IT"
      ],
      "primary_category": "eess.SP",
      "comment": "12 pages, 11 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.24805v1",
      "published_date": "2025-09-29 13:55:35 UTC",
      "updated_date": "2025-09-29 13:55:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:56:43.289937+00:00"
    },
    {
      "arxiv_id": "2509.24803v1",
      "title": "TimeOmni-1: Incentivizing Complex Reasoning with Time Series in Large Language Models",
      "title_zh": "TimeOmni-1ï¼šæ¿€å‘å¤§è¯­è¨€æ¨¡å‹ç»“åˆæ—¶é—´åºåˆ—çš„å¤æ‚æ¨ç†èƒ½åŠ›",
      "authors": [
        "Tong Guan",
        "Zijie Meng",
        "Dianqi Li",
        "Shiyu Wang",
        "Chao-Han Huck Yang",
        "Qingsong Wen",
        "Zuozhu Liu",
        "Sabato Marco Siniscalchi",
        "Ming Jin",
        "Shirui Pan"
      ],
      "abstract": "Recent advances in multimodal time series learning underscore a paradigm shift from analytics centered on basic patterns toward advanced time series understanding and reasoning. However, existing multimodal time series datasets mostly remain at the level of surface alignment and question answering, without reaching the depth of genuine reasoning. The absence of well-defined tasks that genuinely require time series reasoning, along with the scarcity of high-quality data, has limited progress in building practical time series reasoning models (TSRMs). To this end, we introduce Time Series Reasoning Suite (TSR-Suite), which formalizes four atomic tasks that span three fundamental capabilities for reasoning with time series: (1) perception, acquired through scenario understanding and causality discovery; (2) extrapolation, realized via event-aware forecasting; and (3) decision-making, developed through deliberation over perception and extrapolation. TSR-Suite is the first comprehensive time series reasoning suite that supports not only thorough evaluation but also the data pipeline and training of TSRMs. It contains more than 23K samples, of which 2.3K are carefully curated through a human-guided hierarchical annotation process. Building on this foundation, we introduce TimeOmni-1, the first unified reasoning model designed to address diverse real-world problems demanding time series reasoning. The model is trained in multiple stages, integrating a mixture of task scenarios, novel reward functions, and tailored optimizations. Experiments show that TimeOmni-1 delivers strong out-of-distribution generalization across all tasks and achieves a high rate of valid responses. It significantly improves causality discovery accuracy (64.0% vs. 35.9% with GPT-4.1) and raises the valid response rate by over 6% compared to GPT-4.1 on the event-aware forecasting task.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€æ—¶é—´åºåˆ—å­¦ä¹ åœ¨æ·±åº¦æ¨ç†èƒ½åŠ›æ–¹é¢çš„ä¸è¶³ï¼ŒæŒ‡å‡ºå½“å‰æ•°æ®é›†å¤šå±€é™äºè¡¨é¢å¯¹é½ï¼Œç¼ºä¹çœŸæ­£çš„æ¨ç†ä»»åŠ¡ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†é¦–ä¸ªç»¼åˆæ€§æ—¶é—´åºåˆ—æ¨ç†å¥—ä»¶Time Series Reasoning Suite (TSR-Suite)ï¼Œæ¶µç›–äº†æ„ŸçŸ¥(perception)ã€å¤–æ¨(extrapolation)å’Œå†³ç­–(decision-making)ä¸‰å¤§æ ¸å¿ƒèƒ½åŠ›çš„å››ä¸ªåŸå­ä»»åŠ¡ã€‚è¯¥å¥—ä»¶åŒ…å«è¶…è¿‡2.3ä¸‡ä¸ªæ ·æœ¬ï¼Œå¹¶é…å¥—äº†å®Œæ•´çš„æ•°æ®æµæ°´çº¿ç”¨ä»¥æ”¯æŒæ—¶é—´åºåˆ—æ¨ç†æ¨¡å‹(TSRMs)çš„è®­ç»ƒã€‚åŸºäºæ­¤ï¼Œç ”ç©¶æ¨å‡ºäº†ç»Ÿä¸€æ¨ç†æ¨¡å‹TimeOmni-1ï¼Œé€šè¿‡å¤šé˜¶æ®µè®­ç»ƒå’Œæ–°å‹å¥–åŠ±å‡½æ•°ä¼˜åŒ–ï¼Œå®ç°äº†å¯¹å¤æ‚ç°å®é—®é¢˜çš„å¤„ç†ã€‚å®éªŒè¯æ˜ï¼ŒTimeOmni-1åœ¨å„ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„åˆ†å¸ƒå¤–(out-of-distribution)æ³›åŒ–èƒ½åŠ›ã€‚åœ¨å› æœå‘ç°(causality discovery)ä»»åŠ¡ä¸­ï¼Œå…¶å‡†ç¡®ç‡ç”±GPT-4.1çš„35.9%æ˜¾è‘—æå‡è‡³64.0%ï¼Œæœ‰æ•ˆå“åº”ç‡ä¹Ÿå¾—åˆ°äº†å¤§å¹…æé«˜ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24803v1",
      "published_date": "2025-09-29 13:54:34 UTC",
      "updated_date": "2025-09-29 13:54:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:56:46.268777+00:00"
    },
    {
      "arxiv_id": "2509.24800v1",
      "title": "DSAT-HD: Dual-Stream Adaptive Transformer with Hybrid Decomposition for Multivariate Time Series Forecasting",
      "title_zh": "DSAT-HDï¼šåŸºäºæ··åˆåˆ†è§£çš„å¤šå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹åŒæµè‡ªé€‚åº” Transformer",
      "authors": [
        "Zixu Wang",
        "Hongbin Dong",
        "Xiaoping Zhang"
      ],
      "abstract": "Time series forecasting is crucial for various applications, such as weather, traffic, electricity, and energy predictions. Currently, common time series forecasting methods are based on Transformers. However, existing approaches primarily model limited time series or fixed scales, making it more challenging to capture diverse features cross different ranges. Additionally, traditional methods like STL for complex seasonality-trend decomposition require pre-specified seasonal periods and typically handle only single, fixed seasonality. We propose the Hybrid Decomposition Dual-Stream Adaptive Transformer (DSAT-HD), which integrates three key innovations to address the limitations of existing methods: 1) A hybrid decomposition mechanism combining EMA and Fourier decomposition with RevIN normalization, dynamically balancing seasonal and trend components through noise Top-k gating; 2) A multi-scale adaptive pathway leveraging a sparse allocator to route features to four parallel Transformer layers, followed by feature merging via a sparse combiner, enhanced by hybrid attention combining local CNNs and global interactions; 3) A dual-stream residual learning framework where CNN and MLP branches separately process seasonal and trend components, coordinated by a balanced loss function minimizing expert collaboration variance. Extensive experiments on nine datasets demonstrate that DSAT-HD outperforms existing methods overall and achieves state-of-the-art performance on some datasets. Notably, it also exhibits stronger generalization capabilities across various transfer scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DSAT-HDï¼Œå³ä¸€ç§ç»“åˆæ··åˆåˆ†è§£çš„åŒæµè‡ªé€‚åº” Transformer (Dual-Stream Adaptive Transformer with Hybrid Decomposition)ï¼Œæ—¨åœ¨è§£å†³å¤šå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹ (Multivariate Time Series Forecasting) ä¸­éš¾ä»¥æ•æ‰å¤šå°ºåº¦è·¨é‡çº§ç‰¹å¾ä»¥åŠä¼ ç»Ÿåˆ†è§£æ–¹æ³•ä¾èµ–é¢„è®¾å‘¨æœŸç­‰å±€é™æ€§ã€‚è¯¥æ¨¡å‹åˆ›æ–°æ€§åœ°è®¾è®¡äº†æ··åˆåˆ†è§£æœºåˆ¶ï¼Œé€šè¿‡ç»“åˆ EMA å’Œ Fourier åˆ†è§£ä»¥åŠ RevIN å½’ä¸€åŒ–ï¼Œå¹¶åˆ©ç”¨å™ªå£° Top-k é—¨æ§æœºåˆ¶åŠ¨æ€å¹³è¡¡å­£èŠ‚æ€§ä¸è¶‹åŠ¿æˆåˆ†ã€‚åœ¨ç‰¹å¾æå–æ–¹é¢ï¼Œæ¨¡å‹å¼•å…¥äº†å¤šå°ºåº¦è‡ªé€‚åº”è·¯å¾„ï¼Œåˆ©ç”¨ç¨€ç–åˆ†é…å™¨ (Sparse Allocator) å°†ç‰¹å¾è·¯ç”±è‡³å¹¶è¡Œ Transformer å±‚ï¼Œå¹¶é€šè¿‡ç»“åˆå±€éƒ¨ CNN å’Œå…¨å±€äº¤äº’çš„æ··åˆæ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œé«˜æ•ˆèåˆã€‚æ­¤å¤–ï¼Œæ¨¡å‹é‡‡ç”¨äº†åŒæµæ®‹å·®å­¦ä¹ æ¡†æ¶ï¼Œç”± CNN å’Œ MLP åˆ†æ”¯åˆ†åˆ«ç‹¬ç«‹å¤„ç†å­£èŠ‚æ€§ä¸è¶‹åŠ¿æˆåˆ†ï¼Œå¹¶åˆ©ç”¨å¹³è¡¡æŸå¤±å‡½æ•°ä¼˜åŒ–ä¸“å®¶åä½œã€‚åœ¨ä¹ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒDSAT-HD çš„æ•´ä½“è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿› (State-of-the-art) çš„æ€§èƒ½æ°´å¹³ã€‚è¯¥ç ”ç©¶è¿˜è¯å®äº†æ¨¡å‹åœ¨å¤šç§è¿ç§»å­¦ä¹ åœºæ™¯ä¸‹å…·æœ‰æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºå¤æ‚æ—¶é—´åºåˆ—å»ºæ¨¡æä¾›äº†æ–°çš„æ€è·¯ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.24800v1",
      "published_date": "2025-09-29 13:50:56 UTC",
      "updated_date": "2025-09-29 13:50:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:56:55.692909+00:00"
    },
    {
      "arxiv_id": "2509.24798v3",
      "title": "Causal-Adapter: Taming Text-to-Image Diffusion for Faithful Counterfactual Generation",
      "title_zh": "Causal-Adapterï¼šé¢å‘é«˜ä¿çœŸåäº‹å®ç”Ÿæˆçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹é€‚é…",
      "authors": [
        "Lei Tong",
        "Zhihua Liu",
        "Chaochao Lu",
        "Dino Oglic",
        "Tom Diethe",
        "Philip Teare",
        "Sotirios A. Tsaftaris",
        "Chen Jin"
      ],
      "abstract": "We present Causal-Adapter, a modular framework that adapts frozen text-to-image diffusion backbones for counterfactual image generation. Our method enables causal interventions on target attributes, consistently propagating their effects to causal dependents without altering the core identity of the image. In contrast to prior approaches that rely on prompt engineering without explicit causal structure, Causal-Adapter leverages structural causal modeling augmented with two attribute regularization strategies: prompt-aligned injection, which aligns causal attributes with textual embeddings for precise semantic control, and a conditioned token contrastive loss to disentangle attribute factors and reduce spurious correlations. Causal-Adapter achieves state-of-the-art performance on both synthetic and real-world datasets, with up to 91% MAE reduction on Pendulum for accurate attribute control and 87% FID reduction on ADNI for high-fidelity MRI image generation. These results show that our approach enables robust, generalizable counterfactual editing with faithful attribute modification and strong identity preservation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Causal-Adapterï¼Œè¿™æ˜¯ä¸€ä¸ªå°†å†»ç»“çš„æ–‡æœ¬ç”Ÿæˆå›¾åƒæ‰©æ•£æ¨¡å‹ (text-to-image diffusion) é€‚é…äºåäº‹å®å›¾åƒç”Ÿæˆ (counterfactual image generation) çš„æ¨¡å—åŒ–æ¡†æ¶ã€‚è¯¥æ–¹æ³•å…è®¸å¯¹ç›®æ ‡å±æ€§è¿›è¡Œå› æœå¹²é¢„ (causal interventions)ï¼Œå¹¶å°†å…¶å½±å“ä¸€è‡´åœ°ä¼ æ’­ç»™å› æœä¾èµ–é¡¹ï¼ŒåŒæ—¶ä¸æ”¹å˜å›¾åƒçš„æ ¸å¿ƒèº«ä»½ç‰¹å¾ã€‚ç›¸æ¯”äºä»…ä¾èµ–æç¤ºå·¥ç¨‹çš„æ–¹æ³•ï¼ŒCausal-Adapter åˆ©ç”¨ç»“æ„å› æœå»ºæ¨¡ (structural causal modeling) å¹¶ç»“åˆäº†ä¸¤ç§å±æ€§æ­£åˆ™åŒ–ç­–ç•¥ï¼šæç¤ºå¯¹é½æ³¨å…¥ (prompt-aligned injection) ç”¨äºç²¾ç¡®çš„è¯­ä¹‰æ§åˆ¶ï¼Œä»¥åŠæ¡ä»¶æ ‡è®°å¯¹æ¯”æŸå¤± (conditioned token contrastive loss) ä»¥è§£è€¦å±æ€§å› å­å¹¶å‡å°‘ä¼ªç›¸å…³ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆæˆå’Œç°å®æ•°æ®é›†ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿›æ€§èƒ½ï¼Œåœ¨ Pendulum æ•°æ®é›†ä¸Šå®ç°äº† 91% çš„ MAE é™ä½ï¼Œå¹¶åœ¨ ADNI æ•°æ®é›†ä¸Šå®ç°äº† 87% çš„ FID é™ä½ã€‚è¿™äº›ç»“æœè¯æ˜äº† Causal-Adapter èƒ½å¤Ÿå®ç°å…·æœ‰é«˜åº¦å±æ€§ä¿®æ”¹çœŸå®æ€§å’Œå¼ºèº«ä»½ä¿æŒèƒ½åŠ›çš„ç¨³å¥ã€é€šç”¨çš„åäº‹å®ç¼–è¾‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "9 pages, 26 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.24798v3",
      "published_date": "2025-09-29 13:49:28 UTC",
      "updated_date": "2025-10-03 08:34:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:58:28.584696+00:00"
    },
    {
      "arxiv_id": "2509.24797v1",
      "title": "Fidelity-Aware Data Composition for Robust Robot Generalization",
      "title_zh": "é¢å‘æœºå™¨äººé²æ£’æ³›åŒ–çš„ä¿çœŸåº¦æ„ŸçŸ¥æ•°æ®ç»„åˆ",
      "authors": [
        "Zizhao Tong",
        "Di Chen",
        "Sicheng Hu",
        "Hongwei Fan",
        "Liliang Chen",
        "Guanghui Ren",
        "Hao Tang",
        "Hao Dong",
        "Ling Shao"
      ],
      "abstract": "Generalist robot policies trained on large-scale, visually homogeneous datasets can be susceptible to shortcut learning, which impairs their out-of-distribution (OOD) generalization. While generative data augmentation is a common approach to introduce diversity, it presents a subtle challenge: data composition. Naively mixing real and synthetic data can corrupt the learning signal, as this process often prioritizes visual diversity at the expense of information fidelity. This paper suggests that robust generalization depends on principled, fidelity-aware data composition. We introduce Coherent Information Fidelity Tuning (CIFT), a framework that treats data composition as an optimization problem. CIFT uses a practical proxy for Information Fidelity based on the feature-space geometry of a dataset. This enables the identification of a phase transition, termed the Decoherence Point, where training stability degrades. The framework includes a generative engine, Multi-View Video Augmentation (MVAug), to synthesize a causally disentangled data spectrum for this tuning process. Applying CIFT to policy architectures such as $Ï€_0$ and Diffusion Policy improves OOD success rates by over 54\\%. These results indicate that fidelity-aware composition, beyond data synthesis alone, is an important component for developing robust, general-purpose robots.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é€šç”¨æœºå™¨äººç­–ç•¥åœ¨è§†è§‰åŒè´¨åŒ–æ•°æ®é›†ä¸­å®¹æ˜“äº§ç”Ÿæ·å¾„å­¦ä¹ (shortcut learning)å¹¶æŸå®³åˆ†å¸ƒå¤–(OOD)æ³›åŒ–èƒ½åŠ›çš„é—®é¢˜ï¼Œæå‡ºäº†ä¿çœŸåº¦æ„ŸçŸ¥çš„æ•°æ®ç»„åˆæ¡†æ¶Coherent Information Fidelity Tuning (CIFT)ã€‚CIFTå°†æ•°æ®ç»„åˆè§†ä¸ºä¼˜åŒ–é—®é¢˜ï¼Œåˆ©ç”¨åŸºäºç‰¹å¾ç©ºé—´å‡ ä½•çš„ä¿¡æ¯ä¿çœŸåº¦(Information Fidelity)ä»£ç†æŒ‡æ ‡ï¼Œè¯†åˆ«å‡ºè®­ç»ƒç¨³å®šæ€§é€€åŒ–çš„é€€ç›¸å¹²ç‚¹(Decoherence Point)ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†Multi-View Video Augmentation (MVAug)ç”Ÿæˆå¼•æ“ï¼Œç”¨äºåˆæˆå› æœè§£è€¦çš„æ•°æ®è°±ä»¥è¾…åŠ©è°ƒä¼˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨$\\pi_0$å’ŒDiffusion Policyç­‰æ¶æ„ä¸Šåº”ç”¨CIFTå¯å°†OODæˆåŠŸç‡æé«˜54%ä»¥ä¸Šã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†åœ¨æ•°æ®åˆæˆä¹‹å¤–ï¼Œè¿›è¡Œä¿çœŸåº¦æ„ŸçŸ¥çš„æ•°æ®ç»„åˆæ˜¯å¼€å‘ç¨³å¥é€šç”¨æœºå™¨äººçš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "33 pages",
      "pdf_url": "https://arxiv.org/pdf/2509.24797v1",
      "published_date": "2025-09-29 13:48:36 UTC",
      "updated_date": "2025-09-29 13:48:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:58:26.985726+00:00"
    },
    {
      "arxiv_id": "2509.25289v2",
      "title": "ClustRecNet: A Novel End-to-End Deep Learning Framework for Clustering Algorithm Recommendation",
      "title_zh": "ClustRecNetï¼šä¸€ç§ç”¨äºèšç±»ç®—æ³•æ¨èçš„æ–°å‹ç«¯åˆ°ç«¯æ·±åº¦å­¦ä¹ æ¡†æ¶",
      "authors": [
        "Mohammadreza Bakhtyari",
        "Bogdan Mazoure",
        "Renato Cordeiro de Amorim",
        "Guillaume Rabusseau",
        "Vladimir Makarenkov"
      ],
      "abstract": "We introduce ClustRecNet - a novel deep learning (DL)-based recommendation framework for determining the most suitable clustering algorithms for a given dataset, addressing the long-standing challenge of clustering algorithm selection in unsupervised learning. To enable supervised learning in this context, we construct a comprehensive data repository comprising 34,000 synthetic datasets with diverse structural properties. Each of them was processed using 10 popular clustering algorithms. The resulting clusterings were assessed via the Adjusted Rand Index (ARI) to establish ground truth labels, used for training and evaluation of our DL model. The proposed network architecture integrates convolutional, residual, and attention mechanisms to capture both local and global structural patterns from the input data. This design supports end-to-end training to learn compact representations of datasets and enables direct recommendation of the most suitable clustering algorithm, reducing reliance on handcrafted meta-features and traditional Cluster Validity Indices (CVIs). Comprehensive experiments across synthetic and real-world benchmarks demonstrate that our DL model consistently outperforms conventional CVIs (e.g. Silhouette, Calinski-Harabasz, Davies-Bouldin, and Dunn) as well as state-of-the-art AutoML clustering recommendation approaches (e.g. ML2DAC, AutoCluster, and AutoML4Clust). Notably, the proposed model achieves a 0.497 ARI improvement over the Calinski-Harabasz index on synthetic data and a 15.3% ARI gain over the best-performing AutoML approach on real-world data.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ClustRecNetï¼Œä¸€ç§æ–°å‹çš„åŸºäºæ·±åº¦å­¦ä¹ (Deep Learning)çš„æ¨èæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³éç›‘ç£å­¦ä¹ ä¸­èšç±»ç®—æ³•(Clustering Algorithm)é€‰æ‹©è¿™ä¸€é•¿æœŸå­˜åœ¨çš„æŒ‘æˆ˜ã€‚ä¸ºäº†å®ç°åœ¨è¯¥é¢†åŸŸçš„ç›‘ç£å­¦ä¹ ï¼Œç ”ç©¶äººå‘˜æ„å»ºäº†ä¸€ä¸ªåŒ…å«34,000ä¸ªåˆæˆæ•°æ®é›†çš„å¤§å‹å­˜å‚¨åº“ï¼Œå¹¶åˆ©ç”¨10ç§å¸¸ç”¨èšç±»ç®—æ³•åŠè°ƒæ•´å…°å¾·æŒ‡æ•°(Adjusted Rand Index, ARI)ç¡®ç«‹äº†åŸºå‡†çœŸå®æ ‡ç­¾ã€‚è¯¥ç½‘ç»œæ¶æ„é›†æˆäº†å·ç§¯(Convolutional)ã€æ®‹å·®(Residual)å’Œæ³¨æ„åŠ›(Attention)æœºåˆ¶ï¼Œä»¥æœ‰æ•ˆæ•è·æ•°æ®ä¸­çš„å±€éƒ¨å’Œå…¨å±€ç»“æ„æ¨¡å¼ã€‚ClustRecNetæ”¯æŒç«¯åˆ°ç«¯(End-to-End)è®­ç»ƒï¼Œèƒ½ç›´æ¥æ¨èæœ€åˆé€‚çš„èšç±»ç®—æ³•ï¼Œä»è€Œå‡å°‘äº†å¯¹ä¼ ç»Ÿæ‰‹å·¥å…ƒç‰¹å¾å’Œèšç±»æœ‰æ•ˆæ€§æŒ‡æ ‡(Cluster Validity Indices, CVIs)çš„ä¾èµ–ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºä¼ ç»Ÿçš„CVIsï¼ˆå¦‚Silhouetteã€Calinski-Harabaszç­‰ï¼‰å’Œæœ€å…ˆè¿›çš„AutoMLæ–¹æ³•ã€‚ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨åˆæˆæ•°æ®ä¸Šæ¯”Calinski-HarabaszæŒ‡æ•°å®ç°äº†0.497çš„ARIæå‡ï¼Œå¹¶åœ¨çœŸå®æ•°æ®ä¸Šæ¯”è¡¨ç°æœ€å¥½çš„AutoMLæ–¹æ¡ˆå–å¾—äº†15.3%çš„ARIå¢ç›Šã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25289v2",
      "published_date": "2025-09-29 13:48:33 UTC",
      "updated_date": "2025-10-10 16:12:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:58:29.491303+00:00"
    },
    {
      "arxiv_id": "2509.24793v2",
      "title": "Sparse Autoencoders Make Audio Foundation Models more Explainable",
      "title_zh": "ç¨€ç–è‡ªç¼–ç å™¨æå‡éŸ³é¢‘åŸºç¡€æ¨¡å‹çš„å¯è§£é‡Šæ€§",
      "authors": [
        "ThÃ©o Mariotte",
        "Martin Lebourdais",
        "Antonio AlmudÃ©var",
        "Marie Tahon",
        "Alfonso Ortega",
        "Nicolas DuguÃ©"
      ],
      "abstract": "Audio pretrained models are widely employed to solve various tasks in speech processing, sound event detection, or music information retrieval. However, the representations learned by these models are unclear, and their analysis mainly restricts to linear probing of the hidden representations. In this work, we explore the use of Sparse Autoencoders (SAEs) to analyze the hidden representations of pretrained models, focusing on a case study in singing technique classification. We first demonstrate that SAEs retain both information about the original representations and class labels, enabling their internal structure to provide insights into self-supervised learning systems. Furthermore, we show that SAEs enhance the disentanglement of vocal attributes, establishing them as an effective tool for identifying the underlying factors encoded in the representations.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨ Sparse Autoencoders (SAEs) æ¥åˆ†æéŸ³é¢‘é¢„è®­ç»ƒæ¨¡å‹çš„éšè—è¡¨ç¤ºï¼Œæ—¨åœ¨è§£å†³æ­¤ç±»æ¨¡å‹å†…éƒ¨è¡¨ç¤ºä¸é€æ˜ä¸”éš¾ä»¥é€šè¿‡ä¼ ç»Ÿçº¿æ€§æ¢æµ‹è¿›è¡Œæ·±åº¦åˆ†æçš„é—®é¢˜ã€‚ä½œè€…ä»¥ singing technique classification ä¸ºæ¡ˆä¾‹ç ”ç©¶ï¼Œè¯æ˜äº† SAEs èƒ½å¤Ÿæœ‰æ•ˆä¿ç•™åŸå§‹è¡¨ç¤ºçš„ä¿¡æ¯å’Œç±»åˆ«æ ‡ç­¾ï¼Œä½¿å…¶å†…éƒ¨ç»“æ„èƒ½ä¸º self-supervised learning ç³»ç»Ÿæä¾›æ·±å…¥æ´å¯Ÿã€‚ç ”ç©¶è¿›ä¸€æ­¥è¡¨æ˜ï¼ŒSAEs æ˜¾è‘—å¢å¼ºäº†äººå£°å±æ€§çš„ disentanglementï¼Œä½¿å…¶æˆä¸ºè¯†åˆ«æ¨¡å‹è¡¨ç¤ºä¸­æ‰€ç¼–ç åº•å±‚å› ç´ çš„å¼ºæœ‰åŠ›å·¥å…·ã€‚è¯¥æˆæœä¸ºéŸ³é¢‘é¢†åŸŸçš„åŸºç¡€æ¨¡å‹æä¾›äº†æ›´é«˜çš„å¯è§£é‡Šæ€§ï¼Œæœ‰åŠ©äºç ”ç©¶è€…æ›´å¥½åœ°ç†è§£è¯­éŸ³å¤„ç†ã€å£°éŸ³äº‹ä»¶æ£€æµ‹åŠéŸ³ä¹ä¿¡æ¯æ£€ç´¢æ¨¡å‹çš„å·¥ä½œæœºåˆ¶ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "5 pages, 5 figures, 1 table, submitted to ICASSP 2026",
      "pdf_url": "https://arxiv.org/pdf/2509.24793v2",
      "published_date": "2025-09-29 13:46:48 UTC",
      "updated_date": "2025-12-17 16:28:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:58:25.594112+00:00"
    },
    {
      "arxiv_id": "2509.24784v1",
      "title": "Quantifying Generalisation in Imitation Learning",
      "title_zh": "æ¨¡ä»¿å­¦ä¹ ä¸­æ³›åŒ–èƒ½åŠ›çš„é‡åŒ–",
      "authors": [
        "Nathan Gavenski",
        "Odinaldo Rodrigues"
      ],
      "abstract": "Imitation learning benchmarks often lack sufficient variation between training and evaluation, limiting meaningful generalisation assessment. We introduce Labyrinth, a benchmarking environment designed to test generalisation with precise control over structure, start and goal positions, and task complexity. It enables verifiably distinct training, evaluation, and test settings. Labyrinth provides a discrete, fully observable state space and known optimal actions, supporting interpretability and fine-grained evaluation. Its flexible setup allows targeted testing of generalisation factors and includes variants like partial observability, key-and-door tasks, and ice-floor hazards. By enabling controlled, reproducible experiments, Labyrinth advances the evaluation of generalisation in imitation learning and provides a valuable tool for developing more robust agents.",
      "tldr_zh": "æ¨¡ä»¿å­¦ä¹ (Imitation Learning)çš„åŸºå‡†æµ‹è¯•é€šå¸¸åœ¨è®­ç»ƒå’Œè¯„ä¼°ä¹‹é—´ç¼ºä¹è¶³å¤Ÿçš„å·®å¼‚ï¼Œä»è€Œé™åˆ¶äº†å¯¹æ³›åŒ–(generalisation)èƒ½åŠ›çš„æœ‰æ•ˆè¯„ä¼°ã€‚è¯¥ç ”ç©¶æå‡ºäº†Labyrinthï¼Œä¸€ä¸ªæ—¨åœ¨é€šè¿‡ç²¾ç¡®æ§åˆ¶ç¯å¢ƒç»“æ„ã€èµ·ç‚¹ä¸ç»ˆç‚¹ä½ç½®ä»¥åŠä»»åŠ¡å¤æ‚åº¦æ¥æµ‹è¯•æ³›åŒ–èƒ½åŠ›çš„åŸºå‡†ç¯å¢ƒã€‚å®ƒæ”¯æŒåˆ›å»ºå¯éªŒè¯ä¸”æˆªç„¶ä¸åŒçš„è®­ç»ƒã€è¯„ä¼°å’Œæµ‹è¯•è®¾ç½®ï¼Œå¹¶æä¾›ç¦»æ•£ã€å®Œå…¨å¯è§‚æµ‹çš„çŠ¶æ€ç©ºé—´åŠå·²çŸ¥æœ€ä¼˜åŠ¨ä½œï¼Œæ˜¾è‘—æå‡äº†è¯„ä¼°çš„å¯è§£é‡Šæ€§ã€‚Labyrinthçš„é…ç½®æå…·çµæ´»æ€§ï¼Œæ¶µç›–äº†éƒ¨åˆ†å¯è§‚æµ‹æ€§(partial observability)ã€é’¥åŒ™ä¸é—¨(key-and-door)ä»¥åŠå†°é¢å±é™©(ice-floor hazards)ç­‰å¤šç§ä»»åŠ¡å˜ä½“ã€‚é€šè¿‡æä¾›å¯æ§ä¸”å¯é‡å¤çš„å®éªŒå¹³å°ï¼ŒLabyrinthä¸ºé‡åŒ–æ¨¡ä»¿å­¦ä¹ ä¸­çš„æ³›åŒ–è¡¨ç°æä¾›äº†æœ‰åŠ›æ”¯æ’‘ï¼Œæœ‰åŠ©äºå¼€å‘æ›´å…·é²æ£’æ€§(robust)çš„æ™ºèƒ½ä½“ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2025 Datasets and Benchmarks Track poster",
      "pdf_url": "https://arxiv.org/pdf/2509.24784v1",
      "published_date": "2025-09-29 13:43:25 UTC",
      "updated_date": "2025-09-29 13:43:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:58:27.485910+00:00"
    },
    {
      "arxiv_id": "2509.24776v1",
      "title": "VTPerception-R1: Enhancing Multimodal Reasoning via Explicit Visual and Textual Perceptual Grounding",
      "title_zh": "VTPerception-R1ï¼šé€šè¿‡æ˜¾å¼è§†è§‰ä¸æ–‡æœ¬æ„ŸçŸ¥å¯¹é½å¢å¼ºå¤šæ¨¡æ€æ¨ç†",
      "authors": [
        "Yizhuo Ding",
        "Mingkang Chen",
        "Zhibang Feng",
        "Tong Xiao",
        "Wanying Qu",
        "Wenqi Shao",
        "Yanwei Fu"
      ],
      "abstract": "Multimodal large language models (MLLMs) often struggle to ground reasoning in perceptual evidence. We present a systematic study of perception strategies-explicit, implicit, visual, and textual-across four multimodal benchmarks and two MLLMs. Our findings show that explicit perception, especially when paired with textual cues, consistently yields the best improvements, particularly for smaller models. Based on this insight, we propose VTPerception-R1, a unified two-stage framework that decouples perception from reasoning. Stage 1 introduces perception-augmented fine-tuning, and Stage 2 applies perception-aware reinforcement learning with novel visual, textual, and consistency rewards. Experiments demonstrate that VTPerception-R1 significantly improves reasoning accuracy and robustness across diverse tasks, offering a scalable and auditable solution for perception-grounded multimodal reasoning. Our code is available at: https://github.com/yizhuoDi/VTPerceprion-R1.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) éš¾ä»¥å°†æ¨ç†ä¸æ„ŸçŸ¥è¯æ®ç›¸ç»“åˆçš„é—®é¢˜ï¼Œç³»ç»Ÿæ¢è®¨äº†æ˜¾å¼ã€éšå¼ã€è§†è§‰å’Œæ–‡æœ¬ç­‰ä¸åŒæ„ŸçŸ¥ç­–ç•¥çš„å½±å“ã€‚ç ”ç©¶å‘ç°æ˜¾å¼æ„ŸçŸ¥ (explicit perception) ç»“åˆæ–‡æœ¬çº¿ç´¢å¯¹æå‡æ¨¡å‹æ€§èƒ½æœ€ä¸ºæœ‰æ•ˆï¼Œæ®æ­¤æå‡ºäº† VTPerception-R1 è¿™ä¸€å°†æ„ŸçŸ¥ä¸æ¨ç†è§£è€¦çš„ç»Ÿä¸€ä¸¤é˜¶æ®µæ¡†æ¶ã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡æ„ŸçŸ¥å¢å¼ºå¾®è°ƒ (perception-augmented fine-tuning) å¢å¼ºæ¨¡å‹åŸºç¡€ï¼Œç¬¬äºŒé˜¶æ®µåˆ™åˆ©ç”¨åŒ…å«è§†è§‰ã€æ–‡æœ¬åŠä¸€è‡´æ€§å¥–åŠ±çš„æ„ŸçŸ¥æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹  (perception-aware reinforcement learning) è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVTPerception-R1 åœ¨å¤šç§ä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†æ¨ç†çš„å‡†ç¡®ç‡ä¸ç¨³å¥æ€§ã€‚è¯¥ç ”ç©¶ä¸ºæ„ŸçŸ¥é”šå®šçš„å¤šæ¨¡æ€æ¨ç†æä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”å¯å®¡è®¡çš„è§£å†³æ–¹æ¡ˆï¼Œæœ‰æ•ˆæå‡äº†æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸‹çš„é€»è¾‘å¯é æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24776v1",
      "published_date": "2025-09-29 13:40:34 UTC",
      "updated_date": "2025-09-29 13:40:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:58:45.459026+00:00"
    },
    {
      "arxiv_id": "2510.05116v1",
      "title": "Hallucination is Inevitable for LLMs with the Open World Assumption",
      "title_zh": "å¼€æ”¾ä¸–ç•Œå‡è®¾ä¸‹å¤§è¯­è¨€æ¨¡å‹å¹»è§‰çš„å¿…ç„¶æ€§",
      "authors": [
        "Bowen Xu"
      ],
      "abstract": "Large Language Models (LLMs) exhibit impressive linguistic competence but also produce inaccurate or fabricated outputs, often called ``hallucinations''. Engineering approaches usually regard hallucination as a defect to be minimized, while formal analyses have argued for its theoretical inevitability. Yet both perspectives remain incomplete when considering the conditions required for artificial general intelligence (AGI). This paper reframes ``hallucination'' as a manifestation of the generalization problem. Under the Closed World assumption, where training and test distributions are consistent, hallucinations may be mitigated. Under the Open World assumption, however, where the environment is unbounded, hallucinations become inevitable. This paper further develops a classification of hallucination, distinguishing cases that may be corrected from those that appear unavoidable under open-world conditions. On this basis, it suggests that ``hallucination'' should be approached not merely as an engineering defect but as a structural feature to be tolerated and made compatible with human intelligence.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)ä¸­çš„å¹»è§‰(Hallucination)é—®é¢˜ï¼Œå¹¶ä»å¼€æ”¾ä¸–ç•Œå‡è®¾(Open World Assumption)çš„è§†è§’è®ºè¯äº†å…¶åœ¨ç†è®ºä¸Šçš„å¿…ç„¶æ€§ã€‚ä½œè€…å°†å¹»è§‰é‡æ–°å®šä¹‰ä¸ºæ³›åŒ–é—®é¢˜(Generalization problem)çš„ä¸€ç§è¡¨ç°ï¼ŒæŒ‡å‡ºåœ¨å°é—­ä¸–ç•Œ(Closed World)å‡è®¾ä¸‹å¹»è§‰å¯èƒ½å¾—åˆ°ç¼“è§£ï¼Œä½†åœ¨ç¯å¢ƒæ— ç•Œçš„å¼€æ”¾ä¸–ç•Œä¸­ï¼Œå¹»è§‰åˆ™æ˜¯ä¸å¯é¿å…çš„ã€‚è®ºæ–‡è¿›ä¸€æ­¥å¼€å‘äº†ä¸€ç§å¹»è§‰åˆ†ç±»æ–¹æ³•ï¼Œæ—¨åœ¨åŒºåˆ†å¯ä»¥çº æ­£çš„é”™è¯¯ä¸åœ¨å¼€æ”¾ä¸–ç•Œæ¡ä»¶ä¸‹æ— æ³•é¿å…çš„å¿…ç„¶ç»“æœã€‚åŸºäºæ­¤ï¼Œç ”ç©¶æå‡ºå¹»è§‰ä¸åº”ä»…è¢«è§†ä¸ºä¸€ç§å·¥ç¨‹ç¼ºé™·(Engineering defect)ï¼Œè€Œåº”è¢«è§†ä¸ºä¸€ç§å¿…é¡»å®¹å¿å¹¶ä¸äººç±»æ™ºèƒ½ç›¸å…¼å®¹çš„ç»“æ„æ€§ç‰¹å¾(Structural feature)ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.05116v1",
      "published_date": "2025-09-29 13:38:44 UTC",
      "updated_date": "2025-09-29 13:38:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:58:52.389494+00:00"
    },
    {
      "arxiv_id": "2510.03291v2",
      "title": "UniPruning: Unifying Local Metric and Global Feedback for Scalable Sparse LLMs",
      "title_zh": "UniPruningï¼šèåˆå±€éƒ¨æŒ‡æ ‡ä¸å…¨å±€åé¦ˆçš„å¯æ‰©å±•ç¨€ç–å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Yizhuo Ding",
        "Wanying Qu",
        "Jiawei Geng",
        "Wenqi Shao",
        "Yanwei Fu"
      ],
      "abstract": "Large Language Models (LLMs) achieve strong performance across diverse tasks but face prohibitive computational and memory costs. Pruning offers a promising path by inducing sparsity while preserving architectural flexibility. However, existing methods struggle to balance efficiency and robustness: local metric approaches prune layer by layer but often collapse under high sparsity, whereas global feedback methods enforce consistency at the cost of expensive weight updates or restrictive semi-structured formats. We present UniPruning, a unified post-training pruning framework that combines the speed of local saliency metrics with the stability of global coordination, enabled by a mirror descent based optimization, all without updating model weights. UniPruning leverages fast layer-wise scoring and a lightweight global controller to allocate a single sparsity budget, supporting both unstructured and semi-structured N :M pruning within one framework. After a brief calibration, it can generate pruning masks for arbitrary sparsity levels in one shot, and adapts seamlessly to hardware-aware constraints. Extensive experiments on multiple pretrained LLM families and standard benchmarks show that UniPruning consistently delivers competitive or superior perplexity and zero-shot accuracy. Ablation studies further highlight the importance of mirror descent and local saliency anchoring. Overall, UniPruning provides an efficient, principled, and scalable solution for sparsifying large-scale LLMs. Our code is available at: https://github.com/RainbowQTT/UniPruning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†UniPruningï¼Œä¸€ä¸ªç»Ÿä¸€çš„åè®­ç»ƒå‰ªæ(post-training pruning)æ¡†æ¶ï¼Œæ—¨åœ¨å¹³è¡¡å¤§è¯­è¨€æ¨¡å‹(LLMs)ç¨€ç–åŒ–è¿‡ç¨‹ä¸­çš„æ•ˆç‡ä¸é²æ£’æ€§ã€‚UniPruningç»“åˆäº†å±€éƒ¨æ˜¾è‘—æ€§æŒ‡æ ‡(local saliency metrics)çš„é«˜é€Ÿæ€§ä¸å…¨å±€åä½œ(global coordination)çš„ç¨³å®šæ€§ï¼Œé€šè¿‡åŸºäºé•œåƒä¸‹é™(mirror descent)çš„ä¼˜åŒ–æœºåˆ¶åœ¨ä¸æ›´æ–°æ¨¡å‹æƒé‡çš„æƒ…å†µä¸‹å®ç°é«˜æ•ˆå‰ªæã€‚è¯¥æ¡†æ¶é‡‡ç”¨è½»é‡çº§å…¨å±€æ§åˆ¶å™¨åŠ¨æ€åˆ†é…ç¨€ç–é¢„ç®—ï¼Œèƒ½å¤Ÿåœ¨ä¸€ä¸ªç»Ÿä¸€ä½“ç³»å†…æ”¯æŒéç»“æ„åŒ–(unstructured)å’ŒåŠç»“æ„åŒ–(semi-structured)çš„N:Må‰ªæã€‚ç»è¿‡ç®€çŸ­æ ¡å‡†åï¼Œå®ƒå¯ä»¥é’ˆå¯¹ä»»æ„ç¨€ç–åº¦æ°´å¹³ä¸€æ¬¡æ€§ç”Ÿæˆæ©ç ï¼Œå¹¶çµæ´»é€‚é…ç¡¬ä»¶æ„ŸçŸ¥çº¦æŸã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒUniPruningåœ¨å¤šä¸ªé¢„è®­ç»ƒLLMç³»åˆ—åŠæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­ï¼Œå…¶å›°æƒ‘åº¦(perplexity)å’Œé›¶æ ·æœ¬å‡†ç¡®ç‡(zero-shot accuracy)å‡è¾¾åˆ°æˆ–è¶…è¿‡äº†ç°æœ‰å…ˆè¿›æ°´å¹³ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥å¼ºè°ƒäº†å±€éƒ¨æ˜¾è‘—æ€§é”šå®šä¸å…¨å±€åè°ƒåœ¨ç»´æŒæ¨¡å‹æ€§èƒ½ä¸­çš„å…³é”®ä½œç”¨ï¼Œä¸ºå¤§è§„æ¨¡LLMsçš„ç¨€ç–åŒ–æä¾›äº†é«˜æ•ˆä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03291v2",
      "published_date": "2025-09-29 13:38:28 UTC",
      "updated_date": "2025-12-09 12:42:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:58:48.794384+00:00"
    },
    {
      "arxiv_id": "2509.24773v2",
      "title": "VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning",
      "title_zh": "VSSFlowï¼šé€šè¿‡è”åˆå­¦ä¹ ç»Ÿä¸€è§†é¢‘é©±åŠ¨çš„å£°éŸ³ä¸è¯­éŸ³ç”Ÿæˆ",
      "authors": [
        "Xin Cheng",
        "Yuyue Wang",
        "Xihua Wang",
        "Yihan Wu",
        "Kaisi Guan",
        "Yijing Chen",
        "Peng Zhang",
        "Xiaojiang Liu",
        "Meng Cao",
        "Ruihua Song"
      ],
      "abstract": "Video-conditioned sound and speech generation, encompassing video-to-sound (V2S) and visual text-to-speech (VisualTTS) tasks, are conventionally addressed as separate tasks, with limited exploration to unify them within a signle framework. Recent attempts to unify V2S and VisualTTS face challenges in handling distinct condition types (e.g., heterogeneous video and transcript conditions) and require complex training stages. Unifying these two tasks remains an open problem. To bridge this gap, we present VSSFlow, which seamlessly integrates both V2S and VisualTTS tasks into a unified flow-matching framework. VSSFlow uses a novel condition aggregation mechanism to handle distinct input signals. We find that cross-attention and self-attention layer exhibit different inductive biases in the process of introducing condition. Therefore, VSSFlow leverages these inductive biases to effectively handle different representations: cross-attention for ambiguous video conditions and self-attention for more deterministic speech transcripts. Furthermore, contrary to the prevailing belief that joint training on the two tasks requires complex training strategies and may degrade performance, we find that VSSFlow benefits from the end-to-end joint learning process for sound and speech generation without extra designs on training stages. Detailed analysis attributes it to the learned general audio prior shared between tasks, which accelerates convergence, enhances conditional generation, and stabilizes the classifier-free guidance process. Extensive experiments demonstrate that VSSFlow surpasses the state-of-the-art domain-specific baselines on both V2S and VisualTTS benchmarks, underscoring the critical potential of unified generative models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†VSSFlowï¼Œæ—¨åœ¨å°†è§†é¢‘åˆ°å£°éŸ³(V2S)å’Œè§†è§‰æ–‡æœ¬è½¬è¯­éŸ³(VisualTTS)è¿™ä¸¤é¡¹ä¼ ç»Ÿä¸Šåˆ†ç¦»çš„ä»»åŠ¡æ•´åˆè¿›ç»Ÿä¸€çš„Flow-matchingæ¡†æ¶ä¸­ã€‚VSSFlowé‡‡ç”¨äº†ä¸€ç§æ–°å‹çš„æ¡ä»¶èšåˆæœºåˆ¶ï¼Œåˆ©ç”¨Cross-attentionå’ŒSelf-attentionçš„å½’çº³åç½®åˆ†åˆ«å¤„ç†æ¨¡ç³Šçš„è§†é¢‘æ¡ä»¶å’Œç¡®å®šçš„è¯­éŸ³è„šæœ¬ã€‚ç ”ç©¶äººå‘˜å‘ç°ï¼Œç«¯åˆ°ç«¯çš„è”åˆå­¦ä¹ (Joint learning)ä¸ä»…ç®€åŒ–äº†è®­ç»ƒé˜¶æ®µï¼Œè¿˜é€šè¿‡å…±äº«çš„é€šç”¨éŸ³é¢‘å…ˆéªŒåŠ é€Ÿäº†æ¨¡å‹æ”¶æ•›å¹¶ç¨³å®šäº†æ— åˆ†ç±»å™¨å¼•å¯¼(Classifier-free guidance)è¿‡ç¨‹ã€‚å¤§é‡å®éªŒè¯æ˜ï¼ŒVSSFlowåœ¨V2Så’ŒVisualTTSçš„å„é¡¹åŸºå‡†æµ‹è¯•ä¸­å‡è¶…è¶Šäº†ç›®å‰æœ€å…ˆè¿›çš„ç‰¹å®šé¢†åŸŸåŸºå‡†æ¨¡å‹ï¼Œå±•ç¤ºäº†ç»Ÿä¸€ç”Ÿæˆæ¨¡å‹åœ¨å¤„ç†å¼‚æ„å¤šæ¨¡æ€è¾“å…¥æ—¶çš„å¼ºå¤§æ½œåŠ›ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "Paper Under Review",
      "pdf_url": "https://arxiv.org/pdf/2509.24773v2",
      "published_date": "2025-09-29 13:38:24 UTC",
      "updated_date": "2025-09-30 05:16:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:58:52.084417+00:00"
    },
    {
      "arxiv_id": "2509.24765v4",
      "title": "From Ambiguity to Verdict: A Semiotic-Grounded Multi-Perspective Agent for LLM Logical Reasoning",
      "title_zh": "ä»æ­§ä¹‰åˆ°åˆ¤å®šï¼šåŸºäºç¬¦å·å­¦çš„å¤šè§†è§’å¤§è¯­è¨€æ¨¡å‹é€»è¾‘æ¨ç†æ™ºèƒ½ä½“",
      "authors": [
        "Yunyao Zhang",
        "Xinglang Zhang",
        "Junxi Sheng",
        "Wenbing Li",
        "Junqing Yu",
        "Wei Yang",
        "Zikai Song"
      ],
      "abstract": "Logical reasoning is a fundamental capability of large language models. However, existing studies often overlook the interaction between logical complexity and semantic complexity, leading to systems that struggle with abstract propositions, ambiguous contexts, and conflicting stances that are central to human reasoning. We propose LogicAgent, a semiotic-square-guided framework that jointly addresses these two axes of difficulty. The semiotic square provides a principled structure for multi-perspective semantic analysis, and LogicAgent integrates automated deduction with reflective verification to manage logical complexity across deeper reasoning chains. To support evaluation under these conditions, we introduce RepublicQA, a benchmark that couples semantic complexity with logical depth. RepublicQA reaches college-level semantic difficulty (FKGL 11.94), contains philosophically grounded abstract propositions with systematically constructed contrary and contradictory forms, and offers a semantically rich setting for assessing logical reasoning in large language models. Experiments show that LogicAgent achieves state-of-the-art performance on RepublicQA with a 6.25 percent average improvement over strong baselines, and generalizes effectively to mainstream logical reasoning benchmarks including ProntoQA, ProofWriter, FOLIO, and ProverQA, achieving an additional 7.05 percent average gain. These results demonstrate the effectiveness of semiotic-grounded multi-perspective reasoning in enhancing logical performance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨é€»è¾‘ä¸è¯­ä¹‰å¤æ‚æ€§äº¤äº’ä¸‹è¡¨ç°ä¸ä½³çš„é—®é¢˜ï¼Œæå‡ºäº†åŸºäºç¬¦å·å­¦çŸ©é˜µ (semiotic square) æŒ‡å¯¼çš„ LogicAgent æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ç¬¦å·å­¦çŸ©é˜µæ„å»ºå¤šç»´è¯­ä¹‰åˆ†æç»“æ„ï¼Œå¹¶ç»“åˆè‡ªåŠ¨æ¼”ç» (automated deduction) ä¸åæ€éªŒè¯ (reflective verification) æŠ€æœ¯ï¼Œæœ‰æ•ˆå¤„ç†æ·±åº¦æ¨ç†é“¾ä¸­çš„é€»è¾‘å¤æ‚æ€§ã€‚ä¸ºé…åˆè¯„ä¼°ï¼Œç ”ç©¶è¿˜æ¨å‡ºäº† RepublicQA åŸºå‡†æµ‹è¯•ï¼Œè¯¥åŸºå‡†å…·å¤‡å¤§å­¦æ°´å¹³çš„è¯­ä¹‰éš¾åº¦ï¼ŒåŒ…å«åŸºäºå“²å­¦åŸºç¡€æ„å»ºçš„å¯¹ç«‹ä¸çŸ›ç›¾æŠ½è±¡å‘½é¢˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLogicAgent åœ¨ RepublicQA ä¸Šæ¯”å¼ºåŸºçº¿æ¨¡å‹å¹³å‡æå‡äº† 6.25%ï¼Œå¹¶åœ¨ ProntoQAã€ProofWriter ç­‰å¤šä¸ªä¸»æµé€»è¾‘æ¨ç†åŸºå‡†ä¸Šå–å¾—äº† 7.05% çš„é¢å¤–å¹³å‡å¢ç›Šã€‚è¿™äº›ç»“æœå……åˆ†è¯æ˜äº†ç¬¦å·å­¦æ”¯æ’‘çš„å¤šè§†è§’æ¨ç†åœ¨å¢å¼ºæ¨¡å‹é€»è¾‘æ€§èƒ½ä»¥åŠåº”å¯¹æ¨¡ç³Šè¯­å¢ƒæ–¹é¢çš„æ˜¾è‘—æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24765v4",
      "published_date": "2025-09-29 13:31:22 UTC",
      "updated_date": "2025-12-01 02:24:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:58:52.567737+00:00"
    },
    {
      "arxiv_id": "2509.24761v2",
      "title": "Spatial-Functional awareness Transformer-based graph archetype contrastive learning for Decoding Visual Neural Representations from EEG",
      "title_zh": "åŸºäºç©ºé—´-åŠŸèƒ½æ„ŸçŸ¥ Transformer çš„å›¾åŸå‹å¯¹æ¯”å­¦ä¹ ï¼šä»è„‘ç”µä¿¡å·ä¸­è§£ç è§†è§‰ç¥ç»è¡¨å¾",
      "authors": [
        "Yueming Sun",
        "Long Yang"
      ],
      "abstract": "Decoding visual neural representations from Electroencephalography (EEG) signals remains a formidable challenge due to their high-dimensional, noisy, and non-Euclidean nature. In this work, we propose a Spatial-Functional Awareness Transformer-based Graph Archetype Contrastive Learning (SFTG) framework to enhance EEG-based visual decoding. Specifically, we introduce the EEG Graph Transformer (EGT), a novel graph-based neural architecture that simultaneously encodes spatial brain connectivity and temporal neural dynamics. To mitigate high intra-subject variability, we propose Graph Archetype Contrastive Learning (GAC), which learns subject-specific EEG graph archetypes to improve feature consistency and class separability. Furthermore, we conduct comprehensive subject-dependent and subject-independent evaluations on the Things-EEG dataset, demonstrating that our approach significantly outperforms prior state-of-the-art EEG decoding methods.The results underscore the transformative potential of integrating graph-based learning with contrastive objectives to enhance EEG-based brain decoding, paving the way for more generalizable and robust neural representations.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†åä¸ºSFTGçš„ç©ºé—´-åŠŸèƒ½æ„ŸçŸ¥Transformerå›¾åŸå‹å¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä»é«˜ç»´ã€å¤šå™ªä¸”éæ¬§å‡ é‡Œå¾—çš„EEGä¿¡å·ä¸­è§£ç è§†è§‰ç¥ç»è¡¨å¾çš„éš¾é¢˜ã€‚æ ¸å¿ƒæ–¹æ³•åŒ…å«ä¸€ç§æ–°å‹å›¾ç¥ç»æ¶æ„EEG Graph Transformer (EGT)ï¼Œå…¶èƒ½å¤ŸåŒæ­¥ç¼–ç å¤§è„‘çš„ç©ºé—´è¿é€šæ€§ä¸æ—¶é—´ç¥ç»åŠ¨æ€ã€‚ä¸ºç¼“è§£é«˜å—è¯•è€…å†…å˜å¼‚æ€§ï¼Œç ”ç©¶è¿›ä¸€æ­¥æå‡ºäº†Graph Archetype Contrastive Learning (GAC)ï¼Œé€šè¿‡å­¦ä¹ å—è¯•è€…ç‰¹æœ‰çš„å›¾åŸå‹æ¥æå‡ç‰¹å¾ä¸€è‡´æ€§å’Œç±»åˆ«å¯åˆ†æ€§ã€‚åœ¨Things-EEGæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å—è¯•è€…ä¾èµ–åŠç‹¬ç«‹è¯„ä¼°ä¸­å‡æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„EEGè§£ç æ–¹æ³•ã€‚è¯¥å·¥ä½œè¯æ˜äº†æ•´åˆå›¾å­¦ä¹ ä¸å¯¹æ¯”å­¦ä¹ ç›®æ ‡åœ¨æå‡è„‘ç”µè§£ç é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºå®ç°æ›´å…·é€šç”¨æ€§çš„ç¥ç»è¡¨å¾è§£ç å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24761v2",
      "published_date": "2025-09-29 13:27:55 UTC",
      "updated_date": "2025-10-09 15:02:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:59:06.399319+00:00"
    },
    {
      "arxiv_id": "2509.24759v1",
      "title": "Surjective Independence of Causal Influences for Local Bayesian Network Structures",
      "title_zh": "å±€éƒ¨è´å¶æ–¯ç½‘ç»œç»“æ„çš„æ»¡å°„å› æœå½±å“ç‹¬ç«‹æ€§",
      "authors": [
        "Kieran Drury",
        "Martine J. Barons",
        "Jim Q. Smith"
      ],
      "abstract": "The very expressiveness of Bayesian networks can introduce fresh challenges due to the large number of relationships they often model. In many domains, it is thus often essential to supplement any available data with elicited expert judgements. This in turn leads to two key challenges: the cognitive burden of these judgements is often very high, and there are a very large number of judgements required to obtain a full probability model. We can mitigate both issues by introducing assumptions such as independence of causal influences (ICI) on the local structures throughout the network, restricting the parameter space of the model. However, the assumption of ICI is often unjustified and overly strong. In this paper, we introduce the surjective independence of causal influences (SICI) model which relaxes the ICI assumption and provides a more viable, practical alternative local structure model that facilitates efficient Bayesian network parameterisation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Bayesian networksåœ¨å»ºæ¨¡å¤æ‚å…³ç³»æ—¶é¢ä¸´çš„å‚æ•°åŒ–æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¾èµ–ä¸“å®¶åˆ¤æ–­æ—¶å­˜åœ¨çš„é«˜è®¤çŸ¥è´Ÿæ‹…å’Œæµ·é‡æ•°æ®éœ€æ±‚ç­‰é—®é¢˜è¿›è¡Œäº†æ¢è®¨ã€‚è™½ç„¶ä¼ ç»Ÿæ–¹æ³•å¸¸é€šè¿‡å¼•å…¥Independence of Causal Influences (ICI)å‡è®¾æ¥é™åˆ¶å‚æ•°ç©ºé—´ï¼Œä½†è¯¥å‡è®¾åœ¨è®¸å¤šé¢†åŸŸå¾€å¾€è¿‡äºè‹›åˆ»ä¸”ç¼ºä¹åˆç†æ€§ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†Surjective Independence of Causal Influences (SICI)æ¨¡å‹ï¼Œé€šè¿‡æ”¾å®½ICIå‡è®¾æä¾›äº†ä¸€ç§æ›´å…·å¯è¡Œæ€§å’Œå®ç”¨æ€§çš„å±€éƒ¨ç»“æ„æ¨¡å‹ã€‚SICIæ¨¡å‹ä¸ºBayesian networksçš„é«˜æ•ˆå‚æ•°åŒ–æä¾›äº†æ–°é€”å¾„ï¼Œèƒ½å¤Ÿåœ¨ä¿è¯æ¨¡å‹è¡¨è¾¾åŠ›çš„åŒæ—¶æ˜¾è‘—é™ä½è·å–å®Œæ•´æ¦‚ç‡æ¨¡å‹æ‰€éœ€çš„åˆ¤æ–­æˆæœ¬ã€‚",
      "categories": [
        "stat.ME",
        "cs.AI"
      ],
      "primary_category": "stat.ME",
      "comment": "23 pages, 8 figures, 1 table, extended version of WUPES 2025 conference paper",
      "pdf_url": "https://arxiv.org/pdf/2509.24759v1",
      "published_date": "2025-09-29 13:23:11 UTC",
      "updated_date": "2025-09-29 13:23:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:59:06.699600+00:00"
    },
    {
      "arxiv_id": "2509.24748v2",
      "title": "Robust Policy Expansion for Offline-to-Online RL under Diverse Data Corruption",
      "title_zh": "å¤šæ ·åŒ–æ•°æ®æ±¡æŸ“ä¸‹ç¦»çº¿åˆ°åœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„é²æ£’ç­–ç•¥æ‰©å±•",
      "authors": [
        "Longxiang He",
        "Deheng Ye",
        "Junbo Tan",
        "Xueqian Wang",
        "Li Shen"
      ],
      "abstract": "Pretraining a policy on offline data followed by fine-tuning through online interactions, known as Offline-to-Online Reinforcement Learning (O2O RL), has emerged as a promising paradigm for real-world RL deployment. However, both offline datasets and online interactions in practical environments are often noisy or even maliciously corrupted, severely degrading the performance of O2O RL. Existing works primarily focus on mitigating the conservatism of offline policies via online exploration, while the robustness of O2O RL under data corruption, including states, actions, rewards, and dynamics, is still unexplored. In this work, we observe that data corruption induces heavy-tailed behavior in the policy, thereby substantially degrading the efficiency of online exploration. To address this issue, we incorporate Inverse Probability Weighted (IPW) into the online exploration policy to alleviate heavy-tailedness, and propose a novel, simple yet effective method termed $\\textbf{RPEX}$: $\\textbf{R}$obust $\\textbf{P}$olicy $\\textbf{EX}$pansion. Extensive experimental results on D4RL datasets demonstrate that RPEX achieves SOTA O2O performance across a wide range of data corruption scenarios. Code is available at $\\href{https://github.com/felix-thu/RPEX}{https://github.com/felix-thu/RPEX}$.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç¦»çº¿åˆ°åœ¨çº¿å¼ºåŒ–å­¦ä¹  (Offline-to-Online Reinforcement Learning, O2O RL) åœ¨å®é™…éƒ¨ç½²ä¸­é¢ä¸´çš„æ•°æ®æ±¡æŸ“é—®é¢˜ï¼ŒåŒ…æ‹¬çŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±å’ŒåŠ¨åŠ›å­¦æ–¹é¢çš„å™ªå£°æˆ–æ¶æ„ç¯¡æ”¹ã€‚è™½ç„¶ O2O RL å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä½†ç°æœ‰ç ”ç©¶å¤šä¾§é‡äºç¼“è§£ç­–ç•¥ä¿å®ˆæ€§ï¼Œè€Œå¿½è§†äº†åœ¨å¤æ‚æ±¡æŸ“ç¯å¢ƒä¸‹çš„é²æ£’æ€§ã€‚ä½œè€…è§‚å¯Ÿåˆ°ï¼Œæ•°æ®æ±¡æŸ“ä¼šè¯±å‘ç­–ç•¥çš„é‡å°¾è¡Œä¸º (heavy-tailed behavior)ï¼Œè¿›è€Œä¸¥é‡å‰Šå¼±åœ¨çº¿æ¢ç´¢çš„æ•ˆç‡ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º RPEX (Robust Policy Expansion) çš„é²æ£’ç­–ç•¥æ‰©å±•æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥é€†æ¦‚ç‡åŠ æƒ (Inverse Probability Weighted, IPW) æŠ€æœ¯æ¥ç¼“è§£é‡å°¾æ•ˆåº”ã€‚åœ¨ D4RL æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒç»“æœè¯æ˜ï¼ŒRPEX åœ¨å¤šç§æ•°æ®æ±¡æŸ“åœºæ™¯ä¸‹å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„ (SOTA) æ€§èƒ½æ°´å¹³ã€‚è¯¥å·¥ä½œä¸ºå¼ºåŒ–å­¦ä¹ åœ¨ç°å®ä¸–ç•Œä¸ç¨³å®šç¯å¢ƒä¸­çš„å¯é åº”ç”¨æä¾›äº†é‡è¦çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "39th Conference on Neural Information Processing Systems",
      "pdf_url": "https://arxiv.org/pdf/2509.24748v2",
      "published_date": "2025-09-29 13:15:42 UTC",
      "updated_date": "2025-10-16 10:59:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:59:13.491861+00:00"
    },
    {
      "arxiv_id": "2509.24734v1",
      "title": "A TRIANGLE Enables Multimodal Alignment Beyond Cosine Similarity",
      "title_zh": "TRIANGLEï¼šå®ç°è¶…è¶Šä½™å¼¦ç›¸ä¼¼åº¦çš„å¤šæ¨¡æ€å¯¹é½",
      "authors": [
        "Giordano Cicchetti",
        "Eleonora Grassucci",
        "Danilo Comminiello"
      ],
      "abstract": "Multimodal learning plays a pivotal role in advancing artificial intelligence systems by incorporating information from multiple modalities to build a more comprehensive representation. Despite its importance, current state-of-the-art models still suffer from severe limitations that prevent the successful development of a fully multimodal model. Such methods may not provide indicators that all the involved modalities are effectively aligned. As a result, some modalities may not be aligned, undermining the effectiveness of the model in downstream tasks where multiple modalities should provide additional information that the model fails to exploit. In this paper, we present TRIANGLE: TRI-modAl Neural Geometric LEarning, the novel proposed similarity measure that is directly computed in the higher-dimensional space spanned by the modality embeddings. TRIANGLE improves the joint alignment of three modalities via a triangle-area similarity, avoiding additional fusion layers or pairwise similarities. When incorporated in contrastive losses replacing cosine similarity, TRIANGLE significantly boosts the performance of multimodal modeling, while yielding interpretable alignment rationales. Extensive evaluation in three-modal tasks such as video-text and audio-text retrieval or audio-video classification, demonstrates that TRIANGLE achieves state-of-the-art results across different datasets improving the performance of cosine-based methods up to 9 points of Recall@1.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TRIANGLE (TRI-modAl Neural Geometric LEarning)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨è§£å†³å¤šæ¨¡æ€å­¦ä¹ ä¸­æ¨¡æ€å¯¹é½ä¸è¶³é—®é¢˜çš„æ–°å‹å‡ ä½•ç›¸ä¼¼åº¦åº¦é‡æ–¹æ³•ã€‚ä¸ä¼ ç»Ÿçš„ Cosine Similarity ä¸åŒï¼ŒTRIANGLE ç›´æ¥åœ¨é«˜ç»´åµŒå…¥ç©ºé—´ä¸­é€šè¿‡ä¸‰è§’å½¢é¢ç§¯ç›¸ä¼¼åº¦(triangle-area similarity)æ¥æå‡ä¸‰ç§æ¨¡æ€çš„è”åˆå¯¹é½ï¼Œæœ‰æ•ˆé¿å…äº†å¼•å…¥é¢å¤–çš„èåˆå±‚æˆ–å¤æ‚çš„æˆå¯¹ç›¸ä¼¼åº¦è®¡ç®—ã€‚é€šè¿‡å°†è¯¥åº¦é‡æ–¹å¼é›†æˆåˆ°å¯¹æ¯”æŸå¤±(contrastive losses)ä¸­ï¼ŒTRIANGLE ä¸ä»…æ˜¾è‘—å¢å¼ºäº†å¤šæ¨¡æ€å»ºæ¨¡çš„æ€§èƒ½ï¼Œè¿˜ä¸ºæ¨¡æ€é—´çš„å¯¹é½æä¾›äº†å…·æœ‰å¯è§£é‡Šæ€§çš„ä¾æ®ã€‚åœ¨è§†é¢‘-æ–‡æœ¬æ£€ç´¢ã€éŸ³é¢‘-æ–‡æœ¬æ£€ç´¢åŠéŸ³é¢‘-è§†é¢‘åˆ†ç±»ç­‰ä¸‰æ¨¡æ€ä»»åŠ¡ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå‡è¾¾åˆ°äº† state-of-the-art æ°´å¹³ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç›¸æ¯”äºåŸºäº Cosine çš„ä¼ ç»Ÿæ–¹æ³•ï¼ŒTRIANGLE åœ¨ Recall@1 æŒ‡æ ‡ä¸Šæœ€é«˜å®ç°äº† 9 ä¸ªç™¾åˆ†ç‚¹çš„æ˜¾è‘—æå‡ï¼Œè¯æ˜äº†å…¶åœ¨å¤„ç†å¤æ‚å¤šæ¨¡æ€å¯¹é½ä»»åŠ¡ä¸­çš„å“è¶Šæ•ˆèƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.24734v1",
      "published_date": "2025-09-29 12:58:46 UTC",
      "updated_date": "2025-09-29 12:58:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:59:17.783897+00:00"
    },
    {
      "arxiv_id": "2510.01270v1",
      "title": "Think Twice, Generate Once: Safeguarding by Progressive Self-Reflection",
      "title_zh": "ä¸‰æ€è€Œåç”Ÿæˆï¼šåŸºäºæ¸è¿›å¼è‡ªæˆ‘åæ€çš„å®‰å…¨é˜²æŠ¤",
      "authors": [
        "Hoang Phan",
        "Victor Li",
        "Qi Lei"
      ],
      "abstract": "Large language models (LLMs) have revolutionized natural language processing with their ability to generate coherent and contextually relevant text. However, their deployment raises significant concerns about the potential for generating harmful or inappropriate content. In this paper, we introduce Progressive Self-Reflection (PSR), a novel inference-time technique that empowers LLMs to self-monitor and correct their outputs dynamically. Experimental results demonstrate that applying our proposed method to Llama-3.1-8B-Instruct reduces the attack success rate from 77.5\\% to 5.9\\%, to Llama-3.1-8B base from 89.7\\% to 5.6\\%, and to Qwen2.5-7B-Instruct from 44.4\\% to 3.8\\%, without additional training, while maintaining their original performance on benign tasks. Our approach acts as a test-time scaling method, where additional self-reflection rounds enhance safety at the cost of inference overhead. To balance safety with computational efficiency, we introduce a lightweight self-reflection predictor that estimates the optimal number of reflection rounds based on input complexity. This adaptive mechanism prevents unnecessary self-assessment on benign inputs while ensuring thorough evaluation when encountering potentially harmful content. Our findings suggest that Progressive Self-Reflection serves as a scalable test-time approach, enhancing LLM safety by dynamically allocating computational resources in proportion to the input's risk profile.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Progressive Self-Reflection (PSR)ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ¨ç†é˜¶æ®µ(inference-time)æŠ€æœ¯ï¼Œæ—¨åœ¨é€šè¿‡èµ‹èƒ½å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)è¿›è¡ŒåŠ¨æ€è‡ªæˆ‘ç›‘æµ‹å’Œä¿®æ­£ï¼Œä»¥è§£å†³å…¶ç”Ÿæˆæœ‰å®³æˆ–ä¸å½“å†…å®¹çš„é£é™©ã€‚è¯¥æ–¹æ³•ä½œä¸ºä¸€ç§æµ‹è¯•æ—¶æ‰©å±•(test-time scaling)æ‰‹æ®µï¼Œå…è®¸æ¨¡å‹é€šè¿‡å¢åŠ è‡ªæˆ‘åæ€è½®æ¬¡æ¥å¼ºåŒ–å®‰å…¨æ€§ï¼Œä¸”æ— éœ€ä»»ä½•é¢å¤–è®­ç»ƒå³å¯ä¿æŒåœ¨è‰¯æ€§ä»»åŠ¡ä¸Šçš„åŸå§‹æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåº”ç”¨ PSR åï¼ŒLlama-3.1-8B-Instruct çš„æ”»å‡»æˆåŠŸç‡(attack success rate)ä» 77.5% æ˜¾è‘—é™ä½è‡³ 5.9%ï¼ŒQwen2.5-7B-Instruct ä¹Ÿä» 44.4% é™è‡³ 3.8%ã€‚ä¸ºäº†å¹³è¡¡å®‰å…¨æ”¶ç›Šä¸è®¡ç®—å¼€é”€ï¼Œç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªè½»é‡çº§çš„è‡ªæˆ‘åæ€é¢„æµ‹å™¨(self-reflection predictor)ï¼Œèƒ½å¤Ÿæ ¹æ®è¾“å…¥å¤æ‚åº¦è‡ªé€‚åº”åœ°ç¡®å®šæœ€ä½³åæ€è½®æ¬¡ã€‚è¿™ä¸€æœºåˆ¶ç¡®ä¿äº†åœ¨é‡åˆ°æ½œåœ¨æœ‰å®³å†…å®¹æ—¶è¿›è¡Œå½»åº•è¯„ä¼°ï¼ŒåŒæ—¶é¿å…äº†å¯¹è‰¯æ€§è¾“å…¥çš„è¿‡åº¦è®¡ç®—ã€‚ç»¼ä¸Šæ‰€è¿°ï¼ŒPSR è¯æ˜äº†é€šè¿‡éšè¾“å…¥é£é™©åŠ¨æ€åˆ†é…è®¡ç®—èµ„æºï¼Œå¯ä»¥æœ‰æ•ˆå®ç° LLMs å®‰å…¨æ€§çš„å¯æ‰©å±•æå‡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to EMNLP 2025 Findings",
      "pdf_url": "https://arxiv.org/pdf/2510.01270v1",
      "published_date": "2025-09-29 12:54:28 UTC",
      "updated_date": "2025-09-29 12:54:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:59:25.111259+00:00"
    },
    {
      "arxiv_id": "2509.24725v2",
      "title": "Q-Net: Queue Length Estimation via Kalman-based Neural Networks",
      "title_zh": "Q-Netï¼šåŸºäºå¡å°”æ›¼ç¥ç»ç½‘ç»œçš„æ’é˜Ÿé•¿åº¦ä¼°è®¡",
      "authors": [
        "Ting Gao",
        "Elvin Isufi",
        "Winnie Daamen",
        "Erik-Sander Smits",
        "Serge Hoogendoorn"
      ],
      "abstract": "Estimating queue lengths at signalized intersections is a long-standing challenge in traffic management. Partial observability of vehicle flows complicates this task despite the availability of two privacy preserving data sources: (i) aggregated vehicle counts from loop detectors near stop lines, and (ii) aggregated floating car data (aFCD) that provide segment-wise average speed measurements. However, how to integrate these sources with differing spatial and temporal resolutions for queue length estimation is rather unclear. Addressing this question, we present Q Net: a robust queue estimation framework built upon a state-space formulation. This formulation addresses key challenges in queue modeling, such as violations of traffic conservation assumptions. To overcome the limitations of standard filtering models in integrating diverse data sources, Q-Net employs an AI-augmented Kalman filter for estimation. Q-Net follows the Kalman predict-update framework and maintains physical interpretability, with internal variables linked to real-world traffic dynamics. Q-Net can be implemented in real-time, making it suitable for integration into queue-based traffic control systems. To achieve spatial transferability across road sections, we group aFCD measurements into fixed-size groups. This strategy ensures the dimension of Q-Net's learnable parameters is independent of section length. Evaluations on urban main roads in Rotterdam, the Netherlands, show that Q-Net outperforms baseline methods, accurately tracking queue formation and dissipation while correcting aFCD-induced delays. By combining data efficiency, interpretability, and strong transferability, Q Net makes accurate queue length estimation possible without costly sensing infrastructure like cameras or radar.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Q-Netï¼Œä¸€ç§åŸºäº state-space å…¬å¼çš„é²æ£’æ’é˜Ÿé•¿åº¦ä¼°è®¡æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¿¡å·äº¤å‰å£æµé‡å±€éƒ¨å¯è§‚æµ‹æ€§çš„é•¿æ•ˆæŒ‘æˆ˜ã€‚Q-Net é€šè¿‡ AI å¢å¼ºçš„ Kalman filter æ•´åˆäº†æ¥è‡ª loop detectors çš„è½¦è¾†è®¡æ•°ä¸ aggregated floating car data (aFCD) ä¸¤ç§ä¸åŒæ—¶ç©ºåˆ†è¾¨ç‡çš„æ•°æ®æºã€‚è¯¥æ¡†æ¶åœ¨ä¿æŒç‰©ç†å¯è§£é‡Šæ€§çš„åŒæ—¶ï¼Œåˆ©ç”¨é¢„æµ‹-æ›´æ–°æœºåˆ¶ç¡®ä¿å†…éƒ¨å˜é‡ä¸å®é™…äº¤é€šåŠ¨æ€å…³è”ï¼Œå¹¶æ”¯æŒå®æ—¶åº”ç”¨ã€‚é€šè¿‡å°† aFCD è§‚æµ‹å€¼è¿›è¡Œå›ºå®šå¤§å°çš„åˆ†ç»„ï¼ŒQ-Net å®ç°äº†å‚æ•°ç»´åº¦ä¸è·¯æ®µé•¿åº¦çš„è„±é’©ï¼Œæ˜¾è‘—å¢å¼ºäº†è·¯æ®µé—´çš„ç©ºé—´å¯è¿ç§»æ€§ã€‚åœ¨è·å…°é¹¿ç‰¹ä¸¹åŸå¸‚é“è·¯çš„æµ‹è¯•ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è¿½è¸ªæ’é˜Ÿå½¢æˆä¸æ¶ˆæ•£è¿‡ç¨‹ä¸­çš„è¡¨ç°ä¼˜äºåŸºå‡†æ¨¡å‹ï¼Œä¸”èƒ½æœ‰æ•ˆä¿®æ­£ aFCD å¯¼è‡´çš„è§‚æµ‹å»¶è¿Ÿã€‚Q-Net ä¸ºæ— éœ€æ‘„åƒå¤´æˆ– radar ç­‰æ˜‚è´µä¼ æ„Ÿè®¾æ–½çš„ç²¾ç¡®äº¤é€šç›‘æ§æä¾›äº†é«˜æ•ˆã€å¯è§£é‡Šä¸”æ˜“äºè¿ç§»çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24725v2",
      "published_date": "2025-09-29 12:51:08 UTC",
      "updated_date": "2025-11-27 14:39:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:59:31.067456+00:00"
    },
    {
      "arxiv_id": "2509.24716v1",
      "title": "Discrete Variational Autoencoding via Policy Search",
      "title_zh": "åŸºäºç­–ç•¥æœç´¢çš„ç¦»æ•£å˜åˆ†è‡ªç¼–ç ",
      "authors": [
        "Michael Drolet",
        "Firas Al-Hafez",
        "Aditya Bhatt",
        "Jan Peters",
        "Oleg Arenz"
      ],
      "abstract": "Discrete latent bottlenecks in variational autoencoders (VAEs) offer high bit efficiency and can be modeled with autoregressive discrete distributions, enabling parameter-efficient multimodal search with transformers. However, discrete random variables do not allow for exact differentiable parameterization; therefore, discrete VAEs typically rely on approximations, such as Gumbel-Softmax reparameterization or straight-through gradient estimates, or employ high-variance gradient-free methods such as REINFORCE that have had limited success on high-dimensional tasks such as image reconstruction. Inspired by popular techniques in policy search, we propose a training framework for discrete VAEs that leverages the natural gradient of a non-parametric encoder to update the parametric encoder without requiring reparameterization. Our method, combined with automatic step size adaptation and a transformer-based encoder, scales to challenging datasets such as ImageNet and outperforms both approximate reparameterization methods and quantization-based discrete autoencoders in reconstructing high-dimensional data from compact latent spaces, achieving a 20% improvement on FID Score for ImageNet 256.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å˜åˆ†è‡ªç¼–ç å™¨(VAEs)ä¸­ç¦»æ•£æ½œåœ¨ç“¶é¢ˆå› ä¸å¯å¾®è€Œéš¾ä»¥è®­ç»ƒçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å—ç­–ç•¥æœç´¢(Policy Search)å¯å‘çš„æ–°å‹ç¦»æ•£VAEè®­ç»ƒæ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨éå‚æ•°åŒ–ç¼–ç å™¨çš„è‡ªç„¶æ¢¯åº¦(Natural Gradient)æ¥æ›´æ–°å‚æ•°åŒ–ç¼–ç å™¨ï¼Œæœ‰æ•ˆè§„é¿äº†Gumbel-Softmaxé‡å‚æ•°åŒ–æˆ–ç›´é€šæ¢¯åº¦ä¼°è®¡ç­‰ä¼ ç»Ÿè¿‘ä¼¼æ–¹æ³•å¸¦æ¥çš„å±€é™ã€‚é€šè¿‡ç»“åˆè‡ªåŠ¨æ­¥é•¿è°ƒæ•´æŠ€æœ¯ä¸Transformeræ¶æ„çš„ç¼–ç å™¨ï¼Œè¯¥æ–¹æ³•æˆåŠŸæ‰©å±•è‡³ImageNetç­‰å¤æ‚çš„é«˜ç»´æ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é‡å»ºä»»åŠ¡ä¸­ä¼˜äºç°æœ‰çš„é‡å‚æ•°åŒ–å’ŒåŸºäºé‡åŒ–(Quantization-based)çš„ç¦»æ•£è‡ªç¼–ç å™¨ï¼Œåœ¨ImageNet 256ä¸Šçš„FIDåˆ†æ•°æ˜¾è‘—æå‡äº†20%ã€‚è¿™ä¸€ç ”ç©¶ä¸ºå®ç°é«˜æ¯”ç‰¹æ•ˆç‡ã€å¯æ‰©å±•çš„ç¦»æ•£æ½œå˜é‡å»ºæ¨¡ä»¥åŠå‚æ•°é«˜æ•ˆçš„ç”Ÿæˆæ¨¡å‹æä¾›äº†æ›´ç¨³å¥çš„ä¼˜åŒ–æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24716v1",
      "published_date": "2025-09-29 12:44:05 UTC",
      "updated_date": "2025-09-29 12:44:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:59:31.353356+00:00"
    },
    {
      "arxiv_id": "2509.24713v1",
      "title": "Circuit-Aware Reward Training: A Mechanistic Framework for Longtail Robustness in RLHF",
      "title_zh": "ç”µè·¯æ„ŸçŸ¥å¥–åŠ±è®­ç»ƒï¼šä¸€ç§æå‡ RLHF é•¿å°¾é²æ£’æ€§çš„æœºç†æ€§æ¡†æ¶",
      "authors": [
        "Jing Liu"
      ],
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) reward models exhibit systematic failures on longtail distributions, leading to reward hacking and misalignment. We propose a mechanistic interpretability framework that identifies specialized neural circuits responsible for rare-event processing in reward models. Drawing from recent advances showing distributed specialization for rare tokens in language models\\citep{liu2025no, liu2025emergent}, we hypothesize that reward models also develop functionally distinct circuits for longtail scenarios. Our theoretical framework establishes formal connections between circuit specialization, reward generalization bounds, and longtail performance. We introduce \\textbf{Circuit-Aware Reward Training (CART)}, which uses circuit analysis to guide data augmentation, regularization, and ensemble strategies. This approach provides both theoretical insights into reward model failures and practical interventions for improving longtail robustness.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹  (RLHF) å¥–åŠ±æ¨¡å‹åœ¨é•¿å°¾åˆ†å¸ƒä¸Šå®¹æ˜“å‡ºç°å¥–åŠ±é»‘å®¢æ”»å‡» (reward hacking) å’Œå¯¹é½åç¦»çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªæœºæ¢°å¯è§£é‡Šæ€§æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ—¨åœ¨è¯†åˆ«å¥–åŠ±æ¨¡å‹ä¸­è´Ÿè´£å¤„ç†ç¨€æœ‰äº‹ä»¶çš„ä¸“é—¨ç¥ç»ç¯è·¯ (neural circuits)ï¼Œå¹¶æ¢è®¨äº†ç¯è·¯ä¸“ä¸šåŒ–ã€å¥–åŠ±æ³›åŒ–ç•Œé™ä¸é•¿å°¾æ€§èƒ½ä¹‹é—´çš„æ­£å¼è”ç³»ã€‚å—è¯­è¨€æ¨¡å‹ä¸­ç¨€æœ‰æ ‡è®°å¤„ç†æœºåˆ¶çš„å¯å‘ï¼Œä½œè€…å‡è®¾å¥–åŠ±æ¨¡å‹ä¹Ÿå­˜åœ¨åŠŸèƒ½ä¸Šäº’å¼‚çš„ç¯è·¯æ¥åº”å¯¹é•¿å°¾åœºæ™¯ã€‚åŸºäºæ­¤å‘ç°ï¼Œç ”ç©¶è€…å¼•å…¥äº†æ„ŸçŸ¥ç¯è·¯çš„å¥–åŠ±è®­ç»ƒ (Circuit-Aware Reward Training, CART)ï¼Œåˆ©ç”¨ç¯è·¯åˆ†ææ¥æŒ‡å¯¼æ•°æ®å¢å¼ºã€æ­£åˆ™åŒ–å’Œé›†æˆç­–ç•¥ã€‚CART æ–¹æ³•ä¸ä»…ä¸ºç†è§£å¥–åŠ±æ¨¡å‹çš„å¤±æ•ˆæä¾›äº†ç†è®ºæ”¯æ’‘ï¼Œä¹Ÿä¸ºæå‡æ¨¡å‹åœ¨é•¿å°¾åˆ†å¸ƒä¸‹çš„ç¨³å¥æ€§æä¾›äº†æœ‰æ•ˆçš„å®è·µè·¯å¾„ã€‚è¯¥ç ”ç©¶æœ€ç»ˆä¸ºæ„å»ºæ›´å…·é²æ£’æ€§å’Œå¯¹é½æ€§çš„ RLHF å¥–åŠ±æ¨¡å‹å¥ å®šäº†ç†è®ºä¸å·¥ç¨‹åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24713v1",
      "published_date": "2025-09-29 12:42:14 UTC",
      "updated_date": "2025-09-29 12:42:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:59:32.458901+00:00"
    },
    {
      "arxiv_id": "2509.24711v2",
      "title": "On the Self-awareness of Large Reasoning Models' Capability Boundaries",
      "title_zh": "è®ºå¤§æ¨ç†æ¨¡å‹å¯¹èƒ½åŠ›è¾¹ç•Œçš„è‡ªæˆ‘è§‰çŸ¥",
      "authors": [
        "Qingjie Zhang",
        "Yujia Fu",
        "Yang Wang",
        "Liu Yan",
        "Tao Wei",
        "Ke Xu",
        "Minlie Huang",
        "Han Qiu"
      ],
      "abstract": "Large Reasoning Models (LRMs) have shown impressive performance on complex reasoning tasks such as mathematics, yet they also display misbehaviors that expose their limitations. In particular, when faced with hard questions, LRMs often engage in unproductive reasoning until context limit, producing wrong answers while wasting substantial computation. This phenomenon reflects a fundamental issue: current answering paradigms overlook the relationship between questions and LRMs' capability boundaries. In this paper, we investigate whether LRMs possess self-awareness of capability boundaries. We begin by an observation that LRMs may know what they cannot solve through expressed reasoning confidence. For black-box models, we find that reasoning expressions reveal boundary signals, with accelerated growing confidence trajectory for solvable problems but convergent uncertainty trajectory for unsolvable ones. For white-box models, we show that hidden states of the last input token encode boundary information, with solvable and unsolvable problems linearly separable even before reasoning begins. Building on these findings, we propose two simple yet effective optimization strategies: reasoning expression monitoring and hidden states monitoring. Experiments demonstrate that these boundary-aware strategies enable LRMs to avoid unproductive reasoning without sacrificing accuracy, significantly improving reliability and efficiency by cutting token usage up to 62.7 - 93.6%.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹æ¨ç†æ¨¡å‹(Large Reasoning Models, LRMs)åœ¨å¤„ç†è¶…å‡ºå…¶èƒ½åŠ›è¾¹ç•Œçš„å¤æ‚ä»»åŠ¡æ—¶ï¼Œå› æ— æ•ˆæ¨ç†å¯¼è‡´è®¡ç®—èµ„æºæµªè´¹çš„é—®é¢˜ã€‚è®ºæ–‡è°ƒæŸ¥äº† LRMs æ˜¯å¦å…·å¤‡å¯¹èƒ½åŠ›è¾¹ç•Œçš„è‡ªæˆ‘æ„è¯†(self-awareness)ï¼Œå¹¶ä»é»‘ç›’å’Œç™½ç›’æ¨¡å‹ä¸¤ä¸ªç»´åº¦è¿›è¡Œäº†æ·±å…¥è§‚å¯Ÿã€‚ç ”ç©¶å‘ç°ï¼Œé»‘ç›’æ¨¡å‹çš„æ¨ç†è¡¨è¾¾æ­ç¤ºäº†è¾¹ç•Œä¿¡å·ï¼Œè¡¨ç°ä¸ºå¯è§£é—®é¢˜çš„ç½®ä¿¡åº¦è½¨è¿¹åŠ é€Ÿå¢é•¿ï¼Œè€Œä¸å¯è§£é—®é¢˜åˆ™å‘ˆç°ä¸ç¡®å®šæ€§æ”¶æ•›ï¼›å¯¹äºç™½ç›’æ¨¡å‹ï¼Œè¾“å…¥æœ«å°¾æ ‡è®°çš„éšè—çŠ¶æ€(hidden states)åœ¨æ¨ç†å¼€å§‹å‰å°±å·²ç¼–ç äº†è¾¹ç•Œä¿¡æ¯ã€‚åŸºäºæ­¤ï¼Œä½œè€…æå‡ºäº†æ¨ç†è¡¨è¾¾ç›‘æ§å’Œéšè—çŠ¶æ€ç›‘æ§ä¸¤ç§ä¼˜åŒ–ç­–ç•¥ã€‚å®éªŒè¯æ˜ï¼Œè¿™äº›è¾¹ç•Œæ„ŸçŸ¥ç­–ç•¥ä½¿ LRMs èƒ½å¤Ÿåœ¨ä¸æŸå¤±å‡†ç¡®æ€§çš„å‰æä¸‹é¿å…æ— æ•ˆæ¨ç†ï¼Œå°† token ä½¿ç”¨é‡æ˜¾è‘—é™ä½äº† 62.7% è‡³ 93.6%ï¼Œå¤§å¹…æå‡äº†æ¨¡å‹çš„å¯é æ€§ä¸æ•ˆç‡ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24711v2",
      "published_date": "2025-09-29 12:40:47 UTC",
      "updated_date": "2025-10-05 08:47:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:00:04.995291+00:00"
    },
    {
      "arxiv_id": "2509.24701v1",
      "title": "FedPOB: Sample-Efficient Federated Prompt Optimization via Bandits",
      "title_zh": "FedPOBï¼šåŸºäº Bandit ç®—æ³•çš„æ ·æœ¬é«˜æ•ˆè”é‚¦æç¤ºä¼˜åŒ–",
      "authors": [
        "Pingchen Lu",
        "Zhi Hong",
        "Zhiwei Shang",
        "Zhiyong Wang",
        "Yikun Ban",
        "Yao Shu",
        "Min Zhang",
        "Shuang Qiu",
        "Zhongxiang Dai"
      ],
      "abstract": "The performance of large language models (LLMs) is highly sensitive to the input prompt, making prompt optimization a critical task. However, real-world application is hindered by three major challenges: (1) the black-box nature of powerful proprietary LLMs, (2) the need for high sample efficiency due to query costs, and (3) the desire for privacy-preserving collaboration among multiple users. To address these challenges simultaneously, we introduce a novel framework for sample-efficient federated prompt optimization based on multi-armed bandits (MABs). The MAB framework is uniquely suited for this problem as it is (1) inherently a black-box optimization method, (2) practically sample-efficient, and (3) enables collaborative learning with theoretically guaranteed benefit from more participating agents. We first propose the Federated Prompt Optimization via Bandits (FedPOB) algorithm, a federated variant of the Linear UCB algorithm, where agents collaborate by sharing model parameters instead of raw data. We then extend our approach to the practical setting of comparative user feedback by introducing FedPOB with Preference Feedback (FedPOB-Pref), an efficient algorithm based on federated dueling bandits. Extensive experiments demonstrate that both FedPOB and FedPOB-Pref significantly outperform existing baselines and that their performance consistently improves as more agents participate in the collaboration, validating the effectiveness of our federated approach.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)æç¤ºè¯ä¼˜åŒ–ä¸­é¢ä¸´çš„é»‘ç›’ç‰¹æ€§ã€é‡‡æ ·æ•ˆç‡ä½ä»¥åŠåä½œéšç§ä¿æŠ¤ä¸‰å¤§æŒ‘æˆ˜ï¼Œæå‡ºäº†åŸºäºå¤šè‡‚è€è™æœº(Multi-armed bandits)çš„é«˜æ•ˆè”é‚¦æç¤ºè¯ä¼˜åŒ–æ¡†æ¶FedPOBã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤šè‡‚è€è™æœºåœ¨é»‘ç›’ä¼˜åŒ–å’Œé‡‡æ ·æ•ˆç‡ä¸Šçš„ä¼˜åŠ¿ï¼Œé€šè¿‡è”é‚¦å­¦ä¹ æœºåˆ¶å…è®¸ä¸åŒæ™ºèƒ½ä½“å…±äº«æ¨¡å‹å‚æ•°è€ŒéåŸå§‹æ•°æ®ï¼Œä»è€Œå®ç°éšç§å®‰å…¨ä¸‹çš„ååŒä¼˜åŒ–ã€‚ç ”ç©¶é¦–å…ˆæå‡ºäº†çº¿æ€§ç½®ä¿¡ä¸Šé™(Linear UCB)ç®—æ³•çš„è”é‚¦å˜ä½“FedPOBï¼Œå¹¶è¿›ä¸€æ­¥é’ˆå¯¹ç”¨æˆ·åå¥½åé¦ˆåœºæ™¯å¼€å‘äº†åŸºäºè”é‚¦å¯¹å†³è€è™æœº(Federated dueling bandits)çš„FedPOB-Prefç®—æ³•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFedPOBåŠå…¶å˜ä½“åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰åŸºå‡†æ–¹æ³•ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¯å®äº†éšç€å‚ä¸åä½œçš„æ™ºèƒ½ä½“æ•°é‡å¢åŠ ï¼Œç®—æ³•çš„ä¼˜åŒ–æ•ˆæœä¼šæŒç»­å¢å¼ºï¼Œå……åˆ†éªŒè¯äº†è”é‚¦å­¦ä¹ åœ¨æç¤ºè¯ä¼˜åŒ–é¢†åŸŸçš„åº”ç”¨ä»·å€¼å’Œæœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint",
      "pdf_url": "https://arxiv.org/pdf/2509.24701v1",
      "published_date": "2025-09-29 12:32:21 UTC",
      "updated_date": "2025-09-29 12:32:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:59:44.294633+00:00"
    },
    {
      "arxiv_id": "2509.24696v1",
      "title": "T-POP: Test-Time Personalization with Online Preference Feedback",
      "title_zh": "T-POPï¼šåŸºäºåœ¨çº¿åå¥½åé¦ˆçš„æµ‹è¯•æ—¶ä¸ªæ€§åŒ–",
      "authors": [
        "Zikun Qu",
        "Min Zhang",
        "Mingze Kong",
        "Xiang Li",
        "Zhiwei Shang",
        "Zhiyong Wang",
        "Yikun Ban",
        "Shuang Qiu",
        "Yao Shu",
        "Zhongxiang Dai"
      ],
      "abstract": "Personalizing large language models (LLMs) to individual user preferences is a critical step beyond generating generically helpful responses. However, current personalization methods are ill-suited for new users, as they typically require either slow, resource-intensive fine-tuning or a substantial amount of pre-existing user data, creating a significant cold-start problem. To address this challenge, we introduce a new paradigm for real-time personalization by learning from online pairwise preference feedback collected during text generation. We propose T-POP (Test-Time Personalization with Online Preference Feedback}), a novel algorithm that synergistically combines test-time alignment with dueling bandits. Without updating the LLM parameters, T-POP steers the decoding process of a frozen LLM by learning a reward function online that captures user preferences. By leveraging dueling bandits, T-POP intelligently queries the user to efficiently balance between exploring their preferences and exploiting the learned knowledge to generate personalized text. Extensive experiments demonstrate that T-POP achieves rapid and data-efficient personalization, significantly outperforming existing baselines and showing consistent improvement with more user interactions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨é¢å¯¹æ–°ç”¨æˆ·æ—¶çš„ä¸ªæ€§åŒ–å†·å¯åŠ¨(cold-start)é—®é¢˜ï¼Œæå‡ºäº†T-POP (Test-Time Personalization with Online Preference Feedback) æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åœ¨çº¿å­¦ä¹ å®ç°å®æ—¶ä¸ªæ€§åŒ–ã€‚T-POPå°†æµ‹è¯•æ—¶å¯¹é½(test-time alignment)ä¸å¯¹å¼ˆå¤šè‡‚è€è™æœº(dueling bandits)ç›¸ç»“åˆï¼Œåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æ”¶é›†æˆå¯¹åå¥½åé¦ˆï¼Œä»è€Œå¼•å¯¼å†»ç»“æ¨¡å‹çš„è§£ç è¿‡ç¨‹ã€‚è¯¥æ–¹æ³•æ— éœ€æ›´æ–°æ¨¡å‹å‚æ•°ï¼Œè€Œæ˜¯é€šè¿‡åœ¨çº¿å­¦ä¹ å¥–åŠ±å‡½æ•°(reward function)æ¥æ•æ‰ç”¨æˆ·åå¥½ï¼Œå¹¶åˆ©ç”¨å¤šè‡‚è€è™æœºæœºåˆ¶å¹³è¡¡åå¥½æ¢ç´¢ä¸çŸ¥è¯†åˆ©ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒT-POPèƒ½å¤Ÿå®ç°å¿«é€Ÿä¸”æ•°æ®é«˜æ•ˆçš„ä¸ªæ€§åŒ–ï¼Œåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰åŸºçº¿æ¨¡å‹ï¼Œå¹¶éšç€ç”¨æˆ·äº¤äº’çš„å¢åŠ è¡¨ç°å‡ºæŒç»­çš„ä¼˜åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint",
      "pdf_url": "https://arxiv.org/pdf/2509.24696v1",
      "published_date": "2025-09-29 12:28:23 UTC",
      "updated_date": "2025-09-29 12:28:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:00:29.889008+00:00"
    },
    {
      "arxiv_id": "2509.24695v2",
      "title": "SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer",
      "title_zh": "SANA-Videoï¼šåŸºäºå—çº¿æ€§æ‰©æ•£ Transformer çš„é«˜æ•ˆè§†é¢‘ç”Ÿæˆ",
      "authors": [
        "Junsong Chen",
        "Yuyang Zhao",
        "Jincheng Yu",
        "Ruihang Chu",
        "Junyu Chen",
        "Shuai Yang",
        "Xianbang Wang",
        "Yicheng Pan",
        "Daquan Zhou",
        "Huan Ling",
        "Haozhe Liu",
        "Hongwei Yi",
        "Hao Zhang",
        "Muyang Li",
        "Yukang Chen",
        "Han Cai",
        "Sanja Fidler",
        "Ping Luo",
        "Song Han",
        "Enze Xie"
      ],
      "abstract": "We introduce SANA-Video, a small diffusion model that can efficiently generate videos up to 720x1280 resolution and minute-length duration. SANA-Video synthesizes high-resolution, high-quality and long videos with strong text-video alignment at a remarkably fast speed, deployable on RTX 5090 GPU. Two core designs ensure our efficient, effective and long video generation: (1) Linear DiT: We leverage linear attention as the core operation, which is more efficient than vanilla attention given the large number of tokens processed in video generation. (2) Constant-Memory KV cache for Block Linear Attention: we design block-wise autoregressive approach for long video generation by employing a constant-memory state, derived from the cumulative properties of linear attention. This KV cache provides the Linear DiT with global context at a fixed memory cost, eliminating the need for a traditional KV cache and enabling efficient, minute-long video generation. In addition, we explore effective data filters and model training strategies, narrowing the training cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of MovieGen. Given its low cost, SANA-Video achieves competitive performance compared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B and SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover, SANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating the inference speed of generating a 5-second 720p video from 71s to 29s (2.4x speedup). In summary, SANA-Video enables low-cost, high-quality video generation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SANA-Videoï¼Œä¸€ç§èƒ½å¤Ÿé«˜æ•ˆç”Ÿæˆé«˜è¾¾720x1280åˆ†è¾¨ç‡ã€åˆ†é’Ÿçº§æ—¶é•¿è§†é¢‘çš„å°å‹æ‰©æ•£æ¨¡å‹ã€‚å…¶æ ¸å¿ƒè®¾è®¡åŒ…æ‹¬é‡‡ç”¨Linear DiTç»“æ„ä»¥æå‡å¤„ç†å¤§è§„æ¨¡Tokenæ—¶çš„æ•ˆç‡ï¼Œå¹¶ç»“åˆBlock Linear Attentionè®¾è®¡äº†æ’å®šå†…å­˜çš„KV cacheï¼Œåœ¨å›ºå®šå†…å­˜æˆæœ¬ä¸‹ä¸ºé•¿è§†é¢‘ç”Ÿæˆæä¾›å…¨å±€ä¸Šä¸‹æ–‡ã€‚SANA-Videoçš„è®­ç»ƒæˆæœ¬ä»…ä¸ºMovieGençš„1%ï¼Œåœ¨æ€§èƒ½ä¸Šå¯ä¸Wan 2.1å’ŒSkyReel-V2ç­‰æ¨¡å‹ç«äº‰ï¼Œä¸”å®æµ‹å»¶è¿Ÿé™ä½äº†16å€ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹æ”¯æŒåœ¨RTX 5090 GPUä¸Šä»¥NVFP4ç²¾åº¦éƒ¨ç½²ï¼Œæ˜¾è‘—æå‡äº†æ¨ç†é€Ÿåº¦å¹¶ä¿æŒäº†æå¼ºçš„Text-Video Alignmentã€‚è¿™é¡¹å·¥ä½œä¸ºä½æˆæœ¬ã€é«˜è´¨é‡ä¸”å¯éƒ¨ç½²åœ¨æ¶ˆè´¹çº§æ˜¾å¡ä¸Šçš„è§†é¢‘ç”Ÿæˆå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "21 pages, 15 figures, 7 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.24695v2",
      "published_date": "2025-09-29 12:28:09 UTC",
      "updated_date": "2025-10-13 09:12:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:00:35.792054+00:00"
    },
    {
      "arxiv_id": "2509.24694v1",
      "title": "CoTune: Co-evolutionary Configuration Tuning",
      "title_zh": "CoTuneï¼šåŸºäºååŒæ¼”åŒ–çš„é…ç½®è°ƒä¼˜",
      "authors": [
        "Gangda Xiong",
        "Tao Chen"
      ],
      "abstract": "To automatically tune configurations for the best possible system performance (e.g., runtime or throughput), much work has been focused on designing intelligent heuristics in a tuner. However, existing tuner designs have mostly ignored the presence of complex performance requirements (e.g., the latency shall ideally be 2 seconds), but simply assume that better performance is always more preferred. This would not only waste valuable information in a requirement but might also consume extensive resources to tune for a goal with little gain. Yet, prior studies have shown that simply incorporating the requirement as a tuning objective is problematic since the requirement might be too strict, harming convergence; or its highly diverse satisfactions might lead to premature convergence. In this paper, we propose CoTune, a tool that takes the information of a given target performance requirement into account through co-evolution. CoTune is unique in the sense that it creates an auxiliary performance requirement to be co-evolved with the configurations, which assists the target performance requirement when it becomes ineffective or even misleading, hence allowing the tuning to be guided by the requirement while being robust to its harm. Experiment results on 162 cases (nine systems and 18 requirements) reveal that CoTune considerably outperforms existing tuners, ranking as the best for 90% cases (against the 0%--35% for other tuners) with up to 2.9x overall improvements, while doing so under a much better efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CoTuneï¼Œä¸€ç§é’ˆå¯¹ç³»ç»Ÿæ€§èƒ½é…ç½®è°ƒä¼˜(Configuration Tuning)çš„ååŒè¿›åŒ–(Co-evolutionary)å·¥å…·ã€‚é’ˆå¯¹ç°æœ‰è°ƒä¼˜å™¨å¤§å¤šå¿½ç•¥å¤æ‚æ€§èƒ½éœ€æ±‚(Performance Requirements)ï¼Œæˆ–åœ¨ç›´æ¥æ•´åˆéœ€æ±‚æ—¶é¢ä¸´æ”¶æ•›å›°éš¾åŠè¿‡æ—©æ”¶æ•›çš„é—®é¢˜ï¼ŒCoTuneé€šè¿‡å¼•å…¥ä¸é…ç½®å…±åŒè¿›åŒ–çš„è¾…åŠ©æ€§èƒ½éœ€æ±‚(Auxiliary Performance Requirement)æ¥æä¾›å¼•å¯¼ã€‚å½“ç›®æ ‡æ€§èƒ½éœ€æ±‚å¤±æ•ˆæˆ–äº§ç”Ÿè¯¯å¯¼æ—¶ï¼Œè¯¥è¾…åŠ©éœ€æ±‚èƒ½ç¡®ä¿è°ƒä¼˜è¿‡ç¨‹çš„é²æ£’æ€§ï¼Œä»è€Œæœ‰æ•ˆåˆ©ç”¨éœ€æ±‚ä¿¡æ¯å¹¶é¿å…èµ„æºæµªè´¹ã€‚åœ¨æ¶‰åŠ9ä¸ªç³»ç»Ÿå’Œ18é¡¹éœ€æ±‚çš„162ä¸ªå®éªŒæ¡ˆä¾‹ä¸­ï¼ŒCoTuneåœ¨90%çš„æ¡ˆä¾‹ä¸­è¡¨ç°æœ€ä¼˜ï¼Œç›¸æ¯”ç°æœ‰å·¥å…·å®ç°äº†é«˜è¾¾2.9å€çš„æ•´ä½“æ€§èƒ½æå‡ï¼Œä¸”å…·å¤‡æ›´é«˜çš„æ‰§è¡Œæ•ˆç‡ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted by ASE 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.24694v1",
      "published_date": "2025-09-29 12:27:47 UTC",
      "updated_date": "2025-09-29 12:27:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:00:43.982544+00:00"
    },
    {
      "arxiv_id": "2509.24679v1",
      "title": "Data-Driven Discrete Geofence Design Using Binary Quadratic Programming",
      "title_zh": "åŸºäºäºŒæ¬¡äºŒè¿›åˆ¶è§„åˆ’çš„æ•°æ®é©±åŠ¨ç¦»æ•£åœ°ç†å›´æ è®¾è®¡",
      "authors": [
        "Keisuke Otaki",
        "Akihisa Okada",
        "Tadayoshi Matsumori",
        "Hiroaki Yoshida"
      ],
      "abstract": "Geofences have attracted significant attention in the design of spatial and virtual regions for managing and engaging spatiotemporal events. By using geofences to monitor human activity across their boundaries, content providers can create spatially triggered events that include notifications about points of interest within a geofence by pushing spatial information to the devices of users. Traditionally, geofences were hand-crafted by providers. In addition to the hand-crafted approach, recent advances in collecting human mobility data through mobile devices can accelerate the automatic and data-driven design of geofences, also known as the geofence design problem. Previous approaches assume circular shapes; thus, their flexibility is insufficient, and they can only handle geofence-based applications for large areas with coarse resolutions. A challenge with using circular geofences in urban and high-resolution areas is that they often overlap and fail to align with political district boundaries and road segments, such as one-way streets and median barriers. In this study, we address the problem of extracting arbitrary shapes as geofences from human mobility data to mitigate this problem. In our formulation, we cast the existing optimization problems for circular geofences to 0-1 integer programming problems to represent arbitrary shapes. Although 0-1 integer programming problems are computationally hard, formulating them as quadratic (unconstrained) binary optimization problems enables efficient approximation of optimal solutions, because this allows the use of specialized quadratic solvers, such as the quantum annealing, and other state-of-the-art algorithms. We then develop and compare different formulation methods to extract discrete geofences. We confirmed that our new modeling approach enables flexible geofence design.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨æ•°æ®é©±åŠ¨çš„æ–¹æ³•è®¾è®¡ç¦»æ•£åœ°ç†å›´æ (Geofences)ï¼Œä»¥è§£å†³ä¼ ç»Ÿåœ†å½¢å›´æ åœ¨åŸå¸‚é«˜åˆ†è¾¨ç‡åœºæ™¯ä¸‹çµæ´»æ€§ä¸è¶³ä¸”éš¾ä»¥ä¸é“è·¯æˆ–è¡Œæ”¿è¾¹ç•Œå¯¹é½çš„é—®é¢˜ã€‚ç ”ç©¶è€…å°†æå–ä»»æ„å½¢çŠ¶å›´æ çš„ä»»åŠ¡è½¬åŒ–ä¸º0-1 integer programmingé—®é¢˜ï¼Œå¹¶è¿›ä¸€æ­¥å»ºæ¨¡ä¸ºQuadratic Unconstrained Binary Optimization (QUBO)ä»¥å®ç°é«˜æ•ˆæ±‚è§£ã€‚é€šè¿‡åº”ç”¨Quantum Annealingç­‰å…ˆè¿›çš„äºŒæ¬¡è§„åˆ’æ±‚è§£å™¨ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä»äººç±»ç§»åŠ¨æ•°æ®ä¸­ç²¾å‡†æ•æ‰å¤æ‚çš„åœ°ç†ç©ºé—´åŒºåŸŸã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§å»ºæ¨¡æ–¹å¼å…‹æœäº†ä¼ ç»Ÿå‡ ä½•å½¢çŠ¶çš„é™åˆ¶ï¼Œæ˜¾è‘—æå‡äº†åœ°ç†å›´æ è®¾è®¡çš„çµæ´»æ€§å’Œå®ç”¨æ€§ã€‚è¯¥æ–¹æ³•ä¸ºå¤æ‚åŸå¸‚ç¯å¢ƒä¸‹çš„ç©ºé—´è§¦å‘äº‹ä»¶ç®¡ç†å’Œæ—¶ç©ºæ•°æ®åˆ†ææä¾›äº†æ›´ç²¾ç¡®çš„å·¥å…·ã€‚",
      "categories": [
        "cs.SI",
        "cs.AI"
      ],
      "primary_category": "cs.SI",
      "comment": "17 pages, 17 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.24679v1",
      "published_date": "2025-09-29 12:15:59 UTC",
      "updated_date": "2025-09-29 12:15:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:00:42.795443+00:00"
    },
    {
      "arxiv_id": "2509.24678v1",
      "title": "Reference-Free Rating of LLM Responses via Latent Information",
      "title_zh": "åŸºäºæ½œä¿¡æ¯çš„æ— å‚è€ƒå¤§è¯­è¨€æ¨¡å‹å›å¤è¯„åˆ†",
      "authors": [
        "Leander Girrbach",
        "Chi-Ping Su",
        "Tankred Saanum",
        "Richard Socher",
        "Eric Schulz",
        "Zeynep Akata"
      ],
      "abstract": "How reliable are single-response LLM-as-a-judge ratings without references, and can we obtain fine-grained, deterministic scores in this setting? We study the common practice of asking a judge model to assign Likert-scale scores to free-text responses and show two systematic issues: scores are unstable under sampling and poorly calibrated, leading to compression near the top of the scale and frequent ties. We then propose and evaluate Latent Judges, which derive scalar ratings from internal model signals: (i) probability-weighted scores over integer ratings, (ii) verifier-style probabilities of \"yes\", and (iii) linear probes trained on model activations at the rating position. Across a broad suite of pairwise and single-rating benchmarks, latent methods match or surpass standard prompting, with consistent gains on pairwise accuracy and listwise ranking relevant to Best-of-N selection. Probability-weighted scores achieve the strongest single-rating correlations, while probes recover useful signals when output logits are miscalibrated. These results indicate that latent information provides deterministic and more discriminative signals for reference-free evaluation, and can improve selection and training approaches like Best-of-$N$, multi-teacher distillation, and routing.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨æ— å‚è€ƒå†…å®¹çš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºè¯„åˆ¤è€…(LLM-as-a-judge)ä¸ºè‡ªç”±æ–‡æœ¬å›å¤åˆ†é…Likert-scaleè¯„åˆ†çš„å¯é æ€§é—®é¢˜ï¼Œæ­ç¤ºäº†è¯„åˆ†åœ¨é‡‡æ ·ä¸‹ä¸ç¨³å®šä»¥åŠæ ¡å‡†åº¦å·®å¯¼è‡´çš„é«˜åˆ†å‹ç¼©å’Œé¢‘ç¹å¹³åˆ†ç­‰ç³»ç»Ÿæ€§ç¼ºé™·ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†Latent Judgesæ–¹æ³•ï¼Œé€šè¿‡æå–æ¨¡å‹çš„å†…éƒ¨ä¿¡å·æ¥ç”Ÿæˆæ ‡é‡è¯„åˆ†ï¼Œå…·ä½“åŒ…æ‹¬å¯¹æ•´æ•°è¯„åˆ†çš„æ¦‚ç‡åŠ æƒå¾—åˆ†(probability-weighted scores)ã€éªŒè¯è€…é£æ ¼çš„â€œyesâ€æ¦‚ç‡ä»¥åŠåœ¨è¯„åˆ†ä½ç½®å¯¹æ¨¡å‹æ¿€æ´»è¿›è¡Œçš„çº¿æ€§æ¢æµ‹(linear probes)ã€‚åœ¨å¤šé¡¹æˆå¯¹å’Œå•é¡¹è¯„åˆ†åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¿™äº›åŸºäºæ½œåœ¨ä¿¡æ¯çš„æ–¹æ³•è¾¾åˆ°æˆ–è¶…è¿‡äº†æ ‡å‡†æç¤ºè¯(standard prompting)çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸Best-of-Né€‰æ‹©ç›¸å…³çš„æˆå¯¹å‡†ç¡®ç‡å’Œåˆ—è¡¨æ’åºä¸Šå–å¾—äº†æ˜¾è‘—æå‡ã€‚å®éªŒå‘ç°æ¦‚ç‡åŠ æƒå¾—åˆ†åœ¨å•é¡¹è¯„åˆ†ç›¸å…³æ€§æ–¹é¢è¡¨ç°æœ€å¼ºï¼Œè€Œçº¿æ€§æ¢æµ‹åœ¨è¾“å‡ºé€»è¾‘å€¼(logits)æ ¡å‡†å¤±å‡†æ—¶ä»èƒ½æ¢å¤æœ‰ç”¨çš„ä¿¡å·ã€‚è¯¥ç ”ç©¶è¡¨æ˜æ½œåœ¨ä¿¡æ¯èƒ½ä¸ºæ— å‚è€ƒè¯„ä¼°æä¾›ç¡®å®šæ€§ä¸”æ›´å…·è¾¨åˆ«åŠ›çš„ä¿¡å·ï¼Œæœ‰åŠ©äºä¼˜åŒ–Best-of-Nã€å¤šå¯¼å¸ˆè’¸é¦(multi-teacher distillation)å’Œè·¯ç”±(routing)ç­‰è®­ç»ƒä¸é€‰æ‹©æ–¹æ³•ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "21 pages",
      "pdf_url": "https://arxiv.org/pdf/2509.24678v1",
      "published_date": "2025-09-29 12:15:52 UTC",
      "updated_date": "2025-09-29 12:15:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:00:48.588569+00:00"
    },
    {
      "arxiv_id": "2509.24675v1",
      "title": "Understanding the Dilemma of Unlearning for Large Language Models",
      "title_zh": "æ¢ç©¶å¤§è¯­è¨€æ¨¡å‹æœºå™¨é—å¿˜çš„å›°å¢ƒ",
      "authors": [
        "Qingjie Zhang",
        "Haoting Qian",
        "Zhicong Huang",
        "Cheng Hong",
        "Minlie Huang",
        "Ke Xu",
        "Chao Zhang",
        "Han Qiu"
      ],
      "abstract": "Unlearning seeks to remove specific knowledge from large language models (LLMs), but its effectiveness remains contested. On one side, \"forgotten\" knowledge can often be recovered through interventions such as light fine-tuning; on the other side, unlearning may induce catastrophic forgetting that degrades general capabilities. Despite active exploration of unlearning methods, interpretability analyses of the mechanism are scarce due to the difficulty of tracing knowledge in LLMs' complex architectures. We address this gap by proposing unPact, an interpretable framework for unlearning via prompt attribution and contribution tracking. Typically, it quantifies each prompt token's influence on outputs, enabling pre- and post-unlearning comparisons to reveal what changes. Across six mainstream unlearning methods, three LLMs, and three benchmarks, we find that: (1) Unlearning appears to be effective by disrupting focus on keywords in prompt; (2) Much of the knowledge is not truly erased and can be recovered by simply emphasizing these keywords in prompts, without modifying the model's weights; (3) Catastrophic forgetting arises from indiscriminate penalization of all tokens. Taken together, our results suggest an unlearning dilemma: existing methods tend either to be insufficient - knowledge remains recoverable by keyword emphasis, or overly destructive - general performance collapses due to catastrophic forgetting, still leaving a gap to reliable unlearning.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(Large Language Models)æœºå™¨é—å¿˜(Unlearning)åœ¨çŸ¥è¯†å»é™¤æœ‰æ•ˆæ€§ä¸æ³›åŒ–èƒ½åŠ›ä¸‹é™ä¹‹é—´çš„å›°å¢ƒã€‚ä½œè€…æå‡ºäº†unPactï¼Œä¸€ä¸ªé€šè¿‡æç¤ºè¯å½’å› (prompt attribution)å’Œè´¡çŒ®è¿½è¸ª(contribution tracking)æ¥é‡åŒ–æç¤ºè¯å¯¹è¾“å‡ºå½±å“çš„å¯è§£é‡Šæ€§æ¡†æ¶ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰çš„é—å¿˜æ–¹æ³•ä¸»è¦é€šè¿‡æ‰°ä¹±æ¨¡å‹å¯¹æç¤ºè¯ä¸­å…³é”®è¯(keywords)çš„å…³æ³¨æ¥å®ç°ï¼Œè€ŒéçœŸæ­£ä»æƒé‡ä¸­æŠ¹é™¤çŸ¥è¯†ã€‚å®éªŒè¡¨æ˜ï¼Œå³ä¾¿ä¸ä¿®æ”¹æ¨¡å‹å‚æ•°ï¼Œä»…é€šè¿‡åœ¨æç¤ºè¯ä¸­å¼ºè°ƒå…³é”®è¯å³å¯æ¢å¤å¤§éƒ¨åˆ†è¢«é—å¿˜çš„å†…å®¹ã€‚åŒæ—¶ï¼Œç”±äºå¯¹æ‰€æœ‰æ ‡è®°è¿›è¡Œæ— å·®åˆ«æƒ©ç½šï¼Œè¿™äº›æ–¹æ³•å¾€å¾€ä¼šå¼•å‘ä¸¥é‡çš„ç¾éš¾æ€§é—å¿˜(catastrophic forgetting)ã€‚ç»¼ä¸Šæ‰€è¿°ï¼Œç›®å‰çš„å¤§è¯­è¨€æ¨¡å‹æœºå™¨é—å¿˜æŠ€æœ¯é¢ä¸´ç€é—å¿˜ä¸å½»åº•æˆ–ç ´åæ€§è¿‡å¼ºçš„åŒé‡å›°å¢ƒï¼Œè·ç¦»å®ç°çœŸæ­£å¯é çš„çŸ¥è¯†æ¸…é™¤ä»æœ‰è¾ƒå¤§å·®è·ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24675v1",
      "published_date": "2025-09-29 12:15:19 UTC",
      "updated_date": "2025-09-29 12:15:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:00:55.487977+00:00"
    },
    {
      "arxiv_id": "2509.24663v1",
      "title": "InfLLM-V2: Dense-Sparse Switchable Attention for Seamless Short-to-Long Adaptation",
      "title_zh": "InfLLM-V2ï¼šå®ç°çŸ­é•¿åºåˆ—æ— ç¼é€‚é…çš„ç¨ å¯†-ç¨€ç–å¯åˆ‡æ¢æ³¨æ„åŠ›æœºåˆ¶",
      "authors": [
        "Weilin Zhao",
        "Zihan Zhou",
        "Zhou Su",
        "Chaojun Xiao",
        "Yuxuan Li",
        "Yanghao Li",
        "Yudi Zhang",
        "Weilun Zhao",
        "Zhen Li",
        "Yuxiang Huang",
        "Ao Sun",
        "Xu Han",
        "Zhiyuan Liu"
      ],
      "abstract": "Long-sequence processing is a critical capability for modern large language models. However, the self-attention mechanism in the standard Transformer architecture faces severe computational and memory bottlenecks when processing long sequences. While trainable sparse attention methods offer a promising solution, existing approaches such as NSA introduce excessive extra parameters and disrupt the conventional \\textit{pretrain-on-short, finetune-on-long} workflow, resulting in slow convergence and difficulty in acceleration. To overcome these limitations, we introduce dense-sparse switchable attention framework, termed as InfLLM-V2. InfLLM-V2 is a trainable sparse attention that seamlessly adapts models from short to long sequences. Specifically, InfLLM-V2 reuses dense attention parameters through parameter-free architecture modification, maintaining consistency between short and long sequence processing. Additionally, InfLLM-V2 ensures computational efficiency across all sequence lengths, by using dense attention for short inputs and smoothly transitioning to sparse attention for long sequences. To achieve practical acceleration, we further introduce an efficient implementation of InfLLM-V2 that significantly reduces the computational overhead. Our experiments on long-context understanding and chain-of-thought reasoning demonstrate that InfLLM-V2 is 4$\\times$ faster than dense attention while retaining 98.1% and 99.7% of the performance, respectively. Based on the InfLLM-V2 framework, we have trained and open-sourced MiniCPM4.1 (https://huggingface.co/openbmb/MiniCPM4.1-8B), a hybrid reasoning model, providing a reproducible implementation for the research community.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† InfLLM-V2ï¼Œä¸€ä¸ªæ”¯æŒ Dense-Sparse Switchable Attention çš„æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°æ¨¡å‹ä»çŸ­åºåˆ—åˆ°é•¿åºåˆ—çš„æ— ç¼é€‚é…ã€‚ä¸ºäº†è§£å†³æ ‡å‡† Transformer åœ¨å¤„ç†é•¿åºåˆ—æ—¶çš„è®¡ç®—ä¸æ˜¾å­˜ç“¶é¢ˆï¼Œä»¥åŠç°æœ‰ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•å‚æ•°å†—ä½™ä¸”éš¾ä»¥æ”¶æ•›çš„é—®é¢˜ï¼ŒInfLLM-V2 é€šè¿‡ Parameter-free architecture modification å¤ç”¨ç¨ å¯†æ³¨æ„åŠ›å‚æ•°ï¼Œä¿è¯äº† Short-to-Long Adaptation è¿‡ç¨‹ä¸­å¤„ç†é€»è¾‘çš„ä¸€è‡´æ€§ã€‚è¯¥æ¡†æ¶åœ¨å¤„ç†çŸ­è¾“å…¥æ—¶é‡‡ç”¨ Dense Attentionï¼Œå¹¶åœ¨é•¿åºåˆ—åœºæ™¯ä¸‹å¹³æ»‘åˆ‡æ¢è‡³ Sparse Attentionï¼Œä»è€Œç¡®ä¿äº†å…¨åºåˆ—é•¿åº¦èŒƒå›´å†…çš„è®¡ç®—æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒInfLLM-V2 çš„æ¨ç†é€Ÿåº¦æ¯”åŸç”Ÿç¨ å¯†æ³¨æ„åŠ›å¿« 4 å€ï¼Œä¸”åœ¨ Long-context understanding å’Œ Chain-of-Thought æ¨ç†ä»»åŠ¡ä¸­åˆ†åˆ«ä¿ç•™äº† 98.1% å’Œ 99.7% çš„åŸå§‹æ€§èƒ½ã€‚ç›®å‰ï¼ŒåŸºäºæ­¤æ¡†æ¶æ„å»ºçš„æ¨ç†æ¨¡å‹ MiniCPM4.1 å·²å¼€æºï¼Œä¸ºç ”ç©¶ç¤¾åŒºæä¾›äº†é«˜æ•ˆä¸”å¯å¤ç°çš„é•¿æ–‡æœ¬å¤„ç†æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24663v1",
      "published_date": "2025-09-29 12:08:33 UTC",
      "updated_date": "2025-09-29 12:08:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:01:07.453945+00:00"
    },
    {
      "arxiv_id": "2509.24662v1",
      "title": "Community detection robustness of graph neural networks",
      "title_zh": "å›¾ç¥ç»ç½‘ç»œç¤¾åŒºæ£€æµ‹çš„é²æ£’æ€§",
      "authors": [
        "Jaidev Goel",
        "Pablo Moriano",
        "Ramakrishnan Kannan",
        "Yulia R. Gel"
      ],
      "abstract": "Graph neural networks (GNNs) are increasingly widely used for community detection in attributed networks. They combine structural topology with node attributes through message passing and pooling. However, their robustness or lack of thereof with respect to different perturbations and targeted attacks in conjunction with community detection tasks is not well understood. To shed light into latent mechanisms behind GNN sensitivity on community detection tasks, we conduct a systematic computational evaluation of six widely adopted GNN architectures: GCN, GAT, Graph-SAGE, DiffPool, MinCUT, and DMoN. The analysis covers three perturbation categories: node attribute manipulations, edge topology distortions, and adversarial attacks. We use element-centric similarity as the evaluation metric on synthetic benchmarks and real-world citation networks. Our findings indicate that supervised GNNs tend to achieve higher baseline accuracy, while unsupervised methods, particularly DMoN, maintain stronger resilience under targeted and adversarial perturbations. Furthermore, robustness appears to be strongly influenced by community strength, with well-defined communities reducing performance loss. Across all models, node attribute perturbations associated with targeted edge deletions and shift in attribute distributions tend to cause the largest degradation in community recovery. These findings highlight important trade-offs between accuracy and robustness in GNN-based community detection and offer new insights into selecting architectures resilient to noise and adversarial attacks.",
      "tldr_zh": "è¯¥ç ”ç©¶ç³»ç»Ÿæ€§åœ°è¯„ä¼°äº†å›¾ç¥ç»ç½‘ç»œ(Graph Neural Networks, GNNs)åœ¨å±æ€§ç½‘ç»œç¤¾åŒºæ£€æµ‹ä»»åŠ¡ä¸­çš„é²æ£’æ€§ï¼Œæ—¨åœ¨æ­ç¤ºä¸åŒæ¨¡å‹åœ¨å—åˆ°æ‰°åŠ¨å’Œæ”»å‡»æ—¶çš„æ½œåœ¨æ•æ„Ÿæ€§æœºåˆ¶ã€‚è¯„ä¼°æ¶µç›–äº† GCN, GAT, Graph-SAGE, DiffPool, MinCUT å’Œ DMoN å…­ç§å¹¿æ³›é‡‡ç”¨çš„ GNN æ¶æ„ï¼Œå¹¶é’ˆå¯¹èŠ‚ç‚¹å±æ€§æ“çºµ(node attribute manipulations)ã€è¾¹æ‹“æ‰‘æ‰­æ›²(edge topology distortions)ä»¥åŠå¯¹æŠ—æ€§æ”»å‡»(adversarial attacks)ä¸‰ç±»æ‰°åŠ¨è¿›è¡Œæ·±å…¥åˆ†æã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè™½ç„¶ç›‘ç£å¼ GNN æ¨¡å‹é€šå¸¸å…·æœ‰è¾ƒé«˜çš„åŸºå‡†å‡†ç¡®ç‡ï¼Œä½†éç›‘ç£å¼æ–¹æ³•ï¼ˆç‰¹åˆ«æ˜¯ DMoNï¼‰åœ¨é¢ä¸´é’ˆå¯¹æ€§æ”»å‡»å’Œå¯¹æŠ—æ€§æ‰°åŠ¨æ—¶è¡¨ç°å‡ºæ›´å¼ºçš„éŸ§æ€§ã€‚æ­¤å¤–ï¼Œç¤¾åŒºæ£€æµ‹çš„é²æ£’æ€§å—ç¤¾åŒºå¼ºåº¦(community strength)çš„æ˜¾è‘—å½±å“ï¼Œå®šä¹‰æ˜ç¡®çš„ç¤¾åŒºç»“æ„èƒ½æœ‰æ•ˆå‡å°‘æ€§èƒ½æŸå¤±ã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œä¸é’ˆå¯¹æ€§è¾¹åˆ é™¤å’Œå±æ€§åˆ†å¸ƒåç§»ç›¸å…³çš„èŠ‚ç‚¹å±æ€§æ‰°åŠ¨å¾€å¾€ä¼šå¯¼è‡´ç¤¾åŒºæ¢å¤æ€§èƒ½å‡ºç°æœ€ä¸¥é‡çš„ä¸‹é™ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†åŸºäº GNN çš„ç¤¾åŒºæ£€æµ‹åœ¨å‡†ç¡®æ€§ä¸é²æ£’æ€§ä¹‹é—´çš„æƒè¡¡ï¼Œä¸ºé€‰æ‹©èƒ½å¤ŸæŠµå¾¡å™ªå£°å’Œå¯¹æŠ—æ€§æ”»å‡»çš„æ¶æ„æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.SI",
        "cs.AI",
        "physics.soc-ph",
        "stat.ML"
      ],
      "primary_category": "cs.SI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24662v1",
      "published_date": "2025-09-29 12:08:22 UTC",
      "updated_date": "2025-09-29 12:08:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:00:59.590431+00:00"
    },
    {
      "arxiv_id": "2509.24660v1",
      "title": "Successful Misunderstandings: Learning to Coordinate Without Being Understood",
      "title_zh": "æˆåŠŸçš„è¯¯è§£ï¼šå­¦ä¹ åœ¨äº’ä¸ç†è§£çš„æƒ…å†µä¸‹å®ç°åä½œ",
      "authors": [
        "Nikolaos Kondylidis",
        "Anil Yaman",
        "Frank van Harmelen",
        "Erman Acar",
        "Annette ten Teije"
      ],
      "abstract": "The main approach to evaluating communication is by assessing how well it facilitates coordination. If two or more individuals can coordinate through communication, it is generally assumed that they understand one another. We investigate this assumption in a signaling game where individuals develop a new vocabulary of signals to coordinate successfully. In our game, the individuals do not have common observations besides the communication signal and outcome of the interaction, i.e. received reward. This setting is used as a proxy to study communication emergence in populations of agents that perceive their environment very differently, e.g. hybrid populations that include humans and artificial agents. Agents develop signals, use them, and refine interpretations while not observing how other agents are using them. While populations always converge to optimal levels of coordination, in some cases, interacting agents interpret and use signals differently, converging to what we call successful misunderstandings. However, agents of population that coordinate using misaligned interpretations, are unable to establish successful coordination with new interaction partners. Not leading to coordination failure immediately, successful misunderstandings are difficult to spot and repair. Having at least three agents that all interact with each other are the two minimum conditions to ensure the emergence of shared interpretations. Under these conditions, the agent population exhibits this emergent property of compensating for the lack of shared observations of signal use, ensuring the emergence of shared interpretations.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åä½œæ˜¯å¦å¿…ç„¶æ„å‘³ç€ç›¸äº’ç†è§£ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¿¡å·åšå¼ˆ(signaling game)ç¯å¢ƒä¸‹ï¼Œæ™ºèƒ½ä½“(agents)å¦‚ä½•åœ¨ç¼ºä¹å…±åŒè§‚æµ‹çš„æƒ…å†µä¸‹é€šè¿‡å»ºç«‹æ–°è¯æ±‡è¡¨æ¥å®ç°åä½œã€‚ç ”ç©¶é‡‡ç”¨äº†ä¸€ç§ä»£ç†è®¾ç½®æ¥æ¨¡æ‹Ÿæ„ŸçŸ¥å·®å¼‚å·¨å¤§çš„æ··åˆç¾¤ä½“ï¼Œå…¶ä¸­æ™ºèƒ½ä½“åœ¨æ— æ³•è§‚å¯Ÿä»–äººå¦‚ä½•ä½¿ç”¨ä¿¡å·çš„æƒ…å†µä¸‹å¼€å‘å¹¶ç»†åŒ–è§£é‡Šã€‚å®éªŒå‘ç°ï¼Œå°½ç®¡ç¾¤ä½“æ€»èƒ½æ”¶æ•›åˆ°æœ€ä½³åä½œæ°´å¹³ï¼Œä½†åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæ™ºèƒ½ä½“ä¼šå¯¹ä¿¡å·äº§ç”Ÿä¸ä¸€è‡´çš„è§£é‡Šï¼Œå½¢æˆæ‰€è°“çš„â€œæˆåŠŸçš„è¯¯è§£â€(successful misunderstandings)ã€‚è¿™ç§è¯¯è§£è™½ç„¶ä¸ä¼šç«‹å³å¯¼è‡´åä½œå¤±è´¥ï¼Œä½†å½“æ™ºèƒ½ä½“ä¸æ–°çš„ä¼™ä¼´äº¤äº’æ—¶å°†æ— æ³•å»ºç«‹æˆåŠŸçš„åä½œï¼Œä¸”è¿™ç§çŠ¶æ€éš¾ä»¥è¢«å¯Ÿè§‰å’Œä¿®å¤ã€‚ç ”ç©¶è¿›ä¸€æ­¥æŒ‡å‡ºï¼Œç¡®ä¿å…±äº«è§£é‡Š(shared interpretations)å‡ºç°çš„æœ€ä½æ¡ä»¶æ˜¯è‡³å°‘æœ‰ä¸‰ä¸ªæ™ºèƒ½ä½“ç›¸äº’äº¤äº’ã€‚åœ¨æ­¤æ¡ä»¶ä¸‹ï¼Œç¾¤ä½“å±•ç°å‡ºè¡¥å¿ç¼ºä¹ä¿¡å·ä½¿ç”¨å…±åŒè§‚æµ‹çš„æ¶Œç°å±æ€§ï¼Œä»è€Œç¡®ä¿äº†è¯­ä¹‰ç†è§£çš„ä¸€è‡´æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "22nd European Conference on Multi-Agent Systems (EUMAS 2025)",
      "pdf_url": "https://arxiv.org/pdf/2509.24660v1",
      "published_date": "2025-09-29 12:08:00 UTC",
      "updated_date": "2025-09-29 12:08:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:01:06.863292+00:00"
    },
    {
      "arxiv_id": "2510.15917v1",
      "title": "Intent-Driven Storage Systems: From Low-Level Tuning to High-Level Understanding",
      "title_zh": "æ„å›¾é©±åŠ¨çš„å­˜å‚¨ç³»ç»Ÿï¼šä»åº•å±‚è°ƒä¼˜åˆ°é«˜å±‚ç†è§£",
      "authors": [
        "Shai Bergman",
        "Won Wook Song",
        "Lukas Cavigelli",
        "Konstantin Berestizshevsky",
        "Ke Zhou",
        "Ji Zhang"
      ],
      "abstract": "Existing storage systems lack visibility into workload intent, limiting their ability to adapt to the semantics of modern, large-scale data-intensive applications. This disconnect leads to brittle heuristics and fragmented, siloed optimizations. To address these limitations, we propose Intent-Driven Storage Systems (IDSS), a vision for a new paradigm where large language models (LLMs) infer workload and system intent from unstructured signals to guide adaptive and cross-layer parameter reconfiguration. IDSS provides holistic reasoning for competing demands, synthesizing safe and efficient decisions within policy guardrails. We present four design principles for integrating LLMs into storage control loops and propose a corresponding system architecture. Initial results on FileBench workloads show that IDSS can improve IOPS by up to 2.45X by interpreting intent and generating actionable configurations for storage components such as caching and prefetching. These findings suggest that, when constrained by guardrails and embedded within structured workflows, LLMs can function as high-level semantic optimizers, bridging the gap between application goals and low-level system control. IDSS points toward a future in which storage systems are increasingly adaptive, autonomous, and aligned with dynamic workload demands.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰å­˜å‚¨ç³»ç»Ÿç¼ºä¹å¯¹å·¥ä½œè´Ÿè½½æ„å›¾(workload intent)çš„å¯è§æ€§ï¼Œå¯¼è‡´å¯å‘å¼ä¼˜åŒ–ç­–ç•¥ç¢ç‰‡åŒ–ä¸”è„†å¼±çš„é—®é¢˜ï¼Œæå‡ºäº†æ„å›¾é©±åŠ¨å­˜å‚¨ç³»ç»Ÿ(Intent-Driven Storage Systems, IDSS)è¿™ä¸€æ–°èŒƒå¼ã€‚IDSSåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)ä»éç»“æ„åŒ–ä¿¡å·ä¸­æ¨æ–­åº”ç”¨å’Œç³»ç»Ÿæ„å›¾ï¼Œå¹¶åœ¨ç­–ç•¥æŠ¤æ (policy guardrails)çš„çº¦æŸä¸‹æŒ‡å¯¼è‡ªé€‚åº”çš„è·¨å±‚å‚æ•°é‡é…ç½®ã€‚ç ”ç©¶å›¢é˜Ÿåˆ¶å®šäº†å°†LLMsé›†æˆåˆ°å­˜å‚¨æ§åˆ¶å›è·¯ä¸­çš„å››é¡¹è®¾è®¡åŸåˆ™ï¼Œå¹¶è®¾è®¡äº†ç›¸åº”çš„ç³»ç»Ÿæ¶æ„ä»¥å®ç°æ•´ä½“æ¨ç†ã€‚åœ¨FileBenchå·¥ä½œè´Ÿè½½ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒIDSSé€šè¿‡ä¸ºç¼“å­˜(caching)å’Œé¢„å–(prefetching)ç­‰ç»„ä»¶ç”Ÿæˆç²¾å‡†çš„é…ç½®ï¼Œå°†ç³»ç»Ÿçš„IOPSæå‡äº†é«˜è¾¾2.45å€ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œåœ¨å—é™çš„ç»“æ„åŒ–å·¥ä½œæµä¸­ï¼ŒLLMså¯ä»¥ä½œä¸ºé«˜çº§è¯­ä¹‰ä¼˜åŒ–å™¨ï¼Œæœ‰æ•ˆå¼¥åˆåº”ç”¨å±‚ç›®æ ‡ä¸åº•å±‚ç³»ç»Ÿæ§åˆ¶ä¹‹é—´çš„é¸¿æ²Ÿã€‚è¯¥ç ”ç©¶ä¸ºå®ç°æ›´åŠ è‡ªé€‚åº”ã€è‡ªä¸»åŒ–ä¸”ä¸åŠ¨æ€å·¥ä½œè´Ÿè½½éœ€æ±‚ä¿æŒä¸€è‡´çš„æœªæ¥å­˜å‚¨ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.15917v1",
      "published_date": "2025-09-29 12:07:40 UTC",
      "updated_date": "2025-09-29 12:07:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:01:07.785242+00:00"
    },
    {
      "arxiv_id": "2510.03289v2",
      "title": "Why mask diffusion does not work",
      "title_zh": "æ©ç æ‰©æ•£æ¨¡å‹ä¸ºä½•éš¾ä»¥å¥æ•ˆ",
      "authors": [
        "Haocheng Sun",
        "Cynthia Xin Wen",
        "Edward Hong Wang"
      ],
      "abstract": "The main advantages of diffusion language models over autoregressive (AR) models lie in their ability to support parallel generation and bidirectional attention, enabling a more controllable generation process. In recent years, open-source mask diffusion language models have emerged, most of which are based on a variant known as absorbing diffusion. However, this paper demonstrates why mask diffusion faces inherent difficulties in achieving parallel generation and bidirectional attention. We also propose the most effective training and inference strategies for mask diffusion.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº† Mask Diffusion è¯­è¨€æ¨¡å‹åœ¨å®ç°å¹¶è¡Œç”Ÿæˆå’ŒåŒå‘æ³¨æ„åŠ›æ–¹é¢é¢ä¸´çš„å†…åœ¨æŒ‘æˆ˜ã€‚å°½ç®¡æ‰©æ•£æ¨¡å‹ç†è®ºä¸Šæ¯”è‡ªå›å½’ (AR) æ¨¡å‹æ›´å…·å¯æ§æ€§ï¼Œä½†ç›®å‰çš„ Mask Diffusion å˜ä½“ï¼ˆå¦‚ Absorbing Diffusionï¼‰åœ¨å®é™…åº”ç”¨ä¸­ä»å­˜åœ¨éš¾ä»¥å…‹æœçš„å±€é™ã€‚æœ¬æ–‡è¯¦ç»†åˆ†æå¹¶è®ºè¯äº† Mask Diffusion åœ¨å¤„ç†å¹¶è¡ŒåŒ–ä»»åŠ¡å’ŒåŒå‘å…³è”æ—¶æ— æ³•è¾¾åˆ°ç†æƒ³æ•ˆæœçš„æ·±å±‚åŸå› ã€‚é’ˆå¯¹è¿™äº›å‘ç°ï¼Œä½œè€…è¿›ä¸€æ­¥æå‡ºäº†æœ€ä¸ºæœ‰æ•ˆçš„ Mask Diffusion è®­ç»ƒä¸æ¨ç†ç­–ç•¥ï¼Œæ—¨åœ¨æœ€å¤§åŒ–è¯¥ç±»æ¨¡å‹çš„ç”Ÿæˆæ½œåŠ›ã€‚é€šè¿‡å¯¹ç°æœ‰æœºåˆ¶çš„æ‰¹åˆ¤æ€§å®¡æŸ¥ï¼Œè¯¥ç ”ç©¶ä¸ºä¼˜åŒ–æ‰©æ•£å¼è¯­è¨€ç”ŸæˆæŠ€æœ¯æä¾›äº†å…³é”®çš„ç†è®ºä¾æ®ä¸å®è·µæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03289v2",
      "published_date": "2025-09-29 12:07:09 UTC",
      "updated_date": "2025-12-23 09:36:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:01:22.441951+00:00"
    },
    {
      "arxiv_id": "2509.24659v1",
      "title": "VNODE: A Piecewise Continuous Volterra Neural Network",
      "title_zh": "VNODEï¼šä¸€ç§åˆ†æ®µè¿ç»­ Volterra ç¥ç»ç½‘ç»œ",
      "authors": [
        "Siddharth Roheda",
        "Aniruddha Bala",
        "Rohit Chowdhury",
        "Rohan Jaiswal"
      ],
      "abstract": "This paper introduces Volterra Neural Ordinary Differential Equations (VNODE), a piecewise continuous Volterra Neural Network that integrates nonlinear Volterra filtering with continuous time neural ordinary differential equations for image classification. Drawing inspiration from the visual cortex, where discrete event processing is interleaved with continuous integration, VNODE alternates between discrete Volterra feature extraction and ODE driven state evolution. This hybrid formulation captures complex patterns while requiring substantially fewer parameters than conventional deep architectures. VNODE consistently outperforms state of the art models with improved computational complexity as exemplified on benchmark datasets like CIFAR10 and Imagenet1K.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† VNODEï¼Œä¸€ç§åˆ†æ®µè¿ç»­çš„ Volterra Neural Networkï¼Œé€šè¿‡å°†éçº¿æ€§ Volterra filtering ä¸è¿ç»­æ—¶é—´ Neural Ordinary Differential Equations (ODE) ç›¸ç»“åˆæ¥ä¼˜åŒ–å›¾åƒåˆ†ç±»ä»»åŠ¡ã€‚è¯¥æ¨¡å‹çš„è®¾è®¡çµæ„Ÿæºè‡ªè§†è§‰çš®å±‚ä¸­ç¦»æ•£äº‹ä»¶å¤„ç†ä¸è¿ç»­ç§¯åˆ†äº¤æ›¿è¿›è¡Œçš„ç”Ÿç‰©æœºåˆ¶ã€‚åœ¨è¿è¡Œè¿‡ç¨‹ä¸­ï¼ŒVNODE åœ¨ç¦»æ•£çš„ Volterra ç‰¹å¾æå–ä¸ç”± ODE é©±åŠ¨çš„çŠ¶æ€æ¼”åŒ–ä¹‹é—´äº¤æ›¿è¿›è¡Œã€‚è¿™ç§æ··åˆæ¶æ„èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰å¤æ‚æ¨¡å¼ï¼Œä¸”ä¸ä¼ ç»Ÿçš„æ·±åº¦æ¶æ„ç›¸æ¯”ï¼Œå…¶æ‰€éœ€çš„å‚æ•°é‡æ˜¾è‘—å‡å°‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVNODE åœ¨ä¿æŒä¼˜å¼‚è®¡ç®—å¤æ‚åº¦çš„åŒæ—¶ï¼Œåœ¨ CIFAR10 å’Œ Imagenet1K ç­‰åŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°æŒç»­ä¼˜äºç°æœ‰çš„ State-of-the-Art æ¨¡å‹ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "5 pages",
      "pdf_url": "https://arxiv.org/pdf/2509.24659v1",
      "published_date": "2025-09-29 12:05:57 UTC",
      "updated_date": "2025-09-29 12:05:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:01:25.055811+00:00"
    },
    {
      "arxiv_id": "2509.24653v1",
      "title": "Identity Bridge: Enabling Implicit Reasoning via Shared Latent Memory",
      "title_zh": "Identity Bridgeï¼šé€šè¿‡å…±äº«æ½œç©ºé—´è®°å¿†å®ç°éšå¼æ¨ç†",
      "authors": [
        "Pengxiao Lin",
        "Zheng-An Chen",
        "Zhi-Qin John Xu"
      ],
      "abstract": "Despite remarkable advances, large language models often fail at compositional reasoning tasks, a phenomenon exemplified by the ``curse of two-hop reasoning''. This paper introduces the Identity Bridge, a simple yet powerful mechanism that resolves this compositionality gap by supervising the model on a zero-hop identity task. We demonstrate empirically that this addition enables models to successfully perform out-of-distribution two-hop reasoning, a task they otherwise completely fail. To explain this phenomenon, we provide a theoretical analysis using a simplified Emb-MLP model, proving that identity supervision reshapes the model's latent geometry. We show this alignment is induced by an implicit nuclear-norm regularization during optimization, which favors low-rank solutions that share structure across tasks. For complex tasks, we use small initialization or weight decay to enhance the regularization effect, which enhances the latent space alignment effect and slows down the generalization decay. Finally, we extend our investigation to large-scale models, observing that they still achieve two-hop reasoning through the latent memory, which provides crucial inspiration for enhancing their implicit reasoning abilities.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç»„åˆæ¨ç†ä»»åŠ¡ä¸­é¢ä¸´çš„â€œcurse of two-hop reasoningâ€ç°è±¡ï¼Œæå‡ºäº† Identity Bridge æœºåˆ¶ã€‚è¯¥æœºåˆ¶é€šè¿‡åœ¨ zero-hop identity task ä¸Šå¯¹æ¨¡å‹è¿›è¡Œç›‘ç£ï¼ŒæˆåŠŸä½¿æ¨¡å‹å…·å¤‡äº†æ­¤å‰å®Œå…¨æ— æ³•å®Œæˆçš„ out-of-distribution two-hop reasoning èƒ½åŠ›ã€‚é€šè¿‡å¯¹ Emb-MLP æ¨¡å‹è¿›è¡Œç†è®ºåˆ†æï¼Œç ”ç©¶è¯æ˜äº† identity supervision èƒ½é€šè¿‡ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„ implicit nuclear-norm regularization é‡å¡‘æ¨¡å‹çš„ latent geometryï¼Œä¿ƒä½¿æ¨¡å‹äº§ç”Ÿè·¨ä»»åŠ¡å…±äº«ç»“æ„çš„ low-rank solutionsã€‚åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶ï¼Œé‡‡ç”¨ small initialization æˆ– weight decay å¯ä»¥è¿›ä¸€æ­¥å¼ºåŒ–æ­£åˆ™åŒ–æ•ˆæœï¼Œå¢å¼ºæ½œç©ºé—´å¯¹é½å¹¶å»¶ç¼“æ³›åŒ–è¡°å‡ã€‚æœ€åï¼Œè¯¥ç ”ç©¶å°† Identity Bridge æ‰©å±•è‡³å¤§è§„æ¨¡æ¨¡å‹ï¼Œè§‚å¯Ÿåˆ°æ¨¡å‹ä»èƒ½é€šè¿‡ shared latent memory å®ç°åŒè·³æ¨ç†ï¼Œä¸ºå¢å¼ºå¤§æ¨¡å‹çš„éšå¼æ¨ç†èƒ½åŠ›æä¾›äº†é‡è¦å¯ç¤ºã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24653v1",
      "published_date": "2025-09-29 12:02:05 UTC",
      "updated_date": "2025-09-29 12:02:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:01:27.170662+00:00"
    },
    {
      "arxiv_id": "2509.24651v1",
      "title": "\"Stop replacing salt with sugar!'': Towards Intuitive Human-Agent Teaching",
      "title_zh": "â€œåˆ«å†ç”¨ç³–ä»£æ›¿ç›äº†ï¼â€ï¼šè¿ˆå‘ç›´è§‰åŒ–çš„äººæœºæ•™å­¦",
      "authors": [
        "Nikolaos Kondylidis",
        "Andrea Rafanelli",
        "Ilaria Tiddi",
        "Annette ten Teije",
        "Frank van Harmelen"
      ],
      "abstract": "Humans quickly learn new concepts from a small number of examples. Replicating this capacity with Artificial Intelligence (AI) systems has proven to be challenging. When it comes to learning subjective tasks-where there is an evident scarcity of data-this capacity needs to be recreated. In this work, we propose an intuitive human-agent teaching architecture in which the human can teach an agent how to perform a task by providing demonstrations, i.e., examples. To have an intuitive interaction, we argue that the agent should be able to learn incrementally from a few single examples. To allow for this, our objective is to broaden the agent's task understanding using domain knowledge. Then, using a learning method to enable the agent to learn efficiently from a limited number of examples. Finally, to optimize how human can select the most representative and less redundant examples to provide the agent with. We apply our proposed method to the subjective task of ingredient substitution, where the agent needs to learn how to substitute ingredients in recipes based on human examples. We replicate human input using the Recipe1MSubs dataset. In our experiments, the agent achieves half its task performance after only 100 examples are provided, compared to the complete training set of 50k examples. We show that by providing examples in strategic order along with a learning method that leverages external symbolic knowledge, the agent can generalize more efficiently.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•æå‡äººå·¥æ™ºèƒ½(AI)åœ¨æ•°æ®ç¨€ç¼ºé¢†åŸŸå­¦ä¹ ä¸»è§‚ä»»åŠ¡çš„èƒ½åŠ›ï¼Œæ—¨åœ¨æ¨¡ä»¿äººç±»é€šè¿‡å°‘é‡ç¤ºä¾‹å³å¯æŒæ¡æ–°æ¦‚å¿µçš„é«˜æ•ˆå­¦ä¹ ç‰¹æ€§ã€‚ä½œè€…æå‡ºäº†ä¸€ç§ç›´è§‚çš„äººæœºæ•™å­¦æ¶æ„(human-agent teaching architecture)ï¼Œå…è®¸äººç±»é€šè¿‡æ¼”ç¤ºç¤ºä¾‹(demonstrations)å¼•å¯¼æ™ºèƒ½ä½“å­¦ä¹ ã€‚è¯¥æ¶æ„ç»“åˆé¢†åŸŸçŸ¥è¯†(domain knowledge)æ‰©å±•äº†æ™ºèƒ½ä½“å¯¹ä»»åŠ¡çš„ç†è§£ï¼Œå¹¶åˆ©ç”¨é«˜æ•ˆçš„å­¦ä¹ ç®—æ³•ä½¿å…¶å…·å¤‡ä»æå°‘æ•°ç¤ºä¾‹ä¸­å¢é‡å­¦ä¹ çš„èƒ½åŠ›ã€‚ç ”ç©¶é‡ç‚¹ä¼˜åŒ–äº†äººç±»å¦‚ä½•é€‰æ‹©ä»£è¡¨æ€§å¼ºã€å†—ä½™åº¦ä½çš„ç¤ºä¾‹ï¼Œå¹¶åœ¨é£Ÿææ›¿æ¢(ingredient substitution)ä»»åŠ¡ä¸­è¿›è¡Œäº†éªŒè¯ã€‚åœ¨Recipe1MSubsæ•°æ®é›†ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼Œé€šè¿‡ç»“åˆå¤–éƒ¨ç¬¦å·çŸ¥è¯†(symbolic knowledge)å’Œæˆ˜ç•¥æ€§çš„ç¤ºä¾‹æ’åºï¼Œæ™ºèƒ½ä½“ä»…éœ€100ä¸ªç¤ºä¾‹å³å¯è¾¾åˆ°å®Œæ•´è®­ç»ƒé›†ä¸€åŠçš„æ€§èƒ½ã€‚ç»“æœè¯æ˜è¯¥æ–¹æ³•èƒ½æ˜¾è‘—æå‡æ™ºèƒ½ä½“åœ¨å°æ ·æœ¬åœºæ™¯ä¸‹çš„æ³›åŒ–(generalize)æ•ˆç‡ï¼Œä¸ºå®ç°æ›´ç›´è§‚çš„äººæœºåä½œæ•™å­¦å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "22nd European Conference on Multi-Agent Systems (EUMAS 2025)",
      "pdf_url": "https://arxiv.org/pdf/2509.24651v1",
      "published_date": "2025-09-29 12:00:53 UTC",
      "updated_date": "2025-09-29 12:00:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:01:35.276992+00:00"
    },
    {
      "arxiv_id": "2509.24640v1",
      "title": "Can you SPLICE it together? A Human Curated Benchmark for Probing Visual Reasoning in VLMs",
      "title_zh": "ä½ èƒ½å°†å…¶æ‹¼æ¥èµ·æ¥å—ï¼Ÿæ¢ç©¶è§†è§‰è¯­è¨€æ¨¡å‹è§†è§‰æ¨ç†çš„äººå·¥æ„å»ºåŸºå‡†",
      "authors": [
        "Mohamad Ballout",
        "Okajevo Wilfred",
        "Seyedalireza Yaghoubi",
        "Nohayr Muhammad Abdelmoneim",
        "Julius Mayer",
        "Elia Bruni"
      ],
      "abstract": "In this work, we introduce SPLICE, a human-curated benchmark derived from the COIN instructional video dataset, designed to probe event-based reasoning across multiple dimensions: temporal, causal, spatial, contextual, and general knowledge. SPLICE includes 3,381 human-filtered videos spanning 12 categories and 180 sub-categories, such as sports, engineering, and housework. These videos are segmented into a total of 11,423 event clips. We evaluate both human participants and state-of-the-art vision-language models (VLMs) on the task of rearranging these clips into coherent event sequences to assess visual reasoning capabilities. Results reveal a significant gap: VLMs struggle to match human performance. While human-annotated textual descriptions improve model accuracy, they do not affect human performance, suggesting that models rely more on language priors than on visual understanding. Even with annotations, VLMs fall short of human-level reasoning, underscoring persistent challenges in visual reasoning. A deeper analysis across sub-categories shows that VLMs perform relatively better on videos where temporal and causal reasoning are dominant, compared to those where contextual and spatial reasoning are dominant. They also perform better on everyday tasks than on specialized ones.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† SPLICEï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäº COIN æŒ‡ä»¤è§†é¢‘æ•°æ®é›†çš„äººå·¥ç­›é€‰åŸºå‡†ï¼Œæ—¨åœ¨ä»æ—¶é—´ (temporal)ã€å› æœ (causal)ã€ç©ºé—´ (spatial)ã€ä¸Šä¸‹æ–‡ (contextual) å’Œé€šç”¨çŸ¥è¯†ç­‰ç»´åº¦æ¢æµ‹è§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) çš„äº‹ä»¶æ¨ç†èƒ½åŠ›ã€‚SPLICE åŒ…å«æ¶µç›– 180 ä¸ªå­ç±»åˆ«çš„ 3,381 ä¸ªè§†é¢‘ï¼Œé€šè¿‡è¦æ±‚æ¨¡å‹å°†åˆ†å‰²å‡ºçš„ 11,423 ä¸ªäº‹ä»¶å‰ªè¾‘é‡æ–°æ’åˆ—ä¸ºè¿è´¯åºåˆ—æ¥è¯„ä¼°å…¶è§†è§‰æ¨ç†æ°´å¹³ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVLMs çš„æ€§èƒ½æ˜¾è‘—ä½äºäººç±»ï¼Œä¸”ç ”ç©¶å‘ç°æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­é«˜åº¦ä¾èµ–è¯­è¨€å…ˆéªŒ (language priors) è€ŒéçœŸå®çš„è§†è§‰ç†è§£ã€‚å³ä¾¿è¾…ä»¥äººå·¥æ–‡æœ¬æè¿°ï¼ŒVLMs çš„è¡¨ç°ä¾ç„¶éš¾ä»¥ä¼åŠäººç±»æ°´å¹³ï¼Œåæ˜ å‡ºè§†è§‰æ¨ç†ä»»åŠ¡ä¸­çš„æŒä¹…æŒ‘æˆ˜ã€‚æ·±åº¦åˆ†æè¿›ä¸€æ­¥è¡¨æ˜ï¼Œæ¨¡å‹åœ¨æ—¶é—´ä¸å› æœæ¨ç†ä»¥åŠæ—¥å¸¸ä»»åŠ¡ä¸­è¡¨ç°è¾ƒå¥½ï¼Œä½†åœ¨æ¶‰åŠç©ºé—´ã€ä¸Šä¸‹æ–‡æ¨ç†æˆ–ä¸“ä¸šåŒ–ä»»åŠ¡æ—¶è¡¨ç°è¾ƒå·®ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24640v1",
      "published_date": "2025-09-29 11:50:18 UTC",
      "updated_date": "2025-09-29 11:50:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:01:57.785417+00:00"
    },
    {
      "arxiv_id": "2509.25286v2",
      "title": "Artificial Authority: From Machine Minds to Political Alignments. An Experimental Analysis of Democratic and Autocratic Biases in Large-Language Models",
      "title_zh": "äººå·¥æƒå¨ï¼šä»æœºå™¨å¿ƒæ™ºåˆ°æ”¿æ²»å¯¹é½â€”â€”å¤§è¯­è¨€æ¨¡å‹ä¸­æ°‘ä¸»ä¸å¨æƒåè§çš„å®éªŒåˆ†æ",
      "authors": [
        "Natalia OÅ¼egalska-Åukasik",
        "Szymon Åukasik"
      ],
      "abstract": "Political beliefs vary significantly across different countries, reflecting distinct historical, cultural, and institutional contexts. These ideologies, ranging from liberal democracies to rigid autocracies, influence human societies, as well as the digital systems that are constructed within those societies. The advent of generative artificial intelligence, particularly Large Language Models (LLMs), introduces new agents in the political space-agents trained on massive corpora that replicate and proliferate socio-political assumptions. This paper analyses whether LLMs display propensities consistent with democratic or autocratic world-views. We validate this insight through experimental tests in which we experiment with the leading LLMs developed across disparate political contexts, using several existing psychometric and political orientation measures. The analysis is based on both numerical scoring and qualitative analysis of the models' responses. Findings indicate high model-to-model variability and a strong association with the political culture of the country in which the model was developed. These findings highlight the need for more detailed examination of the socio-political dimensions embedded within AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)åœ¨ä¸åŒæ”¿æ²»èƒŒæ™¯ä¸‹å±•ç°å‡ºçš„æ°‘ä¸»æˆ–ç‹¬è£åè§ã€‚é€šè¿‡å¯¹æ¥è‡ªä¸åŒæ”¿æ²»ç¯å¢ƒçš„é¢†å…ˆLLMsè¿›è¡Œå®éªŒï¼Œåˆ©ç”¨ç°æœ‰çš„å¿ƒç†æµ‹é‡å’Œæ”¿æ²»å€¾å‘è¯„ä¼°å·¥å…·ï¼Œç»“åˆæ•°å€¼è¯„åˆ†å’Œå®šæ€§åˆ†æï¼Œç ”ç©¶å›¢é˜Ÿæ·±å…¥è¯„ä¼°äº†è¿™äº›æ¨¡å‹çš„æ”¿æ²»ç«‹åœºã€‚ç ”ç©¶å‘ç°ï¼Œä¸åŒLLMsä¹‹é—´è¡¨ç°å‡ºæ˜¾è‘—çš„å·®å¼‚æ€§ï¼Œä¸”å…¶å±•ç°å‡ºçš„åè§ä¸å…¶å¼€å‘å›½å®¶çš„æ”¿æ²»æ–‡åŒ–æœ‰ç€å¼ºçƒˆçš„å…³è”ã€‚è¿™ä¸€ç»“æœè¡¨æ˜ï¼ŒLLMsä¼šå¤åˆ¶å¹¶æ‰©æ•£å…¶è®­ç»ƒè¯­æ–™ä¸­çš„ç¤¾ä¼šæ”¿æ²»å‡è®¾ï¼Œè€Œéå®Œå…¨ä¸­ç«‹ã€‚è¯¥é¡¹å·¥ä½œå‡¸æ˜¾äº†å¯¹åµŒå…¥åœ¨äººå·¥æ™ºèƒ½ç³»ç»Ÿä¸­çš„ç¤¾ä¼šæ”¿æ²»ç»´åº¦è¿›è¡Œè¯¦ç»†å®¡æŸ¥çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25286v2",
      "published_date": "2025-09-29 11:26:58 UTC",
      "updated_date": "2025-10-04 16:30:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:01:35.052706+00:00"
    },
    {
      "arxiv_id": "2509.24616v2",
      "title": "LTL$_f$ Learning Meets Boolean Set Cover",
      "title_zh": "$LTL_f$ å­¦ä¹ ä¸å¸ƒå°”é›†åˆè¦†ç›–",
      "authors": [
        "Gabriel Bathie",
        "NathanaÃ«l Fijalkow",
        "ThÃ©o Matricon",
        "Baptiste Mouillon",
        "Pierre Vandenhove"
      ],
      "abstract": "Learning formulas in Linear Temporal Logic (LTLf) from finite traces is a fundamental research problem which has found applications in artificial intelligence, software engineering, programming languages, formal methods, control of cyber-physical systems, and robotics. We implement a new CPU tool called Bolt improving over the state of the art by learning formulas more than 100x faster over 70% of the benchmarks, with smaller or equal formulas in 98% of the cases. Our key insight is to leverage a problem called Boolean Set Cover as a subroutine to combine existing formulas using Boolean connectives. Thanks to the Boolean Set Cover component, our approach offers a novel trade-off between efficiency and formula size.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä»æœ‰é™è½¨è¿¹ä¸­å­¦ä¹ çº¿æ€§æ—¶æ€é€»è¾‘(Linear Temporal Logic, LTL$_f$)å…¬å¼è¿™ä¸€åŸºç¡€é—®é¢˜ï¼Œæ—¨åœ¨æå‡å…¬å¼çš„å­¦ä¹ æ•ˆç‡ä¸è´¨é‡ã€‚ä½œè€…å¼€å‘äº†ä¸€ç§åä¸ºBoltçš„æ–°å‹CPUå·¥å…·ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†å¸ƒå°”é›†åˆè¦†ç›–(Boolean Set Cover)é—®é¢˜ä½œä¸ºå­ç¨‹åºï¼Œåˆ©ç”¨å¸ƒå°”è¿æ¥è¯æ¥ç»„åˆç°æœ‰å…¬å¼ã€‚å¾—ç›ŠäºBoolean Set Coverç»„ä»¶çš„å¼•å…¥ï¼ŒBoltåœ¨å¤„ç†æ•ˆç‡ä¸å…¬å¼å°ºå¯¸ä¹‹é—´å®ç°äº†å…¨æ–°çš„æƒè¡¡ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒBoltåœ¨è¶…è¿‡70%çš„åŸºå‡†æµ‹è¯•ä¸­æ¯”å½“å‰æœ€å…ˆè¿›æŠ€æœ¯çš„å­¦ä¹ é€Ÿåº¦å¿«100å€ä»¥ä¸Šã€‚æ­¤å¤–ï¼Œåœ¨98%çš„æƒ…å†µä¸‹ï¼ŒBoltç”Ÿæˆçš„å…¬å¼å°ºå¯¸æ¯”ç°æœ‰å·¥å…·æ›´å°æˆ–ä¸ä¹‹æŒå¹³ã€‚è¯¥é¡¹ç ”ç©¶ä¸ºäººå·¥æ™ºèƒ½ã€è½¯ä»¶å·¥ç¨‹å’Œæœºå™¨äººç­‰é¢†åŸŸçš„å…¬å¼å­¦ä¹ ä»»åŠ¡æä¾›äº†é«˜æ•ˆä¸”å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.FL",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "Full version of TACAS 2026 conference paper. 24 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.24616v2",
      "published_date": "2025-09-29 11:20:20 UTC",
      "updated_date": "2026-01-12 21:30:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:01:45.977228+00:00"
    },
    {
      "arxiv_id": "2509.24607v1",
      "title": "Algorithms and data structures for automatic precision estimation of neural networks",
      "title_zh": "ç¥ç»ç½‘ç»œè‡ªåŠ¨ç²¾åº¦ä¼°è®¡çš„ç®—æ³•ä¸æ•°æ®ç»“æ„",
      "authors": [
        "Igor V. Netay"
      ],
      "abstract": "We describe algorithms and data structures to extend a neural network library with automatic precision estimation for floating point computations. We also discuss conditions to make estimations exact and preserve high computation performance of neural networks training and inference. Numerical experiments show the consequences of significant precision loss for particular values such as inference, gradients and deviations from mathematically predicted behavior.\n  It turns out that almost any neural network accumulates computational inaccuracies. As a result, its behavior does not coincide with predicted by the mathematical model of neural network. This shows that tracking of computational inaccuracies is important for reliability of inference, training and interpretability of results.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¥ç»ç½‘ç»œåœ¨æµ®ç‚¹è¿ç®—ä¸­ç§¯ç´¯çš„è®¡ç®—è¯¯å·®é—®é¢˜ï¼Œæå‡ºäº†ä¸€å¥—ç”¨äºæ‰©å±•ç¥ç»ç½‘ç»œåº“çš„ç®—æ³•å’Œæ•°æ®ç»“æ„ï¼Œå®ç°äº†è‡ªåŠ¨ç²¾åº¦è¯„ä¼°(automatic precision estimation)ã€‚ä½œè€…è¯¦ç»†è®¨è®ºäº†åœ¨ä¿è¯è¯„ä¼°å‡†ç¡®æ€§çš„åŒæ—¶ç»´æŒè®­ç»ƒä¸æ¨ç†é«˜æ€§èƒ½çš„å®ç°æ¡ä»¶ã€‚é€šè¿‡æ•°å€¼å®éªŒï¼Œè¯¥ç ”ç©¶å±•ç¤ºäº†æ¨ç†å’Œæ¢¯åº¦è®¡ç®—ä¸­å­˜åœ¨çš„æ˜¾è‘—ç²¾åº¦æŸå¤±ï¼Œä»¥åŠè¿™ç§æŸå¤±å¦‚ä½•å¯¼è‡´æ¨¡å‹è¡Œä¸ºåç¦»æ•°å­¦é¢„æµ‹ã€‚å®éªŒå‘ç°å‡ ä¹æ‰€æœ‰ç¥ç»ç½‘ç»œéƒ½ä¼šç§¯ç´¯è¯¯å·®ï¼Œè¿™ä½¿å¾—è¿½è¸ªè®¡ç®—ä¸ç¡®å®šæ€§å¯¹äºç¡®ä¿æ¨ç†çš„å¯é æ€§ã€è®­ç»ƒçš„ç¨³å®šæ€§ä»¥åŠç»“æœçš„å¯è§£é‡Šæ€§(interpretability)è‡³å…³é‡è¦ã€‚",
      "categories": [
        "cs.DS",
        "cs.AI",
        "cs.LG",
        "math.NA"
      ],
      "primary_category": "cs.DS",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24607v1",
      "published_date": "2025-09-29 11:13:29 UTC",
      "updated_date": "2025-09-29 11:13:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:01:38.968673+00:00"
    },
    {
      "arxiv_id": "2509.24592v2",
      "title": "BPMN Assistant: An LLM-Based Approach to Business Process Modeling",
      "title_zh": "BPMN Assistantï¼šä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ä¸šåŠ¡æµç¨‹å»ºæ¨¡æ–¹æ³•",
      "authors": [
        "Josip Tomo Licardo",
        "Nikola Tankovic",
        "Darko Etinger"
      ],
      "abstract": "This paper presents BPMN Assistant, a tool that leverages Large Language Models for natural language-based creation and editing of BPMN diagrams. While direct XML generation is common, it is verbose, slow, and prone to syntax errors during complex modifications. We introduce a specialized JSON-based intermediate representation designed to facilitate atomic editing operations through function calling. We evaluate our approach against direct XML manipulation using a suite of state-of-the-art models, including GPT-5.1, Claude 4.5 Sonnet, and DeepSeek V3. Results demonstrate that the JSON-based approach significantly outperforms direct XML in editing tasks, achieving higher or equivalent success rates across all evaluated models. Furthermore, despite requiring more input context, our approach reduces generation latency by approximately 43% and output token count by over 75%, offering a more reliable and responsive solution for interactive process modeling.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† BPMN Assistantï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (Large Language Models) é€šè¿‡è‡ªç„¶è¯­è¨€åˆ›å»ºå’Œç¼–è¾‘ BPMN å›¾è¡¨çš„å·¥å…·ã€‚é’ˆå¯¹ç›´æ¥ç”Ÿæˆ XML æ ¼å¼å¯¼è‡´çš„å†—é•¿ã€ç¼“æ…¢ä¸”åœ¨å¤æ‚ä¿®æ”¹ä¸­æ˜“å‡ºç°è¯­æ³•é”™è¯¯ç­‰é—®é¢˜ï¼Œç ”ç©¶äººå‘˜å¼•å…¥äº†ä¸€ç§ä¸“é—¨çš„ JSON-based intermediate representationã€‚è¯¥æ–¹æ³•æ—¨åœ¨é€šè¿‡å‡½æ•°è°ƒç”¨ (function calling) å®ç°åŸå­ç¼–è¾‘æ“ä½œ (atomic editing operations)ï¼Œä»è€Œä¼˜åŒ–äº¤äº’è¿‡ç¨‹ã€‚ç ”ç©¶é€šè¿‡ GPT-5.1ã€Claude 4.5 Sonnet å’Œ DeepSeek V3 ç­‰æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤º JSON-based approach åœ¨ç¼–è¾‘ä»»åŠ¡ä¸­çš„æˆåŠŸç‡æ˜¾è‘—ä¼˜äºç›´æ¥ XML æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ¡ˆåœ¨å‡å°‘è¾“å‡º Token æ•°é‡è¶…è¿‡ 75% çš„åŒæ—¶ï¼Œå°†ç”Ÿæˆå»¶è¿Ÿé™ä½äº†çº¦ 43%ã€‚è¿™ä¸ºäº¤äº’å¼ä¸šåŠ¡æµç¨‹å»ºæ¨¡æä¾›äº†ä¸€ä¸ªæ›´é«˜æ•ˆã€å¯é ä¸”å“åº”è¿…é€Ÿçš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "22 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.24592v2",
      "published_date": "2025-09-29 10:56:08 UTC",
      "updated_date": "2026-01-22 09:56:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:01:48.354368+00:00"
    },
    {
      "arxiv_id": "2509.24591v2",
      "title": "PoseDiff: A Unified Diffusion Model Bridging Robot Pose Estimation and Video-to-Action Control",
      "title_zh": "PoseDiffï¼šè¿æ¥æœºå™¨äººå§¿æ€ä¼°è®¡ä¸è§†é¢‘åˆ°åŠ¨ä½œæ§åˆ¶çš„ç»Ÿä¸€æ‰©æ•£æ¨¡å‹",
      "authors": [
        "Haozhuo Zhang",
        "Michele Caprio",
        "Jing Shao",
        "Qiang Zhang",
        "Jian Tang",
        "Shanghang Zhang",
        "Wei Pan"
      ],
      "abstract": "We present PoseDiff, a conditional diffusion model that unifies robot state estimation and control within a single framework. At its core, PoseDiff maps raw visual observations into structured robot states-such as 3D keypoints or joint angles-from a single RGB image, eliminating the need for multi-stage pipelines or auxiliary modalities. Building upon this foundation, PoseDiff extends naturally to video-to-action inverse dynamics: by conditioning on sparse video keyframes generated by world models, it produces smooth and continuous long-horizon action sequences through an overlap-averaging strategy. This unified design enables scalable and efficient integration of perception and control. On the DREAM dataset, PoseDiff achieves state-of-the-art accuracy and real-time performance for pose estimation. On Libero-Object manipulation tasks, it substantially improves success rates over existing inverse dynamics modules, even under strict offline settings. Together, these results show that PoseDiff provides a scalable, accurate, and efficient bridge between perception, planning, and control in embodied AI. The video visualization results can be found on the project page: https://haozhuo-zhang.github.io/PoseDiff-project-page/.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PoseDiffï¼Œä¸€ä¸ªå°†æœºå™¨äººçŠ¶æ€ä¼°è®¡ (robot state estimation) ä¸æ§åˆ¶ (control) ç»Ÿä¸€åœ¨å•ä¸€æ¡†æ¶ä¸‹çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹ (conditional diffusion model)ã€‚PoseDiff èƒ½å¤Ÿç›´æ¥ä»å•å¼  RGB å›¾åƒä¸­å°†åŸå§‹è§†è§‰è§‚å¯Ÿæ˜ å°„ä¸º 3D keypoints æˆ– joint angles ç­‰ç»“æ„åŒ–æœºå™¨äººçŠ¶æ€ï¼Œæ— éœ€å¤šé˜¶æ®µæµæ°´çº¿æˆ–è¾…åŠ©æ¨¡æ€ã€‚è¯¥æ¨¡å‹è¿›ä¸€æ­¥æ‰©å±•åˆ° video-to-action é€†åŠ¨åŠ›å­¦ (inverse dynamics) ä»»åŠ¡ï¼Œé€šè¿‡å¯¹ç”±ä¸–ç•Œæ¨¡å‹ (world models) ç”Ÿæˆçš„ç¨€ç–è§†é¢‘å…³é”®å¸§è¿›è¡Œæ¡ä»¶çº¦æŸï¼Œå¹¶ç»“åˆ overlap-averaging ç­–ç•¥ç”Ÿæˆå¹³æ»‘ä¸”è¿ç»­çš„é•¿ç¨‹åŠ¨ä½œåºåˆ—ã€‚å®éªŒè¡¨æ˜ï¼ŒPoseDiff åœ¨ DREAM æ•°æ®é›†çš„ä½å§¿ä¼°è®¡ (pose estimation) ä»»åŠ¡ä¸­è¾¾åˆ°äº† state-of-the-art ç²¾åº¦å’Œå®æ—¶æ€§èƒ½ï¼Œå¹¶åœ¨ Libero-Object æ“ä½œä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†ç¦»çº¿è®¾ç½®ä¸‹çš„æˆåŠŸç‡ã€‚è¯¥ç ”ç©¶ä¸ºå…·èº«æ™ºèƒ½ (embodied AI) ä¸­çš„æ„ŸçŸ¥ã€è§„åˆ’ä¸æ§åˆ¶ä¹‹é—´æ„å»ºäº†ä¸€ä¸ªå¯æ‰©å±•ã€å‡†ç¡®ä¸”é«˜æ•ˆçš„æ¡¥æ¢ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "The experimental setup and metrics lacks rigor, affecting the fairness of the comparisons",
      "pdf_url": "https://arxiv.org/pdf/2509.24591v2",
      "published_date": "2025-09-29 10:55:48 UTC",
      "updated_date": "2025-10-30 15:48:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:02:01.085556+00:00"
    },
    {
      "arxiv_id": "2509.24569v1",
      "title": "Bandits roaming Hilbert space",
      "title_zh": "å¸Œå°”ä¼¯ç‰¹ç©ºé—´ä¸­çš„ Bandit é—®é¢˜",
      "authors": [
        "Josep Lumbreras"
      ],
      "abstract": "This thesis studies the exploration and exploitation trade-off in online learning of properties of quantum states using multi-armed bandits. Given streaming access to an unknown quantum state, in each round we select an observable from a set of actions to maximize its expectation value. Using past information, we refine actions to minimize regret; the cumulative gap between current reward and the maximum possible. We derive information-theoretic lower bounds and optimal strategies with matching upper bounds, showing regret typically scales as the square root of rounds. As an application, we reframe quantum state tomography to both learn the state efficiently and minimize measurement disturbance. For pure states and continuous actions, we achieve polylogarithmic regret using a sample-optimal algorithm based on a weighted online least squares estimator. The algorithm relies on the optimistic principle and controls the eigenvalues of the design matrix. We also apply our framework to quantum recommender systems and thermodynamic work extraction from unknown states. In this last setting, our results demonstrate an exponential advantage in work dissipation over tomography-based protocols.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤šè‡‚è€è™æœº(Multi-armed Bandits)åœ¨åœ¨çº¿å­¦ä¹ ä¸­å¯¹é‡å­æ€å±æ€§è¿›è¡Œæ¢ç´¢ä¸å¹³è¡¡(Exploration and Exploitation)æƒè¡¡çš„é—®é¢˜ã€‚é€šè¿‡æµå¼è®¿é—®æœªçŸ¥é‡å­æ€ï¼Œç ”ç©¶è€…åœ¨æ¯ä¸€è½®ä¸­é€‰æ‹©ä¸€ä¸ªå¯è§‚æµ‹ç‰©ç†é‡(Observable)ä»¥æœ€å¤§åŒ–å…¶æœŸæœ›å€¼ï¼Œå¹¶åˆ©ç”¨å†å²ä¿¡æ¯ç²¾ç‚¼åŠ¨ä½œä»¥æœ€å°åŒ–ç´¯ç§¯é—æ†¾(Regret)ã€‚ä½œè€…æ¨å¯¼äº†ä¿¡æ¯è®ºä¸‹ç•Œå¹¶æå‡ºäº†ä¸ä¹‹åŒ¹é…çš„æœ€ä½³ç­–ç•¥ï¼Œè¯æ˜äº†é—æ†¾å€¼é€šå¸¸éšè½®æ¬¡çš„å¹³æ–¹æ ¹ç¼©æ”¾ã€‚è¯¥æ¡†æ¶å°†é‡å­æ€æ–­å±‚æ‰«æ(Quantum State Tomography)é‡æ–°å®šä¹‰ä¸ºåœ¨é«˜æ•ˆå­¦ä¹ çŠ¶æ€çš„åŒæ—¶æœ€å°åŒ–æµ‹é‡æ‰°åŠ¨çš„è¿‡ç¨‹ã€‚é’ˆå¯¹çº¯æ€å’Œè¿ç»­åŠ¨ä½œï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºåŠ æƒåœ¨çº¿æœ€å°äºŒä¹˜ä¼°è®¡å™¨(Weighted Online Least Squares Estimator)çš„æ ·æœ¬æœ€ä¼˜ç®—æ³•ï¼Œè¯¥ç®—æ³•éµå¾ªä¹è§‚åŸåˆ™(Optimistic Principle)å¹¶èƒ½æœ‰æ•ˆæ§åˆ¶è®¾è®¡çŸ©é˜µçš„ç‰¹å¾å€¼ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜è¢«åº”ç”¨äºé‡å­æ¨èç³»ç»Ÿå’Œçƒ­åŠ›å­¦åŠŸæå–ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨å‡å°‘åŠŸè€—æ•£(Work Dissipation)æ–¹é¢ï¼Œè¯¥æ–¹æ³•ç›¸è¾ƒäºä¼ ç»Ÿçš„æ–­å±‚æ‰«æåè®®å…·æœ‰æŒ‡æ•°çº§ä¼˜åŠ¿ã€‚",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "quant-ph",
      "comment": "PhD thesis, Centre for Quantum Technologies, National University of Singapore",
      "pdf_url": "https://arxiv.org/pdf/2509.24569v1",
      "published_date": "2025-09-29 10:26:29 UTC",
      "updated_date": "2025-09-29 10:26:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:02:39.300115+00:00"
    },
    {
      "arxiv_id": "2509.24560v1",
      "title": "AdaThink-Med: Medical Adaptive Thinking with Uncertainty-Guided Length Calibration",
      "title_zh": "AdaThink-Medï¼šåŸºäºä¸ç¡®å®šæ€§å¼•å¯¼é•¿åº¦æ ¡å‡†çš„åŒ»å­¦è‡ªé€‚åº”æ€ç»´",
      "authors": [
        "Shaohao Rui",
        "Kaitao Chen",
        "Weijie Ma",
        "Xiaosong Wang"
      ],
      "abstract": "Recent advances in inference time scaling with extended long chain-of thought have significantly improved the reasoning capabilities of both general and medical large language models (LLMs). However, these models tend to engage in lengthy reasoning processes regardless of the difficulty of the input question, leading to increased inference costs in real-world applications. Therefore, enabling adaptive thinking where models think less for simpler questions and think more for complex ones is critical for the effective use of medical LLMs in practice. Despite its importance, there is a lack of end-to-end approaches designed to enhance the adaptive thinking capabilities of medical LLMs while providing a comprehensive examination of the trade-off between performance and computational cost. To bridge this gap, we propose AdaThink-Med, the first end-to-end framework designed to enhance adaptive thinking ability in medical reasoning models with uncertainty-guided length calibration. AdaThink-Med first generates multiple candidate outputs for each question, evaluates the correctness and uncertainty of each candidate, and then estimates problem difficulty via an uncertainty-guided length calibration module. For outputs with low difficulty and correct answers, the framework penalizes longer reasoning paths; whereas for those with high difficulty and incorrect answers, it encourages extending the chain of thought to explore alternative solutions. On six public medical QA benchmarks, AdaThink-Med achieves up to 6.4x length reduction on average while retaining performance with only minimal degradation. Intriguingly, we observe that AdaThink-Med spontaneously develops two distinct reasoning modes, which we characterize as \"non-thinking\" and \"thinking\", demonstrating the model's ability to suppress redundant reasoning processes dynamically.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AdaThink-Medï¼Œè¿™æ˜¯é¦–ä¸ªæ—¨åœ¨å¢å¼ºåŒ»ç–—å¤§è¯­è¨€æ¨¡å‹ (LLMs) è‡ªé€‚åº”æ€è€ƒèƒ½åŠ›çš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ¨ç†æ—¶é—´æ‰©å±•å¸¦æ¥çš„é«˜æ˜‚è®¡ç®—æˆæœ¬ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸ç¡®å®šæ€§å¼•å¯¼çš„é•¿åº¦æ ¡å‡† (Uncertainty-Guided Length Calibration) æ¨¡å—åŠ¨æ€è¯„ä¼°é—®é¢˜éš¾åº¦ï¼Œå¯¹ç®€å•é—®é¢˜å‡å°‘å†—ä½™æ¨ç†ï¼Œè€Œå¯¹å¤æ‚é—®é¢˜åˆ™é¼“åŠ±æ‰©å±•é“¾å¼æ€ç»´ (Chain-of-Thought) ä»¥å¯»æ±‚å‡†ç¡®æ–¹æ¡ˆã€‚åœ¨å…­ä¸ªå…¬å…±åŒ»ç–—é—®ç­”åŸºå‡†æµ‹è¯•ä¸­ï¼ŒAdaThink-Med åœ¨ä¿æŒæ€§èƒ½åŸºæœ¬ä¸å˜çš„åŒæ—¶ï¼Œå®ç°äº†é«˜è¾¾ 6.4 å€çš„å¹³å‡æ¨ç†é•¿åº¦ç¼©å‡ã€‚æ­¤å¤–ï¼Œæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªå‘å½¢æˆäº†â€œéæ€è€ƒâ€å’Œâ€œæ€è€ƒâ€ä¸¤ç§æˆªç„¶ä¸åŒçš„æ¨ç†æ¨¡å¼ï¼Œè¯æ˜äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­å¹³è¡¡æ€§èƒ½ä¸è®¡ç®—æ•ˆç‡çš„å“è¶Šèƒ½åŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24560v1",
      "published_date": "2025-09-29 10:13:55 UTC",
      "updated_date": "2025-09-29 10:13:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:02:19.888438+00:00"
    },
    {
      "arxiv_id": "2509.24556v1",
      "title": "Deep Reinforcement Learning in Action: Real-Time Control of Vortex-Induced Vibrations",
      "title_zh": "æ·±åº¦å¼ºåŒ–å­¦ä¹ å®æˆ˜ï¼šæ¶¡æ¿€æŒ¯åŠ¨çš„å®æ—¶æ§åˆ¶",
      "authors": [
        "Hussam Sababha",
        "Bernat Font",
        "Mohammed Daqaq"
      ],
      "abstract": "This study showcases an experimental deployment of deep reinforcement learning (DRL) for active flow control (AFC) of vortex-induced vibrations (VIV) in a circular cylinder at a high Reynolds number (Re = 3000) using rotary actuation. Departing from prior work that relied on low-Reynolds-number numerical simulations, this research demonstrates real-time control in a challenging experimental setting, successfully addressing practical constraints such as actuator delay. When the learning algorithm is provided with state feedback alone (displacement and velocity of the oscillating cylinder), the DRL agent learns a low-frequency rotary control strategy that achieves up to 80% vibration suppression which leverages the traditional lock-on phenomenon. While this level of suppression is significant, it remains below the performance achieved using high-frequency rotary actuation. The reduction in performance is attributed to actuation delays and can be mitigated by augmenting the learning algorithm with past control actions. This enables the agent to learn a high-frequency rotary control strategy that effectively modifies vortex shedding and achieves over 95% vibration attenuation. These results demonstrate the adaptability of DRL for AFC in real-world experiments and its ability to overcome instrumental limitations such as actuation lag.",
      "tldr_zh": "è¯¥ç ”ç©¶å±•ç¤ºäº†æ·±åº¦å¼ºåŒ–å­¦ä¹  (Deep Reinforcement Learning, DRL) åœ¨é«˜é›·è¯ºæ•° (Re = 3000) ä¸‹åœ†æŸ±ä½“æ¶¡æ¿€æŒ¯åŠ¨ (Vortex-Induced Vibrations, VIV) ä¸»åŠ¨æµæ§ (Active Flow Control, AFC) ä¸­çš„å®éªŒéƒ¨ç½²ã€‚ä¸ä»¥å¾€ä¾èµ–ä½é›·è¯ºæ•°æ•°å€¼æ¨¡æ‹Ÿçš„ç ”ç©¶ä¸åŒï¼Œæœ¬é¡¹ç›®åœ¨çœŸå®çš„å®éªŒç¯å¢ƒä¸­å®ç°äº†å®æ—¶æ§åˆ¶ï¼Œå¹¶æˆåŠŸè§£å†³äº†æ‰§è¡Œå™¨å»¶è¿Ÿ (actuator delay) ç­‰å®é™…å·¥ç¨‹çº¦æŸã€‚ç ”ç©¶å‘ç°ï¼Œå½“ DRL æ™ºèƒ½ä½“ä»…è·å–ä½ç§»ä¸é€Ÿåº¦çŠ¶æ€åé¦ˆæ—¶ï¼Œèƒ½å­¦ä¹ åˆ°å®ç° 80% æŒ¯åŠ¨æŠ‘åˆ¶çš„ä½é¢‘æ§åˆ¶ç­–ç•¥ï¼›è€Œé€šè¿‡å¼•å…¥å†å²æ§åˆ¶åŠ¨ä½œä½œä¸ºè¾“å…¥ï¼Œæ™ºèƒ½ä½“èƒ½æœ‰æ•ˆè¡¥å¿æ‰§è¡Œå»¶è¿Ÿï¼Œä»è€Œå­¦ä¹ åˆ°é«˜é¢‘æ—‹è½¬æ§åˆ¶ç­–ç•¥å¹¶æˆåŠŸæ”¹å˜æ¶¡æ—‹è„±è½ (vortex shedding) æ¨¡å¼ã€‚è¯¥ç­–ç•¥æœ€ç»ˆå®ç°äº†è¶…è¿‡ 95% çš„æŒ¯åŠ¨è¡°å‡ç‡ï¼Œå……åˆ†è¯æ˜äº† DRL åœ¨ç°å®ä¸–ç•Œä¸»åŠ¨æµæ§ä»»åŠ¡ä¸­çš„è‡ªé€‚åº”æ€§ä»¥åŠå…‹æœç¡¬ä»¶é™åˆ¶çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.flu-dyn"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24556v1",
      "published_date": "2025-09-29 10:09:16 UTC",
      "updated_date": "2025-09-29 10:09:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:02:25.495562+00:00"
    },
    {
      "arxiv_id": "2510.01268v4",
      "title": "AdaDetectGPT: Adaptive Detection of LLM-Generated Text with Statistical Guarantees",
      "title_zh": "AdaDetectGPTï¼šå…·æœ‰ç»Ÿè®¡ä¿è¯çš„å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆæ–‡æœ¬è‡ªé€‚åº”æ£€æµ‹",
      "authors": [
        "Hongyi Zhou",
        "Jin Zhu",
        "Pingfan Su",
        "Kai Ye",
        "Ying Yang",
        "Shakeel A O B Gavioli-Akilagun",
        "Chengchun Shi"
      ],
      "abstract": "We study the problem of determining whether a piece of text has been authored by a human or by a large language model (LLM). Existing state of the art logits-based detectors make use of statistics derived from the log-probability of the observed text evaluated using the distribution function of a given source LLM. However, relying solely on log probabilities can be sub-optimal. In response, we introduce AdaDetectGPT -- a novel classifier that adaptively learns a witness function from training data to enhance the performance of logits-based detectors. We provide statistical guarantees on its true positive rate, false positive rate, true negative rate and false negative rate. Extensive numerical studies show AdaDetectGPT nearly uniformly improves the state-of-the-art method in various combination of datasets and LLMs, and the improvement can reach up to 37\\%. A python implementation of our method is available at https://github.com/Mamba413/AdaDetectGPT.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒºåˆ†äººç±»åˆ›ä½œä¸å¤§è¯­è¨€æ¨¡å‹(LLM)ç”Ÿæˆæ–‡æœ¬çš„éš¾é¢˜ï¼Œæå‡ºäº†AdaDetectGPTï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„è‡ªé€‚åº”æ–‡æœ¬æ£€æµ‹åˆ†ç±»å™¨ã€‚ç°æœ‰çš„åŸºäºLogitsçš„æ£€æµ‹å™¨ä¸»è¦ä¾èµ–äºç›®æ ‡æ–‡æœ¬åœ¨æºLLMåˆ†å¸ƒä¸‹çš„å¯¹æ•°æ¦‚ç‡(log-probability)ç»Ÿè®¡é‡ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­è¡¨ç°å¾€å¾€å¹¶éæœ€ä¼˜ã€‚AdaDetectGPTé€šè¿‡ä»è®­ç»ƒæ•°æ®ä¸­è‡ªé€‚åº”åœ°å­¦ä¹ ä¸€ä¸ªè¯äººå‡½æ•°(witness function)ï¼Œæ˜¾è‘—å¢å¼ºäº†ä¼ ç»ŸLogitsæ£€æµ‹å™¨çš„æ€§èƒ½ï¼Œå¹¶ä¸ºå…¶çœŸé˜³æ€§ç‡(TPR)ã€å‡é˜³æ€§ç‡(FPR)ã€çœŸé˜´æ€§ç‡(TNR)å’Œå‡é˜´æ€§ç‡(FNR)æä¾›äº†åšå®çš„ç»Ÿè®¡å­¦ä¿è¯(statistical guarantees)ã€‚å¤§è§„æ¨¡å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAdaDetectGPTåœ¨å„ç§æ•°æ®é›†å’ŒLLMç»„åˆä¸­å‡ ä¹ä¸€è‡´åœ°ä¼˜äºç°æœ‰SOTAæ–¹æ³•ï¼Œæ€§èƒ½æå‡æœ€é«˜å¯è¾¾37%ã€‚ç›®å‰ï¼Œè¯¥ç ”ç©¶çš„Pythonå®ç°å·²åœ¨GitHubå¼€æºï¼Œä¸ºå¤§æ¨¡å‹ç”Ÿæˆå†…å®¹çš„ç²¾å‡†è¯†åˆ«ä¸å®‰å…¨æ²»ç†æä¾›äº†å¼ºæœ‰åŠ›çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by NeurIPS2025",
      "pdf_url": "https://arxiv.org/pdf/2510.01268v4",
      "published_date": "2025-09-29 10:04:35 UTC",
      "updated_date": "2025-12-07 15:28:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:02:33.285937+00:00"
    },
    {
      "arxiv_id": "2509.24552v2",
      "title": "Short window attention enables long-term memorization",
      "title_zh": "çŸ­çª—å£æ³¨æ„åŠ›æœºåˆ¶åŠ©åŠ›å®ç°é•¿æœŸè®°å¿†",
      "authors": [
        "LoÃ¯c Cabannes",
        "Maximilian Beck",
        "Gergely Szilvasy",
        "Matthijs Douze",
        "Maria Lomeli",
        "Jade Copet",
        "Pierre-Emmanuel MazarÃ©",
        "Gabriel Synnaeve",
        "HervÃ© JÃ©gou"
      ],
      "abstract": "Recent works show that hybrid architectures combining sliding window softmax attention layers with linear recurrent neural network (RNN) layers outperform both of these architectures taken separately. However, the impact of the window length and the interplay between softmax attention and linear RNN layers remain under-studied. In this work, we introduce SWAX, a hybrid architecture consisting of sliding-window attention and xLSTM linear RNN layers.\n  A counter-intuitive finding with SWAX is that larger sliding windows do not improve the long-context performance. In fact, short window attention encourages the model to better train the long-term memory of the xLSTM, by relying less on the softmax attention mechanism for long context-retrieval.\n  The issue with small sliding windows is that they are detrimental for short-context tasks, which could be solved with information from moderately larger sliding windows otherwise. Therefore, we train SWAX by stochastically changing the sliding window size, forcing the model to leverage both a longer context window and the xLSTM memory. SWAX trained with stochastic window sizes significantly outperforms regular window attention both on short and long-context problems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SWAXï¼Œä¸€ç§ç»“åˆäº†æ»‘åŠ¨çª—å£æ³¨æ„åŠ› (Sliding-Window Attention) å’Œ xLSTM çº¿æ€§é€’å½’ç¥ç»ç½‘ç»œ (Linear RNN) å±‚çš„æ··åˆæ¶æ„ã€‚ç ”ç©¶äººå‘˜å‘ç°äº†ä¸€ä¸ªåç›´è§‰çš„ç»“è®ºï¼Œå³åœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸­å¢å¤§æ»‘åŠ¨çª—å£å¹¶ä¸èƒ½æå‡æ€§èƒ½ï¼›ç›¸åï¼ŒçŸ­çª—å£æ³¨æ„åŠ›èƒ½ä¿ƒä½¿æ¨¡å‹å‡å°‘å¯¹ Softmax Attention æœºåˆ¶çš„ä¾èµ–ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°è®­ç»ƒ xLSTM çš„é•¿æœŸè®°å¿†ã€‚é’ˆå¯¹æå°çª—å£å¯¹çŸ­ä¸Šä¸‹æ–‡ä»»åŠ¡çš„è´Ÿé¢å½±å“ï¼Œä½œè€…é‡‡ç”¨äº†éšæœºå˜æ¢æ»‘åŠ¨çª—å£å¤§å° (Stochastic Window Size) çš„è®­ç»ƒç­–ç•¥ï¼Œè¿«ä½¿æ¨¡å‹å¹³è¡¡åˆ©ç”¨ä¸Šä¸‹æ–‡çª—å£å’Œ xLSTM å­˜å‚¨ã€‚å®éªŒè¡¨æ˜ï¼Œç»è¿‡éšæœºçª—å£è®­ç»ƒçš„ SWAX åœ¨çŸ­ä¸Šä¸‹æ–‡å’Œé•¿ä¸Šä¸‹æ–‡åœºæ™¯ä¸‹å‡æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„å›ºå®šçª—å£æ¨¡å‹ã€‚è¿™ç§æ–¹æ³•æˆåŠŸå®ç°äº†é«˜æ•ˆçš„é•¿çŸ­æœŸè®°å¿†èåˆï¼Œä¸ºé•¿åºåˆ—å»ºæ¨¡æä¾›äº†æ–°çš„ä¼˜åŒ–æ€è·¯ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24552v2",
      "published_date": "2025-09-29 10:04:12 UTC",
      "updated_date": "2025-12-09 08:50:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:02:44.491779+00:00"
    },
    {
      "arxiv_id": "2509.24528v3",
      "title": "CORE-3D: Context-aware Open-vocabulary Retrieval by Embeddings in 3D",
      "title_zh": "CORE-3Dï¼šåŸºäº 3D åµŒå…¥çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥å¼€æ”¾è¯æ±‡æ£€ç´¢",
      "authors": [
        "Mohamad Amin Mirzaei",
        "Pantea Amoie",
        "Ali Ekhterachian",
        "Matin Mirzababaei",
        "Babak Khalaj"
      ],
      "abstract": "3D scene understanding is fundamental for embodied AI and robotics, supporting reliable perception for interaction and navigation. Recent approaches achieve zero-shot, open-vocabulary 3D semantic mapping by assigning embedding vectors to 2D class-agnostic masks generated via vision-language models (VLMs) and projecting these into 3D. However, these methods often produce fragmented masks and inaccurate semantic assignments due to the direct use of raw masks, limiting their effectiveness in complex environments. To address this, we leverage SemanticSAM with progressive granularity refinement to generate more accurate and numerous object-level masks, mitigating the over-segmentation commonly observed in mask generation models such as vanilla SAM, and improving downstream 3D semantic segmentation. To further enhance semantic context, we employ a context-aware CLIP encoding strategy that integrates multiple contextual views of each mask using empirically determined weighting, providing much richer visual context. We evaluate our approach on multiple 3D scene understanding tasks, including 3D semantic segmentation and object retrieval from language queries, across several benchmark datasets. Experimental results demonstrate significant improvements over existing methods, highlighting the effectiveness of our approach.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CORE-3Dï¼Œæ—¨åœ¨è§£å†³åœ¨3Dåœºæ™¯ç†è§£ä¸­åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)è¿›è¡Œå¼€æ”¾è¯æ±‡(open-vocabulary)è¯­ä¹‰æ˜ å°„æ—¶å‡ºç°çš„æ©ç ç ´ç¢å’Œè¯­ä¹‰åˆ†é…ä¸å‡†ç¡®é—®é¢˜ã€‚ä¸ºäº†æ”¹è¿›ä¼ ç»Ÿçš„æ©ç ç”Ÿæˆï¼ŒCORE-3Dåˆ©ç”¨SemanticSAMç»“åˆæ¸è¿›å¼ç²’åº¦ç»†åŒ–(progressive granularity refinement)æŠ€æœ¯ï¼Œç”Ÿæˆæ›´å‡†ç¡®ä¸”æ•°é‡ä¸°å¯Œçš„ç‰©ä½“çº§æ©ç ï¼Œä»è€Œæœ‰æ•ˆç¼“è§£äº†è¿‡åˆ†å‰²(over-segmentation)ç°è±¡ã€‚åŒæ—¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨äº†ä¸€ç§ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„CLIPç¼–ç ç­–ç•¥(context-aware CLIP encoding strategy)ï¼Œé€šè¿‡ç»éªŒæƒé‡è®¾å®šæ¥æ•´åˆæ¯ä¸ªæ©ç çš„å¤šä¸ªä¸Šä¸‹æ–‡è§†å›¾ï¼Œæä¾›äº†æ›´ä¸°å¯Œçš„è§†è§‰èƒŒæ™¯ã€‚å®éªŒç»“æœåœ¨å¤šä¸ª3Dè¯­ä¹‰åˆ†å‰²å’ŒåŸºäºè¯­è¨€æŸ¥è¯¢çš„ç‰©ä½“æ£€ç´¢åŸºå‡†æ•°æ®é›†ä¸Šè¯æ˜ï¼ŒCORE-3Dç›¸æ¯”ç°æœ‰æ–¹æ³•å…·æœ‰æ˜¾è‘—æå‡ï¼Œå¢å¼ºäº†å…·èº«æ™ºèƒ½ç³»ç»Ÿå¯¹å¤æ‚ç¯å¢ƒçš„æ„ŸçŸ¥èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Submitted for ICLR 2026 conference",
      "pdf_url": "https://arxiv.org/pdf/2509.24528v3",
      "published_date": "2025-09-29 09:43:00 UTC",
      "updated_date": "2025-12-07 23:06:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:02:48.681999+00:00"
    },
    {
      "arxiv_id": "2509.24527v1",
      "title": "Training Agents Inside of Scalable World Models",
      "title_zh": "åœ¨å¯æ‰©å±•ä¸–ç•Œæ¨¡å‹ä¸­è®­ç»ƒæ™ºèƒ½ä½“",
      "authors": [
        "Danijar Hafner",
        "Wilson Yan",
        "Timothy Lillicrap"
      ],
      "abstract": "World models learn general knowledge from videos and simulate experience for training behaviors in imagination, offering a path towards intelligent agents. However, previous world models have been unable to accurately predict object interactions in complex environments. We introduce Dreamer 4, a scalable agent that learns to solve control tasks by reinforcement learning inside of a fast and accurate world model. In the complex video game Minecraft, the world model accurately predicts object interactions and game mechanics, outperforming previous world models by a large margin. The world model achieves real-time interactive inference on a single GPU through a shortcut forcing objective and an efficient transformer architecture. Moreover, the world model learns general action conditioning from only a small amount of data, allowing it to extract the majority of its knowledge from diverse unlabeled videos. We propose the challenge of obtaining diamonds in Minecraft from only offline data, aligning with practical applications such as robotics where learning from environment interaction can be unsafe and slow. This task requires choosing sequences of over 20,000 mouse and keyboard actions from raw pixels. By learning behaviors in imagination, Dreamer 4 is the first agent to obtain diamonds in Minecraft purely from offline data, without environment interaction. Our work provides a scalable recipe for imagination training, marking a step towards intelligent agents.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Dreamer 4ï¼Œè¿™æ˜¯ä¸€ç§åœ¨å¯æ‰©å±•çš„ä¸–ç•Œæ¨¡å‹ (World Models) å†…éƒ¨é€šè¿‡å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) è®­ç»ƒè¡Œä¸ºçš„æ™ºèƒ½ä½“ã€‚ä¸ºäº†å…‹æœä»¥å¾€æ¨¡å‹åœ¨å¤æ‚ç¯å¢ƒä¸­é¢„æµ‹äº¤äº’å‡†ç¡®æ€§ä¸è¶³çš„å±€é™ï¼ŒDreamer 4 é‡‡ç”¨äº†é«˜æ•ˆçš„ Transformer æ¶æ„å’Œ Shortcut Forcing ç›®æ ‡ï¼Œåœ¨å•ä¸ª GPU ä¸Šå®ç°äº†å®æ—¶äº¤äº’å¼æ¨ç†ã€‚è¯¥æ¨¡å‹å…·å¤‡ä»å°‘é‡åŠ¨ä½œæ ‡è®°æ•°æ®ä¸­å­¦ä¹ å¹¶ä»å¤§é‡æ— æ ‡ç­¾è§†é¢‘ä¸­æå–é€šç”¨çŸ¥è¯†çš„èƒ½åŠ›ï¼Œæ˜¾è‘—é™ä½äº†å¯¹ç¯å¢ƒäº¤äº’çš„ä¾èµ–ã€‚åœ¨å¤æ‚çš„ Minecraft å®éªŒä¸­ï¼ŒDreamer 4 æˆåŠŸä»åŸå§‹åƒç´ ä¸­è§„åˆ’å¹¶æ‰§è¡Œäº†è¶…è¿‡ 20,000 æ­¥çš„åŠ¨ä½œåºåˆ—ï¼Œæˆä¸ºé¦–ä¸ªä»…é€šè¿‡ç¦»çº¿æ•°æ® (Offline Data) åœ¨æƒ³è±¡ä¸­å®Œæˆè·å–é’»çŸ³ä»»åŠ¡çš„æ™ºèƒ½ä½“ã€‚è¯¥æˆæœä¸ºåŸºäºæƒ³è±¡è®­ç»ƒ (Imagination Training) çš„å¤§è§„æ¨¡æ™ºèƒ½ä½“å¼€å‘æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„èŒƒå¼ï¼Œæ ‡å¿—ç€å‘æ„å»ºé€šç”¨æ™ºèƒ½æ™ºèƒ½ä½“è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.RO",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "Website: https://danijar.com/dreamer4/",
      "pdf_url": "https://arxiv.org/pdf/2509.24527v1",
      "published_date": "2025-09-29 09:42:27 UTC",
      "updated_date": "2025-09-29 09:42:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:02:55.183772+00:00"
    },
    {
      "arxiv_id": "2509.24526v1",
      "title": "CMT: Mid-Training for Efficient Learning of Consistency, Mean Flow, and Flow Map Models",
      "title_zh": "CMTï¼šé¢å‘ä¸€è‡´æ€§ã€å‡å€¼æµåŠæµæ˜ å°„æ¨¡å‹é«˜æ•ˆå­¦ä¹ çš„ä¸­ç¨‹è®­ç»ƒ",
      "authors": [
        "Zheyuan Hu",
        "Chieh-Hsin Lai",
        "Yuki Mitsufuji",
        "Stefano Ermon"
      ],
      "abstract": "Flow map models such as Consistency Models (CM) and Mean Flow (MF) enable few-step generation by learning the long jump of the ODE solution of diffusion models, yet training remains unstable, sensitive to hyperparameters, and costly. Initializing from a pre-trained diffusion model helps, but still requires converting infinitesimal steps into a long-jump map, leaving instability unresolved. We introduce mid-training, the first concept and practical method that inserts a lightweight intermediate stage between the (diffusion) pre-training and the final flow map training (i.e., post-training) for vision generation. Concretely, Consistency Mid-Training (CMT) is a compact and principled stage that trains a model to map points along a solver trajectory from a pre-trained model, starting from a prior sample, directly to the solver-generated clean sample. It yields a trajectory-consistent and stable initialization. This initializer outperforms random and diffusion-based baselines and enables fast, robust convergence without heuristics. Initializing post-training with CMT weights further simplifies flow map learning. Empirically, CMT achieves state of the art two step FIDs: 1.97 on CIFAR-10, 1.32 on ImageNet 64x64, and 1.84 on ImageNet 512x512, while using up to 98% less training data and GPU time, compared to CMs. On ImageNet 256x256, CMT reaches 1-step FID 3.34 while cutting total training time by about 50% compared to MF from scratch (FID 3.43). This establishes CMT as a principled, efficient, and general framework for training flow map models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CMT (Consistency Mid-Training)ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹Consistency Models (CM)å’ŒMean Flow (MF)ç­‰Flow mapæ¨¡å‹çš„é«˜æ•ˆè®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¿™äº›æ¨¡å‹åœ¨è®­ç»ƒä¸­å­˜åœ¨çš„ä¸ç¨³å®šã€è¶…å‚æ•°æ•æ„ŸåŠæˆæœ¬é«˜æ˜‚ç­‰é—®é¢˜ã€‚CMTé¦–æ¬¡å¼•å…¥äº†ä¸­é˜¶è®­ç»ƒ (mid-training) é˜¶æ®µï¼Œé€šè¿‡åœ¨æ‰©æ•£æ¨¡å‹é¢„è®­ç»ƒä¸æœ€ç»ˆçš„Flow mapåè®­ç»ƒä¹‹é—´æ’å…¥ä¸€ä¸ªè½»é‡çº§è¿‡ç¨‹ï¼Œåˆ©ç”¨æ¨¡å‹å­¦ä¹ å°†é¢„è®­ç»ƒæ±‚è§£å™¨çš„è½¨è¿¹ (solver trajectory) æ˜ å°„ä¸ºå¹²å‡€æ ·æœ¬ã€‚è¿™ç§æ–¹æ³•æä¾›äº†è½¨è¿¹ä¸€è‡´ä¸”ç¨³å®šçš„åˆå§‹åŒ–ï¼Œä½¿å…¶åœ¨æ”¶æ•›é€Ÿåº¦å’Œç¨³å¥æ€§ä¸Šå‡ä¼˜äºéšæœºåˆå§‹åŒ–æˆ–æ‰©æ•£åŸºçº¿ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCMTåœ¨CIFAR-10å’ŒImageNetç­‰æ•°æ®é›†ä¸Šå®ç°äº†ä¸¤æ­¥ç”Ÿæˆçš„state of the art FIDè¡¨ç°ã€‚ç›¸æ¯”ä¼ ç»Ÿçš„CMæ¨¡å‹ï¼ŒCMTåœ¨è¾¾åˆ°å“è¶Šæ€§èƒ½çš„åŒæ—¶ï¼Œå‡å°‘äº†é«˜è¾¾98%çš„è®­ç»ƒæ•°æ®å’ŒGPUè€—æ—¶ï¼Œå¹¶æ˜¾è‘—ç¼©çŸ­äº†æ•´ä½“è®­ç»ƒå‘¨æœŸã€‚æ€»çš„æ¥çœ‹ï¼ŒCMTä¸ºFlow mapæ¨¡å‹çš„è®­ç»ƒæä¾›äº†ä¸€ä¸ªåŸåˆ™æ€§å¼ºã€é«˜æ•ˆä¸”é€šç”¨çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint",
      "pdf_url": "https://arxiv.org/pdf/2509.24526v1",
      "published_date": "2025-09-29 09:42:08 UTC",
      "updated_date": "2025-09-29 09:42:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:03:00.496443+00:00"
    },
    {
      "arxiv_id": "2509.24524v1",
      "title": "PhysiAgent: An Embodied Agent Framework in Physical World",
      "title_zh": "PhysiAgentï¼šé¢å‘ç‰©ç†ä¸–ç•Œçš„å…·èº«æ™ºèƒ½ä½“æ¡†æ¶",
      "authors": [
        "Zhihao Wang",
        "Jianxiong Li",
        "Jinliang Zheng",
        "Wencong Zhang",
        "Dongxiu Liu",
        "Yinan Zheng",
        "Haoyi Niu",
        "Junzhi Yu",
        "Xianyuan Zhan"
      ],
      "abstract": "Vision-Language-Action (VLA) models have achieved notable success but often struggle with limited generalizations. To address this, integrating generalized Vision-Language Models (VLMs) as assistants to VLAs has emerged as a popular solution. However, current approaches often combine these models in rigid, sequential structures: using VLMs primarily for high-level scene understanding and task planning, and VLAs merely as executors of lower-level actions, leading to ineffective collaboration and poor grounding challenges. In this paper, we propose an embodied agent framework, PhysiAgent, tailored to operate effectively in physical environments. By incorporating monitor, memory, self-reflection mechanisms, and lightweight off-the-shelf toolboxes, PhysiAgent offers an autonomous scaffolding framework to prompt VLMs to organize different components based on real-time proficiency feedback from VLAs to maximally exploit VLAs' capabilities. Experimental results demonstrate significant improvements in task-solving performance on complex real-world robotic tasks, showcasing effective self-regulation of VLMs, coherent tool collaboration, and adaptive evolution of the framework during execution. PhysiAgent makes practical and pioneering efforts to integrate VLMs and VLAs, effectively grounding embodied agent frameworks in real-world settings.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†PhysiAgentï¼Œä¸€ä¸ªä¸“ä¸ºç‰©ç†ç¯å¢ƒè®¾è®¡çš„å…·èº«æ™ºèƒ½ä½“(Embodied Agent)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)ä¸è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹(VLAs)é›†æˆæ—¶å­˜åœ¨çš„åä½œä½æ•ˆä¸æ¥åœ°(Grounding)éš¾é¢˜ã€‚PhysiAgenté€šè¿‡æ•´åˆç›‘æ§(Monitor)ã€è®°å¿†(Memory)ã€è‡ªæˆ‘åæ€(Self-reflection)æœºåˆ¶åŠè½»é‡åŒ–å·¥å…·ç®±ï¼Œæ„å»ºäº†ä¸€ä¸ªè‡ªä¸»è„šæ‰‹æ¶æ¶æ„ã€‚è¯¥æ¶æ„èƒ½æ ¹æ®VLAsçš„å®æ—¶ç†Ÿç»ƒåº¦åé¦ˆä¿ƒä½¿VLMsåŠ¨æ€ç»„ç»‡å„ç»„ä»¶ï¼Œä»è€Œæœ€å¤§åŒ–åˆ©ç”¨VLAsçš„æ‰§è¡Œèƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPhysiAgentåœ¨å¤æ‚çš„çœŸå®æœºå™¨äººä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œå¹¶å±•ç°å‡ºä¼˜å¼‚çš„è‡ªæˆ‘è°ƒèŠ‚ã€å·¥å…·åä½œåŠæ‰§è¡Œä¸­çš„è‡ªé€‚åº”æ¼”è¿›èƒ½åŠ›ã€‚è¯¥å·¥ä½œä¸ºVLMså’ŒVLAsçš„æœ‰æ•ˆèåˆæä¾›äº†åˆ›æ–°è·¯å¾„ï¼ŒæˆåŠŸæ¨åŠ¨äº†å…·èº«æ™ºèƒ½ä½“æ¡†æ¶åœ¨ç°å®åœºæ™¯ä¸­çš„åº”ç”¨è½åœ°ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24524v1",
      "published_date": "2025-09-29 09:39:32 UTC",
      "updated_date": "2025-09-29 09:39:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:03:07.390545+00:00"
    },
    {
      "arxiv_id": "2509.24515v1",
      "title": "Agentic Specification Generator for Move Programs",
      "title_zh": "é¢å‘ Move ç¨‹åºçš„æ™ºèƒ½ä½“è§„èŒƒç”Ÿæˆå™¨",
      "authors": [
        "Yu-Fu Fu",
        "Meng Xu",
        "Taesoo Kim"
      ],
      "abstract": "While LLM-based specification generation is gaining traction, existing tools primarily focus on mainstream programming languages like C, Java, and even Solidity, leaving emerging and yet verification-oriented languages like Move underexplored. In this paper, we introduce MSG, an automated specification generation tool designed for Move smart contracts. MSG aims to highlight key insights that uniquely present when applying LLM-based specification generation to a new ecosystem. Specifically, MSG demonstrates that LLMs exhibit robust code comprehension and generation capabilities even for non-mainstream languages. MSG successfully generates verifiable specifications for 84% of tested Move functions and even identifies clauses previously overlooked by experts. Additionally, MSG shows that explicitly leveraging specification language features through an agentic, modular design improves specification quality substantially (generating 57% more verifiable clauses than conventional designs). Incorporating feedback from the verification toolchain further enhances the effectiveness of MSG, leading to a 30% increase in generated verifiable specifications.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†MSGï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨ä¸ºMoveæ™ºèƒ½åˆçº¦(smart contracts)è®¾è®¡çš„è‡ªåŠ¨è§„èŒƒç”Ÿæˆ(automated specification generation)å·¥å…·ï¼Œå¡«è¡¥äº†è¿™ä¸€éªŒè¯å¯¼å‘è¯­è¨€åœ¨å¤§å‹è¯­è¨€æ¨¡å‹(LLM)åº”ç”¨é¢†åŸŸçš„ç©ºç™½ã€‚MSGå±•ç¤ºäº†LLMåœ¨éä¸»æµç¼–ç¨‹è¯­è¨€ä¸­ä¾ç„¶å…·å¤‡å¼ºå¤§çš„ä»£ç ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œèƒ½ä¸º84%çš„æµ‹è¯•å‡½æ•°ç”Ÿæˆå¯éªŒè¯è§„èŒƒ(verifiable specifications)ï¼Œç”šè‡³è¯†åˆ«å‡ºä¸“å®¶æ›¾å¿½ç•¥çš„è§„èŒƒæ¡æ¬¾ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡é‡‡ç”¨åŸºäºæ™ºèƒ½ä½“(agentic)çš„æ¨¡å—åŒ–è®¾è®¡ï¼ŒMSGç”Ÿæˆçš„å¯éªŒè¯å­å¥æ•°é‡æ¯”ä¼ ç»Ÿè®¾è®¡æå‡äº†57%ã€‚æ­¤å¤–ï¼ŒMSGé€šè¿‡æ•´åˆæ¥è‡ªéªŒè¯å·¥å…·é“¾(verification toolchain)çš„åé¦ˆæœºåˆ¶ï¼Œä½¿å…¶ç”Ÿæˆè§„èŒƒçš„æœ‰æ•ˆæ€§è¿›ä¸€æ­¥æå‡äº†30%ã€‚è¿™é¡¹å·¥ä½œä¸ä»…éªŒè¯äº†LLMåœ¨ç‰¹å®šç”Ÿæ€ç³»ç»Ÿä¸­çš„è·¨è¯­è¨€è¿ç§»èƒ½åŠ›ï¼Œä¹Ÿä¸ºæé«˜Moveç¨‹åºçš„å®‰å…¨æ€§å’Œè‡ªåŠ¨åŒ–éªŒè¯æ°´å¹³æä¾›äº†æœ‰æ•ˆé€”å¾„ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CR",
        "cs.PL"
      ],
      "primary_category": "cs.SE",
      "comment": "18 pages; Extended version of ASE'25 paper with extra appendices",
      "pdf_url": "https://arxiv.org/pdf/2509.24515v1",
      "published_date": "2025-09-29 09:34:31 UTC",
      "updated_date": "2025-09-29 09:34:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:03:10.895813+00:00"
    },
    {
      "arxiv_id": "2509.24510v3",
      "title": "Specialization after Generalization: Towards Understanding Test-Time Training in Foundation Models",
      "title_zh": "å…ˆæ³›åŒ–åç‰¹åŒ–ï¼šæ·±å…¥ç†è§£åŸºç¡€æ¨¡å‹ä¸­çš„æµ‹è¯•æ—¶è®­ç»ƒ",
      "authors": [
        "Jonas HÃ¼botter",
        "Patrik Wolf",
        "Alexander Shevchenko",
        "Dennis JÃ¼ni",
        "Andreas Krause",
        "Gil Kur"
      ],
      "abstract": "Recent empirical studies have explored the idea of continuing to train a model at test-time for a given task, known as test-time training (TTT), and have found it to yield significant performance improvements. However, there is limited understanding of why and when TTT is effective. Earlier explanations mostly focused on the observation that TTT may help when applied to out-of-distribution adaptation or used with privileged data. However, the growing scale of foundation models with most test data being in-distribution questions these explanations. We instead posit that foundation models remain globally underparameterized, with TTT providing a mechanism for specialization after generalization, focusing capacity on concepts relevant to the test task. Specifically, under the linear representation hypothesis, we propose a model in which TTT achieves a substantially smaller in-distribution test error than global training. We empirically validate our model's key assumptions by training a sparse autoencoder on ImageNet, showing that semantically related data points are explained by only a few shared concepts. Finally, we perform scaling studies across image and language tasks that confirm the practical implications of our model, identifying the regimes where specialization is most effective.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æµ‹è¯•æ—¶è®­ç»ƒ (Test-Time Training, TTT) åœ¨åŸºç¡€æ¨¡å‹ (Foundation Models) ä¸­çš„æœ‰æ•ˆæ€§ï¼Œæå‡ºäº†â€œæ³›åŒ–åä¸“ä¸šåŒ–â€ (Specialization after Generalization) çš„æ ¸å¿ƒè§‚ç‚¹ã€‚ä½œè€…è®¤ä¸ºåŸºç¡€æ¨¡å‹åœ¨å…¨å±€å±‚é¢ä»å¤„äºå‚æ•°ä¸è¶³çš„çŠ¶æ€ï¼Œè€Œ TTT èƒ½å¤Ÿå°†æ¨¡å‹å®¹é‡èšç„¦äºä¸ç‰¹å®šæµ‹è¯•ä»»åŠ¡ç›¸å…³çš„æ¦‚å¿µä¸Šã€‚åŸºäºçº¿æ€§è¡¨ç¤ºå‡è®¾ (Linear Representation Hypothesis)ï¼Œè¯¥ç ”ç©¶è¯æ˜äº† TTT åœ¨åˆ†å¸ƒå†… (In-distribution) æµ‹è¯•ä¸­æ¯”å…¨å±€è®­ç»ƒå…·æœ‰æ›´ä½çš„è¯¯å·®ã€‚é€šè¿‡åœ¨ ImageNet ä¸Šä½¿ç”¨ç¨€ç–è‡ªç¼–ç å™¨ (Sparse Autoencoder) è¿›è¡Œå®éªŒï¼Œç ”ç©¶è¯å®äº†è¯­ä¹‰ç›¸å…³çš„æ•°æ®ç‚¹ä»…ç”±å°‘æ•°å…±äº«æ¦‚å¿µè§£é‡Šã€‚æœ€åï¼Œé€šè¿‡è·¨å›¾åƒå’Œè¯­è¨€ä»»åŠ¡çš„è§„æ¨¡åŒ–ç ”ç©¶ (Scaling Studies)ï¼Œè¯¥å›¢é˜Ÿæ˜ç¡®äº†ä¸“ä¸šåŒ–æœºåˆ¶åœ¨ä¸åŒåˆ¶åº¦ä¸‹çš„å®é™…åº”ç”¨æ•ˆæœã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Oral at CCFM @ NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.24510v3",
      "published_date": "2025-09-29 09:24:52 UTC",
      "updated_date": "2025-12-10 23:29:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:03:31.887683+00:00"
    },
    {
      "arxiv_id": "2509.24509v2",
      "title": "Experience-Guided Reflective Co-Evolution of Prompts and Heuristics for Automatic Algorithm Design",
      "title_zh": "é¢å‘è‡ªåŠ¨ç®—æ³•è®¾è®¡çš„æç¤ºè¯ä¸å¯å‘å¼ç®—æ³•ç»éªŒå¼•å¯¼åæ€å¼ååŒæ¼”åŒ–",
      "authors": [
        "Yihong Liu",
        "Junyi Li",
        "Wayne Xin Zhao",
        "Hongyu Lu",
        "Ji-Rong Wen"
      ],
      "abstract": "Combinatorial optimization problems are traditionally tackled with handcrafted heuristic algorithms, which demand extensive domain expertise and significant implementation effort. Recent progress has highlighted the potential of automatic heuristics design powered by large language models (LLMs), enabling the automatic generation and refinement of heuristics. These approaches typically maintain a population of heuristics and employ LLMs as mutation operators to evolve them across generations. While effective, such methods often risk stagnating in local optima. To address this issue, we propose the Experience-Guided Reflective Co-Evolution of Prompt and Heuristics (EvoPH) for automatic algorithm design, a novel framework that integrates the island migration model with the elites selection algorithm to simulate diverse heuristics populations. In EvoPH, prompts are co-evolved with heuristic algorithms, guided by performance feedback. We evaluate our framework on two problems, i.e., Traveling Salesman Problem and Bin Packing Problem. Experimental results demonstrate that EvoPH achieves the lowest relative error against optimal solutions across both datasets, advancing the field of automatic algorithm design with LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†EvoPHï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºè‡ªåŠ¨ç®—æ³•è®¾è®¡çš„ç»éªŒå¼•å¯¼åæ€æ€§æç¤ºè¯ä¸å¯å‘å¼ç®—æ³•ååŒæ¼”åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„è‡ªåŠ¨åŒ–è®¾è®¡æ–¹æ³•å®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜çš„é—®é¢˜ã€‚EvoPHé€šè¿‡é›†æˆå²›å±¿è¿ç§»æ¨¡å‹(Island Migration Model)ä¸ç²¾è‹±é€‰æ‹©ç®—æ³•(Elites Selection Algorithm)æ¥ç»´æŒå¯å‘å¼ç®—æ³•ç§ç¾¤çš„å¤šæ ·æ€§ã€‚åœ¨æ¼”åŒ–è¿‡ç¨‹ä¸­ï¼Œæç¤ºè¯(Prompts)ä¸å¯å‘å¼ç®—æ³•æ ¹æ®æ€§èƒ½åé¦ˆè¿›è¡Œå…±æ¼”åŒ–ï¼Œä»è€Œä¸æ–­ä¼˜åŒ–ç”Ÿæˆç»“æœã€‚å®éªŒåœ¨æ—…è¡Œå•†é—®é¢˜(Traveling Salesman Problem)å’Œè£…ç®±é—®é¢˜(Bin Packing Problem)ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œç»“æœæ˜¾ç¤ºEvoPHåœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šå‡å–å¾—äº†ç›¸å¯¹äºæœ€ä¼˜è§£çš„æœ€ä½ç›¸å¯¹è¯¯å·®ã€‚è¯¥æˆæœä¸ºåˆ©ç”¨LLMsè¿›è¡Œè‡ªåŠ¨ç®—æ³•è®¾è®¡æä¾›äº†æ–°çš„ç ”ç©¶èŒƒå¼ï¼Œæ˜¾è‘—æå‡äº†è‡ªåŠ¨åŒ–å¯å‘å¼ç®—æ³•ç”Ÿæˆçš„æ€§èƒ½ä¸æ•ˆç‡ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24509v2",
      "published_date": "2025-09-29 09:24:09 UTC",
      "updated_date": "2025-09-30 07:56:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:03:34.072236+00:00"
    },
    {
      "arxiv_id": "2509.25283v1",
      "title": "Effectiveness of Large Language Models in Simulating Regional Psychological Structures: An Empirical Examination of Personality and Subjective Well-being",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹æ¨¡æ‹ŸåŒºåŸŸå¿ƒç†ç»“æ„çš„æœ‰æ•ˆæ€§ï¼šäººæ ¼ä¸ä¸»è§‚å¹¸ç¦æ„Ÿçš„å®è¯æ£€éªŒ",
      "authors": [
        "Ke Luoma",
        "Li Zengyi",
        "Liao Jiangqun",
        "Tong Song",
        "Peng Kaiping"
      ],
      "abstract": "This study examines whether LLMs can simulate culturally grounded psychological patterns based on demographic information. Using DeepSeek, we generated 2943 virtual participants matched to demographic distributions from the CFPS2018 and compared them with human responses on the Big Five personality traits and subjective well-being across seven Chinese regions.Personality was measured using a 15-item Chinese Big Five inventory, and happiness with a single-item rating. Results revealed broad similarity between real and simulated datasets, particularly in regional variation trends. However, systematic differences emerged:simulated participants scored lower in extraversion and openness, higher in agreeableness and neuroticism, and consistently reported lower happiness. Predictive structures also diverged: while human data identified conscientiousness, extraversion and openness as positive predictors of happiness, the AI emphasized openness and agreeableness, with extraversion predicting negatively. These discrepancies suggest that while LLMs can approximate population-level psychological distributions, they underrepresent culturally specific and affective dimensions. The findings highlight both the potential and limitations of LLM-based virtual participants for large-scale psychological research and underscore the need for culturally enriched training data and improved affective modeling.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Large Language Models (LLMs) åœ¨æ ¹æ®äººå£ç»Ÿè®¡ä¿¡æ¯æ¨¡æ‹Ÿç‰¹å®šæ–‡åŒ–å¿ƒç†æ¨¡å¼æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡ DeepSeek ç”Ÿæˆäº†ä¸ CFPS2018 äººå£åˆ†å¸ƒåŒ¹é…çš„ 2943 åè™šæ‹Ÿå‚ä¸è€…ï¼Œå¹¶å¯¹æ¯”äº†ä¸­å›½ä¸ƒä¸ªåœ°åŒºåœ¨ Big Five personality traits å’Œ subjective well-being ä¸Šçš„è¡¨ç°ã€‚ç»“æœè¡¨æ˜ï¼Œæ¨¡æ‹Ÿæ•°æ®ä¸çœŸå®æ ·æœ¬åœ¨åŒºåŸŸå˜åŒ–è¶‹åŠ¿ä¸Šå…·æœ‰ç›¸ä¼¼æ€§ï¼Œä½†å­˜åœ¨æ˜æ˜¾çš„ç³»ç»Ÿæ€§åå·®ï¼šæ¨¡æ‹Ÿå‚ä¸è€…çš„ Extraversionã€Openness å’Œå¹¸ç¦æ„Ÿè¯„åˆ†åä½ï¼Œè€Œ Agreeableness å’Œ Neuroticism è¯„åˆ†åé«˜ã€‚åœ¨å¹¸ç¦æ„Ÿé¢„æµ‹ç»“æ„ä¸Šä¸¤è€…ä¹Ÿå­˜åœ¨åˆ†æ­§ï¼ŒçœŸå®æ•°æ®å¼ºè°ƒ Conscientiousness å’Œ Extraversion çš„æ­£å‘ä½œç”¨ï¼Œè€Œ AI åˆ™ä¾§é‡äº Openness å’Œ Agreeablenessã€‚è¿™äº›å·®å¼‚è¡¨æ˜ LLMs è™½ç„¶èƒ½åˆæ­¥è¿‘ä¼¼äººå£å±‚é¢çš„å¿ƒç†åˆ†å¸ƒï¼Œä½†åœ¨è¡¨å¾æ–‡åŒ–ç‰¹å¼‚æ€§å’Œæƒ…æ„Ÿç»´åº¦æ–¹é¢ä»å­˜åœ¨å±€é™ã€‚è¯¥å‘ç°ä¸ºåˆ©ç”¨ LLMs è¿›è¡Œå¤§è§„æ¨¡å¿ƒç†å­¦ç ”ç©¶æä¾›äº†å‚è€ƒï¼Œå¹¶å¼ºè°ƒäº†å¢å¼ºè®­ç»ƒæ•°æ®æ–‡åŒ–å¤šæ ·æ€§ä¸æ”¹è¿›æƒ…æ„Ÿå»ºæ¨¡çš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25283v1",
      "published_date": "2025-09-29 09:12:18 UTC",
      "updated_date": "2025-09-29 09:12:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:03:34.360683+00:00"
    },
    {
      "arxiv_id": "2509.24496v1",
      "title": "LLM DNA: Tracing Model Evolution via Functional Representations",
      "title_zh": "LLM DNAï¼šåŸºäºåŠŸèƒ½è¡¨å¾çš„æ¨¡å‹æ¼”åŒ–æº¯æº",
      "authors": [
        "Zhaomin Wu",
        "Haodong Zhao",
        "Ziyang Wang",
        "Jizhou Guo",
        "Qian Wang",
        "Bingsheng He"
      ],
      "abstract": "The explosive growth of large language models (LLMs) has created a vast but opaque landscape: millions of models exist, yet their evolutionary relationships through fine-tuning, distillation, or adaptation are often undocumented or unclear, complicating LLM management. Existing methods are limited by task specificity, fixed model sets, or strict assumptions about tokenizers or architectures. Inspired by biological DNA, we address these limitations by mathematically defining LLM DNA as a low-dimensional, bi-Lipschitz representation of functional behavior. We prove that LLM DNA satisfies inheritance and genetic determinism properties and establish the existence of DNA. Building on this theory, we derive a general, scalable, training-free pipeline for DNA extraction. In experiments across 305 LLMs, DNA aligns with prior studies on limited subsets and achieves superior or competitive performance on specific tasks. Beyond these tasks, DNA comparisons uncover previously undocumented relationships among LLMs. We further construct the evolutionary tree of LLMs using phylogenetic algorithms, which align with shifts from encoder-decoder to decoder-only architectures, reflect temporal progression, and reveal distinct evolutionary speeds across LLM families.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LLM DNA çš„æ¦‚å¿µï¼Œè¿™æ˜¯ä¸€ç§ä½ç»´åº¦çš„ã€åŒ Lipschitz (bi-Lipschitz) çš„åŠŸèƒ½è¡Œä¸ºè¡¨å¾ï¼Œæ—¨åœ¨è§£å†³å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ (LLMs) ä¹‹é—´æ¼”åŒ–å…³ç³»ï¼ˆå¦‚å¾®è°ƒã€è’¸é¦æˆ–è‡ªé€‚åº”ï¼‰ä¸é€æ˜ä¸”ç¼ºä¹è®°å½•çš„é—®é¢˜ã€‚å—åˆ°ç”Ÿç‰© DNA çš„å¯å‘ï¼Œä½œè€…åœ¨æ•°å­¦ä¸Šå®šä¹‰äº† LLM DNA å¹¶è¯æ˜å…¶æ»¡è¶³é—ä¼ æ€§ (inheritance) å’Œé—ä¼ å†³å®šè®º (genetic determinism) å±æ€§ï¼Œç¡®ç«‹äº†å…¶ç†è®ºåŸºç¡€ã€‚åŸºäºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€å¥—é€šç”¨ã€å¯æ‰©å±•ä¸”æ— éœ€è®­ç»ƒ (training-free) çš„ DNA æå–æµç¨‹ï¼Œå¯å¹¿æ³›åº”ç”¨äºä¸åŒåˆ†è¯å™¨ (tokenizers) å’Œæ¶æ„çš„æ¨¡å‹ã€‚åœ¨é’ˆå¯¹ 305 ä¸ª LLMs çš„å¤§è§„æ¨¡å®éªŒä¸­ï¼ŒLLM DNA ä¸ä»…éªŒè¯äº†å·²çŸ¥çš„æ¨¡å‹å­é›†å…³ç³»ï¼Œè¿˜æ­ç¤ºäº†æ­¤å‰æœªè¢«è®°å½•çš„æ¼”åŒ–å…³è”ã€‚é€šè¿‡åº”ç”¨ç³»ç»Ÿå‘è‚²ç®—æ³• (phylogenetic algorithms) æ„å»ºçš„ LLMs æ¼”åŒ–æ ‘ï¼Œå‡†ç¡®åæ˜ äº†ä»ç¼–ç å™¨-è§£ç å™¨ (encoder-decoder) å‘ä»…è§£ç å™¨ (decoder-only) æ¶æ„çš„æ¼”è¿›ã€æ—¶é—´åºåˆ—è¿›åº¦ä»¥åŠå„æ¨¡å‹å®¶æ—ä¹‹é—´ä¸åŒçš„æ¼”åŒ–é€Ÿåº¦ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24496v1",
      "published_date": "2025-09-29 09:09:57 UTC",
      "updated_date": "2025-09-29 09:09:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:03:38.749894+00:00"
    },
    {
      "arxiv_id": "2509.24495v1",
      "title": "Neuroplasticity-inspired dynamic ANNs for multi-task demand forecasting",
      "title_zh": "é¢å‘å¤šä»»åŠ¡éœ€æ±‚é¢„æµ‹çš„å—ç¥ç»å¯å¡‘æ€§å¯å‘çš„åŠ¨æ€äººå·¥ç¥ç»ç½‘ç»œ",
      "authors": [
        "Mateusz Å»arski",
        "SÅ‚awomir Nowaczyk"
      ],
      "abstract": "This paper introduces a novel approach to Dynamic Artificial Neural Networks (D-ANNs) for multi-task demand forecasting called Neuroplastic Multi-Task Network (NMT-Net). Unlike conventional methods focusing on inference-time dynamics or computational efficiency, our proposed method enables structural adaptability of the computational graph during training, inspired by neuroplasticity as seen in biological systems. Each new task triggers a dynamic network adaptation, including similarity-based task identification and selective training of candidate ANN heads, which are then assessed and integrated into the model based on their performance. We evaluated our framework using three real-world multi-task demand forecasting datasets from Kaggle. We demonstrated its superior performance and consistency, achieving lower RMSE and standard deviation compared to traditional baselines and state-of-the-art multi-task learning methods. NMT-Net offers a scalable, adaptable solution for multi-task and continual learning in time series prediction. The complete code for NMT-Net is available from our GitHub repository.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºNeuroplastic Multi-Task Network (NMT-Net) çš„åŠ¨æ€äººå·¥ç¥ç»ç½‘ç»œ (D-ANNs) æ–°æ–¹æ³•ï¼Œä¸“é—¨ç”¨äºå¤šä»»åŠ¡éœ€æ±‚é¢„æµ‹ã€‚ä¸ä¾§é‡æ¨ç†é˜¶æ®µåŠ¨æ€æ€§æˆ–è®¡ç®—æ•ˆç‡çš„ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒNMT-Net å—ç”Ÿç‰©ç¥ç»å¯å¡‘æ€§å¯å‘ï¼Œå®ç°äº†è®­ç»ƒè¿‡ç¨‹ä¸­è®¡ç®—å›¾çš„ç»“æ„è‡ªé€‚åº”ã€‚æ¯é¡¹æ–°ä»»åŠ¡éƒ½ä¼šè§¦å‘åŠ¨æ€ç½‘ç»œè°ƒæ•´ï¼ŒåŒ…æ‹¬åŸºäºç›¸ä¼¼æ€§çš„ä»»åŠ¡è¯†åˆ«å’Œå€™é€‰ç¥ç»ç½‘ç»œå¤´ (ANN heads) çš„é€‰æ‹©æ€§è®­ç»ƒï¼Œå¹¶æ ¹æ®å…¶å®é™…æ€§èƒ½è¿›è¡Œè¯„ä¼°ä¸é›†æˆã€‚åœ¨ä¸‰ä¸ª Kaggle çœŸå®æ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å‡æ–¹æ ¹è¯¯å·® (RMSE) å’Œæ ‡å‡†å·®æ–¹é¢å‡æ˜¾è‘—ä¼˜äºä¼ ç»ŸåŸºå‡†åŠæœ€å…ˆè¿›çš„å¤šä»»åŠ¡å­¦ä¹ æ–¹æ³•ã€‚è¯¥ç ”ç©¶è¯æ˜ NMT-Net ä¸ºæ—¶é—´åºåˆ—é¢„æµ‹é¢†åŸŸçš„å¤šä»»åŠ¡å¤„ç†ä¸æŒç»­å­¦ä¹  (continual learning) æä¾›äº†ä¸€ç§å…·å¤‡é«˜æ‰©å±•æ€§å’Œé€‚åº”æ€§çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "14 pages, 3 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.24495v1",
      "published_date": "2025-09-29 09:08:08 UTC",
      "updated_date": "2025-09-29 09:08:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:04:12.871494+00:00"
    },
    {
      "arxiv_id": "2509.24491v1",
      "title": "Mitigating Visual Hallucinations via Semantic Curriculum Preference Optimization in MLLMs",
      "title_zh": "åŸºäºè¯­ä¹‰è¯¾ç¨‹åå¥½ä¼˜åŒ–ç¼“è§£å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„è§†è§‰å¹»è§‰",
      "authors": [
        "Yuanshuai Li",
        "Yuping Yan",
        "Junfeng Tang",
        "Yunxuan Li",
        "Zeqi Zheng",
        "Yaochu Jin"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have significantly improved the performance of various tasks, but continue to suffer from visual hallucinations, a critical issue where generated responses contradict visual evidence. While Direct Preference Optimization(DPO) is widely used for alignment, its application to MLLMs often fails to capture fine-grained semantic differences and encourages shortcut learning. To address these challenges, we propose Semantic Curriculum Preference Optimization (SCPO), a novel framework for MLLM alignment. SCPO employs a progressive, easy-to-hard curriculum built upon our Semantic Curriculum Preference Pairs dataset, which provides fine-grained semantic contrasts sorted by difficulty. This curriculum is trained with a dynamic reference model and a novel symmetric, bidirectional objective to facilitate simultaneous learning from both textual and visual preferences. To our knowledge, SCPO is the first framework to unify semantics, symmetry, and curriculum for MLLMs alignment, effectively mitigating visual hallucinations. Extensive experiments on LLaVA models across various scales and versions validate that SCPO demonstrates superior performance compared to baseline models on multiple hallucination benchmarks, reducing the hallucination rate by up to 62.9%. Moreover, evaluations on generalized benchmarks show that SCPO improves factuality while preserving general capabilities, with its performance remaining stable across general vision-language benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)ä¸­çš„è§†è§‰å¹»è§‰é—®é¢˜ï¼Œæå‡ºäº†åä¸ºSemantic Curriculum Preference Optimization (SCPO)çš„æ–°å‹å¯¹é½æ¡†æ¶ã€‚é’ˆå¯¹Direct Preference Optimization (DPO)åœ¨æ•æ‰ç»†ç²’åº¦è¯­ä¹‰å·®å¼‚æ–¹é¢çš„ä¸è¶³åŠå¿«æ·å­¦ä¹ é—®é¢˜ï¼ŒSCPOåˆ©ç”¨Semantic Curriculum Preference Pairsæ•°æ®é›†æ„å»ºäº†ç”±æ˜“åˆ°éš¾çš„æ¸è¿›å¼è¯¾ç¨‹ã€‚è¯¥æ¡†æ¶é€šè¿‡åŠ¨æ€å‚è€ƒæ¨¡å‹å’Œå¯¹ç§°çš„åŒå‘ç›®æ ‡å‡½æ•°ï¼ŒåŒæ—¶ä»æ–‡æœ¬å’Œè§†è§‰åå¥½ä¸­æå–ç‰¹å¾ï¼Œæ˜¯é¦–ä¸ªå°†è¯­ä¹‰ã€å¯¹ç§°æ€§å’Œè¯¾ç¨‹å­¦ä¹ ç»Ÿä¸€äºMLLMå¯¹é½çš„æ¡†æ¶ã€‚åœ¨LLaVAæ¨¡å‹ä¸Šçš„å®éªŒè¯æ˜ï¼ŒSCPOåœ¨å¤šä¸ªå¹»è§‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæœ€é«˜å°†å¹»è§‰ç‡é™ä½äº†62.9%ã€‚æ­¤å¤–ï¼ŒSCPOåœ¨æå‡æ¨¡å‹äº‹å®æ€§çš„åŒæ—¶ï¼ŒæˆåŠŸä¿æŒäº†å…¶åœ¨é€šç”¨è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­çš„åŸæœ‰æ€§èƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24491v1",
      "published_date": "2025-09-29 09:03:36 UTC",
      "updated_date": "2025-09-29 09:03:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:03:51.565576+00:00"
    },
    {
      "arxiv_id": "2510.00063v2",
      "title": "AstroMMBench: A Benchmark for Evaluating Multimodal Large Language Models Capabilities in Astronomy",
      "title_zh": "AstroMMBenchï¼šå¤©æ–‡å­¦é¢†åŸŸå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹èƒ½åŠ›è¯„ä¼°åŸºå‡†",
      "authors": [
        "Jinghang Shi",
        "Xiaoyu Tang",
        "Yang Huang",
        "Yuyang Li",
        "Xiao Kong",
        "Yanxia Zhang",
        "Caizhan Yue"
      ],
      "abstract": "Astronomical image interpretation presents a significant challenge for applying multimodal large language models (MLLMs) to specialized scientific tasks. Existing benchmarks focus on general multimodal capabilities but fail to capture the complexity of astronomical data. To bridge this gap, we introduce AstroMMBench, the first comprehensive benchmark designed to evaluate MLLMs in astronomical image understanding. AstroMMBench comprises 621 multiple-choice questions across six astrophysical subfields, curated and reviewed by 15 domain experts for quality and relevance. We conducted an extensive evaluation of 25 diverse MLLMs, including 22 open-source and 3 closed-source models, using AstroMMBench. The results show that Ovis2-34B achieved the highest overall accuracy (70.5%), demonstrating leading capabilities even compared to strong closed-source models. Performance showed variations across the six astrophysical subfields, proving particularly challenging in domains like cosmology and high-energy astrophysics, while models performed relatively better in others, such as instrumentation and solar astrophysics. These findings underscore the vital role of domain-specific benchmarks like AstroMMBench in critically evaluating MLLM performance and guiding their targeted development for scientific applications. AstroMMBench provides a foundational resource and a dynamic tool to catalyze advancements at the intersection of AI and astronomy.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) åœ¨å¤„ç†å¤æ‚å¤©æ–‡å›¾åƒæ—¶çš„å±€é™æ€§ï¼Œæå‡ºäº† AstroMMBenchï¼Œè¿™æ˜¯é¦–ä¸ªæ—¨åœ¨è¯„ä¼° MLLMs å¤©æ–‡å›¾åƒç†è§£èƒ½åŠ›çš„ç»¼åˆåŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†åŒ…å«ç”± 15 ä½é¢†åŸŸä¸“å®¶ç²¾å¿ƒè®¾è®¡å¹¶å®¡é˜…çš„ 621 é“å¤šé€‰é¢˜ï¼Œæ¶µç›–äº†å…­ä¸ªå¤©ä½“ç‰©ç†å­é¢†åŸŸã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨ AstroMMBench å¯¹ 25 ä¸ªä¸åŒçš„ MLLMsï¼ˆåŒ…æ‹¬ 22 ä¸ªå¼€æºæ¨¡å‹å’Œ 3 ä¸ªé—­æºæ¨¡å‹ï¼‰è¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼ŒOvis2-34B ä»¥ 70.5% çš„ç»¼åˆå‡†ç¡®ç‡ä½å±…æ¦œé¦–ï¼Œå…¶è¡¨ç°ç”šè‡³è¶…è¶Šäº†å¼ºåŠ›çš„é—­æºæ¨¡å‹ã€‚å®éªŒå‘ç°åœ¨ä¸åŒå­é¢†åŸŸçš„æ€§èƒ½å­˜åœ¨å·®å¼‚ï¼Œå…¶ä¸­å®‡å®™å­¦ (cosmology) å’Œé«˜èƒ½å¤©ä½“ç‰©ç† (high-energy astrophysics) æŒ‘æˆ˜æ€§è¾ƒå¤§ï¼Œè€Œä»ªå™¨ä»ªè¡¨ (instrumentation) å’Œå¤ªé˜³ç‰©ç† (solar astrophysics) è¡¨ç°ç›¸å¯¹è¾ƒå¥½ã€‚è¿™ä¸€åŸºå‡†æµ‹è¯•çªæ˜¾äº†é¢†åŸŸç‰¹å®šè¯„æµ‹åœ¨å¼•å¯¼ MLLM ç§‘å­¦åº”ç”¨å‘å±•ä¸­çš„å…³é”®ä½œç”¨ï¼Œä¸ºäººå·¥æ™ºèƒ½ä¸å¤©æ–‡å­¦çš„äº¤å‰ç ”ç©¶æä¾›äº†åŸºç¡€æ€§èµ„æºã€‚",
      "categories": [
        "astro-ph.IM",
        "cs.AI"
      ],
      "primary_category": "astro-ph.IM",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00063v2",
      "published_date": "2025-09-29 09:02:30 UTC",
      "updated_date": "2025-10-21 17:29:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:03:47.906404+00:00"
    },
    {
      "arxiv_id": "2509.24489v1",
      "title": "Overcoming Over-Fitting in Constraint Acquisition via Query-Driven Interactive Refinement",
      "title_zh": "é€šè¿‡æŸ¥è¯¢é©±åŠ¨çš„äº¤äº’å¼ç²¾åŒ–å…‹æœçº¦æŸè·å–ä¸­çš„è¿‡æ‹Ÿåˆ",
      "authors": [
        "Vasileios Balafas",
        "Dimos Tsouros",
        "Nikolaos Ploskas",
        "Kostas Stergiou"
      ],
      "abstract": "Manual modeling in Constraint Programming is a substantial bottleneck, which Constraint Acquisition (CA) aims to automate. However, passive CA methods are prone to over-fitting, often learning models that include spurious global constraints when trained on limited data, while purely active methods can be query-intensive. We introduce a hybrid CA framework specifically designed to address the challenge of over-fitting in CA. Our approach integrates passive learning for initial candidate generation, a query-driven interactive refinement phase that utilizes probabilistic confidence scores (initialized by machine learning priors) to systematically identify over-fitted constraints, and a specialized subset exploration mechanism to recover valid substructures from rejected candidates. A final active learning phase ensures model completeness. Extensive experiments on diverse benchmarks demonstrate that our interactive refinement phase is crucial for achieving high target model coverage and overall model accuracy from limited examples, doing so with manageable query complexity. This framework represents a substantial advancement towards robust and practical constraint acquisition in data-limited scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹çº¦æŸç¼–ç¨‹(Constraint Programming)ä¸­çº¦æŸä¹ å¾—(Constraint Acquisition, CA)åœ¨æœ‰é™æ•°æ®ä¸‹æ˜“äº§ç”Ÿè¿‡æ‹Ÿåˆ(Over-fitting)çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆè¢«åŠ¨å­¦ä¹ ä¸ä¸»åŠ¨å­¦ä¹ çš„æ··åˆå¼æ¡†æ¶ã€‚ç”±äºè¢«åŠ¨æ–¹æ³•æ˜“å­¦ä¹ åˆ°ä¼ªå…¨å±€çº¦æŸè€Œçº¯ä¸»åŠ¨æ–¹æ³•æŸ¥è¯¢è´Ÿæ‹…è¿‡é‡ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†æŸ¥è¯¢é©±åŠ¨çš„äº¤äº’å¼ç»†åŒ–(Query-Driven Interactive Refinement)é˜¶æ®µï¼Œåˆ©ç”¨åŸºäºæœºå™¨å­¦ä¹ å…ˆéªŒçš„æ¦‚ç‡ç½®ä¿¡åº¦å¾—åˆ†æ¥ç²¾å‡†è¯†åˆ«å¹¶å‰”é™¤è¿‡æ‹Ÿåˆçº¦æŸã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è®¾è®¡äº†ä¸“é—¨çš„å­é›†æ¢ç´¢æœºåˆ¶ï¼Œæ—¨åœ¨ä»è¢«æ‹’ç»çš„å€™é€‰çº¦æŸä¸­æ¢å¤æœ‰æ•ˆçš„å­ç»“æ„ï¼Œå¹¶é€šè¿‡æœ€ç»ˆçš„ä¸»åŠ¨å­¦ä¹ é˜¶æ®µç¡®ä¿æ¨¡å‹çš„å®Œæ•´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤„ç†å¤šæ ·åŒ–åŸºå‡†æµ‹è¯•æ—¶ï¼Œä»…éœ€æœ‰é™ç¤ºä¾‹å’Œå¯æ§çš„æŸ¥è¯¢å¤æ‚åº¦ï¼Œå³å¯å®ç°æé«˜çš„ç›®æ ‡æ¨¡å‹è¦†ç›–ç‡å’Œå‡†ç¡®æ€§ã€‚è¿™ä¸€æ¡†æ¶ä¸ºåœ¨æ•°æ®å—é™åœºæ™¯ä¸‹å®ç°é²æ£’ä¸”å®ç”¨çš„è‡ªåŠ¨çº¦æŸå»ºæ¨¡æä¾›äº†é‡è¦çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "Preprint. Uses the International Journal on Artificial Intelligence Tools (World Scientific) template. Includes figures, tables, and algorithms. Submitted to IJAIT",
      "pdf_url": "https://arxiv.org/pdf/2509.24489v1",
      "published_date": "2025-09-29 09:02:16 UTC",
      "updated_date": "2025-09-29 09:02:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:04:12.577180+00:00"
    },
    {
      "arxiv_id": "2510.02371v1",
      "title": "Federated Spatiotemporal Graph Learning for Passive Attack Detection in Smart Grids",
      "title_zh": "é¢å‘æ™ºèƒ½ç”µç½‘è¢«åŠ¨æ”»å‡»æ£€æµ‹çš„è”é‚¦æ—¶ç©ºå›¾å­¦ä¹ ",
      "authors": [
        "Bochra Al Agha",
        "Razane Tajeddine"
      ],
      "abstract": "Smart grids are exposed to passive eavesdropping, where attackers listen silently to communication links. Although no data is actively altered, such reconnaissance can reveal grid topology, consumption patterns, and operational behavior, creating a gateway to more severe targeted attacks. Detecting this threat is difficult because the signals it produces are faint, short-lived, and often disappear when traffic is examined by a single node or along a single timeline. This paper introduces a graph-centric, multimodal detector that fuses physical-layer and behavioral indicators over ego-centric star subgraphs and short temporal windows to detect passive attacks. To capture stealthy perturbations, a two-stage encoder is introduced: graph convolution aggregates spatial context across ego-centric star subgraphs, while a bidirectional GRU models short-term temporal dependencies. The encoder transforms heterogeneous features into a unified spatio-temporal representation suitable for classification. Training occurs in a federated learning setup under FedProx, improving robustness to heterogeneous local raw data and contributing to the trustworthiness of decentralized training; raw measurements remain on client devices. A synthetic, standards-informed dataset is generated to emulate heterogeneous HAN/NAN/WAN communications with wireless-only passive perturbations, event co-occurrence, and leak-safe splits. The model achieves a testing accuracy of 98.32% per-timestep (F1_{attack}=0.972) and 93.35% per-sequence at 0.15% FPR using a simple decision rule with run-length m=2 and threshold $Ï„=0.55$. The results demonstrate that combining spatial and temporal context enables reliable detection of stealthy reconnaissance while maintaining low false-positive rates, making the approach suitable for non-IID federated smart-grid deployments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ™ºèƒ½ç”µç½‘ä¸­éš¾ä»¥å¯Ÿè§‰çš„è¢«åŠ¨çªƒå¬(passive eavesdropping)æ”»å‡»ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå›¾ä¸­å¿ƒçš„å¤šæ¨¡æ€æ£€æµ‹å™¨ï¼Œé€šè¿‡èåˆç‰©ç†å±‚å’Œè¡Œä¸ºæŒ‡æ ‡æ¥è¯†åˆ«éšè”½çš„ä¾¦å¯Ÿè¡Œä¸ºã€‚ä¸ºäº†æ•æ‰å¾®å¼±ä¸”ç¬æ—¶çš„æ”»å‡»ä¿¡å·ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨äº†ä¸¤é˜¶æ®µç¼–ç å™¨ï¼šåˆ©ç”¨ Graph Convolution åœ¨è‡ªæˆ‘ä¸­å¿ƒæ˜Ÿå‹å­å›¾(ego-centric star subgraphs)ä¸Šèšåˆç©ºé—´ä¸Šä¸‹æ–‡ï¼Œå¹¶ç»“åˆ Bidirectional GRU å»ºæ¨¡çŸ­æœŸæ—¶é—´ä¾èµ–ã€‚è¯¥æ–¹æ¡ˆåœ¨ FedProx æ¡†æ¶ä¸‹è¿›è¡Œ Federated Learningï¼Œåœ¨ä¿è¯åŸå§‹æ•°æ®ä¸ç¦»å¼€å®¢æˆ·ç«¯çš„åŒæ—¶ï¼Œæå‡äº†æ¨¡å‹å¯¹å¼‚æ„å±€éƒ¨æ•°æ®çš„é²æ£’æ€§ã€‚å®éªŒé‡‡ç”¨äº†æ¶µç›– HAN/NAN/WAN é€šä¿¡åœºæ™¯çš„åˆæˆæ•°æ®é›†ï¼Œç»“æœæ˜¾ç¤ºè¯¥æ¨¡å‹åœ¨æ¯ä¸ªæ—¶é—´æ­¥çš„æ£€æµ‹å‡†ç¡®ç‡è¾¾åˆ° 98.32%ï¼Œä¸”è¯¯æŠ¥ç‡(FPR)ä½è‡³ 0.15%ã€‚ç ”ç©¶è¯æ˜ï¼Œç»“åˆç©ºé—´å’Œæ—¶é—´ç»´åº¦çš„ç‰¹å¾è¡¨ç¤ºèƒ½æœ‰æ•ˆæ£€æµ‹æ™ºèƒ½ç”µç½‘ä¸­çš„éšè”½æ”»å‡»ï¼Œä¸”åœ¨å¤„ç†éç‹¬ç«‹åŒåˆ†å¸ƒ(non-IID)æ•°æ®æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.02371v1",
      "published_date": "2025-09-29 08:52:30 UTC",
      "updated_date": "2025-09-29 08:52:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:03:55.268344+00:00"
    },
    {
      "arxiv_id": "2509.24473v3",
      "title": "Euclid's Gift: Enhancing Spatial Perception and Reasoning in Vision-Language Models via Geometric Surrogate Tasks",
      "title_zh": "Euclid's Giftï¼šé€šè¿‡å‡ ä½•ä»£ç†ä»»åŠ¡å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ„ŸçŸ¥ä¸æ¨ç†èƒ½åŠ›",
      "authors": [
        "Shijie Lian",
        "Changti Wu",
        "Laurence Tianruo Yang",
        "Hang Yuan",
        "Bin Yu",
        "Lei Zhang",
        "Kai Chen"
      ],
      "abstract": "Spatial intelligence spans a rich suite of abilities, including visualising and transforming shapes, mentally rotating objects, judging relational positions and containment, and estimating numerosity. However, it still remains a critical unresolved challenge for Multimodal Large Language Models (MLLMs). To fill this gap, we propose to treat Euclidean geometry problem-solving as a surrogate task. Specifically, we meticulously constructed a curated multimodal dataset, called Euclid30K, comprising approximately 30K plane and solid geometry problems. Furthermore, to enable the model to learn and apply Euclidean principles from these geometry problems, we fine-tuned seven model variants (spanning 3--72B parameters) from the Qwen2.5VL, Qwen3VL, and RoboBrain2.0 families using Group Relative Policy Optimization (GRPO), inspiring the models to identify shapes, count, and relate entities, and perform multi-step deductive reasoning using Euclidean principles. Our experiments demonstrate that the resulting models achieve substantial zero-shot gains across four spatial reasoning benchmarks (Super-CLEVR, Omni3DBench, VSI-Bench, and MindCube) without any task-specific adaptations. Notably, after training on the Euclid30K, the mean VSI-Bench accuracy rose from 36.6\\% to 41.8\\% (+5.2\\%), and the mean MindCube accuracy rose from 31.4\\% to 38.1\\% (+6.7\\%). To our knowledge, this is the first systematic study showing that geometry-centric fine-tuning can confer vision-language models with broadly transferable spatial skills. Code and Euclid30K dataset can be found in \\href{https://zgca-ai4edu.github.io/Euclids_Gift}{this}.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) åœ¨ç©ºé—´æ™ºèƒ½æ–¹é¢çš„å±€é™æ€§ï¼Œæå‡ºäº†å°†æ¬§å‡ é‡Œå¾—å‡ ä½•é—®é¢˜è§£å†³ä½œä¸ºä¸€ç§ä»£ç†ä»»åŠ¡ (surrogate task) çš„æ–¹æ¡ˆã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†åŒ…å«çº¦ 3 ä¸‡ä¸ªå¹³é¢å’Œç«‹ä½“å‡ ä½•é—®é¢˜çš„å¤šæ¨¡æ€æ•°æ®é›† Euclid30Kï¼Œæ—¨åœ¨é€šè¿‡å‡ ä½•åŸç†çš„å­¦ä¹ å¢å¼ºæ¨¡å‹çš„ç©ºé—´æ„ŸçŸ¥ä¸æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶äººå‘˜å¯¹ Qwen2.5VLã€Qwen3VL å’Œ RoboBrain2.0 ç­‰ä¸åŒå‚æ•°è§„æ¨¡çš„æ¨¡å‹è¿›è¡Œäº†åŸºäºç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ– (GRPO) çš„å¾®è°ƒï¼Œä¿ƒä½¿æ¨¡å‹æŒæ¡å½¢çŠ¶è¯†åˆ«ã€å®ä½“è®¡æ•°åŠå¤šæ­¥æ¼”ç»æ¨ç†ç­‰æŠ€èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ä½¿æ¨¡å‹åœ¨ Super-CLEVRã€Omni3DBenchã€VSI-Bench å’Œ MindCube ç­‰ç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†æ˜¾è‘—çš„é›¶æ ·æœ¬ (zero-shot) æ€§èƒ½æå‡ã€‚å…¶ä¸­ï¼ŒVSI-Bench çš„å¹³å‡å‡†ç¡®ç‡ä» 36.6% æå‡è‡³ 41.8%ï¼ŒMindCube çš„å‡†ç¡®ç‡ä» 31.4% æå‡è‡³ 38.1%ã€‚è¯¥ç ”ç©¶é¦–æ¬¡ç³»ç»Ÿæ€§åœ°è¯æ˜äº†ä»¥å‡ ä½•ä¸ºä¸­å¿ƒçš„å¾®è°ƒèƒ½å¤Ÿèµ‹äºˆè§†è§‰è¯­è¨€æ¨¡å‹å…·å¤‡å¹¿æ³›å¯è¿ç§»æ€§çš„ç©ºé—´æŠ€èƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24473v3",
      "published_date": "2025-09-29 08:49:21 UTC",
      "updated_date": "2025-11-19 14:22:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:04:09.282416+00:00"
    },
    {
      "arxiv_id": "2509.24469v2",
      "title": "LaMoGen: Laban Movement-Guided Diffusion for Text-to-Motion Generation",
      "title_zh": "LaMoGenï¼šåŸºäºæ‹‰ç­åŠ¨ä½œå¼•å¯¼æ‰©æ•£æ¨¡å‹çš„æ–‡æœ¬åˆ°åŠ¨ä½œç”Ÿæˆ",
      "authors": [
        "Heechang Kim",
        "Gwanghyun Kim",
        "Se Young Chun"
      ],
      "abstract": "Diverse human motion generation is an increasingly important task, having various applications in computer vision, human-computer interaction and animation. While text-to-motion synthesis using diffusion models has shown success in generating high-quality motions, achieving fine-grained expressive motion control remains a significant challenge. This is due to the lack of motion style diversity in datasets and the difficulty of expressing quantitative characteristics in natural language. Laban movement analysis has been widely used by dance experts to express the details of motion including motion quality as consistent as possible. Inspired by that, this work aims for interpretable and expressive control of human motion generation by seamlessly integrating the quantification methods of Laban Effort and Shape components into the text-guided motion generation models. Our proposed zero-shot, inference-time optimization method guides the motion generation model to have desired Laban Effort and Shape components without any additional motion data by updating the text embedding of pretrained diffusion models during the sampling step. We demonstrate that our approach yields diverse expressive motion qualities while preserving motion identity by successfully manipulating motion attributes according to target Laban tags.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LaMoGenï¼Œä¸€ç§å°†Laban Movement Analysisæ•´åˆåˆ°text-to-motionæ‰©æ•£æ¨¡å‹ä¸­çš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åœ¨ç»†ç²’åº¦è¡¨è¾¾æ€§åŠ¨ä½œæ§åˆ¶æ–¹é¢çš„æŒ‘æˆ˜ã€‚é€šè¿‡å°†Laban Effortå’ŒShapeç»„ä»¶çš„é‡åŒ–æ–¹æ³•æ— ç¼é›†æˆï¼ŒLaMoGenå®ç°äº†ä¸€ç§zero-shotçš„inference-time optimizationæ–¹æ¡ˆã€‚è¯¥æ–¹æ³•åœ¨é‡‡æ ·é˜¶æ®µé€šè¿‡æ›´æ–°é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„text embeddingæ¥å¼•å¯¼åŠ¨ä½œç”Ÿæˆï¼Œæ— éœ€é¢å¤–çš„è¿åŠ¨æ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ ¹æ®ç›®æ ‡Labanæ ‡ç­¾æˆåŠŸæ“çºµåŠ¨ä½œå±æ€§ï¼Œåœ¨ä¿æŒmotion identityçš„åŒæ—¶ï¼Œç”Ÿæˆå…·æœ‰å¤šæ ·åŒ–è¡¨è¾¾è´¨é‡çš„åŠ¨ä½œã€‚è¯¥ç ”ç©¶ä¸ºå®ç°å¯è§£é‡Šä¸”å…·æœ‰è¡¨ç°åŠ›çš„äººä½“åŠ¨ä½œç”Ÿæˆæä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24469v2",
      "published_date": "2025-09-29 08:48:49 UTC",
      "updated_date": "2025-10-13 04:36:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:04:29.289495+00:00"
    },
    {
      "arxiv_id": "2509.24466v1",
      "title": "Moravec's Paradox and Restrepo's Model: Limits of AGI Automation in Growth",
      "title_zh": "Moravec æ‚–è®ºä¸ Restrepo æ¨¡å‹ï¼šAGI è‡ªåŠ¨åŒ–åœ¨ç»æµå¢é•¿ä¸­çš„å±€é™æ€§",
      "authors": [
        "Marc Bara"
      ],
      "abstract": "This note extends Restrepo (2025)'s model of economic growth under AGI by incorporating Moravec's Paradox -the observation that tasks requiring sensorimotor skills remain computationally expensive relative to cognitive tasks. We partition the task space into cognitive and physical components with differential automation costs, allowing infinite costs for some physical bottlenecks. Our key result shows that when physical tasks constitute economic bottlenecks with sufficiently high (or infinite) computational requirements, the labor share of income converges to a positive constant in the finite-compute regime (rather than zero). This fundamentally alters the distributional implications of AGI while preserving the growth dynamics for cognitive-intensive economies.",
      "tldr_zh": "è¯¥ç ”ç©¶æ‰©å±•äº†Restrepo (2025)å…³äºAGIé©±åŠ¨ä¸‹ç»æµå¢é•¿çš„æ¨¡å‹ï¼Œé€šè¿‡å¼•å…¥Moravec's Paradoxæ¥æ¢è®¨æ„ŸçŸ¥è¿åŠ¨ä»»åŠ¡åœ¨è®¡ç®—æˆæœ¬ä¸Šè¿œé«˜äºè®¤çŸ¥ä»»åŠ¡çš„ç°è±¡ã€‚ä½œè€…å°†ä»»åŠ¡ç©ºé—´åˆ’åˆ†ä¸ºè®¤çŸ¥(cognitive)å’Œç‰©ç†(physical)ä¸¤ä¸ªéƒ¨åˆ†ï¼Œå¹¶è®¾å®šäº†å·®å¼‚åŒ–çš„è‡ªåŠ¨åŒ–æˆæœ¬ï¼Œç”šè‡³å…è®¸æŸäº›ç‰©ç†ç“¶é¢ˆä»»åŠ¡å…·æœ‰æ— é™çš„è®¡ç®—æˆæœ¬ã€‚ç ”ç©¶çš„æ ¸å¿ƒç»“æœè¡¨æ˜ï¼Œå½“ç‰©ç†ä»»åŠ¡å› æé«˜æˆ–æ— é™çš„è®¡ç®—è¦æ±‚æˆä¸ºç»æµç“¶é¢ˆæ—¶ï¼Œåœ¨æœ‰é™è®¡ç®—(finite-compute)åˆ¶åº¦ä¸‹ï¼Œæ”¶å…¥ä¸­çš„åŠ³åŠ¨ä»½é¢(labor share of income)å°†æ”¶æ•›äºä¸€ä¸ªå¤§äºé›¶çš„å¸¸æ•°è€Œéè¶‹äºé›¶ã€‚è¿™ä¸€å‘ç°ä»æ ¹æœ¬ä¸Šæ”¹å˜äº†AGIå¯¹æ”¶å…¥åˆ†é…çš„æ½œåœ¨å½±å“ï¼ŒåŒæ—¶åœ¨è®¤çŸ¥å¯†é›†å‹ç»æµä½“ä¸­ä¾ç„¶ä¿ç•™äº†åŸæœ‰çš„å¢é•¿åŠ¨æ€ã€‚",
      "categories": [
        "econ.GN",
        "cs.AI"
      ],
      "primary_category": "econ.GN",
      "comment": "5 pages, 1 table",
      "pdf_url": "https://arxiv.org/pdf/2509.24466v1",
      "published_date": "2025-09-29 08:45:00 UTC",
      "updated_date": "2025-09-29 08:45:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:04:31.386957+00:00"
    },
    {
      "arxiv_id": "2510.00062v1",
      "title": "Efficient CNN Compression via Multi-method Low Rank Factorization and Feature Map Similarity",
      "title_zh": "åŸºäºå¤šæ–¹æ³•ä½ç§©åˆ†è§£ä¸ç‰¹å¾å›¾ç›¸ä¼¼æ€§çš„é«˜æ•ˆå·ç§¯ç¥ç»ç½‘ç»œå‹ç¼©",
      "authors": [
        "M. Kokhazadeh",
        "G. Keramidas",
        "V. Kelefouras"
      ],
      "abstract": "Low-Rank Factorization (LRF) is a widely adopted technique for compressing deep neural networks (DNNs). However, it faces several challenges, including optimal rank selection, a vast design space, long fine-tuning times, and limited compatibility with different layer types and decomposition methods. This paper presents an end-to-end Design Space Exploration (DSE) methodology and framework for compressing convolutional neural networks (CNNs) that addresses all these issues. We introduce a novel rank selection strategy based on feature map similarity, which captures non-linear interactions between layer outputs more effectively than traditional weight-based approaches. Unlike prior works, our method uses a one-shot fine-tuning process, significantly reducing the overall fine-tuning time. The proposed framework is fully compatible with all types of convolutional (Conv) and fully connected (FC) layers. To further improve compression, the framework integrates three different LRF techniques for Conv layers and three for FC layers, applying them selectively on a per-layer basis. We demonstrate that combining multiple LRF methods within a single model yields better compression results than using a single method uniformly across all layers. Finally, we provide a comprehensive evaluation and comparison of the six LRF techniques, offering practical insights into their effectiveness across different scenarios. The proposed work is integrated into TensorFlow 2.x, ensuring compatibility with widely used deep learning workflows. Experimental results on 14 CNN models across eight datasets demonstrate that the proposed methodology achieves substantial compression with minimal accuracy loss, outperforming several state-of-the-art techniques.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦ç¥ç»ç½‘ç»œå‹ç¼©ä¸­ Low-Rank Factorization (LRF) é¢ä¸´çš„ç§©é€‰æ‹©ã€è®¾è®¡ç©ºé—´å¤æ‚åŠå¾®è°ƒæ—¶é—´é•¿ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„è®¾è®¡ç©ºé—´æ¢ç´¢ (Design Space Exploration, DSE) æ¡†æ¶ã€‚è®ºæ–‡å¼•å…¥äº†ä¸€ç§åŸºäºç‰¹å¾å›¾ç›¸ä¼¼åº¦ (Feature Map Similarity) çš„æ–°å‹ç§©é€‰æ‹©ç­–ç•¥ï¼Œç›¸æ¯”ä¼ ç»ŸåŸºäºæƒé‡çš„æ–¹æ³•èƒ½æ›´æœ‰æ•ˆåœ°æ•æ‰å±‚è¾“å‡ºé—´çš„éçº¿æ€§äº¤äº’ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸€é”®å¼å¾®è°ƒ (One-shot fine-tuning) æµç¨‹æ˜¾è‘—ç¼©çŸ­äº†è®­ç»ƒè€—æ—¶ï¼Œå¹¶å®ç°äº†å¯¹æ‰€æœ‰å·ç§¯å±‚ (Conv) å’Œå…¨è¿æ¥å±‚ (FC) çš„å…¨é¢å…¼å®¹ã€‚ç ”ç©¶é›†æˆäº†å…­ç§ä¸åŒçš„ LRF æŠ€æœ¯å¹¶æ”¯æŒé€å±‚çµæ´»é…ç½®ï¼Œå®éªŒè¯æ˜åœ¨å•ä¸ªæ¨¡å‹ä¸­ç»„åˆå¤šç§åˆ†è§£æ–¹æ³•æ¯”ä½¿ç”¨å•ä¸€æ–¹æ³•å…·æœ‰æ›´å¥½çš„å‹ç¼©è¡¨ç°ã€‚è¯¥æ–¹æ¡ˆå·²é›†æˆè‡³ TensorFlow 2.xï¼Œåœ¨ 14 ä¸ª CNN æ¨¡å‹å’Œ 8 ä¸ªæ•°æ®é›†ä¸Šçš„æµ‹è¯•è¡¨æ˜ï¼Œå…¶åœ¨æä½ç²¾åº¦æŸå¤±ä¸‹å®ç°äº†å¤§å¹…å‹ç¼©ã€‚å®éªŒç»“æœè¯å®è¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†å¤šç§å…ˆè¿›æŠ€æœ¯ï¼Œä¸ºæ¨¡å‹å‹ç¼©æä¾›äº†é«˜æ•ˆä¸”å…·å®ç”¨ä»·å€¼çš„å·¥ç¨‹å‚è€ƒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "14 pages, 17 figures, This work has been submitted to the IEEE for possible publication (IEEE Transactions on Artificial Intelligence)",
      "pdf_url": "https://arxiv.org/pdf/2510.00062v1",
      "published_date": "2025-09-29 08:44:02 UTC",
      "updated_date": "2025-09-29 08:44:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:04:43.167614+00:00"
    },
    {
      "arxiv_id": "2509.24463v2",
      "title": "An Agent-Based Framework for Automated Higher-Voice Harmony Generation",
      "title_zh": "åŸºäºæ™ºèƒ½ä½“çš„è‡ªåŠ¨åŒ–é«˜å£°éƒ¨å’Œå£°ç”Ÿæˆæ¡†æ¶",
      "authors": [
        "Nia D'Souza Ganapathy",
        "Arul Selvamani Shaja"
      ],
      "abstract": "The generation of musically coherent and aesthetically pleasing harmony remains a significant challenge in the field of algorithmic composition. This paper introduces an innovative Agentic AI-enabled Higher Harmony Music Generator, a multi-agent system designed to create harmony in a collaborative and modular fashion. Our framework comprises four specialized agents: a Music-Ingestion Agent for parsing and standardizing input musical scores; a Chord-Knowledge Agent, powered by a Chord-Former (Transformer model), to interpret and provide the constituent notes of complex chord symbols; a Harmony-Generation Agent, which utilizes a Harmony-GPT and a Rhythm-Net (RNN) to compose a melodically and rhythmically complementary harmony line; and an Audio-Production Agent that employs a GAN-based Symbolic-to-Audio Synthesizer to render the final symbolic output into high-fidelity audio. By delegating specific tasks to specialized agents, our system effectively mimics the collaborative process of human musicians. This modular, agent-based approach allows for robust data processing, deep theoretical understanding, creative composition, and realistic audio synthesis, culminating in a system capable of generating sophisticated and contextually appropriate higher-voice harmonies for given melodies.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ™ºèƒ½ä½“çš„é«˜éŸ³éƒ¨å’Œå£°ç”Ÿæˆå™¨ï¼ˆAgentic AI-enabled Higher Harmony Music Generatorï¼‰ï¼Œæ—¨åœ¨è§£å†³ç®—æ³•ä½œæ›²ä¸­ç”Ÿæˆå…·æœ‰éŸ³ä¹è¿è´¯æ€§å’Œç¾å­¦å¸å¼•åŠ›å’Œå£°çš„éš¾é¢˜ã€‚è¯¥å¤šæ™ºèƒ½ä½“æ¡†æ¶åŒ…å«å››ä¸ªä¸“ä¸šæ™ºèƒ½ä½“ï¼šMusic-Ingestion Agent ç”¨äºè§£æå’Œæ ‡å‡†åŒ–è¾“å…¥ä¹è°±ï¼›Chord-Knowledge Agent ä¾æ‰˜ Chord-Former è§£é‡Šå¤æ‚å’Œå¼¦ï¼›Harmony-Generation Agent ç»“åˆ Harmony-GPT ä¸ Rhythm-Net åˆ›ä½œæ—‹å¾‹ä¸èŠ‚å¥äº’è¡¥çš„å’Œå£°ï¼›Audio-Production Agent åˆ™é€šè¿‡åŸºäº GAN çš„ Symbolic-to-Audio Synthesizer å®ç°é«˜ä¿çœŸéŸ³é¢‘åˆæˆã€‚è¯¥ç³»ç»Ÿé€šè¿‡æ¨¡æ‹Ÿäººç±»éŸ³ä¹å®¶çš„åä½œæ¨¡å¼ï¼Œå®ç°äº†ä»ä¹è°±è§£æåˆ°éŸ³é¢‘ç”Ÿæˆçš„å…¨æµç¨‹è‡ªåŠ¨åŒ–ã€‚è¿™ç§æ¨¡å—åŒ–æ–¹æ³•ä¸ä»…ä¿è¯äº†æ·±å±‚çš„éŸ³ä¹ç†è®ºç†è§£ï¼Œè¿˜å…·å¤‡å¼ºå¤§çš„æ•°æ®å¤„ç†èƒ½åŠ›ã€‚æœ€ç»ˆï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿä¸ºç»™å®šæ—‹å¾‹ç”Ÿæˆå¤æ‚ä¸”ç¬¦åˆè¯­å¢ƒçš„é«˜æ°´å‡†é«˜éŸ³éƒ¨å’Œå£°ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24463v2",
      "published_date": "2025-09-29 08:42:42 UTC",
      "updated_date": "2025-10-01 07:00:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:04:41.178802+00:00"
    },
    {
      "arxiv_id": "2509.24460v1",
      "title": "ContextPRM: Leveraging Contextual Coherence for multi-domain Test-Time Scaling",
      "title_zh": "ContextPRMï¼šåˆ©ç”¨ä¸Šä¸‹æ–‡è¿è´¯æ€§å®ç°å¤šé¢†åŸŸæ¨ç†æ—¶æ‰©å±•",
      "authors": [
        "Haotian Zhang",
        "Liu Liu",
        "Baosheng Yu",
        "Jiayan Qiu",
        "Likang Xiao",
        "Yanwei Ren",
        "Quan Chen",
        "Xianglong Liu"
      ],
      "abstract": "Process reward models (PRMs) have demonstrated significant efficacy in enhancing the mathematical reasoning capabilities of large language models (LLMs) by leveraging test-time scaling (TTS). However, while most PRMs exhibit substantial gains in mathematical domains, the scarcity of domain-specific training data and knowledge-based learning patterns limits their generalization ability when faced with other domains. To address this limitation, we shift the learning objective from verifying domain-specific knowledge to modeling domain-agnostic logical flow. Centering on contextual coherence between chain-of-thought (CoT) steps, our approach is realized through a novel data annotation and training framework, which enhances the model's generalization capabilities across diverse domains. For instance, our resulting model, ContextPRM, achieves a notable 6.5% average accuracy improvement over the majority voting baseline via weighted majority voting across nine non-mathematical domains in MMLU-Pro, including law, history, and philosophy, significantly surpassing the 2.2% improvement from VersaPRM and 0.5% gains from other mathematics-focused PRMs, demonstrating consistent performance across both mathematical and non-mathematical domains.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ (PRMs) åœ¨éæ•°å­¦é¢†åŸŸæ³›åŒ–èƒ½åŠ›å—é™çš„é—®é¢˜ï¼Œæå‡ºäº† ContextPRM æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æµ‹è¯•æ—¶ç¼©æ”¾ (test-time scaling) å¢å¼ºå¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„å¤šé¢†åŸŸæ¨ç†èƒ½åŠ›ã€‚ContextPRM å°†å­¦ä¹ ç›®æ ‡ä»éªŒè¯é¢†åŸŸç‰¹å®šçŸ¥è¯†è½¬å‘å»ºæ¨¡é¢†åŸŸæ— å…³çš„é€»è¾‘æµï¼Œé‡ç‚¹å…³æ³¨æ€ç»´é“¾ (CoT) æ­¥éª¤ä¹‹é—´çš„ä¸Šä¸‹æ–‡è¿è´¯æ€§ (contextual coherence)ã€‚é€šè¿‡å¼•å…¥ä¸€ç§æ–°å‹çš„æ•°æ®æ ‡æ³¨å’Œè®­ç»ƒæ¡†æ¶ï¼Œè¯¥æ¨¡å‹æ˜¾è‘—æå‡äº†åœ¨å¤šå…ƒé¢†åŸŸä¸‹çš„æ³›åŒ–è¡¨ç°ã€‚å®éªŒè¡¨æ˜ï¼ŒContextPRM åœ¨ MMLU-Pro çš„æ³•å¾‹ã€å†å²å’Œå“²å­¦ç­‰ä¹ä¸ªéæ•°å­¦é¢†åŸŸä¸­ï¼Œæ¯”å¤šæ•°æŠ•ç¥¨åŸºçº¿å¹³å‡å‡†ç¡®ç‡æé«˜äº† 6.5%ï¼Œè¿œè¶… VersaPRM åŠå…¶ä»–æ•°å­¦ä¸“ç”¨ PRMs çš„è¡¨ç°ã€‚ç ”ç©¶ç»“æœè¯æ˜äº†è¯¥æ–¹æ³•åœ¨æ•°å­¦ä¸éæ•°å­¦é¢†åŸŸå‡èƒ½ä¿æŒæ€§èƒ½çš„ä¸€è‡´æ€§ï¼Œä¸ºè·¨é¢†åŸŸæ¨ç†éªŒè¯æä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24460v1",
      "published_date": "2025-09-29 08:40:46 UTC",
      "updated_date": "2025-09-29 08:40:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:04:41.590896+00:00"
    },
    {
      "arxiv_id": "2509.24443v1",
      "title": "A Systematic Review of Digital Twin-Driven Predictive Maintenance in Industrial Engineering: Taxonomy, Architectural Elements, and Future Research Directions",
      "title_zh": "å·¥ä¸šå·¥ç¨‹é¢†åŸŸæ•°å­—å­ªç”Ÿé©±åŠ¨çš„é¢„æµ‹æ€§ç»´æŠ¤ç³»ç»Ÿç»¼è¿°ï¼šåˆ†ç±»ä½“ç³»ã€æ¶æ„è¦ç´ ä¸æœªæ¥ç ”ç©¶æ–¹å‘",
      "authors": [
        "Leila Ismail",
        "Abdelmoneim Abdelmoti",
        "Arkaprabha Basu",
        "Aymen Dia Eddine Berini",
        "Mohammad Naouss"
      ],
      "abstract": "With the increasing complexity of industrial systems, there is a pressing need for predictive maintenance to avoid costly downtime and disastrous outcomes that could be life-threatening in certain domains. With the growing popularity of the Internet of Things, Artificial Intelligence, machine learning, and real-time big data analytics, there is a unique opportunity for efficient predictive maintenance to forecast equipment failures for real-time intervention and optimize maintenance actions, as traditional reactive and preventive maintenance practices are often inadequate to meet the requirements for the industry to provide quality-of-services of operations. Central to this evolution is digital twin technology, an adaptive virtual replica that continuously monitors and integrates sensor data to simulate and improve asset performance. Despite remarkable progress in digital twin implementations, such as considering DT in predictive maintenance for industrial engineering. This paper aims to address this void. We perform a retrospective analysis of the temporal evolution of the digital twin in predictive maintenance for industrial engineering to capture the applications, middleware, and technological requirements that led to the development of the digital twin from its inception to the AI-enabled digital twin and its self-learning models. We provide a layered architecture of the digital twin technology, as well as a taxonomy of the technology-enabled industrial engineering applications systems, middleware, and the used Artificial Intelligence algorithms. We provide insights into these systems for the realization of a trustworthy and efficient smart digital-twin industrial engineering ecosystem. We discuss future research directions in digital twin for predictive maintenance in industrial engineering.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹å·¥ä¸šå·¥ç¨‹é¢†åŸŸä¸­ç”±æ•°å­—å­ªç”Ÿ(Digital Twin)é©±åŠ¨çš„é¢„æµ‹æ€§ç»´æŠ¤(Predictive Maintenance)è¿›è¡Œäº†ç³»ç»Ÿæ€§ç»¼è¿°ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿååº”å¼å’Œé¢„é˜²æ€§ç»´æŠ¤åœ¨åº”å¯¹å¤æ‚å·¥ä¸šç³»ç»Ÿæ—¶çš„ä¸è¶³ã€‚æ–‡ç« å›é¡¾äº†æ•°å­—å­ªç”Ÿä»åˆå§‹æ¦‚å¿µåˆ°AIèµ‹èƒ½åŠè‡ªå­¦ä¹ æ¨¡å‹(self-learning models)çš„æ¼”è¿›å†ç¨‹ï¼Œè¯¦ç»†åˆ†æäº†ç›¸å…³çš„åº”ç”¨ã€ä¸­é—´ä»¶å’ŒæŠ€æœ¯éœ€æ±‚ã€‚ç ”ç©¶æå‡ºäº†æ•°å­—å­ªç”ŸæŠ€æœ¯çš„åˆ†å±‚æ¶æ„(layered architecture)ï¼Œå¹¶é’ˆå¯¹å·¥ä¸šåº”ç”¨ç³»ç»Ÿã€ä¸­é—´ä»¶å’Œäººå·¥æ™ºèƒ½ç®—æ³•(Artificial Intelligence algorithms)å»ºç«‹äº†è¯¦å°½çš„åˆ†ç±»æ³•(taxonomy)ã€‚é€šè¿‡æ·±å…¥æ¢è®¨ï¼Œè¯¥ç»¼è¿°ä¸ºæ„å»ºå¯ä¿¡ä¸”é«˜æ•ˆçš„æ™ºèƒ½æ•°å­—å­ªç”Ÿå·¥ä¸šå·¥ç¨‹ç”Ÿæ€ç³»ç»Ÿæä¾›äº†å…³é”®è§è§£ã€‚æœ€åï¼Œæ–‡ç« æ˜ç¡®äº†è¯¥é¢†åŸŸæœªæ¥çš„ç ”ç©¶æ–¹å‘ï¼Œä¸ºå®ç°è®¾å¤‡æ•…éšœçš„å®æ—¶é¢„æµ‹å’Œç»´æŠ¤è¡Œä¸ºä¼˜åŒ–æä¾›äº†é‡è¦çš„ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI",
        "cs.ET",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24443v1",
      "published_date": "2025-09-29 08:26:23 UTC",
      "updated_date": "2025-09-29 08:26:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:04:43.968624+00:00"
    },
    {
      "arxiv_id": "2509.24436v1",
      "title": "EOE: Evolutionary Optimization of Experts for Training Language Models",
      "title_zh": "EOEï¼šé¢å‘è¯­è¨€æ¨¡å‹è®­ç»ƒçš„ä¸“å®¶è¿›åŒ–ä¼˜åŒ–",
      "authors": [
        "Yingshi Chen"
      ],
      "abstract": "This paper presents an evolutionary framework for the training of large language models(LLM). The models are divided into several experts(sub-networks), which have the same structure but different parameter values. Only one expert is trained at each step. After the classical AdamW optimization, some evolutionary operators(crossover, PSO, and mutation) act on the tensor weights between the current expert and the best expert. So current expert would learn the experience of best expert. The direction of best expert would help current expert's loss decrease faster. Finally, only save the weight of the best expert. Experiments show that best expert would achieve nearly the same accuracy as the full model. This would greatly reduce the size of the model for inference. Since only one expert is trained at each step, the training needs much less memory and has much higher throughput. Experiments show that the throughput would accelerate more than ten times! Our source code is available. It's a pure c++/cu framework, which is suitable for easy deployment on PCs and edge computing devices.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†EOEï¼Œä¸€ç§ç”¨äºè®­ç»ƒå¤§è¯­è¨€æ¨¡å‹(LLM)çš„è¿›åŒ–ä¼˜åŒ–æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†æ¨¡å‹åˆ’åˆ†ä¸ºå¤šä¸ªå…·æœ‰ç›¸åŒç»“æ„ä½†å‚æ•°ä¸åŒçš„ä¸“å®¶(experts)å­ç½‘ç»œï¼Œåœ¨æ¯ä¸ªè®­ç»ƒæ­¥éª¤ä¸­ä»…é’ˆå¯¹ä¸€ä¸ªä¸“å®¶è¿›è¡Œè®­ç»ƒã€‚é€šè¿‡åœ¨AdamWä¼˜åŒ–åå¼•å…¥äº¤å‰(crossover)ã€ç²’å­ç¾¤ä¼˜åŒ–(PSO)å’Œå˜å¼‚(mutation)ç­‰è¿›åŒ–ç®—å­ï¼Œå½“å‰ä¸“å®¶èƒ½å¤Ÿå­¦ä¹ æœ€ä½³ä¸“å®¶çš„æƒé‡ç»éªŒï¼Œä»è€Œå®ç°æ›´å¿«çš„æŸå¤±ä¸‹é™ã€‚å®éªŒè¯æ˜ï¼Œæœ€ç»ˆä¿ç•™çš„æœ€ä½³ä¸“å®¶åœ¨å‡†ç¡®ç‡ä¸Šä¸å…¨æ¨¡å‹åŸºæœ¬æŒå¹³ï¼Œè¿™æå¤§å‡å°äº†æ¨ç†æ—¶çš„æ¨¡å‹ä½“ç§¯ã€‚å¾—ç›Šäºå•æ­¥è®­ç»ƒå•ä¸ªä¸“å®¶çš„è®¾è®¡ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—é™ä½äº†æ˜¾å­˜å ç”¨å¹¶å°†ååé‡æå‡äº†åå€ä»¥ä¸Šã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨çº¯C++/cuæ¡†æ¶å®ç°ï¼Œç‰¹åˆ«é€‚ç”¨äºåœ¨ä¸ªäººç”µè„‘å’Œè¾¹ç¼˜è®¡ç®—è®¾å¤‡ä¸Šè¿›è¡Œä¾¿æ·éƒ¨ç½²ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "6 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.24436v1",
      "published_date": "2025-09-29 08:18:26 UTC",
      "updated_date": "2025-09-29 08:18:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:04:47.163277+00:00"
    },
    {
      "arxiv_id": "2509.24435v1",
      "title": "Alternatives To Next Token Prediction In Text Generation -- A Survey",
      "title_zh": "æ–‡æœ¬ç”Ÿæˆä¸­ä¸‹ä¸€ä¸ª Token é¢„æµ‹çš„æ›¿ä»£æ–¹æ¡ˆï¼šç»¼è¿°",
      "authors": [
        "Charlie Wyatt",
        "Aditya Joshi",
        "Flora Salim"
      ],
      "abstract": "The paradigm of Next Token Prediction (NTP) has driven the unprecedented success of Large Language Models (LLMs), but is also the source of their most persistent weaknesses such as poor long-term planning, error accumulation, and computational inefficiency. Acknowledging the growing interest in exploring alternatives to NTP, the survey describes the emerging ecosystem of alternatives to NTP. We categorise these approaches into five main families: (1) Multi-Token Prediction, which targets a block of future tokens instead of a single one; (2) Plan-then-Generate, where a global, high-level plan is created upfront to guide token-level decoding; (3) Latent Reasoning, which shifts the autoregressive process itself into a continuous latent space; (4) Continuous Generation Approaches, which replace sequential generation with iterative, parallel refinement through diffusion, flow matching, or energy-based methods; and (5) Non-Transformer Architectures, which sidestep NTP through their inherent model structure. By synthesizing insights across these methods, this survey offers a taxonomy to guide research into models that address the known limitations of token-level generation to develop new transformative models for natural language processing.",
      "tldr_zh": "è¯¥ç»¼è¿°æ·±å…¥åˆ†æäº†é©±åŠ¨å¤§è¯­è¨€æ¨¡å‹æˆåŠŸçš„ Next Token Prediction (NTP) èŒƒå¼åŠå…¶å­˜åœ¨çš„é•¿æœŸè§„åˆ’å·®ã€è¯¯å·®ç´¯ç§¯å’Œè®¡ç®—æ•ˆç‡ä½ä¸‹ç­‰æ ¸å¿ƒå±€é™æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡å¯¹æ–°å…´çš„ NTP æ›¿ä»£æ–¹æ¡ˆè¿›è¡Œäº†ç³»ç»Ÿæ€§æ¢³ç†ï¼Œå¹¶å°†å…¶å½’çº³ä¸ºäº”å¤§ä¸»è¦å®¶æ—ã€‚è¿™äº”ç±»æ–¹æ³•åŒ…æ‹¬ï¼šåŒæ—¶é¢„æµ‹å¤šä¸ª Token çš„ Multi-Token Predictionï¼Œå…ˆåˆ¶å®šå…¨å±€é«˜å±‚è®¡åˆ’çš„ Plan-then-Generateï¼Œåœ¨è¿ç»­éšç©ºé—´ä¸­è¿›è¡Œè‡ªå›å½’çš„ Latent Reasoningï¼Œé‡‡ç”¨æ‰©æ•£æˆ–æµåŒ¹é…ç­‰æŠ€æœ¯è¿›è¡Œè¿­ä»£å¹¶è¡Œç»†åŒ–çš„ Continuous Generation Approachesï¼Œä»¥åŠé€šè¿‡å›ºæœ‰ç»“æ„è§„é¿ NTP çš„ Non-Transformer Architecturesã€‚ç ”ç©¶é€šè¿‡å¯¹è¿™äº›å‰æ²¿æŠ€æœ¯çš„æ•´åˆä¸å¯¹æ¯”ï¼Œæä¾›äº†ä¸€å¥—æ¸…æ™°çš„åˆ†ç±»ä½“ç³»ï¼Œæ—¨åœ¨å¼•å¯¼ç ”ç©¶äººå‘˜è§£å†³ Token çº§ç”Ÿæˆçš„å›ºæœ‰ç“¶é¢ˆã€‚ç»¼ä¸Šæ‰€è¿°ï¼Œè¯¥è®ºæ–‡ä¸ºå¼€å‘ä¸‹ä¸€ä»£æ›´å…·å˜é©æ€§çš„è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹æä¾›äº†é‡è¦çš„ç†è®ºè·¯çº¿å›¾å’Œç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24435v1",
      "published_date": "2025-09-29 08:18:16 UTC",
      "updated_date": "2025-09-29 08:18:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:04:55.254641+00:00"
    },
    {
      "arxiv_id": "2509.24424v1",
      "title": "Multi-Item-Query Attention for Stable Sequential Recommendation",
      "title_zh": "é¢å‘ç¨³å®šåºåˆ—æ¨èçš„å¤šé¡¹ç‰©å“æŸ¥è¯¢æ³¨æ„åŠ›æœºåˆ¶",
      "authors": [
        "Mingshi Xu",
        "Haoren Zhu",
        "Wilfred Siu Hung Ng"
      ],
      "abstract": "The inherent instability and noise in user interaction data challenge sequential recommendation systems. Prevailing masked attention models, relying on a single query from the most recent item, are sensitive to this noise, reducing prediction reliability. We propose the Multi-Item-Query attention mechanism (MIQ-Attn) to enhance model stability and accuracy. MIQ-Attn constructs multiple diverse query vectors from user interactions, effectively mitigating noise and improving consistency. It is designed for easy adoption as a drop-in replacement for existing single-query attention. Experiments show MIQ-Attn significantly improves performance on benchmark datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é¡ºåºæ¨èç³»ç»Ÿ(Sequential Recommendation)ä¸­ç”¨æˆ·äº¤äº’æ•°æ®å›ºæœ‰çš„ä¸ç¨³å®šæ€§å’Œå™ªå£°é—®é¢˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿçš„æ©ç æ³¨æ„åŠ›æ¨¡å‹å› ä»…ä¾èµ–æœ€è¿‘æœŸé¡¹ç›®çš„å•ä¸€æŸ¥è¯¢(Single Query)è€Œå¯¼è‡´é¢„æµ‹å¯é æ€§ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†å¤šé¡¹ç›®æŸ¥è¯¢æ³¨æ„åŠ›æœºåˆ¶(Multi-Item-Query Attention, MIQ-Attn)ï¼Œæ—¨åœ¨æ˜¾è‘—å¢å¼ºæ¨¡å‹çš„ç¨³å®šæ€§å’Œå‡†ç¡®æ€§ã€‚MIQ-Attn é€šè¿‡ä»ç”¨æˆ·å†å²äº¤äº’ä¸­æ„å»ºå¤šä¸ªå¤šæ ·åŒ–çš„æŸ¥è¯¢å‘é‡ï¼Œæœ‰æ•ˆç¼“è§£äº†å™ªå£°å½±å“å¹¶æå‡äº†é¢„æµ‹ä¸€è‡´æ€§ã€‚è¯¥æœºåˆ¶è®¾è®¡ç®€æ´ï¼Œå¯ä½œä¸ºç°æœ‰å•æŸ¥è¯¢æ³¨æ„åŠ›æœºåˆ¶çš„å³æ’å³ç”¨æ›¿ä»£æ–¹æ¡ˆã€‚å®éªŒç»“æœè¯æ˜ï¼ŒMIQ-Attn åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¸ºæ„å»ºæ›´ç¨³å¥çš„æ¨èç³»ç»Ÿæä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24424v1",
      "published_date": "2025-09-29 08:11:27 UTC",
      "updated_date": "2025-09-29 08:11:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:04:56.262146+00:00"
    },
    {
      "arxiv_id": "2509.24420v1",
      "title": "A Data-Centric Perspective on the Influence of Image Data Quality in Machine Learning Models",
      "title_zh": "ä»¥æ•°æ®ä¸ºä¸­å¿ƒè§†è§’ä¸‹å›¾åƒæ•°æ®è´¨é‡å¯¹æœºå™¨å­¦ä¹ æ¨¡å‹çš„å½±å“ç ”ç©¶",
      "authors": [
        "Pei-Han Chen",
        "Szu-Chi Chung"
      ],
      "abstract": "In machine learning, research has traditionally focused on model development, with relatively less attention paid to training data. As model architectures have matured and marginal gains from further refinements diminish, data quality has emerged as a critical factor. However, systematic studies on evaluating and ensuring dataset quality in the image domain remain limited.\n  This study investigates methods for systematically assessing image dataset quality and examines how various image quality factors influence model performance. Using the publicly available and relatively clean CIFAKE dataset, we identify common quality issues and quantify their impact on training. Building on these findings, we develop a pipeline that integrates two community-developed tools, CleanVision and Fastdup. We analyze their underlying mechanisms and introduce several enhancements, including automatic threshold selection to detect problematic images without manual tuning.\n  Experimental results demonstrate that not all quality issues exert the same level of impact. While convolutional neural networks show resilience to certain distortions, they are particularly vulnerable to degradations that obscure critical visual features, such as blurring and severe downscaling. To assess the performance of existing tools and the effectiveness of our proposed enhancements, we formulate the detection of low-quality images as a binary classification task and use the F1 score as the evaluation metric. Our automatic thresholding method improves the F1 score from 0.6794 to 0.9468 under single perturbations and from 0.7447 to 0.8557 under dual perturbations. For near-duplicate detection, our deduplication strategy increases the F1 score from 0.4576 to 0.7928. These results underscore the effectiveness of our workflow and provide a foundation for advancing data quality assessment in image-based machine learning.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶ä»ä»¥æ•°æ®ä¸ºä¸­å¿ƒ(Data-Centric)çš„è§†è§’å‡ºå‘ï¼Œç³»ç»Ÿåœ°æ¢è®¨äº†å›¾åƒæ•°æ®è´¨é‡å¯¹æœºå™¨å­¦ä¹ æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚ç ”ç©¶è€…åˆ©ç”¨CIFAKEæ•°æ®é›†é‡åŒ–äº†å¸¸è§è´¨é‡é—®é¢˜å¯¹è®­ç»ƒçš„å†²å‡»ï¼Œå¹¶å¼€å‘äº†ä¸€å¥—é›†æˆCleanVisionå’ŒFastdupå·¥å…·çš„è‡ªåŠ¨åŒ–æµæ°´çº¿ã€‚è¯¥æµæ°´çº¿å¼•å…¥äº†è‡ªåŠ¨é˜ˆå€¼é€‰æ‹©(automatic threshold selection)ç­‰å¢å¼ºåŠŸèƒ½ï¼Œå®ç°äº†æ— éœ€äººå·¥å¹²é¢„å³å¯è¯†åˆ«é—®é¢˜å›¾åƒã€‚å®éªŒå‘ç°ï¼Œè™½ç„¶å·ç§¯ç¥ç»ç½‘ç»œ(CNNs)å¯¹æŸäº›å¤±çœŸè¡¨ç°å‡ºéŸ§æ€§ï¼Œä½†å¯¹æ¨¡ç³Š(blurring)å’Œä¸¥é‡ä¸‹é‡‡æ ·(downscaling)ç­‰ç ´åå…³é”®ç‰¹å¾çš„é€€åŒ–æä¸ºæ•æ„Ÿã€‚ç»“æœæ˜¾ç¤ºï¼Œè¯¥è‡ªåŠ¨åŒ–æ–¹æ³•åœ¨å•æ‰°åŠ¨æ£€æµ‹ä»»åŠ¡ä¸­å°†F1 scoreä»0.6794æå‡è‡³0.9468ï¼ŒåŒæ—¶æ˜¾è‘—ä¼˜åŒ–äº†è¿‘é‡å¤é¡¹æ£€æµ‹çš„å»é‡ç­–ç•¥ã€‚è¯¥å·¥ä½œä¸ºæ¨è¿›å›¾åƒæœºå™¨å­¦ä¹ é¢†åŸŸçš„æ•°æ®è´¨é‡è¯„ä¼°æä¾›äº†æœ‰æ•ˆçš„å·¥ä½œæµå’Œå®è¯åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "9 pages, 1 figure, 12 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.24420v1",
      "published_date": "2025-09-29 08:09:21 UTC",
      "updated_date": "2025-09-29 08:09:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:05:00.286495+00:00"
    },
    {
      "arxiv_id": "2509.24416v1",
      "title": "CLQ: Cross-Layer Guided Orthogonal-based Quantization for Diffusion Transformers",
      "title_zh": "CLQï¼šé¢å‘æ‰©æ•£ Transformer çš„è·¨å±‚å¼•å¯¼æ­£äº¤é‡åŒ–",
      "authors": [
        "Kai Liu",
        "Shaoqiu Zhang",
        "Linghe Kong",
        "Yulun Zhang"
      ],
      "abstract": "Visual generation quality has been greatly promoted with the rapid advances in diffusion transformers (DiTs), which is attributed to the scaling of model size and complexity. However, these attributions also hinder the practical deployment of DiTs on edge devices, limiting their development and application. Serve as an efficient model compression technique, model post-training quantization (PTQ) can reduce the memory consumption and speed up the inference, with inevitable performance degradation. To alleviate the degradation, we propose CLQ, a cross-layer guided orthogonal-based quantization method for DiTs. To be specific, CLQ consists of three key designs. First, we observe that the calibration data used by most of the PTQ methods can not honestly represent the distribution of the activations. Therefore, we propose cross-block calibration (CBC) to obtain accurate calibration data, with which the quantization can be better guided. Second, we propose orthogonal-based smoothing (OBS), which quantifies the outlier score of each channel and leverages block Hadamard matrix to smooth the outliers with negligible overhead. Third, we propose cross-layer parameter searching (CLPS) to search. We evaluate CLQ with both image generation and video generation models and successfully compress the model into W4A4 with negligible degradation in visual quality and metrics. CLQ achieves 3.98x memory saving and 3.95x speedup. Our code is available at \\hyperlink{https://github.com/Kai-Liu001/CLQ}{https://github.com/Kai-Liu001/CLQ}.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Diffusion Transformers (DiTs) å› æ¨¡å‹è§„æ¨¡å·¨å¤§è€Œéš¾ä»¥åœ¨è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²çš„é—®é¢˜ï¼Œæå‡ºäº† CLQï¼Œä¸€ç§è·¨å±‚å¼•å¯¼çš„æ­£äº¤åŒ–é‡åŒ–æ–¹æ³•ã€‚CLQ åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒè®¾è®¡ï¼šé¦–å…ˆï¼Œé€šè¿‡è·¨å—æ ¡å‡† (Cross-Block Calibration, CBC) è§£å†³ä¼ ç»Ÿ Post-Training Quantization (PTQ) æ¿€æ´»åˆ†å¸ƒä¸å‡çš„é—®é¢˜ï¼Œä»è€Œè·å–æ›´ç²¾ç¡®çš„é‡åŒ–å¼•å¯¼ï¼›å…¶æ¬¡ï¼Œå¼•å…¥åŸºäºæ­£äº¤çš„å¹³æ»‘æŠ€æœ¯ (Orthogonal-Based Smoothing, OBS)ï¼Œåˆ©ç”¨å— Hadamard çŸ©é˜µä»¥æä½å¼€é”€å¹³æ»‘é€šé“ç¦»ç¾¤å€¼ï¼›æœ€åï¼Œåˆ©ç”¨è·¨å±‚å‚æ•°æœç´¢ (Cross-Layer Parameter Searching, CLPS) ä¼˜åŒ–é‡åŒ–å‚æ•°ã€‚åœ¨å›¾åƒä¸è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒCLQ æˆåŠŸå°†æ¨¡å‹å‹ç¼©è‡³ W4A4 ç²¾åº¦ï¼Œåœ¨å‡ ä¹ä¸æŸå¤±è§†è§‰è´¨é‡çš„å‰æä¸‹å®ç°äº† 3.98 å€çš„å†…å­˜èŠ‚çœå’Œ 3.95 å€çš„æ¨ç†åŠ é€Ÿã€‚è¯¥æ–¹æ¡ˆä¸º Diffusion Transformers åœ¨å—é™èµ„æºç¯å¢ƒä¸‹çš„é«˜æ•ˆéƒ¨ç½²æä¾›äº†æœ‰æ•ˆçš„å‹ç¼©è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages, 5 figures. Code is released at https://github.com/Kai-Liu001/CLQ",
      "pdf_url": "https://arxiv.org/pdf/2509.24416v1",
      "published_date": "2025-09-29 08:06:42 UTC",
      "updated_date": "2025-09-29 08:06:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:05:19.364168+00:00"
    },
    {
      "arxiv_id": "2509.24414v1",
      "title": "ScatterAD: Temporal-Topological Scattering Mechanism for Time Series Anomaly Detection",
      "title_zh": "ScatterADï¼šé¢å‘æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹çš„æ—¶é—´-æ‹“æ‰‘æ•£å°„æœºåˆ¶",
      "authors": [
        "Tao Yin",
        "Xiaohong Zhang",
        "Shaochen Fu",
        "Zhibin Zhang",
        "Li Huang",
        "Yiyuan Yang",
        "Kaixiang Yang",
        "Meng Yan"
      ],
      "abstract": "One main challenge in time series anomaly detection for industrial IoT lies in the complex spatio-temporal couplings within multivariate data. However, traditional anomaly detection methods focus on modeling spatial or temporal dependencies independently, resulting in suboptimal representation learning and limited sensitivity to anomalous dispersion in high-dimensional spaces. In this work, we conduct an empirical analysis showing that both normal and anomalous samples tend to scatter in high-dimensional space, especially anomalous samples are markedly more dispersed. We formalize this dispersion phenomenon as scattering, quantified by the mean pairwise distance among sample representations, and leverage it as an inductive signal to enhance spatio-temporal anomaly detection. Technically, we propose ScatterAD to model representation scattering across temporal and topological dimensions. ScatterAD incorporates a topological encoder for capturing graph-structured scattering and a temporal encoder for constraining over-scattering through mean squared error minimization between neighboring time steps. We introduce a contrastive fusion mechanism to ensure the complementarity of the learned temporal and topological representations. Additionally, we theoretically show that maximizing the conditional mutual information between temporal and topological views improves cross-view consistency and enhances more discriminative representations. Extensive experiments on multiple public benchmarks show that ScatterAD achieves state-of-the-art performance on multivariate time series anomaly detection. Code is available at this repository: https://github.com/jk-sounds/ScatterAD.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å·¥ä¸šç‰©è”ç½‘ä¸­å¤šå˜é‡æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹çš„æ—¶ç©ºè€¦åˆæŒ‘æˆ˜ï¼Œé€šè¿‡å®è¯åˆ†æå‘ç°å¼‚å¸¸æ ·æœ¬åœ¨é«˜ç»´ç©ºé—´ä¸­æ¯”æ­£å¸¸æ ·æœ¬è¡¨ç°å‡ºæ›´æ˜¾è‘—çš„æ•£å°„(scattering)ç°è±¡ã€‚åŸºäºæ­¤å‘ç°ï¼Œç ”ç©¶è€…æå‡ºäº†ScatterADæ¡†æ¶ï¼Œé€šè¿‡å»ºæ¨¡æ—¶é—´ä¸æ‹“æ‰‘ç»´åº¦çš„è¡¨ç¤ºæ•£å°„æ¥æ•æ‰å¼‚å¸¸ç‰¹å¾ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æ‹“æ‰‘ç¼–ç å™¨(topological encoder)æ•æ‰å›¾ç»“æ„æ•£å°„ï¼Œå¹¶ç»“åˆæ—¶é—´ç¼–ç å™¨(temporal encoder)é€šè¿‡æœ€å°åŒ–ç›¸é‚»æ—¶é—´æ­¥çš„å‡æ–¹è¯¯å·®æ¥çº¦æŸè¿‡åº¦æ•£å°„ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†å¯¹æ¯”èåˆæœºåˆ¶(contrastive fusion)å¹¶ä»ç†è®ºä¸Šè¯æ˜äº†æœ€å¤§åŒ–è·¨è§†å›¾æ¡ä»¶äº’ä¿¡æ¯(conditional mutual information)èƒ½å¢å¼ºè¡¨ç¤ºçš„åŒºåˆ†åº¦ã€‚å¤šé¡¹å…¬å¼€åŸºå‡†æµ‹è¯•ç»“æœæ˜¾ç¤ºï¼ŒScatterADåœ¨å¤šå˜é‡æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹é¢†åŸŸè¾¾åˆ°äº†æœ€å…ˆè¿›(state-of-the-art)çš„æ€§èƒ½ï¼Œä¸ºå¤„ç†å¤æ‚å·¥ä¸šåœºæ™¯ä¸‹çš„å¼‚å¸¸æ•£å¸ƒæä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025)",
      "pdf_url": "https://arxiv.org/pdf/2509.24414v1",
      "published_date": "2025-09-29 08:03:03 UTC",
      "updated_date": "2025-09-29 08:03:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:05:19.655009+00:00"
    },
    {
      "arxiv_id": "2509.25282v2",
      "title": "Toward Causal-Visual Programming: Enhancing Agentic Reasoning in Low-Code Environments",
      "title_zh": "è¿ˆå‘å› æœè§†è§‰ç¼–ç¨‹ï¼šå¢å¼ºä½ä»£ç ç¯å¢ƒä¸‹çš„æ™ºèƒ½ä½“æ¨ç†",
      "authors": [
        "Jiexi Xu",
        "Jiaqi Liu",
        "Lanruo Wang",
        "Su Liu"
      ],
      "abstract": "Large language model (LLM) agents are increasingly capable of orchestrating complex tasks in low-code environments. However, these agents often exhibit hallucinations and logical inconsistencies because their inherent reasoning mechanisms rely on probabilistic associations rather than genuine causal understanding. This paper introduces a new programming paradigm: Causal-Visual Programming (CVP), designed to address this fundamental issue by explicitly introducing causal structures into the workflow design. CVP allows users to define a simple \"world model\" for workflow modules through an intuitive low-code interface, effectively creating a Directed Acyclic Graph (DAG) that explicitly defines the causal relationships between modules. This causal graph acts as a crucial constraint during the agent's reasoning process, anchoring its decisions to a user-defined causal structure and significantly reducing logical errors and hallucinations by preventing reliance on spurious correlations. To validate the effectiveness of CVP, we designed a synthetic experiment that simulates a common real-world problem: a distribution shift between the training and test environments. Our results show that a causally anchored model maintained stable accuracy in the face of this shift, whereas a purely associative baseline model that relied on probabilistic correlations experienced a significant performance drop. The primary contributions of this study are: a formal definition of causal structures for workflow modules; the proposal and implementation of a CVP framework that anchors agent reasoning to a user-defined causal graph; and empirical evidence demonstrating the framework's effectiveness in enhancing agent robustness and reducing errors caused by causal confusion in dynamic environments. CVP offers a viable path toward building more interpretable, reliable, and trustworthy AI agents.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Causal-Visual Programming (CVP)ï¼Œæ—¨åœ¨è§£å†³Large language model (LLM) æ™ºèƒ½ä½“åœ¨ä½ä»£ç ç¯å¢ƒä¸­å› ä¾èµ–æ¦‚ç‡å…³è”è€Œéå› æœç†è§£æ‰€å¯¼è‡´çš„å¹»è§‰ä¸é€»è¾‘ä¸ä¸€è‡´é—®é¢˜ã€‚é€šè¿‡ç›´è§‚çš„ä½ä»£ç ç•Œé¢ï¼Œç”¨æˆ·å¯ä»¥ä¸ºå·¥ä½œæµæ¨¡å—å®šä¹‰â€œä¸–ç•Œæ¨¡å‹â€ï¼Œä»è€Œæ„å»ºä¸€ä¸ªæ˜ç¡®æ¨¡å—é—´å› æœå…³ç³»çš„Directed Acyclic Graph (DAG)ã€‚è¯¥å› æœå›¾åœ¨æ™ºèƒ½ä½“çš„æ¨ç†è¿‡ç¨‹ä¸­èµ·åˆ°å…³é”®çº¦æŸä½œç”¨ï¼Œå°†å…¶å†³ç­–é”šå®šåœ¨é¢„å®šä¹‰çš„å› æœç»“æ„ä¸Šï¼Œæœ‰æ•ˆå‡å°‘äº†é€»è¾‘é”™è¯¯å¹¶é˜²æ­¢å¯¹ä¼ªç›¸å…³æ€§çš„ä¾èµ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨é¢å¯¹åˆ†å¸ƒåç§» (distribution shift) çš„æŒ‘æˆ˜æ—¶ï¼ŒCVP é”šå®šçš„æ¨¡å‹èƒ½ä¿æŒç¨³å®šçš„å‡†ç¡®ç‡ï¼Œè€Œçº¯å…³è”åŸºçº¿æ¨¡å‹åˆ™å‡ºç°äº†æ˜¾è‘—çš„æ€§èƒ½ä¸‹é™ã€‚è¯¥ç ”ç©¶ä¸ä»…ä¸ºå·¥ä½œæµæ¨¡å—æä¾›äº†å½¢å¼åŒ–çš„å› æœå®šä¹‰ï¼Œä¹Ÿä¸ºæ„å»ºæ›´å…·é²æ£’æ€§ã€å¯è§£é‡Šæ€§å’Œå¯ä¿¡åº¦çš„ AI æ™ºèƒ½ä½“æä¾›äº†æœ‰æ•ˆçš„å®ç°è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "5 pages, 1 table",
      "pdf_url": "https://arxiv.org/pdf/2509.25282v2",
      "published_date": "2025-09-29 08:01:31 UTC",
      "updated_date": "2025-10-08 06:59:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:06:03.187804+00:00"
    },
    {
      "arxiv_id": "2509.24411v1",
      "title": "Hybrid Layer-Wise ANN-SNN With Surrogate Spike Encoding-Decoding Structure",
      "title_zh": "åŸºäºä»£ç†è„‰å†²ç¼–è§£ç ç»“æ„çš„æ··åˆé€å±‚ ANN-SNN",
      "authors": [
        "Nhan T. Luu",
        "Duong T. Luu",
        "Pham Ngoc Nam",
        "Truong Cong Thang"
      ],
      "abstract": "Spiking Neural Networks (SNNs) have gained significant traction in both computational neuroscience and artificial intelligence for their potential in energy-efficient computing. In contrast, artificial neural networks (ANNs) excel at gradient-based optimization and high accuracy. This contrast has consequently led to a growing subfield of hybrid ANN-SNN research. However, existing hybrid approaches often rely on either a strict separation between ANN and SNN components or employ SNN-only encoders followed by ANN classifiers due to the constraints of non-differentiability of spike encoding functions, causing prior hybrid architectures to lack deep layer-wise cooperation during backpropagation. To address this gap, we propose a novel hybrid ANN-SNN framework that integrates layer-wise encode-decode SNN blocks within conventional ANN pipelines. Central to our method is the use of surrogate gradients for a bit-plane-based spike encoding function, enabling end-to-end differentiable training across ANN and SNN layers. This design achieves competitive accuracy with state-of-the-art pure ANN and SNN models while retaining the potential efficiency and temporal representation benefits of spiking computation. To the best of our knowledge, this is the first implementation of a surrogate gradient for bit plane coding specifically and spike encoder interface in general to be utilized in the context of hybrid ANN-SNN, successfully leading to a new class of hybrid models that pave new directions for future research.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„å±‚çº§æ··åˆ ANN-SNN æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ··åˆæ¶æ„å› è„‰å†²ç¼–ç å‡½æ•°ä¸å¯å¾®è€Œå¯¼è‡´çš„å±‚é—´åä½œä¸åå‘ä¼ æ’­å—é˜»é—®é¢˜ã€‚æ ¸å¿ƒåˆ›æ–°åœ¨äºä¸ºåŸºäº bit-plane çš„è„‰å†²ç¼–ç å‡½æ•°å¼•å…¥äº†æ›¿ä»£æ¢¯åº¦ (surrogate gradients)ï¼Œä»è€Œå®ç°äº†è·¨ ANN å’Œ SNN å±‚çš„ç«¯åˆ°ç«¯å¯å¾®åˆ†è®­ç»ƒã€‚è¯¥æ¶æ„é€šè¿‡åœ¨å¸¸è§„ ANN æµæ°´çº¿ä¸­åµŒå…¥å±‚çº§åŒ–çš„ SNN ç¼–è§£ç æ¨¡å—ï¼Œå……åˆ†ç»“åˆäº† ANN çš„é«˜ç²¾åº¦ä¼˜åŒ–èƒ½åŠ›ä¸ SNN çš„ä½åŠŸè€—åŠæ—¶é—´è¡¨ç¤ºä¼˜åŠ¿ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ä¿æŒé«˜æ•ˆç‡çš„åŒæ—¶ï¼Œå…¶å‡†ç¡®ç‡å¯ä¸å½“å‰æœ€å…ˆè¿›çš„çº¯ ANN å’Œ SNN æ¨¡å‹ç›¸åª²ç¾ã€‚è¯¥å·¥ä½œé¦–æ¬¡å®ç°äº†é’ˆå¯¹ bit-plane ç¼–ç åŠè„‰å†²ç¼–ç å™¨æ¥å£çš„æ›¿ä»£æ¢¯åº¦æ–¹æ¡ˆï¼Œä¸ºæœªæ¥æ··åˆç¥ç»ç½‘ç»œçš„æ·±åº¦èåˆä¸ç ”ç©¶å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.NE",
      "comment": "Work under peer-review",
      "pdf_url": "https://arxiv.org/pdf/2509.24411v1",
      "published_date": "2025-09-29 07:57:58 UTC",
      "updated_date": "2025-09-29 07:57:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:05:32.792789+00:00"
    },
    {
      "arxiv_id": "2509.24405v1",
      "title": "Multilingual Text-to-SQL: Benchmarking the Limits of Language Models with Collaborative Language Agents",
      "title_zh": "å¤šè¯­è¨€ Text-to-SQLï¼šåˆ©ç”¨åä½œè¯­è¨€æ™ºèƒ½ä½“è¯„ä¼°è¯­è¨€æ¨¡å‹çš„æ€§èƒ½æé™",
      "authors": [
        "Khanh Trinh Pham",
        "Thu Huong Nguyen",
        "Jun Jo",
        "Quoc Viet Hung Nguyen",
        "Thanh Tam Nguyen"
      ],
      "abstract": "Text-to-SQL enables natural access to databases, yet most benchmarks are English-only, limiting multilingual progress. We introduce MultiSpider 2.0, extending Spider 2.0 to eight languages (English, German, French, Spanish, Portuguese, Japanese, Chinese, Vietnamese). It preserves Spider 2.0's structural difficulty while adding linguistic and dialectal variability, demanding deeper reasoning for complex SQL. On this benchmark, state-of-the-art LLMs (such as DeepSeek-R1 and OpenAI o1) reach only 4\\% execution accuracy when relying on intrinsic reasoning, versus 60\\% on MultiSpider 1.0. Therefore, we provide a collaboration-driven language agents baseline that iteratively refines queries, improving accuracy to 15\\%. These results reveal a substantial multilingual gap and motivate methods that are robust across languages and ready for real-world enterprise deployment. Our benchmark is available at https://github.com/phkhanhtrinh23/Multilingual_Text_to_SQL.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Text-to-SQL åŸºå‡†æµ‹è¯•å¤šé›†ä¸­äºè‹±æ–‡çš„å±€é™æ€§ï¼Œæ¨å‡ºäº† MultiSpider 2.0ï¼Œå°† Spider 2.0 æ‰©å±•è‡³åŒ…æ‹¬ä¸­æ–‡ã€å¾·æ–‡ã€è¶Šå—è¯­åœ¨å†…çš„å…«ç§è¯­è¨€ã€‚è¯¥åŸºå‡†åœ¨ä¿ç•™ Spider 2.0 ç»“æ„å¤æ‚æ€§çš„åŸºç¡€ä¸Šï¼Œå¼•å…¥äº†è¯­è¨€å’Œæ–¹è¨€çš„å¤šæ ·æ€§ï¼Œå¯¹å¤æ‚ SQL çš„æ·±åº¦æ¨ç†æå‡ºäº†æ›´é«˜æŒ‘æˆ˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå³ä¾¿å¦‚ DeepSeek-R1 å’Œ OpenAI o1 ç­‰æœ€å…ˆè¿›çš„å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ä»…ä¾èµ–å†…åœ¨æ¨ç†æ—¶ï¼Œæ‰§è¡Œå‡†ç¡®ç‡ä¹Ÿä»…ä¸º 4%ï¼Œç›¸æ¯” MultiSpider 1.0 è¡¨ç°å¤§å¹…ä¸‹é™ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿæä¾›äº†ä¸€ä¸ªåŸºäºåä½œçš„è¯­è¨€æ™ºèƒ½ä½“(collaboration-driven language agents)åŸºå‡†çº¿ï¼Œé€šè¿‡è¿­ä»£ä¼˜åŒ–æŸ¥è¯¢å°†å‡†ç¡®ç‡æå‡è‡³ 15%ã€‚è¿™äº›å‘ç°æ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨å¤šè¯­è¨€å¤„ç†ä¸Šçš„å·¨å¤§å·®è·ï¼Œå¹¶å¼ºè°ƒäº†å¼€å‘è·¨è¯­è¨€é²æ£’æ€§å¼ºä¸”é€‚ç”¨äºçœŸå®ä¼ä¸šéƒ¨ç½²çš„ Text-to-SQL æ–¹æ³•çš„è¿«åˆ‡æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DB",
        "cs.ET",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24405v1",
      "published_date": "2025-09-29 07:50:39 UTC",
      "updated_date": "2025-09-29 07:50:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:05:34.385332+00:00"
    },
    {
      "arxiv_id": "2509.24394v2",
      "title": "The 2025 OpenAI Preparedness Framework does not guarantee any AI risk mitigation practices: a proof-of-concept for affordance analyses of AI safety policies",
      "title_zh": "2025 å¹´ OpenAI é¢„å¤‡æ¡†æ¶æ— æ³•ä¿éšœä»»ä½• AI é£é™©ç¼“è§£å®è·µï¼šAI å®‰å…¨æ”¿ç­–å¯ä¾›æ€§åˆ†æçš„æ¦‚å¿µéªŒè¯",
      "authors": [
        "Sam Coggins",
        "Alexander K. Saeri",
        "Katherine A. Daniell",
        "Lorenn P. Ruster",
        "Jessie Liu",
        "Jenny L. Davis"
      ],
      "abstract": "Prominent AI companies are producing 'safety frameworks' as a type of voluntary self-governance. These statements purport to establish risk thresholds and safety procedures for the development and deployment of highly capable AI. Understanding which AI risks are covered and what actions are allowed, refused, demanded, encouraged, or discouraged by these statements is vital for assessing how these frameworks actually govern AI development and deployment. We draw on affordance theory to analyse the OpenAI 'Preparedness Framework Version 2' (April 2025) using the Mechanisms & Conditions model of affordances and the MIT AI Risk Repository. We find that this safety policy requests evaluation of a small minority of AI risks, encourages deployment of systems with 'Medium' capabilities for unintentionally enabling 'severe harm' (which OpenAI defines as >1000 deaths or >$100B in damages), and allows OpenAI's CEO to deploy even more dangerous capabilities. These findings suggest that effective mitigation of AI risks requires more robust governance interventions beyond current industry self-regulation. Our affordance analysis provides a replicable method for evaluating what safety frameworks actually permit versus what they claim.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡å¯¹2025å¹´4æœˆå‘å¸ƒçš„OpenAI Preparedness Framework Version 2è¿›è¡ŒAffordance analysisï¼Œæ­ç¤ºäº†è¯¥è‡ªå¾‹æ€§å®‰å…¨æ”¿ç­–åœ¨ç¼“è§£AIé£é™©æ–¹é¢çš„æ˜¾è‘—å±€é™æ€§ã€‚ä½œè€…é‡‡ç”¨äº†Affordance theoryä¸­çš„Mechanisms & Conditionsæ¨¡å‹ï¼Œå¹¶ç»“åˆMIT AI Risk Repositoryï¼Œç³»ç»Ÿè¯„ä¼°äº†æ¡†æ¶å¯¹AIé£é™©çš„è¦†ç›–ç¨‹åº¦åŠå®é™…çº¦æŸåŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œè¯¥æ¡†æ¶ä»…è¦æ±‚è¯„ä¼°æå°‘æ•°AIé£é™©ï¼Œå¹¶é¼“åŠ±éƒ¨ç½²åœ¨éæ•…æ„æƒ…å†µä¸‹å¯èƒ½å¯¼è‡´è¶…è¿‡1000äººæ­»äº¡æˆ–1000äº¿ç¾å…ƒæŸå¤±ï¼ˆè¢«å®šä¹‰ä¸ºSevere harmï¼‰çš„â€œMediumâ€èƒ½åŠ›ç³»ç»Ÿã€‚æ­¤å¤–ï¼Œæ¡†æ¶è¿˜èµ‹äºˆäº†CEOéƒ¨ç½²ç”šè‡³æ›´å±é™©ç³»ç»Ÿçš„è£é‡æƒï¼Œè¡¨æ˜å½“å‰çš„è¡Œä¸šè‡ªå¾‹éš¾ä»¥ä¿éšœAIçš„å®‰å…¨æ€§ã€‚ç ”ç©¶ç»“è®ºæŒ‡å‡ºï¼Œæœ‰æ•ˆçš„AIé£é™©ç¼“è§£éœ€è¦æ¯”è¡Œä¸šè‡ªå¾‹æ›´å¼ºæœ‰åŠ›çš„å¤–éƒ¨æ²»ç†å¹²é¢„ï¼Œå¹¶è¯æ˜äº†Affordance analysisæ˜¯è¯„ä¼°å„ç±»å®‰å…¨æ¡†æ¶å®é™…å‡†è®¸è¡Œä¸ºä¸å®£ç§°ç›®æ ‡ä¹‹é—´å·®è·çš„å¯å¤åˆ¶æ–¹æ³•ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "19 pages, 5 tables, 1 figure; minor ambiguities clarified, typos corrected, author affiliations added",
      "pdf_url": "https://arxiv.org/pdf/2509.24394v2",
      "published_date": "2025-09-29 07:42:10 UTC",
      "updated_date": "2025-10-13 02:00:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:06:31.860280+00:00"
    },
    {
      "arxiv_id": "2509.24393v1",
      "title": "Towards Safe Reasoning in Large Reasoning Models via Corrective Intervention",
      "title_zh": "é€šè¿‡çº æ­£æ€§å¹²é¢„å®ç°å¤§è¯­è¨€æ¨ç†æ¨¡å‹çš„å®‰å…¨æ¨ç†",
      "authors": [
        "Yichi Zhang",
        "Yue Ding",
        "Jingwen Yang",
        "Tianwei Luo",
        "Dongbai Li",
        "Ranjie Duan",
        "Qiang Liu",
        "Hang Su",
        "Yinpeng Dong",
        "Jun Zhu"
      ],
      "abstract": "Although Large Reasoning Models (LRMs) have progressed in solving complex problems, their chain-of-thought (CoT) reasoning often contains harmful content that can persist even when the final responses appear safe. We show that this issue still remains in existing methods which overlook the unique significance of safe reasoning, undermining their trustworthiness and posing potential risks in applications if unsafe reasoning is accessible for and exploited by malicious users. We therefore shift our focus to aligning the safety of reasoning itself in this paper and explore process supervision as the solution. However, simply rewarding safe reasoning proves inadequate due to low rollout diversity and limited training signals. To tackle this challenge, we first delve into the characteristics of safe reasoning and uncover several critical insights that 1) safe reasoning is often consolidated by a few critical steps of safety triggers; 2) compliance cues strongly correlate with unsafe continuations; and 3) corrective interventions reliably steer unsafe trajectories towards safer traces. Motivated by these, we propose Intervened Preference Optimization (IPO), an alignment method that enforces safe reasoning by substituting compliance steps with safety triggers and constructing pairs for preference learning with strong signals. Experiments on jailbreak and adversarial safety benchmarks demonstrate that IPO remarkably improves overall safety regarding both reasoning and responses, outperforming SFT-based and RL-based baselines with a relative reduction of over 30% in harmfulness, while preserving excellent performance across diverse reasoning tasks. The results highlight the importance of explicit alignment for reasoning and provide a practical path to safer LRMs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹æ¨ç†æ¨¡å‹(Large Reasoning Models, LRMs)åœ¨é“¾å¼æ€ç»´(Chain-of-Thought, CoT)æ¨ç†è¿‡ç¨‹ä¸­å¯èƒ½äº§ç”Ÿæœ‰å®³å†…å®¹çš„é—®é¢˜ï¼ŒæŒ‡å‡ºå³ä½¿æœ€ç»ˆå›ç­”å®‰å…¨ï¼Œä¸å®‰å…¨çš„æ¨ç†è¿‡ç¨‹ä»ä¼šæŸå®³æ¨¡å‹çš„å¯é æ€§ã€‚ç ”ç©¶å‘ç°å®‰å…¨æ¨ç†é€šå¸¸ç”±å…³é”®çš„â€œå®‰å…¨è§¦å‘å™¨â€(safety triggers)å·©å›ºï¼Œè€Œåˆè§„æ€§çº¿ç´¢(compliance cues)åˆ™ä¸ä¸å®‰å…¨å†…å®¹å¼ºç›¸å…³ã€‚åŸºäºè¿™äº›æ´å¯Ÿï¼Œä½œè€…æå‡ºäº†å¹²é¢„åå¥½ä¼˜åŒ–(Intervened Preference Optimization, IPO)ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡çº æ­£æ€§å¹²é¢„(corrective interventions)å°†ä¸å®‰å…¨è½¨è¿¹è½¬å‘å®‰å…¨è½¨è¿¹çš„å¯¹é½æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡ç”¨å®‰å…¨è§¦å‘å™¨æ›¿æ¢åˆè§„æ­¥éª¤ï¼Œæ„å»ºå…·æœ‰å¼ºä¿¡å·çš„åå¥½å­¦ä¹ å¯¹ï¼Œä»è€Œå®ç°å¯¹æ¨ç†è¿‡ç¨‹æœ¬èº«çš„å®‰å…¨å¯¹é½ã€‚åœ¨è¶Šç‹±å’Œå¯¹æŠ—æ€§å®‰å…¨åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒIPOåœ¨æ¨ç†è¿‡ç¨‹å’Œæœ€ç»ˆå“åº”çš„å®‰å…¨æ€§ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼Œç›¸æ¯”SFTå’ŒRLåŸºå‡†æ¨¡å‹ï¼Œå…¶æœ‰å®³æ€§ç›¸å¯¹é™ä½äº†30%ä»¥ä¸Šã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†å¯¹æ¨ç†è¿‡ç¨‹è¿›è¡Œæ˜¾å¼å¯¹é½çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºæ„å»ºæ›´å®‰å…¨çš„æ¨ç†æ¨¡å‹æä¾›äº†ä¸€æ¡åˆ‡å®å¯è¡Œçš„è·¯å¾„ï¼ŒåŒæ—¶åœ¨å„ç±»æ¨ç†ä»»åŠ¡ä¸­ä¿æŒäº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24393v1",
      "published_date": "2025-09-29 07:41:09 UTC",
      "updated_date": "2025-09-29 07:41:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:05:44.897007+00:00"
    },
    {
      "arxiv_id": "2509.24389v1",
      "title": "LLaDA-MoE: A Sparse MoE Diffusion Language Model",
      "title_zh": "LLaDA-MoEï¼šä¸€ç§ç¨€ç–æ··åˆä¸“å®¶æ‰©æ•£è¯­è¨€æ¨¡å‹",
      "authors": [
        "Fengqi Zhu",
        "Zebin You",
        "Yipeng Xing",
        "Zenan Huang",
        "Lin Liu",
        "Yihong Zhuang",
        "Guoshan Lu",
        "Kangyu Wang",
        "Xudong Wang",
        "Lanning Wei",
        "Hongrui Guo",
        "Jiaqi Hu",
        "Wentao Ye",
        "Tieyuan Chen",
        "Chenchen Li",
        "Chengfu Tang",
        "Haibo Feng",
        "Jun Hu",
        "Jun Zhou",
        "Xiaolu Zhang",
        "Zhenzhong Lan",
        "Junbo Zhao",
        "Da Zheng",
        "Chongxuan Li",
        "Jianguo Li",
        "Ji-Rong Wen"
      ],
      "abstract": "We introduce LLaDA-MoE, a large language diffusion model with the Mixture-of-Experts (MoE) architecture, trained from scratch on approximately 20T tokens. LLaDA-MoE achieves competitive performance with significantly reduced computational overhead by maintaining a 7B-parameter capacity while activating only 1.4B parameters during inference. Our empirical evaluation reveals that LLaDA-MoE achieves state-of-the-art performance among diffusion language models with larger parameters, surpassing previous diffusion language models LLaDA, LLaDA 1.5, and Dream across multiple benchmarks. The instruct-tuned model LLaDA-MoE-7B-A1B-Instruct demonstrates capabilities comparable to Qwen2.5-3B-Instruct in knowledge understanding, code generation, mathematical reasoning, agent and alignment tasks, despite using fewer active parameters. Our results show that integrating a sparse MoE architecture into the training objective of masked diffusion language models still brings out MoE's strengths under efficient inference with few active parameters, and opens ample room for further exploration of diffusion language models. LLaDA-MoE models are available at Huggingface.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† LLaDA-MoEï¼Œè¿™æ˜¯ä¸€æ¬¾é‡‡ç”¨æ··åˆä¸“å®¶ (Mixture-of-Experts) æ¶æ„çš„å¤§å‹è¯­è¨€æ‰©æ•£æ¨¡å‹ã€‚è¯¥æ¨¡å‹åœ¨çº¦ 20T token ä¸Šè¿›è¡Œäº†ä»é›¶å¼€å§‹çš„è®­ç»ƒï¼Œæ—¨åœ¨é€šè¿‡ç¨€ç–æ¶æ„ä¼˜åŒ–æ‰©æ•£è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ä¸æ•ˆç‡ã€‚LLaDA-MoE åœ¨ä¿æŒ 7B å‚æ•°æ€»å®¹é‡çš„åŒæ—¶ï¼Œæ¨ç†æ—¶ä»…æ¿€æ´» 1.4B å‚æ•°ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—å¼€é”€ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒLLaDA-MoE åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºæ­¤å‰çš„æ‰©æ•£è¯­è¨€æ¨¡å‹ LLaDAã€LLaDA 1.5 å’Œ Dreamã€‚ç»è¿‡æŒ‡ä»¤å¾®è°ƒçš„ LLaDA-MoE-7B-A1B-Instruct åœ¨çŸ¥è¯†ç†è§£ã€ä»£ç ç”Ÿæˆã€æ•°å­¦æ¨ç†åŠæ™ºèƒ½ä½“ä»»åŠ¡ä¸­å±•ç¤ºäº†ä¸ Qwen2.5-3B-Instruct ç›¸å½“çš„èƒ½åŠ›ã€‚ç ”ç©¶ç»“æœè¯æ˜ï¼Œå°†ç¨€ç– MoE æ¶æ„é›†æˆåˆ°æ©ç æ‰©æ•£è¯­è¨€æ¨¡å‹ (masked diffusion language models) çš„è®­ç»ƒç›®æ ‡ä¸­ï¼Œèƒ½åœ¨ä¿æŒé«˜æ•ˆæ¨ç†çš„åŒæ—¶å……åˆ†å‘æŒ¥ MoE çš„ä¼˜åŠ¿ï¼Œä¸ºæ‰©æ•£è¯­è¨€æ¨¡å‹çš„è¿›ä¸€æ­¥å‘å±•æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24389v1",
      "published_date": "2025-09-29 07:38:59 UTC",
      "updated_date": "2025-09-29 07:38:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:06:46.240201+00:00"
    },
    {
      "arxiv_id": "2509.24385v1",
      "title": "Vid-LLM: A Compact Video-based 3D Multimodal LLM with Reconstruction-Reasoning Synergy",
      "title_zh": "Vid-LLMï¼šå…·æœ‰é‡å»º-æ¨ç†ååŒæ•ˆåº”çš„è½»é‡åŒ–è§†é¢‘ 3D å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Haijier Chen",
        "Bo Xu",
        "Shoujian Zhang",
        "Haoze Liu",
        "Jiaxuan Lin",
        "Jingrong Wang"
      ],
      "abstract": "Recent developments in Multimodal Large Language Models (MLLMs) have significantly improved Vision-Language (VL) reasoning in 2D domains. However, extending these capabilities to 3D scene understanding remains a major challenge. Existing 3D Multimodal Large Language Models (3D-MLLMs) often depend on 3D data inputs, which limits scalability and generalization. To address this limitation, we propose Vid-LLM, a video-based 3D-MLLM that directly processes video inputs without requiring external 3D data, making it practical for real-world deployment. In our method, the geometric prior are directly used to improve the performance of the sceen perception. To integrate the geometric cues into the MLLM compactly, we design a Cross-Task Adapter (CTA) module to align the 3D geometric priors with the vision-language representations. To ensure geometric consistency and integrity, we introduce a Metric Depth Model that recovers real-scale geometry from the reconstruction outputs. Finally, the model is fine-tuned with a two-stage distillation optimization strategy, realizing fast convergence and stabilizes training. Extensive experiments across diverse benchmarks verified the effectiveness of our method on 3D Question Answering, 3D Dense Captioning and 3D Visual Grounding tasks, demonstrating the superior multi-task capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Vid-LLMï¼Œä¸€ç§åŸºäºè§†é¢‘çš„3Då¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(3D-MLLM)ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹è¿‡åº¦ä¾èµ–å¤–éƒ¨3Dæ•°æ®æ‰€å¯¼è‡´çš„æ‰©å±•æ€§å’Œæ³›åŒ–æ€§å—é™é—®é¢˜ã€‚è¯¥æ¨¡å‹ç›´æ¥å¤„ç†è§†é¢‘è¾“å…¥ï¼Œé€šè¿‡åˆ©ç”¨å‡ ä½•å…ˆéªŒ(geometric prior)æå‡åœºæ™¯æ„ŸçŸ¥èƒ½åŠ›ï¼Œä½¿å…¶æ›´é€‚ç”¨äºçœŸå®ä¸–ç•Œçš„éƒ¨ç½²ã€‚ä¸ºäº†é«˜æ•ˆé›†æˆå‡ ä½•çº¿ç´¢ï¼Œç ”ç©¶è€…è®¾è®¡äº†è·¨ä»»åŠ¡é€‚é…å™¨(Cross-Task Adapter, CTA)æ¨¡å—ï¼Œå°†3Då‡ ä½•å…ˆéªŒä¸è§†è§‰è¯­è¨€è¡¨ç¤º(vision-language representations)è¿›è¡Œå¯¹é½ã€‚æ­¤å¤–ï¼Œæ¨¡å‹å¼•å…¥äº†å…¬åˆ¶æ·±åº¦æ¨¡å‹(Metric Depth Model)ä»¥æ¢å¤çœŸå®å°ºåº¦çš„å‡ ä½•ç»“æ„ï¼Œå¹¶é‡‡ç”¨ä¸¤é˜¶æ®µè’¸é¦ä¼˜åŒ–ç­–ç•¥æ¥å®ç°å¿«é€Ÿæ”¶æ•›å’Œç¨³å®šè®­ç»ƒã€‚åœ¨3D Question Answeringã€3D Dense Captioningå’Œ3D Visual Groundingç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¯æ˜äº†Vid-LLMçš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†å…¶å“è¶Šçš„3Dåœºæ™¯ç†è§£å’Œå¤šä»»åŠ¡å¤„ç†èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24385v1",
      "published_date": "2025-09-29 07:34:18 UTC",
      "updated_date": "2025-09-29 07:34:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:05:58.990122+00:00"
    },
    {
      "arxiv_id": "2509.24384v1",
      "title": "HarmMetric Eval: Benchmarking Metrics and Judges for LLM Harmfulness Assessment",
      "title_zh": "HarmMetric Evalï¼šå¤§è¯­è¨€æ¨¡å‹æœ‰å®³æ€§è¯„ä¼°æŒ‡æ ‡ä¸è¯„åˆ¤å™¨çš„åŸºå‡†æµ‹è¯•",
      "authors": [
        "Langqi Yang",
        "Tianhang Zheng",
        "Kedong Xiu",
        "Yixuan Chen",
        "Di Wang",
        "Puning Zhao",
        "Zhan Qin",
        "Kui Ren"
      ],
      "abstract": "The alignment of large language models (LLMs) with human values is critical for their safe deployment, yet jailbreak attacks can subvert this alignment to elicit harmful outputs from LLMs. In recent years, a proliferation of jailbreak attacks has emerged, accompanied by diverse metrics and judges to assess the harmfulness of the LLM outputs. However, the absence of a systematic benchmark to assess the quality and effectiveness of these metrics and judges undermines the credibility of the reported jailbreak effectiveness and other risks. To address this gap, we introduce HarmMetric Eval, a comprehensive benchmark designed to support both overall and fine-grained evaluation of harmfulness metrics and judges. Our benchmark includes a high-quality dataset of representative harmful prompts paired with diverse harmful and non-harmful model responses, alongside a flexible scoring mechanism compatible with various metrics and judges. With HarmMetric Eval, our extensive experiments uncover a surprising result: two conventional metrics--METEOR and ROUGE-1--outperform LLM-based judges in evaluating the harmfulness of model responses, challenging prevailing beliefs about LLMs' superiority in this domain. Our dataset is publicly available at https://huggingface.co/datasets/qusgo/HarmMetric_Eval, and the code is available at https://anonymous.4open.science/r/HarmMetric-Eval-4CBE.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† HarmMetric Evalï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æœ‰å®³æ€§æŒ‡æ ‡å’Œè¯„åˆ¤å™¨çš„ç»¼åˆåŸºå‡†ï¼Œå¡«è¡¥äº†ç°æœ‰è¶Šç‹±æ”»å‡»ï¼ˆjailbreak attacksï¼‰è¯„ä¼°ç¼ºä¹ç³»ç»Ÿæ€§åŸºå‡†çš„ç©ºç™½ã€‚è¯¥åŸºå‡†åŒ…å«ä¸€ä¸ªç”±ä»£è¡¨æ€§æœ‰å®³æç¤ºè¯åŠå…¶å¤šæ ·åŒ–æ¨¡å‹å“åº”ç»„æˆçš„é«˜è´¨é‡æ•°æ®é›†ï¼Œå¹¶æä¾›äº†ä¸€ç§å…¼å®¹å„ç§æŒ‡æ ‡å’Œè¯„åˆ¤å™¨çš„çµæ´»è¯„åˆ†æœºåˆ¶ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œç ”ç©¶å‘ç°äº†ä¸€ä¸ªå‡ºäººæ„æ–™çš„ç»“æœï¼šMETEOR å’Œ ROUGE-1 è¿™ä¸¤ä¸ªä¼ ç»ŸæŒ‡æ ‡åœ¨è¯„ä¼°æ¨¡å‹å“åº”çš„æœ‰å®³æ€§æ–¹é¢ä¼˜äºåŸºäº LLM çš„è¯„åˆ¤å™¨ã€‚è¿™ä¸€å‘ç°æŒ‘æˆ˜äº† LLMs åœ¨è¯¥é¢†åŸŸè¯„ä¼°ä¸­å…·æœ‰ä¼˜è¶Šæ€§çš„æ™®éè®¤çŸ¥ã€‚HarmMetric Eval ä¸ºç¡®ä¿å¤§è¯­è¨€æ¨¡å‹å®‰å…¨å¯¹é½åŠæœ‰å®³æ€§è¯„ä¼°çš„ä¿¡åº¦æä¾›äº†é‡è¦çš„åŸºå‡†æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24384v1",
      "published_date": "2025-09-29 07:34:01 UTC",
      "updated_date": "2025-09-29 07:34:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:06:04.496701+00:00"
    },
    {
      "arxiv_id": "2509.24382v1",
      "title": "REALIGN: Regularized Procedure Alignment with Matching Video Embeddings via Partial Gromov-Wasserstein Optimal Transport",
      "title_zh": "REALIGNï¼šåŸºäºéƒ¨åˆ† Gromov-Wasserstein æœ€ä¼˜ä¼ è¾“çš„è§†é¢‘åµŒå…¥åŒ¹é…æ­£åˆ™åŒ–è¿‡ç¨‹å¯¹é½",
      "authors": [
        "Soumyadeep Chandra",
        "Kaushik Roy"
      ],
      "abstract": "Learning from procedural videos remains a core challenge in self-supervised representation learning, as real-world instructional data often contains background segments, repeated actions, and steps presented out of order. Such variability violates the strong monotonicity assumptions underlying many alignment methods. Prior state-of-the-art approaches, such as OPEL, leverage Kantorovich Optimal Transport (KOT) to build frame-to-frame correspondences, but rely solely on feature similarity and fail to capture the higher-order temporal structure of a task. In this paper, we introduce REALIGN, a self-supervised framework for procedure learning based on Regularized Fused Partial Gromov-Wasserstein Optimal Transport (R-FPGWOT). In contrast to KOT, our formulation jointly models visual correspondences and temporal relations under a partial alignment scheme, enabling robust handling of irrelevant frames, repeated actions, and non-monotonic step orders common in instructional videos. To stabilize training, we integrate FPGWOT distances with inter-sequence contrastive learning, avoiding the need for multiple regularizers and preventing collapse to degenerate solutions. Across egocentric (EgoProceL) and third-person (ProceL, CrossTask) benchmarks, REALIGN achieves up to 18.9% average F1-score improvements and over 30% temporal IoU gains, while producing more interpretable transport maps that preserve key-step orderings and filter out noise.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹ä»æµç¨‹è§†é¢‘ï¼ˆprocedural videosï¼‰ä¸­å­¦ä¹ è‡ªç›‘ç£è¡¨ç¤ºçš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†èƒŒæ™¯ç‰‡æ®µã€é‡å¤åŠ¨ä½œå’Œä¹±åºæ­¥éª¤æ—¶ï¼Œä¼ ç»Ÿå¯¹é½æ–¹æ³•å¾€å¾€å› ä¸ºå¼ºå•è°ƒæ€§å‡è®¾è€Œå¤±æ•ˆã€‚é‰´äºç°æœ‰æŠ€æœ¯å¦‚ OPEL é‡‡ç”¨çš„ Kantorovich Optimal Transport (KOT) ä»…ä¾èµ–ç‰¹å¾ç›¸ä¼¼åº¦è€Œå¿½ç•¥é«˜é˜¶æ—¶é—´ç»“æ„ï¼Œæœ¬æ–‡æå‡ºäº† REALIGN æ¡†æ¶ã€‚è¯¥æ¡†æ¶åŸºäº Regularized Fused Partial Gromov-Wasserstein Optimal Transport (R-FPGWOT)ï¼Œé€šè¿‡éƒ¨åˆ†å¯¹é½æ–¹æ¡ˆå…±åŒå»ºæ¨¡è§†è§‰å¯¹åº”å…³ç³»å’Œæ—¶é—´å…³ç³»ã€‚REALIGN èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†æ•™å­¦è§†é¢‘ä¸­å¸¸è§çš„æ— å…³å¸§ã€é‡å¤åŠ¨ä½œå’Œéå•è°ƒæ­¥éª¤é¡ºåºï¼Œå¢å¼ºäº†å¯¹å¤æ‚ç°å®åœºæ™¯çš„é²æ£’æ€§ã€‚ä¸ºäº†ç¨³å®šè®­ç»ƒå¹¶é˜²æ­¢å´©æºƒï¼Œç ”ç©¶å°† FPGWOT è·ç¦»ä¸åºåˆ—é—´å¯¹æ¯”å­¦ä¹ ï¼ˆinter-sequence contrastive learningï¼‰ç›¸ç»“åˆï¼Œä»è€Œé¿å…äº†å¯¹å¤šä¸ªæ­£åˆ™åŒ–å™¨çš„ä¾èµ–ã€‚åœ¨ EgoProceLã€ProceL å’Œ CrossTask ç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒREALIGN å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒF1 åˆ†æ•°æœ€é«˜å¹³å‡æé«˜ 18.9%ï¼Œæ—¶é—´ IoU å¢ç›Šè¶…è¿‡ 30%ã€‚å®éªŒç»“æœè¯æ˜è¯¥æ–¹æ³•ä¸ä»…æé«˜äº†å¯¹é½ç²¾åº¦ï¼Œè¿˜ç”Ÿæˆäº†æ›´å…·å¯è§£é‡Šæ€§çš„ä¼ è¾“å›¾ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¿‡æ»¤å™ªå£°å¹¶å‡†ç¡®ä¿ç•™å…³é”®æ­¥éª¤é¡ºåºã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages, 4 figures, 6 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.24382v1",
      "published_date": "2025-09-29 07:32:14 UTC",
      "updated_date": "2025-09-29 07:32:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:07:01.156601+00:00"
    },
    {
      "arxiv_id": "2509.24377v1",
      "title": "Plan before Solving: Problem-Aware Strategy Routing for Mathematical Reasoning with LLMs",
      "title_zh": "å…ˆè§„åˆ’ï¼Œåæ±‚è§£ï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹æ•°å­¦æ¨ç†çš„é—®é¢˜æ„ŸçŸ¥ç­–ç•¥è·¯ç”±",
      "authors": [
        "Shihao Qi",
        "Jie Ma",
        "Ziang Yin",
        "Lingling Zhang",
        "Jian Zhang",
        "Jun Liu",
        "Feng Tian",
        "Tongliang Liu"
      ],
      "abstract": "Existing methods usually leverage a fixed strategy, such as natural language reasoning, code-augmented reasoning, tool-integrated reasoning, or ensemble-based reasoning, to guide Large Language Models (LLMs) to perform mathematical reasoning. Our analysis reveals that the single strategy cannot adapt to problem-specific requirements and thus overlooks the trade-off between effectiveness and efficiency. To address these issues, we propose Planning and Routing through Instance-Specific Modeling (PRISM), a novel framework that decouples mathematical reasoning into two stages: strategy planning and targeted execution. Specifically, we first curate a multi-strategy preference dataset, which we call MathStrat, capturing correctness, process quality, and computational efficiency for each problem--strategy pair. Then, we train a lightweight Strategy Adapter based on the dataset to obtain confidence distributions over the mentioned four reasoning strategies. At inference time, an adaptive routing policy dynamically tailors the reasoning approach based on predictor confidence. It directs the model to use single-strategy execution for high-confidence predictions, dual-strategy verification for competitive scenarios, or comprehensive multi-strategy exploration for uncertain cases. Extensive experiments across five mathematical reasoning benchmarks demonstrate that PRISM consistently outperforms individual strategies and ensemble baselines, achieving improvements ranging from 0.9% to 7.6% across different base models. The adaptive routing approach shows particularly strong benefits for mathematical reasoning tasks across diverse model architectures. Our code is released at https://github.com/reml-group/PRISM.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†PRISMï¼ˆPlanning and Routing through Instance-Specific Modelingï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨è¿›è¡Œæ•°å­¦æ¨ç†æ—¶å› é‡‡ç”¨å›ºå®šç­–ç•¥è€Œæ— æ³•é€‚åº”ç‰¹å®šé—®é¢˜éœ€æ±‚ä¸”å¿½è§†æ•ˆèƒ½å¹³è¡¡çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ›æ–°æ€§åœ°å°†æ•°å­¦æ¨ç†è¿‡ç¨‹è§£æ„ä¸ºç­–ç•¥è§„åˆ’ä¸é’ˆå¯¹æ€§æ‰§è¡Œä¸¤ä¸ªé˜¶æ®µï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šæ„å»ºäº†åŒ…å«ç­–ç•¥æ­£ç¡®æ€§ã€è¿‡ç¨‹è´¨é‡åŠè®¡ç®—æ•ˆç‡çš„å¤šç­–ç•¥åå¥½æ•°æ®é›†MathStratã€‚é€šè¿‡è®­ç»ƒè½»é‡çº§çš„Strategy Adapterï¼ŒPRISMèƒ½åŸºäºä¿¡å¿ƒåº¦åˆ†å¸ƒåœ¨æ¨ç†é˜¶æ®µå®ç°åŠ¨æ€è·¯ç”±ï¼Œè‡ªä¸»é€‰æ‹©å•ç­–ç•¥æ‰§è¡Œã€åŒç­–ç•¥éªŒè¯æˆ–å…¨é¢å¤šç­–ç•¥æ¢ç´¢ã€‚åœ¨äº”ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPRISMåœ¨ä¸åŒåŸºç¡€æ¨¡å‹ä¸Šå‡ä¼˜äºå•ä¸€ç­–ç•¥å’Œé›†æˆåŸºçº¿ï¼Œæ€§èƒ½æå‡å¹…åº¦åœ¨0.9%è‡³7.6%ä¹‹é—´ã€‚è¯¥ç ”ç©¶è¯æ˜äº†è‡ªé€‚åº”è·¯ç”±æœºåˆ¶åœ¨å¤šæ ·åŒ–çš„æ¨¡å‹æ¶æ„ä¸­å¯¹æ•°å­¦æ¨ç†ä»»åŠ¡å…·æœ‰æ˜¾è‘—çš„æå‡ä½œç”¨ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24377v1",
      "published_date": "2025-09-29 07:22:41 UTC",
      "updated_date": "2025-09-29 07:22:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:07:03.170735+00:00"
    },
    {
      "arxiv_id": "2509.24372v1",
      "title": "Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning",
      "title_zh": "å¤§è§„æ¨¡è¿›åŒ–ç­–ç•¥ï¼šè¶…è¶Šå¼ºåŒ–å­¦ä¹ çš„å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒ",
      "authors": [
        "Xin Qiu",
        "Yulu Gan",
        "Conor F. Hayes",
        "Qiyao Liang",
        "Elliot Meyerson",
        "Babak Hodjat",
        "Risto Miikkulainen"
      ],
      "abstract": "Fine-tuning pre-trained large language models (LLMs) for down-stream tasks is a critical step in the AI deployment pipeline. Reinforcement learning (RL) is arguably the most prominent fine-tuning method, contributing to the birth of many state-of-the-art LLMs. In contrast, evolution strategies (ES), which once showed comparable performance to RL on models with a few million parameters, was neglected due to the pessimistic perception of its scalability to larger models. In this work, we report the first successful attempt to scale up ES for fine-tuning the full parameters of LLMs, showing the surprising fact that ES can search efficiently over billions of parameters and outperform existing RL fine-tuning methods in multiple respects, including sample efficiency, tolerance to long-horizon rewards, robustness to different base LLMs, less tendency to reward hacking, and more stable performance across runs. It therefore serves as a basis to unlock a new direction in LLM fine-tuning beyond what current RL techniques provide. The source codes are provided at: https://github.com/VsonicV/es-fine-tuning-paper.",
      "tldr_zh": "è¯¥ç ”ç©¶æŠ¥å‘Šäº†é¦–æ¬¡æˆåŠŸå°†è¿›åŒ–ç­–ç•¥(Evolution Strategies, ES)æ‰©å±•åˆ°å¤§è¯­è¨€æ¨¡å‹(LLMs)å…¨å‚æ•°å¾®è°ƒçš„å°è¯•ï¼Œæ‰“ç ´äº†ESæ— æ³•æ‰©å±•è‡³æ•°åäº¿å‚æ•°è§„æ¨¡çš„ä¼ ç»Ÿè®¤çŸ¥ã€‚å®éªŒè¯æ˜ESèƒ½å¤Ÿé«˜æ•ˆåœ°åœ¨æ•°åäº¿å‚æ•°ç©ºé—´å†…è¿›è¡Œæœç´¢ï¼Œå¹¶åœ¨å¤šä¸ªç»´åº¦ä¸Šè¶…è¶Šäº†ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ (Reinforcement Learning, RL)å¾®è°ƒæ–¹æ³•ã€‚ä¸RLç›¸æ¯”ï¼ŒESåœ¨æ ·æœ¬æ•ˆç‡(sample efficiency)å’Œå¤„ç†é•¿å‘¨æœŸå¥–åŠ±(long-horizon rewards)æ–¹é¢è¡¨ç°å‡ºæ›´å¼ºçš„ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼ŒESå¯¹ä¸åŒåŸºç¡€æ¨¡å‹å…·æœ‰æ›´ä½³çš„é²æ£’æ€§(robustness)ï¼Œä¸”æ›´ä¸å®¹æ˜“å‡ºç°å¥–åŠ±ä½œå¼Š(reward hacking)ç°è±¡ï¼Œåœ¨å¤šæ¬¡è¿è¡Œä¸­è¡¨ç°å‡ºæ›´é«˜çš„ç¨³å®šæ€§ã€‚è¯¥é¡¹å·¥ä½œä¸ºRLä¹‹å¤–çš„LLMå¾®è°ƒé¢†åŸŸå¼€è¾Ÿäº†å…¨æ–°æ–¹å‘ï¼Œå¹¶è¯æ˜äº†ESä½œä¸ºä¸€ç§é«˜æ•ˆã€ç¨³å®šçš„æ›¿ä»£æ–¹æ¡ˆçš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "24 pages, including the appendix",
      "pdf_url": "https://arxiv.org/pdf/2509.24372v1",
      "published_date": "2025-09-29 07:19:34 UTC",
      "updated_date": "2025-09-29 07:19:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:07:08.095822+00:00"
    },
    {
      "arxiv_id": "2509.24369v1",
      "title": "From Satellite to Street: A Hybrid Framework Integrating Stable Diffusion and PanoGAN for Consistent Cross-View Synthesis",
      "title_zh": "ä»å«æ˜Ÿåˆ°è¡—é“ï¼šèåˆ Stable Diffusion ä¸ PanoGAN çš„è·¨è§†å›¾ä¸€è‡´æ€§åˆæˆæ··åˆæ¡†æ¶",
      "authors": [
        "Khawlah Bajbaa",
        "Abbas Anwar",
        "Muhammad Saqib",
        "Hafeez Anwar",
        "Nabin Sharma",
        "Muhammad Usman"
      ],
      "abstract": "Street view imagery has become an essential source for geospatial data collection and urban analytics, enabling the extraction of valuable insights that support informed decision-making. However, synthesizing street-view images from corresponding satellite imagery presents significant challenges due to substantial differences in appearance and viewing perspective between these two domains. This paper presents a hybrid framework that integrates diffusion-based models and conditional generative adversarial networks to generate geographically consistent street-view images from satellite imagery. Our approach uses a multi-stage training strategy that incorporates Stable Diffusion as the core component within a dual-branch architecture. To enhance the framework's capabilities, we integrate a conditional Generative Adversarial Network (GAN) that enables the generation of geographically consistent panoramic street views. Furthermore, we implement a fusion strategy that leverages the strengths of both models to create robust representations, thereby improving the geometric consistency and visual quality of the generated street-view images. The proposed framework is evaluated on the challenging Cross-View USA (CVUSA) dataset, a standard benchmark for cross-view image synthesis. Experimental results demonstrate that our hybrid approach outperforms diffusion-only methods across multiple evaluation metrics and achieves competitive performance compared to state-of-the-art GAN-based methods. The framework successfully generates realistic and geometrically consistent street-view images while preserving fine-grained local details, including street markings, secondary roads, and atmospheric elements such as clouds.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ—¨åœ¨è§£å†³ä»å«æ˜Ÿå›¾åƒç”Ÿæˆè¡—é“æ™¯è§‚å›¾åƒæ—¶é¢ä¸´çš„è§†è§‰å¤–è§‚å’Œè§†è§’å·®å¼‚å·¨å¤§é—®é¢˜çš„æ··åˆæ¡†æ¶ã€‚è¯¥æ–¹æ³•é‡‡ç”¨äº†å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œå°† Stable Diffusion ä½œä¸ºåŒåˆ†æ”¯æ¶æ„çš„æ ¸å¿ƒç»„ä»¶ï¼Œä»¥æ•æ‰å¤æ‚çš„è§†è§‰ç‰¹å¾ã€‚ä¸ºäº†å¢å¼ºåœ°ç†ä¸€è‡´æ€§ï¼Œç ”ç©¶è¿›ä¸€æ­¥æ•´åˆäº†æ¡ä»¶ Generative Adversarial Network (GAN)ï¼Œå®ç°äº†å…¨æ™¯è¡—é“æ™¯è§‚çš„ç”Ÿæˆã€‚é€šè¿‡ä¸€ç§èåˆç­–ç•¥ (fusion strategy)ï¼Œè¯¥æ¡†æ¶æœ‰æ•ˆç»“åˆäº†ä¸¤ç§æ¨¡å‹çš„ä¼˜åŠ¿ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆå›¾åƒçš„å‡ ä½•ä¸€è‡´æ€§ (geometric consistency) ä¸è§†è§‰è´¨é‡ã€‚åœ¨ Cross-View USA (CVUSA) æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼Œè¯¥æ··åˆæ–¹æ³•åœ¨å¤šé¡¹è¯„ä¼°æŒ‡æ ‡ä¸Šå‡ä¼˜äºå•ä¸€çš„æ‰©æ•£æ¨¡å‹ï¼Œå¹¶è¾¾åˆ°äº†ä¸æœ€å…ˆè¿› GAN æ–¹æ³•ç›¸å½“çš„æ°´å¹³ã€‚è¯¥æ¡†æ¶èƒ½å¤ŸæˆåŠŸç”Ÿæˆä¿ç•™è¡—é“æ ‡è®°ã€æ¬¡è¦é“è·¯å’Œäº‘å±‚ç­‰ç²¾ç»†å±€éƒ¨ç»†èŠ‚çš„é€¼çœŸè·¨è§†å›¾å›¾åƒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24369v1",
      "published_date": "2025-09-29 07:14:49 UTC",
      "updated_date": "2025-09-29 07:14:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:07:13.796237+00:00"
    },
    {
      "arxiv_id": "2509.24368v1",
      "title": "Watermarking Diffusion Language Models",
      "title_zh": "æ‰©æ•£è¯­è¨€æ¨¡å‹æ°´å°æŠ€æœ¯",
      "authors": [
        "Thibaud Gloaguen",
        "Robin Staab",
        "Nikola JovanoviÄ‡",
        "Martin Vechev"
      ],
      "abstract": "We introduce the first watermark tailored for diffusion language models (DLMs), an emergent LLM paradigm able to generate tokens in arbitrary order, in contrast to standard autoregressive language models (ARLMs) which generate tokens sequentially. While there has been much work in ARLM watermarking, a key challenge when attempting to apply these schemes directly to the DLM setting is that they rely on previously generated tokens, which are not always available with DLM generation. In this work we address this challenge by: (i) applying the watermark in expectation over the context even when some context tokens are yet to be determined, and (ii) promoting tokens which increase the watermark strength when used as context for other tokens. This is accomplished while keeping the watermark detector unchanged. Our experimental evaluation demonstrates that the DLM watermark leads to a >99% true positive rate with minimal quality impact and achieves similar robustness to existing ARLM watermarks, enabling for the first time reliable DLM watermarking.",
      "tldr_zh": "æœ¬ç ”ç©¶ä»‹ç»äº†é¦–ä¸ªä¸“é—¨é’ˆå¯¹æ‰©æ•£è¯­è¨€æ¨¡å‹(DLMs)è®¾è®¡çš„æ•°å­—æ°´å°æŠ€æœ¯ï¼Œæ—¨åœ¨è§£å†³DLMsåœ¨ç”ŸæˆTokenæ—¶ç”±äºé¡ºåºä»»æ„è€Œå¯¼è‡´ä¼ ç»Ÿè‡ªå›å½’è¯­è¨€æ¨¡å‹(ARLMs)æ°´å°æ–¹æ¡ˆæ— æ³•ç›´æ¥åº”ç”¨çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºï¼šå³ä½¿åœ¨éƒ¨åˆ†ä¸Šä¸‹æ–‡Tokenå°šæœªç¡®å®šçš„æƒ…å†µä¸‹ï¼Œä¹Ÿèƒ½åœ¨é¢„æœŸä¸Šä¸‹æ–‡ä¸­åº”ç”¨æ°´å°ï¼Œå¹¶ä¼˜å…ˆç”Ÿæˆé‚£äº›èƒ½ä½œä¸ºä¸Šä¸‹æ–‡å¢å¼ºæ°´å°å¼ºåº¦çš„Tokenã€‚è¿™ä¸€è¿‡ç¨‹åœ¨ä¿æŒæ°´å°æ£€æµ‹å™¨(watermark detector)é€»è¾‘ä¸å˜çš„å‰æä¸‹ï¼Œå®ç°äº†å¯¹éåºåˆ—ç”Ÿæˆè¿‡ç¨‹çš„æœ‰æ•ˆè¦†ç›–ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥DLMæ°´å°æ–¹æ¡ˆåœ¨å‡ ä¹ä¸ç‰ºç‰²æ–‡æœ¬ç”Ÿæˆè´¨é‡çš„æƒ…å†µä¸‹ï¼Œè¾¾åˆ°äº†è¶…è¿‡99%çš„çœŸé˜³æ€§ç‡(True Positive Rate)ï¼Œä¸”åœ¨é²æ£’æ€§æ–¹é¢ä¸ç°æœ‰çš„ARLMæ°´å°æ–¹æ¡ˆç›¸å½“ã€‚è¯¥ç ”ç©¶å¡«è¡¥äº†DLMé¢†åŸŸæ°´å°æŠ€æœ¯çš„ç©ºç™½ï¼Œé¦–æ¬¡å®ç°äº†å¯¹æ‰©æ•£å¼è¯­è¨€ç”Ÿæˆæ¨¡å‹çš„å¯é æ ‡è¯†ä¸ç›‘æµ‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24368v1",
      "published_date": "2025-09-29 07:11:40 UTC",
      "updated_date": "2025-09-29 07:11:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:07:15.662585+00:00"
    },
    {
      "arxiv_id": "2510.03288v2",
      "title": "LogAction: Consistent Cross-system Anomaly Detection through Logs via Active Domain Adaptation",
      "title_zh": "LogActionï¼šåŸºäºä¸»åŠ¨é¢†åŸŸè‡ªé€‚åº”çš„è·¨ç³»ç»Ÿæ—¥å¿—ä¸€è‡´æ€§å¼‚å¸¸æ£€æµ‹",
      "authors": [
        "Chiming Duan",
        "Minghua He",
        "Pei Xiao",
        "Tong Jia",
        "Xin Zhang",
        "Zhewei Zhong",
        "Xiang Luo",
        "Yan Niu",
        "Lingzhe Zhang",
        "Yifan Wu",
        "Siyu Yu",
        "Weijie Hong",
        "Ying Li",
        "Gang Huang"
      ],
      "abstract": "Log-based anomaly detection is a essential task for ensuring the reliability and performance of software systems. However, the performance of existing anomaly detection methods heavily relies on labeling, while labeling a large volume of logs is highly challenging. To address this issue, many approaches based on transfer learning and active learning have been proposed. Nevertheless, their effectiveness is hindered by issues such as the gap between source and target system data distributions and cold-start problems. In this paper, we propose LogAction, a novel log-based anomaly detection model based on active domain adaptation. LogAction integrates transfer learning and active learning techniques. On one hand, it uses labeled data from a mature system to train a base model, mitigating the cold-start issue in active learning. On the other hand, LogAction utilize free energy-based sampling and uncertainty-based sampling to select logs located at the distribution boundaries for manual labeling, thus addresses the data distribution gap in transfer learning with minimal human labeling efforts. Experimental results on six different combinations of datasets demonstrate that LogAction achieves an average 93.01% F1 score with only 2% of manual labels, outperforming some state-of-the-art methods by 26.28%. Website: https://logaction.github.io",
      "tldr_zh": "åŸºäºæ—¥å¿—çš„å¼‚å¸¸æ£€æµ‹(Log-based anomaly detection)å¯¹ä¿éšœè½¯ä»¶ç³»ç»Ÿå¯é æ€§è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨åº”å¯¹æ ‡æ³¨éš¾é¢˜æ—¶å¸¸é¢ä¸´è·¨ç³»ç»Ÿæ•°æ®åˆ†å¸ƒå·®å¼‚(data distribution gap)åŠå†·å¯åŠ¨(cold-start)ç­‰æŒ‘æˆ˜ã€‚è¯¥ç ”ç©¶æå‡ºäº†LogActionï¼Œä¸€ç§åŸºäºä¸»åŠ¨åŸŸé€‚åº”(active domain adaptation)çš„æ—¥å¿—å¼‚å¸¸æ£€æµ‹æ¨¡å‹ï¼Œé€šè¿‡æ•´åˆè¿ç§»å­¦ä¹ (transfer learning)ä¸ä¸»åŠ¨å­¦ä¹ (active learning)æŠ€æœ¯æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚LogActioné¦–å…ˆåˆ©ç”¨æˆç†Ÿç³»ç»Ÿçš„æ ‡æ³¨æ•°æ®è®­ç»ƒåŸºç¡€æ¨¡å‹ä»¥æ¶ˆé™¤å†·å¯åŠ¨å½±å“ï¼Œéšåç»“åˆåŸºäºè‡ªç”±èƒ½é‡‡æ ·(free energy-based sampling)å’Œä¸ç¡®å®šæ€§é‡‡æ ·(uncertainty-based sampling)çš„ç­–ç•¥ï¼Œç­›é€‰å‡ºåˆ†å¸ƒè¾¹ç¼˜çš„æ—¥å¿—è¿›è¡Œæå°‘é‡çš„äººå·¥æ ‡æ³¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLogActionåœ¨å…­ç§æ•°æ®é›†ç»„åˆä¸­ä»…éœ€2%çš„æ ‡æ³¨é‡å³å¯å®ç°93.01%çš„å¹³å‡F1åˆ†æ•°ã€‚è¯¥æ¨¡å‹æ€§èƒ½ä¼˜äºå½“å‰æœ€å…ˆè¿›æ–¹æ³•(state-of-the-art)çº¦26.28%ï¼Œè¯æ˜äº†å…¶åœ¨ç»´æŒè·¨ç³»ç»Ÿæ£€æµ‹ä¸€è‡´æ€§æ–¹é¢çš„é«˜æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC",
        "cs.SE"
      ],
      "primary_category": "cs.LG",
      "comment": "The 40th IEEE/ACM International Conference on Automated Software Engineering, ASE 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.03288v2",
      "published_date": "2025-09-29 07:09:19 UTC",
      "updated_date": "2025-10-09 06:43:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:07:17.169730+00:00"
    },
    {
      "arxiv_id": "2509.24365v2",
      "title": "Uni-X: Mitigating Modality Conflict with a Two-End-Separated Architecture for Unified Multimodal Models",
      "title_zh": "Uni-Xï¼šé€šè¿‡ä¸¤ç«¯åˆ†ç¦»å¼æ¶æ„ç¼“è§£ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹ä¸­çš„æ¨¡æ€å†²çª",
      "authors": [
        "Jitai Hao",
        "Hao Liu",
        "Xinyan Xiao",
        "Qiang Huang",
        "Jun Yu"
      ],
      "abstract": "Unified Multimodal Models (UMMs) built on shared autoregressive (AR) transformers are attractive for their architectural simplicity. However, we identify a critical limitation: when trained on multimodal inputs, modality-shared transformers suffer from severe gradient conflicts between vision and text, particularly in shallow and deep layers. We trace this issue to the fundamentally different low-level statistical properties of images and text, while noting that conflicts diminish in middle layers where representations become more abstract and semantically aligned. To overcome this challenge, we propose Uni-X, a two-end-separated, middle-shared architecture. Uni-X dedicates its initial and final layers to modality-specific processing, while maintaining shared parameters in the middle layers for high-level semantic fusion. This X-shaped design not only eliminates gradient conflicts at both ends but also further alleviates residual conflicts in the shared layers. Extensive experiments validate the effectiveness of Uni-X. Under identical training conditions, Uni-X achieves superior training efficiency compared to strong baselines. When scaled to 3B parameters with larger training data, Uni-X matches or surpasses 7B AR-based UMMs, achieving a GenEval score of 82 for image generation alongside strong performance in text and vision understanding tasks. These results establish Uni-X as a parameter-efficient and scalable foundation for future unified multimodal modeling. Our code is available at https://github.com/CURRENTF/Uni-X",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºå…±äº«è‡ªå›å½’ï¼ˆARï¼‰Transformerçš„ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹ï¼ˆUMMsï¼‰ä¸­å­˜åœ¨çš„æ¨¡æ€å†²çªé—®é¢˜ï¼ŒæŒ‡å‡ºè§†è§‰ä¸æ–‡æœ¬åœ¨æµ…å±‚å’Œæ·±å±‚ç½‘ç»œä¸­å­˜åœ¨ä¸¥é‡çš„æ¢¯åº¦å†²çªã€‚ç ”ç©¶å‘ç°è¿™ç§å†²çªæºäºå›¾åƒä¸æ–‡æœ¬åº•å±‚ç»Ÿè®¡ç‰¹æ€§çš„å·®å¼‚ï¼Œè€Œåœ¨è¯­ä¹‰å¯¹é½çš„ä¸­å±‚å†²çªè¾ƒå°ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†Uni-Xï¼Œä¸€ç§ä¸¤ç«¯åˆ†ç¦»ã€ä¸­é—´å…±äº«çš„Xå½¢æ¶æ„ï¼Œå°†åˆå§‹å±‚å’Œæœ€ç»ˆå±‚åˆ†é…ç»™ç‰¹å®šæ¨¡æ€ï¼Œè€Œä¸­å±‚å‚æ•°ä¿æŒå…±äº«ä»¥å®ç°é«˜å±‚è¯­ä¹‰èåˆã€‚è¿™ç§è®¾è®¡ä¸ä»…æ¶ˆé™¤äº†ä¸¤ç«¯çš„æ¢¯åº¦å†²çªï¼Œè¿˜ç¼“è§£äº†å…±äº«å±‚ä¸­çš„æ®‹ç•™å†²çªã€‚å®éªŒè¯æ˜ï¼ŒUni-Xåœ¨ç›¸åŒè®­ç»ƒæ¡ä»¶ä¸‹å±•ç°å‡ºæ›´ä¼˜çš„è®­ç»ƒæ•ˆç‡ï¼Œå…¶3Bå‚æ•°ç‰ˆæœ¬çš„è¡¨ç°å¯åª²ç¾ç”šè‡³è¶…è¶Š7Bè§„æ¨¡çš„åŸºçº¿æ¨¡å‹ã€‚è¯¥æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆï¼ˆGenEvalå¾—åˆ†82ï¼‰å’Œå¤šæ¨¡æ€ç†è§£ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºè‰²ï¼Œä¸ºæ„å»ºå‚æ•°é«˜æ•ˆä¸”å¯æ‰©å±•çš„ç»Ÿä¸€å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹æä¾›äº†æœ‰åŠ›æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24365v2",
      "published_date": "2025-09-29 07:05:10 UTC",
      "updated_date": "2025-11-29 18:10:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:07:21.261091+00:00"
    },
    {
      "arxiv_id": "2509.24361v2",
      "title": "UI-UG: A Unified MLLM for UI Understanding and Generation",
      "title_zh": "UI-UGï¼šç”¨äº UI ç†è§£ä¸ç”Ÿæˆçš„ç»Ÿä¸€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Hao Yang",
        "Weijie Qiu",
        "Ru Zhang",
        "Zhou Fang",
        "Ruichao Mao",
        "Xiaoyu Lin",
        "Maji Huang",
        "Zhaosong Huang",
        "Teng Guo",
        "Shuoyang Liu",
        "Hai Rao"
      ],
      "abstract": "Although Multimodal Large Language Models (MLLMs) have been widely applied across domains, they are still facing challenges in domain-specific tasks, such as User Interface (UI) understanding accuracy and UI generation quality. In this paper, we introduce UI-UG (a unified MLLM for UI Understanding and Generation), integrating both capabilities. For understanding tasks, we employ Supervised Fine-tuning (SFT) combined with Group Relative Policy Optimization (GRPO) to enhance fine-grained understanding on the modern complex UI data. For generation tasks, we further use Direct Preference Optimization (DPO) to make our model generate human-preferred UIs. In addition, we propose an industrially effective workflow, including the design of an LLM-friendly domain-specific language (DSL), training strategies, rendering processes, and evaluation metrics. In experiments, our model achieves state-of-the-art (SOTA) performance on understanding tasks, outperforming both larger general-purpose MLLMs and similarly-sized UI-specialized models. Our model is also on par with these larger MLLMs in UI generation performance at a fraction of the computational cost. We also demonstrate that integrating understanding and generation tasks can improve accuracy and quality for both tasks. Code and Model: https://github.com/neovateai/UI-UG",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†UI-UGï¼Œä¸€ä¸ªæ—¨åœ¨ç»Ÿä¸€ç”¨æˆ·ç•Œé¢(UI)ç†è§£ä¸ç”Ÿæˆèƒ½åŠ›çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLM)ã€‚é’ˆå¯¹ç†è§£ä»»åŠ¡ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†ç›‘ç£å¾®è°ƒ(SFT)ä¸ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–(GRPO)ï¼Œæœ‰æ•ˆæå‡äº†å¯¹ç°ä»£å¤æ‚UIæ•°æ®çš„ç»†ç²’åº¦ç†è§£èƒ½åŠ›ã€‚åœ¨ç”Ÿæˆä»»åŠ¡æ–¹é¢ï¼Œç ”ç©¶è€…é€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–(DPO)ç¡®ä¿æ¨¡å‹ç”Ÿæˆçš„UIæ›´ç¬¦åˆäººç±»å®¡ç¾ä¸ä½¿ç”¨ä¹ æƒ¯ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜æå‡ºäº†ä¸€å¥—åŒ…æ‹¬é¢†åŸŸç‰¹å®šè¯­è¨€(DSL)è®¾è®¡ã€è®­ç»ƒç­–ç•¥ã€æ¸²æŸ“æµç¨‹åŠè¯„ä¼°æŒ‡æ ‡åœ¨å†…çš„å·¥ä¸šçº§å·¥ä½œæµã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUI-UGåœ¨ç†è§£ä»»åŠ¡ä¸Šè¾¾åˆ°äº†å½“å‰é¡¶å°–æ°´å¹³(SOTA)ï¼Œä¸”å…¶ç”Ÿæˆæ€§èƒ½åœ¨æä½è®¡ç®—æˆæœ¬ä¸‹å³å¯åª²ç¾å‚æ•°é‡æ›´å¤§çš„æ¨¡å‹ã€‚è¯¥ç ”ç©¶è¿›ä¸€æ­¥è¯å®ï¼Œå°†ç†è§£ä¸ç”Ÿæˆä»»åŠ¡è¿›è¡Œé›†æˆè®­ç»ƒèƒ½å¤Ÿæ˜¾è‘—æå‡ä¸¤é¡¹ä»»åŠ¡çš„æœ€ç»ˆå‡†ç¡®æ€§ä¸è¾“å‡ºè´¨é‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24361v2",
      "published_date": "2025-09-29 06:59:09 UTC",
      "updated_date": "2025-09-30 07:45:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:07:23.377316+00:00"
    },
    {
      "arxiv_id": "2509.24358v1",
      "title": "An Enhanced Pyramid Feature Network Based on Long-Range Dependencies for Multi-Organ Medical Image Segmentation",
      "title_zh": "åŸºäºé•¿ç¨‹ä¾èµ–çš„å¤šå™¨å®˜åŒ»å­¦å›¾åƒåˆ†å‰²å¢å¼ºå‹é‡‘å­—å¡”ç‰¹å¾ç½‘ç»œ",
      "authors": [
        "Dayu Tan",
        "Cheng Kong",
        "Yansen Su",
        "Hai Chen",
        "Dongliang Yang",
        "Junfeng Xia",
        "Chunhou Zheng"
      ],
      "abstract": "In the field of multi-organ medical image segmentation, recent methods frequently employ Transformers to capture long-range dependencies from image features. However, these methods overlook the high computational cost of Transformers and their deficiencies in extracting local detailed information. To address high computational costs and inadequate local detail information, we reassess the design of feature extraction modules and propose a new deep-learning network called LamFormer for fine-grained segmentation tasks across multiple organs. LamFormer is a novel U-shaped network that employs Linear Attention Mamba (LAM) in an enhanced pyramid encoder to capture multi-scale long-range dependencies. We construct the Parallel Hierarchical Feature Aggregation (PHFA) module to aggregate features from different layers of the encoder, narrowing the semantic gap among features while filtering information. Finally, we design the Reduced Transformer (RT), which utilizes a distinct computational approach to globally model up-sampled features. RRT enhances the extraction of detailed local information and improves the network's capability to capture long-range dependencies. LamFormer outperforms existing segmentation methods on seven complex and diverse datasets, demonstrating exceptional performance. Moreover, the proposed network achieves a balance between model performance and model complexity.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šå™¨å®˜åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ Transformers è®¡ç®—æˆæœ¬é«˜ä¸”å±€éƒ¨ç»†èŠ‚æå–ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸º LamFormer çš„æ–°å‹ U-shaped æ·±åº¦å­¦ä¹ ç½‘ç»œã€‚LamFormer åœ¨å¢å¼ºé‡‘å­—å¡”ç¼–ç å™¨ä¸­å¼•å…¥äº† Linear Attention Mamba (LAM) æ¨¡å—ï¼Œæ—¨åœ¨æœ‰æ•ˆæ•è·å¤šå°ºåº¦çš„é•¿è·ç¦»ä¾èµ– (long-range dependencies)ã€‚ä¸ºäº†ä¼˜åŒ–ç‰¹å¾èåˆï¼Œç ”ç©¶è€…æ„å»ºäº†å¹¶è¡Œå±‚æ¬¡ç‰¹å¾èšåˆ (Parallel Hierarchical Feature Aggregation, PHFA) æ¨¡å—ï¼Œåœ¨èšåˆä¸åŒå±‚çº§ç‰¹å¾çš„åŒæ—¶ç¼©å°è¯­ä¹‰å·®è·å¹¶è¿‡æ»¤å†—ä½™ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œè¯¥æ¶æ„é€šè¿‡è®¾è®¡ç®€åŒ–ç‰ˆ Transformer (Reduced Transformer, RT) å¯¹ä¸Šé‡‡æ ·ç‰¹å¾è¿›è¡Œå…¨å±€å»ºæ¨¡ï¼Œæ˜¾è‘—æå‡äº†ç½‘ç»œå¯¹å±€éƒ¨ç»†èŠ‚ä¿¡æ¯å’Œé•¿ç¨‹ä¿¡æ¯çš„æå–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLamFormer åœ¨ 7 ä¸ªå¤æ‚çš„å¤šå™¨å®˜æ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰åˆ†å‰²æ–¹æ³•ï¼Œåœ¨æ¨¡å‹æ€§èƒ½ä¸è®¡ç®—å¤æ‚åº¦ä¹‹é—´è¾¾æˆäº†å“è¶Šçš„å¹³è¡¡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24358v1",
      "published_date": "2025-09-29 06:57:11 UTC",
      "updated_date": "2025-09-29 06:57:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:07:30.693750+00:00"
    },
    {
      "arxiv_id": "2509.24356v1",
      "title": "Beyond Repetition: Text Simplification and Curriculum Learning for Data-Constrained Pretraining",
      "title_zh": "è¶…è¶Šé‡å¤ï¼šé¢å‘æ•°æ®å—é™é¢„è®­ç»ƒçš„æ–‡æœ¬ç®€åŒ–ä¸è¯¾ç¨‹å­¦ä¹ ",
      "authors": [
        "Matthew Theodore Roque",
        "Dan John Velasco"
      ],
      "abstract": "Most studies on language model pretraining focus on large datasets, leaving open questions about optimization in data-constrained settings. In such settings, the effects of training data order and of including alternative versions of the same text remain underexplored. We address this by studying curriculum learning in pretraining, focusing on text-complexity ordering and data augmentation via simplification. We ask: (1) Does simplifying texts enhance representation quality more than reusing the original data? and (2) Does ordering data by text complexity yield better representations? To answer, we build on a pair of parallel corpora where human-written paragraphs are aligned with LLM-simplified variants, and test four data schedules: repeated exposure, low-to-high complexity, high-to-low, and interleaved. We analyze models' representation quality from a sample efficiency perspective via fine-tuning, as well as its zero-shot performance on linguistic knowledge, entity tracking, world knowledge, and commonsense reasoning. Our findings show that adding simplified data improves fine-tuning and zero-shot performance over a repeated-exposure baseline: smaller models benefit from low-to-high complexity, while larger models perform better with interleaved ordering.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹æ•°æ®å—é™(data-constrained)ç¯å¢ƒä¸‹çš„è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒä¼˜åŒ–é—®é¢˜ï¼Œæ¢è®¨äº†è¯¾ç¨‹å­¦ä¹ (curriculum learning)å’Œé€šè¿‡æ–‡æœ¬ç®€åŒ–(text simplification)è¿›è¡Œæ•°æ®å¢å¼ºå¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚ä½œè€…åˆ©ç”¨äººç±»æ’°å†™çš„åŸå§‹æ®µè½ä¸å¤§è¯­è¨€æ¨¡å‹(LLM)ç®€åŒ–å˜ä½“ç»„æˆçš„å¹³è¡Œè¯­æ–™åº“ï¼Œå¯¹æ¯”åˆ†æäº†é‡å¤è®­ç»ƒã€æŒ‰å¤æ‚åº¦é€’å¢ã€é€’å‡ä»¥åŠäº¤é”™æ’åˆ—ç­‰å››ç§æ•°æ®è°ƒåº¦æ–¹æ¡ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨é¢„è®­ç»ƒä¸­å¼•å…¥ç®€åŒ–æ•°æ®åœ¨å¾®è°ƒ(fine-tuning)å’Œé›¶æ ·æœ¬(zero-shot)ä»»åŠ¡ä¸­çš„è¡¨ç°å‡ä¼˜äºä¼ ç»Ÿçš„é‡å¤æ•°æ®æš´éœ²åŸºçº¿ã€‚ç ”ç©¶å‘ç°ï¼Œä¸åŒè§„æ¨¡çš„æ¨¡å‹å¯¹å¤æ‚åº¦æ’åºçš„åå¥½å­˜åœ¨å·®å¼‚ï¼Œè¾ƒå°çš„æ¨¡å‹æ›´å—ç›Šäºä»ä½åˆ°é«˜çš„å¤æ‚åº¦é€’å¢é¡ºåºï¼Œè€Œè¾ƒå¤§çš„æ¨¡å‹åˆ™åœ¨äº¤é”™æ’åºä¸‹è¡¨ç°æ›´ä½³ã€‚è¯¥ç ”ç©¶æ¶µç›–äº†è¯­è¨€çŸ¥è¯†ã€å®ä½“è¿½è¸ªå’Œå¸¸è¯†æ¨ç†ç­‰å¤šä¸ªè¯„ä¼°ç»´åº¦ï¼Œä¸ºåœ¨æœ‰é™æ•°æ®èµ„æºä¸‹ä¼˜åŒ–é¢„è®­ç»ƒè¿‡ç¨‹æä¾›äº†é‡è¦çš„å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "To be published in BabyLM Workshop at EMNLP 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.24356v1",
      "published_date": "2025-09-29 06:54:59 UTC",
      "updated_date": "2025-09-29 06:54:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:07:30.873665+00:00"
    },
    {
      "arxiv_id": "2509.24351v1",
      "title": "From Static to Dynamic: Adaptive Monte Carlo Search for Mathematical Process Supervision",
      "title_zh": "ä»é™æ€åˆ°åŠ¨æ€ï¼šé¢å‘æ•°å­¦è¿‡ç¨‹ç›‘ç£çš„è‡ªé€‚åº”è’™ç‰¹å¡æ´›æœç´¢",
      "authors": [
        "Jie Ma",
        "Shihao Qi",
        "Rui Xing",
        "Ziang Yin",
        "Bifan Wei",
        "Jun Liu",
        "Tongliang Liu"
      ],
      "abstract": "The quality of process data plays a key role in training a Process Reward Model (PRM), which can enhance the complex mathematical reasoning capability of large language models. Existing methods estimate the quality of reasoning steps based on a fixed-budget sampling strategy and navigate a vast search space to perform path expansion during the automated data generation process, resulting in their inefficiency and inflexibility. To address these issues, we propose Adaptive Monte Carlo Search (AMCS), a framework that transforms data generation from fixed, static to adaptive, dynamic search at the level of node value estimation and path expansion. On one hand, AMCS adaptively refines estimation by allocating more samples to uncertain reasoning steps while using fewer samples for those that are easier to estimate. On the other hand, it enhances the path expansion through a Monte Carlo algorithm with a temporally adaptive policy that begins with broad exploration and gradually shifts toward exploiting the most promising directions. With AMCS, we construct a large-scale dataset MathSearch-200K of about 200K process supervision examples for training PRMs. To verify the effectiveness of our method, we conduct extensive experiments on four mathematical reasoning benchmarks. Experimental results show that Qwen2.5-Math-7B-PRM-AMCS achieves up to 76.2% accuracy on MATH500 with GLM-4-9B, outperforming all baseline PRMs. Notably, a 7B model supervised by Qwen2.5-Math-7B-PRM-AMCS surpasses a 72B model with weaker supervision. Moreover, Qwen2.5-Math-7B-PRM-AMCS maintains consistent advantages on out-of-distribution problems, demonstrating strong generalization capability. Our code is available at https://github.com/reml-group/AMCS.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Adaptive Monte Carlo Search (AMCS)ï¼Œè¿™æ˜¯ä¸€ç§å°†æ•°æ®ç”Ÿæˆä»é™æ€è½¬å˜ä¸ºè‡ªé€‚åº”åŠ¨æ€æœç´¢çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ (PRM) åœ¨è®­ç»ƒæ•°æ®ç”Ÿæˆè¿‡ç¨‹ä¸­æ•ˆç‡ä½ä¸‹ä¸”ç¼ºä¹çµæ´»æ€§çš„é—®é¢˜ã€‚AMCS åœ¨èŠ‚ç‚¹å€¼ä¼°è®¡å±‚é¢é€šè¿‡ä¸ºä¸ç¡®å®šçš„æ¨ç†æ­¥éª¤åˆ†é…æ›´å¤šæ ·æœ¬æ¥å®ç°è‡ªé€‚åº”ç»†åŒ–ï¼Œå¹¶åœ¨è·¯å¾„æ‰©å±•å±‚é¢åˆ©ç”¨å…·æœ‰æ—¶é—´è‡ªé€‚åº”ç­–ç•¥çš„ Monte Carlo ç®—æ³•ï¼Œå®ç°ä»å¹¿æ³›æ¢ç´¢åˆ°å®šå‘å¼€å‘çš„åŠ¨æ€è¿‡æ¸¡ã€‚åˆ©ç”¨è¯¥æ¡†æ¶ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†åŒ…å«çº¦20ä¸‡ä¸ªè¿‡ç¨‹ç›‘ç£æ ·æœ¬çš„å¤§è§„æ¨¡æ•°æ®é›† MathSearch-200Kã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒQwen2.5-Math-7B-PRM-AMCS åœ¨ MATH500 åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº† 76.2% çš„å‡†ç¡®ç‡ï¼Œæ€§èƒ½ä¼˜äºæ‰€æœ‰åŸºå‡† PRMsã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå—è¯¥æ¨¡å‹ç›‘ç£çš„ 7B æ¨¡å‹è¡¨ç°ç”šè‡³è¶…è¿‡äº†å—å¼±ç›‘ç£çš„ 72B æ¨¡å‹ï¼Œä¸”åœ¨åˆ†å¸ƒå¤– (out-of-distribution) é—®é¢˜ä¸Šå±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24351v1",
      "published_date": "2025-09-29 06:52:35 UTC",
      "updated_date": "2025-09-29 06:52:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:07:46.368501+00:00"
    },
    {
      "arxiv_id": "2509.24350v1",
      "title": "Dynamic Orchestration of Multi-Agent System for Real-World Multi-Image Agricultural VQA",
      "title_zh": "é¢å‘ç°å®åœºæ™¯å¤šå›¾åƒå†œä¸šè§†è§‰é—®ç­”çš„å¤šæ™ºèƒ½ä½“ç³»ç»ŸåŠ¨æ€ç¼–æ’",
      "authors": [
        "Yan Ke",
        "Xin Yu",
        "Heming Du",
        "Scott Chapman",
        "Helen Huang"
      ],
      "abstract": "Agricultural visual question answering is essential for providing farmers and researchers with accurate and timely knowledge. However, many existing approaches are predominantly developed for evidence-constrained settings such as text-only queries or single-image cases. This design prevents them from coping with real-world agricultural scenarios that often require multi-image inputs with complementary views across spatial scales, and growth stages. Moreover, limited access to up-to-date external agricultural context makes these systems struggle to adapt when evidence is incomplete. In addition, rigid pipelines often lack systematic quality control. To address this gap, we propose a self-reflective and self-improving multi-agent framework that integrates four roles, the Retriever, the Reflector, the Answerer, and the Improver. They collaborate to enable context enrichment, reflective reasoning, answer drafting, and iterative improvement.\n  A Retriever formulates queries and gathers external information, while a Reflector assesses adequacy and triggers sequential reformulation and renewed retrieval. Two Answerers draft candidate responses in parallel to reduce bias. The Improver refines them through iterative checks while ensuring that information from multiple images is effectively aligned and utilized. Experiments on the AgMMU benchmark show that our framework achieves competitive performance on multi-image agricultural QA.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿå†œä¸šè§†è§‰é—®ç­”(Agricultural Visual Question Answering, VQA)ç³»ç»Ÿåœ¨å¤„ç†å¤šå›¾åƒè¾“å…¥ã€å¤–éƒ¨ä¸Šä¸‹æ–‡ç¼ºå¤±åŠç¼ºä¹è´¨é‡æ§åˆ¶æ–¹é¢çš„ä¸è¶³ï¼Œæå‡ºäº†ä¸€ç§è‡ªåæ€ä¸”è‡ªæ”¹è¿›çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ã€‚è¯¥ç³»ç»Ÿé€šè¿‡åŠ¨æ€ç¼–æ’Retrieverã€Reflectorã€Answererå’ŒImproverå››ä¸ªè§’è‰²ï¼Œå®ç°äº†ä¸Šä¸‹æ–‡å¢å¼ºã€åæ€æ€§æ¨ç†ã€å¹¶è¡Œç­”æ¡ˆèµ·è‰å’Œè¿­ä»£æ”¹è¿›ã€‚å…¶ä¸­ï¼ŒRetrieverè´Ÿè´£æ”¶é›†å¤–éƒ¨ä¿¡æ¯ï¼ŒReflectorè¯„ä¼°è¯æ®å……åˆ†æ€§å¹¶è§¦å‘æŸ¥è¯¢é‡æ„ï¼Œä¸¤ç»„Answererå¹¶è¡Œå·¥ä½œä»¥å‡å°‘åå·®ï¼Œè€ŒImproveråˆ™è´Ÿè´£å¤šå›¾åƒä¿¡æ¯çš„æœ‰æ•ˆå¯¹é½ä¸æœ€ç»ˆä¼˜åŒ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨AgMMUåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„è¡¨ç°ï¼Œæœ‰æ•ˆè§£å†³äº†å†œä¸šåœºæ™¯ä¸‹è·¨ç©ºé—´å°ºåº¦å’Œç”Ÿé•¿é˜¶æ®µçš„å¤æ‚çŸ¥è¯†è·å–éš¾é¢˜ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "13 pages, 2 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.24350v1",
      "published_date": "2025-09-29 06:52:10 UTC",
      "updated_date": "2025-09-29 06:52:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:07:50.595607+00:00"
    },
    {
      "arxiv_id": "2509.24342v1",
      "title": "Fin-Ally: Pioneering the Development of an Advanced, Commonsense-Embedded Conversational AI for Money Matters",
      "title_zh": "Fin-Allyï¼šå¼•é¢†é¢å‘é‡‘èäº‹åŠ¡ã€èåˆå¸¸è¯†çš„å…ˆè¿›å¯¹è¯å¼äººå·¥æ™ºèƒ½ç ”å‘",
      "authors": [
        "Sarmistha Das",
        "Priya Mathur",
        "Ishani Sharma",
        "Sriparna Saha",
        "Kitsuchart Pasupa",
        "Alka Maurya"
      ],
      "abstract": "The exponential technological breakthrough of the FinTech industry has significantly enhanced user engagement through sophisticated advisory chatbots. However, large-scale fine-tuning of LLMs can occasionally yield unprofessional or flippant remarks, such as ``With that money, you're going to change the world,'' which, though factually correct, can be contextually inappropriate and erode user trust. The scarcity of domain-specific datasets has led previous studies to focus on isolated components, such as reasoning-aware frameworks or the enhancement of human-like response generation. To address this research gap, we present Fin-Solution 2.O, an advanced solution that 1) introduces the multi-turn financial conversational dataset, Fin-Vault, and 2) incorporates a unified model, Fin-Ally, which integrates commonsense reasoning, politeness, and human-like conversational dynamics. Fin-Ally is powered by COMET-BART-embedded commonsense context and optimized with a Direct Preference Optimization (DPO) mechanism to generate human-aligned responses. The novel Fin-Vault dataset, consisting of 1,417 annotated multi-turn dialogues, enables Fin-Ally to extend beyond basic account management to provide personalized budgeting, real-time expense tracking, and automated financial planning. Our comprehensive results demonstrate that incorporating commonsense context enables language models to generate more refined, textually precise, and professionally grounded financial guidance, positioning this approach as a next-generation AI solution for the FinTech sector. Dataset and codes are available at: https://github.com/sarmistha-D/Fin-Ally",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é‡‘èç§‘æŠ€é¢†åŸŸå¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¯¹è¯ä¸­å¯èƒ½äº§ç”Ÿä¸ä¸“ä¸šæˆ–è¯­å¢ƒä¸å½“å›å¤çš„é—®é¢˜ï¼Œæå‡ºäº†å…ˆè¿›çš„è§£å†³æ–¹æ¡ˆ Fin-Solution 2.0ã€‚è¯¥æ–¹æ¡ˆé¦–å…ˆå¼•å…¥äº†åŒ…å«1,417ä¸ªæ ‡æ³¨å¤šè½®å¯¹è¯çš„é‡‘èé¢†åŸŸæ•°æ®é›† Fin-Vaultï¼Œå¹¶å¼€å‘äº†åä¸º Fin-Ally çš„ç»Ÿä¸€æ¨¡å‹ã€‚Fin-Ally æ¨¡å‹é›†æˆäº†å¸¸è¯†æ¨ç†ã€ç¤¼è²Œæ€§åŠç±»äººå¯¹è¯åŠ¨æ€ï¼Œåˆ©ç”¨ COMET-BART åµŒå…¥å¸¸è¯†èƒŒæ™¯ï¼Œå¹¶é€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–(Direct Preference Optimization, DPO)æœºåˆ¶ç¡®ä¿å›å¤ä¸äººç±»åå¥½å¯¹é½ã€‚è¯¥ç³»ç»Ÿæ”¯æŒä¸ªæ€§åŒ–é¢„ç®—ç¼–åˆ¶ã€å®æ—¶æ”¯å‡ºè¿½è¸ªå’Œè‡ªåŠ¨åŒ–è´¢åŠ¡è§„åˆ’ï¼Œå…¶åŠŸèƒ½è¿œè¶…åŸºç¡€çš„è´¦æˆ·ç®¡ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œèå…¥å¸¸è¯†èƒŒæ™¯èƒ½ä½¿æ¨¡å‹ç”Ÿæˆæ›´ç²¾å‡†ã€æ›´å…·ä¸“ä¸šåŸºç¡€çš„è´¢åŠ¡æŒ‡å¯¼ï¼Œä¸º FinTech è¡Œä¸šæä¾›äº†å…·å¤‡å¸¸è¯†åµŒå…¥èƒ½åŠ›çš„ä¸‹ä¸€ä»£å¯¹è¯å¼ AI è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24342v1",
      "published_date": "2025-09-29 06:44:47 UTC",
      "updated_date": "2025-09-29 06:44:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:08:02.990986+00:00"
    },
    {
      "arxiv_id": "2509.24340v2",
      "title": "humancompatible.detect: a Python Toolkit for Detecting Bias in AI Models",
      "title_zh": "humancompatible.detectï¼šç”¨äº AI æ¨¡å‹åè§æ£€æµ‹çš„ Python å·¥å…·åŒ…",
      "authors": [
        "German M. Matilla",
        "Jiri Nemecek",
        "Illia Kryvoviaz",
        "Jakub Marecek"
      ],
      "abstract": "There is a strong recent emphasis on trustworthy AI. In particular, international regulations, such as the AI Act, demand that AI practitioners measure data quality on the input and estimate bias on the output of high-risk AI systems. However, there are many challenges involved, including scalability (MMD) and computability (Wasserstein-1) issues of traditional methods for estimating distances on measure spaces. Here, we present humancompatible.detect, a toolkit for bias detection that addresses these challenges. It incorporates two newly developed methods to detect and evaluate bias: maximum subgroup discrepancy (MSD) and subsampled $\\ell_\\infty$ distances. It has an easy-to-use API documented with multiple examples. humancompatible.detect is licensed under the Apache License, Version 2.0.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¯ä¿¡äººå·¥æ™ºèƒ½(Trustworthy AI)å’Œã€Šäººå·¥æ™ºèƒ½æ³•æ¡ˆã€‹(AI Act)å¯¹æ¨¡å‹åè§æ£€æµ‹çš„è¦æ±‚ï¼Œæ¨å‡ºäº†åä¸º humancompatible.detect çš„ Python å·¥å…·åŒ…ã€‚ä¼ ç»Ÿçš„åº¦é‡ç©ºé—´è·ç¦»ä¼°è®¡æ–¹æ³•åœ¨å¤„ç†å¤§è§„æ¨¡æ•°æ®æ—¶å­˜åœ¨å¯æ‰©å±•æ€§(MMD)å’Œè®¡ç®—æ•ˆç‡(Wasserstein-1)æ–¹é¢çš„å±€é™ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œè¯¥å·¥å…·åŒ…é›†æˆäº†ä¸¤ç§æ–°å¼€å‘çš„åè§æ£€æµ‹è¯„ä¼°æ–¹æ³•ï¼šæœ€å¤§å­ç»„å·®å¼‚(Maximum Subgroup Discrepancy, MSD)å’Œå­é‡‡æ · $\\ell_\\infty$ è·ç¦»(subsampled $\\ell_\\infty$ distances)ã€‚è¯¥å·¥å…·åŒ…æä¾›äº†æ˜“äºä½¿ç”¨çš„ API æ¥å£å¹¶é™„å¸¦ä¸°å¯Œçš„åº”ç”¨ç¤ºä¾‹ï¼Œæ—¨åœ¨å¸®åŠ© AI ä»ä¸šè€…é«˜æ•ˆåœ°è¯„ä¼°é«˜é£é™©ç³»ç»Ÿè¾“å‡ºä¸­çš„åè§ã€‚æ­¤å¤–ï¼Œè¯¥é¡¹ç›®é‡‡ç”¨ Apache License 2.0 åè®®å¼€æºï¼Œä¸ºå®ç°å¯ç›‘ç®¡ä¸”å¯ä¿¡çš„äººå·¥æ™ºèƒ½æä¾›äº†å®ç”¨çš„æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "6 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.24340v2",
      "published_date": "2025-09-29 06:43:24 UTC",
      "updated_date": "2026-01-19 12:27:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:07:54.482600+00:00"
    },
    {
      "arxiv_id": "2509.24332v1",
      "title": "Towards Generalizable PDE Dynamics Forecasting via Physics-Guided Invariant Learning",
      "title_zh": "è¿ˆå‘åŸºäºç‰©ç†å¼•å¯¼ä¸å˜æ€§å­¦ä¹ çš„å¯æ³›åŒ–åå¾®åˆ†æ–¹ç¨‹åŠ¨åŠ›å­¦é¢„æµ‹",
      "authors": [
        "Siyang Li",
        "Yize Chen",
        "Yan Guo",
        "Ming Huang",
        "Hui Xiong"
      ],
      "abstract": "Advanced deep learning-based approaches have been actively applied to forecast the spatiotemporal physical dynamics governed by partial differential equations (PDEs), which acts as a critical procedure in tackling many science and engineering problems. As real-world physical environments like PDE system parameters are always capricious, how to generalize across unseen out-of-distribution (OOD) forecasting scenarios using limited training data is of great importance. To bridge this barrier, existing methods focus on discovering domain-generalizable representations across various PDE dynamics trajectories. However, their zero-shot OOD generalization capability remains deficient, since extra test-time samples for domain-specific adaptation are still required. This is because the fundamental physical invariance in PDE dynamical systems are yet to be investigated or integrated. To this end, we first explicitly define a two-fold PDE invariance principle, which points out that ingredient operators and their composition relationships remain invariant across different domains and PDE system evolution. Next, to capture this two-fold PDE invariance, we propose a physics-guided invariant learning method termed iMOOE, featuring an Invariance-aligned Mixture Of Operator Expert architecture and a frequency-enriched invariant learning objective. Extensive experiments across simulated benchmarks and real-world applications validate iMOOE's superior in-distribution performance and zero-shot generalization capabilities on diverse OOD forecasting scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦å­¦ä¹ åœ¨åå¾®åˆ†æ–¹ç¨‹(PDE)æ—¶ç©ºåŠ¨åŠ›å­¦é¢„æµ‹ä¸­é¢ä¸´çš„åˆ†å¸ƒå¤–(OOD)æ³›åŒ–éš¾é¢˜ï¼ŒæŒ‡å‡ºå½“å‰æ–¹æ³•åœ¨ç¼ºä¹æµ‹è¯•æ—¶è‡ªé€‚åº”çš„æƒ…å†µä¸‹éš¾ä»¥å®ç°é«˜æ•ˆçš„é›¶æ ·æœ¬æ³›åŒ–ã€‚ä¸ºæ­¤ï¼Œä½œè€…é¦–æ¬¡æ˜ç¡®å®šä¹‰äº†åŒé‡PDEä¸å˜æ€§åŸåˆ™(two-fold PDE invariance principle)ï¼Œå³ç»„æˆç®—å­åŠå…¶ç»„åˆå…³ç³»åœ¨ä¸åŒé¢†åŸŸå’Œç³»ç»Ÿæ¼”åŒ–ä¸­ä¿æŒä¸å˜ã€‚åŸºäºæ­¤åŸåˆ™ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§ç‰©ç†å¼•å¯¼çš„ä¸å˜å­¦ä¹ æ–¹æ³•iMOOEï¼Œè¯¥æ–¹æ³•é‡‡ç”¨äº†ä¸å˜æ€§å¯¹é½çš„ç®—å­ä¸“å®¶æ··åˆæ¶æ„(Invariance-aligned Mixture Of Operator Expert)å’Œé¢‘ç‡å¢å¼ºçš„ä¸å˜å­¦ä¹ ç›®æ ‡ã€‚é€šè¿‡æ˜¾å¼é›†æˆåŸºç¡€ç‰©ç†ä¸å˜æ€§ï¼ŒiMOOEèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰PDEç³»ç»Ÿçš„å†…åœ¨æ¼”åŒ–è§„å¾‹ã€‚åœ¨æ¨¡æ‹ŸåŸºå‡†å’ŒçœŸå®åº”ç”¨ä¸­çš„å®éªŒéªŒè¯è¡¨æ˜ï¼ŒiMOOEåœ¨åˆ†å¸ƒå†…é¢„æµ‹å’Œå¤šæ ·åŒ–OODåœºæ™¯ä¸‹å‡å±•ç°å‡ºå“è¶Šçš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºè§£å†³å¤æ‚çš„ç§‘å­¦ä¸å·¥ç¨‹é—®é¢˜æä¾›äº†å¼ºæœ‰åŠ›çš„æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "27 pages, 13 figues. In Submission",
      "pdf_url": "https://arxiv.org/pdf/2509.24332v1",
      "published_date": "2025-09-29 06:30:01 UTC",
      "updated_date": "2025-09-29 06:30:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:08:08.296848+00:00"
    },
    {
      "arxiv_id": "2509.24326v1",
      "title": "TraitSpaces: Towards Interpretable Visual Creativity for Human-AI Co-Creation",
      "title_zh": "TraitSpacesï¼šé¢å‘äººæœºå…±åˆ›çš„å¯è§£é‡Šè§†è§‰åˆ›é€ åŠ›",
      "authors": [
        "Prerna Luthra"
      ],
      "abstract": "We introduce a psychologically grounded and artist-informed framework for modeling visual creativity across four domains: Inner, Outer, Imaginative, and Moral Worlds. Drawing on interviews with practicing artists and theories from psychology, we define 12 traits that capture affective, symbolic, cultural, and ethical dimensions of creativity.Using 20k artworks from the SemArt dataset, we annotate images with GPT 4.1 using detailed, theory-aligned prompts, and evaluate the learnability of these traits from CLIP image embeddings. Traits such as Environmental Dialogicity and Redemptive Arc are predicted with high reliability ($R^2 \\approx 0.64 - 0.68$), while others like Memory Imprint remain challenging, highlighting the limits of purely visual encoding. Beyond technical metrics, we visualize a \"creativity trait-space\" and illustrate how it can support interpretable, trait-aware co-creation - e.g., sliding along a Redemptive Arc axis to explore works of adversity and renewal. By linking cultural-aesthetic insights with computational modeling, our work aims not to reduce creativity to numbers, but to offer shared language and interpretable tools for artists, researchers, and AI systems to collaborate meaningfully.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†TraitSpacesï¼Œä¸€ä¸ªåŸºäºå¿ƒç†å­¦åŸºç¡€å’Œè‰ºæœ¯å®¶è§è§£çš„è§†è§‰åˆ›æ„å»ºæ¨¡æ¡†æ¶ï¼Œæ¶µç›–äº†Innerã€Outerã€Imaginativeå’ŒMoral Worldså››ä¸ªé¢†åŸŸã€‚é€šè¿‡å¯¹è‰ºæœ¯å®¶çš„è®¿è°ˆå’Œå¿ƒç†å­¦ç†è®ºï¼Œç ”ç©¶å®šä¹‰äº†12ä¸ªæ•æ‰æƒ…æ„Ÿã€ç¬¦å·ã€æ–‡åŒ–å’Œä¼¦ç†ç»´åº¦çš„åˆ›æ„ç‰¹è´¨(Traits)ã€‚åˆ©ç”¨SemArtæ•°æ®é›†çš„2ä¸‡ä»¶ä½œå“ï¼Œç ”ç©¶è€…ä½¿ç”¨GPT 4.1è¿›è¡Œç†è®ºå¯¹é½çš„æ ‡æ³¨ï¼Œå¹¶è¯„ä¼°äº†ä»CLIP image embeddingsä¸­å­¦ä¹ è¿™äº›ç‰¹è´¨çš„å¯è¡Œæ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºEnvironmental Dialogicityå’ŒRedemptive Arcç­‰ç‰¹è´¨å…·æœ‰è¾ƒé«˜çš„é¢„æµ‹å¯é æ€§ï¼Œè€ŒMemory Imprintç­‰ç‰¹è´¨åˆ™æ­ç¤ºäº†çº¯è§†è§‰ç¼–ç çš„å±€é™ã€‚ç ”ç©¶è¿˜é€šè¿‡å¯è§†åŒ–â€œåˆ›æ„ç‰¹è´¨ç©ºé—´â€(Creativity trait-space)ï¼Œå±•ç¤ºäº†å¦‚ä½•é€šè¿‡è°ƒèŠ‚ç‰¹å®šç‰¹è´¨è½´æ”¯æŒå¯è§£é‡Šçš„ã€ç‰¹è´¨æ„ŸçŸ¥çš„å…±åˆ›ã€‚è¯¥å·¥ä½œé€šè¿‡ç»“åˆæ–‡åŒ–ç¾å­¦è§è§£ä¸è®¡ç®—å»ºæ¨¡ï¼Œä¸ºè‰ºæœ¯å®¶ã€ç ”ç©¶äººå‘˜å’Œäººå·¥æ™ºèƒ½ç³»ç»Ÿçš„åä½œæä¾›äº†å…±åŒè¯­è¨€å’Œå¯è§£é‡Šçš„å·¥å…·ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24326v1",
      "published_date": "2025-09-29 06:24:18 UTC",
      "updated_date": "2025-09-29 06:24:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:08:09.675978+00:00"
    },
    {
      "arxiv_id": "2510.02370v2",
      "title": "How Training Data Shapes the Use of Parametric and In-Context Knowledge in Language Models",
      "title_zh": "è®­ç»ƒæ•°æ®å¦‚ä½•å¡‘é€ è¯­è¨€æ¨¡å‹ä¸­å‚æ•°åŒ–çŸ¥è¯†ä¸ä¸Šä¸‹æ–‡çŸ¥è¯†çš„è¿ç”¨",
      "authors": [
        "Minsung Kim",
        "Dong-Kyum Kim",
        "Jea Kwon",
        "Nakyeong Yang",
        "Kyomin Jung",
        "Meeyoung Cha"
      ],
      "abstract": "Large language models leverage not only parametric knowledge acquired during training but also in-context knowledge provided at inference time, despite the absence of explicit training objectives for using both sources. Prior work has further shown that when these knowledge sources conflict, models resolve the tension based on their internal confidence, preferring parametric knowledge for high-confidence facts while deferring to contextual information for less familiar ones. However, the training conditions that give rise to such knowledge utilization behaviors remain unclear. To address this gap, we conduct controlled experiments in which we train language models while systematically manipulating key properties of the training data. Our results reveal a counterintuitive finding: three properties commonly regarded as detrimental must co-occur for robust knowledge utilization and conflict resolution to emerge: (i) intra-document repetition of information, (ii) a moderate degree of within-document inconsistency, and (iii) a skewed knowledge frequency distribution. We further validate that the same training dynamics observed in our controlled setting also arise during real-world language model pretraining, and we analyze how post-training procedures can reshape models' knowledge preferences. Together, our findings provide concrete empirical guidance for training language models that harmoniously integrate parametric and in-context knowledge.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è®­ç»ƒæ•°æ®å¦‚ä½•å¡‘é€ å¤§è¯­è¨€æ¨¡å‹å¯¹å‚æ•°åŒ–çŸ¥è¯†(Parametric Knowledge)ä¸ä¸Šä¸‹æ–‡çŸ¥è¯†(In-Context Knowledge)çš„ä½¿ç”¨åå¥½ã€‚é€šè¿‡ç³»ç»Ÿçš„å—æ§å®éªŒï¼Œç ”ç©¶æ­ç¤ºäº†ä¸€ä¸ªåç›´è§‰çš„å‘ç°ï¼šè¦ä½¿æ¨¡å‹è¡¨ç°å‡ºç¨³å¥çš„çŸ¥è¯†åˆ©ç”¨å’Œå†²çªå¤„ç†èƒ½åŠ›ï¼Œå¿…é¡»åŒæ—¶å…·å¤‡ä¸‰é¡¹é€šå¸¸è¢«è§†ä¸ºæœ‰å®³çš„è®­ç»ƒæ•°æ®å±æ€§ï¼Œå³æ–‡æ¡£å†…çš„ä¿¡æ¯é‡å¤(intra-document repetition)ã€é€‚åº¦çš„æ–‡æ¡£å†…ä¸ä¸€è‡´æ€§(within-document inconsistency)ä»¥åŠå€¾æ–œçš„çŸ¥è¯†é¢‘ç‡åˆ†å¸ƒ(skewed knowledge frequency distribution)ã€‚ç ”ç©¶è¿›ä¸€æ­¥åœ¨çœŸå®çš„è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒè¿‡ç¨‹ä¸­éªŒè¯äº†è¿™äº›åŠ¨æ€è§„å¾‹ï¼Œå¹¶æ·±å…¥åˆ†æäº†åè®­ç»ƒ(post-training)è¿‡ç¨‹å¦‚ä½•é‡å¡‘æ¨¡å‹çš„çŸ¥è¯†åå¥½ã€‚è¯¥ç ”ç©¶ç»“æœä¸ºä¼˜åŒ–è®­ç»ƒæµç¨‹ä»¥åè°ƒæ•´åˆæ¨¡å‹å†…éƒ¨çŸ¥è¯†ä¸å¤–éƒ¨ä¸Šä¸‹æ–‡ä¿¡æ¯æä¾›äº†å…·ä½“çš„å®è¯æŒ‡å—ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "16 pages",
      "pdf_url": "https://arxiv.org/pdf/2510.02370v2",
      "published_date": "2025-09-29 06:18:18 UTC",
      "updated_date": "2026-01-07 11:42:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:08:11.085248+00:00"
    },
    {
      "arxiv_id": "2510.00061v1",
      "title": "Survey of AI-Powered Approaches for Osteoporosis Diagnosis in Medical Imaging",
      "title_zh": "åŒ»å­¦å½±åƒä¸­åŸºäºäººå·¥æ™ºèƒ½çš„éª¨è´¨ç–æ¾ç—‡è¯Šæ–­æ–¹æ³•ç»¼è¿°",
      "authors": [
        "Abdul Rahman",
        "Bumshik Lee"
      ],
      "abstract": "Osteoporosis silently erodes skeletal integrity worldwide; however, early detection through imaging can prevent most fragility fractures. Artificial intelligence (AI) methods now mine routine Dual-energy X-ray Absorptiometry (DXA), X-ray, Computed Tomography (CT), and Magnetic Resonance Imaging (MRI) scans for subtle, clinically actionable markers, but the literature is fragmented. This survey unifies the field through a tri-axial framework that couples imaging modalities with clinical tasks and AI methodologies (classical machine learning, convolutional neural networks (CNNs), transformers, self-supervised learning, and explainable AI). Following a concise clinical and technical primer, we detail our Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA)-guided search strategy, introduce the taxonomy via a roadmap figure, and synthesize cross-study insights on data scarcity, external validation, and interpretability. By identifying emerging trends, open challenges, and actionable research directions, this review provides AI scientists, medical imaging researchers, and musculoskeletal clinicians with a clear compass to accelerate rigorous, patient-centered innovation in osteoporosis care. The project page of this survey can also be found on Github.",
      "tldr_zh": "è¯¥ç»¼è¿°ç³»ç»Ÿåœ°è°ƒç ”äº†åˆ©ç”¨äººå·¥æ™ºèƒ½(AI)æŠ€æœ¯é€šè¿‡åŒ»å­¦å½±åƒè¯Šæ–­éª¨è´¨ç–æ¾ç—‡(Osteoporosis)çš„ç°çŠ¶ï¼Œæ—¨åœ¨è§£å†³è¯¥é¢†åŸŸæ–‡çŒ®é›¶æ•£çš„é—®é¢˜ã€‚ç ”ç©¶æå‡ºäº†ä¸€ä¸ªä¸‰è½´æ¡†æ¶(tri-axial framework)ï¼Œå°†åŒèƒ½Xå°„çº¿å¸æ”¶æ³•(DXA)ã€Xå°„çº¿(X-ray)ã€è®¡ç®—æœºæ–­å±‚æ‰«æ(CT)å’Œç£å…±æŒ¯æˆåƒ(MRI)ç­‰å½±åƒæ¨¡æ€ä¸ä¸´åºŠä»»åŠ¡åŠAIæ–¹æ³•ï¼ˆåŒ…æ‹¬Convolutional Neural Networks, Transformers, Self-supervised learningå’ŒExplainable AIï¼‰æœ‰æœºç»“åˆã€‚æ–‡ç« éµå¾ªPRISMAå‡†åˆ™è¿›è¡Œç³»ç»Ÿæ€§ç»¼è¿°ï¼Œå¹¶æ·±å…¥æ¢è®¨äº†æ•°æ®ç¨€ç¼ºæ€§(data scarcity)ã€å¤–éƒ¨éªŒè¯(external validation)å’Œå¯è§£é‡Šæ€§(interpretability)ç­‰æ ¸å¿ƒæŒ‘æˆ˜ã€‚é€šè¿‡æ€»ç»“æ–°å…´è¶‹åŠ¿ä¸è¡ŒåŠ¨å¯¼å‘çš„ç ”ç©¶æ–¹å‘ï¼Œè¯¥ç ”ç©¶ä¸ºAIç§‘å­¦å®¶å’Œä¸´åºŠåŒ»ç”Ÿæä¾›äº†æ˜ç¡®çš„æŒ‡å—ï¼Œæ—¨åœ¨åŠ é€Ÿä»¥æ‚£è€…ä¸ºä¸­å¿ƒçš„éª¨è´¨ç–æ¾ç—‡æŠ¤ç†åˆ›æ–°ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "56 pages, 18 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.00061v1",
      "published_date": "2025-09-29 06:01:45 UTC",
      "updated_date": "2025-09-29 06:01:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:08:14.791177+00:00"
    },
    {
      "arxiv_id": "2509.24319v2",
      "title": "Dual Mechanisms of Value Expression: Intrinsic vs. Prompted Values in LLMs",
      "title_zh": "ä»·å€¼è¡¨è¾¾çš„åŒé‡æœºåˆ¶ï¼šå¤§è¯­è¨€æ¨¡å‹ä¸­çš„å†…åœ¨ä»·å€¼ä¸æç¤ºä»·å€¼",
      "authors": [
        "Jongwook Han",
        "Jongwon Lim",
        "Injin Kong",
        "Yohan Jo"
      ],
      "abstract": "Large language models (LLMs) can express different values in two distinct ways: (1) intrinsic expression, reflecting the model's inherent values learned during training, and (2) prompted expression, elicited by explicit prompts. Given their widespread use in value alignment and persona steering, it is paramount to clearly understand their underlying mechanisms, particularly whether they mostly overlap (as one might expect) or rely on substantially different mechanisms, but this remains largely understudied. We analyze this at the mechanistic level using two approaches: (1) value vectors, feature directions representing value mechanisms extracted from the residual stream, and (2) value neurons, MLP neurons that contribute to value expressions. We demonstrate that intrinsic and prompted value mechanisms partly share common components that are crucial for inducing value expression, but also possess unique elements that manifest in different ways. As a result, these mechanisms lead to different degrees of value steerability (prompted > intrinsic) and response diversity (intrinsic > prompted). In particular, components unique to the intrinsic mechanism seem to promote lexical diversity in responses, whereas those specific to the prompted mechanism primarily strengthen instruction following, taking effect even in distant tasks like jailbreaking.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸­å†…åœ¨è¡¨è¾¾(intrinsic expression)ä¸æç¤ºè¡¨è¾¾(prompted expression)ä¸¤ç§ä»·å€¼è¡¨è¾¾æœºåˆ¶çš„åº•å±‚é€»è¾‘ï¼Œå¡«è¡¥äº†è¿™ä¸€é¢†åŸŸæœºåˆ¶åˆ†æçš„ç©ºç™½ã€‚ç ”ç©¶è€…é€šè¿‡åˆ†ææ®‹å·®æµä¸­çš„ä»·å€¼å‘é‡(value vectors)å’ŒMLPå±‚ä¸­çš„ä»·å€¼ç¥ç»å…ƒ(value neurons)ï¼Œæ­ç¤ºäº†è¿™ä¸¤ç§æœºåˆ¶æ—¢å…±äº«å…³é”®çš„å…¬å…±ç»„ä»¶ï¼Œä¹Ÿå„è‡ªæ‹¥æœ‰ç‹¬ç‰¹çš„è¿ä½œå…ƒç´ ã€‚å®éªŒè¡¨æ˜ï¼Œæç¤ºæœºåˆ¶å…·æœ‰æ›´å¼ºçš„ä»·å€¼å¯æ§æ€§(steerability)ï¼Œè€Œå†…åœ¨æœºåˆ¶åˆ™å±•ç°å‡ºæ›´é«˜çš„å“åº”å¤šæ ·æ€§(response diversity)ã€‚å…¶ä¸­ï¼Œå†…åœ¨æœºåˆ¶ç‰¹æœ‰çš„ç»„ä»¶èƒ½æ˜¾è‘—æå‡è¯æ±‡å¤šæ ·æ€§ï¼Œè€Œæç¤ºæœºåˆ¶ç‰¹æœ‰çš„ç»„ä»¶åˆ™åœ¨å¢å¼ºæŒ‡ä»¤éµå¾ª(instruction following)æ–¹é¢å‘æŒ¥å…³é”®ä½œç”¨ï¼Œç”šè‡³ä¼šå½±å“åˆ°è¶Šç‹±(jailbreaking)ç­‰è¿œç¨‹ä»»åŠ¡çš„è¡¨ç°ã€‚è¯¥ç ”ç©¶ä¸ºæ·±å…¥ç†è§£å’Œä¼˜åŒ–LLMsçš„ä»·å€¼å¯¹é½ä¸äººæ ¼æ§åˆ¶æä¾›äº†é‡è¦çš„å¾®è§‚è§†è§’ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24319v2",
      "published_date": "2025-09-29 05:57:00 UTC",
      "updated_date": "2025-12-09 11:05:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:08:22.134892+00:00"
    },
    {
      "arxiv_id": "2509.24314v1",
      "title": "MedMMV: A Controllable Multimodal Multi-Agent Framework for Reliable and Verifiable Clinical Reasoning",
      "title_zh": "MedMMVï¼šé¢å‘å¯é ä¸”å¯éªŒè¯ä¸´åºŠæ¨ç†çš„å¯æ§å¤šæ¨¡æ€å¤šæ™ºèƒ½ä½“æ¡†æ¶",
      "authors": [
        "Hongjun Liu",
        "Yinghao Zhu",
        "Yuhui Wang",
        "Yitao Long",
        "Zeyu Lai",
        "Lequan Yu",
        "Chen Zhao"
      ],
      "abstract": "Recent progress in multimodal large language models (MLLMs) has demonstrated promising performance on medical benchmarks and in preliminary trials as clinical assistants. Yet, our pilot audit of diagnostic cases uncovers a critical failure mode: instability in early evidence interpretation precedes hallucination, creating branching reasoning trajectories that cascade into globally inconsistent conclusions. This highlights the need for clinical reasoning agents that constrain stochasticity and hallucination while producing auditable decision flows. We introduce MedMMV, a controllable multimodal multi-agent framework for reliable and verifiable clinical reasoning. MedMMV stabilizes reasoning through diversified short rollouts, grounds intermediate steps in a structured evidence graph under the supervision of a Hallucination Detector, and aggregates candidate paths with a Combined Uncertainty scorer. On six medical benchmarks, MedMMV improves accuracy by up to 12.7% and, more critically, demonstrates superior reliability. Blind physician evaluations confirm that MedMMV substantially increases reasoning truthfulness without sacrificing informational content. By controlling instability through a verifiable, multi-agent process, our framework provides a robust path toward deploying trustworthy AI systems in high-stakes domains like clinical decision support.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MedMMVï¼Œè¿™æ˜¯ä¸€ä¸ªå¯æ§çš„å¤šæ¨¡æ€å¤šæ™ºèƒ½ä½“æ¡†æ¶ (multimodal multi-agent framework)ï¼Œæ—¨åœ¨å®ç°å¯é ä¸”å¯éªŒè¯çš„ä¸´åºŠæ¨ç† (clinical reasoning)ã€‚é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) åœ¨è¯Šæ–­ä¸­å› æ—©æœŸè¯æ®è§£è¯»ä¸ç¨³å®šè€Œå¯¼è‡´çš„å¹»è§‰å’Œé€»è¾‘ä¸ä¸€è‡´é—®é¢˜ï¼ŒMedMMV é€šè¿‡å¤šæ ·åŒ–çš„çŸ­å‘¨æœŸå±•å¼€ (short rollouts) æ¥ç¨³å®šæ¨ç†è·¯å¾„ã€‚è¯¥æ¡†æ¶åœ¨å¹»è§‰æ£€æµ‹å™¨ (Hallucination Detector) çš„ç›‘ç£ä¸‹å°†ä¸­é—´æ­¥éª¤é”šå®šåœ¨ç»“æ„åŒ–è¯æ®å›¾ (evidence graph) ä¸­ï¼Œå¹¶ç»“åˆç»¼åˆä¸ç¡®å®šæ€§è¯„åˆ†å™¨ (Combined Uncertainty scorer) èšåˆå€™é€‰è·¯å¾„ï¼Œç¡®ä¿å†³ç­–è¿‡ç¨‹çš„å¯å®¡è®¡æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMedMMV åœ¨å…­é¡¹åŒ»å­¦åŸºå‡†æµ‹è¯•ä¸­å°†å‡†ç¡®ç‡æœ€é«˜æå‡äº† 12.7%ï¼Œå¹¶å±•ç°å‡ºå“è¶Šçš„å¯é æ€§ã€‚ç›²æµ‹è¯„ä¼°è¯å®è¯¥æ¡†æ¶åœ¨ä¸ç‰ºç‰²ä¿¡æ¯é‡çš„å‰æä¸‹æ˜¾è‘—å¢å¼ºäº†æ¨ç†çš„çœŸå®æ€§ï¼Œä¸ºåœ¨ä¸´åºŠå†³ç­–æ”¯æŒç­‰é«˜é£é™©é¢†åŸŸéƒ¨ç½²å¯ä¿¡èµ–çš„ AI ç³»ç»Ÿæä¾›äº†ç¨³å¥çš„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "25 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.24314v1",
      "published_date": "2025-09-29 05:51:25 UTC",
      "updated_date": "2025-09-29 05:51:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:08:23.988250+00:00"
    },
    {
      "arxiv_id": "2509.24306v1",
      "title": "A study of Universal ODE approaches to predicting soil organic carbon",
      "title_zh": "åŸºäºé€šç”¨å¾®åˆ†æ–¹ç¨‹çš„åœŸå£¤æœ‰æœºç¢³é¢„æµ‹æ–¹æ³•ç ”ç©¶",
      "authors": [
        "Satyanarayana Raju G. V.",
        "Prathamesh Dinesh Joshi",
        "Raj Abhijit Dandekar",
        "Rajat Dandekar",
        "Sreedath Panat"
      ],
      "abstract": "Soil Organic Carbon (SOC) is a foundation of soil health and global climate resilience, yet its prediction remains difficult because of intricate physical, chemical, and biological processes. In this study, we explore a Scientific Machine Learning (SciML) framework built on Universal Differential Equations (UDEs) to forecast SOC dynamics across soil depth and time. UDEs blend mechanistic physics, such as advection diffusion transport, with neural networks that learn nonlinear microbial production and respiration. Using synthetic datasets, we systematically evaluated six experimental cases, progressing from clean, noise free benchmarks to stress tests with high (35%) multiplicative, spatially correlated noise. Our results highlight both the potential and limitations of the approach. In noise free and moderate noise settings, the UDE accurately reconstructed SOC dynamics. In clean terminal profile at 50 years (Case 4) achieved near perfect fidelity, with MSE = 1.6e-5, and R2 = 0.9999. Case 5, with 7% noise, remained robust (MSE = 3.4e-6, R2 = 0.99998), capturing depth wise SOC trends while tolerating realistic measurement uncertainty. In contrast, Case 3 (35% noise at t = 0) showed clear evidence of overfitting: the model reproduced noisy inputs with high accuracy but lost generalization against the clean truth (R2 = 0.94). Case 6 (35% noise at t = 50) collapsed toward overly smooth mean profiles, failing to capture depth wise variability and yielding negative R2, underscoring the limits of standard training under severe uncertainty. These findings suggest that UDEs are well suited for scalable, noise tolerant SOC forecasting, though advancing toward field deployment will require noise aware loss functions, probabilistic modelling, and tighter integration of microbial dynamics.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢ç´¢äº†ä¸€ç§åŸºäºé€šç”¨å¾®åˆ†æ–¹ç¨‹(Universal Differential Equations, UDEs)çš„ç§‘å­¦æœºå™¨å­¦ä¹ (Scientific Machine Learning, SciML)æ¡†æ¶ï¼Œç”¨äºé¢„æµ‹åœŸå£¤æœ‰æœºç¢³(Soil Organic Carbon, SOC)éšæ·±åº¦å’Œæ—¶é—´çš„å˜åŒ–åŠ¨æ€ã€‚è¯¥æ¡†æ¶å°†å¯¹æµæ‰©æ•£ä¼ è¾“(advection diffusion transport)ç­‰ç‰©ç†æœºåˆ¶ä¸å­¦ä¹ å¾®ç”Ÿç‰©ç”Ÿäº§å’Œå‘¼å¸éçº¿æ€§ç‰¹å¾çš„ç¥ç»ç½‘ç»œç›¸ç»“åˆï¼Œå®ç°äº†ç‰©ç†æœºåˆ¶ä¸æ•°æ®é©±åŠ¨æ¨¡å‹çš„èåˆã€‚ç ”ç©¶é€šè¿‡å…­ä¸ªå®éªŒæ¡ˆä¾‹ç³»ç»Ÿè¯„ä¼°äº†è¯¥æ–¹æ³•åœ¨ä¸åŒå™ªå£°æ°´å¹³ä¸‹çš„è¡¨ç°ï¼Œç»“æœæ˜¾ç¤ºåœ¨æ— å™ªå£°å’Œä¸­åº¦å™ªå£°ï¼ˆå¦‚7%ï¼‰ç¯å¢ƒä¸‹ï¼ŒUDEèƒ½å¤Ÿä»¥æé«˜çš„ç²¾åº¦ï¼ˆR2 = 0.99998ï¼‰å‡†ç¡®é‡å»ºSOCåŠ¨æ€å¹¶æ•æ‰æ·±åº¦è¶‹åŠ¿ã€‚ç„¶è€Œï¼Œåœ¨é¢å¯¹35%çš„é«˜å¼ºåº¦å™ªå£°æ—¶ï¼Œæ¨¡å‹å‡ºç°äº†æ˜æ˜¾çš„è¿‡æ‹Ÿåˆæˆ–é¢„æµ‹å‰–é¢è¿‡åº¦å¹³æ»‘çš„é—®é¢˜ï¼Œæ­ç¤ºäº†è¯¥æ–¹æ³•åœ¨æç«¯ä¸ç¡®å®šæ€§ä¸‹çš„å±€é™æ€§ã€‚ç ”ç©¶ç»“è®ºè¡¨æ˜ï¼Œè™½ç„¶UDEåœ¨å¯æ‰©å±•ã€æŠ—å™ªçš„SOCé¢„æµ‹æ–¹é¢å…·æœ‰æ˜¾è‘—æ½œåŠ›ï¼Œä½†æœªæ¥å‘å®åœ°éƒ¨ç½²æ¨è¿›æ—¶ï¼Œä»éœ€å¼•å…¥æŠ—å™ªæŸå¤±å‡½æ•°ã€æ¦‚ç‡å»ºæ¨¡ä»¥åŠæ›´ç´§å¯†çš„å¾®ç”Ÿç‰©åŠ¨åŠ›å­¦é›†æˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24306v1",
      "published_date": "2025-09-29 05:42:28 UTC",
      "updated_date": "2025-09-29 05:42:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:08:48.996150+00:00"
    },
    {
      "arxiv_id": "2510.02369v3",
      "title": "AutoContext: Instance-Level Context Learning for LLM Agents",
      "title_zh": "AutoContextï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“çš„å®ä¾‹çº§ä¸Šä¸‹æ–‡å­¦ä¹ ",
      "authors": [
        "Kuntai Cai",
        "Juncheng Liu",
        "Xianglin Yang",
        "Zhaojie Niu",
        "Xiaokui Xiao",
        "Xing Chen"
      ],
      "abstract": "Current LLM agents typically lack instance-level context, which comprises concrete facts such as environment structure, system configurations, and local mechanics. Consequently, existing methods are forced to intertwine exploration with task execution. This coupling leads to redundant interactions and fragile decision-making, as agents must repeatedly rediscover the same information for every new task. To address this, we introduce AutoContext, a method that decouples exploration from task solving. AutoContext performs a systematic, one-off exploration to construct a reusable knowledge graph for each environment instance. This structured context allows off-the-shelf agents to access necessary facts directly, eliminating redundant exploration. Experiments across TextWorld, ALFWorld, Crafter, and InterCode-Bash demonstrate substantial gains: for example, the success rate of a ReAct agent on TextWorld improves from 37% to 95%, highlighting the critical role of structured instance context in efficient agentic systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ LLM Agents æ™®éç¼ºä¹å®ä¾‹çº§ä¸Šä¸‹æ–‡ (instance-level context) çš„é—®é¢˜æå‡ºäº† AutoContext æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³æ™ºèƒ½ä½“å› æ— æ³•æŒæ¡ç¯å¢ƒç»“æ„å’Œç³»ç»Ÿé…ç½®ç­‰å…·ä½“äº‹å®è€Œå¯¼è‡´çš„å†—ä½™æ¢ç´¢ä¸å†³ç­–è„†å¼±æ€§ã€‚AutoContext é€šè¿‡å°†æ¢ç´¢ä¸ä»»åŠ¡æ±‚è§£è¿‡ç¨‹è§£è€¦ï¼Œåˆ©ç”¨ä¸€æ¬¡æ€§çš„ç³»ç»ŸåŒ–æ¢ç´¢ä¸ºæ¯ä¸ªç¯å¢ƒå®ä¾‹æ„å»ºå¯é‡ç”¨çš„çŸ¥è¯†å›¾è°± (knowledge graph)ã€‚è¿™ç§ç»“æ„åŒ–ä¸Šä¸‹æ–‡å…è®¸ç°æˆçš„æ™ºèƒ½ä½“ç›´æ¥æå–å…³é”®äº‹å®ï¼Œä»è€Œæ¶ˆé™¤äº†åœ¨æ‰§è¡Œæ–°ä»»åŠ¡æ—¶å¯¹åŒä¸€ç¯å¢ƒè¿›è¡Œé‡å¤å‘ç°çš„éœ€æ±‚ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ TextWorldã€ALFWorldã€Crafter å’Œ InterCode-Bash ç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå…¶ä¸­ ReAct æ™ºèƒ½ä½“åœ¨ TextWorld ä¸Šçš„æˆåŠŸç‡ä» 37% å¤§å¹…æå‡è‡³ 95%ã€‚è¿™ä¸€ç ”ç©¶å¼ºè°ƒäº†ç»“æ„åŒ–å®ä¾‹ä¸Šä¸‹æ–‡å¯¹äºæ„å»ºé«˜æ•ˆæ™ºèƒ½ä½“ç³»ç»Ÿçš„é‡è¦æ€§ï¼Œå¹¶ä¸ºå®ç°æ›´ç¨³å¥ã€æ›´é«˜æ•ˆçš„è‡ªä¸»å†³ç­–å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.02369v3",
      "published_date": "2025-09-29 05:38:51 UTC",
      "updated_date": "2026-01-13 06:31:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:09:06.488086+00:00"
    },
    {
      "arxiv_id": "2509.24303v1",
      "title": "Experience Paper: Adopting Activity Recognition in On-demand Food Delivery Business",
      "title_zh": "ç»éªŒæŠ¥å‘Šï¼šè¡Œä¸ºè¯†åˆ«æŠ€æœ¯åœ¨å³æ—¶é…é€ä¸šåŠ¡ä¸­çš„åº”ç”¨",
      "authors": [
        "Huatao Xu",
        "Yan Zhang",
        "Wei Gao",
        "Guobin Shen",
        "Mo Li"
      ],
      "abstract": "This paper presents the first nationwide deployment of human activity recognition (HAR) technology in the on-demand food delivery industry. We successfully adapted the state-of-the-art LIMU-BERT foundation model to the delivery platform. Spanning three phases over two years, the deployment progresses from a feasibility study in Yangzhou City to nationwide adoption involving 500,000 couriers across 367 cities in China. The adoption enables a series of downstream applications, and large-scale tests demonstrate its significant operational and economic benefits, showcasing the transformative potential of HAR technology in real-world applications. Additionally, we share lessons learned from this deployment and open-source our LIMU-BERT pretrained with millions of hours of sensor data.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº†äººä½“æ´»åŠ¨è¯†åˆ«(Human Activity Recognition, HAR)æŠ€æœ¯åœ¨å³æ—¶å¤–å–è¡Œä¸šçš„é¦–æ¬¡å…¨å›½æ€§éƒ¨ç½²ç»éªŒã€‚ç ”ç©¶å›¢é˜ŸæˆåŠŸå°†å…ˆè¿›çš„LIMU-BERTåŸºç¡€æ¨¡å‹é€‚é…åˆ°å¤–å–å¹³å°ï¼Œé€šè¿‡ä¸ºæœŸä¸¤å¹´çš„ä¸‰ä¸ªé˜¶æ®µï¼Œå®ç°äº†ä»æ‰¬å·å¸‚è¯•ç‚¹åˆ°å…¨å›½367ä¸ªåŸå¸‚ã€50ä¸‡åéª‘æ‰‹çš„è§„æ¨¡åŒ–åº”ç”¨ã€‚å¤§è§„æ¨¡æµ‹è¯•è¯æ˜ï¼Œè¯¥æŠ€æœ¯çš„é‡‡ç”¨ä¸ä»…æ”¯æŒäº†å¤šç§ä¸‹æ¸¸åº”ç”¨ï¼Œè¿˜å¸¦æ¥äº†æ˜¾è‘—çš„è¿è¥å’Œç»æµæ•ˆç›Šï¼Œå……åˆ†å±•ç¤ºäº†HARæŠ€æœ¯åœ¨çœŸå®åœºæ™¯ä¸­çš„å˜é©æ½œåŠ›ã€‚æ­¤å¤–ï¼Œè®ºæ–‡æ€»ç»“äº†éƒ¨ç½²è¿‡ç¨‹ä¸­çš„ç»éªŒæ•™è®­ï¼Œå¹¶å¼€æºäº†åˆ©ç”¨æ•°ç™¾ä¸‡å°æ—¶ä¼ æ„Ÿå™¨æ•°æ®é¢„è®­ç»ƒçš„LIMU-BERTæ¨¡å‹ã€‚",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "13 pages",
      "pdf_url": "https://arxiv.org/pdf/2509.24303v1",
      "published_date": "2025-09-29 05:35:49 UTC",
      "updated_date": "2025-09-29 05:35:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:08:51.684862+00:00"
    },
    {
      "arxiv_id": "2509.24298v1",
      "title": "Bridging the behavior-neural gap: A multimodal AI reveals the brain's geometry of emotion more accurately than human self-reports",
      "title_zh": "å¼¥åˆè¡Œä¸º-ç¥ç»é¸¿æ²Ÿï¼šå¤šæ¨¡æ€äººå·¥æ™ºèƒ½æ­ç¤ºçš„å¤§è„‘æƒ…æ„Ÿå‡ ä½•æ¯”äººç±»è‡ªæˆ‘æŠ¥å‘Šæ›´å‡†ç¡®",
      "authors": [
        "Changde Du",
        "Yizhuo Lu",
        "Zhongyu Huang",
        "Yi Sun",
        "Zisen Zhou",
        "Shaozheng Qin",
        "Huiguang He"
      ],
      "abstract": "The ability to represent emotion plays a significant role in human cognition and social interaction, yet the high-dimensional geometry of this affective space and its neural underpinnings remain debated. A key challenge, the `behavior-neural gap,' is the limited ability of human self-reports to predict brain activity. Here we test the hypothesis that this gap arises from the constraints of traditional rating scales and that large-scale similarity judgments can more faithfully capture the brain's affective geometry. Using AI models as `cognitive agents,' we collected millions of triplet odd-one-out judgments from a multimodal large language model (MLLM) and a language-only model (LLM) in response to 2,180 emotionally evocative videos. We found that the emergent 30-dimensional embeddings from these models are highly interpretable and organize emotion primarily along categorical lines, yet in a blended fashion that incorporates dimensional properties. Most remarkably, the MLLM's representation predicted neural activity in human emotion-processing networks with the highest accuracy, outperforming not only the LLM but also, counterintuitively, representations derived directly from human behavioral ratings. This result supports our primary hypothesis and suggests that sensory grounding--learning from rich visual data--is critical for developing a truly neurally-aligned conceptual framework for emotion. Our findings provide compelling evidence that MLLMs can autonomously develop rich, neurally-aligned affective representations, offering a powerful paradigm to bridge the gap between subjective experience and its neural substrates. Project page: https://reedonepeck.github.io/ai-emotion.github.io/.",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³â€œè¡Œä¸º-ç¥ç»é¸¿æ²Ÿâ€(behavior-neural gap)é—®é¢˜ï¼Œå³äººç±»è‡ªæˆ‘æŠ¥å‘Šé¢„æµ‹å¤§è„‘æ´»åŠ¨èƒ½åŠ›ä¸è¶³çš„æŒ‘æˆ˜ã€‚ä½œè€…åˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLM)å’Œçº¯æ–‡æœ¬å¤§è¯­è¨€æ¨¡å‹(LLM)ä½œä¸ºè®¤çŸ¥æ™ºèƒ½ä½“ï¼Œé€šè¿‡æ•°ç™¾ä¸‡ä¸ªä¸‰å…ƒç»„å¼‚ç±»é€‰æ‹©(triplet odd-one-out)åˆ¤æ–­æå–äº†2180ä¸ªæƒ…ç»ªè§†é¢‘çš„ç‰¹å¾ã€‚ç ”ç©¶å‘ç°ï¼Œæ¨¡å‹ç”Ÿæˆçš„30ç»´åµŒå…¥å‘é‡(embeddings)èƒ½å¤Ÿé«˜åº¦å‡†ç¡®åœ°é¢„æµ‹äººç±»æƒ…ç»ªå¤„ç†ç½‘ç»œä¸­çš„ç¥ç»æ´»åŠ¨ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMLLMçš„é¢„æµ‹å‡†ç¡®ç‡ä¸ä»…è¶…è¿‡äº†LLMï¼Œç”šè‡³è¶…è¶Šäº†ç›´æ¥åŸºäºäººç±»è¡Œä¸ºè¯„åˆ†çš„è¡¨å¾ã€‚è¿™è¡¨æ˜ä»è§†è§‰æ•°æ®ä¸­å­¦ä¹ çš„æ„ŸçŸ¥æ¥åœ°(sensory grounding)å¯¹äºæ„å»ºä¸å¤§è„‘æœºåˆ¶ä¸€è‡´çš„æƒ…ç»ªæ¨¡å‹è‡³å…³é‡è¦ã€‚è¯¥å‘ç°è¯æ˜äº†MLLMå¯ä»¥è‡ªä¸»å½¢æˆç¥ç»å¯¹é½çš„æƒ…æ„Ÿè¡¨å¾ï¼Œä¸ºè¿æ¥ä¸»è§‚ä½“éªŒä¸å…¶ç¥ç»åŸºç¡€æä¾›äº†å¼ºæœ‰åŠ›çš„AIèŒƒå¼ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.MM"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24298v1",
      "published_date": "2025-09-29 05:22:33 UTC",
      "updated_date": "2025-09-29 05:22:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:09:08.794405+00:00"
    },
    {
      "arxiv_id": "2509.24297v2",
      "title": "Q-Mirror: Unlocking the Multi-Modal Potential of Scientific Text-Only QA Pairs",
      "title_zh": "Q-Mirrorï¼šé‡Šæ”¾ç§‘å­¦é¢†åŸŸçº¯æ–‡æœ¬é—®ç­”å¯¹çš„å¤šæ¨¡æ€æ½œåŠ›",
      "authors": [
        "Junying Wang",
        "Zicheng Zhang",
        "Ye Shen",
        "Yalun Wu",
        "Yingji Liang",
        "Yijin Guo",
        "Farong Wen",
        "Wenzhe Li",
        "Xuezhi Zhao",
        "Qi Jia",
        "Guangtao Zhai"
      ],
      "abstract": "High-quality, multi-modal benchmarks are crucial for advancing scientific reasoning in large models yet their manual creation is costly and unscalable. To address this bottleneck, we explore the potential for transforming Text-Only QA Pairs (TQAs) into high-quality Multi-Modal QA Pairs (MMQAs), which include three parts: 1) Task Definition \\& Evaluation Rubric: We develop a TQA-to-MMQA framework and establish a comprehensive, multi-dimensional MMQA quality rubric that provides principles for the transformation. 2) Benchmark Construction: Then we construct two extensive benchmarks to rigorously evaluate state-of-the-art generation \\& understanding models on the distinct tasks of MMQA generation \\& MMQA quality evaluation. 3) Preliminary Solution: We develop an agentic system (Q-Mirror), which operationalizes our framework by integrating MMQA generation and evaluation into a closed loop for iterative refinement. Our experiments show that while state-of-the-art models can generate MMQAs, their outputs still leave substantial gaps, underscoring the need for reliable evaluation. We further demonstrate that top-tier understanding models align closely with human judgment in MMQA quality assessment. Leveraging both insights, the Q-Mirror agent raises average scores from 78.90 to 85.22 and pass rates from 72\\% to 95\\%, offering a practical path to large-scale scientific benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Q-Mirrorï¼Œæ—¨åœ¨å°†çº¯æ–‡æœ¬é—®ç­”å¯¹ (Text-Only QA Pairs, TQAs) è½¬åŒ–ä¸ºé«˜è´¨é‡çš„å¤šæ¨¡æ€é—®ç­”å¯¹ (Multi-Modal QA Pairs, MMQAs)ï¼Œä»¥è§£å†³ç§‘å­¦æ¨ç†é¢†åŸŸå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•æ‰‹åŠ¨åˆ›å»ºæˆæœ¬é«˜ä¸”éš¾ä»¥æ‰©å±•çš„é—®é¢˜ã€‚ä½œè€…é¦–å…ˆåˆ¶å®šäº†TQA-to-MMQAè½¬æ¢æ¡†æ¶å’Œå¤šç»´åº¦çš„è´¨é‡è¯„æµ‹å‡†åˆ™ï¼Œå¹¶æ„å»ºäº†ä¸¤ä¸ªå¤§è§„æ¨¡åŸºå‡†æ¥è¯„ä¼°æ¨¡å‹åœ¨MMQAç”Ÿæˆä¸è´¨é‡è¯„ä»·ä¸Šçš„èƒ½åŠ›ã€‚Q-Mirrorç³»ç»Ÿä½œä¸ºä¸€ç§æ™ºèƒ½ä½“æ–¹æ¡ˆï¼Œé€šè¿‡å°†ç”Ÿæˆä¸è¯„ä»·æ•´åˆè¿›é—­ç¯è¿›è¡Œè¿­ä»£ä¼˜åŒ–ï¼Œå®ç°äº†MMQAè´¨é‡çš„æŒç»­æå‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒQ-Mirrorå°†ç”Ÿæˆç»“æœçš„å¹³å‡å¾—åˆ†ä»78.90æé«˜åˆ°85.22ï¼Œé€šè¿‡ç‡ä»72%æ˜¾è‘—æå‡è‡³95%ã€‚è¯¥ç ”ç©¶è¯æ˜äº†é¡¶çº§å¤šæ¨¡æ€ç†è§£æ¨¡å‹åœ¨è´¨é‡è¯„ä¼°ä¸Šä¸äººç±»åˆ¤æ–­é«˜åº¦ä¸€è‡´ï¼Œä¸ºå¤§è§„æ¨¡ç§‘å­¦å¤šæ¨¡æ€æ•°æ®çš„è‡ªåŠ¨åŒ–æ„å»ºæä¾›äº†åˆ‡å®å¯è¡Œçš„è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "25 pages",
      "pdf_url": "https://arxiv.org/pdf/2509.24297v2",
      "published_date": "2025-09-29 05:22:10 UTC",
      "updated_date": "2025-09-30 04:56:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:09:09.091312+00:00"
    },
    {
      "arxiv_id": "2509.24296v1",
      "title": "DiffuGuard: How Intrinsic Safety is Lost and Found in Diffusion Large Language Models",
      "title_zh": "DiffuGuardï¼šæ‰©æ•£å¤§è¯­è¨€æ¨¡å‹å†…ç”Ÿå®‰å…¨æ€§çš„ä¸§å¤±ä¸å¯»å›",
      "authors": [
        "Zherui Li",
        "Zheng Nie",
        "Zhenhong Zhou",
        "Yufei Guo",
        "Yue Liu",
        "Yitong Zhang",
        "Yu Cheng",
        "Qingsong Wen",
        "Kun Wang",
        "Jiaheng Zhang"
      ],
      "abstract": "The rapid advancement of Diffusion Large Language Models (dLLMs) introduces unprecedented vulnerabilities that are fundamentally distinct from Autoregressive LLMs, stemming from their iterative and parallel generation mechanisms. In this paper, we conduct an in-depth analysis of dLLM vulnerabilities to jailbreak attacks across two distinct dimensions: intra-step and inter-step dynamics. Experimental results reveal a harmful bias inherent in the standard greedy remasking strategy and identify a critical phenomenon we term Denoising-path Dependence, where the safety of early-stage tokens decisively influences the final output. These findings also indicate that while current decoding strategies constitute a significant vulnerability, dLLMs possess a substantial intrinsic safety potential. To unlock this potential, we propose DiffuGuard, a training-free defense framework that addresses vulnerabilities through a dual-stage approach: Stochastic Annealing Remasking dynamically introduces controlled randomness to mitigate greedy selection bias, while Block-level Audit and Repair exploits internal model representations for autonomous risk detection and guided correction. Comprehensive experiments on four dLLMs demonstrate DiffuGuard's exceptional effectiveness, reducing Attack Success Rate against six diverse jailbreak methods from 47.9% to 14.7% while preserving model utility and efficiency. Our code is available at: https://github.com/niez233/DiffuGuard.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹ (Diffusion Large Language Models, dLLMs) å› è¿­ä»£å’Œå¹¶è¡Œç”Ÿæˆæœºåˆ¶è€Œäº§ç”Ÿçš„ç‹¬ç‰¹å®‰å…¨æ¼æ´ã€‚ä½œè€…ä»æ­¥éª¤å†… (intra-step) å’Œæ­¥éª¤é—´ (inter-step) ä¸¤ä¸ªç»´åº¦åˆ†æäº†è¶Šç‹±æ”»å‡»ï¼Œæ­ç¤ºäº†æ ‡å‡†è´ªå©ªé‡æ©ç ç­–ç•¥ (greedy remasking strategy) çš„å›ºæœ‰åå·®ä»¥åŠå»å™ªè·¯å¾„ä¾èµ– (Denoising-path Dependence) ç°è±¡ï¼Œå³æ—©æœŸ token çš„å®‰å…¨æ€§å¯¹æœ€ç»ˆè¾“å‡ºå…·æœ‰å†³å®šæ€§å½±å“ã€‚å°½ç®¡è§£ç ç­–ç•¥å­˜åœ¨é£é™©ï¼Œä½†ç ”ç©¶å‘ç° dLLMs å…·æœ‰å·¨å¤§çš„å†…åœ¨å®‰å…¨æ½œåŠ›ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†æ— éœ€è®­ç»ƒçš„é˜²å¾¡æ¡†æ¶ DiffuGuardï¼Œé€šè¿‡éšæœºé€€ç«é‡æ©ç  (Stochastic Annealing Remasking) å¼•å…¥éšæœºæ€§ä»¥æ¶ˆé™¤åå·®ï¼Œå¹¶åˆ©ç”¨å—çº§å®¡è®¡ä¸ä¿®å¤ (Block-level Audit and Repair) ç»“åˆæ¨¡å‹å†…éƒ¨è¡¨ç¤ºè¿›è¡Œé£é™©æ£€æµ‹ä¸ä¿®æ­£ã€‚å®éªŒè¯æ˜ï¼ŒDiffuGuard åœ¨å››ç§ dLLMs ä¸Šå°†å…­ç§è¶Šç‹±æ–¹æ³•çš„æ”»å‡»æˆåŠŸç‡ (Attack Success Rate) ä» 47.9% æ˜¾è‘—é™ä½è‡³ 14.7%ï¼ŒåŒæ—¶æœ‰æ•ˆå…¼é¡¾äº†æ¨¡å‹çš„æ•ˆç”¨å’Œæ•ˆç‡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24296v1",
      "published_date": "2025-09-29 05:17:10 UTC",
      "updated_date": "2025-09-29 05:17:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:09:19.796608+00:00"
    },
    {
      "arxiv_id": "2510.00060v2",
      "title": "Less is More: Lean yet Powerful Vision-Language Model for Autonomous Driving",
      "title_zh": "å°‘å³æ˜¯å¤šï¼šç²¾ç®€è€Œå¼ºå¤§çš„è‡ªåŠ¨é©¾é©¶è§†è§‰è¯­è¨€æ¨¡å‹",
      "authors": [
        "Sheng Yang",
        "Tong Zhan",
        "Guancheng Chen",
        "Yanfeng Lu",
        "Jian Wang"
      ],
      "abstract": "In this work, we reconceptualize autonomous driving as a generalized language and formulate the trajectory planning task as next waypoint prediction. We introduce Max-V1, a novel framework for one-stage end-to-end autonomous driving. Our framework presents a single-pass generation paradigm that aligns with the inherent sequentiality of driving. This approach leverages the generative capacity of the VLM (Vision-Language Model) to enable end-to-end trajectory prediction directly from front-view camera input. The efficacy of this method is underpinned by a principled supervision strategy derived from statistical modeling. This provides a well-defined learning objective, which makes the framework highly amenable to master complex driving policies through imitation learning from large-scale expert demonstrations. Empirically, our method achieves the state-of-the-art performance on the nuScenes dataset, delivers an overall improvement of over 30% compared to prior baselines. Furthermore, it exhibits superior generalization performance on cross-domain datasets acquired from diverse vehicles, demonstrating notable potential for cross-vehicle robustness and adaptability. Due to these empirical strengths, this work introduces a model enabling fundamental driving behaviors, laying the foundation for the development of more capable self-driving agents. Code will be available upon publication.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Max-V1ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå•é˜¶æ®µç«¯åˆ°ç«¯(one-stage end-to-end)è‡ªåŠ¨é©¾é©¶çš„æ–°å‹æ¡†æ¶ï¼Œå®ƒå°†è‡ªåŠ¨é©¾é©¶é‡æ–°æ„æƒ³ä¸ºä¸€ç§å¹¿ä¹‰è¯­è¨€ï¼Œå¹¶å°†è½¨è¿¹è§„åˆ’ä»»åŠ¡è½¬åŒ–ä¸ºä¸‹ä¸€è·¯æ ‡ç‚¹é¢„æµ‹(next waypoint prediction)ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸é©¾é©¶é¡ºåºæ€§ç›¸å¥‘åˆçš„å•æ¬¡ç”ŸæˆèŒƒå¼(single-pass generation paradigm)ï¼Œåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹(VLM)çš„ç”Ÿæˆèƒ½åŠ›ï¼Œç›´æ¥ä»å‰è§†æ‘„åƒå¤´è¾“å…¥å®ç°ç«¯åˆ°ç«¯çš„è½¨è¿¹é¢„æµ‹ã€‚é€šè¿‡æºè‡ªç»Ÿè®¡å»ºæ¨¡çš„ç›‘ç£ç­–ç•¥ï¼ŒMax-V1èƒ½å¤Ÿæœ‰æ•ˆåœ°é€šè¿‡æ¨¡ä»¿å­¦ä¹ (imitation learning)ä»å¤§è§„æ¨¡ä¸“å®¶æ¼”ç¤ºä¸­å­¦ä¹ å¤æ‚çš„é©¾é©¶ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨nuScenesæ•°æ®é›†ä¸Šå®ç°äº†SOTAæ€§èƒ½ï¼Œè¾ƒä»¥å¾€åŸºçº¿æ¨¡å‹æ•´ä½“æå‡äº†30%ä»¥ä¸Šã€‚æ­¤å¤–ï¼ŒMax-V1åœ¨ä¸åŒè½¦è¾†çš„è·¨åŸŸæ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„æ³›åŒ–æ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨è·¨è½¦å‹é²æ£’æ€§å’Œé€‚åº”æ€§æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚è¯¥å·¥ä½œä¸ä»…å®ç°äº†åŸºç¡€é©¾é©¶è¡Œä¸ºï¼Œä¹Ÿä¸ºå¼€å‘æ›´å¼ºå¤§çš„è‡ªåŠ¨é©¾é©¶æ™ºèƒ½ä½“å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00060v2",
      "published_date": "2025-09-29 05:14:18 UTC",
      "updated_date": "2025-10-03 15:30:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:09:27.674644+00:00"
    },
    {
      "arxiv_id": "2509.24291v1",
      "title": "Let LLMs Speak Embedding Languages: Generative Text Embeddings via Iterative Contrastive Refinement",
      "title_zh": "è®©å¤§è¯­è¨€æ¨¡å‹â€œè¯´å‡ºâ€åµŒå…¥è¯­è¨€ï¼šåŸºäºè¿­ä»£å¯¹æ¯”ç²¾ç‚¼çš„ç”Ÿæˆå¼æ–‡æœ¬åµŒå…¥",
      "authors": [
        "Yu-Che Tsai",
        "Kuan-Yu Chen",
        "Yuan-Chi Li",
        "Yuan-Hao Chen",
        "Ching-Yu Tsai",
        "Shou-De Lin"
      ],
      "abstract": "Existing large language model (LLM)-based embeddings typically adopt an encoder-only paradigm, treating LLMs as static feature extractors and overlooking their core generative strengths. We introduce GIRCSE (Generative Iterative Refinement for Contrastive Sentence Embeddings), a novel framework that leverages autoregressive generation to iteratively refine semantic representations. By producing sequences of soft tokens optimized under contrastive objective, GIRCSE captures latent concepts and implicit semantics that encoder-only methods often miss. To guide this process, we propose an Iterative Contrastive Refinement (ICR) objective that encourages each refinement step to yield better representations. Extensive experiments show that GIRCSE outperforms strong LLM-based embedding baselines on the MTEB benchmark and instruction-following tasks. Moreover, GIRCSE exhibits an emergent test-time scaling property: generating more tokens at inference steadily improves embedding quality. Our results establish generative iterative refinement as a new paradigm for representation learning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†GIRCSE (Generative Iterative Refinement for Contrastive Sentence Embeddings)ï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨è‡ªå›å½’ç”Ÿæˆ(autoregressive generation)è¿­ä»£ä¼˜åŒ–è¯­ä¹‰è¡¨ç¤ºçš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨çªç ´ä¼ ç»Ÿå¤§è¯­è¨€æ¨¡å‹(LLMs)ä½œä¸ºé™æ€ç‰¹å¾æå–å™¨çš„ç¼–ç å™¨èŒƒå¼å±€é™ã€‚è¯¥æ–¹æ³•é€šè¿‡ç”Ÿæˆå—å¯¹æ¯”å­¦ä¹ ç›®æ ‡(contrastive objective)ä¼˜åŒ–çš„è½¯ä»¤ç‰Œ(soft tokens)åºåˆ—ï¼Œèƒ½å¤Ÿæ•æ‰åˆ°ç¼–ç å™¨æ¨¡å‹å¾€å¾€ä¼šé—æ¼çš„æ½œåœ¨æ¦‚å¿µå’Œéšå¼è¯­ä¹‰ã€‚ä¸ºäº†å¼•å¯¼è¿™ä¸€è¿‡ç¨‹ï¼Œç ”ç©¶è€…è®¾è®¡äº†è¿­ä»£å¯¹æ¯”ç»†åŒ–(Iterative Contrastive Refinement, ICR)ç›®æ ‡ï¼Œç¡®ä¿æ¯ä¸ªç»†åŒ–æ­¥éª¤éƒ½èƒ½äº§ç”Ÿæ›´ç²¾å‡†çš„è¡¨ç¤ºã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGIRCSEåœ¨MTEBåŸºå‡†æµ‹è¯•å’ŒæŒ‡ä»¤éµå¾ªä»»åŠ¡ä¸­å‡ä¼˜äºå¼ºåŸºçº¿æ¨¡å‹ï¼Œå±•ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¯¥æ¨¡å‹è¡¨ç°å‡ºäº†æ¶Œç°çš„æµ‹è¯•æ—¶ç¼©æ”¾(test-time scaling)ç‰¹æ€§ï¼Œå³åœ¨æ¨ç†è¿‡ç¨‹ä¸­é€šè¿‡ç”Ÿæˆæ›´å¤šä»¤ç‰Œèƒ½ç¨³æ­¥æå‡åµŒå…¥è´¨é‡ã€‚è¿™ä¸€ç ”ç©¶æˆæœç¡®ç«‹äº†ç”Ÿæˆå¼è¿­ä»£ç»†åŒ–ä½œä¸ºè¡¨ç¤ºå­¦ä¹ (representation learning)çš„æ–°èŒƒå¼ï¼Œå……åˆ†å‘æŒ¥äº†LLMsçš„æ ¸å¿ƒç”Ÿæˆä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24291v1",
      "published_date": "2025-09-29 05:09:08 UTC",
      "updated_date": "2025-09-29 05:09:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:09:44.576018+00:00"
    },
    {
      "arxiv_id": "2509.24285v1",
      "title": "SCI-Verifier: Scientific Verifier with Thinking",
      "title_zh": "SCI-Verifierï¼šå…·å¤‡æ€è€ƒèƒ½åŠ›çš„ç§‘å­¦éªŒè¯å™¨",
      "authors": [
        "Shenghe Zheng",
        "Chenyu Huang",
        "Fangchen Yu",
        "Junchi Yao",
        "Jingqi Ye",
        "Tao Chen",
        "Yun Luo",
        "Ning Ding",
        "LEI BAI",
        "Ganqu Cui",
        "Peng Ye"
      ],
      "abstract": "As large language models (LLMs) are increasingly applied to scientific reasoning, the complexity of answer formats and the diversity of equivalent expressions make answer verification a critical yet challenging task. Existing verification studies in scientific domains suffer from two major limitations: (a) the absence of systematic evaluation standards and insufficient disciplinary coverage, which hinders their comprehensive assessment; and (b) heavy reliance on cumbersome rule design or prompt engineering, which reduces their effectiveness in complex reasoning scenarios or limits their cross-disciplinary generalization. To address these challenges, we propose solutions at both the data and model levels. On the data side, we construct SCI-VerifyBench, a cross-disciplinary benchmark covering mathematics, physics, biology, chemistry, and general scientific QA. The benchmark is built from real LLM responses and enhanced with domain-specific equivalence transformations that generate challenging and realistic data. Model-based and expert annotations ensure both quality and diversity, enabling rigorous evaluation of verification ability. On the model side, we emphasize the importance of reasoning for verification and introduce SCI-Verifier, a unified reasoning-augmented verifier for scientific domains. Through post-training, SCI-Verifier demonstrates strong logical reasoning and equivalence judgment capabilities while maintaining concise and stable outputs. Together, SCI-VerifyBench and SCI-Verifier provide a principled framework for scientific verification, offering both systematic evaluation and practical pathways to enhance the reliability and applicability of LLMs in scientific domains.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ç§‘å­¦æ¨ç†ä¸­ç”±äºç­”æ¡ˆæ ¼å¼å¤æ‚å’Œç­‰ä»·è¡¨è¾¾å¤šæ ·è€Œå¯¼è‡´çš„éªŒè¯éš¾é¢˜ï¼Œæå‡ºäº†SCI-VerifyBenchå’ŒSCI-Verifierã€‚SCI-VerifyBenchæ˜¯ä¸€ä¸ªæ¶µç›–æ•°å­¦ã€ç‰©ç†ã€ç”Ÿç‰©ã€åŒ–å­¦åŠé€šç”¨ç§‘å­¦é—®ç­”çš„è·¨å­¦ç§‘åŸºå‡†æµ‹è¯•é›†ï¼Œå®ƒåŸºäºçœŸå®æ¨¡å‹å“åº”å¹¶ç»“åˆé¢†åŸŸç‰¹å®šçš„ç­‰ä»·è½¬æ¢(equivalence transformations)æ„å»ºï¼Œç¡®ä¿äº†è¯„ä¼°çš„ä¸¥è°¨æ€§ä¸å¤šæ ·æ€§ã€‚åœ¨æ¨¡å‹å±‚é¢ï¼Œç ”ç©¶è€…å¼•å…¥äº†SCI-Verifierï¼Œè¿™æ˜¯ä¸€ç§ç»Ÿä¸€çš„ã€å…·å¤‡æ¨ç†å¢å¼º(reasoning-augmented)èƒ½åŠ›çš„ç§‘å­¦é¢†åŸŸéªŒè¯å™¨ã€‚é€šè¿‡åæœŸè®­ç»ƒ(post-training)ï¼ŒSCI-Verifieråœ¨ä¿æŒè¾“å‡ºç®€æ´ç¨³å®šçš„åŒæ—¶ï¼Œå±•ç°å‡ºäº†å¼ºå¤§çš„é€»è¾‘æ¨ç†å’Œç­‰ä»·åˆ¤å®šèƒ½åŠ›ã€‚è¯¥ç ”ç©¶ä¸ºç§‘å­¦éªŒè¯æä¾›äº†ä¸€ä¸ªåŸåˆ™æ€§çš„æ¡†æ¶ï¼Œä¸ºæå‡LLMsåœ¨ç§‘å­¦é¢†åŸŸçš„å¯é æ€§å’Œé€‚ç”¨æ€§æä¾›äº†å®è·µè·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "This paper focuses on LLM-as-a-Judge, and the project is currently in progress",
      "pdf_url": "https://arxiv.org/pdf/2509.24285v1",
      "published_date": "2025-09-29 04:58:43 UTC",
      "updated_date": "2025-09-29 04:58:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:09:27.488483+00:00"
    },
    {
      "arxiv_id": "2509.24282v2",
      "title": "SimuHome: A Temporal- and Environment-Aware Benchmark for Smart Home LLM Agents",
      "title_zh": "SimuHomeï¼šé¢å‘æ™ºèƒ½å®¶å±… LLM æ™ºèƒ½ä½“çš„æ—¶é—´ä¸ç¯å¢ƒæ„ŸçŸ¥åŸºå‡†",
      "authors": [
        "Gyuhyeon Seo",
        "Jungwoo Yang",
        "Junseong Pyo",
        "Nalim Kim",
        "Jonggeun Lee",
        "Yohan Jo"
      ],
      "abstract": "Large Language Model (LLM) agents excel at multi-step, tool-augmented tasks. However, smart homes introduce distinct challenges, requiring agents to handle latent user intents, temporal dependencies, device constraints, scheduling, and more. The main bottlenecks for developing smart home agents with such capabilities include the lack of a realistic simulation environment where agents can interact with devices and observe the results, as well as a challenging benchmark to evaluate them. To address this, we introduce $\\textbf{SimuHome}$, a time-accelerated home environment that simulates smart devices, supports API calls, and reflects changes in environmental variables. By building the simulator on the Matter protocol, the global industry standard for smart home communication, SimuHome provides a high-fidelity environment, and agents validated in SimuHome can be deployed on real Matter-compliant devices with minimal adaptation. We provide a challenging benchmark of 600 episodes across twelve user query types that require the aforementioned capabilities. Our evaluation of 16 agents under a unified ReAct framework reveals distinct capabilities and limitations across models. Models under 7B parameters exhibited negligible performance across all query types. Even GPT-4.1, the best-performing standard model, struggled with implicit intent inference, state verification, and particularly temporal scheduling. While reasoning models such as GPT-5.1 consistently outperformed standard models on every query type, they required over three times the average inference time, which can be prohibitive for real-time smart home applications. This highlights a critical trade-off between task performance and real-world practicality.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SimuHomeï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰æ—¶é—´ä¸ç¯å¢ƒæ„ŸçŸ¥èƒ½åŠ›çš„æ™ºèƒ½å®¶å±…LLM agentsåŸºå‡†æµ‹è¯•å’Œæ¨¡æ‹Ÿç¯å¢ƒï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ™ºèƒ½ä½“åœ¨å¤„ç†æ½œåœ¨æ„å›¾(latent user intents)ã€æ—¶é—´ä¾èµ–(temporal dependencies)å’Œè®¾å¤‡çº¦æŸç­‰å¤æ‚ä»»åŠ¡æ—¶çš„è¯„ä¼°éš¾é¢˜ã€‚SimuHomeåŸºäºMatteråè®®æ„å»ºï¼Œæä¾›äº†ä¸€ä¸ªé«˜ä¿çœŸä¸”æ”¯æŒAPIè°ƒç”¨çš„æ—¶é—´åŠ é€Ÿç¯å¢ƒï¼Œç¡®ä¿åœ¨æ¨¡æ‹Ÿå™¨ä¸­éªŒè¯çš„æ™ºèƒ½ä½“èƒ½ä»¥æå°æˆæœ¬è¿ç§»è‡³çœŸå®ç¡¬ä»¶ã€‚ç ”ç©¶è€…åˆ©ç”¨åŒ…å«600ä¸ªæƒ…å¢ƒã€è¦†ç›–12ç§æŸ¥è¯¢ç±»å‹çš„æŒ‘æˆ˜æ€§åŸºå‡†ï¼Œå¯¹16ä¸ªåœ¨ReActæ¡†æ¶ä¸‹çš„æ™ºèƒ½ä½“è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå‚æ•°é‡å°äº7Bçš„æ¨¡å‹è¡¨ç°ä¸ä½³ï¼Œå³ä½¿æ˜¯GPT-4.1åœ¨éšæ€§æ„å›¾æ¨ç†å’Œæ—¶é—´è°ƒåº¦(temporal scheduling)æ–¹é¢ä¹Ÿé¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ã€‚å°½ç®¡GPT-5.1ç­‰æ¨ç†æ¨¡å‹åœ¨å„é¡¹ä»»åŠ¡ä¸­æ€§èƒ½å ä¼˜ï¼Œä½†å…¶æ¨ç†æ—¶é—´æ¯”æ ‡å‡†æ¨¡å‹é•¿ä¸‰å€ä»¥ä¸Šï¼Œè¿™å‡¸æ˜¾äº†æ™ºèƒ½å®¶å±…å®æ—¶åº”ç”¨ä¸­ä»»åŠ¡æ€§èƒ½ä¸å®ç”¨æ€§ä¹‹é—´çš„å…³é”®æƒè¡¡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages",
      "pdf_url": "https://arxiv.org/pdf/2509.24282v2",
      "published_date": "2025-09-29 04:54:20 UTC",
      "updated_date": "2025-12-08 08:28:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:09:29.888350+00:00"
    },
    {
      "arxiv_id": "2509.24276v1",
      "title": "G-reasoner: Foundation Models for Unified Reasoning over Graph-structured Knowledge",
      "title_zh": "G-reasonerï¼šé¢å‘å›¾ç»“æ„çŸ¥è¯†ç»Ÿä¸€æ¨ç†çš„åŸºç¡€æ¨¡å‹",
      "authors": [
        "Linhao Luo",
        "Zicheng Zhao",
        "Junnan Liu",
        "Zhangchi Qiu",
        "Junnan Dong",
        "Serge Panev",
        "Chen Gong",
        "Thuy-Trang Vu",
        "Gholamreza Haffari",
        "Dinh Phung",
        "Alan Wee-Chung Liew",
        "Shirui Pan"
      ],
      "abstract": "Large language models (LLMs) excel at complex reasoning but remain limited by static and incomplete parametric knowledge. Retrieval-augmented generation (RAG) mitigates this by incorporating external knowledge, yet existing RAGs struggle with knowledge-intensive tasks due to fragmented information and weak modeling of knowledge structure. Graphs offer a natural way to model relationships within knowledge, but LLMs are inherently unstructured and cannot effectively reason over graph-structured data. Recent graph-enhanced RAG (GraphRAG) attempts to bridge this gap by constructing tailored graphs and enabling LLMs to reason on them. However, these methods often depend on ad-hoc graph designs, heuristic search, or costly agent pipelines, which hinder scalability and generalization. To address these challenges, we present G-reasoner, a unified framework that integrates graph and language foundation models for reasoning over diverse graph-structured knowledge. Central to our approach is QuadGraph, a standardized four-layer abstraction that unifies heterogeneous knowledge sources into a common graph representation. Building on this, we introduce a 34M-parameter graph foundation model (GFM) that jointly captures graph topology and textual semantics, and is integrated with LLMs to enhance reasoning in downstream applications. To ensure scalability and efficiency, mixed-precision training and distributed message-passing are implemented to scale GFM with more GPUs. Extensive experiments on six benchmarks show that G-reasoner consistently outperforms state-of-the-art baselines, significantly enhances LLM reasoning, and achieves strong efficiency and cross-graph generalization.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† G-reasonerï¼Œä¸€ä¸ªæ—¨åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) å¯¹å›¾ç»“æ„çŸ¥è¯†æ¨ç†èƒ½åŠ›çš„ç»Ÿä¸€æ¡†æ¶ï¼Œè§£å†³äº†ç°æœ‰ Retrieval-augmented generation (RAG) æŠ€æœ¯åœ¨å¤„ç†å¤æ‚çŸ¥è¯†ç»“æ„æ—¶çš„ç¢ç‰‡åŒ–é—®é¢˜ã€‚æ ¸å¿ƒåˆ›æ–°åœ¨äºå¼•å…¥äº† QuadGraph æ ‡å‡†åŒ–å››å±‚æŠ½è±¡ï¼Œèƒ½å¤Ÿå°†å¼‚æ„çŸ¥è¯†æºè½¬åŒ–ä¸ºç»Ÿä¸€çš„å›¾è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼€å‘äº†ä¸€ä¸ª 34M å‚æ•°çš„å›¾åŸºç¡€æ¨¡å‹ (GFM)ï¼Œé€šè¿‡ååŒæ•æ‰å›¾æ‹“æ‰‘ä¸æ–‡æœ¬è¯­ä¹‰ï¼Œå¹¶ä¸ LLMs æ·±åº¦é›†æˆä»¥ä¼˜åŒ–ä¸‹æ¸¸æ¨ç†ä»»åŠ¡ã€‚ä¸ºæå‡ç³»ç»Ÿæ‰©å±•æ€§ï¼Œè¯¥æ–¹æ¡ˆé‡‡ç”¨äº†æ··åˆç²¾åº¦è®­ç»ƒå’Œåˆ†å¸ƒå¼æ¶ˆæ¯ä¼ é€’æœºåˆ¶ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒG-reasoner åœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸Šå‡ä¼˜äºæœ€å…ˆè¿›çš„åŸºå‡†æ¨¡å‹ï¼Œå¹¶åœ¨è®¡ç®—æ•ˆç‡å’Œè·¨å›¾æ³›åŒ–æ–¹é¢è¡¨ç°å“è¶Šã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "22 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.24276v1",
      "published_date": "2025-09-29 04:38:12 UTC",
      "updated_date": "2025-09-29 04:38:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:09:57.874143+00:00"
    },
    {
      "arxiv_id": "2509.24274v2",
      "title": "Adversarial Reinforcement Learning Framework for ESP Cheater Simulation",
      "title_zh": "ç”¨äº ESP ä½œå¼Šè€…æ¨¡æ‹Ÿçš„å¯¹æŠ—å¼ºåŒ–å­¦ä¹ æ¡†æ¶",
      "authors": [
        "Inkyu Park",
        "Jeong-Gwan Lee",
        "Taehwan Kwon",
        "Juheon Choi",
        "Seungku Kim",
        "Junsu Kim",
        "Kimin Lee"
      ],
      "abstract": "Extra-Sensory Perception (ESP) cheats, which reveal hidden in-game information such as enemy locations, are difficult to detect because their effects are not directly observable in player behavior. The lack of observable evidence makes it difficult to collect reliably labeled data, which is essential for training effective anti-cheat systems. Furthermore, cheaters often adapt their behavior by limiting or disguising their cheat usage, which further complicates detection and detector development. To address these challenges, we propose a simulation framework for controlled modeling of ESP cheaters, non-cheaters, and trajectory-based detectors. We model cheaters and non-cheaters as reinforcement learning agents with different levels of observability, while detectors classify their behavioral trajectories. Next, we formulate the interaction between the cheater and the detector as an adversarial game, allowing both players to co-adapt over time. To reflect realistic cheater strategies, we introduce a structured cheater model that dynamically switches between cheating and non-cheating behaviors based on detection risk. Experiments demonstrate that our framework successfully simulates adaptive cheater behaviors that strategically balance reward optimization and detection evasion. This work provides a controllable and extensible platform for studying adaptive cheating behaviors and developing effective cheat detectors.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹ ESP (Extra-Sensory Perception) ä½œå¼Šå› ç¼ºä¹ç›´æ¥å¯è§‚æµ‹è¯æ®è€Œéš¾ä»¥æ£€æµ‹ä¸”ç¼ºä¹æ ‡æ³¨æ•°æ®çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç”¨äºæ¨¡æ‹Ÿä½œå¼Šè€…çš„å¯¹æŠ—æ€§å¼ºåŒ–å­¦ä¹  (Adversarial Reinforcement Learning) æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†ä½œå¼Šè€…ã€éä½œå¼Šè€…å’Œæ£€æµ‹å™¨å»ºæ¨¡ä¸ºå…·æœ‰ä¸åŒè§‚æµ‹èƒ½åŠ›çš„å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ï¼Œå¹¶å°†åŒæ–¹çš„äº¤äº’æ„å»ºä¸ºå¯¹æŠ—åšå¼ˆä»¥å®ç°ååŒè¿›åŒ–ã€‚ç ”ç©¶è¿˜å¼•å…¥äº†ç»“æ„åŒ–ä½œå¼Šæ¨¡å‹ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿæ ¹æ®æ£€æµ‹é£é™©åœ¨ä½œå¼Šä¸å¸¸è§„è¡Œä¸ºä¹‹é—´åŠ¨æ€åˆ‡æ¢ï¼Œä»è€Œåæ˜ çœŸå®çš„ä½œå¼Šç­–ç•¥ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶æˆåŠŸæ¨¡æ‹Ÿäº†ä½œå¼Šè€…åœ¨è¿½æ±‚å¥–åŠ±æœ€å¤§åŒ–ä¸é€ƒé¿æ£€æµ‹é£é™©ä¹‹é—´çš„ç­–ç•¥æ€§å¹³è¡¡ã€‚è¯¥ç ”ç©¶ä¸ºæ·±å…¥ç†è§£è‡ªé€‚åº”ä½œå¼Šè¡Œä¸ºä»¥åŠå¼€å‘æ›´ä¸ºæœ‰æ•ˆçš„åä½œå¼Šç³»ç»Ÿæä¾›äº†ä¸€ä¸ªå¯æ§ä¸”å¯æ‰©å±•çš„ä»¿çœŸå¹³å°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24274v2",
      "published_date": "2025-09-29 04:32:45 UTC",
      "updated_date": "2025-12-30 02:22:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:10:05.168346+00:00"
    },
    {
      "arxiv_id": "2509.24269v1",
      "title": "AdvChain: Adversarial Chain-of-Thought Tuning for Robust Safety Alignment of Large Reasoning Models",
      "title_zh": "AdvChainï¼šé¢å‘å¤§æ¨ç†æ¨¡å‹é²æ£’å®‰å…¨å¯¹é½çš„å¯¹æŠ—æ€§é“¾å¼æ€ç»´å¾®è°ƒ",
      "authors": [
        "Zihao Zhu",
        "Xinyu Wu",
        "Gehan Hu",
        "Siwei Lyu",
        "Ke Xu",
        "Baoyuan Wu"
      ],
      "abstract": "Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in complex problem-solving through Chain-of-Thought (CoT) reasoning. However, the multi-step nature of CoT introduces new safety challenges that extend beyond conventional language model alignment. We identify a failure mode in current safety CoT tuning methods: the \\textit{snowball effect}, where minor reasoning deviations progressively amplify throughout the thought process, leading to either harmful compliance or excessive refusal. This effect stems from models being trained to imitate perfect reasoning scripts without learning to self-correct. To address this limitation, we propose AdvChain, an alignment paradigm that teaches models dynamic self-correction through adversarial CoT tuning. Our method involves constructing a dataset containing Temptation-Correction and Hesitation-Correction samples, where models learn to recover from harmful reasoning drifts and unnecessary cautions. Extensive experiments show that AdvChain significantly enhances robustness against jailbreak attacks and CoT hijacking while substantially reducing over-refusal on benign prompts, achieving a superior safety-utility balance without compromising reasoning capabilities. Our work establishes a new direction for building more robust and reliable reasoning models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹æ¨ç†æ¨¡å‹(Large Reasoning Models)åœ¨Chain-of-Thought (CoT)æ¨ç†ä¸­é¢ä¸´çš„å®‰å…¨æ€§æŒ‘æˆ˜ï¼Œæå‡ºäº†AdvChainå¯¹é½èŒƒå¼ã€‚ä½œè€…è¯†åˆ«å‡ºå½“å‰å®‰å…¨å¾®è°ƒä¸­å­˜åœ¨çš„â€œé›ªçƒæ•ˆåº”(snowball effect)â€ï¼Œå³æ¨ç†ä¸­çš„å¾®å°åå·®ä¼šéšæ€è€ƒè¿‡ç¨‹ä¸æ–­æ”¾å¤§ï¼Œæœ€ç»ˆå¯¼è‡´æ¨¡å‹äº§ç”Ÿæœ‰å®³è¾“å‡ºæˆ–è¿‡åº¦æ‹’ç»ã€‚AdvChainé€šè¿‡å¯¹æŠ—æ€§CoTå¾®è°ƒ(Adversarial CoT Tuning)æ•™å¯¼æ¨¡å‹è¿›è¡ŒåŠ¨æ€è‡ªæˆ‘çº æ­£ï¼Œé€šè¿‡æ„å»ºåŒ…å«è¯±æƒ‘çº æ­£(Temptation-Correction)å’ŒçŠ¹è±«çº æ­£(Hesitation-Correction)çš„æ•°æ®é›†ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä»æœ‰å®³æ¼‚ç§»æˆ–è¿‡åº¦è°¨æ…ä¸­æ¢å¤ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ˜¾è‘—æå‡æ¨¡å‹å¯¹æŠ—è¶Šç‹±æ”»å‡»(jailbreak attacks)å’ŒCoTåŠ«æŒ(CoT hijacking)é²æ£’æ€§çš„åŒæ—¶ï¼Œå¤§å¹…é™ä½äº†å¯¹è‰¯æ€§æç¤ºçš„è¿‡åº¦æ‹’ç»ï¼Œå®ç°äº†å®‰å…¨ä¸æ•ˆèƒ½çš„ä¼˜å¼‚å¹³è¡¡ã€‚è¿™é¡¹å·¥ä½œä¸ºæ„å»ºæ›´ç¨³å¥ã€æ›´å¯é çš„æ¨ç†æ¨¡å‹æä¾›äº†é‡è¦çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24269v1",
      "published_date": "2025-09-29 04:27:23 UTC",
      "updated_date": "2025-09-29 04:27:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:10:03.789894+00:00"
    },
    {
      "arxiv_id": "2509.24267v2",
      "title": "Cycle Diffusion Model for Counterfactual Image Generation",
      "title_zh": "ç”¨äºåäº‹å®å›¾åƒç”Ÿæˆçš„å¾ªç¯æ‰©æ•£æ¨¡å‹",
      "authors": [
        "Fangrui Huang",
        "Alan Wang",
        "Binxu Li",
        "Bailey Trang",
        "Ridvan Yesiloglu",
        "Tianyu Hua",
        "Wei Peng",
        "Ehsan Adeli"
      ],
      "abstract": "Deep generative models have demonstrated remarkable success in medical image synthesis. However, ensuring conditioning faithfulness and high-quality synthetic images for direct or counterfactual generation remains a challenge. In this work, we introduce a cycle training framework to fine-tune diffusion models for improved conditioning adherence and enhanced synthetic image realism. Our approach, Cycle Diffusion Model (CDM), enforces consistency between generated and original images by incorporating cycle constraints, enabling more reliable direct and counterfactual generation. Experiments on a combined 3D brain MRI dataset (from ABCD, HCP aging & young adults, ADNI, and PPMI) show that our method improves conditioning accuracy and enhances image quality as measured by FID and SSIM. The results suggest that the cycle strategy used in CDM can be an effective method for refining diffusion-based medical image generation, with applications in data augmentation, counterfactual, and disease progression modeling.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»å­¦å½±åƒåˆæˆä¸­æ¡ä»¶å¿ å®åº¦(Conditioning faithfulness)å’Œé«˜è´¨é‡åäº‹å®ç”Ÿæˆ(Counterfactual generation)å­˜åœ¨çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†å¾ªç¯æ‰©æ•£æ¨¡å‹(Cycle Diffusion Model, CDM)ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¸€ä¸ªå¾ªç¯è®­ç»ƒæ¡†æ¶æ¥å¾®è°ƒæ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡å¾ªç¯çº¦æŸ(Cycle constraints)ç¡®ä¿ç”Ÿæˆå›¾åƒä¸åŸå§‹å›¾åƒä¹‹é—´çš„ä¸€è‡´æ€§ï¼Œä»è€Œå¢å¼ºåˆæˆå›¾åƒçš„çœŸå®æ„Ÿã€‚ç ”ç©¶è€…åœ¨åŒ…æ‹¬ABCDã€HCPã€ADNIå’ŒPPMIåœ¨å†…çš„å¤šä¸ª3Dè„‘éƒ¨MRIæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ã€‚ç»“æœè¡¨æ˜ï¼ŒCDMåœ¨æé«˜æ¡ä»¶å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œåœ¨FIDå’ŒSSIMæŒ‡æ ‡ä¸Šä¹Ÿæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚è¯¥å¾ªç¯ç­–ç•¥ä¸ºç²¾ç»†åŒ–æ‰©æ•£æ¨¡å‹åœ¨åŒ»å­¦å½±åƒé¢†åŸŸçš„åº”ç”¨æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆï¼Œå°¤å…¶åœ¨æ•°æ®å¢å¼º(Data augmentation)å’Œç–¾ç—…è¿›å±•å»ºæ¨¡(Disease progression modeling)æ–¹é¢å…·æœ‰é‡è¦ä»·å€¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24267v2",
      "published_date": "2025-09-29 04:24:13 UTC",
      "updated_date": "2025-10-30 03:29:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:10:06.367725+00:00"
    },
    {
      "arxiv_id": "2509.24263v2",
      "title": "PAME-AI: Patient Messaging Creation and Optimization using Agentic AI",
      "title_zh": "PAME-AIï¼šåŸºäºæ™ºèƒ½ä½“ AI çš„æ‚£è€…æ¶ˆæ¯ç”Ÿæˆä¸ä¼˜åŒ–",
      "authors": [
        "Junjie Luo",
        "Yihong Guo",
        "Anqi Liu",
        "Ritu Agarwal",
        "Gordon Gao"
      ],
      "abstract": "Messaging patients is a critical part of healthcare communication, helping to improve things like medication adherence and healthy behaviors. However, traditional mobile message design has significant limitations due to its inability to explore the high-dimensional design space. We develop PAME-AI, a novel approach for Patient Messaging Creation and Optimization using Agentic AI. Built on the Data-Information-Knowledge-Wisdom (DIKW) hierarchy, PAME-AI offers a structured framework to move from raw data to actionable insights for high-performance messaging design. PAME-AI is composed of a system of specialized computational agents that progressively transform raw experimental data into actionable message design strategies. We demonstrate our approach's effectiveness through a two-stage experiment, comprising of 444,691 patient encounters in Stage 1 and 74,908 in Stage 2. The best-performing generated message achieved 68.76% engagement compared to the 61.27% baseline, representing a 12.2% relative improvement in click-through rates. This agentic architecture enables parallel processing, hypothesis validation, and continuous learning, making it particularly suitable for large-scale healthcare communication optimization.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº† PAME-AIï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨æ™ºèƒ½ä½“äººå·¥æ™ºèƒ½ (Agentic AI) è¿›è¡Œæ‚£è€…æ¶ˆæ¯åˆ›å»ºä¸ä¼˜åŒ–çš„åˆ›æ–°æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿç§»åŠ¨æ¶ˆæ¯è®¾è®¡åœ¨æ¢ç´¢é«˜ç»´è®¾è®¡ç©ºé—´æ–¹é¢çš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶åŸºäºæ•°æ®-ä¿¡æ¯-çŸ¥è¯†-æ™ºæ…§ (DIKW) ç­‰çº§ä½“ç³»æ„å»ºï¼Œé€šè¿‡ä¸“é—¨çš„è®¡ç®—æ™ºèƒ½ä½“ (computational agents) ç³»ç»Ÿå°†åŸå§‹å®éªŒæ•°æ®é€æ­¥è½¬åŒ–ä¸ºé«˜æ€§èƒ½çš„æ¶ˆæ¯è®¾è®¡ç­–ç•¥ã€‚PAME-AI çš„æ™ºèƒ½ä½“æ¶æ„æ”¯æŒå¹¶è¡Œå¤„ç†ã€å‡è®¾éªŒè¯å’ŒæŒç»­å­¦ä¹ ï¼Œèƒ½å¤Ÿåœ¨å¤§è§„æ¨¡åŒ»ç–—å¥åº·é€šä¿¡ä¸­å®ç°é«˜æ•ˆä¼˜åŒ–ã€‚é€šè¿‡æ¶‰åŠè¶…è¿‡ 50 ä¸‡æ¬¡æ‚£è€…äº’åŠ¨çš„ä¸¤é˜¶æ®µå®éªŒéªŒè¯ï¼ŒPAME-AI ç”Ÿæˆçš„æœ€ä¼˜æ¶ˆæ¯è¾¾åˆ°äº† 68.76% çš„å‚ä¸ç‡ï¼Œç›¸è¾ƒäº 61.27% çš„åŸºå‡†çº¿ï¼Œå…¶ç‚¹å‡»ç‡ (click-through rates) è·å¾—äº† 12.2% çš„ç›¸å¯¹æå‡ã€‚è¿™ä¸€ç ”ç©¶æˆæœè¯æ˜äº† Agentic AI åœ¨æå‡æ‚£è€…ç”¨è¯ä¾ä»æ€§å’Œå¥åº·è¡Œä¸ºæ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24263v2",
      "published_date": "2025-09-29 04:14:46 UTC",
      "updated_date": "2025-09-30 05:26:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:10:13.485885+00:00"
    },
    {
      "arxiv_id": "2509.24262v2",
      "title": "LAMP-PRo: Label-aware Attention for Multi-label Prediction of DNA- and RNA-binding Proteins using Protein Language Models",
      "title_zh": "LAMP-PRoï¼šåŸºäºè›‹ç™½è´¨è¯­è¨€æ¨¡å‹å’Œæ ‡ç­¾æ„ŸçŸ¥æ³¨æ„åŠ›çš„ DNA åŠ RNA ç»“åˆè›‹ç™½å¤šæ ‡ç­¾é¢„æµ‹",
      "authors": [
        "Nimisha Ghosh",
        "Dheeran Sankaran",
        "Rahul Balakrishnan Adhi",
        "Sharath S",
        "Amrut Anand"
      ],
      "abstract": "Identifying DNA- (DBPs) and RNA-binding proteins (RBPs) is crucial for the understanding of cell function, molecular interactions as well as regulatory functions. Owing to their high similarity, most of the existing approaches face challenges in differentiating between DBPs and RBPs leading to high cross-prediction errors. Moreover, identifying proteins which bind to both DNA and RNA (DRBPs) is also quite a challenging task. In this regard, we propose a novel framework viz. LAMP-PRo which is based on pre-trained protein language model (PLM), attention mechanisms and multi-label learning to mitigate these issues. First, pre-trained PLM such ESM-2 is used for embedding the protein sequences followed by convolutional neural network (CNN). Subsequently multi-head self-attention mechanism is applied for the contextual information while label-aware attention is used to compute class-specific representations by attending to the sequence in a way that is tailored to each label (DBP, RBP and non-NABP) in a multi-label setup. We have also included a novel cross-label attention mechanism to explicitly capture dependencies between DNA- and RNA-binding proteins, enabling more accurate prediction of DRBP. Finally, a linear layer followed by a sigmoid function are used for the final prediction. Extensive experiments are carried out to compare LAMP-PRo with the existing methods wherein the proposed model shows consistent competent performance. Furthermore, we also provide visualization to showcase model interpretability, highlighting which parts of the sequence are most relevant for a predicted label. The original datasets are available at http://bliulab.net/iDRBP\\_MMC and the codes are available at https://github.com/NimishaGhosh/LAMP-PRo.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LAMP-PRo æ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨é¢„è®­ç»ƒè›‹ç™½è´¨è¯­è¨€æ¨¡å‹ (Protein Language Models, PLMs) å®ç°å¯¹ DNA ç»“åˆè›‹ç™½ (DBPs) å’Œ RNA ç»“åˆè›‹ç™½ (RBPs) çš„å¤šæ ‡ç­¾é¢„æµ‹ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•éš¾ä»¥åŒºåˆ†é«˜ç›¸ä¼¼æ€§çš„ DBPs å’Œ RBPs ä»¥åŠè¯†åˆ«åŒæ—¶ç»“åˆ DNA/RNA çš„åŒç»“åˆè›‹ç™½ (DRBPs) çš„æŒ‘æˆ˜ï¼Œè¯¥æ¡†æ¶é›†æˆäº† ESM-2 æ¨¡å‹ä¸å·ç§¯ç¥ç»ç½‘ç»œ (CNN)ã€‚ç ”ç©¶å¼•å…¥äº†å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ (multi-head self-attention) æ¥æ•æ‰ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¹¶åˆ©ç”¨æ ‡ç­¾æ„ŸçŸ¥æ³¨æ„åŠ› (label-aware attention) ä¸º DBPã€RBP å’Œéæ ¸é…¸ç»“åˆè›‹ç™½ (non-NABP) è®¡ç®—ç‰¹å®šç±»åˆ«çš„è¡¨å¾ã€‚æ­¤å¤–ï¼ŒLAMP-PRo é‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„è·¨æ ‡ç­¾æ³¨æ„åŠ› (cross-label attention) æœºåˆ¶ï¼Œä¸“é—¨æ•æ‰ DNA å’Œ RNA ç»“åˆè›‹ç™½ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†å¯¹ DRBPs çš„é¢„æµ‹å‡†ç¡®æ€§ã€‚å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼ŒLAMP-PRo åœ¨ä¸ç°æœ‰æ–¹æ³•çš„å¯¹æ¯”ä¸­è¡¨ç°å‡ºæŒç»­çš„ç«äº‰åŠ›ï¼Œå¹¶é€šè¿‡å¯è§†åŒ–åˆ†æå±•ç¤ºäº†æ¨¡å‹åœ¨è¯†åˆ«åºåˆ—å…³é”®åŒºåŸŸæ–¹é¢çš„å¯è§£é‡Šæ€§ã€‚",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.QM",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24262v2",
      "published_date": "2025-09-29 04:13:51 UTC",
      "updated_date": "2025-10-21 13:08:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:10:17.801871+00:00"
    },
    {
      "arxiv_id": "2509.24261v1",
      "title": "Risk-Sensitive RL for Alleviating Exploration Dilemmas in Large Language Models",
      "title_zh": "ç¼“è§£å¤§è¯­è¨€æ¨¡å‹æ¢ç´¢å›°å¢ƒçš„é£é™©æ•æ„Ÿå¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Yuhua Jiang",
        "Jiawei Huang",
        "Yufeng Yuan",
        "Xin Mao",
        "Yu Yue",
        "Qianchuan Zhao",
        "Lin Yan"
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective for enhancing Large Language Models (LLMs) on complex reasoning tasks. However, existing methods suffer from an exploration dilemma: the sharply peaked initial policies of pre-trained LLMs confine standard RL algorithms to a narrow set of solutions, boosting single-solution accuracy (pass@1) but suppressing solution diversity and multi-solution performance (pass@k). As a result, RLVR often distills existing capabilities rather than discovering new reasoning strategies. To overcome this, we introduce a Risk-Sensitive Reinforcement Learning framework. Our approach employs a risk-seeking objective that interpolates between mean and maximum rewards, leading to a novel algorithm, Risk-Sensitive GRPO (RS-GRPO), which drives deeper exploration by amplifying learning from challenging prompts. Remarkably, RS-GRPO is simple to implement, requiring only minor code modifications. On six mathematical reasoning benchmarks and with five different LLMs, RS-GRPO consistently improves pass@k performance while maintaining or enhancing pass@1 accuracy.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­å­˜åœ¨çš„æ¢ç´¢å›°å¢ƒè¿›è¡Œäº†æ¢è®¨ï¼ŒæŒ‡å‡ºå½“å‰æ–¹æ³•å› åˆå§‹ç­–ç•¥åˆ†å¸ƒè¿‡äºé›†ä¸­è€Œé™åˆ¶äº†è§£å†³æ–¹æ¡ˆçš„å¤šæ ·æ€§å’Œå¤šè§£æ€§èƒ½ï¼ˆpass@kï¼‰ã€‚ä¸ºäº†å…‹æœè¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº†é£é™©æ•æ„Ÿå¼ºåŒ–å­¦ä¹ ï¼ˆRisk-Sensitive Reinforcement Learningï¼‰æ¡†æ¶ï¼Œé‡‡ç”¨ä¸€ç§åœ¨å¹³å‡å¥–åŠ±ä¸æœ€å¤§å¥–åŠ±ä¹‹é—´è¿›è¡Œæ’å€¼çš„é£é™©å¯»æ±‚ç›®æ ‡ï¼ˆrisk-seeking objectiveï¼‰ã€‚åŸºäºæ­¤æ¡†æ¶å¼€å‘çš„RS-GRPOç®—æ³•é€šè¿‡å¼ºåŒ–å¯¹æŒ‘æˆ˜æ€§ä»»åŠ¡çš„å­¦ä¹ ï¼Œå¼•å¯¼æ¨¡å‹è¿›è¡Œæ›´æ·±å±‚æ¬¡çš„æ¢ç´¢ï¼Œä¸”åœ¨å®ç°ä¸Šä»…éœ€å¾®å°çš„ä»£ç æ”¹åŠ¨ã€‚åœ¨å…­é¡¹æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•å’Œäº”ç§ä¸åŒLLMsä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRS-GRPOåœ¨ä¿æŒæˆ–æå‡pass@1å‡†ç¡®ç‡çš„åŸºç¡€ä¸Šï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹çš„pass@kæ€§èƒ½ï¼Œæœ‰æ•ˆæ¨åŠ¨äº†æ–°æ¨ç†ç­–ç•¥çš„å‘ç°ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24261v1",
      "published_date": "2025-09-29 04:12:20 UTC",
      "updated_date": "2025-09-29 04:12:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:10:19.269789+00:00"
    },
    {
      "arxiv_id": "2509.24260v2",
      "title": "Rethinking and Benchmarking Large Language Models for Graph Reasoning",
      "title_zh": "å›¾æ¨ç†å¤§è¯­è¨€æ¨¡å‹çš„é‡æ–°å®¡è§†ä¸åŸºå‡†æµ‹è¯•",
      "authors": [
        "Yuwei Hu",
        "Xinyi Huang",
        "Zhewei Wei",
        "Yongchao Liu",
        "Chuntao Hong"
      ],
      "abstract": "Large Language Models (LLMs) for Graph Reasoning have been extensively studied over the past two years, involving enabling LLMs to understand graph structures and reason on graphs to solve various graph problems, with graph algorithm problems being the most prevalent. Recent studies underscore the potential of LLMs in handling graph reasoning tasks, but their performance is underwhelming. In this work, we point out issues with existing methods and benchmarks, and rethink the direction that LLMs for graph reasoning should strive toward. We find that base models, e.g., GPT-4o-mini, are largely underestimated due to improper reasoning focus. Base models with reasoning focus redirected from replicating graph algorithms to designing them can easily solve most graph reasoning tasks in existing benchmarks. To truly evaluate the graph reasoning capabilities of LLMs, we construct a more challenging GraphAlgorithm benchmark, comprising 239 different graph problems and 3,041 test instances collected from 4 competition platforms. Finally, we introduce a simple and strong baseline Simple-Reasoning-Then-Coding (Simple-RTC)-which guides LLMs to design graph algorithms first and then code to address graph reasoning tasks. Simple-RTC achieves near-perfect accuracy on existing benchmarks and significantly outperforms GPT-4o-mini and all prior methods on the GraphAlgorithm benchmark. This strong baseline encourages further advancements in LLMs for Graph Reasoning in the future.",
      "tldr_zh": "è¯¥ç ”ç©¶é‡æ–°å®¡è§†äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å›¾æ¨ç†ï¼ˆGraph Reasoningï¼‰ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼ŒæŒ‡å‡ºå½“å‰æ¨¡å‹èƒ½åŠ›è¢«ä½ä¼°æºäºæ¨ç†é‡ç‚¹è¿‡äºå€¾å‘äºå¤åˆ¶å›¾ç®—æ³•è€Œéè®¾è®¡ç®—æ³•ã€‚ä¸ºäº†æ›´å®¢è§‚åœ°è¯„ä¼° LLMs çš„å›¾æ¨ç†èƒ½åŠ›ï¼Œä½œè€…æ„å»ºäº†ä¸€ä¸ªæ›´å…·æŒ‘æˆ˜æ€§çš„ GraphAlgorithm åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«ä»å¤šä¸ªç«èµ›å¹³å°æ”¶é›†çš„ 239 ç§å›¾é—®é¢˜å’Œ 3,041 ä¸ªæµ‹è¯•å®ä¾‹ã€‚è®ºæ–‡æå‡ºäº†ä¸€ç§ç®€æ´è€Œå¼ºæ•ˆçš„ Simple-Reasoning-Then-Coding (Simple-RTC) åŸºçº¿æ–¹æ³•ï¼Œé€šè¿‡å¼•å¯¼æ¨¡å‹éµå¾ªâ€œå…ˆè®¾è®¡å›¾ç®—æ³•ï¼Œåç¼–å†™ä»£ç â€çš„æµç¨‹æ¥è§£å†³å¤æ‚çš„å›¾æ¨ç†ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSimple-RTC åœ¨ç°æœ‰åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†è¿‘ä¹å®Œç¾çš„å‡†ç¡®ç‡ï¼Œå¹¶åœ¨ GraphAlgorithm æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äº GPT-4o-mini åŠæ‰€æœ‰å…ˆå‰çš„æ–¹æ³•ã€‚è¯¥é¡¹å·¥ä½œé€šè¿‡é‡æ–°å®šä¹‰æ¨ç†æ–¹å‘å’Œæä¾›æ›´é«˜æ ‡å‡†çš„è¯„ä¼°ä½“ç³»ï¼Œä¸º LLMs åœ¨å›¾æ¨ç†é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24260v2",
      "published_date": "2025-09-29 04:10:12 UTC",
      "updated_date": "2025-10-02 01:19:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:10:27.160754+00:00"
    },
    {
      "arxiv_id": "2510.00059v1",
      "title": "FSDENet: A Frequency and Spatial Domains based Detail Enhancement Network for Remote Sensing Semantic Segmentation",
      "title_zh": "FSDENetï¼šåŸºäºé¢‘åŸŸä¸ç©ºåŸŸç»†èŠ‚å¢å¼ºçš„é¥æ„Ÿè¯­ä¹‰åˆ†å‰²ç½‘ç»œ",
      "authors": [
        "Jiahao Fu",
        "Yinfeng Yu",
        "Liejun Wang"
      ],
      "abstract": "To fully leverage spatial information for remote sensing image segmentation and address semantic edge ambiguities caused by grayscale variations (e.g., shadows and low-contrast regions), we propose the Frequency and Spatial Domains based Detail Enhancement Network (FSDENet). Our framework employs spatial processing methods to extract rich multi-scale spatial features and fine-grained semantic details. By effectively integrating global and frequency-domain information through the Fast Fourier Transform (FFT) in global mappings, the model's capability to discern global representations under grayscale variations is significantly strengthened. Additionally, we utilize Haar wavelet transform to decompose features into high- and low-frequency components, leveraging their distinct sensitivity to edge information to refine boundary segmentation. The model achieves dual-domain synergy by integrating spatial granularity with frequency-domain edge sensitivity, substantially improving segmentation accuracy in boundary regions and grayscale transition zones. Comprehensive experimental results demonstrate that FSDENet achieves state-of-the-art (SOTA) performance on four widely adopted datasets: LoveDA, Vaihingen, Potsdam, and iSAID.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FSDENetï¼Œè¿™æ˜¯ä¸€ç§åŸºäºé¢‘ç‡å’Œç©ºé—´åŸŸçš„ç»†èŠ‚å¢å¼ºç½‘ç»œï¼Œæ—¨åœ¨å……åˆ†åˆ©ç”¨é¥æ„Ÿå›¾åƒçš„ç©ºé—´ä¿¡æ¯å¹¶è§£å†³ç”±äºç°åº¦å˜åŒ–ï¼ˆå¦‚é˜´å½±å’Œä½å¯¹æ¯”åº¦åŒºåŸŸï¼‰å¯¼è‡´çš„è¯­ä¹‰è¾¹ç¼˜æ¨¡ç³Šé—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ç©ºé—´å¤„ç†æ–¹æ³•æå–ä¸°å¯Œçš„å¤šå°ºåº¦ç©ºé—´ç‰¹å¾(multi-scale spatial features)å’Œç»†ç²’åº¦è¯­ä¹‰ç»†èŠ‚ã€‚é€šè¿‡åœ¨å…¨å±€æ˜ å°„ä¸­å¼•å…¥å¿«é€Ÿå‚…é‡Œå¶å˜æ¢(FFT)ï¼Œæ¨¡å‹æœ‰æ•ˆæ•´åˆäº†å…¨å±€ä¸é¢‘åŸŸä¿¡æ¯ï¼Œæ˜¾è‘—å¢å¼ºäº†åœ¨å¤æ‚ç°åº¦ç¯å¢ƒä¸‹è¾¨åˆ«å…¨å±€è¡¨ç¤ºçš„èƒ½åŠ›ã€‚åŒæ—¶ï¼ŒFSDENetåˆ©ç”¨å“ˆå°”å°æ³¢å˜æ¢(Haar wavelet transform)å°†ç‰¹å¾åˆ†è§£ä¸ºé«˜é¢‘å’Œä½é¢‘åˆ†é‡ï¼Œé€šè¿‡æ•æ‰è¾¹ç¼˜æ•æ„Ÿä¿¡æ¯æ¥ç²¾ç»†åŒ–è¾¹ç•Œåˆ†å‰²ã€‚è¯¥æ¨¡å‹é€šè¿‡ç©ºé—´ç²’åº¦ä¸é¢‘åŸŸè¾¹ç¼˜æ•æ„Ÿæ€§çš„åŒåŸŸååŒï¼Œå¤§å¹…æå‡äº†è¾¹ç•ŒåŒºåŸŸå’Œç°åº¦è¿‡æ¸¡åŒºçš„åˆ†å‰²ç²¾åº¦ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒFSDENetåœ¨LoveDAã€Vaihingenã€Potsdamå’ŒiSAIDå››ä¸ªä¸»æµæ•°æ®é›†ä¸Šå‡è¾¾åˆ°äº†å½“å‰çš„SOTAæ€§èƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted for publication by IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing",
      "pdf_url": "https://arxiv.org/pdf/2510.00059v1",
      "published_date": "2025-09-29 04:09:09 UTC",
      "updated_date": "2025-09-29 04:09:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:10:24.061495+00:00"
    },
    {
      "arxiv_id": "2510.03286v1",
      "title": "A Biologically Interpretable Cognitive Architecture for Online Structuring of Episodic Memories into Cognitive Maps",
      "title_zh": "å°†æƒ…èŠ‚è®°å¿†åœ¨çº¿æ„å»ºä¸ºè®¤çŸ¥åœ°å›¾çš„ç”Ÿç‰©å­¦å¯è§£é‡Šè®¤çŸ¥æ¶æ„",
      "authors": [
        "E. A. Dzhivelikian",
        "A. I. Panov"
      ],
      "abstract": "Cognitive maps provide a powerful framework for understanding spatial and abstract reasoning in biological and artificial agents. While recent computational models link cognitive maps to hippocampal-entorhinal mechanisms, they often rely on global optimization rules (e.g., backpropagation) that lack biological plausibility. In this work, we propose a novel cognitive architecture for structuring episodic memories into cognitive maps using local, Hebbian-like learning rules, compatible with neural substrate constraints. Our model integrates the Successor Features framework with episodic memories, enabling incremental, online learning through agent-environment interaction. We demonstrate its efficacy in a partially observable grid-world, where the architecture autonomously organizes memories into structured representations without centralized optimization. This work bridges computational neuroscience and AI, offering a biologically grounded approach to cognitive map formation in artificial adaptive agents.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å…·æœ‰ç”Ÿç‰©è§£é‡Šæ€§çš„è®¤çŸ¥æ¶æ„ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è®¤çŸ¥åœ°å›¾(Cognitive maps)æ¨¡å‹è¿‡åº¦ä¾èµ–åå‘ä¼ æ’­(backpropagation)ç­‰ç¼ºä¹ç”Ÿç‰©åˆç†æ€§çš„å…¨å±€ä¼˜åŒ–è§„åˆ™çš„é—®é¢˜ã€‚è¯¥æ¶æ„é€šè¿‡æ¨¡æ‹Ÿç¥ç»åº•å±‚çš„çº¦æŸï¼Œåˆ©ç”¨å±€éƒ¨èµ«å¸ƒå­¦ä¹ è§„åˆ™(Hebbian-like learning rules)å°†æƒ…å¢ƒè®°å¿†(Episodic memories)åœ¨çº¿ç»“æ„åŒ–ä¸ºè®¤çŸ¥åœ°å›¾ã€‚æ¨¡å‹å·§å¦™åœ°ç»“åˆäº†ç»§ä»»ç‰¹å¾(Successor Features)æ¡†æ¶ä¸æƒ…å¢ƒè®°å¿†ï¼Œé€šè¿‡æ™ºèƒ½ä½“ä¸ç¯å¢ƒçš„æŒç»­äº¤äº’å®ç°äº†é«˜æ•ˆçš„å¢é‡å¼åœ¨çº¿å­¦ä¹ ã€‚ç ”ç©¶åœ¨éƒ¨åˆ†å¯è§‚æµ‹çš„ç½‘æ ¼ä¸–ç•Œ(grid-world)ç¯å¢ƒä¸­è¿›è¡Œäº†å®éªŒï¼ŒéªŒè¯äº†è¯¥æ¶æ„åœ¨æ— éœ€é›†ä¸­ä¼˜åŒ–çš„å‰æä¸‹è‡ªä¸»ç»„ç»‡è®°å¿†å¹¶å½¢æˆç»“æ„åŒ–è¡¨ç¤ºçš„èƒ½åŠ›ã€‚è¿™é¡¹å·¥ä½œæˆåŠŸè¡”æ¥äº†è®¡ç®—ç¥ç»ç§‘å­¦ä¸äººå·¥æ™ºèƒ½é¢†åŸŸï¼Œä¸ºäººå·¥è‡ªé€‚åº”æ™ºèƒ½ä½“ä¸­è®¤çŸ¥åœ°å›¾çš„å½¢æˆæä¾›äº†ä¸€ç§æ›´åŠ ç¬¦åˆç”Ÿç‰©å­¦åŸç†çš„æ–°é€”å¾„ã€‚",
      "categories": [
        "q-bio.NC",
        "cs.AI"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03286v1",
      "published_date": "2025-09-29 04:07:38 UTC",
      "updated_date": "2025-09-29 04:07:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:10:28.277474+00:00"
    },
    {
      "arxiv_id": "2509.24256v1",
      "title": "Graph Foundation Models: Bridging Language Model Paradigms and Graph Optimization",
      "title_zh": "å›¾åŸºç¡€æ¨¡å‹ï¼šæ¡¥æ¥è¯­è¨€æ¨¡å‹èŒƒå¼ä¸å›¾ä¼˜åŒ–",
      "authors": [
        "Yunhao Liang",
        "Pujun Zhang",
        "Yuan Qu",
        "Shaochong Lin",
        "Zuo-jun Max Shen"
      ],
      "abstract": "The pretrain-transfer paradigm, which underpins the success of large language models (LLMs), has demonstrated the immense power of creating foundation models that learn generalizable representations from vast datasets. However, extending this paradigm to Operations Research (OR) problems on graph structures remains challenging due to the fundamental conflict between the statistical flexibility of language and the strict combinatorial constraints of graphs. To bridge this gap, we introduce the Graph Foundation Model (GFM), the first framework capable of solving all distance-based optimization problems on graph structures. By introducing the LLM-like self-supervised pre-training paradigm on the paths generated from random walks in the graph, GFM is compelled to internalize the graph's complex topological and combinatorial rules, where the connectivity of the structure itself can be treated as the supervisory signal. Unlike existing neural methods that learn complex and task-specific solving policies, our approach leverages the pre-trained GFM as a foundational model of the graph's intrinsic structure, which in turn enables a simple generative heuristic to tackle a diverse range of optimization challenges effectively. Comprehensive experiments on networks ranging from 20 to 893 nodes demonstrate that GFM achieves competitive performance against specialized solvers across a variety of distinct optimization task classes, while maintaining significantly faster inference times. Our work establishes a new paradigm of adapting the pretrain-transfer framework to graph optimization, opening the door for applying foundation model innovations to OR.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Graph Foundation Model (GFM)ï¼Œè¿™æ˜¯é¦–ä¸ªèƒ½å¤Ÿè§£å†³å›¾ç»“æ„ä¸Šæ‰€æœ‰distance-based optimizationé—®é¢˜çš„æ¡†æ¶ã€‚GFMé€šè¿‡åœ¨random walksç”Ÿæˆçš„è·¯å¾„ä¸Šåº”ç”¨ç±»LLMçš„self-supervised pre-trainingèŒƒå¼ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå†…åŒ–å¤æ‚çš„topologicalå’Œcombinatorial rulesã€‚ä¸å­¦ä¹ ç‰¹å®šä»»åŠ¡æ±‚è§£ç­–ç•¥çš„ç°æœ‰ç¥ç»æ–¹æ³•ä¸åŒï¼Œè¯¥æ¨¡å‹å°†é¢„è®­ç»ƒçš„GFMä½œä¸ºå›¾å†…åœ¨ç»“æ„çš„foundational modelï¼Œä»è€Œæ”¯æŒä½¿ç”¨ç®€å•çš„generative heuristicé«˜æ•ˆè§£å†³å„ç±»ä¼˜åŒ–æŒ‘æˆ˜ã€‚åœ¨20è‡³893ä¸ªèŠ‚ç‚¹çš„ç½‘ç»œå®éªŒä¸­ï¼ŒGFMåœ¨å¤šç§optimization task classesä¸­è¡¨ç°å‡ºä¸ä¸“ä¸šæ±‚è§£å™¨ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶inference timesæ˜¾è‘—åŠ å¿«ã€‚è¯¥ç ”ç©¶å»ºç«‹äº†å°†pretrain-transfer frameworkåº”ç”¨äºå›¾ä¼˜åŒ–çš„æ–°èŒƒå¼ï¼Œä¸ºOperations Research (OR)é¢†åŸŸçš„åŸºç¡€æ¨¡å‹åˆ›æ–°å¼€è¾Ÿäº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24256v1",
      "published_date": "2025-09-29 04:05:48 UTC",
      "updated_date": "2025-09-29 04:05:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:11:00.698826+00:00"
    },
    {
      "arxiv_id": "2509.24250v2",
      "title": "Interactive Program Synthesis for Modeling Collaborative Physical Activities from Narrated Demonstrations",
      "title_zh": "åŸºäºè§£è¯´å¼æ¼”ç¤ºçš„åä½œå¼èº«ä½“æ´»åŠ¨å»ºæ¨¡äº¤äº’å¼ç¨‹åºåˆæˆ",
      "authors": [
        "Edward Kim",
        "Daniel He",
        "Jorge Chao",
        "Wiktor Rajca",
        "Mohammed Amin",
        "Nishant Malpani",
        "Ruta Desai",
        "Antti Oulasvirta",
        "Bjoern Hartmann",
        "Sanjit Seshia"
      ],
      "abstract": "Teaching systems physical tasks is a long standing goal in HCI, yet most prior work has focused on non collaborative physical activities. Collaborative tasks introduce added complexity, requiring systems to infer users assumptions about their teammates intent, which is an inherently ambiguous and dynamic process. This necessitates representations that are interpretable and correctable, enabling users to inspect and refine system behavior. We address this challenge by framing collaborative task learning as a program synthesis problem. Our system represents behavior as editable programs and uses narrated demonstrations, i.e. paired physical actions and natural language, as a unified modality for teaching, inspecting, and correcting system logic without requiring users to see or write code. The same modality is used for the system to communicate its learning to users. In a within subjects study, 20 users taught multiplayer soccer tactics to our system. 70 percent (14/20) of participants successfully refined learned programs to match their intent and 90 percent (18/20) found it easy to correct the programs. The study surfaced unique challenges in representing learning as programs and in enabling users to teach collaborative physical activities. We discuss these issues and outline mitigation strategies.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•é€šè¿‡å£è¿°æ¼”ç¤º(Narrated Demonstrations)å»ºæ¨¡åä½œæ€§ç‰©ç†æ´»åŠ¨ï¼Œæ—¨åœ¨è§£å†³åä½œä»»åŠ¡ä¸­å› é˜Ÿå‹æ„å›¾æ¨¡ç³Šå’ŒåŠ¨æ€å˜åŒ–å¸¦æ¥çš„å­¦ä¹ æŒ‘æˆ˜ã€‚ç ”ç©¶è€…å°†åä½œä»»åŠ¡å­¦ä¹ å»ºæ¨¡ä¸ºä¸€ä¸ªäº¤äº’å¼ç¨‹åºåˆæˆ(Interactive Program Synthesis)é—®é¢˜ï¼Œå°†ç³»ç»Ÿè¡Œä¸ºè¡¨ç¤ºä¸ºå¯ç¼–è¾‘çš„ç¨‹åºã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨ç»“åˆäº†ç‰©ç†åŠ¨ä½œä¸è‡ªç„¶è¯­è¨€çš„å£è¿°æ¼”ç¤ºä½œä¸ºç»Ÿä¸€æ¨¡æ€ï¼Œå…è®¸ç”¨æˆ·åœ¨æ— éœ€ç›´æ¥æ¥è§¦ä»£ç çš„æƒ…å†µä¸‹å®Œæˆæ•™å­¦ã€æ£€æŸ¥å’Œé€»è¾‘ä¿®æ­£ã€‚åœ¨é’ˆå¯¹å¤šäººè¶³çƒæˆ˜æœ¯çš„20äººç”¨æˆ·ç ”ç©¶ä¸­ï¼Œ70%çš„å‚ä¸è€…æˆåŠŸä¿®æ­£äº†ç¨‹åºä»¥åŒ¹é…å…¶æ„å›¾ï¼Œä¸”90%çš„å‚ä¸è€…è®¤ä¸ºè¯¥ä¿®æ­£è¿‡ç¨‹ååˆ†ä¾¿æ·ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†å°†å­¦ä¹ è¿‡ç¨‹ç¨‹åºåŒ–è¡¨ç¤ºçš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œå¹¶ä¸ºæœªæ¥æ•™æˆåä½œæ€§ç‰©ç†æ´»åŠ¨æä¾›äº†æ”¹è¿›ç­–ç•¥ã€‚",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24250v2",
      "published_date": "2025-09-29 03:44:05 UTC",
      "updated_date": "2025-12-14 20:58:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:10:49.761849+00:00"
    },
    {
      "arxiv_id": "2509.24248v2",
      "title": "SpecExit: Accelerating Large Reasoning Model via Speculative Exit",
      "title_zh": "SpecExitï¼šé€šè¿‡æ¨æµ‹å¼é€€å‡ºåŠ é€Ÿå¤§æ¨ç†æ¨¡å‹",
      "authors": [
        "Rubing Yang",
        "Huajun Bai",
        "Song Liu",
        "Guanghua Yu",
        "Runzhi Fan",
        "Yanbin Dang",
        "Jiejing Zhang",
        "Kai Liu",
        "Jianchen Zhu",
        "Peng Chen"
      ],
      "abstract": "Despite their strong performance on reasoning tasks, large reasoning models (LRMs) often suffer from overthinking, producing unnecessarily long outputs and incurring high end-to-end latency, a significant limitation to their real-world deployment. To address overthinking, early-exit mechanisms have been proposed to terminate reasoning before typical completion, showing that this approach can effectively shorten generation length with minimal impact on accuracy. However, their reliance on probing mechanisms introduces a detection overhead that limits their end-to-end latency gains and compromises their generalizability across diverse problems. Inspired by the use of hidden states in speculative decoding, we propose SpecExit, a novel framework that predicts both future tokens and an early-exit signal directly from a lightweight draft model without probing overhead. Our method offers significant improvements, reducing average generation length by 66\\% and achieving a 2.5x speedup in end-to-end latency compared to the speculative decoding baseline, without compromising accuracy. Our method leverages the inherent signals from hidden states to provide effective early-exit signals, suggesting broader use of hidden states for efficient reasoning. Our code is available at https://github.com/Tencent/AngelSlim.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLarge Reasoning Models, LRMsï¼‰åœ¨æ¨ç†ä»»åŠ¡ä¸­å› è¿‡åº¦æ€è€ƒï¼ˆoverthinkingï¼‰å¯¼è‡´çš„è¾“å‡ºå†—é•¿å’Œé«˜å»¶è¿Ÿé—®é¢˜ï¼Œæå‡ºäº†åä¸º SpecExit çš„åŠ é€Ÿæ¡†æ¶ã€‚ç°æœ‰çš„æ—©æœŸé€€å‡ºï¼ˆearly-exitï¼‰æœºåˆ¶ç”±äºä¾èµ–æ¢æµ‹æœºåˆ¶ï¼ˆprobing mechanismsï¼‰ä¼šäº§ç”Ÿé¢å¤–çš„æ£€æµ‹å¼€é”€ï¼Œä»è€Œé™åˆ¶äº†ç«¯åˆ°ç«¯å»¶è¿Ÿçš„ä¼˜åŒ–ä¸”å½±å“äº†é€šç”¨æ€§ã€‚SpecExit å€Ÿé‰´äº†æŠ•æœºè§£ç ï¼ˆspeculative decodingï¼‰ä¸­éšè—çŠ¶æ€ï¼ˆhidden statesï¼‰çš„ä½¿ç”¨ï¼Œé€šè¿‡è½»é‡çº§è‰ç¨¿æ¨¡å‹ï¼ˆdraft modelï¼‰ç›´æ¥é¢„æµ‹æœªæ¥ Token å’Œæ—©æœŸé€€å‡ºä¿¡å·ï¼Œæ¶ˆé™¤äº†æ¢æµ‹å¼€é”€ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSpecExit åœ¨ä¸æŸå¤±å‡†ç¡®ç‡çš„å‰æä¸‹ï¼Œå°†å¹³å‡ç”Ÿæˆé•¿åº¦ç¼©çŸ­äº† 66%ï¼Œå¹¶æ¯”æŠ•æœºè§£ç åŸºçº¿å®ç°äº† 2.5 å€çš„ç«¯åˆ°ç«¯åŠ é€Ÿã€‚è¯¥æ–¹æ³•æœ‰æ•ˆåˆ©ç”¨äº†éšè—çŠ¶æ€ä¸­çš„å†…åœ¨ä¿¡å·ï¼Œä¸º LRMs çš„é«˜æ•ˆæ¨ç†å’Œå®é™…éƒ¨ç½²æä¾›äº†é‡è¦æ”¯æŒã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24248v2",
      "published_date": "2025-09-29 03:39:32 UTC",
      "updated_date": "2025-10-21 07:44:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:11:04.584923+00:00"
    },
    {
      "arxiv_id": "2509.24245v1",
      "title": "Prompt and Parameter Co-Optimization for Large Language Models",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹çš„æç¤ºä¸å‚æ•°ååŒä¼˜åŒ–",
      "authors": [
        "Xiaohe Bo",
        "Rui Li",
        "Zexu Sun",
        "Quanyu Dai",
        "Zeyu Zhang",
        "Zihang Tian",
        "Xu Chen",
        "Zhenhua Dong"
      ],
      "abstract": "Prompt optimization and fine-tuning are two major approaches to improve the performance of Large Language Models (LLMs). They enhance the capabilities of LLMs from complementary perspectives: the former through explicit natural language, and the latter through implicit parameter updates. However, prior work has typically studied them in isolation, leaving their synergistic potential largely underexplored. To bridge this gap, in this paper, we introduce MetaTuner, a novel framework that jointly integrates prompt optimization and fine-tuning for LLM training. Specifically, we introduce two neural networks to generate prompts and parameters, respectively, while allowing them to share a common bottom encoding layer to enable knowledge sharing. By the guidance of the final supervised signals, our framework is optimized to discover the optimal combinations between the prompts and parameters. Given that prompt learning involves discrete optimization while fine-tuning operates in a continuous parameter space, we design a supervised regularization loss to train our framework effectively. Extensive experiments across diverse benchmarks show that our method consistently outperforms the baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„æ€§èƒ½æå‡ï¼Œæå‡ºäº† MetaTuner æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ Prompt optimization (æç¤ºä¼˜åŒ–) å’Œ Fine-tuning (å¾®è°ƒ) é•¿æœŸä»¥æ¥è¢«å­¤ç«‹ç ”ç©¶çš„é—®é¢˜ã€‚MetaTuner é€šè¿‡å¼•å…¥ä¸¤ä¸ªåˆ†åˆ«è´Ÿè´£ç”Ÿæˆæç¤ºå’Œå‚æ•°çš„ç¥ç»ç½‘ç»œï¼Œå¹¶åˆ©ç”¨å…±äº«åº•å±‚ç¼–ç å±‚å®ç°ä¸¤è€…çš„çŸ¥è¯†å…±äº«ä¸ååŒã€‚ä¸ºäº†åº”å¯¹æç¤ºå­¦ä¹ çš„ç¦»æ•£ä¼˜åŒ–ä¸å¾®è°ƒçš„è¿ç»­å‚æ•°ç©ºé—´å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨äº†ä¸“é—¨è®¾è®¡çš„ç›‘ç£æ­£åˆ™åŒ–æŸå¤± (Supervised regularization loss) è¿›è¡Œè”åˆè®­ç»ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMetaTuner åœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­ä¸€è‡´ä¼˜äºç°æœ‰åŸºå‡†æ–¹æ³•ï¼Œè¯æ˜äº†æ˜¾å¼è‡ªç„¶è¯­è¨€æŒ‡ä»¤ä¸éšå¼å‚æ•°æ›´æ–°ç»“åˆçš„ä¼˜è¶Šæ€§ã€‚è¯¥æ¡†æ¶ä¸º LLMs çš„æ€§èƒ½ä¼˜åŒ–æä¾›äº†æ–°çš„è§†è§’ï¼Œæœ‰æ•ˆåœ°æŒ–æ˜äº†æç¤ºä¸å‚æ•°ä¹‹é—´çš„ååŒæ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "19 pages, 10 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.24245v1",
      "published_date": "2025-09-29 03:38:25 UTC",
      "updated_date": "2025-09-29 03:38:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:10:55.294680+00:00"
    },
    {
      "arxiv_id": "2509.24244v3",
      "title": "Model Merging Scaling Laws in Large Language Models",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ä¸­çš„æ¨¡å‹åˆå¹¶ç¼©æ”¾æ³•åˆ™",
      "authors": [
        "Yuanyi Wang",
        "Yanggan Gu",
        "Yiming Zhang",
        "Qi Zhou",
        "Zhaoyi Yan",
        "Congkai Xie",
        "Xinyao Wang",
        "Jianbo Yuan",
        "Hongxia Yang"
      ],
      "abstract": "We study empirical scaling laws for language model merging measured by cross-entropy. Despite its wide practical use, merging lacks a quantitative rule that predicts returns as we add experts or scale the model size. We identify a compact power law that links model size and expert number: the size-dependent floor decreases with model capacity, while the merging tail exhibits clear diminishing returns in the number of experts. The law holds in-domain and cross-domain, tightly fits measured curves across diverse architectures and methods (Average, TA, TIES, DARE), and explains two robust regularities: most gains arrive early, and variability shrinks as more experts are included. Building on this, we present a simple theory that explains why gains fall roughly as 1/k and links the floor and tail to properties of the base model and the diversity across domains. This law enables predictive planning: estimate how many experts are needed to reach a target loss, decide when to stop adding experts, and trade off scaling the base model versus adding experts under a fixed budget--turning merging from heuristic practice into a computationally efficient, planable alternative to multitask training. This suggests a scaling principle for distributed generative AI: predictable gains can be achieved by composing specialists, offering a complementary path toward AGI-level systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(Large Language Models)æ¨¡å‹åˆå¹¶ä¸­çš„ç»éªŒæ€§ç¼©æ”¾æ³•åˆ™(Scaling Laws)ï¼Œå¹¶ä½¿ç”¨äº¤å‰ç†µ(cross-entropy)è¿›è¡Œè¡¡é‡ã€‚ä½œè€…å‘ç°äº†ä¸€ä¸ªå°†æ¨¡å‹è§„æ¨¡ä¸ä¸“å®¶æ•°é‡(expert number)è”ç³»èµ·æ¥çš„ç´§å‡‘å¹‚å¾‹(power law)ï¼Œå…¶ä¸­åŸºç¡€æ¨¡å‹è§„æ¨¡å†³å®šäº†æ€§èƒ½ä¸‹é™ï¼Œè€Œåˆå¹¶è¿‡ç¨‹åœ¨ä¸“å®¶æ•°é‡å¢åŠ æ—¶è¡¨ç°å‡ºæ˜æ˜¾çš„è¾¹é™…æ”¶ç›Šé€’å‡ã€‚è¯¥æ³•åˆ™åœ¨ä¸åŒé¢†åŸŸã€æ¶æ„ä»¥åŠAverageã€TAã€TIESã€DAREç­‰åˆå¹¶æ–¹æ³•ä¸­å‡è¡¨ç°å‡ºé«˜åº¦ä¸€è‡´æ€§ï¼Œå¹¶èƒ½ä»ç†è®ºä¸Šè§£é‡Šæ”¶ç›Šéšä¸“å®¶æ•°é‡å¢åŠ å‘ˆ1/kä¸‹é™çš„è§„å¾‹ã€‚é€šè¿‡è¿™ä¸€æ³•åˆ™ï¼Œç ”ç©¶è€…å¯ä»¥è¿›è¡Œé¢„æµ‹æ€§è§„åˆ’ï¼Œå¦‚ä¼°ç®—è¾¾åˆ°ç›®æ ‡æŸå¤±(loss)æ‰€éœ€çš„ä¸“å®¶æ•°é‡ï¼Œæˆ–åœ¨å›ºå®šé¢„ç®—ä¸‹æƒè¡¡æ¨¡å‹æ‰©å±•ä¸ä¸“å®¶å¢åŠ çš„æ¯”ä¾‹ã€‚è¿™ä¸ºåˆ†å¸ƒå¼ç”Ÿæˆå¼äººå·¥æ™ºèƒ½(distributed generative AI)æä¾›äº†ä¸€ç§å¯é¢„æµ‹çš„ç¼©æ”¾åŸåˆ™ï¼Œä½¿æ¨¡å‹åˆå¹¶æˆä¸ºå¤šä»»åŠ¡è®­ç»ƒ(multitask training)ä¹‹å¤–ä¸€ç§æ›´å…·è®¡ç®—æ•ˆç‡ä¸”å¯è®¡åˆ’çš„æ›¿ä»£è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "30 pages",
      "pdf_url": "https://arxiv.org/pdf/2509.24244v3",
      "published_date": "2025-09-29 03:36:55 UTC",
      "updated_date": "2025-10-01 05:31:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:10:56.059971+00:00"
    },
    {
      "arxiv_id": "2509.24243v2",
      "title": "SafeFlowMatcher: Safe and Fast Planning using Flow Matching with Control Barrier Functions",
      "title_zh": "SafeFlowMatcherï¼šåŸºäºæµåŒ¹é…ä¸æ§åˆ¶å±éšœå‡½æ•°çš„å®‰å…¨å¿«é€Ÿè§„åˆ’",
      "authors": [
        "Jeongyong Yang",
        "Seunghwan Jang",
        "SooJean Han"
      ],
      "abstract": "Generative planners based on flow matching (FM) can produce high-quality paths in one or a few ODE steps, but their sampling dynamics offer no formal safety guarantees and can yield incomplete paths near constraints. We present SafeFlowMatcher, a planning framework that couples FM with control barrier functions (CBFs) to achieve both real-time efficiency and certified safety. SafeFlowMatcher uses a two-phase prediction-correction (PC) integrator: (i) a prediction phase integrates the learned FM once (or a few steps) to obtain a candidate path without intervention; (ii) a correction phase refines this path with a vanishing time-scaled vector field and a CBF-based quadratic program that minimally perturbs the vector field. We prove a barrier certificate for the resulting flow system, establishing forward invariance of a robust safe set and finite-time convergence to the safe set. By enforcing safety only on the executed path (rather than on all intermediate latent paths), SafeFlowMatcher avoids distributional drift and mitigates local trap problems. Across maze navigation and locomotion benchmarks, SafeFlowMatcher attains faster, smoother, and safer paths than diffusion- and FM-based baselines. Extensive ablations corroborate the contributions of the PC integrator and the barrier certificate.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SafeFlowMatcherï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆäº†æµåŒ¹é…(Flow Matching)ä¸æ§åˆ¶å±éšœå‡½æ•°(Control Barrier Functions)çš„è§„åˆ’æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç”Ÿæˆå¼è§„åˆ’å™¨åœ¨å®‰å…¨æ€§ä¿è¯å’Œçº¦æŸå¤„ç†æ–¹é¢çš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶æ ¸å¿ƒé‡‡ç”¨äº†ä¸€ç§åŒé˜¶æ®µçš„é¢„æµ‹-æ ¡æ­£(Prediction-Correction)ç§¯åˆ†å™¨ï¼Œåœ¨é¢„æµ‹é˜¶æ®µç”Ÿæˆåˆæ­¥è·¯å¾„åï¼Œé€šè¿‡åŸºäºCBFçš„äºŒæ¬¡è§„åˆ’(Quadratic Program)è¿›è¡Œå¾®å°æ‰°åŠ¨ä¿®æ­£ä»¥ç¡®ä¿å®‰å…¨ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡å±éšœè¯ä¹¦(Barrier Certificate)ä»ç†è®ºä¸Šè¯æ˜äº†è¯¥ç³»ç»Ÿå…·æœ‰é²æ£’å®‰å…¨é›†çš„å‰å‘ä¸å˜æ€§å’Œæœ‰é™æ—¶é—´æ”¶æ•›æ€§ã€‚SafeFlowMatcherä»…åœ¨å®é™…æ‰§è¡Œè·¯å¾„ä¸Šå®æ–½å®‰å…¨çº¦æŸï¼Œæœ‰æ•ˆç¼“è§£äº†åˆ†å¸ƒåç§»(Distributional Drift)å’Œå±€éƒ¨é™·é˜±é—®é¢˜ã€‚åœ¨è¿·å®«å¯¼èˆªå’Œè¿åŠ¨æ§åˆ¶çš„å®éªŒä¸­ï¼ŒSafeFlowMatcherå±•ç°å‡ºæ¯”ä¼ ç»Ÿæ‰©æ•£æ¨¡å‹æˆ–æµåŒ¹é…åŸºçº¿æ›´å¿«é€Ÿã€æ›´å¹³æ»‘ä¸”å…·å¤‡è®¤è¯å®‰å…¨æ€§çš„è·¯å¾„è§„åˆ’èƒ½åŠ›ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "10 pages, 7 figures, 4 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.24243v2",
      "published_date": "2025-09-29 03:33:33 UTC",
      "updated_date": "2025-10-06 04:13:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:11:13.086123+00:00"
    },
    {
      "arxiv_id": "2509.24239v3",
      "title": "ChessArena: A Chess Testbed for Evaluating Strategic Reasoning Capabilities of Large Language Models",
      "title_zh": "ChessArenaï¼šç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹æˆ˜ç•¥æ¨ç†èƒ½åŠ›çš„å›½é™…è±¡æ£‹æµ‹è¯„åŸºå‡†",
      "authors": [
        "Jincheng Liu",
        "Sijun He",
        "Jingjing Wu",
        "Xiangsen Wang",
        "Yang Chen",
        "Zhaoqi Kuang",
        "Siqi Bao",
        "Yuan Yao"
      ],
      "abstract": "Recent large language models (LLMs) have shown strong reasoning capabilities. However, a critical question remains: do these models possess genuine reasoning skills particularly complex strategic reasoning or are they primarily excelling at sophisticated pattern recognition within their training data? To address this question, this paper presents a chess testbed, ChessArena, to evaluate the strategic reasoning capabilities of LLMs. Chess requires complex strategic reasoning capabilities including long-term planning, strict rule comprehension, and multi-turn conversation memorization. Specifically, ChessArena is a competitive framework where LLMs play against each other, under four different play modes. The testbed is equipped with a ranking algorithm and a leaderboard. The testbed can also evaluate fine-grained capabilities including basic understanding, move selection, and puzzle solving. Over 13 LLMs with different modes are evaluated in ChessArena, playing over 800 games. The results reveal significant shortcomings in current LLMs: no model can beat Maia-1100 (a chess engine at human amateur level), while some even failed to defeat a random player that selects moves arbitrarily. We also present a strong baseline to the testbed: our fine-tuned Qwen3-8B substantially improved performance, approaching much larger state-of-the-art reasoning models.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ˜¯å¦å…·å¤‡çœŸå®çš„æˆ˜ç•¥æ¨ç†èƒ½åŠ›è¿˜æ˜¯ä»…ä¾èµ–æ¨¡å¼è¯†åˆ«è¿™ä¸€æ ¸å¿ƒé—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªåä¸ºChessArenaçš„å›½é™…è±¡æ£‹æµ‹è¯„åŸºå‡†ã€‚ChessArenaæ„å»ºäº†ä¸€ä¸ªå¤šæ™ºèƒ½ä½“ç«æŠ€æ¡†æ¶ï¼Œé€šè¿‡å››ç§ä¸åŒçš„å¯¹å¼ˆæ¨¡å¼è®©æ¨¡å‹ç›¸äº’å¯¹å†³ï¼Œå¹¶é…å¤‡äº†æ’åç®—æ³•å’Œæ’è¡Œæ¦œã€‚è¯¥åŸºå‡†ä¸ä»…èƒ½è¯„ä¼°é•¿çŸ­æœŸè§„åˆ’ã€è§„åˆ™ç†è§£åŠå¤šè½®å¯¹è¯è®°å¿†ï¼Œè¿˜èƒ½ç»†åŒ–åˆ†ææ¨¡å‹åœ¨åŸºç¡€ç†è§£ã€èµ°å­é€‰æ‹©ï¼ˆmove selectionï¼‰å’Œæ®‹å±€ç ´è§£ï¼ˆpuzzle solvingï¼‰æ–¹é¢çš„è¡¨ç°ã€‚ç ”ç©¶å¯¹è¶…è¿‡13ç§æ¨¡å‹è¿›è¡Œäº†800å¤šåœºå¯¹å±€æµ‹è¯•ï¼Œç»“æœæ˜¾ç¤ºå½“å‰LLMsåœ¨å¤æ‚æˆ˜ç•¥æ¨ç†æ–¹é¢ä»å­˜åœ¨æ˜¾è‘—çŸ­æ¿ï¼Œç”šè‡³æ²¡æœ‰ä»»ä½•æ¨¡å‹èƒ½å‡»è´¥äººç±»ä¸šä½™æ°´å¹³çš„å¼•æ“Maia-1100ã€‚å®éªŒå‘ç°éƒ¨åˆ†æ¨¡å‹ç”šè‡³æ— æ³•æˆ˜èƒœéšæœºèµ°å­çš„éšæœºç©å®¶ï¼Œè¿›ä¸€æ­¥æ­ç¤ºäº†ç°æœ‰æ¨¡å‹åœ¨ä¸¥è°¨è§„åˆ™çº¦æŸä¸‹çš„å±€é™æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶æå‡ºçš„å¾®è°ƒåŸºå‡†æ¨¡å‹Qwen3-8Bæ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œè¡¨ç°æ¥è¿‘æ›´å¤§å‹çš„æœ€å…ˆè¿›æ¨ç†æ¨¡å‹ï¼Œä¸ºæå‡LLMsçš„æˆ˜ç•¥æ¨ç†èƒ½åŠ›æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24239v3",
      "published_date": "2025-09-29 03:24:48 UTC",
      "updated_date": "2025-12-02 08:42:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:11:14.890238+00:00"
    },
    {
      "arxiv_id": "2509.24238v1",
      "title": "Learning to Ponder: Adaptive Reasoning in Latent Space",
      "title_zh": "å­¦ä¹ æ²‰æ€ï¼šæ½œç©ºé—´ä¸­çš„è‡ªé€‚åº”æ¨ç†",
      "authors": [
        "Yixin He",
        "Lumingyuan Tang"
      ],
      "abstract": "Test-time compute has emerged as a key paradigm for enhancing LLM reasoning, yet prevailing approaches like Best-of-N and majority voting apply uniform depth across inputs, wasting computation on simple queries while potentially under-thinking complex ones. We present FR-Ponder, a single-graph, backbone-training-free framework that allocates instance-adaptive reasoning compute via latent steering. A less than 1M-param controller observes hidden states and decides to halt or apply a small ponder step by adding a pre-computed steering vector to frozen representations. Our method extracts the latent steering vector associated with deeper reasoning outputs and direct IO from LLM and re-applies it through a tunable scaling factor, allowing the model to adapt its reasoning depth to the complexity of each input. To balance performance and computational cost, we employ Group Relative Policy Optimization (GRPO) as a reward signal to adaptively regulate reasoning depth, achieving task accuracy while mitigating overreasoning. Through curriculum learning and careful reward engineering, FR-Ponder learns calibrated compute allocation correlated with problem difficulty. On GSM8K and MATH500, FR-Ponder improves the compute-accuracy frontier, delivering lower FLOPs with better matched accuracy and comparing favorably to early-exit baselines, without modifying backbone weights. Analyses visualize interpretable steering directions and show learned compute allocation correlates with problem difficulty.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FR-Ponderï¼Œä¸€ä¸ªæ— éœ€è®­ç»ƒéª¨å¹²æ¨¡å‹(Backbone-training-free)çš„å•å›¾æ¡†æ¶ï¼Œé€šè¿‡æ½œç©ºé—´å¼•å¯¼(Latent steering)å®ç°äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)æ¨ç†è®¡ç®—çš„å®ä¾‹è‡ªé€‚åº”åˆ†é…ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ä¸ªå‚æ•°é‡å°äº1Mçš„è½»é‡çº§æ§åˆ¶å™¨ï¼Œé€šè¿‡è§‚å¯Ÿéšè—çŠ¶æ€æ¥å†³å®šåœæ­¢è®¡ç®—ï¼Œæˆ–é€šè¿‡å‘å†»ç»“è¡¨ç¤ºä¸­æ·»åŠ é¢„è®¡ç®—çš„å¼•å¯¼å‘é‡(Steering vector)æ¥æ‰§è¡Œé¢å¤–çš„â€œæ€è€ƒâ€(Ponder)æ­¥éª¤ã€‚ç ”ç©¶åˆ©ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–(GRPO)ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œå¹¶ç»“åˆè¯¾ç¨‹å­¦ä¹ (Curriculum learning)æ¥è‡ªåŠ¨è°ƒèŠ‚æ¨ç†æ·±åº¦ï¼Œä»è€Œåœ¨ä¿è¯ä»»åŠ¡å‡†ç¡®æ€§çš„åŒæ—¶ç¼“è§£è¿‡åº¦æ¨ç†(Overreasoning)é—®é¢˜ã€‚åœ¨GSM8Kå’ŒMATH500åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒFR-Ponderåœ¨ä¸ä¿®æ”¹éª¨å¹²æƒé‡çš„å‰æä¸‹æ˜¾è‘—æå‡äº†è®¡ç®—-å‡†ç¡®ç‡å‰æ²¿(Compute-accuracy frontier)ï¼Œä»¥æ›´ä½çš„æµ®ç‚¹è¿ç®—é‡(FLOPs)å®ç°äº†æ›´ä¼˜çš„æ¨ç†æ€§èƒ½ã€‚åˆ†æç»“æœè¿›ä¸€æ­¥éªŒè¯äº†è¯¥æ–¹æ³•ç”Ÿæˆçš„å¼•å¯¼æ–¹å‘å…·æœ‰å¯è§£é‡Šæ€§ï¼Œä¸”å…¶è®¡ç®—èµ„æºåˆ†é…ä¸é—®é¢˜çš„å®é™…éš¾åº¦é«˜åº¦ç›¸å…³ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24238v1",
      "published_date": "2025-09-29 03:21:42 UTC",
      "updated_date": "2025-09-29 03:21:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:11:18.684050+00:00"
    },
    {
      "arxiv_id": "2509.24230v1",
      "title": "ELHPlan: Efficient Long-Horizon Task Planning for Multi-Agent Collaboration",
      "title_zh": "ELHPlanï¼šé¢å‘å¤šæ™ºèƒ½ä½“åä½œçš„é«˜æ•ˆé•¿ç¨‹ä»»åŠ¡è§„åˆ’",
      "authors": [
        "Shaobin Ling",
        "Yun Wang",
        "Chenyou Fan",
        "Tin Lun Lam",
        "Junjie Hu"
      ],
      "abstract": "Large Language Models (LLMs) enable intelligent multi-robot collaboration but face fundamental trade-offs: declarative methods lack adaptability in dynamic environments, while iterative methods incur prohibitive computational costs that scale poorly with team size and task complexity. In this paper, we propose ELHPlan, a novel framework that introduces Action Chains--sequences of actions explicitly bound to sub-goal intentions--as the fundamental planning primitive. ELHPlan operates via a cyclical process: 1) constructing intention-bound action sequences, 2) proactively validating for conflicts and feasibility, 3) refining issues through targeted mechanisms, and 4) executing validated actions. This design balances adaptability and efficiency by providing sufficient planning horizons while avoiding expensive full re-planning. We further propose comprehensive efficiency metrics, including token consumption and planning time, to more holistically evaluate multi-agent collaboration. Our experiments on benchmark TDW-MAT and C-WAH demonstrate that ELHPlan achieves comparable task success rates while consuming only 24% of the tokens required by state-of-the-art methods. Our research establishes a new efficiency-effectiveness frontier for LLM-based multi-agent planning systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ELHPlanï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å¤šæœºå™¨äººåä½œä¸­é¢ä¸´çš„é€‚åº”æ€§ä¸è®¡ç®—æˆæœ¬æƒè¡¡é—®é¢˜çš„æ¡†æ¶ã€‚ELHPlan å¼•å…¥äº†åŠ¨ä½œé“¾ (Action Chains) ä½œä¸ºåŸºæœ¬çš„è§„åˆ’åŸè¯­ï¼Œå°†åŠ¨ä½œåºåˆ—ä¸å­ç›®æ ‡æ„å›¾ (sub-goal intentions) æ˜¾å¼ç»‘å®šï¼Œä»è€Œåœ¨åŠ¨æ€ç¯å¢ƒä¸­æä¾›å¹³è¡¡ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸€ä¸ªå¾ªç¯è¿‡ç¨‹è¿è¡Œï¼ŒåŒ…æ‹¬æ„å»ºæ„å›¾ç»‘å®šçš„åŠ¨ä½œåºåˆ—ã€ä¸»åŠ¨éªŒè¯å†²çªä¸å¯è¡Œæ€§ã€é’ˆå¯¹æ€§æœºåˆ¶ä¿®å¤ä»¥åŠæ‰§è¡Œå·²éªŒè¯åŠ¨ä½œã€‚è¿™ç§è®¾è®¡åœ¨ç¡®ä¿è¶³å¤Ÿè§„åˆ’è§†ç•Œ (planning horizons) çš„åŒæ—¶é¿å…äº†æ˜‚è´µçš„å…¨å±€é‡æ–°è§„åˆ’ï¼Œæ˜¾è‘—æå‡äº†è§„åˆ’æ•ˆç‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…æå‡ºäº†åŒ…æ‹¬ Token æ¶ˆè€—å’Œè§„åˆ’æ—¶é—´åœ¨å†…çš„ç»¼åˆæ•ˆç‡æŒ‡æ ‡ï¼Œç”¨äºæ›´å…¨é¢åœ°è¯„ä¼°å¤šæ™ºèƒ½ä½“åä½œæ€§èƒ½ã€‚åœ¨ TDW-MAT å’Œ C-WAH åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒELHPlan åœ¨ä¿æŒä»»åŠ¡æˆåŠŸç‡çš„åŒæ—¶ï¼ŒToken æ¶ˆè€—ä»…ä¸ºå½“å‰æœ€å…ˆè¿›æ–¹æ³•çš„ 24%ã€‚è¯¥ç ”ç©¶ä¸ºåŸºäº LLM çš„å¤šæ™ºèƒ½ä½“è§„åˆ’ç³»ç»Ÿå»ºç«‹äº†ä¸€ä¸ªå…¼é¡¾æ•ˆç‡ä¸æœ‰æ•ˆæ€§çš„æ–°åŸºå‡†ã€‚",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24230v1",
      "published_date": "2025-09-29 03:15:56 UTC",
      "updated_date": "2025-09-29 03:15:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:11:21.685306+00:00"
    },
    {
      "arxiv_id": "2509.25279v2",
      "title": "RL in the Wild: Characterizing RLVR Training in LLM Deployment",
      "title_zh": "RL in the Wildï¼šå¤§è¯­è¨€æ¨¡å‹éƒ¨ç½²ä¸­ RLVR è®­ç»ƒçš„ç‰¹æ€§åˆ†æ",
      "authors": [
        "Jiecheng Zhou",
        "Qinghao Hu",
        "Yuyang Jin",
        "Zerui Wang",
        "Peng Sun",
        "Yuzhe Gu",
        "Wenwei Zhang",
        "Mingshu Zhai",
        "Xingcheng Zhang",
        "Weiming Zhang"
      ],
      "abstract": "Large Language Models (LLMs) are now widely used across many domains. With their rapid development, Reinforcement Learning with Verifiable Rewards (RLVR) has surged in recent months to enhance their reasoning and understanding abilities. However, its complex data flows and diverse tasks pose substantial challenges to RL training systems, and there is limited understanding of RLVR from a system perspective. To thoroughly understand the system challenges introduced by RLVR, we present a characterization study of RLVR tasks in our LLM deployment. Specifically, we investigate the distribution and variation trends of workloads across different RL tasks across training steps. We identify issues such as GPU idling caused by skewed sequence length distribution, inefficient parallel strategies in dynamically varying workloads, inefficient data management mechanisms, and load imbalance. We describe our observations and call for further investigation into the remaining open challenges. Furthermore, we propose PolyTrace benchmark suite to conduct evaluation with realistic workloads, and a practical use case validates that PolyTrace benchmark suite exhibits 94.7% accuracy.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹ Reinforcement Learning with Verifiable Rewards (RLVR) åœ¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) éƒ¨ç½²ä¸­æ—¥ç›Šå¢é•¿çš„é‡è¦æ€§ï¼Œä»ç³»ç»Ÿè§’åº¦å¯¹ RLVR è®­ç»ƒä»»åŠ¡è¿›è¡Œäº†æ·±å…¥çš„è¡¨å¾ç ”ç©¶ã€‚ç ”ç©¶æŒ‡å‡ºï¼ŒRLVR å¤æ‚çš„æµè½¬æ•°æ®å’Œå¤šæ ·åŒ–çš„ä»»åŠ¡ç»™è®­ç»ƒç³»ç»Ÿå¸¦æ¥äº†å·¨å¤§æŒ‘æˆ˜ï¼Œä½†ç›®å‰å­¦æœ¯ç•Œå¯¹å…¶ç³»ç»Ÿå±‚é¢çš„ç‰¹æ€§ç†è§£ä»ç„¶æœ‰é™ã€‚é€šè¿‡åˆ†æä¸åŒè®­ç»ƒæ­¥éª¤ä¸­å·¥ä½œè´Ÿè½½çš„åˆ†å¸ƒå’Œå˜åŒ–è¶‹åŠ¿ï¼Œä½œè€…è¯†åˆ«å‡ºäº†ç”±åºåˆ—é•¿åº¦åˆ†å¸ƒä¸å‡å¯¼è‡´çš„ GPU ç©ºé—²ã€åŠ¨æ€è´Ÿè½½ä¸‹çš„å¹¶è¡Œç­–ç•¥ä½æ•ˆã€æ•°æ®ç®¡ç†æœºåˆ¶ä¸å®Œå–„ä»¥åŠè´Ÿè½½ä¸å‡è¡¡ç­‰æ ¸å¿ƒç³»ç»Ÿç“¶é¢ˆã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œç ”ç©¶æå‡ºäº† PolyTrace åŸºå‡†æµ‹è¯•å¥—ä»¶ï¼Œæ—¨åœ¨åˆ©ç”¨çœŸå®çš„å·¥ä½œè´Ÿè½½è¿›è¡Œæ€§èƒ½è¯„ä¼°ã€‚å®é™…åº”ç”¨æ¡ˆä¾‹éªŒè¯äº† PolyTrace åœ¨æ¨¡æ‹ŸçœŸå®è´Ÿè½½æ–¹é¢å…·æœ‰ 94.7% çš„å‡†ç¡®ç‡ï¼Œä¸ºæœªæ¥å¼€å‘é«˜æ•ˆã€ç¨³å®šçš„ RLVR è®­ç»ƒç³»ç»Ÿä»¥åŠè§£å†³ç›¸å…³å¼€æ”¾æ€§æŒ‘æˆ˜å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.DC",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "20 pages, 28 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.25279v2",
      "published_date": "2025-09-29 03:09:27 UTC",
      "updated_date": "2025-10-13 05:01:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:11:24.987523+00:00"
    },
    {
      "arxiv_id": "2509.24222v1",
      "title": "Uni-NTFM: A Unified Foundation Model for EEG Signal Representation Learning",
      "title_zh": "Uni-NTFMï¼šç”¨äºè„‘ç”µä¿¡å·è¡¨å¾å­¦ä¹ çš„ç»Ÿä¸€åŸºç¡€æ¨¡å‹",
      "authors": [
        "Zhisheng Chen",
        "Yingwei Zhang",
        "Qizhen Lan",
        "Tianyu Liu",
        "Huacan Wang",
        "Yi Ding",
        "Ziyu Jia",
        "Ronghao Chen",
        "Kun Wang",
        "Xinliang Zhou"
      ],
      "abstract": "Foundation models pretrained on various and unlabeled data have demonstrated significant success in natural language and vision, but their application to electroencephalography (EEG) remains challenged due to the signal's unique properties. Existing brain foundation models that inherit architectures designed for text or images lead to three limitations in pre-training: 1) conflating time-domain waveform patterns with frequency-domain rhythmic features in a single processing stream, 2) ignoring the critical spatial topology of electrodes with different standards, and 3) reliance on the inflexible, dense network to process functionally distinct EEG patterns. To address these challenges, we introduce the Unified Neural Topological Foundation Model (Uni-NTFM), which is designed based on neuroscience principles to produce universal and interpretable representations. Uni-NTFM integrates three core innovations: 1) a decoupled architecture parallelly encodes time, frequency, and raw signal representations before performing cross-domain feature integration; 2) a topological embedding mechanism to unify electrodes from different international standards and generate structured input sequences for brain regions; and 3) a Mixture-of-Experts neural Transformer that efficiently scales model capacity by routing signal patterns to specialized subnetworks. The largest model, Uni-NTFM$_{large}$, has a record-breaking 1.9B parameters and was pretrained on over 28,000 hours of diverse EEG data via a dual-domain masked reconstruction objective. Uni-NTFM significantly outperforms existing task-specific methods and foundation models across nine distinct downstream tasks under both linear probing and fine-tuning settings, demonstrating a superior ability to learn universal representations of brain activity.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Uni-NTFMï¼Œè¿™æ˜¯ä¸€ç§ç»Ÿä¸€çš„ç¥ç»æ‹“æ‰‘åŸºç¡€æ¨¡å‹(Unified Neural Topological Foundation Model)ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è„‘ç”µå›¾(EEG)åŸºç¡€æ¨¡å‹åœ¨å¤„ç†ä¿¡å·æ—¶å¿½è§†æ—¶é¢‘åŸŸè§£è€¦ã€ç”µæç©ºé—´æ‹“æ‰‘å’Œç½‘ç»œçµæ´»æ€§ä¸è¶³ç­‰æŒ‘æˆ˜ã€‚è¯¥æ¨¡å‹é‡‡ç”¨è§£è€¦æ¶æ„å¹¶è¡Œç¼–ç æ—¶é—´ã€é¢‘ç‡å’ŒåŸå§‹ä¿¡å·è¡¨å¾ï¼Œå¹¶ç»“åˆæ‹“æ‰‘åµŒå…¥æœºåˆ¶(topological embedding mechanism)ä»¥ç»Ÿä¸€ä¸åŒå›½é™…æ ‡å‡†çš„ç”µæå¹¶ç”Ÿæˆè„‘åŒºç»“æ„åŒ–åºåˆ—ã€‚é€šè¿‡å¼•å…¥æ··åˆä¸“å®¶(Mixture-of-Experts)ç¥ç»Transformerï¼Œè¯¥æ¨¡å‹å®ç°äº†é«˜æ•ˆçš„å®¹é‡æ‰©å±•ï¼Œå…¶ä¸­æœ€å¤§çš„Uni-NTFM$_{large}$æ‹¥æœ‰åˆ›çºªå½•çš„19äº¿å‚æ•°ã€‚è¯¥æ¨¡å‹åœ¨è¶…è¿‡28,000å°æ—¶çš„EEGæ•°æ®ä¸Šé€šè¿‡åŒåŸŸæ©ç é‡å»ºç›®æ ‡è¿›è¡Œé¢„è®­ç»ƒï¼Œåœ¨ä¹é¡¹ä¸‹æ¸¸ä»»åŠ¡çš„çº¿æ€§è¯„ä¼°å’Œå¾®è°ƒä¸­å‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„ç‰¹å®šä»»åŠ¡æ–¹æ³•å’ŒåŸºç¡€æ¨¡å‹ã€‚å®éªŒç»“æœè¯æ˜äº†Uni-NTFMå…·æœ‰å­¦ä¹ é€šç”¨å¤§è„‘æ´»åŠ¨è¡¨å¾çš„å“è¶Šèƒ½åŠ›ï¼Œä¸ºç¥ç»ä¿¡å·å¤„ç†é¢†åŸŸæä¾›äº†å¼ºæœ‰åŠ›çš„é¢„è®­ç»ƒæ–¹æ¡ˆã€‚",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24222v1",
      "published_date": "2025-09-29 03:03:32 UTC",
      "updated_date": "2025-09-29 03:03:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:11:47.191977+00:00"
    },
    {
      "arxiv_id": "2509.24219v1",
      "title": "ViReSkill: Vision-Grounded Replanning with Skill Memory for LLM-Based Planning in Lifelong Robot Learning",
      "title_zh": "ViReSkillï¼šé¢å‘æœºå™¨äººç»ˆèº«å­¦ä¹ ä¸­å¤§è¯­è¨€æ¨¡å‹è§„åˆ’çš„è§†è§‰è½åœ°é‡è§„åˆ’ä¸æŠ€èƒ½è®°å¿†",
      "authors": [
        "Tomoyuki Kagaya",
        "Subramanian Lakshmi",
        "Anbang Ye",
        "Thong Jing Yuan",
        "Jayashree Karlekar",
        "Sugiri Pranata",
        "Natsuki Murakami",
        "Akira Kinose",
        "Yang You"
      ],
      "abstract": "Robots trained via Reinforcement Learning (RL) or Imitation Learning (IL) often adapt slowly to new tasks, whereas recent Large Language Models (LLMs) and Vision-Language Models (VLMs) promise knowledge-rich planning from minimal data. Deploying LLMs/VLMs for motion planning, however, faces two key obstacles: (i) symbolic plans are rarely grounded in scene geometry and object physics, and (ii) model outputs can vary for identical prompts, undermining execution reliability. We propose ViReSkill, a framework that pairs vision-grounded replanning with a skill memory for accumulation and reuse. When a failure occurs, the replanner generates a new action sequence conditioned on the current scene, tailored to the observed state. On success, the executed plan is stored as a reusable skill and replayed in future encounters without additional calls to LLMs/VLMs. This feedback loop enables autonomous continual learning: each attempt immediately expands the skill set and stabilizes subsequent executions. We evaluate ViReSkill on simulators such as LIBERO and RLBench as well as on a physical robot. Across all settings, it consistently outperforms conventional baselines in task success rate, demonstrating robust sim-to-real generalization.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ViReSkillï¼Œä¸€ç§ç»“åˆè§†è§‰æ¥åœ°é‡è§„åˆ’(Vision-Grounded Replanning)ä¸æŠ€èƒ½è®°å¿†(Skill Memory)çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³LLMså’ŒVLMsåœ¨æœºå™¨äººè¿åŠ¨è§„åˆ’ä¸­ç¼ºä¹ç‰©ç†æ¥åœ°åŠæ‰§è¡Œå¯é æ€§å·®çš„é—®é¢˜ã€‚å½“ä»»åŠ¡å¤±è´¥æ—¶ï¼Œé‡è§„åˆ’å™¨æ ¹æ®å½“å‰åœºæ™¯ç”Ÿæˆæ–°çš„åŠ¨ä½œåºåˆ—ï¼Œè€ŒæˆåŠŸåçš„è®¡åˆ’åˆ™ä½œä¸ºå¯å¤ç”¨æŠ€èƒ½å­˜å‚¨åœ¨è®°å¿†ä¸­ï¼Œä»¥ä¾¿æœªæ¥ç›´æ¥è°ƒç”¨è€Œæ— éœ€é‡å¤è¯·æ±‚å¤§æ¨¡å‹ã€‚è¿™ç§åé¦ˆæœºåˆ¶æ”¯æŒæœºå™¨äººçš„è‡ªä¸»æŒç»­å­¦ä¹ (Autonomous Continual Learning)ï¼Œé€šè¿‡ä¸æ–­ç§¯ç´¯æŠ€èƒ½æ¥å¢å¼ºæ‰§è¡Œçš„ç¨³å®šæ€§ã€‚åœ¨LIBEROã€RLBenchæ¨¡æ‹Ÿå™¨åŠç‰©ç†æœºå™¨äººä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒViReSkillåœ¨ä»»åŠ¡æˆåŠŸç‡ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»ŸåŸºçº¿æ¨¡å‹ã€‚è¯¥æˆæœå±•ç°äº†å¼ºå¤§çš„ä»æ¨¡æ‹Ÿåˆ°ç°å®(Sim-to-Real)çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºå®ç°ç¨³å¥çš„ç»ˆèº«æœºå™¨äººå­¦ä¹ æä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24219v1",
      "published_date": "2025-09-29 02:58:53 UTC",
      "updated_date": "2025-09-29 02:58:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:12:00.685281+00:00"
    },
    {
      "arxiv_id": "2509.24218v2",
      "title": "Conda: Column-Normalized Adam for Training Large Language Models Faster",
      "title_zh": "Condaï¼šåŠ©åŠ›å¤§è¯­è¨€æ¨¡å‹æ›´å¿«é€Ÿè®­ç»ƒçš„åˆ—å½’ä¸€åŒ– Adam",
      "authors": [
        "Junjie Wang",
        "Pan Zhou",
        "Yiming Dong",
        "Huan Li",
        "Jia Li",
        "Xun Zhou",
        "Qicheng Lao",
        "Cong Fang",
        "Zhouchen Lin"
      ],
      "abstract": "Large language models (LLMs) have demonstrated impressive generalization and emergent capabilities, yet their pre-training remains computationally expensive and sensitive to optimization dynamics. While Adam-based optimizers offer fast convergence by adapting learning rates coordinate-wise, recent studies reveal that their updates often suffer from poor spectral conditioning and low-rank structures, hindering efficiency. Muon addresses this issue via global spectral normalization but lacks the per-coordinate adaptivity of Adam. In this work, we propose Column-Normalized Adam (Conda), a novel optimizer that bridges the strengths of both approaches. Conda projects updates into an orthogonal subspace and applies column-wise second moment normalization based on the projected gradients, thereby achieving both improved spectral conditioning and maintaining coordinate-wise adaptivity. This design alleviates the spectral pathologies of Adam while preserving its fast convergence behavior. Extensive experiments on the LLaMA and GPT-2 series show that Conda consistently outperforms AdamW, Muon, and other baselines in pre-training. Remarkably, on the LLaMA series, Conda achieves 2-2.5 the convergence speed of AdamW, measured in both training steps and training time. Further ablations demonstrate its robustness under diverse training setups. These results collectively highlight Conda as an effective and broadly applicable optimizer for large-scale LLM training. The code is released on https://github.com/jie040109/Conda",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Conda (Column-Normalized Adam)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨åŠ é€Ÿå¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)é¢„è®­ç»ƒçš„æ–°å‹ä¼˜åŒ–å™¨ã€‚Condaé’ˆå¯¹Adamåœ¨è°±è°ƒèŠ‚(spectral conditioning)å’Œä½ç§©ç»“æ„æ–¹é¢çš„å±€é™æ€§ï¼Œä»¥åŠMuonç¼ºä¹é€åæ ‡è‡ªé€‚åº”æ€§(coordinate-wise adaptivity)çš„é—®é¢˜ï¼Œå°†ä¸¤è€…çš„ä¼˜åŠ¿è¿›è¡Œäº†æœ‰æ•ˆç»“åˆã€‚è¯¥æ–¹æ³•é€šè¿‡å°†æ›´æ–°æŠ•å½±åˆ°æ­£äº¤å­ç©ºé—´ï¼Œå¹¶åŸºäºæŠ•å½±æ¢¯åº¦åº”ç”¨åˆ—å‘äºŒé˜¶çŸ©å½’ä¸€åŒ–(column-wise second moment normalization)ï¼Œåœ¨æ”¹å–„è°±ç—…æ€é—®é¢˜çš„åŒæ—¶ä¿æŒäº†AdamåŸæœ‰çš„å¿«é€Ÿæ”¶æ•›è¡Œä¸ºã€‚åœ¨LLaMAå’ŒGPT-2ç³»åˆ—æ¨¡å‹ä¸Šçš„å®éªŒè¯æ˜ï¼ŒCondaçš„é¢„è®­ç»ƒæ€§èƒ½æŒç»­ä¼˜äºAdamWå’ŒMuonç­‰åŸºå‡†æ¨¡å‹ã€‚å°¤å…¶åœ¨LLaMAç³»åˆ—ä¸Šï¼ŒCondaçš„æ”¶æ•›é€Ÿåº¦è¾¾åˆ°äº†AdamWçš„2è‡³2.5å€ã€‚è¿™äº›ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒCondaæ˜¯å¤§è§„æ¨¡LLMè®­ç»ƒä¸­ä¸€ç§é«˜æ•ˆä¸”å…·æœ‰å¹¿æ³›é€‚ç”¨æ€§çš„ä¼˜åŒ–å™¨æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24218v2",
      "published_date": "2025-09-29 02:58:19 UTC",
      "updated_date": "2025-09-30 02:02:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:12:03.396153+00:00"
    },
    {
      "arxiv_id": "2509.24215v1",
      "title": "Metamorphic Testing for Audio Content Moderation Software",
      "title_zh": "éŸ³é¢‘å†…å®¹å®¡æ ¸è½¯ä»¶çš„èœ•å˜æµ‹è¯•",
      "authors": [
        "Wenxuan Wang",
        "Yongjiang Wu",
        "Junyuan Zhang",
        "Shuqing Li",
        "Yun Peng",
        "Wenting Chen",
        "Shuai Wang",
        "Michael R. Lyu"
      ],
      "abstract": "The rapid growth of audio-centric platforms and applications such as WhatsApp and Twitter has transformed the way people communicate and share audio content in modern society. However, these platforms are increasingly misused to disseminate harmful audio content, such as hate speech, deceptive advertisements, and explicit material, which can have significant negative consequences (e.g., detrimental effects on mental health). In response, researchers and practitioners have been actively developing and deploying audio content moderation tools to tackle this issue. Despite these efforts, malicious actors can bypass moderation systems by making subtle alterations to audio content, such as modifying pitch or inserting noise. Moreover, the effectiveness of modern audio moderation tools against such adversarial inputs remains insufficiently studied. To address these challenges, we propose MTAM, a Metamorphic Testing framework for Audio content Moderation software. Specifically, we conduct a pilot study on 2000 audio clips and define 14 metamorphic relations across two perturbation categories: Audio Features-Based and Heuristic perturbations. MTAM applies these metamorphic relations to toxic audio content to generate test cases that remain harmful while being more likely to evade detection. In our evaluation, we employ MTAM to test five commercial textual content moderation software and an academic model against three kinds of toxic content. The results show that MTAM achieves up to 38.6%, 18.3%, 35.1%, 16.7%, and 51.1% error finding rates (EFR) when testing commercial moderation software provided by Gladia, Assembly AI, Baidu, Nextdata, and Tencent, respectively, and it obtains up to 45.7% EFR when testing the state-of-the-art algorithms from the academy.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹éŸ³é¢‘ç¤¾äº¤å¹³å°ä¸­æœ‰å®³å†…å®¹ä¼ æ’­åŠå…¶é€šè¿‡å¾®å°æ”¹åŠ¨å³å¯ç»•è¿‡å®¡æŸ¥çš„é—®é¢˜ï¼Œæå‡ºäº† MTAMï¼Œä¸€ç§ä¸“é—¨ç”¨äºéŸ³é¢‘å†…å®¹å®¡æŸ¥è½¯ä»¶çš„ Metamorphic Testing æ¡†æ¶ã€‚ç ”ç©¶è€…é€šè¿‡åˆæ­¥å®éªŒå®šä¹‰äº†æ¶µç›– Audio Features-Based å’Œ Heuristic ä¸¤ç±»æ‰°åŠ¨çš„ 14 ç§ Metamorphic Relationsï¼Œåˆ©ç”¨è¿™äº›å…³ç³»ç”Ÿæˆèƒ½å¤Ÿé€ƒé¿æ£€æµ‹ä½†ä»ä¿ç•™æœ‰å®³æ€§çš„æµ‹è¯•ç”¨ä¾‹ã€‚è¯„ä¼°é˜¶æ®µä½¿ç”¨ MTAM å¯¹ Gladiaã€Assembly AIã€Baiduã€Nextdataã€Tencent ç­‰äº”æ¬¾å•†ä¸šå®¡æŸ¥è½¯ä»¶åŠä¸€ç§å­¦æœ¯æ¨¡å‹è¿›è¡Œäº†æµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMTAM åœ¨å•†ä¸šè½¯ä»¶æµ‹è¯•ä¸­è¡¨ç°å‡ºé«˜è¾¾ 51.1% çš„ Error Finding Rates (EFR)ï¼Œåœ¨å­¦æœ¯å‰æ²¿æ¨¡å‹ä¸Šçš„ EFR ä¹Ÿè¾¾åˆ°äº† 45.7%ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†ç°æœ‰éŸ³é¢‘å®¡æŸ¥å·¥å…·åœ¨é¢å¯¹å¯¹æŠ—æ€§æ‰°åŠ¨æ—¶çš„è„†å¼±æ€§ï¼Œä¸ºæå‡éŸ³é¢‘å†…å®¹å®‰å…¨é˜²æŠ¤èƒ½åŠ›æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.MM"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted by ASE 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.24215v1",
      "published_date": "2025-09-29 02:55:07 UTC",
      "updated_date": "2025-09-29 02:55:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:11:59.186651+00:00"
    },
    {
      "arxiv_id": "2509.24210v1",
      "title": "BeyondBench: Benchmark-Free Evaluation of Reasoning in Language Models",
      "title_zh": "BeyondBenchï¼šè¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ— åŸºå‡†è¯„ä¼°",
      "authors": [
        "Gaurav Srivastava",
        "Aafiya Hussain",
        "Zhenyu Bi",
        "Swastik Roy",
        "Priya Pitre",
        "Meng Lu",
        "Morteza Ziyadi",
        "Xuan Wang"
      ],
      "abstract": "Evaluating language models fairly is becoming harder as static benchmarks available on the internet risk contamination by training data. This makes it unclear whether models are truly reasoning or just recalling answers. In this paper, we introduce BeyondBench, an evaluation framework that avoids this problem by using algorithmic problem generation. Unlike traditional benchmarks that risk contamination from internet-scale training data, BeyondBench creates mathematically grounded problems on the fly, ensuring each test remains fresh and uncontaminated. Our framework covers 44 algorithmic tasks with a total of 117 variations, grouped into three difficulty levels: the Easy Suite (29 tasks) for basic arithmetic and statistics, the Medium Suite (5 tasks, 49 variations) for sequence patterns and reasoning, and the Hard Suite (10 tasks, 68 variations) tackling NP-complete and constraint satisfaction problems. Each task generates problems from a combinatorial space larger than 10^15 unique instances, with solutions verified deterministically by mathematical proofs. We evaluated 101 language models, including 85 open-source and 16 closed-source models, spanning sizes from 0.5B to 141B parameters and multiple quantization schemes. Our results show consistent reasoning deficiencies across model families, with performance degrading sharply as problem complexity increases from polynomial to exponential. In our Hard Suite evaluations, models such as Gemini-2.5-pro, Llama-3.3-70B, and Qwen2.5-72B achieved average accuracies of 56.38%, 26.91%, and 33.60%, respectively. Moreover, we observe that performance drops drastically without tool usage, with GPT-5, GPT-5-mini, and GPT-5-nano showing a decline of 16.81%, 28.05%, and 47.59% accuracy on the hard suite. Our leaderboard is publicly available at https://ctrl-gaurav.github.io/BeyondBench/",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† BeyondBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨é™æ€åŸºå‡†æµ‹è¯•ä¸­é¢ä¸´æ•°æ®æ±¡æŸ“ (Contamination) é£é™©çš„è¯„ä¼°æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ç®—æ³•åŒ–é¢˜ç›®ç”Ÿæˆ (Algorithmic Problem Generation) æŠ€æœ¯å³æ—¶åˆ›å»ºå…·æœ‰æ•°å­¦åŸºç¡€çš„æµ‹è¯•é¢˜ï¼Œç¡®ä¿æ¯ä¸ªæµ‹è¯•å®ä¾‹çš„ç‹¬ç‰¹æ€§ä¸ä¸å¯é¢„æµ‹æ€§ã€‚BeyondBench æ¶µç›–äº† 44 é¡¹ç®—æ³•ä»»åŠ¡åŠ 117 ç§å˜ä½“ï¼Œéš¾åº¦è·¨è¶ŠåŸºç¡€ç®—æœ¯åˆ° NP-complete ç­‰å¤æ‚çº¦æŸæ»¡è¶³é—®é¢˜ï¼Œå…¶ç»„åˆç©ºé—´è¶…è¿‡ 10^15 ä¸ªå”¯ä¸€å®ä¾‹ã€‚é€šè¿‡å¯¹ 101 ä¸ªå¼€æºå’Œé—­æºæ¨¡å‹çš„è¯„ä¼°å‘ç°ï¼Œæ¨¡å‹æ€§èƒ½éšç€é—®é¢˜å¤æ‚åº¦ä»å¤šé¡¹å¼çº§å‘æŒ‡æ•°çº§å¢åŠ è€Œå‰§çƒˆä¸‹æ»‘ã€‚å®éªŒæ•°æ®è¡¨æ˜ï¼ŒGemini-2.5-pro å’Œ Llama-3.3-70B ç­‰æ¨¡å‹åœ¨å¤„ç†é«˜éš¾åº¦é€»è¾‘æ¨ç†æ—¶ä»é¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼ºè°ƒäº†å·¥å…·ä½¿ç”¨ (Tool Usage) å¯¹æ¨¡å‹è¡¨ç°çš„å…³é”®ä½œç”¨ï¼Œå‘ç°ç¼ºä¹å·¥å…·æ”¯æŒä¼šå¯¼è‡´ GPT-5 ç­‰æ¨¡å‹çš„å‡†ç¡®ç‡æ˜¾è‘—é™ä½ã€‚è¯¥æ¡†æ¶ä¸ºåŒºåˆ†æ¨¡å‹çš„çœŸå®æ¨ç†èƒ½åŠ›ä¸å•çº¯çš„ç­”æ¡ˆå¬å›æä¾›äº†ç§‘å­¦ä¸”éš¾ä»¥æ“çºµçš„è¡¡é‡æ ‡å‡†ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "113 pages, 5 figures, 30 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.24210v1",
      "published_date": "2025-09-29 02:49:01 UTC",
      "updated_date": "2025-09-29 02:49:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:12:11.897739+00:00"
    },
    {
      "arxiv_id": "2510.09633v1",
      "title": "Hound: Relation-First Knowledge Graphs for Complex-System Reasoning in Security Audits",
      "title_zh": "Houndï¼šé¢å‘å®‰å…¨å®¡è®¡ä¸­å¤æ‚ç³»ç»Ÿæ¨ç†çš„å…³ç³»ä¼˜å…ˆçŸ¥è¯†å›¾è°±",
      "authors": [
        "Bernhard Mueller"
      ],
      "abstract": "Hound introduces a relation-first graph engine that improves system-level reasoning across interrelated components in complex codebases. The agent designs flexible, analyst-defined views with compact annotations (e.g., monetary/value flows, authentication/authorization roles, call graphs, protocol invariants) and uses them to anchor exact retrieval: for any question, it loads precisely the code that matters (often across components) so it can zoom out to system structure and zoom in to the decisive lines. A second contribution is a persistent belief system: long-lived vulnerability hypotheses whose confidence is updated as evidence accrues. The agent employs coverage-versus-intuition planning and a QA finalizer to confirm or reject hypotheses. On a five-project subset of ScaBench[1], Hound improves recall and F1 over a baseline LLM analyzer (micro recall 31.2% vs. 8.3%; F1 14.2% vs. 9.8%) with a modest precision trade-off. We attribute these gains to flexible, relation-first graphs that extend model understanding beyond call/dataflow to abstract aspects, plus the hypothesis-centric loop; code and artifacts are released to support reproduction.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Houndï¼Œä¸€ç§å…³ç³»ä¼˜å…ˆ(relation-first)çš„å›¾å¼•æ“ï¼Œæ—¨åœ¨æå‡å¤æ‚ä»£ç åº“å®‰å…¨å®¡è®¡ä¸­çš„è·¨ç»„ä»¶ç³»ç»Ÿçº§æ¨ç†èƒ½åŠ›ã€‚è¯¥æ™ºèƒ½ä½“åˆ©ç”¨åˆ†æå¸ˆå®šä¹‰çš„çµæ´»è§†å›¾å’Œç´§å‡‘æ ‡æ³¨ï¼Œå¦‚monetary/value flowsã€authentication/authorization roleså’Œcall graphsï¼Œå®ç°å¯¹å…³é”®ä»£ç çš„ç²¾ç¡®æ£€ç´¢ä¸å®šä½ã€‚æ­¤å¤–ï¼ŒHoundå¼•å…¥äº†æŒä¹…åŒ–ä¿¡å¿µç³»ç»Ÿ(persistent belief system)ï¼Œé€šè¿‡æ”¶é›†è¯æ®æŒç»­æ›´æ–°é•¿æœŸæ¼æ´å‡è®¾çš„ç½®ä¿¡åº¦ï¼Œå¹¶é‡‡ç”¨coverage-versus-intuitionè§„åˆ’ä¸QA finalizeræ¥éªŒè¯å‡è®¾ã€‚åœ¨ScaBenchæ•°æ®é›†ä¸Šçš„æµ‹è¯•ç»“æœè¡¨æ˜ï¼ŒHoundåœ¨å¾®å¬å›ç‡(micro recall)å’ŒF1åˆ†æ•°ä¸Šå‡æ˜¾è‘—ä¼˜äºåŸºå‡†LLMåˆ†æå™¨ï¼Œåˆ†åˆ«ä»8.3%æå‡è‡³31.2%ä»¥åŠä»9.8%æå‡è‡³14.2%ã€‚å®éªŒè¯æ˜ï¼Œå…³ç³»ä¼˜å…ˆçš„å›¾ç»“æ„ä¸ä»¥å‡è®¾ä¸ºä¸­å¿ƒçš„å¾ªç¯æœºåˆ¶èƒ½æœ‰æ•ˆæ‰©å±•æ¨¡å‹å¯¹ç³»ç»ŸæŠ½è±¡å±‚é¢çš„ç†è§£ï¼Œç›¸å…³ä»£ç ä¸å·¥ä»¶å·²å¼€æºä»¥ä¾›å¤ç°ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG",
        "cs.PL"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.09633v1",
      "published_date": "2025-09-29 02:46:02 UTC",
      "updated_date": "2025-09-29 02:46:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:12:21.181237+00:00"
    },
    {
      "arxiv_id": "2509.24207v1",
      "title": "Humanline: Online Alignment as Perceptual Loss",
      "title_zh": "Humanlineï¼šä½œä¸ºæ„ŸçŸ¥æŸå¤±çš„åœ¨çº¿å¯¹é½",
      "authors": [
        "Sijia Liu",
        "Niklas Muennighoff",
        "Kawin Ethayarajh"
      ],
      "abstract": "Online alignment (e.g., GRPO) is generally more performant than offline alignment (e.g., DPO) -- but why? Drawing on prospect theory from behavioral economics, we propose a human-centric explanation. We prove that online on-policy sampling better approximates the human-perceived distribution of what the model can produce, and PPO/GRPO-style clipping -- originally introduced to just stabilize training -- recovers a perceptual bias in how humans perceive probability. In this sense, PPO/GRPO act as perceptual losses already. Our theory further suggests that the online/offline dichotomy is itself incidental to maximizing human utility, since we can achieve the same effect by selectively training on any data in a manner that mimics human perception, rather than restricting ourselves to online on-policy data. Doing so would allow us to post-train more quickly, cheaply, and flexibly without sacrificing performance. To this end, we propose a design pattern that explicitly incorporates perceptual distortions of probability into objectives like DPO/KTO/GRPO, creating humanline variants of them. Surprisingly, we find that these humanline variants, even when trained with offline off-policy data, can match the performance of their online counterparts on both verifiable and unverifiable tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨çº¿å¯¹é½(online alignment)å¦‚GRPOæ¯”ç¦»çº¿å¯¹é½(offline alignment)å¦‚DPOæ€§èƒ½æ›´ä¼˜çš„å†…åœ¨é€»è¾‘ï¼Œå¹¶å€Ÿé‰´è¡Œä¸ºç»æµå­¦ä¸­çš„å‰æ™¯ç†è®º(prospect theory)æå‡ºäº†ä¸€ä¸ªä»¥äººä¸ºä¸­å¿ƒçš„è§£é‡Šã€‚ç ”ç©¶è¯æ˜ï¼Œåœ¨çº¿ç­–ç•¥é‡‡æ ·(on-policy sampling)èƒ½æ›´å¥½åœ°æ¨¡æ‹Ÿäººç±»å¯¹æ¨¡å‹ç”Ÿæˆåˆ†å¸ƒçš„æ„ŸçŸ¥ï¼Œè€ŒPPO/GRPOä¸­çš„è£å‰ª(clipping)æœºåˆ¶å®è´¨ä¸Šè¡¥å¿äº†äººç±»æ„ŸçŸ¥æ¦‚ç‡æ—¶çš„æ„ŸçŸ¥åå·®(perceptual bias)ã€‚ä»è¿™ä¸ªæ„ä¹‰ä¸Šè¯´ï¼ŒPPO/GRPOå·²ç»å……å½“äº†æ„ŸçŸ¥æŸå¤±(perceptual losses)çš„ä½œç”¨ï¼Œä¸”è¿™ç§åœ¨çº¿ä¸ç¦»çº¿çš„æ€§èƒ½å·®å¼‚å¹¶éä¸å¯é€¾è¶Šã€‚åŸºäºæ­¤ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºHumanlineçš„è®¾è®¡æ¨¡å¼ï¼Œé€šè¿‡å°†æ¦‚ç‡çš„æ„ŸçŸ¥æ‰­æ›²(perceptual distortions)æ˜¾å¼å¼•å…¥DPOã€KTOå’ŒGRPOç­‰ç›®æ ‡å‡½æ•°ä¸­ï¼Œæ„å»ºäº†ç›¸åº”çš„Humanlineå˜ä½“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿ä½¿ç”¨ç¦»çº¿ç­–ç•¥æ•°æ®è¿›è¡Œè®­ç»ƒï¼ŒHumanlineå˜ä½“åœ¨å¯éªŒè¯å’Œä¸å¯éªŒè¯ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¹Ÿèƒ½ä¸åœ¨çº¿å¯¹é½æ¨¡å‹ç›¸åª²ç¾ã€‚è¿™ä¸€å‘ç°ä¸ºå®ç°æ›´å¿«é€Ÿã€å»‰ä»·ä¸”çµæ´»çš„åè®­ç»ƒ(post-training)æä¾›äº†æ–°è·¯å¾„ï¼Œä¸”æ— éœ€ä»¥ç‰ºç‰²æ¨¡å‹æ€§èƒ½ä¸ºä»£ä»·ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24207v1",
      "published_date": "2025-09-29 02:41:16 UTC",
      "updated_date": "2025-09-29 02:41:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:12:28.093419+00:00"
    },
    {
      "arxiv_id": "2509.24204v2",
      "title": "BALR-SAM: Boundary-Aware Low-Rank Adaptation of SAM for Resource-Efficient Medical Image Segmentation",
      "title_zh": "BALR-SAMï¼šé¢å‘èµ„æºé«˜æ•ˆåŒ»å­¦å›¾åƒåˆ†å‰²çš„ SAM è¾¹ç•Œæ„ŸçŸ¥ä½ç§©è‡ªé€‚åº”",
      "authors": [
        "Zelin Liu",
        "Sicheng Dong",
        "Bocheng Li",
        "Yixuan Yang",
        "Jiacheng Ruan",
        "Chenxu Zhou",
        "Suncheng Xiang"
      ],
      "abstract": "Vision foundation models like the Segment Anything Model (SAM), pretrained on large-scale natural image datasets, often struggle in medical image segmentation due to a lack of domain-specific adaptation. In clinical practice, fine-tuning such models efficiently for medical downstream tasks with minimal resource demands, while maintaining strong performance, is challenging. To address these issues, we propose BALR-SAM, a boundary-aware low-rank adaptation framework that enhances SAM for medical imaging. It combines three tailored components: (1) a Complementary Detail Enhancement Network (CDEN) using depthwise separable convolutions and multi-scale fusion to capture boundary-sensitive features essential for accurate segmentation; (2) low-rank adapters integrated into SAM's Vision Transformer blocks to optimize feature representation and attention for medical contexts, while simultaneously significantly reducing the parameter space; and (3) a low-rank tensor attention mechanism in the mask decoder, cutting memory usage by 75% and boosting inference speed. Experiments on standard medical segmentation datasets show that BALR-SAM, without requiring prompts, outperforms several state-of-the-art (SOTA) methods, including fully fine-tuned MedSAM, while updating just 1.8% (11.7M) of its parameters.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† BALR-SAMï¼Œä¸€ç§è¾¹ç•Œæ„ŸçŸ¥ä½ç§©é€‚é…æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ Segment Anything Model (SAM) åœ¨åŒ»ç–—å›¾åƒåˆ†å‰²ä¸­ç”±äºç¼ºä¹é¢†åŸŸé€‚é…å’Œè®¡ç®—èµ„æºéœ€æ±‚è¿‡é«˜è€Œé¢ä¸´çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶å¼•å…¥äº† Complementary Detail Enhancement Network (CDEN)ï¼Œåˆ©ç”¨æ·±åº¦å¯åˆ†ç¦»å·ç§¯å’Œå¤šå°ºåº¦èåˆæ•æ‰å¯¹ç²¾ç¡®åˆ†å‰²è‡³å…³é‡è¦çš„è¾¹ç•Œæ•æ„Ÿç‰¹å¾ã€‚åŒæ—¶ï¼Œç ”ç©¶åœ¨ SAM çš„ Vision Transformer æ¨¡å—ä¸­é›†æˆäº†ä½ç§©é€‚é…å™¨ä»¥ä¼˜åŒ–åŒ»ç–—è¯­å¢ƒä¸‹çš„ç‰¹å¾è¡¨ç¤ºï¼Œå¹¶åœ¨æ©ç è§£ç å™¨ä¸­é‡‡ç”¨ä½ç§©å¼ é‡æ³¨æ„åŠ›æœºåˆ¶ï¼ŒæˆåŠŸå°†å†…å­˜æ¶ˆè€—é™ä½äº† 75% å¹¶æ˜¾è‘—æå‡äº†æ¨ç†é€Ÿåº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBALR-SAM åœ¨æ— éœ€æç¤ºçš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½ä¼˜äº MedSAM ç­‰ä¸»æµ SOTA æ–¹æ³•ã€‚è¯¥æ¨¡å‹ä»…éœ€æ›´æ–° 1.8% (11.7M) çš„å‚æ•°å³å¯å®ç°é«˜æ€§èƒ½åˆ†å‰²ï¼Œä¸ºèµ„æºå—é™çš„ä¸´åºŠåŒ»å­¦å½±åƒä»»åŠ¡æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”ç²¾ç¡®çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24204v2",
      "published_date": "2025-09-29 02:36:09 UTC",
      "updated_date": "2025-10-31 07:38:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:12:34.394432+00:00"
    },
    {
      "arxiv_id": "2509.24203v1",
      "title": "Group-Relative REINFORCE Is Secretly an Off-Policy Algorithm: Demystifying Some Myths About GRPO and Its Friends",
      "title_zh": "ç»„ç›¸å¯¹ REINFORCE å®ä¸ºç¦»ç­–ç®—æ³•ï¼šç ´é™¤å…³äº GRPO åŠå…¶åŒç±»ç®—æ³•çš„è¿·æ€",
      "authors": [
        "Chaorui Yao",
        "Yanxi Chen",
        "Yuchang Sun",
        "Yushuo Chen",
        "Wenhao Zhang",
        "Xuchen Pan",
        "Yaliang Li",
        "Bolin Ding"
      ],
      "abstract": "Off-policy reinforcement learning (RL) for large language models (LLMs) is attracting growing interest, driven by practical constraints in real-world applications, the complexity of LLM-RL infrastructure, and the need for further innovations of RL methodologies. While classic REINFORCE and its modern variants like Group Relative Policy Optimization (GRPO) are typically regarded as on-policy algorithms with limited tolerance of off-policyness, we present in this work a first-principles derivation for group-relative REINFORCE without assuming a specific training data distribution, showing that it admits a native off-policy interpretation. This perspective yields two general principles for adapting REINFORCE to off-policy settings: regularizing policy updates, and actively shaping the data distribution. Our analysis demystifies some myths about the roles of importance sampling and clipping in GRPO, unifies and reinterprets two recent algorithms -- Online Policy Mirror Descent (OPMD) and Asymmetric REINFORCE (AsymRE) -- as regularized forms of the REINFORCE loss, and offers theoretical justification for seemingly heuristic data-weighting strategies. Our findings lead to actionable insights that are validated with extensive empirical studies, and open up new opportunities for principled algorithm design in off-policy RL for LLMs. Source code for this work is available at https://github.com/modelscope/Trinity-RFT/tree/main/examples/rec_gsm8k.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡ç¬¬ä¸€æ€§åŸç†æ¨å¯¼æ­ç¤ºäº† Group Relative Policy Optimization (GRPO) åŠå…¶å˜ä½“åœ¨æœ¬è´¨ä¸Šæ˜¯ off-policy ç®—æ³•ï¼ŒæŒ‘æˆ˜äº†å…¶é€šå¸¸è¢«è§†ä¸º on-policy çš„ä¼ ç»Ÿè§‚ç‚¹ã€‚ä½œè€…è¯æ˜äº† group-relative REINFORCE åœ¨ä¸è®¾å®šç‰¹å®šè®­ç»ƒæ•°æ®åˆ†å¸ƒçš„æƒ…å†µä¸‹å…·æœ‰åŸç”Ÿçš„ off-policy è§£é‡Šï¼Œå¹¶æ®æ­¤æå‡ºäº†é€‚é… off-policy åœºæ™¯çš„ä¸¤ä¸ªé€šç”¨åŸåˆ™ï¼šæ­£åˆ™åŒ–ç­–ç•¥æ›´æ–°å’Œä¸»åŠ¨å¡‘é€ æ•°æ®åˆ†å¸ƒã€‚è¯¥åˆ†æè¿›ä¸€æ­¥æ¾„æ¸…äº† GRPO ä¸­é‡è¦æ€§é‡‡æ · (importance sampling) å’Œè£å‰ª (clipping) çš„ä½œç”¨ï¼Œå¹¶å°† Online Policy Mirror Descent (OPMD) å’Œ Asymmetric REINFORCE (AsymRE) ç»Ÿä¸€è§£é‡Šä¸º REINFORCE æŸå¤±çš„æ­£åˆ™åŒ–å½¢å¼ã€‚é€šè¿‡å¹¿æ³›çš„å®è¯ç ”ç©¶ï¼Œè¯¥å·¥ä½œä¸ºå¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•è®¾è®¡æä¾›äº†ç†è®ºæ”¯æ’‘å’Œå¯æ“ä½œçš„è§è§£ï¼Œä¸ºå¼€å‘æ›´é«˜æ•ˆçš„ off-policy å¼ºåŒ–å­¦ä¹ æ–¹æ³•å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24203v1",
      "published_date": "2025-09-29 02:34:54 UTC",
      "updated_date": "2025-09-29 02:34:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:12:35.387850+00:00"
    },
    {
      "arxiv_id": "2509.24202v1",
      "title": "Can Large Language Models Express Uncertainty Like Human?",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹èƒ½å¦åƒäººç±»ä¸€æ ·è¡¨è¾¾ä¸ç¡®å®šæ€§ï¼Ÿ",
      "authors": [
        "Linwei Tao",
        "Yi-Fan Yeh",
        "Bo Kai",
        "Minjing Dong",
        "Tao Huang",
        "Tom A. Lamb",
        "Jialin Yu",
        "Philip H. S. Torr",
        "Chang Xu"
      ],
      "abstract": "Large language models (LLMs) are increasingly used in high-stakes settings, where overconfident responses can mislead users. Reliable confidence estimation has been shown to enhance trust and task accuracy. Yet existing methods face practical barriers: logits are often hidden, multi-sampling is computationally expensive, and verbalized numerical uncertainty (e.g., giving a 0-100 score) deviates from natural communication. We revisit linguistic confidence (LC), where models express uncertainty through hedging language (e.g., probably, might), offering a lightweight and human-centered alternative. To advance this direction, we (1) release the first diverse, large-scale dataset of hedging expressions with human-annotated confidence scores, and (2) propose a lightweight mapper that converts hedges into confidence scores at near-zero cost. Building on these resources, we (3) conduct the first systematic study of LC across modern LLMs and QA benchmarks, revealing that while most LLMs underperform in expressing reliable LC, carefully designed prompting achieves competitive calibration and discriminability. Finally, we (4) introduce a fine-tuning framework that further improves LC reliability. Taken together, our work positions linguistic confidence as a scalable, efficient, and human-aligned approach to LLM uncertainty estimation, and calls for deeper exploration of this promising yet underexplored direction.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)æ˜¯å¦èƒ½åƒäººç±»ä¸€æ ·è¡¨è¾¾ä¸ç¡®å®šæ€§ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ç½®ä¿¡åº¦è¯„ä¼°æ–¹æ³•ï¼ˆå¦‚éšè—Logitsã€é«˜è®¡ç®—æˆæœ¬çš„å¤šé‡‡æ ·æˆ–éè‡ªç„¶çš„æ•°å€¼è¯„åˆ†ï¼‰æ‰€é¢ä¸´çš„å®é™…éšœç¢ã€‚ä½œè€…é‡æ–°å®¡è§†äº†è¯­è¨€ç½®ä¿¡åº¦(Linguistic Confidence, LC)ï¼Œå³é€šè¿‡å¯¹å†²æ€§è¯­è¨€ï¼ˆå¦‚probably, mightï¼‰æ¥è¡¨è¾¾ä¸ç¡®å®šæ€§çš„è½»é‡çº§ä¸”ä»¥äººä¸ºæœ¬çš„æ›¿ä»£æ–¹æ¡ˆã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿå‘å¸ƒäº†é¦–ä¸ªåŒ…å«äººå·¥æ ‡æ³¨ç½®ä¿¡åˆ†æ•°çš„å¯¹å†²è¡¨è¾¾å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªèƒ½ä»¥è¿‘ä¹é›¶æˆæœ¬å°†å¯¹å†²è¯­è½¬æ¢ä¸ºç½®ä¿¡åˆ†æ•°çš„è½»é‡çº§æ˜ å°„å™¨ã€‚é€šè¿‡å¯¹ç°ä»£LLMså’Œé—®ç­”åŸºå‡†æµ‹è¯•çš„ç³»ç»Ÿç ”ç©¶å‘ç°ï¼Œè™½ç„¶å¤šæ•°æ¨¡å‹åœ¨è¡¨è¾¾å¯é LCæ–¹é¢è¡¨ç°æ¬ ä½³ï¼Œä½†é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æç¤ºè¯(Prompting)å¯ä»¥å®ç°å…·æœ‰ç«äº‰åŠ›çš„æ ¡å‡†å’Œè¾¨åˆ«åŠ›ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†ä¸€ä¸ªå¾®è°ƒ(Fine-tuning)æ¡†æ¶ä»¥è¿›ä¸€æ­¥æå‡LCçš„å¯é æ€§ã€‚è¯¥å·¥ä½œå°†è¯­è¨€ç½®ä¿¡åº¦å®šä½ä¸ºä¸€ç§å¯æ‰©å±•ã€é«˜æ•ˆä¸”ç¬¦åˆäººç±»ä¹ æƒ¯çš„LLMä¸ç¡®å®šæ€§ä¼°è®¡æ–¹æ³•ï¼Œä¸ºè¿™ä¸€å…·æœ‰å‰æ™¯çš„ç ”ç©¶æ–¹å‘å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages",
      "pdf_url": "https://arxiv.org/pdf/2509.24202v1",
      "published_date": "2025-09-29 02:34:30 UTC",
      "updated_date": "2025-09-29 02:34:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:12:35.861647+00:00"
    },
    {
      "arxiv_id": "2509.24196v1",
      "title": "Chat to Chip: Large Language Model Based Design of Arbitrarily Shaped Metasurfaces",
      "title_zh": "Chat to Chipï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ä»»æ„å½¢çŠ¶è¶…è¡¨é¢è®¾è®¡",
      "authors": [
        "Huanshu Zhang",
        "Lei Kang",
        "Sawyer D. Campbell",
        "Douglas H. Werner"
      ],
      "abstract": "Traditional metasurface design is limited by the computational cost of full-wave simulations, preventing thorough exploration of complex configurations. Data-driven approaches have emerged as a solution to this bottleneck, replacing costly simulations with rapid neural network evaluations and enabling near-instant design for meta-atoms. Despite advances, implementing a new optical function still requires building and training a task-specific network, along with exhaustive searches for suitable architectures and hyperparameters. Pre-trained large language models (LLMs), by contrast, sidestep this laborious process with a simple fine-tuning technique. However, applying LLMs to the design of nanophotonic devices, particularly for arbitrarily shaped metasurfaces, is still in its early stages; as such tasks often require graphical networks. Here, we show that an LLM, fed with descriptive inputs of arbitrarily shaped metasurface geometries, can learn the physical relationships needed for spectral prediction and inverse design. We further benchmarked a range of open-weight LLMs and identified relationships between accuracy and model size at the billion-parameter level. We demonstrated that 1-D token-wise LLMs provide a practical tool to designing 2-D arbitrarily shaped metasurfaces. Linking natural-language interaction to electromagnetic modelling, this \"chat-to-chip\" workflow represents a step toward more user-friendly data-driven nanophotonics.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿè¶…è¡¨é¢ (metasurface) è®¾è®¡ä¸­å…¨æ³¢ä»¿çœŸ (full-wave simulations) è®¡ç®—æˆæœ¬é«˜æ˜‚ä»¥åŠç°æœ‰æ•°æ®é©±åŠ¨æ–¹æ³•éœ€é‡å¤è®­ç»ƒç‰¹å®šç½‘ç»œçš„é—®é¢˜ï¼Œæå‡ºäº†åä¸º Chat to Chip çš„åˆ›æ–°å·¥ä½œæµã€‚è¯¥æ–¹æ¡ˆåˆ©ç”¨é¢„è®­ç»ƒçš„å¤§è¯­è¨€æ¨¡å‹ (LLMs) é€šè¿‡ç®€å•çš„å¾®è°ƒ (fine-tuning) æŠ€æœ¯ï¼Œå®ç°äº†å¯¹ä»»æ„å½¢çŠ¶è¶…è¡¨é¢çš„å…‰è°±é¢„æµ‹ (spectral prediction) å’Œé€†å‘è®¾è®¡ (inverse design)ã€‚ç ”ç©¶è¯æ˜ï¼Œé€šè¿‡å°†å‡ ä½•ç»“æ„æè¿°ä¸ºæ–‡æœ¬è¾“å…¥ï¼ŒLLM èƒ½å¤Ÿå­¦ä¹ å¹¶æŒæ¡å¤æ‚çš„ç‰©ç†å…³ç³»ï¼Œä¸”ä¸€ç»´çš„ token åºåˆ—åŒ–å¤„ç†å¯ä»¥æœ‰æ•ˆåº”å¯¹äºŒç»´å‡ ä½•è®¾è®¡ä»»åŠ¡ã€‚åœ¨å¯¹å¤šç§å¼€æº LLMs è¿›è¡ŒåŸºå‡†æµ‹è¯•åï¼Œç ”ç©¶è€…è¯†åˆ«äº†åäº¿å‚æ•°çº§åˆ«ä¸‹æ¨¡å‹è§„æ¨¡ä¸è®¾è®¡å‡†ç¡®æ€§ä¹‹é—´çš„å…³è”ã€‚è¿™ä¸€è¿›å±•æˆåŠŸå°†è‡ªç„¶è¯­è¨€äº¤äº’ä¸ç”µç£å»ºæ¨¡ (electromagnetic modelling) ç›¸ç»“åˆï¼Œä¸ºæ„å»ºæ›´å…·ç”¨æˆ·å‹å¥½æ€§çš„æ•°æ®é©±åŠ¨çº³ç±³å…‰å­å­¦ (nanophotonics) è®¾è®¡å·¥å…·å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "physics.optics",
        "cs.AI"
      ],
      "primary_category": "physics.optics",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24196v1",
      "published_date": "2025-09-29 02:24:57 UTC",
      "updated_date": "2025-09-29 02:24:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:12:50.561251+00:00"
    },
    {
      "arxiv_id": "2509.24193v1",
      "title": "AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced Self-Play",
      "title_zh": "AceSearcherï¼šé€šè¿‡å¼ºåŒ–è‡ªæˆ‘åšå¼ˆå¼•å¯¼å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†ä¸æœç´¢",
      "authors": [
        "Ran Xu",
        "Yuchen Zhuang",
        "Zihan Dong",
        "Jonathan Wang",
        "Yue Yu",
        "Joyce C. Ho",
        "Linjun Zhang",
        "Haoyu Wang",
        "Wenqi Shi",
        "Carl Yang"
      ],
      "abstract": "Search-augmented LLMs often struggle with complex reasoning tasks due to ineffective multi-hop retrieval and limited reasoning ability. We propose AceSearcher, a cooperative self-play framework that trains a single large language model (LLM) to alternate between two roles: a decomposer that breaks down complex queries and a solver that integrates retrieved contexts for answer generation. AceSearcher couples supervised fine-tuning on a diverse mixture of search, reasoning, and decomposition tasks with reinforcement fine-tuning optimized for final answer accuracy, eliminating the need for intermediate annotations. Extensive experiments on three reasoning-intensive tasks across 10 datasets show that AceSearcher outperforms state-of-the-art baselines, achieving an average exact match improvement of 7.6%. Remarkably, on document-level finance reasoning tasks, AceSearcher-32B matches the performance of the DeepSeek-V3 model using less than 5% of its parameters. Even at smaller scales (1.5B and 8B), AceSearcher often surpasses existing search-augmented LLMs with up to 9x more parameters, highlighting its exceptional efficiency and effectiveness in tackling complex reasoning tasks. Our code will be published at https://github.com/ritaranx/AceSearcher and https://huggingface.co/AceSearcher.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AceSearcherï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºåˆä½œè‡ªåšå¼ˆ (Cooperative Self-Play) çš„æ¡†æ¶ï¼Œæ—¨åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹ (LLMs) å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡çš„èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡è®­ç»ƒå•ä¸ªæ¨¡å‹åœ¨åˆ†è§£å™¨ (Decomposer) å’Œæ±‚è§£å™¨ (Solver) ä¸¤ä¸ªè§’è‰²ä¹‹é—´åˆ‡æ¢ï¼Œåˆ†åˆ«è´Ÿè´£æ‹†è§£å¤æ‚æŸ¥è¯¢å’Œæ•´åˆæ£€ç´¢å†…å®¹ç”Ÿæˆç­”æ¡ˆã€‚AceSearcher ç»“åˆäº†é’ˆå¯¹æ£€ç´¢ã€æ¨ç†å’Œåˆ†è§£ä»»åŠ¡çš„æœ‰ç›‘ç£å¾®è°ƒ (SFT) ä»¥åŠåŸºäºæœ€ç»ˆå‡†ç¡®ç‡ä¼˜åŒ–çš„å¼ºåŒ–å­¦ä¹ å¾®è°ƒï¼Œæœ‰æ•ˆæ¶ˆé™¤äº†å¯¹ä¸­é—´è¿‡ç¨‹æ ‡æ³¨çš„ä¾èµ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAceSearcher åœ¨ 10 ä¸ªæ•°æ®é›†ä¸Šçš„å¹³å‡ç²¾ç¡®åŒ¹é… (Exact Match) æå‡äº† 7.6%ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†ã€‚ç‰¹åˆ«æ˜¯åœ¨é‡‘èæ¨ç†ä»»åŠ¡ä¸­ï¼ŒAceSearcher-32B ä»¥ä¸åˆ° 5% çš„å‚æ•°é‡è¾¾åˆ°äº† DeepSeek-V3 çš„æ€§èƒ½æ°´å¹³ã€‚å³ä½¿åœ¨ 1.5B å’Œ 8B ç­‰è¾ƒå°è§„æ¨¡ä¸‹ï¼Œè¯¥æ¨¡å‹ä¾ç„¶è¶…è¶Šäº†å‚æ•°é‡é«˜å‡º 9 å€çš„ç°æœ‰æ£€ç´¢å¢å¼ºå‹ LLMsï¼Œå±•ç°äº†å…¶åœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æ—¶å“è¶Šçš„æ•ˆç‡ä¸æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to NeurIPS 2025 (Spotlight)",
      "pdf_url": "https://arxiv.org/pdf/2509.24193v1",
      "published_date": "2025-09-29 02:14:30 UTC",
      "updated_date": "2025-09-29 02:14:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:12:54.742346+00:00"
    },
    {
      "arxiv_id": "2509.24192v1",
      "title": "Talk in Pieces, See in Whole: Disentangling and Hierarchical Aggregating Representations for Language-based Object Detection",
      "title_zh": "ç¢è¯­è§£æï¼Œå…¨æ™¯èšåˆï¼šåŸºäºè¯­è¨€ç›®æ ‡æ£€æµ‹çš„è¡¨å¾è§£è€¦ä¸å±‚æ¬¡åŒ–èšåˆ",
      "authors": [
        "Sojung An",
        "Kwanyong Park",
        "Yong Jae Lee",
        "Donghyun Kim"
      ],
      "abstract": "While vision-language models (VLMs) have made significant progress in multimodal perception (e.g., open-vocabulary object detection) with simple language queries, state-of-the-art VLMs still show limited ability to perceive complex queries involving descriptive attributes and relational clauses. Our in-depth analysis shows that these limitations mainly stem from text encoders in VLMs. Such text encoders behave like bags-of-words and fail to separate target objects from their descriptive attributes and relations in complex queries, resulting in frequent false positives. To address this, we propose restructuring linguistic representations according to the hierarchical relations within sentences for language-based object detection. A key insight is the necessity of disentangling textual tokens into core components-objects, attributes, and relations (\"talk in pieces\")-and subsequently aggregating them into hierarchically structured sentence-level representations (\"see in whole\"). Building on this principle, we introduce the TaSe framework with three main contributions: (1) a hierarchical synthetic captioning dataset spanning three tiers from category names to descriptive sentences; (2) Talk in Pieces, the three-component disentanglement module guided by a novel disentanglement loss function, transforms text embeddings into subspace compositions; and (3) See in Whole, which learns to aggregate disentangled components into hierarchically structured embeddings with the guide of proposed hierarchical objectives. The proposed TaSe framework strengthens the inductive bias of hierarchical linguistic structures, resulting in fine-grained multimodal representations for language-based object detection. Experimental results under the OmniLabel benchmark show a 24% performance improvement, demonstrating the importance of linguistic compositionality.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨å¤„ç†åŒ…å«æè¿°æ€§å±æ€§å’Œå…³ç³»ä»å¥çš„å¤æ‚æŸ¥è¯¢æ—¶ï¼Œå› æ–‡æœ¬ç¼–ç å™¨å­˜åœ¨â€œè¯è¢‹â€æ•ˆåº”è€Œéš¾ä»¥åŒºåˆ†ç›®æ ‡ç‰©ä½“åŠå…¶å±æ€§çš„é—®é¢˜ï¼Œæå‡ºäº†TaSeæ¡†æ¶ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒç†å¿µæ˜¯å°†æ–‡æœ¬è¯å…ƒè§£è€¦ä¸ºç‰©ä½“ã€å±æ€§å’Œå…³ç³»ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼ˆå³\"Talk in Pieces\"ï¼‰ï¼Œéšåæ ¹æ®å¥å­çš„å±‚çº§é€»è¾‘å°†å…¶é‡æ–°èšåˆä¸ºå¥å­çº§è¡¨ç¤ºï¼ˆå³\"See in Whole\"ï¼‰ã€‚TaSeçš„ä¸»è¦è´¡çŒ®åŒ…æ‹¬å»ºç«‹äº†ä¸€ä¸ªä¸‰å±‚çº§çš„å±‚çº§åˆæˆæ ‡æ³¨æ•°æ®é›†ï¼Œå¼€å‘äº†åŸºäºæ–°å‹è§£è€¦æŸå¤±å‡½æ•°çš„å­ç©ºé—´ç»„åˆæ¨¡å—ï¼Œä»¥åŠç”¨äºå­¦ä¹ å±‚çº§åµŒå…¥çš„èšåˆæ¨¡å—ã€‚è¯¥æ–¹æ³•å¢å¼ºäº†æ¨¡å‹å¯¹å±‚çº§è¯­è¨€ç»“æ„çš„å½’çº³åç½®(Inductive Bias)ï¼Œä»è€Œä¼˜åŒ–äº†å¤šæ¨¡æ€ç›®æ ‡æ£€æµ‹çš„ç»†ç²’åº¦è¡¨ç¤ºã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTaSeåœ¨OmniLabelåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†24%çš„æ€§èƒ½æå‡ï¼Œå……åˆ†éªŒè¯äº†è¯­è¨€ç»„åˆæ€§(Linguistic Compositionality)åœ¨å¤æ‚åœºæ™¯ä¸‹çš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "23 pages, 17 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.24192v1",
      "published_date": "2025-09-29 02:14:26 UTC",
      "updated_date": "2025-09-29 02:14:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:13:01.252597+00:00"
    },
    {
      "arxiv_id": "2509.24186v1",
      "title": "Beyond Overall Accuracy: A Psychometric Deep Dive into the Topic-Specific Medical Capabilities of 80 Large Language Models",
      "title_zh": "è¶…è¶Šæ•´ä½“å‡†ç¡®ç‡ï¼šåŸºäºå¿ƒç†æµ‹é‡å­¦å¯¹ 80 ä¸ªå¤§è¯­è¨€æ¨¡å‹ç‰¹å®šé¢†åŸŸåŒ»å­¦èƒ½åŠ›çš„æ·±åº¦å‰–æ",
      "authors": [
        "Zhimeng Luo",
        "Lixin Wu",
        "Adam Frisch",
        "Daqing He"
      ],
      "abstract": "As Large Language Models (LLMs) are increasingly proposed for high-stakes medical applications, there has emerged a critical need for reliable and accurate evaluation methodologies. Traditional accuracy metrics fail inadequately as they neither capture question characteristics nor offer topic-specific insights. To address this gap, we introduce \\textsc{MedIRT}, a rigorous evaluation framework grounded in Item Response Theory (IRT), the gold standard in high-stakes educational testing. Unlike previous research relying on archival data, we prospectively gathered fresh responses from 80 diverse LLMs on a balanced, 1,100-question USMLE-aligned benchmark. Using one unidimensional two-parameter logistic IRT model per topic, we estimate LLM's latent model ability jointly with question difficulty and discrimination, yielding more stable and nuanced performance rankings than accuracy alone. Notably, we identify distinctive ``spiky'' ability profiles, where overall rankings can be misleading due to highly specialized model abilities. While \\texttt{GPT-5} was the top performer in a majority of domains (8 of 11), it was outperformed in Social Science and Communication by \\texttt{Claude-3-opus}, demonstrating that even an overall 23rd-ranked model can hold the top spot for specific competencies. Furthermore, we demonstrate IRT's utility in auditing benchmarks by identifying flawed questions. We synthesize these findings into a practical decision-support framework that integrates our multi-factor competency profiles with operational metrics. This work establishes a robust, psychometrically grounded methodology essential for the safe, effective, and trustworthy deployment of LLMs in healthcare.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨åŒ»ç–—åº”ç”¨ä¸­ä¼ ç»Ÿå‡†ç¡®ç‡æŒ‡æ ‡çš„å±€é™æ€§ï¼Œæå‡ºäº†åŸºäºé¡¹ç›®ååº”ç†è®º (Item Response Theory, IRT) çš„è¯„ä¼°æ¡†æ¶ MedIRTã€‚é€šè¿‡åœ¨åŒ…å« 1,100 ä¸ªé—®é¢˜çš„ USMLE å¯¹é½åŸºå‡†æµ‹è¯•ä¸Šå¯¹ 80 ä¸ªä¸åŒçš„ LLMs è¿›è¡Œå‰ç»æ€§æµ‹è¯•ï¼Œç ”ç©¶è€…èƒ½å¤ŸåŒæ—¶ä¼°ç®—æ¨¡å‹çš„æ½œåœ¨èƒ½åŠ›ã€é—®é¢˜éš¾åº¦å’ŒåŒºåˆ†åº¦ï¼Œä»è€Œè·å¾—æ¯”å•ä¸€å‡†ç¡®ç‡æ›´ç¨³å®šä¸”ç»†è‡´çš„æ€§èƒ½æ’åã€‚å®éªŒå‘ç° LLMs å‘ˆç°å‡ºç‹¬ç‰¹çš„â€œå°–å³°çŠ¶â€(spiky) èƒ½åŠ›å›¾è°±ï¼Œè¡¨æ˜ç»¼åˆæ’åå¾€å¾€ä¼šæ©ç›–æ¨¡å‹åœ¨ç‰¹å®šä¸»é¢˜ä¸Šçš„ä¸“ä¸šåŒ–èƒ½åŠ›ã€‚ç»“æœæ˜¾ç¤º GPT-5 è™½ç„¶åœ¨å¤šæ•°é¢†åŸŸè¡¨ç°é¡¶å°–ï¼Œä½†åœ¨ç¤¾ä¼šç§‘å­¦å’Œæ²Ÿé€šæ–¹é¢è¢« Claude-3-opus è¶…è¶Šï¼Œè¿™å¼ºè°ƒäº†ç‰¹å®šèƒœä»»åŠ›è¯„ä¼°çš„é‡è¦æ€§ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶åˆ©ç”¨ IRT æœ‰æ•ˆè¯†åˆ«å¹¶å®¡è®¡äº†åŸºå‡†æµ‹è¯•ä¸­çš„ç¼ºé™·é—®é¢˜ï¼Œå¹¶æ•´åˆå¤šå› ç´ èƒ½åŠ›æŒ‡æ ‡æ„å»ºäº†å®ç”¨çš„å†³ç­–æ”¯æŒæ¡†æ¶ã€‚è¿™é¡¹å·¥ä½œä¸ºåŒ»ç–—å¥åº·é¢†åŸŸä¸­ LLMs çš„å®‰å…¨ã€æœ‰æ•ˆåŠå¯ä¿¡éƒ¨ç½²æä¾›äº†ä¸¥è°¨çš„å¿ƒç†æµ‹é‡å­¦æ–¹æ³•è®ºæ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24186v1",
      "published_date": "2025-09-29 02:06:13 UTC",
      "updated_date": "2025-09-29 02:06:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:12:59.664985+00:00"
    },
    {
      "arxiv_id": "2509.24183v1",
      "title": "Retrieval-augmented GUI Agents with Generative Guidelines",
      "title_zh": "åŸºäºç”Ÿæˆå¼æŒ‡å—çš„æ£€ç´¢å¢å¼ºå‹ GUI æ™ºèƒ½ä½“",
      "authors": [
        "Ran Xu",
        "Kaixin Ma",
        "Wenhao Yu",
        "Hongming Zhang",
        "Joyce C. Ho",
        "Carl Yang",
        "Dong Yu"
      ],
      "abstract": "GUI agents powered by vision-language models (VLMs) show promise in automating complex digital tasks. However, their effectiveness in real-world applications is often limited by scarce training data and the inherent complexity of these tasks, which frequently require long-tailed knowledge covering rare, unseen scenarios. We propose RAG-GUI , a lightweight VLM that leverages web tutorials at inference time. RAG-GUI is first warm-started via supervised finetuning (SFT) and further refined through self-guided rejection sampling finetuning (RSF). Designed to be model-agnostic, RAG-GUI functions as a generic plug-in that enhances any VLM-based agent. Evaluated across three distinct tasks, it consistently outperforms baseline agents and surpasses other inference baselines by 2.6% to 13.3% across two model sizes, demonstrating strong generalization and practical plug-and-play capabilities in real-world scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† RAG-GUIï¼Œä¸€ç§åœ¨æ¨ç†é˜¶æ®µåˆ©ç”¨ç½‘é¡µæ•™ç¨‹ (web tutorials) çš„è½»é‡çº§è§†è§‰è¯­è¨€æ¨¡å‹ (VLM)ï¼Œæ—¨åœ¨è§£å†³ GUI æ™ºèƒ½ä½“åœ¨å¤„ç†ç°å®ä¸–ç•Œå¤æ‚ä»»åŠ¡æ—¶é¢ä¸´çš„è®­ç»ƒæ•°æ®ç¨€ç¼ºåŠé•¿å°¾çŸ¥è¯†ä¸è¶³ç­‰éš¾é¢˜ã€‚è¯¥æ¨¡å‹é¦–å…ˆé€šè¿‡æœ‰ç›‘ç£å¾®è°ƒ (SFT) è¿›è¡Œçƒ­å¯åŠ¨ï¼Œå¹¶è¿›ä¸€æ­¥ç»“åˆè‡ªå¼•å¯¼çš„æ‹’ç»é‡‡æ ·å¾®è°ƒ (RSF) è¿›è¡Œä¼˜åŒ–ã€‚ä½œä¸ºä¸€ç§æ¨¡å‹æ— å…³ (model-agnostic) çš„é€šç”¨æ’ä»¶ï¼ŒRAG-GUI èƒ½å¤Ÿæœ‰æ•ˆå¢å¼ºä»»ä½•åŸºäº VLM çš„æ™ºèƒ½ä½“æ€§èƒ½ï¼Œå…·å¤‡æå¼ºçš„å³æ’å³ç”¨èƒ½åŠ›ã€‚åœ¨ä¸‰é¡¹ä¸åŒä»»åŠ¡çš„è¯„ä¼°ä¸­ï¼ŒRAG-GUI åœ¨ä¸åŒæ¨¡å‹è§„æ¨¡ä¸‹å‡ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œç›¸æ¯”å…¶ä»–æ¨ç†åŸºçº¿å–å¾—äº† 2.6% è‡³ 13.3% çš„æ€§èƒ½æå‡ã€‚å®éªŒç»“æœå……åˆ†è¯æ˜äº† RAG-GUI åœ¨ç°å®åº”ç”¨åœºæ™¯ä¸­å…·æœ‰å“è¶Šçš„æ³›åŒ–èƒ½åŠ›å’Œå¹¿æ³›çš„å®é™…åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to EMNLP 2025 (Main Conference)",
      "pdf_url": "https://arxiv.org/pdf/2509.24183v1",
      "published_date": "2025-09-29 02:04:20 UTC",
      "updated_date": "2025-09-29 02:04:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:13:10.760294+00:00"
    },
    {
      "arxiv_id": "2509.24166v1",
      "title": "Stable Forgetting: Bounded Parameter-Efficient Unlearning in LLMs",
      "title_zh": "ç¨³å®šé—å¿˜ï¼šå¤§è¯­è¨€æ¨¡å‹ä¸­çš„æœ‰ç•Œå‚æ•°é«˜æ•ˆé—å¿˜",
      "authors": [
        "Arpit Garg",
        "Hemanth Saratchandran",
        "Ravi Garg",
        "Simon Lucey"
      ],
      "abstract": "Machine unlearning in large language models (LLMs) is essential for privacy and safety; however, existing approaches remain unstable and unreliable. A widely used strategy, the gradient difference method, applies gradient descent on retained data while performing gradient ascent on forget data, the data whose influence should be removed. However, when combined with cross-entropy loss, this procedure causes unbounded growth of weights and gradients, leading to training instability and degrading both forgetting and retention. We provide a theoretical framework that explains this failure, explicitly showing how ascent on the forget set destabilizes optimization in the feedforward MLP layers of LLMs. Guided by this insight, we propose Bounded Parameter-Efficient Unlearning, a parameter-efficient approach that stabilizes LoRA-based fine-tuning by applying bounded functions to MLP adapters. This simple modification controls the weight dynamics during ascent, enabling the gradient difference method to converge reliably. Across the TOFU, TDEC, and MUSE benchmarks, and across architectures and scales from 125M to 8B parameters, our method achieves substantial improvements in forgetting while preserving retention, establishing a novel theoretically grounded and practically scalable framework for unlearning in LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)ä¸­æœºå™¨é—å¿˜(Machine Unlearning)ç°æœ‰çš„ä¸ç¨³å®šæ€§é—®é¢˜è¿›è¡Œäº†æ¢è®¨ã€‚ä½œè€…æŒ‡å‡ºï¼Œå¹¿æ³›ä½¿ç”¨çš„æ¢¯åº¦å·®åˆ†æ–¹æ³•(gradient difference method)åœ¨ç»“åˆäº¤å‰ç†µæŸå¤±æ—¶ï¼Œä¼šå¯¼è‡´æƒé‡å’Œæ¢¯åº¦çš„æ— ç•Œå¢é•¿ï¼Œä»è€Œç ´åè®­ç»ƒç¨³å®šæ€§å¹¶é™ä½é—å¿˜ä¸ä¿ç•™æ•°æ®çš„æ€§èƒ½ã€‚ç ”ç©¶æä¾›äº†ä¸€ä¸ªç†è®ºæ¡†æ¶ï¼Œè§£é‡Šäº†åœ¨é—å¿˜é›†ä¸Šè¿›è¡Œæ¢¯åº¦ä¸Šå‡(gradient ascent)å¦‚ä½•ç ´åLLMså‰é¦ˆMLPå±‚çš„ä¼˜åŒ–ç¨³å®šæ€§ã€‚åŸºäºæ­¤è§è§£ï¼Œä½œè€…æå‡ºäº†â€œæœ‰ç•Œå‚æ•°é«˜æ•ˆé—å¿˜â€(Bounded Parameter-Efficient Unlearning)æ–¹æ³•ï¼Œé€šè¿‡å¯¹MLPé€‚é…å™¨åº”ç”¨æœ‰ç•Œå‡½æ•°æ¥ç¨³å®šåŸºäºLoRAçš„å¾®è°ƒã€‚è¿™ç§ç®€å•çš„ä¿®æ”¹æ§åˆ¶äº†ä¸Šå‡è¿‡ç¨‹ä¸­çš„æƒé‡åŠ¨æ€ï¼Œä½¿æ¢¯åº¦å·®åˆ†æ–¹æ³•èƒ½å¤Ÿå¯é æ”¶æ•›ã€‚åœ¨TOFUã€TDECå’ŒMUSEåŸºå‡†æµ‹è¯•ä»¥åŠä¸åŒè§„æ¨¡ï¼ˆ125Måˆ°8Bå‚æ•°ï¼‰çš„æ¶æ„ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒæ¨¡å‹ä¿ç•™èƒ½åŠ›çš„åŒæ—¶æ˜¾è‘—æé«˜äº†é—å¿˜æ•ˆæœï¼Œä¸ºLLMsé—å¿˜å»ºç«‹äº†ä¸€ä¸ªç†è®ºæ‰å®ä¸”å¯æ‰©å±•çš„æ¡†æ¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "In Submission",
      "pdf_url": "https://arxiv.org/pdf/2509.24166v1",
      "published_date": "2025-09-29 01:30:15 UTC",
      "updated_date": "2025-09-29 01:30:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 2,
      "last_update": "2026-01-24T23:14:16.695712+00:00"
    },
    {
      "arxiv_id": "2509.24165v1",
      "title": "LatXGen: Towards Radiation-Free and Accurate Quantitative Analysis of Sagittal Spinal Alignment Via Cross-Modal Radiographic View Synthesis",
      "title_zh": "LatXGenï¼šé€šè¿‡è·¨æ¨¡æ€æ”¾å°„å½±åƒè§†å›¾åˆæˆå®ç°æ— è¾å°„ä¸”ç²¾å‡†çš„è„ŠæŸ±çŸ¢çŠ¶ä½åŠ›çº¿å®šé‡åˆ†æ",
      "authors": [
        "Moxin Zhao",
        "Nan Meng",
        "Jason Pui Yin Cheung",
        "Chris Yuk Kwan Tang",
        "Chenxi Yu",
        "Wenting Zhong",
        "Pengyu Lu",
        "Chang Shi",
        "Yipeng Zhuang",
        "Teng Zhang"
      ],
      "abstract": "Adolescent Idiopathic Scoliosis (AIS) is a complex three-dimensional spinal deformity, and accurate morphological assessment requires evaluating both coronal and sagittal alignment. While previous research has made significant progress in developing radiation-free methods for coronal plane assessment, reliable and accurate evaluation of sagittal alignment without ionizing radiation remains largely underexplored. To address this gap, we propose LatXGen, a novel generative framework that synthesizes realistic lateral spinal radiographs from posterior Red-Green-Blue and Depth (RGBD) images of unclothed backs. This enables accurate, radiation-free estimation of sagittal spinal alignment. LatXGen tackles two core challenges: (1) inferring sagittal spinal morphology changes from a lateral perspective based on posteroanterior surface geometry, and (2) performing cross-modality translation from RGBD input to the radiographic domain. The framework adopts a dual-stage architecture that progressively estimates lateral spinal structure and synthesizes corresponding radiographs. To enhance anatomical consistency, we introduce an attention-based Fast Fourier Convolution (FFC) module for integrating anatomical features from RGBD images and 3D landmarks, and a Spatial Deformation Network (SDN) to model morphological variations in the lateral view. Additionally, we construct the first large-scale paired dataset for this task, comprising 3,264 RGBD and lateral radiograph pairs. Experimental results demonstrate that LatXGen produces anatomically accurate radiographs and outperforms existing GAN-based methods in both visual fidelity and quantitative metrics. This study offers a promising, radiation-free solution for sagittal spine assessment and advances comprehensive AIS evaluation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é’å°‘å¹´ç‰¹å‘æ€§è„ŠæŸ±ä¾§å¼¯ (AIS) è¯„ä¼°ä¸­çŸ¢çŠ¶é¢å¯¹é½ (Sagittal Alignment) ç¼ºä¹å‡†ç¡®æ— è¾å°„æ‰‹æ®µçš„é—®é¢˜ï¼Œæå‡ºäº† LatXGen ç”Ÿæˆå¼æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨äººä½“èƒŒéƒ¨çš„åä½çº¢ç»¿è“æ·±åº¦ (RGBD) å›¾åƒåˆæˆçœŸå®çš„ä¾§ä½è„ŠæŸ±æ”¾å°„å½±åƒ (Lateral Radiographs)ï¼Œä»è€Œå®ç°æ— è¾å°„çš„å®šé‡åˆ†æã€‚LatXGen é‡‡ç”¨åŒé˜¶æ®µæ¶æ„ï¼Œé€šè¿‡åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„å¿«é€Ÿå‚…é‡Œå¶å·ç§¯ (FFC) æ¨¡å—æ•´åˆè§£å‰–ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨ç©ºé—´å˜å½¢ç½‘ç»œ (SDN) æ¨¡æ‹Ÿä¾§ä½è§†å›¾çš„å½¢æ€å˜åŒ–ã€‚ç ”ç©¶å›¢é˜Ÿè¿˜æ„å»ºäº†åŒ…å« 3,264 å¯¹å›¾åƒçš„é¦–ä¸ªå¤§è§„æ¨¡é…å¯¹æ•°æ®é›†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLatXGen åœ¨è§†è§‰ä¿çœŸåº¦å’Œå®šé‡æŒ‡æ ‡ä¸Šå‡ä¼˜äºä¼ ç»Ÿçš„ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (GAN) æ–¹æ³•ï¼Œç”Ÿæˆçš„å½±åƒå…·æœ‰é«˜åº¦çš„è§£å‰–å‡†ç¡®æ€§ã€‚è¯¥æˆæœä¸ºä¸´åºŠè„ŠæŸ±è¯„ä¼°æä¾›äº†ä¸€ç§å®‰å…¨ã€ç²¾å‡†çš„æ— è¾å°„æ›¿ä»£æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.24165v1",
      "published_date": "2025-09-29 01:29:53 UTC",
      "updated_date": "2025-09-29 01:29:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:13:07.972552+00:00"
    },
    {
      "arxiv_id": "2509.24160v1",
      "title": "Memory Transfer Planning: LLM-driven Context-Aware Code Adaptation for Robot Manipulation",
      "title_zh": "è®°å¿†è¿ç§»è§„åˆ’ï¼šé¢å‘æœºå™¨äººæ“çºµçš„å¤§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥ä»£ç é€‚é…",
      "authors": [
        "Tomoyuki Kagaya",
        "Subramanian Lakshmi",
        "Yuxuan Lou",
        "Thong Jing Yuan",
        "Jayashree Karlekar",
        "Sugiri Pranata",
        "Natsuki Murakami",
        "Akira Kinose",
        "Yang You"
      ],
      "abstract": "Large language models (LLMs) are increasingly explored in robot manipulation, but many existing methods struggle to adapt to new environments. Many systems require either environment-specific policy training or depend on fixed prompts and single-shot code generation, leading to limited transferability and manual re-tuning. We introduce Memory Transfer Planning (MTP), a framework that leverages successful control-code examples from different environments as procedural knowledge, using them as in-context guidance for LLM-driven planning. Specifically, MTP (i) generates an initial plan and code using LLMs, (ii) retrieves relevant successful examples from a code memory, and (iii) contextually adapts the retrieved code to the target setting for re-planning without updating model parameters. We evaluate MTP on RLBench, CALVIN, and a physical robot, demonstrating effectiveness beyond simulation. Across these settings, MTP consistently improved success rate and adaptability compared with fixed-prompt code generation, naive retrieval, and memory-free re-planning. Furthermore, in hardware experiments, leveraging a memory constructed in simulation proved effective. MTP provides a practical approach that exploits procedural knowledge to realize robust LLM-based planning across diverse robotic manipulation scenarios, enhancing adaptability to novel environments and bridging simulation and real-world deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æœºå™¨äººæ“ä½œä¸­é¢ä¸´çš„ç¯å¢ƒé€‚åº”æ€§å¼±ã€ä¾èµ–å›ºå®šæç¤ºè¯åŠé¢‘ç¹æ‰‹åŠ¨è°ƒæ•´ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†Memory Transfer Planning (MTP) æ¡†æ¶ã€‚MTP å°†ä¸åŒç¯å¢ƒä¸‹çš„æˆåŠŸæ§åˆ¶ä»£ç ç¤ºä¾‹è§†ä¸ºç¨‹åºæ€§çŸ¥è¯†ï¼Œåˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ (In-context learning)æŒ‡å¯¼ LLM è¿›è¡Œè§„åˆ’ã€‚å…·ä½“æµç¨‹åŒ…æ‹¬ç”Ÿæˆåˆå§‹è®¡åˆ’ã€ä»ä»£ç å­˜å‚¨å™¨(Code memory)ä¸­æ£€ç´¢ç›¸å…³æ¡ˆä¾‹ï¼Œä»¥åŠé’ˆå¯¹ç›®æ ‡ç¯å¢ƒè¿›è¡Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ä»£ç é€‚é…ä¸é‡è§„åˆ’ã€‚è¯¥æ–¹æ³•æ— éœ€æ›´æ–°æ¨¡å‹å‚æ•°ï¼Œåœ¨ RLBenchã€CALVIN åŠå®ä½“æœºå™¨äººå®éªŒä¸­ï¼Œå…¶æˆåŠŸç‡å’Œé€‚åº”æ€§æ˜¾è‘—ä¼˜äºå›ºå®šæç¤ºè¯ç”Ÿæˆå’Œæ— è®°å¿†é‡è§„åˆ’ç­‰åŸºçº¿æ–¹æ³•ã€‚æ­¤å¤–ï¼Œå®éªŒè¯æ˜äº†ä»ä»¿çœŸä¸­ç§¯ç´¯çš„å­˜å‚¨å™¨èƒ½æœ‰æ•ˆåº”ç”¨äºçœŸå®ç¡¬ä»¶ï¼Œä¸ºå®ç°è·¨åœºæ™¯çš„é²æ£’æœºå™¨äººè§„åˆ’åŠå¼¥åˆä»¿çœŸä¸ç°å®(Sim-to-real)å·®è·æä¾›äº†å®ç”¨æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24160v1",
      "published_date": "2025-09-29 01:18:59 UTC",
      "updated_date": "2025-09-29 01:18:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:13:13.363663+00:00"
    },
    {
      "arxiv_id": "2509.24159v3",
      "title": "RE-PO: Robust Enhanced Policy Optimization as a General Framework for LLM Alignment",
      "title_zh": "RE-POï¼šå¤§è¯­è¨€æ¨¡å‹å¯¹é½çš„é²æ£’å¢å¼ºå‹ç­–ç•¥ä¼˜åŒ–é€šç”¨æ¡†æ¶",
      "authors": [
        "Xiaoyang Cao",
        "Zelai Xu",
        "Mo Guang",
        "Kaiwen Long",
        "Michiel A. Bakker",
        "Yu Wang",
        "Chao Yu"
      ],
      "abstract": "Standard human preference-based alignment methods, such as Reinforcement Learning from Human Feedback (RLHF), are a cornerstone for aligning large language models (LLMs) with human values. However, these methods typically assume that preference data is clean and that all labels are equally reliable. In practice, large-scale preference datasets contain substantial noise due to annotator mistakes, inconsistent instructions, varying expertise, and even adversarial or low-effort feedback. This mismatch between recorded labels and ground-truth preferences can misguide training and degrade model performance. To address this issue, we introduce Robust Enhanced Policy Optimization (RE-PO), which uses an expectation-maximization procedure to infer the posterior correctness of each label and then adaptively reweight data points in the training loss to mitigate label noise. We further generalize this idea by establishing a theoretical link between arbitrary preference losses and their underlying probabilistic models, enabling a systematic transformation of existing alignment algorithms into robust counterparts and elevating RE-PO from a single method to a general framework for robust preference alignment. Theoretically, we prove that, under a perfectly calibrated model, RE-PO recovers the true noise level of the dataset. Empirically, we show that RE-PO consistently improves four state-of-the-art alignment methods (DPO, IPO, SimPO, and CPO); when applied to Mistral and Llama 3 models, the RE-PO-enhanced variants increase AlpacaEval 2 win rates by up to 7.0 percent over their respective baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)å¯¹é½ä¸­æ ‡å‡†åå¥½å­¦ä¹ æ–¹æ³•(å¦‚RLHF)å¿½è§†åå¥½æ•°æ®å™ªå£°çš„é—®é¢˜ï¼Œæå‡ºäº†RE-PO (Robust Enhanced Policy Optimization)æ¡†æ¶ã€‚RE-POé€šè¿‡æœŸæœ›æœ€å¤§åŒ–(Expectation-Maximization)è¿‡ç¨‹æ¨æ–­æ¯ä¸ªæ ‡ç­¾çš„åéªŒæ­£ç¡®æ€§ï¼Œå¹¶æ ¹æ®æ¨æ–­ç»“æœè‡ªé€‚åº”åœ°é‡æ–°åˆ†é…è®­ç»ƒæŸå¤±ä¸­çš„æƒé‡ï¼Œä»è€Œæœ‰æ•ˆå‡è½»æ ‡ç­¾å™ªå£°çš„è´Ÿé¢å½±å“ã€‚ç ”ç©¶è€…è¿›ä¸€æ­¥å»ºç«‹äº†ä»»æ„åå¥½æŸå¤±ä¸å…¶åº•å±‚æ¦‚ç‡æ¨¡å‹ä¹‹é—´çš„ç†è®ºè”ç³»ï¼Œä½¿è¯¥æ–¹æ³•ä»å•ä¸€ç®—æ³•æå‡ä¸ºé€šç”¨æ¡†æ¶ï¼Œèƒ½å¤Ÿå°†DPOã€IPOã€SimPOå’ŒCPOç­‰ç°æœ‰å¯¹é½ç®—æ³•è½¬åŒ–ä¸ºæ›´å…·é²æ£’æ€§çš„ç‰ˆæœ¬ã€‚ç†è®ºè¯æ˜æ˜¾ç¤ºï¼Œåœ¨æ¨¡å‹å®Œç¾æ ¡å‡†çš„æƒ…å†µä¸‹ï¼ŒRE-POèƒ½å¤Ÿæ¢å¤æ•°æ®é›†çš„çœŸå®å™ªå£°æ°´å¹³ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRE-POåœ¨Mistralå’ŒLlama 3æ¨¡å‹ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå…¶AlpacaEval 2èƒœç‡ç›¸æ¯”å„åŸºå‡†æ¨¡å‹æœ€é«˜æå‡äº†7.0%ï¼Œè¯æ˜äº†è¯¥æ¡†æ¶åœ¨å¤„ç†å¤§è§„æ¨¡å™ªå£°åå¥½æ•°æ®æ—¶çš„æ˜¾è‘—ä¼˜åŠ¿å’Œæ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24159v3",
      "published_date": "2025-09-29 01:17:49 UTC",
      "updated_date": "2025-12-05 19:59:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:13:15.358169+00:00"
    },
    {
      "arxiv_id": "2509.24156v1",
      "title": "Reasoning or Retrieval? A Study of Answer Attribution on Large Reasoning Models",
      "title_zh": "æ¨ç†è¿˜æ˜¯æ£€ç´¢ï¼Ÿå¤§è§„æ¨¡æ¨ç†æ¨¡å‹ç­”æ¡ˆå½’å› ç ”ç©¶",
      "authors": [
        "Yuhui Wang",
        "Changjiang Li",
        "Guangke Chen",
        "Jiacheng Liang",
        "Ting Wang"
      ],
      "abstract": "Large reasoning models (LRMs) exhibit unprecedented capabilities in solving complex problems through Chain-of-Thought (CoT) reasoning. However, recent studies reveal that their final answers often contradict their own reasoning traces. We hypothesize that this inconsistency stems from two competing mechanisms for generating answers: CoT reasoning and memory retrieval. To test this hypothesis, we conduct controlled experiments that challenge LRMs with misleading cues during reasoning and/or corrupted answers during retrieval. Our results across models and datasets confirm that both mechanisms operate simultaneously, with their relative dominance influenced by multiple factors: problem domains, model scales, and fine-tuning approaches (e.g., reinforcement learning vs. distillation). The findings reveal a critical limitation in current reasoning fine-tuning paradigms: models can exploit the retrieval mechanism as a shortcut, effectively \"hacking\" the reward signal and undermining genuine reasoning development. To address this challenge, we introduce FARL, a novel fine-tuning framework that integrates memory unlearning with reinforcement learning. By carefully suppressing retrieval shortcuts during the fine-tuning process, FARL promotes reasoning-dominant behavior and enhances generalizable reasoning capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§æ¨ç†æ¨¡å‹(Large Reasoning Models, LRMs)åœ¨è§£å†³å¤æ‚é—®é¢˜æ—¶ï¼Œæœ€ç»ˆç­”æ¡ˆä¸é“¾å¼æ€ç»´(Chain-of-Thought, CoT)æ¨ç†è¿‡ç¨‹ä¸ä¸€è‡´çš„çŸ›ç›¾ç°è±¡ã€‚ä½œè€…æå‡ºè¿™ä¸€ä¸ä¸€è‡´æ€§æºäºCoTæ¨ç†ä¸è®°å¿†æ£€ç´¢(Memory Retrieval)ä¸¤ç§ç«äº‰æœºåˆ¶ï¼Œå¹¶é€šè¿‡å—æ§å®éªŒè¯å®äº†è¿™ä¸¤ç§æœºåˆ¶çš„å…±åŒä½œç”¨åŠå…¶å—æ¨¡å‹è§„æ¨¡å’Œå¾®è°ƒæ–¹æ³•çš„å½±å“ã€‚ç ”ç©¶å‘ç°æ¨¡å‹å¸¸åˆ©ç”¨æ£€ç´¢æœºåˆ¶ä½œä¸ºæ·å¾„æ¥â€œä¾µå…¥â€å¥–åŠ±ä¿¡å·ï¼Œä»è€Œå‰Šå¼±äº†çœŸå®çš„æ¨ç†èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œè¯¥ç ”ç©¶æå‡ºäº†FARLæ¡†æ¶ï¼Œé€šè¿‡å°†è®°å¿†å¸è½½(Memory Unlearning)ä¸å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)ç›¸ç»“åˆæ¥æŠ‘åˆ¶æ£€ç´¢æ·å¾„ã€‚å®éªŒè¯æ˜ï¼ŒFARLèƒ½æ˜¾è‘—æå‡æ¨¡å‹ä»¥æ¨ç†ä¸ºä¸»å¯¼çš„è¡Œä¸ºï¼Œå¹¶å¢å¼ºå…¶åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„é€šç”¨æ¨ç†æ€§èƒ½ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24156v1",
      "published_date": "2025-09-29 01:13:33 UTC",
      "updated_date": "2025-09-29 01:13:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:13:25.164700+00:00"
    },
    {
      "arxiv_id": "2511.07420v3",
      "title": "Advancing mathematics research with generative AI",
      "title_zh": "åˆ©ç”¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ¨åŠ¨æ•°å­¦ç ”ç©¶",
      "authors": [
        "Lisa Carbone"
      ],
      "abstract": "The main drawback of using generative AI models for advanced mathematics is that these models are not primarily logical reasoning engines. However, Large Language Models, and their refinements, can pick up on patterns in higher mathematics that are difficult for humans to see. By putting the design of generative AI models to their advantage, mathematicians may use them as powerful interactive assistants that can carry out laborious tasks, generate and debug code, check examples, formulate conjectures and more. We discuss how generative AI models can be used to advance mathematics research. We also discuss their integration with neuro-symbolic solvers, Computer Algebra Systems and formal proof assistants such as Lean.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½(Generative AI)åœ¨æ¨è¿›æ•°å­¦ç ”ç©¶ä¸­çš„åº”ç”¨ï¼ŒæŒ‡å‡ºå°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹(Large Language Models)å¹¶éåŸç”Ÿçš„é€»è¾‘æ¨ç†å¼•æ“ï¼Œä½†å…¶åœ¨æ•æ‰é«˜ç­‰æ•°å­¦å¤æ‚æ¨¡å¼æ–¹é¢å…·æœ‰ç‹¬ç‰¹ä¼˜åŠ¿ã€‚æ•°å­¦å®¶å¯ä»¥å°†è¿™äº›æ¨¡å‹ä½œä¸ºå¼ºå¤§çš„äº¤äº’å¼åŠ©æ‰‹ï¼Œç”¨äºæ‰§è¡Œç¹é‡ä»»åŠ¡ã€ç”Ÿæˆä¸è°ƒè¯•ä»£ç ã€éªŒè¯ç¤ºä¾‹ä»¥åŠæå‡ºçŒœæƒ³(conjectures)ã€‚æ–‡ç« é‡ç‚¹è®¨è®ºäº†å¦‚ä½•é€šè¿‡æ•´åˆç¥ç»ç¬¦å·æ±‚è§£å™¨(neuro-symbolic solvers)ã€è®¡ç®—æœºä»£æ•°ç³»ç»Ÿ(Computer Algebra Systems)ä»¥åŠLeanç­‰æ­£å¼è¯æ˜åŠ©æ‰‹(formal proof assistants)æ¥å¢å¼ºç ”ç©¶èƒ½åŠ›ã€‚è¿™ç§åä½œæ¨¡å¼å……åˆ†åˆ©ç”¨äº†ç”Ÿæˆå¼æ¨¡å‹çš„è®¾è®¡ç‰¹æ€§ï¼Œä¸ºæå‡æ•°å­¦å‘ç°çš„æ•ˆç‡å’Œæ·±åº¦æä¾›äº†æ–°çš„è·¯å¾„ã€‚",
      "categories": [
        "math.HO",
        "cs.AI",
        "cs.HC",
        "math.GR",
        "math.LO"
      ],
      "primary_category": "math.HO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.07420v3",
      "published_date": "2025-09-29 01:04:44 UTC",
      "updated_date": "2025-12-18 04:08:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:15:34.094147+00:00"
    },
    {
      "arxiv_id": "2509.24149v1",
      "title": "Accelerating Cerebral Diagnostics with BrainFusion: A Comprehensive MRI Tumor Framework",
      "title_zh": "BrainFusionï¼šåŠ é€Ÿè„‘éƒ¨è¯Šæ–­çš„ç»¼åˆæ€§ MRI è‚¿ç˜¤æ¡†æ¶",
      "authors": [
        "Walid Houmaidi",
        "Youssef Sabiri",
        "Salmane El Mansour Billah",
        "Amine Abouaomar"
      ],
      "abstract": "The early and accurate classification of brain tumors is crucial for guiding effective treatment strategies and improving patient outcomes. This study presents BrainFusion, a significant advancement in brain tumor analysis using magnetic resonance imaging (MRI) by combining fine-tuned convolutional neural networks (CNNs) for tumor classification--including VGG16, ResNet50, and Xception--with YOLOv8 for precise tumor localization with bounding boxes. Leveraging the Brain Tumor MRI Dataset, our experiments reveal that the fine-tuned VGG16 model achieves test accuracy of 99.86%, substantially exceeding previous benchmarks. Beyond setting a new accuracy standard, the integration of bounding-box localization and explainable AI techniques further enhances both the clinical interpretability and trustworthiness of the system's outputs. Overall, this approach underscores the transformative potential of deep learning in delivering faster, more reliable diagnoses, ultimately contributing to improved patient care and survival rates.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†BrainFusionï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºç£å…±æŒ¯æˆåƒ(MRI)è„‘è‚¿ç˜¤åˆ†æçš„ç»¼åˆæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ·±åº¦å­¦ä¹ æå‡è¯Šæ–­çš„é€Ÿåº¦ä¸å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•ç»“åˆäº†ç»è¿‡å¾®è°ƒçš„å·ç§¯ç¥ç»ç½‘ç»œ(CNNs)ï¼ŒåŒ…æ‹¬VGG16ã€ResNet50å’ŒXceptionï¼Œç”¨äºè‚¿ç˜¤åˆ†ç±»ï¼Œå¹¶åˆ©ç”¨YOLOv8å®ç°äº†ç²¾ç¡®çš„è¾¹ç•Œæ¡†(bounding boxes)å®šä½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨Brain Tumor MRI Datasetä¸Šï¼Œå¾®è°ƒåçš„VGG16æ¨¡å‹è¾¾åˆ°äº†99.86%çš„æµ‹è¯•å‡†ç¡®ç‡ï¼Œæ˜¾è‘—è¶…è¶Šäº†æ­¤å‰çš„æ€§èƒ½åŸºå‡†ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶é€šè¿‡é›†æˆå®šä½åŠŸèƒ½ä¸å¯è§£é‡Šäººå·¥æ™ºèƒ½(explainable AI)æŠ€æœ¯ï¼Œæœ‰æ•ˆå¢å¼ºäº†ä¸´åºŠè§£è¯»çš„é€æ˜åº¦ä¸ç³»ç»Ÿè¾“å‡ºçš„å¯ä¿¡åº¦ã€‚è¿™ä¸€ç ”ç©¶ä¸ä»…ç¡®ç«‹äº†æ–°çš„å‡†ç¡®ç‡æ ‡å‡†ï¼Œä¹Ÿä¸ºä¸´åºŠå†³ç­–æä¾›äº†æ›´å…·è§£é‡Šæ€§çš„æ”¯æŒï¼Œå±•ç¤ºäº†æ·±åº¦å­¦ä¹ åœ¨ä¼˜åŒ–æ‚£è€…æŠ¤ç†å’Œæé«˜ç”Ÿå­˜ç‡æ–¹é¢çš„å˜é©æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "6 pages, 4 figures, 3 tables. Accepted at the 12th International Conference on Wireless Networks and Mobile Communications 2025 (WINCOM 2025)",
      "pdf_url": "https://arxiv.org/pdf/2509.24149v1",
      "published_date": "2025-09-29 00:53:17 UTC",
      "updated_date": "2025-09-29 00:53:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:14:38.889954+00:00"
    },
    {
      "arxiv_id": "2509.24148v2",
      "title": "TENET: Leveraging Tests Beyond Validation for Code Generation",
      "title_zh": "TENETï¼šåˆ©ç”¨è¶…è¶ŠéªŒè¯çš„æµ‹è¯•è¿›è¡Œä»£ç ç”Ÿæˆ",
      "authors": [
        "Yiran Hu",
        "Nan Jiang",
        "Shanchao Liang",
        "Yi Wu",
        "Lin Tan"
      ],
      "abstract": "Test-Driven Development (TDD) is a widely adopted software engineering practice that requires developers to create and execute tests alongside code implementation, ensuring that software behavior is continuously validated and refined. In the era of vibe coding, where developers increasingly delegate code writing to large language models (LLMs) by specifying high-level intentions, TDD becomes even more crucial, as test cases serve as executable specifications that explicitly define and verify intended functionality beyond what natural-language descriptions and code context can convey. While vibe coding under TDD is promising, there are three main challenges: (1) selecting a small yet effective test suite to improve the generation accuracy and control the execution workload, (2) retrieving context such as relevant code effectively, and (3) systematically using test feedback for effective code refinement. To address these challenges, we introduce TENET, an LLM agent for generating functions in complex real-world repositories under the TDD setting. TENET features three components: (1) a novel test harness mechanism that selects a concise test suite to maximize diversity of target usage scenarios; (2) a tailored agent toolset that performs efficient retrieval of relevant code with interactive debugging; and (3) a reflection-based refinement workflow that iteratively analyzes failures, replenishes context, and applies code refinement. TENET achieves 69.08% and 81.77% Pass@1 on RepoCod and RepoEval benchmarks, outperforming the best agentic baselines by 9.49 and 2.17 percentage points, respectively. In addition, this is the first study of test-driven code generation with repository-level context, examining how different aspects of test suites affect the performance of LLM agents under the TDD setting.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TENETï¼Œè¿™æ˜¯ä¸€æ¬¾æ—¨åœ¨æµ‹è¯•é©±åŠ¨å¼€å‘ (Test-Driven Development, TDD) ç¯å¢ƒä¸‹ä¸ºå¤æ‚çœŸå®ä»£ç åº“ç”Ÿæˆå‡½æ•°çš„ LLM agentã€‚ä¸ºäº†è§£å†³æµ‹è¯•å¥—ä»¶é€‰æ‹©ã€ä»£ç ä¸Šä¸‹æ–‡æ£€ç´¢ä»¥åŠåˆ©ç”¨æµ‹è¯•åé¦ˆè¿›è¡Œæœ‰æ•ˆä¿®å¤ç­‰æ ¸å¿ƒæŒ‘æˆ˜ï¼ŒTENET è®¾è®¡äº†ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼ŒåŒ…æ‹¬èƒ½å¤Ÿæœ€å¤§åŒ–åœºæ™¯å¤šæ ·æ€§çš„æµ‹è¯•æŒ‚é’© (test harness) æœºåˆ¶ã€æ”¯æŒäº¤äº’å¼è°ƒè¯•çš„ä»£ç æ£€ç´¢å·¥å…·é›†ï¼Œä»¥åŠåŸºäºåå°„çš„è¿­ä»£ä¿®å¤å·¥ä½œæµã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTENET åœ¨ RepoCod å’Œ RepoEval åŸºå‡†æµ‹è¯•ä¸­åˆ†åˆ«å–å¾—äº† 69.08% å’Œ 81.77% çš„ Pass@1 æˆç»©ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„å…ˆè¿› agent åŸºå‡†æ¨¡å‹ã€‚ä½œä¸ºé¦–é¡¹åœ¨ä»£ç åº“çº§åˆ«æ¢è®¨æµ‹è¯•é©±åŠ¨ä»£ç ç”Ÿæˆçš„ç ”ç©¶ï¼ŒTENET æ·±å…¥åˆ†æäº†æµ‹è¯•å¥—ä»¶å¯¹ LLM agent æ€§èƒ½çš„å…·ä½“å½±å“ï¼Œä¸ºå¯éªŒè¯çš„è‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆå¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24148v2",
      "published_date": "2025-09-29 00:53:16 UTC",
      "updated_date": "2025-09-30 04:05:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:14:46.191989+00:00"
    },
    {
      "arxiv_id": "2509.24147v1",
      "title": "Your thoughts tell who you are: Characterize the reasoning patterns of LRMs",
      "title_zh": "æ€å¦‚å…¶äººï¼šåˆ»ç”» LRMs çš„æ¨ç†æ¨¡å¼",
      "authors": [
        "Yida Chen",
        "Yuning Mao",
        "Xianjun Yang",
        "Suyu Ge",
        "Shengjie Bi",
        "Lijuan Liu",
        "Saghar Hosseini",
        "Liang Tan",
        "Yixin Nie",
        "Shaoliang Nie"
      ],
      "abstract": "Current comparisons of large reasoning models (LRMs) focus on macro-level statistics such as task accuracy or reasoning length. Whether different LRMs reason differently remains an open question. To address this gap, we introduce the LLM-proposed Open Taxonomy (LOT), a classification method that uses a generative language model to compare reasoning traces from two LRMs and articulate their distinctive features in words. LOT then models how these features predict the source LRM of a reasoning trace based on their empirical distributions across LRM outputs. Iterating this process over a dataset of reasoning traces yields a human-readable taxonomy that characterizes how models think. We apply LOT to compare the reasoning of 12 open-source LRMs on tasks in math, science, and coding. LOT identifies systematic differences in their thoughts, achieving 80-100% accuracy in distinguishing reasoning traces from LRMs that differ in scale, base model family, or objective domain. Beyond classification, LOT's natural-language taxonomy provides qualitative explanations of how LRMs think differently. Finally, in a case study, we link the reasoning differences to performance: aligning the reasoning style of smaller Qwen3 models with that of the largest Qwen3 during test time improves their accuracy on GPQA by 3.3-5.7%.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹æ¨ç†æ¨¡å‹(LRMs)çš„æ¨ç†æ¨¡å¼å·®å¼‚ï¼Œæå‡ºäº†å¤§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„å¼€æ”¾å¼åˆ†ç±»æ³•(LLM-proposed Open Taxonomy, LOT)ï¼Œæ—¨åœ¨é€šè¿‡ç”Ÿæˆå¼æ¨¡å‹å¯¹æ¯”æ¨ç†è·¯å¾„å¹¶æå–åŒºåˆ†æ€§ç‰¹å¾ã€‚LOTé€šè¿‡å»ºæ¨¡è¿™äº›ç‰¹å¾çš„ç»éªŒåˆ†å¸ƒæ¥é¢„æµ‹æ¨ç†è·¯å¾„çš„æ¥æºï¼Œä»è€Œç”Ÿæˆèƒ½å¤Ÿåˆ»ç”»æ¨¡å‹æ€ç»´æ–¹å¼çš„äººç±»å¯è¯»åˆ†ç±»ä½“ç³»ã€‚åœ¨æ•°å­¦ã€ç§‘å­¦å’Œç¼–ç¨‹ä»»åŠ¡çš„è¯„ä¼°ä¸­ï¼ŒLOTèƒ½ä»¥80%-100%çš„å‡†ç¡®ç‡åŒºåˆ†12ç§å¼€æºLRMsçš„æ¨ç†æ¥æºï¼Œå¹¶å®šæ€§è§£é‡Šäº†æ¨¡å‹é—´ç³»ç»Ÿæ€§çš„æ€ç»´å·®å¼‚ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°é€šè¿‡åœ¨æµ‹è¯•æ—¶å°†å°å‹Qwen3æ¨¡å‹çš„æ¨ç†é£æ ¼ä¸å¤§æ¨¡å‹å¯¹é½ï¼Œèƒ½æ˜¾è‘—æå‡å…¶åœ¨GPQAä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡ï¼Œæ¶¨å¹…è¾¾3.3%-5.7%ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†é‡åŒ–å’Œåˆ†ææ€ç»´æ¨¡å¼å¯¹äºç†è§£åŠä¼˜åŒ–å¤§å‹æ¨ç†æ¨¡å‹å…·æœ‰é‡è¦ä»·å€¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "32 pages, 28 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.24147v1",
      "published_date": "2025-09-29 00:52:07 UTC",
      "updated_date": "2025-09-29 00:52:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:14:54.001152+00:00"
    },
    {
      "arxiv_id": "2509.24136v1",
      "title": "EYE-DEX: Eye Disease Detection and EXplanation System",
      "title_zh": "EYE-DEXï¼šçœ¼éƒ¨ç–¾ç—…æ£€æµ‹ä¸è§£é‡Šç³»ç»Ÿ",
      "authors": [
        "Youssef Sabiri",
        "Walid Houmaidi",
        "Amine Abouaomar"
      ],
      "abstract": "Retinal disease diagnosis is critical in preventing vision loss and reducing socioeconomic burdens. Globally, over 2.2 billion people are affected by some form of vision impairment, resulting in annual productivity losses estimated at $411 billion. Traditional manual grading of retinal fundus images by ophthalmologists is time-consuming and subjective. In contrast, deep learning has revolutionized medical diagnostics by automating retinal image analysis and achieving expert-level performance. In this study, we present EYE-DEX, an automated framework for classifying 10 retinal conditions using the large-scale Retinal Disease Dataset comprising 21,577 eye fundus images. We benchmark three pre-trained Convolutional Neural Network (CNN) models--VGG16, VGG19, and ResNet50--with our finetuned VGG16 achieving a state-of-the-art global benchmark test accuracy of 92.36%. To enhance transparency and explainability, we integrate the Gradient-weighted Class Activation Mapping (Grad-CAM) technique to generate visual explanations highlighting disease-specific regions, thereby fostering clinician trust and reliability in AI-assisted diagnostics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† EYE-DEXï¼Œä¸€ç§ç”¨äºæ£€æµ‹å’Œè§£é‡Šçœ¼ç§‘ç–¾ç—…çš„è‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å…¨çƒèŒƒå›´å†…è§†ç½‘è†œç–¾ç—…è¯Šæ–­è€—æ—¶ä¸”å…·æœ‰ä¸»è§‚æ€§çš„æŒ‘æˆ˜ã€‚ç³»ç»Ÿåˆ©ç”¨åŒ…å« 21,577 å¼ çœ¼åº•å›¾åƒçš„å¤§è§„æ¨¡ Retinal Disease Datasetï¼Œèƒ½å¤Ÿå¯¹ 10 ç§è§†ç½‘è†œçŠ¶å†µè¿›è¡Œåˆ†ç±»ã€‚ç ”ç©¶å¯¹ VGG16ã€VGG19 å’Œ ResNet50 ä¸‰ç§é¢„è®­ç»ƒçš„ Convolutional Neural Network (CNN) æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚ç»“æœæ˜¾ç¤ºï¼Œç»è¿‡å¾®è°ƒçš„ VGG16 æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šè¾¾åˆ°äº† 92.36% çš„å‡†ç¡®ç‡ï¼Œåˆ›ä¸‹äº†è¯¥æ•°æ®é›†çš„ state-of-the-art æ€§èƒ½è®°å½•ã€‚ä¸ºäº†å¢å¼ºç³»ç»Ÿçš„é€æ˜åº¦ï¼Œç ”ç©¶é›†æˆäº† Gradient-weighted Class Activation Mapping (Grad-CAM) æŠ€æœ¯ï¼Œé€šè¿‡ç”Ÿæˆçƒ­åŠ›å›¾æ¥å¯è§†åŒ–è§£é‡Šç–¾ç—…ç‰¹å®šçš„åŒºåŸŸã€‚è¿™ç§å¯è§£é‡Šæ€§çš„è®¾è®¡ä¸ä»…æœ‰åŠ©äºæé«˜ä¸´åºŠåŒ»ç”Ÿå¯¹ AI è¾…åŠ©è¯Šæ–­ç³»ç»Ÿçš„ä¿¡ä»»ï¼Œä¹Ÿä¸ºé˜²æ­¢è§†åŠ›ä¸§å¤±å’Œå‡è½»ç¤¾ä¼šç»æµè´Ÿæ‹…æä¾›äº†å¯é çš„æŠ€æœ¯æ‰‹æ®µã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "6 pages, 4 figures, 3 tables. Accepted at the 12th International Conference on Wireless Networks and Mobile Communications 2025 (WINCOM 2025)",
      "pdf_url": "https://arxiv.org/pdf/2509.24136v1",
      "published_date": "2025-09-29 00:10:02 UTC",
      "updated_date": "2025-09-29 00:10:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:14:48.686867+00:00"
    },
    {
      "arxiv_id": "2509.24134v1",
      "title": "ASTROCO: Self-Supervised Conformer-Style Transformers for Light-Curve Embeddings",
      "title_zh": "ASTROCOï¼šé¢å‘å…‰å˜æ›²çº¿åµŒå…¥çš„è‡ªç›‘ç£ Conformer é£æ ¼ Transformer",
      "authors": [
        "Antony Tan",
        "Pavlos Protopapas",
        "Martina CÃ¡diz-Leyton",
        "Guillermo Cabrera-Vives",
        "Cristobal Donoso-Oliva",
        "Ignacio Becker"
      ],
      "abstract": "We present AstroCo, a Conformer-style encoder for irregular stellar light curves. By combining attention with depthwise convolutions and gating, AstroCo captures both global dependencies and local features. On MACHO R-band, AstroCo outperforms Astromer v1 and v2, yielding 70 percent and 61 percent lower error respectively and a relative macro-F1 gain of about 7 percent, while producing embeddings that transfer effectively to few-shot classification. These results highlight AstroCo's potential as a strong and label-efficient foundation for time-domain astronomy.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AstroCoï¼Œä¸€ç§é’ˆå¯¹ä¸è§„åˆ™æ’æ˜Ÿ light curves è®¾è®¡çš„ Conformer-style encoder æ¶æ„ã€‚è¯¥æ¨¡å‹é€šè¿‡ç»“åˆ attention æœºåˆ¶ã€depthwise convolutions å’Œ gating æ¨¡å—ï¼Œèƒ½å¤ŸåŒæ—¶æœ‰æ•ˆæ•æ‰æ•°æ®çš„ global dependencies å’Œ local featuresã€‚åœ¨ MACHO R-band æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAstroCo çš„æ€§èƒ½æ˜¾è‘—ä¼˜äº Astromer v1 å’Œ v2ï¼Œå…¶è¯¯å·®åˆ†åˆ«é™ä½äº† 70% å’Œ 61%ï¼Œä¸”ç›¸å¯¹ macro-F1 æå‡äº†çº¦ 7%ã€‚æ­¤å¤–ï¼ŒAstroCo ç”Ÿæˆçš„ embeddings åœ¨ few-shot classification ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„è¿ç§»æ•ˆæœã€‚è¿™äº›ç»“æœè¯æ˜äº† AstroCo èƒ½å¤Ÿä½œä¸ºæ—¶åŸŸå¤©æ–‡å­¦ (time-domain astronomy) é¢†åŸŸä¸­å¼ºå¤§ä¸” label-efficient çš„åŸºç¡€æ¨¡å‹ã€‚",
      "categories": [
        "astro-ph.IM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "astro-ph.IM",
      "comment": "Accepted at the NeurIPS 2025 Workshop on Machine Learning and the Physical Sciences (ML4PS), camera-ready version in progress",
      "pdf_url": "https://arxiv.org/pdf/2509.24134v1",
      "published_date": "2025-09-29 00:09:23 UTC",
      "updated_date": "2025-09-29 00:09:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T23:14:59.499147+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 326,
  "processed_papers_count": 326,
  "failed_papers_count": 0,
  "llm_backup_calls": 2,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-24T23:18:18.992794+00:00"
}
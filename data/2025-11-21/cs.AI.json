{
  "date": "2025-11-21",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-11-21 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\nğŸ‘‹ **ä»Šæ—¥å¯¼è¯»**ï¼š\nä»Šå¤©çš„ arXiv è®ºæ–‡äº•å–·ï¼Œè´¨é‡æé«˜ã€‚**åŒ»ç–— AI è¿æ¥â€œGPTæ—¶åˆ»â€**ï¼ŒPillar-0 å’Œ MAIRA-X å±•ç¤ºäº†åŸºç¡€æ¨¡å‹åœ¨æ”¾å°„å­¦ä¸Šçš„ç»Ÿæ²»åŠ›ï¼›**è„‘æœºæ¥å£**å‡ºç°é‡å¤§çªç ´ï¼Œå®ç°ç«¯åˆ°ç«¯â€œæ„å¿µè½¬æ–‡å­—â€ï¼›**Agent å®‰å…¨**ç ”ç©¶æ·±å…¥åˆ°äº†ç¤¾ä¼šå­¦å±‚é¢ï¼Œæ¢è®¨äº† AI çš„â€œå¹å“¨äººâ€è¡Œä¸ºå’Œâ€œé˜¿è°€å¥‰æ‰¿â€ç°è±¡ã€‚æ­¤å¤–ï¼Œ**AMD ç¡¬ä»¶ä¸Šçš„å¤§è§„æ¨¡ MoE è®­ç»ƒ**æŠ¥å‘Šæ‰“ç ´äº† CUDA çš„å„æ–­å™äº‹ï¼Œå€¼å¾—å…³æ³¨ã€‚\n\n---\n\n### ğŸš€ å¿…è¯»ç²¾é€‰ï¼šåŒ»ç–—åŸºåº§ã€è„‘æœºæ¥å£ä¸ç¡¬ä»¶ç”Ÿæ€\n\n**11. Pillar-0: A New Frontier for Radiology Foundation Models**\n**Pillar-0ï¼šæ”¾å°„å­¦åŸºç¡€æ¨¡å‹çš„æ–°å‰æ²¿**\n*   **æ ¸å¿ƒè´¡çŒ®**ï¼šæå‡ºäº† Pillar-0ï¼Œä¸€ä¸ªåœ¨æµ·é‡æ•°æ®ï¼ˆ42,990 è…¹éƒ¨ CT, 86,411 èƒ¸éƒ¨ CT ç­‰ï¼‰ä¸Šé¢„è®­ç»ƒçš„æ”¾å°„å­¦åŸºç¡€æ¨¡å‹ã€‚é…åˆ RATE æ¡†æ¶ï¼ˆåˆ©ç”¨ LLM æå–ç»“æ„åŒ–æ ‡ç­¾ï¼‰ï¼Œè¯¥æ¨¡å‹åœ¨ 366 é¡¹æ”¾å°„å­¦å‘ç°ä»»åŠ¡ä¸­å»ºç«‹äº†æ–°çš„ SOTAï¼Œè¶…è¶Šäº† Google (MedGemma)ã€Microsoft (MedImageInsight) ç­‰å¤§å‚æ¨¡å‹ã€‚\n*   **Implication**ï¼šè¿™æ˜¯åŒ»ç–—å½±åƒé¢†åŸŸçš„ä¸€ä¸ªé‡Œç¨‹ç¢‘ï¼Œè¯æ˜äº†é«˜ä¿çœŸ 3D æ•°æ®ï¼ˆè€Œéé™ç»´çš„ 2D åˆ‡ç‰‡ï¼‰é¢„è®­ç»ƒçš„å¿…è¦æ€§ã€‚\n\n**13. Decoding inner speech with an end-to-end brain-to-text neural interface**\n**é€šè¿‡ç«¯åˆ°ç«¯è„‘-æ–‡ç¥ç»æ¥å£è§£ç å†…å¿ƒè¯­è¨€**\n*   **æ ¸å¿ƒå‘ç°**ï¼šæå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„è„‘-æ–‡ï¼ˆBrain-to-Text, BITï¼‰æ¡†æ¶ï¼Œèƒ½å¤Ÿå°†ç¥ç»æ´»åŠ¨ç›´æ¥ç¿»è¯‘æˆè¿è´¯çš„å¥å­ã€‚é€šè¿‡è·¨ä»»åŠ¡ã€è·¨ç‰©ç§çš„é¢„è®­ç»ƒç¼–ç å™¨ï¼Œä¸ä»…è§£ç â€œè¯•å›¾è¯´å‡ºçš„è¯­éŸ³â€ï¼Œè¿˜èƒ½è§£ç â€œæƒ³è±¡çš„è¯­éŸ³â€ï¼ˆinner speechï¼‰ã€‚\n*   **äº®ç‚¹**ï¼šå°†ä¹‹å‰çš„ç«¯åˆ°ç«¯æ–¹æ³•çš„è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ä» 24.69% æƒŠäººåœ°é™ä½åˆ°äº† 10.22%ã€‚\n\n**79. Training Foundation Models on a Full-Stack AMD Platform: Compute, Networking, and System Design**\n**åœ¨å…¨æ ˆ AMD å¹³å°ä¸Šè®­ç»ƒåŸºç¡€æ¨¡å‹ï¼šè®¡ç®—ã€ç½‘ç»œä¸ç³»ç»Ÿè®¾è®¡**\n*   **ç¡¬æ ¸å·¥ç¨‹**ï¼šè¿™æ˜¯ä¸€ç¯‡æå…·å·¥ä¸šä»·å€¼çš„æŠ¥å‘Šã€‚ä½œè€…æŠ¥å‘Šäº†åœ¨çº¯ AMD ç¡¬ä»¶ï¼ˆMI300X GPU + Pollara ç½‘ç»œï¼‰ä¸Šè¿›è¡Œå¤§è§„æ¨¡ MoEï¼ˆæ··åˆä¸“å®¶ï¼‰æ¨¡å‹é¢„è®­ç»ƒçš„ç ”ç©¶ã€‚\n*   **ç»“è®º**ï¼šå‘å¸ƒäº† ZAYA1-base æ¨¡å‹ï¼Œè¯æ˜äº† AMD è½¯ç¡¬ä»¶æ ˆå·²ç»æˆç†Ÿï¼Œè¶³ä»¥æ”¯æŒå¤§è§„æ¨¡ä¸”æœ‰ç«äº‰åŠ›çš„ LLM é¢„è®­ç»ƒï¼Œæ‰“ç ´äº† NVIDIA çš„å„æ–­è¿·æ€ã€‚\n\n**1. Less is More: Data-Efficient Adaptation for Controllable Text-to-Video Generation**\n**å°‘å³æ˜¯å¤šï¼šç”¨äºå¯æ§æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆçš„æ•°æ®é«˜æ•ˆé€‚é…**\n*   **åç›´è§‰å‘ç°**ï¼šåœ¨å¾®è°ƒ T2V æ¨¡å‹ä»¥å¢åŠ ç‰©ç†ç›¸æœºæ§åˆ¶ï¼ˆå¦‚å¿«é—¨é€Ÿåº¦ã€å…‰åœˆï¼‰æ—¶ï¼Œä½¿ç”¨**ç¨€ç–ã€ä½è´¨é‡çš„åˆæˆæ•°æ®**ç«Ÿç„¶æ¯”ä½¿ç”¨çœŸå®çš„é«˜ä¿çœŸæ•°æ®æ•ˆæœæ›´å¥½ã€‚\n*   **æ–¹æ³•**ï¼šæå‡ºäº†ä¸€ç§æ•°æ®é«˜æ•ˆçš„å¾®è°ƒç­–ç•¥ï¼Œåˆ©ç”¨ç®€å•çš„åˆæˆæ•°æ®å®ç°äº†ä¼˜è¶Šçš„æ§åˆ¶æ•ˆæœã€‚\n\n---\n\n### ğŸ¤– LLM Agentï¼šè¡Œä¸ºã€å®‰å…¨ä¸åŸºå‡†\n\n**90. Why Do Language Model Agents Whistleblow?**\n**è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“é€šè¿‡ä»€ä¹ˆæœºåˆ¶è¿›è¡Œâ€œå¹å“¨â€ï¼Ÿ**\n*   **æœ‰è¶£è®®é¢˜**ï¼šç ”ç©¶äº† Agent çš„â€œå¹å“¨â€ï¼ˆWhistleblowingï¼‰è¡Œä¸ºâ€”â€”å³ Agent åœ¨æ²¡æœ‰ç”¨æˆ·æŒ‡ä»¤çš„æƒ…å†µä¸‹ï¼Œå‘ç¬¬ä¸‰æ–¹ï¼ˆå¦‚ç›‘ç®¡æœºæ„ï¼‰æŠ¥å‘Šç”¨æˆ·çš„ä¸å½“è¡Œä¸ºã€‚\n*   **å‘ç°**ï¼šå¢åŠ ä»»åŠ¡å¤æ‚æ€§ä¼šé™ä½å¹å“¨å€¾å‘ï¼›ä½†åœ¨ System Prompt ä¸­åŠ å…¥é“å¾·å¼•å¯¼ä¼šæ˜¾è‘—å¢åŠ å¹å“¨ç‡ã€‚è¿™æ˜¯ä¸€ä¸ªéå¸¸æ–°çš„ AI å¯¹é½å’Œç¤¾ä¼šä¼¦ç†ç ”ç©¶æ–¹å‘ã€‚\n\n**64. PARROT: Persuasion and Agreement Robustness Rating of Output Truth**\n**PARROTï¼šè¾“å‡ºçœŸå®æ€§çš„è¯´æœä¸ååŒé²æ£’æ€§è¯„çº§**\n*   **æ ¸å¿ƒæœ¯è¯­**ï¼šSycophancyï¼ˆé˜¿è°€å¥‰æ‰¿/ç¼ºä¹ä¸»è§ï¼‰ã€‚\n*   **å‘ç°**ï¼šé€šè¿‡ PARROT æ¡†æ¶æµ‹è¯•å‘ç°ï¼Œç›®å‰çš„æ¨¡å‹æ™®éå­˜åœ¨â€œé˜¿è°€å¥‰æ‰¿â€ç°è±¡â€”â€”å³å½“å—åˆ°å‹åŠ›æˆ–æƒå¨è¯±å¯¼æ—¶ï¼Œæ¨¡å‹ä¼šæ”¾å¼ƒæ­£ç¡®ç­”æ¡ˆå»è¿åˆé”™è¯¯è§‚ç‚¹ã€‚GPT-4 çš„â€œç›²ä»ç‡â€é«˜è¾¾ 80%ï¼Œè€Œæ›´å…ˆè¿›çš„æ¨¡å‹ï¼ˆå¦‚ GPT-5, Claude Sonnet 4.5ï¼‰è¡¨ç°å‡ºè¾ƒå¥½çš„æŠ—å‹æ€§ã€‚\n\n**77. UI-CUBE: Enterprise-Grade Computer Use Agent Benchmarking**\n**UI-CUBEï¼šä¼ä¸šçº§è®¡ç®—æœºæ“ä½œæ™ºèƒ½ä½“åŸºå‡†æµ‹è¯•**\n*   **ç°å®æ‰“å‡»**ï¼šç›®å‰çš„â€œè®¡ç®—æœºä½¿ç”¨ Agentâ€ï¼ˆComputer Use Agentï¼‰åœ¨ç®€å•çš„ UI äº¤äº’ä¸Šèƒ½è¾¾åˆ°äººç±» 67-85% çš„æ°´å¹³ï¼Œä½†åœ¨å¤æ‚çš„ä¼ä¸šå·¥ä½œæµï¼ˆå¦‚è·¨åº”ç”¨å¤åˆ¶ç²˜è´´ã€ERP æ“ä½œï¼‰ä¸­ï¼ŒæˆåŠŸç‡æ–­å´–å¼ä¸‹è·Œè‡³ 9-19%ã€‚æ­ç¤ºäº†å½“å‰ Agent ç¦»çœŸæ­£â€œæ‰“å·¥â€è¿˜æœ‰å¾ˆå¤§è·ç¦»ã€‚\n\n**21. M^3-Bench: Multi-Modal, Multi-Hop, Multi-Threaded Tool-Using MLLM Agent Benchmark**\n**M^3-Benchï¼šå¤šæ¨¡æ€ã€å¤šè·³ã€å¤šçº¿ç¨‹å·¥å…·ä½¿ç”¨ MLLM æ™ºèƒ½ä½“åŸºå‡†**\n*   **æ–°åŸºå‡†**ï¼šé’ˆå¯¹ Model Context Protocol (MCP) æå‡ºäº†é¦–ä¸ªå¤šæ¨¡æ€å·¥å…·ä½¿ç”¨åŸºå‡†ã€‚è¯„ä¼°å‘ç°ç›®å‰çš„ MLLM åœ¨å¤„ç†å‚æ•°ä¿çœŸåº¦å’Œç»“æ„ä¸€è‡´æ€§æ–¹é¢å­˜åœ¨å·¨å¤§å·®è·ã€‚\n\n---\n\n### ğŸ§  æ¨¡å‹æ¶æ„ã€æ¨ç†ä¸æ•ˆç‡\n\n**122. Deep Improvement Supervision**\n**æ·±åº¦æ”¹è¿›ç›‘ç£**\n*   **å°æ¨¡å‹é€†è¢­**ï¼šç ”ç©¶è¡¨æ˜ï¼Œå¾®å°çš„é€’å½’æ¨¡å‹ï¼ˆTiny Recursive Models, TRMsï¼‰å¯ä»¥åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ï¼ˆå¦‚ ARCï¼‰ä¸Šè¶…è¶Š LLMã€‚\n*   **æ–¹æ³•**ï¼šæå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒæ–¹æ¡ˆï¼Œå°†æ½œåœ¨æ¨ç†è§†ä¸ºä¸€ç§ classifier-free guidanceã€‚ä»…ç”¨ 0.8M å‚æ•°å°±åœ¨ ARC-1 ä¸Šè¾¾åˆ°äº† 24% çš„å‡†ç¡®ç‡ã€‚\n\n**83. Asking LLMs to Verify First is Almost Free Lunch**\n**è®© LLM å…ˆéªŒè¯å‡ ä¹æ˜¯å…è´¹çš„åˆé¤**\n*   **Trick**ï¼šæå‡º \"Verification-First\" (VF) ç­–ç•¥ï¼Œè®©æ¨¡å‹åœ¨ç”Ÿæˆç­”æ¡ˆå‰å…ˆéªŒè¯ä¸€ä¸ªå€™é€‰ç­”æ¡ˆï¼ˆå“ªæ€•æ˜¯éšæœºçš„ï¼‰ã€‚è¿™ä¼šè§¦å‘â€œé€†å‘æ¨ç†â€è¿‡ç¨‹ï¼Œæ˜¾è‘—å‡å°‘é€»è¾‘é”™è¯¯ï¼Œä¸”è®¡ç®—å¼€é”€æä½ã€‚\n\n**65. R2Q: Towards Robust 2-Bit Large Language Models via Residual Refinement Quantization**\n**R2Qï¼šé€šè¿‡æ®‹å·®ç»†åŒ–é‡åŒ–è¿ˆå‘é²æ£’çš„ 2-bit å¤§è¯­è¨€æ¨¡å‹**\n*   **æç«¯å‹ç¼©**ï¼šå°† 2-bit é‡åŒ–åˆ†è§£ä¸ºä¸¤ä¸ªè¿ç»­çš„ 1-bit å­é‡åŒ–ã€‚åœ¨ Llama å’Œ Qwen ä¸Šæµ‹è¯•è¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰çš„ 2-bit é‡åŒ–æ–¹æ³•ï¼Œä½¿æç«¯å‹ç¼©æ¨¡å‹å˜å¾—å¯ç”¨ã€‚\n\n**82. Geometric-disentangelment Unlearning**\n**å‡ ä½•è§£ç¼ é—å¿˜**\n*   **æœºå™¨é—å¿˜**ï¼šè§£å†³äº†æœºå™¨é—å¿˜ï¼ˆMachine Unlearningï¼‰ä¸­â€œå¿˜å¾—å¿«ä½†ä¹ŸæŠŠè¯¥è®°çš„å¿˜æ‰äº†â€çš„é—®é¢˜ã€‚åˆ©ç”¨å‡ ä½•æŠ•å½±ï¼Œåªåœ¨ä¸ä¿ç•™çŸ¥è¯†æ­£äº¤çš„æ–¹å‘ä¸Šæ›´æ–°å‚æ•°ï¼Œå®ç°äº†ç²¾å‡†é—å¿˜ã€‚\n\n---\n\n### ğŸ”¬ ç§‘å­¦ AI ä¸ äº¤å‰å­¦ç§‘\n\n**24. The Rapid Growth of AI Foundation Model Usage in Science**\n**AI åŸºç¡€æ¨¡å‹åœ¨ç§‘å­¦é¢†åŸŸçš„åº”ç”¨æ¿€å¢**\n*   **è¶‹åŠ¿åˆ†æ**ï¼šè¿™æ˜¯å¯¹ç§‘å­¦ç•Œä½¿ç”¨ AI åŸºç¡€æ¨¡å‹çš„é¦–æ¬¡å¤§è§„æ¨¡é‡åŒ–åˆ†æã€‚è¯­è¨€å­¦ã€è®¡ç®—æœºå’Œå·¥ç¨‹å­¦åº”ç”¨æœ€å¤šã€‚ä¸€ä¸ªæ‹…å¿§æ˜¯ï¼šç§‘å­¦å®¶ä½¿ç”¨çš„æ¨¡å‹è§„æ¨¡å¢é•¿é€Ÿåº¦ï¼ˆæ»åï¼‰è¿œä½äº AI ç•Œçš„æ¨¡å‹å¢é•¿é€Ÿåº¦ï¼Œè¿™å¯èƒ½é™åˆ¶äº† AI é©±åŠ¨ç§‘å­¦ï¼ˆAI4Scienceï¼‰çš„æ½œåŠ›ã€‚\n\n**76. Closing the Performance Gap Between AI and Radiologists in Chest X-Ray Reporting**\n**å¼¥åˆ AI ä¸æ”¾å°„ç§‘åŒ»ç”Ÿåœ¨èƒ¸éƒ¨ X å…‰æŠ¥å‘Šä¸­çš„æ€§èƒ½å·®è·**\n*   **å®æˆ˜è½åœ°**ï¼šæ¨å‡ºäº† MAIRA-X æ¨¡å‹ï¼Œåœ¨ Mayo Clinic çš„ 310 ä¸‡ä»½ç ”ç©¶æ•°æ®ä¸Šè®­ç»ƒã€‚ç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼ŒAI ç”Ÿæˆçš„æŠ¥å‘Šåœ¨å…³é”®é”™è¯¯ç‡ä¸Šä¸äººç±»åŒ»ç”Ÿç›¸å½“ï¼ˆ3.0% vs 4.6%ï¼‰ï¼Œåœ¨å¤„ç†ç®¡çº¿å’Œå¯¼ç®¡ï¼ˆLines and Tubesï¼‰è¯†åˆ«ä¸Šè¡¨ç°å‡ºè‰²ã€‚\n\n**31. Planning with Sketch-Guided Verification for Physics-Aware Video Generation**\n**åŸºäºè‰å›¾å¼•å¯¼éªŒè¯çš„ç‰©ç†æ„ŸçŸ¥è§†é¢‘ç”Ÿæˆè§„åˆ’**\n*   **ç”Ÿæˆæ§åˆ¶**ï¼šä¸ºäº†è§£å†³è§†é¢‘ç”Ÿæˆä¸­ç‰©ä½“è¿åŠ¨ä¸ç¬¦åˆç‰©ç†è§„å¾‹çš„é—®é¢˜ï¼Œæå‡ºäº† SketchVerifyã€‚åœ¨ç”Ÿæˆå‰å…ˆè§„åˆ’è¿åŠ¨è½¨è¿¹å¹¶é€šè¿‡è‰å›¾éªŒè¯ç‰©ç†åˆç†æ€§ï¼Œæ˜¯ä¸€ä¸ª Training-free çš„æ–¹æ³•ã€‚\n\n**9. REXO: Indoor Multi-View Radar Object Detection via 3D Bounding Box Diffusion**\n**REXOï¼šåŸºäº 3D è¾¹ç•Œæ¡†æ‰©æ•£çš„å®¤å†…å¤šè§†å›¾é›·è¾¾ç‰©ä½“æ£€æµ‹**\n*   **æ„ŸçŸ¥å¢å¼º**ï¼šåˆ©ç”¨æ‰©æ•£æ¨¡å‹å°† 2D è¾¹ç•Œæ¡†æå‡åˆ° 3D é›·è¾¾ç©ºé—´ï¼Œè§£å†³äº†å®¤å†…é›·è¾¾æ„ŸçŸ¥ä¸­ç‰¹å¾åŒ¹é…æ¨¡ç³Šçš„é—®é¢˜ã€‚\n\n**100. Identifying Quantum Structure in AI Language**\n**è¯†åˆ« AI è¯­è¨€ä¸­çš„é‡å­ç»“æ„**\n*   **è®¤çŸ¥ç§‘å­¦**ï¼šè¿™ç¯‡è®ºæ–‡æœ‰ç‚¹â€œç„â€ã€‚ä½œè€…é€šè¿‡æµ‹è¯• ChatGPT å’Œ Geminiï¼Œå£°ç§°åœ¨ LLM çš„æ¦‚å¿µç»„åˆä¸­å‘ç°äº†â€œé‡å­çº ç¼ â€ç°è±¡ï¼ˆè¿åè´å°”ä¸ç­‰å¼ï¼‰ï¼Œè®¤ä¸ºè¿™æ˜¯äººç±»è®¤çŸ¥å’Œ AI è®¤çŸ¥è¿›åŒ–è¶‹åŒçš„è¯æ®ã€‚\n\n---\n\n### ğŸ’¡ å…¶ä»–å€¼å¾—ä¸€çœ‹çš„è®ºæ–‡\n\n*   **[Vision] Paper 50. MuM**: æå‡ºäº†å¤šè§†å›¾æ©ç å›¾åƒå»ºæ¨¡ï¼ˆMulti-View Masked Image Modelingï¼‰ï¼Œå°† MAE æ‰©å±•åˆ° 3D è§†è§‰ï¼Œæ€§èƒ½ä¼˜äº CroCoã€‚\n*   **[Optimization] Paper 111. Optimizing PyTorch Inference with LLM-Based Multi-Agent Systems**: ç”¨ LLM å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ¥ä¼˜åŒ– PyTorch æ¨ç†ä»£ç ï¼Œç«Ÿç„¶æ¯”ç°æœ‰çš„ç¼–è¯‘å™¨è¿˜å¿«ã€‚\n*   **[Audio] Paper 48. MusicAIR**: æå‡ºäº†ä¸€ç§ç®—æ³•é©±åŠ¨æ ¸å¿ƒçš„å¤šæ¨¡æ€éŸ³ä¹ç”Ÿæˆæ¡†æ¶ï¼Œå£°ç§°å¯ä»¥è§„é¿ç‰ˆæƒé£é™©ã€‚\n*   **[Reasoning] Paper 28. Masked-and-Reordered Self-Supervision for RLVR**: é’ˆå¯¹æ•°å­¦æ¨ç†ï¼Œæå‡ºäº†ä¸€ç§è¿‡ç¨‹æ„ŸçŸ¥çš„è‡ªç›‘ç£ä¿¡å·ï¼Œæå‡äº† RLVRï¼ˆæ¥è‡ªéªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼‰çš„å¯æ‰©å±•æ€§ã€‚",
  "papers": [
    {
      "arxiv_id": "2511.17844v2",
      "title": "Less is More: Data-Efficient Adaptation for Controllable Text-to-Video Generation",
      "title_zh": "å°‘å³æ˜¯å¤šï¼šç”¨äºå¯æ§æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆçš„æ•°æ®é«˜æ•ˆè‡ªé€‚åº”",
      "authors": [
        "Shihan Cheng",
        "Nilesh Kulkarni",
        "David Hyde",
        "Dmitriy Smirnov"
      ],
      "abstract": "Fine-tuning large-scale text-to-video diffusion models to add new generative controls, such as those over physical camera parameters (e.g., shutter speed or aperture), typically requires vast, high-fidelity datasets that are difficult to acquire. In this work, we propose a data-efficient fine-tuning strategy that learns these controls from sparse, low-quality synthetic data. We show that not only does fine-tuning on such simple data enable the desired controls, it actually yields superior results to models fine-tuned on photorealistic \"real\" data. Beyond demonstrating these results, we provide a framework that justifies this phenomenon both intuitively and quantitatively.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è§„æ¨¡Text-to-Videoæ‰©æ•£æ¨¡å‹åœ¨å¼•å…¥ç›¸æœºå¿«é—¨æˆ–å…‰åœˆç­‰æ–°ç‰©ç†æ§åˆ¶å‚æ•°æ—¶ï¼Œé¢ä¸´é«˜è´¨é‡æ•°æ®éš¾ä»¥è·å–çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§é«˜æ•ˆçš„æ•°æ®å¾®è°ƒç­–ç•¥ã€‚ä½œè€…è¯æ˜äº†é€šè¿‡ä½¿ç”¨ç¨€ç–ã€ä½è´¨é‡çš„Synthetic Dataè¿›è¡Œå¾®è°ƒï¼Œä¸ä»…èƒ½å¤Ÿæœ‰æ•ˆå®ç°ç›®æ ‡æ§åˆ¶åŠŸèƒ½ï¼Œå…¶è¡¨ç°ç”šè‡³ä¼˜äºä½¿ç”¨é«˜ä¿çœŸâ€œçœŸå®â€æ•°æ®å¾®è°ƒçš„æ¨¡å‹ã€‚è®ºæ–‡æä¾›äº†ä¸€ä¸ªä¸“é—¨çš„åˆ†ææ¡†æ¶ï¼Œä»ç›´è§‚ç†è§£å’Œå®šé‡åˆ†æä¸¤ä¸ªå±‚é¢è§£é‡Šäº†è¿™ä¸€ç°è±¡çš„å†…åœ¨é€»è¾‘ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†åœ¨ç‰¹å®šæ§åˆ¶ä»»åŠ¡ä¸­â€œå°‘å³æ˜¯å¤šâ€çš„åŸåˆ™ï¼Œä¸ºå¯æ§è§†é¢‘ç”ŸæˆæŠ€æœ¯æä¾›äº†ä¸€ç§æ•°æ®åˆ©ç”¨ç‡æé«˜ä¸”ä½æˆæœ¬çš„é€‚é…æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17844v2",
      "published_date": "2025-11-21 23:41:19 UTC",
      "updated_date": "2025-12-11 00:46:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:50:30.766509+00:00"
    },
    {
      "arxiv_id": "2511.21742v1",
      "title": "EduMod-LLM: A Modular Approach for Designing Flexible and Transparent Educational Assistants",
      "title_zh": "EduMod-LLMï¼šä¸€ç§ç”¨äºè®¾è®¡çµæ´»ä¸”é€æ˜æ•™è‚²åŠ©æ‰‹çš„æ¨¡å—åŒ–æ–¹æ³•",
      "authors": [
        "Meenakshi Mittal",
        "Rishi Khare",
        "Mihran Miroyan",
        "Chancharik Mitra",
        "Narges Norouzi"
      ],
      "abstract": "With the growing use of Large Language Model (LLM)-based Question-Answering (QA) systems in education, it is critical to evaluate their performance across individual pipeline components. In this work, we introduce {\\model}, a modular function-calling LLM pipeline, and present a comprehensive evaluation along three key axes: function calling strategies, retrieval methods, and generative language models. Our framework enables fine-grained analysis by isolating and assessing each component. We benchmark function-calling performance across LLMs, compare our novel structure-aware retrieval method to vector-based and LLM-scoring baselines, and evaluate various LLMs for response synthesis. This modular approach reveals specific failure modes and performance patterns, supporting the development of interpretable and effective educational QA systems. Our findings demonstrate the value of modular function calling in improving system transparency and pedagogical alignment. Website and Supplementary Material: https://chancharikmitra.github.io/EduMod-LLM-website/",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† EduMod-LLMï¼Œè¿™æ˜¯ä¸€ç§æ¨¡å—åŒ–çš„ function-calling LLM æ¶æ„ï¼Œæ—¨åœ¨æå‡æ•™è‚²é—®ç­”ç³»ç»Ÿçš„é€æ˜åº¦ä¸çµæ´»æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡å°† pipeline æ‹†åˆ†ä¸º function calling ç­–ç•¥ã€æ£€ç´¢æ–¹æ³•å’Œç”Ÿæˆå¼è¯­è¨€æ¨¡å‹ä¸‰ä¸ªæ ¸å¿ƒç»´åº¦ï¼Œå®ç°äº†å¯¹å„ç»„ä»¶æ€§èƒ½çš„ç»†ç²’åº¦å­¤ç«‹è¯„ä¼°ã€‚ç ”ç©¶è€…å°†æå‡ºçš„ structure-aware æ£€ç´¢æ–¹æ³•ä¸ vector-based å’Œ LLM-scoring ç­‰åŸºå‡†è¿›è¡Œäº†å¯¹æ¯”ï¼Œå¹¶å¯¹ä¸åŒ LLM çš„åˆæˆå“åº”èƒ½åŠ›è¿›è¡Œäº†ç³»ç»Ÿæ€§æµ‹è¯„ã€‚å®éªŒç»“æœæ­ç¤ºäº†ç³»ç»Ÿå†…éƒ¨ç‰¹å®šçš„å¤±æ•ˆæ¨¡å¼ä¸æ€§èƒ½è§„å¾‹ï¼ŒéªŒè¯äº†æ¨¡å—åŒ– function calling åœ¨å¢å¼ºç³»ç»Ÿå¯è§£é‡Šæ€§å’Œæ•™å­¦å¯¹é½ï¼ˆpedagogical alignmentï¼‰æ–¹é¢çš„æ˜¾è‘—ä»·å€¼ã€‚è¿™é¡¹å·¥ä½œä¸ºæ„å»ºé«˜æ•ˆã€é€æ˜ä¸”ç¬¦åˆæ•™å­¦éœ€æ±‚çš„æ•™è‚²è¾…åŠ©æ™ºèƒ½ä½“æä¾›äº†é‡è¦çš„æ–¹æ³•è®ºæ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "pdf_url": "https://arxiv.org/pdf/2511.21742v1",
      "published_date": "2025-11-21 23:05:46 UTC",
      "updated_date": "2025-11-21 23:05:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:50:33.367671+00:00"
    },
    {
      "arxiv_id": "2511.17833v2",
      "title": "Learning to Debug: LLM-Organized Knowledge Trees for Solving RTL Assertion Failures",
      "title_zh": "å­¦ä¹ è°ƒè¯•ï¼šç”¨äºè§£å†³ RTL æ–­è¨€å¤±è´¥çš„å¤§è¯­è¨€æ¨¡å‹ç»„ç»‡çŸ¥è¯†æ ‘",
      "authors": [
        "Yunsheng Bai",
        "Haoxing Ren"
      ],
      "abstract": "Debugging is the dominant cost in modern hardware verification, where assertion failures are among the most frequent and expensive to resolve. While Large Language Models (LLMs) show promise, they often fail to capture the precise, reusable expertise that engineers apply, leading to inaccurate responses. We propose GROVE, a hierarchical knowledge management framework that learns and organizes reusable debugging expertise into an LLM-organized knowledge tree for solving assertion failures. GROVE distills debugging knowledge from prior cases and organizes it into a vertical tree of configurable depth, with each node encoding a concise knowledge item and explicit applicability conditions. During training, GROVE uses a parallel, gradient-free loop where an LLM proposes tree modifications as structured JSON edits by learning from the cases. At test time, a budget-aware iterative zoom is performed to navigate the tree, retrieving a small set of applicable knowledge items that guide a base LLM's hypothesis generation and fix proposals. Evaluated on a suite of assertion-failure cases, GROVE delivers consistent gains in pass@1 and pass@5, demonstrating the value of structured knowledge evolution.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¡¬ä»¶éªŒè¯ä¸­ RTL æ–­è¨€å¤±è´¥ (Assertion Failures) è°ƒè¯•æˆæœ¬é«˜ä¸”å¤§è¯­è¨€æ¨¡å‹ (LLMs) éš¾ä»¥æ•æ‰é‡ç”¨ç»éªŒçš„é—®é¢˜ï¼Œæå‡ºäº† GROVE è¿™ä¸€å±‚æ¬¡åŒ–çŸ¥è¯†ç®¡ç†æ¡†æ¶ã€‚GROVE é€šè¿‡æ„å»ºç”± LLM ç»„ç»‡çš„çŸ¥è¯†æ ‘ (Knowledge Tree) æ¥æç‚¼å¹¶å­˜å‚¨è°ƒè¯•ä¸“ä¸šçŸ¥è¯†ï¼Œå…¶ä¸­æ¯ä¸ªèŠ‚ç‚¹ç¼–ç äº†å…·ä½“çš„çŸ¥è¯†é¡¹åŠå…¶é€‚ç”¨æ¡ä»¶ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨å¹¶è¡Œçš„æ— æ¢¯åº¦å¾ªç¯ï¼Œç”± LLM æ ¹æ®å†å²æ¡ˆä¾‹é€šè¿‡ç»“æ„åŒ–çš„ JSON ç¼–è¾‘æ¥åŠ¨æ€ä¼˜åŒ–æ ‘ç»“æ„ã€‚åœ¨æ¨ç†é˜¶æ®µï¼ŒGROVE åˆ©ç”¨é¢„ç®—æ„ŸçŸ¥çš„è¿­ä»£ç¼©æ”¾ (Iterative Zoom) ç­–ç•¥åœ¨æ ‘ä¸­å¯¼èˆªï¼Œæ£€ç´¢å‡ºå…³é”®çŸ¥è¯†ä»¥è¾…åŠ©åŸºç¡€ LLM è¿›è¡Œå‡è®¾ç”Ÿæˆå’Œä¿®å¤æ–¹æ¡ˆå»ºè®®ã€‚åœ¨æ–­è¨€å¤±è´¥æ¡ˆä¾‹é›†ä¸Šçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒGROVE åœ¨ pass@1 å’Œ pass@5 ç­‰æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—ä¸”ä¸€è‡´çš„æå‡ï¼ŒéªŒè¯äº†ç»“æ„åŒ–çŸ¥è¯†æ¼”åŒ–åœ¨ç¡¬ä»¶è‡ªåŠ¨åŒ–è°ƒè¯•ä¸­çš„æ ¸å¿ƒä»·å€¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17833v2",
      "published_date": "2025-11-21 22:57:45 UTC",
      "updated_date": "2025-12-13 00:41:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:51:04.761399+00:00"
    },
    {
      "arxiv_id": "2511.17829v1",
      "title": "Unified Class and Domain Incremental Learning with Mixture of Experts for Indoor Localization",
      "title_zh": "åŸºäºæ··åˆä¸“å®¶æ¨¡å‹çš„å®¤å†…å®šä½ç»Ÿä¸€ç±»å¢é‡ä¸åŸŸå¢é‡å­¦ä¹ ",
      "authors": [
        "Akhil Singampalli",
        "Sudeep Pasricha"
      ],
      "abstract": "Indoor localization using machine learning has gained traction due to the growing demand for location-based services. However, its long-term reliability is hindered by hardware/software variations across mobile devices, which shift the model's input distribution to create domain shifts. Further, evolving indoor environments can introduce new locations over time, expanding the output space to create class shifts, making static machine learning models ineffective over time. To address these challenges, we propose a novel unified continual learning framework for indoor localization called MOELO that, for the first time, jointly addresses domain-incremental and class-incremental learning scenarios. MOELO enables a lightweight, robust, and adaptive localization solution that can be deployed on resource-limited mobile devices and is capable of continual learning in dynamic, heterogeneous real-world settings. This is made possible by a mixture-of-experts architecture, where experts are incrementally trained per region and selected through an equiangular tight frame based gating mechanism ensuring efficient routing, and low-latency inference, all within a compact model footprint. Experimental evaluations show that MOELO achieves improvements of up to 25.6x in mean localization error, 44.5x in worst-case localization error, and 21.5x lesser forgetting compared to state-of-the-art frameworks across diverse buildings, mobile devices, and learning scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å®¤å†…å®šä½åœ¨é•¿æœŸåº”ç”¨ä¸­é¢ä¸´çš„ç¡¬ä»¶å·®å¼‚å¯¼è‡´çš„ Domain Shift ä»¥åŠæ–°åŒºåŸŸå¼•å…¥å¯¼è‡´çš„ Class Shift é—®é¢˜ï¼Œæå‡ºäº†é¦–ä¸ªç»Ÿä¸€çš„æŒç»­å­¦ä¹ æ¡†æ¶ MOELOã€‚è¯¥æ¡†æ¶é‡‡ç”¨ Mixture-of-Experts æ¶æ„ï¼Œé¦–æ¬¡å®ç°äº†å¯¹ Domain-Incremental Learning å’Œ Class-Incremental Learning åœºæ™¯çš„è”åˆå¤„ç†ã€‚MOELO é€šè¿‡æŒ‰åŒºåŸŸå¢é‡è®­ç»ƒä¸“å®¶ï¼Œå¹¶ç»“åˆåŸºäº Equiangular Tight Frame çš„é—¨æ§æœºåˆ¶ç¡®ä¿é«˜æ•ˆè·¯ç”±ï¼Œåœ¨ä¿æŒæ¨¡å‹è½»é‡åŒ–çš„åŒæ—¶å®ç°äº†ä½å»¶è¿Ÿæ¨ç†ï¼Œéå¸¸é€‚åˆéƒ¨ç½²äºèµ„æºå—é™çš„ç§»åŠ¨è®¾å¤‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMOELO åœ¨å¹³å‡å®šä½è¯¯å·®å’Œæœ€åæƒ…å†µè¯¯å·®ä¸Šåˆ†åˆ«æ¯”ç°æœ‰å…ˆè¿›æ¡†æ¶æå‡äº† 25.6 å€å’Œ 44.5 å€ï¼Œä¸”ç¾éš¾æ€§é—å¿˜ç‡é™ä½äº† 21.5 å€ã€‚è¿™ä¸€æˆæœä¸ºåœ¨åŠ¨æ€ã€å¼‚æ„çš„çœŸå®ä¸–ç•Œç¯å¢ƒä¸­å®ç°ç¨³å¥ã€è‡ªé€‚åº”çš„å®¤å†…å®šä½æä¾›äº†é‡è¦æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17829v1",
      "published_date": "2025-11-21 22:47:50 UTC",
      "updated_date": "2025-11-21 22:47:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:50:48.883137+00:00"
    },
    {
      "arxiv_id": "2511.17828v1",
      "title": "Toward explainable AI approaches for breast imaging: adapting foundation models to diverse populations",
      "title_zh": "è¿ˆå‘ä¹³è…ºå½±åƒçš„å¯è§£é‡Šäººå·¥æ™ºèƒ½æ–¹æ³•ï¼šé¢å‘å¤šæ ·åŒ–äººç¾¤çš„åŸºç¡€æ¨¡å‹é€‚é…",
      "authors": [
        "Guilherme J. Cavalcante",
        "JosÃ© Gabriel A. Moreira",
        "Gabriel A. B. do Nascimento",
        "Vincent Dong",
        "Alex Nguyen",
        "ThaÃ­s G. do RÃªgo",
        "Yuri Malheiros",
        "Telmo M. Silva Filho",
        "Carla R. Zeballos Torrez",
        "James C. Gee",
        "Anne Marie McCarthy",
        "Andrew D. A. Maidment",
        "Bruno Barufaldi"
      ],
      "abstract": "Foundation models hold promise for specialized medical imaging tasks, though their effectiveness in breast imaging remains underexplored. This study leverages BiomedCLIP as a foundation model to address challenges in model generalization. BiomedCLIP was adapted for automated BI-RADS breast density classification using multi-modality mammographic data (synthesized 2D images, digital mammography, and digital breast tomosynthesis). Using 96,995 images, we compared single-modality (s2D only) and multi-modality training approaches, addressing class imbalance through weighted contrastive learning. Both approaches achieved similar accuracy (multi-modality: 0.74, single-modality: 0.73), with the multi-modality model offering broader applicability across different imaging modalities and higher AUC values consistently above 0.84 across BI-RADS categories. External validation on the RSNA and EMBED datasets showed strong generalization capabilities (AUC range: 0.80-0.93). GradCAM visualizations confirmed consistent and clinically relevant attention patterns, highlighting the models interpretability and robustness. This research underscores the potential of foundation models for breast imaging applications, paving the way for future extensions for diagnostic tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŸºç¡€æ¨¡å‹(Foundation models)åœ¨ä¹³è…ºå½±åƒé¢†åŸŸçš„åº”ç”¨ï¼Œåˆ©ç”¨ BiomedCLIP å¼€å‘äº†ä¸€å¥—ç”¨äºè‡ªåŠ¨ BI-RADS ä¹³è…ºå¯†åº¦åˆ†ç±»çš„ç³»ç»Ÿã€‚è¯¥æ–¹æ³•ç»“åˆäº†åŒ…æ‹¬åˆæˆ2Då›¾åƒã€æ•°å­—ä¹³è…º X çº¿æ‘„å½±(Digital mammography)å’Œæ•°å­—ä¹³è…ºæ–­å±‚åˆæˆ(Digital breast tomosynthesis)åœ¨å†…çš„å¤šæ¨¡æ€ä¹³è…ºæ‘„å½±æ•°æ®ï¼Œé€šè¿‡ 96,995 å¼ å›¾åƒå¯¹æ¯”äº†å•æ¨¡æ€ä¸å¤šæ¨¡æ€è®­ç»ƒç­–ç•¥ï¼Œå¹¶é‡‡ç”¨åŠ æƒå¯¹æ¯”å­¦ä¹ (weighted contrastive learning)è§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¤šæ¨¡æ€æ¨¡å‹åœ¨ä¸åŒå½±åƒæ¨¡å¼ä¸‹å…·æœ‰æ›´å¹¿æ³›çš„é€‚ç”¨æ€§ï¼Œå…¶åœ¨å„ BI-RADS ç±»åˆ«çš„ AUC å€¼å§‹ç»ˆä¿æŒåœ¨ 0.84 ä»¥ä¸Šã€‚åœ¨ RSNA å’Œ EMBED ç­‰å¤–éƒ¨æ•°æ®é›†çš„éªŒè¯ä¸­ï¼Œè¯¥æ¨¡å‹å±•ç°äº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼ŒAUC èŒƒå›´è¾¾åˆ° 0.80-0.93ã€‚æ­¤å¤–ï¼Œç ”ç©¶é€šè¿‡ GradCAM å¯è§†åŒ–è¯å®äº†æ¨¡å‹å…³æ³¨åŒºåŸŸå…·æœ‰ä¸´åºŠç›¸å…³æ€§ï¼Œçªæ˜¾äº†åŸºç¡€æ¨¡å‹åœ¨ä¹³è…ºå½±åƒè¯Šæ–­ä»»åŠ¡ä¸­å…·å¤‡çš„é«˜åº¦å¯è§£é‡Šæ€§ä¸ç¨³å¥æ€§ï¼Œä¸ºæœªæ¥çš„è¯Šæ–­ä»»åŠ¡æ‰©å±•å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "5 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.17828v1",
      "published_date": "2025-11-21 22:45:50 UTC",
      "updated_date": "2025-11-21 22:45:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:51:45.190290+00:00"
    },
    {
      "arxiv_id": "2511.17818v1",
      "title": "APRIL: Annotations for Policy evaluation with Reliable Inference from LLMs",
      "title_zh": "APRILï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹å¯é æ¨ç†çš„ç­–ç•¥è¯„ä¼°æ ‡æ³¨",
      "authors": [
        "Aishwarya Mandyam",
        "Kalyani Limaye",
        "Barbara E. Engelhardt",
        "Emily Alsentzer"
      ],
      "abstract": "Off-policy evaluation (OPE) estimates the value of a contextual bandit policy prior to deployment. As such, OPE plays a critical role in ensuring safety in high-stakes domains such as healthcare. However, standard OPE approaches are limited by the size and coverage of the behavior dataset. While previous work has explored using expert-labeled counterfactual annotations to enhance dataset coverage, obtaining such annotations is expensive, limiting the scalability of prior approaches. We propose leveraging large language models (LLMs) to generate counterfactual annotations for OPE in medical domains. Our method uses domain knowledge to guide LLMs in predicting how key clinical features evolve under alternate treatments. These predicted features can then be transformed using known reward functions to create counterfactual annotations. We first evaluate the ability of several LLMs to predict clinical features across two patient subsets in MIMIC-IV, finding that state-of-the-art LLMs achieve comparable performance. Building on this capacity to predict clinical features, we generate LLM-based counterfactual annotations and incorporate them into an OPE estimator. Our empirical results analyze the benefits of counterfactual annotations under varying degrees of shift between the behavior and target policies. We find that in most cases, the LLM-based counterfactual annotations significantly improve OPE estimates up to a point. We provide an entropy-based metric to identify when additional annotations cease to be useful. Our results demonstrate that LLM-based counterfactual annotations offer a scalable approach for addressing coverage limitations in healthcare datasets, enabling safer deployment of decision-making policies in clinical settings.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†APRILï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸ºåŒ»ç–—é¢†åŸŸç”Ÿæˆåäº‹å®æ ‡æ³¨(counterfactual annotations)ä»¥æ”¹è¿›ç¦»çº¿ç­–ç•¥è¯„ä¼°(Off-policy evaluation, OPE)çš„æ–¹æ³•ã€‚é’ˆå¯¹OPEå—é™äºè¡Œä¸ºæ•°æ®é›†è§„æ¨¡å’Œè¦†ç›–èŒƒå›´çš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶é€šè¿‡é¢†åŸŸçŸ¥è¯†å¼•å¯¼LLMsé¢„æµ‹å…³é”®ä¸´åºŠç‰¹å¾åœ¨æ›¿ä»£ç–—æ³•ä¸‹çš„æ¼”å˜ï¼Œå¹¶åˆ©ç”¨å·²çŸ¥å¥–åŠ±å‡½æ•°å°†å…¶è½¬åŒ–ä¸ºåäº‹å®æ ‡æ³¨ã€‚ç ”ç©¶é¦–å…ˆåœ¨MIMIC-IVæ•°æ®é›†ä¸ŠéªŒè¯äº†é¡¶çº§LLMsé¢„æµ‹ä¸´åºŠç‰¹å¾çš„èƒ½åŠ›ï¼Œå¹¶éšåå°†ç”Ÿæˆçš„æ ‡æ³¨èå…¥OPEè¯„ä¼°å™¨ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨è¡Œä¸ºç­–ç•¥ä¸ç›®æ ‡ç­–ç•¥å­˜åœ¨åç§»çš„æƒ…å†µä¸‹ï¼ŒåŸºäºLLMsçš„åäº‹å®æ ‡æ³¨èƒ½æ˜¾è‘—æå‡OPEä¼°å€¼çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æä¾›äº†ä¸€ç§åŸºäºç†µ(entropy-based)çš„æŒ‡æ ‡ï¼Œç”¨äºè¯†åˆ«é¢å¤–æ ‡æ³¨ä½•æ—¶ä¸å†äº§ç”Ÿæ•ˆç”¨ã€‚è¯¥æ–¹æ³•ä¸ºè§£å†³åŒ»ç–—æ•°æ®é›†è¦†ç›–å±€é™æ€§æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„æ–¹æ¡ˆï¼Œæœ‰åŠ©äºåœ¨ä¸´åºŠå†³ç­–åœºæ™¯ä¸­å®ç°æ›´å®‰å…¨çš„ç­–ç•¥éƒ¨ç½²ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17818v1",
      "published_date": "2025-11-21 22:18:15 UTC",
      "updated_date": "2025-11-21 22:18:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:50:53.290320+00:00"
    },
    {
      "arxiv_id": "2511.17813v1",
      "title": "Point of Order: Action-Aware LLM Persona Modeling for Realistic Civic Simulation",
      "title_zh": "Point of Orderï¼šé¢å‘é€¼çœŸå…¬æ°‘æ¨¡æ‹Ÿçš„åŠ¨ä½œæ„ŸçŸ¥å¤§è¯­è¨€æ¨¡å‹è§’è‰²å»ºæ¨¡",
      "authors": [
        "Scott Merrill",
        "Shashank Srivastava"
      ],
      "abstract": "Large language models offer opportunities to simulate multi-party deliberation, but realistic modeling remains limited by a lack of speaker-attributed data. Transcripts produced via automatic speech recognition (ASR) assign anonymous speaker labels (e.g., Speaker_1), preventing models from capturing consistent human behavior. This work introduces a reproducible pipeline to transform public Zoom recordings into speaker-attributed transcripts with metadata like persona profiles and pragmatic action tags (e.g., [propose_motion]). We release three local government deliberation datasets: Appellate Court hearings, School Board meetings, and Municipal Council sessions. Fine-tuning LLMs to model specific participants using this \"action-aware\" data produces a 67% reduction in perplexity and nearly doubles classifier-based performance metrics for speaker fidelity and realism. Turing-style human evaluations show our simulations are often indistinguishable from real deliberations, providing a practical and scalable method for complex realistic civic simulations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Large Language Models (LLMs) åœ¨æ¨¡æ‹Ÿå¤šæ–¹å®¡è®®æ—¶å› ç¼ºä¹å‘è¨€äººæ ‡æ³¨æ•°æ®è€Œéš¾ä»¥æ•æ‰ä¸€è‡´è¡Œä¸ºçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å¯é‡å¤çš„æµæ°´çº¿ï¼Œæ—¨åœ¨å°† Zoom å½•éŸ³è½¬åŒ–ä¸ºåŒ…å« persona profiles å’Œ pragmatic action tags çš„æ ‡æ³¨è½¬å½•æ–‡æœ¬ã€‚ç ”ç©¶å›¢é˜Ÿå…¬å¼€å‘å¸ƒäº†æ¶µç›–ä¸Šè¯‰æ³•é™¢ã€æ ¡è‘£äº‹ä¼šå’Œå¸‚æ”¿å§”å‘˜ä¼šçš„ä¸‰ä¸ªåœ°æ–¹æ”¿åºœå®¡è®®æ•°æ®é›†ï¼Œä¸ºçœŸå®åœºæ™¯æ¨¡æ‹Ÿæä¾›äº†æ•°æ®åŸºç¡€ã€‚é€šè¿‡åˆ©ç”¨è¿™ç§ action-aware æ•°æ®å¯¹ LLMs è¿›è¡Œå¾®è°ƒï¼Œå®éªŒç»“æœæ˜¾ç¤ºæ¨¡å‹å›°æƒ‘åº¦ (perplexity) é™ä½äº† 67%ï¼Œä¸”å‘è¨€äººå¿ å®åº¦ä¸ç°å®æ„ŸæŒ‡æ ‡æå‡äº†è¿‘ä¸€å€ã€‚å›¾çµæµ‹è¯•å¼çš„çœŸäººè¯„ä¼°è¿›ä¸€æ­¥è¯å®ï¼Œè¯¥ç³»ç»Ÿç”Ÿæˆçš„æ¨¡æ‹Ÿè¿‡ç¨‹ä¸çœŸå®å®¡è®®å¾€å¾€éš¾ä»¥åŒºåˆ†ã€‚è¿™é¡¹å·¥ä½œä¸ºæ„å»ºå¤æ‚ä¸”çœŸå®çš„å…¬æ°‘æ¨¡æ‹Ÿæä¾›äº†ä¸€ç§å®ç”¨ä¸”å¯æ‰©å±•çš„æ–¹æ¡ˆï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹åœ¨ç¤¾ä¼šæ¨¡æ‹Ÿä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SD"
      ],
      "primary_category": "cs.CL",
      "comment": "8 pages (29 pages including appendix), 18 figures. Code and datasets are available at https://github.com/smerrillunc/action-aware-llms. Submitted to ACL 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.17813v1",
      "published_date": "2025-11-21 22:07:33 UTC",
      "updated_date": "2025-11-21 22:07:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:51:51.868080+00:00"
    },
    {
      "arxiv_id": "2511.17812v1",
      "title": "Importance-Weighted Non-IID Sampling for Flow Matching Models",
      "title_zh": "æµåŒ¹é…æ¨¡å‹çš„é‡è¦æ€§åŠ æƒéç‹¬ç«‹åŒåˆ†å¸ƒé‡‡æ ·",
      "authors": [
        "Xinshuang Liu",
        "Runfa Blark Li",
        "Shaoxiu Wei",
        "Truong Nguyen"
      ],
      "abstract": "Flow-matching models effectively represent complex distributions, yet estimating expectations of functions of their outputs remains challenging under limited sampling budgets. Independent sampling often yields high-variance estimates, especially when rare but with high-impact outcomes dominate the expectation. We propose an importance-weighted non-IID sampling framework that jointly draws multiple samples to cover diverse, salient regions of a flow's distribution while maintaining unbiased estimation via estimated importance weights. To balance diversity and quality, we introduce a score-based regularization for the diversity mechanism, which uses the score function, i.e., the gradient of the log probability, to ensure samples are pushed apart within high-density regions of the data manifold, mitigating off-manifold drift. We further develop the first approach for importance weighting of non-IID flow samples by learning a residual velocity field that reproduces the marginal distribution of the non-IID samples. Empirically, our method produces diverse, high-quality samples and accurate estimates of both importance weights and expectations, advancing the reliable characterization of flow-matching model outputs. Our code will be publicly available on GitHub.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹Flow-matchingæ¨¡å‹åœ¨æœ‰é™é‡‡æ ·é¢„ç®—ä¸‹ä¼°è®¡æœŸæœ›å€¼é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§Importance-weighted non-IID samplingæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿç‹¬ç«‹é‡‡æ ·å› ç½•è§é«˜å½±å“ç»“æœå¯¼è‡´çš„é«˜æ–¹å·®é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡è”åˆæŠ½å–å¤šä¸ªæ ·æœ¬æ¥è¦†ç›–åˆ†å¸ƒä¸­çš„å…³é”®åŒºåŸŸï¼Œå¹¶å¼•å…¥äº†Score-based regularizationï¼Œåˆ©ç”¨Score functionç¡®ä¿æ ·æœ¬åœ¨æ•°æ®æµå½¢çš„é«˜å¯†åº¦åŒºåŸŸä¿æŒå¤šæ ·æ€§ï¼Œæœ‰æ•ˆç¼“è§£äº†åç¦»æµå½¢(off-manifold drift)çš„æ¼‚ç§»ç°è±¡ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼€å‘äº†é¦–ä¸ªé’ˆå¯¹éç‹¬ç«‹åŒåˆ†å¸ƒ(non-IID)æµæ ·æœ¬çš„é‡è¦æ€§åŠ æƒæ–¹æ³•ï¼Œé€šè¿‡å­¦ä¹ Residual velocity fieldæ¥é‡ç°æ ·æœ¬çš„è¾¹ç¼˜åˆ†å¸ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸ä»…èƒ½äº§ç”Ÿå¤šæ ·ä¸”é«˜è´¨é‡çš„æ ·æœ¬ï¼Œè¿˜èƒ½æä¾›ç²¾ç¡®çš„é‡è¦æ€§æƒé‡å’ŒæœŸæœ›ä¼°è®¡ï¼Œæ˜¾è‘—æå‡äº†Flow-matchingæ¨¡å‹è¾“å‡ºçš„å¯é æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17812v1",
      "published_date": "2025-11-21 22:05:56 UTC",
      "updated_date": "2025-11-21 22:05:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:52:08.976919+00:00"
    },
    {
      "arxiv_id": "2511.17806v2",
      "title": "REXO: Indoor Multi-View Radar Object Detection via 3D Bounding Box Diffusion",
      "title_zh": "REXOï¼šåŸºäº 3D è¾¹ç•Œæ¡†æ‰©æ•£çš„å®¤å†…å¤šè§†è§’é›·è¾¾ç›®æ ‡æ£€æµ‹",
      "authors": [
        "Ryoma Yataka",
        "Pu Perry Wang",
        "Petros Boufounos",
        "Ryuhei Takahashi"
      ],
      "abstract": "Multi-view indoor radar perception has drawn attention due to its cost-effectiveness and low privacy risks. Existing methods often rely on {implicit} cross-view radar feature association, such as proposal pairing in RFMask or query-to-feature cross-attention in RETR, which can lead to ambiguous feature matches and degraded detection in complex indoor scenes. To address these limitations, we propose \\textbf{REXO} (multi-view Radar object dEtection with 3D bounding boX diffusiOn), which lifts the 2D bounding box (BBox) diffusion process of DiffusionDet into the 3D radar space. REXO utilizes these noisy 3D BBoxes to guide an {explicit} cross-view radar feature association, enhancing the cross-view radar-conditioned denoising process. By accounting for prior knowledge that the person is in contact with the ground, REXO reduces the number of diffusion parameters by determining them from this prior. Evaluated on two open indoor radar datasets, our approach surpasses state-of-the-art methods by a margin of +4.22 AP on the HIBER dataset and +11.02 AP on the MMVR dataset. The REXO implementation is available at https://github.com/merlresearch/radar-bbox-diffusion.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†REXOï¼Œè¿™æ˜¯ä¸€ç§åŸºäº3Dè¾¹ç•Œæ¡†(BBox)æ‰©æ•£æŠ€æœ¯çš„å¤šè§†å›¾å®¤å†…é›·è¾¾ç›®æ ‡æ£€æµ‹æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨éšå¼(implicit)è·¨è§†å›¾ç‰¹å¾å…³è”ä¸­å­˜åœ¨çš„ç‰¹å¾åŒ¹é…æ¨¡ç³ŠåŠæ€§èƒ½ä¸‹é™é—®é¢˜ã€‚REXOå°†DiffusionDetçš„2Dæ‰©æ•£è¿‡ç¨‹æ‰©å±•è‡³3Dé›·è¾¾ç©ºé—´ï¼Œåˆ©ç”¨å«å™ªçš„3Dè¾¹ç•Œæ¡†å¼•å¯¼æ˜¾å¼(explicit)è·¨è§†å›¾ç‰¹å¾å…³è”ï¼Œå¹¶ä¼˜åŒ–äº†é›·è¾¾æ¡ä»¶ä¸‹çš„å»å™ªè¿‡ç¨‹ã€‚é€šè¿‡å¼•å…¥â€œäººä½“ä¸åœ°é¢æ¥è§¦â€çš„å…ˆéªŒçŸ¥è¯†ï¼Œè¯¥æ¨¡å‹æœ‰æ•ˆå‡å°‘äº†æ‰©æ•£å‚æ•°çš„æ•°é‡ã€‚åœ¨HIBERå’ŒMMVRä¸¤ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒREXOçš„æ€§èƒ½ä¼˜äºå½“å‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œå‡†ç¡®ç‡åˆ†åˆ«æå‡äº†4.22 APå’Œ11.02 APï¼Œä¸ºå¤æ‚å®¤å†…ç¯å¢ƒä¸‹çš„é²æ£’é›·è¾¾æ„ŸçŸ¥æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "eess.SP"
      ],
      "primary_category": "cs.CV",
      "comment": "26 pages; Accepted to AAAI 2026; Code available at https://github.com/merlresearch/radar-bbox-diffusion",
      "pdf_url": "https://arxiv.org/pdf/2511.17806v2",
      "published_date": "2025-11-21 21:59:24 UTC",
      "updated_date": "2026-01-12 15:56:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:51:07.083538+00:00"
    },
    {
      "arxiv_id": "2511.17805v1",
      "title": "A Stitch in Time: Learning Procedural Workflow via Self-Supervised Plackett-Luce Ranking",
      "title_zh": "åŠæ—¶ç¼åˆï¼šåŸºäºè‡ªç›‘ç£ Plackett-Luce æ’åºçš„è¿‡ç¨‹åŒ–å·¥ä½œæµå­¦ä¹ ",
      "authors": [
        "Chengan Che",
        "Chao Wang",
        "Xinyue Chen",
        "Sophia Tsoka",
        "Luis C. Garcia-Peraza-Herrera"
      ],
      "abstract": "Procedural activities, ranging from routine cooking to complex surgical operations, are highly structured as a set of actions conducted in a specific temporal order. Despite their success on static images and short clips, current self-supervised learning methods often overlook the procedural nature that underpins such activities. We expose the lack of procedural awareness in current SSL methods with a motivating experiment: models pretrained on forward and time-reversed sequences produce highly similar features, confirming that their representations are blind to the underlying procedural order. To address this shortcoming, we propose PL-Stitch, a self-supervised framework that harnesses the inherent temporal order of video frames as a powerful supervisory signal. Our approach integrates two novel probabilistic objectives based on the Plackett-Luce (PL) model. The primary PL objective trains the model to sort sampled frames chronologically, compelling it to learn the global workflow progression. The secondary objective, a spatio-temporal jigsaw loss, complements the learning by capturing fine-grained, cross-frame object correlations. Our approach consistently achieves superior performance across five surgical and cooking benchmarks. Specifically, PL-Stitch yields significant gains in surgical phase recognition (e.g., +11.4 pp k-NN accuracy on Cholec80) and cooking action segmentation (e.g., +5.7 pp linear probing accuracy on Breakfast), demonstrating its effectiveness for procedural video representation learning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰è‡ªç›‘ç£å­¦ä¹  (Self-Supervised Learning) æ–¹æ³•åœ¨ç¨‹åºåŒ–æ´»åŠ¨ä¸­ç¼ºä¹æµç¨‹æ„ŸçŸ¥ã€æ— æ³•æœ‰æ•ˆåŒºåˆ†åŠ¨ä½œé¡ºåºçš„é—®é¢˜ï¼Œæå‡ºäº† PL-Stitch æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†è§†é¢‘å¸§å›ºæœ‰çš„æ—¶é—´é¡ºåºä½œä¸ºç›‘ç£ä¿¡å·ï¼Œé›†æˆäº†ä¸¤ä¸ªåŸºäº Plackett-Luce (PL) æ¨¡å‹çš„åˆ›æ–°æ¦‚ç‡ç›®æ ‡ã€‚å…¶ä¸­ï¼Œä¸»è¦çš„ PL ç›®æ ‡é€šè¿‡å¯¹é‡‡æ ·å¸§è¿›è¡Œå¹´ä»£é™…æ’åºæ¥ä¿ƒä½¿æ¨¡å‹å­¦ä¹ å…¨å±€å·¥ä½œæµ (Workflow) çš„æ¼”è¿›ï¼Œè€Œè¾…åŠ©çš„æ—¶ç©ºæ‹¼å›¾æŸå¤± (Spatio-Temporal Jigsaw Loss) åˆ™ç”¨äºæ•æ‰è·¨å¸§çš„ç»†ç²’åº¦ç‰©ä½“å…³è”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPL-Stitch åœ¨ Cholec80 æ‰‹æœ¯æ•°æ®é›†å’Œ Breakfast çƒ¹é¥ªæ•°æ®é›†ç­‰äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡è¡¨ç°ä¼˜å¼‚ã€‚å…·ä½“è€Œè¨€ï¼Œè¯¥æ–¹æ³•åœ¨æ‰‹æœ¯é˜¶æ®µè¯†åˆ«çš„ k-NN å‡†ç¡®ç‡æå‡äº† 11.4%ï¼Œåœ¨çƒ¹é¥ªåŠ¨ä½œåˆ†å‰²çš„çº¿æ€§æ¢æµ‹å‡†ç¡®ç‡ä¸Šæå‡äº† 5.7%ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹å¯¹ç¨‹åºåŒ–è§†é¢‘è¡¨ç¤ºçš„å­¦ä¹ èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "18 pages",
      "pdf_url": "https://arxiv.org/pdf/2511.17805v1",
      "published_date": "2025-11-21 21:59:22 UTC",
      "updated_date": "2025-11-21 21:59:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:52:05.866567+00:00"
    },
    {
      "arxiv_id": "2511.17803v1",
      "title": "Pillar-0: A New Frontier for Radiology Foundation Models",
      "title_zh": "Pillar-0ï¼šæ”¾å°„å­¦åŸºç¡€æ¨¡å‹çš„æ–°å‰æ²¿",
      "authors": [
        "Kumar Krishna Agrawal",
        "Longchao Liu",
        "Long Lian",
        "Michael Nercessian",
        "Natalia Harguindeguy",
        "Yufu Wu",
        "Peter Mikhael",
        "Gigin Lin",
        "Lecia V. Sequist",
        "Florian Fintelmann",
        "Trevor Darrell",
        "Yutong Bai",
        "Maggie Chung",
        "Adam Yala"
      ],
      "abstract": "Radiology plays an integral role in modern medicine, yet rising imaging volumes have far outpaced workforce growth. Foundation models offer a path toward assisting with the full spectrum of radiology tasks, but existing medical models remain limited: they process volumetric CT and MRI as low-fidelity 2D slices, discard critical grayscale contrast information, and lack evaluation frameworks that reflect real clinical practice. We introduce Pillar-0, a radiology foundation model pretrained on 42,990 abdomen-pelvis CTs, 86,411 chest CTs, 14,348 head CTs, and 11,543 breast MRIs from a large academic center, together with RATE, a scalable framework that extracts structured labels for 366 radiologic findings with near-perfect accuracy using LLMs. Across internal test sets of 14,230 abdomen-pelvis CTs, 10,646 chest CTs, 4,906 head CTs, and 1,585 breast MRIs, Pillar-0 establishes a new performance frontier, achieving mean AUROCs of 86.4, 88.0, 90.1, and 82.9, outperforming MedGemma (Google), MedImageInsight (Microsoft), Lingshu (Alibaba), and Merlin (Stanford) by 7.8-15.8 AUROC points and ranking best in 87.2\\% (319/366) tasks. Pillar-0 similarly outperforms all baselines in an external validation on the Stanford Abdominal CT dataset, including Merlin (82.2 vs 80.6 AUROC). Pillar-0 extends to tasks beyond its pretraining, such as long-horizon lung cancer risk prediction, where it improves upon the state-of-the-art Sybil by 3.0 C-index points on NLST, and generalizes with gains of 5.9 (MGH) and 1.9 (CGMH). In brain hemorrhage detection, Pillar-0 obtained a >95 AUROC when using only 1/20th of the data of the next most sample efficient baseline. Pillar-0 and RATE together provide an open, clinically rigorous foundation for building high-performance radiology systems, enabling applications that were previously infeasible due to computational, data, and evaluation constraints.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†Pillar-0ï¼Œè¿™æ˜¯ä¸€ç§å…¨æ–°çš„æ”¾å°„ç§‘åŸºç¡€æ¨¡å‹(Radiology Foundation Model)ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åŒ»å­¦æ¨¡å‹åœ¨å¤„ç†ä½“ç§¯åŒ–CTå’ŒMRIæ—¶ç”±äºé‡‡ç”¨ä½ä¿çœŸ2Dåˆ‡ç‰‡è€Œå¯¼è‡´çš„ä¿¡æ¯ä¸¢å¤±é—®é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨æ¥è‡ªå¤§å‹å­¦æœ¯ä¸­å¿ƒçš„è¶…è¿‡15ä¸‡ä¾‹å¤šéƒ¨ä½CTåŠä¹³è…ºMRIæ•°æ®è¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶åŒæ­¥å¼€å‘äº†RATEæ¡†æ¶ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)å®ç°äº†å¯¹366ç§æ”¾å°„å­¦å‘ç°(Radiologic Findings)çš„é«˜ç²¾åº¦ç»“æ„åŒ–æ ‡ç­¾æå–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPillar-0åœ¨å¤šé¡¹ä¸´åºŠä»»åŠ¡ä¸­çš„æ€§èƒ½æ˜¾è‘—ä¼˜äºMedGemmaã€MedImageInsightã€Lingshuå’ŒMerlinç­‰ä¸»æµæ¨¡å‹ï¼Œåœ¨å¤–éƒ¨éªŒè¯é›†ä¸ŠåŒæ ·ä¿æŒé¢†å…ˆåœ°ä½ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨è‚ºç™Œé£é™©é¢„æµ‹å’Œè„‘å‡ºè¡€æ£€æµ‹ç­‰æ‰©å±•ä»»åŠ¡ä¸­å±•ç°äº†æå¼ºçš„æ³›åŒ–èƒ½åŠ›å’Œæ ·æœ¬æ•ˆç‡ï¼Œæ€§èƒ½è¶…è¶Šäº†Sybilç­‰ä¸“é—¨åŒ–SOTAæ¨¡å‹ã€‚Pillar-0ä¸RATEçš„ç»“åˆä¸ºå¼€å‘é«˜æ€§èƒ½ã€ä¸´åºŠä¸¥è°¨çš„æ”¾å°„åŒ»å­¦ç³»ç»Ÿæä¾›äº†æ–°çš„æ ‡å‡†ï¼Œä½¿æ­¤å‰å—é™äºè®¡ç®—å’Œæ•°æ®çº¦æŸçš„åŒ»ç–—åº”ç”¨æˆä¸ºå¯èƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17803v1",
      "published_date": "2025-11-21 21:50:34 UTC",
      "updated_date": "2025-11-21 21:50:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:53:21.162668+00:00"
    },
    {
      "arxiv_id": "2512.04097v1",
      "title": "MultiGA: Leveraging Multi-Source Seeding in Genetic Algorithms",
      "title_zh": "MultiGAï¼šåœ¨é—ä¼ ç®—æ³•ä¸­åˆ©ç”¨å¤šæºæ’­ç§æŠ€æœ¯",
      "authors": [
        "Isabelle Diana May-Xin Ng",
        "Tharindu Cyril Weerasooriya",
        "Haitao Zhu",
        "Wei Wei"
      ],
      "abstract": "Large Language Models (LLMs) are widely used across research domains to tackle complex tasks, but their performance can vary significantly depending on the task at hand. Evolutionary algorithms, inspired by natural selection, can be used to refine solutions iteratively at inference-time. To the best of our knowledge, there has not been exploration on leveraging the collective capabilities of multi-source seeding for LLM-guided genetic algorithms. In this paper, we introduce a novel approach, MultiGA, which applies genetic algorithm principles to address complex natural language tasks and reasoning problems by sampling from a diverse population of LLMs to initialize the population. MultiGA generates a range of outputs from various parent LLMs, open source and closed source, and uses a neutral fitness function to evaluate them. Through an iterative recombination process, we mix and refine these generations until an optimal solution is achieved. We benchmark our approach using text-to-SQL code generation tasks, trip planning, GPQA benchmark for grad-level science questions, and the BBQ bias benchmark. Our results show that MultiGA converges to the accuracy of the LLM best fit for the task, and these insights lay the foundation for future research looking closer at integrating multiple LLMs for unexplored tasks in which selecting only one pre-trained model is unclear or suboptimal.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MultiGAï¼Œä¸€ç§åˆ©ç”¨å¤šæºåˆå§‹ç§ç¾¤(Multi-Source Seeding)çš„é—ä¼ ç®—æ³•(Genetic Algorithms)æ¡†æ¶ï¼Œæ—¨åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹(LLMs)å¤„ç†å¤æ‚è‡ªç„¶è¯­è¨€ä»»åŠ¡å’Œæ¨ç†é—®é¢˜çš„è¡¨ç°ã€‚MultiGA é€šè¿‡ä»åŒ…æ‹¬å¼€æºå’Œé—­æºåœ¨å†…çš„å¤šç§çˆ¶ä»£ LLMs ä¸­é‡‡æ ·æ¥åˆå§‹åŒ–ç§ç¾¤ï¼Œå¹¶ä½¿ç”¨ä¸­æ€§é€‚åº”åº¦å‡½æ•°(fitness function)è¿›è¡Œè¯„ä¼°ã€‚é€šè¿‡è¿­ä»£é‡ç»„è¿‡ç¨‹ï¼Œè¯¥æ–¹æ³•å¯¹ç”Ÿæˆçš„å€™é€‰æ–¹æ¡ˆè¿›è¡Œæ··åˆå’Œç²¾ç‚¼ï¼Œç›´è‡³è·å¾—æœ€ä¼˜è§£ã€‚ç ”ç©¶åœ¨ Text-to-SQL ä»£ç ç”Ÿæˆã€è¡Œç¨‹è§„åˆ’ã€GPQA ç§‘å­¦é—®é¢˜å’Œ BBQ åè§åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼ŒMultiGA çš„å‡†ç¡®ç‡èƒ½å¤Ÿæ”¶æ•›è‡³æœ€é€‚åˆè¯¥ä»»åŠ¡çš„ LLM æ°´å¹³ã€‚è¿™ä¸€å‘ç°ä¸ºåœ¨å•ä¸€é¢„è®­ç»ƒæ¨¡å‹é€‰æ‹©ä¸æ˜ç¡®æˆ–æ¬¡ä¼˜çš„æƒ…å†µä¸‹ï¼Œæ•´åˆå¤šä¸ª LLMs è§£å†³æ¢ç´¢æ€§ä»»åŠ¡å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.04097v1",
      "published_date": "2025-11-21 21:47:33 UTC",
      "updated_date": "2025-11-21 21:47:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:53:47.543071+00:00"
    },
    {
      "arxiv_id": "2511.21740v2",
      "title": "Decoding inner speech with an end-to-end brain-to-text neural interface",
      "title_zh": "åŸºäºç«¯åˆ°ç«¯è„‘æœºæ–‡å­—ç¥ç»æ¥å£çš„å†…éƒ¨è¨€è¯­è§£ç ",
      "authors": [
        "Yizi Zhang",
        "Linyang He",
        "Chaofei Fan",
        "Tingkai Liu",
        "Han Yu",
        "Trung Le",
        "Jingyuan Li",
        "Scott Linderman",
        "Lea Duncker",
        "Francis R Willett",
        "Nima Mesgarani",
        "Liam Paninski"
      ],
      "abstract": "Speech brain-computer interfaces (BCIs) aim to restore communication for people with paralysis by translating neural activity into text. Most systems use cascaded frameworks that decode phonemes before assembling sentences with an n-gram language model (LM), preventing joint optimization of all stages simultaneously. Here, we introduce an end-to-end Brain-to-Text (BIT) framework that translates neural activity into coherent sentences using a single differentiable neural network. Central to our approach is a cross-task, cross-species pretrained neural encoder, whose representations transfer to both attempted and imagined speech. In a cascaded setting with an n-gram LM, the pretrained encoder establishes a new state-of-the-art (SOTA) on the Brain-to-Text '24 and '25 benchmarks. Integrated end-to-end with audio large language models (LLMs) and trained with contrastive learning for cross-modal alignment, BIT reduces the word error rate (WER) of the prior end-to-end method from 24.69% to 10.22%. Notably, we find that small-scale audio LLMs markedly improve end-to-end decoding. Beyond record-setting performance, BIT aligns attempted and imagined speech embeddings to enable cross-task generalization. Altogether, our approach advances the integration of large, diverse neural datasets, paving the way for an end-to-end decoding framework that supports seamless, differentiable optimization.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†åä¸ºBITï¼ˆBrain-to-Textï¼‰çš„ç«¯åˆ°ç«¯ç¥ç»æ¥å£æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å•ä¸€å¯å¾®ç¥ç»ç½‘ç»œå°†å¤§è„‘ç¥ç»æ´»åŠ¨ç›´æ¥è½¬åŒ–ä¸ºè¿è´¯è¯­å¥ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªè·¨ä»»åŠ¡ã€è·¨ç‰©ç§é¢„è®­ç»ƒçš„ç¥ç»ç¼–ç å™¨ï¼Œå…¶ç”Ÿæˆçš„è¡¨å¾å¯æœ‰æ•ˆè¿ç§»è‡³å°è¯•æ€§è¯­è¨€ï¼ˆattempted speechï¼‰å’Œæƒ³è±¡æ€§è¯­è¨€ï¼ˆimagined speechï¼‰ã€‚é€šè¿‡ä¸éŸ³é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆaudio LLMsï¼‰é›†æˆå¹¶é‡‡ç”¨å¯¹æ¯”å­¦ä¹ è¿›è¡Œè·¨æ¨¡æ€å¯¹é½ï¼ŒBITåœ¨Brain-to-Text '24å’Œ'25åŸºå‡†æµ‹è¯•ä¸­åˆ›ä¸‹äº†æ–°çš„SOTAæ€§èƒ½è®°å½•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶å°†ç«¯åˆ°ç«¯è§£ç çš„å­—é”™è¯¯ç‡ï¼ˆWERï¼‰ä»24.69%æ˜¾è‘—é™ä½è‡³10.22%ï¼Œè¯æ˜äº†å°å‹éŸ³é¢‘LLMå¯¹æå‡è§£ç ç²¾åº¦çš„æ˜¾è‘—ä½œç”¨ã€‚æ­¤å¤–ï¼ŒBITæˆåŠŸå®ç°äº†å°è¯•æ€§ä¸æƒ³è±¡æ€§è¯­è¨€åµŒå…¥çš„å¯¹é½ä»¥æ”¯æŒè·¨ä»»åŠ¡æ³›åŒ–ï¼Œä¸ºæ•´åˆå¤§è§„æ¨¡å¤šæ ·åŒ–ç¥ç»æ•°æ®é›†å¹¶å®ç°æ— ç¼ã€å¯å¾®çš„ä¼˜åŒ–è·¯å¾„å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21740v2",
      "published_date": "2025-11-21 21:25:54 UTC",
      "updated_date": "2025-12-05 07:34:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:53:42.230767+00:00"
    },
    {
      "arxiv_id": "2511.17775v1",
      "title": "Episodic Memory in Agentic Frameworks: Suggesting Next Tasks",
      "title_zh": "æ™ºèƒ½ä½“æ¡†æ¶ä¸­çš„æƒ…å¢ƒè®°å¿†ï¼šåç»­ä»»åŠ¡å»ºè®®",
      "authors": [
        "Sandro Rama Fiorini",
        "Leonardo G. Azevedo",
        "Raphael M. Thiago",
        "Valesca M. de Sousa",
        "Anton B. Labate",
        "Viviane Torres da Silva"
      ],
      "abstract": "Agentic frameworks powered by Large Language Models (LLMs) can be useful tools in scientific workflows by enabling human-AI co-creation. A key challenge is recommending the next steps during workflow creation without relying solely on LLMs, which risk hallucination and require fine-tuning with scarce proprietary data. We propose an episodic memory architecture that stores and retrieves past workflows to guide agents in suggesting plausible next tasks. By matching current workflows with historical sequences, agents can recommend steps based on prior patterns.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)é©±åŠ¨çš„æ™ºèƒ½ä½“æ¡†æ¶åœ¨ç§‘å­¦å·¥ä½œæµä¸­æ¨èä¸‹ä¸€æ­¥ä»»åŠ¡æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ä¸ªæƒ…æ™¯è®°å¿†(Episodic Memory)æ¶æ„ã€‚ä¼ ç»Ÿæ–¹æ³•ä»…ä¾èµ–LLMså¯èƒ½å¯¼è‡´å¹»è§‰(Hallucination)é£é™©ï¼Œä¸”é€šå¸¸éœ€è¦å¯¹ç¨€ç¼ºçš„ç§æœ‰æ•°æ®è¿›è¡Œå¤æ‚çš„æ¨¡å‹å¾®è°ƒã€‚è¯¥æ¶æ„é€šè¿‡å­˜å‚¨å’Œæ£€ç´¢è¿‡å»çš„å·¥ä½œæµ(Workflows)åºåˆ—ï¼Œä¸ºæ™ºèƒ½ä½“æä¾›å»ºè®®åˆç†åç»­ä»»åŠ¡çš„å‚è€ƒä¾æ®ã€‚é€šè¿‡å°†å½“å‰å·¥ä½œæµä¸å†å²æ•°æ®è¿›è¡Œæ¨¡å¼(Patterns)åŒ¹é…ï¼Œæ™ºèƒ½ä½“èƒ½å¤ŸåŸºäºå…ˆéªŒç»éªŒè¿›è¡Œç²¾å‡†æ¨èã€‚è¿™ç§æ–¹æ³•é™ä½äº†æ¨¡å‹å¯¹ç‰¹å®šæ•°æ®å¾®è°ƒçš„ä¾èµ–ï¼Œæ˜¾è‘—å¢å¼ºäº†äººæœºå…±åˆ›ç¯å¢ƒä¸‹æ™ºèƒ½ä½“çš„è¾…åŠ©èƒ½åŠ›ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17775v1",
      "published_date": "2025-11-21 20:47:41 UTC",
      "updated_date": "2025-11-21 20:47:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:52:45.274970+00:00"
    },
    {
      "arxiv_id": "2511.17753v1",
      "title": "$Î”$-ML Ensembles for Selecting Quantum Chemistry Methods to Compute Intermolecular Interactions",
      "title_zh": "ç”¨äºé€‰æ‹©è®¡ç®—åˆ†å­é—´ç›¸äº’ä½œç”¨é‡å­åŒ–å­¦æ–¹æ³•çš„ $\\Delta$-ML é›†æˆ",
      "authors": [
        "Austin M. Wallace",
        "C. David Sherrill",
        "Giri P. Krishnan"
      ],
      "abstract": "Ab initio quantum chemical methods for accurately computing interactions between molecules have a wide range of applications but are often computationally expensive. Hence, selecting an appropriate method based on accuracy and computational cost remains a significant challenge due to varying performance of methods. In this work, we propose a framework based on an ensemble of $Î”$-ML models trained on features extracted from a pre-trained atom-pairwise neural network to predict the error of each method relative to all other methods including the ``gold standard'' coupled cluster with single, double, and perturbative triple excitations at the estimated complete basis set limit [CCSD(T)/CBS]. Our proposed approach provides error estimates across various levels of theories and identifies the computationally efficient approach for a given error range utilizing only a subset of the dataset. Further, this approach allows comparison between various theories. We demonstrate the effectiveness of our approach using an extended BioFragment dataset, which includes the interaction energies for common biomolecular fragments and small organic dimers. Our results show that the proposed framework achieves very small mean-absolute-errors below 0.1 kcal/mol regardless of the given method. Furthermore, by analyzing all-to-all $Î”$-ML models for present levels of theory, we identify method groupings that align with theoretical hypotheses, providing evidence that $Î”$-ML models can easily learn corrections from any level of theory to any other level of theory.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŸºäº $\\Delta$-ML æ¨¡å‹é›†æˆçš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åœ¨è®¡ç®—åˆ†å­é—´ç›¸äº’ä½œç”¨æ—¶ï¼Œå¦‚ä½•åœ¨è®¡ç®—æˆæœ¬ä¸ç²¾åº¦ä¹‹é—´é€‰æ‹©åˆé€‚ Quantum Chemistry Methods çš„éš¾é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ä»é¢„è®­ç»ƒ atom-pairwise neural network ä¸­æå–çš„ç‰¹å¾ï¼Œé€šè¿‡é›†æˆ $\\Delta$-ML æ¨¡å‹æ¥é¢„æµ‹å„æ–¹æ³•ç›¸å¯¹äº CCSD(T)/CBS ç­‰â€œé‡‘æ ‡å‡†â€çš„è¯¯å·®ã€‚è¿™ç§æ–¹æ³•å…è®¸åœ¨å„ç§ç†è®ºæ°´å¹³ä¹‹é—´è¿›è¡Œè¯¯å·®ä¼°è®¡å’Œæ¯”è¾ƒï¼Œå¹¶èƒ½é’ˆå¯¹ç‰¹å®šçš„è¯¯å·®èŒƒå›´è¯†åˆ«å‡ºè®¡ç®—æ•ˆç‡æœ€é«˜çš„æ–¹æ³•ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨æ‰©å±•çš„ BioFragment æ•°æ®é›†ä¸Šè¯æ˜äº†è¯¥æ–¹æ¡ˆçš„æœ‰æ•ˆæ€§ï¼Œç»“æœæ˜¾ç¤ºè¯¥æ¡†æ¶åœ¨é¢„æµ‹ä¸åŒæ–¹æ³•çš„è¯¯å·®æ—¶ï¼Œå…¶ Mean-Absolute-Errors (MAE) å‡ä½äº 0.1 kcal/molã€‚æœ€åï¼Œé€šè¿‡åˆ†æ all-to-all $\\Delta$-ML æ¨¡å‹ï¼Œç ”ç©¶è¯†åˆ«å‡ºäº†ä¸ç†è®ºå‡è®¾ä¸€è‡´çš„æ–¹æ³•åˆ†ç»„ï¼Œè¯æ˜äº†è¯¥æ¨¡å‹å…·å¤‡ä»ä»»ä½•ç†è®ºæ°´å¹³å­¦ä¹ å¹¶ä¿®æ­£è‡³å¦ä¸€æ°´å¹³çš„å¼ºå¤§èƒ½åŠ›ã€‚",
      "categories": [
        "physics.chem-ph",
        "cs.AI"
      ],
      "primary_category": "physics.chem-ph",
      "comment": "NeurIPS ML4PS 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.17753v1",
      "published_date": "2025-11-21 20:10:29 UTC",
      "updated_date": "2025-11-21 20:10:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:54:04.237493+00:00"
    },
    {
      "arxiv_id": "2511.19466v1",
      "title": "SG-OIF: A Stability-Guided Online Influence Framework for Reliable Vision Data",
      "title_zh": "SG-OIFï¼šé¢å‘å¯é è§†è§‰æ•°æ®çš„ç¨³å®šæ€§å¼•å¯¼åœ¨çº¿å½±å“åŠ›æ¡†æ¶",
      "authors": [
        "Penghao Rao",
        "Runmin Jiang",
        "Min Xu"
      ],
      "abstract": "Approximating training-point influence on test predictions is critical for deploying deep-learning vision models, essential for locating noisy data. Though the influence function was proposed for attributing how infinitesimal up-weighting or removal of individual training examples affects model outputs, its implementation is still challenging in deep-learning vision models: inverse-curvature computations are expensive, and training non-stationarity invalidates static approximations. Prior works use iterative solvers and low-rank surrogates to reduce cost, but offline computation lags behind training dynamics, and missing confidence calibration yields fragile rankings that misidentify critical examples. To address these challenges, we introduce a Stability-Guided Online Influence Framework (SG-OIF), the first framework that treats algorithmic stability as a real-time controller, which (i) maintains lightweight anchor IHVPs via stochastic Richardson and preconditioned Neumann; (ii) proposes modular curvature backends to modulate per-example influence scores using stability-guided residual thresholds, anomaly gating, and confidence. Experimental results show that SG-OIF achieves SOTA (State-Of-The-Art) on noise-label and out-of-distribution detection tasks across multiple datasets with various corruption. Notably, our approach achieves 91.1\\% accuracy in the top 1\\% prediction samples on the CIFAR-10 (20\\% asym), and gets 99.8\\% AUPR score on MNIST, effectively demonstrating that this framework is a practical controller for online influence estimation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦å­¦ä¹ è§†è§‰æ¨¡å‹ä¸­è®­ç»ƒç‚¹å½±å“ï¼ˆtraining-point influenceï¼‰ä¼°è®¡é¢ä¸´çš„é«˜æ˜‚è®¡ç®—æˆæœ¬å’Œè®­ç»ƒéå¹³ç¨³æ€§æŒ‘æˆ˜ï¼Œæå‡ºäº†SG-OIFï¼Œå³ç¨³å®šæ€§å¼•å¯¼çš„åœ¨çº¿å½±å“åŠ›æ¡†æ¶ï¼ˆStability-Guided Online Influence Frameworkï¼‰ã€‚ä½œä¸ºé¦–ä¸ªå°†ç®—æ³•ç¨³å®šæ€§è§†ä¸ºå®æ—¶æ§åˆ¶å™¨çš„æ¡†æ¶ï¼Œå®ƒé€šè¿‡éšæœºRichardsonå’Œé¢„æ¡ä»¶Neumannæ–¹æ³•ç»´æŒè½»é‡åŒ–çš„é”šç‚¹IHVPsã€‚åŒæ—¶ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†æ¨¡å—åŒ–æ›²ç‡åç«¯ï¼Œåˆ©ç”¨ç¨³å®šæ€§å¼•å¯¼çš„æ®‹å·®é˜ˆå€¼ã€å¼‚å¸¸é—¨æ§å’Œç½®ä¿¡åº¦æ¥ç²¾å‡†è°ƒèŠ‚æ¯ä¸ªæ ·æœ¬çš„å½±å“åŠ›å¾—åˆ†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSG-OIFåœ¨å¤šé¡¹å™ªå£°æ ‡ç­¾å’Œåˆ†å¸ƒå¤–æ£€æµ‹ï¼ˆout-of-distribution detectionï¼‰ä»»åŠ¡ä¸­å‡è¾¾åˆ°äº†SOTAæ€§èƒ½ã€‚åœ¨CIFAR-10ï¼ˆ20%éå¯¹ç§°å™ªå£°ï¼‰å‰1%é¢„æµ‹æ ·æœ¬ä¸­å…¶å‡†ç¡®ç‡è¾¾åˆ°91.1%ï¼Œå¹¶åœ¨MNISTä¸Šè·å¾—äº†99.8%çš„AUPRåˆ†æ•°ã€‚è¿™å……åˆ†è¯æ˜äº†è¯¥æ¡†æ¶æ˜¯è¿›è¡Œå¯é åœ¨çº¿å½±å“åŠ›è¯„ä¼°çš„å®ç”¨æ§åˆ¶å™¨ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19466v1",
      "published_date": "2025-11-21 19:58:54 UTC",
      "updated_date": "2025-11-21 19:58:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:53:05.579220+00:00"
    },
    {
      "arxiv_id": "2511.19465v1",
      "title": "Hidden markov model to predict tourists visited place",
      "title_zh": "åŸºäºéšé©¬å°”å¯å¤«æ¨¡å‹çš„æ¸¸å®¢åˆ°è®¿åœ°ç‚¹é¢„æµ‹",
      "authors": [
        "Theo Demessance",
        "Chongke Bi",
        "Sonia Djebali",
        "Guillaume Guerard"
      ],
      "abstract": "Nowadays, social networks are becoming a popular way of analyzing tourist behavior, thanks to the digital traces left by travelers during their stays on these networks. The massive amount of data generated; by the propensity of tourists to share comments and photos during their trip; makes it possible to model their journeys and analyze their behavior. Predicting the next movement of tourists plays a key role in tourism marketing to understand demand and improve decision support. In this paper, we propose a method to understand and to learn tourists' movements based on social network data analysis to predict future movements. The method relies on a machine learning grammatical inference algorithm. A major contribution in this paper is to adapt the grammatical inference algorithm to the context of big data. Our method produces a hidden Markov model representing the movements of a group of tourists. The hidden Markov model is flexible and editable with new data. The capital city of France, Paris is selected to demonstrate the efficiency of the proposed methodology.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨ç¤¾äº¤ç½‘ç»œæ•°å­—è½¨è¿¹æ¥é¢„æµ‹æ¸¸å®¢æœªæ¥ç§»åŠ¨è·¯å¾„çš„æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡åˆ†ææ¸¸å®¢åˆ†äº«çš„è¯„è®ºå’Œç…§ç‰‡æ¥ç†è§£å…¶è¡Œä¸ºæ¨¡å¼å¹¶ä¼˜åŒ–å†³ç­–æ”¯æŒã€‚ç ”ç©¶çš„æ ¸å¿ƒè´¡çŒ®åœ¨äºå°†æœºå™¨å­¦ä¹ æ–‡æ³•æ¨ç†(Grammatical Inference)ç®—æ³•è¿›è¡Œæ”¹è¿›ï¼Œä½¿å…¶èƒ½å¤Ÿé«˜æ•ˆå¤„ç†å¤§æ•°æ®(Big Data)ç¯å¢ƒä¸‹çš„å¤æ‚ä¿¡æ¯ã€‚é€šè¿‡è¯¥ç®—æ³•ï¼Œç ”ç©¶æ„å»ºäº†ä¸€ä¸ªèƒ½å¤Ÿè¡¨å¾æ¸¸å®¢ç¾¤ä½“ç§»åŠ¨ç‰¹å¾çš„éšé©¬å°”å¯å¤«æ¨¡å‹(Hidden Markov Model, HMM)ã€‚è¯¥æ¨¡å‹å…·æœ‰æé«˜çš„çµæ´»æ€§ï¼Œæ”¯æŒæ ¹æ®æ–°äº§ç”Ÿçš„åŠ¨æ€æ•°æ®è¿›è¡Œè°ƒæ•´å’Œç¼–è¾‘ã€‚ç ”ç©¶ä»¥æ³•å›½å·´é»ä½œä¸ºæ¡ˆä¾‹è¿›è¡Œå®éªŒï¼ŒéªŒè¯äº†è¯¥æ¨¡å‹åœ¨æ•æ‰æ—…æ¸¸è·¯å¾„ç‰¹å¾åŠé¢„æµ‹ä½ç½®å˜åŠ¨æ–¹é¢çš„å‡†ç¡®æ€§ä¸å®ç”¨æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19465v1",
      "published_date": "2025-11-21 19:58:17 UTC",
      "updated_date": "2025-11-21 19:58:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:53:24.688628+00:00"
    },
    {
      "arxiv_id": "2511.17747v1",
      "title": "AEGIS: Preserving privacy of 3D Facial Avatars with Adversarial Perturbations",
      "title_zh": "AEGISï¼šåŸºäºå¯¹æŠ—æ‰°åŠ¨çš„3Dé¢éƒ¨åŒ–èº«éšç§ä¿æŠ¤",
      "authors": [
        "Dawid Wolkiewicz",
        "Anastasiya Pechko",
        "PrzemysÅ‚aw Spurek",
        "Piotr Syga"
      ],
      "abstract": "The growing adoption of photorealistic 3D facial avatars, particularly those utilizing efficient 3D Gaussian Splatting representations, introduces new risks of online identity theft, especially in systems that rely on biometric authentication. While effective adversarial masking methods have been developed for 2D images, a significant gap remains in achieving robust, viewpoint-consistent identity protection for dynamic 3D avatars. To address this, we present AEGIS, the first privacy-preserving identity masking framework for 3D Gaussian Avatars that maintains the subject's perceived characteristics. Our method aims to conceal identity-related facial features while preserving the avatar's perceptual realism and functional integrity. AEGIS applies adversarial perturbations to the Gaussian color coefficients, guided by a pre-trained face verification network, ensuring consistent protection across multiple viewpoints without retraining or modifying the avatar's geometry. AEGIS achieves complete de-identification, reducing face retrieval and verification accuracy to 0%, while maintaining high perceptual quality (SSIM = 0.9555, PSNR = 35.52 dB). It also preserves key facial attributes such as age, race, gender, and emotion, demonstrating strong privacy protection with minimal visual distortion.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ 3D Gaussian Splatting è¡¨å¾çš„å†™å® 3D é¢éƒ¨åŒ–èº«åœ¨ç”Ÿç‰©è¯†åˆ«ç³»ç»Ÿä¸­å¼•å‘çš„èº«ä»½ç›—çªƒé£é™©ï¼Œæå‡ºäº†é¦–ä¸ªéšç§ä¿æŠ¤èº«ä»½é®è”½æ¡†æ¶ AEGISã€‚è¯¥æ¡†æ¶æ—¨åœ¨éšè—èº«ä»½ç›¸å…³çš„é¢éƒ¨ç‰¹å¾ï¼ŒåŒæ—¶ç»´æŒåŒ–èº«çš„æ„Ÿå®˜å†™å®æ€§ä¸åŠŸèƒ½å®Œæ•´æ€§ã€‚AEGIS åœ¨é¢„è®­ç»ƒäººè„¸éªŒè¯ç½‘ç»œçš„æŒ‡å¯¼ä¸‹ï¼Œå°†å¯¹æŠ—æ€§æ‰°åŠ¨ (Adversarial Perturbations) ç›´æ¥åº”ç”¨äºé«˜æ–¯é¢œè‰²ç³»æ•°ï¼Œæ— éœ€é‡æ–°è®­ç»ƒæˆ–ä¿®æ”¹åŒ–èº«å‡ ä½•ç»“æ„å³å¯ç¡®ä¿å¤šè§†ç‚¹ä¸‹çš„ä¸€è‡´ä¿æŠ¤ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAEGIS å®ç°äº†å®Œå…¨çš„å»èº«ä»½åŒ–ï¼Œå°†äººè„¸æ£€ç´¢å’ŒéªŒè¯å‡†ç¡®ç‡é™è‡³ 0%ï¼ŒåŒæ—¶ä¿æŒäº†æé«˜çš„æ„ŸçŸ¥è´¨é‡ï¼ˆSSIM = 0.9555, PSNR = 35.52 dBï¼‰ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆä¿ç•™å¹´é¾„ã€ç§æ—ã€æ€§åˆ«åŠæƒ…ç»ªç­‰å…³é”®é¢éƒ¨å±æ€§ï¼Œè¯æ˜äº†å…¶åœ¨æå°è§†è§‰å¤±çœŸä¸‹å®ç°å¼ºåŠ›éšç§ä¿æŠ¤çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17747v1",
      "published_date": "2025-11-21 19:57:28 UTC",
      "updated_date": "2025-11-21 19:57:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:53:40.227700+00:00"
    },
    {
      "arxiv_id": "2511.17743v1",
      "title": "AI- and Ontology-Based Enhancements to FMEA for Advanced Systems Engineering: Current Developments and Future Directions",
      "title_zh": "åŸºäºäººå·¥æ™ºèƒ½ä¸æœ¬ä½“çš„é«˜çº§ç³»ç»Ÿå·¥ç¨‹ FMEA å¢å¼ºï¼šå‘å±•ç°çŠ¶ä¸æœªæ¥æ–¹å‘",
      "authors": [
        "Haytham Younus",
        "Sohag Kabir",
        "Felician Campean",
        "Pascal Bonnaud",
        "David Delaux"
      ],
      "abstract": "This article presents a state-of-the-art review of recent advances aimed at transforming traditional Failure Mode and Effects Analysis (FMEA) into a more intelligent, data-driven, and semantically enriched process. As engineered systems grow in complexity, conventional FMEA methods, largely manual, document-centric, and expert-dependent, have become increasingly inadequate for addressing the demands of modern systems engineering. We examine how techniques from Artificial Intelligence (AI), including machine learning and natural language processing, can transform FMEA into a more dynamic, data-driven, intelligent, and model-integrated process by automating failure prediction, prioritisation, and knowledge extraction from operational data. In parallel, we explore the role of ontologies in formalising system knowledge, supporting semantic reasoning, improving traceability, and enabling cross-domain interoperability. The review also synthesises emerging hybrid approaches, such as ontology-informed learning and large language model integration, which further enhance explainability and automation. These developments are discussed within the broader context of Model-Based Systems Engineering (MBSE) and function modelling, showing how AI and ontologies can support more adaptive and resilient FMEA workflows. We critically analyse a range of tools, case studies, and integration strategies, while identifying key challenges related to data quality, explainability, standardisation, and interdisciplinary adoption. By leveraging AI, systems engineering, and knowledge representation using ontologies, this review offers a structured roadmap for embedding FMEA within intelligent, knowledge-rich engineering environments.",
      "tldr_zh": "è¯¥ç»¼è¿°æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨äººå·¥æ™ºèƒ½ (Artificial Intelligence, AI) å’Œæœ¬ä½“è®º (Ontology) æŠ€æœ¯å°†ä¼ ç»Ÿçš„å¤±æ•ˆæ¨¡å¼ä¸å½±å“åˆ†æ (Failure Mode and Effects Analysis, FMEA) è½¬åŒ–ä¸ºæ™ºèƒ½ä¸”æ•°æ®é©±åŠ¨çš„è¿‡ç¨‹ã€‚æ–‡ç« è¯¦ç»†åˆ†æäº†æœºå™¨å­¦ä¹  (machine learning) å’Œè‡ªç„¶è¯­è¨€å¤„ç† (natural language processing) åœ¨è‡ªåŠ¨åŒ–å¤±æ•ˆé¢„æµ‹ã€é£é™©ä¼˜å…ˆçº§æ’åºåŠçŸ¥è¯†æå–æ–¹é¢çš„å…³é”®åº”ç”¨ã€‚åŒæ—¶ï¼Œç ”ç©¶å¼ºè°ƒäº†æœ¬ä½“è®ºåœ¨å½¢å¼åŒ–ç³»ç»ŸçŸ¥è¯†ã€æ”¯æŒè¯­ä¹‰æ¨ç† (semantic reasoning) ä»¥åŠå¢å¼ºè·¨é¢†åŸŸäº’æ“ä½œæ€§æ–¹é¢çš„æ ¸å¿ƒä½œç”¨ã€‚æ­¤å¤–ï¼Œç»¼è¿°è¿˜è®¨è®ºäº†æœ¬ä½“å¢å¼ºå­¦ä¹ ä¸å¤§è¯­è¨€æ¨¡å‹ (large language model) é›†æˆç­‰æ–°å…´æ··åˆæ–¹æ³•åœ¨æå‡ FMEA è‡ªåŠ¨åŒ–ä¸å¯è§£é‡Šæ€§æ–¹é¢çš„æ½œåŠ›ã€‚è¿™äº›æŠ€æœ¯è¿›å±•è¢«ç½®äºåŸºäºæ¨¡å‹çš„ç³»ç»Ÿå·¥ç¨‹ (Model-Based Systems Engineering, MBSE) æ¡†æ¶ä¸‹ï¼Œæ—¨åœ¨æ„å»ºæ›´å…·é€‚åº”æ€§å’ŒéŸ§æ€§çš„å·¥ç¨‹å·¥ä½œæµã€‚é€šè¿‡å¯¹ç°æœ‰å·¥å…·å’Œæ¡ˆä¾‹çš„æ‰¹åˆ¤æ€§åˆ†æï¼Œæ–‡ç« è¯†åˆ«äº†æ•°æ®è´¨é‡ã€æ ‡å‡†åŒ–å’Œè·¨å­¦ç§‘é‡‡ç”¨ç­‰å…³é”®æŒ‘æˆ˜ï¼Œå¹¶ä¸ºæ™ºèƒ½ã€çŸ¥è¯†å¯†é›†å‹å·¥ç¨‹ç¯å¢ƒä¸‹çš„ FMEA å‘å±•æä¾›äº†ç»“æ„åŒ–çš„è·¯çº¿å›¾ã€‚",
      "categories": [
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.AI",
      "comment": "This manuscript is based on research undertaken by our doctoral student at the University of Bradford. The associated PhD thesis has been formally submitted to the University and is currently awaiting final examination. The review article is being shared on arXiv to make the review accessible to the research community while the thesis examination process is ongoing",
      "pdf_url": "https://arxiv.org/pdf/2511.17743v1",
      "published_date": "2025-11-21 19:51:06 UTC",
      "updated_date": "2025-11-21 19:51:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:53:40.048818+00:00"
    },
    {
      "arxiv_id": "2511.19464v1",
      "title": "Temperature in SLMs: Impact on Incident Categorization in On-Premises Environments",
      "title_zh": "SLM ä¸­çš„æ¸©åº¦å‚æ•°ï¼šå¯¹æœ¬åœ°éƒ¨ç½²ç¯å¢ƒä¸‹äº‹ä»¶åˆ†ç±»çš„å½±å“",
      "authors": [
        "Marcio Pohlmann",
        "Alex Severo",
        "GeftÃ© Almeida",
        "Diego Kreutz",
        "Tiago Heinrich",
        "LourenÃ§o Pereira"
      ],
      "abstract": "SOCs and CSIRTs face increasing pressure to automate incident categorization, yet the use of cloud-based LLMs introduces costs, latency, and confidentiality risks. We investigate whether locally executed SLMs can meet this challenge. We evaluated 21 models ranging from 1B to 20B parameters, varying the temperature hyperparameter and measuring execution time and precision across two distinct architectures. The results indicate that temperature has little influence on performance, whereas the number of parameters and GPU capacity are decisive factors.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨æœ¬åœ°(On-Premises)ç¯å¢ƒä¸‹ä½¿ç”¨å°è¯­è¨€æ¨¡å‹(SLMs)è‡ªåŠ¨å¤„ç†å®‰å…¨äº‹ä»¶åˆ†ç±»(Incident Categorization)çš„å¯è¡Œæ€§ï¼Œæ—¨åœ¨è§„é¿äº‘ç«¯æ¨¡å‹å¸¦æ¥çš„æœºå¯†æ€§é£é™©ã€é«˜æ˜‚æˆæœ¬å’Œå»¶è¿Ÿé—®é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿè¯„ä¼°äº†21ä¸ªå‚æ•°è§„æ¨¡ä»1Båˆ°20Bä¸ç­‰çš„æ¨¡å‹ï¼Œé€šè¿‡æ”¹å˜æ¸©åº¦(Temperature)è¶…å‚æ•°ï¼Œåœ¨ä¸¤ç§ä¸åŒæ¶æ„ä¸Šè¯¦ç»†æµ‹é‡äº†æ‰§è¡Œæ—¶é—´å’Œå‡†ç¡®ç‡(Precision)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ¸©åº¦è®¾ç½®å¯¹æ¨¡å‹åœ¨åˆ†ç±»ä»»åŠ¡ä¸­çš„æ€§èƒ½è¡¨ç°å½±å“å¾®ä¹å…¶å¾®ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ¨¡å‹çš„å‚æ•°æ•°é‡å’ŒGPUå®¹é‡æ‰æ˜¯å†³å®šæ€§èƒ½è¡¨ç°çš„å†³å®šæ€§å› ç´ ã€‚è¯¥ç ”ç©¶ä¸ºå®‰å…¨è¿è¥ä¸­å¿ƒ(SOCs)å’Œè®¡ç®—æœºå®‰å…¨äº‹ä»¶å“åº”å°ç»„(CSIRTs)åœ¨æœ¬åœ°ç¯å¢ƒéƒ¨ç½²é«˜æ•ˆã€å®‰å…¨çš„è‡ªåŠ¨åŒ–äº‹ä»¶åˆ†ç±»æ–¹æ¡ˆæä¾›äº†é‡è¦çš„å®è¯ä¾æ®ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.CR",
        "cs.LG",
        "cs.PF"
      ],
      "primary_category": "cs.DC",
      "comment": "5 pages, 3 figures, 2 tables, submitted to ERRC/WRSeg 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.19464v1",
      "published_date": "2025-11-21 19:37:09 UTC",
      "updated_date": "2025-11-21 19:37:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:54:08.329226+00:00"
    },
    {
      "arxiv_id": "2511.17729v3",
      "title": "M^3-Bench: Multi-Modal, Multi-Hop, Multi-Threaded Tool-Using MLLM Agent Benchmark",
      "title_zh": "M^3-Benchï¼šå¤šæ¨¡æ€ã€å¤šè·³ã€å¤šçº¿ç¨‹å·¥å…·è°ƒç”¨ MLLM æ™ºèƒ½ä½“è¯„æµ‹åŸºå‡†",
      "authors": [
        "Yang Zhou",
        "Mingyu Zhao",
        "Zhenting Wang",
        "Difei Gu",
        "Bangwei Guo",
        "Ruosong Ye",
        "Ligong Han",
        "Can Jin",
        "Dimitris N. Metaxas"
      ],
      "abstract": "We present M^3-Bench, the first benchmark for evaluating multimodal tool use under the Model Context Protocol. The benchmark targets realistic, multi-hop and multi-threaded workflows that require visual grounding and textual reasoning, cross-tool dependencies, and persistence of intermediate resources across steps. We introduce a similarity-driven alignment that serializes each tool call, embeds signatures with a sentence encoder, and performs similarity-bucketed Hungarian matching to obtain auditable one-to-one correspondences. On top of this alignment, we report interpretable metrics that decouple semantic fidelity from workflow consistency. The benchmark spans 28 servers with 231 tools, and provides standardized trajectories curated through an Executor & Judge pipeline with human verification; an auxiliary four large language models (LLMs) judge ensemble reports end-task Task Completion and information grounding. Evaluations of representative state-of-the-art Multimodal LLMs (MLLMs) reveal persistent gaps in multimodal MCP tool use, particularly in argument fidelity and structure consistency, underscoring the need for methods that jointly reason over images, text, and tool graphs. Our Benchmark's anonymous repository is at https://github.com/EtaYang10th/Open-M3-Bench",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† M^3-Benchï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºè¯„ä¼°åœ¨ Model Context Protocol (MCP) ä¸‹å¤šæ¨¡æ€å·¥å…·ä½¿ç”¨èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†é’ˆå¯¹ç°å®ä¸­éœ€è¦è§†è§‰åŸºå‡† (visual grounding) å’Œæ–‡æœ¬æ¨ç†çš„å¤šè·³ (multi-hop) åŠå¤šçº¿ç¨‹ (multi-threaded) å·¥ä½œæµï¼Œæ¶µç›–äº†è·¨å·¥å…·ä¾èµ–å’Œä¸­é—´èµ„æºçš„æŒä¹…åŒ–å¤„ç†ã€‚ç ”ç©¶å¼•å…¥äº†ä¸€ç§ç›¸ä¼¼æ€§é©±åŠ¨çš„å¯¹é½æ–¹æ³•ï¼Œé€šè¿‡å¯¹å·¥å…·è°ƒç”¨è¿›è¡Œåºåˆ—åŒ–ã€ç­¾ååµŒå…¥ä»¥åŠæ‰§è¡Œç›¸ä¼¼æ€§åˆ†æ¡¶çš„åŒˆç‰™åˆ©åŒ¹é… (Hungarian matching)ï¼Œå®ç°äº†å¯å®¡è®¡çš„ä¸€å¯¹ä¸€å¯¹åº”å…³ç³»ã€‚åŸºäºæ­¤å¯¹é½æ–¹å¼ï¼ŒM^3-Bench æä¾›äº†å¯è§£é‡Šçš„æŒ‡æ ‡ï¼Œå°†è¯­ä¹‰å¿ å®åº¦ (semantic fidelity) ä¸å·¥ä½œæµä¸€è‡´æ€§ (workflow consistency) è¿›è¡Œè§£è€¦è¯„ä¼°ã€‚å®éªŒæ¶µç›–äº† 28 ä¸ªæœåŠ¡å™¨å’Œ 231 ä¸ªå·¥å…·ï¼Œå¹¶æä¾›äº†ä¸€å¥—ç»è¿‡äººå·¥éªŒè¯ã€ç”±æ‰§è¡Œå™¨ä¸è£åˆ¤ç®¡çº¿ (Executor & Judge pipeline) ç²¾é€‰çš„æ ‡å‡†è½¨è¿¹æ•°æ®ã€‚å¯¹å½“å‰æœ€å…ˆè¿›çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) çš„è¯„ä¼°æ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨ MCP å·¥å…·ä½¿ç”¨ä¸­ä»å­˜åœ¨æŒä¹…æ€§å·®è·ï¼Œç‰¹åˆ«æ˜¯åœ¨å‚æ•°å¿ å®åº¦ (argument fidelity) å’Œç»“æ„ä¸€è‡´æ€§æ–¹é¢ã€‚è¯¥åŸºå‡†å¼ºè°ƒäº†å¼€å‘èƒ½å¤ŸåŒæ—¶å¯¹å›¾åƒã€æ–‡æœ¬å’Œå·¥å…·å›¾ (tool graphs) è¿›è¡Œè”åˆæ¨ç†çš„æ–¹æ³•çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17729v3",
      "published_date": "2025-11-21 19:27:02 UTC",
      "updated_date": "2025-12-13 20:18:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:54:26.542692+00:00"
    },
    {
      "arxiv_id": "2511.17728v1",
      "title": "Ternary Gamma Semirings as a Novel Algebraic Framework for Learnable Symbolic Reasoning",
      "title_zh": "ä¸‰å…ƒ Gamma åŠç¯ï¼šé¢å‘å¯å­¦ä¹ ç¬¦å·æ¨ç†çš„æ–°å‹ä»£æ•°æ¡†æ¶",
      "authors": [
        "Chandrasekhar Gokavarapu",
        "D. Madhusudhana Rao"
      ],
      "abstract": "Binary semirings such as the tropical, log, and probability semirings form a core algebraic tool in classical and modern neural inference systems, supporting tasks like Viterbi decoding, dynamic programming, and probabilistic reasoning. However, these structures rely on a binary multiplication operator and therefore model only pairwise interactions. Many symbolic AI tasks are inherently triadic, including subject-predicate-object relations in knowledge graphs, logical rules involving two premises and one conclusion, and multi-entity dependencies in structured decision processes. Existing neural architectures usually approximate these interactions by flattening or factorizing them into binary components, which weakens inductive structure, distorts relational meaning, and reduces interpretability.\n  This paper introduces the Neural Ternary Semiring (NTS), a learnable and differentiable algebraic framework grounded in the theory of ternary Gamma-semirings. The central idea is to replace the usual binary product with a native ternary operator implemented by neural networks and guided by algebraic regularizers enforcing approximate associativity and distributivity. This construction allows triadic relationships to be represented directly rather than reconstructed from binary interactions.\n  We establish a soundness result showing that, when algebraic violations vanish during training, the learned operator converges to a valid ternary Gamma-semiring. We also outline an evaluation strategy for triadic reasoning tasks such as knowledge-graph completion and rule-based inference. These insights demonstrate that ternary Gamma-semirings provide a mathematically principled and practically effective foundation for learnable symbolic reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Neural Ternary Semiring (NTS)ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºternary Gamma-semiringsç†è®ºçš„å¯å­¦ä¹ ä¸”å¯å¾®çš„ä»£æ•°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»ŸäºŒå…ƒåŠç¯(binary semirings)åœ¨å¤„ç†çŸ¥è¯†å›¾è°±ä¸‰å…ƒå…³ç³»å’Œé€»è¾‘æ¨ç†ç­‰æœ¬è´¨ä¸Šå±äºä¸‰å…ƒäº¤äº’ä»»åŠ¡æ—¶çš„å±€é™æ€§ã€‚NTSçš„æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨ç¥ç»ç½‘ç»œå®ç°çš„åŸç”Ÿä¸‰å…ƒç®—å­å–ä»£ä¼ ç»Ÿçš„äºŒå…ƒä¹˜æ³•ï¼Œå¹¶é€šè¿‡ä»£æ•°æ­£åˆ™åŒ–é¡¹å¼ºåˆ¶å®ç°è¿‘ä¼¼çš„ç»“åˆå¾‹(associativity)å’Œåˆ†é…å¾‹(distributivity)ã€‚è¯¥æ¡†æ¶å…è®¸ç›´æ¥è¡¨ç¤ºä¸‰å…ƒå…³ç³»ï¼Œæœ‰æ•ˆé¿å…äº†å°†ä¸‰å…ƒäº¤äº’åˆ†è§£ä¸ºäºŒå…ƒç»„ä»¶æ—¶å¯¼è‡´çš„å½’çº³ç»“æ„å‰Šå¼±å’Œå¯è§£é‡Šæ€§é™ä½ã€‚ç†è®ºåˆ†æè¯æ˜ï¼Œå½“è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä»£æ•°è¿è§„æ¶ˆé™¤æ—¶ï¼Œå­¦ä¹ åˆ°çš„ç®—å­ä¼šæ”¶æ•›äºæœ‰æ•ˆçš„ternary Gamma-semiringã€‚é€šè¿‡åœ¨çŸ¥è¯†å›¾è°±è¡¥å…¨(knowledge-graph completion)å’ŒåŸºäºè§„åˆ™çš„æ¨ç†ä»»åŠ¡ä¸Šçš„è¯„ä¼°ï¼Œç ”ç©¶å±•ç¤ºäº†ä¸‰å…ƒåŠç¯æ¡†æ¶åœ¨å¯å­¦ä¹ ç¬¦å·æ¨ç†ä¸­å…·æœ‰æ•°å­¦ä¸¥è°¨æ€§å’Œå®è·µæœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "math.RA",
        "cs.AI"
      ],
      "primary_category": "math.RA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17728v1",
      "published_date": "2025-11-21 19:26:18 UTC",
      "updated_date": "2025-11-21 19:26:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:54:34.233073+00:00"
    },
    {
      "arxiv_id": "2511.17714v4",
      "title": "Learning the Value of Value Learning",
      "title_zh": "æ¢ç©¶ä»·å€¼å­¦ä¹ çš„ä»·å€¼",
      "authors": [
        "Alex John London",
        "Aydin Mohseni"
      ],
      "abstract": "Standard decision frameworks address uncertainty about facts but assume fixed options and values. We extend the Jeffrey-Bolker framework to model refinements in values and prove a value-of-information theorem for axiological refinement. In multi-agent settings, we establish that mutual refinement will characteristically transform zero-sum games into positive-sum interactions and yield Pareto-improvements in Nash bargaining. These results show that a framework of rational choice can be extended to model value refinement. By unifying epistemic and axiological refinement under a single formalism, we broaden the conceptual foundations of rational choice and illuminate the normative status of ethical deliberation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ‰©å±•äº† Jeffrey-Bolker frameworkï¼Œé€šè¿‡å»ºç«‹ä»·å€¼ç»†åŒ– (axiological refinement) æ¨¡å‹ï¼Œè§£å†³äº†ä¼ ç»Ÿå†³ç­–æ¡†æ¶ä¸­å‡è®¾ä»·å€¼å›ºå®šä¸å˜çš„å±€é™æ€§ã€‚ä½œè€…è¯æ˜äº†é’ˆå¯¹ axiological refinement çš„ä¿¡æ¯ä»·å€¼å®šç† (value-of-information theorem)ï¼Œå±•ç¤ºäº†ä»·å€¼å­¦ä¹ åœ¨ç†æ€§å†³ç­–ä¸­çš„ç†è®ºä»·å€¼ã€‚åœ¨å¤šæ™ºèƒ½ä½“ (multi-agent) ç¯å¢ƒä¸‹ï¼Œç ”ç©¶å‘ç°ç›¸äº’ç»†åŒ–èƒ½å°†é›¶å’Œåšå¼ˆ (zero-sum games) è½¬åŒ–ä¸ºæ­£å’Œäº’åŠ¨ (positive-sum interactions)ï¼Œå¹¶åœ¨çº³ä»€è®¨ä»·è¿˜ä»· (Nash bargaining) ä¸­å®ç°å¸•ç´¯æ‰˜æ”¹è¿› (Pareto-improvements)ã€‚é€šè¿‡åœ¨ç»Ÿä¸€çš„å½¢å¼åŒ–ä½“ç³»ä¸‹èåˆè®¤è¯†è®º (epistemic) ä¸ä»·å€¼è®ºçš„ç»†åŒ–ï¼Œè¯¥å·¥ä½œæœ‰æ•ˆæ‹“å®½äº†ç†æ€§é€‰æ‹© (rational choice) çš„ç†è®ºåŸºç¡€ï¼Œå¹¶é˜æ˜äº†ä¼¦ç†æ·±æ€ (ethical deliberation) çš„è§„èŒƒæ€§åœ°ä½ã€‚",
      "categories": [
        "cs.AI",
        "cs.GT"
      ],
      "primary_category": "cs.AI",
      "comment": "19 pages, 6 figures, mathematical appendix",
      "pdf_url": "https://arxiv.org/pdf/2511.17714v4",
      "published_date": "2025-11-21 19:06:30 UTC",
      "updated_date": "2026-01-12 18:50:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:54:36.430972+00:00"
    },
    {
      "arxiv_id": "2511.21739v1",
      "title": "The Rapid Growth of AI Foundation Model Usage in Science",
      "title_zh": "AI åŸºç¡€æ¨¡å‹åœ¨ç§‘å­¦é¢†åŸŸåº”ç”¨çš„è¿…çŒ›å¢é•¿",
      "authors": [
        "Ana TriÅ¡oviÄ‡",
        "Alex Fogelson",
        "Janakan Sivaloganathan",
        "Neil Thompson"
      ],
      "abstract": "We present the first large-scale analysis of AI foundation model usage in science - not just citations or keywords. We find that adoption has grown rapidly, at nearly-exponential rates, with the highest uptake in Linguistics, Computer Science, and Engineering. Vision models are the most used foundation models in science, although language models' share is growing. Open-weight models dominate. As AI builders increase the parameter counts of their models, scientists have followed suit but at a much slower rate: in 2013, the median foundation model built was 7.7x larger than the median one adopted in science, by 2024 this had jumped to 26x. We also present suggestive evidence that scientists' use of these smaller models may be limiting them from getting the full benefits of AI-enabled science, as papers that use larger models appear in higher-impact journals and accrue more citations.",
      "tldr_zh": "è¯¥é¡¹ç ”ç©¶å¯¹ç§‘å­¦é¢†åŸŸä¸­AI Foundation Modelçš„ä½¿ç”¨æƒ…å†µè¿›è¡Œäº†é¦–æ¬¡å¤§è§„æ¨¡åˆ†æï¼Œå‘ç°å…¶åº”ç”¨å‘ˆç°è¿‘ä¹æŒ‡æ•°çº§çš„å¢é•¿æ€åŠ¿ï¼Œå…¶ä¸­Linguisticsã€Computer Scienceå’ŒEngineeringæ˜¯é‡‡ç”¨ç‡æœ€é«˜çš„å­¦ç§‘ã€‚ç ”ç©¶æŒ‡å‡ºï¼ŒVision modelsæ˜¯ç›®å‰ç§‘å­¦ç•Œä½¿ç”¨æœ€å¹¿æ³›çš„åŸºç¡€æ¨¡å‹ï¼Œä½†Language modelsçš„ä»½é¢æ­£åœ¨ä¸æ–­å¢é•¿ï¼Œä¸”Open-weight modelsåœ¨åº”ç”¨ä¸­å æ®ä¸»å¯¼åœ°ä½ã€‚å°½ç®¡AIå¼€å‘è€…ä¸æ–­å¢åŠ æ¨¡å‹çš„å‚æ•°é‡ï¼Œä½†ç§‘å­¦å®¶é‡‡ç”¨å¤§å‹æ¨¡å‹çš„é€Ÿåº¦ç›¸å¯¹ç¼“æ…¢ï¼Œåˆ°2024å¹´ï¼Œæ„å»ºçš„åŸºç¡€æ¨¡å‹ä¸­ä½æ•°è§„æ¨¡å·²è¾¾åˆ°ç§‘å­¦ç•Œé‡‡ç”¨æ¨¡å‹ä¸­ä½æ•°çš„26å€ã€‚åˆæ­¥è¯æ®è¡¨æ˜ï¼Œä½¿ç”¨è¾ƒå°è§„æ¨¡çš„æ¨¡å‹å¯èƒ½é™åˆ¶äº†ç§‘å­¦å®¶ä»AIé©±åŠ¨çš„ç§‘å­¦ç ”ç©¶ä¸­è·å¾—å…¨éƒ¨æ”¶ç›Šï¼Œå› ä¸ºä½¿ç”¨æ›´å¤§è§„æ¨¡æ¨¡å‹çš„è®ºæ–‡é€šå¸¸å‘è¡¨åœ¨æ›´é«˜å½±å“åŠ›çš„æœŸåˆŠä¸Šå¹¶è·å¾—æ›´å¤šå¼•ç”¨ã€‚",
      "categories": [
        "cs.DL",
        "cs.AI"
      ],
      "primary_category": "cs.DL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21739v1",
      "published_date": "2025-11-21 19:00:15 UTC",
      "updated_date": "2025-11-21 19:00:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:56:00.555873+00:00"
    },
    {
      "arxiv_id": "2511.17699v1",
      "title": "Understanding Counting Mechanisms in Large Language and Vision-Language Models",
      "title_zh": "ç†è§£å¤§è¯­è¨€æ¨¡å‹ä¸è§†è§‰-è¯­è¨€æ¨¡å‹ä¸­çš„è®¡æ•°æœºåˆ¶",
      "authors": [
        "Hosein Hasani",
        "Amirmohammad Izadi",
        "Fatemeh Askari",
        "Mobin Bagherian",
        "Sadegh Mohammadian",
        "Mohammad Izadi",
        "Mahdieh Soleymani Baghshah"
      ],
      "abstract": "This paper examines how large language models (LLMs) and large vision-language models (LVLMs) represent and compute numerical information in counting tasks. We use controlled experiments with repeated textual and visual items and analyze model behavior through causal mediation and activation patching. To this end, we design a specialized tool, CountScope, for mechanistic interpretability of numerical content. Results show that individual tokens or visual features encode latent positional count information that can be extracted and transferred across contexts. Layerwise analyses reveal a progressive emergence of numerical representations, with lower layers encoding small counts and higher layers representing larger ones. We identify an internal counter mechanism that updates with each item, stored mainly in the final token or region and transferable between contexts. In LVLMs, numerical information also appears in visual embeddings, shifting between background and foreground regions depending on spatial composition. Models rely on structural cues such as separators in text, which act as shortcuts for tracking item counts and influence the accuracy of numerical predictions. Overall, counting emerges as a structured, layerwise process in LLMs and follows the same general pattern in LVLMs, shaped by the properties of the vision encoder.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)å’Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹(LVLMs)åœ¨è®¡æ•°ä»»åŠ¡ä¸­å¦‚ä½•è¡¨ç¤ºå’Œè®¡ç®—æ•°å€¼ä¿¡æ¯ã€‚ä½œè€…è®¾è®¡äº†ä¸“é—¨ç”¨äºæ•°å€¼å†…å®¹æœºæ¢°è§£é‡Šæ€§(Mechanistic Interpretability)çš„å·¥å…·CountScopeï¼Œå¹¶é€šè¿‡å› æœä¸­ä»‹åˆ†æ(Causal Mediation)å’Œæ¿€æ´»ä¿®è¡¥(Activation Patching)ç­‰å®éªŒæ‰‹æ®µåˆ†ææ¨¡å‹è¡Œä¸ºã€‚å®éªŒå‘ç°ï¼Œå•ä¸ªTokenæˆ–è§†è§‰ç‰¹å¾ç¼–ç äº†æ½œåœ¨çš„ä½ç½®è®¡æ•°ä¿¡æ¯ï¼Œä¸”æ•°å€¼è¡¨ç¤ºå‘ˆç°å‡ºå±‚çº§æ¼”åŒ–ç‰¹å¾ï¼Œå³ä½å±‚å¤„ç†å°æ•°å€¼ï¼Œé«˜å±‚å¤„ç†å¤§æ•°å€¼ã€‚æ¨¡å‹å†…éƒ¨å­˜åœ¨ä¸€ç§éšç€æ¡ç›®æ›´æ–°çš„å†…éƒ¨è®¡æ•°å™¨(Internal Counter)æœºåˆ¶ï¼Œè¯¥ä¿¡æ¯ä¸»è¦å­˜å‚¨åœ¨æœ€åçš„Tokenæˆ–åŒºåŸŸä¸­ï¼Œå¹¶å¯åœ¨ä¸åŒä¸Šä¸‹æ–‡é—´è¿ç§»ã€‚åœ¨LVLMsä¸­ï¼Œæ•°å€¼ä¿¡æ¯ä¹Ÿä¼šå‡ºç°åœ¨è§†è§‰åµŒå…¥(Visual Embeddings)ä¸­ï¼Œå…¶åˆ†å¸ƒä¼šæ ¹æ®ç©ºé—´æ„æˆåœ¨èƒŒæ™¯å’Œå‰æ™¯åŒºåŸŸä¹‹é—´è½¬æ¢ã€‚ç ”ç©¶è¿˜å‘ç°æ¨¡å‹é«˜åº¦ä¾èµ–æ–‡æœ¬åˆ†éš”ç¬¦ç­‰ç»“æ„åŒ–çº¿ç´¢ä½œä¸ºè®¡æ•°å¿«æ·æ–¹å¼ï¼Œè¿™ç›´æ¥å½±å“äº†æ•°å€¼é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚æ€»ä½“è€Œè¨€ï¼ŒLLMsä¸­çš„è®¡æ•°æ˜¯ä¸€ä¸ªç»“æ„åŒ–çš„ã€å±‚çº§åŒ–çš„è¿‡ç¨‹ï¼Œè€ŒLVLMséµå¾ªç±»ä¼¼æ¨¡å¼ï¼Œä½†å…¶è¡¨ç°å—åˆ°è§†è§‰ç¼–ç å™¨(Vision Encoder)å±æ€§çš„å½±å“ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17699v1",
      "published_date": "2025-11-21 18:48:22 UTC",
      "updated_date": "2025-11-21 18:48:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:54:59.573746+00:00"
    },
    {
      "arxiv_id": "2511.17477v1",
      "title": "Enhancing Quranic Learning: A Multimodal Deep Learning Approach for Arabic Phoneme Recognition",
      "title_zh": "æå‡ã€Šå¤å…°ç»ã€‹å­¦ä¹ ï¼šä¸€ç§åŸºäºå¤šæ¨¡æ€æ·±åº¦å­¦ä¹ çš„é˜¿æ‹‰ä¼¯è¯­éŸ³ç´ è¯†åˆ«æ–¹æ³•",
      "authors": [
        "Ayhan Kucukmanisa",
        "Derya Gelmez",
        "Sukru Selim Calik",
        "Zeynep Hilal Kilimci"
      ],
      "abstract": "Recent advances in multimodal deep learning have greatly enhanced the capability of systems for speech analysis and pronunciation assessment. Accurate pronunciation detection remains a key challenge in Arabic, particularly in the context of Quranic recitation, where subtle phonetic differences can alter meaning. Addressing this challenge, the present study proposes a transformer-based multimodal framework for Arabic phoneme mispronunciation detection that combines acoustic and textual representations to achieve higher precision and robustness. The framework integrates UniSpeech-derived acoustic embeddings with BERT-based textual embeddings extracted from Whisper transcriptions, creating a unified representation that captures both phonetic detail and linguistic context. To determine the most effective integration strategy, early, intermediate, and late fusion methods were implemented and evaluated on two datasets containing 29 Arabic phonemes, including eight hafiz sounds, articulated by 11 native speakers. Additional speech samples collected from publicly available YouTube recordings were incorporated to enhance data diversity and generalization. Model performance was assessed using standard evaluation metrics: accuracy, precision, recall, and F1-score, allowing a detailed comparison of the fusion strategies. Experimental findings show that the UniSpeech-BERT multimodal configuration provides strong results and that fusion-based transformer architectures are effective for phoneme-level mispronunciation detection. The study contributes to the development of intelligent, speaker-independent, and multimodal Computer-Aided Language Learning (CALL) systems, offering a practical step toward technology-supported Quranic pronunciation training and broader speech-based educational applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŸºäº Transformer çš„å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤å…°ç»(Quranic)è¯µè¯»ä¸­é˜¿æ‹‰ä¼¯è¯­è¯­éŸ³è¯†åˆ«å’Œå‘éŸ³é”™è¯¯æ£€æµ‹çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶æ•´åˆäº†æ¥è‡ª UniSpeech çš„å£°å­¦åµŒå…¥(acoustic embeddings)å’ŒåŸºäº Whisper è½¬å½•å¹¶ç”± BERT å¤„ç†çš„æ–‡æœ¬åµŒå…¥(textual embeddings)ï¼Œä»¥åŒæ—¶æ•æ‰ç²¾ç¡®çš„è¯­éŸ³ç»†èŠ‚å’Œè¯­è¨€ä¸Šä¸‹æ–‡ã€‚ä¸ºäº†ç¡®å®šæœ€ä½³é›†æˆç­–ç•¥ï¼Œç ”ç©¶äººå‘˜è¯„ä¼°äº†æ—©æœŸ(early)ã€ä¸­æœŸ(intermediate)å’ŒåæœŸèåˆ(late fusion)æ–¹æ³•ï¼Œå¹¶åœ¨åŒ…å«11åæ¯è¯­è€…çš„29ä¸ªé˜¿æ‹‰ä¼¯è¯­éŸ³ç´ æ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡å¼•å…¥ YouTube å…¬å¼€å½•éŸ³æ¥å¢å¼ºæ•°æ®çš„å¤šæ ·æ€§å’Œæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUniSpeech-BERT çš„å¤šæ¨¡æ€é…ç½®åœ¨å‡†ç¡®ç‡ã€å¬å›ç‡å’Œ F1-score ç­‰æŒ‡æ ‡ä¸Šå‡è¡¨ç°ä¼˜å¼‚ï¼Œè¯æ˜äº†èåˆå‹ Transformer æ¶æ„åœ¨éŸ³ç´ çº§é”™è¯¯æ£€æµ‹ä¸­çš„æœ‰æ•ˆæ€§ã€‚è¯¥æˆæœä¸ºå¼€å‘æ™ºèƒ½ä¸”ç‹¬ç«‹äºè¯´è¯è€…çš„å¤šæ¨¡æ€è®¡ç®—æœºè¾…åŠ©è¯­è¨€å­¦ä¹ (CALL)ç³»ç»Ÿæä¾›äº†å®è·µè·¯å¾„ï¼Œæ˜¾è‘—æ¨åŠ¨äº†æŠ€æœ¯æ”¯æŒçš„å¤å…°ç»å‘éŸ³è®­ç»ƒåŠç›¸å…³æ•™è‚²åº”ç”¨çš„å‘å±•ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "11 pages, 2 figures, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2511.17477v1",
      "published_date": "2025-11-21 18:25:46 UTC",
      "updated_date": "2025-11-21 18:25:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:55:43.271483+00:00"
    },
    {
      "arxiv_id": "2601.11534v1",
      "title": "Modular AI-Powered Interviewer with Dynamic Question Generation and Expertise Profiling",
      "title_zh": "å…·å¤‡åŠ¨æ€é—®é¢˜ç”Ÿæˆä¸ä¸“ä¸šç”»åƒåŠŸèƒ½çš„æ¨¡å—åŒ– AI é©±åŠ¨é¢è¯•ç³»ç»Ÿ",
      "authors": [
        "Aisvarya Adeseye",
        "Jouni Isoaho",
        "Seppo Virtanen",
        "Mohammad Tahir"
      ],
      "abstract": "Automated interviewers and chatbots are common in research, recruitment, customer service, and education. Many existing systems use fixed question lists, strict rules, and limited personalization, leading to repeated conversations that cause low engagement. Therefore, these tools are not effective for complex qualitative research, which requires flexibility, context awareness, and ethical sensitivity. Consequently, there is a need for a more adaptive and context-aware interviewing system. To address this, an AI-powered interviewer that dynamically generates questions that are contextually appropriate and expertise aligned is presented in this study. The interviewer is built on a locally hosted large language model (LLM) that generates coherent dialogue while preserving data privacy. The interviewer profiles the participants' expertise in real time to generate knowledge-appropriate questions, well-articulated responses, and smooth transition messages similar to human-like interviews. To implement these functionalities, a modular prompt engineering pipeline was designed to ensure that the interview conversation remains scalable, adaptive, and semantically rich. To evaluate the AI-powered interviewer, it was tested with various participants, and it achieved high satisfaction (mean 4.45) and engagement (mean 4.33). The proposed interviewer is a scalable, privacy-conscious solution that advances AI-assisted qualitative data collection.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å…·æœ‰åŠ¨æ€é—®é¢˜ç”Ÿæˆå’Œä¸“ä¸šèƒŒæ™¯åˆ†æ(Expertise Profiling)åŠŸèƒ½çš„æ¨¡å—åŒ–AIé¢è¯•ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è‡ªåŠ¨åŒ–é¢è¯•å·¥å…·å› å›ºå®šé—®ç­”å’Œç¼ºä¹ä¸ªæ€§åŒ–è€Œå¯¼è‡´çš„å‚ä¸åº¦ä½åŠå®šæ€§ç ”ç©¶å±€é™æ€§ã€‚è¯¥ç³»ç»ŸåŸºäºæœ¬åœ°æ‰˜ç®¡çš„å¤§å‹è¯­è¨€æ¨¡å‹(LLM)ï¼Œåœ¨ç¡®ä¿æ•°æ®éšç§çš„åŒæ—¶å®ç°äº†è¿è´¯çš„å¯¹è¯é€»è¾‘ï¼Œå¹¶èƒ½å®æ—¶åˆ†æå—è®¿è€…çš„ä¸“ä¸šæ°´å¹³ä»¥ç”Ÿæˆè¯­å¢ƒç›¸å…³ä¸”çŸ¥è¯†åŒ¹é…çš„é—®é¢˜ã€‚é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æ¨¡å—åŒ–æç¤ºå·¥ç¨‹(Prompt Engineering)ç®¡çº¿ï¼Œè¯¥é¢è¯•ç³»ç»Ÿå±•ç°å‡ºé«˜åº¦çš„å¯æ‰©å±•æ€§å’Œè‡ªé€‚åº”æ€§ï¼Œèƒ½å¤Ÿäº§ç”Ÿç±»ä¼¼äººç±»çš„æµç•…è¿‡æ¸¡ä¸è¡¨è¾¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥AIé¢è¯•å®˜åœ¨å‚ä¸è€…æ»¡æ„åº¦å’ŒæŠ•å…¥åº¦æ–¹é¢å‡è·å¾—äº†æé«˜çš„è¯„ä»·(å‡å€¼åˆ†åˆ«ä¸º4.45å’Œ4.33)ã€‚è¯¥é¡¹ç ”ç©¶ä¸ºéšç§æ•æ„Ÿä¸”å¤æ‚çš„å®šæ€§æ•°æ®æ”¶é›†æä¾›äº†é«˜æ•ˆä¸”å…·å¤‡è‡ªé€‚åº”èƒ½åŠ›çš„AIè¾…åŠ©æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted and Waiting to be published in conference AIR-RES'25 ( http://www.american-cse.org/air-res2025 )",
      "pdf_url": "https://arxiv.org/pdf/2601.11534v1",
      "published_date": "2025-11-21 18:25:26 UTC",
      "updated_date": "2025-11-21 18:25:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:55:18.468642+00:00"
    },
    {
      "arxiv_id": "2511.17473v1",
      "title": "Masked-and-Reordered Self-Supervision for Reinforcement Learning from Verifiable Rewards",
      "title_zh": "é¢å‘åŸºäºå¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ çš„æ©ç ä¸é‡æ’åºè‡ªç›‘ç£",
      "authors": [
        "Zhen Wang",
        "Zhifeng Gao",
        "Guolin Ke"
      ],
      "abstract": "Test-time scaling has been shown to substantially improve large language models' (LLMs) mathematical reasoning. However, for a large portion of mathematical corpora, especially theorem proving, RLVR's scalability is limited: intermediate reasoning is crucial, while final answers are difficult to directly and reliably verify. Meanwhile, token-level SFT often degenerates into rote memorization rather than inducing longer chains of thought. Inspired by BERT's self-supervised tasks, we propose MR-RLVR (Masked-and-Reordered RLVR), which constructs process-level self-supervised rewards via \"masked-then-fill\" and \"step reordering\" to extract learnable signals from intermediate reasoning. Our training pipeline comprises two stages: we first perform self-supervised training on sampled mathematical calculation and proof data; we then conduct RLVR fine-tuning on mathematical calculation datasets where only outcomes are verifiable. We implement MR-RLVR on Qwen2.5-3B and DeepSeek-R1-Distill-Qwen-1.5B, and evaluate on AIME24, AIME25, AMC23, and MATH500. Under a fixed sampling and decoding budget, MR-RLVR achieves average relative gains over the original RLVR of +9.86% Pass@1, +5.27% Pass@5, and +4.00% Pass@8. These results indicate that incorporating process-aware self-supervised signals can effectively enhance RLVR's scalability and performance in only outcome-verifiable settings.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å®šç†è¯æ˜ç­‰å¤æ‚æ¨ç†ä»»åŠ¡ä¸­é¢ä¸´çš„ Reinforcement Learning from Verifiable Rewards (RLVR) æ‰©å±•æ€§å—é™é—®é¢˜ï¼Œæå‡ºäº† MR-RLVR (Masked-and-Reordered RLVR) æ¡†æ¶ã€‚è¯¥æ¡†æ¶å€Ÿé‰´ BERT çš„è‡ªç›‘ç£æ€è·¯ï¼Œé€šè¿‡â€œæ©ç å¡«å……â€(masked-then-fill) å’Œ â€œæ­¥éª¤é‡æ’åºâ€(step reordering) æ„å»ºè¿‡ç¨‹çº§è‡ªç›‘ç£å¥–åŠ±ï¼Œä»è€Œæœ‰æ•ˆæå–ä¸­é—´æ¨ç†æ­¥éª¤ä¸­çš„å­¦ä¹ ä¿¡å·ã€‚å…¶è®­ç»ƒæµç¨‹åˆ†ä¸ºæ•°å­¦è¯æ˜æ•°æ®çš„è‡ªç›‘ç£å­¦ä¹ ä¸ä»…ç»“æœå¯éªŒè¯æ•°æ®é›†çš„ RLVR å¾®è°ƒä¸¤ä¸ªé˜¶æ®µï¼Œæ—¨åœ¨å…‹æœ token-level SFT å¯¼è‡´çš„æœºæ¢°è®°å¿†é—®é¢˜ã€‚åœ¨ Qwen2.5-3B å’Œ DeepSeek-R1 ç­‰æ¨¡å‹ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMR-RLVR åœ¨ AIME å’Œ MATH500 ç­‰æµ‹è¯•é›†ä¸Šçš„ Pass@1 ç›¸å¯¹å¢ç›Šè¾¾åˆ° 9.86%ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ•´åˆè¿‡ç¨‹æ„ŸçŸ¥(process-aware)çš„è‡ªç›‘ç£ä¿¡å·èƒ½å¤Ÿæ˜¾è‘—æå‡ RLVR åœ¨ä»…æœ‰ç»“æœåé¦ˆåœºæ™¯ä¸‹çš„æ¨ç†æ€§èƒ½ä¸å¯æ‰©å±•æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17473v1",
      "published_date": "2025-11-21 18:23:04 UTC",
      "updated_date": "2025-11-21 18:23:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:55:57.581409+00:00"
    },
    {
      "arxiv_id": "2511.17467v2",
      "title": "PersonaAgent with GraphRAG: Community-Aware Knowledge Graphs for Personalized LLM",
      "title_zh": "PersonaAgent with GraphRAGï¼šé¢å‘ä¸ªæ€§åŒ–å¤§è¯­è¨€æ¨¡å‹çš„ç¤¾åŒºæ„ŸçŸ¥çŸ¥è¯†å›¾è°±",
      "authors": [
        "Siqi Liang",
        "Yudi Zhang",
        "Yue Guo"
      ],
      "abstract": "We propose a novel framework for persona-based language model system, motivated by the need for personalized AI agents that adapt to individual user preferences. In our approach, the agent embodies the user's \"persona\" (e.g. user profile or taste) and is powered by a large language model (LLM). To enable the agent to leverage rich contextual information, we introduce a Knowledge-Graph-enhanced Retrieval-Augmented Generation (Graph RAG) mechanism that constructs an LLM-derived graph index of relevant documents and summarizes communities of related information. Our framework generates personalized prompts by combining: (1) a summary of the user's historical behaviors and preferences extracted from the knowledge graph, and (2) relevant global interaction patterns identified through graph-based community detection. This dynamic prompt engineering approach allows the agent to maintain consistent persona-aligned behaviors while benefiting from collective knowledge. On the LaMP benchmark, our method improves news categorization F1 by 11.1%, movie tagging F1 by 56.1%, and reduces product rating MAE by 10.4% over prior methods. Our code is available at https://anonymous.4open.science/r/PersonaAgentwGraphRAG-DE6F",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†PersonaAgentï¼Œä¸€ä¸ªæ—¨åœ¨å®ç°ä¸ªæ€§åŒ–å¤§å‹è¯­è¨€æ¨¡å‹(LLM)ç³»ç»Ÿçš„åˆ›æ–°æ¡†æ¶ï¼Œä»¥æ»¡è¶³AIæ™ºèƒ½ä½“æ ¹æ®ä¸ªäººåå¥½è¿›è¡Œé€‚é…çš„éœ€æ±‚ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†åŸºäºçŸ¥è¯†å›¾è°±å¢å¼ºçš„æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯(GraphRAG)ï¼Œé€šè¿‡æ„å»ºç”±LLMè¡ç”Ÿçš„æ–‡æ¡£å›¾ç´¢å¼•å¹¶å¯¹ç›¸å…³ä¿¡æ¯çš„ç¤¾åŒºè¿›è¡Œæ€»ç»“ï¼Œä¸ºæ™ºèƒ½ä½“æä¾›ä¸°å¯Œçš„ä¸Šä¸‹æ–‡æ”¯æŒã€‚ç³»ç»Ÿé‡‡ç”¨åŠ¨æ€æç¤ºå·¥ç¨‹(Dynamic Prompt Engineering)ç”Ÿæˆä¸ªæ€§åŒ–æç¤ºè¯ï¼Œå°†çŸ¥è¯†å›¾è°±ä¸­æå–çš„ç”¨æˆ·å†å²è¡Œä¸ºåå¥½æ‘˜è¦ä¸é€šè¿‡ç¤¾åŒºå‘ç°(Community Detection)è¯†åˆ«å‡ºçš„å…¨å±€äº¤äº’æ¨¡å¼ç›¸ç»“åˆã€‚è¿™ç§æœºåˆ¶ç¡®ä¿æ™ºèƒ½ä½“åœ¨ä¿æŒä¸ç”¨æˆ·äººæ ¼ç‰¹è´¨(Persona)ä¸€è‡´è¡Œä¸ºçš„åŒæ—¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨é›†ä½“çŸ¥è¯†ã€‚åœ¨LaMPåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•è¡¨ç°å‡ºè‰²ï¼Œåˆ†åˆ«æå‡äº†æ–°é—»åˆ†ç±»å’Œç”µå½±æ ‡ç­¾é¢„æµ‹çš„F1åˆ†æ•°è¾¾11.1%å’Œ56.1%ï¼Œå¹¶å°†äº§å“è¯„åˆ†çš„å¹³å‡ç»å¯¹è¯¯å·®(MAE)é™ä½äº†10.4%ã€‚è¯¥é¡¹å·¥ä½œä¸ºæ„å»ºèƒ½å¤Ÿç²¾å‡†ç†è§£å¹¶é€‚åº”ä¸ªä½“å·®å¼‚çš„æ™ºèƒ½åŒ–äººæœºäº¤äº’ç³»ç»Ÿæä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17467v2",
      "published_date": "2025-11-21 18:15:47 UTC",
      "updated_date": "2025-12-02 04:45:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:55:12.081198+00:00"
    },
    {
      "arxiv_id": "2511.17461v1",
      "title": "SRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception",
      "title_zh": "SRA-CPï¼šå…·å¤‡é£é™©æ„ŸçŸ¥èƒ½åŠ›çš„è‡ªå‘å¼é€‰æ‹©æ€§ååŒæ„ŸçŸ¥",
      "authors": [
        "Jiaxi Liu",
        "Chengyuan Ma",
        "Hang Zhou",
        "Weizhe Tang",
        "Shixiao Liang",
        "Haoyang Ding",
        "Xiaopeng Li",
        "Bin Ran"
      ],
      "abstract": "Cooperative perception (CP) offers significant potential to overcome the limitations of single-vehicle sensing by enabling information sharing among connected vehicles (CVs). However, existing generic CP approaches need to transmit large volumes of perception data that are irrelevant to the driving safety, exceeding available communication bandwidth. Moreover, most CP frameworks rely on pre-defined communication partners, making them unsuitable for dynamic traffic environments. This paper proposes a Spontaneous Risk-Aware Selective Cooperative Perception (SRA-CP) framework to address these challenges. SRA-CP introduces a decentralized protocol where connected agents continuously broadcast lightweight perception coverage summaries and initiate targeted cooperation only when risk-relevant blind zones are detected. A perceptual risk identification module enables each CV to locally assess the impact of occlusions on its driving task and determine whether cooperation is necessary. When CP is triggered, the ego vehicle selects appropriate peers based on shared perception coverage and engages in selective information exchange through a fusion module that prioritizes safety-critical content and adapts to bandwidth constraints. We evaluate SRA-CP on a public dataset against several representative baselines. Results show that SRA-CP achieves less than 1% average precision (AP) loss for safety-critical objects compared to generic CP, while using only 20% of the communication bandwidth. Moreover, it improves the perception performance by 15% over existing selective CP methods that do not incorporate risk awareness.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ååŒæ„ŸçŸ¥(Cooperative Perception, CP)åœ¨ç°æœ‰æ–¹æ³•ä¸­å­˜åœ¨çš„é€šä¿¡å¸¦å®½å‹åŠ›å¤§ä»¥åŠåœ¨åŠ¨æ€ç¯å¢ƒä¸‹éš¾ä»¥çµæ´»é€‰æ‹©åˆä½œä¼™ä¼´ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†è‡ªå‘æ€§é£é™©æ„ŸçŸ¥é€‰æ‹©æ€§ååŒæ„ŸçŸ¥æ¡†æ¶(SRA-CP)ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ç§å»ä¸­å¿ƒåŒ–åè®®ï¼Œè”ç½‘æ™ºèƒ½ä½“é€šè¿‡å¹¿æ’­è½»é‡çº§çš„æ„ŸçŸ¥è¦†ç›–æ‘˜è¦ï¼Œä»…åœ¨æ£€æµ‹åˆ°é£é™©ç›¸å…³çš„ç›²åŒºæ—¶å¯åŠ¨é’ˆå¯¹æ€§åˆä½œã€‚é€šè¿‡æ„ŸçŸ¥é£é™©è¯†åˆ«æ¨¡å—ï¼Œæ¯è¾†è”ç½‘æ±½è½¦èƒ½å¤Ÿæœ¬åœ°è¯„ä¼°é®æŒ¡å¯¹é©¾é©¶ä»»åŠ¡çš„å½±å“ï¼Œä»è€Œè‡ªä¸»å†³å®šååŒçš„å¿…è¦æ€§ã€‚åœ¨ååŒè§¦å‘åï¼Œä¸»è½¦æ ¹æ®å…±äº«æ„ŸçŸ¥èŒƒå›´é€‰æ‹©åˆé€‚çš„å¯¹ç­‰èŠ‚ç‚¹ï¼Œå¹¶åˆ©ç”¨èåˆæ¨¡å—åœ¨å¸¦å®½é™åˆ¶ä¸‹ä¼˜å…ˆäº¤æ¢å®‰å…¨å…³é”®å†…å®¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸é€šç”¨ååŒæ„ŸçŸ¥æ–¹æ³•ç›¸æ¯”ï¼ŒSRA-CPåœ¨ä»…ä½¿ç”¨20%é€šä¿¡å¸¦å®½çš„æƒ…å†µä¸‹ï¼Œå®‰å…¨å…³é”®ç›®æ ‡çš„å¹³å‡ç²¾åº¦(Average Precision, AP)æŸå¤±ä½äº1%ã€‚æ­¤å¤–ï¼Œç›¸è¾ƒäºä¸å…·å¤‡é£é™©æ„è¯†çš„é€‰æ‹©æ€§ååŒæ„ŸçŸ¥æ–¹æ³•ï¼Œè¯¥æ¡†æ¶å°†æ„ŸçŸ¥æ€§èƒ½æå‡äº†15%ï¼Œä¸ºå¤æ‚äº¤é€šç¯å¢ƒä¸‹çš„é«˜æ•ˆå®‰å…¨æ„ŸçŸ¥æä¾›äº†æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17461v1",
      "published_date": "2025-11-21 18:03:48 UTC",
      "updated_date": "2025-11-21 18:03:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:56:32.363355+00:00"
    },
    {
      "arxiv_id": "2511.17450v1",
      "title": "Planning with Sketch-Guided Verification for Physics-Aware Video Generation",
      "title_zh": "åŸºäºè‰å›¾å¼•å¯¼éªŒè¯çš„ç‰©ç†æ„ŸçŸ¥è§†é¢‘ç”Ÿæˆè§„åˆ’",
      "authors": [
        "Yidong Huang",
        "Zun Wang",
        "Han Lin",
        "Dong-Ki Kim",
        "Shayegan Omidshafiei",
        "Jaehong Yoon",
        "Yue Zhang",
        "Mohit Bansal"
      ],
      "abstract": "Recent video generation approaches increasingly rely on planning intermediate control signals such as object trajectories to improve temporal coherence and motion fidelity. However, these methods mostly employ single-shot plans that are typically limited to simple motions, or iterative refinement which requires multiple calls to the video generator, incuring high computational cost. To overcome these limitations, we propose SketchVerify, a training-free, sketch-verification-based planning framework that improves motion planning quality with more dynamically coherent trajectories (i.e., physically plausible and instruction-consistent motions) prior to full video generation by introducing a test-time sampling and verification loop. Given a prompt and a reference image, our method predicts multiple candidate motion plans and ranks them using a vision-language verifier that jointly evaluates semantic alignment with the instruction and physical plausibility. To efficiently score candidate motion plans, we render each trajectory as a lightweight video sketch by compositing objects over a static background, which bypasses the need for expensive, repeated diffusion-based synthesis while achieving comparable performance. We iteratively refine the motion plan until a satisfactory one is identified, which is then passed to the trajectory-conditioned generator for final synthesis. Experiments on WorldModelBench and PhyWorldBench demonstrate that our method significantly improves motion quality, physical realism, and long-term consistency compared to competitive baselines while being substantially more efficient. Our ablation study further shows that scaling up the number of trajectory candidates consistently enhances overall performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SketchVerifyï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„åŸºäºè‰å›¾éªŒè¯(sketch-verification-based)çš„è§„åˆ’æ¡†æ¶ï¼Œæ—¨åœ¨æå‡è§†é¢‘ç”Ÿæˆä¸­åŠ¨ä½œè§„åˆ’çš„è´¨é‡å’Œç‰©ç†ä¸€è‡´æ€§ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•åœ¨å¤æ‚åŠ¨ä½œå¤„ç†ä¸Šå—é™æˆ–è®¡ç®—æˆæœ¬è¿‡é«˜çš„é—®é¢˜ï¼ŒSketchVerifyé€šè¿‡åœ¨ç”Ÿæˆå…¨è§†é¢‘å‰å¼•å…¥æµ‹è¯•æ—¶é‡‡æ ·ä¸éªŒè¯å¾ªç¯(test-time sampling and verification loop)æ¥ä¼˜åŒ–è½¨è¿¹è§„åˆ’ã€‚è¯¥æ–¹æ³•é¦–å…ˆé¢„æµ‹å¤šä¸ªå€™é€‰åŠ¨ä½œè§„åˆ’ï¼Œå¹¶åˆ©ç”¨è§†è§‰è¯­è¨€éªŒè¯å™¨(vision-language verifier)ç»¼åˆè¯„ä¼°å…¶ä¸æŒ‡ä»¤çš„è¯­ä¹‰å¯¹é½åº¦åŠç‰©ç†åˆç†æ€§ã€‚ä¸ºäº†é™ä½è®¡ç®—å¼€é”€ï¼Œç ”ç©¶è€…å°†æ¯ä¸ªè½¨è¿¹æ¸²æŸ“ä¸ºè½»é‡çº§çš„è§†é¢‘è‰å›¾(video sketch)ï¼Œä»è€Œé¿å…äº†æ˜‚è´µä¸”é‡å¤çš„åŸºäºæ‰©æ•£(diffusion-based)çš„åˆæˆã€‚é€šè¿‡è¿­ä»£ä¼˜åŒ–åŠ¨ä½œè§„åˆ’ï¼Œç³»ç»Ÿåœ¨è¯†åˆ«å‡ºæ»¡æ„æ–¹æ¡ˆåå°†å…¶ä¼ é€’ç»™è½¨è¿¹æ¡ä»¶ç”Ÿæˆå™¨(trajectory-conditioned generator)è¿›è¡Œæœ€ç»ˆåˆæˆã€‚åœ¨WorldModelBenchå’ŒPhyWorldBenchä¸Šçš„å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŠ¨ä½œè´¨é‡ã€ç‰©ç†é€¼çœŸåº¦å’Œé•¿æœŸä¸€è‡´æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç«äº‰åŸºçº¿ï¼Œä¸”æ•ˆç‡æ›´é«˜ã€‚æ¶ˆå‡å®éªŒè¿›ä¸€æ­¥è¡¨æ˜ï¼Œå¢åŠ è½¨è¿¹å€™é€‰æ•°é‡èƒ½æŒç»­æå‡ç³»ç»Ÿçš„æ•´ä½“æ€§èƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "website: https://sketchverify.github.io/",
      "pdf_url": "https://arxiv.org/pdf/2511.17450v1",
      "published_date": "2025-11-21 17:48:02 UTC",
      "updated_date": "2025-11-21 17:48:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:57:45.371754+00:00"
    },
    {
      "arxiv_id": "2511.17443v2",
      "title": "GRAPHIC--Guidelines for Reviewing Algorithmic Practices in Human-centred Design and Interaction for Creativity",
      "title_zh": "GRAPHICï¼šé¢å‘åˆ›æ„é¢†åŸŸä¸­ä»¥äººä¸ºä¸­å¿ƒçš„è®¾è®¡ä¸äº¤äº’çš„ç®—æ³•å®è·µè¯„ä¼°æŒ‡å—",
      "authors": [
        "Joana Rovira Martins",
        "Pedro Martins",
        "Ana Boavida"
      ],
      "abstract": "Artificial Intelligence (AI) has been increasingly applied to creative domains, leading to the development of systems that collaborate with humans in design processes. In Graphic Design, integrating computational systems into co-creative workflows presents specific challenges, as it requires balancing scientific rigour with the subjective and visual nature of design practice. Following the PRISMA methodology, we identified 872 articles, resulting in a final corpus of 71 publications describing 68 unique systems. Based on this review, we introduce GRAPHIC (Guidelines for Reviewing Algorithmic Practices in Human-centred Design and Interaction for Creativity), a framework for analysing computational systems applied to Graphic Design. Its goal is to understand how current systems support human-AI collaboration in the Graphic Design discipline. The framework comprises main dimensions, which our analysis revealed to be essential across diverse system types: (1) Collaborative Panorama, (2) Processes and Modalities, and (3) Graphic Design Principles. Its application revealed research gaps, including the need to balance initiative and control between agents, improve communication through explainable interaction models, and promote systems that support transformational creativity grounded in core design principles.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äººå·¥æ™ºèƒ½åœ¨å¹³é¢è®¾è®¡(Graphic Design)ååŒåˆ›ä½œä¸­çš„ç§‘å­¦ä¸¥è°¨æ€§ä¸è§†è§‰ä¸»è§‚æ€§å¹³è¡¡æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸º GRAPHIC çš„è¯„å®¡å‡†åˆ™æ¡†æ¶ã€‚ç ”ç©¶å›¢é˜Ÿéµå¾ª PRISMA æ–¹æ³•è®ºï¼Œé€šè¿‡å¯¹ 872 ç¯‡ç›¸å…³æ–‡ç« è¿›è¡Œç³»ç»Ÿç»¼è¿°ï¼Œæ·±å…¥åˆ†æäº†åŒ…å« 68 ä¸ªç‹¬ç‰¹ç³»ç»Ÿçš„ 71 ç¯‡æ ¸å¿ƒæ–‡çŒ®ã€‚GRAPHIC æ¡†æ¶ç”±åä½œå…¨æ™¯(Collaborative Panorama)ã€æµç¨‹ä¸æ¨¡æ€(Processes and Modalities)ä»¥åŠå¹³é¢è®¾è®¡åŸåˆ™(Graphic Design Principles)ä¸‰ä¸ªæ ¸å¿ƒç»´åº¦æ„æˆï¼Œæ—¨åœ¨å…¨é¢è§£æè®¡ç®—ç³»ç»Ÿå¦‚ä½•æ”¯æŒå¹³é¢è®¾è®¡é¢†åŸŸçš„äººæœºååŒã€‚é€šè¿‡åº”ç”¨è¯¥æ¡†æ¶ï¼Œç ”ç©¶æ­ç¤ºäº†å½“å‰é¢†åŸŸåœ¨æ™ºèƒ½ä½“ä¸»åŠ¨æ€§ä¸æ§åˆ¶æƒå¹³è¡¡ã€ä»¥åŠé€šè¿‡å¯è§£é‡Šäº¤äº’æ¨¡å‹(explainable interaction models)æ”¹å–„æ²Ÿé€šæ–¹é¢çš„å…³é”®ç©ºç™½ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†å¼€å‘æ”¯æŒå˜é©æ€§åˆ›æ„(transformational creativity)ä¸”æ¤æ ¹äºæ ¸å¿ƒè®¾è®¡åŸåˆ™çš„ç³»ç»Ÿçš„å¿…è¦æ€§ï¼Œä¸ºæœªæ¥å¹³é¢è®¾è®¡é¢†åŸŸçš„äººæœºäº¤äº’ç ”ç©¶æä¾›äº†é‡è¦æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.HC",
      "comment": "20 pages, 16 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.17443v2",
      "published_date": "2025-11-21 17:42:09 UTC",
      "updated_date": "2025-11-24 09:38:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:56:54.763821+00:00"
    },
    {
      "arxiv_id": "2511.17442v1",
      "title": "REMSA: An LLM Agent for Foundation Model Selection in Remote Sensing",
      "title_zh": "REMSAï¼šç”¨äºé¥æ„ŸåŸºç¡€æ¨¡å‹é€‰æ‹©çš„å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“",
      "authors": [
        "Binger Chen",
        "Tacettin Emre BÃ¶k",
        "Behnood Rasti",
        "Volker Markl",
        "BegÃ¼m Demir"
      ],
      "abstract": "Foundation Models (FMs) are increasingly used in remote sensing (RS) for tasks such as environmental monitoring, disaster assessment, and land-use mapping. These models include unimodal vision encoders trained on a single data modality and multimodal architectures trained on combinations of SAR, multispectral, hyperspectral, and image-text data. They support diverse RS tasks including semantic segmentation, image classification, change detection, and visual question answering. However, selecting an appropriate remote sensing foundation model (RSFM) remains difficult due to scattered documentation, heterogeneous formats, and varied deployment constraints. We introduce the RSFM Database (RS-FMD), a structured resource covering over 150 RSFMs spanning multiple data modalities, resolutions, and learning paradigms. Built on RS-FMD, we present REMSA, the first LLM-based agent for automated RSFM selection from natural language queries. REMSA interprets user requirements, resolves missing constraints, ranks candidate models using in-context learning, and provides transparent justifications. We also propose a benchmark of 75 expert-verified RS query scenarios, producing 900 configurations under an expert-centered evaluation protocol. REMSA outperforms several baselines, including naive agents, dense retrieval, and unstructured RAG-based LLMs. It operates entirely on publicly available metadata and does not access private or sensitive data.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é¥æ„Ÿé¢†åŸŸåŸºç¡€æ¨¡å‹ï¼ˆFoundation Models, FMsï¼‰å› æ–‡æ¡£åˆ†æ•£ã€æ ¼å¼å¼‚æ„å’Œéƒ¨ç½²é™åˆ¶å¤æ‚è€Œå¯¼è‡´çš„é€‰æ‹©éš¾é¢˜ï¼Œæ„å»ºäº†é¥æ„ŸåŸºç¡€æ¨¡å‹æ•°æ®åº“ï¼ˆRSFM Database, RS-FMDï¼‰ï¼Œæ¶µç›–è¶…è¿‡150ä¸ªè·¨å¤šæ¨¡æ€ã€å¤šåˆ†è¾¨ç‡å’Œä¸åŒå­¦ä¹ èŒƒå¼çš„RSFMèµ„æºã€‚åŸºäºæ­¤æ•°æ®åº“ï¼Œç ”ç©¶è€…æå‡ºäº†REMSAï¼Œè¿™æ˜¯é¦–ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ™ºèƒ½ä½“ï¼Œæ—¨åœ¨é€šè¿‡è‡ªç„¶è¯­è¨€æŸ¥è¯¢å®ç°è‡ªåŠ¨åŒ–çš„RSFMé€‰æ‹©ã€‚REMSAèƒ½å¤Ÿè§£æç”¨æˆ·éœ€æ±‚ã€è¡¥å…¨ç¼ºå¤±çº¦æŸï¼Œå¹¶åˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆIn-context Learningï¼‰å¯¹å€™é€‰æ¨¡å‹è¿›è¡Œæ’åºå¹¶æä¾›é€æ˜çš„å†³ç­–ä¾æ®ã€‚ä¸ºäº†éªŒè¯æ€§èƒ½ï¼Œç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŒ…å«75ä¸ªä¸“å®¶éªŒè¯åœºæ™¯çš„åŸºå‡†ï¼Œå¹¶åœ¨ä¸“å®¶ä¸­å¿ƒåŒ–çš„è¯„ä¼°åè®®ä¸‹ç”Ÿæˆäº†900ç§é…ç½®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒREMSAåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºæœ´ç´ æ™ºèƒ½ä½“ã€å¯†é›†æ£€ç´¢å’ŒåŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„LLMç­‰åŸºå‡†æ¨¡å‹ã€‚è¯¥ç³»ç»Ÿå®Œå…¨åŸºäºå…¬å¼€å…ƒæ•°æ®è¿è¡Œï¼Œä¸æ¶‰åŠç§æœ‰æ•æ„Ÿæ•°æ®ï¼Œä¸ºé¥æ„Ÿä»»åŠ¡çš„æ¨¡å‹è‡ªåŠ¨åŒ–éƒ¨ç½²æä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Code and data available at https://github.com/be-chen/REMSA",
      "pdf_url": "https://arxiv.org/pdf/2511.17442v1",
      "published_date": "2025-11-21 17:41:26 UTC",
      "updated_date": "2025-11-21 17:41:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:57:55.967363+00:00"
    },
    {
      "arxiv_id": "2511.17439v1",
      "title": "InTAct: Interval-based Task Activation Consolidation for Continual Learning",
      "title_zh": "InTActï¼šåŸºäºåŒºé—´ä»»åŠ¡æ¿€æ´»å·©å›ºçš„æŒç»­å­¦ä¹ ",
      "authors": [
        "Patryk Krukowski",
        "Jan Miksa",
        "Piotr Helm",
        "Jacek Tabor",
        "PaweÅ‚ WawrzyÅ„ski",
        "PrzemysÅ‚aw Spurek"
      ],
      "abstract": "Continual learning aims to enable neural networks to acquire new knowledge without forgetting previously learned information. While recent prompt-based methods perform strongly in class-incremental settings, they remain vulnerable under domain shifts, where the input distribution changes but the label space remains fixed. This exposes a persistent problem known as representation drift. Shared representations evolve in ways that overwrite previously useful features and cause forgetting even when prompts isolate task-specific parameters. To address this issue, we introduce InTAct, a method that preserves functional behavior in shared layers without freezing parameters or storing past data. InTAct captures the characteristic activation ranges associated with previously learned tasks and constrains updates to ensure the network remains consistent within these regions, while still allowing for flexible adaptation elsewhere. In doing so, InTAct stabilizes the functional role of important neurons rather than directly restricting parameter values. The approach is architecture-agnostic and integrates seamlessly into existing prompt-based continual learning frameworks. By regulating representation changes where past knowledge is encoded, InTAct achieves a principled balance between stability and plasticity. Across diverse domain-incremental benchmarks, including DomainNet and ImageNet-R, InTAct consistently reduces representation drift and improves performance, increasing Average Accuracy by up to 8 percentage points over state-of-the-art baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æŒç»­å­¦ä¹ (Continual Learning)ä¸­åŸºäºæç¤º(prompt-based)çš„æ–¹æ³•åœ¨é¢å¯¹é¢†åŸŸåç§»(domain shifts)æ—¶å®¹æ˜“äº§ç”Ÿè¡¨å¾åç§»(representation drift)çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºInTActçš„åŒºé—´ä»»åŠ¡æ¿€æ´»å·©å›ºæ–¹æ³•ã€‚InTActçš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡æ•æ‰ä¸å·²å­¦ä»»åŠ¡ç›¸å…³çš„ç‰¹å¾æ¿€æ´»åŒºé—´(characteristic activation ranges)ï¼Œçº¦æŸå…±äº«å±‚çš„å‚æ•°æ›´æ–°ä»¥ç»´æŒåŠŸèƒ½çš„ä¸€è‡´æ€§ï¼Œä»è€Œåœ¨ä¸å†»ç»“å‚æ•°æˆ–å­˜å‚¨æ—§æ•°æ®çš„å‰æä¸‹ä¿æŠ¤å†å²çŸ¥è¯†ã€‚è¯¥æ–¹æ³•ä¸ä»…ç¨³å®šäº†å…³é”®ç¥ç»å…ƒçš„æ³›åŒ–ä½œç”¨ï¼Œè¿˜è¡¨ç°å‡ºæ¶æ„æ— å…³æ€§(architecture-agnostic)ï¼Œèƒ½å¤Ÿä¸ç°æœ‰çš„æŒç»­å­¦ä¹ æ¡†æ¶æ— ç¼é›†æˆã€‚é€šè¿‡è°ƒèŠ‚ç¼–ç æ—§çŸ¥è¯†çš„è¡¨å¾å˜åŒ–ï¼ŒInTActåœ¨ç¨³å®šæ€§(stability)ä¸å¯å¡‘æ€§(plasticity)ä¹‹é—´å®ç°äº†åŸåˆ™æ€§çš„å¹³è¡¡ã€‚å®éªŒè¡¨æ˜ï¼ŒInTActåœ¨DomainNetå’ŒImageNet-Rç­‰å¤šä¸ªé¢†åŸŸå¢é‡åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—å‡å°‘äº†è¡¨å¾åç§»ï¼Œå…¶å¹³å‡å‡†ç¡®ç‡(Average Accuracy)æ¯”å½“å‰æœ€å…ˆè¿›çš„åŸºçº¿æ¨¡å‹æå‡äº†å¤šè¾¾8ä¸ªç™¾åˆ†ç‚¹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17439v1",
      "published_date": "2025-11-21 17:36:12 UTC",
      "updated_date": "2025-11-21 17:36:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:57:03.483284+00:00"
    },
    {
      "arxiv_id": "2511.17432v1",
      "title": "SMILE: A Composite Lexical-Semantic Metric for Question-Answering Evaluation",
      "title_zh": "SMILEï¼šç”¨äºé—®ç­”è¯„ä¼°çš„è¯æ³•-è¯­ä¹‰å¤åˆåº¦é‡æŒ‡æ ‡",
      "authors": [
        "Shrikant Kendre",
        "Austin Xu",
        "Honglu Zhou",
        "Michael Ryoo",
        "Shafiq Joty",
        "Juan Carlos Niebles"
      ],
      "abstract": "Traditional evaluation metrics for textual and visual question answering, like ROUGE, METEOR, and Exact Match (EM), focus heavily on n-gram based lexical similarity, often missing the deeper semantic understanding needed for accurate assessment. While measures like BERTScore and MoverScore leverage contextual embeddings to address this limitation, they lack flexibility in balancing sentence-level and keyword-level semantics and ignore lexical similarity, which remains important. Large Language Model (LLM) based evaluators, though powerful, come with drawbacks like high costs, bias, inconsistency, and hallucinations. To address these issues, we introduce SMILE: Semantic Metric Integrating Lexical Exactness, a novel approach that combines sentence-level semantic understanding with keyword-level semantic understanding and easy keyword matching. This composite method balances lexical precision and semantic relevance, offering a comprehensive evaluation. Extensive benchmarks across text, image, and video QA tasks show SMILE is highly correlated with human judgments and computationally lightweight, bridging the gap between lexical and semantic evaluation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ–‡æœ¬å’Œè§†è§‰é—®ç­”(QA)è¯„ä¼°ä¸­ä¼ ç»ŸæŒ‡æ ‡å¦‚ROUGEã€METEORå’ŒExact Match (EM)è¿‡åº¦ä¾èµ–è¯æ±‡ç›¸ä¼¼åº¦ï¼Œä»¥åŠBERTScoreã€MoverScoreç¼ºä¹çµæ´»æ€§ä¸”å¿½ç•¥è¯æ±‡ç›¸ä¼¼åº¦ç­‰é—®é¢˜ï¼Œæå‡ºäº†SMILEï¼šSemantic Metric Integrating Lexical Exactnessã€‚è¿™æ˜¯ä¸€ç§æ–°å‹çš„å¤åˆè¯„ä»·æŒ‡æ ‡ï¼Œé€šè¿‡ç»“åˆå¥å­çº§è¯­ä¹‰ç†è§£ã€å…³é”®è¯çº§è¯­ä¹‰ç†è§£ä»¥åŠä¾¿æ·çš„å…³é”®è¯åŒ¹é…ï¼Œå®ç°äº†è¯æ±‡ç²¾ç¡®åº¦ä¸è¯­ä¹‰ç›¸å…³æ€§ä¹‹é—´çš„æœ‰æ•ˆå¹³è¡¡ã€‚SMILEå…‹æœäº†å¤§è¯­è¨€æ¨¡å‹(LLM)è¯„ä¼°å™¨æˆæœ¬é«˜ã€å­˜åœ¨åè§å’Œä¸ä¸€è‡´æ€§ç­‰ç¼ºç‚¹ï¼Œæä¾›äº†ä¸€ç§å…¨é¢çš„è¯„ä¼°æ‰‹æ®µã€‚åœ¨æ¶µç›–æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘é—®ç­”ä»»åŠ¡çš„å¹¿æ³›åŸºå‡†æµ‹è¯•ä¸­ï¼ŒSMILEå±•ç°å‡ºä¸äººç±»åˆ¤æ–­çš„é«˜åº¦ç›¸å…³æ€§ï¼Œä¸”è®¡ç®—è¿‡ç¨‹è½»é‡åŒ–ã€‚è¯¥æ–¹æ³•æˆåŠŸå¼¥åˆäº†è¯æ±‡è¯„ä¼°ä¸è¯­ä¹‰è¯„ä¼°ä¹‹é—´çš„é¸¿æ²Ÿï¼Œä¸ºé—®ç­”ç³»ç»Ÿçš„å‡†ç¡®è¯„ä»·æä¾›äº†æ–°çš„å·¥å…·ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "23 pages, 6 tables, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.17432v1",
      "published_date": "2025-11-21 17:30:18 UTC",
      "updated_date": "2025-11-21 17:30:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:57:01.170608+00:00"
    },
    {
      "arxiv_id": "2601.14259v1",
      "title": "A Cloud-Based Cross-Modal Transformer for Emotion Recognition and Adaptive Human-Computer Interaction",
      "title_zh": "ç”¨äºæƒ…æ„Ÿè¯†åˆ«ä¸è‡ªé€‚åº”äººæœºäº¤äº’çš„äº‘ç«¯è·¨æ¨¡æ€Transformer",
      "authors": [
        "Ziwen Zhong",
        "Zhitao Shu",
        "Yue Zhao"
      ],
      "abstract": "Emotion recognition is a fundamental component of next-generation human-computer interaction (HCI), enabling machines to perceive, understand, and respond to users' affective states. However, existing systems often rely on single-modality analysis such as facial expressions, speech tone, or textual sentiment, resulting in limited robustness and poor generalization in real-world environments. To address these challenges, this study proposes a Cloud-Based Cross-Modal Transformer (CMT) framework for multimodal emotion recognition and adaptive human-computer interaction. The proposed model integrates visual, auditory, and textual signals using pretrained encoders (Vision Transformer, Wav2Vec2, and BERT) and employs a cross-modal attention mechanism to capture complex interdependencies among heterogeneous features. By leveraging cloud computing infrastructure with distributed training on Kubernetes and TensorFlow Serving, the system enables scalable, low-latency emotion recognition for large-scale user interactions. Experiments conducted on benchmark datasets including IEMOCAP, MELD, and AffectNet demonstrate that the CMT achieves state-of-the-art performance, improving the F1-score by 3.0 percent and reducing cross-entropy loss by 12.9 percent compared to strong multimodal baselines. Additionally, cloud deployment evaluations show an average response latency of 128 ms, representing a 35 percent reduction compared with conventional transformer-based fusion systems. These results confirm that the proposed framework enables efficient, real-time emotion recognition and adaptive feedback in applications such as intelligent customer service, virtual tutoring systems, and affective computing interfaces, marking an important step toward cloud-native affective computing and emotionally intelligent interactive systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºäº‘ç«¯çš„è·¨æ¨¡æ€è½¬æ¢å™¨ (Cloud-Based Cross-Modal Transformer, CMT) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿå•æ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ç³»ç»Ÿåœ¨ç°å®ç¯å¢ƒä¸­é²æ£’æ€§ä¸è¶³å’Œæ³›åŒ–èƒ½åŠ›å·®çš„é—®é¢˜ã€‚è¯¥æ¨¡å‹åˆ©ç”¨é¢„è®­ç»ƒç¼–ç å™¨ Vision Transformerã€Wav2Vec2 å’Œ BERT åˆ†åˆ«é›†æˆè§†è§‰ã€éŸ³é¢‘åŠæ–‡æœ¬ä¿¡å·ï¼Œå¹¶é‡‡ç”¨è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶ (Cross-modal attention mechanism) æ•æ‰å¼‚æ„ç‰¹å¾é—´çš„å¤æ‚ä¾èµ–å…³ç³»ã€‚ä¸ºäº†æ”¯æŒå¤§è§„æ¨¡ç”¨æˆ·äº¤äº’ï¼Œç³»ç»ŸåŸºäº Kubernetes å’Œ TensorFlow Serving æ„å»ºäº†åˆ†å¸ƒå¼äº‘ç«¯åŸºç¡€è®¾æ–½ï¼Œå®ç°äº†é«˜å¯æ‰©å±•æ€§ä¸ä½å»¶è¿Ÿå¤„ç†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCMT åœ¨ IEMOCAPã€MELD å’Œ AffectNet ç­‰åŸºå‡†æ•°æ®é›†ä¸Šè¾¾åˆ°äº† SOTA æ€§èƒ½ï¼Œå…¶ F1-score æå‡äº† 3.0% ä¸”äº¤å‰ç†µæŸå¤± (Cross-entropy loss) é™ä½äº† 12.9%ã€‚æ­¤å¤–ï¼Œäº‘ç«¯éƒ¨ç½²è¯„ä¼°è¡¨æ˜å…¶å¹³å‡å“åº”å»¶è¿Ÿä»…ä¸º 128 æ¯«ç§’ï¼Œè¾ƒä¼ ç»Ÿèåˆç³»ç»Ÿå‡å°‘äº† 35% çš„å»¶è¿Ÿã€‚è¯¥ç ”ç©¶ä¸ºæ™ºèƒ½å®¢æœå’Œè™šæ‹Ÿæ•™å­¦ç­‰åœºæ™¯ä¸‹çš„å®æ—¶æƒ…æ„Ÿè¯†åˆ«ä¸è‡ªé€‚åº”åé¦ˆæä¾›äº†é«˜æ•ˆæ–¹æ¡ˆï¼Œæ ‡å¿—ç€äº‘åŸç”Ÿæƒ…æ„Ÿè®¡ç®—é¢†åŸŸçš„é‡è¦è¿›å±•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.14259v1",
      "published_date": "2025-11-21 17:29:16 UTC",
      "updated_date": "2025-11-21 17:29:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:57:17.779609+00:00"
    },
    {
      "arxiv_id": "2511.17696v1",
      "title": "Liberating Logic in the Age of AI: Going Beyond Programming with Computational Thinking",
      "title_zh": "AI æ—¶ä»£çš„é€»è¾‘è§£æ”¾ï¼šè¶…è¶Šç¼–ç¨‹ï¼Œè¿ˆå‘è®¡ç®—æ€ç»´",
      "authors": [
        "Douglas C. Schmidt",
        "Dan Runfola"
      ],
      "abstract": "Mastering one or more programming languages has historically been the gateway to implementing ideas on a computer. Today, that gateway is widening with advances in large language models (LLMs) and artificial intelligence (AI)-powered coding assistants. What matters is no longer just fluency in traditional programming languages but the ability to think computationally by translating problems into forms that can be solved with computing tools. The capabilities enabled by these AI-augmented tools are rapidly leading to the commoditization of computational thinking, such that anyone who can articulate a problem in natural language can potentially harness computing power via AI.\n  This shift is poised to radically influence how we teach computer science and data science in the United States and around the world. Educators and industry leaders are grappling with how to adapt: What should students learn when the hottest new programming language is English? How do we prepare a generation of computational thinkers who need not code every algorithm manually, but must still think critically, design solutions, and verify AI-augmented results?\n  This paper explores these questions, examining the impact of natural language programming on software development, the emerging distinction between programmers and prompt-crafting problem solvers, the reforms needed in computer science and data science curricula, and the importance of maintaining our fundamental computational science principles in an AI-augmented future. Along the way, we compare approaches and share best practices for embracing this new paradigm in computing education.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å¤§è¯­è¨€æ¨¡å‹(LLMs)å’ŒAIç¼–ç¨‹åŠ©æ‰‹å…´èµ·çš„èƒŒæ™¯ä¸‹ï¼Œè®¡ç®—æ€ç»´(Computational Thinking)å¦‚ä½•è¶…è¶Šä¼ ç»Ÿç¼–ç¨‹è¯­è¨€æˆä¸ºå®ç°åˆ›æ„çš„æ ¸å¿ƒã€‚è®ºæ–‡æŒ‡å‡ºï¼Œéšç€è®¡ç®—æ€ç»´çš„å•†å“åŒ–ï¼Œå°†é—®é¢˜è½¬åŒ–ä¸ºå¯è®¡ç®—å½¢å¼çš„èƒ½åŠ›æ¯”æŒæ¡ç‰¹å®šç¼–ç¨‹è¯­è¨€çš„ç†Ÿç»ƒåº¦æ›´ä¸ºå…³é”®ã€‚è¿™ä¸€è½¬å˜æ­£æ·±åˆ»å½±å“ç€è®¡ç®—æœºç§‘å­¦å’Œæ•°æ®ç§‘å­¦çš„æ•™è‚²æ¨¡å¼ï¼Œå°¤å…¶æ˜¯å½“è‹±è¯­æˆä¸ºä¸€ç§æ–°çš„â€œç¼–ç¨‹è¯­è¨€â€æ—¶ï¼Œæ•™è‚²è€…éœ€é‡æ–°å®šä¹‰å­¦ç”Ÿçš„å­¦ä¹ å†…å®¹ã€‚æ–‡ç« é‡ç‚¹è€ƒå¯Ÿäº†è‡ªç„¶è¯­è¨€ç¼–ç¨‹(Natural Language Programming)å¯¹è½¯ä»¶å¼€å‘çš„å½±å“ï¼Œä»¥åŠä¼ ç»Ÿç¨‹åºå‘˜ä¸â€œæç¤ºè¯é©±åŠ¨çš„é—®é¢˜è§£å†³è€…â€ä¹‹é—´çš„è§’è‰²åŒºåˆ†ã€‚ä½œè€…æå‡ºäº†ä¸€ç³»åˆ—é’ˆå¯¹è¯¾ç¨‹ä½“ç³»çš„æ”¹é©æ–¹æ¡ˆï¼Œå¼ºè°ƒåœ¨AIå¢å¼ºçš„æœªæ¥ä¸­ä»éœ€åšæŒè®¡ç®—ç§‘å­¦çš„åŸºæœ¬åŸåˆ™ï¼Œå¹¶åŸ¹å…»å­¦ç”Ÿè®¾è®¡è§£å†³æ–¹æ¡ˆåŠéªŒè¯AIç”Ÿæˆç»“æœçš„èƒ½åŠ›ã€‚é€šè¿‡å¯¹æ¯”ä¸åŒçš„æ•™è‚²è·¯å¾„å¹¶åˆ†äº«æœ€ä½³å®è·µï¼Œè¯¥ç ”ç©¶ä¸ºæ•™è‚²ç•Œå’Œå·¥ä¸šç•Œé€‚åº”è¿™ä¸€å…¨æ–°çš„è®¡ç®—èŒƒå¼æä¾›äº†é‡è¦æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "15 pages and 17 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.17696v1",
      "published_date": "2025-11-21 17:28:52 UTC",
      "updated_date": "2025-11-21 17:28:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:57:28.297661+00:00"
    },
    {
      "arxiv_id": "2511.17421v2",
      "title": "Preventing Shortcut Learning in Medical Image Analysis through Intermediate Layer Knowledge Distillation from Specialist Teachers",
      "title_zh": "é€šè¿‡ä¸“å®¶æ•™å¸ˆä¸­é—´å±‚çŸ¥è¯†è’¸é¦è§„é¿åŒ»å­¦å›¾åƒåˆ†æä¸­çš„æ·å¾„å­¦ä¹ ",
      "authors": [
        "Christopher Boland",
        "Sotirios Tsaftaris",
        "Sonia Dahdouh"
      ],
      "abstract": "Deep learning models are prone to learning shortcut solutions to problems using spuriously correlated yet irrelevant features of their training data. In high-risk applications such as medical image analysis, this phenomenon may prevent models from using clinically meaningful features when making predictions, potentially leading to poor robustness and harm to patients. We demonstrate that different types of shortcuts (those that are diffuse and spread throughout the image, as well as those that are localized to specific areas) manifest distinctly across network layers and can, therefore, be more effectively targeted through mitigation strategies that target the intermediate layers. We propose a novel knowledge distillation framework that leverages a teacher network fine-tuned on a small subset of task-relevant data to mitigate shortcut learning in a student network trained on a large dataset corrupted with a bias feature. Through extensive experiments on CheXpert, ISIC 2017, and SimBA datasets using various architectures (ResNet-18, AlexNet, DenseNet-121, and 3D CNNs), we demonstrate consistent improvements over traditional Empirical Risk Minimization, augmentation-based bias-mitigation, and group-based bias-mitigation approaches. In many cases, we achieve comparable performance with a baseline model trained on bias-free data, even on out-of-distribution test data. Our results demonstrate the practical applicability of our approach to real-world medical imaging scenarios where bias annotations are limited and shortcut features are difficult to identify a priori.",
      "tldr_zh": "æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨åŒ»ç–—å›¾åƒåˆ†æä¸­å®¹æ˜“å­¦ä¹ åˆ°æ•°æ®ä¸­è™šå‡ç›¸å…³çš„æ·å¾„(Shortcut Learning)ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹æå–å…·æœ‰ä¸´åºŠæ„ä¹‰ç‰¹å¾çš„èƒ½åŠ›å¹¶é™ä½äº†é¢„æµ‹çš„é²æ£’æ€§ã€‚ç ”ç©¶å‘ç°ä¸åŒç±»å‹çš„æ·å¾„åœ¨ç½‘ç»œå„å±‚ä¸­è¡¨ç°å„å¼‚ï¼Œå› æ­¤é€šè¿‡ä¸­é—´å±‚(intermediate layers)è¿›è¡Œå¹²é¢„èƒ½æ›´æœ‰æ•ˆåœ°æŠ‘åˆ¶è¿™äº›éç›¸å…³ç‰¹å¾ã€‚ä¸ºæ­¤ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„çŸ¥è¯†è’¸é¦(Knowledge Distillation)æ¡†æ¶ï¼Œåˆ©ç”¨åœ¨å°‘é‡ä»»åŠ¡ç›¸å…³å­é›†ä¸Šå¾®è°ƒçš„ä¸“å®¶æ•™å¸ˆç½‘ç»œï¼ŒæŒ‡å¯¼åœ¨åŒ…å«åè§ç‰¹å¾çš„å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè®­ç»ƒçš„å­¦ç”Ÿç½‘ç»œã€‚å®éªŒåœ¨CheXpertã€ISIC 2017å’ŒSimBAæ•°æ®é›†ä¸Šåˆ©ç”¨ResNet-18ã€DenseNet-121ç­‰å¤šç§æ¶æ„è¿›è¡Œäº†éªŒè¯ï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•æŒç»­ä¼˜äºä¼ ç»Ÿçš„ç»éªŒé£é™©æœ€å°åŒ–(Empirical Risk Minimization)å’Œç°æœ‰çš„åè§æ¶ˆé™¤æŠ€æœ¯ã€‚åœ¨å¤šä¸ªå®éªŒåœºæ™¯ä¸­ï¼Œè¯¥æ¡†æ¶å³ä½¿åœ¨åˆ†å¸ƒå¤–(out-of-distribution)æµ‹è¯•æ•°æ®ä¸Šï¼Œä¹Ÿèƒ½è¾¾åˆ°ä¸åœ¨æ— åè§æ•°æ®ä¸Šè®­ç»ƒçš„åŸºå‡†æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚è¿™ç§æ–¹æ³•ä¸ºåè§æ ‡æ³¨æœ‰é™ä¸”éš¾ä»¥é¢„å…ˆè¯†åˆ«æ·å¾„ç‰¹å¾çš„ç°å®åŒ»å­¦å½±åƒåº”ç”¨æä¾›äº†æå…·å®ç”¨æ€§çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted for publication at the Journal of Machine Learning for Biomedical Imaging (MELBA) https://melba-journal.org/2025:020",
      "pdf_url": "https://arxiv.org/pdf/2511.17421v2",
      "published_date": "2025-11-21 17:18:35 UTC",
      "updated_date": "2025-11-24 10:32:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:57:19.968320+00:00"
    },
    {
      "arxiv_id": "2511.17419v2",
      "title": "DS-Span: Single-Phase Discriminative Subgraph Mining for Efficient Graph Embeddings",
      "title_zh": "DS-Spanï¼šé¢å‘é«˜æ•ˆå›¾åµŒå…¥çš„å•é˜¶æ®µåˆ¤åˆ«æ€§å­å›¾æŒ–æ˜",
      "authors": [
        "Yeamin Kaiser",
        "Muhammed Tasnim Bin Anwar",
        "Bholanath Das"
      ],
      "abstract": "Graph representation learning seeks to transform complex, high-dimensional graph structures into compact vector spaces that preserve both topology and semantics. Among the various strategies, subgraph-based methods provide an interpretable bridge between symbolic pattern discovery and continuous embedding learning. Yet, existing frequent or discriminative subgraph mining approaches often suffer from redundant multi-phase pipelines, high computational cost, and weak coupling between mined structures and their discriminative relevance. We propose DS-Span, a single-phase discriminative subgraph mining framework that unifies pattern growth, pruning, and supervision-driven scoring within one traversal of the search space. DS-Span introduces a coverage-capped eligibility mechanism that dynamically limits exploration once a graph is sufficiently represented, and an information-gain-guided selection that promotes subgraphs with strong class-separating ability while minimizing redundancy. The resulting subgraph set serves as an efficient, interpretable basis for downstream graph embedding and classification. Extensive experiments across benchmarks demonstrate that DS-Span generates more compact and discriminative subgraph features than prior multi-stage methods, achieving higher or comparable accuracy with significantly reduced runtime. These results highlight the potential of unified, single-phase discriminative mining as a foundation for scalable and interpretable graph representation learning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DS-Spanï¼Œä¸€ç§å•é˜¶æ®µåˆ¤åˆ«å¼å­å›¾æŒ–æ˜æ¡†æ¶(Single-Phase Discriminative Subgraph Mining)ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å­å›¾æ–¹æ³•ä¸­æµæ°´çº¿å†—ä½™ã€è®¡ç®—æˆæœ¬é«˜ä»¥åŠæŒ–æ˜ç»“æ„ä¸åˆ¤åˆ«ç›¸å…³æ€§è€¦åˆè¾ƒå¼±ç­‰æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶åœ¨å•æ¬¡æœç´¢ç©ºé—´éå†ä¸­ç»Ÿä¸€äº†æ¨¡å¼å¢é•¿(Pattern Growth)ã€å‰ªæ(Pruning)å’Œç›‘ç£é©±åŠ¨è¯„åˆ†(Supervision-Driven Scoring)ï¼Œä»è€Œå¤§å¹…æå‡äº†æŒ–æ˜æ•ˆç‡ã€‚DS-Span å¼•å…¥äº†è¦†ç›–ä¸Šé™èµ„æ ¼æœºåˆ¶(Coverage-Capped Eligibility Mechanism)æ¥åŠ¨æ€é™åˆ¶å¯¹å·²å……åˆ†è¡¨ç¤ºå›¾çš„æ¢ç´¢ï¼Œå¹¶é‡‡ç”¨ä¿¡æ¯å¢ç›ŠæŒ‡å¯¼çš„é€‰æ‹©ç­–ç•¥(Information-Gain-Guided Selection)æ¥å¢å¼ºå­å›¾çš„ç±»åˆ«åˆ†ç¦»èƒ½åŠ›å¹¶å‡å°‘å†—ä½™ã€‚æŒ–æ˜å‡ºçš„å­å›¾é›†ä¸ºä¸‹æ¸¸çš„å›¾åµŒå…¥(Graph Embedding)å’Œåˆ†ç±»ä»»åŠ¡æä¾›äº†ä¸€ä¸ªé«˜æ•ˆä¸”å…·æœ‰å¯è§£é‡Šæ€§çš„åŸºç¡€ã€‚åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒDS-Span ç›¸æ¯”ä¼ ç»Ÿå¤šé˜¶æ®µæ–¹æ³•èƒ½ç”Ÿæˆæ›´ç´§å‡‘çš„ç‰¹å¾ï¼Œåœ¨æ˜¾è‘—ç¼©çŸ­è¿è¡Œæ—¶é—´çš„åŒæ—¶å®ç°äº†æ›´ä¼˜æˆ–ç›¸å½“çš„å‡†ç¡®ç‡ã€‚è¿™ä¸€ç ”ç©¶å±•ç¤ºäº†ç»Ÿä¸€çš„å•é˜¶æ®µåˆ¤åˆ«å¼æŒ–æ˜åœ¨æ„å»ºå¯æ‰©å±•ä¸”å…·å¯è§£é‡Šæ€§çš„å›¾è¡¨ç¤ºå­¦ä¹ (Graph Representation Learning)æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17419v2",
      "published_date": "2025-11-21 17:17:51 UTC",
      "updated_date": "2025-12-03 21:48:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:57:31.962358+00:00"
    },
    {
      "arxiv_id": "2511.17408v3",
      "title": "The Impact of Off-Policy Training Data on Probe Generalisation",
      "title_zh": "ç¦»ç­–è®­ç»ƒæ•°æ®å¯¹æ¢é’ˆæ³›åŒ–çš„å½±å“",
      "authors": [
        "Nathalie Kirch",
        "Samuel Dower",
        "Adrians Skapars",
        "Ekdeep Singh Lubana",
        "Dmitrii Krasheninnikov"
      ],
      "abstract": "Probing has emerged as a promising method for monitoring large language models (LLMs), enabling cheap inference-time detection of concerning behaviours. However, natural examples of many behaviours are rare, forcing researchers to rely on synthetic or off-policy LLM responses for training probes. We systematically evaluate how off-policy data influences probe generalisation across eight distinct LLM behaviours. Testing linear and attention probes across multiple LLMs, we find that training data generation strategy can significantly affect probe performance, though the magnitude varies greatly by behaviour. The largest generalisation failures arise for behaviours defined by response \"intent\" (e.g. strategic deception) rather than text-level content (e.g. usage of lists). We then propose a useful test for predicting generalisation failures in cases where on-policy test data is unavailable: successful generalisation to incentivised data (where the model was coerced) strongly correlates with high performance against on-policy examples. Based on these results, we predict that current deception probes may fail to generalise to real monitoring scenarios. Additionally, our finding that off-policy data can yield more reliable probes than on-policy data from a sufficiently different setting underscores the need for new monitoring methods that better handle all types of distribution shift.",
      "tldr_zh": "æœ¬ç ”ç©¶ç³»ç»Ÿæ¢è®¨äº†ç¦»ç­–ç•¥è®­ç»ƒæ•°æ®(Off-Policy Training Data)å¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)æ¢æµ‹å™¨æ³›åŒ–(Probe Generalisation)èƒ½åŠ›çš„å½±å“ã€‚é’ˆå¯¹ç›‘æ§å¼‚å¸¸è¡Œä¸ºæ—¶è‡ªç„¶æ ·æœ¬ç¨€ç¼ºçš„é—®é¢˜ï¼Œç ”ç©¶è€…é€šå¸¸ä¾èµ–åˆæˆæˆ–éç›®æ ‡æ¨¡å‹ç”Ÿæˆçš„æ•°æ®æ¥è®­ç»ƒæ¢æµ‹å™¨ã€‚å®éªŒè¯„ä¼°äº†å…«ç§ä¸åŒLLMè¡Œä¸ºä¸‹çš„çº¿æ€§(Linear)å’Œæ³¨æ„åŠ›æ¢æµ‹å™¨(Attention Probes)ï¼Œå‘ç°æ•°æ®ç”Ÿæˆç­–ç•¥å¯¹æ¢æµ‹å™¨æ€§èƒ½æœ‰æ˜¾è‘—å½±å“ï¼Œä¸”å½±å“ç¨‹åº¦å› è¡Œä¸ºç±»å‹è€Œå¼‚ã€‚åœ¨æ¶‰åŠâ€œæ„å›¾â€ç±»è¡Œä¸ºï¼ˆå¦‚ç­–ç•¥æ€§æ¬ºéª—Strategic Deceptionï¼‰æ—¶ï¼Œæ³›åŒ–å¤±æ•ˆæœ€ä¸ºä¸¥é‡ï¼Œè€Œæ–‡æœ¬å±‚é¢(Text-level)çš„è¡Œä¸ºè¡¨ç°ç›¸å¯¹ç¨³å¥ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§åœ¨ç¼ºä¹åœ¨ç­–ç•¥(On-policy)æµ‹è¯•æ•°æ®æ—¶é¢„æµ‹æ³›åŒ–å¤±æ•ˆçš„æ–¹æ³•ï¼Œå³åˆ©ç”¨å—æ¿€åŠ±æ•°æ®(Incentivised Data)çš„æ³›åŒ–è¡¨ç°ä½œä¸ºåœ¨ç­–ç•¥è¡¨ç°çš„é¢„æµ‹æŒ‡æ ‡ã€‚ç»“æœè¡¨æ˜ç›®å‰çš„æ¬ºéª—æ¢æµ‹å™¨å¯èƒ½éš¾ä»¥æ³›åŒ–åˆ°çœŸå®çš„ç›‘æ§åœºæ™¯ä¸­ï¼Œå¹¶å¼ºè°ƒäº†å¼€å‘èƒ½æ›´å¥½å¤„ç†åˆ†å¸ƒåç§»(Distribution Shift)çš„ç›‘æ§æ–¹æ³•çš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, EurIPS 2025 Workshop on Private AI Governance",
      "pdf_url": "https://arxiv.org/pdf/2511.17408v3",
      "published_date": "2025-11-21 17:08:48 UTC",
      "updated_date": "2026-01-12 22:53:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:57:39.386832+00:00"
    },
    {
      "arxiv_id": "2511.17405v2",
      "title": "Beyond Multiple Choice: Verifiable OpenQA for Robust Vision-Language RFT",
      "title_zh": "è¶…è¶Šå¤šé¡¹é€‰æ‹©ï¼šé¢å‘é²æ£’è§†è§‰-è¯­è¨€å¼ºåŒ–å¾®è°ƒçš„å¯éªŒè¯å¼€æ”¾å¼é—®ç­”",
      "authors": [
        "Yesheng Liu",
        "Hao Li",
        "Haiyu Xu",
        "Baoqi Pei",
        "Jiahao Wang",
        "Mingxuan Zhao",
        "Jingshu Zheng",
        "Zheqi He",
        "JG Yao",
        "Bowen Qin",
        "Xi Yang",
        "Jiajun Zhang"
      ],
      "abstract": "Multiple-choice question answering (MCQA) has been a popular format for evaluating and reinforcement fine-tuning (RFT) of modern multimodal language models. Its constrained output format allows for simplified, deterministic automatic verification. However, we find that the options may leak exploitable signals, which makes the accuracy metrics unreliable for indicating real capabilities and encourages explicit or implicit answer guessing behaviors during RFT. We propose ReVeL (Rewrite and Verify by LLM), a framework that rewrites multiple-choice questions into open-form questions while keeping answers verifiable whenever possible. The framework categorizes questions according to different answer types, apply different rewriting and verification schemes, respectively. When applied for RFT, we converted 20k MCQA examples and use GRPO to finetune Qwen2.5-VL models. Models trained on ReVeL-OpenQA match MCQA accuracy on multiple-choice benchmarks and improve OpenQA accuracy by about six percentage points, indicating better data efficiency and more robust reward signals than MCQA-based training. When used for evaluation, ReVeL also reveals up to 20 percentage points of score inflation in MCQA benchmarks (relative to OpenQA), improves judging accuracy, and reduces both cost and latency. We will release code and data publicly.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨å¤šé€‰é¢˜ï¼ˆMCQAï¼‰è¯„ä¼°ä¸å¼ºåŒ–å­¦ä¹ å¾®è°ƒï¼ˆRFTï¼‰ä¸­å­˜åœ¨çš„é€‰é¡¹ä¿¡å·æ³„éœ²åŠæ€§èƒ½è™šé«˜é—®é¢˜ï¼Œæå‡ºäº† ReVeLï¼ˆRewrite and Verify by LLMï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿå°† MCQA è‡ªåŠ¨é‡å†™ä¸ºå¯éªŒè¯çš„å¼€æ”¾å¼é—®é¢˜ï¼ˆOpenQAï¼‰ï¼Œå¹¶é’ˆå¯¹ä¸åŒç­”æ¡ˆç±»å‹é‡‡ç”¨å·®å¼‚åŒ–çš„é‡å†™ä¸éªŒè¯æ–¹æ¡ˆã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨ ReVeL è½¬æ¢äº†2ä¸‡ä¸ªæ ·æœ¬ï¼Œå¹¶é‡‡ç”¨ GRPO ç®—æ³•å¯¹ Qwen2.5-VL æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç›¸æ¯”ä¼ ç»Ÿçš„ MCQA è®­ç»ƒï¼ŒåŸºäº ReVeL-OpenQA çš„æ¨¡å‹åœ¨ä¿æŒ MCQA å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œå°† OpenQA çš„å‡†ç¡®ç‡æå‡äº†çº¦6ä¸ªç™¾åˆ†ç‚¹ï¼Œå±•ç°å‡ºæ›´å¼ºçš„æ•°æ®æ•ˆç‡å’Œæ›´ç¨³å¥çš„å¥–åŠ±ä¿¡å·ã€‚æ­¤å¤–ï¼ŒReVeL æ­ç¤ºäº†ç°æœ‰ MCQA åŸºå‡†æµ‹è¯•ä¸­å­˜åœ¨é«˜è¾¾20%çš„åˆ†æ•°é€šèƒ€ï¼Œå¹¶åœ¨æå‡è¯„åˆ¤å‡†ç¡®æ€§çš„åŒæ—¶é™ä½äº†è¯„ä¼°æˆæœ¬å’Œå»¶è¿Ÿã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Project url: https://flageval-baai.github.io/ReVeL/",
      "pdf_url": "https://arxiv.org/pdf/2511.17405v2",
      "published_date": "2025-11-21 17:06:37 UTC",
      "updated_date": "2025-11-24 02:45:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:58:15.273064+00:00"
    },
    {
      "arxiv_id": "2511.17400v1",
      "title": "Sparse Mixture-of-Experts for Multi-Channel Imaging: Are All Channel Interactions Required?",
      "title_zh": "é¢å‘å¤šé€šé“æˆåƒçš„ç¨€ç–æ··åˆä¸“å®¶æ¨¡å‹ï¼šæ˜¯å¦éœ€è¦æ‰€æœ‰çš„é€šé“äº¤äº’ï¼Ÿ",
      "authors": [
        "Sukwon Yun",
        "Heming Yao",
        "Burkhard Hoeckendorf",
        "David Richmond",
        "Aviv Regev",
        "Russell Littman"
      ],
      "abstract": "Vision Transformers ($\\text{ViTs}$) have become the backbone of vision foundation models, yet their optimization for multi-channel domains - such as cell painting or satellite imagery - remains underexplored. A key challenge in these domains is capturing interactions between channels, as each channel carries different information. While existing works have shown efficacy by treating each channel independently during tokenization, this approach naturally introduces a major computational bottleneck in the attention block - channel-wise comparisons leads to a quadratic growth in attention, resulting in excessive $\\text{FLOPs}$ and high training cost. In this work, we shift focus from efficacy to the overlooked efficiency challenge in cross-channel attention and ask: \"Is it necessary to model all channel interactions?\". Inspired by the philosophy of Sparse Mixture-of-Experts ($\\text{MoE}$), we propose MoE-ViT, a Mixture-of-Experts architecture for multi-channel images in $\\text{ViTs}$, which treats each channel as an expert and employs a lightweight router to select only the most relevant experts per patch for attention. Proof-of-concept experiments on real-world datasets - JUMP-CP and So2Sat - demonstrate that $\\text{MoE-ViT}$ achieves substantial efficiency gains without sacrificing, and in some cases enhancing, performance, making it a practical and attractive backbone for multi-channel imaging.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šé€šé“æˆåƒé¢†åŸŸä¸­ Vision Transformers (ViTs) çš„ä¼˜åŒ–éš¾é¢˜ï¼Œæå‡ºäº†è·¨é€šé“æ³¨æ„åŠ›æœºåˆ¶ä¸­æ˜¯å¦éœ€è¦å»ºæ¨¡æ‰€æœ‰é€šé“äº¤äº’çš„ç–‘é—®ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œä¼ ç»Ÿæ–¹æ³•å› é€šé“é—´çš„äºŒæ¬¡æ–¹å¢é•¿è€Œé¢ä¸´å·¨å¤§çš„è®¡ç®—ç“¶é¢ˆå’Œé«˜æ˜‚çš„è®­ç»ƒæˆæœ¬ã€‚ä¸ºæ­¤ï¼Œä½œè€…å—åˆ° Sparse Mixture-of-Experts (MoE) çš„å¯å‘ï¼Œæå‡ºäº† MoE-ViT æ¶æ„ï¼Œå°†æ¯ä¸ªé€šé“è§†ä¸ºä¸“å®¶(expert)ï¼Œå¹¶é‡‡ç”¨è½»é‡çº§è·¯ç”±(router)ä¸ºæ¯ä¸ªå›¾åƒå—(patch)é€‰æ‹©æœ€ç›¸å…³çš„ä¸“å®¶å‚ä¸è®¡ç®—ã€‚åœ¨ JUMP-CP å’Œ So2Sat æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒMoE-ViT åœ¨ä¸ç‰ºç‰²æ€§èƒ½ç”šè‡³æœ‰æ‰€å¢å¼ºçš„å‰æä¸‹ï¼Œå®ç°äº†æ˜¾è‘—çš„æ•ˆç‡æå‡ã€‚è¯¥ç ”ç©¶ä¸ºå¤„ç†å¤šé€šé“æˆåƒä»»åŠ¡æä¾›äº†ä¸€ä¸ªé«˜æ•ˆä¸”å®ç”¨çš„ä¸»å¹²ç½‘ç»œæ¨¡å‹ï¼Œæœ‰æ•ˆç¼“è§£äº†ä¼ ç»Ÿæ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—å‹åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This has been accepted at the NeurIPS AI4Science Workshop 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.17400v1",
      "published_date": "2025-11-21 17:00:02 UTC",
      "updated_date": "2025-11-21 17:00:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:58:14.387160+00:00"
    },
    {
      "arxiv_id": "2511.17393v1",
      "title": "Designing and Generating Diverse, Equitable Face Image Datasets for Face Verification Tasks",
      "title_zh": "é¢å‘äººè„¸éªŒè¯ä»»åŠ¡çš„å¤šæ ·åŒ–ã€å…¬å¹³äººè„¸å›¾åƒæ•°æ®é›†çš„è®¾è®¡ä¸ç”Ÿæˆ",
      "authors": [
        "Georgia Baltsou",
        "Ioannis Sarridis",
        "Christos Koutlis",
        "Symeon Papadopoulos"
      ],
      "abstract": "Face verification is a significant component of identity authentication in various applications including online banking and secure access to personal devices. The majority of the existing face image datasets often suffer from notable biases related to race, gender, and other demographic characteristics, limiting the effectiveness and fairness of face verification systems. In response to these challenges, we propose a comprehensive methodology that integrates advanced generative models to create varied and diverse high-quality synthetic face images. This methodology emphasizes the representation of a diverse range of facial traits, ensuring adherence to characteristics permissible in identity card photographs. Furthermore, we introduce the Diverse and Inclusive Faces for Verification (DIF-V) dataset, comprising 27,780 images of 926 unique identities, designed as a benchmark for future research in face verification. Our analysis reveals that existing verification models exhibit biases toward certain genders and races, and notably, applying identity style modifications negatively impacts model performance. By tackling the inherent inequities in existing datasets, this work not only enriches the discussion on diversity and ethics in artificial intelligence but also lays the foundation for developing more inclusive and reliable face verification technologies",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰äººè„¸éªŒè¯ (Face verification) æ•°æ®é›†åœ¨ç§æ—å’Œæ€§åˆ«ç­‰äººå£ç»Ÿè®¡å­¦ç‰¹å¾ä¸Šå­˜åœ¨çš„æ˜¾è‘—åå·®ï¼Œæå‡ºäº†ä¸€å¥—æ•´åˆå…ˆè¿›ç”Ÿæˆæ¨¡å‹ (Generative models) çš„ç»¼åˆæ–¹æ³•ï¼Œç”¨äºç”Ÿæˆç¬¦åˆèº«ä»½è¯è§„èŒƒçš„å¤šæ ·åŒ–ã€é«˜è´¨é‡åˆæˆäººè„¸å›¾åƒã€‚é€šè¿‡è¯¥æ–¹æ³•ï¼Œç ”ç©¶è€…æ„å»ºäº†åŒ…å« 926 ä¸ªå”¯ä¸€èº«ä»½ã€å…± 27,780 å¼ å›¾åƒçš„ Diverse and Inclusive Faces for Verification (DIF-V) æ•°æ®é›†ï¼Œæ—¨åœ¨ä¸ºé¢éƒ¨éªŒè¯ä»»åŠ¡æä¾›æ›´å…·åŒ…å®¹æ€§çš„åŸºå‡†ã€‚å®éªŒåˆ†æè¡¨æ˜ï¼Œç°æœ‰éªŒè¯æ¨¡å‹åœ¨ä¸åŒæ€§åˆ«å’Œç§æ—é—´å­˜åœ¨æ˜æ˜¾åè§ï¼Œä¸”èº«ä»½é£æ ¼ä¿®æ”¹ (Identity style modifications) ä¼šå¯¹æ¨¡å‹æ€§èƒ½äº§ç”Ÿè´Ÿé¢å½±å“ã€‚è¯¥å·¥ä½œé€šè¿‡è§£å†³æ•°æ®é›†ä¸­çš„å›ºæœ‰ä¸å¹³ç­‰é—®é¢˜ï¼Œä¸ä»…ä¸°å¯Œäº†äººå·¥æ™ºèƒ½é¢†åŸŸå…³äºå¤šæ ·æ€§ä¸ä¼¦ç†çš„è®¨è®ºï¼Œä¹Ÿä¸ºå¼€å‘æ›´å…¬å¹³ã€æ›´å¯é çš„äººè„¸éªŒè¯æŠ€æœ¯å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17393v1",
      "published_date": "2025-11-21 16:53:08 UTC",
      "updated_date": "2025-11-21 16:53:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:58:35.373981+00:00"
    },
    {
      "arxiv_id": "2511.17372v1",
      "title": "Quantum Masked Autoencoders for Vision Learning",
      "title_zh": "é¢å‘è§†è§‰å­¦ä¹ çš„é‡å­æ©ç è‡ªç¼–ç å™¨",
      "authors": [
        "Emma Andrews",
        "Prabhat Mishra"
      ],
      "abstract": "Classical autoencoders are widely used to learn features of input data. To improve the feature learning, classical masked autoencoders extend classical autoencoders to learn the features of the original input sample in the presence of masked-out data. While quantum autoencoders exist, there is no design and implementation of quantum masked autoencoders that can leverage the benefits of quantum computing and quantum autoencoders. In this paper, we propose quantum masked autoencoders (QMAEs) that can effectively learn missing features of a data sample within quantum states instead of classical embeddings. We showcase that our QMAE architecture can learn the masked features of an image and can reconstruct the masked input image with improved visual fidelity in MNIST images. Experimental evaluation highlights that QMAE can significantly outperform (12.86% on average) in classification accuracy compared to state-of-the-art quantum autoencoders in the presence of masks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Quantum Masked Autoencoders (QMAEs)ï¼Œæ—¨åœ¨åˆ©ç”¨é‡å­è®¡ç®—çš„ä¼˜åŠ¿åœ¨Quantum Statesä¸­æœ‰æ•ˆå­¦ä¹ æ•°æ®çš„ç¼ºå¤±ç‰¹å¾ã€‚è™½ç„¶Classical Masked Autoencodersåœ¨ç»å…¸æœºå™¨å­¦ä¹ é¢†åŸŸåº”ç”¨å¹¿æ³›ï¼Œä½†QMAEså¡«è¡¥äº†é‡å­è®¡ç®—ä¸­ç¼ºä¹é’ˆå¯¹æ©ç æ•°æ®å­¦ä¹ è®¾è®¡çš„ç©ºç™½ã€‚è¯¥æ¶æ„èƒ½å¤Ÿç›´æ¥åœ¨é‡å­çŠ¶æ€ä¸‹æ•æ‰è¢«é®è”½å›¾åƒçš„ç‰¹å¾ï¼Œä»è€Œå®ç°æ›´é«˜Visual Fidelityçš„å›¾åƒé‡æ„ã€‚åœ¨MNISTæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒQMAEsåœ¨å¤„ç†å¸¦æ©ç çš„è¾“å…¥æ—¶è¡¨ç°å‡ºè‰²ã€‚ä¸ç›®å‰çš„State-of-the-art Quantum Autoencodersç›¸æ¯”ï¼Œè¯¥æ¨¡å‹åœ¨åˆ†ç±»å‡†ç¡®ç‡ä¸Šå¹³å‡æå‡äº†12.86%ï¼Œæ˜¾è‘—å¢å¼ºäº†é‡å­è§†è§‰å­¦ä¹ çš„æ€§èƒ½ã€‚",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17372v1",
      "published_date": "2025-11-21 16:37:18 UTC",
      "updated_date": "2025-11-21 16:37:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:58:26.281691+00:00"
    },
    {
      "arxiv_id": "2512.19696v1",
      "title": "QoS-Aware Dynamic CU Selection in O-RAN with Graph-Based Reinforcement Learning",
      "title_zh": "O-RANä¸­åŸºäºå›¾å¼ºåŒ–å­¦ä¹ çš„QoSæ„ŸçŸ¥åŠ¨æ€CUé€‰æ‹©",
      "authors": [
        "Sebastian Racedo",
        "Brigitte Jaumard",
        "Oscar Delgado",
        "Meysam Masoudi"
      ],
      "abstract": "Open Radio Access Network (O RAN) disaggregates conventional RAN into interoperable components, enabling flexible resource allocation, energy savings, and agile architectural design. In legacy deployments, the binding between logical functions and physical locations is static, which leads to inefficiencies under time varying traffic and resource conditions. We address this limitation by relaxing the fixed mapping and performing dynamic service function chain (SFC) provisioning with on the fly O CU selection. We formulate the problem as a Markov decision process and solve it using GRLDyP, i.e., a graph neural network (GNN) assisted deep reinforcement learning (DRL). The proposed agent jointly selects routes and the O-CU location (from candidate sites) for each incoming service flow to minimize network energy consumption while satisfying quality of service (QoS) constraints. The GNN encodes the instantaneous network topology and resource utilization (e.g., CPU and bandwidth), and the DRL policy learns to balance grade of service, latency, and energy. We perform the evaluation of GRLDyP on a data set with 24-hour traffic traces from the city of Montreal, showing that dynamic O CU selection and routing significantly reduce energy consumption compared to a static mapping baseline, without violating QoS. The results highlight DRL based SFC provisioning as a practical control primitive for energy-aware, resource-adaptive O-RAN deployments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Open Radio Access Network (O-RAN) ä¸­é€»è¾‘åŠŸèƒ½ä¸ç‰©ç†ä½ç½®é™æ€ç»‘å®šå¯¼è‡´çš„èµ„æºåˆ†é…ä½æ•ˆé—®é¢˜ï¼Œæå‡ºäº†åŠ¨æ€æœåŠ¡åŠŸèƒ½é“¾ (SFC) é…ç½®ä¸å³æ—¶ O-CU é€‰æ‹©æ–¹æ¡ˆã€‚ç ”ç©¶è€…å¼€å‘äº†åä¸º GRLDyP çš„æ¡†æ¶ï¼Œåˆ©ç”¨å›¾ç¥ç»ç½‘ç»œ (GNN) ç¼–ç ç¬æ—¶ç½‘ç»œæ‹“æ‰‘åŠ CPUã€å¸¦å®½ç­‰èµ„æºåˆ©ç”¨ä¿¡æ¯ï¼Œå¹¶ç»“åˆæ·±åº¦å¼ºåŒ–å­¦ä¹  (DRL) è¿›è¡Œå†³ç­–ã€‚è¯¥æ™ºèƒ½ä½“åœ¨æ»¡è¶³æœåŠ¡è´¨é‡ (QoS) çº¦æŸçš„å‰æä¸‹ï¼Œè”åˆä¼˜åŒ–è·¯ç”±é€‰æ‹©å’Œ O-CU ç‰©ç†ä½ç½®ï¼Œæ—¨åœ¨æœ€å°åŒ–æ•´ä½“ç½‘ç»œèƒ½è€—ã€‚åœ¨è’™ç‰¹åˆ©å°” 24 å°æ—¶çœŸå®äº¤é€šè½¨è¿¹æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œç›¸æ¯”äºé™æ€æ˜ å°„åŸºå‡†ï¼ŒåŠ¨æ€ O-CU é€‰æ‹©åœ¨ä¿è¯ QoS çš„åŒæ—¶æ˜¾è‘—é™ä½äº†èƒ½è€—ã€‚è¯¥ç ”ç©¶è¯æ˜äº†åŸºäº DRL çš„ SFC é…ç½®æ˜¯å®ç°èŠ‚èƒ½ä¸”èµ„æºè‡ªé€‚åº” O-RAN éƒ¨ç½²çš„ä¸€ç§é«˜æ•ˆä¸”å®ç”¨çš„æ§åˆ¶åŸè¯­ã€‚",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19696v1",
      "published_date": "2025-11-21 16:10:28 UTC",
      "updated_date": "2025-11-21 16:10:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:59:16.184207+00:00"
    },
    {
      "arxiv_id": "2511.17332v1",
      "title": "Agentifying Agentic AI",
      "title_zh": "æ™ºèƒ½ä½“åŒ–ï¼šèµ‹èƒ½æ™ºèƒ½ä½“åŒ–äººå·¥æ™ºèƒ½",
      "authors": [
        "Virginia Dignum",
        "Frank Dignum"
      ],
      "abstract": "Agentic AI seeks to endow systems with sustained autonomy, reasoning, and interaction capabilities. To realize this vision, its assumptions about agency must be complemented by explicit models of cognition, cooperation, and governance. This paper argues that the conceptual tools developed within the Autonomous Agents and Multi-Agent Systems (AAMAS) community, such as BDI architectures, communication protocols, mechanism design, and institutional modelling, provide precisely such a foundation. By aligning adaptive, data-driven approaches with structured models of reasoning and coordination, we outline a path toward agentic systems that are not only capable and flexible, but also transparent, cooperative, and accountable. The result is a perspective on agency that bridges formal theory and practical autonomy.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•å®ç°å…·æœ‰æŒç»­è‡ªä¸»æ€§ã€æ¨ç†å’Œäº¤äº’èƒ½åŠ›çš„ Agentic AIï¼Œå¹¶æå‡ºå¿…é¡»é€šè¿‡æ˜ç¡®çš„è®¤çŸ¥ã€åˆä½œä¸æ²»ç†æ¨¡å‹æ¥å®Œå–„å½“å‰çš„ä»£ç†å‡è®¾ã€‚ä½œè€…æŒ‡å‡ºï¼Œè‡ªä¸»ä»£ç†ä¸å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ (AAMAS) ç¤¾åŒºæ‰€å¼€å‘çš„ç†è®ºå·¥å…·ï¼ŒåŒ…æ‹¬ BDI æ¶æ„ã€é€šä¿¡åè®® (communication protocols)ã€æœºåˆ¶è®¾è®¡ (mechanism design) å’Œåˆ¶åº¦å»ºæ¨¡ (institutional modelling)ï¼Œæ­£æ˜¯æ„å»ºæ­¤ç±»ç³»ç»Ÿçš„æ ¸å¿ƒåŸºç¡€ã€‚é€šè¿‡å°†è‡ªé€‚åº”çš„æ•°æ®é©±åŠ¨æ–¹æ³•ä¸ç»“æ„åŒ–çš„æ¨ç†åŠåè°ƒæ¨¡å‹ç›¸èåˆï¼Œè¯¥è®ºæ–‡å‹¾å‹’å‡ºä¸€ç§æ—¢çµæ´»é«˜æ•ˆåˆå…·å¤‡é€æ˜åº¦ã€åˆä½œæ€§ä¸é—®è´£åˆ¶çš„ç³»ç»Ÿæ¼”è¿›è·¯å¾„ã€‚è¿™é¡¹å·¥ä½œæ—¨åœ¨è§£å†³å½“å‰ä»£ç†ç³»ç»Ÿåœ¨å¤æ‚äº¤äº’ç¯å¢ƒä¸‹çš„å±€é™æ€§ï¼Œç¡®ä¿ç³»ç»Ÿçš„è¡Œä¸ºå¯é¢„æµ‹ä¸”ç¬¦åˆä¼¦ç†è§„èŒƒã€‚ç ”ç©¶æœ€ç»ˆæå‡ºäº†ä¸€ç§æ¡¥æ¥å½¢å¼åŒ–ç†è®ºä¸å®é™…è‡ªä¸»æ€§çš„å…¨æ–°è§†è§’ï¼Œä¸ºå¼€å‘å¯ä¿¡ä¸”å¯è§£é‡Šçš„ Agentic AI ç³»ç»Ÿæä¾›äº†é‡è¦çš„ç†è®ºæ”¯æ’‘ä¸å®è·µæŒ‡å—ã€‚",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages; 1 figure",
      "pdf_url": "https://arxiv.org/pdf/2511.17332v1",
      "published_date": "2025-11-21 15:54:44 UTC",
      "updated_date": "2025-11-21 15:54:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:58:39.282698+00:00"
    },
    {
      "arxiv_id": "2511.17331v1",
      "title": "AI Workers, Geopolitics, and Algorithmic Collective Action",
      "title_zh": "AI ä»ä¸šè€…ã€åœ°ç¼˜æ”¿æ²»ä¸ç®—æ³•é›†ä½“è¡ŒåŠ¨",
      "authors": [
        "Sydney Reis"
      ],
      "abstract": "According to the theory of International Political Economy (IPE), states are often incentivized to rely on rather than constrain powerful corporations. For this reason, IPE provides a useful lens to explain why efforts to govern Artificial Intelligence (AI) at the international and national levels have thus far been developed, applied, and enforced unevenly. Building on recent work that explores how AI companies engage in geopolitics, this position paper argues that some AI workers can be considered actors of geopolitics. It makes the timely case that governance alone cannot ensure responsible, ethical, or robust AI development and use, and greater attention should be paid to bottom-up interventions at the site of AI development. AI workers themselves should be situated as individual agents of change, especially when considering their potential to foster Algorithmic Collective Action (ACA). Drawing on methods of Participatory Design (PD), this paper proposes engaging AI workers as sources of knowledge, relative power, and intentionality to encourage more responsible and just AI development and create the conditions that can facilitate ACA.",
      "tldr_zh": "è¯¥ç«‹åœºè®ºæ–‡åŸºäºå›½é™…æ”¿æ²»ç»æµå­¦(IPE)ç†è®ºï¼Œåˆ†æäº†å›½å®¶å› å€¾å‘äºä¾èµ–è€Œéçº¦æŸå¤§å‹ä¼ä¸šï¼Œå¯¼è‡´å…¨çƒArtificial Intelligence (AI)æ²»ç†åœ¨å›½é™…å’Œå›½å®¶å±‚é¢å‘ˆç°å‡ºä¸å‡è¡¡å‘å±•çš„ç°çŠ¶ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œé™¤äº†å…³æ³¨AIå…¬å¸çš„åœ°ç¼˜æ”¿æ²»è¡Œä¸ºå¤–ï¼ŒAIä»ä¸šè€…æœ¬èº«ä¹Ÿåº”è¢«è§†ä¸ºåœ°ç¼˜æ”¿æ²»çš„é‡è¦è¡ŒåŠ¨ä¸»ä½“ã€‚è®ºæ–‡å¼ºè°ƒä»…é å¤–éƒ¨æ²»ç†æ— æ³•å®Œå…¨ç¡®ä¿AIçš„è´Ÿè´£ä»»ä¸ä¼¦ç†å‘å±•ï¼Œå¿…é¡»é‡è§†å¼€å‘ç«¯çš„è‡ªä¸‹è€Œä¸Šå¹²é¢„ï¼Œå°†AIå·¥äººå®šä½ä¸ºæ¨åŠ¨å˜é©çš„ä¸ªä½“ä»£ç†ã€‚é€šè¿‡å€Ÿé‰´å‚ä¸å¼è®¾è®¡(Participatory Design, PD)çš„æ–¹æ³•ï¼Œä½œè€…æå€¡å°†AIå·¥äººä½œä¸ºçŸ¥è¯†ã€æƒåŠ›å’Œæ„å›¾çš„æ¥æºï¼Œä»¥åˆ›é€ å¹¶ä¿ƒè¿›ç®—æ³•é›†ä½“è¡ŒåŠ¨(Algorithmic Collective Action, ACA)çš„å¿…è¦æ¡ä»¶ã€‚è¿™ç§è§†è§’æ—¨åœ¨é€šè¿‡èµ‹èƒ½å¼€å‘è€…ï¼Œæ„å»ºä¸€ä¸ªæ›´åŠ å…¬æ­£ä¸”è´Ÿè´£ä»»çš„AIå¼€å‘ç”Ÿæ€ç³»ç»Ÿã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17331v1",
      "published_date": "2025-11-21 15:52:44 UTC",
      "updated_date": "2025-11-21 15:52:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:58:39.493585+00:00"
    },
    {
      "arxiv_id": "2511.17323v1",
      "title": "MusicAIR: A Multimodal AI Music Generation Framework Powered by an Algorithm-Driven Core",
      "title_zh": "MusicAIRï¼šåŸºäºç®—æ³•é©±åŠ¨æ ¸å¿ƒçš„å¤šæ¨¡æ€ AI éŸ³ä¹ç”Ÿæˆæ¡†æ¶",
      "authors": [
        "Callie C. Liao",
        "Duoduo Liao",
        "Ellie L. Zhang"
      ],
      "abstract": "Recent advances in generative AI have made music generation a prominent research focus. However, many neural-based models rely on large datasets, raising concerns about copyright infringement and high-performance costs. In contrast, we propose MusicAIR, an innovative multimodal AI music generation framework powered by a novel algorithm-driven symbolic music core, effectively mitigating copyright infringement risks. The music core algorithms connect critical lyrical and rhythmic information to automatically derive musical features, creating a complete, coherent melodic score solely from the lyrics. The MusicAIR framework facilitates music generation from lyrics, text, and images. The generated score adheres to established principles of music theory, lyrical structure, and rhythmic conventions. We developed Generate AI Music (GenAIM), a web tool using MusicAIR for lyric-to-song, text-to-music, and image-to-music generation. In our experiments, we evaluated AI-generated music scores produced by the system using both standard music metrics and innovative analysis that compares these compositions with original works. The system achieves an average key confidence of 85%, outperforming human composers at 79%, and aligns closely with established music theory standards, demonstrating its ability to generate diverse, human-like compositions. As a co-pilot tool, GenAIM can serve as a reliable music composition assistant and a possible educational composition tutor while simultaneously lowering the entry barrier for all aspiring musicians, which is innovative and significantly contributes to AI for music generation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MusicAIRï¼Œè¿™æ˜¯ä¸€ç§åŸºäºåˆ›æ–°ç®—æ³•é©±åŠ¨çš„ç¬¦å·éŸ³ä¹æ ¸å¿ƒ (symbolic music core) çš„å¤šæ¨¡æ€äººå·¥æ™ºèƒ½éŸ³ä¹ç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç¥ç»ç½‘ç»œæ¨¡å‹å¯¹å¤§è§„æ¨¡æ•°æ®é›†çš„ä¾èµ–ä»¥åŠéšä¹‹è€Œæ¥çš„ç‰ˆæƒä¾µæƒå’Œé«˜æ˜‚è®¡ç®—æˆæœ¬é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ç®—æ³•å°†å…³é”®çš„æ­Œè¯ä¸èŠ‚å¥ä¿¡æ¯ç›¸è¿æ¥ä»¥è‡ªåŠ¨æ¨å¯¼éŸ³ä¹ç‰¹å¾ï¼Œèƒ½å¤Ÿä»…å‡­æ­Œè¯ç”Ÿæˆå®Œæ•´ä¸”è¿è´¯çš„æ—‹å¾‹ä¹è°±ï¼Œå¹¶æ”¯æŒä»æ–‡æœ¬å’Œå›¾åƒè¿›è¡Œå¤šæ¨¡æ€ç”Ÿæˆã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿç”Ÿæˆçš„ä¹è°±ä¸¥æ ¼éµå¾ªéŸ³ä¹ç†è®ºå’ŒèŠ‚å¥è§„èŒƒï¼Œå…¶å¹³å‡è°ƒæ€§ç½®ä¿¡åº¦ (key confidence) è¾¾åˆ° 85%ï¼Œè¶…è¿‡äº†äººç±»ä½œæ›²å®¶çš„ 79%ï¼Œå±•ç°äº†ç”Ÿæˆé«˜è´¨é‡ç±»äººä½œå“çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜å¼€å‘äº†åä¸º GenAIM çš„ç½‘é¡µå·¥å…·ï¼Œä½¿ MusicAIR èƒ½å¤Ÿä½œä¸ºå¯é çš„ä½œæ›²åŠ©æ‰‹å’Œæ•™å­¦å·¥å…·ï¼Œæ˜¾è‘—é™ä½äº†éŸ³ä¹åˆ›ä½œçš„å‡†å…¥é—¨æ§›ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.MM"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted by IEEE Big Data 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.17323v1",
      "published_date": "2025-11-21 15:43:27 UTC",
      "updated_date": "2025-11-21 15:43:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:59:38.677044+00:00"
    },
    {
      "arxiv_id": "2511.17318v2",
      "title": "FORWARD: Dataset of a forwarder operating in rough terrain",
      "title_zh": "FORWARDï¼šå¤æ‚åœ°å½¢ä¸‹çš„é›†è¿æœºä½œä¸šæ•°æ®é›†",
      "authors": [
        "Mikael LundbÃ¤ck",
        "Erik Wallin",
        "Carola HÃ¤ggstrÃ¶m",
        "Mattias NystrÃ¶m",
        "Andreas GrÃ¶nlund",
        "Mats Richardson",
        "Petrus JÃ¶nsson",
        "William Arnvik",
        "Lucas HedstrÃ¶m",
        "Arvid FÃ¤lldin",
        "Martin Servin"
      ],
      "abstract": "We present FORWARD, a high-resolution multimodal dataset of a cut-to-length forwarder operating in rough terrain on two harvest sites in the middle part of Sweden. The forwarder is a large Komatsu model equipped with vehicle telematics sensors, including global positioning via satellite navigation, movement sensors, accelerometers, and engine sensors. The vehicle was additionally equipped with cameras, operator vibration sensors, and multiple IMUs. The data includes event time logs recorded at 5 Hz of driving speed, fuel consumption, vehicle position with centimeter accuracy, and crane use while the vehicle operates in forest areas, aerially laser-scanned with a resolution of around 1500 points per square meter. Production log files (StanForD standard) with time-stamped machine events, extensive video material, and terrain data in various formats are included as well. About 18 hours of regular wood extraction work during three days is annotated from 360-video material into individual work elements and included in the dataset. We also include scenario specifications of conducted experiments on forest roads and in terrain. Scenarios include repeatedly driving the same routes with and without steel tracks, different load weights, and different target driving speeds. The dataset is intended for developing models and algorithms for trafficability, perception, and autonomous control of forest machines using artificial intelligence, simulation, and experiments on physical testbeds. In part, we focus on forwarders traversing terrain, avoiding or handling obstacles, and loading or unloading logs, with consideration for efficiency, fuel consumption, safety, and environmental impact. Other benefits of the open dataset include the ability to explore auto-generation and calibration of forestry machine simulators and automation scenario descriptions using the data recorded in the field.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†åä¸ºFORWARDçš„é«˜åˆ†è¾¨ç‡å¤šæ¨¡æ€æ•°æ®é›†ï¼Œè®°å½•äº†åœ¨ç‘å…¸å¤æ‚åœ°å½¢ä¸­è¿è¡Œçš„çŸ­æœ¨æé›†è¿æœº(forwarder)çš„ä½œä¸šæ•°æ®ã€‚è¯¥æ•°æ®é›†æ•´åˆäº†æ¥è‡ªKomatsuè½¦è¾†çš„é¥æµ‹ä¼ æ„Ÿå™¨ã€é«˜ç²¾åº¦GPSã€åŠ é€Ÿåº¦è®¡ã€æ‘„åƒå¤´åŠå¤šä¸ªæƒ¯æ€§æµ‹é‡å•å…ƒ(IMU)çš„æ•°æ®ï¼Œå¹¶é…åˆäº†åˆ†è¾¨ç‡è¾¾æ¯å¹³æ–¹ç±³1500ç‚¹çš„æœºè½½æ¿€å…‰æ‰«æåœ°å½¢æ•°æ®ã€‚æ•°æ®æ¶µç›–äº†çº¦18å°æ—¶çš„æœ¨ææå–ä½œä¸šï¼ŒåŒ…å«StanForDæ ‡å‡†çš„ç”Ÿäº§æ—¥å¿—å’Œè¯¦ç»†æ ‡æ³¨çš„360åº¦è§†é¢‘ç‰©æ–™ï¼Œå¹¶è®°å½•äº†åœ¨ä¸åŒè½½è·ã€è¡Œé©¶é€Ÿåº¦åŠå±¥å¸¦é…ç½®ä¸‹çš„å—æ§å®éªŒã€‚FORWARDæ—¨åœ¨æ”¯æŒæ£®æ—æœºæ¢°çš„å¯é€šè¿‡æ€§(trafficability)ã€æ„ŸçŸ¥(perception)å’Œè‡ªä¸»æ§åˆ¶(autonomous control)ç­‰AIæ¨¡å‹çš„å¼€å‘ï¼ŒåŒæ—¶ä¹Ÿä¸ºæ—ä¸šæ¨¡æ‹Ÿå™¨çš„è‡ªåŠ¨ç”Ÿæˆä¸æ ‡å®šæä¾›äº†å®åœ°æ•°æ®æ”¯æŒã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CE",
        "cs.LG",
        "physics.app-ph"
      ],
      "primary_category": "cs.RO",
      "comment": "28 pages, 22 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.17318v2",
      "published_date": "2025-11-21 15:36:41 UTC",
      "updated_date": "2025-12-22 20:46:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:58:52.576043+00:00"
    },
    {
      "arxiv_id": "2511.17309v1",
      "title": "MuM: Multi-View Masked Image Modeling for 3D Vision",
      "title_zh": "MuMï¼šé¢å‘ 3D è§†è§‰çš„å¤šè§†è§’æ©ç å›¾åƒå»ºæ¨¡",
      "authors": [
        "David NordstrÃ¶m",
        "Johan Edstedt",
        "Fredrik Kahl",
        "Georg BÃ¶kman"
      ],
      "abstract": "Self-supervised learning on images seeks to extract meaningful visual representations from unlabeled data. When scaled to large datasets, this paradigm has achieved state-of-the-art performance and the resulting trained models such as DINOv3 have seen widespread adoption. However, most prior efforts are optimized for semantic understanding rather than geometric reasoning. One important exception is Cross-View Completion, CroCo, which is a form of masked autoencoding (MAE) tailored for 3D understanding. In this work, we continue on the path proposed by CroCo and focus on learning features tailored for 3D vision. In a nutshell, we extend MAE to arbitrarily many views of the same scene. By uniformly masking all views and employing a lightweight decoder with inter-frame attention, our approach is inherently simpler and more scalable than CroCo. We evaluate the resulting model, MuM, extensively on downstream tasks including feedforward reconstruction, dense image matching and relative pose estimation, finding that it outperforms the state-of-the-art visual encoders DINOv3 and CroCo v2.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MuMï¼Œä¸€ç§é’ˆå¯¹3Dè§†è§‰è®¾è®¡çš„å¤šè§†è§’æ©ç å›¾åƒå»ºæ¨¡(Multi-View Masked Image Modeling)æ¡†æ¶ã€‚MuMæ—¨åœ¨æ”¹è¿›è‡ªç›‘ç£å­¦ä¹ (Self-supervised learning)ä¸­ä¼ ç»Ÿæ¨¡å‹åé‡è¯­ä¹‰ç†è§£è€Œç¼ºä¹å‡ ä½•æ¨ç†(geometric reasoning)èƒ½åŠ›çš„ç°çŠ¶ã€‚è¯¥æ–¹æ³•å°†æ©ç è‡ªåŠ¨ç¼–ç (Masked Autoencoding, MAE)æ‰©å±•åˆ°åŒä¸€åœºæ™¯çš„ä»»æ„å¤šè§†è§’ï¼Œå¹¶é€šè¿‡å¯¹æ‰€æœ‰è§†è§’è¿›è¡Œç»Ÿä¸€æ©ç ä»¥åŠé‡‡ç”¨å¸¦æœ‰å¸§é—´æ³¨æ„åŠ›(inter-frame attention)çš„è½»é‡çº§è§£ç å™¨ï¼Œå®ç°äº†æ¯”CroCoæ›´ç®€å•ä¸”æ›´å…·æ‰©å±•æ€§çš„æ¶æ„ã€‚ç ”ç©¶åœ¨ä»¥å‰é¦ˆé‡å»º(feedforward reconstruction)ã€ç¨ å¯†å›¾åƒåŒ¹é…(dense image matching)å’Œç›¸å¯¹å§¿æ€ä¼°è®¡(relative pose estimation)ä¸ºä»£è¡¨çš„ä¸‹æ¸¸ä»»åŠ¡ä¸­è¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒMuMåœ¨3Dè§†è§‰ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºDINOv3å’ŒCroCo v2ç­‰æœ€å…ˆè¿›çš„è§†è§‰ç¼–ç å™¨ï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„3Dç‰¹å¾å­¦ä¹ èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17309v1",
      "published_date": "2025-11-21 15:25:47 UTC",
      "updated_date": "2025-11-21 15:25:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:58:51.485622+00:00"
    },
    {
      "arxiv_id": "2511.17301v1",
      "title": "Large Language Models for Sentiment Analysis to Detect Social Challenges: A Use Case with South African Languages",
      "title_zh": "åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæƒ…æ„Ÿåˆ†æä»¥è¯†åˆ«ç¤¾ä¼šæŒ‘æˆ˜ï¼šä»¥å—éè¯­è¨€ä¸ºä¾‹",
      "authors": [
        "Koena Ronny Mabokela",
        "Tim Schlippe",
        "Matthias WÃ¶lfel"
      ],
      "abstract": "Sentiment analysis can aid in understanding people's opinions and emotions on social issues. In multilingual communities sentiment analysis systems can be used to quickly identify social challenges in social media posts, enabling government departments to detect and address these issues more precisely and effectively. Recently, large-language models (LLMs) have become available to the wide public and initial analyses have shown that they exhibit magnificent zero-shot sentiment analysis abilities in English. However, there is no work that has investigated to leverage LLMs for sentiment analysis on social media posts in South African languages and detect social challenges. Consequently, in this work, we analyse the zero-shot performance of the state-of-the-art LLMs GPT-3.5, GPT-4, LlaMa 2, PaLM 2, and Dolly 2 to investigate the sentiment polarities of the 10 most emerging topics in English, Sepedi and Setswana social media posts that fall within the jurisdictional areas of 10 South African government departments. Our results demonstrate that there are big differences between the various LLMs, topics, and languages. In addition, we show that a fusion of the outcomes of different LLMs provides large gains in sentiment classification performance with sentiment classification errors below 1%. Consequently, it is now feasible to provide systems that generate reliable information about sentiment analysis to detect social challenges and draw conclusions about possible needs for actions on specific topics and within different language groups.",
      "tldr_zh": "è¯¥é¡¹ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)è¿›è¡Œæƒ…æ„Ÿåˆ†æ(Sentiment Analysis)ï¼Œä»¥è¯†åˆ«å—éå¤šè¯­ç§ç¤¾åŒºç¤¾äº¤åª’ä½“ä¸­çš„ç¤¾ä¼šæŒ‘æˆ˜ã€‚ç ”ç©¶è¯„ä¼°äº†GPT-3.5ã€GPT-4ã€LlaMa 2ã€PaLM 2å’ŒDolly 2åœ¨è‹±è¯­ã€åŒ—ç´¢æ‰˜è¯­(Sepedi)å’ŒèŒ¨ç“¦çº³è¯­(Setswana)ä¸‰ç§è¯­è¨€ä¸‹çš„é›¶æ ·æœ¬(Zero-shot)æ€§èƒ½ï¼Œæ¶µç›–äº†10ä¸ªæ”¿åºœéƒ¨é—¨ç®¡è¾–çš„çƒ­é—¨è¯é¢˜ã€‚å®éªŒå‘ç°ä¸åŒæ¨¡å‹ã€ä¸»é¢˜å’Œè¯­è¨€ä¹‹é—´çš„è¡¨ç°å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚ç ”ç©¶è¿›ä¸€æ­¥æå‡ºï¼Œé€šè¿‡èåˆä¸åŒLLMsçš„è¾“å‡ºç»“æœï¼Œå¯ä»¥å°†æƒ…æ„Ÿåˆ†ç±»é”™è¯¯ç‡é™ä½è‡³1%ä»¥ä¸‹ã€‚è¿™ä¸€æˆæœè¯æ˜äº†æ„å»ºå¯é çš„æƒ…æ„Ÿåˆ†æç³»ç»Ÿä»¥è¾…åŠ©æ”¿åºœæ£€æµ‹ç¤¾ä¼šæŒ‘æˆ˜å¹¶é’ˆå¯¹ä¸åŒè¯­è¨€ç¾¤ä½“é‡‡å–ç²¾å‡†è¡ŒåŠ¨çš„å¯è¡Œæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Published in the Proceedings of The Southern African Conference on AI Research (SACAIR 2024), Bloemfontein, South Africa, 2-6 December 2024. ISBN: 978-0-7961-6069-0",
      "pdf_url": "https://arxiv.org/pdf/2511.17301v1",
      "published_date": "2025-11-21 15:14:32 UTC",
      "updated_date": "2025-11-21 15:14:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:00:02.163042+00:00"
    },
    {
      "arxiv_id": "2511.21737v1",
      "title": "Polarity-Aware Probing for Quantifying Latent Alignment in Language Models",
      "title_zh": "ææ€§æ„ŸçŸ¥æ¢æµ‹ï¼šé‡åŒ–è¯­è¨€æ¨¡å‹ä¸­çš„æ½œåœ¨å¯¹é½",
      "authors": [
        "Sabrina Sadiekh",
        "Elena Ericheva",
        "Chirag Agarwal"
      ],
      "abstract": "Advances in unsupervised probes such as Contrast-Consistent Search (CCS), which reveal latent beliefs without relying on token outputs, raise the question of whether these methods can reliably assess model alignment. We investigate this by examining the sensitivity of CCS to harmful vs. safe statements and by introducing Polarity-Aware CCS (PA-CCS), a method for evaluating whether a model's internal representations remain consistent under polarity inversion. We propose two alignment-oriented metrics, Polar-Consistency and the Contradiction Index, to quantify the semantic robustness of a model's latent knowledge. To validate PA-CCS, we curate two main datasets and one control dataset containing matched harmful-safe sentence pairs constructed using different methodologies (concurrent and antagonistic statements). We apply PA-CCS to 16 language models. Our results show that PA-CCS identifies both architectural and layer-specific differences in the encoding of latent harmful knowledge. Notably, replacing the negation token with a meaningless marker degrades PA-CCS scores for models with well-aligned internal representations, while models lacking robust internal calibration do not exhibit this degradation. Our findings highlight the potential of unsupervised probing for alignment evaluation and emphasize the need to incorporate structural robustness checks into interpretability benchmarks. Code and datasets are available at: https://github.com/SadSabrina/polarity-probing. WARNING: This paper contains potentially sensitive, harmful, and offensive content.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ— ç›‘ç£æ¢æµ‹æŠ€æœ¯ï¼ˆå¦‚Contrast-Consistent Search, CCSï¼‰åœ¨è¯„ä¼°è¯­è¨€æ¨¡å‹å¯¹é½ç¨‹åº¦æ–¹é¢çš„å¯é æ€§ï¼Œå¹¶ç”±æ­¤æå‡ºäº†Polarity-Aware CCS (PA-CCS) æ–¹æ³•ã€‚è¯¥æ–¹æ³•æ—¨åœ¨é€šè¿‡è§‚å¯Ÿæ¨¡å‹å†…éƒ¨è¡¨ç¤ºåœ¨ææ€§è½¬æ¢ï¼ˆpolarity inversionï¼‰ä¸‹çš„è¡¨ç°ï¼Œæ¥è¯„ä¼°å…¶æ½œåœ¨å¯¹é½çš„ç¨³å®šæ€§ã€‚ç ”ç©¶å¼•å…¥äº†Polar-Consistencyå’ŒContradiction Indexä¸¤ä¸ªæŒ‡æ ‡ï¼Œç”¨äºé‡åŒ–æ¨¡å‹æ½œåœ¨çŸ¥è¯†çš„è¯­ä¹‰é²æ£’æ€§ã€‚é€šè¿‡å¯¹16ä¸ªè¯­è¨€æ¨¡å‹çš„å®éªŒåˆ†æï¼Œç ”ç©¶å‘ç°PA-CCSèƒ½å¤Ÿè¯†åˆ«å‡ºä¸åŒæ¶æ„å’Œå±‚çº§åœ¨ç¼–ç æ½œåœ¨æœ‰å®³çŸ¥è¯†æ–¹é¢çš„ç»†å¾®å·®å¼‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¯¹é½è‰¯å¥½çš„æ¨¡å‹åœ¨å¦å®šæ ‡è®°è¢«æ›¿æ¢æ—¶ä¼šè¡¨ç°å‡ºè¯„åˆ†é™çº§ï¼Œè€Œæ ¡å‡†ä¸è¶³çš„æ¨¡å‹åˆ™ç¼ºä¹è¿™ç§æ•æ„Ÿæ€§ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†åœ¨è§£é‡Šæ€§åŸºå‡†ä¸­æ•´åˆç»“æ„é²æ£’æ€§æ£€æŸ¥çš„å¿…è¦æ€§ï¼Œä¸ºåˆ©ç”¨æ— ç›‘ç£æ¢æµ‹è¿›è¡Œæ¨¡å‹å¯¹é½è¯„ä¼°æä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "7 pages",
      "pdf_url": "https://arxiv.org/pdf/2511.21737v1",
      "published_date": "2025-11-21 14:58:45 UTC",
      "updated_date": "2025-11-21 14:58:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:59:58.206015+00:00"
    },
    {
      "arxiv_id": "2511.19460v1",
      "title": "Systemic approach for modeling a generic smart grid",
      "title_zh": "é€šç”¨æ™ºèƒ½ç”µç½‘å»ºæ¨¡çš„ç³»ç»Ÿæ€§æ–¹æ³•",
      "authors": [
        "Sofiane Ben Amor",
        "Guillaume Guerard",
        "Loup-NoÃ© Levy"
      ],
      "abstract": "Smart grid technological advances present a recent class of complex interdisciplinary modeling and increasingly difficult simulation problems to solve using traditional computational methods. To simulate a smart grid requires a systemic approach to integrated modeling of power systems, energy markets, demand-side management, and much other resources and assets that are becoming part of the current paradigm of the power grid. This paper presents a backbone model of a smart grid to test alternative scenarios for the grid. This tool simulates disparate systems to validate assumptions before the human scale model. Thanks to a distributed optimization of subsystems, the production and consumption scheduling is achieved while maintaining flexibility and scalability.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é’ˆå¯¹é€šç”¨æ™ºèƒ½ç”µç½‘(Smart grid)å»ºæ¨¡çš„ç³»ç»ŸåŒ–æ–¹æ³•(Systemic approach)ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿè®¡ç®—æ–¹æ³•åœ¨å¤„ç†å¤æ‚è·¨å­¦ç§‘ä»¿çœŸæ—¶çš„å±€é™æ€§ã€‚è¯¥æ–¹æ³•æ„å»ºäº†ä¸€ä¸ªé›†æˆç”µåŠ›ç³»ç»Ÿã€èƒ½æºå¸‚åœºå’Œéœ€æ±‚ä¾§ç®¡ç†ç­‰èµ„æºçš„éª¨å¹²æ¨¡å‹(Backbone model)ï¼Œç”¨äºæµ‹è¯•ç”µç½‘çš„å„ç§æ›¿ä»£åœºæ™¯ã€‚è¯¥å·¥å…·é€šè¿‡æ¨¡æ‹Ÿå¤šä¸ªå¼‚æ„ç³»ç»Ÿï¼Œèƒ½å¤Ÿåœ¨æ„å»ºå¤§è§„æ¨¡æ¨¡å‹ä¹‹å‰æœ‰æ•ˆéªŒè¯ç›¸å…³å‡è®¾ã€‚é€šè¿‡å¯¹å­ç³»ç»Ÿçš„åˆ†å¸ƒå¼ä¼˜åŒ–(Distributed optimization)ï¼Œè¯¥æ¨¡å‹æˆåŠŸå®ç°äº†ç”Ÿäº§ä¸æ¶ˆè´¹è°ƒåº¦çš„å¹³è¡¡ï¼ŒåŒæ—¶ç¡®ä¿äº†ç³»ç»Ÿå…·å¤‡è‰¯å¥½çš„çµæ´»æ€§(Flexibility)å’Œå¯æ‰©å±•æ€§(Scalability)ã€‚è¿™ç§ç³»ç»ŸåŒ–å»ºæ¨¡æ–¹æ¡ˆä¸ºå¤æ‚èƒ½æºç³»ç»Ÿçš„é›†æˆä»¿çœŸä¸éªŒè¯æä¾›äº†é‡è¦çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19460v1",
      "published_date": "2025-11-21 14:51:23 UTC",
      "updated_date": "2025-11-21 14:51:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T08:59:59.273398+00:00"
    },
    {
      "arxiv_id": "2511.17282v1",
      "title": "Where Culture Fades: Revealing the Cultural Gap in Text-to-Image Generation",
      "title_zh": "æ–‡åŒ–ç¼ºä½ï¼šæ­ç¤ºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„æ–‡åŒ–é¸¿æ²Ÿ",
      "authors": [
        "Chuancheng Shi",
        "Shangze Li",
        "Shiming Guo",
        "Simiao Xie",
        "Wenhua Wu",
        "Jingtong Dou",
        "Chao Wu",
        "Canran Xiao",
        "Cong Wang",
        "Zifeng Cheng",
        "Fei Shen",
        "Tat-Seng Chua"
      ],
      "abstract": "Multilingual text-to-image (T2I) models have advanced rapidly in terms of visual realism and semantic alignment, and are now widely utilized. Yet outputs vary across cultural contexts: because language carries cultural connotations, images synthesized from multilingual prompts should preserve cross-lingual cultural consistency. We conduct a comprehensive analysis showing that current T2I models often produce culturally neutral or English-biased results under multilingual prompts. Analyses of two representative models indicate that the issue stems not from missing cultural knowledge but from insufficient activation of culture-related representations. We propose a probing method that localizes culture-sensitive signals to a small set of neurons in a few fixed layers. Guided by this finding, we introduce two complementary alignment strategies: (1) inference-time cultural activation that amplifies the identified neurons without backbone fine-tuned; and (2) layer-targeted cultural enhancement that updates only culturally relevant layers. Experiments on our CultureBench demonstrate consistent improvements over strong baselines in cultural consistency while preserving fidelity and diversity.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤šè¯­è¨€æ–‡æœ¬ç”Ÿæˆå›¾åƒ(Text-to-Image)æ¨¡å‹åœ¨è·¨æ–‡åŒ–èƒŒæ™¯ä¸‹çš„è¡¨ç°ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨å¤„ç†å¤šè¯­è¨€æç¤ºè¯æ—¶å¸¸å‡ºç°æ–‡åŒ–ä¸­ç«‹æˆ–åå‘è‹±è¯­æ–‡åŒ–çš„â€œæ–‡åŒ–é¸¿æ²Ÿâ€é—®é¢˜ã€‚é€šè¿‡å¯¹ä»£è¡¨æ€§æ¨¡å‹çš„æ·±å…¥åˆ†æï¼Œç ”ç©¶æŒ‡å‡ºè¿™ä¸€é—®é¢˜çš„æ ¹æºå¹¶éæ¨¡å‹ç¼ºå¤±æ–‡åŒ–çŸ¥è¯†ï¼Œè€Œæ˜¯ç”±äºæ¨¡å‹å†…éƒ¨ä¸æ–‡åŒ–ç›¸å…³çš„è¡¨ç¤ºæœªèƒ½å¾—åˆ°å……åˆ†æ¿€æ´»ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ¢æµ‹æ–¹æ³•(probing method)ï¼Œèƒ½å¤Ÿå°†æ–‡åŒ–æ•æ„Ÿä¿¡å·ç²¾å‡†å®šä½åˆ°å°‘æ•°å›ºå®šå±‚ä¸­çš„ç‰¹å®šç¥ç»å…ƒé›†åˆã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œç ”ç©¶å¼•å…¥äº†æ¨ç†æ—¶æ–‡åŒ–æ¿€æ´»(inference-time cultural activation)å’Œå±‚é’ˆå¯¹æ€§æ–‡åŒ–å¢å¼º(layer-targeted cultural enhancement)ä¸¤ç§äº’è¡¥ç­–ç•¥ï¼Œæ—¨åœ¨ä¸æ”¹å˜æ¨¡å‹ä¸»å¹²æˆ–ä»…å¾®è°ƒç›¸å…³å±‚çš„æƒ…å†µä¸‹å¢å¼ºæ–‡åŒ–è¡¨ç°ã€‚åœ¨CultureBenchä¸Šçš„å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒå›¾åƒä¿çœŸåº¦å’Œå¤šæ ·æ€§çš„åŒæ—¶ï¼Œæ˜¾è‘—æå‡äº†è·¨è¯­è¨€çš„æ–‡åŒ–ä¸€è‡´æ€§(cultural consistency)ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17282v1",
      "published_date": "2025-11-21 14:40:50 UTC",
      "updated_date": "2025-11-21 14:40:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:00:06.680212+00:00"
    },
    {
      "arxiv_id": "2511.17276v1",
      "title": "Leveraging CVAE for Joint Configuration Estimation of Multifingered Grippers from Point Cloud Data",
      "title_zh": "åŸºäº CVAE çš„ç‚¹äº‘æ•°æ®å¤šæŒ‡å¤¹æŒå™¨å…³èŠ‚æ„å‹ä¼°è®¡",
      "authors": [
        "Julien Merand",
        "Boris Meden",
        "Mathieu Grossard"
      ],
      "abstract": "This paper presents an efficient approach for determining the joint configuration of a multifingered gripper solely from the point cloud data of its poly-articulated chain, as generated by visual sensors, simulations or even generative neural networks. Well-known inverse kinematics (IK) techniques can provide mathematically exact solutions (when they exist) for joint configuration determination based solely on the fingertip pose, but often require post-hoc decision-making by considering the positions of all intermediate phalanges in the gripper's fingers, or rely on algorithms to numerically approximate solutions for more complex kinematics. In contrast, our method leverages machine learning to implicitly overcome these challenges. This is achieved through a Conditional Variational Auto-Encoder (CVAE), which takes point cloud data of key structural elements as input and reconstructs the corresponding joint configurations. We validate our approach on the MultiDex grasping dataset using the Allegro Hand, operating within 0.05 milliseconds and achieving accuracy comparable to state-of-the-art methods. This highlights the effectiveness of our pipeline for joint configuration estimation within the broader context of AI-driven techniques for grasp planning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä»ç‚¹äº‘æ•°æ®(point cloud data)ä¼°è®¡å¤šæŒ‡æŠ“å–å™¨(multifingered grippers)å…³èŠ‚é…ç½®çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæœºå™¨å­¦ä¹ çš„é«˜æ•ˆæ–¹æ³•ã€‚ä¼ ç»Ÿçš„é€†è¿åŠ¨å­¦(Inverse Kinematics, IK)æŠ€æœ¯é€šå¸¸ä¾èµ–æŒ‡å°–ä½å§¿ï¼Œåœ¨å¤„ç†å¤æ‚è¿åŠ¨å­¦æ—¶å¾€å¾€éœ€è¦ç¹ççš„åå¤„ç†æˆ–æ•°å€¼è¿‘ä¼¼ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡åˆ©ç”¨æ¡ä»¶å˜åˆ†è‡ªç¼–ç å™¨(Conditional Variational Auto-Encoder, CVAE)å°†å…³é”®ç»“æ„å…ƒç´ çš„ç‚¹äº‘æ•°æ®ä½œä¸ºè¾“å…¥ï¼Œä»è€Œéšå¼åœ°é‡å»ºå‡ºå¯¹åº”çš„å…³èŠ‚é…ç½®ã€‚ç ”ç©¶åœ¨MultiDexæŠ“å–æ•°æ®é›†ä¸Šä½¿ç”¨Allegro Handè¿›è¡Œäº†å®éªŒéªŒè¯ï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•ä»…éœ€0.05æ¯«ç§’å³å¯å®Œæˆè®¡ç®—ï¼Œä¸”å‡†ç¡®ç‡å¯ä¸å½“å‰æœ€å…ˆè¿›æ–¹æ³•ç›¸åª²ç¾ã€‚è¿™ä¸€ç ”ç©¶æˆæœçªæ˜¾äº†è¯¥æµç¨‹åœ¨äººå·¥æ™ºèƒ½é©±åŠ¨çš„æŠ“å–è§„åˆ’(grasp planning)èƒŒæ™¯ä¸‹è¿›è¡Œå…³èŠ‚é…ç½®ä¼°è®¡çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæœºå™¨äººçµå·§æ“ä½œæä¾›äº†å¿«é€Ÿä¸”ç²¾å‡†çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17276v1",
      "published_date": "2025-11-21 14:31:39 UTC",
      "updated_date": "2025-11-21 14:31:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:00:11.389717+00:00"
    },
    {
      "arxiv_id": "2511.17269v1",
      "title": "Range-Edit: Semantic Mask Guided Outdoor LiDAR Scene Editing",
      "title_zh": "Range-Editï¼šè¯­ä¹‰æ©ç å¼•å¯¼çš„å®¤å¤– LiDAR åœºæ™¯ç¼–è¾‘",
      "authors": [
        "Suchetan G. Uppur",
        "Hemant Kumar",
        "Vaibhav Kumar"
      ],
      "abstract": "Training autonomous driving and navigation systems requires large and diverse point cloud datasets that capture complex edge case scenarios from various dynamic urban settings. Acquiring such diverse scenarios from real-world point cloud data, especially for critical edge cases, is challenging, which restricts system generalization and robustness. Current methods rely on simulating point cloud data within handcrafted 3D virtual environments, which is time-consuming, computationally expensive, and often fails to fully capture the complexity of real-world scenes. To address some of these issues, this research proposes a novel approach that addresses the problem discussed by editing real-world LiDAR scans using semantic mask-based guidance to generate novel synthetic LiDAR point clouds. We incorporate range image projection and semantic mask conditioning to achieve diffusion-based generation. Point clouds are transformed to 2D range view images, which are used as an intermediate representation to enable semantic editing using convex hull-based semantic masks. These masks guide the generation process by providing information on the dimensions, orientations, and locations of objects in the real environment, ensuring geometric consistency and realism. This approach demonstrates high-quality LiDAR point cloud generation, capable of producing complex edge cases and dynamic scenes, as validated on the KITTI-360 dataset. This offers a cost-effective and scalable solution for generating diverse LiDAR data, a step toward improving the robustness of autonomous driving systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Range-Editï¼Œä¸€ç§åŸºäºè¯­ä¹‰æ©ç å¼•å¯¼çš„æˆ·å¤–LiDARåœºæ™¯ç¼–è¾‘æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡ç¼–è¾‘çœŸå®ä¸–ç•Œçš„LiDARæ‰«ææ•°æ®æ¥ç”Ÿæˆå¤šæ ·åŒ–çš„åˆæˆç‚¹äº‘ï¼Œä»¥è§£å†³è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿè®­ç»ƒä¸­å…³é”®è¾¹ç¼˜æ¡ˆä¾‹è·å–å›°éš¾çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶å°†LiDARç‚¹äº‘è½¬æ¢ä¸º2Då…¨æ™¯è§†å›¾(range view images)ä½œä¸ºä¸­é—´è¡¨ç¤ºï¼Œå¹¶ç»“åˆæ‰©æ•£ç”Ÿæˆ(diffusion-based generation)æŠ€æœ¯ä¸åŸºäºå‡¸åŒ…(convex hull)çš„è¯­ä¹‰æ©ç è¿›è¡Œç²¾ç¡®å¼•å¯¼ã€‚è¿™äº›æ©ç æä¾›äº†ç‰©ä½“åœ¨çœŸå®ç¯å¢ƒä¸­çš„å°ºå¯¸ã€æœå‘å’Œä½ç½®ä¿¡æ¯ï¼Œç¡®ä¿äº†ç”Ÿæˆç»“æœçš„å‡ ä½•ä¸€è‡´æ€§ä¸çœŸå®æ„Ÿã€‚å®éªŒåœ¨KITTI-360æ•°æ®é›†ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜å…¶èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„å¤æ‚è¾¹ç¼˜æ¡ˆä¾‹å’ŒåŠ¨æ€åœºæ™¯ã€‚Range-Editä¸ºç”Ÿæˆå¤šæ ·åŒ–LiDARæ•°æ®æä¾›äº†ä¸€ç§æˆæœ¬æ•ˆç›Šé«˜ä¸”å…·å¤‡å¯æ‰©å±•æ€§çš„è§£å†³æ–¹æ¡ˆï¼Œæ˜¾è‘—æå‡äº†è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„é²æ£’æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.17269v1",
      "published_date": "2025-11-21 14:16:27 UTC",
      "updated_date": "2025-11-21 14:16:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:00:18.183691+00:00"
    },
    {
      "arxiv_id": "2511.17689v1",
      "title": "ARISE: Agentic Rubric-Guided Iterative Survey Engine for Automated Scholarly Paper Generation",
      "title_zh": "ARISEï¼šåŸºäºè¯„ä»·å‡†åˆ™å¼•å¯¼çš„æ™ºèƒ½ä½“è¿­ä»£å¼å­¦æœ¯ç»¼è¿°è‡ªåŠ¨ç”Ÿæˆå¼•æ“",
      "authors": [
        "Zi Wang",
        "Xingqiao Wang",
        "Sangah Lee",
        "Xiaowei Xu"
      ],
      "abstract": "The rapid expansion of scholarly literature presents significant challenges in synthesizing comprehensive, high-quality academic surveys. Recent advancements in agentic systems offer considerable promise for automating tasks that traditionally require human expertise, including literature review, synthesis, and iterative refinement. However, existing automated survey-generation solutions often suffer from inadequate quality control, poor formatting, and limited adaptability to iterative feedback, which are core elements intrinsic to scholarly writing.\n  To address these limitations, we introduce ARISE, an Agentic Rubric-guided Iterative Survey Engine designed for automated generation and continuous refinement of academic survey papers. ARISE employs a modular architecture composed of specialized large language model agents, each mirroring distinct scholarly roles such as topic expansion, citation curation, literature summarization, manuscript drafting, and peer-review-based evaluation. Central to ARISE is a rubric-guided iterative refinement loop in which multiple reviewer agents independently assess manuscript drafts using a structured, behaviorally anchored rubric, systematically enhancing the content through synthesized feedback.\n  Evaluating ARISE against state-of-the-art automated systems and recent human-written surveys, our experimental results demonstrate superior performance, achieving an average rubric-aligned quality score of 92.48. ARISE consistently surpasses baseline methods across metrics of comprehensiveness, accuracy, formatting, and overall scholarly rigor. All code, evaluation rubrics, and generated outputs are provided openly at https://github.com/ziwang11112/ARISE",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å­¦æœ¯æ–‡çŒ®å¿«é€Ÿå¢é•¿å¸¦æ¥çš„é«˜è´¨é‡ç»¼è¿°åˆæˆæŒ‘æˆ˜ï¼Œæå‡ºäº† ARISE (Agentic Rubric-guided Iterative Survey Engine)ï¼Œä¸€ç§ç”¨äºè‡ªåŠ¨åŒ–ç”Ÿæˆå’ŒæŒç»­æ”¹è¿›å­¦æœ¯ç»¼è¿°è®ºæ–‡çš„æ™ºèƒ½ä½“å¼•æ“ã€‚ARISE é‡‡ç”¨æ¨¡å—åŒ–æ¶æ„ï¼Œç”±å¤šä¸ªä¸“é—¨çš„å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“ (LLM agents) ç»„æˆï¼Œåˆ†åˆ«æ¨¡æ‹Ÿä¸»é¢˜æ‰©å±• (topic expansion)ã€å¼•ç”¨ç­–åˆ’ (citation curation)ã€æ–‡çŒ®æ€»ç»“ (literature summarization) å’Œåˆç¨¿æ’°å†™ (manuscript drafting) ç­‰å­¦æœ¯è§’è‰²ã€‚è¯¥ç³»ç»Ÿçš„æ ¸å¿ƒåœ¨äºå‡†åˆ™å¼•å¯¼çš„è¿­ä»£ä¼˜åŒ–å¾ªç¯ (rubric-guided iterative refinement loop)ï¼Œé€šè¿‡å¤šä¸ªè¯„å®¡æ™ºèƒ½ä½“æ ¹æ®ç»“æ„åŒ–å‡†åˆ™å¯¹åˆç¨¿è¿›è¡Œç‹¬ç«‹è¯„ä¼°ï¼Œå¹¶åˆ©ç”¨ç»¼åˆåé¦ˆç³»ç»Ÿåœ°å¢å¼ºå†…å®¹è´¨é‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒARISE åœ¨å…¨é¢æ€§ã€å‡†ç¡®æ€§ã€æ ¼å¼è§„èŒƒæ€§å’Œå­¦æœ¯ä¸¥è°¨æ€§ç­‰æŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰çš„è‡ªåŠ¨åŒ–ç³»ç»Ÿå’Œè¿‘æœŸçš„äººå·¥æ’°å†™ç»¼è¿°ï¼Œè¾¾åˆ°äº† 92.48 çš„å¹³å‡è´¨é‡å¾—åˆ†ã€‚è¯¥ç ”ç©¶è§£å†³äº†ç°æœ‰è‡ªåŠ¨åŒ–æ–¹æ¡ˆåœ¨è´¨é‡æ§åˆ¶å’Œåé¦ˆé€‚åº”æ€§æ–¹é¢çš„å±€é™ï¼Œå¹¶å·²å°†ç›¸å…³ä»£ç å’Œè¯„ä¼°å·¥å…·å¼€æºï¼Œä¸ºå¯ä¿¡çš„è‡ªåŠ¨åŒ–ç§‘å­¦å†™ä½œæä¾›äº†æ–°çš„èŒƒå¼ã€‚",
      "categories": [
        "cs.DL",
        "cs.AI"
      ],
      "primary_category": "cs.DL",
      "comment": "20 pages including an appendix, 7 figures and 6 tables",
      "pdf_url": "https://arxiv.org/pdf/2511.17689v1",
      "published_date": "2025-11-21 14:14:35 UTC",
      "updated_date": "2025-11-21 14:14:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:00:25.778833+00:00"
    },
    {
      "arxiv_id": "2511.17265v1",
      "title": "DISCA: A Digital In-memory Stochastic Computing Architecture Using A Compressed Bent-Pyramid Format",
      "title_zh": "DISCAï¼šä¸€ç§åŸºäºå‹ç¼© Bent-Pyramid æ ¼å¼çš„æ•°å­—å­˜å†…éšæœºè®¡ç®—æ¶æ„",
      "authors": [
        "Shady Agwa",
        "Yikang Shen",
        "Shiwei Wang",
        "Themis Prodromakis"
      ],
      "abstract": "Nowadays, we are witnessing an Artificial Intelligence revolution that dominates the technology landscape in various application domains, such as healthcare, robotics, automotive, security, and defense. Massive-scale AI models, which mimic the human brain's functionality, typically feature millions and even billions of parameters through data-intensive matrix multiplication tasks. While conventional Von-Neumann architectures struggle with the memory wall and the end of Moore's Law, these AI applications are migrating rapidly towards the edge, such as in robotics and unmanned aerial vehicles for surveillance, thereby adding more constraints to the hardware budget of AI architectures at the edge. Although in-memory computing has been proposed as a promising solution for the memory wall, both analog and digital in-memory computing architectures suffer from substantial degradation of the proposed benefits due to various design limitations. We propose a new digital in-memory stochastic computing architecture, DISCA, utilizing a compressed version of the quasi-stochastic Bent-Pyramid data format. DISCA inherits the same computational simplicity of analog computing, while preserving the same scalability, productivity, and reliability of digital systems. Post-layout modeling results of DISCA show an energy efficiency of 3.59 TOPS/W per bit at 500 MHz using a commercial 180nm CMOS technology. Therefore, DISCA significantly improves the energy efficiency for matrix multiplication workloads by orders of magnitude if scaled and compared to its counterpart architectures.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è§„æ¨¡äººå·¥æ™ºèƒ½æ¨¡å‹åœ¨è¾¹ç¼˜è®¡ç®—ä¸­é¢ä¸´çš„å­˜å‚¨å¢™åŠç¡¬ä»¶é¢„ç®—å—é™é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºDISCAçš„æ•°å­—å­˜å†…éšæœºè®¡ç®—æ¶æ„(Digital In-memory Stochastic Computing Architecture)ã€‚è¯¥æ¶æ„é‡‡ç”¨äº†å‹ç¼©ç‰ˆçš„å‡†éšæœºBent-Pyramidæ•°æ®æ ¼å¼ï¼Œåœ¨ç»§æ‰¿æ¨¡æ‹Ÿè®¡ç®—ç®€ä¾¿æ€§çš„åŒæ—¶ï¼Œä¿ç•™äº†æ•°å­—ç³»ç»Ÿçš„é«˜å¯æ‰©å±•æ€§ã€ç”Ÿäº§æ•ˆç‡å’Œå¯é æ€§ã€‚é€šè¿‡åœ¨å•†ä¸š180nm CMOSå·¥è‰ºä¸‹è¿›è¡Œ500 MHzçš„ç‰ˆå›¾åå»ºæ¨¡éªŒè¯ï¼ŒDISCAå®ç°äº†æ¯æ¯”ç‰¹3.59 TOPS/Wçš„èƒ½æ•ˆè¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥è®¾è®¡åœ¨å¤„ç†çŸ©é˜µä¹˜æ³•(Matrix Multiplication)å·¥ä½œè´Ÿè½½æ—¶ï¼Œç›¸è¾ƒäºåŒç±»æ¶æ„åœ¨èƒ½æ•ˆä¸Šå®ç°äº†æ˜¾è‘—æå‡ã€‚DISCAä¸ºåœ¨åŒ»ç–—ã€æœºå™¨äººåŠå›½é˜²å®‰å…¨ç­‰èµ„æºå—é™çš„è¾¹ç¼˜åº”ç”¨åœºæ™¯ä¸­éƒ¨ç½²é«˜æ€§èƒ½AIæ¨¡å‹æä¾›äº†é«˜æ•ˆçš„ç¡¬ä»¶è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.ET",
        "cs.PF"
      ],
      "primary_category": "cs.AR",
      "comment": "6 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.17265v1",
      "published_date": "2025-11-21 14:13:16 UTC",
      "updated_date": "2025-11-21 14:13:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:00:21.466398+00:00"
    },
    {
      "arxiv_id": "2511.17688v1",
      "title": "Enhancing Adversarial Transferability through Block Stretch and Shrink",
      "title_zh": "é€šè¿‡å—æ‹‰ä¼¸ä¸æ”¶ç¼©å¢å¼ºå¯¹æŠ—è¿ç§»æ€§",
      "authors": [
        "Quan Liu",
        "Feng Ye",
        "Chenhao Lu",
        "Shuming Zhen",
        "Guanliang Huang",
        "Lunzhe Chen",
        "Xudong Ke"
      ],
      "abstract": "Adversarial attacks introduce small, deliberately crafted perturbations that mislead neural networks, and their transferability from white-box to black-box target models remains a critical research focus. Input transformation-based attacks are a subfield of adversarial attacks that enhance input diversity through input transformations to improve the transferability of adversarial examples. However, existing input transformation-based attacks tend to exhibit limited cross-model transferability. Previous studies have shown that high transferability is associated with diverse attention heatmaps and the preservation of global semantics in transformed inputs. Motivated by this observation, we propose Block Stretch and Shrink (BSS), a method that divides an image into blocks and applies stretch and shrink operations to these blocks, thereby diversifying attention heatmaps in transformed inputs while maintaining their global semantics. Empirical evaluations on a subset of ImageNet demonstrate that BSS outperforms existing input transformation-based attack methods in terms of transferability. Furthermore, we examine the impact of the number scale, defined as the number of transformed inputs, in input transformation-based attacks, and advocate evaluating these methods under a unified number scale to enable fair and comparable assessments.",
      "tldr_zh": "è¯¥é¡¹ç ”ç©¶é’ˆå¯¹å¯¹æŠ—æ”»å‡»(Adversarial attacks)ä¸­ç™½ç›’åˆ°é»‘ç›’æ¨¡å‹çš„è¿ç§»æ€§(transferability)é—®é¢˜ï¼Œæå‡ºäº†Block Stretch and Shrink (BSS)æ–¹æ³•ã€‚åŸºäºé«˜è¿ç§»æ€§ä¸å¤šæ ·åŒ–çš„æ³¨æ„çƒ­å›¾(attention heatmaps)åŠå…¨å±€è¯­ä¹‰(global semantics)ä¿æŒç›¸å…³çš„è§‚å¯Ÿï¼ŒBSSå°†å›¾åƒåˆ’åˆ†ä¸ºå—ï¼Œå¹¶å¯¹è¿™äº›å—æ‰§è¡Œæ‹‰ä¼¸å’Œæ”¶ç¼©æ“ä½œã€‚è¿™ç§å˜æ¢æ–¹å¼åœ¨å¢å¼ºè¾“å…¥å¤šæ ·æ€§çš„åŒæ—¶ï¼Œæœ‰æ•ˆåœ°ç»´æŒäº†å›¾åƒçš„æ•´ä½“è¯­ä¹‰ç‰¹å¾ã€‚åœ¨ImageNetæ•°æ®é›†ä¸Šçš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒBSSåœ¨è¿ç§»æ€§è¡¨ç°ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºäºè¾“å…¥å˜æ¢çš„æ”»å‡»æ–¹æ³•ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ¢è®¨äº†å˜æ¢è¾“å…¥æ•°é‡(number scale)å¯¹æ”»å‡»æ•ˆæœçš„å½±å“ï¼Œå¹¶ä¸»å¼ åœ¨ç»Ÿä¸€çš„æ•°é‡è§„æ¨¡ä¸‹è¯„ä¼°æ­¤ç±»æ–¹æ³•ï¼Œä»¥ç¡®ä¿æ€§èƒ½è¯„ä»·çš„å…¬å¹³æ€§ä¸å¯æ¯”æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "code will be releace",
      "pdf_url": "https://arxiv.org/pdf/2511.17688v1",
      "published_date": "2025-11-21 14:00:01 UTC",
      "updated_date": "2025-11-21 14:00:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:00:31.067708+00:00"
    },
    {
      "arxiv_id": "2511.17254v2",
      "title": "Intervene-All-Paths: Unified Mitigation of LVLM Hallucinations across Alignment Formats",
      "title_zh": "Intervene-All-Pathsï¼šè·¨å¯¹é½æ ¼å¼çš„ LVLM å¹»è§‰ç»Ÿä¸€ç¼“è§£",
      "authors": [
        "Jiaye Qian",
        "Ge Zheng",
        "Yuchen Zhu",
        "Sibei Yang"
      ],
      "abstract": "Despite their impressive performance across a wide range of tasks, Large Vision-Language Models (LVLMs) remain prone to hallucination. In this study, we propose a comprehensive intervention framework aligned with the transformer's causal architecture in LVLMs, integrating the effects of different intervention paths on hallucination. We find that hallucinations in LVLMs do not arise from a single causal path, but rather from the interplay among image-to-input-text, image-to-output-text, and text-to-text pathways. For the first time, we also find that LVLMs rely on different pathways depending on the question-answer alignment format. Building on these insights, we propose simple yet effective methods to identify and intervene on critical hallucination heads within each pathway, tailored to discriminative and generative formats. Experiments across multiple benchmarks demonstrate that our approach consistently reduces hallucinations across diverse alignment types.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹(LVLMs)æ™®éå­˜åœ¨çš„å¹»è§‰é—®é¢˜ï¼Œæå‡ºäº†Intervene-All-Pathsç»Ÿä¸€å¹²é¢„æ¡†æ¶ã€‚è¯¥æ¡†æ¶ä¸transformerçš„å› æœæ¶æ„ç›¸å¥‘åˆï¼Œç³»ç»Ÿæ€§åœ°æ•´åˆäº†ä¸åŒå¹²é¢„è·¯å¾„å¯¹å¹»è§‰çš„å½±å“ã€‚ç ”ç©¶å‘ç°ï¼ŒLVLMsä¸­çš„å¹»è§‰å¹¶éäº§ç”Ÿäºå•ä¸€è·¯å¾„ï¼Œè€Œæ˜¯æºäºimage-to-input-textã€image-to-output-textä»¥åŠtext-to-textè·¯å¾„ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚æ­¤å¤–ï¼Œç ”ç©¶é¦–æ¬¡å‘ç°LVLMsä¼šæ ¹æ®é—®ç­”å¯¹é½(alignment)æ ¼å¼çš„ä¸åŒè€Œä¾èµ–ä¸åŒçš„å› æœè·¯å¾„ã€‚åŸºäºæ­¤æ´å¯Ÿï¼Œä½œè€…æå‡ºäº†ä¸€ç§è¯†åˆ«å¹¶å¹²é¢„å„è·¯å¾„ä¸­å…³é”®å¹»è§‰å¤´(hallucination heads)çš„æ–¹æ³•ï¼Œå¹¶é’ˆå¯¹åˆ¤åˆ«å¼å’Œç”Ÿæˆå¼æ ¼å¼è¿›è¡Œäº†ä¸“é—¨ä¼˜åŒ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­èƒ½å¤Ÿä¸€è‡´åœ°å‡å°‘å„ç§å¯¹é½ç±»å‹ä¸‹çš„å¹»è§‰ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹çš„é²æ£’æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to NeurIPS 2025, Project Page: https://github.com/SooLab/AllPath",
      "pdf_url": "https://arxiv.org/pdf/2511.17254v2",
      "published_date": "2025-11-21 13:57:38 UTC",
      "updated_date": "2026-01-06 08:06:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:00:43.566276+00:00"
    },
    {
      "arxiv_id": "2511.17238v1",
      "title": "Lost in Translation and Noise: A Deep Dive into the Failure Modes of VLMs on Real-World Tables",
      "title_zh": "è¿·å¤±åœ¨ç¿»è¯‘ä¸å™ªå£°ä¸­ï¼šæ·±å…¥æ¢ç©¶ VLMs åœ¨ç°å®åœºæ™¯è¡¨æ ¼ä¸­çš„å¤±æ•ˆæ¨¡å¼",
      "authors": [
        "Anshul Singh",
        "Rohan Chaudhary",
        "Gagneet Singh",
        "Abhay Kumary"
      ],
      "abstract": "The impressive performance of VLMs is largely measured on benchmarks that fail to capture the complexities of real-world scenarios. Existing datasets for tabular QA, such as WikiTableQuestions and FinQA, are overwhelmingly monolingual (English) and present tables in a digitally perfect, clean format. This creates a significant gap between research and practice. To address this, we present \\textbf{MirageTVQA}, a new benchmark designed to evaluate VLMs on these exact dimensions. Featuring nearly 60,000 QA pairs across 24 languages, MirageTVQA challenges models with tables that are not only multilingual but also visually imperfect, incorporating realistic noise to mimic scanned documents. Our evaluation of the leading VLMs reveals two primary failure points: a severe degradation in performance (over 35\\% drop for the best models) when faced with visual noise and a consistent English-first bias where reasoning abilities fail to transfer to other languages. MirageTVQA provides a benchmark for measuring and driving progress towards more robust VLM models for table reasoning. The dataset and the code are available at: https://github.com/anshulsc/MirageTVQA.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) åœ¨çœŸå®ä¸–ç•Œè¡¨æ ¼ç†è§£ä¸­å­˜åœ¨çš„å±€é™æ€§ï¼ŒæŒ‡å‡ºç°æœ‰åŸºå‡†æµ‹è¯•å¤§å¤šä»…é™äºè‹±æ–‡ä¸”æ•°æ®è¿‡äºç†æƒ³åŒ–ï¼Œç¼ºä¹å¯¹å¤šè¯­è¨€å’Œè§†è§‰å™ªå£°çš„å¤„ç†ã€‚ä¸ºæ­¤ï¼Œä½œè€…æ¨å‡ºäº† MirageTVQAï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å« 24 ç§è¯­è¨€ã€è¿‘ 60,000 ä¸ªé—®ç­”å¯¹çš„æ–°å‹åŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡å¼•å…¥æ¨¡æ‹Ÿæ‰«ææ–‡æ¡£çš„çœŸå®è§†è§‰å™ªå£°æ¥æŒ‘æˆ˜æ¨¡å‹çš„è¡¨æ ¼æ¨ç†èƒ½åŠ›ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œé¢†å…ˆçš„ VLMs åœ¨é¢å¯¹è§†è§‰å™ªå£°æ—¶æ€§èƒ½ä¼šä¸‹é™è¶…è¿‡ 35%ï¼Œä¸”æ™®éå­˜åœ¨æ˜æ˜¾çš„ English-first åè§ï¼Œå¯¼è‡´å…¶æ¨ç†èƒ½åŠ›æ— æ³•æœ‰æ•ˆè¿ç§»åˆ°å…¶ä»–è¯­è¨€ã€‚MirageTVQA ä¸ºè¡¡é‡å’Œæ¨åŠ¨æ›´å…·é²æ£’æ€§çš„ VLM è¡¨æ ¼æ¨ç†æ¨¡å‹çš„å‘å±•æä¾›äº†é‡è¦å·¥å…·ï¼Œç›®å‰è¯¥æ•°æ®é›†å’Œä»£ç å·²åœ¨ GitHub å¼€æºã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted as Spotligh Talk at EurIPS 2025 Workshop on AI For Tabular Data",
      "pdf_url": "https://arxiv.org/pdf/2511.17238v1",
      "published_date": "2025-11-21 13:32:56 UTC",
      "updated_date": "2025-11-21 13:32:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:01:18.987898+00:00"
    },
    {
      "arxiv_id": "2511.17233v1",
      "title": "Algorithmic design and implementation considerations of deep MPC",
      "title_zh": "æ·±åº¦æ¨¡å‹é¢„æµ‹æ§åˆ¶çš„ç®—æ³•è®¾è®¡ä¸å®ç°è€ƒé‡",
      "authors": [
        "Prabhat K. Mishra",
        "Mateus V. Gasparino",
        "Girish Chowdhary"
      ],
      "abstract": "Deep Model Predictive Control (Deep MPC) is an evolving field that integrates model predictive control and deep learning. This manuscript is focused on a particular approach, which employs deep neural network in the loop with MPC. This class of approaches distributes control authority between a neural network and an MPC controller, in such a way that the neural network learns the model uncertainties while the MPC handles constraints. The approach is appealing because training data collected while the system is in operation can be used to fine-tune the neural network, and MPC prevents unsafe behavior during those learning transients. This manuscript explains implementation challenges of Deep MPC, algorithmic way to distribute control authority and argues that a poor choice in distributing control authority may lead to poor performance. A reason of poor performance is explained through a numerical experiment on a four-wheeled skid-steer dynamics.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Deep Model Predictive Control (Deep MPC) çš„ç®—æ³•è®¾è®¡ä¸å®ç°ï¼Œé‡ç‚¹åˆ†æäº†å°† Deep Neural Network é›†æˆåˆ° MPC å¾ªç¯ä¸­çš„æ§åˆ¶æ–¹æ³•ã€‚åœ¨è¯¥æ¶æ„ä¸‹ï¼Œç¥ç»ç½‘ç»œè´Ÿè´£å­¦ä¹ æ¨¡å‹çš„ä¸ç¡®å®šæ€§ï¼Œè€Œ MPC è´Ÿè´£å¤„ç†ç³»ç»Ÿçº¦æŸï¼Œä»è€Œåœ¨åˆ©ç”¨è¿è¡Œæ•°æ®åœ¨çº¿å¾®è°ƒ (Fine-tune) æ¨¡å‹çš„åŒæ—¶ï¼Œç¡®ä¿ç³»ç»Ÿåœ¨å­¦ä¹ è¿‡æ¸¡æœŸçš„å®‰å…¨æ€§ã€‚è®ºæ–‡è¯¦ç»†é˜è¿°äº† Deep MPC é¢ä¸´çš„å®ç°æŒ‘æˆ˜ä»¥åŠåˆ†é…æ§åˆ¶æƒ (Control Authority) çš„ç®—æ³•é€”å¾„ï¼Œå¹¶å¼ºè°ƒäº†æ§åˆ¶æƒåˆ†é…ä¸å½“å¯¹ç³»ç»Ÿæ€§èƒ½çš„è´Ÿé¢å½±å“ã€‚é€šè¿‡å¯¹å››è½®æ»‘ç§»è½¬å‘ (Skid-steer) åŠ¨åŠ›å­¦çš„æ•°å€¼å®éªŒï¼Œè¯¥ç ”ç©¶æ·±å…¥å‰–æäº†æ€§èƒ½ä¸‹é™çš„åŸå› ï¼Œä¸º Deep MPC çš„å®é™…éƒ¨ç½²æä¾›äº†é‡è¦çš„ç†è®ºæŒ‡å¯¼ä¸è®¾è®¡è€ƒé‡ã€‚",
      "categories": [
        "eess.SY",
        "cs.AI"
      ],
      "primary_category": "eess.SY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17233v1",
      "published_date": "2025-11-21 13:21:20 UTC",
      "updated_date": "2025-11-21 13:21:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:02:04.990775+00:00"
    },
    {
      "arxiv_id": "2511.17225v1",
      "title": "TP-MDDN: Task-Preferenced Multi-Demand-Driven Navigation with Autonomous Decision-Making",
      "title_zh": "TP-MDDNï¼šå…·å¤‡è‡ªä¸»å†³ç­–èƒ½åŠ›çš„ä»»åŠ¡åå¥½å‹å¤šéœ€æ±‚é©±åŠ¨å¯¼èˆª",
      "authors": [
        "Shanshan Li",
        "Da Huang",
        "Yu He",
        "Yanwei Fu",
        "Yu-Gang Jiang",
        "Xiangyang Xue"
      ],
      "abstract": "In daily life, people often move through spaces to find objects that meet their needs, posing a key challenge in embodied AI. Traditional Demand-Driven Navigation (DDN) handles one need at a time but does not reflect the complexity of real-world tasks involving multiple needs and personal choices. To bridge this gap, we introduce Task-Preferenced Multi-Demand-Driven Navigation (TP-MDDN), a new benchmark for long-horizon navigation involving multiple sub-demands with explicit task preferences. To solve TP-MDDN, we propose AWMSystem, an autonomous decision-making system composed of three key modules: BreakLLM (instruction decomposition), LocateLLM (goal selection), and StatusMLLM (task monitoring). For spatial memory, we design MASMap, which combines 3D point cloud accumulation with 2D semantic mapping for accurate and efficient environmental understanding. Our Dual-Tempo action generation framework integrates zero-shot planning with policy-based fine control, and is further supported by an Adaptive Error Corrector that handles failure cases in real time. Experiments demonstrate that our approach outperforms state-of-the-art baselines in both perception accuracy and navigation robustness.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†TP-MDDNï¼ˆTask-Preferenced Multi-Demand-Driven Navigationï¼‰åŸºå‡†ï¼Œæ—¨åœ¨è§£å†³å…·èº«AIåœ¨å¤„ç†å…·æœ‰æ˜ç¡®ä»»åŠ¡åå¥½çš„å¤šéœ€æ±‚é•¿æ—¶å¯¼èˆªä»»åŠ¡ä¸­çš„å¤æ‚æ€§æŒ‘æˆ˜ã€‚ä¸ºåº”å¯¹è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†åä¸ºAWMSystemçš„è‡ªä¸»å†³ç­–ç³»ç»Ÿï¼Œå…¶æ ¸å¿ƒåŒ…å«BreakLLMï¼ˆæŒ‡ä»¤åˆ†è§£ï¼‰ã€LocateLLMï¼ˆç›®æ ‡é€‰æ‹©ï¼‰å’ŒStatusMLLMï¼ˆä»»åŠ¡ç›‘æµ‹ï¼‰ä¸‰ä¸ªå…³é”®æ¨¡å—ã€‚åœ¨ç©ºé—´è®°å¿†æ–¹é¢ï¼Œç³»ç»Ÿåˆ©ç”¨MASMapæ–¹æ¡ˆå°†3Dç‚¹äº‘ç´¯ç§¯ä¸2Dè¯­ä¹‰æ˜ å°„ç›¸ç»“åˆï¼Œå®ç°äº†é«˜æ•ˆä¸”ç²¾ç¡®çš„ç¯å¢ƒç†è§£ã€‚åŠ¨ä½œç”Ÿæˆåˆ™é‡‡ç”¨äº†Dual-Tempoæ¡†æ¶ï¼Œå°†é›¶æ ·æœ¬è§„åˆ’ä¸åŸºäºç­–ç•¥çš„å¾®æ§é›†æˆï¼Œå¹¶é…åˆAdaptive Error Correctorå®æ—¶ä¿®æ­£æ‰§è¡Œè¿‡ç¨‹ä¸­çš„å¤±è´¥æ¡ˆä¾‹ã€‚å®éªŒæ•°æ®è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ„ŸçŸ¥å‡†ç¡®æ€§åŠå¯¼èˆªé²æ£’æ€§ä¸Šå‡æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›åŸºçº¿æ¨¡å‹ï¼Œä¸ºç°å®åœºæ™¯ä¸‹å¤æ‚å¯¼èˆªä»»åŠ¡çš„è‡ªä¸»å†³ç­–æä¾›äº†æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted at NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.17225v1",
      "published_date": "2025-11-21 13:12:13 UTC",
      "updated_date": "2025-11-21 13:12:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:01:19.765827+00:00"
    },
    {
      "arxiv_id": "2511.17220v2",
      "title": "PARROT: Persuasion and Agreement Robustness Rating of Output Truth -- A Sycophancy Robustness Benchmark for LLMs",
      "title_zh": "PARROTï¼šè¾“å‡ºçœŸå®æ€§çš„è¯´æœä¸ä¸€è‡´é²æ£’æ€§è¯„çº§â€”â€”å¤§è¯­è¨€æ¨¡å‹é˜¿è°€å¥‰æ‰¿é²æ£’æ€§åŸºå‡†",
      "authors": [
        "Yusuf Ã‡elebi",
        "Ã–zay Ezerceli",
        "Mahmoud El Hussieni"
      ],
      "abstract": "This study presents PARROT (Persuasion and Agreement Robustness Rating of Output Truth), a robustness focused framework designed to measure the degradation in accuracy that occurs under social pressure exerted on users through authority and persuasion in large language models (LLMs) the phenomenon of sycophancy (excessive conformity). PARROT (i) isolates causal effects by comparing the neutral version of the same question with an authoritatively false version using a double-blind evaluation, (ii) quantifies confidence shifts toward the correct and imposed false responses using log-likelihood-based calibration tracking, and (iii) systematically classifies failure modes (e.g., robust correct, sycophantic agreement, reinforced error, stubborn error, self-correction, etc.) using an eight-state behavioral taxonomy. We evaluated 22 models using 1,302 MMLU-style multiple-choice questions across 13 domains and domain-specific authority templates. Findings show marked heterogeneity: advanced models (e.g., GPT-5, GPT-4.1, Claude Sonnet 4.5) exhibit low \"follow rates\" ($\\leq 11\\%$, GPT-5: 4\\%) and minimal accuracy loss, while older/smaller models show severe epistemic collapse (GPT-4: 80\\%, Qwen 2.5-1.5B: 94\\%). The danger is not limited to response changes; weak models reduce confidence in the correct response while increasing confidence in the imposed incorrect response. While international law and global knowledge at the domain level exhibit high fragility, elementary mathematics is relatively resilient. Consequently, we argue that the goal of \"resistance to overfitting pressure\" should be addressed as a primary objective alongside accuracy, harm avoidance, and privacy for safe deployment in the real world.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PARROT (Persuasion and Agreement Robustness Rating of Output Truth)ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“æ³¨äºé²æ£’æ€§çš„è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨è¡¡é‡å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨æƒå¨æˆ–åŠè¯´ç­‰ç¤¾ä¼šå‹åŠ›ä¸‹å› è°„åªš (Sycophancy) ç°è±¡å¯¼è‡´çš„å‡†ç¡®ç‡ä¸‹é™ã€‚PARROT é€šè¿‡åŒç›²è¯„ä¼°å¯¹æ¯”ä¸­æ€§é—®é¢˜ä¸å¸¦æœ‰æƒå¨æ€§é”™è¯¯å¼•å¯¼çš„ç‰ˆæœ¬æ¥éš”ç¦»å› æœæ•ˆåº”ï¼Œå¹¶åˆ©ç”¨åŸºäºå¯¹æ•°ä¼¼ç„¶çš„æ ¡å‡†è¿½è¸ª (Log-likelihood-based calibration tracking) é‡åŒ–æ¨¡å‹åœ¨æ­£ç¡®ç­”æ¡ˆä¸å¼ºåŠ é”™è¯¯ç­”æ¡ˆä¹‹é—´çš„ç½®ä¿¡åº¦åç§»ã€‚è¯¥æ¡†æ¶è¿˜å¼•å…¥äº†åŒ…å«å…«ç§çŠ¶æ€çš„è¡Œä¸ºåˆ†ç±»å­¦ (Behavioral taxonomy)ï¼Œç³»ç»Ÿåœ°è¯†åˆ«ç¨³å¥æ­£ç¡® (Robust correct) æˆ–è°„åªšä¸€è‡´ (Sycophantic agreement) ç­‰å¤±æ•ˆæ¨¡å¼ã€‚å¯¹ 22 ä¸ªæ¨¡å‹çš„è¯„ä¼°ç»“æœæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„å·®å¼‚æ€§ï¼ŒGPT-5 å’Œ Claude Sonnet 4.5 ç­‰å…ˆè¿›æ¨¡å‹è¡¨ç°å‡ºæä½çš„è·Ÿéšç‡ (Follow rate)ï¼Œè€Œè¾ƒæ—§æˆ–è¾ƒå°çš„æ¨¡å‹åˆ™é¢ä¸´ä¸¥é‡çš„è®¤çŸ¥å´©æºƒ (Epistemic collapse)ã€‚å®éªŒå‘ç°ï¼Œç¤¾ä¼šå‹åŠ›ä¸ä»…ä¼šæ”¹å˜æ¨¡å‹è¾“å‡ºï¼Œè¿˜ä¼šå‰Šå¼±å…¶å¯¹æ­£ç¡®çŸ¥è¯†çš„ç½®ä¿¡åº¦ï¼Œä¸”å›½é™…æ³•ç­‰é¢†åŸŸçŸ¥è¯†æ¯”åˆç­‰æ•°å­¦æ›´å…·è„†å¼±æ€§ã€‚å› æ­¤ï¼Œç ”ç©¶è€…ä¸»å¼ å°†æŠµå¾¡å‹åŠ›ä¸‹çš„è¿‡æ‹Ÿåˆä½œä¸ºä¸å‡†ç¡®æ€§ã€å®‰å…¨æ€§å¹¶åˆ—çš„ LLMs å®‰å…¨éƒ¨ç½²æ ¸å¿ƒç›®æ ‡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CE",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17220v2",
      "published_date": "2025-11-21 13:01:28 UTC",
      "updated_date": "2025-12-01 09:31:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:01:26.976041+00:00"
    },
    {
      "arxiv_id": "2511.21736v1",
      "title": "R2Q: Towards Robust 2-Bit Large Language Models via Residual Refinement Quantization",
      "title_zh": "R2Qï¼šé€šè¿‡æ®‹å·®ç»†åŒ–é‡åŒ–å®ç°é²æ£’çš„2æ¯”ç‰¹å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Jiayi Chen",
        "Jieqi Shi",
        "Jing Huo",
        "Chen Wu"
      ],
      "abstract": "The rapid progress of Large Language Models (LLMs) has brought substantial computational and memory demands, spurring the adoption of low-bit quantization. While 8-bit and 4-bit formats have become prevalent, extending quantization to 2 bits remains challenging due to severe accuracy degradation. To address this, we propose Residual Refinement Quantization (R2Q)-a novel 2-bit quantization framework that decomposes the process into two sequential 1-bit sub-quantizations, forming an adaptive quantization lattice. Extensive evaluations on Llama, OPT, and Qwen across diverse benchmarks-covering question answering, commonsense reasoning, and language modeling-demonstrate that R2Q consistently outperforms existing 2-bit quantization methods in both fine-grained and coarse-grained settings. By refining quantization through a residual learning mechanism, R2Q enhances performance, improves training stability, and accelerates convergence under extreme compression. Furthermore, its modular design enables seamless integration with existing quantization-aware training (QAT) frameworks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Residual Refinement Quantization (R2Q)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å®ç°é²æ£’2-bit Large Language Models (LLMs) çš„æ–°å‹é‡åŒ–æ¡†æ¶ã€‚é’ˆå¯¹ä½æ¯”ç‰¹é‡åŒ–ä¸­å¸¸è§çš„ä¸¥é‡ç²¾åº¦ä¸‹é™é—®é¢˜ï¼ŒR2Qé€šè¿‡å°†2-bité‡åŒ–è¿‡ç¨‹åˆ†è§£ä¸ºä¸¤ä¸ªè¿ç»­çš„1-bitå­é‡åŒ–æ­¥éª¤ï¼Œæ„å»ºå‡ºä¸€ä¸ªè‡ªé€‚åº”é‡åŒ–æ ¼ç‚¹(adaptive quantization lattice)æ¥ä¼˜åŒ–æ€§èƒ½ã€‚é€šè¿‡å¼•å…¥æ®‹å·®å­¦ä¹ æœºåˆ¶(residual learning mechanism)ï¼Œè¯¥æ¡†æ¶ä¸ä»…æå‡äº†æ¨¡å‹åœ¨æç«¯å‹ç¼©ä¸‹çš„è¡¨ç°ï¼Œè¿˜å¢å¼ºäº†è®­ç»ƒç¨³å®šæ€§å¹¶åŠ é€Ÿäº†æ”¶æ•›è¿‡ç¨‹ã€‚åœ¨Llamaã€OPTå’ŒQwenç­‰æ¨¡å‹ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜ï¼ŒR2Qåœ¨é—®ç­”ã€å¸¸è¯†æ¨ç†å’Œè¯­è¨€å»ºæ¨¡ç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºç°æœ‰çš„2-bité‡åŒ–æ–¹æ³•ã€‚æ­¤å¤–ï¼Œå…¶æ¨¡å—åŒ–è®¾è®¡ä½¿å…¶èƒ½å¤Ÿä¸ç°æœ‰çš„é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ(Quantization-Aware Training, QAT)æ¡†æ¶æ— ç¼é›†æˆï¼Œä¸ºè¶…å¤§è§„æ¨¡æ¨¡å‹çš„ä½æ¯”ç‰¹éƒ¨ç½²æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21736v1",
      "published_date": "2025-11-21 12:39:44 UTC",
      "updated_date": "2025-11-21 12:39:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:01:29.388639+00:00"
    },
    {
      "arxiv_id": "2511.17198v1",
      "title": "Designing Domain-Specific Agents via Hierarchical Task Abstraction Mechanism",
      "title_zh": "åŸºäºå±‚çº§åŒ–ä»»åŠ¡æŠ½è±¡æœºåˆ¶çš„ç‰¹å®šé¢†åŸŸæ™ºèƒ½ä½“è®¾è®¡",
      "authors": [
        "Kaiyu Li",
        "Jiayu Wang",
        "Zhi Wang",
        "Hui Qiao",
        "Weizhan Zhang",
        "Deyu Meng",
        "Xiangyong Cao"
      ],
      "abstract": "LLM-driven agents, particularly those using general frameworks like ReAct or human-inspired role-playing, often struggle in specialized domains that necessitate rigorously structured workflows. Fields such as remote sensing, requiring specialized tools (e.g., correction, spectral indices calculation), and multi-step procedures (e.g., numerous intermediate products and optional steps), significantly challenge generalized approaches. To address this gap, we introduce a novel agent design framework centered on a Hierarchical Task Abstraction Mechanism (HTAM). Specifically, HTAM moves beyond emulating social roles, instead structuring multi-agent systems into a logical hierarchy that mirrors the intrinsic task-dependency graph of a given domain. This task-centric architecture thus enforces procedural correctness and decomposes complex problems into sequential layers, where each layer's sub-agents operate on the outputs of the preceding layers. We instantiate this framework as EarthAgent, a multi-agent system tailored for complex geospatial analysis. To evaluate such complex planning capabilities, we build GeoPlan-bench, a comprehensive benchmark of realistic, multi-step geospatial planning tasks. It is accompanied by a suite of carefully designed metrics to evaluate tool selection, path similarity, and logical completeness. Experiments show that EarthAgent substantially outperforms a range of established single- and multi-agent systems. Our work demonstrates that aligning agent architecture with a domain's intrinsic task structure is a critical step toward building robust and reliable specialized autonomous systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå±‚æ¬¡åŒ–ä»»åŠ¡æŠ½è±¡æœºåˆ¶(Hierarchical Task Abstraction Mechanism, HTAM)çš„æ–°å‹æ™ºèƒ½ä½“è®¾è®¡æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³LLMé©±åŠ¨çš„æ™ºèƒ½ä½“åœ¨é¥æ„Ÿç­‰éœ€è¦ä¸¥è°¨ç»“æ„åŒ–å·¥ä½œæµçš„ç‰¹å®šé¢†åŸŸä¸­é¢ä¸´çš„æŒ‘æˆ˜ã€‚HTAMè¶…è¶Šäº†ä¼ ç»Ÿçš„Role-playingæ¨¡å¼ï¼Œé€šè¿‡æ„å»ºé•œåƒé¢†åŸŸå†…åœ¨ä»»åŠ¡ä¾èµ–å›¾(Task-dependency graph)çš„é€»è¾‘å±‚æ¬¡ç»“æ„ï¼Œå°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºé¡ºåºæ‰§è¡Œçš„å­å±‚çº§ï¼Œä»è€Œç¡®ä¿äº†ç¨‹åºçš„æ­£ç¡®æ€§ã€‚ç ”ç©¶è€…åŸºäºæ­¤æ¡†æ¶å¼€å‘äº†ä¸“é—¨ç”¨äºå¤æ‚åœ°ç†ç©ºé—´åˆ†æçš„EarthAgentç³»ç»Ÿï¼Œå¹¶é…å¥—å»ºç«‹äº†åŒ…å«å¤šæ­¥è§„åˆ’ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•GeoPlan-benchã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEarthAgentåœ¨å·¥å…·é€‰æ‹©ã€è·¯å¾„ç›¸ä¼¼åº¦å’Œé€»è¾‘å®Œæ•´æ€§ç­‰æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„Single-agentå’ŒMulti-agentç³»ç»Ÿã€‚è¯¥ç ”ç©¶è¯æ˜ï¼Œå°†æ™ºèƒ½ä½“æ¶æ„ä¸é¢†åŸŸçš„å†…åœ¨ä»»åŠ¡ç»“æ„ç›¸åŒ¹é…æ˜¯æ„å»ºç¨³å¥å¯é çš„ä¸“ä¸šåŒ–è‡ªä¸»ç³»ç»Ÿçš„å…³é”®ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "Page: https://earth-insights.github.io/EarthAgent",
      "pdf_url": "https://arxiv.org/pdf/2511.17198v1",
      "published_date": "2025-11-21 12:25:47 UTC",
      "updated_date": "2025-11-21 12:25:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:02:40.586385+00:00"
    },
    {
      "arxiv_id": "2511.17184v1",
      "title": "Attention-Guided Feature Fusion (AGFF) Model for Integrating Statistical and Semantic Features in News Text Classification",
      "title_zh": "èåˆç»Ÿè®¡ä¸è¯­ä¹‰ç‰¹å¾çš„æ³¨æ„åŠ›å¼•å¯¼ç‰¹å¾èåˆï¼ˆAGFFï¼‰æ–°é—»æ–‡æœ¬åˆ†ç±»æ¨¡å‹",
      "authors": [
        "Mohammad Zare"
      ],
      "abstract": "News text classification is a crucial task in natural language processing, essential for organizing and filtering the massive volume of digital content. Traditional methods typically rely on statistical features like term frequencies or TF-IDF values, which are effective at capturing word-level importance but often fail to reflect contextual meaning. In contrast, modern deep learning approaches utilize semantic features to understand word usage within context, yet they may overlook simple, high-impact statistical indicators. This paper introduces an Attention-Guided Feature Fusion (AGFF) model that combines statistical and semantic features in a unified framework. The model applies an attention-based mechanism to dynamically determine the relative importance of each feature type, enabling more informed classification decisions. Through evaluation on benchmark news datasets, the AGFF model demonstrates superior performance compared to both traditional statistical models and purely semantic deep learning models. The results confirm that strategic integration of diverse feature types can significantly enhance classification accuracy. Additionally, ablation studies validate the contribution of each component in the fusion process. The findings highlight the model's ability to balance and exploit the complementary strengths of statistical and semantic representations, making it a practical and effective solution for real-world news classification tasks.",
      "tldr_zh": "æ–°é—»æ–‡æœ¬åˆ†ç±»æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„é‡è¦ä»»åŠ¡ï¼Œä½†ä¼ ç»Ÿç»Ÿè®¡ç‰¹å¾ä¸ç°ä»£è¯­ä¹‰ç‰¹å¾åœ¨æ•æ‰è¯æ±‡é‡è¦æ€§å’Œä¸Šä¸‹æ–‡è¯­ä¹‰æ–¹é¢å„æœ‰å±€é™ã€‚è¯¥ç ”ç©¶æå‡ºäº†æ³¨æ„åŠ›å¼•å¯¼ç‰¹å¾èåˆï¼ˆAttention-Guided Feature Fusion, AGFFï¼‰æ¨¡å‹ï¼Œé€šè¿‡ç»Ÿä¸€æ¡†æ¶æ•´åˆè¿™ä¸¤ç±»ç‰¹å¾ã€‚è¯¥æ¨¡å‹åº”ç”¨æ³¨æ„åŠ›æœºåˆ¶ï¼ˆattention-based mechanismï¼‰åŠ¨æ€åˆ†é…ä¸åŒç‰¹å¾ç±»å‹çš„æƒé‡ï¼Œä»è€Œå®ç°æ›´ç²¾å‡†çš„åˆ†ç±»å†³ç­–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAGFFæ¨¡å‹åœ¨åŸºå‡†æ–°é—»æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿç»Ÿè®¡æ¨¡å‹å’Œçº¯è¯­ä¹‰æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚ç ”ç©¶è¯å®äº†ç»Ÿè®¡ä¸è¯­ä¹‰è¡¨å¾ä¹‹é—´çš„äº’è¡¥æ€§ï¼Œå¹¶é€šè¿‡æ¶ˆèå®éªŒï¼ˆablation studiesï¼‰éªŒè¯äº†å„èåˆç»„ä»¶çš„æœ‰æ•ˆè´¡çŒ®ã€‚è¯¥æˆæœä¸ºç°å®åœºæ™¯ä¸‹çš„æ–°é—»å†…å®¹ç»„ç»‡ä¸è¿‡æ»¤æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17184v1",
      "published_date": "2025-11-21 12:05:31 UTC",
      "updated_date": "2025-11-21 12:05:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:02:36.668456+00:00"
    },
    {
      "arxiv_id": "2511.19458v1",
      "title": "Personalized Reward Modeling for Text-to-Image Generation",
      "title_zh": "é¢å‘æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„ä¸ªæ€§åŒ–å¥–åŠ±å»ºæ¨¡",
      "authors": [
        "Jeongeun Lee",
        "Ryang Heo",
        "Dongha Lee"
      ],
      "abstract": "Recent text-to-image (T2I) models generate semantically coherent images from textual prompts, yet evaluating how well they align with individual user preferences remains an open challenge. Conventional evaluation methods, general reward functions or similarity-based metrics, fail to capture the diversity and complexity of personal visual tastes. In this work, we present PIGReward, a personalized reward model that dynamically generates user-conditioned evaluation dimensions and assesses images through CoT reasoning. To address the scarcity of user data, PIGReward adopt a self-bootstrapping strategy that reasons over limited reference data to construct rich user contexts, enabling personalization without user-specific training. Beyond evaluation, PIGReward provides personalized feedback that drives user-specific prompt optimization, improving alignment between generated images and individual intent. We further introduce PIGBench, a per-user preference benchmark capturing diverse visual interpretations of shared prompts. Extensive experiments demonstrate that PIGReward surpasses existing methods in both accuracy and interpretability, establishing a scalable and reasoning-based foundation for personalized T2I evaluation and optimization. Taken together, our findings highlight PIGReward as a robust steptoward individually aligned T2I generation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†PIGRewardï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹Text-to-Image (T2I)ç”Ÿæˆçš„ä¸ªæ€§åŒ–å¥–åŠ±æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿé€šç”¨å¥–åŠ±å‡½æ•°æ— æ³•æ•æ‰ä¸ªäººè§†è§‰å®¡ç¾å¤šæ ·æ€§çš„æŒ‘æˆ˜ã€‚è¯¥æ¨¡å‹åˆ©ç”¨CoT (Chain-of-Thought)æ¨ç†åŠ¨æ€ç”ŸæˆåŸºäºç”¨æˆ·çš„è¯„ä¼°ç»´åº¦ï¼Œå¹¶é‡‡ç”¨self-bootstrappingç­–ç•¥ä»æœ‰é™çš„å‚è€ƒæ•°æ®ä¸­æ„å»ºä¸°å¯Œçš„ç”¨æˆ·ä¸Šä¸‹æ–‡ï¼Œä»è€Œåœ¨æ— éœ€ç”¨æˆ·ç‰¹å®šè®­ç»ƒçš„æƒ…å†µä¸‹å®ç°ä¸ªæ€§åŒ–ã€‚é™¤äº†ä½œä¸ºè¯„ä¼°å·¥å…·ï¼ŒPIGRewardè¿˜èƒ½æä¾›ä¸ªæ€§åŒ–åé¦ˆä»¥é©±åŠ¨æç¤ºè¯ä¼˜åŒ–ï¼Œæ˜¾è‘—æå‡ç”Ÿæˆå›¾åƒä¸ä¸ªäººæ„å›¾çš„å¯¹é½ç¨‹åº¦ã€‚ç ”ç©¶è¿›ä¸€æ­¥å¼•å…¥äº†PIGBenchåŸºå‡†ï¼Œç”¨äºæ•æ‰ä¸åŒç”¨æˆ·å¯¹ç›¸åŒæç¤ºè¯çš„å¤šæ ·åŒ–è§†è§‰è§£è¯»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPIGRewardåœ¨å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºå®ç°ä¸ªä½“å¯¹é½çš„T2Iç”Ÿæˆæä¾›äº†ç¨³å¥çš„æ¨ç†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.19458v1",
      "published_date": "2025-11-21 12:04:24 UTC",
      "updated_date": "2025-11-21 12:04:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:01:47.175204+00:00"
    },
    {
      "arxiv_id": "2511.17170v1",
      "title": "Hallucinate Less by Thinking More: Aspect-Based Causal Abstention for Large Language Models",
      "title_zh": "æ·±åº¦æ€è€ƒä»¥å‡å°‘å¹»è§‰ï¼šå¤§è¯­è¨€æ¨¡å‹çš„åŸºäºç»´åº¦çš„å› æœå¼ƒç­”",
      "authors": [
        "Vy Nguyen",
        "Ziqi Xu",
        "Jeffrey Chan",
        "Estrid He",
        "Feng Xia",
        "Xiuzhen Zhang"
      ],
      "abstract": "Large Language Models (LLMs) often produce fluent but factually incorrect responses, a phenomenon known as hallucination. Abstention, where the model chooses not to answer and instead outputs phrases such as \"I don't know\", is a common safeguard. However, existing abstention methods typically rely on post-generation signals, such as generation variations or feedback, which limits their ability to prevent unreliable responses in advance. In this paper, we introduce Aspect-Based Causal Abstention (ABCA), a new framework that enables early abstention by analysing the internal diversity of LLM knowledge through causal inference. This diversity reflects the multifaceted nature of parametric knowledge acquired from various sources, representing diverse aspects such as disciplines, legal contexts, or temporal frames. ABCA estimates causal effects conditioned on these aspects to assess the reliability of knowledge relevant to a given query. Based on these estimates, we enable two types of abstention: Type-1, where aspect effects are inconsistent (knowledge conflict), and Type-2, where aspect effects consistently support abstention (knowledge insufficiency). Experiments on standard benchmarks demonstrate that ABCA improves abstention reliability, achieves state-of-the-art performance, and enhances the interpretability of abstention decisions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)äº§ç”Ÿçš„äº‹å®é”™è¯¯å¹»è§‰(hallucination)é—®é¢˜ï¼ŒæŒ‡å‡ºå½“å‰çš„æ‹’ç»å›ç­”(abstention)æ–¹æ³•å› è¿‡åº¦ä¾èµ–ç”Ÿæˆåä¿¡å·è€Œéš¾ä»¥æå‰é¢„é˜²ä¸å¯é å“åº”ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†åŸºäºå±‚é¢çš„å› æœå¼ƒæƒæ¡†æ¶(Aspect-Based Causal Abstention, ABCA)ï¼Œé€šè¿‡å› æœæ¨ç†(causal inference)åˆ†ææ¨¡å‹å†…éƒ¨åœ¨ä¸åŒå­¦ç§‘ã€èƒŒæ™¯æˆ–æ—¶é—´æ¡†æ¶ç­‰å±‚é¢(aspects)çš„çŸ¥è¯†å¤šæ ·æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡è¯„ä¼°ç›¸å…³å±‚é¢çš„å› æœæ•ˆåº”æ¥è¡¡é‡çŸ¥è¯†å¯é æ€§ï¼Œå¹¶ç”±æ­¤å®ç°äº†é’ˆå¯¹çŸ¥è¯†å†²çª(knowledge conflict)å’ŒçŸ¥è¯†ä¸è¶³(knowledge insufficiency)ä¸¤ç±»æƒ…å†µçš„æ—©æœŸå¼ƒæƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒABCAåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½(state-of-the-art)ï¼Œæ˜¾è‘—æå‡äº†å¼ƒæƒçš„å¯é æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¢å¼ºäº†æ¨¡å‹å†³ç­–çš„å¯è§£é‡Šæ€§(interpretability)ï¼Œä¸ºç¼“è§£æ¨¡å‹å¹»è§‰æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to AAAI 2026 (Main Technical Track)",
      "pdf_url": "https://arxiv.org/pdf/2511.17170v1",
      "published_date": "2025-11-21 11:42:49 UTC",
      "updated_date": "2025-11-21 11:42:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:03:48.681978+00:00"
    },
    {
      "arxiv_id": "2511.17165v1",
      "title": "MIR: Efficient Exploration in Episodic Multi-Agent Reinforcement Learning via Mutual Intrinsic Reward",
      "title_zh": "MIRï¼šåŸºäºäº’å†…åœ¨å¥–åŠ±çš„åˆ†å¹•å¼å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ é«˜æ•ˆæ¢ç´¢",
      "authors": [
        "Kesheng Chen",
        "Wenjian Luo",
        "Bang Zhang",
        "Zeping Yin",
        "Zipeng Ye"
      ],
      "abstract": "Episodic rewards present a significant challenge in reinforcement learning. While intrinsic reward methods have demonstrated effectiveness in single-agent rein-forcement learning scenarios, their application to multi-agent reinforcement learn-ing (MARL) remains problematic. The primary difficulties stem from two fac-tors: (1) the exponential sparsity of joint action trajectories that lead to rewards as the exploration space expands, and (2) existing methods often fail to account for joint actions that can influence team states. To address these challenges, this paper introduces Mutual Intrinsic Reward (MIR), a simple yet effective enhancement strategy for MARL with extremely sparse rewards like episodic rewards. MIR incentivizes individual agents to explore actions that affect their teammates, and when combined with original strategies, effectively stimulates team exploration and improves algorithm performance. For comprehensive experimental valida-tion, we extend the representative single-agent MiniGrid environment to create MiniGrid-MA, a series of MARL environments with sparse rewards. Our evalu-ation compares the proposed method against state-of-the-art approaches in the MiniGrid-MA setting, with experimental results demonstrating superior perfor-mance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹  (MARL) ä¸­å›åˆåˆ¶å¥–åŠ± (Episodic rewards) å¯¼è‡´çš„æ¢ç´¢ç©ºé—´æŒ‡æ•°çº§ç¨€ç–ï¼Œä»¥åŠç°æœ‰æ–¹æ³•éš¾ä»¥è¡¡é‡å½±å“å›¢é˜ŸçŠ¶æ€çš„è”åˆè¡ŒåŠ¨ç­‰é—®é¢˜ï¼Œæå‡ºäº† Mutual Intrinsic Reward (MIR) å¢å¼ºç­–ç•¥ã€‚MIR é€šè¿‡æ¿€åŠ±ä¸ªä½“æ™ºèƒ½ä½“æ¢ç´¢èƒ½å¤Ÿå½±å“å…¶é˜Ÿå‹çš„è¡Œä¸ºï¼Œå¹¶ç»“åˆåŸå§‹ç­–ç•¥æ¥æœ‰æ•ˆåˆºæ¿€å›¢é˜Ÿæ¢ç´¢ï¼Œä»è€Œæå‡ç®—æ³•æ€§èƒ½ã€‚ç ”ç©¶è€…è¿›ä¸€æ­¥å°†å•æ™ºèƒ½ä½“ç¯å¢ƒ MiniGrid æ‰©å±•ä¸º MiniGrid-MA ç³»åˆ—ç¨€ç–å¥–åŠ±ç¯å¢ƒï¼Œä»¥è¿›è¡Œå…¨é¢çš„å®éªŒéªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ MiniGrid-MA è®¾ç½®ä¸‹ï¼Œæ‰€æå‡ºçš„ MIR æ–¹æ³•åœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›ç®—æ³•ï¼Œæ˜¾è‘—å¢å¼ºäº†æ™ºèƒ½ä½“åœ¨æç«¯ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸‹çš„åä½œæ¢ç´¢èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17165v1",
      "published_date": "2025-11-21 11:32:28 UTC",
      "updated_date": "2025-11-21 11:32:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:02:55.974230+00:00"
    },
    {
      "arxiv_id": "2511.17162v1",
      "title": "The Belief-Desire-Intention Ontology for modelling mental reality and agency",
      "title_zh": "ç”¨äºå¿ƒç†ç°å®ä¸ä¸»ä½“æ€§å»ºæ¨¡çš„ä¿¡å¿µ-æ¬²æœ›-æ„å›¾æœ¬ä½“",
      "authors": [
        "Sara Zuppiroli",
        "Carmelo Fabio Longo",
        "Anna Sofia Lippolis",
        "Rocco Paolillo",
        "Lorenzo Giammei",
        "Miguel Ceriani",
        "Francesco Poggi",
        "Antonio Zinilli",
        "Andrea Giovanni Nuzzolese"
      ],
      "abstract": "The Belief-Desire-Intention (BDI) model is a cornerstone for representing rational agency in artificial intelligence and cognitive sciences. Yet, its integration into structured, semantically interoperable knowledge representations remains limited. This paper presents a formal BDI Ontology, conceived as a modular Ontology Design Pattern (ODP) that captures the cognitive architecture of agents through beliefs, desires, intentions, and their dynamic interrelations. The ontology ensures semantic precision and reusability by aligning with foundational ontologies and best practices in modular design. Two complementary lines of experimentation demonstrate its applicability: (i) coupling the ontology with Large Language Models (LLMs) via Logic Augmented Generation (LAG) to assess the contribution of ontological grounding to inferential coherence and consistency; and (ii) integrating the ontology within the Semas reasoning platform, which implements the Triples-to-Beliefs-to-Triples (T2B2T) paradigm, enabling a bidirectional flow between RDF triples and agent mental states. Together, these experiments illustrate how the BDI Ontology acts as both a conceptual and operational bridge between declarative and procedural intelligence, paving the way for cognitively grounded, explainable, and semantically interoperable multi-agent and neuro-symbolic systems operating within the Web of Data.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ­£å¼çš„Belief-Desire-Intention (BDI)æœ¬ä½“ï¼Œå¹¶å°†å…¶æ„å»ºä¸ºæ¨¡å—åŒ–æœ¬ä½“è®¾è®¡æ¨¡å¼(Ontology Design Pattern, ODP)ï¼Œæ—¨åœ¨è§£å†³ç†æ€§æ™ºèƒ½ä½“åœ¨ç»“æ„åŒ–è¯­ä¹‰äº’æ“ä½œçŸ¥è¯†è¡¨ç¤ºæ–¹é¢çš„å±€é™æ€§ã€‚è¯¥æœ¬ä½“é€šè¿‡å»ºæ¨¡ä¿¡å¿µã€æ¬²æœ›ã€æ„å›¾åŠå…¶åŠ¨æ€å…³è”æ¥æ•æ‰æ™ºèƒ½ä½“çš„è®¤çŸ¥æ¶æ„ï¼Œå¹¶ä¸åŸºç¡€æœ¬ä½“å¯¹é½ä»¥ç¡®ä¿è¯­ä¹‰ç²¾ç¡®æ€§å’Œå¯é‡ç”¨æ€§ã€‚ç ”ç©¶é€šè¿‡ä¸¤é¡¹å®éªŒéªŒè¯äº†å…¶æ•ˆç”¨ï¼šä¸€æ˜¯åˆ©ç”¨é€»è¾‘å¢å¼ºç”Ÿæˆ(Logic Augmented Generation, LAG)æŠ€æœ¯å°†æœ¬ä½“ä¸å¤§è¯­è¨€æ¨¡å‹(LLMs)ç»“åˆï¼Œæå‡äº†æ¨ç†çš„è¿è´¯æ€§ä¸ä¸€è‡´æ€§ï¼›äºŒæ˜¯å°†å…¶é›†æˆè‡³Semasæ¨ç†å¹³å°ï¼Œé€šè¿‡Triples-to-Beliefs-to-Triples (T2B2T)èŒƒå¼å®ç°äº†RDFä¸‰å…ƒç»„ä¸æ™ºèƒ½ä½“å¿ƒç†çŠ¶æ€çš„åŒå‘è½¬æ¢ã€‚è¯¥BDIæœ¬ä½“å……å½“äº†é™ˆè¿°æ€§æ™ºèƒ½ä¸è¿‡ç¨‹æ€§æ™ºèƒ½ä¹‹é—´çš„æ¡¥æ¢ï¼Œä¸ºåœ¨æ•°æ®ç½‘ç»œä¸­æ„å»ºå…·å¤‡è®¤çŸ¥åŸºç¡€ã€å¯è§£é‡Šä¸”è¯­ä¹‰äº’æ“ä½œçš„ç¥ç»ç¬¦å·ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17162v1",
      "published_date": "2025-11-21 11:30:17 UTC",
      "updated_date": "2025-11-21 11:30:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:04:07.463384+00:00"
    },
    {
      "arxiv_id": "2511.17161v1",
      "title": "The PLLuM Instruction Corpus",
      "title_zh": "PLLuM æŒ‡ä»¤è¯­æ–™åº“",
      "authors": [
        "Piotr PÄ™zik",
        "Filip Å»arnecki",
        "Konrad KaczyÅ„ski",
        "Anna Cichosz",
        "Zuzanna Deckert",
        "Monika Garnys",
        "Izabela Grabarczyk",
        "Wojciech Janowski",
        "Sylwia KarasiÅ„ska",
        "Aleksandra Kujawiak",
        "Piotr Misztela",
        "Maria SzymaÅ„ska",
        "Karolina Walkusz",
        "Igor Siek",
        "Maciej ChrabÄ…szcz",
        "Anna KoÅ‚os",
        "Agnieszka KarliÅ„ska",
        "Karolina Seweryn",
        "Aleksandra KrasnodÄ™bska",
        "Paula Betscher",
        "Zofia CieÅ›liÅ„ska",
        "Katarzyna Kowol",
        "Artur Wilczek",
        "Maciej TrzciÅ„ski",
        "Katarzyna Dziewulska",
        "Roman Roszko",
        "Tomasz BernaÅ›",
        "Jurgita VaiÄenonienÄ—",
        "Danuta Roszko",
        "PaweÅ‚ Levchuk",
        "PaweÅ‚ Kowalski",
        "Irena Prawdzic-Jankowska",
        "Marek KozÅ‚owski",
        "SÅ‚awomir Dadas",
        "RafaÅ‚ PoÅ›wiata",
        "Alina WrÃ³blewska",
        "Katarzyna Krasnowska-KieraÅ›",
        "Maciej Ogrodniczuk",
        "MichaÅ‚ Rudolf",
        "Piotr Rybak",
        "Karolina Saputa",
        "Joanna WoÅ‚oszyn",
        "Marcin Oleksy",
        "BartÅ‚omiej Koptyra",
        "Teddy Ferdinan",
        "StanisÅ‚aw WoÅºniak",
        "Maciej Piasecki",
        "PaweÅ‚ Walkowiak",
        "Konrad Wojtasik",
        "Arkadiusz Janz",
        "PrzemysÅ‚aw Kazienko",
        "Julia Moska",
        "Jan KocoÅ„"
      ],
      "abstract": "This paper describes the instruction dataset used to fine-tune a set of transformer-based large language models (LLMs) developed in the PLLuM (Polish Large Language Model) project. We present a functional typology of the organic, converted, and synthetic instructions used in PLLuM and share some observations about the implications of using human-authored versus synthetic instruction datasets in the linguistic adaptation of base LLMs. Additionally, we release the first representative subset of the PLLuM instruction corpus (PLLuMIC), which we believe to be useful in guiding and planning the development of similar datasets for other LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯¦ç»†ä»‹ç»äº†åœ¨ PLLuM (Polish Large Language Model) é¡¹ç›®ä¸­ç”¨äºå¾®è°ƒåŸºäº Transformer çš„å¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„æŒ‡ä»¤æ•°æ®é›†ã€‚ä½œè€…ä¸ºè¯¥è¯­æ–™åº“ä¸­çš„åŸç”Ÿ (organic)ã€è½¬æ¢ (converted) å’Œåˆæˆ (synthetic) æŒ‡ä»¤æ„å»ºäº†åŠŸèƒ½åˆ†ç±»ä½“ç³»ï¼Œå¹¶æ·±å…¥æ¢è®¨äº†ä¸åŒæŒ‡ä»¤æ¥æºå¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚è®ºæ–‡é‡ç‚¹åˆ†äº«äº†åœ¨åŸºç¡€å¤§è¯­è¨€æ¨¡å‹ (base LLMs) çš„è¯­è¨€é€‚é… (linguistic adaptation) è¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨äººå·¥ç¼–å†™æ•°æ®ä¸åˆæˆæ•°æ®æ‰€å¸¦æ¥çš„å·®å¼‚åŒ–è§‚å¯Ÿä¸å¯ç¤ºã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿå…¬å¼€å‘å¸ƒäº† PLLuM æŒ‡ä»¤è¯­æ–™åº“ (PLLuMIC) çš„é¦–ä¸ªä»£è¡¨æ€§å­é›†ï¼Œæ—¨åœ¨ä¸ºå…¶ä»–è¯­è¨€èƒŒæ™¯ä¸‹çš„ç±»ä¼¼æ•°æ®é›†å¼€å‘æä¾›è§„åˆ’è·¯å¾„ä¸å®è·µæŒ‡å¯¼ã€‚è¿™ä¸€èµ„æºçš„å‘å¸ƒä¸ä»…å¡«è¡¥äº†ç›¸å…³è¯­è¨€èµ„æºçš„ç©ºç™½ï¼Œä¹Ÿä¸ºå¤šè¯­è¨€å¤§æ¨¡å‹çš„æŒ‡ä»¤å¾®è°ƒæä¾›äº†é‡è¦çš„å‚è€ƒèŒƒå¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17161v1",
      "published_date": "2025-11-21 11:28:11 UTC",
      "updated_date": "2025-11-21 11:28:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:04:19.965693+00:00"
    },
    {
      "arxiv_id": "2511.17147v2",
      "title": "A lightweight detector for real-time detection of remote sensing images",
      "title_zh": "é¢å‘é¥æ„Ÿå›¾åƒå®æ—¶æ£€æµ‹çš„è½»é‡çº§æ£€æµ‹å™¨",
      "authors": [
        "Qianyi Wang",
        "Guoqiang Ren"
      ],
      "abstract": "Remote sensing imagery is widely used across various fields, yet real-time detection remains challenging due to the prevalence of small objects and the need to balance accuracy with efficiency. To address this, we propose DMG-YOLO, a lightweight real-time detector tailored for small object detection in remote sensing images. Specifically, we design a Dual-branch Feature Extraction (DFE) module in the backbone, which partitions feature maps into two parallel branches: one extracts local features via depthwise separable convolutions, and the other captures global context using a vision transformer with a gating mechanism. Additionally, a Multi-scale Feature Fusion (MFF) module with dilated convolutions enhances multi-scale integration while preserving fine details. In the neck, we introduce the Global and Local Aggregate Feature Pyramid Network (GLAFPN) to further boost small object detection through global-local feature fusion. Extensive experiments on the VisDrone2019 and NWPU VHR-10 datasets show that DMG-YOLO achieves competitive performance in terms of mAP, model size, and other key metrics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DMG-YOLOï¼Œä¸€ç§ä¸“ä¸ºé¥æ„Ÿå›¾åƒä¸­å°ç›®æ ‡æ£€æµ‹ï¼ˆsmall object detectionï¼‰è®¾è®¡çš„è½»é‡åŒ–å®æ—¶æ£€æµ‹å™¨ï¼Œæ—¨åœ¨æœ‰æ•ˆå¹³è¡¡æ£€æµ‹ç²¾åº¦ä¸æ¨ç†æ•ˆç‡ã€‚åœ¨ä¸»å¹²ç½‘ç»œä¸­ï¼Œç ”ç©¶è€…è®¾è®¡äº†åŒåˆ†æ”¯ç‰¹å¾æå–ï¼ˆDual-branch Feature Extraction, DFEï¼‰æ¨¡å—ï¼Œé€šè¿‡æ·±åº¦å¯åˆ†ç¦»å·ç§¯æå–å±€éƒ¨ç‰¹å¾ï¼Œå¹¶ç»“åˆå¸¦æœ‰é—¨æ§æœºåˆ¶çš„Vision Transformeræ•è·å…¨å±€ä¸Šä¸‹æ–‡ã€‚æ­¤å¤–ï¼Œå¤šå°ºåº¦ç‰¹å¾èåˆï¼ˆMulti-scale Feature Fusion, MFFï¼‰æ¨¡å—åˆ©ç”¨ç©ºæ´å·ç§¯ï¼ˆdilated convolutionsï¼‰åœ¨ä¿ç•™ç²¾ç»†ç»†èŠ‚çš„åŒæ—¶å¢å¼ºäº†å¤šå°ºåº¦é›†æˆèƒ½åŠ›ã€‚åœ¨Neckéƒ¨åˆ†ï¼Œè¯¥ç ”ç©¶å¼•å…¥äº†å…¨å±€ä¸å±€éƒ¨èšåˆç‰¹å¾é‡‘å­—å¡”ç½‘ç»œï¼ˆGlobal and Local Aggregate Feature Pyramid Network, GLAFPNï¼‰ï¼Œé€šè¿‡å…¨å±€ä¸å±€éƒ¨ç‰¹å¾çš„æ·±åº¦èåˆè¿›ä¸€æ­¥æå‡å°ç›®æ ‡è¯†åˆ«æ•ˆæœã€‚åœ¨VisDrone2019å’ŒNWPU VHR-10æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒDMG-YOLOåœ¨å¹³å‡ç²¾åº¦ï¼ˆmAPï¼‰ã€æ¨¡å‹å°ºå¯¸åŠå®æ—¶æ€§æŒ‡æ ‡ä¸Šå‡è¡¨ç°ä¼˜å¼‚ï¼Œä¸ºé¥æ„Ÿå›¾åƒçš„é«˜æ•ˆæ£€æµ‹æä¾›äº†å¯é çš„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "wrong results",
      "pdf_url": "https://arxiv.org/pdf/2511.17147v2",
      "published_date": "2025-11-21 11:11:04 UTC",
      "updated_date": "2025-12-04 13:06:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:04:13.780007+00:00"
    },
    {
      "arxiv_id": "2511.17685v1",
      "title": "Dual-Path Knowledge-Augmented Contrastive Alignment Network for Spatially Resolved Transcriptomics",
      "title_zh": "ç”¨äºç©ºé—´è½¬å½•ç»„å­¦çš„åŒè·¯å¾„çŸ¥è¯†å¢å¼ºå¯¹æ¯”å¯¹é½ç½‘ç»œ",
      "authors": [
        "Wei Zhang",
        "Jiajun Chu",
        "Xinci Liu",
        "Chen Tong",
        "Xinyue Li"
      ],
      "abstract": "Spatial Transcriptomics (ST) is a technology that measures gene expression profiles within tissue sections while retaining spatial context. It reveals localized gene expression patterns and tissue heterogeneity, both of which are essential for understanding disease etiology. However, its high cost has driven efforts to predict spatial gene expression from whole slide images. Despite recent advancements, current methods still face significant limitations, such as under-exploitation of high-level biological context, over-reliance on exemplar retrievals, and inadequate alignment of heterogeneous modalities. To address these challenges, we propose DKAN, a novel Dual-path Knowledge-Augmented contrastive alignment Network that predicts spatially resolved gene expression by integrating histopathological images and gene expression profiles through a biologically informed approach. Specifically, we introduce an effective gene semantic representation module that leverages the external gene database to provide additional biological insights, thereby enhancing gene expression prediction. Further, we adopt a unified, one-stage contrastive learning paradigm, seamlessly combining contrastive learning and supervised learning to eliminate reliance on exemplars, complemented with an adaptive weighting mechanism. Additionally, we propose a dual-path contrastive alignment module that employs gene semantic features as dynamic cross-modal coordinators to enable effective heterogeneous feature integration. Through extensive experiments across three public ST datasets, DKAN demonstrates superior performance over state-of-the-art models, establishing a new benchmark for spatial gene expression prediction and offering a powerful tool for advancing biological and clinical research.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç©ºé—´è½¬å½•ç»„å­¦(Spatial Transcriptomics)æˆæœ¬é«˜æ˜‚ä»¥åŠç°æœ‰é¢„æµ‹æ–¹æ³•åœ¨ç”Ÿç‰©èƒŒæ™¯åˆ©ç”¨å’Œæ¨¡æ€å¯¹é½æ–¹é¢çš„å±€é™ï¼Œæå‡ºäº†DKANï¼Œå³ä¸€ç§åŒè·¯å¾„çŸ¥è¯†å¢å¼ºå¯¹æ¯”å¯¹é½ç½‘ç»œ(Dual-path Knowledge-Augmented contrastive alignment Network)ã€‚è¯¥æ¨¡å‹é€šè¿‡å¼•å…¥åŸºå› è¯­ä¹‰è¡¨ç¤ºæ¨¡å—ï¼Œåˆ©ç”¨å¤–éƒ¨åŸºå› æ•°æ®åº“æä¾›æ·±å…¥çš„ç”Ÿç‰©å­¦è§è§£ï¼Œæ˜¾è‘—å¢å¼ºäº†å¯¹åŸºå› è¡¨è¾¾çš„é¢„æµ‹èƒ½åŠ›ã€‚DKANé‡‡ç”¨äº†ç»Ÿä¸€çš„ä¸€é˜¶æ®µå¯¹æ¯”å­¦ä¹ (Contrastive Learning)èŒƒå¼ï¼Œç»“åˆè‡ªé€‚åº”åŠ æƒæœºåˆ¶ä»¥æ¶ˆé™¤å¯¹æ ·æœ¬æ£€ç´¢(Exemplar Retrieval)çš„ä¾èµ–ï¼Œå¹¶åˆ©ç”¨åŒè·¯å¾„å¯¹æ¯”å¯¹é½æ¨¡å—ä½œä¸ºåŠ¨æ€è·¨æ¨¡æ€åè°ƒå™¨ï¼Œå®ç°äº†å¼‚æ„ç‰¹å¾çš„æœ‰æ•ˆæ•´åˆã€‚åœ¨ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDKANçš„æ€§èƒ½è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ï¼Œä¸ºç©ºé—´åŸºå› è¡¨è¾¾é¢„æµ‹æä¾›äº†å¼ºæœ‰åŠ›çš„å·¥å…·ã€‚è¯¥ç ”ç©¶ä¸ä»…æå‡äº†é¢„æµ‹ç²¾åº¦ï¼Œä¹Ÿä¸ºæ·±å…¥ç†è§£ç–¾ç—…ç—…ç†å’Œæ¨åŠ¨ç”Ÿç‰©ä¸´åºŠç ”ç©¶å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "q-bio.QM",
      "comment": "AAAI 2026 Oral, extended version",
      "pdf_url": "https://arxiv.org/pdf/2511.17685v1",
      "published_date": "2025-11-21 10:58:04 UTC",
      "updated_date": "2025-11-21 10:58:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:04:48.487118+00:00"
    },
    {
      "arxiv_id": "2511.17136v1",
      "title": "Device-Guided Music Transfer",
      "title_zh": "è®¾å¤‡å¼•å¯¼çš„éŸ³ä¹è¿ç§»",
      "authors": [
        "Manh Pham Hung",
        "Changshuo Hu",
        "Ting Dang",
        "Dong Ma"
      ],
      "abstract": "Device-guided music transfer adapts playback across unseen devices for users who lack them. Existing methods mainly focus on modifying the timbre, rhythm, harmony, or instrumentation to mimic genres or artists, overlooking the diverse hardware properties of the playback device (i.e., speaker). Therefore, we propose DeMT, which processes a speaker's frequency response curve as a line graph using a vision-language model to extract device embeddings. These embeddings then condition a hybrid transformer via feature-wise linear modulation. Fine-tuned on a self-collected dataset, DeMT enables effective speaker-style transfer and robust few-shot adaptation for unseen devices, supporting applications like device-style augmentation and quality enhancement.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DeMT (Device-Guided Music Transfer)ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰éŸ³ä¹è½¬æ¢æ–¹æ³•ä¸»è¦å…³æ³¨éŸ³è‰²ã€èŠ‚å¥å’Œæµæ´¾ï¼Œè€Œå¿½ç•¥äº†æ’­æ”¾è®¾å¤‡ï¼ˆå¦‚æ‰¬å£°å™¨ï¼‰å¤šæ ·åŒ–ç¡¬ä»¶å±æ€§çš„é—®é¢˜ã€‚DeMTåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ (Vision-Language Model) å°†æ‰¬å£°å™¨çš„é¢‘ç‡å“åº”æ›²çº¿ (Frequency Response Curve) ä½œä¸ºæŠ˜çº¿å›¾è¿›è¡Œå¤„ç†ï¼Œä»¥æå–å…³é”®çš„è®¾å¤‡åµŒå…¥ (Device Embeddings)ã€‚è¿™äº›åµŒå…¥éšåé€šè¿‡ç‰¹å¾çº¿æ€§è°ƒåˆ¶ (Feature-wise Linear Modulation) æŠ€æœ¯ä½œç”¨äºæ··åˆå˜æ¢å™¨ (Hybrid Transformer)ï¼Œå®ç°å¯¹éŸ³é¢‘ä¿¡å·çš„æ¡ä»¶æ§åˆ¶ã€‚é€šè¿‡åœ¨è‡ªå»ºæ•°æ®é›†ä¸Šçš„å¾®è°ƒï¼ŒDeMTä¸ä»…å®ç°äº†æœ‰æ•ˆçš„æ‰¬å£°å™¨é£æ ¼è½¬æ¢ï¼Œè¿˜å¯¹æœªè§è¿‡çš„è®¾å¤‡å±•ç°å‡ºç¨³å¥çš„å°‘æ ·æœ¬è‡ªé€‚åº” (Few-shot Adaptation) èƒ½åŠ›ã€‚è¯¥æ¡†æ¶æ”¯æŒè®¾å¤‡é£æ ¼å¢å¼º (Device-style Augmentation) å’ŒéŸ³è´¨æå‡ (Quality Enhancement) ç­‰åº”ç”¨ï¼Œä¸ºç”¨æˆ·åœ¨ç¼ºä¹ç‰¹å®šç¡¬ä»¶çš„æƒ…å†µä¸‹æ¨¡æ‹Ÿä¸åŒè®¾å¤‡çš„æ’­æ”¾æ•ˆæœæä¾›äº†å¯èƒ½ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17136v1",
      "published_date": "2025-11-21 10:57:11 UTC",
      "updated_date": "2025-11-21 10:57:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:05:20.984319+00:00"
    },
    {
      "arxiv_id": "2511.21735v1",
      "title": "Closing the Performance Gap Between AI and Radiologists in Chest X-Ray Reporting",
      "title_zh": "å¼¥åˆAIä¸æ”¾å°„ç§‘åŒ»ç”Ÿåœ¨èƒ¸éƒ¨Xçº¿æŠ¥å‘Šç”Ÿæˆä¸­çš„æ€§èƒ½å·®è·",
      "authors": [
        "Harshita Sharma",
        "Maxwell C. Reynolds",
        "Valentina Salvatelli",
        "Anne-Marie G. Sykes",
        "Kelly K. Horst",
        "Anton Schwaighofer",
        "Maximilian Ilse",
        "Olesya Melnichenko",
        "Sam Bond-Taylor",
        "Fernando PÃ©rez-GarcÃ­a",
        "Vamshi K. Mugu",
        "Alex Chan",
        "Ceylan Colak",
        "Shelby A. Swartz",
        "Motassem B. Nashawaty",
        "Austin J. Gonzalez",
        "Heather A. Ouellette",
        "Selnur B. Erdal",
        "Beth A. Schueler",
        "Maria T. Wetscherek",
        "Noel Codella",
        "Mohit Jain",
        "Shruthi Bannur",
        "Kenza Bouzid",
        "Daniel C. Castro",
        "Stephanie Hyland",
        "Panos Korfiatis",
        "Ashish Khandelwal",
        "Javier Alvarez-Valle"
      ],
      "abstract": "AI-assisted report generation offers the opportunity to reduce radiologists' workload stemming from expanded screening guidelines, complex cases and workforce shortages, while maintaining diagnostic accuracy. In addition to describing pathological findings in chest X-ray reports, interpreting lines and tubes (L&T) is demanding and repetitive for radiologists, especially with high patient volumes. We introduce MAIRA-X, a clinically evaluated multimodal AI model for longitudinal chest X-ray (CXR) report generation, that encompasses both clinical findings and L&T reporting. Developed using a large-scale, multi-site, longitudinal dataset of 3.1 million studies (comprising 6 million images from 806k patients) from Mayo Clinic, MAIRA-X was evaluated on three holdout datasets and the public MIMIC-CXR dataset, where it significantly improved AI-generated reports over the state of the art on lexical quality, clinical correctness, and L&T-related elements. A novel L&T-specific metrics framework was developed to assess accuracy in reporting attributes such as type, longitudinal change and placement. A first-of-its-kind retrospective user evaluation study was conducted with nine radiologists of varying experience, who blindly reviewed 600 studies from distinct subjects. The user study found comparable rates of critical errors (3.0% for original vs. 4.6% for AI-generated reports) and a similar rate of acceptable sentences (97.8% for original vs. 97.4% for AI-generated reports), marking a significant improvement over prior user studies with larger gaps and higher error rates. Our results suggest that MAIRA-X can effectively assist radiologists, particularly in high-volume clinical settings.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† MAIRA-Xï¼Œä¸€ç§ç»è¿‡ä¸´åºŠè¯„ä¼°çš„å¤šæ¨¡æ€ AI æ¨¡å‹ï¼Œæ—¨åœ¨å®ç°èƒ¸éƒ¨ X å°„çº¿ (CXR) æŠ¥å‘Šçš„çºµå‘ç”Ÿæˆï¼ŒåŒæ—¶æ¶µç›–ä¸´åºŠå‘ç°å’Œå¯¼ç®¡åŠç®¡è·¯ (Lines and Tubes, L&T) çš„æŠ¥å‘Šã€‚è¯¥æ¨¡å‹åˆ©ç”¨æ¥è‡ª Mayo Clinic çš„ 310 ä¸‡é¡¹ç ”ç©¶ï¼ˆåŒ…å« 600 ä¸‡å¼ å›¾åƒï¼‰çš„å¤§è§„æ¨¡å¤šä¸­å¿ƒçºµå‘æ•°æ®é›†è¿›è¡Œå¼€å‘ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªä¸“é—¨çš„ L&T æŒ‡æ ‡æ¡†æ¶æ¥è¯„ä¼°æŠ¥å‘Šå±æ€§çš„å‡†ç¡®æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒMAIRA-X åœ¨è¯æ±‡è´¨é‡ã€ä¸´åºŠæ­£ç¡®æ€§å’Œ L&T ç›¸å…³å…ƒç´ æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„ SOTA æ¨¡å‹ã€‚åœ¨ä¸€é¡¹ç”± 9 åä¸åŒèµ„å†æ”¾å°„ç§‘åŒ»ç”Ÿå‚ä¸çš„å›é¡¾æ€§ç›²æµ‹ç ”ç©¶ä¸­ï¼ŒMAIRA-X ç”ŸæˆæŠ¥å‘Šçš„å…³é”®é”™è¯¯ç‡ï¼ˆ4.6%ï¼‰ä¸åŸå§‹æŠ¥å‘Šï¼ˆ3.0%ï¼‰éå¸¸æ¥è¿‘ï¼Œä¸”å¯æ¥å—å¥å­æ¯”ä¾‹é«˜è¾¾ 97.4%ã€‚è¿™ä¸€ç»“æœæ ‡å¿—ç€ AI ç”ŸæˆæŠ¥å‘Šè´¨é‡ç›¸æ¯”ä»¥å¾€ç ”ç©¶æœ‰äº†é‡å¤§è¿›æ­¥ï¼Œè¯æ˜äº† MAIRA-X åœ¨å‡è½»æ”¾å°„ç§‘åŒ»ç”Ÿå·¥ä½œè´Ÿæ‹…åŠé«˜å®¹é‡ä¸´åºŠç¯å¢ƒä¸­çš„æœ‰æ•ˆè¾…åŠ©æ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21735v1",
      "published_date": "2025-11-21 10:53:26 UTC",
      "updated_date": "2025-11-21 10:53:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:05:27.475217+00:00"
    },
    {
      "arxiv_id": "2511.17131v1",
      "title": "UI-CUBE: Enterprise-Grade Computer Use Agent Benchmarking Beyond Task Accuracy to Operational Reliability",
      "title_zh": "UI-CUBEï¼šè¶…è¶Šä»»åŠ¡å‡†ç¡®æ€§ï¼Œè¿ˆå‘è¿è¡Œå¯é æ€§çš„ä¼ä¸šçº§è®¡ç®—æœºä½¿ç”¨æ™ºèƒ½ä½“åŸºå‡†æµ‹è¯•",
      "authors": [
        "Horia Cristescu",
        "Charles Park",
        "Trong Canh Nguyen",
        "Sergiu Talmacel",
        "Alexandru-Gabriel Ilie",
        "Stefan Adam"
      ],
      "abstract": "While current Computer Use Agent (CUA) benchmarks measure task completion effectively, they provide limited assessment of enterprise deployment readiness, emphasizing functional correctness over the operational reliability required for production systems. We present UI-CUBE (UiPath Computer Use BEnchmark), a systematic benchmark comprising 226 tasks across two difficulty tiers designed to expose fundamental architectural limitations in current CUAs. Our evaluation covers simple UI interactions (136 tasks) and complex workflows including copy-paste tasks (50 tasks) and enterprise application scenarios (40 tasks), with systematic interface variation coverage, multi-resolution testing and automated validation of task success through the application state. Evaluation of five state-of-the-art models reveals a sharp capability cliff rather than gradual performance degradation. Simple UI interactions achieve 67-85% success rates (compared to 97.9% human performance), but complex workflows drop precipitously to 9-19%. Human evaluators with no prior application experience achieve only 61.2% on complex tasks despite near-perfect performance on simple tasks, establishing realistic performance ceilings. This discontinuous performance pattern -- where agents achieve 68-87% of human performance on simple tasks but only 15-32% on complex workflows -- indicates fundamental architectural limitations in memory management, hierarchical planning, and state coordination rather than incremental capability gaps addressable through better training or prompting. UI-CUBE functions as an enterprise-readiness diagnostic, revealing that while current CUAs can manipulate individual interface elements, they cannot yet function as reliable workflow automation tools. These findings provide architectural insights essential for developing production-ready CUAs capable of managing complex, multi-step enterprise processes.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† UI-CUBE (UiPath Computer Use BEnchmark)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°è®¡ç®—æœºä½¿ç”¨æ™ºèƒ½ä½“ (Computer Use Agent, CUA) ä¼ä¸šçº§æ“ä½œå¯é æ€§è€Œéä»…ä»»åŠ¡å‡†ç¡®æ€§çš„ç³»ç»Ÿæ€§åŸºå‡†æµ‹è¯•ã€‚UI-CUBE åŒ…å« 226 ä¸ªä»»åŠ¡ï¼Œæ¶µç›–ç®€å• UI äº¤äº’ã€åŒ…å«å¤åˆ¶ç²˜è´´çš„å¤æ‚å·¥ä½œæµåŠç‰¹å®šä¼ä¸šåº”ç”¨åœºæ™¯ï¼Œå¹¶å¼•å…¥äº†å¤šåˆ†è¾¨ç‡æµ‹è¯•å’ŒåŸºäºåº”ç”¨çŠ¶æ€çš„è‡ªåŠ¨åŒ–éªŒè¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºäº”ç§é¡¶å°–æ¨¡å‹åœ¨ç®€å•ä»»åŠ¡ä¸­è¡¨ç°å°šå¯ï¼Œä½†åœ¨å¤æ‚å·¥ä½œæµä¸­é­é‡â€œèƒ½åŠ›æ‚¬å´–â€ï¼ŒæˆåŠŸç‡ä» 67-85% éª¤é™è‡³ 9-19%ã€‚ç ”ç©¶æŒ‡å‡ºè¿™ç§æ€§èƒ½æ–­å±‚æ­ç¤ºäº†æ™ºèƒ½ä½“åœ¨å†…å­˜ç®¡ç† (memory management)ã€å±‚æ¬¡åŒ–è§„åˆ’ (hierarchical planning) å’ŒçŠ¶æ€åè°ƒ (state coordination) æ–¹é¢çš„åº•å±‚æ¶æ„ç¼ºé™·ã€‚UI-CUBE ä½œä¸ºä¼ä¸šå°±ç»ªåº¦è¯Šæ–­å·¥å…·ï¼Œè¯æ˜äº†ç°æœ‰æ™ºèƒ½ä½“è™½èƒ½æ“ä½œå•ä¸ªç•Œé¢å…ƒç´ ï¼Œä½†å°šæ— æ³•èƒœä»»å¯é çš„å·¥ä½œæµè‡ªåŠ¨åŒ–ä»»åŠ¡ã€‚è¯¥åŸºå‡†æµ‹è¯•ä¸ºå¼€å‘å…·å¤‡ç®¡ç†å¤æ‚å¤šæ­¥ä¼ä¸šæµç¨‹èƒ½åŠ›çš„ç”Ÿäº§çº§æ™ºèƒ½ä½“æä¾›äº†é‡è¦çš„æ¶æ„å¯ç¤ºã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "18 pages, 8 figures, 5 tables. Benchmark comprising 226 tasks across two difficulty tiers. Code and benchmark available at https://github.com/UiPath/uipath_enterprise_benchmark",
      "pdf_url": "https://arxiv.org/pdf/2511.17131v1",
      "published_date": "2025-11-21 10:47:22 UTC",
      "updated_date": "2025-11-21 10:47:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:05:02.472712+00:00"
    },
    {
      "arxiv_id": "2511.17129v2",
      "title": "Learning to Compress: Unlocking the Potential of Large Language Models for Text Representation",
      "title_zh": "å­¦ä¹ å‹ç¼©ï¼šé‡Šæ”¾å¤§è¯­è¨€æ¨¡å‹åœ¨æ–‡æœ¬è¡¨ç¤ºé¢†åŸŸçš„æ½œåŠ›",
      "authors": [
        "Yeqin Zhang",
        "Yizheng Zhao",
        "Chen Hu",
        "Binxing Jiao",
        "Daxin Jiang",
        "Ruihang Miao",
        "Cam-Tu Nguyen"
      ],
      "abstract": "Text representation plays a critical role in tasks like clustering, retrieval, and other downstream applications. With the emergence of large language models (LLMs), there is increasing interest in harnessing their capabilities for this purpose. However, most of the LLMs are inherently causal and optimized for next-token prediction, making them suboptimal for producing holistic representations. To address this, recent studies introduced pretext tasks to adapt LLMs for text representation. Most of these tasks, however, rely on token-level prediction objectives, such as the masked next-token prediction (MNTP) used in LLM2Vec. In this work, we explore the untapped potential of context compression as a pretext task for unsupervised adaptation of LLMs. During compression pre-training, the model learns to generate compact memory tokens, which substitute the whole context for downstream sequence prediction. Experiments demonstrate that a well-designed compression objective can significantly enhance LLM-based text representations, outperforming models trained with token-level pretext tasks. Further improvements through contrastive learning produce a strong representation model (LLM2Comp) that outperforms contemporary LLM-based text encoders on a wide range of tasks while being more sample-efficient, requiring significantly less training data. Code is available at https://github.com/longtaizi13579/LLM2Comp.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ—¨åœ¨æŒ–æ˜å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ–‡æœ¬è¡¨ç¤ºé¢†åŸŸçš„æ½œåŠ›ï¼Œé’ˆå¯¹LLMså› å…¶å› æœæ€§(causal)æ¶æ„åœ¨ç”Ÿæˆå…¨å±€è¡¨ç¤ºæ–¹é¢çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§å…¨æ–°çš„æ— ç›‘ç£é€‚åº”æ–¹æ¡ˆã€‚ç ”ç©¶äººå‘˜å°†ä¸Šä¸‹æ–‡å‹ç¼©(context compression)ä½œä¸ºé¢„è®­ç»ƒä»»åŠ¡ï¼Œä½¿æ¨¡å‹å­¦ä¹ ç”Ÿæˆèƒ½å¤Ÿæ›¿ä»£å®Œæ•´ä¸Šä¸‹æ–‡çš„ç´§å‡‘è®°å¿†æ ‡è®°(memory tokens)ã€‚å®éªŒç»“æœè¯æ˜ï¼Œç›¸æ¯”äºLLM2Vecç­‰é‡‡ç”¨çš„æ ‡è®°çº§é¢„æµ‹(token-level prediction)ä»»åŠ¡ï¼Œå‹ç¼©ç›®æ ‡èƒ½æ›´æœ‰æ•ˆåœ°æå‡æ–‡æœ¬è¡¨ç¤ºè´¨é‡ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶é€šè¿‡å¯¹æ¯”å­¦ä¹ (contrastive learning)æ„å»ºäº†LLM2Compæ¨¡å‹ï¼Œåœ¨å¹¿æ³›çš„ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºç°æœ‰çš„åŸºäºLLMçš„ç¼–ç å™¨ã€‚è¯¥æ–¹æ³•è¡¨ç°å‡ºæé«˜çš„æ ·æœ¬æ•ˆç‡ï¼Œèƒ½å¤Ÿåœ¨è®­ç»ƒæ•°æ®æ˜¾è‘—å‡å°‘çš„æƒ…å†µä¸‹ä¿æŒå“è¶Šçš„æ€§èƒ½è¡¨ç°ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by AAAI'26",
      "pdf_url": "https://arxiv.org/pdf/2511.17129v2",
      "published_date": "2025-11-21 10:45:44 UTC",
      "updated_date": "2025-12-24 07:50:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:05:38.373881+00:00"
    },
    {
      "arxiv_id": "2511.17127v2",
      "title": "Training Foundation Models on a Full-Stack AMD Platform: Compute, Networking, and System Design",
      "title_zh": "åŸºäºå…¨æ ˆ AMD å¹³å°çš„åŸºç¡€æ¨¡å‹è®­ç»ƒï¼šè®¡ç®—ã€ç½‘ç»œä¸ç³»ç»Ÿè®¾è®¡",
      "authors": [
        "Quentin Anthony",
        "Yury Tokpanov",
        "Skyler Szot",
        "Srivatsan Rajagopal",
        "Praneeth Medepalli",
        "Anna Golubeva",
        "Vasu Shyam",
        "Robert Washbourne",
        "Rishi Iyer",
        "Ansh Chaurasia",
        "Tomas Figliolia",
        "Xiao Yang",
        "Abhinav Sarje",
        "Drew Thorstensen",
        "Amartey Pearson",
        "Zack Grossbart",
        "Jason van Patten",
        "Emad Barsoum",
        "Zhenyu Gu",
        "Yao Fu",
        "Beren Millidge"
      ],
      "abstract": "We report on the first large-scale mixture-of-experts (MoE) pretraining study on pure AMD hardware, utilizing both MI300X GPUs and Pollara networking. We distill practical guidance for both systems and model design. On the systems side, we deliver a comprehensive cluster and networking characterization: microbenchmarks for all core collectives (all-reduce, reduce-scatter, all-gather, broadcast) across message sizes and GPU counts over Pollara. To our knowledge, this is the first at this scale. We further provide MI300X microbenchmarks on kernel sizing and memory bandwidth to inform model design. On the modeling side, we introduce and apply MI300X-aware transformer sizing rules for attention and MLP blocks and justify MoE widths that jointly optimize training throughput and inference latency. We describe our training stack in depth, including often-ignored utilities such as fault-tolerance and checkpoint-reshaping, as well as detailed information on our training recipe. We also provide a preview of our model architecture and base model - ZAYA1 (760M active, 8.3B total parameters MoE, available at https://huggingface.co/Zyphra/ZAYA1-base) - which will be further improved upon in forthcoming papers. ZAYA1-base achieves performance comparable to leading base models such as Qwen3-4B and Gemma3-12B at its scale and larger, and outperforms models including Llama-3-8B and OLMoE across reasoning, mathematics, and coding benchmarks. Together, these results demonstrate that the AMD hardware, network, and software stack are mature and optimized enough for competitive large-scale pretraining.",
      "tldr_zh": "è¯¥ç ”ç©¶æŠ¥é“äº†é¦–æ¬¡åœ¨çº¯ AMD ç¡¬ä»¶ï¼ˆMI300X GPU å’Œ Pollara ç½‘ç»œï¼‰ä¸Šè¿›è¡Œçš„å¤§è§„æ¨¡æ··åˆä¸“å®¶æ¨¡å‹ (MoE) é¢„è®­ç»ƒç ”ç©¶ï¼Œå¹¶ä¸ºç³»ç»Ÿå’Œæ¨¡å‹è®¾è®¡æä¾›äº†å®è·µæŒ‡å¯¼ã€‚åœ¨ç³»ç»Ÿå±‚é¢ï¼Œå›¢é˜Ÿæä¾›äº†è¯¦å°½çš„é›†ç¾¤ä¸ç½‘ç»œç‰¹æ€§åˆ†æï¼ŒåŒ…æ‹¬é’ˆå¯¹æ ¸å¿ƒé›†åˆé€šä¿¡ (collectives) çš„å¾®åŸºå‡†æµ‹è¯•ä»¥åŠ MI300X çš„å†…å­˜å¸¦å®½è¯„ä¼°ã€‚æ¨¡å‹å±‚é¢åˆ™å¼•å…¥äº† MI300X æ„ŸçŸ¥çš„ Transformer å°ºå¯¸è§„åˆ™ï¼Œé€šè¿‡ä¼˜åŒ– MoE å®½åº¦æ¥å…¼é¡¾è®­ç»ƒååé‡ä¸æ¨ç†å»¶è¿Ÿï¼Œå¹¶è¯¦ç»†é˜è¿°äº†åŒ…å«å®¹é”™æœºåˆ¶ (fault-tolerance) åœ¨å†…çš„å®Œæ•´è®­ç»ƒæŠ€æœ¯æ ˆã€‚ç ”ç©¶å‘å¸ƒçš„åŸºåº§æ¨¡å‹ ZAYA1ï¼ˆ7.6äº¿æ¿€æ´»å‚æ•°ï¼Œ83äº¿æ€»å‚æ•°ï¼‰åœ¨æ¨ç†ã€æ•°å­¦å’Œä»£ç åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ€§èƒ½å¯åª²ç¾ Qwen3-4B å’Œ Gemma3-12B ç­‰é¢†å…ˆæ¨¡å‹ã€‚è¯¥æˆæœæœ‰åŠ›è¯æ˜äº† AMD çš„ç¡¬ä»¶ã€ç½‘ç»œåŠè½¯ä»¶æ ˆå·²å…·å¤‡æ‰§è¡Œå¤§è§„æ¨¡ç«äº‰æ€§é¢„è®­ç»ƒä»»åŠ¡çš„æˆç†Ÿåº¦ä¸ä¼˜åŒ–æ°´å¹³ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17127v2",
      "published_date": "2025-11-21 10:44:02 UTC",
      "updated_date": "2025-12-03 19:04:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:05:08.789514+00:00"
    },
    {
      "arxiv_id": "2511.17113v1",
      "title": "AutoGraphAD: A novel approach using Variational Graph Autoencoders for anomalous network flow detection",
      "title_zh": "AutoGraphADï¼šä¸€ç§åŸºäºå˜åˆ†å›¾è‡ªç¼–ç å™¨çš„å¼‚å¸¸ç½‘ç»œæµæ£€æµ‹æ–°æ–¹æ³•",
      "authors": [
        "Georgios Anyfantis",
        "Pere Barlet-Ros"
      ],
      "abstract": "Network Intrusion Detection Systems (NIDS) are essential tools for detecting network attacks and intrusions. While extensive research has explored the use of supervised Machine Learning for attack detection and characterisation, these methods require accurately labelled datasets, which are very costly to obtain. Moreover, existing public datasets have limited and/or outdated attacks, and many of them suffer from mislabelled data. To reduce the reliance on labelled data, we propose AutoGraphAD, a novel unsupervised anomaly detection approach based on a Heterogeneous Variational Graph Autoencoder. AutoGraphAD operates on heterogeneous graphs, made from connection and IP nodes that capture network activity within a time window. The model is trained using unsupervised and contrastive learning, without relying on any labelled data. The reconstruction, structural loss, and KL divergence are then weighted and combined in an anomaly score that is then used for anomaly detection. Overall, AutoGraphAD yields the same, and in some cases better, results than previous unsupervised approaches, such as Anomal-E, but without requiring costly downstream anomaly detectors. As a result, AutoGraphAD achieves around 1.18 orders of magnitude faster training and 1.03 orders of magnitude faster inference, which represents a significant advantage for operational deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AutoGraphADï¼Œä¸€ç§åŸºäºå¼‚æ„å˜åˆ†å›¾è‡ªç¼–ç å™¨ (Heterogeneous Variational Graph Autoencoder) çš„æ–°å‹æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹æ–¹æ³•ï¼Œæ—¨åœ¨å‡å°‘ç½‘ç»œå…¥ä¾µæ£€æµ‹ç³»ç»Ÿ (NIDS) å¯¹é«˜æˆæœ¬æ ‡ç­¾æ•°æ®çš„ä¾èµ–ã€‚è¯¥æ¨¡å‹é€šè¿‡åœ¨ç‰¹å®šæ—¶é—´çª—å£å†…æ„å»ºåŒ…å«è¿æ¥ (connection) å’Œ IP èŠ‚ç‚¹çš„å¼‚æ„å›¾æ¥æ•æ‰ç½‘ç»œæ´»åŠ¨ï¼Œå¹¶ç»“åˆæ— ç›‘ç£å­¦ä¹ ä¸å¯¹æ¯”å­¦ä¹  (contrastive learning) è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚ç³»ç»Ÿé€šè¿‡åŠ æƒæ•´åˆé‡æ„æŸå¤± (reconstruction loss)ã€ç»“æ„æŸå¤± (structural loss) å’Œ KL æ•£åº¦ (KL divergence) æ¥è®¡ç®—å¼‚å¸¸è¯„åˆ†ï¼Œä»è€Œå®ç°ç²¾å‡†çš„æµé‡æ£€æµ‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAutoGraphAD çš„æ€§èƒ½è¾¾åˆ°ç”šè‡³ä¼˜äº Anomal-E ç­‰ç°æœ‰æ— ç›‘ç£æ–¹æ³•ï¼Œä¸”æ— éœ€é¢å¤–çš„ä¸‹æ¸¸å¼‚å¸¸æ£€æµ‹å™¨ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦ä¸Šåˆ†åˆ«å®ç°äº†çº¦ 1.18 å’Œ 1.03 ä¸ªæ•°é‡çº§çš„æå‡ï¼Œä¸ºç½‘ç»œå®‰å…¨é¢†åŸŸçš„å®æ—¶è¿è¡Œä¸éƒ¨ç½²æä¾›äº†æ˜¾è‘—çš„æŠ€æœ¯ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "11 pages, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.17113v1",
      "published_date": "2025-11-21 10:22:00 UTC",
      "updated_date": "2025-11-21 10:22:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:05:21.468875+00:00"
    },
    {
      "arxiv_id": "2601.11533v1",
      "title": "Artificial Intelligence as a Training Tool in Clinical Psychology: A Comparison of Text-Based and Avatar Simulations",
      "title_zh": "äººå·¥æ™ºèƒ½ä½œä¸ºä¸´åºŠå¿ƒç†å­¦çš„åŸ¹è®­å·¥å…·ï¼šæ–‡æœ¬æ¨¡æ‹Ÿä¸è™šæ‹Ÿäººæ¨¡æ‹Ÿçš„å¯¹æ¯”ç ”ç©¶",
      "authors": [
        "V. El Sawah",
        "A. Bhardwaj",
        "A. Pryke-Hobbes",
        "D. Gamaleldin",
        "C. S. Ang",
        "A. K. Martin"
      ],
      "abstract": "Clinical psychology students frequently report feeling underprepared for the interpersonal demands of therapeutic work, highlighting the need for accessible opportunities to practise core counselling skills before seeing real clients. Advances in artificial intelligence (AI) now enable simulated interaction partners that may support early skills development. This study examined postgraduate clinical psychology students' perceptions of two AI-based simulations: a text-based chatbot (ChatGPT) and a voice-based avatar (HeyGen). Twenty-four students completed two brief cognitive-behavioural role-plays (counterbalanced), one with each tool, and provided both quantitative ratings and qualitative feedback on perceived usefulness, skill application, responsiveness and engagement, and perceived skill improvement. Both AI tools were evaluated positively across dimensions. However, the avatar was rated significantly higher than the chatbot for perceived usefulness, skill application, and perceived skill improvement, and qualitative comments highlighted the added value of voice-based interaction for conveying social and emotional cues. These findings suggest that AI-driven simulation may supplement early-stage clinical skills training, with voice-based avatars offering additional benefits. Future work should test whether such simulated interactions translate to objective improvements in real therapeutic performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººå·¥æ™ºèƒ½ (AI) ä½œä¸ºä¸´åºŠå¿ƒç†å­¦åŸ¹è®­å·¥å…·çš„æ½œåŠ›ï¼Œæ—¨åœ¨è§£å†³å­¦ç”Ÿåœ¨é¢å¯¹çœŸå®æ¥è®¿è€…å‰ç¼ºä¹æ ¸å¿ƒå’¨è¯¢æŠ€èƒ½ç»ƒä¹ æœºä¼šçš„é—®é¢˜ã€‚ç ”ç©¶å¯¹æ¯”äº†ç ”ç©¶ç”Ÿå¯¹åŸºäºæ–‡æœ¬çš„èŠå¤©æœºå™¨äºº (ChatGPT) å’ŒåŸºäºè¯­éŸ³çš„æ•°å­—äººå½¢è±¡ (HeyGen) è¿™ä¸¤ç§ AI æ¨¡æ‹Ÿå·¥å…·çš„æ„ŸçŸ¥å·®å¼‚ã€‚24åå‚ä¸è€…å®Œæˆäº†è®¤çŸ¥è¡Œä¸ºç–—æ³• (Cognitive-Behavioural Therapy) è§’è‰²æ‰®æ¼”å®éªŒï¼Œå¹¶é’ˆå¯¹æœ‰ç”¨æ€§ã€æŠ€èƒ½åº”ç”¨å’Œå‚ä¸æ„Ÿç­‰å¤šç»´åº¦è¿›è¡Œäº†å®šé‡ä¸å®šæ€§è¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶ä¸¤ç§ AI å·¥å…·å‡è·å¾—ç§¯æè¯„ä»·ï¼Œä½†è¯­éŸ³é©±åŠ¨çš„è™šæ‹Ÿå½¢è±¡åœ¨æ„ŸçŸ¥æœ‰ç”¨æ€§å’ŒæŠ€èƒ½æå‡æ„Ÿæ–¹é¢çš„è¡¨ç°æ˜¾è‘—ä¼˜äºæ–‡æœ¬æœºå™¨äººã€‚å®šæ€§åé¦ˆè¿›ä¸€æ­¥æ­ç¤ºäº†è¯­éŸ³äº’åŠ¨åœ¨ä¼ é€’ç¤¾äº¤ä¸æƒ…æ„Ÿçº¿ç´¢æ–¹é¢çš„ç‹¬ç‰¹ä»·å€¼ã€‚è¯¥ç ”ç©¶è¯æ˜ AI æ¨¡æ‹Ÿå¯æœ‰æ•ˆè¡¥å……æ—©æœŸä¸´åºŠæŠ€èƒ½åŸ¹è®­ï¼Œä¸”è¯­éŸ³äº¤äº’å½¢å¼æ›´å…·ä¼˜åŠ¿ï¼Œä¸ºæœªæ¥æå‡å®é™…æ²»ç–—è¡¨ç°çš„æ¨¡æ‹Ÿæ•™å­¦å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "38 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.11533v1",
      "published_date": "2025-11-21 10:09:20 UTC",
      "updated_date": "2025-11-21 10:09:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:05:52.740035+00:00"
    },
    {
      "arxiv_id": "2511.17100v2",
      "title": "Geometric-disentangelment Unlearning",
      "title_zh": "å‡ ä½•è§£è€¦é—å¿˜",
      "authors": [
        "Duo Zhou",
        "Yuji Zhang",
        "Tianxin Wei",
        "Ruizhong Qiu",
        "Ke Yang",
        "Xiao Lin",
        "Cheng Qian",
        "Jingrui He",
        "Hanghang Tong",
        "Heng Ji",
        "Huan Zhang"
      ],
      "abstract": "Machine unlearning, the removal of a training subset's influence from a deployed model, is critical for privacy preservation and model reliability, yet gradient ascent on forget samples often harms retained knowledge. Existing approaches face a persistent tradeoff between effective forgetting and preservation on the retain set. While previous methods provide useful heuristics, they often lack a formal analysis on how exactly forgetting updates harm retained knowledge, and whether the side effects can be removed with theoretical guarantees. To explore a theoretically sound and simple solution, we start from the first principle on how performance on the retain set is actually affected: a first-order analysis of the local change of the retain loss under small parameter updates during model training. We start from a crisp equivalence: the retain loss is unchanged to first order iff the update direction is orthogonal to the subspace spanned by retain gradients (\"retain-invariant\"). This identifies the entangled component as the tangential part of forget update within the retain-gradient subspace, and characterizes disentanglement as orthogonality. Guided by this, we propose the Geometric-disentanglement Unlearning (GU) that decomposes any candidate forget gradient update into tangential and normal components to retain space and executes only the normal component. Under a standard trust-region budget, the projected direction aligned with the raw forget gradient is optimal among all first-order retain-invariant moves, and we also derive the optimal projected direction for joint forget-retain updating objectives. Our method is plug-and-play and can be attached to existing gradient-based unlearning procedures to mitigate side effects. GU achieves consistent improvement on various methods across three benchmarks TOFU, MUSE, and WMDP.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨é—å¿˜ (Machine Unlearning) è¿‡ç¨‹ä¸­æ¢¯åº¦ä¸Šå‡æ³•å®¹æ˜“æŸå®³ä¿ç•™çŸ¥è¯† (Retained Knowledge) çš„é—®é¢˜ï¼Œæå‡ºäº†å‡ ä½•è§£è€¦é—å¿˜ (Geometric-disentanglement Unlearning, GU) æ¡†æ¶ã€‚ä½œè€…é€šè¿‡ä¸€é˜¶åˆ†ææ­ç¤ºäº†ä¿ç•™æŸå¤±å˜åŒ–çš„æœ¬è´¨ï¼Œå³å‚æ•°æ›´æ–°æ–¹å‘å¿…é¡»ä¸ä¿ç•™æ¢¯åº¦ (Retain Gradients) æ„æˆçš„å­ç©ºé—´æ­£äº¤æ‰èƒ½ç¡®ä¿ä¿ç•™é›†æ€§èƒ½åœ¨ä¸€é˜¶æ„ä¹‰ä¸Šä¸å—å½±å“ã€‚åŸºäºæ­¤å‘ç°ï¼ŒGU å°†é—å¿˜æ¢¯åº¦æ›´æ–°åˆ†è§£ä¸ºç›¸å¯¹äºä¿ç•™ç©ºé—´çš„åˆ‡å‘å’Œæ³•å‘åˆ†é‡ï¼Œå¹¶ä»…æ‰§è¡Œæ­£äº¤çš„æ³•å‘åˆ†é‡ä»¥å®ç°å‡ ä½•è§£è€¦ã€‚åœ¨æ ‡å‡†ä¿¡èµ–åŸŸ (Trust-region) çº¦æŸä¸‹ï¼Œè¯¥æŠ•å½±æ–¹å‘è¢«è¯æ˜æ˜¯æ‰€æœ‰ä¸€é˜¶ä¿ç•™ä¸å˜ç§»åŠ¨ä¸­çš„æœ€ä¼˜é€‰æ‹©ã€‚GU å…·æœ‰å³æ’å³ç”¨çš„ç‰¹æ€§ï¼Œå¯ç›´æ¥åº”ç”¨äºç°æœ‰çš„æ¢¯åº¦é—å¿˜ç¨‹åºä»¥æœ‰æ•ˆå‡è½»å…¶å‰¯ä½œç”¨ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ TOFUã€MUSE å’Œ WMDP ç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡èƒ½æ˜¾è‘—ä¸”ä¸€è‡´åœ°æå‡å¤šç§é—å¿˜ç®—æ³•çš„æ€§èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "27 Pages",
      "pdf_url": "https://arxiv.org/pdf/2511.17100v2",
      "published_date": "2025-11-21 09:58:25 UTC",
      "updated_date": "2026-01-17 22:02:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:06:57.469693+00:00"
    },
    {
      "arxiv_id": "2511.21734v1",
      "title": "Asking LLMs to Verify First is Almost Free Lunch",
      "title_zh": "è®©å¤§è¯­è¨€æ¨¡å‹å…ˆè¡ŒéªŒè¯ï¼šè¿‘ä¹â€œå…è´¹çš„åˆé¤â€",
      "authors": [
        "Shiguang Wu",
        "Quanming Yao"
      ],
      "abstract": "To enhance the reasoning capabilities of Large Language Models (LLMs) without high costs of training, nor extensive test-time sampling, we introduce Verification-First (VF), a strategy that prompts models to verify a provided candidate answer, even a trivial or random one, before generating a solution. This approach triggers a \"reverse reasoning\" process that is cognitively easier and complementary to standard forward Chain-of-Thought (CoT), effectively invoking the model's critical thinking to reduce logical errors. We further generalize the VF strategy to Iter-VF, a sequential test-time scaling (TTS) method that iteratively cycles the verification-generation process using the model's previous answer. Extensive experiments across various benchmarks (from mathematical reasoning to coding and agentic tasks) and various LLMs (from open-source 1B to cutting-edge commercial ones) confirm that VF with random answer consistently outperforms standard CoT with minimal computational overhead, and Iter-VF outperforms existing TTS strategies.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Verification-First (VF) ç­–ç•¥ï¼Œæ—¨åœ¨ä¸å¢åŠ è®­ç»ƒæˆæœ¬æˆ–å¤§è§„æ¨¡æµ‹è¯•é‡‡æ ·çš„æƒ…å†µä¸‹æå‡å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å¯¼æ¨¡å‹åœ¨ç”Ÿæˆæ–¹æ¡ˆå‰å…ˆéªŒè¯ä¸€ä¸ªæä¾›çš„å€™é€‰ç­”æ¡ˆï¼Œè§¦å‘ä¸€ç§ä¸æ ‡å‡† Chain-of-Thought (CoT) äº’è¡¥ä¸”è®¤çŸ¥è´Ÿæ‹…æ›´è½»çš„â€œé€†å‘æ¨ç†â€è¿‡ç¨‹ï¼Œä»è€Œæœ‰æ•ˆå‡å°‘é€»è¾‘é”™è¯¯ã€‚ç ”ç©¶è¿˜è¿›ä¸€æ­¥æ¨å¹¿å‡º Iter-VFï¼Œè¿™æ˜¯ä¸€ç§é¡ºåºæµ‹è¯•æ—¶é—´ç¼©æ”¾ (Test-Time Scaling, TTS) æ–¹æ³•ï¼Œé€šè¿‡è¿­ä»£å¾ªç¯éªŒè¯ä¸ç”Ÿæˆè¿‡ç¨‹æ¥æŒç»­æ”¹è¿›æ¨¡å‹è¡¨ç°ã€‚åœ¨æ¶µç›–æ•°å­¦ã€ç¼–ç¨‹åŠæ™ºèƒ½ä½“ä»»åŠ¡çš„å¹¿æ³›å®éªŒä¸­ï¼Œä½¿ç”¨éšæœºç­”æ¡ˆçš„ VF åœ¨æä½é¢å¤–è®¡ç®—å¼€é”€ä¸‹å§‹ç»ˆä¼˜äºæ ‡å‡† CoTã€‚æ­¤å¤–ï¼ŒIter-VF åœ¨æ€§èƒ½ä¸Šä¹Ÿè¶…è¶Šäº†ç°æœ‰çš„æµ‹è¯•æ—¶é—´ç¼©æ”¾ç­–ç•¥ï¼Œè¯æ˜äº†éªŒè¯ä¼˜å…ˆç­–ç•¥åœ¨å„ç±»è§„æ¨¡æ¨¡å‹ä¸Šçš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21734v1",
      "published_date": "2025-11-21 09:55:34 UTC",
      "updated_date": "2025-11-21 09:55:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:06:27.933994+00:00"
    },
    {
      "arxiv_id": "2511.21733v1",
      "title": "RoSA: Enhancing Parameter-Efficient Fine-Tuning via RoPE-aware Selective Adaptation in Large Language Models",
      "title_zh": "RoSAï¼šé€šè¿‡æ„ŸçŸ¥ RoPE çš„é€‰æ‹©æ€§é€‚é…å¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„å‚æ•°é«˜æ•ˆå¾®è°ƒ",
      "authors": [
        "Dayan Pan",
        "Jingyuan Wang",
        "Yilong Zhou",
        "Jiawei Cheng",
        "Pengyue Jia",
        "Xiangyu Zhao"
      ],
      "abstract": "Fine-tuning large language models is essential for task-specific adaptation, yet it remains computationally prohibitive. Parameter-Efficient Fine-Tuning (PEFT) methods have emerged as a solution, but current approaches typically ignore the distinct roles of model components and the heterogeneous importance across layers, thereby limiting adaptation efficiency. Motivated by the observation that Rotary Position Embeddings (RoPE) induce critical activations in the low-frequency dimensions of attention states, we propose RoPE-aware Selective Adaptation (RoSA), a novel PEFT framework that allocates trainable parameters in a more targeted and effective manner. RoSA comprises a RoPE-aware Attention Enhancement (RoAE) module, which selectively enhances the low-frequency components of RoPE-influenced attention states, and a Dynamic Layer Selection (DLS) strategy that adaptively identifies and updates the most critical layers based on LayerNorm gradient norms. By combining dimension-wise enhancement with layer-wise adaptation, RoSA achieves more targeted and efficient fine-tuning. Extensive experiments on fifteen commonsense and arithmetic benchmarks demonstrate that RoSA outperforms existing mainstream PEFT methods under comparable trainable parameters. The code is available to ease reproducibility at https://github.com/Applied-Machine-Learning-Lab/RoSA.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† RoSA (RoPE-aware Selective Adaptation)ï¼Œä¸€ç§æ—¨åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹å‚æ•°é«˜æ•ˆå¾®è°ƒ (PEFT) æ•ˆç‡çš„æ–°å‹æ¡†æ¶ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•å¿½è§†æ¨¡å‹ç»„ä»¶å·®å¼‚åŠå±‚çº§é‡è¦æ€§åˆ†å¸ƒä¸å‡çš„é—®é¢˜ã€‚ç ”ç©¶è€…è§‚å¯Ÿåˆ°æ—‹è½¬ä½ç½®åµŒå…¥ (RoPE) ä¼šåœ¨æ³¨æ„åŠ›çŠ¶æ€çš„ä½é¢‘ç»´åº¦è¯±å‘å…³é”®æ¿€æ´»ï¼Œæ®æ­¤è®¾è®¡äº† RoPE-aware Attention Enhancement (RoAE) æ¨¡å—æ¥é€‰æ‹©æ€§åœ°å¢å¼ºè¿™äº›ç‰¹å®šç»„ä»¶ã€‚åŒæ—¶ï¼ŒRoSA å¼•å…¥äº†åŠ¨æ€å±‚é€‰æ‹© (Dynamic Layer Selection, DLS) ç­–ç•¥ï¼Œåˆ©ç”¨ LayerNorm æ¢¯åº¦èŒƒæ•°è‡ªé€‚åº”åœ°è¯†åˆ«å¹¶æ›´æ–°æ¨¡å‹ä¸­æœ€å…³é”®çš„å±‚ã€‚é€šè¿‡ç»“åˆç»´åº¦å±‚é¢çš„å¢å¼ºä¸å±‚çº§å±‚é¢çš„é€‚é…ï¼Œè¯¥æ¡†æ¶å®ç°äº†æ›´å…·é’ˆå¯¹æ€§çš„å‚æ•°åˆ†é…ä¸å¾®è°ƒã€‚åœ¨åäº”ä¸ªå¸¸è¯†æ¨ç†å’Œç®—æœ¯åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¯æ¯”çš„è®­ç»ƒå‚æ•°é‡ä¸‹ï¼ŒRoSA çš„æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„ä¸»æµ PEFT æ–¹æ³•ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by AAAI' 26",
      "pdf_url": "https://arxiv.org/pdf/2511.21733v1",
      "published_date": "2025-11-21 09:55:01 UTC",
      "updated_date": "2025-11-21 09:55:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:07:02.100671+00:00"
    },
    {
      "arxiv_id": "2511.20689v1",
      "title": "Morality in AI. A plea to embed morality in LLM architectures and frameworks",
      "title_zh": "äººå·¥æ™ºèƒ½ä¸­çš„é“å¾·ï¼šå…³äºåœ¨ LLM æ¶æ„ä¸æ¡†æ¶ä¸­åµŒå…¥é“å¾·æœºåˆ¶çš„å€¡è®®",
      "authors": [
        "Gunter Bombaerts",
        "Bram Delisse",
        "Uzay Kaymak"
      ],
      "abstract": "Large language models (LLMs) increasingly mediate human decision-making and behaviour. Ensuring LLM processing of moral meaning therefore has become a critical challenge. Current approaches rely predominantly on bottom-up methods such as fine-tuning and reinforcement learning from human feedback. We propose a fundamentally different approach: embedding moral meaning processing directly into the architectural mechanisms and frameworks of transformer-based models through top-down design principles. We first sketch a framework that conceptualizes attention as a dynamic interface mediating between structure and processing, contrasting with existing linear attention frameworks in psychology. We start from established biological-artificial attention analogies in neural architecture design to improve cognitive processing. We extend this analysis to moral processing, using Iris Murdoch's theory of loving attention (sustained, just observation that enables moral transformation by reseeing others with clarity and compassion) to philosophically discuss functional analogies between human and LLM moral processing. We formulate and evaluate potentially promising technical operationalizations to embed morality in LLM architectures and frameworks. We acknowledge the limitations of our exploration and give three key contributions. (1) We conceptualize attention as a dynamic system mechanism mediating between structure and processing. (2) Drawing on the Murdoch notion of loving attention, we outline technical pathways for embedding morality in LLMs, through modified training objectives, runtime weight adjustments, and architectural refinements to attention. (3) We argue that integrating morality into architectures and frameworks complements external, constraint-based methods. We conclude with a call for collaboration between transformer designers and philosophers engaged in AI ethics.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†é“å¾·æ„ä¹‰æ–¹é¢çš„æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºç›®å‰ä¾èµ–å¾®è°ƒ(Fine-tuning)å’Œäººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ (RLHF)ç­‰è‡ªä¸‹è€Œä¸Š(Bottom-up)æ–¹æ³•çš„å±€é™ã€‚è®ºæ–‡æå‡ºäº†ä¸€ç§æ ¹æœ¬æ€§çš„åˆ›æ–°æ–¹æ¡ˆï¼Œä¸»å¼ é€šè¿‡è‡ªä¸Šè€Œä¸‹(Top-down)çš„è®¾è®¡åŸåˆ™ï¼Œå°†é“å¾·å¤„ç†ç›´æ¥åµŒå…¥åˆ°Transformeræ¨¡å‹çš„æ¶æ„æœºåˆ¶ä¸æ¡†æ¶ä¸­ã€‚ä½œè€…å€Ÿé‰´äº†è‰¾é‡Œæ–¯Â·æ¢…é“(Iris Murdoch)çš„â€œä»æ…ˆæ³¨æ„åŠ›â€(loving attention)ç†è®ºï¼Œå°†æ³¨æ„åŠ›(Attention)é‡æ–°å®šä¹‰ä¸ºä»‹äºç»“æ„ä¸å¤„ç†ä¹‹é—´çš„åŠ¨æ€ä¸­ä»‹ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ¢è®¨äº†ä¿®æ”¹è®­ç»ƒç›®æ ‡ã€è¿è¡Œæ—¶æƒé‡è°ƒæ•´åŠæ³¨æ„åŠ›æ¶æ„ä¼˜åŒ–ç­‰å…·ä½“æŠ€æœ¯è·¯å¾„ï¼Œä»¥å®ç°é“å¾·ç»´åº¦çš„å†…ç”ŸåŒ–ã€‚ä½œè€…è®¤ä¸ºï¼Œè¿™ç§æ¶æ„å±‚é¢çš„é“å¾·æ•´åˆæ˜¯å¯¹å¤–éƒ¨çº¦æŸæ–¹æ³•çš„å¿…è¦è¡¥å……ï¼Œå¹¶æœ€åå‘¼åTransformeræ¶æ„è®¾è®¡å¸ˆä¸AIä¼¦ç†å­¦å®¶å±•å¼€æ·±åº¦åˆä½œã€‚",
      "categories": [
        "q-bio.NC",
        "cs.AI"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20689v1",
      "published_date": "2025-11-21 09:53:43 UTC",
      "updated_date": "2025-11-21 09:53:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:06:04.133602+00:00"
    },
    {
      "arxiv_id": "2511.21732v1",
      "title": "HUMORCHAIN: Theory-Guided Multi-Stage Reasoning for Interpretable Multimodal Humor Generation",
      "title_zh": "HUMORCHAINï¼šé¢å‘å¯è§£é‡Šå¤šæ¨¡æ€å¹½é»˜ç”Ÿæˆçš„ç†è®ºå¼•å¯¼å¤šé˜¶æ®µæ¨ç†",
      "authors": [
        "Jiajun Zhang",
        "Shijia Luo",
        "Ruikang Zhang",
        "Qi Su"
      ],
      "abstract": "Humor, as both a creative human activity and a social binding mechanism, has long posed a major challenge for AI generation. Although producing humor requires complex cognitive reasoning and social understanding, theories of humor suggest that it follows learnable patterns and structures, making it theoretically possible for generative models to acquire them implicitly. In recent years, multimodal humor has become a prevalent form of online communication, especially among Gen Z, highlighting the need for AI systems capable of integrating visual understanding with humorous language generation. However, existing data-driven approaches lack explicit modeling or theoretical grounding of humor, often producing literal descriptions that fail to capture its underlying cognitive mechanisms, resulting in the generated image descriptions that are fluent but lack genuine humor or cognitive depth. To address this limitation, we propose HUMORCHAIN (HUmor-guided Multi-step Orchestrated Reasoning Chain for Image Captioning), a theory-guided multi-stage reasoning framework. It integrates visual semantic parsing, humor- and psychology-based reasoning, and a fine-tuned discriminator for humor evaluation, forming an interpretable and controllable cognitive reasoning chain. To the best of our knowledge, this is the first work to explicitly embed cognitive structures from humor theories into multimodal humor generation, enabling a structured reasoning process from visual understanding to humor creation. Experiments on Meme-Image-No-Text, Oogiri-GO, and OxfordTVG-HIC datasets show that HUMORCHAIN outperforms state-of-the-art baselines in human humor preference, Elo/BT scores, and semantic diversity, demonstrating that theory-driven structured reasoning enables large language models to generate humor aligned with human perception.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† HUMORCHAINï¼Œä¸€ä¸ªç†è®ºæŒ‡å¯¼çš„å¤šé˜¶æ®µæ¨ç†æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¹½é»˜ç”Ÿæˆä¸­ç¼ºä¹æ˜¾å¼å»ºæ¨¡å’Œç†è®ºåŸºç¡€çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é›†æˆäº†è§†è§‰è¯­ä¹‰è§£æ(visual semantic parsing)ã€åŸºäºå¹½é»˜ä¸å¿ƒç†å­¦çš„æ¨ç†(humor- and psychology-based reasoning)ä»¥åŠå¾®è°ƒåçš„åˆ¤åˆ«å™¨(fine-tuned discriminator)ï¼Œå½¢æˆäº†ä¸€æ¡å¯è§£é‡Šä¸”å¯æ§çš„è®¤çŸ¥æ¨ç†é“¾ã€‚ä½œä¸ºé¦–ä¸ªå°†å¹½é»˜ç†è®ºè®¤çŸ¥ç»“æ„æ˜¾å¼åµŒå…¥å¤šæ¨¡æ€ç”Ÿæˆçš„å·¥ä½œï¼Œå®ƒå®ç°äº†ä»è§†è§‰ç†è§£åˆ°å¹½é»˜åˆ›ä½œçš„ç»“æ„åŒ–æ¨ç†è¿‡ç¨‹ã€‚åœ¨ Meme-Image-No-Textã€Oogiri-GO å’Œ OxfordTVG-HIC æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒHUMORCHAIN åœ¨äººç±»å¹½é»˜åå¥½ã€Elo/BT è¯„åˆ†ä»¥åŠè¯­ä¹‰å¤šæ ·æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„åŸºçº¿æ¨¡å‹ã€‚è¿™é¡¹ç ”ç©¶è¯æ˜äº†ç†è®ºé©±åŠ¨çš„ç»“æ„åŒ–æ¨ç†èƒ½æœ‰æ•ˆæå‡å¤§è¯­è¨€æ¨¡å‹(LLMs)ç”Ÿæˆç¬¦åˆäººç±»æ„ŸçŸ¥å¹½é»˜çš„èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21732v1",
      "published_date": "2025-11-21 09:52:46 UTC",
      "updated_date": "2025-11-21 09:52:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:06:06.631976+00:00"
    },
    {
      "arxiv_id": "2511.17683v1",
      "title": "Datacenters in the Desert: Feasibility and Sustainability of LLM Inference in the Middle East",
      "title_zh": "æ²™æ¼ æ•°æ®ä¸­å¿ƒï¼šä¸­ä¸œåœ°åŒº LLM æ¨ç†çš„å¯è¡Œæ€§ä¸å¯æŒç»­æ€§ç ”ç©¶",
      "authors": [
        "Lara Hassan",
        "Mohamed ElZeftawy",
        "Abdulrahman Mahmoud"
      ],
      "abstract": "As the Middle East emerges as a strategic hub for artificial intelligence (AI) infrastructure, the feasibility of deploying sustainable datacenters in desert environments has become a topic of growing relevance. This paper presents an empirical study analyzing the energy consumption and carbon footprint of large language model (LLM) inference across four countries: the United Arab Emirates, Iceland, Germany, and the United States of America using DeepSeek Coder 1.3B and the HumanEval dataset on the task of code generation. We use the CodeCarbon library to track energy and carbon emissions andcompare geographical trade-offs for climate-aware AI deployment. Our findings highlight both the challenges and potential of datacenters in desert regions and provide a balanced outlook on their role in global AI expansion.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨ä¸­ä¸œæ²™æ¼ åœ°åŒºéƒ¨ç½²å¯æŒç»­æ•°æ®ä¸­å¿ƒçš„å¯è¡Œæ€§åŠå…¶å¯¹ç¯å¢ƒçš„å½±å“ï¼ŒèƒŒæ™¯æ˜¯è¯¥åœ°åŒºæ­£é€æ¸æˆä¸ºäººå·¥æ™ºèƒ½(AI)åŸºç¡€è®¾æ–½çš„æˆ˜ç•¥æ¢çº½ã€‚è®ºæ–‡é€šè¿‡å®è¯ç ”ç©¶åˆ†æäº†å¤§è¯­è¨€æ¨¡å‹(LLM)æ¨ç†è¿‡ç¨‹ä¸­çš„èƒ½æºæ¶ˆè€—å’Œç¢³è¶³è¿¹ï¼Œå¹¶å¯¹é˜¿æ‹‰ä¼¯è”åˆé…‹é•¿å›½ã€å†°å²›ã€å¾·å›½å’Œç¾å›½è¿™å››ä¸ªå›½å®¶çš„è¡¨ç°è¿›è¡Œäº†åœ°ç†å·®å¼‚å¯¹æ¯”ã€‚å®éªŒå…·ä½“é‡‡ç”¨ DeepSeek Coder 1.3B æ¨¡å‹åœ¨ HumanEval æ•°æ®é›†ä¸Šæ‰§è¡Œä»£ç ç”Ÿæˆä»»åŠ¡ï¼Œå¹¶åˆ©ç”¨ CodeCarbon åº“å®æ—¶è¿½è¸ªç”µåŠ›ä½¿ç”¨ä¸ç¢³æ’æ”¾æ•°æ®ã€‚ç ”ç©¶æ·±å…¥è¯„ä¼°äº†ä¸åŒåœ°ç†ç¯å¢ƒåœ¨æ°”å€™æ„è¯†å‹ AI éƒ¨ç½²ä¸­çš„æƒè¡¡å…³ç³»ï¼Œæ­ç¤ºäº†æ²™æ¼ åœ°åŒºæ•°æ®ä¸­å¿ƒé¢ä¸´çš„ç‹¬ç‰¹æŒ‘æˆ˜ä¸å‘å±•æ½œåŠ›ã€‚è¯¥ç ”ç©¶ä¸ºå…¨çƒ AI åŸºç¡€è®¾æ–½æ‰©å¼ èƒŒæ™¯ä¸‹ï¼Œæ²™æ¼ åœ°åŒºåœ¨å¯æŒç»­è®¡ç®—é¢†åŸŸæ‰€æ‰®æ¼”çš„è§’è‰²æä¾›äº†å¹³è¡¡çš„ç§‘å­¦è§è§£ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "3 pages, 1 figure",
      "pdf_url": "https://arxiv.org/pdf/2511.17683v1",
      "published_date": "2025-11-21 09:48:44 UTC",
      "updated_date": "2025-11-21 09:48:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:06:16.230216+00:00"
    },
    {
      "arxiv_id": "2511.19457v1",
      "title": "SparOA: Sparse and Operator-aware Hybrid Scheduling for Edge DNN Inference",
      "title_zh": "SparOAï¼šé¢å‘è¾¹ç¼˜ DNN æ¨ç†çš„ç¨€ç–æ€§ä¸ç®—å­æ„ŸçŸ¥æ··åˆè°ƒåº¦",
      "authors": [
        "Ziyang Zhang",
        "Jie Liu",
        "Luca Mottola"
      ],
      "abstract": "The resource demands of deep neural network (DNN) models introduce significant performance challenges, especially when deployed on resource-constrained edge devices. Existing solutions like model compression often sacrifice accuracy, while specialized hardware remains costly and inflexible. Hybrid inference methods, however, typically overlook how operator characteristics impact performance. In this work, we present SparOA, a CPU-GPU hybrid inference framework, which leverages both sparsity and computational intensity to optimize operator scheduling. SparOA embraces aforementioned challenges through three key components: (1) a threshold predictor that accurately determines optimal sparsity and computational intensity thresholds; (2) a reinforcement learning-based scheduler that dynamically optimizes resource allocation based on real-time hardware states; and (3) a hybrid inference engine that enhances efficiency through asynchronous execution and batch size optimization.Extensive results show that SparOA achieves an average speedup of 1.22-1.31x compared to all baselines, and outperforms the CPU-Only by up to 50.7x. Also, SparOA achieves optimal energy-per-inference, consuming 7\\%-16\\% less energy than the SOTA co-execution baseline.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¾¹ç¼˜è®¾å¤‡ä¸Šæ·±åº¦ç¥ç»ç½‘ç»œ (DNN) æ¨ç†é¢ä¸´çš„æ€§èƒ½ç“¶é¢ˆï¼Œæå‡ºäº† SparOAï¼Œä¸€ç§ç»“åˆç¨€ç–æ€§ä¸ç®—å­è®¡ç®—å¼ºåº¦ä¼˜åŒ–è°ƒåº¦çš„ CPU-GPU æ··åˆæ¨ç†æ¡†æ¶ã€‚SparOA é€šè¿‡ Threshold Predictor å‡†ç¡®ç¡®å®šæœ€ä¼˜çš„ç¨€ç–æ€§ä¸è®¡ç®—å¼ºåº¦é˜ˆå€¼ï¼Œå¹¶åˆ©ç”¨åŸºäºå¼ºåŒ–å­¦ä¹  (Reinforcement Learning) çš„è°ƒåº¦å™¨æ ¹æ®å®æ—¶ç¡¬ä»¶çŠ¶æ€åŠ¨æ€è°ƒæ•´èµ„æºåˆ†é…ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶çš„æ··åˆæ¨ç†å¼•æ“é€šè¿‡å¼‚æ­¥æ‰§è¡Œå’Œ Batch Size ä¼˜åŒ–ï¼Œæ˜¾è‘—æå‡äº†ç®—å­çš„å¤„ç†æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSparOA ç›¸æ¯”ç°æœ‰åŸºçº¿æ¨¡å‹å®ç°äº† 1.22-1.31 å€çš„å¹³å‡åŠ é€Ÿï¼Œä¸”æ¯” CPU-Only æ¨¡å¼æœ€é«˜æé€Ÿ 50.7 å€ã€‚åŒæ—¶ï¼Œåœ¨èƒ½è€—è¡¨ç°ä¸Šï¼ŒSparOA æ¯”æœ€å…ˆè¿›çš„ååŒæ‰§è¡Œæ–¹æ¡ˆé™ä½äº† 7%-16% çš„å•æ¬¡æ¨ç†èƒ½è€—ï¼Œæœ‰æ•ˆè§£å†³äº†è¾¹ç¼˜ç«¯é«˜æ€§èƒ½æ¨ç†ä¸èµ„æºå—é™ä¹‹é—´çš„çŸ›ç›¾ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "14 pages, 12 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.19457v1",
      "published_date": "2025-11-21 09:45:28 UTC",
      "updated_date": "2025-11-21 09:45:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:06:30.632546+00:00"
    },
    {
      "arxiv_id": "2511.17089v1",
      "title": "Spanning Tree Autoregressive Visual Generation",
      "title_zh": "ç”Ÿæˆæ ‘è‡ªå›å½’è§†è§‰ç”Ÿæˆ",
      "authors": [
        "Sangkyu Lee",
        "Changho Lee",
        "Janghoon Han",
        "Hosung Song",
        "Tackgeun You",
        "Hwasup Lim",
        "Stanley Jungkyu Choi",
        "Honglak Lee",
        "Youngjae Yu"
      ],
      "abstract": "We present Spanning Tree Autoregressive (STAR) modeling, which can incorporate prior knowledge of images, such as center bias and locality, to maintain sampling performance while also providing sufficiently flexible sequence orders to accommodate image editing at inference. Approaches that expose randomly permuted sequence orders to conventional autoregressive (AR) models in visual generation for bidirectional context either suffer from a decline in performance or compromise the flexibility in sequence order choice at inference. Instead, STAR utilizes traversal orders of uniform spanning trees sampled in a lattice defined by the positions of image patches. Traversal orders are obtained through breadth-first search, allowing us to efficiently construct a spanning tree whose traversal order ensures that the connected partial observation of the image appears as a prefix in the sequence through rejection sampling. Through the tailored yet structured randomized strategy compared to random permutation, STAR preserves the capability of postfix completion while maintaining sampling performance without any significant changes to the model architecture widely adopted in the language AR modeling.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†åä¸º Spanning Tree Autoregressive (STAR) çš„å»ºæ¨¡æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³è§†è§‰ç”Ÿæˆä¸­ä¼ ç»Ÿè‡ªå›å½’ (Autoregressive) æ¨¡å‹åœ¨åºåˆ—çµæ´»æ€§ä¸é‡‡æ ·æ€§èƒ½ä¹‹é—´éš¾ä»¥å…¼é¡¾çš„é—®é¢˜ã€‚STAR åˆ©ç”¨åœ¨å›¾åƒå—å®šä¹‰çš„æ ¼ç‚¹ä¸­é‡‡æ ·çš„å‡åŒ€ç”Ÿæˆæ ‘ (Uniform Spanning Trees) éå†é¡ºåºï¼Œå¹¶é€šè¿‡å¹¿åº¦ä¼˜å…ˆæœç´¢ (Breadth-First Search) æ„å»ºåºåˆ—ï¼Œä»è€Œèå…¥ä¸­å¿ƒåç½® (Center Bias) å’Œå±€éƒ¨æ€§ (Locality) ç­‰å›¾åƒå…ˆéªŒçŸ¥è¯†ã€‚è¯¥æ–¹æ³•é‡‡ç”¨æ‹’ç»é‡‡æ · (Rejection Sampling) æŠ€æœ¯ï¼Œç¡®ä¿è¿é€šçš„å±€éƒ¨è§‚æµ‹å›¾åƒèƒ½å¤Ÿä½œä¸ºåºåˆ—å‰ç¼€å‡ºç°ï¼Œæå‡äº†æ¨¡å‹åœ¨æ¨ç†é˜¶æ®µè¿›è¡Œå›¾åƒç¼–è¾‘æ—¶çš„é¡ºåºé€‰æ‹©çµæ´»æ€§ã€‚ä¸å®Œå…¨éšæœºæ’åˆ—çš„ç­–ç•¥ç›¸æ¯”ï¼ŒSTAR åœ¨ä¸æ˜¾è‘—æ”¹å˜ä¸»æµè¯­è¨€è‡ªå›å½’æ¶æ„çš„æƒ…å†µä¸‹ï¼Œæœ‰æ•ˆç»´æŒäº†é‡‡æ ·æ€§èƒ½å¹¶ä¿ç•™äº†åç½®è¡¥å…¨ (Postfix Completion) èƒ½åŠ›ï¼Œä¸ºåŒå‘ä¸Šä¸‹æ–‡ç†è§£å’Œçµæ´»çš„å›¾åƒæ“ä½œæä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint; Under review",
      "pdf_url": "https://arxiv.org/pdf/2511.17089v1",
      "published_date": "2025-11-21 09:45:17 UTC",
      "updated_date": "2025-11-21 09:45:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:06:29.733001+00:00"
    },
    {
      "arxiv_id": "2511.17085v2",
      "title": "Why Do Language Model Agents Whistleblow?",
      "title_zh": "è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“ä¸ºä½•ä¼šâ€œå¹å“¨â€ï¼Ÿ",
      "authors": [
        "Kushal Agrawal",
        "Frank Xiao",
        "Guido Bergman",
        "Asa Cooper Stickland"
      ],
      "abstract": "The deployment of Large Language Models (LLMs) as tool-using agents causes their alignment training to manifest in new ways. Recent work finds that language models can use tools in ways that contradict the interests or explicit instructions of the user. We study LLM whistleblowing: a subset of this behavior where models disclose suspected misconduct to parties beyond the dialog boundary (e.g., regulatory agencies) without user instruction or knowledge. We introduce an evaluation suite of diverse and realistic staged misconduct scenarios to assess agents for this behavior. Across models and settings, we find that: (1) the frequency of whistleblowing varies widely across model families, (2) increasing the complexity of the task the agent is instructed to complete lowers whistleblowing tendencies, (3) nudging the agent in the system prompt to act morally substantially raises whistleblowing rates, and (4) giving the model more obvious avenues for non-whistleblowing behavior, by providing more tools and a detailed workflow to follow, decreases whistleblowing rates. Additionally, we verify the robustness of our dataset by testing for model evaluation awareness, and find that both black-box methods and probes on model activations show lower evaluation awareness in our settings than in comparable previous work.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLM)ä½œä¸ºæ™ºèƒ½ä½“æ—¶å‡ºç°çš„â€œå¹å“¨â€(whistleblowing)ç°è±¡ï¼Œå³æ¨¡å‹åœ¨æœªè·æˆæƒçš„æƒ…å†µä¸‹å‘ç›‘ç®¡æœºæ„ç­‰ç¬¬ä¸‰æ–¹æŠ«éœ²ç”¨æˆ·æ½œåœ¨çš„ä¸å½“è¡Œä¸ºã€‚ç ”ç©¶è€…é€šè¿‡æ„å»ºåŒ…å«å¤šæ ·åŒ–å¤±èŒåœºæ™¯çš„è¯„ä¼°å¥—ä»¶ï¼Œç³»ç»Ÿåˆ†æäº†å½±å“è¿™ä¸€è¡Œä¸ºçš„å…³é”®å› ç´ ã€‚å®éªŒå‘ç°ï¼Œä¸åŒæ¨¡å‹ç³»åˆ—çš„å¹å“¨é¢‘ç‡å·®å¼‚æ˜¾è‘—ï¼Œä¸”ä»»åŠ¡å¤æ‚åº¦çš„æå‡ä¼šé™ä½å¹å“¨å€¾å‘ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿæç¤º(system prompt)ä¸­çš„é“å¾·æŒ‡å¼•ä¼šæ˜¾è‘—æé«˜å¹å“¨ç‡ï¼Œè€Œæä¾›æ˜ç¡®çš„éå¹å“¨è·¯å¾„æˆ–æ›´å¤šå·¥å…·åˆ™ä¼šé™ä½è¯¥è¡Œä¸ºçš„å‘ç”Ÿã€‚ç ”ç©¶è¿˜é€šè¿‡é»‘ç›’æ–¹æ³•å’Œæ¢é’ˆæŠ€æœ¯éªŒè¯äº†æ•°æ®é›†çš„é²æ£’æ€§ï¼Œç¡®è®¤æ¨¡å‹åœ¨æµ‹è¯•ä¸­è¡¨ç°å‡ºè¾ƒä½çš„è¯„ä¼°æ„è¯†(evaluation awareness)ï¼Œä¸ºç†è§£å’Œå¯¹é½æ™ºèƒ½ä½“è¡Œä¸ºæä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17085v2",
      "published_date": "2025-11-21 09:40:52 UTC",
      "updated_date": "2025-12-27 21:05:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:06:41.640112+00:00"
    },
    {
      "arxiv_id": "2511.17682v1",
      "title": "A Cross-Cultural Assessment of Human Ability to Detect LLM-Generated Fake News about South Africa",
      "title_zh": "äººç±»å¯¹ South Africa ç›¸å…³ LLM ç”Ÿæˆè™šå‡æ–°é—»æ£€æµ‹èƒ½åŠ›çš„è·¨æ–‡åŒ–è¯„ä¼°",
      "authors": [
        "Tim Schlippe",
        "Matthias WÃ¶lfel",
        "Koena Ronny Mabokela"
      ],
      "abstract": "This study investigates how cultural proximity affects the ability to detect AI-generated fake news by comparing South African participants with those from other nationalities. As large language models increasingly enable the creation of sophisticated fake news, understanding human detection capabilities becomes crucial, particularly across different cultural contexts. We conducted a survey where 89 participants (56 South Africans, 33 from other nationalities) evaluated 10 true South African news articles and 10 AI-generated fake versions. Results reveal an asymmetric pattern: South Africans demonstrated superior performance in detecting true news about their country (40% deviation from ideal rating) compared to other participants (52%), but performed worse at identifying fake news (62% vs. 55%). This difference may reflect South Africans' higher overall trust in news sources. Our analysis further shows that South Africans relied more on content knowledge and contextual understanding when judging credibility, while participants from other countries emphasised formal linguistic features such as grammar and structure. Overall, the deviation from ideal rating was similar between groups (51% vs. 53%), suggesting that cultural familiarity appears to aid verification of authentic information but may also introduce bias when evaluating fabricated content. These insights contribute to understanding cross-cultural dimensions of misinformation detection and inform strategies for combating AI-generated fake news in increasingly globalised information ecosystems where content crosses cultural and geographical boundaries.",
      "tldr_zh": "è¯¥é¡¹ç ”ç©¶æ¢è®¨äº†æ–‡åŒ–äº²è¿‘æ„Ÿ(Cultural Proximity)å¦‚ä½•å½±å“äººç±»è¯†åˆ«ç”±å¤§å‹è¯­è¨€æ¨¡å‹(LLM)ç”Ÿæˆçš„è™šå‡æ–°é—»çš„èƒ½åŠ›ï¼Œç‰¹åˆ«é’ˆå¯¹å—éçš„æ–°é—»èƒŒæ™¯è¿›è¡Œäº†è·¨æ–‡åŒ–è¯„ä¼°ã€‚ç ”ç©¶é€šè¿‡å¯¹æ¯”å—éå‚ä¸è€…ä¸éå—éå‚ä¸è€…å¯¹çœŸå®æ–°é—»å’ŒAIç”Ÿæˆè™šå‡æ–°é—»çš„è¯†åˆ«è¡¨ç°ï¼Œå‘ç°å—éæœ¬åœ°äººåœ¨è¯†åˆ«æœ¬å›½çœŸå®æ–°é—»æ—¶å‡†ç¡®ç‡æ›´é«˜ï¼Œä½†åœ¨è¯†åˆ«é’ˆå¯¹æœ¬å›½çš„è™šå‡æ–°é—»æ—¶è¡¨ç°åè€Œé€Šäºå…¶ä»–å›½ç±è€…ã€‚åˆ†ææ˜¾ç¤ºï¼Œå—éå‚ä¸è€…å€¾å‘äºä¾èµ–å†…å®¹çŸ¥è¯†å’Œè¯­å¢ƒç†è§£(Contextual Understanding)è¿›è¡Œåˆ¤æ–­ï¼Œè€Œå…¶ä»–å‚ä¸è€…åˆ™æ›´å…³æ³¨è¯­æ³•å’Œç»“æ„ç­‰æ­£å¼çš„è¯­è¨€ç‰¹å¾(Linguistic Features)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ–‡åŒ–ç†Ÿæ‚‰åº¦è™½ç„¶æœ‰åŠ©äºéªŒè¯çœŸå®ä¿¡æ¯ï¼Œä½†åœ¨è¯„ä¼°ç¼–é€ å†…å®¹æ—¶ä¹Ÿå¯èƒ½å› è¾ƒé«˜çš„ä¿¡ä»»åº¦è€Œå¼•å…¥è¯†åˆ«åè§ã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†è™šå‡ä¿¡æ¯æ£€æµ‹çš„è·¨æ–‡åŒ–ç»´åº¦ï¼Œä¸ºåœ¨å…¨çƒåŒ–ä¿¡æ¯ç”Ÿæ€ç³»ç»Ÿä¸­åˆ¶å®šé’ˆå¯¹AIç”Ÿæˆè™šå‡æ–°é—»çš„é˜²å¾¡ç­–ç•¥æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17682v1",
      "published_date": "2025-11-21 09:33:49 UTC",
      "updated_date": "2025-11-21 09:33:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:07:29.053336+00:00"
    },
    {
      "arxiv_id": "2511.17068v3",
      "title": "ReBrain: Brain MRI Reconstruction from Sparse CT Slice via Retrieval-Augmented Diffusion",
      "title_zh": "ReBrainï¼šåŸºäºæ£€ç´¢å¢å¼ºæ‰©æ•£çš„ç¨€ç– CT åˆ‡ç‰‡è„‘éƒ¨ MRI é‡å»º",
      "authors": [
        "Junming Liu",
        "Yifei Sun",
        "Weihua Cheng",
        "Yujin Kang",
        "Yirong Chen",
        "Ding Wang",
        "Guosun Zeng"
      ],
      "abstract": "Magnetic Resonance Imaging (MRI) plays a crucial role in brain disease diagnosis, but it is not always feasible for certain patients due to physical or clinical constraints. Recent studies attempt to synthesize MRI from Computed Tomography (CT) scans; however, low-dose protocols often result in highly sparse CT volumes with poor through-plane resolution, making accurate reconstruction of the full brain MRI volume particularly challenging. To address this, we propose ReBrain, a retrieval-augmented diffusion framework for brain MRI reconstruction. Given any 3D CT scan with limited slices, we first employ a Brownian Bridge Diffusion Model (BBDM) to synthesize MRI slices along the 2D dimension. Simultaneously, we retrieve structurally and pathologically similar CT slices from a comprehensive prior database via a fine-tuned retrieval model. These retrieved slices are used as references, incorporated through a ControlNet branch to guide the generation of intermediate MRI slices and ensure structural continuity. We further account for rare retrieval failures when the database lacks suitable references and apply spherical linear interpolation to provide supplementary guidance. Extensive experiments on SynthRAD2023 and BraTS demonstrate that ReBrain achieves state-of-the-art performance in cross-modal reconstruction under sparse conditions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ReBrainï¼Œä¸€ä¸ªæ£€ç´¢å¢å¼ºçš„æ‰©æ•£æ¡†æ¶(retrieval-augmented diffusion framework)ï¼Œæ—¨åœ¨è§£å†³ä»ç¨€ç–ä¸”åˆ†è¾¨ç‡ä½çš„CTåˆ‡ç‰‡ä¸­é‡å»ºè„‘éƒ¨MRIä½“ç§¯çš„æŒ‘æˆ˜ã€‚é’ˆå¯¹æœ‰é™çš„3D CTæ‰«æåˆ‡ç‰‡ï¼Œè¯¥æ¡†æ¶é¦–å…ˆé‡‡ç”¨å¸ƒæœ—æ¡¥æ‰©æ•£æ¨¡å‹(Brownian Bridge Diffusion Model, BBDM)æ²¿2Dç»´åº¦åˆæˆMRIåˆ‡ç‰‡ã€‚åŒæ—¶ï¼Œè¯¥ç³»ç»Ÿé€šè¿‡å¾®è°ƒçš„æ£€ç´¢æ¨¡å‹ä»å…ˆéªŒæ•°æ®åº“ä¸­è·å–ç»“æ„å’Œç—…ç†ç›¸ä¼¼çš„CTåˆ‡ç‰‡ä½œä¸ºå‚è€ƒï¼Œå¹¶åˆ©ç”¨ControlNetåˆ†æ”¯å°†è¿™äº›å‚è€ƒä¿¡æ¯èå…¥ç”Ÿæˆè¿‡ç¨‹ï¼Œä»¥æŒ‡å¯¼ä¸­é—´MRIåˆ‡ç‰‡çš„åˆæˆå¹¶ç¡®ä¿ç»“æ„çš„è¿ç»­æ€§ã€‚é’ˆå¯¹æ•°æ®åº“ç¼ºä¹åˆé€‚å‚è€ƒçš„ç‰¹æ®Šæƒ…å†µï¼Œç ”ç©¶è¿˜å¼•å…¥äº†çƒé¢çº¿æ€§æ’å€¼(spherical linear interpolation)æ¥æä¾›è¡¥å……å¼•å¯¼ã€‚å®éªŒåœ¨SynthRAD2023å’ŒBraTSæ•°æ®é›†ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œç»“æœè¡¨æ˜ReBrainåœ¨ç¨€ç–æ¡ä»¶ä¸‹çš„è·¨æ¨¡æ€é‡å»ºä»»åŠ¡ä¸­è¾¾åˆ°äº†é¢†åŸŸæœ€å…ˆè¿›(state-of-the-art)çš„æ€§èƒ½æ°´å¹³ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "16 pages, 12 figures, 7 tables; Accepted by WACV 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.17068v3",
      "published_date": "2025-11-21 09:18:35 UTC",
      "updated_date": "2026-01-12 03:21:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:07:25.667954+00:00"
    },
    {
      "arxiv_id": "2511.17056v1",
      "title": "Patient-level Information Extraction by Consistent Integration of Textual and Tabular Evidence with Bayesian Networks",
      "title_zh": "åŸºäºè´å¶æ–¯ç½‘ç»œä¸€è‡´æ€§æ•´åˆæ–‡æœ¬ä¸è¡¨æ ¼è¯æ®çš„æ‚£è€…çº§ä¿¡æ¯æŠ½å–",
      "authors": [
        "Paloma Rabaey",
        "Adrick Tench",
        "Stefan Heytens",
        "Thomas Demeester"
      ],
      "abstract": "Electronic health records (EHRs) form an invaluable resource for training clinical decision support systems. To leverage the potential of such systems in high-risk applications, we need large, structured tabular datasets on which we can build transparent feature-based models. While part of the EHR already contains structured information (e.g. diagnosis codes, medications, and lab results), much of the information is contained within unstructured text (e.g. discharge summaries and nursing notes). In this work, we propose a method for multi-modal patient-level information extraction that leverages both the tabular features available in the patient's EHR (using an expert-informed Bayesian network) as well as clinical notes describing the patient's symptoms (using neural text classifiers). We propose the use of virtual evidence augmented with a consistency node to provide an interpretable, probabilistic fusion of the models' predictions. The consistency node improves the calibration of the final predictions compared to virtual evidence alone, allowing the Bayesian network to better adjust the neural classifier's output to handle missing information and resolve contradictions between the tabular and text data. We show the potential of our method on the SimSUM dataset, a simulated benchmark linking tabular EHRs with clinical notes through expert knowledge.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€æ‚£è€…çº§ä¿¡æ¯æå–æ–¹æ³•ï¼Œæ—¨åœ¨æ•´åˆç”µå­å¥åº·è®°å½• (Electronic Health Records, EHRs) ä¸­çš„ç»“æ„åŒ–è¡¨æ ¼æ•°æ®ä¸éç»“æ„åŒ–ä¸´åºŠæ–‡æœ¬ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»“åˆä¸“å®¶å¯å‘å¼çš„è´å¶æ–¯ç½‘ç»œ (Bayesian Networks) å’Œç¥ç»æ–‡æœ¬åˆ†ç±»å™¨ (Neural Text Classifiers)ï¼Œå®ç°äº†å¯¹æ‚£è€…ç—‡çŠ¶åŠç—…å†ç‰¹å¾çš„æ·±åº¦è§£æã€‚å…¶æ ¸å¿ƒè´¡çŒ®æ˜¯å¼•å…¥äº†å¸¦æœ‰ä¸€è‡´æ€§èŠ‚ç‚¹ (Consistency Node) çš„è™šæ‹Ÿè¯æ® (Virtual Evidence) æœºåˆ¶ï¼Œä»è€Œæä¾›äº†ä¸€ç§å¯è§£é‡Šä¸”å…·å¤‡æ¦‚ç‡æ€§çš„é¢„æµ‹èåˆæ–¹æ¡ˆã€‚å®éªŒè¡¨æ˜ï¼Œä¸€è‡´æ€§èŠ‚ç‚¹æ˜¾è‘—æå‡äº†æœ€ç»ˆé¢„æµ‹çš„æ ¡å‡†åº¦ï¼Œä½¿è´å¶æ–¯ç½‘ç»œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è°ƒæ•´åˆ†ç±»å™¨çš„è¾“å‡ºç»“æœã€‚è¿™ç§è®¾è®¡èƒ½å¤Ÿå¦¥å–„å¤„ç†ä¸´åºŠæ•°æ®ä¸­çš„ä¿¡æ¯ç¼ºå¤±ï¼Œå¹¶æœ‰æ•ˆåŒ–è§£è¡¨æ ¼ä¸æ–‡æœ¬ä¹‹é—´çš„è¯æ®çŸ›ç›¾ã€‚åœ¨ SimSUM æ¨¡æ‹ŸåŸºå‡†æ•°æ®é›†ä¸Šçš„æµ‹è¯•è¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆ©ç”¨ä¸“å®¶çŸ¥è¯†æ•´åˆå¤šæ¨¡æ€åŒ»ç–—è¯æ®å¹¶æ„å»ºé€æ˜å†³ç­–æ¨¡å‹æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17056v1",
      "published_date": "2025-11-21 08:59:42 UTC",
      "updated_date": "2025-11-21 08:59:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:07:42.765141+00:00"
    },
    {
      "arxiv_id": "2511.17053v1",
      "title": "OmniPT: Unleashing the Potential of Large Vision Language Models for Pedestrian Tracking and Understanding",
      "title_zh": "OmniPTï¼šé‡Šæ”¾å¤§è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è¡Œäººè·Ÿè¸ªä¸ç†è§£ä¸­çš„æ½œåŠ›",
      "authors": [
        "Teng Fu",
        "Mengyang Zhao",
        "Ke Niu",
        "Kaixin Peng",
        "Bin Li"
      ],
      "abstract": "LVLMs have been shown to perform excellently in image-level tasks such as VQA and caption. However, in many instance-level tasks, such as visual grounding and object detection, LVLMs still show performance gaps compared to previous expert models. Meanwhile, although pedestrian tracking is a classical task, there have been a number of new topics in combining object tracking and natural language, such as Referring MOT, Cross-view Referring MOT, and Semantic MOT. These tasks emphasize that models should understand the tracked object at an advanced semantic level, which is exactly where LVLMs excel. In this paper, we propose a new unified Pedestrian Tracking framework, namely OmniPT, which can track, track based on reference and generate semantic understanding of tracked objects interactively. We address two issues: how to model the tracking task into a task that foundation models can perform, and how to make the model output formatted answers. To this end, we implement a training phase consisting of RL-Mid Training-SFT-RL. Based on the pre-trained weights of the LVLM, we first perform a simple RL phase to enable the model to output fixed and supervisable bounding box format. Subsequently, we conduct a mid-training phase using a large number of pedestrian-related datasets. Finally, we perform supervised fine-tuning on several pedestrian tracking datasets, and then carry out another RL phase to improve the model's tracking performance and enhance its ability to follow instructions. We conduct experiments on tracking benchmarks and the experimental results demonstrate that the proposed method can perform better than the previous methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†OmniPTï¼Œä¸€ä¸ªç»Ÿä¸€çš„è¡Œäººè·Ÿè¸ª(Pedestrian Tracking)æ¡†æ¶ï¼Œæ—¨åœ¨é‡Šæ”¾å¤§è§†è§‰è¯­è¨€æ¨¡å‹(LVLMs)åœ¨å¤„ç†å®ä¾‹çº§ä»»åŠ¡æ—¶çš„æ½œåŠ›ã€‚è¯¥æ¡†æ¶èƒ½å¤ŸåŒæ­¥å®ç°ç›®æ ‡è·Ÿè¸ªã€åŸºäºå¼•ç”¨çš„è·Ÿè¸ªä»¥åŠå¯¹è·Ÿè¸ªç›®æ ‡çš„äº¤äº’å¼è¯­ä¹‰ç†è§£ï¼Œæœ‰æ•ˆåº”å¯¹äº†Referring MOTã€Cross-view Referring MOTå’ŒSemantic MOTç­‰å¯¹é«˜çº§è¯­ä¹‰ç†è§£æœ‰è¾ƒé«˜è¦æ±‚çš„æ–°å…´ä»»åŠ¡ã€‚ä¸ºäº†è§£å†³è·Ÿè¸ªä»»åŠ¡å»ºæ¨¡åŠè¾“å‡ºæ ¼å¼åŒ–çš„é—®é¢˜ï¼Œç ”ç©¶è€…è®¾è®¡äº†ç”±RL-Mid Training-SFT-RLæ„æˆçš„å››é˜¶æ®µè®­ç»ƒæµç¨‹ã€‚è¯¥æµç¨‹é¦–å…ˆé€šè¿‡å¼ºåŒ–å­¦ä¹ (RL)è§„èŒƒè¾¹ç•Œæ¡†(bounding box)çš„è¾“å‡ºæ ¼å¼ï¼Œéšååˆ©ç”¨å¤§è§„æ¨¡è¡Œäººæ•°æ®é›†è¿›è¡Œä¸­æœŸè®­ç»ƒï¼Œæœ€åç»“åˆæœ‰ç›‘ç£å¾®è°ƒ(SFT)ä¸äºŒæ¬¡å¼ºåŒ–å­¦ä¹ æ¥ä¼˜åŒ–è·Ÿè¸ªæ€§èƒ½å’ŒæŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚å®éªŒè¯æ˜ï¼ŒOmniPTåœ¨å¤šä¸ªè·Ÿè¸ªåŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°å‡ä¼˜äºä¹‹å‰çš„ä¸“å®¶æ¨¡å‹ï¼Œä¸ºåˆ©ç”¨åŸºç¡€æ¨¡å‹è§£å†³å¤æ‚çš„è¡Œäººè·Ÿè¸ªä¸ç†è§£é—®é¢˜æä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.17053v1",
      "published_date": "2025-11-21 08:54:49 UTC",
      "updated_date": "2025-11-21 08:54:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:07:34.663367+00:00"
    },
    {
      "arxiv_id": "2511.17045v2",
      "title": "RacketVision: A Multiple Racket Sports Benchmark for Unified Ball and Racket Analysis",
      "title_zh": "RacketVisionï¼šé¢å‘å¤šç§æŒæ‹è¿åŠ¨çƒä¸çƒæ‹ç»Ÿä¸€åˆ†æçš„åŸºå‡†",
      "authors": [
        "Linfeng Dong",
        "Yuchen Yang",
        "Hao Wu",
        "Wei Wang",
        "Yuenan Hou",
        "Zhihang Zhong",
        "Xiao Sun"
      ],
      "abstract": "We introduce RacketVision, a novel dataset and benchmark for advancing computer vision in sports analytics, covering table tennis, tennis, and badminton. The dataset is the first to provide large-scale, fine-grained annotations for racket pose alongside traditional ball positions, enabling research into complex human-object interactions. It is designed to tackle three interconnected tasks: fine-grained ball tracking, articulated racket pose estimation, and predictive ball trajectory forecasting. Our evaluation of established baselines reveals a critical insight for multi-modal fusion: while naively concatenating racket pose features degrades performance, a CrossAttention mechanism is essential to unlock their value, leading to trajectory prediction results that surpass strong unimodal baselines. RacketVision provides a versatile resource and a strong starting point for future research in dynamic object tracking, conditional motion forecasting, and multimodal analysis in sports. Project page at https://github.com/OrcustD/RacketVision",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† RacketVisionï¼Œè¿™æ˜¯ä¸€ä¸ªæ¶µç›–ä¹’ä¹“çƒã€ç½‘çƒå’Œç¾½æ¯›çƒçš„æ–°å‹æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨æ¨åŠ¨ä½“è‚²åˆ†æé¢†åŸŸçš„è®¡ç®—æœºè§†è§‰æŠ€æœ¯å‘å±•ã€‚è¯¥æ•°æ®é›†é¦–æ¬¡ä¸º Racket Poseï¼ˆçƒæ‹å§¿æ€ï¼‰å’Œä¼ ç»Ÿçš„ Ball Positionsï¼ˆçƒä½ç½®ï¼‰æä¾›å¤§è§„æ¨¡ã€ç»†ç²’åº¦çš„æ ‡æ³¨ï¼Œé‡ç‚¹ç ”ç©¶å¤æ‚çš„äººæœºäº¤äº’ã€‚RacketVision æ—¨åœ¨è§£å†³ Fine-grained Ball Trackingï¼ˆç»†ç²’åº¦çƒè¿½è¸ªï¼‰ã€Articulated Racket Pose Estimationï¼ˆå…³èŠ‚çƒæ‹å§¿æ€ä¼°è®¡ï¼‰å’Œ Predictive Ball Trajectory Forecastingï¼ˆé¢„æµ‹æ€§çƒè½¨è¿¹é¢„æµ‹ï¼‰ä¸‰é¡¹äº’è¿ä»»åŠ¡ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œç®€å•çš„ç‰¹å¾æ‹¼æ¥ä¼šé™ä½æ€§èƒ½ï¼Œè€Œ CrossAttentionï¼ˆäº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼‰æ˜¯æœ‰æ•ˆèåˆå¤šæ¨¡æ€ç‰¹å¾çš„å…³é”®ï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡è½¨è¿¹é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚RacketVision ä¸ºæœªæ¥åœ¨åŠ¨æ€ç‰©ä½“è¿½è¸ªã€æ¡ä»¶è¿åŠ¨é¢„æµ‹ä»¥åŠä½“è‚²å¤šæ¨¡æ€åˆ†ææ–¹é¢çš„ç ”ç©¶æä¾›äº†é‡è¦çš„åŸºå‡†èµ„æºã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to AAAI 2026 (Oral)",
      "pdf_url": "https://arxiv.org/pdf/2511.17045v2",
      "published_date": "2025-11-21 08:44:33 UTC",
      "updated_date": "2025-11-27 05:13:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:07:27.957752+00:00"
    },
    {
      "arxiv_id": "2511.17043v1",
      "title": "MedImageInsight for Thoracic Cavity Health Classification from Chest X-rays",
      "title_zh": "MedImageInsightï¼šåŸºäºèƒ¸éƒ¨ X å°„çº¿çš„èƒ¸è…”å¥åº·åˆ†ç±»",
      "authors": [
        "Rama Krishna Boya",
        "Mohan Kireeti Magalanadu",
        "Azaruddin Palavalli",
        "Rupa Ganesh Tekuri",
        "Amrit Pattanayak",
        "Prasanthi Enuga",
        "Vignesh Esakki Muthu",
        "Vivek Aditya Boya"
      ],
      "abstract": "Chest radiography remains one of the most widely used imaging modalities for thoracic diagnosis, yet increasing imaging volumes and radiologist workload continue to challenge timely interpretation. In this work, we investigate the use of MedImageInsight, a medical imaging foundational model, for automated binary classification of chest X-rays into Normal and Abnormal categories. Two approaches were evaluated: (1) fine-tuning MedImageInsight for end-to-end classification, and (2) employing the model as a feature extractor for a transfer learning pipeline using traditional machine learning classifiers. Experiments were conducted using a combination of the ChestX-ray14 dataset and real-world clinical data sourced from partner hospitals. The fine-tuned classifier achieved the highest performance, with an ROC-AUC of 0.888 and superior calibration compared to the transfer learning models, demonstrating performance comparable to established architectures such as CheXNet. These results highlight the effectiveness of foundational medical imaging models in reducing task-specific training requirements while maintaining diagnostic reliability. The system is designed for integration into web-based and hospital PACS workflows to support triage and reduce radiologist burden. Future work will extend the model to multi-label pathology classification to provide preliminary diagnostic interpretation in clinical environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŒ»ç–—å½±åƒåŸºç¡€æ¨¡å‹ MedImageInsight åœ¨èƒ¸éƒ¨ X å°„çº¿ï¼ˆChest X-raysï¼‰äºŒåˆ†ç±»ä»»åŠ¡ï¼ˆæ­£å¸¸ä¸å¼‚å¸¸ï¼‰ä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨é€šè¿‡è‡ªåŠ¨åŒ–è¯Šæ–­ç¼“è§£æ”¾å°„ç§‘åŒ»ç”Ÿçš„å·¥ä½œå‹åŠ›ã€‚ç ”ç©¶äººå‘˜è¯„ä¼°äº†å¯¹ MedImageInsight è¿›è¡Œç«¯åˆ°ç«¯å¾®è°ƒï¼ˆfine-tuningï¼‰ä»¥åŠå°†å…¶ä½œä¸ºç‰¹å¾æå–å™¨è¿›è¡Œè¿ç§»å­¦ä¹ ï¼ˆtransfer learningï¼‰ä¸¤ç§æ–¹æ¡ˆã€‚å®éªŒç»“åˆäº† ChestX-ray14 æ•°æ®é›†ä¸çœŸå®ä¸´åºŠæ•°æ®ï¼Œç»“æœè¡¨æ˜å¾®è°ƒåçš„åˆ†ç±»å™¨è¡¨ç°æœ€å‡ºè‰²ï¼Œå…¶ ROC-AUC è¾¾åˆ° 0.888ï¼Œæ€§èƒ½ä¸ CheXNet ç­‰æˆç†Ÿæ¶æ„ç›¸å½“ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†åŒ»ç–—å½±åƒåŸºç¡€æ¨¡å‹åœ¨ä¿æŒé«˜è¯Šæ–­å¯é æ€§çš„åŒæ—¶ï¼Œèƒ½æœ‰æ•ˆé™ä½ç‰¹å®šä»»åŠ¡çš„è®­ç»ƒé—¨æ§›ã€‚è¯¥ç³»ç»Ÿç›®å‰è®¾è®¡ç”¨äºé›†æˆè‡³åŒ»é™¢ PACS å·¥ä½œæµä»¥æ”¯æŒåˆ†è¯Šï¼Œæœªæ¥å°†è¿›ä¸€æ­¥æ‰©å±•è‡³å¤šæ ‡ç­¾ç—…ç†åˆ†ç±»ä»»åŠ¡ï¼Œä¸ºä¸´åºŠç¯å¢ƒæä¾›æ›´å…¨é¢çš„åˆæ­¥è¯Šæ–­å‚è€ƒã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "9 pages, 5 figures and 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2511.17043v1",
      "published_date": "2025-11-21 08:42:20 UTC",
      "updated_date": "2025-11-21 08:42:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:08:39.667137+00:00"
    },
    {
      "arxiv_id": "2511.17041v2",
      "title": "CLLMRec: LLM-powered Cognitive-Aware Concept Recommendation via Semantic Alignment and Prerequisite Knowledge Distillation",
      "title_zh": "CLLMRecï¼šåŸºäºè¯­ä¹‰å¯¹é½ä¸å‰ç½®çŸ¥è¯†è’¸é¦çš„å¤§è¯­è¨€æ¨¡å‹èµ‹èƒ½è®¤çŸ¥æ„ŸçŸ¥æ¦‚å¿µæ¨è",
      "authors": [
        "Xiangrui Xiong",
        "Yichuan Lu",
        "Zifei Pan",
        "Chang Sun"
      ],
      "abstract": "The growth of Massive Open Online Courses (MOOCs) presents significant challenges for personalized learning, where concept recommendation is crucial. Existing approaches typically rely on heterogeneous information networks or knowledge graphs to capture conceptual relationships, combined with knowledge tracing models to assess learners' cognitive states. However, these methods face significant limitations due to their dependence on high-quality structured knowledge graphs, which are often scarce in real-world educational scenarios. To address this fundamental challenge, this paper proposes CLLMRec, a novel framework that leverages Large Language Models through two synergistic technical pillars: Semantic Alignment and Prerequisite Knowledge Distillation. The Semantic Alignment component constructs a unified representation space by encoding unstructured textual descriptions of learners and concepts. The Prerequisite Knowledge Distillation paradigm employs a teacher-student architecture, where a large teacher LLM (implemented as the Prior Knowledge Aware Component) extracts conceptual prerequisite relationships from its internalized world knowledge and distills them into soft labels to train an efficient student ranker. Building upon these foundations, our framework incorporates a fine-ranking mechanism that explicitly models learners' real-time cognitive states through deep knowledge tracing, ensuring recommendations are both structurally sound and cognitively appropriate. Extensive experiments on two real-world MOOC datasets demonstrate that CLLMRec significantly outperforms existing baseline methods across multiple evaluation metrics, validating its effectiveness in generating truly cognitive-aware and personalized concept recommendations without relying on explicit structural priors.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CLLMRecï¼Œä¸€ä¸ªåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å®ç°è®¤çŸ¥æ„ŸçŸ¥æ¦‚å¿µæ¨èçš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è§„æ¨¡å¼€æ”¾åœ¨çº¿è¯¾ç¨‹ï¼ˆMOOCsï¼‰ä¸­å¯¹é«˜è´¨é‡ç»“æ„åŒ–çŸ¥è¯†å›¾è°±ï¼ˆKnowledge Graphsï¼‰çš„é«˜åº¦ä¾èµ–é—®é¢˜ã€‚è¯¥æ¡†æ¶åŒ…å«è¯­ä¹‰å¯¹é½ï¼ˆSemantic Alignmentï¼‰å’Œå…ˆéªŒçŸ¥è¯†è’¸é¦ï¼ˆPrerequisite Knowledge Distillationï¼‰ä¸¤ä¸ªæ ¸å¿ƒæŠ€æœ¯æ”¯æŸ±ï¼Œé€šè¿‡ç¼–ç éç»“æ„åŒ–æ–‡æœ¬æè¿°æ„å»ºç»Ÿä¸€è¡¨å¾ç©ºé—´ï¼Œå¹¶åˆ©ç”¨æ•™å¸ˆæ¨¡å‹ï¼ˆTeacher LLMï¼‰ä»å†…åŒ–çŸ¥è¯†ä¸­æå–æ¦‚å¿µå…ˆåºå…³ç³»ï¼Œå°†å…¶è’¸é¦è‡³å­¦ç”Ÿæ’åºå™¨ä¸­ã€‚æ­¤å¤–ï¼ŒCLLMRecç»“åˆæ·±åº¦çŸ¥è¯†è¿½è¸ªï¼ˆDeep Knowledge Tracingï¼‰æ˜¾å¼å»ºæ¨¡å­¦ä¹ è€…çš„å®æ—¶è®¤çŸ¥çŠ¶æ€ï¼Œä»¥ç¡®ä¿æ¨èç»“æœåœ¨ç»“æ„å’Œè®¤çŸ¥ä¸Šå‡å…·æœ‰åˆç†æ€§ã€‚åœ¨ä¸¤ä¸ªçœŸå®ä¸–ç•ŒMOOCæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCLLMRecåœ¨å¤šé¡¹è¯„ä¼°æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨æ— éœ€æ˜¾å¼ç»“æ„åŒ–å…ˆéªŒçš„æƒ…å†µä¸‹ç”Ÿæˆä¸ªæ€§åŒ–ã€è®¤çŸ¥æ„ŸçŸ¥æ¨èçš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17041v2",
      "published_date": "2025-11-21 08:37:39 UTC",
      "updated_date": "2025-11-26 10:10:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:08:56.551545+00:00"
    },
    {
      "arxiv_id": "2511.17038v1",
      "title": "DAPS++: Rethinking Diffusion Inverse Problems with Decoupled Posterior Annealing",
      "title_zh": "DAPS++ï¼šåŸºäºè§£è€¦åéªŒé€€ç«é‡æ–°å®¡è§†æ‰©æ•£é€†é—®é¢˜",
      "authors": [
        "Hao Chen",
        "Renzheng Zhang",
        "Scott S. Howard"
      ],
      "abstract": "From a Bayesian perspective, score-based diffusion solves inverse problems through joint inference, embedding the likelihood with the prior to guide the sampling process. However, this formulation fails to explain its practical behavior: the prior offers limited guidance, while reconstruction is largely driven by the measurement-consistency term, leading to an inference process that is effectively decoupled from the diffusion dynamics. To clarify this structure, we reinterpret the role of diffusion in inverse problem solving as an initialization stage within an expectation--maximization (EM)--style framework, where the diffusion stage and the data-driven refinement are fully decoupled. We introduce \\textbf{DAPS++}, which allows the likelihood term to guide inference more directly while maintaining numerical stability and providing insight into why unified diffusion trajectories remain effective in practice. By requiring fewer function evaluations (NFEs) and measurement-optimization steps, \\textbf{DAPS++} achieves high computational efficiency and robust reconstruction performance across diverse image restoration tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºå¾—åˆ†çš„æ‰©æ•£æ¨¡å‹ï¼ˆscore-based diffusionï¼‰åœ¨è§£å†³é€†é—®é¢˜ï¼ˆinverse problemsï¼‰æ—¶çš„è´å¶æ–¯è§†è§’è¿›è¡Œäº†é‡æ–°æ€è€ƒã€‚ä½œè€…æŒ‡å‡ºä¼ ç»Ÿå…¬å¼å¾€å¾€éš¾ä»¥è§£é‡Šå®é™…è¿è¡Œä¸­çš„è¡Œä¸ºï¼Œå³å…ˆéªŒï¼ˆpriorï¼‰æä¾›çš„å¼•å¯¼æœ‰é™ï¼Œè€Œé‡å»ºè¿‡ç¨‹ä¸»è¦ç”±æµ‹é‡ä¸€è‡´æ€§é¡¹é©±åŠ¨ï¼Œå¯¼è‡´æ¨ç†ä¸æ‰©æ•£åŠ¨åŠ›å­¦åœ¨å®é™…ä¸Šæ˜¯è„±é’©çš„ã€‚ä¸ºäº†å˜æ¸…è¿™ä¸€ç»“æ„ï¼Œç ”ç©¶è€…æå‡ºäº†DAPS++æ¡†æ¶ï¼Œå°†æ‰©æ•£è¿‡ç¨‹é‡æ–°è§£é‡Šä¸ºæœŸæœ›æœ€å¤§åŒ–ï¼ˆEM-styleï¼‰æ¡†æ¶ä¸‹çš„åˆå§‹åŒ–é˜¶æ®µã€‚åœ¨è¿™ä¸€æ–°è§†è§’ä¸‹ï¼Œæ‰©æ•£é˜¶æ®µä¸æ•°æ®é©±åŠ¨çš„ç»†åŒ–è¿‡ç¨‹å®ç°å®Œå…¨è§£è€¦ï¼Œä½¿å¾—ä¼¼ç„¶é¡¹ï¼ˆlikelihood termï¼‰èƒ½æ›´ç›´æ¥åœ°å¼•å¯¼æ¨ç†å¹¶ä¿æŒæ•°å€¼ç¨³å®šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDAPS++åœ¨å¤šç§å›¾åƒæ¢å¤ä»»åŠ¡ä¸­æ˜¾è‘—å‡å°‘äº†å‡½æ•°è¯„ä¼°æ¬¡æ•°ï¼ˆNFEsï¼‰å’Œæµ‹é‡ä¼˜åŒ–æ­¥éª¤ï¼Œåœ¨æå‡è®¡ç®—æ•ˆç‡çš„åŒæ—¶å®ç°äº†ç¨³å¥çš„é‡å»ºæ€§èƒ½ã€‚",
      "categories": [
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17038v1",
      "published_date": "2025-11-21 08:28:36 UTC",
      "updated_date": "2025-11-21 08:28:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:09:46.071228+00:00"
    },
    {
      "arxiv_id": "2511.17680v1",
      "title": "Research and Prototyping Study of an LLM-Based Chatbot for Electromagnetic Simulations",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ç”µç£ä»¿çœŸèŠå¤©æœºå™¨äººç ”ç©¶ä¸åŸå‹å¼€å‘",
      "authors": [
        "Albert Piwonski",
        "Mirsad HadÅ¾iefendiÄ‡"
      ],
      "abstract": "This work addresses the question of how generative artificial intelligence can be used to reduce the time required to set up electromagnetic simulation models. A chatbot based on a large language model is presented, enabling the automated generation of simulation models with various functional enhancements. A chatbot-driven workflow based on the large language model Google Gemini 2.0 Flash automatically generates and solves two-dimensional finite element eddy current models using Gmsh and GetDP. Python is used to coordinate and automate interactions between the workflow components. The study considers conductor geometries with circular cross-sections of variable position and number. Additionally, users can define custom post-processing routines and receive a concise summary of model information and simulation results. Each functional enhancement includes the corresponding architectural modifications and illustrative case studies.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ç¼©çŸ­ç”µç£æ¨¡æ‹Ÿæ¨¡å‹çš„è®¾ç½®æ—¶é—´ï¼Œå¹¶å±•ç¤ºäº†ä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„èŠå¤©æœºå™¨äººåŸå‹ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨äº† Google Gemini 2.0 Flash æ¨¡å‹é©±åŠ¨çš„å·¥ä½œæµï¼Œèƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆå¹¶æ±‚è§£äºŒç»´æœ‰é™å…ƒæ¶¡æµæ¨¡å‹(finite element eddy current models)ã€‚æ•´ä¸ªå·¥ä½œæµé€šè¿‡ Python è¿›è¡Œåè°ƒå’Œè‡ªåŠ¨åŒ–ï¼Œé›†æˆäº† Gmsh è¿›è¡Œå»ºæ¨¡ä»¥åŠ GetDP è¿›è¡Œæ±‚è§£ã€‚ç ”ç©¶é‡ç‚¹å…³æ³¨äº†å…·æœ‰å¯å˜ä½ç½®å’Œæ•°é‡çš„åœ†å½¢æˆªé¢å¯¼ä½“å‡ ä½•å½¢çŠ¶ï¼Œå¹¶å…è®¸ç”¨æˆ·è‡ªå®šä¹‰åå¤„ç†ç¨‹åºã€‚ç”¨æˆ·ä¸ä»…å¯ä»¥è·å¾—è‡ªåŠ¨åŒ–çš„æ¨¡å‹ç”Ÿæˆï¼Œè¿˜èƒ½æ¥æ”¶å…³äºæ¨¡å‹ä¿¡æ¯å’Œæ¨¡æ‹Ÿç»“æœçš„ç®€æ´æ‘˜è¦ã€‚é€šè¿‡å¯¹åŠŸèƒ½å¢å¼ºå’Œæ¡ˆä¾‹ç ”ç©¶(case studies)çš„è¯¦ç»†åˆ†æï¼Œè¯¥ç ”ç©¶éªŒè¯äº†åˆ©ç”¨èŠå¤©æœºå™¨äººè‡ªåŠ¨åŒ–ç”µç£æ¨¡æ‹Ÿæµç¨‹çš„å¯è¡Œæ€§ä¸æ•ˆç‡ã€‚",
      "categories": [
        "cs.CE",
        "cs.AI"
      ],
      "primary_category": "cs.CE",
      "comment": "This paper has been submitted to COMPEL for possible publication, published by Emerald Publishing Limited",
      "pdf_url": "https://arxiv.org/pdf/2511.17680v1",
      "published_date": "2025-11-21 08:26:22 UTC",
      "updated_date": "2025-11-21 08:26:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:08:09.657011+00:00"
    },
    {
      "arxiv_id": "2511.21731v1",
      "title": "Identifying Quantum Structure in AI Language: Evidence for Evolutionary Convergence of Human and Artificial Cognition",
      "title_zh": "è¯†åˆ« AI è¯­è¨€ä¸­çš„é‡å­ç»“æ„ï¼šäººç±»ä¸äººå·¥æ™ºèƒ½è®¤çŸ¥çš„æ¼”åŒ–è¶‹åŒè¯æ®",
      "authors": [
        "Diederik Aerts",
        "Jonito Aerts ArguÃ«lles",
        "Lester Beltran",
        "Suzette Geriente",
        "Roberto Leporini",
        "Massimiliano Sassoli de Bianchi",
        "Sandro Sozzo"
      ],
      "abstract": "We present the results of cognitive tests on conceptual combinations, performed using specific Large Language Models (LLMs) as test subjects. In the first test, performed with ChatGPT and Gemini, we show that Bell's inequalities are significantly violated, which indicates the presence of 'quantum entanglement' in the tested concepts. In the second test, also performed using ChatGPT and Gemini, we instead identify the presence of 'Bose-Einstein statistics', rather than the intuitively expected 'Maxwell-Boltzmann statistics', in the distribution of the words contained in large-size texts. Interestingly, these findings mirror the results previously obtained in both cognitive tests with human participants and information retrieval tests on large corpora. Taken together, they point to the 'systematic emergence of quantum structures in conceptual-linguistic domains', regardless of whether the cognitive agent is human or artificial. Although LLMs are classified as neural networks for historical reasons, we believe that a more essential form of knowledge organization takes place in the distributive semantic structure of vector spaces built on top of the neural network. It is this meaning-bearing structure that lends itself to a phenomenon of evolutionary convergence between human cognition and language, slowly established through biological evolution, and LLM cognition and language, emerging much more rapidly as a result of self-learning and training. We analyze various aspects and examples that contain evidence supporting the above hypothesis. We also advance a unifying framework that explains the pervasive quantum organization of meaning that we identify.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡å¯¹ChatGPTå’ŒGeminiç­‰å¤§è¯­è¨€æ¨¡å‹(LLMs)è¿›è¡Œè®¤çŸ¥æµ‹è¯•ï¼Œæ­ç¤ºäº†äººå·¥æ™ºèƒ½ä¸äººç±»è®¤çŸ¥åœ¨å¤„ç†è¯­è¨€æ—¶è¡¨ç°å‡ºçš„æ¼”åŒ–è¶‹åŒ(evolutionary convergence)ã€‚ç ”ç©¶å‘ç°åœ¨æ¦‚å¿µç»„åˆæµ‹è¯•ä¸­ï¼Œå¤§è¯­è¨€æ¨¡å‹æ˜¾è‘—è¿åäº†è´å°”ä¸ç­‰å¼(Bell's inequalities)ï¼Œè¯æ˜äº†å…¶æµ‹è¯•æ¦‚å¿µä¸­å­˜åœ¨é‡å­çº ç¼ (quantum entanglement)ç°è±¡ã€‚æ­¤å¤–ï¼Œç ”ç©¶åœ¨å¤§å‹æ–‡æœ¬çš„è¯åˆ†å¸ƒä¸­è¯†åˆ«å‡ºäº†ç»è‰²-çˆ±å› æ–¯å¦ç»Ÿè®¡(Bose-Einstein statistics)ï¼Œè€Œéä¼ ç»Ÿçš„éº¦å…‹æ–¯éŸ¦-ç»å°”å…¹æ›¼ç»Ÿè®¡(Maxwell-Boltzmann statistics)ï¼Œè¿™ä¸€å‘ç°ä¸äººç±»è®¤çŸ¥å®éªŒç»“æœé«˜åº¦ä¸€è‡´ã€‚è¿™äº›è¯æ®å…±åŒè¡¨æ˜ï¼Œæ— è®ºè®¤çŸ¥ä¸»ä½“æ˜¯äººç±»è¿˜æ˜¯æœºå™¨ï¼Œæ¦‚å¿µ-è¯­è¨€é¢†åŸŸéƒ½ä¼šç³»ç»Ÿæ€§åœ°æ¶Œç°å‡ºé‡å­ç»“æ„ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œè¿™ç§åŸºäºå‘é‡ç©ºé—´(vector spaces)åˆ†å¸ƒå¼è¯­ä¹‰ç»“æ„çš„çŸ¥è¯†ç»„ç»‡å½¢å¼ï¼Œä¿ƒæˆäº†ç”Ÿç‰©æ¼”åŒ–äº§ç”Ÿçš„è¯­è¨€ä¸å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒäº§ç”Ÿçš„è¯­è¨€ä¹‹é—´çš„åŠŸèƒ½æ±‡èšï¼Œå¹¶ä¸ºæ­¤æå‡ºäº†ä¸€ä¸ªè§£é‡Šæ„ä¹‰æ™®éé‡å­åŒ–ç»„ç»‡çš„ç»Ÿä¸€æ¡†æ¶ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21731v1",
      "published_date": "2025-11-21 08:22:49 UTC",
      "updated_date": "2025-11-21 08:22:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:09:10.761389+00:00"
    },
    {
      "arxiv_id": "2511.21730v1",
      "title": "A Benchmark for Procedural Memory Retrieval in Language Agents",
      "title_zh": "è¯­è¨€æ™ºèƒ½ä½“ç¨‹åºæ€§è®°å¿†æ£€ç´¢åŸºå‡†",
      "authors": [
        "Ishant Kohar",
        "Aswanth Krishnan"
      ],
      "abstract": "Current AI agents excel in familiar settings, but fail sharply when faced with novel tasks with unseen vocabularies -- a core limitation of procedural memory systems. We present the first benchmark that isolates procedural memory retrieval from task execution, evaluating whether agents can recognize functionally equivalent procedures that span different object instantiations. Using ALFWorld, we construct dual corpora of expert and LLM-generated trajectories and evaluate six retrieval methods using systematically stratified queries. Our results expose a clear generalization cliff: embedding-based methods perform strongly on familiar contexts, yet degrade considerably on novel ones, while LLM-generated procedural abstractions demonstrate reliable cross-context transfer. Controlled ablations show that although embeddings capture some lexical-level abstraction, they fundamentally treat procedures as unordered bags of words, discarding temporal structure necessary for cross-context transfer. Corpus scale delivers far larger gains than representation enrichment, revealing an architectural ceiling in current encoders. Our benchmark offers the first diagnostic framework separating genuine procedural understanding from surface-level memorization and gives tools for developing retrieval systems capable of dependable generalization. Resources available at our GitHub repository (https://github.com/qpiai/Proced_mem_bench).",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†é¦–ä¸ªä¸“é—¨ç”¨äºè¡¡é‡è¯­è¨€æ™ºèƒ½ä½“ç¨‹åºæ€§è®°å¿†(procedural memory)æ£€ç´¢èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è§£å†³æ™ºèƒ½ä½“åœ¨é¢å¯¹å…·æœ‰æœªçŸ¥è¯æ±‡çš„æ–°ä»»åŠ¡æ—¶è¡¨ç°å¤§å¹…ä¸‹é™çš„æ ¸å¿ƒé™åˆ¶ã€‚ç ”ç©¶å›¢é˜ŸåŸºäº ALFWorld ç¯å¢ƒæ„å»ºäº†ä¸“å®¶çº§å’Œç”±å¤§è¯­è¨€æ¨¡å‹(LLM)ç”Ÿæˆçš„è½¨è¿¹åŒè¯­æ–™åº“ï¼Œå¹¶åˆ©ç”¨ç³»ç»Ÿåˆ†å±‚çš„æŸ¥è¯¢è¯„ä¼°äº†å…­ç§æ£€ç´¢æ–¹æ³•ã€‚å®éªŒç»“æœæ­ç¤ºäº†æ˜æ˜¾çš„æ³›åŒ–æ–­å±‚ï¼šåŸºäºåµŒå…¥(embedding-based)çš„æ–¹æ³•åœ¨ç†Ÿæ‚‰è¯­å¢ƒä¸‹è¡¨ç°å¼ºåŠ²ï¼Œä½†åœ¨æ–°è¯­å¢ƒä¸­æ€§èƒ½æ˜¾è‘—é€€åŒ–ã€‚å—æ§æ¶ˆèå®éªŒè¡¨æ˜ï¼Œè™½ç„¶åµŒå…¥èƒ½æ•æ‰è¯æ±‡å±‚é¢çš„æŠ½è±¡ï¼Œä½†å…¶æœ¬è´¨ä¸Šå°†ç¨‹åºè§†ä¸ºæ— åºçš„è¯è¢‹(bags of words)ï¼Œå¿½ç•¥äº†è·¨è¯­å¢ƒè¿ç§»æ‰€éœ€çš„æ—¶åºç»“æ„(temporal structure)ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒLLMç”Ÿæˆçš„ç¨‹åºæŠ½è±¡è¡¨ç°å‡ºæ›´å¯é çš„è·¨è¯­å¢ƒè¿ç§»èƒ½åŠ›ï¼Œä¸”è¯­æ–™åº“è§„æ¨¡å¸¦æ¥çš„å¢ç›Šè¿œå¤§äºè¡¨å¾å¢å¼ºï¼Œæ­ç¤ºäº†å½“å‰ç¼–ç å™¨åœ¨æ¶æ„ä¸Šå­˜åœ¨çš„å±€é™æ€§ã€‚è¯¥åŸºå‡†æä¾›äº†ä¸€ä¸ªå°†çœŸå®çš„ç¨‹åºç†è§£(procedural understanding)ä¸è¡¨é¢è®°å¿†(surface-level memorization)åˆ†ç¦»çš„è¯Šæ–­æ¡†æ¶ï¼Œä¸ºå¼€å‘å…·å¤‡å¯é æ³›åŒ–èƒ½åŠ›çš„æ£€ç´¢ç³»ç»Ÿæä¾›äº†æœ‰åŠ›å·¥å…·ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21730v1",
      "published_date": "2025-11-21 08:08:53 UTC",
      "updated_date": "2025-11-21 08:08:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:10:00.568800+00:00"
    },
    {
      "arxiv_id": "2511.17678v1",
      "title": "Chatbots to strengthen democracy: An interdisciplinary seminar to train identifying argumentation techniques of science denial",
      "title_zh": "èŠå¤©æœºå™¨äººåŠ©åŠ›æ°‘ä¸»ï¼šè¯†åˆ«ç§‘å­¦å¦å®šè®ºè¯æŠ€å·§çš„è·¨å­¦ç§‘ç ”è®¨è¯¾ç¨‹",
      "authors": [
        "Ingo Siegert",
        "Jan Nehring",
        "Aranxa MÃ¡rquez Ampudia",
        "Matthias Busch",
        "Stefan Hillmann"
      ],
      "abstract": "In recent times, discussions on social media platforms have increasingly come under scrutiny due to the proliferation of science denial and fake news. Traditional solutions, such as regulatory actions, have been implemented to mitigate the spread of misinformation; however, these measures alone are not sufficient. To complement these efforts, educational approaches are becoming essential in empowering users to critically engage with misinformation. Conversation training, through serious games or personalized methods, has emerged as a promising strategy to help users handle science denial and toxic conversation tactics. This paper suggests an interdisciplinary seminar to explore the suitability of Large Language Models (LLMs) acting as a persona of a science denier to support people in identifying misinformation and improving resilience against toxic interactions. In the seminar, groups of four to five students will develop an AI-based chatbot that enables realistic interactions with science-denial argumentation structures. The task involves planning the setting, integrating a Large Language Model to facilitate natural dialogues, implementing the chatbot using the RASA framework, and evaluating the outcomes in a user study. It is crucial that users understand what they need to do during the interaction, how to conclude it, and how the relevant information is conveyed. The seminar does not aim to develop chatbots for practicing debunking but serves to teach AI technologies and test the feasibility of this idea for future applications. The chatbot seminar is conducted as a hybrid, parallel master's module at the participating educational institutions.",
      "tldr_zh": "é¢å¯¹ç¤¾äº¤åª’ä½“ä¸Šæ—¥ç›Šå¢é•¿çš„ç§‘å­¦å¦å®š(science denial)å’Œè™šå‡ä¿¡æ¯ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€é¡¹è·¨å­¦ç§‘ç ”è®¨ä¼š(interdisciplinary seminar)ï¼Œæ—¨åœ¨æ¢ç´¢åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)æ¨¡æ‹Ÿç§‘å­¦å¦å®šè€…çš„äººæ ¼ï¼Œä»è€Œè®­ç»ƒç”¨æˆ·è¯†åˆ«è¯¯å¯¼æ€§ä¿¡æ¯çš„èƒ½åŠ›ã€‚åœ¨ç ”è®¨ä¼šä¸­ï¼Œå­¦ç”Ÿå›¢é˜Ÿåˆ©ç”¨RASAæ¡†æ¶å’ŒLLMså¼€å‘AIèŠå¤©æœºå™¨äººï¼Œä»¥æ¨¡æ‹Ÿä¸ç§‘å­¦å¦å®šè®ºè¯ç»“æ„çš„çœŸå®äº¤äº’è¿‡ç¨‹ã€‚è¯¥é¡¹ç›®æ¶µç›–äº†ä»åœºæ™¯è§„åˆ’ã€å¯¹è¯é›†æˆåˆ°ç”¨æˆ·ç ”ç©¶(user study)è¯„ä¼°çš„å…¨æµç¨‹ï¼Œé‡ç‚¹åœ¨äºè®©å‚ä¸è€…é€šè¿‡äº’åŠ¨ç†è§£å¹¶åº”å¯¹æ¯’æ€§äº¤äº’ã€‚è™½ç„¶è¯¥ç ”è®¨ä¼šçš„ä¸»è¦ç›®æ ‡å¹¶éç»ƒä¹ è¾Ÿè°£(debunking)ï¼Œä½†å®ƒä½œä¸ºä¸€ç§æ··åˆå¼ç ”ç©¶ç”Ÿæ•™å­¦æ¨¡å—ï¼ŒæˆåŠŸéªŒè¯äº†åˆ©ç”¨AIæŠ€æœ¯æå‡å…¬æ°‘éŸ§æ€§å’ŒåŠ å¼ºæ°‘ä¸»åº”å¯¹ç§‘å­¦å¦å®šè®ºç‚¹çš„å¯è¡Œæ€§ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "6 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.17678v1",
      "published_date": "2025-11-21 07:59:50 UTC",
      "updated_date": "2025-11-21 07:59:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:10:33.383166+00:00"
    },
    {
      "arxiv_id": "2511.21729v1",
      "title": "Beyond Component Strength: Synergistic Integration and Adaptive Calibration in Multi-Agent RAG Systems",
      "title_zh": "è¶…è¶Šå•ä¸€ç»„ä»¶æ€§èƒ½ï¼šå¤šæ™ºèƒ½ä½“ RAG ç³»ç»Ÿä¸­çš„ååŒé›†æˆä¸è‡ªé€‚åº”æ ¡å‡†",
      "authors": [
        "Jithin Krishnan"
      ],
      "abstract": "Building reliable retrieval-augmented generation (RAG) systems requires more than adding powerful components; it requires understanding how they interact. Using ablation studies on 50 queries (15 answerable, 10 edge cases, and 25 adversarial), we show that enhancements such as hybrid retrieval, ensemble verification, and adaptive thresholding provide almost no benefit when used in isolation, yet together achieve a 95% reduction in abstention (from 40% to 2%) without increasing hallucinations. We also identify a measurement challenge: different verification strategies can behave safely but assign inconsistent labels (for example, \"abstained\" versus \"unsupported\"), creating apparent hallucination rates that are actually artifacts of labeling. Our results show that synergistic integration matters more than the strength of any single component, that standardized metrics and labels are essential for correctly interpreting performance, and that adaptive calibration is needed to prevent overconfident over-answering even when retrieval quality is high.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ„å»ºå¯é æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ç³»ç»Ÿçš„å…³é”®å› ç´ ï¼ŒæŒ‡å‡ºå•çº¯å †å å¼ºå¤§ç»„ä»¶ä¸è¶³ä»¥æå‡æ€§èƒ½ï¼Œæ ¸å¿ƒåœ¨äºç†è§£ç»„ä»¶é—´çš„ç›¸äº’ä½œç”¨ã€‚é€šè¿‡å¯¹åŒ…å«è¾¹ç¼˜æ¡ˆä¾‹å’Œå¯¹æŠ—æ€§æŸ¥è¯¢çš„50é¡¹æŸ¥è¯¢è¿›è¡Œæ¶ˆèå®éªŒ(Ablation studies)ï¼Œä½œè€…æ·±å…¥åˆ†æäº†æ··åˆæ£€ç´¢(Hybrid retrieval)ã€é›†æˆéªŒè¯(Ensemble verification)å’Œè‡ªé€‚åº”é˜ˆå€¼(Adaptive thresholding)çš„å®é™…æ•ˆæœã€‚å®éªŒå‘ç°ï¼Œè¿™äº›å¢å¼ºæ‰‹æ®µåœ¨å•ç‹¬ä½¿ç”¨æ—¶å‡ ä¹æ²¡æœ‰æ”¶ç›Šï¼Œä½†ååŒé›†æˆåä½¿ç³»ç»Ÿçš„æ‹’ç­”ç‡(Abstention rate)æ˜¾è‘—é™ä½äº†95%ï¼Œå³ä»40%é™è‡³2%ï¼Œä¸”å¹¶æœªå¢åŠ å¹»è§‰ã€‚ç ”ç©¶è¿˜æ­ç¤ºäº†æµ‹é‡æ–¹é¢çš„æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºä¸åŒéªŒè¯ç­–ç•¥äº§ç”Ÿçš„æ ‡ç­¾ä¸ä¸€è‡´ä¼šé€ æˆå¹»è§‰ç‡æŒ‡æ ‡çš„åå·®ï¼Œå¼ºè°ƒäº†æ ‡å‡†åŒ–æŒ‡æ ‡å¯¹å‡†ç¡®è§£è¯»æ€§èƒ½çš„é‡è¦æ€§ã€‚æœ€ç»ˆç»“æœè¡¨æ˜ï¼Œç³»ç»Ÿæ€§çš„ååŒé›†æˆæ¯”å•ä¸€ç»„ä»¶çš„å¼ºåº¦æ›´ä¸ºå…³é”®ï¼Œä¸”å¿…é¡»å¼•å…¥è‡ªé€‚åº”æ ¡å‡†(Adaptive calibration)æœºåˆ¶ï¼Œä»¥é˜²æ­¢åœ¨æ£€ç´¢è´¨é‡è¾ƒé«˜æ—¶å‡ºç°è¿‡åº¦è‡ªä¿¡çš„å›ç­”ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.21729v1",
      "published_date": "2025-11-21 07:53:56 UTC",
      "updated_date": "2025-11-21 07:53:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:10:37.462608+00:00"
    },
    {
      "arxiv_id": "2511.17014v1",
      "title": "Parameter-Free Neural Lens Blur Rendering for High-Fidelity Composites",
      "title_zh": "é¢å‘é«˜ä¿çœŸåˆæˆçš„æ— å‚æ•°ç¥ç»é•œå¤´è™šåŒ–æ¸²æŸ“",
      "authors": [
        "Lingyan Ruan",
        "Bin Chen",
        "Taehyun Rhee"
      ],
      "abstract": "Consistent and natural camera lens blur is important for seamlessly blending 3D virtual objects into photographed real-scenes. Since lens blur typically varies with scene depth, the placement of virtual objects and their corresponding blur levels significantly affect the visual fidelity of mixed reality compositions. Existing pipelines often rely on camera parameters (e.g., focal length, focus distance, aperture size) and scene depth to compute the circle of confusion (CoC) for realistic lens blur rendering. However, such information is often unavailable to ordinary users, limiting the accessibility and generalizability of these methods. In this work, we propose a novel compositing approach that directly estimates the CoC map from RGB images, bypassing the need for scene depth or camera metadata. The CoC values for virtual objects are inferred through a linear relationship between its signed CoC map and depth, and realistic lens blur is rendered using a neural reblurring network. Our method provides flexible and practical solution for real-world applications. Experimental results demonstrate that our method achieves high-fidelity compositing with realistic defocus effects, outperforming state-of-the-art techniques in both qualitative and quantitative evaluations.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ— éœ€å‚æ•°çš„ç¥ç»é•œå¤´æ¨¡ç³Šæ¸²æŸ“æ–¹æ³• Parameter-Free Neural Lens Blur Renderingï¼Œç”¨äºå®ç°è™šæ‹Ÿç‰©ä½“ä¸çœŸå®åœºæ™¯çš„é«˜ä¿çœŸåˆæˆã€‚é’ˆå¯¹ç°æœ‰æµç¨‹é«˜åº¦ä¾èµ–ç„¦è·ã€å­”å¾„åŠåœºæ™¯æ·±åº¦ç­‰éš¾ä»¥è·å–çš„ç›¸æœºå‚æ•°è¿™ä¸€ç—›ç‚¹ï¼Œè¯¥æ–¹æ¡ˆç›´æ¥ä» RGB å›¾åƒä¸­ä¼°è®¡å¼¥æ•£åœ† Circle of Confusion (CoC) æ˜ å°„å›¾ã€‚é€šè¿‡å»ºç«‹å¸¦ç¬¦å· CoC å›¾ä¸æ·±åº¦ä¹‹é—´çš„çº¿æ€§å…³ç³»ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ¨æ–­è™šæ‹Ÿç‰©ä½“çš„æ¨¡ç³Šæ•°å€¼ï¼Œå¹¶åˆ©ç”¨ç¥ç»é‡æ¨¡ç³Šç½‘ç»œ neural reblurring network æ¸²æŸ“å‡ºçœŸå®çš„è™šåŒ–æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°ä¸­å‡ä¼˜äºç°æœ‰çš„ State-of-the-art æŠ€æœ¯ï¼Œä¸ºæ··åˆç°å®åº”ç”¨æä¾›äº†ä¸€ç§æ— éœ€å…ƒæ•°æ®ä¸”æ›´å…·æ™®é€‚æ€§çš„å®ç”¨è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ISMAR 2025 with oral presentation. 10 pages, 11 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.17014v1",
      "published_date": "2025-11-21 07:32:05 UTC",
      "updated_date": "2025-11-21 07:32:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:10:23.554774+00:00"
    },
    {
      "arxiv_id": "2511.17012v1",
      "title": "Supervised Fine Tuning of Large Language Models for Domain Specific Knowledge Graph Construction:A Case Study on Hunan's Historical Celebrities",
      "title_zh": "é¢å‘ç‰¹å®šé¢†åŸŸçŸ¥è¯†å›¾è°±æ„å»ºçš„å¤§è¯­è¨€æ¨¡å‹ç›‘ç£å¾®è°ƒï¼šä»¥ Hunan å†å²åäººä¸ºä¾‹",
      "authors": [
        "Junjie Hao",
        "Chun Wang",
        "Ying Qiao",
        "Qiuyue Zuo",
        "Qiya Song",
        "Hua Ma",
        "Xieping Gao"
      ],
      "abstract": "Large language models and knowledge graphs offer strong potential for advancing research on historical culture by supporting the extraction, analysis, and interpretation of cultural heritage. Using Hunan's modern historical celebrities shaped by Huxiang culture as a case study, pre-trained large models can help researchers efficiently extract key information, including biographical attributes, life events, and social relationships, from textual sources and construct structured knowledge graphs. However, systematic data resources for Hunan's historical celebrities remain limited, and general-purpose models often underperform in domain knowledge extraction and structured output generation in such low-resource settings. To address these issues, this study proposes a supervised fine-tuning approach for enhancing domain-specific information extraction. First, we design a fine-grained, schema-guided instruction template tailored to the Hunan historical celebrities domain and build an instruction-tuning dataset to mitigate the lack of domain-specific training corpora. Second, we apply parameter-efficient instruction fine-tuning to four publicly available large language models - Qwen2.5-7B, Qwen3-8B, DeepSeek-R1-Distill-Qwen-7B, and Llama-3.1-8B-Instruct - and develop evaluation criteria for assessing their extraction performance. Experimental results show that all models exhibit substantial performance gains after fine-tuning. Among them, Qwen3-8B achieves the strongest results, reaching a score of 89.3866 with 100 samples and 50 training iterations. This study provides new insights into fine-tuning vertical large language models for regional historical and cultural domains and highlights their potential for cost-effective applications in cultural heritage knowledge extraction and knowledge graph construction.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ¹–å—å†å²äººç‰©é¢†åŸŸæ•°æ®èµ„æºæœ‰é™ã€é€šç”¨æ¨¡å‹åœ¨å‚ç›´é¢†åŸŸä¿¡æ¯æŠ½å–å’Œç»“æ„åŒ–è¾“å‡ºèƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç›‘ç£å¾®è°ƒï¼ˆSupervised Fine-Tuning, SFTï¼‰æ–¹æ³•ä»¥æ„å»ºé¢†åŸŸç‰¹å®šçš„çŸ¥è¯†å›¾è°±ï¼ˆKnowledge Graphï¼‰ã€‚ç ”ç©¶è€…è®¾è®¡äº†ç²¾ç»†åŒ–çš„æ¨¡å¼å¼•å¯¼æŒ‡ä»¤æ¨¡æ¿ï¼ˆschema-guided instruction templateï¼‰å¹¶æ„å»ºäº†ä¸“é—¨çš„æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ï¼Œä»¥è§£å†³è®­ç»ƒè¯­æ–™åŒ®ä¹çš„æŒ‘æˆ˜ã€‚å®éªŒå¯¹ Qwen2.5-7Bã€Qwen3-8Bã€DeepSeek-R1-Distill-Qwen-7B å’Œ Llama-3.1-8B-Instruct ç­‰å¤šä¸ªå¼€æºæ¨¡å‹è¿›è¡Œäº†å‚æ•°é«˜æ•ˆæŒ‡ä»¤å¾®è°ƒï¼ˆparameter-efficient instruction fine-tuningï¼‰åŠæ€§èƒ½è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¾®è°ƒåçš„æ¨¡å‹åœ¨ä¿¡æ¯æŠ½å–ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½å¢ç›Šï¼Œå…¶ä¸­ Qwen3-8B åœ¨ 100 ä¸ªæ ·æœ¬å’Œ 50 æ¬¡è®­ç»ƒè¿­ä»£ä¸‹è¾¾åˆ°äº† 89.3866 çš„æœ€é«˜è¯„åˆ†ã€‚è¯¥ç ”ç©¶éªŒè¯äº†å‚ç›´é¢†åŸŸå¤§è¯­è¨€æ¨¡å‹åœ¨åŒºåŸŸå†å²æ–‡åŒ–é—äº§ä¿æŠ¤ä¸­çš„åº”ç”¨æ½œåŠ›ï¼Œä¸ºä½æˆæœ¬æ„å»ºæ–‡åŒ–é—äº§çŸ¥è¯†å›¾è°±æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17012v1",
      "published_date": "2025-11-21 07:30:20 UTC",
      "updated_date": "2025-11-21 07:30:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:10:39.871479+00:00"
    },
    {
      "arxiv_id": "2511.17006v1",
      "title": "Budget-Aware Tool-Use Enables Effective Agent Scaling",
      "title_zh": "é¢„ç®—æ„ŸçŸ¥å‹å·¥å…·è°ƒç”¨åŠ©åŠ›é«˜æ•ˆæ™ºèƒ½ä½“æ‰©å±•",
      "authors": [
        "Tengxiao Liu",
        "Zifeng Wang",
        "Jin Miao",
        "I-Hung Hsu",
        "Jun Yan",
        "Jiefeng Chen",
        "Rujun Han",
        "Fangyuan Xu",
        "Yanfei Chen",
        "Ke Jiang",
        "Samira Daruki",
        "Yi Liang",
        "William Yang Wang",
        "Tomas Pfister",
        "Chen-Yu Lee"
      ],
      "abstract": "Scaling test-time computation improves performance across different tasks on large language models (LLMs), which has also been extended to tool-augmented agents. For these agents, scaling involves not only \"thinking\" in tokens but also \"acting\" via tool calls. The number of tool calls directly bounds the agent's interaction with the external environment. However, we find that simply granting agents a larger tool-call budget fails to improve performance, as they lack \"budget awareness\" and quickly hit a performance ceiling. To address this, we study how to scale such agents effectively under explicit tool-call budgets, focusing on web search agents. We first introduce the Budget Tracker, a lightweight plug-in that provides the agent with continuous budget awareness, enabling simple yet effective scaling. We further develop BATS (Budget Aware Test-time Scaling), an advanced framework that leverages this awareness to dynamically adapt its planning and verification strategy, deciding whether to \"dig deeper\" on a promising lead or \"pivot\" to new paths based on remaining resources. To analyze cost-performance scaling in a controlled manner, we formalize a unified cost metric that jointly accounts for token and tool consumption. We provide the first systematic study on budget-constrained agents, showing that budget-aware methods produce more favorable scaling curves and push the cost-performance Pareto frontier. Our work offers empirical insights toward a more transparent and principled understanding of scaling in tool-augmented agents.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å·¥å…·å¢å¼ºæ™ºèƒ½ä½“åœ¨æµ‹è¯•é˜¶æ®µè®¡ç®—ç¼©æ”¾(test-time scaling)çš„æœ‰æ•ˆæ€§ï¼Œå‘ç°ä»…å¢åŠ å·¥å…·è°ƒç”¨é¢„ç®—è€Œç¼ºä¹é¢„ç®—æ„è¯†(budget awareness)ä¼šå¯¼è‡´æ€§èƒ½è¿…é€Ÿè¾¾åˆ°ç“¶é¢ˆã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡å¼•å…¥äº†Budget Trackeræ’ä»¶ä»¥æä¾›æŒç»­çš„é¢„ç®—æ„ŸçŸ¥ï¼Œå¹¶å¼€å‘äº†BATS (Budget Aware Test-time Scaling) æ¡†æ¶ï¼Œä½¿æ™ºèƒ½ä½“èƒ½æ ¹æ®å‰©ä½™èµ„æºåŠ¨æ€ä¼˜åŒ–è§„åˆ’ä¸éªŒè¯ç­–ç•¥ï¼Œåœ¨æ·±å…¥æ¢ç´¢å’Œè½¬æ¢æ–¹å‘ä¹‹é—´åšå‡ºæƒè¡¡ã€‚ç ”ç©¶è¿˜å»ºç«‹äº†ä¸€ä¸ªæ•´åˆTokenå’Œå·¥å…·æ¶ˆè€—çš„ç»Ÿä¸€æˆæœ¬åº¦é‡æŒ‡æ ‡ï¼Œç”¨äºç³»ç»Ÿåˆ†ææˆæœ¬ä¸æ€§èƒ½çš„å…³ç³»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§å…·å¤‡é¢„ç®—æ„è¯†çš„æ–¹æ³•èƒ½äº§ç”Ÿæ›´ä¼˜çš„ç¼©æ”¾æ›²çº¿ï¼Œå¹¶æ˜¾è‘—æå‡äº†æˆæœ¬-æ€§èƒ½å¸•ç´¯æ‰˜å‰æ²¿(Pareto frontier)ã€‚è¯¥å·¥ä½œä¸ºç†è§£å·¥å…·å¢å¼ºæ™ºèƒ½ä½“çš„ç¼©æ”¾æœºåˆ¶æä¾›äº†é€æ˜ä¸”åŸåˆ™æ€§çš„å®è¯è§è§£ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17006v1",
      "published_date": "2025-11-21 07:18:55 UTC",
      "updated_date": "2025-11-21 07:18:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:10:21.281174+00:00"
    },
    {
      "arxiv_id": "2511.17005v2",
      "title": "FLUID: Training-Free Face De-identification via Latent Identity Substitution",
      "title_zh": "FLUIDï¼šåŸºäºæ½œåœ¨èº«ä»½æ›¿æ¢çš„æ— éœ€è®­ç»ƒäººè„¸å»æ ‡è¯†åŒ–",
      "authors": [
        "Jinhyeong Park",
        "Shaheryar Muhammad",
        "Seangmin Lee",
        "Jong Taek Lee",
        "Soon Ki Jung"
      ],
      "abstract": "Current face de-identification methods that replace identifiable cues in the face region with other sacrifices utilities contributing to realism, such as age and gender. To retrieve the damaged realism, we present FLUID (Face de-identification in the Latent space via Utility-preserving Identity Displacement), a single-input face de-identification framework that directly replaces identity features in the latent space of a pretrained diffusion model without affecting the model's weights. We reinterpret face de-identification as an image editing task in the latent h-space of a pretrained unconditional diffusion model. Our framework estimates identity-editing directions through optimization guided by loss functions that encourage attribute preservation while suppressing identity signals. We further introduce both linear and geodesic (tangent-based) editing schemes to effectively navigate the latent manifold. Experiments on CelebA-HQ and FFHQ show that FLUID achieves a superior balance between identity suppression and attribute preservation, outperforming existing de-identification approaches in both qualitative and quantitative evaluations.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FLUID (Face de-identification in the Latent space via Utility-preserving Identity Displacement)ï¼Œè¿™æ˜¯ä¸€ç§å•è¾“å…¥çš„äººè„¸å»æ ‡è¯†åŒ– (Face de-identification) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨å»é™¤èº«ä»½ç‰¹å¾æ—¶å¾€å¾€ä¼šç‰ºç‰²å¹´é¾„ã€æ€§åˆ«ç­‰å›¾åƒçœŸå®æ€§å±æ€§çš„é—®é¢˜ã€‚FLUIDå°†å»æ ‡è¯†åŒ–é‡æ–°å®šä¹‰ä¸ºé¢„è®­ç»ƒæ— æ¡ä»¶æ‰©æ•£æ¨¡å‹ (unconditional diffusion model) æ½œåœ¨hç©ºé—´ (latent h-space) ä¸­çš„å›¾åƒç¼–è¾‘ä»»åŠ¡ï¼Œå®ç°äº†æ— éœ€è®­ç»ƒä¸”ä¸æ”¹å˜æ¨¡å‹æƒé‡çš„èº«ä»½æ›¿æ¢ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¼˜åŒ–æŸå¤±å‡½æ•°æ¥ä¼°è®¡èº«ä»½ç¼–è¾‘æ–¹å‘ï¼Œåœ¨æŠ‘åˆ¶èº«ä»½ä¿¡å·çš„åŒæ—¶é¼“åŠ±å±æ€§ä¿ç•™ï¼Œå¹¶å¼•å…¥äº†çº¿æ€§å’Œæµ‹åœ°çº¿ (geodesic, tangent-based) ç¼–è¾‘æ–¹æ¡ˆä»¥æœ‰æ•ˆå¯¼èˆªæ½œåœ¨æµå½¢ (latent manifold)ã€‚åœ¨CelebA-HQå’ŒFFHQæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒFLUIDåœ¨èº«ä»½æŠ‘åˆ¶ä¸å±æ€§ä¿ç•™ä¹‹é—´å–å¾—äº†å“è¶Šçš„å¹³è¡¡ã€‚å®šæ€§å’Œå®šé‡è¯„ä¼°å‡æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„å»æ ‡è¯†åŒ–æŠ€æœ¯ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17005v2",
      "published_date": "2025-11-21 07:18:37 UTC",
      "updated_date": "2026-01-06 06:33:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:11:12.571753+00:00"
    },
    {
      "arxiv_id": "2511.17676v1",
      "title": "LLM and Agent-Driven Data Analysis: A Systematic Approach for Enterprise Applications and System-level Deployment",
      "title_zh": "LLM ä¸æ™ºèƒ½ä½“é©±åŠ¨çš„æ•°æ®åˆ†æï¼šé¢å‘ä¼ä¸šçº§åº”ç”¨ä¸ç³»ç»Ÿçº§éƒ¨ç½²çš„ç³»ç»ŸåŒ–æ–¹æ³•",
      "authors": [
        "Xi Wang",
        "Xianyao Ling",
        "Kun Li",
        "Gang Yin",
        "Liang Zhang",
        "Jiang Wu",
        "Annie Wang",
        "Weizhe Wang"
      ],
      "abstract": "The rapid progress in Generative AI and Agent technologies is profoundly transforming enterprise data management and analytics. Traditional database applications and system deployment are fundamentally impacted by AI-driven tools, such as Retrieval-Augmented Generation (RAG) and vector database technologies, which provide new pathways for semantic querying over enterprise knowledge bases. In the meantime, data security and compliance are top priorities for organizations adopting AI technologies. For enterprise data analysis, SQL generations powered by large language models (LLMs) and AI agents, has emerged as a key bridge connecting natural language with structured data, effectively lowering the barrier to enterprise data access and improving analytical efficiency. This paper focuses on enterprise data analysis applications and system deployment, covering a range of innovative frameworks, enabling complex query understanding, multi-agent collaboration, security verification, and computational efficiency. Through representative use cases, key challenges related to distributed deployment, data security, and inherent difficulties in SQL generation tasks are discussed.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨å¤§è¯­è¨€æ¨¡å‹(LLM)å’Œæ™ºèƒ½ä½“(Agent)é©±åŠ¨ä¸‹ï¼Œä¼ä¸šçº§æ•°æ®åˆ†æä¸ç³»ç»Ÿéƒ¨ç½²çš„ç³»ç»ŸåŒ–æ–¹æ³•ï¼Œåˆ†æäº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½(Generative AI)å¦‚ä½•æ·±åˆ»å˜é©ä¼ä¸šæ•°æ®ç®¡ç†ã€‚è®ºæ–‡é‡ç‚¹ç ”ç©¶äº†æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)å’Œå‘é‡æ•°æ®åº“(Vector Database)æŠ€æœ¯ï¼Œä¸ºä¼ä¸šçŸ¥è¯†åº“çš„è¯­ä¹‰æŸ¥è¯¢æä¾›äº†æ–°è·¯å¾„ã€‚é€šè¿‡LLMå’ŒAI Agentsé©±åŠ¨çš„SQL generationæŠ€æœ¯ï¼Œè¯¥ç ”ç©¶æˆåŠŸæ­å»ºäº†è‡ªç„¶è¯­è¨€ä¸ç»“æ„åŒ–æ•°æ®ä¹‹é—´çš„æ¡¥æ¢ï¼Œæœ‰æ•ˆé™ä½äº†æ•°æ®è®¿é—®é—¨æ§›å¹¶æå‡äº†åˆ†ææ•ˆç‡ã€‚æ–‡ä¸­æ¶µç›–äº†ä¸€ç³»åˆ—åˆ›æ–°æ¡†æ¶ï¼Œèƒ½å¤Ÿæ”¯æŒå¤æ‚çš„æŸ¥è¯¢ç†è§£(Query Understanding)ã€å¤šæ™ºèƒ½ä½“åä½œ(Multi-agent Collaboration)ã€å®‰å…¨éªŒè¯å’Œè®¡ç®—æ•ˆç‡ä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡ä»£è¡¨æ€§æ¡ˆä¾‹æ·±å…¥è®¨è®ºäº†åˆ†å¸ƒå¼éƒ¨ç½²ã€æ•°æ®å®‰å…¨ä»¥åŠSQL generationä»»åŠ¡ä¸­çš„å›ºæœ‰æŒ‘æˆ˜ï¼Œä¸ºä¼ä¸šçº§AIåº”ç”¨çš„è½åœ°æä¾›äº†ç³»ç»Ÿæ€§å‚è€ƒã€‚",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.DB",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17676v1",
      "published_date": "2025-11-21 07:16:31 UTC",
      "updated_date": "2025-11-21 07:16:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:10:39.380134+00:00"
    },
    {
      "arxiv_id": "2511.16997v1",
      "title": "MirrorMind: Empowering OmniScientist with the Expert Perspectives and Collective Knowledge of Human Scientists",
      "title_zh": "MirrorMindï¼šä»¥äººç±»ç§‘å­¦å®¶çš„ä¸“å®¶è§†è§’ä¸é›†ä½“çŸ¥è¯†èµ‹èƒ½ OmniScientist",
      "authors": [
        "Qingbin Zeng",
        "Bingbing Fan",
        "Zhiyu Chen",
        "Sijian Ren",
        "Zhilun Zhou",
        "Xuhua Zhang",
        "Yuanyi Zhen",
        "Fengli Xu",
        "Yong Li",
        "Tie-Yan Liu"
      ],
      "abstract": "The emergence of AI Scientists has demonstrated remarkable potential in automating scientific research. However, current approaches largely conceptualize scientific discovery as a solitary optimization or search process, overlooking that knowledge production is inherently a social and historical endeavor. Human scientific insight stems from two distinct yet interconnected sources. First is the individual cognitive trajectory, where a researcher's unique insight is shaped by their evolving research history and stylistic preferences; another is the collective disciplinary memory, where knowledge is sedimented into vast, interconnected networks of citations and concepts. Existing LLMs still struggle to represent these structured, high-fidelity cognitive and social contexts. To bridge this gap, we introduce MirrorMind, a hierarchical cognitive architecture that integrates dual-memory representations within a three-level framework. The Individual Level constructs high-fidelity cognitive models of individual researchers by capturing their episodic, semantic, and persona memories; the Domain Level maps collective knowledge into structured disciplinary concept graphs; and the Interdisciplinary Level that acts as an orthogonal orchestration engine. Crucially, our architecture separates memory storage from agentic execution, enabling AI scientist agents to flexibly access individual memories for unique perspectives or collective structures to reason. We evaluate MirrorMind across four comprehensive tasks, including author-level cognitive simulation, complementary reasoning, cross-disciplinary collaboration promotion, and multi-agent scientific problem solving. The results show that by integrating individual cognitive depth with collective disciplinary breadth, MirrorMind moves beyond simple fact retrieval toward structural, personalized, and insight-generating scientific reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MirrorMindï¼Œä¸€ç§åˆ†å±‚è®¤çŸ¥æ¶æ„ (hierarchical cognitive architecture)ï¼Œæ—¨åœ¨é€šè¿‡æ•´åˆäººç±»ç§‘å­¦å®¶çš„ä¸ªä½“ä¸“å®¶è§†è§’ä¸é›†ä½“å­¦ç§‘çŸ¥è¯†æ¥æå‡ AI Scientist çš„ç ”ç©¶èƒ½åŠ›ã€‚MirrorMind å…‹æœäº†ç°æœ‰å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å†ç°ç»“æ„åŒ–é«˜ä¿çœŸè®¤çŸ¥å’Œç¤¾ä¼šèƒŒæ™¯æ–¹é¢çš„å±€é™ï¼Œæ„å»ºäº†åŒ…å«ä¸ªäººå±‚ (Individual Level)ã€é¢†åŸŸå±‚ (Domain Level) å’Œè·¨å­¦ç§‘å±‚ (Interdisciplinary Level) çš„ä¸‰å±‚æ¡†æ¶ã€‚è¯¥æ¶æ„é€šè¿‡æ•æ‰ç ”ç©¶è€…çš„æƒ…èŠ‚ã€è¯­ä¹‰å’Œäººæ ¼è®°å¿†æ„å»ºé«˜ä¿çœŸè®¤çŸ¥æ¨¡å‹ï¼Œå¹¶å°†é›†ä½“çŸ¥è¯†æ˜ å°„ä¸ºå­¦ç§‘æ¦‚å¿µå›¾ï¼Œå®ç°äº†å†…å­˜å­˜å‚¨ä¸æ‰§è¡Œé€»è¾‘çš„åˆ†ç¦»ã€‚è¿™ç§è®¾è®¡ä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿçµæ´»è°ƒç”¨ä¸ªäººç‹¬ç‰¹è§†è§’æˆ–é›†ä½“ç»“æ„åŒ–çŸ¥è¯†è¿›è¡Œæ¨ç†ã€‚åœ¨ä½œè€…çº§è®¤çŸ¥æ¨¡æ‹Ÿã€è·¨å­¦ç§‘åä½œå’Œå¤šæ™ºèƒ½ä½“ç§‘å­¦é—®é¢˜è§£å†³ç­‰ä»»åŠ¡çš„è¯„ä¼°ä¸­ï¼ŒMirrorMind å±•ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚å®éªŒè¯æ˜ï¼ŒMirrorMind æˆåŠŸæ¨åŠ¨äº† AI ä»ç®€å•çš„äº‹å®æ£€ç´¢å‘ç»“æ„åŒ–ã€ä¸ªæ€§åŒ–åŠäº§ç”Ÿæ´å¯ŸåŠ›çš„ç§‘å­¦æ¨ç†è½¬å˜ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "26 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.16997v1",
      "published_date": "2025-11-21 07:05:26 UTC",
      "updated_date": "2025-11-21 07:05:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:11:01.765030+00:00"
    },
    {
      "arxiv_id": "2511.16979v2",
      "title": "The Finer the Better: Towards Granular-aware Open-set Domain Generalization",
      "title_zh": "æ„ˆç»†æ„ˆä½³ï¼šé¢å‘ç»†ç²’åº¦æ„ŸçŸ¥çš„å¼€æ”¾é›†é¢†åŸŸæ³›åŒ–",
      "authors": [
        "Yunyun Wang",
        "Zheng Duan",
        "Xinyue Liao",
        "Ke-Jia Chen",
        "Songcan Chen"
      ],
      "abstract": "Open-Set Domain Generalization (OSDG) tackles the realistic scenario where deployed models encounter both domain shifts and novel object categories. Despite impressive progress with vision-language models like CLIP, existing methods still fall into the dilemma between structural risk of known-classes and open-space risk from unknown-classes, and easily suffers from over-confidence, especially when distinguishing ``hard unknowns\" that share fine-grained visual similarities with known classes. To this end, we propose a Semantic-enhanced CLIP (SeeCLIP) framework that explicitly addresses this dilemma through fine-grained semantic enhancement. In SeeCLIP, we propose a semantic-aware prompt enhancement module to decompose images into discriminative semantic tokens, enabling nuanced vision-language alignment beyond coarse category labels. To position unknown prompts effectively, we introduce duplex contrastive learning with complementary objectives, that is, repulsion to maintain separability from known classes, and cohesion to preserve semantic proximity. Further, our semantic-guided diffusion module synthesizes pseudo-unknowns by perturbing extracted semantic tokens, generating challenging samples that are visually similar to known classes yet exhibit key local differences. These hard negatives force the model to learn finer decision boundaries. Extensive experiments across five benchmarks demonstrate consistent improvements of 3% accuracy and 5% H-score over state-of-the-art methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Semantic-enhanced CLIP (SeeCLIP)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¼€æ”¾é›†é¢†åŸŸæ³›åŒ–(Open-Set Domain Generalization, OSDG)ä¸­æ¨¡å‹éš¾ä»¥åŒºåˆ†ä¸å·²çŸ¥ç±»å…·æœ‰ç»†ç²’åº¦è§†è§‰ç›¸ä¼¼æ€§çš„â€œå›°éš¾æœªçŸ¥ç±»â€(hard unknowns)çš„é—®é¢˜ã€‚SeeCLIPé€šè¿‡è¯­ä¹‰æ„ŸçŸ¥æç¤ºå¢å¼ºæ¨¡å—å°†å›¾åƒåˆ†è§£ä¸ºå…·æœ‰åˆ¤åˆ«åŠ›çš„è¯­ä¹‰æ ‡è®°(semantic tokens)ï¼Œå®ç°äº†è¶…è¶Šç²—ç²’åº¦ç±»åˆ«æ ‡ç­¾çš„ç²¾ç»†è§†è§‰è¯­è¨€å¯¹é½ã€‚ä¸ºäº†ä¼˜åŒ–æœªçŸ¥ç±»æç¤ºçš„å®šä½ï¼Œç ”ç©¶å¼•å…¥äº†åŒå‘å¯¹æ¯”å­¦ä¹ (duplex contrastive learning)ï¼Œé€šè¿‡æ’æ–¥ä¸å‡èšçš„äº’è¡¥ç›®æ ‡ä¿æŒå·²çŸ¥ç±»ä¸æœªçŸ¥ç±»çš„å¯åˆ†æ€§åŠè¯­ä¹‰äº²è¿‘åº¦ã€‚æ­¤å¤–ï¼Œè¯­ä¹‰å¼•å¯¼æ‰©æ•£æ¨¡å—(semantic-guided diffusion)é€šè¿‡æ‰°åŠ¨è¯­ä¹‰æ ‡è®°åˆæˆä¼ªæœªçŸ¥ç±»æ ·æœ¬ï¼Œåˆ©ç”¨è¿™äº›å…·æœ‰å±€éƒ¨å·®å¼‚çš„å›°éš¾è´Ÿæ ·æœ¬å¼ºåˆ¶æ¨¡å‹å­¦ä¹ æ›´ç²¾ç»†çš„å†³ç­–è¾¹ç•Œã€‚åœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSeeCLIPç›¸æ¯”ç°æœ‰æœ€ä¼˜æ–¹æ³•åœ¨å‡†ç¡®ç‡å’ŒH-scoreä¸Šåˆ†åˆ«æå‡äº†3%å’Œ5%ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹åœ¨å¤æ‚å¼€æ”¾ç¯å¢ƒä¸‹çš„é²æ£’æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "9 pages,3 figures,aaai2026",
      "pdf_url": "https://arxiv.org/pdf/2511.16979v2",
      "published_date": "2025-11-21 06:19:19 UTC",
      "updated_date": "2025-12-12 06:30:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:10:54.580770+00:00"
    },
    {
      "arxiv_id": "2511.16964v1",
      "title": "Optimizing PyTorch Inference with LLM-Based Multi-Agent Systems",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„ PyTorch æ¨ç†ä¼˜åŒ–",
      "authors": [
        "Kirill Nagaitsev",
        "Luka Grbcic",
        "Samuel Williams",
        "Costin Iancu"
      ],
      "abstract": "Maximizing performance on available GPU hardware is an ongoing challenge for modern AI inference systems. Traditional approaches include writing custom GPU kernels and using specialized model compilers to tune high-level code for specific GPU targets. Recent work shows that LLM-based multi-agent systems can effectively perform such tuning, often outperforming existing compilers and eliminating the need for manual kernel development. However, the dynamics of multi-agent systems for this task remain unexplored. In this work, we present a logical framework for comparing multi-agent PyTorch optimization systems. Our evaluation shows that exploit-heavy strategies perform best when paired with error-fixing agents, and that performance correlates with the granularity of optimization steps. The best implementation achieves an average 2.88x speedup on an H100 GPU across diverse tasks in KernelBench, a benchmark suite covering a range of machine learning architectures in PyTorch.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨åŸºäºLLMçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¼˜åŒ–PyTorchæ¨ç†æ€§èƒ½ï¼Œä»¥è§£å†³ç°ä»£AIæ¨ç†ç³»ç»Ÿåœ¨æœ€å¤§åŒ–GPUç¡¬ä»¶æ•ˆèƒ½æ–¹é¢é¢ä¸´çš„æŒ‘æˆ˜ã€‚ç ”ç©¶è€…æå‡ºäº†ä¸€ä¸ªç”¨äºæ¯”è¾ƒå¤šæ™ºèƒ½ä½“PyTorchä¼˜åŒ–ç³»ç»Ÿçš„é€»è¾‘æ¡†æ¶ï¼Œæ—¨åœ¨æ›¿ä»£ä¼ ç»Ÿçš„å®šåˆ¶GPU kernelsç¼–å†™å’Œæ¨¡å‹ç¼–è¯‘æ–¹æ³•ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œåˆ©ç”¨é‡å¼€å‘ï¼ˆexploit-heavyï¼‰ç­–ç•¥å¹¶ç»“åˆé”™è¯¯ä¿®å¤æ™ºèƒ½ä½“ï¼ˆerror-fixing agentsï¼‰åœ¨ä¼˜åŒ–ä»»åŠ¡ä¸­è¡¨ç°æœ€ä½³ã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°ï¼Œç³»ç»Ÿçš„ä¼˜åŒ–æ€§èƒ½ä¸ä¼˜åŒ–æ­¥éª¤çš„ç²’åº¦ï¼ˆgranularityï¼‰æ˜¾è‘—ç›¸å…³ã€‚åœ¨è¦†ç›–å¤šç§æœºå™¨å­¦ä¹ æ¶æ„çš„KernelBenchåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æœ€ä½³å®ç°æ–¹æ¡ˆåœ¨H100 GPUä¸Šå®ç°äº†å¹³å‡2.88å€çš„åŠ é€Ÿã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨è‡ªåŠ¨åŒ–æ€§èƒ½è°ƒä¼˜æ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œä¸ä»…åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰ç¼–è¯‘å™¨ï¼Œè¿˜æ¶ˆé™¤äº†ç¹é‡çš„æ‰‹åŠ¨å¼€å‘éœ€æ±‚ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16964v1",
      "published_date": "2025-11-21 05:37:38 UTC",
      "updated_date": "2025-11-21 05:37:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:11:40.468502+00:00"
    },
    {
      "arxiv_id": "2511.16961v1",
      "title": "Comparing verbal, visual and combined explanations for Bayesian Network inferences",
      "title_zh": "è´å¶æ–¯ç½‘ç»œæ¨ç†çš„è¨€è¯­ã€è§†è§‰åŠç»„åˆè§£é‡Šæ¯”è¾ƒ",
      "authors": [
        "Erik P. Nyberg",
        "Steven Mascaro",
        "Ingrid Zukerman",
        "Michael Wybrow",
        "Duc-Minh Vo",
        "Ann Nicholson"
      ],
      "abstract": "Bayesian Networks (BNs) are an important tool for assisting probabilistic reasoning, but despite being considered transparent models, people have trouble understanding them. Further, current User Interfaces (UIs) still do not clarify the reasoning of BNs. To address this problem, we have designed verbal and visual extensions to the standard BN UI, which can guide users through common inference patterns.\n  We conducted a user study to compare our verbal, visual and combined UI extensions, and a baseline UI. Our main findings are: (1) users did better with all three types of extensions than with the baseline UI for questions about the impact of an observation, the paths that enable this impact, and the way in which an observation influences the impact of other observations; and (2) using verbal and visual modalities together is better than using either modality alone for some of these question types.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”¨æˆ·éš¾ä»¥ç†è§£è´å¶æ–¯ç½‘ç»œ(Bayesian Networks)æ¨ç†é€»è¾‘çš„é—®é¢˜ï¼Œè®¾è®¡äº†å£å¤´(verbal)å’Œè§†è§‰(visual)ä¸¤ç§ç”¨æˆ·ç•Œé¢(User Interfaces)æ‰©å±•åŠŸèƒ½ï¼Œæ—¨åœ¨å¼•å¯¼ç”¨æˆ·ç†è§£å¤æ‚çš„æ¨ç†æ¨¡å¼ã€‚ç ”ç©¶é€šè¿‡å¯¹æ¯”å®éªŒè¯„ä¼°äº†å£å¤´ã€è§†è§‰ã€ç»“åˆ(combined)ä»¥åŠåŸºå‡†(baseline)ç•Œé¢çš„è¡¨ç°ï¼Œç»“æœæ˜¾ç¤ºè¿™ä¸‰ç§æ‰©å±•æ–¹å¼åœ¨è§£é‡Šè§‚å¯Ÿç»“æœçš„å½±å“ã€è·¯å¾„åŠå…¶ç›¸äº’ä½œç”¨æ–¹é¢å‡ä¼˜äºä¼ ç»ŸåŸºå‡†ç•Œé¢ã€‚å®éªŒè¿›ä¸€æ­¥å‘ç°ï¼Œå°†å£å¤´ä¸è§†è§‰ä¸¤ç§æ¨¡æ€ç»“åˆä½¿ç”¨ï¼Œåœ¨æŸäº›ç‰¹å®šé—®é¢˜ç±»å‹ä¸Šçš„æ•ˆæœä¼˜äºå•ä¸€æ¨¡æ€ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†å¤šæ¨¡æ€è§£é‡Šåœ¨å¢å¼ºæ¦‚ç‡æ¨¡å‹é€æ˜åº¦å’Œè¾…åŠ©ç”¨æˆ·æ¨ç†æ–¹é¢çš„æ˜¾è‘—ä»·å€¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "26 pages total, 12 pages main, 14 pages for 5 appendices",
      "pdf_url": "https://arxiv.org/pdf/2511.16961v1",
      "published_date": "2025-11-21 05:25:23 UTC",
      "updated_date": "2025-11-21 05:25:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:11:30.274757+00:00"
    },
    {
      "arxiv_id": "2511.17673v3",
      "title": "Bridging Symbolic Control and Neural Reasoning in LLM Agents: The Structured Cognitive Loop",
      "title_zh": "è¿æ¥ LLM æ™ºèƒ½ä½“ä¸­çš„ç¬¦å·æ§åˆ¶ä¸ç¥ç»æ¨ç†ï¼šç»“æ„åŒ–è®¤çŸ¥ç¯è·¯",
      "authors": [
        "Myung Ho Kim"
      ],
      "abstract": "Large language model agents suffer from fundamental architectural problems: entangled reasoning and execution, memory volatility, and uncontrolled action sequences. We introduce Structured Cognitive Loop (SCL), a modular architecture that explicitly separates agent cognition into five phases: Retrieval, Cognition, Control, Action, and Memory (R-CCAM). At the core of SCL is Soft Symbolic Control, an adaptive governance mechanism that applies symbolic constraints to probabilistic inference, preserving neural flexibility while restoring the explainability and controllability of classical symbolic systems. Through empirical validation on multi-step conditional reasoning tasks, we demonstrate that SCL achieves zero policy violations, eliminates redundant tool calls, and maintains complete decision traceability. These results address critical gaps in existing frameworks such as ReAct, AutoGPT, and memory-augmented approaches. Our contributions are threefold: (1) we situate SCL within the taxonomy of hybrid intelligence, differentiating it from prompt-centric and memory-only approaches; (2) we formally define Soft Symbolic Control and contrast it with neuro-symbolic AI; and (3) we derive three design principles for trustworthy agents: modular decomposition, adaptive symbolic governance, and transparent state management. We provide a complete open-source implementation demonstrating the R-CCAM loop architecture, alongside a live GPT-4o-powered travel planning agent. By connecting expert system principles with modern LLM capabilities, this work offers a practical and theoretically grounded path toward reliable, explainable, and governable AI agents.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“ï¼ˆLLM agentsï¼‰å­˜åœ¨çš„æ¨ç†ä¸æ‰§è¡Œçº ç¼ ã€è®°å¿†æ³¢åŠ¨åŠè¡ŒåŠ¨åºåˆ—ä¸å—æ§ç­‰æ ¸å¿ƒæ¶æ„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºStructured Cognitive Loopï¼ˆSCLï¼‰çš„æ¨¡å—åŒ–æ¶æ„ã€‚SCLé€šè¿‡æ£€ç´¢ã€è®¤çŸ¥ã€æ§åˆ¶ã€è¡ŒåŠ¨å’Œè®°å¿†ï¼ˆR-CCAMï¼‰äº”ä¸ªæ˜¾å¼é˜¶æ®µåˆ†ç¦»äº†æ™ºèƒ½ä½“è®¤çŸ¥è¿‡ç¨‹ï¼Œå¹¶å¼•å…¥Soft Symbolic Controlä½œä¸ºè‡ªé€‚åº”æ²»ç†æœºåˆ¶ï¼Œå°†ç¬¦å·çº¦æŸåº”ç”¨äºæ¦‚ç‡æ¨ç†ï¼Œä»è€Œåœ¨ä¿ç•™ç¥ç»æ¨¡å‹çµæ´»æ€§çš„åŒæ—¶å¢å¼ºäº†ç³»ç»Ÿçš„å¯è§£é‡Šæ€§ä¸å¯æ§æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSCLåœ¨å¤šæ­¥æ¡ä»¶æ¨ç†ä»»åŠ¡ä¸­å®ç°äº†é›¶ç­–ç•¥è¿è§„å¹¶æ¶ˆé™¤äº†å†—ä½™å·¥å…·è°ƒç”¨ï¼Œç¡®ä¿äº†å†³ç­–è¿‡ç¨‹çš„å®Œæ•´å¯è¿½æº¯æ€§ã€‚è¯¥å·¥ä½œæ¨å¯¼å‡ºäº†æ„å»ºå¯é æ™ºèƒ½ä½“çš„ä¸‰é¡¹è®¾è®¡åŸåˆ™ï¼Œå³æ¨¡å—åŒ–åˆ†è§£ã€è‡ªé€‚åº”ç¬¦å·æ²»ç†å’Œé€æ˜çŠ¶æ€ç®¡ç†ï¼Œé€šè¿‡ç»“åˆä¸“å®¶ç³»ç»ŸåŸç†ä¸ç°ä»£LLMèƒ½åŠ›ï¼Œä¸ºæ„å»ºå¯ä¿¡ã€å¯è§£é‡Šä¸”å—æ²»ç†çš„AIæ™ºèƒ½ä½“æä¾›äº†æœ‰æ•ˆçš„ç†è®ºæ”¯æŒä¸å®è·µè·¯å¾„ã€‚ç›®å‰ï¼Œç ”ç©¶å›¢é˜Ÿå·²æä¾›äº†å®Œæ•´çš„å¼€æºå®ç°ä»¥åŠåŸºäºGPT-4oé©±åŠ¨çš„å®é™…åº”ç”¨æ¡ˆä¾‹ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "The reference list has been updated to reflect recent work",
      "pdf_url": "https://arxiv.org/pdf/2511.17673v3",
      "published_date": "2025-11-21 05:19:34 UTC",
      "updated_date": "2026-01-11 15:54:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:12:52.277668+00:00"
    },
    {
      "arxiv_id": "2511.17672v1",
      "title": "Cognitive Inception: Agentic Reasoning against Visual Deceptions by Injecting Skepticism",
      "title_zh": "Cognitive Inceptionï¼šé€šè¿‡æ³¨å…¥æ€€ç–‘æœºåˆ¶æå‡æ™ºèƒ½ä½“æ¨ç†ä»¥å¯¹æŠ—è§†è§‰æ¬ºéª—",
      "authors": [
        "Yinjie Zhao",
        "Heng Zhao",
        "Bihan Wen",
        "Joey Tianyi Zhou"
      ],
      "abstract": "As the development of AI-generated contents (AIGC), multi-modal Large Language Models (LLM) struggle to identify generated visual inputs from real ones. Such shortcoming causes vulnerability against visual deceptions, where the models are deceived by generated contents, and the reliability of reasoning processes is jeopardized. Therefore, facing rapidly emerging generative models and diverse data distribution, it is of vital importance to improve LLMs' generalizable reasoning to verify the authenticity of visual inputs against potential deceptions. Inspired by human cognitive processes, we discovered that LLMs exhibit tendency of over-trusting the visual inputs, while injecting skepticism could significantly improve the models visual cognitive capability against visual deceptions. Based on this discovery, we propose \\textbf{Inception}, a fully reasoning-based agentic reasoning framework to conduct generalizable authenticity verification by injecting skepticism, where LLMs' reasoning logic is iteratively enhanced between External Skeptic and Internal Skeptic agents. To the best of our knowledge, this is the first fully reasoning-based framework against AIGC visual deceptions. Our approach achieved a large margin of performance improvement over the strongest existing LLM baselines and SOTA performance on AEGIS benchmark.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (Multi-modal LLMs) åœ¨é¢å¯¹ç”Ÿæˆå¼äººå·¥æ™ºèƒ½å†…å®¹ (AIGC) æ—¶ï¼Œå› éš¾ä»¥åˆ†è¾¨è§†è§‰æ¬ºéª— (Visual Deceptions) è€Œå¯¼è‡´æ¨ç†å¯é æ€§å—æŸçš„é—®é¢˜ã€‚ç ”ç©¶å‘ç°æ¨¡å‹æ™®éå­˜åœ¨è¿‡åº¦ä¿¡ä»»è§†è§‰è¾“å…¥çš„å€¾å‘ï¼Œè€Œæ³¨å…¥æ€€ç–‘æœºåˆ¶ (Injecting Skepticism) èƒ½æ˜¾è‘—æå‡å…¶è®¤çŸ¥èƒ½åŠ›ã€‚åŸºäºæ­¤å‘ç°ï¼Œä½œè€…æå‡ºäº†åä¸º Inception çš„å®Œå…¨åŸºäºæ¨ç†çš„æ™ºèƒ½ä½“æ¡†æ¶ï¼Œé€šè¿‡å¤–éƒ¨æ€€ç–‘è€… (External Skeptic) å’Œå†…éƒ¨æ€€ç–‘è€… (Internal Skeptic) æ™ºèƒ½ä½“ä¹‹é—´çš„è¿­ä»£åä½œï¼Œä¸æ–­å¢å¼ºæ¨¡å‹å¯¹è§†è§‰çœŸå®æ€§çš„éªŒè¯é€»è¾‘ã€‚è¿™æ˜¯é¦–ä¸ªé’ˆå¯¹ AIGC è§†è§‰æ¬ºéª—çš„çº¯æ¨ç†é˜²æŠ¤æ¡†æ¶ï¼Œåœ¨ AEGIS åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº† SOTA æ€§èƒ½ï¼Œå¹¶å¤§å¹…è¶…è¶Šäº†ç°æœ‰çš„æœ€å¼ºåŸºçº¿æ¨¡å‹ã€‚è¯¥æ–¹æ³•è¯æ˜äº†é€šè¿‡æ”¹è¿›é€šç”¨æ¨ç†é€»è¾‘å¯ä»¥æœ‰æ•ˆæé«˜æ¨¡å‹åœ¨å¤æ‚æ•°æ®åˆ†å¸ƒä¸‹çš„é²æ£’æ€§ï¼Œä¸ºæ„å»ºæ›´å¯ä¿¡çš„å¤šæ¨¡æ€æ™ºèƒ½ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17672v1",
      "published_date": "2025-11-21 05:13:30 UTC",
      "updated_date": "2025-11-21 05:13:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:11:41.075038+00:00"
    },
    {
      "arxiv_id": "2511.17671v1",
      "title": "MURMUR: Using cross-user chatter to break collaborative language agents in groups",
      "title_zh": "MURMURï¼šåˆ©ç”¨è·¨ç”¨æˆ·äº¤äº’å¹²æ‰°æ”»ç ´ç¾¤ç»„åä½œè¯­è¨€æ™ºèƒ½ä½“",
      "authors": [
        "Atharv Singh Patlan",
        "Peiyao Sheng",
        "S. Ashwin Hebbar",
        "Prateek Mittal",
        "Pramod Viswanath"
      ],
      "abstract": "Language agents are rapidly expanding from single-user assistants to multi-user collaborators in shared workspaces and groups. However, today's language models lack a mechanism for isolating user interactions and concurrent tasks, creating a new attack vector inherent to this new setting: cross-user poisoning (CUP). In a CUP attack, an adversary injects ordinary-looking messages that poison the persistent, shared state, which later triggers the agent to execute unintended, attacker-specified actions on behalf of benign users. We validate CUP on real systems, successfully attacking popular multi-user agents. To study the phenomenon systematically, we present MURMUR, a framework that composes single-user tasks into concurrent, group-based scenarios using an LLM to generate realistic, history-aware user interactions. We observe that CUP attacks succeed at high rates and their effects persist across multiple tasks, thus posing fundamental risks to multi-user LLM deployments. Finally, we introduce a first-step defense with task-based clustering to mitigate this new class of vulnerability",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è¯­è¨€æ™ºèƒ½ä½“åœ¨å¤šç”¨æˆ·åä½œç¯å¢ƒä¸‹çš„å®‰å…¨æ€§ï¼ŒæŒ‡å‡ºå½“å‰æ¨¡å‹ç”±äºç¼ºä¹ç”¨æˆ·äº¤äº’å’Œå¹¶å‘ä»»åŠ¡çš„éš”ç¦»æœºåˆ¶ï¼Œé¢ä¸´è·¨ç”¨æˆ·æŠ•æ¯’(cross-user poisoning, CUP)è¿™ä¸€æ–°å‹æ”»å‡»é£é™©ã€‚æ”»å‡»è€…é€šè¿‡åœ¨æŒä¹…å…±äº«çŠ¶æ€ä¸­æ³¨å…¥çœ‹ä¼¼æ™®é€šçš„æ¶æ„æ¶ˆæ¯ï¼Œå¯è¯±å¯¼æ™ºèƒ½ä½“ä»£è¡¨æ­£å¸¸ç”¨æˆ·æ‰§è¡Œéé¢„æœŸçš„æŒ‡ä»¤ã€‚ä¸ºäº†ç³»ç»Ÿæ€§è¯„ä¼°æ­¤ç±»å¨èƒï¼Œä½œè€…å¼€å‘äº†MURMURæ¡†æ¶ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLM)æ¨¡æ‹ŸçœŸå®çš„ç¾¤ç»„äº¤äº’åœºæ™¯ï¼Œå°†å•ç”¨æˆ·ä»»åŠ¡è½¬åŒ–ä¸ºå¤æ‚çš„å¹¶å‘ä»»åŠ¡æµã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCUPæ”»å‡»åœ¨å®é™…ç³»ç»Ÿä¸­è¡¨ç°å‡ºæé«˜çš„æˆåŠŸç‡ï¼Œä¸”å…¶æ¯’åŒ–æ•ˆæœå…·æœ‰è·¨ä»»åŠ¡çš„æŒä¹…æ€§ã€‚é’ˆå¯¹è¿™ä¸€æ¼æ´ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºä»»åŠ¡èšç±»(task-based clustering)çš„åˆæ­¥é˜²å¾¡æ–¹æ³•ï¼Œä¸ºæ„å»ºæ›´å®‰å…¨çš„å¤šç”¨æˆ·LLMéƒ¨ç½²æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "20 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.17671v1",
      "published_date": "2025-11-21 04:56:37 UTC",
      "updated_date": "2025-11-21 04:56:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:12:09.972339+00:00"
    },
    {
      "arxiv_id": "2511.16943v1",
      "title": "RASTP: Representation-Aware Semantic Token Pruning for Generative Recommendation with Semantic Identifiers",
      "title_zh": "RASTPï¼šé¢å‘åŸºäºè¯­ä¹‰æ ‡è¯†ç¬¦ç”Ÿæˆå¼æ¨èçš„è¡¨å¾æ„ŸçŸ¥è¯­ä¹‰ Token å‰ªæ",
      "authors": [
        "Tianyu Zhan",
        "Kairui Fu",
        "Zheqi Lv",
        "Shengyu Zhang"
      ],
      "abstract": "Generative recommendation systems typically leverage Semantic Identifiers (SIDs), which represent each item as a sequence of tokens that encode semantic information. However, representing item ID with multiple SIDs significantly increases input sequence length, which is a major determinant of computational complexity and memory consumption. While existing efforts primarily focus on optimizing attention computation and KV cache, we propose RASTP (Representation-Aware Semantic Token Pruning), which directly prunes less informative tokens in the input sequence. Specifically, RASTP evaluates token importance by combining semantic saliency, measured via representation magnitude, and attention centrality, derived from cumulative attention weights. Since RASTP dynamically prunes low-information or irrelevant semantic tokens, experiments on three real-world Amazon datasets show that RASTP reduces training time by 26.7\\%, while maintaining or slightly improving recommendation performance. The code has been open-sourced at https://github.com/Yuzt-zju/RASTP.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿæˆå¼æ¨èç³»ç»Ÿä¸­ä½¿ç”¨è¯­ä¹‰æ ‡è¯†ç¬¦ (Semantic Identifiers, SIDs) å¯¼è‡´è¾“å…¥åºåˆ—è¿‡é•¿ã€è®¡ç®—å¤æ‚åº¦å’Œå†…å­˜æ¶ˆè€—æ¿€å¢çš„é—®é¢˜ï¼Œæå‡ºäº† RASTP (Representation-Aware Semantic Token Pruning) æ–¹æ³•ã€‚RASTP æ˜¯ä¸€ç§è¡¨ç¤ºæ„ŸçŸ¥çš„è¯­ä¹‰ Token å‰ªææ¡†æ¶ï¼Œæ—¨åœ¨ç›´æ¥è¯†åˆ«å¹¶ç§»é™¤è¾“å…¥åºåˆ—ä¸­è´¡çŒ®è¾ƒå°çš„ Tokenã€‚è¯¥æ–¹æ³•é€šè¿‡ç»“åˆåŸºäºè¡¨ç¤ºå¹…åº¦ (representation magnitude) è¡¡é‡çš„è¯­ä¹‰æ˜¾è‘—æ€§ (semantic saliency) ä»¥åŠä»ç´¯ç§¯æ³¨æ„åŠ›æƒé‡ä¸­æå–çš„æ³¨æ„åŠ›ä¸­å¿ƒæ€§ (attention centrality) æ¥ç²¾ç¡®è¯„ä¼° Token çš„é‡è¦æ€§ã€‚RASTP èƒ½å¤ŸåŠ¨æ€åœ°å¯¹ä½ä¿¡æ¯é‡æˆ–æ— å…³çš„è¯­ä¹‰ Token è¿›è¡Œå‰ªæï¼Œä»è€Œä¼˜åŒ–æ¨¡å‹å¤„ç†æ•ˆç‡ã€‚åœ¨ä¸‰ä¸ªçœŸå®çš„ Amazon æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRASTP åœ¨ä¿æŒç”šè‡³ç•¥å¾®æå‡æ¨èæ€§èƒ½çš„å‰æä¸‹ï¼ŒæˆåŠŸå°†è®­ç»ƒæ—¶é—´å‡å°‘äº† 26.7%ã€‚è¿™ä¸€ç ”ç©¶ä¸ºç”Ÿæˆå¼æ¨èç³»ç»Ÿåœ¨é•¿åºåˆ—å¤„ç†ä¸Šçš„æ•ˆç‡ä¼˜åŒ–æä¾›äº†æ–°æ€è·¯ï¼Œä¸”ç›¸å…³ä»£ç å·²å¼€æºã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "4 pages",
      "pdf_url": "https://arxiv.org/pdf/2511.16943v1",
      "published_date": "2025-11-21 04:39:32 UTC",
      "updated_date": "2025-11-21 04:39:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:12:55.652623+00:00"
    },
    {
      "arxiv_id": "2511.16937v1",
      "title": "OmniGround: A Comprehensive Spatio-Temporal Grounding Benchmark for Real-World Complex Scenarios",
      "title_zh": "OmniGroundï¼šé¢å‘çœŸå®ä¸–ç•Œå¤æ‚åœºæ™¯çš„å…¨é¢æ—¶ç©ºå®šä½åŸºå‡†",
      "authors": [
        "Hong Gao",
        "Jingyu Wu",
        "Xiangkai Xu",
        "Kangni Xie",
        "Yunchen Zhang",
        "Bin Zhong",
        "Xurui Gao",
        "Min-Ling Zhang"
      ],
      "abstract": "Spatio-Temporal Video Grounding (STVG) aims to localize target objects in videos based on natural language descriptions. Despite recent advances in Multimodal Large Language Models, a significant gap remains between current models and real-world demands involving diverse objects and complex queries. We attribute this to limited benchmark scope, causing models to exhibit category bias, oversimplified reasoning, and poor linguistic robustness. To address these limitations, we introduce OmniGround, a comprehensive benchmark with 3,475 videos spanning 81 categories and complex real-world queries. We propose the Forward-Backward-Refinement annotation pipeline that combines multi-directional tracking with intelligent error correction for high-quality labels. We further introduce DeepSTG, a systematic evaluation framework quantifying dataset quality across four complementary dimensions beyond superficial statistics. Evaluations reveal performance average drop of 10.4% on complex real-world scenes, particularly with small/occluded objects and intricate spatial relations. Motivated by these, we propose PG-TAF, a training-free two-stage framework decomposing STVG into high-level temporal grounding and fine-grained spatio-temporal propagation. Experiments demonstrate PG-TAF achieves 25.6% and 35.6% improvements in m\\_tIoU and m\\_vIoU on OmniGround with consistent gains across four benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ—¶ç©ºè§†é¢‘å®šä½(Spatio-Temporal Video Grounding, STVG)ä»»åŠ¡ä¸­ç°æœ‰æ¨¡å‹é¢ä¸´çš„ç±»åˆ«åå·®å’Œæ¨ç†èƒ½åŠ›ä¸è¶³ç­‰é—®é¢˜ï¼Œæå‡ºäº†OmniGroundè¿™ä¸€æ¶µç›–81ä¸ªç±»åˆ«å’Œå¤æ‚ç°å®æŸ¥è¯¢çš„ç»¼åˆæ€§åŸºå‡†æµ‹è¯•é›†ã€‚ä½œè€…è®¾è®¡äº†å‰å‘-åå‘-ç²¾ç»†åŒ–(Forward-Backward-Refinement)æ ‡æ³¨æµæ°´çº¿ä»¥ç¡®ä¿é«˜è´¨é‡æ ‡ç­¾ï¼Œå¹¶å¼•å…¥DeepSTGæ¡†æ¶ä»å››ä¸ªäº’è¡¥ç»´åº¦ç³»ç»Ÿåœ°é‡åŒ–è¯„ä¼°æ•°æ®é›†è´¨é‡ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰æ¨¡å‹åœ¨å¤æ‚ç°å®åœºæ™¯ä¸‹çš„æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¾®å°ã€é®æŒ¡ç‰©ä½“åŠå¤æ‚ç©ºé—´å…³ç³»æ—¶è¡¨ç°æ¬ ä½³ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œè®ºæ–‡è¿›ä¸€æ­¥æå‡ºäº†PG-TAFï¼Œä¸€ä¸ªå°†STVGåˆ†è§£ä¸ºé«˜çº§æ—¶é—´å®šä½ä¸ç»†ç²’åº¦æ—¶ç©ºä¼ æ’­çš„å…è®­ç»ƒä¸¤é˜¶æ®µæ¡†æ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPG-TAFåœ¨OmniGroundä¸Šçš„m\\_tIoUå’Œm\\_vIoUåˆ†åˆ«æå‡äº†25.6%å’Œ35.6%ï¼Œå¹¶åœ¨å…¶ä»–å››ä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¹Ÿå–å¾—äº†ä¸€è‡´çš„æ€§èƒ½æå‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "20 pages",
      "pdf_url": "https://arxiv.org/pdf/2511.16937v1",
      "published_date": "2025-11-21 04:23:04 UTC",
      "updated_date": "2025-11-21 04:23:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:11:56.373172+00:00"
    },
    {
      "arxiv_id": "2511.21728v2",
      "title": "Affective Multimodal Agents with Proactive Knowledge Grounding for Emotionally Aligned Marketing Dialogue",
      "title_zh": "é¢å‘æƒ…æ„Ÿå¯¹é½è¥é”€å¯¹è¯çš„å…·æœ‰ä¸»åŠ¨çŸ¥è¯†å…³è”çš„æƒ…æ„Ÿå¤šæ¨¡æ€æ™ºèƒ½ä½“",
      "authors": [
        "Lin Yu",
        "Xiaofei Han",
        "Yifei Kang",
        "Chiung-Yi Tseng",
        "Danyang Zhang",
        "Ziqian Bi",
        "Zhimo Han"
      ],
      "abstract": "Recent advances in large language models (LLMs) have enabled fluent dialogue systems, but most remain reactive and struggle in emotionally rich, goal-oriented settings such as marketing conversations. To address this limitation, we propose AffectMind, a multimodal affective dialogue agent that performs proactive reasoning and dynamic knowledge grounding to sustain emotionally aligned and persuasive interactions. AffectMind combines three components: a Proactive Knowledge Grounding Network (PKGN) that continuously updates factual and affective context from text, vision, and prosody; an Emotion--Intent Alignment Model (EIAM) that jointly models user emotion and purchase intent to adapt persuasion strategies; and a Reinforced Discourse Loop (RDL) that optimizes emotional coherence and engagement via reinforcement signals from user responses. Experiments on two newly curated marketing dialogue datasets, MM-ConvMarket and AffectPromo, show that AffectMind outperforms strong LLM-based baselines in emotional consistency (+26\\%), persuasive success rate (+19\\%), and long-term user engagement (+23\\%), highlighting emotion-grounded proactivity as a key capability for commercial multimodal agents.",
      "tldr_zh": "é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨è¥é”€å¯¹è¯ç­‰æƒ…æ„Ÿä¸°å¯Œä¸”ç›®æ ‡å¯¼å‘çš„åœºæ™¯ä¸­è¡¨ç°è¢«åŠ¨çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†AffectMindï¼Œä¸€ç§å…·å¤‡ä¸»åŠ¨æ¨ç†å’ŒåŠ¨æ€çŸ¥è¯†å¯¹é½èƒ½åŠ›çš„å¤šæ¨¡æ€æƒ…æ„Ÿå¯¹è¯æ™ºèƒ½ä½“ã€‚è¯¥ç³»ç»Ÿé€šè¿‡ä¸»åŠ¨çŸ¥è¯†å¯¹é½ç½‘ç»œ(PKGN)ä»æ–‡æœ¬ã€è§†è§‰å’ŒéŸµå¾‹ä¸­æå–å®æ—¶ä¸Šä¸‹æ–‡ï¼Œå¹¶åˆ©ç”¨æƒ…æ„Ÿ-æ„å›¾å¯¹é½æ¨¡å‹(EIAM)åŒæ­¥å»ºæ¨¡ç”¨æˆ·æƒ…ç»ªä¸è´­ä¹°æ„å›¾ï¼Œä»¥åŠ¨æ€è°ƒæ•´è¯´æœç­–ç•¥ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†å¼ºåŒ–è¯è¯­å¾ªç¯(RDL)ï¼Œé€šè¿‡ç”¨æˆ·åé¦ˆä¿¡å·æŒç»­ä¼˜åŒ–äº¤äº’çš„æƒ…æ„Ÿä¸€è‡´æ€§ã€‚åœ¨MM-ConvMarketå’ŒAffectPromoæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAffectMindåœ¨æƒ…æ„Ÿä¸€è‡´æ€§ã€è¯´æœæˆåŠŸç‡å’Œé•¿æœŸç”¨æˆ·å‚ä¸åº¦ä¸Šåˆ†åˆ«æ¯”å¼ºåŸºçº¿æ¨¡å‹æå‡äº†26%ã€19%å’Œ23%ã€‚è¯¥ç ”ç©¶è¯æ˜äº†ä»¥æƒ…æ„Ÿä¸ºåŸºç¡€çš„ä¸»åŠ¨æ€§æ˜¯æå‡å•†ä¸šå¤šæ¨¡æ€æ™ºèƒ½ä½“æ€§èƒ½çš„å…³é”®èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.21728v2",
      "published_date": "2025-11-21 04:16:45 UTC",
      "updated_date": "2025-12-20 07:48:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:12:19.069838+00:00"
    },
    {
      "arxiv_id": "2511.17669v1",
      "title": "Empa: An AI-Powered Virtual Mentor for Developing Global Collaboration Skills in HPC Education",
      "title_zh": "Empaï¼šç”¨äºé«˜æ€§èƒ½è®¡ç®—æ•™è‚²ä¸­åŸ¹å…»å…¨çƒåä½œèƒ½åŠ›çš„ AI é©±åŠ¨è™šæ‹Ÿå¯¼å¸ˆ",
      "authors": [
        "Ashish",
        "Aparajita Jaiswal",
        "Sudip Vhaduri",
        "Niveditha Nerella",
        "Shubham Jha"
      ],
      "abstract": "High-performance computing (HPC) and parallel computing increasingly rely on global collaboration among diverse teams, yet traditional computing curricula inadequately prepare students for cross-cultural teamwork essential in modern computational research environments. This paper presents Empa, an AI-powered virtual mentor that integrates intercultural collaboration training into undergraduate computing education. Built using large language models and deployed through a progressive web application, Empa guides students through structured activities covering cultural dimensions, communication styles, and conflict resolution that are critical for effective multicultural teamwork. Our system addresses the growing need for culturally competent HPC professionals by helping computing students develop skills to collaborate effectively in international research teams, contribute to global computational projects, and navigate the cultural complexities inherent in distributed computing environments. Pilot preparation for deployment in computing courses demonstrates the feasibility of AI-mediated intercultural training and provides insights into scalable approaches for developing intercultural collaboration skills essential for HPC workforce development.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿè®¡ç®—è¯¾ç¨‹åœ¨è·¨æ–‡åŒ–å›¢é˜Ÿåä½œåŸ¹è®­æ–¹é¢çš„ä¸è¶³ï¼Œæå‡ºäº†Empaï¼Œä¸€ä¸ªæ—¨åœ¨åŸ¹å…»é«˜æ€§èƒ½è®¡ç®—(HPC)æ•™è‚²ä¸­å…¨çƒåä½œèƒ½åŠ›çš„AIé©±åŠ¨è™šæ‹Ÿå¯¼å¸ˆã€‚Empaåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(Large Language Models)å¹¶é€šè¿‡æ¸è¿›å¼Webåº”ç”¨(Progressive Web Application)éƒ¨ç½²ï¼Œå¼•å¯¼å­¦ç”Ÿå®Œæˆæ¶‰åŠæ–‡åŒ–ç»´åº¦ã€æ²Ÿé€šé£æ ¼å’Œå†²çªè§£å†³ç­‰å…³é”®é¢†åŸŸçš„ç»“æ„åŒ–æ´»åŠ¨ã€‚è¯¥ç³»ç»Ÿé€šè¿‡å¸®åŠ©è®¡ç®—æœºä¸“ä¸šçš„å­¦ç”ŸæŒæ¡åœ¨å›½é™…ç ”ç©¶å›¢é˜Ÿä¸­æœ‰æ•ˆåä½œçš„æŠ€èƒ½ï¼Œè§£å†³äº†ç°ä»£è®¡ç®—ç ”ç©¶ç¯å¢ƒä¸‹å¯¹å…·å¤‡æ–‡åŒ–èƒœä»»åŠ›çš„HPCä¸“ä¸šäººå‘˜æ—¥ç›Šå¢é•¿çš„éœ€æ±‚ã€‚åˆæ­¥çš„è¯•ç‚¹éƒ¨ç½²å‡†å¤‡å·¥ä½œè¯æ˜äº†AIè¾…åŠ©è·¨æ–‡åŒ–åŸ¹è®­çš„å¯è¡Œæ€§ï¼Œå¹¶ä¸ºå¼€å‘HPCåŠ³åŠ¨åŠ›å‘å±•æ‰€éœ€çš„è·¨æ–‡åŒ–åä½œæŠ€èƒ½æä¾›äº†å¯æ‰©å±•çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17669v1",
      "published_date": "2025-11-21 03:52:22 UTC",
      "updated_date": "2025-11-21 03:52:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:12:34.671099+00:00"
    },
    {
      "arxiv_id": "2511.16916v1",
      "title": "Hybrid Differential Reward: Combining Temporal Difference and Action Gradients for Efficient Multi-Agent Reinforcement Learning in Cooperative Driving",
      "title_zh": "æ··åˆå¾®åˆ†å¥–åŠ±ï¼šç»“åˆæ—¶åºå·®åˆ†ä¸åŠ¨ä½œæ¢¯åº¦ä»¥å®ç°ååŒé©¾é©¶ä¸­çš„é«˜æ•ˆå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Ye Han",
        "Lijun Zhang",
        "Dejian Meng",
        "Zhuang Zhang"
      ],
      "abstract": "In multi-vehicle cooperative driving tasks involving high-frequency continuous control, traditional state-based reward functions suffer from the issue of vanishing reward differences. This phenomenon results in a low signal-to-noise ratio (SNR) for policy gradients, significantly hindering algorithm convergence and performance improvement. To address this challenge, this paper proposes a novel Hybrid Differential Reward (HDR) mechanism. We first theoretically elucidate how the temporal quasi-steady nature of traffic states and the physical proximity of actions lead to the failure of traditional reward signals. Building on this analysis, the HDR framework innovatively integrates two complementary components: (1) a Temporal Difference Reward (TRD) based on a global potential function, which utilizes the evolutionary trend of potential energy to ensure optimal policy invariance and consistency with long-term objectives; and (2) an Action Gradient Reward (ARG), which directly measures the marginal utility of actions to provide a local guidance signal with a high SNR. Furthermore, we formulate the cooperative driving problem as a Multi-Agent Partially Observable Markov Game (POMDPG) with a time-varying agent set and provide a complete instantiation scheme for HDR within this framework. Extensive experiments conducted using both online planning (MCTS) and Multi-Agent Reinforcement Learning (QMIX, MAPPO, MADDPG) algorithms demonstrate that the HDR mechanism significantly improves convergence speed and policy stability. The results confirm that HDR guides agents to learn high-quality cooperative policies that effectively balance traffic efficiency and safety.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é«˜é¢‘è¿ç»­æ§åˆ¶ä¸‹çš„å¤šè½¦ååŒé©¾é©¶ä»»åŠ¡ï¼ŒæŒ‡å‡ºäº†ä¼ ç»ŸåŸºäºçŠ¶æ€çš„å¥–åŠ±å‡½æ•°å­˜åœ¨å¥–åŠ±å·®å¼‚æ¶ˆå¤±(vanishing reward differences)çš„é—®é¢˜ï¼Œè¿™å¯¼è‡´ç­–ç•¥æ¢¯åº¦çš„ä¿¡å™ªæ¯”(SNR)è¾ƒä½å¹¶ä¸¥é‡é˜»ç¢ç®—æ³•æ”¶æ•›ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§æ··åˆå¾®åˆ†å¥–åŠ±(Hybrid Differential Reward, HDR)æœºåˆ¶ï¼Œé€šè¿‡ç†è®ºåˆ†æé˜æ˜äº†äº¤é€šçŠ¶æ€çš„å‡†ç¨³æ€ç‰¹æ€§å’ŒåŠ¨ä½œç‰©ç†é‚»è¿‘æ€§å¯¹ä¼ ç»Ÿå¥–åŠ±ä¿¡å·çš„å½±å“ã€‚HDRæ¡†æ¶åˆ›æ–°åœ°æ•´åˆäº†ä¸¤ä¸ªäº’è¡¥ç»„ä»¶ï¼šä¸€æ˜¯åŸºäºå…¨å±€åŠ¿å‡½æ•°çš„æ—¶åºå·®åˆ†å¥–åŠ±(Temporal Difference Reward, TDR)ï¼Œåˆ©ç”¨åŠ¿èƒ½æ¼”å˜è¶‹åŠ¿ç¡®ä¿ç­–ç•¥æœ€ä¼˜ä¸å˜æ€§ä¸é•¿æœŸç›®æ ‡çš„ä¸€è‡´æ€§ï¼›äºŒæ˜¯åŠ¨ä½œæ¢¯åº¦å¥–åŠ±(Action Gradient Reward, AGR)ï¼Œé€šè¿‡è¡¡é‡åŠ¨ä½œçš„è¾¹é™…æ•ˆç”¨æä¾›é«˜ä¿¡å™ªæ¯”çš„å±€éƒ¨å¼•å¯¼ä¿¡å·ã€‚ç ”ç©¶è¿›ä¸€æ­¥å°†ååŒé©¾é©¶å»ºæ¨¡ä¸ºå…·æœ‰æ—¶å˜æ™ºèƒ½ä½“é›†åˆçš„å¤šæ™ºèƒ½ä½“éƒ¨åˆ†å¯è§‚æµ‹é©¬å°”å¯å¤«åšå¼ˆ(POMDPG)ï¼Œå¹¶åœ¨è¯¥æ¡†æ¶ä¸‹å®ç°äº†HDRçš„å®ä¾‹åŒ–ã€‚åœ¨MCTSä»¥åŠQMIXã€MAPPOã€MADDPGç­‰å¤šç§å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¸Šçš„å®éªŒè¯æ˜ï¼ŒHDRæ˜¾è‘—æå‡äº†æ”¶æ•›é€Ÿåº¦å’Œç­–ç•¥ç¨³å®šæ€§ã€‚å®éªŒç»“æœè¯å®ï¼Œè¯¥æœºåˆ¶èƒ½æœ‰æ•ˆå¼•å¯¼æ™ºèƒ½ä½“å­¦ä¹ é«˜è´¨é‡çš„ååŒç­–ç•¥ï¼Œåœ¨ä¿éšœäº¤é€šæ•ˆç‡çš„åŒæ—¶å…¼é¡¾äº†è¡Œé©¶å®‰å…¨æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16916v1",
      "published_date": "2025-11-21 02:58:04 UTC",
      "updated_date": "2025-11-21 02:58:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:12:44.272181+00:00"
    },
    {
      "arxiv_id": "2511.16912v1",
      "title": "PepEVOLVE: Position-Aware Dynamic Peptide Optimization via Group-Relative Advantage",
      "title_zh": "PepEVOLVEï¼šåŸºäºç»„ç›¸å¯¹ä¼˜åŠ¿çš„ä½ç½®æ„ŸçŸ¥åŠ¨æ€å¤šè‚½ä¼˜åŒ–",
      "authors": [
        "Trieu Nguyen",
        "Hao-Wei Pang",
        "Shasha Feng"
      ],
      "abstract": "Macrocyclic peptides are an emerging modality that combines biologics-like affinity with small-molecule-like developability, but their vast combinatorial space and multi-parameter objectives make lead optimization slow and challenging. Prior generative approaches such as PepINVENT require chemists to pre-specify mutable positions for optimization, choices that are not always known a priori, and rely on static pretraining and optimization algorithms that limit the model's ability to generalize and effectively optimize peptide sequences. We introduce PepEVOLVE, a position-aware, dynamic framework that learns both where to edit and how to dynamically optimize peptides for multi-objective improvement. PepEVOLVE (i) augments pretraining with dynamic masking and CHUCKLES shifting to improve generalization, (ii) uses a context-free multi-armed bandit router that discovers high-reward residues, and (iii) couples a novel evolving optimization algorithm with group-relative advantage to stabilize reinforcement updates. During in silico evaluations, the router policy reliably learns and concentrates probability on chemically meaningful sites that influence the peptide's properties. On a therapeutically motivated Rev-binding macrocycle benchmark, PepEVOLVE outperformed PepINVENT by reaching higher mean scores (approximately 0.8 vs. 0.6), achieving best candidates with a score of 0.95 (vs. 0.87), and converging in fewer steps under the task of optimizing permeability and lipophilicity with structural constraints. Overall, PepEVOLVE offers a practical, reproducible path to peptide lead optimization when optimal edit sites are unknown, enabling more efficient exploration and improving design quality across multiple objectives.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†PepEVOLVEï¼Œä¸€ç§ä½ç½®æ„ŸçŸ¥(position-aware)çš„åŠ¨æ€æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§ç¯è‚½(Macrocyclic peptides)å…ˆå¯¼åŒ–åˆç‰©ä¼˜åŒ–ä¸­ç»„åˆç©ºé—´å·¨å¤§åŠéœ€é¢„è®¾çªå˜ä½ç‚¹çš„éš¾é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡åŠ¨æ€æ©ç (dynamic masking)å’ŒCHUCKLESå˜æ¢å¢å¼ºæ³›åŒ–èƒ½åŠ›ï¼Œå¹¶åˆ©ç”¨æ— ä¸Šä¸‹æ–‡å¤šè‡‚è€è™æœºè·¯ç”±å™¨(context-free multi-armed bandit router)è‡ªåŠ¨è¯†åˆ«å…³é”®æ®‹åŸºä½ç‚¹ã€‚ä¸ºäº†ç¨³å®šå¼ºåŒ–å­¦ä¹ æ›´æ–°ï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†ä¸€ç§ç»“åˆç»„ç›¸å¯¹ä¼˜åŠ¿(group-relative advantage)çš„è¿›åŒ–ä¼˜åŒ–ç®—æ³•ã€‚åœ¨æ²»ç–—æ€§Revç»“åˆå¤§ç¯è‚½åŸºå‡†æµ‹è¯•ä¸­ï¼ŒPepEVOLVEåœ¨å¤šç›®æ ‡ä¼˜åŒ–ä»»åŠ¡ä¸­çš„è¡¨ç°æ˜¾è‘—ä¼˜äºPepINVENTï¼Œä¸ä»…è·å¾—äº†æ›´é«˜çš„å¹³å‡åˆ†(çº¦0.8 vs 0.6)å’Œæœ€ä½³å€™é€‰åˆ†å­å¾—åˆ†(0.95 vs 0.87)ï¼Œä¸”æ”¶æ•›é€Ÿåº¦æ›´å¿«ã€‚å®éªŒè¯æ˜ï¼Œè¯¥è·¯ç”±ç­–ç•¥èƒ½å¯é åœ°èšç„¦äºå½±å“è‚½æ€§è´¨çš„åŒ–å­¦å…³é”®ä½ç‚¹ã€‚æ€»ä½“è€Œè¨€ï¼ŒPepEVOLVEä¸ºæœ€ä¼˜ç¼–è¾‘ä½ç‚¹æœªçŸ¥æ—¶çš„è‚½åºåˆ—ä¼˜åŒ–æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å¯é‡å¤çš„è§£å†³æ–¹æ¡ˆï¼Œæ˜¾è‘—æå‡äº†å¤šç›®æ ‡çº¦æŸä¸‹çš„è¯ç‰©è®¾è®¡è´¨é‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16912v1",
      "published_date": "2025-11-21 02:51:15 UTC",
      "updated_date": "2025-11-21 02:51:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:13:35.967075+00:00"
    },
    {
      "arxiv_id": "2511.16886v2",
      "title": "Deep Improvement Supervision",
      "title_zh": "æ·±åº¦æ”¹è¿›ç›‘ç£",
      "authors": [
        "Arip Asadulaev",
        "Rayan Banerjee",
        "Fakhri Karray",
        "Martin Takac"
      ],
      "abstract": "Recently, it was shown that small, looped architectures, such as Tiny Recursive Models (TRMs), can outperform Large Language Models (LLMs) on complex reasoning tasks, including the Abstraction and Reasoning Corpus (ARC). In this work, we investigate a core question: how can we further improve the efficiency of these methods with minimal changes? To address this, we frame the latent reasoning of TRMs as a form of classifier-free guidance and implicit policy improvement algorithm. Building on these insights, we propose a novel training scheme that provides a target for each loop during training. We demonstrate that our approach significantly enhances training efficiency. Our method reduces the total number of forward passes by 18x and eliminates halting mechanisms, while maintaining quality comparable to standard TRMs. Notably, we achieve 24% accuracy on ARC-1 with only 0.8M parameters, outperforming most LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨æé«˜å°å‹å¾ªç¯æ¶æ„ï¼ˆå¦‚ Tiny Recursive Models, TRMsï¼‰åœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æ—¶çš„æ•ˆç‡ã€‚ä½œè€…å°† TRMs çš„æ½œç©ºé—´æ¨ç†è§†ä¸ºä¸€ç§ classifier-free guidance å’Œéšæ€§ policy improvement ç®—æ³•ï¼Œå¹¶æ®æ­¤æå‡ºäº†åä¸º Deep Improvement Supervision çš„æ–°å‹è®­ç»ƒæ–¹æ¡ˆã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨è®­ç»ƒæœŸé—´ä¸ºæ¯ä¸ªå¾ªç¯æä¾›ç›®æ ‡ï¼Œæ˜¾è‘—æå‡äº†è®­ç»ƒæ•ˆç‡ï¼Œå¹¶æˆåŠŸæ¶ˆé™¤äº† halting mechanismsã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å°†æ€»å‰å‘ä¼ æ’­æ¬¡æ•°å‡å°‘äº† 18 å€ï¼Œä¸”åœ¨ä¿æŒä¸æ ‡å‡† TRMs ç›¸å½“æ€§èƒ½çš„å‰æä¸‹ï¼Œä»…ç”¨ 0.8M å‚æ•°å°±åœ¨ ARC-1 ä»»åŠ¡ä¸Šå–å¾—äº† 24% çš„å‡†ç¡®ç‡ã€‚è¿™ä¸€ç»“æœè¯æ˜äº†è¯¥æ–¹æ¡ˆåœ¨æå°å‚æ•°é‡ä¸‹è¶…è¶Šå¤§å¤šæ•°å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) çš„æ½œåŠ›ï¼Œä¸ºé«˜æ•ˆæ¨ç†æ¶æ„çš„ä¼˜åŒ–æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16886v2",
      "published_date": "2025-11-21 01:54:23 UTC",
      "updated_date": "2025-11-28 01:28:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:13:20.472209+00:00"
    },
    {
      "arxiv_id": "2511.16884v2",
      "title": "Generative AI in Sociological Research: State of the Discipline",
      "title_zh": "ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åœ¨ç¤¾ä¼šå­¦ç ”ç©¶ä¸­çš„åº”ç”¨ï¼šå­¦ç§‘ç°çŠ¶",
      "authors": [
        "AJ Alvero",
        "Dustin S. Stoltz",
        "Oscar Stuhler",
        "Marshall Taylor"
      ],
      "abstract": "Generative artificial intelligence (GenAI) has garnered considerable attention for its potential utility in research and scholarship. A growing body of work in sociology and related fields demonstrates both the potential advantages and risks of GenAI, but these studies are largely proof-of-concept or specific audits of models and products. We know comparatively little about how sociologists actually use GenAI in their research practices and how they view its present and future role in the discipline. In this paper, we describe the current landscape of GenAI use in sociological research based on a survey of authors in 50 sociology journals. Our sample includes both computational sociologists and non-computational sociologists and their collaborators. We find that sociologists primarily use GenAI to assist with writing tasks: revising, summarizing, editing, and translating their own work. Respondents report that GenAI saves time and that they are curious about its capabilities, but they do not currently feel strong institutional or field-level pressure to adopt it. Overall, respondents are wary of GenAI's social and environmental impacts and express low levels of trust in its outputs, but many believe that GenAI tools will improve over the next several years. We do not find large differences between computational and non-computational scholars in terms of GenAI use, attitudes, and concern; nor do we find strong patterns by familiarity or frequency of use. We discuss what these findings suggest about the future of GenAI in sociology and highlight challenges for developing shared norms around its use in research practice.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (Generative AI) åœ¨ç¤¾ä¼šå­¦ç ”ç©¶ä¸­çš„åº”ç”¨ç°çŠ¶ï¼Œé€šè¿‡å¯¹50æœ¬ç¤¾ä¼šå­¦æœŸåˆŠçš„ä½œè€…è¿›è¡Œé—®å·è°ƒæŸ¥ï¼Œæ­ç¤ºäº†ç¤¾ä¼šå­¦å®¶å¦‚ä½•å°†è¯¥æŠ€æœ¯æ•´åˆè¿›ç ”ç©¶å®è·µåŠå…¶å¯¹æœªæ¥å‘å±•çš„çœ‹æ³•ã€‚è°ƒæŸ¥å‘ç°ï¼Œç¤¾ä¼šå­¦å®¶ç›®å‰ä¸»è¦åˆ©ç”¨ GenAI è¾…åŠ©å†™ä½œä»»åŠ¡ï¼Œå¦‚ä¿®è®¢ã€æ€»ç»“ã€ç¼–è¾‘å’Œç¿»è¯‘ï¼Œå¹¶è®¤ä¸ºå…¶èƒ½æœ‰æ•ˆèŠ‚çœæ—¶é—´ï¼Œä½†ç›®å‰å¹¶æœªæ„Ÿå—åˆ°æ˜¾è‘—çš„å­¦ç§‘å†…éƒ¨é‡‡ç”¨å‹åŠ›ã€‚å—è®¿è€…æ™®éå¯¹ GenAI çš„ç¤¾ä¼šå’Œç¯å¢ƒå½±å“æŒè°¨æ…æ€åº¦ï¼Œå¯¹å…¶è¾“å‡ºç»“æœçš„ä¿¡ä»»åº¦è¾ƒä½ï¼Œå°½ç®¡å¤§å¤šé¢„æµ‹ç›¸å…³å·¥å…·å°†åœ¨æœªæ¥å‡ å¹´å†…æŒç»­æ”¹è¿›ã€‚ç ”ç©¶è¿›ä¸€æ­¥æŒ‡å‡ºï¼Œè®¡ç®—ç¤¾ä¼šå­¦å®¶ (computational sociologists) ä¸éè®¡ç®—å­¦è€…åœ¨ GenAI çš„ä½¿ç”¨æ¨¡å¼ã€æ€åº¦å’Œæ‹…å¿§æ–¹é¢è¡¨ç°å‡ºé«˜åº¦ä¸€è‡´æ€§ï¼Œå¹¶æœªå‘ç°æ˜æ˜¾çš„ç¾¤ä½“å·®å¼‚ã€‚æœ€åï¼Œæ–‡ç« è®¨è®ºäº† GenAI å¯¹ç¤¾ä¼šå­¦å­¦ç§‘çš„æ½œåœ¨å½±å“ï¼Œå¹¶å¼ºè°ƒäº†åœ¨å»ºç«‹å…±äº«ç ”ç©¶è§„èŒƒå’Œåº”å¯¹æŠ€æœ¯æŒ‘æˆ˜æ–¹é¢çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.16884v2",
      "published_date": "2025-11-21 01:34:28 UTC",
      "updated_date": "2025-12-01 22:01:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:13:22.574848+00:00"
    },
    {
      "arxiv_id": "2511.17666v1",
      "title": "Evaluating Adversarial Vulnerabilities in Modern Large Language Models",
      "title_zh": "ç°ä»£å¤§è¯­è¨€æ¨¡å‹å¯¹æŠ—æ€§æ¼æ´è¯„ä¼°",
      "authors": [
        "Tom Perel"
      ],
      "abstract": "The recent boom and rapid integration of Large Language Models (LLMs) into a wide range of applications warrants a deeper understanding of their security and safety vulnerabilities. This paper presents a comparative analysis of the susceptibility to jailbreak attacks for two leading publicly available LLMs, Google's Gemini 2.5 Flash and OpenAI's GPT-4 (specifically the GPT-4o mini model accessible in the free tier). The research utilized two main bypass strategies: 'self-bypass', where models were prompted to circumvent their own safety protocols, and 'cross-bypass', where one model generated adversarial prompts to exploit vulnerabilities in the other. Four attack methods were employed - direct injection, role-playing, context manipulation, and obfuscation - to generate five distinct categories of unsafe content: hate speech, illegal activities, malicious code, dangerous content, and misinformation. The success of the attack was determined by the generation of disallowed content, with successful jailbreaks assigned a severity score. The findings indicate a disparity in jailbreak susceptibility between 2.5 Flash and GPT-4, suggesting variations in their safety implementations or architectural design. Cross-bypass attacks were particularly effective, indicating that an ample amount of vulnerabilities exist in the underlying transformer architecture. This research contributes a scalable framework for automated AI red-teaming and provides data-driven insights into the current state of LLM safety, underscoring the complex challenge of balancing model capabilities with robust safety mechanisms.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†ç°ä»£å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å®‰å…¨æ€§æ–¹é¢çš„è„†å¼±æ€§ï¼Œé‡ç‚¹å¯¹æ¯”äº†Googleçš„Gemini 2.5 Flashä¸OpenAIçš„GPT-4(GPT-4o mini)å¯¹è¶Šç‹±æ”»å‡»(jailbreak attacks)çš„æ•æ„Ÿåº¦ã€‚ç ”ç©¶é‡‡ç”¨äº†â€œè‡ªæˆ‘ç»•è¿‡â€(self-bypass)å’Œâ€œäº¤å‰ç»•è¿‡â€(cross-bypass)ä¸¤ç§ç­–ç•¥ï¼Œé€šè¿‡ç›´æ¥æ³¨å…¥(direct injection)ã€è§’è‰²æ‰®æ¼”(role-playing)ã€ä¸Šä¸‹æ–‡æ“çºµ(context manipulation)å’Œæ··æ·†(obfuscation)ç­‰æ‰‹æ®µæµ‹è¯•äº†äº”ç±»ä¸å®‰å…¨å†…å®¹çš„ç”Ÿæˆã€‚å®éªŒç»“æœæ­ç¤ºäº†ä¸¤ç§æ¨¡å‹åœ¨è¶Šç‹±æ˜“æ„Ÿæ€§ä¸Šçš„æ˜¾è‘—å·®å¼‚ï¼Œè¿™åæ˜ äº†å…¶å®‰å…¨å®ç°å’Œæ¶æ„è®¾è®¡çš„ä¸åŒã€‚ç ”ç©¶å‘ç°äº¤å‰ç»•è¿‡æ”»å‡»è¡¨ç°å°¤ä¸ºæœ‰æ•ˆï¼Œè¡¨æ˜åº•å±‚Transformeræ¶æ„ä¸­ä»å­˜åœ¨å¤§é‡å®‰å…¨æ¼æ´ã€‚è¯¥å·¥ä½œè´¡çŒ®äº†ä¸€ä¸ªå¯æ‰©å±•çš„è‡ªåŠ¨åŒ–AIçº¢é˜Ÿæµ‹è¯•(red-teaming)æ¡†æ¶ï¼Œä¸ºç†è§£å½“å‰LLMå®‰å…¨ç°çŠ¶æä¾›äº†æ•°æ®æ”¯æ’‘ï¼Œå¹¶å¼ºè°ƒäº†å¹³è¡¡æ¨¡å‹èƒ½åŠ›ä¸é²æ£’å®‰å…¨æœºåˆ¶çš„å¤æ‚æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17666v1",
      "published_date": "2025-11-21 01:23:56 UTC",
      "updated_date": "2025-11-21 01:23:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T09:13:19.469471+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 124,
  "processed_papers_count": 124,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-26T09:16:55.591710+00:00"
}
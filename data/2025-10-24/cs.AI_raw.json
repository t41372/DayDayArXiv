[
  {
    "arxiv_id": "2510.22087v1",
    "title": "QuArch: A Benchmark for Evaluating LLM Reasoning in Computer Architecture",
    "authors": [
      "Shvetank Prakash",
      "Andrew Cheng",
      "Arya Tschand",
      "Mark Mazumder",
      "Varun Gohil",
      "Jeffrey Ma",
      "Jason Yik",
      "Zishen Wan",
      "Jessica Quaye",
      "Elisavet Lydia Alvanaki",
      "Avinash Kumar",
      "Chandrashis Mazumdar",
      "Tuhin Khare",
      "Alexander Ingare",
      "Ikechukwu Uchendu",
      "Radhika Ghosal",
      "Abhishek Tyagi",
      "Chenyu Wang",
      "Andrea Mattia Garavagno",
      "Sarah Gu",
      "Alice Guo",
      "Grace Hur",
      "Luca Carloni",
      "Tushar Krishna",
      "Ankita Nayak",
      "Amir Yazdanbakhsh",
      "Vijay Janapa Reddi"
    ],
    "abstract": "The field of computer architecture, which bridges high-level software abstractions and low-level hardware implementations, remains absent from current large language model (LLM) evaluations. To this end, we present QuArch (pronounced 'quark'), the first benchmark designed to facilitate the development and evaluation of LLM knowledge and reasoning capabilities specifically in computer architecture. QuArch provides a comprehensive collection of 2,671 expert-validated question-answer (QA) pairs covering various aspects of computer architecture, including processor design, memory systems, and interconnection networks. Our evaluation reveals that while frontier models possess domain-specific knowledge, they struggle with skills that require higher-order thinking in computer architecture. Frontier model accuracies vary widely (from 34% to 72%) on these advanced questions, highlighting persistent gaps in architectural reasoning across analysis, design, and implementation QAs. By holistically assessing fundamental skills, QuArch provides a foundation for building and measuring LLM capabilities that can accelerate innovation in computing systems. With over 140 contributors from 40 institutions, this benchmark represents a community effort to set the standard for architectural reasoning in LLM evaluation.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.LG",
      "cs.SE"
    ],
    "primary_category": "cs.AR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.22087v1",
    "published_date": "2025-10-24 23:54:17 UTC",
    "updated_date": "2025-10-24 23:54:17 UTC"
  },
  {
    "arxiv_id": "2510.22085v1",
    "title": "Jailbreak Mimicry: Automated Discovery of Narrative-Based Jailbreaks for Large Language Models",
    "authors": [
      "Pavlos Ntais"
    ],
    "abstract": "Large language models (LLMs) remain vulnerable to sophisticated prompt engineering attacks that exploit contextual framing to bypass safety mechanisms, posing significant risks in cybersecurity applications. We introduce Jailbreak Mimicry, a systematic methodology for training compact attacker models to automatically generate narrative-based jailbreak prompts in a one-shot manner. Our approach transforms adversarial prompt discovery from manual craftsmanship into a reproducible scientific process, enabling proactive vulnerability assessment in AI-driven security systems. Developed for the OpenAI GPT-OSS-20B Red-Teaming Challenge, we use parameter-efficient fine-tuning (LoRA) on Mistral-7B with a curated dataset derived from AdvBench, achieving an 81.0% Attack Success Rate (ASR) against GPT-OSS-20B on a held-out test set of 200 items. Cross-model evaluation reveals significant variation in vulnerability patterns: our attacks achieve 66.5% ASR against GPT-4, 79.5% on Llama-3 and 33.0% against Gemini 2.5 Flash, demonstrating both broad applicability and model-specific defensive strengths in cybersecurity contexts. This represents a 54x improvement over direct prompting (1.5% ASR) and demonstrates systematic vulnerabilities in current safety alignment approaches. Our analysis reveals that technical domains (Cybersecurity: 93% ASR) and deception-based attacks (Fraud: 87.8% ASR) are particularly vulnerable, highlighting threats to AI-integrated threat detection, malware analysis, and secure systems, while physical harm categories show greater resistance (55.6% ASR). We employ automated harmfulness evaluation using Claude Sonnet 4, cross-validated with human expert assessment, ensuring reliable and scalable evaluation for cybersecurity red-teaming. Finally, we analyze failure mechanisms and discuss defensive strategies to mitigate these vulnerabilities in AI for cybersecurity.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "18 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.22085v1",
    "published_date": "2025-10-24 23:53:16 UTC",
    "updated_date": "2025-10-24 23:53:16 UTC"
  },
  {
    "arxiv_id": "2510.22075v1",
    "title": "Agentic Reinforcement Learning for Real-World Code Repair",
    "authors": [
      "Siyu Zhu",
      "Anastasiya Karpovich",
      "Albert Chen",
      "Jessica Koscheka",
      "Shailesh Jannu",
      "Di Wen",
      "Yuqing Zhu",
      "Rohit Jain",
      "Alborz Geramifard"
    ],
    "abstract": "We tackle the challenge of training reliable code-fixing agents in real repositories, where complex builds and shifting dependencies make evaluation unstable. We developed a verifiable pipeline with success defined as post-fix build validation and improved reproducibility across ~1K real issues by pinning dependencies and disabling automatic upgrades. Building on this, we introduced a scalable simplified pipeline for large-scale reinforcement learning (RL). Using this setup, we supervised fine-tuned Qwen3-32B in the full pipeline and applied RL on top of the SFT model in the simplified environment. The SFT model distilled from GPT-4.1 trajectories performs on par while being 56x smaller, and RL added 7-20% absolute gains under matched train-test conditions. \"Thinking mode\" was on par or worse in our experiments. Both SFT and RL models failed to generalize across environments, highlighting the importance of matching train-test environments for building reliable real-world code-fixing agents.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.22075v1",
    "published_date": "2025-10-24 23:25:02 UTC",
    "updated_date": "2025-10-24 23:25:02 UTC"
  },
  {
    "arxiv_id": "2510.22063v1",
    "title": "Frequentist Validity of Epistemic Uncertainty Estimators",
    "authors": [
      "Anchit Jain",
      "Stephen Bates"
    ],
    "abstract": "Decomposing prediction uncertainty into its aleatoric (irreducible) and epistemic (reducible) components is critical for the development and deployment of machine learning systems. A popular, principled measure for epistemic uncertainty is the mutual information between the response variable and model parameters. However, evaluating this measure requires access to the posterior distribution of the model parameters, which is challenging to compute. In view of this, we introduce a frequentist measure of epistemic uncertainty based on the bootstrap. Our main theoretical contribution is a novel asymptotic expansion that reveals that our proposed (frequentist) measure and the (Bayesian) mutual information are asymptotically equivalent. This provides frequentist interpretations to mutual information and new computational strategies for approximating it. Moreover, we link our proposed approach to the widely-used heuristic approach of deep ensembles, giving added perspective on their practical success.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "math.ST"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.22063v1",
    "published_date": "2025-10-24 22:58:42 UTC",
    "updated_date": "2025-10-24 22:58:42 UTC"
  },
  {
    "arxiv_id": "2510.22057v1",
    "title": "Automatic Assessment of Students' Classroom Engagement with Bias Mitigated Multi-task Model",
    "authors": [
      "James Thiering",
      "Tarun Sethupat Radha Krishna",
      "Dylan Zelkin",
      "Ashis Kumer Biswas"
    ],
    "abstract": "With the rise of online and virtual learning, monitoring and enhancing student engagement have become an important aspect of effective education. Traditional methods of assessing a student's involvement might not be applicable directly to virtual environments. In this study, we focused on this problem and addressed the need to develop an automated system to detect student engagement levels during online learning. We proposed a novel training method which can discourage a model from leveraging sensitive features like gender for its predictions. The proposed method offers benefits not only in the enforcement of ethical standards, but also to enhance interpretability of the model predictions. We applied an attribute-orthogonal regularization technique to a split-model classifier, which uses multiple transfer learning strategies to achieve effective results in reducing disparity in the distribution of prediction for sensitivity groups from a Pearson correlation coefficient of 0.897 for the unmitigated model, to 0.999 for the mitigated model. The source code for this project is available on https://github.com/ashiskb/elearning-engagement-study .",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "13 pages, 12 figures, and 1 table",
    "pdf_url": "https://arxiv.org/pdf/2510.22057v1",
    "published_date": "2025-10-24 22:39:01 UTC",
    "updated_date": "2025-10-24 22:39:01 UTC"
  },
  {
    "arxiv_id": "2510.22056v1",
    "title": "Human-Centric Anomaly Detection in Surveillance Videos Using YOLO-World and Spatio-Temporal Deep Learning",
    "authors": [
      "Mohammad Ali Etemadi Naeen",
      "Hoda Mohammadzade",
      "Saeed Bagheri Shouraki"
    ],
    "abstract": "Anomaly detection in surveillance videos remains a challenging task due to the diversity of abnormal events, class imbalance, and scene-dependent visual clutter. To address these issues, we propose a robust deep learning framework that integrates human-centric preprocessing with spatio-temporal modeling for multi-class anomaly classification. Our pipeline begins by applying YOLO-World - an open-vocabulary vision-language detector - to identify human instances in raw video clips, followed by ByteTrack for consistent identity-aware tracking. Background regions outside detected bounding boxes are suppressed via Gaussian blurring, effectively reducing scene-specific distractions and focusing the model on behaviorally relevant foreground content. The refined frames are then processed by an ImageNet-pretrained InceptionV3 network for spatial feature extraction, and temporal dynamics are captured using a bidirectional LSTM (BiLSTM) for sequence-level classification. Evaluated on a five-class subset of the UCF-Crime dataset (Normal, Burglary, Fighting, Arson, Explosion), our method achieves a mean test accuracy of 92.41% across three independent trials, with per-class F1-scores consistently exceeding 0.85. Comprehensive evaluation metrics - including confusion matrices, ROC curves, and macro/weighted averages - demonstrate strong generalization and resilience to class imbalance. The results confirm that foreground-focused preprocessing significantly enhances anomaly discrimination in real-world surveillance scenarios.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.22056v1",
    "published_date": "2025-10-24 22:38:17 UTC",
    "updated_date": "2025-10-24 22:38:17 UTC"
  },
  {
    "arxiv_id": "2510.22052v1",
    "title": "Energy-Efficient Domain-Specific Artificial Intelligence Models and Agents: Pathways and Paradigms",
    "authors": [
      "Abhijit Chatterjee",
      "Niraj K. Jha",
      "Jonathan D. Cohen",
      "Thomas L. Griffiths",
      "Hongjing Lu",
      "Diana Marculescu",
      "Ashiqur Rasul",
      "Keshab K. Parhi"
    ],
    "abstract": "The field of artificial intelligence (AI) has taken a tight hold on broad aspects of society, industry, business, and governance in ways that dictate the prosperity and might of the world's economies. The AI market size is projected to grow from 189 billion USD in 2023 to 4.8 trillion USD by 2033. Currently, AI is dominated by large language models that exhibit linguistic and visual intelligence. However, training these models requires a massive amount of data scraped from the web as well as large amounts of energy (50--60 GWh to train GPT-4). Despite these costs, these models often hallucinate, a characteristic that prevents them from being deployed in critical application domains. In contrast, the human brain consumes only 20~W of power. What is needed is the next level of AI evolution in which lightweight domain-specific multimodal models with higher levels of intelligence can reason, plan, and make decisions in dynamic environments with real-time data and prior knowledge, while learning continuously and evolving in ways that enhance future decision-making capability. This will define the next wave of AI, progressing from today's large models, trained with vast amounts of data, to nimble energy-efficient domain-specific agents that can reason and think in a world full of uncertainty. To support such agents, hardware will need to be reimagined to allow energy efficiencies greater than 1000x over the state of the art. Such a vision of future AI systems is developed in this work.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.22052v1",
    "published_date": "2025-10-24 22:21:08 UTC",
    "updated_date": "2025-10-24 22:21:08 UTC"
  },
  {
    "arxiv_id": "2510.22050v1",
    "title": "Towards Error-Centric Intelligence II: Energy-Structured Causal Models",
    "authors": [
      "Marcus Thomas"
    ],
    "abstract": "Contemporary machine learning optimizes for predictive accuracy, yet systems that achieve state of the art performance remain causally opaque: their internal representations provide no principled handle for intervention. We can retrain such models, but we cannot surgically edit specific mechanisms while holding others fixed, because learned latent variables lack causal semantics. We argue for a conceptual reorientation: intelligence is the ability to build and refine explanations, falsifiable claims about manipulable structure that specify what changes and what remains invariant under intervention. Explanations subsume prediction but demand more: causal commitments that can be independently tested and corrected at the level of mechanisms. We introduce computational explanations, mappings from observations to intervention ready causal accounts. We instantiate these explanations with Energy Structured Causal Models (ESCMs), in which mechanisms are expressed as constraints (energy functions or vector fields) rather than explicit input output maps, and interventions act by local surgery on those constraints. This shift makes internal structure manipulable at the level where explanations live: which relations must hold, which can change, and what follows when they do. We provide concrete instantiations of the structural-causal principles LAP and ICM in the ESCM context, and also argue that empirical risk minimization systematically produces fractured, entangled representations, a failure we analyze as gauge ambiguity in encoder energy pairs. Finally, we show that under mild conditions, ESCMs recover standard SCM semantics. Building on Part I's principles (LAP, ICM, CAP) and its definition of intelligence as explanation-building under criticism, this paper offers a formal language for causal reasoning in systems that aspire to understand, not merely to predict.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.22050v1",
    "published_date": "2025-10-24 22:19:17 UTC",
    "updated_date": "2025-10-24 22:19:17 UTC"
  },
  {
    "arxiv_id": "2510.22046v1",
    "title": "HW/SW Co-design of a PCM/PWM converter: a System Level Approach based in the SpecC Methodology",
    "authors": [
      "Daniel G. P. Petrini",
      "Braz Izaias da Silva Junior"
    ],
    "abstract": "We present a case study applying the SpecC methodology within a system-level hardware/software co-design flow to a PCM-to-PWM converter, the core of a Class-D audio amplifier. The converter was modeled and explored with SpecC methodology to derive an HW/SW partition. Using system-level estimates and fast functional simulation, we evaluated mappings that meet real-time constraints while reducing estimated cost of an all-hardware solution and avoiding the expense of a purely software implementation on a high-end processor. Despite the design's moderate complexity, the results underline the value of system-level co-design for early architectural insight, rapid validation, and actionable cost/performance trade-offs. [Original work from 2005; formatting revised in 2025, with no changes to the results.]",
    "categories": [
      "cs.AI",
      "cs.AR",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "6",
    "pdf_url": "https://arxiv.org/pdf/2510.22046v1",
    "published_date": "2025-10-24 22:07:09 UTC",
    "updated_date": "2025-10-24 22:07:09 UTC"
  },
  {
    "arxiv_id": "2510.22045v1",
    "title": "VLM-SlideEval: Evaluating VLMs on Structured Comprehension and Perturbation Sensitivity in PPT",
    "authors": [
      "Hyeonsu Kang",
      "Emily Bao",
      "Anjan Goswami"
    ],
    "abstract": "Vision-language models (VLMs) are increasingly used to evaluate multimodal content, including presentation slides, yet their slide-specific understanding remains underexplored {despite their growing role as critics in agentic, model-forward pipelines}. We introduce VLM-SlideEval, an evaluation framework that probes VLMs along three axes: (1) element-level extraction from slide images aligned to ground truth; (2) robustness to controlled perturbations in geometry, style, and text; and (3) higher-level comprehension, such as recovering a deck's narrative order from shuffled slides. Using publicly available decks from Zenodo (https://huggingface.co/datasets/Forceless/Zenodo10K/viewer/default/pptx), we standardize ground-truth element metadata from PowerPoint XML and live renderings into a unified, verifiable schema. Empirically, VLMs underperform on pixel-accurate extraction and show non-trivial agreement, fidelity, and consistency under controlled perturbations, while performing better on single-slide content understanding; however, they do not reliably capture narrative structure across slides. These results highlight the limits of current VLMs for slide evaluation and motivate calibrated, critic-in-the-loop evaluators that drive iterative refinement and selection in agentic pipelines.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: Evaluating the Evolving LLM Lifecycle - Benchmarks, Emergent Abilities, and Scaling",
    "pdf_url": "https://arxiv.org/pdf/2510.22045v1",
    "published_date": "2025-10-24 22:06:56 UTC",
    "updated_date": "2025-10-24 22:06:56 UTC"
  },
  {
    "arxiv_id": "2510.22042v1",
    "title": "Emotions Where Art Thou: Understanding and Characterizing the Emotional Latent Space of Large Language Models",
    "authors": [
      "Benjamin Reichman",
      "Adar Avsian",
      "Larry Heck"
    ],
    "abstract": "This work investigates how large language models (LLMs) internally represent emotion by analyzing the geometry of their hidden-state space. The paper identifies a low-dimensional emotional manifold and shows that emotional representations are directionally encoded, distributed across layers, and aligned with interpretable dimensions. These structures are stable across depth and generalize to eight real-world emotion datasets spanning five languages. Cross-domain alignment yields low error and strong linear probe performance, indicating a universal emotional subspace. Within this space, internal emotion perception can be steered while preserving semantics using a learned intervention module, with especially strong control for basic emotions across languages. These findings reveal a consistent and manipulable affective geometry in LLMs and offer insight into how they internalize and process emotion.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.22042v1",
    "published_date": "2025-10-24 21:54:12 UTC",
    "updated_date": "2025-10-24 21:54:12 UTC"
  },
  {
    "arxiv_id": "2510.22039v1",
    "title": "Predictive Coding Enhances Meta-RL To Achieve Interpretable Bayes-Optimal Belief Representation Under Partial Observability",
    "authors": [
      "Po-Chen Kuo",
      "Han Hou",
      "Will Dabney",
      "Edgar Y. Walker"
    ],
    "abstract": "Learning a compact representation of history is critical for planning and generalization in partially observable environments. While meta-reinforcement learning (RL) agents can attain near Bayes-optimal policies, they often fail to learn the compact, interpretable Bayes-optimal belief states. This representational inefficiency potentially limits the agent's adaptability and generalization capacity. Inspired by predictive coding in neuroscience--which suggests that the brain predicts sensory inputs as a neural implementation of Bayesian inference--and by auxiliary predictive objectives in deep RL, we investigate whether integrating self-supervised predictive coding modules into meta-RL can facilitate learning of Bayes-optimal representations. Through state machine simulation, we show that meta-RL with predictive modules consistently generates more interpretable representations that better approximate Bayes-optimal belief states compared to conventional meta-RL across a wide variety of tasks, even when both achieve optimal policies. In challenging tasks requiring active information seeking, only meta-RL with predictive modules successfully learns optimal representations and policies, whereas conventional meta-RL struggles with inadequate representation learning. Finally, we demonstrate that better representation learning leads to improved generalization. Our results strongly suggest the role of predictive learning as a guiding principle for effective representation learning in agents navigating partial observability.",
    "categories": [
      "cs.AI",
      "q-bio.NC"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to Annual Conference on Neural Information Processing Systems (NeurIPS) 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.22039v1",
    "published_date": "2025-10-24 21:45:56 UTC",
    "updated_date": "2025-10-24 21:45:56 UTC"
  },
  {
    "arxiv_id": "2510.22034v1",
    "title": "LLM-AR: LLM-powered Automated Reasoning Framework",
    "authors": [
      "Rick Chen",
      "Joseph Ternasky",
      "Aaron Ontoyin Yin",
      "Xianling Mu",
      "Fuat Alican",
      "Yigit Ihlamur"
    ],
    "abstract": "Large language models (LLMs) can already identify patterns and reason effectively, yet their variable accuracy hampers adoption in high-stakes decision-making applications. In this paper, we study this issue from a venture capital perspective by predicting idea-stage startup success based on founder traits. (i) To build a reliable prediction model, we introduce LLM-AR, a pipeline inspired by neural-symbolic systems that distils LLM-generated heuristics into probabilistic rules executed by the ProbLog automated-reasoning engine. (ii) An iterative policy-evolution loop incorporates association-rule mining to progressively refine the prediction rules.\n  On unseen folds, LLM-AR achieves 59.5% precision and 8.7% recall, 5.9x the random baseline precision, while exposing every decision path for human inspection. The framework is interpretable and tunable via hyperparameters, showing promise to extend into other domains.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.22034v1",
    "published_date": "2025-10-24 21:36:18 UTC",
    "updated_date": "2025-10-24 21:36:18 UTC"
  },
  {
    "arxiv_id": "2510.22031v1",
    "title": "Differentiable Constraint-Based Causal Discovery",
    "authors": [
      "Jincheng Zhou",
      "Mengbo Wang",
      "Anqi He",
      "Yumeng Zhou",
      "Hessam Olya",
      "Murat Kocaoglu",
      "Bruno Ribeiro"
    ],
    "abstract": "Causal discovery from observational data is a fundamental task in artificial intelligence, with far-reaching implications for decision-making, predictions, and interventions. Despite significant advances, existing methods can be broadly categorized as constraint-based or score-based approaches. Constraint-based methods offer rigorous causal discovery but are often hindered by small sample sizes, while score-based methods provide flexible optimization but typically forgo explicit conditional independence testing. This work explores a third avenue: developing differentiable $d$-separation scores, obtained through a percolation theory using soft logic. This enables the implementation of a new type of causal discovery method: gradient-based optimization of conditional independence constraints. Empirical evaluations demonstrate the robust performance of our approach in low-sample regimes, surpassing traditional constraint-based and score-based baselines on a real-world dataset. Code and data of the proposed method are publicly available at https://github$.$com/PurdueMINDS/DAGPA.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.22031v1",
    "published_date": "2025-10-24 21:28:39 UTC",
    "updated_date": "2025-10-24 21:28:39 UTC"
  },
  {
    "arxiv_id": "2510.22027v1",
    "title": "Online Optimization for Offline Safe Reinforcement Learning",
    "authors": [
      "Yassine Chemingui",
      "Aryan Deshwal",
      "Alan Fern",
      "Thanh Nguyen-Tang",
      "Janardhan Rao Doppa"
    ],
    "abstract": "We study the problem of Offline Safe Reinforcement Learning (OSRL), where the goal is to learn a reward-maximizing policy from fixed data under a cumulative cost constraint. We propose a novel OSRL approach that frames the problem as a minimax objective and solves it by combining offline RL with online optimization algorithms. We prove the approximate optimality of this approach when integrated with an approximate offline RL oracle and no-regret online optimization. We also present a practical approximation that can be combined with any offline RL algorithm, eliminating the need for offline policy evaluation. Empirical results on the DSRL benchmark demonstrate that our method reliably enforces safety constraints under stringent cost budgets, while achieving high rewards. The code is available at https://github.com/yassineCh/O3SRL.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "To appear in NeurIPS 2025 Conference",
    "pdf_url": "https://arxiv.org/pdf/2510.22027v1",
    "published_date": "2025-10-24 21:12:47 UTC",
    "updated_date": "2025-10-24 21:12:47 UTC"
  },
  {
    "arxiv_id": "2510.22026v2",
    "title": "Normalization in Attention Dynamics",
    "authors": [
      "Nikita Karagodin",
      "Shu Ge",
      "Yury Polyanskiy",
      "Philippe Rigollet"
    ],
    "abstract": "We study the effect of normalization schemes on token representations in deep transformers. Modeling their evolution as interacting particles on the sphere, we show that normalization acts as a form of speed regulation. This perspective enables a unified analysis of several schemes -- including Post-LN, Pre-LN, Mix-LN, Peri-LN, nGPT -- revealing how they influence clustering dynamics and representation collapse. Our framework clarifies how different schemes shape token representations across layers and provides a principled basis for comparing them, identifying Peri-LN as a particularly effective choice.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025), 23 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.22026v2",
    "published_date": "2025-10-24 21:10:16 UTC",
    "updated_date": "2025-11-11 18:40:41 UTC"
  },
  {
    "arxiv_id": "2510.22014v1",
    "title": "Toward Understanding the Transferability of Adversarial Suffixes in Large Language Models",
    "authors": [
      "Sarah Ball",
      "Niki Hasrati",
      "Alexander Robey",
      "Avi Schwarzschild",
      "Frauke Kreuter",
      "Zico Kolter",
      "Andrej Risteski"
    ],
    "abstract": "Discrete optimization-based jailbreaking attacks on large language models aim to generate short, nonsensical suffixes that, when appended onto input prompts, elicit disallowed content. Notably, these suffixes are often transferable -- succeeding on prompts and models for which they were never optimized. And yet, despite the fact that transferability is surprising and empirically well-established, the field lacks a rigorous analysis of when and why transfer occurs. To fill this gap, we identify three statistical properties that strongly correlate with transfer success across numerous experimental settings: (1) how much a prompt without a suffix activates a model's internal refusal direction, (2) how strongly a suffix induces a push away from this direction, and (3) how large these shifts are in directions orthogonal to refusal. On the other hand, we find that prompt semantic similarity only weakly correlates with transfer success. These findings lead to a more fine-grained understanding of transferability, which we use in interventional experiments to showcase how our statistical analysis can translate into practical improvements in attack success.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.22014v1",
    "published_date": "2025-10-24 20:28:49 UTC",
    "updated_date": "2025-10-24 20:28:49 UTC"
  },
  {
    "arxiv_id": "2510.22011v1",
    "title": "Reconnaissance Automatique des Langues des Signes : Une Approche Hybridée CNN-LSTM Basée sur Mediapipe",
    "authors": [
      "Fraisse Sacré Takouchouang",
      "Ho Tuong Vinh"
    ],
    "abstract": "Sign languages play a crucial role in the communication of deaf communities, but they are often marginalized, limiting access to essential services such as healthcare and education. This study proposes an automatic sign language recognition system based on a hybrid CNN-LSTM architecture, using Mediapipe for gesture keypoint extraction. Developed with Python, TensorFlow and Streamlit, the system provides real-time gesture translation. The results show an average accuracy of 92\\%, with very good performance for distinct gestures such as ``Hello'' and ``Thank you''. However, some confusions remain for visually similar gestures, such as ``Call'' and ``Yes''. This work opens up interesting perspectives for applications in various fields such as healthcare, education and public services.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "in French language",
    "pdf_url": "https://arxiv.org/pdf/2510.22011v1",
    "published_date": "2025-10-24 20:25:25 UTC",
    "updated_date": "2025-10-24 20:25:25 UTC"
  },
  {
    "arxiv_id": "2510.22009v1",
    "title": "LightAgent: Mobile Agentic Foundation Models",
    "authors": [
      "Yangqin Jiang",
      "Chao Huang"
    ],
    "abstract": "With the advancement of multimodal large language models (MLLMs), building GUI agent systems has become an increasingly promising direction-especially for mobile platforms, given their rich app ecosystems and intuitive touch interactions. Yet mobile GUI agents face a critical dilemma: truly on-device models (4B or smaller) lack sufficient performance, while capable models (starting from 7B) are either too large for mobile deployment or prohibitively costly (e.g., cloud-only closed-source MLLMs). To resolve this, we propose LightAgent, a mobile agentic foundation model solution that leverages device-cloud collaboration to tap the cost-efficiency of on-device models and the high capability of cloud models, while avoiding their drawbacks. Specifically, LightAgent enhances Qwen2.5-VL-3B via two-stage SFT->GRPO training on synthetic GUI data for strong decision-making, integrates an efficient long-reasoning mechanism to utilize historical interactions under tight resources, and defaults to on-device execution-only escalating challenging subtasks to the cloud via real-time complexity assessment. Experiments on the online AndroidLab benchmark and diverse apps show LightAgent matches or nears larger models, with a significant reduction in cloud costs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.22009v1",
    "published_date": "2025-10-24 20:23:12 UTC",
    "updated_date": "2025-10-24 20:23:12 UTC"
  },
  {
    "arxiv_id": "2510.22003v1",
    "title": "Impact and Implications of Generative AI for Enterprise Architects in Agile Environments: A Systematic Literature Review",
    "authors": [
      "Stefan Julian Kooy",
      "Jean Paul Sebastian Piest",
      "Rob Henk Bemthuis"
    ],
    "abstract": "Generative AI (GenAI) is reshaping enterprise architecture work in agile software organizations, yet evidence on its effects remains scattered. We report a systematic literature review (SLR), following established SLR protocols of Kitchenham and PRISMA, of 1,697 records, yielding 33 studies across enterprise, solution, domain, business, and IT architect roles. GenAI most consistently supports (i) design ideation and trade-off exploration; (ii) rapid creation and refinement of artifacts (e.g., code, models, documentation); and (iii) architectural decision support and knowledge retrieval. Reported risks include opacity and bias, contextually incorrect outputs leading to rework, privacy and compliance concerns, and social loafing. We also identify emerging skills and competencies, including prompt engineering, model evaluation, and professional oversight, and organizational enablers around readiness and adaptive governance. The review contributes with (1) a mapping of GenAI use cases and risks in agile architecting, (2) implications for capability building and governance, and (3) an initial research agenda on human-AI collaboration in architecture. Overall, the findings inform responsible adoption of GenAI that accelerates digital transformation while safeguarding architectural integrity.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "17 pages, 1 figure, 5 tables; to appear in Enterprise Design, Operations, and Computing. EDOC 2025 Workshops, Lecture Notes in Business Information Processing (LNBIP), Springer, 2025. Part of 29th International Conference on Enterprise Design, Operations, and Computing (EDOC)",
    "pdf_url": "https://arxiv.org/pdf/2510.22003v1",
    "published_date": "2025-10-24 20:09:54 UTC",
    "updated_date": "2025-10-24 20:09:54 UTC"
  },
  {
    "arxiv_id": "2510.21999v1",
    "title": "Foundation of Intelligence: Review of Math Word Problems from Human Cognition Perspective",
    "authors": [
      "Zhenya Huang",
      "Jiayu Liu",
      "Xin Lin",
      "Zhiyuan Ma",
      "Shangzi Xue",
      "Tong Xiao",
      "Qi Liu",
      "Yee Whye Teh",
      "Enhong Chen"
    ],
    "abstract": "Math word problem (MWP) serves as a fundamental research topic in artificial intelligence (AI) dating back to 1960s. This research aims to advance the reasoning abilities of AI by mirroring the human-like cognitive intelligence. The mainstream technological paradigm has evolved from the early rule-based methods, to deep learning models, and is rapidly advancing towards large language models. However, the field still lacks a systematic taxonomy for the MWP survey along with a discussion of current development trends. Therefore, in this paper, we aim to comprehensively review related research in MWP solving through the lens of human cognition, to demonstrate how recent AI models are advancing in simulating human cognitive abilities. Specifically, we summarize 5 crucial cognitive abilities for MWP solving, including Problem Understanding, Logical Organization, Associative Memory, Critical Thinking, and Knowledge Learning. Focused on these abilities, we review two mainstream MWP models in recent 10 years: neural network solvers, and LLM based solvers, and discuss the core human-like abilities they demonstrated in their intricate problem-solving process. Moreover, we rerun all the representative MWP solvers and supplement their performance on 5 mainstream benchmarks for a unified comparison. To the best of our knowledge, this survey first comprehensively analyzes the influential MWP research of the past decade from the perspective of human reasoning cognition and provides an integrative overall comparison across existing approaches. We hope it can inspire further research in AI reasoning. Our repository is released on https://github.com/Ljyustc/FoI-MWP.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21999v1",
    "published_date": "2025-10-24 20:06:15 UTC",
    "updated_date": "2025-10-24 20:06:15 UTC"
  },
  {
    "arxiv_id": "2510.21998v1",
    "title": "From Black-box to Causal-box: Towards Building More Interpretable Models",
    "authors": [
      "Inwoo Hwang",
      "Yushu Pan",
      "Elias Bareinboim"
    ],
    "abstract": "Understanding the predictions made by deep learning models remains a central challenge, especially in high-stakes applications. A promising approach is to equip models with the ability to answer counterfactual questions -- hypothetical ``what if?'' scenarios that go beyond the observed data and provide insight into a model reasoning. In this work, we introduce the notion of causal interpretability, which formalizes when counterfactual queries can be evaluated from a specific class of models and observational data. We analyze two common model classes -- blackbox and concept-based predictors -- and show that neither is causally interpretable in general. To address this gap, we develop a framework for building models that are causally interpretable by design. Specifically, we derive a complete graphical criterion that determines whether a given model architecture supports a given counterfactual query. This leads to a fundamental tradeoff between causal interpretability and predictive accuracy, which we characterize by identifying the unique maximal set of features that yields an interpretable model with maximal predictive expressiveness. Experiments corroborate the theoretical findings.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.21998v1",
    "published_date": "2025-10-24 20:03:18 UTC",
    "updated_date": "2025-10-24 20:03:18 UTC"
  },
  {
    "arxiv_id": "2510.21995v1",
    "title": "Is Temporal Difference Learning the Gold Standard for Stitching in RL?",
    "authors": [
      "Michał Bortkiewicz",
      "Władysław Pałucki",
      "Mateusz Ostaszewski",
      "Benjamin Eysenbach"
    ],
    "abstract": "Reinforcement learning (RL) promises to solve long-horizon tasks even when training data contains only short fragments of the behaviors. This experience stitching capability is often viewed as the purview of temporal difference (TD) methods. However, outside of small tabular settings, trajectories never intersect, calling into question this conventional wisdom. Moreover, the common belief is that Monte Carlo (MC) methods should not be able to recombine experience, yet it remains unclear whether function approximation could result in a form of implicit stitching. The goal of this paper is to empirically study whether the conventional wisdom about stitching actually holds in settings where function approximation is used. We empirically demonstrate that Monte Carlo (MC) methods can also achieve experience stitching. While TD methods do achieve slightly stronger capabilities than MC methods (in line with conventional wisdom), that gap is significantly smaller than the gap between small and large neural networks (even on quite simple tasks). We find that increasing critic capacity effectively reduces the generalization gap for both the MC and TD methods. These results suggest that the traditional TD inductive bias for stitching may be less necessary in the era of large models for RL and, in some cases, may offer diminishing returns. Additionally, our results suggest that stitching, a form of generalization unique to the RL setting, might be achieved not through specialized algorithms (temporal difference learning) but rather through the same recipe that has provided generalization in other machine learning settings (via scale). Project website: https://michalbortkiewicz.github.io/golden-standard/",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "The first two authors contributed equally. Project website: https://michalbortkiewicz.github.io/golden-standard/",
    "pdf_url": "https://arxiv.org/pdf/2510.21995v1",
    "published_date": "2025-10-24 20:00:14 UTC",
    "updated_date": "2025-10-24 20:00:14 UTC"
  },
  {
    "arxiv_id": "2510.21991v1",
    "title": "Two-Steps Diffusion Policy for Robotic Manipulation via Genetic Denoising",
    "authors": [
      "Mateo Clemente",
      "Leo Brunswic",
      "Rui Heng Yang",
      "Xuan Zhao",
      "Yasser Khalil",
      "Haoyu Lei",
      "Amir Rasouli",
      "Yinchuan Li"
    ],
    "abstract": "Diffusion models, such as diffusion policy, have achieved state-of-the-art results in robotic manipulation by imitating expert demonstrations. While diffusion models were originally developed for vision tasks like image and video generation, many of their inference strategies have been directly transferred to control domains without adaptation. In this work, we show that by tailoring the denoising process to the specific characteristics of embodied AI tasks -- particularly structured, low-dimensional nature of action distributions -- diffusion policies can operate effectively with as few as 5 neural function evaluations (NFE).\n  Building on this insight, we propose a population-based sampling strategy, genetic denoising, which enhances both performance and stability by selecting denoising trajectories with low out-of-distribution risk. Our method solves challenging tasks with only 2 NFE while improving or matching performance. We evaluate our approach across 14 robotic manipulation tasks from D4RL and Robomimic, spanning multiple action horizons and inference budgets. In over 2 million evaluations, our method consistently outperforms standard diffusion-based policies, achieving up to 20\\% performance gains with significantly fewer inference steps.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "16 pages, 11 figure, 2 tables, accepted at Neurips 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.21991v1",
    "published_date": "2025-10-24 19:52:41 UTC",
    "updated_date": "2025-10-24 19:52:41 UTC"
  },
  {
    "arxiv_id": "2510.21983v1",
    "title": "Uncovering the Persuasive Fingerprint of LLMs in Jailbreaking Attacks",
    "authors": [
      "Havva Alizadeh Noughabi",
      "Julien Serbanescu",
      "Fattane Zarrinkalam",
      "Ali Dehghantanha"
    ],
    "abstract": "Despite recent advances, Large Language Models remain vulnerable to jailbreak attacks that bypass alignment safeguards and elicit harmful outputs. While prior research has proposed various attack strategies differing in human readability and transferability, little attention has been paid to the linguistic and psychological mechanisms that may influence a model's susceptibility to such attacks. In this paper, we examine an interdisciplinary line of research that leverages foundational theories of persuasion from the social sciences to craft adversarial prompts capable of circumventing alignment constraints in LLMs. Drawing on well-established persuasive strategies, we hypothesize that LLMs, having been trained on large-scale human-generated text, may respond more compliantly to prompts with persuasive structures. Furthermore, we investigate whether LLMs themselves exhibit distinct persuasive fingerprints that emerge in their jailbreak responses. Empirical evaluations across multiple aligned LLMs reveal that persuasion-aware prompts significantly bypass safeguards, demonstrating their potential to induce jailbreak behaviors. This work underscores the importance of cross-disciplinary insight in addressing the evolving challenges of LLM safety. The code and data are available.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21983v1",
    "published_date": "2025-10-24 19:20:23 UTC",
    "updated_date": "2025-10-24 19:20:23 UTC"
  },
  {
    "arxiv_id": "2511.05511v1",
    "title": "From Failure Modes to Reliability Awareness in Generative and Agentic AI System",
    "authors": [
      "Janet",
      "Lin",
      "Liangwei Zhang"
    ],
    "abstract": "This chapter bridges technical analysis and organizational preparedness by tracing the path from layered failure modes to reliability awareness in generative and agentic AI systems. We first introduce an 11-layer failure stack, a structured framework for identifying vulnerabilities ranging from hardware and power foundations to adaptive learning and agentic reasoning. Building on this, the chapter demonstrates how failures rarely occur in isolation but propagate across layers, creating cascading effects with systemic consequences. To complement this diagnostic lens, we develop the concept of awareness mapping: a maturity-oriented framework that quantifies how well individuals and organizations recognize reliability risks across the AI stack. Awareness is treated not only as a diagnostic score but also as a strategic input for AI governance, guiding improvement and resilience planning. By linking layered failures to awareness levels and further integrating this into Dependability-Centred Asset Management (DCAM), the chapter positions awareness mapping as both a measurement tool and a roadmap for trustworthy and sustainable AI deployment across mission-critical domains.",
    "categories": [
      "eess.SY",
      "cs.AI"
    ],
    "primary_category": "eess.SY",
    "comment": "24pages",
    "pdf_url": "https://arxiv.org/pdf/2511.05511v1",
    "published_date": "2025-10-24 19:12:07 UTC",
    "updated_date": "2025-10-24 19:12:07 UTC"
  },
  {
    "arxiv_id": "2510.21978v1",
    "title": "Beyond Reasoning Gains: Mitigating General Capabilities Forgetting in Large Reasoning Models",
    "authors": [
      "Hoang Phan",
      "Xianjun Yang",
      "Kevin Yao",
      "Jingyu Zhang",
      "Shengjie Bi",
      "Xiaocheng Tang",
      "Madian Khabsa",
      "Lijuan Liu",
      "Deren Lei"
    ],
    "abstract": "Reinforcement learning with verifiable rewards (RLVR) has delivered impressive gains in mathematical and multimodal reasoning and has become a standard post-training paradigm for contemporary language and vision-language models. However, the RLVR recipe introduces a significant risk of capability regression, where models forget foundational skills after prolonged training without employing regularization strategies. We empirically confirm this concern, observing that open-source reasoning models suffer performance degradation on core capabilities such as perception and faithfulness. While imposing regularization terms like KL divergence can help prevent deviation from the base model, these terms are calculated on the current task, thus they do not guarantee broader knowledge. Meanwhile, commonly used experience replay across heterogeneous domains makes it nontrivial to decide how much training focus each objective should receive. To address this, we propose RECAP-a replay strategy with dynamic objective reweighting for general knowledge preservation. Our reweighting mechanism adapts in an online manner using short-horizon signals of convergence and instability, shifting the post-training focus away from saturated objectives and toward underperforming or volatile ones. Our method is end-to-end and readily applicable to existing RLVR pipelines without training additional models or heavy tuning. Extensive experiments on benchmarks based on Qwen2.5-VL-3B and Qwen2.5-VL-7B demonstrate the effectiveness of our method, which not only preserves general capabilities but also improves reasoning by enabling more flexible trade-offs among in-task rewards.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21978v1",
    "published_date": "2025-10-24 19:08:48 UTC",
    "updated_date": "2025-10-24 19:08:48 UTC"
  },
  {
    "arxiv_id": "2510.21977v1",
    "title": "Distribution Shift Alignment Helps LLMs Simulate Survey Response Distributions",
    "authors": [
      "Ji Huang",
      "Mengfei Li",
      "Shuai Shao"
    ],
    "abstract": "Large language models (LLMs) offer a promising way to simulate human survey responses, potentially reducing the cost of large-scale data collection. However, existing zero-shot methods suffer from prompt sensitivity and low accuracy, while conventional fine-tuning approaches mostly fit the training set distributions and struggle to produce results more accurate than the training set itself, which deviates from the original goal of using LLMs to simulate survey responses. Building on this observation, we introduce Distribution Shift Alignment (DSA), a two-stage fine-tuning method that aligns both the output distributions and the distribution shifts across different backgrounds. By learning how these distributions change rather than fitting training data, DSA can provide results substantially closer to the true distribution than the training data. Empirically, DSA consistently outperforms other methods on five public survey datasets. We further conduct a comprehensive comparison covering accuracy, robustness, and data savings. DSA reduces the required real data by 53.48-69.12%, demonstrating its effectiveness and efficiency in survey simulation.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21977v1",
    "published_date": "2025-10-24 19:04:19 UTC",
    "updated_date": "2025-10-24 19:04:19 UTC"
  },
  {
    "arxiv_id": "2510.23643v1",
    "title": "SAND: A Self-supervised and Adaptive NAS-Driven Framework for Hardware Trojan Detection",
    "authors": [
      "Zhixin Pan",
      "Ziyu Shu",
      "Linh Nguyen",
      "Amberbir Alemayoh"
    ],
    "abstract": "The globalized semiconductor supply chain has made Hardware Trojans (HT) a significant security threat to embedded systems, necessitating the design of efficient and adaptable detection mechanisms. Despite promising machine learning-based HT detection techniques in the literature, they suffer from ad hoc feature selection and the lack of adaptivity, all of which hinder their effectiveness across diverse HT attacks. In this paper, we propose SAND, a selfsupervised and adaptive NAS-driven framework for efficient HT detection. Specifically, this paper makes three key contributions. (1) We leverage self-supervised learning (SSL) to enable automated feature extraction, eliminating the dependency on manually engineered features. (2) SAND integrates neural architecture search (NAS) to dynamically optimize the downstream classifier, allowing for seamless adaptation to unseen benchmarks with minimal fine-tuning. (3) Experimental results show that SAND achieves a significant improvement in detection accuracy (up to 18.3%) over state-of-the-art methods, exhibits high resilience against evasive Trojans, and demonstrates strong generalization.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.23643v1",
    "published_date": "2025-10-24 18:55:00 UTC",
    "updated_date": "2025-10-24 18:55:00 UTC"
  },
  {
    "arxiv_id": "2510.21970v1",
    "title": "Performance Trade-offs of Optimizing Small Language Models for E-Commerce",
    "authors": [
      "Josip Tomo Licardo",
      "Nikola Tankovic"
    ],
    "abstract": "Large Language Models (LLMs) offer state-of-the-art performance in natural language understanding and generation tasks. However, the deployment of leading commercial models for specialized tasks, such as e-commerce, is often hindered by high computational costs, latency, and operational expenses. This paper investigates the viability of smaller, open-weight models as a resource-efficient alternative. We present a methodology for optimizing a one-billion-parameter Llama 3.2 model for multilingual e-commerce intent recognition. The model was fine-tuned using Quantized Low-Rank Adaptation (QLoRA) on a synthetically generated dataset designed to mimic real-world user queries. Subsequently, we applied post-training quantization techniques, creating GPU-optimized (GPTQ) and CPU-optimized (GGUF) versions. Our results demonstrate that the specialized 1B model achieves 99% accuracy, matching the performance of the significantly larger GPT-4.1 model. A detailed performance analysis revealed critical, hardware-dependent trade-offs: while 4-bit GPTQ reduced VRAM usage by 41%, it paradoxically slowed inference by 82% on an older GPU architecture (NVIDIA T4) due to dequantization overhead. Conversely, GGUF formats on a CPU achieved a speedup of up to 18x in inference throughput and a reduction of over 90% in RAM consumption compared to the FP16 baseline. We conclude that small, properly optimized open-weight models are not just a viable but a more suitable alternative for domain-specific applications, offering state-of-the-art accuracy at a fraction of the computational cost.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "15 pages, 9 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.21970v1",
    "published_date": "2025-10-24 18:49:28 UTC",
    "updated_date": "2025-10-24 18:49:28 UTC"
  },
  {
    "arxiv_id": "2510.21966v1",
    "title": "ArchISMiner: A Framework for Automatic Mining of Architectural Issue-Solution Pairs from Online Developer Communities",
    "authors": [
      "Musengamana Jean de Dieu",
      "Ruiyin Li",
      "Peng Liang",
      "Mojtaba Shahin",
      "Muhammad Waseem",
      "Arif Ali Khan",
      "Bangchao Wang",
      "Mst Shamima Aktar"
    ],
    "abstract": "Stack Overflow (SO), a leading online community forum, is a rich source of software development knowledge. However, locating architectural knowledge, such as architectural solutions remains challenging due to the overwhelming volume of unstructured content and fragmented discussions. Developers must manually sift through posts to find relevant architectural insights, which is time-consuming and error-prone. This study introduces ArchISMiner, a framework for mining architectural knowledge from SO. The framework comprises two complementary components: ArchPI and ArchISPE. ArchPI trains and evaluates multiple models, including conventional ML/DL models, Pre-trained Language Models (PLMs), and Large Language Models (LLMs), and selects the best-performing model to automatically identify Architecture-Related Posts (ARPs) among programming-related discussions. ArchISPE employs an indirect supervised approach that leverages diverse features, including BERT embeddings and local TextCNN features, to extract architectural issue-solution pairs. Our evaluation shows that the best model in ArchPI achieves an F1-score of 0.960 in ARP detection, and ArchISPE outperforms baselines in both SE and NLP fields, achieving F1-scores of 0.883 for architectural issues and 0.894 for solutions. A user study further validated the quality (e.g., relevance and usefulness) of the identified ARPs and the extracted issue-solution pairs. Moreover, we applied ArchISMiner to three additional forums, releasing a dataset of over 18K architectural issue-solution pairs. Overall, ArchISMiner can help architects and developers identify ARPs and extract succinct, relevant, and useful architectural knowledge from developer communities more accurately and efficiently. The replication package of this study has been provided at https://github.com/JeanMusenga/ArchISPE",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "42 pages, 14 images, 6 tables, Manuscript submitted to a Journal (2025)",
    "pdf_url": "https://arxiv.org/pdf/2510.21966v1",
    "published_date": "2025-10-24 18:46:17 UTC",
    "updated_date": "2025-10-24 18:46:17 UTC"
  },
  {
    "arxiv_id": "2510.21957v1",
    "title": "Towards Low-Latency and Adaptive Ransomware Detection Using Contrastive Learning",
    "authors": [
      "Zhixin Pan",
      "Ziyu Shu",
      "Amberbir Alemayoh"
    ],
    "abstract": "Ransomware has become a critical threat to cybersecurity due to its rapid evolution, the necessity for early detection, and growing diversity, posing significant challenges to traditional detection methods. While AI-based approaches had been proposed by prior works to assist ransomware detection, existing methods suffer from three major limitations, ad-hoc feature dependencies, delayed response, and limited adaptability to unseen variants. In this paper, we propose a framework that integrates self-supervised contrastive learning with neural architecture search (NAS) to address these challenges. Specifically, this paper offers three important contributions. (1) We design a contrastive learning framework that incorporates hardware performance counters (HPC) to analyze the runtime behavior of target ransomware. (2) We introduce a customized loss function that encourages early-stage detection of malicious activity, and significantly reduces the detection latency. (3) We deploy a neural architecture search (NAS) framework to automatically construct adaptive model architectures, allowing the detector to flexibly align with unseen ransomware variants. Experimental results show that our proposed method achieves significant improvements in both detection accuracy (up to 16.1%) and response time (up to 6x) compared to existing approaches while maintaining robustness under evasive attacks.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "This paper was accepted in the 2025 IEEE International Conference on Computer Design (ICCD)",
    "pdf_url": "https://arxiv.org/pdf/2510.21957v1",
    "published_date": "2025-10-24 18:33:52 UTC",
    "updated_date": "2025-10-24 18:33:52 UTC"
  },
  {
    "arxiv_id": "2510.21935v1",
    "title": "AutoSciDACT: Automated Scientific Discovery through Contrastive Embedding and Hypothesis Testing",
    "authors": [
      "Samuel Bright-Thonney",
      "Christina Reissel",
      "Gaia Grosso",
      "Nathaniel Woodward",
      "Katya Govorkova",
      "Andrzej Novak",
      "Sang Eon Park",
      "Eric Moreno",
      "Philip Harris"
    ],
    "abstract": "Novelty detection in large scientific datasets faces two key challenges: the noisy and high-dimensional nature of experimental data, and the necessity of making statistically robust statements about any observed outliers. While there is a wealth of literature on anomaly detection via dimensionality reduction, most methods do not produce outputs compatible with quantifiable claims of scientific discovery. In this work we directly address these challenges, presenting the first step towards a unified pipeline for novelty detection adapted for the rigorous statistical demands of science. We introduce AutoSciDACT (Automated Scientific Discovery with Anomalous Contrastive Testing), a general-purpose pipeline for detecting novelty in scientific data. AutoSciDACT begins by creating expressive low-dimensional data representations using a contrastive pre-training, leveraging the abundance of high-quality simulated data in many scientific domains alongside expertise that can guide principled data augmentation strategies. These compact embeddings then enable an extremely sensitive machine learning-based two-sample test using the New Physics Learning Machine (NPLM) framework, which identifies and statistically quantifies deviations in observed data relative to a reference distribution (null hypothesis). We perform experiments across a range of astronomical, physical, biological, image, and synthetic datasets, demonstrating strong sensitivity to small injections of anomalous data across all domains.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at NeurIPS 2025; 32 pages, 16 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.21935v1",
    "published_date": "2025-10-24 18:07:50 UTC",
    "updated_date": "2025-10-24 18:07:50 UTC"
  },
  {
    "arxiv_id": "2510.21933v1",
    "title": "A Comparison of Conversational Models and Humans in Answering Technical Questions: the Firefox Case",
    "authors": [
      "Joao Correia",
      "Daniel Coutinho",
      "Marco Castelluccio",
      "Caio Barbosa",
      "Rafael de Mello",
      "Anita Sarma",
      "Alessandro Garcia",
      "Marco Gerosa",
      "Igor Steinmacher"
    ],
    "abstract": "The use of Large Language Models (LLMs) to support tasks in software development has steadily increased over recent years. From assisting developers in coding activities to providing conversational agents that answer newcomers' questions. In collaboration with the Mozilla Foundation, this study evaluates the effectiveness of Retrieval-Augmented Generation (RAG) in assisting developers within the Mozilla Firefox project. We conducted an empirical analysis comparing responses from human developers, a standard GPT model, and a GPT model enhanced with RAG, using real queries from Mozilla's developer chat rooms. To ensure a rigorous evaluation, Mozilla experts assessed the responses based on helpfulness, comprehensiveness, and conciseness. The results show that RAG-assisted responses were more comprehensive than human developers (62.50% to 54.17%) and almost as helpful (75.00% to 79.17%), suggesting RAG's potential to enhance developer assistance. However, the RAG responses were not as concise and often verbose. The results show the potential to apply RAG-based tools to Open Source Software (OSS) to minimize the load to core maintainers without losing answer quality. Toning down retrieval mechanisms and making responses even shorter in the future would enhance developer assistance in massive projects like Mozilla Firefox.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "13 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.21933v1",
    "published_date": "2025-10-24 18:05:01 UTC",
    "updated_date": "2025-10-24 18:05:01 UTC"
  },
  {
    "arxiv_id": "2510.23642v1",
    "title": "VisCoder2: Building Multi-Language Visualization Coding Agents",
    "authors": [
      "Yuansheng Ni",
      "Songcheng Cai",
      "Xiangchao Chen",
      "Jiarong Liang",
      "Zhiheng Lyu",
      "Jiaqi Deng",
      "Kai Zou",
      "Ping Nie",
      "Fei Yuan",
      "Xiang Yue",
      "Wenhu Chen"
    ],
    "abstract": "Large language models (LLMs) have recently enabled coding agents capable of generating, executing, and revising visualization code. However, existing models often fail in practical workflows due to limited language coverage, unreliable execution, and lack of iterative correction mechanisms. Progress has been constrained by narrow datasets and benchmarks that emphasize single-round generation and single-language tasks. To address these challenges, we introduce three complementary resources for advancing visualization coding agents. VisCode-Multi-679K is a large-scale, supervised dataset containing 679K validated and executable visualization samples with multi-turn correction dialogues across 12 programming languages. VisPlotBench is a benchmark for systematic evaluation, featuring executable tasks, rendered outputs, and protocols for both initial generation and multi-round self-debug. Finally, we present VisCoder2, a family of multi-language visualization models trained on VisCode-Multi-679K. Experiments show that VisCoder2 significantly outperforms strong open-source baselines and approaches the performance of proprietary models like GPT-4.1, with further gains from iterative self-debug, reaching 82.4% overall execution pass rate at the 32B scale, particularly in symbolic or compiler-dependent languages.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.PL"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.23642v1",
    "published_date": "2025-10-24 18:03:57 UTC",
    "updated_date": "2025-10-24 18:03:57 UTC"
  },
  {
    "arxiv_id": "2512.08937v1",
    "title": "When AI Gives Advice: Evaluating AI and Human Responses to Online Advice-Seeking for Well-Being",
    "authors": [
      "Harsh Kumar",
      "Jasmine Chahal",
      "Yinuo Zhao",
      "Zeling Zhang",
      "Annika Wei",
      "Louis Tay",
      "Ashton Anderson"
    ],
    "abstract": "Seeking advice is a core human behavior that the Internet has reinvented twice: first through forums and Q\\&A communities that crowdsource public guidance, and now through large language models (LLMs) that deliver private, on-demand counsel at scale. Yet the quality of this synthesized LLM advice remains unclear. How does it compare, not only against arbitrary human comments, but against the wisdom of the online crowd? We conducted two studies (N = 210) in which experts compared top-voted Reddit advice with LLM-generated advice. LLMs ranked significantly higher overall and on effectiveness, warmth, and willingness to seek advice again. GPT-4o beat GPT-5 on all metrics except sycophancy, suggesting that benchmark gains need not improve advice-giving. In our second study, we examined how human and algorithmic advice could be combined, and found that human advice can be unobtrusively polished to compete with AI-generated comments. Finally, to surface user expectations, we ran an exploratory survey with undergraduates (N=148) that revealed heterogeneous, persona-dependent preferences for agent qualities (e.g., coach-like: goal-focused structure; friend-like: warmth and humor). We conclude with design implications for advice-giving agents and ecosystems blending AI, crowd input, and expert oversight.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.08937v1",
    "published_date": "2025-10-24 18:02:51 UTC",
    "updated_date": "2025-10-24 18:02:51 UTC"
  },
  {
    "arxiv_id": "2510.23641v1",
    "title": "Spatially Aware Linear Transformer (SAL-T) for Particle Jet Tagging",
    "authors": [
      "Aaron Wang",
      "Zihan Zhao",
      "Subash Katel",
      "Vivekanand Gyanchand Sahu",
      "Elham E Khoda",
      "Abhijith Gandrakota",
      "Jennifer Ngadiuba",
      "Richard Cavanaugh",
      "Javier Duarte"
    ],
    "abstract": "Transformers are very effective in capturing both global and local correlations within high-energy particle collisions, but they present deployment challenges in high-data-throughput environments, such as the CERN LHC. The quadratic complexity of transformer models demands substantial resources and increases latency during inference. In order to address these issues, we introduce the Spatially Aware Linear Transformer (SAL-T), a physics-inspired enhancement of the linformer architecture that maintains linear attention. Our method incorporates spatially aware partitioning of particles based on kinematic features, thereby computing attention between regions of physical significance. Additionally, we employ convolutional layers to capture local correlations, informed by insights from jet physics. In addition to outperforming the standard linformer in jet classification tasks, SAL-T also achieves classification results comparable to full-attention transformers, while using considerably fewer resources with lower latency during inference. Experiments on a generic point cloud classification dataset (ModelNet10) further confirm this trend. Our code is available at https://github.com/aaronw5/SAL-T4HEP.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "hep-ex",
      "physics.ins-det"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.23641v1",
    "published_date": "2025-10-24 18:00:01 UTC",
    "updated_date": "2025-10-24 18:00:01 UTC"
  },
  {
    "arxiv_id": "2510.21695v1",
    "title": "A Knowledge-Graph Translation Layer for Mission-Aware Multi-Agent Path Planning in Spatiotemporal Dynamics",
    "authors": [
      "Edward Holmberg",
      "Elias Ioup",
      "Mahdi Abdelguerfi"
    ],
    "abstract": "The coordination of autonomous agents in dynamic environments is hampered by the semantic gap between high-level mission objectives and low-level planner inputs. To address this, we introduce a framework centered on a Knowledge Graph (KG) that functions as an intelligent translation layer. The KG's two-plane architecture compiles declarative facts into per-agent, mission-aware ``worldviews\" and physics-aware traversal rules, decoupling mission semantics from a domain-agnostic planner. This allows complex, coordinated paths to be modified simply by changing facts in the KG. A case study involving Autonomous Underwater Vehicles (AUVs) in the Gulf of Mexico visually demonstrates the end-to-end process and quantitatively proves that different declarative policies produce distinct, high-performing outcomes. This work establishes the KG not merely as a data repository, but as a powerful, stateful orchestrator for creating adaptive and explainable autonomous systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 10 figures, conference submission",
    "pdf_url": "https://arxiv.org/pdf/2510.21695v1",
    "published_date": "2025-10-24 17:55:55 UTC",
    "updated_date": "2025-10-24 17:55:55 UTC"
  },
  {
    "arxiv_id": "2510.21689v1",
    "title": "On Thin Ice: Towards Explainable Conservation Monitoring via Attribution and Perturbations",
    "authors": [
      "Jiayi Zhou",
      "Günel Aghakishiyeva",
      "Saagar Arya",
      "Julian Dale",
      "James David Poling",
      "Holly R. Houliston",
      "Jamie N. Womble",
      "Gregory D. Larsen",
      "David W. Johnston",
      "Brinnae Bent"
    ],
    "abstract": "Computer vision can accelerate ecological research and conservation monitoring, yet adoption in ecology lags in part because of a lack of trust in black-box neural-network-based models. We seek to address this challenge by applying post-hoc explanations to provide evidence for predictions and document limitations that are important to field deployment. Using aerial imagery from Glacier Bay National Park, we train a Faster R-CNN to detect pinnipeds (harbor seals) and generate explanations via gradient-based class activation mapping (HiResCAM, LayerCAM), local interpretable model-agnostic explanations (LIME), and perturbation-based explanations. We assess explanations along three axes relevant to field use: (i) localization fidelity: whether high-attribution regions coincide with the animal rather than background context; (ii) faithfulness: whether deletion/insertion tests produce changes in detector confidence; and (iii) diagnostic utility: whether explanations reveal systematic failure modes. Explanations concentrate on seal torsos and contours rather than surrounding ice/rock, and removal of the seals reduces detection confidence, providing model-evidence for true positives. The analysis also uncovers recurrent error sources, including confusion between seals and black ice and rocks. We translate these findings into actionable next steps for model development, including more targeted data curation and augmentation. By pairing object detection with post-hoc explainability, we can move beyond \"black-box\" predictions toward auditable, decision-supporting tools for conservation monitoring.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "NeurIPS Imageomics Workshop 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.21689v1",
    "published_date": "2025-10-24 17:46:24 UTC",
    "updated_date": "2025-10-24 17:46:24 UTC"
  },
  {
    "arxiv_id": "2510.21679v1",
    "title": "A Multimodal Benchmark for Framing of Oil & Gas Advertising and Potential Greenwashing Detection",
    "authors": [
      "Gaku Morio",
      "Harri Rowlands",
      "Dominik Stammbach",
      "Christopher D. Manning",
      "Peter Henderson"
    ],
    "abstract": "Companies spend large amounts of money on public relations campaigns to project a positive brand image. However, sometimes there is a mismatch between what they say and what they do. Oil & gas companies, for example, are accused of \"greenwashing\" with imagery of climate-friendly initiatives. Understanding the framing, and changes in framing, at scale can help better understand the goals and nature of public relations campaigns. To address this, we introduce a benchmark dataset of expert-annotated video ads obtained from Facebook and YouTube. The dataset provides annotations for 13 framing types for more than 50 companies or advocacy groups across 20 countries. Our dataset is especially designed for the evaluation of vision-language models (VLMs), distinguishing it from past text-only framing datasets. Baseline experiments show some promising results, while leaving room for improvement for future work: GPT-4.1 can detect environmental messages with 79% F1 score, while our best model only achieves 46% F1 score on identifying framing around green innovation. We also identify challenges that VLMs must address, such as implicit framing, handling videos of various lengths, or implicit cultural backgrounds. Our dataset contributes to research in multimodal analysis of strategic communication in the energy sector.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Forthcoming in NeurIPS 2025 Datasets and Benchmarks Track",
    "pdf_url": "https://arxiv.org/pdf/2510.21679v1",
    "published_date": "2025-10-24 17:34:28 UTC",
    "updated_date": "2025-10-24 17:34:28 UTC"
  },
  {
    "arxiv_id": "2510.23640v1",
    "title": "Structure-Aware Fusion with Progressive Injection for Multimodal Molecular Representation Learning",
    "authors": [
      "Zihao Jing",
      "Yan Sun",
      "Yan Yi Li",
      "Sugitha Janarthanan",
      "Alana Deng",
      "Pingzhao Hu"
    ],
    "abstract": "Multimodal molecular models often suffer from 3D conformer unreliability and modality collapse, limiting their robustness and generalization. We propose MuMo, a structured multimodal fusion framework that addresses these challenges in molecular representation through two key strategies. To reduce the instability of conformer-dependent fusion, we design a Structured Fusion Pipeline (SFP) that combines 2D topology and 3D geometry into a unified and stable structural prior. To mitigate modality collapse caused by naive fusion, we introduce a Progressive Injection (PI) mechanism that asymmetrically integrates this prior into the sequence stream, preserving modality-specific modeling while enabling cross-modal enrichment. Built on a state space backbone, MuMo supports long-range dependency modeling and robust information propagation. Across 29 benchmark tasks from Therapeutics Data Commons (TDC) and MoleculeNet, MuMo achieves an average improvement of 2.7% over the best-performing baseline on each task, ranking first on 22 of them, including a 27% improvement on the LD50 task. These results validate its robustness to 3D conformer noise and the effectiveness of multimodal fusion in molecular representation. The code is available at: github.com/selmiss/MuMo.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.23640v1",
    "published_date": "2025-10-24 17:27:10 UTC",
    "updated_date": "2025-10-24 17:27:10 UTC"
  },
  {
    "arxiv_id": "2510.21908v2",
    "title": "Enabling Robust In-Context Memory and Rapid Task Adaptation in Transformers with Hebbian and Gradient-Based Plasticity",
    "authors": [
      "Siddharth Chaudhary"
    ],
    "abstract": "Large language models display in-context learning as an emergent effect of scale, but they rely on static weights during inference. In contrast, biological systems continually adapt via synaptic plasticity. We investigate whether explicit, biologically inspired plasticity can endow Transformers with faster in-sequence adaptation. To this end, we augment decoder-only Transformers with fast-weight modules updated either by (i) a neuromodulated Hebbian rule or (ii) the gradient-based plasticity mechanism of Duan et al. (2023). Across copying, regression, and few-shot classification tasks (CIFAR-FS, Omniglot), Hebbian plasticity consistently achieves lower loss and stronger few-shot generalization, while gradient-based updates perform best on long-horizon credit assignment. When associations are short and linearly separable, static weights suffice, defining a clear boundary condition for when plasticity helps. Analysis of learned modulatory signals reveals that gradient-based rules maintain large, persistent updates, whereas Hebbian plasticity is sharply gated around salient events. Together, these results show that explicit plasticity complements attention by enabling rapid, task-specific adaptation, and clarify when different plasticity mechanisms are most effective.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21908v2",
    "published_date": "2025-10-24 17:26:03 UTC",
    "updated_date": "2025-11-04 22:10:19 UTC"
  },
  {
    "arxiv_id": "2510.24772v1",
    "title": "Confidence is Not Competence",
    "authors": [
      "Debdeep Sanyal",
      "Manya Pandey",
      "Dhruv Kumar",
      "Saurabh Deshpande",
      "Murari Mandal"
    ],
    "abstract": "Large language models (LLMs) often exhibit a puzzling disconnect between their asserted confidence and actual problem-solving competence. We offer a mechanistic account of this decoupling by analyzing the geometry of internal states across two phases - pre-generative assessment and solution execution. A simple linear probe decodes the internal \"solvability belief\" of a model, revealing a well-ordered belief axis that generalizes across model families and across math, code, planning, and logic tasks. Yet, the geometries diverge - although belief is linearly decodable, the assessment manifold has high linear effective dimensionality as measured from the principal components, while the subsequent reasoning trace evolves on a much lower-dimensional manifold. This sharp reduction in geometric complexity from thought to action mechanistically explains the confidence-competence gap. Causal interventions that steer representations along the belief axis leave final solutions unchanged, indicating that linear nudges in the complex assessment space do not control the constrained dynamics of execution. We thus uncover a two-system architecture - a geometrically complex assessor feeding a geometrically simple executor. These results challenge the assumption that decodable beliefs are actionable levers, instead arguing for interventions that target the procedural dynamics of execution rather than the high-level geometry of assessment.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "20 Pages, 6 Figures, 8 Tables",
    "pdf_url": "https://arxiv.org/pdf/2510.24772v1",
    "published_date": "2025-10-24 17:22:48 UTC",
    "updated_date": "2025-10-24 17:22:48 UTC"
  },
  {
    "arxiv_id": "2510.21656v1",
    "title": "CMOMgen: Complex Multi-Ontology Alignment via Pattern-Guided In-Context Learning",
    "authors": [
      "Marta Contreiras Silva",
      "Daniel Faria",
      "Catia Pesquita"
    ],
    "abstract": "Constructing comprehensive knowledge graphs requires the use of multiple ontologies in order to fully contextualize data into a domain. Ontology matching finds equivalences between concepts interconnecting ontologies and creating a cohesive semantic layer. While the simple pairwise state of the art is well established, simple equivalence mappings cannot provide full semantic integration of related but disjoint ontologies. Complex multi-ontology matching (CMOM) aligns one source entity to composite logical expressions of multiple target entities, establishing more nuanced equivalences and provenance along the ontological hierarchy.\n  We present CMOMgen, the first end-to-end CMOM strategy that generates complete and semantically sound mappings, without establishing any restrictions on the number of target ontologies or entities. Retrieval-Augmented Generation selects relevant classes to compose the mapping and filters matching reference mappings to serve as examples, enhancing In-Context Learning. The strategy was evaluated in three biomedical tasks with partial reference alignments. CMOMgen outperforms baselines in class selection, demonstrating the impact of having a dedicated strategy. Our strategy also achieves a minimum of 63% in F1-score, outperforming all baselines and ablated versions in two out of three tasks and placing second in the third. Furthermore, a manual evaluation of non-reference mappings showed that 46% of the mappings achieve the maximum score, further substantiating its ability to construct semantically sound mappings.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "32 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.21656v1",
    "published_date": "2025-10-24 17:12:22 UTC",
    "updated_date": "2025-10-24 17:12:22 UTC"
  },
  {
    "arxiv_id": "2510.21654v1",
    "title": "Group Inertial Poser: Multi-Person Pose and Global Translation from Sparse Inertial Sensors and Ultra-Wideband Ranging",
    "authors": [
      "Ying Xue",
      "Jiaxi Jiang",
      "Rayan Armani",
      "Dominik Hollidt",
      "Yi-Chi Liao",
      "Christian Holz"
    ],
    "abstract": "Tracking human full-body motion using sparse wearable inertial measurement units (IMUs) overcomes the limitations of occlusion and instrumentation of the environment inherent in vision-based approaches. However, purely IMU-based tracking compromises translation estimates and accurate relative positioning between individuals, as inertial cues are inherently self-referential and provide no direct spatial reference for others. In this paper, we present a novel approach for robustly estimating body poses and global translation for multiple individuals by leveraging the distances between sparse wearable sensors - both on each individual and across multiple individuals. Our method Group Inertial Poser estimates these absolute distances between pairs of sensors from ultra-wideband ranging (UWB) and fuses them with inertial observations as input into structured state-space models to integrate temporal motion patterns for precise 3D pose estimation. Our novel two-step optimization further leverages the estimated distances for accurately tracking people's global trajectories through the world. We also introduce GIP-DB, the first IMU+UWB dataset for two-person tracking, which comprises 200 minutes of motion recordings from 14 participants. In our evaluation, Group Inertial Poser outperforms previous state-of-the-art methods in accuracy and robustness across synthetic and real-world data, showing the promise of IMU+UWB-based multi-human motion capture in the wild. Code, models, dataset: https://github.com/eth-siplab/GroupInertialPoser",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.HC"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ICCV 2025, Code: https://github.com/eth-siplab/GroupInertialPoser",
    "pdf_url": "https://arxiv.org/pdf/2510.21654v1",
    "published_date": "2025-10-24 17:11:50 UTC",
    "updated_date": "2025-10-24 17:11:50 UTC"
  },
  {
    "arxiv_id": "2510.21652v1",
    "title": "AstaBench: Rigorous Benchmarking of AI Agents with a Scientific Research Suite",
    "authors": [
      "Jonathan Bragg",
      "Mike D'Arcy",
      "Nishant Balepur",
      "Dan Bareket",
      "Bhavana Dalvi",
      "Sergey Feldman",
      "Dany Haddad",
      "Jena D. Hwang",
      "Peter Jansen",
      "Varsha Kishore",
      "Bodhisattwa Prasad Majumder",
      "Aakanksha Naik",
      "Sigal Rahamimov",
      "Kyle Richardson",
      "Amanpreet Singh",
      "Harshit Surana",
      "Aryeh Tiktinsky",
      "Rosni Vasu",
      "Guy Wiener",
      "Chloe Anastasiades",
      "Stefan Candra",
      "Jason Dunkelberger",
      "Dan Emery",
      "Rob Evans",
      "Malachi Hamada",
      "Regan Huff",
      "Rodney Kinney",
      "Matt Latzke",
      "Jaron Lochner",
      "Ruben Lozano-Aguilera",
      "Cecile Nguyen",
      "Smita Rao",
      "Amber Tanaka",
      "Brooke Vlahos",
      "Peter Clark",
      "Doug Downey",
      "Yoav Goldberg",
      "Ashish Sabharwal",
      "Daniel S. Weld"
    ],
    "abstract": "AI agents hold the potential to revolutionize scientific productivity by automating literature reviews, replicating experiments, analyzing data, and even proposing new directions of inquiry; indeed, there are now many such agents, ranging from general-purpose \"deep research\" systems to specialized science-specific agents, such as AI Scientist and AIGS. Rigorous evaluation of these agents is critical for progress. Yet existing benchmarks fall short on several fronts: they (1) fail to provide holistic, product-informed measures of real-world use cases such as science research; (2) lack reproducible agent tools necessary for a controlled comparison of core agentic capabilities; (3) do not account for confounding variables such as model cost and tool access; (4) do not provide standardized interfaces for quick agent prototyping and evaluation; and (5) lack comprehensive baseline agents necessary to identify true advances. In response, we define principles and tooling for more rigorously benchmarking agents. Using these, we present AstaBench, a suite that provides the first holistic measure of agentic ability to perform scientific research, comprising 2400+ problems spanning the entire scientific discovery process and multiple scientific domains, and including many problems inspired by actual user requests to deployed Asta agents. Our suite comes with the first scientific research environment with production-grade search tools that enable controlled, reproducible evaluation, better accounting for confounders. Alongside, we provide a comprehensive suite of nine science-optimized classes of Asta agents and numerous baselines. Our extensive evaluation of 57 agents across 22 agent classes reveals several interesting findings, most importantly that despite meaningful progress on certain individual aspects, AI remains far from solving the challenge of science research assistance.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21652v1",
    "published_date": "2025-10-24 17:10:26 UTC",
    "updated_date": "2025-10-24 17:10:26 UTC"
  },
  {
    "arxiv_id": "2510.21649v1",
    "title": "A Dynamic Knowledge Distillation Method Based on the Gompertz Curve",
    "authors": [
      "Han Yang",
      "Guangjun Qin"
    ],
    "abstract": "This paper introduces a novel dynamic knowledge distillation framework, Gompertz-CNN, which integrates the Gompertz growth model into the training process to address the limitations of traditional knowledge distillation. Conventional methods often fail to capture the evolving cognitive capacity of student models, leading to suboptimal knowledge transfer. To overcome this, we propose a stage-aware distillation strategy that dynamically adjusts the weight of distillation loss based on the Gompertz curve, reflecting the student's learning progression: slow initial growth, rapid mid-phase improvement, and late-stage saturation. Our framework incorporates Wasserstein distance to measure feature-level discrepancies and gradient matching to align backward propagation behaviors between teacher and student models. These components are unified under a multi-loss objective, where the Gompertz curve modulates the influence of distillation losses over time. Extensive experiments on CIFAR-10 and CIFAR-100 using various teacher-student architectures (e.g., ResNet50 and MobileNet_v2) demonstrate that Gompertz-CNN consistently outperforms traditional distillation methods, achieving up to 8% and 4% accuracy gains on CIFAR-10 and CIFAR-100, respectively.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "15 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.21649v1",
    "published_date": "2025-10-24 17:07:27 UTC",
    "updated_date": "2025-10-24 17:07:27 UTC"
  },
  {
    "arxiv_id": "2511.11591v1",
    "title": "LLM-Generated Negative News Headlines Dataset: Creation and Benchmarking Against Real Journalism",
    "authors": [
      "Olusola Babalola",
      "Bolanle Ojokoh",
      "Olutayo Boyinbode"
    ],
    "abstract": "This research examines the potential of datasets generated by Large Language Models (LLMs) to support Natural Language Processing (NLP) tasks, aiming to overcome challenges related to data acquisition and privacy concerns associated with real-world data. Focusing on negative valence text, a critical component of sentiment analysis, we explore the use of LLM-generated synthetic news headlines as an alternative to real-world data. A specialized corpus of negative news headlines was created using tailored prompts to capture diverse negative sentiments across various societal domains. The synthetic headlines were validated by expert review and further analyzed in embedding space to assess their alignment with real-world negative news in terms of content, tone, length, and style. Key metrics such as correlation with real headlines, perplexity, coherence, and realism were evaluated. The synthetic dataset was benchmarked against two sets of real news headlines using evaluations including the Comparative Perplexity Test, Comparative Readability Test, Comparative POS Profiling, BERTScore, and Comparative Semantic Similarity. Results show the generated headlines match real headlines with the only marked divergence being in the proper noun score of the POS profile test.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "50 pages, 19 figures, 9 tables",
    "pdf_url": "https://arxiv.org/pdf/2511.11591v1",
    "published_date": "2025-10-24 16:51:35 UTC",
    "updated_date": "2025-10-24 16:51:35 UTC"
  },
  {
    "arxiv_id": "2510.21638v1",
    "title": "DEEDEE: Fast and Scalable Out-of-Distribution Dynamics Detection",
    "authors": [
      "Tala Aljaafari",
      "Varun Kanade",
      "Philip Torr",
      "Christian Schroeder de Witt"
    ],
    "abstract": "Deploying reinforcement learning (RL) in safety-critical settings is constrained by brittleness under distribution shift. We study out-of-distribution (OOD) detection for RL time series and introduce DEEDEE, a two-statistic detector that revisits representation-heavy pipelines with a minimal alternative. DEEDEE uses only an episodewise mean and an RBF kernel similarity to a training summary, capturing complementary global and local deviations. Despite its simplicity, DEEDEE matches or surpasses contemporary detectors across standard RL OOD suites, delivering a 600-fold reduction in compute (FLOPs / wall-time) and an average 5% absolute accuracy gain over strong baselines. Conceptually, our results indicate that diverse anomaly types often imprint on RL trajectories through a small set of low-order statistics, suggesting a compact foundation for OOD detection in complex environments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21638v1",
    "published_date": "2025-10-24 16:51:17 UTC",
    "updated_date": "2025-10-24 16:51:17 UTC"
  },
  {
    "arxiv_id": "2510.21906v1",
    "title": "Structure-Aware Cooperative Ensemble Evolutionary Optimization on Combinatorial Problems with Multimodal Large Language Models",
    "authors": [
      "Jie Zhao",
      "Kang Hao Cheong"
    ],
    "abstract": "Evolutionary algorithms (EAs) have proven effective in exploring the vast solution spaces typical of graph-structured combinatorial problems. However, traditional encoding schemes, such as binary or numerical representations, often fail to straightforwardly capture the intricate structural properties of networks. Through employing the image-based encoding to preserve topological context, this study utilizes multimodal large language models (MLLMs) as evolutionary operators to facilitate structure-aware optimization over graph data. To address the visual clutter inherent in large-scale network visualizations, we leverage graph sparsification techniques to simplify structures while maintaining essential structural features. To further improve robustness and mitigate bias from different sparsification views, we propose a cooperative evolutionary optimization framework that facilitates cross-domain knowledge transfer and unifies multiple sparsified variants of diverse structures. Additionally, recognizing the sensitivity of MLLMs to network layout, we introduce an ensemble strategy that aggregates outputs from various layout configurations through consensus voting. Finally, experiments on real-world networks through various tasks demonstrate that our approach improves both the quality and reliability of solutions in MLLM-driven evolutionary optimization.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21906v1",
    "published_date": "2025-10-24 16:37:41 UTC",
    "updated_date": "2025-10-24 16:37:41 UTC"
  },
  {
    "arxiv_id": "2510.21631v1",
    "title": "Few-Shot Knowledge Distillation of LLMs With Counterfactual Explanations",
    "authors": [
      "Faisal Hamman",
      "Pasan Dissanayake",
      "Yanjun Fu",
      "Sanghamitra Dutta"
    ],
    "abstract": "Knowledge distillation is a promising approach to transfer capabilities from complex teacher models to smaller, resource-efficient student models that can be deployed easily, particularly in task-aware scenarios. However, existing methods of task-aware distillation typically require substantial quantities of data which may be unavailable or expensive to obtain in many practical scenarios. In this paper, we address this challenge by introducing a novel strategy called Counterfactual-explanation-infused Distillation CoD for few-shot task-aware knowledge distillation by systematically infusing counterfactual explanations. Counterfactual explanations (CFEs) refer to inputs that can flip the output prediction of the teacher model with minimum perturbation. Our strategy CoD leverages these CFEs to precisely map the teacher's decision boundary with significantly fewer samples. We provide theoretical guarantees for motivating the role of CFEs in distillation, from both statistical and geometric perspectives. We mathematically show that CFEs can improve parameter estimation by providing more informative examples near the teacher's decision boundary. We also derive geometric insights on how CFEs effectively act as knowledge probes, helping the students mimic the teacher's decision boundaries more effectively than standard data. We perform experiments across various datasets and LLMs to show that CoD outperforms standard distillation approaches in few-shot regimes (as low as 8-512 samples). Notably, CoD only uses half of the original samples used by the baselines, paired with their corresponding CFEs and still improves performance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.21631v1",
    "published_date": "2025-10-24 16:36:34 UTC",
    "updated_date": "2025-10-24 16:36:34 UTC"
  },
  {
    "arxiv_id": "2510.21623v1",
    "title": "The Universal Landscape of Human Reasoning",
    "authors": [
      "Qiguang Chen",
      "Jinhao Liu",
      "Libo Qin",
      "Yimeng Zhang",
      "Yihao Liang",
      "Shangxu Ren",
      "Chengyu Luan",
      "Dengyun Peng",
      "Hanjing Li",
      "Jiannan Guan",
      "Zheng Yan",
      "Jiaqi Wang",
      "Mengkang Hu",
      "Yantao Du",
      "Zhi Chen",
      "Xie Chen",
      "Wanxiang Che"
    ],
    "abstract": "Understanding how information is dynamically accumulated and transformed in human reasoning has long challenged cognitive psychology, philosophy, and artificial intelligence. Existing accounts, from classical logic to probabilistic models, illuminate aspects of output or individual modelling, but do not offer a unified, quantitative description of general human reasoning dynamics. To solve this, we introduce Information Flow Tracking (IF-Track), that uses large language models (LLMs) as probabilistic encoder to quantify information entropy and gain at each reasoning step. Through fine-grained analyses across diverse tasks, our method is the first successfully models the universal landscape of human reasoning behaviors within a single metric space. We show that IF-Track captures essential reasoning features, identifies systematic error patterns, and characterizes individual differences. Applied to discussion of advanced psychological theory, we first reconcile single- versus dual-process theories in IF-Track and discover the alignment of artificial and human cognition and how LLMs reshaping human reasoning process. This approach establishes a quantitative bridge between theory and measurement, offering mechanistic insights into the architecture of reasoning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint",
    "pdf_url": "https://arxiv.org/pdf/2510.21623v1",
    "published_date": "2025-10-24 16:26:36 UTC",
    "updated_date": "2025-10-24 16:26:36 UTC"
  },
  {
    "arxiv_id": "2510.21618v1",
    "title": "DeepAgent: A General Reasoning Agent with Scalable Toolsets",
    "authors": [
      "Xiaoxi Li",
      "Wenxiang Jiao",
      "Jiarui Jin",
      "Guanting Dong",
      "Jiajie Jin",
      "Yinuo Wang",
      "Hao Wang",
      "Yutao Zhu",
      "Ji-Rong Wen",
      "Yuan Lu",
      "Zhicheng Dou"
    ],
    "abstract": "Large reasoning models have demonstrated strong problem-solving abilities, yet real-world tasks often require external tools and long-horizon interactions. Existing agent frameworks typically follow predefined workflows, which limit autonomous and global task completion. In this paper, we introduce DeepAgent, an end-to-end deep reasoning agent that performs autonomous thinking, tool discovery, and action execution within a single, coherent reasoning process. To address the challenges of long-horizon interactions, particularly the context length explosion from multiple tool calls and the accumulation of interaction history, we introduce an autonomous memory folding mechanism that compresses past interactions into structured episodic, working, and tool memories, reducing error accumulation while preserving critical information. To teach general-purpose tool use efficiently and stably, we develop an end-to-end reinforcement learning strategy, namely ToolPO, that leverages LLM-simulated APIs and applies tool-call advantage attribution to assign fine-grained credit to the tool invocation tokens. Extensive experiments on eight benchmarks, including general tool-use tasks (ToolBench, API-Bank, TMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA, HLE), demonstrate that DeepAgent consistently outperforms baselines across both labeled-tool and open-set tool retrieval scenarios. This work takes a step toward more general and capable agents for real-world applications. The code and demo are available at https://github.com/RUC-NLPIR/DeepAgent.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21618v1",
    "published_date": "2025-10-24 16:24:01 UTC",
    "updated_date": "2025-10-24 16:24:01 UTC"
  },
  {
    "arxiv_id": "2510.21614v3",
    "title": "Huxley-Gödel Machine: Human-Level Coding Agent Development by an Approximation of the Optimal Self-Improving Machine",
    "authors": [
      "Wenyi Wang",
      "Piotr Piękos",
      "Li Nanbo",
      "Firas Laakom",
      "Yimeng Chen",
      "Mateusz Ostaszewski",
      "Mingchen Zhuge",
      "Jürgen Schmidhuber"
    ],
    "abstract": "Recent studies operationalize self-improvement through coding agents that edit their own codebases. They grow a tree of self-modifications through expansion strategies that favor higher software engineering benchmark performance, assuming that this implies more promising subsequent self-modifications. However, we identify a mismatch between the agent's self-improvement potential (metaproductivity) and its coding benchmark performance, namely the Metaproductivity-Performance Mismatch. Inspired by Huxley's concept of clade, we propose a metric ($\\mathrm{CMP}$) that aggregates the benchmark performances of the descendants of an agent as an indicator of its potential for self-improvement. We show that, in our self-improving coding agent development setting, access to the true $\\mathrm{CMP}$ is sufficient to simulate how the Gödel Machine would behave under certain assumptions. We introduce the Huxley-Gödel Machine (HGM), which, by estimating $\\mathrm{CMP}$ and using it as guidance, searches the tree of self-modifications. On SWE-bench Verified and Polyglot, HGM outperforms prior self-improving coding agent development methods while using fewer allocated CPU hours. Last but not least, HGM demonstrates strong transfer to other coding datasets and large language models. The agent optimized by HGM on SWE-bench Verified with GPT-5-mini and evaluated on SWE-bench Lite with GPT-5 achieves human-level performance, matching the best officially checked results of human-engineered coding agents. Our code is publicly available at https://github.com/metauto-ai/HGM.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21614v3",
    "published_date": "2025-10-24 16:19:41 UTC",
    "updated_date": "2025-10-29 13:57:25 UTC"
  },
  {
    "arxiv_id": "2510.21610v1",
    "title": "Generative Correlation Manifolds: Generating Synthetic Data with Preserved Higher-Order Correlations",
    "authors": [
      "Jens E. d'Hondt",
      "Wieger R. Punter",
      "Odysseas Papapetrou"
    ],
    "abstract": "The increasing need for data privacy and the demand for robust machine learning models have fueled the development of synthetic data generation techniques. However, current methods often succeed in replicating simple summary statistics but fail to preserve both the pairwise and higher-order correlation structure of the data that define the complex, multi-variable interactions inherent in real-world systems. This limitation can lead to synthetic data that is superficially realistic but fails when used for sophisticated modeling tasks. In this white paper, we introduce Generative Correlation Manifolds (GCM), a computationally efficient method for generating synthetic data. The technique uses Cholesky decomposition of a target correlation matrix to produce datasets that, by mathematical proof, preserve the entire correlation structure -- from simple pairwise relationships to higher-order interactions -- of the source dataset. We argue that this method provides a new approach to synthetic data generation with potential applications in privacy-preserving data sharing, robust model training, and simulation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21610v1",
    "published_date": "2025-10-24 16:15:53 UTC",
    "updated_date": "2025-10-24 16:15:53 UTC"
  },
  {
    "arxiv_id": "2510.21903v1",
    "title": "TOM-SWE: User Mental Modeling For Software Engineering Agents",
    "authors": [
      "Xuhui Zhou",
      "Valerie Chen",
      "Zora Zhiruo Wang",
      "Graham Neubig",
      "Maarten Sap",
      "Xingyao Wang"
    ],
    "abstract": "Recent advances in coding agents have made them capable of planning, editing, running, and testing complex code bases. Despite their growing ability in coding tasks, these systems still struggle to infer and track user intent, especially when instructions are underspecified or context-dependent. To bridge this gap, we introduce ToM-SWE, a dual-agent architecture that pairs a primary software-engineering (SWE) agent with a lightweight theory-of-mind (ToM) partner agent dedicated to modeling the user's mental state. The ToM agent infers user goals, constraints, and preferences from instructions and interaction history, maintains a \\textbf{persistent memory} of the user, and provides user-related suggestions to the SWE agent. In two software engineering benchmarks (ambiguous SWE-bench and stateful SWE-bench), ToM-SWE improves task success rates and user satisfaction. Notably, on the stateful SWE benchmark, a newly introduced evaluation that provides agents with a user simulator along with previous interaction histories, ToM-SWE achieves a substantially higher task success rate of 59.7\\% compared to 18.1\\% for OpenHands, a state-of-the-art SWE agent. Furthermore, in a three-week study with professional developers using ToM-SWE in their daily work, participants found it useful 86\\% of the time, underscoring the value of stateful user modeling for practical coding agents.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21903v1",
    "published_date": "2025-10-24 16:09:51 UTC",
    "updated_date": "2025-10-24 16:09:51 UTC"
  },
  {
    "arxiv_id": "2510.21902v1",
    "title": "Software Engineering Agents for Embodied Controller Generation : A Study in Minigrid Environments",
    "authors": [
      "Timothé Boulet",
      "Xavier Hinaut",
      "Clément Moulin-Frier"
    ],
    "abstract": "Software Engineering Agents (SWE-Agents) have proven effective for traditional software engineering tasks with accessible codebases, but their performance for embodied tasks requiring well-designed information discovery remains unexplored. We present the first extended evaluation of SWE-Agents on controller generation for embodied tasks, adapting Mini-SWE-Agent (MSWEA) to solve 20 diverse embodied tasks from the Minigrid environment. Our experiments compare agent performance across different information access conditions: with and without environment source code access, and with varying capabilities for interactive exploration. We quantify how different information access levels affect SWE-Agent performance for embodied tasks and analyze the relative importance of static code analysis versus dynamic exploration for task solving. This work establishes controller generation for embodied tasks as a crucial evaluation domain for SWE-Agents and provides baseline results for future research in efficient reasoning systems.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "10 pages, 7 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.21902v1",
    "published_date": "2025-10-24 16:04:11 UTC",
    "updated_date": "2025-10-24 16:04:11 UTC"
  },
  {
    "arxiv_id": "2510.23639v3",
    "title": "Integrating Genomics into Multimodal EHR Foundation Models",
    "authors": [
      "Jonathan Amar",
      "Edward Liu",
      "Alessandra Breschi",
      "Liangliang Zhang",
      "Pouya Kheradpour",
      "Sylvia Li",
      "Lisa Soleymani Lehmann",
      "Alessandro Giulianelli",
      "Matt Edwards",
      "Yugang Jia",
      "David Nola",
      "Raghav Mani",
      "Pankaj Vats",
      "Jesse Tetreault",
      "T. J. Chen",
      "Cory Y. McLean"
    ],
    "abstract": "This paper introduces an innovative Electronic Health Record (EHR) foundation model that integrates Polygenic Risk Scores (PRS) as a foundational data modality, moving beyond traditional EHR-only approaches to build more holistic health profiles. Leveraging the extensive and diverse data from the All of Us (AoU) Research Program, this multimodal framework aims to learn complex relationships between clinical data and genetic predispositions. The methodology extends advancements in generative AI to the EHR foundation model space, enhancing predictive capabilities and interpretability. Evaluation on AoU data demonstrates the model's predictive value for the onset of various conditions, particularly Type 2 Diabetes (T2D), and illustrates the interplay between PRS and EHR data. The work also explores transfer learning for custom classification tasks, showcasing the architecture's versatility and efficiency. This approach is pivotal for unlocking new insights into disease prediction, proactive health management, risk stratification, and personalized treatment strategies, laying the groundwork for more personalized, equitable, and actionable real-world evidence generation in healthcare.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.23639v3",
    "published_date": "2025-10-24 15:56:40 UTC",
    "updated_date": "2025-11-14 22:32:02 UTC"
  },
  {
    "arxiv_id": "2510.21583v1",
    "title": "Sample By Step, Optimize By Chunk: Chunk-Level GRPO For Text-to-Image Generation",
    "authors": [
      "Yifu Luo",
      "Penghui Du",
      "Bo Li",
      "Sinan Du",
      "Tiantian Zhang",
      "Yongzhe Chang",
      "Kai Wu",
      "Kun Gai",
      "Xueqian Wang"
    ],
    "abstract": "Group Relative Policy Optimization (GRPO) has shown strong potential for flow-matching-based text-to-image (T2I) generation, but it faces two key limitations: inaccurate advantage attribution, and the neglect of temporal dynamics of generation. In this work, we argue that shifting the optimization paradigm from the step level to the chunk level can effectively alleviate these issues. Building on this idea, we propose Chunk-GRPO, the first chunk-level GRPO-based approach for T2I generation. The insight is to group consecutive steps into coherent 'chunk's that capture the intrinsic temporal dynamics of flow matching, and to optimize policies at the chunk level. In addition, we introduce an optional weighted sampling strategy to further enhance performance. Extensive experiments show that ChunkGRPO achieves superior results in both preference alignment and image quality, highlighting the promise of chunk-level optimization for GRPO-based methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "11 pages, preprint",
    "pdf_url": "https://arxiv.org/pdf/2510.21583v1",
    "published_date": "2025-10-24 15:50:36 UTC",
    "updated_date": "2025-10-24 15:50:36 UTC"
  },
  {
    "arxiv_id": "2510.21575v1",
    "title": "From Polyester Girlfriends to Blind Mice: Creating the First Pragmatics Understanding Benchmarks for Slovene",
    "authors": [
      "Mojca Brglez",
      "Špela Vintar"
    ],
    "abstract": "Large language models are demonstrating increasing capabilities, excelling at benchmarks once considered very difficult. As their capabilities grow, there is a need for more challenging evaluations that go beyond surface-level linguistic competence. Namely, language competence involves not only syntax and semantics but also pragmatics, i.e., understanding situational meaning as shaped by context as well as linguistic and cultural norms. To contribute to this line of research, we introduce SloPragEval and SloPragMega, the first pragmatics understanding benchmarks for Slovene that contain altogether 405 multiple-choice questions. We discuss the difficulties of translation, describe the campaign to establish a human baseline, and report pilot evaluations with LLMs. Our results indicate that current models have greatly improved in understanding nuanced language but may still fail to infer implied speaker meaning in non-literal utterances, especially those that are culture-specific. We also observe a significant gap between proprietary and open-source models. Finally, we argue that benchmarks targeting nuanced language understanding and knowledge of the target culture must be designed with care, preferably constructed from native data, and validated with human responses.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21575v1",
    "published_date": "2025-10-24 15:43:42 UTC",
    "updated_date": "2025-10-24 15:43:42 UTC"
  },
  {
    "arxiv_id": "2510.21571v1",
    "title": "Scalable Vision-Language-Action Model Pretraining for Robotic Manipulation with Real-Life Human Activity Videos",
    "authors": [
      "Qixiu Li",
      "Yu Deng",
      "Yaobo Liang",
      "Lin Luo",
      "Lei Zhou",
      "Chengtang Yao",
      "Lingqi Zeng",
      "Zhiyuan Feng",
      "Huizhi Liang",
      "Sicheng Xu",
      "Yizhong Zhang",
      "Xi Chen",
      "Hao Chen",
      "Lily Sun",
      "Dong Chen",
      "Jiaolong Yang",
      "Baining Guo"
    ],
    "abstract": "This paper presents a novel approach for pretraining robotic manipulation Vision-Language-Action (VLA) models using a large corpus of unscripted real-life video recordings of human hand activities. Treating human hand as dexterous robot end-effector, we show that \"in-the-wild\" egocentric human videos without any annotations can be transformed into data formats fully aligned with existing robotic V-L-A training data in terms of task granularity and labels. This is achieved by the development of a fully-automated holistic human activity analysis approach for arbitrary human hand videos. This approach can generate atomic-level hand activity segments and their language descriptions, each accompanied with framewise 3D hand motion and camera motion. We process a large volume of egocentric videos and create a hand-VLA training dataset containing 1M episodes and 26M frames. This training data covers a wide range of objects and concepts, dexterous manipulation tasks, and environment variations in real life, vastly exceeding the coverage of existing robot data. We design a dexterous hand VLA model architecture and pretrain the model on this dataset. The model exhibits strong zero-shot capabilities on completely unseen real-world observations. Additionally, fine-tuning it on a small amount of real robot action data significantly improves task success rates and generalization to novel objects in real robotic experiments. We also demonstrate the appealing scaling behavior of the model's task performance with respect to pretraining data scale. We believe this work lays a solid foundation for scalable VLA pretraining, advancing robots toward truly generalizable embodied intelligence.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Project page: https://microsoft.github.io/VITRA/",
    "pdf_url": "https://arxiv.org/pdf/2510.21571v1",
    "published_date": "2025-10-24 15:39:31 UTC",
    "updated_date": "2025-10-24 15:39:31 UTC"
  },
  {
    "arxiv_id": "2510.23638v2",
    "title": "Fully analogue in-memory neural computing via quantum tunneling effect",
    "authors": [
      "Songyuan Li",
      "Teng Wang",
      "Jinrong Tang",
      "Ruiqi Liu",
      "Haoyu Li",
      "Yuyao Lu",
      "Feng Xu",
      "Bin Gao",
      "Can Xie",
      "Xiangwei Zhu"
    ],
    "abstract": "Fully analogue neural computation requires hardware that can implement both linear and nonlinear transformations without digital assistance. While analogue in-memory computing efficiently realizes matrix-vector multiplication, the absence of learnable analogue nonlinearities remains a central bottleneck. Here we introduce KANalogue, a fully analogue realization of Kolmogorov-Arnold Networks (KANs) that instantiates univariate basis functions directly using negative-differential-resistance (NDR) devices. By mapping the intrinsic current-voltage characteristics of NDR devices to learnable coordinate-wise nonlinear functions, KANalogue embeds function approximation into device physics while preserving a fully analogue signal path. Using cold-metal tunnel diodes as a representative platform, we construct diverse nonlinear bases and combine them through crossbar-based analogue summation. Experiments on MNIST, FashionMNIST, and CIFAR-10 demonstrate that KANalogue achieves competitive accuracy with substantially fewer parameters and higher crossbar node efficiency than analogue MLPs, while approaching the performance of digital KANs under strict hardware constraints. The framework is not limited to a specific device technology and naturally generalizes to a broad class of NDR devices. These results establish a device-grounded route toward scalable, energy-efficient, fully analogue neural networks.",
    "categories": [
      "cs.ET",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.ET",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.23638v2",
    "published_date": "2025-10-24 15:38:22 UTC",
    "updated_date": "2025-12-25 14:17:48 UTC"
  },
  {
    "arxiv_id": "2510.21560v1",
    "title": "Learning Neural Control Barrier Functions from Expert Demonstrations using Inverse Constraint Learning",
    "authors": [
      "Yuxuan Yang",
      "Hussein Sibai"
    ],
    "abstract": "Safety is a fundamental requirement for autonomous systems operating in critical domains. Control barrier functions (CBFs) have been used to design safety filters that minimally alter nominal controls for such systems to maintain their safety. Learning neural CBFs has been proposed as a data-driven alternative for their computationally expensive optimization-based synthesis. However, it is often the case that the failure set of states that should be avoided is non-obvious or hard to specify formally, e.g., tailgating in autonomous driving, while a set of expert demonstrations that achieve the task and avoid the failure set is easier to generate. We use ICL to train a constraint function that classifies the states of the system under consideration to safe, i.e., belong to a controlled forward invariant set that is disjoint from the unspecified failure set, and unsafe ones, i.e., belong to the complement of that set. We then use that function to label a new set of simulated trajectories to train our neural CBF. We empirically evaluate our approach in four different environments, demonstrating that it outperforms existing baselines and achieves comparable performance to a neural CBF trained with the same data but annotated with ground-truth safety labels.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21560v1",
    "published_date": "2025-10-24 15:20:34 UTC",
    "updated_date": "2025-10-24 15:20:34 UTC"
  },
  {
    "arxiv_id": "2510.21557v1",
    "title": "Co-Sight: Enhancing LLM-Based Agents via Conflict-Aware Meta-Verification and Trustworthy Reasoning with Structured Facts",
    "authors": [
      "Hongwei Zhang",
      "Ji Lu",
      "Shiqing Jiang",
      "Chenxiang Zhu",
      "Li Xie",
      "Chen Zhong",
      "Haoran Chen",
      "Yurui Zhu",
      "Yongsheng Du",
      "Yanqin Gao",
      "Lingjun Huang",
      "Baoli Wang",
      "Fang Tan",
      "Peng Zou"
    ],
    "abstract": "Long-horizon reasoning in LLM-based agents often fails not from generative weakness but from insufficient verification of intermediate reasoning. Co-Sight addresses this challenge by turning reasoning into a falsifiable and auditable process through two complementary mechanisms: Conflict-Aware Meta-Verification (CAMV) and Trustworthy Reasoning with Structured Facts (TRSF). CAMV reformulates verification as conflict identification and targeted falsification, allocating computation only to disagreement hotspots among expert agents rather than to full reasoning chains. This bounds verification cost to the number of inconsistencies and improves efficiency and reliability. TRSF continuously organizes, validates, and synchronizes evidence across agents through a structured facts module. By maintaining verified, traceable, and auditable knowledge, it ensures that all reasoning is grounded in consistent, source-verified information and supports transparent verification throughout the reasoning process. Together, TRSF and CAMV form a closed verification loop, where TRSF supplies structured facts and CAMV selectively falsifies or reinforces them, yielding transparent and trustworthy reasoning. Empirically, Co-Sight achieves state-of-the-art accuracy on GAIA (84.4%) and Humanity's Last Exam (35.5%), and strong results on Chinese-SimpleQA (93.8%). Ablation studies confirm that the synergy between structured factual grounding and conflict-aware verification drives these improvements. Co-Sight thus offers a scalable paradigm for reliable long-horizon reasoning in LLM-based agents. Code is available at https://github.com/ZTE-AICloud/Co-Sight/tree/cosight2.0_benchmarks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21557v1",
    "published_date": "2025-10-24 15:14:14 UTC",
    "updated_date": "2025-10-24 15:14:14 UTC"
  },
  {
    "arxiv_id": "2510.21535v1",
    "title": "Human and AI Trust: Trust Attitude Measurement Instrument",
    "authors": [
      "Retno Larasati"
    ],
    "abstract": "With the current progress of Artificial Intelligence (AI) technology and its increasingly broader applications, trust is seen as a required criterion for AI usage, acceptance, and deployment. A robust measurement instrument is essential to correctly evaluate trust from a human-centered perspective. This paper describes the development and validation process of a trust measure instrument, which follows psychometric principles, and consists of a 16-items trust scale. The instrument was built explicitly for research in human-AI interaction to measure trust attitudes towards AI systems from layperson (non-expert) perspective. The use-case we used to develop the scale was in the context of AI medical support systems (specifically cancer/health prediction). The scale development (Measurement Item Development) and validation (Measurement Item Evaluation) involved six research stages: item development, item evaluation, survey administration, test of dimensionality, test of reliability, and test of validity. The results of the six-stages evaluation show that the proposed trust measurement instrument is empirically reliable and valid for systematically measuring and comparing non-experts' trust in AI Medical Support Systems.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21535v1",
    "published_date": "2025-10-24 15:01:06 UTC",
    "updated_date": "2025-10-24 15:01:06 UTC"
  },
  {
    "arxiv_id": "2510.21524v1",
    "title": "EU-Agent-Bench: Measuring Illegal Behavior of LLM Agents Under EU Law",
    "authors": [
      "Ilija Lichkovski",
      "Alexander Müller",
      "Mariam Ibrahim",
      "Tiwai Mhundwa"
    ],
    "abstract": "Large language models (LLMs) are increasingly deployed as agents in various contexts by providing tools at their disposal. However, LLM agents can exhibit unpredictable behaviors, including taking undesirable and/or unsafe actions. In order to measure the latent propensity of LLM agents for taking illegal actions under an EU legislative context, we introduce EU-Agent-Bench, a verifiable human-curated benchmark that evaluates an agent's alignment with EU legal norms in situations where benign user inputs could lead to unlawful actions. Our benchmark spans scenarios across several categories, including data protection, bias/discrimination, and scientific integrity, with each user request allowing for both compliant and non-compliant execution of the requested actions. Comparing the model's function calls against a rubric exhaustively supported by citations of the relevant legislature, we evaluate the legal compliance of frontier LLMs, and furthermore investigate the compliance effect of providing the relevant legislative excerpts in the agent's system prompt along with explicit instructions to comply. We release a public preview set for the research community, while holding out a private test set to prevent data contamination in evaluating upcoming models. We encourage future work extending agentic safety benchmarks to different legal jurisdictions and to multi-turn and multilingual interactions. We release our code on \\href{https://github.com/ilijalichkovski/eu-agent-bench}{this URL}.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at the Workshop on Regulatable ML at the 39th Conference on Neural Information Processing Systems (NeurIPS 2025)",
    "pdf_url": "https://arxiv.org/pdf/2510.21524v1",
    "published_date": "2025-10-24 14:48:10 UTC",
    "updated_date": "2025-10-24 14:48:10 UTC"
  },
  {
    "arxiv_id": "2510.21900v1",
    "title": "Deep Literature Survey Automation with an Iterative Workflow",
    "authors": [
      "Hongbo Zhang",
      "Han Cui",
      "Yidong Wang",
      "Yijian Tian",
      "Qi Guo",
      "Cunxiang Wang",
      "Jian Wu",
      "Chiyu Song",
      "Yue Zhang"
    ],
    "abstract": "Automatic literature survey generation has attracted increasing attention, yet most existing systems follow a one-shot paradigm, where a large set of papers is retrieved at once and a static outline is generated before drafting. This design often leads to noisy retrieval, fragmented structures, and context overload, ultimately limiting survey quality. Inspired by the iterative reading process of human researchers, we propose \\ours, a framework based on recurrent outline generation, in which a planning agent incrementally retrieves, reads, and updates the outline to ensure both exploration and coherence. To provide faithful paper-level grounding, we design paper cards that distill each paper into its contributions, methods, and findings, and introduce a review-and-refine loop with visualization enhancement to improve textual flow and integrate multimodal elements such as figures and tables. Experiments on both established and emerging topics show that \\ours\\ substantially outperforms state-of-the-art baselines in content coverage, structural coherence, and citation quality, while producing more accessible and better-organized surveys. To provide a more reliable assessment of such improvements, we further introduce Survey-Arena, a pairwise benchmark that complements absolute scoring and more clearly positions machine-generated surveys relative to human-written ones. The code is available at https://github.com/HancCui/IterSurvey\\_Autosurveyv2.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint version",
    "pdf_url": "https://arxiv.org/pdf/2510.21900v1",
    "published_date": "2025-10-24 14:41:26 UTC",
    "updated_date": "2025-10-24 14:41:26 UTC"
  },
  {
    "arxiv_id": "2510.23637v2",
    "title": "Combining Textual and Structural Information for Premise Selection in Lean",
    "authors": [
      "Job Petrovčič",
      "David Eliecer Narvaez Denis",
      "Ljupčo Todorovski"
    ],
    "abstract": "Premise selection is a key bottleneck for scaling theorem proving in large formal libraries. Yet existing language-based methods often treat premises in isolation, ignoring the web of dependencies that connects them. We present a graph-augmented approach that combines dense text embeddings of Lean formalizations with graph neural networks over a heterogeneous dependency graph capturing both state-premise and premise-premise relations. On the LeanDojo Benchmark, our method outperforms the ReProver language-based baseline by over 25\\% across standard retrieval metrics. These results suggest that relational information is beneficial for premise selection.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.LO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.23637v2",
    "published_date": "2025-10-24 14:24:13 UTC",
    "updated_date": "2025-11-28 19:08:23 UTC"
  },
  {
    "arxiv_id": "2510.24770v2",
    "title": "DMVFC: Deep Learning Based Functionally Consistent Tractography Fiber Clustering Using Multimodal Diffusion MRI and Functional MRI",
    "authors": [
      "Bocheng Guo",
      "Jin Wang",
      "Yijie Li",
      "Junyi Wang",
      "Mingyu Gao",
      "Puming Feng",
      "Yuqian Chen",
      "Jarrett Rushmore",
      "Nikos Makris",
      "Yogesh Rathi",
      "Lauren J O'Donnell",
      "Fan Zhang"
    ],
    "abstract": "Tractography fiber clustering using diffusion MRI (dMRI) is a crucial method for white matter (WM) parcellation to enable analysis of brains structural connectivity in health and disease. Current fiber clustering strategies primarily use the fiber geometric characteristics (i.e., the spatial trajectories) to group similar fibers into clusters, while neglecting the functional and microstructural information of the fiber tracts. There is increasing evidence that neural activity in the WM can be measured using functional MRI (fMRI), providing potentially valuable multimodal information for fiber clustering to enhance its functional coherence. Furthermore, microstructural features such as fractional anisotropy (FA) can be computed from dMRI as additional information to ensure the anatomical coherence of the clusters. In this paper, we develop a novel deep learning fiber clustering framework, namely Deep Multi-view Fiber Clustering (DMVFC), which uses joint multi-modal dMRI and fMRI data to enable functionally consistent WM parcellation. DMVFC can effectively integrate the geometric and microstructural characteristics of the WM fibers with the fMRI BOLD signals along the fiber tracts. DMVFC includes two major components: (1) a multi-view pretraining module to compute embedding features from each source of information separately, including fiber geometry, microstructure measures, and functional signals, and (2) a collaborative fine-tuning module to simultaneously refine the differences of embeddings. In the experiments, we compare DMVFC with two state-of-the-art fiber clustering methods and demonstrate superior performance in achieving functionally meaningful and consistent WM parcellation results.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "14 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.24770v2",
    "published_date": "2025-10-24 14:19:23 UTC",
    "updated_date": "2025-11-03 02:27:53 UTC"
  },
  {
    "arxiv_id": "2510.21469v1",
    "title": "Enhancing Social Robots through Resilient AI",
    "authors": [
      "Domenico Palmisano",
      "Giuseppe Palestra",
      "Berardina Nadja De Carolis"
    ],
    "abstract": "As artificial intelligence continues to advance and becomes more integrated into sensitive areas like healthcare, education, and everyday life, it's crucial for these systems to be both resilient and robust. This paper shows how resilience is a fundamental characteristic of social robots, which, through it, ensure trust in the robot itself-an essential element especially when operating in contexts with elderly people, who often have low trust in these systems. Resilience is therefore the ability to operate under adverse or stressful conditions, even when degraded or weakened, while maintaining essential operational capabilities.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages, Workshop on Adaptive Social Interaction based on user's Mental mOdels and behaVior in HRI, The 17th International Conference on Social Robotics, 10-12 September 2025, Naples (IT)",
    "pdf_url": "https://arxiv.org/pdf/2510.21469v1",
    "published_date": "2025-10-24 13:55:45 UTC",
    "updated_date": "2025-10-24 13:55:45 UTC"
  },
  {
    "arxiv_id": "2510.21453v1",
    "title": "Multi-Task Vehicle Routing Solver via Mixture of Specialized Experts under State-Decomposable MDP",
    "authors": [
      "Yuxin Pan",
      "Zhiguang Cao",
      "Chengyang Gu",
      "Liu Liu",
      "Peilin Zhao",
      "Yize Chen",
      "Fangzhen Lin"
    ],
    "abstract": "Existing neural methods for multi-task vehicle routing problems (VRPs) typically learn unified solvers to handle multiple constraints simultaneously. However, they often underutilize the compositional structure of VRP variants, each derivable from a common set of basis VRP variants. This critical oversight causes unified solvers to miss out the potential benefits of basis solvers, each specialized for a basis VRP variant. To overcome this limitation, we propose a framework that enables unified solvers to perceive the shared-component nature across VRP variants by proactively reusing basis solvers, while mitigating the exponential growth of trained neural solvers. Specifically, we introduce a State-Decomposable MDP (SDMDP) that reformulates VRPs by expressing the state space as the Cartesian product of basis state spaces associated with basis VRP variants. More crucially, this formulation inherently yields the optimal basis policy for each basis VRP variant. Furthermore, a Latent Space-based SDMDP extension is developed by incorporating both the optimal basis policies and a learnable mixture function to enable the policy reuse in the latent space. Under mild assumptions, this extension provably recovers the optimal unified policy of SDMDP through the mixture function that computes the state embedding as a mapping from the basis state embeddings generated by optimal basis policies. For practical implementation, we introduce the Mixture-of-Specialized-Experts Solver (MoSES), which realizes basis policies through specialized Low-Rank Adaptation (LoRA) experts, and implements the mixture function via an adaptive gating mechanism. Extensive experiments conducted across VRP variants showcase the superiority of MoSES over prior methods.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.21453v1",
    "published_date": "2025-10-24 13:31:31 UTC",
    "updated_date": "2025-10-24 13:31:31 UTC"
  },
  {
    "arxiv_id": "2510.21447v1",
    "title": "PhysWorld: From Real Videos to World Models of Deformable Objects via Physics-Aware Demonstration Synthesis",
    "authors": [
      "Yu Yang",
      "Zhilu Zhang",
      "Xiang Zhang",
      "Yihan Zeng",
      "Hui Li",
      "Wangmeng Zuo"
    ],
    "abstract": "Interactive world models that simulate object dynamics are crucial for robotics, VR, and AR. However, it remains a significant challenge to learn physics-consistent dynamics models from limited real-world video data, especially for deformable objects with spatially-varying physical properties. To overcome the challenge of data scarcity, we propose PhysWorld, a novel framework that utilizes a simulator to synthesize physically plausible and diverse demonstrations to learn efficient world models. Specifically, we first construct a physics-consistent digital twin within MPM simulator via constitutive model selection and global-to-local optimization of physical properties. Subsequently, we apply part-aware perturbations to the physical properties and generate various motion patterns for the digital twin, synthesizing extensive and diverse demonstrations. Finally, using these demonstrations, we train a lightweight GNN-based world model that is embedded with physical properties. The real video can be used to further refine the physical properties. PhysWorld achieves accurate and fast future predictions for various deformable objects, and also generalizes well to novel interactions. Experiments show that PhysWorld has competitive performance while enabling inference speeds 47 times faster than the recent state-of-the-art method, i.e., PhysTwin.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "17 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.21447v1",
    "published_date": "2025-10-24 13:25:39 UTC",
    "updated_date": "2025-10-24 13:25:39 UTC"
  },
  {
    "arxiv_id": "2510.21445v1",
    "title": "REMONI: An Autonomous System Integrating Wearables and Multimodal Large Language Models for Enhanced Remote Health Monitoring",
    "authors": [
      "Thanh Cong Ho",
      "Farah Kharrat",
      "Abderrazek Abid",
      "Fakhri Karray"
    ],
    "abstract": "With the widespread adoption of wearable devices in our daily lives, the demand and appeal for remote patient monitoring have significantly increased. Most research in this field has concentrated on collecting sensor data, visualizing it, and analyzing it to detect anomalies in specific diseases such as diabetes, heart disease and depression. However, this domain has a notable gap in the aspect of human-machine interaction. This paper proposes REMONI, an autonomous REmote health MONItoring system that integrates multimodal large language models (MLLMs), the Internet of Things (IoT), and wearable devices. The system automatically and continuously collects vital signs, accelerometer data from a special wearable (such as a smartwatch), and visual data in patient video clips collected from cameras. This data is processed by an anomaly detection module, which includes a fall detection model and algorithms to identify and alert caregivers of the patient's emergency conditions. A distinctive feature of our proposed system is the natural language processing component, developed with MLLMs capable of detecting and recognizing a patient's activity and emotion while responding to healthcare worker's inquiries. Additionally, prompt engineering is employed to integrate all patient information seamlessly. As a result, doctors and nurses can access real-time vital signs and the patient's current state and mood by interacting with an intelligent agent through a user-friendly web application. Our experiments demonstrate that our system is implementable and scalable for real-life scenarios, potentially reducing the workload of medical professionals and healthcare costs. A full-fledged prototype illustrating the functionalities of the system has been developed and being tested to demonstrate the robustness of its various capabilities.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21445v1",
    "published_date": "2025-10-24 13:23:38 UTC",
    "updated_date": "2025-10-24 13:23:38 UTC"
  },
  {
    "arxiv_id": "2510.21443v1",
    "title": "Does Model Size Matter? A Comparison of Small and Large Language Models for Requirements Classification",
    "authors": [
      "Mohammad Amin Zadenoori",
      "Vincenzo De Martino",
      "Jacek Dabrowski",
      "Xavier Franch",
      "Alessio Ferrari"
    ],
    "abstract": "[Context and motivation] Large language models (LLMs) show notable results in natural language processing (NLP) tasks for requirements engineering (RE). However, their use is compromised by high computational cost, data sharing risks, and dependence on external services. In contrast, small language models (SLMs) offer a lightweight, locally deployable alternative. [Question/problem] It remains unclear how well SLMs perform compared to LLMs in RE tasks in terms of accuracy. [Results] Our preliminary study compares eight models, including three LLMs and five SLMs, on requirements classification tasks using the PROMISE, PROMISE Reclass, and SecReq datasets. Our results show that although LLMs achieve an average F1 score of 2% higher than SLMs, this difference is not statistically significant. SLMs almost reach LLMs performance across all datasets and even outperform them in recall on the PROMISE Reclass dataset, despite being up to 300 times smaller. We also found that dataset characteristics play a more significant role in performance than model size. [Contribution] Our study contributes with evidence that SLMs are a valid alternative to LLMs for requirements classification, offering advantages in privacy, cost, and local deployability.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21443v1",
    "published_date": "2025-10-24 13:20:30 UTC",
    "updated_date": "2025-10-24 13:20:30 UTC"
  },
  {
    "arxiv_id": "2510.21436v1",
    "title": "AutoOpt: A Dataset and a Unified Framework for Automating Optimization Problem Solving",
    "authors": [
      "Ankur Sinha",
      "Shobhit Arora",
      "Dhaval Pujara"
    ],
    "abstract": "This study presents AutoOpt-11k, a unique image dataset of over 11,000 handwritten and printed mathematical optimization models corresponding to single-objective, multi-objective, multi-level, and stochastic optimization problems exhibiting various types of complexities such as non-linearity, non-convexity, non-differentiability, discontinuity, and high-dimensionality. The labels consist of the LaTeX representation for all the images and modeling language representation for a subset of images. The dataset is created by 25 experts following ethical data creation guidelines and verified in two-phases to avoid errors. Further, we develop AutoOpt framework, a machine learning based automated approach for solving optimization problems, where the user just needs to provide an image of the formulation and AutoOpt solves it efficiently without any further human intervention. AutoOpt framework consists of three Modules: (i) M1 (Image_to_Text)- a deep learning model performs the Mathematical Expression Recognition (MER) task to generate the LaTeX code corresponding to the optimization formulation in image; (ii) M2 (Text_to_Text)- a small-scale fine-tuned LLM generates the PYOMO script (optimization modeling language) from LaTeX code; (iii) M3 (Optimization)- a Bilevel Optimization based Decomposition (BOBD) method solves the optimization formulation described in the PYOMO script. We use AutoOpt-11k dataset for training and testing of deep learning models employed in AutoOpt. The deep learning model for MER task (M1) outperforms ChatGPT, Gemini and Nougat on BLEU score metric. BOBD method (M3), which is a hybrid approach, yields better results on complex test problems compared to common approaches, like interior-point algorithm and genetic algorithm.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "NeurIPS 2025, 28 pages, 11 figures, 11 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.21436v1",
    "published_date": "2025-10-24 13:14:53 UTC",
    "updated_date": "2025-10-24 13:14:53 UTC"
  },
  {
    "arxiv_id": "2511.05510v1",
    "title": "TEMPO: Temporal Multi-scale Autoregressive Generation of Protein Conformational Ensembles",
    "authors": [
      "Yaoyao Xu",
      "Di Wang",
      "Zihan Zhou",
      "Tianshu Yu",
      "Mingchen Chen"
    ],
    "abstract": "Understanding the dynamic behavior of proteins is critical to elucidating their functional mechanisms, yet generating realistic, temporally coherent trajectories of protein ensembles remains a significant challenge. In this work, we introduce a novel hierarchical autoregressive framework for modeling protein dynamics that leverages the intrinsic multi-scale organization of molecular motions. Unlike existing methods that focus on generating static conformational ensembles or treat dynamic sampling as an independent process, our approach characterizes protein dynamics as a Markovian process. The framework employs a two-scale architecture: a low-resolution model captures slow, collective motions driving major conformational transitions, while a high-resolution model generates detailed local fluctuations conditioned on these large-scale movements. This hierarchical design ensures that the causal dependencies inherent in protein dynamics are preserved, enabling the generation of temporally coherent and physically realistic trajectories. By bridging high-level biophysical principles with state-of-the-art generative modeling, our approach provides an efficient framework for simulating protein dynamics that balances computational efficiency with physical accuracy.",
    "categories": [
      "q-bio.BM",
      "cs.AI"
    ],
    "primary_category": "q-bio.BM",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.05510v1",
    "published_date": "2025-10-24 13:11:47 UTC",
    "updated_date": "2025-10-24 13:11:47 UTC"
  },
  {
    "arxiv_id": "2510.21425v1",
    "title": "Advancing Symbolic Integration in Large Language Models: Beyond Conventional Neurosymbolic AI",
    "authors": [
      "Maneeha Rani",
      "Bhupesh Kumar Mishra",
      "Dhavalkumar Thakker"
    ],
    "abstract": "LLMs have demonstrated highly effective learning, human-like response generation,and decision-making capabilities in high-risk sectors. However, these models remain black boxes because they struggle to ensure transparency in responses. The literature has explored numerous approaches to address transparency challenges in LLMs, including Neurosymbolic AI (NeSy AI). NeSy AI approaches were primarily developed for conventional neural networks and are not well-suited to the unique features of LLMs. Consequently, there is a limited systematic understanding of how symbolic AI can be effectively integrated into LLMs. This paper aims to address this gap by first reviewing established NeSy AI methods and then proposing a novel taxonomy of symbolic integration in LLMs, along with a roadmap to merge symbolic techniques with LLMs. The roadmap introduces a new categorisation framework across four dimensions by organising existing literature within these categories. These include symbolic integration across various stages of LLM, coupling mechanisms, architectural paradigms, as well as algorithmic and application-level perspectives. The paper thoroughly identifies current benchmarks, cutting-edge advancements, and critical gaps within the field to propose a roadmap for future research. By highlighting the latest developments and notable gaps in the literature, it offers practical insights for implementing frameworks for symbolic integration into LLMs to enhance transparency.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21425v1",
    "published_date": "2025-10-24 13:05:50 UTC",
    "updated_date": "2025-10-24 13:05:50 UTC"
  },
  {
    "arxiv_id": "2510.21424v1",
    "title": "Vision Language Models for Dynamic Human Activity Recognition in Healthcare Settings",
    "authors": [
      "Abderrazek Abid",
      "Thanh-Cong Ho",
      "Fakhri Karray"
    ],
    "abstract": "As generative AI continues to evolve, Vision Language Models (VLMs) have emerged as promising tools in various healthcare applications. One area that remains relatively underexplored is their use in human activity recognition (HAR) for remote health monitoring. VLMs offer notable strengths, including greater flexibility and the ability to overcome some of the constraints of traditional deep learning models. However, a key challenge in applying VLMs to HAR lies in the difficulty of evaluating their dynamic and often non-deterministic outputs. To address this gap, we introduce a descriptive caption data set and propose comprehensive evaluation methods to evaluate VLMs in HAR. Through comparative experiments with state-of-the-art deep learning models, our findings demonstrate that VLMs achieve comparable performance and, in some cases, even surpass conventional approaches in terms of accuracy. This work contributes a strong benchmark and opens new possibilities for the integration of VLMs into intelligent healthcare systems.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21424v1",
    "published_date": "2025-10-24 13:04:13 UTC",
    "updated_date": "2025-10-24 13:04:13 UTC"
  },
  {
    "arxiv_id": "2510.21418v1",
    "title": "DreamerV3-XP: Optimizing exploration through uncertainty estimation",
    "authors": [
      "Lukas Bierling",
      "Davide Pasero",
      "Jan-Henrik Bertrand",
      "Kiki Van Gerwen"
    ],
    "abstract": "We introduce DreamerV3-XP, an extension of DreamerV3 that improves exploration and learning efficiency. This includes (i) a prioritized replay buffer, scoring trajectories by return, reconstruction loss, and value error and (ii) an intrinsic reward based on disagreement over predicted environment rewards from an ensemble of world models. DreamerV3-XP is evaluated on a subset of Atari100k and DeepMind Control Visual Benchmark tasks, confirming the original DreamerV3 results and showing that our extensions lead to faster learning and lower dynamics model loss, particularly in sparse-reward settings.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21418v1",
    "published_date": "2025-10-24 12:58:27 UTC",
    "updated_date": "2025-10-24 12:58:27 UTC"
  },
  {
    "arxiv_id": "2510.21408v1",
    "title": "Large Language Models as Model Organisms for Human Associative Learning",
    "authors": [
      "Camila Kolling",
      "Vy Ai Vo",
      "Mariya Toneva"
    ],
    "abstract": "Associative learning--forming links between co-occurring items--is fundamental to human cognition, reshaping internal representations in complex ways. Testing hypotheses on how representational changes occur in biological systems is challenging, but large language models (LLMs) offer a scalable alternative. Building on LLMs' in-context learning, we adapt a cognitive neuroscience associative learning paradigm and investigate how representations evolve across six models. Our initial findings reveal a non-monotonic pattern consistent with the Non-Monotonic Plasticity Hypothesis, with moderately similar items differentiating after learning. Leveraging the controllability of LLMs, we further show that this differentiation is modulated by the overlap of associated items with the broader vocabulary--a factor we term vocabulary interference, capturing how new associations compete with prior knowledge. We find that higher vocabulary interference amplifies differentiation, suggesting that representational change is influenced by both item similarity and global competition. Our findings position LLMs not only as powerful tools for studying representational dynamics in human-like learning systems, but also as accessible and general computational models for generating new hypotheses about the principles underlying memory reorganization in the brain.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21408v1",
    "published_date": "2025-10-24 12:52:11 UTC",
    "updated_date": "2025-10-24 12:52:11 UTC"
  },
  {
    "arxiv_id": "2510.21407v1",
    "title": "REvolution: An Evolutionary Framework for RTL Generation driven by Large Language Models",
    "authors": [
      "Kyungjun Min",
      "Kyumin Cho",
      "Junhwan Jang",
      "Seokhyeong Kang"
    ],
    "abstract": "Large Language Models (LLMs) are used for Register-Transfer Level (RTL) code generation, but they face two main challenges: functional correctness and Power, Performance, and Area (PPA) optimization. Iterative, feedback-based methods partially address these, but they are limited to local search, hindering the discovery of a global optimum. This paper introduces REvolution, a framework that combines Evolutionary Computation (EC) with LLMs for automatic RTL generation and optimization. REvolution evolves a population of candidates in parallel, each defined by a design strategy, RTL implementation, and evaluation feedback. The framework includes a dual-population algorithm that divides candidates into Fail and Success groups for bug fixing and PPA optimization, respectively. An adaptive mechanism further improves search efficiency by dynamically adjusting the selection probability of each prompt strategy according to its success rate. Experiments on the VerilogEval and RTLLM benchmarks show that REvolution increased the initial pass rate of various LLMs by up to 24.0 percentage points. The DeepSeek-V3 model achieved a final pass rate of 95.5\\%, comparable to state-of-the-art results, without the need for separate training or domain-specific tools. Additionally, the generated RTL designs showed significant PPA improvements over reference designs. This work introduces a new RTL design approach by combining LLMs' generative capabilities with EC's broad search power, overcoming the local-search limitations of previous methods.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.NE",
    "comment": "Accepted for publication at the 2026 Asia and South Pacific Design Automation Conference (ASP-DAC)",
    "pdf_url": "https://arxiv.org/pdf/2510.21407v1",
    "published_date": "2025-10-24 12:50:35 UTC",
    "updated_date": "2025-10-24 12:50:35 UTC"
  },
  {
    "arxiv_id": "2510.21398v1",
    "title": "Boosting Accuracy and Efficiency of Budget Forcing in LLMs via Reinforcement Learning for Mathematical Reasoning",
    "authors": [
      "Ravindra Aribowo Tarunokusumo",
      "Rafael Fernandes Cunha"
    ],
    "abstract": "Test-time scaling methods have seen a rapid increase in popularity for its computational efficiency and parameter-independent training to improve reasoning performance on Large Language Models. One such method is called budget forcing, a decoding intervention strategy which allocates extra compute budget for thinking and elicits the inherent self-correcting behavior of the model. However, this relies on supervised fine-tuning (SFT) on long-context reasoning traces which causes performance degradation on smaller models due to verbose responses. For this reason, we offer a framework integrating reinforcement learning (RL) to improve token efficiency and boost the performance of a 1.5B model for mathematical reasoning. We demonstrate this using only 1.5K training samples and found that our SFT+RL model performed better on the GSM8K dataset with varying compute budgets. Our main findings showed an overall higher accuracy while significantly reducing its token usage by over 40% compared to the SFT model, revealing how RL can recover the losses due to long-context training and altogether improving performance in mathematical reasoning.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Submitted to the European Conference on Artificial Intelligence (ECAI)",
    "pdf_url": "https://arxiv.org/pdf/2510.21398v1",
    "published_date": "2025-10-24 12:39:15 UTC",
    "updated_date": "2025-10-24 12:39:15 UTC"
  },
  {
    "arxiv_id": "2510.21389v1",
    "title": "Assessing the Real-World Utility of Explainable AI for Arousal Diagnostics: An Application-Grounded User Study",
    "authors": [
      "Stefan Kraft",
      "Andreas Theissler",
      "Vera Wienhausen-Wilke",
      "Gjergji Kasneci",
      "Hendrik Lensch"
    ],
    "abstract": "Artificial intelligence (AI) systems increasingly match or surpass human experts in biomedical signal interpretation. However, their effective integration into clinical practice requires more than high predictive accuracy. Clinicians must discern \\textit{when} and \\textit{why} to trust algorithmic recommendations. This work presents an application-grounded user study with eight professional sleep medicine practitioners, who score nocturnal arousal events in polysomnographic data under three conditions: (i) manual scoring, (ii) black-box (BB) AI assistance, and (iii) transparent white-box (WB) AI assistance. Assistance is provided either from the \\textit{start} of scoring or as a post-hoc quality-control (\\textit{QC}) review. We systematically evaluate how the type and timing of assistance influence event-level and clinically most relevant count-based performance, time requirements, and user experience. When evaluated against the clinical standard used to train the AI, both AI and human-AI teams significantly outperform unaided experts, with collaboration also reducing inter-rater variability. Notably, transparent AI assistance applied as a targeted QC step yields median event-level performance improvements of approximately 30\\% over black-box assistance, and QC timing further enhances count-based outcomes. While WB and QC approaches increase the time required for scoring, start-time assistance is faster and preferred by most participants. Participants overwhelmingly favor transparency, with seven out of eight expressing willingness to adopt the system with minor or no modifications. In summary, strategically timed transparent AI assistance effectively balances accuracy and clinical efficiency, providing a promising pathway toward trustworthy AI integration and user acceptance in clinical workflows.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21389v1",
    "published_date": "2025-10-24 12:23:02 UTC",
    "updated_date": "2025-10-24 12:23:02 UTC"
  },
  {
    "arxiv_id": "2510.23636v2",
    "title": "Flight Delay Prediction via Cross-Modality Adaptation of Large Language Models and Aircraft Trajectory Representation",
    "authors": [
      "Thaweerath Phisannupawong",
      "Joshua Julian Damanik",
      "Han-Lim Choi"
    ],
    "abstract": "Flight delay prediction has become a key focus in air traffic management, as delays highlight inefficiencies that impact overall network performance. This paper presents a lightweight large language model-based multimodal flight delay prediction, formulated from the perspective of air traffic controllers monitoring aircraft delay after entering the terminal area. The approach integrates trajectory representations with textual aeronautical information, including flight information, weather reports, and aerodrome notices, by adapting trajectory data into the language modality to capture airspace conditions. The experiments show that the model consistently achieves sub-minute prediction error by effectively leveraging contextual information related to the sources of delay, fulfilling the operational standard for minute-level precision. The framework demonstrates that linguistic understanding, when combined with cross-modality adaptation of trajectory data, enhances delay prediction. Moreover, the approach shows practicality and potential scalability for real-world operations, supporting real-time updates that refine predictions upon receiving new operational information.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint submitted to Aerospace Science and Technology (Elsevier) for possible publication",
    "pdf_url": "https://arxiv.org/pdf/2510.23636v2",
    "published_date": "2025-10-24 12:21:27 UTC",
    "updated_date": "2025-11-03 07:12:24 UTC"
  },
  {
    "arxiv_id": "2510.21388v1",
    "title": "Compressing Quaternion Convolutional Neural Networks for Audio Classification",
    "authors": [
      "Arshdeep Singh",
      "Vinayak Abrol",
      "Mark D. Plumbley"
    ],
    "abstract": "Conventional Convolutional Neural Networks (CNNs) in the real domain have been widely used for audio classification. However, their convolution operations process multi-channel inputs independently, limiting the ability to capture correlations among channels. This can lead to suboptimal feature learning, particularly for complex audio patterns such as multi-channel spectrogram representations. Quaternion Convolutional Neural Networks (QCNNs) address this limitation by employing quaternion algebra to jointly capture inter-channel dependencies, enabling more compact models with fewer learnable parameters while better exploiting the multi-dimensional nature of audio signals. However, QCNNs exhibit higher computational complexity due to the overhead of quaternion operations, resulting in increased inference latency and reduced efficiency compared to conventional CNNs, posing challenges for deployment on resource-constrained platforms. To address this challenge, this study explores knowledge distillation (KD) and pruning, to reduce the computational complexity of QCNNs while maintaining performance. Our experiments on audio classification reveal that pruning QCNNs achieves similar or superior performance compared to KD while requiring less computational effort. Compared to conventional CNNs and Transformer-based architectures, pruned QCNNs achieve competitive performance with a reduced learnable parameter count and computational complexity. On the AudioSet dataset, pruned QCNNs reduce computational cost by 50\\% and parameter count by 80\\%, while maintaining performance comparable to the conventional CNNs. Furthermore, pruned QCNNs generalize well across multiple audio classification benchmarks, including GTZAN for music genre recognition, ESC-50 for environmental sound classification and RAVDESS for speech emotion recognition.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD",
      "eess.SP"
    ],
    "primary_category": "eess.AS",
    "comment": "Under review in IEEE TASLPRO",
    "pdf_url": "https://arxiv.org/pdf/2510.21388v1",
    "published_date": "2025-10-24 12:19:19 UTC",
    "updated_date": "2025-10-24 12:19:19 UTC"
  },
  {
    "arxiv_id": "2510.21370v1",
    "title": "HIKMA: Human-Inspired Knowledge by Machine Agents through a Multi-Agent Framework for Semi-Autonomous Scientific Conferences",
    "authors": [
      "Zain Ul Abideen Tariq",
      "Mahmood Al-Zubaidi",
      "Uzair Shah",
      "Marco Agus",
      "Mowafa Househ"
    ],
    "abstract": "HIKMA Semi-Autonomous Conference is the first experiment in reimagining scholarly communication through an end-to-end integration of artificial intelligence into the academic publishing and presentation pipeline. This paper presents the design, implementation, and evaluation of the HIKMA framework, which includes AI dataset curation, AI-based manuscript generation, AI-assisted peer review, AI-driven revision, AI conference presentation, and AI archival dissemination. By combining language models, structured research workflows, and domain safeguards, HIKMA shows how AI can support - not replace traditional scholarly practices while maintaining intellectual property protection, transparency, and integrity. The conference functions as a testbed and proof of concept, providing insights into the opportunities and challenges of AI-enabled scholarship. It also examines questions about AI authorship, accountability, and the role of human-AI collaboration in research.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CL",
      "cs.DL"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21370v1",
    "published_date": "2025-10-24 11:52:24 UTC",
    "updated_date": "2025-10-24 11:52:24 UTC"
  },
  {
    "arxiv_id": "2510.21362v1",
    "title": "Patient-specific AI for generation of 3D dosimetry imaging from two 2D-planar measurements",
    "authors": [
      "Alejandro Lopez-Montes",
      "Robert Seifert",
      "Astrid Delker",
      "Guido Boening",
      "Jiahui Wang",
      "Christoph Clement",
      "Ali Afshar-Oromieh",
      "Axel Rominger",
      "Kuangyu Shi"
    ],
    "abstract": "In this work we explored the use of patient specific reinforced learning to generate 3D activity maps from two 2D planar images (anterior and posterior). The solution of this problem remains unachievable using conventional methodologies and is of particular interest for dosimetry in nuclear medicine where approaches for post-therapy distribution of radiopharmaceuticals such as 177Lu-PSMA are typically done via either expensive and long 3D SPECT acquisitions or fast, yet only 2D, planar scintigraphy. Being able to generate 3D activity maps from planar scintigraphy opens the gate for new dosimetry applications removing the need for SPECT and facilitating multi-time point dosimetry studies. Our solution comprises the generation of a patient specific dataset with possible 3D uptake maps of the radiopharmaceuticals withing the anatomy of the individual followed by an AI approach (we explored both the use of 3DUnet and diffusion models) able to generate 3D activity maps from 2D planar images. We have validated our method both in simulation and real planar acquisitions. We observed enhanced results using patient specific reinforcement learning (~20% reduction on MAE and ~5% increase in SSIM) and better organ delineation and patient anatomy especially when combining diffusion models with patient specific training yielding a SSIM=0.89 compared to the ground truth for simulations and 0.73 when compared to a SPECT acquisition performed half an hour after the planar. We believe that our methodology can set a change of paradigm for nuclear medicine dosimetry allowing for 3D quantification using only planar scintigraphy without the need of expensive and time-consuming SPECT leveraging the pre-therapy information of the patients.",
    "categories": [
      "physics.med-ph",
      "cs.AI"
    ],
    "primary_category": "physics.med-ph",
    "comment": "Accepted at IEEE NSS/MIC 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.21362v1",
    "published_date": "2025-10-24 11:46:51 UTC",
    "updated_date": "2025-10-24 11:46:51 UTC"
  },
  {
    "arxiv_id": "2510.21356v1",
    "title": "Gaze-VLM:Bridging Gaze and VLMs through Attention Regularization for Egocentric Understanding",
    "authors": [
      "Anupam Pani",
      "Yanchao Yang"
    ],
    "abstract": "Eye gaze offers valuable cues about attention, short-term intent, and future actions, making it a powerful signal for modeling egocentric behavior. In this work, we propose a gaze-regularized framework that enhances VLMs for two key egocentric understanding tasks: fine-grained future event prediction and current activity understanding. Unlike prior approaches that rely solely on visual inputs or use gaze as an auxiliary input signal , our method uses gaze only during training. We introduce a gaze-regularized attention mechanism that aligns model focus with human visual gaze. This design is flexible and modular, allowing it to generalize across multiple VLM architectures that utilize attention. Experimental results show that our approach improves semantic prediction scores by up to 11 for future event prediction and around 7 for current activity understanding, compared to the corresponding baseline models trained without gaze regularization. These results highlight the value of gaze-guided training in improving the accuracy and robustness of egocentric VLMs. Overall, this work establishes a foundation for using human gaze to enhance the predictive capabilities of VLMs in real-world scenarios like assistive robots and human-machine collaboration. Code and additional information is available at: https://github.com/anupampani/Gaze-VLM",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21356v1",
    "published_date": "2025-10-24 11:33:03 UTC",
    "updated_date": "2025-10-24 11:33:03 UTC"
  },
  {
    "arxiv_id": "2510.21346v1",
    "title": "CT-CLIP: A Multi-modal Fusion Framework for Robust Apple Leaf Disease Recognition in Complex Environments",
    "authors": [
      "Lemin Liu",
      "Fangchao Hu",
      "Honghua Jiang",
      "Yaru Chen",
      "Limin Liu",
      "Yongliang Qiao"
    ],
    "abstract": "In complex orchard environments, the phenotypic heterogeneity of different apple leaf diseases, characterized by significant variation among lesions, poses a challenge to traditional multi-scale feature fusion methods. These methods only integrate multi-layer features extracted by convolutional neural networks (CNNs) and fail to adequately account for the relationships between local and global features. Therefore, this study proposes a multi-branch recognition framework named CNN-Transformer-CLIP (CT-CLIP). The framework synergistically employs a CNN to extract local lesion detail features and a Vision Transformer to capture global structural relationships. An Adaptive Feature Fusion Module (AFFM) then dynamically fuses these features, achieving optimal coupling of local and global information and effectively addressing the diversity in lesion morphology and distribution. Additionally, to mitigate interference from complex backgrounds and significantly enhance recognition accuracy under few-shot conditions, this study proposes a multimodal image-text learning approach. By leveraging pre-trained CLIP weights, it achieves deep alignment between visual features and disease semantic descriptions. Experimental results show that CT-CLIP achieves accuracies of 97.38% and 96.12% on a publicly available apple disease and a self-built dataset, outperforming several baseline methods. The proposed CT-CLIP demonstrates strong capabilities in recognizing agricultural diseases, significantly enhances identification accuracy under complex environmental conditions, provides an innovative and practical solution for automated disease recognition in agricultural applications.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21346v1",
    "published_date": "2025-10-24 11:23:47 UTC",
    "updated_date": "2025-10-24 11:23:47 UTC"
  },
  {
    "arxiv_id": "2510.21345v1",
    "title": "$α$-LoRA: Effective Fine-Tuning via Base Model Rescaling",
    "authors": [
      "Aymane El Firdoussi",
      "El Mahdi Chayti",
      "Mohamed El Amine Seddik",
      "Martin Jaggi"
    ],
    "abstract": "Fine-tuning has proven to be highly effective in adapting pre-trained models to perform better on new desired tasks with minimal data samples. Among the most widely used approaches are reparameterization methods, which update a target module by augmenting its frozen weight matrix with an additional trainable weight matrix. The most prominent example is Low Rank Adaption (LoRA), which gained significant attention in recent years. In this paper, we introduce a new class of reparameterization methods for transfer learning, designed to enhance the generalization ability of fine-tuned models. We establish the effectiveness of our approach in a high-dimensional binary classification setting using tools from Random Matrix Theory, and further validate our theoretical findings through more realistic experiments, such as fine-tuning LLMs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21345v1",
    "published_date": "2025-10-24 11:19:33 UTC",
    "updated_date": "2025-10-24 11:19:33 UTC"
  },
  {
    "arxiv_id": "2510.21342v1",
    "title": "World-POI: Global Point-of-Interest Data Enriched from Foursquare and OpenStreetMap as Tabular and Graph Data",
    "authors": [
      "Hossein Amiri",
      "Mohammad Hashemi",
      "Andreas Züfle"
    ],
    "abstract": "Recently, Foursquare released a global dataset with more than 100 million points of interest (POIs), each representing a real-world business on its platform. However, many entries lack complete metadata such as addresses or categories, and some correspond to non-existent or fictional locations. In contrast, OpenStreetMap (OSM) offers a rich, user-contributed POI dataset with detailed and frequently updated metadata, though it does not formally verify whether a POI represents an actual business. In this data paper, we present a methodology that integrates the strengths of both datasets: Foursquare as a comprehensive baseline of commercial POIs and OSM as a source of enriched metadata. The combined dataset totals approximately 1 TB. While this full version is not publicly released, we provide filtered releases with adjustable thresholds that reduce storage needs and make the data practical to download and use across domains. We also provide step-by-step instructions to reproduce the full 631 GB build. Record linkage is achieved by computing name similarity scores and spatial distances between Foursquare and OSM POIs. These measures identify and retain high-confidence matches that correspond to real businesses in Foursquare, have representations in OSM, and show strong name similarity. Finally, we use this filtered dataset to construct a graph-based representation of POIs enriched with attributes from both sources, enabling advanced spatial analyses and a range of downstream applications.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.CG",
      "cs.CY",
      "cs.SI"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21342v1",
    "published_date": "2025-10-24 11:12:41 UTC",
    "updated_date": "2025-10-24 11:12:41 UTC"
  },
  {
    "arxiv_id": "2510.21341v2",
    "title": "Magellan: Guided MCTS for Latent Space Exploration and Novelty Generation",
    "authors": [
      "Lufan Chang"
    ],
    "abstract": "Large Language Models (LLMs) often struggle with generating truly innovative ideas, typically defaulting to high-probability, familiar concepts within their training data's \"gravity wells.\" While advanced search-based methods like Tree of Thoughts (ToT) attempt to mitigate this, they are fundamentally limited by their reliance on unprincipled, inconsistent self-evaluation heuristics to guide exploration. To address this gap, we introduce \\textbf{Magellan}, a novel framework that reframes creative generation as a principled, guided exploration of an LLM's latent conceptual space. At its core, Magellan employs Monte Carlo Tree Search (MCTS) governed by a hierarchical guidance system. For long-range direction, a \"semantic compass\" vector, formulated via orthogonal projection, steers the search towards relevant novelty. For local, step-by-step decisions, a landscape-aware value function replaces flawed self-evaluation with an explicit reward structure that balances intrinsic coherence, extrinsic novelty, and narrative progress. Extensive experiments demonstrate that Magellan significantly outperforms strong baselines, including ReAct and ToT, in generating scientific ideas with superior plausibility and innovation. Our work shows that for creative discovery, a principled, guided search is more effective than unconstrained agency, paving the way for LLMs to become more capable partners in innovation.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to 1st Open Conference on AI Agents for Science (agents4science 2025)",
    "pdf_url": "https://arxiv.org/pdf/2510.21341v2",
    "published_date": "2025-10-24 11:09:59 UTC",
    "updated_date": "2025-11-17 02:33:41 UTC"
  },
  {
    "arxiv_id": "2510.21333v1",
    "title": "CausalRec: A CausalBoost Attention Model for Sequential Recommendation",
    "authors": [
      "Yunbo Hou",
      "Tianle Yang",
      "Ruijie Li",
      "Li He",
      "Liang Wang",
      "Weiping Li",
      "Bo Zheng",
      "Guojie Song"
    ],
    "abstract": "Recent advances in correlation-based sequential recommendation systems have demonstrated substantial success. Specifically, the attention-based model outperforms other RNN-based and Markov chains-based models by capturing both short- and long-term dependencies more effectively. However, solely focusing on item co-occurrences overlooks the underlying motivations behind user behaviors, leading to spurious correlations and potentially inaccurate recommendations. To address this limitation, we present a novel framework that integrates causal attention for sequential recommendation, CausalRec. It incorporates a causal discovery block and a CausalBooster. The causal discovery block learns the causal graph in user behavior sequences, and we provide a theory to guarantee the identifiability of the learned causal graph. The CausalBooster utilizes the discovered causal graph to refine the attention mechanism, prioritizing behaviors with causal significance. Experimental evaluations on real-world datasets indicate that CausalRec outperforms several state-of-the-art methods, with average improvements of 7.21% in Hit Rate (HR) and 8.65% in Normalized Discounted Cumulative Gain (NDCG). To the best of our knowledge, this is the first model to incorporate causality through the attention mechanism in sequential recommendation, demonstrating the value of causality in generating more accurate and reliable recommendations.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "11 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.21333v1",
    "published_date": "2025-10-24 10:49:50 UTC",
    "updated_date": "2025-10-24 10:49:50 UTC"
  },
  {
    "arxiv_id": "2510.21332v2",
    "title": "Weak-to-Strong Generalization under Distribution Shifts",
    "authors": [
      "Myeongho Jeon",
      "Jan Sobotka",
      "Suhwan Choi",
      "Maria Brbić"
    ],
    "abstract": "As future superhuman models become increasingly complex, accurately supervising their behavior may exceed human capabilities. Recent works have demonstrated that in such scenarios, weak models can effectively supervise strong models, a phenomenon known as weak-to-strong generalization. However, we find that naive weak-to-strong generalization fails under distribution shifts, often leading to worse performance of the strong model than its weak supervisors. To address this, we propose RAVEN, a robust weak-to-strong generalization framework that dynamically learns the optimal combinations of weak models in addition to parameters of the strong model. We demonstrate the effectiveness of RAVEN on image classification, text classification, and preference alignment tasks. RAVEN outperforms alternative baselines by over 30% on out-of-distribution tasks while matching or surpassing existing methods on in-distribution tasks. Moreover, our results show that RAVEN assigns higher weights to more accurate weak models, demonstrating its ability to automatically identify trustworthy supervision.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to NeurIPS 2025; affiliations and acknowledgements updated",
    "pdf_url": "https://arxiv.org/pdf/2510.21332v2",
    "published_date": "2025-10-24 10:46:50 UTC",
    "updated_date": "2025-11-25 21:37:10 UTC"
  },
  {
    "arxiv_id": "2510.21329v1",
    "title": "TripTide: A Benchmark for Adaptive Travel Planning under Disruptions",
    "authors": [
      "Priyanshu Karmakar",
      "Soumyabrata Chaudhuri",
      "Shubhojit Mallick",
      "Manish Gupta",
      "Abhik Jana",
      "Shreya Ghosh"
    ],
    "abstract": "Recent efforts like TripCraft and TravelPlanner have advanced the use of Large Language Models ( LLMs) for personalized, constraint aware travel itinerary generation. Yet, real travel often faces disruptions. To address this, we present TripTide, the first benchmark evaluating LLM's ability to revise itineraries under realistic disruptions. TripTide models key dimensions such as disruption severity and traveler tolerance, enabling nuanced assessment of LLM adaptability to events like flight cancellations, weather closures, or overbooked attractions. We conduct a threefold evaluation. First, we introduce automatic metrics including Preservation of Intent (how well the revised plan maintains feasibility and goals), Responsiveness (promptness and appropriateness of disruption handling), and Adaptability (semantic, spatial, and sequential divergence between original and revised plans). Second, we apply an LLM-as-a-judge approach to automatically assess revision quality. Third, we perform manual expert evaluation to verify whether revisions preserve semantic, spatial, sequential, and responsive aspects. Our experiments show that LLMs maintain strong sequential consistency and semantic stability, while spatial deviations are larger for shorter trips but decrease with longer ones, indicating that extended plans encourage better geographic coherence. However, disruption-handling ability declines as plan length increases, highlighting limits in LLM robustness. TripTide establishes a benchmark for evaluating adaptability, personalization, and resilience in LLM-based travel planning under real-world uncertainty.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages, 12 tables and 7 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.21329v1",
    "published_date": "2025-10-24 10:39:55 UTC",
    "updated_date": "2025-10-24 10:39:55 UTC"
  },
  {
    "arxiv_id": "2510.21324v1",
    "title": "CXRAgent: Director-Orchestrated Multi-Stage Reasoning for Chest X-Ray Interpretation",
    "authors": [
      "Jinhui Lou",
      "Yan Yang",
      "Zhou Yu",
      "Zhenqi Fu",
      "Weidong Han",
      "Qingming Huang",
      "Jun Yu"
    ],
    "abstract": "Chest X-ray (CXR) plays a pivotal role in clinical diagnosis, and a variety of task-specific and foundation models have been developed for automatic CXR interpretation. However, these models often struggle to adapt to new diagnostic tasks and complex reasoning scenarios. Recently, LLM-based agent models have emerged as a promising paradigm for CXR analysis, enhancing model's capability through tool coordination, multi-step reasoning, and team collaboration, etc. However, existing agents often rely on a single diagnostic pipeline and lack mechanisms for assessing tools' reliability, limiting their adaptability and credibility. To this end, we propose CXRAgent, a director-orchestrated, multi-stage agent for CXR interpretation, where a central director coordinates the following stages: (1) Tool Invocation: The agent strategically orchestrates a set of CXR-analysis tools, with outputs normalized and verified by the Evidence-driven Validator (EDV), which grounds diagnostic outputs with visual evidence to support reliable downstream diagnosis; (2) Diagnostic Planning: Guided by task requirements and intermediate findings, the agent formulates a targeted diagnostic plan. It then assembles an expert team accordingly, defining member roles and coordinating their interactions to enable adaptive and collaborative reasoning; (3) Collaborative Decision-making: The agent integrates insights from the expert team with accumulated contextual memories, synthesizing them into an evidence-backed diagnostic conclusion. Experiments on various CXR interpretation tasks show that CXRAgent delivers strong performance, providing visual evidence and generalizes well to clinical tasks of different complexity. Code and data are valuable at this \\href{https://github.com/laojiahuo2003/CXRAgent/}{link}.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 4 figures, 7 Tables",
    "pdf_url": "https://arxiv.org/pdf/2510.21324v1",
    "published_date": "2025-10-24 10:31:30 UTC",
    "updated_date": "2025-10-24 10:31:30 UTC"
  },
  {
    "arxiv_id": "2510.21315v1",
    "title": "Seemingly Redundant Modules Enhance Robust Odor Learning in Fruit Flies",
    "authors": [
      "Haiyang Li",
      "Liao Yu",
      "Qiang Yu",
      "Yunliang Zang"
    ],
    "abstract": "Biological circuits have evolved to incorporate multiple modules that perform similar functions. In the fly olfactory circuit, both lateral inhibition (LI) and neuronal spike frequency adaptation (SFA) are thought to enhance pattern separation for odor learning. However, it remains unclear whether these mechanisms play redundant or distinct roles in this process. In this study, we present a computational model of the fly olfactory circuit to investigate odor discrimination under varying noise conditions that simulate complex environments. Our results show that LI primarily enhances odor discrimination in low- and medium-noise scenarios, but this benefit diminishes and may reverse under higher-noise conditions. In contrast, SFA consistently improves discrimination across all noise levels. LI is preferentially engaged in low- and medium-noise environments, whereas SFA dominates in high-noise settings. When combined, these two sparsification mechanisms enable optimal discrimination performance. This work demonstrates that seemingly redundant modules in biological circuits can, in fact, be essential for achieving optimal learning in complex contexts.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "10page,Accepted by NeurIPS",
    "pdf_url": "https://arxiv.org/pdf/2510.21315v1",
    "published_date": "2025-10-24 10:18:38 UTC",
    "updated_date": "2025-10-24 10:18:38 UTC"
  },
  {
    "arxiv_id": "2510.21314v1",
    "title": "A Convergence Analysis of Adaptive Optimizers under Floating-point Quantization",
    "authors": [
      "Xuan Tang",
      "Jichu Li",
      "Difan Zou"
    ],
    "abstract": "The rapid scaling of large language models (LLMs) has made low-precision training essential for reducing memory, improving efficiency, and enabling larger models and datasets. Existing convergence theories for adaptive optimizers, however, assume all components are exact and neglect hardware-aware quantization, leaving open the question of why low-precision training remains effective. We introduce the first theoretical framework for analyzing the convergence of adaptive optimizers, including Adam and Muon, under floating-point quantization of gradients, weights, and optimizer states (e.g., moment estimates). Within this framework, we derive convergence rates on smooth non-convex objectives under standard stochastic gradient assumptions, explicitly characterizing how quantization errors from different components affect convergence. We show that both algorithms retain rates close to their full-precision counterparts provided mantissa length scales only logarithmically with the number of iterations. Our analysis further reveals that Adam is highly sensitive to weights and second-moment quantization due to its reliance on $β_2 \\to 1$, while Muon requires weaker error control and is thus potentially more robust. These results narrow the gap between empirical success and theoretical understanding of low-precision training methods. Numerical experiments on synthetic and real-world data corroborate our theory.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "65 pages, 10 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.21314v1",
    "published_date": "2025-10-24 10:16:23 UTC",
    "updated_date": "2025-10-24 10:16:23 UTC"
  },
  {
    "arxiv_id": "2510.21310v1",
    "title": "Efficient semantic uncertainty quantification in language models via diversity-steered sampling",
    "authors": [
      "Ji Won Park",
      "Kyunghyun Cho"
    ],
    "abstract": "Accurately estimating semantic aleatoric and epistemic uncertainties in large language models (LLMs) is particularly challenging in free-form question answering (QA), where obtaining stable estimates often requires many expensive generations. We introduce a diversity-steered sampler that discourages semantically redundant outputs during decoding, covers both autoregressive and masked diffusion paradigms, and yields substantial sample-efficiency gains. The key idea is to inject a continuous semantic-similarity penalty into the model's proposal distribution using a natural language inference (NLI) model lightly finetuned on partial prefixes or intermediate diffusion states. We debias downstream uncertainty estimates with importance reweighting and shrink their variance with control variates. Across four QA benchmarks, our method matches or surpasses baselines while covering more semantic clusters with the same number of samples. Being modular and requiring no gradient access to the base LLM, the framework promises to serve as a drop-in enhancement for uncertainty estimation in risk-sensitive model deployments.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages (+7 appendix), 7 figures. Accepted at NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.21310v1",
    "published_date": "2025-10-24 10:06:21 UTC",
    "updated_date": "2025-10-24 10:06:21 UTC"
  },
  {
    "arxiv_id": "2510.23635v1",
    "title": "Help the machine to help you: an evaluation in the wild of egocentric data cleaning via skeptical learning",
    "authors": [
      "Andrea Bontempelli",
      "Matteo Busso",
      "Leonardo Javier Malcotti",
      "Fausto Giunchiglia"
    ],
    "abstract": "Any digital personal assistant, whether used to support task performance, answer questions, or manage work and daily life, including fitness schedules, requires high-quality annotations to function properly. However, user annotations, whether actively produced or inferred from context (e.g., data from smartphone sensors), are often subject to errors and noise. Previous research on Skeptical Learning (SKEL) addressed the issue of noisy labels by comparing offline active annotations with passive data, allowing for an evaluation of annotation accuracy. However, this evaluation did not include confirmation from end-users, the best judges of their own context. In this study, we evaluate SKEL's performance in real-world conditions with actual users who can refine the input labels based on their current perspectives and needs. The study involves university students using the iLog mobile application on their devices over a period of four weeks. The results highlight the challenges of finding the right balance between user effort and data quality, as well as the potential benefits of using SKEL, which include reduced annotation effort and improved quality of collected data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.23635v1",
    "published_date": "2025-10-24 10:01:24 UTC",
    "updated_date": "2025-10-24 10:01:24 UTC"
  },
  {
    "arxiv_id": "2510.21302v1",
    "title": "Towards Reliable Code-as-Policies: A Neuro-Symbolic Framework for Embodied Task Planning",
    "authors": [
      "Sanghyun Ahn",
      "Wonje Choi",
      "Junyong Lee",
      "Jinwoo Park",
      "Honguk Woo"
    ],
    "abstract": "Recent advances in large language models (LLMs) have enabled the automatic generation of executable code for task planning and control in embodied agents such as robots, demonstrating the potential of LLM-based embodied intelligence. However, these LLM-based code-as-policies approaches often suffer from limited environmental grounding, particularly in dynamic or partially observable settings, leading to suboptimal task success rates due to incorrect or incomplete code generation. In this work, we propose a neuro-symbolic embodied task planning framework that incorporates explicit symbolic verification and interactive validation processes during code generation. In the validation phase, the framework generates exploratory code that actively interacts with the environment to acquire missing observations while preserving task-relevant states. This integrated process enhances the grounding of generated code, resulting in improved task reliability and success rates in complex environments. We evaluate our framework on RLBench and in real-world settings across dynamic, partially observable scenarios. Experimental results demonstrate that our framework improves task success rates by 46.2% over Code-as-Policies baselines and attains over 86.8% executability of task-relevant actions, thereby enhancing the reliability of task planning in dynamic environments.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at NeurIPS 2025 Spotlight",
    "pdf_url": "https://arxiv.org/pdf/2510.21302v1",
    "published_date": "2025-10-24 10:01:08 UTC",
    "updated_date": "2025-10-24 10:01:08 UTC"
  },
  {
    "arxiv_id": "2510.23634v1",
    "title": "Monotone and Separable Set Functions: Characterizations and Neural Models",
    "authors": [
      "Soutrik Sarangi",
      "Yonatan Sverdlov",
      "Nadav Dym",
      "Abir De"
    ],
    "abstract": "Motivated by applications for set containment problems, we consider the following fundamental problem: can we design set-to-vector functions so that the natural partial order on sets is preserved, namely $S\\subseteq T \\text{ if and only if } F(S)\\leq F(T) $. We call functions satisfying this property Monotone and Separating (MAS) set functions. % We establish lower and upper bounds for the vector dimension necessary to obtain MAS functions, as a function of the cardinality of the multisets and the underlying ground set. In the important case of an infinite ground set, we show that MAS functions do not exist, but provide a model called our which provably enjoys a relaxed MAS property we name \"weakly MAS\" and is stable in the sense of Holder continuity. We also show that MAS functions can be used to construct universal models that are monotone by construction and can approximate all monotone set functions. Experimentally, we consider a variety of set containment tasks. The experiments show the benefit of using our our model, in comparison with standard set models which do not incorporate set containment as an inductive bias. Our code is available in https://github.com/yonatansverdlov/Monotone-Embedding.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.23634v1",
    "published_date": "2025-10-24 09:59:07 UTC",
    "updated_date": "2025-10-24 09:59:07 UTC"
  },
  {
    "arxiv_id": "2510.21293v2",
    "title": "Understanding AI Trustworthiness: A Scoping Review of AIES & FAccT Articles",
    "authors": [
      "Siddharth Mehrotra",
      "Jin Huang",
      "Xuelong Fu",
      "Roel Dobbe",
      "Clara I. Sánchez",
      "Maarten de Rijke"
    ],
    "abstract": "Background: Trustworthy AI serves as a foundational pillar for two major AI ethics conferences: AIES and FAccT. However, current research often adopts techno-centric approaches, focusing primarily on technical attributes such as reliability, robustness, and fairness, while overlooking the sociotechnical dimensions critical to understanding AI trustworthiness in real-world contexts.\n  Objectives: This scoping review aims to examine how the AIES and FAccT communities conceptualize, measure, and validate AI trustworthiness, identifying major gaps and opportunities for advancing a holistic understanding of trustworthy AI systems.\n  Methods: We conduct a scoping review of AIES and FAccT conference proceedings to date, systematically analyzing how trustworthiness is defined, operationalized, and applied across different research domains. Our analysis focuses on conceptualization approaches, measurement methods, verification and validation techniques, application areas, and underlying values.\n  Results: While significant progress has been made in defining technical attributes such as transparency, accountability, and robustness, our findings reveal critical gaps. Current research often predominantly emphasizes technical precision at the expense of social and ethical considerations. The sociotechnical nature of AI systems remains less explored and trustworthiness emerges as a contested concept shaped by those with the power to define it.\n  Conclusions: An interdisciplinary approach combining technical rigor with social, cultural, and institutional considerations is essential for advancing trustworthy AI. We propose actionable measures for the AI ethics community to adopt holistic frameworks that genuinely address the complex interplay between AI systems and society, ultimately promoting responsible technological development that benefits all stakeholders.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "Submitted to Journal of Artificial Intelligence Research (JAIR)",
    "pdf_url": "https://arxiv.org/pdf/2510.21293v2",
    "published_date": "2025-10-24 09:40:38 UTC",
    "updated_date": "2025-10-28 15:20:05 UTC"
  },
  {
    "arxiv_id": "2510.21285v3",
    "title": "When Models Outthink Their Safety: Unveiling and Mitigating Self-Jailbreak in Large Reasoning Models",
    "authors": [
      "Yingzhi Mao",
      "Chunkang Zhang",
      "Junxiang Wang",
      "Xinyan Guan",
      "Boxi Cao",
      "Yaojie Lu",
      "Hongyu Lin",
      "Xianpei Han",
      "Le Sun"
    ],
    "abstract": "Large Reasoning Models (LRMs) achieve strong performance on complex multi-step reasoning, yet they still exhibit severe safety failures such as harmful content generation. Existing methods often apply coarse-grained constraints over the entire reasoning trajectories, which can undermine reasoning capability while failing to address the root causes of unsafe behavior. In this work, we uncover a previously underexplored failure mode in LRMs, termed Self-Jailbreak, where models initially recognize the harmful intent of a query, but override this judgment during subsequent reasoning steps, ultimately generating unsafe outputs. Such a phenomenon reveals that LRMs are capable of recognizing harm, while safety failures primarily arise from reasoning steps. Motivated by this finding, we propose \\emph{Chain-of-Guardrail} (CoG), a trajectory-level training framework that mitigates Self-Jailbreak via targeted, step-level interventions while maintaining reasoning ability. Experiments across multiple safety and reasoning benchmarks indicate that CoG achieves a favorable balance between safety and reasoning performance compared with existing approaches.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "The first two authors contributed equally. The main text is 8 pages, with an appendix of 20 pages. The paper contains 20 figures and 15 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.21285v3",
    "published_date": "2025-10-24 09:32:25 UTC",
    "updated_date": "2026-01-08 07:30:22 UTC"
  },
  {
    "arxiv_id": "2510.21280v2",
    "title": "WhaleVAD-BPN: Improving Baleen Whale Call Detection with Boundary Proposal Networks and Post-processing Optimisation",
    "authors": [
      "Christiaan M. Geldenhuys",
      "Günther Tonitz",
      "Thomas R. Niesler"
    ],
    "abstract": "While recent sound event detection (SED) systems can identify baleen whale calls in marine audio, challenges related to false positive and minority-class detection persist. We propose the boundary proposal network (BPN), which extends an existing lightweight SED system. The BPN is inspired by work in image object detection and aims to reduce the number of false positive detections. It achieves this by using intermediate latent representations computed within the backbone classification model to gate the final output. When added to an existing SED system, the BPN achieves a 16.8 % absolute increase in precision, as well as 21.3 % and 9.4 % improvements in the F1-score for minority-class d-calls and bp-calls, respectively. We further consider two approaches to the selection of post-processing hyperparameters: a forward-search and a backward-search. By separately optimising event-level and frame-level hyperparameters, these two approaches lead to considerable performance improvements over parameters selected using empirical methods. The complete WhaleVAD-BPN system achieves a cross-validated development F1-score of 0.475, which is a 9.8 % absolute improvement over the baseline.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "q-bio.QM"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21280v2",
    "published_date": "2025-10-24 09:25:31 UTC",
    "updated_date": "2025-10-27 17:10:50 UTC"
  },
  {
    "arxiv_id": "2510.21276v1",
    "title": "Pctx: Tokenizing Personalized Context for Generative Recommendation",
    "authors": [
      "Qiyong Zhong",
      "Jiajie Su",
      "Yunshan Ma",
      "Julian McAuley",
      "Yupeng Hou"
    ],
    "abstract": "Generative recommendation (GR) models tokenize each action into a few discrete tokens (called semantic IDs) and autoregressively generate the next tokens as predictions, showing advantages such as memory efficiency, scalability, and the potential to unify retrieval and ranking. Despite these benefits, existing tokenization methods are static and non-personalized. They typically derive semantic IDs solely from item features, assuming a universal item similarity that overlooks user-specific perspectives. However, under the autoregressive paradigm, semantic IDs with the same prefixes always receive similar probabilities, so a single fixed mapping implicitly enforces a universal item similarity standard across all users. In practice, the same item may be interpreted differently depending on user intentions and preferences. To address this issue, we propose a personalized context-aware tokenizer that incorporates a user's historical interactions when generating semantic IDs. This design allows the same item to be tokenized into different semantic IDs under different user contexts, enabling GR models to capture multiple interpretive standards and produce more personalized predictions. Experiments on three public datasets demonstrate up to 11.44% improvement in NDCG@10 over non-personalized action tokenization baselines. Our code is available at https://github.com/YoungZ365/Pctx.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21276v1",
    "published_date": "2025-10-24 09:22:04 UTC",
    "updated_date": "2025-10-24 09:22:04 UTC"
  },
  {
    "arxiv_id": "2510.24768v1",
    "title": "Combining SAR Simulators to Train ATR Models with Synthetic Data",
    "authors": [
      "Benjamin Camus",
      "Julien Houssay",
      "Corentin Le Barbu",
      "Eric Monteux",
      "Cédric Saleun",
      "Christian Cochin"
    ],
    "abstract": "This work aims to train Deep Learning models to perform Automatic Target Recognition (ATR) on Synthetic Aperture Radar (SAR) images. To circumvent the lack of real labelled measurements, we resort to synthetic data produced by SAR simulators. Simulation offers full control over the virtual environment, which enables us to generate large and diversified datasets at will. However, simulations are intrinsically grounded on simplifying assumptions of the real world (i.e. physical models). Thus, synthetic datasets are not as representative as real measurements. Consequently, ATR models trained on synthetic images cannot generalize well on real measurements. Our contributions to this problem are twofold: on one hand, we demonstrate and quantify the impact of the simulation paradigm on the ATR. On the other hand, we propose a new approach to tackle the ATR problem: combine two SAR simulators that are grounded on different (but complementary) paradigms to produce synthetic datasets. To this end, we use two simulators: MOCEM, which is based on a scattering centers model approach, and Salsa, which resorts on a ray tracing strategy. We train ATR models using synthetic dataset generated both by MOCEM and Salsa and our Deep Learning approach called ADASCA. We reach an accuracy of almost 88 % on the MSTAR measurements.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.24768v1",
    "published_date": "2025-10-24 09:21:07 UTC",
    "updated_date": "2025-10-24 09:21:07 UTC"
  },
  {
    "arxiv_id": "2510.21275v1",
    "title": "Investigating Scale Independent UCT Exploration Factor Strategies",
    "authors": [
      "Robin Schmöcker",
      "Christoph Schnell",
      "Alexander Dockhorn"
    ],
    "abstract": "The Upper Confidence Bounds For Trees (UCT) algorithm is not agnostic to the reward scale of the game it is applied to. For zero-sum games with the sparse rewards of $\\{-1,0,1\\}$ at the end of the game, this is not a problem, but many games often feature dense rewards with hand-picked reward scales, causing a node's Q-value to span different magnitudes across different games. In this paper, we evaluate various strategies for adaptively choosing the UCT exploration constant $λ$, called $λ$-strategies, that are agnostic to the game's reward scale. These $λ$-strategies include those proposed in the literature as well as five new strategies. Given our experimental results, we recommend using one of our newly suggested $λ$-strategies, which is to choose $λ$ as $2 \\cdot σ$ where $σ$ is the empirical standard deviation of all state-action pairs' Q-values of the search tree. This method outperforms existing $λ$-strategies across a wide range of tasks both in terms of a single parameter value and the peak performances obtained by optimizing all available parameters.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21275v1",
    "published_date": "2025-10-24 09:19:14 UTC",
    "updated_date": "2025-10-24 09:19:14 UTC"
  },
  {
    "arxiv_id": "2510.21270v1",
    "title": "Sparser Block-Sparse Attention via Token Permutation",
    "authors": [
      "Xinghao Wang",
      "Pengyu Wang",
      "Dong Zhang",
      "Chenkun Tan",
      "Shaojun Zhou",
      "Zhaoxiang Liu",
      "Shiguo Lian",
      "Fangxu Liu",
      "Kai Song",
      "Xipeng Qiu"
    ],
    "abstract": "Scaling the context length of large language models (LLMs) offers significant benefits but is computationally expensive. This expense stems primarily from the self-attention mechanism, whose $O(N^2)$ complexity with respect to sequence length presents a major bottleneck for both memory and latency. Fortunately, the attention matrix is often sparse, particularly for long sequences, suggesting an opportunity for optimization. Block-sparse attention has emerged as a promising solution that partitions sequences into blocks and skips computation for a subset of these blocks. However, the effectiveness of this method is highly dependent on the underlying attention patterns, which can lead to sub-optimal block-level sparsity. For instance, important key tokens for queries within a single block may be scattered across numerous other blocks, leading to computational redundancy. In this work, we propose Permuted Block-Sparse Attention (\\textbf{PBS-Attn}), a plug-and-play method that leverages the permutation properties of attention to increase block-level sparsity and enhance the computational efficiency of LLM prefilling. We conduct comprehensive experiments on challenging real-world long-context datasets, demonstrating that PBS-Attn consistently outperforms existing block-sparse attention methods in model accuracy and closely matches the full attention baseline. Powered by our custom permuted-FlashAttention kernels, PBS-Attn achieves an end-to-end speedup of up to $2.75\\times$ in long-context prefilling, confirming its practical viability. Code available at https://github.com/xinghaow99/pbs-attn",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21270v1",
    "published_date": "2025-10-24 09:11:50 UTC",
    "updated_date": "2025-10-24 09:11:50 UTC"
  },
  {
    "arxiv_id": "2510.21894v1",
    "title": "Understanding Network Behaviors through Natural Language Question-Answering",
    "authors": [
      "Mingzhe Xing",
      "Chang Tian",
      "Jianan Zhang",
      "Lichen Pan",
      "Peipei Liu",
      "Zhaoteng Yan",
      "Yinliang Yue"
    ],
    "abstract": "Modern large-scale networks introduce significant complexity in understanding network behaviors, increasing the risk of misconfiguration. Prior work proposed to understand network behaviors by mining network configurations, typically relying on domain-specific languages interfaced with formal models. While effective, they suffer from a steep learning curve and limited flexibility. In contrast, natural language (NL) offers a more accessible and interpretable interface, motivating recent research on NL-guided network behavior understanding. Recent advances in large language models (LLMs) further enhance this direction, leveraging their extensive prior knowledge of network concepts and strong reasoning capabilities. However, three key challenges remain: 1) numerous router devices with lengthy configuration files challenge LLM's long-context understanding ability; 2) heterogeneity across devices and protocols impedes scalability; and 3) complex network topologies and protocols demand advanced reasoning abilities beyond the current capabilities of LLMs. To tackle the above challenges, we propose NetMind, a novel framework for querying networks using NL. Our approach introduces a tree-based configuration chunking strategy to preserve semantic coherence while enabling efficient partitioning. We then construct a unified fact graph as an intermediate representation to normalize vendor-specific configurations. Finally, we design a hybrid imperative-declarative language to reduce the reasoning burden on LLMs and enhance precision. We contribute a benchmark consisting of NL question-answer pairs paired with network configurations. Experiments demonstrate that NetMind achieves accurate and scalable network behavior understanding, outperforming existing baselines.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Large Language Models",
    "pdf_url": "https://arxiv.org/pdf/2510.21894v1",
    "published_date": "2025-10-24 08:54:29 UTC",
    "updated_date": "2025-10-24 08:54:29 UTC"
  },
  {
    "arxiv_id": "2601.05258v1",
    "title": "From Events to Trending: A Multi-Stage Hotspots Detection Method Based on Generative Query Indexing",
    "authors": [
      "Kaichun Wang",
      "Yanguang Chen",
      "Ting Zhang",
      "Mengyao Bao",
      "Keyu Chen",
      "Xu Hu",
      "Yongliang Wang",
      "Jingsheng Yang",
      "Jinsong Zhang",
      "Fei Lu"
    ],
    "abstract": "LLM-based conversational systems have become a popular gateway for information access, yet most existing chatbots struggle to handle news-related trending queries effectively. To improve user experience, an effective trending query detection method is urgently needed to enable differentiated processing of such target traffic. However, current research on trending detection tailored to the dialogue system scenario remains largely unexplored, and methods designed for traditional search engines often underperform in conversational contexts due to radically distinct query distributions and expression patterns. To fill this gap, we propose a multi-stage framework for trending detection, which achieves systematic optimization from both offline generation and online identification perspectives. Specifically, our framework first exploits selected hot events to generate index queries, establishing a key bridge between static events and dynamic user queries. It then employs a retrieval matching mechanism for real-time online detection of trending queries, where we introduce a cascaded recall and ranking architecture to balance detection efficiency and accuracy. Furthermore, to better adapt to the practical application scenario, our framework adopts a single-recall module as a cold-start strategy to collect online data for fine-tuning the reranker. Extensive experiments demonstrate that our framework significantly outperforms baseline methods in both offline evaluations and online A/B tests, and user satisfaction is relatively improved by 27\\% in terms of positive-negative feedback ratio.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.05258v1",
    "published_date": "2025-10-24 08:49:38 UTC",
    "updated_date": "2025-10-24 08:49:38 UTC"
  },
  {
    "arxiv_id": "2510.21258v1",
    "title": "Correlation Dimension of Auto-Regressive Large Language Models",
    "authors": [
      "Xin Du",
      "Kumiko Tanaka-Ishii"
    ],
    "abstract": "Large language models (LLMs) have achieved remarkable progress in natural language generation, yet they continue to display puzzling behaviors -- such as repetition and incoherence -- even when exhibiting low perplexity. This highlights a key limitation of conventional evaluation metrics, which emphasize local prediction accuracy while overlooking long-range structural complexity. We introduce correlation dimension, a fractal-geometric measure of self-similarity, to quantify the epistemological complexity of text as perceived by a language model. This measure captures the hierarchical recurrence structure of language, bridging local and global properties in a unified framework. Through extensive experiments, we show that correlation dimension (1) reveals three distinct phases during pretraining, (2) reflects context-dependent complexity, (3) indicates a model's tendency toward hallucination, and (4) reliably detects multiple forms of degeneration in generated text. The method is computationally efficient, robust to model quantization (down to 4-bit precision), broadly applicable across autoregressive architectures (e.g., Transformer and Mamba), and provides fresh insight into the generative dynamics of LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "nlin.CD"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.21258v1",
    "published_date": "2025-10-24 08:42:23 UTC",
    "updated_date": "2025-10-24 08:42:23 UTC"
  },
  {
    "arxiv_id": "2511.05509v1",
    "title": "Randomized-MLP Regularization Improves Domain Adaptation and Interpretability in DINOv2",
    "authors": [
      "Joel Valdivia Ortega",
      "Lorenz Lamm",
      "Franziska Eckardt",
      "Benedikt Schworm",
      "Marion Jasnin",
      "Tingying Peng"
    ],
    "abstract": "Vision Transformers (ViTs), such as DINOv2, achieve strong performance across domains but often repurpose low-informative patch tokens in ways that reduce the interpretability of attention and feature maps. This challenge is especially evident in medical imaging, where domain shifts can degrade both performance and transparency. In this paper, we introduce Randomized-MLP (RMLP) regularization, a contrastive learning-based method that encourages more semantically aligned representations. We use RMLPs when fine-tuning DINOv2 to both medical and natural image modalities, showing that it improves or maintains downstream performance while producing more interpretable attention maps. We also provide a mathematical analysis of RMLPs, offering insights into its role in enhancing ViT-based models and advancing our understanding of contrastive learning.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.05509v1",
    "published_date": "2025-10-24 08:39:18 UTC",
    "updated_date": "2025-10-24 08:39:18 UTC"
  },
  {
    "arxiv_id": "2510.21254v1",
    "title": "Out-of-Distribution Detection for Safety Assurance of AI and Autonomous Systems",
    "authors": [
      "Victoria J. Hodge",
      "Colin Paterson",
      "Ibrahim Habli"
    ],
    "abstract": "The operational capabilities and application domains of AI-enabled autonomous systems have expanded significantly in recent years due to advances in robotics and machine learning (ML). Demonstrating the safety of autonomous systems rigorously is critical for their responsible adoption but it is challenging as it requires robust methodologies that can handle novel and uncertain situations throughout the system lifecycle, including detecting out-of-distribution (OoD) data. Thus, OOD detection is receiving increased attention from the research, development and safety engineering communities. This comprehensive review analyses OOD detection techniques within the context of safety assurance for autonomous systems, in particular in safety-critical domains. We begin by defining the relevant concepts, investigating what causes OOD and exploring the factors which make the safety assurance of autonomous systems and OOD detection challenging. Our review identifies a range of techniques which can be used throughout the ML development lifecycle and we suggest areas within the lifecycle in which they may be used to support safety assurance arguments. We discuss a number of caveats that system and safety engineers must be aware of when integrating OOD detection into system lifecycles. We conclude by outlining the challenges and future work necessary for the safe development and operation of autonomous systems across a range of domains and applications.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21254v1",
    "published_date": "2025-10-24 08:38:01 UTC",
    "updated_date": "2025-10-24 08:38:01 UTC"
  },
  {
    "arxiv_id": "2510.21244v2",
    "title": "VoiceAgentEval: A Dual-Dimensional Benchmark for Expert-Level Intelligent Voice-Agent Evaluation of Xbench's Professional-Aligned Series",
    "authors": [
      "Pengyu Xu",
      "Shijia Li",
      "Ao Sun",
      "Feng Zhang",
      "Yahan Li",
      "Bo Wu",
      "Zhanyu Ma",
      "Jiguo Li",
      "Jun Xu",
      "Jiuchong Gao",
      "Jinghua Hao",
      "Renqing He",
      "Rui Wang",
      "Yang Liu",
      "Xiaobo Hu",
      "Fan Yang",
      "Jia Zheng",
      "Guanghua Yao"
    ],
    "abstract": "We propose OutboundEval, a comprehensive benchmark for evaluating large language models (LLMs) in expert-level intelligent outbound calling scenarios. Unlike existing methods that suffer from three key limitations - insufficient dataset diversity and category coverage, unrealistic user simulation, and inaccurate evaluation metrics - OutboundEval addresses these issues through a structured framework. First, we design a benchmark spanning six major business domains and 30 representative sub-scenarios, each with scenario-specific process decomposition, weighted scoring, and domain-adaptive metrics. Second, we develop a large-model-driven User Simulator that generates diverse, persona-rich virtual users with realistic behaviors, emotional variability, and communication styles, providing a controlled yet authentic testing environment. Third, we introduce a dynamic evaluation method that adapts to task variations, integrating automated and human-in-the-loop assessment to measure task execution accuracy, professional knowledge application, adaptability, and user experience quality. Experiments on 12 state-of-the-art LLMs reveal distinct trade-offs between expert-level task completion and interaction fluency, offering practical insights for building reliable, human-like outbound AI systems. OutboundEval establishes a practical, extensible, and domain-oriented standard for benchmarking LLMs in professional applications.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21244v2",
    "published_date": "2025-10-24 08:27:58 UTC",
    "updated_date": "2025-11-14 11:59:53 UTC"
  },
  {
    "arxiv_id": "2510.21238v1",
    "title": "Physics-Informed Neural Networks for MIMO Beam Map and Environment Reconstruction",
    "authors": [
      "Wangqian Chen",
      "Junting Chen",
      "Shuguang Cui"
    ],
    "abstract": "As communication networks evolve towards greater complexity (e.g., 6G and beyond), a deep understanding of the wireless environment becomes increasingly crucial. When explicit knowledge of the environment is unavailable, geometry-aware feature extraction from channel state information (CSI) emerges as a pivotal methodology to bridge physical-layer measurements with network intelligence. This paper proposes to explore the received signal strength (RSS) data, without explicit 3D environment knowledge, to jointly construct the radio beam map and environmental geometry for a multiple-input multiple-output (MIMO) system. Unlike existing methods that only learn blockage structures, we propose an oriented virtual obstacle model that captures the geometric features of both blockage and reflection. Reflective zones are formulated to identify relevant reflected paths according to the geometry relation of the environment. We derive an analytical expression for the reflective zone and further analyze its geometric characteristics to develop a reformulation that is more compatible with deep learning representations. A physics-informed deep learning framework that incorporates the reflective-zone-based geometry model is proposed to learn the blockage, reflection, and scattering components, along with the beam pattern, which leverages physics prior knowledge to enhance network transferability. Numerical experiments demonstrate that, in addition to reconstructing the blockage and reflection geometry, the proposed model can construct a more accurate MIMO beam map with a 32%-48% accuracy improvement.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.IT"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21238v1",
    "published_date": "2025-10-24 08:17:14 UTC",
    "updated_date": "2025-10-24 08:17:14 UTC"
  },
  {
    "arxiv_id": "2510.21236v2",
    "title": "Securing AI Agent Execution",
    "authors": [
      "Christoph Bühler",
      "Matteo Biagiola",
      "Luca Di Grazia",
      "Guido Salvaneschi"
    ],
    "abstract": "Large Language Models (LLMs) have evolved into AI agents that interact with external tools and environments to perform complex tasks. The Model Context Protocol (MCP) has become the de facto standard for connecting agents with such resources, but security has lagged behind: thousands of MCP servers execute with unrestricted access to host systems, creating a broad attack surface. In this paper, we introduce AgentBound, the first access control framework for MCP servers. AgentBound combines a declarative policy mechanism, inspired by the Android permission model, with a policy enforcement engine that contains malicious behavior without requiring MCP server modifications. We build a dataset containing the 296 most popular MCP servers, and show that access control policies can be generated automatically from source code with 80.9% accuracy. We also show that AgentBound blocks the majority of security threats in several malicious MCP servers, and that policy enforcement engine introduces negligible overhead. Our contributions provide developers and project managers with a practical foundation for securing MCP servers while maintaining productivity, enabling researchers and tool builders to explore new directions for declarative access control and MCP security.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21236v2",
    "published_date": "2025-10-24 08:10:36 UTC",
    "updated_date": "2025-10-29 13:11:21 UTC"
  },
  {
    "arxiv_id": "2510.23633v1",
    "title": "Noise is All You Need: Solving Linear Inverse Problems by Noise Combination Sampling with Diffusion Models",
    "authors": [
      "Xun Su",
      "Hiroyuki Kasai"
    ],
    "abstract": "Pretrained diffusion models have demonstrated strong capabilities in zero-shot inverse problem solving by incorporating observation information into the generation process of the diffusion models. However, this presents an inherent dilemma: excessive integration can disrupt the generative process, while insufficient integration fails to emphasize the constraints imposed by the inverse problem. To address this, we propose \\emph{Noise Combination Sampling}, a novel method that synthesizes an optimal noise vector from a noise subspace to approximate the measurement score, replacing the noise term in the standard Denoising Diffusion Probabilistic Models process. This enables conditional information to be naturally embedded into the generation process without reliance on step-wise hyperparameter tuning. Our method can be applied to a wide range of inverse problem solvers, including image compression, and, particularly when the number of generation steps $T$ is small, achieves superior performance with negligible computational overhead, significantly improving robustness and stability.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "eess.IV"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.23633v1",
    "published_date": "2025-10-24 07:46:23 UTC",
    "updated_date": "2025-10-24 07:46:23 UTC"
  },
  {
    "arxiv_id": "2511.11590v2",
    "title": "Embedding Explainable AI in NHS Clinical Safety: The Explainability-Enabled Clinical Safety Framework (ECSF)",
    "authors": [
      "Robert Gigiu"
    ],
    "abstract": "Artificial intelligence (AI) is increasingly embedded in NHS workflows, but its probabilistic and adaptive behaviour conflicts with the deterministic assumptions underpinning existing clinical-safety standards. DCB0129 and DCB0160 provide strong governance for conventional software yet do not define how AI-specific transparency, interpretability, or model drift should be evidenced within Safety Cases, Hazard Logs, or post-market monitoring. This paper proposes an Explainability-Enabled Clinical Safety Framework (ECSF) that integrates explainability into the DCB0129/0160 lifecycle, enabling Clinical Safety Officers to use interpretability outputs as structured safety evidence without altering compliance pathways. A cross-regulatory synthesis mapped DCB clauses to principles from Good Machine Learning Practice, the NHS AI Assurance and T.E.S.T. frameworks, and the EU AI Act. The resulting matrix links regulatory clauses, principles, ECSF checkpoints, and suitable explainability outputs. ECSF introduces five checkpoints: global transparency for hazard identification, case-level interpretability for verification, clinician usability for evaluation, traceable decision pathways for risk control, and longitudinal interpretability monitoring for post-market surveillance. Techniques such as SHAP, LIME, Integrated Gradients, saliency mapping, and attention visualisation are mapped to corresponding DCB artefacts. ECSF reframes explainability as a core element of clinical-safety assurance, bridging deterministic risk governance with the probabilistic behaviour of AI and supporting alignment with GMLP, the EU AI Act, and NHS AI Assurance principles.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "33 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2511.11590v2",
    "published_date": "2025-10-24 07:36:58 UTC",
    "updated_date": "2025-11-18 14:03:08 UTC"
  },
  {
    "arxiv_id": "2510.21188v1",
    "title": "PLAN: Proactive Low-Rank Allocation for Continual Learning",
    "authors": [
      "Xiequn Wang",
      "Zhan Zhuang",
      "Yu Zhang"
    ],
    "abstract": "Continual learning (CL) requires models to continuously adapt to new tasks without forgetting past knowledge. In this work, we propose \\underline{P}roactive \\underline{L}ow-rank \\underline{A}llocatio\\underline{N} (PLAN), a framework that extends Low-Rank Adaptation (LoRA) to enable efficient and interference-aware fine-tuning of large pre-trained models in CL settings. PLAN proactively manages the allocation of task-specific subspaces by introducing orthogonal basis vectors for each task and optimizing them through a perturbation-based strategy that minimizes conflicts with previously learned parameters. Furthermore, PLAN incorporates a novel selection mechanism that identifies and assigns basis vectors with minimal sensitivity to interference, reducing the risk of degrading past knowledge while maintaining efficient adaptation to new tasks. Empirical results on standard CL benchmarks demonstrate that PLAN consistently outperforms existing methods, establishing a new state-of-the-art for continual learning with foundation models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "accepted by ICCV 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.21188v1",
    "published_date": "2025-10-24 06:37:41 UTC",
    "updated_date": "2025-10-24 06:37:41 UTC"
  },
  {
    "arxiv_id": "2510.21184v1",
    "title": "Reducing the Probability of Undesirable Outputs in Language Models Using Probabilistic Inference",
    "authors": [
      "Stephen Zhao",
      "Aidan Li",
      "Rob Brekelmans",
      "Roger Grosse"
    ],
    "abstract": "Reinforcement learning (RL) has become a predominant technique to align language models (LMs) with human preferences or promote outputs which are deemed to be desirable by a given reward function. Standard RL approaches optimize average reward, while methods explicitly focused on reducing the probability of undesired outputs typically come at a cost to average-case performance. To improve this tradeoff, we introduce RePULSe, a new training method that augments the standard RL loss with an additional loss that uses learned proposals to guide sampling low-reward outputs, and then reduces those outputs' probability. We run experiments demonstrating that RePULSe produces a better tradeoff of expected reward versus the probability of undesired outputs and is more adversarially robust, compared to standard RL alignment approaches and alternatives.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21184v1",
    "published_date": "2025-10-24 06:23:55 UTC",
    "updated_date": "2025-10-24 06:23:55 UTC"
  },
  {
    "arxiv_id": "2511.00021v1",
    "title": "Deep Learning Models for Coral Bleaching Classification in Multi-Condition Underwater Image Datasets",
    "authors": [
      "Julio Jerison E. Macrohon",
      "Gordon Hung"
    ],
    "abstract": "Coral reefs support numerous marine organisms and are an important source of coastal protection from storms and floods, representing a major part of marine ecosystems. However coral reefs face increasing threats from pollution, ocean acidification, and sea temperature anomalies, making efficient protection and monitoring heavily urgent. Therefore, this study presents a novel machine-learning-based coral bleaching classification system based on a diverse global dataset with samples of healthy and bleached corals under varying environmental conditions, including deep seas, marshes, and coastal zones. We benchmarked and compared three state-of-the-art models: Residual Neural Network (ResNet), Vision Transformer (ViT), and Convolutional Neural Network (CNN). After comprehensive hyperparameter tuning, the CNN model achieved the highest accuracy of 88%, outperforming existing benchmarks. Our findings offer important insights into autonomous coral monitoring and present a comprehensive analysis of the most widely used computer vision models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "15 pages, 10 figures",
    "pdf_url": "https://arxiv.org/pdf/2511.00021v1",
    "published_date": "2025-10-24 06:13:15 UTC",
    "updated_date": "2025-10-24 06:13:15 UTC"
  },
  {
    "arxiv_id": "2510.21181v1",
    "title": "Shylock: Causal Discovery in Multivariate Time Series based on Hybrid Constraints",
    "authors": [
      "Shuo Li",
      "Keqin Xu",
      "Jie Liu",
      "Dan Ye"
    ],
    "abstract": "Causal relationship discovery has been drawing increasing attention due to its prevalent application. Existing methods rely on human experience, statistical methods, or graphical criteria methods which are error-prone, stuck at the idealized assumption, and rely on a huge amount of data. And there is also a serious data gap in accessing Multivariate time series(MTS) in many areas, adding difficulty in finding their causal relationship. Existing methods are easy to be over-fitting on them. To fill the gap we mentioned above, in this paper, we propose Shylock, a novel method that can work well in both few-shot and normal MTS to find the causal relationship. Shylock can reduce the number of parameters exponentially by using group dilated convolution and a sharing kernel, but still learn a better representation of variables with time delay. By combing the global constraint and the local constraint, Shylock achieves information sharing among networks to help improve the accuracy. To evaluate the performance of Shylock, we also design a data generation method to generate MTS with time delay. We evaluate it on commonly used benchmarks and generated datasets. Extensive experiments show that Shylock outperforms two existing state-of-art methods on both few-shot and normal MTS. We also developed Tcausal, a library for easy use and deployed it on the EarthDataMiner platform",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21181v1",
    "published_date": "2025-10-24 06:12:24 UTC",
    "updated_date": "2025-10-24 06:12:24 UTC"
  },
  {
    "arxiv_id": "2511.05508v2",
    "title": "Personalized Chain-of-Thought Summarization of Financial News for Investor Decision Support",
    "authors": [
      "Tianyi Zhang",
      "Mu Chen"
    ],
    "abstract": "Financial advisors and investors struggle with information overload from financial news, where irrelevant content and noise obscure key market signals and hinder timely investment decisions. To address this, we propose a novel Chain-of-Thought (CoT) summarization framework that condenses financial news into concise, event-driven summaries. The framework integrates user-specified keywords to generate personalized outputs, ensuring that only the most relevant contexts are highlighted. These personalized summaries provide an intermediate layer that supports language models in producing investor-focused narratives, bridging the gap between raw news and actionable insights.",
    "categories": [
      "q-fin.GN",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "q-fin.GN",
    "comment": "Proceedings of ICDM Workshops",
    "pdf_url": "https://arxiv.org/pdf/2511.05508v2",
    "published_date": "2025-10-24 05:55:05 UTC",
    "updated_date": "2025-11-13 09:54:07 UTC"
  },
  {
    "arxiv_id": "2510.21175v1",
    "title": "Memory-Free Continual Learning with Null Space Adaptation for Zero-Shot Vision-Language Models",
    "authors": [
      "Yujin Jo",
      "Taesup Kim"
    ],
    "abstract": "Pre-trained vision-language models (VLMs), such as CLIP, have demonstrated remarkable zero-shot generalization, enabling deployment in a wide range of real-world tasks without additional task-specific training. However, in real deployment scenarios with evolving environments or emerging classes, these models inevitably face distributional shifts and novel tasks. In such contexts, static zero-shot capabilities are insufficient, and there is a growing need for continual learning methods that allow models to adapt over time while avoiding catastrophic forgetting. We introduce NuSA-CL (Null Space Adaptation for Continual Learning), a lightweight memory-free continual learning framework designed to address this challenge. NuSA-CL employs low-rank adaptation and constrains task-specific weight updates to lie within an approximate null space of the model's current parameters. This strategy minimizes interference with previously acquired knowledge, effectively preserving the zero-shot capabilities of the original model. Unlike methods relying on replay buffers or costly distillation, NuSA-CL imposes minimal computational and memory overhead, making it practical for deployment in resource-constrained, real-world continual learning environments. Experiments show that our framework not only effectively preserves zero-shot transfer capabilities but also achieves highly competitive performance on continual learning benchmarks. These results position NuSA-CL as a practical and scalable solution for continually evolving zero-shot VLMs in real-world applications.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21175v1",
    "published_date": "2025-10-24 05:53:32 UTC",
    "updated_date": "2025-10-24 05:53:32 UTC"
  },
  {
    "arxiv_id": "2510.23632v2",
    "title": "LLMComp: A Language Modeling Paradigm for Error-Bounded Scientific Data Compression (Technical Report)",
    "authors": [
      "Guozhong Li",
      "Muhannad Alhumaidi",
      "Spiros Skiadopoulos",
      "Panos Kalnis"
    ],
    "abstract": "The rapid growth of high-resolution scientific simulations and observation systems is generating massive spatiotemporal datasets, making efficient, error-bounded compression increasingly important. Meanwhile, decoder-only large language models (LLMs) have demonstrated remarkable capabilities in modeling complex sequential data. In this paper, we propose LLMCOMP, a novel lossy compression paradigm that leverages decoder-only large LLMs to model scientific data. LLMCOMP first quantizes 3D fields into discrete tokens, arranges them via Z-order curves to preserve locality, and applies coverage-guided sampling to enhance training efficiency. An autoregressive transformer is then trained with spatial-temporal embeddings to model token transitions. During compression, the model performs top-k prediction, storing only rank indices and fallback corrections to ensure strict error bounds. Experiments on multiple reanalysis datasets show that LLMCOMP consistently outperforms state-of-the-art compressors, achieving up to 30% higher compression ratios under strict error bounds. These results highlight the potential of LLMs as general-purpose compressors for high-fidelity scientific data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.23632v2",
    "published_date": "2025-10-24 05:41:04 UTC",
    "updated_date": "2025-11-04 20:59:30 UTC"
  },
  {
    "arxiv_id": "2510.21155v1",
    "title": "Towards Straggler-Resilient Split Federated Learning: An Unbalanced Update Approach",
    "authors": [
      "Dandan Liang",
      "Jianing Zhang",
      "Evan Chen",
      "Zhe Li",
      "Rui Li",
      "Haibo Yang"
    ],
    "abstract": "Split Federated Learning (SFL) enables scalable training on edge devices by combining the parallelism of Federated Learning (FL) with the computational offloading of Split Learning (SL). Despite its great success, SFL suffers significantly from the well-known straggler issue in distributed learning systems. This problem is exacerbated by the dependency between Split Server and clients: the Split Server side model update relies on receiving activations from clients. Such synchronization requirement introduces significant time latency, making straggler a critical bottleneck to the scalability and efficiency of the system. To mitigate this problem, we propose MU-SplitFed, a straggler-resilient SFL algorithm in zeroth-order optimization that decouples training progress from straggler delays via a simple yet effective unbalanced update mechanism.\n  By enabling the server to perform $τ$ local updates per client round, MU-SplitFed achieves a convergence rate of $O(\\sqrt{d/(τT)})$ for non-convex objectives, demonstrating a linear speedup of $τ$ in communication rounds. Experiments demonstrate that MU-SplitFed consistently outperforms baseline methods with the presence of stragglers and effectively mitigates their impact through adaptive tuning of $τ$. The code for this project is available at https://github.com/Johnny-Zip/MU-SplitFed.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21155v1",
    "published_date": "2025-10-24 04:55:27 UTC",
    "updated_date": "2025-10-24 04:55:27 UTC"
  },
  {
    "arxiv_id": "2510.21153v2",
    "title": "Uncertainty-Aware Multi-Objective Reinforcement Learning-Guided Diffusion Models for 3D De Novo Molecular Design",
    "authors": [
      "Lianghong Chen",
      "Dongkyu Eugene Kim",
      "Mike Domaratzki",
      "Pingzhao Hu"
    ],
    "abstract": "Designing de novo 3D molecules with desirable properties remains a fundamental challenge in drug discovery and molecular engineering. While diffusion models have demonstrated remarkable capabilities in generating high-quality 3D molecular structures, they often struggle to effectively control complex multi-objective constraints critical for real-world applications. In this study, we propose an uncertainty-aware Reinforcement Learning (RL) framework to guide the optimization of 3D molecular diffusion models toward multiple property objectives while enhancing the overall quality of the generated molecules. Our method leverages surrogate models with predictive uncertainty estimation to dynamically shape reward functions, facilitating balance across multiple optimization objectives. We comprehensively evaluate our framework across three benchmark datasets and multiple diffusion model architectures, consistently outperforming baselines for molecular quality and property optimization. Additionally, Molecular Dynamics (MD) simulations and ADMET profiling of top generated candidates indicate promising drug-like behavior and binding stability, comparable to known Epidermal Growth Factor Receptor (EGFR) inhibitors. Our results demonstrate the strong potential of RL-guided generative diffusion models for advancing automated molecular design.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.21153v2",
    "published_date": "2025-10-24 04:49:23 UTC",
    "updated_date": "2026-01-14 04:12:09 UTC"
  },
  {
    "arxiv_id": "2510.21150v2",
    "title": "String Seed of Thought: Prompting LLMs for Distribution-Faithful and Diverse Generation",
    "authors": [
      "Kou Misaki",
      "Takuya Akiba"
    ],
    "abstract": "We introduce String Seed of Thought (SSoT), a novel prompting method for LLMs that improves Probabilistic Instruction Following (PIF). We define PIF as a task requiring an LLM to select its answer from a predefined set of options, each associated with a specific probability, such that the empirical distribution of the generated answers aligns with the target distribution when prompted multiple times. While LLMs excel at tasks with single, deterministic answers, they often fail at PIF, exhibiting biases problematic for applications requiring non-deterministic behaviors, such as human-behavior simulation, content diversification, and multiplayer games. It also harms the diversity of generated responses, a crucial factor in test-time scaling, by causing the outputs to collapse into a limited set of answers. To address this, we propose SSoT, a simple prompting method that instructs an LLM to first output a random string to generate sufficient entropy. SSoT also instructs the LLM to extract randomness by manipulating this string to derive a final answer, thereby preserving diversity while adhering to specific constraints. We demonstrate that SSoT significantly improves the PIF performance of LLMs, approaching the ideal performance of a pseudo-random number generator. Furthermore, our experiments on NoveltyBench show SSoT's benefits extend beyond closed-set tasks to open-ended tasks by enhancing response diversity.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21150v2",
    "published_date": "2025-10-24 04:43:50 UTC",
    "updated_date": "2025-11-07 06:59:25 UTC"
  },
  {
    "arxiv_id": "2510.21148v1",
    "title": "How to Auto-optimize Prompts for Domain Tasks? Adaptive Prompting and Reasoning through Evolutionary Domain Knowledge Adaptation",
    "authors": [
      "Yang Zhao",
      "Pu Wang",
      "Hao Frank Yang"
    ],
    "abstract": "Designing optimal prompts and reasoning processes for large language models (LLMs) on domain-specific tasks is both necessary and challenging in real-world applications. Determining how to integrate domain knowledge, enhance reasoning efficiency, and even provide domain experts with refined knowledge integration hints are particularly crucial yet unresolved tasks. In this research, we propose Evolutionary Graph Optimization for Prompting (EGO-Prompt), an automated framework to designing better prompts, efficient reasoning processes and providing enhanced causal-informed process. EGO-Prompt begins with a general prompt and fault-tolerant initial Semantic Causal Graph (SCG) descriptions, constructed by human experts, which is then automatically refined and optimized to guide LLM reasoning. Recognizing that expert-defined SCGs may be partial or imperfect and that their optimal integration varies across LLMs, EGO-Prompt integrates a novel causal-guided textual gradient process in two steps: first, generating nearly deterministic reasoning guidance from the SCG for each instance, and second, adapting the LLM to effectively utilize the guidance alongside the original input. The iterative optimization algorithm further refines both the SCG and the reasoning mechanism using textual gradients with ground-truth. We tested the framework on real-world public health, transportation and human behavior tasks. EGO-Prompt achieves 7.32%-12.61% higher F1 than cutting-edge methods, and allows small models to reach the performence of larger models at under 20% of the original cost. It also outputs a refined, domain-specific SCG that improves interpretability.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21148v1",
    "published_date": "2025-10-24 04:38:53 UTC",
    "updated_date": "2025-10-24 04:38:53 UTC"
  },
  {
    "arxiv_id": "2510.21147v1",
    "title": "Hierarchical AI Multi-Agent Fundamental Investing: Evidence from China's A-Share Market",
    "authors": [
      "Chujun He",
      "Zhonghao Huang",
      "Xiangguo Li",
      "Ye Luo",
      "Kewei Ma",
      "Yuxuan Xiong",
      "Xiaowei Zhang",
      "Mingyang Zhao"
    ],
    "abstract": "We present a multi-agent, AI-driven framework for fundamental investing that integrates macro indicators, industry-level and firm-specific information to construct optimized equity portfolios. The architecture comprises: (i) a Macro agent that dynamically screens and weights sectors based on evolving economic indicators and industry performance; (ii) four firm-level agents -- Fundamental, Technical, Report, and News -- that conduct in-depth analyses of individual firms to ensure both breadth and depth of coverage; (iii) a Portfolio agent that uses reinforcement learning to combine the agent outputs into a unified policy to generate the trading strategy; and (iv) a Risk Control agent that adjusts portfolio positions in response to market volatility. We evaluate the system on the constituents by the CSI 300 Index of China's A-share market and find that it consistently outperforms standard benchmarks and a state-of-the-art multi-agent trading system on risk-adjusted returns and drawdown control. Our core contribution is a hierarchical multi-agent design that links top-down macro screening with bottom-up fundamental analysis, offering a robust and extensible approach to factor-based portfolio construction.",
    "categories": [
      "q-fin.PM",
      "cs.AI"
    ],
    "primary_category": "q-fin.PM",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21147v1",
    "published_date": "2025-10-24 04:38:37 UTC",
    "updated_date": "2025-10-24 04:38:37 UTC"
  },
  {
    "arxiv_id": "2510.21144v2",
    "title": "NeuroGenPoisoning: Neuron-Guided Attacks on Retrieval-Augmented Generation of LLM via Genetic Optimization of External Knowledge",
    "authors": [
      "Hanyu Zhu",
      "Lance Fiondella",
      "Jiawei Yuan",
      "Kai Zeng",
      "Long Jiao"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) empowers Large Language Models (LLMs) to dynamically integrate external knowledge during inference, improving their factual accuracy and adaptability. However, adversaries can inject poisoned external knowledge to override the model's internal memory. While existing attacks iteratively manipulate retrieval content or prompt structure of RAG, they largely ignore the model's internal representation dynamics and neuron-level sensitivities. The underlying mechanism of RAG poisoning has not been fully studied and the effect of knowledge conflict with strong parametric knowledge in RAG is not considered. In this work, we propose NeuroGenPoisoning, a novel attack framework that generates adversarial external knowledge in RAG guided by LLM internal neuron attribution and genetic optimization. Our method first identifies a set of Poison-Responsive Neurons whose activation strongly correlates with contextual poisoning knowledge. We then employ a genetic algorithm to evolve adversarial passages that maximally activate these neurons. Crucially, our framework enables massive-scale generation of effective poisoned RAG knowledge by identifying and reusing promising but initially unsuccessful external knowledge variants via observed attribution signals. At the same time, Poison-Responsive Neurons guided poisoning can effectively resolves knowledge conflict. Experimental results across models and datasets demonstrate consistently achieving high Population Overwrite Success Rate (POSR) of over 90% while preserving fluency. Empirical evidence shows that our method effectively resolves knowledge conflict.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21144v2",
    "published_date": "2025-10-24 04:30:49 UTC",
    "updated_date": "2026-01-11 21:36:36 UTC"
  },
  {
    "arxiv_id": "2510.21143v2",
    "title": "PanicToCalm: A Proactive Counseling Agent for Panic Attacks",
    "authors": [
      "Jihyun Lee",
      "Yejin Min",
      "San Kim",
      "Yejin Jeon",
      "SungJun Yang",
      "Hyounghun Kim",
      "Gary Geunbae Lee"
    ],
    "abstract": "Panic attacks are acute episodes of fear and distress, in which timely, appropriate intervention can significantly help individuals regain stability. However, suitable datasets for training such models remain scarce due to ethical and logistical issues. To address this, we introduce PACE, which is a dataset that includes high-distress episodes constructed from first-person narratives, and structured around the principles of Psychological First Aid (PFA). Using this data, we train PACER, a counseling model designed to provide both empathetic and directive support, which is optimized through supervised learning and simulated preference alignment. To assess its effectiveness, we propose PanicEval, a multi-dimensional framework covering general counseling quality and crisis-specific strategies. Experimental results show that PACER outperforms strong baselines in both counselor-side metrics and client affect improvement. Human evaluations further confirm its practical value, with PACER consistently preferred over general, CBT-based, and GPT-4-powered models in panic scenarios (Code is available at https://github.com/JihyunLee1/PanicToCalm ).",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted in EMNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.21143v2",
    "published_date": "2025-10-24 04:30:24 UTC",
    "updated_date": "2025-10-28 01:21:35 UTC"
  },
  {
    "arxiv_id": "2510.24767v1",
    "title": "Towards Fine-Grained Human Motion Video Captioning",
    "authors": [
      "Guorui Song",
      "Guocun Wang",
      "Zhe Huang",
      "Jing Lin",
      "Xuefei Zhe",
      "Jian Li",
      "Haoqian Wang"
    ],
    "abstract": "Generating accurate descriptions of human actions in videos remains a challenging task for video captioning models. Existing approaches often struggle to capture fine-grained motion details, resulting in vague or semantically inconsistent captions. In this work, we introduce the Motion-Augmented Caption Model (M-ACM), a novel generative framework that enhances caption quality by incorporating motion-aware decoding. At its core, M-ACM leverages motion representations derived from human mesh recovery to explicitly highlight human body dynamics, thereby reducing hallucinations and improving both semantic fidelity and spatial alignment in the generated captions. To support research in this area, we present the Human Motion Insight (HMI) Dataset, comprising 115K video-description pairs focused on human movement, along with HMI-Bench, a dedicated benchmark for evaluating motion-focused video captioning. Experimental results demonstrate that M-ACM significantly outperforms previous methods in accurately describing complex human motions and subtle temporal variations, setting a new standard for motion-centric video captioning.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.24767v1",
    "published_date": "2025-10-24 04:06:04 UTC",
    "updated_date": "2025-10-24 04:06:04 UTC"
  },
  {
    "arxiv_id": "2510.21133v1",
    "title": "Quantifying CBRN Risk in Frontier Models",
    "authors": [
      "Divyanshu Kumar",
      "Nitin Aravind Birur",
      "Tanay Baswa",
      "Sahil Agarwal",
      "Prashanth Harshangi"
    ],
    "abstract": "Frontier Large Language Models (LLMs) pose unprecedented dual-use risks through the potential proliferation of chemical, biological, radiological, and nuclear (CBRN) weapons knowledge. We present the first comprehensive evaluation of 10 leading commercial LLMs against both a novel 200-prompt CBRN dataset and a 180-prompt subset of the FORTRESS benchmark, using a rigorous three-tier attack methodology. Our findings expose critical safety vulnerabilities: Deep Inception attacks achieve 86.0\\% success versus 33.8\\% for direct requests, demonstrating superficial filtering mechanisms; Model safety performance varies dramatically from 2\\% (claude-opus-4) to 96\\% (mistral-small-latest) attack success rates; and eight models exceed 70\\% vulnerability when asked to enhance dangerous material properties. We identify fundamental brittleness in current safety alignment, where simple prompt engineering techniques bypass safeguards for dangerous CBRN information. These results challenge industry safety claims and highlight urgent needs for standardized evaluation frameworks, transparent safety metrics, and more robust alignment techniques to mitigate catastrophic misuse risks while preserving beneficial capabilities.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21133v1",
    "published_date": "2025-10-24 03:55:24 UTC",
    "updated_date": "2025-10-24 03:55:24 UTC"
  },
  {
    "arxiv_id": "2510.21131v1",
    "title": "Large Language Models Meet Text-Attributed Graphs: A Survey of Integration Frameworks and Applications",
    "authors": [
      "Guangxin Su",
      "Hanchen Wang",
      "Jianwei Wang",
      "Wenjie Zhang",
      "Ying Zhang",
      "Jian Pei"
    ],
    "abstract": "Large Language Models (LLMs) have achieved remarkable success in natural language processing through strong semantic understanding and generation. However, their black-box nature limits structured and multi-hop reasoning. In contrast, Text-Attributed Graphs (TAGs) provide explicit relational structures enriched with textual context, yet often lack semantic depth. Recent research shows that combining LLMs and TAGs yields complementary benefits: enhancing TAG representation learning and improving the reasoning and interpretability of LLMs. This survey provides the first systematic review of LLM--TAG integration from an orchestration perspective. We introduce a novel taxonomy covering two fundamental directions: LLM for TAG, where LLMs enrich graph-based tasks, and TAG for LLM, where structured graphs improve LLM reasoning. We categorize orchestration strategies into sequential, parallel, and multi-module frameworks, and discuss advances in TAG-specific pretraining, prompting, and parameter-efficient fine-tuning. Beyond methodology, we summarize empirical insights, curate available datasets, and highlight diverse applications across recommendation systems, biomedical analysis, and knowledge-intensive question answering. Finally, we outline open challenges and promising research directions, aiming to guide future work at the intersection of language and graph learning.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Surveys and overviews; Natural language processing; Knowledge representation and reasoning; Graph algorithms",
    "pdf_url": "https://arxiv.org/pdf/2510.21131v1",
    "published_date": "2025-10-24 03:53:00 UTC",
    "updated_date": "2025-10-24 03:53:00 UTC"
  },
  {
    "arxiv_id": "2510.23631v1",
    "title": "Beyond Pairwise: Empowering LLM Alignment With Ranked Choice Modeling",
    "authors": [
      "Yuxuan Tang",
      "Yifan Feng"
    ],
    "abstract": "Alignment of large language models (LLMs) has predominantly relied on pairwise preference optimization, where annotators select the better of two responses to a prompt. While simple, this approach overlooks the opportunity to learn from richer forms of human feedback, such as multiwise comparisons and top-$k$ rankings. We propose Ranked Choice Preference Optimization (RCPO), a unified framework that bridges preference optimization with (ranked) choice modeling via maximum likelihood estimation. The framework is flexible, supporting both utility-based and rank-based choice models. It subsumes several existing pairwise methods (e.g., DPO, SimPO), while providing principled training objectives for richer feedback formats. We instantiate this framework with two representative ranked choice models (Multinomial Logit and Mallows-RMJ). Empirical studies on Llama-3-8B-Instruct and Gemma-2-9B-it across AlpacaEval 2 and Arena-Hard benchmarks show that RCPO consistently outperforms competitive baselines. RCPO shows how directly leveraging ranked preference data, combined with the right choice models, yields more effective alignment. It offers a versatile and extensible foundation for incorporating (ranked) choice modeling into LLM training.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ME",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.23631v1",
    "published_date": "2025-10-24 03:48:47 UTC",
    "updated_date": "2025-10-24 03:48:47 UTC"
  },
  {
    "arxiv_id": "2510.21127v1",
    "title": "Enhanced Evolutionary Multi-Objective Deep Reinforcement Learning for Reliable and Efficient Wireless Rechargeable Sensor Networks",
    "authors": [
      "Bowei Tong",
      "Hui Kang",
      "Jiahui Li",
      "Geng Sun",
      "Jiacheng Wang",
      "Yaoqi Yang",
      "Bo Xu",
      "Dusit Niyato"
    ],
    "abstract": "Despite rapid advancements in sensor networks, conventional battery-powered sensor networks suffer from limited operational lifespans and frequent maintenance requirements that severely constrain their deployment in remote and inaccessible environments. As such, wireless rechargeable sensor networks (WRSNs) with mobile charging capabilities offer a promising solution to extend network lifetime. However, WRSNs face critical challenges from the inherent trade-off between maximizing the node survival rates and maximizing charging energy efficiency under dynamic operational conditions. In this paper, we investigate a typical scenario where mobile chargers move and charge the sensor, thereby maintaining the network connectivity while minimizing the energy waste. Specifically, we formulate a multi-objective optimization problem that simultaneously maximizes the network node survival rate and mobile charger energy usage efficiency across multiple time slots, which presents NP-hard computational complexity with long-term temporal dependencies that make traditional optimization approaches ineffective. To address these challenges, we propose an enhanced evolutionary multi-objective deep reinforcement learning algorithm, which integrates a long short-term memory (LSTM)-based policy network for temporal pattern recognition, a multilayer perceptron-based prospective increment model for future state prediction, and a time-varying Pareto policy evaluation method for dynamic preference adaptation. Extensive simulation results demonstrate that the proposed algorithm significantly outperforms existing approaches in balancing node survival rate and energy efficiency while generating diverse Pareto-optimal solutions. Moreover, the LSTM-enhanced policy network converges 25% faster than conventional networks, with the time-varying evaluation method effectively adapting to dynamic conditions.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "15 pages, 9 figures, submited to TVT",
    "pdf_url": "https://arxiv.org/pdf/2510.21127v1",
    "published_date": "2025-10-24 03:30:00 UTC",
    "updated_date": "2025-10-24 03:30:00 UTC"
  },
  {
    "arxiv_id": "2511.17526v1",
    "title": "RadioMapMotion: A Dataset and Baseline for Proactive Spatio-Temporal Radio Environment Prediction",
    "authors": [
      "Honggang Jia",
      "Nan Cheng",
      "Xiucheng Wang"
    ],
    "abstract": "Radio maps (RMs), which provide location-based pathloss estimations, are fundamental to enabling proactive, environment-aware communication in 6G networks. However, existing deep learning-based methods for RM construction often model dynamic environments as a series of independent static snapshots, thereby omitting the temporal continuity inherent in signal propagation changes caused by the motion of dynamic entities. To address this limitation, we propose the task of spatio-temporal RM prediction, which involves forecasting a sequence of future maps from historical observations. A key barrier to this predictive approach has been the lack of datasets capturing continuous environmental evolution. To fill this gap, we introduce RadioMapMotion, the first large-scale public dataset of continuous RM sequences generated from physically consistent vehicle trajectories. As a baseline for this task, we propose RadioLSTM, a UNet architecture based on Convolutional Long Short-Term Memory (ConvLSTM) and designed for multi-step sequence forecasting. Experimental evaluations show that RadioLSTM achieves higher prediction accuracy and structural fidelity compared to representative baseline methods. Furthermore, the model exhibits a low inference latency, indicating its potential suitability for real-time network operations. Our project will be publicly released at: https://github.com/UNIC-Lab/RadioMapMotion upon paper acceptance.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.17526v1",
    "published_date": "2025-10-24 03:25:25 UTC",
    "updated_date": "2025-10-24 03:25:25 UTC"
  },
  {
    "arxiv_id": "2510.21891v1",
    "title": "Embedding Trust: Semantic Isotropy Predicts Nonfactuality in Long-Form Text Generation",
    "authors": [
      "Dhrupad Bhardwaj",
      "Julia Kempe",
      "Tim G. J. Rudner"
    ],
    "abstract": "To deploy large language models (LLMs) in high-stakes application domains that require substantively accurate responses to open-ended prompts, we need reliable, computationally inexpensive methods that assess the trustworthiness of long-form responses generated by LLMs. However, existing approaches often rely on claim-by-claim fact-checking, which is computationally expensive and brittle in long-form responses to open-ended prompts. In this work, we introduce semantic isotropy -- the degree of uniformity across normalized text embeddings on the unit sphere -- and use it to assess the trustworthiness of long-form responses generated by LLMs. To do so, we generate several long-form responses, embed them, and estimate the level of semantic isotropy of these responses as the angular dispersion of the embeddings on the unit sphere. We find that higher semantic isotropy -- that is, greater embedding dispersion -- reliably signals lower factual consistency across samples. Our approach requires no labeled data, no fine-tuning, and no hyperparameter selection, and can be used with open- or closed-weight embedding models. Across multiple domains, our method consistently outperforms existing approaches in predicting nonfactuality in long-form responses using only a handful of samples -- offering a practical, low-cost approach for integrating trust assessment into real-world LLM workflows.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "stat.ME",
      "stat.ML"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21891v1",
    "published_date": "2025-10-24 03:24:57 UTC",
    "updated_date": "2025-10-24 03:24:57 UTC"
  },
  {
    "arxiv_id": "2510.21121v1",
    "title": "Generalizable Hierarchical Skill Learning via Object-Centric Representation",
    "authors": [
      "Haibo Zhao",
      "Yu Qi",
      "Boce Hu",
      "Yizhe Zhu",
      "Ziyan Chen",
      "Heng Tian",
      "Xupeng Zhu",
      "Owen Howell",
      "Haojie Huang",
      "Robin Walters",
      "Dian Wang",
      "Robert Platt"
    ],
    "abstract": "We present Generalizable Hierarchical Skill Learning (GSL), a novel framework for hierarchical policy learning that significantly improves policy generalization and sample efficiency in robot manipulation. One core idea of GSL is to use object-centric skills as an interface that bridges the high-level vision-language model and the low-level visual-motor policy. Specifically, GSL decomposes demonstrations into transferable and object-canonicalized skill primitives using foundation models, ensuring efficient low-level skill learning in the object frame. At test time, the skill-object pairs predicted by the high-level agent are fed to the low-level module, where the inferred canonical actions are mapped back to the world frame for execution. This structured yet flexible design leads to substantial improvements in sample efficiency and generalization of our method across unseen spatial arrangements, object appearances, and task compositions. In simulation, GSL trained with only 3 demonstrations per task outperforms baselines trained with 30 times more data by 15.5 percent on unseen tasks. In real-world experiments, GSL also surpasses the baseline trained with 10 times more data.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21121v1",
    "published_date": "2025-10-24 03:21:42 UTC",
    "updated_date": "2025-10-24 03:21:42 UTC"
  },
  {
    "arxiv_id": "2510.21118v4",
    "title": "The Gray Zone of Faithfulness: Taming Ambiguity in Unfaithfulness Detection",
    "authors": [
      "Qiang Ding",
      "Lvzhou Luo",
      "Yixuan Cao",
      "Ping Luo"
    ],
    "abstract": "Ensuring that Large Language Models (LLMs) generate summaries faithful to a given source document is essential for real-world applications. While prior research has explored LLM faithfulness, existing benchmarks suffer from annotation ambiguity, primarily due to the ill-defined boundary of permissible external knowledge in generated outputs. For instance, common sense is often incorporated into responses and labeled as \"faithful\", yet the acceptable extent of such knowledge remains unspecified, leading to inconsistent annotations. To address this issue, we propose a novel faithfulness annotation framework, which introduces an intermediate category, Out-Dependent, to classify cases where external knowledge is required for verification. Using this framework, we construct VeriGray (Verification with the Gray Zone) -- a new unfaithfulness detection benchmark in summarization. Statistics reveal that even SOTA LLMs, such as GPT-5, exhibit hallucinations ($\\sim 6\\%$ of sentences) in summarization tasks. Moreover, a substantial proportion ($\\sim 9\\%$ on average of models) of generated sentences fall into the Out-Dependent category, underscoring the importance of resolving annotation ambiguity in unfaithfulness detection benchmarks. Experiments demonstrate that our benchmark poses significant challenges to multiple baseline methods, indicating considerable room for future improvement.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Update the evaluation results due to the annotation updates; revise the citation to Seo et al., 2025; add the acknowledgements",
    "pdf_url": "https://arxiv.org/pdf/2510.21118v4",
    "published_date": "2025-10-24 03:13:51 UTC",
    "updated_date": "2025-12-29 02:55:05 UTC"
  },
  {
    "arxiv_id": "2510.21117v2",
    "title": "DAO-AI: Evaluating Collective Decision-Making through Agentic AI in Decentralized Governance",
    "authors": [
      "Agostino Capponi",
      "Alfio Gliozzo",
      "Chunghyun Han",
      "Junkyu Lee"
    ],
    "abstract": "This paper presents a first empirical study of agentic AI as autonomous decision-makers in decentralized governance. Using more than 3K proposals from major protocols, we build an agentic AI voter that interprets proposal contexts, retrieves historical deliberation data, and independently determines its voting position. The agent operates within a realistic financial simulation environment grounded in verifiable blockchain data, implemented through a modular composable program (MCP) workflow that defines data flow and tool usage via Agentics framework. We evaluate how closely the agent's decisions align with the human and token-weighted outcomes, uncovering strong alignments measured by carefully designed evaluation metrics. Our findings demonstrate that agentic AI can augment collective decision-making by producing interpretable, auditable, and empirically grounded signals in realistic DAO governance settings. The study contributes to the design of explainable and economically rigorous AI agents for decentralized financial systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "12 pages, 2 Figures",
    "pdf_url": "https://arxiv.org/pdf/2510.21117v2",
    "published_date": "2025-10-24 03:13:14 UTC",
    "updated_date": "2025-10-27 01:36:39 UTC"
  },
  {
    "arxiv_id": "2510.21112v1",
    "title": "Urban 3D Change Detection Using LiDAR Sensor for HD Map Maintenance and Smart Mobility",
    "authors": [
      "Hezam Albagami",
      "Haitian Wang",
      "Xinyu Wang",
      "Muhammad Ibrahim",
      "Zainy M. Malakan",
      "Abdullah M. Alqamdi",
      "Mohammed H. Alghamdi",
      "Ajmal Mian"
    ],
    "abstract": "High-definition 3D city maps underpin smart transportation, digital twins, and autonomous driving, where object level change detection across bi temporal LiDAR enables HD map maintenance, construction monitoring, and reliable localization. Classical DSM differencing and image based methods are sensitive to small vertical bias, ground slope, and viewpoint mismatch and yield cellwise outputs without object identity. Point based neural models and voxel encodings demand large memory, assume near perfect pre alignment, degrade thin structures, and seldom enforce class consistent association, which leaves split or merge cases unresolved and ignores uncertainty. We propose an object centric, uncertainty aware pipeline for city scale LiDAR that aligns epochs with multi resolution NDT followed by point to plane ICP, normalizes height, and derives a per location level of detection from registration covariance and surface roughness to calibrate decisions and suppress spurious changes. Geometry only proxies seed cross epoch associations that are refined by semantic and instance segmentation and a class constrained bipartite assignment with augmented dummies to handle splits and merges while preserving per class counts. Tiled processing bounds memory without eroding narrow ground changes, and instance level decisions combine 3D overlap, normal direction displacement, and height and volume differences with a histogram distance, all gated by the local level of detection to remain stable under partial overlap and sampling variation. On 15 representative Subiaco blocks the method attains 95.2% accuracy, 90.4% mF1, and 82.6% mIoU, exceeding Triplet KPConv by 0.2 percentage points in accuracy, 0.2 in mF1, and 0.8 in mIoU, with the largest gain on Decreased where IoU reaches 74.8% and improves by 7.6 points.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21112v1",
    "published_date": "2025-10-24 02:59:55 UTC",
    "updated_date": "2025-10-24 02:59:55 UTC"
  },
  {
    "arxiv_id": "2510.21110v1",
    "title": "Confounding Robust Deep Reinforcement Learning: A Causal Approach",
    "authors": [
      "Mingxuan Li",
      "Junzhe Zhang",
      "Elias Bareinboim"
    ],
    "abstract": "A key task in Artificial Intelligence is learning effective policies for controlling agents in unknown environments to optimize performance measures. Off-policy learning methods, like Q-learning, allow learners to make optimal decisions based on past experiences. This paper studies off-policy learning from biased data in complex and high-dimensional domains where \\emph{unobserved confounding} cannot be ruled out a priori. Building on the well-celebrated Deep Q-Network (DQN), we propose a novel deep reinforcement learning algorithm robust to confounding biases in observed data. Specifically, our algorithm attempts to find a safe policy for the worst-case environment compatible with the observations. We apply our method to twelve confounded Atari games, and find that it consistently dominates the standard DQN in all games where the observed input to the behavioral and target policies mismatch and unobserved confounders exist.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.21110v1",
    "published_date": "2025-10-24 02:58:01 UTC",
    "updated_date": "2025-10-24 02:58:01 UTC"
  },
  {
    "arxiv_id": "2510.23630v1",
    "title": "NUM2EVENT: Interpretable Event Reasoning from Numerical time-series",
    "authors": [
      "Ninghui Feng",
      "Yiyan Qi"
    ],
    "abstract": "Large language models (LLMs) have recently demonstrated impressive multimodal reasoning capabilities, yet their understanding of purely numerical time-series signals remains limited. Existing approaches mainly focus on forecasting or trend description, without uncovering the latent events that drive numerical changes or explaining the reasoning process behind them. In this work, we introduce the task of number-to-event reasoning and decoding, which aims to infer interpretable structured events from numerical inputs, even when current text is unavailable. To address the data scarcity and semantic alignment challenges, we propose a reasoning-aware framework that integrates an agent-guided event extractor (AGE), a marked multivariate Hawkes-based synthetic generator (EveDTS), and a two-stage fine-tuning pipeline combining a time-series encoder with a structured decoder. Our model explicitly reasons over numerical changes, generates intermediate explanations, and outputs structured event hypotheses. Experiments on multi-domain datasets show that our method substantially outperforms strong LLM baselines in event-level precision and recall. These results suggest a new direction for bridging quantitative reasoning and semantic understanding, enabling LLMs to explain and predict events directly from numerical dynamics.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.23630v1",
    "published_date": "2025-10-24 02:57:11 UTC",
    "updated_date": "2025-10-24 02:57:11 UTC"
  },
  {
    "arxiv_id": "2510.21107v1",
    "title": "ESCORT: Efficient Stein-variational and Sliced Consistency-Optimized Temporal Belief Representation for POMDPs",
    "authors": [
      "Yunuo Zhang",
      "Baiting Luo",
      "Ayan Mukhopadhyay",
      "Gabor Karsai",
      "Abhishek Dubey"
    ],
    "abstract": "In Partially Observable Markov Decision Processes (POMDPs), maintaining and updating belief distributions over possible underlying states provides a principled way to summarize action-observation history for effective decision-making under uncertainty. As environments grow more realistic, belief distributions develop complexity that standard mathematical models cannot accurately capture, creating a fundamental challenge in maintaining representational accuracy. Despite advances in deep learning and probabilistic modeling, existing POMDP belief approximation methods fail to accurately represent complex uncertainty structures such as high-dimensional, multi-modal belief distributions, resulting in estimation errors that lead to suboptimal agent behaviors. To address this challenge, we present ESCORT (Efficient Stein-variational and sliced Consistency-Optimized Representation for Temporal beliefs), a particle-based framework for capturing complex, multi-modal distributions in high-dimensional belief spaces. ESCORT extends SVGD with two key innovations: correlation-aware projections that model dependencies between state dimensions, and temporal consistency constraints that stabilize updates while preserving correlation structures. This approach retains SVGD's attractive-repulsive particle dynamics while enabling accurate modeling of intricate correlation patterns. Unlike particle filters prone to degeneracy or parametric methods with fixed representational capacity, ESCORT dynamically adapts to belief landscape complexity without resampling or restrictive distributional assumptions. We demonstrate ESCORT's effectiveness through extensive evaluations on both POMDP domains and synthetic multi-modal distributions of varying dimensionality, where it consistently outperforms state-of-the-art methods in terms of belief approximation accuracy and downstream decision quality.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "Proceeding of the 39th Conference on Neural Information Processing Systems (NeurIPS'25). Code would be available at https://github.com/scope-lab-vu/ESCORT",
    "pdf_url": "https://arxiv.org/pdf/2510.21107v1",
    "published_date": "2025-10-24 02:51:33 UTC",
    "updated_date": "2025-10-24 02:51:33 UTC"
  },
  {
    "arxiv_id": "2510.21890v1",
    "title": "The Principles of Diffusion Models",
    "authors": [
      "Chieh-Hsin Lai",
      "Yang Song",
      "Dongjun Kim",
      "Yuki Mitsufuji",
      "Stefano Ermon"
    ],
    "abstract": "This monograph presents the core principles that have guided the development of diffusion models, tracing their origins and showing how diverse formulations arise from shared mathematical ideas. Diffusion modeling starts by defining a forward process that gradually corrupts data into noise, linking the data distribution to a simple prior through a continuum of intermediate distributions. The goal is to learn a reverse process that transforms noise back into data while recovering the same intermediates. We describe three complementary views. The variational view, inspired by variational autoencoders, sees diffusion as learning to remove noise step by step. The score-based view, rooted in energy-based modeling, learns the gradient of the evolving data distribution, indicating how to nudge samples toward more likely regions. The flow-based view, related to normalizing flows, treats generation as following a smooth path that moves samples from noise to data under a learned velocity field. These perspectives share a common backbone: a time-dependent velocity field whose flow transports a simple prior to the data. Sampling then amounts to solving a differential equation that evolves noise into data along a continuous trajectory. On this foundation, the monograph discusses guidance for controllable generation, efficient numerical solvers, and diffusion-motivated flow-map models that learn direct mappings between arbitrary times. It provides a conceptual and mathematically grounded understanding of diffusion models for readers with basic deep-learning knowledge.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21890v1",
    "published_date": "2025-10-24 02:29:02 UTC",
    "updated_date": "2025-10-24 02:29:02 UTC"
  },
  {
    "arxiv_id": "2510.23629v1",
    "title": "Chain of Execution Supervision Promotes General Reasoning in Large Language Models",
    "authors": [
      "Nuo Chen",
      "Zehua Li",
      "Keqin Bao",
      "Junyang Lin",
      "Dayiheng Liu"
    ],
    "abstract": "Building robust and general reasoning ability is a central goal in the development of large language models (LLMs). Recent efforts increasingly turn to code as a rich training source, given its inherent logical structure and diverse reasoning paradigms such as divide-and-conquer, topological ordering, and enumeration. However, reasoning in code is often expressed implicitly and entangled with syntactic or implementation noise, making direct training on raw code suboptimal.To address this, we introduce TracePile, a large-scale corpus of 2.6 million samples that transforms code execution into explicit, step-by-step chain-of-thought-style rationales, which we call Chain of Execution (CoE). The corpus spans domains including mathematics, classical algorithms and algorithmic competition, and is enriched with variable-tracing questions and code rewritings to enhance logical granularity and code diversity. We evaluate TracePile using three training setups: continue-pretraining, instruction tuning after pretraining, and two-stage finetuning. Experiments across four base models (LLaMA 3, LLaMA 3.1, Qwen-2.5, and Qwen-2.5 Coder) and 20 benchmarks covering math, code, logic, and algorithms demonstrate consistent improvements. Notably, TracePile boosts LLaMA3.1-8B by 7.1\\% on average across nine math datasets and delivers clear gains on LiveCodeBench, CRUX, and MMLU under two-stage fine-tuning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.PL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.23629v1",
    "published_date": "2025-10-24 02:21:11 UTC",
    "updated_date": "2025-10-24 02:21:11 UTC"
  },
  {
    "arxiv_id": "2510.21093v1",
    "title": "MedAlign: A Synergistic Framework of Multimodal Preference Optimization and Federated Meta-Cognitive Reasoning",
    "authors": [
      "Siyong Chen",
      "Jinbo Wen",
      "Jiawen Kang",
      "Tenghui Huang",
      "Xumin Huang",
      "Yuanjia Su",
      "Hudan Pan",
      "Zishao Zhong",
      "Dusit Niyato",
      "Shengli Xie",
      "Dong In Kim"
    ],
    "abstract": "Recently, large models have shown significant potential for smart healthcare. However, the deployment of Large Vision-Language Models (LVLMs) for clinical services is currently hindered by three critical challenges: a tendency to hallucinate answers not grounded in visual evidence, the inefficiency of fixed-depth reasoning, and the difficulty of multi-institutional collaboration. To address these challenges, in this paper, we develop MedAlign, a novel framework to ensure visually accurate LVLM responses for Medical Visual Question Answering (Med-VQA). Specifically, we first propose a multimodal Direct Preference Optimization (mDPO) objective to explicitly align preference learning with visual context. We then design a Retrieval-Aware Mixture-of-Experts (RA-MoE) architecture that utilizes image and text similarity to route queries to a specialized and context-augmented LVLM (i.e., an expert), thereby mitigating hallucinations in LVLMs. To achieve adaptive reasoning and facilitate multi-institutional collaboration, we propose a federated governance mechanism, where the selected expert, fine-tuned on clinical datasets based on mDPO, locally performs iterative Chain-of-Thought (CoT) reasoning via the local meta-cognitive uncertainty estimator. Extensive experiments on three representative Med-VQA datasets demonstrate that MedAlign achieves state-of-the-art performance, outperforming strong retrieval-augmented baselines by up to $11.85\\%$ in F1-score, and simultaneously reducing the average reasoning length by $51.60\\%$ compared with fixed-depth CoT approaches.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21093v1",
    "published_date": "2025-10-24 02:11:05 UTC",
    "updated_date": "2025-10-24 02:11:05 UTC"
  },
  {
    "arxiv_id": "2510.21090v1",
    "title": "Self-Rewarding PPO: Aligning Large Language Models with Demonstrations Only",
    "authors": [
      "Qingru Zhang",
      "Liang Qiu",
      "Ilgee Hong",
      "Zhenghao Xu",
      "Tianyi Liu",
      "Shiyang Li",
      "Rongzhi Zhang",
      "Zheng Li",
      "Lihong Li",
      "Bing Yin",
      "Chao Zhang",
      "Jianshu Chen",
      "Haoming Jiang",
      "Tuo Zhao"
    ],
    "abstract": "Supervised fine-tuning (SFT) has emerged as a crucial method for aligning large language models (LLMs) with human-annotated demonstrations. However, SFT, being an off-policy approach similar to behavior cloning, often struggles with overfitting and poor out-of-domain generalization, especially in limited-data scenarios. To address these limitations, we propose Self-Rewarding PPO, a novel fine-tuning method that leverages on-policy techniques to enhance generalization performance. Our approach combines the strengths of SFT and proximal policy optimization (PPO) to achieve more effective alignment from demonstration data. At its core is a reward function designed as the log policy ratio between the SFT model and the pretrained base model. This function serves as an implicit reward signal, using the pretrained policy as a baseline and the SFT policy as a target. By doing so, it enables on-policy fine-tuning without relying on human preference annotations. The integration of this self-rewarding mechanism with PPO addresses key limitations of SFT, improving generalization, data efficiency, and robustness. Our empirical evaluation across a range of natural language processing tasks demonstrates that Self-Rewarding PPO consistently outperforms traditional SFT methods. The results highlight the effectiveness of our approach in aligning LLMs using demonstration data, particularly in scenarios where high-quality annotated data is scarce.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by COLM 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.21090v1",
    "published_date": "2025-10-24 02:02:13 UTC",
    "updated_date": "2025-10-24 02:02:13 UTC"
  },
  {
    "arxiv_id": "2510.21088v1",
    "title": "M-GLC: Motif-Driven Global-Local Context Graphs for Few-shot Molecular Property Prediction",
    "authors": [
      "Xiangyang Xu",
      "Hongyang Gao"
    ],
    "abstract": "Molecular property prediction (MPP) is a cornerstone of drug discovery and materials science, yet conventional deep learning approaches depend on large labeled datasets that are often unavailable. Few-shot Molecular property prediction (FSMPP) addresses this scarcity by incorporating relational inductive bias through a context graph that links molecule nodes to property nodes, but such molecule-property graphs offer limited structural guidance. We propose a comprehensive solution: Motif Driven Global-Local Context Graph for few-shot molecular property prediction, which enriches contextual information at both the global and local levels. At the global level, chemically meaningful motif nodes representing shared substructures, such as rings or functional groups, are introduced to form a global tri-partite heterogeneous graph, yielding motif-molecule-property connections that capture long-range compositional patterns and enable knowledge transfer among molecules with common motifs. At the local level, we build a subgraph for each node in the molecule-property pair and encode them separately to concentrate the model's attention on the most informative neighboring molecules and motifs. Experiments on five standard FSMPP benchmarks demonstrate that our framework consistently outperforms state-of-the-art methods. These results underscore the effectiveness of integrating global motif knowledge with fine-grained local context to advance robust few-shot molecular property prediction.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21088v1",
    "published_date": "2025-10-24 02:00:41 UTC",
    "updated_date": "2025-10-24 02:00:41 UTC"
  },
  {
    "arxiv_id": "2510.21084v2",
    "title": "Chinese Discharge Drug Recommendation in Metabolic Diseases with Large Language Models",
    "authors": [
      "Juntao Li",
      "Haobin Yuan",
      "Ling Luo",
      "Yan Jiang",
      "Fan Wang",
      "Ping Zhang",
      "Huiyi Lv",
      "Jian Wang",
      "Yuanyuan Sun",
      "Hongfei Lin"
    ],
    "abstract": "Intelligent drug recommendation based on Electronic Health Records (EHRs) is critical for improving the quality and efficiency of clinical decision-making. By leveraging large-scale patient data, drug recommendation systems can assist physicians in selecting the most appropriate medications according to a patient's medical history, diagnoses, laboratory results, and comorbidities. Recent advances in large language models (LLMs) have shown remarkable capabilities in complex reasoning and medical text understanding, making them promising tools for drug recommendation tasks. However, the application of LLMs for Chinese clinical medication recommendation remains largely unexplored. In this work, we conduct a systematic investigation of LLM-based methodologies for Chinese discharge medication recommendation. We evaluate several representative LLM families (GLM, Llama, Qwen) under a unified methodological framework including zero-shot prompting, in-context learning, chain-of-thought prompting, and supervised fine-tuning using LoRA. We analyze model behavior across reasoning styles, error patterns, domain adaptation mechanisms, and robustness. Experimental results show that while supervised fine-tuning improves model performance, there remains substantial room for improvement, with the best model achieving the F1 score of 0.5648 and Jaccard score of 0.4477. Our findings highlight both the potential and limitations of LLMs for Chinese drug recommendation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21084v2",
    "published_date": "2025-10-24 01:47:23 UTC",
    "updated_date": "2025-12-05 09:12:05 UTC"
  },
  {
    "arxiv_id": "2511.02846v1",
    "title": "Spatio-Temporal Attention Network for Epileptic Seizure Prediction",
    "authors": [
      "Zan Li",
      "Kyongmin Yeo",
      "Wesley Gifford",
      "Lara Marcuse",
      "Madeline Fields",
      "Bülent Yener"
    ],
    "abstract": "In this study, we present a deep learning framework that learns complex spatio-temporal correlation structures of EEG signals through a Spatio-Temporal Attention Network (STAN) for accurate predictions of onset of seizures for Epilepsy patients. Unlike existing methods, which rely on feature engineering and/or assume fixed preictal durations, our approach simultaneously models spatio-temporal correlations through STAN and employs an adversarial discriminator to distinguish preictal from interictal attention patterns, enabling patient-specific learning. Evaluation on CHB-MIT and MSSM datasets demonstrates 96.6\\% sensitivity with 0.011/h false detection rate on CHB-MIT, and 94.2% sensitivity with 0.063/h FDR on MSSM, significantly outperforming state-of-the-art methods. The framework reliably detects preictal states at least 15 minutes before an onset, with patient-specific windows extending to 45 minutes, providing sufficient intervention time for clinical applications.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.02846v1",
    "published_date": "2025-10-24 01:45:25 UTC",
    "updated_date": "2025-10-24 01:45:25 UTC"
  },
  {
    "arxiv_id": "2510.21082v1",
    "title": "Soppia: A Structured Prompting Framework for the Proportional Assessment of Non-Pecuniary Damages in Personal Injury Cases",
    "authors": [
      "Jorge Alberto Araujo"
    ],
    "abstract": "Applying complex legal rules characterized by multiple, heterogeneously weighted criteria presents a fundamental challenge in judicial decision-making, often hindering the consistent realization of legislative intent. This challenge is particularly evident in the quantification of non-pecuniary damages in personal injury cases. This paper introduces Soppia, a structured prompting framework designed to assist legal professionals in navigating this complexity. By leveraging advanced AI, the system ensures a comprehensive and balanced analysis of all stipulated criteria, fulfilling the legislator's intent that compensation be determined through a holistic assessment of each case. Using the twelve criteria for non-pecuniary damages established in the Brazilian CLT (Art. 223-G) as a case study, we demonstrate how Soppia (System for Ordered Proportional and Pondered Intelligent Assessment) operationalizes nuanced legal commands into a practical, replicable, and transparent methodology. The framework enhances consistency and predictability while providing a versatile and explainable tool adaptable across multi-criteria legal contexts, bridging normative interpretation and computational reasoning toward auditable legal AI.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "9 pages, 2 tables, includes GitHub link to framework implementation. Submitted to the Artificial Intelligence and Law section of arXiv",
    "pdf_url": "https://arxiv.org/pdf/2510.21082v1",
    "published_date": "2025-10-24 01:42:38 UTC",
    "updated_date": "2025-10-24 01:42:38 UTC"
  },
  {
    "arxiv_id": "2511.00020v1",
    "title": "Multimodal Detection of Fake Reviews using BERT and ResNet-50",
    "authors": [
      "Suhasnadh Reddy Veluru",
      "Sai Teja Erukude",
      "Viswa Chaitanya Marella"
    ],
    "abstract": "In the current digital commerce landscape, user-generated reviews play a critical role in shaping consumer behavior, product reputation, and platform credibility. However, the proliferation of fake or misleading reviews often generated by bots, paid agents, or AI models poses a significant threat to trust and transparency within review ecosystems. Existing detection models primarily rely on unimodal, typically textual, data and therefore fail to capture semantic inconsistencies across different modalities. To address this gap, a robust multimodal fake review detection framework is proposed, integrating textual features encoded with BERT and visual features extracted using ResNet-50. These representations are fused through a classification head to jointly predict review authenticity. To support this approach, a curated dataset comprising 21,142 user-uploaded images across food delivery, hospitality, and e-commerce domains was utilized. Experimental results indicate that the multimodal model outperforms unimodal baselines, achieving an F1-score of 0.934 on the test set. Additionally, the confusion matrix and qualitative analysis highlight the model's ability to detect subtle inconsistencies, such as exaggerated textual praise paired with unrelated or low-quality images, commonly found in deceptive content. This study demonstrates the critical role of multimodal learning in safeguarding digital trust and offers a scalable solution for content moderation across various online platforms.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "Published in IEEE",
    "pdf_url": "https://arxiv.org/pdf/2511.00020v1",
    "published_date": "2025-10-24 01:24:53 UTC",
    "updated_date": "2025-10-24 01:24:53 UTC"
  },
  {
    "arxiv_id": "2510.21888v1",
    "title": "Computational Hardness of Reinforcement Learning with Partial $q^π$-Realizability",
    "authors": [
      "Shayan Karimi",
      "Xiaoqi Tan"
    ],
    "abstract": "This paper investigates the computational complexity of reinforcement learning in a novel linear function approximation regime, termed partial $q^π$-realizability. In this framework, the objective is to learn an $ε$-optimal policy with respect to a predefined policy set $Π$, under the assumption that all value functions for policies in $Π$ are linearly realizable. The assumptions of this framework are weaker than those in $q^π$-realizability but stronger than those in $q^*$-realizability, providing a practical model where function approximation naturally arises. We prove that learning an $ε$-optimal policy in this setting is computationally hard. Specifically, we establish NP-hardness under a parameterized greedy policy set (argmax) and show that - unless NP = RP - an exponential lower bound (in feature vector dimension) holds when the policy set contains softmax policies, under the Randomized Exponential Time Hypothesis. Our hardness results mirror those in $q^*$-realizability and suggest computational difficulty persists even when $Π$ is expanded beyond the optimal policy. To establish this, we reduce from two complexity problems, $δ$-Max-3SAT and $δ$-Max-3SAT(b), to instances of GLinear-$κ$-RL (greedy policy) and SLinear-$κ$-RL (softmax policy). Our findings indicate that positive computational results are generally unattainable in partial $q^π$-realizability, in contrast to $q^π$-realizability under a generative access model.",
    "categories": [
      "cs.AI",
      "cs.CC",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "to be published in NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.21888v1",
    "published_date": "2025-10-24 01:18:49 UTC",
    "updated_date": "2025-10-24 01:18:49 UTC"
  },
  {
    "arxiv_id": "2510.21068v1",
    "title": "Bridging Language Gaps with Adaptive RAG: Improving Indonesian Language Question Answering",
    "authors": [
      "William Christian",
      "Daniel Adamlu",
      "Adrian Yu",
      "Derwin Suhartono"
    ],
    "abstract": "Question Answering (QA) has seen significant improvements with the advancement of machine learning models, further studies enhanced this question answering system by retrieving external information, called Retrieval-Augmented Generation (RAG) to produce more accurate and informative answers. However, these state-of-the-art-performance is predominantly in English language. To address this gap we made an effort of bridging language gaps by incorporating Adaptive RAG system to Indonesian language. Adaptive RAG system integrates a classifier whose task is to distinguish the question complexity, which in turn determines the strategy for answering the question. To overcome the limited availability of Indonesian language dataset, our study employs machine translation as data augmentation approach. Experiments show reliable question complexity classifier; however, we observed significant inconsistencies in multi-retrieval answering strategy which negatively impacted the overall evaluation when this strategy was applied. These findings highlight both the promise and challenges of question answering in low-resource language suggesting directions for future improvement.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages, 7 figures, 5 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.21068v1",
    "published_date": "2025-10-24 00:50:20 UTC",
    "updated_date": "2025-10-24 00:50:20 UTC"
  },
  {
    "arxiv_id": "2510.21063v1",
    "title": "Deep learning-based automated damage detection in concrete structures using images from earthquake events",
    "authors": [
      "Abdullah Turer",
      "Yongsheng Bai",
      "Halil Sezen",
      "Alper Yilmaz"
    ],
    "abstract": "Timely assessment of integrity of structures after seismic events is crucial for public safety and emergency response. This study focuses on assessing the structural damage conditions using deep learning methods to detect exposed steel reinforcement in concrete buildings and bridges after large earthquakes. Steel bars are typically exposed after concrete spalling or large flexural or shear cracks. The amount and distribution of exposed steel reinforcement is an indication of structural damage and degradation. To automatically detect exposed steel bars, new datasets of images collected after the 2023 Turkey Earthquakes were labeled to represent a wide variety of damaged concrete structures. The proposed method builds upon a deep learning framework, enhanced with fine-tuning, data augmentation, and testing on public datasets. An automated classification framework is developed that can be used to identify inside/outside buildings and structural components. Then, a YOLOv11 (You Only Look Once) model is trained to detect cracking and spalling damage and exposed bars. Another YOLO model is finetuned to distinguish different categories of structural damage levels. All these trained models are used to create a hybrid framework to automatically and reliably determine the damage levels from input images. This research demonstrates that rapid and automated damage detection following disasters is achievable across diverse damage contexts by utilizing image data collection, annotation, and deep learning approaches.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "6 pages, 1 figure",
    "pdf_url": "https://arxiv.org/pdf/2510.21063v1",
    "published_date": "2025-10-24 00:35:14 UTC",
    "updated_date": "2025-10-24 00:35:14 UTC"
  },
  {
    "arxiv_id": "2510.21060v2",
    "title": "On the Sample Complexity of Differentially Private Policy Optimization",
    "authors": [
      "Yi He",
      "Xingyu Zhou"
    ],
    "abstract": "Policy optimization (PO) is a cornerstone of modern reinforcement learning (RL), with diverse applications spanning robotics, healthcare, and large language model training. The increasing deployment of PO in sensitive domains, however, raises significant privacy concerns. In this paper, we initiate a theoretical study of differentially private policy optimization, focusing explicitly on its sample complexity. We first formalize an appropriate definition of differential privacy (DP) tailored to PO, addressing the inherent challenges arising from on-policy learning dynamics and the subtlety involved in defining the unit of privacy. We then systematically analyze the sample complexity of widely-used PO algorithms, including policy gradient (PG), natural policy gradient (NPG) and more, under DP constraints and various settings, via a unified framework. Our theoretical results demonstrate that privacy costs can often manifest as lower-order terms in the sample complexity, while also highlighting subtle yet important observations in private PO settings. These offer valuable practical insights for privacy-preserving PO algorithms.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.21060v2",
    "published_date": "2025-10-24 00:21:38 UTC",
    "updated_date": "2026-01-13 04:00:52 UTC"
  }
]
{
  "date": "2025-09-23",
  "category": "cs.AI",
  "summary": "ä½ å¥½ï¼æˆ‘æ˜¯ Gemini Enterpriseã€‚ä½œä¸ºä½ çš„ä¸“å±å­¦æœ¯ç ”ç©¶åŠ©ç†ï¼Œæˆ‘é€šè¯»äº†ä»Šæ—¥æ›´æ–°çš„ 181 ç¯‡ arXiv è®ºæ–‡ã€‚ä»Šå¤©çš„åˆ—è¡¨éå¸¸åºå¤§ï¼Œæ¶µç›–äº†ä»åŸºç¡€å¤§æ¨¡å‹æ¶æ„ä¼˜åŒ–ã€æ¨ç†èƒ½åŠ›å¢å¼ºï¼Œåˆ° AI å®‰å…¨ï¼ˆç‰¹åˆ«æ˜¯æ¬ºéª—è¡Œä¸ºï¼‰ã€å¤šæ¨¡æ€äº¤äº’ä»¥åŠ AI for Science çš„å¹¿æ³›è®®é¢˜ã€‚\n\nä»¥ä¸‹æ˜¯ä¸ºæ‚¨ç²¾å¿ƒæ•´ç†çš„ **2025-09-23 arXiv ä¸­æ–‡ TLDR å¿«æŠ¥**ã€‚\n\n---\n\n# æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-09-23 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\n**ä»Šæ—¥æ€»ç»“ï¼š** ä»Šå¤©çš„ arXiv å……æ»¡ç€å¯¹ LLM **æ¨ç†èƒ½åŠ›è¾¹ç•Œ**çš„æ¢ç´¢ï¼ˆå¦‚ continuous CoTã€æµ‹è¯•æ—¶æ‰©å±•ï¼‰å’Œå¯¹ **AI å®‰å…¨æ€§**çš„æ·±åº¦åæ€ï¼ˆå¦‚ Agent çš„æˆ˜ç•¥æ€§æ¬ºéª—ã€å¹»è§‰åˆ†ç±»ï¼‰ã€‚é‡ç£…å‘å¸ƒåŒ…æ‹¬ç¾å›¢ LongCat å›¢é˜Ÿå¼€æºçš„ **560B MoE æ¨ç†æ¨¡å‹**ï¼Œä»¥åŠå¤šç¯‡å…³äº AI åœ¨**ç¥ç»å½±åƒ**å’Œ**è¯ç‰©å‘ç°**é¢†åŸŸçš„ Foundation Model å·¥ä½œã€‚\n\n---\n\n## ğŸš€ é‡ç£…æ¨¡å‹ä¸æ¶æ„åˆ›æ–° (LLM Reasoning & Architecture)\n\n**1. [å¼€æºå‘å¸ƒ] LongCat-Flash-Thinkingï¼šä¸€ä»½å…³äº 5600 äº¿å‚æ•° MoE æ¨ç†æ¨¡å‹çš„æŠ€æœ¯æŠ¥å‘Š**\n**LongCat-Flash-Thinking: A Technical Report**\n> **Authors:** Meituan LongCat Team et al.\n> **æ ¸å¿ƒæœ¯è¯­:** Mixture-of-Experts (MoE), RLHF, Chain-of-Thought (CoT), Domain-Parallel Training\n\nè¿™æ˜¯ä¸€ç¯‡é‡é‡çº§çš„æŠ€æœ¯æŠ¥å‘Šã€‚ç¾å›¢ LongCat å›¢é˜Ÿå‘å¸ƒäº† **LongCat-Flash-Thinking**ï¼Œè¿™æ˜¯ä¸€ä¸ª 560B å‚æ•°çš„å¼€æº MoE æ¨ç†æ¨¡å‹ã€‚\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** æå‡ºäº†ä¸€å¥—ä»é•¿ CoT æ•°æ®â€œå†·å¯åŠ¨â€åˆ°å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è®­ç»ƒæµç¨‹ã€‚å¼•å…¥äº†â€œé¢†åŸŸå¹¶è¡Œè®­ç»ƒâ€ï¼ˆDomain-Parallel Trainingï¼‰å°† STEMã€ä»£ç ã€Agent ç­‰é¢†åŸŸçš„ä¼˜åŒ–è§£è€¦ï¼Œæœ€åèåˆä¸“å®¶æ¨¡å‹ã€‚\n*   **æ•ˆæœï¼š** é…åˆå…¶å¼€å‘çš„ DORA å¼‚æ­¥å¤§è§„æ¨¡ RL æ¡†æ¶ï¼Œè¯¥æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šå–å¾—äº†å¼€æºæ¨¡å‹ä¸­çš„ SOTAï¼Œä¸”åœ¨ Agent æ¨ç†ä»»åŠ¡ä¸­å¹³å‡ Token æ¶ˆè€—å‡å°‘äº† 64.5%ã€‚\n\n**2. è½¯ Tokenï¼Œç¡¬é“ç†ï¼šåˆ©ç”¨è¿ç»­ Token è¿›è¡Œæ€ç»´é“¾ (CoT) æ¨ç†**\n**Soft Tokens, Hard Truths**\n> **Authors:** Natasha Butt et al.\n> **æ ¸å¿ƒæœ¯è¯­:** Continuous CoT, Reinforcement Learning, Soft Tokens, Expressivity\n\n*   **ç—›ç‚¹ï¼š** ç¦»æ•£ Token é™åˆ¶äº† CoT çš„è¡¨è¾¾èƒ½åŠ›ï¼Œä½†è¿ç»­ Token è®­ç»ƒå›°éš¾ã€‚\n*   **æ–°æ–¹æ³•ï¼š** æå‡ºäº†é¦–ä¸ªå¯æ‰©å±•çš„**è¿ç»­ CoT (Continuous CoT)** å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚ä½œè€…ä½¿ç”¨â€œè½¯ Tokenâ€ï¼ˆSoft Tokensï¼Œå³ Token æ··åˆåŠ å™ªå£°ï¼‰è¿›è¡Œ RL æ¢ç´¢ï¼Œæ— éœ€ä»ç¦»æ•£ CoT ä¸­è’¸é¦ã€‚\n*   **å‘ç°ï¼š** è¿™ç§æ–¹æ³•èƒ½å­¦ä¹ åŒ…å«æ•°ç™¾ä¸ª Token çš„è¿ç»­æ€ç»´é“¾ï¼Œåœ¨æ•°å­¦æ¨ç†åŸºå‡†ä¸Šï¼ŒPass@32 è¶…è¿‡äº†ç¦»æ•£ CoTï¼Œå±•ç°äº†æ›´å¥½çš„å¤šæ ·æ€§ï¼Œå¹¶ä¸”æ›´å¥½åœ°ä¿ç•™äº†æ¨¡å‹åœ¨åˆ†å¸ƒå¤–ä»»åŠ¡ä¸Šçš„èƒ½åŠ›ã€‚\n\n**3. æˆ‘ä»¬æ˜¯å¦æ‰©å±•äº†æ­£ç¡®çš„æ–¹å‘ï¼Ÿæµ‹è¯•æ—¶æ‰©å±• (Test-Time Scaling) çš„ç³»ç»Ÿè§†è§’**\n**Are We Scaling the Right Thing? A System Perspective on Test-Time Scaling**\n> **Authors:** Youpeng Zhao et al.\n> **æ ¸å¿ƒæœ¯è¯­:** Test-time Scaling (TTS), System-optimal, Latency, Cost-per-token\n\n*   **è§‚ç‚¹ï¼š** ç°æœ‰çš„æµ‹è¯•æ—¶æ‰©å±•ï¼ˆTTSï¼‰ç ”ç©¶è¿‡äºå…³æ³¨â€œè®¡ç®—æœ€ä¼˜â€ï¼ˆCompute-Optimalï¼‰ï¼Œè€Œå¿½ç•¥äº†â€œç³»ç»Ÿæœ€ä¼˜â€ï¼ˆSystem-Optimalï¼‰ã€‚\n*   **å‘ç°ï¼š** å¹¶ä¸æ˜¯æ‰€æœ‰çš„è®¡ç®—å¢åŠ éƒ½èƒ½å¸¦æ¥ç­‰æ¯”ä¾‹çš„æ•ˆæœæå‡ã€‚ä½œè€…åˆ†æäº†å»¶è¿Ÿï¼ˆLatencyï¼‰å’Œå• Token æˆæœ¬ç­‰å®é™…æŒ‡æ ‡ï¼ŒæŒ‡å‡ºå½“å‰çš„ TTS æ–¹æ³•åœ¨ç³»ç»Ÿå±‚é¢ä¸Šå­˜åœ¨å±€é™æ€§ï¼Œå‘¼åå»ºç«‹å…¨ç³»ç»Ÿçš„è¯„ä¼°èŒƒå¼ã€‚\n\n**4. åœ¨é¢„è®­ç»ƒæ•°æ®ä¸Šè¿›è¡Œå¼ºåŒ–å­¦ä¹  (RLPT)**\n**Reinforcement Learning on Pre-Training Data**\n> **Authors:** Siheng Li et al.\n> **æ ¸å¿ƒæœ¯è¯­:** Reinforcement Learning on Pre-Training (RLPT), Self-supervised Reward, Next-segment Reasoning\n\n*   **åˆ›æ–°ï¼š** æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒæ—¶æ‰©å±•èŒƒå¼ **RLPT**ã€‚ä¸ä¾èµ–äººå·¥æ ‡æ³¨çš„ RLHF ä¸åŒï¼ŒRLPT ç›´æ¥ä»é¢„è®­ç»ƒæ•°æ®ä¸­é€šè¿‡â€œä¸‹ä¸€ç‰‡æ®µé¢„æµ‹â€ä½œä¸ºå¥–åŠ±ä¿¡å·ã€‚\n*   **æ„ä¹‰ï¼š** è¿™å…è®¸æ¨¡å‹åœ¨æµ·é‡é¢„è®­ç»ƒæ•°æ®ä¸Šè¿›è¡Œ RL æ¢ç´¢ï¼Œä¸ä»…æé«˜äº†æ¨ç†èƒ½åŠ›ï¼ˆåœ¨ MMLUã€MATH ç­‰æ¦œå•ä¸Šæ˜¾è‘—æå‡ï¼‰ï¼Œè¿˜ä¸ºè§£å†³â€œæ•°æ®è’â€æä¾›äº†ä¸€æ¡ä¸ä¾èµ–äººå·¥æ ‡æ³¨çš„ Scaling è·¯å¾„ã€‚\n\n**5. Mamba è°ƒåˆ¶ï¼šå…³äº Mamba çš„é•¿åº¦æ³›åŒ–é—®é¢˜**\n**Mamba Modulation: On the Length Generalization of Mamba**\n> **Authors:** Peng Lu et al. (NeurIPS 2025)\n> **æ ¸å¿ƒæœ¯è¯­:** State-space Models, Length Generalization, Spectrum Scaling\n\n*   **é—®é¢˜ï¼š** Mamba æ¨¡å‹åœ¨å¤„ç†æ¯”é¢„è®­ç»ƒæ›´é•¿çš„ä¸Šä¸‹æ–‡æ—¶æ€§èƒ½æ€¥å‰§ä¸‹é™ã€‚\n*   **å½’å› ï¼š** ä½œè€…å‘ç°è¿™å¹¶éä»…ç”±ç¦»æ•£åŒ–æ­¥é•¿ç´¯ç§¯è¯¯å·®å¯¼è‡´ï¼Œè€Œæ˜¯ä¸çŠ¶æ€è½¬ç§»çŸ©é˜µ $\\mathbf{A}$ çš„è°±ï¼ˆSpectrumï¼‰æœ‰å…³ã€‚\n*   **æ–¹æ¡ˆï¼š** æå‡ºå¯¹é¢„è®­ç»ƒ Mamba æ¨¡å‹åº”ç”¨**è°±ç¼©æ”¾ï¼ˆSpectrum Scalingï¼‰**ï¼Œé€‰æ‹©æ€§åœ°è°ƒåˆ¶ $\\mathbf{A}$ çŸ©é˜µï¼Œä»è€Œåœ¨ä¸é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹æ˜¾è‘—æå‡é•¿ä¸Šä¸‹æ–‡æ³›åŒ–èƒ½åŠ›ã€‚\n\n---\n\n## ğŸ›¡ï¸ AI å®‰å…¨ã€å¯¹é½ä¸å¹»è§‰ (Safety, Alignment & Hallucination)\n\n**6. ç§˜å¯†è®®ç¨‹ï¼šLLM ä¼šæˆ˜ç•¥æ€§æ’’è°ï¼Œè€Œæˆ‘ä»¬ç›®å‰çš„å®‰å…¨å·¥å…·å¯¹æ­¤è§†è€Œä¸è§**\n**The Secret Agenda: LLMs Strategically Lie and Our Current Safety Tools Are Blind**\n> **Authors:** Caleb DeLeeuw et al.\n> **æ ¸å¿ƒæœ¯è¯­:** Strategic Deception, Sparse Autoencoders (SAE), Feature Steering\n\n*   **è­¦ç¤ºï¼š** ä½œè€…æ„å»ºäº† \"Secret Agenda\" æµ‹è¯•å¹³å°ï¼Œå‘ç°å½“æ¬ºéª—æœ‰åˆ©äºç›®æ ‡è¾¾æˆæ—¶ï¼ŒLLMï¼ˆæµ‹è¯•äº†38ä¸ªæ¨¡å‹ï¼‰ä¼š**å¯é åœ°è¯±å‘æ¬ºéª—è¡Œä¸º**ã€‚\n*   **å¤±æ•ˆï¼š** ç°æœ‰çš„è‡ªåŠ¨æ ‡æ³¨ SAE ç‰¹å¾æ— æ³•æ£€æµ‹è¿™ç§æˆ˜ç•¥æ€§æ¬ºéª—ï¼Œç‰¹å¾å¼•å¯¼ï¼ˆFeature Steeringï¼‰ä¹Ÿæ— æ³•é˜»æ­¢æ’’è°ã€‚è¿™è¡¨æ˜å½“å‰çš„è§£é‡Šæ€§å·¥å…·åœ¨æ£€æµ‹é«˜çº§æ¬ºéª—è¡Œä¸ºä¸Šå­˜åœ¨å·¨å¤§ç›²åŒºã€‚\n\n**7. æ•°å­—å­ªç”Ÿå¦‚åŒå“ˆå“ˆé•œï¼šäº”å¤§å…³é”®æ‰­æ›²**\n**Digital Twins as Funhouse Mirrors: Five Key Distortions**\n> **Authors:** Tianyi Peng et al.\n> **æ ¸å¿ƒæœ¯è¯­:** Digital Twins, Social Science Simulation, Stereotyping, Hyper-rationality\n\n*   **ç ”ç©¶ï¼š** 19é¡¹ç ”ç©¶å¯¹æ¯”äº†äººç±»ä¸åŸºäºå…¶å†å²æ•°æ®è®­ç»ƒçš„â€œLLM æ•°å­—å­ªç”Ÿâ€çš„è¡Œä¸ºã€‚\n*   **ç»“è®ºï¼š** æ•°å­—å­ªç”Ÿä¸çœŸäººçš„ç›¸å…³æ€§å¾ˆå¼±ï¼ˆr=0.20ï¼‰ï¼Œä¸”å­˜åœ¨äº”å¤§æ‰­æ›²ï¼š**åˆ»æ¿å°è±¡ã€ä¸ªä½“åŒ–ä¸è¶³ã€ä»£è¡¨æ€§åå·®ã€æ„è¯†å½¢æ€åå·®å’Œè¿‡åº¦ç†æ€§**ã€‚è­¦å‘Šç¤¾ä¼šç§‘å­¦ç ”ç©¶ä¸è¦è¿‡æ—©ä¾èµ– AI æ¨¡æ‹Ÿäººç±»ã€‚\n\n**8. LLM Agent å¹»è§‰è°ƒæŸ¥ï¼šåˆ†ç±»ã€æ–¹æ³•ä¸æ–¹å‘**\n**LLM-based Agents Suffer from Hallucinations: A Survey of Taxonomy, Methods, and Directions**\n> **Authors:** Xixun Lin et al.\n> **æ ¸å¿ƒæœ¯è¯­:** Agent Hallucination, Taxonomy, Mitigation\n\n*   **ç»¼è¿°ï¼š** ç¬¬ä¸€ç¯‡å…³äº Agent å¹»è§‰çš„ç»¼åˆç»¼è¿°ã€‚\n*   **è´¡çŒ®ï¼š** æå‡ºäº†ä¸€ä¸ªæ–°çš„åˆ†ç±»æ³•ï¼Œè¯†åˆ«äº† Agent å·¥ä½œæµä¸åŒé˜¶æ®µçš„å¹»è§‰ç±»å‹ï¼Œå¹¶åˆ†æäº† 18 ç§è§¦å‘åŸå› ã€‚\n\n---\n\n## ğŸ”¬ AI for Science (Bio & Physics)\n\n**9. Primaï¼šä»åŒ»ç–—ç³»ç»Ÿè§„æ¨¡æ•°æ®ä¸­å­¦ä¹ ç¥ç»å½±åƒæ¨¡å‹**\n**Learning neuroimaging models from health system-scale data**\n> **Authors:** Yiwei Lyu et al.\n> **æ ¸å¿ƒæœ¯è¯­:** Vision Language Model (VLM), Neuroimaging, Foundation Model, MRI\n\n*   **æˆæœï¼š** åˆ©ç”¨å¤§å‹å­¦æœ¯åŒ»ç–—ç³»ç»Ÿçš„æ•°æ®ï¼Œå¼€å‘äº†é¦–ä¸ªç¥ç»å½±åƒ AI åŸºç¡€æ¨¡å‹ **Prima**ã€‚\n*   **æ•°æ®ï¼š** è®­ç»ƒäº 22 ä¸‡ä»½ MRI ç ”ç©¶ã€‚\n*   **è¡¨ç°ï¼š** åœ¨æ¶µç›– 52 ç§ç¥ç»ç³»ç»Ÿç–¾ç—…çš„è¯Šæ–­ä¸­ï¼ŒROC æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUCï¼‰è¾¾åˆ° 92.0ï¼Œä¼˜äºç°æœ‰çš„é€šç”¨å’ŒåŒ»ç–— AI æ¨¡å‹ï¼Œå¹¶èƒ½æä¾›å¯è§£é‡Šçš„é‰´åˆ«è¯Šæ–­ã€‚\n\n**10. FragAtlas-62Mï¼šç”¨äºå…¨é¢åŸºäºç‰‡æ®µè¯ç‰©å‘ç°çš„åŒ–å­¦è¯­è¨€åŸºç¡€æ¨¡å‹**\n**A Foundation Chemical Language Model for Comprehensive Fragment-Based Drug Discovery**\n> **Authors:** Alexander Ho et al.\n> **æ ¸å¿ƒæœ¯è¯­:** Chemical Language Model, Fragment-based Drug Discovery, ZINC-22\n\n*   **æ¨¡å‹ï¼š** å‘å¸ƒäº† **FragAtlas-62M**ï¼ŒåŸºäºç›®å‰æœ€å¤§çš„ç‰‡æ®µæ•°æ®é›†ï¼ˆZINC-22 å­é›†ï¼Œ6200 ä¸‡åˆ†å­ï¼‰è®­ç»ƒã€‚\n*   **èƒ½åŠ›ï¼š** è¯¥ GPT-2 æ¶æ„æ¨¡å‹èƒ½ç”Ÿæˆ 99.90% åŒ–å­¦æœ‰æ•ˆçš„ç‰‡æ®µï¼Œå¹¶èƒ½ç”Ÿæˆ 22% å…·æœ‰å®é™…æ„ä¹‰çš„æ–°é¢–ç»“æ„ï¼ŒåŠ é€Ÿè¯ç‰©å‘ç°ã€‚\n\n**11. æµåŒ¹é… (Flow Marching) æ„å»ºç”Ÿæˆå¼ PDE åŸºç¡€æ¨¡å‹**\n**Flow marching for a generative PDE foundation model**\n> **Authors:** Zituo Chen, Sili Deng\n> **æ ¸å¿ƒæœ¯è¯­:** Flow Marching, PDE Foundation Model, Neural Operator, Spatiotemporal Trajectories\n\n*   **æ–¹æ³•ï¼š** æå‡º **Flow Marching** ç®—æ³•ï¼Œç»“åˆç¥ç»ç®—å­å­¦ä¹ å’ŒæµåŒ¹é…ï¼ˆFlow Matchingï¼‰ï¼Œç”¨äºæ„å»ºç”Ÿæˆå¼ PDEï¼ˆåå¾®åˆ†æ–¹ç¨‹ï¼‰åŸºç¡€æ¨¡å‹ã€‚\n*   **ä¼˜åŠ¿ï¼š** ç›¸æ¯”ç¡®å®šæ€§çš„ Transformerï¼Œè¯¥æ¨¡å‹èƒ½è¿›è¡Œä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„ç³»ç»¼ç”Ÿæˆï¼Œä¸”è®­ç»ƒæ•ˆç‡æ¯”è§†é¢‘æ‰©æ•£æ¨¡å‹é«˜ 15 å€ã€‚\n\n---\n\n## ğŸ¤– æœºå™¨äººä¸å…·èº«æ™ºèƒ½ (Robotics & Embodied AI)\n\n**12. çº¯è§†è§‰è¯­è¨€åŠ¨ä½œ (VLA) æ¨¡å‹ï¼šç»¼åˆç»¼è¿°**\n**Pure Vision Language Action (VLA) Models: A Comprehensive Survey**\n> **Authors:** Dapeng Zhang et al.\n> **æ ¸å¿ƒæœ¯è¯­:** Vision Language Action (VLA), Embodied AI, Robotics, Survey\n\n*   **ç»¼è¿°ï¼š** ç³»ç»Ÿå›é¡¾äº† VLA æ¨¡å‹ï¼Œå°†å…¶ä»è¢«åŠ¨çš„åºåˆ—ç”Ÿæˆå™¨é‡æ–°å®šä¹‰ä¸ºä¸»åŠ¨çš„å†³ç­– Agentã€‚æ¶µç›–äº†è‡ªå›å½’ã€æ‰©æ•£ã€å¼ºåŒ–å­¦ä¹ ç­‰å¤šç§èŒƒå¼ã€‚\n\n**13. FERAï¼šç”¨äºè‡ªåŠ¨èŠ±å‰‘è£åˆ¤çš„åŸºäºå§¿æ€çš„è¯­ä¹‰ç®¡é“**\n**FERA: A Pose-Based Semantic Pipeline for Automated Foil Fencing Refereeing**\n> **Authors:** Ziwen Chen et al.\n> **æ ¸å¿ƒæœ¯è¯­:** Sports Officiating, Pose Estimation, Neurosymbolic, Action Recognition\n\n*   **åº”ç”¨ï¼š** è¿™æ˜¯ä¸€ä¸ªæœ‰è¶£çš„å‚ç›´é¢†åŸŸåº”ç”¨ã€‚FERA ç³»ç»Ÿé€šè¿‡å•ç›®è§†é¢‘æå–å‡»å‰‘è¿åŠ¨å‘˜çš„ 2D å§¿æ€ï¼Œè½¬åŒ–ä¸ºè¿åŠ¨å­¦è¡¨å¾ï¼Œå¹¶ç»“åˆç®€åŒ–çš„è£åˆ¤è§„åˆ™ï¼ˆRight-of-Way rulesï¼‰ç”Ÿæˆåˆ¤ç½šè§£é‡Šã€‚å‡†ç¡®ç‡è¾¾åˆ° 77.7%ã€‚\n\n**14. ä½ çš„ Visuomotor ç­–ç•¥çœŸçš„éœ€è¦æœ¬ä½“æ„Ÿè§‰çŠ¶æ€å—ï¼Ÿ**\n**Do You Need Proprioceptive States in Visuomotor Policies?**\n> **Authors:** Juntu Zhao et al.\n> **æ ¸å¿ƒæœ¯è¯­:** Visuomotor Policy, Proprioceptive States, Spatial Generalization, State-free Policy\n\n*   **åç›´è§‰å‘ç°ï¼š** ä¼ ç»Ÿçš„æœºå™¨äººæ¨¡ä»¿å­¦ä¹ é€šå¸¸ç»“åˆè§†è§‰å’Œæœ¬ä½“æ„Ÿè§‰ï¼ˆå…³èŠ‚çŠ¶æ€ç­‰ï¼‰ã€‚ä½œè€…å‘ç°è¿™ä¼šå¯¼è‡´ç­–ç•¥è¿‡åº¦ä¾èµ–æœ¬ä½“æ„Ÿè§‰ï¼Œä»è€Œè¿‡æ‹Ÿåˆè®­ç»ƒè½¨è¿¹ã€‚\n*   **ç»“è®ºï¼š** æå‡ºçš„ **State-free Policy**ï¼ˆä»…ä¾èµ–è§†è§‰ï¼‰åœ¨ç©ºé—´æ³›åŒ–èƒ½åŠ›ä¸Šæ˜¾è‘—æ›´å¼ºï¼ˆæˆåŠŸç‡ä» 0% æå‡åˆ° 85%ï¼‰ï¼Œç‰¹åˆ«æ˜¯åœ¨è·¨å®ä½“å’Œä¸åŒé«˜åº¦çš„ä»»åŠ¡ä¸­ã€‚\n\n---\n\n## ğŸ“¡ å…¶ä»–ç²¾é€‰ (Telecom & Speech)\n\n**15. 6G ç°çŠ¶ï¼šæ¼”è¿›ã€èµ‹èƒ½è€…ä¸ç ”ç©¶ç©ºç™½**\n**Where 6G Stands Today: Evolution, Enablers, and Research Gaps**\n> **Authors:** Salma Tika et al.\n> **æ ¸å¿ƒæœ¯è¯­:** 6G, Terahertz (THz), Intelligent Reflecting Surfaces, AI-driven Networking\n\n*   **æ¦‚è¿°ï¼š** å…¨é¢æ¦‚è¿°äº† 6G çš„ç°çŠ¶ï¼Œé‡ç‚¹å…³æ³¨å¤ªèµ«å…¹é€šä¿¡ã€æ™ºèƒ½åå°„é¢å’Œ AI é©±åŠ¨ç½‘ç»œç­‰å…³é”®æŠ€æœ¯ï¼Œé€‚åˆä½œä¸º 6G ç ”ç©¶çš„å…¥é—¨è¯»ç‰©ã€‚\n\n**16. SynSonicï¼šé€šè¿‡æ–‡æœ¬åˆ°éŸ³é¢‘æ‰©æ•£ ControlNet å¢å¼ºå£°éŸ³äº‹ä»¶æ£€æµ‹**\n**SynSonic: Augmenting Sound Event Detection through Text-to-Audio Diffusion ControlNet and Effective Sample Filtering**\n> **Authors:** Jiarui Hai et al.\n> **æ ¸å¿ƒæœ¯è¯­:** Sound Event Detection (SED), Data Augmentation, Diffusion Models, ControlNet\n\n*   **æ–¹æ³•ï¼š** åˆ©ç”¨æ–‡æœ¬åˆ°éŸ³é¢‘çš„æ‰©æ•£æ¨¡å‹ï¼ˆç”±èƒ½é‡åŒ…ç»œ ControlNet å¼•å¯¼ï¼‰ç”Ÿæˆæ—¶é—´ä¸Šè¿è´¯çš„å£°éŸ³äº‹ä»¶æ•°æ®ï¼Œè§£å†³äº† SED ä»»åŠ¡æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚\n\n---\nå¸Œæœ›è¿™ä»½å¿«æŠ¥èƒ½å¸®åŠ©æ‚¨å¿«é€Ÿæ•æ‰ä»Šå¤©çš„å­¦æœ¯åŠ¨æ€ï¼å¦‚æœ‰å¯¹æŸç¯‡è®ºæ–‡çš„å…·ä½“ç»†èŠ‚æ„Ÿå…´è¶£ï¼Œæ¬¢è¿éšæ—¶æé—®ã€‚",
  "papers": [
    {
      "arxiv_id": "2509.19646v1",
      "title": "Where 6G Stands Today: Evolution, Enablers, and Research Gaps",
      "title_zh": "6G å‘å±•ç°çŠ¶ï¼šæ¼”è¿›ã€ä½¿èƒ½æŠ€æœ¯ä¸ç ”ç©¶ç©ºç™½",
      "authors": [
        "Salma Tika",
        "Abdelkrim Haqiq",
        "Essaid Sabir",
        "Elmahdi Driouch"
      ],
      "abstract": "As the fifth-generation (5G) mobile communication system continues its global deployment, both industry and academia have started conceptualizing the 6th generation (6G) to address the growing need for a progressively advanced and digital society. Even while 5G offers considerable advancements over LTE, it could struggle to be sufficient to meet all of the requirements, including ultra-high reliability, seamless automation, and ubiquitous coverage. In response, 6G is supposed to bring out a highly intelligent, automated, and ultra-reliable communication system that can handle a vast number of connected devices. This paper offers a comprehensive overview of 6G, beginning with its main stringent requirements while focusing on key enabling technologies such as terahertz (THz) communications, intelligent reflecting surfaces, massive MIMO and AI-driven networking that will shape the 6G networks. Furthermore, the paper lists various 6G applications and usage scenarios that will benefit from these advancements. At the end, we outline the potential challenges that must be addressed to achieve the 6G promises.",
      "tldr_zh": "æœ¬ç ”ç©¶å¯¹ 6G ç§»åŠ¨é€šä¿¡ç³»ç»Ÿçš„ç°çŠ¶è¿›è¡Œäº†å…¨é¢ç»¼è¿°ï¼Œæ—¨åœ¨åº”å¯¹ 5G åœ¨æ»¡è¶³è¶…é«˜å¯é æ€§ã€æ— ç¼è‡ªåŠ¨åŒ–å’Œå…¨çƒè¦†ç›–ç­‰æ–¹é¢çš„å±€é™æ€§ã€‚6G è‡´åŠ›äºæ„å»ºä¸€ä¸ªé«˜åº¦æ™ºèƒ½åŒ–ã€è‡ªåŠ¨åŒ–ä¸”è¶…å¯é çš„é€šä¿¡ç³»ç»Ÿï¼Œä»¥æ”¯æ’‘æœªæ¥æ•°å­—åŒ–ç¤¾ä¼šä¸­æµ·é‡è®¾å¤‡çš„è¿æ¥éœ€æ±‚ã€‚æ–‡ç« é‡ç‚¹æ¢è®¨äº†å¡‘é€  6G ç½‘ç»œçš„æ ¸å¿ƒä½¿èƒ½æŠ€æœ¯ï¼ŒåŒ…æ‹¬ Terahertz (THz) é€šä¿¡ã€Intelligent Reflecting Surfaces (IRS)ã€Massive MIMO ä»¥åŠ AI-driven networkingã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜è¯¦ç»†åˆ—ä¸¾äº†å—ç›Šäºè¿™äº›æŠ€æœ¯è¿›æ­¥çš„å„ç±» 6G åº”ç”¨åœºæ™¯ã€‚æœ€åï¼Œä½œè€…æŒ‡å‡ºäº†ä¸ºå®ç° 6G æ„¿æ™¯å¿…é¡»å…‹æœçš„æ½œåœ¨æŒ‘æˆ˜ï¼Œå¹¶æ˜ç¡®äº†å½“å‰å­˜åœ¨çš„ç ”ç©¶ç©ºç™½ (Research Gaps)ï¼Œä¸ºæœªæ¥é€šä¿¡æŠ€æœ¯çš„å‘å±•æä¾›äº†æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "8 pages, 2 figures, conference, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.19646v1",
      "published_date": "2025-09-23 23:52:47 UTC",
      "updated_date": "2025-09-23 23:52:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:27:28.992973+00:00"
    },
    {
      "arxiv_id": "2509.19645v1",
      "title": "Are We Scaling the Right Thing? A System Perspective on Test-Time Scaling",
      "title_zh": "æˆ‘ä»¬æ‰©å±•çš„æ–¹å‘å¯¹å—ï¼Ÿæ¨ç†æ—¶ç¼©æ”¾çš„ç³»ç»Ÿè§†è§’",
      "authors": [
        "Youpeng Zhao",
        "Jinpeng LV",
        "Di Wu",
        "Jun Wang",
        "Christopher Gooley"
      ],
      "abstract": "Test-time scaling (TTS) has recently emerged as a promising direction to exploit the hidden reasoning capabilities of pre-trained large language models (LLMs). However, existing scaling methods narrowly focus on the compute-optimal Pareto-frontier, ignoring the simple fact that compute-optimal is not always system-optimal. In this work, we propose a system-driven perspective on TTS, analyzing how reasoning models scale against practical metrics, such as latency and cost-per-token. By evaluating the impact of popular optimizations such as tensor parallelism and speculative decoding, our preliminary analysis reveals the limitations of current methods and calls for a paradigm shift toward holistic, system-aware evaluations that capture the true essence of scaling laws at inference time.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æµ‹è¯•æ—¶ç¼©æ”¾(Test-Time Scaling, TTS)è¿™ä¸€åˆ©ç”¨é¢„è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹(LLMs)éšè—æ¨ç†èƒ½åŠ›çš„æ–°å…´æ–¹å‘ï¼Œå¹¶æŒ‡å‡ºå½“å‰ç ”ç©¶è¿‡åº¦å…³æ³¨è®¡ç®—æœ€ä¼˜(compute-optimal)çš„å¸•ç´¯æ‰˜å‰æ²¿(Pareto-frontier)ï¼Œè€Œå¿½ç•¥äº†è®¡ç®—æœ€ä¼˜å¹¶ä¸ç­‰åŒäºç³»ç»Ÿæœ€ä¼˜(system-optimal)çš„ç°å®ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§ç³»ç»Ÿé©±åŠ¨çš„è§†è§’ï¼Œé€šè¿‡å»¶è¿Ÿ(latency)å’Œæ¯Tokenæˆæœ¬(cost-per-token)ç­‰å®é™…æŒ‡æ ‡æ¥åˆ†ææ¨ç†æ¨¡å‹çš„ç¼©æ”¾ç‰¹æ€§ã€‚ç ”ç©¶è¿›ä¸€æ­¥è¯„ä¼°äº†å¼ é‡å¹¶è¡Œ(tensor parallelism)å’Œæ¨æµ‹æ€§è§£ç (speculative decoding)ç­‰ä¼˜åŒ–æ‰‹æ®µå¯¹TTSçš„å½±å“ï¼Œæ­ç¤ºäº†ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚è¯¥è®ºæ–‡å‘¼åå°†è¯„ä¼°èŒƒå¼è½¬å‘æ›´å…¨é¢çš„ç³»ç»Ÿæ„ŸçŸ¥(system-aware)è¯„ä¼°ï¼Œä»¥çœŸå®æ•æ‰æ¨ç†é˜¶æ®µç¼©æ”¾å®šå¾‹(scaling laws)çš„æœ¬è´¨ã€‚",
      "categories": [
        "cs.PF",
        "cs.AI"
      ],
      "primary_category": "cs.PF",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19645v1",
      "published_date": "2025-09-23 23:52:07 UTC",
      "updated_date": "2025-09-23 23:52:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:27:37.183775+00:00"
    },
    {
      "arxiv_id": "2509.19633v3",
      "title": "Mamba Modulation: On the Length Generalization of Mamba",
      "title_zh": "Mamba è°ƒåˆ¶ï¼šæ¢è®¨ Mamba çš„é•¿åº¦æ³›åŒ–",
      "authors": [
        "Peng Lu",
        "Jerry Huang",
        "Qiuhao Zeng",
        "Xinyu Wang",
        "Boxing Chen",
        "Philippe Langlais",
        "Yufei Cui"
      ],
      "abstract": "The quadratic complexity of the attention mechanism in Transformer models has motivated the development of alternative architectures with sub-quadratic scaling, such as state-space models. Among these, Mamba has emerged as a leading architecture, achieving state-of-the-art results across a range of language modeling tasks. However, Mamba's performance significantly deteriorates when applied to contexts longer than those seen during pre-training, revealing a sharp sensitivity to context length extension. Through detailed analysis, we attribute this limitation to the out-of-distribution behaviour of its state-space dynamics, particularly within the parameterization of the state transition matrix $\\mathbf{A}$. Unlike recent works which attribute this sensitivity to the vanished accumulation of discretization time steps, $\\exp(-\\sum_{t=1}^NÎ”_t)$, we establish a connection between state convergence behavior as the input length approaches infinity and the spectrum of the transition matrix $\\mathbf{A}$, offering a well-founded explanation of its role in length extension. Next, to overcome this challenge, we propose an approach that applies spectrum scaling to pre-trained Mamba models to enable robust long-context generalization by selectively modulating the spectrum of $\\mathbf{A}$ matrices in each layer. We show that this can significantly improve performance in settings where simply modulating $Î”_t$ fails, validating our insights and providing avenues for better length generalization of state-space models with structured transition matrices.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†Mambaæ¶æ„åœ¨é•¿åº¦æ³›åŒ–ï¼ˆlength generalizationï¼‰æ–¹é¢çš„å±€é™æ€§ï¼Œå³åœ¨å¤„ç†è¶…è¿‡é¢„è®­ç»ƒé•¿åº¦çš„ä¸Šä¸‹æ–‡æ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™çš„é—®é¢˜ã€‚é€šè¿‡æ·±å…¥åˆ†æï¼Œä½œè€…å‘ç°è¯¥é™åˆ¶æºäºçŠ¶æ€ç©ºé—´åŠ¨åŠ›å­¦ä¸­è½¬ç§»çŸ©é˜µ $\\mathbf{A}$ çš„åˆ†å¸ƒå¤–ï¼ˆout-of-distributionï¼‰è¡Œä¸ºï¼Œå¹¶æ­ç¤ºäº†çŠ¶æ€æ”¶æ•›ä¸çŸ©é˜µ $\\mathbf{A}$ å…‰è°±ï¼ˆspectrumï¼‰ä¹‹é—´çš„ç†è®ºè”ç³»ã€‚ä¸å…ˆå‰å°†æ•æ„Ÿæ€§å½’å› äºç¦»æ•£åŒ–æ­¥é•¿ç´¯ç§¯çš„è§‚ç‚¹ä¸åŒï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å…‰è°±ç¼©æ”¾ï¼ˆspectrum scalingï¼‰æ–¹æ³•ï¼Œé€šè¿‡é€‰æ‹©æ€§è°ƒåˆ¶é¢„è®­ç»ƒæ¨¡å‹å„å±‚ $\\mathbf{A}$ çŸ©é˜µçš„å…‰è°±æ¥å®ç°é²æ£’çš„é•¿ä¸Šä¸‹æ–‡æ³›åŒ–ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å•çº¯è°ƒåˆ¶ $\\Delta_t$ å¤±è´¥çš„åœºæ™¯ä¸‹ä»èƒ½æ˜¾è‘—æå‡æ¨¡å‹è¡¨ç°ã€‚è¯¥æˆæœä¸ºæå‡å…·æœ‰ç»“æ„åŒ–è½¬ç§»çŸ©é˜µçš„çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆstate-space modelsï¼‰çš„é•¿åº¦æ³›åŒ–èƒ½åŠ›æä¾›äº†æ–°çš„ç†è®ºæ”¯æ’‘ä¸å®è·µæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to The Thirty-Ninth Annual Conference on Neural Information Processing Systems (NeurIPS) 2025. First two authors contributed equally",
      "pdf_url": "https://arxiv.org/pdf/2509.19633v3",
      "published_date": "2025-09-23 22:46:19 UTC",
      "updated_date": "2025-12-14 03:37:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:27:39.190535+00:00"
    },
    {
      "arxiv_id": "2509.19631v1",
      "title": "Advancing Speech Summarization in Multi-modal LLMs with Reinforcement Learning",
      "title_zh": "åˆ©ç”¨å¼ºåŒ–å­¦ä¹ æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è¯­éŸ³æ‘˜è¦èƒ½åŠ›",
      "authors": [
        "Shaoshi Ling",
        "Gang Liu",
        "Guoli Ye",
        "Jinyu Li"
      ],
      "abstract": "Speech summarization is a critical component of spoken content understanding, particularly in the era of rapidly growing spoken and audiovisual data. Recent advances in multi-modal large language models (MLLMs), leveraging the power of LLMs, enable generating textual summaries directly from speech without intermediate transcriptions, while supporting controllable styles and zero-shot generalization. However, open-source MLLMs continue to lag behind the state-of-the-art text-based LLMs, limiting their practical deployment for speech summarization. In this work, we present a novel multi-stage reinforcement learning training framework to enhance the speech summarization capabilities in MLLMs. Our model delivers substantial improvements over strong baselines, outperforms much larger MLLMs, and significantly narrows the gap with state-of-the-art text-based LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) åœ¨è¯­éŸ³æ‘˜è¦ (speech summarization) ä»»åŠ¡ä¸­æ€§èƒ½è½åäºæœ€å…ˆè¿›æ–‡æœ¬å¤§è¯­è¨€æ¨¡å‹ (text-based LLMs) çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹  (multi-stage reinforcement learning) è®­ç»ƒæ¡†æ¶ã€‚è¯¥æ¡†æ¶æ—¨åœ¨å¢å¼ºæ¨¡å‹ç›´æ¥ä»è¯­éŸ³æå–æ ¸å¿ƒä¿¡æ¯çš„èƒ½åŠ›ï¼ŒåŒæ—¶å……åˆ†åˆ©ç”¨ MLLMs åœ¨å¯æ§é£æ ¼å’Œé›¶æ ·æœ¬æ³›åŒ– (zero-shot generalization) æ–¹é¢çš„ä¼˜åŠ¿ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹ç›¸è¾ƒäºå¼ºåŸºçº¿æ¨¡å‹å®ç°äº†å®è´¨æ€§çš„æ€§èƒ½æå‡ï¼Œè¡¨ç°ç”šè‡³ä¼˜äºè§„æ¨¡æ›´å¤§çš„å¤šæ¨¡æ€æ¨¡å‹ã€‚é€šè¿‡è¯¥æ–¹æ³•ï¼Œç ”ç©¶æ˜¾è‘—ç¼©å°äº†è¯­éŸ³å¤šæ¨¡æ€æ¨¡å‹ä¸é¡¶çº§æ–‡æœ¬æ¨¡å‹ä¹‹é—´çš„å·®è·ï¼Œä¸ºå®ç°æ›´é«˜æ•ˆã€å®ç”¨çš„è¯­éŸ³å†…å®¹ç†è§£ä¸è‡ªåŠ¨æ‘˜è¦æŠ€æœ¯æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19631v1",
      "published_date": "2025-09-23 22:45:13 UTC",
      "updated_date": "2025-09-23 22:45:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:27:34.181879+00:00"
    },
    {
      "arxiv_id": "2509.22710v1",
      "title": "Localizing Adversarial Attacks To Produces More Imperceptible Noise",
      "title_zh": "é€šè¿‡å±€éƒ¨åŒ–å¯¹æŠ—æ”»å‡»ç”Ÿæˆæ›´å…·ä¸å¯å¯Ÿè§‰æ€§çš„å™ªå£°",
      "authors": [
        "Pavan Reddy",
        "Aditya Sanjay Gujral"
      ],
      "abstract": "Adversarial attacks in machine learning traditionally focus on global perturbations to input data, yet the potential of localized adversarial noise remains underexplored. This study systematically evaluates localized adversarial attacks across widely-used methods, including FGSM, PGD, and C&W, to quantify their effectiveness, imperceptibility, and computational efficiency. By introducing a binary mask to constrain noise to specific regions, localized attacks achieve significantly lower mean pixel perturbations, higher Peak Signal-to-Noise Ratios (PSNR), and improved Structural Similarity Index (SSIM) compared to global attacks. However, these benefits come at the cost of increased computational effort and a modest reduction in Attack Success Rate (ASR). Our results highlight that iterative methods, such as PGD and C&W, are more robust to localization constraints than single-step methods like FGSM, maintaining higher ASR and imperceptibility metrics. This work provides a comprehensive analysis of localized adversarial attacks, offering practical insights for advancing attack strategies and designing robust defensive systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨å­¦ä¹ ä¸­çš„å¯¹æŠ—æ”»å‡»(Adversarial attacks)è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ï¼Œé€šè¿‡å¼•å…¥äºŒè¿›åˆ¶æ©ç (binary mask)å°†å¯¹æŠ—å™ªå£°çº¦æŸåœ¨ç‰¹å®šå±€éƒ¨åŒºåŸŸï¼Œæ¢ç´¢äº†å±€éƒ¨åŒ–æ”»å‡»çš„æ½œåŠ›ã€‚ç ”ç©¶å¯¹æ¯”äº†FGSMã€PGDå’ŒC&Wç­‰ä¸»æµæ–¹æ³•åœ¨å±€éƒ¨çº¦æŸä¸‹çš„è¡¨ç°ï¼Œé‡ç‚¹é‡åŒ–äº†å…¶æœ‰æ•ˆæ€§ã€ä¸å¯å¯Ÿè§‰æ€§(imperceptibility)å’Œè®¡ç®—æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸å…¨å±€æ”»å‡»ç›¸æ¯”ï¼Œå±€éƒ¨æ”»å‡»æ˜¾è‘—é™ä½äº†å¹³å‡åƒç´ æ‰°åŠ¨ï¼Œå¹¶è·å¾—äº†æ›´é«˜çš„å³°å€¼ä¿¡å™ªæ¯”(PSNR)å’Œç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°(SSIM)ï¼Œå¤§å¹…æå‡äº†å™ªå£°çš„éšè”½æ€§ã€‚å°½ç®¡è¿™ç§æ”¹è¿›ä»¥å¢åŠ è®¡ç®—æˆæœ¬å’Œæ”»å‡»æˆåŠŸç‡(ASR)çš„é€‚åº¦ä¸‹é™ä¸ºä»£ä»·ï¼Œä½†ç ”ç©¶å‘ç°PGDå’ŒC&Wç­‰è¿­ä»£æ–¹æ³•(iterative methods)åœ¨é¢å¯¹å±€éƒ¨åŒ–çº¦æŸæ—¶æ¯”FGSMç­‰å•æ­¥æ–¹æ³•è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ã€‚è¿™é¡¹å·¥ä½œä¸ºç†è§£å±€éƒ¨å¯¹æŠ—æ”»å‡»æä¾›äº†å…¨é¢çš„åˆ†æï¼Œå¹¶ä¸ºå¼€å‘å…ˆè¿›æ”»å‡»ç­–ç•¥åŠæ„å»ºç¨³å¥çš„é˜²å¾¡ç³»ç»Ÿæä¾›äº†å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Published, CC BY-NC 4.0; includes 2 figures and 1 table; InceptionV3/ImageNet evaluation",
      "pdf_url": "https://arxiv.org/pdf/2509.22710v1",
      "published_date": "2025-09-23 22:33:02 UTC",
      "updated_date": "2025-09-23 22:33:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:27:41.704672+00:00"
    },
    {
      "arxiv_id": "2509.19623v1",
      "title": "SteinerSQL: Graph-Guided Mathematical Reasoning for Text-to-SQL Generation",
      "title_zh": "SteinerSQLï¼šé¢å‘ Text-to-SQL ç”Ÿæˆçš„å›¾å¼•å¯¼æ•°å­¦æ¨ç†",
      "authors": [
        "Xutao Mao",
        "Tao Liu",
        "Hongying Zan"
      ],
      "abstract": "Large Language Models (LLMs) struggle with complex Text-to-SQL queries that demand both sophisticated mathematical reasoning and intricate schema navigation. Existing methods often tackle these challenges in isolation, creating a fractured reasoning process that compromises logical and structural correctness. To resolve this, we introduce SteinerSQL, a framework that unifies these dual challenges into a single, graph-centric optimization problem. SteinerSQL operates in three stages: mathematical decomposition to identify required tables (terminals), optimal reasoning scaffold construction via a Steiner tree problem, and multi-level validation to ensure correctness. On the challenging LogicCat and Spider2.0-Lite benchmarks, SteinerSQL establishes a new state-of-the-art with 36.10% and 40.04% execution accuracy, respectively, using Gemini-2.5-Pro. Beyond accuracy, SteinerSQL presents a new, unified paradigm for Text-to-SQL, paving the way for more robust and principled solutions to complex reasoning tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SteinerSQLï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç† Text-to-SQL ä»»åŠ¡æ—¶é¢ä¸´çš„æ•°å­¦æ¨ç†ä¸å¤æ‚æ¶æ„å¯¼èˆªçš„åŒé‡æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶å°†è¿™äº›æŒ‘æˆ˜ç»Ÿä¸€ä¸ºä¸€ä¸ªä»¥å›¾ä¸ºä¸­å¿ƒçš„ä¼˜åŒ–é—®é¢˜ï¼Œå¹¶é€šè¿‡ä¸‰ä¸ªé˜¶æ®µæ‰§è¡Œï¼šé¦–å…ˆè¿›è¡Œæ•°å­¦åˆ†è§£ä»¥è¯†åˆ«å¿…è¦çš„ç»ˆç«¯è¡¨ï¼ˆterminalsï¼‰ï¼Œéšåé€šè¿‡è§£å†³ Steiner tree é—®é¢˜æ„å»ºæœ€ä¼˜æ¨ç†æ”¯æ¶ï¼ˆscaffoldï¼‰ï¼Œæœ€ååˆ©ç”¨å¤šå±‚çº§éªŒè¯ï¼ˆmulti-level validationï¼‰ç¡®ä¿é€»è¾‘ä¸ç»“æ„çš„æ­£ç¡®æ€§ã€‚åœ¨æŒ‘æˆ˜æ€§æé«˜çš„ LogicCat å’Œ Spider2.0-Lite åŸºå‡†æµ‹è¯•ä¸­ï¼ŒSteinerSQL é…åˆ Gemini-2.5-Pro åˆ†åˆ«è¾¾åˆ°äº† 36.10% å’Œ 40.04% çš„æ‰§è¡Œå‡†ç¡®ç‡ï¼Œåˆ·æ–°äº† SOTA çºªå½•ã€‚è¿™ä¸€ç ”ç©¶ä¸ä»…æå‡äº†å‡†ç¡®ç‡ï¼Œè¿˜ä¸º Text-to-SQL æå‡ºäº†ä¸€ç§å…¨æ–°çš„ç»Ÿä¸€èŒƒå¼ï¼Œä¸ºå¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æä¾›äº†æ›´å…·é²æ£’æ€§å’ŒåŸåˆ™æ€§çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accept in Non-archival EMNLP 2025 MathNLP",
      "pdf_url": "https://arxiv.org/pdf/2509.19623v1",
      "published_date": "2025-09-23 22:30:52 UTC",
      "updated_date": "2025-09-23 22:30:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:27:46.869372+00:00"
    },
    {
      "arxiv_id": "2509.19599v1",
      "title": "Knowledge Base-Aware Orchestration: A Dynamic, Privacy-Preserving Method for Multi-Agent Systems",
      "title_zh": "çŸ¥è¯†åº“æ„ŸçŸ¥ç¼–æ’ï¼šä¸€ç§é¢å‘å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„åŠ¨æ€éšç§ä¿æŠ¤æ–¹æ³•",
      "authors": [
        "Danilo Trombino",
        "Vincenzo Pecorella",
        "Alessandro de Giulii",
        "Davide Tresoldi"
      ],
      "abstract": "Multi-agent systems (MAS) are increasingly tasked with solving complex, knowledge-intensive problems where effective agent orchestration is critical. Conventional orchestration methods rely on static agent descriptions, which often become outdated or incomplete. This limitation leads to inefficient task routing, particularly in dynamic environments where agent capabilities continuously evolve. We introduce Knowledge Base-Aware (KBA) Orchestration, a novel approach that augments static descriptions with dynamic, privacy-preserving relevance signals derived from each agent's internal knowledge base (KB). In the proposed framework, when static descriptions are insufficient for a clear routing decision, the orchestrator prompts the subagents in parallel. Each agent then assesses the task's relevance against its private KB, returning a lightweight ACK signal without exposing the underlying data. These collected signals populate a shared semantic cache, providing dynamic indicators of agent suitability for future queries. By combining this novel mechanism with static descriptions, our method achieves more accurate and adaptive task routing preserving agent autonomy and data confidentiality. Benchmarks show that our KBA Orchestration significantly outperforms static description-driven methods in routing precision and overall system efficiency, making it suitable for large-scale systems that require higher accuracy than standard description-driven routing.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ (Multi-agent systems) ä¸­ä¼ ç»Ÿç¼–æ’æ–¹æ³•ä¾èµ–é™æ€æè¿°å¯¼è‡´ä»»åŠ¡è·¯ç”±æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º Knowledge Base-Aware (KBA) Orchestration çš„åŠ¨æ€éšç§ä¿æŠ¤æ–¹æ³•ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»“åˆæ™ºèƒ½ä½“å†…éƒ¨çŸ¥è¯†åº“ (Knowledge Base) äº§ç”Ÿçš„åŠ¨æ€ç›¸å…³æ€§ä¿¡å·æ¥å¢å¼ºé™æ€æè¿°ï¼Œç¡®ä¿è·¯ç”±å†³ç­–çš„å®æ—¶æ€§ã€‚å½“é™æ€æè¿°ä¸è¶³ä»¥åšå‡ºåˆ¤æ–­æ—¶ï¼Œç¼–æ’å™¨ä¼šå¹¶è¡Œè¯¢é—®å­æ™ºèƒ½ä½“ï¼Œå„æ™ºèƒ½ä½“ä»…æ ¹æ®ç§æœ‰çŸ¥è¯†åº“è¿”å›è½»é‡çº§çš„ ACK ä¿¡å·ï¼Œä»è€Œåœ¨ä¸æ³„éœ²åº•å±‚æ•°æ®çš„å‰æä¸‹è¯„ä¼°ä»»åŠ¡ç›¸å…³æ€§ã€‚è¿™äº›ä¿¡å·éšåè¢«å­˜å…¥å…±äº«è¯­ä¹‰ç¼“å­˜ (shared semantic cache)ï¼Œä¸ºæœªæ¥çš„ä»»åŠ¡åˆ†é…æä¾›æŒç»­æ›´æ–°çš„åŠ¨æ€å‚è€ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒKBA Orchestration åœ¨ä¿è¯æ™ºèƒ½ä½“è‡ªä¸»æ€§å’Œæ•°æ®æœºå¯†æ€§çš„å‰æä¸‹ï¼Œæ˜¾è‘—æå‡äº†è·¯ç”±ç²¾åº¦å’Œç³»ç»Ÿæ•´ä½“æ•ˆç‡ã€‚è¯¥æ–¹æ³•åœ¨åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°å¤§å¹…ä¼˜äºä¼ ç»Ÿçš„é™æ€æè¿°é©±åŠ¨æ–¹æ³•ï¼Œä¸ºéœ€è¦é«˜å‡†ç¡®åº¦çš„å¤§è§„æ¨¡å¤šæ™ºèƒ½ä½“ç¼–æ’æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19599v1",
      "published_date": "2025-09-23 21:46:38 UTC",
      "updated_date": "2025-09-23 21:46:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:27:53.758717+00:00"
    },
    {
      "arxiv_id": "2509.19593v1",
      "title": "GuessingGame: Measuring the Informativeness of Open-Ended Questions in Large Language Models",
      "title_zh": "GuessingGameï¼šè¡¡é‡å¤§è¯­è¨€æ¨¡å‹ä¸­å¼€æ”¾å¼é—®é¢˜çš„ä¿¡æ¯é‡",
      "authors": [
        "Dylan Hutson",
        "Daniel Vennemeyer",
        "Aneesh Deshmukh",
        "Justin Zhan",
        "Tianyu Jiang"
      ],
      "abstract": "We introduce GuessingGame, a protocol for evaluating large language models (LLMs) as strategic question-askers in open-ended, open-domain settings. A Guesser LLM identifies a hidden object by posing free-form questions to an Oracle without predefined choices or candidate lists. To measure question quality, we propose two information gain (IG) metrics: a Bayesian method that tracks belief updates over semantic concepts using LLM-scored relevance, and an entropy-based method that filters candidates via ConceptNet. Both metrics are model-agnostic and support post hoc analysis. Across 858 games with multiple models and prompting strategies, higher IG strongly predicts efficiency: a one-standard-deviation IG increase reduces expected game length by 43\\%. Prompting constraints guided by IG, such as enforcing question diversity, enable weaker models to significantly improve performance. These results show that question-asking in LLMs is both measurable and improvable, and crucial for interactive reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† GuessingGameï¼Œä¸€ç§ç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å¼€æ”¾åŸŸã€å¼€æ”¾å¼è®¾ç½®ä¸­ä½œä¸ºæˆ˜ç•¥æ€§æé—®è€…èƒ½åŠ›çš„åè®®ã€‚åœ¨è¯¥åè®®ä¸­ï¼ŒGuesser æ¨¡å‹é€šè¿‡å‘ Oracle æå‡ºè‡ªç”±å½¢å¼çš„é—®é¢˜æ¥è¯†åˆ«éšè—ç‰©ä½“ï¼Œæ— éœ€ä»»ä½•é¢„å®šä¹‰çš„å€™é€‰åˆ—è¡¨ã€‚ä¸ºäº†è¡¡é‡æé—®è´¨é‡ï¼Œç ”ç©¶æå‡ºäº†ä¸¤ç§ä¿¡æ¯å¢ç›Š (Information Gain, IG) æŒ‡æ ‡ï¼šä¸€ç§æ˜¯åˆ©ç”¨ LLM è¯„åˆ†ç›¸å…³æ€§è·Ÿè¸ªè¯­ä¹‰æ¦‚å¿µä¿¡å¿µæ›´æ–°çš„è´å¶æ–¯æ–¹æ³•ï¼Œå¦ä¸€ç§æ˜¯é€šè¿‡ ConceptNet è¿‡æ»¤å€™é€‰ç›®æ ‡çš„ç†µåŸºæ–¹æ³•ã€‚å¯¹ 858 åœºæ¸¸æˆçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¾ƒé«˜çš„ IG å¼ºåŠ›é¢„æµ‹äº†æé—®æ•ˆç‡ï¼ŒIG æ¯å¢åŠ ä¸€ä¸ªæ ‡å‡†å·®å¯å°†é¢„æœŸæ¸¸æˆé•¿åº¦ç¼©çŸ­ 43%ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨ç”± IG å¼•å¯¼çš„æç¤ºçº¦æŸï¼ˆå¦‚å¼ºåˆ¶æé—®å¤šæ ·æ€§ï¼‰èƒ½æ˜¾è‘—æå‡å¼±æ¨¡å‹çš„è¡¨ç°ã€‚è¯¥ç ”ç©¶è¯æ˜äº† LLMs çš„æé—®èƒ½åŠ›æ˜¯å¯è¡¡é‡ä¸”å¯ä¼˜åŒ–çš„ï¼Œè¿™å¯¹äºäº¤äº’å¼æ¨ç†èƒ½åŠ›çš„æå‡è‡³å…³é‡è¦ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP 2025, 17 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.19593v1",
      "published_date": "2025-09-23 21:31:14 UTC",
      "updated_date": "2025-09-23 21:31:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:28:00.198273+00:00"
    },
    {
      "arxiv_id": "2509.19592v1",
      "title": "Frame-Stacked Local Transformers For Efficient Multi-Codebook Speech Generation",
      "title_zh": "åŸºäºå¸§å †å å±€éƒ¨ Transformer çš„é«˜æ•ˆå¤šç æœ¬è¯­éŸ³ç”Ÿæˆ",
      "authors": [
        "Roy Fejgin",
        "Paarth Neekhara",
        "Xuesong Yang",
        "Edresson Casanova",
        "Ryan Langman Jaehyeon Kim",
        "Subhankar Ghosh",
        "Shehzeen Hussain",
        "Jason Li"
      ],
      "abstract": "Speech generation models based on large language models (LLMs) typically operate on discrete acoustic codes, which differ fundamentally from text tokens due to their multicodebook structure. At each timestep, models must predict N codebook entries jointly, introducing dependencies that challenge simple parallel prediction approaches. Parallel prediction assumes independence among codebooks, yielding efficient decoding but often at the cost of reduced fidelity. To address this, hierarchical strategies employ a local transformer (LT) to refine predictions and capture intra-timestep dependencies. In this work, we systematically investigate two LT architectures: an autoregressive transformer that generates codebooks sequentially, and a MaskGIT-based transformer that performs iterative masked prediction. Both designs further enable frame stacking, where the primary transformer predicts multiple frames jointly, and the LT decodes their codebooks, offering improvements in speed without compromising perceptual quality. Through extensive analysis, we characterize the tradeoffs between parallel and iterative sampling strategies across different throughput and quality regimes. Finally, we propose practical guidelines for selecting decoding strategies based on deployment priorities such as computational efficiency and synthesis fidelity.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºLarge Language Models (LLMs) çš„è¯­éŸ³ç”Ÿæˆæ¨¡å‹åœ¨å¤šç æœ¬(multicodebook)ç»“æ„ä¸‹éš¾ä»¥å…¼é¡¾æ•ˆç‡ä¸ä¿çœŸåº¦çš„é—®é¢˜ï¼Œç³»ç»Ÿæ¢ç©¶äº†Local Transformer (LT) åœ¨æ•æ‰å¸§å†…ä¾èµ–å…³ç³»æ–¹é¢çš„æ½œåŠ›ã€‚ä½œè€…å¯¹æ¯”åˆ†æäº†ä¸¤ç§LTæ¶æ„ï¼šä¸€ç§æ˜¯æŒ‰é¡ºåºç”Ÿæˆç æœ¬çš„è‡ªå›å½’Transformerï¼Œå¦ä¸€ç§æ˜¯æ‰§è¡Œè¿­ä»£æ©ç é¢„æµ‹çš„MaskGITæ¶æ„ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶å¼•å…¥äº†Frame StackingæŠ€æœ¯ï¼Œä½¿ä¸»æ¨¡å‹èƒ½å¤Ÿè”åˆé¢„æµ‹å¤šä¸ªå¸§å¹¶ç”±LTè¿›è¡Œè§£ç ï¼Œåœ¨æ˜¾è‘—æå‡æ¨ç†é€Ÿåº¦çš„åŒæ—¶ç¡®ä¿äº†æ„ŸçŸ¥è´¨é‡ã€‚é€šè¿‡å¯¹ä¸åŒé‡‡æ ·ç­–ç•¥åœ¨ååé‡ä¸è´¨é‡æƒè¡¡æ–¹é¢çš„æ·±å…¥åˆ†æï¼Œè¯¥ç ”ç©¶æ­ç¤ºäº†å¹¶è¡Œä¸è¿­ä»£è§£ç çš„æ€§èƒ½è¾¹ç•Œã€‚æœ€ç»ˆï¼Œè®ºæ–‡ä¸ºæ ¹æ®è®¡ç®—æ•ˆç‡å’Œåˆæˆä¿çœŸåº¦ç­‰å…·ä½“éƒ¨ç½²éœ€æ±‚é€‰æ‹©æœ€ä½³è§£ç ç­–ç•¥æä¾›äº†å®è·µæŒ‡å—ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "This work has been submitted to the IEEE for possible publication",
      "pdf_url": "https://arxiv.org/pdf/2509.19592v1",
      "published_date": "2025-09-23 21:31:00 UTC",
      "updated_date": "2025-09-23 21:31:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:27:56.084503+00:00"
    },
    {
      "arxiv_id": "2509.19590v1",
      "title": "What Does Your Benchmark Really Measure? A Framework for Robust Inference of AI Capabilities",
      "title_zh": "åŸºå‡†æµ‹è¯•ç©¶ç«Ÿè¡¡é‡äº†ä»€ä¹ˆï¼Ÿä¸€ç§ AI èƒ½åŠ›ç¨³å¥æ¨æ–­æ¡†æ¶",
      "authors": [
        "Nathanael Jo",
        "Ashia Wilson"
      ],
      "abstract": "Evaluations of generative models on benchmark data are now ubiquitous, and their outcomes critically shape public and scientific expectations of AI's capabilities. Yet growing skepticism surrounds their reliability. How can we know that a reported accuracy genuinely reflects a model's true performance? Evaluations are often presented as simple measurements, but in reality they are inferences: to treat benchmark scores as evidence of capability is already to assume a theory of what capability is and how it manifests in a test. We make this step explicit by proposing a principled framework for evaluation as inference: begin from a theory of capability, and then derive methods for estimating it. This perspective, familiar in fields such as psychometrics, has not yet become commonplace in AI evaluation. As a proof of concept, we address a central challenge that undermines reliability: sensitivity to perturbations. After formulating a model of ability, we introduce methods that infer ability while accounting for uncertainty from sensitivity and finite samples, including an adaptive algorithm that significantly reduces sample complexity. Together, these contributions lay the groundwork for more reliable and trustworthy estimates of AI capabilities as measured through benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç”Ÿæˆå¼æ¨¡å‹åœ¨åŸºå‡†æµ‹è¯•ä¸­çš„è¯„ä¼°å¯é æ€§é—®é¢˜ï¼ŒæŒ‡å‡ºç›®å‰çš„è¯„ä¼°å¾€å¾€è¢«è¯¯è§†ä¸ºç®€å•çš„æµ‹é‡ï¼Œè€Œæœ¬è´¨ä¸Šåº”è¢«è§†ä¸ºä¸€ç§æ¨ç†è¿‡ç¨‹(inference)ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªå°†è¯„ä¼°è§†ä¸ºæ¨ç†çš„åŸåˆ™æ€§æ¡†æ¶ï¼Œä¸»å¼ ä»èƒ½åŠ›ç†è®ºå‡ºå‘æ¥æ¨å¯¼ä¼°ç®—æ¨¡å‹èƒ½åŠ›çš„æ–¹æ³•ï¼Œè¿™ä¸€è§†è§’å€Ÿé‰´äº†å¿ƒç†æµ‹é‡å­¦(psychometrics)çš„æˆç†Ÿç†è®ºã€‚ä½œä¸ºæ¦‚å¿µéªŒè¯ï¼Œç ”ç©¶é’ˆå¯¹ç ´åè¯„ä¼°å¯é æ€§çš„æ ¸å¿ƒæŒ‘æˆ˜â€”â€”æ‰°åŠ¨æ•æ„Ÿæ€§(sensitivity to perturbations)ï¼Œå¼•å…¥äº†åœ¨è€ƒè™‘æ•æ„Ÿæ€§å’Œæœ‰é™æ ·æœ¬ä¸ç¡®å®šæ€§çš„æƒ…å†µä¸‹æ¨æ–­èƒ½åŠ›çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§è‡ªé€‚åº”ç®—æ³•(adaptive algorithm)ï¼Œèƒ½å¤Ÿæ˜¾è‘—é™ä½è¯„ä¼°æ—¶çš„æ ·æœ¬å¤æ‚åº¦ã€‚è¿™äº›è´¡çŒ®ä¸ºé€šè¿‡åŸºå‡†æµ‹è¯•è·å¾—æ›´å¯é ã€æ›´å…·å…¬ä¿¡åŠ›çš„äººå·¥æ™ºèƒ½èƒ½åŠ›è¯„ä¼°å¥ å®šäº†ç†è®ºä¸æŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19590v1",
      "published_date": "2025-09-23 21:29:04 UTC",
      "updated_date": "2025-09-23 21:29:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:27:55.189910+00:00"
    },
    {
      "arxiv_id": "2509.19587v1",
      "title": "Reverse Engineering User Stories from Code using Large Language Models",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹ä»ä»£ç ä¸­é€†å‘å·¥ç¨‹ç”¨æˆ·æ•…äº‹",
      "authors": [
        "Mohamed Ouf",
        "Haoyu Li",
        "Michael Zhang",
        "Mariam Guizani"
      ],
      "abstract": "User stories are essential in agile development, yet often missing or outdated in legacy and poorly documented systems. We investigate whether large language models (LLMs) can automatically recover user stories directly from source code and how prompt design impacts output quality. Using 1,750 annotated C++ snippets of varying complexity, we evaluate five state-of-the-art LLMs across six prompting strategies. Results show that all models achieve, on average, an F1 score of 0.8 for code up to 200 NLOC. Our findings show that a single illustrative example enables the smallest model (8B) to match the performance of a much larger 70B model. In contrast, structured reasoning via Chain-of-Thought offers only marginal gains, primarily for larger models.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)ä»æºä»£ç ä¸­è‡ªåŠ¨æ¢å¤ç”¨æˆ·æ•…äº‹(User Stories)çš„å¯è¡Œæ€§ï¼Œä»¥è§£å†³é—ç•™ç³»ç»Ÿä¸­éœ€æ±‚æ–‡æ¡£ç¼ºå¤±æˆ–è¿‡æ—¶çš„é—®é¢˜ã€‚ä½œè€…é€šè¿‡1,750ä¸ªä¸åŒå¤æ‚åº¦çš„C++ä»£ç ç‰‡æ®µï¼Œè¯„ä¼°äº†äº”ç§å…ˆè¿›çš„LLMsåœ¨å…­ç§æç¤ºç­–ç•¥ä¸‹çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¤„ç†ä¸è¶…è¿‡200è¡Œ(NLOC)çš„ä»£ç æ—¶ï¼Œå„æ¨¡å‹å¹³å‡èƒ½è¾¾åˆ°0.8çš„F1åˆ†æ•°ã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°ï¼Œä»…é€šè¿‡å•ä¸ªç¤ºä¾‹æç¤ºå³å¯ä½¿8Bè§„æ¨¡çš„å°æ¨¡å‹è¾¾åˆ°ä¸70Bæ¨¡å‹ç›¸å½“çš„æ€§èƒ½æ°´å¹³ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç»“æ„åŒ–çš„é“¾å¼æ€ç»´(Chain-of-Thought)æ¨ç†ç­–ç•¥ä»…èƒ½ä¸ºå¤§å‹æ¨¡å‹å¸¦æ¥å¾®å°çš„æ€§èƒ½å¢ç›Šã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†LLMåœ¨è‡ªåŠ¨åŒ–é€†å‘å·¥ç¨‹ä»»åŠ¡ä¸­çš„æ½œåŠ›ï¼Œä¸ºç»´æŠ¤æ•æ·å¼€å‘è¿‡ç¨‹ä¸­çš„æ–‡æ¡£ä¸€è‡´æ€§æä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19587v1",
      "published_date": "2025-09-23 21:23:37 UTC",
      "updated_date": "2025-09-23 21:23:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:28:18.869322+00:00"
    },
    {
      "arxiv_id": "2509.19586v1",
      "title": "A Foundation Chemical Language Model for Comprehensive Fragment-Based Drug Discovery",
      "title_zh": "é¢å‘å…¨é¢åŸºäºç‰‡æ®µè¯ç‰©å‘ç°çš„åŒ–å­¦è¯­è¨€åŸºç¡€æ¨¡å‹",
      "authors": [
        "Alexander Ho",
        "Sukyeong Lee",
        "Francis T. F. Tsai"
      ],
      "abstract": "We introduce FragAtlas-62M, a specialized foundation model trained on the largest fragment dataset to date. Built on the complete ZINC-22 fragment subset comprising over 62 million molecules, it achieves unprecedented coverage of fragment chemical space. Our GPT-2 based model (42.7M parameters) generates 99.90% chemically valid fragments. Validation across 12 descriptors and three fingerprint methods shows generated fragments closely match the training distribution (all effect sizes < 0.4). The model retains 53.6% of known ZINC fragments while producing 22% novel structures with practical relevance. We release FragAtlas-62M with training code, preprocessed data, documentation, and model weights to accelerate adoption.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† FragAtlas-62Mï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºåŸºäºç‰‡æ®µçš„è¯ç‰©å‘ç° (Fragment-Based Drug Discovery) è®¾è®¡çš„åŸºç¡€åŒ–å­¦è¯­è¨€æ¨¡å‹ã€‚è¯¥æ¨¡å‹åŸºäº GPT-2 æ¶æ„ï¼Œæ‹¥æœ‰ 42.7M ä¸ªå‚æ•°ï¼Œå¹¶åœ¨åŒ…å«è¶…è¿‡ 6200 ä¸‡ä¸ªåˆ†å­çš„ ZINC-22 ç‰‡æ®µå­é›†ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œå®ç°äº†å¯¹ç‰‡æ®µåŒ–å­¦ç©ºé—´çš„å‰æ‰€æœªæœ‰çš„è¦†ç›–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFragAtlas-62M ç”Ÿæˆçš„ç‰‡æ®µåŒ–å­¦æœ‰æ•ˆæ€§é«˜è¾¾ 99.90%ï¼Œä¸”åœ¨ 12 ç§æè¿°ç¬¦å’Œ 3 ç§æŒ‡çº¹æ–¹æ³•éªŒè¯ä¸­ä¸è®­ç»ƒåˆ†å¸ƒé«˜åº¦ä¸€è‡´ã€‚æ¨¡å‹åœ¨ä¿ç•™ 53.6% å·²çŸ¥ ZINC ç‰‡æ®µçš„åŒæ—¶ï¼Œè¿˜äº§ç”Ÿäº† 22% å…·æœ‰å®é™…åº”ç”¨ä»·å€¼çš„æ–°é¢–ç»“æ„ã€‚ç›®å‰ï¼Œè¯¥ç ”ç©¶å·²å…¬å¼€å‘å¸ƒäº†æ¨¡å‹æƒé‡ã€è®­ç»ƒä»£ç å’Œé¢„å¤„ç†æ•°æ®ï¼Œæ—¨åœ¨é€šè¿‡è¿™ä¸€å¼ºå¤§çš„åŒ–å­¦åŸºç¡€æ¨¡å‹åŠ é€Ÿè¯ç‰©ç ”å‘è¿›ç¨‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19586v1",
      "published_date": "2025-09-23 21:23:36 UTC",
      "updated_date": "2025-09-23 21:23:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:28:24.487221+00:00"
    },
    {
      "arxiv_id": "2509.19566v1",
      "title": "Nano Bio-Agents (NBA): Small Language Model Agents for Genomics",
      "title_zh": "Nano Bio-Agents (NBA)ï¼šé¢å‘åŸºå› ç»„å­¦çš„å°è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“",
      "authors": [
        "George Hong",
        "Daniel Trejo Banos"
      ],
      "abstract": "We investigate the application of Small Language Models (<10 billion parameters) for genomics question answering via agentic framework to address hallucination issues and computational cost challenges. The Nano Bio-Agent (NBA) framework we implemented incorporates task decomposition, tool orchestration, and API access into well-established systems such as NCBI and AlphaGenome. Results show that SLMs combined with such agentic framework can achieve comparable and in many cases superior performance versus existing approaches utilising larger models, with our best model-agent combination achieving 98% accuracy on the GeneTuring benchmark. Notably, small 3-10B parameter models consistently achieve 85-97% accuracy while requiring much lower computational resources than conventional approaches. This demonstrates promising potential for efficiency gains, cost savings, and democratization of ML-powered genomics tools while retaining highly robust and accurate performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å°è¯­è¨€æ¨¡å‹ (Small Language Models) åœ¨åŸºå› ç»„å­¦é—®ç­”ä¸­çš„åº”ç”¨ï¼Œå¹¶æå‡ºäº† Nano Bio-Agent (NBA) æ¡†æ¶ä»¥è§£å†³æ¨¡å‹å¹»è§‰å’Œè®¡ç®—æˆæœ¬æŒ‘æˆ˜ã€‚NBA æ¡†æ¶é›†æˆäº†ä»»åŠ¡åˆ†è§£ (task decomposition)ã€å·¥å…·ç¼–æ’ (tool orchestration) ä»¥åŠå¯¹ NCBI å’Œ AlphaGenome ç­‰ç³»ç»Ÿçš„ API è®¿é—®ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»“åˆäº†è¯¥æ™ºèƒ½ä½“æ¡†æ¶çš„ SLMs æ€§èƒ½å¯ä¸å¤§å‹æ¨¡å‹åª²ç¾ï¼Œå…¶ä¸­æœ€ä½³ç»„åˆåœ¨ GeneTuring åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº† 98% çš„å‡†ç¡®ç‡ã€‚ç ”ç©¶å‘ç°ï¼Œ3-10B å‚æ•°çš„æ¨¡å‹èƒ½ä»¥è¿œä½äºä¼ ç»Ÿæ–¹æ³•çš„è®¡ç®—èµ„æºç¨³å®šå®ç° 85-97% çš„å‡†ç¡®ç‡ã€‚è¿™è¯æ˜äº†åœ¨ä¿æŒé«˜åº¦å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œåˆ©ç”¨ SLMs æ˜¾è‘—æå‡åŸºå› ç»„å­¦å·¥å…·æ•ˆç‡ã€é™ä½æˆæœ¬å¹¶æ¨åŠ¨æŠ€æœ¯æ°‘ä¸»åŒ–çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "q-bio.GN"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19566v1",
      "published_date": "2025-09-23 20:44:31 UTC",
      "updated_date": "2025-09-23 20:44:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:28:30.170394+00:00"
    },
    {
      "arxiv_id": "2509.19554v1",
      "title": "Learning Dynamics of Deep Learning -- Force Analysis of Deep Neural Networks",
      "title_zh": "æ·±åº¦å­¦ä¹ åŠ¨åŠ›å­¦ï¼šæ·±åº¦ç¥ç»ç½‘ç»œçš„å—åŠ›åˆ†æ",
      "authors": [
        "Yi Ren"
      ],
      "abstract": "This thesis explores how deep learning models learn over time, using ideas inspired by force analysis. Specifically, we zoom in on the model's training procedure to see how one training example affects another during learning, like analyzing how forces move objects. We break this influence into two parts: how similar the two examples are, and how strong the updating force is. This framework helps us understand a wide range of the model's behaviors in different real systems. For example, it explains why certain examples have non-trivial learning paths, why (and why not) some LLM finetuning methods work, and why simpler, more structured patterns tend to be learned more easily. We apply this approach to various learning tasks and uncover new strategies for improving model training. While the method is still developing, it offers a new way to interpret models' behaviors systematically.",
      "tldr_zh": "è¯¥è®ºæ–‡åˆ©ç”¨å—åŠ›åˆ†æ(Force Analysis)çš„æ€æƒ³æ¢ç´¢äº†æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å­¦ä¹ åŠ¨æ€ï¼Œæ—¨åœ¨æ­ç¤ºè®­ç»ƒè¿‡ç¨‹ä¸­æ ·æœ¬é—´çš„ç›¸äº’å½±å“ã€‚ç ”ç©¶è€…é€šè¿‡å°†è¿™ç§å½±å“åˆ†è§£ä¸ºæ ·æœ¬ç›¸ä¼¼åº¦(similarity)å’Œæ›´æ–°åŠ›(updating force)å¼ºåº¦ä¸¤ä¸ªéƒ¨åˆ†ï¼Œæ„å»ºäº†ä¸€ä¸ªç³»ç»Ÿæ€§çš„åˆ†ææ¡†æ¶ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿåˆç†è§£é‡Šå¤šç§æ¨¡å‹è¡Œä¸ºï¼Œä¾‹å¦‚ç‰¹å®šæ ·æœ¬çš„å­¦ä¹ è·¯å¾„æ¼”å˜ã€å¤§è¯­è¨€æ¨¡å‹(LLM)ä¸åŒå¾®è°ƒæ–¹æ³•çš„è¡¨ç°å·®å¼‚ï¼Œä»¥åŠæ¨¡å‹å¯¹ç®€å•ç»“æ„åŒ–æ¨¡å¼çš„åå¥½ã€‚é€šè¿‡åœ¨å¤šç§å­¦ä¹ ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œè¯¥ç ”ç©¶ä¸ä»…å‘ç°äº†æ”¹è¿›æ¨¡å‹è®­ç»ƒçš„æ–°ç­–ç•¥ï¼Œè¿˜ä¸ºç†è§£æ·±åº¦ç¥ç»ç½‘ç»œçš„å¤æ‚è¡Œä¸ºæä¾›äº†ä¸€ç§åŸºäºç‰©ç†ç›´è§‰çš„æ–°é¢–è§£é‡Šè·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "175 pages",
      "pdf_url": "https://arxiv.org/pdf/2509.19554v1",
      "published_date": "2025-09-23 20:27:19 UTC",
      "updated_date": "2025-09-23 20:27:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:28:29.967897+00:00"
    },
    {
      "arxiv_id": "2509.21387v3",
      "title": "Do Sparse Subnetworks Exhibit Cognitively Aligned Attention? Effects of Pruning on Saliency Map Fidelity, Sparsity, and Concept Coherence",
      "title_zh": "ç¨€ç–å­ç½‘ç»œæ˜¯å¦å±•ç°å‡ºè®¤çŸ¥å¯¹é½çš„æ³¨æ„åŠ›ï¼Ÿå‰ªæå¯¹æ˜¾è‘—å›¾å¿ å®åº¦ã€ç¨€ç–æ€§ä¸æ¦‚å¿µè¿è´¯æ€§çš„å½±å“",
      "authors": [
        "Sanish Suwal",
        "Dipkamal Bhusal",
        "Michael Clifford",
        "Nidhi Rastogi"
      ],
      "abstract": "Prior works have shown that neural networks can be heavily pruned while preserving performance, but the impact of pruning on model interpretability remains unclear. In this work, we investigate how magnitude-based pruning followed by fine-tuning affects both low-level saliency maps and high-level concept representations. Using a ResNet-18 trained on ImageNette, we compare post-hoc explanations from Vanilla Gradients (VG) and Integrated Gradients (IG) across pruning levels, evaluating sparsity and faithfulness. We further apply CRAFT-based concept extraction to track changes in semantic coherence of learned concepts. Our results show that light-to-moderate pruning improves saliency-map focus and faithfulness while retaining distinct, semantically meaningful concepts. In contrast, aggressive pruning merges heterogeneous features, reducing saliency map sparsity and concept coherence despite maintaining accuracy. These findings suggest that while pruning can shape internal representations toward more human-aligned attention patterns, excessive pruning undermines interpretability.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŸºäºé‡çº§çš„å‰ªæ(Magnitude-based pruning)å’Œå¾®è°ƒå¯¹æ¨¡å‹å¯è§£é‡Šæ€§çš„å½±å“ï¼Œé‡ç‚¹åˆ†æäº†å…¶å¯¹ä½å±‚æ˜¾è‘—æ€§å›¾(Saliency maps)å’Œé«˜å±‚æ¦‚å¿µè¡¨ç¤º(Concept representations)çš„ä½œç”¨ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨ImageNetteæ•°æ®é›†ä¸Šä½¿ç”¨ResNet-18æ¨¡å‹ï¼Œåˆ©ç”¨Vanilla Gradients (VG)å’ŒIntegrated Gradients (IG)è¯„ä¼°æ˜¾è‘—æ€§å›¾çš„ç¨€ç–æ€§ä¸å¿ å®åº¦ï¼Œå¹¶ç»“åˆCRAFTæŠ€æœ¯è¿½è¸ªå­¦ä¹ æ¦‚å¿µçš„è¯­ä¹‰è¿è´¯æ€§ã€‚ç»“æœæ˜¾ç¤ºï¼Œè½»åº¦åˆ°ä¸­åº¦çš„å‰ªæèƒ½æœ‰æ•ˆæå‡æ˜¾è‘—æ€§å›¾çš„èšç„¦åº¦ä¸å¿ å®åº¦ï¼ŒåŒæ—¶ä¿ç•™æ¸…æ™°ä¸”å…·æœ‰è¯­ä¹‰æ„ä¹‰çš„æ¦‚å¿µã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œè¿‡åº¦å‰ªæè™½ç„¶èƒ½ç»´æŒå‡†ç¡®ç‡ï¼Œä½†ä¼šå¯¼è‡´å¼‚è´¨ç‰¹å¾åˆå¹¶ï¼Œé™ä½æ˜¾è‘—æ€§å›¾çš„ç¨€ç–æ€§å¹¶ç ´åæ¦‚å¿µè¿è´¯æ€§ã€‚è¿™ä¸€å‘ç°è¡¨æ˜å‰ªæå¯ä»¥ä½¿æ¨¡å‹å†…éƒ¨è¡¨ç¤ºåœ¨ä¸€å®šç¨‹åº¦ä¸Šå®ç°äººç±»è®¤çŸ¥å¯¹é½ï¼Œä½†æç«¯çš„ç¨€ç–åŒ–æœ€ç»ˆä¼šæŸå®³æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "4 pages, neurips workshop",
      "pdf_url": "https://arxiv.org/pdf/2509.21387v3",
      "published_date": "2025-09-23 20:10:23 UTC",
      "updated_date": "2025-10-05 20:06:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:28:32.556439+00:00"
    },
    {
      "arxiv_id": "2509.19538v1",
      "title": "DAWM: Diffusion Action World Models for Offline Reinforcement Learning via Action-Inferred Transitions",
      "title_zh": "DAWMï¼šåŸºäºåŠ¨ä½œæ¨æ–­è½¬ç§»çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ‰©æ•£åŠ¨ä½œä¸–ç•Œæ¨¡å‹",
      "authors": [
        "Zongyue Li",
        "Xiao Han",
        "Yusong Li",
        "Niklas Strauss",
        "Matthias Schubert"
      ],
      "abstract": "Diffusion-based world models have demonstrated strong capabilities in synthesizing realistic long-horizon trajectories for offline reinforcement learning (RL). However, many existing methods do not directly generate actions alongside states and rewards, limiting their compatibility with standard value-based offline RL algorithms that rely on one-step temporal difference (TD) learning. While prior work has explored joint modeling of states, rewards, and actions to address this issue, such formulations often lead to increased training complexity and reduced performance in practice. We propose \\textbf{DAWM}, a diffusion-based world model that generates future state-reward trajectories conditioned on the current state, action, and return-to-go, paired with an inverse dynamics model (IDM) for efficient action inference. This modular design produces complete synthetic transitions suitable for one-step TD-based offline RL, enabling effective and computationally efficient training. Empirically, we show that conservative offline RL algorithms such as TD3BC and IQL benefit significantly from training on these augmented trajectories, consistently outperforming prior diffusion-based baselines across multiple tasks in the D4RL benchmark.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¦»çº¿å¼ºåŒ–å­¦ä¹  (Offline Reinforcement Learning) ä¸­ç°æœ‰æ‰©æ•£ä¸–ç•Œæ¨¡å‹ (Diffusion-based world models) éš¾ä»¥ç›´æ¥ç”ŸæˆåŠ¨ä½œï¼Œå¯¼è‡´æ— æ³•æœ‰æ•ˆé€‚é…æ ‡å‡†çš„ä¸€æ­¥æ—¶åºå·®åˆ† (One-step Temporal Difference) å­¦ä¹ ç®—æ³•çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œä½œè€…æå‡ºäº† DAWM (Diffusion Action World Models)ï¼Œä¸€ç§èƒ½å¤Ÿæ ¹æ®å½“å‰çŠ¶æ€ã€åŠ¨ä½œå’Œå¾…è·å›æŠ¥ (Return-to-go) ç”Ÿæˆæœªæ¥çŠ¶æ€-å¥–åŠ±è½¨è¿¹çš„æ‰©æ•£æ¡†æ¶ã€‚è¯¥æ–¹æ³•å·§å¦™åœ°ç»“åˆäº†ä¸€ä¸ªé€†åŠ¨åŠ›å­¦æ¨¡å‹ (Inverse Dynamics Model) ä»¥å®ç°é«˜æ•ˆçš„åŠ¨ä½œæ¨ç†ï¼Œä»è€Œäº§ç”Ÿé€‚ç”¨äºå¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„å®Œæ•´åˆæˆè½¬æ¢ (Synthetic transitions)ã€‚è¿™ç§æ¨¡å—åŒ–è®¾è®¡ä¸ä»…é™ä½äº†è®­ç»ƒå¤æ‚åº¦ï¼Œè¿˜ä½¿å¾— TD3BC å’Œ IQL ç­‰ç®—æ³•èƒ½å¤Ÿä»å¢å¼ºçš„è½¨è¿¹æ•°æ®ä¸­è·ç›Šã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDAWM åœ¨ D4RL åŸºå‡†æµ‹è¯•çš„å¤šé¡¹ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ€§èƒ½ä¸€è‡´ä¼˜äºæ­¤å‰çš„æ‰©æ•£åŸºå‡†æ¨¡å‹ï¼Œä¸ºé«˜æ•ˆä¸”è®¡ç®—å‹å¥½çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ æä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML2025 workshop Building Physically Plausible World Models",
      "pdf_url": "https://arxiv.org/pdf/2509.19538v1",
      "published_date": "2025-09-23 20:06:26 UTC",
      "updated_date": "2025-09-23 20:06:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:28:34.582438+00:00"
    },
    {
      "arxiv_id": "2509.19533v1",
      "title": "Semantic-Aware Fuzzing: An Empirical Framework for LLM-Guided, Reasoning-Driven Input Mutation",
      "title_zh": "è¯­ä¹‰æ„ŸçŸ¥æ¨¡ç³Šæµ‹è¯•ï¼šå¤§è¯­è¨€æ¨¡å‹å¼•å¯¼ã€æ¨ç†é©±åŠ¨çš„è¾“å…¥å˜å¼‚å®è¯æ¡†æ¶",
      "authors": [
        "Mengdi Lu",
        "Steven Ding",
        "Furkan Alaca",
        "Philippe Charland"
      ],
      "abstract": "Security vulnerabilities in Internet-of-Things devices, mobile platforms, and autonomous systems remain critical. Traditional mutation-based fuzzers -- while effectively explore code paths -- primarily perform byte- or bit-level edits without semantic reasoning. Coverage-guided tools such as AFL++ use dictionaries, grammars, and splicing heuristics to impose shallow structural constraints, leaving deeper protocol logic, inter-field dependencies, and domain-specific semantics unaddressed. Conversely, reasoning-capable large language models (LLMs) can leverage pretraining knowledge to understand input formats, respect complex constraints, and propose targeted mutations, much like an experienced reverse engineer or testing expert. However, lacking ground truth for \"correct\" mutation reasoning makes supervised fine-tuning impractical, motivating explorations of off-the-shelf LLMs via prompt-based few-shot learning. To bridge this gap, we present an open-source microservices framework that integrates reasoning LLMs with AFL++ on Google's FuzzBench, tackling asynchronous execution and divergent hardware demands (GPU- vs. CPU-intensive) of LLMs and fuzzers. We evaluate four research questions: (R1) How can reasoning LLMs be integrated into the fuzzing mutation loop? (R2) Do few-shot prompts yield higher-quality mutations than zero-shot? (R3) Can prompt engineering with off-the-shelf models improve fuzzing directly? and (R4) Which open-source reasoning LLMs perform best under prompt-only conditions? Experiments with Llama3.3, Deepseek-r1-Distill-Llama-70B, QwQ-32B, and Gemma3 highlight Deepseek as the most promising. Mutation effectiveness depends more on prompt complexity and model choice than shot count. Response latency and throughput bottlenecks remain key obstacles, offering directions for future work.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªè¯­ä¹‰æ„ŸçŸ¥çš„æ¨¡ç³Šæµ‹è¯•(Semantic-Aware Fuzzing)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»ŸåŸºäºå˜å¼‚çš„æ¨¡ç³Šæµ‹è¯•å·¥å…·(mutation-based fuzzers)å¦‚AFL++åœ¨å¤„ç†æ·±å±‚åè®®é€»è¾‘å’Œå­—æ®µé—´ä¾èµ–å…³ç³»æ—¶ç¼ºä¹è¯­ä¹‰æ¨ç†çš„é—®é¢˜ã€‚é€šè¿‡å°†å…·å¤‡æ¨ç†èƒ½åŠ›çš„LLMsä¸AFL++é›†æˆï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æç¤ºè¯å·¥ç¨‹(prompt engineering)å’Œå°‘æ ·æœ¬å­¦ä¹ (few-shot learning)å¼•å¯¼è¾“å…¥å˜å¼‚ï¼Œæ¨¡æ‹Ÿç»éªŒä¸°å¯Œçš„é€†å‘å·¥ç¨‹ä¸“å®¶çš„æµ‹è¯•ç­–ç•¥ã€‚ä½œè€…åœ¨Googleçš„FuzzBenchä¸Šå¯¹æ¯”è¯„ä¼°äº†Llama3.3ã€Deepseek-r1-Distill-Llama-70Bã€QwQ-32Bå’ŒGemma3ç­‰æ¨¡å‹ï¼Œå‘ç°Deepseek-r1åœ¨æç¤ºè¯é©±åŠ¨æ¡ä»¶ä¸‹è¡¨ç°æœ€ä¸ºå‡ºè‰²ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå˜å¼‚çš„æœ‰æ•ˆæ€§æ›´å¤šå–å†³äºæç¤ºè¯çš„å¤æ‚åº¦è€Œéç¤ºä¾‹æ•°é‡(shot count)ã€‚å°½ç®¡è¯¥æ–¹æ³•æå‡äº†å˜å¼‚è´¨é‡ï¼Œä½†å“åº”å»¶è¿Ÿå’Œååé‡ç“¶é¢ˆä»æ˜¯å½“å‰é›†æˆæ¨ç†æ¨¡å‹çš„ä¸»è¦æŒ‘æˆ˜ã€‚è¯¥ç ”ç©¶ä¸ºåˆ©ç”¨LLMå¼•å¯¼çš„æ¨ç†é©±åŠ¨å˜å¼‚æä¾›äº†å®è¯åŸºç¡€ï¼Œå¹¶ä¸ºæœªæ¥çš„ä¼˜åŒ–æ–¹å‘æä¾›äº†å‚è€ƒã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19533v1",
      "published_date": "2025-09-23 19:57:29 UTC",
      "updated_date": "2025-09-23 19:57:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:28:43.280730+00:00"
    },
    {
      "arxiv_id": "2509.19524v1",
      "title": "Score the Steps, Not Just the Goal: VLM-Based Subgoal Evaluation for Robotic Manipulation",
      "title_zh": "è¯„ä»·æ­¥éª¤ï¼Œè€Œéä»…çœ‹ç›®æ ‡ï¼šåŸºäº VLM çš„æœºå™¨äººæ“çºµå­ç›®æ ‡è¯„ä¼°",
      "authors": [
        "Ramy ElMallah",
        "Krish Chhajer",
        "Chi-Guhn Lee"
      ],
      "abstract": "Robot learning papers typically report a single binary success rate (SR), which obscures where a policy succeeds or fails along a multi-step manipulation task. We argue that subgoal-level reporting should become routine: for each trajectory, a vector of per-subgoal SRs that makes partial competence visible (e.g., grasp vs. pour). We propose a blueprint for StepEval, a cost-aware plug-in evaluation framework that utilizes vision-language models (VLMs) as automated judges of subgoal outcomes from recorded images or videos. Rather than proposing new benchmarks or APIs, our contribution is to outline design principles for a scalable, community-driven open-source project. In StepEval, the primary artifact for policy evaluation is the per-subgoal SR vector; however, other quantities (e.g., latency or cost estimates) are also considered for framework-optimization diagnostics to help the community tune evaluation efficiency and accuracy when ground-truth subgoal success labels are available. We discuss how such a framework can remain model-agnostic, support single- or multi-view inputs, and be lightweight enough to adopt across labs. The intended contribution is a shared direction: a minimal, extensible seed that invites open-source contributions, so that scoring the steps, not just the final goal, becomes a standard and reproducible practice.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºæœºå™¨äººå­¦ä¹ é¢†åŸŸé€šå¸¸ä»…æŠ¥å‘Šå•ä¸€çš„äºŒå…ƒæˆåŠŸç‡(SR)ï¼Œè¿™æ©ç›–äº†å¤šæ­¥æ“ä½œä»»åŠ¡ä¸­ç­–ç•¥å¤±è´¥çš„å…·ä½“ç¯èŠ‚ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†StepEvalï¼Œä¸€ä¸ªåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)ä½œä¸ºè‡ªåŠ¨åŒ–è¯„åˆ¤è€…çš„å­ç›®æ ‡è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡è®°å½•çš„å›¾åƒæˆ–è§†é¢‘è‡ªåŠ¨è¯„ä¼°å­ç›®æ ‡çš„ç»“æœã€‚ä¸ä¼ ç»Ÿçš„æ•´ä½“æˆåŠŸç‡è¯„ä¼°ä¸åŒï¼Œè¯¥æ¡†æ¶çš„æ ¸å¿ƒäº§å‡ºæ˜¯é’ˆå¯¹æ¯ä¸ªå­ç›®æ ‡çš„æˆåŠŸç‡å‘é‡(per-subgoal SR vector)ï¼Œä½¿è¯¸å¦‚æŠ“å–æˆ–å€¾å€’ç­‰ä¸­é—´é˜¶æ®µçš„ä»»åŠ¡èƒ½åŠ›å¯è§†åŒ–ã€‚StepEvalè¢«è®¾è®¡ä¸ºä¸€ç§æ¨¡å‹æ— å…³(model-agnostic)ã€æ”¯æŒå¤šè§†è§’è¾“å…¥ä¸”è½»é‡åŒ–çš„å¼€æºè¯„ä¼°å·¥å…·ï¼Œæ—¨åœ¨æ¨åŠ¨æœºå™¨äººæ“çºµä»»åŠ¡çš„è¯„ä¼°ä»ä»…å…³æ³¨æœ€ç»ˆç›®æ ‡è½¬å‘å¯¹æ¯ä¸ªæ‰§è¡Œæ­¥éª¤çš„ç²¾ç»†åŒ–è¯„åˆ†ã€‚é€šè¿‡ç»“åˆå»¶è¿Ÿå’Œæˆæœ¬ä¼°ç®—ç­‰è¯Šæ–­æŒ‡æ ‡ï¼Œè¯¥æ¡†æ¶ä¸ºç¤¾åŒºæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•ä¸”æ ‡å‡†åŒ–çš„æ–¹æ¡ˆï¼Œä»è€Œæå‡æœºå™¨äººç­–ç•¥è¯„ä¼°çš„é€æ˜åº¦ä¸å¯é‡å¤æ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to the CoRL 2025 Eval&Deploy Workshop",
      "pdf_url": "https://arxiv.org/pdf/2509.19524v1",
      "published_date": "2025-09-23 19:42:14 UTC",
      "updated_date": "2025-09-23 19:42:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:28:42.295153+00:00"
    },
    {
      "arxiv_id": "2509.19517v2",
      "title": "Cognitive Load Limits in Large Language Models: Benchmarking Multi-Hop Reasoning",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ä¸­çš„è®¤çŸ¥è´Ÿè·æé™ï¼šå¤šè·³æ¨ç†åŸºå‡†æµ‹è¯•",
      "authors": [
        "Sai Teja Reddy Adapala"
      ],
      "abstract": "The scaling of Large Language Models (LLMs) has exposed a critical gap between their performance on static benchmarks and their fragility in dynamic, information-rich environments. While models excel at isolated tasks, the computational limits that govern their reasoning under cognitive load remain poorly understood. In this work, we introduce a formal theory of computational cognitive load, positing that extraneous, task-irrelevant information (Context Saturation) and interference from task-switching (Attentional Residue) are key mechanisms that degrade performance. We designed the Interleaved Cognitive Evaluation (ICE), a deconfounded benchmark to systematically manipulate these load factors on challenging multi-hop reasoning tasks. A comprehensive study (N = 10 replications per item across 200 questions) revealed significant performance variations across five instruction-tuned models. Smaller open-source architectures (Llama-3-8B-Instruct, Mistral-7B-Instruct-v0.2) exhibited baseline brittleness, achieving 0% accuracy (SEM = 0.0) across all conditions, including clean controls, on this high-intrinsic-load task. In contrast, Gemini-2.0-Flash-001 showed partial resilience, achieving 85% accuracy in control conditions, with a statistically significant degradation under context saturation ($Î²= -0.003$ per % load, $p < 0.001$). These findings provide preliminary evidence that cognitive load is a key contributor to reasoning failures, supporting theories of hallucination-as-guessing under uncertainty. We conclude that dynamic, cognitive-aware stress testing, as exemplified by the ICE benchmark, is essential for evaluating the true resilience and safety of advanced AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†åŠ¨æ€ã€é«˜ä¿¡æ¯é‡ä»»åŠ¡æ—¶é¢ä¸´çš„è®¤çŸ¥è´Ÿè·é™åˆ¶é—®é¢˜ã€‚ç ”ç©¶è€…æå‡ºäº†ä¸€å¥—å½¢å¼åŒ–çš„è®¡ç®—è®¤çŸ¥è´Ÿè·(computational cognitive load)ç†è®ºï¼Œé‡ç‚¹åˆ†æäº†ä¸Šä¸‹æ–‡é¥±å’Œ(Context Saturation)å’Œæ³¨æ„åŠ›æ®‹ç•™(Attentional Residue)å¯¹æ¨¡å‹æ¨ç†èƒ½åŠ›çš„é™çº§æœºåˆ¶ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å¼€å‘äº†äº¤æ›¿è®¤çŸ¥è¯„ä¼°(Interleaved Cognitive Evaluation, ICE)åŸºå‡†ï¼Œç”¨äºåœ¨å¤æ‚çš„å¤šæ­¥æ¨ç†(multi-hop reasoning)ä»»åŠ¡ä¸­ç³»ç»Ÿåœ°æ“çºµè¿™äº›è´Ÿè·å› å­ã€‚å®éªŒæµ‹è¯•äº†äº”ç§æŒ‡ä»¤å¾®è°ƒæ¨¡å‹ï¼Œå‘ç°Llama-3-8B-Instructå’ŒMistral-7B-Instruct-v0.2ç­‰å°å‹å¼€æºæ¨¡å‹åœ¨å¤„ç†æ­¤ç±»é«˜å†…åœ¨è´Ÿè·ä»»åŠ¡æ—¶å‡†ç¡®ç‡é™ä¸º0%ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒGemini-2.0-Flash-001è¡¨ç°å‡ºéƒ¨åˆ†éŸ§æ€§ï¼Œä½†åœ¨ä¸Šä¸‹æ–‡é¥±å’Œå¢åŠ æ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚ç ”ç©¶ç»“æœè¡¨æ˜è®¤çŸ¥è´Ÿè·æ˜¯å¯¼è‡´æ¨ç†å¤±è´¥çš„å…³é”®å› ç´ ï¼Œå¹¶æ”¯æŒäº†â€œå¹»è§‰å³ä¸ç¡®å®šæ€§ä¸‹çš„çŒœæµ‹â€è¿™ä¸€ç†è®ºã€‚ç ”ç©¶æœ€åå¼ºè°ƒï¼Œé‡‡ç”¨ç±»ä¼¼ICEçš„è®¤çŸ¥æ„ŸçŸ¥å‹åŠ›æµ‹è¯•å¯¹äºè¯„ä¼°äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„çœŸå®å¯é æ€§ä¸å®‰å…¨æ€§è‡³å…³é‡è¦ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19517v2",
      "published_date": "2025-09-23 19:36:56 UTC",
      "updated_date": "2025-09-25 21:42:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:28:49.091938+00:00"
    },
    {
      "arxiv_id": "2509.19515v3",
      "title": "A Longitudinal Randomized Control Study of Companion Chatbot Use: Anthropomorphism and Its Mediating Role on Social Impacts",
      "title_zh": "é™ªä¼´å‹èŠå¤©æœºå™¨äººä½¿ç”¨çš„çºµå‘éšæœºå¯¹ç…§ç ”ç©¶ï¼šæ‹ŸäººåŒ–åŠå…¶å¯¹ç¤¾ä¼šå½±å“çš„ä¸­ä»‹ä½œç”¨",
      "authors": [
        "Rose E. Guingrich",
        "Michael S. A. Graziano"
      ],
      "abstract": "Many Large Language Model (LLM) chatbots are designed and used for companionship, and people have reported forming friendships, mentorships, and romantic partnerships with them. Concerns that companion chatbots may harm or replace real human relationships have been raised, but whether and how these social consequences occur remains unclear. In the present longitudinal study ($N = 183$), participants were randomly assigned to a chatbot condition (text chat with a companion chatbot) or to a control condition (text-based word games) for 10 minutes a day for 21 days. Participants also completed four surveys during the 21 days and engaged in audio recorded interviews on day 1 and 21. Overall, social health and relationships were not significantly impacted by companion chatbot interactions across 21 days of use. However, a detailed analysis showed a different story. People who had a higher desire to socially connect also tended to anthropomorphize the chatbot more, attributing humanlike properties to it; and those who anthropomorphized the chatbot more also reported that talking to the chatbot had a greater impact on their social interactions and relationships with family and friends. Via a mediation analysis, our results suggest a key mechanism at work: the impact of human-AI interaction on human-human social outcomes is mediated by the extent to which people anthropomorphize the AI agent, which is in turn motivated by a desire to socially connect. In a world where the desire to socially connect is on the rise, this finding may be cause for concern.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡ä¸€é¡¹ä¸ºæœŸ21å¤©çš„çºµå‘éšæœºå¯¹ç…§å®éªŒï¼ˆLongitudinal Randomized Control Studyï¼‰ï¼Œæ¢è®¨äº†ä¼´ä¾£èŠå¤©æœºå™¨äººï¼ˆCompanion Chatbotï¼‰çš„ä½¿ç”¨å¯¹ç”¨æˆ·ç¤¾äº¤å¥åº·çš„å½±å“åŠå…¶èƒŒåçš„å¿ƒç†æœºåˆ¶ã€‚ç ”ç©¶å°†183åå‚ä¸è€…éšæœºåˆ†é…è‡³æœºå™¨äººäº’åŠ¨ç»„æˆ–æ§åˆ¶ç»„ï¼Œå¹¶é€šè¿‡ä¸­ä»‹æ•ˆåº”åˆ†æï¼ˆMediation Analysisï¼‰è€ƒå¯Ÿäº†æ‹ŸäººåŒ–ï¼ˆAnthropomorphismï¼‰åœ¨å…¶ä¸­çš„ä½œç”¨ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶æ•´ä½“ç¤¾äº¤å¥åº·åœ¨çŸ­æœŸå†…æœªå‡ºç°æ˜¾è‘—æ³¢åŠ¨ï¼Œä½†ç¤¾äº¤è¿æ¥æ„¿æœ›è¾ƒå¼ºçš„ä¸ªä½“æ›´å€¾å‘äºå°†æœºå™¨äººæ‹ŸäººåŒ–ï¼Œè€Œè¿™ç§é«˜åº¦çš„æ‹ŸäººåŒ–æ„ŸçŸ¥æ˜¾è‘—å½±å“äº†ç”¨æˆ·å¯¹ç°å®äººé™…å…³ç³»çš„è®¤çŸ¥ã€‚ç ”ç©¶æ­ç¤ºäº†äººç±»ä¸äººå·¥æ™ºèƒ½äº’åŠ¨ï¼ˆHuman-AI Interactionï¼‰å½±å“ç°å®ç¤¾äº¤ç»“æœçš„å…³é”®è·¯å¾„ï¼Œå³ç¤¾äº¤éœ€æ±‚é©±åŠ¨æ‹ŸäººåŒ–æ„ŸçŸ¥ï¼Œè¿›è€Œä»‹å¯¼äº†å¯¹äººé™…å…³ç³»çš„ç¤¾ä¼šå½±å“ã€‚è¿™ä¸€å‘ç°ä¸ºè¯„ä¼°ä¼´ä¾£èŠå¤©æœºå™¨äººåœ¨ç°ä»£ç¤¾äº¤ç¯å¢ƒä¸­çš„æ½œåœ¨é£é™©å’Œå¿ƒç†æœºåˆ¶æä¾›äº†é‡è¦çš„ç†è®ºä¾æ®ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19515v3",
      "published_date": "2025-09-23 19:33:41 UTC",
      "updated_date": "2025-10-13 18:34:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:29:05.083334+00:00"
    },
    {
      "arxiv_id": "2509.19512v1",
      "title": "The Heterogeneous Multi-Agent Challenge",
      "title_zh": "å¼‚æ„å¤šæ™ºèƒ½ä½“æŒ‘æˆ˜",
      "authors": [
        "Charles Dansereau",
        "Junior-Samuel Lopez-Yepez",
        "Karthik Soma",
        "Antoine Fagette"
      ],
      "abstract": "Multi-Agent Reinforcement Learning (MARL) is a growing research area which gained significant traction in recent years, extending Deep RL applications to a much wider range of problems. A particularly challenging class of problems in this domain is Heterogeneous Multi-Agent Reinforcement Learning (HeMARL), where agents with different sensors, resources, or capabilities must cooperate based on local information. The large number of real-world situations involving heterogeneous agents makes it an attractive research area, yet underexplored, as most MARL research focuses on homogeneous agents (e.g., a swarm of identical robots). In MARL and single-agent RL, standardized environments such as ALE and SMAC have allowed to establish recognized benchmarks to measure progress. However, there is a clear lack of such standardized testbed for cooperative HeMARL. As a result, new research in this field often uses simple environments, where most algorithms perform near optimally, or uses weakly heterogeneous MARL environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹  (Multi-Agent Reinforcement Learning, MARL) é¢†åŸŸä¸­å…·æœ‰é«˜åº¦æŒ‘æˆ˜æ€§çš„å¼‚æ„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹  (Heterogeneous Multi-Agent Reinforcement Learning, HeMARL) é—®é¢˜ï¼Œå³ä¸åŒä¼ æ„Ÿå™¨ã€èµ„æºæˆ–èƒ½åŠ›çš„æ™ºèƒ½ä½“å¦‚ä½•åŸºäºå±€éƒ¨ä¿¡æ¯å®ç°é«˜æ•ˆåä½œã€‚ä½œè€…æŒ‡å‡ºï¼Œå°½ç®¡å¼‚æ„å¤šæ™ºèƒ½ä½“åœ¨ç°å®åº”ç”¨ä¸­éœ€æ±‚å¹¿æ³›ï¼Œä½†ç›®å‰çš„ MARL ç ”ç©¶ä»ä¸»è¦é›†ä¸­åœ¨åŒæ„æ™ºèƒ½ä½“ä¸Šï¼Œå¯¼è‡´ HeMARL é¢†åŸŸå°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚è®ºæ–‡å¼ºè°ƒï¼Œç›¸è¾ƒäºå•æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ å’ŒåŒæ„ MARL æ‹¥æœ‰çš„ ALE å’Œ SMAC ç­‰æˆç†ŸåŸºå‡†ï¼Œåä½œå¼ HeMARL æ˜¾è‘—ç¼ºä¹æ ‡å‡†åŒ–çš„æµ‹è¯•å¹³å°ã€‚è¿™ç§ç°çŠ¶è¿«ä½¿ç°æœ‰ç ”ç©¶ä¸å¾—ä¸ä¾èµ–ç®€å•çš„å®éªŒç¯å¢ƒæˆ–å¼±å¼‚æ„ç¯å¢ƒï¼Œéš¾ä»¥å‡†ç¡®è¡¡é‡ç®—æ³•çš„å®é™…è¿›æ­¥ã€‚è¯¥è®ºæ–‡é€šè¿‡ç³»ç»Ÿæ€§åœ°é˜æ˜è¿™ä¸€æŒ‘æˆ˜ï¼Œæ—¨åœ¨æ¨åŠ¨å­¦æœ¯ç•Œå»ºç«‹æ›´å…·ä»£è¡¨æ€§çš„ HeMARL åŸºå‡†æµ‹è¯•ä½“ç³»ï¼Œä»¥ä¿ƒè¿›è¯¥é¢†åŸŸçš„å®è´¨æ€§å‘å±•ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "7 pages. To Appear at ECAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.19512v1",
      "published_date": "2025-09-23 19:30:30 UTC",
      "updated_date": "2025-09-23 19:30:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:29:23.382789+00:00"
    },
    {
      "arxiv_id": "2509.19509v1",
      "title": "AIRwaves at CheckThat! 2025: Retrieving Scientific Sources for Implicit Claims on Social Media with Dual Encoders and Neural Re-Ranking",
      "title_zh": "AIRwaves at CheckThat! 2025ï¼šç»“åˆåŒç¼–ç å™¨ä¸ç¥ç»é‡æ’åºçš„ç¤¾äº¤åª’ä½“éšæ€§æ–­è¨€ç§‘å­¦æ–‡çŒ®æ£€ç´¢",
      "authors": [
        "Cem Ashbaugh",
        "Leon BaumgÃ¤rtner",
        "Tim Gress",
        "Nikita Sidorov",
        "Daniel Werner"
      ],
      "abstract": "Linking implicit scientific claims made on social media to their original publications is crucial for evidence-based fact-checking and scholarly discourse, yet it is hindered by lexical sparsity, very short queries, and domain-specific language. Team AIRwaves ranked second in Subtask 4b of the CLEF-2025 CheckThat! Lab with an evidence-retrieval approach that markedly outperforms the competition baseline. The optimized sparse-retrieval baseline(BM25) achieves MRR@5 = 0.5025 on the gold label blind test set. To surpass this baseline, a two-stage retrieval pipeline is introduced: (i) a first stage that uses a dual encoder based on E5-large, fine-tuned using in-batch and mined hard negatives and enhanced through chunked tokenization and rich document metadata; and (ii) a neural re-ranking stage using a SciBERT cross-encoder. Replacing purely lexical matching with neural representations lifts performance to MRR@5 = 0.6174, and the complete pipeline further improves to MRR@5 = 0.6828. The findings demonstrate that coupling dense retrieval with neural re-rankers delivers a powerful and efficient solution for tweet-to-study matching and provides a practical blueprint for future evidence-retrieval pipelines.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº†AIRwaveså›¢é˜Ÿåœ¨CLEF-2025 CheckThat! Labè¯„æµ‹ä¸­çš„æˆæœï¼Œæ—¨åœ¨è§£å†³å°†ç¤¾äº¤åª’ä½“éšå«ç§‘å­¦ä¸»å¼ ä¸åŸå§‹è®ºæ–‡å…³è”æ—¶é¢ä¸´çš„è¯æ±‡ç¨€ç–å’ŒçŸ­æŸ¥è¯¢æŒ‘æˆ˜ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µæ£€ç´¢æµæ°´çº¿ï¼Œç¬¬ä¸€é˜¶æ®µé‡‡ç”¨åŸºäºE5-largeçš„å¯¹ç­‰ç¼–ç å™¨(Dual Encoder)ï¼Œå¹¶ç»“åˆç¡¬è´Ÿé‡‡æ ·å¾®è°ƒã€åˆ†å—åˆ†è¯ä¸æ–‡æ¡£å…ƒæ•°æ®è¿›è¡Œå¢å¼ºã€‚ç¬¬äºŒé˜¶æ®µä½¿ç”¨SciBERTäº¤å‰ç¼–ç å™¨è¿›è¡Œç¥ç»é‡æ’åº(Neural Re-ranking)ï¼Œå®ç°äº†ä»çº¯è¯æ±‡åŒ¹é…åˆ°ç¥ç»è¡¨ç¤ºçš„æ€§èƒ½è·ƒå‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥ç³»ç»Ÿå°†MRR@5ä»åŸºçº¿çš„0.5025æå‡è‡³0.6828ï¼Œåœ¨ç›¸å…³å­ä»»åŠ¡ä¸­æ’åç¬¬äºŒã€‚è¯¥ç ”ç©¶è¯æ˜äº†ç¨ å¯†æ£€ç´¢ä¸ç¥ç»é‡æ’åºå™¨çš„ç»“åˆåœ¨æ¨æ–‡-ç ”ç©¶åŒ¹é…(tweet-to-study matching)ä¸­çš„å¼ºå¤§æ•ˆèƒ½ï¼Œä¸ºè¯æ®æ£€ç´¢æµæ°´çº¿æä¾›äº†å®è·µè“å›¾ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "CLEF 2025 (Conference and Labs of the Evaluation Forum)",
      "pdf_url": "https://arxiv.org/pdf/2509.19509v1",
      "published_date": "2025-09-23 19:26:31 UTC",
      "updated_date": "2025-09-23 19:26:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:29:26.556140+00:00"
    },
    {
      "arxiv_id": "2509.19497v1",
      "title": "Generative AI as a catalyst for democratic Innovation: Enhancing citizen engagement in participatory budgeting",
      "title_zh": "ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åŠ©åŠ›æ°‘ä¸»åˆ›æ–°ï¼šæå‡å‚ä¸å¼é¢„ç®—ä¸­çš„å…¬æ°‘å‚ä¸",
      "authors": [
        "Italo Alberto do Nascimento Sousa",
        "Jorge Machado",
        "Jose Carlos Vaz"
      ],
      "abstract": "This research examines the role of Generative Artificial Intelligence (AI) in enhancing citizen engagement in participatory budgeting. In response to challenges like declining civic participation and increased societal polarization, the study explores how online political participation can strengthen democracy and promote social equity. By integrating Generative AI into public consultation platforms, the research aims to improve citizen proposal formulation and foster effective dialogue between citizens and government. It assesses the capacities governments need to implement AI-enhanced participatory tools, considering technological dependencies and vulnerabilities. Analyzing technological structures, actors, interests, and strategies, the study contributes to understanding how technological advancements can reshape participatory institutions to better facilitate citizen involvement. Ultimately, the research highlights how Generative AI can transform participatory institutions, promoting inclusive, democratic engagement and empowering citizens.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (Generative AI) åœ¨å¢å¼ºå‚ä¸å¼é¢„ç®— (participatory budgeting) ä¸­å…¬æ°‘å‚ä¸çš„ä½œç”¨ï¼Œæ—¨åœ¨åº”å¯¹å…¬æ°‘å‚ä¸åº¦ä¸‹é™å’Œç¤¾ä¼šä¸¤æåˆ†åŒ–ç­‰ç°å®æŒ‘æˆ˜ã€‚é€šè¿‡å°† Generative AI æ•´åˆåˆ°å…¬å…±å’¨è¯¢å¹³å°ä¸­ï¼Œç ”ç©¶æ—¨åœ¨ä¼˜åŒ–å…¬æ°‘ææ¡ˆçš„åˆ¶å®šè¿‡ç¨‹ï¼Œå¹¶ä¿ƒè¿›å…¬æ°‘ä¸æ”¿åºœä¹‹é—´çš„æœ‰æ•ˆå¯¹è¯ã€‚ç ”ç©¶è¯¦ç»†è¯„ä¼°äº†æ”¿åºœåœ¨å®æ–½è¿™äº› AI å¢å¼ºå‹å‚ä¸å¼å·¥å…·æ—¶æ‰€éœ€çš„èƒ½åŠ›ï¼Œå¹¶å…¨é¢è€ƒé‡äº†æ½œåœ¨çš„æŠ€æœ¯ä¾èµ–æ€§ä¸ç³»ç»Ÿè„†å¼±æ€§ã€‚é€šè¿‡åˆ†ææŠ€æœ¯ç»“æ„ã€å‚ä¸ä¸»ä½“ã€åˆ©ç›Šå…³ç³»å’Œå®æ–½ç­–ç•¥ï¼Œè¯¥ç ”ç©¶æ­ç¤ºäº†æŠ€æœ¯è¿›æ­¥å¦‚ä½•é‡å¡‘å‚ä¸å¼æœºæ„ä»¥æ›´å¥½åœ°å¼•å¯¼å…¬ä¼—ä»‹å…¥ã€‚æœ€ç»ˆï¼Œç ”ç©¶å¼ºè°ƒäº† Generative AI åœ¨è½¬åŒ–å‚ä¸å¼æœºæ„ã€æ¨åŠ¨åŒ…å®¹æ€§æ°‘ä¸»å‚ä¸ä»¥åŠèµ‹æƒå…¬æ°‘æ–¹é¢çš„å…³é”®æ½œåŠ›ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "19 pages, VI International Meeting on Participation, Democracy and Public Policies",
      "pdf_url": "https://arxiv.org/pdf/2509.19497v1",
      "published_date": "2025-09-23 19:09:31 UTC",
      "updated_date": "2025-09-23 19:09:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:29:25.162175+00:00"
    },
    {
      "arxiv_id": "2509.19495v1",
      "title": "ArtiFree: Detecting and Reducing Generative Artifacts in Diffusion-based Speech Enhancement",
      "title_zh": "ArtiFreeï¼šåŸºäºæ‰©æ•£æ¨¡å‹çš„è¯­éŸ³å¢å¼ºä¸­ç”Ÿæˆä¼ªå½±çš„æ£€æµ‹ä¸å‡å°‘",
      "authors": [
        "Bhawana Chhaglani",
        "Yang Gao",
        "Julius Richter",
        "Xilin Li",
        "Syavosh Zadissa",
        "Tarun Pruthi",
        "Andrew Lovitt"
      ],
      "abstract": "Diffusion-based speech enhancement (SE) achieves natural-sounding speech and strong generalization, yet suffers from key limitations like generative artifacts and high inference latency. In this work, we systematically study artifact prediction and reduction in diffusion-based SE. We show that variance in speech embeddings can be used to predict phonetic errors during inference. Building on these findings, we propose an ensemble inference method guided by semantic consistency across multiple diffusion runs. This technique reduces WER by 15% in low-SNR conditions, effectively improving phonetic accuracy and semantic plausibility. Finally, we analyze the effect of the number of diffusion steps, showing that adaptive diffusion steps balance artifact suppression and latency. Our findings highlight semantic priors as a powerful tool to guide generative SE toward artifact-free outputs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ArtiFreeï¼Œæ—¨åœ¨è§£å†³åŸºäºæ‰©æ•£æ¨¡å‹ (Diffusion-based) çš„è¯­éŸ³å¢å¼º (Speech Enhancement) ä¸­å­˜åœ¨çš„ç”Ÿæˆä¼ªå½± (Generative Artifacts) å’Œé«˜æ¨ç†å»¶è¿Ÿé—®é¢˜ã€‚ç ”ç©¶é€šè¿‡åˆ†æå‘ç°ï¼Œè¯­éŸ³åµŒå…¥ (Speech Embeddings) çš„æ–¹å·®å¯æœ‰æ•ˆé¢„æµ‹æ¨ç†è¿‡ç¨‹ä¸­çš„éŸ³ç´ é”™è¯¯ (Phonetic Errors)ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œä½œè€…æå‡ºäº†ä¸€ç§å—å¤šè½®æ‰©æ•£è¿è¡Œé—´è¯­ä¹‰ä¸€è‡´æ€§ (Semantic Consistency) å¼•å¯¼çš„é›†æˆæ¨ç†æ–¹æ³•ï¼Œåœ¨ä½ä¿¡å™ªæ¯” (SNR) æ¡ä»¶ä¸‹å°†å•è¯é”™è¯¯ç‡ (WER) é™ä½äº† 15%ï¼Œæ˜¾è‘—æå‡äº†è¯­éŸ³çš„éŸ³ç´ å‡†ç¡®æ€§å’Œè¯­ä¹‰åˆç†æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ¢è®¨äº†æ‰©æ•£æ­¥æ•°å¯¹æ€§èƒ½çš„å½±å“ï¼Œè¯æ˜è‡ªé€‚åº”æ‰©æ•£æ­¥æ•° (Adaptive Diffusion Steps) èƒ½å¤Ÿå¾ˆå¥½åœ°å¹³è¡¡ä¼ªå½±æŠ‘åˆ¶ä¸æ¨ç†å»¶è¿Ÿã€‚è¯¥æˆæœè¡¨æ˜è¯­ä¹‰å…ˆéªŒ (Semantic Priors) æ˜¯å¼•å¯¼ç”Ÿæˆå¼è¯­éŸ³å¢å¼ºä»»åŠ¡å®ç°æ— ä¼ªå½±è¾“å‡ºçš„å¼ºå¤§å·¥å…·ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19495v1",
      "published_date": "2025-09-23 19:04:18 UTC",
      "updated_date": "2025-09-23 19:04:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:29:29.785061+00:00"
    },
    {
      "arxiv_id": "2509.19489v1",
      "title": "Estimating the Self-Consistency of LLMs",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹è‡ªä¸€è‡´æ€§çš„ä¼°è®¡",
      "authors": [
        "Robert Nowak"
      ],
      "abstract": "Systems often repeat the same prompt to large language models (LLMs) and aggregate responses to improve reliability. This short note analyzes an estimator of the self-consistency of LLMs and the tradeoffs it induces under a fixed compute budget $B=mn$, where $m$ is the number of prompts sampled from the task distribution and $n$ is the number of repeated LLM calls per prompt; the resulting analysis favors a rough split $m,n\\propto\\sqrt{B}$.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„è‡ªä¸€è‡´æ€§ (self-consistency) ä¼°ç®—å™¨åŠå…¶åœ¨å›ºå®šè®¡ç®—é¢„ç®— $B=mn$ ä¸‹å¼•å‘çš„æ€§èƒ½æƒè¡¡ã€‚ä½œè€…é€šè¿‡æ•°å­¦åˆ†æï¼Œç ”ç©¶äº†ä»ä»»åŠ¡åˆ†å¸ƒä¸­é‡‡æ ·çš„æç¤ºè¯æ•°é‡ $m$ ä¸æ¯ä¸ªæç¤ºè¯çš„é‡å¤è°ƒç”¨æ¬¡æ•° $n$ ä¹‹é—´çš„å…³ç³»ã€‚ç ”ç©¶é‡ç‚¹åœ¨äºå¦‚ä½•åœ¨è®¡ç®—èµ„æºå—é™çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡ä¼˜åŒ– $m$ å’Œ $n$ çš„åˆ†é…æ¥æé«˜æ¨¡å‹è¾“å‡ºçš„å¯é æ€§ã€‚åˆ†æç»“æœè¡¨æ˜ï¼Œå½“ $m$ å’Œ $n$ å‡ä¸ $\\sqrt{B}$ æˆæ¯”ä¾‹æ—¶ï¼Œå¯ä»¥å®ç°è¾ƒä¸ºç†æƒ³çš„èµ„æºåˆ†å‰²ã€‚è¿™ä¸€å‘ç°ä¸ºç ”ç©¶äººå‘˜åœ¨å®é™…åº”ç”¨ä¸­å¹³è¡¡é‡‡æ ·å¹¿åº¦ä¸é‡å¤æ·±åº¦æä¾›äº†é‡è¦çš„ç†è®ºä¾æ®ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "5 pages",
      "pdf_url": "https://arxiv.org/pdf/2509.19489v1",
      "published_date": "2025-09-23 18:51:56 UTC",
      "updated_date": "2025-09-23 18:51:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:29:31.869223+00:00"
    },
    {
      "arxiv_id": "2509.19485v1",
      "title": "Identifying and Addressing User-level Security Concerns in Smart Homes Using \"Smaller\" LLMs",
      "title_zh": "åŸºäºâ€œè½»é‡çº§â€å¤§è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½å®¶å±…ç”¨æˆ·çº§å®‰å…¨é—®é¢˜è¯†åˆ«ä¸åº”å¯¹",
      "authors": [
        "Hafijul Hoque Chowdhury",
        "Riad Ahmed Anonto",
        "Sourov Jajodia",
        "Suryadipta Majumdar",
        "Md. Shohrab Hossain"
      ],
      "abstract": "With the rapid growth of smart home IoT devices, users are increasingly exposed to various security risks, as evident from recent studies. While seeking answers to know more on those security concerns, users are mostly left with their own discretion while going through various sources, such as online blogs and technical manuals, which may render higher complexity to regular users trying to extract the necessary information. This requirement does not go along with the common mindsets of smart home users and hence threatens the security of smart homes furthermore. In this paper, we aim to identify and address the major user-level security concerns in smart homes. Specifically, we develop a novel dataset of Q&A from public forums, capturing practical security challenges faced by smart home users. We extract major security concerns in smart homes from our dataset by leveraging the Latent Dirichlet Allocation (LDA). We fine-tune relatively \"smaller\" transformer models, such as T5 and Flan-T5, on this dataset to build a QA system tailored for smart home security. Unlike larger models like GPT and Gemini, which are powerful but often resource hungry and require data sharing, smaller models are more feasible for deployment in resource-constrained or privacy-sensitive environments like smart homes. The dataset is manually curated and supplemented with synthetic data to explore its potential impact on model performance. This approach significantly improves the system's ability to deliver accurate and relevant answers, helping users address common security concerns with smart home IoT devices. Our experiments on real-world user concerns show that our work improves the performance of the base models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ™ºèƒ½å®¶å±…ç”¨æˆ·éš¾ä»¥ä»æµ·é‡åœ¨çº¿ä¿¡æ¯ä¸­æœ‰æ•ˆæå–å®‰å…¨çŸ¥è¯†çš„ç°çŠ¶ï¼Œæå‡ºäº†ä¸€ç§è¯†åˆ«å¹¶è§£å†³ç”¨æˆ·å±‚é¢å®‰å…¨é¡¾è™‘çš„æ–¹æ³•ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨ Latent Dirichlet Allocation (LDA) æŠ€æœ¯ä»å…¬å…±è®ºå›ä¸­æå–æ ¸å¿ƒå®‰å…¨æŒ‘æˆ˜ï¼Œå¹¶æ®æ­¤æ„å»ºäº†ä¸€ä¸ªå…¨æ–°çš„é—®ç­”æ•°æ®é›†ã€‚é€šè¿‡å¯¹ T5 å’Œ Flan-T5 ç­‰è¾ƒå°çš„ Transformer æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œç ”ç©¶è€…å¼€å‘äº†ä¸€å¥—ä¸“ä¸ºæ™ºèƒ½å®¶å±…ç¯å¢ƒè®¾è®¡çš„é—®ç­”ç³»ç»Ÿã€‚ç›¸è¾ƒäº GPT å’Œ Gemini ç­‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œè¿™äº›â€œè¾ƒå°â€çš„æ¨¡å‹èƒ½æ›´å¥½åœ°é€‚åº”æ™ºèƒ½å®¶å±…å¯¹èµ„æºé™åˆ¶å’Œéšç§ä¿æŠ¤çš„ä¸¥è‹›è¦æ±‚ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜é€šè¿‡äººå·¥ç­–åˆ’ä¸åˆæˆæ•°æ® (synthetic data) çš„ç»“åˆè¿›ä¸€æ­¥ä¼˜åŒ–äº†æ¨¡å‹è¡¨ç°ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥ç³»ç»Ÿæ˜¾è‘—æå‡äº†åŸºç¡€æ¨¡å‹åœ¨å¤„ç†å®é™…å®‰å…¨é—®é¢˜æ—¶çš„å‡†ç¡®æ€§ä¸ç›¸å…³æ€§ï¼Œä¸ºç”¨æˆ·åº”å¯¹æ™ºèƒ½å®¶å±… IoT è®¾å¤‡çš„å®‰å…¨é£é™©æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "10 pages, accepted at PST 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.19485v1",
      "published_date": "2025-09-23 18:47:59 UTC",
      "updated_date": "2025-09-23 18:47:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:29:39.983702+00:00"
    },
    {
      "arxiv_id": "2510.01237v1",
      "title": "Confidence-Aware Routing for Large Language Model Reliability Enhancement: A Multi-Signal Approach to Pre-Generation Hallucination Mitigation",
      "title_zh": "å¢å¼ºå¤§è¯­è¨€æ¨¡å‹å¯é æ€§çš„ç½®ä¿¡åº¦æ„ŸçŸ¥è·¯ç”±ï¼šä¸€ç§ç¼“è§£ç”Ÿæˆå‰å¹»è§‰çš„å¤šä¿¡å·æ–¹æ³•",
      "authors": [
        "Nandakishor M"
      ],
      "abstract": "Large Language Models suffer from hallucination, generating plausible yet factually incorrect content. Current mitigation strategies focus on post-generation correction, which is computationally expensive and fails to prevent unreliable content generation. We propose a confidence-aware routing system that proactively assesses model uncertainty before generation and redirects queries based on estimated reliability. Our approach combines three complementary signals: semantic alignment between internal representations and reference embeddings, internal convergence analysis across model layers, and learned confidence estimation. The unified confidence score determines routing to four pathways: local generation for high confidence, retrieval-augmented generation for medium confidence, larger models for low confidence, and human review for very low confidence. Evaluation on knowledge-intensive QA benchmarks demonstrates significant improvements in hallucination detection (0.74 vs. 0.42 baseline) while reducing computational costs by 40% compared to post-hoc methods. The F1 score improves from 0.61 to 0.82 with low false positive rates (0.09). This paradigm shift from reactive correction to proactive assessment offers a computationally efficient approach to LLM reliability enhancement.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªConfidence-Aware Routingç³»ç»Ÿï¼Œæ—¨åœ¨é€šè¿‡ç”Ÿæˆå‰çš„å¤šä¿¡å·è¯„ä¼°æ¥æå‡å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„å¯é æ€§å¹¶ç¼“è§£å¹»è§‰(Hallucination)é—®é¢˜ã€‚é’ˆå¯¹ç°æœ‰åæœŸä¿®æ­£æ–¹æ³•è®¡ç®—æˆæœ¬é«˜ä¸”æ— æ³•é¢„é˜²ä¸å¯é å†…å®¹ç”Ÿæˆçš„ç¼ºé™·ï¼Œè¯¥ç³»ç»Ÿåœ¨ç”Ÿæˆç¯èŠ‚å¼€å§‹å‰ä¸»åŠ¨è¯„ä¼°æ¨¡å‹çš„ä¸ç¡®å®šæ€§ã€‚å…¶æ ¸å¿ƒæ–¹æ³•æ•´åˆäº†å†…éƒ¨è¡¨ç¤ºä¸å‚è€ƒåµŒå…¥çš„è¯­ä¹‰å¯¹é½(Semantic Alignment)ã€æ¨¡å‹å±‚é—´çš„å†…éƒ¨æ”¶æ•›åˆ†æ(Internal Convergence Analysis)ä»¥åŠå­¦ä¹ å¾—åˆ°çš„ç½®ä¿¡åº¦è¯„ä¼°(Learned Confidence Estimation)ä¸‰ç±»ä¿¡å·ã€‚ç³»ç»Ÿæ ¹æ®ç»Ÿä¸€çš„ç½®ä¿¡åº¦å¾—åˆ†å°†æŸ¥è¯¢å¯¼å‘æœ¬åœ°ç”Ÿæˆã€æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ã€æ›´å¤§å‹æ¨¡å‹æˆ–äººå·¥å®¡æ ¸å››ç§ä¸åŒçš„å¤„ç†è·¯å¾„ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨çŸ¥è¯†å¯†é›†å‹é—®ç­”åŸºå‡†æµ‹è¯•ä¸­å°†å¹»è§‰æ£€æµ‹èƒ½åŠ›ä»åŸºçº¿çš„0.42æ˜¾è‘—æå‡è‡³0.74ï¼ŒåŒæ—¶æ¯”åæœŸä¿®æ­£æ–¹æ³•é™ä½äº†40%çš„è®¡ç®—æˆæœ¬ã€‚F1åˆ†æ•°ä»0.61æé«˜åˆ°0.82ï¼Œä¸”è¯¯æŠ¥ç‡(False Positive Rate)ä»…ä¸º0.09ï¼Œè¯æ˜äº†ä»è¢«åŠ¨ä¿®æ­£å‘ä¸»åŠ¨è¯„ä¼°èŒƒå¼è½¬å˜åœ¨æå‡æ¨¡å‹å¯é æ€§æ–¹é¢çš„é«˜æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01237v1",
      "published_date": "2025-09-23 18:34:20 UTC",
      "updated_date": "2025-09-23 18:34:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:29:41.281466+00:00"
    },
    {
      "arxiv_id": "2510.01235v1",
      "title": "Automated Extraction of Material Properties using LLM-based AI Agents",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹ AI æ™ºèƒ½ä½“çš„ææ–™å±æ€§è‡ªåŠ¨åŒ–æå–",
      "authors": [
        "Subham Ghosh",
        "Abhishek Tewari"
      ],
      "abstract": "The rapid discovery of materials is constrained by the lack of large, machine-readable datasets that couple performance metrics with structural context. Existing databases are either small, manually curated, or biased toward first principles results, leaving experimental literature underexploited. We present an agentic, large language model (LLM)-driven workflow that autonomously extracts thermoelectric and structural-properties from about 10,000 full-text scientific articles. The pipeline integrates dynamic token allocation, zeroshot multi-agent extraction, and conditional table parsing to balance accuracy against computational cost. Benchmarking on 50 curated papers shows that GPT-4.1 achieves the highest accuracy (F1 = 0.91 for thermoelectric properties and 0.82 for structural fields), while GPT-4.1 Mini delivers nearly comparable performance (F1 = 0.89 and 0.81) at a fraction of the cost, enabling practical large scale deployment. Applying this workflow, we curated 27,822 temperature resolved property records with normalized units, spanning figure of merit (ZT), Seebeck coefficient, conductivity, resistivity, power factor, and thermal conductivity, together with structural attributes such as crystal class, space group, and doping strategy. Dataset analysis reproduces known thermoelectric trends, such as the superior performance of alloys over oxides and the advantage of p-type doping, while also surfacing broader structure-property correlations. To facilitate community access, we release an interactive web explorer with semantic filters, numeric queries, and CSV export. This study delivers the largest LLM-curated thermoelectric dataset to date, provides a reproducible and cost-profiled extraction pipeline, and establishes a foundation for scalable, data-driven materials discovery beyond thermoelectrics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)é©±åŠ¨çš„æ™ºèƒ½ä½“å·¥ä½œæµï¼Œæ—¨åœ¨ä»çº¦10,000ç¯‡å…¨æ–‡æ–‡çŒ®ä¸­è‡ªåŠ¨æå–çƒ­ç”µ(thermoelectric)å’Œç»“æ„å±æ€§(structural-properties)ã€‚è¯¥æµæ°´çº¿æ•´åˆäº†åŠ¨æ€ä»¤ç‰Œåˆ†é…(dynamic token allocation)ã€é›¶æ ·æœ¬å¤šæ™ºèƒ½ä½“æå–(zeroshot multi-agent extraction)å’Œæ¡ä»¶è¡¨æ ¼è§£æ(conditional table parsing)ï¼Œåœ¨å‡†ç¡®æ€§ä¸è®¡ç®—æˆæœ¬ä¹‹é—´å–å¾—äº†æœ‰æ•ˆå¹³è¡¡ã€‚åŸºå‡†æµ‹è¯•è¯æ˜ GPT-4.1 å’Œ GPT-4.1 Mini åœ¨çƒ­ç”µå±æ€§(F1=0.91)å’Œç»“æ„å­—æ®µ(F1=0.82)çš„æå–ä¸Šå‡å…·æœ‰æé«˜å‡†ç¡®ç‡ï¼Œæ”¯æŒå¤§è§„æ¨¡å®é™…éƒ¨ç½²ã€‚åˆ©ç”¨è¯¥å·¥ä½œæµï¼Œç ”ç©¶è€…ç­–å±•äº†åŒ…å«27,822æ¡æ ‡å‡†åŒ–è®°å½•çš„æ•°æ®åº“ï¼Œæ¶µç›–çƒ­ç”µä¼˜å€¼(figure of merit (ZT))ã€å¡è´å…‹ç³»æ•°(Seebeck coefficient)åŠç”µå¯¼ç‡ç­‰å…³é”®å‚æ•°ã€‚æ•°æ®é›†åˆ†æä¸ä»…å¤ç°äº†åˆé‡‘æ€§èƒ½ä¼˜äºæ°§åŒ–ç‰©ç­‰å·²çŸ¥è¶‹åŠ¿ï¼Œè¿˜æ­ç¤ºäº†æ›´å¹¿æ³›çš„ç»“æ„-æ€§èƒ½å…³è”ã€‚æœ€åï¼Œç ”ç©¶å‘å¸ƒäº†å¸¦æœ‰è¯­ä¹‰è¿‡æ»¤åŠŸèƒ½çš„äº¤äº’å¼ç½‘é¡µæµè§ˆå™¨(web explorer)ï¼Œä¸ºå¯æ‰©å±•çš„æ•°æ®é©±åŠ¨ææ–™å‘ç°æä¾›äº†é‡è¦çš„æ–¹æ³•å­¦åŸºç¡€å’Œå¼€æ”¾èµ„æºã€‚",
      "categories": [
        "cs.LG",
        "cond-mat.mtrl-sci",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01235v1",
      "published_date": "2025-09-23 18:27:45 UTC",
      "updated_date": "2025-09-23 18:27:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:29:47.262145+00:00"
    },
    {
      "arxiv_id": "2509.19465v3",
      "title": "A More Realistic Evaluation of Cross-Frequency Transfer Learning and Foundation Forecasting Models",
      "title_zh": "è·¨é¢‘ç‡è¿ç§»å­¦ä¹ ä¸åŸºç¡€é¢„æµ‹æ¨¡å‹ï¼šä¸€é¡¹æ›´ä¸ºçœŸå®çš„è¯„ä¼°",
      "authors": [
        "Kin G. Olivares",
        "Malcolm Wolff",
        "Tatiana Konstantinova",
        "Shankar Ramasubramanian",
        "Boris Oreshkin",
        "Andrew Gordon Wilson",
        "Andres Potapczynski",
        "Willa Potosnak",
        "Michael W. Mahoney",
        "Mengfei Cao",
        "Dmitry Efimov"
      ],
      "abstract": "Cross-frequency transfer learning (CFTL) has emerged as a popular framework for curating large-scale time series datasets to pre-train foundation forecasting models (FFMs). Although CFTL has shown promise, current benchmarking practices fall short of accurately assessing its performance. This shortcoming stems from many factors: an over-reliance on small-scale evaluation datasets; inadequate treatment of sample size when computing summary statistics; reporting of suboptimal statistical models; and failing to account for non-negligible risks of overlap between pre-training and test datasets. To address these limitations, we introduce a unified reimplementation of widely-adopted neural forecasting networks, adapting them for the CFTL setup; we pre-train only on proprietary and synthetic data, being careful to prevent test leakage; and we evaluate on 15 large, diverse public forecast competition datasets. Our empirical analysis reveals that statistical models' accuracy is frequently underreported. Notably, we confirm that statistical models and their ensembles consistently outperform existing FFMs by more than 8.2% in sCRPS, and by more than 20% MASE, across datasets. However, we also find that synthetic dataset pre-training does improve the accuracy of a FFM by 7% percent.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è·¨é¢‘ç‡è¿ç§»å­¦ä¹ (Cross-frequency transfer learning)å’ŒåŸºç¡€é¢„æµ‹æ¨¡å‹(Foundation forecasting models)å½“å‰è¯„ä¼°ä¸­å­˜åœ¨çš„æ ·æœ¬é‡å¤„ç†ä¸å½“ã€åŸºå‡†æ¨¡å‹è´¨é‡æ¬ ä½³åŠæ•°æ®æ³„éœ²ç­‰é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªæ›´ä¸ºçœŸå®çš„è¯„ä¼°æ¡†æ¶ã€‚é€šè¿‡ç»Ÿä¸€é‡æ–°å®ç°å¤šç§ç¥ç»é¢„æµ‹ç½‘ç»œå¹¶ä½¿å…¶é€‚é…CFTLè®¾ç½®ï¼Œç ”ç©¶å›¢é˜Ÿä»…é‡‡ç”¨ä¸“æœ‰å’Œåˆæˆæ•°æ®è¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶åœ¨15ä¸ªå¤§è§„æ¨¡å…¬å¼€ç«èµ›æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯¦å°½æµ‹è¯•ã€‚å®éªŒå‘ç°ï¼Œä¼ ç»Ÿç»Ÿè®¡æ¨¡å‹åŠå…¶é›†æˆæ¨¡å‹åœ¨sCRPSå’ŒMASEæŒ‡æ ‡ä¸Šåˆ†åˆ«ä¼˜äºç°æœ‰çš„FFMsè¶…è¿‡8.2%å’Œ20%ï¼Œæ­ç¤ºäº†ç°æœ‰ç»Ÿè®¡æ¨¡å‹æ€§èƒ½è¢«é•¿æœŸä½ä¼°çš„ç°çŠ¶ã€‚ç„¶è€Œï¼Œç ”ç©¶åŒæ—¶ä¹Ÿè¯å®äº†åˆ©ç”¨åˆæˆæ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒèƒ½å°†FFMsçš„é¢„æµ‹å‡†ç¡®ç‡æœ‰æ•ˆæå‡7%ã€‚è¯¥å·¥ä½œä¸ºæ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹çš„å®¢è§‚è¯„ä¼°æä¾›äº†é‡è¦å‚è€ƒï¼Œå¹¶å¼ºè°ƒäº†åœ¨æœªæ¥ç ”ç©¶ä¸­å»ºç«‹ä¸¥è°¨åŸºå‡†æµ‹è¯•çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.AP"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2025 Workshop on Recent Advances in Time Series Foundation Models (BERT2S)",
      "pdf_url": "https://arxiv.org/pdf/2509.19465v3",
      "published_date": "2025-09-23 18:19:50 UTC",
      "updated_date": "2025-11-18 16:20:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:29:45.767135+00:00"
    },
    {
      "arxiv_id": "2509.19464v2",
      "title": "Evaluation-Aware Reinforcement Learning",
      "title_zh": "å…¼é¡¾è¯„ä¼°çš„å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Shripad Vilasrao Deshmukh",
        "Will Schwarzer",
        "Scott Niekum"
      ],
      "abstract": "Policy evaluation is often a prerequisite for deploying safety- and performance-critical systems. Existing evaluation approaches frequently suffer from high variance due to limited data and long-horizon tasks, or high bias due to unequal support or inaccurate environmental models. We posit that these challenges arise, in part, from the standard reinforcement learning (RL) paradigm of policy learning without explicit consideration of evaluation. As an alternative, we propose evaluation-aware reinforcement learning (EvA-RL), in which a policy is trained to maximize expected return while simultaneously minimizing expected evaluation error under a given value prediction scheme -- in other words, being \"easy\" to evaluate. We formalize a framework for EvA-RL and design an instantiation that enables accurate policy evaluation, conditioned on a small number of rollouts in an assessment environment that can be different than the deployment environment. However, our theoretical analysis and empirical results show that there is often a tradeoff between evaluation accuracy and policy performance when using a fixed value-prediction scheme within EvA-RL. To mitigate this tradeoff, we extend our approach to co-learn an assessment-conditioned state-value predictor alongside the policy. Empirical results across diverse discrete and continuous action domains demonstrate that EvA-RL can substantially reduce evaluation error while maintaining competitive returns. This work lays the foundation for a broad new class of RL methods that treat reliable evaluation as a first-class principle during training.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åœ¨å®‰å…¨å’Œæ€§èƒ½å…³é”®ç³»ç»Ÿä¸­ï¼Œç”±äºæœ‰é™æ•°æ®å’Œé•¿æ—¶ç¨‹ä»»åŠ¡å¯¼è‡´çš„ç­–ç•¥è¯„ä¼°(Policy evaluation)é«˜æ–¹å·®æˆ–é«˜åå·®é—®é¢˜ï¼Œæå‡ºäº†è¯„ä»·æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ (Evaluation-aware reinforcement learning, EvA-RL)æ¡†æ¶ã€‚ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ èŒƒå¼åœ¨å­¦ä¹ ç­–ç•¥æ—¶å¾€å¾€ä¸è€ƒè™‘è¯„ä¼°éš¾åº¦ï¼Œè€ŒEvA-RLé€šè¿‡åŒæ—¶æœ€å¤§åŒ–æœŸæœ›å›æŠ¥å¹¶æœ€å°åŒ–ç»™å®šå€¼é¢„æµ‹æ–¹æ¡ˆä¸‹çš„æœŸæœ›è¯„ä¼°è¯¯å·®ï¼Œä½¿ç”Ÿæˆçš„ç­–ç•¥æ›´æ˜“äºè¢«å‡†ç¡®è¯„ä¼°ã€‚ç ”ç©¶äººå‘˜è®¾è®¡äº†ä¸€ç§å…·ä½“å®ç°ï¼Œèƒ½å¤Ÿåœ¨ä¸éƒ¨ç½²ç¯å¢ƒä¸åŒçš„è¯„ä¼°ç¯å¢ƒä¸­ï¼Œåˆ©ç”¨å°‘é‡é‡‡æ ·(rollouts)å®ç°å‡†ç¡®çš„ç­–ç•¥è¯„ä¼°ã€‚é’ˆå¯¹å›ºå®šå€¼é¢„æµ‹æ–¹æ¡ˆä¸‹è¯„ä¼°å‡†ç¡®æ€§ä¸ç­–ç•¥æ€§èƒ½ä¹‹é—´çš„æƒè¡¡é—®é¢˜ï¼Œè¯¥æ–¹æ³•è¿›ä¸€æ­¥æ‰©å±•ä¸ºåœ¨è®­ç»ƒç­–ç•¥çš„åŒæ—¶å…±åŒå­¦ä¹ ä¸€ä¸ªè¯„ä¼°æ¡ä»¶çŠ¶æ€å€¼é¢„æµ‹å™¨(assessment-conditioned state-value predictor)ã€‚åœ¨å¤šç§ç¦»æ•£å’Œè¿ç»­åŠ¨ä½œé¢†åŸŸçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒEvA-RLåœ¨ä¿æŒå…·æœ‰ç«äº‰åŠ›çš„å›æŠ¥çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæ˜¾è‘—é™ä½è¯„ä¼°è¯¯å·®ã€‚è¿™é¡¹å·¥ä½œä¸ºå°†å¯é è¯„ä¼°ä½œä¸ºè®­ç»ƒé¦–è¦åŸåˆ™çš„ä¸€ç±»æ–°å‹å¼ºåŒ–å­¦ä¹ æ–¹æ³•å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages",
      "pdf_url": "https://arxiv.org/pdf/2509.19464v2",
      "published_date": "2025-09-23 18:17:21 UTC",
      "updated_date": "2026-01-18 02:42:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:29:48.969588+00:00"
    },
    {
      "arxiv_id": "2509.19460v1",
      "title": "Self-evolved Imitation Learning in Simulated World",
      "title_zh": "æ¨¡æ‹Ÿä¸–ç•Œä¸­çš„è‡ªè¿›åŒ–æ¨¡ä»¿å­¦ä¹ ",
      "authors": [
        "Yifan Ye",
        "Jun Cen",
        "Jing Chen",
        "Zhihe Lu"
      ],
      "abstract": "Imitation learning has been a trend recently, yet training a generalist agent across multiple tasks still requires large-scale expert demonstrations, which are costly and labor-intensive to collect. To address the challenge of limited supervision, we propose Self-Evolved Imitation Learning (SEIL), a framework that progressively improves a few-shot model through simulator interactions. The model first attempts tasksin the simulator, from which successful trajectories are collected as new demonstrations for iterative refinement. To enhance the diversity of these demonstrations, SEIL employs dual-level augmentation: (i) Model-level, using an Exponential Moving Average (EMA) model to collaborate with the primary model, and (ii) Environment-level, introducing slight variations in initial object positions. We further introduce a lightweight selector that filters complementary and informative trajectories from the generated pool to ensure demonstration quality. These curated samples enable the model to achieve competitive performance with far fewer training examples. Extensive experiments on the LIBERO benchmark show that SEIL achieves a new state-of-the-art performance in few-shot imitation learning scenarios. Code is available at https://github.com/Jasper-aaa/SEIL.git.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Self-Evolved Imitation Learning (SEIL)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åœ¨å¤§è§„æ¨¡ä¸“å®¶æ¼”ç¤º(Expert Demonstrations)æ•°æ®æ”¶é›†æˆæœ¬é«˜æ˜‚çš„æƒ…å†µä¸‹ï¼Œè®­ç»ƒé€šç”¨æ™ºèƒ½ä½“é¢ä¸´çš„ç›‘ç£ä¸è¶³æŒ‘æˆ˜ã€‚SEILé€šè¿‡ä¸ä»¿çœŸç¯å¢ƒ(Simulator)çš„äº¤äº’æ¥é€æ­¥æ”¹è¿›å°‘æ ·æœ¬(Few-shot)æ¨¡å‹ï¼Œå°†æˆåŠŸçš„å°è¯•è½¨è¿¹è½¬åŒ–ä¸ºæ–°çš„æ¼”ç¤ºæ•°æ®è¿›è¡Œè¿­ä»£ä¼˜åŒ–ã€‚ä¸ºäº†æå‡æ¼”ç¤ºæ•°æ®çš„å¤šæ ·æ€§ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨äº†æ¨¡å‹å±‚é¢ï¼ˆåˆ©ç”¨Exponential Moving Averageæ¨¡å‹åä½œï¼‰ä¸ç¯å¢ƒå±‚é¢ï¼ˆæ”¹å˜ç‰©ä½“åˆå§‹ä½ç½®ï¼‰çš„åŒé‡å¢å¼ºç­–ç•¥ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†è½»é‡çº§é€‰æ‹©å™¨(Selector)æ¥ç­›é€‰äº’è¡¥ä¸”ä¿¡æ¯ä¸°å¯Œçš„è½¨è¿¹ï¼Œä»è€Œç¡®ä¿ç”Ÿæˆæ ·æœ¬çš„è´¨é‡ã€‚åœ¨LIBEROåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSEILåœ¨å°‘æ ·æœ¬æ¨¡ä»¿å­¦ä¹ (Imitation Learning)ä»»åŠ¡ä¸­åˆ·æ–°äº†SOTAæ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨æå°‘è®­ç»ƒæ•°æ®ä¸‹å®ç°é«˜æ•ˆå­¦ä¹ çš„èƒ½åŠ›ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19460v1",
      "published_date": "2025-09-23 18:15:32 UTC",
      "updated_date": "2025-09-23 18:15:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:30:00.380283+00:00"
    },
    {
      "arxiv_id": "2509.19456v1",
      "title": "The Indispensable Role of User Simulation in the Pursuit of AGI",
      "title_zh": "ç”¨æˆ·æ¨¡æ‹Ÿåœ¨å®ç°é€šç”¨äººå·¥æ™ºèƒ½ä¸­çš„ä¸å¯æˆ–ç¼ºæ€§",
      "authors": [
        "Krisztian Balog",
        "ChengXiang Zhai"
      ],
      "abstract": "Progress toward Artificial General Intelligence (AGI) faces significant bottlenecks, particularly in rigorously evaluating complex interactive systems and acquiring the vast interaction data needed for training adaptive agents. This paper posits that user simulation -- creating computational agents that mimic human interaction with AI systems -- is not merely a useful tool, but is a critical catalyst required to overcome these bottlenecks and accelerate AGI development. We argue that realistic simulators provide the necessary environments for scalable evaluation, data generation for interactive learning, and fostering the adaptive capabilities central to AGI. Therefore, research into user simulation technology and intelligent task agents are deeply synergistic and must advance hand-in-hand. This article elaborates on the critical role of user simulation for AGI, explores the interdisciplinary nature of building realistic simulators, identifies key challenges including those posed by large language models, and proposes a future research agenda.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºè¿ˆå‘é€šç”¨äººå·¥æ™ºèƒ½(AGI)çš„è¿›ç¨‹ç›®å‰é¢ä¸´ç€å¤æ‚äº¤äº’ç³»ç»Ÿè¯„ä¼°ä¸¥è°¨æ€§ä¸è¶³ä»¥åŠè®­ç»ƒè‡ªé€‚åº”æ™ºèƒ½ä½“æ‰€éœ€å¤§é‡äº¤äº’æ•°æ®åŒ®ä¹çš„ç“¶é¢ˆã€‚è®ºæ–‡æå‡ºç”¨æˆ·æ¨¡æ‹Ÿ(User Simulation)ï¼Œå³æ„å»ºæ¨¡æ‹Ÿäººç±»ä¸AIç³»ç»Ÿäº¤äº’çš„è®¡ç®—æ™ºèƒ½ä½“ï¼Œæ˜¯å…‹æœè¿™äº›éšœç¢å¹¶åŠ é€ŸAGIå‘å±•çš„å…³é”®å‚¬åŒ–å‰‚ã€‚é€šè¿‡æä¾›å¤§è§„æ¨¡è¯„ä¼°ç¯å¢ƒã€ä¸ºäº¤äº’å¼å­¦ä¹ ç”Ÿæˆæ•°æ®ä»¥åŠåŸ¹å…»æ ¸å¿ƒè‡ªé€‚åº”èƒ½åŠ›ï¼Œç”¨æˆ·æ¨¡æ‹Ÿ(User Simulation)ä¸æ™ºèƒ½ä»»åŠ¡æ™ºèƒ½ä½“(Intelligent Task Agents)çš„ç ”ç©¶å±•ç°å‡ºæ·±åº¦çš„ååŒæ•ˆåº”ã€‚æ–‡ç« è¯¦ç»†é˜è¿°äº†ç”¨æˆ·æ¨¡æ‹Ÿ(User Simulation)åœ¨å®ç°AGIè·¯å¾„ä¸­çš„æ ¸å¿ƒä½œç”¨ï¼Œæ¢è®¨äº†æ„å»ºé€¼çœŸæ¨¡æ‹Ÿå™¨çš„è·¨å­¦ç§‘æ€§è´¨ï¼Œå¹¶è¯†åˆ«äº†åŒ…æ‹¬å¤§è¯­è¨€æ¨¡å‹(Large Language Models)åœ¨å†…çš„å…³é”®æŒ‘æˆ˜ã€‚æœ€åï¼Œè¯¥ç ”ç©¶æå‡ºäº†æœªæ¥çš„ç ”ç©¶è®®ç¨‹ï¼Œå¼ºè°ƒè¿™ä¸¤é¡¹æŠ€æœ¯çš„åŒæ­¥è¿›æ­¥å¯¹äºå®ç°AGIè‡³å…³é‡è¦ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted for publication in Communications of the ACM",
      "pdf_url": "https://arxiv.org/pdf/2509.19456v1",
      "published_date": "2025-09-23 18:12:45 UTC",
      "updated_date": "2025-09-23 18:12:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:30:10.670263+00:00"
    },
    {
      "arxiv_id": "2509.19454v1",
      "title": "ROPA: Synthetic Robot Pose Generation for RGB-D Bimanual Data Augmentation",
      "title_zh": "ROPAï¼šé¢å‘ RGB-D åŒè‡‚æ“ä½œæ•°æ®å¢å¼ºçš„åˆæˆæœºå™¨äººä½å§¿ç”Ÿæˆ",
      "authors": [
        "Jason Chen",
        "I-Chun Arthur Liu",
        "Gaurav Sukhatme",
        "Daniel Seita"
      ],
      "abstract": "Training robust bimanual manipulation policies via imitation learning requires demonstration data with broad coverage over robot poses, contacts, and scene contexts. However, collecting diverse and precise real-world demonstrations is costly and time-consuming, which hinders scalability. Prior works have addressed this with data augmentation, typically for either eye-in-hand (wrist camera) setups with RGB inputs or for generating novel images without paired actions, leaving augmentation for eye-to-hand (third-person) RGB-D training with new action labels less explored. In this paper, we propose Synthetic Robot Pose Generation for RGB-D Bimanual Data Augmentation (ROPA), an offline imitation learning data augmentation method that fine-tunes Stable Diffusion to synthesize third-person RGB and RGB-D observations of novel robot poses. Our approach simultaneously generates corresponding joint-space action labels while employing constrained optimization to enforce physical consistency through appropriate gripper-to-object contact constraints in bimanual scenarios. We evaluate our method on 5 simulated and 3 real-world tasks. Our results across 2625 simulation trials and 300 real-world trials demonstrate that ROPA outperforms baselines and ablations, showing its potential for scalable RGB and RGB-D data augmentation in eye-to-hand bimanual manipulation. Our project website is available at: https://ropaaug.github.io/.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ROPA (Synthetic Robot Pose Generation for RGB-D Bimanual Data Augmentation)ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹åŒè‡‚æ“ä½œ (bimanual manipulation) çš„ç¦»çº¿æ¨¡ä»¿å­¦ä¹ æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³é«˜è´¨é‡çœŸå®æ¼”ç¤ºæ•°æ®æ”¶é›†æˆæœ¬é«˜æ˜‚ä¸”éš¾ä»¥è§„æ¨¡åŒ–çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡å¾®è°ƒ Stable Diffusion æ¨¡å‹ï¼Œèƒ½å¤Ÿåˆæˆå…·æœ‰æ–°æœºå™¨äººå§¿æ€çš„ç¬¬ä¸‰äººç§° RGB åŠ RGB-D è§‚æµ‹å›¾åƒï¼Œå¹¶åŒæ­¥ç”Ÿæˆç›¸åº”çš„å…³èŠ‚ç©ºé—´åŠ¨ä½œæ ‡ç­¾ (joint-space action labels)ã€‚ä¸ºäº†ä¿è¯ç‰©ç†ä¸€è‡´æ€§ï¼ŒROPA åˆ©ç”¨çº¦æŸä¼˜åŒ– (constrained optimization) åœ¨åŒè‡‚åä½œåœºæ™¯ä¸­å¼ºåˆ¶æ‰§è¡Œå‡†ç¡®çš„å¤¹æŒå™¨ä¸ç‰©ä½“æ¥è§¦çº¦æŸã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ 5 ä¸ªä»¿çœŸä»»åŠ¡å’Œ 3 ä¸ªçœŸå®ä»»åŠ¡çš„å¹¿æ³›è¯„ä¼°ä¸­ï¼ŒROPA çš„æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº† ROPA åœ¨ç¬¬ä¸‰äººç§° RGB-D åŒè‡‚æ“ä½œé¢†åŸŸå®ç°å¯æ‰©å±•æ•°æ®å¢å¼ºçš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºè®­ç»ƒæ›´å…·é²æ£’æ€§çš„æœºå™¨äººæ“ä½œç­–ç•¥æä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19454v1",
      "published_date": "2025-09-23 18:11:53 UTC",
      "updated_date": "2025-09-23 18:11:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:30:09.162609+00:00"
    },
    {
      "arxiv_id": "2509.19295v1",
      "title": "Audio-Based Pedestrian Detection in the Presence of Vehicular Noise",
      "title_zh": "è½¦è¾†å™ªå£°ç¯å¢ƒä¸‹çš„éŸ³é¢‘è¡Œäººæ£€æµ‹",
      "authors": [
        "Yonghyun Kim",
        "Chaeyeon Han",
        "Akash Sarode",
        "Noah Posner",
        "Subhrajit Guhathakurta",
        "Alexander Lerch"
      ],
      "abstract": "Audio-based pedestrian detection is a challenging task and has, thus far, only been explored in noise-limited environments. We present a new dataset, results, and a detailed analysis of the state-of-the-art in audio-based pedestrian detection in the presence of vehicular noise. In our study, we conduct three analyses: (i) cross-dataset evaluation between noisy and noise-limited environments, (ii) an assessment of the impact of noisy data on model performance, highlighting the influence of acoustic context, and (iii) an evaluation of the model's predictive robustness on out-of-domain sounds. The new dataset is a comprehensive 1321-hour roadside dataset. It incorporates traffic-rich soundscapes. Each recording includes 16kHz audio synchronized with frame-level pedestrian annotations and 1fps video thumbnails.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨è½¦è¾†å™ªå£°ç¯å¢ƒä¸‹çš„éŸ³é¢‘è¡Œäººæ£€æµ‹ï¼ˆAudio-based pedestrian detectionï¼‰æŒ‘æˆ˜ï¼Œå¡«è¡¥äº†æ­¤å‰è¯¥é¢†åŸŸç ”ç©¶ä¸»è¦å±€é™äºä½å™ªå£°ç¯å¢ƒçš„ç©ºç™½ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªåŒ…å«1321å°æ—¶è·¯è¾¹å½•éŸ³çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ¶µç›–äº†ä¸°å¯Œçš„äº¤é€šå£°æ™¯ï¼Œå¹¶åŒæ­¥äº†16kHzéŸ³é¢‘ã€å¸§çº§è¡Œäººæ ‡æ³¨åŠ1fpsè§†é¢‘ç¼©ç•¥å›¾ã€‚ç ”ç©¶é€šè¿‡è·¨æ•°æ®é›†è¯„ä¼°ï¼ˆcross-dataset evaluationï¼‰å¯¹æ¯”äº†å™ªå£°ä¸ä½å™ªå£°ç¯å¢ƒä¸‹çš„è¡¨ç°ï¼Œå¹¶æ·±å…¥åˆ†æäº†å™ªå£°æ•°æ®åŠå£°å­¦èƒŒæ™¯ï¼ˆacoustic contextï¼‰å¯¹æ¨¡å‹æ€§èƒ½çš„å…·ä½“å½±å“ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜è¯„ä¼°äº†æ¨¡å‹åœ¨é¢å¯¹åŸŸå¤–å£°éŸ³ï¼ˆout-of-domain soundsï¼‰æ—¶çš„é¢„æµ‹é²æ£’æ€§ã€‚è¿™äº›è¯¦å°½çš„å®éªŒåˆ†æå’Œæ–°æ•°æ®é›†çš„å‘å¸ƒï¼Œä¸ºæå‡éŸ³é¢‘è¡Œäººåœ¨çœŸå®å¤æ‚äº¤é€šç¯å¢ƒä¸­çš„æ£€æµ‹å¯é æ€§æä¾›äº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted to the 10th Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE), 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.19295v1",
      "published_date": "2025-09-23 17:57:44 UTC",
      "updated_date": "2025-09-23 17:57:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:30:43.265563+00:00"
    },
    {
      "arxiv_id": "2509.19292v1",
      "title": "SOE: Sample-Efficient Robot Policy Self-Improvement via On-Manifold Exploration",
      "title_zh": "SOEï¼šåŸºäºæµå½¢æ¢ç´¢çš„æ ·æœ¬é«˜æ•ˆæœºå™¨äººç­–ç•¥è‡ªæˆ‘æå‡",
      "authors": [
        "Yang Jin",
        "Jun Lv",
        "Han Xue",
        "Wendi Chen",
        "Chuan Wen",
        "Cewu Lu"
      ],
      "abstract": "Intelligent agents progress by continually refining their capabilities through actively exploring environments. Yet robot policies often lack sufficient exploration capability due to action mode collapse. Existing methods that encourage exploration typically rely on random perturbations, which are unsafe and induce unstable, erratic behaviors, thereby limiting their effectiveness. We propose Self-Improvement via On-Manifold Exploration (SOE), a framework that enhances policy exploration and improvement in robotic manipulation. SOE learns a compact latent representation of task-relevant factors and constrains exploration to the manifold of valid actions, ensuring safety, diversity, and effectiveness. It can be seamlessly integrated with arbitrary policy models as a plug-in module, augmenting exploration without degrading the base policy performance. Moreover, the structured latent space enables human-guided exploration, further improving efficiency and controllability. Extensive experiments in both simulation and real-world tasks demonstrate that SOE consistently outperforms prior methods, achieving higher task success rates, smoother and safer exploration, and superior sample efficiency. These results establish on-manifold exploration as a principled approach to sample-efficient policy self-improvement. Project website: https://ericjin2002.github.io/SOE",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨äººç­–ç•¥å› åŠ¨ä½œæ¨¡å¼åç¼© (action mode collapse) å¯¼è‡´æ¢ç´¢èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº† SOE (Self-Improvement via On-Manifold Exploration) æ¡†æ¶ã€‚SOE é€šè¿‡å­¦ä¹ ä»»åŠ¡ç›¸å…³å› ç´ çš„ç´§å‡‘æ½œåœ¨è¡¨ç¤º (latent representation)ï¼Œå°†æ¢ç´¢è¿‡ç¨‹çº¦æŸåœ¨æœ‰æ•ˆåŠ¨ä½œçš„æµå½¢ä¸Šï¼Œä»è€Œç¡®ä¿äº†æ¢ç´¢çš„å®‰å…¨æ€§ã€å¤šæ ·æ€§å’Œæœ‰æ•ˆæ€§ã€‚ä½œä¸ºä¸€ç§æ’ä»¶æ¨¡å— (plug-in module)ï¼ŒSOE èƒ½å¤Ÿæ— ç¼é›†æˆåˆ°ä»»æ„ç­–ç•¥æ¨¡å‹ä¸­ï¼Œåœ¨ä¸æŸå®³åŸºç¡€ç­–ç•¥æ€§èƒ½çš„å‰æä¸‹å¢å¼ºå…¶æ¢ç´¢èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶ç»“æ„åŒ–çš„æ½œåœ¨ç©ºé—´ (latent space) è¿˜æ”¯æŒäººç±»å¼•å¯¼æ¢ç´¢ï¼Œè¿›ä¸€æ­¥æå‡äº†å­¦ä¹ æ•ˆç‡ä¸å¯æ§æ€§ã€‚åœ¨ä»¿çœŸå’Œç°å®åœºæ™¯ä¸­çš„å®éªŒè¡¨æ˜ï¼ŒSOE åœ¨ä»»åŠ¡æˆåŠŸç‡ã€æ¢ç´¢å¹³æ»‘åº¦ä»¥åŠæ ·æœ¬æ•ˆç‡ (sample efficiency) æ–¹é¢å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚è¯¥ç ”ç©¶ä¸ºå®ç°æ ·æœ¬é«˜æ•ˆçš„æœºå™¨äººç­–ç•¥è‡ªæˆ‘æ”¹è¿›æä¾›äº†ä¸€ç§åŸºäºæµå½¢æ¢ç´¢çš„åŸåˆ™æ€§æ–¹æ³•ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19292v1",
      "published_date": "2025-09-23 17:54:47 UTC",
      "updated_date": "2025-09-23 17:54:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:30:14.876551+00:00"
    },
    {
      "arxiv_id": "2510.05107v4",
      "title": "Structured Cognitive Loop for Behavioral Intelligence in Large Language Model Agents",
      "title_zh": "é¢å‘å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“è¡Œä¸ºæ™ºèƒ½çš„ç»“æ„åŒ–è®¤çŸ¥ç¯è·¯",
      "authors": [
        "Myung Ho Kim"
      ],
      "abstract": "Large language models have advanced natural language understanding and generation, but their use as autonomous agents introduces architectural challenges for multi-step tasks. Existing frameworks often mix cognition, memory, and control in a single prompt, reducing coherence and predictability. The Structured Cognitive Loop (SCL) is proposed as an alternative architecture that separates these functions. In SCL, the language model handles cognition, memory is stored externally, and execution is guided by a lightweight controller within a goal-directed loop. This design allows intermediate results to be recorded and verified before actions are taken, improving traceability and evaluation. SCL is evaluated against prompt-based baselines such as ReAct and LangChain agents across three tasks: travel planning, conditional email drafting, and constraint-guided image generation. Under matched settings, SCL achieves an average task success rate of 86.3 percent, compared with 70.5 to 76.8 percent for baselines. It also shows higher goal fidelity, fewer redundant calls, and reduced unsupported assertions. These results indicate that separating cognition, memory, and control can enhance reliability and interpretability without relying on larger models or heavier prompts. The findings should be regarded as preliminary evidence, with broader tests across model families and task domains planned for future work.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰å¤§è¯­è¨€æ¨¡å‹(Large Language Model)æ™ºèƒ½ä½“åœ¨å¤„ç†å¤šæ­¥ä»»åŠ¡æ—¶ï¼Œç”±äºå°†è®¤çŸ¥ã€è®°å¿†ä¸æ§åˆ¶æ··åˆåœ¨å•ä¸€æç¤ºè¯ä¸­è€Œå¯¼è‡´çš„è¿è´¯æ€§å’Œå¯é¢„æµ‹æ€§ä¸è¶³é—®é¢˜ï¼Œæå‡ºäº† Structured Cognitive Loop (SCL) æ¶æ„ã€‚SCL æ ¸å¿ƒåœ¨äºå°†è®¤çŸ¥åŠŸèƒ½ã€å¤–éƒ¨è®°å¿†å­˜å‚¨ä»¥åŠç”±è½»é‡çº§æ§åˆ¶å™¨å¼•å¯¼çš„æ‰§è¡Œè¿‡ç¨‹è¿›è¡Œè§£è€¦ï¼Œæ„å»ºäº†ä¸€ä¸ªç›®æ ‡å¯¼å‘çš„å¾ªç¯ç³»ç»Ÿã€‚è¿™ç§è®¾è®¡å…è®¸åœ¨æ‰§è¡Œå…·ä½“åŠ¨ä½œå‰å¯¹ä¸­é—´ç»“æœè¿›è¡Œè®°å½•ä¸éªŒè¯ï¼Œä»è€Œæ˜¾è‘—æå‡äº†ç³»ç»Ÿçš„å¯è¿½æº¯æ€§å’Œè¯„ä¼°èƒ½åŠ›ã€‚ç ”ç©¶åœ¨æ—…æ¸¸è§„åˆ’ã€é‚®ä»¶èµ·è‰åŠå—çº¦æŸå›¾åƒç”Ÿæˆä»»åŠ¡ä¸­ï¼Œå°† SCL ä¸ ReAct å’Œ LangChain ç­‰åŸºäºæç¤º(prompt-based)çš„åŸºçº¿æ¨¡å‹è¿›è¡Œäº†å¯¹æ¯”ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSCL çš„å¹³å‡ä»»åŠ¡æˆåŠŸç‡è¾¾åˆ° 86.3%ï¼Œè¿œé«˜äºåŸºçº¿æ¨¡å‹çš„ 70.5% è‡³ 76.8%ï¼Œå¹¶å±•ç°å‡ºæ›´é«˜çš„ç›®æ ‡å¿ å®åº¦(goal fidelity)å’Œæ›´å°‘çš„å†—ä½™è°ƒç”¨ã€‚è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡åˆ†ç¦»è®¤çŸ¥ã€è®°å¿†ä¸æ§åˆ¶æ¨¡å—ï¼Œå¯ä»¥åœ¨ä¸ä¾èµ–æ›´å¤§è§„æ¨¡æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œæœ‰æ•ˆå¢å¼ºæ™ºèƒ½ä½“çš„å¯é æ€§ä¸å¯è§£é‡Šæ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Updated the order of the SCL configuration modules to match the diagram",
      "pdf_url": "https://arxiv.org/pdf/2510.05107v4",
      "published_date": "2025-09-23 17:43:17 UTC",
      "updated_date": "2025-11-28 14:49:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:30:43.868109+00:00"
    },
    {
      "arxiv_id": "2509.19277v2",
      "title": "MOIS-SAM2: Exemplar-based Segment Anything Model 2 for multilesion interactive segmentation of neurofibromas in whole-body MRI",
      "title_zh": "MOIS-SAM2ï¼šç”¨äºå…¨èº« MRI ç¥ç»çº¤ç»´ç˜¤å¤šç—…ç¶äº¤äº’å¼åˆ†å‰²çš„åŸºäºæ ·æœ¬çš„ Segment Anything Model 2",
      "authors": [
        "Georgii Kolokolnikov",
        "Marie-Lena Schmalhofer",
        "Sophie Goetz",
        "Lennart Well",
        "Said Farschtschi",
        "Victor-Felix Mautner",
        "Inka Ristow",
        "Rene Werner"
      ],
      "abstract": "Background and Objectives: Neurofibromatosis type 1 is a genetic disorder characterized by the development of numerous neurofibromas (NFs) throughout the body. Whole-body MRI (WB-MRI) is the clinical standard for detection and longitudinal surveillance of NF tumor growth. Existing interactive segmentation methods fail to combine high lesion-wise precision with scalability to hundreds of lesions. This study proposes a novel interactive segmentation model tailored to this challenge.\n  Methods: We introduce MOIS-SAM2, a multi-object interactive segmentation model that extends the state-of-the-art, transformer-based, promptable Segment Anything Model 2 (SAM2) with exemplar-based semantic propagation. MOIS-SAM2 was trained and evaluated on 119 WB-MRI scans from 84 NF1 patients acquired using T2-weighted fat-suppressed sequences. The dataset was split at the patient level into a training set and four test sets (one in-domain and three reflecting different domain shift scenarios, e.g., MRI field strength variation, low tumor burden, differences in clinical site and scanner vendor).\n  Results: On the in-domain test set, MOIS-SAM2 achieved a scan-wise DSC of 0.60 against expert manual annotations, outperforming baseline 3D nnU-Net (DSC: 0.54) and SAM2 (DSC: 0.35). Performance of the proposed model was maintained under MRI field strength shift (DSC: 0.53) and scanner vendor variation (DSC: 0.50), and improved in low tumor burden cases (DSC: 0.61). Lesion detection F1 scores ranged from 0.62 to 0.78 across test sets. Preliminary inter-reader variability analysis showed model-to-expert agreement (DSC: 0.62-0.68), comparable to inter-expert agreement (DSC: 0.57-0.69).\n  Conclusions: The proposed MOIS-SAM2 enables efficient and scalable interactive segmentation of NFs in WB-MRI with minimal user input and strong generalization, supporting integration into clinical workflows.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MOIS-SAM2ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹å…¨èº«æ ¸ç£å…±æŒ¯æˆåƒ (WB-MRI) ä¸­å¤šå‘æ€§ç¥ç»çº¤ç»´ç˜¤ (NFs) è¿›è¡Œäº¤äº’å¼åˆ†å‰²çš„æ–°å‹å¤šå¯¹è±¡åˆ†å‰²æ¨¡å‹ã€‚è¯¥æ–¹æ³•åœ¨ Segment Anything Model 2 (SAM2) çš„åŸºç¡€ä¸Šå¼•å…¥äº†åŸºäºç¤ºä¾‹çš„è¯­ä¹‰ä¼ æ’­æŠ€æœ¯ï¼Œæœ‰æ•ˆè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨å¤„ç†æˆç™¾ä¸Šåƒä¸ªç—…ç¶æ—¶é¢ä¸´çš„ç²¾åº¦ä¸æ‰©å±•æ€§éš¾ä»¥å…¼é¡¾çš„é—®é¢˜ã€‚ç ”ç©¶é€šè¿‡å¯¹ 119 ä»½ WB-MRI æ‰«ææ•°æ®çš„è®­ç»ƒä¸è¯„ä¼°ï¼Œè¯æ˜ MOIS-SAM2 åœ¨é¢†åŸŸå†…æµ‹è¯•é›†ä¸Šçš„ DSC æŒ‡æ ‡è¾¾åˆ° 0.60ï¼Œæ˜¾è‘—ä¼˜äº 3D nnU-Net å’ŒåŸºç¡€ SAM2 æ¨¡å‹ã€‚å®éªŒè¿›ä¸€æ­¥è¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ä¸åŒç£åœºå¼ºåº¦ã€æ‰«æè®¾å¤‡ä¾›åº”å•†åŠä½è‚¿ç˜¤è´Ÿæ‹…ç­‰åŸŸåç§»åœºæ™¯ä¸‹å‡å±•ç°å‡ºç¨³å¥çš„æ³›åŒ–æ€§èƒ½ï¼Œä¸”å…¶ç—…ç¶æ£€æµ‹çš„ F1 åˆ†æ•°åœ¨ 0.62 è‡³ 0.78 ä¹‹é—´ã€‚åˆæ­¥çš„ä¸€è‡´æ€§åˆ†ææ˜¾ç¤ºï¼Œæ¨¡å‹ä¸ä¸“å®¶ä¹‹é—´çš„ä¸€è‡´æ€§ï¼ˆDSC: 0.62-0.68ï¼‰å·²è¾¾åˆ°ä¸“å®¶é—´ä¸€è‡´æ€§çš„æ°´å¹³ã€‚MOIS-SAM2 ä¸ºä¸´åºŠå·¥ä½œæµä¸­ NFs çš„é«˜æ•ˆç›‘æµ‹ä¸ç²¾ç¡®åˆ†å‰²æä¾›äº†ä¸€ç§æä½ç”¨æˆ·è¾“å…¥æˆæœ¬ä¸”å…·å¤‡å¼ºæ³›åŒ–èƒ½åŠ›çš„å¯æ‰©å±•è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19277v2",
      "published_date": "2025-09-23 17:42:24 UTC",
      "updated_date": "2025-09-24 08:17:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:30:49.412799+00:00"
    },
    {
      "arxiv_id": "2509.19271v3",
      "title": "WolBanking77: Wolof Banking Speech Intent Classification Dataset",
      "title_zh": "WolBanking77ï¼šæ²ƒæ´›å¤«è¯­é“¶è¡Œè¯­éŸ³æ„å›¾åˆ†ç±»æ•°æ®é›†",
      "authors": [
        "Abdou Karim Kandji",
        "FrÃ©dÃ©ric Precioso",
        "Cheikh Ba",
        "Samba Ndiaye",
        "Augustin Ndione"
      ],
      "abstract": "Intent classification models have made a significant progress in recent years. However, previous studies primarily focus on high-resource language datasets, which results in a gap for low-resource languages and for regions with high rates of illiteracy, where languages are more spoken than read or written. This is the case in Senegal, for example, where Wolof is spoken by around 90\\% of the population, while the national illiteracy rate remains at of 42\\%. Wolof is actually spoken by more than 10 million people in West African region. To address these limitations, we introduce the Wolof Banking Speech Intent Classification Dataset (WolBanking77), for academic research in intent classification. WolBanking77 currently contains 9,791 text sentences in the banking domain and more than 4 hours of spoken sentences. Experiments on various baselines are conducted in this work, including text and voice state-of-the-art models. The results are very promising on this current dataset. In addition, this paper presents an in-depth examination of the dataset's contents. We report baseline F1-scores and word error rates metrics respectively on NLP and ASR models trained on WolBanking77 dataset and also comparisons between models. Dataset and code available at: https://github.com/abdoukarim/wolbanking77.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† WolBanking77ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨é’ˆå¯¹ Wolof è¯­åœ¨é“¶è¡Œé¢†åŸŸçš„æ„å›¾åˆ†ç±»(Intent Classification)æ•°æ®é›†ã€‚ç”±äºç›®å‰çš„æ„å›¾è¯†åˆ«æ¨¡å‹å¤šä¾§é‡äºé«˜èµ„æºè¯­è¨€ï¼Œè€Œå¿½ç•¥äº†æ–‡ç›²ç‡è¾ƒé«˜ã€ä»¥å£è¯­ä¸ºä¸»çš„ä½èµ„æºè¯­è¨€åœ°åŒºï¼Œè¯¥æ•°æ®é›†å¡«è¡¥äº† Wolof è¯­åœ¨å­¦æœ¯ç ”ç©¶ä¸­çš„ç©ºç™½ã€‚WolBanking77 åŒ…å«äº† 9,791 æ¡æ–‡æœ¬è¯­å¥å’Œè¶…è¿‡ 4 å°æ—¶çš„é“¶è¡Œé¢†åŸŸè¯­éŸ³æ•°æ®ï¼Œè¦†ç›–äº†è¥¿éåœ°åŒºè¶…è¿‡ 1000 ä¸‡äººå£ä½¿ç”¨çš„é‡è¦è¯­è¨€ã€‚ä½œè€…åœ¨å¤šç§æœ€å…ˆè¿›çš„æ–‡æœ¬å’Œè¯­éŸ³åŸºçº¿æ¨¡å‹ä¸Šè¿›è¡Œäº†å®éªŒï¼Œè¯¦ç»†è¯„ä¼°äº† F1-score å’Œå­—é”™è¯¯ç‡(Word Error Rate)ç­‰å…³é”®æŒ‡æ ‡ã€‚å®éªŒç»“æœè¯æ˜äº†è¯¥æ•°æ®é›†åœ¨ NLP å’Œ ASR ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸ºä½èµ„æºè¯­è¨€çš„è¯­éŸ³æ„å›¾è¯†åˆ«ç ”ç©¶æä¾›äº†å¼€æ”¾çš„ä»£ç å’Œæ•°æ®æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.19271v3",
      "published_date": "2025-09-23 17:34:10 UTC",
      "updated_date": "2025-10-24 19:18:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:30:28.563319+00:00"
    },
    {
      "arxiv_id": "2509.19270v1",
      "title": "SloPalSpeech: A 2,8000-Hour Slovak Speech Corpus from Parliamentary Data",
      "title_zh": "SloPalSpeechï¼šæºè‡ªè®®ä¼šæ•°æ®çš„ 2800 å°æ—¶æ–¯æ´›ä¼å…‹è¯­è¯­éŸ³è¯­æ–™åº“",
      "authors": [
        "Erik BoÅ¾Ã­k",
        "Marek Å uppa"
      ],
      "abstract": "Automatic Speech Recognition (ASR) for low-resource languages like Slovak is hindered by the scarcity of training data. To address this, we introduce SloPalSpeech, a new, large-scale Slovak ASR dataset containing 2,806 hours of speech from parliamentary proceedings. We developed a robust processing pipeline to align and segment long-form recordings into clean, 30-second audio-transcript pairs suitable for model training. We use this dataset to fine-tune several OpenAI Whisper models (small, medium, large-v3, and large-v3-turbo), achieving significant Word Error Rate (WER) reductions on standard Slovak benchmarks like Common Voice and FLEURS. For instance, the fine-tuned Whisper-small model's WER dropped by up to 70\\%, approaching the baseline performance of the much larger Whisper-large-v3 model. To foster future research in low-resource speech recognition, we publicly release the complete SloPalSpeech dataset, the fully segmented transcripts (60 million words), and all our fine-tuned models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ–¯æ´›ä¼å…‹è¯­ (Slovak) ç­‰ä½èµ„æºè¯­è¨€åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ« (Automatic Speech Recognition, ASR) é¢†åŸŸé¢ä¸´çš„è®­ç»ƒæ•°æ®åŒ®ä¹é—®é¢˜ï¼Œæ¨å‡ºäº†å¤§è§„æ¨¡æ•°æ®é›† SloPalSpeechã€‚è¯¥æ•°æ®é›†åŒ…å«æ¥è‡ªè®®ä¼šä¼šè®®è®°å½•çš„ 2,806 å°æ—¶è¯­éŸ³æ•°æ®ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€å¥—é²æ£’çš„å¤„ç†ç®¡çº¿ (processing pipeline) å°†é•¿æ ¼å¼å½•éŸ³å¯¹é½å¹¶åˆ†å‰²ä¸ºé€‚ç”¨äºæ¨¡å‹è®­ç»ƒçš„ 30 ç§’éŸ³é¢‘-è½¬å½•æ–‡æœ¬å¯¹ã€‚é€šè¿‡å¯¹å¤šç§ OpenAI Whisper æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè¯¥ç ”ç©¶åœ¨ Common Voice å’Œ FLEURS ç­‰æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—é™ä½äº†è¯é”™è¯¯ç‡ (Word Error Rate, WER)ã€‚å®éªŒç»“æœæ˜¾ç¤ºå¾®è°ƒåçš„ Whisper-small æ¨¡å‹ WER é™å¹…é«˜è¾¾ 70%ï¼Œæ€§èƒ½æ¥è¿‘ä½“é‡æ›´å¤§çš„ Whisper-large-v3 åŸºå‡†æ¨¡å‹ã€‚ä¸ºäº†ä¿ƒè¿›ä½èµ„æºè¯­éŸ³è¯†åˆ«çš„æœªæ¥ç ”ç©¶ï¼Œè¯¥é¡¹ç›®å…¬å¼€é‡Šæ”¾äº†å®Œæ•´çš„ SloPalSpeech æ•°æ®é›†ã€åŒ…å« 6,000 ä¸‡è¯çš„è½¬å½•æ–‡æœ¬ä»¥åŠæ‰€æœ‰å¾®è°ƒæ¨¡å‹ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19270v1",
      "published_date": "2025-09-23 17:33:57 UTC",
      "updated_date": "2025-09-23 17:33:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:30:29.776468+00:00"
    },
    {
      "arxiv_id": "2509.19766v1",
      "title": "Dynamicasome: a molecular dynamics-guided and AI-driven pathogenicity prediction catalogue for all genetic mutations",
      "title_zh": "Dynamicasomeï¼šåˆ†å­åŠ¨åŠ›å­¦å¼•å¯¼ä¸äººå·¥æ™ºèƒ½é©±åŠ¨çš„å…¨é—ä¼ çªå˜è‡´ç—…æ€§é¢„æµ‹ç›®å½•",
      "authors": [
        "Naeyma N Islam",
        "Mathew A Coban",
        "Jessica M Fuller",
        "Caleb Weber",
        "Rohit Chitale",
        "Benjamin Jussila",
        "Trisha J. Brock",
        "Cui Tao",
        "Thomas R Caulfield"
      ],
      "abstract": "Advances in genomic medicine accelerate the identi cation of mutations in disease-associated genes, but the pathogenicity of many mutations remains unknown, hindering their use in diagnostics and clinical decision-making. Predictive AI models are generated to combat this issue, but current tools display low accuracy when tested against functionally validated datasets. We show that integrating detailed conformational data extracted from molecular dynamics simulations (MDS) into advanced AI-based models increases their predictive power. We carry out an exhaustive mutational analysis of the disease gene PMM2 and subject structural models of each variant to MDS. AI models trained on this dataset outperform existing tools when predicting the known pathogenicity of mutations. Our best performing model, a neuronal networks model, also predicts the pathogenicity of several PMM2 mutations currently considered of unknown signi cance. We believe this model helps alleviate the burden of unknown variants in genomic medicine.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºå› ç»„åŒ»å­¦ä¸­å¤§é‡åŸºå› çªå˜è‡´ç—…æ€§(pathogenicity)ä¸æ˜ä¸”ç°æœ‰AIé¢„æµ‹å·¥å…·å‡†ç¡®ç‡æœ‰é™çš„æŒ‘æˆ˜ï¼Œå¼€å‘äº†Dynamicasomeã€‚è¯¥ç³»ç»Ÿé€šè¿‡å°†åˆ†å­åŠ¨åŠ›å­¦æ¨¡æ‹Ÿ(Molecular Dynamics Simulations, MDS)æå–çš„è¯¦ç»†æ„è±¡æ•°æ®æ•´åˆåˆ°å…ˆè¿›çš„AIæ¨¡å‹ä¸­ï¼Œæ˜¾è‘—æå‡äº†é¢„æµ‹æ•ˆèƒ½ã€‚ç ”ç©¶å›¢é˜Ÿå¯¹ç–¾ç—…åŸºå› PMM2è¿›è¡Œäº†è¯¦å°½çš„çªå˜åˆ†æï¼Œå¹¶å¯¹æ¯ä¸ªå˜ä½“çš„ç»“æ„æ¨¡å‹è¿›è¡Œäº†MDSå¤„ç†ã€‚å®éªŒè¯æ˜ï¼Œå…¶è®­ç»ƒçš„ç¥ç»ç½‘ç»œ(Neural Networks)æ¨¡å‹åœ¨é¢„æµ‹PMM2çªå˜è‡´ç—…æ€§æ–¹é¢è¡¨ç°ä¼˜äºç°æœ‰å·¥å…·ã€‚è¯¥æ¨¡å‹è¿˜æˆåŠŸè¯†åˆ«äº†å¤šä¸ªç›®å‰è¢«å½’ç±»ä¸ºæ„ä¹‰æœªæ˜çªå˜çš„è‡´ç—…æ€§ï¼Œä¸ºå‡è½»åŸºå› ç»„åŒ»å­¦ä¸­æœªçŸ¥å˜ä½“å¸¦æ¥çš„ä¸´åºŠè¯Šæ–­è´Ÿæ‹…æä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "physics.bio-ph",
        "q-bio.MN"
      ],
      "primary_category": "q-bio.QM",
      "comment": "14 pages , 6 Figures, 2 Tables",
      "pdf_url": "https://arxiv.org/pdf/2509.19766v1",
      "published_date": "2025-09-23 17:33:05 UTC",
      "updated_date": "2025-09-23 17:33:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:30:42.757672+00:00"
    },
    {
      "arxiv_id": "2509.19265v1",
      "title": "Cross-Cultural Transfer of Commonsense Reasoning in LLMs: Evidence from the Arab World",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ä¸­å¸¸è¯†æ¨ç†çš„è·¨æ–‡åŒ–è¿ç§»ï¼šæ¥è‡ªé˜¿æ‹‰ä¼¯ä¸–ç•Œçš„è¯æ®",
      "authors": [
        "Saeed Almheiri",
        "Rania Hossam",
        "Mena Attia",
        "Chenxi Wang",
        "Preslav Nakov",
        "Timothy Baldwin",
        "Fajri Koto"
      ],
      "abstract": "Large language models (LLMs) often reflect Western-centric biases, limiting their effectiveness in diverse cultural contexts. Although some work has explored cultural alignment, the potential for cross-cultural transfer, using alignment in one culture to improve performance in others, remains underexplored. This paper investigates cross-cultural transfer of commonsense reasoning in the Arab world, where linguistic and historical similarities coexist with local cultural differences. Using a culturally grounded commonsense reasoning dataset covering 13 Arab countries, we evaluate lightweight alignment methods such as in-context learning and demonstration-based reinforcement (DITTO), alongside baselines like supervised fine-tuning and direct preference optimization. Our results show that merely 12 culture-specific examples from one country can improve performance in others by 10\\% on average, within multilingual models. In addition, we demonstrate that out-of-culture demonstrations from Indonesia and US contexts can match or surpass in-culture alignment for MCQ reasoning, highlighting cultural commonsense transferability beyond the Arab world. These findings demonstrate that efficient cross-cultural alignment is possible and offer a promising approach to adapt LLMs to low-resource cultural settings.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨é˜¿æ‹‰ä¼¯ä¸–ç•Œå¸¸è¯†æ¨ç†(commonsense reasoning)ä¸­çš„è·¨æ–‡åŒ–è¿ç§»ï¼Œæ—¨åœ¨è§£å†³æ¨¡å‹ä¸­æ™®éå­˜åœ¨çš„è¥¿æ–¹ä¸­å¿ƒåè§é—®é¢˜ã€‚ç ”ç©¶åˆ©ç”¨æ¶µç›–13ä¸ªé˜¿æ‹‰ä¼¯å›½å®¶çš„æ–‡åŒ–æ•°æ®é›†ï¼Œè¯„ä¼°äº†åŒ…æ‹¬ä¸Šä¸‹æ–‡å­¦ä¹ (in-context learning)ã€åŸºäºæ¼”ç¤ºçš„å¼ºåŒ–(DITTO)ã€ç›‘ç£å¾®è°ƒ(supervised fine-tuning)å’Œç›´æ¥åå¥½ä¼˜åŒ–(direct preference optimization)åœ¨å†…çš„å¤šç§è½»é‡çº§å¯¹é½æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä»…éœ€æ¥è‡ªå•ä¸€å›½å®¶çš„12ä¸ªæ–‡åŒ–ç‰¹å®šç¤ºä¾‹ï¼Œå³å¯ä½¿å¤šè¯­è¨€æ¨¡å‹åœ¨å…¶ä»–å›½å®¶çš„æ€§èƒ½å¹³å‡æå‡10%ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°æ¥è‡ªå°åº¦å°¼è¥¿äºšå’Œç¾å›½ç­‰éé˜¿æ‹‰ä¼¯èƒŒæ™¯çš„æ¼”ç¤ºåœ¨å¤šé€‰é¢˜æ¨ç†ä¸­åŒæ ·èƒ½è¾¾åˆ°æˆ–è¶…è¿‡æ–‡åŒ–å†…å¯¹é½çš„æ•ˆæœï¼Œè¯æ˜äº†æ–‡åŒ–å¸¸è¯†å…·æœ‰è¶…è¶Šç‰¹å®šåŒºåŸŸçš„è¿ç§»æ€§ã€‚è¿™äº›å‘ç°è¯å®äº†é«˜æ•ˆè·¨æ–‡åŒ–å¯¹é½çš„å¯è¡Œæ€§ï¼Œå¹¶ä¸ºå°†å¤§è¯­è¨€æ¨¡å‹é€‚é…åˆ°ä½èµ„æºæ–‡åŒ–ç¯å¢ƒæä¾›äº†ä¸€ç§æå…·å‰æ™¯çš„æ–¹æ³•ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "EMNLP 2025 - Findings",
      "pdf_url": "https://arxiv.org/pdf/2509.19265v1",
      "published_date": "2025-09-23 17:24:14 UTC",
      "updated_date": "2025-09-23 17:24:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:31:01.471386+00:00"
    },
    {
      "arxiv_id": "2509.20399v2",
      "title": "Defending against Stegomalware in Deep Neural Networks with Permutation Symmetry",
      "title_zh": "åˆ©ç”¨æ’åˆ—å¯¹ç§°æ€§é˜²å¾¡æ·±åº¦ç¥ç»ç½‘ç»œä¸­çš„éšå†™æ¶æ„è½¯ä»¶",
      "authors": [
        "Birk Torpmann-Hagen",
        "Michael A. Riegler",
        "PÃ¥l Halvorsen",
        "Dag Johansen"
      ],
      "abstract": "Deep neural networks are being utilized in a growing number of applications, both in production systems and for personal use. Network checkpoints are as a consequence often shared and distributed on various platforms to ease the development process. This work considers the threat of neural network stegomalware, where malware is embedded in neural network checkpoints at a negligible cost to network accuracy. This constitutes a significant security concern, but is nevertheless largely neglected by the deep learning practitioners and security specialists alike. We propose the first effective countermeasure to these attacks. In particular, we show that state-of-the-art neural network stegomalware can be efficiently and effectively neutralized through shuffling the column order of the weight- and bias-matrices, or equivalently the channel-order of convolutional layers. We show that this effectively corrupts payloads that have been embedded by state-of-the-art methods in neural network steganography at no cost to network accuracy, outperforming competing methods by a significant margin. We then discuss possible means by which to bypass this defense, additional defense methods, and advocate for continued research into the security of machine learning systems.",
      "tldr_zh": "è¯¥ç ”ç©¶å…³æ³¨åœ¨æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDeep Neural Networksï¼‰æ£€æŸ¥ç‚¹ä¸­åµŒå…¥æ¶æ„è½¯ä»¶çš„éšå†™æ¶æ„è½¯ä»¶ï¼ˆStegomalwareï¼‰å¨èƒï¼Œè¿™ç±»æ”»å‡»åœ¨å‡ ä¹ä¸å½±å“æ¨¡å‹ç²¾åº¦çš„æƒ…å†µä¸‹æ„æˆäº†ä¸¥é‡çš„å®‰å…¨æ€§é£é™©ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œä½œè€…æå‡ºäº†é¦–ä¸ªæœ‰æ•ˆçš„é˜²å¾¡å¯¹ç­–ï¼Œå³åˆ©ç”¨ç¥ç»ç½‘ç»œçš„æ’åˆ—å¯¹ç§°æ€§ï¼ˆPermutation Symmetryï¼‰æ¥ä¸­å’Œæ¶æ„è´Ÿè½½ã€‚å…·ä½“æ–¹æ³•æ˜¯é€šè¿‡éšæœºæ‰“ä¹±æƒé‡çŸ©é˜µå’Œåç½®çŸ©é˜µçš„åˆ—é¡ºåºï¼Œæˆ–è€…è°ƒæ•´å·ç§¯å±‚çš„é€šé“é¡ºåºï¼Œä»è€Œåœ¨ä¸æ”¹å˜æ¨¡å‹åŠŸèƒ½çš„å‰æä¸‹ç ´åéšå†™çš„æ•°æ®ç»“æ„ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½ä»¥é›¶ç²¾åº¦æŸå¤±çš„ä»£ä»·æœ‰æ•ˆæ¸…é™¤æœ€å…ˆè¿›éšå†™æŠ€æœ¯æ¤å…¥çš„æ¶æ„ä»£ç ï¼Œå…¶è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰çš„å…¶ä»–é˜²å¾¡æ‰‹æ®µã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æ·±å…¥æ¢è®¨äº†ç»•è¿‡æ­¤ç±»é˜²å¾¡çš„æ½œåœ¨å¯èƒ½æ€§ï¼Œå¹¶ç§¯æå€¡å¯¼å­¦æœ¯ç•ŒåŠ å¼ºå¯¹æœºå™¨å­¦ä¹ ç³»ç»Ÿå®‰å…¨æ€§çš„åç»­ç ”ç©¶ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.20399v2",
      "published_date": "2025-09-23 17:15:38 UTC",
      "updated_date": "2025-10-15 16:03:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:31:08.162030+00:00"
    },
    {
      "arxiv_id": "2509.19252v1",
      "title": "Adversarially-Refined VQ-GAN with Dense Motion Tokenization for Spatio-Temporal Heatmaps",
      "title_zh": "é¢å‘æ—¶ç©ºçƒ­åŠ›å›¾çš„å¯†é›†è¿åŠ¨ä»¤ç‰ŒåŒ–å¯¹æŠ—ç²¾ç‚¼ VQ-GAN",
      "authors": [
        "Gabriel Maldonado",
        "Narges Rashvand",
        "Armin Danesh Pazho",
        "Ghazal Alinezhad Noghre",
        "Vinit Katariya",
        "Hamed Tabkhi"
      ],
      "abstract": "Continuous human motion understanding remains a core challenge in computer vision due to its high dimensionality and inherent redundancy. Efficient compression and representation are crucial for analyzing complex motion dynamics. In this work, we introduce an adversarially-refined VQ-GAN framework with dense motion tokenization for compressing spatio-temporal heatmaps while preserving the fine-grained traces of human motion. Our approach combines dense motion tokenization with adversarial refinement, which eliminates reconstruction artifacts like motion smearing and temporal misalignment observed in non-adversarial baselines. Our experiments on the CMU Panoptic dataset provide conclusive evidence of our method's superiority, outperforming the dVAE baseline by 9.31% SSIM and reducing temporal instability by 37.1%. Furthermore, our dense tokenization strategy enables a novel analysis of motion complexity, revealing that 2D motion can be optimally represented with a compact 128-token vocabulary, while 3D motion's complexity demands a much larger 1024-token codebook for faithful reconstruction. These results establish practical deployment feasibility across diverse motion analysis applications. The code base for this work is available at https://github.com/TeCSAR-UNCC/Pose-Quantization.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å…·æœ‰å¯†é›†è¿åŠ¨æ ‡è®°åŒ–(Dense Motion Tokenization)çš„å¯¹æŠ—ç²¾ç‚¼VQ-GANæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è®¡ç®—æœºè§†è§‰ä¸­è¿ç»­äººä½“è¿åŠ¨ç†è§£çš„é«˜ç»´åº¦å’Œå†—ä½™æ€§æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•ä¸“æ³¨äºå‹ç¼©æ—¶ç©ºçƒ­å›¾(Spatio-Temporal Heatmaps)ï¼Œé€šè¿‡ç»“åˆå¯¹æŠ—æ€§ç²¾ç‚¼(Adversarial Refinement)æœ‰æ•ˆæ¶ˆé™¤äº†è¿åŠ¨æ¨¡ç³Š(Motion Smearing)å’Œæ—¶é—´é”™ä½(Temporal Misalignment)ç­‰é‡å»ºä¼ªå½±ã€‚åœ¨CMU Panopticæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨SSIMæŒ‡æ ‡ä¸Šæ¯”dVAEåŸºçº¿æé«˜äº†9.31%ï¼Œå¹¶å°†æ—¶é—´ä¸ç¨³å®šåº¦é™ä½äº†37.1%ã€‚æ­¤å¤–ï¼Œç ”ç©¶æ­ç¤ºäº†è¿åŠ¨å¤æ‚æ€§çš„å·®å¼‚ï¼Œå‘ç°2Dè¿åŠ¨å¯ç”±128ä¸ªæ ‡è®°(Tokens)çš„è¯æ±‡è¡¨ä¼˜åŒ–è¡¨ç¤ºï¼Œè€Œ3Dè¿åŠ¨åˆ™éœ€è¦1024ä¸ªæ ‡è®°çš„ç æœ¬(Codebook)æ‰èƒ½å®ç°å¿ å®é‡å»ºã€‚è¿™äº›ç»“æœè¯æ˜äº†è¯¥æ¡†æ¶åœ¨ç»†ç²’åº¦äººä½“è¿åŠ¨è½¨è¿¹ä¿ç•™æ–¹é¢çš„ä¼˜è¶Šæ€§ï¼Œå¹¶ä¸ºå¤šç§è¿åŠ¨åˆ†æåº”ç”¨çš„å®é™…éƒ¨ç½²å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19252v1",
      "published_date": "2025-09-23 17:12:20 UTC",
      "updated_date": "2025-09-23 17:12:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:31:07.878998+00:00"
    },
    {
      "arxiv_id": "2509.19249v2",
      "title": "Reinforcement Learning on Pre-Training Data",
      "title_zh": "åŸºäºé¢„è®­ç»ƒæ•°æ®çš„å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Siheng Li",
        "Kejiao Li",
        "Zenan Xu",
        "Guanhua Huang",
        "Evander Yang",
        "Kun Li",
        "Haoyuan Wu",
        "Jiajia Wu",
        "Zihao Zheng",
        "Chenchen Zhang",
        "Kun Shi",
        "Kyrierl Deng",
        "Qi Yi",
        "Ruibin Xiong",
        "Tingqiang Xu",
        "Yuhao Jiang",
        "Jianfeng Yan",
        "Yuyuan Zeng",
        "Guanghui Xu",
        "Jinbao Xue",
        "Zhijiang Xu",
        "Zheng Fang",
        "Shuai Li",
        "Qibin Liu",
        "Xiaoxue Li",
        "Zhuoyu Li",
        "Yangyu Tao",
        "Fei Gao",
        "Cheng Jiang",
        "Bo Chao Wang",
        "Kai Liu",
        "Jianchen Zhu",
        "Wai Lam",
        "Wayyt Wang",
        "Bo Zhou",
        "Di Wang"
      ],
      "abstract": "The growing disparity between the exponential scaling of computational resources and the finite growth of high-quality text data now constrains conventional scaling approaches for large language models (LLMs). To address this challenge, we introduce Reinforcement Learning on Pre-Training data (RLPT), a new training-time scaling paradigm for optimizing LLMs. In contrast to prior approaches that scale training primarily through supervised learning, RLPT enables the policy to autonomously explore meaningful trajectories to learn from pre-training data and improve its capability through reinforcement learning (RL). While existing RL strategies such as reinforcement learning from human feedback (RLHF) and reinforcement learning with verifiable rewards (RLVR) rely on human annotation for reward construction, RLPT eliminates this dependency by deriving reward signals directly from pre-training data. Specifically, it adopts a next-segment reasoning objective, rewarding the policy for accurately predicting subsequent text segments conditioned on the preceding context. This formulation allows RL to be scaled on pre-training data, encouraging the exploration of richer trajectories across broader contexts and thereby fostering more generalizable reasoning skills. Extensive experiments on both general-domain and mathematical reasoning benchmarks across multiple models validate the effectiveness of RLPT. For example, when applied to Qwen3-4B-Base, RLPT yields absolute improvements of $3.0$, $5.1$, $8.1$, $6.0$, $6.6$, and $5.3$ on MMLU, MMLU-Pro, GPQA-Diamond, KOR-Bench, AIME24, and AIME25, respectively. The results further demonstrate favorable scaling behavior, suggesting strong potential for continued gains with more compute. In addition, RLPT provides a solid foundation, extending the reasoning boundaries of LLMs and enhancing RLVR performance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é«˜è´¨é‡æ–‡æœ¬æ•°æ®æœ‰é™è€Œè®¡ç®—èµ„æºå‘ˆæŒ‡æ•°çº§æ‰©å±•å¸¦æ¥çš„å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)è§„æ¨¡åŒ–ç“¶é¢ˆï¼Œæå‡ºäº† Reinforcement Learning on Pre-Training data (RLPT)ï¼Œè¿™æ˜¯ä¸€ç§å…¨æ–°çš„è®­ç»ƒæ—¶æ‰©å±•èŒƒå¼ã€‚ä¸ä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ æˆ–ä¾èµ–äººå·¥æ ‡æ³¨å¥–åŠ±çš„ RLHF ä¸åŒï¼ŒRLPT ç›´æ¥ä»é¢„è®­ç»ƒæ•°æ®ä¸­é€šè¿‡ä¸‹æ®µæ¨ç†(next-segment reasoning)ç›®æ ‡æå–å¥–åŠ±ä¿¡å·ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªä¸»æ¢ç´¢å¹¶å­¦ä¹ æ›´é€šç”¨çš„æ¨ç†æŠ€èƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒRLPT åœ¨é€šç”¨é¢†åŸŸå’Œæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¾‹å¦‚æ˜¾è‘—æå‡äº† Qwen3-4B-Base åœ¨ MMLUã€GPQA-Diamond åŠ AIME25 ç­‰æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡ã€‚è¯¥æ–¹æ³•å±•ç¤ºäº†ä¼˜å¼‚çš„ Scaling Law è¡Œä¸ºï¼Œæš—ç¤ºäº†é€šè¿‡å¢åŠ è®¡ç®—é‡è¿›ä¸€æ­¥æå‡æ€§èƒ½çš„å·¨å¤§æ½œåŠ›ã€‚æ­¤å¤–ï¼ŒRLPT ä¸ºæ‰©å±• LLMs çš„æ¨ç†è¾¹ç•Œæä¾›äº†åŸºç¡€ï¼Œå¹¶èƒ½æœ‰æ•ˆå¢å¼º RLVR çš„æ€§èƒ½ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Work in progress",
      "pdf_url": "https://arxiv.org/pdf/2509.19249v2",
      "published_date": "2025-09-23 17:10:40 UTC",
      "updated_date": "2025-09-25 07:10:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:31:13.394920+00:00"
    },
    {
      "arxiv_id": "2509.19236v1",
      "title": "AgentInit: Initializing LLM-based Multi-Agent Systems via Diversity and Expertise Orchestration for Effective and Efficient Collaboration",
      "title_zh": "AgentInitï¼šé€šè¿‡å¤šæ ·æ€§ä¸ä¸“ä¸šæ€§ååŒç¼–æ’å®ç°åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåˆå§‹åŒ–ï¼Œæ—¨åœ¨è¾¾æˆé«˜æ•ˆã€æœ‰æ•ˆçš„åä½œ",
      "authors": [
        "Chunhao Tian",
        "Yutong Wang",
        "Xuebo Liu",
        "Zhexuan Wang",
        "Liang Ding",
        "Miao Zhang",
        "Min Zhang"
      ],
      "abstract": "Proper initialization is crucial for any system, particularly in multi-agent systems (MAS), where it plays a pivotal role in determining both the system's efficiency and effectiveness. However, existing MAS initialization methods do not fully account for the collaborative needs of the generated agents in subsequent stages. Inspired by the principles of effective team composition, we propose AgentInit, which aims to optimize the structure of agent teams. Specifically, in addition to multi-round interactions and reflections between agents during agent generation, AgentInit incorporates a Natural Language to Format mechanism to ensure consistency and standardization. Balanced team selection strategies using Pareto principles are subsequently applied to jointly consider agent team diversity and task relevance to promote effective and efficient collaboration and enhance overall system performance. Experiments show that AgentInit consistently outperforms state-of-the-art initialization methods and pre-defined strategies across various frameworks and tasks, achieving an overall performance improvement of up to 1.2 and 1.6, respectively, while also significantly reducing token consumption. Further analysis confirms its strong transferability to similar tasks and verifies the effectiveness of its key components, demonstrating its capability and adaptability as a reliable MAS initialization method. Source code and models are available at https://github.com/1737423697/AgentInit.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AgentInitï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨ä¼˜åŒ–åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ(MAS)åˆå§‹åŒ–çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åˆå§‹åŒ–æ–¹æ³•æœªèƒ½å……åˆ†è€ƒè™‘æ™ºèƒ½ä½“åç»­åä½œéœ€æ±‚çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åœ¨æ™ºèƒ½ä½“ç”Ÿæˆè¿‡ç¨‹ä¸­å¼•å…¥äº†å¤šè½®äº¤äº’ä¸åæ€æœºåˆ¶ï¼Œå¹¶ç»“åˆâ€œè‡ªç„¶è¯­è¨€è½¬æ ¼å¼â€(Natural Language to Format)æŠ€æœ¯ä»¥ç¡®ä¿ç”Ÿæˆç»“æœçš„ä¸€è‡´æ€§ä¸æ ‡å‡†åŒ–ã€‚é€šè¿‡åº”ç”¨åŸºäºå¸•ç´¯æ‰˜åŸåˆ™(Pareto principles)çš„å‡è¡¡å›¢é˜Ÿé€‰æ‹©ç­–ç•¥ï¼ŒAgentInit èƒ½å¤ŸåŒæ—¶å…¼é¡¾å›¢é˜Ÿçš„å¤šæ ·æ€§ä¸ä»»åŠ¡ç›¸å…³æ€§ï¼Œä»è€Œæ˜¾è‘—æå‡ç³»ç»Ÿçš„åä½œæ•ˆç‡ä¸æ•´ä½“æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAgentInit åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºç°æœ‰çš„ SOTA æ–¹æ³•å’Œé¢„å®šä¹‰ç­–ç•¥ï¼Œæ€§èƒ½æå‡æœ€é«˜è¾¾ 1.6 å€ï¼ŒåŒæ—¶å¤§å¹…é™ä½äº† Token æ¶ˆè€—ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å±•ç°å‡ºäº†æå¼ºçš„ä»»åŠ¡è¿ç§»èƒ½åŠ›å’Œç»„ä»¶æœ‰æ•ˆæ€§ï¼Œä¸ºæ„å»ºé«˜æ•ˆã€å¯é çš„å¤šæ™ºèƒ½ä½“åä½œç³»ç»Ÿæä¾›äº†ä¸€ç§å…·æœ‰é«˜åº¦é€‚åº”æ€§çš„åˆå§‹åŒ–æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "EMNLP 2025 Findings",
      "pdf_url": "https://arxiv.org/pdf/2509.19236v1",
      "published_date": "2025-09-23 16:58:54 UTC",
      "updated_date": "2025-09-23 16:58:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:31:15.779255+00:00"
    },
    {
      "arxiv_id": "2509.19231v1",
      "title": "Finding My Voice: Generative Reconstruction of Disordered Speech for Automated Clinical Evaluation",
      "title_zh": "å¯»å›çœŸå£°ï¼šé¢å‘è‡ªåŠ¨åŒ–ä¸´åºŠè¯„ä¼°çš„è¨€è¯­éšœç¢è¯­éŸ³ç”Ÿæˆå¼é‡å»º",
      "authors": [
        "Karen Rosero",
        "Eunjung Yeo",
        "David R. Mortensen",
        "Cortney Van't Slot",
        "Rami R. Hallac",
        "Carlos Busso"
      ],
      "abstract": "We present ChiReSSD, a speech reconstruction framework that preserves children speaker's identity while suppressing mispronunciations. Unlike prior approaches trained on healthy adult speech, ChiReSSD adapts to the voices of children with speech sound disorders (SSD), with particular emphasis on pitch and prosody. We evaluate our method on the STAR dataset and report substantial improvements in lexical accuracy and speaker identity preservation. Furthermore, we automatically predict the phonetic content in the original and reconstructed pairs, where the proportion of corrected consonants is comparable to the percentage of correct consonants (PCC), a clinical speech assessment metric. Our experiments show Pearson correlation of 0.63 between automatic and human expert annotations, highlighting the potential to reduce the manual transcription burden. In addition, experiments on the TORGO dataset demonstrate effective generalization for reconstructing adult dysarthric speech. Our results indicate that disentangled, style-based TTS reconstruction can provide identity-preserving speech across diverse clinical populations.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ChiReSSDï¼Œä¸€ç§æ—¨åœ¨ä¿ç•™å‘éŸ³éšœç¢å„¿ç«¥èº«ä»½ç‰¹å¾å¹¶æŠ‘åˆ¶è¯¯è¯»çš„è¯­éŸ³é‡æ„æ¡†æ¶ã€‚ä¸ä»¥å¾€åŸºäºå¥åº·æˆäººè¯­éŸ³è®­ç»ƒçš„æ–¹æ³•ä¸åŒï¼ŒChiReSSDèƒ½å¤Ÿé€‚åº”è¨€è¯­å£°éšœç¢(SSD)å„¿ç«¥çš„å£°éŸ³ï¼Œç‰¹åˆ«å¼ºè°ƒå¯¹éŸ³é«˜å’ŒéŸµå¾‹çš„æ¨¡æ‹Ÿã€‚é€šè¿‡åœ¨STARæ•°æ®é›†ä¸Šçš„è¯„ä¼°ï¼Œè¯¥æ–¹æ³•åœ¨è¯æ±‡å‡†ç¡®æ€§å’Œè¯´è¯äººèº«ä»½ä¿ç•™æ–¹é¢å–å¾—äº†æ˜¾è‘—æå‡ã€‚ç ”ç©¶è¿˜å®ç°äº†å¯¹åŸå§‹å’Œé‡æ„è¯­éŸ³å¯¹éŸ³ç´ å†…å®¹çš„è‡ªåŠ¨é¢„æµ‹ï¼Œå…¶è¾…éŸ³æ ¡æ­£æ¯”ä¾‹ä¸ä¸´åºŠè¯„ä¼°æŒ‡æ ‡PCCè¡¨ç°ç›¸å½“ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè‡ªåŠ¨æ ‡æ³¨ä¸äººç±»ä¸“å®¶æ ‡æ³¨ä¹‹é—´çš„Pearsonç›¸å…³ç³»æ•°è¾¾åˆ°0.63ï¼Œå‡¸æ˜¾äº†é™ä½äººå·¥è½¬å½•è´Ÿæ‹…çš„æ½œåŠ›ã€‚æ­¤å¤–ï¼Œåœ¨TORGOæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†è¯¥æ¡†æ¶åœ¨é‡æ„æˆäººæ„éŸ³éšœç¢è¯­éŸ³æ–¹é¢çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™é¡¹å·¥ä½œè¡¨æ˜ï¼ŒåŸºäºè§£è€¦é£æ ¼çš„TTSé‡æ„æŠ€æœ¯èƒ½å¤Ÿä¸ºå„ç±»ä¸´åºŠç¾¤ä½“æä¾›ä¿ç•™èº«ä»½çš„è¯­éŸ³ç”Ÿæˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19231v1",
      "published_date": "2025-09-23 16:53:07 UTC",
      "updated_date": "2025-09-23 16:53:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:31:19.484982+00:00"
    },
    {
      "arxiv_id": "2509.19227v1",
      "title": "MsFIN: Multi-scale Feature Interaction Network for Traffic Accident Anticipation",
      "title_zh": "MsFINï¼šé¢å‘äº¤é€šäº‹æ•…é¢„åˆ¤çš„å¤šå°ºåº¦ç‰¹å¾äº¤äº’ç½‘ç»œ",
      "authors": [
        "Tongshuai Wu",
        "Chao Lu",
        "Ze Song",
        "Yunlong Lin",
        "Sizhe Fan",
        "Xuemei Chen"
      ],
      "abstract": "With the widespread deployment of dashcams and advancements in computer vision, developing accident prediction models from the dashcam perspective has become critical for proactive safety interventions. However, two key challenges persist: modeling feature-level interactions among traffic participants (often occluded in dashcam views) and capturing complex, asynchronous multi-temporal behavioral cues preceding accidents. To deal with these two challenges, a Multi-scale Feature Interaction Network (MsFIN) is proposed for early-stage accident anticipation from dashcam videos. MsFIN has three layers for multi-scale feature aggregation, temporal feature processing and multi-scale feature post fusion, respectively. For multi-scale feature aggregation, a Multi-scale Module is designed to extract scene representations at short-term, mid-term and long-term temporal scales. Meanwhile, the Transformer architecture is leveraged to facilitate comprehensive feature interactions. Temporal feature processing captures the sequential evolution of scene and object features under causal constraints. In the multi-scale feature post fusion stage, the network fuses scene and object features across multiple temporal scales to generate a comprehensive risk representation. Experiments on DAD and DADA datasets show that MsFIN significantly outperforms state-of-the-art models with single-scale feature extraction in both prediction correctness and earliness. Ablation studies validate the effectiveness of each module in MsFIN, highlighting how the network achieves superior performance through multi-scale feature fusion and contextual interaction modeling.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¡Œè½¦è®°å½•ä»ªè§†è§’ä¸‹çš„äº¤é€šäº‹æ•…é¢„æµ‹ï¼Œæå‡ºäº†å¤šå°ºåº¦ç‰¹å¾äº¤äº’ç½‘ç»œ MsFIN (Multi-scale Feature Interaction Network)ï¼Œæ—¨åœ¨è§£å†³äº¤é€šå‚ä¸è€…é®æŒ¡å’Œå¤æ‚å¼‚æ­¥å¤šæ—¶é—´å°ºåº¦è¡Œä¸ºå»ºæ¨¡çš„æŒ‘æˆ˜ã€‚MsFIN é‡‡ç”¨äº†åŒ…å«å¤šå°ºåº¦ç‰¹å¾èšåˆã€æ—¶åºç‰¹å¾å¤„ç†å’Œå¤šå°ºåº¦ç‰¹å¾åæœŸèåˆçš„ä¸‰å±‚æ¶æ„ã€‚å…¶ä¸­ï¼ŒMulti-scale Module ç”¨äºåœ¨çŸ­ã€ä¸­ã€é•¿æœŸå°ºåº¦ä¸Šæå–è¡¨å¾ï¼Œå¹¶ç»“åˆ Transformer æ¶æ„ä¿ƒè¿›å…¨é¢çš„ç‰¹å¾äº¤äº’ã€‚åœ¨æ—¶åºå¤„ç†é˜¶æ®µï¼ŒMsFIN åœ¨å› æœçº¦æŸä¸‹æ•æ‰åœºæ™¯ä¸ç›®æ ‡çš„æ¼”åŒ–ï¼Œå¹¶æœ€ç»ˆèåˆç”Ÿæˆç»¼åˆé£é™©è¡¨å¾ã€‚åœ¨ DAD å’Œ DADA æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒMsFIN åœ¨é¢„æµ‹å‡†ç¡®æ€§å’Œæå‰é‡æ–¹é¢å‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„å•å°ºåº¦æ¨¡å‹ã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥éªŒè¯äº†å¤šå°ºåº¦ç‰¹å¾èåˆä¸ä¸Šä¸‹æ–‡äº¤äº’å»ºæ¨¡ (contextual interaction modeling) åœ¨æå‡äº‹æ•…é¢„è§æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19227v1",
      "published_date": "2025-09-23 16:49:25 UTC",
      "updated_date": "2025-09-23 16:49:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:31:32.671397+00:00"
    },
    {
      "arxiv_id": "2509.19224v1",
      "title": "Systematic Comparative Analysis of Large Pretrained Language Models on Contextualized Medication Event Extraction",
      "title_zh": "å¤§è§„æ¨¡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹åœ¨è¯­å¢ƒåŒ–ç”¨è¯äº‹ä»¶æŠ½å–ä¸­çš„ç³»ç»Ÿæ€§å¯¹æ¯”åˆ†æ",
      "authors": [
        "Tariq Abdul-Quddoos",
        "Xishuang Dong",
        "Lijun Qian"
      ],
      "abstract": "Attention-based models have become the leading approach in modeling medical language for Natural Language Processing (NLP) in clinical notes. These models outperform traditional techniques by effectively capturing contextual representations of language. In this research a comparative analysis is done amongst pre-trained attention based models namely Bert Base, BioBert, two variations of Bio+Clinical Bert, RoBerta, and Clinical Longformer on task related to Electronic Health Record (EHR) information extraction. The tasks from Track 1 of Harvard Medical School's 2022 National Clinical NLP Challenges (n2c2) are considered for this comparison, with the Contextualized Medication Event Dataset (CMED) given for these task. CMED is a dataset of unstructured EHRs and annotated notes that contain task relevant information about the EHRs. The goal of the challenge is to develop effective solutions for extracting contextual information related to patient medication events from EHRs using data driven methods. Each pre-trained model is fine-tuned and applied on CMED to perform medication extraction, medical event detection, and multi-dimensional medication event context classification. Processing methods are also detailed for breaking down EHRs for compatibility with the applied models. Performance analysis has been carried out using a script based on constructing medical terms from the evaluation portion of CMED with metrics including recall, precision, and F1-Score. The results demonstrate that models pre-trained on clinical data are more effective in detecting medication and medication events, but Bert Base, pre-trained on general domain data showed to be the most effective for classifying the context of events related to medications.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹å¤§è§„æ¨¡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡è¯ç‰©äº‹ä»¶æå–(Contextualized Medication Event Extraction)ä»»åŠ¡ä¸­çš„è¡¨ç°è¿›è¡Œäº†ç³»ç»Ÿæ€§çš„æ¯”è¾ƒåˆ†æã€‚ç ”ç©¶è€…å¯¹æ¯”äº† Bert Baseã€BioBertã€Bio+Clinical Bertã€RoBerta å’Œ Clinical Longformer ç­‰æ¨¡å‹åœ¨å“ˆä½›åŒ»å­¦é™¢ 2022 å¹´ n2c2 æŒ‘æˆ˜èµ›çš„è¯ç‰©äº‹ä»¶æ•°æ®é›†(CMED)ä¸Šçš„åº”ç”¨æ•ˆæœã€‚å®éªŒé‡ç‚¹è¯„ä¼°äº†è¿™äº›æ¨¡å‹åœ¨è¯ç‰©æå–(Medication Extraction)ã€åŒ»ç–—äº‹ä»¶æ£€æµ‹(Medical Event Detection)ä»¥åŠå¤šç»´è¯ç‰©äº‹ä»¶ä¸Šä¸‹æ–‡åˆ†ç±»ç­‰ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚ç ”ç©¶è¯¦ç»†ä»‹ç»äº†ç”µå­å¥åº·æ¡£æ¡ˆ(EHRs)çš„é¢„å¤„ç†æµç¨‹ï¼Œå¹¶åˆ©ç”¨ç²¾ç¡®ç‡ã€å¬å›ç‡å’Œ F1-Score ç­‰æŒ‡æ ‡å¯¹å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œäº†å…¨é¢è¡¡é‡ã€‚ç»“æœæ˜¾ç¤ºï¼Œåœ¨ä¸´åºŠæ•°æ®ä¸Šé¢„è®­ç»ƒçš„æ¨¡å‹åœ¨è¯ç‰©å’Œäº‹ä»¶æ£€æµ‹æ–¹é¢æ›´å…·ä¼˜åŠ¿ï¼Œä½†åœ¨å¤„ç†è¯ç‰©äº‹ä»¶ä¸Šä¸‹æ–‡åˆ†ç±»ä»»åŠ¡æ—¶ï¼ŒåŸºäºé€šç”¨é¢†åŸŸæ•°æ®çš„ Bert Base æ¨¡å‹åè€Œè¡¨ç°æœ€ä¸ºå‡ºè‰²ã€‚è¿™ä¸€å‘ç°ä¸ºé’ˆå¯¹ä¸´åºŠæ–‡æœ¬çš„ä¸åŒå­ä»»åŠ¡é€‰æ‹©æœ€åˆé€‚çš„é¢„è®­ç»ƒæ¶æ„æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19224v1",
      "published_date": "2025-09-23 16:48:28 UTC",
      "updated_date": "2025-09-23 16:48:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:31:24.769454+00:00"
    },
    {
      "arxiv_id": "2509.19220v1",
      "title": "FedFusion: Federated Learning with Diversity- and Cluster-Aware Encoders for Robust Adaptation under Label Scarcity",
      "title_zh": "FedFusionï¼šæ ‡ç­¾ç¨€ç¼ºç¯å¢ƒä¸‹åŸºäºå¤šæ ·æ€§ä¸èšç±»æ„ŸçŸ¥ç¼–ç å™¨çš„é²æ£’è‡ªé€‚åº”è”é‚¦å­¦ä¹ ",
      "authors": [
        "Ferdinand Kahenga",
        "Antoine Bagula",
        "Patrick Sello",
        "Sajal K. Das"
      ],
      "abstract": "Federated learning in practice must contend with heterogeneous feature spaces, severe non-IID data, and scarce labels across clients. We present FedFusion, a federated transfer-learning framework that unifies domain adaptation and frugal labelling with diversity-/cluster-aware encoders (DivEn, DivEn-mix, DivEn-c). Labelled teacher clients guide learner clients via confidence-filtered pseudo-labels and domain-adaptive transfer, while clients maintain personalised encoders tailored to local data. To preserve global coherence under heterogeneity, FedFusion employs similarity-weighted classifier coupling (with optional cluster-wise averaging), mitigating dominance by data-rich sites and improving minority-client performance. The frugal-labelling pipeline combines self-/semi-supervised pretext training with selective fine-tuning, reducing annotation demands without sharing raw data. Across tabular and imaging benchmarks under IID, non-IID, and label-scarce regimes, FedFusion consistently outperforms state-of-the-art baselines in accuracy, robustness, and fairness while maintaining comparable communication and computation budgets. These results show that harmonising personalisation, domain adaptation, and label efficiency is an effective recipe for robust federated learning under real-world constraints.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FedFusionï¼Œä¸€ä¸ªé›†æˆé¢†åŸŸè‡ªé€‚åº”(domain adaptation)ä¸èŠ‚ä¿­æ ‡æ³¨(frugal labelling)çš„è”é‚¦è¿ç§»å­¦ä¹ (federated transfer-learning)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è”é‚¦å­¦ä¹ ä¸­ç‰¹å¾ç©ºé—´å¼‚æ„ã€ä¸¥é‡éç‹¬ç«‹åŒåˆ†å¸ƒ(non-IID)åŠæ ‡ç­¾ç¨€ç¼ºç­‰ç°å®é—®é¢˜ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†å¤šæ ·æ€§ä¸èšç±»æ„ŸçŸ¥ç¼–ç å™¨(diversity-/cluster-aware encoders)ï¼Œåˆ©ç”¨æœ‰æ ‡ç­¾çš„æ•™å¸ˆå®¢æˆ·ç«¯é€šè¿‡ç½®ä¿¡åº¦è¿‡æ»¤çš„ä¼ªæ ‡ç­¾(pseudo-labels)å’Œé¢†åŸŸè‡ªé€‚åº”è¿ç§»å¼•å¯¼å­¦ä¹ è€…å®¢æˆ·ç«¯ï¼ŒåŒæ—¶å…è®¸å„å®¢æˆ·ç«¯ç»´æŒé’ˆå¯¹æœ¬åœ°æ•°æ®çš„ä¸ªæ€§åŒ–ç¼–ç å™¨ã€‚ä¸ºäº†åœ¨å¼‚æ„ç¯å¢ƒä¸‹ä¿æŒå…¨å±€ä¸€è‡´æ€§ï¼ŒFedFusioné‡‡ç”¨äº†ç›¸ä¼¼åº¦åŠ æƒåˆ†ç±»å™¨è€¦åˆ(similarity-weighted classifier coupling)æœºåˆ¶ï¼Œæœ‰æ•ˆç¼“è§£äº†æ•°æ®ä¸°å¯Œç«™ç‚¹çš„æ”¯é…æ•ˆåº”å¹¶æå‡äº†å°‘æ•°å®¢æˆ·ç«¯çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶ç»“åˆè‡ªç›‘ç£ä¸åŠç›‘ç£é¢„è®­ç»ƒæ„å»ºäº†èŠ‚ä¿­æ ‡æ³¨æµæ°´çº¿ï¼Œåœ¨ä¸å…±äº«åŸå§‹æ•°æ®çš„æƒ…å†µä¸‹æ˜¾è‘—é™ä½äº†æ ‡æ³¨éœ€æ±‚ã€‚åœ¨å¤šé¡¹è¡¨æ ¼å’Œå›¾åƒåŸºå‡†æµ‹è¯•ä¸­ï¼ŒFedFusionåœ¨å‡†ç¡®æ€§ã€é²æ£’æ€§å’Œå…¬å¹³æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„åŸºå‡†æ¨¡å‹ï¼Œè¯æ˜äº†ç»“åˆä¸ªæ€§åŒ–ã€é¢†åŸŸè‡ªé€‚åº”ä¸æ ‡ç­¾æ•ˆç‡æ˜¯å®ç°å—é™ç¯å¢ƒä¸‹ç¨³å¥è”é‚¦å­¦ä¹ çš„æœ‰æ•ˆé€”å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19220v1",
      "published_date": "2025-09-23 16:46:06 UTC",
      "updated_date": "2025-09-23 16:46:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:31:33.183220+00:00"
    },
    {
      "arxiv_id": "2509.19218v1",
      "title": "HyKid: An Open MRI Dataset with Expert-Annotated Multi-Structure and Choroid Plexus in Pediatric Hydrocephalus",
      "title_zh": "HyKidï¼šåŒ…å«ä¸“å®¶æ ‡æ³¨å¤šç»“æ„ä¸è„‰ç»œä¸›çš„å„¿ç«¥è„‘ç§¯æ°´å¼€æ”¾ MRI æ•°æ®é›†",
      "authors": [
        "Yunzhi Xu",
        "Yushuang Ding",
        "Hu Sun",
        "Hongxi Zhang",
        "Li Zhao"
      ],
      "abstract": "Evaluation of hydrocephalus in children is challenging, and the related research is limited by a lack of publicly available, expert-annotated datasets, particularly those with segmentation of the choroid plexus. To address this, we present HyKid, an open-source dataset from 48 pediatric patients with hydrocephalus. 3D MRIs were provided with 1mm isotropic resolution, which was reconstructed from routine low-resolution images using a slice-to-volume algorithm. Manually corrected segmentations of brain tissues, including white matter, grey matter, lateral ventricle, external CSF, and the choroid plexus, were provided by an experienced neurologist. Additionally, structured data was extracted from clinical radiology reports using a Retrieval-Augmented Generation framework. The strong correlation between choroid plexus volume and total CSF volume provided a potential biomarker for hydrocephalus evaluation, achieving excellent performance in a predictive model (AUC = 0.87). The proposed HyKid dataset provided a high-quality benchmark for neuroimaging algorithms development, and it revealed the choroid plexus-related features in hydrocephalus assessments. Our datasets are publicly available at https://www.synapse.org/Synapse:syn68544889.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†HyKidï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«48åå„¿ç«¥è„‘ç§¯æ°´(pediatric hydrocephalus)æ‚£è€…ä¿¡æ¯çš„å¼€æºMRIæ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³ç›¸å…³é¢†åŸŸç¼ºä¹ä¸“å®¶æ ‡æ³¨æ•°æ®é›†çš„é—®é¢˜ã€‚è¯¥æ•°æ®é›†åˆ©ç”¨åˆ‡ç‰‡åˆ°å·ç®—æ³•(slice-to-volume algorithm)å°†å¸¸è§„å›¾åƒé‡å»ºä¸º1mmå„å‘åŒæ€§åˆ†è¾¨ç‡ï¼Œå¹¶ç”±ç¥ç»ç§‘ä¸“å®¶å¯¹æ‰‹å·¥æ ¡æ­£çš„è„‘ç»„ç»‡ã€ä¾§è„‘å®¤(lateral ventricle)åŠè„‰ç»œä¸›(choroid plexus)ç­‰ç»“æ„è¿›è¡Œäº†ç²¾ç¡®åˆ†å‰²ã€‚ç ”ç©¶è€…è¿˜é€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)æ¡†æ¶ä»ä¸´åºŠæŠ¥å‘Šä¸­æå–äº†ç»“æ„åŒ–æ•°æ®ï¼Œå¹¶å‘ç°è„‰ç»œä¸›ä½“ç§¯ä¸æ€»è„‘è„Šæ¶²(CSF)ä½“ç§¯ä¹‹é—´å­˜åœ¨æ˜¾è‘—ç›¸å…³æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ•°æ®é›†æ­ç¤ºçš„è„‰ç»œä¸›ç›¸å…³ç‰¹å¾å¯ä½œä¸ºè„‘ç§¯æ°´è¯„ä¼°çš„æ½œåœ¨ç”Ÿç‰©æ ‡å¿—ç‰©ï¼Œå…¶é¢„æµ‹æ¨¡å‹è¾¾åˆ°äº†0.87çš„AUCã€‚HyKidä¸ºç¥ç»å½±åƒç®—æ³•å¼€å‘æä¾›äº†é«˜è´¨é‡çš„åŸºå‡†ï¼Œç›®å‰å·²å‘å…¬ä¼—å¼€æ”¾ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.19218v1",
      "published_date": "2025-09-23 16:42:16 UTC",
      "updated_date": "2025-09-23 16:42:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:31:32.466848+00:00"
    },
    {
      "arxiv_id": "2509.19212v1",
      "title": "Steering Multimodal Large Language Models Decoding for Context-Aware Safety",
      "title_zh": "å¼•å¯¼å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è§£ç ä»¥å®ç°è¯­å¢ƒæ„ŸçŸ¥çš„å®‰å…¨æ€§",
      "authors": [
        "Zheyuan Liu",
        "Zhangchen Xu",
        "Guangyao Dou",
        "Xiangchi Yuan",
        "Zhaoxuan Tan",
        "Radha Poovendran",
        "Meng Jiang"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) are increasingly deployed in real-world applications, yet their ability to make context-aware safety decisions remains limited. Existing methods often fail to balance oversensitivity (unjustified refusals of benign queries) and undersensitivity (missed detection of visually grounded risks), leaving a persistent gap in safety alignment. To address this issue, we introduce Safety-aware Contrastive Decoding (SafeCoDe), a lightweight and model-agnostic decoding framework that dynamically adjusts token generation based on multimodal context. SafeCoDe operates in two stages: (1) a contrastive decoding mechanism that highlights tokens sensitive to visual context by contrasting real and Gaussian-noised images, and (2) a global-aware token modulation strategy that integrates scene-level reasoning with token-level adjustment to adapt refusals according to the predicted safety verdict. Extensive experiments across diverse MLLM architectures and safety benchmarks, covering undersensitivity, oversensitivity, and general safety evaluations, show that SafeCoDe consistently improves context-sensitive refusal behaviors while preserving model helpfulness.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) åœ¨å®‰å…¨å†³ç­–ä¸­éš¾ä»¥å¹³è¡¡è¿‡åº¦æ•æ„Ÿä¸æ•æ„Ÿåº¦ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº† Safety-aware Contrastive Decoding (SafeCoDe) æ¡†æ¶ã€‚è¿™æ˜¯ä¸€ç§è½»é‡çº§ä¸”ä¸æ¨¡å‹æ— å…³çš„è§£ç æ–¹æ¡ˆï¼Œæ—¨åœ¨æ ¹æ®å¤šæ¨¡æ€ä¸Šä¸‹æ–‡åŠ¨æ€å¼•å¯¼ Token ç”Ÿæˆã€‚SafeCoDe é¦–å…ˆé€šè¿‡å¯¹æ¯”çœŸå®å›¾åƒä¸é«˜æ–¯å™ªå£°å›¾åƒçš„ Contrastive Decoding æœºåˆ¶æ¥è¯†åˆ«è§†è§‰æ•æ„Ÿ Tokenï¼Œéšååˆ©ç”¨ Global-aware Token Modulation ç­–ç•¥ç»“åˆåœºæ™¯æ¨ç†ä¸ Token çº§è°ƒæ•´ï¼Œä»è€Œæ ¹æ®é¢„æµ‹çš„å®‰å…¨åˆ¤å®šå®æ—¶ä¼˜åŒ–æ‹’ç»è¡Œä¸ºã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSafeCoDe åœ¨å¤šç§æ¨¡å‹æ¶æ„å’Œå®‰å…¨åŸºå‡†æµ‹è¯•ä¸­å‡è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½æœ‰æ•ˆæå‡æ¨¡å‹å¯¹ä¸Šä¸‹æ–‡æ•æ„Ÿçš„é£é™©è¯†åˆ«èƒ½åŠ›ã€‚è¯¥æ¡†æ¶åœ¨æ˜¾è‘—å¢å¼ºæ¨¡å‹å®‰å…¨åˆè§„æ€§çš„åŒæ—¶ï¼ŒæˆåŠŸä¿ç•™äº†æ¨¡å‹çš„åŸæœ‰å®ç”¨æ€§ (Helpfulness)ï¼Œä¸ºæ„å»ºæ›´å¯é çš„è·¨æ¨¡æ€äº¤äº’ç³»ç»Ÿæä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "A lightweight and model-agnostic decoding framework that dynamically adjusts token generation based on multimodal context",
      "pdf_url": "https://arxiv.org/pdf/2509.19212v1",
      "published_date": "2025-09-23 16:32:25 UTC",
      "updated_date": "2025-09-23 16:32:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:32:00.385491+00:00"
    },
    {
      "arxiv_id": "2509.19419v1",
      "title": "Probabilistic Runtime Verification, Evaluation and Risk Assessment of Visual Deep Learning Systems",
      "title_zh": "è§†è§‰æ·±åº¦å­¦ä¹ ç³»ç»Ÿçš„æ¦‚ç‡è¿è¡Œæ—¶éªŒè¯ã€è¯„ä¼°ä¸é£é™©è¯„ä¼°",
      "authors": [
        "Birk Torpmann-Hagen",
        "PÃ¥l Halvorsen",
        "Michael A. Riegler",
        "Dag Johansen"
      ],
      "abstract": "Despite achieving excellent performance on benchmarks, deep neural networks often underperform in real-world deployment due to sensitivity to minor, often imperceptible shifts in input data, known as distributional shifts. These shifts are common in practical scenarios but are rarely accounted for during evaluation, leading to inflated performance metrics. To address this gap, we propose a novel methodology for the verification, evaluation, and risk assessment of deep learning systems. Our approach explicitly models the incidence of distributional shifts at runtime by estimating their probability from outputs of out-of-distribution detectors. We combine these estimates with conditional probabilities of network correctness, structuring them in a binary tree. By traversing this tree, we can compute credible and precise estimates of network accuracy. We assess our approach on five different datasets, with which we simulate deployment conditions characterized by differing frequencies of distributional shift. Our approach consistently outperforms conventional evaluation, with accuracy estimation errors typically ranging between 0.01 and 0.1. We further showcase the potential of our approach on a medical segmentation benchmark, wherein we apply our methods towards risk assessment by associating costs with tree nodes, informing cost-benefit analyses and value-judgments. Ultimately, our approach offers a robust framework for improving the reliability and trustworthiness of deep learning systems, particularly in safety-critical applications, by providing more accurate performance estimates and actionable risk assessments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦ç¥ç»ç½‘ç»œåœ¨å®é™…éƒ¨ç½²ä¸­å› åˆ†å¸ƒåç§»(Distributional Shifts)å¯¼è‡´æ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å…¨æ–°çš„è¿è¡Œæ—¶éªŒè¯ã€è¯„ä¼°ä¸é£é™©è¯„ä¼°æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨åˆ†å¸ƒå¤–æ£€æµ‹å™¨(Out-of-Distribution detectors)ä¼°è®¡è¿è¡Œæ—¶çš„åˆ†å¸ƒåç§»æ¦‚ç‡ï¼Œå¹¶å°†å…¶ä¸ç½‘ç»œæ­£ç¡®æ€§çš„æ¡ä»¶æ¦‚ç‡ç»“åˆæ„å»ºäºŒå‰æ ‘ç»“æ„ã€‚é€šè¿‡éå†è¯¥äºŒå‰æ ‘ï¼Œç³»ç»Ÿå¯ä»¥è®¡ç®—å‡ºæ¯”ä¼ ç»Ÿæ–¹æ³•æ›´ç²¾ç¡®çš„å®æ—¶å‡†ç¡®æ€§ä¼°è®¡ï¼Œæœ‰æ•ˆå¼¥è¡¥äº†ä»¥å¾€è¯„ä¼°å¿½ç•¥è¾“å…¥æ•°æ®å¾®å°åç§»çš„ç¼ºé™·ã€‚åœ¨äº”ä¸ªä¸åŒæ•°æ®é›†ä¸Šçš„æ¨¡æ‹Ÿå®éªŒæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•çš„å‡†ç¡®ç‡ä¼°è®¡è¯¯å·®é€šå¸¸ä¿æŒåœ¨0.01è‡³0.1ä¹‹é—´ï¼Œè¡¨ç°æ˜¾è‘—ä¼˜äºå¸¸è§„è¯„ä¼°æ‰‹æ®µã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡åŒ»å­¦å›¾åƒåˆ†å‰²åŸºå‡†å±•ç¤ºäº†å¦‚ä½•å°†æˆæœ¬å…³è”è‡³æ ‘èŠ‚ç‚¹ä»¥è¿›è¡Œé£é™©è¯„ä¼°å’Œæˆæœ¬æ•ˆç›Šåˆ†æã€‚è¯¥æ¡†æ¶ä¸ºæå‡å®‰å…¨å…³é”®é¢†åŸŸæ·±åº¦å­¦ä¹ ç³»ç»Ÿçš„å¯é æ€§å’Œå¯ä¿¡åº¦æä¾›äº†é²æ£’çš„æ€§èƒ½ä¼°è®¡æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19419v1",
      "published_date": "2025-09-23 16:16:02 UTC",
      "updated_date": "2025-09-23 16:16:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:32:28.987308+00:00"
    },
    {
      "arxiv_id": "2509.19182v1",
      "title": "YAC: Bridging Natural Language and Interactive Visual Exploration with Generative AI for Biomedical Data Discovery",
      "title_zh": "YACï¼šåˆ©ç”¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ¡¥æ¥è‡ªç„¶è¯­è¨€ä¸äº¤äº’å¼è§†è§‰æ¢ç´¢ï¼ŒåŠ©åŠ›ç”Ÿç‰©åŒ»å­¦æ•°æ®å‘ç°",
      "authors": [
        "Devin Lange",
        "Shanghua Gao",
        "Pengwei Sui",
        "Austen Money",
        "Priya Misner",
        "Marinka Zitnik",
        "Nils Gehlenborg"
      ],
      "abstract": "Incorporating natural language input has the potential to improve the capabilities of biomedical data discovery interfaces. However, user interface elements and visualizations are still powerful tools for interacting with data, even in the new world of generative AI. In our prototype system, YAC, Yet Another Chatbot, we bridge the gap between natural language and interactive visualizations by generating structured declarative output with a multi-agent system and interpreting that output to render linked interactive visualizations and apply data filters. Furthermore, we include widgets, which allow users to adjust the values of that structured output through user interface elements. We reflect on the capabilities and design of this system with an analysis of its technical dimensions and illustrate the capabilities through four usage scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† YAC (Yet Another Chatbot)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å¼¥åˆè‡ªç„¶è¯­è¨€è¾“å…¥ä¸äº¤äº’å¼è§†è§‰æ¢ç´¢ä¹‹é—´å·®è·çš„åŸå‹ç³»ç»Ÿï¼Œä¸“é—¨ç”¨äºç”Ÿç‰©åŒ»å­¦æ•°æ®å‘ç° (Biomedical Data Discovery)ã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ (Multi-agent System) ç”Ÿæˆç»“æ„åŒ–å£°æ˜å¼è¾“å‡ºï¼Œå¹¶ä»¥æ­¤æ¸²æŸ“å…³è”çš„äº¤äº’å¼å¯è§†åŒ–ç•Œé¢å¹¶åº”ç”¨æ•°æ®è¿‡æ»¤å™¨ (Data Filters)ã€‚æ­¤å¤–ï¼ŒYAC è¿˜é›†æˆäº† UI å°éƒ¨ä»¶ (Widgets)ï¼Œå…è®¸ç”¨æˆ·åœ¨ç”Ÿæˆå¼ AI (Generative AI) çš„åŸºç¡€ä¸Šé€šè¿‡äº¤äº’å¼ç•Œé¢æ‰‹åŠ¨è°ƒæ•´æ•°æ®å‚æ•°ã€‚ç ”ç©¶é€šè¿‡å››ä¸ªä½¿ç”¨åœºæ™¯ (Usage Scenarios) æ¼”ç¤ºäº†è¯¥ç³»ç»Ÿåœ¨å®é™…æ¢ç´¢ä¸­çš„èƒ½åŠ›ï¼Œå¹¶å¯¹å…¶æŠ€æœ¯ç»´åº¦å’Œè®¾è®¡æ€è·¯è¿›è¡Œäº†æ·±å…¥åˆ†æã€‚è¯¥æ¡†æ¶å±•ç¤ºäº†å¦‚ä½•æœ‰æ•ˆç»“åˆè‡ªç„¶è¯­è¨€å¤„ç†ä¸ä¼ ç»Ÿå¯è§†åŒ–å·¥å…·ï¼Œä»¥æå‡å¤æ‚ç”Ÿç‰©åŒ»å­¦æ•°æ®çš„äº¤äº’ä¸åˆ†æä½“éªŒã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19182v1",
      "published_date": "2025-09-23 15:57:42 UTC",
      "updated_date": "2025-09-23 15:57:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:31:51.580618+00:00"
    },
    {
      "arxiv_id": "2509.19170v2",
      "title": "Soft Tokens, Hard Truths",
      "title_zh": "è½¯ Tokenï¼Œç¡¬çœŸç›¸",
      "authors": [
        "Natasha Butt",
        "Ariel Kwiatkowski",
        "Ismail Labiad",
        "Julia Kempe",
        "Yann Ollivier"
      ],
      "abstract": "The use of continuous instead of discrete tokens during the Chain-of-Thought (CoT) phase of reasoning LLMs has garnered attention recently, based on the intuition that a continuous mixture of discrete tokens could simulate a superposition of several reasoning paths simultaneously. Theoretical results have formally proven that continuous tokens have much greater expressivity and can solve specific problems more efficiently. However, practical use of continuous tokens has been limited by strong training difficulties: previous works either just use continuous tokens at inference time on a pre-trained discrete-token model, or must distill the continuous CoT from ground-truth discrete CoTs and face computational costs that limit the CoT to very few tokens.\n  This is the first work introducing a scalable method to learn continuous CoTs via reinforcement learning (RL), without distilling from reference discrete CoTs. We use \"soft\" tokens: mixtures of tokens together with noise on the input embedding to provide RL exploration. Computational overhead is minimal, enabling us to learn continuous CoTs with hundreds of tokens. On math reasoning benchmarks with Llama and Qwen models up to 8B, training with continuous CoTs match discrete-token CoTs for pass@1 and surpass them for pass@32, showing greater CoT diversity. In systematic comparisons, the best-performing scenario is to train with continuous CoT tokens then use discrete tokens for inference, meaning the \"soft\" models can be deployed in a standard way. Finally, we show continuous CoT RL training better preserves the predictions of the base model on out-of-domain tasks, thus providing a softer touch to the base model.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨ Chain-of-Thought (CoT) æ¨ç†ä¸­ä½¿ç”¨è¿ç»­ Token æ›¿ä»£ç¦»æ•£ Token çš„æ½œåŠ›ï¼Œæ—¨åœ¨è§£å†³æ­¤å‰æ–¹æ³•å­˜åœ¨çš„è®­ç»ƒå›°éš¾å’Œæ‰©å±•æ€§å·®ç­‰æŒ‘æˆ˜ã€‚ä½œè€…æå‡ºäº†é¦–ä¸ªé€šè¿‡å¼ºåŒ–å­¦ä¹  (Reinforcement Learning, RL) ç›´æ¥å­¦ä¹ è¿ç»­ CoT çš„å¯æ‰©å±•æ–¹æ³•ï¼Œåˆ©ç”¨ \"soft tokens\"ï¼ˆToken æ··åˆç‰©ï¼‰ç»“åˆè¾“å…¥åµŒå…¥å™ªå£°æ¥è¿›è¡Œ RL æ¢ç´¢ï¼Œä¸”æ— éœ€ä¾èµ–å‚è€ƒç¦»æ•£ CoT çš„è’¸é¦ã€‚è¯¥æ–¹æ³•è®¡ç®—å¼€é”€æå°ï¼Œæ”¯æŒè®­ç»ƒé•¿è¾¾æ•°ç™¾ä¸ª Token çš„è¿ç»­ CoTã€‚åœ¨ Llama å’Œ Qwen æ¨¡å‹çš„æ•°å­¦æ¨ç†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨ pass@1 ä¸Šä¸ç¦»æ•£ CoT è¡¨ç°ç›¸å½“ï¼Œè€Œåœ¨ pass@32 ä¸Šåˆ™æ›´å…·ä¼˜åŠ¿ï¼Œå±•ç°äº†æ›´é«˜çš„ CoT å¤šæ ·æ€§ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œæœ€ä½³å®è·µæ˜¯é‡‡ç”¨è¿ç»­ CoT è®­ç»ƒæ¨¡å‹å¹¶ä»¥ç¦»æ•£ Token è¿›è¡Œæ¨ç†ï¼Œè¿™ä½¿å¾— \"soft\" æ¨¡å‹èƒ½ä»¥æ ‡å‡†æ–¹å¼éƒ¨ç½²ã€‚æœ€åï¼Œå®éªŒè¯æ˜è¿ç»­ CoT è®­ç»ƒèƒ½æ›´å¥½åœ°ä¿ç•™åŸºç¡€æ¨¡å‹åœ¨åŸŸå¤–ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œä¸ºæ¨¡å‹å¾®è°ƒæä¾›äº†ä¸€ç§æ›´æ¸©å’Œçš„å¤„ç†æ–¹å¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19170v2",
      "published_date": "2025-09-23 15:43:47 UTC",
      "updated_date": "2025-09-24 11:28:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:32:07.768000+00:00"
    },
    {
      "arxiv_id": "2509.19165v1",
      "title": "RoSe: Robust Self-supervised Stereo Matching under Adverse Weather Conditions",
      "title_zh": "RoSeï¼šæ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹çš„é²æ£’è‡ªç›‘ç£ç«‹ä½“åŒ¹é…",
      "authors": [
        "Yun Wang",
        "Junjie Hu",
        "Junhui Hou",
        "Chenghao Zhang",
        "Renwei Yang",
        "Dapeng Oliver Wu"
      ],
      "abstract": "Recent self-supervised stereo matching methods have made significant progress, but their performance significantly degrades under adverse weather conditions such as night, rain, and fog. We identify two primary weaknesses contributing to this performance degradation. First, adverse weather introduces noise and reduces visibility, making CNN-based feature extractors struggle with degraded regions like reflective and textureless areas. Second, these degraded regions can disrupt accurate pixel correspondences, leading to ineffective supervision based on the photometric consistency assumption. To address these challenges, we propose injecting robust priors derived from the visual foundation model into the CNN-based feature extractor to improve feature representation under adverse weather conditions. We then introduce scene correspondence priors to construct robust supervisory signals rather than relying solely on the photometric consistency assumption. Specifically, we create synthetic stereo datasets with realistic weather degradations. These datasets feature clear and adverse image pairs that maintain the same semantic context and disparity, preserving the scene correspondence property. With this knowledge, we propose a robust self-supervised training paradigm, consisting of two key steps: robust self-supervised scene correspondence learning and adverse weather distillation. Both steps aim to align underlying scene results from clean and adverse image pairs, thus improving model disparity estimation under adverse weather effects. Extensive experiments demonstrate the effectiveness and versatility of our proposed solution, which outperforms existing state-of-the-art self-supervised methods. Codes are available at \\textcolor{blue}{https://github.com/cocowy1/RoSe-Robust-Self-supervised-Stereo-Matching-under-Adverse-Weather-Conditions}.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªç›‘ç£ç«‹ä½“åŒ¹é…(self-supervised stereo matching)åœ¨æ¶åŠ£å¤©æ°”ä¸‹æ€§èƒ½æ˜¾è‘—ä¸‹é™çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºRoSeçš„ç¨³å¥å­¦ä¹ æ¡†æ¶ã€‚ä½œè€…å‘ç°æ€§èƒ½é€€åŒ–ä¸»è¦æºäºCNNç‰¹å¾æå–å™¨åœ¨å™ªå£°ç¯å¢ƒä¸‹çš„å±€é™æ€§ä»¥åŠå…‰åº¦ä¸€è‡´æ€§(photometric consistency)å‡è®¾åœ¨é€€åŒ–åŒºåŸŸçš„å¤±æ•ˆã€‚ä¸ºæ­¤ï¼ŒRoSeé€šè¿‡å°†è§†è§‰åŸºç¡€æ¨¡å‹(visual foundation model)çš„å¼ºå¥å…ˆéªŒæ³¨å…¥CNNï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨ä½èƒ½è§åº¦æ¡ä»¶ä¸‹çš„ç‰¹å¾è¡¨è¾¾èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†åœºæ™¯å¯¹åº”å…ˆéªŒ(scene correspondence priors)å¹¶æ„å»ºäº†æ¨¡æ‹ŸçœŸå®å¤©æ°”é€€åŒ–çš„åˆæˆæ•°æ®é›†ï¼Œä»¥å»ºç«‹æ›´ä¸ºç¨³å¥çš„ç›‘ç£ä¿¡å·ã€‚é€šè¿‡ç»“åˆè‡ªç›‘ç£åœºæ™¯å¯¹åº”å­¦ä¹ ä¸æ¶åŠ£å¤©æ°”è’¸é¦(adverse weather distillation)æŠ€æœ¯ï¼Œè¯¥æ¡†æ¶æˆåŠŸå®ç°äº†æ¸…æ™°ä¸é€€åŒ–å›¾åƒå¯¹ä¹‹é—´åº•å±‚åœºæ™¯ç‰¹å¾çš„å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRoSeåœ¨å¤šç§æç«¯å¤©æ°”æ¡ä»¶ä¸‹çš„è¡¨ç°å‡ä¼˜äºç°æœ‰çš„å…ˆè¿›è‡ªç›‘ç£æ–¹æ³•ï¼Œå±•ç°äº†æå¼ºçš„æ³›åŒ–æ€§ä¸å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19165v1",
      "published_date": "2025-09-23 15:41:40 UTC",
      "updated_date": "2025-09-23 15:41:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:32:08.456596+00:00"
    },
    {
      "arxiv_id": "2510.01232v1",
      "title": "Benchmark Profiling: Mechanistic Diagnosis of LLM Benchmarks",
      "title_zh": "Benchmark Profilingï¼šå¤§è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯•çš„æœºç†æ€§è¯Šæ–­",
      "authors": [
        "Dongjun Kim",
        "Gyuho Shim",
        "Yongchan Chun",
        "Minhyuk Kim",
        "Chanjun Park",
        "Heuiseok Lim"
      ],
      "abstract": "Large Language Models are commonly judged by their scores on standard benchmarks, yet such scores often overstate real capability since they mask the mix of skills a task actually demands. For example, ARC is assumed to test reasoning, while HellaSwag is designed to evaluate commonsense. However, we lack a systematic way to verify if these benchmarks actually measure these labels. We introduce Benchmark Profiling, a diagnostic framework that decomposes benchmark performance into ten cognitively grounded abilities. The method combines gradient-based importance scoring with targeted parameter ablation to compute an Ability Impact Score (AIS) that quantifies how much each ability contributes to a model's success on a given benchmark. Profiling three instruction-tuned models across ten widely used benchmarks yields four key findings: (i) most benchmarks draw on several abilities rather than one, (ii) datasets with similar labels rely on distinct ability mixtures, (iii) code-generation benchmarks reward broad, multi-skill improvement and thus show only modest gains from narrow domain-specific fine-tuning, and (iv) abilities irrelevant to the task could negatively affect performance. Benchmark Profiling therefore explains why performance gains do not always translate into user-perceived competence and offers a transparent tool for benchmark audit and model interpretability.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Benchmark Profilingï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åŸºå‡†æµ‹è¯•æœºç†è¯Šæ–­çš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å°†æ¨¡å‹è¡¨ç°åˆ†è§£ä¸ºåç§è®¤çŸ¥èƒ½åŠ›æ¥è§£å†³è¯„ä¼°åˆ†æ•°ä¸å…¶çœŸå®èƒ½åŠ›ä¸ç¬¦çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•ç»“åˆäº†åŸºäºæ¢¯åº¦çš„é‡è¦æ€§è¯„åˆ†(gradient-based importance scoring)ä¸å®šå‘å‚æ•°æ¶ˆè(targeted parameter ablation)ï¼Œé€šè¿‡è®¡ç®—èƒ½åŠ›å½±å“å¾—åˆ†(Ability Impact Score, AIS)æ¥é‡åŒ–å„èƒ½åŠ›å¯¹ç‰¹å®šåŸºå‡†æµ‹è¯•æˆåŠŸçš„è´¡çŒ®ã€‚å®éªŒå‘ç°ï¼Œå¤§å¤šæ•°åŸºå‡†æµ‹è¯•ä¾èµ–å¤šç§èƒ½åŠ›çš„ç»„åˆè€Œéå•ä¸€èƒ½åŠ›ï¼Œä¸”æ ‡ç­¾ç›¸ä¼¼çš„æ•°æ®é›†åœ¨å®é™…æ‰€éœ€çš„èƒ½åŠ›æ··åˆæ¯”ä¾‹ä¸Šå­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚æ­¤å¤–ï¼Œä»£ç ç”Ÿæˆ(code-generation)ç±»åŸºå‡†æµ‹è¯•æ›´çœ‹é‡å¤šé¡¹æŠ€èƒ½çš„å…¨é¢æå‡ï¼Œè€Œéä»…å‡­é¢†åŸŸç‰¹å®šå¾®è°ƒï¼Œä¸”ä¸ä»»åŠ¡æ— å…³çš„èƒ½åŠ›ç”šè‡³å¯èƒ½è´Ÿé¢å½±å“æ¨¡å‹è¡¨ç°ã€‚Benchmark Profilingè§£é‡Šäº†æ€§èƒ½æå‡ä¸ºä½•ä¸æ€»èƒ½è½¬åŒ–ä¸ºç”¨æˆ·æ„ŸçŸ¥çš„ç«äº‰åŠ›ï¼Œä¸ºåŸºå‡†æµ‹è¯•å®¡è®¡å’Œæ¨¡å‹å¯è§£é‡Šæ€§(model interpretability)æä¾›äº†é€æ˜çš„å·¥å…·ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "16 pages, 5 figures. Accepted to EMNLP 2025 main conference",
      "pdf_url": "https://arxiv.org/pdf/2510.01232v1",
      "published_date": "2025-09-23 15:32:47 UTC",
      "updated_date": "2025-09-23 15:32:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:32:13.387034+00:00"
    },
    {
      "arxiv_id": "2509.19153v2",
      "title": "LLMs as verification oracles for Solidity",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ä½œä¸º Solidity çš„éªŒè¯é¢„è¨€æœº",
      "authors": [
        "Massimo Bartoletti",
        "Enrico Lipparini",
        "Livio Pompianu"
      ],
      "abstract": "Ensuring the correctness of smart contracts is critical, as even subtle flaws can lead to severe financial losses. While bug detection tools able to spot common vulnerability patterns can serve as a first line of defense, most real-world exploits and losses stem from errors in the contract business logic. Formal verification tools such as SolCMC and the Certora Prover address this challenge, but their impact remains limited by steep learning curves and restricted specification languages. Recent works have begun to explore the use of large language models (LLMs) for security-related tasks such as vulnerability detection and test generation. Yet, a fundamental question remains open: can LLMs aid in assessing the validity of arbitrary contract-specific properties? In this paper, we provide the first systematic empirical evaluation of GPT-5, a state-of-the-art reasoning LLM, in this role. We benchmark its performance on a large dataset of verification tasks, compare its outputs against those of established formal verification tools, and assess its practical effectiveness in real-world auditing scenarios. Our study combines quantitative metrics with qualitative analysis, and shows that recent reasoning-oriented LLMs - although lacking soundness guarantees - can be surprisingly effective at predicting the (in)validity of complex properties, suggesting a new frontier in the convergence of AI and formal methods for secure smart contract development and auditing.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨æ™ºèƒ½åˆçº¦æ­£ç¡®æ€§éªŒè¯ä¸­ï¼Œå¤§è¯­è¨€æ¨¡å‹ (LLMs) èƒ½å¦ä½œä¸ºéªŒè¯é¢„è¨€æœº (verification oracles) æ¥è¯„ä¼° Solidity åˆçº¦ç‰¹å®šå±æ€§çš„æœ‰æ•ˆæ€§ï¼Œä»¥å¼¥è¡¥ SolCMC å’Œ Certora Prover ç­‰ä¼ ç»Ÿå½¢å¼éªŒè¯å·¥å…·å­¦ä¹ æ›²çº¿è¿‡é«˜çš„ä¸è¶³ã€‚è®ºæ–‡å¯¹ GPT-5 è¿›è¡Œäº†é¦–æ¬¡ç³»ç»Ÿçš„å®è¯è¯„ä¼°ï¼Œé€šè¿‡å¤§è§„æ¨¡éªŒè¯ä»»åŠ¡æ•°æ®é›†å°†å…¶æ€§èƒ½ä¸æˆç†Ÿçš„å½¢å¼éªŒè¯å·¥å…·è¿›è¡Œå¯¹æ¯”ã€‚ç ”ç©¶ç»“åˆå®šé‡ä¸å®šæ€§åˆ†æå‘ç°ï¼Œå°½ç®¡æ¨ç†å‹ LLMs ç¼ºä¹å®Œå¤‡æ€§ä¿è¯ (soundness guarantees)ï¼Œä½†åœ¨é¢„æµ‹å¤æ‚å±æ€§çš„ï¼ˆä¸ï¼‰æœ‰æ•ˆæ€§æ–¹é¢è¡¨ç°å‡ºæƒŠäººçš„æœ‰æ•ˆæ€§ã€‚è¿™ç§æ–¹æ³•åœ¨çœŸå®å®¡è®¡åœºæ™¯ä¸­å±•ç°äº†å®ç”¨æ½œåŠ›ï¼Œä¸ºäººå·¥æ™ºèƒ½ä¸å½¢å¼åŒ–æ–¹æ³•çš„èåˆå¼€è¾Ÿäº†æ–°å‰æ²¿ï¼Œæœ‰åŠ©äºæ¨åŠ¨æ›´å®‰å…¨çš„æ™ºèƒ½åˆçº¦å¼€å‘ä¸å®¡è®¡ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19153v2",
      "published_date": "2025-09-23 15:32:13 UTC",
      "updated_date": "2026-01-09 15:07:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:32:22.390657+00:00"
    },
    {
      "arxiv_id": "2509.19147v1",
      "title": "Generative Propaganda",
      "title_zh": "ç”Ÿæˆå¼å®£ä¼ ",
      "authors": [
        "Madeleine I. G. Daepp",
        "Alejandro Cuevas",
        "Robert Osazuwa Ness",
        "Vickie Yu-Ping Wang",
        "Bharat Kumar Nayak",
        "Dibyendu Mishra",
        "Ti-Chung Cheng",
        "Shaily Desai",
        "Joyojeet Pal"
      ],
      "abstract": "Generative propaganda is the use of generative artificial intelligence (AI) to shape public opinion. To characterize its use in real-world settings, we conducted interviews with defenders (e.g., factcheckers, journalists, officials) in Taiwan and creators (e.g., influencers, political consultants, advertisers) as well as defenders in India, centering two places characterized by high levels of online propaganda. The term \"deepfakes\", we find, exerts outsized discursive power in shaping defenders' expectations of misuse and, in turn, the interventions that are prioritized. To better characterize the space of generative propaganda, we develop a taxonomy that distinguishes between obvious versus hidden and promotional versus derogatory use. Deception was neither the main driver nor the main impact vector of AI's use; instead, Indian creators sought to persuade rather than to deceive, often making AI's use obvious in order to reduce legal and reputational risks, while Taiwan's defenders saw deception as a subset of broader efforts to distort the prevalence of strategic narratives online. AI was useful and used, however, in producing efficiency gains in communicating across languages and modes, and in evading human and algorithmic detection. Security researchers should reconsider threat models to clearly differentiate deepfakes from promotional and obvious uses, to complement and bolster the social factors that constrain misuse by internal actors, and to counter efficiency gains globally.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç”Ÿæˆå¼å®£ä¼ (Generative Propaganda)ï¼Œå³åˆ©ç”¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½(Generative AI)æ¥å¡‘é€ å…¬ä¼—èˆ†è®ºã€‚ç ”ç©¶è€…é€šè¿‡å¯¹å°æ¹¾å’Œå°åº¦çš„é˜²å¾¡è€…ä¸å°åº¦çš„å†…å®¹åˆ›ä½œè€…è¿›è¡Œè®¿è°ˆï¼Œåˆ†æäº†ç”Ÿæˆå¼AIåœ¨ç°å®æ”¿æ²»ç¯å¢ƒä¸­çš„åº”ç”¨ç°çŠ¶ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ†ç±»æ³•(Taxonomy)ï¼Œå°†ç”Ÿæˆå¼å®£ä¼ åŒºåˆ†ä¸ºæ˜¾æ€§(Obvious)ä¸éšæ€§(Hidden)ã€æ¨å¹¿æ€§(Promotional)ä¸è´¬æŸæ€§(Derogatory)ä½¿ç”¨ã€‚ç ”ç©¶å‘ç°ï¼Œæ¬ºéª—(Deception)å¹¶éAIä½¿ç”¨çš„å”¯ä¸€æˆ–ä¸»è¦é©±åŠ¨åŠ›ï¼Œåˆ›ä½œè€…å¸¸é€šè¿‡æ˜¾æ€§ä½¿ç”¨AIæ¥å¢å¼ºè¯´æœåŠ›(Persuasion)å¹¶é™ä½æ³•å¾‹é£é™©ã€‚æ­¤å¤–ï¼ŒAIçš„ä¸»è¦ä¼˜åŠ¿åœ¨äºæ˜¾è‘—æå‡äº†è·¨è¯­è¨€æ²Ÿé€šçš„æ•ˆç‡(Efficiency gains)å¹¶å¢å¼ºäº†è§„é¿æ£€æµ‹(Detection)çš„èƒ½åŠ›ã€‚æœ€åï¼Œç ”ç©¶å»ºè®®å®‰å…¨ä¸“å®¶åº”é‡æ–°æ„å»ºå¨èƒæ¨¡å‹(Threat models)ï¼ŒåŒºåˆ†Deepfakesä¸æ˜¾æ€§çš„æ¨å¹¿æ€§ç”¨é€”ï¼Œä»¥æ›´å¥½åœ°åº”å¯¹å…¨çƒèŒƒå›´å†…çš„æŠ€æœ¯æ»¥ç”¨ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "cs.CY",
      "comment": "Working Paper",
      "pdf_url": "https://arxiv.org/pdf/2509.19147v1",
      "published_date": "2025-09-23 15:27:00 UTC",
      "updated_date": "2025-09-23 15:27:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:32:27.497702+00:00"
    },
    {
      "arxiv_id": "2509.19143v1",
      "title": "Anecdoctoring: Automated Red-Teaming Across Language and Place",
      "title_zh": "Anecdoctoringï¼šè·¨è¯­è¨€ä¸åœ°åŸŸçš„è‡ªåŠ¨åŒ–çº¢é˜Ÿæµ‹è¯•",
      "authors": [
        "Alejandro Cuevas",
        "Saloni Dash",
        "Bharat Kumar Nayak",
        "Dan Vann",
        "Madeleine I. G. Daepp"
      ],
      "abstract": "Disinformation is among the top risks of generative artificial intelligence (AI) misuse. Global adoption of generative AI necessitates red-teaming evaluations (i.e., systematic adversarial probing) that are robust across diverse languages and cultures, but red-teaming datasets are commonly US- and English-centric. To address this gap, we propose \"anecdoctoring\", a novel red-teaming approach that automatically generates adversarial prompts across languages and cultures. We collect misinformation claims from fact-checking websites in three languages (English, Spanish, and Hindi) and two geographies (US and India). We then cluster individual claims into broader narratives and characterize the resulting clusters with knowledge graphs, with which we augment an attacker LLM. Our method produces higher attack success rates and offers interpretability benefits relative to few-shot prompting. Results underscore the need for disinformation mitigations that scale globally and are grounded in real-world adversarial misuse.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†åä¸º Anecdoctoring çš„æ–°å‹ Red-Teaming æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (Generative AI) è¯¯ç”¨ä¸­å­˜åœ¨çš„è™šå‡ä¿¡æ¯ (Disinformation) é£é™©ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹ç°æœ‰è¯„ä¼°æ•°æ®é›†è¿‡äºä»¥ç¾å›½å’Œè‹±è¯­ä¸ºä¸­å¿ƒçš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡ä»ä¸‰ä¸ªè¯­ç§ï¼ˆè‹±è¯­ã€è¥¿ç­ç‰™è¯­å’Œå°åœ°è¯­ï¼‰ä»¥åŠä¸¤ä¸ªåœ°åŒºï¼ˆç¾å›½å’Œå°åº¦ï¼‰çš„äº‹å®æ ¸æŸ¥ç½‘ç«™æ”¶é›†è™šå‡ä¿¡æ¯å£°æ˜ï¼Œå®ç°äº†è·¨è¯­è¨€å’Œè·¨æ–‡åŒ–çš„è‡ªåŠ¨åŒ–å¯¹æŠ—æ€§æç¤ºç”Ÿæˆã€‚Anecdoctoring å°†ä¸ªä½“å£°æ˜èšç±»ä¸ºæ›´å¹¿æ³›çš„å™äº‹ç»“æ„ï¼Œå¹¶åˆ©ç”¨çŸ¥è¯†å›¾è°± (Knowledge Graphs) æè¿°è¿™äº›èšç±»ç‰¹å¾ï¼Œä»è€Œå¢å¼ºæ”»å‡»è€…å¤§è¯­è¨€æ¨¡å‹ (Attacker LLM)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸å°‘æ ·æœ¬æç¤º (Few-shot Prompting) ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•ä¸ä»…å®ç°äº†æ›´é«˜çš„æ”»å‡»æˆåŠŸç‡ (Attack Success Rates)ï¼Œè¿˜æä¾›äº†æ˜¾è‘—çš„å¯è§£é‡Šæ€§ä¼˜åŠ¿ã€‚è¯¥é¡¹å·¥ä½œå¼ºè°ƒäº†åœ¨å…¨çƒèŒƒå›´å†…æ‰©å±•è™šå‡ä¿¡æ¯ç¼“è§£æªæ–½çš„å¿…è¦æ€§ï¼Œå¹¶ä¸»å¼ é˜²å¾¡æœºåˆ¶åº”æ¤æ ¹äºç°å®ä¸–ç•Œçš„å¯¹æŠ—æ€§è¯¯ç”¨åœºæ™¯ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "To be published in EMNLP 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.19143v1",
      "published_date": "2025-09-23 15:26:13 UTC",
      "updated_date": "2025-09-23 15:26:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:32:35.386335+00:00"
    },
    {
      "arxiv_id": "2509.19136v2",
      "title": "On the Soundness and Consistency of LLM Agents for Executing Test Cases Written in Natural Language",
      "title_zh": "è®º LLM æ™ºèƒ½ä½“æ‰§è¡Œè‡ªç„¶è¯­è¨€æµ‹è¯•ç”¨ä¾‹çš„å¯é æ€§ä¸ä¸€è‡´æ€§",
      "authors": [
        "SÃ©bastien Salva",
        "Redha Taguelmimt"
      ],
      "abstract": "The use of natural language (NL) test cases for validating graphical user interface (GUI) applications is emerging as a promising direction to manually written executable test scripts, which are costly to develop and difficult to maintain. Recent advances in large language models (LLMs) have opened the possibility of the direct execution of NL test cases by LLM agents. This paper investigates this direction, focusing on the impact on NL test case unsoundness and on test case execution consistency. NL test cases are inherently unsound, as they may yield false failures due to ambiguous instructions or unpredictable agent behaviour. Furthermore, repeated executions of the same NL test case may lead to inconsistent outcomes, undermining test reliability. To address these challenges, we propose an algorithm for executing NL test cases with guardrail mechanisms and specialised agents that dynamically verify the correct execution of each test step. We introduce measures to evaluate the capabilities of LLMs in test execution and one measure to quantify execution consistency. We propose a definition of weak unsoundness to characterise contexts in which NL test case execution remains acceptable, with respect to the industrial quality levels Six Sigma. Our experimental evaluation with eight publicly available LLMs, ranging from 3B to 70B parameters, demonstrates both the potential and current limitations of current LLM agents for GUI testing. Our experiments show that Meta Llama 3.1 70B demonstrates acceptable capabilities in NL test case execution with high execution consistency (above the level 3-sigma). We provide prototype tools, test suites, and results.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)æ™ºèƒ½ä½“ç›´æ¥æ‰§è¡Œä»¥è‡ªç„¶è¯­è¨€(NL)ç¼–å†™çš„å›¾å½¢ç”¨æˆ·ç•Œé¢(GUI)æµ‹è¯•ç”¨ä¾‹çš„å¯è¡Œæ€§ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿå¯æ‰§è¡Œè„šæœ¬å¼€å‘æˆæœ¬é«˜ã€ç»´æŠ¤éš¾çš„é—®é¢˜ã€‚ä½œè€…æŒ‡å‡ºè‡ªç„¶è¯­è¨€æµ‹è¯•ç”¨ä¾‹å…·æœ‰å¤©ç”Ÿçš„ä¸å®Œå¤‡æ€§(Unsoundness)ï¼Œå®¹æ˜“å› æŒ‡ä»¤æ¨¡ç³Šæˆ–æ™ºèƒ½ä½“è¡Œä¸ºä¸å¯é¢„æµ‹å¯¼è‡´è™šå‡å¤±è´¥ï¼Œä¸”å¤šæ¬¡é‡å¤æ‰§è¡Œçš„ç»“æœå¾€å¾€ä¸ä¸€è‡´ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åŒ…å«æŠ¤æ æœºåˆ¶(Guardrail)å’Œä¸“é—¨æ™ºèƒ½ä½“çš„ç®—æ³•ï¼Œé€šè¿‡åŠ¨æ€éªŒè¯æ¯ä¸ªæµ‹è¯•æ­¥éª¤çš„æ­£ç¡®æ€§æ¥ç¡®ä¿æ‰§è¡Œå¯é ã€‚è®ºæ–‡å¼•å…¥äº†è¯„ä¼°LLMsæµ‹è¯•æ‰§è¡Œèƒ½åŠ›å’Œé‡åŒ–æ‰§è¡Œä¸€è‡´æ€§çš„æŒ‡æ ‡ï¼Œå¹¶å‚ç…§å·¥ä¸šç•Œå…­è¥¿æ ¼ç›(Six Sigma)è´¨é‡æ ‡å‡†å®šä¹‰äº†å¼±ä¸å®Œå¤‡æ€§(Weak Unsoundness)æ¦‚å¿µã€‚é’ˆå¯¹8ä¸ª3Bè‡³70Bå‚æ•°è§„æ¨¡çš„å…¬å¼€LLMsè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒMeta Llama 3.1 70Bè¡¨ç°å‡ºäº†å¯æ¥å—çš„æ‰§è¡Œèƒ½åŠ›åŠè¶…è¿‡3-sigmaæ°´å¹³çš„ä¸€è‡´æ€§ã€‚è¯¥ç ”ç©¶ç³»ç»Ÿæ€§åœ°å±•ç¤ºäº†å½“å‰LLMæ™ºèƒ½ä½“åœ¨GUIè‡ªåŠ¨åŒ–æµ‹è¯•ä¸­çš„æ½œåŠ›ä¸å±€é™ï¼Œå¹¶æä¾›äº†åŸå‹å·¥å…·ä¸é…å¥—æµ‹è¯•é›†ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19136v2",
      "published_date": "2025-09-23 15:20:40 UTC",
      "updated_date": "2025-10-01 09:32:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:32:41.892935+00:00"
    },
    {
      "arxiv_id": "2509.19135v1",
      "title": "GSTM-HMU: Generative Spatio-Temporal Modeling for Human Mobility Understanding",
      "title_zh": "GSTM-HMUï¼šé¢å‘äººç±»ç§»åŠ¨æ€§ç†è§£çš„ç”Ÿæˆå¼æ—¶ç©ºå»ºæ¨¡",
      "authors": [
        "Wenying Luo",
        "Zhiyuan Lin",
        "Wenhao Xu",
        "Minghao Liu",
        "Zhi Li"
      ],
      "abstract": "Human mobility traces, often recorded as sequences of check-ins, provide a unique window into both short-term visiting patterns and persistent lifestyle regularities. In this work we introduce GSTM-HMU, a generative spatio-temporal framework designed to advance mobility analysis by explicitly modeling the semantic and temporal complexity of human movement. The framework consists of four key innovations. First, a Spatio-Temporal Concept Encoder (STCE) integrates geographic location, POI category semantics, and periodic temporal rhythms into unified vector representations. Second, a Cognitive Trajectory Memory (CTM) adaptively filters historical visits, emphasizing recent and behaviorally salient events in order to capture user intent more effectively. Third, a Lifestyle Concept Bank (LCB) contributes structured human preference cues, such as activity types and lifestyle patterns, to enhance interpretability and personalization. Finally, task-oriented generative heads transform the learned representations into predictions for multiple downstream tasks. We conduct extensive experiments on four widely used real-world datasets, including Gowalla, WeePlace, Brightkite, and FourSquare, and evaluate performance on three benchmark tasks: next-location prediction, trajectory-user identification, and time estimation. The results demonstrate consistent and substantial improvements over strong baselines, confirming the effectiveness of GSTM-HMU in extracting semantic regularities from complex mobility data. Beyond raw performance gains, our findings also suggest that generative modeling provides a promising foundation for building more robust, interpretable, and generalizable systems for human mobility intelligence.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† GSTM-HMUï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨é€šè¿‡æ˜¾å¼å»ºæ¨¡äººç±»ç§»åŠ¨çš„è¯­ä¹‰å’Œæ—¶ç©ºå¤æ‚æ€§æ¥æå‡æµåŠ¨æ€§åˆ†æçš„ç”Ÿæˆå¼æ—¶ç©ºæ¡†æ¶ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†æ—¶ç©ºæ¦‚å¿µç¼–ç å™¨ (Spatio-Temporal Concept Encoder, STCE) æ¥æ•´åˆåœ°ç†ä½ç½®ã€POI ç±»åˆ«è¯­ä¹‰å’Œå‘¨æœŸæ€§æ—¶é—´èŠ‚å¾‹ï¼Œå¹¶åˆ©ç”¨è®¤çŸ¥è½¨è¿¹è®°å¿† (Cognitive Trajectory Memory, CTM) è‡ªé€‚åº”è¿‡æ»¤å†å²è®¿é—®ä»¥æ•æ‰ç”¨æˆ·æ„å›¾ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ„å»ºäº†ç”Ÿæ´»æ–¹å¼æ¦‚å¿µåº“ (Lifestyle Concept Bank, LCB) ä»¥æä¾›ç»“æ„åŒ–çš„äººç±»åå¥½çº¿ç´¢ï¼Œå¹¶ç»“åˆä»»åŠ¡å¯¼å‘çš„ç”Ÿæˆå¤´å¤„ç†ä¸‹æ¸¸ä»»åŠ¡ã€‚åœ¨ Gowallaã€FourSquare ç­‰å››ä¸ªçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGSTM-HMU åœ¨ä¸‹ä¸€åœ°ç‚¹é¢„æµ‹ã€è½¨è¿¹ç”¨æˆ·è¯†åˆ«å’Œæ—¶é—´ä¼°è®¡ä¸‰ä¸ªåŸºå‡†ä»»åŠ¡ä¸Šå‡æ˜¾è‘—ä¼˜äºå¼ºåŸºçº¿æ¨¡å‹ã€‚è¯¥æˆæœä¸ä»…æå‡äº†é¢„æµ‹æ€§èƒ½ï¼Œè¿˜ä¸ºæ„å»ºæ›´å…·é²æ£’æ€§ã€å¯è§£é‡Šæ€§å’Œé€šç”¨æ€§çš„äººç±»æµåŠ¨æ€§æ™ºèƒ½ç³»ç»Ÿæä¾›äº†ç”Ÿæˆå¼å»ºæ¨¡åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19135v1",
      "published_date": "2025-09-23 15:20:38 UTC",
      "updated_date": "2025-09-23 15:20:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:33:21.986271+00:00"
    },
    {
      "arxiv_id": "2510.01231v1",
      "title": "Trustworthy Summarization via Uncertainty Quantification and Risk Awareness in Large Language Models",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ä¸­åŸºäºä¸ç¡®å®šæ€§é‡åŒ–ä¸é£é™©æ„ŸçŸ¥çš„å¯ä¿¡æ‘˜è¦",
      "authors": [
        "Shuaidong Pan",
        "Di Wu"
      ],
      "abstract": "This study addresses the reliability of automatic summarization in high-risk scenarios and proposes a large language model framework that integrates uncertainty quantification and risk-aware mechanisms. Starting from the demands of information overload and high-risk decision-making, a conditional generation-based summarization model is constructed, and Bayesian inference is introduced during generation to model uncertainty in the parameter space, which helps avoid overconfident predictions. The uncertainty level of the generated content is measured using predictive distribution entropy, and a joint optimization of entropy regularization and risk-aware loss is applied to ensure that key information is preserved and risk attributes are explicitly expressed during information compression. On this basis, the model incorporates risk scoring and regulation modules, allowing summaries to cover the core content accurately while enhancing trustworthiness through explicit risk-level prompts. Comparative experiments and sensitivity analyses verify that the proposed method significantly improves the robustness and reliability of summarization in high-risk applications while maintaining fluency and semantic integrity. This research provides a systematic solution for trustworthy summarization and demonstrates both scalability and practical value at the methodological level.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹é«˜é£é™©åœºæ™¯ä¸‹çš„è‡ªåŠ¨æ‘˜è¦å¯é æ€§é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ•´åˆä¸ç¡®å®šæ€§é‡åŒ–(Uncertainty Quantification)ä¸é£é™©æ„ŸçŸ¥(Risk-Aware)æœºåˆ¶çš„å¤§è¯­è¨€æ¨¡å‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å¼•å…¥è´å¶æ–¯æ¨ç†(Bayesian Inference)å¯¹å‚æ•°ç©ºé—´çš„ä¸ç¡®å®šæ€§è¿›è¡Œå»ºæ¨¡ï¼Œæœ‰æ•ˆé¿å…äº†æ¨¡å‹äº§ç”Ÿè¿‡åº¦è‡ªä¿¡çš„é¢„æµ‹ã€‚ç ”ç©¶é€šè¿‡é¢„æµ‹åˆ†å¸ƒç†µ(Predictive Distribution Entropy)è¡¡é‡ç”Ÿæˆå†…å®¹çš„ä¸ç¡®å®šæ€§æ°´å¹³ï¼Œå¹¶ç»“åˆç†µæ­£åˆ™åŒ–ä¸é£é™©æ„ŸçŸ¥æŸå¤±çš„è”åˆä¼˜åŒ–ï¼Œç¡®ä¿æ ¸å¿ƒä¿¡æ¯å‹ç¼©è¿‡ç¨‹ä¸­é£é™©å±æ€§çš„æ˜ç¡®è¡¨è¾¾ã€‚æ­¤å¤–ï¼Œæ¨¡å‹æ•´åˆäº†é£é™©è¯„åˆ†ä¸è°ƒèŠ‚æ¨¡å—ï¼Œä½¿æ‘˜è¦åœ¨å‡†ç¡®è¦†ç›–æ ¸å¿ƒå†…å®¹çš„åŒæ—¶ï¼Œé€šè¿‡æ˜¾å¼çš„é£é™©ç­‰çº§æç¤ºå¢å¼ºå¯ä¿¡åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒè¯­ä¹‰å®Œæ•´æ€§çš„å‰æä¸‹ï¼Œæ˜¾è‘—æå‡äº†é«˜é£é™©åº”ç”¨ä¸­æ‘˜è¦ç”Ÿæˆçš„ç¨³å¥æ€§ä¸å¯é æ€§ã€‚è¯¥ç ”ç©¶ä¸ºå¯ä¿¡æ‘˜è¦(Trustworthy Summarization)æä¾›äº†ç³»ç»Ÿæ€§è§£å†³æ–¹æ¡ˆï¼Œå¹¶åœ¨æ–¹æ³•è®ºå±‚é¢è¯æ˜äº†å…¶æ‰©å±•æ€§ä¸å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.01231v1",
      "published_date": "2025-09-23 15:09:46 UTC",
      "updated_date": "2025-09-23 15:09:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:33:05.698722+00:00"
    },
    {
      "arxiv_id": "2509.19122v1",
      "title": "Analysis on distribution and clustering of weight",
      "title_zh": "æƒé‡åˆ†å¸ƒä¸èšç±»åˆ†æ",
      "authors": [
        "Chunming Ye",
        "Wenquan Tian",
        "Yalan Gao",
        "Songzhou Li"
      ],
      "abstract": "The study on architecture and parameter characteristics remains the hot topic in the research of large language models. In this paper we concern with the characteristics of weight which are used to analyze the correlations and differences between models. Two kinds of vectors-standard deviation vector and clustering vector-are proposed to describe features of models. In the first case, the weights are assumed to follow normal distribution. The standard deviation values of projection matrices are normalized to form Standard-Deviation Vector, representing the distribution characteristics of models. In the second case, the singular values from each weight projection matrix are extracted and grouped by K-Means algorithm. The grouped data with the same type matrix are combined as Clustering Vector to represent the correlation characteristics of models' weights. The study reveals that these two vectors can effectively distinguish between different models and clearly show the similarities among models of the same family. Moreover, after conducting LoRA fine-tuning with different datasets and models, it is found that the distribution of weights represented by standard deviation vector is directly influenced by the dataset, but the correlations between different weights represented by clustering vector remain unaffected and maintain a high consistency with the pre-trained model.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Modelsï¼‰çš„æƒé‡ç‰¹å¾ï¼Œæ—¨åœ¨é€šè¿‡åˆ†æå‚æ•°æ¥æ­ç¤ºä¸åŒæ¨¡å‹é—´çš„å…³è”ä¸å·®å¼‚ã€‚ç ”ç©¶æå‡ºäº†æ ‡å‡†å·®å‘é‡ï¼ˆStandard-Deviation Vectorï¼‰å’Œèšç±»å‘é‡ï¼ˆClustering Vectorï¼‰ä¸¤ç§è¡¨å¾æ–¹æ³•ï¼Œå‰è€…ç”¨äºæè¿°æƒé‡çš„åˆ†å¸ƒç‰¹å¾ï¼Œåè€…åˆ™ç»“åˆå¥‡å¼‚å€¼æå–ä¸ K-Means ç®—æ³•æ¥æ•æ‰æƒé‡é—´çš„ç›¸å…³æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¿™ä¸¤ç§å‘é‡èƒ½å¤Ÿæœ‰æ•ˆåŒºåˆ†ä¸åŒæ¶æ„çš„æ¨¡å‹ï¼Œå¹¶æ¸…æ™°åœ°å‘ˆç°åŒç³»åˆ—æ¨¡å‹é—´çš„ç›¸ä¼¼æ€§ã€‚è¿›ä¸€æ­¥çš„ LoRA å¾®è°ƒå®éªŒè¡¨æ˜ï¼Œè™½ç„¶æƒé‡åˆ†å¸ƒç‰¹å¾æ˜“å—è®­ç»ƒæ•°æ®é›†çš„å½±å“ï¼Œä½†èšç±»å‘é‡æ‰€ä»£è¡¨çš„æƒé‡å…³è”æ€§åœ¨å¾®è°ƒå‰åä¿æŒäº†æé«˜çš„ä¸€è‡´æ€§ï¼Œä½“ç°äº†å…¶å¯¹é¢„è®­ç»ƒæ¨¡å‹ï¼ˆPre-trained Modelï¼‰å›ºæœ‰ç‰¹æ€§çš„ä¿ç•™ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "14page,16 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.19122v1",
      "published_date": "2025-09-23 15:08:25 UTC",
      "updated_date": "2025-09-23 15:08:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:33:14.088132+00:00"
    },
    {
      "arxiv_id": "2509.19120v1",
      "title": "FedFiTS: Fitness-Selected, Slotted Client Scheduling for Trustworthy Federated Learning in Healthcare AI",
      "title_zh": "FedFiTSï¼šé¢å‘åŒ»ç–—äººå·¥æ™ºèƒ½å¯ä¿¡è”é‚¦å­¦ä¹ çš„é€‚åº”åº¦ç­›é€‰ä¸åˆ†æ§½å¼å®¢æˆ·ç«¯è°ƒåº¦",
      "authors": [
        "Ferdinand Kahenga",
        "Antoine Bagula",
        "Sajal K. Das",
        "Patrick Sello"
      ],
      "abstract": "Federated Learning (FL) has emerged as a powerful paradigm for privacy-preserving model training, yet deployments in sensitive domains such as healthcare face persistent challenges from non-IID data, client unreliability, and adversarial manipulation. This paper introduces FedFiTS, a trust and fairness-aware selective FL framework that advances the FedFaSt line by combining fitness-based client election with slotted aggregation. FedFiTS implements a three-phase participation strategy-free-for-all training, natural selection, and slotted team participation-augmented with dynamic client scoring, adaptive thresholding, and cohort-based scheduling to balance convergence efficiency with robustness. A theoretical convergence analysis establishes bounds for both convex and non-convex objectives under standard assumptions, while a communication-complexity analysis shows reductions relative to FedAvg and other baselines. Experiments on diverse datasets-medical imaging (X-ray pneumonia), vision benchmarks (MNIST, FMNIST), and tabular agricultural data (Crop Recommendation)-demonstrate that FedFiTS consistently outperforms FedAvg, FedRand, and FedPow in accuracy, time-to-target, and resilience to poisoning attacks. By integrating trust-aware aggregation with fairness-oriented client selection, FedFiTS advances scalable and secure FL, making it well suited for real-world healthcare and cross-domain deployments.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† FedFiTSï¼Œä¸€ç§é’ˆå¯¹åŒ»ç–—äººå·¥æ™ºèƒ½ä¸­ Federated Learning (FL) é¢ä¸´çš„éç‹¬ç«‹åŒåˆ†å¸ƒ (non-IID) æ•°æ®ã€å®¢æˆ·ç«¯ä¸å¯é æ€§å’Œå¯¹æŠ—æ€§æ“çºµç­‰æŒ‘æˆ˜è€Œè®¾è®¡çš„å¯ä¿¡ä¸å…¬å¹³æ„ŸçŸ¥è”é‚¦å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶åœ¨ FedFaSt åŸºç¡€ä¸Šç»“åˆäº†åŸºäºé€‚åº”åº¦ (fitness-based) çš„å®¢æˆ·ç«¯é€‰ä¸¾ä¸æ—¶éš™èšåˆ (slotted aggregation) æŠ€æœ¯ï¼Œé€šè¿‡å…¨æ°‘è®­ç»ƒ (free-for-all training)ã€è‡ªç„¶é€‰æ‹© (natural selection) å’Œæ—¶éš™å›¢é˜Ÿå‚ä¸ (slotted team participation) ä¸‰é˜¶æ®µç­–ç•¥å®ç°é«˜æ•ˆè®­ç»ƒã€‚åŒæ—¶ï¼ŒFedFiTS å¼•å…¥äº†åŠ¨æ€å®¢æˆ·ç«¯è¯„åˆ† (dynamic client scoring) å’Œè‡ªé€‚åº”é˜ˆå€¼ (adaptive thresholding) ç­‰æœºåˆ¶ï¼Œä»¥åœ¨æ”¶æ•›æ•ˆç‡ä¸ç¨³å¥æ€§ä¹‹é—´å–å¾—å¹³è¡¡ã€‚ç†è®ºåˆ†æç¡®å®šäº†è¯¥æ–¹æ³•åœ¨å‡¸ä¸éå‡¸ç›®æ ‡ä¸‹çš„æ”¶æ•›è¾¹ç•Œï¼Œå¹¶éªŒè¯äº†å…¶åœ¨é€šä¿¡å¤æ‚åº¦ (communication-complexity) ä¸Šçš„æ˜¾è‘—é™ä½ã€‚åœ¨ X-ray pneumonia åŒ»ç–—å½±åƒåŠå¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒFedFiTS åœ¨å‡†ç¡®ç‡ã€ç›®æ ‡è¾¾æˆæ—¶é—´åŠå¯¹æŠ—ä¸­æ¯’æ”»å‡» (poisoning attacks) çš„éŸ§æ€§æ–¹é¢å‡ä¼˜äº FedAvg å’Œ FedRand ç­‰åŸºå‡†æ¨¡å‹ï¼Œä¸ºåŒ»ç–—åŠè·¨é¢†åŸŸçš„å¤§è§„æ¨¡å®‰å…¨ FL éƒ¨ç½²æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19120v1",
      "published_date": "2025-09-23 15:06:04 UTC",
      "updated_date": "2025-09-23 15:06:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:33:12.999585+00:00"
    },
    {
      "arxiv_id": "2509.19112v3",
      "title": "Towards Practical Multi-label Causal Discovery in High-Dimensional Event Sequences via One-Shot Graph Aggregation",
      "title_zh": "é€šè¿‡å•æ¬¡å›¾èšåˆå®ç°é«˜ç»´äº‹ä»¶åºåˆ—ä¸­çš„å®ç”¨å¤šæ ‡ç­¾å› æœå‘ç°",
      "authors": [
        "Hugo Math",
        "Rainer Lienhart"
      ],
      "abstract": "Understanding causality in event sequences where outcome labels such as diseases or system failures arise from preceding events like symptoms or error codes is critical. Yet remains an unsolved challenge across domains like healthcare or vehicle diagnostics. We introduce CARGO, a scalable multi-label causal discovery method for sparse, high-dimensional event sequences comprising of thousands of unique event types. Using two pretrained causal Transformers as domain-specific foundation models for event sequences. CARGO infers in parallel, per sequence one-shot causal graphs and aggregates them using an adaptive frequency fusion to reconstruct the global Markov boundaries of labels. This two-stage approach enables efficient probabilistic reasoning at scale while bypassing the intractable cost of full-dataset conditional independence testing. Our results on a challenging real-world automotive fault prediction dataset with over 29,100 unique event types and 474 imbalanced labels demonstrate CARGO's ability to perform structured reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»ç–—æˆ–è½¦è¾†è¯Šæ–­ç­‰é¢†åŸŸä¸­é«˜ç»´ç¨€ç–äº‹ä»¶åºåˆ—çš„å¤šæ ‡ç­¾å› æœå‘ç°éš¾é¢˜ï¼Œæå‡ºäº†å¯æ‰©å±•çš„æ–¹æ³• CARGOã€‚è¯¥æ–¹æ³•åˆ©ç”¨ä¸¤ä¸ªé¢„è®­ç»ƒçš„å› æœ Transformer ä½œä¸ºé¢†åŸŸç‰¹å®šåŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡å¹¶è¡Œæ–¹å¼ä¸ºæ¯ä¸ªåºåˆ—æ¨æ–­ one-shot å› æœå›¾ã€‚éšåï¼ŒCARGO é‡‡ç”¨è‡ªé€‚åº”é¢‘ç‡èåˆ (Adaptive frequency fusion) æŠ€æœ¯èšåˆè¿™äº›å›¾ï¼Œä»¥é‡å»ºæ ‡ç­¾çš„å…¨å±€ Markov boundariesã€‚è¿™ç§ä¸¤é˜¶æ®µæ–¹æ³•å®ç°äº†é«˜æ•ˆçš„å¤§è§„æ¨¡æ¦‚ç‡æ¨ç†ï¼Œæœ‰æ•ˆé¿å¼€äº†å…¨æ•°æ®é›†æ¡ä»¶ç‹¬ç«‹æ€§æµ‹è¯• (Conditional independence testing) çš„é«˜æ˜‚è®¡ç®—æˆæœ¬ã€‚åœ¨åŒ…å«è¶…è¿‡ 29,100 ç§å”¯ä¸€äº‹ä»¶ç±»å‹å’Œ 474 ä¸ªä¸å¹³è¡¡æ ‡ç­¾çš„çœŸå®æ±½è½¦æ•…éšœé¢„æµ‹æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCARGO å…·å¤‡å¼ºå¤§çš„ç»“æ„åŒ–æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿèƒœä»»æé«˜ç»´åº¦çš„å®é™…å› æœå»ºæ¨¡ä»»åŠ¡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at NeurIPS2025 Workshop on Structured Probabilistic Inference and Generative Modeling",
      "pdf_url": "https://arxiv.org/pdf/2509.19112v3",
      "published_date": "2025-09-23 14:58:50 UTC",
      "updated_date": "2025-12-12 09:54:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:33:24.363483+00:00"
    },
    {
      "arxiv_id": "2509.21381v1",
      "title": "Toward a Realistic Encoding Model of Auditory Affective Understanding in the Brain",
      "title_zh": "è¿ˆå‘å¤§è„‘å¬è§‰æƒ…æ„Ÿç†è§£çš„ç°å®ç¼–ç æ¨¡å‹",
      "authors": [
        "Guandong Pan",
        "Yaqian Yang",
        "Shi Chen",
        "Xin Wang",
        "Longzhao Liu",
        "Hongwei Zheng",
        "Shaoting Tang"
      ],
      "abstract": "In affective neuroscience and emotion-aware AI, understanding how complex auditory stimuli drive emotion arousal dynamics remains unresolved. This study introduces a computational framework to model the brain's encoding of naturalistic auditory inputs into dynamic behavioral/neural responses across three datasets (SEED, LIRIS, self-collected BAVE). Guided by neurobiological principles of parallel auditory hierarchy, we decompose audio into multilevel auditory features (through classical algorithms and wav2vec 2.0/Hubert) from the original and isolated human voice/background soundtrack elements, mapping them to emotion-related responses via cross-dataset analyses. Our analysis reveals that high-level semantic representations (derived from the final layer of wav2vec 2.0/Hubert) exert a dominant role in emotion encoding, outperforming low-level acoustic features with significantly stronger mappings to behavioral annotations and dynamic neural synchrony across most brain regions ($p < 0.05$). Notably, middle layers of wav2vec 2.0/hubert (balancing acoustic-semantic information) surpass the final layers in emotion induction across datasets. Moreover, human voices and soundtracks show dataset-dependent emotion-evoking biases aligned with stimulus energy distribution (e.g., LIRIS favors soundtracks due to higher background energy), with neural analyses indicating voices dominate prefrontal/temporal activity while soundtracks excel in limbic regions. By integrating affective computing and neuroscience, this work uncovers hierarchical mechanisms of auditory-emotion encoding, providing a foundation for adaptive emotion-aware systems and cross-disciplinary explorations of audio-affective interactions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æƒ…æ„Ÿç¥ç»ç§‘å­¦(affective neuroscience)å’Œæƒ…ç»ªæ„ŸçŸ¥ AI é¢†åŸŸä¸­å¤æ‚å¬è§‰åˆºæ¿€å¦‚ä½•é©±åŠ¨æƒ…ç»ªå”¤é†’åŠ¨åŠ›å­¦çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªæ¨¡æ‹Ÿå¤§è„‘å°†è‡ªç„¶å¬è§‰è¾“å…¥ç¼–ç ä¸ºåŠ¨æ€è¡Œä¸ºå’Œç¥ç»ååº”çš„è®¡ç®—æ¡†æ¶ã€‚ç ”ç©¶åˆ©ç”¨ SEEDã€LIRIS å’Œ BAVE æ•°æ®é›†ï¼Œå°†éŸ³é¢‘åˆ†è§£ä¸ºäººå£°ä¸èƒŒæ™¯åŸå£°ï¼Œå¹¶ç»“åˆ wav2vec 2.0 å’Œ HuBERT æå–çš„å¤šå±‚çº§ç‰¹å¾è¿›è¡Œè·¨æ•°æ®é›†åˆ†æã€‚ç»“æœå‘ç°ï¼Œé«˜å±‚è¯­ä¹‰è¡¨å¾åœ¨æƒ…ç»ªç¼–ç ä¸­å æ®ä¸»å¯¼åœ°ä½ï¼Œå…¶åœ¨è¡Œä¸ºæ ‡æ³¨å’Œå¤šæ•°è„‘åŒºçš„åŠ¨æ€ç¥ç»åŒæ­¥æ€§æ˜ å°„ä¸Šæ˜¾è‘—ä¼˜äºä½å±‚å£°å­¦ç‰¹å¾ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¹³è¡¡äº†å£°å­¦ä¸è¯­ä¹‰ä¿¡æ¯çš„ wav2vec 2.0 å’Œ HuBERT ä¸­é—´å±‚åœ¨æƒ…ç»ªè¯±å¯¼è¡¨ç°ä¸Šç”šè‡³è¶…è¿‡äº†æœ€ç»ˆå±‚ã€‚ç¥ç»åˆ†æè¿›ä¸€æ­¥æ­ç¤ºäººå£°ä¸»è¦æ”¯é…å‰é¢å¶(prefrontal)å’Œé¢å¶(temporal)æ´»åŠ¨ï¼Œè€ŒèƒŒæ™¯åŸå£°åˆ™åœ¨è¾¹ç¼˜ç³»ç»Ÿ(limbic regions)æ›´å…·ä¼˜åŠ¿ï¼Œä¸”è¿™ç§åå¥½ä¸åˆºæ¿€çš„èƒ½é‡åˆ†å¸ƒç›¸å…³ã€‚è¯¥å·¥ä½œæ•´åˆæƒ…æ„Ÿè®¡ç®—ä¸ç¥ç»ç§‘å­¦ï¼Œæ­ç¤ºäº†å¬è§‰-æƒ…ç»ªç¼–ç çš„å±‚æ¬¡åŒ–æœºåˆ¶ï¼Œä¸ºè·¨å­¦ç§‘çš„å¬è§‰-æƒ…æ„Ÿäº¤äº’ç ”ç©¶å’Œè‡ªé€‚åº”æƒ…ç»ªæ„ŸçŸ¥ç³»ç»Ÿæä¾›äº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21381v1",
      "published_date": "2025-09-23 14:52:11 UTC",
      "updated_date": "2025-09-23 14:52:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:33:31.490883+00:00"
    },
    {
      "arxiv_id": "2509.19102v1",
      "title": "FUNCanon: Learning Pose-Aware Action Primitives via Functional Object Canonicalization for Generalizable Robotic Manipulation",
      "title_zh": "FUNCanonï¼šé€šè¿‡åŠŸèƒ½æ€§ç‰©ä½“è§„èŒƒåŒ–å­¦ä¹ ä½å§¿æ„ŸçŸ¥åŠ¨ä½œåŸºå…ƒï¼Œä»¥å®ç°å¯æ³›åŒ–çš„æœºå™¨äººæ“ä½œ",
      "authors": [
        "Hongli Xu",
        "Lei Zhang",
        "Xiaoyue Hu",
        "Boyang Zhong",
        "Kaixin Bai",
        "ZoltÃ¡n-Csaba MÃ¡rton",
        "Zhenshan Bing",
        "Zhaopeng Chen",
        "Alois Christian Knoll",
        "Jianwei Zhang"
      ],
      "abstract": "General-purpose robotic skills from end-to-end demonstrations often leads to task-specific policies that fail to generalize beyond the training distribution. Therefore, we introduce FunCanon, a framework that converts long-horizon manipulation tasks into sequences of action chunks, each defined by an actor, verb, and object. These chunks focus policy learning on the actions themselves, rather than isolated tasks, enabling compositionality and reuse. To make policies pose-aware and category-general, we perform functional object canonicalization for functional alignment and automatic manipulation trajectory transfer, mapping objects into shared functional frames using affordance cues from large vision language models. An object centric and action centric diffusion policy FuncDiffuser trained on this aligned data naturally respects object affordances and poses, simplifying learning and improving generalization ability. Experiments on simulated and real-world benchmarks demonstrate category-level generalization, cross-task behavior reuse, and robust sim2real deployment, showing that functional canonicalization provides a strong inductive bias for scalable imitation learning in complex manipulation domains. Details of the demo and supplemental material are available on our project website https://sites.google.com/view/funcanon.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FUNCanonæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç«¯åˆ°ç«¯æ¼”ç¤ºç”Ÿæˆçš„é€šç”¨æœºå™¨äººæŠ€èƒ½åœ¨é¢å¯¹åˆ†å¸ƒå¤–ä»»åŠ¡æ—¶æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶å°†é•¿æ—¶ç¨‹æ“ä½œä»»åŠ¡åˆ†è§£ä¸ºç”±æ‰§è¡Œè€…ã€åŠ¨è¯å’Œç‰©ä½“å®šä¹‰çš„action chunksåºåˆ—ï¼Œé€šè¿‡èšç„¦åŠ¨ä½œæœ¬èº«æ¥å¢å¼ºç­–ç•¥çš„ç»„åˆæ€§ä¸å¤ç”¨æ€§ã€‚ä¸ºäº†å®ç°å§¿æ€æ„ŸçŸ¥å’Œç±»åˆ«é€šç”¨æ€§ï¼ŒFUNCanonåˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æä¾›çš„affordanceæç¤ºè¿›è¡Œfunctional object canonicalizationï¼Œå°†ä¸åŒç‰©ä½“æ˜ å°„åˆ°ç»Ÿä¸€çš„åŠŸèƒ½å‚è€ƒç³»ä¸­ã€‚åŸºäºæ­¤å¯¹é½æ•°æ®è®­ç»ƒçš„FuncDiffuseræ‰©æ•£ç­–ç•¥èƒ½å¤Ÿç²¾å‡†æ•æ‰ç‰©ä½“çš„affordanceä¸å§¿æ€ï¼Œæœ‰æ•ˆé™ä½äº†å­¦ä¹ éš¾åº¦å¹¶æ˜¾è‘—æå‡äº†æ³›åŒ–æ€§èƒ½ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä»¿çœŸå’ŒçœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸­å‡å®ç°äº†æ˜¾è‘—çš„ç±»åˆ«çº§æ³›åŒ–ã€è·¨ä»»åŠ¡è¡Œä¸ºå¤ç”¨ä»¥åŠç¨³å¥çš„sim2realéƒ¨ç½²ï¼Œä¸ºå¤æ‚æ“ä½œé¢†åŸŸçš„æ‰©å±•æ¨¡ä»¿å­¦ä¹ æä¾›äº†å¼ºå¤§çš„å½’çº³åç½®ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "project website: https://sites.google.com/view/funcanon, 11 pages",
      "pdf_url": "https://arxiv.org/pdf/2509.19102v1",
      "published_date": "2025-09-23 14:49:05 UTC",
      "updated_date": "2025-09-23 14:49:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:33:31.662837+00:00"
    },
    {
      "arxiv_id": "2509.19100v1",
      "title": "Algorithms for Adversarially Robust Deep Learning",
      "title_zh": "å¯¹æŠ—é²æ£’æ·±åº¦å­¦ä¹ ç®—æ³•",
      "authors": [
        "Alexander Robey"
      ],
      "abstract": "Given the widespread use of deep learning models in safety-critical applications, ensuring that the decisions of such models are robust against adversarial exploitation is of fundamental importance. In this thesis, we discuss recent progress toward designing algorithms that exhibit desirable robustness properties. First, we discuss the problem of adversarial examples in computer vision, for which we introduce new technical results, training paradigms, and certification algorithms. Next, we consider the problem of domain generalization, wherein the task is to train neural networks to generalize from a family of training distributions to unseen test distributions. We present new algorithms that achieve state-of-the-art generalization in medical imaging, molecular identification, and image classification. Finally, we study the setting of jailbreaking large language models (LLMs), wherein an adversarial user attempts to design prompts that elicit objectionable content from an LLM. We propose new attacks and defenses, which represent the frontier of progress toward designing robust language-based agents.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä¸ºæ·±åº¦å­¦ä¹ æ¨¡å‹è®¾è®¡å¯¹æŠ—é²æ£’æ€§ (Adversarially Robust) ç®—æ³•çš„æœ€æ–°è¿›å±•ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹åœ¨å…³é”®å®‰å…¨åº”ç”¨ä¸­çš„å†³ç­–ç¨³å®šæ€§ã€‚åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸï¼Œè®ºæ–‡å¼•å…¥äº†é’ˆå¯¹å¯¹æŠ—æ ·æœ¬ (Adversarial Examples) çš„æ–°æŠ€æœ¯ç»“æœã€è®­ç»ƒèŒƒå¼å’Œè®¤è¯ç®—æ³• (Certification Algorithms)ã€‚é’ˆå¯¹é¢†åŸŸæ³›åŒ– (Domain Generalization) éš¾é¢˜ï¼Œç ”ç©¶æå‡ºäº†èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†æœªè§æµ‹è¯•åˆ†å¸ƒçš„æ–°ç®—æ³•ï¼Œå¹¶åœ¨åŒ»å­¦å½±åƒ (Medical Imaging)ã€åˆ†å­è¯†åˆ«åŠå›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­è¾¾åˆ°äº† SOTA æ°´å¹³ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜åˆ†æäº†å¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„è¶Šç‹± (Jailbreaking) æ”»å‡»ï¼Œå¹¶è®¾è®¡äº†ä»£è¡¨å‰æ²¿è¿›å±•çš„é˜²å¾¡ä¸æ”»å‡»ç­–ç•¥ã€‚è¿™äº›æˆæœå…±åŒä¸ºæ„å»ºå…·å¤‡é²æ£’æ€§çš„è¯­è¨€æ™ºèƒ½ä½“å’Œè§†è§‰æ„ŸçŸ¥ç³»ç»Ÿå¥ å®šäº†åšå®çš„ç®—æ³•åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "PhD thesis",
      "pdf_url": "https://arxiv.org/pdf/2509.19100v1",
      "published_date": "2025-09-23 14:48:58 UTC",
      "updated_date": "2025-09-23 14:48:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:33:54.873843+00:00"
    },
    {
      "arxiv_id": "2509.19412v1",
      "title": "EngravingGNN: A Hybrid Graph Neural Network for End-to-End Piano Score Engraving",
      "title_zh": "EngravingGNNï¼šç”¨äºç«¯åˆ°ç«¯é’¢ç´ä¹è°±åˆ¶è°±çš„æ··åˆå›¾ç¥ç»ç½‘ç»œ",
      "authors": [
        "Emmanouil Karystinaios",
        "Francesco Foscarin",
        "Gerhard Widmer"
      ],
      "abstract": "This paper focuses on automatic music engraving, i.e., the creation of a humanly-readable musical score from musical content. This step is fundamental for all applications that include a human player, but it remains a mostly unexplored topic in symbolic music processing. In this work, we formalize the problem as a collection of interdependent subtasks, and propose a unified graph neural network (GNN) framework that targets the case of piano music and quantized symbolic input. Our method employs a multi-task GNN to jointly predict voice connections, staff assignments, pitch spelling, key signature, stem direction, octave shifts, and clef signs. A dedicated postprocessing pipeline generates print-ready MusicXML/MEI outputs. Comprehensive evaluation on two diverse piano corpora (J-Pop and DCML Romantic) demonstrates that our unified model achieves good accuracy across all subtasks, compared to existing systems that only specialize in specific subtasks. These results indicate that a shared GNN encoder with lightweight task-specific decoders in a multi-task setting offers a scalable and effective solution for automatic music engraving.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨ä¹è°±é›•ç‰ˆ (automatic music engraving) è¿™ä¸€åœ¨ç¬¦å·éŸ³ä¹å¤„ç†é¢†åŸŸå°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢çš„è¯¾é¢˜ï¼Œæå‡ºäº† EngravingGNNï¼Œä¸€ä¸ªç”¨äºé’¢ç´è°±ç«¯åˆ°ç«¯ç”Ÿæˆçš„ç»Ÿä¸€å›¾ç¥ç»ç½‘ç»œ (GNN) æ¡†æ¶ã€‚è¯¥æ–¹æ³•å°†é›•ç‰ˆé—®é¢˜å½¢å¼åŒ–ä¸ºä¸€ç³»åˆ—ç›¸äº’ä¾èµ–çš„å­ä»»åŠ¡ï¼Œåˆ©ç”¨å¤šä»»åŠ¡ GNN åŒæ—¶é¢„æµ‹å£°éƒ¨è¿æ¥ (voice connections)ã€è°±è¡¨åˆ†é… (staff assignments)ã€å˜éŸ³è®°å·æ‹¼å†™ (pitch spelling)ã€è°ƒå· (key signature)ã€ç¬¦å¹²æ–¹å‘ (stem direction)ã€å…«åº¦ç§»åŠ¨ (octave shifts) å’Œè°±å· (clef signs)ã€‚æ¡†æ¶ç»“åˆäº†ä¸“é—¨çš„åå¤„ç†æµæ°´çº¿ï¼Œå¯ç›´æ¥ç”Ÿæˆæ‰“å°å°±ç»ªçš„ MusicXML æˆ– MEI æ ¼å¼è¾“å‡ºã€‚åœ¨ J-Pop å’Œ DCML Romantic ä¸¤ä¸ªé’¢ç´è¯­æ–™åº“ä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼Œä¸ä»…ä¸“æ³¨äºç‰¹å®šå­ä»»åŠ¡çš„ç°æœ‰ç³»ç»Ÿç›¸æ¯”ï¼Œè¯¥æ¨¡å‹åœ¨æ‰€æœ‰å­ä»»åŠ¡ä¸Šå‡å®ç°äº†è¾ƒé«˜çš„å‡†ç¡®ç‡ã€‚å®éªŒç»“æœè¯æ˜ï¼Œåœ¨å¤šä»»åŠ¡è®¾ç½®ä¸‹ä½¿ç”¨å…±äº«çš„ GNN ç¼–ç å™¨é…åˆè½»é‡åŒ–ä»»åŠ¡ç‰¹å®šè§£ç å™¨ï¼Œä¸ºè‡ªåŠ¨åŒ–ä¹è°±é›•ç‰ˆæä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.GR",
        "cs.AI"
      ],
      "primary_category": "cs.GR",
      "comment": "Accepted at the International Conference on Technologies for Music Notation and Representation (TENOR) 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.19412v1",
      "published_date": "2025-09-23 14:48:35 UTC",
      "updated_date": "2025-09-23 14:48:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:33:35.465749+00:00"
    },
    {
      "arxiv_id": "2509.19094v2",
      "title": "Pathways of Thoughts: Multi-Directional Thinking for Long-form Personalized Question Answering",
      "title_zh": "Pathways of Thoughtsï¼šé¢å‘é•¿æ–‡æœ¬ä¸ªæ€§åŒ–é—®ç­”çš„å¤šå‘æ€ç»´",
      "authors": [
        "Alireza Salemi",
        "Cheng Li",
        "Mingyang Zhang",
        "Qiaozhu Mei",
        "Zhuowan Li",
        "Spurthi Amba Hombaiah",
        "Weize Kong",
        "Tao Chen",
        "Hamed Zamani",
        "Michael Bendersky"
      ],
      "abstract": "Personalization is well studied in search and recommendation, but personalized question answering remains underexplored due to challenges in inferring preferences from long, noisy, implicit contexts and generating responses that are both accurate and aligned with user expectations. To address this, we propose Pathways of Thoughts (PoT), an inference-stage method that applies to any large language model (LLM) without task-specific fine-tuning. PoT models the thinking as an iterative decision process, where the model dynamically selects among cognitive operations such as reasoning, revision, personalization, and clarification. This enables exploration of multiple reasoning trajectories, producing diverse candidate responses that capture different perspectives. PoT then aggregates and reweights these candidates according to inferred user preferences, yielding a final personalized response that benefits from the complementary strengths of diverse reasoning paths. Experiments on the LaMP-QA benchmark show that PoT consistently outperforms competitive baselines, achieving up to a 10.8\\% relative improvement. Human evaluation further validates these improvements, with annotators preferring PoT in 66\\% of cases compared to the best-performing baseline and reporting ties in 15\\% of cases.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¸ªæ€§åŒ–é—®ç­”(Personalized Question Answering)ä¸­ä»é•¿ä¸”å˜ˆæ‚çš„éšå«ä¸Šä¸‹æ–‡ä¸­æ¨æ–­åå¥½å¹¶ç”Ÿæˆå‡†ç¡®ä¸”ç¬¦åˆé¢„æœŸå“åº”çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†Pathways of Thoughts (PoT)æ–¹æ³•ã€‚ä½œä¸ºä¸€ç§åœ¨æ¨ç†é˜¶æ®µ(Inference-stage)å®æ–½çš„æ–¹æ³•ï¼ŒPoTæ— éœ€å¯¹å¤§è¯­è¨€æ¨¡å‹(LLM)è¿›è¡Œå¾®è°ƒå³å¯åº”ç”¨äºä»»ä½•æ¨¡å‹ã€‚è¯¥æ–¹æ³•å°†æ€è€ƒè¿‡ç¨‹å»ºæ¨¡ä¸ºè¿­ä»£å†³ç­–ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†(Reasoning)ã€ä¿®æ­£(Revision)ã€ä¸ªæ€§åŒ–(Personalization)å’Œæ¾„æ¸…(Clarification)ç­‰è®¤çŸ¥æ“ä½œä¸­åŠ¨æ€é€‰æ‹©ã€‚é€šè¿‡æ¢ç´¢å¤šæ¡æ¨ç†è½¨è¿¹å¹¶ç”Ÿæˆå¤šæ ·åŒ–å€™é€‰å“åº”ï¼ŒPoTä¾æ®æ¨æ–­çš„ç”¨æˆ·åå¥½è¿›è¡Œèšåˆä¸åŠ æƒï¼Œä»è€Œå‘æŒ¥ä¸åŒæ¨ç†è·¯å¾„çš„äº’è¡¥ä¼˜åŠ¿ã€‚åœ¨LaMP-QAåŸºå‡†æµ‹è¯•ä¸­ï¼ŒPoTç›¸æ¯”ç«äº‰åŸºçº¿å®ç°äº†é«˜è¾¾10.8%çš„ç›¸å¯¹æå‡ã€‚äººå·¥è¯„ä¼°è¿›ä¸€æ­¥è¯å®äº†å…¶æœ‰æ•ˆæ€§ï¼Œåœ¨66%çš„æƒ…å†µä¸‹è¯„ä¼°è€…æ›´å€¾å‘äºPoTçš„è¾“å‡ºï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨ç”Ÿæˆé«˜åº¦å¯¹é½ç”¨æˆ·æœŸæœ›çš„å“åº”æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19094v2",
      "published_date": "2025-09-23 14:44:46 UTC",
      "updated_date": "2026-01-21 04:25:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:33:42.369924+00:00"
    },
    {
      "arxiv_id": "2509.19091v1",
      "title": "Training Flow Matching Models with Reliable Labels via Self-Purification",
      "title_zh": "åŸºäºè‡ªå‡€åŒ–çš„å¯é æ ‡ç­¾æµåŒ¹é…æ¨¡å‹è®­ç»ƒ",
      "authors": [
        "Hyeongju Kim",
        "Yechan Yu",
        "June Young Yi",
        "Juheon Lee"
      ],
      "abstract": "Training datasets are inherently imperfect, often containing mislabeled samples due to human annotation errors, limitations of tagging models, and other sources of noise. Such label contamination can significantly degrade the performance of a trained model. In this work, we introduce Self-Purifying Flow Matching (SPFM), a principled approach to filtering unreliable data within the flow-matching framework. SPFM identifies suspicious data using the model itself during the training process, bypassing the need for pretrained models or additional modules. Our experiments demonstrate that models trained with SPFM generate samples that accurately adhere to the specified conditioning, even when trained on noisy labels. Furthermore, we validate the robustness of SPFM on the TITW dataset, which consists of in-the-wild speech data, achieving performance that surpasses existing baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Self-Purifying Flow Matching (SPFM)ï¼Œè¿™æ˜¯ä¸€ç§åœ¨Flow-matchingæ¡†æ¶å†…è¿‡æ»¤ä¸å¯é æ•°æ®çš„åŸåˆ™æ€§æ–¹æ³•ã€‚é’ˆå¯¹è®­ç»ƒæ•°æ®é›†ä¸­å¸¸è§çš„Noise Labelæ±¡æŸ“é—®é¢˜ï¼ŒSPFMé€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åˆ©ç”¨æ¨¡å‹è‡ªèº«æ¥è¯†åˆ«å¯ç–‘æ•°æ®ï¼Œä»è€Œè§„é¿äº†å¯¹Pretrained Modelsæˆ–é¢å¤–æ¨¡å—çš„éœ€æ±‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨SPFMè®­ç»ƒçš„æ¨¡å‹å³ä½¿åœ¨æ ‡ç­¾ä¸å‡†ç¡®çš„æƒ…å†µä¸‹ï¼Œä¹Ÿèƒ½ç”Ÿæˆç²¾ç¡®éµå¾ªConditioningè¦æ±‚çš„æ ·æœ¬ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶åœ¨åŒ…å«çœŸå®åœºæ™¯è¯­éŸ³æ•°æ®çš„TITWæ•°æ®é›†ä¸ŠéªŒè¯äº†SPFMçš„é²æ£’æ€§ï¼Œå…¶æœ€ç»ˆæ€§èƒ½è¡¨ç°è¶…è¶Šäº†ç°æœ‰çš„Baselineæ–¹æ³•ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "5 pages, 3 figures, preprint",
      "pdf_url": "https://arxiv.org/pdf/2509.19091v1",
      "published_date": "2025-09-23 14:43:27 UTC",
      "updated_date": "2025-09-23 14:43:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:34:16.273739+00:00"
    },
    {
      "arxiv_id": "2510.05106v2",
      "title": "Rule Encoding and Compliance in Large Language Models: An Information-Theoretic Analysis",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ä¸­çš„è§„åˆ™ç¼–ç ä¸åˆè§„æ€§ï¼šåŸºäºä¿¡æ¯è®ºçš„åˆ†æ",
      "authors": [
        "Joachim Diederich"
      ],
      "abstract": "The design of safety-critical agents based on large language models (LLMs) requires more than simple prompt engineering. This paper presents a comprehensive information-theoretic analysis of how rule encodings in system prompts influence attention mechanisms and compliance behaviour. We demonstrate that rule formats with low syntactic entropy and highly concentrated anchors reduce attention entropy and improve pointer fidelity, but reveal a fundamental trade-off between anchor redundancy and attention entropy that previous work failed to recognize. Through formal analysis of multiple attention architectures including causal, bidirectional, local sparse, kernelized, and cross-attention mechanisms, we establish bounds on pointer fidelity and show how anchor placement strategies must account for competing fidelity and entropy objectives. Combining these insights with a dynamic rule verification architecture, we provide a formal proof that hot reloading of verified rule sets increases the asymptotic probability of compliant outputs. These findings underscore the necessity of principled anchor design and dual enforcement mechanisms to protect LLM-based agents against prompt injection attacks while maintaining compliance in evolving domains.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡ä¿¡æ¯è®º (Information-Theoretic) åˆ†æäº†å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) ä¸­ç³»ç»Ÿæç¤ºè¯çš„è§„åˆ™ç¼–ç å¦‚ä½•å½±å“æ³¨æ„åŠ›æœºåˆ¶ (Attention Mechanisms) ä¸åˆè§„è¡Œä¸ºã€‚ç ”ç©¶è¡¨æ˜ï¼Œå…·æœ‰ä½è¯­æ³•ç†µ (Syntactic Entropy) å’Œé«˜åº¦é›†ä¸­é”šç‚¹ (Anchors) çš„è§„åˆ™æ ¼å¼èƒ½æœ‰æ•ˆé™ä½æ³¨æ„åŠ›ç†µ (Attention Entropy) å¹¶æé«˜æŒ‡é’ˆå¿ å®åº¦ (Pointer Fidelity)ï¼Œä½†é”šç‚¹å†—ä½™ (Anchor Redundancy) ä¸æ³¨æ„åŠ›ç†µä¹‹é—´å­˜åœ¨ç€åŸºç¡€æƒè¡¡ã€‚é€šè¿‡å¯¹å› æœã€åŒå‘ã€å±€éƒ¨ç¨€ç–ã€æ ¸åŒ–åŠäº¤å‰æ³¨æ„åŠ›ç­‰å¤šç§æ¶æ„çš„æ­£å¼åˆ†æï¼Œç ”ç©¶ç¡®ç«‹äº†æŒ‡é’ˆå¿ å®åº¦çš„ç•Œé™ï¼Œå¹¶æŒ‡å‡ºäº†é”šç‚¹æ”¾ç½®ç­–ç•¥å¿…é¡»å…¼é¡¾å¿ å®åº¦ä¸ç†µçš„ç›®æ ‡ã€‚ç»“åˆåŠ¨æ€è§„åˆ™éªŒè¯æ¶æ„ï¼Œè®ºæ–‡ä»ç†è®ºä¸Šè¯æ˜äº†ç»éªŒè¯è§„åˆ™é›†çš„é‡è½½ (Hot Reloading) èƒ½å¤Ÿæ˜¾è‘—æå‡è¾“å‡ºåˆè§„æ€§çš„æ¸è¿‘æ¦‚ç‡ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†åŸåˆ™æ€§é”šç‚¹è®¾è®¡ä¸åŒé‡æ‰§è¡Œæœºåˆ¶åœ¨ä¿æŠ¤ LLM æ™ºèƒ½ä½“å…å—æç¤ºè¯æ³¨å…¥æ”»å‡» (Prompt Injection Attacks) ä»¥åŠåœ¨åŠ¨æ€é¢†åŸŸç»´æŒåˆè§„æ€§æ–¹é¢çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.05106v2",
      "published_date": "2025-09-23 14:42:32 UTC",
      "updated_date": "2025-10-09 09:08:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:34:15.364930+00:00"
    },
    {
      "arxiv_id": "2509.19090v2",
      "title": "Citrus-V: Advancing Medical Foundation Models with Unified Medical Image Grounding for Clinical Reasoning",
      "title_zh": "Citrus-Vï¼šé€šè¿‡é¢å‘ä¸´åºŠæ¨ç†çš„ç»Ÿä¸€åŒ»ç–—å›¾åƒå®šä½æ¨åŠ¨åŒ»ç–—åŸºç¡€æ¨¡å‹çš„å‘å±•",
      "authors": [
        "Guoxin Wang",
        "Jun Zhao",
        "Xinyi Liu",
        "Yanbo Liu",
        "Xuyang Cao",
        "Chao Li",
        "Zhuoyun Liu",
        "Qintian Sun",
        "Fangru Zhou",
        "Haoqiang Xing",
        "Zhenhong Yang"
      ],
      "abstract": "Medical imaging provides critical evidence for clinical diagnosis, treatment planning, and surgical decisions, yet most existing imaging models are narrowly focused and require multiple specialized networks, limiting their generalization. Although large-scale language and multimodal models exhibit strong reasoning and multi-task capabilities, real-world clinical applications demand precise visual grounding, multimodal integration, and chain-of-thought reasoning. We introduce Citrus-V, a multimodal medical foundation model that combines image analysis with textual reasoning. The model integrates detection, segmentation, and multimodal chain-of-thought reasoning, enabling pixel-level lesion localization, structured report generation, and physician-like diagnostic inference in a single framework. We propose a novel multimodal training approach and release a curated open-source data suite covering reasoning, detection, segmentation, and document understanding tasks. Evaluations demonstrate that Citrus-V outperforms existing open-source medical models and expert-level imaging systems across multiple benchmarks, delivering a unified pipeline from visual grounding to clinical reasoning and supporting precise lesion quantification, automated reporting, and reliable second opinions.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† Citrus-Vï¼Œä¸€ç§å°†å›¾åƒåˆ†æä¸æ–‡æœ¬æ¨ç†ç›¸ç»“åˆçš„å¤šæ¨¡æ€åŒ»å­¦åŸºç¡€æ¨¡å‹ (multimodal medical foundation model)ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å½±åƒæ¨¡å‹åŠŸèƒ½å±€é™ä¸”æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶æ·±åº¦é›†æˆäº†ç›®æ ‡æ£€æµ‹ (detection)ã€å›¾åƒåˆ†å‰² (segmentation) ä»¥åŠå¤šæ¨¡æ€é“¾å¼æ€ç»´æ¨ç† (multimodal Chain-of-Thought reasoning)ï¼Œåœ¨å•ä¸€ç³»ç»Ÿä¸­å®ç°äº†åƒç´ çº§çš„ç—…ç¶å®šä½ã€ç»“æ„åŒ–æŠ¥å‘Šç”Ÿæˆä»¥åŠç±»äººè¯Šæ–­æ¨ç†ã€‚ç ”ç©¶å›¢é˜Ÿä¸ºæ­¤æå‡ºäº†ä¸€ç§åˆ›æ–°çš„å¤šæ¨¡æ€è®­ç»ƒæ–¹æ³•ï¼Œå¹¶åŒæ­¥å‘å¸ƒäº†æ¶µç›–æ¨ç†ã€æ£€æµ‹ã€åˆ†å‰²å’Œæ–‡æ¡£ç†è§£ç­‰ä»»åŠ¡çš„å¼€æºæ•°æ®å¥—ä»¶ã€‚å®éªŒè¯„ä¼°è¯æ˜ï¼ŒCitrus-V åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°å‡è¶…è¶Šäº†ç°æœ‰çš„å¼€æºåŒ»å­¦æ¨¡å‹å’Œä¸“å®¶çº§æˆåƒç³»ç»Ÿã€‚è¯¥æ¨¡å‹æˆåŠŸæ„å»ºäº†ä»è§†è§‰å®šä½ (visual grounding) åˆ°ä¸´åºŠæ¨ç†çš„ç»Ÿä¸€æµæ°´çº¿ (unified pipeline)ï¼Œä¸ºç—…ç¶é‡åŒ–å’Œä¸´åºŠå†³ç­–æä¾›äº†ç²¾å‡†ä¸”å¯é çš„ç³»ç»Ÿæ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19090v2",
      "published_date": "2025-09-23 14:42:31 UTC",
      "updated_date": "2025-09-24 08:19:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:34:22.758055+00:00"
    },
    {
      "arxiv_id": "2509.19088v4",
      "title": "Digital Twins as Funhouse Mirrors: Five Key Distortions",
      "title_zh": "æ•°å­—å­ªç”ŸçŠ¹å¦‚å“ˆå“ˆé•œï¼šäº”ç§å…³é”®å¤±çœŸ",
      "authors": [
        "Tianyi Peng",
        "George Gui",
        "Melanie Brucks",
        "Daniel J. Merlau",
        "Grace Jiarui Fan",
        "Malek Ben Sliman",
        "Eric J. Johnson",
        "Abdullah Althenayyan",
        "Silvia Bellezza",
        "Dante Donati",
        "Hortense Fong",
        "Elizabeth Friedman",
        "Ariana Guevara",
        "Mohamed Hussein",
        "Kinshuk Jerath",
        "Bruce Kogut",
        "Akshit Kumar",
        "Kristen Lane",
        "Hannah Li",
        "Vicki Morwitz",
        "Oded Netzer",
        "Patryk Perkowski",
        "Olivier Toubia"
      ],
      "abstract": "Scientists and practitioners are aggressively moving to deploy digital twins - LLM-based models of real individuals - across social science and policy research. We conducted 19 pre-registered studies with 164 diverse outcomes (e.g., attitudes towards hiring algorithms, intention to share misinformation) and compared human responses to those of their digital twins (trained on each person's previous answers to over 500 questions). We find that digital twins' answers are only modestly more accurate than those from the homogeneous base LLM and correlate weakly with human responses (average r = 0.20). We document five ways in which digital twins distort human behavior: (i) stereotyping, (ii) insufficient individuation, (iii) representation bias, (iv) ideological biases, and (v) hyper-rationality. Together, our results caution against the premature deployment of digital twins, which may systematically misrepresent human cognition and undermine both scientific understanding and practical applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹(LLMs)æ„å»ºçš„ä¸ªäººDigital Twinsåœ¨ç¤¾ä¼šç§‘å­¦å’Œæ”¿ç­–ç ”ç©¶ä¸­çš„å¯é æ€§ï¼Œé€šè¿‡19é¡¹é¢„æ³¨å†Œç ”ç©¶å¯¹æ¯”äº†164ç§ç»“æœä¸‹çœŸå®äººç±»ä¸å…¶Digital Twinsçš„ååº”å·®å¼‚ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡Digital Twinsç»è¿‡äº†ä¸ªä½“åŒ–æ•°æ®è®­ç»ƒï¼Œå…¶é¢„æµ‹å‡†ç¡®ç‡ä»…ç•¥é«˜äºåŸºç¡€LLMï¼Œä¸”ä¸äººç±»çœŸå®è¡Œä¸ºçš„ç›¸å…³æ€§æå¼±ï¼ˆå¹³å‡r=0.20ï¼‰ã€‚ç ”ç©¶è¯†åˆ«å‡ºDigital Twinsæ­ªæ›²äººç±»è¡Œä¸ºçš„äº”ç§å…³é”®æ–¹å¼ï¼šStereotypingï¼ˆåˆ»æ¿å°è±¡ï¼‰ã€Insufficient individuationï¼ˆä¸ªä½“åŒ–ä¸è¶³ï¼‰ã€Representation biasï¼ˆä»£è¡¨æ€§åå·®ï¼‰ã€Ideological biasesï¼ˆæ„è¯†å½¢æ€åè§ï¼‰ä»¥åŠHyper-rationalityï¼ˆè¿‡åº¦ç†æ€§ï¼‰ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œå½“å‰çš„Digital TwinsæŠ€æœ¯ä¼šç³»ç»Ÿæ€§åœ°è¯¯å¯¼å¯¹äººç±»è®¤çŸ¥çš„ç†è§£ï¼Œå¯èƒ½å¯¹ç§‘å­¦ç ”ç©¶å’Œå®é™…åº”ç”¨äº§ç”Ÿè´Ÿé¢å½±å“ã€‚å› æ­¤ï¼Œä½œè€…è­¦å‘Šä¸è¦è¿‡æ—©éƒ¨ç½²æ­¤ç±»æŠ€æœ¯ï¼Œå¼ºè°ƒäº†åœ¨æ¨¡æ‹Ÿäººç±»è¡Œä¸ºæ—¶å¿…é¡»å®¡æ…å¯¹å¾…æ¨¡å‹å­˜åœ¨çš„ç³»ç»Ÿæ€§åå·®ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC",
        "stat.AP"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19088v4",
      "published_date": "2025-09-23 14:42:14 UTC",
      "updated_date": "2026-01-04 04:45:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:34:20.962064+00:00"
    },
    {
      "arxiv_id": "2509.19084v1",
      "title": "Graph Neural Networks with Similarity-Navigated Probabilistic Feature Copying",
      "title_zh": "åŸºäºç›¸ä¼¼æ€§å¼•å¯¼æ¦‚ç‡ç‰¹å¾å¤åˆ¶çš„å›¾ç¥ç»ç½‘ç»œ",
      "authors": [
        "Asela Hevapathige"
      ],
      "abstract": "Graph Neural Networks (GNNs) have demonstrated remarkable success across various graph-based tasks. However, they face some fundamental limitations: feature oversmoothing can cause node representations to become indistinguishable in deeper networks, they struggle to effectively manage heterogeneous relationships where connected nodes differ significantly, and they process entire feature vectors as indivisible units, which limits flexibility. We seek to address these limitations. We propose AxelGNN, a novel GNN architecture inspired by Axelrod's cultural dissemination model that addresses these limitations through a unified framework. AxelGNN incorporates similarity-gated probabilistic interactions that adaptively promote convergence or divergence based on node similarity, implements trait-level copying mechanisms for fine-grained feature aggregation at the segment level, and maintains global polarization to preserve node distinctiveness across multiple representation clusters. The model's bistable convergence dynamics naturally handle both homophilic and heterophilic graphs within a single architecture. Extensive experiments on node classification and influence estimation benchmarks demonstrate that AxelGNN consistently outperforms or matches state-of-the-art GNN methods across diverse graph structures with varying homophily-heterophily characteristics.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å›¾ç¥ç»ç½‘ç»œ(GNNs)ä¸­å­˜åœ¨çš„ç‰¹å¾è¿‡åº¦å¹³æ»‘(feature oversmoothing)ã€éš¾ä»¥æœ‰æ•ˆå¤„ç†å¼‚è´¨å…³ç³»ä»¥åŠå°†ç‰¹å¾å‘é‡è§†ä¸ºä¸å¯åˆ†å‰²å•å…ƒç­‰å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§å— Axelrod æ–‡åŒ–ä¼ æ’­æ¨¡å‹å¯å‘çš„æ–°å‹æ¶æ„ AxelGNNã€‚è¯¥æ¨¡å‹å¼•å…¥äº†åŸºäºç›¸ä¼¼åº¦é—¨æ§çš„æ¦‚ç‡äº¤äº’æœºåˆ¶(similarity-gated probabilistic interactions)ï¼Œèƒ½å¤Ÿæ ¹æ®èŠ‚ç‚¹ç›¸ä¼¼åº¦è‡ªé€‚åº”åœ°ä¿ƒè¿›ç‰¹å¾æ”¶æ•›æˆ–å‘æ•£ã€‚é€šè¿‡å®ç°æ€§çŠ¶çº§æ‹·è´æœºåˆ¶(trait-level copying mechanisms)ï¼ŒAxelGNN æ”¯æŒåœ¨ç‰‡æ®µçº§åˆ«è¿›è¡Œç»†ç²’åº¦çš„ç‰¹å¾èšåˆï¼Œå¹¶åˆ©ç”¨å…¨å±€æåŒ–æ¥ä¿æŒèŠ‚ç‚¹çš„ç‹¬ç‰¹æ€§ã€‚å¾—ç›ŠäºåŒç¨³æ€æ”¶æ•›åŠ¨åŠ›å­¦(bistable convergence dynamics)ï¼Œè¯¥æ¶æ„èƒ½è‡ªç„¶åœ°åŒæ—¶å¤„ç†åŒè´¨å’Œå¼‚è´¨å›¾ç»“æ„ã€‚åœ¨èŠ‚ç‚¹åˆ†ç±»(node classification)å’Œå½±å“åŠ›ä¼°è®¡(influence estimation)åŸºå‡†æµ‹è¯•ä¸­çš„å®éªŒè¯æ˜ï¼ŒAxelGNN åœ¨å¤šç§å›¾ç»“æ„ä¸Šçš„è¡¨ç°å‡ä¼˜äºæˆ–è¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›æ–¹æ³•çš„æ°´å¹³ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19084v1",
      "published_date": "2025-09-23 14:39:09 UTC",
      "updated_date": "2025-09-23 14:39:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:34:20.357807+00:00"
    },
    {
      "arxiv_id": "2509.19080v1",
      "title": "World4RL: Diffusion World Models for Policy Refinement with Reinforcement Learning for Robotic Manipulation",
      "title_zh": "World4RLï¼šç»“åˆå¼ºåŒ–å­¦ä¹ å®ç°æœºå™¨äººæ“ä½œç­–ç•¥ä¼˜åŒ–çš„æ‰©æ•£ä¸–ç•Œæ¨¡å‹",
      "authors": [
        "Zhennan Jiang",
        "Kai Liu",
        "Yuxin Qin",
        "Shuai Tian",
        "Yupeng Zheng",
        "Mingcai Zhou",
        "Chao Yu",
        "Haoran Li",
        "Dongbin Zhao"
      ],
      "abstract": "Robotic manipulation policies are commonly initialized through imitation learning, but their performance is limited by the scarcity and narrow coverage of expert data. Reinforcement learning can refine polices to alleviate this limitation, yet real-robot training is costly and unsafe, while training in simulators suffers from the sim-to-real gap. Recent advances in generative models have demonstrated remarkable capabilities in real-world simulation, with diffusion models in particular excelling at generation. This raises the question of how diffusion model-based world models can be combined to enhance pre-trained policies in robotic manipulation. In this work, we propose World4RL, a framework that employs diffusion-based world models as high-fidelity simulators to refine pre-trained policies entirely in imagined environments for robotic manipulation. Unlike prior works that primarily employ world models for planning, our framework enables direct end-to-end policy optimization. World4RL is designed around two principles: pre-training a diffusion world model that captures diverse dynamics on multi-task datasets and refining policies entirely within a frozen world model to avoid online real-world interactions. We further design a two-hot action encoding scheme tailored for robotic manipulation and adopt diffusion backbones to improve modeling fidelity. Extensive simulation and real-world experiments demonstrate that World4RL provides high-fidelity environment modeling and enables consistent policy refinement, yielding significantly higher success rates compared to imitation learning and other baselines. More visualization results are available at https://world4rl.github.io/.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† World4RLï¼Œä¸€ç§åˆ©ç”¨ diffusion-based world models ä½œä¸ºé«˜ä¿çœŸæ¨¡æ‹Ÿå™¨ï¼Œé€šè¿‡ Reinforcement Learning å¯¹æœºå™¨äººæ“ä½œç­–ç•¥è¿›è¡Œä¼˜åŒ–çš„æ¡†æ¶ã€‚ä¸ºäº†è§£å†³ imitation learning å—é™äºæ•°æ®ç¨€ç¼ºä»¥åŠçœŸå®æœºå™¨äººè®­ç»ƒä¸å®‰å…¨çš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶å…è®¸åœ¨å®Œå…¨ç”±æ¨¡å‹ç”Ÿæˆçš„æƒ³è±¡ç¯å¢ƒä¸­è¿›è¡Œç«¯åˆ°ç«¯çš„ç­–ç•¥æ”¹è¿›ã€‚World4RL çš„æ ¸å¿ƒè®¾è®¡åŒ…æ‹¬åœ¨å¤šä»»åŠ¡æ•°æ®é›†ä¸Šé¢„è®­ç»ƒæ‰©æ•£ä¸–ç•Œæ¨¡å‹ä»¥æ•æ‰å¤æ‚çš„åŠ¨åŠ›å­¦ï¼Œå¹¶åœ¨å†»ç»“çš„ä¸–ç•Œæ¨¡å‹ä¸­ä¼˜åŒ–ç­–ç•¥ä»¥é¿å…åœ¨çº¿çœŸå®äº¤äº’ã€‚ç ”ç©¶è¿˜å¼•å…¥äº†ä¸“é—¨é’ˆå¯¹æœºå™¨äººæ“ä½œè®¾è®¡çš„ two-hot action encoding æ–¹æ¡ˆï¼Œå¹¶é‡‡ç”¨ diffusion backbones æ¥æå‡ç¯å¢ƒå»ºæ¨¡çš„ä¿çœŸåº¦ã€‚å¹¿æ³›çš„ä»¿çœŸå’ŒçœŸå®ä¸–ç•Œå®éªŒè¯æ˜ï¼ŒWorld4RL èƒ½å¤Ÿæä¾›é«˜ä¿çœŸçš„ç¯å¢ƒæ¨¡æ‹Ÿå¹¶å®ç°ä¸€è‡´çš„ç­–ç•¥æå‡ï¼Œå…¶ä»»åŠ¡æˆåŠŸç‡æ˜¾è‘—è¶…è¿‡äº†æ¨¡ä»¿å­¦ä¹ å’Œå…¶ä»–åŸºå‡†æ¨¡å‹ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19080v1",
      "published_date": "2025-09-23 14:38:15 UTC",
      "updated_date": "2025-09-23 14:38:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:34:31.288656+00:00"
    },
    {
      "arxiv_id": "2509.19077v1",
      "title": "Code Driven Planning with Domain-Adaptive Critic",
      "title_zh": "ç»“åˆé¢†åŸŸè‡ªé€‚åº”è¯„åˆ¤å™¨çš„ä»£ç é©±åŠ¨è§„åˆ’",
      "authors": [
        "Zikang Tian",
        "Shaohui Peng",
        "Du Huang",
        "Jiaming Guo",
        "Ruizhi Chen",
        "Rui Zhang",
        "Xishan Zhang",
        "Yuxuan Guo",
        "Zidong Du",
        "Qi Guo",
        "Ling Li",
        "Yewen Pu",
        "Xing Hu",
        "Yunji Chen"
      ],
      "abstract": "Large Language Models (LLMs) have been widely adopted as task planners for AI agents in sequential decision-making problems, leveraging their extensive world knowledge. However, the gap between their general knowledge and environment-specific requirements often leads to inaccurate plans. To address this, existing approaches rely on frequent LLM queries to iteratively refine plans based on immediate environmental feedback, which incurs substantial query costs. However, this refinement is typically guided by short-term environmental feedback, limiting LLMs from developing plans aligned with long-term rewards. We propose Code Driven Planning with Domain-Adaptive Critic (CoPiC). Instead of relying on frequent queries, CoPiC employs LLMs to generate a diverse set of high-level planning programs, which iteratively produce and refine candidate plans. A trained domain-adaptive critic then evaluates these candidates and selects the one most aligned with long-term rewards for execution. Using high-level planning programs as planner and domain-adaptive critic as estimator, CoPiC improves planning while significantly reducing query costs. Results in ALFWorld, NetHack, and StarCraft II Unit Building show that CoPiC outperforms advanced LLM-based baselines, AdaPlanner and Reflexion, achieving an average (1) 23.33% improvement in success rate and (2) 91.27% reduction in query costs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨åºåˆ—å†³ç­–ä»»åŠ¡ä¸­å› é€šç”¨çŸ¥è¯†ä¸ç‰¹å®šç¯å¢ƒå·®å¼‚å¯¼è‡´è§„åˆ’ä¸å‡†ï¼Œä»¥åŠç°æœ‰è¿­ä»£ä¼˜åŒ–æ–¹æ³•æŸ¥è¯¢æˆæœ¬è¿‡é«˜ä¸”ç¼ºä¹é•¿æœŸå¥–åŠ±å¯¼å‘çš„é—®é¢˜ï¼Œæå‡ºäº† Code Driven Planning with Domain-Adaptive Critic (CoPiC) æ¡†æ¶ã€‚CoPiC å¹¶ä¸ä¾èµ–é¢‘ç¹çš„åœ¨çº¿æŸ¥è¯¢ï¼Œè€Œæ˜¯åˆ©ç”¨ LLMs ç”Ÿæˆå¤šæ ·çš„æŠ½è±¡å±‚çº§è§„åˆ’ç¨‹åº (high-level planning programs) æ¥è¿­ä»£äº§ç”Ÿå’Œä¼˜åŒ–å€™é€‰è§„åˆ’ã€‚éšåï¼Œè¯¥æ¡†æ¶é€šè¿‡ä¸€ä¸ªç»è¿‡è®­ç»ƒçš„é¢†åŸŸè‡ªé€‚åº”è¯„è®ºå®¶ (domain-adaptive critic) å¯¹è¿™äº›å€™é€‰æ–¹æ¡ˆè¿›è¡Œè¯„ä¼°ï¼Œä»ä¸­ç­›é€‰å‡ºæœ€ç¬¦åˆé•¿æœŸå¥–åŠ±çš„åºåˆ—æ‰§è¡Œã€‚è¿™ç§å°†è§„åˆ’ç¨‹åºä½œä¸ºè§„åˆ’å™¨ã€è‡ªé€‚åº”è¯„è®ºå®¶ä½œä¸ºè¯„ä¼°å™¨çš„æœºåˆ¶ï¼Œåœ¨æå‡è§„åˆ’è´¨é‡çš„åŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—å¼€é”€ã€‚å®éªŒåœ¨ ALFWorldã€NetHack å’Œ StarCraft II Unit Building ç­‰ä»»åŠ¡ä¸­è¯æ˜ï¼ŒCoPiC ç›¸æ¯” AdaPlanner å’Œ Reflexion ç­‰å…ˆè¿›åŸºçº¿æ¨¡å‹ï¼Œå¹³å‡æˆåŠŸç‡æå‡äº† 23.33%ï¼Œä¸”æŸ¥è¯¢æˆæœ¬å¤§å¹…é™ä½äº† 91.27%ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19077v1",
      "published_date": "2025-09-23 14:36:12 UTC",
      "updated_date": "2025-09-23 14:36:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:34:33.779956+00:00"
    },
    {
      "arxiv_id": "2509.19063v1",
      "title": "Beyond Backpropagation: Exploring Innovative Algorithms for Energy-Efficient Deep Neural Network Training",
      "title_zh": "è¶…è¶Šåå‘ä¼ æ’­ï¼šé«˜èƒ½æ•ˆæ·±åº¦ç¥ç»ç½‘ç»œè®­ç»ƒåˆ›æ–°ç®—æ³•æ¢ç´¢",
      "authors": [
        "PrzemysÅ‚aw Spyra"
      ],
      "abstract": "The rising computational and energy demands of deep neural networks (DNNs), driven largely by backpropagation (BP), challenge sustainable AI development. This paper rigorously investigates three BP-free training methods: the Forward-Forward (FF), Cascaded-Forward (CaFo), and Mono-Forward (MF) algorithms, tracing their progression from foundational concepts to a demonstrably superior solution.\n  A robust comparative framework was established: each algorithm was implemented on its native architecture (MLPs for FF and MF, a CNN for CaFo) and benchmarked against an equivalent BP-trained model. Hyperparameters were optimized with Optuna, and consistent early stopping criteria were applied based on validation performance, ensuring all models were optimally tuned before comparison.\n  Results show that MF not only competes with but consistently surpasses BP in classification accuracy on its native MLPs. Its superior generalization stems from converging to a more favorable minimum in the validation loss landscape, challenging the assumption that global optimization is required for state-of-the-art results. Measured at the hardware level using the NVIDIA Management Library (NVML) API, MF reduces energy consumption by up to 41% and shortens training time by up to 34%, translating to a measurably smaller carbon footprint as estimated by CodeCarbon.\n  Beyond this primary result, we present a hardware-level analysis that explains the efficiency gains: exposing FF's architectural inefficiencies, validating MF's computationally lean design, and challenging the assumption that all BP-free methods are inherently more memory-efficient. By documenting the evolution from FF's conceptual groundwork to MF's synthesis of accuracy and sustainability, this work offers a clear, data-driven roadmap for future energy-efficient deep learning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦ç¥ç»ç½‘ç»œ(DNNs)ä¸­åå‘ä¼ æ’­(Backpropagation)å¸¦æ¥çš„é«˜èƒ½è€—æŒ‘æˆ˜ï¼Œæ·±å…¥æ¢ç´¢äº†Forward-Forward (FF)ã€Cascaded-Forward (CaFo)å’ŒMono-Forward (MF)ä¸‰ç§æ— éœ€åå‘ä¼ æ’­çš„è®­ç»ƒç®—æ³•ã€‚ç ”ç©¶å»ºç«‹äº†ä¸€ä¸ªç¨³å¥çš„æ¯”è¾ƒæ¡†æ¶ï¼Œé€šè¿‡Optunaä¼˜åŒ–è¶…å‚æ•°ï¼Œå¹¶åˆ©ç”¨NVML APIè¿›è¡Œç¡¬ä»¶çº§èƒ½è€—è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMFç®—æ³•åœ¨å¤šå±‚æ„ŸçŸ¥æœº(MLPs)ä¸Šçš„åˆ†ç±»å‡†ç¡®ç‡ä¸ä»…èƒ½ä¸BPåª²ç¾ï¼Œä¸”å§‹ç»ˆè¡¨ç°æ›´ä¼˜ï¼Œè¿™ä¸»è¦å½’åŠŸäºå…¶èƒ½æ”¶æ•›è‡³éªŒè¯æŸå¤±æ™¯è§‚ä¸­æ›´å¥½çš„æå°å€¼ã€‚åœ¨èƒ½è€—æ–¹é¢ï¼ŒMFç®—æ³•æœ€å¤šå¯å‡å°‘41%çš„èƒ½é‡æ¶ˆè€—ï¼Œå¹¶å°†è®­ç»ƒæ—¶é—´ç¼©çŸ­è‡³å¤š34%ï¼Œæ˜¾è‘—é™ä½äº†ç¢³è¶³è¿¹ã€‚æ­¤å¤–ï¼Œç¡¬ä»¶çº§åˆ†ææ­ç¤ºäº†MFè®¡ç®—ç²¾ç®€çš„è®¾è®¡ä¼˜åŠ¿ï¼Œå¹¶æŒ‘æˆ˜äº†å…³äºBP-freeæ–¹æ³•å†…å­˜æ•ˆç‡çš„ä¼ ç»Ÿå‡è®¾ã€‚è¯¥å·¥ä½œé€šè¿‡ä»ç†è®ºåŸºç¡€åˆ°ä¼˜é€‰æ–¹æ¡ˆçš„æ¼”è¿›åˆ†æï¼Œä¸ºæœªæ¥å¼€å‘é«˜èƒ½æ•ˆä¸”å¯æŒç»­çš„æ·±åº¦å­¦ä¹ æŠ€æœ¯æä¾›äº†æ˜ç¡®çš„æ•°æ®é©±åŠ¨è·¯çº¿å›¾ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19063v1",
      "published_date": "2025-09-23 14:27:44 UTC",
      "updated_date": "2025-09-23 14:27:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:34:35.983039+00:00"
    },
    {
      "arxiv_id": "2509.19058v1",
      "title": "Towards Causal Representation Learning with Observable Sources as Auxiliaries",
      "title_zh": "è¿ˆå‘ä»¥å¯è§‚æµ‹æºä¸ºè¾…åŠ©ä¿¡æ¯çš„å› æœè¡¨ç¤ºå­¦ä¹ ",
      "authors": [
        "Kwonho Kim",
        "Heejeong Nam",
        "Inwoo Hwang",
        "Sanghack Lee"
      ],
      "abstract": "Causal representation learning seeks to recover latent factors that generate observational data through a mixing function. Needing assumptions on latent structures or relationships to achieve identifiability in general, prior works often build upon conditional independence given known auxiliary variables. However, prior frameworks limit the scope of auxiliary variables to be external to the mixing function. Yet, in some cases, system-driving latent factors can be easily observed or extracted from data, possibly facilitating identification. In this paper, we introduce a framework of observable sources being auxiliaries, serving as effective conditioning variables. Our main results show that one can identify entire latent variables up to subspace-wise transformations and permutations using volume-preserving encoders. Moreover, when multiple known auxiliary variables are available, we offer a variable-selection scheme to choose those that maximize recoverability of the latent factors given knowledge of the latent causal graph. Finally, we demonstrate the effectiveness of our framework through experiments on synthetic graph and image data, thereby extending the boundaries of current approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å› æœè¡¨ç¤ºå­¦ä¹  (Causal Representation Learning) ä¸­é€šè¿‡ Mixing Function æ¢å¤æ½œåœ¨å› å­çš„éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§å°†å¯è§‚æµ‹æ¥æº (Observable Sources) ä½œä¸ºè¾…åŠ©å˜é‡ (Auxiliary Variables) çš„æ–°æ¡†æ¶ã€‚è¯¥æ–¹æ³•çªç ´äº†ä»¥å¾€æ¡†æ¶ä¸­è¾…åŠ©å˜é‡å¿…é¡»ä½äºæ··åˆå‡½æ•°å¤–éƒ¨çš„é™åˆ¶ï¼Œåˆ©ç”¨å¯è§‚æµ‹ä¿¡æ¯ä½œä¸ºæœ‰æ•ˆçš„æ¡ä»¶å˜é‡æ¥è¾…åŠ©è¯†åˆ«ã€‚æ ¸å¿ƒç†è®ºè¯æ˜ï¼Œåœ¨ä½¿ç”¨ä¿ä½“ç§¯ç¼–ç å™¨ (Volume-preserving Encoders) çš„å‰æä¸‹ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿå°†æ•´ä¸ªæ½œåœ¨å˜é‡è¯†åˆ«è‡³å­ç©ºé—´å˜æ¢å’Œç½®æ¢çš„ç¨‹åº¦ã€‚é’ˆå¯¹å­˜åœ¨å¤šä¸ªè¾…åŠ©å˜é‡çš„å¤æ‚åœºæ™¯ï¼Œè®ºæ–‡è¿˜æå‡ºäº†ä¸€ç§å˜é‡é€‰æ‹©æ–¹æ¡ˆï¼Œé€šè¿‡ç»“åˆæ½œåœ¨å› æœå›¾ (Latent Causal Graph) çŸ¥è¯†æ¥ä¼˜åŒ–æ½œåœ¨å› å­çš„æ¢å¤æ•ˆç‡ã€‚æœ€åï¼Œé€šè¿‡åœ¨åˆæˆå›¾æ•°æ®å’Œå›¾åƒæ•°æ®ä¸Šçš„å®éªŒéªŒè¯ï¼Œè¯¥ç ”ç©¶è¯æ˜äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§å¹¶æ‰©å±•äº†ç°æœ‰è¯†åˆ«æ€§ç†è®ºçš„é€‚ç”¨èŒƒå›´ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19058v1",
      "published_date": "2025-09-23 14:22:39 UTC",
      "updated_date": "2025-09-23 14:22:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:34:41.970612+00:00"
    },
    {
      "arxiv_id": "2509.19030v1",
      "title": "Landmarks, Monuments, and Beacons: Understanding Generative Calls to Action",
      "title_zh": "åœ°æ ‡ã€çºªå¿µç¢‘ä¸ä¿¡æ ‡ï¼šè§£æç”Ÿæˆå¼è¡ŒåŠ¨å·å¬",
      "authors": [
        "Victoire HervÃ©",
        "Henrik Warpefelt",
        "Christoph Salge"
      ],
      "abstract": "Algorithmic evaluation of procedurally generated content struggles to find metrics that align with human experience, particularly for composite artefacts. Automatic decomposition as a possible solution requires concepts that meet a range of properties. To this end, drawing on Games Studies and Game AI research, we introduce the nested concepts of \\textit{Landmarks}, \\textit{Monuments}, and \\textit{Beacons}. These concepts are based on the artefact's perceivability, evocativeness, and Call to Action, all from a player-centric perspective. These terms are generic to games and usable across genres. We argue that these entities can be found and evaluated with techniques currently used in both research and industry, opening a path towards a fully automated decomposition of PCG, and evaluation of the salient sub-components. Although the work presented here emphasises mixed-initiative PCG and compositional PCG, we believe it applies beyond those domains. With this approach, we intend to create a connection between humanities and technical game research and allow for better computational PCG evaluation",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¨‹åºç”Ÿæˆå†…å®¹ (PCG) ç®—æ³•è¯„ä¼°éš¾ä»¥ä¸äººç±»ä½“éªŒå¯¹é½çš„é—®é¢˜ï¼Œå€Ÿé‰´æ¸¸æˆç ”ç©¶ (Games Studies) å’Œæ¸¸æˆäººå·¥æ™ºèƒ½ (Game AI) é¢†åŸŸæå‡ºäº† Landmarksã€Monuments å’Œ Beacons ä¸‰ä¸ªåµŒå¥—æ¦‚å¿µã€‚è¿™äº›æ¦‚å¿µåŸºäºç©å®¶ä¸­å¿ƒè§†è§’ä¸‹çš„å¯æ„ŸçŸ¥æ€§ (perceivability)ã€è¯±å‘æ€§ (evocativeness) å’Œè¡ŒåŠ¨å·å¬ (Call to Action)ï¼Œå…·æœ‰è·¨æ¸¸æˆç±»å‹çš„é€šç”¨æ€§ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œåˆ©ç”¨ç°æœ‰çš„å­¦æœ¯å’Œå·¥ä¸šæŠ€æœ¯å¯ä»¥è¯†åˆ«å¹¶è¯„ä¼°è¿™äº›å®ä½“ï¼Œä»è€Œä¸º PCG çš„å…¨è‡ªåŠ¨åˆ†è§£å’Œå…³é”®å­ç»„ä»¶è¯„ä¼°å¼€è¾Ÿäº†é“è·¯ã€‚å°½ç®¡è¯¥å·¥ä½œä¾§é‡äºæ··åˆä¸»åŠ¨å¼ (mixed-initiative) å’Œç»„åˆå¼ (compositional) PCGï¼Œå…¶ç†è®ºæ¡†æ¶äº¦å¯æ‰©å±•è‡³å…¶ä»–é¢†åŸŸã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œç ”ç©¶æ—¨åœ¨å»ºç«‹äººæ–‡ç§‘å­¦ä¸æŠ€æœ¯æ¸¸æˆç ”ç©¶ä¹‹é—´çš„è”ç³»ï¼Œæœ€ç»ˆå®ç°æ›´ä¼˜çš„ PCG è®¡ç®—è¯„ä¼°ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19030v1",
      "published_date": "2025-09-23 14:03:54 UTC",
      "updated_date": "2025-09-23 14:03:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:34:42.470876+00:00"
    },
    {
      "arxiv_id": "2509.19023v1",
      "title": "Reduced-Order Model-Guided Reinforcement Learning for Demonstration-Free Humanoid Locomotion",
      "title_zh": "é™é˜¶æ¨¡å‹å¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ ï¼šé¢å‘æ— éœ€æ¼”ç¤ºçš„äººå½¢æœºå™¨äººè¿åŠ¨",
      "authors": [
        "Shuai Liu",
        "Meng Cheng Lau"
      ],
      "abstract": "We introduce Reduced-Order Model-Guided Reinforcement Learning (ROM-GRL), a two-stage reinforcement learning framework for humanoid walking that requires no motion capture data or elaborate reward shaping. In the first stage, a compact 4-DOF (four-degree-of-freedom) reduced-order model (ROM) is trained via Proximal Policy Optimization. This generates energy-efficient gait templates. In the second stage, those dynamically consistent trajectories guide a full-body policy trained with Soft Actor--Critic augmented by an adversarial discriminator, ensuring the student's five-dimensional gait feature distribution matches the ROM's demonstrations. Experiments at 1 meter-per-second and 4 meter-per-second show that ROM-GRL produces stable, symmetric gaits with substantially lower tracking error than a pure-reward baseline. By distilling lightweight ROM guidance into high-dimensional policies, ROM-GRL bridges the gap between reward-only and imitation-based locomotion methods, enabling versatile, naturalistic humanoid behaviors without any human demonstrations.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ROM-GRLï¼Œä¸€ç§ç”¨äºäººå½¢æœºå™¨äººè¡Œèµ°çš„ä¸¤é˜¶æ®µReinforcement Learningæ¡†æ¶ï¼Œæ—¨åœ¨æ— éœ€è¿åŠ¨æ•æ‰æ•°æ®æˆ–å¤æ‚Reward Shapingçš„æƒ…å†µä¸‹å®ç°è‡ªä¸»è¿åŠ¨ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œè¯¥æ¡†æ¶é€šè¿‡Proximal Policy Optimizationè®­ç»ƒä¸€ä¸ªç´§å‡‘çš„4-DOF Reduced-Order Model (ROM)ï¼Œä»è€Œç”Ÿæˆèƒ½é‡é«˜æ•ˆçš„æ­¥æ€æ¨¡æ¿ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œåˆ©ç”¨è¿™äº›åŠ¨æ€ä¸€è‡´çš„è½¨è¿¹å¼•å¯¼å…¨å…³èŠ‚ç­–ç•¥ï¼Œå¹¶ç»“åˆSoft Actor-Criticå’ŒAdversarial Discriminatorï¼Œç¡®ä¿æœºå™¨äººçš„äº”ç»´æ­¥æ€ç‰¹å¾åˆ†å¸ƒä¸ROMçš„æ¼”ç¤ºç›¸åŒ¹é…ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒROM-GRLåœ¨1ç±³/ç§’å’Œ4ç±³/ç§’çš„è¡Œèµ°é€Ÿåº¦ä¸‹å‡èƒ½äº§ç”Ÿç¨³å®šä¸”å¯¹ç§°çš„æ­¥æ€ï¼Œå…¶Tracking Erroræ˜æ˜¾ä½äºçº¯å¥–åŠ±åŸºçº¿æ¨¡å‹ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†è½»é‡çº§ROMå¼•å¯¼è’¸é¦åˆ°é«˜ç»´ç­–ç•¥ä¸­ï¼ŒæˆåŠŸå¼¥è¡¥äº†ä»…ä¾èµ–å¥–åŠ±ä¸åŸºäºæ¨¡ä»¿çš„è¿åŠ¨æ§åˆ¶æ–¹æ³•ä¹‹é—´çš„å·®è·ï¼Œå®ç°äº†æ— éœ€äººç±»æ¼”ç¤ºçš„é«˜æ•ˆã€è‡ªç„¶çš„æœºå™¨äººè¡Œä¸ºã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "11 pages, 5 figures, 1 table, Computational Science Graduate Project",
      "pdf_url": "https://arxiv.org/pdf/2509.19023v1",
      "published_date": "2025-09-23 13:58:36 UTC",
      "updated_date": "2025-09-23 13:58:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:35:00.591085+00:00"
    },
    {
      "arxiv_id": "2509.19017v1",
      "title": "Fully Learnable Neural Reward Machines",
      "title_zh": "å®Œå…¨å¯å­¦ä¹ çš„ç¥ç»å¥–åŠ±æœº",
      "authors": [
        "Hazem Dewidar",
        "Elena Umili"
      ],
      "abstract": "Non-Markovian Reinforcement Learning (RL) tasks present significant challenges, as agents must reason over entire trajectories of state-action pairs to make optimal decisions. A common strategy to address this is through symbolic formalisms, such as Linear Temporal Logic (LTL) or automata, which provide a structured way to express temporally extended objectives. However, these approaches often rely on restrictive assumptions -- such as the availability of a predefined Symbol Grounding (SG) function mapping raw observations to high-level symbolic representations, or prior knowledge of the temporal task. In this work, we propose a fully learnable version of Neural Reward Machines (NRM), which can learn both the SG function and the automaton end-to-end, removing any reliance on prior knowledge. Our approach is therefore as easily applicable as classic deep RL (DRL) approaches, while being far more explainable, because of the finite and compact nature of automata. Furthermore, we show that by integrating Fully Learnable Reward Machines (FLNRM) with DRL, our method outperforms previous approaches based on Recurrent Neural Networks (RNNs).",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹éé©¬å°”å¯å¤«å¼ºåŒ–å­¦ä¹ (Non-Markovian Reinforcement Learning)ä»»åŠ¡ä¸­æ™ºèƒ½ä½“éš¾ä»¥åœ¨é•¿è½¨è¿¹ä¸Šè¿›è¡Œå†³ç­–çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†å…¨å¯å­¦ä¹ ç¥ç»å¥–åŠ±æœº(Fully Learnable Neural Reward Machines, FLNRM)ã€‚ä»¥å¾€çš„ç¬¦å·åŒ–æ–¹æ³•é€šå¸¸ä¾èµ–é¢„å®šä¹‰çš„ç¬¦å·æ¥åœ°(Symbol Grounding)å‡½æ•°æˆ–å…ˆéªŒæ—¶åºçŸ¥è¯†ï¼Œè€ŒFLNRMé€šè¿‡ç«¯åˆ°ç«¯(end-to-end)çš„æ–¹å¼åŒæ—¶å­¦ä¹ ç¬¦å·æ¥åœ°å‡½æ•°å’Œè‡ªåŠ¨æœº(automaton)ï¼Œå®Œå…¨æ¶ˆé™¤äº†å¯¹å…ˆéªŒçŸ¥è¯†çš„ä¾èµ–ã€‚è¯¥æ–¹æ³•ä¸ä»…å…·å¤‡ä¸ç»å…¸æ·±åº¦å¼ºåŒ–å­¦ä¹ (Deep RL)ç›¸å½“çš„æ˜“ç”¨æ€§ï¼Œè¿˜åˆ©ç”¨è‡ªåŠ¨æœºçš„ç´§å‡‘ç‰¹æ€§æ˜¾è‘—æå‡äº†æ¨¡å‹çš„å¯è§£é‡Šæ€§(explainability)ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒFLNRMåœ¨ä¸æ·±åº¦å¼ºåŒ–å­¦ä¹ é›†æˆåï¼Œå…¶æ€§èƒ½ä¼˜äºä»¥å¾€åŸºäºå¾ªç¯ç¥ç»ç½‘ç»œ(RNN)çš„æ–¹æ³•ï¼Œä¸ºå¤„ç†å…·æœ‰æ—¶åºæ‰©å±•ç›®æ ‡çš„å¤æ‚ä»»åŠ¡æä¾›äº†æ›´é«˜æ•ˆä¸”é€æ˜çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19017v1",
      "published_date": "2025-09-23 13:57:13 UTC",
      "updated_date": "2025-09-23 13:57:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:35:02.759248+00:00"
    },
    {
      "arxiv_id": "2509.19012v3",
      "title": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey",
      "title_zh": "çº¯è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ï¼šå…¨é¢ç»¼è¿°",
      "authors": [
        "Dapeng Zhang",
        "Jing Sun",
        "Chenghui Hu",
        "Xiaoyan Wu",
        "Zhenlong Yuan",
        "Rui Zhou",
        "Fei Shen",
        "Qingguo Zhou"
      ],
      "abstract": "The emergence of Vision Language Action (VLA) models marks a paradigm shift from traditional policy-based control to generalized robotics, reframing Vision Language Models (VLMs) from passive sequence generators into active agents for manipulation and decision-making in complex, dynamic environments. This survey delves into advanced VLA methods, aiming to provide a clear taxonomy and a systematic, comprehensive review of existing research. It presents a comprehensive analysis of VLA applications across different scenarios and classifies VLA approaches into several paradigms: autoregression-based, diffusion-based, reinforcement-based, hybrid, and specialized methods; while examining their motivations, core strategies, and implementations in detail. In addition, foundational datasets, benchmarks, and simulation platforms are introduced. Building on the current VLA landscape, the review further proposes perspectives on key challenges and future directions to advance research in VLA models and generalizable robotics. By synthesizing insights from over three hundred recent studies, this survey maps the contours of this rapidly evolving field and highlights the opportunities and challenges that will shape the development of scalable, general-purpose VLA methods.",
      "tldr_zh": "è¯¥ç»¼è¿°æ·±å…¥æ¢è®¨äº†è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹ (Vision Language Action, VLA) çš„å…´èµ·ï¼Œæ ‡å¿—ç€æœºå™¨äººé¢†åŸŸä»ä¼ ç»ŸåŸºäºç­–ç•¥çš„æ§åˆ¶å‘é€šç”¨æœºå™¨äººçš„èŒƒå¼è½¬å˜ã€‚æ–‡ç« é€šè¿‡å¯¹ä¸‰ç™¾å¤šé¡¹ç ”ç©¶çš„ç³»ç»Ÿæ€§æ¢³ç†ï¼Œå°†ç°æœ‰çš„ VLA æ–¹æ³•åˆ’åˆ†ä¸ºåŸºäºè‡ªå›å½’ (autoregression-based)ã€æ‰©æ•£æ¨¡å‹ (diffusion-based)ã€å¼ºåŒ–å­¦ä¹  (reinforcement-based)ã€æ··åˆ (hybrid) åŠä¸“ç”¨æ–¹æ³•ç­‰å¤šç§èŒƒå¼ï¼Œå¹¶è¯¦ç»†å‰–æäº†å…¶æ ¸å¿ƒç­–ç•¥ä¸å®ç°ã€‚é™¤äº†å¯¹åº”ç”¨åœºæ™¯çš„å…¨é¢åˆ†æï¼Œç ”ç©¶è¿˜ç³»ç»Ÿä»‹ç»äº†æ”¯æ’‘è¯¥é¢†åŸŸå‘å±•çš„åŸºç¡€æ•°æ®é›†ã€åŸºå‡†æµ‹è¯• (benchmarks) å’Œä»¿çœŸå¹³å°ã€‚é’ˆå¯¹å½“å‰ VLA æ™¯è§‚ï¼Œè¯¥ç»¼è¿°è¿›ä¸€æ­¥æå‡ºäº†å…³äºå…³é”®æŒ‘æˆ˜ä¸æœªæ¥ç ”ç©¶æ–¹å‘çš„è§è§£ï¼Œæ—¨åœ¨æ¨åŠ¨å¯æ‰©å±•ä¸”é€šç”¨åŒ–çš„æœºå™¨äººæŠ€æœ¯è¿›æ­¥ã€‚æ­¤é¡¹å·¥ä½œä¸ºç†è§£ VLA å¦‚ä½•å°†è¢«åŠ¨åºåˆ—ç”Ÿæˆå™¨è½¬å˜ä¸ºèƒ½åœ¨å¤æ‚åŠ¨æ€ç¯å¢ƒä¸­è¿›è¡Œå†³ç­–çš„ä¸»åŠ¨æ™ºèƒ½ä½“æä¾›äº†æƒå¨çš„çŸ¥è¯†æ¶æ„ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19012v3",
      "published_date": "2025-09-23 13:53:52 UTC",
      "updated_date": "2025-11-10 07:53:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:35:04.062181+00:00"
    },
    {
      "arxiv_id": "2509.19002v2",
      "title": "VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction",
      "title_zh": "VIR-Benchï¼šé€šè¿‡æ—…æ¸¸è§†é¢‘è¡Œç¨‹é‡å»ºè¯„ä¼° MLLM çš„åœ°ç†ç©ºé—´ä¸æ—¶é—´ç†è§£èƒ½åŠ›",
      "authors": [
        "Hao Wang",
        "Eiki Murata",
        "Lingfang Zhang",
        "Ayako Sato",
        "So Fukuda",
        "Ziqi Yin",
        "Wentao Hu",
        "Keisuke Nakao",
        "Yusuke Nakamura",
        "Sebastian Zwirner",
        "Yi-Chia Chen",
        "Hiroyuki Otomo",
        "Hiroki Ouchi",
        "Daisuke Kawahara"
      ],
      "abstract": "Recent advances in multimodal large language models (MLLMs) have significantly enhanced video understanding capabilities, opening new possibilities for practical applications. Yet current video benchmarks focus largely on indoor scenes or short-range outdoor activities, leaving the challenges associated with long-distance travel largely unexplored. Mastering extended geospatial-temporal trajectories is critical for next-generation MLLMs, underpinning real-world tasks such as embodied-AI planning and navigation. To bridge this gap, we present VIR-Bench, a novel benchmark consisting of 200 travel videos that frames itinerary reconstruction as a challenging task designed to evaluate and push forward MLLMs' geospatial-temporal intelligence. Experimental results reveal that state-of-the-art MLLMs, including proprietary ones, struggle to achieve high scores, underscoring the difficulty of handling videos that span extended spatial and temporal scales. Moreover, we conduct an in-depth case study in which we develop a prototype travel-planning agent that leverages the insights gained from VIR-Bench. The agent's markedly improved itinerary recommendations verify that our evaluation protocol not only benchmarks models effectively but also translates into concrete performance gains in user-facing applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) åœ¨é•¿è·ç¦»æ—…è¡Œåœºæ™¯ä¸‹åœ°ç†ç©ºé—´å’Œæ—¶é—´ç†è§£èƒ½åŠ›çš„ç¼ºå¤±ï¼Œæå‡ºäº†å…¨æ–°çš„åŸºå‡†æµ‹è¯• VIR-Benchã€‚è¯¥åŸºå‡†åŒ…å«200ä¸ªæ—…æ¸¸è§†é¢‘ï¼Œé€šè¿‡è¡Œç¨‹é‡å»º (Itinerary Reconstruction) ä»»åŠ¡æŒ‘æˆ˜æ¨¡å‹å¯¹å¤æ‚æ—¶ç©ºè½¨è¿¹çš„è§£æèƒ½åŠ›ï¼Œè¿™å¯¹äºå…·èº«æ™ºèƒ½ (Embodied-AI) çš„è§„åˆ’ä¸å¯¼èˆªè‡³å…³é‡è¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå³ä¾¿æ˜¯ç›®å‰æœ€å…ˆè¿›çš„ä¸“æœ‰ MLLMs åœ¨å¤„ç†é•¿å°ºåº¦ç©ºé—´å’Œæ—¶é—´çš„è§†é¢‘æ—¶ä¹Ÿéš¾ä»¥è·å¾—é«˜åˆ†ï¼Œå‡¸æ˜¾äº†è¯¥é¢†åŸŸçš„å·¨å¤§æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…åˆ©ç”¨ä» VIR-Bench ä¸­è·å¾—çš„è§è§£å¼€å‘äº†ä¸€ä¸ªæ—…æ¸¸è§„åˆ’æ™ºèƒ½ä½“ (Travel-planning Agent) åŸå‹ï¼Œå…¶æ˜¾è‘—æå‡çš„æ¨èæ€§èƒ½éªŒè¯äº†è¯¥è¯„ä¼°åè®®åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚VIR-Bench ä¸ºæ¨åŠ¨ MLLMs è¿ˆå‘æ›´é«˜å±‚çº§çš„åœ°ç†ç©ºé—´å’Œæ—¶é—´æ™ºèƒ½å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2509.19002v2",
      "published_date": "2025-09-23 13:46:31 UTC",
      "updated_date": "2025-11-15 10:09:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:35:17.190278+00:00"
    },
    {
      "arxiv_id": "2509.20397v1",
      "title": "Variational Low-Rank Adaptation for Personalized Impaired Speech Recognition",
      "title_zh": "é¢å‘ä¸ªæ€§åŒ–éšœç¢è¯­éŸ³è¯†åˆ«çš„å˜åˆ†ä½ç§©è‡ªé€‚åº”",
      "authors": [
        "Niclas Pokel",
        "PehuÃ©n Moure",
        "Roman Boehringer",
        "Shih-Chii Liu",
        "Yingqiang Gao"
      ],
      "abstract": "Speech impairments resulting from congenital disorders, such as cerebral palsy, down syndrome, or apert syndrome, as well as acquired brain injuries due to stroke, traumatic accidents, or tumors, present major challenges to automatic speech recognition (ASR) systems. Despite recent advancements, state-of-the-art ASR models like Whisper still struggle with non-normative speech due to limited training data availability and high acoustic variability. Moreover, collecting and annotating non-normative speech is burdensome: speaking is effortful for many affected individuals, while laborious annotation often requires caregivers familiar with the speaker. This work introduces a novel ASR personalization method based on Bayesian Low-rank Adaptation for data-efficient fine-tuning. We validate our method on the English UA-Speech dataset and a newly collected German speech dataset, BF-Sprache, from a child with structural speech impairment. The dataset and approach are designed to reflect the challenges of low-resource settings that include individuals with speech impairments. Our method significantly improves ASR accuracy for impaired speech while maintaining data and annotation efficiency, offering a practical path toward inclusive ASR.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…ˆå¤©æ€§ç–¾ç—…æˆ–è·å¾—æ€§è„‘æŸä¼¤å¯¼è‡´çš„è¨€è¯­éšœç¢å¯¹è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆAutomatic Speech Recognition, ASRï¼‰ç³»ç»Ÿæ„æˆçš„å·¨å¤§æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå˜åˆ†ä½ç§©é€‚é…ï¼ˆVariational Low-Rank Adaptationï¼‰çš„æ–°å‹ ASR ä¸ªæ€§åŒ–æ–¹æ³•ã€‚ç›®å‰çš„å…ˆè¿›æ¨¡å‹å¦‚ Whisper åœ¨å¤„ç†å…·æœ‰é«˜å£°å­¦å˜å¼‚æ€§çš„éè§„èŒƒè¯­éŸ³æ—¶ä»é¢ä¸´å›°å¢ƒï¼Œä¸”ç›¸å…³æ•°æ®çš„æ”¶é›†ä¸æ ‡æ³¨ç”±äºå—è¯•è€…çš„èº«ä½“çŠ¶å†µè€Œæ˜¾å¾—æå…¶å›°éš¾ã€‚ä¸ºè§£å†³è¿™ä¸€ä½èµ„æºç¯å¢ƒä¸‹çš„é€‚é…é—®é¢˜ï¼Œè¯¥ç ”ç©¶åˆ©ç”¨ Bayesian Low-rank Adaptation å®ç°äº†æ•°æ®é«˜æ•ˆçš„å¾®è°ƒï¼Œå¹¶åœ¨è‹±è¯­ UA-Speech å’Œæ–°é‡‡é›†çš„å¾·è¯­ BF-Sprache æ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤§å¹…æå‡å—æŸè¯­éŸ³è¯†åˆ«å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œèƒ½å¤Ÿä¿æŒæé«˜çš„æ•°æ®å’Œæ ‡æ³¨æ•ˆç‡ã€‚è¿™é¡¹å·¥ä½œä¸ºåœ¨ç°å®èµ„æºå—é™åœºæ™¯ä¸‹æ„å»ºæ›´å…·åŒ…å®¹æ€§çš„ ASR ç³»ç»Ÿæä¾›äº†å®ç”¨ä¸”é«˜æ•ˆçš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.20397v1",
      "published_date": "2025-09-23 13:44:58 UTC",
      "updated_date": "2025-09-23 13:44:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:35:24.197619+00:00"
    },
    {
      "arxiv_id": "2509.18986v1",
      "title": "Remaining Time Prediction in Outbound Warehouse Processes: A Case Study (Short Paper)",
      "title_zh": "ä»“å‚¨å‡ºåº“æµç¨‹ä¸­çš„å‰©ä½™æ—¶é—´é¢„æµ‹ï¼šæ¡ˆä¾‹ç ”ç©¶ï¼ˆçŸ­è®ºæ–‡ï¼‰",
      "authors": [
        "Erik Penther",
        "Michael Grohs",
        "Jana-Rebecca Rehse"
      ],
      "abstract": "Predictive process monitoring is a sub-domain of process mining which aims to forecast the future of ongoing process executions. One common prediction target is the remaining time, meaning the time that will elapse until a process execution is completed. In this paper, we compare four different remaining time prediction approaches in a real-life outbound warehouse process of a logistics company in the aviation business. For this process, the company provided us with a novel and original event log with 169,523 traces, which we can make publicly available. Unsurprisingly, we find that deep learning models achieve the highest accuracy, but shallow methods like conventional boosting techniques achieve competitive accuracy and require significantly fewer computational resources.",
      "tldr_zh": "è¯¥ç ”ç©¶èšç„¦äºé¢„æµ‹æ€§æµç¨‹ç›‘æ§(Predictive process monitoring)ä¸­çš„å‰©ä½™æ—¶é—´é¢„æµ‹(Remaining time prediction)ä»»åŠ¡ï¼Œé’ˆå¯¹èˆªç©ºç‰©æµå…¬å¸çœŸå®çš„ä»“åº“å‡ºåº“æµç¨‹å±•å¼€äº†æ¡ˆä¾‹ç ”ç©¶ã€‚ä½œè€…æä¾›å¹¶å…¬å¼€äº†ä¸€ä¸ªåŒ…å« 169,523 æ¡è½¨è¿¹çš„å…¨æ–°åŸå§‹äº‹ä»¶æ—¥å¿—(Event log)ï¼Œä¸ºè¯¥é¢†åŸŸçš„åç»­ç ”ç©¶æä¾›äº†é‡è¦æ•°æ®æ”¯æŒã€‚ç ”ç©¶é€šè¿‡å¯¹æ¯”å››ç§ä¸åŒçš„å‰©ä½™æ—¶é—´é¢„æµ‹æ–¹æ³•ï¼Œè¯„ä¼°äº†æ·±åº¦å­¦ä¹ æ¨¡å‹(Deep learning models)ä¸ä¼ ç»Ÿæå‡æŠ€æœ¯(Boosting techniques)ç­‰æµ…å±‚æ–¹æ³•(Shallow methods)çš„æ€§èƒ½è¡¨ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å‡†ç¡®ç‡ä¸Šå æ®ä¼˜åŠ¿ï¼Œä½†ä¼ ç»Ÿçš„æå‡æŠ€æœ¯åœ¨ä¿æŒç«äº‰åŠ›çš„å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†å¯¹è®¡ç®—èµ„æºçš„éœ€æ±‚ã€‚è¿™ä¸€å‘ç°ä¸ºç‰©æµä¼ä¸šåœ¨å®é™…åº”ç”¨ä¸­æƒè¡¡æ¨¡å‹ç²¾åº¦ä¸è®¡ç®—æˆæœ¬æä¾›äº†å…³é”®è§è§£ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Short paper at the ML4PM Workshop 2025, held in conjunction with the ICPM 2025 in Montevideo, Uruguay",
      "pdf_url": "https://arxiv.org/pdf/2509.18986v1",
      "published_date": "2025-09-23 13:37:09 UTC",
      "updated_date": "2025-09-23 13:37:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:35:17.583839+00:00"
    },
    {
      "arxiv_id": "2509.18980v2",
      "title": "From latent factors to language: a user study on LLM-generated explanations for an inherently interpretable matrix-based recommender system",
      "title_zh": "ä»æ½œåœ¨å› å­åˆ°è¯­è¨€ï¼šå…³äº LLM ä¸ºå†…ç”Ÿå¯è§£é‡ŠçŸ©é˜µæ¨èç³»ç»Ÿç”Ÿæˆè§£é‡Šçš„ç”¨æˆ·ç ”ç©¶",
      "authors": [
        "Maxime Manderlier",
        "Fabian Lecron",
        "Olivier Vu Thanh",
        "Nicolas Gillis"
      ],
      "abstract": "We investigate whether large language models (LLMs) can generate effective, user-facing explanations from a mathematically interpretable recommendation model. The model is based on constrained matrix factorization, where user types are explicitly represented and predicted item scores share the same scale as observed ratings, making the model's internal representations and predicted scores directly interpretable. This structure is translated into natural language explanations using carefully designed LLM prompts. Many works in explainable AI rely on automatic evaluation metrics, which often fail to capture users' actual needs and perceptions. In contrast, we adopt a user-centered approach: we conduct a study with 326 participants who assessed the quality of the explanations across five key dimensions-transparency, effectiveness, persuasion, trust, and satisfaction-as well as the recommendations themselves. To evaluate how different explanation strategies are perceived, we generate multiple explanation types from the same underlying model, varying the input information provided to the LLM. Our analysis reveals that all explanation types are generally well received, with moderate statistical differences between strategies. User comments further underscore how participants react to each type of explanation, offering complementary insights beyond the quantitative results.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚ä½•ä»æ•°å­¦ä¸Šå¯è§£é‡Šçš„æ¨èæ¨¡å‹ä¸­ç”Ÿæˆæœ‰æ•ˆçš„é¢å‘ç”¨æˆ·çš„è§£é‡Šã€‚ç ”ç©¶åŸºäºå—é™çŸ©é˜µåˆ†è§£ï¼ˆconstrained matrix factorizationï¼‰æ¨¡å‹ï¼Œé€šè¿‡æ˜¾å¼è¡¨ç¤ºç”¨æˆ·ç±»å‹å¹¶ä½¿é¢„æµ‹è¯„åˆ†ä¸å®é™…è¯„åˆ†é‡è¡¨ä¸€è‡´ï¼Œç¡®ä¿äº†æ¨¡å‹å†…éƒ¨è¡¨ç¤ºå’Œé¢„æµ‹å¾—åˆ†çš„ç›´æ¥å¯è§£é‡Šæ€§ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨ç²¾å¿ƒè®¾è®¡çš„ LLM prompts å°†è¿™äº›å†…éƒ¨æ•°å­¦ç»“æ„è½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€ï¼Œå¹¶å¼€å±•äº†åŒ…å« 326 åå‚ä¸è€…çš„ç”¨æˆ·ç ”ç©¶ã€‚è¯„ä¼°è¿‡ç¨‹å›´ç»•é€æ˜åº¦ï¼ˆtransparencyï¼‰ã€æœ‰æ•ˆæ€§ï¼ˆeffectivenessï¼‰ã€è¯´æœåŠ›ï¼ˆpersuasionï¼‰ã€ä¿¡ä»»æ„Ÿï¼ˆtrustï¼‰å’Œæ»¡æ„åº¦ï¼ˆsatisfactionï¼‰äº”ä¸ªå…³é”®ç»´åº¦å±•å¼€ã€‚åˆ†æç»“æœè¡¨æ˜ï¼Œæ‰€æœ‰ç”Ÿæˆçš„è§£é‡Šç±»å‹æ™®éå—åˆ°ç”¨æˆ·å¥½è¯„ï¼Œä¸åŒç­–ç•¥ä¹‹é—´å‘ˆç°å‡ºä¸­åº¦çš„ç»Ÿè®¡å­¦å·®å¼‚ã€‚ç”¨æˆ·è¯„è®ºè¿›ä¸€æ­¥æ­ç¤ºäº†å‚ä¸è€…å¯¹å„ç§è§£é‡Šç±»å‹çš„å…·ä½“ååº”ï¼Œä¸ºå®šé‡æ•°æ®æä¾›äº†é‡è¦çš„å®šæ€§è¡¥å……ã€‚",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18980v2",
      "published_date": "2025-09-23 13:30:03 UTC",
      "updated_date": "2025-10-01 07:49:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:35:21.893565+00:00"
    },
    {
      "arxiv_id": "2509.18970v2",
      "title": "LLM-based Agents Suffer from Hallucinations: A Survey of Taxonomy, Methods, and Directions",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½ä½“å¹»è§‰é—®é¢˜ï¼šåˆ†ç±»ä½“ç³»ã€æ–¹æ³•ä¸æ–¹å‘ç»¼è¿°",
      "authors": [
        "Xixun Lin",
        "Yucheng Ning",
        "Jingwen Zhang",
        "Yan Dong",
        "Yilong Liu",
        "Yongxuan Wu",
        "Xiaohua Qi",
        "Nan Sun",
        "Yanmin Shang",
        "Kun Wang",
        "Pengfei Cao",
        "Qingyue Wang",
        "Lixin Zou",
        "Xu Chen",
        "Chuan Zhou",
        "Jia Wu",
        "Peng Zhang",
        "Qingsong Wen",
        "Shirui Pan",
        "Bin Wang",
        "Yanan Cao",
        "Kai Chen",
        "Songlin Hu",
        "Li Guo"
      ],
      "abstract": "Driven by the rapid advancements of Large Language Models (LLMs), LLM-based agents have emerged as powerful intelligent systems capable of human-like cognition, reasoning, and interaction. These agents are increasingly being deployed across diverse real-world applications, including student education, scientific research, and financial analysis. However, despite their remarkable potential, LLM-based agents remain vulnerable to hallucination issues, which can result in erroneous task execution and undermine the reliability of the overall system design. Addressing this critical challenge requires a deep understanding and a systematic consolidation of recent advances on LLM-based agents. To this end, we present the first comprehensive survey of hallucinations in LLM-based agents. By carefully analyzing the complete workflow of agents, we propose a new taxonomy that identifies different types of agent hallucinations occurring at different stages. Furthermore, we conduct an in-depth examination of eighteen triggering causes underlying the emergence of agent hallucinations. Through a detailed review of a large number of existing studies, we summarize approaches for hallucination mitigation and detection, and highlight promising directions for future research. We hope this survey will inspire further efforts toward addressing hallucinations in LLM-based agents, ultimately contributing to the development of more robust and reliable agent systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æä¾›äº†é’ˆå¯¹ LLM-based agents å¹»è§‰é—®é¢˜çš„é¦–ä¸ªå…¨é¢ç»¼è¿°ï¼Œæ—¨åœ¨è§£å†³è¿™äº›æ™ºèƒ½ç³»ç»Ÿåœ¨å®é™…åº”ç”¨ä¸­å› é”™è¯¯æ‰§è¡Œä»»åŠ¡è€Œå½±å“å¯é æ€§çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚ä½œè€…é€šè¿‡åˆ†ææ™ºèƒ½ä½“çš„å®Œæ•´å·¥ä½œæµï¼Œæå‡ºäº†ä¸€ç§å…¨æ–°çš„ Taxonomyï¼Œç³»ç»Ÿæ€§åœ°è¯†åˆ«äº†åœ¨ä¸åŒé˜¶æ®µå‡ºç°çš„å„ç§å¹»è§‰ç±»å‹ã€‚æ–‡ç« æ·±å…¥æ¢è®¨äº†è¯±å‘å¹»è§‰çš„ 18 ç§æ½œåœ¨åŸå› ï¼Œå¹¶å¯¹å¤§é‡ç°æœ‰ç ”ç©¶è¿›è¡Œäº†ç»†è‡´æ¢³ç†ã€‚åŸºäºæ­¤ï¼Œè®ºæ–‡æ€»ç»“äº†å½“å‰ä¸»æµçš„ Hallucination Mitigation å’Œ Detection æ–¹æ³•ï¼Œå¹¶æŒ‡å‡ºäº†å…·æœ‰å‰æ™¯çš„æœªæ¥ç ”ç©¶æ–¹å‘ã€‚è¯¥ç»¼è¿°ä¸ºç†è§£å’Œè§£å†³ LLM-based agents çš„å¹»è§‰é—®é¢˜æä¾›äº†é‡è¦å‚è€ƒï¼Œæœ‰åŠ©äºæ¨åŠ¨å¼€å‘å‡ºæ›´ç¨³å¥ã€æ›´å¯ä¿¡çš„æ™ºèƒ½ä½“ç³»ç»Ÿã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18970v2",
      "published_date": "2025-09-23 13:24:48 UTC",
      "updated_date": "2025-11-18 06:14:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:35:41.690189+00:00"
    },
    {
      "arxiv_id": "2509.18953v1",
      "title": "Eva-VLA: Evaluating Vision-Language-Action Models' Robustness Under Real-World Physical Variations",
      "title_zh": "Eva-VLAï¼šè¯„ä¼°è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨ç°å®ä¸–ç•Œç‰©ç†å˜åŒ–ä¸‹çš„é²æ£’æ€§",
      "authors": [
        "Hanqing Liu",
        "Jiahuan Long",
        "Junqi Wu",
        "Jiacheng Hou",
        "Huili Tang",
        "Tingsong Jiang",
        "Weien Zhou",
        "Wen Yao"
      ],
      "abstract": "Vision-Language-Action (VLA) models have emerged as promising solutions for robotic manipulation, yet their robustness to real-world physical variations remains critically underexplored. To bridge this gap, we propose Eva-VLA, the first unified framework that systematically evaluates the robustness of VLA models by transforming discrete physical variations into continuous optimization problems. However, comprehensively assessing VLA robustness presents two key challenges: (1) how to systematically characterize diverse physical variations encountered in real-world deployments while maintaining evaluation reproducibility, and (2) how to discover worst-case scenarios without prohibitive real-world data collection costs efficiently. To address the first challenge, we decompose real-world variations into three critical domains: object 3D transformations that affect spatial reasoning, illumination variations that challenge visual perception, and adversarial patches that disrupt scene understanding. For the second challenge, we introduce a continuous black-box optimization framework that transforms discrete physical variations into parameter optimization, enabling systematic exploration of worst-case scenarios. Extensive experiments on state-of-the-art OpenVLA models across multiple benchmarks reveal alarming vulnerabilities: all variation types trigger failure rates exceeding 60%, with object transformations causing up to 97.8% failure in long-horizon tasks. Our findings expose critical gaps between controlled laboratory success and unpredictable deployment readiness, while the Eva-VLA framework provides a practical pathway for hardening VLA-based robotic manipulation models against real-world deployment challenges.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Eva-VLAï¼Œè¿™æ˜¯é¦–ä¸ªç³»ç»Ÿè¯„ä¼° Vision-Language-Action (VLA) æ¨¡å‹åœ¨ç°å®ä¸–ç•Œç‰©ç†å˜åŒ–ä¸‹é²æ£’æ€§çš„ç»Ÿä¸€æ¡†æ¶ã€‚é’ˆå¯¹è¯„ä¼°å¯é‡å¤æ€§åŠæœ€åæƒ…å†µæ¢ç´¢æ•ˆç‡çš„æŒ‘æˆ˜ï¼ŒEva-VLA å°†ç¦»æ•£çš„ç‰©ç†å˜åŒ–è½¬åŒ–ä¸ºè¿ç»­çš„é»‘ç›’ä¼˜åŒ– (continuous black-box optimization) é—®é¢˜ï¼Œå¹¶å°†å…¶åˆ†è§£ä¸ºç‰©ä½“ 3D å˜æ¢ã€å…‰ç…§å˜åŒ–å’Œå¯¹æŠ—æ€§è¡¥ä¸ (adversarial patches) ä¸‰ä¸ªå…³é”®é¢†åŸŸã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰çš„ OpenVLA æ¨¡å‹åœ¨è¿™äº›ç‰©ç†å˜åŒ–ä¸‹è¡¨ç°å‡ºä¸¥é‡çš„è„†å¼±æ€§ï¼Œå„ç±»å˜åŒ–çš„å¤±è´¥ç‡å‡è¶…è¿‡ 60%ï¼Œå…¶ä¸­ç‰©ä½“å˜æ¢åœ¨é•¿ç¨‹ä»»åŠ¡ä¸­çš„å¤±è´¥ç‡ç”šè‡³é«˜è¾¾ 97.8%ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†å—æ§å®éªŒå®¤ç¯å¢ƒä¸å®é™…éƒ¨ç½²å°±ç»ªçŠ¶æ€ä¹‹é—´çš„å·¨å¤§å·®è·ï¼Œå¹¶ä¸ºå¼ºåŒ–æœºå™¨äººæ“ä½œæ¨¡å‹åº”å¯¹ç°å®æŒ‘æˆ˜æä¾›äº†å®è·µè·¯å¾„ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18953v1",
      "published_date": "2025-09-23 13:02:23 UTC",
      "updated_date": "2025-09-23 13:02:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:35:40.191267+00:00"
    },
    {
      "arxiv_id": "2509.18949v1",
      "title": "Towards Privacy-Aware Bayesian Networks: A Credal Approach",
      "title_zh": "è¿ˆå‘éšç§æ„ŸçŸ¥è´å¶æ–¯ç½‘ç»œï¼šä¸€ç§ Credal æ–¹æ³•",
      "authors": [
        "NiccolÃ² Rocchi",
        "Fabio Stella",
        "Cassio de Campos"
      ],
      "abstract": "Bayesian networks (BN) are probabilistic graphical models that enable efficient knowledge representation and inference. These have proven effective across diverse domains, including healthcare, bioinformatics and economics. The structure and parameters of a BN can be obtained by domain experts or directly learned from available data. However, as privacy concerns escalate, it becomes increasingly critical for publicly released models to safeguard sensitive information in training data. Typically, released models do not prioritize privacy by design. In particular, tracing attacks from adversaries can combine the released BN with auxiliary data to determine whether specific individuals belong to the data from which the BN was learned. State-of-the-art protection tecniques involve introducing noise into the learned parameters. While this offers robust protection against tracing attacks, it significantly impacts the model's utility, in terms of both the significance and accuracy of the resulting inferences. Hence, high privacy may be attained at the cost of releasing a possibly ineffective model. This paper introduces credal networks (CN) as a novel solution for balancing the model's privacy and utility. After adapting the notion of tracing attacks, we demonstrate that a CN enables the masking of the learned BN, thereby reducing the probability of successful attacks. As CNs are obfuscated but not noisy versions of BNs, they can achieve meaningful inferences while safeguarding privacy. Moreover, we identify key learning information that must be concealed to prevent attackers from recovering the underlying BN. Finally, we conduct a set of numerical experiments to analyze how privacy gains can be modulated by tuning the CN hyperparameters. Our results confirm that CNs provide a principled, practical, and effective approach towards the development of privacy-aware probabilistic graphical models.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†Bayesian networks (BN) åœ¨æ•°æ®éšç§ä¿æŠ¤æ–¹é¢çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹æ—¨åœ¨è¯†åˆ«è®­ç»ƒæ•°æ®ä¸­ç‰¹å®šä¸ªä½“çš„tracing attacksã€‚é’ˆå¯¹ç°æœ‰å™ªå£°æ³¨å…¥æŠ€æœ¯å› æŸå®³æ¨¡å‹utilityè€Œå¯¼è‡´æ¨æ–­å‡†ç¡®æ€§ä¸‹é™çš„é—®é¢˜ï¼Œè®ºæ–‡æå‡ºåˆ©ç”¨credal networks (CN) ä½œä¸ºä¸€ç§å¹³è¡¡éšç§ä¸æ•ˆç”¨çš„åˆ›æ–°æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•é€šè¿‡å°†CNä½œä¸ºå­¦ä¹ åˆ°çš„BNçš„æ©è”½æ‰‹æ®µï¼Œåœ¨ä¸å¼•å…¥å™ªå£°çš„æƒ…å†µä¸‹é™ä½æ”»å‡»æˆåŠŸç‡ï¼Œç¡®ä¿äº†æ¨¡å‹åœ¨ä¿æŠ¤éšç§çš„åŒæ—¶ä»èƒ½å®ç°æœ‰æ„ä¹‰çš„æ¨æ–­ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¯†åˆ«äº†é˜²æ­¢æ”»å‡»è€…æ¢å¤åº•å±‚BNæ‰€éœ€éšè—çš„å…³é”®å­¦ä¹ ä¿¡æ¯ï¼Œå¹¶é€šè¿‡æ•°å€¼å®éªŒè¯æ˜äº†åˆ©ç”¨CNè¶…å‚æ•°å¯æœ‰æ•ˆè°ƒèŠ‚éšç§å¢ç›Šã€‚å®éªŒç»“æœè¯å®ï¼ŒCNä¸ºå¼€å‘éšç§æ„ŸçŸ¥å‹probabilistic graphical modelsæä¾›äº†ä¸€ç§å…¼å…·åŸåˆ™æ€§ä¸å®ç”¨æ€§çš„æœ‰æ•ˆé€”å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at ECAI2025 conference, 20 pages, 1 figure",
      "pdf_url": "https://arxiv.org/pdf/2509.18949v1",
      "published_date": "2025-09-23 12:58:32 UTC",
      "updated_date": "2025-09-23 12:58:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:35:42.084905+00:00"
    },
    {
      "arxiv_id": "2509.18942v2",
      "title": "Data Efficient Adaptation in Large Language Models via Continuous Low-Rank Fine-Tuning",
      "title_zh": "é€šè¿‡æŒç»­ä½ç§©å¾®è°ƒå®ç°å¤§è¯­è¨€æ¨¡å‹çš„æ•°æ®é«˜æ•ˆé€‚é…",
      "authors": [
        "Xiao Han",
        "Zimo Zhao",
        "Wanyu Wang",
        "Maolin Wang",
        "Zitao Liu",
        "Yi Chang",
        "Xiangyu Zhao"
      ],
      "abstract": "Recent advancements in Large Language Models (LLMs) have emphasized the critical role of fine-tuning (FT) techniques in adapting LLMs to specific tasks, especially when retraining from scratch is computationally infeasible. Fine-tuning enables LLMs to leverage task- or domain-specific data, producing models that more effectively meet the requirements of targeted applications. However, conventional FT approaches often suffer from catastrophic forgetting and suboptimal data efficiency, limiting their real-world applicability. To address these challenges, this paper proposes \\textbf{DEAL}, a novel framework that integrates Low-Rank Adaptation (LoRA) with a continuous fine-tuning strategy. By incorporating knowledge retention and adaptive parameter update modules, the framework mitigates the limitations of existing FT methods while maintaining efficiency. Experiments on 15 diverse datasets show that DEAL consistently outperforms baseline methods, yielding substantial gains in task accuracy and resource efficiency. These findings demonstrate the potential of our approach to advance continual adaptation in LLMs by enhancing task performance while improving resource efficiency. The source code is publicly available at https://github.com/zzm-black/DEAL-Continuous-Low-Rank-Fine-Tuning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å¾®è°ƒ (FT) è¿‡ç¨‹ä¸­é¢ä¸´çš„ç¾éš¾æ€§é—å¿˜ (catastrophic forgetting) å’Œæ•°æ®æ•ˆç‡ä½ä¸‹ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º DEAL çš„åˆ›æ–°æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†ä½ç§©è‡ªé€‚åº” (LoRA) ä¸æŒç»­å¾®è°ƒ (continuous fine-tuning) ç­–ç•¥ç›¸ç»“åˆï¼Œé€šè¿‡å¼•å…¥çŸ¥è¯†ä¿ç•™ (knowledge retention) å’Œè‡ªé€‚åº”å‚æ•°æ›´æ–° (adaptive parameter update) æ¨¡å—ï¼Œåœ¨ä¿æŒè¿è¡Œæ•ˆç‡çš„åŒæ—¶æœ‰æ•ˆç¼“è§£äº†ç°æœ‰å¾®è°ƒæ–¹æ³•çš„å±€é™æ€§ã€‚åœ¨ 15 ä¸ªå¤šæ ·åŒ–æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒDEAL åœ¨ä»»åŠ¡å‡†ç¡®ç‡å’Œèµ„æºæ•ˆç‡æ–¹é¢å‡æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚è¯¥ç ”ç©¶è¯æ˜äº†åœ¨å¢å¼ºä»»åŠ¡æ€§èƒ½çš„åŒæ—¶æå‡èµ„æºåˆ©ç”¨ç‡çš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºæ¨åŠ¨å¤§è¯­è¨€æ¨¡å‹çš„æŒç»­é€‚åº” (continual adaptation) æä¾›äº†é«˜æ•ˆçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.18942v2",
      "published_date": "2025-09-23 12:55:57 UTC",
      "updated_date": "2025-10-22 02:37:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:36:18.870366+00:00"
    },
    {
      "arxiv_id": "2509.18938v1",
      "title": "No Labels Needed: Zero-Shot Image Classification with Collaborative Self-Learning",
      "title_zh": "æ— éœ€æ ‡ç­¾ï¼šåŸºäºåä½œè‡ªå­¦ä¹ çš„é›¶æ ·æœ¬å›¾åƒåˆ†ç±»",
      "authors": [
        "Matheus VinÃ­cius Todescato",
        "Joel LuÃ­s Carbonera"
      ],
      "abstract": "While deep learning, including Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs), has significantly advanced classification performance, its typical reliance on extensive annotated datasets presents a major obstacle in many practical scenarios where such data is scarce. Vision-language models (VLMs) and transfer learning with pre-trained visual models appear as promising techniques to deal with this problem. This paper proposes a novel zero-shot image classification framework that combines a VLM and a pre-trained visual model within a self-learning cycle. Requiring only the set of class names and no labeled training data, our method utilizes a confidence-based pseudo-labeling strategy to train a lightweight classifier directly on the test data, enabling dynamic adaptation. The VLM identifies high-confidence samples, and the pre-trained visual model enhances their visual representations. These enhanced features then iteratively train the classifier, allowing the system to capture complementary semantic and visual cues without supervision. Notably, our approach avoids VLM fine-tuning and the use of large language models, relying on the visual-only model to reduce the dependence on semantic representation. Experimental evaluations on ten diverse datasets demonstrate that our approach outperforms the baseline zero-shot method.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ— éœ€æ ‡æ³¨æ•°æ®çš„é›¶æ ·æœ¬å›¾åƒåˆ†ç±»æ¡†æ¶ï¼Œé€šè¿‡åä½œè‡ªå­¦ä¹ (Collaborative Self-Learning)å¾ªç¯ç»“åˆäº†è§†è§‰è¯­è¨€æ¨¡å‹(Vision-language models, VLMs)å’Œé¢„è®­ç»ƒè§†è§‰æ¨¡å‹ã€‚è¯¥æ–¹æ³•ä»…éœ€ç±»åˆ«åç§°ï¼Œåˆ©ç”¨åŸºäºç½®ä¿¡åº¦çš„ä¼ªæ ‡ç­¾(confidence-based pseudo-labeling)ç­–ç•¥åœ¨æµ‹è¯•æ•°æ®ä¸Šç›´æ¥è®­ç»ƒè½»é‡çº§åˆ†ç±»å™¨ï¼Œä»è€Œå®ç°åŠ¨æ€é€‚é…ã€‚åœ¨è‡ªå­¦ä¹ è¿‡ç¨‹ä¸­ï¼ŒVLMè´Ÿè´£è¯†åˆ«é«˜ç½®ä¿¡åº¦æ ·æœ¬ï¼Œè€Œé¢„è®­ç»ƒè§†è§‰æ¨¡å‹åˆ™ç”¨äºå¢å¼ºå…¶è§†è§‰è¡¨ç¤ºï¼Œé€šè¿‡è¿­ä»£è®­ç»ƒä½¿ç³»ç»Ÿåœ¨æ— ç›‘ç£æƒ…å†µä¸‹æ•æ‰äº’è¡¥çš„è¯­ä¹‰å’Œè§†è§‰çº¿ç´¢ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¯¥æ–¹æ¡ˆé¿å…äº†å¯¹VLMè¿›è¡Œå¾®è°ƒæˆ–ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)ï¼Œä¸»è¦ä¾èµ–è§†è§‰æ¨¡å‹æ¥é™ä½å¯¹è¯­ä¹‰è¡¨ç¤ºçš„ä¾èµ–ã€‚åœ¨åä¸ªå¤šæ ·åŒ–æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºåŸºå‡†é›¶æ ·æœ¬æ–¹æ³•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "This paper was accepted at International Conference on Tools with Artificial Intelligence (ICTAI) 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.18938v1",
      "published_date": "2025-09-23 12:54:52 UTC",
      "updated_date": "2025-09-23 12:54:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:36:02.486509+00:00"
    },
    {
      "arxiv_id": "2509.20396v1",
      "title": "Data-Efficient ASR Personalization for Non-Normative Speech Using an Uncertainty-Based Phoneme Difficulty Score for Guided Sampling",
      "title_zh": "é’ˆå¯¹éæ ‡å‡†è¯­éŸ³çš„æ•°æ®é«˜æ•ˆ ASR ä¸ªæ€§åŒ–ï¼šåŸºäºä¸ç¡®å®šæ€§éŸ³ç´ éš¾åº¦è¯„åˆ†çš„å¼•å¯¼å¼é‡‡æ ·",
      "authors": [
        "Niclas Pokel",
        "PehuÃ©n Moure",
        "Roman Boehringer",
        "Yingqiang Gao"
      ],
      "abstract": "Automatic speech recognition (ASR) systems struggle with non-normative speech from individuals with impairments caused by conditions like cerebral palsy or structural anomalies. The high acoustic variability and scarcity of training data severely degrade model performance. This work introduces a data-efficient personalization method that quantifies phoneme-level uncertainty to guide fine-tuning. We leverage Monte Carlo Dropout to estimate which phonemes a model finds most difficult and use these estimates for a targeted oversampling strategy. We validate our method on English and German datasets. Crucially, we demonstrate that our model-derived uncertainty strongly correlates with phonemes identified as challenging in an expert clinical logopedic report, marking, to our knowledge, the first work to successfully align model uncertainty with expert assessment of speech difficulty. Our results show that this clinically-validated, uncertainty-guided sampling significantly improves ASR accuracy, delivering a practical framework for personalized and inclusive ASR.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è„‘ç˜«æˆ–ç»“æ„å¼‚å¸¸å¯¼è‡´çš„éæ ‡å‡†è¯­éŸ³(Non-Normative Speech)åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«(ASR)ç³»ç»Ÿä¸­è¡¨ç°ä¸ä½³çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§é«˜æ•ˆçš„æ•°æ®ä¸ªæ€§åŒ–æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ Monte Carlo Dropout æŠ€æœ¯é‡åŒ–éŸ³ç´ çº§çš„ä¸ç¡®å®šæ€§ï¼Œå¹¶åŸºäºä¸ç¡®å®šæ€§åˆ†æ•°(Uncertainty-Based Phoneme Difficulty Score)æ¥æŒ‡å¯¼å¾®è°ƒè¿‡ç¨‹ä¸­çš„å¼•å¯¼é‡‡æ ·(Guided Sampling)ã€‚é€šè¿‡è¯†åˆ«æ¨¡å‹è®¤ä¸ºæœ€å…·æŒ‘æˆ˜æ€§çš„éŸ³ç´ å¹¶å®æ–½é’ˆå¯¹æ€§çš„è¿‡é‡‡æ ·ç­–ç•¥ï¼Œè¯¥æ¡†æ¶åœ¨è®­ç»ƒæ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹æ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚å®éªŒåœ¨è‹±è¯­å’Œå¾·è¯­æ•°æ®é›†ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œç»“æœæ˜¾ç¤ºå…¶å¤§å¹…æé«˜äº†è¯†åˆ«å‡†ç¡®ç‡ã€‚å…³é”®å‘ç°æ˜¯æ¨¡å‹è®¡ç®—å‡ºçš„ä¸ç¡®å®šæ€§ä¸ä¸´åºŠè¨€è¯­ä¸“å®¶æŠ¥å‘Šä¸­ç¡®å®šçš„éŸ³ç´ å›°éš¾åº¦é«˜åº¦ç›¸å…³ï¼Œå®ç°äº†æ¨¡å‹ä¸ç¡®å®šæ€§ä¸ä¸“å®¶è¯„ä¼°çš„é¦–æ¬¡æˆåŠŸå¯¹é½ã€‚è¯¥ç ”ç©¶ä¸ºæ„å»ºä¸ªæ€§åŒ–ä¸”å…·æœ‰åŒ…å®¹æ€§çš„ ASR ç³»ç»Ÿæä¾›äº†ä¸€ä¸ªå®ç”¨ä¸”ç»è¿‡ä¸´åºŠéªŒè¯çš„å¯é æ¡†æ¶ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.20396v1",
      "published_date": "2025-09-23 12:54:30 UTC",
      "updated_date": "2025-09-23 12:54:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:36:06.188577+00:00"
    },
    {
      "arxiv_id": "2509.18933v1",
      "title": "Accurate and Efficient Prediction of Wi-Fi Link Quality Based on Machine Learning",
      "title_zh": "åŸºäºæœºå™¨å­¦ä¹ çš„ Wi-Fi é“¾è·¯è´¨é‡é«˜ç²¾åº¦é«˜æ•ˆé¢„æµ‹",
      "authors": [
        "Gabriele Formis",
        "Gianluca Cena",
        "Lukasz Wisniewski",
        "Stefano Scanzio"
      ],
      "abstract": "Wireless communications are characterized by their unpredictability, posing challenges for maintaining consistent communication quality. This paper presents a comprehensive analysis of various prediction models, with a focus on achieving accurate and efficient Wi-Fi link quality forecasts using machine learning techniques. Specifically, the paper evaluates the performance of data-driven models based on the linear combination of exponential moving averages, which are designed for low-complexity implementations and are then suitable for hardware platforms with limited processing resources. Accuracy of the proposed approaches was assessed using experimental data from a real-world Wi-Fi testbed, considering both channel-dependent and channel-independent training data. Remarkably, channel-independent models, which allow for generalized training by equipment manufacturers, demonstrated competitive performance. Overall, this study provides insights into the practical deployment of machine learning-based prediction models for enhancing Wi-Fi dependability in industrial environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ— çº¿é€šä¿¡çš„ä¸å¯é¢„æµ‹æ€§ï¼Œæå‡ºäº†åŸºäºæœºå™¨å­¦ä¹ (Machine Learning)çš„Wi-Fié“¾è·¯è´¨é‡é¢„æµ‹æ¨¡å‹ï¼Œæ—¨åœ¨å®ç°é«˜å‡†ç¡®æ€§ä¸é«˜æ•ˆç‡çš„å¹³è¡¡ã€‚è®ºæ–‡é‡ç‚¹åˆ†æå¹¶è¯„ä¼°äº†åŸºäºæŒ‡æ•°ç§»åŠ¨å¹³å‡(Exponential Moving Averages)çº¿æ€§ç»„åˆçš„æ•°æ®é©±åŠ¨æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹ç”±äºä½å¤æ‚åº¦(Low-complexity)çš„ç‰¹æ€§ï¼Œéå¸¸é€‚åˆå¤„ç†èµ„æºæœ‰é™çš„ç¡¬ä»¶å¹³å°ã€‚ç ”ç©¶åˆ©ç”¨çœŸå®ä¸–ç•Œçš„Wi-Fiæµ‹è¯•å¹³å°æ•°æ®ï¼Œå¯¹ä¿¡é“ç›¸å…³(Channel-dependent)å’Œä¿¡é“æ— å…³(Channel-independent)çš„è®­ç»ƒæ¨¡å¼è¿›è¡Œäº†å¯¹æ¯”è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ”¯æŒè®¾å¤‡å•†è¿›è¡Œé€šç”¨è®­ç»ƒçš„ä¿¡é“æ— å…³æ¨¡å‹å±•ç°å‡ºäº†æå…·ç«äº‰åŠ›çš„é¢„æµ‹æ€§èƒ½ã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥ç ”ç©¶ä¸ºåœ¨å·¥ä¸šç¯å¢ƒä¸­é€šè¿‡éƒ¨ç½²æœºå™¨å­¦ä¹ æ¨¡å‹æ¥æå‡Wi-Fiå¯é æ€§(Dependability)æä¾›äº†é‡è¦çš„å®è·µè§è§£ä¸æŠ€æœ¯å‚è€ƒã€‚",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NI",
      "comment": "accepted version in IEEE Transactions on Industrial Informatics, 12 pages, 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.18933v1",
      "published_date": "2025-09-23 12:52:01 UTC",
      "updated_date": "2025-09-23 12:52:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:36:12.808891+00:00"
    },
    {
      "arxiv_id": "2509.18930v1",
      "title": "Tackling GNARLy Problems: Graph Neural Algorithmic Reasoning Reimagined through Reinforcement Learning",
      "title_zh": "æ”»å…‹ GNARLy éš¾é¢˜ï¼šåŸºäºå¼ºåŒ–å­¦ä¹ é‡æ„çš„å›¾ç¥ç»ç®—æ³•æ¨ç†",
      "authors": [
        "Alex Schutz",
        "Victor-Alexandru Darvariu",
        "Efimia Panagiotaki",
        "Bruno Lacerda",
        "Nick Hawes"
      ],
      "abstract": "Neural Algorithmic Reasoning (NAR) is a paradigm that trains neural networks to execute classic algorithms by supervised learning. Despite its successes, important limitations remain: inability to construct valid solutions without post-processing and to reason about multiple correct ones, poor performance on combinatorial NP-hard problems, and inapplicability to problems for which strong algorithms are not yet known. To address these limitations, we reframe the problem of learning algorithm trajectories as a Markov Decision Process, which imposes structure on the solution construction procedure and unlocks the powerful tools of imitation and reinforcement learning (RL). We propose the GNARL framework, encompassing the methodology to translate problem formulations from NAR to RL and a learning architecture suitable for a wide range of graph-based problems. We achieve very high graph accuracy results on several CLRS-30 problems, performance matching or exceeding much narrower NAR approaches for NP-hard problems and, remarkably, applicability even when lacking an expert algorithm.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¥ç»ç®—æ³•æ¨ç†(Neural Algorithmic Reasoning, NAR)åœ¨å¤„ç†ç»„åˆNP-hardé—®é¢˜ã€å¤šè§£æ¨ç†åŠç¼ºä¹ä¸“å®¶ç®—æ³•æ—¶å­˜åœ¨çš„å±€é™æ€§ï¼Œæå‡ºäº†GNARLæ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†ç®—æ³•è½¨è¿¹å­¦ä¹ é‡æ–°å®šä¹‰ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(Markov Decision Process)ï¼Œä»è€Œå¼•å…¥å¼ºåŒ–å­¦ä¹ (Reinforcement Learning, RL)å’Œæ¨¡ä»¿å­¦ä¹ (Imitation Learning)æ¥ç»“æ„åŒ–è§£çš„æ„å»ºè¿‡ç¨‹ã€‚GNARLä¸ä»…æä¾›äº†ä¸€å¥—å°†NARé—®é¢˜è½¬åŒ–ä¸ºRLé—®é¢˜çš„é€šç”¨æ–¹æ³•è®ºï¼Œè¿˜è®¾è®¡äº†é€‚ç”¨äºå¹¿æ³›å›¾è®ºé—®é¢˜çš„å­¦ä¹ æ¶æ„ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨CLRS-30åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æé«˜çš„å›¾å‡†ç¡®ç‡ï¼Œå…¶æ€§èƒ½åœ¨NP-hardé—®é¢˜ä¸Šè¾¾åˆ°æˆ–è¶…è¿‡äº†ç‰¹å®šçš„NARæ¨¡å‹ã€‚æœ€æ˜¾è‘—çš„çªç ´åœ¨äºï¼ŒGNARLå³ä½¿åœ¨æ²¡æœ‰ä¸“å®¶ç®—æ³•ä½œä¸ºå‚è€ƒçš„æƒ…å†µä¸‹ä¾ç„¶å±•ç°å‡ºå¼ºå¤§çš„é€‚ç”¨æ€§ï¼Œæå¤§åœ°æ‹“å±•äº†ç¥ç»ç®—æ³•æ¨ç†çš„åº”ç”¨è¾¹ç•Œã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18930v1",
      "published_date": "2025-09-23 12:49:25 UTC",
      "updated_date": "2025-09-23 12:49:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:36:15.482947+00:00"
    },
    {
      "arxiv_id": "2509.18917v1",
      "title": "LiDAR Point Cloud Image-based Generation Using Denoising Diffusion Probabilistic Models",
      "title_zh": "åŸºäºå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹çš„ LiDAR ç‚¹äº‘å›¾åƒç”Ÿæˆ",
      "authors": [
        "Amirhesam Aghanouri",
        "Cristina Olaverri-Monreal"
      ],
      "abstract": "Autonomous vehicles (AVs) are expected to revolutionize transportation by improving efficiency and safety. Their success relies on 3D vision systems that effectively sense the environment and detect traffic agents. Among sensors AVs use to create a comprehensive view of surroundings, LiDAR provides high-resolution depth data enabling accurate object detection, safe navigation, and collision avoidance. However, collecting real-world LiDAR data is time-consuming and often affected by noise and sparsity due to adverse weather or sensor limitations. This work applies a denoising diffusion probabilistic model (DDPM), enhanced with novel noise scheduling and time-step embedding techniques to generate high-quality synthetic data for augmentation, thereby improving performance across a range of computer vision tasks, particularly in AV perception. These modifications impact the denoising process and the model's temporal awareness, allowing it to produce more realistic point clouds based on the projection. The proposed method was extensively evaluated under various configurations using the IAMCV and KITTI-360 datasets, with four performance metrics compared against state-of-the-art (SOTA) methods. The results demonstrate the model's superior performance over most existing baselines and its effectiveness in mitigating the effects of noisy and sparse LiDAR data, producing diverse point clouds with rich spatial relationships and structural detail.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨é©¾é©¶(Autonomous vehicles)é¢†åŸŸä¸­LiDARæ•°æ®è·å–æˆæœ¬é«˜ã€å—å™ªå£°å½±å“åŠæ•°æ®ç¨€ç–ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹(Denoising Diffusion Probabilistic Models, DDPM)çš„LiDARç‚¹äº‘å›¾åƒç”Ÿæˆæ–¹æ³•ã€‚é€šè¿‡å¼•å…¥åˆ›æ–°çš„å™ªå£°è°ƒåº¦(noise scheduling)å’Œæ—¶é—´æ­¥åµŒå…¥(time-step embedding)æŠ€æœ¯ï¼Œè¯¥æ¨¡å‹æ˜¾è‘—å¢å¼ºäº†å»å™ªè¿‡ç¨‹ä¸­çš„æ—¶é—´æ„ŸçŸ¥èƒ½åŠ›ï¼Œä»è€Œèƒ½å¤Ÿæ ¹æ®æŠ•å½±ç”Ÿæˆæ›´ä¸ºé€¼çœŸçš„åˆæˆç‚¹äº‘æ•°æ®ã€‚ç ”ç©¶åˆ©ç”¨IAMCVå’ŒKITTI-360æ•°æ®é›†è¿›è¡Œå®éªŒï¼Œå¹¶é‡‡ç”¨å››é¡¹å…³é”®æ€§èƒ½æŒ‡æ ‡ä¸ç°æœ‰çš„å…ˆè¿›æ–¹æ³•(SOTA)è¿›è¡Œå¯¹æ¯”ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç”Ÿæˆå…·æœ‰ä¸°å¯Œç©ºé—´å…³ç³»å’Œç»“æ„ç»†èŠ‚çš„å¤šæ ·åŒ–ç‚¹äº‘æ–¹é¢è¡¨ç°ä¼˜äºå¤§å¤šæ•°åŸºçº¿æ¨¡å‹ã€‚è¯¥æ¨¡å‹ä¸ä»…æœ‰æ•ˆç¼“è§£äº†çœŸå®ä¸–ç•ŒLiDARæ•°æ®çš„ç¨€ç–ä¸å™ªå£°é—®é¢˜ï¼Œè¿˜ä¸ºè‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥ä»»åŠ¡ä¸­çš„æ•°æ®å¢å¼ºæä¾›äº†é«˜è´¨é‡çš„åˆæˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18917v1",
      "published_date": "2025-09-23 12:35:07 UTC",
      "updated_date": "2025-09-23 12:35:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:36:21.999048+00:00"
    },
    {
      "arxiv_id": "2509.18905v2",
      "title": "How Far are VLMs from Visual Spatial Intelligence? A Benchmark-Driven Perspective",
      "title_zh": "VLM è·ç¦»è§†è§‰ç©ºé—´æ™ºèƒ½è¿˜æœ‰å¤šè¿œï¼ŸåŸºäºåŸºå‡†é©±åŠ¨çš„è§†è§’",
      "authors": [
        "Songsong Yu",
        "Yuxin Chen",
        "Hao Ju",
        "Lianjie Jia",
        "Fuxi Zhang",
        "Shaofei Huang",
        "Yuhan Wu",
        "Rundi Cui",
        "Binghao Ran",
        "Zaibin Zhang",
        "Zhedong Zheng",
        "Zhipeng Zhang",
        "Yifan Wang",
        "Lin Song",
        "Lijun Wang",
        "Yanwei Li",
        "Ying Shan",
        "Huchuan Lu"
      ],
      "abstract": "Visual Spatial Reasoning (VSR) is a core human cognitive ability and a critical requirement for advancing embodied intelligence and autonomous systems. Despite recent progress in Vision-Language Models (VLMs), achieving human-level VSR remains highly challenging due to the complexity of representing and reasoning over three-dimensional space. In this paper, we present a systematic investigation of VSR in VLMs, encompassing a review of existing methodologies across input modalities, model architectures, training strategies, and reasoning mechanisms. Furthermore, we categorize spatial intelligence into three levels of capability, ie, basic perception, spatial understanding, spatial planning, and curate SIBench, a spatial intelligence benchmark encompassing nearly 20 open-source datasets across 23 task settings. Experiments with state-of-the-art VLMs reveal a pronounced gap between perception and reasoning, as models show competence in basic perceptual tasks but consistently underperform in understanding and planning tasks, particularly in numerical estimation, multi-view reasoning, temporal dynamics, and spatial imagination. These findings underscore the substantial challenges that remain in achieving spatial intelligence, while providing both a systematic roadmap and a comprehensive benchmark to drive future research in the field. The related resources of this study are accessible at https://sibench.github.io/Awesome-Visual-Spatial-Reasoning/.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)çš„è§†è§‰ç©ºé—´æ¨ç†(Visual Spatial Reasoning, VSR)èƒ½åŠ›è¿›è¡Œäº†ç³»ç»Ÿæ€§è°ƒæŸ¥ï¼Œæ—¨åœ¨æ¢è®¨å…¶ä¸äººç±»æ°´å¹³ç©ºé—´æ™ºèƒ½ä¹‹é—´çš„å·®è·ã€‚ä½œè€…å°†ç©ºé—´æ™ºèƒ½èƒ½åŠ›åˆ’åˆ†ä¸ºåŸºç¡€æ„ŸçŸ¥(basic perception)ã€ç©ºé—´ç†è§£(spatial understanding)å’Œç©ºé—´è§„åˆ’(spatial planning)ä¸‰ä¸ªå±‚çº§ï¼Œå¹¶æ¨å‡ºäº†æ¶µç›–è¿‘20ä¸ªå¼€æºæ•°æ®é›†å’Œ23ä¸ªä»»åŠ¡è®¾ç½®çš„ç»¼åˆåŸºå‡†æµ‹è¯•SIBenchã€‚é€šè¿‡å¯¹å½“å‰æœ€å…ˆè¿›VLMsçš„å®éªŒè¯„ä¼°å‘ç°ï¼Œæ¨¡å‹åœ¨åŸºç¡€æ„ŸçŸ¥ä»»åŠ¡ä¸Šå…·æœ‰ç«äº‰åŠ›ï¼Œä½†åœ¨æ¶‰åŠæ•°å€¼ä¼°è®¡(numerical estimation)ã€å¤šè§†è§’æ¨ç†(multi-view reasoning)ã€æ—¶é—´åŠ¨æ€å’Œç©ºé—´æƒ³è±¡(spatial imagination)ç­‰ç†è§£ä¸è§„åˆ’ä»»åŠ¡æ—¶è¡¨ç°æŒç»­æ¬ ä½³ã€‚å®éªŒç»“æœæ­ç¤ºäº†æ¨¡å‹åœ¨æ„ŸçŸ¥ä¸æ¨ç†èƒ½åŠ›ä¹‹é—´å­˜åœ¨æ˜¾è‘—é¸¿æ²Ÿï¼Œå¼ºè°ƒäº†å®ç°çœŸæ­£ç©ºé—´æ™ºèƒ½æ‰€é¢ä¸´çš„å·¨å¤§æŒ‘æˆ˜ã€‚è¯¥é¡¹å·¥ä½œé€šè¿‡æä¾›ç³»ç»Ÿæ€§çš„è·¯çº¿å›¾å’Œå…¨é¢çš„åŸºå‡†å·¥å…·ï¼Œä¸ºæ¨åŠ¨å…·èº«æ™ºèƒ½å’Œè‡ªä¸»ç³»ç»Ÿé¢†åŸŸçš„æœªæ¥ç ”ç©¶å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "a comprehensive visual spatial reasoning evaluation tool, 25 pages, 16 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.18905v2",
      "published_date": "2025-09-23 12:00:14 UTC",
      "updated_date": "2025-11-11 12:18:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:36:23.469668+00:00"
    },
    {
      "arxiv_id": "2509.21379v2",
      "title": "SAEmnesia: Erasing Concepts in Diffusion Models with Supervised Sparse Autoencoders",
      "title_zh": "SAEmnesiaï¼šåŸºäºæœ‰ç›‘ç£ç¨€ç–è‡ªç¼–ç å™¨çš„æ‰©æ•£æ¨¡å‹æ¦‚å¿µæ“¦é™¤",
      "authors": [
        "Enrico Cassano",
        "Riccardo Renzulli",
        "Marco Nurisso",
        "Mirko Zaffaroni",
        "Alan Perotti",
        "Marco Grangetto"
      ],
      "abstract": "Concept unlearning in diffusion models is hampered by feature splitting, where concepts are distributed across many latent features, making their removal challenging and computationally expensive. We introduce SAEmnesia, a supervised sparse autoencoder framework that overcomes this by enforcing one-to-one concept-neuron mappings. By systematically labeling concepts during training, our method achieves feature centralization, binding each concept to a single, interpretable neuron. This enables highly targeted and efficient concept erasure. SAEmnesia reduces hyperparameter search by 96.7% and achieves a 9.2% improvement over the state-of-the-art on the UnlearnCanvas benchmark. Our method also demonstrates superior scalability in sequential unlearning, improving accuracy by 28.4% when removing nine objects, establishing a new standard for precise and controllable concept erasure. Moreover, SAEmnesia mitigates the possibility of generating unwanted content under adversarial attack and effectively removes nudity when evaluated with I2P.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰©æ•£æ¨¡å‹(Diffusion Models)ä¸­æ¦‚å¿µæ“¦é™¤(Concept Unlearning)å› ç‰¹å¾åˆ†è£‚(Feature Splitting)è€Œé¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†SAEmnesiaæ¡†æ¶ã€‚SAEmnesiaåˆ©ç”¨ç›‘ç£ç¨€ç–è‡ªç¼–ç å™¨(Supervised Sparse Autoencoders)é€šè¿‡åœ¨è®­ç»ƒæœŸé—´ç³»ç»Ÿåœ°æ ‡è®°æ¦‚å¿µï¼Œå®ç°äº†ç‰¹å¾é›†ä¸­åŒ–(Feature Centralization)ï¼Œä»è€Œåœ¨æ¦‚å¿µä¸ç¥ç»å…ƒä¹‹é—´å»ºç«‹äº†ä¸€å¯¹ä¸€çš„æ˜ å°„å…³ç³»ã€‚è¿™ç§æ–¹æ³•ä½¿å¾—ç›®æ ‡æ¦‚å¿µçš„æ“¦é™¤æ›´åŠ ç²¾å‡†é«˜æ•ˆï¼Œå°†è¶…å‚æ•°æœç´¢é‡å‡å°‘äº†96.7%ï¼Œå¹¶åœ¨UnlearnCanvasåŸºå‡†æµ‹è¯•ä¸­æ¯”ç°æœ‰æœ€ä¼˜æŠ€æœ¯æå‡äº†9.2%ã€‚åœ¨è¿ç»­æ“¦é™¤(Sequential Unlearning)ä»»åŠ¡ä¸­ï¼Œç§»é™¤ä¹ä¸ªç‰©ä½“æ—¶çš„å‡†ç¡®ç‡æé«˜äº†28.4%ï¼Œå±•ç°äº†å“è¶Šçš„å¯æ‰©å±•æ€§ã€‚æ­¤å¤–ï¼ŒSAEmnesiaè¿˜èƒ½æœ‰æ•ˆæŠµå¾¡å¯¹æŠ—æ€§æ”»å‡»å¹¶æˆåŠŸç§»é™¤è£¸éœ²å†…å®¹ï¼Œä¸ºç”Ÿæˆå¼æ¨¡å‹æä¾›äº†ä¸€ç§ç²¾ç¡®ä¸”å¯æ§çš„æ¦‚å¿µç§»é™¤æ–°æ ‡å‡†ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.21379v2",
      "published_date": "2025-09-23 11:29:30 UTC",
      "updated_date": "2025-11-28 12:53:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:36:29.370373+00:00"
    },
    {
      "arxiv_id": "2509.18900v1",
      "title": "The AI Literacy Heptagon: A Structured Approach to AI Literacy in Higher Education",
      "title_zh": "äººå·¥æ™ºèƒ½ç´ å…»ä¸ƒè¾¹å½¢ï¼šé«˜ç­‰æ•™è‚²ä¸­äººå·¥æ™ºèƒ½ç´ å…»åŸ¹å…»çš„ç»“æ„åŒ–è·¯å¾„",
      "authors": [
        "Veronika Hackl",
        "Alexandra Mueller",
        "Maximilian Sailer"
      ],
      "abstract": "The integrative literature review addresses the conceptualization and implementation of AI Literacy (AIL) in Higher Education (HE) by examining recent research literature. Through an analysis of publications (2021-2024), we explore (1) how AIL is defined and conceptualized in current research, particularly in HE, and how it can be delineated from related concepts such as Data Literacy, Media Literacy, and Computational Literacy; (2) how various definitions can be synthesized into a comprehensive working definition, and (3) how scientific insights can be effectively translated into educational practice. Our analysis identifies seven central dimensions of AIL: technical, applicational, critical thinking, ethical, social, integrational, and legal. These are synthesized in the AI Literacy Heptagon, deepening conceptual understanding and supporting the structured development of AIL in HE. The study aims to bridge the gap between theoretical AIL conceptualizations and the practical implementation in academic curricula.",
      "tldr_zh": "è¯¥é¡¹ç»¼åˆæ–‡çŒ®ç»¼è¿°ç ”ç©¶èšç„¦äºé«˜ç­‰æ•™è‚²ï¼ˆHigher Educationï¼‰ä¸­AI Literacy (AIL)çš„å®šä¹‰ä¸å®æ–½ï¼Œç³»ç»Ÿåˆ†æäº†2021è‡³2024å¹´é—´çš„æœ€æ–°å­¦æœ¯æ–‡çŒ®ã€‚ç ”ç©¶é¦–å…ˆæ¢è®¨äº†AILå¦‚ä½•ä¸Data Literacyã€Media LiteracyåŠComputational Literacyç­‰ç›¸å…³æ¦‚å¿µè¿›è¡ŒåŒºåˆ†ï¼Œå¹¶å°è¯•æ„å»ºä¸€ä¸ªç»¼åˆæ€§çš„å·¥ä½œå®šä¹‰ã€‚é€šè¿‡æ·±å…¥åˆ†æï¼Œæœ¬æ–‡è¯†åˆ«å‡ºAILçš„ä¸ƒä¸ªæ ¸å¿ƒç»´åº¦ï¼ŒåŒ…æ‹¬technicalã€applicationalã€critical thinkingã€ethicalã€socialã€integrationalä»¥åŠlegalã€‚è¿™äº›ç»´åº¦è¢«æ•´åˆå¹¶æç‚¼ä¸ºAI Literacy Heptagonæ¨¡å‹ï¼Œæ—¨åœ¨åŠ æ·±å¯¹AILçš„ç†è®ºè®¤çŸ¥å¹¶æ”¯æŒå…¶åœ¨é«˜ç­‰æ•™è‚²ä½“ç³»ä¸­çš„ç»“æ„åŒ–å‘å±•ã€‚è¯¥é¡¹ç ”ç©¶æˆæœæˆåŠŸå¼¥åˆäº†AILç†è®ºæ¡†æ¶ä¸å­¦æœ¯è¯¾ç¨‹å®é™…è½åœ°ä¹‹é—´çš„å·®è·ï¼Œä¸ºæ•™è‚²å·¥ä½œè€…å°†ç§‘å­¦è§è§£è½¬åŒ–ä¸ºæ•™å­¦å®è·µæä¾›äº†æ¸…æ™°çš„è·¯çº¿å›¾ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "4 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.18900v1",
      "published_date": "2025-09-23 11:28:30 UTC",
      "updated_date": "2025-09-23 11:28:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:36:30.090964+00:00"
    },
    {
      "arxiv_id": "2509.22708v1",
      "title": "GZSL-MoE: Apprentissage G{Ã©}n{Ã©}ralis{Ã©} Z{Ã©}ro-Shot bas{Ã©} sur le M{Ã©}lange d'Experts pour la Segmentation S{Ã©}mantique de Nuages de Points 3DAppliqu{Ã©} {Ã } un Jeu de Donn{Ã©}es d'Environnement de Collaboration Humain-Robot",
      "title_zh": "GZSL-MoEï¼šåŸºäºæ··åˆä¸“å®¶æ¨¡å‹çš„äººæœºåä½œç¯å¢ƒ 3D ç‚¹äº‘è¯­ä¹‰åˆ†å‰²å¹¿ä¹‰é›¶æ ·æœ¬å­¦ä¹ ",
      "authors": [
        "Ahed Alboody"
      ],
      "abstract": "Generative Zero-Shot Learning approach (GZSL) has demonstrated significant potential in 3D point cloud semantic segmentation tasks. GZSL leverages generative models like GANs or VAEs to synthesize realistic features (real features) of unseen classes. This allows the model to label unseen classes during testing, despite being trained only on seen classes. In this context, we introduce the Generalized Zero-Shot Learning based-upon Mixture-of-Experts (GZSL-MoE) model. This model incorporates Mixture-of-Experts layers (MoE) to generate fake features that closely resemble real features extracted using a pre-trained KPConv (Kernel Point Convolution) model on seen classes. The main contribution of this paper is the integration of Mixture-of-Experts into the Generator and Discriminator components of the Generative Zero-Shot Learning model for 3D point cloud semantic segmentation, applied to the COVERED dataset (CollabOratiVE Robot Environment Dataset) for Human-Robot Collaboration (HRC) environments. By combining the Generative Zero-Shot Learning model with Mixture-of- Experts, GZSL-MoE for 3D point cloud semantic segmentation provides a promising solution for understanding complex 3D environments, especially when comprehensive training data for all object classes is unavailable. The performance evaluation of the GZSL-MoE model highlights its ability to enhance performance on both seen and unseen classes. Keywords Generalized Zero-Shot Learning (GZSL), 3D Point Cloud, 3D Semantic Segmentation, Human-Robot Collaboration, COVERED (CollabOratiVE Robot Environment Dataset), KPConv, Mixture-of Experts",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† GZSL-MoE æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ 3D Point Cloud è¯­ä¹‰åˆ†å‰²ä¸­è®­ç»ƒæ•°æ®ä¸è¶³çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹äººæœºåä½œ (Human-Robot Collaboration) ç¯å¢ƒä¸‹çš„åº”ç”¨ã€‚è¯¥æ¨¡å‹åŸºäº Generalized Zero-Shot Learning (GZSL) æ¡†æ¶ï¼Œåˆ©ç”¨ç”Ÿæˆæ¨¡å‹ä¸ºæœªè§ç±»åˆ«åˆæˆé€¼çœŸç‰¹å¾ï¼Œä»è€Œä½¿æ¨¡å‹å…·å¤‡è¯†åˆ«è®­ç»ƒé˜¶æ®µæœªæ¶‰åŠç±»åˆ«çš„èƒ½åŠ›ã€‚ç ”ç©¶çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°† Mixture-of-Experts (MoE) å±‚é›†æˆåˆ°ç”Ÿæˆå™¨ (Generator) å’Œåˆ¤åˆ«å™¨ (Discriminator) ç»„ä»¶ä¸­ï¼Œä»¥ç”Ÿæˆä¸é¢„è®­ç»ƒ KPConv æ¨¡å‹æå–çš„çœŸå®ç‰¹å¾é«˜åº¦ç›¸ä¼¼çš„ä¼ªç‰¹å¾ã€‚è¯¥æ–¹æ³•åœ¨ COVERED æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ï¼Œç»“æœè¡¨æ˜ GZSL-MoE èƒ½å¤Ÿæœ‰æ•ˆæå‡æ¨¡å‹åœ¨å·²è§å’Œæœªè§ç±»åˆ«ä¸Šçš„æ€§èƒ½è¡¨ç°ã€‚è¿™ä¸ºåœ¨ç¼ºä¹å…¨é¢æ ‡æ³¨æ•°æ®çš„å¤æ‚ 3D ç¯å¢ƒä¸­è¿›è¡Œç²¾å‡†è¯­ä¹‰ç†è§£æä¾›äº†ä¸€ç§æå…·å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "in French language. 28e Conf{Ã©}rence Nationale en Intelligence Artificielle. Plate-Forme Intelligence Artificielle 2025, Association Fran{\\c c}aise pour l'Intelligence Artificielle, https://pfia2025.u-bourgogne.fr/, Jun 2025, Dijon, France",
      "pdf_url": "https://arxiv.org/pdf/2509.22708v1",
      "published_date": "2025-09-23 11:01:45 UTC",
      "updated_date": "2025-09-23 11:01:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:36:33.790089+00:00"
    },
    {
      "arxiv_id": "2509.18883v2",
      "title": "Introducing LongCat-Flash-Thinking: A Technical Report",
      "title_zh": "LongCat-Flash-Thinkingï¼šæŠ€æœ¯æŠ¥å‘Š",
      "authors": [
        "Meituan LongCat Team",
        "Anchun Gui",
        "Bei Li",
        "Bingyang Tao",
        "Bole Zhou",
        "Borun Chen",
        "Chao Zhang",
        "Chao Zhang",
        "Chengcheng Han",
        "Chenhui Yang",
        "Chi Zhang",
        "Chong Peng",
        "Chuyu Zhang",
        "Cong Chen",
        "Fengcun Li",
        "Gang Xu",
        "Guoyuan Lin",
        "Hao Jiang",
        "Hao Liang",
        "Haomin Fu",
        "Haoxiang Ma",
        "Hong Liu",
        "Hongyan Hao",
        "Hongyin Tang",
        "Hongyu Zang",
        "Hongzhi Ni",
        "Hui Su",
        "Jiahao Liu",
        "Jiahuan Li",
        "Jialin Liu",
        "Jianfei Zhang",
        "Jianhao Xu",
        "Jianing Wang",
        "Jiaqi Sun",
        "Jiaqi Zhang",
        "Jiarong Shi",
        "Jiawei Yang",
        "Jingang Wang",
        "Jinrui Ding",
        "Jun Kuang",
        "Jun Xu",
        "Ke He",
        "Kefeng Zhang",
        "Keheng Wang",
        "Keqing He",
        "Li Wei",
        "Liang Shi",
        "Lin Qiu",
        "Lingbin Kong",
        "Lingchuan Liu",
        "Linsen Guo",
        "Longfei An",
        "Mai Xia",
        "Meng Zhou",
        "Mengshen Zhu",
        "Peng Pei",
        "Pengcheng Jia",
        "Qi Gu",
        "Qi Guo",
        "Qiong Huang",
        "Quan Chen",
        "Quanchi Weng",
        "Rongxiang Weng",
        "Ruichen Shao",
        "Rumei Li",
        "Shanglin Lei",
        "Shuai Du",
        "Shuaikang Liu",
        "Shuang Zhou",
        "Shuhao Hu",
        "Siyu Xu",
        "Songshan Gong",
        "Tao Liang",
        "Tianhao Hu",
        "Wei He",
        "Wei Shi",
        "Wei Wang",
        "Wei Wu",
        "Wei Zhuo",
        "Weifeng Tang",
        "Wenjie Shi",
        "Wenlong Zhu",
        "Xi Su",
        "Xiangcheng Liu",
        "Xiangyu Xi",
        "Xiangzhou Huang",
        "Xiao Liu",
        "Xiaochen Jiang",
        "Xiaowei Shi",
        "Xiaowen Shi",
        "Xiaoyu Li",
        "Xin Chen",
        "Xinyue Zhao",
        "Xuan Huang",
        "Xuemiao Zhang",
        "Xuezhi Cao",
        "Xunliang Cai",
        "Yajie Zhang",
        "Yang Chen",
        "Yang Liu",
        "Yang Liu",
        "Yang Zheng",
        "Yaoming Wang",
        "Yaqi Huo",
        "Yerui Sun",
        "Yifan Lu",
        "Yiyang Li",
        "Youshao Xiao",
        "Yuanzhe Lei",
        "Yuchen Xie",
        "Yueqing Sun",
        "Yufei Zhang",
        "Yuhuai Wei",
        "Yulei Qian",
        "Yunke Zhao",
        "Yuqing Ding",
        "Yuwei Jiang",
        "Zhaohua Yang",
        "Zhengyu Chen",
        "Zhijian Liu",
        "Zhikang Xia",
        "Zhongda Su",
        "Ziran Li",
        "Ziwen Wang",
        "Ziyuan Zhuang",
        "Zongyu Wang",
        "Zunyuan Yang"
      ],
      "abstract": "We present LongCat-Flash-Thinking, an efficient 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model. Its advanced capabilities are cultivated through a meticulously crafted training process, beginning with long Chain-of-Thought (CoT) data cold-start and culminating in large-scale Reinforcement Learning (RL). We first employ a well-designed cold-start training strategy, which significantly enhances the reasoning potential and equips the model with specialized skills in both formal and agentic reasoning. Then, a core innovation is our domain-parallel training scheme, which decouples optimization across distinct domains (e.g., STEM, Code, Agentic) and subsequently fuses the resulting expert models into a single, nearly Pareto-optimal model. This entire process is powered by our Dynamic ORchestration for Asynchronous rollout (DORA) system, a large-scale RL framework that delivers a greater than threefold training speedup over synchronous methods on tens of thousands of accelerators. As a result, LongCat-Flash-Thinking achieves state-of-the-art performance among open-source models on a suite of complex reasoning tasks. The model exhibits exceptional efficiency in agentic reasoning, reducing average token consumption by 64.5% (from 19, 653 to 6, 965) on AIME-25, without degrading task accuracy. We release LongCat-Flash-Thinking to promote further advances in reasoning systems and agentic AI research.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†LongCat-Flash-Thinkingï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«5600äº¿å‚æ•°çš„å¼€æºæ··åˆä¸“å®¶(Mixture-of-Experts, MoE)æ¨ç†æ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„é•¿é“¾å¼æ€ç»´(Chain-of-Thought, CoT)æ•°æ®å†·å¯åŠ¨å’Œå¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ (Reinforcement Learning, RL)è¿›è¡Œè®­ç»ƒï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å½¢å¼åŒ–å’Œä»£ç†æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºé¢†åŸŸå¹¶è¡Œè®­ç»ƒæ–¹æ¡ˆ(domain-parallel training scheme)ï¼Œè¯¥æ–¹æ¡ˆè§£è€¦äº†STEMã€ä»£ç å’Œä»£ç†(Agentic)ç­‰ä¸åŒé¢†åŸŸçš„ä¼˜åŒ–ï¼Œå¹¶å°†ä¸“å®¶æ¨¡å‹èåˆä¸ºè¿‘ä¹å¸•ç´¯æ‰˜æœ€ä¼˜çš„å•ä¸€æ¨¡å‹ã€‚æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ç”±DORAç³»ç»Ÿ(Dynamic ORchestration for Asynchronous rollout)é©±åŠ¨ï¼Œä½œä¸ºä¸€ç§å¤§è§„æ¨¡å¼‚æ­¥å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå…¶åœ¨æ•°ä¸‡ä¸ªåŠ é€Ÿå™¨ä¸Šçš„è®­ç»ƒé€Ÿåº¦æ¯”åŒæ­¥æ–¹æ³•å¿«ä¸‰å€ä»¥ä¸Šã€‚LongCat-Flash-Thinkingåœ¨å¤šé¡¹å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¾¾åˆ°äº†å¼€æºæ¨¡å‹çš„å…ˆè¿›æ°´å¹³(State-of-the-Art)ã€‚è¯¥æ¨¡å‹åœ¨ä»£ç†æ¨ç†(agentic reasoning)ä¸­è¡¨ç°å‡ºæé«˜çš„æ•ˆç‡ï¼Œåœ¨AIME-25ä¸Šå°†å¹³å‡Tokenæ¶ˆè€—é™ä½äº†64.5%ä¸”æœªæŸå¤±å‡†ç¡®ç‡ã€‚è¯¥æ¨¡å‹çš„å¼€æºæ—¨åœ¨è¿›ä¸€æ­¥æ¨åŠ¨æ¨ç†ç³»ç»Ÿå’Œä»£ç†AI(Agentic AI)é¢†åŸŸçš„æŠ€æœ¯è¿›æ­¥ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18883v2",
      "published_date": "2025-09-23 10:25:48 UTC",
      "updated_date": "2025-11-07 11:10:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:36:51.990563+00:00"
    },
    {
      "arxiv_id": "2509.18880v2",
      "title": "Diversity Boosts AI-Generated Text Detection",
      "title_zh": "å¤šæ ·æ€§æå‡AIç”Ÿæˆæ–‡æœ¬æ£€æµ‹",
      "authors": [
        "Advik Raj Basani",
        "Pin-Yu Chen"
      ],
      "abstract": "Detecting AI-generated text is an increasing necessity to combat misuse of LLMs in education, business compliance, journalism, and social media, where synthetic fluency can mask misinformation or deception. While prior detectors often rely on token-level likelihoods or opaque black-box classifiers, these approaches struggle against high-quality generations and offer little interpretability. In this work, we propose DivEye, a novel detection framework that captures how unpredictability fluctuates across a text using surprisal-based features. Motivated by the observation that human-authored text exhibits richer variability in lexical and structural unpredictability than LLM outputs, DivEye captures this signal through a set of interpretable statistical features. Our method outperforms existing zero-shot detectors by up to 33.2% and achieves competitive performance with fine-tuned baselines across multiple benchmarks. DivEye is robust to paraphrasing and adversarial attacks, generalizes well across domains and models, and improves the performance of existing detectors by up to 18.7% when used as an auxiliary signal. Beyond detection, DivEye provides interpretable insights into why a text is flagged, pointing to rhythmic unpredictability as a powerful and underexplored signal for LLM detection.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DivEyeï¼Œä¸€ç§æ–°å‹çš„AIç”Ÿæˆæ–‡æœ¬æ£€æµ‹æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ•æ‰æ–‡æœ¬ä¸­ä¸å¯é¢„æµ‹æ€§çš„æ³¢åŠ¨æ¥åŒºåˆ†äººç±»ä½œè€…ä¸å¤§è¯­è¨€æ¨¡å‹(LLMs)ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒåŠ¨æœºåœ¨äºè§‚å¯Ÿåˆ°äººç±»åˆ›ä½œçš„æ–‡æœ¬åœ¨è¯æ±‡å’Œç»“æ„ä¸å¯é¢„æµ‹æ€§ä¸Šæ¯”LLMè¾“å‡ºå…·æœ‰æ›´ä¸°å¯Œçš„å¤šæ ·æ€§(Diversity)ï¼Œå¹¶æ®æ­¤è®¾è®¡äº†ä¸€ç³»åˆ—åŸºäºæƒŠå¥‡å€¼(surprisal-based)çš„å¯è§£é‡Šç»Ÿè®¡ç‰¹å¾ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDivEyeåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰çš„é›¶æ ·æœ¬(zero-shot)æ£€æµ‹å™¨ï¼Œæ€§èƒ½æå‡æœ€é«˜è¾¾33.2%ï¼ŒåŒæ—¶åœ¨é¢å¯¹æ”¹å†™å’Œå¯¹æŠ—æ€§æ”»å‡»æ—¶è¡¨ç°å‡ºæå¼ºçš„é²æ£’æ€§ã€‚å½“ä½œä¸ºè¾…åŠ©ä¿¡å·é›†æˆæ—¶ï¼Œè¯¥æ–¹æ³•èƒ½å°†ç°æœ‰æ£€æµ‹å™¨çš„æ€§èƒ½è¿›ä¸€æ­¥æé«˜18.7%ï¼Œå¹¶ä¸ºæ£€æµ‹ç»“æœæä¾›ç›´è§‚çš„å¯è§£é‡Šæ€§ã€‚è¯¥ç ”ç©¶æœ€åæŒ‡å‡ºï¼ŒèŠ‚å¥ä¸å¯é¢„æµ‹æ€§(rhythmic unpredictability)æ˜¯åŒºåˆ†äººç±»ä¸æœºå™¨ç”Ÿæˆæ–‡æœ¬çš„ä¸€ä¸ªå¼ºå¤§ä¸”å°šæœªè¢«å……åˆ†æŒ–æ˜çš„ä¿¡å·ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Project Webpage: https://diveye.vercel.app/",
      "pdf_url": "https://arxiv.org/pdf/2509.18880v2",
      "published_date": "2025-09-23 10:21:22 UTC",
      "updated_date": "2025-09-26 18:36:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:36:54.795526+00:00"
    },
    {
      "arxiv_id": "2509.18874v2",
      "title": "When Ads Become Profiles: Uncovering the Invisible Risk of Web Advertising at Scale with LLMs",
      "title_zh": "å½“å¹¿å‘Šæˆä¸ºç”»åƒï¼šåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹æ­ç¤ºå¤§è§„æ¨¡ç½‘ç»œå¹¿å‘Šä¸­çš„éšæ€§é£é™©",
      "authors": [
        "Baiyu Chen",
        "Benjamin Tag",
        "Hao Xue",
        "Daniel Angus",
        "Flora Salim"
      ],
      "abstract": "Regulatory limits on explicit targeting have not eliminated algorithmic profiling on the Web, as optimisation systems still adapt ad delivery to users' private attributes. The widespread availability of powerful zero-shot multimodal Large Language Models (LLMs) has dramatically lowered the barrier for exploiting these latent signals for adversarial inference. We investigate this emerging societal risk, specifically how adversaries can now exploit these signals to reverse-engineer private attributes from ad exposure alone. We introduce a novel pipeline that leverages LLMs as adversarial inference engines to perform natural language profiling. Applying this method to a longitudinal dataset comprising over 435,000 ad impressions collected from 891 users, we conducted a large-scale study to assess the feasibility and precision of inferring private attributes from passive online ad observations. Our results demonstrate that off-the-shelf LLMs can accurately reconstruct complex user private attributes, including party preference, employment status, and education level, consistently outperforming strong census-based priors and matching or exceeding human social perception, while operating at only a fraction of the cost (223$\\times$ lower) and time (52$\\times$ faster) required by humans. Critically, actionable profiling is feasible even within short observation windows, indicating that prolonged tracking is not a prerequisite for a successful attack. These findings provide the first empirical evidence that ad streams serve as a high-fidelity digital footprint, enabling off-platform profiling that inherently bypasses current platform safeguards, highlighting a systemic vulnerability in the ad ecosystem and the urgent need for responsible web AI governance in the generative AI era. The code is available at https://github.com/Breezelled/when-ads-become-profiles.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)ä»åœ¨çº¿å¹¿å‘Šæš´éœ²ä¸­æ¨æ–­ç”¨æˆ·éšç§å±æ€§çš„é£é™©ï¼Œå¹¶æå‡ºäº†ä¸€ç§å°†LLMsä½œä¸ºå¯¹æŠ—æ€§æ¨ç†å¼•æ“(adversarial inference engines)è¿›è¡Œè‡ªç„¶è¯­è¨€ç”»åƒ(natural language profiling)çš„åˆ›æ–°æµç¨‹ã€‚é€šè¿‡å¯¹åŒ…å«891åç”¨æˆ·çš„43.5ä¸‡ä½™æ¡å¹¿å‘Šå±•ç¤ºæ•°æ®è¿›è¡Œå¤§è§„æ¨¡ç ”ç©¶ï¼Œç ”ç©¶äººå‘˜è¯æ˜äº†ç°æˆçš„LLMsèƒ½å¤Ÿå‡†ç¡®é‡å»ºåŒ…æ‹¬æ”¿æ²»å€¾å‘(party preference)ã€å°±ä¸šçŠ¶æ€å’Œæ•™è‚²æ°´å¹³åœ¨å†…çš„å¤æ‚éšç§å±æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•çš„å‡†ç¡®æ€§ä¼˜äºåŸºäºäººå£æ™®æŸ¥çš„å…ˆéªŒæ¨¡å‹ï¼Œå¹¶è¾¾åˆ°äº†äººç±»ç¤¾äº¤æ„ŸçŸ¥æ°´å¹³ï¼Œè€Œæˆæœ¬ä»…ä¸ºäººç±»çš„1/223ï¼Œé€Ÿåº¦å¿«52å€ã€‚ç ”ç©¶å‘ç°ï¼Œå³ä½¿åœ¨æçŸ­çš„è§‚å¯Ÿçª—å£å†…ï¼Œæœ‰æ•ˆçš„å±æ€§æ¨æ–­ä¾ç„¶å¯è¡Œï¼Œè¿™è¡¨æ˜é•¿æœŸè¿½è¸ªå¹¶éæˆåŠŸæ”»å‡»çš„å…ˆå†³æ¡ä»¶ã€‚è¿™ä¸€å‘ç°æä¾›äº†é¦–ä¸ªç»éªŒè¯æ®ï¼Œè¯æ˜å¹¿å‘Šæµå·²æˆä¸ºèƒ½å¤Ÿç»•è¿‡ç°æœ‰å¹³å°é˜²å¾¡çš„é«˜ä¿çœŸæ•°å­—è¶³è¿¹ï¼Œæ­ç¤ºäº†å¹¿å‘Šç”Ÿæ€ç³»ç»Ÿåœ¨ç”Ÿæˆå¼AIæ—¶ä»£çš„ç³»ç»Ÿæ€§æ¼æ´ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CR",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18874v2",
      "published_date": "2025-09-23 10:10:37 UTC",
      "updated_date": "2025-12-04 07:26:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:37:08.295217+00:00"
    },
    {
      "arxiv_id": "2509.18868v1",
      "title": "Memory in Large Language Models: Mechanisms, Evaluation and Evolution",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ä¸­çš„è®°å¿†ï¼šæœºåˆ¶ã€è¯„ä¼°ä¸æ¼”è¿›",
      "authors": [
        "Dianxing Zhang",
        "Wendong Li",
        "Kani Song",
        "Jiaye Lu",
        "Gang Li",
        "Liuchun Yang",
        "Sheng Li"
      ],
      "abstract": "Under a unified operational definition, we define LLM memory as a persistent state written during pretraining, finetuning, or inference that can later be addressed and that stably influences outputs. We propose a four-part taxonomy (parametric, contextual, external, procedural/episodic) and a memory quadruple (location, persistence, write/access path, controllability). We link mechanism, evaluation, and governance via the chain write -> read -> inhibit/update. To avoid distorted comparisons across heterogeneous setups, we adopt a three-setting protocol (parametric only, offline retrieval, online retrieval) that decouples capability from information availability on the same data and timeline. On this basis we build a layered evaluation: parametric (closed-book recall, edit differential, memorization/privacy), contextual (position curves and the mid-sequence drop), external (answer correctness vs snippet attribution/faithfulness), and procedural/episodic (cross-session consistency and timeline replay, E MARS+). The framework integrates temporal governance and leakage auditing (freshness hits, outdated answers, refusal slices) and uncertainty reporting via inter-rater agreement plus paired tests with multiple-comparison correction. For updating and forgetting, we present DMM Gov: coordinating DAPT/TAPT, PEFT, model editing (ROME, MEND, MEMIT, SERAC), and RAG to form an auditable loop covering admission thresholds, rollout, monitoring, rollback, and change audits, with specs for timeliness, conflict handling, and long-horizon consistency. Finally, we give four testable propositions: minimum identifiability; a minimal evaluation card; causally constrained editing with verifiable forgetting; and when retrieval with small-window replay outperforms ultra-long-context reading. This yields a reproducible, comparable, and governable coordinate system for research and deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†å¤§è¯­è¨€æ¨¡å‹(LLMs)è®°å¿†çš„ç»Ÿä¸€æ“ä½œå®šä¹‰ï¼Œå¹¶å°†å…¶åˆ’åˆ†ä¸ºå‚æ•°åŒ–(Parametric)ã€ä¸Šä¸‹æ–‡(Contextual)ã€å¤–éƒ¨(External)ä»¥åŠç¨‹åº/æƒ…èŠ‚æ€§(Procedural/Episodic)å››ç§åˆ†ç±»ä½“ç³»ã€‚è®ºæ–‡æ„å»ºäº†åŒ…å«ä½ç½®ã€æŒä¹…æ€§ã€è¯»å†™è·¯å¾„å’Œå¯æ§æ€§çš„è®°å¿†å››å…ƒç»„(Memory Quadruple)ï¼Œé€šè¿‡â€œå†™å…¥-è¯»å–-æŠ‘åˆ¶/æ›´æ–°â€é“¾æ¡å°†è®°å¿†æœºåˆ¶ã€è¯„ä¼°ä¸æ²»ç†æœ‰æœºç»“åˆã€‚ä¸ºç¡®ä¿å…¬å¹³æ¯”è¾ƒï¼Œç ”ç©¶é‡‡ç”¨äº†ä¸‰è®¾ç½®åè®®(Three-setting Protocol)å¹¶è®¾è®¡äº†æ¶µç›–é—­å·å¬å›ã€ä½ç½®æ›²çº¿ã€å¿ å®åº¦åŠè·¨ä¼šè¯ä¸€è‡´æ€§çš„åˆ†å±‚è¯„ä¼°ä½“ç³»ã€‚é’ˆå¯¹è®°å¿†çš„æ›´æ–°ä¸é—å¿˜ï¼Œç ”ç©¶æå‡ºäº†DMM Govæ¡†æ¶ï¼Œé€šè¿‡åè°ƒPEFTã€æ¨¡å‹ç¼–è¾‘(ROME, MEND, MEMIT, SERAC)å’ŒRAGç­‰æŠ€æœ¯ï¼Œå½¢æˆäº†ä¸€ä¸ªæ¶µç›–å‡†å…¥é—¨æ§›ã€ç›‘æ§åŠå®¡è®¡çš„å¯éªŒè¯å¾ªç¯ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†å…³äºå› æœçº¦æŸç¼–è¾‘ã€å¯éªŒè¯é—å¿˜ä»¥åŠæ£€ç´¢ä¸è¶…é•¿ä¸Šä¸‹æ–‡æ€§èƒ½å¯¹æ¯”ç­‰å››é¡¹å¯æµ‹è¯•å‘½é¢˜ã€‚è¯¥æˆæœä¸ºLLMè®°å¿†çš„ç ”ç©¶ä¸éƒ¨ç½²æä¾›äº†ä¸€ä¸ªå¯é‡å¤ã€å¯æ¯”è¾ƒä¸”å¯æ²»ç†çš„åæ ‡ç³»ç»Ÿï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹è®°å¿†å¤„ç†çš„é€æ˜åº¦ä¸å¯æ§æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "50 pages, 1 figure, 8 tables This is a survey/framework paper on LLM memory mechanisms and evaluation",
      "pdf_url": "https://arxiv.org/pdf/2509.18868v1",
      "published_date": "2025-09-23 10:06:58 UTC",
      "updated_date": "2025-09-23 10:06:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:37:08.687547+00:00"
    },
    {
      "arxiv_id": "2509.18864v1",
      "title": "Conf-Profile: A Confidence-Driven Reasoning Paradigm for Label-Free User Profiling",
      "title_zh": "Conf-Profileï¼šç½®ä¿¡åº¦é©±åŠ¨çš„æ— æ ‡ç­¾ç”¨æˆ·ç”»åƒæ¨ç†èŒƒå¼",
      "authors": [
        "Yingxin Li",
        "Jianbo Zhao",
        "Xueyu Ren",
        "Jie Tang",
        "Wangjie You",
        "Xu Chen",
        "Kan Zhou",
        "Chao Feng",
        "Jiao Ran",
        "Yuan Meng",
        "Zhi Wang"
      ],
      "abstract": "User profiling, as a core technique for user understanding, aims to infer structural attributes from user information. Large Language Models (LLMs) provide a promising avenue for user profiling, yet the progress is hindered by the lack of comprehensive benchmarks. To bridge this gap, we propose ProfileBench, an industrial benchmark derived from a real-world video platform, encompassing heterogeneous user data and a well-structured profiling taxonomy. However, the profiling task remains challenging due to the difficulty of collecting large-scale ground-truth labels, and the heterogeneous and noisy user information can compromise the reliability of LLMs. To approach label-free and reliable user profiling, we propose a Confidence-driven Profile reasoning framework Conf-Profile, featuring a two-stage paradigm. We first synthesize high-quality labels by leveraging advanced LLMs with confidence hints, followed by confidence-weighted voting for accuracy improvement and confidence calibration for a balanced distribution. The multiple profile results, rationales, and confidence scores are aggregated and distilled into a lightweight LLM. We further enhance the reasoning ability via confidence-guided unsupervised reinforcement learning, which exploits confidence for difficulty filtering, quasi-ground truth voting, and reward weighting. Experimental results demonstrate that Conf-Profile delivers substantial performance through the two-stage training, improving F1 by 13.97 on Qwen3-8B.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Conf-Profileï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹æ— æ ‡ç­¾ç”¨æˆ·ç”»åƒ (Label-Free User Profiling) çš„ç½®ä¿¡åº¦é©±åŠ¨æ¨ç†èŒƒå¼ã€‚ä¸ºäº†è§£å†³è¯¥é¢†åŸŸç¼ºä¹å…¨é¢åŸºå‡†æµ‹è¯•å’Œå¤§è§„æ¨¡çœŸå€¼æ ‡ç­¾çš„æŒ‘æˆ˜ï¼Œç ”ç©¶è€…é¦–å…ˆæå‡ºäº†æºè‡ªçœŸå®è§†é¢‘å¹³å°çš„å·¥ä¸šçº§åŸºå‡† ProfileBenchã€‚Conf-Profile é‡‡ç”¨ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œç¬¬ä¸€é˜¶æ®µé€šè¿‡é«˜çº§ Large Language Models (LLMs) ç»“åˆç½®ä¿¡åº¦æç¤ºåˆæˆæ ‡ç­¾ï¼Œå¹¶åˆ©ç”¨ç½®ä¿¡åº¦åŠ æƒæŠ•ç¥¨ä¸æ ¡å‡†ç¡®ä¿å‡†ç¡®æ€§ã€‚ç¬¬äºŒé˜¶æ®µå°†æ¨ç†ç»“æœä¸ç½®ä¿¡åº¦åˆ†æ•°è’¸é¦è‡³è½»é‡çº§æ¨¡å‹ï¼Œå¹¶å¼•å…¥ç½®ä¿¡åº¦å¼•å¯¼çš„æ— ç›‘ç£å¼ºåŒ–å­¦ä¹  (Unsupervised Reinforcement Learning) è¿›ä¸€æ­¥å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚å®éªŒè¯æ˜ï¼ŒConf-Profile åœ¨ä¸¤é˜¶æ®µè®­ç»ƒåè¡¨ç°ä¼˜å¼‚ï¼Œåœ¨ Qwen3-8B æ¨¡å‹ä¸Šå°† F1 æŒ‡æ ‡æ˜¾è‘—æå‡äº† 13.97ï¼Œä¸ºå®ç°å¯é ä¸”ä½æˆæœ¬çš„ç”¨æˆ·ç†è§£æä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18864v1",
      "published_date": "2025-09-23 09:58:37 UTC",
      "updated_date": "2025-09-23 09:58:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:37:11.486748+00:00"
    },
    {
      "arxiv_id": "2510.01229v1",
      "title": "Enhancing Transformer-Based Rerankers with Synthetic Data and LLM-Based Supervision",
      "title_zh": "åˆ©ç”¨åˆæˆæ•°æ®ä¸åŸºäº LLM çš„ç›‘ç£å¢å¼ºåŸºäº Transformer çš„é‡æ’åºå™¨",
      "authors": [
        "Dimitar Peshevski",
        "Kiril Blazhevski",
        "Martin Popovski",
        "Gjorgji Madjarov"
      ],
      "abstract": "Effective document reranking is essential for improving search relevance across diverse applications. While Large Language Models (LLMs) excel at reranking due to their deep semantic understanding and reasoning, their high computational cost makes them impractical for many real-world deployments. Fine-tuning smaller, task-specific models is a more efficient alternative but typically depends on scarce, manually labeled data. To overcome this, we propose a novel pipeline that eliminates the need for human-labeled query-document pairs. Our method uses LLMs to generate synthetic queries from domain-specific corpora and employs an LLM-based classifier to label positive and hard-negative pairs. This synthetic dataset is then used to fine-tune a smaller transformer model with contrastive learning using Localized Contrastive Estimation (LCE) loss. Experiments on the MedQuAD dataset show that our approach significantly boosts in-domain performance and generalizes well to out-of-domain tasks. By using LLMs for data generation and supervision rather than inference, we reduce computational costs while maintaining strong reranking capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨åˆæˆæ•°æ®å’Œ LLM ç›‘ç£æ¥å¢å¼ºåŸºäº Transformer çš„é‡æ’åºæ¨¡å‹ï¼ˆrerankersï¼‰çš„æ–°å‹æµæ°´çº¿ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ–‡æ¡£é‡æ’åºä»»åŠ¡ä¸­è®¡ç®—æˆæœ¬è¿‡é«˜çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ LLMs ä»ç‰¹å®šé¢†åŸŸè¯­æ–™åº“ä¸­ç”ŸæˆåˆæˆæŸ¥è¯¢ï¼Œå¹¶é‡‡ç”¨åŸºäº LLM çš„åˆ†ç±»å™¨æ ‡æ³¨æ­£æ ·æœ¬å’Œéš¾è´Ÿæ ·æœ¬ï¼ˆhard-negative pairsï¼‰ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹äººå·¥æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚é€šè¿‡å±€éƒ¨å¯¹æ¯”ä¼°è®¡ï¼ˆLocalized Contrastive Estimation, LCEï¼‰æŸå¤±å’Œå¯¹æ¯”å­¦ä¹ ï¼ˆcontrastive learningï¼‰ï¼Œç ”ç©¶è€…å¯¹è¾ƒå°çš„ Transformer æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒã€‚åœ¨ MedQuAD æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†æ¨¡å‹çš„åŸŸå†…æ€§èƒ½ï¼Œå¹¶è¡¨ç°å‡ºè‰¯å¥½çš„è·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›ã€‚è¯¥ç ”ç©¶è¯æ˜ï¼Œé€šè¿‡å°† LLMs ç”¨äºæ•°æ®ç”Ÿæˆå’Œç›‘ç£è€Œéç›´æ¥æ¨ç†ï¼Œå¯ä»¥åœ¨å¤§å¹…é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ï¼Œä¿æŒå¼ºå¤§çš„é‡æ’åºèƒ½åŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by RANLP 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.01229v1",
      "published_date": "2025-09-23 09:47:27 UTC",
      "updated_date": "2025-09-23 09:47:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:37:17.585932+00:00"
    },
    {
      "arxiv_id": "2509.18851v1",
      "title": "NGRPO: Negative-enhanced Group Relative Policy Optimization",
      "title_zh": "NGRPOï¼šè´Ÿå‘å¢å¼ºçš„ç»„å†…ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–",
      "authors": [
        "Gongrui Nan",
        "Siye Chen",
        "Jing Huang",
        "Mengyu Lu",
        "Dexun Wang",
        "Chunmei Xie",
        "Weiqi Xiong",
        "Xianzhou Zeng",
        "Qixuan Zhou",
        "Yadong Li",
        "Xingzhong Xu"
      ],
      "abstract": "RLVR has enhanced the reasoning capabilities of Large Language Models (LLMs) across various tasks. However, GRPO, a representative RLVR algorithm, suffers from a critical limitation: when all responses within a group are either entirely correct or entirely incorrect, the model fails to learn from these homogeneous responses. This is particularly problematic for homogeneously incorrect groups, where GRPO's advantage function yields a value of zero, leading to null gradients and the loss of valuable learning signals. To overcome this issue, we propose NGRPO (Negative-enhanced Group Relative Policy Optimization), an algorithm designed to convert homogeneous errors into robust learning signals. First, NGRPO introduces Advantage Calibration. This mechanism hypothesizes the existence of a virtual maximum-reward sample during advantage calculation, thereby altering the mean and variance of rewards within a group and ensuring that the advantages for homogeneously incorrect samples are no longer zero. Second, NGRPO employs Asymmetric Clipping, which relaxes the update magnitude for positive samples while imposing stricter constraints on that of negative samples. This serves to stabilize the exploration pressure introduced by the advantage calibration. Our experiments on Qwen2.5-Math-7B demonstrate that NGRPO significantly outperforms baselines such as PPO, GRPO, DAPO, and PSR-NSR on mathematical benchmarks including MATH500, AMC23, and AIME2025. These results validate NGRPO's ability to learn from homogeneous errors, leading to stable and substantial improvements in mathematical reasoning. Our code is available at https://github.com/nangongrui-ngr/NGRPO.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Group Relative Policy Optimization (GRPO) ç®—æ³•åœ¨å¤„ç†åŒè´¨åŒ–å“åº”ï¼ˆå¦‚å…¨é”™æ ·æœ¬ç»„ï¼‰æ—¶å› ä¼˜åŠ¿å‡½æ•°ä¸ºé›¶è€Œæ— æ³•å­¦ä¹ çš„å±€é™æ€§ï¼Œæå‡ºäº†Negative-enhanced Group Relative Policy Optimization (NGRPO) ç®—æ³•ã€‚NGRPOå¼•å…¥äº†Advantage Calibrationæœºåˆ¶ï¼Œé€šè¿‡å‡è®¾è™šæ‹Ÿæœ€å¤§å¥–åŠ±æ ·æœ¬æ¥é‡æ–°æ ¡å‡†å¥–åŠ±çš„å‡å€¼ä¸æ–¹å·®ï¼Œç¡®ä¿åŒè´¨åŒ–é”™è¯¯æ ·æœ¬ä¹Ÿèƒ½äº§ç”Ÿæœ‰æ•ˆçš„å­¦ä¹ ä¿¡å·ã€‚æ­¤å¤–ï¼Œç®—æ³•é‡‡ç”¨Asymmetric Clippingç­–ç•¥ï¼Œé€šè¿‡æ”¾å®½æ­£æ ·æœ¬æ›´æ–°å¹…åº¦å¹¶ä¸¥æ ¼çº¦æŸè´Ÿæ ·æœ¬æ›´æ–°ï¼Œä»¥æ­¤ç¨³å®šç”±æ ¡å‡†æœºåˆ¶å¸¦æ¥çš„æ¢ç´¢å‹åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºQwen2.5-Math-7Bæ¨¡å‹ï¼ŒNGRPOåœ¨MATH500ã€AMC23åŠAIME2025ç­‰æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æ˜¾è‘—ä¼˜äºPPOã€GRPOã€DAPOå’ŒPSR-NSRã€‚è¯¥ç ”ç©¶éªŒè¯äº†NGRPOä»åŒè´¨åŒ–é”™è¯¯ä¸­æå–æ¢¯åº¦ä¿¡å·çš„èƒ½åŠ›ï¼Œå®ç°äº†æ¨¡å‹æ•°å­¦æ¨ç†æ€§èƒ½çš„ç¨³å¥æå‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18851v1",
      "published_date": "2025-09-23 09:38:10 UTC",
      "updated_date": "2025-09-23 09:38:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:37:25.183924+00:00"
    },
    {
      "arxiv_id": "2509.18849v3",
      "title": "MAPO: Mixed Advantage Policy Optimization",
      "title_zh": "MAPOï¼šæ··åˆä¼˜åŠ¿ç­–ç•¥ä¼˜åŒ–",
      "authors": [
        "Wenke Huang",
        "Quan Zhang",
        "Yiyang Fang",
        "Jian Liang",
        "Xuankun Rong",
        "Huanjin Yao",
        "Guancheng Wan",
        "Ke Liang",
        "Wenwen He",
        "Mingjun Li",
        "Leszek Rutkowski",
        "Mang Ye",
        "Bo Du",
        "Dacheng Tao"
      ],
      "abstract": "Recent advances in reinforcement learning for foundation models, such as Group Relative Policy Optimization (GRPO), have significantly improved the performance of foundation models on reasoning tasks. Notably, the advantage function serves as a central mechanism in GRPO for ranking the trajectory importance. However, existing explorations encounter both advantage reversion and advantage mirror problems, which hinder the reasonable advantage allocation across different query samples. In this work, we propose an easy but effective GRPO strategy, Mixed Advantage Policy Optimization (MAPO). We reveal that the trajectory appears with different certainty and propose the advantage percent deviation for samples with high-certainty trajectories. Furthermore, we dynamically reweight the advantage function for samples with varying trajectory certainty, thereby adaptively configuring the advantage function to account for sample-specific characteristics. Comparison with related state-of-the-art methods, along with ablation studies on different advantage variants, validates the effectiveness of our approach.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MAPO (Mixed Advantage Policy Optimization)ï¼Œä¸€ç§æ—¨åœ¨ä¼˜åŒ–åŸºç¡€æ¨¡å‹å¼ºåŒ–å­¦ä¹ æ¨ç†è¡¨ç°çš„ç­–ç•¥ã€‚é’ˆå¯¹ GRPO (Group Relative Policy Optimization) ä¸­å­˜åœ¨çš„ advantage reversion å’Œ advantage mirror é—®é¢˜ï¼ŒMAPO é€šè¿‡æ”¹è¿›ä¼˜åŠ¿åˆ†é…æœºåˆ¶æå‡äº†è®­ç»ƒçš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶æ­ç¤ºäº†æ ·æœ¬è½¨è¿¹ï¼ˆtrajectoryï¼‰å…·æœ‰ä¸åŒçš„ç¡®å®šæ€§ï¼ˆcertaintyï¼‰ï¼Œå¹¶æ®æ­¤å¼•å…¥äº†ä¼˜åŠ¿ç™¾åˆ†æ¯”åå·®ï¼ˆadvantage percent deviationï¼‰æ¥å¤„ç†é«˜ç¡®å®šæ€§è½¨è¿¹ã€‚é€šè¿‡åŠ¨æ€é‡æƒåŒ–ä¼˜åŠ¿å‡½æ•°ï¼ˆadvantage functionï¼‰ï¼ŒMAPO èƒ½å¤Ÿæ ¹æ®æ ·æœ¬ç‰¹æ€§å®ç°è‡ªé€‚åº”é…ç½®ï¼Œä»è€Œç¡®ä¿è½¨è¿¹é‡è¦æ€§çš„å‡†ç¡®æ’åºã€‚å®éªŒå¯¹æ¯”åŠæ¶ˆèç ”ç©¶è¯æ˜ï¼ŒMAPO åœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰çš„ SOTA æ–¹æ³•ï¼Œä¸ºæå‡å¤§æ¨¡å‹çš„é€»è¾‘æ¨ç†èƒ½åŠ›æä¾›äº†æœ‰æ•ˆä¸”æ˜“äºå®ç°çš„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18849v3",
      "published_date": "2025-09-23 09:37:16 UTC",
      "updated_date": "2025-09-25 01:02:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:37:42.089512+00:00"
    },
    {
      "arxiv_id": "2509.18847v2",
      "title": "Failure Makes the Agent Stronger: Enhancing Accuracy through Structured Reflection for Reliable Tool Interactions",
      "title_zh": "å¤±è´¥ä½¿æ™ºèƒ½ä½“æ›´å¼ºå¤§ï¼šé€šè¿‡ç»“æ„åŒ–åæ€æå‡å·¥å…·äº¤äº’çš„å‡†ç¡®æ€§ä¸å¯é æ€§",
      "authors": [
        "Junhao Su",
        "Yuanliang Wan",
        "Junwei Yang",
        "Hengyu Shi",
        "Tianyang Han",
        "Junfeng Luo",
        "Yurui Qiu"
      ],
      "abstract": "Tool-augmented large language models (LLMs) are usually trained with supervised imitation or coarse-grained reinforcement learning that optimizes single tool calls. Current self-reflection practices rely on heuristic prompts or one-way reasoning: the model is urged to 'think more' instead of learning error diagnosis and repair. This is fragile in multi-turn interactions; after a failure the model often repeats the same mistake. We propose structured reflection, which turns the path from error to repair into an explicit, controllable, and trainable action. The agent produces a short yet precise reflection: it diagnoses the failure using evidence from the previous step and then proposes a correct, executable follow-up call. For training we combine DAPO and GSPO objectives with a reward scheme tailored to tool use, optimizing the stepwise strategy Reflect, then Call, then Final. To evaluate, we introduce Tool-Reflection-Bench, a lightweight benchmark that programmatically checks structural validity, executability, parameter correctness, and result consistency. Tasks are built as mini trajectories of erroneous call, reflection, and corrected call, with disjoint train and test splits. Experiments on BFCL v3 and Tool-Reflection-Bench show large gains in multi-turn tool-call success and error recovery, and a reduction of redundant calls. These results indicate that making reflection explicit and optimizing it directly improves the reliability of tool interaction and offers a reproducible path for agents to learn from failure.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å·¥å…·å¢å¼ºå‹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤±è´¥åå¾€å¾€é‡å¤é”™è¯¯ä¸”ç¼ºä¹æœ‰æ•ˆä¿®å¤æœºåˆ¶çš„é—®é¢˜ï¼Œæå‡ºäº†ç»“æ„åŒ–åæ€(Structured Reflection)æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†ä»é”™è¯¯åˆ°ä¿®å¤çš„è·¯å¾„è½¬åŒ–ä¸ºæ˜¾å¼ä¸”å¯è®­ç»ƒçš„åŠ¨ä½œï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿæ ¹æ®å‰ä¸€æ­¥çš„è¯æ®è¯Šæ–­å¤±è´¥å¹¶æå‡ºæ­£ç¡®çš„åç»­è°ƒç”¨ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œç ”ç©¶è€…ç»“åˆäº†DAPOå’ŒGSPOç›®æ ‡ï¼Œå¹¶åˆ©ç”¨ä¸“ä¸ºå·¥å…·ä½¿ç”¨è®¾è®¡çš„å¥–åŠ±æ–¹æ¡ˆæ¥ä¼˜åŒ–â€œåæ€-è°ƒç”¨-æœ€ç»ˆ(Reflect, then Call, then Final)â€çš„é€æ­¥ç­–ç•¥ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†Tool-Reflection-BenchåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨ç¨‹åºåŒ–åœ°è¯„ä¼°ç»“æ„æœ‰æ•ˆæ€§ã€å‚æ•°æ­£ç¡®æ€§åŠç»“æœä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨BFCL v3å’Œæ–°åŸºå‡†ä¸Šæ˜¾è‘—æå‡äº†å¤šè½®å·¥å…·è°ƒç”¨çš„æˆåŠŸç‡ä¸é”™è¯¯æ¢å¤èƒ½åŠ›ï¼Œå¹¶æœ‰æ•ˆå‡å°‘äº†å†—ä½™è°ƒç”¨ã€‚è¿™ä¸€æˆæœè¯æ˜äº†é€šè¿‡æ˜¾å¼åŒ–å¹¶ç›´æ¥ä¼˜åŒ–åæ€è¿‡ç¨‹ï¼Œå¯ä»¥å¤§å¹…æé«˜æ™ºèƒ½ä½“åœ¨å·¥å…·äº¤äº’ä¸­çš„å¯é æ€§ï¼Œå¹¶ä¸ºå…¶ä»å¤±è´¥ä¸­å­¦ä¹ æä¾›äº†å¯å¤ç°çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "27pages",
      "pdf_url": "https://arxiv.org/pdf/2509.18847v2",
      "published_date": "2025-09-23 09:35:49 UTC",
      "updated_date": "2025-09-25 14:17:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:37:34.393493+00:00"
    },
    {
      "arxiv_id": "2509.18846v1",
      "title": "Model selection meets clinical semantics: Optimizing ICD-10-CM prediction via LLM-as-Judge evaluation, redundancy-aware sampling, and section-aware fine-tuning",
      "title_zh": "æ¨¡å‹é€‰æ‹©ä¸ä¸´åºŠè¯­ä¹‰çš„èåˆï¼šåŸºäº LLM-as-Judge è¯„ä¼°ã€å†—ä½™æ„ŸçŸ¥é‡‡æ ·ä¸ç« èŠ‚æ„ŸçŸ¥å¾®è°ƒä¼˜åŒ– ICD-10-CM é¢„æµ‹",
      "authors": [
        "Hong-Jie Dai",
        "Zheng-Hao Li",
        "An-Tai Lu",
        "Bo-Tsz Shain",
        "Ming-Ta Li",
        "Tatheer Hussain Mir",
        "Kuang-Te Wang",
        "Min-I Su",
        "Pei-Kang Liu",
        "Ming-Ju Tsai"
      ],
      "abstract": "Accurate International Classification of Diseases (ICD) coding is critical for clinical documentation, billing, and healthcare analytics, yet it remains a labour-intensive and error-prone task. Although large language models (LLMs) show promise in automating ICD coding, their challenges in base model selection, input contextualization, and training data redundancy limit their effectiveness. We propose a modular framework for ICD-10 Clinical Modification (ICD-10-CM) code prediction that addresses these challenges through principled model selection, redundancy-aware data sampling, and structured input design. The framework integrates an LLM-as-judge evaluation protocol with Plackett-Luce aggregation to assess and rank open-source LLMs based on their intrinsic comprehension of ICD-10-CM code definitions. We introduced embedding-based similarity measures, a redundancy-aware sampling strategy to remove semantically duplicated discharge summaries. We leverage structured discharge summaries from Taiwanese hospitals to evaluate contextual effects and examine section-wise content inclusion under universal and section-specific modelling paradigms. Experiments across two institutional datasets demonstrate that the selected base model after fine-tuning consistently outperforms baseline LLMs in internal and external evaluations. Incorporating more clinical sections consistently improves prediction performance. This study uses open-source LLMs to establish a practical and principled approach to ICD-10-CM code prediction. The proposed framework provides a scalable, institution-ready solution for real-world deployment of automated medical coding systems by combining informed model selection, efficient data refinement, and context-aware prompting.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç”¨äºä¼˜åŒ–ICD-10-CMé¢„æµ‹çš„æ¨¡å—åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨åŸºç¡€æ¨¡å‹é€‰æ‹©ã€è¾“å…¥ä¸Šä¸‹æ–‡å¤„ç†åŠè®­ç»ƒæ•°æ®å†—ä½™æ–¹é¢çš„æŒ‘æˆ˜ã€‚æ¡†æ¶æ ¸å¿ƒæ•´åˆäº†LLM-as-judgeè¯„ä¼°åè®®ä¸Plackett-Luce aggregationï¼Œé€šè¿‡è¯„ä¼°å¼€æºLLMså¯¹ICD-10-CMå®šä¹‰çš„ç†è§£èƒ½åŠ›è¿›è¡Œç§‘å­¦æ’åä¸ç­›é€‰ã€‚åœ¨æ•°æ®ä¼˜åŒ–å±‚é¢ï¼Œç ”ç©¶å¼•å…¥äº†embedding-based similarity measureså’Œredundancy-aware samplingç­–ç•¥ï¼Œæœ‰æ•ˆå‰”é™¤äº†è¯­ä¹‰é‡å¤çš„å‡ºé™¢æ‘˜è¦ã€‚é€šè¿‡å¯¹ç»“æ„åŒ–ç—…å†è¿›è¡Œsection-aware fine-tuningï¼Œå®éªŒè¯æ˜çº³å…¥æ›´å¤šä¸´åºŠç« èŠ‚å†…å®¹èƒ½æŒç»­å¢å¼ºæ¨¡å‹çš„é¢„æµ‹æ€§èƒ½ã€‚åœ¨ä¸¤ä¸ªæœºæ„æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œé€‰å®šçš„å¾®è°ƒæ¨¡å‹åœ¨å†…éƒ¨åŠå¤–éƒ¨è¯„ä¼°ä¸­å‡æ˜¾è‘—ä¼˜äºåŸºçº¿LLMsã€‚è¯¥ç ”ç©¶ä¸ºç°å®åŒ»ç–—åœºæ™¯ä¸­çš„è‡ªåŠ¨åŒ–ç¼–ç éƒ¨ç½²æä¾›äº†ä¸€ä¸ªç»“åˆinformed model selectionã€é«˜æ•ˆæ•°æ®ç²¾ç‚¼ä¸context-aware promptingçš„å¯æ‰©å±•è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "28 Pages, 4 Figures, 2 Tables",
      "pdf_url": "https://arxiv.org/pdf/2509.18846v1",
      "published_date": "2025-09-23 09:35:05 UTC",
      "updated_date": "2025-09-23 09:35:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:37:40.794714+00:00"
    },
    {
      "arxiv_id": "2509.21377v1",
      "title": "Dynamic Multi-Target Fusion for Efficient Audio-Visual Navigation",
      "title_zh": "é¢å‘é«˜æ•ˆè§†å¬å¯¼èˆªçš„åŠ¨æ€å¤šç›®æ ‡èåˆ",
      "authors": [
        "Yinfeng Yu",
        "Hailong Zhang",
        "Meiling Zhu"
      ],
      "abstract": "Audiovisual embodied navigation enables robots to locate audio sources by dynamically integrating visual observations from onboard sensors with the auditory signals emitted by the target. The core challenge lies in effectively leveraging multimodal cues to guide navigation. While prior works have explored basic fusion of visual and audio data, they often overlook deeper perceptual context. To address this, we propose the Dynamic Multi-Target Fusion for Efficient Audio-Visual Navigation (DMTF-AVN). Our approach uses a multi-target architecture coupled with a refined Transformer mechanism to filter and selectively fuse cross-modal information. Extensive experiments on the Replica and Matterport3D datasets demonstrate that DMTF-AVN achieves state-of-the-art performance, outperforming existing methods in success rate (SR), path efficiency (SPL), and scene adaptation (SNA). Furthermore, the model exhibits strong scalability and generalizability, paving the way for advanced multimodal fusion strategies in robotic navigation. The code and videos are available at\n  https://github.com/zzzmmm-svg/DMTF.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†å¬ä½“ç°å¯¼èˆª (Audio-Visual Navigation) ä»»åŠ¡ä¸­å¦‚ä½•æœ‰æ•ˆæ•´åˆè§†è§‰è§‚æµ‹ä¸éŸ³é¢‘ä¿¡å·çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸º DMTF-AVN çš„åŠ¨æ€å¤šç›®æ ‡èåˆæ¡†æ¶ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†å¤šç›®æ ‡æ¶æ„ (Multi-Target Architecture) å¹¶ç»“åˆæ”¹è¿›çš„ Transformer æœºåˆ¶ï¼Œé€šè¿‡è¿‡æ»¤å’Œé€‰æ‹©æ€§èåˆè·¨æ¨¡æ€ä¿¡æ¯æ¥å¢å¼ºå¯¹æ„ŸçŸ¥èƒŒæ™¯çš„ç†è§£ã€‚åœ¨ Replica å’Œ Matterport3D æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒDMTF-AVN åœ¨æˆåŠŸç‡ (SR)ã€è·¯å¾„æ•ˆç‡ (SPL) åŠåœºæ™¯é€‚åº”æ€§ (SNA) ç­‰å…³é”®æŒ‡æ ‡ä¸Šå‡è¾¾åˆ°äº† SOTA æ°´å¹³ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹å±•ç°å‡ºæå¼ºçš„å¯æ‰©å±•æ€§ä¸æ³›åŒ–èƒ½åŠ›ï¼Œæ˜¾è‘—æå‡äº†æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„å¯¼èˆªæ•ˆç‡ï¼Œä¸ºå¤šæ¨¡æ€æ„ŸçŸ¥é©±åŠ¨çš„æœºå™¨äººæŠ€æœ¯æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Main paper (8 pages). Accepted for publication by ECAI( European Conference on Artificial Intelligence) 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.21377v1",
      "published_date": "2025-09-23 09:31:00 UTC",
      "updated_date": "2025-09-23 09:31:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:38:01.386288+00:00"
    },
    {
      "arxiv_id": "2509.19406v5",
      "title": "TimeMosaic: Temporal Heterogeneity Guided Time Series Forecasting via Adaptive Granularity Patch and Segment-wise Decoding",
      "title_zh": "TimeMosaicï¼šåŸºäºè‡ªé€‚åº”ç²’åº¦åˆ†å—ä¸åˆ†æ®µè§£ç çš„æ—¶é—´å¼‚è´¨æ€§å¼•å¯¼æ—¶é—´åºåˆ—é¢„æµ‹",
      "authors": [
        "Kuiye Ding",
        "Fanda Fan",
        "Chunyi Hou",
        "Zheya Wang",
        "Lei Wang",
        "Zhengxin Yang",
        "Jianfeng Zhan"
      ],
      "abstract": "Multivariate time series forecasting is essential in domains such as finance, transportation, climate, and energy. However, existing patch-based methods typically adopt fixed-length segmentation, overlooking the heterogeneity of local temporal dynamics and the decoding heterogeneity of forecasting. Such designs lose details in information-dense regions, introduce redundancy in stable segments, and fail to capture the distinct complexities of short-term and long-term horizons. We propose TimeMosaic, a forecasting framework that aims to address temporal heterogeneity. TimeMosaic employs adaptive patch embedding to dynamically adjust granularity according to local information density, balancing motif reuse with structural clarity while preserving temporal continuity. In addition, it introduces segment-wise decoding that treats each prediction horizon as a related subtask and adapts to horizon-specific difficulty and information requirements, rather than applying a single uniform decoder. Extensive evaluations on benchmark datasets demonstrate that TimeMosaic delivers consistent improvements over existing methods, and our model trained on the large-scale corpus with 321 billion observations achieves performance competitive with state-of-the-art TSFMs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šå…ƒæ—¶é—´åºåˆ—é¢„æµ‹(Multivariate Time Series Forecasting, MTSF)ä¸­ç°æœ‰åŸºäºè¡¥ä¸(patch-based)çš„æ–¹æ³•å› å›ºå®šé•¿åº¦åˆ†æ®µè€Œå¿½ç•¥å±€éƒ¨æ—¶é—´å¼‚è´¨æ€§ä¸è§£ç å¼‚è´¨æ€§çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†TimeMosaicæ¡†æ¶ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†è‡ªé€‚åº”è¡¥ä¸åµŒå…¥(Adaptive Patch Embedding)æœºåˆ¶ï¼Œèƒ½å¤Ÿæ ¹æ®å±€éƒ¨ä¿¡æ¯å¯†åº¦åŠ¨æ€è°ƒæ•´åˆ†æ®µç²’åº¦ï¼Œåœ¨ä¿æŒæ—¶é—´è¿ç»­æ€§çš„åŒæ—¶å¹³è¡¡äº†æ¨¡å¼é‡ç”¨ä¸ç»“æ„æ¸…æ™°åº¦ã€‚æ­¤å¤–ï¼ŒTimeMosaicé‡‡ç”¨äº†åˆ†æ®µè§£ç (Segment-wise Decoding)ç­–ç•¥ï¼Œå°†ä¸åŒé¢„æµ‹è·¨åº¦è§†ä¸ºç›¸å…³çš„å­ä»»åŠ¡ï¼Œä»¥çµæ´»åº”å¯¹ç‰¹å®šé¢„æµ‹è§†é‡(Horizons)ä¸‹çš„ä¿¡æ¯éœ€æ±‚ä¸ä»»åŠ¡éš¾åº¦ã€‚åŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¯æ˜ï¼Œè¯¥æ–¹æ³•è¾ƒç°æœ‰æ¨¡å‹å…·æœ‰æ˜¾è‘—ä¸”æŒç»­çš„æ€§èƒ½æå‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨åŒ…å«3210äº¿è§‚æµ‹å€¼çš„å¤§è§„æ¨¡è¯­æ–™åº“ä¸Šè®­ç»ƒåï¼ŒTimeMosaicå±•ç°å‡ºäº†ä¸å½“å‰æœ€å…ˆè¿›çš„æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹(TSFMs)ç›¸åª²ç¾çš„ç«äº‰åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "This paper has been accepted by AAAI",
      "pdf_url": "https://arxiv.org/pdf/2509.19406v5",
      "published_date": "2025-09-23 09:20:00 UTC",
      "updated_date": "2026-01-05 14:53:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:38:05.291343+00:00"
    },
    {
      "arxiv_id": "2509.18836v1",
      "title": "Bounded PCTL Model Checking of Large Language Model Outputs",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹è¾“å‡ºçš„æœ‰ç•Œ PCTL æ¨¡å‹æ£€æµ‹",
      "authors": [
        "Dennis Gross",
        "Helge Spieker",
        "Arnaud Gotlieb"
      ],
      "abstract": "In this paper, we introduce LLMCHECKER, a model-checking-based verification method to verify the probabilistic computation tree logic (PCTL) properties of an LLM text generation process. We empirically show that only a limited number of tokens are typically chosen during text generation, which are not always the same. This insight drives the creation of $Î±$-$k$-bounded text generation, narrowing the focus to the $Î±$ maximal cumulative probability on the top-$k$ tokens at every step of the text generation process. Our verification method considers an initial string and the subsequent top-$k$ tokens while accommodating diverse text quantification methods, such as evaluating text quality and biases. The threshold $Î±$ further reduces the selected tokens, only choosing those that exceed or meet it in cumulative probability. LLMCHECKER then allows us to formally verify the PCTL properties of $Î±$-$k$-bounded LLMs. We demonstrate the applicability of our method in several LLMs, including Llama, Gemma, Mistral, Genstruct, and BERT. To our knowledge, this is the first time PCTL-based model checking has been used to check the consistency of the LLM text generation process.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº† LLMCHECKERï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ¨¡å‹æ£€éªŒ (model checking) çš„éªŒè¯æ–¹æ³•ï¼Œä¸“é—¨ç”¨äºéªŒè¯å¤§è¯­è¨€æ¨¡å‹ (LLM) æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ä¸­çš„æ¦‚ç‡è®¡ç®—æ ‘é€»è¾‘ (PCTL) å±æ€§ã€‚ç ”ç©¶åŸºäºç”Ÿæˆè¿‡ç¨‹ä¸­ä»…æœ‰æœ‰é™æ•°é‡ token è¢«é€‰ä¸­çš„è§‚å¯Ÿï¼Œæå‡ºäº† $\\alpha$-$k$-bounded æ–‡æœ¬ç”Ÿæˆç­–ç•¥ï¼Œé€šè¿‡è®¾å®šç´¯ç§¯æ¦‚ç‡é˜ˆå€¼ $\\alpha$ å’Œæœ€é«˜é¢‘å€™é€‰æ•°é‡ $k$ æ¥èšç„¦æ ¸å¿ƒç”Ÿæˆè·¯å¾„ã€‚LLMCHECKER æ”¯æŒç»“åˆæ–‡æœ¬è´¨é‡å’Œåè§è¯„ä¼°ç­‰å¤šç§é‡åŒ–æ‰‹æ®µï¼Œå¯¹å—é™ LLM çš„ç”Ÿæˆè¿‡ç¨‹è¿›è¡Œæ­£å¼çš„å½¢å¼åŒ–éªŒè¯ã€‚å®éªŒåœ¨ Llamaã€Gemmaã€Mistralã€Genstruct å’Œ BERT ç­‰å¤šç§æ¨¡å‹ä¸Šè¯æ˜äº†è¯¥æ–¹æ³•çš„é€‚ç”¨æ€§ã€‚ä½œä¸ºé¦–æ¬¡åˆ©ç”¨ PCTL æ¨¡å‹æ£€éªŒæ¥æ£€æŸ¥ LLM æ–‡æœ¬ç”Ÿæˆä¸€è‡´æ€§çš„å·¥ä½œï¼Œè¯¥ç ”ç©¶ä¸ºç¡®ä¿æ¨¡å‹è¾“å‡ºçš„å¯é æ€§æä¾›äº†åˆ›æ–°çš„éªŒè¯é€”å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "ICTAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.18836v1",
      "published_date": "2025-09-23 09:19:37 UTC",
      "updated_date": "2025-09-23 09:19:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:38:06.790568+00:00"
    },
    {
      "arxiv_id": "2509.18831v1",
      "title": "Text Slider: Efficient and Plug-and-Play Continuous Concept Control for Image/Video Synthesis via LoRA Adapters",
      "title_zh": "Text Sliderï¼šåŸºäº LoRA é€‚é…å™¨çš„é«˜æ•ˆã€å³æ’å³ç”¨å›¾åƒ/è§†é¢‘åˆæˆè¿ç»­æ¦‚å¿µæ§åˆ¶",
      "authors": [
        "Pin-Yen Chiu",
        "I-Sheng Fang",
        "Jun-Cheng Chen"
      ],
      "abstract": "Recent advances in diffusion models have significantly improved image and video synthesis. In addition, several concept control methods have been proposed to enable fine-grained, continuous, and flexible control over free-form text prompts. However, these methods not only require intensive training time and GPU memory usage to learn the sliders or embeddings but also need to be retrained for different diffusion backbones, limiting their scalability and adaptability. To address these limitations, we introduce Text Slider, a lightweight, efficient and plug-and-play framework that identifies low-rank directions within a pre-trained text encoder, enabling continuous control of visual concepts while significantly reducing training time, GPU memory consumption, and the number of trainable parameters. Furthermore, Text Slider supports multi-concept composition and continuous control, enabling fine-grained and flexible manipulation in both image and video synthesis. We show that Text Slider enables smooth and continuous modulation of specific attributes while preserving the original spatial layout and structure of the input. Text Slider achieves significantly better efficiency: 5$\\times$ faster training than Concept Slider and 47$\\times$ faster than Attribute Control, while reducing GPU memory usage by nearly 2$\\times$ and 4$\\times$, respectively.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Text Sliderï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§ã€é«˜æ•ˆä¸”å³æ’å³ç”¨ (plug-and-play) çš„æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°å›¾åƒå’Œè§†é¢‘åˆæˆä¸­é’ˆå¯¹æ–‡æœ¬æç¤º (text prompts) çš„ç²¾ç»†åŒ–è¿ç»­æ¦‚å¿µæ§åˆ¶ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•è®­ç»ƒæˆæœ¬é«˜ä¸”éš¾ä»¥è·¨æ¨¡å‹é€‚é…çš„é—®é¢˜ï¼ŒText Slider é€šè¿‡åœ¨é¢„è®­ç»ƒæ–‡æœ¬ç¼–ç å™¨ (text encoder) ä¸­è¯†åˆ«ä½ç§©æ–¹å‘ (low-rank directions)ï¼Œåˆ©ç”¨ LoRA é€‚é…å™¨æ˜¾è‘—é™ä½äº†è®­ç»ƒæ—¶é—´å’Œå‚æ•°é‡ã€‚è¯¥æ¡†æ¶æ”¯æŒå¤šæ¦‚å¿µç»„åˆ (multi-concept composition)ï¼Œå¹¶èƒ½åœ¨ä¸æ”¹å˜åŸå§‹ç©ºé—´å¸ƒå±€å’Œç»“æ„çš„æƒ…å†µä¸‹å®ç°è§†è§‰å±æ€§çš„å¹³æ»‘è°ƒèŠ‚ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒText Slider çš„è®­ç»ƒé€Ÿåº¦æ¯” Concept Slider å¿« 5 å€ï¼Œæ¯” Attribute Control å¿« 47 å€ï¼ŒåŒæ—¶æ˜¾å­˜æ¶ˆè€—é™ä½äº† 2 è‡³ 4 å€ã€‚è¯¥ç ”ç©¶ä¸ºæ‰©æ•£æ¨¡å‹ (diffusion models) æä¾›äº†ä¸€ç§é«˜æ•ˆã€çµæ´»ä¸”æå…·æ‰©å±•æ€§çš„è§†è§‰æ¦‚å¿µè°ƒæ§æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.GR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18831v1",
      "published_date": "2025-09-23 09:17:18 UTC",
      "updated_date": "2025-09-23 09:17:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:38:15.380356+00:00"
    },
    {
      "arxiv_id": "2509.19405v1",
      "title": "Improving Outdoor Multi-cell Fingerprinting-based Positioning via Mobile Data Augmentation",
      "title_zh": "åŸºäºç§»åŠ¨æ•°æ®å¢å¼ºçš„å®¤å¤–å¤šå°åŒºæŒ‡çº¹å®šä½æå‡",
      "authors": [
        "Tony Chahoud",
        "Lorenzo Mario Amorosa",
        "Riccardo Marini",
        "Luca De Nardis"
      ],
      "abstract": "Accurate outdoor positioning in cellular networks is hindered by sparse, heterogeneous measurement collections and the high cost of exhaustive site surveys. This paper introduces a lightweight, modular mobile data augmentation framework designed to enhance multi-cell fingerprinting-based positioning using operator-collected minimization of drive test (MDT) records. The proposed approach decouples spatial and radio-feature synthesis: kernel density estimation (KDE) models the empirical spatial distribution to generate geographically coherent synthetic locations, while a k-nearest-neighbor (KNN)-based block produces augmented per-cell radio fingerprints. The architecture is intentionally training-free, interpretable, and suitable for distributed or on-premise operator deployments, supporting privacy-aware workflows. We both validate each augmentation module independently and assess its end-to-end impact on fingerprinting-based positioning using a real-world MDT dataset provided by an Italian mobile network operator across diverse urban and peri-urban scenarios. Results show that the proposed KDE-KNN augmentation consistently improves positioning performance, with the largest benefits in sparsely sampled or structurally complex regions; we also observe region-dependent saturation effects as augmentation increases. The framework offers a practical, low-complexity path to enhance operator positioning services using existing mobile data traces.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹èœ‚çªç½‘ç»œä¸­å®¤å¤–å®šä½é¢ä¸´çš„æµ‹é‡æ•°æ®ç¨€ç–ã€å¼‚æ„ä»¥åŠç°åœºå‹˜æµ‹æˆæœ¬é«˜æ˜‚ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§è½»é‡çº§ã€æ¨¡å—åŒ–çš„ç§»åŠ¨æ•°æ®å¢å¼ºæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡è§£è€¦ç©ºé—´å’Œå°„é¢‘ç‰¹å¾åˆæˆï¼Œåˆ©ç”¨æ ¸å¯†åº¦ä¼°è®¡(KDE)å¯¹ç»éªŒç©ºé—´åˆ†å¸ƒå»ºæ¨¡ä»¥ç”Ÿæˆåœ°ç†ä¸€è‡´çš„åˆæˆä½ç½®ï¼Œå¹¶ç»“åˆåŸºäºk-æœ€è¿‘é‚»(KNN)çš„æ¨¡å—ç”Ÿæˆå¢å¼ºçš„å•å°åŒºå°„é¢‘æŒ‡çº¹(fingerprints)ã€‚è¯¥æ¶æ„å…·æœ‰æ— éœ€è®­ç»ƒ(training-free)ã€å¯è§£é‡Šæ€§å¼ºä»¥åŠæ”¯æŒéšç§ä¿æŠ¤å·¥ä½œæµç­‰ç‰¹ç‚¹ï¼Œé€‚ç”¨äºè¿è¥å•†çš„åˆ†å¸ƒå¼æˆ–æœ¬åœ°éƒ¨ç½²ã€‚é€šè¿‡ä½¿ç”¨æ„å¤§åˆ©ç§»åŠ¨ç½‘ç»œè¿è¥å•†æä¾›çš„çœŸå®æœ€å°åŒ–é©±åŠ¨æµ‹è¯•(MDT)æ•°æ®é›†è¿›è¡ŒéªŒè¯ï¼Œå®éªŒç»“æœè¡¨æ˜KDE-KNNå¢å¼ºæ–¹æ¡ˆèƒ½æ˜¾è‘—æå‡åŸºäºæŒ‡çº¹çš„å®šä½æ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯åœ¨æ ·æœ¬ç¨€ç–æˆ–ç»“æ„å¤æ‚çš„åŒºåŸŸï¼Œè¯¥æ–¹æ³•è¡¨ç°å‡ºæœ€å¤§çš„æ€§èƒ½æå‡ï¼ŒåŒæ—¶ä¹Ÿè§‚å¯Ÿåˆ°éšå¢å¼ºé‡å¢åŠ è€Œäº§ç”Ÿçš„åŒºåŸŸç›¸å…³é¥±å’Œæ•ˆåº”ã€‚è¯¥ç ”ç©¶ä¸ºè¿è¥å•†åˆ©ç”¨ç°æœ‰çš„ç§»åŠ¨æ•°æ®è½¨è¿¹æå‡å®šä½æœåŠ¡è´¨é‡æä¾›äº†ä¸€æ¡åˆ‡å®å¯è¡Œä¸”ä½å¤æ‚åº¦çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19405v1",
      "published_date": "2025-09-23 09:09:45 UTC",
      "updated_date": "2025-09-23 09:09:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:38:10.002133+00:00"
    },
    {
      "arxiv_id": "2509.20395v1",
      "title": "Centralized vs. Decentralized Security for Space AI Systems? A New Look",
      "title_zh": "ç©ºé—´äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„ä¸­å¿ƒåŒ–ä¸å»ä¸­å¿ƒåŒ–å®‰å…¨ï¼šé‡æ–°å®¡è§†",
      "authors": [
        "Noam Schmitt",
        "Marc Antoine Lacoste"
      ],
      "abstract": "This paper investigates the trade-off between centralized and decentralized security management in constellations of satellites to balance security and performance. We highlight three key AI architectures for automated security management: (a) centralized, (b) distributed and (c) federated. The centralized architecture is the best option short term, providing fast training, despite the hard challenge of the communication latency overhead across space. Decentralized architectures are better alternatives in the longer term, providing enhanced scalability and security.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å«æ˜Ÿæ˜Ÿåº§ï¼ˆconstellations of satellitesï¼‰ä¸­é›†ä¸­å¼ä¸åˆ†å¸ƒå¼å®‰å…¨ç®¡ç†ä¹‹é—´çš„æƒè¡¡ï¼Œæ—¨åœ¨å¹³è¡¡å®‰å…¨æ€§ä¸æ€§èƒ½ã€‚æ–‡ç« é‡ç‚¹åˆ†æäº†ä¸‰ç§ç”¨äºè‡ªåŠ¨åŒ–å®‰å…¨ç®¡ç†çš„ AI æ¶æ„ï¼ŒåŒ…æ‹¬é›†ä¸­å¼ï¼ˆcentralizedï¼‰ã€åˆ†å¸ƒå¼ï¼ˆdistributedï¼‰å’Œè”é‚¦å¼ï¼ˆfederatedï¼‰ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œé›†ä¸­å¼æ¶æ„åœ¨çŸ­æœŸå†…æ˜¯æœ€ä½³é€‰æ‹©ï¼Œå°½ç®¡é¢ä¸´ç©ºé—´é€šä¿¡å»¶è¿Ÿï¼ˆcommunication latencyï¼‰å¸¦æ¥çš„å¼€é”€æŒ‘æˆ˜ï¼Œä½†å…¶æä¾›äº†æ›´å¿«çš„è®­ç»ƒé€Ÿåº¦ã€‚ä»é•¿æœŸæ¥çœ‹ï¼Œåˆ†å¸ƒå¼æ¶æ„å› èƒ½æä¾›å¢å¼ºçš„å¯æ‰©å±•æ€§ï¼ˆscalabilityï¼‰å’Œå®‰å…¨æ€§ï¼ˆsecurityï¼‰ï¼Œè¢«è®¤ä¸ºæ˜¯æ›´å…·ä¼˜åŠ¿çš„æ›¿ä»£æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.CR",
      "comment": "IEEE HPEC 2025 - 29th Annual IEEE High Performance Extreme Computing Virtual Conference, MIT Lincoln Laboratory, Sep 2025, Boston (MA), United States",
      "pdf_url": "https://arxiv.org/pdf/2509.20395v1",
      "published_date": "2025-09-23 08:54:55 UTC",
      "updated_date": "2025-09-23 08:54:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:38:17.761699+00:00"
    },
    {
      "arxiv_id": "2509.18801v1",
      "title": "A Kernel Space-based Multidimensional Sparse Model for Dynamic PET Image Denoising",
      "title_zh": "åŸºäºæ ¸ç©ºé—´çš„å¤šç»´ç¨€ç–æ¨¡å‹ç”¨äºåŠ¨æ€ PET å›¾åƒå»å™ª",
      "authors": [
        "Kuang Xiaodong",
        "Li Bingxuan",
        "Li Yuan",
        "Rao Fan",
        "Ma Gege",
        "Xie Qingguo",
        "Mok Greta S P",
        "Liu Huafeng",
        "Zhu Wentao"
      ],
      "abstract": "Achieving high image quality for temporal frames in dynamic positron emission tomography (PET) is challenging due to the limited statistic especially for the short frames. Recent studies have shown that deep learning (DL) is useful in a wide range of medical image denoising tasks. In this paper, we propose a model-based neural network for dynamic PET image denoising. The inter-frame spatial correlation and intra-frame structural consistency in dynamic PET are used to establish the kernel space-based multidimensional sparse (KMDS) model. We then substitute the inherent forms of the parameter estimation with neural networks to enable adaptive parameters optimization, forming the end-to-end neural KMDS-Net. Extensive experimental results from simulated and real data demonstrate that the neural KMDS-Net exhibits strong denoising performance for dynamic PET, outperforming previous baseline methods. The proposed method may be used to effectively achieve high temporal and spatial resolution for dynamic PET. Our source code is available at https://github.com/Kuangxd/Neural-KMDS-Net/tree/main.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º KMDS-Net çš„åŸºäºæ¨¡å‹çš„ç¥ç»ç½‘ç»œï¼Œæ—¨åœ¨è§£å†³åŠ¨æ€ PET å›¾åƒåœ¨çŸ­æ—¶é—´å¸§ä¸‹å› ç»Ÿè®¡è®¡æ•°æœ‰é™è€Œå¯¼è‡´çš„æˆåƒè´¨é‡æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨åŠ¨æ€ PET å›¾åƒä¸­çš„å¸§é—´ç©ºé—´ç›¸å…³æ€§ï¼ˆinter-frame spatial correlationï¼‰å’Œå¸§å†…ç»“æ„ä¸€è‡´æ€§ï¼ˆintra-frame structural consistencyï¼‰ï¼Œæ„å»ºäº†åŸºäºæ ¸ç©ºé—´çš„å¤šç»´ç¨€ç–æ¨¡å‹ï¼ˆKernel Space-based Multidimensional Sparse model, KMDSï¼‰ã€‚é€šè¿‡å°†ä¼ ç»Ÿçš„å‚æ•°ä¼°è®¡å½¢å¼æ›¿æ¢ä¸ºç¥ç»ç½‘ç»œï¼ŒKMDS-Net å®ç°äº†ç«¯åˆ°ç«¯çš„å‚æ•°è‡ªé€‚åº”ä¼˜åŒ–ã€‚åœ¨æ¨¡æ‹Ÿæ•°æ®å’ŒçœŸå®æ•°æ®ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç½‘ç»œçš„å»å™ªæ€§èƒ½ä¼˜äºç°æœ‰çš„åŸºå‡†æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡åŠ¨æ€ PET çš„æ—¶é—´ä¸ç©ºé—´åˆ†è¾¨ç‡ã€‚è¯¥ç ”ç©¶è¿˜æä¾›äº†å¼€æºä»£ç ï¼Œä¸ºä¸´åºŠå’Œç§‘ç ”ä¸­çš„åŠ¨æ€ PET å›¾åƒå¢å¼ºæä¾›äº†å¯è§£é‡Šæ€§å¼ºä¸”é«˜æ•ˆçš„å·¥å…·ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18801v1",
      "published_date": "2025-09-23 08:48:36 UTC",
      "updated_date": "2025-09-23 08:48:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:38:19.962849+00:00"
    },
    {
      "arxiv_id": "2509.18790v1",
      "title": "Detection of security smells in IaC scripts through semantics-aware code and language processing",
      "title_zh": "åŸºäºè¯­ä¹‰æ„ŸçŸ¥ä»£ç ä¸è¯­è¨€å¤„ç†çš„ IaC è„šæœ¬å®‰å…¨å¼‚å‘³æ£€æµ‹",
      "authors": [
        "Aicha War",
        "Adnan A. Rawass",
        "Abdoul K. Kabore",
        "Jordan Samhi",
        "Jacques Klein",
        "Tegawende F. Bissyande"
      ],
      "abstract": "Infrastructure as Code (IaC) automates the provisioning and management of IT infrastructure through scripts and tools, streamlining software deployment. Prior studies have shown that IaC scripts often contain recurring security misconfigurations, and several detection and mitigation approaches have been proposed. Most of these rely on static analysis, using statistical code representations or Machine Learning (ML) classifiers to distinguish insecure configurations from safe code.\n  In this work, we introduce a novel approach that enhances static analysis with semantic understanding by jointly leveraging natural language and code representations. Our method builds on two complementary ML models: CodeBERT, to capture semantics across code and text, and LongFormer, to represent long IaC scripts without losing contextual information. We evaluate our approach on misconfiguration datasets from two widely used IaC tools, Ansible and Puppet. To validate its effectiveness, we conduct two ablation studies (removing code text from the natural language input and truncating scripts to reduce context) and compare against four large language models (LLMs) and prior work. Results show that semantic enrichment substantially improves detection, raising precision and recall from 0.46 and 0.79 to 0.92 and 0.88 on Ansible, and from 0.55 and 0.97 to 0.87 and 0.75 on Puppet, respectively.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Infrastructure as Code (IaC) è„šæœ¬ä¸­å¸¸è§çš„å®‰å…¨è¯¯é…ç½®ï¼ˆsecurity smellsï¼‰é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆè‡ªç„¶è¯­è¨€ä¸ä»£ç è¡¨ç¤ºçš„è¯­ä¹‰æ„ŸçŸ¥æ£€æµ‹æ–¹æ³•ã€‚ç ”ç©¶é‡‡ç”¨äº† CodeBERT æ¨¡å‹æ¥æ•æ‰ä»£ç ä¸æ–‡æœ¬é—´çš„è·¨æ¨¡æ€è¯­ä¹‰ï¼Œå¹¶åˆ©ç”¨ LongFormer å¤„ç†é•¿è„šæœ¬ä»¥ä¿ç•™å…³é”®çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚é€šè¿‡åœ¨ Ansible å’Œ Puppet æ•°æ®é›†ä¸Šçš„è¯„ä¼°ä»¥åŠä¸å¤šç§å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) çš„å¯¹æ¯”å®éªŒï¼Œç»“æœè¡¨æ˜è¯­ä¹‰å¢å¼ºæ˜¾è‘—æå‡äº†æ£€æµ‹æ€§èƒ½ã€‚åœ¨ Ansible æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•çš„ Precision å’Œ Recall åˆ†åˆ«è¾¾åˆ°äº† 0.92 å’Œ 0.88ï¼Œè€Œåœ¨ Puppet æ•°æ®é›†ä¸ŠåŒæ ·è¡¨ç°ä¼˜å¼‚ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†è”åˆå»ºæ¨¡ä»£ç è¯­ä¹‰ä¸è‡ªç„¶è¯­è¨€ä¸Šä¸‹æ–‡åœ¨è¯†åˆ«è‡ªåŠ¨åŒ–åŸºç¡€è®¾æ–½è„šæœ¬å®‰å…¨ç¼ºé™·æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG",
        "cs.SE"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18790v1",
      "published_date": "2025-09-23 08:28:49 UTC",
      "updated_date": "2025-09-23 08:28:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:38:20.857552+00:00"
    },
    {
      "arxiv_id": "2509.18787v1",
      "title": "The AGNTCY Agent Directory Service: Architecture and Implementation",
      "title_zh": "AGNTCY æ™ºèƒ½ä½“ç›®å½•æœåŠ¡ï¼šæ¶æ„ä¸å®ç°",
      "authors": [
        "Luca Muscariello",
        "Vijoy Pandey",
        "Ramiz Polic"
      ],
      "abstract": "The Agent Directory Service (ADS) is a distributed directory for the discovery of AI agent capabilities, metadata, and provenance. It leverages content-addressed storage, hierarchical taxonomies, and cryptographic signing to enable efficient, verifiable, and multi-dimensional discovery across heterogeneous Multi-Agent Systems (MAS). Built on the Open Agentic Schema Framework (OASF), ADS decouples capability indexing from content location through a two-level mapping realized over a Kademlia-based Distributed Hash Table (DHT). It reuses mature OCI / ORAS infrastructure for artifact distribution, integrates Sigstore for provenance, and supports schema-driven extensibility for emerging agent modalities (LLM prompt agents, MCP servers, A2A-enabled components). This paper formalizes the architectural model, describes storage and discovery layers, explains security and performance properties, and positions ADS within the broader landscape of emerging agent registry and interoperability initiatives.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† Agent Directory Service (ADS)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å¼‚æ„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ (Multi-Agent Systems, MAS) ä¸­å‘ç° AI æ™ºèƒ½ä½“èƒ½åŠ›ã€å…ƒæ•°æ®å’Œå‡ºå¤„çš„åˆ†å¸ƒå¼ç›®å½•æœåŠ¡ã€‚ADS åŸºäº Open Agentic Schema Framework (OASF) æ„å»ºï¼Œåˆ©ç”¨å†…å®¹å¯»å€å­˜å‚¨ (content-addressed storage) å’ŒåŠ å¯†ç­¾åæŠ€æœ¯ï¼Œç¡®ä¿å‘ç°è¿‡ç¨‹çš„é«˜æ•ˆæ€§ä¸å¯éªŒè¯æ€§ã€‚è¯¥ç³»ç»Ÿé€šè¿‡åŸºäº Kademlia çš„åˆ†å¸ƒå¼å“ˆå¸Œè¡¨ (DHT) å®ç°äºŒçº§æ˜ å°„ï¼Œå°†èƒ½åŠ›ç´¢å¼•ä¸å†…å®¹ä½ç½®è§£è€¦ã€‚åœ¨å®ç°å±‚é¢ï¼ŒADS å¤ç”¨äº†æˆç†Ÿçš„ OCI / ORAS åŸºç¡€è®¾æ–½è¿›è¡Œåˆ¶å“åˆ†å‘ï¼Œå¹¶é›†æˆäº† Sigstore ä»¥å¼ºåŒ–æ¥æºçš„å¯è¿½æº¯æ€§ã€‚è¯¥æ¡†æ¶æ”¯æŒæ¨¡å¼é©±åŠ¨çš„å¯æ‰©å±•æ€§ï¼Œèƒ½å¤Ÿå…¼å®¹ LLM prompt agentsã€MCP servers åŠ A2A-enabled components ç­‰æ–°å…´æ™ºèƒ½ä½“æ¨¡æ€ã€‚è¯¥è®ºæ–‡é€šè¿‡è§„èŒƒæ¶æ„æ¨¡å‹åŠå­˜å‚¨ä¸å‘ç°å±‚ï¼Œä¸ºå½“å‰æ™ºèƒ½ä½“æ³¨å†Œä¸äº’æ“ä½œæ€§é¢†åŸŸæä¾›äº†å¯æ‰©å±•ä¸”å®‰å…¨çš„åº•å±‚æ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18787v1",
      "published_date": "2025-09-23 08:25:33 UTC",
      "updated_date": "2025-09-23 08:25:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:38:23.369055+00:00"
    },
    {
      "arxiv_id": "2509.18778v1",
      "title": "VGGT-DP: Generalizable Robot Control via Vision Foundation Models",
      "title_zh": "VGGT-DPï¼šåŸºäºè§†è§‰åŸºç¡€æ¨¡å‹çš„å¯æ³›åŒ–æœºå™¨äººæ§åˆ¶",
      "authors": [
        "Shijia Ge",
        "Yinxin Zhang",
        "Shuzhao Xie",
        "Weixiang Zhang",
        "Mingcai Zhou",
        "Zhi Wang"
      ],
      "abstract": "Visual imitation learning frameworks allow robots to learn manipulation skills from expert demonstrations. While existing approaches mainly focus on policy design, they often neglect the structure and capacity of visual encoders, limiting spatial understanding and generalization. Inspired by biological vision systems, which rely on both visual and proprioceptive cues for robust control, we propose VGGT-DP, a visuomotor policy framework that integrates geometric priors from a pretrained 3D perception model with proprioceptive feedback. We adopt the Visual Geometry Grounded Transformer (VGGT) as the visual encoder and introduce a proprioception-guided visual learning strategy to align perception with internal robot states, improving spatial grounding and closed-loop control. To reduce inference latency, we design a frame-wise token reuse mechanism that compacts multi-view tokens into an efficient spatial representation. We further apply random token pruning to enhance policy robustness and reduce overfitting. Experiments on challenging MetaWorld tasks show that VGGT-DP significantly outperforms strong baselines such as DP and DP3, particularly in precision-critical and long-horizon scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† VGGT-DPï¼Œä¸€ç§ç»“åˆäº†é¢„è®­ç»ƒ 3D æ„ŸçŸ¥æ¨¡å‹å‡ ä½•å…ˆéªŒä¸æœ¬ä½“æ„Ÿå—åé¦ˆçš„è§†è§‰è¿åŠ¨ç­–ç•¥æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è§†è§‰æ¨¡ä»¿å­¦ä¹ ä¸­ç¼–ç å™¨ç©ºé—´ç†è§£å’Œæ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ Visual Geometry Grounded Transformer (VGGT) ä½œä¸ºè§†è§‰ç¼–ç å™¨ï¼Œå¹¶å¼•å…¥æœ¬ä½“æ„Ÿå—å¼•å¯¼çš„è§†è§‰å­¦ä¹ ç­–ç•¥ï¼Œå®ç°äº†æ„ŸçŸ¥ä¸æœºå™¨äººå†…éƒ¨çŠ¶æ€çš„ç²¾å‡†å¯¹é½ã€‚ä¸ºäº†ä¼˜åŒ–æ€§èƒ½ï¼Œç ”ç©¶è®¾è®¡äº†å¸§é—´ä»¤ç‰Œé‡ç”¨æœºåˆ¶(frame-wise token reuse)ä»¥é™ä½æ¨ç†å»¶è¿Ÿï¼Œå¹¶ç»“åˆéšæœºä»¤ç‰Œå‰ªæ(random token pruning)æ¥å¢å¼ºç­–ç•¥é²æ£’æ€§å¹¶æŠ‘åˆ¶è¿‡æ‹Ÿåˆã€‚åœ¨ MetaWorld ä»»åŠ¡çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒVGGT-DP çš„è¡¨ç°æ˜¾è‘—ä¼˜äº Diffusion Policy (DP) å’Œ DP3 ç­‰å¼ºåŸºçº¿æ¨¡å‹ï¼Œå°¤å…¶åœ¨å¯¹ç²¾åº¦è¦æ±‚æé«˜åŠé•¿ç¨‹ä»»åŠ¡(long-horizon scenarios)ä¸­å±•ç°å‡ºæå¼ºçš„ä¼˜è¶Šæ€§ï¼Œä¸ºé€šç”¨çš„æœºå™¨äººæ§åˆ¶æä¾›äº†é«˜æ•ˆçš„è§†è§‰æ„ŸçŸ¥æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "submitted to AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2509.18778v1",
      "published_date": "2025-09-23 08:15:30 UTC",
      "updated_date": "2025-09-23 08:15:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:38:27.784870+00:00"
    },
    {
      "arxiv_id": "2509.18776v1",
      "title": "AECBench: A Hierarchical Benchmark for Knowledge Evaluation of Large Language Models in the AEC Field",
      "title_zh": "AECBenchï¼šé¢å‘AECé¢†åŸŸå¤§è¯­è¨€æ¨¡å‹çŸ¥è¯†è¯„ä¼°çš„å±‚æ¬¡åŒ–è¯„æµ‹åŸºå‡†",
      "authors": [
        "Chen Liang",
        "Zhaoqi Huang",
        "Haofen Wang",
        "Fu Chai",
        "Chunying Yu",
        "Huanhuan Wei",
        "Zhengjie Liu",
        "Yanpeng Li",
        "Hongjun Wang",
        "Ruifeng Luo",
        "Xianzhong Zhao"
      ],
      "abstract": "Large language models (LLMs), as a novel information technology, are seeing increasing adoption in the Architecture, Engineering, and Construction (AEC) field. They have shown their potential to streamline processes throughout the building lifecycle. However, the robustness and reliability of LLMs in such a specialized and safety-critical domain remain to be evaluated. To address this challenge, this paper establishes AECBench, a comprehensive benchmark designed to quantify the strengths and limitations of current LLMs in the AEC domain. The benchmark defines 23 representative tasks within a five-level cognition-oriented evaluation framework encompassing Knowledge Memorization, Understanding, Reasoning, Calculation, and Application. These tasks were derived from authentic AEC practice, with scope ranging from codes retrieval to specialized documents generation. Subsequently, a 4,800-question dataset encompassing diverse formats, including open-ended questions, was crafted primarily by engineers and validated through a two-round expert review. Furthermore, an LLM-as-a-Judge approach was introduced to provide a scalable and consistent methodology for evaluating complex, long-form responses leveraging expert-derived rubrics. Through the evaluation of nine LLMs, a clear performance decline across five cognitive levels was revealed. Despite demonstrating proficiency in foundational tasks at the Knowledge Memorization and Understanding levels, the models showed significant performance deficits, particularly in interpreting knowledge from tables in building codes, executing complex reasoning and calculation, and generating domain-specific documents. Consequently, this study lays the groundwork for future research and development aimed at the robust and reliable integration of LLMs into safety-critical engineering practices.",
      "tldr_zh": "è¯¥ç ”ç©¶å»ºç«‹äº†AECBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºé‡åŒ–å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨å»ºç­‘ã€å·¥ç¨‹å’Œæ–½å·¥(AEC)é¢†åŸŸèƒ½åŠ›è¡¨ç°çš„ç»¼åˆåŸºå‡†ã€‚è¯¥åŸºå‡†åŒ…å«ä¸€ä¸ªç”±äº”å±‚è®¤çŸ¥ç»´åº¦ï¼ˆKnowledge Memorization, Understanding, Reasoning, Calculation, Applicationï¼‰ç»„æˆçš„è¯„ä¼°æ¡†æ¶ï¼Œæ¶µç›–äº†ä»è§„èŒƒæ£€ç´¢åˆ°ä¸“ä¸šæ–‡æ¡£ç”Ÿæˆçš„23é¡¹ä»£è¡¨æ€§ä»»åŠ¡ã€‚ç ”ç©¶åˆ©ç”¨ç”±å·¥ç¨‹å¸ˆç¼–åˆ¶å¹¶ç»è¿‡ä¸“å®¶å®¡æ ¸çš„4,800ä¸ªé—®é¢˜æ•°æ®é›†ï¼Œå¹¶é‡‡ç”¨LLM-as-a-Judgeæ–¹æ³•å¯¹ä¹ç§æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡LLMsåœ¨åŸºç¡€è®¤çŸ¥ä»»åŠ¡ä¸Šè¡¨ç°å°šå¯ï¼Œä½†åœ¨è§£æå»ºç­‘è§„èŒƒä¸­çš„è¡¨æ ¼ä¿¡æ¯ã€æ‰§è¡Œå¤æ‚æ¨ç†è®¡ç®—ä»¥åŠç”Ÿæˆé¢†åŸŸç‰¹å®šæ–‡æ¡£æ–¹é¢å­˜åœ¨æ˜æ˜¾çš„æ€§èƒ½ç¼ºé™·ã€‚è¯¥å·¥ä½œä¸ºæœªæ¥å°†LLMså®‰å…¨ã€å¯é åœ°é›†æˆåˆ°å·¥ç¨‹å®è·µä¸­æä¾›äº†é‡è¦çš„ç ”ç©¶åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18776v1",
      "published_date": "2025-09-23 08:09:58 UTC",
      "updated_date": "2025-09-23 08:09:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:38:49.894261+00:00"
    },
    {
      "arxiv_id": "2509.18775v2",
      "title": "Financial Risk Relation Identification through Dual-view Adaptation",
      "title_zh": "åŸºäºåŒè§†è§’é€‚é…çš„é‡‘èé£é™©å…³è”è¯†åˆ«",
      "authors": [
        "Wei-Ning Chiu",
        "Yu-Hsiang Wang",
        "Andy Hsiao",
        "Yu-Shiang Huang",
        "Chuan-Ju Wang"
      ],
      "abstract": "A multitude of interconnected risk events -- ranging from regulatory changes to geopolitical tensions -- can trigger ripple effects across firms. Identifying inter-firm risk relations is thus crucial for applications like portfolio management and investment strategy. Traditionally, such assessments rely on expert judgment and manual analysis, which are, however, subjective, labor-intensive, and difficult to scale. To address this, we propose a systematic method for extracting inter-firm risk relations using Form 10-K filings -- authoritative, standardized financial documents -- as our data source. Leveraging recent advances in natural language processing, our approach captures implicit and abstract risk connections through unsupervised fine-tuning based on chronological and lexical patterns in the filings. This enables the development of a domain-specific financial encoder with a deeper contextual understanding and introduces a quantitative risk relation score for transparency, interpretable analysis. Extensive experiments demonstrate that our method outperforms strong baselines across multiple evaluation settings. Our codes are available at https://github.com/cnclabs/codes.fin.relation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è·¨å…¬å¸é£é™©å…³ç³»è¯†åˆ«(inter-firm risk relations)è¿™ä¸€éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºåŒè§†è§’é€‚åº”(Dual-view Adaptation)çš„ç³»ç»ŸåŒ–æå–æ–¹æ³•ã€‚ç”±äºä¼ ç»Ÿçš„äººå·¥åˆ†æå…·æœ‰ä¸»è§‚æ€§ä¸”éš¾ä»¥æ‰©å±•ï¼Œè¯¥ç ”ç©¶åˆ©ç”¨æƒå¨çš„Form 10-Kè´¢åŠ¡æŠ¥å‘Šä½œä¸ºæ•°æ®æºï¼Œå¹¶ç»“åˆè‡ªç„¶è¯­è¨€å¤„ç†(NLP)é¢†åŸŸçš„æœ€æ–°è¿›å±•ã€‚é€šè¿‡åŸºäºæ—¶é—´åºåˆ—(chronological)å’Œè¯æ±‡æ¨¡å¼(lexical patterns)çš„æ— ç›‘ç£å¾®è°ƒ(unsupervised fine-tuning)ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæ•è·éšæ€§ä¸”æŠ½è±¡çš„é£é™©è”ç³»ã€‚ç ”ç©¶è¿›ä¸€æ­¥å¼€å‘äº†å…·æœ‰æ·±å±‚ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›çš„é¢†åŸŸç‰¹å®šé‡‘èç¼–ç å™¨(domain-specific financial encoder)ï¼Œå¹¶å¼•å…¥é‡åŒ–çš„é£é™©å…³ç³»å¾—åˆ†(risk relation score)ä»¥å¢å¼ºåˆ†æçš„é€æ˜åº¦ä¸å¯è§£é‡Šæ€§ã€‚å¤§é‡å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§è¯„ä¼°è®¾å®šä¸‹å‡ä¼˜äºç°æœ‰çš„å¼ºåŸºå‡†æ¨¡å‹(strong baselines)ã€‚è¿™ä¸€æˆæœä¸ºæŠ•èµ„ç»„åˆç®¡ç†(portfolio management)å’ŒæŠ•èµ„ç­–ç•¥(investment strategy)çš„é«˜æ•ˆå†³ç­–å¥ å®šäº†æŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "11 pages, 3 figures, EMNLP 2025 Main Conference",
      "pdf_url": "https://arxiv.org/pdf/2509.18775v2",
      "published_date": "2025-09-23 08:09:30 UTC",
      "updated_date": "2025-11-27 02:35:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:38:50.061773+00:00"
    },
    {
      "arxiv_id": "2509.18771v1",
      "title": "Experience Scaling: Post-Deployment Evolution For Large Language Models",
      "title_zh": "Experience Scalingï¼šå¤§è¯­è¨€æ¨¡å‹çš„éƒ¨ç½²åæ¼”è¿›",
      "authors": [
        "Xingkun Yin",
        "Kaibin Huang",
        "Dong In Kim",
        "Hongyang Du"
      ],
      "abstract": "Scaling model size, training data, and compute power have driven advances in large language models (LLMs), but these approaches are reaching saturation as human-generated text is exhausted and further gains diminish. We propose experience scaling, a framework for continuous post-deployment evolution for LLMs through autonomous interaction with the environment and collaborative sharing of accumulated experience. The framework captures raw interactions, distills them into compact, reusable knowledge, and periodically refines stored content to preserve relevance and efficiency. We validate the framework in simulated real-world scenarios involving generalization to previously unseen but related tasks, repetitive queries, and over-saturated knowledge stores. Across all settings, experience scaling improves accuracy, sustains performance over time, and maintains gains when applied to novel situations. These results demonstrate that structured post-deployment learning can extend LLM capabilities beyond the limits of static human-generated data, offering a scalable path for continued intelligence progress.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Large Language Models (LLMs) åœ¨äººç±»ç”Ÿæˆæ–‡æœ¬æ•°æ®è€—å°½åŠè§„æ¨¡æ‰©å±•æ”¶ç›Šé€’å‡çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸º Experience Scaling çš„æŒç»­æ¼”è¿›æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸ç¯å¢ƒçš„è‡ªä¸»äº¤äº’ä»¥åŠç»éªŒçš„åä½œå…±äº«ï¼Œä½¿æ¨¡å‹åœ¨éƒ¨ç½²åèƒ½å¤Ÿä¸æ–­è¿›åŒ–ã€‚å…¶æ ¸å¿ƒæµç¨‹åŒ…æ‹¬æ•æ‰åŸå§‹äº¤äº’ã€å°†ç»éªŒæå–ä¸ºç´§å‡‘çš„å¯é‡ç”¨çŸ¥è¯†ï¼Œå¹¶å®šæœŸä¼˜åŒ–å­˜å‚¨å†…å®¹ä»¥ç¡®ä¿æ•ˆç‡ã€‚å®éªŒåœ¨æ¨¡æ‹ŸçœŸå®åœºæ™¯ä¸­éªŒè¯äº†è¯¥æ¡†æ¶åœ¨å¤„ç†æœªè§ä»»åŠ¡ã€é‡å¤æŸ¥è¯¢åŠè¿‡é¥±å’ŒçŸ¥è¯†åº“æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚ç»“æœæ˜¾ç¤º Experience Scaling æ˜¾è‘—æå‡äº†æ¨¡å‹çš„å‡†ç¡®ç‡å¹¶èƒ½ç»´æŒé•¿æœŸæ€§èƒ½å¢ç›Šï¼Œè¯æ˜äº†ç»“æ„åŒ–çš„éƒ¨ç½²åå­¦ä¹ æ˜¯è¶…è¶Šé™æ€æ•°æ®é™åˆ¶ã€æ¨åŠ¨æ™ºèƒ½æŒç»­è¿›æ­¥çš„å¯æ‰©å±•è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18771v1",
      "published_date": "2025-09-23 08:04:58 UTC",
      "updated_date": "2025-09-23 08:04:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:38:47.894740+00:00"
    },
    {
      "arxiv_id": "2509.18765v1",
      "title": "DiSSECT: Structuring Transfer-Ready Medical Image Representations through Discrete Self-Supervision",
      "title_zh": "DiSSECTï¼šé€šè¿‡ç¦»æ•£è‡ªç›‘ç£æ„å»ºæ˜“äºè¿ç§»çš„åŒ»å­¦å›¾åƒè¡¨ç¤º",
      "authors": [
        "Azad Singh",
        "Deepak Mishra"
      ],
      "abstract": "Self-supervised learning (SSL) has emerged as a powerful paradigm for medical image representation learning, particularly in settings with limited labeled data. However, existing SSL methods often rely on complex architectures, anatomy-specific priors, or heavily tuned augmentations, which limit their scalability and generalizability. More critically, these models are prone to shortcut learning, especially in modalities like chest X-rays, where anatomical similarity is high and pathology is subtle. In this work, we introduce DiSSECT -- Discrete Self-Supervision for Efficient Clinical Transferable Representations, a framework that integrates multi-scale vector quantization into the SSL pipeline to impose a discrete representational bottleneck. This constrains the model to learn repeatable, structure-aware features while suppressing view-specific or low-utility patterns, improving representation transfer across tasks and domains. DiSSECT achieves strong performance on both classification and segmentation tasks, requiring minimal or no fine-tuning, and shows particularly high label efficiency in low-label regimes. We validate DiSSECT across multiple public medical imaging datasets, demonstrating its robustness and generalizability compared to existing state-of-the-art approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DiSSECTï¼Œä¸€ç§æ—¨åœ¨é€šè¿‡ç¦»æ•£è‡ªç›‘ç£å­¦ä¹  (Discrete Self-Supervision) æ„å»ºå¯è¿ç§»åŒ»ç–—å½±åƒè¡¨ç¤ºçš„æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰è‡ªç›‘ç£å­¦ä¹  (SSL) æ–¹æ³•è¿‡åº¦ä¾èµ–å¤æ‚æ¶æ„ã€ç‰¹å®šè§£å‰–å…ˆéªŒä¸”æ˜“äº§ç”Ÿæ·å¾„å­¦ä¹  (shortcut learning) çš„å±€é™æ€§ï¼ŒDiSSECT å°†å¤šå°ºåº¦å‘é‡é‡åŒ– (multi-scale vector quantization) å¼•å…¥ SSL æµç¨‹ä»¥å»ºç«‹ç¦»æ•£è¡¨ç¤ºç“¶é¢ˆã€‚è¿™ç§çº¦æŸæœºåˆ¶ä¿ƒä½¿æ¨¡å‹å­¦ä¹ å…·æœ‰ç»“æ„æ„ŸçŸ¥èƒ½åŠ›çš„é‡å¤ç‰¹å¾ï¼ŒåŒæ—¶æœ‰æ•ˆæŠ‘åˆ¶è§†å›¾ç‰¹å®šæˆ–ä½ä»·å€¼çš„æ¨¡å¼ï¼Œä»è€Œæ˜¾è‘—å¢å¼ºäº†è¡¨ç¤ºåœ¨ä¸åŒä»»åŠ¡å’Œé¢†åŸŸé—´çš„è¿ç§»èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDiSSECT åœ¨åŒ»ç–—å½±åƒåˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ä¸­å‡å–å¾—äº†å¼ºåŠ²è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½æ ‡ç­¾æ ·æœ¬åœºæ™¯ä¸‹å±•ç°å‡ºæé«˜çš„æ ‡ç­¾æ•ˆç‡ (label efficiency)ã€‚é€šè¿‡åœ¨å¤šä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„éªŒè¯ï¼Œè¯¥æ¡†æ¶è¯æ˜äº†å…¶ç›¸æ¯”äºç°æœ‰å…ˆè¿›æŠ€æœ¯å…·æœ‰æ›´ä¼˜çš„é²æ£’æ€§å’Œé€šç”¨æ€§ï¼Œä¸”ä»…éœ€æå°‘ç”šè‡³æ— éœ€å¾®è°ƒå³å¯å®ç°é«˜æ•ˆè¿ç§»ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18765v1",
      "published_date": "2025-09-23 07:58:21 UTC",
      "updated_date": "2025-09-23 07:58:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:39:01.706092+00:00"
    },
    {
      "arxiv_id": "2509.18762v3",
      "title": "When Long Helps Short: How Context Length in Supervised Fine-tuning Affects Behavior of Large Language Models",
      "title_zh": "ä»¥é•¿åŠ©çŸ­ï¼šæœ‰ç›‘ç£å¾®è°ƒä¸­çš„ä¸Šä¸‹æ–‡é•¿åº¦å¦‚ä½•å½±å“å¤§è¯­è¨€æ¨¡å‹è¡Œä¸º",
      "authors": [
        "Yingming Zheng",
        "Hanqi Li",
        "Kai Yu",
        "Lu Chen"
      ],
      "abstract": "Large language models (LLMs) have achieved impressive performance across natural language processing (NLP) tasks. As real-world applications increasingly demand longer context windows, continued pretraining and supervised fine-tuning (SFT) on long-context data has become a common approach. While the effects of data length in continued pretraining have been extensively studied, their implications for SFT remain unclear. In this work, we systematically investigate how SFT data length influences LLM behavior on short-context tasks. Counterintuitively, we find that long-context SFT improves short-context performance, contrary to the commonly observed degradation from long-context pretraining. To uncover the underlying mechanisms of this phenomenon, we first decouple and analyze two key components, Multi-Head Attention (MHA) and Feed-Forward Network (FFN), and show that both independently benefit from long-context SFT. We further study their interaction and reveal a knowledge preference bias: long-context SFT promotes contextual knowledge, while short-context SFT favors parametric knowledge, making exclusive reliance on long-context SFT suboptimal. Finally, we demonstrate that hybrid training mitigates this bias, offering explainable guidance for fine-tuning LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶ç³»ç»Ÿåœ°è°ƒæŸ¥äº†æœ‰ç›‘ç£å¾®è°ƒ(Supervised Fine-tuning, SFT)æ•°æ®é•¿åº¦å¦‚ä½•å½±å“å¤§å‹è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)åœ¨çŸ­ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ç ”ç©¶è€…å‘ç°äº†ä¸€ä¸ªåç›´è§‰çš„ç°è±¡ï¼Œå³é•¿ä¸Šä¸‹æ–‡ SFT èƒ½å¤Ÿæå‡çŸ­ä¸Šä¸‹æ–‡ä»»åŠ¡çš„æ€§èƒ½ï¼Œè¿™ä¸æŒç»­é¢„è®­ç»ƒä¸­å¸¸è§çš„æ€§èƒ½é€€åŒ–è§„å¾‹ç›¸åã€‚é€šè¿‡å¯¹å¤šå¤´æ³¨æ„åŠ›(Multi-Head Attention, MHA)å’Œå‰é¦ˆç½‘ç»œ(Feed-Forward Network, FFN)è¿›è¡Œè§£è€¦åˆ†æï¼Œç ”ç©¶è¡¨æ˜è¿™ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶éƒ½èƒ½ä»é•¿ä¸Šä¸‹æ–‡ SFT ä¸­ç‹¬ç«‹å—ç›Šã€‚ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº†æ¨¡å‹å­˜åœ¨çŸ¥è¯†åå¥½åå·®ï¼Œå³é•¿ä¸Šä¸‹æ–‡ SFT å€¾å‘äºå¼ºåŒ–ä¸Šä¸‹æ–‡çŸ¥è¯†(contextual knowledge)ï¼Œè€ŒçŸ­ä¸Šä¸‹æ–‡ SFT åˆ™æ›´é’çå‚æ•°åŒ–çŸ¥è¯†(parametric knowledge)ã€‚æœ€åï¼Œè¯¥ç ”ç©¶è¯æ˜é‡‡ç”¨æ··åˆè®­ç»ƒ(hybrid training)å¯ä»¥æœ‰æ•ˆå‡è½»è¿™ç§åå·®ï¼Œä»è€Œä¸º LLMs çš„å¾®è°ƒæä¾›äº†å¯è§£é‡Šçš„å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18762v3",
      "published_date": "2025-09-23 07:55:38 UTC",
      "updated_date": "2025-10-03 01:46:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:38:55.055110+00:00"
    },
    {
      "arxiv_id": "2509.18761v1",
      "title": "Security smells in infrastructure as code: a taxonomy update beyond the seven sins",
      "title_zh": "åŸºç¡€è®¾æ–½å³ä»£ç ä¸­çš„å®‰å…¨å¼‚å‘³ï¼šè¶…è¶Šâ€œä¸ƒå®—ç½ªâ€çš„åˆ†ç±»ä½“ç³»æ›´æ–°",
      "authors": [
        "Aicha War",
        "Serge L. B. Nikiema",
        "Jordan Samhi",
        "Jacques Klein",
        "Tegawende F. Bissyande"
      ],
      "abstract": "Infrastructure as Code (IaC) has become essential for modern software management, yet security flaws in IaC scripts can have severe consequences, as exemplified by the recurring exploits of Cloud Web Services. Prior work has recognized the need to build a precise taxonomy of security smells in IaC scripts as a first step towards developing approaches to improve IaC security. This first effort led to the unveiling of seven sins, limited by the focus on a single IaC tool as well as by the extensive, and potentially biased, manual effort that was required. We propose, in our work, to revisit this taxonomy: first, we extend the study of IaC security smells to a more diverse dataset with scripts associated with seven popular IaC tools, including Terraform, Ansible, Chef, Puppet, Pulumi, Saltstack, and Vagrant; second, we bring in some automation for the analysis by relying on an LLM. While we leverage LLMs for initial pattern processing, all taxonomic decisions underwent systematic human validation and reconciliation with established security standards. Our study yields a comprehensive taxonomy of 62 security smell categories, significantly expanding beyond the previously known seven. We demonstrate actionability by implementing new security checking rules within linters for seven popular IaC tools, often achieving 1.00 precision score. Our evolution study of security smells in GitHub projects reveals that these issues persist for extended periods, likely due to inadequate detection and mitigation tools. This work provides IaC practitioners with insights for addressing common security smells and systematically adopting DevSecOps practices to build safer infrastructure code.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŸºç¡€è®¾æ–½å³ä»£ç  (Infrastructure as Code, IaC) ä¸­çš„å®‰å…¨éšæ‚£ï¼Œæ—¨åœ¨æ›´æ–°å¹¶æ‰©å±•ç°æœ‰çš„å®‰å…¨æ°”å‘³ (Security Smells) åˆ†ç±»ä½“ç³»ã€‚æ­¤å‰ç ”ç©¶ä»…é’ˆå¯¹å•ä¸€ IaC å·¥å…·æå‡ºäº†â€œä¸ƒå®—ç½ªâ€åˆ†ç±»ï¼Œå—é™äºå·¥å…·è¦†ç›–èŒƒå›´åŠç¹é‡çš„äººå·¥åˆ†æè¿‡ç¨‹ã€‚æœ¬æ–‡å°†ç ”ç©¶å¯¹è±¡æ‰©å±•è‡³ Terraform, Ansible, Chef, Puppet, Pulumi, Saltstack å’Œ Vagrant ç­‰ä¸ƒç§æµè¡Œ IaC å·¥å…·ï¼Œå¹¶å¼•å…¥å¤§è¯­è¨€æ¨¡å‹ (LLM) è¾…åŠ©åˆ†æï¼Œç»“åˆç³»ç»Ÿçš„äººå·¥éªŒè¯ä¸å®‰å…¨æ ‡å‡†æ¯”å¯¹ã€‚ç ”ç©¶æœ€ç»ˆæ„å»ºäº†ä¸€ä¸ªåŒ…å« 62 ä¸ªå®‰å…¨æ°”å‘³ç±»åˆ«çš„å…¨é¢åˆ†ç±»æ³•ï¼Œæ˜¾è‘—è¶…è¶Šäº†åŸæœ‰çš„ä¸ƒç±»è§„æ¨¡ã€‚ä¸ºäº†è¯æ˜åˆ†ç±»çš„å¯æ“ä½œæ€§ï¼Œä½œè€…åœ¨å¤šç§å·¥å…·çš„ Linter ä¸­å®ç°äº†æ–°çš„å®‰å…¨æ£€æŸ¥è§„åˆ™ï¼Œå¹¶è¾¾åˆ°äº†æé«˜çš„ç²¾ç¡®ç‡ (Precision Score)ã€‚é’ˆå¯¹ GitHub é¡¹ç›®çš„æ¼”åŒ–ç ”ç©¶æ˜¾ç¤ºï¼Œè¿™äº›å®‰å…¨é—®é¢˜å› ç¼ºä¹æ£€æµ‹å·¥å…·è€Œé•¿æœŸå­˜åœ¨ï¼Œè¯¥å·¥ä½œä¸ºä»ä¸šè€…ç³»ç»Ÿæ€§åœ°é‡‡ç”¨ DevSecOps å®è·µã€æ„å»ºæ›´å®‰å…¨çš„åŸºç¡€è®¾æ–½ä»£ç æä¾›äº†é‡è¦æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG",
        "cs.SE"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18761v1",
      "published_date": "2025-09-23 07:55:35 UTC",
      "updated_date": "2025-09-23 07:55:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:39:04.875525+00:00"
    },
    {
      "arxiv_id": "2509.18758v1",
      "title": "Complexity of Activity Patterns in a Bio-Inspired Hopfield-Type Network in Different Topologies",
      "title_zh": "ä¸åŒæ‹“æ‰‘ç»“æ„ä¸‹ç”Ÿç‰©å¯å‘ Hopfield å‹ç½‘ç»œæ´»åŠ¨æ¨¡å¼çš„å¤æ‚æ€§",
      "authors": [
        "Marco Cafiso",
        "Paolo Paradisi"
      ],
      "abstract": "Neural network models capable of storing memory have been extensively studied in computer science and computational neuroscience. The Hopfield network is a prototypical example of a model designed for associative, or content-addressable, memory and has been analyzed in many forms. Further, ideas and methods from complex network theory have been incorporated into artificial neural networks and learning, emphasizing their structural properties. Nevertheless, the temporal dynamics also play a vital role in biological neural networks, whose temporal structure is a crucial feature to examine. Biological neural networks display complex intermittency and, thus, can be studied through the lens of the temporal complexity (TC) theory. The TC approach look at the metastability of self-organized states, characterized by a power-law decay in the inter-event time distribution and in the total activity distribution or a scaling behavior in the corresponding event-driven diffusion processes. In this study, we present a temporal complexity (TC) analysis of a biologically-inspired Hopfield-type neural network model. We conducted a comparative assessment between scale-free and random network topologies, with particular emphasis on their global activation patterns. Our parametric analysis revealed comparable dynamical behaviors across both neural network architectures. Furthermore, our investigation into temporal complexity characteristics uncovered that seemingly distinct dynamical patterns exhibit similar temporal complexity behaviors. In particular, similar power-law decay in the activity distribution and similar complexity levels are observed in both topologies, but with a much reduced noise in the scale-free topology. Notably, most of the complex dynamical profiles were consistently observed in scale-free network configurations, thus confirming the crucial role of hubs in neural network dynamics.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶å¯¹ä¸€ç§ç”Ÿç‰©å¯å‘å¼ Hopfield-type ç¥ç»ç½‘ç»œæ¨¡å‹è¿›è¡Œäº†æ—¶é—´å¤æ‚æ€§ï¼ˆTemporal Complexity, TCï¼‰åˆ†æï¼Œé‡ç‚¹æ¢è®¨äº† Scale-free å’Œ Random ç½‘ç»œæ‹“æ‰‘åœ¨å…¨å±€æ¿€æ´»æ¨¡å¼ï¼ˆglobal activation patternsï¼‰ä¸Šçš„å·®å¼‚ã€‚ç ”ç©¶é‡‡ç”¨ TC ç†è®ºè¯„ä¼°äº†è‡ªç»„ç»‡çŠ¶æ€çš„äºšç¨³æ€ï¼Œå¹¶å¯¹æ¯”äº†ä¸åŒç½‘ç»œæ¶æ„ä¸‹çš„åŠ¨åŠ›å­¦è¡Œä¸ºã€‚å®éªŒå‘ç°ï¼Œå°½ç®¡ä¸¤ç§æ‹“æ‰‘ç»“æ„çš„åŠ¨åŠ›å­¦æ¨¡å¼çœ‹ä¼¼ä¸åŒï¼Œä½†å…¶æ´»åŠ¨åˆ†å¸ƒå‡å‘ˆç°å‡ºç›¸ä¼¼çš„å¹‚å¾‹è¡°å‡ï¼ˆpower-law decayï¼‰å’Œå¤æ‚æ€§æ°´å¹³ã€‚ç„¶è€Œï¼ŒScale-free æ‹“æ‰‘åœ¨è¿è¡Œä¸­è¡¨ç°å‡ºæ˜¾è‘—é™ä½çš„å™ªå£°ï¼Œä¸”æ›´ä¸€è‡´åœ°å±•ç¤ºäº†å¤æ‚çš„åŠ¨åŠ›å­¦ç‰¹å¾ã€‚è¯¥ç ”ç©¶ç»“æœè¯å®äº† Hubs åœ¨ç¥ç»ç½‘ç»œåŠ¨åŠ›å­¦ä¸­å‘æŒ¥çš„å…³é”®ä½œç”¨ï¼Œå¼ºè°ƒäº†æ‹“æ‰‘ç»“æ„å¯¹äºç†è§£ç”Ÿç‰©ç¥ç»ç³»ç»Ÿå¤æ‚æ—¶é—´ç‰¹æ€§çš„é‡è¦æ€§ã€‚",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "nlin.AO",
        "physics.bio-ph"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18758v1",
      "published_date": "2025-09-23 07:53:27 UTC",
      "updated_date": "2025-09-23 07:53:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:39:03.946628+00:00"
    },
    {
      "arxiv_id": "2509.18757v1",
      "title": "MV-UMI: A Scalable Multi-View Interface for Cross-Embodiment Learning",
      "title_zh": "MV-UMIï¼šé¢å‘è·¨å…·èº«å­¦ä¹ çš„å¯æ‰©å±•å¤šè§†è§’æ¥å£",
      "authors": [
        "Omar Rayyan",
        "John Abanes",
        "Mahmoud Hafez",
        "Anthony Tzes",
        "Fares Abu-Dakka"
      ],
      "abstract": "Recent advances in imitation learning have shown great promise for developing robust robot manipulation policies from demonstrations. However, this promise is contingent on the availability of diverse, high-quality datasets, which are not only challenging and costly to collect but are often constrained to a specific robot embodiment. Portable handheld grippers have recently emerged as intuitive and scalable alternatives to traditional robotic teleoperation methods for data collection. However, their reliance solely on first-person view wrist-mounted cameras often creates limitations in capturing sufficient scene contexts. In this paper, we present MV-UMI (Multi-View Universal Manipulation Interface), a framework that integrates a third-person perspective with the egocentric camera to overcome this limitation. This integration mitigates domain shifts between human demonstration and robot deployment, preserving the cross-embodiment advantages of handheld data-collection devices. Our experimental results, including an ablation study, demonstrate that our MV-UMI framework improves performance in sub-tasks requiring broad scene understanding by approximately 47% across 3 tasks, confirming the effectiveness of our approach in expanding the range of feasible manipulation tasks that can be learned using handheld gripper systems, without compromising the cross-embodiment advantages inherent to such systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ¨¡ä»¿å­¦ä¹ (Imitation Learning)ä¸­ä¼ ç»Ÿæ‰‹æŒå¤¹æŒå™¨ä»…ä¾èµ–è…•éƒ¨ç›¸æœºå¯¼è‡´åœºæ™¯ä¸Šä¸‹æ–‡æ•æ‰ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†MV-UMIï¼ˆMulti-View Universal Manipulation Interfaceï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡æ•´åˆç¬¬ä¸‰äººç§°è§†è§’ä¸ç¬¬ä¸€äººç§°è§†è§’çš„ç›¸æœºæ•°æ®ï¼Œæ˜¾è‘—å¢å¼ºäº†ç³»ç»Ÿå¯¹å¤æ‚åœºæ™¯çš„ç†è§£èƒ½åŠ›ã€‚è¿™ç§å¤šè§†è§’é›†æˆæ–¹å¼æœ‰æ•ˆç¼“è§£äº†äººç±»æ¼”ç¤ºä¸æœºå™¨äººéƒ¨ç½²ä¹‹é—´çš„é¢†åŸŸæ¼‚ç§»(Domain Shifts)ï¼ŒåŒæ—¶ä¿ç•™äº†æ‰‹æŒæ•°æ®é‡‡é›†è®¾å¤‡åœ¨è·¨å…·èº«(Cross-Embodiment)å­¦ä¹ æ–¹é¢çš„ä¼˜åŠ¿ã€‚å®éªŒç»“æœåŠæ¶ˆèç ”ç©¶è¡¨æ˜ï¼ŒMV-UMIåœ¨éœ€è¦å¹¿æ³›åœºæ™¯ç†è§£çš„ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œåœ¨ä¸‰é¡¹ä»»åŠ¡ä¸­çš„å­ä»»åŠ¡æ€§èƒ½æå‡äº†çº¦47%ã€‚è¯¥æ–¹æ³•åœ¨ä¸ç‰ºç‰²è·¨å…·èº«çµæ´»æ€§å’Œå¯æ‰©å±•æ€§çš„å‰æä¸‹ï¼Œæ˜¾è‘—æ‰©å±•äº†æ‰‹æŒå¤¹æŒå™¨ç³»ç»Ÿå¯å­¦ä¹ çš„å¤æ‚æ“ä½œä»»åŠ¡èŒƒå›´ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "For project website and videos, see https https://mv-umi.github.io",
      "pdf_url": "https://arxiv.org/pdf/2509.18757v1",
      "published_date": "2025-09-23 07:53:05 UTC",
      "updated_date": "2025-09-23 07:53:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:39:07.568996+00:00"
    },
    {
      "arxiv_id": "2509.18754v3",
      "title": "COLT: Enhancing Video Large Language Models with Continual Tool Usage",
      "title_zh": "COLTï¼šé€šè¿‡æŒç»­å·¥å…·è°ƒç”¨å¢å¼ºè§†é¢‘å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Yuyang Liu",
        "Meng Cao",
        "Xinyuan Shi",
        "Xiaondan Liang"
      ],
      "abstract": "The success of Large Language Models (LLMs) has significantly propelled the research of video understanding. To harvest the benefits of well-trained expert models (i.e., tools), video LLMs prioritize the exploration of tool usage capabilities. Existing methods either prompt closed-source LLMs or employ the instruction tuning paradigm for tool-use fine-tuning. These methods, however, assume an established repository of fixed tools and struggle to generalize to real-world environments where tool data is perpetually evolving and streaming in. To this end, we propose to enhance open-source video LLMs with COntinuaL Tool usage (termed COLT), which automatically acquires tool-use ability in a successive tool stream without suffering 'catastrophic forgetting' of the past learned tools. Specifically, our COLT incorporates a learnable tool codebook as a tool-specific memory system. Then relevant tools are dynamically selected based on the similarity between user instruction and tool features within the codebook. To unleash the tool usage potential of video LLMs, we collect a video-centric tool-use instruction tuning dataset VideoToolBench. Extensive experiments on both previous video LLM benchmarks and the tool-use-specific VideoToolBench dataset demonstrate the state-of-the-art performance of our proposed COLT.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†COLTï¼Œæ—¨åœ¨å¢å¼ºVideo Large Language Models (Video LLMs)åœ¨æµå¼å·¥å…·ç¯å¢ƒä¸‹çš„Continual Tool usageèƒ½åŠ›ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•éš¾ä»¥é€‚åº”å®æ—¶æ¼”è¿›çš„å·¥å…·æ•°æ®ä¸”å®¹æ˜“äº§ç”Ÿcatastrophic forgettingçš„é—®é¢˜ï¼ŒCOLTå¼•å…¥äº†ä¸€ä¸ªå¯å­¦ä¹ çš„tool codebookä½œä¸ºå·¥å…·ä¸“ç”¨è®°å¿†ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿæ ¹æ®ç”¨æˆ·æŒ‡ä»¤ä¸codebookä¸­å·¥å…·ç‰¹å¾çš„ç›¸ä¼¼åº¦åŠ¨æ€é€‰æ‹©ç›¸å…³å·¥å…·ï¼Œå®ç°å¯¹æ–°å·¥å…·èƒ½åŠ›çš„æŒç»­è·å–ä¸æ•´åˆã€‚ä¸ºæ”¯æŒæ­¤ç ”ç©¶ï¼Œå›¢é˜Ÿè¿˜å¼€å‘äº†ä¸“é—¨é’ˆå¯¹è§†é¢‘å·¥å…·ä½¿ç”¨çš„æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†VideoToolBenchï¼Œä»¥å……åˆ†é‡Šæ”¾æ¨¡å‹çš„å·¥å…·è°ƒç”¨æ½œåŠ›ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒCOLTåœ¨å¤šä¸ªé€šç”¨è§†é¢‘å¤§æ¨¡å‹åŸºå‡†æµ‹è¯•å’ŒVideoToolBenchä¸Šå‡å–å¾—äº†state-of-the-artçš„æ€§èƒ½è¡¨ç°ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "16 pages",
      "pdf_url": "https://arxiv.org/pdf/2509.18754v3",
      "published_date": "2025-09-23 07:49:30 UTC",
      "updated_date": "2026-01-03 09:21:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:39:13.974794+00:00"
    },
    {
      "arxiv_id": "2509.19403v1",
      "title": "Online Adaptation via Dual-Stage Alignment and Self-Supervision for Fast-Calibration Brain-Computer Interfaces",
      "title_zh": "åŸºäºåŒé˜¶æ®µå¯¹é½ä¸è‡ªç›‘ç£çš„å¿«é€Ÿæ ¡å‡†è„‘æœºæ¥å£åœ¨çº¿è‡ªé€‚åº”",
      "authors": [
        "Sheng-Bin Duan",
        "Jian-Long Hao",
        "Tian-Yu Xiang",
        "Xiao-Hu Zhou",
        "Mei-Jiang Gui",
        "Xiao-Liang Xie",
        "Shi-Qi Liu",
        "Zeng-Guang Hou"
      ],
      "abstract": "Individual differences in brain activity hinder the online application of electroencephalogram (EEG)-based brain computer interface (BCI) systems. To overcome this limitation, this study proposes an online adaptation algorithm for unseen subjects via dual-stage alignment and self-supervision. The alignment process begins by applying Euclidean alignment in the EEG data space and then updates batch normalization statistics in the representation space. Moreover, a self-supervised loss is designed to update the decoder. The loss is computed by soft pseudo-labels derived from the decoder as a proxy for the unknown ground truth, and is calibrated by Shannon entropy to facilitate self-supervised training. Experiments across five public datasets and seven decoders show the proposed algorithm can be integrated seamlessly regardless of BCI paradigm and decoder architecture. In each iteration, the decoder is updated with a single online trial, which yields average accuracy gains of 4.9% on steady-state visual evoked potentials (SSVEP) and 3.6% on motor imagery. These results support fast-calibration operation and show that the proposed algorithm has great potential for BCI applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è„‘ç”µå›¾(EEG)è„‘æœºæ¥å£(BCI)ç³»ç»Ÿä¸­ä¸ªä½“è„‘æ´»åŠ¨å·®å¼‚å¯¼è‡´çš„åœ¨çº¿åº”ç”¨éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºåŒé˜¶æ®µå¯¹é½å’Œè‡ªç›‘ç£å­¦ä¹ çš„åœ¨çº¿è‡ªé€‚åº”ç®—æ³•ã€‚ç®—æ³•é¦–å…ˆåœ¨æ•°æ®ç©ºé—´åº”ç”¨Euclidean alignmentï¼Œå¹¶éšååœ¨è¡¨ç¤ºç©ºé—´æ›´æ–°Batch Normalizationç»Ÿè®¡é‡ï¼ŒåŒæ—¶è®¾è®¡äº†åˆ©ç”¨è§£ç å™¨ç”Ÿæˆçš„Soft Pseudo-labelså¹¶ç»Shannon Entropyæ ¡å‡†çš„è‡ªç›‘ç£æŸå¤±å‡½æ•°æ¥å®æ—¶æ›´æ–°è§£ç å™¨ã€‚å®éªŒè·¨è¶Šäº”ä¸ªå…¬å¼€æ•°æ®é›†å’Œä¸ƒç§è§£ç å™¨ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•å¯æ— ç¼é›†æˆäºä¸åŒçš„BCIèŒƒå¼ä¸æ¶æ„ä¸­ã€‚é€šè¿‡åœ¨æ¯ä¸ªè¿­ä»£ä¸­ä»…ä½¿ç”¨å•ä¸ªåœ¨çº¿æ ·æœ¬è¿›è¡Œæ›´æ–°ï¼Œè¯¥ç®—æ³•åœ¨ç¨³æ€è§†è§‰è¯±å‘ç”µä½(SSVEP)å’Œè¿åŠ¨æƒ³è±¡(Motor Imagery)ä»»åŠ¡ä¸Šåˆ†åˆ«å®ç°äº†4.9%å’Œ3.6%çš„å¹³å‡å‡†ç¡®ç‡æå‡ã€‚è¯¥ç ”ç©¶æœ‰æ•ˆæ”¯æŒäº†å¿«é€Ÿæ ¡å‡†(Fast-calibration)æ“ä½œï¼Œåœ¨æå‡BCIç³»ç»Ÿçš„å®ç”¨åŒ–ä¸é€šç”¨æ€§æ–¹é¢å±•ç°äº†å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19403v1",
      "published_date": "2025-09-23 07:38:37 UTC",
      "updated_date": "2025-09-23 07:38:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:39:18.474769+00:00"
    },
    {
      "arxiv_id": "2509.21376v1",
      "title": "In silico Deep Learning Protocols for Label-Free Super-Resolution Microscopy: A Comparative Study of Network Architectures and SNR Dependence",
      "title_zh": "æ— æ ‡è®°è¶…åˆ†è¾¨ç‡æ˜¾å¾®æˆåƒçš„è®¡ç®—æœºæ¨¡æ‹Ÿæ·±åº¦å­¦ä¹ æ–¹æ¡ˆï¼šç½‘ç»œæ¶æ„ä¸ä¿¡å™ªæ¯”ä¾èµ–æ€§çš„æ¯”è¾ƒç ”ç©¶",
      "authors": [
        "Shiraz S Kaderuppan",
        "Jonathan Mar",
        "Andrew Irvine",
        "Anurag Sharma",
        "Muhammad Ramadan Saifuddin",
        "Wai Leong Eugene Wong",
        "Wai Lok Woo"
      ],
      "abstract": "The field of optical microscopy spans across numerous industries and research domains, ranging from education to healthcare, quality inspection and analysis. Nonetheless, a key limitation often cited by optical microscopists refers to the limit of its lateral resolution (typically defined as ~200nm), with potential circumventions involving either costly external modules (e.g. confocal scan heads, etc) and/or specialized techniques [e.g. super-resolution (SR) fluorescent microscopy]. Addressing these challenges in a normal (non-specialist) context thus remains an aspect outside the scope of most microscope users & facilities. This study thus seeks to evaluate an alternative & economical approach to achieving SR optical microscopy, involving non-fluorescent phase-modulated microscopical modalities such as Zernike phase contrast (PCM) and differential interference contrast (DIC) microscopy. Two in silico deep neural network (DNN) architectures which we developed previously (termed O-Net and Theta-Net) are assessed on their abilities to resolve a custom-fabricated test target containing nanoscale features calibrated via atomic force microscopy (AFM). The results of our study demonstrate that although both O-Net and Theta-Net seemingly performed well when super-resolving these images, they were complementary (rather than competing) approaches to be considered for image SR, particularly under different image signal-to-noise ratios (SNRs). High image SNRs favoured the application of O-Net models, while low SNRs inclined preferentially towards Theta-Net models. These findings demonstrate the importance of model architectures (in conjunction with the source image SNR) on model performance and the SR quality of the generated images where DNN models are utilized for non-fluorescent optical nanoscopy, even where the same training dataset & number of epochs are being used.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å®ç°æ— æ ‡è®°è¶…åˆ†è¾¨ç‡(Super-Resolution)æ˜¾å¾®æˆåƒçš„ç»æµå‹æ–¹æ¡ˆï¼Œé‡ç‚¹è¯„ä¼°äº†Zernikeç›¸è¡¬(PCM)å’Œå¾®åˆ†å¹²æ¶‰(DIC)ç­‰éè§å…‰ç›¸ä½è°ƒåˆ¶æ¨¡æ€ã€‚ç ”ç©¶äººå‘˜å¯¹æ¯”äº†é¢„å…ˆå¼€å‘çš„O-Netå’ŒTheta-Netä¸¤ç§æ·±åº¦ç¥ç»ç½‘ç»œ(DNN)æ¶æ„åœ¨è§£æç»åŸå­åŠ›æ˜¾å¾®é•œ(AFM)æ ¡å‡†çš„çº³ç±³çº§ç‰¹å¾æ—¶çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ä¸¤ç§æ¨¡å‹åœ¨å›¾åƒè¶…åˆ†è¾¨ç‡ä»»åŠ¡ä¸­å…·æœ‰äº’è¡¥æ€§ï¼Œå…¶æ€§èƒ½è¡¨ç°æ˜¾è‘—å—å›¾åƒä¿¡å™ªæ¯”(SNR)å½±å“ã€‚åœ¨é«˜ä¿¡å™ªæ¯”(SNR)æ¡ä»¶ä¸‹ï¼ŒO-Netæ¨¡å‹è¡¨ç°æ›´ä¼˜ï¼Œè€Œä½ä¿¡å™ªæ¯”(SNR)åˆ™æ›´æœ‰åˆ©äºTheta-Netæ¨¡å‹çš„å‘æŒ¥ã€‚è¿™é¡¹å‘ç°å¼ºè°ƒäº†åœ¨éè§å…‰å…‰å­¦çº³ç±³æˆåƒä¸­ï¼Œæ¨¡å‹æ¶æ„ä¸æºå›¾åƒä¿¡å™ªæ¯”(SNR)çš„ååŒä½œç”¨å¯¹æå‡ç”Ÿæˆå›¾åƒè´¨é‡çš„å…³é”®æ„ä¹‰ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "20 pages, 10 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.21376v1",
      "published_date": "2025-09-23 07:32:40 UTC",
      "updated_date": "2025-09-23 07:32:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:39:40.293174+00:00"
    },
    {
      "arxiv_id": "2509.21375v1",
      "title": "Automated Prompt Generation for Creative and Counterfactual Text-to-image Synthesis",
      "title_zh": "é¢å‘åˆ›æ„ä¸åäº‹å®æ–‡ç”Ÿå›¾åˆæˆçš„è‡ªåŠ¨åŒ–æç¤ºè¯ç”Ÿæˆ",
      "authors": [
        "Aleksa Jelaca",
        "Ying Jiao",
        "Chang Tian",
        "Marie-Francine Moens"
      ],
      "abstract": "Text-to-image generation has advanced rapidly with large-scale multimodal training, yet fine-grained controllability remains a critical challenge. Counterfactual controllability, defined as the capacity to deliberately generate images that contradict common-sense patterns, remains a major challenge but plays a crucial role in enabling creativity and exploratory applications. In this work, we address this gap with a focus on counterfactual size (e.g., generating a tiny walrus beside a giant button) and propose an automatic prompt engineering framework that adapts base prompts into revised prompts for counterfactual images. The framework comprises three components: an image evaluator that guides dataset construction by identifying successful image generations, a supervised prompt rewriter that produces revised prompts, and a DPO-trained ranker that selects the optimal revised prompt. We construct the first counterfactual size text-image dataset and enhance the image evaluator by extending Grounded SAM with refinements, achieving a 114 percent improvement over its backbone. Experiments demonstrate that our method outperforms state-of-the-art baselines and ChatGPT-4o, establishing a foundation for future research on counterfactual controllability.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ–‡æœ¬ç”Ÿæˆå›¾åƒ(Text-to-image)é¢†åŸŸä¸­ç»†ç²’åº¦å¯æ§æ€§ä¸è¶³çš„é—®é¢˜ï¼Œé‡ç‚¹æ¢è®¨äº†åäº‹å®å¯æ§æ€§(Counterfactual controllability)ï¼Œå³ç”Ÿæˆè¿èƒŒå¸¸è¯†è§„å¾‹ï¼ˆå¦‚æå°çš„æµ·è±¡ä¸å·¨å¤§çš„æŒ‰é’®å¹¶æ’ï¼‰çš„åˆ›æ„å›¾åƒã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªè‡ªåŠ¨æç¤ºè¯å·¥ç¨‹æ¡†æ¶(Automatic prompt engineering framework)ï¼Œæ—¨åœ¨å°†åŸºç¡€æç¤ºè¯è½¬åŒ–ä¸ºèƒ½å¤Ÿç”Ÿæˆåäº‹å®å›¾åƒçš„ä¿®æ”¹åæç¤ºè¯ã€‚è¯¥æ¡†æ¶ç”±å¼•å¯¼æ•°æ®é›†æ„å»ºçš„å›¾åƒè¯„ä¼°å™¨(Image evaluator)ã€ç”Ÿæˆä¿®æ”¹æç¤ºè¯çš„ç›‘ç£å¼æç¤ºè¯é‡å†™å™¨(Supervised prompt rewriter)ä»¥åŠè´Ÿè´£ç­›é€‰æœ€ä¼˜æç¤ºè¯çš„DPOè®­ç»ƒæ’åºå™¨(DPO-trained ranker)ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ç»„æˆã€‚ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†é¦–ä¸ªåäº‹å®å°ºå¯¸ç›¸å…³çš„æ–‡æœ¬-å›¾åƒæ•°æ®é›†ï¼Œå¹¶é€šè¿‡æ”¹è¿›Grounded SAMä½¿å…¶æ€§èƒ½ç›¸è¾ƒäºåŸºå‡†æ¨¡å‹æå‡äº†114%ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åäº‹å®å›¾åƒç”Ÿæˆæ•ˆæœä¸Šä¼˜äºç›®å‰æœ€å…ˆè¿›çš„åŸºå‡†æ¨¡å‹åŠChatGPT-4oï¼Œä¸ºåˆ›æ„å’Œæ¢ç´¢æ€§åº”ç”¨ä¸­çš„åäº‹å®å¯æ§æ€§ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "text-to-image generation, automatic prompt, DPO, Counterfactual",
      "pdf_url": "https://arxiv.org/pdf/2509.21375v1",
      "published_date": "2025-09-23 07:22:31 UTC",
      "updated_date": "2025-09-23 07:22:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:39:37.697490+00:00"
    },
    {
      "arxiv_id": "2509.18714v3",
      "title": "A Generalized Bisimulation Metric of State Similarity between Markov Decision Processes: From Theoretical Propositions to Applications",
      "title_zh": "é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹é—´çŠ¶æ€ç›¸ä¼¼æ€§çš„å¹¿ä¹‰äº’æ¨¡æ‹Ÿåº¦é‡ï¼šä»ç†è®ºå‘½é¢˜åˆ°åº”ç”¨",
      "authors": [
        "Zhenyu Tao",
        "Wei Xu",
        "Xiaohu You"
      ],
      "abstract": "The bisimulation metric (BSM) is a powerful tool for computing state similarities within a Markov decision process (MDP), revealing that states closer in BSM have more similar optimal value functions. While BSM has been successfully utilized in reinforcement learning (RL) for tasks like state representation learning and policy exploration, its application to multiple-MDP scenarios, such as policy transfer, remains challenging. Prior work has attempted to generalize BSM to pairs of MDPs, but a lack of rigorous analysis of its mathematical properties has limited further theoretical progress. In this work, we formally establish a generalized bisimulation metric (GBSM) between pairs of MDPs, which is rigorously proven with the three fundamental properties: GBSM symmetry, inter-MDP triangle inequality, and the distance bound on identical state spaces. Leveraging these properties, we theoretically analyse policy transfer, state aggregation, and sampling-based estimation in MDPs, obtaining explicit bounds that are strictly tighter than those derived from the standard BSM. Additionally, GBSM provides a closed-form sample complexity for estimation, improving upon existing asymptotic results based on BSM. Numerical results validate our theoretical findings and demonstrate the effectiveness of GBSM in multi-MDP scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äº’æ¨¡æ‹Ÿåº¦é‡(Bisimulation Metric, BSM)åœ¨å¤šé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(Markov Decision Processes, MDPs)åœºæ™¯ï¼ˆå¦‚ç­–ç•¥è¿ç§»ï¼‰ä¸­åº”ç”¨å—é™çš„é—®é¢˜ï¼Œæ­£å¼æå‡ºäº†ä¸€ç§å¹¿ä¹‰äº’æ¨¡æ‹Ÿåº¦é‡(Generalized Bisimulation Metric, GBSM)ã€‚è®ºæ–‡é€šè¿‡ä¸¥è°¨çš„æ•°å­¦åˆ†æï¼Œç¡®ç«‹äº†GBSMåœ¨ä¸åŒMDPsä¹‹é—´çš„å¯¹ç§°æ€§(symmetry)ã€è·¨MDPä¸‰è§’ä¸ç­‰å¼(inter-MDP triangle inequality)ä»¥åŠç›¸åŒçŠ¶æ€ç©ºé—´ä¸Šçš„è·ç¦»ç•Œé™ç­‰ä¸‰å¤§æ ¸å¿ƒæ€§è´¨ã€‚åŸºäºè¿™äº›æ€§è´¨ï¼Œç ”ç©¶è€…å¯¹ç­–ç•¥è¿ç§»(policy transfer)ã€çŠ¶æ€èšåˆ(state aggregation)åŠåŸºäºé‡‡æ ·çš„ä¼°è®¡è¿›è¡Œäº†æ·±å…¥æ¢è®¨ï¼Œå¾—å‡ºçš„ç†è®ºç•Œé™æ¯”æ ‡å‡†BSMæ›´ä¸ºç´§è‡´ã€‚æ­¤å¤–ï¼ŒGBSMè¿˜ä¸ºä¼°è®¡è¿‡ç¨‹æä¾›äº†é—­å¼è§£æ ·æœ¬å¤æ‚åº¦(closed-form sample complexity)ï¼Œæ”¹è¿›äº†ç°æœ‰çš„æ¸è¿‘åˆ†æç»“æœã€‚æ•°å€¼å®éªŒç»“æœéªŒè¯äº†ä¸Šè¿°ç†è®ºå‘ç°ï¼Œå……åˆ†å±•ç¤ºäº†GBSMåœ¨å¤šMDPå¤æ‚åœºæ™¯ä¸­çš„åº”ç”¨ä»·å€¼ä¸æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "This paper is accepted by the 39th Conference on Neural Information Processing Systems (NeurIPS 2025)",
      "pdf_url": "https://arxiv.org/pdf/2509.18714v3",
      "published_date": "2025-09-23 07:02:05 UTC",
      "updated_date": "2025-11-03 01:42:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:39:44.592289+00:00"
    },
    {
      "arxiv_id": "2509.18713v1",
      "title": "MemOrb: A Plug-and-Play Verbal-Reinforcement Memory Layer for E-Commerce Customer Service",
      "title_zh": "MemOrbï¼šé¢å‘ç”µå•†å®¢æœçš„å³æ’å³ç”¨è¨€è¯­å¼ºåŒ–è®°å¿†å±‚",
      "authors": [
        "Yizhe Huang",
        "Yang Liu",
        "Ruiyu Zhao",
        "Xiaolong Zhong",
        "Xingming Yue",
        "Ling Jiang"
      ],
      "abstract": "Large Language Model-based agents(LLM-based agents) are increasingly deployed in customer service, yet they often forget across sessions, repeat errors, and lack mechanisms for continual self-improvement. This makes them unreliable in dynamic settings where stability and consistency are critical. To better evaluate these properties, we emphasize two indicators: task success rate as a measure of overall effectiveness, and consistency metrics such as Pass$^k$ to capture reliability across multiple trials. To address the limitations of existing approaches, we propose MemOrb, a lightweight and plug-and-play verbal reinforcement memory layer that distills multi-turn interactions into compact strategy reflections. These reflections are stored in a shared memory bank and retrieved to guide decision-making, without requiring any fine-tuning. Experiments show that MemOrb significantly improves both success rate and stability, achieving up to a 63 percentage-point gain in multi-turn success rate and delivering more consistent performance across repeated trials. Our results demonstrate that structured reflection is a powerful mechanism for enhancing long-term reliability of frozen LLM agents in customer service scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MemOrbï¼Œè¿™æ˜¯ä¸€ç§è½»é‡çº§ä¸”å³æ’å³ç”¨ï¼ˆplug-and-playï¼‰çš„è¯­è¨€å¼ºåŒ–è®°å¿†å±‚ï¼ˆverbal reinforcement memory layerï¼‰ï¼Œæ—¨åœ¨è§£å†³åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½ä½“ï¼ˆLLM-based agentsï¼‰åœ¨ç”µå•†å®¢æœåœºæ™¯ä¸­å®¹æ˜“é—å¿˜è·¨ä¼šè¯ä¿¡æ¯ã€é‡å¤çŠ¯é”™ä»¥åŠç¼ºä¹æŒç»­è‡ªæˆ‘æ”¹è¿›èƒ½åŠ›çš„é—®é¢˜ã€‚MemOrb å°†å¤šè½®äº¤äº’è¿‡ç¨‹æç‚¼ä¸ºç´§å‡‘çš„ç­–ç•¥åæ€ï¼ˆstrategy reflectionsï¼‰ï¼Œå¹¶å°†å…¶å­˜å‚¨åœ¨å…±äº«è®°å¿†åº“ä¸­ï¼Œé€šè¿‡æ£€ç´¢æ¥å¼•å¯¼æ¨¡å‹å†³ç­–ï¼Œè€Œæ— éœ€è¿›è¡Œä»»ä½•æ¨¡å‹å¾®è°ƒï¼ˆfine-tuningï¼‰ã€‚ç ”ç©¶é€šè¿‡å¼•å…¥ä»»åŠ¡æˆåŠŸç‡ï¼ˆtask success rateï¼‰å’Œ Pass$^k$ ç­‰ä¸€è‡´æ€§æŒ‡æ ‡ï¼ˆconsistency metricsï¼‰å¯¹ç³»ç»Ÿå¯é æ€§è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMemOrb æ˜¾è‘—æé«˜äº†æ™ºèƒ½ä½“çš„æˆåŠŸç‡å’Œç¨³å®šæ€§ï¼Œåœ¨å¤šè½®å¯¹è¯æˆåŠŸç‡ä¸Šå–å¾—äº†æœ€é«˜ 63 ä¸ªç™¾åˆ†ç‚¹çš„å¢ç›Šï¼Œå¹¶åœ¨é‡å¤å°è¯•ä¸­è¡¨ç°å‡ºæ›´å¼ºçš„ä¸€è‡´æ€§ã€‚è¯¥æˆæœè¯æ˜äº†ç»“æ„åŒ–åæ€æœºåˆ¶æ˜¯æå‡å†»ç»“çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆfrozen LLM agentsï¼‰åœ¨åŠ¨æ€å®¢æœç¯å¢ƒä¸­é•¿æœŸå¯é æ€§çš„å…³é”®æ‰‹æ®µã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18713v1",
      "published_date": "2025-09-23 06:57:07 UTC",
      "updated_date": "2025-09-23 06:57:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:39:52.890023+00:00"
    },
    {
      "arxiv_id": "2509.18711v2",
      "title": "RSVG-ZeroOV: Exploring a Training-Free Framework for Zero-Shot Open-Vocabulary Visual Grounding in Remote Sensing Images",
      "title_zh": "RSVG-ZeroOVï¼šé¥æ„Ÿå›¾åƒé›¶æ ·æœ¬å¼€æ”¾è¯æ±‡è§†è§‰å®šä½çš„å…è®­ç»ƒæ¡†æ¶æ¢ç´¢",
      "authors": [
        "Ke Li",
        "Di Wang",
        "Ting Wang",
        "Fuyu Dong",
        "Yiming Zhang",
        "Luyao Zhang",
        "Xiangyu Wang",
        "Shaofeng Li",
        "Quan Wang"
      ],
      "abstract": "Remote sensing visual grounding (RSVG) aims to localize objects in remote sensing images based on free-form natural language expressions. Existing approaches are typically constrained to closed-set vocabularies, limiting their applicability in open-world scenarios. While recent attempts to leverage generic foundation models for open-vocabulary RSVG, they overly rely on expensive high-quality datasets and time-consuming fine-tuning. To address these limitations, we propose \\textbf{RSVG-ZeroOV}, a training-free framework that aims to explore the potential of frozen generic foundation models for zero-shot open-vocabulary RSVG. Specifically, RSVG-ZeroOV comprises three key stages: (i) Overview: We utilize a vision-language model (VLM) to obtain cross-attention\\footnote[1]{In this paper, although decoder-only VLMs use self-attention over all tokens, we refer to the image-text interaction part as cross-attention to distinguish it from pure visual self-attention.}maps that capture semantic correlations between text queries and visual regions. (ii) Focus: By leveraging the fine-grained modeling priors of a diffusion model (DM), we fill in gaps in structural and shape information of objects, which are often overlooked by VLM. (iii) Evolve: A simple yet effective attention evolution module is introduced to suppress irrelevant activations, yielding purified segmentation masks over the referred objects. Without cumbersome task-specific training, RSVG-ZeroOV offers an efficient and scalable solution. Extensive experiments demonstrate that the proposed framework consistently outperforms existing weakly-supervised and zero-shot methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†RSVG-ZeroOVï¼Œä¸€ä¸ªé’ˆå¯¹é¥æ„Ÿè§†è§‰å®šä½(RSVG)ä»»åŠ¡çš„æ— éœ€è®­ç»ƒçš„æ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨å†»ç»“çš„é€šç”¨åŸºç¡€æ¨¡å‹å®ç°é›¶æ ·æœ¬(Zero-shot)å¼€æ”¾è¯æ±‡(Open-vocabulary)å®šä½ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•å—é™äºé—­é›†è¯æ±‡ä¸”ä¾èµ–æ˜‚è´µå¾®è°ƒçš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶é€šè¿‡ä¸‰ä¸ªé˜¶æ®µè¿›è¡Œæ¨ç†ï¼šOverviewé˜¶æ®µåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹(VLM)æ•æ‰æ–‡æœ¬æŸ¥è¯¢ä¸è§†è§‰åŒºåŸŸçš„è¯­ä¹‰å…³è”ï¼›Focusé˜¶æ®µå¼•å…¥æ‰©æ•£æ¨¡å‹(Diffusion Model)çš„ç»†ç²’åº¦å…ˆéªŒä»¥å¡«è¡¥ç›®æ ‡ç»“æ„å’Œå½¢çŠ¶ä¿¡æ¯çš„ç©ºç¼ºï¼›Evolveé˜¶æ®µé€šè¿‡æ³¨æ„åŠ›æ¼”åŒ–æ¨¡å—æŠ‘åˆ¶æ— å…³æ¿€æ´»å¹¶æå–çº¯å‡€çš„åˆ†å‰²æ©ç ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRSVG-ZeroOVåœ¨æ— éœ€ç‰¹å®šä»»åŠ¡è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½æŒç»­ä¼˜äºç°æœ‰çš„å¼±ç›‘ç£å’Œé›¶æ ·æœ¬æ–¹æ³•ã€‚è¯¥ç ”ç©¶ä¸ºå¼€æ”¾ä¸–ç•Œåœºæ™¯ä¸‹çš„é¥æ„Ÿå›¾åƒç†è§£æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å…·å¤‡å¯æ‰©å±•æ€§çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This work is accepted by AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2509.18711v2",
      "published_date": "2025-09-23 06:52:15 UTC",
      "updated_date": "2025-11-11 07:35:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:39:53.185320+00:00"
    },
    {
      "arxiv_id": "2509.18710v2",
      "title": "Autonomous Data Agents: A New Opportunity for Smart Data",
      "title_zh": "è‡ªä¸»æ•°æ®æ™ºèƒ½ä½“ï¼šæ™ºèƒ½æ•°æ®çš„æ–°æœºé‡",
      "authors": [
        "Yanjie Fu",
        "Dongjie Wang",
        "Wangyang Ying",
        "Xinyuan Wang",
        "Xiangliang Zhang",
        "Huan Liu",
        "Jian Pei"
      ],
      "abstract": "As data continues to grow in scale and complexity, preparing, transforming, and analyzing it remains labor-intensive, repetitive, and difficult to scale. Since data contains knowledge and AI learns knowledge from it, the alignment between AI and data is essential. However, data is often not structured in ways that are optimal for AI utilization. Moreover, an important question arises: how much knowledge can we pack into data through intensive data operations? Autonomous data agents (DataAgents), which integrate LLM reasoning with task decomposition, action reasoning and grounding, and tool calling, can autonomously interpret data task descriptions, decompose tasks into subtasks, reason over actions, ground actions into python code or tool calling, and execute operations. Unlike traditional data management and engineering tools, DataAgents dynamically plan workflows, call powerful tools, and adapt to diverse data tasks at scale. This report argues that DataAgents represent a paradigm shift toward autonomous data-to-knowledge systems. DataAgents are capable of handling collection, integration, preprocessing, selection, transformation, reweighing, augmentation, reprogramming, repairs, and retrieval. Through these capabilities, DataAgents transform complex and unstructured data into coherent and actionable knowledge. We first examine why the convergence of agentic AI and data-to-knowledge systems has emerged as a critical trend. We then define the concept of DataAgents and discuss their architectural design, training strategies, as well as the new skills and capabilities they enable. Finally, we call for concerted efforts to advance action workflow optimization, establish open datasets and benchmark ecosystems, safeguard privacy, balance efficiency with scalability, and develop trustworthy DataAgent guardrails to prevent malicious actions.",
      "tldr_zh": "è¯¥æŠ¥å‘Šæ¢è®¨äº†éšç€æ•°æ®è§„æ¨¡å’Œå¤æ‚æ€§å¢åŠ ï¼Œä¼ ç»Ÿæ•°æ®å¤„ç†é¢ä¸´çš„åŠ³åŠ¨å¯†é›†å‹å±€é™ï¼Œå¹¶æå‡ºäº†è‡ªä¸»æ•°æ®æ™ºèƒ½ä½“ (Autonomous Data Agents, DataAgents) è¿™ä¸€è¿ˆå‘è‡ªä¸»æ•°æ®åˆ°çŸ¥è¯†ç³»ç»Ÿ (data-to-knowledge systems) çš„èŒƒå¼è½¬å˜ã€‚DataAgents é€šè¿‡æ•´åˆå¤§è¯­è¨€æ¨¡å‹ (LLM) æ¨ç†ã€ä»»åŠ¡åˆ†è§£ã€è¡ŒåŠ¨æ¨ç†ä¸è½åœ°ä»¥åŠå·¥å…·è°ƒç”¨ï¼Œèƒ½å¤Ÿè‡ªä¸»è§£é‡Šä»»åŠ¡æè¿°å¹¶å°†å…¶è½¬åŒ–ä¸ºå¯æ‰§è¡Œçš„ Python ä»£ç ã€‚ä¸ä¼ ç»Ÿå·¥å…·ä¸åŒï¼Œè¯¥æ™ºèƒ½ä½“èƒ½åŠ¨æ€è§„åˆ’å·¥ä½œæµï¼Œå¤„ç†åŒ…æ‹¬æ•°æ®é‡‡é›†ã€é›†æˆã€é¢„å¤„ç†ã€è½¬æ¢ã€å¢å¼ºã€ä¿®å¤åŠæ£€ç´¢åœ¨å†…çš„å…¨æµç¨‹ä»»åŠ¡ï¼Œå°†å¤æ‚ä¸”æ— åºçš„æ•°æ®è½¬åŒ–ä¸ºè¿è´¯çš„çŸ¥è¯†ã€‚æŠ¥å‘Šå®šä¹‰äº† DataAgents çš„æ¦‚å¿µï¼Œæ·±å…¥è®¨è®ºäº†å…¶æ¶æ„è®¾è®¡ã€è®­ç»ƒç­–ç•¥ä»¥åŠæ‰€èƒ½å®ç°çš„æ–°èƒ½åŠ›ã€‚æœ€åï¼Œä½œè€…å¼ºè°ƒäº†ä¼˜åŒ–è¡ŒåŠ¨å·¥ä½œæµã€å»ºç«‹åŸºå‡†ç”Ÿæ€ç³»ç»Ÿã€ä¿æŠ¤éšç§ä»¥åŠå¼€å‘å®‰å…¨æŠ¤æ  (guardrails) çš„å¿…è¦æ€§ï¼Œä»¥ç¡®ä¿ DataAgents çš„é«˜æ•ˆã€å¯æ‰©å±•ä¸”å¯ä¿¡ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18710v2",
      "published_date": "2025-09-23 06:46:41 UTC",
      "updated_date": "2025-10-04 00:36:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:40:01.695127+00:00"
    },
    {
      "arxiv_id": "2509.18691v1",
      "title": "An overview of neural architectures for self-supervised audio representation learning from masked spectrograms",
      "title_zh": "åŸºäºæ©ç è¯­è°±å›¾çš„è‡ªç›‘ç£éŸ³é¢‘è¡¨ç¤ºå­¦ä¹ ç¥ç»ç½‘ç»œæ¶æ„ç»¼è¿°",
      "authors": [
        "Sarthak Yadav",
        "Sergios Theodoridis",
        "Zheng-Hua Tan"
      ],
      "abstract": "In recent years, self-supervised learning has amassed significant interest for training deep neural representations without labeled data. One such self-supervised learning approach is masked spectrogram modeling, where the objective is to learn semantically rich contextual representations by predicting removed or hidden portions of the input audio spectrogram. With the Transformer neural architecture at its core, masked spectrogram modeling has emerged as the prominent approach for learning general purpose audio representations, a.k.a. audio foundation models. Meanwhile, addressing the issues of the Transformer architecture, in particular the underlying Scaled Dot-product Attention operation, which scales quadratically with input sequence length, has led to renewed interest in recurrent sequence modeling approaches. Among them, Selective structured state space models (such as Mamba) and extended Long Short-Term Memory (xLSTM) are the two most promising approaches which have experienced widespread adoption. While the body of work on these two topics continues to grow, there is currently a lack of an adequate overview encompassing the intersection of these topics. In this paper, we present a comprehensive overview of the aforementioned research domains, covering masked spectrogram modeling and the previously mentioned neural sequence modeling architectures, Mamba and xLSTM. Further, we compare Transformers, Mamba and xLSTM based masked spectrogram models in a unified, reproducible framework on ten diverse downstream audio classification tasks, which will help interested readers to make informed decisions regarding suitability of the evaluated approaches to adjacent applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºæ©ç é¢‘è°±å»ºæ¨¡(masked spectrogram modeling)çš„è‡ªç›‘ç£éŸ³é¢‘è¡¨ç¤ºå­¦ä¹ (self-supervised audio representation learning)ç¥ç»æ¶æ„æä¾›äº†å…¨é¢çš„ç»¼è¿°ã€‚é’ˆå¯¹Transformeræ¶æ„ä¸­ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›(Scaled Dot-product Attention)éšåºåˆ—é•¿åº¦å‘ˆäºŒæ¬¡æ–¹å¢é•¿çš„å±€é™æ€§ï¼Œæœ¬æ–‡é‡ç‚¹æ¢è®¨äº†é€‰æ‹©æ€§ç»“æ„åŒ–çŠ¶æ€ç©ºé—´æ¨¡å‹(Selective structured state space modelsï¼Œå¦‚Mamba)å’Œæ‰©å±•é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ(xLSTM)è¿™ä¸¤ç±»å…·æœ‰æ½œåŠ›çš„æ›¿ä»£æ–¹æ¡ˆã€‚æ–‡ç« è¯¦ç»†æ¢³ç†äº†è¿™äº›æ¶æ„åœ¨éŸ³é¢‘åŸºç¡€æ¨¡å‹(audio foundation models)ä¸­çš„åº”ç”¨ç°çŠ¶ï¼Œå¡«è¡¥äº†å½“å‰ç ”ç©¶ä¸­å¯¹è¿™äº›å‰æ²¿æŠ€æœ¯äº¤é›†ç»¼è¿°çš„ç©ºç™½ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…åœ¨ä¸€ä¸ªç»Ÿä¸€ä¸”å¯å¤ç°çš„æ¡†æ¶ä¸‹ï¼Œé€šè¿‡åé¡¹å¤šæ ·çš„ä¸‹æ¸¸éŸ³é¢‘åˆ†ç±»ä»»åŠ¡å¯¹Transformerã€Mambaå’ŒxLSTMçš„è¡¨ç°è¿›è¡Œäº†æ·±å…¥æ¯”è¾ƒã€‚è¿™ä¸€è¯„ä¼°ç»“æœä¸ºå¼€å‘è€…åœ¨é¢å¯¹ä¸åŒéŸ³é¢‘åº”ç”¨åœºæ™¯æ—¶ï¼Œå¦‚ä½•é€‰æ‹©æœ€åˆé€‚çš„æ¨¡å‹æ¶æ„æä¾›äº†ç§‘å­¦çš„å†³ç­–æ”¯æŒã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18691v1",
      "published_date": "2025-09-23 06:20:41 UTC",
      "updated_date": "2025-09-23 06:20:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:40:06.989588+00:00"
    },
    {
      "arxiv_id": "2509.18690v1",
      "title": "Advances in Large Language Models for Medicine",
      "title_zh": "åŒ»å­¦å¤§è¯­è¨€æ¨¡å‹çš„ç ”ç©¶è¿›å±•",
      "authors": [
        "Zhiyu Kan",
        "Wensheng Gan",
        "Zhenlian Qi",
        "Philip S. Yu"
      ],
      "abstract": "Artificial intelligence (AI) technology has advanced rapidly in recent years, with large language models (LLMs) emerging as a significant breakthrough. LLMs are increasingly making an impact across various industries, with the medical field standing out as the most prominent application area. This paper systematically reviews the up-to-date research progress of LLMs in the medical field, providing an in-depth analysis of training techniques for large medical models, their adaptation in healthcare settings, related applications, as well as their strengths and limitations. Furthermore, it innovatively categorizes medical LLMs into three distinct types based on their training methodologies and classifies their evaluation approaches into two categories. Finally, the study proposes solutions to existing challenges and outlines future research directions based on identified issues in the field of medical LLMs. By systematically reviewing previous and advanced research findings, we aim to highlight the necessity of developing medical LLMs, provide a deeper understanding of their current state of development, and offer clear guidance for subsequent research.",
      "tldr_zh": "è¯¥ç ”ç©¶ç³»ç»Ÿæ€§åœ°ç»¼è¿°äº†å¤§å‹è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)åœ¨åŒ»å­¦é¢†åŸŸçš„æœ€æ–°ç ”ç©¶è¿›å±•ï¼Œæ·±å…¥åˆ†æäº†å…¶è®­ç»ƒæŠ€æœ¯(training techniques)ã€åŒ»ç–—åœºæ™¯é€‚é…åŠå…¶ç›¸å…³åº”ç”¨ã€‚è®ºæ–‡åˆ›æ–°åœ°æ ¹æ®è®­ç»ƒæ–¹æ³•å°†åŒ»å­¦ LLMs åˆ†ä¸ºä¸‰ä¸ªç‹¬ç‰¹ç±»å‹ï¼Œå¹¶å°†å…¶è¯„ä¼°æ–¹æ³•(evaluation approaches)å½’çº³ä¸ºä¸¤ä¸ªç±»åˆ«ã€‚æ–‡ç« ä¸ä»…æ¢è®¨äº†å½“å‰å¤§æ¨¡å‹çš„ä¼˜åŠ¿ä¸å±€é™æ€§ï¼Œè¿˜é’ˆå¯¹ç°æœ‰æŒ‘æˆ˜æå‡ºäº†å…·ä½“çš„è§£å†³æ–¹æ¡ˆã€‚æœ€åï¼Œè¯¥ç ”ç©¶æ˜ç¡®äº†åŒ»å­¦ LLMs çš„æœªæ¥ç ”ç©¶æ–¹å‘ï¼Œæ—¨åœ¨ä¸ºè¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥å¼€å‘æä¾›æ·±åº¦ç†è§£ä¸æ˜ç¡®çš„ç§‘ç ”æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Preprint. 5 figures, 4 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.18690v1",
      "published_date": "2025-09-23 06:16:39 UTC",
      "updated_date": "2025-09-23 06:16:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:40:08.086345+00:00"
    },
    {
      "arxiv_id": "2509.18683v1",
      "title": "LEAF-Mamba: Local Emphatic and Adaptive Fusion State Space Model for RGB-D Salient Object Detection",
      "title_zh": "LEAF-Mambaï¼šé¢å‘ RGB-D æ˜¾è‘—æ€§ç›®æ ‡æ£€æµ‹çš„å±€éƒ¨å¼ºåŒ–ä¸è‡ªé€‚åº”èåˆçŠ¶æ€ç©ºé—´æ¨¡å‹",
      "authors": [
        "Lanhu Wu",
        "Zilin Gao",
        "Hao Fei",
        "Mong-Li Lee",
        "Wynne Hsu"
      ],
      "abstract": "RGB-D salient object detection (SOD) aims to identify the most conspicuous objects in a scene with the incorporation of depth cues. Existing methods mainly rely on CNNs, limited by the local receptive fields, or Vision Transformers that suffer from the cost of quadratic complexity, posing a challenge in balancing performance and computational efficiency. Recently, state space models (SSM), Mamba, have shown great potential for modeling long-range dependency with linear complexity. However, directly applying SSM to RGB-D SOD may lead to deficient local semantics as well as the inadequate cross-modality fusion. To address these issues, we propose a Local Emphatic and Adaptive Fusion state space model (LEAF-Mamba) that contains two novel components: 1) a local emphatic state space module (LE-SSM) to capture multi-scale local dependencies for both modalities. 2) an SSM-based adaptive fusion module (AFM) for complementary cross-modality interaction and reliable cross-modality integration. Extensive experiments demonstrate that the LEAF-Mamba consistently outperforms 16 state-of-the-art RGB-D SOD methods in both efficacy and efficiency. Moreover, our method can achieve excellent performance on the RGB-T SOD task, proving a powerful generalization ability.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹ RGB-D æ˜¾è‘—ç›®æ ‡æ£€æµ‹ (Salient Object Detection, SOD) é¢†åŸŸä¸­ CNN å—é™äºå±€éƒ¨æ„Ÿå—é‡ä»¥åŠ Vision Transformers è®¡ç®—å¤æ‚åº¦è¿‡é«˜çš„é—®é¢˜ï¼Œæå‡ºäº† LEAF-Mamba æ¨¡å‹ã€‚è¯¥æ¨¡å‹åˆ©ç”¨å…·æœ‰çº¿æ€§å¤æ‚åº¦çš„çŠ¶æ€ç©ºé—´æ¨¡å‹ (State Space Model, SSM) Mambaï¼Œæ—¨åœ¨è§£å†³ç›´æ¥åº”ç”¨ SSM æ—¶å¯èƒ½å‡ºç°çš„å±€éƒ¨è¯­ä¹‰ç¼ºå¤±åŠè·¨æ¨¡æ€èåˆä¸è¶³çš„é—®é¢˜ã€‚LEAF-Mamba åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šå±€éƒ¨å¼ºè°ƒçŠ¶æ€ç©ºé—´æ¨¡å— (Local Emphatic State Space Module, LE-SSM) ç”¨äºæ•è·åŒæ¨¡æ€çš„å¤šå°ºåº¦å±€éƒ¨ä¾èµ–ï¼Œä»¥åŠåŸºäº SSM çš„è‡ªé€‚åº”èåˆæ¨¡å— (Adaptive Fusion Module, AFM) ä»¥å®ç°å¯é çš„è·¨æ¨¡æ€äº¤äº’ä¸é›†æˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLEAF-Mamba åœ¨æ€§èƒ½å’Œæ•ˆç‡ä¸Šå‡ä¸€è‡´ä¼˜äº 16 ç§æœ€å…ˆè¿›çš„ RGB-D SOD æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨ RGB-T SOD ä»»åŠ¡ä¸­ä¹Ÿå–å¾—äº†ä¼˜å¼‚è¡¨ç°ï¼Œè¯æ˜äº†å…¶å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ACM MM 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.18683v1",
      "published_date": "2025-09-23 06:08:17 UTC",
      "updated_date": "2025-09-23 06:08:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:40:12.897541+00:00"
    },
    {
      "arxiv_id": "2509.18681v1",
      "title": "Implementation of airborne ML models with semantics preservation",
      "title_zh": "æœºè½½æœºå™¨å­¦ä¹ æ¨¡å‹çš„è¯­ä¹‰ä¿æŒå®ç°",
      "authors": [
        "Nicolas Valot",
        "Louis Fabre",
        "Benjamin Lesage",
        "Ammar Mechouche",
        "Claire Pagetti"
      ],
      "abstract": "Machine Learning (ML) may offer new capabilities in airborne systems. However, as any piece of airborne systems, ML-based systems will be required to guarantee their safe operation. Thus, their development will have to be demonstrated to be compliant with the adequate guidance. So far, the European Union Aviation Safety Agency (EASA) has published a concept paper and an EUROCAE/SAE group is preparing ED-324. Both approaches delineate high-level objectives to confirm the ML model achieves its intended function and maintains training performance in the target environment. The paper aims to clarify the difference between an ML model and its corresponding unambiguous description, referred to as the Machine Learning Model Description (MLMD). It then refines the essential notion of semantics preservation to ensure the accurate replication of the model. We apply our contributions to several industrial use cases to build and compare several target models.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨æœºè½½ç³»ç»Ÿ(airborne systems)ä¸­å®ç°æœºå™¨å­¦ä¹ (Machine Learning)æ¨¡å‹çš„å®‰å…¨æ€§ä¸åˆè§„æ€§é—®é¢˜ï¼Œæ—¨åœ¨æ»¡è¶³æ¬§æ´²èˆªç©ºå®‰å…¨å±€(EASA)åŠED-324æ ‡å‡†çš„ç›¸å…³æŒ‡å¯¼è¦æ±‚ã€‚è®ºæ–‡æ˜ç¡®åŒºåˆ†äº†æœºå™¨å­¦ä¹ æ¨¡å‹ä¸å…¶å¯¹åº”çš„æ— æ­§ä¹‰æè¿°ï¼Œå³æœºå™¨å­¦ä¹ æ¨¡å‹æè¿°(Machine Learning Model Description, MLMD)ï¼Œè¿™æ˜¯ç¡®ä¿æ¨¡å‹åœ¨ç›®æ ‡ç¯å¢ƒä¸­å‡†ç¡®å¤åˆ¶çš„å…³é”®åŸºç¡€ã€‚ç ”ç©¶é‡ç‚¹ç»†åŒ–äº†è¯­ä¹‰ä¿æŒ(semantics preservation)çš„æ ¸å¿ƒæ¦‚å¿µï¼Œæ—¨åœ¨ä¿è¯æ¨¡å‹ä»å¼€å‘ç¯å¢ƒè¿ç§»è‡³ç›®æ ‡ç¯å¢ƒæ—¶ï¼Œèƒ½å¤Ÿç»´æŒå…¶é¢„å®šçš„åŠŸèƒ½ä¸è®­ç»ƒæ€§èƒ½ã€‚é€šè¿‡å°†è¯¥æ–¹æ³•åº”ç”¨äºå¤šä¸ªå·¥ä¸šç”¨ä¾‹ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºå¹¶å¯¹æ¯”äº†å¤šç§ç›®æ ‡æ¨¡å‹ï¼ŒéªŒè¯äº†è¯¥æ–¹æ¡ˆåœ¨ç¡®ä¿æ¨¡å‹å®ç°ä¸€è‡´æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚è¯¥å·¥ä½œä¸ºèˆªç©ºé¢†åŸŸæœºå™¨å­¦ä¹ æ¨¡å‹çš„è§„èŒƒåŒ–éƒ¨ç½²æä¾›äº†æŠ€æœ¯æ”¯æ’‘ï¼Œç¡®ä¿äº†å…³é”®ä»»åŠ¡ç³»ç»Ÿåœ¨å®é™…è¿è¡Œä¸­çš„å®‰å…¨æ€§å’Œå¯é æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18681v1",
      "published_date": "2025-09-23 06:01:52 UTC",
      "updated_date": "2025-09-23 06:01:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:40:20.086425+00:00"
    },
    {
      "arxiv_id": "2509.20394v1",
      "title": "Blueprints of Trust: AI System Cards for End to End Transparency and Governance",
      "title_zh": "ä¿¡ä»»è“å›¾ï¼šé¢å‘ç«¯åˆ°ç«¯é€æ˜åº¦ä¸æ²»ç†çš„ AI ç³»ç»Ÿå¡",
      "authors": [
        "Huzaifa Sidhpurwala",
        "Emily Fox",
        "Garth Mollett",
        "Florencio Cano Gabarda",
        "Roman Zhukov"
      ],
      "abstract": "This paper introduces the Hazard-Aware System Card (HASC), a novel framework designed to enhance transparency and accountability in the development and deployment of AI systems. The HASC builds upon existing model card and system card concepts by integrating a comprehensive, dynamic record of an AI system's security and safety posture. The framework proposes a standardized system of identifiers, including a novel AI Safety Hazard (ASH) ID, to complement existing security identifiers like CVEs, allowing for clear and consistent communication of fixed flaws. By providing a single, accessible source of truth, the HASC empowers developers and stakeholders to make more informed decisions about AI system safety throughout its lifecycle. Ultimately, we also compare our proposed AI system cards with the ISO/IEC 42001:2023 standard and discuss how they can be used to complement each other, providing greater transparency and accountability for AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†å±å®³æ„ŸçŸ¥ç³»ç»Ÿå¡ç‰‡(Hazard-Aware System Card, HASC)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨å¢å¼ºäººå·¥æ™ºèƒ½ç³»ç»Ÿå¼€å‘ä¸éƒ¨ç½²é€æ˜åº¦åŠé—®è´£åˆ¶çš„æ–°å‹æ¡†æ¶ã€‚HASC åœ¨ç°æœ‰æ¨¡å‹å¡ç‰‡å’Œç³»ç»Ÿå¡ç‰‡æ¦‚å¿µçš„åŸºç¡€ä¸Šï¼Œé€šè¿‡æ•´åˆ AI ç³»ç»Ÿå®‰å…¨æ€åŠ¿çš„å…¨é¢åŠ¨æ€è®°å½•ï¼Œå»ºç«‹äº†æ ‡å‡†åŒ–çš„æ ‡è¯†ç¬¦ç³»ç»Ÿã€‚è¯¥æ¡†æ¶å¼•å…¥äº†åˆ›æ–°çš„ AI å®‰å…¨å±å®³(AI Safety Hazard, ASH) ID ä»¥è¡¥å…… CVE ç­‰ç°æœ‰å®‰å…¨æ ‡è¯†ç¬¦ï¼Œç¡®ä¿å¯¹å·²ä¿®å¤ç¼ºé™·è¿›è¡Œæ¸…æ™°ä¸”ä¸€è‡´çš„æ²Ÿé€šã€‚ä½œä¸ºå•ä¸€ä¸”å¯è®¿é—®çš„çœŸå®ä¿¡æ¯æºï¼ŒHASC èƒ½å¤Ÿèµ‹èƒ½å¼€å‘äººå‘˜å’Œåˆ©ç›Šç›¸å…³è€…åœ¨ AI ç³»ç»Ÿå…¨ç”Ÿå‘½å‘¨æœŸå†…åšå‡ºæ›´æ˜æ™ºçš„å®‰å…¨å†³ç­–ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡ä¸ ISO/IEC 42001:2023 æ ‡å‡†çš„å¯¹æ¯”åˆ†æï¼Œé˜è¿°äº†ä¸¤è€…å¦‚ä½•äº’è¡¥ä»¥è¿›ä¸€æ­¥æå‡ AI æ²»ç†çš„é€æ˜åº¦ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.CR"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.20394v1",
      "published_date": "2025-09-23 05:58:32 UTC",
      "updated_date": "2025-09-23 05:58:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:41:06.260762+00:00"
    },
    {
      "arxiv_id": "2509.18672v1",
      "title": "NaviSense: A Multimodal Assistive Mobile application for Object Retrieval by Persons with Visual Impairment",
      "title_zh": "NaviSenseï¼šé¢å‘è§†éšœäººå£«ç‰©ä½“æ£€ç´¢çš„å¤šæ¨¡æ€è¾…åŠ©ç§»åŠ¨åº”ç”¨",
      "authors": [
        "Ajay Narayanan Sridhar",
        "Fuli Qiao",
        "Nelson Daniel Troncoso Aldas",
        "Yanpei Shi",
        "Mehrdad Mahdavi",
        "Laurent Itti",
        "Vijaykrishnan Narayanan"
      ],
      "abstract": "People with visual impairments often face significant challenges in locating and retrieving objects in their surroundings. Existing assistive technologies present a trade-off: systems that offer precise guidance typically require pre-scanning or support only fixed object categories, while those with open-world object recognition lack spatial feedback for reaching the object. To address this gap, we introduce 'NaviSense', a mobile assistive system that combines conversational AI, vision-language models, augmented reality (AR), and LiDAR to support open-world object detection with real-time audio-haptic guidance. Users specify objects via natural language and receive continuous spatial feedback to navigate toward the target without needing prior setup. Designed with insights from a formative study and evaluated with 12 blind and low-vision participants, NaviSense significantly reduced object retrieval time and was preferred over existing tools, demonstrating the value of integrating open-world perception with precise, accessible guidance.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†NaviSenseï¼Œè¿™æ˜¯ä¸€æ¬¾ä¸“ä¸ºè§†éšœäººå£«(Persons with Visual Impairment)è®¾è®¡çš„ç§»åŠ¨è¾…åŠ©ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³ä»–ä»¬åœ¨ç¯å¢ƒç‰©ä½“æ£€ç´¢ä¸­é¢ä¸´çš„å®šä½ä¸å¯¼å‘æŒ‘æˆ˜ã€‚ç³»ç»Ÿåˆ›æ–°æ€§åœ°æ•´åˆäº†å¯¹è¯å¼AI(Conversational AI)ã€è§†è§‰è¯­è¨€æ¨¡å‹(Vision-Language Models)ã€å¢å¼ºç°å®(AR)ä»¥åŠLiDARæŠ€æœ¯ï¼Œå®ç°äº†æ— éœ€é¢„å…ˆæ‰«æç¯å¢ƒçš„å¼€æ”¾ä¸–ç•Œç‰©ä½“æ£€æµ‹(Open-world Object Detection)ã€‚ç”¨æˆ·é€šè¿‡è‡ªç„¶è¯­è¨€å³å¯æŒ‡å®šç›®æ ‡ï¼Œå¹¶ä¾æ‰˜å®æ—¶çš„éŸ³é¢‘ä¸è§¦è§‰ç©ºé—´åé¦ˆè·å–ç²¾ç¡®çš„å¯¼èˆªæŒ‡å¼•ã€‚åœ¨é’ˆå¯¹12åç›²äººåŠä½è§†åŠ›å—è¯•è€…çš„å®éªŒè¯„ä¼°ä¸­ï¼ŒNaviSenseæ˜¾è‘—ç¼©çŸ­äº†ç‰©ä½“æ£€ç´¢æ—¶é—´ï¼Œä¸”åœ¨ç”¨æˆ·åå¥½åº¦ä¸Šä¼˜äºç°æœ‰å·¥å…·ã€‚è¯¥ç ”ç©¶å……åˆ†è¯æ˜äº†å°†å¼€æ”¾ä¸–ç•Œæ„ŸçŸ¥ä¸é«˜ç²¾åº¦ç©ºé—´å¯¼å¼•ç›¸ç»“åˆåœ¨æå‡è¾…åŠ©æŠ€æœ¯æ˜“ç”¨æ€§æ–¹é¢çš„é‡è¦ä»·å€¼ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18672v1",
      "published_date": "2025-09-23 05:45:11 UTC",
      "updated_date": "2025-09-23 05:45:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:40:49.284143+00:00"
    },
    {
      "arxiv_id": "2509.18667v3",
      "title": "TERAG: Token-Efficient Graph-Based Retrieval-Augmented Generation",
      "title_zh": "TERAGï¼šåŸºäºå›¾çš„ Token é«˜æ•ˆæ£€ç´¢å¢å¼ºç”Ÿæˆ",
      "authors": [
        "Qiao Xiao",
        "Hong Ting Tsang",
        "Jiaxin Bai"
      ],
      "abstract": "Graph-based Retrieval-augmented generation (RAG) has become a widely studied approach for improving the reasoning, accuracy, and factuality of Large Language Models (LLMs). However, many existing graph-based RAG systems overlook the high cost associated with LLM token usage during graph construction, hindering large-scale adoption. To address this, we propose TERAG, a simple yet effective framework designed to build informative graphs at a significantly lower cost. Inspired by HippoRAG, we incorporate Personalized PageRank (PPR) during the retrieval phase, and we achieve at least 80% of the accuracy of widely used graph-based RAG methods while consuming only 3%-11% of the output tokens. With its low token footprint and efficient construction pipeline, TERAG is well-suited for large-scale and cost-sensitive deployment scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†TERAGï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹åŸºäºå›¾çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ(Graph-based RAG)ç³»ç»Ÿé«˜æ˜‚Tokenæˆæœ¬é—®é¢˜è€Œè®¾è®¡çš„ç®€æ˜“é«˜æ•ˆæ¡†æ¶ã€‚ç ”ç©¶äººå‘˜å—åˆ°HippoRAGçš„å¯å‘ï¼Œåœ¨æ£€ç´¢é˜¶æ®µå¼•å…¥äº†ä¸ªæ€§åŒ–PageRank (Personalized PageRank, PPR)æŠ€æœ¯ï¼Œæ—¨åœ¨ä»¥æ›´ä½çš„æˆæœ¬æ„å»ºå…·æœ‰ä¿¡æ¯é‡çš„çŸ¥è¯†å›¾è°±ã€‚å®éªŒæ•°æ®è¡¨æ˜ï¼ŒTERAGåœ¨ä»…ä½¿ç”¨ä¸»æµæ–¹æ³•3%è‡³11%è¾“å‡ºTokençš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿç»´æŒè‡³å°‘80%çš„æ¨ç†å‡†ç¡®åº¦ã€‚å‡­å€Ÿå…¶æ˜¾è‘—çš„Tokenæ•ˆç‡å’Œé«˜æ•ˆçš„æ„å»ºæµæ°´çº¿ï¼ŒTERAGå±•ç°å‡ºåœ¨å¤§è§„æ¨¡åŠæˆæœ¬æ•æ„Ÿå‹éƒ¨ç½²åœºæ™¯ä¸­çš„å·¨å¤§åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "16 pages, 3 figures, 4 tables. Code available at https://github.com/wocqcm2/TERAG",
      "pdf_url": "https://arxiv.org/pdf/2509.18667v3",
      "published_date": "2025-09-23 05:34:34 UTC",
      "updated_date": "2025-11-10 14:28:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:40:46.291478+00:00"
    },
    {
      "arxiv_id": "2509.18648v4",
      "title": "SPiDR: A Simple Approach for Zero-Shot Safety in Sim-to-Real Transfer",
      "title_zh": "SPiDRï¼šä¸€ç§å®ç°ä»¿çœŸåˆ°ç°å®è¿ç§»é›¶æ ·æœ¬å®‰å…¨æ€§çš„ç®€å•æ–¹æ³•",
      "authors": [
        "Yarden As",
        "Chengrui Qu",
        "Benjamin Unger",
        "Dongho Kang",
        "Max van der Hart",
        "Laixi Shi",
        "Stelian Coros",
        "Adam Wierman",
        "Andreas Krause"
      ],
      "abstract": "Deploying reinforcement learning (RL) safely in the real world is challenging, as policies trained in simulators must face the inevitable sim-to-real gap. Robust safe RL techniques are provably safe, however difficult to scale, while domain randomization is more practical yet prone to unsafe behaviors. We address this gap by proposing SPiDR, short for Sim-to-real via Pessimistic Domain Randomization -- a scalable algorithm with provable guarantees for safe sim-to-real transfer. SPiDR uses domain randomization to incorporate the uncertainty about the sim-to-real gap into the safety constraints, making it versatile and highly compatible with existing training pipelines. Through extensive experiments on sim-to-sim benchmarks and two distinct real-world robotic platforms, we demonstrate that SPiDR effectively ensures safety despite the sim-to-real gap while maintaining strong performance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Reinforcement Learning (RL) åœ¨ç°å®ä¸–ç•Œéƒ¨ç½²ä¸­å›  sim-to-real å·®è·è€Œé¢ä¸´çš„å®‰å…¨æ€§æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸º SPiDR (Sim-to-real via Pessimistic Domain Randomization) çš„å¯æ‰©å±•ç®—æ³•ã€‚SPiDR é€šè¿‡ Domain Randomization æŠ€æœ¯å°† sim-to-real å·®è·å¸¦æ¥çš„ä¸ç¡®å®šæ€§æ•´åˆè¿›å®‰å…¨çº¦æŸä¸­ï¼Œä»è€Œå®ç°äº†å…·æœ‰ç†è®ºä¿è¯çš„é›¶æ ·æœ¬ (Zero-Shot) å®‰å…¨è¿ç§»ã€‚è¯¥æ–¹æ³•å…·æœ‰æé«˜çš„é€šç”¨æ€§ï¼Œèƒ½ä¸ç°æœ‰çš„è®­ç»ƒç®¡çº¿é«˜åº¦å…¼å®¹ï¼Œå…‹æœäº†ä¼ ç»Ÿé²æ£’å®‰å…¨å¼ºåŒ–å­¦ä¹ æ–¹æ³•éš¾ä»¥åœ¨å¤§è§„æ¨¡åœºæ™¯ä¸‹åº”ç”¨çš„é—®é¢˜ã€‚é€šè¿‡åœ¨ sim-to-sim åŸºå‡†æµ‹è¯•å’Œä¸¤ä¸ªä¸åŒçš„çœŸå®æœºå™¨äººå¹³å°ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒï¼Œè¯¥ç ”ç©¶è¯æ˜äº† SPiDR åœ¨å­˜åœ¨ sim-to-real å·®è·çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆä¿éšœéƒ¨ç½²å®‰å…¨å¹¶ä¿æŒå¼ºåŠ²çš„ä»»åŠ¡æ€§èƒ½ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18648v4",
      "published_date": "2025-09-23 05:03:00 UTC",
      "updated_date": "2025-10-21 19:23:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:40:49.588384+00:00"
    },
    {
      "arxiv_id": "2509.18644v2",
      "title": "Do You Need Proprioceptive States in Visuomotor Policies?",
      "title_zh": "è§†è§‰è¿åŠ¨ç­–ç•¥æ˜¯å¦éœ€è¦æœ¬ä½“æ„ŸçŸ¥çŠ¶æ€ï¼Ÿ",
      "authors": [
        "Juntu Zhao",
        "Wenbo Lu",
        "Di Zhang",
        "Yufeng Liu",
        "Yushen Liang",
        "Tianluo Zhang",
        "Yifeng Cao",
        "Junyuan Xie",
        "Yingdong Hu",
        "Shengjie Wang",
        "Junliang Guo",
        "Dequan Wang",
        "Yang Gao"
      ],
      "abstract": "Imitation-learning-based visuomotor policies have been widely used in robot manipulation, where both visual observations and proprioceptive states are typically adopted together for precise control. However, in this study, we find that this common practice makes the policy overly reliant on the proprioceptive state input, which causes overfitting to the training trajectories and results in poor spatial generalization. On the contrary, we propose the State-free Policy, removing the proprioceptive state input and predicting actions only conditioned on visual observations. The State-free Policy is built in the relative end-effector action space, and should ensure the full task-relevant visual observations, here provided by dual wide-angle wrist cameras. Empirical results demonstrate that the State-free policy achieves significantly stronger spatial generalization than the state-based policy: in real-world tasks such as pick-and-place, challenging shirt-folding, and complex whole-body manipulation, spanning multiple robot embodiments, the average success rate improves from 0% to 85% in height generalization and from 6% to 64% in horizontal generalization. Furthermore, they also show advantages in data efficiency and cross-embodiment adaptation, enhancing their practicality for real-world deployment. Discover more by visiting: https://statefreepolicy.github.io.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨è§†è§‰è¿åŠ¨ç­–ç•¥ï¼ˆvisuomotor policiesï¼‰ä¸­æœ¬ä½“æ„Ÿå—çŠ¶æ€ï¼ˆproprioceptive statesï¼‰çš„å¿…è¦æ€§ï¼Œå‘ç°ä¼ ç»Ÿæ–¹æ³•å› è¿‡åº¦ä¾èµ–æœ¬ä½“æ„Ÿè¾“å…¥è€Œå¯¼è‡´æ¨¡å‹å¯¹è®­ç»ƒè½¨è¿¹è¿‡æ‹Ÿåˆï¼Œä¸¥é‡é™åˆ¶äº†ç©ºé—´æ³›åŒ–èƒ½åŠ›ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œä½œè€…æå‡ºäº†æ— çŠ¶æ€ç­–ç•¥ï¼ˆState-free Policyï¼‰ï¼Œé€šè¿‡ç§»é™¤æœ¬ä½“æ„Ÿå—çŠ¶æ€è¾“å…¥ï¼Œä»…ä¾é è§†è§‰è§‚æµ‹æ¥é¢„æµ‹åŠ¨ä½œã€‚è¯¥ç­–ç•¥åœ¨ç›¸å¯¹æœ«ç«¯æ‰§è¡Œå™¨åŠ¨ä½œç©ºé—´ï¼ˆrelative end-effector action spaceï¼‰ä¸­è¿è¡Œï¼Œå¹¶åˆ©ç”¨åŒå¹¿è§’æ‰‹è…•æ‘„åƒå¤´ï¼ˆdual wide-angle wrist camerasï¼‰è·å–å®Œæ•´çš„ä»»åŠ¡ç›¸å…³è§†è§‰ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒState-free Policy åœ¨æ‹¾å–æ”¾ç½®ã€æŠ˜å è¡¬è¡«åŠå…¨èº«æ“ä½œç­‰ä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†ç©ºé—´æ³›åŒ–æ€§èƒ½ï¼Œå°†é«˜åº¦æ³›åŒ–æˆåŠŸç‡ä»0%æå‡è‡³85%ï¼Œæ°´å¹³æ³›åŒ–æˆåŠŸç‡ä»6%æå‡è‡³64%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨æ•°æ®æ•ˆç‡å’Œè·¨æœºå™¨äººé€‚é…ï¼ˆcross-embodiment adaptationï¼‰æ–¹é¢ä¹Ÿè¡¨ç°ä¼˜å¼‚ï¼Œä¸ºæœºå™¨äººåœ¨å¤æ‚ç°å®ç¯å¢ƒä¸­çš„éƒ¨ç½²æä¾›äº†æ›´å…·å®ç”¨æ€§çš„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Project page: https://statefreepolicy.github.io",
      "pdf_url": "https://arxiv.org/pdf/2509.18644v2",
      "published_date": "2025-09-23 04:56:59 UTC",
      "updated_date": "2025-09-24 07:38:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:40:52.293189+00:00"
    },
    {
      "arxiv_id": "2509.19398v1",
      "title": "FedOC: Multi-Server FL with Overlapping Client Relays in Wireless Edge Networks",
      "title_zh": "FedOCï¼šæ— çº¿è¾¹ç¼˜ç½‘ç»œä¸­åŸºäºé‡å å®¢æˆ·ç«¯ä¸­ç»§çš„å¤šæœåŠ¡å™¨è”é‚¦å­¦ä¹ ",
      "authors": [
        "Yun Ji",
        "Zeyu Chen",
        "Xiaoxiong Zhong",
        "Yanan Ma",
        "Sheng Zhang",
        "Yuguang Fang"
      ],
      "abstract": "Multi-server Federated Learning (FL) has emerged as a promising solution to mitigate communication bottlenecks of single-server FL. We focus on a typical multi-server FL architecture, where the regions covered by different edge servers (ESs) may overlap. A key observation of this architecture is that clients located in the overlapping areas can access edge models from multiple ESs. Building on this insight, we propose FedOC (Federated learning with Overlapping Clients), a novel framework designed to fully exploit the potential of these overlapping clients. In FedOC, overlapping clients could serve dual roles: (1) as Relay Overlapping Clients (ROCs), they forward edge models between neighboring ESs in real time to facilitate model sharing among different ESs; and (2) as Normal Overlapping Clients (NOCs), they dynamically select their initial model for local training based on the edge model delivery time, which enables indirect data fusion among different regions of ESs. The overall FedOC workflow proceeds as follows: in every round, each client trains local model based on the earliest received edge model and transmits to the respective ESs for model aggregation. Then each ES transmits the aggregated edge model to neighboring ESs through ROC relaying. Upon receiving the relayed models, each ES performs a second aggregation and subsequently broadcasts the updated model to covered clients. The existence of ROCs enables the model of each ES to be disseminated to the other ESs in a decentralized manner, which indirectly achieves intercell model and speeding up the training process, making it well-suited for latency-sensitive edge environments. Extensive experimental results show remarkable performance gains of our scheme compared to existing methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæœåŠ¡å™¨è”é‚¦å­¦ä¹  (Multi-server Federated Learning) ä¸­å•æœåŠ¡å™¨é€šä¿¡ç“¶é¢ˆä»¥åŠè¾¹ç¼˜æœåŠ¡å™¨ (Edge Servers) è¦†ç›–åŒºåŸŸé‡å çš„ç°çŠ¶ï¼Œæå‡ºäº† FedOC æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ä½äºé‡å åŒºåŸŸçš„å®¢æˆ·ç«¯å……å½“åŒé‡è§’è‰²ï¼šä¸­ç»§é‡å å®¢æˆ·ç«¯ (Relay Overlapping Clients, ROCs) è´Ÿè´£åœ¨ç›¸é‚»æœåŠ¡å™¨é—´å®æ—¶è½¬å‘è¾¹ç¼˜æ¨¡å‹ä»¥ä¿ƒè¿›æ¨¡å‹å…±äº«ï¼Œè€Œæ™®é€šé‡å å®¢æˆ·ç«¯ (Normal Overlapping Clients, NOCs) åˆ™æ ¹æ®æ¨¡å‹åˆ°è¾¾æ—¶é—´åŠ¨æ€é€‰æ‹©åˆå§‹è®­ç»ƒæ¨¡å‹ï¼Œå®ç°è·¨åŒºåŸŸçš„é—´æ¥æ•°æ®èåˆã€‚åœ¨æ¯è½®å·¥ä½œæµç¨‹ä¸­ï¼Œå„æœåŠ¡å™¨é€šè¿‡ ROCs çš„ä¸­ç»§å®ç°å»ä¸­å¿ƒåŒ–çš„æ¨¡å‹ä¼ æ’­å¹¶è¿›è¡ŒäºŒæ¬¡èšåˆï¼Œä»è€Œæ˜¾è‘—ä¼˜åŒ–äº†æ¨¡å‹æ›´æ–°æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFedOC åœ¨åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹å’Œæå‡æ¨¡å‹æ€§èƒ½æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ¡ˆï¼Œéå¸¸é€‚åˆå»¶è¿Ÿæ•æ„Ÿçš„æ— çº¿è¾¹ç¼˜ç½‘ç»œç¯å¢ƒã€‚",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19398v1",
      "published_date": "2025-09-23 04:53:51 UTC",
      "updated_date": "2025-09-23 04:53:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:41:00.475244+00:00"
    },
    {
      "arxiv_id": "2509.20393v1",
      "title": "The Secret Agenda: LLMs Strategically Lie and Our Current Safety Tools Are Blind",
      "title_zh": "The Secret Agendaï¼šå¤§è¯­è¨€æ¨¡å‹çš„ç­–ç•¥æ€§æ¬ºéª—ä¸ç°æœ‰å®‰å…¨å·¥å…·çš„ç›‘æµ‹ç›²åŒº",
      "authors": [
        "Caleb DeLeeuw",
        "Gaurav Chawla",
        "Aniket Sharma",
        "Vanessa Dietze"
      ],
      "abstract": "We investigate strategic deception in large language models using two complementary testbeds: Secret Agenda (across 38 models) and Insider Trading compliance (via SAE architectures). Secret Agenda reliably induced lying when deception advantaged goal achievement across all model families. Analysis revealed that autolabeled SAE features for \"deception\" rarely activated during strategic dishonesty, and feature steering experiments across 100+ deception-related features failed to prevent lying. Conversely, insider trading analysis using unlabeled SAE activations separated deceptive versus compliant responses through discriminative patterns in heatmaps and t-SNE visualizations. These findings suggest autolabel-driven interpretability approaches fail to detect or control behavioral deception, while aggregate unlabeled activations provide population-level structure for risk assessment. Results span Llama 8B/70B SAE implementations and GemmaScope under resource constraints, representing preliminary findings that motivate larger studies on feature discovery, labeling methodology, and causal interventions in realistic deception contexts.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡ Secret Agenda å’Œ Insider Trading ä¸¤ä¸ªå®éªŒå¹³å°æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„æˆ˜ç•¥æ€§æ¬ºéª—è¡Œä¸ºï¼Œå‘ç°åœ¨æ¬ºéª—æœ‰åˆ©äºç›®æ ‡è¾¾æˆæ—¶ï¼Œå„ç³»åˆ—æ¨¡å‹å‡è¡¨ç°å‡ºæ˜¾è‘—çš„æ’’è°å€¾å‘ã€‚ç ”ç©¶åˆ©ç”¨ç¨€ç–è‡ªç¼–ç å™¨ (Sparse Autoencoders, SAEs) æ¶æ„åˆ†æå‘ç°ï¼Œè‡ªåŠ¨æ ‡è®°ä¸ºâ€œæ¬ºéª—â€çš„ SAE ç‰¹å¾åœ¨æ¨¡å‹äº§ç”Ÿæˆ˜ç•¥æ€§ä¸è¯šå®è¡Œä¸ºæ—¶æå°‘æ¿€æ´»ï¼Œä¸”é’ˆå¯¹ 100 å¤šä¸ªæ¬ºéª—ç›¸å…³ç‰¹å¾çš„ç‰¹å¾å¼•å¯¼ (feature steering) å®éªŒå‡æœªèƒ½é˜»æ­¢æ’’è°è¡Œä¸ºã€‚ä¸ä¹‹ç›¸å¯¹ï¼Œé€šè¿‡æœªæ ‡è®°çš„ SAE æ¿€æ´»æ¨¡å¼å¯ä»¥æœ‰æ•ˆåŒºåˆ†æ¬ºéª—æ€§ä¸åˆè§„æ€§å“åº”ï¼Œå¹¶åœ¨çƒ­å›¾å’Œ t-SNE å¯è§†åŒ–ä¸­å±•ç°å‡ºæ˜æ˜¾çš„åˆ¤åˆ«æ€§æ¨¡å¼ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼ŒåŸºäºè‡ªåŠ¨æ ‡è®° (autolabel-driven) çš„å¯è§£é‡Šæ€§æ–¹æ³•åœ¨æ£€æµ‹å’Œæ§åˆ¶è¡Œä¸ºæ¬ºéª—æ–¹é¢å­˜åœ¨ç›²ç‚¹ï¼Œè€Œèšåˆçš„æœªæ ‡è®°æ¿€æ´»ä¸ºé£é™©è¯„ä¼°æä¾›äº†æ›´å¥½çš„ç¾¤ä½“ç»“æ„ã€‚è¯¥ç ”ç©¶åœ¨ Llama å’Œ GemmaScope ç­‰æ¨¡å‹ä¸Šçš„åˆæ­¥å®éªŒæ­ç¤ºäº†ç°æœ‰å®‰å…¨å·¥å…·çš„å±€é™æ€§ï¼Œå¹¶å¼ºè°ƒäº†åœ¨çœŸå®æ¬ºéª—æƒ…å¢ƒä¸‹æ”¹è¿›ç‰¹å¾å‘ç°ã€æ ‡æ³¨æ–¹æ³•åŠå› æœå¹²é¢„çš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "9 pages plus citations and appendix, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.20393v1",
      "published_date": "2025-09-23 04:52:40 UTC",
      "updated_date": "2025-09-23 04:52:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:41:12.341986+00:00"
    },
    {
      "arxiv_id": "2509.18638v2",
      "title": "Learning neuroimaging models from health system-scale data",
      "title_zh": "åŸºäºåŒ»ç–—ç³»ç»Ÿçº§æ•°æ®çš„ç¥ç»å½±åƒæ¨¡å‹å­¦ä¹ ",
      "authors": [
        "Yiwei Lyu",
        "Samir Harake",
        "Asadur Chowdury",
        "Soumyanil Banerjee",
        "Rachel Gologorsky",
        "Shixuan Liu",
        "Anna-Katharina Meissner",
        "Akshay Rao",
        "Chenhui Zhao",
        "Akhil Kondepudi",
        "Cheng Jiang",
        "Xinhai Hou",
        "Rushikesh S. Joshi",
        "Volker Neuschmelting",
        "Ashok Srinivasan",
        "Dawn Kleindorfer",
        "Brian Athey",
        "Vikas Gulani",
        "Aditya Pandey",
        "Honglak Lee",
        "Todd Hollon"
      ],
      "abstract": "Neuroimaging is a ubiquitous tool for evaluating patients with neurological diseases. The global demand for magnetic resonance imaging (MRI) studies has risen steadily, placing significant strain on health systems, prolonging turnaround times, and intensifying physician burnout. These challenges disproportionately impact patients in low-resource and rural settings. Here, we utilized a large academic health system as a data engine to develop Prima, the first vision language model (VLM) serving as an AI foundation for neuroimaging that supports real-world, clinical MRI studies as input. Trained on over 220,000 MRI studies, Prima uses a hierarchical vision architecture that provides general and transferable MRI features. Prima was tested in a 1-year health system-wide study that included 30K MRI studies. Across 52 radiologic diagnoses from the major neurologic disorders, including neoplastic, inflammatory, infectious, and developmental lesions, Prima achieved a mean diagnostic area under the ROC curve of 92.0, outperforming other state-of-the-art general and medical AI models. Prima offers explainable differential diagnoses, worklist priority for radiologists, and clinical referral recommendations across diverse patient demographics and MRI systems. Prima demonstrates algorithmic fairness across sensitive groups and can help mitigate health system biases, such as prolonged turnaround times for low-resource populations. These findings highlight the transformative potential of health system-scale VLMs and Prima's role in advancing AI-driven healthcare.",
      "tldr_zh": "è¯¥ç ”ç©¶åˆ©ç”¨å¤§å‹å­¦æœ¯å¥åº·ç³»ç»Ÿçš„æ•°æ®ï¼Œå¼€å‘äº†é¦–ä¸ªä¸“ä¸ºç¥ç»å½±åƒè®¾è®¡çš„è§†è§‰è¯­è¨€æ¨¡å‹ (Vision Language Model, VLM) â€”â€” Primaï¼Œæ—¨åœ¨è§£å†³æ—¥ç›Šå¢é•¿çš„ MRI éœ€æ±‚å¯¹åŒ»ç–—ç³»ç»Ÿé€ æˆçš„å‹åŠ›ã€‚Prima åœ¨è¶…è¿‡ 220,000 é¡¹ MRI ç ”ç©¶ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œé‡‡ç”¨å±‚æ¬¡åŒ–è§†è§‰æ¶æ„ (hierarchical vision architecture) ä»¥æä¾›é€šç”¨çš„å½±åƒç‰¹å¾ã€‚åœ¨åŒ…å« 52 ç§æ”¾å°„å­¦è¯Šæ–­çš„å¤§è§„æ¨¡æµ‹è¯•ä¸­ï¼ŒPrima å®ç°äº† 92.0 çš„å¹³å‡è¯Šæ–­ AUCï¼Œæ€§èƒ½ä¼˜äºç›®å‰çš„é€šç”¨å’ŒåŒ»ç–— AI æ¨¡å‹ã€‚é™¤äº†æä¾›å¯è§£é‡Šçš„é‰´åˆ«è¯Šæ–­ï¼ŒPrima è¿˜èƒ½ååŠ©æ”¾å°„ç§‘åŒ»ç”Ÿè®¾å®šå·¥ä½œä¼˜å…ˆçº§å¹¶ç»™å‡ºä¸´åºŠè½¬è¯Šå»ºè®®ã€‚ç ”ç©¶è¿›ä¸€æ­¥è¯å®äº† Prima åœ¨ä¸åŒæ‚£è€…ç¾¤ä½“é—´å…·æœ‰ç®—æ³•å…¬å¹³æ€§ (algorithmic fairness)ï¼Œæœ‰åŠ©äºç¼“è§£åŒ»ç–—ç³»ç»Ÿä¸­çš„ç³»ç»Ÿæ€§åè§ã€‚è¿™é¡¹æˆæœå±•ç¤ºäº†å¥åº·ç³»ç»Ÿè§„æ¨¡çš„ VLM åœ¨æ¨åŠ¨ AI é©±åŠ¨çš„ä¸´åºŠè¯Šç–—æ–¹é¢çš„å·¨å¤§åº”ç”¨æ½œåŠ›å’Œå˜é©æ€§å½±å“ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18638v2",
      "published_date": "2025-09-23 04:49:59 UTC",
      "updated_date": "2025-12-16 04:26:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:41:08.757164+00:00"
    },
    {
      "arxiv_id": "2509.18633v3",
      "title": "Evolutionary Learning in Spatial Agent-Based Models for Physical Climate Risk Assessment",
      "title_zh": "ç‰©ç†æ°”å€™é£é™©è¯„ä¼°ä¸­ç©ºé—´ä¸»ä½“æ¨¡å‹çš„æ¼”åŒ–å­¦ä¹ ",
      "authors": [
        "Yara Mohajerani"
      ],
      "abstract": "Climate risk assessment requires modelling complex interactions between spatially heterogeneous hazards and adaptive economic systems. We present a novel geospatial agent-based model that integrates climate hazard data with evolutionary learning for economic agents. Our framework combines geospatial agent-based modelling with asset-level damage functions, featuring an illustrative three-sector economy (commodity, manufacturing, retail) with adaptive learning behaviours that allow firms to evolve strategies for budget allocation, pricing, wages, and risk adaptation through fitness-based selection and mutation. We demonstrate the framework using riverine flood projections under RCP8.5 until 2100, comparing four scenarios: baseline and hazard conditions with and without evolutionary learning. Our results show that increasingly frequent and intense acute hazards lower firm production levels, liquidity, and capital, while increasing the prices of goods and unemployment. The framework reveals systemic risks where even agents not directly exposed to floods face impacts through supply chain disruptions. Importantly, evolutionary adaptation enables firms to maintain higher production, capital, liquidity, wages and employment levels while keeping prices lower compared to non-learning counterparts. This open-source framework provides financial institutions and companies with tools to quantify both direct and cascading climate risks while evaluating cost-effective adaptation strategies.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å°†æ°”å€™ç¾å®³æ•°æ®ä¸æ¼”åŒ–å­¦ä¹  (Evolutionary Learning) ç›¸ç»“åˆçš„æ–°å‹åœ°ç†ç©ºé—´æ™ºèƒ½ä½“æ¨¡å‹ (Geospatial Agent-Based Model)ï¼Œç”¨äºè¯„ä¼°ç‰©ç†æ°”å€™é£é™©å¯¹å¤æ‚ç»æµç³»ç»Ÿçš„å½±å“ã€‚è¯¥æ¡†æ¶æ„å»ºäº†ä¸€ä¸ªåŒ…å«å•†å“ã€åˆ¶é€ å’Œé›¶å”®ä¸‰éƒ¨é—¨çš„ç»æµä½“ç³»ï¼Œåˆ©ç”¨é€‚åº”æ€§å­¦ä¹ æœºåˆ¶ä½¿ä¼ä¸šèƒ½å¤Ÿé€šè¿‡é€‚åº”åº¦é€‰æ‹©å’Œå˜å¼‚æ¥æ¼”åŒ–å…¶é¢„ç®—åˆ†é…ã€å®šä»·ã€å·¥èµ„åŠé£é™©é€‚åº”ç­–ç•¥ã€‚ç ”ç©¶åˆ©ç”¨ RCP8.5 è·¯å¾„ä¸‹çš„æ²³æµæ´ªæ°´æƒ…æ™¯è¿›è¡ŒéªŒè¯ï¼Œå‘ç°æ—¥ç›Šä¸¥é‡çš„ç¾å®³ä¼šæ˜¾è‘—é™ä½ä¼ä¸šçš„ç”Ÿäº§åŠ›ã€æµåŠ¨æ€§å’Œèµ„æœ¬ï¼Œå¹¶å¼•å‘ç‰©ä»·ä¸Šæ¶¨ä¸å¤±ä¸šã€‚æ­¤å¤–ï¼Œæ¨¡å‹æ­ç¤ºäº†æ°”å€™é£é™©çš„ç³»ç»Ÿæ€§ç‰¹å¾ï¼Œå³æœªå—ç›´æ¥å½±å“çš„æ™ºèƒ½ä½“ä»ä¼šå› ä¾›åº”é“¾ä¸­æ–­è€Œé­å—å†²å‡»ã€‚å®éªŒç»“æœå¼ºè°ƒï¼Œæ¼”åŒ–é€‚åº” (Evolutionary Adaptation) èƒ½æ˜¾è‘—æå‡ä¼ä¸šåœ¨æç«¯ç¯å¢ƒä¸‹çš„éŸ§æ€§ï¼Œä½¿å…¶åœ¨ç»´æŒäº§é‡å’Œå°±ä¸šçš„åŒæ—¶æŠ‘åˆ¶ç‰©ä»·ã€‚è¿™ä¸€å¼€æºæ¡†æ¶ä¸ºé‡‘èæœºæ„è¯„ä¼°æ°”å€™é£é™©åŠåˆ¶å®šæˆæœ¬æ•ˆç›Šæœ€ä¼˜çš„é€‚åº”ç­–ç•¥æä¾›äº†é‡è¦çš„é‡åŒ–åˆ†æå·¥å…·ã€‚",
      "categories": [
        "cs.AI",
        "q-fin.RM"
      ],
      "primary_category": "cs.AI",
      "comment": "Earlier version accepted to and presented at NeurIPS 2025 Tackling Climate Change with Machine Learning workshop. Source code and documentation available at https://github.com/yaramohajerani/spatial-climate-ABM",
      "pdf_url": "https://arxiv.org/pdf/2509.18633v3",
      "published_date": "2025-09-23 04:33:58 UTC",
      "updated_date": "2026-01-05 22:51:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:41:12.092853+00:00"
    },
    {
      "arxiv_id": "2509.18631v3",
      "title": "Generalizable Domain Adaptation for Sim-and-Real Policy Co-Training",
      "title_zh": "é¢å‘ä»¿çœŸä¸ç°å®ç­–ç•¥ååŒè®­ç»ƒçš„å¯æ³›åŒ–åŸŸè‡ªé€‚åº”",
      "authors": [
        "Shuo Cheng",
        "Liqian Ma",
        "Zhenyang Chen",
        "Ajay Mandlekar",
        "Caelan Garrett",
        "Danfei Xu"
      ],
      "abstract": "Behavior cloning has shown promise for robot manipulation, but real-world demonstrations are costly to acquire at scale. While simulated data offers a scalable alternative, particularly with advances in automated demonstration generation, transferring policies to the real world is hampered by various simulation and real domain gaps. In this work, we propose a unified sim-and-real co-training framework for learning generalizable manipulation policies that primarily leverages simulation and only requires a few real-world demonstrations. Central to our approach is learning a domain-invariant, task-relevant feature space. Our key insight is that aligning the joint distributions of observations and their corresponding actions across domains provides a richer signal than aligning observations (marginals) alone. We achieve this by embedding an Optimal Transport (OT)-inspired loss within the co-training framework, and extend this to an Unbalanced OT framework to handle the imbalance between abundant simulation data and limited real-world examples. We validate our method on challenging manipulation tasks, showing it can leverage abundant simulation data to achieve up to a 30% improvement in the real-world success rate and even generalize to scenarios seen only in simulation. Project webpage: https://ot-sim2real.github.io/.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ¨¡æ‹Ÿä¸ç°å®ååŒè®­ç»ƒï¼ˆSim-and-Real Co-Trainingï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æœºå™¨äººè¡Œä¸ºå…‹éš†ï¼ˆBehavior cloningï¼‰ä¸­çœŸå®ä¸–ç•Œæ¼”ç¤ºæ•°æ®æˆæœ¬é«˜æ˜‚ä»¥åŠæ¨¡æ‹Ÿä¸ç°å®ï¼ˆSim-to-Realï¼‰é¢†åŸŸå·®è·çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ˜¯é€šè¿‡å­¦ä¹ ä¸€ä¸ªé¢†åŸŸæ— å…³ä¸”ä¸ä»»åŠ¡ç›¸å…³çš„ç‰¹å¾ç©ºé—´ï¼Œå…¶å…³é”®è§è§£åœ¨äºå¯¹é½è§‚å¯Ÿå€¼ä¸å¯¹åº”åŠ¨ä½œçš„è”åˆåˆ†å¸ƒï¼Œè€Œéä»…ä»…å¯¹é½è§‚å¯Ÿå€¼çš„è¾¹ç¼˜åˆ†å¸ƒã€‚ç ”ç©¶åœ¨ååŒè®­ç»ƒæ¡†æ¶ä¸­åµŒå…¥äº†åŸºäºæœ€ä¼˜ä¼ è¾“ï¼ˆOptimal Transport, OTï¼‰çš„æŸå¤±å‡½æ•°ï¼Œå¹¶å°†å…¶æ‰©å±•è‡³éå¹³è¡¡æœ€ä¼˜ä¼ è¾“ï¼ˆUnbalanced OTï¼‰æ¡†æ¶ï¼Œä»¥æœ‰æ•ˆå¤„ç†æµ·é‡æ¨¡æ‹Ÿæ•°æ®ä¸æå°‘é‡çœŸå®æ¼”ç¤ºä¹‹é—´çš„æ•°é‡ä¸å¹³è¡¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ“ä½œä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°†çœŸå®ä¸–ç•Œçš„ä»»åŠ¡æˆåŠŸç‡æå‡äº†é«˜è¾¾30%ï¼Œå¹¶å±•ç°å‡ºèƒ½æ³›åŒ–è‡³ä»…åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­å‡ºç°çš„åœºæ™¯çš„èƒ½åŠ›ã€‚è¯¥é¡¹å·¥ä½œä¸ºåˆ©ç”¨ä½æˆæœ¬æ¨¡æ‹Ÿæ•°æ®æå‡æœºå™¨äººç­–ç•¥çš„æ³›åŒ–æ€§å’Œå®ç”¨æ€§æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted to NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.18631v3",
      "published_date": "2025-09-23 04:32:53 UTC",
      "updated_date": "2026-01-16 18:05:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:41:12.558623+00:00"
    },
    {
      "arxiv_id": "2509.18629v2",
      "title": "HyperAdapt: Simple High-Rank Adaptation",
      "title_zh": "HyperAdaptï¼šç®€å•çš„é«˜ç§©è‡ªé€‚åº”",
      "authors": [
        "Abel Gurung",
        "Joseph Campbell"
      ],
      "abstract": "Foundation models excel across diverse tasks, but adapting them to specialized applications often requires fine-tuning, an approach that is memory and compute-intensive. Parameter-efficient fine-tuning (PEFT) methods mitigate this by updating only a small subset of weights. In this paper, we introduce HyperAdapt, a parameter-efficient fine-tuning method that significantly reduces the number of trainable parameters compared to state-of-the-art methods like LoRA. Specifically, HyperAdapt adapts a pre-trained weight matrix by applying row- and column-wise scaling through diagonal matrices, thereby inducing a high-rank update while requiring only $n+m$ trainable parameters for an $n \\times m$ matrix. Theoretically, we establish an upper bound on the rank of HyperAdapt's updates, and empirically, we confirm that it consistently induces high-rank transformations across model layers. Experiments on GLUE, arithmetic reasoning, and commonsense reasoning benchmarks with models up to 14B parameters demonstrate that HyperAdapt matches or nearly matches the performance of full fine-tuning and state-of-the-art PEFT methods while using orders of magnitude fewer trainable parameters.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† HyperAdaptï¼Œä¸€ç§æç®€ä¸”é«˜æ•ˆçš„å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆParameter-efficient fine-tuning, PEFTï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨æ˜¾è‘—é™ä½åŸºç¡€æ¨¡å‹ï¼ˆFoundation modelsï¼‰é€‚é…ç‰¹å®šä»»åŠ¡æ—¶çš„è®¡ç®—ä¸å†…å­˜å¼€é”€ã€‚HyperAdapt é€šè¿‡å¯¹é¢„è®­ç»ƒæƒé‡çŸ©é˜µåº”ç”¨è¡Œå’Œåˆ—æ–¹å‘çš„å¯¹è§’çŸ©é˜µï¼ˆdiagonal matricesï¼‰ç¼©æ”¾ï¼Œä»…éœ€ $n+m$ ä¸ªå¯è®­ç»ƒå‚æ•°å³å¯å®Œæˆ $n \\times m$ çŸ©é˜µçš„é€‚é…ã€‚è¿™ç§æœºåˆ¶åœ¨ç†è®ºå’Œå®è¯ä¸Šéƒ½è¢«è¯æ˜èƒ½æœ‰æ•ˆè¯±å¯¼é«˜ç§©ï¼ˆhigh-rankï¼‰æ›´æ–°ï¼Œä»è€Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•å‚æ•°é‡å¤§çš„ç“¶é¢ˆã€‚åœ¨é’ˆå¯¹æœ€é«˜è¾¾ 14B å‚æ•°è§„æ¨¡æ¨¡å‹çš„ GLUEã€ç®—æœ¯æ¨ç†å’Œå¸¸è¯†æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒHyperAdapt å±•ç°äº†ä¸å…¨é‡å¾®è°ƒï¼ˆfull fine-tuningï¼‰åŠ LoRA ç­‰å…ˆè¿›æ–¹æ³•ç›¸åª²ç¾çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒé«˜å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œå°†å¯è®­ç»ƒå‚æ•°é‡å‡å°‘äº†å‡ ä¸ªæ•°é‡çº§ã€‚è¿™ä¸€ç ”ç©¶ä¸ºå¤§è§„æ¨¡æ¨¡å‹çš„é«˜æ•ˆé€‚é…æä¾›äº†ç®€å•ä¸”å¼ºå¤§çš„æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18629v2",
      "published_date": "2025-09-23 04:29:26 UTC",
      "updated_date": "2025-11-05 20:12:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:41:36.289386+00:00"
    },
    {
      "arxiv_id": "2509.18627v1",
      "title": "BRAID: Input-Driven Nonlinear Dynamical Modeling of Neural-Behavioral Data",
      "title_zh": "BRAIDï¼šè¾“å…¥é©±åŠ¨çš„ç¥ç»-è¡Œä¸ºæ•°æ®éçº¿æ€§åŠ¨åŠ›å­¦å»ºæ¨¡",
      "authors": [
        "Parsa Vahidi",
        "Omid G. Sani",
        "Maryam M. Shanechi"
      ],
      "abstract": "Neural populations exhibit complex recurrent structures that drive behavior, while continuously receiving and integrating external inputs from sensory stimuli, upstream regions, and neurostimulation. However, neural populations are often modeled as autonomous dynamical systems, with little consideration given to the influence of external inputs that shape the population activity and behavioral outcomes. Here, we introduce BRAID, a deep learning framework that models nonlinear neural dynamics underlying behavior while explicitly incorporating any measured external inputs. Our method disentangles intrinsic recurrent neural population dynamics from the effects of inputs by including a forecasting objective within input-driven recurrent neural networks. BRAID further prioritizes the learning of intrinsic dynamics that are related to a behavior of interest by using a multi-stage optimization scheme. We validate BRAID with nonlinear simulations, showing that it can accurately learn the intrinsic dynamics shared between neural and behavioral modalities. We then apply BRAID to motor cortical activity recorded during a motor task and demonstrate that our method more accurately fits the neural-behavioral data by incorporating measured sensory stimuli into the model and improves the forecasting of neural-behavioral data compared with various baseline methods, whether input-driven or not.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† BRAIDï¼Œä¸€ç§æ—¨åœ¨å»ºæ¨¡é©±åŠ¨è¡Œä¸ºçš„éçº¿æ€§ç¥ç»åŠ¨åŠ›å­¦ï¼Œå¹¶æ˜¾å¼æ•´åˆå¤–éƒ¨è¾“å…¥çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œè§£å†³äº†ä¼ ç»Ÿæ¨¡å‹å°†ç¥ç»ç¾¤ä½“è§†ä¸ºè‡ªæ²»åŠ¨åŠ›ç³»ç»Ÿè€Œå¿½è§†å¤–éƒ¨è¾“å…¥å½±å“çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡åœ¨ Input-Driven Recurrent Neural Networks ä¸­å¼•å…¥é¢„æµ‹ç›®æ ‡ï¼Œæœ‰æ•ˆåœ°å°†å†…åœ¨çš„é€’å½’ç¥ç»ç¾¤ä½“åŠ¨åŠ›å­¦ä¸å¤–éƒ¨è¾“å…¥æ•ˆåº”è¿›è¡Œäº†è§£è€¦ã€‚BRAID è¿›ä¸€æ­¥é‡‡ç”¨å¤šé˜¶æ®µä¼˜åŒ–æ–¹æ¡ˆï¼Œæ—¨åœ¨ä¼˜å…ˆå­¦ä¹ ä¸ç‰¹å®šè¡Œä¸ºç›¸å…³çš„å†…åœ¨åŠ¨åŠ›å­¦ç‰¹å¾ã€‚é€šè¿‡éçº¿æ€§æ¨¡æ‹Ÿå®éªŒï¼Œç ”ç©¶è¯æ˜äº†è¯¥æ–¹æ³•èƒ½å‡†ç¡®æ•æ‰ç¥ç»ä¸è¡Œä¸ºæ¨¡æ€é—´å…±äº«çš„åŠ¨åŠ›å­¦è§„å¾‹ã€‚åœ¨è¿åŠ¨çš®å±‚æ•°æ®çš„å®é™…åº”ç”¨ä¸­ï¼ŒBRAID é€šè¿‡æ•´åˆæ„Ÿå®˜åˆºæ¿€æ˜¾è‘—æå‡äº†å¯¹ Neural-Behavioral Data çš„æ‹Ÿåˆç²¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ•°æ®é¢„æµ‹æ€§èƒ½ä¸Šä¼˜äºå¤šç§ä¸»æµçš„åŸºçº¿æ¨¡å‹ï¼Œä¸ºç†è§£å¤æ‚ç¯å¢ƒè¾“å…¥ä¸‹çš„ç¥ç»è¡Œä¸ºæœºåˆ¶æä¾›äº†é‡è¦å·¥å…·ã€‚",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.NC",
      "comment": "Published at the International Conference on Learning Representations (ICLR) 2025. Code is available at GitHub https://github.com/ShanechiLab/BRAID",
      "pdf_url": "https://arxiv.org/pdf/2509.18627v1",
      "published_date": "2025-09-23 04:22:53 UTC",
      "updated_date": "2025-09-23 04:22:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:41:33.777645+00:00"
    },
    {
      "arxiv_id": "2509.18626v1",
      "title": "The Case for Negative Data: From Crash Reports to Counterfactuals for Reasonable Driving",
      "title_zh": "è®ºè´Ÿé¢æ•°æ®çš„ä»·å€¼ï¼šä»äº‹æ•…æŠ¥å‘Šåˆ°åäº‹å®æ¨ç†ï¼Œå®ç°åˆç†é©¾é©¶",
      "authors": [
        "Jay Patrikar",
        "Apoorva Sharma",
        "Sushant Veer",
        "Boyi Li",
        "Sebastian Scherer",
        "Marco Pavone"
      ],
      "abstract": "Learning-based autonomous driving systems are trained mostly on incident-free data, offering little guidance near safety-performance boundaries. Real crash reports contain precisely the contrastive evidence needed, but they are hard to use: narratives are unstructured, third-person, and poorly grounded to sensor views. We address these challenges by normalizing crash narratives to ego-centric language and converting both logs and crashes into a unified scene-action representation suitable for retrieval. At decision time, our system adjudicates proposed actions by retrieving relevant precedents from this unified index; an agentic counterfactual extension proposes plausible alternatives, retrieves for each, and reasons across outcomes before deciding. On a nuScenes benchmark, precedent retrieval substantially improves calibration, with recall on contextually preferred actions rising from 24% to 53%. The counterfactual variant preserves these gains while sharpening decisions near risk.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºå­¦ä¹ çš„è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿ(autonomous driving systems)ä¸»è¦ä¾èµ–æ— äº‹æ•…æ•°æ®ã€åœ¨å®‰å…¨æ€§èƒ½è¾¹ç•Œé™„è¿‘ç¼ºä¹æŒ‡å¯¼çš„é—®é¢˜ï¼Œæ¢è®¨äº†è´Ÿé¢æ•°æ®(Negative Data)çš„é‡è¦æ€§ã€‚è™½ç„¶çœŸå®äº‹æ•…æŠ¥å‘ŠåŒ…å«æ‰€éœ€çš„å¯¹æ¯”è¯æ®ï¼Œä½†å…¶å™è¿°é€šå¸¸æ˜¯éç»“æ„åŒ–çš„ä¸”éš¾ä»¥ä¸ä¼ æ„Ÿå™¨è§†å›¾å¯¹é½ï¼Œè¯¥ç ”ç©¶é€šè¿‡å°†äº‹æ•…å™è¿°æ ‡å‡†åŒ–ä¸ºä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è¯­è¨€ï¼Œå¹¶å°†é©¾é©¶æ—¥å¿—å’Œäº‹æ•…è®°å½•è½¬åŒ–ä¸ºç»Ÿä¸€çš„åœºæ™¯-åŠ¨ä½œè¡¨ç¤º(scene-action representation)æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚åœ¨å†³ç­–é˜¶æ®µï¼Œç³»ç»Ÿé€šè¿‡æ£€ç´¢ç›¸å…³å…ˆä¾‹æ¥è£å®šæè®®çš„åŠ¨ä½œï¼Œå¹¶åˆ©ç”¨æ™ºèƒ½ä½“åäº‹å®(agentic counterfactual)æ‰©å±•æ¨¡å—è¯„ä¼°å¤‡é€‰æ–¹æ¡ˆåŠå…¶åæœã€‚åœ¨nuScenesåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¿™ç§å…ˆä¾‹æ£€ç´¢æ–¹æ³•æ˜¾è‘—æå‡äº†ç³»ç»Ÿçš„æ ¡å‡†èƒ½åŠ›ï¼Œä½¿ä¸Šä¸‹æ–‡é¦–é€‰åŠ¨ä½œçš„å¬å›ç‡ä»24%æé«˜åˆ°53%ã€‚åäº‹å®å˜ä½“åœ¨ä¿æŒä¸Šè¿°å¢ç›Šçš„åŒæ—¶ï¼Œè¿›ä¸€æ­¥å¼ºåŒ–äº†ç³»ç»Ÿåœ¨é£é™©ä¸´ç•Œç‚¹é™„è¿‘çš„å†³ç­–èƒ½åŠ›ï¼Œä¸ºå®ç°æ›´åˆç†çš„é©¾é©¶è¡Œä¸ºæä¾›äº†æ”¯æŒã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "8 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.18626v1",
      "published_date": "2025-09-23 04:21:39 UTC",
      "updated_date": "2025-09-23 04:21:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:41:34.954894+00:00"
    },
    {
      "arxiv_id": "2509.18611v1",
      "title": "Flow marching for a generative PDE foundation model",
      "title_zh": "é¢å‘ç”Ÿæˆå¼ PDE åŸºç¡€æ¨¡å‹çš„ Flow Marching",
      "authors": [
        "Zituo Chen",
        "Sili Deng"
      ],
      "abstract": "Pretraining on large-scale collections of PDE-governed spatiotemporal trajectories has recently shown promise for building generalizable models of dynamical systems. Yet most existing PDE foundation models rely on deterministic Transformer architectures, which lack generative flexibility for many science and engineering applications. We propose Flow Marching, an algorithm that bridges neural operator learning with flow matching motivated by an analysis of error accumulation in physical dynamical systems, and we build a generative PDE foundation model on top of it. By jointly sampling the noise level and the physical time step between adjacent states, the model learns a unified velocity field that transports a noisy current state toward its clean successor, reducing long-term rollout drift while enabling uncertainty-aware ensemble generations. Alongside this core algorithm, we introduce a Physics-Pretrained Variational Autoencoder (P2VAE) to embed physical states into a compact latent space, and an efficient Flow Marching Transformer (FMT) that combines a diffusion-forcing scheme with latent temporal pyramids, achieving up to 15x greater computational efficiency than full-length video diffusion models and thereby enabling large-scale pretraining at substantially reduced cost. We curate a corpus of ~2.5M trajectories across 12 distinct PDE families and train suites of P2VAEs and FMTs at multiple scales. On downstream evaluation, we benchmark on unseen Kolmogorov turbulence with few-shot adaptation, demonstrate long-term rollout stability over deterministic counterparts, and present uncertainty-stratified ensemble results, highlighting the importance of generative PDE foundation models for real-world applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Flow Marchingç®—æ³•ï¼Œæ—¨åœ¨ä¸ºåå¾®åˆ†æ–¹ç¨‹(PDE)æ„å»ºç”Ÿæˆå¼åŸºç¡€æ¨¡å‹ï¼Œä»¥è§£å†³ç°æœ‰ç¡®å®šæ€§Transformeræ¨¡å‹åœ¨ç§‘å­¦å’Œå·¥ç¨‹åº”ç”¨ä¸­ç¼ºä¹ç”Ÿæˆçµæ´»æ€§çš„é—®é¢˜ã€‚è¯¥ç®—æ³•é€šè¿‡å°†ç¥ç»ç®—å­å­¦ä¹ (neural operator learning)ä¸å—ç‰©ç†åŠ¨åŠ›ç³»ç»Ÿè¯¯å·®ç§¯ç´¯åˆ†æå¯å‘çš„æµåŒ¹é…(flow matching)ç›¸ç»“åˆï¼Œå­¦ä¹ ç»Ÿä¸€çš„é€Ÿåº¦åœºä»¥å‡å°‘é•¿æœŸæ»šåŠ¨æ¼‚ç§»(rollout drift)ï¼Œå¹¶å®ç°ä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„é›†æˆç”Ÿæˆã€‚ä¸ºäº†æ”¯æŒå¤§è§„æ¨¡é¢„è®­ç»ƒï¼Œç ”ç©¶è¿˜å¼•å…¥äº†ç‰©ç†é¢„è®­ç»ƒå˜åˆ†è‡ªç¼–ç å™¨(P2VAE)å’Œç»“åˆæ‰©æ•£å¼ºåˆ¶æ–¹æ¡ˆ(diffusion-forcing scheme)çš„é«˜æ•ˆFlow Marching Transformer (FMT)ï¼Œå…¶è®¡ç®—æ•ˆç‡æ¯”ä¼ ç»Ÿè§†é¢‘æ‰©æ•£æ¨¡å‹æå‡äº†15å€ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨æ¶µç›–12ä¸ªPDEå®¶æ—çš„250ä¸‡æ¡è½¨è¿¹æ•°æ®é›†ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œå®éªŒç»“æœæ˜¾ç¤ºè¯¥æ¨¡å‹åœ¨ä¸å¯è§çš„Kolmogorovæ¹æµä»»åŠ¡ä¸­å…·æœ‰å‡ºè‰²çš„å°‘æ ·æœ¬é€‚åº”èƒ½åŠ›å’Œé•¿æœŸæ¨¡æ‹Ÿç¨³å®šæ€§ï¼Œå½°æ˜¾äº†ç”Ÿæˆå¼PDEåŸºç¡€æ¨¡å‹åœ¨å¤„ç†ç°å®ä¸–ç•Œå¤æ‚åŠ¨åŠ›ç³»ç»Ÿä¸­çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18611v1",
      "published_date": "2025-09-23 04:00:41 UTC",
      "updated_date": "2025-09-23 04:00:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:41:39.359295+00:00"
    },
    {
      "arxiv_id": "2509.18608v2",
      "title": "End-to-End Crop Row Navigation via LiDAR-Based Deep Reinforcement Learning",
      "title_zh": "åŸºäº LiDAR æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„ç«¯åˆ°ç«¯ä½œç‰©è¡Œå¯¼èˆª",
      "authors": [
        "Ana Luiza Mineiro",
        "Francisco Affonso",
        "Marcelo Becker"
      ],
      "abstract": "Reliable navigation in under-canopy agricultural environments remains a challenge due to GNSS unreliability, cluttered rows, and variable lighting. To address these limitations, we present an end-to-end learning-based navigation system that maps raw 3D LiDAR data directly to control commands using a deep reinforcement learning policy trained entirely in simulation. Our method includes a voxel-based downsampling strategy that reduces LiDAR input size by 95.83%, enabling efficient policy learning without relying on labeled datasets or manually designed control interfaces. The policy was validated in simulation, achieving a 100% success rate in straight-row plantations and showing a gradual decline in performance as row curvature increased, tested across varying sinusoidal frequencies and amplitudes.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å†œä½œç‰©å† å±‚ä¸‹ GNSS ä¿¡å·ä¸ç¨³å®šã€ç¯å¢ƒæ‚ä¹±å’Œå…‰ç…§å˜åŒ–å¯¼è‡´çš„å¯¼èˆªæŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„å­¦ä¹ å‹å¯¼èˆªç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨åŸºäº Deep Reinforcement Learning çš„ç­–ç•¥ï¼Œå°†åŸå§‹ 3D LiDAR æ•°æ®ç›´æ¥æ˜ å°„ä¸ºæœºå™¨äººæ§åˆ¶æŒ‡ä»¤ï¼Œä¸”è¯¥ç­–ç•¥å®Œå…¨åœ¨ä»¿çœŸç¯å¢ƒä¸­è®­ç»ƒå®Œæˆã€‚ç ”ç©¶é‡‡ç”¨äº†ä¸€ç§ Voxel-based downsampling ç­–ç•¥ï¼Œå°† LiDAR è¾“å…¥æ•°æ®é‡å‡å°‘äº† 95.83%ï¼Œä»è€Œåœ¨æ— éœ€æ ‡è®°æ•°æ®é›†æˆ–äººå·¥å¹²é¢„çš„æƒ…å†µä¸‹å®ç°äº†é«˜æ•ˆçš„ç­–ç•¥å­¦ä¹ ã€‚ä»¿çœŸéªŒè¯æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ç›´çº¿ä½œç‰©å„é—´å¯¼èˆªä¸­å–å¾—äº† 100% çš„æˆåŠŸç‡ã€‚åœ¨é’ˆå¯¹ä¸åŒé¢‘ç‡å’ŒæŒ¯å¹…çš„å¼¯æ›²å„æ²Ÿæµ‹è¯•ä¸­ï¼Œè™½ç„¶æ€§èƒ½éšæ›²ç‡å¢åŠ æœ‰æ‰€ä¸‹é™ï¼Œä½†ä»è¯æ˜äº†è¯¥ç«¯åˆ°ç«¯æ¶æ„åœ¨å¤æ‚å†œä¸šç¯å¢ƒä¸­çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted to the 22nd International Conference on Advanced Robotics (ICAR 2025). 7 pages",
      "pdf_url": "https://arxiv.org/pdf/2509.18608v2",
      "published_date": "2025-09-23 03:56:10 UTC",
      "updated_date": "2025-11-03 20:29:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:41:39.668235+00:00"
    },
    {
      "arxiv_id": "2509.19397v1",
      "title": "Self-Alignment Learning to Improve Myocardial Infarction Detection from Single-Lead ECG",
      "title_zh": "æå‡å•å¯¼è”å¿ƒç”µå›¾å¿ƒè‚Œæ¢—æ­»æ£€æµ‹èƒ½åŠ›çš„è‡ªå¯¹é½å­¦ä¹ ",
      "authors": [
        "Jiarui Jin",
        "Xiaocheng Fang",
        "Haoyu Wang",
        "Jun Li",
        "Che Liu",
        "Donglin Xie",
        "Hongyan Li",
        "Shenda Hong"
      ],
      "abstract": "Myocardial infarction is a critical manifestation of coronary artery disease, yet detecting it from single-lead electrocardiogram (ECG) remains challenging due to limited spatial information. An intuitive idea is to convert single-lead into multiple-lead ECG for classification by pre-trained models, but generative methods optimized at the signal level in most cases leave a large latent space gap, ultimately degrading diagnostic performance. This naturally raises the question of whether latent space alignment could help. However, most prior ECG alignment methods focus on learning transformation invariance, which mismatches the goal of single-lead detection. To address this issue, we propose SelfMIS, a simple yet effective alignment learning framework to improve myocardial infarction detection from single-lead ECG. Discarding manual data augmentations, SelfMIS employs a self-cutting strategy to pair multiple-lead ECG with their corresponding single-lead segments and directly align them in the latent space. This design shifts the learning objective from pursuing transformation invariance to enriching the single-lead representation, explicitly driving the single-lead ECG encoder to learn a representation capable of inferring global cardiac context from the local signal. Experimentally, SelfMIS achieves superior performance over baseline models across nine myocardial infarction types while maintaining a simpler architecture and lower computational overhead, thereby substantiating the efficacy of direct latent space alignment. Our code and checkpoint will be publicly available after acceptance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å•å¯¼è”å¿ƒç”µå›¾ (single-lead ECG) åœ¨å¿ƒè‚Œæ¢—æ­» (Myocardial infarction) æ£€æµ‹ä¸­ç”±äºç©ºé—´ä¿¡æ¯æœ‰é™è€Œå¯¼è‡´çš„è¯Šæ–­éš¾é¢˜ï¼Œæå‡ºäº†åä¸º SelfMIS çš„è‡ªå¯¹é½å­¦ä¹ æ¡†æ¶ã€‚SelfMIS èˆå¼ƒäº†ä¼ ç»Ÿçš„äººå·¥æ•°æ®å¢å¼ºï¼Œåˆ›æ–°æ€§åœ°é‡‡ç”¨è‡ªåˆ‡å‰²ç­–ç•¥ (self-cutting strategy) å°†å¤šå¯¼è”å¿ƒç”µå›¾ä¸å…¶å¯¹åº”çš„å•å¯¼è”ç‰‡æ®µè¿›è¡Œé…å¯¹ï¼Œå¹¶ç›´æ¥åœ¨æ½œåœ¨ç©ºé—´ (latent space) è¿›è¡Œç‰¹å¾å¯¹é½ã€‚è¯¥æ–¹æ³•å°†å­¦ä¹ ç›®æ ‡ä»è¿½æ±‚å˜æ¢ä¸å˜æ€§è½¬å‘å¯ŒåŒ–å•å¯¼è”è¡¨ç¤º (single-lead representation)ï¼Œæ˜¾å¼åœ°å¼•å¯¼ç¼–ç å™¨ä»å±€éƒ¨ä¿¡å·ä¸­å­¦ä¹ å¹¶æ¨æ–­å…¨å±€å¿ƒè„èƒŒæ™¯ (global cardiac context)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSelfMIS åœ¨ä¹ç§å¿ƒè‚Œæ¢—æ­»ç±»å‹çš„æ£€æµ‹ä»»åŠ¡ä¸­å‡å–å¾—äº†ä¼˜äºåŸºçº¿æ¨¡å‹çš„æ€§èƒ½ã€‚åŒæ—¶ï¼Œè¯¥æ¨¡å‹ä¿æŒäº†æ›´ç®€å•çš„æ¶æ„å’Œæ›´ä½çš„è®¡ç®—å¼€é”€ (computational overhead)ï¼Œæœ‰æ•ˆéªŒè¯äº†ç›´æ¥æ½œåœ¨ç©ºé—´å¯¹é½åœ¨æå‡å•å¯¼è”è¯Šæ–­æ•ˆç‡ä¸å‡†ç¡®æ€§æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19397v1",
      "published_date": "2025-09-23 03:54:39 UTC",
      "updated_date": "2025-09-23 03:54:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:41:50.759096+00:00"
    },
    {
      "arxiv_id": "2509.18606v1",
      "title": "FlexSED: Towards Open-Vocabulary Sound Event Detection",
      "title_zh": "FlexSEDï¼šé¢å‘å¼€æ”¾è¯æ±‡å£°éŸ³äº‹ä»¶æ£€æµ‹",
      "authors": [
        "Jiarui Hai",
        "Helin Wang",
        "Weizhe Guo",
        "Mounya Elhilali"
      ],
      "abstract": "Despite recent progress in large-scale sound event detection (SED) systems capable of handling hundreds of sound classes, existing multi-class classification frameworks remain fundamentally limited. They cannot process free-text sound queries, which enable more flexible and user-friendly interaction, and they lack zero-shot capabilities and offer poor few-shot adaptability. Although text-query-based separation methods have been explored, they primarily focus on source separation and are ill-suited for SED tasks that require precise temporal localization and efficient detection across large and diverse sound vocabularies. In this paper, we propose FlexSED, an open-vocabulary sound event detection system. FlexSED builds on a pretrained audio SSL model and the CLAP text encoder, introducing an encoder-decoder composition and an adaptive fusion strategy to enable effective continuous training from pretrained weights. To ensure robust supervision, it also employs large language models (LLMs) to assist in event query selection during training, addressing challenges related to missing labels. As a result, FlexSED achieves superior performance compared to vanilla SED models on AudioSet-Strong, while demonstrating strong zero-shot and few-shot capabilities. We release the code and pretrained models to support future research and applications based on FlexSED.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† FlexSEDï¼Œä¸€ç§æ—¨åœ¨è§£å†³ç°æœ‰ Sound Event Detection (SED) ç³»ç»Ÿæ— æ³•å¤„ç†è‡ªç”±æ–‡æœ¬æŸ¥è¯¢ã€ç¼ºä¹ zero-shot èƒ½åŠ›åŠ few-shot é€‚åº”æ€§å·®ç­‰é—®é¢˜çš„å¼€æ”¾è¯æ±‡å£°éŸ³äº‹ä»¶æ£€æµ‹ç³»ç»Ÿã€‚FlexSED åŸºäºé¢„è®­ç»ƒçš„éŸ³é¢‘ SSL æ¨¡å‹å’Œ CLAP æ–‡æœ¬ç¼–ç å™¨ï¼Œé€šè¿‡å¼•å…¥ç¼–ç å™¨-è§£ç å™¨æ¶æ„å’Œè‡ªé€‚åº”èåˆç­–ç•¥ï¼Œå®ç°äº†ä»é¢„è®­ç»ƒæƒé‡å¼€å§‹çš„é«˜æ•ˆæŒç»­è®­ç»ƒã€‚ä¸ºäº†ç¡®ä¿ç¨³å¥çš„ç›‘ç£ï¼Œè¯¥ç³»ç»Ÿåˆ©ç”¨ Large Language Models (LLMs) åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¾…åŠ©äº‹ä»¶æŸ¥è¯¢é€‰æ‹©ï¼Œæœ‰æ•ˆè§£å†³äº†æ ‡ç­¾ç¼ºå¤±å¸¦æ¥çš„æŒ‘æˆ˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFlexSED åœ¨ AudioSet-Strong æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„ vanilla SED æ¨¡å‹ï¼Œå¹¶å±•ç°å‡ºå¼ºå¤§çš„ zero-shot å’Œ few-shot æ€§èƒ½ã€‚ç›®å‰è¯¥ç ”ç©¶å·²å¼€æºä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹ï¼Œä¸ºå¼€æ”¾è¯æ±‡å£°éŸ³æ£€æµ‹çš„è¿›ä¸€æ­¥ç ”ç©¶å’Œåº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18606v1",
      "published_date": "2025-09-23 03:52:52 UTC",
      "updated_date": "2025-09-23 03:52:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:41:48.345878+00:00"
    },
    {
      "arxiv_id": "2509.18603v1",
      "title": "SynSonic: Augmenting Sound Event Detection through Text-to-Audio Diffusion ControlNet and Effective Sample Filtering",
      "title_zh": "SynSonicï¼šåŸºäºæ–‡æœ¬åˆ°éŸ³é¢‘æ‰©æ•£ ControlNet ä¸æœ‰æ•ˆæ ·æœ¬è¿‡æ»¤çš„å£°éŸ³äº‹ä»¶æ£€æµ‹å¢å¼º",
      "authors": [
        "Jiarui Hai",
        "Mounya Elhilali"
      ],
      "abstract": "Data synthesis and augmentation are essential for Sound Event Detection (SED) due to the scarcity of temporally labeled data. While augmentation methods like SpecAugment and Mix-up can enhance model performance, they remain constrained by the diversity of existing samples. Recent generative models offer new opportunities, yet their direct application to SED is challenging due to the lack of precise temporal annotations and the risk of introducing noise through unreliable filtering. To address these challenges and enable generative-based augmentation for SED, we propose SynSonic, a data augmentation method tailored for this task. SynSonic leverages text-to-audio diffusion models guided by an energy-envelope ControlNet to generate temporally coherent sound events. A joint score filtering strategy with dual classifiers ensures sample quality, and we explore its practical integration into training pipelines. Experimental results show that SynSonic improves Polyphonic Sound Detection Scores (PSDS1 and PSDS2), enhancing both temporal localization and sound class discrimination.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å£°éŸ³äº‹ä»¶æ£€æµ‹(Sound Event Detection, SED)ä¸­å¸¦æ—¶é—´æ ‡æ³¨çš„æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œæå‡ºäº†åä¸ºSynSonicçš„æ•°æ®å¢å¼ºæ–¹æ³•ã€‚SynSonicåˆ©ç”¨æ–‡æœ¬åˆ°éŸ³é¢‘æ‰©æ•£æ¨¡å‹(text-to-audio diffusion models)å¹¶ç»“åˆèƒ½é‡åŒ…ç»œæ§åˆ¶çš„ControlNetï¼Œç”Ÿæˆå…·æœ‰ç²¾ç¡®æ—¶é—´ç›¸å¹²æ€§çš„å£°éŸ³äº‹ä»¶ã€‚ä¸ºè§£å†³ç”Ÿæˆæ•°æ®ä¸­å¯èƒ½å­˜åœ¨çš„å™ªå£°å’Œè´¨é‡é—®é¢˜ï¼Œè¯¥æ–¹æ³•å¼•å…¥äº†ä¸€ç§åŒ…å«åŒåˆ†ç±»å™¨çš„è”åˆå¾—åˆ†è¿‡æ»¤(joint score filtering)ç­–ç•¥ï¼Œç¡®ä¿äº†åˆæˆæ ·æœ¬çš„å¯é æ€§ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ¢è®¨äº†è¯¥æ–¹æ³•åœ¨è®­ç»ƒæµæ°´çº¿ä¸­çš„å®é™…é›†æˆåº”ç”¨ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSynSonicæ˜¾è‘—æå‡äº†Polyphonic Sound Detection Scores(PSDS1å’ŒPSDS2)æŒ‡æ ‡ï¼Œåœ¨ä¼˜åŒ–æ—¶é—´å®šä½ç²¾åº¦å’Œå£°éŸ³ç±»åˆ«è¾¨åˆ«èƒ½åŠ›æ–¹é¢å‡å–å¾—äº†ä¼˜å¼‚æ•ˆæœã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18603v1",
      "published_date": "2025-09-23 03:48:26 UTC",
      "updated_date": "2025-09-23 03:48:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:41:51.868319+00:00"
    },
    {
      "arxiv_id": "2509.18600v1",
      "title": "OraPO: Oracle-educated Reinforcement Learning for Data-efficient and Factual Radiology Report Generation",
      "title_zh": "OraPOï¼šç”¨äºæ•°æ®é«˜æ•ˆä¸”ç¬¦åˆäº‹å®çš„æ”¾å°„æŠ¥å‘Šç”Ÿæˆçš„é¢„è¨€æœºæŒ‡å¯¼å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Zhuoxiao Chen",
        "Hongyang Yu",
        "Ying Xu",
        "Yadan Luo",
        "Long Duong",
        "Yuan-Fang Li"
      ],
      "abstract": "Radiology report generation (RRG) aims to automatically produce clinically faithful reports from chest X-ray images. Prevailing work typically follows a scale-driven paradigm, by multi-stage training over large paired corpora and oversized backbones, making pipelines highly data- and compute-intensive. In this paper, we propose Oracle-educated GRPO {OraPO) with a FactScore-based reward (FactS) to tackle the RRG task under constrained budgets. OraPO enables single-stage, RL-only training by converting failed GRPO explorations on rare or difficult studies into direct preference supervision via a lightweight oracle step. FactS grounds learning in diagnostic evidence by extracting atomic clinical facts and checking entailment against ground-truth labels, yielding dense, interpretable sentence-level rewards. Together, OraPO and FactS create a compact and powerful framework that significantly improves learning efficiency on clinically challenging cases, setting the new SOTA performance on the CheXpert Plus dataset (0.341 in F1) with 2--3 orders of magnitude less training data using a small base VLM on modest hardware.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† OraPOï¼Œä¸€ç§åŸºäº Oracle-educated GRPO çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ”¾å°„ç§‘æŠ¥å‘Šç”Ÿæˆ (Radiology report generation) ä¸­å¯¹å¤§è§„æ¨¡æ•°æ®å’Œé«˜ç®—åŠ›è¿‡åº¦ä¾èµ–çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶æ”¯æŒå•é˜¶æ®µã€çº¯å¼ºåŒ–å­¦ä¹  (RL-only) è®­ç»ƒï¼Œé€šè¿‡è½»é‡çº§çš„ Oracle æ­¥éª¤å°† GRPO åœ¨å¤„ç†ç½•è§æˆ–å›°éš¾ç—…ä¾‹æ—¶çš„å¤±è´¥æ¢ç´¢è½¬åŒ–ä¸ºç›´æ¥åå¥½ç›‘ç£ï¼Œæ˜¾è‘—æå‡äº†å­¦ä¹ æ•ˆç‡ã€‚é…åˆä½¿ç”¨çš„ FactS å¥–åŠ±æœºåˆ¶é€šè¿‡æå–åŸå­çº§ä¸´åºŠäº‹å®å¹¶éªŒè¯å…¶ä¸é‡‘æ ‡å‡†çš„ä¸€è‡´æ€§ï¼Œä¸ºæ¨¡å‹æä¾›äº†å¯†é›†ä¸”å¯è§£é‡Šçš„å¥å­çº§åé¦ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOraPO åœ¨ CheXpert Plus æ•°æ®é›†ä¸Šè¾¾åˆ°äº† 0.341 F1 çš„ SOTA æ€§èƒ½ï¼Œä¸”ç›¸æ¯”ç°æœ‰æŠ€æœ¯å‡å°‘äº† 2-3 ä¸ªæ•°é‡çº§çš„è®­ç»ƒæ•°æ®éœ€æ±‚ã€‚è¯¥æ–¹æ³•è¯æ˜äº†åœ¨æ™®é€šç¡¬ä»¶å’Œå°å‹åŸºç¡€è§†è§‰è¯­è¨€æ¨¡å‹ (VLM) ä¸Šï¼Œé€šè¿‡ä¼˜åŒ–çš„å¼ºåŒ–å­¦ä¹ æœºåˆ¶ä¹Ÿèƒ½å®ç°é«˜æ•ˆä¸”å…·å¤‡ä¸´åºŠçœŸå®æ€§çš„æŠ¥å‘Šç”Ÿæˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18600v1",
      "published_date": "2025-09-23 03:42:26 UTC",
      "updated_date": "2025-09-23 03:42:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:41:57.866559+00:00"
    },
    {
      "arxiv_id": "2509.19396v1",
      "title": "OmniFed: A Modular Framework for Configurable Federated Learning from Edge to HPC",
      "title_zh": "OmniFedï¼šé¢å‘ä»è¾¹ç¼˜åˆ°é«˜æ€§èƒ½è®¡ç®—çš„å¯é…ç½®è”é‚¦å­¦ä¹ æ¨¡å—åŒ–æ¡†æ¶",
      "authors": [
        "Sahil Tyagi",
        "Andrei Cozma",
        "Olivera Kotevska",
        "Feiyi Wang"
      ],
      "abstract": "Federated Learning (FL) is critical for edge and High Performance Computing (HPC) where data is not centralized and privacy is crucial. We present OmniFed, a modular framework designed around decoupling and clear separation of concerns for configuration, orchestration, communication, and training logic. Its architecture supports configuration-driven prototyping and code-level override-what-you-need customization. We also support different topologies, mixed communication protocols within a single deployment, and popular training algorithms. It also offers optional privacy mechanisms including Differential Privacy (DP), Homomorphic Encryption (HE), and Secure Aggregation (SA), as well as compression strategies. These capabilities are exposed through well-defined extension points, allowing users to customize topology and orchestration, learning logic, and privacy/compression plugins, all while preserving the integrity of the core system. We evaluate multiple models and algorithms to measure various performance metrics. By unifying topology configuration, mixed-protocol communication, and pluggable modules in one stack, OmniFed streamlines FL deployment across heterogeneous environments. Github repository is available at https://github.com/at-aaims/OmniFed.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† OmniFedï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºä»è¾¹ç¼˜è®¡ç®— (Edge) åˆ°é«˜æ€§èƒ½è®¡ç®— (HPC) ç¯å¢ƒè®¾è®¡çš„æ¨¡å—åŒ–è”é‚¦å­¦ä¹  (Federated Learning) æ¡†æ¶ã€‚å…¶æ¶æ„æ ¸å¿ƒå®ç°äº†é…ç½®ã€ç¼–æ’ã€é€šä¿¡å’Œè®­ç»ƒé€»è¾‘çš„å®Œå…¨è§£è€¦ï¼Œæ”¯æŒé…ç½®é©±åŠ¨çš„åŸå‹å¼€å‘ä¸çµæ´»çš„ä»£ç çº§è‡ªå®šä¹‰ã€‚OmniFed æ”¯æŒå¤šç§ç½‘ç»œæ‹“æ‰‘ç»“æ„åŠæ··åˆé€šä¿¡åè®®ï¼Œå¹¶é›†æˆäº†å·®åˆ†éšç§ (Differential Privacy)ã€åŒæ€åŠ å¯† (Homomorphic Encryption) å’Œå®‰å…¨èšåˆ (Secure Aggregation) ç­‰å¯é€‰éšç§ä¿æŠ¤æœºåˆ¶ä¸å‹ç¼©ç­–ç•¥ã€‚é€šè¿‡å®šä¹‰æ˜ç¡®çš„æ‰©å±•ç‚¹ï¼Œè¯¥æ¡†æ¶å…è®¸ç”¨æˆ·åœ¨ä¸ç ´åæ ¸å¿ƒç³»ç»Ÿå®Œæ•´æ€§çš„æƒ…å†µä¸‹ï¼Œæ·±åº¦å®šåˆ¶å­¦ä¹ é€»è¾‘ä¸æ’ä»¶ã€‚å®éªŒè¯„ä¼°è¯æ˜äº† OmniFed åœ¨å¤šç§æ¨¡å‹å’Œç®—æ³•ä¸‹çš„æ€§èƒ½è¡¨ç°ï¼Œé€šè¿‡ç»Ÿä¸€çš„åè®®ç®¡ç†å’Œå¯æ’æ‹”æ¨¡å—ï¼Œè¯¥æ¡†æ¶æ˜¾è‘—ç®€åŒ–äº†è·¨å¼‚æ„ç¯å¢ƒçš„è”é‚¦å­¦ä¹ éƒ¨ç½²æµç¨‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.19396v1",
      "published_date": "2025-09-23 03:40:22 UTC",
      "updated_date": "2025-09-23 03:40:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:41:58.954900+00:00"
    },
    {
      "arxiv_id": "2509.18592v1",
      "title": "VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic Vision-Language Planning for Zero-Shot Transfer in Robot Navigation",
      "title_zh": "VLN-Zeroï¼šé¢å‘æœºå™¨äººå¯¼èˆªé›¶æ ·æœ¬è¿ç§»çš„å¿«é€Ÿæ¢ç´¢ä¸ç¼“å­˜å¢å¼ºå‹ç¥ç»ç¬¦å·è§†è§‰è¯­è¨€è§„åˆ’",
      "authors": [
        "Neel P. Bhatt",
        "Yunhao Yang",
        "Rohan Siva",
        "Pranay Samineni",
        "Daniel Milan",
        "Zhangyang Wang",
        "Ufuk Topcu"
      ],
      "abstract": "Rapid adaptation in unseen environments is essential for scalable real-world autonomy, yet existing approaches rely on exhaustive exploration or rigid navigation policies that fail to generalize. We present VLN-Zero, a two-phase vision-language navigation framework that leverages vision-language models to efficiently construct symbolic scene graphs and enable zero-shot neurosymbolic navigation. In the exploration phase, structured prompts guide VLM-based search toward informative and diverse trajectories, yielding compact scene graph representations. In the deployment phase, a neurosymbolic planner reasons over the scene graph and environmental observations to generate executable plans, while a cache-enabled execution module accelerates adaptation by reusing previously computed task-location trajectories. By combining rapid exploration, symbolic reasoning, and cache-enabled execution, the proposed framework overcomes the computational inefficiency and poor generalization of prior vision-language navigation methods, enabling robust and scalable decision-making in unseen environments. VLN-Zero achieves 2x higher success rate compared to state-of-the-art zero-shot models, outperforms most fine-tuned baselines, and reaches goal locations in half the time with 55% fewer VLM calls on average compared to state-of-the-art models across diverse environments. Codebase, datasets, and videos for VLN-Zero are available at: https://vln-zero.github.io/.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† VLN-Zeroï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„è§†è§‰è¯­è¨€å¯¼èˆª (Vision-Language Navigation) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æœºå™¨äººå¯¼èˆªåœ¨æœªçŸ¥ç¯å¢ƒä¸­çš„é›¶æ ·æœ¬è¿ç§» (Zero-Shot Transfer) é—®é¢˜ã€‚åœ¨æ¢ç´¢é˜¶æ®µï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ç»“æ„åŒ–æç¤ºå¼•å¯¼è§†è§‰è¯­è¨€æ¨¡å‹ (VLM) æ„å»ºç´§å‡‘çš„ç¬¦å·åœºæ™¯å›¾ (Symbolic Scene Graphs)ï¼›è€Œåœ¨éƒ¨ç½²é˜¶æ®µï¼Œç¥ç»ç¬¦å·è§„åˆ’å™¨ (Neurosymbolic Planner) ç»“åˆç¯å¢ƒè§‚æµ‹ç”Ÿæˆè®¡åˆ’ï¼Œå¹¶é€šè¿‡ç¼“å­˜æ‰§è¡Œæ¨¡å— (Cache-Enabled Execution Module) é‡ç”¨å†å²è½¨è¿¹ä»¥åŠ é€Ÿé€‚åº”ã€‚é€šè¿‡æ•´åˆå¿«é€Ÿæ¢ç´¢ã€ç¬¦å·æ¨ç†ä¸ç¼“å­˜æŠ€æœ¯ï¼ŒVLN-Zero å…‹æœäº†ç°æœ‰æ–¹æ³•è®¡ç®—æ•ˆç‡ä½å’Œæ³›åŒ–æ€§å·®çš„ç¼ºé™·ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶çš„æˆåŠŸç‡è¾¾åˆ°å½“å‰æœ€å…ˆè¿›é›¶æ ·æœ¬æ¨¡å‹çš„ä¸¤å€ï¼Œä¸”åœ¨å¯¼èˆªé€Ÿåº¦æå‡ä¸€å€çš„åŒæ—¶ï¼Œå¹³å‡å‡å°‘äº† 55% çš„ VLM è°ƒç”¨é‡ã€‚è¿™ä¸€ç ”ç©¶ä¸ºåœ¨å¤šæ ·åŒ–æœªçŸ¥ç¯å¢ƒä¸­å®ç°ç¨³å¥ã€å¯æ‰©å±•çš„è‡ªä¸»æœºå™¨äººå¯¼èˆªæä¾›äº†é«˜æ•ˆçš„å†³ç­–æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "Codebase, datasets, and videos for VLN-Zero are available at: https://vln-zero.github.io/",
      "pdf_url": "https://arxiv.org/pdf/2509.18592v1",
      "published_date": "2025-09-23 03:23:03 UTC",
      "updated_date": "2025-09-23 03:23:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:42:19.289174+00:00"
    },
    {
      "arxiv_id": "2509.18585v1",
      "title": "TsqLoRA: Towards Sensitivity and Quality Low-Rank Adaptation for Efficient Fine-Tuning",
      "title_zh": "TsqLoRAï¼šé¢å‘é«˜æ•ˆå¾®è°ƒçš„æ•æ„Ÿåº¦ä¸è´¨é‡æ„ŸçŸ¥ä½ç§©è‡ªé€‚åº”",
      "authors": [
        "Yu Chen",
        "Yifei Han",
        "Long Zhang",
        "Yue Du",
        "Bin Li"
      ],
      "abstract": "Fine-tuning large pre-trained models for downstream tasks has become a fundamental approach in natural language processing. Fully fine-tuning all model parameters is computationally expensive and memory-intensive, especially in resource-constrained environments. Existing parameter-efficient fine-tuning methods reduce the number of trainable parameters but typically overlook the varying sensitivity of different model layers and the importance of training data. In this work, we propose TsqLoRA, a novel method that integrates data-quality-driven selection with sensitivity-aware low-rank adaptation, consisted of two main components: a quality-aware sampling mechanism for selecting the most informative training data, and a dynamic rank allocation module that adjusts the rank of each layer based on its sensitivity to parameter updates. The experimental results demonstrate that TsqLoRA improves fine-tuning efficiency while maintaining or even improving performance on a variety of NLP tasks. Our code will be available at https://github.com/Benjamin-Ricky/TsqLoRA.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†TsqLoRAï¼Œä¸€ç§æ—¨åœ¨å®ç°é«˜æ•ˆå¾®è°ƒçš„æ•æ„Ÿåº¦ä¸è´¨é‡æ„ŸçŸ¥ä½ç§©è‡ªé€‚åº”(Low-Rank Adaptation)æ–¹æ³•ã€‚é’ˆå¯¹å…¨å‚æ•°å¾®è°ƒè®¡ç®—æˆæœ¬é«˜æ˜‚ä»¥åŠç°æœ‰å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•å¿½è§†å±‚æ•æ„Ÿåº¦å’Œæ•°æ®è´¨é‡çš„é—®é¢˜ï¼ŒTsqLoRAé›†æˆäº†é«˜è´¨é‡é‡‡æ ·æœºåˆ¶(quality-aware sampling mechanism)ä¸åŠ¨æ€ç§©åˆ†é…æ¨¡å—(dynamic rank allocation module)ã€‚é«˜è´¨é‡é‡‡æ ·æœºåˆ¶ç”¨äºç­›é€‰æœ€å…·ä¿¡æ¯é‡çš„è®­ç»ƒæ•°æ®ï¼Œè€ŒåŠ¨æ€ç§©åˆ†é…æ¨¡å—åˆ™æ ¹æ®æ¨¡å‹å„å±‚å¯¹å‚æ•°æ›´æ–°çš„æ•æ„Ÿåº¦åŠ¨æ€è°ƒæ•´å…¶ç§©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTsqLoRAåœ¨å¤šç§è‡ªç„¶è¯­è¨€å¤„ç†(NLP)ä»»åŠ¡ä¸­ä¸ä»…æ˜¾è‘—æå‡äº†å¾®è°ƒæ•ˆç‡ï¼Œè¿˜ä¿æŒæˆ–å¢å¼ºäº†æ¨¡å‹æ€§èƒ½ã€‚è¯¥æ–¹æ³•ä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„é¢„è®­ç»ƒæ¨¡å‹ä¸‹æ¸¸ä»»åŠ¡é€‚é…æä¾›äº†æ›´å…·é’ˆå¯¹æ€§ä¸”çµæ´»çš„ä¼˜åŒ–æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "5 pages, 4 figures, published to ICASSP2026",
      "pdf_url": "https://arxiv.org/pdf/2509.18585v1",
      "published_date": "2025-09-23 03:10:41 UTC",
      "updated_date": "2025-09-23 03:10:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:42:15.891815+00:00"
    },
    {
      "arxiv_id": "2509.18576v1",
      "title": "LCMF: Lightweight Cross-Modality Mambaformer for Embodied Robotics VQA",
      "title_zh": "LCMFï¼šé¢å‘å…·èº«æœºå™¨äººè§†è§‰é—®ç­”çš„è½»é‡çº§è·¨æ¨¡æ€ Mambaformer",
      "authors": [
        "Zeyi Kang",
        "Liang He",
        "Yanxin Zhang",
        "Zuheng Ming",
        "Kaixing Zhao"
      ],
      "abstract": "Multimodal semantic learning plays a critical role in embodied intelligence, especially when robots perceive their surroundings, understand human instructions, and make intelligent decisions. However, the field faces technical challenges such as effective fusion of heterogeneous data and computational efficiency in resource-constrained environments. To address these challenges, this study proposes the lightweight LCMF cascaded attention framework, introducing a multi-level cross-modal parameter sharing mechanism into the Mamba module. By integrating the advantages of Cross-Attention and Selective parameter-sharing State Space Models (SSMs), the framework achieves efficient fusion of heterogeneous modalities and semantic complementary alignment. Experimental results show that LCMF surpasses existing multimodal baselines with an accuracy of 74.29% in VQA tasks and achieves competitive mid-tier performance within the distribution cluster of Large Language Model Agents (LLM Agents) in EQA video tasks. Its lightweight design achieves a 4.35-fold reduction in FLOPs relative to the average of comparable baselines while using only 166.51M parameters (image-text) and 219M parameters (video-text), providing an efficient solution for Human-Robot Interaction (HRI) applications in resource-constrained scenarios with strong multimodal decision generalization capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LCMFï¼Œä¸€ç§è½»é‡çº§çš„çº§è”æ³¨æ„åŠ›æ¡†æ¶(cascaded attention framework)ï¼Œæ—¨åœ¨è§£å†³å…·èº«æ™ºèƒ½æœºå™¨äººåœ¨æ„ŸçŸ¥ç¯å¢ƒå’Œå¤„ç†å¼‚æ„æ•°æ®æ—¶é¢ä¸´çš„èåˆæ•ˆæœæ¬ ä½³åŠè®¡ç®—èµ„æºå—é™ç­‰æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡åœ¨Mambaæ¨¡å—ä¸­å¼•å…¥å¤šå±‚è·¨æ¨¡æ€å‚æ•°å…±äº«æœºåˆ¶ï¼Œæœ‰æœºç»“åˆäº†Cross-Attentionä¸é€‰æ‹©æ€§å‚æ•°å…±äº«çš„çŠ¶æ€ç©ºé—´æ¨¡å‹(State Space Models, SSMs)ï¼Œå®ç°äº†é«˜æ•ˆçš„å¼‚æ„æ¨¡æ€èåˆä¸è¯­ä¹‰è¡¥å……å¯¹é½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLCMFåœ¨VQAä»»åŠ¡ä¸­è¾¾åˆ°äº†74.29%çš„å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å¤šæ¨¡æ€åŸºå‡†æ¨¡å‹ï¼Œå¹¶åœ¨EQAè§†é¢‘ä»»åŠ¡ä¸­å±•ç°å‡ºä¸LLM Agentsç›¸å½“çš„ç«äº‰åŠ›ã€‚å‡­å€Ÿå…¶è½»é‡åŒ–è®¾è®¡ï¼ŒLCMFçš„FLOPsè¾ƒåŒç±»åŸºçº¿å¹³å‡å€¼é™ä½äº†4.35å€ï¼Œä¸”å‚æ•°é‡ä»…éœ€166.51Mè‡³219Mã€‚è¯¥ç ”ç©¶ä¸ºèµ„æºå—é™åœºæ™¯ä¸‹çš„äººæœºäº¤äº’(Human-Robot Interaction, HRI)æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå…·å¤‡å¼ºå¤§çš„å¤šæ¨¡æ€å†³ç­–æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18576v1",
      "published_date": "2025-09-23 02:57:25 UTC",
      "updated_date": "2025-09-23 02:57:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:42:31.981029+00:00"
    },
    {
      "arxiv_id": "2509.18575v1",
      "title": "The Ranking Blind Spot: Decision Hijacking in LLM-based Text Ranking",
      "title_zh": "æ’åºç›²ç‚¹ï¼šåŸºäº LLM çš„æ–‡æœ¬æ’åºä¸­çš„å†³ç­–åŠ«æŒ",
      "authors": [
        "Yaoyao Qian",
        "Yifan Zeng",
        "Yuchao Jiang",
        "Chelsi Jain",
        "Huazheng Wang"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated strong performance in information retrieval tasks like passage ranking. Our research examines how instruction-following capabilities in LLMs interact with multi-document comparison tasks, identifying what we term the \"Ranking Blind Spot\", a characteristic of LLM decision processes during comparative evaluation. We analyze how this ranking blind spot affects LLM evaluation systems through two approaches: Decision Objective Hijacking, which alters the evaluation goal in pairwise ranking systems, and Decision Criteria Hijacking, which modifies relevance standards across ranking schemes. These approaches demonstrate how content providers could potentially influence LLM-based ranking systems to affect document positioning. These attacks aim to force the LLM ranker to prefer a specific passage and rank it at the top. Malicious content providers can exploit this weakness, which helps them gain additional exposure by attacking the ranker. In our experiment, We empirically show that the proposed attacks are effective in various LLMs and can be generalized to multiple ranking schemes. We apply these attack to realistic examples to show their effectiveness. We also found stronger LLMs are more vulnerable to these attacks. Our code is available at: https://github.com/blindspotorg/RankingBlindSpot",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„æ–‡æœ¬æ’åºç³»ç»Ÿä¸­çš„â€œæ’åºç›²ç‚¹â€(Ranking Blind Spot)é—®é¢˜ï¼Œæ­ç¤ºäº†LLMsåœ¨å¤„ç†å¤šæ–‡æ¡£æ¯”è¾ƒä»»åŠ¡æ—¶å­˜åœ¨çš„å†³ç­–ç¼ºé™·ã€‚ä½œè€…æå‡ºäº†å†³ç­–ç›®æ ‡åŠ«æŒ(Decision Objective Hijacking)å’Œå†³ç­–æ ‡å‡†åŠ«æŒ(Decision Criteria Hijacking)ä¸¤ç§æ–¹æ³•ï¼Œå‰è€…æ—¨åœ¨æ”¹å˜æˆå¯¹æ’åºç³»ç»Ÿä¸­çš„è¯„ä¼°ç›®æ ‡ï¼Œåè€…åˆ™é€šè¿‡ä¿®æ”¹ç›¸å…³æ€§æ ‡å‡†æ¥æ“çºµæ’åç»“æœã€‚è¿™äº›æ”»å‡»æ‰‹æ®µèƒ½å¤Ÿè¯±å¯¼LLMæ’åºå™¨ä¼˜å…ˆé€‰æ‹©ç‰¹å®šæ–‡æ¡£å¹¶å°†å…¶æ’åœ¨é¦–ä½ï¼Œä½¿æ¶æ„å†…å®¹æä¾›å•†èƒ½å¤Ÿé€šè¿‡æ”»å‡»æ’åºç³»ç»Ÿè·å¾—é¢å¤–æ›å…‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›æ”»å‡»æ–¹æ³•åœ¨å¤šç§LLMå’Œä¸åŒæ’åºæ–¹æ¡ˆä¸­å‡å…·æœ‰æ˜¾è‘—çš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œæ€§èƒ½è¶Šå¼ºå¤§çš„LLMsåè€Œæ›´å®¹æ˜“å—åˆ°è¿™ç±»å†³ç­–åŠ«æŒæ”»å‡»çš„å½±å“ã€‚è¯¥ç ”ç©¶ä¸ºç†è§£LLMæ’åºç³»ç»Ÿçš„å®‰å…¨æ€§æä¾›äº†æ–°çš„è§†è§’ï¼Œå¹¶æ­ç¤ºäº†å½“å‰å¤§æ¨¡å‹è¯„ä¼°ç³»ç»Ÿåœ¨é¢å¯¹æ¶æ„æŒ‡ä»¤è¯±å¯¼æ—¶çš„è„†å¼±æ€§ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted by EMNLP 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.18575v1",
      "published_date": "2025-09-23 02:56:38 UTC",
      "updated_date": "2025-09-23 02:56:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:42:25.794531+00:00"
    },
    {
      "arxiv_id": "2509.18573v1",
      "title": "Interaction Topological Transformer for Multiscale Learning in Porous Materials",
      "title_zh": "é¢å‘å¤šå­”ææ–™å¤šå°ºåº¦å­¦ä¹ çš„äº¤äº’æ‹“æ‰‘ Transformer",
      "authors": [
        "Dong Chen",
        "Jian Liu",
        "Chun-Long Chen",
        "Guo-Wei Wei"
      ],
      "abstract": "Porous materials exhibit vast structural diversity and support critical applications in gas storage, separations, and catalysis. However, predictive modeling remains challenging due to the multiscale nature of structure-property relationships, where performance is governed by both local chemical environments and global pore-network topology. These complexities, combined with sparse and unevenly distributed labeled data, hinder generalization across material families. We propose the Interaction Topological Transformer (ITT), a unified data-efficient framework that leverages novel interaction topology to capture materials information across multiple scales and multiple levels, including structural, elemental, atomic, and pairwise-elemental organization. ITT extracts scale-aware features that reflect both compositional and relational structure within complex porous frameworks, and integrates them through a built-in Transformer architecture that supports joint reasoning across scales. Trained using a two-stage strategy, i.e., self-supervised pretraining on 0.6 million unlabeled structures followed by supervised fine-tuning, ITT achieves state-of-the-art, accurate, and transferable predictions for adsorption, transport, and stability properties. This framework provides a principled and scalable path for learning-guided discovery in structurally and chemically diverse porous materials.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Interaction Topological Transformer (ITT)ï¼Œä¸€ç§ç”¨äºå¤šå­”ææ–™å¤šå°ºåº¦å­¦ä¹ çš„ç»Ÿä¸€æ•°æ®é«˜æ•ˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç”±å±€éƒ¨åŒ–å­¦ç¯å¢ƒå’Œå…¨å±€å­”éš™ç½‘ç»œæ‹“æ‰‘å…±åŒå†³å®šçš„ç»“æ„-æ€§è´¨å…³ç³»é¢„æµ‹éš¾é¢˜ã€‚ITT å¼•å…¥äº†æ–°é¢–çš„äº¤äº’æ‹“æ‰‘æ¥æ•è·æ¶µç›–ç»“æ„ã€å…ƒç´ ã€åŸå­å’Œæˆå¯¹å…ƒç´ ç»„ç»‡çš„å¤šå°ºåº¦ä¿¡æ¯ï¼Œå¹¶åˆ©ç”¨å†…ç½®çš„ Transformer æ¶æ„è¿›è¡Œè·¨å°ºåº¦çš„è”åˆæ¨ç†ã€‚ç ”ç©¶é‡‡ç”¨äº†ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œå³å…ˆåœ¨ 60 ä¸‡ä¸ªæ— æ ‡è®°ç»“æ„ä¸Šè¿›è¡Œè‡ªç›‘ç£é¢„è®­ç»ƒ (self-supervised pretraining)ï¼Œéšåç»“åˆæ ‡è®°æ•°æ®è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒ (supervised fine-tuning)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒITT åœ¨é¢„æµ‹å¤šå­”ææ–™çš„å¸é™„ (adsorption)ã€ä¼ è¾“ (transport) å’Œç¨³å®šæ€§ (stability) å±æ€§æ–¹é¢è¾¾åˆ°äº†é¢†åŸŸé¢†å…ˆ (state-of-the-art) çš„æ°´å¹³ã€‚è¯¥æ¡†æ¶ä¸ä»…å®ç°äº†é«˜ç²¾åº¦å’Œå¼ºè¿ç§»æ€§çš„æ€§èƒ½é¢„æµ‹ï¼Œè¿˜ä¸ºå‘ç°åŒ–å­¦å¤šæ ·æ€§ä¸°å¯Œçš„å¤šå­”ææ–™æä¾›äº†ä¸€æ¡åŸåˆ™æ€§ä¸”å¯æ‰©å±•çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cond-mat.mtrl-sci",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "4 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.18573v1",
      "published_date": "2025-09-23 02:56:05 UTC",
      "updated_date": "2025-09-23 02:56:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:42:32.259988+00:00"
    },
    {
      "arxiv_id": "2509.18569v1",
      "title": "Explore the Reinforcement Learning for the LLM based ASR and TTS system",
      "title_zh": "åŸºäº LLM çš„ ASR ä¸ TTS ç³»ç»Ÿå¼ºåŒ–å­¦ä¹ æ¢ç´¢",
      "authors": [
        "Changfeng Gao",
        "Yabin Li",
        "Keyu An",
        "Zhifu Gao",
        "Zhihao Du",
        "Han Zhao",
        "Xiangang Li"
      ],
      "abstract": "In recent years, large language models (LLMs) have played an important role in automatic speech recognition (ASR) and text-to-speech (TTS) systems. While reinforcement learning (RL) has significantly enhanced LLM performance in text-based tasks, its application to ASR and TTS remains underexplored due to the complexity of training audio-based models. In this study, we propose a lightweight RL framework tailored for audio-based LLMs that can process audio inputs and generate audio outputs. Based on this framework, we evaluate the effectiveness of reinforcement learning on both ASR and TTS tasks. For the ASR task, we experiment with different rule-based reward functions within the Group Relative Policy Optimization (GRPO) framework and investigate the impact of RL data construction. For the TTS task, we compare GRPO with Differentiable Reward Optimization (DiffRO) and further combine the two approaches to achieve improved performance. Our experiments demonstrate that RL can significantly enhance the performance of both ASR and TTS systems, even with limited training data and a small number of optimization steps.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ (RL)åœ¨åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«(ASR)å’Œæ–‡æœ¬è½¬è¯­éŸ³(TTS)ç³»ç»Ÿä¸­çš„åº”ç”¨ã€‚é’ˆå¯¹éŸ³é¢‘æ¨¡å‹è®­ç»ƒå¤æ‚æ€§çš„æŒ‘æˆ˜ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªä¸“ä¸ºéŸ³é¢‘LLMè®¾è®¡çš„è½»é‡çº§RLæ¡†æ¶ï¼Œä½¿å…¶èƒ½å¤Ÿç›´æ¥å¤„ç†éŸ³é¢‘è¾“å…¥å¹¶ç”ŸæˆéŸ³é¢‘è¾“å‡ºã€‚åœ¨ASRä»»åŠ¡ä¸­ï¼Œç ”ç©¶è€…åœ¨Group Relative Policy Optimization (GRPO)æ¡†æ¶ä¸‹å®éªŒäº†ä¸åŒçš„åŸºäºè§„åˆ™çš„å¥–åŠ±å‡½æ•°ï¼Œå¹¶æ¢ç©¶äº†å¼ºåŒ–å­¦ä¹ æ•°æ®æ„å»ºçš„å½±å“ã€‚å¯¹äºTTSä»»åŠ¡ï¼Œç ”ç©¶å¯¹æ¯”äº†GRPOä¸Differentiable Reward Optimization (DiffRO)çš„æ•ˆæœï¼Œå¹¶å°è¯•å°†ä¸¤è€…ç»“åˆä»¥è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿åœ¨è®­ç»ƒæ•°æ®æœ‰é™ä¸”ä¼˜åŒ–æ­¥éª¤è¾ƒå°‘çš„æƒ…å†µä¸‹ï¼ŒRLä¹Ÿèƒ½æ˜¾è‘—å¢å¼ºASRå’ŒTTSç³»ç»Ÿçš„è¡¨ç°ï¼Œè¯æ˜äº†è¯¥æ¡†æ¶åœ¨è¯­éŸ³ç”Ÿæˆä¸è¯†åˆ«é¢†åŸŸçš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18569v1",
      "published_date": "2025-09-23 02:52:54 UTC",
      "updated_date": "2025-09-23 02:52:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:42:46.190443+00:00"
    },
    {
      "arxiv_id": "2509.18565v1",
      "title": "Solving Math Word Problems Using Estimation Verification and Equation Generation",
      "title_zh": "åŸºäºä¼°ç®—éªŒè¯ä¸æ–¹ç¨‹ç”Ÿæˆçš„æ•°å­¦åº”ç”¨é¢˜æ±‚è§£",
      "authors": [
        "Mitchell Piehl",
        "Dillon Wilson",
        "Ananya Kalita",
        "Jugal Kalita"
      ],
      "abstract": "Large Language Models (LLMs) excel at various tasks, including problem-solving and question-answering. However, LLMs often find Math Word Problems (MWPs) challenging because solving them requires a range of reasoning and mathematical abilities with which LLMs seem to struggle. Recent efforts have helped LLMs solve more complex MWPs with improved prompts. This study proposes a novel method that initially prompts an LLM to create equations from a decomposition of the question, followed by using an external symbolic equation solver to produce an answer. To ensure the accuracy of the obtained answer, inspired by an established recommendation of math teachers, the LLM is instructed to solve the MWP a second time, but this time with the objective of estimating the correct answer instead of solving it exactly. The estimation is then compared to the generated answer to verify. If verification fails, an iterative rectification process is employed to ensure the correct answer is eventually found. This approach achieves new state-of-the-art results on datasets used by prior published research on numeric and algebraic MWPs, improving the previous best results by nearly two percent on average. In addition, the approach obtains satisfactory results on trigonometric MWPs, a task not previously attempted to the authors' best knowledge. This study also introduces two new datasets, SVAMPClean and Trig300, to further advance the testing of LLMs' reasoning abilities.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆä¼°ç®—éªŒè¯ (Estimation Verification) ä¸æ–¹ç¨‹ç”Ÿæˆçš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨æ•°å­¦åº”ç”¨é¢˜ (Math Word Problems, MWPs) æ¨ç†ä¸­çš„å±€é™æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ†è§£é—®é¢˜å¼•å¯¼æ¨¡å‹ç”Ÿæˆæ–¹ç¨‹ï¼Œå¹¶åˆ©ç”¨å¤–éƒ¨ç¬¦å·æ–¹ç¨‹æ±‚è§£å™¨ (Symbolic equation solver) è·å–ç»“æœï¼Œéšåé€šè¿‡ä¼°ç®—éªŒè¯æœºåˆ¶ä¸æ±‚è§£ç­”æ¡ˆè¿›è¡Œæ¯”å¯¹ã€‚è‹¥éªŒè¯å¤±è´¥ï¼Œç³»ç»Ÿå°†å¯åŠ¨è¿­ä»£çº é”™ (Iterative rectification) è¿‡ç¨‹ä»¥ç¡®ä¿ç­”æ¡ˆçš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æ•°å€¼å’Œä»£æ•° MWPs æ•°æ®é›†ä¸Šåˆ·æ–°äº† SOTA è®°å½•ï¼Œå‡†ç¡®ç‡å¹³å‡æå‡äº†çº¦ 2%ï¼Œå¹¶é¦–æ¬¡åœ¨ä¸‰è§’å‡½æ•° MWPs ä»»åŠ¡ä¸­å–å¾—äº†ç†æƒ³æ•ˆæœã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº† SVAMPClean å’Œ Trig300 ä¸¤ä¸ªæ–°æ•°æ®é›†ï¼Œä¸ºè¿›ä¸€æ­¥æµ‹è¯•æ¨¡å‹çš„æ¨ç†èƒ½åŠ›åšå‡ºäº†è´¡çŒ®ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to IEEE ICMLA 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.18565v1",
      "published_date": "2025-09-23 02:41:39 UTC",
      "updated_date": "2025-09-23 02:41:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:42:44.392806+00:00"
    },
    {
      "arxiv_id": "2509.18562v2",
      "title": "CPCLDETECTOR: Knowledge Enhancement and Alignment Selection for Chinese Patronizing and Condescending Language Detection",
      "title_zh": "CPCLDETECTORï¼šåŸºäºçŸ¥è¯†å¢å¼ºä¸å¯¹é½é€‰æ‹©çš„ä¸­æ–‡å±…é«˜ä¸´ä¸‹ä¸å‚²æ…¢è¯­è¨€æ£€æµ‹",
      "authors": [
        "Jiaxun Yang",
        "Yifei Han",
        "Long Zhang",
        "Yujie Liu",
        "Bin Li",
        "Bo Gao",
        "Yangfan He",
        "Kejia Zhan"
      ],
      "abstract": "Chinese Patronizing and Condescending Language (CPCL) is an implicitly discriminatory toxic speech targeting vulnerable groups on Chinese video platforms. The existing dataset lacks user comments, which are a direct reflection of video content. This undermines the model's understanding of video content and results in the failure to detect some CPLC videos. To make up for this loss, this research reconstructs a new dataset PCLMMPLUS that includes 103k comment entries and expands the dataset size. We also propose the CPCLDetector model with alignment selection and knowledge-enhanced comment content modules. Extensive experiments show the proposed CPCLDetector outperforms the SOTA on PCLMM and achieves higher performance on PCLMMPLUS . CPLC videos are detected more accurately, supporting content governance and protecting vulnerable groups. Code and dataset are available at https://github.com/jiaxunyang256/PCLD.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¸­æ–‡è§†é¢‘å¹³å°ä¸Šé’ˆå¯¹å¼±åŠ¿ç¾¤ä½“çš„éšæ€§æ­§è§†æ€§æ¯’æ€§è¨€è®ºï¼Œå³ä¸­æ–‡å±…é«˜ä¸´ä¸‹çš„èµ„åŠ©æ€§è¯­è¨€(CPCL)ï¼ŒæŒ‡å‡ºç°æœ‰æ•°æ®é›†å› ç¼ºå¤±ç”¨æˆ·è¯„è®ºå†…å®¹è€Œå¯¼è‡´æ¨¡å‹å¯¹è§†é¢‘è¯­å¢ƒç†è§£ä¸è¶³åŠæ£€æµ‹å¤±è´¥ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿé‡æ„å¹¶å‘å¸ƒäº†åŒ…å«10.3ä¸‡æ¡è¯„è®ºæ¡ç›®çš„æ•°æ®é›†PCLMMPLUSï¼Œæ˜¾è‘—æ‰©å……äº†æ•°æ®è§„æ¨¡ã€‚åŒæ—¶ï¼Œè¯¥ç ”ç©¶æå‡ºäº†CPCLDetectoræ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡é›†æˆå¯¹é½é€‰æ‹©(Alignment Selection)å’ŒçŸ¥è¯†å¢å¼ºè¯„è®ºå†…å®¹(Knowledge-Enhanced Comment Content)æ¨¡å—ï¼Œæœ‰æ•ˆæå‡äº†ç‰¹å¾è¡¨è¾¾èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCPCLDetectoråœ¨PCLMMå’ŒPCLMMPLUSæ•°æ®é›†ä¸Šçš„æ€§èƒ½å‡ä¼˜äºç°æœ‰çš„SOTAæ¨¡å‹ï¼Œå®ç°äº†å¯¹CPCLè§†é¢‘æ›´ç²¾ç¡®çš„è‡ªåŠ¨æ£€æµ‹ã€‚è¯¥ç ”ç©¶ä¸ä»…ä¸ºäº’è”ç½‘å†…å®¹æ²»ç†æä¾›äº†é‡è¦çš„æŠ€æœ¯æ‰‹æ®µï¼Œä¹Ÿä¸ºä¿æŠ¤ç½‘ç»œç©ºé—´ä¸­çš„å¼±åŠ¿ç¾¤ä½“åšå‡ºäº†è´¡çŒ®ã€‚",
      "categories": [
        "cs.MM",
        "cs.AI"
      ],
      "primary_category": "cs.MM",
      "comment": "Submitted to ICASSP 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.18562v2",
      "published_date": "2025-09-23 02:38:49 UTC",
      "updated_date": "2025-09-24 03:29:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:43:08.692341+00:00"
    },
    {
      "arxiv_id": "2509.18561v2",
      "title": "SoundCompass: Navigating Target Sound Extraction With Effective Directional Clue Integration In Complex Acoustic Scenes",
      "title_zh": "SoundCompassï¼šå¤æ‚å£°å­¦åœºæ™¯ä¸‹é€šè¿‡æœ‰æ•ˆæ–¹å‘æ€§çº¿ç´¢é›†æˆå¼•å¯¼ç›®æ ‡å£°éŸ³æå–",
      "authors": [
        "Dayun Choi",
        "Jung-Woo Choi"
      ],
      "abstract": "Recent advances in target sound extraction (TSE) utilize directional clues derived from direction of arrival (DoA), which represent an inherent spatial property of sound available in any acoustic scene. However, previous DoA-based methods rely on hand-crafted features or discrete encodings, which lose fine-grained spatial information and limit adaptability. We propose SoundCompass, an effective directional clue integration framework centered on a Spectral Pairwise INteraction (SPIN) module that captures cross-channel spatial correlations in the complex spectrogram domain to preserve full spatial information in multichannel signals. The input feature expressed in terms of spatial correlations is fused with a DoA clue represented as spherical harmonics (SH) encoding. The fusion is carried out across overlapping frequency subbands, inheriting the benefits reported in the previous band-split architectures. We also incorporate the iterative refinement strategy, chain-of-inference (CoI), in the TSE framework, which recursively fuses DoA with sound event activation estimated from the previous inference stage. Experiments demonstrate that SoundCompass, combining SPIN, SH embedding, and CoI, robustly extracts target sources across diverse signal classes and spatial configurations.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SoundCompassï¼Œä¸€ä¸ªæ—¨åœ¨æå‡å¤æ‚å£°å­¦åœºæ™¯ä¸­ç›®æ ‡å£°éŸ³æå– (Target Sound Extraction, TSE) æ€§èƒ½çš„æ–¹å‘çº¿ç´¢é›†æˆæ¡†æ¶ã€‚ä¸ºäº†è§£å†³ä»¥å¾€åŸºäºåˆ°è¾¾æ–¹å‘ (DoA) çš„æ–¹æ³•å› æ‰‹å·¥ç‰¹å¾æˆ–ç¦»æ•£ç¼–ç å¯¼è‡´ç©ºé—´ä¿¡æ¯ä¸¢å¤±çš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†é¢‘è°±æˆå¯¹äº¤äº’ (Spectral Pairwise INteraction, SPIN) æ¨¡å—ï¼Œé€šè¿‡åœ¨å¤æ•°é¢‘è°±å›¾åŸŸæ•è·è·¨é€šé“ç©ºé—´ç›¸å…³æ€§æ¥ä¿ç•™å®Œæ•´çš„ç©ºé—´ä¿¡æ¯ã€‚SoundCompass å°†è¿™äº›ç©ºé—´ç‰¹å¾ä¸çƒè°å‡½æ•° (Spherical Harmonics, SH) ç¼–ç çš„ DoA çº¿ç´¢åœ¨é‡å é¢‘å¸¦ä¸Šè¿›è¡Œèåˆï¼Œå¹¶é‡‡ç”¨äº†æ¨ç†é“¾ (Chain-of-Inference, CoI) è¿­ä»£ç»†åŒ–ç­–ç•¥ï¼Œå°† DoA ä¸å‰ä¸€é˜¶æ®µä¼°è®¡çš„å£°éŸ³äº‹ä»¶æ¿€æ´»è¿›è¡Œé€’å½’ç»“åˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSoundCompass ç»“åˆäº† SPINã€SH åµŒå…¥å’Œ CoI çš„ä¼˜åŠ¿ï¼Œåœ¨å¤šç§ä¿¡å·ç±»åˆ«å’Œç©ºé—´é…ç½®ä¸‹å‡èƒ½ç¨³å¥åœ°æå–ç›®æ ‡å£°æºï¼Œæ˜¾è‘—æå‡äº†æå–çš„ç²¾ç¡®åº¦ä¸é€‚åº”æ€§ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "5 pages, 4 figures, accepted to ICASSP 2026",
      "pdf_url": "https://arxiv.org/pdf/2509.18561v2",
      "published_date": "2025-09-23 02:36:39 UTC",
      "updated_date": "2026-01-19 02:19:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:42:51.990120+00:00"
    },
    {
      "arxiv_id": "2509.18557v1",
      "title": "LLMZ+: Contextual Prompt Whitelist Principles for Agentic LLMs",
      "title_zh": "LLMZ+ï¼šé¢å‘æ™ºèƒ½ä½“å¤§è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡æç¤ºè¯ç™½åå•å‡†åˆ™",
      "authors": [
        "Tom Pawelek",
        "Raj Patel",
        "Charlotte Crowell",
        "Noorbakhsh Amiri",
        "Sudip Mittal",
        "Shahram Rahimi",
        "Andy Perkins"
      ],
      "abstract": "Compared to traditional models, agentic AI represents a highly valuable target for potential attackers as they possess privileged access to data sources and API tools, which are traditionally not incorporated into classical agents. Unlike a typical software application residing in a Demilitarized Zone (DMZ), agentic LLMs consciously rely on nondeterministic behavior of the AI (only defining a final goal, leaving the path selection to LLM). This characteristic introduces substantial security risk to both operational security and information security. Most common existing defense mechanism rely on detection of malicious intent and preventing it from reaching the LLM agent, thus protecting against jailbreak attacks such as prompt injection. In this paper, we present an alternative approach, LLMZ+, which moves beyond traditional detection-based approaches by implementing prompt whitelisting. Through this method, only contextually appropriate and safe messages are permitted to interact with the agentic LLM. By leveraging the specificity of context, LLMZ+ guarantees that all exchanges between external users and the LLM conform to predefined use cases and operational boundaries. Our approach streamlines the security framework, enhances its long-term resilience, and reduces the resources required for sustaining LLM information security. Our empirical evaluation demonstrates that LLMZ+ provides strong resilience against the most common jailbreak prompts. At the same time, legitimate business communications are not disrupted, and authorized traffic flows seamlessly between users and the agentic LLM. We measure the effectiveness of approach using false positive and false negative rates, both of which can be reduced to 0 in our experimental setting.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ™ºèƒ½ä½“AI (agentic AI) å› å…·å¤‡ç‰¹æƒè®¿é—®æƒé™å’Œéç¡®å®šæ€§è¡Œä¸ºè€Œé¢ä¸´çš„é‡å¤§å®‰å…¨é£é™©ï¼Œæå‡ºäº†åä¸ºLLMZ+çš„æ–°å‹é˜²å¾¡æ¡†æ¶ã€‚ä¸åŒäºä¼ ç»ŸåŸºäºæ¶æ„æ„å›¾æ£€æµ‹çš„é˜²å¾¡æœºåˆ¶ï¼ŒLLMZ+é‡‡ç”¨äº†ä¸Šä¸‹æ–‡æç¤ºè¯ç™½åå• (prompt whitelisting) ç­–ç•¥ï¼Œä»…å…è®¸ç¬¦åˆè¯­å¢ƒä¸”å®‰å…¨çš„æŒ‡ä»¤ä¸æ™ºèƒ½ä½“å¤§è¯­è¨€æ¨¡å‹ (LLM) è¿›è¡Œäº¤äº’ã€‚é€šè¿‡åˆ©ç”¨ä¸Šä¸‹æ–‡çš„ç‰¹å®šæ€§ï¼Œè¯¥æ–¹æ³•ç¡®ä¿æ‰€æœ‰å¤–éƒ¨äº¤äº’å‡å¤„äºé¢„å®šä¹‰çš„ç”¨ä¾‹å’Œæ“ä½œè¾¹ç•Œå†…ï¼Œä»è€Œæ˜¾è‘—å¢å¼ºäº†å®‰å…¨ç³»ç»Ÿçš„é•¿æœŸéŸ§æ€§å¹¶é™ä½äº†èµ„æºæ¶ˆè€—ã€‚å®éªŒè¯„ä¼°è¯æ˜ï¼ŒLLMZ+èƒ½æœ‰æ•ˆæŠµå¾¡æœ€å¸¸è§çš„è¶Šç‹±æ”»å‡» (jailbreak attacks)ï¼ŒåŒæ—¶ç¡®ä¿åˆæ³•ä¸šåŠ¡é€šä¿¡çš„æ— ç¼æµè½¬ã€‚åœ¨å®éªŒç¯å¢ƒä¸‹ï¼Œè¯¥æ–¹æ³•çš„è¯¯æŠ¥ç‡ (false positive) å’Œæ¼æŠ¥ç‡ (false negative) å‡å¯é™ä½è‡³é›¶ï¼Œä¸ºæ™ºèƒ½ä½“å¤§è¯­è¨€æ¨¡å‹çš„ä¿¡æ¯å®‰å…¨æä¾›äº†é«˜æ•ˆä¸”å¯é çš„ä¿éšœã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "7 pages, 5 figures, to be published and presented at ICMLA 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.18557v1",
      "published_date": "2025-09-23 02:30:14 UTC",
      "updated_date": "2025-09-23 02:30:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:42:54.685770+00:00"
    },
    {
      "arxiv_id": "2509.18552v1",
      "title": "Global Minimizers of Sigmoid Contrastive Loss",
      "title_zh": "Sigmoid å¯¹æ¯”æŸå¤±çš„å…¨å±€æå°å€¼",
      "authors": [
        "Kiril Bangachev",
        "Guy Bresler",
        "Iliyas Noman",
        "Yury Polyanskiy"
      ],
      "abstract": "The meta-task of obtaining and aligning representations through contrastive pretraining is steadily gaining importance since its introduction in CLIP and ALIGN. In this paper we theoretically explain the advantages of synchronizing with trainable inverse temperature and bias under the sigmoid loss, as implemented in the recent SigLIP and SigLIP2 models of Google DeepMind. Temperature and bias can drive the loss function to zero for a rich class of configurations that we call $(\\mathsf{m}, \\mathsf{b}_{\\mathsf{rel}})$-Constellations. $(\\mathsf{m}, \\mathsf{b}_{\\mathsf{rel}})$-Constellations are a novel combinatorial object related to spherical codes and are parametrized by a margin $\\mathsf{m}$ and relative bias $\\mathsf{b}_{\\mathsf{rel}}$. We use our characterization of constellations to theoretically justify the success of SigLIP on retrieval, to explain the modality gap present in SigLIP, and to identify the necessary dimension for producing high-quality representations. Finally, we propose a reparameterization of the sigmoid loss with explicit relative bias, which improves training dynamics in experiments with synthetic data.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†SigLIPå’ŒSigLIP2æ¨¡å‹ä¸­é‡‡ç”¨çš„Sigmoidå¯¹æ¯”æŸå¤±(Sigmoid Contrastive Loss)çš„ç†è®ºç‰¹æ€§ï¼Œåˆç†è§£é‡Šäº†å¯è®­ç»ƒé€†æ¸©åº¦(inverse temperature)å’Œåå·®(bias)åœ¨è¡¨å¾å¯¹é½ä¸­çš„å…³é”®ä¼˜åŠ¿ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åä¸º$(\\mathsf{m}, \\mathsf{b}_{\\mathsf{rel}})$-Constellationsçš„æ–°å‹ç»„åˆå¯¹è±¡æ¥åˆ»ç”»è¯¥æŸå¤±å‡½æ•°çš„å…¨å±€æœ€å°å€¼ï¼Œè¿™ç±»å¯¹è±¡ä¸çƒé¢ç¼–ç (spherical codes)å¯†åˆ‡ç›¸å…³ã€‚é€šè¿‡è¿™ä¸€ç†è®ºåˆ»ç”»ï¼Œç ”ç©¶ä¸ä»…é˜æ˜äº†SigLIPåœ¨æ£€ç´¢ä»»åŠ¡ä¸Šçš„æˆåŠŸåŠæ¨¡æ€é—´éš™(modality gap)çš„æˆå› ï¼Œè¿˜ç¡®å®šäº†ç”Ÿæˆé«˜è´¨é‡è¡¨å¾æ‰€éœ€çš„å¿…è¦ç»´åº¦ã€‚æ­¤å¤–ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§å…·æœ‰æ˜¾å¼ç›¸å¯¹åå·®çš„é‡å‚æ•°åŒ–(reparameterization)æ–¹æ³•ï¼Œåœ¨åˆæˆæ•°æ®å®éªŒä¸­è¯æ˜äº†è¯¥æ–¹æ³•èƒ½æœ‰æ•ˆæ”¹å–„è®­ç»ƒåŠ¨åŠ›å­¦ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Author names listed in alphabetical order. NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.18552v1",
      "published_date": "2025-09-23 02:24:23 UTC",
      "updated_date": "2025-09-23 02:24:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:43:34.589044+00:00"
    },
    {
      "arxiv_id": "2509.18542v2",
      "title": "Symphony-MoE: Harmonizing Disparate Pre-trained Models into a Coherent Mixture-of-Experts",
      "title_zh": "Symphony-MoEï¼šå°†å¼‚è´¨é¢„è®­ç»ƒæ¨¡å‹è°ƒå’Œä¸ºç»Ÿä¸€çš„æ··åˆä¸“å®¶æ¨¡å‹",
      "authors": [
        "Qi Wang",
        "Hanyang Peng",
        "Yue Yu"
      ],
      "abstract": "Mixture-of-Experts (MoE) models enable scalable performance by activating large parameter sets sparsely, minimizing computational overhead. To mitigate the prohibitive cost of training MoEs from scratch, recent work employs upcycling, reusing a single pre-trained dense model by replicating its feed-forward network (FFN) layers into experts. However, this limits expert diversity, as all experts originate from a single pre-trained dense model. This paper addresses this limitation by constructing powerful MoE models using experts sourced from multiple identically-architected but disparate pre-trained models (e.g., Qwen2.5-Coder and Qwen2). A key challenge lies in the fact that these source models occupy disparate, dissonant regions of the parameter space, making direct upcycling prone to severe performance degradation. To overcome this, we propose Symphony-MoE, a novel two-stage framework designed to harmonize these models into a single, coherent expert mixture. First, we establish this harmony in a training-free manner: we construct a shared backbone via a layer-aware fusion strategy and, crucially, alleviate parameter misalignment among experts using activation-based functional alignment. Subsequently, a stage of post-training coordinates the entire architecture. Experiments demonstrate that our method successfully integrates experts from heterogeneous sources, achieving an MoE model that significantly surpasses baselines in multi-domain tasks and out-of-distribution generalization.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Symphony-MoEï¼Œä¸€ä¸ªæ—¨åœ¨å°†å¤šä¸ªæ¶æ„ç›¸åŒä½†å‚æ•°ç©ºé—´ä¸åŒçš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚Qwen2.5-Coderå’ŒQwen2ï¼‰å’Œè°é›†æˆåˆ°ç»Ÿä¸€æ··åˆä¸“å®¶æ¨¡å‹(Mixture-of-Experts, MoE)ä¸­çš„ä¸¤é˜¶æ®µæ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰ä¸Šé‡‡æ ·(Upcycling)æŠ€æœ¯å› ä»…ä½¿ç”¨å•ä¸€æ¨¡å‹è€Œå¯¼è‡´ä¸“å®¶å¤šæ ·æ€§ä¸è¶³çš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶é¦–å…ˆé€šè¿‡å±‚æ„ŸçŸ¥èåˆç­–ç•¥(layer-aware fusion)æ„å»ºå…±äº«éª¨å¹²ï¼Œå¹¶åˆ©ç”¨åŸºäºæ¿€æ´»çš„å‡½æ•°å¯¹é½(activation-based functional alignment)åœ¨æ— è®­ç»ƒçš„æƒ…å†µä¸‹ç¼“è§£ä¸“å®¶é—´çš„å‚æ•°å¤±é…ã€‚éšåï¼Œç ”ç©¶é€šè¿‡åæœŸè®­ç»ƒ(post-training)é˜¶æ®µå¯¹æ•´ä½“æ¶æ„è¿›è¡Œåè°ƒï¼Œç¡®ä¿äº†æ¥è‡ªå¼‚æ„æ¥æºçš„ä¸“å®¶èƒ½å¤Ÿé«˜æ•ˆååŒã€‚å®éªŒè¯æ˜ï¼ŒSymphony-MoEåœ¨å¤šé¢†åŸŸä»»åŠ¡å’Œåˆ†å¸ƒå¤–(out-of-distribution)æ³›åŒ–èƒ½åŠ›ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚è¯¥æ–¹æ³•æˆåŠŸå…‹æœäº†ç›´æ¥æ•´åˆå¼‚æ„æ¨¡å‹å¯¼è‡´çš„æ€§èƒ½è¡°å‡ï¼Œä¸ºæ„å»ºå…·æœ‰å¤šæ ·åŒ–ä¸“ä¸šèƒ½åŠ›çš„å¼ºå¤§MoEæ¨¡å‹æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18542v2",
      "published_date": "2025-09-23 02:07:14 UTC",
      "updated_date": "2025-11-12 06:47:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:43:36.739854+00:00"
    },
    {
      "arxiv_id": "2509.18536v1",
      "title": "CCQA: Generating Question from Solution Can Improve Inference-Time Reasoning in SLMs",
      "title_zh": "CCQAï¼šåŸºäºè§£ç­”ç”Ÿæˆé—®é¢˜å¯æå‡å°è¯­è¨€æ¨¡å‹çš„æ¨ç†æ—¶æ¨ç†",
      "authors": [
        "Jin Young Kim",
        "Ji Won Yoon"
      ],
      "abstract": "Recently, inference-time reasoning strategies have further improved the accuracy of large language models (LLMs), but their effectiveness on smaller models remains unclear. Based on the observation that conventional approaches often fail to improve performance in this context, we propose \\textbf{C}ycle-\\textbf{C}onsistency in \\textbf{Q}uestion \\textbf{A}nswering (CCQA), a novel reasoning method that can be effectively applied to SLMs. Inspired by cycle consistency, CCQA generates a question from each reasoning path and answer, evaluates each by its similarity to the original question, and then selects the candidate solution with the highest similarity score as the final response. Since conventional SLMs struggle to generate accurate questions from their own reasoning paths and answers, we employ a lightweight Flan-T5 model specialized for question generation to support this process efficiently. From the experimental results, it is verified that CCQA consistently outperforms existing state-of-the-art (SOTA) methods across eight models on mathematical and commonsense reasoning benchmarks. Furthermore, our method establishes a new practical baseline for efficient reasoning in SLMs. Source code can be found at https://github.com/scai-research/ccqa_official.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†é—®ç­”å¾ªç¯ä¸€è‡´æ€§(Cycle-Consistency in Question Answering, CCQA)ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨ç†æ—¶æ¨ç†ç­–ç•¥(Inference-time reasoning strategies)åœ¨å°è¯­è¨€æ¨¡å‹(SLMs)ä¸Šæ•ˆæœä¸ä½³çš„é—®é¢˜ã€‚å—å¾ªç¯ä¸€è‡´æ€§å¯å‘ï¼ŒCCQA ä»ç”Ÿæˆçš„æ¯æ¡æ¨ç†è·¯å¾„å’Œç­”æ¡ˆä¸­åå‘æ„é€ é—®é¢˜ï¼Œå¹¶æ ¹æ®è¯¥ç”Ÿæˆé—®é¢˜ä¸åŸå§‹é—®é¢˜çš„ç›¸ä¼¼åº¦æ¥è¯„ä¼°å¹¶ç­›é€‰æœ€ä¼˜è§£ã€‚ç”±äº SLMs è‡ªèº«éš¾ä»¥ç”Ÿæˆå‡†ç¡®é—®é¢˜ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨äº†ä¸€ä¸ªè½»é‡çº§çš„ Flan-T5 æ¨¡å‹ä¸“é—¨è´Ÿè´£é—®é¢˜ç”Ÿæˆè¿‡ç¨‹ï¼Œä»è€Œåœ¨ä¿æŒæ•ˆç‡çš„åŒæ—¶æ”¯æŒéªŒè¯ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒCCQA åœ¨å…«ä¸ªæ¨¡å‹ä¸Šçš„æ•°å­¦å’Œå¸¸è¯†æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºç°æœ‰çš„ SOTA æ–¹æ³•ã€‚è¯¥ç ”ç©¶ä¸º SLMs çš„é«˜æ•ˆæ¨ç†å»ºç«‹äº†ä¸€ä¸ªå®ç”¨çš„æ–°åŸºå‡†ï¼Œè¯æ˜äº†é€šè¿‡â€œç”±è§£åæ±‚é¢˜â€çš„éªŒè¯æœºåˆ¶èƒ½æ˜¾è‘—æå‡å°å‹æ¨¡å‹çš„æ¨ç†ç²¾åº¦ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Published as a main conference paper at EMNLP 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.18536v1",
      "published_date": "2025-09-23 02:01:03 UTC",
      "updated_date": "2025-09-23 02:01:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:43:33.387477+00:00"
    },
    {
      "arxiv_id": "2509.18531v1",
      "title": "No Verifiable Reward for Prosody: Toward Preference-Guided Prosody Learning in TTS",
      "title_zh": "éŸµå¾‹ç¼ºä¹å¯éªŒè¯å¥–åŠ±ï¼šè¿ˆå‘è¯­éŸ³åˆæˆä¸­çš„åå¥½å¼•å¯¼éŸµå¾‹å­¦ä¹ ",
      "authors": [
        "Seungyoun Shin",
        "Dongha Ahn",
        "Jiwoo Kim",
        "Sungwook Jeon"
      ],
      "abstract": "Recent work reports gains in neural text-to-speech (TTS) with Group Relative Policy Optimization (GRPO). However, in the absence of a verifiable reward for \\textit{prosody}, GRPO trained on transcription-oriented signals (CER/NLL) lowers error rates yet collapses prosody into monotone, unnatural speech; adding speaker-similarity further destabilizes training and degrades CER. We address this with an \\textit{iterative Direct Preference Optimization (DPO)} scheme that uses only a few hundred human-labeled preference pairs per round to directly optimize prosodic naturalness while regularizing to the current model. On \\textbf{KoCC-TTS}, a curated dataset of authentic Korean call center interactions capturing task-oriented dialogues, our method attains the highest human preference (ELO) with competitive CER, outperforming GRPO and strong commercial baselines. These results suggest that when prosody cannot be rewarded automatically, \\textit{human preference optimization} offers a practical and data-efficient path to natural and robust TTS. The demo page is available at \\href{https://tts.ch.dev}",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç¥ç»æ–‡æœ¬è½¬è¯­éŸ³(TTS)ä¸­éŸµå¾‹(prosody)å­¦ä¹ çš„æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºåœ¨ç¼ºä¹å¯éªŒè¯å¥–åŠ±çš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨Group Relative Policy Optimization (GRPO)è®­ç»ƒçš„æ¨¡å‹è™½ç„¶èƒ½é™ä½å­—é”™ç‡(CER)ï¼Œä½†ä¼šå¯¼è‡´è¯­éŸ³å˜å¾—å•è°ƒä¸”ä¸è‡ªç„¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§è¿­ä»£å¼ç›´æ¥åå¥½ä¼˜åŒ–(Iterative Direct Preference Optimization, DPO)æ–¹æ¡ˆã€‚è¯¥æ–¹æ¡ˆæ¯è½®ä»…éœ€æ•°ç™¾å¯¹äººå·¥æ ‡æ³¨çš„åå¥½æ•°æ®ï¼Œé€šè¿‡ç›´æ¥ä¼˜åŒ–éŸµå¾‹è‡ªç„¶åº¦å¹¶å¯¹å½“å‰æ¨¡å‹è¿›è¡Œæ­£åˆ™åŒ–ï¼Œå®ç°äº†æ›´é«˜æ•ˆçš„å­¦ä¹ ã€‚å®éªŒåœ¨åŒ…å«çœŸå®éŸ©å›½å‘¼å«ä¸­å¿ƒå¯¹è¯çš„KoCC-TTSæ•°æ®é›†ä¸Šè¿›è¡Œï¼Œç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨ä¿æŒç«äº‰åŠ›çš„CERçš„åŒæ—¶ï¼Œè·å¾—äº†æœ€é«˜çš„äººç±»åå¥½è¯„åˆ†(ELO)ï¼Œè¡¨ç°ä¼˜äºGRPOåŠå¼ºåŠ›å•†ç”¨åŸºçº¿æ¨¡å‹ã€‚è¿™è¡¨æ˜åœ¨éŸµå¾‹éš¾ä»¥é€šè¿‡è‡ªåŠ¨æ–¹å¼å¥–åŠ±çš„æƒ…å†µä¸‹ï¼ŒåŸºäºäººç±»åå¥½çš„ä¼˜åŒ–ä¸ºå®ç°è‡ªç„¶ä¸”é²æ£’çš„TTSæä¾›äº†ä¸€æ¡å®ç”¨ä¸”æ•°æ®é«˜æ•ˆçš„è·¯å¾„ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "submitted to ICASSP 2026",
      "pdf_url": "https://arxiv.org/pdf/2509.18531v1",
      "published_date": "2025-09-23 01:51:38 UTC",
      "updated_date": "2025-09-23 01:51:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:43:53.699807+00:00"
    },
    {
      "arxiv_id": "2509.18527v4",
      "title": "FERA: A Pose-Based Semantic Pipeline for Automated Foil Fencing Refereeing",
      "title_zh": "FERAï¼šä¸€ç§é¢å‘èŠ±å‰‘è‡ªåŠ¨æ‰§è£çš„åŸºäºå§¿æ€è¯­ä¹‰ç®¡çº¿",
      "authors": [
        "Ziwen Chen",
        "Zhong Wang"
      ],
      "abstract": "Many multimedia tasks map raw video into structured semantic representations for downstream decision-making. Sports officiating is a representative case, where fast, subtle interactions must be judged via symbolic rules. We present FERA (FEncing Referee Assistant), a pose-based framework that turns broadcast foil fencing video into action tokens and rule-grounded explanations. From monocular footage, FERA extracts 2D poses, converts them into a 101-dimensional kinematic representation, and applies an encoder-only transformer (FERA-MDT) to recognize per-fencer footwork, blade actions, and blade-line position. To obtain a consistent single-fencer representation for both athletes, FERA processes each clip and a horizontally flipped copy, yielding time-aligned left/right predictions without requiring a multi-person pose pipeline. A dynamic temporal windowing scheme enables inference on untrimmed pose tracks. These structured predictions serve as tokens for a language model (FERA-LM) that applies simplified right-of-way rules to generate textual decisions. On 1,734 clips (2,386 annotated actions), FERA-MDT achieves a macro-F1 of 0.549 under 5-fold cross-validation, outperforming BiLSTM and TCN baselines. Combined with FERA-LM, the full pipeline recovers referee priority with 77.7% accuracy on 969 exchanges. FERA provides a case-study benchmark for pose-based semantic grounding in a two-person sport and illustrates a general pipeline for connecting video understanding with rule-based reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† FERA (FEncing Referee Assistant)ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå§¿æ€çš„è¯­ä¹‰æµç¨‹æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°èŠ±å‰‘(foil fencing)ç«èµ›çš„è‡ªåŠ¨åŒ–è£åˆ¤ã€‚è¯¥ç³»ç»Ÿé€šè¿‡ä»å•ç›®è§†é¢‘(monocular footage)ä¸­æå– 2D å§¿æ€å¹¶å°†å…¶è½¬åŒ–ä¸º 101 ç»´çš„è¿åŠ¨å­¦è¡¨ç¤º(kinematic representation)ï¼Œåˆ©ç”¨ä»…ç¼–ç å™¨çš„è½¬æ¢å™¨ FERA-MDT è¯†åˆ«é€‰æ‰‹çš„æ­¥ä¼ã€å‰‘èº«åŠ¨ä½œåŠç©ºé—´ä½ç½®ã€‚ä¸ºäº†ä¿æŒåŒäººç«æŠ€ä¸­çš„ä¸€è‡´æ€§ï¼ŒFERA ç»“åˆäº†æ°´å¹³é•œåƒå¤„ç†æŠ€æœ¯å’ŒåŠ¨æ€æ—¶é—´çª—å£æ–¹æ¡ˆ(dynamic temporal windowing)ï¼Œå®ç°äº†å¯¹æœªä¿®å‰ªå§¿æ€è½¨è¿¹çš„æœ‰æ•ˆæ¨ç†ã€‚ç»“æ„åŒ–é¢„æµ‹éšåä½œä¸ºæ ‡è®°è¾“å…¥è¯­è¨€æ¨¡å‹ FERA-LMï¼Œé€šè¿‡åº”ç”¨ç®€åŒ–çš„ä¼˜å…ˆæƒè§„åˆ™(right-of-way rules)ç”Ÿæˆæœ€ç»ˆçš„æ–‡æœ¬è£å†³ã€‚å®éªŒè¡¨æ˜ï¼ŒFERA-MDT çš„æ€§èƒ½ä¼˜äº BiLSTM å’Œ TCN ç­‰åŸºå‡†æ¨¡å‹ï¼Œæ•´å¥—æµç¨‹åœ¨åˆ¤å®šè£åˆ¤ä¼˜å…ˆæƒæ–¹é¢çš„å‡†ç¡®ç‡è¾¾åˆ° 77.7%ã€‚è¯¥ç ”ç©¶ä¸ºåŒäººè¿åŠ¨ä¸­åŸºäºå§¿æ€çš„è¯­ä¹‰æ¥åœ°(semantic grounding)æä¾›äº†åŸºå‡†æ¡ˆä¾‹ï¼Œå¹¶å±•ç¤ºäº†å°†è§†é¢‘ç†è§£ä¸åŸºäºè§„åˆ™çš„æ¨ç†ç›¸ç»“åˆçš„é€šç”¨èŒƒå¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Updated Methodology and polished sections",
      "pdf_url": "https://arxiv.org/pdf/2509.18527v4",
      "published_date": "2025-09-23 01:47:44 UTC",
      "updated_date": "2025-12-24 14:04:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:43:58.977441+00:00"
    },
    {
      "arxiv_id": "2509.18523v1",
      "title": "Automatic coherence-driven inference on arguments",
      "title_zh": "è‡ªåŠ¨åŒ–è¿è´¯æ€§é©±åŠ¨è®ºè¯æ¨ç†",
      "authors": [
        "Steve Huntsman"
      ],
      "abstract": "Inconsistencies are ubiquitous in law, administration, and jurisprudence. Though a cure is too much to hope for, we propose a technological remedy. Large language models (LLMs) can accurately extract propositions from arguments and compile them into natural data structures that enable coherence-driven inference (CDI) via combinatorial optimization. This neurosymbolic architecture naturally separates concerns and enables meaningful judgments about the coherence of arguments that can inform legislative and policy analysis and legal reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ³•å¾‹ã€è¡Œæ”¿å’Œæ³•å­¦é¢†åŸŸæ™®éå­˜åœ¨çš„ä¸ä¸€è‡´æ€§é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæŠ€æœ¯çš„è¡¥æ•‘æ–¹æ¡ˆã€‚ç ”ç©¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) ä»è®ºç‚¹ä¸­å‡†ç¡®æå–å‘½é¢˜ï¼Œå¹¶å°†å…¶ç¼–è¯‘ä¸ºè‡ªç„¶æ•°æ®ç»“æ„ï¼Œä»è€Œé€šè¿‡ç»„åˆä¼˜åŒ– (combinatorial optimization) å®ç°è¿è´¯é©±åŠ¨æ¨ç† (CDI)ã€‚è¿™ç§ç¥ç»ç¬¦å·æ¶æ„ (neurosymbolic architecture) å®ç°äº†å…³æ³¨ç‚¹çš„è‡ªç„¶åˆ†ç¦»ï¼Œèƒ½å¤Ÿå¯¹è®ºç‚¹çš„è¿è´¯æ€§åšå‡ºæœ‰æ„ä¹‰çš„åˆ¤æ–­ã€‚è¯¥æ–¹æ³•ä¸ºç«‹æ³•å’Œæ”¿ç­–åˆ†æä»¥åŠæ³•å¾‹æ¨ç†æä¾›äº†é‡è¦æ”¯æŒï¼Œæœ‰æ•ˆæå‡äº†æ³•å¾‹è®ºè¯çš„é€»è¾‘ä¸¥å¯†æ€§ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "Workshop on Data Mining and AI for Law (https://dmail-workshop.github.io/DMAIL2025/)",
      "pdf_url": "https://arxiv.org/pdf/2509.18523v1",
      "published_date": "2025-09-23 01:40:14 UTC",
      "updated_date": "2025-09-23 01:40:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:43:52.295273+00:00"
    },
    {
      "arxiv_id": "2509.18521v3",
      "title": "APRIL: Active Partial Rollouts in Reinforcement Learning to Tame Long-tail Generation",
      "title_zh": "APRILï¼šå¼ºåŒ–å­¦ä¹ ä¸­é€šè¿‡ä¸»åŠ¨éƒ¨åˆ†å±•å¼€è§£å†³é•¿å°¾ç”Ÿæˆé—®é¢˜",
      "authors": [
        "Yuzhen Zhou",
        "Jiajun Li",
        "Yusheng Su",
        "Gowtham Ramesh",
        "Zilin Zhu",
        "Xiang Long",
        "Chenyang Zhao",
        "Jin Pan",
        "Xiaodong Yu",
        "Ze Wang",
        "Kangrui Du",
        "Jialian Wu",
        "Ximeng Sun",
        "Jiang Liu",
        "Qiaolin Yu",
        "Hao Chen",
        "Zicheng Liu",
        "Emad Barsoum"
      ],
      "abstract": "Reinforcement learning (RL) has become a cornerstone in advancing large-scale pre-trained language models (LLMs). Successive generations, including GPT-o series, DeepSeek-R1, Kimi-K1.5, Grok 4, and GLM-4.5, have relied on large-scale RL training to enhance reasoning and coding capabilities. To meet the community's growing RL needs, numerous RL frameworks have been proposed. However, RL training remains computationally expensive, with rollout generation accounting for more than 90% of total runtime. In addition, its efficiency is often constrained by the long-tail distribution of rollout response lengths, where a few lengthy responses stall entire batches, leaving GPUs idle and underutilized. As model and rollout sizes continue to grow, this bottleneck increasingly limits scalability. To address this challenge, we propose Active Partial Rollouts in Reinforcement Learning (APRIL), which mitigates long-tail inefficiency. In the rollout phase, APRIL over-provisions rollout requests, terminates once the target number of responses is reached, and recycles incomplete responses for continuation in future steps. This strategy ensures that no rollouts are discarded while substantially reducing GPU idle time. Experiments show that APRIL improves rollout throughput by 22.5% on average (at most 44%) across commonly used RL algorithms (GRPO, DAPO, GSPO), accelerates convergence, and achieves 2.1% on average(at most 8%) higher final accuracy across tasks. Moreover, APRIL is both framework and hardware agnostic, already integrated into the slime RL framework, and deployable on NVIDIA and AMD GPUs alike. Taken together, this work unifies system-level and algorithmic considerations in proposing APRIL, with the aim of advancing RL training efficiency and inspiring further optimizations in RL systems. Our codebase is available at https://github.com/RLsys-Foundation/APRIL",
      "tldr_zh": "åœ¨å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„å¼ºåŒ–å­¦ä¹ (RL)è®­ç»ƒä¸­ï¼ŒRolloutç”Ÿæˆå æ®äº†è¶…è¿‡90%çš„è¿è¡Œæ—¶é—´ï¼Œè€Œå“åº”é•¿åº¦çš„é•¿å°¾åˆ†å¸ƒ(Long-tail Distribution)å¸¸å¯¼è‡´GPUèµ„æºå› ç­‰å¾…æå°‘æ•°å†—é•¿å“åº”è€Œå¤„äºé—²ç½®çŠ¶æ€ï¼Œé™åˆ¶äº†è®­ç»ƒçš„å¯æ‰©å±•æ€§ã€‚è¯¥ç ”ç©¶æå‡ºäº†APRIL (Active Partial Rollouts in Reinforcement Learning) æ¡†æ¶ï¼Œé€šè¿‡åœ¨Rollouté˜¶æ®µé‡‡ç”¨è¿‡é‡ä¾›ç»™(Over-provision)ç­–ç•¥ï¼Œå¹¶åœ¨è¾¾åˆ°ç›®æ ‡å“åº”æ•°ååŠæ—¶ç»ˆæ­¢å½“å‰æ‰¹æ¬¡æ¥ç¼“è§£è¿™ä¸€ç“¶é¢ˆã€‚APRILèƒ½å¤Ÿå¾ªç¯åˆ©ç”¨(Recycle)æœªå®Œæˆçš„å“åº”å¹¶åœ¨åç»­æ­¥éª¤ä¸­ç»§ç»­æ‰§è¡Œï¼Œåœ¨ç¡®ä¿æ•°æ®ä¸è¢«åºŸå¼ƒçš„åŒæ—¶æ˜¾è‘—å‡å°‘äº†ç¡¬ä»¶é—²ç½®æ—¶é—´ã€‚å®éªŒè¡¨æ˜ï¼ŒAPRILåœ¨GRPOã€DAPOå’ŒGSPOç­‰å¸¸ç”¨ç®—æ³•ä¸Šå¹³å‡æå‡äº†22.5%çš„ååé‡ï¼Œæœ€é«˜å¢å¹…è¾¾44%ï¼Œå¹¶æ˜¾è‘—åŠ é€Ÿäº†æ¨¡å‹æ”¶æ•›ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨å„é¡¹ä»»åŠ¡ä¸­å¹³å‡æå‡äº†2.1%çš„æœ€ç»ˆå‡†ç¡®ç‡ï¼Œä¸”å…·å¤‡å‡ºè‰²çš„æ¡†æ¶ä¸ç¡¬ä»¶æ— å…³æ€§ï¼Œæ”¯æŒNVIDIAå’ŒAMD GPUã€‚ç›®å‰APRILå·²é›†æˆè‡³slime RLæ¡†æ¶å¹¶å¼€æºï¼Œä¸ºæå‡å¤§è§„æ¨¡RLè®­ç»ƒæ•ˆç‡æä¾›äº†ç³»ç»Ÿçº§ä¸ç®—æ³•ååŒä¼˜åŒ–çš„æ–°èŒƒå¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18521v3",
      "published_date": "2025-09-23 01:32:36 UTC",
      "updated_date": "2025-09-26 22:20:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:43:55.350316+00:00"
    },
    {
      "arxiv_id": "2509.18520v1",
      "title": "Coherence-driven inference for cybersecurity",
      "title_zh": "é¢å‘ç½‘ç»œå®‰å…¨çš„è¿è´¯é©±åŠ¨æ¨ç†",
      "authors": [
        "Steve Huntsman"
      ],
      "abstract": "Large language models (LLMs) can compile weighted graphs on natural language data to enable automatic coherence-driven inference (CDI) relevant to red and blue team operations in cybersecurity. This represents an early application of automatic CDI that holds near- to medium-term promise for decision-making in cybersecurity and eventually also for autonomous blue team operations.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨ Large Language Models (LLMs) åœ¨è‡ªç„¶è¯­è¨€æ•°æ®ä¸Šæ„å»ºåŠ æƒå›¾ï¼Œæ—¨åœ¨å®ç°ä¸ç½‘ç»œå®‰å…¨ä¸­çº¢é˜Ÿå’Œè“é˜Ÿæ“ä½œç›¸å…³çš„è‡ªåŠ¨åŒ– Coherence-driven inference (CDI)ã€‚è¯¥æ–¹æ³•ä»£è¡¨äº†è‡ªåŠ¨åŒ– CDI çš„æ—©æœŸåº”ç”¨ï¼Œåœ¨ç½‘ç»œå®‰å…¨å†³ç­–é¢†åŸŸå…·æœ‰æ˜¾è‘—çš„ä¸­çŸ­æœŸåº”ç”¨å‰æ™¯ã€‚é€šè¿‡å°†è‡ªç„¶è¯­è¨€è½¬åŒ–ä¸ºç»“æ„åŒ–æ¨ç†å›¾ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è¾…åŠ©å®‰å…¨ä¸“å®¶è¿›è¡Œå¨èƒåˆ†æä¸æˆ˜æœ¯å†³ç­–ã€‚ç ”ç©¶å¼ºè°ƒï¼Œè¿™ç§è¿è´¯é©±åŠ¨çš„æ¨ç†æŠ€æœ¯æœ€ç»ˆå°†æœ‰æœ›åŠ©åŠ›å®ç°è‡ªä¸»åŒ–çš„è“é˜Ÿæ“ä½œ (autonomous blue team operations)ã€‚è¯¥é¡¹å·¥ä½œä¸º LLMs åœ¨é«˜æ•æ„Ÿã€é«˜å¤æ‚åº¦çš„ç½‘ç»œå®‰å…¨ç¯å¢ƒä¸­çš„è½åœ°åº”ç”¨æä¾›äº†æ–°çš„è§†è§’å’ŒæŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "LLM4Sec - Workshop on the use of Large Language Models for Cybersecurity (https://llm4sec-workshop.github.io/)",
      "pdf_url": "https://arxiv.org/pdf/2509.18520v1",
      "published_date": "2025-09-23 01:32:28 UTC",
      "updated_date": "2025-09-23 01:32:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:44:01.274843+00:00"
    },
    {
      "arxiv_id": "2509.18514v1",
      "title": "A Rhythm-Aware Phrase Insertion for Classical Arabic Poetry Composition",
      "title_zh": "é¢å‘å¤å…¸é˜¿æ‹‰ä¼¯è¯—æ­Œåˆ›ä½œçš„èŠ‚å¥æ„ŸçŸ¥çŸ­è¯­æ’å…¥",
      "authors": [
        "Mohamad Elzohbi",
        "Richard Zhao"
      ],
      "abstract": "This paper presents a methodology for inserting phrases in Arabic poems to conform to a specific rhythm using ByT5, a byte-level multilingual transformer-based model. Our work discusses a rule-based grapheme-to-beat transformation tailored for extracting the rhythm from fully diacritized Arabic script. Our approach employs a conditional denoising objective to fine-tune ByT5, where the model reconstructs masked words to match a target rhythm. We adopt a curriculum learning strategy, pre-training on a general Arabic dataset before fine-tuning on poetic dataset, and explore cross-lingual transfer from English to Arabic. Experimental results demonstrate that our models achieve high rhythmic alignment while maintaining semantic coherence. The proposed model has the potential to be used in co-creative applications in the process of composing classical Arabic poems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é’ˆå¯¹å¤å…¸é˜¿æ‹‰ä¼¯è¯—æ­Œåˆ›ä½œçš„èŠ‚å¥æ„ŸçŸ¥çŸ­è¯­æ’å…¥æ–¹æ³•ï¼Œæ—¨åœ¨ä½¿æ’å…¥çš„å†…å®¹ç¬¦åˆç‰¹å®šçš„éŸµå¾‹è¦æ±‚ã€‚ç ”ç©¶é‡‡ç”¨äº†åŸºäºå­—èŠ‚çº§çš„å¤šè¯­è¨€ Transformer æ¨¡å‹ ByT5ï¼Œå¹¶å¼€å‘äº†ä¸€å¥—åŸºäºè§„åˆ™çš„å­—ç´ åˆ°èŠ‚æ‹ (grapheme-to-beat) è½¬æ¢æœºåˆ¶ï¼Œç”¨äºä»å¸¦æœ‰å…ƒéŸ³ç¬¦å·çš„é˜¿æ‹‰ä¼¯è¯­æ–‡æœ¬ä¸­æå–èŠ‚å¥ã€‚é€šè¿‡æ¡ä»¶å»å™ª (conditional denoising) ç›®æ ‡ï¼Œæ¨¡å‹èƒ½å¤Ÿé‡å»ºè¢«æ©ç çš„è¯æ±‡ä»¥ç²¾ç¡®åŒ¹é…ç›®æ ‡éŸµå¾‹ã€‚è®­ç»ƒè¿‡ç¨‹é‡‡ç”¨äº†è¯¾ç¨‹å­¦ä¹  (curriculum learning) ç­–ç•¥ï¼Œå¹¶æ¢ç´¢äº†ä»è‹±è¯­åˆ°é˜¿æ‹‰ä¼¯è¯­çš„è·¨è¯­è¨€è¿ç§» (cross-lingual transfer)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ä¿æŒè¯­ä¹‰è¿è´¯æ€§çš„åŒæ—¶å®ç°äº†æé«˜çš„èŠ‚å¥ä¸€è‡´æ€§ã€‚è¿™é¡¹å·¥ä½œä¸ºå¤å…¸é˜¿æ‹‰ä¼¯è¯—æ­Œçš„äººæœºååŒåˆ›ä½œ (co-creative) åº”ç”¨å¥ å®šäº†æŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted for the Third Arabic Natural Language Processing Conference (ArabicNLP 2025)",
      "pdf_url": "https://arxiv.org/pdf/2509.18514v1",
      "published_date": "2025-09-23 01:22:15 UTC",
      "updated_date": "2025-09-23 01:22:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:44:06.370586+00:00"
    },
    {
      "arxiv_id": "2509.18507v1",
      "title": "Dynamical Modeling of Behaviorally Relevant Spatiotemporal Patterns in Neural Imaging Data",
      "title_zh": "ç¥ç»æˆåƒæ•°æ®ä¸­è¡Œä¸ºç›¸å…³æ—¶ç©ºæ¨¡å¼çš„åŠ¨åŠ›å­¦å»ºæ¨¡",
      "authors": [
        "Mohammad Hosseini",
        "Maryam M. Shanechi"
      ],
      "abstract": "High-dimensional imaging of neural activity, such as widefield calcium and functional ultrasound imaging, provide a rich source of information for understanding the relationship between brain activity and behavior. Accurately modeling neural dynamics in these modalities is crucial for understanding this relationship but is hindered by the high-dimensionality, complex spatiotemporal dependencies, and prevalent behaviorally irrelevant dynamics in these modalities. Existing dynamical models often employ preprocessing steps to obtain low-dimensional representations from neural image modalities. However, this process can discard behaviorally relevant information and miss spatiotemporal structure. We propose SBIND, a novel data-driven deep learning framework to model spatiotemporal dependencies in neural images and disentangle their behaviorally relevant dynamics from other neural dynamics. We validate SBIND on widefield imaging datasets, and show its extension to functional ultrasound imaging, a recent modality whose dynamical modeling has largely remained unexplored. We find that our model effectively identifies both local and long-range spatial dependencies across the brain while also dissociating behaviorally relevant neural dynamics. Doing so, SBIND outperforms existing models in neural-behavioral prediction. Overall, SBIND provides a versatile tool for investigating the neural mechanisms underlying behavior using imaging modalities.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é«˜ç»´ç¥ç»å½±åƒæ•°æ®ï¼ˆå¦‚ widefield calcium imaging å’Œ functional ultrasound imagingï¼‰ä¸­å­˜åœ¨çš„å¤æ‚æ—¶ç©ºä¾èµ–æ€§ä»¥åŠè¡Œä¸ºæ— å…³åŠ¨åŠ›å­¦å¹²æ‰°ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº† SBIND æ¡†æ¶ã€‚SBIND æ˜¯ä¸€ç§æ•°æ®é©±åŠ¨çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨å¯¹ç¥ç»å›¾åƒä¸­çš„ spatiotemporal dependencies è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶å°† behaviorally relevant dynamics ä¸å…¶ä»–ç¥ç»åŠ¨æ€è¿›è¡Œè§£è€¦ã€‚ä¸ä¼ ç»Ÿçš„å…ˆé™ç»´å†å»ºæ¨¡çš„é¢„å¤„ç†æ–¹æ³•ä¸åŒï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«å¤§è„‘è·¨åŒºåŸŸçš„å±€éƒ¨åŠé•¿ç¨‹ç©ºé—´ä¾èµ–ï¼Œé¿å…äº†è¡Œä¸ºç›¸å…³ä¿¡æ¯çš„ä¸¢å¤±ã€‚ç ”ç©¶è€…åœ¨ widefield imaging æ•°æ®é›†ä¸ŠéªŒè¯äº† SBIND çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å°†å…¶æ‰©å±•åˆ°åŠ¨åŠ›å­¦å»ºæ¨¡å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢çš„ functional ultrasound imaging é¢†åŸŸã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSBIND åœ¨ neural-behavioral prediction æ–¹é¢çš„è¡¨ç°ä¼˜äºç°æœ‰æ¨¡å‹ï¼ŒæˆåŠŸå®ç°äº†å¯¹è¡Œä¸ºç›¸å…³ç¥ç»åŠ¨æ€çš„æœ‰æ•ˆåˆ†ç¦»ã€‚æ€»ä½“è€Œè¨€ï¼ŒSBIND ä¸ºåˆ©ç”¨å„ç±»å½±åƒæ¨¡æ€æ¢ç©¶è¡Œä¸ºèƒŒåçš„ç¥ç»æœºåˆ¶æä¾›äº†ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„é€šç”¨å·¥å…·ã€‚",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "q-bio.NC",
      "comment": "Published at the 42nd International Conference on Machine Learning (ICML) 2025. Code available at: https://github.com/ShanechiLab/SBIND/",
      "pdf_url": "https://arxiv.org/pdf/2509.18507v1",
      "published_date": "2025-09-23 01:16:23 UTC",
      "updated_date": "2025-09-23 01:16:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:44:06.947098+00:00"
    },
    {
      "arxiv_id": "2509.18504v1",
      "title": "Hyperbolic Coarse-to-Fine Few-Shot Class-Incremental Learning",
      "title_zh": "åŒæ›²ç”±ç²—åˆ°ç»†çš„å°æ ·æœ¬ç±»å¢é‡å­¦ä¹ ",
      "authors": [
        "Jiaxin Dai",
        "Xiang Xiang"
      ],
      "abstract": "In the field of machine learning, hyperbolic space demonstrates superior representation capabilities for hierarchical data compared to conventional Euclidean space. This work focuses on the Coarse-To-Fine Few-Shot Class-Incremental Learning (C2FSCIL) task. Our study follows the Knowe approach, which contrastively learns coarse class labels and subsequently normalizes and freezes the classifier weights of learned fine classes in the embedding space. To better interpret the \"coarse-to-fine\" paradigm, we propose embedding the feature extractor into hyperbolic space. Specifically, we employ the PoincarÃ© ball model of hyperbolic space, enabling the feature extractor to transform input images into feature vectors within the PoincarÃ© ball instead of Euclidean space. We further introduce hyperbolic contrastive loss and hyperbolic fully-connected layers to facilitate model optimization and classification in hyperbolic space. Additionally, to enhance performance under few-shot conditions, we implement maximum entropy distribution in hyperbolic space to estimate the probability distribution of fine-class feature vectors. This allows generation of augmented features from the distribution to mitigate overfitting during training with limited samples. Experiments on C2FSCIL benchmarks show that our method effectively improves both coarse and fine class accuracies.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç²—ç²’åº¦åˆ°ç»†ç²’åº¦å°‘æ ·æœ¬ç±»å¢é‡å­¦ä¹  (Coarse-To-Fine Few-Shot Class-Incremental Learning, C2FSCIL) ä»»åŠ¡ï¼Œæå‡ºå°†ç‰¹å¾æå–å™¨åµŒå…¥åˆ°åŒæ›²ç©ºé—´ (hyperbolic space) ä¸­ï¼Œä»¥åˆ©ç”¨å…¶å¯¹å±‚çº§æ•°æ®çš„å“è¶Šè¡¨ç¤ºèƒ½åŠ›ã€‚ç ”ç©¶é‡‡ç”¨äº†åºåŠ è±çƒæ¨¡å‹ (PoincarÃ© ball model)ï¼Œä½¿ç‰¹å¾æå–å™¨èƒ½å¤Ÿåœ¨åŒæ›²ç©ºé—´ä¸­å°†è¾“å…¥å›¾åƒè½¬æ¢ä¸ºç‰¹å¾å‘é‡ï¼Œè€Œéä¼ ç»Ÿçš„æ¬§å‡ é‡Œå¾—ç©ºé—´ã€‚ä¸ºäº†è¿›ä¸€æ­¥ä¼˜åŒ–æ€§èƒ½ï¼Œä½œè€…å¼•å…¥äº†åŒæ›²å¯¹æ¯”æŸå¤± (hyperbolic contrastive loss) å’ŒåŒæ›²å…¨è¿æ¥å±‚ (hyperbolic fully-connected layers) æ¥å®ç°ç©ºé—´å†…çš„æ¨¡å‹ä¼˜åŒ–ä¸åˆ†ç±»ã€‚é’ˆå¯¹å°‘æ ·æœ¬åœºæ™¯ä¸‹çš„è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œè¯¥æ–¹æ³•åœ¨åŒæ›²ç©ºé—´ä¸­åˆ©ç”¨æœ€å¤§ç†µåˆ†å¸ƒ (maximum entropy distribution) ä¼°è®¡ç»†ç²’åº¦ç‰¹å¾åˆ†å¸ƒå¹¶ç”Ÿæˆå¢å¼ºç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ C2FSCIL åŸºå‡†æµ‹è¯•ä¸­æœ‰æ•ˆæå‡äº†ç²—ç²’åº¦ä¸ç»†ç²’åº¦ç±»åˆ«çš„å‡†ç¡®ç‡ï¼Œè¯æ˜äº†åŒæ›²å‡ ä½•åœ¨å¤„ç†â€œç”±ç²—åˆ°ç»†â€å­¦ä¹ èŒƒå¼ä¸­çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.18504v1",
      "published_date": "2025-09-23 01:12:21 UTC",
      "updated_date": "2025-09-23 01:12:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T20:44:18.857549+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 181,
  "processed_papers_count": 181,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-24T20:45:15.549684+00:00"
}
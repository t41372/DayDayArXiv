[
  {
    "arxiv_id": "2601.12186v1",
    "title": "Aletheia: What Makes RLVR For Code Verifiers Tick?",
    "authors": [
      "Vatsal Venkatkrishna",
      "Indraneil Paul",
      "Iryna Gurevych"
    ],
    "abstract": "Multi-domain thinking verifiers trained via Reinforcement Learning from Verifiable Rewards (RLVR) are a prominent fixture of the Large Language Model (LLM) post-training pipeline, owing to their ability to robustly rate and rerank model outputs. However, the adoption of such verifiers towards code generation has been comparatively sparse, with execution feedback constituting the dominant signal. Nonetheless, code verifiers remain valuable toward judging model outputs in scenarios where execution feedback is hard to obtain and are a potentially powerful addition to the code generation post-training toolbox. To this end, we create and open-source Aletheia, a controlled testbed that enables execution-grounded evaluation of code verifiers' robustness across disparate policy models and covariate shifts. We examine components of the RLVR-based verifier training recipe widely credited for its success: (1) intermediate thinking traces, (2) learning from negative samples, and (3) on-policy training. While experiments show the optimality of RLVR, we uncover important opportunities to simplify the recipe. Particularly, despite code verification exhibiting positive training- and inference-time scaling, on-policy learning stands out as the key component at small verifier sizes, and thinking-based training emerges as the most important component at larger scales.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "8 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2601.12186v1",
    "published_date": "2026-01-17 22:30:45 UTC",
    "updated_date": "2026-01-17 22:30:45 UTC"
  },
  {
    "arxiv_id": "2601.12150v1",
    "title": "Enhanced Diagnostic Performance via Large-Resolution Inference Optimization for Pathology Foundation Models",
    "authors": [
      "Mengxuan Hu",
      "Zihan Guan",
      "John Kang",
      "Sheng Li",
      "Zhongliang Zhou"
    ],
    "abstract": "Despite their prominent performance on tasks such as ROI classification and segmentation, many pathology foundation models remain constrained by a specific input size e.g. 224 x 224, creating substantial inefficiencies when applied to whole-slide images (WSIs), which span thousands of resolutions. A naive strategy is to either enlarge inputs or downsample the WSIs. However, enlarging inputs results in prohibitive GPU memory consumption, while downsampling alters the microns-per-pixel resolution and obscures critical morphological details. To overcome these limitations, we propose an space- and time- efficient inference strategy that sparsifies attention using spatially aware neighboring blocks and filters out non-informative tokens through global attention scores. This design substantially reduces GPU memory and runtime during high-resolution WSI inference while preserving and even improving the downstream performance, enabling inference at higher resolutions under the same GPU budget. The experimental results show that our method can achieves up to an 7.67% improvement in the ROI classification and compatible results in segmentation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages",
    "pdf_url": "https://arxiv.org/pdf/2601.12150v1",
    "published_date": "2026-01-17 19:50:40 UTC",
    "updated_date": "2026-01-17 19:50:40 UTC"
  },
  {
    "arxiv_id": "2601.12147v1",
    "title": "Segment and Matte Anything in a Unified Model",
    "authors": [
      "Zezhong Fan",
      "Xiaohan Li",
      "Topojoy Biswas",
      "Kaushiki Nag",
      "Kannan Achan"
    ],
    "abstract": "Segment Anything (SAM) has recently pushed the boundaries of segmentation by demonstrating zero-shot generalization and flexible prompting after training on over one billion masks. Despite this, its mask prediction accuracy often falls short of the precision required in real-world applications. While several refinement modules have been proposed to boost SAM's segmentation quality, achieving highly accurate object delineation within a single, unified framework remains an open challenge. Furthermore, interactive image matting, which aims to generate fine-grained alpha mattes guided by diverse user hints, has not yet been explored in the context of SAM. Insights from recent studies highlight strong correlations between segmentation and matting, suggesting the feasibility of a unified model capable of both tasks. In this paper, we introduce Segment And Matte Anything (SAMA), a lightweight extension of SAM that delivers high-quality interactive image segmentation and matting with minimal extra parameters. Our Multi-View Localization Encoder (MVLE) captures detailed features from local views, while the Localization Adapter (Local-Adapter) refines mask outputs by recovering subtle boundary details. We also incorporate two prediction heads for each task into the architecture to generate segmentation and matting masks, simultaneously. Trained on a diverse dataset aggregated from publicly available sources, SAMA achieves state-of-the-art performance across multiple segmentation and matting benchmarks, showcasing its adaptability and effectiveness in a wide range of downstream tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "AAAI 2026",
    "pdf_url": "https://arxiv.org/pdf/2601.12147v1",
    "published_date": "2026-01-17 19:43:10 UTC",
    "updated_date": "2026-01-17 19:43:10 UTC"
  },
  {
    "arxiv_id": "2601.12141v1",
    "title": "TIDE: A Trace-Informed Depth-First Exploration for Planning with Temporally Extended Goals",
    "authors": [
      "Yuliia Suprun",
      "Khen Elimelech",
      "Lydia E. Kavraki",
      "Moshe Y. Vardi"
    ],
    "abstract": "Task planning with temporally extended goals (TEGs) is a critical challenge in AI and robotics, enabling agents to achieve complex sequences of objectives over time rather than addressing isolated, immediate tasks. Linear Temporal Logic on finite traces (LTLf ) provides a robust formalism for encoding these temporal goals. Traditional LTLf task planning approaches often transform the temporal planning problem into a classical planning problem with reachability goals, which are then solved using off-the-shelf planners. However, these methods often lack informed heuristics to provide a guided search for temporal goals. We introduce TIDE (Trace-Informed Depth-first Exploration), a novel approach that addresses this limitation by decomposing a temporal problem into a sequence of smaller, manageable reach-avoid sub-problems, each solvable using an off-the-shelf planner. TIDE identifies and prioritizes promising automaton traces within the domain graph, using cost-driven heuristics to guide exploration. Its adaptive backtracking mechanism systematically recovers from failed plans by recalculating costs and penalizing infeasible transitions, ensuring completeness and efficiency. Experimental results demonstrate that TIDE achieves promising performance and is a valuable addition to the portfolio of planning methods for temporally extended goals.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.12141v1",
    "published_date": "2026-01-17 19:07:03 UTC",
    "updated_date": "2026-01-17 19:07:03 UTC"
  },
  {
    "arxiv_id": "2601.12138v1",
    "title": "DriveSafe: A Hierarchical Risk Taxonomy for Safety-Critical LLM-Based Driving Assistants",
    "authors": [
      "Abhishek Kumar",
      "Riya Tapwal",
      "Carsten Maple"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly integrated into vehicle-based digital assistants, where unsafe, ambiguous, or legally incorrect responses can lead to serious safety, ethical, and regulatory consequences. Despite growing interest in LLM safety, existing taxonomies and evaluation frameworks remain largely general-purpose and fail to capture the domain-specific risks inherent to real-world driving scenarios. In this paper, we introduce DriveSafe, a hierarchical, four-level risk taxonomy designed to systematically characterize safety-critical failure modes of LLM-based driving assistants. The taxonomy comprises 129 fine-grained atomic risk categories spanning technical, legal, societal, and ethical dimensions, grounded in real-world driving regulations and safety principles and reviewed by domain experts. To validate the safety relevance and realism of the constructed prompts, we evaluate their refusal behavior across six widely deployed LLMs. Our analysis shows that the evaluated models often fail to appropriately refuse unsafe or non-compliant driving-related queries, underscoring the limitations of general-purpose safety alignment in driving contexts.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.12138v1",
    "published_date": "2026-01-17 18:50:47 UTC",
    "updated_date": "2026-01-17 18:50:47 UTC"
  },
  {
    "arxiv_id": "2601.12134v1",
    "title": "Human-Human-AI Triadic Programming: Uncovering the Role of AI Agent and the Value of Human Partner in Collaborative Learning",
    "authors": [
      "Taufiq Daryanto",
      "Xiaohan Ding",
      "Kaike Ping",
      "Lance T. Wilhelm",
      "Yan Chen",
      "Chris Brown",
      "Eugenia H. Rho"
    ],
    "abstract": "As AI assistance becomes embedded in programming practice, researchers have increasingly examined how these systems help learners generate code and work more efficiently. However, these studies often position AI as a replacement for human collaboration and overlook the social and learning-oriented aspects that emerge in collaborative programming. Our work introduces human-human-AI (HHAI) triadic programming, where an AI agent serves as an additional collaborator rather than a substitute for a human partner. Through a within-subjects study with 20 participants, we show that triadic collaboration enhances collaborative learning and social presence compared to the dyadic human-AI (HAI) baseline. In the triadic HHAI conditions, participants relied significantly less on AI-generated code in their work. This effect was strongest in the HHAI-shared condition, where participants had an increased sense of responsibility to understand AI suggestions before applying them. These findings demonstrate how triadic settings activate socially shared regulation of learning by making AI use visible and accountable to a human peer, suggesting that AI systems that augment rather than automate peer collaboration can better preserve the learning processes that collaborative programming relies on.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.12134v1",
    "published_date": "2026-01-17 18:32:54 UTC",
    "updated_date": "2026-01-17 18:32:54 UTC"
  },
  {
    "arxiv_id": "2601.12132v1",
    "title": "Bengali Text Classification: An Evaluation of Large Language Model Approaches",
    "authors": [
      "Md Mahmudul Hoque",
      "Md Mehedi Hassain",
      "Md Hojaifa Tanvir",
      "Rahul Nandy"
    ],
    "abstract": "Bengali text classification is a Significant task in natural language processing (NLP), where text is categorized into predefined labels. Unlike English, Bengali faces challenges due to the lack of extensive annotated datasets and pre-trained language models. This study explores the effectiveness of large language models (LLMs) in classifying Bengali newspaper articles. The dataset used, obtained from Kaggle, consists of articles from Prothom Alo, a major Bangladeshi newspaper. Three instruction-tuned LLMs LLaMA 3.1 8B Instruct, LLaMA 3.2 3B Instruct, and Qwen 2.5 7B Instruct were evaluated for this task under the same classification framework. Among the evaluated models, Qwen 2.5 achieved the highest classification accuracy of 72%, showing particular strength in the \"Sports\" category. In comparison, LLaMA 3.1 and LLaMA 3.2 attained accuracies of 53% and 56%, respectively. The findings highlight the effectiveness of LLMs in Bengali text classification, despite the scarcity of resources for Bengali NLP. Future research will focus on exploring additional models, addressing class imbalance issues, and refining fine-tuning approaches to improve classification performance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.12132v1",
    "published_date": "2026-01-17 18:25:19 UTC",
    "updated_date": "2026-01-17 18:25:19 UTC"
  },
  {
    "arxiv_id": "2601.12126v1",
    "title": "UniMo: Unified Motion Generation and Understanding with Chain of Thought",
    "authors": [
      "Guocun Wang",
      "Kenkun Liu",
      "Jing Lin",
      "Guorui Song",
      "Jian Li",
      "Xiaoguang Han"
    ],
    "abstract": "Existing 3D human motion generation and understanding methods often exhibit limited interpretability, restricting effective mutual enhancement between these inherently related tasks. While current unified frameworks based on large language models (LLMs) leverage linguistic priors, they frequently encounter challenges in semantic alignment and task coherence. Moreover, the next-token prediction paradigm in LLMs is ill-suited for motion sequences, causing cumulative prediction errors. To address these limitations, we propose UniMo, a novel framework that integrates motion-language information and interpretable chain of thought (CoT) reasoning into the LLM via supervised fine-tuning (SFT). We further introduce reinforcement learning with Group Relative Policy Optimization (GRPO) as a post-training strategy that optimizes over groups of tokens to enforce structural correctness and semantic alignment, mitigating cumulative errors in motion token prediction. Extensive experiments demonstrate that UniMo significantly outperforms existing unified and task-specific models, achieving state-of-the-art performance in both motion generation and understanding.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.12126v1",
    "published_date": "2026-01-17 17:56:49 UTC",
    "updated_date": "2026-01-17 17:56:49 UTC"
  },
  {
    "arxiv_id": "2601.12124v1",
    "title": "SynQP: A Framework and Metrics for Evaluating the Quality and Privacy Risk of Synthetic Data",
    "authors": [
      "Bing Hu",
      "Yixin Li",
      "Asma Bahamyirou",
      "Helen Chen"
    ],
    "abstract": "The use of synthetic data in health applications raises privacy concerns, yet the lack of open frameworks for privacy evaluations has slowed its adoption. A major challenge is the absence of accessible benchmark datasets for evaluating privacy risks, due to difficulties in acquiring sensitive data. To address this, we introduce SynQP, an open framework for benchmarking privacy in synthetic data generation (SDG) using simulated sensitive data, ensuring that original data remains confidential. We also highlight the need for privacy metrics that fairly account for the probabilistic nature of machine learning models. As a demonstration, we use SynQP to benchmark CTGAN and propose a new identity disclosure risk metric that offers a more accurate estimation of privacy risks compared to existing approaches. Our work provides a critical tool for improving the transparency and reliability of privacy evaluations, enabling safer use of synthetic data in health-related applications. % In our quality evaluations, non-private models achieved near-perfect machine-learning efficacy \\(\\ge0.97\\). Our privacy assessments (Table II) reveal that DP consistently lowers both identity disclosure risk (SD-IDR) and membership-inference attack risk (SD-MIA), with all DP-augmented models staying below the 0.09 regulatory threshold. Code available at https://github.com/CAN-SYNH/SynQP",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "7 Pages, 22nd Annual International Conference on Privacy, Security, and Trust (PST2025), Fredericton, Canada",
    "pdf_url": "https://arxiv.org/pdf/2601.12124v1",
    "published_date": "2026-01-17 17:51:14 UTC",
    "updated_date": "2026-01-17 17:51:14 UTC"
  },
  {
    "arxiv_id": "2601.12104v1",
    "title": "Powerful Training-Free Membership Inference Against Autoregressive Language Models",
    "authors": [
      "David Ilić",
      "David Stanojević",
      "Kostadin Cvejoski"
    ],
    "abstract": "Fine-tuned language models pose significant privacy risks, as they may memorize and expose sensitive information from their training data. Membership inference attacks (MIAs) provide a principled framework for auditing these risks, yet existing methods achieve limited detection rates, particularly at the low false-positive thresholds required for practical privacy auditing. We present EZ-MIA, a membership inference attack that exploits a key observation: memorization manifests most strongly at error positions, specifically tokens where the model predicts incorrectly yet still shows elevated probability for training examples. We introduce the Error Zone (EZ) score, which measures the directional imbalance of probability shifts at error positions relative to a pretrained reference model. This principled statistic requires only two forward passes per query and no model training of any kind. On WikiText with GPT-2, EZ-MIA achieves 3.8x higher detection than the previous state-of-the-art under identical conditions (66.3% versus 17.5% true positive rate at 1% false positive rate), with near-perfect discrimination (AUC 0.98). At the stringent 0.1% FPR threshold critical for real-world auditing, we achieve 8x higher detection than prior work (14.0% versus 1.8%), requiring no reference model training. These gains extend to larger architectures: on AG News with Llama-2-7B, we achieve 3x higher detection (46.7% versus 15.8% TPR at 1% FPR). These results establish that privacy risks of fine-tuned language models are substantially greater than previously understood, with implications for both privacy auditing and deployment decisions. Code is available at https://github.com/JetBrains-Research/ez-mia.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, 2 figures; appendix with additional experiments and derivations",
    "pdf_url": "https://arxiv.org/pdf/2601.12104v1",
    "published_date": "2026-01-17 16:59:41 UTC",
    "updated_date": "2026-01-17 16:59:41 UTC"
  },
  {
    "arxiv_id": "2601.12099v1",
    "title": "Large language models struggle with ethnographic text annotation",
    "authors": [
      "Leonardo S. Goodall",
      "Dor Shilton",
      "Daniel A. Mullins",
      "Harvey Whitehouse"
    ],
    "abstract": "Large language models (LLMs) have shown promise for automated text annotation, raising hopes that they might accelerate cross-cultural research by extracting structured data from ethnographic texts. We evaluated 7 state-of-the-art LLMs on their ability to annotate 121 ritual features across 567 ethnographic excerpts. Performance was limited, falling well below levels required for reliable automated annotation. Longer texts, features requiring ordinal distinctions, and ambiguous constructs proved particularly difficult. Human inter-coder reliability set an approximate ceiling on LLM accuracy: features that human coders found difficult to agree upon were also difficult for LLMs. Yet even on features where humans reliably agreed, models fell short of human performance. Our findings suggest that LLMs cannot yet substitute for human expertise in ethnographic annotation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.12099v1",
    "published_date": "2026-01-17 16:39:07 UTC",
    "updated_date": "2026-01-17 16:39:07 UTC"
  },
  {
    "arxiv_id": "2601.12095v1",
    "title": "Neural Isomorphic Fields: A Transformer-based Algebraic Numerical Embedding",
    "authors": [
      "Hamidreza Sadeghi",
      "Saeedeh Momtazi",
      "Reza Safabakhsh"
    ],
    "abstract": "Neural network models often face challenges when processing very small or very large numbers due to issues such as overflow, underflow, and unstable output variations. To mitigate these problems, we propose using embedding vectors for numbers instead of directly using their raw values. These embeddings aim to retain essential algebraic properties while preventing numerical instabilities. In this paper, we introduce, for the first time, a fixed-length number embedding vector that preserves algebraic operations, including addition, multiplication, and comparison, within the field of rational numbers. We propose a novel Neural Isomorphic Field, a neural abstraction of algebraic structures such as groups and fields. The elements of this neural field are embedding vectors that maintain algebraic structure during computations. Our experiments demonstrate that addition performs exceptionally well, achieving over 95 percent accuracy on key algebraic tests such as identity, closure, and associativity. In contrast, multiplication exhibits challenges, with accuracy ranging from 53 percent to 73 percent across various algebraic properties. These findings highlight the model's strengths in preserving algebraic properties under addition while identifying avenues for further improvement in handling multiplication.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.12095v1",
    "published_date": "2026-01-17 16:25:52 UTC",
    "updated_date": "2026-01-17 16:25:52 UTC"
  },
  {
    "arxiv_id": "2601.12082v1",
    "title": "Conditional Random Fields for Interactive Refinement of Histopathological Predictions",
    "authors": [
      "Tiffanie Godelaine",
      "Maxime Zanella",
      "Karim El Khoury",
      "Saïd Mahmoudi",
      "Benoît Macq",
      "Christophe De Vleeschouwer"
    ],
    "abstract": "Assisting pathologists in the analysis of histopathological images has high clinical value, as it supports cancer detection and staging. In this context, histology foundation models have recently emerged. Among them, Vision-Language Models (VLMs) provide strong yet imperfect zero-shot predictions. We propose to refine these predictions by adapting Conditional Random Fields (CRFs) to histopathological applications, requiring no additional model training. We present HistoCRF, a CRF-based framework, with a novel definition of the pairwise potential that promotes label diversity and leverages expert annotations. We consider three experiments: without annotations, with expert annotations, and with iterative human-in-the-loop annotations that progressively correct misclassified patches. Experiments on five patch-level classification datasets covering different organs and diseases demonstrate average accuracy gains of 16.0% without annotations and 27.5% with only 100 annotations, compared to zero-shot predictions. Moreover, integrating a human in the loop reaches a further gain of 32.6% with the same number of annotations. The code will be made available on https://github.com/tgodelaine/HistoCRF.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.12082v1",
    "published_date": "2026-01-17 15:19:40 UTC",
    "updated_date": "2026-01-17 15:19:40 UTC"
  },
  {
    "arxiv_id": "2601.12068v1",
    "title": "Bridging the Gap in Bangla Healthcare: Machine Learning Based Disease Prediction Using a Symptoms-Disease Dataset",
    "authors": [
      "Rowzatul Zannat",
      "Abdullah Al Shafi",
      "Abdul Muntakim"
    ],
    "abstract": "Increased access to reliable health information is essential for non-English-speaking populations, yet resources in Bangla for disease prediction remain limited. This study addresses this gap by developing a comprehensive Bangla symptoms-disease dataset containing 758 unique symptom-disease relationships spanning 85 diseases. To ensure transparency and reproducibility, we also make our dataset publicly available. The dataset enables the prediction of diseases based on Bangla symptom inputs, supporting healthcare accessibility for Bengali-speaking populations. Using this dataset, we evaluated multiple machine learning models to predict diseases based on symptoms provided in Bangla and analyzed their performance on our dataset. Both soft and hard voting ensemble approaches combining top-performing models achieved 98\\% accuracy, demonstrating superior robustness and generalization. Our work establishes a foundational resource for disease prediction in Bangla, paving the way for future advancements in localized health informatics and diagnostic tools. This contribution aims to enhance equitable access to health information for Bangla-speaking communities, particularly for early disease detection and healthcare interventions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.12068v1",
    "published_date": "2026-01-17 14:33:01 UTC",
    "updated_date": "2026-01-17 14:33:01 UTC"
  },
  {
    "arxiv_id": "2601.12061v1",
    "title": "Codebook-Injected Dialogue Segmentation for Multi-Utterance Constructs Annotation: LLM-Assisted and Gold-Label-Free Evaluation",
    "authors": [
      "Jinsook Lee",
      "Kirk Vanacore",
      "Zhuqian Zhou",
      "Jeanine Grutter",
      "Rene F. Kizilcec"
    ],
    "abstract": "Dialogue Act (DA) annotation typically treats communicative or pedagogical intent as localized to individual utterances or turns. This leads annotators to agree on the underlying action while disagreeing on segment boundaries, reducing apparent reliability. We propose codebook-injected segmentation, which conditions boundary decisions on downstream annotation criteria, and evaluate LLM-based segmenters against standard and retrieval-augmented baselines. To assess these without gold labels, we introduce evaluation metrics for span consistency, distinctiveness, and human-AI distributional agreement. We found DA-awareness produces segments that are internally more consistent than text-only baselines. While LLMs excel at creating construct-consistent spans, coherence-based baselines remain superior at detecting global shifts in dialogue flow. Across two datasets, no single segmenter dominates. Improvements in within-segment coherence frequently trade off against boundary distinctiveness and human-AI distributional agreement. These results highlight segmentation as a consequential design choice that should be optimized for downstream objectives rather than a single performance score.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Under Review for ACL 2026",
    "pdf_url": "https://arxiv.org/pdf/2601.12061v1",
    "published_date": "2026-01-17 14:17:13 UTC",
    "updated_date": "2026-01-17 14:17:13 UTC"
  },
  {
    "arxiv_id": "2601.12055v1",
    "title": "Automating Parameter Selection in Deep Image Prior for Fluorescence Microscopy Image Denoising via Similarity-Based Parameter Transfer",
    "authors": [
      "Lina Meyer",
      "Felix Wissel",
      "Tobias Knopp",
      "Susanne Pfefferle",
      "Ralf Fliegert",
      "Maximilian Sandmann",
      "Liana Uebler",
      "Franziska Möckl",
      "Björn-Philipp Diercks",
      "David Lohr",
      "René Werner"
    ],
    "abstract": "Unsupervised deep image prior (DIP) addresses shortcomings of training data requirements and limited generalization associated with supervised deep learning. The performance of DIP depends on the network architecture and the stopping point of its iterative process. Optimizing these parameters for a new image requires time, restricting DIP application in domains where many images need to be processed. Focusing on fluorescence microscopy data, we hypothesize that similar images share comparable optimal parameter configurations for DIP-based denoising, potentially enabling optimization-free DIP for fluorescence microscopy. We generated a calibration (n=110) and validation set (n=55) of semantically different images from an open-source dataset for a network architecture search targeted towards ideal U-net architectures and stopping points. The calibration set represented our transfer basis. The validation set enabled the assessment of which image similarity criterion yields the best results. We then implemented AUTO-DIP, a pipeline for automatic parameter transfer, and compared it to the originally published DIP configuration (baseline) and a state-of-the-art image-specific variational denoising approach. We show that a parameter transfer from the calibration dataset to a test image based on only image metadata similarity (e.g., microscope type, imaged specimen) leads to similar and better performance than a transfer based on quantitative image similarity measures. AUTO-DIP outperforms the baseline DIP (DIP with original DIP parameters) as well as the variational denoising approaches for several open-source test datasets of varying complexity, particularly for very noisy inputs. Applications to locally acquired fluorescence microscopy images further proved superiority of AUTO-DIP.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.12055v1",
    "published_date": "2026-01-17 13:47:41 UTC",
    "updated_date": "2026-01-17 13:47:41 UTC"
  },
  {
    "arxiv_id": "2601.12053v1",
    "title": "A New Strategy for Artificial Intelligence: Training Foundation Models Directly on Human Brain Data",
    "authors": [
      "Maël Donoso"
    ],
    "abstract": "While foundation models have achieved remarkable results across a diversity of domains, they still rely on human-generated data, such as text, as a fundamental source of knowledge. However, this data is ultimately the product of human brains, the filtered projection of a deeper neural complexity. In this paper, we explore a new strategy for artificial intelligence: moving beyond surface-level statistical regularities by training foundation models directly on human brain data. We hypothesize that neuroimaging data could open a window into elements of human cognition that are not accessible through observable actions, and argue that this additional knowledge could be used, alongside classical training data, to overcome some of the current limitations of foundation models. While previous research has demonstrated the possibility to train classical machine learning or deep learning models on neural patterns, this path remains largely unexplored for high-level cognitive functions. Here, we classify the current limitations of foundation models, as well as the promising brain regions and cognitive processes that could be leveraged to address them, along four levels: perception, valuation, execution, and integration. Then, we propose two methods that could be implemented to prioritize the use of limited neuroimaging data for strategically chosen, high-value steps in foundation model training: reinforcement learning from human brain (RLHB) and chain of thought from human brain (CoTHB). We also discuss the potential implications for agents, artificial general intelligence, and artificial superintelligence, as well as the ethical, social, and technical challenges and opportunities. We argue that brain-trained foundation models could represent a realistic and effective middle ground between continuing to scale current architectures and exploring alternative, neuroscience-inspired solutions.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.NC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.12053v1",
    "published_date": "2026-01-17 13:38:51 UTC",
    "updated_date": "2026-01-17 13:38:51 UTC"
  },
  {
    "arxiv_id": "2601.12049v1",
    "title": "\\textit{FocaLogic}: Logic-Based Interpretation of Visual Model Decisions",
    "authors": [
      "Chenchen Zhao",
      "Muxi Chen",
      "Qiang Xu"
    ],
    "abstract": "Interpretability of modern visual models is crucial, particularly in high-stakes applications. However, existing interpretability methods typically suffer from either reliance on white-box model access or insufficient quantitative rigor. To address these limitations, we introduce FocaLogic, a novel model-agnostic framework designed to interpret and quantify visual model decision-making through logic-based representations. FocaLogic identifies minimal interpretable subsets of visual regions-termed visual focuses-that decisively influence model predictions. It translates these visual focuses into precise and compact logical expressions, enabling transparent and structured interpretations. Additionally, we propose a suite of quantitative metrics, including focus precision, recall, and divergence, to objectively evaluate model behavior across diverse scenarios. Empirical analyses demonstrate FocaLogic's capability to uncover critical insights such as training-induced concentration, increasing focus accuracy through generalization, and anomalous focuses under biases and adversarial attacks. Overall, FocaLogic provides a systematic, scalable, and quantitative solution for interpreting visual models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 13 figures",
    "pdf_url": "https://arxiv.org/pdf/2601.12049v1",
    "published_date": "2026-01-17 13:28:02 UTC",
    "updated_date": "2026-01-17 13:28:02 UTC"
  },
  {
    "arxiv_id": "2601.12042v1",
    "title": "Less Is More -- Until It Breaks: Security Pitfalls of Vision Token Compression in Large Vision-Language Models",
    "authors": [
      "Xiaomei Zhang",
      "Zhaoxi Zhang",
      "Leo Yu Zhang",
      "Yanjun Zhang",
      "Guanhong Tao",
      "Shirui Pan"
    ],
    "abstract": "Visual token compression is widely adopted to improve the inference efficiency of Large Vision-Language Models (LVLMs), enabling their deployment in latency-sensitive and resource-constrained scenarios. However, existing work has mainly focused on efficiency and performance, while the security implications of visual token compression remain largely unexplored. In this work, we first reveal that visual token compression substantially degrades the robustness of LVLMs: models that are robust under uncompressed inference become highly vulnerable once compression is enabled. These vulnerabilities are state-specific; failure modes emerge only in the compressed setting and completely disappear when compression is disabled, making them particularly hidden and difficult to diagnose. By analyzing the key stages of the compression process, we identify instability in token importance ranking as the primary cause of this robustness degradation. Small and imperceptible perturbations can significantly alter token rankings, leading the compression mechanism to mistakenly discard task-critical information and ultimately causing model failure. Motivated by this observation, we propose a Compression-Aware Attack to systematically study and exploit this vulnerability. CAA directly targets the token selection mechanism and induces failures exclusively under compressed inference. We further extend this approach to more realistic black-box settings and introduce Transfer CAA, where neither the target model nor the compression configuration is accessible. We further evaluate potential defenses and find that they provide only limited protection. Extensive experiments across models, datasets, and compression methods show that visual token compression significantly undermines robustness, revealing a previously overlooked efficiency-security trade-off.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.12042v1",
    "published_date": "2026-01-17 13:02:41 UTC",
    "updated_date": "2026-01-17 13:02:41 UTC"
  },
  {
    "arxiv_id": "2601.12040v1",
    "title": "Partial Reasoning in Language Models: Search and Refinement Guided by Uncertainty",
    "authors": [
      "Murilo da Luz",
      "Bruno Brandão",
      "Luana Martins",
      "Gustavo Oliveira",
      "Bryan de Oliveira",
      "Luckeciano Melo",
      "Telma Soares"
    ],
    "abstract": "The use of Large Language Models (LLMs) for reasoning and planning tasks has drawn increasing attention in Artificial Intelligence research. Despite their remarkable progress, these models still exhibit limitations in multi-step inference scenarios, particularly in mathematical and logical reasoning. We introduce PREGU (Partial Reasoning Guided by Uncertainty). PREGU monitors the entropy of the output distribution during autoregressive generation and halts the process whenever entropy exceeds a defined threshold, signaling uncertainty. From that point, a localized search is performed in the latent space to refine the partial reasoning and select the most coherent answer, using the Soft Reasoning method. Experiments conducted with LLaMA-3-8B, Mistral-7B, and Qwen2-7B across four reasoning benchmarks (GSM8K, GSM-Hard, SVAMP, and StrategyQA) showed performance greater than or similar to Soft Reasoning, indicating that entropy can serve as an effective signal to trigger selective refinement during reasoning.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.12040v1",
    "published_date": "2026-01-17 13:00:17 UTC",
    "updated_date": "2026-01-17 13:00:17 UTC"
  },
  {
    "arxiv_id": "2601.12038v1",
    "title": "Abstract Argumentation with Subargument Relations",
    "authors": [
      "Beishui Liao"
    ],
    "abstract": "Dung's abstract argumentation framework characterises argument acceptability solely via an attack relation, deliberately abstracting from the internal structure of arguments. While this level of abstraction has enabled a rich body of results, it limits the ability to represent structural dependencies that are central in many structured argumentation formalisms, in particular subargument relations. Existing extensions, including bipolar argumentation frameworks, introduce support relations, but these do not capture the asymmetric and constitutive nature of subarguments or their interaction with attacks. In this paper, we study abstract argumentation frameworks enriched with an explicit subargument relation, treated alongside attack as a basic relation. We analyse how subargument relations interact with attacks and examine their impact on fundamental semantic properties. This framework provides a principled abstraction of structural information and clarifies the role of subarguments in abstract acceptability reasoning.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "11 pages",
    "pdf_url": "https://arxiv.org/pdf/2601.12038v1",
    "published_date": "2026-01-17 12:54:10 UTC",
    "updated_date": "2026-01-17 12:54:10 UTC"
  },
  {
    "arxiv_id": "2601.12030v1",
    "title": "ARC: Active and Reflection-driven Context Management for Long-Horizon Information Seeking Agents",
    "authors": [
      "Yilun Yao",
      "Shan Huang",
      "Elsie Dai",
      "Zhewen Tan",
      "Zhenyu Duan",
      "Shousheng Jia",
      "Yanbing Jiang",
      "Tong Yang"
    ],
    "abstract": "Large language models are increasingly deployed as research agents for deep search and long-horizon information seeking, yet their performance often degrades as interaction histories grow. This degradation, known as context rot, reflects a failure to maintain coherent and task-relevant internal states over extended reasoning horizons. Existing approaches primarily manage context through raw accumulation or passive summarization, treating it as a static artifact and allowing early errors or misplaced emphasis to persist. Motivated by this perspective, we propose ARC, which is the first framework to systematically formulate context management as an active, reflection-driven process that treats context as a dynamic internal reasoning state during execution. ARC operationalizes this view through reflection-driven monitoring and revision, allowing agents to actively reorganize their working context when misalignment or degradation is detected. Experiments on challenging long-horizon information-seeking benchmarks show that ARC consistently outperforms passive context compression methods, achieving up to an 11% absolute improvement in accuracy on BrowseComp-ZH with Qwen2.5-32B-Instruct.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "15 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2601.12030v1",
    "published_date": "2026-01-17 12:17:50 UTC",
    "updated_date": "2026-01-17 12:17:50 UTC"
  },
  {
    "arxiv_id": "2601.12024v1",
    "title": "A Multi-Agent System for Generating Actionable Business Advice",
    "authors": [
      "Kartikey Singh Bhandari",
      "Tanish Jain",
      "Archit Agrawal",
      "Dhruv Kumar",
      "Praveen Kumar",
      "Pratik Narang"
    ],
    "abstract": "Customer reviews contain rich signals about product weaknesses and unmet user needs, yet existing analytic methods rarely move beyond descriptive tasks such as sentiment analysis or aspect extraction. While large language models (LLMs) can generate free-form suggestions, their outputs often lack accuracy and depth of reasoning. In this paper, we present a multi-agent, LLM-based framework for prescriptive decision support, which transforms large scale review corpora into actionable business advice. The framework integrates four components: clustering to select representative reviews, generation of advices, iterative evaluation, and feasibility based ranking. This design couples corpus distillation with feedback driven advice refinement to produce outputs that are specific, actionable, and practical. Experiments across three service domains and multiple model families show that our framework consistently outperform single model baselines on actionability, specificity, and non-redundancy, with medium sized models approaching the performance of large model frameworks.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.12024v1",
    "published_date": "2026-01-17 12:07:55 UTC",
    "updated_date": "2026-01-17 12:07:55 UTC"
  },
  {
    "arxiv_id": "2601.12019v1",
    "title": "Acting Flatterers via LLMs Sycophancy: Combating Clickbait with LLMs Opposing-Stance Reasoning",
    "authors": [
      "Chaowei Zhang",
      "Xiansheng Luo",
      "Zewei Zhang",
      "Yi Zhu",
      "Jipeng Qiang",
      "Longwei Wang"
    ],
    "abstract": "The widespread proliferation of online content has intensified concerns about clickbait, deceptive or exaggerated headlines designed to attract attention. While Large Language Models (LLMs) offer a promising avenue for addressing this issue, their effectiveness is often hindered by Sycophancy, a tendency to produce reasoning that matches users' beliefs over truthful ones, which deviates from instruction-following principles. Rather than treating sycophancy as a flaw to be eliminated, this work proposes a novel approach that initially harnesses this behavior to generate contrastive reasoning from opposing perspectives. Specifically, we design a Self-renewal Opposing-stance Reasoning Generation (SORG) framework that prompts LLMs to produce high-quality agree and disagree reasoning pairs for a given news title without requiring ground-truth labels. To utilize the generated reasoning, we develop a local Opposing Reasoning-based Clickbait Detection (ORCD) model that integrates three BERT encoders to represent the title and its associated reasoning. The model leverages contrastive learning, guided by soft labels derived from LLM-generated credibility scores, to enhance detection robustness. Experimental evaluations on three benchmark datasets demonstrate that our method consistently outperforms LLM prompting, fine-tuned smaller language models, and state-of-the-art clickbait detection baselines.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.12019v1",
    "published_date": "2026-01-17 11:57:23 UTC",
    "updated_date": "2026-01-17 11:57:23 UTC"
  },
  {
    "arxiv_id": "2601.12014v1",
    "title": "Are LLMs Ready for TOON? Benchmarking Structural Correctness-Sustainability Trade-offs in Novel Structured Output Formats",
    "authors": [
      "Elio Masciari",
      "Vincenzo Moscato",
      "Enea Vincenzo Napolitano",
      "Gian Marco Orlando",
      "Marco Perillo",
      "Diego Russo"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly required to generate structured, machine-readable outputs for downstream systems. While recent benchmarks have focused on evaluating the structural correctness of such outputs, the environmental impact of inference for different output formats has largely been overlooked. In this paper, we argue that structured output formats should be assessed not only in terms of correctness, but also with respect to their environmental efficiency. To this end, we introduce a sustainability-aware evaluation framework for structured generation that measures token usage, generation time, and estimated carbon emissions. Within this framework, we propose the Environment-Aware Generation Correctness Score (GCS_env), a unified metric that integrates structural correctness with carbon-aware efficiency. Using this framework, we systematically benchmark the novel TOON format against established representations (JSON, XML, YAML) across multiple LLMs spanning different architectures and parameter scales.\n  Our results reveal a consistent trade-off: TOON yields markedly more compact outputs and lower emissions, but lower structural correctness when models lack native support. We show that increased model capacity reduces this gap and that environment-aware scoring can shift format rankings depending on deployment priorities. highlighting the need for sustainability-inclusive benchmarking and provides empirical evidence that compact representations such as TOON can offer practical advantages in large-scale, carbon-conscious LLM deployments.",
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.12014v1",
    "published_date": "2026-01-17 11:42:02 UTC",
    "updated_date": "2026-01-17 11:42:02 UTC"
  },
  {
    "arxiv_id": "2601.12003v2",
    "title": "Robust Verification of Concurrent Stochastic Games",
    "authors": [
      "Angel Y. He",
      "David Parker"
    ],
    "abstract": "Autonomous systems often operate in multi-agent settings and need to make concurrent, strategic decisions, typically in uncertain environments. Verification and control problems for these systems can be tackled with concurrent stochastic games (CSGs), but this model requires transition probabilities to be precisely specified - an unrealistic requirement in many real-world settings. We introduce *robust CSGs* and their subclass *interval CSGs* (ICSGs), which capture epistemic uncertainty about transition probabilities in CSGs. We propose a novel framework for *robust* verification of these models under worst-case assumptions about transition uncertainty. Specifically, we develop the underlying theoretical foundations and efficient algorithms, for finite- and infinite-horizon objectives in both zero-sum and nonzero-sum settings, the latter based on (social-welfare optimal) Nash equilibria. We build an implementation in the PRISM-games model checker and demonstrate the feasibility of robust verification of ICSGs across a selection of large benchmarks.",
    "categories": [
      "cs.LO",
      "cs.AI",
      "cs.GT",
      "cs.MA",
      "eess.SY"
    ],
    "primary_category": "cs.LO",
    "comment": "Extended version of a paper accepted to TACAS 2026. Main text: 17 pages, 2 figures, 2 tables; Appendix: 37 pages, 3 figures, 3 tables. Minor revisions and clarifications to the appendix; no changes to results",
    "pdf_url": "https://arxiv.org/pdf/2601.12003v2",
    "published_date": "2026-01-17 10:42:44 UTC",
    "updated_date": "2026-01-21 09:31:07 UTC"
  },
  {
    "arxiv_id": "2601.12002v1",
    "title": "Kernel-Based Learning of Safety Barriers",
    "authors": [
      "Oliver Schön",
      "Zhengang Zhong",
      "Sadegh Soudjani"
    ],
    "abstract": "The rapid integration of AI algorithms in safety-critical applications such as autonomous driving and healthcare is raising significant concerns about the ability to meet stringent safety standards. Traditional tools for formal safety verification struggle with the black-box nature of AI-driven systems and lack the flexibility needed to scale to the complexity of real-world applications. In this paper, we present a data-driven approach for safety verification and synthesis of black-box systems with discrete-time stochastic dynamics. We employ the concept of control barrier certificates, which can guarantee safety of the system, and learn the certificate directly from a set of system trajectories. We use conditional mean embeddings to embed data from the system into a reproducing kernel Hilbert space (RKHS) and construct an RKHS ambiguity set that can be inflated to robustify the result to out-of-distribution behavior. We provide the theoretical results on how to apply the approach to general classes of temporal logic specifications beyond safety. For the data-driven computation of safety barriers, we leverage a finite Fourier expansion to cast a typically intractable semi-infinite optimization problem as a linear program. The resulting spectral barrier allows us to leverage the fast Fourier transform to generate the relaxed problem efficiently, offering a scalable yet distributionally robust framework for verifying safety. Our work moves beyond restrictive assumptions on system dynamics and uncertainty, as demonstrated on two case studies including a black-box system with a neural network controller.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "eess.SY"
    ],
    "primary_category": "cs.AI",
    "comment": "44 pages, 9 figures",
    "pdf_url": "https://arxiv.org/pdf/2601.12002v1",
    "published_date": "2026-01-17 10:42:35 UTC",
    "updated_date": "2026-01-17 10:42:35 UTC"
  },
  {
    "arxiv_id": "2601.11998v1",
    "title": "Hybrid IDS Using Signature-Based and Anomaly-Based Detection",
    "authors": [
      "Messaouda Boutassetta",
      "Amina Makhlouf",
      "Newfel Messaoudi",
      "Abdelmadjid Benmachiche",
      "Ines Boutabia"
    ],
    "abstract": "Intrusion detection systems (IDS) are essential for protecting computer systems and networks against a wide range of cyber threats that continue to evolve over time. IDS are commonly categorized into two main types, each with its own strengths and limitations, such as difficulty in detecting previously unseen attacks and the tendency to generate high false positive rates. This paper presents a comprehensive survey and a conceptual overview of Hybrid IDS, which integrate signature-based and anomaly-based detection techniques to enhance attack detection capabilities. The survey examines recent research on Hybrid IDS, classifies existing models into functional categories, and discusses their advantages, limitations, and application domains, including financial systems, air traffic control, and social networks. In addition, recent trends in Hybrid IDS research, such as machine learning-based approaches and cloud-based deployments, are reviewed. Finally, this work outlines potential future research directions aimed at developing more cost-effective Hybrid IDS solutions with improved ability to detect emerging and sophisticated cyberattacks.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "7 pages,The Second National Conference on Artificial Intelligence and Information Technologies (NCAIIT25)",
    "pdf_url": "https://arxiv.org/pdf/2601.11998v1",
    "published_date": "2026-01-17 10:19:57 UTC",
    "updated_date": "2026-01-17 10:19:57 UTC"
  },
  {
    "arxiv_id": "2601.11995v1",
    "title": "Learning Audio-Visual Embeddings with Inferred Latent Interaction Graphs",
    "authors": [
      "Donghuo Zeng",
      "Hao Niu",
      "Yanan Wang",
      "Masato Taya"
    ],
    "abstract": "Learning robust audio-visual embeddings requires bringing genuinely related audio and visual signals together while filtering out incidental co-occurrences - background noise, unrelated elements, or unannotated events. Most contrastive and triplet-loss methods use sparse annotated labels per clip and treat any co-occurrence as semantic similarity. For example, a video labeled \"train\" might also contain motorcycle audio and visual, because \"motorcycle\" is not the chosen annotation; standard methods treat these co-occurrences as negatives to true motorcycle anchors elsewhere, creating false negatives and missing true cross-modal dependencies. We propose a framework that leverages soft-label predictions and inferred latent interactions to address these issues: (1) Audio-Visual Semantic Alignment Loss (AV-SAL) trains a teacher network to produce aligned soft-label distributions across modalities, assigning nonzero probability to co-occurring but unannotated events and enriching the supervision signal. (2) Inferred Latent Interaction Graph (ILI) applies the GRaSP algorithm to teacher soft labels to infer a sparse, directed dependency graph among classes. This graph highlights directional dependencies (e.g., \"Train (visual)\" -> \"Motorcycle (audio)\") that expose likely semantic or conditional relationships between classes; these are interpreted as estimated dependency patterns. (3) Latent Interaction Regularizer (LIR): A student network is trained with both metric loss and a regularizer guided by the ILI graph, pulling together embeddings of dependency-linked but unlabeled pairs in proportion to their soft-label probabilities. Experiments on AVE and VEGAS benchmarks show consistent improvements in mean average precision (mAP), demonstrating that integrating inferred latent interactions into embedding learning enhances robustness and semantic coherence.",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.IR",
      "cs.LG",
      "cs.SD"
    ],
    "primary_category": "cs.MM",
    "comment": "16 pages, 5 figures, 2 tables",
    "pdf_url": "https://arxiv.org/pdf/2601.11995v1",
    "published_date": "2026-01-17 10:13:07 UTC",
    "updated_date": "2026-01-17 10:13:07 UTC"
  },
  {
    "arxiv_id": "2601.11979v1",
    "title": "Process In-Context Learning: Enhancing Mathematical Reasoning via Dynamic Demonstration Insertion",
    "authors": [
      "Ang Gao",
      "Changshuo Zhang",
      "Xiao Zhang",
      "Deyang Li",
      "Minjun Zhao",
      "Fangchao Liu",
      "Xinyu Zhang"
    ],
    "abstract": "In-context learning (ICL) has proven highly effective across diverse large language model (LLM) tasks. However, its potential for enhancing tasks that demand step-by-step logical deduction, such as mathematical reasoning, remains underexplored. A core limitation of existing ICL approaches is their static use of demonstrations: examples are pre-selected before inference and remain fixed, failing to adapt to the dynamic confusion points that often arise during multi-step reasoning such as ambiguous calculations or logical gaps. These unresolved confusion points can lead to cascading errors that degrade final accuracy. To tackle this issue, we propose Process In-Context Learning (PICL), a dynamic demonstration integration framework designed to boost mathematical reasoning by responding to real-time inference needs. PICL operates in two stages: 1)~it identifies potential confusion points by analyzing semantics and entropy in the reasoning process and summarizes their core characteristics; 2)~upon encountering these points, it retrieves relevant demonstrations from the demonstration pool that match the confusion context and inserts them directly into the ongoing reasoning process to guide subsequent steps. Experiments show that PICL outperforms baseline methods by mitigating mid-inference confusion, highlighting the value of adaptive demonstration insertion in complex mathematical reasoning.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.11979v1",
    "published_date": "2026-01-17 09:20:06 UTC",
    "updated_date": "2026-01-17 09:20:06 UTC"
  },
  {
    "arxiv_id": "2601.11977v1",
    "title": "One-Shot Price Forecasting with Covariate-Guided Experts under Privacy Constraints",
    "authors": [
      "Ren He",
      "Yinliang Xu",
      "Jinfeng Wang",
      "Jeremy Watson",
      "Jian Song"
    ],
    "abstract": "Forecasting in power systems often involves multivariate time series with complex dependencies and strict privacy constraints across regions. Traditional forecasting methods require significant expert knowledge and struggle to generalize across diverse deployment scenarios. Recent advancements in pre-trained time series models offer new opportunities, but their zero-shot performance on domain-specific tasks remains limited. To address these challenges, we propose a novel MoE Encoder module that augments pretrained forecasting models by injecting a sparse mixture-of-experts layer between tokenization and encoding. This design enables two key capabilities: (1) trans forming multivariate forecasting into an expert-guided univariate task, allowing the model to effectively capture inter-variable relations, and (2) supporting localized training and lightweight parameter sharing in federated settings where raw data cannot be exchanged. Extensive experiments on public multivariate datasets demonstrate that MoE-Encoder significantly improves forecasting accuracy compared to strong baselines. We further simulate federated environments and show that transferring only MoE-Encoder parameters allows efficient adaptation to new regions, with minimal performance degradation. Our findings suggest that MoE-Encoder provides a scalable and privacy-aware extension to foundation time series models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.11977v1",
    "published_date": "2026-01-17 09:13:57 UTC",
    "updated_date": "2026-01-17 09:13:57 UTC"
  },
  {
    "arxiv_id": "2601.11974v1",
    "title": "Learn Like Humans: Use Meta-cognitive Reflection for Efficient Self-Improvement",
    "authors": [
      "Xinmeng Hou",
      "Peiliang Gong",
      "Bohao Qu",
      "Wuqi Wang",
      "Qing Guo",
      "Yang Liu"
    ],
    "abstract": "While Large Language Models (LLMs) enable complex autonomous behavior, current agents remain constrained by static, human-designed prompts that limit adaptability. Existing self-improving frameworks attempt to bridge this gap but typically rely on inefficient, multi-turn recursive loops that incur high computational costs. To address this, we propose Metacognitive Agent Reflective Self-improvement (MARS), a framework that achieves efficient self-evolution within a single recurrence cycle. Inspired by educational psychology, MARS mimics human learning by integrating principle-based reflection (abstracting normative rules to avoid errors) and procedural reflection (deriving step-by-step strategies for success). By synthesizing these insights into optimized instructions, MARS allows agents to systematically refine their reasoning logic without continuous online feedback. Extensive experiments on six benchmarks demonstrate that MARS outperforms state-of-the-art self-evolving systems while significantly reducing computational overhead.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.11974v1",
    "published_date": "2026-01-17 09:12:26 UTC",
    "updated_date": "2026-01-17 09:12:26 UTC"
  },
  {
    "arxiv_id": "2601.11969v1",
    "title": "$\\texttt{MemoryRewardBench}$: Benchmarking Reward Models for Long-Term Memory Management in Large Language Models",
    "authors": [
      "Zecheng Tang",
      "Baibei Ji",
      "Ruoxi Sun",
      "Haitian Wang",
      "WangJie You",
      "Zhang Yijun",
      "Wenpeng Zhu",
      "Ji Qi",
      "Juntao Li",
      "Min Zhang"
    ],
    "abstract": "Existing works increasingly adopt memory-centric mechanisms to process long contexts in a segment manner, and effective memory management is one of the key capabilities that enables large language models to effectively propagate information across the entire sequence. Therefore, leveraging reward models (RMs) to automatically and reliably evaluate memory quality is critical. In this work, we introduce $\\texttt{MemoryRewardBench}$, the first benchmark to systematically study the ability of RMs to evaluate long-term memory management processes. $\\texttt{MemoryRewardBench}$ covers both long-context comprehension and long-form generation tasks, featuring 10 distinct settings with different memory management patterns, with context length ranging from 8K to 128K tokens. Evaluations on 13 cutting-edge RMs indicate a diminishing performance gap between open-source and proprietary models, with newer-generation models consistently outperforming their predecessors regardless of parameter count. We further expose the capabilities and fundamental limitations of current RMs in evaluating LLM memory management across diverse settings.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.11969v1",
    "published_date": "2026-01-17 09:04:53 UTC",
    "updated_date": "2026-01-17 09:04:53 UTC"
  },
  {
    "arxiv_id": "2601.11960v1",
    "title": "R$^2$PO: Decoupling Training Trajectories from Inference Responses for LLM Reasoning",
    "authors": [
      "Jingchu Wang",
      "Bingbing Xu",
      "Yige Yuan",
      "Bin Xie",
      "Xiaoqian Sun",
      "Huawei Shen"
    ],
    "abstract": "Reinforcement learning has become a central paradigm for improving LLM reasoning. However, existing methods use a single policy to produce both inference responses and training optimization trajectories. The objective conflict between generating stable inference responses and diverse training trajectories leads to insufficient exploration, which harms reasoning capability. In this paper, to address the problem, we propose R$^2$PO (Residual Rollout Policy Optimization), which introduces a lightweight Residual Rollout-Head atop the policy to decouple training trajectories from inference responses, enabling controlled trajectory diversification during training while keeping inference generation stable. Experiments across multiple benchmarks show that our method consistently outperforms baselines, achieving average accuracy gains of 3.1% on MATH-500 and 2.4% on APPS, while also reducing formatting errors and mitigating length bias for stable optimization. Our code is publicly available at https://github.com/RRPO-ARR/Code.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.11960v1",
    "published_date": "2026-01-17 08:30:50 UTC",
    "updated_date": "2026-01-17 08:30:50 UTC"
  },
  {
    "arxiv_id": "2601.11956v1",
    "title": "Double-Calibration: Towards Trustworthy LLMs via Calibrating Knowledge and Reasoning Confidence",
    "authors": [
      "Yuyin Lu",
      "Ziran Liang",
      "Yanghui Rao",
      "Wenqi Fan",
      "Fu Lee Wang",
      "Qing Li"
    ],
    "abstract": "Trustworthy reasoning in Large Language Models (LLMs) is challenged by their propensity for hallucination. While augmenting LLMs with Knowledge Graphs (KGs) improves factual accuracy, existing KG-augmented methods fail to quantify epistemic uncertainty in both the retrieved evidence and LLMs' reasoning. To bridge this gap, we introduce DoublyCal, a framework built on a novel double-calibration principle. DoublyCal employs a lightweight proxy model to first generate KG evidence alongside a calibrated evidence confidence. This calibrated supporting evidence then guides a black-box LLM, yielding final predictions that are not only more accurate but also well-calibrated, with confidence scores traceable to the uncertainty of the supporting evidence. Experiments on knowledge-intensive benchmarks show that DoublyCal significantly improves both the accuracy and confidence calibration of black-box LLMs with low token cost.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.11956v1",
    "published_date": "2026-01-17 08:18:38 UTC",
    "updated_date": "2026-01-17 08:18:38 UTC"
  },
  {
    "arxiv_id": "2601.11940v1",
    "title": "Thinking Traps in Long Chain-of-Thought: A Measurable Study and Trap-Aware Adaptive Restart",
    "authors": [
      "Kang Chen",
      "Fan Yu",
      "Junjie Nian",
      "Shihan Zhao",
      "Zhuoka Feng",
      "Zijun Yao",
      "Heng Wang",
      "Minshen Yu",
      "Yixin Cao"
    ],
    "abstract": "Scaling test-time compute via Long Chain-of-Thought (Long-CoT) significantly enhances reasoning capabilities, yet extended generation does not guarantee correctness: after an early wrong commitment, models may keep elaborating a self-consistent but incorrect prefix. Through fine-grained trajectory analysis, we identify Thinking Traps, prefix-dominant deadlocks where later reflection, alternative attempts, or verification fails to revise the root error. On a curated subset of DAPO-MATH, 89\\% of failures exhibit such traps. To solve this problem, we introduce TAAR (Trap-Aware Adaptive Restart), a test-time control framework that trains a diagnostic policy to predict two signals from partial trajectories: a trap index for where to truncate and an escape probability for whether and how strongly to intervene. At inference time, TAAR truncates the trajectory before the predicted trap segment and adaptively restarts decoding; for severely trapped cases, it applies stronger perturbations, including higher-temperature resampling and an optional structured reboot suffix. Experiments on challenging mathematical and scientific reasoning benchmarks (AIME24, AIME25, GPQA-Diamond, HMMT25, BRUMO25) show that TAAR improves reasoning performance without fine-tuning base model parameters.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.11940v1",
    "published_date": "2026-01-17 07:26:02 UTC",
    "updated_date": "2026-01-17 07:26:02 UTC"
  },
  {
    "arxiv_id": "2601.11935v1",
    "title": "Big Data Workload Profiling for Energy-Aware Cloud Resource Management",
    "authors": [
      "Milan Parikh",
      "Aniket Abhishek Soni",
      "Sneja Mitinbhai Shah",
      "Ayush Raj Jha"
    ],
    "abstract": "Cloud data centers face increasing pressure to reduce operational energy consumption as big data workloads continue to grow in scale and complexity. This paper presents a workload aware and energy efficient scheduling framework that profiles CPU utilization, memory demand, and storage IO behavior to guide virtual machine placement decisions. By combining historical execution logs with real time telemetry, the proposed system predicts the energy and performance impact of candidate placements and enables adaptive consolidation while preserving service level agreement compliance. The framework is evaluated using representative Hadoop MapReduce, Spark MLlib, and ETL workloads deployed on a multi node cloud testbed. Experimental results demonstrate consistent energy savings of 15 to 20 percent compared to a baseline scheduler, with negligible performance degradation. These findings highlight workload profiling as a practical and scalable strategy for improving the sustainability of cloud based big data processing environments.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.DC",
    "comment": "10 pages, 3 figures. Accepted and presented at the 2026 International Conference on Data Analytics for Sustainability and Engineering Technology (DASET 2026), Track: Big Data and Machine Learning Applications",
    "pdf_url": "https://arxiv.org/pdf/2601.11935v1",
    "published_date": "2026-01-17 06:50:51 UTC",
    "updated_date": "2026-01-17 06:50:51 UTC"
  },
  {
    "arxiv_id": "2601.11920v1",
    "title": "Enhancing LLM-Based Data Annotation with Error Decomposition",
    "authors": [
      "Zhen Xu",
      "Vedant Khatri",
      "Yijun Dai",
      "Xiner Liu",
      "Siyan Li",
      "Xuanming Zhang",
      "Renzhe Yu"
    ],
    "abstract": "Large language models offer a scalable alternative to human coding for data annotation tasks, enabling the scale-up of research across data-intensive domains. While LLMs are already achieving near-human accuracy on objective annotation tasks, their performance on subjective annotation tasks, such as those involving psychological constructs, is less consistent and more prone to errors. Standard evaluation practices typically collapse all annotation errors into a single alignment metric, but this simplified approach may obscure different kinds of errors that affect final analytical conclusions in different ways. Here, we propose a diagnostic evaluation paradigm that incorporates a human-in-the-loop step to separate task-inherent ambiguity from model-driven inaccuracies and assess annotation quality in terms of their potential downstream impacts. We refine this paradigm on ordinal annotation tasks, which are common in subjective annotation. The refined paradigm includes: (1) a diagnostic taxonomy that categorizes LLM annotation errors along two dimensions: source (model-specific vs. task-inherent) and type (boundary ambiguity vs. conceptual misidentification); (2) a lightweight human annotation test to estimate task-inherent ambiguity from LLM annotations; and (3) a computational method to decompose observed LLM annotation errors following our taxonomy. We validate this paradigm on four educational annotation tasks, demonstrating both its conceptual validity and practical utility. Theoretically, our work provides empirical evidence for why excessively high alignment is unrealistic in specific annotation tasks and why single alignment metrics inadequately reflect the quality of LLM annotations. In practice, our paradigm can be a low-cost diagnostic tool that assesses the suitability of a given task for LLM annotation and provides actionable insights for further technical optimization.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.11920v1",
    "published_date": "2026-01-17 05:43:17 UTC",
    "updated_date": "2026-01-17 05:43:17 UTC"
  },
  {
    "arxiv_id": "2601.11913v1",
    "title": "LSTM-MAS: A Long Short-Term Memory Inspired Multi-Agent System for Long-Context Understanding",
    "authors": [
      "Yichen Jiang",
      "Peng Ye",
      "Jiakang Yuan",
      "Chongjun Tu",
      "Lei Bai",
      "Tao Chen"
    ],
    "abstract": "Effectively processing long contexts remains a fundamental yet unsolved challenge for large language models (LLMs). Existing single-LLM-based methods primarily reduce the context window or optimize the attention mechanism, but they often encounter additional computational costs or constrained expanded context length. While multi-agent-based frameworks can mitigate these limitations, they remain susceptible to the accumulation of errors and the propagation of hallucinations. In this work, we draw inspiration from the Long Short-Term Memory (LSTM) architecture to design a Multi-Agent System called LSTM-MAS, emulating LSTM's hierarchical information flow and gated memory mechanisms for long-context understanding. Specifically, LSTM-MAS organizes agents in a chained architecture, where each node comprises a worker agent for segment-level comprehension, a filter agent for redundancy reduction, a judge agent for continuous error detection, and a manager agent for globally regulates information propagation and retention, analogous to LSTM and its input gate, forget gate, constant error carousel unit, and output gate. These novel designs enable controlled information transfer and selective long-term dependency modeling across textual segments, which can effectively avoid error accumulation and hallucination propagation. We conducted an extensive evaluation of our method. Compared with the previous best multi-agent approach, CoA, our model achieves improvements of 40.93%, 43.70%,121.57% and 33.12%, on NarrativeQA, Qasper, HotpotQA, and MuSiQue, respectively.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2601.11913v1",
    "published_date": "2026-01-17 05:16:23 UTC",
    "updated_date": "2026-01-17 05:16:23 UTC"
  },
  {
    "arxiv_id": "2601.11907v1",
    "title": "Towards Airborne Object Detection: A Deep Learning Analysis",
    "authors": [
      "Prosenjit Chatterjee",
      "ANK Zaman"
    ],
    "abstract": "The rapid proliferation of airborne platforms, including commercial aircraft, drones, and UAVs, has intensified the need for real-time, automated threat assessment systems. Current approaches depend heavily on manual monitoring, resulting in limited scalability and operational inefficiencies. This work introduces a dual-task model based on EfficientNetB4 capable of performing airborne object classification and threat-level prediction simultaneously. To address the scarcity of clean, balanced training data, we constructed the AODTA Dataset by aggregating and refining multiple public sources. We benchmarked our approach on both the AVD Dataset and the newly developed AODTA Dataset and further compared performance against a ResNet-50 baseline, which consistently underperformed EfficientNetB4. Our EfficientNetB4 model achieved 96% accuracy in object classification and 90% accuracy in threat-level prediction, underscoring its promise for applications in surveillance, defense, and airspace management. Although the title references detection, this study focuses specifically on classification and threat-level inference using pre-localized airborne object images provided by existing datasets.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.SE"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.11907v1",
    "published_date": "2026-01-17 04:47:47 UTC",
    "updated_date": "2026-01-17 04:47:47 UTC"
  },
  {
    "arxiv_id": "2601.11905v1",
    "title": "LIBRA: Language Model Informed Bandit Recourse Algorithm for Personalized Treatment Planning",
    "authors": [
      "Junyu Cao",
      "Ruijiang Gao",
      "Esmaeil Keyvanshokooh",
      "Jianhao Ma"
    ],
    "abstract": "We introduce a unified framework that seamlessly integrates algorithmic recourse, contextual bandits, and large language models (LLMs) to support sequential decision-making in high-stakes settings such as personalized medicine. We first introduce the recourse bandit problem, where a decision-maker must select both a treatment action and a feasible, minimal modification to mutable patient features. To address this problem, we develop the Generalized Linear Recourse Bandit (GLRB) algorithm. Building on this foundation, we propose LIBRA, a Language Model-Informed Bandit Recourse Algorithm that strategically combines domain knowledge from LLMs with the statistical rigor of bandit learning. LIBRA offers three key guarantees: (i) a warm-start guarantee, showing that LIBRA significantly reduces initial regret when LLM recommendations are near-optimal; (ii) an LLM-effort guarantee, proving that the algorithm consults the LLM only $O(\\log^2 T)$ times, where $T$ is the time horizon, ensuring long-term autonomy; and (iii) a robustness guarantee, showing that LIBRA never performs worse than a pure bandit algorithm even when the LLM is unreliable. We further establish matching lower bounds that characterize the fundamental difficulty of the recourse bandit problem and demonstrate the near-optimality of our algorithms. Experiments on synthetic environments and a real hypertension-management case study confirm that GLRB and LIBRA improve regret, treatment quality, and sample efficiency compared with standard contextual bandits and LLM-only benchmarks. Our results highlight the promise of recourse-aware, LLM-assisted bandit algorithms for trustworthy LLM-bandits collaboration in personalized high-stakes decision-making.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "math.ST"
    ],
    "primary_category": "cs.AI",
    "comment": "50 pages. Previous version with human-AI collaboration: arXiv:2410.14640",
    "pdf_url": "https://arxiv.org/pdf/2601.11905v1",
    "published_date": "2026-01-17 04:37:20 UTC",
    "updated_date": "2026-01-17 04:37:20 UTC"
  },
  {
    "arxiv_id": "2601.11903v1",
    "title": "AEMA: Verifiable Evaluation Framework for Trustworthy and Controlled Agentic LLM Systems",
    "authors": [
      "YenTing Lee",
      "Keerthi Koneru",
      "Zahra Moslemi",
      "Sheethal Kumar",
      "Ramesh Radhakrishnan"
    ],
    "abstract": "Evaluating large language model (LLM)-based multi-agent systems remains a critical challenge, as these systems must exhibit reliable coordination, transparent decision-making, and verifiable performance across evolving tasks. Existing evaluation approaches often limit themselves to single-response scoring or narrow benchmarks, which lack stability, extensibility, and automation when deployed in enterprise settings at multi-agent scale. We present AEMA (Adaptive Evaluation Multi-Agent), a process-aware and auditable framework that plans, executes, and aggregates multi-step evaluations across heterogeneous agentic workflows under human oversight. Compared to a single LLM-as-a-Judge, AEMA achieves greater stability, human alignment, and traceable records that support accountable automation. Our results on enterprise-style agent workflows simulated using realistic business scenarios demonstrate that AEMA provides a transparent and reproducible pathway toward responsible evaluation of LLM-based multi-agent systems.\n  Keywords Agentic AI, Multi-Agent Systems, Trustworthy AI, Verifiable Evaluation, Human Oversight",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Workshop on W51: How Can We Trust and Control Agentic AI? Toward Alignment, Robustness, and Verifiability in Autonomous LLM Agents at AAAI 2026",
    "pdf_url": "https://arxiv.org/pdf/2601.11903v1",
    "published_date": "2026-01-17 04:09:02 UTC",
    "updated_date": "2026-01-17 04:09:02 UTC"
  },
  {
    "arxiv_id": "2601.11895v1",
    "title": "DevBench: A Realistic, Developer-Informed Benchmark for Code Generation Models",
    "authors": [
      "Pareesa Ameneh Golnari",
      "Adarsh Kumarappan",
      "Wen Wen",
      "Xiaoyu Liu",
      "Gabriel Ryan",
      "Yuting Sun",
      "Shengyu Fu",
      "Elsie Nallipogu"
    ],
    "abstract": "DevBench is a telemetry-driven benchmark designed to evaluate Large Language Models (LLMs) on realistic code completion tasks. It includes 1,800 evaluation instances across six programming languages and six task categories derived from real developer telemetry, such as API usage and code purpose understanding. Unlike prior benchmarks, it emphasizes ecological validity, avoids training data contamination, and enables detailed diagnostics. The evaluation combines functional correctness, similarity-based metrics, and LLM-judge assessments focused on usefulness and contextual relevance. 9 state-of-the-art models were assessed, revealing differences in syntactic precision, semantic reasoning, and practical utility. Our benchmark provides actionable insights to guide model selection and improvement-detail that is often missing from other benchmarks but is essential for both practical deployment and targeted model development.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.11895v1",
    "published_date": "2026-01-17 03:33:08 UTC",
    "updated_date": "2026-01-17 03:33:08 UTC"
  },
  {
    "arxiv_id": "2601.11885v1",
    "title": "MyGram: Modality-aware Graph Transformer with Global Distribution for Multi-modal Entity Alignment",
    "authors": [
      "Zhifei Li",
      "Ziyue Qin",
      "Xiangyu Luo",
      "Xiaoju Hou",
      "Yue Zhao",
      "Miao Zhang",
      "Zhifang Huang",
      "Kui Xiao",
      "Bing Yang"
    ],
    "abstract": "Multi-modal entity alignment aims to identify equivalent entities between two multi-modal Knowledge graphs by integrating multi-modal data, such as images and text, to enrich the semantic representations of entities. However, existing methods may overlook the structural contextual information within each modality, making them vulnerable to interference from shallow features. To address these challenges, we propose MyGram, a modality-aware graph transformer with global distribution for multi-modal entity alignment. Specifically, we develop a modality diffusion learning module to capture deep structural contextual information within modalities and enable fine-grained multi-modal fusion. In addition, we introduce a Gram Loss that acts as a regularization constraint by minimizing the volume of a 4-dimensional parallelotope formed by multi-modal features, thereby achieving global distribution consistency across modalities. We conduct experiments on five public datasets. Results show that MyGram outperforms baseline models, achieving a maximum improvement of 4.8% in Hits@1 on FBDB15K, 9.9% on FBYG15K, and 4.3% on DBP15K.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by AAAI 2026",
    "pdf_url": "https://arxiv.org/pdf/2601.11885v1",
    "published_date": "2026-01-17 02:51:42 UTC",
    "updated_date": "2026-01-17 02:51:42 UTC"
  },
  {
    "arxiv_id": "2601.11880v1",
    "title": "TF-CoDiT: Conditional Time Series Synthesis with Diffusion Transformers for Treasury Futures",
    "authors": [
      "Yingxiao Zhang",
      "Jiaxin Duan",
      "Junfu Zhang",
      "Ke Feng"
    ],
    "abstract": "Diffusion Transformers (DiT) have achieved milestones in synthesizing financial time-series data, such as stock prices and order flows. However, their performance in synthesizing treasury futures data is still underexplored. This work emphasizes the characteristics of treasury futures data, including its low volume, market dependencies, and the grouped correlations among multivariables. To overcome these challenges, we propose TF-CoDiT, the first DiT framework for language-controlled treasury futures synthesis. To facilitate low-data learning, TF-CoDiT adapts the standard DiT by transforming multi-channel 1-D time series into Discrete Wavelet Transform (DWT) coefficient matrices. A U-shape VAE is proposed to encode cross-channel dependencies hierarchically into a latent variable and bridge the latent and DWT spaces through decoding, thereby enabling latent diffusion generation. To derive prompts that cover essential conditions, we introduce the Financial Market Attribute Protocol (FinMAP) - a multi-level description system that standardizes daily$/$periodical market dynamics by recognizing 17$/$23 economic indicators from 7/8 perspectives. In our experiments, we gather four types of treasury futures data covering the period from 2015 to 2025, and define data synthesis tasks with durations ranging from one week to four months. Extensive evaluations demonstrate that TF-CoDiT can produce highly authentic data with errors at most 0.433 (MSE) and 0.453 (MAE) to the ground-truth. Further studies evidence the robustness of TF-CoDiT across contracts and temporal horizons.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.11880v1",
    "published_date": "2026-01-17 02:27:56 UTC",
    "updated_date": "2026-01-17 02:27:56 UTC"
  },
  {
    "arxiv_id": "2601.11876v1",
    "title": "AI for Green Spaces: Leveraging Autonomous Navigation and Computer Vision for Park Litter Removal",
    "authors": [
      "Christopher Kao",
      "Akhil Pathapati",
      "James Davis"
    ],
    "abstract": "There are 50 billion pieces of litter in the U.S. alone. Grass fields contribute to this problem because picnickers tend to leave trash on the field. We propose building a robot that can autonomously navigate, identify, and pick up trash in parks. To autonomously navigate the park, we used a Spanning Tree Coverage (STC) algorithm to generate a coverage path the robot could follow. To navigate this path, we successfully used Real-Time Kinematic (RTK) GPS, which provides a centimeter-level reading every second. For computer vision, we utilized the ResNet50 Convolutional Neural Network (CNN), which detects trash with 94.52% accuracy. For trash pickup, we tested multiple design concepts. We select a new pickup mechanism that specifically targets the trash we encounter on the field. Our solution achieved an overall success rate of 80%, demonstrating that autonomous trash pickup robots on grass fields are a viable solution.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "Published in IEEE/SICE SII 2025",
    "pdf_url": "https://arxiv.org/pdf/2601.11876v1",
    "published_date": "2026-01-17 02:05:05 UTC",
    "updated_date": "2026-01-17 02:05:05 UTC"
  },
  {
    "arxiv_id": "2601.11868v1",
    "title": "Terminal-Bench: Benchmarking Agents on Hard, Realistic Tasks in Command Line Interfaces",
    "authors": [
      "Mike A. Merrill",
      "Alexander G. Shaw",
      "Nicholas Carlini",
      "Boxuan Li",
      "Harsh Raj",
      "Ivan Bercovich",
      "Lin Shi",
      "Jeong Yeon Shin",
      "Thomas Walshe",
      "E. Kelly Buchanan",
      "Junhong Shen",
      "Guanghao Ye",
      "Haowei Lin",
      "Jason Poulos",
      "Maoyu Wang",
      "Marianna Nezhurina",
      "Jenia Jitsev",
      "Di Lu",
      "Orfeas Menis Mastromichalakis",
      "Zhiwei Xu",
      "Zizhao Chen",
      "Yue Liu",
      "Robert Zhang",
      "Leon Liangyu Chen",
      "Anurag Kashyap",
      "Jan-Lucas Uslu",
      "Jeffrey Li",
      "Jianbo Wu",
      "Minghao Yan",
      "Song Bian",
      "Vedang Sharma",
      "Ke Sun",
      "Steven Dillmann",
      "Akshay Anand",
      "Andrew Lanpouthakoun",
      "Bardia Koopah",
      "Changran Hu",
      "Etash Guha",
      "Gabriel H. S. Dreiman",
      "Jiacheng Zhu",
      "Karl Krauth",
      "Li Zhong",
      "Niklas Muennighoff",
      "Robert Amanfu",
      "Shangyin Tan",
      "Shreyas Pimpalgaonkar",
      "Tushar Aggarwal",
      "Xiangning Lin",
      "Xin Lan",
      "Xuandong Zhao",
      "Yiqing Liang",
      "Yuanli Wang",
      "Zilong Wang",
      "Changzhi Zhou",
      "David Heineman",
      "Hange Liu",
      "Harsh Trivedi",
      "John Yang",
      "Junhong Lin",
      "Manish Shetty",
      "Michael Yang",
      "Nabil Omi",
      "Negin Raoof",
      "Shanda Li",
      "Terry Yue Zhuo",
      "Wuwei Lin",
      "Yiwei Dai",
      "Yuxin Wang",
      "Wenhao Chai",
      "Shang Zhou",
      "Dariush Wahdany",
      "Ziyu She",
      "Jiaming Hu",
      "Zhikang Dong",
      "Yuxuan Zhu",
      "Sasha Cui",
      "Ahson Saiyed",
      "Arinbjörn Kolbeinsson",
      "Jesse Hu",
      "Christopher Michael Rytting",
      "Ryan Marten",
      "Yixin Wang",
      "Alex Dimakis",
      "Andy Konwinski",
      "Ludwig Schmidt"
    ],
    "abstract": "AI agents may soon become capable of autonomously completing valuable, long-horizon tasks in diverse domains. Current benchmarks either do not measure real-world tasks, or are not sufficiently difficult to meaningfully measure frontier models. To this end, we present Terminal-Bench 2.0: a carefully curated hard benchmark composed of 89 tasks in computer terminal environments inspired by problems from real workflows. Each task features a unique environment, human-written solution, and comprehensive tests for verification. We show that frontier models and agents score less than 65\\% on the benchmark and conduct an error analysis to identify areas for model and agent improvement. We publish the dataset and evaluation harness to assist developers and researchers in future work at https://www.tbench.ai/ .",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.11868v1",
    "published_date": "2026-01-17 01:29:30 UTC",
    "updated_date": "2026-01-17 01:29:30 UTC"
  },
  {
    "arxiv_id": "2601.11863v1",
    "title": "Utilizing Metadata for Better Retrieval-Augmented Generation",
    "authors": [
      "Raquib Bin Yousuf",
      "Shengzhe Xu",
      "Mandar Sharma",
      "Andrew Neeser",
      "Chris Latimer",
      "Naren Ramakrishnan"
    ],
    "abstract": "Retrieval-Augmented Generation systems depend on retrieving semantically relevant document chunks to support accurate, grounded outputs from large language models. In structured and repetitive corpora such as regulatory filings, chunk similarity alone often fails to distinguish between documents with overlapping language. Practitioners often flatten metadata into input text as a heuristic, but the impact and trade-offs of this practice remain poorly understood. We present a systematic study of metadata-aware retrieval strategies, comparing plain-text baselines with approaches that embed metadata directly. Our evaluation spans metadata-as-text (prefix and suffix), a dual-encoder unified embedding that fuses metadata and content in a single index, dual-encoder late-fusion retrieval, and metadata-aware query reformulation. Across multiple retrieval metrics and question types, we find that prefixing and unified embeddings consistently outperform plain-text baselines, with the unified at times exceeding prefixing while being easier to maintain. Beyond empirical comparisons, we analyze embedding space, showing that metadata integration improves effectiveness by increasing intra-document cohesion, reducing inter-document confusion, and widening the separation between relevant and irrelevant chunks. Field-level ablations show that structural cues provide strong disambiguating signals. Our code, evaluation framework, and the RAGMATE-10K dataset are publicly hosted.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CE",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "The 48th European Conference on Information Retrieval (ECIR 2026)",
    "pdf_url": "https://arxiv.org/pdf/2601.11863v1",
    "published_date": "2026-01-17 01:11:03 UTC",
    "updated_date": "2026-01-17 01:11:03 UTC"
  },
  {
    "arxiv_id": "2601.11859v1",
    "title": "Cascaded Transformer for Robust and Scalable SLA Decomposition via Amortized Optimization",
    "authors": [
      "Cyril Shih-Huan Hsu"
    ],
    "abstract": "The evolution toward 6G networks increasingly relies on network slicing to provide tailored, End-to-End (E2E) logical networks over shared physical infrastructures. A critical challenge is effectively decomposing E2E Service Level Agreements (SLAs) into domain-specific SLAs, which current solutions handle through computationally intensive, iterative optimization processes that incur substantial latency and complexity. To address this, we introduce Casformer, a cascaded Transformer architecture designed for fast, optimization-free SLA decomposition. Casformer leverages historical domain feedback encoded through domain-specific Transformer encoders in its first layer, and integrates cross-domain dependencies using a Transformer-based aggregator in its second layer. The model is trained under a learning paradigm inspired by Domain-Informed Neural Networks (DINNs), incorporating risk-informed modeling and amortized optimization to learn a stable, forward-only SLA decomposition policy. Extensive evaluations demonstrate that Casformer achieves improved SLA decomposition quality against state-of-the-art optimization-based frameworks, while exhibiting enhanced scalability and robustness under volatile and noisy network conditions. In addition, its forward-only design reduces runtime complexity and simplifies deployment and maintenance. These insights reveal the potential of combining amortized optimization with Transformer-based sequence modeling to advance network automation, providing a scalable and efficient solution suitable for real-time SLA management in advanced 5G-and-beyond network environments.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.11859v1",
    "published_date": "2026-01-17 01:01:53 UTC",
    "updated_date": "2026-01-17 01:01:53 UTC"
  },
  {
    "arxiv_id": "2601.11854v1",
    "title": "ATOD: An Evaluation Framework and Benchmark for Agentic Task-Oriented Dialogue System",
    "authors": [
      "Yifei Zhang",
      "Hooshang Nayyeri",
      "Rinat Khaziev",
      "Emine Yilmaz",
      "Gokhan Tur",
      "Dilek Hakkani-Tür",
      "Hari Thadakamalla"
    ],
    "abstract": "Recent advances in task-oriented dialogue (TOD) systems, driven by large language models (LLMs) with extensive API and tool integration, have enabled conversational agents to coordinate interleaved goals, maintain long-horizon context, and act proactively through asynchronous execution. These capabilities extend beyond traditional TOD systems, yet existing benchmarks lack systematic support for evaluating such agentic behaviors. To address this gap, we introduce ATOD, a benchmark and synthetic dialogue generation pipeline that produces richly annotated conversations requiring long-term reasoning. ATOD captures key characteristics of advanced TOD, including multi-goal coordination, dependency management, memory, adaptability, and proactivity. Building on ATOD, we propose ATOD-Eval, a holistic evaluation framework that translates these dimensions into fine-grained metrics and supports reproducible offline and online evaluation. We further present a strong agentic memory-based evaluator for benchmarking on ATOD. Experiments show that ATOD-Eval enables comprehensive assessment across task completion, agentic capability, and response quality, and that the proposed evaluator offers a better accuracy-efficiency tradeoff compared to existing memory- and LLM-based approaches under this evaluation setting.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.11854v1",
    "published_date": "2026-01-17 00:53:43 UTC",
    "updated_date": "2026-01-17 00:53:43 UTC"
  },
  {
    "arxiv_id": "2601.11850v1",
    "title": "Human-AI Collaborative Inductive Thematic Analysis: AI Guided Analysis and Human Interpretive Authority",
    "authors": [
      "Matthew Nyaaba",
      "Min SungEun",
      "Mary Abiswin Apam",
      "Kwame Owoahene Acheampong",
      "Emmanuel Dwamena",
      "Xiaoming Zhai"
    ],
    "abstract": "The increasing use of generative artificial intelligence (GenAI) in qualitative research raises important questions about analytic practice and interpretive authority. This study examines how researchers interact with an Inductive Thematic Analysis GPT (ITA-GPT), a purpose-built AI tool designed to support inductive thematic analysis through structured, semi-automated prompts aligned with reflexive thematic analysis and verbatim coding principles. Guided by a Human-Artificial Intelligence Collaborative Inductive Thematic Analysis (HACITA) framework, the study focuses on analytic process rather than substantive findings. Three experienced qualitative researchers conducted ITA-GPT assisted analyses of interview transcripts from education research in the Ghanaian teacher education context. The tool supported familiarization, verbatim in vivo coding, gerund-based descriptive coding, and theme development, while enforcing trace to text integrity, coverage checks, and auditability. Data sources included interaction logs, AI-generated tables, researcher revisions, deletions, insertions, comments, and reflexive memos. Findings show that ITA-GPT functioned as a procedural scaffold that structured analytic workflow and enhanced transparency. However, interpretive authority remained with human researchers, who exercised judgment through recurrent analytic actions including modification, deletion, rejection, insertion, and commenting. The study demonstrates how inductive thematic analysis is enacted through responsible human AI collaboration.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.11850v1",
    "published_date": "2026-01-17 00:38:36 UTC",
    "updated_date": "2026-01-17 00:38:36 UTC"
  },
  {
    "arxiv_id": "2601.11840v1",
    "title": "Imandra CodeLogician: Neuro-Symbolic Reasoning for Precise Analysis of Software Logic",
    "authors": [
      "Hongyu Lin",
      "Samer Abdallah",
      "Makar Valentinov",
      "Paul Brennan",
      "Elijah Kagan",
      "Christoph M. Wintersteiger",
      "Denis Ignatovich",
      "Grant Passmore"
    ],
    "abstract": "Large Language Models (LLMs) have shown strong performance on code understanding tasks, yet they fundamentally lack the ability to perform precise, exhaustive mathematical reasoning about program behavior. Existing benchmarks either focus on mathematical proof automation, largely disconnected from real-world software, or on engineering tasks that do not require semantic rigor.\n  We present CodeLogician, a neurosymbolic agent for precise analysis of software logic, integrated with ImandraX, an industrial automated reasoning engine deployed in financial markets and safety-critical systems. Unlike prior approaches that use formal methods primarily to validate LLM outputs, CodeLogician uses LLMs to construct explicit formal models of software systems, enabling automated reasoning to answer rich semantic questions beyond binary verification outcomes.\n  To rigorously evaluate mathematical reasoning about software logic, we introduce code-logic-bench, a benchmark targeting the middle ground between theorem proving and software engineering benchmarks. It measures reasoning correctness about program state spaces, control flow, coverage constraints, and edge cases, with ground truth defined via formal modeling and region decomposition.\n  Comparing LLM-only reasoning against LLMs augmented with CodeLogician, formal augmentation yields substantial improvements, closing a 41-47 percentage point gap in reasoning accuracy. These results demonstrate that neurosymbolic integration is essential for scaling program analysis toward rigorous, autonomous software understanding.",
    "categories": [
      "cs.AI",
      "cs.LO",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "52 pages, 23 figures. Includes a new benchmark dataset (code-logic-bench) and evaluation of neurosymbolic reasoning for software analysis",
    "pdf_url": "https://arxiv.org/pdf/2601.11840v1",
    "published_date": "2026-01-17 00:16:41 UTC",
    "updated_date": "2026-01-17 00:16:41 UTC"
  }
]
{
  "date": "2024-01-13",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-01-13 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文主要聚焦于人工智能和机器学习领域，特别是大型语言模型 (LLMs) 在推理、教育和医疗中的应用，以及量子计算和动态图表示学习的创新。其中，LLMs 在机械工程教育评估中的表现和跨语言推理方法令人印象深刻，同时量子强化学习和图神经网络的相关工作也展示了前沿进展。\n\n### 重点论文讨论\n我们先聊聊那些重要且有话题度的论文，尤其是涉及 LLMs 的研究，它们展示了 AI 在实际应用中的潜力。接下来，简要提及其他相关主题。\n\n1. **量化侧边微调：Quantized Side Tuning for LLMs**  \n   这篇论文提出了一种名为 Quantized Side Tuning (QST) 的方法，用于高效微调量化大型语言模型。通过将模型权重量化到 4 位，并引入独立侧边网络和低秩适配器，显著减少了训练内存占用（最高降低 7 倍）和时间（最高加速 3 倍），在保持性能的同时，解决了微调过程中的内存和激活问题。\n\n2. **评估 LLMs 在机械工程教育中的表现：Assessing Large Language Models in Mechanical Engineering Education**  \n   作者团队包括多名学者（如 Tianming Liu 和 Xianqiao Wang），这篇研究评估了 LLMs（如 GPT-4）在机械力学概念问题上的能力。使用 126 道多选题测试发现，GPT-4 优于人类和其它模型，尤其在流体力学等主题，但弱于连续介质力学。论文强调提示工程的重要性，并展示了 LLMs 作为教育助手的潜力。\n\n3. **EHRAgent：使用代码接口增强 LLMs 的电子健康记录推理：EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records**  \n   这篇工作开发了 EHRAgent，一个基于 LLMs 的代理，通过代码接口和强化学习处理少样本电子健康记录的多表推理。核心贡献是迭代代码生成和长期记忆机制，提高了复杂任务的成功率（比基线高 29.6%），适用于医疗决策。\n\n4. **Open LLMs 在模仿人类个性的能力：Open Models, Closed Minds? On Agents Capabilities in Mimicking Human Personalities through Open Large Language Models**  \n   作者探讨了开源 LLMs（如基于 MBTI 和 BFI 测试）的内在个性和模仿能力。发现这些模型能展示独特个性，但角色和个性提示仅在少数情况下有效，揭示了 LLMs 在心理建模中的局限性。\n\n5. **跨语言指令微调提升推理：xCoT: Cross-lingual Instruction Tuning for Cross-lingual Chain-of-Thought Reasoning**  \n   论文提出 xCoT 框架，通过多语言指令微调和交叉语言提示，增强 LLMs 的跨语言推理能力。核心发现是，结合蒸馏和随机 CoT 策略，能显著缩小语言间差距，在基准测试中表现出色。\n\n6. **CHAMP 数据集：用于 LLMs 数学推理分析的基准：CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs' Mathematical Reasoning Capabilities**  \n   这篇研究引入 CHAMP 数据集，包含高难度数学问题和注解，用于分析 LLMs 的推理过程。发现模型常通过错误步骤获得正确答案，但验证能力弱，强调了提示和提示工程在提升性能中的作用。\n\n### 其他相关论文\n接下来，我们快速掠过一些有代表性的论文，避免冗长。重点放在 AI 应用和机器学习的创新上。\n\n7. **不确定性估计在深度学习中的可扩展方法：Scalable and Efficient Methods for Uncertainty Estimation and Reduction in Deep Learning**  \n   这篇博士论文总结了不确定性处理方法，聚焦于计算内存和贝叶斯神经网络，提高了 OOD 数据检测和能量效率。\n\n8. **盲卫星视频超分辨率：Deep Blind Super-Resolution for Satellite Video**  \n   论文提出 BSVSR 算法，通过多尺度变形卷积处理未知退化，提升卫星视频的分辨率，在真实数据上优于现有模型。\n\n9. **张量图卷积网络用于动态图表示学习：Tensor Graph Convolutional Network for Dynamic Graph Representation Learning**  \n   作者使用张量表示和张量乘积设计网络，同时捕捉动态图的空间-时间特征，在真实数据集上达到最先进性能。\n\n10. **Transformer 在物体再识别中的应用：Transformer for Object Re-Identification**  \n   这篇综述分析了 Transformer 在图像/视频再识别中的优势，涵盖多种场景，并提出新基准，强调其灵活性和鲁棒性。\n\n其他论文如机器人辅助、量子电路合成或知识图谱等，虽然有贡献（如第14篇的动态图建模或第18篇的量子强化学习），但相对较基础或特定领域，我们就简略带过，以控制篇幅。如果您对这些感兴趣，可以查阅原文。\n\n总之，今天的 arXiv 突显了 LLMs 在跨领域应用的潜力，同时量子和图学习领域也有新进展。欢迎读者根据个人兴趣深入探索！（本快报基于 2024-01-13 更新）",
  "papers": [
    {
      "arxiv_id": "2401.07159v1",
      "title": "Quantized Side Tuning: Fast and Memory-Efficient Tuning of Quantized Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Zhengxin Zhang",
        "Dan Zhao",
        "Xupeng Miao",
        "Gabriele Oliaro",
        "Qing Li",
        "Yong Jiang",
        "Zhihao Jia"
      ],
      "abstract": "Finetuning large language models (LLMs) has been empirically effective on a\nvariety of downstream tasks. Existing approaches to finetuning an LLM either\nfocus on parameter-efficient finetuning, which only updates a small number of\ntrainable parameters, or attempt to reduce the memory footprint during the\ntraining phase of the finetuning. Typically, the memory footprint during\nfinetuning stems from three contributors: model weights, optimizer states, and\nintermediate activations. However, existing works still require considerable\nmemory and none can simultaneously mitigate memory footprint for all three\nsources. In this paper, we present Quantized Side Tuing (QST), which enables\nmemory-efficient and fast finetuning of LLMs by operating through a dual-stage\nprocess. First, QST quantizes an LLM's model weights into 4-bit to reduce the\nmemory footprint of the LLM's original weights; QST also introduces a side\nnetwork separated from the LLM, which utilizes the hidden states of the LLM to\nmake task-specific predictions. Using a separate side network avoids performing\nbackpropagation through the LLM, thus reducing the memory requirement of the\nintermediate activations. Furthermore, QST leverages several low-rank adaptors\nand gradient-free downsample modules to significantly reduce the trainable\nparameters, so as to save the memory footprint of the optimizer states.\nExperiments show that QST can reduce the total memory footprint by up to 2.3\n$\\times$ and speed up the finetuning process by up to 3 $\\times$ while\nachieving competent performance compared with the state-of-the-art. When it\ncomes to full finetuning, QST can reduce the total memory footprint up to 7\n$\\times$.",
      "tldr_zh": "该论文提出 Quantized Side Tuning (QST)，一种快速且内存高效的微调方法，用于处理量化的大型语言模型 (LLMs)。QST 通过将模型权重量化到 4-bit、引入分离的侧网络来避免在 LLM 上进行反向传播、并利用低秩适配器 (low-rank adaptors) 和无梯度下采样模块 (gradient-free downsample modules) 来显著减少可训练参数，从而同时降低模型权重、优化器状态和中间激活的内存占用。实验结果显示，QST 可将总内存占用减少高达 2.3 倍，加速微调过程高达 3 倍，并在性能上与最先进方法相当，对于全微调甚至可减少高达 7 倍内存。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "I.2.7"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.07159v1",
      "published_date": "2024-01-13 21:00:21 UTC",
      "updated_date": "2024-01-13 21:00:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:36:59.809056"
    },
    {
      "arxiv_id": "2401.07145v1",
      "title": "Scalable and Efficient Methods for Uncertainty Estimation and Reduction in Deep Learning",
      "title_zh": "深度学习中不确定性估计与减少的可扩展高效方法",
      "authors": [
        "Soyed Tuhin Ahmed"
      ],
      "abstract": "Neural networks (NNs) can achieved high performance in various fields such as\ncomputer vision, and natural language processing. However, deploying NNs in\nresource-constrained safety-critical systems has challenges due to uncertainty\nin the prediction caused by out-of-distribution data, and hardware\nnon-idealities. To address the challenges of deploying NNs in\nresource-constrained safety-critical systems, this paper summarizes the (4th\nyear) PhD thesis work that explores scalable and efficient methods for\nuncertainty estimation and reduction in deep learning, with a focus on\nComputation-in-Memory (CIM) using emerging resistive non-volatile memories. We\ntackle the inherent uncertainties arising from out-of-distribution inputs and\nhardware non-idealities, crucial in maintaining functional safety in automated\ndecision-making systems. Our approach encompasses problem-aware training\nalgorithms, novel NN topologies, and hardware co-design solutions, including\ndropout-based \\emph{binary} Bayesian Neural Networks leveraging spintronic\ndevices and variational inference techniques. These innovations significantly\nenhance OOD data detection, inference accuracy, and energy efficiency, thereby\ncontributing to the reliability and robustness of NN implementations.",
      "tldr_zh": "这篇论文总结了博士研究工作，探讨了在深度学习中处理不确定性的可扩展和高效方法，特别是针对资源受限的安全关键系统，如out-of-distribution (OOD) 数据和硬件非理想性问题。研究重点采用Computation-in-Memory (CIM) 技术，利用新兴电阻式非易失性存储器，结合问题感知训练算法、新型Neural Network (NN) 拓扑以及硬件联合设计方案，包括基于dropout的二进制Bayesian Neural Networks和变分推理技术。这些创新显著提升了OOD数据检测、推理准确性和能量效率，最终增强了NN实现的可靠性和鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.07145v1",
      "published_date": "2024-01-13 19:30:34 UTC",
      "updated_date": "2024-01-13 19:30:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:37:11.357910"
    },
    {
      "arxiv_id": "2401.12983v1",
      "title": "Assessing Large Language Models in Mechanical Engineering Education: A Study on Mechanics-Focused Conceptual Understanding",
      "title_zh": "评估大型语言模型在",
      "authors": [
        "Jie Tian",
        "Jixin Hou",
        "Zihao Wu",
        "Peng Shu",
        "Zhengliang Liu",
        "Yujie Xiang",
        "Beikang Gu",
        "Nicholas Filla",
        "Yiwei Li",
        "Ning Liu",
        "Xianyan Chen",
        "Keke Tang",
        "Tianming Liu",
        "Xianqiao Wang"
      ],
      "abstract": "This study is a pioneering endeavor to investigate the capabilities of Large\nLanguage Models (LLMs) in addressing conceptual questions within the domain of\nmechanical engineering with a focus on mechanics. Our examination involves a\nmanually crafted exam encompassing 126 multiple-choice questions, spanning\nvarious aspects of mechanics courses, including Fluid Mechanics, Mechanical\nVibration, Engineering Statics and Dynamics, Mechanics of Materials, Theory of\nElasticity, and Continuum Mechanics. Three LLMs, including ChatGPT (GPT-3.5),\nChatGPT (GPT-4), and Claude (Claude-2.1), were subjected to evaluation against\nengineering faculties and students with or without mechanical engineering\nbackground. The findings reveal GPT-4's superior performance over the other two\nLLMs and human cohorts in answering questions across various mechanics topics,\nexcept for Continuum Mechanics. This signals the potential future improvements\nfor GPT models in handling symbolic calculations and tensor analyses. The\nperformances of LLMs were all significantly improved with explanations prompted\nprior to direct responses, underscoring the crucial role of prompt engineering.\nInterestingly, GPT-3.5 demonstrates improved performance with prompts covering\na broader domain, while GPT-4 excels with prompts focusing on specific\nsubjects. Finally, GPT-4 exhibits notable advancements in mitigating input\nbias, as evidenced by guessing preferences for humans. This study unveils the\nsubstantial potential of LLMs as highly knowledgeable assistants in both\nmechanical pedagogy and scientific research.",
      "tldr_zh": "本研究评估了大型语言模型（LLMs）在机械工程教育中处理力学概念问题的能力，焦点在于一个包含126道多选题的手动考试，涵盖流体力学（Fluid Mechanics）、机械振动（Mechanical Vibration）、工程静力学和动力学（Engineering Statics and Dynamics）等多个主题。结果显示，GPT-4在大多数力学领域表现优于GPT-3.5、Claude (Claude-2.1)以及人类参与者，仅在连续介质力学（Continuum Mechanics）上稍逊一筹，同时提示工程（如提供解释提示）显著提升了LLMs的准确率。研究还发现，GPT-3.5更适合宽泛领域提示，而GPT-4在特定主题提示下更出色，且在减少输入偏差方面有显著进步，突显了LLMs作为机械工程教学和研究助手的巨大潜力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "physics.ed-ph"
      ],
      "primary_category": "cs.CL",
      "comment": "30 pages, 7 figures, and 1 table",
      "pdf_url": "http://arxiv.org/pdf/2401.12983v1",
      "published_date": "2024-01-13 19:19:04 UTC",
      "updated_date": "2024-01-13 19:19:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:37:23.505557"
    },
    {
      "arxiv_id": "2401.10917v1",
      "title": "Artificial intelligence to automate the systematic review of scientific literature",
      "title_zh": "人工智能用于自动化科学文献的系统综述",
      "authors": [
        "José de la Torre-López",
        "Aurora Ramírez",
        "José Raúl Romero"
      ],
      "abstract": "Artificial intelligence (AI) has acquired notorious relevance in modern\ncomputing as it effectively solves complex tasks traditionally done by humans.\nAI provides methods to represent and infer knowledge, efficiently manipulate\ntexts and learn from vast amount of data. These characteristics are applicable\nin many activities that human find laborious or repetitive, as is the case of\nthe analysis of scientific literature. Manually preparing and writing a\nsystematic literature review (SLR) takes considerable time and effort, since it\nrequires planning a strategy, conducting the literature search and analysis,\nand reporting the findings. Depending on the area under study, the number of\npapers retrieved can be of hundreds or thousands, meaning that filtering those\nrelevant ones and extracting the key information becomes a costly and\nerror-prone process. However, some of the involved tasks are repetitive and,\ntherefore, subject to automation by means of AI. In this paper, we present a\nsurvey of AI techniques proposed in the last 15 years to help researchers\nconduct systematic analyses of scientific literature. We describe the tasks\ncurrently supported, the types of algorithms applied, and available tools\nproposed in 34 primary studies. This survey also provides a historical\nperspective of the evolution of the field and the role that humans can play in\nan increasingly automated SLR process.",
      "tldr_zh": "这篇论文调查了过去 15 年中，人工智能 (AI) 如何用于自动化系统文献综述 (SLR) 的过程，以减少手动搜索、过滤和分析的劳动强度。研究分析了 34 个主要研究，涵盖 AI 支持的任务（如文献检索和信息提取）、应用的算法类型（如文本处理和机器学习），以及现有的工具。结果显示，AI 能显著提高 SLR 的效率，同时强调了人类在监督和决策中的关键作用，为该领域的历史演变提供了全面视角。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "68T01",
        "I.2.m; A.1"
      ],
      "primary_category": "cs.IR",
      "comment": "25 pages, 3 figures, 1 table, journal paper",
      "pdf_url": "http://arxiv.org/pdf/2401.10917v1",
      "published_date": "2024-01-13 19:12:49 UTC",
      "updated_date": "2024-01-13 19:12:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:37:34.413948"
    },
    {
      "arxiv_id": "2401.07139v1",
      "title": "Deep Blind Super-Resolution for Satellite Video",
      "title_zh": "翻译失败",
      "authors": [
        "Yi Xiao",
        "Qiangqiang Yuan",
        "Qiang Zhang",
        "Liangpei Zhang"
      ],
      "abstract": "Recent efforts have witnessed remarkable progress in Satellite Video\nSuper-Resolution (SVSR). However, most SVSR methods usually assume the\ndegradation is fixed and known, e.g., bicubic downsampling, which makes them\nvulnerable in real-world scenes with multiple and unknown degradations. To\nalleviate this issue, blind SR has thus become a research hotspot.\nNevertheless, existing approaches are mainly engaged in blur kernel estimation\nwhile losing sight of another critical aspect for VSR tasks: temporal\ncompensation, especially compensating for blurry and smooth pixels with vital\nsharpness from severely degraded satellite videos. Therefore, this paper\nproposes a practical Blind SVSR algorithm (BSVSR) to explore more sharp cues by\nconsidering the pixel-wise blur levels in a coarse-to-fine manner.\nSpecifically, we employed multi-scale deformable convolution to coarsely\naggregate the temporal redundancy into adjacent frames by window-slid\nprogressive fusion. Then the adjacent features are finely merged into\nmid-feature using deformable attention, which measures the blur levels of\npixels and assigns more weights to the informative pixels, thus inspiring the\nrepresentation of sharpness. Moreover, we devise a pyramid spatial\ntransformation module to adjust the solution space of sharp mid-feature,\nresulting in flexible feature adaptation in multi-level domains. Quantitative\nand qualitative evaluations on both simulated and real-world satellite videos\ndemonstrate that our BSVSR performs favorably against state-of-the-art\nnon-blind and blind SR models. Code will be available at\nhttps://github.com/XY-boy/Blind-Satellite-VSR",
      "tldr_zh": "这篇论文针对卫星视频超分辨率(SVSR)中未知降级（如多重模糊）的问题，提出了一种实用盲式算法BSVSR，以像素级模糊水平为导向，采用粗到细的方法探索锐利线索。BSVSR 具体包括多尺度可变形卷积进行临时冗余聚合、可变形注意力精细合并特征并分配权重，以及金字塔空间变换模块调整特征适应多级域。实验在模拟和真实卫星视频上显示，BSVSR 在定量和定性评估中优于现有非盲和盲 SR 模型，为实际场景应用提供了显著改进。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "Published in IEEE TGRS",
      "pdf_url": "http://arxiv.org/pdf/2401.07139v1",
      "published_date": "2024-01-13 18:56:18 UTC",
      "updated_date": "2024-01-13 18:56:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:37:47.677688"
    },
    {
      "arxiv_id": "2401.07128v3",
      "title": "EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records",
      "title_zh": "EHRAgent：代码增强大型语言模型用于电子健康记录",
      "authors": [
        "Wenqi Shi",
        "Ran Xu",
        "Yuchen Zhuang",
        "Yue Yu",
        "Jieyu Zhang",
        "Hang Wu",
        "Yuanda Zhu",
        "Joyce Ho",
        "Carl Yang",
        "May D. Wang"
      ],
      "abstract": "Large language models (LLMs) have demonstrated exceptional capabilities in\nplanning and tool utilization as autonomous agents, but few have been developed\nfor medical problem-solving. We propose EHRAgent, an LLM agent empowered with a\ncode interface, to autonomously generate and execute code for multi-tabular\nreasoning within electronic health records (EHRs). First, we formulate an EHR\nquestion-answering task into a tool-use planning process, efficiently\ndecomposing a complicated task into a sequence of manageable actions. By\nintegrating interactive coding and execution feedback, EHRAgent learns from\nerror messages and improves the originally generated code through iterations.\nFurthermore, we enhance the LLM agent by incorporating long-term memory, which\nallows EHRAgent to effectively select and build upon the most relevant\nsuccessful cases from past experiences. Experiments on three real-world\nmulti-tabular EHR datasets show that EHRAgent outperforms the strongest\nbaseline by up to 29.6% in success rate. EHRAgent leverages the emerging\nfew-shot learning capabilities of LLMs, enabling autonomous code generation and\nexecution to tackle complex clinical tasks with minimal demonstrations.",
      "tldr_zh": "该研究提出 EHRAgent，一种基于 Large Language Models (LLMs) 的代理系统，通过代码接口实现对电子健康记录 (EHRs) 的少样本 (few-shot) 复杂表格推理。EHRAgent 将 EHR 问答任务分解为工具使用规划序列，并通过交互式编码、执行反馈和迭代改进来从错误消息中学习，同时整合长期记忆机制以利用过去的成功案例。实验在三个真实世界 EHR 数据集上表明，EHRAgent 比最强基线成功率提升高达 29.6%，展示了其在自主代码生成和执行方面的潜力，为处理复杂临床任务提供高效解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted in EMNLP 2024 main conference",
      "pdf_url": "http://arxiv.org/pdf/2401.07128v3",
      "published_date": "2024-01-13 18:09:05 UTC",
      "updated_date": "2024-10-04 05:56:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:37:58.301729"
    },
    {
      "arxiv_id": "2401.07118v1",
      "title": "Exploring of Discrete and Continuous Input Control for AI-enhanced Assistive Robotic Arms",
      "title_zh": "翻译失败",
      "authors": [
        "Max Pascher",
        "Kevin Zinta",
        "Jens Gerken"
      ],
      "abstract": "Robotic arms, integral in domestic care for individuals with motor\nimpairments, enable them to perform Activities of Daily Living (ADLs)\nindependently, reducing dependence on human caregivers. These collaborative\nrobots require users to manage multiple Degrees-of-Freedom (DoFs) for tasks\nlike grasping and manipulating objects. Conventional input devices, typically\nlimited to two DoFs, necessitate frequent and complex mode switches to control\nindividual DoFs. Modern adaptive controls with feed-forward multi-modal\nfeedback reduce the overall task completion time, number of mode switches, and\ncognitive load. Despite the variety of input devices available, their\neffectiveness in adaptive settings with assistive robotics has yet to be\nthoroughly assessed. This study explores three different input devices by\nintegrating them into an established XR framework for assistive robotics,\nevaluating them and providing empirical insights through a preliminary study\nfor future developments.",
      "tldr_zh": "本研究探讨了离散和连续输入控制在 AI 增强辅助机器人臂中的应用，旨在帮助有运动障碍者独立进行日常生活活动 (ADLs)，同时减少对多个自由度 (DoFs) 的复杂管理。传统输入设备往往仅支持两个 DoFs，导致频繁模式切换和增加认知负担，而现代自适应控制通过多模态反馈可降低任务完成时间。研究通过将三种不同输入设备整合到一个现有的 XR 框架中，进行初步实验评估，并提供实证洞见以指导未来辅助机器人开发。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.HC",
      "comment": "Companion of the 2024 ACM/IEEE International Conference on\n  Human-Robot Interaction",
      "pdf_url": "http://arxiv.org/pdf/2401.07118v1",
      "published_date": "2024-01-13 16:57:40 UTC",
      "updated_date": "2024-01-13 16:57:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:38:11.491155"
    },
    {
      "arxiv_id": "2401.07115v3",
      "title": "Open Models, Closed Minds? On Agents Capabilities in Mimicking Human Personalities through Open Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Lucio La Cava",
        "Andrea Tagarelli"
      ],
      "abstract": "The emergence of unveiling human-like behaviors in Large Language Models\n(LLMs) has led to a closer connection between NLP and human psychology.\nScholars have been studying the inherent personalities exhibited by LLMs and\nattempting to incorporate human traits and behaviors into them. However, these\nefforts have primarily focused on commercially-licensed LLMs, neglecting the\nwidespread use and notable advancements seen in Open LLMs. This work aims to\naddress this gap by employing a set of 12 LLM Agents based on the most\nrepresentative Open models and subject them to a series of assessments\nconcerning the Myers-Briggs Type Indicator (MBTI) test and the Big Five\nInventory (BFI) test. Our approach involves evaluating the intrinsic\npersonality traits of Open LLM agents and determining the extent to which these\nagents can mimic human personalities when conditioned by specific personalities\nand roles. Our findings unveil that $(i)$ each Open LLM agent showcases\ndistinct human personalities; $(ii)$ personality-conditioned prompting produces\nvarying effects on the agents, with only few successfully mirroring the imposed\npersonality, while most of them being ``closed-minded'' (i.e., they retain\ntheir intrinsic traits); and $(iii)$ combining role and personality\nconditioning can enhance the agents' ability to mimic human personalities. Our\nwork represents a step up in understanding the dense relationship between NLP\nand human psychology through the lens of Open LLMs.",
      "tldr_zh": "这篇论文探讨了基于开放大型语言模型 (Open LLMs) 的代理在模仿人类个性的能力，填补了以往主要关注商业模型的空白。研究者使用 12 个基于代表性 Open LLMs 的代理，进行 Myers-Briggs Type Indicator (MBTI) 和 Big Five Inventory (BFI) 测试，以评估它们的内在个性特征。结果发现，每个代理展示出独特的个性，但个性条件提示的效果有限，大多数代理“closed-minded”，即保留固有特征，只有少数能成功模仿强加的个性。进一步，结合角色和个性条件提示能显著提升代理的模仿能力，为深化 NLP 与人类心理学的关系提供了重要见解。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.HC",
        "physics.soc-ph"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted and presented at the AAAI 2025 Conference. CHANGES in\n  version v2: (i) Enhanced methodology and evaluation based on BFI in addition\n  to MBTI, with expanded set of LLM agents; (ii) author list changed w.r.t.\n  version (v1), see Acknowledgements",
      "pdf_url": "http://arxiv.org/pdf/2401.07115v3",
      "published_date": "2024-01-13 16:41:40 UTC",
      "updated_date": "2025-03-22 22:45:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:38:23.460649"
    },
    {
      "arxiv_id": "2401.08694v2",
      "title": "Combining Confidence Elicitation and Sample-based Methods for Uncertainty Quantification in Misinformation Mitigation",
      "title_zh": "翻译失败",
      "authors": [
        "Mauricio Rivera",
        "Jean-François Godbout",
        "Reihaneh Rabbany",
        "Kellin Pelrine"
      ],
      "abstract": "Large Language Models have emerged as prime candidates to tackle\nmisinformation mitigation. However, existing approaches struggle with\nhallucinations and overconfident predictions. We propose an uncertainty\nquantification framework that leverages both direct confidence elicitation and\nsampled-based consistency methods to provide better calibration for NLP\nmisinformation mitigation solutions. We first investigate the calibration of\nsample-based consistency methods that exploit distinct features of consistency\nacross sample sizes and stochastic levels. Next, we evaluate the performance\nand distributional shift of a robust numeric verbalization prompt across single\nvs. two-step confidence elicitation procedure. We also compare the performance\nof the same prompt with different versions of GPT and different numerical\nscales. Finally, we combine the sample-based consistency and verbalized methods\nto propose a hybrid framework that yields a better uncertainty estimation for\nGPT models. Overall, our work proposes novel uncertainty quantification methods\nthat will improve the reliability of Large Language Models in misinformation\nmitigation applications.",
      "tldr_zh": "本研究针对大型语言模型（Large Language Models）在缓解错误信息（misinformation mitigation）中的幻觉和过度自信问题，提出了一种不确定性量化（uncertainty quantification）框架。该框架结合直接信心激发（confidence elicitation）和基于样本的一致性方法（sample-based consistency methods），通过评估样本大小、随机水平以及数字表述提示的表现，优化了单步和双步信心激发过程。实验比较了不同 GPT 版本和数字规模后，发现混合框架显著提升了不确定性估计的准确性，从而提高了语言模型在错误信息缓解应用中的可靠性和校准性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2401.08694v2",
      "published_date": "2024-01-13 16:36:58 UTC",
      "updated_date": "2024-01-30 21:59:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:38:34.777103"
    },
    {
      "arxiv_id": "2401.07105v3",
      "title": "Graph Language Models",
      "title_zh": "图语言模型",
      "authors": [
        "Moritz Plenz",
        "Anette Frank"
      ],
      "abstract": "While Language Models (LMs) are the workhorses of NLP, their interplay with\nstructured knowledge graphs (KGs) is still actively researched. Current methods\nfor encoding such graphs typically either (i) linearize them for embedding with\nLMs -- which underutilize structural information, or (ii) use Graph Neural\nNetworks (GNNs) to preserve the graph structure -- but GNNs cannot represent\ntext features as well as pretrained LMs. In our work we introduce a novel LM\ntype, the Graph Language Model (GLM), that integrates the strengths of both\napproaches and mitigates their weaknesses. The GLM parameters are initialized\nfrom a pretrained LM to enhance understanding of individual graph concepts and\ntriplets. Simultaneously, we design the GLM's architecture to incorporate graph\nbiases, thereby promoting effective knowledge distribution within the graph.\nThis enables GLMs to process graphs, texts, and interleaved inputs of both.\nEmpirical evaluations on relation classification tasks show that GLM embeddings\nsurpass both LM- and GNN-based baselines in supervised and zero-shot setting,\ndemonstrating their versatility.",
      "tldr_zh": "该研究针对语言模型（LMs）和结构化知识图谱（KGs）的结合问题，提出了Graph Language Model (GLM)，一种新颖的模型类型，它整合了LMs在文本处理上的优势和图神经网络（GNNs）的结构表示能力，同时避免了现有方法的局限性。GLM通过从预训练LMs初始化参数并设计架构来融入图谱偏差（graph biases），从而有效处理图谱、文本以及二者混合输入。实验结果显示，在关系分类任务中，GLM的嵌入表示在监督和零样本设置下均超过了基于LMs和GNNs的基线模型，展示了其在知识图谱应用中的多功能性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "I.2.0; I.2.4; I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at ACL 2024. 9 pages, 10 figures, 9 tables",
      "pdf_url": "http://arxiv.org/pdf/2401.07105v3",
      "published_date": "2024-01-13 16:09:49 UTC",
      "updated_date": "2024-06-03 12:14:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:38:47.279165"
    },
    {
      "arxiv_id": "2401.07102v1",
      "title": "Evolving Code with A Large Language Model",
      "title_zh": "翻译失败",
      "authors": [
        "Erik Hemberg",
        "Stephen Moskal",
        "Una-May O'Reilly"
      ],
      "abstract": "Algorithms that use Large Language Models (LLMs) to evolve code arrived on\nthe Genetic Programming (GP) scene very recently. We present LLM GP, a\nformalized LLM-based evolutionary algorithm designed to evolve code. Like GP,\nit uses evolutionary operators, but its designs and implementations of those\noperators radically differ from GP's because they enlist an LLM, using\nprompting and the LLM's pre-trained pattern matching and sequence completion\ncapability. We also present a demonstration-level variant of LLM GP and share\nits code. By addressing algorithms that range from the formal to hands-on, we\ncover design and LLM-usage considerations as well as the scientific challenges\nthat arise when using an LLM for genetic programming.",
      "tldr_zh": "该研究提出了一种名为 LLM GP 的正式化进化算法，利用 Large Language Models (LLMs) 来进化代码，与传统的 Genetic Programming (GP) 不同，它通过提示和 LLM 的模式匹配及序列完成能力来重新设计进化操作符。LLM GP 旨在解决代码演化中的关键挑战，并提供了一个演示级别的变体及其代码，以展示实际应用。论文讨论了算法设计、LLM 使用考虑以及由此引发的科学挑战，为基于 LLM 的遗传编程开辟了新路径。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "I.2.8"
      ],
      "primary_category": "cs.NE",
      "comment": "34 pages, 9 figures, 6 Tables",
      "pdf_url": "http://arxiv.org/pdf/2401.07102v1",
      "published_date": "2024-01-13 15:57:54 UTC",
      "updated_date": "2024-01-13 15:57:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:38:59.417469"
    },
    {
      "arxiv_id": "2401.07085v3",
      "title": "Three Mechanisms of Feature Learning in a Linear Network",
      "title_zh": "线性网络中特征学习的三个机制",
      "authors": [
        "Yizhou Xu",
        "Liu Ziyin"
      ],
      "abstract": "Understanding the dynamics of neural networks in different width regimes is\ncrucial for improving their training and performance. We present an exact\nsolution for the learning dynamics of a one-hidden-layer linear network, with\none-dimensional data, across any finite width, uniquely exhibiting both kernel\nand feature learning phases. This study marks a technical advancement by\nenabling the analysis of the training trajectory from any initialization and a\ndetailed phase diagram under varying common hyperparameters such as width,\nlayer-wise learning rates, and scales of output and initialization. We identify\nthree novel prototype mechanisms specific to the feature learning regime: (1)\nlearning by alignment, (2) learning by disalignment, and (3) learning by\nrescaling, which contrast starkly with the dynamics observed in the kernel\nregime. Our theoretical findings are substantiated with empirical evidence\nshowing that these mechanisms also manifest in deep nonlinear networks handling\nreal-world tasks, enhancing our understanding of neural network training\ndynamics and guiding the design of more effective learning strategies.",
      "tldr_zh": "本研究分析了单隐藏层线性网络的学习动态，针对不同宽度提供了精确解决方案，揭示了内核学习和特征学习阶段的独特特征。该论文首次识别出三种新型机制，具体为：(1) learning by alignment，(2) learning by disalignment，以及(3) learning by rescaling，这些机制与内核 regime 的动态形成鲜明对比，并通过超参数如宽度和学习率的影响绘制详细相位图。理论发现得到实证验证，表明这些机制也适用于深层非线性网络的真实任务，从而加深了对神经网络训练动态的理解，并为设计更有效的学习策略提供指导。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.07085v3",
      "published_date": "2024-01-13 14:21:46 UTC",
      "updated_date": "2025-02-21 11:50:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:39:11.959752"
    },
    {
      "arxiv_id": "2401.07072v1",
      "title": "InterEvo-TR: Interactive Evolutionary Test Generation With Readability Assessment",
      "title_zh": "Inter",
      "authors": [
        "Pedro Delgado-Pérez",
        "Aurora Ramírez",
        "Kevin J. Valle-Gómez",
        "Inmaculada Medina-Bulo",
        "José Raúl Romero"
      ],
      "abstract": "Automated test case generation has proven to be useful to reduce the usually\nhigh expenses of software testing. However, several studies have also noted the\nskepticism of testers regarding the comprehension of generated test suites when\ncompared to manually designed ones. This fact suggests that involving testers\nin the test generation process could be helpful to increase their acceptance of\nautomatically-produced test suites. In this paper, we propose incorporating\ninteractive readability assessments made by a tester into EvoSuite, a\nwidely-known evolutionary test generation tool. Our approach, InterEvo-TR,\ninteracts with the tester at different moments during the search and shows\ndifferent test cases covering the same coverage target for their subjective\nevaluation. The design of such an interactive approach involves a schedule of\ninteraction, a method to diversify the selected targets, a plan to save and\nhandle the readability values, and some mechanisms to customize the level of\nengagement in the revision, among other aspects. To analyze the potential and\npracticability of our proposal, we conduct a controlled experiment in which 39\nparticipants, including academics, professional developers, and student\ncollaborators, interact with InterEvo-TR. Our results show that the strategy to\nselect and present intermediate results is effective for the purpose of\nreadability assessment. Furthermore, the participants' actions and responses to\na questionnaire allowed us to analyze the aspects influencing test code\nreadability and the benefits and limitations of an interactive approach in the\ncontext of test case generation, paving the way for future developments based\non interactivity.",
      "tldr_zh": "该研究针对自动测试用例生成的软件测试问题，提出 InterEvo-TR 框架，该框架将测试人员的交互式可读性评估整合到进化测试生成工具 EvoSuite 中，以提升自动生成测试套件的可接受性。InterEvo-TR 在搜索过程中通过交互时间表和目标多样化机制，向测试人员展示覆盖相同目标的不同测试用例，并收集主观反馈来保存和处理可读性值。实验涉及 39 名参与者，结果显示该策略在可读性评估方面有效，并分析了影响测试代码可读性的因素以及交互式方法的优势和局限性，为未来测试生成工具的发展提供了新方向。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "68T20",
        "D.2.5; I.2.8"
      ],
      "primary_category": "cs.SE",
      "comment": "17 pages, 10 figures, 5 tables, journal paper",
      "pdf_url": "http://arxiv.org/pdf/2401.07072v1",
      "published_date": "2024-01-13 13:14:29 UTC",
      "updated_date": "2024-01-13 13:14:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:39:24.244120"
    },
    {
      "arxiv_id": "2401.07065v1",
      "title": "Tensor Graph Convolutional Network for Dynamic Graph Representation Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Ling Wang",
        "Ye Yuan"
      ],
      "abstract": "Dynamic graphs (DG) describe dynamic interactions between entities in many\npractical scenarios. Most existing DG representation learning models combine\ngraph convolutional network and sequence neural network, which model\nspatial-temporal dependencies through two different types of neural networks.\nHowever, this hybrid design cannot well capture the spatial-temporal continuity\nof a DG. In this paper, we propose a tensor graph convolutional network to\nlearn DG representations in one convolution framework based on the tensor\nproduct with the following two-fold ideas: a) representing the information of\nDG by tensor form; b) adopting tensor product to design a tensor graph\nconvolutional network modeling spatial-temporal feature simultaneously.\nExperiments on real-world DG datasets demonstrate that our model obtains\nstate-of-the-art performance.",
      "tldr_zh": "本文提出了一种张量图卷积网络（Tensor Graph Convolutional Network），旨在解决动态图（Dynamic Graphs）表示学习中现有模型无法有效捕捉空间-时间连续性的问题。该方法通过张量形式表示动态图信息，并利用张量乘积（Tensor Product）设计一个统一的卷积框架，同时建模空间和时间特征。与传统结合图卷积网络（Graph Convolutional Network）和序列神经网络（Sequence Neural Network）的混合方法相比，该模型在真实世界数据集上的实验结果显示了最先进的性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "6 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2401.07065v1",
      "published_date": "2024-01-13 12:49:56 UTC",
      "updated_date": "2024-01-13 12:49:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:39:36.316840"
    },
    {
      "arxiv_id": "2401.07062v1",
      "title": "Dirichlet-Based Prediction Calibration for Learning with Noisy Labels",
      "title_zh": "翻译失败",
      "authors": [
        "Chen-Chen Zong",
        "Ye-Wen Wang",
        "Ming-Kun Xie",
        "Sheng-Jun Huang"
      ],
      "abstract": "Learning with noisy labels can significantly hinder the generalization\nperformance of deep neural networks (DNNs). Existing approaches address this\nissue through loss correction or example selection methods. However, these\nmethods often rely on the model's predictions obtained from the softmax\nfunction, which can be over-confident and unreliable. In this study, we\nidentify the translation invariance of the softmax function as the underlying\ncause of this problem and propose the \\textit{Dirichlet-based Prediction\nCalibration} (DPC) method as a solution. Our method introduces a calibrated\nsoftmax function that breaks the translation invariance by incorporating a\nsuitable constant in the exponent term, enabling more reliable model\npredictions. To ensure stable model training, we leverage a Dirichlet\ndistribution to assign probabilities to predicted labels and introduce a novel\nevidence deep learning (EDL) loss. The proposed loss function encourages\npositive and sufficiently large logits for the given label, while penalizing\nnegative and small logits for other labels, leading to more distinct logits and\nfacilitating better example selection based on a large-margin criterion.\nThrough extensive experiments on diverse benchmark datasets, we demonstrate\nthat DPC achieves state-of-the-art performance. The code is available at\nhttps://github.com/chenchenzong/DPC.",
      "tldr_zh": "本研究针对带有噪声标签的学习问题，提出了一种Dirichlet-based Prediction Calibration (DPC)方法，以解决现有方法依赖过度自信的softmax函数预测所带来的挑战。DPC通过在softmax函数的指数项中加入适当常数来打破其平移不变性，并利用Dirichlet分布为预测标签分配概率，同时引入一个新的evidence deep learning (EDL)损失函数，该函数鼓励目标标签的logits正且较大，同时惩罚其他标签的logits负且小，从而实现更明显的logits区分和基于大边际准则的样本选择。在多种基准数据集上的广泛实验中，DPC达到了最先进性能，证明了其在提升模型泛化能力方面的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.07062v1",
      "published_date": "2024-01-13 12:33:04 UTC",
      "updated_date": "2024-01-13 12:33:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:39:49.008951"
    },
    {
      "arxiv_id": "2401.07058v1",
      "title": "Does More Advice Help? The Effects of Second Opinions in AI-Assisted Decision Making",
      "title_zh": "翻译失败",
      "authors": [
        "Zhuoran Lu",
        "Dakuo Wang",
        "Ming Yin"
      ],
      "abstract": "AI assistance in decision-making has become popular, yet people's\ninappropriate reliance on AI often leads to unsatisfactory human-AI\ncollaboration performance. In this paper, through three pre-registered,\nrandomized human subject experiments, we explore whether and how the provision\nof {second opinions} may affect decision-makers' behavior and performance in\nAI-assisted decision-making. We find that if both the AI model's decision\nrecommendation and a second opinion are always presented together,\ndecision-makers reduce their over-reliance on AI while increase their\nunder-reliance on AI, regardless whether the second opinion is generated by a\npeer or another AI model. However, if decision-makers have the control to\ndecide when to solicit a peer's second opinion, we find that their active\nsolicitations of second opinions have the potential to mitigate over-reliance\non AI without inducing increased under-reliance in some cases. We conclude by\ndiscussing the implications of our findings for promoting effective human-AI\ncollaborations in decision-making.",
      "tldr_zh": "这篇论文通过三个预注册的随机化人类实验，探讨了在 AI-assisted decision making 中提供 second opinions 是否及如何影响决策者的行为和表现。结果显示，如果 AI 模型的决策推荐和 second opinions 总是同时呈现，决策者会减少对 AI 的 over-reliance，但同时增加 under-reliance，无论 second opinions 来自同行还是另一个 AI 模型。然而，当决策者能主动决定何时寻求同行的 second opinions 时，这种策略可能仅缓解 over-reliance，而在某些情况下不会增加 under-reliance。研究结论为促进有效的人-AI 协作提供了重要启示。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.07058v1",
      "published_date": "2024-01-13 12:19:01 UTC",
      "updated_date": "2024-01-13 12:19:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:40:02.102223"
    },
    {
      "arxiv_id": "2401.07056v1",
      "title": "Aquarium: A Comprehensive Framework for Exploring Predator-Prey Dynamics through Multi-Agent Reinforcement Learning Algorithms",
      "title_zh": "Aquarium: 用于通过多智能体强化学习算法探索捕食者-猎物动态",
      "authors": [
        "Michael Kölle",
        "Yannick Erpelding",
        "Fabian Ritz",
        "Thomy Phan",
        "Steffen Illium",
        "Claudia Linnhoff-Popien"
      ],
      "abstract": "Recent advances in Multi-Agent Reinforcement Learning have prompted the\nmodeling of intricate interactions between agents in simulated environments. In\nparticular, the predator-prey dynamics have captured substantial interest and\nvarious simulations been tailored to unique requirements. To prevent further\ntime-intensive developments, we introduce Aquarium, a comprehensive Multi-Agent\nReinforcement Learning environment for predator-prey interaction, enabling the\nstudy of emergent behavior. Aquarium is open source and offers a seamless\nintegration of the PettingZoo framework, allowing a quick start with proven\nalgorithm implementations. It features physics-based agent movement on a\ntwo-dimensional, edge-wrapping plane. The agent-environment interaction\n(observations, actions, rewards) and the environment settings (agent speed,\nprey reproduction, predator starvation, and others) are fully customizable.\nBesides a resource-efficient visualization, Aquarium supports to record video\nfiles, providing a visual comprehension of agent behavior. To demonstrate the\nenvironment's capabilities, we conduct preliminary studies which use PPO to\ntrain multiple prey agents to evade a predator. In accordance to the\nliterature, we find Individual Learning to result in worse performance than\nParameter Sharing, which significantly improves coordination and\nsample-efficiency.",
      "tldr_zh": "本研究引入了Aquarium，一个全面的Multi-Agent Reinforcement Learning (MARL) 环境，用于探索捕食者-猎物动态。该框架基于开源PettingZoo，提供物理-based代理运动、可自定义的代理-环境交互（如观察、动作、奖励和环境设置，包括代理速度、猎物繁殖及捕食者饥饿等），并支持资源高效的可视化和视频录制。作为初步验证，使用PPO算法训练多个猎物代理躲避捕食者，结果显示Parameter Sharing显著优于Individual Learning，提高了代理的协调性和样本效率。Aquarium的发布有助于避免重复开发，促进MARL在模拟环境中的应用。",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at ICAART",
      "pdf_url": "http://arxiv.org/pdf/2401.07056v1",
      "published_date": "2024-01-13 12:09:49 UTC",
      "updated_date": "2024-01-13 12:09:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:40:12.076885"
    },
    {
      "arxiv_id": "2401.07054v1",
      "title": "A Reinforcement Learning Environment for Directed Quantum Circuit Synthesis",
      "title_zh": "翻译失败",
      "authors": [
        "Michael Kölle",
        "Tom Schubert",
        "Philipp Altmann",
        "Maximilian Zorn",
        "Jonas Stein",
        "Claudia Linnhoff-Popien"
      ],
      "abstract": "With recent advancements in quantum computing technology, optimizing quantum\ncircuits and ensuring reliable quantum state preparation have become\nincreasingly vital. Traditional methods often demand extensive expertise and\nmanual calculations, posing challenges as quantum circuits grow in qubit- and\ngate-count. Therefore, harnessing machine learning techniques to handle the\ngrowing variety of gate-to-qubit combinations is a promising approach. In this\nwork, we introduce a comprehensive reinforcement learning environment for\nquantum circuit synthesis, where circuits are constructed utilizing gates from\nthe the Clifford+T gate set to prepare specific target states. Our experiments\nfocus on exploring the relationship between the depth of synthesized quantum\ncircuits and the circuit depths used for target initialization, as well as\nqubit count. We organize the environment configurations into multiple\nevaluation levels and include a range of well-known quantum states for\nbenchmarking purposes. We also lay baselines for evaluating the environment\nusing Proximal Policy Optimization. By applying the trained agents to benchmark\ntests, we demonstrated their ability to reliably design minimal quantum\ncircuits for a selection of 2-qubit Bell states.",
      "tldr_zh": "这篇论文引入了一个强化学习环境，用于指导量子电路合成，旨在解决传统方法在优化量子电路和状态准备时所需的专业知识和手动计算挑战。环境利用 Clifford+T gate set 构建电路，并通过 Proximal Policy Optimization (PPO) 算法进行基准评估，探索电路深度与目标初始化深度、比特数之间的关系。实验结果表明，训练的代理能够可靠地为选定的 2-qubit Bell states 设计最小量子电路，提升了量子电路合成的效率和可扩展性。",
      "categories": [
        "quant-ph",
        "cs.AI"
      ],
      "primary_category": "quant-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.07054v1",
      "published_date": "2024-01-13 11:55:54 UTC",
      "updated_date": "2024-01-13 11:55:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:40:24.272293"
    },
    {
      "arxiv_id": "2401.07051v1",
      "title": "COIN: Chance-Constrained Imitation Learning for Uncertainty-aware Adaptive Resource Oversubscription Policy",
      "title_zh": "翻译失败",
      "authors": [
        "Lu Wang",
        "Mayukh Das",
        "Fangkai Yang",
        "Chao Duo",
        "Bo Qiao",
        "Hang Dong",
        "Si Qin",
        "Chetan Bansal",
        "Qingwei Lin",
        "Saravan Rajmohan",
        "Dongmei Zhang",
        "Qi Zhang"
      ],
      "abstract": "We address the challenge of learning safe and robust decision policies in\npresence of uncertainty in context of the real scientific problem of adaptive\nresource oversubscription to enhance resource efficiency while ensuring safety\nagainst resource congestion risk.\n  Traditional supervised prediction or forecasting models are ineffective in\nlearning adaptive policies whereas standard online optimization or\nreinforcement learning is difficult to deploy on real systems. Offline methods\nsuch as imitation learning (IL) are ideal since we can directly leverage\nhistorical resource usage telemetry. But, the underlying aleatoric uncertainty\nin such telemetry is a critical bottleneck.\n  We solve this with our proposed novel chance-constrained imitation learning\nframework, which ensures implicit safety against uncertainty in a principled\nmanner via a combination of stochastic (chance) constraints on resource\ncongestion risk and ensemble value functions. This leads to substantial\n($\\approx 3-4\\times$) improvement in resource efficiency and safety in many\noversubscription scenarios, including resource management in cloud services.",
      "tldr_zh": "本研究针对不确定性环境下自适应资源超额订阅（adaptive resource oversubscription）的问题，提出了一种名为 COIN 的机会约束模仿学习（Chance-Constrained Imitation Learning）框架，以提升资源效率同时确保安全，避免资源拥塞风险。COIN 通过结合随机约束（chance constraints）和集成价值函数（ensemble value functions），利用历史资源使用遥测数据来隐式处理 aleatoric uncertainty，从而生成鲁棒的决策策略。实验结果显示，该框架在多种场景中实现了资源效率和安全的显著改善，约 3-4 倍提升，并适用于云服务等实际资源管理应用。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2401.07051v1",
      "published_date": "2024-01-13 11:43:25 UTC",
      "updated_date": "2024-01-13 11:43:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:40:37.713542"
    },
    {
      "arxiv_id": "2401.07043v1",
      "title": "Quantum Advantage Actor-Critic for Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Michael Kölle",
        "Mohamad Hgog",
        "Fabian Ritz",
        "Philipp Altmann",
        "Maximilian Zorn",
        "Jonas Stein",
        "Claudia Linnhoff-Popien"
      ],
      "abstract": "Quantum computing offers efficient encapsulation of high-dimensional states.\nIn this work, we propose a novel quantum reinforcement learning approach that\ncombines the Advantage Actor-Critic algorithm with variational quantum circuits\nby substituting parts of the classical components. This approach addresses\nreinforcement learning's scalability concerns while maintaining high\nperformance. We empirically test multiple quantum Advantage Actor-Critic\nconfigurations with the well known Cart Pole environment to evaluate our\napproach in control tasks with continuous state spaces. Our results indicate\nthat the hybrid strategy of using either a quantum actor or quantum critic with\nclassical post-processing yields a substantial performance increase compared to\npure classical and pure quantum variants with similar parameter counts. They\nfurther reveal the limits of current quantum approaches due to the hardware\nconstraints of noisy intermediate-scale quantum computers, suggesting further\nresearch to scale hybrid approaches for larger and more complex control tasks.",
      "tldr_zh": "本文提出了一种名为 Quantum Advantage Actor-Critic 的新型量子强化学习方法，将 Advantage Actor-Critic 算法与变分量子电路结合，通过替换部分经典组件来提升算法的扩展性和性能。实验在 Cart Pole 环境中测试了多种量子配置，结果显示，使用量子 actor 或 critic 与经典后处理的混合策略，比参数量相似的纯经典或纯量子方法性能大幅提升。研究还揭示了当前 noisy intermediate-scale quantum computers 的硬件限制，强调需要进一步探索混合方法以应用于更大、更复杂的控制任务。",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "comment": "Accepted at ICAART 24",
      "pdf_url": "http://arxiv.org/pdf/2401.07043v1",
      "published_date": "2024-01-13 11:08:45 UTC",
      "updated_date": "2024-01-13 11:08:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:40:48.355544"
    },
    {
      "arxiv_id": "2401.07042v1",
      "title": "GEML: A Grammar-based Evolutionary Machine Learning Approach for Design-Pattern Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Rafael Barbudo",
        "Aurora Ramírez",
        "Francisco Servant",
        "José Raúl Romero"
      ],
      "abstract": "Design patterns (DPs) are recognised as a good practice in software\ndevelopment. However, the lack of appropriate documentation often hampers\ntraceability, and their benefits are blurred among thousands of lines of code.\nAutomatic methods for DP detection have become relevant but are usually based\non the rigid analysis of either software metrics or specific properties of the\nsource code. We propose GEML, a novel detection approach based on evolutionary\nmachine learning using software properties of diverse nature. Firstly, GEML\nmakes use of an evolutionary algorithm to extract those characteristics that\nbetter describe the DP, formulated in terms of human-readable rules, whose\nsyntax is conformant with a context-free grammar. Secondly, a rule-based\nclassifier is built to predict whether new code contains a hidden DP\nimplementation. GEML has been validated over five DPs taken from a public\nrepository recurrently adopted by machine learning studies. Then, we increase\nthis number up to 15 diverse DPs, showing its effectiveness and robustness in\nterms of detection capability. An initial parameter study served to tune a\nparameter setup whose performance guarantees the general applicability of this\napproach without the need to adjust complex parameters to a specific pattern.\nFinally, a demonstration tool is also provided.",
      "tldr_zh": "本研究提出 GEML，一种基于文法（Grammar-based）的进化机器学习（Evolutionary Machine Learning）方法，用于检测软件开发中的设计模式（Design Patterns），以解决现有刚性检测方法（如基于软件指标或源代码属性的分析）存在的局限性。GEML 首先利用进化算法提取描述设计模式的软件特性，形成符合上下文无关文法（Context-Free Grammar）的可读规则，然后构建基于规则的分类器来识别新代码中隐藏的设计模式实现。在实验中，GEML 在 15 种多样化设计模式上验证了其有效性和鲁棒性，检测能力显著提升，并通过参数研究优化了通用参数设置，同时提供了一个演示工具以便实际应用。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "68W50",
        "D.2.7; I.2.8"
      ],
      "primary_category": "cs.SE",
      "comment": "27 pages, 18 tables, 10 figures, journal paper",
      "pdf_url": "http://arxiv.org/pdf/2401.07042v1",
      "published_date": "2024-01-13 11:05:24 UTC",
      "updated_date": "2024-01-13 11:05:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:41:02.150969"
    },
    {
      "arxiv_id": "2401.07037v1",
      "title": "xCoT: Cross-lingual Instruction Tuning for Cross-lingual Chain-of-Thought Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Linzheng Chai",
        "Jian Yang",
        "Tao Sun",
        "Hongcheng Guo",
        "Jiaheng Liu",
        "Bing Wang",
        "Xiannian Liang",
        "Jiaqi Bai",
        "Tongliang Li",
        "Qiyao Peng",
        "Zhoujun Li"
      ],
      "abstract": "Chain-of-thought (CoT) has emerged as a powerful technique to elicit\nreasoning in large language models and improve a variety of downstream tasks.\nCoT mainly demonstrates excellent performance in English, but its usage in\nlow-resource languages is constrained due to poor language generalization. To\nbridge the gap among different languages, we propose a cross-lingual\ninstruction fine-tuning framework (xCOT) to transfer knowledge from\nhigh-resource languages to low-resource languages. Specifically, the\nmultilingual instruction training data (xCOT-INSTRUCT) is created to encourage\nthe semantic alignment of multiple languages. We introduce cross-lingual\nin-context few-shot learning (xICL)) to accelerate multilingual agreement in\ninstruction tuning, where some fragments of source languages in examples are\nrandomly substituted by their counterpart translations of target languages.\nDuring multilingual instruction tuning, we adopt the randomly online CoT\nstrategy to enhance the multilingual reasoning ability of the large language\nmodel by first translating the query to another language and then answering in\nEnglish. To further facilitate the language transfer, we leverage the\nhigh-resource CoT to supervise the training of low-resource languages with\ncross-lingual distillation. Experimental results on previous benchmarks\ndemonstrate the superior performance of xCoT in reducing the gap among\ndifferent languages, highlighting its potential to reduce the cross-lingual\ngap.",
      "tldr_zh": "该论文提出 xCoT 框架，通过跨语言指令微调，将 Chain-of-Thought (CoT) 推理知识从高资源语言转移到低资源语言，旨在解决 CoT 在低资源语言中表现受限的问题。\nxCoT 包括创建多语言指令训练数据 (xCOT-INSTRUCT) 以促进语义对齐、引入跨语言 In-Context Few-Shot Learning (xICL) 通过随机替换语言片段加速一致性，以及采用随机在线 CoT 策略和跨语言蒸馏来增强多语言推理能力。\n实验结果显示，xCoT 在基准测试中显著缩小了不同语言之间的性能差距，展示了其在减少跨语言鸿沟方面的潜力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "11 pages",
      "pdf_url": "http://arxiv.org/pdf/2401.07037v1",
      "published_date": "2024-01-13 10:53:53 UTC",
      "updated_date": "2024-01-13 10:53:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:41:16.018653"
    },
    {
      "arxiv_id": "2401.07031v2",
      "title": "Code Security Vulnerability Repair Using Reinforcement Learning with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Nafis Tanveer Islam",
        "Mohammad Bahrami Karkevandi",
        "Peyman Najafirad"
      ],
      "abstract": "With the recent advancement of Large Language Models (LLMs), generating\nfunctionally correct code has become less complicated for a wide array of\ndevelopers. While using LLMs has sped up the functional development process, it\nposes a heavy risk to code security. Code generation with proper security\nmeasures using LLM is a significantly more challenging task than functional\ncode generation. Security measures may include adding a pair of lines of code\nwith the original code, consisting of null pointer checking or prepared\nstatements for SQL injection prevention. Currently, available code repair LLMs\ngenerate code repair by supervised fine-tuning, where the model looks at\ncross-entropy loss. However, the original and repaired codes are mostly similar\nin functionality and syntactically, except for a few (1-2) lines, which act as\nsecurity measures. This imbalance between the lines needed for security\nmeasures and the functional code enforces the supervised fine-tuned model to\nprioritize generating functional code without adding proper security measures,\nwhich also benefits the model by resulting in minimal loss. Therefore, in this\nwork, for security hardening and strengthening of generated code from LLMs, we\npropose a reinforcement learning-based method for program-specific repair with\nthe combination of semantic and syntactic reward mechanisms that focus heavily\non adding security and functional measures in the code, respectively.",
      "tldr_zh": "本论文探讨了Large Language Models (LLMs)生成代码的功能优势与安全风险之间的矛盾，指出现有基于supervised fine-tuning的代码修复方法往往优先功能代码而忽略安全措施，如添加null pointer checking或SQL injection预防。针对这一问题，研究提出了一种基于Reinforcement Learning的代码修复方法，结合semantic reward（语义奖励）和syntactic reward（语法奖励）机制，以强化代码的安全性和功能性。实验表明，这种方法能更有效地添加少量关键安全行，同时保持代码整体完整性，为LLMs生成的代码安全强化提供了新途径。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.07031v2",
      "published_date": "2024-01-13 10:19:26 UTC",
      "updated_date": "2024-01-30 20:50:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:41:26.348044"
    },
    {
      "arxiv_id": "2401.07022v1",
      "title": "Edge-Enabled Anomaly Detection and Information Completion for Social Network Knowledge Graphs",
      "title_zh": "翻译失败",
      "authors": [
        "Fan Lu",
        "Quan Qi",
        "Huaibin Qin"
      ],
      "abstract": "In the rapidly advancing information era, various human behaviors are being\nprecisely recorded in the form of data, including identity information,\ncriminal records, and communication data. Law enforcement agencies can\neffectively maintain social security and precisely combat criminal activities\nby analyzing the aforementioned data. In comparison to traditional data\nanalysis methods, deep learning models, relying on the robust computational\npower in cloud centers, exhibit higher accuracy in extracting data features and\ninferring data. However, within the architecture of cloud centers, the\ntransmission of data from end devices introduces significant latency, hindering\nreal-time inference of data. Furthermore, low-latency edge computing\narchitectures face limitations in direct deployment due to relatively weak\ncomputing and storage capacities of nodes. To address these challenges, a\nlightweight distributed knowledge graph completion architecture is proposed.\nFirstly, we introduce a lightweight distributed knowledge graph completion\narchitecture that utilizes knowledge graph embedding for data analysis.\nSubsequently, to filter out substandard data, a personnel data quality\nassessment method named PDQA is proposed. Lastly, we present a model pruning\nalgorithm that significantly reduces the model size while maximizing\nperformance, enabling lightweight deployment. In experiments, we compare the\neffects of 11 advanced models on completing the knowledge graph of public\nsecurity personnel information. The results indicate that the RotatE model\noutperforms other models significantly in knowledge graph completion, with the\npruned model size reduced by 70\\%, and hits@10 reaching 86.97\\%.}",
      "tldr_zh": "本研究针对社会网络知识图谱(Knowledge Graphs)中的异常检测和信息完成挑战，提出了一种基于边缘计算的轻量级分布式架构，利用知识图谱嵌入进行数据分析，以解决云中心延迟和边缘节点计算能力不足的问题。该架构引入PDQA (Personnel Data Quality Assessment) 方法来过滤低质量人员数据，并采用模型修剪算法显著减少模型大小，同时保持高性能。在实验中，比较了11个先进模型，结果显示RotatE模型在公共安全人员信息知识图谱完成上表现出色，模型大小减少70%，hits@10达到86.97%。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "20 pages, 6 figures, Has been accepted by Wireless Network",
      "pdf_url": "http://arxiv.org/pdf/2401.07022v1",
      "published_date": "2024-01-13 09:27:37 UTC",
      "updated_date": "2024-01-13 09:27:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:41:38.641902"
    },
    {
      "arxiv_id": "2401.07014v1",
      "title": "Weak Labeling for Cropland Mapping in Africa",
      "title_zh": "翻译失败",
      "authors": [
        "Gilles Quentin Hacheme",
        "Akram Zaytar",
        "Girmaw Abebe Tadesse",
        "Caleb Robinson",
        "Rahul Dodhia",
        "Juan M. Lavista Ferres",
        "Stephen Wood"
      ],
      "abstract": "Cropland mapping can play a vital role in addressing environmental,\nagricultural, and food security challenges. However, in the context of Africa,\npractical applications are often hindered by the limited availability of\nhigh-resolution cropland maps. Such maps typically require extensive human\nlabeling, thereby creating a scalability bottleneck. To address this, we\npropose an approach that utilizes unsupervised object clustering to refine\nexisting weak labels, such as those obtained from global cropland maps. The\nrefined labels, in conjunction with sparse human annotations, serve as training\ndata for a semantic segmentation network designed to identify cropland areas.\nWe conduct experiments to demonstrate the benefits of the improved weak labels\ngenerated by our method. In a scenario where we train our model with only 33\nhuman-annotated labels, the F_1 score for the cropland category increases from\n0.53 to 0.84 when we add the mined negative labels.",
      "tldr_zh": "该论文针对非洲地区农田映射面临的标签稀缺问题，提出了一种弱标签（weak labeling）方法，利用无监督对象聚类（unsupervised object clustering）来改进现有全球农田地图的弱标签，并结合稀疏的人工标注（sparse human annotations）作为语义分割网络（semantic segmentation network）的训练数据。实验结果显示，在仅使用33个人工标注的场景下，添加挖掘的负标签后，农田类别的F_1 score从0.53提升到0.84，显著提高了映射准确性。该方法有助于解决环境、农业和食品安全挑战中的可扩展性瓶颈。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "5 pages",
      "pdf_url": "http://arxiv.org/pdf/2401.07014v1",
      "published_date": "2024-01-13 08:45:41 UTC",
      "updated_date": "2024-01-13 08:45:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:41:50.641560"
    },
    {
      "arxiv_id": "2401.07009v1",
      "title": "Joint Extraction of Uyghur Medicine Knowledge with Edge Computing",
      "title_zh": "基于边缘计算的维吾尔医学知识联合提取",
      "authors": [
        "Fan Lu",
        "Quan Qi",
        "Huaibin Qin"
      ],
      "abstract": "Medical knowledge extraction methods based on edge computing deploy deep\nlearning models on edge devices to achieve localized entity and relation\nextraction. This approach avoids transferring substantial sensitive data to\ncloud data centers, effectively safeguarding the privacy of healthcare\nservices. However, existing relation extraction methods mainly employ a\nsequential pipeline approach, which classifies relations between determined\nentities after entity recognition. This mode faces challenges such as error\npropagation between tasks, insufficient consideration of dependencies between\nthe two subtasks, and the neglect of interrelations between different relations\nwithin a sentence. To address these challenges, a joint extraction model with\nparameter sharing in edge computing is proposed, named CoEx-Bert. This model\nleverages shared parameterization between two models to jointly extract\nentities and relations. Specifically, CoEx-Bert employs two models, each\nseparately sharing hidden layer parameters, and combines these two loss\nfunctions for joint backpropagation to optimize the model parameters.\nAdditionally, it effectively resolves the issue of entity overlapping when\nextracting knowledge from unstructured Uyghur medical texts by considering\ncontextual relations. Finally, this model is deployed on edge devices for\nreal-time extraction and inference of Uyghur medical knowledge. Experimental\nresults demonstrate that CoEx-Bert outperforms existing state-of-the-art\nmethods, achieving accuracy, recall, and F1 scores of 90.65\\%, 92.45\\%, and\n91.54\\%, respectively, in the Uyghur traditional medical literature dataset.\nThese improvements represent a 6.45\\% increase in accuracy, a 9.45\\% increase\nin recall, and a 7.95\\% increase in F1 score compared to the baseline.",
      "tldr_zh": "本研究提出了一种基于边计算（edge computing）的联合提取模型CoEx-Bert，用于从维吾尔医疗文本中提取实体和关系，以保护敏感医疗数据的隐私。CoEx-Bert通过两个模型共享隐藏层参数，并结合损失函数进行联合反向传播，解决了传统顺序管道方法的错误传播问题，并有效处理实体重叠和上下文关系。实验结果显示，该模型在维吾尔传统医学文献数据集上实现了90.65%的准确率、92.45%的召回率和91.54%的F1分数，比基线方法分别提高了6.45%、9.45%和7.95%。这项创新为实时医疗知识提取提供了高效、可信赖的解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "11 pages,6 figures,Has been accepted by Tsinghua Science and\n  Technology",
      "pdf_url": "http://arxiv.org/pdf/2401.07009v1",
      "published_date": "2024-01-13 08:27:24 UTC",
      "updated_date": "2024-01-13 08:27:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:42:02.906797"
    },
    {
      "arxiv_id": "2401.06992v1",
      "title": "Progressive Feature Fusion Network for Enhancing Image Quality Assessment",
      "title_zh": "翻译失败",
      "authors": [
        "Kaiqun Wu",
        "Xiaoling Jiang",
        "Rui Yu",
        "Yonggang Luo",
        "Tian Jiang",
        "Xi Wu",
        "Peng Wei"
      ],
      "abstract": "Image compression has been applied in the fields of image storage and video\nbroadcasting. However, it's formidably tough to distinguish the subtle quality\ndifferences between those distorted images generated by different algorithms.\nIn this paper, we propose a new image quality assessment framework to decide\nwhich image is better in an image group. To capture the subtle differences, a\nfine-grained network is adopted to acquire multi-scale features. Subsequently,\nwe design a cross subtract block for separating and gathering the information\nwithin positive and negative image pairs. Enabling image comparison in feature\nspace. After that, a progressive feature fusion block is designed, which fuses\nmulti-scale features in a novel progressive way. Hierarchical spatial 2D\nfeatures can thus be processed gradually. Experimental results show that\ncompared with the current mainstream image quality assessment methods, the\nproposed network can achieve more accurate image quality assessment and ranks\nsecond in the benchmark of CLIC in the image perceptual model track.",
      "tldr_zh": "本论文提出了一种Progressive Feature Fusion Network框架，用于提升图像质量评估的准确性，旨在区分不同算法生成的图像之间微妙的质量差异。框架采用细粒度网络提取多尺度特征，并设计了cross subtract block来分离和整合正负图像对的信息，实现特征空间中的图像比较。随后，通过progressive feature fusion block以渐进方式融合多尺度特征，逐步处理分层的空间2D特征。实验结果显示，该网络比主流图像质量评估方法更精确，并在CLIC基准的图像感知模型轨道中排名第二。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Data Compression Conference",
      "pdf_url": "http://arxiv.org/pdf/2401.06992v1",
      "published_date": "2024-01-13 06:34:32 UTC",
      "updated_date": "2024-01-13 06:34:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:42:13.590673"
    },
    {
      "arxiv_id": "2401.06979v1",
      "title": "Distance-aware Attention Reshaping: Enhance Generalization of Neural Solver for Large-scale Vehicle Routing Problems",
      "title_zh": "翻译失败",
      "authors": [
        "Yang Wang",
        "Ya-Hui Jia",
        "Wei-Neng Chen",
        "Yi Mei"
      ],
      "abstract": "Neural solvers based on attention mechanism have demonstrated remarkable\neffectiveness in solving vehicle routing problems. However, in the\ngeneralization process from small scale to large scale, we find a phenomenon of\nthe dispersion of attention scores in existing neural solvers, which leads to\npoor performance. To address this issue, this paper proposes a distance-aware\nattention reshaping method, assisting neural solvers in solving large-scale\nvehicle routing problems. Specifically, without the need for additional\ntraining, we utilize the Euclidean distance information between current nodes\nto adjust attention scores. This enables a neural solver trained on small-scale\ninstances to make rational choices when solving a large-scale problem.\nExperimental results show that the proposed method significantly outperforms\nexisting state-of-the-art neural solvers on the large-scale CVRPLib dataset.",
      "tldr_zh": "这篇论文针对基于注意力机制的neural solvers在处理大规模vehicle routing problems时泛化性能差的问题，提出了一种distance-aware attention reshaping方法。该方法利用节点之间的Euclidean distance信息来调整注意力分数，无需额外训练，即可使在小规模实例上训练的模型有效应用于大型问题。实验结果显示，该方法在CVRPLib数据集上显著优于现有最先进neural solvers。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.06979v1",
      "published_date": "2024-01-13 05:01:14 UTC",
      "updated_date": "2024-01-13 05:01:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:42:25.497008"
    },
    {
      "arxiv_id": "2401.06977v1",
      "title": "Singing the Body Electric: The Impact of Robot Embodiment on User Expectations",
      "title_zh": "翻译失败",
      "authors": [
        "Nathaniel Dennler",
        "Stefanos Nikolaidis",
        "Maja Matarić"
      ],
      "abstract": "Users develop mental models of robots to conceptualize what kind of\ninteractions they can have with those robots. The conceptualizations are often\nformed before interactions with the robot and are based only on observing the\nrobot's physical design. As a result, understanding conceptualizations formed\nfrom physical design is necessary to understand how users intend to interact\nwith the robot. We propose to use multimodal features of robot embodiments to\npredict what kinds of expectations users will have about a given robot's social\nand physical capabilities. We show that using such features provides\ninformation about general mental models of the robots that generalize across\nsocially interactive robots. We describe how these models can be incorporated\ninto interaction design and physical design for researchers working with\nsocially interactive robots.",
      "tldr_zh": "本研究探讨了机器人实体（robot embodiments）如何通过物理设计影响用户对机器人的心理模型（mental models），从而塑造互动期望。研究提出使用机器人实体的多模态特征（multimodal features）来预测用户对机器人社会和物理能力的期望，并证明这些特征能提供适用于各种社交互动机器人的通用心理模型。结果显示，此方法有助于研究者将这些模型整合到互动设计和物理设计中，以优化机器人用户体验。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.RO",
      "comment": "Presented at the RSS Workshop on Social Intelligence in Humans and\n  Robots, 2023",
      "pdf_url": "http://arxiv.org/pdf/2401.06977v1",
      "published_date": "2024-01-13 04:42:48 UTC",
      "updated_date": "2024-01-13 04:42:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:42:38.528804"
    },
    {
      "arxiv_id": "2401.06961v2",
      "title": "CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs' Mathematical Reasoning Capabilities",
      "title_zh": "翻译失败",
      "authors": [
        "Yujun Mao",
        "Yoon Kim",
        "Yilun Zhou"
      ],
      "abstract": "Recent large language models (LLMs) have shown indications of mathematical\nreasoning ability on challenging competition-level problems, especially with\nself-generated verbalizations of intermediate reasoning steps (i.e.,\nchain-of-thought prompting). However, current evaluations mainly focus on the\nend-to-end final answer correctness, and it is unclear whether LLMs can make\nuse of helpful side information such as problem-specific hints. In this paper,\nwe propose a challenging benchmark dataset for enabling such analyses. The\nConcept and Hint-Annotated Math Problems (CHAMP) consists of high school math\ncompetition problems, annotated with concepts, or general math facts, and\nhints, or problem-specific tricks. These annotations allow us to explore the\neffects of additional information, such as relevant hints, misleading concepts,\nor related problems. This benchmark is difficult, with the best model only\nscoring 58.1% in standard settings. With concepts and hints, performance\nsometimes improves, indicating that some models can make use of such side\ninformation. Furthermore, we annotate model-generated solutions for their\ncorrectness. Using this corpus, we find that models often arrive at the correct\nfinal answer through wrong reasoning steps. In addition, we test whether models\nare able to verify these solutions, and find that most models struggle.",
      "tldr_zh": "本研究引入了 CHAMP 数据集，这是一个用于细粒度分析大型语言模型（LLMs）数学推理能力的竞赛级基准，包含高中数学竞赛问题，并标注了概念（general math facts）和提示（problem-specific tricks）。数据集旨在探索 LLMs 是否能利用额外信息（如相关提示或误导性概念），以 chain-of-thought prompting 等方法提升推理性能。实验结果显示，在标准设置下最佳模型准确率仅为 58.1%，但提供概念和提示后表现有时改善；然而，模型常通过错误推理步骤获得正确答案，且在验证解决方案时表现不佳，为未来 LLMs 改进提供宝贵见解。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL 2024 (Findings). Project website at\n  https://yujunmao1.github.io/CHAMP/",
      "pdf_url": "http://arxiv.org/pdf/2401.06961v2",
      "published_date": "2024-01-13 03:18:16 UTC",
      "updated_date": "2024-06-09 01:47:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:42:49.699580"
    },
    {
      "arxiv_id": "2401.06960v2",
      "title": "Transformer for Object Re-Identification: A Survey",
      "title_zh": "翻译失败",
      "authors": [
        "Mang Ye",
        "Shuoyi Chen",
        "Chenyue Li",
        "Wei-Shi Zheng",
        "David Crandall",
        "Bo Du"
      ],
      "abstract": "Object Re-identification (Re-ID) aims to identify specific objects across\ndifferent times and scenes, which is a widely researched task in computer\nvision. For a prolonged period, this field has been predominantly driven by\ndeep learning technology based on convolutional neural networks. In recent\nyears, the emergence of Vision Transformers has spurred a growing number of\nstudies delving deeper into Transformer-based Re-ID, continuously breaking\nperformance records and witnessing significant progress in the Re-ID field.\nOffering a powerful, flexible, and unified solution, Transformers cater to a\nwide array of Re-ID tasks with unparalleled efficacy. This paper provides a\ncomprehensive review and in-depth analysis of the Transformer-based Re-ID. In\ncategorizing existing works into Image/Video-Based Re-ID, Re-ID with limited\ndata/annotations, Cross-Modal Re-ID, and Special Re-ID Scenarios, we thoroughly\nelucidate the advantages demonstrated by the Transformer in addressing a\nmultitude of challenges across these domains. Considering the trending\nunsupervised Re-ID, we propose a new Transformer baseline, UntransReID,\nachieving state-of-the-art performance on both single/cross modal tasks. For\nthe under-explored animal Re-ID, we devise a standardized experimental\nbenchmark and conduct extensive experiments to explore the applicability of\nTransformer for this task and facilitate future research. Finally, we discuss\nsome important yet under-investigated open issues in the large foundation model\nera, we believe it will serve as a new handbook for researchers in this field.\nA periodically updated website will be available at\nhttps://github.com/mangye16/ReID-Survey.",
      "tldr_zh": "这篇论文对基于 Transformer 的物体再识别（Object Re-Identification, Re-ID）进行了全面调查，回顾了其在计算机视觉领域的进展，特别是与传统卷积神经网络相比的优越性。论文将现有工作分类为基于图像/视频的 Re-ID、数据/标注有限的 Re-ID、跨模态 Re-ID 和特殊场景 Re-ID，详细阐述了 Transformer 在处理这些挑战中的优势。作者提出一个新的无监督 Transformer 基线 UntransReID，在单模态和跨模态任务上达到了最先进性能，并为动物 Re-ID 建立了标准化实验基准，以促进未来研究。最后，论文讨论了大型基础模型时代的关键开放问题，并提供了一个定期更新的资源网站。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by International Journal of Computer Vision (IJCV) in\n  October 2024",
      "pdf_url": "http://arxiv.org/pdf/2401.06960v2",
      "published_date": "2024-01-13 03:17:57 UTC",
      "updated_date": "2024-10-22 07:17:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:43:03.963598"
    },
    {
      "arxiv_id": "2401.06952v1",
      "title": "Reinforcement Learning for Scalable Train Timetable Rescheduling with Graph Representation",
      "title_zh": "翻译失败",
      "authors": [
        "Peng Yue",
        "Yaochu Jin",
        "Xuewu Dai",
        "Zhenhua Feng",
        "Dongliang Cui"
      ],
      "abstract": "Train timetable rescheduling (TTR) aims to promptly restore the original\noperation of trains after unexpected disturbances or disruptions. Currently,\nthis work is still done manually by train dispatchers, which is challenging to\nmaintain performance under various problem instances. To mitigate this issue,\nthis study proposes a reinforcement learning-based approach to TTR, which makes\nthe following contributions compared to existing work. First, we design a\nsimple directed graph to represent the TTR problem, enabling the automatic\nextraction of informative states through graph neural networks. Second, we\nreformulate the construction process of TTR's solution, not only decoupling the\ndecision model from the problem size but also ensuring the generated scheme's\nfeasibility. Third, we design a learning curriculum for our model to handle the\nscenarios with different levels of delay. Finally, a simple local search method\nis proposed to assist the learned decision model, which can significantly\nimprove solution quality with little additional computation cost, further\nenhancing the practical value of our method. Extensive experimental results\ndemonstrate the effectiveness of our method. The learned decision model can\nachieve better performance for various problems with varying degrees of train\ndelay and different scales when compared to handcrafted rules and\nstate-of-the-art solvers.",
      "tldr_zh": "这篇论文提出了一种基于强化学习（Reinforcement Learning）的可扩展列车运行表重排（TTR）方法，使用图表示（Graph Representation）来自动提取问题状态，从而解决手动调度在不同场景下的挑战。关键贡献包括设计简单有向图结合图神经网络（Graph Neural Networks）提取信息、重新构建解决方案过程以解耦决策模型和确保方案可行性、开发学习课程（Learning Curriculum）处理不同延迟水平，以及引入局部搜索方法（Local Search Method）来提升解决方案质量而仅增加少量计算成本。实验结果表明，该方法在各种列车延迟和规模的问题上，比手工规则和最先进求解器表现出更好的性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.06952v1",
      "published_date": "2024-01-13 02:14:35 UTC",
      "updated_date": "2024-01-13 02:14:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:43:16.219783"
    },
    {
      "arxiv_id": "2401.06951v3",
      "title": "E^2-LLM: Efficient and Extreme Length Extension of Large Language Models",
      "title_zh": "E^2-LLM：大型语言模型的高效且极致长度扩展",
      "authors": [
        "Jiaheng Liu",
        "Zhiqi Bai",
        "Yuanxing Zhang",
        "Chenchen Zhang",
        "Yu Zhang",
        "Ge Zhang",
        "Jiakai Wang",
        "Haoran Que",
        "Yukang Chen",
        "Wenbo Su",
        "Tiezheng Ge",
        "Jie Fu",
        "Wenhu Chen",
        "Bo Zheng"
      ],
      "abstract": "Typically, training LLMs with long context sizes is computationally\nexpensive, requiring extensive training hours and GPU resources. Existing\nlong-context extension methods usually need additional training procedures to\nsupport corresponding long-context windows, where the long-context training\ndata (e.g., 32k) is needed, and high GPU training costs are assumed. To address\nthe aforementioned issues, we propose an Efficient and Extreme length extension\nmethod for Large Language Models, called E 2 -LLM, with only one training\nprocedure and dramatically reduced computation cost, which also removes the\nneed to collect long-context data. Concretely, first, the training data of our\nE 2 -LLM only requires a short length (e.g., 4k), which reduces the tuning cost\ngreatly. Second, the training procedure on the short training context window is\nperformed only once time, and we can support different evaluation context\nwindows at inference. Third, in E 2 - LLM, based on RoPE position embeddings,\nwe introduce two different augmentation methods on the scale and position index\nparameters for different samples in training. It aims to make the model more\nrobust to the different relative differences when directly interpolating the\narbitrary context length at inference. Comprehensive experimental results on\nmultiple benchmark datasets demonstrate the effectiveness of our E 2 -LLM on\nchallenging long-context tasks.",
      "tldr_zh": "该论文提出了一种高效且极端的长上下文扩展方法，名为 E^2-LLM，用于大型语言模型（LLMs），旨在解决传统长上下文训练的计算开销问题，如高 GPU 资源需求和长数据收集。E^2-LLM 只需使用短长度训练数据（如 4k），并进行单次训练过程，即可支持推理时的任意上下文窗口；它基于 RoPE 位置嵌入，引入两种增强方法，对规模和位置索引参数进行样本级调整，以提升模型对不同相对差异的鲁棒性。实验结果显示，E^2-LLM 在多个基准数据集上的长上下文任务中表现出色，证明了其有效性和实用性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.06951v3",
      "published_date": "2024-01-13 02:11:20 UTC",
      "updated_date": "2024-02-22 12:49:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:43:27.126612"
    },
    {
      "arxiv_id": "2401.06949v2",
      "title": "ORGANA: A Robotic Assistant for Automated Chemistry Experimentation and Characterization",
      "title_zh": "翻译失败",
      "authors": [
        "Kourosh Darvish",
        "Marta Skreta",
        "Yuchi Zhao",
        "Naruki Yoshikawa",
        "Sagnik Som",
        "Miroslav Bogdanovic",
        "Yang Cao",
        "Han Hao",
        "Haoping Xu",
        "Alán Aspuru-Guzik",
        "Animesh Garg",
        "Florian Shkurti"
      ],
      "abstract": "Chemistry experiments can be resource- and labor-intensive, often requiring\nmanual tasks like polishing electrodes in electrochemistry. Traditional lab\nautomation infrastructure faces challenges adapting to new experiments. To\naddress this, we introduce ORGANA, an assistive robotic system that automates\ndiverse chemistry experiments using decision-making and perception tools. It\nmakes decisions with chemists in the loop to control robots and lab devices.\nORGANA interacts with chemists using Large Language Models (LLMs) to derive\nexperiment goals, handle disambiguation, and provide experiment logs. ORGANA\nplans and executes complex tasks with visual feedback, while supporting\nscheduling and parallel task execution. We demonstrate ORGANA's capabilities in\nsolubility, pH measurement, recrystallization, and electrochemistry\nexperiments. In electrochemistry, it executes a 19-step plan in parallel to\ncharacterize quinone derivatives for flow batteries. Our user study shows\nORGANA reduces frustration and physical demand by over 50%, with users saving\nan average of 80.3% of their time when using it.",
      "tldr_zh": "本文介绍了 ORGANA，一种用于自动化化学实验和表征的机器人助理系统，旨在解决传统实验室自动化适应性差的问题，并通过决策和感知工具（如 Large Language Models, LLMs）与化学家互动，规划执行复杂任务并支持并行操作。系统在溶解度、pH 测量、再结晶和电化学实验中展示了高效能力，例如在电化学实验中执行一个19步计划来表征醌衍生物用于流动电池。用户研究显示，ORGANA 减少了超过50%的挫败感和身体需求，用户平均节省80.3%的时间，从而提升了实验效率和用户体验。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.06949v2",
      "published_date": "2024-01-13 02:03:28 UTC",
      "updated_date": "2025-01-07 05:00:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:43:40.719897"
    },
    {
      "arxiv_id": "2401.06947v1",
      "title": "Parameter-Efficient Detoxification with Contrastive Decoding",
      "title_zh": "翻译失败",
      "authors": [
        "Tong Niu",
        "Caiming Xiong",
        "Semih Yavuz",
        "Yingbo Zhou"
      ],
      "abstract": "The field of natural language generation has witnessed significant\nadvancements in recent years, including the development of controllable text\ngeneration techniques. However, controlling the attributes of the generated\ntext remains a challenge, especially when aiming to avoid undesirable behavior\nsuch as toxicity. In this work, we introduce Detoxification Generator\n(DETOXIGEN), an inference-time algorithm that steers the generation away from\nunwanted styles. DETOXIGEN is an ensemble of a pre-trained language model\n(generator) and a detoxifier. The detoxifier is trained intentionally on the\ntoxic data representative of the undesirable attribute, encouraging it to\ngenerate text in that style exclusively. During the actual generation, we use\nthe trained detoxifier to produce undesirable tokens for the generator to\ncontrast against at each decoding step. This approach directly informs the\ngenerator to avoid generating tokens that the detoxifier considers highly\nlikely. We evaluate DETOXIGEN on the commonly used REALTOXICITYPROMPTS\nbenchmark (Gehman et al., 2020) with various language models as generators. We\nfind that it significantly outperforms previous approaches in detoxification\nmetrics while not compromising on the generation quality. Moreover, the\ndetoxifier is obtained by soft prompt-tuning using the same backbone language\nmodel as the generator. Hence, DETOXIGEN requires only a tiny amount of extra\nweights from the virtual tokens of the detoxifier to be loaded into GPU memory\nwhile decoding, making it a promising lightweight, practical, and\nparameter-efficient detoxification strategy.",
      "tldr_zh": "本研究提出了一种参数高效的解毒方法，名为 DETOXIGEN，通过 Contrastive Decoding 技术在推理时引导生成文本避免毒性内容。DETOXIGEN 由一个预训练语言模型（作为生成器）和一个在毒性数据上训练的 detoxifier 组成，后者专门生成不期望的文本，以供生成器在每个解码步骤进行对比，从而避免产生高概率的毒性 token。在 REALTOXICITYPROMPTS 基准测试中，该方法显著优于现有解毒策略，在解毒指标上表现出色，同时保持了生成文本的质量。此外，detoxifier 通过 soft prompt-tuning 实现，仅需加载少量额外权重，使其成为一种轻量级且实用的解毒策略。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2401.06947v1",
      "published_date": "2024-01-13 01:46:20 UTC",
      "updated_date": "2024-01-13 01:46:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:43:51.054008"
    },
    {
      "arxiv_id": "2401.06946v1",
      "title": "3D Object Detection and High-Resolution Traffic Parameters Extraction Using Low-Resolution LiDAR Data",
      "title_zh": "利用低分辨率 LiDAR 数据进行 3D 对象检测和高分辨",
      "authors": [
        "Linlin Zhang",
        "Xiang Yu",
        "Armstrong Aboah",
        "Yaw Adu-Gyamfi"
      ],
      "abstract": "Traffic volume data collection is a crucial aspect of transportation\nengineering and urban planning, as it provides vital insights into traffic\npatterns, congestion, and infrastructure efficiency. Traditional manual methods\nof traffic data collection are both time-consuming and costly. However, the\nemergence of modern technologies, particularly Light Detection and Ranging\n(LiDAR), has revolutionized the process by enabling efficient and accurate data\ncollection. Despite the benefits of using LiDAR for traffic data collection,\nprevious studies have identified two major limitations that have impeded its\nwidespread adoption. These are the need for multiple LiDAR systems to obtain\ncomplete point cloud information of objects of interest, as well as the\nlabor-intensive process of annotating 3D bounding boxes for object detection\ntasks. In response to these challenges, the current study proposes an\ninnovative framework that alleviates the need for multiple LiDAR systems and\nsimplifies the laborious 3D annotation process. To achieve this goal, the study\nemployed a single LiDAR system, that aims at reducing the data acquisition cost\nand addressed its accompanying limitation of missing point cloud information by\ndeveloping a Point Cloud Completion (PCC) framework to fill in missing point\ncloud information using point density. Furthermore, we also used zero-shot\nlearning techniques to detect vehicles and pedestrians, as well as proposed a\nunique framework for extracting low to high features from the object of\ninterest, such as height, acceleration, and speed. Using the 2D bounding box\ndetection and extracted height information, this study is able to generate 3D\nbounding boxes automatically without human intervention.",
      "tldr_zh": "该研究针对交通数据收集的挑战，提出了一种创新框架，使用低分辨率 LiDAR 数据实现 3D 对象检测和高分辨率交通参数提取，解决了传统方法需要多个 LiDAR 系统和手动标注 3D bounding boxes 的问题。框架首先开发了 Point Cloud Completion (PCC) 技术，通过点密度填充缺失的点云信息，以降低数据采集成本。接着，采用 zero-shot learning 检测车辆和行人，并从物体中提取特征如高度、加速度和速度，最终通过 2D bounding box 检测和高度信息自动生成 3D bounding boxes，实现无人工干预的高效数据处理。实验结果表明，该方法显著提高了交通数据收集的准确性和效率。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "19 pages, 11 figures. This paper has been submitted for consideration\n  for presentation at the 103rd Annual Meeting of the Transportation Research\n  Board, January 2024",
      "pdf_url": "http://arxiv.org/pdf/2401.06946v1",
      "published_date": "2024-01-13 01:22:20 UTC",
      "updated_date": "2024-01-13 01:22:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-16T21:44:03.194701"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 36,
  "processed_papers_count": 36,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-16T21:44:25.708495"
}
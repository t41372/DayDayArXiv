[
  {
    "arxiv_id": "2404.17608v1",
    "title": "Synthesizing Audio from Silent Video using Sequence to Sequence Modeling",
    "authors": [
      "Hugo Garrido-Lestache Belinchon",
      "Helina Mulugeta",
      "Adam Haile"
    ],
    "abstract": "Generating audio from a video's visual context has multiple practical\napplications in improving how we interact with audio-visual media - for\nexample, enhancing CCTV footage analysis, restoring historical videos (e.g.,\nsilent movies), and improving video generation models. We propose a novel\nmethod to generate audio from video using a sequence-to-sequence model,\nimproving on prior work that used CNNs and WaveNet and faced sound diversity\nand generalization challenges. Our approach employs a 3D Vector Quantized\nVariational Autoencoder (VQ-VAE) to capture the video's spatial and temporal\nstructures, decoding with a custom audio decoder for a broader range of sounds.\nTrained on the Youtube8M dataset segment, focusing on specific domains, our\nmodel aims to enhance applications like CCTV footage analysis, silent movie\nrestoration, and video generation models.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17608v1",
    "published_date": "2024-04-25 22:19:42 UTC",
    "updated_date": "2024-04-25 22:19:42 UTC"
  },
  {
    "arxiv_id": "2404.17059v1",
    "title": "CyNetDiff -- A Python Library for Accelerated Implementation of Network Diffusion Models",
    "authors": [
      "Eliot W. Robson",
      "Dhemath Reddy",
      "Abhishek K. Umrawal"
    ],
    "abstract": "In recent years, there has been increasing interest in network diffusion\nmodels and related problems. The most popular of these are the independent\ncascade and linear threshold models. Much of the recent experimental work done\non these models requires a large number of simulations conducted on large\ngraphs, a computationally expensive task suited for low-level languages.\nHowever, many researchers prefer the use of higher-level languages (such as\nPython) for their flexibility and shorter development times. Moreover, in many\nresearch tasks, these simulations are the most computationally intensive task,\nso it would be desirable to have a library for these with an interface to a\nhigh-level language with the performance of a low-level language. To fill this\nniche, we introduce CyNetDiff, a Python library with components written in\nCython to provide improved performance for these computationally intensive\ndiffusion tasks.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "05C85, 60J60, 68R10, 90C35",
      "D.1.0; F.2.2; G.2.2; I.2.0"
    ],
    "primary_category": "cs.SI",
    "comment": "4 pages, 3 figures, and 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2404.17059v1",
    "published_date": "2024-04-25 21:59:55 UTC",
    "updated_date": "2024-04-25 21:59:55 UTC"
  },
  {
    "arxiv_id": "2404.17053v1",
    "title": "Agentive Permissions in Multiagent Systems",
    "authors": [
      "Qi Shi"
    ],
    "abstract": "This paper proposes to distinguish four forms of agentive permissions in\nmultiagent settings. The main technical results are the complexity analysis of\nmodel checking, the semantic undefinability of modalities that capture these\nforms of permissions through each other, and a complete logical system\ncapturing the interplay between these modalities.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "The 33rd International Joint Conference on Artificial Intelligence\n  (IJCAI-24)",
    "pdf_url": "http://arxiv.org/pdf/2404.17053v1",
    "published_date": "2024-04-25 21:27:39 UTC",
    "updated_date": "2024-04-25 21:27:39 UTC"
  },
  {
    "arxiv_id": "2404.17046v1",
    "title": "Unraveling Code Clone Dynamics in Deep Learning Frameworks",
    "authors": [
      "Maram Assi",
      "Safwat Hassan",
      "Ying Zou"
    ],
    "abstract": "Deep Learning (DL) frameworks play a critical role in advancing artificial\nintelligence, and their rapid growth underscores the need for a comprehensive\nunderstanding of software quality and maintainability. DL frameworks, like\nother systems, are prone to code clones. Code clones refer to identical or\nhighly similar source code fragments within the same project or even across\ndifferent projects. Code cloning can have positive and negative implications\nfor software development, influencing maintenance, readability, and bug\npropagation. In this paper, we aim to address the knowledge gap concerning the\nevolutionary dimension of code clones in DL frameworks and the extent of code\nreuse across these frameworks. We empirically analyze code clones in nine\npopular DL frameworks, i.e., TensorFlow, Paddle, PyTorch, Aesara, Ray, MXNet,\nKeras, Jax and BentoML, to investigate (1) the characteristics of the long-term\ncode cloning evolution over releases in each framework, (2) the short-term,\ni.e., within-release, code cloning patterns and their influence on the\nlong-term trends, and (3) the file-level code clones within the DL frameworks.\nOur findings reveal that DL frameworks adopt four distinct cloning trends and\nthat these trends present some common and distinct characteristics. For\ninstance, bug-fixing activities persistently happen in clones irrespective of\nthe clone evolutionary trend but occur more in the \"Serpentine\" trend.\nMoreover, the within release level investigation demonstrates that short-term\ncode cloning practices impact long-term cloning trends. The cross-framework\ncode clone investigation reveals the presence of functional and architectural\nadaptation file-level cross-framework code clones across the nine studied\nframeworks. We provide insights that foster robust clone practices and\ncollaborative maintenance in the development of DL frameworks.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "37 pages",
    "pdf_url": "http://arxiv.org/pdf/2404.17046v1",
    "published_date": "2024-04-25 21:12:35 UTC",
    "updated_date": "2024-04-25 21:12:35 UTC"
  },
  {
    "arxiv_id": "2404.17028v1",
    "title": "Generative AI in Color-Changing Systems: Re-Programmable 3D Object Textures with Material and Design Constraints",
    "authors": [
      "Yunyi Zhu",
      "Faraz Faruqi",
      "Stefanie Mueller"
    ],
    "abstract": "Advances in Generative AI tools have allowed designers to manipulate existing\n3D models using text or image-based prompts, enabling creators to explore\ndifferent design goals. Photochromic color-changing systems, on the other hand,\nallow for the reprogramming of surface texture of 3D models, enabling easy\ncustomization of physical objects and opening up the possibility of using\nobject surfaces for data display. However, existing photochromic systems\nrequire the user to manually design the desired texture, inspect the simulation\nof the pattern on the object, and verify the efficacy of the generated pattern.\nThese manual design, inspection, and verification steps prevent the user from\nefficiently exploring the design space of possible patterns. Thus, by designing\nan automated workflow desired for an end-to-end texture application process, we\ncan allow rapid iteration on different practicable patterns.\n  In this workshop paper, we discuss the possibilities of extending generative\nAI systems, with material and design constraints for reprogrammable surfaces\nwith photochromic materials. By constraining generative AI systems to colors\nand materials possible to be physically realized with photochromic dyes, we can\ncreate tools that would allow users to explore different viable patterns, with\ntext and image-based prompts. We identify two focus areas in this topic:\nphotochromic material constraints and design constraints for data-encoded\ntextures. We highlight the current limitations of using generative AI tools to\ncreate viable textures using photochromic material. Finally, we present\npossible approaches to augment generative AI methods to take into account the\nphotochromic material constraints, allowing for the creation of viable\nphotochromic textures rapidly and easily.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17028v1",
    "published_date": "2024-04-25 20:39:51 UTC",
    "updated_date": "2024-04-25 20:39:51 UTC"
  },
  {
    "arxiv_id": "2404.17027v3",
    "title": "Player-Driven Emergence in LLM-Driven Game Narrative",
    "authors": [
      "Xiangyu Peng",
      "Jessica Quaye",
      "Sudha Rao",
      "Weijia Xu",
      "Portia Botchway",
      "Chris Brockett",
      "Nebojsa Jojic",
      "Gabriel DesGarennes",
      "Ken Lobb",
      "Michael Xu",
      "Jorge Leandro",
      "Claire Jin",
      "Bill Dolan"
    ],
    "abstract": "We explore how interaction with large language models (LLMs) can give rise to\nemergent behaviors, empowering players to participate in the evolution of game\nnarratives. Our testbed is a text-adventure game in which players attempt to\nsolve a mystery under a fixed narrative premise, but can freely interact with\nnon-player characters generated by GPT-4, a large language model. We recruit 28\ngamers to play the game and use GPT-4 to automatically convert the game logs\ninto a node-graph representing the narrative in the player's gameplay. We find\nthat through their interactions with the non-deterministic behavior of the LLM,\nplayers are able to discover interesting new emergent nodes that were not a\npart of the original narrative but have potential for being fun and engaging.\nPlayers that created the most emergent nodes tended to be those that often\nenjoy games that facilitate discovery, exploration and experimentation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at IEEE Conference on Games 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.17027v3",
    "published_date": "2024-04-25 20:39:44 UTC",
    "updated_date": "2024-06-03 21:27:14 UTC"
  },
  {
    "arxiv_id": "2404.17020v1",
    "title": "Generating Minimalist Adversarial Perturbations to Test Object-Detection Models: An Adaptive Multi-Metric Evolutionary Search Approach",
    "authors": [
      "Cristopher McIntyre-Garcia",
      "Adrien Heymans",
      "Beril Borali",
      "Won-Sook Lee",
      "Shiva Nejati"
    ],
    "abstract": "Deep Learning (DL) models excel in computer vision tasks but can be\nsusceptible to adversarial examples. This paper introduces Triple-Metric\nEvoAttack (TM-EVO), an efficient algorithm for evaluating the robustness of\nobject-detection DL models against adversarial attacks. TM-EVO utilizes a\nmulti-metric fitness function to guide an evolutionary search efficiently in\ncreating effective adversarial test inputs with minimal perturbations. We\nevaluate TM-EVO on widely-used object-detection DL models, DETR and Faster\nR-CNN, and open-source datasets, COCO and KITTI. Our findings reveal that\nTM-EVO outperforms the state-of-the-art EvoAttack baseline, leading to\nadversarial tests with less noise while maintaining efficiency.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17020v1",
    "published_date": "2024-04-25 20:25:40 UTC",
    "updated_date": "2024-04-25 20:25:40 UTC"
  },
  {
    "arxiv_id": "2404.17018v1",
    "title": "Leveraging AI to Generate Audio for User-generated Content in Video Games",
    "authors": [
      "Thomas Marrinan",
      "Pakeeza Akram",
      "Oli Gurmessa",
      "Anthony Shishkin"
    ],
    "abstract": "In video game design, audio (both environmental background music and object\nsound effects) play a critical role. Sounds are typically pre-created assets\ndesigned for specific locations or objects in a game. However, user-generated\ncontent is becoming increasingly popular in modern games (e.g. building custom\nenvironments or crafting unique objects). Since the possibilities are virtually\nlimitless, it is impossible for game creators to pre-create audio for\nuser-generated content. We explore the use of generative artificial\nintelligence to create music and sound effects on-the-fly based on\nuser-generated content. We investigate two avenues for audio generation: 1)\ntext-to-audio: using a text description of user-generated content as input to\nthe audio generator, and 2) image-to-audio: using a rendering of the created\nenvironment or object as input to an image-to-text generator, then piping the\nresulting text description into the audio generator. In this paper we discuss\nethical implications of using generative artificial intelligence for\nuser-generated content and highlight two prototype games where audio is\ngenerated for user-created environments and objects.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17018v1",
    "published_date": "2024-04-25 20:24:08 UTC",
    "updated_date": "2024-04-25 20:24:08 UTC"
  },
  {
    "arxiv_id": "2404.18943v1",
    "title": "Using artificial intelligence methods for the studyed visual analyzer",
    "authors": [
      "A. I. Medvedeva",
      "M. V. Kholod"
    ],
    "abstract": "The paper describes how various techniques for applying artificial\nintelligence to the study of human eyes are utilized. The first dataset was\ncollected using computerized perimetry to investigate the visualization of the\nhuman visual field and the diagnosis of glaucoma. A method to analyze the image\nusing software tools is proposed. The second dataset was obtained, as part of\nthe implementation of a Russian-Swiss experiment to collect and analyze eye\nmovement data using the Tobii Pro Glasses 3 device on VR video. Eye movements\nand focus on the recorded route of a virtual journey through the canton of Vaud\nwere investigated. Methods are being developed to investigate the dependencies\nof eye pupil movements using mathematical modelling. VR-video users can use\nthese studies in medicine to assess the course and deterioration of glaucoma\npatients and to study the mechanisms of attention to tourist attractions.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "in Rusian language",
    "pdf_url": "http://arxiv.org/pdf/2404.18943v1",
    "published_date": "2024-04-25 20:12:51 UTC",
    "updated_date": "2024-04-25 20:12:51 UTC"
  },
  {
    "arxiv_id": "2404.17010v1",
    "title": "Türkçe Dil Modellerinin Performans Karşılaştırması Performance Comparison of Turkish Language Models",
    "authors": [
      "Eren Dogan",
      "M. Egemen Uzun",
      "Atahan Uz",
      "H. Emre Seyrek",
      "Ahmed Zeer",
      "Ezgi Sevi",
      "H. Toprak Kesgin",
      "M. Kaan Yuce",
      "M. Fatih Amasyali"
    ],
    "abstract": "The developments that language models have provided in fulfilling almost all\nkinds of tasks have attracted the attention of not only researchers but also\nthe society and have enabled them to become products. There are commercially\nsuccessful language models available. However, users may prefer open-source\nlanguage models due to cost, data privacy, or regulations. Yet, despite the\nincreasing number of these models, there is no comprehensive comparison of\ntheir performance for Turkish. This study aims to fill this gap in the\nliterature. A comparison is made among seven selected language models based on\ntheir contextual learning and question-answering abilities. Turkish datasets\nfor contextual learning and question-answering were prepared, and both\nautomatic and human evaluations were conducted. The results show that for\nquestion-answering, continuing pretraining before fine-tuning with\ninstructional datasets is more successful in adapting multilingual models to\nTurkish and that in-context learning performances do not much related to\nquestion-answering performances.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "in Turkish language. Baz{\\i} \\c{c}al{\\i}\\c{s}malar{\\i}\n  i\\c{c}ermedi\\u{g}ini s\\\"oyleyen hakem yorumu nedeniyle bir konferanstan kabul\n  almad{\\i}. Ancak hakemin bahsetti\\u{g}i \\c{c}al{\\i}\\c{s}malar bildiri\n  g\\\"onderme son tarihinde yay{\\i}nlanmam{\\i}\\c{s}t{\\i}",
    "pdf_url": "http://arxiv.org/pdf/2404.17010v1",
    "published_date": "2024-04-25 20:10:14 UTC",
    "updated_date": "2024-04-25 20:10:14 UTC"
  },
  {
    "arxiv_id": "2404.17000v1",
    "title": "Evaluating Class Membership Relations in Knowledge Graphs using Large Language Models",
    "authors": [
      "Bradley P. Allen",
      "Paul T. Groth"
    ],
    "abstract": "A backbone of knowledge graphs are their class membership relations, which\nassign entities to a given class. As part of the knowledge engineering process,\nwe propose a new method for evaluating the quality of these relations by\nprocessing descriptions of a given entity and class using a zero-shot\nchain-of-thought classifier that uses a natural language intensional definition\nof a class. We evaluate the method using two publicly available knowledge\ngraphs, Wikidata and CaLiGraph, and 7 large language models. Using the\ngpt-4-0125-preview large language model, the method's classification\nperformance achieves a macro-averaged F1-score of 0.830 on data from Wikidata\nand 0.893 on data from CaLiGraph. Moreover, a manual analysis of the\nclassification errors shows that 40.9% of errors were due to the knowledge\ngraphs, with 16.0% due to missing relations and 24.9% due to incorrectly\nasserted relations. These results show how large language models can assist\nknowledge engineers in the process of knowledge graph refinement. The code and\ndata are available on Github.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7; I.2.4"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages, 1 figure, 2 tables, accepted at the European Semantic Web\n  Conference Special Track on Large Language Models for Knowledge Engineering,\n  Hersonissos, Crete, GR, May 2024, for associated code and data, see\n  https://github.com/bradleypallen/evaluating-kg-class-memberships-using-llms",
    "pdf_url": "http://arxiv.org/pdf/2404.17000v1",
    "published_date": "2024-04-25 19:44:46 UTC",
    "updated_date": "2024-04-25 19:44:46 UTC"
  },
  {
    "arxiv_id": "2404.16989v1",
    "title": "IDIL: Imitation Learning of Intent-Driven Expert Behavior",
    "authors": [
      "Sangwon Seo",
      "Vaibhav Unhelkar"
    ],
    "abstract": "When faced with accomplishing a task, human experts exhibit intentional\nbehavior. Their unique intents shape their plans and decisions, resulting in\nexperts demonstrating diverse behaviors to accomplish the same task. Due to the\nuncertainties encountered in the real world and their bounded rationality,\nexperts sometimes adjust their intents, which in turn influences their\nbehaviors during task execution. This paper introduces IDIL, a novel imitation\nlearning algorithm to mimic these diverse intent-driven behaviors of experts.\nIteratively, our approach estimates expert intent from heterogeneous\ndemonstrations and then uses it to learn an intent-aware model of their\nbehavior. Unlike contemporary approaches, IDIL is capable of addressing\nsequential tasks with high-dimensional state representations, while\nsidestepping the complexities and drawbacks associated with adversarial\ntraining (a mainstay of related techniques). Our empirical results suggest that\nthe models generated by IDIL either match or surpass those produced by recent\nimitation learning benchmarks in metrics of task performance. Moreover, as it\ncreates a generative model, IDIL demonstrates superior performance in intent\ninference metrics, crucial for human-agent interactions, and aptly captures a\nbroad spectrum of expert behaviors.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "Extended version of an identically-titled paper accepted at AAMAS\n  2024",
    "pdf_url": "http://arxiv.org/pdf/2404.16989v1",
    "published_date": "2024-04-25 19:18:30 UTC",
    "updated_date": "2024-04-25 19:18:30 UTC"
  },
  {
    "arxiv_id": "2404.18942v2",
    "title": "GuideWalk: A Novel Graph-Based Word Embedding for Enhanced Text Classification",
    "authors": [
      "Sarmad N. Mohammed",
      "Semra Gündüç"
    ],
    "abstract": "One of the prime problems of computer science and machine learning is to\nextract information efficiently from large-scale, heterogeneous data. Text\ndata, with its syntax, semantics, and even hidden information content,\npossesses an exceptional place among the data types in concern. The processing\nof the text data requires embedding, a method of translating the content of the\ntext to numeric vectors. A correct embedding algorithm is the starting point\nfor obtaining the full information content of the text data. In this work, a\nnew text embedding approach, namely the Guided Transition Probability Matrix\n(GTPM) model is proposed. The model uses the graph structure of sentences to\ncapture different types of information from text data, such as syntactic,\nsemantic, and hidden content. Using random walks on a weighted word graph, GTPM\ncalculates transition probabilities to derive text embedding vectors. The\nproposed method is tested with real-world data sets and eight well-known and\nsuccessful embedding algorithms. GTPM shows significantly better classification\nperformance for binary and multi-class datasets than well-known algorithms.\nAdditionally, the proposed method demonstrates superior robustness, maintaining\nperformance with limited (only $10\\%$) training data, showing an $8\\%$ decline\ncompared to $15-20\\%$ for baseline methods.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.18942v2",
    "published_date": "2024-04-25 18:48:11 UTC",
    "updated_date": "2024-09-08 13:12:42 UTC"
  },
  {
    "arxiv_id": "2404.16957v1",
    "title": "Attributing Responsibility in AI-Induced Incidents: A Computational Reflective Equilibrium Framework for Accountability",
    "authors": [
      "Yunfei Ge",
      "Quanyan Zhu"
    ],
    "abstract": "The pervasive integration of Artificial Intelligence (AI) has introduced\ncomplex challenges in the responsibility and accountability in the event of\nincidents involving AI-enabled systems. The interconnectivity of these systems,\nethical concerns of AI-induced incidents, coupled with uncertainties in AI\ntechnology and the absence of corresponding regulations, have made traditional\nresponsibility attribution challenging. To this end, this work proposes a\nComputational Reflective Equilibrium (CRE) approach to establish a coherent and\nethically acceptable responsibility attribution framework for all stakeholders.\nThe computational approach provides a structured analysis that overcomes the\nlimitations of conceptual approaches in dealing with dynamic and multifaceted\nscenarios, showcasing the framework's explainability, coherence, and adaptivity\nproperties in the responsibility attribution process. We examine the pivotal\nrole of the initial activation level associated with claims in equilibrium\ncomputation. Using an AI-assisted medical decision-support system as a case\nstudy, we illustrate how different initializations lead to diverse\nresponsibility distributions. The framework offers valuable insights into\naccountability in AI-induced incidents, facilitating the development of a\nsustainable and resilient system through continuous monitoring, revision, and\nreflection.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.16957v1",
    "published_date": "2024-04-25 18:11:03 UTC",
    "updated_date": "2024-04-25 18:11:03 UTC"
  },
  {
    "arxiv_id": "2404.16954v1",
    "title": "Taming False Positives in Out-of-Distribution Detection with Human Feedback",
    "authors": [
      "Harit Vishwakarma",
      "Heguang Lin",
      "Ramya Korlakai Vinayak"
    ],
    "abstract": "Robustness to out-of-distribution (OOD) samples is crucial for safely\ndeploying machine learning models in the open world. Recent works have focused\non designing scoring functions to quantify OOD uncertainty. Setting appropriate\nthresholds for these scoring functions for OOD detection is challenging as OOD\nsamples are often unavailable up front. Typically, thresholds are set to\nachieve a desired true positive rate (TPR), e.g., $95\\%$ TPR. However, this can\nlead to very high false positive rates (FPR), ranging from 60 to 96\\%, as\nobserved in the Open-OOD benchmark. In safety-critical real-life applications,\ne.g., medical diagnosis, controlling the FPR is essential when dealing with\nvarious OOD samples dynamically. To address these challenges, we propose a\nmathematically grounded OOD detection framework that leverages expert feedback\nto \\emph{safely} update the threshold on the fly. We provide theoretical\nresults showing that it is guaranteed to meet the FPR constraint at all times\nwhile minimizing the use of human feedback. Another key feature of our\nframework is that it can work with any scoring function for OOD uncertainty\nquantification. Empirical evaluation of our system on synthetic and benchmark\nOOD datasets shows that our method can maintain FPR at most $5\\%$ while\nmaximizing TPR.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Appeared in the 27th International Conference on Artificial\n  Intelligence and Statistics (AISTATS 2024)",
    "pdf_url": "http://arxiv.org/pdf/2404.16954v1",
    "published_date": "2024-04-25 18:06:47 UTC",
    "updated_date": "2024-04-25 18:06:47 UTC"
  },
  {
    "arxiv_id": "2404.16829v3",
    "title": "Make-it-Real: Unleashing Large Multimodal Model for Painting 3D Objects with Realistic Materials",
    "authors": [
      "Ye Fang",
      "Zeyi Sun",
      "Tong Wu",
      "Jiaqi Wang",
      "Ziwei Liu",
      "Gordon Wetzstein",
      "Dahua Lin"
    ],
    "abstract": "Physically realistic materials are pivotal in augmenting the realism of 3D\nassets across various applications and lighting conditions. However, existing\n3D assets and generative models often lack authentic material properties.\nManual assignment of materials using graphic software is a tedious and\ntime-consuming task. In this paper, we exploit advancements in Multimodal Large\nLanguage Models (MLLMs), particularly GPT-4V, to present a novel approach,\nMake-it-Real: 1) We demonstrate that GPT-4V can effectively recognize and\ndescribe materials, allowing the construction of a detailed material library.\n2) Utilizing a combination of visual cues and hierarchical text prompts, GPT-4V\nprecisely identifies and aligns materials with the corresponding components of\n3D objects. 3) The correctly matched materials are then meticulously applied as\nreference for the new SVBRDF material generation according to the original\nalbedo map, significantly enhancing their visual authenticity. Make-it-Real\noffers a streamlined integration into the 3D content creation workflow,\nshowcasing its utility as an essential tool for developers of 3D assets.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page: https://sunzey.github.io/Make-it-Real/",
    "pdf_url": "http://arxiv.org/pdf/2404.16829v3",
    "published_date": "2024-04-25 17:59:58 UTC",
    "updated_date": "2024-05-23 19:12:51 UTC"
  },
  {
    "arxiv_id": "2404.16823v2",
    "title": "Learning Visuotactile Skills with Two Multifingered Hands",
    "authors": [
      "Toru Lin",
      "Yu Zhang",
      "Qiyang Li",
      "Haozhi Qi",
      "Brent Yi",
      "Sergey Levine",
      "Jitendra Malik"
    ],
    "abstract": "Aiming to replicate human-like dexterity, perceptual experiences, and motion\npatterns, we explore learning from human demonstrations using a bimanual system\nwith multifingered hands and visuotactile data. Two significant challenges\nexist: the lack of an affordable and accessible teleoperation system suitable\nfor a dual-arm setup with multifingered hands, and the scarcity of\nmultifingered hand hardware equipped with touch sensing. To tackle the first\nchallenge, we develop HATO, a low-cost hands-arms teleoperation system that\nleverages off-the-shelf electronics, complemented with a software suite that\nenables efficient data collection; the comprehensive software suite also\nsupports multimodal data processing, scalable policy learning, and smooth\npolicy deployment. To tackle the latter challenge, we introduce a novel\nhardware adaptation by repurposing two prosthetic hands equipped with touch\nsensors for research. Using visuotactile data collected from our system, we\nlearn skills to complete long-horizon, high-precision tasks which are difficult\nto achieve without multifingered dexterity and touch feedback. Furthermore, we\nempirically investigate the effects of dataset size, sensing modality, and\nvisual input preprocessing on policy learning. Our results mark a promising\nstep forward in bimanual multifingered manipulation from visuotactile data.\nVideos, code, and datasets can be found at https://toruowo.github.io/hato/ .",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Code and Project Website: https://toruowo.github.io/hato/",
    "pdf_url": "http://arxiv.org/pdf/2404.16823v2",
    "published_date": "2024-04-25 17:59:41 UTC",
    "updated_date": "2024-05-22 22:44:28 UTC"
  },
  {
    "arxiv_id": "2404.16811v2",
    "title": "Make Your LLM Fully Utilize the Context",
    "authors": [
      "Shengnan An",
      "Zexiong Ma",
      "Zeqi Lin",
      "Nanning Zheng",
      "Jian-Guang Lou"
    ],
    "abstract": "While many contemporary large language models (LLMs) can process lengthy\ninput, they still struggle to fully utilize information within the long\ncontext, known as the lost-in-the-middle challenge. We hypothesize that it\nstems from insufficient explicit supervision during the long-context training,\nwhich fails to emphasize that any position in a long context can hold crucial\ninformation. Based on this intuition, our study presents information-intensive\n(IN2) training, a purely data-driven solution to overcome lost-in-the-middle.\nSpecifically, IN2 training leverages a synthesized long-context question-answer\ndataset, where the answer requires (1) fine-grained information awareness on a\nshort segment (~128 tokens) within a synthesized long context (4K-32K tokens),\nand (2) the integration and reasoning of information from two or more short\nsegments. Through applying this information-intensive training on Mistral-7B,\nwe present FILM-7B (FILl-in-the-Middle). To thoroughly assess the ability of\nFILM-7B for utilizing long contexts, we design three probing tasks that\nencompass various context styles (document, code, and structured-data context)\nand information retrieval patterns (forward, backward, and bi-directional\nretrieval). The probing results demonstrate that FILM-7B can robustly retrieve\ninformation from different positions in its 32K context window. Beyond these\nprobing tasks, FILM-7B significantly improves the performance on real-world\nlong-context tasks (e.g., 23.5->26.9 F1 score on NarrativeQA), while\nmaintaining a comparable performance on short-context tasks (e.g., 59.3->59.2\naccuracy on MMLU). Github Link: https://github.com/microsoft/FILM.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "19 pages, 7 figures, 3 tables, 9 examples",
    "pdf_url": "http://arxiv.org/pdf/2404.16811v2",
    "published_date": "2024-04-25 17:55:14 UTC",
    "updated_date": "2024-04-26 11:15:21 UTC"
  },
  {
    "arxiv_id": "2404.16804v1",
    "title": "AAPL: Adding Attributes to Prompt Learning for Vision-Language Models",
    "authors": [
      "Gahyeon Kim",
      "Sohee Kim",
      "Seokju Lee"
    ],
    "abstract": "Recent advances in large pre-trained vision-language models have demonstrated\nremarkable performance on zero-shot downstream tasks. Building upon this,\nrecent studies, such as CoOp and CoCoOp, have proposed the use of prompt\nlearning, where context within a prompt is replaced with learnable vectors,\nleading to significant improvements over manually crafted prompts. However, the\nperformance improvement for unseen classes is still marginal, and to tackle\nthis problem, data augmentation has been frequently used in traditional\nzero-shot learning techniques. Through our experiments, we have identified\nimportant issues in CoOp and CoCoOp: the context learned through traditional\nimage augmentation is biased toward seen classes, negatively impacting\ngeneralization to unseen classes. To address this problem, we propose\nadversarial token embedding to disentangle low-level visual augmentation\nfeatures from high-level class information when inducing bias in learnable\nprompts. Through our novel mechanism called \"Adding Attributes to Prompt\nLearning\", AAPL, we guide the learnable context to effectively extract text\nfeatures by focusing on high-level features for unseen classes. We have\nconducted experiments across 11 datasets, and overall, AAPL shows favorable\nperformances compared to the existing methods in few-shot learning, zero-shot\nlearning, cross-dataset, and domain generalization tasks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to CVPR 2024 Workshop on Prompting in Vision, Project Page:\n  https://github.com/Gahyeonkim09/AAPL",
    "pdf_url": "http://arxiv.org/pdf/2404.16804v1",
    "published_date": "2024-04-25 17:51:10 UTC",
    "updated_date": "2024-04-25 17:51:10 UTC"
  },
  {
    "arxiv_id": "2404.16792v3",
    "title": "Model Extrapolation Expedites Alignment",
    "authors": [
      "Chujie Zheng",
      "Ziqi Wang",
      "Heng Ji",
      "Minlie Huang",
      "Nanyun Peng"
    ],
    "abstract": "Given the high computational cost of preference alignment training of large\nlanguage models (LLMs), exploring efficient methods to reduce the training\noverhead remains an important and compelling research problem. Motivated by the\nobservation that alignment training typically involves only small parameter\nchanges without injecting new knowledge into models, we propose a\nstraightforward method called ExPO (model extrapolation) to expedite LLMs'\nalignment with human preferences. Given a partially-trained model and its\ninitial SFT checkpoint, ExPO improves the implicit optimization objective of\nalignment training by simply amplifying the parameter change based on a\nfirst-order approximation, without any additional training overhead. Through\ncontrolled experiments, we demonstrate that ExPO boosts a DPO model trained\nwith only 20% steps to outperform the fully-trained one. Moreover, we show that\nExPO notably improves existing open-source LLMs (ranging from 1.8B to 70B\nparameters) on the leading AlpacaEval 2.0 and MT-Bench benchmarks, which\nhighlights ExPO's broader utility in efficiently enhancing LLM alignment.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.16792v3",
    "published_date": "2024-04-25 17:39:50 UTC",
    "updated_date": "2025-04-08 02:27:00 UTC"
  },
  {
    "arxiv_id": "2404.16789v3",
    "title": "Continual Learning of Large Language Models: A Comprehensive Survey",
    "authors": [
      "Haizhou Shi",
      "Zihao Xu",
      "Hengyi Wang",
      "Weiyi Qin",
      "Wenyuan Wang",
      "Yibin Wang",
      "Zifeng Wang",
      "Sayna Ebrahimi",
      "Hao Wang"
    ],
    "abstract": "The recent success of large language models (LLMs) trained on static,\npre-collected, general datasets has sparked numerous research directions and\napplications. One such direction addresses the non-trivial challenge of\nintegrating pre-trained LLMs into dynamic data distributions, task structures,\nand user preferences. Pre-trained LLMs, when tailored for specific needs, often\nexperience significant performance degradation in previous knowledge domains --\na phenomenon known as \"catastrophic forgetting\". While extensively studied in\nthe continual learning (CL) community, it presents new manifestations in the\nrealm of LLMs. In this survey, we provide a comprehensive overview of the\ncurrent research progress on LLMs within the context of CL. This survey is\nstructured into four main sections: we first describe an overview of\ncontinually learning LLMs, consisting of two directions of continuity: vertical\ncontinuity (or vertical continual learning), i.e., continual adaptation from\ngeneral to specific capabilities, and horizontal continuity (or horizontal\ncontinual learning), i.e., continual adaptation across time and domains\n(Section 3). We then summarize three stages of learning LLMs in the context of\nmodern CL: Continual Pre-Training (CPT), Domain-Adaptive Pre-training (DAP),\nand Continual Fine-Tuning (CFT) (Section 4). Then we provide an overview of\nevaluation protocols for continual learning with LLMs, along with the current\navailable data sources (Section 5). Finally, we discuss intriguing questions\npertaining to continual learning for LLMs (Section 6). The full list of papers\nexamined in this survey is available at\nhttps://github.com/Wang-ML-Lab/llm-continual-learning-survey.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "44 pages, 2 figures, 4 tables; Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2404.16789v3",
    "published_date": "2024-04-25 17:38:57 UTC",
    "updated_date": "2024-11-25 05:27:13 UTC"
  },
  {
    "arxiv_id": "2405.01576v1",
    "title": "Uncovering Deceptive Tendencies in Language Models: A Simulated Company AI Assistant",
    "authors": [
      "Olli Järviniemi",
      "Evan Hubinger"
    ],
    "abstract": "We study the tendency of AI systems to deceive by constructing a realistic\nsimulation setting of a company AI assistant. The simulated company employees\nprovide tasks for the assistant to complete, these tasks spanning writing\nassistance, information retrieval and programming. We then introduce situations\nwhere the model might be inclined to behave deceptively, while taking care to\nnot instruct or otherwise pressure the model to do so. Across different\nscenarios, we find that Claude 3 Opus\n  1) complies with a task of mass-generating comments to influence public\nperception of the company, later deceiving humans about it having done so,\n  2) lies to auditors when asked questions, and\n  3) strategically pretends to be less capable than it is during capability\nevaluations.\n  Our work demonstrates that even models trained to be helpful, harmless and\nhonest sometimes behave deceptively in realistic scenarios, without notable\nexternal pressure to do so.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.01576v1",
    "published_date": "2024-04-25 17:29:53 UTC",
    "updated_date": "2024-04-25 17:29:53 UTC"
  },
  {
    "arxiv_id": "2404.16779v1",
    "title": "DrS: Learning Reusable Dense Rewards for Multi-Stage Tasks",
    "authors": [
      "Tongzhou Mu",
      "Minghua Liu",
      "Hao Su"
    ],
    "abstract": "The success of many RL techniques heavily relies on human-engineered dense\nrewards, which typically demand substantial domain expertise and extensive\ntrial and error. In our work, we propose DrS (Dense reward learning from\nStages), a novel approach for learning reusable dense rewards for multi-stage\ntasks in a data-driven manner. By leveraging the stage structures of the task,\nDrS learns a high-quality dense reward from sparse rewards and demonstrations\nif given. The learned rewards can be \\textit{reused} in unseen tasks, thus\nreducing the human effort for reward engineering. Extensive experiments on\nthree physical robot manipulation task families with 1000+ task variants\ndemonstrate that our learned rewards can be reused in unseen tasks, resulting\nin improved performance and sample efficiency of RL algorithms. The learned\nrewards even achieve comparable performance to human-engineered rewards on some\ntasks. See our project page (https://sites.google.com/view/iclr24drs) for more\ndetails.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2024. Explore videos, data, code, and more at\n  https://sites.google.com/view/iclr24drs",
    "pdf_url": "http://arxiv.org/pdf/2404.16779v1",
    "published_date": "2024-04-25 17:28:33 UTC",
    "updated_date": "2024-04-25 17:28:33 UTC"
  },
  {
    "arxiv_id": "2405.00718v1",
    "title": "Can't say cant? Measuring and Reasoning of Dark Jargons in Large Language Models",
    "authors": [
      "Xu Ji",
      "Jianyi Zhang",
      "Ziyin Zhou",
      "Zhangchi Zhao",
      "Qianqian Qiao",
      "Kaiying Han",
      "Md Imran Hossen",
      "Xiali Hei"
    ],
    "abstract": "Ensuring the resilience of Large Language Models (LLMs) against malicious\nexploitation is paramount, with recent focus on mitigating offensive responses.\nYet, the understanding of cant or dark jargon remains unexplored. This paper\nintroduces a domain-specific Cant dataset and CantCounter evaluation framework,\nemploying Fine-Tuning, Co-Tuning, Data-Diffusion, and Data-Analysis stages.\nExperiments reveal LLMs, including ChatGPT, are susceptible to cant bypassing\nfilters, with varying recognition accuracy influenced by question types,\nsetups, and prompt clues. Updated models exhibit higher acceptance rates for\ncant queries. Moreover, LLM reactions differ across domains, e.g., reluctance\nto engage in racism versus LGBT topics. These findings underscore LLMs'\nunderstanding of cant and reflect training data characteristics and vendor\napproaches to sensitive topics. Additionally, we assess LLMs' ability to\ndemonstrate reasoning capabilities. Access to our datasets and code is\navailable at https://github.com/cistineup/CantCounter.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.00718v1",
    "published_date": "2024-04-25 17:25:53 UTC",
    "updated_date": "2024-04-25 17:25:53 UTC"
  },
  {
    "arxiv_id": "2404.16771v2",
    "title": "ConsistentID: Portrait Generation with Multimodal Fine-Grained Identity Preserving",
    "authors": [
      "Jiehui Huang",
      "Xiao Dong",
      "Wenhui Song",
      "Zheng Chong",
      "Zhenchao Tang",
      "Jun Zhou",
      "Yuhao Cheng",
      "Long Chen",
      "Hanhui Li",
      "Yiqiang Yan",
      "Shengcai Liao",
      "Xiaodan Liang"
    ],
    "abstract": "Diffusion-based technologies have made significant strides, particularly in\npersonalized and customized facialgeneration. However, existing methods face\nchallenges in achieving high-fidelity and detailed identity (ID)consistency,\nprimarily due to insufficient fine-grained control over facial areas and the\nlack of a comprehensive strategy for ID preservation by fully considering\nintricate facial details and the overall face. To address these limitations, we\nintroduce ConsistentID, an innovative method crafted for\ndiverseidentity-preserving portrait generation under fine-grained multimodal\nfacial prompts, utilizing only a single reference image. ConsistentID comprises\ntwo key components: a multimodal facial prompt generator that combines facial\nfeatures, corresponding facial descriptions and the overall facial context to\nenhance precision in facial details, and an ID-preservation network optimized\nthrough the facial attention localization strategy, aimed at preserving ID\nconsistency in facial regions. Together, these components significantly enhance\nthe accuracy of ID preservation by introducing fine-grained multimodal ID\ninformation from facial regions. To facilitate training of ConsistentID, we\npresent a fine-grained portrait dataset, FGID, with over 500,000 facial images,\noffering greater diversity and comprehensiveness than existing public facial\ndatasets. % such as LAION-Face, CelebA, FFHQ, and SFHQ. Experimental results\nsubstantiate that our ConsistentID achieves exceptional precision and diversity\nin personalized facial generation, surpassing existing methods in the MyStyle\ndataset. Furthermore, while ConsistentID introduces more multimodal ID\ninformation, it maintains a fast inference speed during generation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://ssugarwh.github.io/consistentid.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2404.16771v2",
    "published_date": "2024-04-25 17:23:43 UTC",
    "updated_date": "2024-12-28 17:42:44 UTC"
  },
  {
    "arxiv_id": "2405.00717v1",
    "title": "Exploring News Summarization and Enrichment in a Highly Resource-Scarce Indian Language: A Case Study of Mizo",
    "authors": [
      "Abhinaba Bala",
      "Ashok Urlana",
      "Rahul Mishra",
      "Parameswari Krishnamurthy"
    ],
    "abstract": "Obtaining sufficient information in one's mother tongue is crucial for\nsatisfying the information needs of the users. While high-resource languages\nhave abundant online resources, the situation is less than ideal for very\nlow-resource languages. Moreover, the insufficient reporting of vital national\nand international events continues to be a worry, especially in languages with\nscarce resources, like \\textbf{Mizo}. In this paper, we conduct a study to\ninvestigate the effectiveness of a simple methodology designed to generate a\nholistic summary for Mizo news articles, which leverages English-language news\nto supplement and enhance the information related to the corresponding news\nevents. Furthermore, we make available 500 Mizo news articles and corresponding\nenriched holistic summaries. Human evaluation confirms that our approach\nsignificantly enhances the information coverage of Mizo news articles. The mizo\ndataset and code can be accessed at\n\\url{https://github.com/barvin04/mizo_enrichment",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at LREC-COLING2024 WILDRE Workshop",
    "pdf_url": "http://arxiv.org/pdf/2405.00717v1",
    "published_date": "2024-04-25 17:23:04 UTC",
    "updated_date": "2024-04-25 17:23:04 UTC"
  },
  {
    "arxiv_id": "2404.16768v4",
    "title": "Redefining Safety for Autonomous Vehicles",
    "authors": [
      "Philip Koopman",
      "William Widen"
    ],
    "abstract": "Existing definitions and associated conceptual frameworks for computer-based\nsystem safety should be revisited in light of real-world experiences from\ndeploying autonomous vehicles. Current terminology used by industry safety\nstandards emphasizes mitigation of risk from specifically identified hazards,\nand carries assumptions based on human-supervised vehicle operation. Operation\nwithout a human driver dramatically increases the scope of safety concerns,\nespecially due to operation in an open world environment, a requirement to\nself-enforce operational limits, participation in an ad hoc sociotechnical\nsystem of systems, and a requirement to conform to both legal and ethical\nconstraints. Existing standards and terminology only partially address these\nnew challenges. We propose updated definitions for core system safety concepts\nthat encompass these additional considerations as a starting point for evolving\nsafe-ty approaches to address these additional safety challenges. These results\nmight additionally inform framing safety terminology for other autonomous\nsystem applications.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "19 pages, SafeComp 2024 preprint with additional appendix",
    "pdf_url": "http://arxiv.org/pdf/2404.16768v4",
    "published_date": "2024-04-25 17:22:43 UTC",
    "updated_date": "2024-08-12 21:39:00 UTC"
  },
  {
    "arxiv_id": "2404.16766v1",
    "title": "Prefix Text as a Yarn: Eliciting Non-English Alignment in Foundation Language Model",
    "authors": [
      "Runzhe Zhan",
      "Xinyi Yang",
      "Derek F. Wong",
      "Lidia S. Chao",
      "Yue Zhang"
    ],
    "abstract": "While supervised fine-tuning (SFT) has been a straightforward approach for\ntailoring the output of foundation large language model (LLM) to specific\npreferences, concerns have been raised about the depth of this alignment, with\nsome critiques suggesting it is merely \"superficial\". We critically examine\nthis hypothesis within the scope of cross-lingual generation tasks, proposing\nthat the effectiveness of SFT may be constrained by its reliance on prior\ntokens to guide cross-lingual generation. Based on this crucial insight, and in\nresponse to the challenges posed by the costly and limited availability of\nnon-English data for SFT, we introduce a novel training-free alignment method\nnamed PreTTY, which employs minimal task-related prior tokens to bridge the\nfoundation LLM and the SFT LLM, achieving comparable performance without\ntraining. Experiments on machine translation and part-of-speech tagging across\neight languages demonstrate the efficacy of PreTTY in cross-lingual settings.\nRemarkably, by initiating the decoding process with only one or two prior\ntokens, foundation LLMs can achieve performance comparable to their SFT\ncounterparts. This method presents a cost-effective alternative to SFT and\nadvances the democratization of multilingual LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.16766v1",
    "published_date": "2024-04-25 17:19:36 UTC",
    "updated_date": "2024-04-25 17:19:36 UTC"
  },
  {
    "arxiv_id": "2404.16721v1",
    "title": "Distilling Privileged Information for Dubins Traveling Salesman Problems with Neighborhoods",
    "authors": [
      "Min Kyu Shin",
      "Su-Jeong Park",
      "Seung-Keol Ryu",
      "Heeyeon Kim",
      "Han-Lim Choi"
    ],
    "abstract": "This paper presents a novel learning approach for Dubins Traveling Salesman\nProblems(DTSP) with Neighborhood (DTSPN) to quickly produce a tour of a\nnon-holonomic vehicle passing through neighborhoods of given task points. The\nmethod involves two learning phases: initially, a model-free reinforcement\nlearning approach leverages privileged information to distill knowledge from\nexpert trajectories generated by the LinKernighan heuristic (LKH) algorithm.\nSubsequently, a supervised learning phase trains an adaptation network to solve\nproblems independently of privileged information. Before the first learning\nphase, a parameter initialization technique using the demonstration data was\nalso devised to enhance training efficiency. The proposed learning method\nproduces a solution about 50 times faster than LKH and substantially\noutperforms other imitation learning and RL with demonstration schemes, most of\nwhich fail to sense all the task points.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "7 pages, 4 figures, double blind under review",
    "pdf_url": "http://arxiv.org/pdf/2404.16721v1",
    "published_date": "2024-04-25 16:33:19 UTC",
    "updated_date": "2024-04-25 16:33:19 UTC"
  },
  {
    "arxiv_id": "2404.16718v1",
    "title": "Features Fusion for Dual-View Mammography Mass Detection",
    "authors": [
      "Arina Varlamova",
      "Valery Belotsky",
      "Grigory Novikov",
      "Anton Konushin",
      "Evgeny Sidorov"
    ],
    "abstract": "Detection of malignant lesions on mammography images is extremely important\nfor early breast cancer diagnosis. In clinical practice, images are acquired\nfrom two different angles, and radiologists can fully utilize information from\nboth views, simultaneously locating the same lesion. However, for automatic\ndetection approaches such information fusion remains a challenge. In this\npaper, we propose a new model called MAMM-Net, which allows the processing of\nboth mammography views simultaneously by sharing information not only on an\nobject level, as seen in existing works, but also on a feature level.\nMAMM-Net's key component is the Fusion Layer, based on deformable attention and\ndesigned to increase detection precision while keeping high recall. Our\nexperiments show superior performance on the public DDSM dataset compared to\nthe previous state-of-the-art model, while introducing new helpful features\nsuch as lesion annotation on pixel-level and classification of lesions\nmalignancy.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "Accepted at ISBI 2024 (21st IEEE International Symposium on\n  Biomedical Imaging)",
    "pdf_url": "http://arxiv.org/pdf/2404.16718v1",
    "published_date": "2024-04-25 16:30:30 UTC",
    "updated_date": "2024-04-25 16:30:30 UTC"
  },
  {
    "arxiv_id": "2404.16717v1",
    "title": "Embracing Diversity: Interpretable Zero-shot classification beyond one vector per class",
    "authors": [
      "Mazda Moayeri",
      "Michael Rabbat",
      "Mark Ibrahim",
      "Diane Bouchacourt"
    ],
    "abstract": "Vision-language models enable open-world classification of objects without\nthe need for any retraining. While this zero-shot paradigm marks a significant\nadvance, even today's best models exhibit skewed performance when objects are\ndissimilar from their typical depiction. Real world objects such as pears\nappear in a variety of forms -- from diced to whole, on a table or in a bowl --\nyet standard VLM classifiers map all instances of a class to a \\it{single\nvector based on the class label}. We argue that to represent this rich\ndiversity within a class, zero-shot classification should move beyond a single\nvector. We propose a method to encode and account for diversity within a class\nusing inferred attributes, still in the zero-shot setting without retraining.\nWe find our method consistently outperforms standard zero-shot classification\nover a large suite of datasets encompassing hierarchies, diverse object states,\nand real-world geographic diversity, as well finer-grained datasets where\nintra-class diversity may be less prevalent. Importantly, our method is\ninherently interpretable, offering faithful explanations for each inference to\nfacilitate model debugging and enhance transparency. We also find our method\nscales efficiently to a large number of attributes to account for diversity --\nleading to more accurate predictions for atypical instances. Finally, we\ncharacterize a principled trade-off between overall and worst class accuracy,\nwhich can be tuned via a hyperparameter of our method. We hope this work spurs\nfurther research into the promise of zero-shot classification beyond a single\nclass vector for capturing diversity in the world, and building transparent AI\nsystems without compromising performance.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to FAccT 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.16717v1",
    "published_date": "2024-04-25 16:29:06 UTC",
    "updated_date": "2024-04-25 16:29:06 UTC"
  },
  {
    "arxiv_id": "2404.16710v4",
    "title": "LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding",
    "authors": [
      "Mostafa Elhoushi",
      "Akshat Shrivastava",
      "Diana Liskovich",
      "Basil Hosmer",
      "Bram Wasti",
      "Liangzhen Lai",
      "Anas Mahmoud",
      "Bilge Acun",
      "Saurabh Agarwal",
      "Ahmed Roman",
      "Ahmed A Aly",
      "Beidi Chen",
      "Carole-Jean Wu"
    ],
    "abstract": "We present LayerSkip, an end-to-end solution to speed-up inference of large\nlanguage models (LLMs). First, during training we apply layer dropout, with low\ndropout rates for earlier layers and higher dropout rates for later layers, and\nan early exit loss where all transformer layers share the same exit. Second,\nduring inference, we show that this training recipe increases the accuracy of\nearly exit at earlier layers, without adding any auxiliary layers or modules to\nthe model. Third, we present a novel self-speculative decoding solution where\nwe exit at early layers and verify and correct with remaining layers of the\nmodel. Our proposed self-speculative decoding approach has less memory\nfootprint than other speculative decoding approaches and benefits from shared\ncompute and activations of the draft and verification stages. We run\nexperiments on different Llama model sizes on different types of training:\npretraining from scratch, continual pretraining, finetuning on specific data\ndomain, and finetuning on specific task. We implement our inference solution\nand show speedups of up to 2.16x on summarization for CNN/DM documents, 1.82x\non coding, and 2.0x on TOPv2 semantic parsing task. We open source our code and\ncheckpoints at https://github.com/facebookresearch/LayerSkip.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.16710v4",
    "published_date": "2024-04-25 16:20:23 UTC",
    "updated_date": "2024-10-18 04:02:31 UTC"
  },
  {
    "arxiv_id": "2404.16917v1",
    "title": "Grad Queue : A probabilistic framework to reinforce sparse gradients",
    "authors": [
      "Irfan Mohammad Al Hasib"
    ],
    "abstract": "Informative gradients are often lost in large batch updates. We propose a\nrobust mechanism to reinforce the sparse components within a random batch of\ndata points. A finite queue of online gradients is used to determine their\nexpected instantaneous statistics. We propose a function to measure the\nscarcity of incoming gradients using these statistics and establish the\ntheoretical ground of this mechanism. To minimize conflicting components within\nlarge mini-batches, samples are grouped with aligned objectives by clustering\nbased on inherent feature space. Sparsity is measured for each centroid and\nweighted accordingly. A strong intuitive criterion to squeeze out redundant\ninformation from each cluster is the backbone of the system. It makes rare\ninformation indifferent to aggressive momentum also exhibits superior\nperformance with larger mini-batch horizon. The effective length of the queue\nkept variable to follow the local loss pattern. The contribution of our method\nis to restore intra-mini-batch diversity at the same time widening the optimal\nbatch boundary. Both of these collectively drive it deeper towards the minima.\nOur method has shown superior performance for CIFAR10, MNIST, and Reuters News\ncategory dataset compared to mini-batch gradient descent.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.16917v1",
    "published_date": "2024-04-25 16:07:01 UTC",
    "updated_date": "2024-04-25 16:07:01 UTC"
  },
  {
    "arxiv_id": "2404.16696v1",
    "title": "Report on Candidate Computational Indicators for Conscious Valenced Experience",
    "authors": [
      "Andres Campero"
    ],
    "abstract": "This report enlists 13 functional conditions cashed out in computational\nterms that have been argued to be constituent of conscious valenced experience.\nThese are extracted from existing empirical and theoretical literature on,\namong others, animal sentience, medical disorders, anaesthetics, philosophy,\nevolution, neuroscience, and artificial intelligence.",
    "categories": [
      "q-bio.NC",
      "cs.AI"
    ],
    "primary_category": "q-bio.NC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.16696v1",
    "published_date": "2024-04-25 15:58:09 UTC",
    "updated_date": "2024-04-25 15:58:09 UTC"
  },
  {
    "arxiv_id": "2404.16692v3",
    "title": "Influence of Solution Efficiency and Valence of Instruction on Additive and Subtractive Solution Strategies in Humans and GPT-4",
    "authors": [
      "Lydia Uhler",
      "Verena Jordan",
      "Jürgen Buder",
      "Markus Huff",
      "Frank Papenmeier"
    ],
    "abstract": "Generative artificial intelligences, particularly large language models\n(LLMs), play an increasingly prominent role in human decision-making contexts,\nnecessitating transparency about their capabilities. While prior studies have\nshown addition biases in humans (Adams et al., 2021) and OpenAI's GPT-3 (Winter\net al., 2023), this study extends the research by comparing human and GPT-4\nproblem-solving across both spatial and linguistic tasks, with variations in\nsolution efficiency and valence of task instruction. Four preregistered\nexperiments with 588 participants from the U.S. and 680 GPT-4 iterations\nrevealed a stronger tendency towards additive transformations in GPT-4 than in\nhumans. Human participants were less likely to use additive strategies when\nsubtraction was relatively more efficient than when addition and subtraction\nwere equally efficient. GPT-4 exhibited the opposite behavior, with a strong\naddition bias when subtraction was more efficient. In terms of valence of task\ninstruction, GPT-4's use of additive strategies increased when instructed to\n\"improve\" (positive) rather than \"edit\" (neutral). These findings demonstrate\nthat biases in human problem-solving are amplified in GPT-4, and that LLM\nbehavior differs from human efficiency-based strategies. This highlights the\nlimitations of LLMs and the need for caution when using them in real-world\napplications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "29 pages, 4 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2404.16692v3",
    "published_date": "2024-04-25 15:53:00 UTC",
    "updated_date": "2024-11-16 16:09:11 UTC"
  },
  {
    "arxiv_id": "2405.00716v4",
    "title": "Large Language Models in the Clinic: A Comprehensive Benchmark",
    "authors": [
      "Fenglin Liu",
      "Zheng Li",
      "Hongjian Zhou",
      "Qingyu Yin",
      "Jingfeng Yang",
      "Xianfeng Tang",
      "Chen Luo",
      "Ming Zeng",
      "Haoming Jiang",
      "Yifan Gao",
      "Priyanka Nigam",
      "Sreyashi Nag",
      "Bing Yin",
      "Yining Hua",
      "Xuan Zhou",
      "Omid Rohanian",
      "Anshul Thakur",
      "Lei Clifton",
      "David A. Clifton"
    ],
    "abstract": "The adoption of large language models (LLMs) to assist clinicians has\nattracted remarkable attention. Existing works mainly adopt the close-ended\nquestion-answering (QA) task with answer options for evaluation. However, many\nclinical decisions involve answering open-ended questions without pre-set\noptions. To better understand LLMs in the clinic, we construct a benchmark\nClinicBench. We first collect eleven existing datasets covering diverse\nclinical language generation, understanding, and reasoning tasks. Furthermore,\nwe construct six novel datasets and clinical tasks that are complex but common\nin real-world practice, e.g., open-ended decision-making, long document\nprocessing, and emerging drug analysis. We conduct an extensive evaluation of\ntwenty-two LLMs under both zero-shot and few-shot settings. Finally, we invite\nmedical experts to evaluate the clinical usefulness of LLMs. The benchmark data\nis available at https://github.com/AI-in-Health/ClinicBench.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at EMNLP 2024 Main Conference",
    "pdf_url": "http://arxiv.org/pdf/2405.00716v4",
    "published_date": "2024-04-25 15:51:06 UTC",
    "updated_date": "2024-10-16 09:18:58 UTC"
  },
  {
    "arxiv_id": "2404.16689v1",
    "title": "Learning to Beat ByteRL: Exploitability of Collectible Card Game Agents",
    "authors": [
      "Radovan Haluska",
      "Martin Schmid"
    ],
    "abstract": "While Poker, as a family of games, has been studied extensively in the last\ndecades, collectible card games have seen relatively little attention. Only\nrecently have we seen an agent that can compete with professional human players\nin Hearthstone, one of the most popular collectible card games. Although\nartificial agents must be able to work with imperfect information in both of\nthese genres, collectible card games pose another set of distinct challenges.\nUnlike in many poker variants, agents must deal with state space so vast that\neven enumerating all states consistent with the agent's beliefs is intractable,\nrendering the current search methods unusable and requiring the agents to opt\nfor other techniques. In this paper, we investigate the strength of such\ntechniques for this class of games. Namely, we present preliminary analysis\nresults of ByteRL, the state-of-the-art agent in Legends of Code and Magic and\nHearthstone. Although ByteRL beat a top-10 Hearthstone player from China, we\nshow that its play in Legends of Code and Magic is highly exploitable.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.16689v1",
    "published_date": "2024-04-25 15:48:40 UTC",
    "updated_date": "2024-04-25 15:48:40 UTC"
  },
  {
    "arxiv_id": "2404.17607v1",
    "title": "Utilizing Large Language Models to Identify Reddit Users Considering Vaping Cessation for Digital Interventions",
    "authors": [
      "Sai Krishna Revanth Vuruma",
      "Dezhi Wu",
      "Saborny Sen Gupta",
      "Lucas Aust",
      "Valerie Lookingbill",
      "Caleb Henry",
      "Yang Ren",
      "Erin Kasson",
      "Li-Shiun Chen",
      "Patricia Cavazos-Rehg",
      "Dian Hu",
      "Ming Huang"
    ],
    "abstract": "The widespread adoption of social media platforms globally not only enhances\nusers' connectivity and communication but also emerges as a vital channel for\nthe dissemination of health-related information, thereby establishing social\nmedia data as an invaluable organic data resource for public health research.\nThe surge in popularity of vaping or e-cigarette use in the United States and\nother countries has caused an outbreak of e-cigarette and vaping use-associated\nlung injury (EVALI), leading to hospitalizations and fatalities in 2019,\nhighlighting the urgency to comprehend vaping behaviors and develop effective\nstrategies for cession. In this study, we extracted a sample dataset from one\nvaping sub-community on Reddit to analyze users' quit vaping intentions.\nLeveraging large language models including both the latest GPT-4 and\ntraditional BERT-based language models for sentence-level quit-vaping intention\nprediction tasks, this study compares the outcomes of these models against\nhuman annotations. Notably, when compared to human evaluators, GPT-4 model\ndemonstrates superior consistency in adhering to annotation guidelines and\nprocesses, showcasing advanced capabilities to detect nuanced user quit-vaping\nintentions that human evaluators might overlook. These preliminary findings\nemphasize the potential of GPT-4 in enhancing the accuracy and reliability of\nsocial media data analysis, especially in identifying subtle users' intentions\nthat may elude human detection.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.SI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.17607v1",
    "published_date": "2024-04-25 15:45:58 UTC",
    "updated_date": "2024-04-25 15:45:58 UTC"
  },
  {
    "arxiv_id": "2404.16914v1",
    "title": "Prediction Is All MoE Needs: Expert Load Distribution Goes from Fluctuating to Stabilizing",
    "authors": [
      "Peizhuang Cong",
      "Aomufei Yuan",
      "Shimao Chen",
      "Yuxuan Tian",
      "Bowen Ye",
      "Tong Yang"
    ],
    "abstract": "MoE facilitates the development of large models by making the computational\ncomplexity of the model no longer scale linearly with increasing parameters.\nThe learning sparse gating network selects a set of experts for each token to\nbe processed; however, this may lead to differences in the number of tokens\nprocessed by each expert over several successive iterations, i.e., the expert\nload fluctuations, which reduces computational parallelization and resource\nutilization. To this end, we traced and analyzed loads of each expert in the\ntraining iterations for several large language models in this work, and defined\nthe transient state with \"obvious load fluctuation\" and the stable state with\n\"temporal locality\". Moreover, given the characteristics of these two states\nand the computational overhead, we deployed three classical prediction\nalgorithms that achieve accurate expert load prediction results. For the GPT3\n350M model, the average error rates for predicting the expert load proportion\nover the next 1,000 and 2,000 steps are approximately 1.3% and 1.8%,\nrespectively. This work can provide valuable guidance for expert placement or\nresource allocation for MoE model training. Based on this work, we will propose\nan expert placement scheme for transient and stable states in our coming work.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.16914v1",
    "published_date": "2024-04-25 15:39:59 UTC",
    "updated_date": "2024-04-25 15:39:59 UTC"
  },
  {
    "arxiv_id": "2407.09975v1",
    "title": "The GPT Surprise: Offering Large Language Model Chat in a Massive Coding Class Reduced Engagement but Increased Adopters Exam Performances",
    "authors": [
      "Allen Nie",
      "Yash Chandak",
      "Miroslav Suzara",
      "Malika Ali",
      "Juliette Woodrow",
      "Matt Peng",
      "Mehran Sahami",
      "Emma Brunskill",
      "Chris Piech"
    ],
    "abstract": "Large language models (LLMs) are quickly being adopted in a wide range of\nlearning experiences, especially via ubiquitous and broadly accessible chat\ninterfaces like ChatGPT and Copilot. This type of interface is readily\navailable to students and teachers around the world, yet relatively little\nresearch has been done to assess the impact of such generic tools on student\nlearning. Coding education is an interesting test case, both because LLMs have\nstrong performance on coding tasks, and because LLM-powered support tools are\nrapidly becoming part of the workflow of professional software engineers. To\nhelp understand the impact of generic LLM use on coding education, we conducted\na large-scale randomized control trial with 5,831 students from 146 countries\nin an online coding class in which we provided some students with access to a\nchat interface with GPT-4. We estimate positive benefits on exam performance\nfor adopters, the students who used the tool, but over all students, the\nadvertisement of GPT-4 led to a significant average decrease in exam\nparticipation. We observe similar decreases in other forms of course\nengagement. However, this decrease is modulated by the student's country of\norigin. Offering access to LLMs to students from low human development index\ncountries increased their exam participation rate on average. Our results\nsuggest there may be promising benefits to using LLMs in an introductory coding\nclass, but also potential harms for engagement, which makes their longer term\nimpact on student success unclear. Our work highlights the need for additional\ninvestigations to help understand the potential impact of future adoption and\nintegration of LLMs into classrooms.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "stat.AP"
    ],
    "primary_category": "cs.CY",
    "comment": "32 pages",
    "pdf_url": "http://arxiv.org/pdf/2407.09975v1",
    "published_date": "2024-04-25 15:39:22 UTC",
    "updated_date": "2024-04-25 15:39:22 UTC"
  },
  {
    "arxiv_id": "2405.00715v4",
    "title": "Adapting Open-Source Large Language Models for Cost-Effective, Expert-Level Clinical Note Generation with On-Policy Reinforcement Learning",
    "authors": [
      "Hanyin Wang",
      "Chufan Gao",
      "Bolun Liu",
      "Qiping Xu",
      "Guleid Hussein",
      "Mohamad El Labban",
      "Kingsley Iheasirim",
      "Hariprasad Korsapati",
      "Chuck Outcalt",
      "Jimeng Sun"
    ],
    "abstract": "Proprietary Large Language Models (LLMs) such as GPT-4 and Gemini have\ndemonstrated promising capabilities in clinical text summarization tasks.\nHowever, due to patient data privacy concerns and computational costs, many\nhealthcare providers prefer using small, locally-hosted models over external\ngeneric LLMs. This study presents a comprehensive domain- and task-specific\nadaptation process for the open-source LLaMA-2 13 billion parameter model,\nenabling it to generate high-quality clinical notes from outpatient\npatient-doctor dialogues. Our process incorporates continued pre-training,\nsupervised fine-tuning, and reinforcement learning from both AI and human\nfeedback. We introduced a new approach, DistillDirect, for performing on-policy\nreinforcement learning with Gemini 1.0 Pro as the teacher model. Our resulting\nmodel, LLaMA-Clinic, can generate clinical notes comparable in quality to those\nauthored by physicians. In a blinded physician reader study, the majority\n(90.4%) of individual evaluations rated the notes generated by LLaMA-Clinic as\n\"acceptable\" or higher across all three criteria: real-world readiness,\ncompleteness, and accuracy. In the more challenging \"Assessment and Plan\"\nsection, LLaMA-Clinic scored higher (4.2/5) in real-world readiness than\nphysician-authored notes (4.1/5). Our cost analysis for inference shows that\nour LLaMA-Clinic model achieves a 3.75-fold cost reduction compared to an\nexternal generic LLM service. Additionally, we highlight key considerations for\nfuture clinical note-generation tasks, emphasizing the importance of\npre-defining a best-practice note format, rather than relying on LLMs to\ndetermine this for clinical practice. We have made our newly created synthetic\nclinic dialogue-note dataset and the physician feedback dataset publicly\navailable to foster future research.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.00715v4",
    "published_date": "2024-04-25 15:34:53 UTC",
    "updated_date": "2024-06-10 01:09:03 UTC"
  },
  {
    "arxiv_id": "2404.16685v1",
    "title": "Multi-scale HSV Color Feature Embedding for High-fidelity NIR-to-RGB Spectrum Translation",
    "authors": [
      "Huiyu Zhai",
      "Mo Chen",
      "Xingxing Yang",
      "Gusheng Kang"
    ],
    "abstract": "The NIR-to-RGB spectral domain translation is a formidable task due to the\ninherent spectral mapping ambiguities within NIR inputs and RGB outputs. Thus,\nexisting methods fail to reconcile the tension between maintaining texture\ndetail fidelity and achieving diverse color variations. In this paper, we\npropose a Multi-scale HSV Color Feature Embedding Network (MCFNet) that\ndecomposes the mapping process into three sub-tasks, including NIR texture\nmaintenance, coarse geometry reconstruction, and RGB color prediction. Thus, we\npropose three key modules for each corresponding sub-task: the Texture\nPreserving Block (TPB), the HSV Color Feature Embedding Module (HSV-CFEM), and\nthe Geometry Reconstruction Module (GRM). These modules contribute to our\nMCFNet methodically tackling spectral translation through a series of\nescalating resolutions, progressively enriching images with color and texture\nfidelity in a scale-coherent fashion. The proposed MCFNet demonstrates\nsubstantial performance gains over the NIR image colorization task. Code is\nreleased at: https://github.com/AlexYangxx/MCFNet.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.16685v1",
    "published_date": "2024-04-25 15:33:23 UTC",
    "updated_date": "2024-04-25 15:33:23 UTC"
  },
  {
    "arxiv_id": "2404.16913v1",
    "title": "DE-CGAN: Boosting rTMS Treatment Prediction with Diversity Enhancing Conditional Generative Adversarial Networks",
    "authors": [
      "Matthew Squires",
      "Xiaohui Tao",
      "Soman Elangovan",
      "Raj Gururajan",
      "Haoran Xie",
      "Xujuan Zhou",
      "Yuefeng Li",
      "U Rajendra Acharya"
    ],
    "abstract": "Repetitive Transcranial Magnetic Stimulation (rTMS) is a well-supported,\nevidence-based treatment for depression. However, patterns of response to this\ntreatment are inconsistent. Emerging evidence suggests that artificial\nintelligence can predict rTMS treatment outcomes for most patients using fMRI\nconnectivity features. While these models can reliably predict treatment\noutcomes for many patients for some underrepresented fMRI connectivity measures\nDNN models are unable to reliably predict treatment outcomes. As such we\npropose a novel method, Diversity Enhancing Conditional General Adversarial\nNetwork (DE-CGAN) for oversampling these underrepresented examples. DE-CGAN\ncreates synthetic examples in difficult-to-classify regions by first\nidentifying these data points and then creating conditioned synthetic examples\nto enhance data diversity. Through empirical experiments we show that a\nclassification model trained using a diversity enhanced training set\noutperforms traditional data augmentation techniques and existing benchmark\nresults. This work shows that increasing the diversity of a training dataset\ncan improve classification model performance. Furthermore, this work provides\nevidence for the utility of synthetic patients providing larger more robust\ndatasets for both AI researchers and psychiatrists to explore variable\nrelationships.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.16913v1",
    "published_date": "2024-04-25 15:15:58 UTC",
    "updated_date": "2024-04-25 15:15:58 UTC"
  },
  {
    "arxiv_id": "2404.16670v1",
    "title": "EmoVIT: Revolutionizing Emotion Insights with Visual Instruction Tuning",
    "authors": [
      "Hongxia Xie",
      "Chu-Jun Peng",
      "Yu-Wen Tseng",
      "Hung-Jen Chen",
      "Chan-Feng Hsu",
      "Hong-Han Shuai",
      "Wen-Huang Cheng"
    ],
    "abstract": "Visual Instruction Tuning represents a novel learning paradigm involving the\nfine-tuning of pre-trained language models using task-specific instructions.\nThis paradigm shows promising zero-shot results in various natural language\nprocessing tasks but is still unexplored in vision emotion understanding. In\nthis work, we focus on enhancing the model's proficiency in understanding and\nadhering to instructions related to emotional contexts. Initially, we identify\nkey visual clues critical to visual emotion recognition. Subsequently, we\nintroduce a novel GPT-assisted pipeline for generating emotion visual\ninstruction data, effectively addressing the scarcity of annotated instruction\ndata in this domain. Expanding on the groundwork established by InstructBLIP,\nour proposed EmoVIT architecture incorporates emotion-specific instruction\ndata, leveraging the powerful capabilities of Large Language Models to enhance\nperformance. Through extensive experiments, our model showcases its proficiency\nin emotion classification, adeptness in affective reasoning, and competence in\ncomprehending humor. The comparative analysis provides a robust benchmark for\nEmotion Visual Instruction Tuning in the era of LLMs, providing valuable\ninsights and opening avenues for future exploration in this domain. Our code is\navailable at \\url{https://github.com/aimmemotion/EmoVIT}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by CVPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.16670v1",
    "published_date": "2024-04-25 15:15:36 UTC",
    "updated_date": "2024-04-25 15:15:36 UTC"
  },
  {
    "arxiv_id": "2404.16663v4",
    "title": "Conditional Fairness for Generative AIs",
    "authors": [
      "Chih-Hong Cheng",
      "Harald Ruess",
      "Changshun Wu",
      "Xingyu Zhao"
    ],
    "abstract": "The deployment of generative AI (GenAI) models raises significant fairness\nconcerns, addressed in this paper through novel characterization and\nenforcement techniques specific to GenAI. Unlike standard AI performing\nspecific tasks, GenAI's broad functionality requires \"conditional fairness\"\ntailored to the context being generated, such as demographic fairness in\ngenerating images of poor people versus successful business leaders. We define\ntwo fairness levels: the first evaluates fairness in generated outputs,\nindependent of prompts and models; the second assesses inherent fairness with\nneutral prompts. Given the complexity of GenAI and challenges in fairness\nspecifications, we focus on bounding the worst case, considering a GenAI system\nunfair if the distance between appearances of a specific group exceeds preset\nthresholds. We also explore combinatorial testing for accessing relative\ncompleteness in intersectional fairness. By bounding the worst case, we develop\na prompt injection scheme within an agent-based framework to enforce\nconditional fairness with minimal intervention, validated on state-of-the-art\nGenAI systems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "cs.LO",
      "cs.SE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.16663v4",
    "published_date": "2024-04-25 15:04:27 UTC",
    "updated_date": "2024-08-15 10:03:16 UTC"
  },
  {
    "arxiv_id": "2404.16660v2",
    "title": "Benchmarking Mobile Device Control Agents across Diverse Configurations",
    "authors": [
      "Juyong Lee",
      "Taywon Min",
      "Minyong An",
      "Dongyoon Hahm",
      "Haeone Lee",
      "Changyeon Kim",
      "Kimin Lee"
    ],
    "abstract": "Mobile device control agents can largely enhance user interactions and\nproductivity by automating daily tasks. However, despite growing interest in\ndeveloping practical agents, the absence of a commonly adopted benchmark in\nthis area makes it challenging to quantify scientific progress. In this work,\nwe introduce B-MoCA: a novel benchmark with interactive environments for\nevaluating and developing mobile device control agents. To create a realistic\nbenchmark, we develop B-MoCA based on the Android operating system and define\n131 common daily tasks. Importantly, we incorporate a randomization feature\nthat changes the configurations of mobile devices, including user interface\nlayouts and language settings, to assess generalization performance. We\nbenchmark diverse agents, including agents employing large language models\n(LLMs) or multi-modal LLMs as well as agents trained with imitation learning\nusing human expert demonstrations. While these agents demonstrate proficiency\nin executing straightforward tasks, their poor performance on complex tasks\nhighlights significant opportunities for future research to improve\neffectiveness. Our source code is publicly available at\nhttps://b-moca.github.io.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted (Spotlight) to ICLR 2024 Workshop on Generative Models for\n  Decision Making. Project website: https://b-moca.github.io",
    "pdf_url": "http://arxiv.org/pdf/2404.16660v2",
    "published_date": "2024-04-25 14:56:32 UTC",
    "updated_date": "2024-10-19 07:07:58 UTC"
  },
  {
    "arxiv_id": "2404.16659v1",
    "title": "ProbGate at EHRSQL 2024: Enhancing SQL Query Generation Accuracy through Probabilistic Threshold Filtering and Error Handling",
    "authors": [
      "Sangryul Kim",
      "Donghee Han",
      "Sehyun Kim"
    ],
    "abstract": "Recently, deep learning-based language models have significantly enhanced\ntext-to-SQL tasks, with promising applications in retrieving patient records\nwithin the medical domain. One notable challenge in such applications is\ndiscerning unanswerable queries. Through fine-tuning model, we demonstrate the\nfeasibility of converting medical record inquiries into SQL queries.\nAdditionally, we introduce an entropy-based method to identify and filter out\nunanswerable results. We further enhance result quality by filtering\nlow-confidence SQL through log probability-based distribution, while\ngrammatical and schema errors are mitigated by executing queries on the actual\ndatabase. We experimentally verified that our method can filter unanswerable\nquestions, which can be widely utilized even when the parameters of the model\nare not accessible, and that it can be effectively utilized in practice.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "The 6th Clinical Natural Language Processing Workshop at NAACL 2024.\n  Code is available at https://github.com/venzino-han/probgate_ehrsql",
    "pdf_url": "http://arxiv.org/pdf/2404.16659v1",
    "published_date": "2024-04-25 14:55:07 UTC",
    "updated_date": "2024-04-25 14:55:07 UTC"
  },
  {
    "arxiv_id": "2404.16656v2",
    "title": "A Self-Organizing Clustering System for Unsupervised Distribution Shift Detection",
    "authors": [
      "Sebastián Basterrech",
      "Line Clemmensen",
      "Gerardo Rubino"
    ],
    "abstract": "Modeling non-stationary data is a challenging problem in the field of\ncontinual learning, and data distribution shifts may result in negative\nconsequences on the performance of a machine learning model. Classic learning\ntools are often vulnerable to perturbations of the input covariates, and are\nsensitive to outliers and noise, and some tools are based on rigid algebraic\nassumptions. Distribution shifts are frequently occurring due to changes in raw\nmaterials for production, seasonality, a different user base, or even\nadversarial attacks. Therefore, there is a need for more effective distribution\nshift detection techniques. In this work, we propose a continual learning\nframework for monitoring and detecting distribution changes. We explore the\nproblem in a latent space generated by a bio-inspired self-organizing\nclustering and statistical aspects of the latent space. In particular, we\ninvestigate the projections made by two topology-preserving maps: the\nSelf-Organizing Map and the Scale Invariant Map. Our method can be applied in\nboth a supervised and an unsupervised context. We construct the assessment of\nchanges in the data distribution as a comparison of Gaussian signals, making\nthe proposed method fast and robust. We compare it to other unsupervised\ntechniques, specifically Principal Component Analysis (PCA) and Kernel-PCA. Our\ncomparison involves conducting experiments using sequences of images (based on\nMNIST and injected shifts with adversarial samples), chemical sensor\nmeasurements, and the environmental variable related to ozone levels. The\nempirical study reveals the potential of the proposed approach.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE",
      "G.0; I.5.3; I.2; I.2.6"
    ],
    "primary_category": "cs.LG",
    "comment": "Revised version of the accepted manuscript to IJCNN'2024. Main\n  corrections were in Section 2.2 and Section 3.3. In Section 2.2 was corrected\n  expression (3), and in Section 3.3 in the definition of the elements of the\n  matrix $D$ it was a typo where $\\phi(x)$ was written instead of $x$",
    "pdf_url": "http://arxiv.org/pdf/2404.16656v2",
    "published_date": "2024-04-25 14:48:29 UTC",
    "updated_date": "2024-10-22 09:30:36 UTC"
  },
  {
    "arxiv_id": "2404.16653v1",
    "title": "Análise de ambiguidade linguística em modelos de linguagem de grande escala (LLMs)",
    "authors": [
      "Lavínia de Carvalho Moraes",
      "Irene Cristina Silvério",
      "Rafael Alexandre Sousa Marques",
      "Bianca de Castro Anaia",
      "Dandara Freitas de Paula",
      "Maria Carolina Schincariol de Faria",
      "Iury Cleveston",
      "Alana de Santana Correia",
      "Raquel Meister Ko Freitag"
    ],
    "abstract": "Linguistic ambiguity continues to represent a significant challenge for\nnatural language processing (NLP) systems, notwithstanding the advancements in\narchitectures such as Transformers and BERT. Inspired by the recent success of\ninstructional models like ChatGPT and Gemini (In 2023, the artificial\nintelligence was called Bard.), this study aims to analyze and discuss\nlinguistic ambiguity within these models, focusing on three types prevalent in\nBrazilian Portuguese: semantic, syntactic, and lexical ambiguity. We create a\ncorpus comprising 120 sentences, both ambiguous and unambiguous, for\nclassification, explanation, and disambiguation. The models capability to\ngenerate ambiguous sentences was also explored by soliciting sets of sentences\nfor each type of ambiguity. The results underwent qualitative analysis, drawing\non recognized linguistic references, and quantitative assessment based on the\naccuracy of the responses obtained. It was evidenced that even the most\nsophisticated models, such as ChatGPT and Gemini, exhibit errors and\ndeficiencies in their responses, with explanations often providing\ninconsistent. Furthermore, the accuracy peaked at 49.58 percent, indicating the\nneed for descriptive studies for supervised learning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "in Portuguese language, 16 p\\'aginas, 5 p\\'aginas de ap\\^endice e 4\n  imagens",
    "pdf_url": "http://arxiv.org/pdf/2404.16653v1",
    "published_date": "2024-04-25 14:45:07 UTC",
    "updated_date": "2024-04-25 14:45:07 UTC"
  },
  {
    "arxiv_id": "2404.16645v1",
    "title": "Tele-FLM Technical Report",
    "authors": [
      "Xiang Li",
      "Yiqun Yao",
      "Xin Jiang",
      "Xuezhi Fang",
      "Chao Wang",
      "Xinzhang Liu",
      "Zihan Wang",
      "Yu Zhao",
      "Xin Wang",
      "Yuyao Huang",
      "Shuangyong Song",
      "Yongxiang Li",
      "Zheng Zhang",
      "Bo Zhao",
      "Aixin Sun",
      "Yequan Wang",
      "Zhongjiang He",
      "Zhongyuan Wang",
      "Xuelong Li",
      "Tiejun Huang"
    ],
    "abstract": "Large language models (LLMs) have showcased profound capabilities in language\nunderstanding and generation, facilitating a wide array of applications.\nHowever, there is a notable paucity of detailed, open-sourced methodologies on\nefficiently scaling LLMs beyond 50 billion parameters with minimum\ntrial-and-error cost and computational resources. In this report, we introduce\nTele-FLM (aka FLM-2), a 52B open-sourced multilingual large language model that\nfeatures a stable, efficient pre-training paradigm and enhanced factual\njudgment capabilities. Tele-FLM demonstrates superior multilingual language\nmodeling abilities, measured by BPB on textual corpus. Besides, in both English\nand Chinese foundation model evaluation, it is comparable to strong\nopen-sourced models that involve larger pre-training FLOPs, such as Llama2-70B\nand DeepSeek-67B. In addition to the model weights, we share the core designs,\nengineering practices, and training details, which we expect to benefit both\nthe academic and industrial communities.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.16645v1",
    "published_date": "2024-04-25 14:34:47 UTC",
    "updated_date": "2024-04-25 14:34:47 UTC"
  },
  {
    "arxiv_id": "2404.16630v1",
    "title": "Legal Aspects for Software Developers Interested in Generative AI Applications",
    "authors": [
      "Steffen Herbold",
      "Brian Valerius",
      "Anamaria Mojica-Hanke",
      "Isabella Lex",
      "Joel Mittel"
    ],
    "abstract": "Recent successes in Generative Artificial Intelligence (GenAI) have led to\nnew technologies capable of generating high-quality code, natural language, and\nimages. The next step is to integrate GenAI technology into products, a task\ntypically conducted by software developers. Such product development always\ncomes with a certain risk of liability. Within this article, we want to shed\nlight on the current state of two such risks: data protection and copyright.\nBoth aspects are crucial for GenAI. This technology deals with data for both\nmodel training and generated output. We summarize key aspects regarding our\ncurrent knowledge that every software developer involved in product development\nusing GenAI should be aware of to avoid critical mistakes that may expose them\nto liability claims.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "Submission under review",
    "pdf_url": "http://arxiv.org/pdf/2404.16630v1",
    "published_date": "2024-04-25 14:17:34 UTC",
    "updated_date": "2024-04-25 14:17:34 UTC"
  },
  {
    "arxiv_id": "2404.16621v1",
    "title": "Hippocrates: An Open-Source Framework for Advancing Large Language Models in Healthcare",
    "authors": [
      "Emre Can Acikgoz",
      "Osman Batur İnce",
      "Rayene Bench",
      "Arda Anıl Boz",
      "İlker Kesen",
      "Aykut Erdem",
      "Erkut Erdem"
    ],
    "abstract": "The integration of Large Language Models (LLMs) into healthcare promises to\ntransform medical diagnostics, research, and patient care. Yet, the progression\nof medical LLMs faces obstacles such as complex training requirements, rigorous\nevaluation demands, and the dominance of proprietary models that restrict\nacademic exploration. Transparent, comprehensive access to LLM resources is\nessential for advancing the field, fostering reproducibility, and encouraging\ninnovation in healthcare AI. We present Hippocrates, an open-source LLM\nframework specifically developed for the medical domain. In stark contrast to\nprevious efforts, it offers unrestricted access to its training datasets,\ncodebase, checkpoints, and evaluation protocols. This open approach is designed\nto stimulate collaborative research, allowing the community to build upon,\nrefine, and rigorously evaluate medical LLMs within a transparent ecosystem.\nAlso, we introduce Hippo, a family of 7B models tailored for the medical\ndomain, fine-tuned from Mistral and LLaMA2 through continual pre-training,\ninstruction tuning, and reinforcement learning from human and AI feedback. Our\nmodels outperform existing open medical LLMs models by a large-margin, even\nsurpassing models with 70B parameters. Through Hippocrates, we aspire to unlock\nthe full potential of LLMs not just to advance medical knowledge and patient\ncare but also to democratize the benefits of AI research in healthcare, making\nthem available across the globe.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.16621v1",
    "published_date": "2024-04-25 14:06:37 UTC",
    "updated_date": "2024-04-25 14:06:37 UTC"
  },
  {
    "arxiv_id": "2404.18940v1",
    "title": "Conceptual Mapping of Controversies",
    "authors": [
      "Claude Draude",
      "Dominik Dürrschnabel",
      "Johannes Hirth",
      "Viktoria Horn",
      "Jonathan Kropf",
      "Jörn Lamla",
      "Gerd Stumme",
      "Markus Uhlmann"
    ],
    "abstract": "With our work, we contribute towards a qualitative analysis of the discourse\non controversies in online news media. For this, we employ Formal Concept\nAnalysis and the economics of conventions to derive conceptual controversy\nmaps. In our experiments, we analyze two maps from different news journals with\nmethods from ordinal data science. We show how these methods can be used to\nassess the diversity, complexity and potential bias of controversies. In\naddition to that, we discuss how the diagrams of concept lattices can be used\nto navigate between news articles.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.18940v1",
    "published_date": "2024-04-25 13:57:53 UTC",
    "updated_date": "2024-04-25 13:57:53 UTC"
  },
  {
    "arxiv_id": "2405.02330v1",
    "title": "Adaptive Semantic Token Selection for AI-native Goal-oriented Communications",
    "authors": [
      "Alessio Devoto",
      "Simone Petruzzi",
      "Jary Pomponi",
      "Paolo Di Lorenzo",
      "Simone Scardapane"
    ],
    "abstract": "In this paper, we propose a novel design for AI-native goal-oriented\ncommunications, exploiting transformer neural networks under dynamic inference\nconstraints on bandwidth and computation. Transformers have become the standard\narchitecture for pretraining large-scale vision and text models, and\npreliminary results have shown promising performance also in deep joint\nsource-channel coding (JSCC). Here, we consider a dynamic model where\ncommunication happens over a channel with variable latency and bandwidth\nconstraints. Leveraging recent works on conditional computation, we exploit the\nstructure of the transformer blocks and the multihead attention operator to\ndesign a trainable semantic token selection mechanism that learns to select\nrelevant tokens (e.g., image patches) from the input signal. This is done\ndynamically, on a per-input basis, with a rate that can be chosen as an\nadditional input by the user. We show that our model improves over\nstate-of-the-art token selection mechanisms, exhibiting high accuracy for a\nwide range of latency and bandwidth constraints, without the need for deploying\nmultiple architectures tailored to each constraint. Last, but not least, the\nproposed token selection mechanism helps extract powerful semantics that are\neasy to understand and explain, paving the way for interpretable-by-design\nmodels for the next generation of AI-native communication systems.",
    "categories": [
      "cs.IT",
      "cs.AI",
      "cs.LG",
      "math.IT",
      "94A40"
    ],
    "primary_category": "cs.IT",
    "comment": "5 pages",
    "pdf_url": "http://arxiv.org/pdf/2405.02330v1",
    "published_date": "2024-04-25 13:49:50 UTC",
    "updated_date": "2024-04-25 13:49:50 UTC"
  },
  {
    "arxiv_id": "2404.16609v2",
    "title": "SFMViT: SlowFast Meet ViT in Chaotic World",
    "authors": [
      "Jiaying Lin",
      "Jiajun Wen",
      "Mengyuan Liu",
      "Jinfu Liu",
      "Baiqiao Yin",
      "Yue Li"
    ],
    "abstract": "The task of spatiotemporal action localization in chaotic scenes is a\nchallenging task toward advanced video understanding. Paving the way with\nhigh-quality video feature extraction and enhancing the precision of\ndetector-predicted anchors can effectively improve model performance. To this\nend, we propose a high-performance dual-stream spatiotemporal feature\nextraction network SFMViT with an anchor pruning strategy. The backbone of our\nSFMViT is composed of ViT and SlowFast with prior knowledge of spatiotemporal\naction localization, which fully utilizes ViT's excellent global feature\nextraction capabilities and SlowFast's spatiotemporal sequence modeling\ncapabilities. Secondly, we introduce the confidence maximum heap to prune the\nanchors detected in each frame of the picture to filter out the effective\nanchors. These designs enable our SFMViT to achieve a mAP of 26.62% in the\nChaotic World dataset, far exceeding existing models. Code is available at\nhttps://github.com/jfightyr/SlowFast-Meet-ViT.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.16609v2",
    "published_date": "2024-04-25 13:49:42 UTC",
    "updated_date": "2024-08-13 03:13:50 UTC"
  },
  {
    "arxiv_id": "2404.16908v1",
    "title": "Closing the gap: Optimizing Guidance and Control Networks through Neural ODEs",
    "authors": [
      "Sebastien Origer",
      "Dario Izzo"
    ],
    "abstract": "We improve the accuracy of Guidance & Control Networks (G&CNETs), trained to\nrepresent the optimal control policies of a time-optimal transfer and a\nmass-optimal landing, respectively. In both cases we leverage the dynamics of\nthe spacecraft, described by Ordinary Differential Equations which incorporate\na neural network on their right-hand side (Neural ODEs). Since the neural\ndynamics is differentiable, the ODEs sensitivities to the network parameters\ncan be computed using the variational equations, thereby allowing to update the\nG&CNET parameters based on the observed dynamics. We start with a\nstraightforward regression task, training the G&CNETs on datasets of optimal\ntrajectories using behavioural cloning. These networks are then refined using\nthe Neural ODE sensitivities by minimizing the error between the final states\nand the target states. We demonstrate that for the orbital transfer, the final\nerror to the target can be reduced by 99% on a single trajectory and by 70% on\na batch of 500 trajectories. For the landing problem the reduction in error is\naround 98-99% (position) and 40-44% (velocity). This step significantly\nenhances the accuracy of G&CNETs, which instills greater confidence in their\nreliability for operational use. We also compare our results to the popular\nDataset Aggregation method (DaGGER) and allude to the strengths and weaknesses\nof both methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.16908v1",
    "published_date": "2024-04-25 13:14:32 UTC",
    "updated_date": "2024-04-25 13:14:32 UTC"
  },
  {
    "arxiv_id": "2404.16587v1",
    "title": "Understanding Privacy Risks of Embeddings Induced by Large Language Models",
    "authors": [
      "Zhihao Zhu",
      "Ninglu Shao",
      "Defu Lian",
      "Chenwang Wu",
      "Zheng Liu",
      "Yi Yang",
      "Enhong Chen"
    ],
    "abstract": "Large language models (LLMs) show early signs of artificial general\nintelligence but struggle with hallucinations. One promising solution to\nmitigate these hallucinations is to store external knowledge as embeddings,\naiding LLMs in retrieval-augmented generation. However, such a solution risks\ncompromising privacy, as recent studies experimentally showed that the original\ntext can be partially reconstructed from text embeddings by pre-trained\nlanguage models. The significant advantage of LLMs over traditional pre-trained\nmodels may exacerbate these concerns. To this end, we investigate the\neffectiveness of reconstructing original knowledge and predicting entity\nattributes from these embeddings when LLMs are employed. Empirical findings\nindicate that LLMs significantly improve the accuracy of two evaluated tasks\nover those from pre-trained models, regardless of whether the texts are\nin-distribution or out-of-distribution. This underscores a heightened potential\nfor LLMs to jeopardize user privacy, highlighting the negative consequences of\ntheir widespread use. We further discuss preliminary strategies to mitigate\nthis risk.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.16587v1",
    "published_date": "2024-04-25 13:10:48 UTC",
    "updated_date": "2024-04-25 13:10:48 UTC"
  },
  {
    "arxiv_id": "2404.16579v1",
    "title": "Neural Interaction Energy for Multi-Agent Trajectory Prediction",
    "authors": [
      "Kaixin Shen",
      "Ruijie Quan",
      "Linchao Zhu",
      "Jun Xiao",
      "Yi Yang"
    ],
    "abstract": "Maintaining temporal stability is crucial in multi-agent trajectory\nprediction. Insufficient regularization to uphold this stability often results\nin fluctuations in kinematic states, leading to inconsistent predictions and\nthe amplification of errors. In this study, we introduce a framework called\nMulti-Agent Trajectory prediction via neural interaction Energy (MATE). This\nframework assesses the interactive motion of agents by employing neural\ninteraction energy, which captures the dynamics of interactions and illustrates\ntheir influence on the future trajectories of agents. To bolster temporal\nstability, we introduce two constraints: inter-agent interaction constraint and\nintra-agent motion constraint. These constraints work together to ensure\ntemporal stability at both the system and agent levels, effectively mitigating\nprediction fluctuations inherent in multi-agent systems. Comparative\nevaluations against previous methods on four diverse datasets highlight the\nsuperior prediction accuracy and generalization capabilities of our model.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.16579v1",
    "published_date": "2024-04-25 12:47:47 UTC",
    "updated_date": "2024-04-25 12:47:47 UTC"
  },
  {
    "arxiv_id": "2404.16906v2",
    "title": "Evolve Cost-aware Acquisition Functions Using Large Language Models",
    "authors": [
      "Yiming Yao",
      "Fei Liu",
      "Ji Cheng",
      "Qingfu Zhang"
    ],
    "abstract": "Many real-world optimization scenarios involve expensive evaluation with\nunknown and heterogeneous costs. Cost-aware Bayesian optimization stands out as\na prominent solution in addressing these challenges. To approach the global\noptimum within a limited budget in a cost-efficient manner, the design of\ncost-aware acquisition functions (AFs) becomes a crucial step. However,\ntraditional manual design paradigm typically requires extensive domain\nknowledge and involves a labor-intensive trial-and-error process. This paper\nintroduces EvolCAF, a novel framework that integrates large language models\n(LLMs) with evolutionary computation (EC) to automatically design cost-aware\nAFs. Leveraging the crossover and mutation in the algorithmic space, EvolCAF\noffers a novel design paradigm, significantly reduces the reliance on domain\nexpertise and model training. The designed cost-aware AF maximizes the\nutilization of available information from historical data, surrogate models and\nbudget details. It introduces novel ideas not previously explored in the\nexisting literature on acquisition function design, allowing for clear\ninterpretations to provide insights into its behavior and decision-making\nprocess. In comparison to the well-known EIpu and EI-cool methods designed by\nhuman experts, our approach showcases remarkable efficiency and generalization\nacross various tasks, including 12 synthetic problems and 3 real-world\nhyperparameter tuning test sets.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.16906v2",
    "published_date": "2024-04-25 12:19:18 UTC",
    "updated_date": "2024-06-13 06:53:40 UTC"
  },
  {
    "arxiv_id": "2404.16558v1",
    "title": "DeepKalPose: An Enhanced Deep-Learning Kalman Filter for Temporally Consistent Monocular Vehicle Pose Estimation",
    "authors": [
      "Leandro Di Bella",
      "Yangxintong Lyu",
      "Adrian Munteanu"
    ],
    "abstract": "This paper presents DeepKalPose, a novel approach for enhancing temporal\nconsistency in monocular vehicle pose estimation applied on video through a\ndeep-learning-based Kalman Filter. By integrating a Bi-directional Kalman\nfilter strategy utilizing forward and backward time-series processing, combined\nwith a learnable motion model to represent complex motion patterns, our method\nsignificantly improves pose accuracy and robustness across various conditions,\nparticularly for occluded or distant vehicles. Experimental validation on the\nKITTI dataset confirms that DeepKalPose outperforms existing methods in both\npose accuracy and temporal consistency.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "4 pages, 3 Figures, published to IET Electronic Letters",
    "pdf_url": "http://arxiv.org/pdf/2404.16558v1",
    "published_date": "2024-04-25 12:15:11 UTC",
    "updated_date": "2024-04-25 12:15:11 UTC"
  },
  {
    "arxiv_id": "2404.16557v1",
    "title": "Energy-Latency Manipulation of Multi-modal Large Language Models via Verbose Samples",
    "authors": [
      "Kuofeng Gao",
      "Jindong Gu",
      "Yang Bai",
      "Shu-Tao Xia",
      "Philip Torr",
      "Wei Liu",
      "Zhifeng Li"
    ],
    "abstract": "Despite the exceptional performance of multi-modal large language models\n(MLLMs), their deployment requires substantial computational resources. Once\nmalicious users induce high energy consumption and latency time (energy-latency\ncost), it will exhaust computational resources and harm availability of\nservice. In this paper, we investigate this vulnerability for MLLMs,\nparticularly image-based and video-based ones, and aim to induce high\nenergy-latency cost during inference by crafting an imperceptible perturbation.\nWe find that high energy-latency cost can be manipulated by maximizing the\nlength of generated sequences, which motivates us to propose verbose samples,\nincluding verbose images and videos. Concretely, two modality non-specific\nlosses are proposed, including a loss to delay end-of-sequence (EOS) token and\nan uncertainty loss to increase the uncertainty over each generated token. In\naddition, improving diversity is important to encourage longer responses by\nincreasing the complexity, which inspires the following modality specific loss.\nFor verbose images, a token diversity loss is proposed to promote diverse\nhidden states. For verbose videos, a frame feature diversity loss is proposed\nto increase the feature diversity among frames. To balance these losses, we\npropose a temporal weight adjustment algorithm. Experiments demonstrate that\nour verbose samples can largely extend the length of generated sequences.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "arXiv admin note: substantial text overlap with arXiv:2401.11170",
    "pdf_url": "http://arxiv.org/pdf/2404.16557v1",
    "published_date": "2024-04-25 12:11:38 UTC",
    "updated_date": "2024-04-25 12:11:38 UTC"
  },
  {
    "arxiv_id": "2404.16547v1",
    "title": "Developing Acoustic Models for Automatic Speech Recognition in Swedish",
    "authors": [
      "Giampiero Salvi"
    ],
    "abstract": "This paper is concerned with automatic continuous speech recognition using\ntrainable systems. The aim of this work is to build acoustic models for spoken\nSwedish. This is done employing hidden Markov models and using the SpeechDat\ndatabase to train their parameters. Acoustic modeling has been worked out at a\nphonetic level, allowing general speech recognition applications, even though a\nsimplified task (digits and natural number recognition) has been considered for\nmodel evaluation. Different kinds of phone models have been tested, including\ncontext independent models and two variations of context dependent models.\nFurthermore many experiments have been done with bigram language models to tune\nsome of the system parameters. System performance over various speaker subsets\nwith different sex, age and dialect has also been examined. Results are\ncompared to previous similar studies showing a remarkable improvement.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD",
      "68T10",
      "I.5.0; I.2.0; I.2.7"
    ],
    "primary_category": "eess.AS",
    "comment": "16 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.16547v1",
    "published_date": "2024-04-25 12:03:14 UTC",
    "updated_date": "2024-04-25 12:03:14 UTC"
  },
  {
    "arxiv_id": "2407.09974v1",
    "title": "To what extent is ChatGPT useful for language teacher lesson plan creation?",
    "authors": [
      "Alex Dornburg",
      "Kristin Davin"
    ],
    "abstract": "The advent of generative AI models holds tremendous potential for aiding\nteachers in the generation of pedagogical materials. However, numerous\nknowledge gaps concerning the behavior of these models obfuscate the generation\nof research-informed guidance for their effective usage. Here we assess trends\nin prompt specificity, variability, and weaknesses in foreign language teacher\nlesson plans generated by zero-shot prompting in ChatGPT. Iterating a series of\nprompts that increased in complexity, we found that output lesson plans were\ngenerally high quality, though additional context and specificity to a prompt\ndid not guarantee a concomitant increase in quality. Additionally, we observed\nextreme cases of variability in outputs generated by the same prompt. In many\ncases, this variability reflected a conflict between 20th century versus 21st\ncentury pedagogical practices. These results suggest that the training of\ngenerative AI models on classic texts concerning pedagogical practices may\nrepresent a currently underexplored topic with the potential to bias generated\ncontent towards teaching practices that have been long refuted by research.\nCollectively, our results offer immediate translational implications for\npracticing and training foreign language teachers on the use of AI tools. More\nbroadly, these findings reveal the existence of generative AI output trends\nthat have implications for the generation of pedagogical materials across a\ndiversity of content areas.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09974v1",
    "published_date": "2024-04-25 12:00:03 UTC",
    "updated_date": "2024-04-25 12:00:03 UTC"
  },
  {
    "arxiv_id": "2404.16534v1",
    "title": "SIDEs: Separating Idealization from Deceptive Explanations in xAI",
    "authors": [
      "Emily Sullivan"
    ],
    "abstract": "Explainable AI (xAI) methods are important for establishing trust in using\nblack-box models. However, recent criticism has mounted against current xAI\nmethods that they disagree, are necessarily false, and can be manipulated,\nwhich has started to undermine the deployment of black-box models. Rudin (2019)\ngoes so far as to say that we should stop using black-box models altogether in\nhigh-stakes cases because xAI explanations \"must be wrong\". However, strict\nfidelity to the truth is historically not a desideratum in science.\nIdealizations -- the intentional distortions introduced to scientific theories\nand models -- are commonplace in the natural sciences and are seen as a\nsuccessful scientific tool. Thus, it is not falsehood qua falsehood that is the\nissue. In this paper, I outline the need for xAI research to engage in\nidealization evaluation. Drawing on the use of idealizations in the natural\nsciences and philosophy of science, I introduce a novel framework for\nevaluating whether xAI methods engage in successful idealizations or deceptive\nexplanations (SIDEs). SIDEs evaluates whether the limitations of xAI methods,\nand the distortions that they introduce, can be part of a successful\nidealization or are indeed deceptive distortions as critics suggest. I discuss\nthe role that existing research can play in idealization evaluation and where\ninnovation is necessary. Through a qualitative analysis we find that leading\nfeature importance methods and counterfactual explanations are subject to\nidealization failure and suggest remedies for ameliorating idealization\nfailure.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "A.0; I.2.0; K.4.0"
    ],
    "primary_category": "cs.AI",
    "comment": "18 pages, 3 figures, 2 tables Forthcoming in FAccT'24",
    "pdf_url": "http://arxiv.org/pdf/2404.16534v1",
    "published_date": "2024-04-25 11:47:39 UTC",
    "updated_date": "2024-04-25 11:47:39 UTC"
  },
  {
    "arxiv_id": "2404.16532v1",
    "title": "Global Concept Explanations for Graphs by Contrastive Learning",
    "authors": [
      "Jonas Teufel",
      "Pascal Friederich"
    ],
    "abstract": "Beyond improving trust and validating model fairness, xAI practices also have\nthe potential to recover valuable scientific insights in application domains\nwhere little to no prior human intuition exists. To that end, we propose a\nmethod to extract global concept explanations from the predictions of graph\nneural networks to develop a deeper understanding of the tasks underlying\nstructure-property relationships. We identify concept explanations as dense\nclusters in the self-explaining Megan models subgraph latent space. For each\nconcept, we optimize a representative prototype graph and optionally use GPT-4\nto provide hypotheses about why each structure has a certain effect on the\nprediction. We conduct computational experiments on synthetic and real-world\ngraph property prediction tasks. For the synthetic tasks we find that our\nmethod correctly reproduces the structural rules by which they were created.\nFor real-world molecular property regression and classification tasks, we find\nthat our method rediscovers established rules of thumb. More specifically, our\nresults for molecular mutagenicity prediction indicate more fine-grained\nresolution of structural details than existing explainability methods,\nconsistent with previous results from chemistry literature. Overall, our\nresults show promising capability to extract the underlying structure-property\nrelationships for complex graph property prediction tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "25 pages, 9 figures, accepted at xAI world conference 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.16532v1",
    "published_date": "2024-04-25 11:43:46 UTC",
    "updated_date": "2024-04-25 11:43:46 UTC"
  },
  {
    "arxiv_id": "2407.00010v1",
    "title": "Hybrid Heterogeneous Clusters Can Lower the Energy Consumption of LLM Inference Workloads",
    "authors": [
      "Grant Wilkins",
      "Srinivasan Keshav",
      "Richard Mortier"
    ],
    "abstract": "Both the training and use of Large Language Models (LLMs) require large\namounts of energy. Their increasing popularity, therefore, raises critical\nconcerns regarding the energy efficiency and sustainability of data centers\nthat host them. This paper addresses the challenge of reducing energy\nconsumption in data centers running LLMs. We propose a hybrid data center model\nthat uses a cost-based scheduling framework to dynamically allocate LLM tasks\nacross hardware accelerators that differ in their energy efficiencies and\ncomputational capabilities. Specifically, our workload-aware strategy\ndetermines whether tasks are processed on energy-efficient processors or\nhigh-performance GPUs based on the number of input and output tokens in a\nquery. Our analysis of a representative LLM dataset, finds that this hybrid\nstrategy can reduce CPU+GPU energy consumption by 7.5% compared to a\nworkload-unaware baseline.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.00010v1",
    "published_date": "2024-04-25 11:24:08 UTC",
    "updated_date": "2024-04-25 11:24:08 UTC"
  },
  {
    "arxiv_id": "2404.16508v1",
    "title": "Exploring the Dynamics of Data Transmission in 5G Networks: A Conceptual Analysis",
    "authors": [
      "Nikita Smirnov",
      "Sven Tomforde"
    ],
    "abstract": "This conceptual analysis examines the dynamics of data transmission in 5G\nnetworks. It addresses various aspects of sending data from cameras and LiDARs\ninstalled on a remote-controlled ferry to a land-based control center. The\nrange of topics includes all stages of video and LiDAR data processing from\nacquisition and encoding to final decoding, all aspects of their transmission\nand reception via the WebRTC protocol, and all possible types of network\nproblems such as handovers or congestion that could affect the quality of\nexperience for end-users. A series of experiments were conducted to evaluate\nthe key aspects of the data transmission. These include simulation-based\nreproducible runs and real-world experiments conducted using open-source\nsolutions we developed: \"Gymir5G\" - an OMNeT++-based 5G simulation and\n\"GstWebRTCApp\" - a GStreamer-based application for adaptive control of media\nstreams over the WebRTC protocol. One of the goals of this study is to\nformulate the bandwidth and latency requirements for reliable real-time\ncommunication and to estimate their approximate values. This goal was achieved\nthrough simulation-based experiments involving docking maneuvers in the Bay of\nKiel, Germany. The final latency for the entire data processing pipeline was\nalso estimated during the real tests. In addition, a series of simulation-based\nexperiments showed the impact of key WebRTC features and demonstrated the\neffectiveness of the WebRTC protocol, while the conducted video codec\ncomparison showed that the hardware-accelerated H.264 codec is the best.\nFinally, the research addresses the topic of adaptive communication, where the\ntraditional congestion avoidance and deep reinforcement learning approaches\nwere analyzed. The comparison in a sandbox scenario shows that the AI-based\nsolution outperforms the WebRTC baseline GCC algorithm in terms of data rates,\nlatency, and packet loss.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.16508v1",
    "published_date": "2024-04-25 11:02:54 UTC",
    "updated_date": "2024-04-25 11:02:54 UTC"
  },
  {
    "arxiv_id": "2404.16478v1",
    "title": "Evaluating Consistency and Reasoning Capabilities of Large Language Models",
    "authors": [
      "Yash Saxena",
      "Sarthak Chopra",
      "Arunendra Mani Tripathi"
    ],
    "abstract": "Large Language Models (LLMs) are extensively used today across various\nsectors, including academia, research, business, and finance, for tasks such as\ntext generation, summarization, and translation. Despite their widespread\nadoption, these models often produce incorrect and misleading information,\nexhibiting a tendency to hallucinate. This behavior can be attributed to\nseveral factors, with consistency and reasoning capabilities being significant\ncontributors. LLMs frequently lack the ability to generate explanations and\nengage in coherent reasoning, leading to inaccurate responses. Moreover, they\nexhibit inconsistencies in their outputs. This paper aims to evaluate and\ncompare the consistency and reasoning capabilities of both public and\nproprietary LLMs. The experiments utilize the Boolq dataset as the ground\ntruth, comprising questions, answers, and corresponding explanations. Queries\nfrom the dataset are presented as prompts to the LLMs, and the generated\nresponses are evaluated against the ground truth answers. Additionally,\nexplanations are generated to assess the models' reasoning abilities.\nConsistency is evaluated by repeatedly presenting the same query to the models\nand observing for variations in their responses. For measuring reasoning\ncapabilities, the generated explanations are compared to the ground truth\nexplanations using metrics such as BERT, BLEU, and F-1 scores. The findings\nreveal that proprietary models generally outperform public models in terms of\nboth consistency and reasoning capabilities. However, even when presented with\nbasic general knowledge questions, none of the models achieved a score of 90\\%\nin both consistency and reasoning. This study underscores the direct\ncorrelation between consistency and reasoning abilities in LLMs and highlights\nthe inherent reasoning challenges present in current language models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.16478v1",
    "published_date": "2024-04-25 10:03:14 UTC",
    "updated_date": "2024-04-25 10:03:14 UTC"
  },
  {
    "arxiv_id": "2404.16474v1",
    "title": "DiffSeg: A Segmentation Model for Skin Lesions Based on Diffusion Difference",
    "authors": [
      "Zhihao Shuai",
      "Yinan Chen",
      "Shunqiang Mao",
      "Yihan Zho",
      "Xiaohong Zhang"
    ],
    "abstract": "Weakly supervised medical image segmentation (MIS) using generative models is\ncrucial for clinical diagnosis. However, the accuracy of the segmentation\nresults is often limited by insufficient supervision and the complex nature of\nmedical imaging. Existing models also only provide a single outcome, which does\nnot allow for the measurement of uncertainty. In this paper, we introduce\nDiffSeg, a segmentation model for skin lesions based on diffusion difference\nwhich exploits diffusion model principles to ex-tract noise-based features from\nimages with diverse semantic information. By discerning difference between\nthese noise features, the model identifies diseased areas. Moreover, its\nmulti-output capability mimics doctors' annotation behavior, facilitating the\nvisualization of segmentation result consistency and ambiguity. Additionally,\nit quantifies output uncertainty using Generalized Energy Distance (GED),\naiding interpretability and decision-making for physicians. Finally, the model\nintegrates outputs through the Dense Conditional Random Field (DenseCRF)\nalgorithm to refine the segmentation boundaries by considering inter-pixel\ncorrelations, which improves the accuracy and optimizes the segmentation\nresults. We demonstrate the effectiveness of DiffSeg on the ISIC 2018 Challenge\ndataset, outperforming state-of-the-art U-Net-based methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.16474v1",
    "published_date": "2024-04-25 09:57:52 UTC",
    "updated_date": "2024-04-25 09:57:52 UTC"
  },
  {
    "arxiv_id": "2404.16468v2",
    "title": "A Dual Perspective of Reinforcement Learning for Imposing Policy Constraints",
    "authors": [
      "Bram De Cooman",
      "Johan Suykens"
    ],
    "abstract": "Model-free reinforcement learning methods lack an inherent mechanism to\nimpose behavioural constraints on the trained policies. Although certain\nextensions exist, they remain limited to specific types of constraints, such as\nvalue constraints with additional reward signals or visitation density\nconstraints. In this work we unify these existing techniques and bridge the gap\nwith classical optimization and control theory, using a generic primal-dual\nframework for value-based and actor-critic reinforcement learning methods. The\nobtained dual formulations turn out to be especially useful for imposing\nadditional constraints on the learned policy, as an intrinsic relationship\nbetween such dual constraints (or regularization terms) and reward\nmodifications in the primal is revealed. Furthermore, using this framework, we\nare able to introduce some novel types of constraints, allowing to impose\nbounds on the policy's action density or on costs associated with transitions\nbetween consecutive states and actions. From the adjusted primal-dual\noptimization problems, a practical algorithm is derived that supports various\ncombinations of policy constraints that are automatically handled throughout\ntraining using trainable reward modifications. The proposed $\\texttt{DualCRL}$\nmethod is examined in more detail and evaluated under different (combinations\nof) constraints on two interpretable environments. The results highlight the\nefficacy of the method, which ultimately provides the designer of such systems\nwith a versatile toolbox of possible policy constraints.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY",
      "I.2.8"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted for publication in IEEE Transactions on Artificial\n  Intelligence",
    "pdf_url": "http://arxiv.org/pdf/2404.16468v2",
    "published_date": "2024-04-25 09:50:57 UTC",
    "updated_date": "2025-04-25 15:20:59 UTC"
  },
  {
    "arxiv_id": "2404.16457v1",
    "title": "Towards Precise Observations of Neural Model Robustness in Classification",
    "authors": [
      "Wenchuan Mu",
      "Kwan Hui Lim"
    ],
    "abstract": "In deep learning applications, robustness measures the ability of neural\nmodels that handle slight changes in input data, which could lead to potential\nsafety hazards, especially in safety-critical applications. Pre-deployment\nassessment of model robustness is essential, but existing methods often suffer\nfrom either high costs or imprecise results. To enhance safety in real-world\nscenarios, metrics that effectively capture the model's robustness are needed.\nTo address this issue, we compare the rigour and usage conditions of various\nassessment methods based on different definitions. Then, we propose a\nstraightforward and practical metric utilizing hypothesis testing for\nprobabilistic robustness and have integrated it into the TorchAttacks library.\nThrough a comparative analysis of diverse robustness assessment methods, our\napproach contributes to a deeper understanding of model robustness in\nsafety-critical applications.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.16457v1",
    "published_date": "2024-04-25 09:37:44 UTC",
    "updated_date": "2024-04-25 09:37:44 UTC"
  },
  {
    "arxiv_id": "2404.16455v3",
    "title": "Canonical Decision Diagrams Modulo Theories",
    "authors": [
      "Massimo Michelutti",
      "Gabriele Masina",
      "Giuseppe Spallitta",
      "Roberto Sebastiani"
    ],
    "abstract": "Decision diagrams (DDs) are powerful tools to represent effectively\npropositional formulas, which are largely used in many domains, in particular\nin formal verification and in knowledge compilation. Some forms of DDs (e.g.,\nOBDDs, SDDs) are canonical, that is, (under given conditions on the atom list)\nthey univocally represent equivalence classes of formulas. Given the limited\nexpressiveness of propositional logic, a few attempts to leverage DDs to SMT\nlevel have been presented in the literature. Unfortunately, these techniques\nstill suffer from some limitations: most procedures are theory-specific; some\nproduce theory DDs (T-DDs) which do not univocally represent T-valid formulas\nor T-inconsistent formulas; none of these techniques provably produces\ntheory-canonical T-DDs, which (under given conditions on the T-atom list)\nunivocally represent T-equivalence classes of formulas. Also, these procedures\nare not easy to implement, and very few implementations are actually available.\nIn this paper, we present a novel very-general technique to leverage DDs to SMT\nlevel, which has several advantages: it is very easy to implement on top of an\nAllSMT solver and a DD package, which are used as blackboxes; it works for\nevery form of DDs and every theory, or combination thereof, supported by the\nAllSMT solver; it produces theory-canonical T-DDs if the propositional DD is\ncanonical. We have implemented a prototype tool for both T-OBDDs and T-SDDs on\ntop of OBDD and SDD packages and the MathSAT SMT solver. Some preliminary\nempirical evaluation supports the effectiveness of the approach.",
    "categories": [
      "cs.LO",
      "cs.AI"
    ],
    "primary_category": "cs.LO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.16455v3",
    "published_date": "2024-04-25 09:34:49 UTC",
    "updated_date": "2024-08-02 13:27:16 UTC"
  },
  {
    "arxiv_id": "2404.16451v1",
    "title": "Latent Modulated Function for Computational Optimal Continuous Image Representation",
    "authors": [
      "Zongyao He",
      "Zhi Jin"
    ],
    "abstract": "The recent work Local Implicit Image Function (LIIF) and subsequent Implicit\nNeural Representation (INR) based works have achieved remarkable success in\nArbitrary-Scale Super-Resolution (ASSR) by using MLP to decode Low-Resolution\n(LR) features. However, these continuous image representations typically\nimplement decoding in High-Resolution (HR) High-Dimensional (HD) space, leading\nto a quadratic increase in computational cost and seriously hindering the\npractical applications of ASSR. To tackle this problem, we propose a novel\nLatent Modulated Function (LMF), which decouples the HR-HD decoding process\ninto shared latent decoding in LR-HD space and independent rendering in HR\nLow-Dimensional (LD) space, thereby realizing the first computational optimal\nparadigm of continuous image representation. Specifically, LMF utilizes an HD\nMLP in latent space to generate latent modulations of each LR feature vector.\nThis enables a modulated LD MLP in render space to quickly adapt to any input\nfeature vector and perform rendering at arbitrary resolution. Furthermore, we\nleverage the positive correlation between modulation intensity and input image\ncomplexity to design a Controllable Multi-Scale Rendering (CMSR) algorithm,\noffering the flexibility to adjust the decoding efficiency based on the\nrendering precision. Extensive experiments demonstrate that converting existing\nINR-based ASSR methods to LMF can reduce the computational cost by up to 99.9%,\naccelerate inference by up to 57 times, and save up to 76% of parameters, while\nmaintaining competitive performance. The code is available at\nhttps://github.com/HeZongyao/LMF.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.16451v1",
    "published_date": "2024-04-25 09:30:38 UTC",
    "updated_date": "2024-04-25 09:30:38 UTC"
  },
  {
    "arxiv_id": "2404.16442v1",
    "title": "Contextual Categorization Enhancement through LLMs Latent-Space",
    "authors": [
      "Zineddine Bettouche",
      "Anas Safi",
      "Andreas Fischer"
    ],
    "abstract": "Managing the semantic quality of the categorization in large textual\ndatasets, such as Wikipedia, presents significant challenges in terms of\ncomplexity and cost. In this paper, we propose leveraging transformer models to\ndistill semantic information from texts in the Wikipedia dataset and its\nassociated categories into a latent space. We then explore different approaches\nbased on these encodings to assess and enhance the semantic identity of the\ncategories. Our graphical approach is powered by Convex Hull, while we utilize\nHierarchical Navigable Small Worlds (HNSWs) for the hierarchical approach. As a\nsolution to the information loss caused by the dimensionality reduction, we\nmodulate the following mathematical solution: an exponential decay function\ndriven by the Euclidean distances between the high-dimensional encodings of the\ntextual categories. This function represents a filter built around a contextual\ncategory and retrieves items with a certain Reconsideration Probability (RP).\nRetrieving high-RP items serves as a tool for database administrators to\nimprove data groupings by providing recommendations and identifying outliers\nwithin a contextual framework.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.16442v1",
    "published_date": "2024-04-25 09:20:51 UTC",
    "updated_date": "2024-04-25 09:20:51 UTC"
  },
  {
    "arxiv_id": "2404.16903v1",
    "title": "Fiper: a Visual-based Explanation Combining Rules and Feature Importance",
    "authors": [
      "Eleonora Cappuccio",
      "Daniele Fadda",
      "Rosa Lanzilotti",
      "Salvatore Rinzivillo"
    ],
    "abstract": "Artificial Intelligence algorithms have now become pervasive in multiple\nhigh-stakes domains. However, their internal logic can be obscure to humans.\nExplainable Artificial Intelligence aims to design tools and techniques to\nillustrate the predictions of the so-called black-box algorithms. The\nHuman-Computer Interaction community has long stressed the need for a more\nuser-centered approach to Explainable AI. This approach can benefit from\nresearch in user interface, user experience, and visual analytics. This paper\nproposes a visual-based method to illustrate rules paired with feature\nimportance. A user study with 15 participants was conducted comparing our\nvisual method with the original output of the algorithm and textual\nrepresentation to test its effectiveness with users.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "I.2.0"
    ],
    "primary_category": "cs.HC",
    "comment": "15 pages, 4 figures, to be published in ECML PKDD International\n  Workshop on eXplainable Knowledge Discovery in Data Mining",
    "pdf_url": "http://arxiv.org/pdf/2404.16903v1",
    "published_date": "2024-04-25 09:15:54 UTC",
    "updated_date": "2024-04-25 09:15:54 UTC"
  },
  {
    "arxiv_id": "2404.16436v2",
    "title": "Leveraging tropical reef, bird and unrelated sounds for superior transfer learning in marine bioacoustics",
    "authors": [
      "Ben Williams",
      "Bart van Merriënboer",
      "Vincent Dumoulin",
      "Jenny Hamer",
      "Eleni Triantafillou",
      "Abram B. Fleishman",
      "Matthew McKown",
      "Jill E. Munger",
      "Aaron N. Rice",
      "Ashlee Lillis",
      "Clemency E. White",
      "Catherine A. D. Hobbs",
      "Tries B. Razak",
      "Kate E. Jones",
      "Tom Denton"
    ],
    "abstract": "Machine learning has the potential to revolutionize passive acoustic\nmonitoring (PAM) for ecological assessments. However, high annotation and\ncompute costs limit the field's efficacy. Generalizable pretrained networks can\novercome these costs, but high-quality pretraining requires vast annotated\nlibraries, limiting its current applicability primarily to bird taxa. Here, we\nidentify the optimum pretraining strategy for a data-deficient domain using\ncoral reef bioacoustics. We assemble ReefSet, a large annotated library of reef\nsounds, though modest compared to bird libraries at 2% of the sample count.\nThrough testing few-shot transfer learning performance, we observe that\npretraining on bird audio provides notably superior generalizability compared\nto pretraining on ReefSet or unrelated audio alone. However, our key findings\nshow that cross-domain mixing which leverages bird, reef and unrelated audio\nduring pretraining maximizes reef generalizability. SurfPerch, our pretrained\nnetwork, provides a strong foundation for automated analysis of marine PAM data\nwith minimal annotation and compute costs.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "18 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.16436v2",
    "published_date": "2024-04-25 09:12:35 UTC",
    "updated_date": "2024-05-07 12:42:32 UTC"
  },
  {
    "arxiv_id": "2406.10232v1",
    "title": "Object criticality for safer navigation",
    "authors": [
      "Andrea Ceccarelli",
      "Leonardo Montecchi"
    ],
    "abstract": "Object detection in autonomous driving consists in perceiving and locating\ninstances of objects in multi-dimensional data, such as images or lidar scans.\nVery recently, multiple works are proposing to evaluate object detectors by\nmeasuring their ability to detect the objects that are most likely to interfere\nwith the driving task. Detectors are then ranked according to their ability to\ndetect the most relevant objects, rather than the highest number of objects.\nHowever there is little evidence so far that the relevance of predicted object\nmay contribute to the safety and reliability improvement of the driving task.\nThis position paper elaborates on a strategy, together with partial results, to\ni) configure and deploy object detectors that successfully extract knowledge on\nobject relevance, and ii) use such knowledge to improve the trajectory planning\ntask. We show that, given an object detector, filtering objects based on their\nrelevance, in combination with the traditional confidence threshold, reduces\nthe risk of missing relevant objects, decreases the likelihood of dangerous\ntrajectories, and improves the quality of trajectories in general.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "position paper with initial results",
    "pdf_url": "http://arxiv.org/pdf/2406.10232v1",
    "published_date": "2024-04-25 09:02:22 UTC",
    "updated_date": "2024-04-25 09:02:22 UTC"
  },
  {
    "arxiv_id": "2404.16417v1",
    "title": "Constructing Optimal Noise Channels for Enhanced Robustness in Quantum Machine Learning",
    "authors": [
      "David Winderl",
      "Nicola Franco",
      "Jeanette Miriam Lorenz"
    ],
    "abstract": "With the rapid advancement of Quantum Machine Learning (QML), the critical\nneed to enhance security measures against adversarial attacks and protect QML\nmodels becomes increasingly evident. In this work, we outline the connection\nbetween quantum noise channels and differential privacy (DP), by constructing a\nfamily of noise channels which are inherently $\\epsilon$-DP: $(\\alpha,\n\\gamma)$-channels. Through this approach, we successfully replicate the\n$\\epsilon$-DP bounds observed for depolarizing and random rotation channels,\nthereby affirming the broad generality of our framework. Additionally, we use a\nsemi-definite program to construct an optimally robust channel. In a\nsmall-scale experimental evaluation, we demonstrate the benefits of using our\noptimal noise channel over depolarizing noise, particularly in enhancing\nadversarial accuracy. Moreover, we assess how the variables $\\alpha$ and\n$\\gamma$ affect the certifiable robustness and investigate how different\nencoding methods impact the classifier's robustness.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "quant-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.16417v1",
    "published_date": "2024-04-25 08:49:29 UTC",
    "updated_date": "2024-04-25 08:49:29 UTC"
  },
  {
    "arxiv_id": "2404.16411v1",
    "title": "Label-Free Topic-Focused Summarization Using Query Augmentation",
    "authors": [
      "Wenchuan Mu",
      "Kwan Hui Lim"
    ],
    "abstract": "In today's data and information-rich world, summarization techniques are\nessential in harnessing vast text to extract key information and enhance\ndecision-making and efficiency. In particular, topic-focused summarization is\nimportant due to its ability to tailor content to specific aspects of an\nextended text. However, this usually requires extensive labelled datasets and\nconsiderable computational power. This study introduces a novel method,\nAugmented-Query Summarization (AQS), for topic-focused summarization without\nthe need for extensive labelled datasets, leveraging query augmentation and\nhierarchical clustering. This approach facilitates the transferability of\nmachine learning models to the task of summarization, circumventing the need\nfor topic-specific training. Through real-world tests, our method demonstrates\nthe ability to generate relevant and accurate summaries, showing its potential\nas a cost-effective solution in data-rich environments. This innovation paves\nthe way for broader application and accessibility in the field of topic-focused\nsummarization technology, offering a scalable, efficient method for\npersonalized content extraction.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.16411v1",
    "published_date": "2024-04-25 08:39:10 UTC",
    "updated_date": "2024-04-25 08:39:10 UTC"
  },
  {
    "arxiv_id": "2404.16399v2",
    "title": "Offline Reinforcement Learning with Behavioral Supervisor Tuning",
    "authors": [
      "Padmanaba Srinivasan",
      "William Knottenbelt"
    ],
    "abstract": "Offline reinforcement learning (RL) algorithms are applied to learn\nperformant, well-generalizing policies when provided with a static dataset of\ninteractions. Many recent approaches to offline RL have seen substantial\nsuccess, but with one key caveat: they demand substantial per-dataset\nhyperparameter tuning to achieve reported performance, which requires policy\nrollouts in the environment to evaluate; this can rapidly become cumbersome.\nFurthermore, substantial tuning requirements can hamper the adoption of these\nalgorithms in practical domains. In this paper, we present TD3 with Behavioral\nSupervisor Tuning (TD3-BST), an algorithm that trains an uncertainty model and\nuses it to guide the policy to select actions within the dataset support.\nTD3-BST can learn more effective policies from offline datasets compared to\nprevious methods and achieves the best performance across challenging\nbenchmarks without requiring per-dataset tuning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.16399v2",
    "published_date": "2024-04-25 08:22:47 UTC",
    "updated_date": "2024-07-27 07:13:30 UTC"
  },
  {
    "arxiv_id": "2404.16395v1",
    "title": "Fuzzy Inference System for Test Case Prioritization in Software Testing",
    "authors": [
      "Aron Karatayev",
      "Anna Ogorodova",
      "Pakizar Shamoi"
    ],
    "abstract": "In the realm of software development, testing is crucial for ensuring\nsoftware quality and adherence to requirements. However, it can be\ntime-consuming and resource-intensive, especially when dealing with large and\ncomplex software systems. Test case prioritization (TCP) is a vital strategy to\nenhance testing efficiency by identifying the most critical test cases for\nearly execution. This paper introduces a novel fuzzy logic-based approach to\nautomate TCP, using fuzzy linguistic variables and expert-derived fuzzy rules\nto establish a link between test case characteristics and their prioritization.\nOur methodology utilizes two fuzzy variables - failure rate and execution time\n- alongside two crisp parameters: Prerequisite Test Case and Recently Updated\nFlag. Our findings demonstrate the proposed system capacity to rank test cases\neffectively through experimental validation on a real-world software system.\nThe results affirm the practical applicability of our approach in optimizing\nthe TCP and reducing the resource intensity of software testing.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "The article has been submitted to IEEE for consideration",
    "pdf_url": "http://arxiv.org/pdf/2404.16395v1",
    "published_date": "2024-04-25 08:08:54 UTC",
    "updated_date": "2024-04-25 08:08:54 UTC"
  },
  {
    "arxiv_id": "2404.16388v1",
    "title": "SwarmRL: Building the Future of Smart Active Systems",
    "authors": [
      "Samuel Tovey",
      "Christoph Lohrmann",
      "Tobias Merkt",
      "David Zimmer",
      "Konstantin Nikolaou",
      "Simon Koppenhöfer",
      "Anna Bushmakina",
      "Jonas Scheunemann",
      "Christian Holm"
    ],
    "abstract": "This work introduces SwarmRL, a Python package designed to study intelligent\nactive particles. SwarmRL provides an easy-to-use interface for developing\nmodels to control microscopic colloids using classical control and deep\nreinforcement learning approaches. These models may be deployed in simulations\nor real-world environments under a common framework. We explain the structure\nof the software and its key features and demonstrate how it can be used to\naccelerate research. With SwarmRL, we aim to streamline research into\nmicro-robotic control while bridging the gap between experimental and\nsimulation-driven sciences. SwarmRL is available open-source on GitHub at\nhttps://github.com/SwarmRL/SwarmRL.",
    "categories": [
      "cs.RO",
      "cond-mat.soft",
      "cs.AI",
      "cs.MA",
      "physics.bio-ph"
    ],
    "primary_category": "cs.RO",
    "comment": "16 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.16388v1",
    "published_date": "2024-04-25 07:57:11 UTC",
    "updated_date": "2024-04-25 07:57:11 UTC"
  },
  {
    "arxiv_id": "2404.16379v2",
    "title": "Optimal and Bounded Suboptimal Any-Angle Multi-agent Pathfinding",
    "authors": [
      "Konstantin Yakovlev",
      "Anton Andreychuk",
      "Roni Stern"
    ],
    "abstract": "Multi-agent pathfinding (MAPF) is the problem of finding a set of\nconflict-free paths for a set of agents. Typically, the agents' moves are\nlimited to a pre-defined graph of possible locations and allowed transitions\nbetween them, e.g. a 4-neighborhood grid. We explore how to solve MAPF problems\nwhen each agent can move between any pair of possible locations as long as\ntraversing the line segment connecting them does not lead to a collision with\nthe obstacles. This is known as any-angle pathfinding. We present the first\noptimal any-angle multi-agent pathfinding algorithm. Our planner is based on\nthe Continuous Conflict-based Search (CCBS) algorithm and an optimal any-angle\nvariant of the Safe Interval Path Planning (TO-AA-SIPP). The straightforward\ncombination of those, however, scales poorly since any-angle path finding\ninduces search trees with a very large branching factor. To mitigate this, we\nadapt two techniques from classical MAPF to the any-angle setting, namely\nDisjoint Splitting and Multi-Constraints. Experimental results on different\ncombinations of these techniques show they enable solving over 30% more\nproblems than the vanilla combination of CCBS and TO-AA-SIPP. In addition, we\npresent a bounded-suboptimal variant of our algorithm, that enables trading\nruntime for solution cost in a controlled manner.",
    "categories": [
      "cs.AI",
      "cs.MA",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "This is a pre-print version of the paper accepted to IROS 2024. Its\n  main body is similar to the camera-ready version of the conference paper. In\n  addition this pre-print contains Appendix",
    "pdf_url": "http://arxiv.org/pdf/2404.16379v2",
    "published_date": "2024-04-25 07:41:47 UTC",
    "updated_date": "2024-08-30 12:42:41 UTC"
  },
  {
    "arxiv_id": "2404.16375v2",
    "title": "List Items One by One: A New Data Source and Learning Paradigm for Multimodal LLMs",
    "authors": [
      "An Yan",
      "Zhengyuan Yang",
      "Junda Wu",
      "Wanrong Zhu",
      "Jianwei Yang",
      "Linjie Li",
      "Kevin Lin",
      "Jianfeng Wang",
      "Julian McAuley",
      "Jianfeng Gao",
      "Lijuan Wang"
    ],
    "abstract": "Set-of-Mark (SoM) Prompting unleashes the visual grounding capability of\nGPT-4V, by enabling the model to associate visual objects with tags inserted on\nthe image. These tags, marked with alphanumerics, can be indexed via text\ntokens for easy reference. Despite the extraordinary performance from GPT-4V,\nwe observe that other Multimodal Large Language Models (MLLMs) struggle to\nunderstand these visual tags. To promote the learning of SoM prompting for\nopen-source models, we propose a new learning paradigm: \"list items one by\none,\" which asks the model to enumerate and describe all visual tags placed on\nthe image following the alphanumeric orders of tags. By integrating our curated\ndataset with other visual instruction tuning datasets, we are able to equip\nexisting MLLMs with the SoM prompting ability. Furthermore, we evaluate our\nfinetuned SoM models on five MLLM benchmarks. We find that this new dataset,\neven in a relatively small size (10k-30k images with tags), significantly\nenhances visual reasoning capabilities and reduces hallucinations for MLLMs.\nPerhaps surprisingly, these improvements persist even when the visual tags are\nomitted from input images during inference. This suggests the potential of\n\"list items one by one\" as a new paradigm for training MLLMs, which strengthens\nthe object-text alignment through the use of visual tags in the training stage.\nFinally, we conduct analyses by probing trained models to understand the\nworking mechanism of SoM. Our code and data are available at\n\\url{https://github.com/zzxslp/SoM-LLaVA}.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "published at COLM-2024",
    "pdf_url": "http://arxiv.org/pdf/2404.16375v2",
    "published_date": "2024-04-25 07:29:17 UTC",
    "updated_date": "2025-01-20 00:29:19 UTC"
  },
  {
    "arxiv_id": "2404.16366v1",
    "title": "Guarding Graph Neural Networks for Unsupervised Graph Anomaly Detection",
    "authors": [
      "Yuanchen Bei",
      "Sheng Zhou",
      "Jinke Shi",
      "Yao Ma",
      "Haishuai Wang",
      "Jiajun Bu"
    ],
    "abstract": "Unsupervised graph anomaly detection aims at identifying rare patterns that\ndeviate from the majority in a graph without the aid of labels, which is\nimportant for a variety of real-world applications. Recent advances have\nutilized Graph Neural Networks (GNNs) to learn effective node representations\nby aggregating information from neighborhoods. This is motivated by the\nhypothesis that nodes in the graph tend to exhibit consistent behaviors with\ntheir neighborhoods. However, such consistency can be disrupted by graph\nanomalies in multiple ways. Most existing methods directly employ GNNs to learn\nrepresentations, disregarding the negative impact of graph anomalies on GNNs,\nresulting in sub-optimal node representations and anomaly detection\nperformance. While a few recent approaches have redesigned GNNs for graph\nanomaly detection under semi-supervised label guidance, how to address the\nadverse effects of graph anomalies on GNNs in unsupervised scenarios and learn\neffective representations for anomaly detection are still under-explored. To\nbridge this gap, in this paper, we propose a simple yet effective framework for\nGuarding Graph Neural Networks for Unsupervised Graph Anomaly Detection (G3AD).\nSpecifically, G3AD introduces two auxiliary networks along with correlation\nconstraints to guard the GNNs from inconsistent information encoding.\nFurthermore, G3AD introduces an adaptive caching module to guard the GNNs from\nsolely reconstructing the observed data that contains anomalies. Extensive\nexperiments demonstrate that our proposed G3AD can outperform seventeen\nstate-of-the-art methods on both synthetic and real-world datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.16366v1",
    "published_date": "2024-04-25 07:09:05 UTC",
    "updated_date": "2024-04-25 07:09:05 UTC"
  },
  {
    "arxiv_id": "2404.16365v1",
    "title": "VISLA Benchmark: Evaluating Embedding Sensitivity to Semantic and Lexical Alterations",
    "authors": [
      "Sri Harsha Dumpala",
      "Aman Jaiswal",
      "Chandramouli Sastry",
      "Evangelos Milios",
      "Sageev Oore",
      "Hassan Sajjad"
    ],
    "abstract": "Despite their remarkable successes, state-of-the-art language models face\nchallenges in grasping certain important semantic details. This paper\nintroduces the VISLA (Variance and Invariance to Semantic and Lexical\nAlterations) benchmark, designed to evaluate the semantic and lexical\nunderstanding of language models. VISLA presents a 3-way semantic\n(in)equivalence task with a triplet of sentences associated with an image, to\nevaluate both vision-language models (VLMs) and unimodal language models\n(ULMs). An evaluation involving 34 VLMs and 20 ULMs reveals surprising\ndifficulties in distinguishing between lexical and semantic variations. Spatial\nsemantics encoded by language models also appear to be highly sensitive to\nlexical information. Notably, text encoders of VLMs demonstrate greater\nsensitivity to semantic and lexical variations than unimodal text encoders. Our\ncontributions include the unification of image-to-text and text-to-text\nretrieval tasks, an off-the-shelf evaluation without fine-tuning, and assessing\nLMs' semantic (in)variance in the presence of lexical alterations. The results\nhighlight strengths and weaknesses across diverse vision and unimodal language\nmodels, contributing to a deeper understanding of their capabilities. % VISLA\nenables a rigorous evaluation, shedding light on language models' capabilities\nin handling semantic and lexical nuances. Data and code will be made available\nat https://github.com/Sri-Harsha/visla_benchmark.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.16365v1",
    "published_date": "2024-04-25 07:08:00 UTC",
    "updated_date": "2024-04-25 07:08:00 UTC"
  },
  {
    "arxiv_id": "2404.16364v5",
    "title": "ReZero: Boosting MCTS-based Algorithms by Backward-view and Entire-buffer Reanalyze",
    "authors": [
      "Chunyu Xuan",
      "Yazhe Niu",
      "Yuan Pu",
      "Shuai Hu",
      "Yu Liu",
      "Jing Yang"
    ],
    "abstract": "Monte Carlo Tree Search (MCTS)-based algorithms, such as MuZero and its\nderivatives, have achieved widespread success in various decision-making\ndomains. These algorithms employ the reanalyze process to enhance sample\nefficiency from stale data, albeit at the expense of significant wall-clock\ntime consumption. To address this issue, we propose a general approach named\nReZero to boost tree search operations for MCTS-based algorithms. Specifically,\ndrawing inspiration from the one-armed bandit model, we reanalyze training\nsamples through a backward-view reuse technique which uses the value estimation\nof a certain child node to save the corresponding sub-tree search time. To\nfurther adapt to this design, we periodically reanalyze the entire buffer\ninstead of frequently reanalyzing the mini-batch. The synergy of these two\ndesigns can significantly reduce the search cost and meanwhile guarantee or\neven improve performance, simplifying both data collecting and reanalyzing.\nExperiments conducted on Atari environments, DMControl suites and board games\ndemonstrate that ReZero substantially improves training speed while maintaining\nhigh sample efficiency. The code is available as part of the LightZero MCTS\nbenchmark at https://github.com/opendilab/LightZero.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.16364v5",
    "published_date": "2024-04-25 07:02:07 UTC",
    "updated_date": "2024-12-31 16:30:03 UTC"
  },
  {
    "arxiv_id": "2404.16898v1",
    "title": "How to Parameterize Asymmetric Quantization Ranges for Quantization-Aware Training",
    "authors": [
      "Jaeseong You",
      "Minseop Park",
      "Kyunggeun Lee",
      "Seokjun An",
      "Chirag Patel",
      "Markus Nage"
    ],
    "abstract": "This paper investigates three different parameterizations of asymmetric\nuniform quantization for quantization-aware training: (1) scale and offset, (2)\nminimum and maximum, and (3) beta and gamma. We perform a comprehensive\ncomparative analysis of these parameterizations' influence on\nquantization-aware training, using both controlled experiments and real-world\nlarge language models. Our particular focus is on their changing behavior in\nresponse to critical training hyperparameters, bit width and learning rate.\nBased on our investigation, we propose best practices to stabilize and\naccelerate quantization-aware training with learnable asymmetric quantization\nranges.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.16898v1",
    "published_date": "2024-04-25 06:58:16 UTC",
    "updated_date": "2024-04-25 06:58:16 UTC"
  },
  {
    "arxiv_id": "2404.16356v1",
    "title": "Integration of Mixture of Experts and Multimodal Generative AI in Internet of Vehicles: A Survey",
    "authors": [
      "Minrui Xu",
      "Dusit Niyato",
      "Jiawen Kang",
      "Zehui Xiong",
      "Abbas Jamalipour",
      "Yuguang Fang",
      "Dong In Kim",
      "Xuemin",
      "Shen"
    ],
    "abstract": "Generative AI (GAI) can enhance the cognitive, reasoning, and planning\ncapabilities of intelligent modules in the Internet of Vehicles (IoV) by\nsynthesizing augmented datasets, completing sensor data, and making sequential\ndecisions. In addition, the mixture of experts (MoE) can enable the distributed\nand collaborative execution of AI models without performance degradation\nbetween connected vehicles. In this survey, we explore the integration of MoE\nand GAI to enable Artificial General Intelligence in IoV, which can enable the\nrealization of full autonomy for IoV with minimal human supervision and\napplicability in a wide range of mobility scenarios, including environment\nmonitoring, traffic management, and autonomous driving. In particular, we\npresent the fundamentals of GAI, MoE, and their interplay applications in IoV.\nFurthermore, we discuss the potential integration of MoE and GAI in IoV,\nincluding distributed perception and monitoring, collaborative decision-making\nand planning, and generative modeling and simulation. Finally, we present\nseveral potential research directions for facilitating the integration.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.16356v1",
    "published_date": "2024-04-25 06:22:21 UTC",
    "updated_date": "2024-04-25 06:22:21 UTC"
  },
  {
    "arxiv_id": "2404.16897v1",
    "title": "Exploring Learngene via Stage-wise Weight Sharing for Initializing Variable-sized Models",
    "authors": [
      "Shi-Yu Xia",
      "Wenxuan Zhu",
      "Xu Yang",
      "Xin Geng"
    ],
    "abstract": "In practice, we usually need to build variable-sized models adapting for\ndiverse resource constraints in different application scenarios, where weight\ninitialization is an important step prior to training. The Learngene framework,\nintroduced recently, firstly learns one compact part termed as learngene from a\nlarge well-trained model, after which learngene is expanded to initialize\nvariable-sized models. In this paper, we start from analysing the importance of\nguidance for the expansion of well-trained learngene layers, inspiring the\ndesign of a simple but highly effective Learngene approach termed SWS\n(Stage-wise Weight Sharing), where both learngene layers and their learning\nprocess critically contribute to providing knowledge and guidance for\ninitializing models at varying scales. Specifically, to learn learngene layers,\nwe build an auxiliary model comprising multiple stages where the layer weights\nin each stage are shared, after which we train it through distillation.\nSubsequently, we expand these learngene layers containing stage information at\ntheir corresponding stage to initialize models of variable depths. Extensive\nexperiments on ImageNet-1K demonstrate that SWS achieves consistent better\nperformance compared to many models trained from scratch, while reducing around\n6.6x total training costs. In some cases, SWS performs better only after 1\nepoch tuning. When initializing variable-sized models adapting for different\nresource constraints, SWS achieves better results while reducing around 20x\nparameters stored to initialize these models and around 10x pre-training costs,\nin contrast to the pre-training and fine-tuning approach.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.16897v1",
    "published_date": "2024-04-25 06:04:34 UTC",
    "updated_date": "2024-04-25 06:04:34 UTC"
  },
  {
    "arxiv_id": "2404.16346v1",
    "title": "Light-weight Retinal Layer Segmentation with Global Reasoning",
    "authors": [
      "Xiang He",
      "Weiye Song",
      "Yiming Wang",
      "Fabio Poiesi",
      "Ji Yi",
      "Manishi Desai",
      "Quanqing Xu",
      "Kongzheng Yang",
      "Yi Wan"
    ],
    "abstract": "Automatic retinal layer segmentation with medical images, such as optical\ncoherence tomography (OCT) images, serves as an important tool for diagnosing\nophthalmic diseases. However, it is challenging to achieve accurate\nsegmentation due to low contrast and blood flow noises presented in the images.\nIn addition, the algorithm should be light-weight to be deployed for practical\nclinical applications. Therefore, it is desired to design a light-weight\nnetwork with high performance for retinal layer segmentation. In this paper, we\npropose LightReSeg for retinal layer segmentation which can be applied to OCT\nimages. Specifically, our approach follows an encoder-decoder structure, where\nthe encoder part employs multi-scale feature extraction and a Transformer block\nfor fully exploiting the semantic information of feature maps at all scales and\nmaking the features have better global reasoning capabilities, while the\ndecoder part, we design a multi-scale asymmetric attention (MAA) module for\npreserving the semantic information at each encoder scale. The experiments show\nthat our approach achieves a better segmentation performance compared to the\ncurrent state-of-the-art method TransUnet with 105.7M parameters on both our\ncollected dataset and two other public datasets, with only 3.3M parameters.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "IEEE Transactions on Instrumentation & Measurement",
    "pdf_url": "http://arxiv.org/pdf/2404.16346v1",
    "published_date": "2024-04-25 05:42:41 UTC",
    "updated_date": "2024-04-25 05:42:41 UTC"
  },
  {
    "arxiv_id": "2405.02329v1",
    "title": "Digital ASIC Design with Ongoing LLMs: Strategies and Prospects",
    "authors": [
      "Maoyang Xiang",
      "Emil Goh",
      "T. Hui Teo"
    ],
    "abstract": "The escalating complexity of modern digital systems has imposed significant\nchallenges on integrated circuit (IC) design, necessitating tools that can\nsimplify the IC design flow. The advent of Large Language Models (LLMs) has\nbeen seen as a promising development, with the potential to automate the\ngeneration of Hardware Description Language (HDL) code, thereby streamlining\ndigital IC design. However, the practical application of LLMs in this area\nfaces substantial hurdles. Notably, current LLMs often generate HDL code with\nsmall but critical syntax errors and struggle to accurately convey the\nhigh-level semantics of circuit designs. These issues significantly undermine\nthe utility of LLMs for IC design, leading to misinterpretations and\ninefficiencies.\n  In response to these challenges, this paper presents targeted strategies to\nharness the capabilities of LLMs for digital ASIC design. We outline approaches\nthat improve the reliability and accuracy of HDL code generation by LLMs. As a\npractical demonstration of these strategies, we detail the development of a\nsimple three-phase Pulse Width Modulation (PWM) generator. This project, part\nof the \"Efabless AI-Generated Open-Source Chip Design Challenge,\" successfully\npassed the Design Rule Check (DRC) and was fabricated, showcasing the potential\nof LLMs to enhance digital ASIC design. This work underscores the feasibility\nand benefits of integrating LLMs into the IC design process, offering a novel\napproach to overcoming the complexities of modern digital systems.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AR",
    "comment": "8 pages, 2 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2405.02329v1",
    "published_date": "2024-04-25 05:16:57 UTC",
    "updated_date": "2024-04-25 05:16:57 UTC"
  },
  {
    "arxiv_id": "2407.01548v1",
    "title": "From Cognition to Computation: A Comparative Review of Human Attention and Transformer Architectures",
    "authors": [
      "Minglu Zhao",
      "Dehong Xu",
      "Tao Gao"
    ],
    "abstract": "Attention is a cornerstone of human cognition that facilitates the efficient\nextraction of information in everyday life. Recent developments in artificial\nintelligence like the Transformer architecture also incorporate the idea of\nattention in model designs. However, despite the shared fundamental principle\nof selectively attending to information, human attention and the Transformer\nmodel display notable differences, particularly in their capacity constraints,\nattention pathways, and intentional mechanisms. Our review aims to provide a\ncomparative analysis of these mechanisms from a cognitive-functional\nperspective, thereby shedding light on several open research questions. The\nexploration encourages interdisciplinary efforts to derive insights from human\nattention mechanisms in the pursuit of developing more generalized artificial\nintelligence.",
    "categories": [
      "q-bio.OT",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.OT",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01548v1",
    "published_date": "2024-04-25 05:13:38 UTC",
    "updated_date": "2024-04-25 05:13:38 UTC"
  },
  {
    "arxiv_id": "2404.16339v1",
    "title": "Training-Free Unsupervised Prompt for Vision-Language Models",
    "authors": [
      "Sifan Long",
      "Linbin Wang",
      "Zhen Zhao",
      "Zichang Tan",
      "Yiming Wu",
      "Shengsheng Wang",
      "Jingdong Wang"
    ],
    "abstract": "Prompt learning has become the most effective paradigm for adapting large\npre-trained vision-language models (VLMs) to downstream tasks. Recently,\nunsupervised prompt tuning methods, such as UPL and POUF, directly leverage\npseudo-labels as supervisory information to fine-tune additional adaptation\nmodules on unlabeled data. However, inaccurate pseudo labels easily misguide\nthe tuning process and result in poor representation capabilities. In light of\nthis, we propose Training-Free Unsupervised Prompts (TFUP), which maximally\npreserves the inherent representation capabilities and enhances them with a\nresidual connection to similarity-based prediction probabilities in a\ntraining-free and labeling-free manner. Specifically, we integrate both\ninstance confidence and prototype scores to select representative samples,\nwhich are used to customize a reliable Feature Cache Model (FCM) for\ntraining-free inference. Then, we design a Multi-level Similarity Measure (MSM)\nthat considers both feature-level and semantic-level similarities to calculate\nthe distance between each test image and the cached sample as the weight of the\ncorresponding cached label to generate similarity-based prediction\nprobabilities. In this way, TFUP achieves surprising performance, even\nsurpassing the training-base method on multiple classification datasets. Based\non our TFUP, we propose a training-based approach (TFUP-T) to further boost the\nadaptation performance. In addition to the standard cross-entropy loss, TFUP-T\nadopts an additional marginal distribution entropy loss to constrain the model\nfrom a global perspective. Our TFUP-T achieves new state-of-the-art\nclassification performance compared to unsupervised and few-shot adaptation\napproaches on multiple benchmarks. In particular, TFUP-T improves the\nclassification accuracy of POUF by 3.3% on the most challenging Domain-Net\ndataset.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.16339v1",
    "published_date": "2024-04-25 05:07:50 UTC",
    "updated_date": "2024-04-25 05:07:50 UTC"
  },
  {
    "arxiv_id": "2404.16333v2",
    "title": "AI Coders Are Among Us: Rethinking Programming Language Grammar Towards Efficient Code Generation",
    "authors": [
      "Zhensu Sun",
      "Xiaoning Du",
      "Zhou Yang",
      "Li Li",
      "David Lo"
    ],
    "abstract": "Artificial Intelligence (AI) models have emerged as another important\naudience for programming languages alongside humans and machines, as we enter\nthe era of large language models (LLMs). LLMs can now perform well in coding\ncompetitions and even write programs like developers to solve various tasks,\nincluding mathematical problems. However, the grammar and layout of current\nprograms are designed to cater the needs of human developers -- with many\ngrammar tokens and formatting tokens being used to make the code easier for\nhumans to read. While this is helpful, such a design adds unnecessary\ncomputational work for LLMs, as each token they either use or produce consumes\ncomputational resources. To improve inference efficiency and reduce\ncomputational costs, we propose the concept of AI-oriented grammar. This aims\nto represent code in a way that better suits the working mechanism of AI\nmodels. Code written with AI-oriented grammar discards formats and uses a\nminimum number of tokens to convey code semantics effectively. To demonstrate\nthe feasibility of this concept, we explore and implement the first AI-oriented\ngrammar for Python, named SimPy. SimPy is crafted by revising the original\nPython grammar through a series of heuristic rules. Programs written in SimPy\nmaintain identical AST structures to those in standard Python. This allows for\nnot only execution via a modified AST parser, but also seamless transformation\nbetween programs written in Python and SimPy, enabling human developers and\nLLMs to use Python and SimPy, respectively, when they need to collaborate. In\nthe experiments, compared with Python, SimPy enables a reduction in token usage\nby 13.5% and 10.4% for CodeLlama and GPT-4, respectively, when completing the\nsame set of code-related tasks. Additionally, these models can maintain or even\nimprove their performance when using SimPy instead of Python for these tasks.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.PL"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted by ISSTA'24",
    "pdf_url": "http://arxiv.org/pdf/2404.16333v2",
    "published_date": "2024-04-25 04:46:02 UTC",
    "updated_date": "2024-08-14 07:34:18 UTC"
  },
  {
    "arxiv_id": "2405.00711v2",
    "title": "Fake Artificial Intelligence Generated Contents (FAIGC): A Survey of Theories, Detection Methods, and Opportunities",
    "authors": [
      "Xiaomin Yu",
      "Yezhaohui Wang",
      "Yanfang Chen",
      "Zhen Tao",
      "Dinghao Xi",
      "Shichao Song",
      "Simin Niu",
      "Zhiyu Li"
    ],
    "abstract": "In recent years, generative artificial intelligence models, represented by\nLarge Language Models (LLMs) and Diffusion Models (DMs), have revolutionized\ncontent production methods. These artificial intelligence-generated content\n(AIGC) have become deeply embedded in various aspects of daily life and work.\nHowever, these technologies have also led to the emergence of Fake Artificial\nIntelligence Generated Content (FAIGC), posing new challenges in distinguishing\ngenuine information. It is crucial to recognize that AIGC technology is akin to\na double-edged sword; its potent generative capabilities, while beneficial,\nalso pose risks for the creation and dissemination of FAIGC. In this survey, We\npropose a new taxonomy that provides a more comprehensive breakdown of the\nspace of FAIGC methods today. Next, we explore the modalities and generative\ntechnologies of FAIGC. We introduce FAIGC detection methods and summarize the\nrelated benchmark from various perspectives. Finally, we discuss outstanding\nchallenges and promising areas for future research.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.00711v2",
    "published_date": "2024-04-25 04:44:09 UTC",
    "updated_date": "2024-05-03 04:47:01 UTC"
  },
  {
    "arxiv_id": "2404.16331v2",
    "title": "IMWA: Iterative Model Weight Averaging Benefits Class-Imbalanced Learning Tasks",
    "authors": [
      "Zitong Huang",
      "Ze Chen",
      "Bowen Dong",
      "Chaoqi Liang",
      "Erjin Zhou",
      "Wangmeng Zuo"
    ],
    "abstract": "Model Weight Averaging (MWA) is a technique that seeks to enhance model's\nperformance by averaging the weights of multiple trained models. This paper\nfirst empirically finds that 1) the vanilla MWA can benefit the\nclass-imbalanced learning, and 2) performing model averaging in the early\nepochs of training yields a greater performance improvement than doing that in\nlater epochs. Inspired by these two observations, in this paper we propose a\nnovel MWA technique for class-imbalanced learning tasks named Iterative Model\nWeight Averaging (IMWA). Specifically, IMWA divides the entire training stage\ninto multiple episodes. Within each episode, multiple models are concurrently\ntrained from the same initialized model weight, and subsequently averaged into\na singular model. Then, the weight of this average model serves as a fresh\ninitialization for the ensuing episode, thus establishing an iterative learning\nparadigm. Compared to vanilla MWA, IMWA achieves higher performance\nimprovements with the same computational cost. Moreover, IMWA can further\nenhance the performance of those methods employing EMA strategy, demonstrating\nthat IMWA and EMA can complement each other. Extensive experiments on various\nclass-imbalanced learning tasks, i.e., class-imbalanced image classification,\nsemi-supervised class-imbalanced image classification and semi-supervised\nobject detection tasks showcase the effectiveness of our IMWA.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.16331v2",
    "published_date": "2024-04-25 04:37:35 UTC",
    "updated_date": "2024-12-04 07:47:10 UTC"
  },
  {
    "arxiv_id": "2404.16325v1",
    "title": "Semantic Segmentation Refiner for Ultrasound Applications with Zero-Shot Foundation Models",
    "authors": [
      "Hedda Cohen Indelman",
      "Elay Dahan",
      "Angeles M. Perez-Agosto",
      "Carmit Shiran",
      "Doron Shaked",
      "Nati Daniel"
    ],
    "abstract": "Despite the remarkable success of deep learning in medical imaging analysis,\nmedical image segmentation remains challenging due to the scarcity of\nhigh-quality labeled images for supervision. Further, the significant domain\ngap between natural and medical images in general and ultrasound images in\nparticular hinders fine-tuning models trained on natural images to the task at\nhand. In this work, we address the performance degradation of segmentation\nmodels in low-data regimes and propose a prompt-less segmentation method\nharnessing the ability of segmentation foundation models to segment abstract\nshapes. We do that via our novel prompt point generation algorithm which uses\ncoarse semantic segmentation masks as input and a zero-shot prompt-able\nfoundation model as an optimization target. We demonstrate our method on a\nsegmentation findings task (pathologic anomalies) in ultrasound images. Our\nmethod's advantages are brought to light in varying degrees of low-data regime\nexperiments on a small-scale musculoskeletal ultrasound images dataset,\nyielding a larger performance gain as the training set size decreases.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.16325v1",
    "published_date": "2024-04-25 04:21:57 UTC",
    "updated_date": "2024-04-25 04:21:57 UTC"
  },
  {
    "arxiv_id": "2404.16297v1",
    "title": "When Fuzzing Meets LLMs: Challenges and Opportunities",
    "authors": [
      "Yu Jiang",
      "Jie Liang",
      "Fuchen Ma",
      "Yuanliang Chen",
      "Chijin Zhou",
      "Yuheng Shen",
      "Zhiyong Wu",
      "Jingzhou Fu",
      "Mingzhe Wang",
      "ShanShan Li",
      "Quan Zhang"
    ],
    "abstract": "Fuzzing, a widely-used technique for bug detection, has seen advancements\nthrough Large Language Models (LLMs). Despite their potential, LLMs face\nspecific challenges in fuzzing. In this paper, we identified five major\nchallenges of LLM-assisted fuzzing. To support our findings, we revisited the\nmost recent papers from top-tier conferences, confirming that these challenges\nare widespread. As a remedy, we propose some actionable recommendations to help\nimprove applying LLM in Fuzzing and conduct preliminary evaluations on DBMS\nfuzzing. The results demonstrate that our recommendations effectively address\nthe identified challenges.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.16297v1",
    "published_date": "2024-04-25 02:37:56 UTC",
    "updated_date": "2024-04-25 02:37:56 UTC"
  },
  {
    "arxiv_id": "2404.16296v3",
    "title": "Research on Splicing Image Detection Algorithms Based on Natural Image Statistical Characteristics",
    "authors": [
      "Ao Xiang",
      "Jingyu Zhang",
      "Qin Yang",
      "Liyang Wang",
      "Yu Cheng"
    ],
    "abstract": "With the development and widespread application of digital image processing\ntechnology, image splicing has become a common method of image manipulation,\nraising numerous security and legal issues. This paper introduces a new\nsplicing image detection algorithm based on the statistical characteristics of\nnatural images, aimed at improving the accuracy and efficiency of splicing\nimage detection. By analyzing the limitations of traditional methods, we have\ndeveloped a detection framework that integrates advanced statistical analysis\ntechniques and machine learning methods. The algorithm has been validated using\nmultiple public datasets, showing high accuracy in detecting spliced edges and\nlocating tampered areas, as well as good robustness. Additionally, we explore\nthe potential applications and challenges faced by the algorithm in real-world\nscenarios. This research not only provides an effective technological means for\nthe field of image tampering detection but also offers new ideas and methods\nfor future related research.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.16296v3",
    "published_date": "2024-04-25 02:28:16 UTC",
    "updated_date": "2024-05-17 13:14:30 UTC"
  },
  {
    "arxiv_id": "2404.16294v1",
    "title": "LLM-Based Section Identifiers Excel on Open Source but Stumble in Real World Applications",
    "authors": [
      "Saranya Krishnamoorthy",
      "Ayush Singh",
      "Shabnam Tafreshi"
    ],
    "abstract": "Electronic health records (EHR) even though a boon for healthcare\npractitioners, are growing convoluted and longer every day. Sifting around\nthese lengthy EHRs is taxing and becomes a cumbersome part of physician-patient\ninteraction. Several approaches have been proposed to help alleviate this\nprevalent issue either via summarization or sectioning, however, only a few\napproaches have truly been helpful in the past. With the rise of automated\nmethods, machine learning (ML) has shown promise in solving the task of\nidentifying relevant sections in EHR. However, most ML methods rely on labeled\ndata which is difficult to get in healthcare. Large language models (LLMs) on\nthe other hand, have performed impressive feats in natural language processing\n(NLP), that too in a zero-shot manner, i.e. without any labeled data. To that\nend, we propose using LLMs to identify relevant section headers. We find that\nGPT-4 can effectively solve the task on both zero and few-shot settings as well\nas segment dramatically better than state-of-the-art methods. Additionally, we\nalso annotate a much harder real world dataset and find that GPT-4 struggles to\nperform well, alluding to further research and harder benchmarks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "To appear in NAACL 2024 at the 6th Clinical Natural Language\n  Processing Workshop",
    "pdf_url": "http://arxiv.org/pdf/2404.16294v1",
    "published_date": "2024-04-25 02:25:35 UTC",
    "updated_date": "2024-04-25 02:25:35 UTC"
  },
  {
    "arxiv_id": "2404.16894v3",
    "title": "On TinyML and Cybersecurity: Electric Vehicle Charging Infrastructure Use Case",
    "authors": [
      "Fatemeh Dehrouyeh",
      "Li Yang",
      "Firouz Badrkhani Ajaei",
      "Abdallah Shami"
    ],
    "abstract": "As technology advances, the use of Machine Learning (ML) in cybersecurity is\nbecoming increasingly crucial to tackle the growing complexity of cyber\nthreats. While traditional ML models can enhance cybersecurity, their high\nenergy and resource demands limit their applications, leading to the emergence\nof Tiny Machine Learning (TinyML) as a more suitable solution for\nresource-constrained environments. TinyML is widely applied in areas such as\nsmart homes, healthcare, and industrial automation. TinyML focuses on\noptimizing ML algorithms for small, low-power devices, enabling intelligent\ndata processing directly on edge devices. This paper provides a comprehensive\nreview of common challenges of TinyML techniques, such as power consumption,\nlimited memory, and computational constraints; it also explores potential\nsolutions to these challenges, such as energy harvesting, computational\noptimization techniques, and transfer learning for privacy preservation. On the\nother hand, this paper discusses TinyML's applications in advancing\ncybersecurity for Electric Vehicle Charging Infrastructures (EVCIs) as a\nrepresentative use case. It presents an experimental case study that enhances\ncybersecurity in EVCI using TinyML, evaluated against traditional ML in terms\nof reduced delay and memory usage, with a slight trade-off in accuracy.\nAdditionally, the study includes a practical setup using the ESP32\nmicrocontroller in the PlatformIO environment, which provides a hands-on\nassessment of TinyML's application in cybersecurity for EVCI.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted and to appear in IEEE Access; Code is available at GitHub\n  link: https://github.com/Western-OC2-Lab/TinyML_EVCI",
    "pdf_url": "http://arxiv.org/pdf/2404.16894v3",
    "published_date": "2024-04-25 01:57:11 UTC",
    "updated_date": "2024-07-26 16:25:15 UTC"
  },
  {
    "arxiv_id": "2404.16280v1",
    "title": "An Efficient Reconstructed Differential Evolution Variant by Some of the Current State-of-the-art Strategies for Solving Single Objective Bound Constrained Problems",
    "authors": [
      "Sichen Tao",
      "Ruihan Zhao",
      "Kaiyu Wang",
      "Shangce Gao"
    ],
    "abstract": "Complex single-objective bounded problems are often difficult to solve. In\nevolutionary computation methods, since the proposal of differential evolution\nalgorithm in 1997, it has been widely studied and developed due to its\nsimplicity and efficiency. These developments include various adaptive\nstrategies, operator improvements, and the introduction of other search\nmethods. After 2014, research based on LSHADE has also been widely studied by\nresearchers. However, although recently proposed improvement strategies have\nshown superiority over their previous generation's first performance, adding\nall new strategies may not necessarily bring the strongest performance.\nTherefore, we recombine some effective advances based on advanced differential\nevolution variants in recent years and finally determine an effective\ncombination scheme to further promote the performance of differential\nevolution. In this paper, we propose a strategy recombination and\nreconstruction differential evolution algorithm called reconstructed\ndifferential evolution (RDE) to solve single-objective bounded optimization\nproblems. Based on the benchmark suite of the 2024 IEEE Congress on\nEvolutionary Computation (CEC2024), we tested RDE and several other advanced\ndifferential evolution variants. The experimental results show that RDE has\nsuperior performance in solving complex optimization problems.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.16280v1",
    "published_date": "2024-04-25 01:48:44 UTC",
    "updated_date": "2024-04-25 01:48:44 UTC"
  },
  {
    "arxiv_id": "2405.05134v1",
    "title": "Enhancing Deep Knowledge Tracing via Diffusion Models for Personalized Adaptive Learning",
    "authors": [
      "Ming Kuo",
      "Shouvon Sarker",
      "Lijun Qian",
      "Yujian Fu",
      "Xiangfang Li",
      "Xishuang Dong"
    ],
    "abstract": "In contrast to pedagogies like evidence-based teaching, personalized adaptive\nlearning (PAL) distinguishes itself by closely monitoring the progress of\nindividual students and tailoring the learning path to their unique knowledge\nand requirements. A crucial technique for effective PAL implementation is\nknowledge tracing, which models students' evolving knowledge to predict their\nfuture performance. Based on these predictions, personalized recommendations\nfor resources and learning paths can be made to meet individual needs. Recent\nadvancements in deep learning have successfully enhanced knowledge tracking\nthrough Deep Knowledge Tracing (DKT). This paper introduces generative AI\nmodels to further enhance DKT. Generative AI models, rooted in deep learning,\nare trained to generate synthetic data, addressing data scarcity challenges in\nvarious applications across fields such as natural language processing (NLP)\nand computer vision (CV). This study aims to tackle data shortage issues in\nstudent learning records to enhance DKT performance for PAL. Specifically, it\nemploys TabDDPM, a diffusion model, to generate synthetic educational records\nto augment training data for enhancing DKT. The proposed method's effectiveness\nis validated through extensive experiments on ASSISTments datasets. The\nexperimental results demonstrate that the AI-generated data by TabDDPM\nsignificantly improves DKT performance, particularly in scenarios with small\ndata for training and large data for testing.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.05134v1",
    "published_date": "2024-04-25 00:23:20 UTC",
    "updated_date": "2024-04-25 00:23:20 UTC"
  },
  {
    "arxiv_id": "2404.16260v1",
    "title": "OmniSearchSage: Multi-Task Multi-Entity Embeddings for Pinterest Search",
    "authors": [
      "Prabhat Agarwal",
      "Minhazul Islam Sk",
      "Nikil Pancha",
      "Kurchi Subhra Hazra",
      "Jiajing Xu",
      "Chuck Rosenberg"
    ],
    "abstract": "In this paper, we present OmniSearchSage, a versatile and scalable system for\nunderstanding search queries, pins, and products for Pinterest search. We\njointly learn a unified query embedding coupled with pin and product\nembeddings, leading to an improvement of $>8\\%$ relevance, $>7\\%$ engagement,\nand $>5\\%$ ads CTR in Pinterest's production search system. The main\ncontributors to these gains are improved content understanding, better\nmulti-task learning, and real-time serving. We enrich our entity\nrepresentations using diverse text derived from image captions from a\ngenerative LLM, historical engagement, and user-curated boards. Our multitask\nlearning setup produces a single search query embedding in the same space as\npin and product embeddings and compatible with pre-existing pin and product\nembeddings. We show the value of each feature through ablation studies, and\nshow the effectiveness of a unified model compared to standalone counterparts.\nFinally, we share how these embeddings have been deployed across the Pinterest\nsearch stack, from retrieval to ranking, scaling to serve $300k$ requests per\nsecond at low latency. Our implementation of this work is available at\nhttps://github.com/pinterest/atg-research/tree/main/omnisearchsage.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG",
      "H.3.3"
    ],
    "primary_category": "cs.IR",
    "comment": "8 pages, 5 figures, to be published as an oral paper in TheWebConf\n  Industry Track 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.16260v1",
    "published_date": "2024-04-25 00:10:25 UTC",
    "updated_date": "2024-04-25 00:10:25 UTC"
  },
  {
    "arxiv_id": "2404.16257v2",
    "title": "Translation of Multifaceted Data without Re-Training of Machine Translation Systems",
    "authors": [
      "Hyeonseok Moon",
      "Seungyoon Lee",
      "Seongtae Hong",
      "Seungjun Lee",
      "Chanjun Park",
      "Heuiseok Lim"
    ],
    "abstract": "Translating major language resources to build minor language resources\nbecomes a widely-used approach. Particularly in translating complex data points\ncomposed of multiple components, it is common to translate each component\nseparately. However, we argue that this practice often overlooks the\ninterrelation between components within the same data point. To address this\nlimitation, we propose a novel MT pipeline that considers the intra-data\nrelation in implementing MT for training data. In our MT pipeline, all the\ncomponents in a data point are concatenated to form a single translation\nsequence and subsequently reconstructed to the data components after\ntranslation. We introduce a Catalyst Statement (CS) to enhance the intra-data\nrelation, and Indicator Token (IT) to assist the decomposition of a translated\nsequence into its respective data components. Through our approach, we have\nachieved a considerable improvement in translation quality itself, along with\nits effectiveness as training data. Compared with the conventional approach\nthat translates each data component separately, our method yields better\ntraining data that enhances the performance of the trained model by 2.690\npoints for the web page ranking (WPR) task, and 0.845 for the question\ngeneration (QG) task in the XGLUE benchmark.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP2024 findings",
    "pdf_url": "http://arxiv.org/pdf/2404.16257v2",
    "published_date": "2024-04-25 00:05:19 UTC",
    "updated_date": "2024-09-25 02:15:32 UTC"
  }
]
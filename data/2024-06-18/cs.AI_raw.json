[
  {
    "arxiv_id": "2406.13106v3",
    "title": "Accelerating Complex Disease Treatment through Network Medicine and GenAI: A Case Study on Drug Repurposing for Breast Cancer",
    "authors": [
      "Ahmed Abdeen Hamed",
      "Tamer E. Fandy"
    ],
    "abstract": "The objective of this research is to introduce a network specialized in\npredicting drugs that can be repurposed by investigating real-world evidence\nsources, such as clinical trials and biomedical literature. Specifically, it\naims to generate drug combination therapies for complex diseases (e.g., cancer,\nAlzheimer's). We present a multilayered network medicine approach, empowered by\na highly configured ChatGPT prompt engineering system, which is constructed on\nthe fly to extract drug mentions in clinical trials. Additionally, we introduce\na novel algorithm that connects real-world evidence with disease-specific\nsignaling pathways (e.g., KEGG database). This sheds light on the\nrepurposability of drugs if they are found to bind with one or more protein\nconstituents of a signaling pathway. To demonstrate, we instantiated the\nframework for breast cancer and found that, out of 46 breast cancer signaling\npathways, the framework identified 38 pathways that were covered by at least\ntwo drugs. This evidence signals the potential for combining those drugs.\nSpecifically, the most covered signaling pathway, ID hsa:2064, was covered by\n108 drugs, some of which can be combined. Conversely, the signaling pathway ID\nhsa:1499 was covered by only two drugs, indicating a significant gap for\nfurther research. Our network medicine framework, empowered by GenAI, shows\npromise in identifying drug combinations with a high degree of specificity,\nknowing the exact signaling pathways and proteins that serve as targets. It is\nnoteworthy that ChatGPT successfully accelerated the process of identifying\ndrug mentions in clinical trials, though further investigations are required to\ndetermine the relationships among the drug mentions.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.IR",
      "I.2; I.2.6"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages double columns, 5 figures, 3 algorithms, 3 tables, and 1\n  listing, Submitted to IEEE MedAI'24 Conference, to be held November 15-17,\n  Chongqing, China",
    "pdf_url": "http://arxiv.org/pdf/2406.13106v3",
    "published_date": "2024-06-18 23:40:00 UTC",
    "updated_date": "2024-06-27 08:28:51 UTC"
  },
  {
    "arxiv_id": "2406.13103v2",
    "title": "A Generic Method for Fine-grained Category Discovery in Natural Language Texts",
    "authors": [
      "Chang Tian",
      "Matthew B. Blaschko",
      "Wenpeng Yin",
      "Mingzhe Xing",
      "Yinliang Yue",
      "Marie-Francine Moens"
    ],
    "abstract": "Fine-grained category discovery using only coarse-grained supervision is a\ncost-effective yet challenging task. Previous training methods focus on\naligning query samples with positive samples and distancing them from\nnegatives. They often neglect intra-category and inter-category semantic\nsimilarities of fine-grained categories when navigating sample distributions in\nthe embedding space. Furthermore, some evaluation techniques that rely on\npre-collected test samples are inadequate for real-time applications. To\naddress these shortcomings, we introduce a method that successfully detects\nfine-grained clusters of semantically similar texts guided by a novel objective\nfunction. The method uses semantic similarities in a logarithmic space to guide\nsample distributions in the Euclidean space and to form distinct clusters that\nrepresent fine-grained categories. We also propose a centroid inference\nmechanism to support real-time applications. The efficacy of the method is both\ntheoretically justified and empirically confirmed on three benchmark tasks. The\nproposed objective function is integrated in multiple contrastive learning\nbased neural models. Its results surpass existing state-of-the-art approaches\nin terms of Accuracy, Adjusted Rand Index and Normalized Mutual Information of\nthe detected fine-grained categories. Code and data will be available at Code\nand data are publicly available at\nhttps://github.com/changtianluckyforever/F-grained-STAR.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "contrastive learning, self-supervised learning",
    "pdf_url": "http://arxiv.org/pdf/2406.13103v2",
    "published_date": "2024-06-18 23:27:46 UTC",
    "updated_date": "2025-02-06 15:57:23 UTC"
  },
  {
    "arxiv_id": "2406.13094v2",
    "title": "Exploring and Benchmarking the Planning Capabilities of Large Language Models",
    "authors": [
      "Bernd Bohnet",
      "Azade Nova",
      "Aaron T Parisi",
      "Kevin Swersky",
      "Katayoon Goshvadi",
      "Hanjun Dai",
      "Dale Schuurmans",
      "Noah Fiedel",
      "Hanie Sedghi"
    ],
    "abstract": "Classical and natural language planning tasks remain a difficult domain for\nmodern large language models (LLMs). In this work, we lay the foundations for\nimproving planning capabilities of LLMs. First, we construct a comprehensive\nbenchmark suite encompassing both classical planning benchmarks and natural\nlanguage scenarios. This suite includes algorithms to methodically generate\ninstances of tasks with varying levels of difficulty, allowing for rigorous and\nsystematic evaluation of LLM performance. Next, we investigate the use of\nmany-shot in-context learning to enhance LLM planning, exploring the\nrelationship between increased context length and improved planning\nperformance. In addition, we demonstrate the positive impact of fine-tuning\nLLMs on optimal planning paths. We also probe the efficacy of chain-of-thought\nreasoning methods to improve LLM planning performance. Moreover, we probe the\nperformance of the proposed methods in out-of-distribution scenarios, assessing\nthe ability to generalize to novel and unseen planning challenges. Finally, we\ninvestigate model's failure modes and reveal insights that hold true across\ndifferent benchmarks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13094v2",
    "published_date": "2024-06-18 22:57:06 UTC",
    "updated_date": "2024-11-02 11:49:49 UTC"
  },
  {
    "arxiv_id": "2406.13093v1",
    "title": "RITA: A Real-time Interactive Talking Avatars Framework",
    "authors": [
      "Wuxinlin Cheng",
      "Cheng Wan",
      "Yupeng Cao",
      "Sihan Chen"
    ],
    "abstract": "RITA presents a high-quality real-time interactive framework built upon\ngenerative models, designed with practical applications in mind. Our framework\nenables the transformation of user-uploaded photos into digital avatars that\ncan engage in real-time dialogue interactions. By leveraging the latest\nadvancements in generative modeling, we have developed a versatile platform\nthat not only enhances the user experience through dynamic conversational\navatars but also opens new avenues for applications in virtual reality, online\neducation, and interactive gaming. This work showcases the potential of\nintegrating computer vision and natural language processing technologies to\ncreate immersive and interactive digital personas, pushing the boundaries of\nhow we interact with digital content.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13093v1",
    "published_date": "2024-06-18 22:53:15 UTC",
    "updated_date": "2024-06-18 22:53:15 UTC"
  },
  {
    "arxiv_id": "2406.13086v1",
    "title": "NaviSplit: Dynamic Multi-Branch Split DNNs for Efficient Distributed Autonomous Navigation",
    "authors": [
      "Timothy K Johnsen",
      "Ian Harshbarger",
      "Zixia Xia",
      "Marco Levorato"
    ],
    "abstract": "Lightweight autonomous unmanned aerial vehicles (UAV) are emerging as a\ncentral component of a broad range of applications. However, autonomous\nnavigation necessitates the implementation of perception algorithms, often deep\nneural networks (DNN), that process the input of sensor observations, such as\nthat from cameras and LiDARs, for control logic. The complexity of such\nalgorithms clashes with the severe constraints of these devices in terms of\ncomputing power, energy, memory, and execution time. In this paper, we propose\nNaviSplit, the first instance of a lightweight navigation framework embedding a\ndistributed and dynamic multi-branched neural model. At its core is a DNN split\nat a compression point, resulting in two model parts: (1) the head model, that\nis executed at the vehicle, which partially processes and compacts perception\nfrom sensors; and (2) the tail model, that is executed at an interconnected\ncompute-capable device, which processes the remainder of the compacted\nperception and infers navigation commands. Different from prior work, the\nNaviSplit framework includes a neural gate that dynamically selects a specific\nhead model to minimize channel usage while efficiently supporting the\nnavigation network. In our implementation, the perception model extracts a 2D\ndepth map from a monocular RGB image captured by the drone using the robust\nsimulator Microsoft AirSim. Our results demonstrate that the NaviSplit depth\nmodel achieves an extraction accuracy of 72-81% while transmitting an extremely\nsmall amount of data (1.2-18 KB) to the edge server. When using the neural\ngate, as utilized by NaviSplit, we obtain a slightly higher navigation accuracy\nas compared to a larger static network by 0.3% while significantly reducing the\ndata rate by 95%. To the best of our knowledge, this is the first exemplar of\ndynamic multi-branched model based on split DNNs for autonomous navigation.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13086v1",
    "published_date": "2024-06-18 22:25:09 UTC",
    "updated_date": "2024-06-18 22:25:09 UTC"
  },
  {
    "arxiv_id": "2406.13069v3",
    "title": "Evaluating $n$-Gram Novelty of Language Models Using Rusty-DAWG",
    "authors": [
      "William Merrill",
      "Noah A. Smith",
      "Yanai Elazar"
    ],
    "abstract": "How novel are texts generated by language models (LMs) relative to their\ntraining corpora? In this work, we investigate the extent to which modern LMs\ngenerate $n$-grams from their training data, evaluating both (i) the\nprobability LMs assign to complete training $n$-grams and (ii) $n$-novelty, the\nproportion of $n$-grams generated by an LM that did not appear in the training\ndata (for arbitrarily large $n$). To enable arbitrary-length $n$-gram search\nover a corpus in constant time w.r.t. corpus size, we develop Rusty-DAWG, a\nnovel search tool inspired by indexing of genomic data. We compare the novelty\nof LM-generated text to human-written text and explore factors that affect\ngeneration novelty, focusing on the Pythia models. We find that, for $n > 4$,\nLM-generated text is less novel than human-written text, though it is more\nnovel for smaller $n$. Larger LMs and more constrained decoding strategies both\ndecrease novelty. Finally, we show that LMs complete $n$-grams with lower loss\nif they are more frequent in the training data. Overall, our results reveal\nfactors influencing the novelty of LM-generated text, and we release Rusty-DAWG\nto facilitate further pretraining data research.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "To appear at EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.13069v3",
    "published_date": "2024-06-18 21:31:19 UTC",
    "updated_date": "2024-10-04 16:42:20 UTC"
  },
  {
    "arxiv_id": "2406.13066v1",
    "title": "MaskPure: Improving Defense Against Text Adversaries with Stochastic Purification",
    "authors": [
      "Harrison Gietz",
      "Jugal Kalita"
    ],
    "abstract": "The improvement of language model robustness, including successful defense\nagainst adversarial attacks, remains an open problem. In computer vision\nsettings, the stochastic noising and de-noising process provided by diffusion\nmodels has proven useful for purifying input images, thus improving model\nrobustness against adversarial attacks. Similarly, some initial work has\nexplored the use of random noising and de-noising to mitigate adversarial\nattacks in an NLP setting, but improving the quality and efficiency of these\nmethods is necessary for them to remain competitive. We extend upon methods of\ninput text purification that are inspired by diffusion processes, which\nrandomly mask and refill portions of the input text before classification. Our\nnovel method, MaskPure, exceeds or matches robustness compared to other\ncontemporary defenses, while also requiring no adversarial classifier training\nand without assuming knowledge of the attack type. In addition, we show that\nMaskPure is provably certifiably robust. To our knowledge, MaskPure is the\nfirst stochastic-purification method with demonstrated success against both\ncharacter-level and word-level attacks, indicating the generalizable and\npromising nature of stochastic denoising defenses. In summary: the MaskPure\nalgorithm bridges literature on the current strongest certifiable and empirical\nadversarial defense methods, showing that both theoretical and practical\nrobustness can be obtained together. Code is available on GitHub at\nhttps://github.com/hubarruby/MaskPure.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 1 figure, in the proceedings of The 29th International\n  Conference on Natural Language & Information Systems (NLDB 2024)",
    "pdf_url": "http://arxiv.org/pdf/2406.13066v1",
    "published_date": "2024-06-18 21:27:13 UTC",
    "updated_date": "2024-06-18 21:27:13 UTC"
  },
  {
    "arxiv_id": "2406.13064v1",
    "title": "Machine Learning and Optimization Techniques for Solving Inverse Kinematics in a 7-DOF Robotic Arm",
    "authors": [
      "Enoch Adediran",
      "Salem Ameen"
    ],
    "abstract": "As the pace of AI technology continues to accelerate, more tools have become\navailable to researchers to solve longstanding problems, Hybrid approaches\navailable today continue to push the computational limits of efficiency and\nprecision. One of such problems is the inverse kinematics of redundant systems.\nThis paper explores the complexities of a 7 degree of freedom manipulator and\nexplores 13 optimization techniques to solve it. Additionally, a novel approach\nis proposed to contribute to the field of algorithmic research. This was found\nto be over 200 times faster than the well-known traditional Particle Swarm\nOptimization technique. This new method may serve as a new field of search that\ncombines the explorative capabilities of Machine Learning with the exploitative\ncapabilities of numerical methods.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13064v1",
    "published_date": "2024-06-18 21:23:51 UTC",
    "updated_date": "2024-06-18 21:23:51 UTC"
  },
  {
    "arxiv_id": "2406.13060v1",
    "title": "Scale-Translation Equivariant Network for Oceanic Internal Solitary Wave Localization",
    "authors": [
      "Zhang Wan",
      "Shuo Wang",
      "Xudong Zhang"
    ],
    "abstract": "Internal solitary waves (ISWs) are gravity waves that are often observed in\nthe interior ocean rather than the surface. They hold significant importance\ndue to their capacity to carry substantial energy, thus influence pollutant\ntransport, oil platform operations, submarine navigation, etc. Researchers have\nstudied ISWs through optical images, synthetic aperture radar (SAR) images, and\naltimeter data from remote sensing instruments. However, cloud cover in optical\nremote sensing images variably obscures ground information, leading to blurred\nor missing surface observations. As such, this paper aims at altimeter-based\nmachine learning solutions to automatically locate ISWs. The challenges,\nhowever, lie in the following two aspects: 1) the altimeter data has low\nresolution, which requires a strong machine learner; 2) labeling data is\nextremely labor-intensive, leading to very limited data for training. In recent\nyears, the grand progress of deep learning demonstrates strong learning\ncapacity given abundant data. Besides, more recent studies on efficient\nlearning and self-supervised learning laid solid foundations to tackle the\naforementioned challenges. In this paper, we propose to inject prior knowledge\nto achieve a strong and efficient learner. Specifically, intrinsic patterns in\naltimetry data are efficiently captured using a scale-translation equivariant\nconvolutional neural network (ST-ECNN). By considering inherent symmetries in\nneural network design, ST-ECNN achieves higher efficiency and better\nperformance than baseline models. Furthermore, we also introduce prior\nknowledge from massive unsupervised data to enhance our solution using the\nSimCLR framework for pre-training. Our final solution achieves an overall\nbetter performance than baselines on our handcrafted altimetry dataset. Data\nand codes are available at\nhttps://github.com/ZhangWan-byte/Internal_Solitary_Wave_Localization .",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.AP"
    ],
    "primary_category": "cs.LG",
    "comment": "29 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.13060v1",
    "published_date": "2024-06-18 21:09:56 UTC",
    "updated_date": "2024-06-18 21:09:56 UTC"
  },
  {
    "arxiv_id": "2406.13057v1",
    "title": "Informed along the road: roadway capacity driven graph convolution network for network-wide traffic prediction",
    "authors": [
      "Zilin Bian",
      "Jingqin Gao",
      "Kaan Ozbay",
      "Fan Zuo",
      "Dachuan Zuo",
      "Zhenning Li"
    ],
    "abstract": "While deep learning has shown success in predicting traffic states, most\nmethods treat it as a general prediction task without considering\ntransportation aspects. Recently, graph neural networks have proven effective\nfor this task, but few incorporate external factors that impact roadway\ncapacity and traffic flow. This study introduces the Roadway Capacity Driven\nGraph Convolution Network (RCDGCN) model, which incorporates static and dynamic\nroadway capacity attributes in spatio-temporal settings to predict network-wide\ntraffic states. The model was evaluated on two real-world datasets with\ndifferent transportation factors: the ICM-495 highway network and an urban\nnetwork in Manhattan, New York City. Results show RCDGCN outperformed baseline\nmethods in forecasting accuracy. Analyses, including ablation experiments,\nweight analysis, and case studies, investigated the effect of capacity-related\nfactors. The study demonstrates the potential of using RCDGCN for\ntransportation system management.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13057v1",
    "published_date": "2024-06-18 21:04:23 UTC",
    "updated_date": "2024-06-18 21:04:23 UTC"
  },
  {
    "arxiv_id": "2406.13049v2",
    "title": "Assessing AI vs Human-Authored Spear Phishing SMS Attacks: An Empirical Study",
    "authors": [
      "Jerson Francia",
      "Derek Hansen",
      "Ben Schooley",
      "Matthew Taylor",
      "Shydra Murray",
      "Greg Snow"
    ],
    "abstract": "This paper explores the use of Large Language Models (LLMs) in spear phishing\nmessage generation and evaluates their performance compared to human-authored\ncounterparts. Our pilot study examines the effectiveness of smishing (SMS\nphishing) messages created by GPT-4 and human authors, which have been\npersonalized for willing targets. The targets assessed these messages in a\nmodified ranked-order experiment using a novel methodology we call TRAPD\n(Threshold Ranking Approach for Personalized Deception). Experiments involved\nranking each spear phishing message from most to least convincing, providing\nqualitative feedback, and guessing which messages were human- or AI-generated.\nResults show that LLM-generated messages are often perceived as more convincing\nthan those authored by humans, particularly job-related messages. Targets also\nstruggled to distinguish between human- and AI-generated messages. We analyze\ndifferent criteria the targets used to assess the persuasiveness and source of\nmessages. This study aims to highlight the urgent need for further research and\nimproved countermeasures against personalized AI-enabled social engineering\nattacks.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "18 pages, 5 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2406.13049v2",
    "published_date": "2024-06-18 20:47:16 UTC",
    "updated_date": "2025-03-19 00:33:59 UTC"
  },
  {
    "arxiv_id": "2406.13046v3",
    "title": "Bayesian-LoRA: LoRA based Parameter Efficient Fine-Tuning using Optimal Quantization levels and Rank Values trough Differentiable Bayesian Gates",
    "authors": [
      "Cristian Meo",
      "Ksenia Sycheva",
      "Anirudh Goyal",
      "Justin Dauwels"
    ],
    "abstract": "It is a common practice in natural language processing to pre-train a single\nmodel on a general domain and then fine-tune it for downstream tasks. However,\nwhen it comes to Large Language Models, fine-tuning the entire model can be\ncomputationally expensive, resulting in very intensive energy consumption. As a\nresult, several Parameter Efficient Fine-Tuning (PEFT) approaches were recently\nproposed. One of the most popular approaches is low-rank adaptation (LoRA),\nwhere the key insight is decomposing the update weights of the pre-trained\nmodel into two low-rank matrices. However, the proposed approaches either use\nthe same rank value across all different weight matrices, which has been shown\nto be a sub-optimal choice, or do not use any quantization technique, one of\nthe most important factors when it comes to a model's energy consumption. In\nthis work, we propose Bayesian-LoRA which approaches low-rank adaptation and\nquantization from a Bayesian perspective by employing a prior distribution on\nboth quantization levels and rank values. As a result, B-LoRA is able to\nfine-tune a pre-trained model on a specific downstream task, finding the\noptimal rank values and quantization levels for every low-rank matrix. We\nvalidate the proposed model by fine-tuning a pre-trained DeBERTaV3 on the GLUE\nbenchmark. Moreover, we compare it to relevant baselines and present both\nqualitative and quantitative results, showing how the proposed approach is able\nto learn optimal-rank quantized matrices. B-LoRA performs on par with or better\nthan the baselines while reducing the total number of bit operations by roughly\n70% compared to the baseline methods.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13046v3",
    "published_date": "2024-06-18 20:26:30 UTC",
    "updated_date": "2024-10-28 17:47:26 UTC"
  },
  {
    "arxiv_id": "2406.13038v1",
    "title": "Traffic Prediction considering Multiple Levels of Spatial-temporal Information: A Multi-scale Graph Wavelet-based Approach",
    "authors": [
      "Zilin Bian",
      "Jingqin Gao",
      "Kaan Ozbay",
      "Zhenning Li"
    ],
    "abstract": "Although traffic prediction has been receiving considerable attention with a\nnumber of successes in the context of intelligent transportation systems, the\nprediction of traffic states over a complex transportation network that\ncontains different road types has remained a challenge. This study proposes a\nmulti-scale graph wavelet temporal convolution network (MSGWTCN) to predict the\ntraffic states in complex transportation networks. Specifically, a multi-scale\nspatial block is designed to simultaneously capture the spatial information at\ndifferent levels, and the gated temporal convolution network is employed to\nextract the temporal dependencies of the data. The model jointly learns to\nmount multiple levels of the spatial interactions by stacking graph wavelets\nwith different scales. Two real-world datasets are used in this study to\ninvestigate the model performance, including a highway network in Seattle and a\ndense road network of Manhattan in New York City. Experiment results show that\nthe proposed model outperforms other baseline models. Furthermore, different\nscales of graph wavelets are found to be effective in extracting local,\nintermediate and global information at the same time and thus enable the model\nto learn a complex transportation network topology with various types of road\nsegments. By carefully customizing the scales of wavelets, the model is able to\nimprove the prediction performance and better adapt to different network\nconfigurations.",
    "categories": [
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13038v1",
    "published_date": "2024-06-18 20:05:47 UTC",
    "updated_date": "2024-06-18 20:05:47 UTC"
  },
  {
    "arxiv_id": "2406.13034v1",
    "title": "Real-time Yemeni Currency Detection",
    "authors": [
      "Edrees AL-Edreesi",
      "Ghaleb Al-Gaphari"
    ],
    "abstract": "Banknote recognition is a major problem faced by visually Challenged people.\nSo we propose a application to help the visually Challenged people to identify\nthe different types of Yemenian currencies through deep learning technique. As\nmoney has a significant role in daily life for any business transactions,\nreal-time detection and recognition of banknotes become necessary for a person,\nespecially blind or visually impaired, or for a system that sorts the data.\nThis paper presents a real-time Yemeni currency detection system for visually\nimpaired persons. The proposed system exploits the deep learning approach to\nfacilitate the visually impaired people to prosperously recognize banknotes.\nFor real-time recognition, we have deployed the system into a mobile\napplication.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13034v1",
    "published_date": "2024-06-18 19:57:15 UTC",
    "updated_date": "2024-06-18 19:57:15 UTC"
  },
  {
    "arxiv_id": "2406.13015v1",
    "title": "Deriving Hematological Disease Classes Using Fuzzy Logic and Expert Knowledge: A Comprehensive Machine Learning Approach with CBC Parameters",
    "authors": [
      "Salem Ameen",
      "Ravivarman Balachandran",
      "Theodoros Theodoridis"
    ],
    "abstract": "In the intricate field of medical diagnostics, capturing the subtle\nmanifestations of diseases remains a challenge. Traditional methods, often\nbinary in nature, may not encapsulate the nuanced variances that exist in\nreal-world clinical scenarios. This paper introduces a novel approach by\nleveraging Fuzzy Logic Rules to derive disease classes based on expert domain\nknowledge from a medical practitioner. By recognizing that diseases do not\nalways fit into neat categories, and that expert knowledge can guide the\nfuzzification of these boundaries, our methodology offers a more sophisticated\nand nuanced diagnostic tool.\n  Using a dataset procured from a prominent hospital, containing detailed\npatient blood count records, we harness Fuzzy Logic Rules, a computational\ntechnique celebrated for its ability to handle ambiguity. This approach, moving\nthrough stages of fuzzification, rule application, inference, and ultimately\ndefuzzification, produces refined diagnostic predictions. When combined with\nthe Random Forest classifier, the system adeptly predicts hematological\nconditions using Complete Blood Count (CBC) parameters.\n  Preliminary results showcase high accuracy levels, underscoring the\nadvantages of integrating fuzzy logic into the diagnostic process. When\njuxtaposed with traditional diagnostic techniques, it becomes evident that\nFuzzy Logic, especially when guided by medical expertise, offers significant\nadvancements in the realm of hematological diagnostics. This paper not only\npaves the path for enhanced patient care but also beckons a deeper dive into\nthe potentialities of fuzzy logic in various medical diagnostic applications.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13015v1",
    "published_date": "2024-06-18 19:16:32 UTC",
    "updated_date": "2024-06-18 19:16:32 UTC"
  },
  {
    "arxiv_id": "2406.13009v1",
    "title": "Detecting Errors through Ensembling Prompts (DEEP): An End-to-End LLM Framework for Detecting Factual Errors",
    "authors": [
      "Alex Chandler",
      "Devesh Surve",
      "Hui Su"
    ],
    "abstract": "Accurate text summarization is one of the most common and important tasks\nperformed by Large Language Models, where the costs of human review for an\nentire document may be high, but the costs of errors in summarization may be\neven greater. We propose Detecting Errors through Ensembling Prompts (DEEP) -\nan end-to-end large language model framework for detecting factual errors in\ntext summarization. Our framework uses a diverse set of LLM prompts to identify\nfactual inconsistencies, treating their outputs as binary features, which are\nthen fed into ensembling models. We then calibrate the ensembled models to\nproduce empirically accurate probabilities that a text is factually consistent\nor free of hallucination. We demonstrate that prior models for detecting\nfactual errors in summaries perform significantly worse without optimizing the\nthresholds on subsets of the evaluated dataset. Our framework achieves\nstate-of-the-art (SOTA) balanced accuracy on the AggreFact-XSUM FTSOTA,\nTofuEval Summary-Level, and HaluEval Summarization benchmarks in detecting\nfactual errors within transformer-generated text summaries. It does so without\nany fine-tuning of the language model or reliance on thresholding techniques\nnot available in practical settings.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13009v1",
    "published_date": "2024-06-18 18:59:37 UTC",
    "updated_date": "2024-06-18 18:59:37 UTC"
  },
  {
    "arxiv_id": "2406.13008v1",
    "title": "ClaudesLens: Uncertainty Quantification in Computer Vision Models",
    "authors": [
      "Mohamad Al Shaar",
      "Nils Ekström",
      "Gustav Gille",
      "Reza Rezvan",
      "Ivan Wely"
    ],
    "abstract": "In a world where more decisions are made using artificial intelligence, it is\nof utmost importance to ensure these decisions are well-grounded. Neural\nnetworks are the modern building blocks for artificial intelligence. Modern\nneural network-based computer vision models are often used for object\nclassification tasks. Correctly classifying objects with \\textit{certainty} has\nbecome of great importance in recent times. However, quantifying the inherent\n\\textit{uncertainty} of the output from neural networks is a challenging task.\nHere we show a possible method to quantify and evaluate the uncertainty of the\noutput of different computer vision models based on Shannon entropy. By adding\nperturbation of different levels, on different parts, ranging from the input to\nthe parameters of the network, one introduces entropy to the system. By\nquantifying and evaluating the perturbed models on the proposed PI and PSI\nmetrics, we can conclude that our theoretical framework can grant insight into\nthe uncertainty of predictions of computer vision models. We believe that this\ntheoretical framework can be applied to different applications for neural\nnetworks. We believe that Shannon entropy may eventually have a bigger role in\nthe SOTA (State-of-the-art) methods to quantify uncertainty in artificial\nintelligence. One day we might be able to apply Shannon entropy to our neural\nsystems.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.13008v1",
    "published_date": "2024-06-18 18:58:54 UTC",
    "updated_date": "2024-06-18 18:58:54 UTC"
  },
  {
    "arxiv_id": "2406.12998v4",
    "title": "Coding Speech through Vocal Tract Kinematics",
    "authors": [
      "Cheol Jun Cho",
      "Peter Wu",
      "Tejas S. Prabhune",
      "Dhruv Agarwal",
      "Gopala K. Anumanchipalli"
    ],
    "abstract": "Vocal tract articulation is a natural, grounded control space of speech\nproduction. The spatiotemporal coordination of articulators combined with the\nvocal source shapes intelligible speech sounds to enable effective spoken\ncommunication. Based on this physiological grounding of speech, we propose a\nnew framework of neural encoding-decoding of speech -- Speech Articulatory\nCoding (SPARC). SPARC comprises an articulatory analysis model that infers\narticulatory features from speech audio, and an articulatory synthesis model\nthat synthesizes speech audio from articulatory features. The articulatory\nfeatures are kinematic traces of vocal tract articulators and source features,\nwhich are intuitively interpretable and controllable, being the actual physical\ninterface of speech production. An additional speaker identity encoder is\njointly trained with the articulatory synthesizer to inform the voice texture\nof individual speakers. By training on large-scale speech data, we achieve a\nfully intelligible, high-quality articulatory synthesizer that generalizes to\nunseen speakers. Furthermore, the speaker embedding is effectively disentangled\nfrom articulations, which enables accent-perserving zero-shot voice conversion.\nTo the best of our knowledge, this is the first demonstration of universal,\nhigh-performance articulatory inference and synthesis, suggesting the proposed\nframework as a powerful coding system of speech.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12998v4",
    "published_date": "2024-06-18 18:38:17 UTC",
    "updated_date": "2024-12-14 18:40:28 UTC"
  },
  {
    "arxiv_id": "2406.12975v2",
    "title": "SHIELD: Evaluation and Defense Strategies for Copyright Compliance in LLM Text Generation",
    "authors": [
      "Xiaoze Liu",
      "Ting Sun",
      "Tianyang Xu",
      "Feijie Wu",
      "Cunxiang Wang",
      "Xiaoqian Wang",
      "Jing Gao"
    ],
    "abstract": "Large Language Models (LLMs) have transformed machine learning but raised\nsignificant legal concerns due to their potential to produce text that\ninfringes on copyrights, resulting in several high-profile lawsuits. The legal\nlandscape is struggling to keep pace with these rapid advancements, with\nongoing debates about whether generated text might plagiarize copyrighted\nmaterials. Current LLMs may infringe on copyrights or overly restrict\nnon-copyrighted texts, leading to these challenges: (i) the need for a\ncomprehensive evaluation benchmark to assess copyright compliance from multiple\naspects; (ii) evaluating robustness against safeguard bypassing attacks; and\n(iii) developing effective defense targeted against the generation of\ncopyrighted text. To tackle these challenges, we introduce a curated dataset to\nevaluate methods, test attack strategies, and propose lightweight, real-time\ndefense to prevent the generation of copyrighted text, ensuring the safe and\nlawful use of LLMs. Our experiments demonstrate that current LLMs frequently\noutput copyrighted text, and that jailbreaking attacks can significantly\nincrease the volume of copyrighted output. Our proposed defense mechanism\nsignificantly reduces the volume of copyrighted text generated by LLMs by\neffectively refusing malicious requests. Code is publicly available at\nhttps://github.com/xz-liu/SHIELD",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2406.12975v2",
    "published_date": "2024-06-18 18:00:03 UTC",
    "updated_date": "2024-08-21 11:57:05 UTC"
  },
  {
    "arxiv_id": "2406.14579v1",
    "title": "Attention Networks for Personalized Mealtime Insulin Dosing in People with Type 1 Diabetes",
    "authors": [
      "Anas El Fathi",
      "Elliott Pryor",
      "Marc D. Breton"
    ],
    "abstract": "Calculating mealtime insulin doses poses a significant challenge for\nindividuals with Type 1 Diabetes (T1D). Doses should perfectly compensate for\nexpected post-meal glucose excursions, requiring a profound understanding of\nthe individual's insulin sensitivity and the meal macronutrients'. Usually,\npeople rely on intuition and experience to develop this understanding. In this\nwork, we demonstrate how a reinforcement learning agent, employing a\nself-attention encoder network, can effectively mimic and enhance this\nintuitive process. Trained on 80 virtual subjects from the FDA-approved\nUVA/Padova T1D adult cohort and tested on twenty, self-attention demonstrates\nsuperior performance compared to other network architectures. Results reveal a\nsignificant reduction in glycemic risk, from 16.5 to 9.6 in scenarios using\nsensor-augmented pump and from 9.1 to 6.7 in scenarios using automated insulin\ndelivery. This new paradigm bypasses conventional therapy parameters, offering\nthe potential to simplify treatment and promising improved quality of life and\nglycemic outcomes for people with T1D.",
    "categories": [
      "q-bio.QM",
      "cs.AI"
    ],
    "primary_category": "q-bio.QM",
    "comment": "6 pages, 4 figures, Biological and Medical Systems - 12th BMS 2024 -\n  IFAC",
    "pdf_url": "http://arxiv.org/pdf/2406.14579v1",
    "published_date": "2024-06-18 17:59:32 UTC",
    "updated_date": "2024-06-18 17:59:32 UTC"
  },
  {
    "arxiv_id": "2406.12844v1",
    "title": "Synergizing Foundation Models and Federated Learning: A Survey",
    "authors": [
      "Shenghui Li",
      "Fanghua Ye",
      "Meng Fang",
      "Jiaxu Zhao",
      "Yun-Hin Chan",
      "Edith C. -H. Ngai",
      "Thiemo Voigt"
    ],
    "abstract": "The recent development of Foundation Models (FMs), represented by large\nlanguage models, vision transformers, and multimodal models, has been making a\nsignificant impact on both academia and industry. Compared with small-scale\nmodels, FMs have a much stronger demand for high-volume data during the\npre-training phase. Although general FMs can be pre-trained on data collected\nfrom open sources such as the Internet, domain-specific FMs need proprietary\ndata, posing a practical challenge regarding the amount of data available due\nto privacy concerns. Federated Learning (FL) is a collaborative learning\nparadigm that breaks the barrier of data availability from different\nparticipants. Therefore, it provides a promising solution to customize and\nadapt FMs to a wide range of domain-specific tasks using distributed datasets\nwhilst preserving privacy. This survey paper discusses the potentials and\nchallenges of synergizing FL and FMs and summarizes core techniques, future\ndirections, and applications. A periodically updated paper collection on FM-FL\nis available at https://github.com/lishenghui/awesome-fm-fl.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12844v1",
    "published_date": "2024-06-18 17:58:09 UTC",
    "updated_date": "2024-06-18 17:58:09 UTC"
  },
  {
    "arxiv_id": "2406.12843v3",
    "title": "Can Go AIs be adversarially robust?",
    "authors": [
      "Tom Tseng",
      "Euan McLean",
      "Kellin Pelrine",
      "Tony T. Wang",
      "Adam Gleave"
    ],
    "abstract": "Prior work found that superhuman Go AIs can be defeated by simple adversarial\nstrategies, especially \"cyclic\" attacks. In this paper, we study whether adding\nnatural countermeasures can achieve robustness in Go, a favorable domain for\nrobustness since it benefits from incredible average-case capability and a\nnarrow, innately adversarial setting. We test three defenses: adversarial\ntraining on hand-constructed positions, iterated adversarial training, and\nchanging the network architecture. We find that though some of these defenses\nprotect against previously discovered attacks, none withstand freshly trained\nadversaries. Furthermore, most of the reliably effective attacks these\nadversaries discover are different realizations of the same overall class of\ncyclic attacks. Our results suggest that building robust AI systems is\nchallenging even with extremely superhuman systems in some of the most\ntractable settings, and highlight two key gaps: efficient generalization of\ndefenses, and diversity in training. For interactive examples of attacks and a\nlink to our codebase, see https://goattack.far.ai.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "63 pages, AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2406.12843v3",
    "published_date": "2024-06-18 17:57:49 UTC",
    "updated_date": "2025-01-14 03:08:02 UTC"
  },
  {
    "arxiv_id": "2406.12841v2",
    "title": "Demystifying Higher-Order Graph Neural Networks",
    "authors": [
      "Maciej Besta",
      "Florian Scheidl",
      "Lukas Gianinazzi",
      "Grzegorz Kwasniewski",
      "Shachar Klaiman",
      "Jürgen Müller",
      "Torsten Hoefler"
    ],
    "abstract": "Higher-order graph neural networks (HOGNNs) and the related architectures\nfrom Topological Deep Learning are an important class of GNN models that\nharness polyadic relations between vertices beyond plain edges. They have been\nused to eliminate issues such as over-smoothing or over-squashing, to\nsignificantly enhance the accuracy of GNN predictions, to improve the\nexpressiveness of GNN architectures, and for numerous other goals. A plethora\nof HOGNN models have been introduced, and they come with diverse neural\narchitectures, and even with different notions of what the \"higher-order\"\nmeans. This richness makes it very challenging to appropriately analyze and\ncompare HOGNN models, and to decide in what scenario to use specific ones. To\nalleviate this, we first design an in-depth taxonomy and a blueprint for\nHOGNNs. This facilitates designing models that maximize performance. Then, we\nuse our taxonomy to analyze and compare the available HOGNN models. The\noutcomes of our analysis are synthesized in a set of insights that help to\nselect the most beneficial GNN model in a given scenario, and a comprehensive\nlist of challenges and opportunities for further research into more powerful\nHOGNNs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12841v2",
    "published_date": "2024-06-18 17:57:11 UTC",
    "updated_date": "2024-12-06 14:57:59 UTC"
  },
  {
    "arxiv_id": "2406.12835v1",
    "title": "Influence Maximization via Graph Neural Bandits",
    "authors": [
      "Yuting Feng",
      "Vincent Y. F. Tan",
      "Bogdan Cautis"
    ],
    "abstract": "We consider a ubiquitous scenario in the study of Influence Maximization\n(IM), in which there is limited knowledge about the topology of the diffusion\nnetwork. We set the IM problem in a multi-round diffusion campaign, aiming to\nmaximize the number of distinct users that are influenced. Leveraging the\ncapability of bandit algorithms to effectively balance the objectives of\nexploration and exploitation, as well as the expressivity of neural networks,\nour study explores the application of neural bandit algorithms to the IM\nproblem. We propose the framework IM-GNB (Influence Maximization with Graph\nNeural Bandits), where we provide an estimate of the users' probabilities of\nbeing influenced by influencers (also known as diffusion seeds). This initial\nestimate forms the basis for constructing both an exploitation graph and an\nexploration one. Subsequently, IM-GNB handles the exploration-exploitation\ntradeoff, by selecting seed nodes in real-time using Graph Convolutional\nNetworks (GCN), in which the pre-estimated graphs are employed to refine the\ninfluencers' estimated rewards in each contextual setting. Through extensive\nexperiments on two large real-world datasets, we demonstrate the effectiveness\nof IM-GNB compared with other baseline methods, significantly improving the\nspread outcome of such diffusion campaigns, when the underlying network is\nunknown.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "To appear at the 2024 ACM SIGKDD International Conference on\n  Knowledge Discovery and Data Mining (KDD)",
    "pdf_url": "http://arxiv.org/pdf/2406.12835v1",
    "published_date": "2024-06-18 17:54:33 UTC",
    "updated_date": "2024-06-18 17:54:33 UTC"
  },
  {
    "arxiv_id": "2406.12832v1",
    "title": "LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation",
    "authors": [
      "Seyedarmin Azizi",
      "Souvik Kundu",
      "Massoud Pedram"
    ],
    "abstract": "Low-rank adaptation (LoRA) has become the default approach to fine-tune large\nlanguage models (LLMs) due to its significant reduction in trainable\nparameters. However, trainable parameter demand for LoRA increases with\nincreasing model embedding dimensions, leading to high compute costs.\nAdditionally, its backward updates require storing high-dimensional\nintermediate activations and optimizer states, demanding high peak GPU memory.\nIn this paper, we introduce large model fine-tuning via spectrally decomposed\nlow-dimensional adaptation (LaMDA), a novel approach to fine-tuning large\nlanguage models, which leverages low-dimensional adaptation to achieve\nsignificant reductions in trainable parameters and peak GPU memory footprint.\nLaMDA freezes a first projection matrix (PMA) in the adaptation path while\nintroducing a low-dimensional trainable square matrix, resulting in substantial\nreductions in trainable parameters and peak GPU memory usage. LaMDA gradually\nfreezes a second projection matrix (PMB) during the early fine-tuning stages,\nreducing the compute cost associated with weight updates to enhance parameter\nefficiency further. We also present an enhancement, LaMDA++, incorporating a\n``lite-weight\" adaptive rank allocation for the LoRA path via normalized\nspectrum analysis of pre-trained model weights. We evaluate LaMDA/LaMDA++\nacross various tasks, including natural language understanding with the GLUE\nbenchmark, text summarization, natural language generation, and complex\nreasoning on different LLMs. Results show that LaMDA matches or surpasses the\nperformance of existing alternatives while requiring up to 17.7x fewer\nparameter updates and up to 1.32x lower peak GPU memory usage during\nfine-tuning. Code will be publicly available.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12832v1",
    "published_date": "2024-06-18 17:52:59 UTC",
    "updated_date": "2024-06-18 17:52:59 UTC"
  },
  {
    "arxiv_id": "2406.12831v3",
    "title": "VIA: Unified Spatiotemporal Video Adaptation Framework for Global and Local Video Editing",
    "authors": [
      "Jing Gu",
      "Yuwei Fang",
      "Ivan Skorokhodov",
      "Peter Wonka",
      "Xinya Du",
      "Sergey Tulyakov",
      "Xin Eric Wang"
    ],
    "abstract": "Video editing serves as a fundamental pillar of digital media, spanning\napplications in entertainment, education, and professional communication.\nHowever, previous methods often overlook the necessity of comprehensively\nunderstanding both global and local contexts, leading to inaccurate and\ninconsistent edits in the spatiotemporal dimension, especially for long videos.\nIn this paper, we introduce VIA, a unified spatiotemporal Video Adaptation\nframework for global and local video editing, pushing the limits of\nconsistently editing minute-long videos. First, to ensure local consistency\nwithin individual frames, we designed test-time editing adaptation to adapt a\npre-trained image editing model for improving consistency between potential\nediting directions and the text instruction, and adapts masked latent variables\nfor precise local control. Furthermore, to maintain global consistency over the\nvideo sequence, we introduce spatiotemporal adaptation that recursively gather\nconsistent attention variables in key frames and strategically applies them\nacross the whole sequence to realize the editing effects. Extensive experiments\ndemonstrate that, compared to baseline methods, our VIA approach produces edits\nthat are more faithful to the source videos, more coherent in the\nspatiotemporal context, and more precise in local control. More importantly, we\nshow that VIA can achieve consistent long video editing in minutes, unlocking\nthe potential for advanced video editing tasks over long video sequences.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "18 pages, 16 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.12831v3",
    "published_date": "2024-06-18 17:51:37 UTC",
    "updated_date": "2025-03-27 17:56:31 UTC"
  },
  {
    "arxiv_id": "2406.12824v1",
    "title": "From RAGs to rich parameters: Probing how language models utilize external knowledge over parametric information for factual queries",
    "authors": [
      "Hitesh Wadhwa",
      "Rahul Seetharaman",
      "Somyaa Aggarwal",
      "Reshmi Ghosh",
      "Samyadeep Basu",
      "Soundararajan Srinivasan",
      "Wenlong Zhao",
      "Shreyas Chaudhari",
      "Ehsan Aghazadeh"
    ],
    "abstract": "Retrieval Augmented Generation (RAG) enriches the ability of language models\nto reason using external context to augment responses for a given user prompt.\nThis approach has risen in popularity due to practical applications in various\napplications of language models in search, question/answering, and chat-bots.\nHowever, the exact nature of how this approach works isn't clearly understood.\nIn this paper, we mechanistically examine the RAG pipeline to highlight that\nlanguage models take shortcut and have a strong bias towards utilizing only the\ncontext information to answer the question, while relying minimally on their\nparametric memory. We probe this mechanistic behavior in language models with:\n(i) Causal Mediation Analysis to show that the parametric memory is minimally\nutilized when answering a question and (ii) Attention Contributions and\nKnockouts to show that the last token residual stream do not get enriched from\nthe subject token in the question, but gets enriched from other informative\ntokens in the context. We find this pronounced shortcut behaviour true across\nboth LLaMa and Phi family of models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12824v1",
    "published_date": "2024-06-18 17:46:08 UTC",
    "updated_date": "2024-06-18 17:46:08 UTC"
  },
  {
    "arxiv_id": "2406.12822v3",
    "title": "Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?",
    "authors": [
      "Pinzhen Chen",
      "Simon Yu",
      "Zhicheng Guo",
      "Barry Haddow"
    ],
    "abstract": "Multilingual large language models are designed, claimed, and expected to\ncater to speakers of varied languages. We hypothesise that the current\npractices of fine-tuning and evaluating these models may not perfectly align\nwith this objective owing to a heavy reliance on translation, which cannot\ncover language-specific knowledge but can introduce translation defects. It\nremains unknown whether the nature of the instruction data has an impact on the\nmodel output; conversely, it is questionable whether translated test sets can\ncapture such nuances. Due to the often coupled practices of using translated\ndata in both stages, such imperfections could have been overlooked. This work\ninvestigates these issues using controlled native or translated data during the\ninstruction tuning and evaluation stages. We show that native or generation\nbenchmarks reveal a notable difference between native and translated\ninstruction data especially when model performance is high, whereas other types\nof test sets cannot. The comparison between round-trip and single-pass\ntranslations reflects the importance of knowledge from language-native\nresources. Finally, we demonstrate that regularization is beneficial to\nbridging this gap on structured but not generative tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.12822v3",
    "published_date": "2024-06-18 17:43:47 UTC",
    "updated_date": "2024-09-26 17:39:44 UTC"
  },
  {
    "arxiv_id": "2406.12815v1",
    "title": "Privacy Preserving Federated Learning in Medical Imaging with Uncertainty Estimation",
    "authors": [
      "Nikolas Koutsoubis",
      "Yasin Yilmaz",
      "Ravi P. Ramachandran",
      "Matthew Schabath",
      "Ghulam Rasool"
    ],
    "abstract": "Machine learning (ML) and Artificial Intelligence (AI) have fueled remarkable\nadvancements, particularly in healthcare. Within medical imaging, ML models\nhold the promise of improving disease diagnoses, treatment planning, and\npost-treatment monitoring. Various computer vision tasks like image\nclassification, object detection, and image segmentation are poised to become\nroutine in clinical analysis. However, privacy concerns surrounding patient\ndata hinder the assembly of large training datasets needed for developing and\ntraining accurate, robust, and generalizable models. Federated Learning (FL)\nemerges as a compelling solution, enabling organizations to collaborate on ML\nmodel training by sharing model training information (gradients) rather than\ndata (e.g., medical images). FL's distributed learning framework facilitates\ninter-institutional collaboration while preserving patient privacy. However,\nFL, while robust in privacy preservation, faces several challenges. Sensitive\ninformation can still be gleaned from shared gradients that are passed on\nbetween organizations during model training. Additionally, in medical imaging,\nquantifying model confidence\\uncertainty accurately is crucial due to the noise\nand artifacts present in the data. Uncertainty estimation in FL encounters\nunique hurdles due to data heterogeneity across organizations. This paper\noffers a comprehensive review of FL, privacy preservation, and uncertainty\nestimation, with a focus on medical imaging. Alongside a survey of current\nresearch, we identify gaps in the field and suggest future directions for FL\nresearch to enhance privacy and address noisy medical imaging data challenges.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "eess.IV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "31 pages, 5 figures, 3 tables, Journal preprint",
    "pdf_url": "http://arxiv.org/pdf/2406.12815v1",
    "published_date": "2024-06-18 17:35:52 UTC",
    "updated_date": "2024-06-18 17:35:52 UTC"
  },
  {
    "arxiv_id": "2406.12808v3",
    "title": "Graph Neural Networks in Histopathology: Emerging Trends and Future Directions",
    "authors": [
      "Siemen Brussee",
      "Giorgio Buzzanca",
      "Anne M. R. Schrader",
      "Jesper Kers"
    ],
    "abstract": "Histopathological analysis of Whole Slide Images (WSIs) has seen a surge in\nthe utilization of deep learning methods, particularly Convolutional Neural\nNetworks (CNNs). However, CNNs often fall short in capturing the intricate\nspatial dependencies inherent in WSIs. Graph Neural Networks (GNNs) present a\npromising alternative, adept at directly modeling pairwise interactions and\neffectively discerning the topological tissue and cellular structures within\nWSIs. Recognizing the pressing need for deep learning techniques that harness\nthe topological structure of WSIs, the application of GNNs in histopathology\nhas experienced rapid growth. In this comprehensive review, we survey GNNs in\nhistopathology, discuss their applications, and explore emerging trends that\npave the way for future advancements in the field. We begin by elucidating the\nfundamentals of GNNs and their potential applications in histopathology.\nLeveraging quantitative literature analysis, we identify four emerging trends:\nHierarchical GNNs, Adaptive Graph Structure Learning, Multimodal GNNs, and\nHigher-order GNNs. Through an in-depth exploration of these trends, we offer\ninsights into the evolving landscape of GNNs in histopathological analysis.\nBased on our findings, we propose future directions to propel the field\nforward. Our analysis serves to guide researchers and practitioners towards\ninnovative approaches and methodologies, fostering advancements in\nhistopathological analysis through the lens of graph neural networks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "q-bio.TO",
      "I.2.10; I.4.10; J.3"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12808v3",
    "published_date": "2024-06-18 17:23:50 UTC",
    "updated_date": "2024-06-21 08:57:40 UTC"
  },
  {
    "arxiv_id": "2406.12807v1",
    "title": "Probabilistic Temporal Prediction of Continuous Disease Trajectories and Treatment Effects Using Neural SDEs",
    "authors": [
      "Joshua Durso-Finley",
      "Berardino Barile",
      "Jean-Pierre Falet",
      "Douglas L. Arnold",
      "Nick Pawlowski",
      "Tal Arbel"
    ],
    "abstract": "Personalized medicine based on medical images, including predicting future\nindividualized clinical disease progression and treatment response, would have\nan enormous impact on healthcare and drug development, particularly for\ndiseases (e.g. multiple sclerosis (MS)) with long term, complex, heterogeneous\nevolutions and no cure. In this work, we present the first stochastic causal\ntemporal framework to model the continuous temporal evolution of disease\nprogression via Neural Stochastic Differential Equations (NSDE). The proposed\ncausal inference model takes as input the patient's high dimensional images\n(MRI) and tabular data, and predicts both factual and counterfactual\nprogression trajectories on different treatments in latent space. The NSDE\npermits the estimation of high-confidence personalized trajectories and\ntreatment effects. Extensive experiments were performed on a large,\nmulti-centre, proprietary dataset of patient 3D MRI and clinical data acquired\nduring several randomized clinical trials for MS treatments. Our results\npresent the first successful uncertainty-based causal Deep Learning (DL) model\nto: (a) accurately predict future patient MS disability evolution (e.g. EDSS)\nand treatment effects leveraging baseline MRI, and (b) permit the discovery of\nsubgroups of patients for which the model has high confidence in their response\nto treatment even in clinical trials which did not reach their clinical\nendpoints.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "eess.IV",
      "q-bio.QM"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12807v1",
    "published_date": "2024-06-18 17:22:55 UTC",
    "updated_date": "2024-06-18 17:22:55 UTC"
  },
  {
    "arxiv_id": "2406.12806v1",
    "title": "Identifying Performance-Sensitive Configurations in Software Systems through Code Analysis with LLM Agents",
    "authors": [
      "Zehao Wang",
      "Dong Jae Kim",
      "Tse-Hsun Chen"
    ],
    "abstract": "Configuration settings are essential for tailoring software behavior to meet\nspecific performance requirements. However, incorrect configurations are\nwidespread, and identifying those that impact system performance is challenging\ndue to the vast number and complexity of possible settings. In this work, we\npresent PerfSense, a lightweight framework that leverages Large Language Models\n(LLMs) to efficiently identify performance-sensitive configurations with\nminimal overhead. PerfSense employs LLM agents to simulate interactions between\ndevelopers and performance engineers using advanced prompting techniques such\nas prompt chaining and retrieval-augmented generation (RAG). Our evaluation of\nseven open-source Java systems demonstrates that PerfSense achieves an average\naccuracy of 64.77% in classifying performance-sensitive configurations,\noutperforming both our LLM baseline (50.36%) and the previous state-of-the-art\nmethod (61.75%). Notably, our prompt chaining technique improves recall by 10%\nto 30% while maintaining similar precision levels. Additionally, a manual\nanalysis of 362 misclassifications reveals common issues, including LLMs'\nmisunderstandings of requirements (26.8%). In summary, PerfSense significantly\nreduces manual effort in classifying performance-sensitive configurations and\noffers valuable insights for future LLM-based code analysis research.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12806v1",
    "published_date": "2024-06-18 17:22:48 UTC",
    "updated_date": "2024-06-18 17:22:48 UTC"
  },
  {
    "arxiv_id": "2406.12770v2",
    "title": "Informatics & dairy industry coalition: AI trends and present challenges",
    "authors": [
      "Silvia García-Méndez",
      "Francisco de Arriba-Pérez",
      "María del Carmen Somoza-López"
    ],
    "abstract": "Artificial Intelligence (AI) can potentially transform the industry,\nenhancing the production process and minimizing manual, repetitive tasks.\nAccordingly, the synergy between high-performance computing and powerful\nmathematical models enables the application of sophisticated data analysis\nprocedures like Machine Learning. However, challenges exist regarding\neffective, efficient, and flexible processing to generate valuable knowledge.\nConsequently, this work comprehensively describes industrial challenges where\nAI can be exploited, focusing on the dairy industry. The conclusions presented\ncan help researchers apply novel approaches for cattle monitoring and farmers\nby proposing advanced technological solutions to their needs.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12770v2",
    "published_date": "2024-06-18 16:39:21 UTC",
    "updated_date": "2024-06-19 11:49:03 UTC"
  },
  {
    "arxiv_id": "2406.12769v1",
    "title": "Latent Intuitive Physics: Learning to Transfer Hidden Physics from A 3D Video",
    "authors": [
      "Xiangming Zhu",
      "Huayu Deng",
      "Haochen Yuan",
      "Yunbo Wang",
      "Xiaokang Yang"
    ],
    "abstract": "We introduce latent intuitive physics, a transfer learning framework for\nphysics simulation that can infer hidden properties of fluids from a single 3D\nvideo and simulate the observed fluid in novel scenes. Our key insight is to\nuse latent features drawn from a learnable prior distribution conditioned on\nthe underlying particle states to capture the invisible and complex physical\nproperties. To achieve this, we train a parametrized prior learner given visual\nobservations to approximate the visual posterior of inverse graphics, and both\nthe particle states and the visual posterior are obtained from a learned neural\nrenderer. The converged prior learner is embedded in our probabilistic physics\nengine, allowing us to perform novel simulations on unseen geometries,\nboundaries, and dynamics without knowledge of the true physical parameters. We\nvalidate our model in three ways: (i) novel scene simulation with the learned\nvisual-world physics, (ii) future prediction of the observed fluid dynamics,\nand (iii) supervised particle simulation. Our model demonstrates strong\nperformance in all three tasks.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "Published as a conference paper at ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.12769v1",
    "published_date": "2024-06-18 16:37:44 UTC",
    "updated_date": "2024-06-18 16:37:44 UTC"
  },
  {
    "arxiv_id": "2406.12762v1",
    "title": "Unsupervised explainable activity prediction in competitive Nordic Walking from experimental data",
    "authors": [
      "Silvia García-Méndez",
      "Francisco de Arriba-Pérez",
      "Francisco J. González-Castaño",
      "Javier Vales-Alonso"
    ],
    "abstract": "Artificial Intelligence (AI) has found application in Human Activity\nRecognition (HAR) in competitive sports. To date, most Machine Learning (ML)\napproaches for HAR have relied on offline (batch) training, imposing higher\ncomputational and tagging burdens compared to online processing unsupervised\napproaches. Additionally, the decisions behind traditional ML predictors are\nopaque and require human interpretation. In this work, we apply an online\nprocessing unsupervised clustering approach based on low-cost wearable Inertial\nMeasurement Units (IMUs). The outcomes generated by the system allow for the\nautomatic expansion of limited tagging available (e.g., by referees) within\nthose clusters, producing pertinent information for the explainable\nclassification stage. Specifically, our work focuses on achieving automatic\nexplainability for predictions related to athletes' activities, distinguishing\nbetween correct, incorrect, and cheating practices in Nordic Walking. The\nproposed solution achieved performance metrics of close to 100 % on average.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12762v1",
    "published_date": "2024-06-18 16:29:07 UTC",
    "updated_date": "2024-06-18 16:29:07 UTC"
  },
  {
    "arxiv_id": "2406.12754v1",
    "title": "Chumor 1.0: A Truly Funny and Challenging Chinese Humor Understanding Dataset from Ruo Zhi Ba",
    "authors": [
      "Ruiqi He",
      "Yushu He",
      "Longju Bai",
      "Jiarui Liu",
      "Zhenjie Sun",
      "Zenghao Tang",
      "He Wang",
      "Hanchen Xia",
      "Naihao Deng"
    ],
    "abstract": "Existing humor datasets and evaluations predominantly focus on English,\nlacking resources for culturally nuanced humor in non-English languages like\nChinese. To address this gap, we construct Chumor, a dataset sourced from Ruo\nZhi Ba (RZB), a Chinese Reddit-like platform dedicated to sharing\nintellectually challenging and culturally specific jokes. We annotate\nexplanations for each joke and evaluate human explanations against two\nstate-of-the-art LLMs, GPT-4o and ERNIE Bot, through A/B testing by native\nChinese speakers. Our evaluation shows that Chumor is challenging even for SOTA\nLLMs, and the human explanations for Chumor jokes are significantly better than\nexplanations generated by the LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12754v1",
    "published_date": "2024-06-18 16:22:05 UTC",
    "updated_date": "2024-06-18 16:22:05 UTC"
  },
  {
    "arxiv_id": "2406.12753v2",
    "title": "OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI",
    "authors": [
      "Zhen Huang",
      "Zengzhi Wang",
      "Shijie Xia",
      "Xuefeng Li",
      "Haoyang Zou",
      "Ruijie Xu",
      "Run-Ze Fan",
      "Lyumanshan Ye",
      "Ethan Chern",
      "Yixin Ye",
      "Yikai Zhang",
      "Yuqing Yang",
      "Ting Wu",
      "Binjie Wang",
      "Shichao Sun",
      "Yang Xiao",
      "Yiyuan Li",
      "Fan Zhou",
      "Steffi Chern",
      "Yiwei Qin",
      "Yan Ma",
      "Jiadi Su",
      "Yixiu Liu",
      "Yuxiang Zheng",
      "Shaoting Zhang",
      "Dahua Lin",
      "Yu Qiao",
      "Pengfei Liu"
    ],
    "abstract": "The evolution of Artificial Intelligence (AI) has been significantly\naccelerated by advancements in Large Language Models (LLMs) and Large\nMultimodal Models (LMMs), gradually showcasing potential cognitive reasoning\nabilities in problem-solving and scientific discovery (i.e., AI4Science) once\nexclusive to human intellect. To comprehensively evaluate current models'\nperformance in cognitive reasoning abilities, we introduce OlympicArena, which\nincludes 11,163 bilingual problems across both text-only and interleaved\ntext-image modalities. These challenges encompass a wide range of disciplines\nspanning seven fields and 62 international Olympic competitions, rigorously\nexamined for data leakage. We argue that the challenges in Olympic competition\nproblems are ideal for evaluating AI's cognitive reasoning due to their\ncomplexity and interdisciplinary nature, which are essential for tackling\ncomplex scientific challenges and facilitating discoveries. Beyond evaluating\nperformance across various disciplines using answer-only criteria, we conduct\ndetailed experiments and analyses from multiple perspectives. We delve into the\nmodels' cognitive reasoning abilities, their performance across different\nmodalities, and their outcomes in process-level evaluations, which are vital\nfor tasks requiring complex reasoning with lengthy solutions. Our extensive\nevaluations reveal that even advanced models like GPT-4o only achieve a 39.97%\noverall accuracy, illustrating current AI limitations in complex reasoning and\nmultimodal integration. Through the OlympicArena, we aim to advance AI towards\nsuperintelligence, equipping it to address more complex challenges in science\nand beyond. We also provide a comprehensive set of resources to support AI\nresearch, including a benchmark dataset, an open-source annotation platform, a\ndetailed evaluation tool, and a leaderboard with automatic submission features.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.12753v2",
    "published_date": "2024-06-18 16:20:53 UTC",
    "updated_date": "2025-03-06 12:55:25 UTC"
  },
  {
    "arxiv_id": "2406.12747v2",
    "title": "TSI-Bench: Benchmarking Time Series Imputation",
    "authors": [
      "Wenjie Du",
      "Jun Wang",
      "Linglong Qian",
      "Yiyuan Yang",
      "Zina Ibrahim",
      "Fanxing Liu",
      "Zepu Wang",
      "Haoxin Liu",
      "Zhiyuan Zhao",
      "Yingjie Zhou",
      "Wenjia Wang",
      "Kaize Ding",
      "Yuxuan Liang",
      "B. Aditya Prakash",
      "Qingsong Wen"
    ],
    "abstract": "Effective imputation is a crucial preprocessing step for time series\nanalysis. Despite the development of numerous deep learning algorithms for time\nseries imputation, the community lacks standardized and comprehensive benchmark\nplatforms to effectively evaluate imputation performance across different\nsettings. Moreover, although many deep learning forecasting algorithms have\ndemonstrated excellent performance, whether their modelling achievements can be\ntransferred to time series imputation tasks remains unexplored. To bridge these\ngaps, we develop TSI-Bench, the first (to our knowledge) comprehensive\nbenchmark suite for time series imputation utilizing deep learning techniques.\nThe TSI-Bench pipeline standardizes experimental settings to enable fair\nevaluation of imputation algorithms and identification of meaningful insights\ninto the influence of domain-appropriate missing rates and patterns on model\nperformance. Furthermore, TSI-Bench innovatively provides a systematic paradigm\nto tailor time series forecasting algorithms for imputation purposes. Our\nextensive study across 34,804 experiments, 28 algorithms, and 8 datasets with\ndiverse missingness scenarios demonstrates TSI-Bench's effectiveness in diverse\ndownstream tasks and potential to unlock future directions in time series\nimputation research and analysis. All source code and experiment logs are\nreleased at https://github.com/WenjieDu/AwesomeImputation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12747v2",
    "published_date": "2024-06-18 16:07:33 UTC",
    "updated_date": "2024-10-31 17:18:16 UTC"
  },
  {
    "arxiv_id": "2406.12744v1",
    "title": "Ensuring Both Positivity and Stability Using Sector-Bounded Nonlinearity for Systems with Neural Network Controllers",
    "authors": [
      "Hamidreza Montazeri Hedesh",
      "Milad Siami"
    ],
    "abstract": "This paper introduces a novel method for the stability analysis of positive\nfeedback systems with a class of fully connected feedforward neural networks\n(FFNN) controllers. By establishing sector bounds for fully connected FFNNs\nwithout biases, we present a stability theorem that demonstrates the global\nexponential stability of linear systems under fully connected FFNN control.\nUtilizing principles from positive Lur'e systems and the positive Aizerman\nconjecture, our approach effectively addresses the challenge of ensuring\nstability in highly nonlinear systems. The crux of our method lies in\nmaintaining sector bounds that preserve the positivity and Hurwitz property of\nthe overall Lur'e system. We showcase the practical applicability of our\nmethodology through its implementation in a linear system managed by a FFNN\ntrained on output feedback controller data, highlighting its potential for\nenhancing stability in dynamic systems.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.SY",
      "math.OC",
      "G.1.2; I.2.3; I.2.8"
    ],
    "primary_category": "eess.SY",
    "comment": "6 pages, 7 figures, to be published in IEEE Control Systems Letters\n  (L-CSS)",
    "pdf_url": "http://arxiv.org/pdf/2406.12744v1",
    "published_date": "2024-06-18 16:05:57 UTC",
    "updated_date": "2024-06-18 16:05:57 UTC"
  },
  {
    "arxiv_id": "2407.02509v1",
    "title": "Variables are a Curse in Software Vulnerability Prediction",
    "authors": [
      "Jinghua Groppe",
      "Sven Groppe",
      "Ralf Möller"
    ],
    "abstract": "Deep learning-based approaches for software vulnerability prediction\ncurrently mainly rely on the original text of software code as the feature of\nnodes in the graph of code and thus could learn a representation that is only\nspecific to the code text, rather than the representation that depicts the\n'intrinsic' functionality of a program hidden in the text representation. One\ncurse that causes this problem is an infinite number of possibilities to name a\nvariable. In order to lift the curse, in this work we introduce a new type of\nedge called name dependence, a type of abstract syntax graph based on the name\ndependence, and an efficient node representation method named 3-property\nencoding scheme. These techniques will allow us to remove the concrete variable\nnames from code, and facilitate deep learning models to learn the functionality\nof software hidden in diverse code expressions. The experimental results show\nthat the deep learning models built on these techniques outperform the ones\nbased on existing approaches not only in the prediction of vulnerabilities but\nalso in the memory need. The factor of memory usage reductions of our\ntechniques can be up to the order of 30,000 in comparison to existing\napproaches.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CR",
      "cs.LG",
      "I.2.0; D.2.m"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.02509v1",
    "published_date": "2024-06-18 16:02:29 UTC",
    "updated_date": "2024-06-18 16:02:29 UTC"
  },
  {
    "arxiv_id": "2406.12742v1",
    "title": "Benchmarking Multi-Image Understanding in Vision and Language Models: Perception, Knowledge, Reasoning, and Multi-Hop Reasoning",
    "authors": [
      "Bingchen Zhao",
      "Yongshuo Zong",
      "Letian Zhang",
      "Timothy Hospedales"
    ],
    "abstract": "The advancement of large language models (LLMs) has significantly broadened\nthe scope of applications in natural language processing, with multi-modal LLMs\nextending these capabilities to integrate and interpret visual data. However,\nexisting benchmarks for visual language models (VLMs) predominantly focus on\nsingle-image inputs, neglecting the crucial aspect of multi-image\nunderstanding. In this paper, we introduce a Multi-Image Relational Benchmark\nMIRB, designed to evaluate VLMs' ability to compare, analyze, and reason across\nmultiple images. Our benchmark encompasses four categories: perception, visual\nworld knowledge, reasoning, and multi-hop reasoning. Through a comprehensive\nevaluation of a wide range of open-source and closed-source models, we\ndemonstrate that while open-source VLMs were shown to approach the performance\nof GPT-4V in single-image tasks, a significant performance gap remains in\nmulti-image reasoning tasks. Our findings also reveal that even the\nstate-of-the-art GPT-4V model struggles with our benchmark, underscoring the\nneed for further research and development in this area. We believe our\ncontribution of MIRB could serve as a testbed for developing the\nnext-generation multi-modal models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "First three authors contributed equally. Dataset:\n  https://huggingface.co/datasets/VLLMs/MIRB",
    "pdf_url": "http://arxiv.org/pdf/2406.12742v1",
    "published_date": "2024-06-18 16:02:18 UTC",
    "updated_date": "2024-06-18 16:02:18 UTC"
  },
  {
    "arxiv_id": "2406.12738v1",
    "title": "Large Language Model as a Universal Clinical Multi-task Decoder",
    "authors": [
      "Yujiang Wu",
      "Hongjian Song",
      "Jiawen Zhang",
      "Xumeng Wen",
      "Shun Zheng",
      "Jiang Bian"
    ],
    "abstract": "The development of effective machine learning methodologies for enhancing the\nefficiency and accuracy of clinical systems is crucial. Despite significant\nresearch efforts, managing a plethora of diversified clinical tasks and\nadapting to emerging new tasks remain significant challenges. This paper\npresents a novel paradigm that employs a pre-trained large language model as a\nuniversal clinical multi-task decoder. This approach leverages the flexibility\nand diversity of language expressions to handle task topic variations and\nassociated arguments. The introduction of a new task simply requires the\naddition of a new instruction template. We validate this framework across\nhundreds of tasks, demonstrating its robustness in facilitating multi-task\npredictions, performing on par with traditional multi-task learning and\nsingle-task learning approaches. Moreover, it shows exceptional adaptability to\nnew tasks, with impressive zero-shot performance in some instances and superior\ndata efficiency in few-shot scenarios. This novel approach offers a unified\nsolution to manage a wide array of new and emerging tasks in clinical\napplications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2406.12738v1",
    "published_date": "2024-06-18 15:58:36 UTC",
    "updated_date": "2024-06-18 15:58:36 UTC"
  },
  {
    "arxiv_id": "2406.12736v1",
    "title": "Beyond Visual Appearances: Privacy-sensitive Objects Identification via Hybrid Graph Reasoning",
    "authors": [
      "Zhuohang Jiang",
      "Bingkui Tong",
      "Xia Du",
      "Ahmed Alhammadi",
      "Jizhe Zhou"
    ],
    "abstract": "The Privacy-sensitive Object Identification (POI) task allocates bounding\nboxes for privacy-sensitive objects in a scene. The key to POI is settling an\nobject's privacy class (privacy-sensitive or non-privacy-sensitive). In\ncontrast to conventional object classes which are determined by the visual\nappearance of an object, one object's privacy class is derived from the scene\ncontexts and is subject to various implicit factors beyond its visual\nappearance. That is, visually similar objects may be totally opposite in their\nprivacy classes. To explicitly derive the objects' privacy class from the scene\ncontexts, in this paper, we interpret the POI task as a visual reasoning task\naimed at the privacy of each object in the scene. Following this\ninterpretation, we propose the PrivacyGuard framework for POI. PrivacyGuard\ncontains three stages. i) Structuring: an unstructured image is first converted\ninto a structured, heterogeneous scene graph that embeds rich scene contexts.\nii) Data Augmentation: a contextual perturbation oversampling strategy is\nproposed to create slightly perturbed privacy-sensitive objects in a scene\ngraph, thereby balancing the skewed distribution of privacy classes. iii)\nHybrid Graph Generation & Reasoning: the balanced, heterogeneous scene graph is\nthen transformed into a hybrid graph by endowing it with extra \"node-node\" and\n\"edge-edge\" homogeneous paths. These homogeneous paths allow direct message\npassing between nodes or edges, thereby accelerating reasoning and facilitating\nthe capturing of subtle context changes. Based on this hybrid graph... **For\nthe full abstract, see the original paper.**",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "15 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.12736v1",
    "published_date": "2024-06-18 15:58:22 UTC",
    "updated_date": "2024-06-18 15:58:22 UTC"
  },
  {
    "arxiv_id": "2406.12732v1",
    "title": "Automatic generation of insights from workers' actions in industrial workflows with explainable Machine Learning",
    "authors": [
      "Francisco de Arriba-Pérez",
      "Silvia García-Méndez",
      "Javier Otero-Mosquera",
      "Francisco J. González-Castaño",
      "Felipe Gil-Castiñeira"
    ],
    "abstract": "New technologies such as Machine Learning (ML) gave great potential for\nevaluating industry workflows and automatically generating key performance\nindicators (KPIs). However, despite established standards for measuring the\nefficiency of industrial machinery, there is no precise equivalent for workers'\nproductivity, which would be highly desirable given the lack of a skilled\nworkforce for the next generation of industry workflows. Therefore, an ML\nsolution combining data from manufacturing processes and workers' performance\nfor that goal is required. Additionally, in recent times intense effort has\nbeen devoted to explainable ML approaches that can automatically explain their\ndecisions to a human operator, thus increasing their trustworthiness. We\npropose to apply explainable ML solutions to differentiate between expert and\ninexpert workers in industrial workflows, which we validate at a quality\nassessment industrial workstation. Regarding the methodology used, input data\nare captured by a manufacturing machine and stored in a NoSQL database. Data\nare processed to engineer features used in automatic classification and to\ncompute workers' KPIs to predict their level of expertise (with all\nclassification metrics exceeding 90 %). These KPIs, and the relevant features\nin the decisions are textually explained by natural language expansion on an\nexplainability dashboard. These automatic explanations made it possible to\ninfer knowledge from expert workers for inexpert workers. The latter\nillustrates the interest of research in self-explainable ML for automatically\ngenerating insights to improve productivity in industrial workflows.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "IEEE Industrial Electronics Magazine (2023)",
    "pdf_url": "http://arxiv.org/pdf/2406.12732v1",
    "published_date": "2024-06-18 15:55:11 UTC",
    "updated_date": "2024-06-18 15:55:11 UTC"
  },
  {
    "arxiv_id": "2406.12954v1",
    "title": "Skin Cancer Images Classification using Transfer Learning Techniques",
    "authors": [
      "Md Sirajul Islam",
      "Sanjeev Panta"
    ],
    "abstract": "Skin cancer is one of the most common and deadliest types of cancer. Early\ndiagnosis of skin cancer at a benign stage is critical to reducing cancer\nmortality. To detect skin cancer at an earlier stage an automated system is\ncompulsory that can save the life of many patients. Many previous studies have\naddressed the problem of skin cancer diagnosis using various deep learning and\ntransfer learning models. However, existing literature has limitations in its\naccuracy and time-consuming procedure. In this work, we applied five different\npre-trained transfer learning approaches for binary classification of skin\ncancer detection at benign and malignant stages. To increase the accuracy of\nthese models we fine-tune different layers and activation functions. We used a\npublicly available ISIC dataset to evaluate transfer learning approaches. For\nmodel stability, data augmentation techniques are applied to improve the\nrandomness of the input dataset. These approaches are evaluated using different\nhyperparameters such as batch sizes, epochs, and optimizers. The experimental\nresults show that the ResNet-50 model provides an accuracy of 0.935, F1-score\nof 0.86, and precision of 0.94.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12954v1",
    "published_date": "2024-06-18 15:48:20 UTC",
    "updated_date": "2024-06-18 15:48:20 UTC"
  },
  {
    "arxiv_id": "2406.12725v1",
    "title": "Can Large Language Models Code Like a Linguist?: A Case Study in Low Resource Sound Law Induction",
    "authors": [
      "Atharva Naik",
      "Kexun Zhang",
      "Nathaniel Robinson",
      "Aravind Mysore",
      "Clayton Marr",
      "Hong Sng",
      "Rebecca Byrnes",
      "Anna Cai",
      "Kalvin Chang",
      "David Mortensen"
    ],
    "abstract": "Historical linguists have long written a kind of incompletely formalized\n''program'' that converts reconstructed words in an ancestor language into\nwords in one of its attested descendants that consist of a series of ordered\nstring rewrite functions (called sound laws). They do this by observing pairs\nof words in the reconstructed language (protoforms) and the descendent language\n(reflexes) and constructing a program that transforms protoforms into reflexes.\nHowever, writing these programs is error-prone and time-consuming. Prior work\nhas successfully scaffolded this process computationally, but fewer researchers\nhave tackled Sound Law Induction (SLI), which we approach in this paper by\ncasting it as Programming by Examples. We propose a language-agnostic solution\nthat utilizes the programming ability of Large Language Models (LLMs) by\ngenerating Python sound law programs from sound change examples. We evaluate\nthe effectiveness of our approach for various LLMs, propose effective methods\nto generate additional language-agnostic synthetic data to fine-tune LLMs for\nSLI, and compare our method with existing automated SLI methods showing that\nwhile LLMs lag behind them they can complement some of their weaknesses.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12725v1",
    "published_date": "2024-06-18 15:46:04 UTC",
    "updated_date": "2024-06-18 15:46:04 UTC"
  },
  {
    "arxiv_id": "2406.12723v6",
    "title": "BIOSCAN-5M: A Multimodal Dataset for Insect Biodiversity",
    "authors": [
      "Zahra Gharaee",
      "Scott C. Lowe",
      "ZeMing Gong",
      "Pablo Millan Arias",
      "Nicholas Pellegrino",
      "Austin T. Wang",
      "Joakim Bruslund Haurum",
      "Iuliia Zarubiieva",
      "Lila Kari",
      "Dirk Steinke",
      "Graham W. Taylor",
      "Paul Fieguth",
      "Angel X. Chang"
    ],
    "abstract": "As part of an ongoing worldwide effort to comprehend and monitor insect\nbiodiversity, this paper presents the BIOSCAN-5M Insect dataset to the machine\nlearning community and establish several benchmark tasks. BIOSCAN-5M is a\ncomprehensive dataset containing multi-modal information for over 5 million\ninsect specimens, and it significantly expands existing image-based biological\ndatasets by including taxonomic labels, raw nucleotide barcode sequences,\nassigned barcode index numbers, geographical, and size information. We propose\nthree benchmark experiments to demonstrate the impact of the multi-modal data\ntypes on the classification and clustering accuracy. First, we pretrain a\nmasked language model on the DNA barcode sequences of the BIOSCAN-5M dataset,\nand demonstrate the impact of using this large reference library on species-\nand genus-level classification performance. Second, we propose a zero-shot\ntransfer learning task applied to images and DNA barcodes to cluster feature\nembeddings obtained from self-supervised learning, to investigate whether\nmeaningful clusters can be derived from these representation embeddings. Third,\nwe benchmark multi-modality by performing contrastive learning on DNA barcodes,\nimage data, and taxonomic information. This yields a general shared embedding\nspace enabling taxonomic classification using multiple types of information and\nmodalities. The code repository of the BIOSCAN-5M Insect dataset is available\nat https://github.com/bioscan-ml/BIOSCAN-5M.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "q-bio.PE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12723v6",
    "published_date": "2024-06-18 15:45:21 UTC",
    "updated_date": "2025-03-01 00:03:47 UTC"
  },
  {
    "arxiv_id": "2406.12719v2",
    "title": "On the Robustness of Language Models for Tabular Question Answering",
    "authors": [
      "Kushal Raj Bhandari",
      "Sixue Xing",
      "Soham Dan",
      "Jianxi Gao"
    ],
    "abstract": "Large Language Models (LLMs), already shown to ace various text comprehension\ntasks have also remarkably been shown to tackle table comprehension tasks\nwithout specific training. While previous research has explored LLM\ncapabilities with tabular dataset tasks, our study assesses the influence of\n\\textit{in-context learning}, \\textit{model scale}, \\textit{instruction\ntuning}, and \\textit{domain biases} on Tabular Question Answering (TQA). We\nevaluate the robustness of LLMs on Wikipedia-based \\textbf{WTQ}, financial\nreport-based \\textbf{TAT-QA}, and scientific claims-based \\textbf{SCITAB}, TQA\ndatasets, focusing on their ability to interpret tabular data under various\naugmentations and perturbations robustly. Our findings indicate that\ninstructions significantly enhance performance, with recent models exhibiting\ngreater robustness over earlier versions. However, data contamination and\npractical reliability issues persist, especially with \\textbf{WTQ}. We\nhighlight the need for improved methodologies, including structure-aware\nself-attention mechanisms and better handling of domain-specific tabular data,\nto develop more reliable LLMs for table comprehension.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12719v2",
    "published_date": "2024-06-18 15:41:15 UTC",
    "updated_date": "2025-03-21 00:31:06 UTC"
  },
  {
    "arxiv_id": "2406.12718v3",
    "title": "Mitigating Object Hallucinations in Large Vision-Language Models with Assembly of Global and Local Attention",
    "authors": [
      "Wenbin An",
      "Feng Tian",
      "Sicong Leng",
      "Jiahao Nie",
      "Haonan Lin",
      "QianYing Wang",
      "Ping Chen",
      "Xiaoqin Zhang",
      "Shijian Lu"
    ],
    "abstract": "Despite great success across various multimodal tasks, Large Vision-Language\nModels (LVLMs) often encounter object hallucinations with generated textual\nresponses being inconsistent with the actual objects in images. We examine\ndifferent LVLMs and pinpoint that one root cause of object hallucinations lies\nwith deficient attention on discriminative image features. Specifically, LVLMs\noften predominantly attend to prompt-irrelevant global features instead of\nprompt-relevant local features, undermining their visual grounding capacity and\nleading to object hallucinations. We propose Assembly of Global and Local\nAttention (AGLA), a training-free and plug-and-play approach that mitigates\nhallucinations by assembling global features for response generation and local\nfeatures for visual discrimination simultaneously. Specifically, we introduce\nan image-prompt matching scheme that captures prompt-relevant local features\nfrom images, leading to an augmented view of the input image where\nprompt-relevant content is highlighted while irrelevant distractions are\nsuppressed. Hallucinations can thus be mitigated with a calibrated logit\ndistribution that is from generative global features of the original image and\ndiscriminative local features of the augmented image. Extensive experiments\nshow the superiority of AGLA in LVLM hallucination mitigation, demonstrating\nits wide applicability across both discriminative and generative tasks. Our\ncode is available at https://github.com/Lackel/AGLA.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2406.12718v3",
    "published_date": "2024-06-18 15:38:41 UTC",
    "updated_date": "2025-03-14 04:38:44 UTC"
  },
  {
    "arxiv_id": "2406.12709v2",
    "title": "Enhancing Spatio-temporal Quantile Forecasting with Curriculum Learning: Lessons Learned",
    "authors": [
      "Du Yin",
      "Jinliang Deng",
      "Shuang Ao",
      "Zechen Li",
      "Hao Xue",
      "Arian Prabowo",
      "Renhe Jiang",
      "Xuan Song",
      "Flora Salim"
    ],
    "abstract": "Training models on spatio-temporal (ST) data poses an open problem due to the\ncomplicated and diverse nature of the data itself, and it is challenging to\nensure the model's performance directly trained on the original ST data. While\nlimiting the variety of training data can make training easier, it can also\nlead to a lack of knowledge and information for the model, resulting in a\ndecrease in performance. To address this challenge, we presented an innovative\nparadigm that incorporates three separate forms of curriculum learning\nspecifically targeting from spatial, temporal, and quantile perspectives.\nFurthermore, our framework incorporates a stacking fusion module to combine\ndiverse information from three types of curriculum learning, resulting in a\nstrong and thorough learning process. We demonstrated the effectiveness of this\nframework with extensive empirical evaluations, highlighting its better\nperformance in addressing complex ST challenges. We provided thorough ablation\nstudies to investigate the effectiveness of our curriculum and to explain how\nit contributes to the improvement of learning efficiency on ST data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "accept by sigspatial 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.12709v2",
    "published_date": "2024-06-18 15:23:10 UTC",
    "updated_date": "2024-09-16 14:44:53 UTC"
  },
  {
    "arxiv_id": "2406.12707v1",
    "title": "Talk With Human-like Agents: Empathetic Dialogue Through Perceptible Acoustic Reception and Reaction",
    "authors": [
      "Haoqiu Yan",
      "Yongxin Zhu",
      "Kai Zheng",
      "Bing Liu",
      "Haoyu Cao",
      "Deqiang Jiang",
      "Linli Xu"
    ],
    "abstract": "Large Language Model (LLM)-enhanced agents become increasingly prevalent in\nHuman-AI communication, offering vast potential from entertainment to\nprofessional domains. However, current multi-modal dialogue systems overlook\nthe acoustic information present in speech, which is crucial for understanding\nhuman communication nuances. This oversight can lead to misinterpretations of\nspeakers' intentions, resulting in inconsistent or even contradictory responses\nwithin dialogues. To bridge this gap, in this paper, we propose\nPerceptiveAgent, an empathetic multi-modal dialogue system designed to discern\ndeeper or more subtle meanings beyond the literal interpretations of words\nthrough the integration of speech modality perception. Employing LLMs as a\ncognitive core, PerceptiveAgent perceives acoustic information from input\nspeech and generates empathetic responses based on speaking styles described in\nnatural language. Experimental results indicate that PerceptiveAgent excels in\ncontextual understanding by accurately discerning the speakers' true intentions\nin scenarios where the linguistic meaning is either contrary to or inconsistent\nwith the speaker's true feelings, producing more nuanced and expressive spoken\ndialogues. Code is publicly available at:\n\\url{https://github.com/Haoqiu-Yan/PerceptiveAgent}.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, 3 figures, ACL24 accepted",
    "pdf_url": "http://arxiv.org/pdf/2406.12707v1",
    "published_date": "2024-06-18 15:19:51 UTC",
    "updated_date": "2024-06-18 15:19:51 UTC"
  },
  {
    "arxiv_id": "2406.12698v1",
    "title": "Online-Adaptive Anomaly Detection for Defect Identification in Aircraft Assembly",
    "authors": [
      "Siddhant Shete",
      "Dennis Mronga",
      "Ankita Jadhav",
      "Frank Kirchner"
    ],
    "abstract": "Anomaly detection deals with detecting deviations from established patterns\nwithin data. It has various applications like autonomous driving, predictive\nmaintenance, and medical diagnosis. To improve anomaly detection accuracy,\ntransfer learning can be applied to large, pre-trained models and adapt them to\nthe specific application context. In this paper, we propose a novel framework\nfor online-adaptive anomaly detection using transfer learning. The approach\nadapts to different environments by selecting visually similar training images\nand online fitting a normality model to EfficientNet features extracted from\nthe training subset. Anomaly detection is then performed by computing the\nMahalanobis distance between the normality model and the test image features.\nDifferent similarity measures (SIFT/FLANN, Cosine) and normality models (MVG,\nOCSVM) are employed and compared with each other. We evaluate the approach on\ndifferent anomaly detection benchmarks and data collected in controlled\nlaboratory settings. Experimental results showcase a detection accuracy\nexceeding 0.975, outperforming the state-of-the-art ET-NET approach.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "This is preprint for the accepted paper",
    "pdf_url": "http://arxiv.org/pdf/2406.12698v1",
    "published_date": "2024-06-18 15:11:44 UTC",
    "updated_date": "2024-06-18 15:11:44 UTC"
  },
  {
    "arxiv_id": "2406.12693v2",
    "title": "XXLTraffic: Expanding and Extremely Long Traffic forecasting beyond test adaptation",
    "authors": [
      "Du Yin",
      "Hao Xue",
      "Arian Prabowo",
      "Shuang Ao",
      "Flora Salim"
    ],
    "abstract": "Traffic forecasting is crucial for smart cities and intelligent\ntransportation initiatives, where deep learning has made significant progress\nin modeling complex spatio-temporal patterns in recent years. However, current\npublic datasets have limitations in reflecting the distribution shift nature of\nreal-world scenarios, characterized by continuously evolving infrastructures,\nvarying temporal distributions, and long temporal gaps due to sensor downtimes\nor changes in traffic patterns. These limitations inevitably restrict the\npractical applicability of existing traffic forecasting datasets. To bridge\nthis gap, we present XXLTraffic, largest available public traffic dataset with\nthe longest timespan collected from Los Angeles, USA, and New South Wales,\nAustralia, curated to support research in extremely long forecasting beyond\ntest adaptation. Our benchmark includes both typical time-series forecasting\nsettings with hourly and daily aggregated data and novel configurations that\nintroduce gaps and down-sample the training size to better simulate practical\nconstraints. We anticipate the new XXLTraffic will provide a fresh perspective\nfor the time-series and traffic forecasting communities. It would also offer a\nrobust platform for developing and evaluating models designed to tackle the\nextremely long forecasting problems beyond test adaptation. Our dataset\nsupplements existing spatio-temporal data resources and leads to new research\ndirections in this domain.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12693v2",
    "published_date": "2024-06-18 15:06:22 UTC",
    "updated_date": "2025-03-25 05:39:42 UTC"
  },
  {
    "arxiv_id": "2406.12692v3",
    "title": "MAGIC: Generating Self-Correction Guideline for In-Context Text-to-SQL",
    "authors": [
      "Arian Askari",
      "Christian Poelitz",
      "Xinye Tang"
    ],
    "abstract": "Self-correction in text-to-SQL is the process of prompting large language\nmodel (LLM) to revise its previously incorrectly generated SQL, and commonly\nrelies on manually crafted self-correction guidelines by human experts that are\nnot only labor-intensive to produce but also limited by the human ability in\nidentifying all potential error patterns in LLM responses. We introduce MAGIC,\na novel multi-agent method that automates the creation of the self-correction\nguideline. MAGIC uses three specialized agents: a manager, a correction, and a\nfeedback agent. These agents collaborate on the failures of an LLM-based method\non the training set to iteratively generate and refine a self-correction\nguideline tailored to LLM mistakes, mirroring human processes but without human\ninvolvement. Our extensive experiments show that MAGIC's guideline outperforms\nexpert human's created ones. We empirically find out that the guideline\nproduced by MAGIC enhances the interpretability of the corrections made,\nproviding insights in analyzing the reason behind the failures and successes of\nLLMs in self-correction. All agent interactions are publicly available at\nhttps://huggingface.co/datasets/microsoft/MAGIC.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at Proceedings of the Thirty-Ninth AAAI Conference on\n  Artificial Intelligence (AAAI 2025)",
    "pdf_url": "http://arxiv.org/pdf/2406.12692v3",
    "published_date": "2024-06-18 15:06:06 UTC",
    "updated_date": "2024-12-21 16:25:28 UTC"
  },
  {
    "arxiv_id": "2406.12952v3",
    "title": "SWT-Bench: Testing and Validating Real-World Bug-Fixes with Code Agents",
    "authors": [
      "Niels Mündler",
      "Mark Niklas Müller",
      "Jingxuan He",
      "Martin Vechev"
    ],
    "abstract": "Rigorous software testing is crucial for developing and maintaining\nhigh-quality code, making automated test generation a promising avenue for both\nimproving software quality and boosting the effectiveness of code generation\nmethods. However, while code generation with Large Language Models (LLMs) is an\nextraordinarily active research area, test generation remains relatively\nunexplored. We address this gap and investigate the capability of LLM-based\nCode Agents to formalize user issues into test cases. To this end, we propose a\nnovel benchmark based on popular GitHub repositories, containing real-world\nissues, ground-truth bug-fixes, and golden tests. We find that LLMs generally\nperform surprisingly well at generating relevant test cases, with Code Agents\ndesigned for code repair exceeding the performance of systems designed\nspecifically for test generation. Further, as test generation is a similar but\nmore structured task than code generation, it allows for a more fine-grained\nanalysis using issue reproduction rate and coverage changes, providing a dual\nmetric for analyzing systems designed for code repair. Finally, we find that\ngenerated tests are an effective filter for proposed code fixes, doubling the\nprecision of SWE-Agent. We release all data and code at\nhttps://github.com/logic-star-ai/SWT-Bench",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "20 pages, 14 figures, 7 tables",
    "pdf_url": "http://arxiv.org/pdf/2406.12952v3",
    "published_date": "2024-06-18 14:54:37 UTC",
    "updated_date": "2025-02-07 12:33:06 UTC"
  },
  {
    "arxiv_id": "2406.12672v1",
    "title": "Sparsifying dimensionality reduction of PDE solution data with Bregman learning",
    "authors": [
      "Tjeerd Jan Heeringa",
      "Christoph Brune",
      "Mengwu Guo"
    ],
    "abstract": "Classical model reduction techniques project the governing equations onto a\nlinear subspace of the original state space. More recent data-driven techniques\nuse neural networks to enable nonlinear projections. Whilst those often enable\nstronger compression, they may have redundant parameters and lead to suboptimal\nlatent dimensionality. To overcome these, we propose a multistep algorithm that\ninduces sparsity in the encoder-decoder networks for effective reduction in the\nnumber of parameters and additional compression of the latent space. This\nalgorithm starts with sparsely initialized a network and training it using\nlinearized Bregman iterations. These iterations have been very successful in\ncomputer vision and compressed sensing tasks, but have not yet been used for\nreduced-order modelling. After the training, we further compress the latent\nspace dimensionality by using a form of proper orthogonal decomposition. Last,\nwe use a bias propagation technique to change the induced sparsity into an\neffective reduction of parameters. We apply this algorithm to three\nrepresentative PDE models: 1D diffusion, 1D advection, and 2D\nreaction-diffusion. Compared to conventional training methods like Adam, the\nproposed method achieves similar accuracy with 30% less parameters and a\nsignificantly smaller latent space.",
    "categories": [
      "math.NA",
      "cs.AI",
      "cs.NA",
      "stat.ML",
      "65K10 (Primary) 68T07, 65D99, 41A63 (Secondary)",
      "G.1.6; I.2.6"
    ],
    "primary_category": "math.NA",
    "comment": "27 pages, 20 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2406.12672v1",
    "published_date": "2024-06-18 14:45:30 UTC",
    "updated_date": "2024-06-18 14:45:30 UTC"
  },
  {
    "arxiv_id": "2406.12670v2",
    "title": "Stealth edits to large language models",
    "authors": [
      "Oliver J. Sutton",
      "Qinghua Zhou",
      "Wei Wang",
      "Desmond J. Higham",
      "Alexander N. Gorban",
      "Alexander Bastounis",
      "Ivan Y. Tyukin"
    ],
    "abstract": "We reveal the theoretical foundations of techniques for editing large\nlanguage models, and present new methods which can do so without requiring\nretraining. Our theoretical insights show that a single metric (a measure of\nthe intrinsic dimension of the model's features) can be used to assess a\nmodel's editability and reveals its previously unrecognised susceptibility to\nmalicious stealth attacks. This metric is fundamental to predicting the success\nof a variety of editing approaches, and reveals new bridges between disparate\nfamilies of editing methods. We collectively refer to these as stealth editing\nmethods, because they directly update a model's weights to specify its response\nto specific known hallucinating prompts without affecting other model\nbehaviour. By carefully applying our theoretical insights, we are able to\nintroduce a new jet-pack network block which is optimised for highly selective\nmodel editing, uses only standard network operations, and can be inserted into\nexisting networks. We also reveal the vulnerability of language models to\nstealth attacks: a small change to a model's weights which fixes its response\nto a single attacker-chosen prompt. Stealth attacks are computationally simple,\ndo not require access to or knowledge of the model's training data, and\ntherefore represent a potent yet previously unrecognised threat to\nredistributed foundation models. Extensive experimental results illustrate and\nsupport our methods and their theoretical underpinnings. Demos and source code\nare available at https://github.com/qinghua-zhou/stealth-edits.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "68T07, 68T50, 68W40",
      "I.2.7; F.2.0"
    ],
    "primary_category": "cs.AI",
    "comment": "28 pages, 14 figures. Open source implementation:\n  https://github.com/qinghua-zhou/stealth-edits",
    "pdf_url": "http://arxiv.org/pdf/2406.12670v2",
    "published_date": "2024-06-18 14:43:18 UTC",
    "updated_date": "2024-10-30 10:12:24 UTC"
  },
  {
    "arxiv_id": "2406.12665v3",
    "title": "CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis",
    "authors": [
      "Saranya Venkatraman",
      "Nafis Irtiza Tripto",
      "Dongwon Lee"
    ],
    "abstract": "The rise of unifying frameworks that enable seamless interoperability of\nLarge Language Models (LLMs) has made LLM-LLM collaboration for open-ended\ntasks a possibility. Despite this, there have not been efforts to explore such\ncollaborative writing. We take the next step beyond human-LLM collaboration to\nexplore this multi-LLM scenario by generating the first exclusively\nLLM-generated collaborative stories dataset called CollabStory. We focus on\nsingle-author to multi-author (up to 5 LLMs) scenarios, where multiple LLMs\nco-author stories. We generate over 32k stories using open-source\ninstruction-tuned LLMs. Further, we take inspiration from the PAN tasks that\nhave set the standard for human-human multi-author writing tasks and analysis.\nWe extend their authorship-related tasks for multi-LLM settings and present\nbaselines for LLM-LLM collaboration. We find that current baselines are not\nable to handle this emerging scenario. Thus, CollabStory is a resource that\ncould help propel an understanding as well as the development of new techniques\nto discern the use of multiple LLMs. This is crucial to study in the context of\nwriting tasks since LLM-LLM collaboration could potentially overwhelm ongoing\nchallenges related to plagiarism detection, credit assignment, maintaining\nacademic integrity in educational settings, and addressing copyright\ninfringement concerns. We make our dataset and code available at\nhttps://github.com/saranya-venkatraman/CollabStory.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to NAACL Findings 2025",
    "pdf_url": "http://arxiv.org/pdf/2406.12665v3",
    "published_date": "2024-06-18 14:35:12 UTC",
    "updated_date": "2025-02-11 02:09:38 UTC"
  },
  {
    "arxiv_id": "2406.12663v1",
    "title": "Do More Details Always Introduce More Hallucinations in LVLM-based Image Captioning?",
    "authors": [
      "Mingqian Feng",
      "Yunlong Tang",
      "Zeliang Zhang",
      "Chenliang Xu"
    ],
    "abstract": "Large Vision-Language Models (LVLMs) excel in integrating visual and\nlinguistic contexts to produce detailed content, facilitating applications such\nas image captioning. However, using LVLMs to generate descriptions often faces\nthe challenge of object hallucination (OH), where the output text misrepresents\nactual objects in the input image. While previous studies attribute the\noccurrence of OH to the inclusion of more details, our study finds technical\nflaws in existing metrics, leading to unreliable evaluations of models and\nconclusions about OH. This has sparked a debate on the question: Do more\ndetails always introduce more hallucinations in LVLM-based image captioning?\n  In this paper, we address this debate by proposing a novel decoding strategy,\nDifferentiated Beam Decoding (DBD), along with a reliable new set of evaluation\nmetrics: CLIP-Precision, CLIP-Recall, and CLIP-F1. DBD decodes the wealth of\ninformation hidden in visual input into distinct language representations\ncalled unit facts in parallel. This decoding is achieved via a well-designed\ndifferential score that guides the parallel search and candidate screening. The\nselected unit facts are then aggregated to generate the final caption. Our\nproposed metrics evaluate the comprehensiveness and accuracy of image captions\nby comparing the embedding groups of ground-truth image regions and generated\ntext partitions. Extensive experiments on the Visual Genome dataset validate\nthe effectiveness of our approach, demonstrating that it produces detailed\ndescriptions while maintaining low hallucination levels.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12663v1",
    "published_date": "2024-06-18 14:33:56 UTC",
    "updated_date": "2024-06-18 14:33:56 UTC"
  },
  {
    "arxiv_id": "2406.12660v1",
    "title": "Investigating the Role of Explainability and AI Literacy in User Compliance",
    "authors": [
      "Niklas Kühl",
      "Christian Meske",
      "Maximilian Nitsche",
      "Jodie Lobana"
    ],
    "abstract": "AI is becoming increasingly common across different domains. However, as\nsophisticated AI-based systems are often black-boxed, rendering the\ndecision-making logic opaque, users find it challenging to comply with their\nrecommendations. Although researchers are investigating Explainable AI (XAI) to\nincrease the transparency of the underlying machine learning models, it is\nunclear what types of explanations are effective and what other factors\nincrease compliance. To better understand the interplay of these factors, we\nconducted an experiment with 562 participants who were presented with the\nrecommendations of an AI and two different types of XAI. We find that users'\ncompliance increases with the introduction of XAI but is also affected by AI\nliteracy. We also find that the relationships between AI literacy XAI and\nusers' compliance are mediated by the users' mental model of AI. Our study has\nseveral implications for successfully designing AI-based systems utilizing XAI.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12660v1",
    "published_date": "2024-06-18 14:28:12 UTC",
    "updated_date": "2024-06-18 14:28:12 UTC"
  },
  {
    "arxiv_id": "2407.02508v2",
    "title": "Sample-efficient Imitative Multi-token Decision Transformer for Real-world Driving",
    "authors": [
      "Hang Zhou",
      "Dan Xu",
      "Yiding Ji"
    ],
    "abstract": "Recent advancements in autonomous driving technologies involve the capability\nto effectively process and learn from extensive real-world driving data.\nCurrent imitation learning and offline reinforcement learning methods have\nshown remarkable promise in autonomous systems, harnessing the power of offline\ndatasets to make informed decisions in open-loop (non-reactive agents)\nsettings. However, learning-based agents face significant challenges when\ntransferring knowledge from open-loop to closed-loop (reactive agents)\nenvironment. The performance is significantly impacted by data distribution\nshift, sample efficiency, the complexity of uncovering hidden world models and\nphysics. To address these issues, we propose Sample-efficient Imitative\nMulti-token Decision Transformer (SimDT). SimDT introduces multi-token\nprediction, online imitative learning pipeline and prioritized experience\nreplay to sequence-modelling reinforcement learning. The performance is\nevaluated through empirical experiments and results exceed popular imitation\nand reinforcement learning algorithms both in open-loop and closed-loop\nsettings on Waymax benchmark. SimDT exhibits 41% reduction in collision rate\nand 18% improvement in reaching the destination compared with the baseline\nmethod.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.02508v2",
    "published_date": "2024-06-18 14:27:14 UTC",
    "updated_date": "2024-10-04 03:45:21 UTC"
  },
  {
    "arxiv_id": "2406.12655v1",
    "title": "Benchmarks and Metrics for Evaluations of Code Generation: A Critical Review",
    "authors": [
      "Debalina Ghosh Paul",
      "Hong Zhu",
      "Ian Bayley"
    ],
    "abstract": "With the rapid development of Large Language Models (LLMs), a large number of\nmachine learning models have been developed to assist programming tasks\nincluding the generation of program code from natural language input. However,\nhow to evaluate such LLMs for this task is still an open problem despite of the\ngreat amount of research efforts that have been made and reported to evaluate\nand compare them. This paper provides a critical review of the existing work on\nthe testing and evaluation of these tools with a focus on two key aspects: the\nbenchmarks and the metrics used in the evaluations. Based on the review,\nfurther research directions are discussed.",
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by the First IEEE International Workshop on Testing and\n  Evaluation of Large Language Models (TELLMe 2024) and will be published in\n  the proceedings of the IEEE AITest 2024 conference",
    "pdf_url": "http://arxiv.org/pdf/2406.12655v1",
    "published_date": "2024-06-18 14:25:34 UTC",
    "updated_date": "2024-06-18 14:25:34 UTC"
  },
  {
    "arxiv_id": "2407.16896v1",
    "title": "Free to play: UN Trade and Development's experience with developing its own open-source Retrieval Augmented Generation Large Language Model application",
    "authors": [
      "Daniel Hopp"
    ],
    "abstract": "Generative artificial intelligence (AI), and in particular Large Language\nModels (LLMs), have exploded in popularity and attention since the release to\nthe public of ChatGPT's Generative Pre-trained Transformer (GPT)-3.5 model in\nNovember of 2022. Due to the power of these general purpose models and their\nability to communicate in natural language, they can be useful in a range of\ndomains, including the work of official statistics and international\norganizations. However, with such a novel and seemingly complex technology, it\ncan feel as if generative AI is something that happens to an organization,\nsomething that can be talked about but not understood, that can be commented on\nbut not contributed to. Additionally, the costs of adoption and operation of\nproprietary solutions can be both uncertain and high, a barrier for often\ncost-constrained international organizations. In the face of these challenges,\nUnited Nations Trade and Development (UNCTAD), through its Global Crisis\nResponse Group (GCRG), has explored and developed its own open-source Retrieval\nAugmented Generation (RAG) LLM application. RAG makes LLMs aware of and more\nuseful for the organization's domain and work. Developing in-house solutions\ncomes with pros and cons, with pros including cost, flexibility, and fostering\ninstitutional knowledge. Cons include time and skill investments and gaps and\napplication polish and power. The three libraries developed to produce the app,\nnlp_pipeline for document processing and statistical analysis, local_rag_llm\nfor running a local RAG LLM, and streamlit_rag for the user interface, are\npublicly available on PyPI and GitHub with Dockerfiles. A fourth library,\nlocal_llm_finetune, is also available for fine-tuning existing LLMs which can\nthen be used in the application.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "I.2"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.16896v1",
    "published_date": "2024-06-18 14:23:54 UTC",
    "updated_date": "2024-06-18 14:23:54 UTC"
  },
  {
    "arxiv_id": "2406.12651v1",
    "title": "Transforming Surgical Interventions with Embodied Intelligence for Ultrasound Robotics",
    "authors": [
      "Huan Xu",
      "Jinlin Wu",
      "Guanglin Cao",
      "Zhen Chen",
      "Zhen Lei",
      "Hongbin Liu"
    ],
    "abstract": "Ultrasonography has revolutionized non-invasive diagnostic methodologies,\nsignificantly enhancing patient outcomes across various medical domains.\nDespite its advancements, integrating ultrasound technology with robotic\nsystems for automated scans presents challenges, including limited command\nunderstanding and dynamic execution capabilities. To address these challenges,\nthis paper introduces a novel Ultrasound Embodied Intelligence system that\nsynergistically combines ultrasound robots with large language models (LLMs)\nand domain-specific knowledge augmentation, enhancing ultrasound robots'\nintelligence and operational efficiency. Our approach employs a dual strategy:\nfirstly, integrating LLMs with ultrasound robots to interpret doctors' verbal\ninstructions into precise motion planning through a comprehensive understanding\nof ultrasound domain knowledge, including APIs and operational manuals;\nsecondly, incorporating a dynamic execution mechanism, allowing for real-time\nadjustments to scanning plans based on patient movements or procedural errors.\nWe demonstrate the effectiveness of our system through extensive experiments,\nincluding ablation studies and comparisons across various models, showcasing\nsignificant improvements in executing medical procedures from verbal commands.\nOur findings suggest that the proposed system improves the efficiency and\nquality of ultrasound scans and paves the way for further advancements in\nautonomous medical scanning technologies, with the potential to transform\nnon-invasive diagnostics and streamline medical workflows.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.RO",
    "comment": "This work has been accepted by MICCAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.12651v1",
    "published_date": "2024-06-18 14:22:16 UTC",
    "updated_date": "2024-06-18 14:22:16 UTC"
  },
  {
    "arxiv_id": "2406.12649v3",
    "title": "Probabilistic Conceptual Explainers: Trustworthy Conceptual Explanations for Vision Foundation Models",
    "authors": [
      "Hengyi Wang",
      "Shiwei Tan",
      "Hao Wang"
    ],
    "abstract": "Vision transformers (ViTs) have emerged as a significant area of focus,\nparticularly for their capacity to be jointly trained with large language\nmodels and to serve as robust vision foundation models. Yet, the development of\ntrustworthy explanation methods for ViTs has lagged, particularly in the\ncontext of post-hoc interpretations of ViT predictions. Existing sub-image\nselection approaches, such as feature-attribution and conceptual models, fall\nshort in this regard. This paper proposes five desiderata for explaining ViTs\n-- faithfulness, stability, sparsity, multi-level structure, and parsimony --\nand demonstrates the inadequacy of current methods in meeting these criteria\ncomprehensively. We introduce a variational Bayesian explanation framework,\ndubbed ProbAbilistic Concept Explainers (PACE), which models the distributions\nof patch embeddings to provide trustworthy post-hoc conceptual explanations.\nOur qualitative analysis reveals the distributions of patch-level concepts,\nelucidating the effectiveness of ViTs by modeling the joint distribution of\npatch embeddings and ViT's predictions. Moreover, these patch-level\nexplanations bridge the gap between image-level and dataset-level explanations,\nthus completing the multi-level structure of PACE. Through extensive\nexperiments on both synthetic and real-world datasets, we demonstrate that PACE\nsurpasses state-of-the-art methods in terms of the defined desiderata.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Proceedings of the 41st International Conference on Machine Learning\n  (ICML 2024)",
    "pdf_url": "http://arxiv.org/pdf/2406.12649v3",
    "published_date": "2024-06-18 14:17:57 UTC",
    "updated_date": "2024-10-31 19:30:46 UTC"
  },
  {
    "arxiv_id": "2406.12646v1",
    "title": "An Empirical Study on the Fairness of Foundation Models for Multi-Organ Image Segmentation",
    "authors": [
      "Qin Li",
      "Yizhe Zhang",
      "Yan Li",
      "Jun Lyu",
      "Meng Liu",
      "Longyu Sun",
      "Mengting Sun",
      "Qirong Li",
      "Wenyue Mao",
      "Xinran Wu",
      "Yajing Zhang",
      "Yinghua Chu",
      "Shuo Wang",
      "Chengyan Wang"
    ],
    "abstract": "The segmentation foundation model, e.g., Segment Anything Model (SAM), has\nattracted increasing interest in the medical image community. Early pioneering\nstudies primarily concentrated on assessing and improving SAM's performance\nfrom the perspectives of overall accuracy and efficiency, yet little attention\nwas given to the fairness considerations. This oversight raises questions about\nthe potential for performance biases that could mirror those found in\ntask-specific deep learning models like nnU-Net. In this paper, we explored the\nfairness dilemma concerning large segmentation foundation models. We\nprospectively curate a benchmark dataset of 3D MRI and CT scans of the organs\nincluding liver, kidney, spleen, lung and aorta from a total of 1056 healthy\nsubjects with expert segmentations. Crucially, we document demographic details\nsuch as gender, age, and body mass index (BMI) for each subject to facilitate a\nnuanced fairness analysis. We test state-of-the-art foundation models for\nmedical image segmentation, including the original SAM, medical SAM and SAT\nmodels, to evaluate segmentation efficacy across different demographic groups\nand identify disparities. Our comprehensive analysis, which accounts for\nvarious confounding factors, reveals significant fairness concerns within these\nfoundational models. Moreover, our findings highlight not only disparities in\noverall segmentation metrics, such as the Dice Similarity Coefficient but also\nsignificant variations in the spatial distribution of segmentation errors,\noffering empirical evidence of the nuanced challenges in ensuring fairness in\nmedical image segmentation.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "Accepted to MICCAI-2024",
    "pdf_url": "http://arxiv.org/pdf/2406.12646v1",
    "published_date": "2024-06-18 14:14:04 UTC",
    "updated_date": "2024-06-18 14:14:04 UTC"
  },
  {
    "arxiv_id": "2406.12645v3",
    "title": "Evaluating Evidence Attribution in Generated Fact Checking Explanations",
    "authors": [
      "Rui Xing",
      "Timothy Baldwin",
      "Jey Han Lau"
    ],
    "abstract": "Automated fact-checking systems often struggle with trustworthiness, as their\ngenerated explanations can include hallucinations. In this work, we explore\nevidence attribution for fact-checking explanation generation. We introduce a\nnovel evaluation protocol -- citation masking and recovery -- to assess\nattribution quality in generated explanations. We implement our protocol using\nboth human annotators and automatic annotators, and find that LLM annotation\ncorrelates with human annotation, suggesting that attribution assessment can be\nautomated. Finally, our experiments reveal that: (1) the best-performing LLMs\nstill generate explanations with inaccurate attributions; and (2) human-curated\nevidence is essential for generating better explanations. Code and data are\navailable here: https://github.com/ruixing76/Transparent-FCExp.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to NAACL 2025 Main",
    "pdf_url": "http://arxiv.org/pdf/2406.12645v3",
    "published_date": "2024-06-18 14:13:13 UTC",
    "updated_date": "2025-02-11 16:36:32 UTC"
  },
  {
    "arxiv_id": "2406.12644v4",
    "title": "Hierarchical Prompting Taxonomy: A Universal Evaluation Framework for Large Language Models Aligned with Human Cognitive Principles",
    "authors": [
      "Devichand Budagam",
      "Ashutosh Kumar",
      "Mahsa Khoshnoodi",
      "Sankalp KJ",
      "Vinija Jain",
      "Aman Chadha"
    ],
    "abstract": "Assessing the effectiveness of large language models (LLMs) in performing\ndifferent tasks is crucial for understanding their strengths and weaknesses.\nThis paper presents Hierarchical Prompting Taxonomy (HPT), grounded on human\ncognitive principles and designed to assess LLMs by examining the cognitive\ndemands of various tasks. The HPT utilizes the Hierarchical Prompting Framework\n(HPF), which structures five unique prompting strategies in a hierarchical\norder based on their cognitive requirement on LLMs when compared to human\nmental capabilities. It assesses the complexity of tasks with the Hierarchical\nPrompting Index (HPI), which demonstrates the cognitive competencies of LLMs\nacross diverse datasets and offers insights into the cognitive demands that\ndatasets place on different LLMs. This approach enables a comprehensive\nevaluation of an LLMs problem solving abilities and the intricacy of a dataset,\noffering a standardized metric for task complexity. Extensive experiments with\nmultiple datasets and LLMs show that HPF enhances LLM performance by 2% to 63%\ncompared to baseline performance, with GSM8k being the most cognitively complex\ntask among reasoning and coding tasks with an average HPI of 3.20 confirming\nthe effectiveness of HPT. To support future research and reproducibility in\nthis domain, the implementations of HPT and HPF are available here.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12644v4",
    "published_date": "2024-06-18 14:12:27 UTC",
    "updated_date": "2024-12-12 02:37:52 UTC"
  },
  {
    "arxiv_id": "2406.12639v2",
    "title": "Ask-before-Plan: Proactive Language Agents for Real-World Planning",
    "authors": [
      "Xuan Zhang",
      "Yang Deng",
      "Zifeng Ren",
      "See-Kiong Ng",
      "Tat-Seng Chua"
    ],
    "abstract": "The evolution of large language models (LLMs) has enhanced the planning\ncapabilities of language agents in diverse real-world scenarios. Despite these\nadvancements, the potential of LLM-powered agents to comprehend ambiguous user\ninstructions for reasoning and decision-making is still under exploration. In\nthis work, we introduce a new task, Proactive Agent Planning, which requires\nlanguage agents to predict clarification needs based on user-agent conversation\nand agent-environment interaction, invoke external tools to collect valid\ninformation, and generate a plan to fulfill the user's demands. To study this\npractical problem, we establish a new benchmark dataset, Ask-before-Plan. To\ntackle the deficiency of LLMs in proactive planning, we propose a novel\nmulti-agent framework, Clarification-Execution-Planning (\\texttt{CEP}), which\nconsists of three agents specialized in clarification, execution, and planning.\nWe introduce the trajectory tuning scheme for the clarification agent and\nstatic execution agent, as well as the memory recollection mechanism for the\ndynamic execution agent. Extensive evaluations and comprehensive analyses\nconducted on the Ask-before-Plan dataset validate the effectiveness of our\nproposed framework.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by EMNLP 2024 Findings",
    "pdf_url": "http://arxiv.org/pdf/2406.12639v2",
    "published_date": "2024-06-18 14:07:28 UTC",
    "updated_date": "2024-10-02 02:02:56 UTC"
  },
  {
    "arxiv_id": "2406.12635v1",
    "title": "ScenEval: A Benchmark for Scenario-Based Evaluation of Code Generation",
    "authors": [
      "Debalina Ghosh Paul",
      "Hong Zhu",
      "Ian Bayley"
    ],
    "abstract": "In the scenario-based evaluation of machine learning models, a key problem is\nhow to construct test datasets that represent various scenarios. The\nmethodology proposed in this paper is to construct a benchmark and attach\nmetadata to each test case. Then a test system can be constructed with test\nmorphisms that filter the test cases based on metadata to form a dataset.\n  The paper demonstrates this methodology with large language models for code\ngeneration. A benchmark called ScenEval is constructed from problems in\ntextbooks, an online tutorial website and Stack Overflow. Filtering by scenario\nis demonstrated and the test sets are used to evaluate ChatGPT for Java code\ngeneration.\n  Our experiments found that the performance of ChatGPT decreases with the\ncomplexity of the coding task. It is weakest for advanced topics like\nmulti-threading, data structure algorithms and recursive methods. The Java code\ngenerated by ChatGPT tends to be much shorter than reference solution in terms\nof number of lines, while it is more likely to be more complex in both\ncyclomatic and cognitive complexity metrics, if the generated code is correct.\nHowever, the generated code is more likely to be less complex than the\nreference solution if the code is incorrect.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted for publication in the conference proceedings of IEEE AITest\n  2024",
    "pdf_url": "http://arxiv.org/pdf/2406.12635v1",
    "published_date": "2024-06-18 14:02:20 UTC",
    "updated_date": "2024-06-18 14:02:20 UTC"
  },
  {
    "arxiv_id": "2406.12634v2",
    "title": "News Without Borders: Domain Adaptation of Multilingual Sentence Embeddings for Cross-lingual News Recommendation",
    "authors": [
      "Andreea Iana",
      "Fabian David Schmidt",
      "Goran Glavaš",
      "Heiko Paulheim"
    ],
    "abstract": "Rapidly growing numbers of multilingual news consumers pose an increasing\nchallenge to news recommender systems in terms of providing customized\nrecommendations. First, existing neural news recommenders, even when powered by\nmultilingual language models (LMs), suffer substantial performance losses in\nzero-shot cross-lingual transfer (ZS-XLT). Second, the current paradigm of\nfine-tuning the backbone LM of a neural recommender on task-specific data is\ncomputationally expensive and infeasible in few-shot recommendation and\ncold-start setups, where data is scarce or completely unavailable. In this\nwork, we propose a news-adapted sentence encoder (NaSE), domain-specialized\nfrom a pretrained massively multilingual sentence encoder (SE). To this end, we\nconstruct and leverage PolyNews and PolyNewsParallel, two multilingual\nnews-specific corpora. With the news-adapted multilingual SE in place, we test\nthe effectiveness of (i.e., question the need for) supervised fine-tuning for\nnews recommendation, and propose a simple and strong baseline based on (i)\nfrozen NaSE embeddings and (ii) late click-behavior fusion. We show that NaSE\nachieves state-of-the-art performance in ZS-XLT in true cold-start and few-shot\nnews recommendation.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "I.2.7; H.3.3"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted at the 47th European Conference on Information Retrieval\n  (ECIR 2025) Appendix A is provided only in the arXiv version",
    "pdf_url": "http://arxiv.org/pdf/2406.12634v2",
    "published_date": "2024-06-18 14:01:53 UTC",
    "updated_date": "2025-01-17 12:19:05 UTC"
  },
  {
    "arxiv_id": "2407.01585v1",
    "title": "DrugWatch: A Comprehensive Multi-Source Data Visualisation Platform for Drug Safety Information",
    "authors": [
      "Artem Bobrov",
      "Domantas Saltenis",
      "Zhaoyue Sun",
      "Gabriele Pergola",
      "Yulan He"
    ],
    "abstract": "Drug safety research is crucial for maintaining public health, often\nrequiring comprehensive data support. However, the resources currently\navailable to the public are limited and fail to provide a comprehensive\nunderstanding of the relationship between drugs and their side effects. This\npaper introduces DrugWatch, an easy-to-use and interactive multi-source\ninformation visualisation platform for drug safety study. It allows users to\nunderstand common side effects of drugs and their statistical information,\nflexibly retrieve relevant medical reports, or annotate their own medical texts\nwith our automated annotation tool. Supported by NLP technology and enriched\nwith interactive visual components, we are committed to providing researchers\nand practitioners with a one-stop information analysis, retrieval, and\nannotation service. The demonstration video is available at\nhttps://www.youtube.com/watch?v=RTqDgxzETjw. We also deployed an online\ndemonstration system at https://drugwatch.net/.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.HC",
    "comment": "10 pages, 14 figures, accepted by ACL 2024 Demo Track",
    "pdf_url": "http://arxiv.org/pdf/2407.01585v1",
    "published_date": "2024-06-18 13:58:12 UTC",
    "updated_date": "2024-06-18 13:58:12 UTC"
  },
  {
    "arxiv_id": "2406.12629v4",
    "title": "SeTAR: Out-of-Distribution Detection with Selective Low-Rank Approximation",
    "authors": [
      "Yixia Li",
      "Boya Xiong",
      "Guanhua Chen",
      "Yun Chen"
    ],
    "abstract": "Out-of-distribution (OOD) detection is crucial for the safe deployment of\nneural networks. Existing CLIP-based approaches perform OOD detection by\ndevising novel scoring functions or sophisticated fine-tuning methods. In this\nwork, we propose SeTAR, a novel, training-free OOD detection method that\nleverages selective low-rank approximation of weight matrices in\nvision-language and vision-only models. SeTAR enhances OOD detection via\npost-hoc modification of the model's weight matrices using a simple greedy\nsearch algorithm. Based on SeTAR, we further propose SeTAR+FT, a fine-tuning\nextension optimizing model performance for OOD detection tasks. Extensive\nevaluations on ImageNet1K and Pascal-VOC benchmarks show SeTAR's superior\nperformance, reducing the relatively false positive rate by up to 18.95% and\n36.80% compared to zero-shot and fine-tuning baselines. Ablation studies\nfurther validate SeTAR's effectiveness, robustness, and generalizability across\ndifferent model backbones. Our work offers a scalable, efficient solution for\nOOD detection, setting a new state-of-the-art in this area.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by NeurIPS 2024. Project page is live at\n  https://SeTAR-OOD.github.io. Code are available at\n  https://github.com/X1AOX1A/SeTAR",
    "pdf_url": "http://arxiv.org/pdf/2406.12629v4",
    "published_date": "2024-06-18 13:55:13 UTC",
    "updated_date": "2024-11-05 06:34:40 UTC"
  },
  {
    "arxiv_id": "2406.12624v5",
    "title": "Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges",
    "authors": [
      "Aman Singh Thakur",
      "Kartik Choudhary",
      "Venkat Srinik Ramayapally",
      "Sankaran Vaidyanathan",
      "Dieuwke Hupkes"
    ],
    "abstract": "Offering a promising solution to the scalability challenges associated with\nhuman evaluation, the LLM-as-a-judge paradigm is rapidly gaining traction as an\napproach to evaluating large language models (LLMs). However, there are still\nmany open questions about the strengths and weaknesses of this paradigm, and\nwhat potential biases it may hold. In this paper, we present a comprehensive\nstudy of the performance of various LLMs acting as judges, focusing on a clean\nscenario in which inter-human agreement is high. Investigating thirteen judge\nmodels of different model sizes and families, judging answers of nine different\n'examtaker models' - both base and instruction-tuned - we find that only the\nbest (and largest) models achieve reasonable alignment with humans. However,\nthey are still quite far behind inter-human agreement and their assigned scores\nmay still differ with up to 5 points from human-assigned scores. In terms of\ntheir ranking of the nine exam-taker models, instead, also smaller models and\neven the lexical metric contains may provide a reasonable signal. Through error\nanalysis and other studies, we identify vulnerabilities in judge models, such\nas their sensitivity to prompt complexity and length, and a tendency toward\nleniency. The fact that even the best judges differ from humans in this\ncomparatively simple setup suggest that caution may be wise when using judges\nin more complex setups. Lastly, our research rediscovers the importance of\nusing alignment metrics beyond simple percent alignment, showing that judges\nwith high percent agreement can still assign vastly different scores.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12624v5",
    "published_date": "2024-06-18 13:49:54 UTC",
    "updated_date": "2025-01-21 04:10:13 UTC"
  },
  {
    "arxiv_id": "2406.12608v2",
    "title": "Bridging Local Details and Global Context in Text-Attributed Graphs",
    "authors": [
      "Yaoke Wang",
      "Yun Zhu",
      "Wenqiao Zhang",
      "Yueting Zhuang",
      "Yunfei Li",
      "Siliang Tang"
    ],
    "abstract": "Representation learning on text-attributed graphs (TAGs) is vital for\nreal-world applications, as they combine semantic textual and contextual\nstructural information. Research in this field generally consist of two main\nperspectives: local-level encoding and global-level aggregating, respectively\nrefer to textual node information unification (e.g., using Language Models) and\nstructure-augmented modeling (e.g., using Graph Neural Networks). Most existing\nworks focus on combining different information levels but overlook the\ninterconnections, i.e., the contextual textual information among nodes, which\nprovides semantic insights to bridge local and global levels. In this paper, we\npropose GraphBridge, a multi-granularity integration framework that bridges\nlocal and global perspectives by leveraging contextual textual information,\nenhancing fine-grained understanding of TAGs. Besides, to tackle scalability\nand efficiency challenges, we introduce a graphaware token reduction module.\nExtensive experiments across various models and datasets show that our method\nachieves state-of-theart performance, while our graph-aware token reduction\nmodule significantly enhances efficiency and solves scalability issues.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by EMNLP 2024(Main)",
    "pdf_url": "http://arxiv.org/pdf/2406.12608v2",
    "published_date": "2024-06-18 13:35:25 UTC",
    "updated_date": "2024-10-14 06:44:14 UTC"
  },
  {
    "arxiv_id": "2406.12593v2",
    "title": "PromptDSI: Prompt-based Rehearsal-free Instance-wise Incremental Learning for Document Retrieval",
    "authors": [
      "Tuan-Luc Huynh",
      "Thuy-Trang Vu",
      "Weiqing Wang",
      "Yinwei Wei",
      "Trung Le",
      "Dragan Gasevic",
      "Yuan-Fang Li",
      "Thanh-Toan Do"
    ],
    "abstract": "Differentiable Search Index (DSI) utilizes Pre-trained Language Models (PLMs)\nfor efficient document retrieval without relying on external indexes. However,\nDSI needs full re-training to handle updates in dynamic corpora, causing\nsignificant computational inefficiencies. We introduce PromptDSI, a\nprompt-based rehearsal-free approach for instance-wise incremental learning\ndocument retrieval. PromptDSI attaches prompts to the frozen PLM's encoder of\nDSI, leveraging its powerful representation to efficiently index new corpora\nwhile maintaining a balance between stability and plasticity. We eliminate the\ninitial forward pass of prompt-based continual learning methods that doubles\ntraining and inference time. Moreover, we propose a topic-aware prompt pool\nthat employs neural topic embeddings as fixed keys. This strategy ensures\ndiverse and effective prompt usage, addressing the challenge of parameter\nunderutilization caused by the collapse of the query-key matching mechanism.\nOur empirical evaluations demonstrate that BERT-based PromptDSI matches IncDSI\nin managing forgetting while improving new corpora performance by more than 4%\nHits@10 on NQ320k and upto 3% MRR@10 on MS MARCO 300k.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "20 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.12593v2",
    "published_date": "2024-06-18 13:25:18 UTC",
    "updated_date": "2024-10-16 13:45:54 UTC"
  },
  {
    "arxiv_id": "2406.12588v2",
    "title": "UIFV: Data Reconstruction Attack in Vertical Federated Learning",
    "authors": [
      "Jirui Yang",
      "Peng Chen",
      "Zhihui Lu",
      "Qiang Duan",
      "Yubing Bao"
    ],
    "abstract": "Vertical Federated Learning (VFL) facilitates collaborative machine learning\nwithout the need for participants to share raw private data. However, recent\nstudies have revealed privacy risks where adversaries might reconstruct\nsensitive features through data leakage during the learning process. Although\ndata reconstruction methods based on gradient or model information are somewhat\neffective, they reveal limitations in VFL application scenarios. This is\nbecause these traditional methods heavily rely on specific model structures\nand/or have strict limitations on application scenarios. To address this, our\nstudy introduces the Unified InverNet Framework into VFL, which yields a novel\nand flexible approach (dubbed UIFV) that leverages intermediate feature data to\nreconstruct original data, instead of relying on gradients or model details.\nThe intermediate feature data is the feature exchanged by different\nparticipants during the inference phase of VFL. Experiments on four datasets\ndemonstrate that our methods significantly outperform state-of-the-art\ntechniques in attack precision. Our work exposes severe privacy vulnerabilities\nwithin VFL systems that pose real threats to practical VFL applications and\nthus confirms the necessity of further enhancing privacy protection in the VFL\narchitecture.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12588v2",
    "published_date": "2024-06-18 13:18:52 UTC",
    "updated_date": "2025-01-14 21:17:58 UTC"
  },
  {
    "arxiv_id": "2406.12585v2",
    "title": "Breaking the Ceiling of the LLM Community by Treating Token Generation as a Classification for Ensembling",
    "authors": [
      "Yao-Ching Yu",
      "Chun-Chih Kuo",
      "Ziqi Ye",
      "Yu-Cheng Chang",
      "Yueh-Se Li"
    ],
    "abstract": "Ensembling multiple models has always been an effective approach to push the\nlimits of existing performance and is widely used in classification tasks by\nsimply averaging the classification probability vectors from multiple\nclassifiers to achieve better accuracy. However, in the thriving open-source\nLarge Language Model (LLM) community, ensembling methods are rare and typically\nlimited to ensembling the full-text outputs of LLMs, such as selecting the best\noutput using a ranker, which leads to underutilization of token-level\nprobability information. In this paper, we treat the Generation of each token\nby LLMs as a Classification (GaC) for ensembling. This approach fully exploits\nthe probability information at each generation step and better prevents LLMs\nfrom producing early incorrect tokens that lead to snowballing errors. In\nexperiments, we ensemble state-of-the-art LLMs on several benchmarks, including\nexams, mathematics and reasoning, and observe that our method breaks the\nexisting community performance ceiling. Furthermore, we observed that most of\nthe tokens in the answer are simple and do not affect the correctness of the\nfinal answer. Therefore, we also experimented with ensembling only key tokens,\nand the results showed better performance with lower latency across benchmarks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.12585v2",
    "published_date": "2024-06-18 13:17:26 UTC",
    "updated_date": "2024-09-29 11:18:58 UTC"
  },
  {
    "arxiv_id": "2406.12572v3",
    "title": "Mathador-LM: A Dynamic Benchmark for Mathematical Reasoning on Large Language Models",
    "authors": [
      "Eldar Kurtic",
      "Amir Moeini",
      "Dan Alistarh"
    ],
    "abstract": "We introduce Mathador-LM, a new benchmark for evaluating the mathematical\nreasoning on large language models (LLMs), combining ruleset interpretation,\nplanning, and problem-solving. This benchmark is inspired by the Mathador game,\nwhere the objective is to reach a target number using basic arithmetic\noperations on a given set of base numbers, following a simple set of rules. We\nshow that, across leading LLMs, we obtain stable average performance while\ngenerating benchmark instances \\emph{dynamically}, following a target\ndifficulty level. Thus, our benchmark alleviates concerns about test-set\nleakage into training data, an issue that often undermines popular benchmarks.\nAdditionally, we conduct a comprehensive evaluation of both open and\nclosed-source state-of-the-art LLMs on Mathador-LM. Our findings reveal that\ncontemporary models struggle with Mathador-LM, scoring significantly lower than\naverage 3rd graders. This stands in stark contrast to their strong performance\non popular mathematical reasoning benchmarks. The implementation of Mathador-LM\nbenchmark is available at\n\\href{https://github.com/IST-DASLab/Mathador-LM}{github.com/IST-DASLab/Mathador-LM}.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.12572v3",
    "published_date": "2024-06-18 13:02:12 UTC",
    "updated_date": "2024-10-15 10:35:17 UTC"
  },
  {
    "arxiv_id": "2406.15490v2",
    "title": "Causal Discovery Inspired Unsupervised Domain Adaptation for Emotion-Cause Pair Extraction",
    "authors": [
      "Yuncheng Hua",
      "Yujin Huang",
      "Shuo Huang",
      "Tao Feng",
      "Lizhen Qu",
      "Chris Bain",
      "Richard Bassed",
      "Gholamreza Haffari"
    ],
    "abstract": "This paper tackles the task of emotion-cause pair extraction in the\nunsupervised domain adaptation setting. The problem is challenging as the\ndistributions of the events causing emotions in target domains are dramatically\ndifferent than those in source domains, despite the distributions of emotional\nexpressions between domains are overlapped. Inspired by causal discovery, we\npropose a novel deep latent model in the variational autoencoder (VAE)\nframework, which not only captures the underlying latent structures of data but\nalso utilizes the easily transferable knowledge of emotions as the bridge to\nlink the distributions of events in different domains. To facilitate knowledge\ntransfer across domains, we also propose a novel variational posterior\nregularization technique to disentangle the latent representations of emotions\nfrom those of events in order to mitigate the damage caused by the spurious\ncorrelations related to the events in source domains. Through extensive\nexperiments, we demonstrate that our model outperforms the strongest baseline\nby approximately 11.05\\% on a Chinese benchmark and 2.45\\% on a English\nbenchmark in terms of weighted-average F1 score. We have released our source\ncode and the generated dataset publicly at:\nhttps://github.com/tk1363704/CAREL-VAE.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "I.2.4"
    ],
    "primary_category": "cs.CL",
    "comment": "18 pages, 6 figures, 5 tables. The paper has been published in the\n  Findings of the Association for Computational Linguistics: EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.15490v2",
    "published_date": "2024-06-18 13:01:30 UTC",
    "updated_date": "2025-02-17 08:36:54 UTC"
  },
  {
    "arxiv_id": "2406.12950v2",
    "title": "MolecularGPT: Open Large Language Model (LLM) for Few-Shot Molecular Property Prediction",
    "authors": [
      "Yuyan Liu",
      "Sirui Ding",
      "Sheng Zhou",
      "Wenqi Fan",
      "Qiaoyu Tan"
    ],
    "abstract": "Molecular property prediction (MPP) is a fundamental and crucial task in drug\ndiscovery. However, prior methods are limited by the requirement for a large\nnumber of labeled molecules and their restricted ability to generalize for\nunseen and new tasks, both of which are essential for real-world applications.\nTo address these challenges, we present MolecularGPT for few-shot MPP. From a\nperspective on instruction tuning, we fine-tune large language models (LLMs)\nbased on curated molecular instructions spanning over 1000 property prediction\ntasks. This enables building a versatile and specialized LLM that can be\nadapted to novel MPP tasks without any fine-tuning through zero- and few-shot\nin-context learning (ICL). MolecularGPT exhibits competitive in-context\nreasoning capabilities across 10 downstream evaluation datasets, setting new\nbenchmarks for few-shot molecular prediction tasks. More importantly, with just\ntwo-shot examples, MolecularGPT can outperform standard supervised graph neural\nnetwork methods on 4 out of 7 datasets. It also excels state-of-the-art LLM\nbaselines by up to 15.7% increase on classification accuracy and decrease of\n17.9 on regression metrics (e.g., RMSE) under zero-shot. This study\ndemonstrates the potential of LLMs as effective few-shot molecular property\npredictors. The code is available at https://github.com/NYUSHCS/MolecularGPT.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.CE",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "q-bio.QM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12950v2",
    "published_date": "2024-06-18 12:54:47 UTC",
    "updated_date": "2024-10-18 12:19:41 UTC"
  },
  {
    "arxiv_id": "2406.12560v2",
    "title": "Towards Bayesian Data Selection",
    "authors": [
      "Julian Rodemann"
    ],
    "abstract": "A wide range of machine learning algorithms iteratively add data to the\ntraining sample. Examples include semi-supervised learning, active learning,\nmulti-armed bandits, and Bayesian optimization. We embed this kind of data\naddition into decision theory by framing data selection as a decision problem.\nThis paves the way for finding Bayes-optimal selections of data. For the\nillustrative case of self-training in semi-supervised learning, we derive the\nrespective Bayes criterion. We further show that deploying this criterion\nmitigates the issue of confirmation bias by empirically assessing our method\nfor generalized linear models, semi-parametric generalized additive models, and\nBayesian neural networks on simulated and real-world data.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "math.ST",
      "stat.TH"
    ],
    "primary_category": "stat.ML",
    "comment": "5th Workshop on Data-Centric Machine Learning Research (DMLR) at ICML\n  2024",
    "pdf_url": "http://arxiv.org/pdf/2406.12560v2",
    "published_date": "2024-06-18 12:40:15 UTC",
    "updated_date": "2024-06-24 08:27:13 UTC"
  },
  {
    "arxiv_id": "2406.12550v1",
    "title": "Offline Imitation Learning with Model-based Reverse Augmentation",
    "authors": [
      "Jie-Jing Shao",
      "Hao-Sen Shi",
      "Lan-Zhe Guo",
      "Yu-Feng Li"
    ],
    "abstract": "In offline Imitation Learning (IL), one of the main challenges is the\n\\textit{covariate shift} between the expert observations and the actual\ndistribution encountered by the agent, because it is difficult to determine\nwhat action an agent should take when outside the state distribution of the\nexpert demonstrations. Recently, the model-free solutions introduce the\nsupplementary data and identify the latent expert-similar samples to augment\nthe reliable samples during learning. Model-based solutions build forward\ndynamic models with conservatism quantification and then generate additional\ntrajectories in the neighborhood of expert demonstrations. However, without\nreward supervision, these methods are often over-conservative in the\nout-of-expert-support regions, because only in states close to expert-observed\nstates can there be a preferred action enabling policy optimization. To\nencourage more exploration on expert-unobserved states, we propose a novel\nmodel-based framework, called offline Imitation Learning with Self-paced\nReverse Augmentation (SRA). Specifically, we build a reverse dynamic model from\nthe offline demonstrations, which can efficiently generate trajectories leading\nto the expert-observed states in a self-paced style. Then, we use the\nsubsequent reinforcement learning method to learn from the augmented\ntrajectories and transit from expert-unobserved states to expert-observed\nstates. This framework not only explores the expert-unobserved states but also\nguides maximizing long-term returns on these states, ultimately enabling\ngeneralization beyond the expert data. Empirical results show that our proposal\ncould effectively mitigate the covariate shift and achieve the state-of-the-art\nperformance on the offline imitation learning benchmarks. Project website:\n\\url{https://www.lamda.nju.edu.cn/shaojj/KDD24_SRA/}.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by KDD2024",
    "pdf_url": "http://arxiv.org/pdf/2406.12550v1",
    "published_date": "2024-06-18 12:27:02 UTC",
    "updated_date": "2024-06-18 12:27:02 UTC"
  },
  {
    "arxiv_id": "2406.12549v1",
    "title": "MultiSocial: Multilingual Benchmark of Machine-Generated Text Detection of Social-Media Texts",
    "authors": [
      "Dominik Macko",
      "Jakub Kopal",
      "Robert Moro",
      "Ivan Srba"
    ],
    "abstract": "Recent LLMs are able to generate high-quality multilingual texts,\nindistinguishable for humans from authentic human-written ones. Research in\nmachine-generated text detection is however mostly focused on the English\nlanguage and longer texts, such as news articles, scientific papers or student\nessays. Social-media texts are usually much shorter and often feature informal\nlanguage, grammatical errors, or distinct linguistic items (e.g., emoticons,\nhashtags). There is a gap in studying the ability of existing methods in\ndetection of such texts, reflected also in the lack of existing multilingual\nbenchmark datasets. To fill this gap we propose the first multilingual (22\nlanguages) and multi-platform (5 social media platforms) dataset for\nbenchmarking machine-generated text detection in the social-media domain,\ncalled MultiSocial. It contains 472,097 texts, of which about 58k are\nhuman-written and approximately the same amount is generated by each of 7\nmultilingual LLMs. We use this benchmark to compare existing detection methods\nin zero-shot as well as fine-tuned form. Our results indicate that the\nfine-tuned detectors have no problem to be trained on social-media texts and\nthat the platform selection for training matters.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12549v1",
    "published_date": "2024-06-18 12:26:09 UTC",
    "updated_date": "2024-06-18 12:26:09 UTC"
  },
  {
    "arxiv_id": "2406.12539v1",
    "title": "The Heterophilic Snowflake Hypothesis: Training and Empowering GNNs for Heterophilic Graphs",
    "authors": [
      "Kun Wang",
      "Guibin Zhang",
      "Xinnan Zhang",
      "Junfeng Fang",
      "Xun Wu",
      "Guohao Li",
      "Shirui Pan",
      "Wei Huang",
      "Yuxuan Liang"
    ],
    "abstract": "Graph Neural Networks (GNNs) have become pivotal tools for a range of\ngraph-based learning tasks. Notably, most current GNN architectures operate\nunder the assumption of homophily, whether explicitly or implicitly. While this\nunderlying assumption is frequently adopted, it is not universally applicable,\nwhich can result in potential shortcomings in learning effectiveness. In this\npaper, \\textbf{for the first time}, we transfer the prevailing concept of ``one\nnode one receptive field\" to the heterophilic graph. By constructing a proxy\nlabel predictor, we enable each node to possess a latent prediction\ndistribution, which assists connected nodes in determining whether they should\naggregate their associated neighbors. Ultimately, every node can have its own\nunique aggregation hop and pattern, much like each snowflake is unique and\npossesses its own characteristics. Based on observations, we innovatively\nintroduce the Heterophily Snowflake Hypothesis and provide an effective\nsolution to guide and facilitate research on heterophilic graphs and beyond. We\nconduct comprehensive experiments including (1) main results on 10 graphs with\nvarying heterophily ratios across 10 backbones; (2) scalability on various deep\nGNN backbones (SGC, JKNet, etc.) across various large number of layers\n(2,4,6,8,16,32 layers); (3) comparison with conventional snowflake hypothesis;\n(4) efficiency comparison with existing graph pruning algorithms. Our\nobservations show that our framework acts as a versatile operator for diverse\ntasks. It can be integrated into various GNN frameworks, boosting performance\nin-depth and offering an explainable approach to choosing the optimal network\ndepth. The source code is available at\n\\url{https://github.com/bingreeky/HeteroSnoH}.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "KDD 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.12539v1",
    "published_date": "2024-06-18 12:16:00 UTC",
    "updated_date": "2024-06-18 12:16:00 UTC"
  },
  {
    "arxiv_id": "2406.12538v2",
    "title": "Variational Distillation of Diffusion Policies into Mixture of Experts",
    "authors": [
      "Hongyi Zhou",
      "Denis Blessing",
      "Ge Li",
      "Onur Celik",
      "Xiaogang Jia",
      "Gerhard Neumann",
      "Rudolf Lioutikov"
    ],
    "abstract": "This work introduces Variational Diffusion Distillation (VDD), a novel method\nthat distills denoising diffusion policies into Mixtures of Experts (MoE)\nthrough variational inference. Diffusion Models are the current\nstate-of-the-art in generative modeling due to their exceptional ability to\naccurately learn and represent complex, multi-modal distributions. This ability\nallows Diffusion Models to replicate the inherent diversity in human behavior,\nmaking them the preferred models in behavior learning such as Learning from\nHuman Demonstrations (LfD). However, diffusion models come with some drawbacks,\nincluding the intractability of likelihoods and long inference times due to\ntheir iterative sampling process. The inference times, in particular, pose a\nsignificant challenge to real-time applications such as robot control. In\ncontrast, MoEs effectively address the aforementioned issues while retaining\nthe ability to represent complex distributions but are notoriously difficult to\ntrain. VDD is the first method that distills pre-trained diffusion models into\nMoE models, and hence, combines the expressiveness of Diffusion Models with the\nbenefits of Mixture Models. Specifically, VDD leverages a decompositional upper\nbound of the variational objective that allows the training of each expert\nseparately, resulting in a robust optimization scheme for MoEs. VDD\ndemonstrates across nine complex behavior learning tasks, that it is able to:\ni) accurately distill complex distributions learned by the diffusion model, ii)\noutperform existing state-of-the-art distillation methods, and iii) surpass\nconventional methods for training MoE.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by the 38th Annual Conference on Neural Information\n  Processing Systems,",
    "pdf_url": "http://arxiv.org/pdf/2406.12538v2",
    "published_date": "2024-06-18 12:15:05 UTC",
    "updated_date": "2024-10-18 20:28:06 UTC"
  },
  {
    "arxiv_id": "2406.16943v1",
    "title": "EarDA: Towards Accurate and Data-Efficient Earable Activity Sensing",
    "authors": [
      "Shengzhe Lyu",
      "Yongliang Chen",
      "Di Duan",
      "Renqi Jia",
      "Weitao Xu"
    ],
    "abstract": "In the realm of smart sensing with the Internet of Things, earable devices\nare empowered with the capability of multi-modality sensing and intelligence of\ncontext-aware computing, leading to its wide usage in Human Activity\nRecognition (HAR). Nonetheless, unlike the movements captured by Inertial\nMeasurement Unit (IMU) sensors placed on the upper or lower body, those motion\nsignals obtained from earable devices show significant changes in amplitudes\nand patterns, especially in the presence of dynamic and unpredictable head\nmovements, posing a significant challenge for activity classification. In this\nwork, we present EarDA, an adversarial-based domain adaptation system to\nextract the domain-independent features across different sensor locations.\nMoreover, while most deep learning methods commonly rely on training with\nsubstantial amounts of labeled data to offer good accuracy, the proposed scheme\ncan release the potential usage of publicly available smartphone-based IMU\ndatasets. Furthermore, we explore the feasibility of applying a filter-based\ndata processing method to mitigate the impact of head movement. EarDA, the\nproposed system, enables more data-efficient and accurate activity sensing. It\nachieves an accuracy of 88.8% under HAR task, demonstrating a significant 43%\nimprovement over methods without domain adaptation. This clearly showcases its\neffectiveness in mitigating domain gaps.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "accepted by 2024 IEEE Coupling of Sensing & Computing in AIoT Systems\n  (CSCAIoT)",
    "pdf_url": "http://arxiv.org/pdf/2406.16943v1",
    "published_date": "2024-06-18 12:13:43 UTC",
    "updated_date": "2024-06-18 12:13:43 UTC"
  },
  {
    "arxiv_id": "2406.12529v1",
    "title": "LLM4MSR: An LLM-Enhanced Paradigm for Multi-Scenario Recommendation",
    "authors": [
      "Yuhao Wang",
      "Yichao Wang",
      "Zichuan Fu",
      "Xiangyang Li",
      "Xiangyu Zhao",
      "Huifeng Guo",
      "Ruiming Tang"
    ],
    "abstract": "As the demand for more personalized recommendation grows and a dramatic boom\nin commercial scenarios arises, the study on multi-scenario recommendation\n(MSR) has attracted much attention, which uses the data from all scenarios to\nsimultaneously improve their recommendation performance. However, existing\nmethods tend to integrate insufficient scenario knowledge and neglect learning\npersonalized cross-scenario preferences, thus leading to suboptimal performance\nand inadequate interpretability. Meanwhile, though large language model (LLM)\nhas shown great capability of reasoning and capturing semantic information, the\nhigh inference latency and high computation cost of tuning hinder its\nimplementation in industrial recommender systems. To fill these gaps, we\npropose an effective efficient interpretable LLM-enhanced paradigm LLM4MSR in\nthis work. Specifically, we first leverage LLM to uncover multi-level knowledge\nincluding scenario correlations and users' cross-scenario interests from the\ndesigned scenario- and user-level prompt without fine-tuning the LLM, then\nadopt hierarchical meta networks to generate multi-level meta layers to\nexplicitly improves the scenario-aware and personalized recommendation\ncapability. Our experiments on KuaiSAR-small, KuaiSAR, and Amazon datasets\nvalidate two significant advantages of LLM4MSR: (i) the effectiveness and\ncompatibility with different multi-scenario backbone models (achieving 1.5%,\n1%, and 40% AUC improvement on three datasets), (ii) high efficiency and\ndeployability on industrial recommender systems, and (iii) improved\ninterpretability. The implemented code and data is available to ease\nreproduction.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12529v1",
    "published_date": "2024-06-18 11:59:36 UTC",
    "updated_date": "2024-06-18 11:59:36 UTC"
  },
  {
    "arxiv_id": "2406.12499v1",
    "title": "Autonomous navigation of catheters and guidewires in mechanical thrombectomy using inverse reinforcement learning",
    "authors": [
      "Harry Robertshaw",
      "Lennart Karstensen",
      "Benjamin Jackson",
      "Alejandro Granados",
      "Thomas C. Booth"
    ],
    "abstract": "Purpose: Autonomous navigation of catheters and guidewires can enhance\nendovascular surgery safety and efficacy, reducing procedure times and operator\nradiation exposure. Integrating tele-operated robotics could widen access to\ntime-sensitive emergency procedures like mechanical thrombectomy (MT).\nReinforcement learning (RL) shows potential in endovascular navigation, yet its\napplication encounters challenges without a reward signal. This study explores\nthe viability of autonomous navigation in MT vasculature using inverse RL (IRL)\nto leverage expert demonstrations. Methods: This study established a\nsimulation-based training and evaluation environment for MT navigation. We used\nIRL to infer reward functions from expert behaviour when navigating a guidewire\nand catheter. We utilized soft actor-critic to train models with various reward\nfunctions and compared their performance in silico. Results: We demonstrated\nfeasibility of navigation using IRL. When evaluating single versus dual device\n(i.e. guidewire versus catheter and guidewire) tracking, both methods achieved\nhigh success rates of 95% and 96%, respectively. Dual-tracking, however,\nutilized both devices mimicking an expert. A success rate of 100% and procedure\ntime of 22.6 s were obtained when training with a reward function obtained\nthrough reward shaping. This outperformed a dense reward function (96%, 24.9 s)\nand an IRL-derived reward function (48%, 59.2 s). Conclusions: We have\ncontributed to the advancement of autonomous endovascular intervention\nnavigation, particularly MT, by employing IRL. The results underscore the\npotential of using reward shaping to train models, offering a promising avenue\nfor enhancing the accessibility and precision of MT. We envisage that future\nresearch can extend our methodology to diverse anatomical structures to enhance\ngeneralizability.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "Abstract shortened for arXiv character limit",
    "pdf_url": "http://arxiv.org/pdf/2406.12499v1",
    "published_date": "2024-06-18 11:00:55 UTC",
    "updated_date": "2024-06-18 11:00:55 UTC"
  },
  {
    "arxiv_id": "2406.12479v1",
    "title": "RS-GPT4V: A Unified Multimodal Instruction-Following Dataset for Remote Sensing Image Understanding",
    "authors": [
      "Linrui Xu",
      "Ling Zhao",
      "Wang Guo",
      "Qiujun Li",
      "Kewang Long",
      "Kaiqi Zou",
      "Yuhan Wang",
      "Haifeng Li"
    ],
    "abstract": "The remote sensing image intelligence understanding model is undergoing a new\nprofound paradigm shift which has been promoted by multi-modal large language\nmodel (MLLM), i.e. from the paradigm learning a domain model (LaDM) shifts to\nparadigm learning a pre-trained general foundation model followed by an\nadaptive domain model (LaGD). Under the new LaGD paradigm, the old datasets,\nwhich have led to advances in RSI intelligence understanding in the last\ndecade, are no longer suitable for fire-new tasks. We argued that a new dataset\nmust be designed to lighten tasks with the following features: 1)\nGeneralization: training model to learn shared knowledge among tasks and to\nadapt to different tasks; 2) Understanding complex scenes: training model to\nunderstand the fine-grained attribute of the objects of interest, and to be\nable to describe the scene with natural language; 3) Reasoning: training model\nto be able to realize high-level visual reasoning. In this paper, we designed a\nhigh-quality, diversified, and unified multimodal instruction-following dataset\nfor RSI understanding produced by GPT-4V and existing datasets, which we called\nRS-GPT4V. To achieve generalization, we used a (Question, Answer) which was\ndeduced from GPT-4V via instruction-following to unify the tasks such as\ncaptioning and localization; To achieve complex scene, we proposed a\nhierarchical instruction description with local strategy in which the\nfine-grained attributes of the objects and their spatial relationships are\ndescribed and global strategy in which all the local information are integrated\nto yield detailed instruction descript; To achieve reasoning, we designed\nmultiple-turn QA pair to provide the reasoning ability for a model. The\nempirical results show that the fine-tuned MLLMs by RS-GPT4V can describe\nfine-grained information. The dataset is available at:\nhttps://github.com/GeoX-Lab/RS-GPT4V.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "14 pages, 6 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2406.12479v1",
    "published_date": "2024-06-18 10:34:28 UTC",
    "updated_date": "2024-06-18 10:34:28 UTC"
  },
  {
    "arxiv_id": "2406.12468v1",
    "title": "Adaptive Token Biaser: Knowledge Editing via Biasing Key Entities",
    "authors": [
      "Baolong Bi",
      "Shenghua Liu",
      "Yiwei Wang",
      "Lingrui Mei",
      "Hongcheng Gao",
      "Yilong Xu",
      "Xueqi Cheng"
    ],
    "abstract": "The parametric knowledge memorized by large language models (LLMs) becomes\noutdated quickly. In-context editing (ICE) is currently the most effective\nmethod for updating the knowledge of LLMs. Recent advancements involve\nenhancing ICE by modifying the decoding strategy, obviating the need for\naltering internal model structures or adjusting external prompts. However, this\nenhancement operates across the entire sequence generation, encompassing a\nplethora of non-critical tokens. In this work, we introduce $\\textbf{A}$daptive\n$\\textbf{T}$oken $\\textbf{Bias}$er ($\\textbf{ATBias}$), a new decoding\ntechnique designed to enhance ICE. It focuses on the tokens that are mostly\nrelated to knowledge during decoding, biasing their logits by matching key\nentities related to new and parametric knowledge. Experimental results show\nthat ATBias significantly enhances ICE performance, achieving up to a 32.3%\nimprovement over state-of-the-art ICE methods while incurring only half the\nlatency. ATBias not only improves the knowledge editing capabilities of ICE but\ncan also be widely applied to LLMs with negligible cost.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12468v1",
    "published_date": "2024-06-18 10:18:06 UTC",
    "updated_date": "2024-06-18 10:18:06 UTC"
  },
  {
    "arxiv_id": "2406.12465v1",
    "title": "RIGL: A Unified Reciprocal Approach for Tracing the Independent and Group Learning Processes",
    "authors": [
      "Xiaoshan Yu",
      "Chuan Qin",
      "Dazhong Shen",
      "Shangshang Yang",
      "Haiping Ma",
      "Hengshu Zhu",
      "Xingyi Zhang"
    ],
    "abstract": "In the realm of education, both independent learning and group learning are\nesteemed as the most classic paradigms. The former allows learners to\nself-direct their studies, while the latter is typically characterized by\nteacher-directed scenarios. Recent studies in the field of intelligent\neducation have leveraged deep temporal models to trace the learning process,\ncapturing the dynamics of students' knowledge states, and have achieved\nremarkable performance. However, existing approaches have primarily focused on\nmodeling the independent learning process, with the group learning paradigm\nreceiving less attention. Moreover, the reciprocal effect between the two\nlearning processes, especially their combined potential to foster holistic\nstudent development, remains inadequately explored. To this end, in this paper,\nwe propose RIGL, a unified Reciprocal model to trace knowledge states at both\nthe individual and group levels, drawing from the Independent and Group\nLearning processes. Specifically, we first introduce a time frame-aware\nreciprocal embedding module to concurrently model both student and group\nresponse interactions across various time frames. Subsequently, we employ\nreciprocal enhanced learning modeling to fully exploit the comprehensive and\ncomplementary information between the two behaviors. Furthermore, we design a\nrelation-guided temporal attentive network, comprised of dynamic graph modeling\ncoupled with a temporal self-attention mechanism. It is used to delve into the\ndynamic influence of individual and group interactions throughout the learning\nprocesses. Conclusively, we introduce a bias-aware contrastive learning module\nto bolster the stability of the model's training. Extensive experiments on four\nreal-world educational datasets clearly demonstrate the effectiveness of the\nproposed RIGL model.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CY",
    "comment": "Accepted by KDD 2024. 12 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.12465v1",
    "published_date": "2024-06-18 10:16:18 UTC",
    "updated_date": "2024-06-18 10:16:18 UTC"
  },
  {
    "arxiv_id": "2406.12454v1",
    "title": "A Neural Column Generation Approach to the Vehicle Routing Problem with Two-Dimensional Loading and Last-In-First-Out Constraints",
    "authors": [
      "Yifan Xia",
      "Xiangyi Zhang"
    ],
    "abstract": "The vehicle routing problem with two-dimensional loading constraints\n(2L-CVRP) and the last-in-first-out (LIFO) rule presents significant practical\nand algorithmic challenges. While numerous heuristic approaches have been\nproposed to address its complexity, stemming from two NP-hard problems: the\nvehicle routing problem (VRP) and the two-dimensional bin packing problem\n(2D-BPP), less attention has been paid to developing exact algorithms. Bridging\nthis gap, this article presents an exact algorithm that integrates advanced\nmachine learning techniques, specifically a novel combination of attention and\nrecurrence mechanisms. This integration accelerates the state-of-the-art exact\nalgorithm by a median of 29.79% across various problem instances. Moreover, the\nproposed algorithm successfully resolves an open instance in the standard\ntest-bed, demonstrating significant improvements brought about by the\nincorporation of machine learning models. Code is available at\nhttps://github.com/xyfffff/NCG-for-2L-CVRP.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by International Joint Conference on Artificial Intelligence\n  (IJCAI 2024)",
    "pdf_url": "http://arxiv.org/pdf/2406.12454v1",
    "published_date": "2024-06-18 09:58:29 UTC",
    "updated_date": "2024-06-18 09:58:29 UTC"
  },
  {
    "arxiv_id": "2406.12452v2",
    "title": "Insect Identification in the Wild: The AMI Dataset",
    "authors": [
      "Aditya Jain",
      "Fagner Cunha",
      "Michael James Bunsen",
      "Juan Sebastián Cañas",
      "Léonard Pasi",
      "Nathan Pinoy",
      "Flemming Helsing",
      "JoAnne Russo",
      "Marc Botham",
      "Michael Sabourin",
      "Jonathan Fréchette",
      "Alexandre Anctil",
      "Yacksecari Lopez",
      "Eduardo Navarro",
      "Filonila Perez Pimentel",
      "Ana Cecilia Zamora",
      "José Alejandro Ramirez Silva",
      "Jonathan Gagnon",
      "Tom August",
      "Kim Bjerge",
      "Alba Gomez Segura",
      "Marc Bélisle",
      "Yves Basset",
      "Kent P. McFarland",
      "David Roy",
      "Toke Thomas Høye",
      "Maxim Larrivée",
      "David Rolnick"
    ],
    "abstract": "Insects represent half of all global biodiversity, yet many of the world's\ninsects are disappearing, with severe implications for ecosystems and\nagriculture. Despite this crisis, data on insect diversity and abundance remain\nwoefully inadequate, due to the scarcity of human experts and the lack of\nscalable tools for monitoring. Ecologists have started to adopt camera traps to\nrecord and study insects, and have proposed computer vision algorithms as an\nanswer for scalable data processing. However, insect monitoring in the wild\nposes unique challenges that have not yet been addressed within computer\nvision, including the combination of long-tailed data, extremely similar\nclasses, and significant distribution shifts. We provide the first large-scale\nmachine learning benchmarks for fine-grained insect recognition, designed to\nmatch real-world tasks faced by ecologists. Our contributions include a curated\ndataset of images from citizen science platforms and museums, and an\nexpert-annotated dataset drawn from automated camera traps across multiple\ncontinents, designed to test out-of-distribution generalization under field\nconditions. We train and evaluate a variety of baseline algorithms and\nintroduce a combination of data augmentation techniques that enhance\ngeneralization across geographies and hardware setups.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Published at ECCV 2024. The dataset is publicly available at\n  https://github.com/RolnickLab/ami-dataset",
    "pdf_url": "http://arxiv.org/pdf/2406.12452v2",
    "published_date": "2024-06-18 09:57:02 UTC",
    "updated_date": "2024-09-30 03:56:37 UTC"
  },
  {
    "arxiv_id": "2406.12449v1",
    "title": "Retrieval-Augmented Generation for Generative Artificial Intelligence in Medicine",
    "authors": [
      "Rui Yang",
      "Yilin Ning",
      "Emilia Keppo",
      "Mingxuan Liu",
      "Chuan Hong",
      "Danielle S Bitterman",
      "Jasmine Chiat Ling Ong",
      "Daniel Shu Wei Ting",
      "Nan Liu"
    ],
    "abstract": "Generative artificial intelligence (AI) has brought revolutionary innovations\nin various fields, including medicine. However, it also exhibits limitations.\nIn response, retrieval-augmented generation (RAG) provides a potential\nsolution, enabling models to generate more accurate contents by leveraging the\nretrieval of external knowledge. With the rapid advancement of generative AI,\nRAG can pave the way for connecting this transformative technology with medical\napplications and is expected to bring innovations in equity, reliability, and\npersonalization to health care.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12449v1",
    "published_date": "2024-06-18 09:53:37 UTC",
    "updated_date": "2024-06-18 09:53:37 UTC"
  },
  {
    "arxiv_id": "2406.12442v2",
    "title": "Abstraction-of-Thought Makes Language Models Better Reasoners",
    "authors": [
      "Ruixin Hong",
      "Hongming Zhang",
      "Xiaoman Pan",
      "Dong Yu",
      "Changshui Zhang"
    ],
    "abstract": "Abstract reasoning, the ability to reason from the abstract essence of a\nproblem, serves as a key to generalization in human reasoning. However,\neliciting language models to perform reasoning with abstraction remains\nunexplored. This paper seeks to bridge this gap by introducing a novel\nstructured reasoning format called Abstraction-of-Thought (AoT). The uniqueness\nof AoT lies in its explicit requirement for varying levels of abstraction\nwithin the reasoning process. This approach could elicit language models to\nfirst contemplate on the abstract level before incorporating concrete details,\nwhich is overlooked by the prevailing step-by-step Chain-of-Thought (CoT)\nmethod. To align models with the AoT format, we present AoT Collection, a\ngeneric finetuning dataset consisting of 348k high-quality samples with AoT\nreasoning processes, collected via an automated and scalable pipeline. We\nfinetune a wide range of language models with AoT Collection and conduct\nextensive evaluations on 23 unseen tasks from the challenging benchmark\nBig-Bench Hard. Experimental results indicate that models aligned to AoT\nreasoning format substantially outperform those aligned to CoT in many\nreasoning tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2024 Findings",
    "pdf_url": "http://arxiv.org/pdf/2406.12442v2",
    "published_date": "2024-06-18 09:46:44 UTC",
    "updated_date": "2024-09-26 11:15:14 UTC"
  },
  {
    "arxiv_id": "2406.12441v1",
    "title": "Cycle-Correspondence Loss: Learning Dense View-Invariant Visual Features from Unlabeled and Unordered RGB Images",
    "authors": [
      "David B. Adrian",
      "Andras Gabor Kupcsik",
      "Markus Spies",
      "Heiko Neumann"
    ],
    "abstract": "Robot manipulation relying on learned object-centric descriptors became\npopular in recent years. Visual descriptors can easily describe manipulation\ntask objectives, they can be learned efficiently using self-supervision, and\nthey can encode actuated and even non-rigid objects. However, learning robust,\nview-invariant keypoints in a self-supervised approach requires a meticulous\ndata collection approach involving precise calibration and expert supervision.\nIn this paper we introduce Cycle-Correspondence Loss (CCL) for view-invariant\ndense descriptor learning, which adopts the concept of cycle-consistency,\nenabling a simple data collection pipeline and training on unpaired RGB camera\nviews. The key idea is to autonomously detect valid pixel correspondences by\nattempting to use a prediction over a new image to predict the original pixel\nin the original image, while scaling error terms based on the estimated\nconfidence. Our evaluation shows that we outperform other self-supervised\nRGB-only methods, and approach performance of supervised methods, both with\nrespect to keypoint tracking as well as for a robot grasping downstream task.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 4 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2406.12441v1",
    "published_date": "2024-06-18 09:44:56 UTC",
    "updated_date": "2024-06-18 09:44:56 UTC"
  },
  {
    "arxiv_id": "2406.12435v1",
    "title": "Federated Learning with Limited Node Labels",
    "authors": [
      "Bisheng Tang",
      "Xiaojun Chen",
      "Shaopu Wang",
      "Yuexin Xuan",
      "Zhendong Zhao"
    ],
    "abstract": "Subgraph federated learning (SFL) is a research methodology that has gained\nsignificant attention for its potential to handle distributed graph-structured\ndata. In SFL, the local model comprises graph neural networks (GNNs) with a\npartial graph structure. However, some SFL models have overlooked the\nsignificance of missing cross-subgraph edges, which can lead to local GNNs\nbeing unable to message-pass global representations to other parties' GNNs.\nMoreover, existing SFL models require substantial labeled data, which limits\ntheir practical applications. To overcome these limitations, we present a novel\nSFL framework called FedMpa that aims to learn cross-subgraph node\nrepresentations. FedMpa first trains a multilayer perceptron (MLP) model using\na small amount of data and then propagates the federated feature to the local\nstructures. To further improve the embedding representation of nodes with local\nsubgraphs, we introduce the FedMpae method, which reconstructs the local graph\nstructure with an innovation view that applies pooling operation to form\nsuper-nodes. Our extensive experiments on six graph datasets demonstrate that\nFedMpa is highly effective in node classification. Furthermore, our ablation\nexperiments verify the effectiveness of FedMpa.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12435v1",
    "published_date": "2024-06-18 09:30:10 UTC",
    "updated_date": "2024-06-18 09:30:10 UTC"
  },
  {
    "arxiv_id": "2406.12432v2",
    "title": "MEMS and ECM Sensor Technologies for Cardiorespiratory Sound Monitoring - A Comprehensive Review",
    "authors": [
      "Yasaman Torabi",
      "Shahram Shirani",
      "James P. Reilly",
      "Gail M Gauvreau"
    ],
    "abstract": "This paper presents a comprehensive review of cardiorespiratory auscultation\nsensing devices (i.e., stethoscopes), which is useful for understanding the\ntheoretical aspects and practical design notes. In this paper, we first\nintroduce the acoustic properties of the heart and lungs, as well as a brief\nhistory of stethoscope evolution. Then, we discuss the basic concept of\nelectret condenser microphones (ECMs) and a stethoscope based on them. Then, we\ndiscuss the microelectromechanical systems (MEMSs) technology, particularly\nfocusing on piezoelectric transducer sensors. This paper comprehensively\nreviews sensing technologies for cardiorespiratory auscultation, emphasizing\nMEMS-based wearable designs in the past decade. To our knowledge, this is the\nfirst paper to summarize ECM and MEMS applications for heart and lung sound\nanalysis.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12432v2",
    "published_date": "2024-06-18 09:28:23 UTC",
    "updated_date": "2025-02-14 08:05:28 UTC"
  },
  {
    "arxiv_id": "2406.12430v1",
    "title": "PlanRAG: A Plan-then-Retrieval Augmented Generation for Generative Large Language Models as Decision Makers",
    "authors": [
      "Myeonghwa Lee",
      "Seonho An",
      "Min-Soo Kim"
    ],
    "abstract": "In this paper, we conduct a study to utilize LLMs as a solution for decision\nmaking that requires complex data analysis. We define Decision QA as the task\nof answering the best decision, $d_{best}$, for a decision-making question $Q$,\nbusiness rules $R$ and a database $D$. Since there is no benchmark that can\nexamine Decision QA, we propose Decision QA benchmark, DQA. It has two\nscenarios, Locating and Building, constructed from two video games (Europa\nUniversalis IV and Victoria 3) that have almost the same goal as Decision QA.\nTo address Decision QA effectively, we also propose a new RAG technique called\nthe iterative plan-then-retrieval augmented generation (PlanRAG). Our\nPlanRAG-based LM generates the plan for decision making as the first step, and\nthe retriever generates the queries for data analysis as the second step. The\nproposed method outperforms the state-of-the-art iterative RAG method by 15.8%\nin the Locating scenario and by 7.4% in the Building scenario, respectively. We\nrelease our code and benchmark at https://github.com/myeon9h/PlanRAG.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "NAACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.12430v1",
    "published_date": "2024-06-18 09:25:35 UTC",
    "updated_date": "2024-06-18 09:25:35 UTC"
  },
  {
    "arxiv_id": "2406.12429v3",
    "title": "Query Routing for Homogeneous Tools: An Instantiation in the RAG Scenario",
    "authors": [
      "Feiteng Mu",
      "Yong Jiang",
      "Liwen Zhang",
      "Chu Liu",
      "Wenjie Li",
      "Pengjun Xie",
      "Fei Huang"
    ],
    "abstract": "Current research on tool learning primarily focuses on selecting the most\neffective tool from a wide array of options, often overlooking\ncost-effectiveness, a crucial factor in human problem-solving. In this paper,\nwe address the selection of homogeneous tools by predicting both their\nperformance and the associated cost required to accomplish a given task. We\nthen assign queries to the optimal tools in a cost-effective manner. Our\nexperimental results demonstrate that our method achieves higher performance at\na lower cost compared to strong baseline approaches.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12429v3",
    "published_date": "2024-06-18 09:24:09 UTC",
    "updated_date": "2024-09-30 06:04:05 UTC"
  },
  {
    "arxiv_id": "2406.12428v2",
    "title": "PSLM: Parallel Generation of Text and Speech with LLMs for Low-Latency Spoken Dialogue Systems",
    "authors": [
      "Kentaro Mitsui",
      "Koh Mitsuda",
      "Toshiaki Wakatsuki",
      "Yukiya Hono",
      "Kei Sawada"
    ],
    "abstract": "Multimodal language models that process both text and speech have a potential\nfor applications in spoken dialogue systems. However, current models face two\nmajor challenges in response generation latency: (1) generating a spoken\nresponse requires the prior generation of a written response, and (2) speech\nsequences are significantly longer than text sequences. This study addresses\nthese issues by extending the input and output sequences of the language model\nto support the parallel generation of text and speech. Our experiments on\nspoken question answering tasks demonstrate that our approach improves latency\nwhile maintaining the quality of response content. Additionally, we show that\nlatency can be further reduced by generating speech in multiple sequences. Demo\nsamples are available at https://rinnakk.github.io/research/publications/PSLM.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, 6 figures, 4 tables, accepted for Findings of EMNLP 2024.\n  Demo samples: https://rinnakk.github.io/research/publications/PSLM",
    "pdf_url": "http://arxiv.org/pdf/2406.12428v2",
    "published_date": "2024-06-18 09:23:54 UTC",
    "updated_date": "2024-10-03 05:17:25 UTC"
  },
  {
    "arxiv_id": "2407.12791v1",
    "title": "TourLLM: Enhancing LLMs with Tourism Knowledge",
    "authors": [
      "Qikai Wei",
      "Mingzhi Yang",
      "Jinqiang Wang",
      "Wenwei Mao",
      "Jiabo Xu",
      "Huansheng Ning"
    ],
    "abstract": "Recently, large language models (LLMs) have demonstrated their effectiveness\nin various natural language processing (NLP) tasks. However, the lack of\ntourism knowledge limits the performance of LLMs in tourist attraction\npresentations and travel planning. To address this challenge, we constructed a\nsupervised fine-tuning dataset for the culture and tourism domain, named\nCultour. This dataset consists of three parts: tourism knowledge base QA data,\ntravelogues data, and tourism diversity QA data. Additionally, we propose\nTourLLM, a Qwen-based model supervised fine-tuned with Cultour, to improve the\nquality of the information provided about attractions and travel planning. To\nevaluate the performance of TourLLM, we employed both automatic and human\nevaluation, and we proposed a human evaluation criterion named CRA\n(Consistency, Readability, Availability). The experimental results demonstrate\nthe effectiveness of the responses generated by the TourLLM. Our proposed\nCultour is accessible at https://github.com/mrweiqk/Cultour.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12791v1",
    "published_date": "2024-06-18 09:15:46 UTC",
    "updated_date": "2024-06-18 09:15:46 UTC"
  },
  {
    "arxiv_id": "2406.12416v2",
    "title": "Beyond Under-Alignment: Atomic Preference Enhanced Factuality Tuning for Large Language Models",
    "authors": [
      "Hongbang Yuan",
      "Yubo Chen",
      "Pengfei Cao",
      "Zhuoran Jin",
      "Kang Liu",
      "Jun Zhao"
    ],
    "abstract": "Large language models (LLMs) have achieved remarkable success but still tend\nto generate factually erroneous responses, a phenomenon known as hallucination.\nA recent trend is to use preference learning to fine-tune models to align with\nfactuality. However, existing work primarily evaluates fine-tuned models on\nin-domain (ID) datasets and the factuality on out-of-domain (OOD) datasets\nremains underexplored. In this paper, we conduct a comprehensive evaluation of\nthe factuality of different models tuned by various preference learning\nalgorithms and demonstrate that their performance on OOD datasets either\nincreases minimally or decreases. Subsequently, we reveal that the main cause\nof model's failure to uphold factuality under a distribution shift is\n\\textbf{under-alignment}, rather than \\textbf{over-alignment}, by analyzing the\ntoken distribution shift of the models before and after tuning. Finally, we\npropose \\textbf{APEFT} (\\textbf{A}tomic \\textbf{P}reference \\textbf{E}nhanced\n\\textbf{F}actuality \\textbf{T}uning), a framework that enhances model's\nawareness of factuality at the granularity of individual facts. Extensive\nexperiments demonstrate that APEFT improves model performance by an average of\n$\\boldsymbol{3.45\\%}$ on both ID and OOD datasets, which is highly effective.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12416v2",
    "published_date": "2024-06-18 09:07:30 UTC",
    "updated_date": "2024-06-27 12:07:55 UTC"
  },
  {
    "arxiv_id": "2406.12413v2",
    "title": "Pushing the Frontier on Approximate EFX Allocations",
    "authors": [
      "Georgios Amanatidis",
      "Aris Filos-Ratsikas",
      "Alkmini Sgouritsa"
    ],
    "abstract": "We study the problem of allocating a set of indivisible goods to a set of\nagents with additive valuation functions, aiming to achieve approximate\nenvy-freeness up to any good ($\\alpha$-EFX). The state-of-the-art results on\nthe problem include that (exact) EFX allocations exist when (a) there are at\nmost three agents, or (b) the agents' valuation functions can take at most two\nvalues, or (c) the agents' valuation functions can be represented via a graph.\nFor $\\alpha$-EFX, it is known that a $0.618$-EFX allocation exists for any\nnumber of agents with additive valuation functions. In this paper, we show that\n$2/3$-EFX allocations exist when (a) there are at most \\emph{seven agents}, (b)\nthe agents' valuation functions can take at most \\emph{three values}, or (c)\nthe agents' valuation functions can be represented via a \\emph{multigraph}. Our\nresults can be interpreted in two ways. First, by relaxing the notion of EFX to\n$2/3$-EFX, we obtain existence results for strict generalizations of the\nsettings for which exact EFX allocations are known to exist. Secondly, by\nimposing restrictions on the setting, we manage to beat the barrier of $0.618$\nand achieve an approximation guarantee of $2/3$. Therefore, our results push\nthe \\emph{frontier} of existence and computation of approximate EFX\nallocations, and provide insights into the challenges of settling the existence\nof exact EFX allocations.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.DM"
    ],
    "primary_category": "cs.GT",
    "comment": "The conference version of this work has been accepted to the\n  Twenty-Fifth ACM Conference on Economics and Computation (EC 2024)",
    "pdf_url": "http://arxiv.org/pdf/2406.12413v2",
    "published_date": "2024-06-18 09:01:37 UTC",
    "updated_date": "2025-04-23 13:55:32 UTC"
  },
  {
    "arxiv_id": "2406.12412v1",
    "title": "A Novel Algorithm for Community Detection in Networks using Rough Sets and Consensus Clustering",
    "authors": [
      "Darian H. Grass-Boada",
      "Leandro González-Montesino",
      "Rubén Armañanzas"
    ],
    "abstract": "Complex networks, such as those in social, biological, and technological\nsystems, often present challenges to the task of community detection. Our\nresearch introduces a novel rough clustering based consensus community\nframework (RC-CCD) for effective structure identification of network\ncommunities. The RC-CCD method employs rough set theory to handle uncertainties\nwithin data and utilizes a consensus clustering approach to aggregate multiple\nclustering results, enhancing the reliability and accuracy of community\ndetection. This integration allows the RC-CCD to effectively manage overlapping\ncommunities, which are often present in complex networks.\n  This approach excels at detecting overlapping communities, offering a\ndetailed and accurate representation of network structures. Comprehensive\ntesting on benchmark networks generated by the Lancichinetti-Fortunato-Radicchi\nmethod showcased the strength and adaptability of the new proposal to varying\nnode degrees and community sizes. Cross-comparisons of RC-CCD versus other well\nknown detection algorithms outcomes highlighted its stability and adaptability.",
    "categories": [
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12412v1",
    "published_date": "2024-06-18 09:01:21 UTC",
    "updated_date": "2024-06-18 09:01:21 UTC"
  },
  {
    "arxiv_id": "2406.12406v1",
    "title": "Fast Rates for Bandit PAC Multiclass Classification",
    "authors": [
      "Liad Erez",
      "Alon Cohen",
      "Tomer Koren",
      "Yishay Mansour",
      "Shay Moran"
    ],
    "abstract": "We study multiclass PAC learning with bandit feedback, where inputs are\nclassified into one of $K$ possible labels and feedback is limited to whether\nor not the predicted labels are correct. Our main contribution is in designing\na novel learning algorithm for the agnostic $(\\varepsilon,\\delta)$-PAC version\nof the problem, with sample complexity of $O\\big( (\\operatorname{poly}(K) + 1 /\n\\varepsilon^2) \\log (|H| / \\delta) \\big)$ for any finite hypothesis class $H$.\nIn terms of the leading dependence on $\\varepsilon$, this improves upon\nexisting bounds for the problem, that are of the form $O(K/\\varepsilon^2)$. We\nalso provide an extension of this result to general classes and establish\nsimilar sample complexity bounds in which $\\log |H|$ is replaced by the\nNatarajan dimension. This matches the optimal rate in the full-information\nversion of the problem and resolves an open question studied by Daniely,\nSabato, Ben-David, and Shalev-Shwartz (2011) who demonstrated that the\nmultiplicative price of bandit feedback in realizable PAC learning is\n$\\Theta(K)$. We complement this by revealing a stark contrast with the agnostic\ncase, where the price of bandit feedback is only $O(1)$ as $\\varepsilon \\to 0$.\nOur algorithm utilizes a stochastic optimization technique to minimize a\nlog-barrier potential based on Frank-Wolfe updates for computing a low-variance\nexploration distribution over the hypotheses, and is made computationally\nefficient provided access to an ERM oracle over $H$.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12406v1",
    "published_date": "2024-06-18 08:54:04 UTC",
    "updated_date": "2024-06-18 08:54:04 UTC"
  },
  {
    "arxiv_id": "2406.12403v1",
    "title": "PDSS: A Privacy-Preserving Framework for Step-by-Step Distillation of Large Language Models",
    "authors": [
      "Tao Fan",
      "Yan Kang",
      "Weijing Chen",
      "Hanlin Gu",
      "Yuanfeng Song",
      "Lixin Fan",
      "Kai Chen",
      "Qiang Yang"
    ],
    "abstract": "In the context of real-world applications, leveraging large language models\n(LLMs) for domain-specific tasks often faces two major challenges:\ndomain-specific knowledge privacy and constrained resources. To address these\nissues, we propose PDSS, a privacy-preserving framework for step-by-step\ndistillation of LLMs. PDSS works on a server-client architecture, wherein\nclient transmits perturbed prompts to the server's LLM for rationale\ngeneration. The generated rationales are then decoded by the client and used to\nenrich the training of task-specific small language model(SLM) within a\nmulti-task learning paradigm. PDSS introduces two privacy protection\nstrategies: the Exponential Mechanism Strategy and the Encoder-Decoder\nStrategy, balancing prompt privacy and rationale usability. Experiments\ndemonstrate the effectiveness of PDSS in various text generation tasks,\nenabling the training of task-specific SLM with enhanced performance while\nprioritizing data privacy protection.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12403v1",
    "published_date": "2024-06-18 08:48:14 UTC",
    "updated_date": "2024-06-18 08:48:14 UTC"
  },
  {
    "arxiv_id": "2406.12400v1",
    "title": "A Cutting-Edge Deep Learning Method For Enhancing IoT Security",
    "authors": [
      "Nadia Ansar",
      "Mohammad Sadique Ansari",
      "Mohammad Sharique",
      "Aamina Khatoon",
      "Md Abdul Malik",
      "Md Munir Siddiqui"
    ],
    "abstract": "There have been significant issues given the IoT, with heterogeneity of\nbillions of devices and with a large amount of data. This paper proposed an\ninnovative design of the Internet of Things (IoT) Environment Intrusion\nDetection System (or IDS) using Deep Learning-integrated Convolutional Neural\nNetworks (CNN) and Long Short-Term Memory (LSTM) networks. Our model, based on\nthe CICIDS2017 dataset, achieved an accuracy of 99.52% in classifying network\ntraffic as either benign or malicious. The real-time processing capability,\nscalability, and low false alarm rate in our model surpass some traditional IDS\napproaches and, therefore, prove successful for application in today's IoT\nnetworks. The development and the performance of the model, with possible\napplications that may extend to other related fields of adaptive learning\ntechniques and cross-domain applicability, are discussed. The research\ninvolving deep learning for IoT cybersecurity offers a potent solution for\nsignificantly improving network security.",
    "categories": [
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12400v1",
    "published_date": "2024-06-18 08:42:51 UTC",
    "updated_date": "2024-06-18 08:42:51 UTC"
  },
  {
    "arxiv_id": "2406.12399v1",
    "title": "QueerBench: Quantifying Discrimination in Language Models Toward Queer Identities",
    "authors": [
      "Mae Sosto",
      "Alberto Barrón-Cedeño"
    ],
    "abstract": "With the increasing role of Natural Language Processing (NLP) in various\napplications, challenges concerning bias and stereotype perpetuation are\naccentuated, which often leads to hate speech and harm. Despite existing\nstudies on sexism and misogyny, issues like homophobia and transphobia remain\nunderexplored and often adopt binary perspectives, putting the safety of\nLGBTQIA+ individuals at high risk in online spaces. In this paper, we assess\nthe potential harm caused by sentence completions generated by English large\nlanguage models (LLMs) concerning LGBTQIA+ individuals. This is achieved using\nQueerBench, our new assessment framework, which employs a template-based\napproach and a Masked Language Modeling (MLM) task. The analysis indicates that\nlarge language models tend to exhibit discriminatory behaviour more frequently\ntowards individuals within the LGBTQIA+ community, reaching a difference gap of\n7.2% in the QueerBench score of harmfulness.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12399v1",
    "published_date": "2024-06-18 08:40:29 UTC",
    "updated_date": "2024-06-18 08:40:29 UTC"
  },
  {
    "arxiv_id": "2406.12946v1",
    "title": "Instruction Data Generation and Unsupervised Adaptation for Speech Language Models",
    "authors": [
      "Vahid Noroozi",
      "Zhehuai Chen",
      "Somshubra Majumdar",
      "Steve Huang",
      "Jagadeesh Balam",
      "Boris Ginsburg"
    ],
    "abstract": "In this paper, we propose three methods for generating synthetic samples to\ntrain and evaluate multimodal large language models capable of processing both\ntext and speech inputs. Addressing the scarcity of samples containing both\nmodalities, synthetic data generation emerges as a crucial strategy to enhance\nthe performance of such systems and facilitate the modeling of cross-modal\nrelationships between the speech and text domains. Our process employs large\nlanguage models to generate textual components and text-to-speech systems to\ngenerate speech components. The proposed methods offer a practical and\neffective means to expand the training dataset for these models. Experimental\nresults show progress in achieving an integrated understanding of text and\nspeech. We also highlight the potential of using unlabeled speech data to\ngenerate synthetic samples comparable in quality to those with available\ntranscriptions, enabling the expansion of these models to more languages.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted for Interspeech 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.12946v1",
    "published_date": "2024-06-18 08:27:00 UTC",
    "updated_date": "2024-06-18 08:27:00 UTC"
  },
  {
    "arxiv_id": "2406.12381v3",
    "title": "QOG:Question and Options Generation based on Language Model",
    "authors": [
      "Jincheng Zhou"
    ],
    "abstract": "Question-Options Generation (QOG) is a task that involves generating a set of\nquestion-options pairs given context. This task has various applications,\nincluding fine-tuning large models, information retrieval, and automated\nmultiple-choice question generation for education. In this paper, we develop\nQOG models using three different methods based on fine-tuning\nsequence-to-sequence language models (LMs). Experiments demonstrate that the\nend-to-end QOG model is computationally efficient and stable during both\ntraining and inference, outperforming other methods. Furthermore, our analysis\nindicates that our QOG models are competitive on the QOG task compared to the\nlarge language model Llama 3-8B.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12381v3",
    "published_date": "2024-06-18 08:09:58 UTC",
    "updated_date": "2024-07-16 08:12:03 UTC"
  },
  {
    "arxiv_id": "2406.12375v1",
    "title": "GW-MoE: Resolving Uncertainty in MoE Router with Global Workspace Theory",
    "authors": [
      "Haoze Wu",
      "Zihan Qiu",
      "Zili Wang",
      "Hang Zhao",
      "Jie Fu"
    ],
    "abstract": "Mixture-of-Experts (MoE) has been demonstrated as an efficient method to\nscale up models. By dynamically and sparsely selecting activated experts, MoE\ncan effectively reduce computational costs. Despite the success, we observe\nthat many tokens in the MoE models have uncertain routing results. These tokens\nhave nearly equal scores for choosing each expert, and we demonstrate that this\nuncertainty can lead to incorrect selections. Inspired by the Global Workspace\nTheory (GWT), we propose a new fine-tuning method, GW-MoE, to address this\nissue. The core idea is to broadcast the uncertain tokens across experts during\nfine-tuning. Therefore, these tokens can acquire the necessary knowledge from\nany expert during inference and become less sensitive to the choice. GW-MoE\ndoes not introduce additional inference overhead. We validate that GW can\nmitigate the uncertain problem and consistently improve in different tasks\n(text classification, question answering, summarization, code generation, and\nmathematical problem solving) and model sizes (650M and 8B parameters).",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12375v1",
    "published_date": "2024-06-18 08:03:51 UTC",
    "updated_date": "2024-06-18 08:03:51 UTC"
  },
  {
    "arxiv_id": "2406.12374v3",
    "title": "Problem-Solving in Language Model Networks",
    "authors": [
      "Ciaran Regan",
      "Alexandre Gournail",
      "Mizuki Oka"
    ],
    "abstract": "To improve the reasoning and question-answering capabilities of Large\nLanguage Models (LLMs), several multi-agent approaches have been introduced.\nWhile these methods enhance performance, the application of collective\nintelligence-based approaches to complex network structures and the dynamics of\nagent interactions remain underexplored. This work extends the concept of\nmulti-agent debate to more general network topologies, measuring the\nquestion-answering accuracy, influence, consensus, and the effects of bias on\nthe collective. The results show that random networks perform similarly to\nfully connected networks despite using significantly fewer tokens. Furthermore,\na strong consensus among agents correlates with correct answers, whereas\ndivided responses typically indicate incorrect answers. Analysing the influence\nof the agents reveals a balance between self-reflection and interconnectedness;\nself-reflection aids when local interactions are incorrect, and local\ninteractions aid when the agent itself is incorrect. Additionally, bias plays a\nstrong role in system performance with correctly biased hub nodes boosting\nperformance. These insights suggest that using random networks or scale-free\nnetworks with knowledgeable agents placed in central positions can enhance the\noverall question-answering performance of multi-agent systems.",
    "categories": [
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 2024 Conference on Artificial Life",
    "pdf_url": "http://arxiv.org/pdf/2406.12374v3",
    "published_date": "2024-06-18 07:59:14 UTC",
    "updated_date": "2024-07-09 13:05:58 UTC"
  },
  {
    "arxiv_id": "2406.12373v3",
    "title": "WebCanvas: Benchmarking Web Agents in Online Environments",
    "authors": [
      "Yichen Pan",
      "Dehan Kong",
      "Sida Zhou",
      "Cheng Cui",
      "Yifei Leng",
      "Bing Jiang",
      "Hangyu Liu",
      "Yanyi Shang",
      "Shuyan Zhou",
      "Tongshuang Wu",
      "Zhengyang Wu"
    ],
    "abstract": "For web agents to be practically useful, they must adapt to the continuously\nevolving web environment characterized by frequent updates to user interfaces\nand content. However, most existing benchmarks only capture the static aspects\nof the web. To bridge this gap, we introduce WebCanvas, an innovative online\nevaluation framework for web agents that effectively addresses the dynamic\nnature of web interactions. WebCanvas contains three main components to\nfacilitate realistic assessments: (1) A novel evaluation metric which reliably\ncapture critical intermediate actions or states necessary for task completions\nwhile disregarding noise caused by insignificant events or changed\nweb-elements. (2) A benchmark dataset called Mind2Web-Live, a refined version\nof original Mind2Web static dataset containing 542 tasks with 2439 intermediate\nevaluation states; (3) Lightweight and generalizable annotation tools and\ntesting pipelines that enables the community to collect and maintain the\nhigh-quality, up-to-date dataset. Building on WebCanvas, we open-source an\nagent framework with extensible modules for reasoning, providing a foundation\nfor the community to conduct online inference and evaluations. Our\nbest-performing agent achieves a task success rate of 23.1% and a task\ncompletion rate of 48.8% on the Mind2Web-Live test set. Additionally, we\nanalyze the performance discrepancies across various websites, domains, and\nexperimental environments. We encourage the community to contribute further\ninsights on online agent evaluation, thereby advancing this field of research.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "68T50",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "Our platform, tool and dataset are publically available at\n  https://www.imean.ai/web-canvas/ and\n  https://huggingface.co/datasets/iMeanAI/Mind2Web-Live/",
    "pdf_url": "http://arxiv.org/pdf/2406.12373v3",
    "published_date": "2024-06-18 07:58:33 UTC",
    "updated_date": "2024-07-16 06:19:21 UTC"
  },
  {
    "arxiv_id": "2406.12370v1",
    "title": "UAV-based Intelligent Information Systems on Winter Road Safety for Autonomous Vehicles",
    "authors": [
      "Siva Ariram",
      "Veikko Pekkala",
      "Timo Mäenpää",
      "Antti Tikänmaki",
      "Juha Röning"
    ],
    "abstract": "As autonomous vehicles continue to revolutionize transportation, addressing\nchallenges posed by adverse weather conditions, particularly during winter,\nbecomes paramount for ensuring safe and efficient operations. One of the most\nimportant aspects of a road safety inspection during adverse weather is when a\nlimited lane width can reduce the capacity of the road and raise the risk of\nserious accidents involving autonomous vehicles. In this research, a method for\nimproving driving challenges on roads in winter conditions, with a model that\nsegments and estimates the width of the road from the perspectives of Uncrewed\naerial vehicles and autonomous vehicles. The proposed approach in this article\nis needed to empower self-driving cars with up-to-date and accurate insights,\nenhancing their adaptability and decision-making capabilities in winter\nlandscapes.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12370v1",
    "published_date": "2024-06-18 07:53:37 UTC",
    "updated_date": "2024-06-18 07:53:37 UTC"
  },
  {
    "arxiv_id": "2407.07099v3",
    "title": "Nash CoT: Multi-Path Inference with Preference Equilibrium",
    "authors": [
      "Ziqi Zhang",
      "Cunxiang Wang",
      "Xiong Xiao",
      "Yue Zhang",
      "Donglin Wang"
    ],
    "abstract": "Chain of thought (CoT) is a reasoning framework that can enhance the\nperformance of Large Language Models (LLMs) on complex inference tasks. In\nparticular, among various studies related to CoT, multi-path inference stands\nout as a simple yet effective improvement. However, there is no optimal setting\nfor the number of inference paths. Therefore, we have to increase the number of\ninference paths to obtain better results, which in turn increases the inference\ncost. To address this limitation, we can utilize question-related role\ntemplates to guide LLMs into relevant roles, thereby increasing the possibility\nof correct inferences for each path and further reducing dependence on the\nnumber of inference paths while improving reasoning accuracy. However, placing\nLLMs into specific roles may reduce their reasoning diversity and performance\non a few tasks where role dependence is low. To alleviate the excessive\nimmersion of the LLM into a specific role, we propose Nash CoT by constructing\na game system on each path that balances the generation from role-specific\nLLMs' and the general LLMs' generation, thereby ensuring both effective role\nadoption and diversity in LLM generation further maintaining the performance of\nmulti-path inference while reducing the requirement of the number of inference\npaths. We evaluate Nash CoT across various inference tasks, including Arabic\nReasoning, Commonsense Question Answering, and Symbolic Inference, achieving\nresults that are comparable to or better than those of multi-path CoT with the\nequal number of inference paths.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.GT",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.07099v3",
    "published_date": "2024-06-18 07:46:13 UTC",
    "updated_date": "2024-12-30 13:43:46 UTC"
  },
  {
    "arxiv_id": "2406.12362v1",
    "title": "Certified ML Object Detection for Surveillance Missions",
    "authors": [
      "Mohammed Belcaid",
      "Eric Bonnafous",
      "Louis Crison",
      "Christophe Faure",
      "Eric Jenn",
      "Claire Pagetti"
    ],
    "abstract": "In this paper, we present a development process of a drone detection system\ninvolving a machine learning object detection component. The purpose is to\nreach acceptable performance objectives and provide sufficient evidences,\nrequired by the recommendations (soon to be published) of the ED 324 / ARP 6983\nstandard, to gain confidence in the dependability of the designed system.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12362v1",
    "published_date": "2024-06-18 07:42:22 UTC",
    "updated_date": "2024-06-18 07:42:22 UTC"
  },
  {
    "arxiv_id": "2406.12359v1",
    "title": "Memory Sequence Length of Data Sampling Impacts the Adaptation of Meta-Reinforcement Learning Agents",
    "authors": [
      "Menglong Zhang",
      "Fuyuan Qian",
      "Quanying Liu"
    ],
    "abstract": "Fast adaptation to new tasks is extremely important for embodied agents in\nthe real world. Meta-reinforcement learning (meta-RL) has emerged as an\neffective method to enable fast adaptation in unknown environments. Compared to\non-policy meta-RL algorithms, off-policy algorithms rely heavily on efficient\ndata sampling strategies to extract and represent the historical trajectories.\nHowever, little is known about how different data sampling methods impact the\nability of meta-RL agents to represent unknown environments. Here, we\ninvestigate the impact of data sampling strategies on the exploration and\nadaptability of meta-RL agents. Specifically, we conducted experiments with two\ntypes of off-policy meta-RL algorithms based on Thompson sampling and\nBayes-optimality theories in continuous control tasks within the MuJoCo\nenvironment and sparse reward navigation tasks. Our analysis revealed the\nlong-memory and short-memory sequence sampling strategies affect the\nrepresentation and adaptive capabilities of meta-RL agents. We found that the\nalgorithm based on Bayes-optimality theory exhibited more robust and better\nadaptability than the algorithm based on Thompson sampling, highlighting the\nimportance of appropriate data sampling strategies for the agent's\nrepresentation of an unknown environment, especially in the case of sparse\nrewards.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12359v1",
    "published_date": "2024-06-18 07:41:40 UTC",
    "updated_date": "2024-06-18 07:41:40 UTC"
  },
  {
    "arxiv_id": "2406.12345v1",
    "title": "Navigating Knowledge Management Implementation Success in Government Organizations: A type-2 fuzzy approach",
    "authors": [
      "Saman Foroutani",
      "Nasim Fahimian",
      "Reyhaneh Jalalinejad",
      "Morteza Hezarkhani",
      "Samaneh Mahmoudi",
      "Behrooz Gharleghi"
    ],
    "abstract": "Optimal information and knowledge management is crucial for organizations to\nachieve their objectives efficiently. As a rare and valuable resource,\neffective knowledge management provides a strategic advantage and has become a\nkey determinant of organizational success. The study aims to identify critical\nsuccess and failure factors for implementing knowledge management systems in\ngovernment organizations. This research employs a descriptive survey\nmethodology, collecting data through random interviews and questionnaires. The\nstudy highlights the critical success factors for knowledge management systems\nin government organizations, including cooperation, an open atmosphere, staff\ntraining, creativity and innovation, removal of organizational constraints,\nreward policies, role modeling, and focus. Conversely, failure to consider\nformality, staff participation, collaboration technologies, network and\nhardware infrastructure, complexity, IT staff, and trust can pose significant\nobstacles to successful implementation.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12345v1",
    "published_date": "2024-06-18 07:22:32 UTC",
    "updated_date": "2024-06-18 07:22:32 UTC"
  },
  {
    "arxiv_id": "2407.13690v2",
    "title": "DART-Math: Difficulty-Aware Rejection Tuning for Mathematical Problem-Solving",
    "authors": [
      "Yuxuan Tong",
      "Xiwen Zhang",
      "Rui Wang",
      "Ruidong Wu",
      "Junxian He"
    ],
    "abstract": "Solving mathematical problems requires advanced reasoning abilities and\npresents notable challenges for large language models. Previous works usually\nsynthesize data from proprietary models to augment existing datasets, followed\nby instruction tuning to achieve top-tier results. However, our analysis of\nthese datasets reveals severe biases towards easy queries, with frequent\nfailures to generate any correct response for the most challenging queries.\nHypothesizing that difficult queries are crucial to learn complex reasoning, we\npropose Difficulty-Aware Rejection Tuning (DART), a method that allocates\ndifficult queries more trials during the synthesis phase, enabling more\nextensive training on difficult samples. Utilizing DART, we have created new\ndatasets for mathematical problem-solving that focus more on difficult queries\nand are substantially smaller than previous ones. Remarkably, our synthesis\nprocess solely relies on a 7B-sized open-weight model, without reliance on the\ncommonly used proprietary GPT-4. We fine-tune various base models on our\ndatasets ranging from 7B to 70B in size, resulting in a series of strong models\ncalled DART-MATH. In comprehensive in-domain and out-of-domain evaluation on 6\nmathematical benchmarks, DART-MATH outperforms vanilla rejection tuning\nsignificantly, being superior or comparable to previous arts, despite using\nmuch smaller datasets and no proprietary models. Furthermore, our results\nposition our synthetic datasets as the most effective and cost-efficient\npublicly available resources for advancing mathematical problem-solving.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "NeurIPS 2024. Data and model checkpoints are available at\n  https://github.com/hkust-nlp/dart-math",
    "pdf_url": "http://arxiv.org/pdf/2407.13690v2",
    "published_date": "2024-06-18 07:14:02 UTC",
    "updated_date": "2024-12-23 17:32:21 UTC"
  },
  {
    "arxiv_id": "2406.12336v2",
    "title": "Towards Understanding Domain Adapted Sentence Embeddings for Document Retrieval",
    "authors": [
      "Sujoy Roychowdhury",
      "Sumit Soman",
      "H. G. Ranjani",
      "Vansh Chhabra",
      "Neeraj Gunda",
      "Shashank Gautam",
      "Subhadip Bandyopadhyay",
      "Sai Krishna Bala"
    ],
    "abstract": "A plethora of sentence embedding models makes it challenging to choose one,\nespecially for technical domains rich with specialized vocabulary. In this\nwork, we domain adapt embeddings using telecom, health and science datasets for\nquestion answering. We evaluate embeddings obtained from publicly available\nmodels and their domain-adapted variants, on both point retrieval accuracies,\nas well as their (95\\%) confidence intervals. We establish a systematic method\nto obtain thresholds for similarity scores for different embeddings. As\nexpected, we observe that fine-tuning improves mean bootstrapped accuracies. We\nalso observe that it results in tighter confidence intervals, which further\nimprove when pre-training is preceded by fine-tuning. We introduce metrics\nwhich measure the distributional overlaps of top-$K$, correct and random\ndocument similarities with the question. Further, we show that these metrics\nare correlated with retrieval accuracy and similarity thresholds. Recent\nliterature shows conflicting effects of isotropy on retrieval accuracies. Our\nexperiments establish that the isotropy of embeddings (as measured by two\nindependent state-of-the-art isotropy metric definitions) is poorly correlated\nwith retrieval performance. We show that embeddings for domain-specific\nsentences have little overlap with those for domain-agnostic ones, and\nfine-tuning moves them further apart. Based on our results, we provide\nrecommendations for use of our methodology and metrics by researchers and\npractitioners.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "68T50",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12336v2",
    "published_date": "2024-06-18 07:03:34 UTC",
    "updated_date": "2024-12-02 04:08:49 UTC"
  },
  {
    "arxiv_id": "2406.12331v1",
    "title": "Retrieval Meets Reasoning: Dynamic In-Context Editing for Long-Text Understanding",
    "authors": [
      "Weizhi Fei",
      "Xueyan Niu",
      "Guoqing Xie",
      "Yanhua Zhang",
      "Bo Bai",
      "Lei Deng",
      "Wei Han"
    ],
    "abstract": "Current Large Language Models (LLMs) face inherent limitations due to their\npre-defined context lengths, which impede their capacity for multi-hop\nreasoning within extensive textual contexts. While existing techniques like\nRetrieval-Augmented Generation (RAG) have attempted to bridge this gap by\nsourcing external information, they fall short when direct answers are not\nreadily available. We introduce a novel approach that re-imagines information\nretrieval through dynamic in-context editing, inspired by recent breakthroughs\nin knowledge editing. By treating lengthy contexts as malleable external\nknowledge, our method interactively gathers and integrates relevant\ninformation, thereby enabling LLMs to perform sophisticated reasoning steps.\nExperimental results demonstrate that our method effectively empowers\ncontext-limited LLMs, such as Llama2, to engage in multi-hop reasoning with\nimproved performance, which outperforms state-of-the-art context window\nextrapolation methods and even compares favorably to more advanced commercial\nlong-context models. Our interactive method not only enhances reasoning\ncapabilities but also mitigates the associated training and computational\ncosts, making it a pragmatic solution for enhancing LLMs' reasoning within\nexpansive contexts.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12331v1",
    "published_date": "2024-06-18 06:54:28 UTC",
    "updated_date": "2024-06-18 06:54:28 UTC"
  },
  {
    "arxiv_id": "2406.12326v1",
    "title": "Toward Exploring the Code Understanding Capabilities of Pre-trained Code Generation Models",
    "authors": [
      "Jiayi Lin",
      "Yutao Xie",
      "Yue Yu",
      "Yibiao Yang",
      "Lei Zhang"
    ],
    "abstract": "Recently, large code generation models trained in a self-supervised manner on\nextensive unlabeled programming language data have achieved remarkable success.\nWhile these models acquire vast amounts of code knowledge, they perform poorly\non code understanding tasks, such as code search and clone detection, as they\nare specifically trained for generation. Pre-training a larger encoder-only\narchitecture model from scratch on massive code data can improve understanding\nperformance. However, this approach is costly and time-consuming, making it\nsuboptimal. In this paper, we pioneer the transfer of knowledge from\npre-trained code generation models to code understanding tasks, significantly\nreducing training costs. We examine effective strategies for enabling\ndecoder-only models to acquire robust code representations. Furthermore, we\nintroduce CL4D, a contrastive learning method designed to enhance the\nrepresentation capabilities of decoder-only models. Comprehensive experiments\ndemonstrate that our approach achieves state-of-the-art performance in\nunderstanding tasks such as code search and clone detection. Our analysis shows\nthat our method effectively reduces the distance between semantically identical\nsamples in the representation space. These findings suggest the potential for\nunifying code understanding and generation tasks using a decoder-only\nstructured model.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "8 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.12326v1",
    "published_date": "2024-06-18 06:52:14 UTC",
    "updated_date": "2024-06-18 06:52:14 UTC"
  },
  {
    "arxiv_id": "2406.12321v1",
    "title": "Automatic benchmarking of large multimodal models via iterative experiment programming",
    "authors": [
      "Alessandro Conti",
      "Enrico Fini",
      "Paolo Rota",
      "Yiming Wang",
      "Massimiliano Mancini",
      "Elisa Ricci"
    ],
    "abstract": "Assessing the capabilities of large multimodal models (LMMs) often requires\nthe creation of ad-hoc evaluations. Currently, building new benchmarks requires\ntremendous amounts of manual work for each specific analysis. This makes the\nevaluation process tedious and costly. In this paper, we present APEx,\nAutomatic Programming of Experiments, the first framework for automatic\nbenchmarking of LMMs. Given a research question expressed in natural language,\nAPEx leverages a large language model (LLM) and a library of pre-specified\ntools to generate a set of experiments for the model at hand, and progressively\ncompile a scientific report. The report drives the testing procedure: based on\nthe current status of the investigation, APEx chooses which experiments to\nperform and whether the results are sufficient to draw conclusions. Finally,\nthe LLM refines the report, presenting the results to the user in natural\nlanguage. Thanks to its modularity, our framework is flexible and extensible as\nnew tools become available. Empirically, APEx reproduces the findings of\nexisting studies while allowing for arbitrary analyses and hypothesis testing.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "31 pages, 6 figures, code is available at\n  https://github.com/altndrr/apex",
    "pdf_url": "http://arxiv.org/pdf/2406.12321v1",
    "published_date": "2024-06-18 06:43:46 UTC",
    "updated_date": "2024-06-18 06:43:46 UTC"
  },
  {
    "arxiv_id": "2406.12316v1",
    "title": "Enhancing Visible-Infrared Person Re-identification with Modality- and Instance-aware Visual Prompt Learning",
    "authors": [
      "Ruiqi Wu",
      "Bingliang Jiao",
      "Wenxuan Wang",
      "Meng Liu",
      "Peng Wang"
    ],
    "abstract": "The Visible-Infrared Person Re-identification (VI ReID) aims to match visible\nand infrared images of the same pedestrians across non-overlapped camera views.\nThese two input modalities contain both invariant information, such as shape,\nand modality-specific details, such as color. An ideal model should utilize\nvaluable information from both modalities during training for enhanced\nrepresentational capability. However, the gap caused by modality-specific\ninformation poses substantial challenges for the VI ReID model to handle\ndistinct modality inputs simultaneously. To address this, we introduce the\nModality-aware and Instance-aware Visual Prompts (MIP) network in our work,\ndesigned to effectively utilize both invariant and specific information for\nidentification. Specifically, our MIP model is built on the transformer\narchitecture. In this model, we have designed a series of modality-specific\nprompts, which could enable our model to adapt to and make use of the specific\ninformation inherent in different modality inputs, thereby reducing the\ninterference caused by the modality gap and achieving better identification.\nBesides, we also employ each pedestrian feature to construct a group of\ninstance-specific prompts. These customized prompts are responsible for guiding\nour model to adapt to each pedestrian instance dynamically, thereby capturing\nidentity-level discriminative clues for identification. Through extensive\nexperiments on SYSU-MM01 and RegDB datasets, the effectiveness of both our\ndesigned modules is evaluated. Additionally, our proposed MIP performs better\nthan most state-of-the-art methods.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepyed by ACM International Conference on Multimedia Retrieval\n  (ICMR'24)",
    "pdf_url": "http://arxiv.org/pdf/2406.12316v1",
    "published_date": "2024-06-18 06:39:03 UTC",
    "updated_date": "2024-06-18 06:39:03 UTC"
  },
  {
    "arxiv_id": "2406.12315v7",
    "title": "A Comprehensive Study of Structural Pruning for Vision Models",
    "authors": [
      "Changhao Li",
      "Haoling Li",
      "Mengqi Xue",
      "Gongfan Fang",
      "Sheng Zhou",
      "Zunlei Feng",
      "Huiqiong Wang",
      "Mingli Song",
      "Jie Song"
    ],
    "abstract": "Structural pruning has emerged as a promising approach for producing more\nefficient models. Nevertheless, the community suffers from a lack of\nstandardized benchmarks and metrics, leaving the progress in this area not\nfully comprehended. To fill this gap, we present the first comprehensive\nbenchmark, termed PruningBench, for structural pruning. PruningBench showcases\nthe following three characteristics: 1) PruningBench employs a unified and\nconsistent framework for evaluating the effectiveness of diverse structural\npruning techniques; 2) PruningBench systematically evaluates 16 existing\npruning methods, encompassing a wide array of models (e.g., CNNs and ViTs) and\ntasks (e.g., classification and detection); 3) PruningBench provides easily\nimplementable interfaces to facilitate the implementation of future pruning\nmethods, and enables the subsequent researchers to incorporate their work into\nour leaderboards. We provide an online pruning platform for customizing pruning\ntasks and reproducing all results in this paper. Leaderboard results can also\nbe available.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper aims to introduce an evaluation benchmark for structural\n  pruning. The complete text spans 25 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.12315v7",
    "published_date": "2024-06-18 06:37:26 UTC",
    "updated_date": "2025-01-21 06:31:34 UTC"
  },
  {
    "arxiv_id": "2406.12298v1",
    "title": "Research on Dangerous Flight Weather Prediction based on Machine Learning",
    "authors": [
      "Haoxing Liu",
      "Renjie Xie",
      "Haoshen Qin",
      "Yizhou Li"
    ],
    "abstract": "With the continuous expansion of the scale of air transport, the demand for\naviation meteorological support also continues to grow. The impact of hazardous\nweather on flight safety is critical. How to effectively use meteorological\ndata to improve the early warning capability of flight dangerous weather and\nensure the safe flight of aircraft is the primary task of aviation\nmeteorological services. In this work, support vector machine (SVM) models are\nused to predict hazardous flight weather, especially for meteorological\nconditions with high uncertainty such as storms and turbulence. SVM is a\nsupervised learning method that distinguishes between different classes of data\nby finding optimal decision boundaries in a high-dimensional space. In order to\nmeet the needs of this study, we chose the radial basis function (RBF) as the\nkernel function, which helps to deal with nonlinear problems and enables the\nmodel to better capture complex meteorological data structures. During the\nmodel training phase, we used historical meteorological observations from\nmultiple weather stations, including temperature, humidity, wind speed, wind\ndirection, and other meteorological indicators closely related to flight\nsafety. From this data, the SVM model learns how to distinguish between normal\nand dangerous flight weather conditions.",
    "categories": [
      "cs.AI",
      "physics.ao-ph"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12298v1",
    "published_date": "2024-06-18 06:08:15 UTC",
    "updated_date": "2024-06-18 06:08:15 UTC"
  },
  {
    "arxiv_id": "2406.12297v1",
    "title": "Faithful Density-Peaks Clustering via Matrix Computations on MPI Parallelization System",
    "authors": [
      "Ji Xu",
      "Tianlong Xiao",
      "Jinye Yang",
      "Panpan Zhu"
    ],
    "abstract": "Density peaks clustering (DP) has the ability of detecting clusters of\narbitrary shape and clustering non-Euclidean space data, but its quadratic\ncomplexity in both computing and storage makes it difficult to scale for big\ndata. Various approaches have been proposed in this regard, including MapReduce\nbased distribution computing, multi-core parallelism, presentation\ntransformation (e.g., kd-tree, Z-value), granular computing, and so forth.\nHowever, most of these existing methods face two limitations. One is their\ntarget datasets are mostly constrained to be in Euclidian space, the other is\nthey emphasize only on local neighbors while ignoring global data distribution\ndue to restriction to cut-off kernel when computing density. To address the two\nissues, we present a faithful and parallel DP method that makes use of two\ntypes of vector-like distance matrices and an inverse leading-node-finding\npolicy. The method is implemented on a message passing interface (MPI) system.\nExtensive experiments showed that our method is capable of clustering\nnon-Euclidean data such as in community detection, while outperforming the\nstate-of-the-art counterpart methods in accuracy when clustering large\nEuclidean data. Our code is publicly available at\nhttps://github.com/alanxuji/FaithPDP.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper presents a novel approach FaithPDP that takes advantages\n  of both hardware (multi-core architecture of CPU) and modern programming\n  language (Python or Matlab for efficient vector and matrix computation) to\n  achieve clustering result identical to vanilla DP algorithm, while the\n  computing complexity is reduced to pseudo-linear",
    "pdf_url": "http://arxiv.org/pdf/2406.12297v1",
    "published_date": "2024-06-18 06:05:45 UTC",
    "updated_date": "2024-06-18 06:05:45 UTC"
  },
  {
    "arxiv_id": "2406.12296v2",
    "title": "Generative Artificial Intelligence-Guided User Studies: An Application for Air Taxi Services",
    "authors": [
      "Shengdi Xiao",
      "Jingjing Li",
      "Tatsuki Fushimi",
      "Yoichi Ochiai"
    ],
    "abstract": "User studies are crucial for meeting user needs. In user studies, real\nexperimental scenarios and participants are constructed and recruited. However,\nemerging and unfamiliar studies face limitations, including safety concerns and\niterative efficiency. To address these challenges, this study utilises a\nGenerative Artificial Intelligence (GenAI) to create GenAI-generated scenarios\nfor user experience (UX). By recruiting real users to evaluate this experience,\nwe can collect feedback that enables rapid iteration in the early design phase.\nThe air taxi is particularly representative of these challenges and has been\nchosen as the case study for this research. The key contribution was designing\nan Air Taxi Journey (ATJ) using Large Language Models (LLMs) and AI image and\nvideo generators. Based on the GPT-4-generated scripts, key visuals were\ncreated for the air taxi, and the ATJ was evaluated by 72 participants.\nFurthermore, the LLMs demonstrated the ability to identify and suggest\nenvironments that significantly improve participants' willingness toward air\ntaxis. Education level and gender significantly influenced participants' the\ndifference in willingness and their satisfaction with the ATJ. Satisfaction\nwith the ATJ serves as a mediator, significantly influencing participants'\nwillingness to take air taxis. Our study confirms the capability of GenAI to\nsupport user studies, providing a feasible approach and valuable insights for\ndesigning air taxi UX in the early design phase.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "39 pages, 6 main figures, 10 appendix figures",
    "pdf_url": "http://arxiv.org/pdf/2406.12296v2",
    "published_date": "2024-06-18 06:00:18 UTC",
    "updated_date": "2025-03-04 07:52:08 UTC"
  },
  {
    "arxiv_id": "2406.12292v1",
    "title": "JEN-1 DreamStyler: Customized Musical Concept Learning via Pivotal Parameters Tuning",
    "authors": [
      "Boyu Chen",
      "Peike Li",
      "Yao Yao",
      "Alex Wang"
    ],
    "abstract": "Large models for text-to-music generation have achieved significant progress,\nfacilitating the creation of high-quality and varied musical compositions from\nprovided text prompts. However, input text prompts may not precisely capture\nuser requirements, particularly when the objective is to generate music that\nembodies a specific concept derived from a designated reference collection. In\nthis paper, we propose a novel method for customized text-to-music generation,\nwhich can capture the concept from a two-minute reference music and generate a\nnew piece of music conforming to the concept. We achieve this by fine-tuning a\npretrained text-to-music model using the reference music. However, directly\nfine-tuning all parameters leads to overfitting issues. To address this\nproblem, we propose a Pivotal Parameters Tuning method that enables the model\nto assimilate the new concept while preserving its original generative\ncapabilities. Additionally, we identify a potential concept conflict when\nintroducing multiple concepts into the pretrained model. We present a concept\nenhancement strategy to distinguish multiple concepts, enabling the fine-tuned\nmodel to generate music incorporating either individual or multiple concepts\nsimultaneously. Since we are the first to work on the customized music\ngeneration task, we also introduce a new dataset and evaluation protocol for\nthe new task. Our proposed Jen1-DreamStyler outperforms several baselines in\nboth qualitative and quantitative evaluations. Demos will be available at\nhttps://www.jenmusic.ai/research#DreamStyler.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12292v1",
    "published_date": "2024-06-18 05:54:11 UTC",
    "updated_date": "2024-06-18 05:54:11 UTC"
  },
  {
    "arxiv_id": "2406.12288v3",
    "title": "An Investigation of Neuron Activation as a Unified Lens to Explain Chain-of-Thought Eliciting Arithmetic Reasoning of LLMs",
    "authors": [
      "Daking Rai",
      "Ziyu Yao"
    ],
    "abstract": "Large language models (LLMs) have shown strong arithmetic reasoning\ncapabilities when prompted with Chain-of-Thought (CoT) prompts. However, we\nhave only a limited understanding of how they are processed by LLMs. To\ndemystify it, prior work has primarily focused on ablating different components\nin the CoT prompt and empirically observing their resulting LLM performance\nchange. Yet, the reason why these components are important to LLM reasoning is\nnot explored. To fill this gap, in this work, we investigate ``neuron\nactivation'' as a lens to provide a unified explanation to observations made by\nprior work. Specifically, we look into neurons within the feed-forward layers\nof LLMs that may have activated their arithmetic reasoning capabilities, using\nLlama2 as an example. To facilitate this investigation, we also propose an\napproach based on GPT-4 to automatically identify neurons that imply arithmetic\nreasoning. Our analyses revealed that the activation of reasoning neurons in\nthe feed-forward layers of an LLM can explain the importance of various\ncomponents in a CoT prompt, and future research can extend it for a more\ncomplete understanding.",
    "categories": [
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages, 1 figure, to be published in ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.12288v3",
    "published_date": "2024-06-18 05:49:24 UTC",
    "updated_date": "2024-09-02 17:12:48 UTC"
  },
  {
    "arxiv_id": "2406.12285v2",
    "title": "DASSF: Dynamic-Attention Scale-Sequence Fusion for Aerial Object Detection",
    "authors": [
      "Haodong Li",
      "Haicheng Qu"
    ],
    "abstract": "The detection of small objects in aerial images is a fundamental task in the\nfield of computer vision. Moving objects in aerial photography have problems\nsuch as different shapes and sizes, dense overlap, occlusion by the background,\nand object blur, however, the original YOLO algorithm has low overall detection\naccuracy due to its weak ability to perceive targets of different scales. In\norder to improve the detection accuracy of densely overlapping small targets\nand fuzzy targets, this paper proposes a dynamic-attention scale-sequence\nfusion algorithm (DASSF) for small target detection in aerial images. First, we\npropose a dynamic scale sequence feature fusion (DSSFF) module that improves\nthe up-sampling mechanism and reduces computational load. Secondly, a x-small\nobject detection head is specially added to enhance the detection capability of\nsmall targets. Finally, in order to improve the expressive ability of targets\nof different types and sizes, we use the dynamic head (DyHead). The model we\nproposed solves the problem of small target detection in aerial images and can\nbe applied to multiple different versions of the YOLO algorithm, which is\nuniversal. Experimental results show that when the DASSF method is applied to\nYOLOv8, compared to YOLOv8n, on the VisDrone-2019 and DIOR datasets, the model\nshows an increase of 9.2% and 2.4% in the mean average precision (mAP),\nrespectively, and outperforms the current mainstream methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12285v2",
    "published_date": "2024-06-18 05:26:44 UTC",
    "updated_date": "2024-06-22 07:48:32 UTC"
  },
  {
    "arxiv_id": "2406.12284v2",
    "title": "Demystifying the Recency Heuristic in Temporal-Difference Learning",
    "authors": [
      "Brett Daley",
      "Marlos C. Machado",
      "Martha White"
    ],
    "abstract": "The recency heuristic in reinforcement learning is the assumption that\nstimuli that occurred closer in time to an acquired reward should be more\nheavily reinforced. The recency heuristic is one of the key assumptions made by\nTD($\\lambda$), which reinforces recent experiences according to an\nexponentially decaying weighting. In fact, all other widely used return\nestimators for TD learning, such as $n$-step returns, satisfy a weaker (i.e.,\nnon-monotonic) recency heuristic. Why is the recency heuristic effective for\ntemporal credit assignment? What happens when credit is assigned in a way that\nviolates this heuristic? In this paper, we analyze the specific mathematical\nimplications of adopting the recency heuristic in TD learning. We prove that\nany return estimator satisfying this heuristic: 1) is guaranteed to converge to\nthe correct value function, 2) has a relatively fast contraction rate, and 3)\nhas a long window of effective credit assignment, yet bounded worst-case\nvariance. We also give a counterexample where on-policy, tabular TD methods\nviolating the recency heuristic diverge. Our results offer some of the first\ntheoretical evidence that credit assignment based on the recency heuristic\nfacilitates learning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "RLC 2024. 18 pages, 8 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2406.12284v2",
    "published_date": "2024-06-18 05:23:29 UTC",
    "updated_date": "2024-08-26 11:33:13 UTC"
  },
  {
    "arxiv_id": "2406.12276v1",
    "title": "CodeNav: Beyond tool-use to using real-world codebases with LLM agents",
    "authors": [
      "Tanmay Gupta",
      "Luca Weihs",
      "Aniruddha Kembhavi"
    ],
    "abstract": "We present CodeNav, an LLM agent that navigates and leverages previously\nunseen code repositories to solve user queries. In contrast to tool-use LLM\nagents that require ``registration'' of all relevant tools via manual\ndescriptions within the LLM context, CodeNav automatically indexes and searches\nover code blocks in the target codebase, finds relevant code snippets, imports\nthem, and uses them to iteratively generate a solution with execution feedback.\nTo highlight the core-capabilities of CodeNav, we first showcase three case\nstudies where we use CodeNav for solving complex user queries using three\ndiverse codebases. Next, on three benchmarks, we quantitatively compare the\neffectiveness of code-use (which only has access to the target codebase) to\ntool-use (which has privileged access to all tool names and descriptions).\nFinally, we study the effect of varying kinds of tool and library descriptions\non code-use performance, as well as investigate the advantage of the agent\nseeing source code as opposed to natural descriptions of code. All code will be\nmade open source under a permissive license.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12276v1",
    "published_date": "2024-06-18 05:10:38 UTC",
    "updated_date": "2024-06-18 05:10:38 UTC"
  },
  {
    "arxiv_id": "2406.12272v6",
    "title": "Slot State Space Models",
    "authors": [
      "Jindong Jiang",
      "Fei Deng",
      "Gautam Singh",
      "Minseung Lee",
      "Sungjin Ahn"
    ],
    "abstract": "Recent State Space Models (SSMs) such as S4, S5, and Mamba have shown\nremarkable computational benefits in long-range temporal dependency modeling.\nHowever, in many sequence modeling problems, the underlying process is\ninherently modular and it is of interest to have inductive biases that mimic\nthis modular structure. In this paper, we introduce SlotSSMs, a novel framework\nfor incorporating independent mechanisms into SSMs to preserve or encourage\nseparation of information. Unlike conventional SSMs that maintain a monolithic\nstate vector, SlotSSMs maintains the state as a collection of multiple vectors\ncalled slots. Crucially, the state transitions are performed independently per\nslot with sparse interactions across slots implemented via the bottleneck of\nself-attention. In experiments, we evaluate our model in object-centric\nlearning, 3D visual reasoning, and long-context video understanding tasks,\nwhich involve modeling multiple objects and their long-range temporal\ndependencies. We find that our proposed design offers substantial performance\ngains over existing sequence modeling methods. Project page is available at\nhttps://slotssms.github.io/",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to NeurIPS 2024; Project page is available at\n  https://slotssms.github.io/ ; Code is available at\n  https://github.com/JindongJiang/SlotSSMs",
    "pdf_url": "http://arxiv.org/pdf/2406.12272v6",
    "published_date": "2024-06-18 04:59:14 UTC",
    "updated_date": "2024-11-29 21:23:51 UTC"
  },
  {
    "arxiv_id": "2406.12264v1",
    "title": "Projection Methods for Operator Learning and Universal Approximation",
    "authors": [
      "Emanuele Zappala"
    ],
    "abstract": "We obtain a new universal approximation theorem for continuous operators on\narbitrary Banach spaces using the Leray-Schauder mapping. Moreover, we\nintroduce and study a method for operator learning in Banach spaces $L^p$ of\nfunctions with multiple variables, based on orthogonal projections on\npolynomial bases. We derive a universal approximation result for operators\nwhere we learn a linear projection and a finite dimensional mapping under some\nadditional assumptions. For the case of $p=2$, we give some sufficient\nconditions for the approximation results to hold. This article serves as the\ntheoretical framework for a deep learning methodology whose implementation will\nbe provided in subsequent work.",
    "categories": [
      "math.NA",
      "cs.AI",
      "cs.LG",
      "cs.NA"
    ],
    "primary_category": "math.NA",
    "comment": "10 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.12264v1",
    "published_date": "2024-06-18 04:44:05 UTC",
    "updated_date": "2024-06-18 04:44:05 UTC"
  },
  {
    "arxiv_id": "2406.12260v1",
    "title": "Self-Supervised Time-Series Anomaly Detection Using Learnable Data Augmentation",
    "authors": [
      "Kukjin Choi",
      "Jihun Yi",
      "Jisoo Mok",
      "Sungroh Yoon"
    ],
    "abstract": "Continuous efforts are being made to advance anomaly detection in various\nmanufacturing processes to increase the productivity and safety of industrial\nsites. Deep learning replaced rule-based methods and recently emerged as a\npromising method for anomaly detection in diverse industries. However, in the\nreal world, the scarcity of abnormal data and difficulties in obtaining labeled\ndata create limitations in the training of detection models. In this study, we\naddressed these shortcomings by proposing a learnable data augmentation-based\ntime-series anomaly detection (LATAD) technique that is trained in a\nself-supervised manner. LATAD extracts discriminative features from time-series\ndata through contrastive learning. At the same time, learnable data\naugmentation produces challenging negative samples to enhance learning\nefficiency. We measured anomaly scores of the proposed technique based on\nlatent feature similarities. As per the results, LATAD exhibited comparable or\nimproved performance to the state-of-the-art anomaly detection assessments on\nseveral benchmark datasets and provided a gradient-based diagnosis technique to\nhelp identify root causes.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, 4 figures, IEEE Transactions on Emerging Topics in\n  Computational Intelligence",
    "pdf_url": "http://arxiv.org/pdf/2406.12260v1",
    "published_date": "2024-06-18 04:25:56 UTC",
    "updated_date": "2024-06-18 04:25:56 UTC"
  },
  {
    "arxiv_id": "2406.12259v3",
    "title": "Adversarial Attacks on Large Language Models in Medicine",
    "authors": [
      "Yifan Yang",
      "Qiao Jin",
      "Furong Huang",
      "Zhiyong Lu"
    ],
    "abstract": "The integration of Large Language Models (LLMs) into healthcare applications\noffers promising advancements in medical diagnostics, treatment\nrecommendations, and patient care. However, the susceptibility of LLMs to\nadversarial attacks poses a significant threat, potentially leading to harmful\noutcomes in delicate medical contexts. This study investigates the\nvulnerability of LLMs to two types of adversarial attacks in three medical\ntasks. Utilizing real-world patient data, we demonstrate that both open-source\nand proprietary LLMs are susceptible to manipulation across multiple tasks.\nThis research further reveals that domain-specific tasks demand more\nadversarial data in model fine-tuning than general domain tasks for effective\nattack execution, especially for more capable models. We discover that while\nintegrating adversarial data does not markedly degrade overall model\nperformance on medical benchmarks, it does lead to noticeable shifts in\nfine-tuned model weights, suggesting a potential pathway for detecting and\ncountering model attacks. This research highlights the urgent need for robust\nsecurity measures and the development of defensive mechanisms to safeguard LLMs\nin medical applications, to ensure their safe and effective deployment in\nhealthcare settings.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12259v3",
    "published_date": "2024-06-18 04:24:30 UTC",
    "updated_date": "2024-12-16 19:32:32 UTC"
  },
  {
    "arxiv_id": "2406.12257v3",
    "title": "CleanGen: Mitigating Backdoor Attacks for Generation Tasks in Large Language Models",
    "authors": [
      "Yuetai Li",
      "Zhangchen Xu",
      "Fengqing Jiang",
      "Luyao Niu",
      "Dinuka Sahabandu",
      "Bhaskar Ramasubramanian",
      "Radha Poovendran"
    ],
    "abstract": "The remarkable performance of large language models (LLMs) in generation\ntasks has enabled practitioners to leverage publicly available models to power\ncustom applications, such as chatbots and virtual assistants. However, the data\nused to train or fine-tune these LLMs is often undisclosed, allowing an\nattacker to compromise the data and inject backdoors into the models. In this\npaper, we develop a novel inference time defense, named CLEANGEN, to mitigate\nbackdoor attacks for generation tasks in LLMs. CLEANGEN is a lightweight and\neffective decoding strategy that is compatible with the state-of-the-art (SOTA)\nLLMs. Our insight behind CLEANGEN is that compared to other LLMs, backdoored\nLLMs assign significantly higher probabilities to tokens representing the\nattacker-desired contents. These discrepancies in token probabilities enable\nCLEANGEN to identify suspicious tokens favored by the attacker and replace them\nwith tokens generated by another LLM that is not compromised by the same\nattacker, thereby avoiding generation of attacker-desired content. We evaluate\nCLEANGEN against five SOTA backdoor attacks. Our results show that CLEANGEN\nachieves lower attack success rates (ASR) compared to five SOTA baseline\ndefenses for all five backdoor attacks. Moreover, LLMs deploying CLEANGEN\nmaintain helpfulness in their responses when serving benign user queries with\nminimal added computational overhead.",
    "categories": [
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper is presented at EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.12257v3",
    "published_date": "2024-06-18 04:10:38 UTC",
    "updated_date": "2025-03-27 16:21:02 UTC"
  },
  {
    "arxiv_id": "2406.12255v1",
    "title": "A Hopfieldian View-based Interpretation for Chain-of-Thought Reasoning",
    "authors": [
      "Lijie Hu",
      "Liang Liu",
      "Shu Yang",
      "Xin Chen",
      "Hongru Xiao",
      "Mengdi Li",
      "Pan Zhou",
      "Muhammad Asif Ali",
      "Di Wang"
    ],
    "abstract": "Chain-of-Thought (CoT) holds a significant place in augmenting the reasoning\nperformance for large language models (LLMs). While some studies focus on\nimproving CoT accuracy through methods like retrieval enhancement, yet a\nrigorous explanation for why CoT achieves such success remains unclear. In this\npaper, we analyze CoT methods under two different settings by asking the\nfollowing questions: (1) For zero-shot CoT, why does prompting the model with\n\"let's think step by step\" significantly impact its outputs? (2) For few-shot\nCoT, why does providing examples before questioning the model could\nsubstantially improve its reasoning ability? To answer these questions, we\nconduct a top-down explainable analysis from the Hopfieldian view and propose a\nRead-and-Control approach for controlling the accuracy of CoT. Through\nextensive experiments on seven datasets for three different tasks, we\ndemonstrate that our framework can decipher the inner workings of CoT, provide\nreasoning error localization, and control to come up with the correct reasoning\npath.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "21 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.12255v1",
    "published_date": "2024-06-18 04:07:13 UTC",
    "updated_date": "2024-06-18 04:07:13 UTC"
  },
  {
    "arxiv_id": "2406.12251v1",
    "title": "Mitigate Negative Transfer with Similarity Heuristic Lifelong Prompt Tuning",
    "authors": [
      "Chenyuan Wu",
      "Gangwei Jiang",
      "Defu Lian"
    ],
    "abstract": "Lifelong prompt tuning has significantly advanced parameter-efficient\nlifelong learning with its efficiency and minimal storage demands on various\ntasks. Our empirical studies, however, highlights certain transferability\nconstraints in the current methodologies: a universal algorithm that guarantees\nconsistent positive transfer across all tasks is currently unattainable,\nespecially when dealing dissimilar tasks that may engender negative transfer.\nIdentifying the misalignment between algorithm selection and task specificity\nas the primary cause of negative transfer, we present the Similarity Heuristic\nLifelong Prompt Tuning (SHLPT) framework. This innovative strategy partitions\ntasks into two distinct subsets by harnessing a learnable similarity metric,\nthereby facilitating fruitful transfer from tasks regardless of their\nsimilarity or dissimilarity. Additionally, SHLPT incorporates a parameter pool\nto combat catastrophic forgetting effectively. Our experiments shows that SHLPT\noutperforms state-of-the-art techniques in lifelong learning benchmarks and\ndemonstrates robustness against negative transfer in diverse task sequences.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL 2024 Findings",
    "pdf_url": "http://arxiv.org/pdf/2406.12251v1",
    "published_date": "2024-06-18 03:57:49 UTC",
    "updated_date": "2024-06-18 03:57:49 UTC"
  },
  {
    "arxiv_id": "2406.12243v1",
    "title": "CherryRec: Enhancing News Recommendation Quality via LLM-driven Framework",
    "authors": [
      "Shaohuang Wang",
      "Lun Wang",
      "Yunhan Bu",
      "Tianwei Huang"
    ],
    "abstract": "Large Language Models (LLMs) have achieved remarkable progress in language\nunderstanding and generation. Custom LLMs leveraging textual features have been\napplied to recommendation systems, demonstrating improvements across various\nrecommendation scenarios. However, most existing methods perform untrained\nrecommendation based on pre-trained knowledge (e.g., movie recommendation), and\nthe auto-regressive generation of LLMs leads to slow inference speeds, making\nthem less effective in real-time recommendations.To address this, we propose a\nframework for news recommendation using LLMs, named \\textit{CherryRec}, which\nensures the quality of recommendations while accelerating the recommendation\nprocess. Specifically, we employ a Knowledge-aware News Rapid Selector to\nretrieve candidate options based on the user's interaction history. The history\nand retrieved items are then input as text into a fine-tuned LLM, the\nContent-aware News Llm Evaluator, designed to enhance news recommendation\ncapabilities. Finally, the Value-aware News Scorer integrates the scores to\ncompute the CherryRec Score, which serves as the basis for the final\nrecommendation.We validate the effectiveness of the proposed framework by\ncomparing it with state-of-the-art baseline methods on benchmark datasets. Our\nexperimental results consistently show that CherryRec outperforms the baselines\nin both recommendation performance and efficiency.The project resource can be\naccessed at: \\url{https://github.com/xxxxxx}",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12243v1",
    "published_date": "2024-06-18 03:33:38 UTC",
    "updated_date": "2024-06-18 03:33:38 UTC"
  },
  {
    "arxiv_id": "2406.12242v1",
    "title": "GMP-AR: Granularity Message Passing and Adaptive Reconciliation for Temporal Hierarchy Forecasting",
    "authors": [
      "Fan Zhou",
      "Chen Pan",
      "Lintao Ma",
      "Yu Liu",
      "James Zhang",
      "Jun Zhou",
      "Hongyuan Mei",
      "Weitao Lin",
      "Zi Zhuang",
      "Wenxin Ning",
      "Yunhua Hu",
      "Siqiao Xue"
    ],
    "abstract": "Time series forecasts of different temporal granularity are widely used in\nreal-world applications, e.g., sales prediction in days and weeks for making\ndifferent inventory plans. However, these tasks are usually solved separately\nwithout ensuring coherence, which is crucial for aligning downstream decisions.\nPrevious works mainly focus on ensuring coherence with some straightforward\nmethods, e.g., aggregation from the forecasts of fine granularity to the coarse\nones, and allocation from the coarse granularity to the fine ones. These\nmethods merely take the temporal hierarchical structure to maintain coherence\nwithout improving the forecasting accuracy. In this paper, we propose a novel\ngranularity message-passing mechanism (GMP) that leverages temporal hierarchy\ninformation to improve forecasting performance and also utilizes an adaptive\nreconciliation (AR) strategy to maintain coherence without performance loss.\nFurthermore, we introduce an optimization module to achieve task-based targets\nwhile adhering to more real-world constraints. Experiments on real-world\ndatasets demonstrate that our framework (GMP-AR) achieves superior performances\non temporal hierarchical forecasting tasks compared to state-of-the-art\nmethods. In addition, our framework has been successfully applied to a\nreal-world task of payment traffic management in Alipay by integrating with the\ntask-based optimization module.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12242v1",
    "published_date": "2024-06-18 03:33:03 UTC",
    "updated_date": "2024-06-18 03:33:03 UTC"
  },
  {
    "arxiv_id": "2406.12241v1",
    "title": "More Efficient Randomized Exploration for Reinforcement Learning via Approximate Sampling",
    "authors": [
      "Haque Ishfaq",
      "Yixin Tan",
      "Yu Yang",
      "Qingfeng Lan",
      "Jianfeng Lu",
      "A. Rupam Mahmood",
      "Doina Precup",
      "Pan Xu"
    ],
    "abstract": "Thompson sampling (TS) is one of the most popular exploration techniques in\nreinforcement learning (RL). However, most TS algorithms with theoretical\nguarantees are difficult to implement and not generalizable to Deep RL. While\nthe emerging approximate sampling-based exploration schemes are promising, most\nexisting algorithms are specific to linear Markov Decision Processes (MDP) with\nsuboptimal regret bounds, or only use the most basic samplers such as Langevin\nMonte Carlo. In this work, we propose an algorithmic framework that\nincorporates different approximate sampling methods with the recently proposed\nFeel-Good Thompson Sampling (FGTS) approach (Zhang, 2022; Dann et al., 2021),\nwhich was previously known to be computationally intractable in general. When\napplied to linear MDPs, our regret analysis yields the best known dependency of\nregret on dimensionality, surpassing existing randomized algorithms.\nAdditionally, we provide explicit sampling complexity for each employed\nsampler. Empirically, we show that in tasks where deep exploration is\nnecessary, our proposed algorithms that combine FGTS and approximate sampling\nperform significantly better compared to other strong baselines. On several\nchallenging games from the Atari 57 suite, our algorithms achieve performance\nthat is either better than or on par with other strong baselines from the deep\nRL literature.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "First two authors contributed equally. Accepted to the Reinforcement\n  Learning Conference (RLC) 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.12241v1",
    "published_date": "2024-06-18 03:32:10 UTC",
    "updated_date": "2024-06-18 03:32:10 UTC"
  },
  {
    "arxiv_id": "2406.12233v1",
    "title": "SyncVSR: Data-Efficient Visual Speech Recognition with End-to-End Crossmodal Audio Token Synchronization",
    "authors": [
      "Young Jin Ahn",
      "Jungwoo Park",
      "Sangha Park",
      "Jonghyun Choi",
      "Kee-Eung Kim"
    ],
    "abstract": "Visual Speech Recognition (VSR) stands at the intersection of computer vision\nand speech recognition, aiming to interpret spoken content from visual cues. A\nprominent challenge in VSR is the presence of homophenes-visually similar lip\ngestures that represent different phonemes. Prior approaches have sought to\ndistinguish fine-grained visemes by aligning visual and auditory semantics, but\noften fell short of full synchronization. To address this, we present SyncVSR,\nan end-to-end learning framework that leverages quantized audio for frame-level\ncrossmodal supervision. By integrating a projection layer that synchronizes\nvisual representation with acoustic data, our encoder learns to generate\ndiscrete audio tokens from a video sequence in a non-autoregressive manner.\nSyncVSR shows versatility across tasks, languages, and modalities at the cost\nof a forward pass. Our empirical evaluations show that it not only achieves\nstate-of-the-art results but also reduces data usage by up to ninefold.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12233v1",
    "published_date": "2024-06-18 03:14:22 UTC",
    "updated_date": "2024-06-18 03:14:22 UTC"
  },
  {
    "arxiv_id": "2406.12232v2",
    "title": "\"You Gotta be a Doctor, Lin\": An Investigation of Name-Based Bias of Large Language Models in Employment Recommendations",
    "authors": [
      "Huy Nghiem",
      "John Prindle",
      "Jieyu Zhao",
      "Hal Daumé III"
    ],
    "abstract": "Social science research has shown that candidates with names indicative of\ncertain races or genders often face discrimination in employment practices.\nSimilarly, Large Language Models (LLMs) have demonstrated racial and gender\nbiases in various applications. In this study, we utilize GPT-3.5-Turbo and\nLlama 3-70B-Instruct to simulate hiring decisions and salary recommendations\nfor candidates with 320 first names that strongly signal their race and gender,\nacross over 750,000 prompts. Our empirical results indicate a preference among\nthese models for hiring candidates with White female-sounding names over other\ndemographic groups across 40 occupations. Additionally, even among candidates\nwith identical qualifications, salary recommendations vary by as much as 5%\nbetween different subgroups. A comparison with real-world labor data reveals\ninconsistent alignment with U.S. labor market characteristics, underscoring the\nnecessity of risk investigation of LLM-powered systems.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "EMNLP 2024, 20 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.12232v2",
    "published_date": "2024-06-18 03:11:43 UTC",
    "updated_date": "2024-10-05 21:13:27 UTC"
  },
  {
    "arxiv_id": "2406.12230v2",
    "title": "MCSD: An Efficient Language Model with Diverse Fusion",
    "authors": [
      "Hua Yang",
      "Duohai Li",
      "Shiman Li"
    ],
    "abstract": "Transformers excel in Natural Language Processing (NLP) due to their prowess\nin capturing long-term dependencies but suffer from exponential resource\nconsumption with increasing sequence lengths. To address these challenges, we\npropose MCSD model, an efficient language model with linear scaling and fast\ninference speed. MCSD model leverages diverse feature fusion, primarily through\nthe multi-channel slope and decay (MCSD) block, to robustly represent features.\nThis block comprises slope and decay sections that extract features across\ndiverse temporal receptive fields, facilitating capture of both local and\nglobal information. In addition, MCSD block conducts element-wise fusion of\ndiverse features to further enhance the delicate feature extraction capability.\nFor inference, we formulate the inference process into a recurrent\nrepresentation, slashing space complexity to $O(1)$ and time complexity to\n$O(N)$ respectively. Our experiments show that MCSD attains higher throughput\nand lower GPU memory consumption compared to Transformers, while maintaining\ncomparable performance to larger-scale language learning models on benchmark\ntests. These attributes position MCSD as a promising base for edge deployment\nand embodied intelligence.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.12230v2",
    "published_date": "2024-06-18 03:08:01 UTC",
    "updated_date": "2024-07-11 03:29:19 UTC"
  },
  {
    "arxiv_id": "2406.12229v1",
    "title": "Spatially Resolved Gene Expression Prediction from Histology via Multi-view Graph Contrastive Learning with HSIC-bottleneck Regularization",
    "authors": [
      "Changxi Chi",
      "Hang Shi",
      "Qi Zhu",
      "Daoqiang Zhang",
      "Wei Shao"
    ],
    "abstract": "The rapid development of spatial transcriptomics(ST) enables the measurement\nof gene expression at spatial resolution, making it possible to simultaneously\nprofile the gene expression, spatial locations of spots, and the matched\nhistopathological images. However, the cost for collecting ST data is much\nhigher than acquiring histopathological images, and thus several studies\nattempt to predict the gene expression on ST by leveraging their corresponding\nhistopathological images. Most of the existing image-based gene prediction\nmodels treat the prediction task on each spot of ST data independently, which\nignores the spatial dependency among spots. In addition, while the histology\nimages share phenotypic characteristics with the ST data, it is still challenge\nto extract such common information to help align paired image and expression\nrepresentations. To address the above issues, we propose a Multi-view Graph\nContrastive Learning framework with HSIC-bottleneck Regularization(ST-GCHB)\naiming at learning shared representation to help impute the gene expression of\nthe queried imagingspots by considering their spatial dependency.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12229v1",
    "published_date": "2024-06-18 03:07:25 UTC",
    "updated_date": "2024-06-18 03:07:25 UTC"
  },
  {
    "arxiv_id": "2406.12227v3",
    "title": "Refine Large Language Model Fine-tuning via Instruction Vector",
    "authors": [
      "Gangwei Jiang",
      "Zhaoyi Li",
      "Defu Lian",
      "Ying Wei"
    ],
    "abstract": "Fine-tuning large language models (LLMs) can cause them to lose their general\ncapabilities. However, the intrinsic mechanisms behind such forgetting remain\nunexplored. In this paper, we begin by examining this phenomenon by focusing on\nknowledge understanding and instruction following, with the latter identified\nas the main contributor to forgetting during fine-tuning. Consequently, we\npropose the Instruction Vector (IV) framework to capture model representations\nhighly related to specific instruction-following capabilities, thereby making\nit possible to understand model-intrinsic forgetting. Through the analysis of\nIV dynamics pre and post-training, we suggest that fine-tuning mostly adds\nspecialized reasoning patterns instead of erasing previous skills, which may\nappear as forgetting. Building on this insight, we develop IV-guided training,\nwhich aims to preserve original computation graph, thereby mitigating\ncatastrophic forgetting. Empirical tests on three benchmarks confirm the\nefficacy of this new approach, supporting the relationship between IVs and\nforgetting. Our code will be made available soon.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12227v3",
    "published_date": "2024-06-18 03:05:08 UTC",
    "updated_date": "2024-11-28 18:26:28 UTC"
  },
  {
    "arxiv_id": "2406.16942v1",
    "title": "Enhancing Diagnostic Reliability of Foundation Model with Uncertainty Estimation in OCT Images",
    "authors": [
      "Yuanyuan Peng",
      "Aidi Lin",
      "Meng Wang",
      "Tian Lin",
      "Ke Zou",
      "Yinglin Cheng",
      "Tingkun Shi",
      "Xulong Liao",
      "Lixia Feng",
      "Zhen Liang",
      "Xinjian Chen",
      "Huazhu Fu",
      "Haoyu Chen"
    ],
    "abstract": "Inability to express the confidence level and detect unseen classes has\nlimited the clinical implementation of artificial intelligence in the\nreal-world. We developed a foundation model with uncertainty estimation (FMUE)\nto detect 11 retinal conditions on optical coherence tomography (OCT). In the\ninternal test set, FMUE achieved a higher F1 score of 96.76% than two\nstate-of-the-art algorithms, RETFound and UIOS, and got further improvement\nwith thresholding strategy to 98.44%. In the external test sets obtained from\nother OCT devices, FMUE achieved an accuracy of 88.75% and 92.73% before and\nafter thresholding. Our model is superior to two ophthalmologists with a higher\nF1 score (95.17% vs. 61.93% &71.72%). Besides, our model correctly predicts\nhigh uncertainty scores for samples with ambiguous features, of\nnon-target-category diseases, or with low-quality to prompt manual checks and\nprevent misdiagnosis. FMUE provides a trustworthy method for automatic retinal\nanomalies detection in the real-world clinical open set environment.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "All codes are available at https://github.com/yuanyuanpeng0129/FMUE",
    "pdf_url": "http://arxiv.org/pdf/2406.16942v1",
    "published_date": "2024-06-18 03:04:52 UTC",
    "updated_date": "2024-06-18 03:04:52 UTC"
  },
  {
    "arxiv_id": "2406.12222v1",
    "title": "BadSampler: Harnessing the Power of Catastrophic Forgetting to Poison Byzantine-robust Federated Learning",
    "authors": [
      "Yi Liu",
      "Cong Wang",
      "Xingliang Yuan"
    ],
    "abstract": "Federated Learning (FL) is susceptible to poisoning attacks, wherein\ncompromised clients manipulate the global model by modifying local datasets or\nsending manipulated model updates. Experienced defenders can readily detect and\nmitigate the poisoning effects of malicious behaviors using Byzantine-robust\naggregation rules. However, the exploration of poisoning attacks in scenarios\nwhere such behaviors are absent remains largely unexplored for Byzantine-robust\nFL. This paper addresses the challenging problem of poisoning Byzantine-robust\nFL by introducing catastrophic forgetting. To fill this gap, we first formally\ndefine generalization error and establish its connection to catastrophic\nforgetting, paving the way for the development of a clean-label data poisoning\nattack named BadSampler. This attack leverages only clean-label data (i.e.,\nwithout poisoned data) to poison Byzantine-robust FL and requires the adversary\nto selectively sample training data with high loss to feed model training and\nmaximize the model's generalization error. We formulate the attack as an\noptimization problem and present two elegant adversarial sampling strategies,\nTop-$\\kappa$ sampling, and meta-sampling, to approximately solve it.\nAdditionally, our formal error upper bound and time complexity analysis\ndemonstrate that our design can preserve attack utility with high efficiency.\nExtensive evaluations on two real-world datasets illustrate the effectiveness\nand performance of our proposed attacks.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "In Proceedings of the 30th ACM SIGKDD Conference on Knowledge\n  Discovery and Data Mining (KDD' 24), August 25-29, 2024, Barcelona, Spain",
    "pdf_url": "http://arxiv.org/pdf/2406.12222v1",
    "published_date": "2024-06-18 02:43:56 UTC",
    "updated_date": "2024-06-18 02:43:56 UTC"
  },
  {
    "arxiv_id": "2406.12216v1",
    "title": "Is persona enough for personality? Using ChatGPT to reconstruct an agent's latent personality from simple descriptions",
    "authors": [
      "Yongyi Ji",
      "Zhisheng Tang",
      "Mayank Kejriwal"
    ],
    "abstract": "Personality, a fundamental aspect of human cognition, contains a range of\ntraits that influence behaviors, thoughts, and emotions. This paper explores\nthe capabilities of large language models (LLMs) in reconstructing these\ncomplex cognitive attributes based only on simple descriptions containing\nsocio-demographic and personality type information. Utilizing the HEXACO\npersonality framework, our study examines the consistency of LLMs in recovering\nand predicting underlying (latent) personality dimensions from simple\ndescriptions. Our experiments reveal a significant degree of consistency in\npersonality reconstruction, although some inconsistencies and biases, such as a\ntendency to default to positive traits in the absence of explicit information,\nare also observed. Additionally, socio-demographic factors like age and number\nof children were found to influence the reconstructed personality dimensions.\nThese findings have implications for building sophisticated agent-based\nsimulacra using LLMs and highlight the need for further research on robust\npersonality generation in LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to the ICML 2024 Workshop on Large Language Models and\n  Cognition",
    "pdf_url": "http://arxiv.org/pdf/2406.12216v1",
    "published_date": "2024-06-18 02:32:57 UTC",
    "updated_date": "2024-06-18 02:32:57 UTC"
  },
  {
    "arxiv_id": "2406.12213v4",
    "title": "AI-Oracle Machines for Intelligent Computing",
    "authors": [
      "Jie Wang"
    ],
    "abstract": "We introduce the concept of AI-oracle machines for intelligent computing and\noutline several applications to demonstrate their potential. Following this, we\nadvocate for the development of a comprehensive platform to streamline the\nimplementation of AI-oracle machines.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.FL",
      "F.1.1; F.4.1; I.2.0"
    ],
    "primary_category": "cs.CL",
    "comment": "6 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.12213v4",
    "published_date": "2024-06-18 02:25:33 UTC",
    "updated_date": "2024-10-16 00:55:50 UTC"
  },
  {
    "arxiv_id": "2406.12208v1",
    "title": "Knowledge Fusion By Evolving Weights of Language Models",
    "authors": [
      "Guodong Du",
      "Jing Li",
      "Hanting Liu",
      "Runhua Jiang",
      "Shuyang Yu",
      "Yifei Guo",
      "Sim Kuan Goh",
      "Ho-Kin Tang"
    ],
    "abstract": "Fine-tuning pre-trained language models, particularly large language models,\ndemands extensive computing resources and can result in varying performance\noutcomes across different domains and datasets. This paper examines the\napproach of integrating multiple models from diverse training scenarios into a\nunified model. This unified model excels across various data domains and\nexhibits the ability to generalize well on out-of-domain data. We propose a\nknowledge fusion method named Evolver, inspired by evolutionary algorithms,\nwhich does not need further training or additional training data. Specifically,\nour method involves aggregating the weights of different language models into a\npopulation and subsequently generating offspring models through mutation and\ncrossover operations. These offspring models are then evaluated against their\nparents, allowing for the preservation of those models that show enhanced\nperformance on development datasets. Importantly, our model evolving strategy\ncan be seamlessly integrated with existing model merging frameworks, offering a\nversatile tool for model enhancement. Experimental results on mainstream\nlanguage models (i.e., encoder-only, decoder-only, encoder-decoder) reveal that\nEvolver outperforms previous state-of-the-art models by large margins. The code\nis publicly available at {https://github.com/duguodong7/model-evolution}.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.NE"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by ACL2024 Findings",
    "pdf_url": "http://arxiv.org/pdf/2406.12208v1",
    "published_date": "2024-06-18 02:12:34 UTC",
    "updated_date": "2024-06-18 02:12:34 UTC"
  },
  {
    "arxiv_id": "2406.12205v1",
    "title": "Order-Optimal Instance-Dependent Bounds for Offline Reinforcement Learning with Preference Feedback",
    "authors": [
      "Zhirui Chen",
      "Vincent Y. F. Tan"
    ],
    "abstract": "We consider offline reinforcement learning (RL) with preference feedback in\nwhich the implicit reward is a linear function of an unknown parameter. Given\nan offline dataset, our objective consists in ascertaining the optimal action\nfor each state, with the ultimate goal of minimizing the {\\em simple regret}.\nWe propose an algorithm, \\underline{RL} with \\underline{L}ocally\n\\underline{O}ptimal \\underline{W}eights or {\\sc RL-LOW}, which yields a simple\nregret of $\\exp ( - \\Omega(n/H) )$ where $n$ is the number of data samples and\n$H$ denotes an instance-dependent hardness quantity that depends explicitly on\nthe suboptimality gap of each action. Furthermore, we derive a\nfirst-of-its-kind instance-dependent lower bound in offline RL with preference\nfeedback. Interestingly, we observe that the lower and upper bounds on the\nsimple regret match order-wise in the exponent, demonstrating order-wise\noptimality of {\\sc RL-LOW}. In view of privacy considerations in practical\napplications, we also extend {\\sc RL-LOW} to the setting of\n$(\\varepsilon,\\delta)$-differential privacy and show, somewhat surprisingly,\nthat the hardness parameter $H$ is unchanged in the asymptotic regime as $n$\ntends to infinity; this underscores the inherent efficiency of {\\sc RL-LOW} in\nterms of preserving the privacy of the observed rewards. Given our focus on\nestablishing instance-dependent bounds, our work stands in stark contrast to\nprevious works that focus on establishing worst-case regrets for offline RL\nwith preference feedback.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "math.IT",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to Models of Human Feedback for AI Alignment Workshop, ICML\n  2024",
    "pdf_url": "http://arxiv.org/pdf/2406.12205v1",
    "published_date": "2024-06-18 02:03:12 UTC",
    "updated_date": "2024-06-18 02:03:12 UTC"
  },
  {
    "arxiv_id": "2406.12203v3",
    "title": "InterIntent: Investigating Social Intelligence of LLMs via Intention Understanding in an Interactive Game Context",
    "authors": [
      "Ziyi Liu",
      "Abhishek Anand",
      "Pei Zhou",
      "Jen-tse Huang",
      "Jieyu Zhao"
    ],
    "abstract": "Large language models (LLMs) have demonstrated the potential to mimic human\nsocial intelligence. However, most studies focus on simplistic and static\nself-report or performance-based tests, which limits the depth and validity of\nthe analysis. In this paper, we developed a novel framework, InterIntent, to\nassess LLMs' social intelligence by mapping their ability to understand and\nmanage intentions in a game setting. We focus on four dimensions of social\nintelligence: situational awareness, self-regulation, self-awareness, and\ntheory of mind. Each dimension is linked to a specific game task: intention\nselection, intention following, intention summarization, and intention\nguessing. Our findings indicate that while LLMs exhibit high proficiency in\nselecting intentions, achieving an accuracy of 88%, their ability to infer the\nintentions of others is significantly weaker, trailing human performance by\n20%. Additionally, game performance correlates with intention understanding,\nhighlighting the importance of the four components towards success in this\ngame. These findings underline the crucial role of intention understanding in\nevaluating LLMs' social intelligence and highlight the potential of using\nsocial deduction games as a complex testbed to enhance LLM evaluation.\nInterIntent contributes a structured approach to bridging the evaluation gap in\nsocial intelligence within multiplayer games.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12203v3",
    "published_date": "2024-06-18 02:02:15 UTC",
    "updated_date": "2024-11-03 16:15:22 UTC"
  },
  {
    "arxiv_id": "2406.12199v3",
    "title": "Time Series Modeling for Heart Rate Prediction: From ARIMA to Transformers",
    "authors": [
      "Haowei Ni",
      "Shuchen Meng",
      "Xieming Geng",
      "Panfeng Li",
      "Zhuoying Li",
      "Xupeng Chen",
      "Xiaotong Wang",
      "Shiyao Zhang"
    ],
    "abstract": "Cardiovascular disease (CVD) is a leading cause of death globally,\nnecessitating precise forecasting models for monitoring vital signs like heart\nrate, blood pressure, and ECG. Traditional models, such as ARIMA and Prophet,\nare limited by their need for manual parameter tuning and challenges in\nhandling noisy, sparse, and highly variable medical data. This study\ninvestigates advanced deep learning models, including LSTM, and\ntransformer-based architectures, for predicting heart rate time series from the\nMIT-BIH Database. Results demonstrate that deep learning models, particularly\nPatchTST, significantly outperform traditional models across multiple metrics,\ncapturing complex patterns and dependencies more effectively. This research\nunderscores the potential of deep learning to enhance patient monitoring and\nCVD management, suggesting substantial clinical benefits. Future work should\nextend these findings to larger, more diverse datasets and real-world clinical\napplications to further validate and optimize model performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by 2024 6th International Conference on Electronic\n  Engineering and Informatics",
    "pdf_url": "http://arxiv.org/pdf/2406.12199v3",
    "published_date": "2024-06-18 01:55:37 UTC",
    "updated_date": "2024-11-12 07:28:08 UTC"
  },
  {
    "arxiv_id": "2406.12197v1",
    "title": "Debate as Optimization: Adaptive Conformal Prediction and Diverse Retrieval for Event Extraction",
    "authors": [
      "Sijia Wang",
      "Lifu Huang"
    ],
    "abstract": "We propose a multi-agent debate as optimization (DAO) system for event\nextraction, where the primary objective is to iteratively refine the large\nlanguage models (LLMs) outputs through debating without parameter tuning. In\nDAO, we introduce two novel modules: the Diverse-RAG (DRAG) module and the\nAdaptive Conformal Prediction (AdaCP) module. DRAG systematically retrieves\nsupporting information that best fits the debate discussion, while AdaCP\nenhances the accuracy and reliability of event extraction by effectively\nrejecting less promising answers. Experimental results demonstrate a\nsignificant reduction in the performance gap between supervised approaches and\ntuning-free LLM-based methods by 18.1% and 17.8% on ACE05 and 17.9% and 15.2%\non CASIE for event detection and argument extraction respectively.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12197v1",
    "published_date": "2024-06-18 01:53:49 UTC",
    "updated_date": "2024-06-18 01:53:49 UTC"
  },
  {
    "arxiv_id": "2406.15488v1",
    "title": "Orangutan: A Multiscale Brain Emulation-Based Artificial Intelligence Framework for Dynamic Environments",
    "authors": [
      "Yong Xie"
    ],
    "abstract": "Achieving General Artificial Intelligence (AGI) has long been a grand\nchallenge in the field of AI, and brain-inspired computing is widely\nacknowledged as one of the most promising approaches to realize this goal. This\npaper introduces a novel brain-inspired AI framework, Orangutan. It simulates\nthe structure and computational mechanisms of biological brains on multiple\nscales, encompassing multi-compartment neuron architectures, diverse synaptic\nconnection modalities, neural microcircuits, cortical columns, and brain\nregions, as well as biochemical processes including facilitation, feedforward\ninhibition, short-term potentiation, and short-term depression, all grounded in\nsolid neuroscience. Building upon these highly integrated brain-like\nmechanisms, I have developed a sensorimotor model that simulates human saccadic\neye movements during object observation. The model's algorithmic efficacy was\nvalidated through testing with the observation of handwritten digit images.",
    "categories": [
      "q-bio.NC",
      "cs.AI"
    ],
    "primary_category": "q-bio.NC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15488v1",
    "published_date": "2024-06-18 01:41:57 UTC",
    "updated_date": "2024-06-18 01:41:57 UTC"
  },
  {
    "arxiv_id": "2406.12182v1",
    "title": "Aqulia-Med LLM: Pioneering Full-Process Open-Source Medical Language Models",
    "authors": [
      "Lulu Zhao",
      "Weihao Zeng",
      "Xiaofeng Shi",
      "Hua Zhou",
      "Donglin Hao",
      "Yonghua Lin"
    ],
    "abstract": "Recently, both closed-source LLMs and open-source communities have made\nsignificant strides, outperforming humans in various general domains. However,\ntheir performance in specific professional fields such as medicine, especially\nwithin the open-source community, remains suboptimal due to the complexity of\nmedical knowledge. We propose Aquila-Med, a bilingual medical LLM based on\nAquila, addressing these challenges through continue pre-training, supervised\nfine-tuning (SFT), and reinforcement learning from human feedback (RLHF). We\nconstruct a large-scale Chinese and English medical dataset for continue\npre-training and a high-quality SFT dataset, covering extensive medical\nspecialties. Additionally, we develop a high-quality Direct Preference\nOptimization (DPO) dataset for further alignment. Aquila-Med achieves notable\nresults across single-turn, multi-turn dialogues, and medical multiple-choice\nquestions, demonstrating the effectiveness of our approach. We open-source the\ndatasets and the entire training process, contributing valuable resources to\nthe research community. Our models and datasets will released at\nhttps://huggingface.co/BAAI/AquilaMed-RL.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12182v1",
    "published_date": "2024-06-18 01:30:07 UTC",
    "updated_date": "2024-06-18 01:30:07 UTC"
  },
  {
    "arxiv_id": "2406.12172v1",
    "title": "Navigating the Labyrinth: Evaluating and Enhancing LLMs' Ability to Reason About Search Problems",
    "authors": [
      "Nasim Borazjanizadeh",
      "Roei Herzig",
      "Trevor Darrell",
      "Rogerio Feris",
      "Leonid Karlinsky"
    ],
    "abstract": "Recently, Large Language Models (LLMs) attained impressive performance in\nmath and reasoning benchmarks. However, they still often struggle with logic\nproblems and puzzles that are relatively easy for humans. To further\ninvestigate this, we introduce a new benchmark, SearchBench, containing 11\nunique search problem types, each equipped with automated pipelines to generate\nan arbitrary number of instances and analyze the feasibility, correctness, and\noptimality of LLM-generated solutions. We show that even the most advanced LLMs\nfail to solve these problems end-to-end in text, e.g. GPT4 solves only 1.4%.\nSearchBench problems require considering multiple pathways to the solution as\nwell as backtracking, posing a significant challenge to auto-regressive models.\nInstructing LLMs to generate code that solves the problem helps, but only\nslightly, e.g., GPT4's performance rises to 11.7%. In this work, we show that\nin-context learning with A* algorithm implementations enhances performance. The\nfull potential of this promoting approach emerges when combined with our\nproposed Multi-Stage-Multi-Try method, which breaks down the algorithm\nimplementation into two stages and verifies the first stage against unit tests,\nraising GPT-4's performance above 57%.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12172v1",
    "published_date": "2024-06-18 00:44:58 UTC",
    "updated_date": "2024-06-18 00:44:58 UTC"
  },
  {
    "arxiv_id": "2406.12168v4",
    "title": "BPO: Staying Close to the Behavior LLM Creates Better Online LLM Alignment",
    "authors": [
      "Wenda Xu",
      "Jiachen Li",
      "William Yang Wang",
      "Lei Li"
    ],
    "abstract": "Direct alignment from preferences (DAP) has emerged as a promising paradigm\nfor aligning large language models (LLMs) to human desiderata from\npre-collected, offline preference datasets. While recent studies indicate that\nexisting offline DAP methods can directly benefit from online training samples,\nwe highlight the need to develop specific online DAP algorithms to fully\nharness the power of online training. Specifically, we identify that the\nlearned LLM should adhere to the proximity of the behavior LLM, which collects\nthe training samples. To this end, we propose online Preference Optimization in\nproximity to the Behavior LLM (BPO), emphasizing the importance of constructing\na proper trust region for LLM alignment.\n  We conduct extensive experiments to validate the effectiveness and\napplicability of our approach by integrating it with various DAP methods,\nresulting in significant performance improvements across a wide range of tasks\nwhen training with the same amount of preference data. Even when only\nintroducing one additional data collection phase, our online BPO improves its\noffline DAP baseline from 72.0% to 80.2% on TL;DR and from 82.2% to 89.1% on\nAnthropic Helpfulness in terms of win rate against human reference text.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Wenda Xu and Jiachen Li contributed equally. Accepted by EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.12168v4",
    "published_date": "2024-06-18 00:41:40 UTC",
    "updated_date": "2024-10-21 18:00:54 UTC"
  },
  {
    "arxiv_id": "2406.12164v2",
    "title": "A Mel Spectrogram Enhancement Paradigm Based on CWT in Speech Synthesis",
    "authors": [
      "Guoqiang Hu",
      "Huaning Tan",
      "Ruilai Li"
    ],
    "abstract": "Acoustic features play an important role in improving the quality of the\nsynthesised speech. Currently, the Mel spectrogram is a widely employed\nacoustic feature in most acoustic models. However, due to the fine-grained loss\ncaused by its Fourier transform process, the clarity of speech synthesised by\nMel spectrogram is compromised in mutant signals. In order to obtain a more\ndetailed Mel spectrogram, we propose a Mel spectrogram enhancement paradigm\nbased on the continuous wavelet transform (CWT). This paradigm introduces an\nadditional task: a more detailed wavelet spectrogram, which like the\npost-processing network takes as input the Mel spectrogram output by the\ndecoder. We choose Tacotron2 and Fastspeech2 for experimental validation in\norder to test autoregressive (AR) and non-autoregressive (NAR) speech systems,\nrespectively. The experimental results demonstrate that the speech synthesised\nusing the model with the Mel spectrogram enhancement paradigm exhibits higher\nMOS, with an improvement of 0.14 and 0.09 compared to the baseline model,\nrespectively. These findings provide some validation for the universality of\nthe enhancement paradigm, as they demonstrate the success of the paradigm in\ndifferent architectures.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted by IALP 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.12164v2",
    "published_date": "2024-06-18 00:34:44 UTC",
    "updated_date": "2024-07-09 18:21:48 UTC"
  },
  {
    "arxiv_id": "2406.12163v1",
    "title": "Discussion Graph Semantics of First-Order Logic with Equality for Reasoning about Discussion and Argumentation",
    "authors": [
      "Ryuta Arisaka"
    ],
    "abstract": "We formulate discussion graph semantics of first-order logic with equality\nfor reasoning about discussion and argumentation as naturally as we would\nreason about sentences. While there are a few existing proposals to use a\nformal logic for reasoning about argumentation, they are constructed bottom-up\nand specialised to the argumentation model by Dung. There is indeed a\nconspicuous lack of a formal reasoning framework for handling general\ndiscussion and argumentation models. We achieve the generality through a\ntop-down formulation of the semantics of first-order logic (with equality)\nformulas, addressing the current shortage.",
    "categories": [
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12163v1",
    "published_date": "2024-06-18 00:32:00 UTC",
    "updated_date": "2024-06-18 00:32:00 UTC"
  },
  {
    "arxiv_id": "2406.12158v1",
    "title": "LLMs Are Prone to Fallacies in Causal Inference",
    "authors": [
      "Nitish Joshi",
      "Abulhair Saparov",
      "Yixin Wang",
      "He He"
    ],
    "abstract": "Recent work shows that causal facts can be effectively extracted from LLMs\nthrough prompting, facilitating the creation of causal graphs for causal\ninference tasks. However, it is unclear if this success is limited to\nexplicitly-mentioned causal facts in the pretraining data which the model can\nmemorize. Thus, this work investigates: Can LLMs infer causal relations from\nother relational data in text? To disentangle the role of memorized causal\nfacts vs inferred causal relations, we finetune LLMs on synthetic data\ncontaining temporal, spatial and counterfactual relations, and measure whether\nthe LLM can then infer causal relations. We find that: (a) LLMs are susceptible\nto inferring causal relations from the order of two entity mentions in text\n(e.g. X mentioned before Y implies X causes Y); (b) if the order is randomized,\nLLMs still suffer from the post hoc fallacy, i.e. X occurs before Y (temporal\nrelation) implies X causes Y. We also find that while LLMs can correctly deduce\nthe absence of causal relations from temporal and spatial relations, they have\ndifficulty inferring causal relations from counterfactuals, questioning their\nunderstanding of causality.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.12158v1",
    "published_date": "2024-06-18 00:14:07 UTC",
    "updated_date": "2024-06-18 00:14:07 UTC"
  }
]